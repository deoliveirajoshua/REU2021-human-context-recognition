{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '90 tBodyAccJerk-max()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 40),\n",
    "            classifier_block(40, 30),\n",
    "            classifier_block(30, 25),\n",
    "            classifier_block(25, 20),\n",
    "            nn.Linear(20, 18)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def get_act_matrix(batch_size, a_dim):\n",
    "    indexes = np.random.randint(a_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((len(indexes), indexes.max()+1))\n",
    "    one_hot[np.arange(len(indexes)),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "    \n",
    "def get_usr_matrix(batch_size, u_dim):\n",
    "    indexes = np.random.randint(u_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((indexes.size, indexes.max()+1))\n",
    "    one_hot[np.arange(indexes.size),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Real Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "    \n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_10 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_11 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_12 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_13 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_14 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_15 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_16 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_17 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_18 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9, X_10, X_11, X_12, X_13, X_14, X_15, X_16, X_17, X_18))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9) + [9] * len(X_10) + [10] * len(X_11) + [11] * len(X_12) + [12] * len(X_13) + [13] * len(X_14) + [14] * len(X_15) + [15] * len(X_16) + [16] * len(X_17) + [17] * len(X_18)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5, 7, 8, 11]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 11.486918687820435, Final Batch Loss: 2.798377275466919\n",
      "Epoch 2, Loss: 11.599191904067993, Final Batch Loss: 2.9199044704437256\n",
      "Epoch 3, Loss: 11.635527610778809, Final Batch Loss: 2.9617674350738525\n",
      "Epoch 4, Loss: 11.411921262741089, Final Batch Loss: 2.746338367462158\n",
      "Epoch 5, Loss: 11.50093126296997, Final Batch Loss: 2.846264123916626\n",
      "Epoch 6, Loss: 11.637593984603882, Final Batch Loss: 2.9973459243774414\n",
      "Epoch 7, Loss: 11.63011884689331, Final Batch Loss: 2.990257978439331\n",
      "Epoch 8, Loss: 11.442826747894287, Final Batch Loss: 2.8147552013397217\n",
      "Epoch 9, Loss: 11.402079343795776, Final Batch Loss: 2.780205726623535\n",
      "Epoch 10, Loss: 11.511808395385742, Final Batch Loss: 2.905625820159912\n",
      "Epoch 11, Loss: 11.507941961288452, Final Batch Loss: 2.9171841144561768\n",
      "Epoch 12, Loss: 11.445812940597534, Final Batch Loss: 2.87876558303833\n",
      "Epoch 13, Loss: 11.36445689201355, Final Batch Loss: 2.822486162185669\n",
      "Epoch 14, Loss: 11.456492185592651, Final Batch Loss: 2.942516326904297\n",
      "Epoch 15, Loss: 11.441012859344482, Final Batch Loss: 2.953533411026001\n",
      "Epoch 16, Loss: 11.318420648574829, Final Batch Loss: 2.871185541152954\n",
      "Epoch 17, Loss: 10.772670030593872, Final Batch Loss: 2.3589510917663574\n",
      "Epoch 18, Loss: 11.59068489074707, Final Batch Loss: 3.241422176361084\n",
      "Epoch 19, Loss: 11.077812671661377, Final Batch Loss: 2.7993011474609375\n",
      "Epoch 20, Loss: 11.012638330459595, Final Batch Loss: 2.7753169536590576\n",
      "Epoch 21, Loss: 11.070128202438354, Final Batch Loss: 2.922328472137451\n",
      "Epoch 22, Loss: 10.626765727996826, Final Batch Loss: 2.5507917404174805\n",
      "Epoch 23, Loss: 11.214023351669312, Final Batch Loss: 3.2043778896331787\n",
      "Epoch 24, Loss: 10.68525218963623, Final Batch Loss: 2.7308762073516846\n",
      "Epoch 25, Loss: 10.050905466079712, Final Batch Loss: 2.152385711669922\n",
      "Epoch 26, Loss: 10.892082929611206, Final Batch Loss: 3.078981399536133\n",
      "Epoch 27, Loss: 10.223842144012451, Final Batch Loss: 2.4960033893585205\n",
      "Epoch 28, Loss: 10.399426698684692, Final Batch Loss: 2.7122929096221924\n",
      "Epoch 29, Loss: 10.509904384613037, Final Batch Loss: 2.875572681427002\n",
      "Epoch 30, Loss: 10.57608699798584, Final Batch Loss: 2.9989418983459473\n",
      "Epoch 31, Loss: 10.244765996932983, Final Batch Loss: 2.738997459411621\n",
      "Epoch 32, Loss: 10.493338346481323, Final Batch Loss: 3.0308687686920166\n",
      "Epoch 33, Loss: 10.310154676437378, Final Batch Loss: 2.910679340362549\n",
      "Epoch 34, Loss: 10.438592433929443, Final Batch Loss: 3.0974385738372803\n",
      "Epoch 35, Loss: 9.243208050727844, Final Batch Loss: 1.9519778490066528\n",
      "Epoch 36, Loss: 9.358975172042847, Final Batch Loss: 2.054612874984741\n",
      "Epoch 37, Loss: 9.617079973220825, Final Batch Loss: 2.3838140964508057\n",
      "Epoch 38, Loss: 9.611811637878418, Final Batch Loss: 2.4649674892425537\n",
      "Epoch 39, Loss: 8.823369026184082, Final Batch Loss: 1.7391812801361084\n",
      "Epoch 40, Loss: 8.523847818374634, Final Batch Loss: 1.490311622619629\n",
      "Epoch 41, Loss: 9.601291179656982, Final Batch Loss: 2.6008687019348145\n",
      "Epoch 42, Loss: 8.971969604492188, Final Batch Loss: 2.010439872741699\n",
      "Epoch 43, Loss: 9.071169376373291, Final Batch Loss: 2.132392406463623\n",
      "Epoch 44, Loss: 9.881520509719849, Final Batch Loss: 3.0103063583374023\n",
      "Epoch 45, Loss: 8.312034368515015, Final Batch Loss: 1.4573373794555664\n",
      "Epoch 46, Loss: 8.394840121269226, Final Batch Loss: 1.6641196012496948\n",
      "Epoch 47, Loss: 8.939610242843628, Final Batch Loss: 2.2917420864105225\n",
      "Epoch 48, Loss: 9.218341827392578, Final Batch Loss: 2.5760560035705566\n",
      "Epoch 49, Loss: 7.584843397140503, Final Batch Loss: 1.0052635669708252\n",
      "Epoch 50, Loss: 8.894497156143188, Final Batch Loss: 2.359262466430664\n",
      "Epoch 51, Loss: 8.104703783988953, Final Batch Loss: 1.68724524974823\n",
      "Epoch 52, Loss: 8.864736080169678, Final Batch Loss: 2.3550920486450195\n",
      "Epoch 53, Loss: 9.083705186843872, Final Batch Loss: 2.7512614727020264\n",
      "Epoch 54, Loss: 7.342721700668335, Final Batch Loss: 1.0082285404205322\n",
      "Epoch 55, Loss: 8.45883846282959, Final Batch Loss: 2.175143241882324\n",
      "Epoch 56, Loss: 8.327207565307617, Final Batch Loss: 2.095973491668701\n",
      "Epoch 57, Loss: 9.07203722000122, Final Batch Loss: 2.8452560901641846\n",
      "Epoch 58, Loss: 8.1324542760849, Final Batch Loss: 1.9956642389297485\n",
      "Epoch 59, Loss: 8.656372308731079, Final Batch Loss: 2.489220380783081\n",
      "Epoch 60, Loss: 7.851642489433289, Final Batch Loss: 1.623867154121399\n",
      "Epoch 61, Loss: 8.469441413879395, Final Batch Loss: 2.2591514587402344\n",
      "Epoch 62, Loss: 8.038228988647461, Final Batch Loss: 1.8527100086212158\n",
      "Epoch 63, Loss: 8.263980627059937, Final Batch Loss: 2.1109466552734375\n",
      "Epoch 64, Loss: 8.09408688545227, Final Batch Loss: 1.9881870746612549\n",
      "Epoch 65, Loss: 7.919499158859253, Final Batch Loss: 1.7534937858581543\n",
      "Epoch 66, Loss: 8.278510689735413, Final Batch Loss: 2.206686496734619\n",
      "Epoch 67, Loss: 7.38087522983551, Final Batch Loss: 1.3136571645736694\n",
      "Epoch 68, Loss: 7.63238000869751, Final Batch Loss: 1.6451332569122314\n",
      "Epoch 69, Loss: 7.4814149141311646, Final Batch Loss: 1.501404047012329\n",
      "Epoch 70, Loss: 7.811169385910034, Final Batch Loss: 1.7824585437774658\n",
      "Epoch 71, Loss: 8.422782182693481, Final Batch Loss: 2.4999070167541504\n",
      "Epoch 72, Loss: 7.677126049995422, Final Batch Loss: 1.7205332517623901\n",
      "Epoch 73, Loss: 8.20791471004486, Final Batch Loss: 2.207839250564575\n",
      "Epoch 74, Loss: 7.869602084159851, Final Batch Loss: 1.9356606006622314\n",
      "Epoch 75, Loss: 8.365870833396912, Final Batch Loss: 2.5098671913146973\n",
      "Epoch 76, Loss: 7.343169212341309, Final Batch Loss: 1.4746614694595337\n",
      "Epoch 77, Loss: 7.220970988273621, Final Batch Loss: 1.3543483018875122\n",
      "Epoch 78, Loss: 8.295525074005127, Final Batch Loss: 2.472771644592285\n",
      "Epoch 79, Loss: 7.9646607637405396, Final Batch Loss: 2.0481679439544678\n",
      "Epoch 80, Loss: 7.086764097213745, Final Batch Loss: 1.1309032440185547\n",
      "Epoch 81, Loss: 7.4357041120529175, Final Batch Loss: 1.5541330575942993\n",
      "Epoch 82, Loss: 7.709172368049622, Final Batch Loss: 1.9573500156402588\n",
      "Epoch 83, Loss: 7.781893253326416, Final Batch Loss: 1.9142677783966064\n",
      "Epoch 84, Loss: 7.276702284812927, Final Batch Loss: 1.4526697397232056\n",
      "Epoch 85, Loss: 7.659197568893433, Final Batch Loss: 1.8696794509887695\n",
      "Epoch 86, Loss: 7.50048553943634, Final Batch Loss: 1.7754627466201782\n",
      "Epoch 87, Loss: 7.238165497779846, Final Batch Loss: 1.4575023651123047\n",
      "Epoch 88, Loss: 7.820049285888672, Final Batch Loss: 1.9633301496505737\n",
      "Epoch 89, Loss: 8.718458652496338, Final Batch Loss: 2.9456276893615723\n",
      "Epoch 90, Loss: 7.788189649581909, Final Batch Loss: 2.070002555847168\n",
      "Epoch 91, Loss: 7.389732837677002, Final Batch Loss: 1.6659700870513916\n",
      "Epoch 92, Loss: 8.122710108757019, Final Batch Loss: 2.46101450920105\n",
      "Epoch 93, Loss: 7.5905152559280396, Final Batch Loss: 1.9470170736312866\n",
      "Epoch 94, Loss: 7.315362215042114, Final Batch Loss: 1.5085351467132568\n",
      "Epoch 95, Loss: 8.115604043006897, Final Batch Loss: 2.3940887451171875\n",
      "Epoch 96, Loss: 7.995926737785339, Final Batch Loss: 2.2823009490966797\n",
      "Epoch 97, Loss: 7.802658915519714, Final Batch Loss: 2.1181888580322266\n",
      "Epoch 98, Loss: 7.603109121322632, Final Batch Loss: 1.9519835710525513\n",
      "Epoch 99, Loss: 6.87555992603302, Final Batch Loss: 1.201633334159851\n",
      "Epoch 100, Loss: 7.806674003601074, Final Batch Loss: 2.2089831829071045\n",
      "Epoch 101, Loss: 6.418807089328766, Final Batch Loss: 0.8035405278205872\n",
      "Epoch 102, Loss: 7.405337691307068, Final Batch Loss: 1.8022539615631104\n",
      "Epoch 103, Loss: 7.366805553436279, Final Batch Loss: 1.7224295139312744\n",
      "Epoch 104, Loss: 6.887804388999939, Final Batch Loss: 1.2665131092071533\n",
      "Epoch 105, Loss: 7.512970924377441, Final Batch Loss: 1.8995766639709473\n",
      "Epoch 106, Loss: 7.300808072090149, Final Batch Loss: 1.6226141452789307\n",
      "Epoch 107, Loss: 6.76896858215332, Final Batch Loss: 1.1134672164916992\n",
      "Epoch 108, Loss: 7.237298846244812, Final Batch Loss: 1.5388484001159668\n",
      "Epoch 109, Loss: 7.170178055763245, Final Batch Loss: 1.3298957347869873\n",
      "Epoch 110, Loss: 7.210780620574951, Final Batch Loss: 1.4608674049377441\n",
      "Epoch 111, Loss: 7.997260093688965, Final Batch Loss: 2.27109432220459\n",
      "Epoch 112, Loss: 6.420096039772034, Final Batch Loss: 0.6722047328948975\n",
      "Epoch 113, Loss: 7.296299576759338, Final Batch Loss: 1.6475567817687988\n",
      "Epoch 114, Loss: 7.151209235191345, Final Batch Loss: 1.45084810256958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115, Loss: 7.2396416664123535, Final Batch Loss: 1.5786840915679932\n",
      "Epoch 116, Loss: 7.486827492713928, Final Batch Loss: 1.866039514541626\n",
      "Epoch 117, Loss: 7.19476854801178, Final Batch Loss: 1.5948172807693481\n",
      "Epoch 118, Loss: 7.391346216201782, Final Batch Loss: 1.7725478410720825\n",
      "Epoch 119, Loss: 7.1940836906433105, Final Batch Loss: 1.5019824504852295\n",
      "Epoch 120, Loss: 6.965549349784851, Final Batch Loss: 1.3292608261108398\n",
      "Epoch 121, Loss: 6.796924591064453, Final Batch Loss: 1.1219438314437866\n",
      "Epoch 122, Loss: 6.946574926376343, Final Batch Loss: 1.2762773036956787\n",
      "Epoch 123, Loss: 7.292706847190857, Final Batch Loss: 1.6512038707733154\n",
      "Epoch 124, Loss: 6.354308903217316, Final Batch Loss: 0.8046358227729797\n",
      "Epoch 125, Loss: 7.070627331733704, Final Batch Loss: 1.4095733165740967\n",
      "Epoch 126, Loss: 7.710625648498535, Final Batch Loss: 1.9397917985916138\n",
      "Epoch 127, Loss: 6.353235900402069, Final Batch Loss: 0.5350474715232849\n",
      "Epoch 128, Loss: 7.784826755523682, Final Batch Loss: 2.1115658283233643\n",
      "Epoch 129, Loss: 6.60129988193512, Final Batch Loss: 0.9660840034484863\n",
      "Epoch 130, Loss: 7.530933380126953, Final Batch Loss: 1.9742636680603027\n",
      "Epoch 131, Loss: 7.522172451019287, Final Batch Loss: 1.8082631826400757\n",
      "Epoch 132, Loss: 6.5548765659332275, Final Batch Loss: 1.0711992979049683\n",
      "Epoch 133, Loss: 6.933685660362244, Final Batch Loss: 1.4007426500320435\n",
      "Epoch 134, Loss: 7.085328698158264, Final Batch Loss: 1.6473652124404907\n",
      "Epoch 135, Loss: 7.468095183372498, Final Batch Loss: 1.9698963165283203\n",
      "Epoch 136, Loss: 6.9122233390808105, Final Batch Loss: 1.5206224918365479\n",
      "Epoch 137, Loss: 7.553930878639221, Final Batch Loss: 2.110966682434082\n",
      "Epoch 138, Loss: 6.653859615325928, Final Batch Loss: 1.2748017311096191\n",
      "Epoch 139, Loss: 7.337223768234253, Final Batch Loss: 1.922041416168213\n",
      "Epoch 140, Loss: 7.091906547546387, Final Batch Loss: 1.7232403755187988\n",
      "Epoch 141, Loss: 7.286388993263245, Final Batch Loss: 1.8567694425582886\n",
      "Epoch 142, Loss: 7.69786262512207, Final Batch Loss: 2.173412561416626\n",
      "Epoch 143, Loss: 6.923871397972107, Final Batch Loss: 1.4624996185302734\n",
      "Epoch 144, Loss: 7.800156593322754, Final Batch Loss: 2.3747196197509766\n",
      "Epoch 145, Loss: 6.249306559562683, Final Batch Loss: 0.8956844806671143\n",
      "Epoch 146, Loss: 6.532943964004517, Final Batch Loss: 1.050157070159912\n",
      "Epoch 147, Loss: 6.4689964056015015, Final Batch Loss: 1.1393933296203613\n",
      "Epoch 148, Loss: 6.982211589813232, Final Batch Loss: 1.660208821296692\n",
      "Epoch 149, Loss: 7.255212068557739, Final Batch Loss: 1.919583797454834\n",
      "Epoch 150, Loss: 6.612122654914856, Final Batch Loss: 1.2477315664291382\n",
      "Epoch 151, Loss: 8.612757444381714, Final Batch Loss: 3.088515281677246\n",
      "Epoch 152, Loss: 6.885547399520874, Final Batch Loss: 1.464115858078003\n",
      "Epoch 153, Loss: 6.849989533424377, Final Batch Loss: 1.3965235948562622\n",
      "Epoch 154, Loss: 7.3737980127334595, Final Batch Loss: 2.0238823890686035\n",
      "Epoch 155, Loss: 7.132994771003723, Final Batch Loss: 1.8577485084533691\n",
      "Epoch 156, Loss: 6.587161064147949, Final Batch Loss: 1.2478286027908325\n",
      "Epoch 157, Loss: 6.5387221574783325, Final Batch Loss: 1.2954094409942627\n",
      "Epoch 158, Loss: 7.699549674987793, Final Batch Loss: 2.393251895904541\n",
      "Epoch 159, Loss: 7.541789770126343, Final Batch Loss: 2.1980433464050293\n",
      "Epoch 160, Loss: 7.572737812995911, Final Batch Loss: 2.3326592445373535\n",
      "Epoch 161, Loss: 9.006508231163025, Final Batch Loss: 3.698490619659424\n",
      "Epoch 162, Loss: 6.87268340587616, Final Batch Loss: 1.3217875957489014\n",
      "Epoch 163, Loss: 6.5974918603897095, Final Batch Loss: 0.8976298570632935\n",
      "Epoch 164, Loss: 8.742199897766113, Final Batch Loss: 3.220327854156494\n",
      "Epoch 165, Loss: 6.744709014892578, Final Batch Loss: 1.472159743309021\n",
      "Epoch 166, Loss: 7.666774392127991, Final Batch Loss: 2.168006420135498\n",
      "Epoch 167, Loss: 7.393303632736206, Final Batch Loss: 1.6309391260147095\n",
      "Epoch 168, Loss: 7.160588622093201, Final Batch Loss: 1.3232672214508057\n",
      "Epoch 169, Loss: 7.216317057609558, Final Batch Loss: 1.6381511688232422\n",
      "Epoch 170, Loss: 7.056977033615112, Final Batch Loss: 1.6707552671432495\n",
      "Epoch 171, Loss: 6.996559500694275, Final Batch Loss: 1.6926594972610474\n",
      "Epoch 172, Loss: 6.363837838172913, Final Batch Loss: 0.967950701713562\n",
      "Epoch 173, Loss: 6.389414310455322, Final Batch Loss: 1.0179884433746338\n",
      "Epoch 174, Loss: 7.73937451839447, Final Batch Loss: 2.4769515991210938\n",
      "Epoch 175, Loss: 6.347986459732056, Final Batch Loss: 0.9940985441207886\n",
      "Epoch 176, Loss: 6.706124901771545, Final Batch Loss: 1.3878554105758667\n",
      "Epoch 177, Loss: 6.852365612983704, Final Batch Loss: 1.5498298406600952\n",
      "Epoch 178, Loss: 8.41628611087799, Final Batch Loss: 3.1924304962158203\n",
      "Epoch 179, Loss: 8.786478877067566, Final Batch Loss: 3.490753173828125\n",
      "Epoch 180, Loss: 8.077068328857422, Final Batch Loss: 2.8670687675476074\n",
      "Epoch 181, Loss: 7.419515609741211, Final Batch Loss: 2.129777431488037\n",
      "Epoch 182, Loss: 7.125467419624329, Final Batch Loss: 1.8840941190719604\n",
      "Epoch 183, Loss: 7.329703211784363, Final Batch Loss: 2.1122236251831055\n",
      "Epoch 184, Loss: 6.700772166252136, Final Batch Loss: 1.483394980430603\n",
      "Epoch 185, Loss: 6.680480003356934, Final Batch Loss: 1.4749231338500977\n",
      "Epoch 186, Loss: 6.275535583496094, Final Batch Loss: 1.070920467376709\n",
      "Epoch 187, Loss: 6.942025184631348, Final Batch Loss: 1.7633845806121826\n",
      "Epoch 188, Loss: 7.091773748397827, Final Batch Loss: 1.915043830871582\n",
      "Epoch 189, Loss: 6.024282872676849, Final Batch Loss: 0.7494261860847473\n",
      "Epoch 190, Loss: 6.621798396110535, Final Batch Loss: 1.4841063022613525\n",
      "Epoch 191, Loss: 7.302513122558594, Final Batch Loss: 2.0167922973632812\n",
      "Epoch 192, Loss: 6.905787825584412, Final Batch Loss: 1.7196917533874512\n",
      "Epoch 193, Loss: 8.155430793762207, Final Batch Loss: 3.0470407009124756\n",
      "Epoch 194, Loss: 6.742509365081787, Final Batch Loss: 1.530139684677124\n",
      "Epoch 195, Loss: 6.7785773277282715, Final Batch Loss: 1.580399990081787\n",
      "Epoch 196, Loss: 6.645611047744751, Final Batch Loss: 1.392508864402771\n",
      "Epoch 197, Loss: 7.415359258651733, Final Batch Loss: 2.2039499282836914\n",
      "Epoch 198, Loss: 7.343973636627197, Final Batch Loss: 2.1465704441070557\n",
      "Epoch 199, Loss: 6.624775409698486, Final Batch Loss: 1.5198372602462769\n",
      "Epoch 200, Loss: 7.594875335693359, Final Batch Loss: 2.315639019012451\n",
      "Epoch 201, Loss: 7.2462064027786255, Final Batch Loss: 2.0490198135375977\n",
      "Epoch 202, Loss: 7.528114080429077, Final Batch Loss: 2.3334741592407227\n",
      "Epoch 203, Loss: 6.164851427078247, Final Batch Loss: 1.037673830986023\n",
      "Epoch 204, Loss: 6.941247940063477, Final Batch Loss: 1.7146154642105103\n",
      "Epoch 205, Loss: 7.268945336341858, Final Batch Loss: 2.0485472679138184\n",
      "Epoch 206, Loss: 7.080795407295227, Final Batch Loss: 1.8077600002288818\n",
      "Epoch 207, Loss: 6.697840332984924, Final Batch Loss: 1.5544898509979248\n",
      "Epoch 208, Loss: 6.639795422554016, Final Batch Loss: 1.516159176826477\n",
      "Epoch 209, Loss: 5.433111369609833, Final Batch Loss: 0.347480833530426\n",
      "Epoch 210, Loss: 7.087916851043701, Final Batch Loss: 2.0012338161468506\n",
      "Epoch 211, Loss: 6.870551347732544, Final Batch Loss: 1.7328524589538574\n",
      "Epoch 212, Loss: 6.001487612724304, Final Batch Loss: 0.8857742547988892\n",
      "Epoch 213, Loss: 7.174768447875977, Final Batch Loss: 2.032907009124756\n",
      "Epoch 214, Loss: 6.739516019821167, Final Batch Loss: 1.5218249559402466\n",
      "Epoch 215, Loss: 6.124529004096985, Final Batch Loss: 0.9509592056274414\n",
      "Epoch 216, Loss: 6.113842010498047, Final Batch Loss: 1.065012812614441\n",
      "Epoch 217, Loss: 5.3762916922569275, Final Batch Loss: 0.34227675199508667\n",
      "Epoch 218, Loss: 6.048006653785706, Final Batch Loss: 1.047731637954712\n",
      "Epoch 219, Loss: 5.8369099497795105, Final Batch Loss: 0.773999035358429\n",
      "Epoch 220, Loss: 6.936862230300903, Final Batch Loss: 1.8927710056304932\n",
      "Epoch 221, Loss: 6.493372201919556, Final Batch Loss: 1.3959100246429443\n",
      "Epoch 222, Loss: 6.850065588951111, Final Batch Loss: 1.752062439918518\n",
      "Epoch 223, Loss: 6.14927351474762, Final Batch Loss: 1.065535068511963\n",
      "Epoch 224, Loss: 6.2511982917785645, Final Batch Loss: 1.2543398141860962\n",
      "Epoch 225, Loss: 6.990131855010986, Final Batch Loss: 1.9730091094970703\n",
      "Epoch 226, Loss: 6.979888439178467, Final Batch Loss: 1.953021764755249\n",
      "Epoch 227, Loss: 6.94241201877594, Final Batch Loss: 1.944711685180664\n",
      "Epoch 228, Loss: 10.392870664596558, Final Batch Loss: 5.389806747436523\n",
      "Epoch 229, Loss: 6.566803932189941, Final Batch Loss: 1.5013054609298706\n",
      "Epoch 230, Loss: 5.7984218299388885, Final Batch Loss: 0.3360292613506317\n",
      "Epoch 231, Loss: 6.732341408729553, Final Batch Loss: 1.1604210138320923\n",
      "Epoch 232, Loss: 5.730015695095062, Final Batch Loss: 0.27611786127090454\n",
      "Epoch 233, Loss: 6.851157903671265, Final Batch Loss: 1.5988577604293823\n",
      "Epoch 234, Loss: 6.7899768352508545, Final Batch Loss: 1.783422827720642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235, Loss: 6.68075966835022, Final Batch Loss: 1.5962300300598145\n",
      "Epoch 236, Loss: 5.947876513004303, Final Batch Loss: 0.9293424487113953\n",
      "Epoch 237, Loss: 5.620170474052429, Final Batch Loss: 0.614837646484375\n",
      "Epoch 238, Loss: 6.640827775001526, Final Batch Loss: 1.595555305480957\n",
      "Epoch 239, Loss: 7.081352472305298, Final Batch Loss: 2.1042845249176025\n",
      "Epoch 240, Loss: 6.387154936790466, Final Batch Loss: 1.4415926933288574\n",
      "Epoch 241, Loss: 6.3840168714523315, Final Batch Loss: 1.4844155311584473\n",
      "Epoch 242, Loss: 5.189683020114899, Final Batch Loss: 0.20042544603347778\n",
      "Epoch 243, Loss: 5.713385164737701, Final Batch Loss: 0.8102836012840271\n",
      "Epoch 244, Loss: 5.828915894031525, Final Batch Loss: 0.8962194323539734\n",
      "Epoch 245, Loss: 6.15729546546936, Final Batch Loss: 1.1651731729507446\n",
      "Epoch 246, Loss: 7.124658107757568, Final Batch Loss: 2.1449666023254395\n",
      "Epoch 247, Loss: 6.077902436256409, Final Batch Loss: 1.1694879531860352\n",
      "Epoch 248, Loss: 7.078617334365845, Final Batch Loss: 2.1833653450012207\n",
      "Epoch 249, Loss: 6.180131912231445, Final Batch Loss: 1.3189269304275513\n",
      "Epoch 250, Loss: 8.293165445327759, Final Batch Loss: 3.4193897247314453\n",
      "Epoch 251, Loss: 5.278917521238327, Final Batch Loss: 0.35913726687431335\n",
      "Epoch 252, Loss: 6.476782441139221, Final Batch Loss: 1.607484221458435\n",
      "Epoch 253, Loss: 7.579859972000122, Final Batch Loss: 2.502997398376465\n",
      "Epoch 254, Loss: 6.282785654067993, Final Batch Loss: 1.2277206182479858\n",
      "Epoch 255, Loss: 5.305461019277573, Final Batch Loss: 0.24599727988243103\n",
      "Epoch 256, Loss: 6.445780396461487, Final Batch Loss: 1.4726314544677734\n",
      "Epoch 257, Loss: 6.674573659896851, Final Batch Loss: 1.8137598037719727\n",
      "Epoch 258, Loss: 6.1023231744766235, Final Batch Loss: 1.2570912837982178\n",
      "Epoch 259, Loss: 7.901245713233948, Final Batch Loss: 3.0166549682617188\n",
      "Epoch 260, Loss: 6.5657042264938354, Final Batch Loss: 1.6306986808776855\n",
      "Epoch 261, Loss: 6.758848667144775, Final Batch Loss: 1.7767060995101929\n",
      "Epoch 262, Loss: 6.366899132728577, Final Batch Loss: 1.4281766414642334\n",
      "Epoch 263, Loss: 8.20414686203003, Final Batch Loss: 3.158719062805176\n",
      "Epoch 264, Loss: 6.142137885093689, Final Batch Loss: 1.2410444021224976\n",
      "Epoch 265, Loss: 7.074767708778381, Final Batch Loss: 2.220092535018921\n",
      "Epoch 266, Loss: 6.861962199211121, Final Batch Loss: 2.0097715854644775\n",
      "Epoch 267, Loss: 5.778823614120483, Final Batch Loss: 1.0100758075714111\n",
      "Epoch 268, Loss: 6.374460458755493, Final Batch Loss: 1.4958640336990356\n",
      "Epoch 269, Loss: 6.27479088306427, Final Batch Loss: 1.4329026937484741\n",
      "Epoch 270, Loss: 5.604749262332916, Final Batch Loss: 0.8193871378898621\n",
      "Epoch 271, Loss: 6.311189532279968, Final Batch Loss: 1.4993672370910645\n",
      "Epoch 272, Loss: 7.060238838195801, Final Batch Loss: 2.316676378250122\n",
      "Epoch 273, Loss: 6.462702989578247, Final Batch Loss: 1.6672182083129883\n",
      "Epoch 274, Loss: 5.4400410652160645, Final Batch Loss: 0.7033833265304565\n",
      "Epoch 275, Loss: 6.90331494808197, Final Batch Loss: 2.187380790710449\n",
      "Epoch 276, Loss: 6.576424241065979, Final Batch Loss: 1.9297465085983276\n",
      "Epoch 277, Loss: 7.631393194198608, Final Batch Loss: 2.848707437515259\n",
      "Epoch 278, Loss: 6.446834087371826, Final Batch Loss: 1.7026447057724\n",
      "Epoch 279, Loss: 6.296541333198547, Final Batch Loss: 1.4434748888015747\n",
      "Epoch 280, Loss: 5.786298632621765, Final Batch Loss: 0.9519774913787842\n",
      "Epoch 281, Loss: 6.007070422172546, Final Batch Loss: 1.1776660680770874\n",
      "Epoch 282, Loss: 6.671344637870789, Final Batch Loss: 1.7719531059265137\n",
      "Epoch 283, Loss: 6.227929353713989, Final Batch Loss: 1.3766446113586426\n",
      "Epoch 284, Loss: 7.156007289886475, Final Batch Loss: 2.2869081497192383\n",
      "Epoch 285, Loss: 6.285039663314819, Final Batch Loss: 1.4093728065490723\n",
      "Epoch 286, Loss: 6.717128872871399, Final Batch Loss: 2.00980544090271\n",
      "Epoch 287, Loss: 5.3978278040885925, Final Batch Loss: 0.5568978190422058\n",
      "Epoch 288, Loss: 7.115316271781921, Final Batch Loss: 2.3465709686279297\n",
      "Epoch 289, Loss: 6.493051767349243, Final Batch Loss: 1.797002911567688\n",
      "Epoch 290, Loss: 7.050521969795227, Final Batch Loss: 2.3280696868896484\n",
      "Epoch 291, Loss: 6.471221446990967, Final Batch Loss: 1.760471224784851\n",
      "Epoch 292, Loss: 8.473800659179688, Final Batch Loss: 3.61592435836792\n",
      "Epoch 293, Loss: 6.381052017211914, Final Batch Loss: 1.5168509483337402\n",
      "Epoch 294, Loss: 5.727401614189148, Final Batch Loss: 1.0510025024414062\n",
      "Epoch 295, Loss: 6.2186479568481445, Final Batch Loss: 1.552215814590454\n",
      "Epoch 296, Loss: 6.29213809967041, Final Batch Loss: 1.512627124786377\n",
      "Epoch 297, Loss: 6.187478303909302, Final Batch Loss: 1.4021098613739014\n",
      "Epoch 298, Loss: 6.316539525985718, Final Batch Loss: 1.5945351123809814\n",
      "Epoch 299, Loss: 6.849067687988281, Final Batch Loss: 2.172714948654175\n",
      "Epoch 300, Loss: 6.877595782279968, Final Batch Loss: 2.1302993297576904\n",
      "Epoch 301, Loss: 5.384444415569305, Final Batch Loss: 0.6720499396324158\n",
      "Epoch 302, Loss: 6.401087403297424, Final Batch Loss: 1.6367719173431396\n",
      "Epoch 303, Loss: 6.352121829986572, Final Batch Loss: 1.7542093992233276\n",
      "Epoch 304, Loss: 6.3177759647369385, Final Batch Loss: 1.5321385860443115\n",
      "Epoch 305, Loss: 7.837597370147705, Final Batch Loss: 3.160998582839966\n",
      "Epoch 306, Loss: 6.592996835708618, Final Batch Loss: 1.845512866973877\n",
      "Epoch 307, Loss: 6.4622862339019775, Final Batch Loss: 1.689378261566162\n",
      "Epoch 308, Loss: 6.030532479286194, Final Batch Loss: 1.3140965700149536\n",
      "Epoch 309, Loss: 6.242414474487305, Final Batch Loss: 1.5626333951950073\n",
      "Epoch 310, Loss: 6.696407079696655, Final Batch Loss: 2.15023136138916\n",
      "Epoch 311, Loss: 5.934142231941223, Final Batch Loss: 1.364484429359436\n",
      "Epoch 312, Loss: 7.451295733451843, Final Batch Loss: 2.7783589363098145\n",
      "Epoch 313, Loss: 6.40901780128479, Final Batch Loss: 1.8246504068374634\n",
      "Epoch 314, Loss: 6.2472580671310425, Final Batch Loss: 1.5565203428268433\n",
      "Epoch 315, Loss: 5.896350383758545, Final Batch Loss: 1.1121307611465454\n",
      "Epoch 316, Loss: 6.055228352546692, Final Batch Loss: 1.2651182413101196\n",
      "Epoch 317, Loss: 6.997374415397644, Final Batch Loss: 2.208003282546997\n",
      "Epoch 318, Loss: 6.020692706108093, Final Batch Loss: 1.2560389041900635\n",
      "Epoch 319, Loss: 6.0732808113098145, Final Batch Loss: 1.4632203578948975\n",
      "Epoch 320, Loss: 6.382853388786316, Final Batch Loss: 1.7779054641723633\n",
      "Epoch 321, Loss: 5.374512672424316, Final Batch Loss: 0.8319076299667358\n",
      "Epoch 322, Loss: 4.799147993326187, Final Batch Loss: 0.33006373047828674\n",
      "Epoch 323, Loss: 6.57127320766449, Final Batch Loss: 1.9807517528533936\n",
      "Epoch 324, Loss: 6.550221085548401, Final Batch Loss: 2.055833339691162\n",
      "Epoch 325, Loss: 6.665231108665466, Final Batch Loss: 2.2386512756347656\n",
      "Epoch 326, Loss: 5.3830485343933105, Final Batch Loss: 0.9537509679794312\n",
      "Epoch 327, Loss: 5.436786592006683, Final Batch Loss: 0.9646360278129578\n",
      "Epoch 328, Loss: 5.455132484436035, Final Batch Loss: 1.0656658411026\n",
      "Epoch 329, Loss: 6.060642719268799, Final Batch Loss: 1.6640682220458984\n",
      "Epoch 330, Loss: 5.434936285018921, Final Batch Loss: 1.0076463222503662\n",
      "Epoch 331, Loss: 5.012741148471832, Final Batch Loss: 0.5471195578575134\n",
      "Epoch 332, Loss: 5.902023911476135, Final Batch Loss: 1.4125112295150757\n",
      "Epoch 333, Loss: 6.235772252082825, Final Batch Loss: 1.6510798931121826\n",
      "Epoch 334, Loss: 4.678639501333237, Final Batch Loss: 0.17878267168998718\n",
      "Epoch 335, Loss: 5.845153331756592, Final Batch Loss: 1.3069578409194946\n",
      "Epoch 336, Loss: 6.214661955833435, Final Batch Loss: 1.6265774965286255\n",
      "Epoch 337, Loss: 4.872659206390381, Final Batch Loss: 0.3375711441040039\n",
      "Epoch 338, Loss: 4.955366283655167, Final Batch Loss: 0.48131313920021057\n",
      "Epoch 339, Loss: 6.2606494426727295, Final Batch Loss: 1.689457654953003\n",
      "Epoch 340, Loss: 6.663929224014282, Final Batch Loss: 2.1140711307525635\n",
      "Epoch 341, Loss: 5.841663122177124, Final Batch Loss: 1.3904142379760742\n",
      "Epoch 342, Loss: 5.990758776664734, Final Batch Loss: 1.437302827835083\n",
      "Epoch 343, Loss: 6.2832876443862915, Final Batch Loss: 1.7998379468917847\n",
      "Epoch 344, Loss: 5.607168436050415, Final Batch Loss: 1.2252821922302246\n",
      "Epoch 345, Loss: 6.739235162734985, Final Batch Loss: 2.3417253494262695\n",
      "Epoch 346, Loss: 5.454720497131348, Final Batch Loss: 0.9159145355224609\n",
      "Epoch 347, Loss: 6.119995474815369, Final Batch Loss: 1.54002046585083\n",
      "Epoch 348, Loss: 6.222964525222778, Final Batch Loss: 1.7064036130905151\n",
      "Epoch 349, Loss: 6.11311948299408, Final Batch Loss: 1.6556673049926758\n",
      "Epoch 350, Loss: 6.805770516395569, Final Batch Loss: 2.323671817779541\n",
      "Epoch 351, Loss: 4.725291401147842, Final Batch Loss: 0.35013797879219055\n",
      "Epoch 352, Loss: 6.385277271270752, Final Batch Loss: 1.9705314636230469\n",
      "Epoch 353, Loss: 5.32207179069519, Final Batch Loss: 0.9045934677124023\n",
      "Epoch 354, Loss: 6.971323013305664, Final Batch Loss: 2.4551942348480225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355, Loss: 5.793393015861511, Final Batch Loss: 1.3220722675323486\n",
      "Epoch 356, Loss: 5.916965126991272, Final Batch Loss: 1.1316766738891602\n",
      "Epoch 357, Loss: 7.581018328666687, Final Batch Loss: 2.9031522274017334\n",
      "Epoch 358, Loss: 6.0466402769088745, Final Batch Loss: 1.5415936708450317\n",
      "Epoch 359, Loss: 7.022238731384277, Final Batch Loss: 2.4649133682250977\n",
      "Epoch 360, Loss: 5.676608204841614, Final Batch Loss: 1.2806627750396729\n",
      "Epoch 361, Loss: 6.655315279960632, Final Batch Loss: 2.109907865524292\n",
      "Epoch 362, Loss: 6.463378548622131, Final Batch Loss: 1.8324031829833984\n",
      "Epoch 363, Loss: 5.8807350397109985, Final Batch Loss: 1.302881121635437\n",
      "Epoch 364, Loss: 5.474005103111267, Final Batch Loss: 0.867477297782898\n",
      "Epoch 365, Loss: 5.896258473396301, Final Batch Loss: 1.4634393453598022\n",
      "Epoch 366, Loss: 5.898759365081787, Final Batch Loss: 1.5066261291503906\n",
      "Epoch 367, Loss: 6.1246994733810425, Final Batch Loss: 1.7544528245925903\n",
      "Epoch 368, Loss: 4.394489474594593, Final Batch Loss: 0.08908242732286453\n",
      "Epoch 369, Loss: 6.981400728225708, Final Batch Loss: 2.625761032104492\n",
      "Epoch 370, Loss: 6.038361310958862, Final Batch Loss: 1.6840217113494873\n",
      "Epoch 371, Loss: 6.562608122825623, Final Batch Loss: 2.1987674236297607\n",
      "Epoch 372, Loss: 5.602986693382263, Final Batch Loss: 1.269561767578125\n",
      "Epoch 373, Loss: 6.0118409395217896, Final Batch Loss: 1.6322554349899292\n",
      "Epoch 374, Loss: 4.555803209543228, Final Batch Loss: 0.18858793377876282\n",
      "Epoch 375, Loss: 4.533847585320473, Final Batch Loss: 0.16287781298160553\n",
      "Epoch 376, Loss: 5.610819697380066, Final Batch Loss: 1.3354084491729736\n",
      "Epoch 377, Loss: 10.301382780075073, Final Batch Loss: 5.934109210968018\n",
      "Epoch 378, Loss: 5.195566415786743, Final Batch Loss: 0.9588668346405029\n",
      "Epoch 379, Loss: 5.595361709594727, Final Batch Loss: 1.241381287574768\n",
      "Epoch 380, Loss: 4.860686749219894, Final Batch Loss: 0.47525331377983093\n",
      "Epoch 381, Loss: 5.183945298194885, Final Batch Loss: 0.8015910387039185\n",
      "Epoch 382, Loss: 6.2573171854019165, Final Batch Loss: 1.8565372228622437\n",
      "Epoch 383, Loss: 5.653576016426086, Final Batch Loss: 1.3706417083740234\n",
      "Epoch 384, Loss: 4.527093026787043, Final Batch Loss: 0.04145381227135658\n",
      "Epoch 385, Loss: 5.1559354066848755, Final Batch Loss: 0.7391570806503296\n",
      "Epoch 386, Loss: 6.10284423828125, Final Batch Loss: 1.7169654369354248\n",
      "Epoch 387, Loss: 6.870236158370972, Final Batch Loss: 2.5023984909057617\n",
      "Epoch 388, Loss: 7.0559327602386475, Final Batch Loss: 2.6854233741760254\n",
      "Epoch 389, Loss: 4.991198480129242, Final Batch Loss: 0.7500665783882141\n",
      "Epoch 390, Loss: 5.843009829521179, Final Batch Loss: 1.6008623838424683\n",
      "Epoch 391, Loss: 5.5225523710250854, Final Batch Loss: 1.1977578401565552\n",
      "Epoch 392, Loss: 4.278121970593929, Final Batch Loss: 0.037019990384578705\n",
      "Epoch 393, Loss: 5.614691257476807, Final Batch Loss: 1.4495030641555786\n",
      "Epoch 394, Loss: 4.618878275156021, Final Batch Loss: 0.3986589014530182\n",
      "Epoch 395, Loss: 6.089742422103882, Final Batch Loss: 1.8690232038497925\n",
      "Epoch 396, Loss: 6.9229313135147095, Final Batch Loss: 2.592057704925537\n",
      "Epoch 397, Loss: 5.060083329677582, Final Batch Loss: 0.8714291453361511\n",
      "Epoch 398, Loss: 6.107402682304382, Final Batch Loss: 1.8432708978652954\n",
      "Epoch 399, Loss: 5.497851490974426, Final Batch Loss: 1.291234016418457\n",
      "Epoch 400, Loss: 4.703784286975861, Final Batch Loss: 0.5458303093910217\n",
      "Epoch 401, Loss: 5.023125946521759, Final Batch Loss: 0.8702055811882019\n",
      "Epoch 402, Loss: 7.12234354019165, Final Batch Loss: 2.8773233890533447\n",
      "Epoch 403, Loss: 7.130969166755676, Final Batch Loss: 2.8310794830322266\n",
      "Epoch 404, Loss: 5.2023080587387085, Final Batch Loss: 0.8957116603851318\n",
      "Epoch 405, Loss: 6.247103214263916, Final Batch Loss: 2.0329315662384033\n",
      "Epoch 406, Loss: 5.609659671783447, Final Batch Loss: 1.3548074960708618\n",
      "Epoch 407, Loss: 6.063997507095337, Final Batch Loss: 1.9556992053985596\n",
      "Epoch 408, Loss: 5.6780478954315186, Final Batch Loss: 1.417437195777893\n",
      "Epoch 409, Loss: 6.532516241073608, Final Batch Loss: 2.1612327098846436\n",
      "Epoch 410, Loss: 5.501686334609985, Final Batch Loss: 1.1518619060516357\n",
      "Epoch 411, Loss: 5.766969203948975, Final Batch Loss: 1.3960998058319092\n",
      "Epoch 412, Loss: 5.759433031082153, Final Batch Loss: 1.3614983558654785\n",
      "Epoch 413, Loss: 5.560657382011414, Final Batch Loss: 1.1697325706481934\n",
      "Epoch 414, Loss: 5.843257904052734, Final Batch Loss: 1.4424443244934082\n",
      "Epoch 415, Loss: 5.213123261928558, Final Batch Loss: 0.9895169138908386\n",
      "Epoch 416, Loss: 5.574915409088135, Final Batch Loss: 1.4095661640167236\n",
      "Epoch 417, Loss: 5.609521269798279, Final Batch Loss: 1.4150364398956299\n",
      "Epoch 418, Loss: 6.529762268066406, Final Batch Loss: 2.1167633533477783\n",
      "Epoch 419, Loss: 6.33874785900116, Final Batch Loss: 2.1143126487731934\n",
      "Epoch 420, Loss: 6.678666949272156, Final Batch Loss: 2.393070936203003\n",
      "Epoch 421, Loss: 4.836918830871582, Final Batch Loss: 0.6366609334945679\n",
      "Epoch 422, Loss: 5.610462546348572, Final Batch Loss: 1.3847020864486694\n",
      "Epoch 423, Loss: 5.756935358047485, Final Batch Loss: 1.5124870538711548\n",
      "Epoch 424, Loss: 4.359192907810211, Final Batch Loss: 0.22321993112564087\n",
      "Epoch 425, Loss: 5.991646409034729, Final Batch Loss: 1.8066811561584473\n",
      "Epoch 426, Loss: 4.946621477603912, Final Batch Loss: 0.6796002984046936\n",
      "Epoch 427, Loss: 6.147857427597046, Final Batch Loss: 1.900335431098938\n",
      "Epoch 428, Loss: 4.543018013238907, Final Batch Loss: 0.1972462236881256\n",
      "Epoch 429, Loss: 6.242892026901245, Final Batch Loss: 1.9396597146987915\n",
      "Epoch 430, Loss: 6.884586334228516, Final Batch Loss: 2.585146188735962\n",
      "Epoch 431, Loss: 5.5734076499938965, Final Batch Loss: 1.3947304487228394\n",
      "Epoch 432, Loss: 4.965457379817963, Final Batch Loss: 0.8035514950752258\n",
      "Epoch 433, Loss: 4.486805289983749, Final Batch Loss: 0.2564125955104828\n",
      "Epoch 434, Loss: 5.680146813392639, Final Batch Loss: 1.4158878326416016\n",
      "Epoch 435, Loss: 6.4396408796310425, Final Batch Loss: 2.2746176719665527\n",
      "Epoch 436, Loss: 4.594476997852325, Final Batch Loss: 0.4527927041053772\n",
      "Epoch 437, Loss: 4.277687571942806, Final Batch Loss: 0.07834317535161972\n",
      "Epoch 438, Loss: 4.46800234913826, Final Batch Loss: 0.35378673672676086\n",
      "Epoch 439, Loss: 4.290512025356293, Final Batch Loss: 0.18310290575027466\n",
      "Epoch 440, Loss: 5.461798906326294, Final Batch Loss: 1.341605305671692\n",
      "Epoch 441, Loss: 5.776450514793396, Final Batch Loss: 1.6405538320541382\n",
      "Epoch 442, Loss: 5.310498833656311, Final Batch Loss: 1.0803745985031128\n",
      "Epoch 443, Loss: 6.178740501403809, Final Batch Loss: 2.1543993949890137\n",
      "Epoch 444, Loss: 5.7488943338394165, Final Batch Loss: 1.7099487781524658\n",
      "Epoch 445, Loss: 4.974068820476532, Final Batch Loss: 0.9065396189689636\n",
      "Epoch 446, Loss: 6.05817174911499, Final Batch Loss: 1.9703834056854248\n",
      "Epoch 447, Loss: 4.139470800757408, Final Batch Loss: 0.029311761260032654\n",
      "Epoch 448, Loss: 6.428285360336304, Final Batch Loss: 2.3813533782958984\n",
      "Epoch 449, Loss: 5.130282640457153, Final Batch Loss: 1.0358424186706543\n",
      "Epoch 450, Loss: 5.510462522506714, Final Batch Loss: 1.3518445491790771\n",
      "Epoch 451, Loss: 4.236399106681347, Final Batch Loss: 0.08898919075727463\n",
      "Epoch 452, Loss: 8.683878183364868, Final Batch Loss: 4.554183483123779\n",
      "Epoch 453, Loss: 6.772763609886169, Final Batch Loss: 2.642876386642456\n",
      "Epoch 454, Loss: 5.448635578155518, Final Batch Loss: 1.361986517906189\n",
      "Epoch 455, Loss: 5.288514971733093, Final Batch Loss: 1.2013967037200928\n",
      "Epoch 456, Loss: 5.52957808971405, Final Batch Loss: 1.4335352182388306\n",
      "Epoch 457, Loss: 7.21327531337738, Final Batch Loss: 3.1358208656311035\n",
      "Epoch 458, Loss: 5.383155465126038, Final Batch Loss: 1.2342588901519775\n",
      "Epoch 459, Loss: 6.090282320976257, Final Batch Loss: 1.9388625621795654\n",
      "Epoch 460, Loss: 4.617617249488831, Final Batch Loss: 0.5222537517547607\n",
      "Epoch 461, Loss: 4.2421704307198524, Final Batch Loss: 0.06566033512353897\n",
      "Epoch 462, Loss: 5.277407646179199, Final Batch Loss: 1.1759995222091675\n",
      "Epoch 463, Loss: 5.433512330055237, Final Batch Loss: 1.3546608686447144\n",
      "Epoch 464, Loss: 6.053984045982361, Final Batch Loss: 1.8932201862335205\n",
      "Epoch 465, Loss: 4.467223554849625, Final Batch Loss: 0.39500680565834045\n",
      "Epoch 466, Loss: 5.651466965675354, Final Batch Loss: 1.5690760612487793\n",
      "Epoch 467, Loss: 5.519662618637085, Final Batch Loss: 1.3778623342514038\n",
      "Epoch 468, Loss: 5.314155340194702, Final Batch Loss: 1.129612922668457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 469, Loss: 5.099093377590179, Final Batch Loss: 0.9346840977668762\n",
      "Epoch 470, Loss: 5.967965364456177, Final Batch Loss: 1.8493486642837524\n",
      "Epoch 471, Loss: 4.9720951318740845, Final Batch Loss: 0.8654394149780273\n",
      "Epoch 472, Loss: 5.587938189506531, Final Batch Loss: 1.554125189781189\n",
      "Epoch 473, Loss: 6.188748598098755, Final Batch Loss: 2.068449020385742\n",
      "Epoch 474, Loss: 5.363922595977783, Final Batch Loss: 1.3762527704238892\n",
      "Epoch 475, Loss: 5.0501062870025635, Final Batch Loss: 1.010066270828247\n",
      "Epoch 476, Loss: 5.45414924621582, Final Batch Loss: 1.4579862356185913\n",
      "Epoch 477, Loss: 5.578767657279968, Final Batch Loss: 1.5176451206207275\n",
      "Epoch 478, Loss: 5.458387017250061, Final Batch Loss: 1.4896526336669922\n",
      "Epoch 479, Loss: 5.634007930755615, Final Batch Loss: 1.5802116394042969\n",
      "Epoch 480, Loss: 6.42603075504303, Final Batch Loss: 2.43857741355896\n",
      "Epoch 481, Loss: 4.139693811535835, Final Batch Loss: 0.07877428829669952\n",
      "Epoch 482, Loss: 4.314639985561371, Final Batch Loss: 0.2594913840293884\n",
      "Epoch 483, Loss: 5.146456956863403, Final Batch Loss: 1.0467389822006226\n",
      "Epoch 484, Loss: 7.744811773300171, Final Batch Loss: 3.6022491455078125\n",
      "Epoch 485, Loss: 5.246908903121948, Final Batch Loss: 1.1986831426620483\n",
      "Epoch 486, Loss: 5.361057877540588, Final Batch Loss: 1.323481798171997\n",
      "Epoch 487, Loss: 5.675269722938538, Final Batch Loss: 1.546893835067749\n",
      "Epoch 488, Loss: 4.143084743991494, Final Batch Loss: 0.02280590869486332\n",
      "Epoch 489, Loss: 4.386956989765167, Final Batch Loss: 0.2685942053794861\n",
      "Epoch 490, Loss: 5.846017837524414, Final Batch Loss: 1.7294433116912842\n",
      "Epoch 491, Loss: 5.615786194801331, Final Batch Loss: 1.4787847995758057\n",
      "Epoch 492, Loss: 5.2183297872543335, Final Batch Loss: 1.2129688262939453\n",
      "Epoch 493, Loss: 5.99859356880188, Final Batch Loss: 1.9213037490844727\n",
      "Epoch 494, Loss: 4.518809884786606, Final Batch Loss: 0.4359411895275116\n",
      "Epoch 495, Loss: 4.400007665157318, Final Batch Loss: 0.36001259088516235\n",
      "Epoch 496, Loss: 4.635214149951935, Final Batch Loss: 0.7200164198875427\n",
      "Epoch 497, Loss: 6.660866975784302, Final Batch Loss: 2.729762315750122\n",
      "Epoch 498, Loss: 5.412476897239685, Final Batch Loss: 1.488115906715393\n",
      "Epoch 499, Loss: 4.64197564125061, Final Batch Loss: 0.6502412557601929\n",
      "Epoch 500, Loss: 5.8367133140563965, Final Batch Loss: 1.8603070974349976\n",
      "Epoch 501, Loss: 7.118499040603638, Final Batch Loss: 3.0643534660339355\n",
      "Epoch 502, Loss: 6.352205157279968, Final Batch Loss: 2.2173078060150146\n",
      "Epoch 503, Loss: 4.97263503074646, Final Batch Loss: 0.7618824243545532\n",
      "Epoch 504, Loss: 5.475627899169922, Final Batch Loss: 1.2464805841445923\n",
      "Epoch 505, Loss: 5.749277472496033, Final Batch Loss: 1.479292392730713\n",
      "Epoch 506, Loss: 4.80375811457634, Final Batch Loss: 0.4636940658092499\n",
      "Epoch 507, Loss: 5.145050287246704, Final Batch Loss: 0.9649252891540527\n",
      "Epoch 508, Loss: 6.083950757980347, Final Batch Loss: 1.937172770500183\n",
      "Epoch 509, Loss: 6.971412658691406, Final Batch Loss: 2.972805976867676\n",
      "Epoch 510, Loss: 6.263034820556641, Final Batch Loss: 2.309985637664795\n",
      "Epoch 511, Loss: 4.461259365081787, Final Batch Loss: 0.4830862283706665\n",
      "Epoch 512, Loss: 5.4119508266448975, Final Batch Loss: 1.2357239723205566\n",
      "Epoch 513, Loss: 5.645247578620911, Final Batch Loss: 1.3795465230941772\n",
      "Epoch 514, Loss: 6.007954478263855, Final Batch Loss: 1.6938178539276123\n",
      "Epoch 515, Loss: 5.797488570213318, Final Batch Loss: 1.6446807384490967\n",
      "Epoch 516, Loss: 5.84922981262207, Final Batch Loss: 1.7486422061920166\n",
      "Epoch 517, Loss: 5.366719007492065, Final Batch Loss: 1.3520244359970093\n",
      "Epoch 518, Loss: 6.751213073730469, Final Batch Loss: 2.711988925933838\n",
      "Epoch 519, Loss: 4.178451672196388, Final Batch Loss: 0.05940474569797516\n",
      "Epoch 520, Loss: 5.784538149833679, Final Batch Loss: 1.7064783573150635\n",
      "Epoch 521, Loss: 4.895122468471527, Final Batch Loss: 0.7986943125724792\n",
      "Epoch 522, Loss: 5.501444578170776, Final Batch Loss: 1.3487516641616821\n",
      "Epoch 523, Loss: 4.577351093292236, Final Batch Loss: 0.5582972764968872\n",
      "Epoch 524, Loss: 4.6054189801216125, Final Batch Loss: 0.6636833548545837\n",
      "Epoch 525, Loss: 5.760512351989746, Final Batch Loss: 1.8264825344085693\n",
      "Epoch 526, Loss: 5.142167091369629, Final Batch Loss: 1.2323464155197144\n",
      "Epoch 527, Loss: 4.094732783734798, Final Batch Loss: 0.07848050445318222\n",
      "Epoch 528, Loss: 5.027613878250122, Final Batch Loss: 1.0889170169830322\n",
      "Epoch 529, Loss: 4.264148950576782, Final Batch Loss: 0.37950217723846436\n",
      "Epoch 530, Loss: 7.315394759178162, Final Batch Loss: 3.45182466506958\n",
      "Epoch 531, Loss: 5.791514754295349, Final Batch Loss: 1.8817352056503296\n",
      "Epoch 532, Loss: 5.137478232383728, Final Batch Loss: 1.2131774425506592\n",
      "Epoch 533, Loss: 4.77406907081604, Final Batch Loss: 1.003889560699463\n",
      "Epoch 534, Loss: 5.0122681856155396, Final Batch Loss: 1.1710760593414307\n",
      "Epoch 535, Loss: 5.391474723815918, Final Batch Loss: 1.4939165115356445\n",
      "Epoch 536, Loss: 7.075700163841248, Final Batch Loss: 3.0535531044006348\n",
      "Epoch 537, Loss: 3.9993240870535374, Final Batch Loss: 0.062047723680734634\n",
      "Epoch 538, Loss: 4.725928544998169, Final Batch Loss: 0.7852898836135864\n",
      "Epoch 539, Loss: 4.298729687929153, Final Batch Loss: 0.3392855226993561\n",
      "Epoch 540, Loss: 6.411154270172119, Final Batch Loss: 2.451813220977783\n",
      "Epoch 541, Loss: 6.009293079376221, Final Batch Loss: 2.0313432216644287\n",
      "Epoch 542, Loss: 6.138087034225464, Final Batch Loss: 2.1329712867736816\n",
      "Epoch 543, Loss: 4.449344098567963, Final Batch Loss: 0.5610565543174744\n",
      "Epoch 544, Loss: 4.04793182015419, Final Batch Loss: 0.16161897778511047\n",
      "Epoch 545, Loss: 4.190415531396866, Final Batch Loss: 0.23631873726844788\n",
      "Epoch 546, Loss: 4.9696571826934814, Final Batch Loss: 1.0176349878311157\n",
      "Epoch 547, Loss: 6.353055715560913, Final Batch Loss: 2.4238078594207764\n",
      "Epoch 548, Loss: 4.807713866233826, Final Batch Loss: 0.89073646068573\n",
      "Epoch 549, Loss: 5.27324914932251, Final Batch Loss: 1.4109385013580322\n",
      "Epoch 550, Loss: 5.055367410182953, Final Batch Loss: 0.9896462559700012\n",
      "Epoch 551, Loss: 4.892126262187958, Final Batch Loss: 0.8274573683738708\n",
      "Epoch 552, Loss: 6.030983328819275, Final Batch Loss: 1.9520094394683838\n",
      "Epoch 553, Loss: 5.584543585777283, Final Batch Loss: 1.6053029298782349\n",
      "Epoch 554, Loss: 4.070288851857185, Final Batch Loss: 0.23436065018177032\n",
      "Epoch 555, Loss: 5.411477446556091, Final Batch Loss: 1.5164426565170288\n",
      "Epoch 556, Loss: 5.60019588470459, Final Batch Loss: 1.6272813081741333\n",
      "Epoch 557, Loss: 4.431810200214386, Final Batch Loss: 0.5494301915168762\n",
      "Epoch 558, Loss: 4.73799741268158, Final Batch Loss: 0.9138848781585693\n",
      "Epoch 559, Loss: 4.403353363275528, Final Batch Loss: 0.4632551968097687\n",
      "Epoch 560, Loss: 4.861817479133606, Final Batch Loss: 0.9959009885787964\n",
      "Epoch 561, Loss: 6.0004963874816895, Final Batch Loss: 2.0675768852233887\n",
      "Epoch 562, Loss: 5.0406516790390015, Final Batch Loss: 1.1961674690246582\n",
      "Epoch 563, Loss: 5.594326972961426, Final Batch Loss: 1.7551589012145996\n",
      "Epoch 564, Loss: 5.164253234863281, Final Batch Loss: 1.4238018989562988\n",
      "Epoch 565, Loss: 5.704684257507324, Final Batch Loss: 1.959254503250122\n",
      "Epoch 566, Loss: 5.812700510025024, Final Batch Loss: 2.047060012817383\n",
      "Epoch 567, Loss: 5.624392986297607, Final Batch Loss: 1.691853404045105\n",
      "Epoch 568, Loss: 4.45825856924057, Final Batch Loss: 0.5396671891212463\n",
      "Epoch 569, Loss: 5.369627952575684, Final Batch Loss: 1.4832260608673096\n",
      "Epoch 570, Loss: 4.810299873352051, Final Batch Loss: 0.9366483688354492\n",
      "Epoch 571, Loss: 5.724310278892517, Final Batch Loss: 1.8350117206573486\n",
      "Epoch 572, Loss: 4.133429601788521, Final Batch Loss: 0.2421407252550125\n",
      "Epoch 573, Loss: 5.881433129310608, Final Batch Loss: 2.0748705863952637\n",
      "Epoch 574, Loss: 4.21922442317009, Final Batch Loss: 0.34867164492607117\n",
      "Epoch 575, Loss: 4.584961831569672, Final Batch Loss: 0.7955171465873718\n",
      "Epoch 576, Loss: 4.75605446100235, Final Batch Loss: 0.9519317746162415\n",
      "Epoch 577, Loss: 5.176331520080566, Final Batch Loss: 1.3537120819091797\n",
      "Epoch 578, Loss: 6.080376148223877, Final Batch Loss: 2.242159605026245\n",
      "Epoch 579, Loss: 4.384641170501709, Final Batch Loss: 0.5201035737991333\n",
      "Epoch 580, Loss: 5.613212585449219, Final Batch Loss: 1.7062957286834717\n",
      "Epoch 581, Loss: 4.4446896612644196, Final Batch Loss: 0.3545810282230377\n",
      "Epoch 582, Loss: 4.644729673862457, Final Batch Loss: 0.5844528079032898\n",
      "Epoch 583, Loss: 4.78452342748642, Final Batch Loss: 0.891684353351593\n",
      "Epoch 584, Loss: 5.529719829559326, Final Batch Loss: 1.6337358951568604\n",
      "Epoch 585, Loss: 5.109977126121521, Final Batch Loss: 1.415215015411377\n",
      "Epoch 586, Loss: 5.1304086446762085, Final Batch Loss: 1.352316975593567\n",
      "Epoch 587, Loss: 4.747217893600464, Final Batch Loss: 1.0579720735549927\n",
      "Epoch 588, Loss: 4.850578784942627, Final Batch Loss: 1.1112571954727173\n",
      "Epoch 589, Loss: 4.450380086898804, Final Batch Loss: 0.6744908094406128\n",
      "Epoch 590, Loss: 5.606990456581116, Final Batch Loss: 1.8184040784835815\n",
      "Epoch 591, Loss: 5.166241407394409, Final Batch Loss: 1.4188212156295776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 592, Loss: 4.605070114135742, Final Batch Loss: 0.9485471248626709\n",
      "Epoch 593, Loss: 4.766549587249756, Final Batch Loss: 1.0873262882232666\n",
      "Epoch 594, Loss: 4.4308802485466, Final Batch Loss: 0.7713876366615295\n",
      "Epoch 595, Loss: 3.834444597363472, Final Batch Loss: 0.21936042606830597\n",
      "Epoch 596, Loss: 4.6573121547698975, Final Batch Loss: 0.9017683267593384\n",
      "Epoch 597, Loss: 4.6136462688446045, Final Batch Loss: 0.9347959756851196\n",
      "Epoch 598, Loss: 4.718655109405518, Final Batch Loss: 0.8732662200927734\n",
      "Epoch 599, Loss: 5.0026938915252686, Final Batch Loss: 1.1771502494812012\n",
      "Epoch 600, Loss: 4.568662345409393, Final Batch Loss: 0.8942102789878845\n",
      "Epoch 601, Loss: 3.735297417268157, Final Batch Loss: 0.01918814145028591\n",
      "Epoch 602, Loss: 4.418348848819733, Final Batch Loss: 0.7085759043693542\n",
      "Epoch 603, Loss: 5.081886887550354, Final Batch Loss: 1.420588731765747\n",
      "Epoch 604, Loss: 4.682586431503296, Final Batch Loss: 0.9845906496047974\n",
      "Epoch 605, Loss: 4.8867104053497314, Final Batch Loss: 1.1707556247711182\n",
      "Epoch 606, Loss: 4.540268242359161, Final Batch Loss: 0.9138484597206116\n",
      "Epoch 607, Loss: 4.55580335855484, Final Batch Loss: 0.9791874289512634\n",
      "Epoch 608, Loss: 5.136337518692017, Final Batch Loss: 1.470720887184143\n",
      "Epoch 609, Loss: 5.627607583999634, Final Batch Loss: 1.975356936454773\n",
      "Epoch 610, Loss: 5.076728582382202, Final Batch Loss: 1.4494714736938477\n",
      "Epoch 611, Loss: 5.050422668457031, Final Batch Loss: 1.4506428241729736\n",
      "Epoch 612, Loss: 6.137832164764404, Final Batch Loss: 2.4004712104797363\n",
      "Epoch 613, Loss: 5.9931321144104, Final Batch Loss: 2.2419705390930176\n",
      "Epoch 614, Loss: 5.61021363735199, Final Batch Loss: 1.8764806985855103\n",
      "Epoch 615, Loss: 5.542959809303284, Final Batch Loss: 1.7411305904388428\n",
      "Epoch 616, Loss: 5.6094053983688354, Final Batch Loss: 1.753568172454834\n",
      "Epoch 617, Loss: 4.522377252578735, Final Batch Loss: 0.6031222343444824\n",
      "Epoch 618, Loss: 5.830557107925415, Final Batch Loss: 2.02492094039917\n",
      "Epoch 619, Loss: 3.9282028675079346, Final Batch Loss: 0.21016979217529297\n",
      "Epoch 620, Loss: 4.003347530961037, Final Batch Loss: 0.13274015486240387\n",
      "Epoch 621, Loss: 5.355719089508057, Final Batch Loss: 1.393328309059143\n",
      "Epoch 622, Loss: 4.976754426956177, Final Batch Loss: 0.9879730939865112\n",
      "Epoch 623, Loss: 4.235720723867416, Final Batch Loss: 0.3288376033306122\n",
      "Epoch 624, Loss: 4.886328220367432, Final Batch Loss: 1.0502334833145142\n",
      "Epoch 625, Loss: 4.885960221290588, Final Batch Loss: 1.1228504180908203\n",
      "Epoch 626, Loss: 5.916390776634216, Final Batch Loss: 2.0827465057373047\n",
      "Epoch 627, Loss: 5.489550232887268, Final Batch Loss: 1.5221182107925415\n",
      "Epoch 628, Loss: 7.167784810066223, Final Batch Loss: 3.1962099075317383\n",
      "Epoch 629, Loss: 5.898955464363098, Final Batch Loss: 2.06573748588562\n",
      "Epoch 630, Loss: 3.9602472335100174, Final Batch Loss: 0.22642262279987335\n",
      "Epoch 631, Loss: 5.869783401489258, Final Batch Loss: 2.121553659439087\n",
      "Epoch 632, Loss: 5.4651939868927, Final Batch Loss: 1.7789419889450073\n",
      "Epoch 633, Loss: 4.969147324562073, Final Batch Loss: 1.2591854333877563\n",
      "Epoch 634, Loss: 7.519561052322388, Final Batch Loss: 3.782377243041992\n",
      "Epoch 635, Loss: 4.424255013465881, Final Batch Loss: 0.7834817171096802\n",
      "Epoch 636, Loss: 4.478618800640106, Final Batch Loss: 0.8415859341621399\n",
      "Epoch 637, Loss: 5.397021412849426, Final Batch Loss: 1.7221335172653198\n",
      "Epoch 638, Loss: 3.705604220740497, Final Batch Loss: 0.008732701651751995\n",
      "Epoch 639, Loss: 3.720426984131336, Final Batch Loss: 0.06011433154344559\n",
      "Epoch 640, Loss: 5.77499532699585, Final Batch Loss: 2.1406922340393066\n",
      "Epoch 641, Loss: 4.91398298740387, Final Batch Loss: 1.3579494953155518\n",
      "Epoch 642, Loss: 5.262857913970947, Final Batch Loss: 1.6733083724975586\n",
      "Epoch 643, Loss: 5.278387784957886, Final Batch Loss: 1.6389366388320923\n",
      "Epoch 644, Loss: 6.158374071121216, Final Batch Loss: 2.602384328842163\n",
      "Epoch 645, Loss: 4.820428490638733, Final Batch Loss: 1.197889804840088\n",
      "Epoch 646, Loss: 5.507250189781189, Final Batch Loss: 1.863584280014038\n",
      "Epoch 647, Loss: 4.059504389762878, Final Batch Loss: 0.4479883909225464\n",
      "Epoch 648, Loss: 3.731632150709629, Final Batch Loss: 0.09660116583108902\n",
      "Epoch 649, Loss: 5.3345417976379395, Final Batch Loss: 1.7870336771011353\n",
      "Epoch 650, Loss: 5.0729875564575195, Final Batch Loss: 1.544094443321228\n",
      "Epoch 651, Loss: 4.615358352661133, Final Batch Loss: 1.04275381565094\n",
      "Epoch 652, Loss: 4.811184287071228, Final Batch Loss: 1.3029131889343262\n",
      "Epoch 653, Loss: 4.4723857045173645, Final Batch Loss: 0.8931789994239807\n",
      "Epoch 654, Loss: 5.101588129997253, Final Batch Loss: 1.4107146263122559\n",
      "Epoch 655, Loss: 4.914961576461792, Final Batch Loss: 1.3095402717590332\n",
      "Epoch 656, Loss: 4.347693383693695, Final Batch Loss: 0.7398123145103455\n",
      "Epoch 657, Loss: 3.8233737125992775, Final Batch Loss: 0.09258309751749039\n",
      "Epoch 658, Loss: 5.361373424530029, Final Batch Loss: 1.7605700492858887\n",
      "Epoch 659, Loss: 4.31143057346344, Final Batch Loss: 0.6653103828430176\n",
      "Epoch 660, Loss: 4.0611196756362915, Final Batch Loss: 0.57612144947052\n",
      "Epoch 661, Loss: 4.879417061805725, Final Batch Loss: 1.2607390880584717\n",
      "Epoch 662, Loss: 6.279054284095764, Final Batch Loss: 2.683986186981201\n",
      "Epoch 663, Loss: 4.299847483634949, Final Batch Loss: 0.8251792192459106\n",
      "Epoch 664, Loss: 5.681065678596497, Final Batch Loss: 2.2171995639801025\n",
      "Epoch 665, Loss: 5.105006814002991, Final Batch Loss: 1.6414544582366943\n",
      "Epoch 666, Loss: 4.513373851776123, Final Batch Loss: 0.9494726657867432\n",
      "Epoch 667, Loss: 4.7614099979400635, Final Batch Loss: 1.2432382106781006\n",
      "Epoch 668, Loss: 4.652995228767395, Final Batch Loss: 1.1208128929138184\n",
      "Epoch 669, Loss: 4.941963791847229, Final Batch Loss: 1.4979099035263062\n",
      "Epoch 670, Loss: 4.147252857685089, Final Batch Loss: 0.6418506503105164\n",
      "Epoch 671, Loss: 4.187537252902985, Final Batch Loss: 0.7894042134284973\n",
      "Epoch 672, Loss: 3.768473654985428, Final Batch Loss: 0.18176940083503723\n",
      "Epoch 673, Loss: 3.6621804386377335, Final Batch Loss: 0.03647567331790924\n",
      "Epoch 674, Loss: 4.330991148948669, Final Batch Loss: 0.7067973613739014\n",
      "Epoch 675, Loss: 5.302079081535339, Final Batch Loss: 1.7216967344284058\n",
      "Epoch 676, Loss: 4.686430931091309, Final Batch Loss: 1.1892791986465454\n",
      "Epoch 677, Loss: 3.703536607325077, Final Batch Loss: 0.023296691477298737\n",
      "Epoch 678, Loss: 4.976163148880005, Final Batch Loss: 1.0454027652740479\n",
      "Epoch 679, Loss: 4.6406301856040955, Final Batch Loss: 0.9571972489356995\n",
      "Epoch 680, Loss: 5.16510796546936, Final Batch Loss: 1.522905707359314\n",
      "Epoch 681, Loss: 5.676536202430725, Final Batch Loss: 2.152078628540039\n",
      "Epoch 682, Loss: 4.057467743754387, Final Batch Loss: 0.1889200657606125\n",
      "Epoch 683, Loss: 4.958194017410278, Final Batch Loss: 1.1925286054611206\n",
      "Epoch 684, Loss: 4.341261029243469, Final Batch Loss: 0.7541053295135498\n",
      "Epoch 685, Loss: 3.7277178317308426, Final Batch Loss: 0.16433079540729523\n",
      "Epoch 686, Loss: 5.932891845703125, Final Batch Loss: 2.459618330001831\n",
      "Epoch 687, Loss: 4.323622643947601, Final Batch Loss: 0.7334160208702087\n",
      "Epoch 688, Loss: 4.446082711219788, Final Batch Loss: 0.9174627065658569\n",
      "Epoch 689, Loss: 4.352509319782257, Final Batch Loss: 0.6715585589408875\n",
      "Epoch 690, Loss: 5.311424016952515, Final Batch Loss: 1.7260205745697021\n",
      "Epoch 691, Loss: 6.097469925880432, Final Batch Loss: 2.6856539249420166\n",
      "Epoch 692, Loss: 4.4359365701675415, Final Batch Loss: 1.101758360862732\n",
      "Epoch 693, Loss: 4.900833368301392, Final Batch Loss: 1.4417591094970703\n",
      "Epoch 694, Loss: 4.020327866077423, Final Batch Loss: 0.5837814211845398\n",
      "Epoch 695, Loss: 4.646215319633484, Final Batch Loss: 1.2214983701705933\n",
      "Epoch 696, Loss: 4.775105595588684, Final Batch Loss: 1.4093409776687622\n",
      "Epoch 697, Loss: 5.131253242492676, Final Batch Loss: 1.7067434787750244\n",
      "Epoch 698, Loss: 5.131980299949646, Final Batch Loss: 1.7116843461990356\n",
      "Epoch 699, Loss: 4.810326457023621, Final Batch Loss: 1.3723652362823486\n",
      "Epoch 700, Loss: 3.4299611262977123, Final Batch Loss: 0.021339576691389084\n",
      "Epoch 701, Loss: 3.9153977632522583, Final Batch Loss: 0.3731722831726074\n",
      "Epoch 702, Loss: 3.4869238045066595, Final Batch Loss: 0.014605274423956871\n",
      "Epoch 703, Loss: 6.907002091407776, Final Batch Loss: 3.400650978088379\n",
      "Epoch 704, Loss: 4.8543219566345215, Final Batch Loss: 1.4870222806930542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 705, Loss: 3.531790904700756, Final Batch Loss: 0.0809822604060173\n",
      "Epoch 706, Loss: 5.628765344619751, Final Batch Loss: 2.139904499053955\n",
      "Epoch 707, Loss: 3.8921831399202347, Final Batch Loss: 0.1774166375398636\n",
      "Epoch 708, Loss: 4.862535834312439, Final Batch Loss: 1.1841185092926025\n",
      "Epoch 709, Loss: 4.048898607492447, Final Batch Loss: 0.49970999360084534\n",
      "Epoch 710, Loss: 4.583760499954224, Final Batch Loss: 1.149860143661499\n",
      "Epoch 711, Loss: 5.389185428619385, Final Batch Loss: 1.9626456499099731\n",
      "Epoch 712, Loss: 5.101428866386414, Final Batch Loss: 1.6830456256866455\n",
      "Epoch 713, Loss: 3.4675175324082375, Final Batch Loss: 0.051747240126132965\n",
      "Epoch 714, Loss: 3.5226705372333527, Final Batch Loss: 0.20268800854682922\n",
      "Epoch 715, Loss: 4.80199933052063, Final Batch Loss: 1.423640251159668\n",
      "Epoch 716, Loss: 3.5981819480657578, Final Batch Loss: 0.20975099503993988\n",
      "Epoch 717, Loss: 4.100739896297455, Final Batch Loss: 0.7373926043510437\n",
      "Epoch 718, Loss: 5.174638271331787, Final Batch Loss: 1.9505517482757568\n",
      "Epoch 719, Loss: 4.911559104919434, Final Batch Loss: 1.4646174907684326\n",
      "Epoch 720, Loss: 4.406604290008545, Final Batch Loss: 1.0618048906326294\n",
      "Epoch 721, Loss: 4.346649765968323, Final Batch Loss: 0.9559701681137085\n",
      "Epoch 722, Loss: 4.159000873565674, Final Batch Loss: 0.7967267036437988\n",
      "Epoch 723, Loss: 3.4490934759378433, Final Batch Loss: 0.21995462477207184\n",
      "Epoch 724, Loss: 4.49639368057251, Final Batch Loss: 1.1448967456817627\n",
      "Epoch 725, Loss: 4.424922823905945, Final Batch Loss: 1.0305677652359009\n",
      "Epoch 726, Loss: 4.013413846492767, Final Batch Loss: 0.7542749047279358\n",
      "Epoch 727, Loss: 3.575385093688965, Final Batch Loss: 0.13216710090637207\n",
      "Epoch 728, Loss: 4.643063068389893, Final Batch Loss: 1.182739496231079\n",
      "Epoch 729, Loss: 5.001416444778442, Final Batch Loss: 1.59342360496521\n",
      "Epoch 730, Loss: 4.098016738891602, Final Batch Loss: 0.6322829723358154\n",
      "Epoch 731, Loss: 4.3266714215278625, Final Batch Loss: 0.8400949835777283\n",
      "Epoch 732, Loss: 3.7568623572587967, Final Batch Loss: 0.08502466976642609\n",
      "Epoch 733, Loss: 3.598547674715519, Final Batch Loss: 0.010512925684452057\n",
      "Epoch 734, Loss: 5.171599626541138, Final Batch Loss: 1.5312069654464722\n",
      "Epoch 735, Loss: 4.8375197649002075, Final Batch Loss: 1.4402021169662476\n",
      "Epoch 736, Loss: 3.8248968720436096, Final Batch Loss: 0.4201285243034363\n",
      "Epoch 737, Loss: 4.918400526046753, Final Batch Loss: 1.4447495937347412\n",
      "Epoch 738, Loss: 3.8479273915290833, Final Batch Loss: 0.3458065390586853\n",
      "Epoch 739, Loss: 5.050822138786316, Final Batch Loss: 1.5567975044250488\n",
      "Epoch 740, Loss: 3.787743717432022, Final Batch Loss: 0.3408905565738678\n",
      "Epoch 741, Loss: 5.132668375968933, Final Batch Loss: 1.7455828189849854\n",
      "Epoch 742, Loss: 3.37952515296638, Final Batch Loss: 0.029300300404429436\n",
      "Epoch 743, Loss: 5.952349305152893, Final Batch Loss: 2.723912239074707\n",
      "Epoch 744, Loss: 4.7723153829574585, Final Batch Loss: 1.5355669260025024\n",
      "Epoch 745, Loss: 4.964479923248291, Final Batch Loss: 1.6711876392364502\n",
      "Epoch 746, Loss: 6.198609113693237, Final Batch Loss: 2.8400793075561523\n",
      "Epoch 747, Loss: 3.3387086167931557, Final Batch Loss: 0.08947024494409561\n",
      "Epoch 748, Loss: 3.8923588395118713, Final Batch Loss: 0.5359479784965515\n",
      "Epoch 749, Loss: 3.8712560534477234, Final Batch Loss: 0.5226781964302063\n",
      "Epoch 750, Loss: 4.345608115196228, Final Batch Loss: 1.0823924541473389\n",
      "Epoch 751, Loss: 3.5799348950386047, Final Batch Loss: 0.130251944065094\n",
      "Epoch 752, Loss: 4.374035954475403, Final Batch Loss: 1.0159649848937988\n",
      "Epoch 753, Loss: 4.508717060089111, Final Batch Loss: 1.1696813106536865\n",
      "Epoch 754, Loss: 3.278513252735138, Final Batch Loss: 0.006350338459014893\n",
      "Epoch 755, Loss: 7.0352795124053955, Final Batch Loss: 3.7234063148498535\n",
      "Epoch 756, Loss: 4.633895516395569, Final Batch Loss: 1.2659220695495605\n",
      "Epoch 757, Loss: 4.243044972419739, Final Batch Loss: 0.8682043552398682\n",
      "Epoch 758, Loss: 5.350384950637817, Final Batch Loss: 2.0200226306915283\n",
      "Epoch 759, Loss: 3.4834719002246857, Final Batch Loss: 0.18106582760810852\n",
      "Epoch 760, Loss: 3.9268493056297302, Final Batch Loss: 0.5972611308097839\n",
      "Epoch 761, Loss: 4.893436312675476, Final Batch Loss: 1.4995598793029785\n",
      "Epoch 762, Loss: 4.014621317386627, Final Batch Loss: 0.7134475111961365\n",
      "Epoch 763, Loss: 4.208491325378418, Final Batch Loss: 0.8548561334609985\n",
      "Epoch 764, Loss: 4.99469518661499, Final Batch Loss: 1.6187430620193481\n",
      "Epoch 765, Loss: 4.016551733016968, Final Batch Loss: 0.803095281124115\n",
      "Epoch 766, Loss: 3.573529064655304, Final Batch Loss: 0.2313569188117981\n",
      "Epoch 767, Loss: 3.8072004914283752, Final Batch Loss: 0.5097900032997131\n",
      "Epoch 768, Loss: 3.9031258821487427, Final Batch Loss: 0.6192883253097534\n",
      "Epoch 769, Loss: 5.6289873123168945, Final Batch Loss: 2.310326099395752\n",
      "Epoch 770, Loss: 5.114390015602112, Final Batch Loss: 1.8810441493988037\n",
      "Epoch 771, Loss: 4.800691485404968, Final Batch Loss: 1.4245399236679077\n",
      "Epoch 772, Loss: 4.493787407875061, Final Batch Loss: 1.0505841970443726\n",
      "Epoch 773, Loss: 3.5312540978193283, Final Batch Loss: 0.18713252246379852\n",
      "Epoch 774, Loss: 3.833309292793274, Final Batch Loss: 0.5716264247894287\n",
      "Epoch 775, Loss: 3.5923316180706024, Final Batch Loss: 0.2967468202114105\n",
      "Epoch 776, Loss: 3.907452404499054, Final Batch Loss: 0.6081016659736633\n",
      "Epoch 777, Loss: 3.962775707244873, Final Batch Loss: 0.7446858882904053\n",
      "Epoch 778, Loss: 6.5807353258132935, Final Batch Loss: 3.189443588256836\n",
      "Epoch 779, Loss: 3.9558287262916565, Final Batch Loss: 0.6544287800788879\n",
      "Epoch 780, Loss: 4.521211385726929, Final Batch Loss: 1.2289502620697021\n",
      "Epoch 781, Loss: 3.549063205718994, Final Batch Loss: 0.19024574756622314\n",
      "Epoch 782, Loss: 3.779469132423401, Final Batch Loss: 0.5070725679397583\n",
      "Epoch 783, Loss: 4.133307695388794, Final Batch Loss: 0.9860938787460327\n",
      "Epoch 784, Loss: 3.3738595508038998, Final Batch Loss: 0.040094878524541855\n",
      "Epoch 785, Loss: 3.7066431045532227, Final Batch Loss: 0.3620063066482544\n",
      "Epoch 786, Loss: 3.985579550266266, Final Batch Loss: 0.6381837725639343\n",
      "Epoch 787, Loss: 3.35023795068264, Final Batch Loss: 0.13148783147335052\n",
      "Epoch 788, Loss: 4.412014961242676, Final Batch Loss: 1.2204631567001343\n",
      "Epoch 789, Loss: 3.4141473844647408, Final Batch Loss: 0.09815049916505814\n",
      "Epoch 790, Loss: 4.054630398750305, Final Batch Loss: 0.8862254619598389\n",
      "Epoch 791, Loss: 3.404150813817978, Final Batch Loss: 0.24216589331626892\n",
      "Epoch 792, Loss: 3.2042700424790382, Final Batch Loss: 0.07686273008584976\n",
      "Epoch 793, Loss: 3.208226967602968, Final Batch Loss: 0.03836624696850777\n",
      "Epoch 794, Loss: 3.284323327243328, Final Batch Loss: 0.010409466922283173\n",
      "Epoch 795, Loss: 4.365417242050171, Final Batch Loss: 1.071159839630127\n",
      "Epoch 796, Loss: 5.081357836723328, Final Batch Loss: 1.8876804113388062\n",
      "Epoch 797, Loss: 4.53308367729187, Final Batch Loss: 1.2921727895736694\n",
      "Epoch 798, Loss: 4.286422848701477, Final Batch Loss: 1.0877459049224854\n",
      "Epoch 799, Loss: 3.495678186416626, Final Batch Loss: 0.2529038190841675\n",
      "Epoch 800, Loss: 3.763078808784485, Final Batch Loss: 0.5156075954437256\n",
      "Epoch 801, Loss: 4.241193532943726, Final Batch Loss: 1.0365864038467407\n",
      "Epoch 802, Loss: 5.026310682296753, Final Batch Loss: 1.837816834449768\n",
      "Epoch 803, Loss: 3.271877136081457, Final Batch Loss: 0.045283880084753036\n",
      "Epoch 804, Loss: 4.356075286865234, Final Batch Loss: 1.1770349740982056\n",
      "Epoch 805, Loss: 4.015490174293518, Final Batch Loss: 0.8814488649368286\n",
      "Epoch 806, Loss: 4.006675124168396, Final Batch Loss: 0.8703978657722473\n",
      "Epoch 807, Loss: 5.036349773406982, Final Batch Loss: 1.7771759033203125\n",
      "Epoch 808, Loss: 3.7717553973197937, Final Batch Loss: 0.5755829215049744\n",
      "Epoch 809, Loss: 3.9114550352096558, Final Batch Loss: 0.695214033126831\n",
      "Epoch 810, Loss: 5.447747826576233, Final Batch Loss: 2.2836105823516846\n",
      "Epoch 811, Loss: 4.531746208667755, Final Batch Loss: 1.425344705581665\n",
      "Epoch 812, Loss: 4.920198321342468, Final Batch Loss: 1.8255261182785034\n",
      "Epoch 813, Loss: 4.261610567569733, Final Batch Loss: 1.169698715209961\n",
      "Epoch 814, Loss: 3.891664505004883, Final Batch Loss: 0.6884791851043701\n",
      "Epoch 815, Loss: 3.6946322321891785, Final Batch Loss: 0.6177610158920288\n",
      "Epoch 816, Loss: 3.189431915525347, Final Batch Loss: 0.0027138092555105686\n",
      "Epoch 817, Loss: 7.429658591747284, Final Batch Loss: 4.30147647857666\n",
      "Epoch 818, Loss: 4.606178283691406, Final Batch Loss: 1.4090509414672852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 819, Loss: 4.028589248657227, Final Batch Loss: 0.8412944078445435\n",
      "Epoch 820, Loss: 3.9713065028190613, Final Batch Loss: 0.6964666247367859\n",
      "Epoch 821, Loss: 4.962869048118591, Final Batch Loss: 1.4978004693984985\n",
      "Epoch 822, Loss: 4.85985791683197, Final Batch Loss: 1.2996412515640259\n",
      "Epoch 823, Loss: 4.42228502035141, Final Batch Loss: 0.9936287999153137\n",
      "Epoch 824, Loss: 3.728360742330551, Final Batch Loss: 0.3841594159603119\n",
      "Epoch 825, Loss: 4.32788610458374, Final Batch Loss: 1.0253108739852905\n",
      "Epoch 826, Loss: 5.239862561225891, Final Batch Loss: 2.0407638549804688\n",
      "Epoch 827, Loss: 4.523605585098267, Final Batch Loss: 1.3689894676208496\n",
      "Epoch 828, Loss: 3.851924777030945, Final Batch Loss: 0.6393204927444458\n",
      "Epoch 829, Loss: 3.1495662331581116, Final Batch Loss: 0.0330008864402771\n",
      "Epoch 830, Loss: 3.1900810971856117, Final Batch Loss: 0.06361345201730728\n",
      "Epoch 831, Loss: 3.609019935131073, Final Batch Loss: 0.3517248034477234\n",
      "Epoch 832, Loss: 4.684763431549072, Final Batch Loss: 1.5314617156982422\n",
      "Epoch 833, Loss: 4.249472379684448, Final Batch Loss: 1.0254143476486206\n",
      "Epoch 834, Loss: 3.251293893903494, Final Batch Loss: 0.0184401236474514\n",
      "Epoch 835, Loss: 3.2938070446252823, Final Batch Loss: 0.13036693632602692\n",
      "Epoch 836, Loss: 3.8004857301712036, Final Batch Loss: 0.5568287372589111\n",
      "Epoch 837, Loss: 5.456074237823486, Final Batch Loss: 2.2276344299316406\n",
      "Epoch 838, Loss: 5.565127849578857, Final Batch Loss: 2.4530012607574463\n",
      "Epoch 839, Loss: 4.8536383509635925, Final Batch Loss: 1.8050363063812256\n",
      "Epoch 840, Loss: 3.4966717958450317, Final Batch Loss: 0.32763993740081787\n",
      "Epoch 841, Loss: 4.007043421268463, Final Batch Loss: 0.7959528565406799\n",
      "Epoch 842, Loss: 3.260156515520066, Final Batch Loss: 0.004878166597336531\n",
      "Epoch 843, Loss: 3.2437018919736147, Final Batch Loss: 0.023338036611676216\n",
      "Epoch 844, Loss: 3.24157770909369, Final Batch Loss: 0.026280010119080544\n",
      "Epoch 845, Loss: 4.615463018417358, Final Batch Loss: 1.4425147771835327\n",
      "Epoch 846, Loss: 3.6989582777023315, Final Batch Loss: 0.5302709341049194\n",
      "Epoch 847, Loss: 4.80883252620697, Final Batch Loss: 1.693304419517517\n",
      "Epoch 848, Loss: 5.151362299919128, Final Batch Loss: 1.9607534408569336\n",
      "Epoch 849, Loss: 3.8766921758651733, Final Batch Loss: 0.5114777088165283\n",
      "Epoch 850, Loss: 3.4444186985492706, Final Batch Loss: 0.26510027050971985\n",
      "Epoch 851, Loss: 3.2560489550232887, Final Batch Loss: 0.08200732618570328\n",
      "Epoch 852, Loss: 5.510925650596619, Final Batch Loss: 2.2938034534454346\n",
      "Epoch 853, Loss: 5.03721022605896, Final Batch Loss: 1.8654593229293823\n",
      "Epoch 854, Loss: 3.974187970161438, Final Batch Loss: 0.822120726108551\n",
      "Epoch 855, Loss: 4.164693772792816, Final Batch Loss: 0.958536684513092\n",
      "Epoch 856, Loss: 3.2625278925988823, Final Batch Loss: 0.003465125570073724\n",
      "Epoch 857, Loss: 3.2997937835752964, Final Batch Loss: 0.003254357725381851\n",
      "Epoch 858, Loss: 3.5751191476592794, Final Batch Loss: 0.0009662011871114373\n",
      "Epoch 859, Loss: 5.805805087089539, Final Batch Loss: 2.340338945388794\n",
      "Epoch 860, Loss: 5.706192255020142, Final Batch Loss: 2.394378662109375\n",
      "Epoch 861, Loss: 3.965472400188446, Final Batch Loss: 0.6412258744239807\n",
      "Epoch 862, Loss: 6.083327889442444, Final Batch Loss: 2.636812448501587\n",
      "Epoch 863, Loss: 4.264536261558533, Final Batch Loss: 0.9403234720230103\n",
      "Epoch 864, Loss: 4.205665588378906, Final Batch Loss: 0.7552112340927124\n",
      "Epoch 865, Loss: 3.69525732845068, Final Batch Loss: 0.07179882377386093\n",
      "Epoch 866, Loss: 5.490193247795105, Final Batch Loss: 1.7809557914733887\n",
      "Epoch 867, Loss: 5.086885094642639, Final Batch Loss: 1.4395273923873901\n",
      "Epoch 868, Loss: 4.280804932117462, Final Batch Loss: 0.9059101939201355\n",
      "Epoch 869, Loss: 4.4128721952438354, Final Batch Loss: 1.0642484426498413\n",
      "Epoch 870, Loss: 4.412742495536804, Final Batch Loss: 1.103973150253296\n",
      "Epoch 871, Loss: 3.7012972831726074, Final Batch Loss: 0.4149744510650635\n",
      "Epoch 872, Loss: 3.836708426475525, Final Batch Loss: 0.621666431427002\n",
      "Epoch 873, Loss: 4.928818702697754, Final Batch Loss: 1.7739835977554321\n",
      "Epoch 874, Loss: 6.096240758895874, Final Batch Loss: 2.878293514251709\n",
      "Epoch 875, Loss: 3.965625524520874, Final Batch Loss: 0.8234300017356873\n",
      "Epoch 876, Loss: 5.0474772453308105, Final Batch Loss: 1.8429006338119507\n",
      "Epoch 877, Loss: 5.8187196254730225, Final Batch Loss: 2.618807077407837\n",
      "Epoch 878, Loss: 5.044019818305969, Final Batch Loss: 1.8461458683013916\n",
      "Epoch 879, Loss: 3.4722536131739616, Final Batch Loss: 0.1247592493891716\n",
      "Epoch 880, Loss: 5.038774490356445, Final Batch Loss: 1.7373652458190918\n",
      "Epoch 881, Loss: 5.064598083496094, Final Batch Loss: 1.6905138492584229\n",
      "Epoch 882, Loss: 4.017763316631317, Final Batch Loss: 0.781408429145813\n",
      "Epoch 883, Loss: 3.819401502609253, Final Batch Loss: 0.5820345878601074\n",
      "Epoch 884, Loss: 4.514701247215271, Final Batch Loss: 1.4084150791168213\n",
      "Epoch 885, Loss: 3.8089144229888916, Final Batch Loss: 0.7446837425231934\n",
      "Epoch 886, Loss: 3.2088124500587583, Final Batch Loss: 0.01297530997544527\n",
      "Epoch 887, Loss: 4.798727571964264, Final Batch Loss: 1.584848165512085\n",
      "Epoch 888, Loss: 4.491283297538757, Final Batch Loss: 1.3232176303863525\n",
      "Epoch 889, Loss: 3.9563998579978943, Final Batch Loss: 0.8909657597541809\n",
      "Epoch 890, Loss: 4.14159482717514, Final Batch Loss: 1.03612220287323\n",
      "Epoch 891, Loss: 3.792749047279358, Final Batch Loss: 0.7191939949989319\n",
      "Epoch 892, Loss: 4.194247364997864, Final Batch Loss: 1.0769824981689453\n",
      "Epoch 893, Loss: 3.0983933694660664, Final Batch Loss: 0.035263705998659134\n",
      "Epoch 894, Loss: 4.8274253606796265, Final Batch Loss: 1.6995964050292969\n",
      "Epoch 895, Loss: 3.111516484990716, Final Batch Loss: 0.0070155952125787735\n",
      "Epoch 896, Loss: 4.317752003669739, Final Batch Loss: 1.2915573120117188\n",
      "Epoch 897, Loss: 6.484404444694519, Final Batch Loss: 3.3984622955322266\n",
      "Epoch 898, Loss: 4.146007001399994, Final Batch Loss: 1.0567454099655151\n",
      "Epoch 899, Loss: 3.7112282514572144, Final Batch Loss: 0.5582321882247925\n",
      "Epoch 900, Loss: 4.041877329349518, Final Batch Loss: 0.8488028645515442\n",
      "Epoch 901, Loss: 4.59655225276947, Final Batch Loss: 1.4455595016479492\n",
      "Epoch 902, Loss: 5.506298184394836, Final Batch Loss: 2.2776331901550293\n",
      "Epoch 903, Loss: 3.3815969601273537, Final Batch Loss: 0.11251560598611832\n",
      "Epoch 904, Loss: 5.109182834625244, Final Batch Loss: 1.964537262916565\n",
      "Epoch 905, Loss: 3.407610461115837, Final Batch Loss: 0.1941724270582199\n",
      "Epoch 906, Loss: 3.9723227620124817, Final Batch Loss: 0.7896153330802917\n",
      "Epoch 907, Loss: 5.348233222961426, Final Batch Loss: 2.180013418197632\n",
      "Epoch 908, Loss: 5.441109120845795, Final Batch Loss: 2.383596181869507\n",
      "Epoch 909, Loss: 4.823919892311096, Final Batch Loss: 1.7724711894989014\n",
      "Epoch 910, Loss: 4.1743385791778564, Final Batch Loss: 1.0801100730895996\n",
      "Epoch 911, Loss: 3.1866603437811136, Final Batch Loss: 0.02099352516233921\n",
      "Epoch 912, Loss: 4.666798770427704, Final Batch Loss: 1.5947545766830444\n",
      "Epoch 913, Loss: 4.724632143974304, Final Batch Loss: 1.5431535243988037\n",
      "Epoch 914, Loss: 4.300289213657379, Final Batch Loss: 1.2803590297698975\n",
      "Epoch 915, Loss: 3.900326669216156, Final Batch Loss: 0.7574300169944763\n",
      "Epoch 916, Loss: 4.718561768531799, Final Batch Loss: 1.53768789768219\n",
      "Epoch 917, Loss: 3.481419175863266, Final Batch Loss: 0.3011889159679413\n",
      "Epoch 918, Loss: 4.298982620239258, Final Batch Loss: 1.1240081787109375\n",
      "Epoch 919, Loss: 4.508205533027649, Final Batch Loss: 1.3334994316101074\n",
      "Epoch 920, Loss: 3.2865245938301086, Final Batch Loss: 0.1493322253227234\n",
      "Epoch 921, Loss: 4.717535495758057, Final Batch Loss: 1.58474600315094\n",
      "Epoch 922, Loss: 4.729564905166626, Final Batch Loss: 1.609135627746582\n",
      "Epoch 923, Loss: 3.106660556048155, Final Batch Loss: 0.052095603197813034\n",
      "Epoch 924, Loss: 4.157992720603943, Final Batch Loss: 0.9356886148452759\n",
      "Epoch 925, Loss: 3.6119344830513, Final Batch Loss: 0.578312873840332\n",
      "Epoch 926, Loss: 5.732161104679108, Final Batch Loss: 2.6830763816833496\n",
      "Epoch 927, Loss: 4.447137117385864, Final Batch Loss: 1.3707423210144043\n",
      "Epoch 928, Loss: 4.893037140369415, Final Batch Loss: 1.6669697761535645\n",
      "Epoch 929, Loss: 4.366041958332062, Final Batch Loss: 0.8420526385307312\n",
      "Epoch 930, Loss: 4.034255027770996, Final Batch Loss: 0.5576496124267578\n",
      "Epoch 931, Loss: 3.9492025077342987, Final Batch Loss: 0.49448344111442566\n",
      "Epoch 932, Loss: 4.626201152801514, Final Batch Loss: 1.3523229360580444\n",
      "Epoch 933, Loss: 3.205552950501442, Final Batch Loss: 0.16954512894153595\n",
      "Epoch 934, Loss: 3.229447614401579, Final Batch Loss: 0.025664400309324265\n",
      "Epoch 935, Loss: 6.272865176200867, Final Batch Loss: 2.8655591011047363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 936, Loss: 4.700728893280029, Final Batch Loss: 1.224351167678833\n",
      "Epoch 937, Loss: 4.337537527084351, Final Batch Loss: 0.9131799936294556\n",
      "Epoch 938, Loss: 3.585965022444725, Final Batch Loss: 0.14768050611019135\n",
      "Epoch 939, Loss: 4.114428520202637, Final Batch Loss: 0.7921100854873657\n",
      "Epoch 940, Loss: 5.408758282661438, Final Batch Loss: 2.09023380279541\n",
      "Epoch 941, Loss: 4.849722743034363, Final Batch Loss: 1.670259952545166\n",
      "Epoch 942, Loss: 5.044605076313019, Final Batch Loss: 1.8570994138717651\n",
      "Epoch 943, Loss: 3.836009979248047, Final Batch Loss: 0.7380503416061401\n",
      "Epoch 944, Loss: 4.439187169075012, Final Batch Loss: 1.4311542510986328\n",
      "Epoch 945, Loss: 4.956303238868713, Final Batch Loss: 1.8288129568099976\n",
      "Epoch 946, Loss: 3.132132178172469, Final Batch Loss: 0.022093242034316063\n",
      "Epoch 947, Loss: 5.500027358531952, Final Batch Loss: 2.4214043617248535\n",
      "Epoch 948, Loss: 3.0509522184729576, Final Batch Loss: 0.024180613458156586\n",
      "Epoch 949, Loss: 3.7843517661094666, Final Batch Loss: 0.46808117628097534\n",
      "Epoch 950, Loss: 7.51588237285614, Final Batch Loss: 3.837857246398926\n",
      "Epoch 951, Loss: 3.821767032146454, Final Batch Loss: 0.12880021333694458\n",
      "Epoch 952, Loss: 4.979361653327942, Final Batch Loss: 1.0104849338531494\n",
      "Epoch 953, Loss: 4.902838468551636, Final Batch Loss: 1.0469157695770264\n",
      "Epoch 954, Loss: 7.416432976722717, Final Batch Loss: 3.875755548477173\n",
      "Epoch 955, Loss: 3.3117906618863344, Final Batch Loss: 0.021478431299328804\n",
      "Epoch 956, Loss: 3.6830892860889435, Final Batch Loss: 0.47452113032341003\n",
      "Epoch 957, Loss: 3.4634566716849804, Final Batch Loss: 0.02223793789744377\n",
      "Epoch 958, Loss: 4.527754306793213, Final Batch Loss: 0.9884154796600342\n",
      "Epoch 959, Loss: 5.028654932975769, Final Batch Loss: 1.6464502811431885\n",
      "Epoch 960, Loss: 3.3820084035396576, Final Batch Loss: 0.1743657886981964\n",
      "Epoch 961, Loss: 3.942034900188446, Final Batch Loss: 0.8275430798530579\n",
      "Epoch 962, Loss: 3.138220988214016, Final Batch Loss: 0.07981350272893906\n",
      "Epoch 963, Loss: 5.875841975212097, Final Batch Loss: 2.792651653289795\n",
      "Epoch 964, Loss: 3.0944313406944275, Final Batch Loss: 0.03305763006210327\n",
      "Epoch 965, Loss: 3.000557204708457, Final Batch Loss: 0.024372359737753868\n",
      "Epoch 966, Loss: 4.524501025676727, Final Batch Loss: 1.571445345878601\n",
      "Epoch 967, Loss: 4.430696666240692, Final Batch Loss: 1.4014748334884644\n",
      "Epoch 968, Loss: 3.6344701051712036, Final Batch Loss: 0.5236454010009766\n",
      "Epoch 969, Loss: 4.607639729976654, Final Batch Loss: 1.574139952659607\n",
      "Epoch 970, Loss: 3.059062272310257, Final Batch Loss: 0.11289337277412415\n",
      "Epoch 971, Loss: 3.8623313903808594, Final Batch Loss: 0.8277794122695923\n",
      "Epoch 972, Loss: 4.024100601673126, Final Batch Loss: 0.9460817575454712\n",
      "Epoch 973, Loss: 3.8653072118759155, Final Batch Loss: 0.720054030418396\n",
      "Epoch 974, Loss: 3.412179410457611, Final Batch Loss: 0.16507989168167114\n",
      "Epoch 975, Loss: 5.018875241279602, Final Batch Loss: 1.6870373487472534\n",
      "Epoch 976, Loss: 5.594412922859192, Final Batch Loss: 2.4406397342681885\n",
      "Epoch 977, Loss: 4.8350090980529785, Final Batch Loss: 1.6338077783584595\n",
      "Epoch 978, Loss: 3.191771686077118, Final Batch Loss: 0.10201853513717651\n",
      "Epoch 979, Loss: 3.5014243125915527, Final Batch Loss: 0.34504103660583496\n",
      "Epoch 980, Loss: 4.7111106514930725, Final Batch Loss: 1.6450755596160889\n",
      "Epoch 981, Loss: 3.801205635070801, Final Batch Loss: 0.6932371854782104\n",
      "Epoch 982, Loss: 3.683107316493988, Final Batch Loss: 0.5368901491165161\n",
      "Epoch 983, Loss: 3.5680145025253296, Final Batch Loss: 0.3871150016784668\n",
      "Epoch 984, Loss: 4.898154973983765, Final Batch Loss: 1.7383830547332764\n",
      "Epoch 985, Loss: 4.3074105978012085, Final Batch Loss: 1.335496425628662\n",
      "Epoch 986, Loss: 4.216312289237976, Final Batch Loss: 1.1027767658233643\n",
      "Epoch 987, Loss: 4.873034238815308, Final Batch Loss: 1.7624614238739014\n",
      "Epoch 988, Loss: 5.486090660095215, Final Batch Loss: 2.529153347015381\n",
      "Epoch 989, Loss: 3.4587498903274536, Final Batch Loss: 0.3775445222854614\n",
      "Epoch 990, Loss: 5.222709000110626, Final Batch Loss: 2.1755332946777344\n",
      "Epoch 991, Loss: 4.377362251281738, Final Batch Loss: 1.3477115631103516\n",
      "Epoch 992, Loss: 4.140501379966736, Final Batch Loss: 1.0443122386932373\n",
      "Epoch 993, Loss: 3.979941189289093, Final Batch Loss: 0.9726977944374084\n",
      "Epoch 994, Loss: 5.765395402908325, Final Batch Loss: 2.7486379146575928\n",
      "Epoch 995, Loss: 5.047392666339874, Final Batch Loss: 2.0019631385803223\n",
      "Epoch 996, Loss: 3.986898899078369, Final Batch Loss: 0.8603548407554626\n",
      "Epoch 997, Loss: 4.519592642784119, Final Batch Loss: 1.2765413522720337\n",
      "Epoch 998, Loss: 5.717495441436768, Final Batch Loss: 2.3631558418273926\n",
      "Epoch 999, Loss: 3.818075567483902, Final Batch Loss: 0.47172310948371887\n",
      "Epoch 1000, Loss: 3.4285215130075812, Final Batch Loss: 0.013298033736646175\n",
      "Epoch 1001, Loss: 3.4598391950130463, Final Batch Loss: 0.0774594247341156\n",
      "Epoch 1002, Loss: 3.269518915563822, Final Batch Loss: 0.030220571905374527\n",
      "Epoch 1003, Loss: 3.201027389615774, Final Batch Loss: 0.05502616986632347\n",
      "Epoch 1004, Loss: 4.206667900085449, Final Batch Loss: 1.177736520767212\n",
      "Epoch 1005, Loss: 4.512152552604675, Final Batch Loss: 1.5334587097167969\n",
      "Epoch 1006, Loss: 4.84104585647583, Final Batch Loss: 1.855500340461731\n",
      "Epoch 1007, Loss: 3.2753480672836304, Final Batch Loss: 0.2629762291908264\n",
      "Epoch 1008, Loss: 3.6360182762145996, Final Batch Loss: 0.6879149079322815\n",
      "Epoch 1009, Loss: 4.132125020027161, Final Batch Loss: 1.123792290687561\n",
      "Epoch 1010, Loss: 3.5025851130485535, Final Batch Loss: 0.517135500907898\n",
      "Epoch 1011, Loss: 3.9807509779930115, Final Batch Loss: 0.9855644106864929\n",
      "Epoch 1012, Loss: 5.490803241729736, Final Batch Loss: 2.395096778869629\n",
      "Epoch 1013, Loss: 2.935772914905101, Final Batch Loss: 0.0013668728061020374\n",
      "Epoch 1014, Loss: 3.7252443432807922, Final Batch Loss: 0.6525110602378845\n",
      "Epoch 1015, Loss: 3.633120059967041, Final Batch Loss: 0.3879506587982178\n",
      "Epoch 1016, Loss: 4.343906760215759, Final Batch Loss: 1.0984737873077393\n",
      "Epoch 1017, Loss: 4.020933747291565, Final Batch Loss: 0.8713183403015137\n",
      "Epoch 1018, Loss: 4.5759459137916565, Final Batch Loss: 1.3997375965118408\n",
      "Epoch 1019, Loss: 3.1363822128623724, Final Batch Loss: 0.027222206816077232\n",
      "Epoch 1020, Loss: 3.9431395530700684, Final Batch Loss: 0.9286540746688843\n",
      "Epoch 1021, Loss: 4.6516202092170715, Final Batch Loss: 1.6605076789855957\n",
      "Epoch 1022, Loss: 2.9282931853085756, Final Batch Loss: 0.027878480032086372\n",
      "Epoch 1023, Loss: 3.9766659140586853, Final Batch Loss: 0.9787629842758179\n",
      "Epoch 1024, Loss: 4.888717472553253, Final Batch Loss: 1.8912187814712524\n",
      "Epoch 1025, Loss: 3.7904999256134033, Final Batch Loss: 0.8451958298683167\n",
      "Epoch 1026, Loss: 3.7370648980140686, Final Batch Loss: 0.7009573578834534\n",
      "Epoch 1027, Loss: 4.489033818244934, Final Batch Loss: 1.3186637163162231\n",
      "Epoch 1028, Loss: 5.045135498046875, Final Batch Loss: 1.695830225944519\n",
      "Epoch 1029, Loss: 4.233445167541504, Final Batch Loss: 1.107616662979126\n",
      "Epoch 1030, Loss: 4.414025545120239, Final Batch Loss: 1.2554534673690796\n",
      "Epoch 1031, Loss: 3.4420922994613647, Final Batch Loss: 0.48445624113082886\n",
      "Epoch 1032, Loss: 3.8637205362319946, Final Batch Loss: 0.8414844274520874\n",
      "Epoch 1033, Loss: 3.249344617128372, Final Batch Loss: 0.2530306279659271\n",
      "Epoch 1034, Loss: 3.195510245859623, Final Batch Loss: 0.0850364938378334\n",
      "Epoch 1035, Loss: 3.0834045242518187, Final Batch Loss: 0.02073107473552227\n",
      "Epoch 1036, Loss: 4.4776105880737305, Final Batch Loss: 1.4503283500671387\n",
      "Epoch 1037, Loss: 6.099205791950226, Final Batch Loss: 3.038851737976074\n",
      "Epoch 1038, Loss: 3.0627835988998413, Final Batch Loss: 0.06203103065490723\n",
      "Epoch 1039, Loss: 5.1452460289001465, Final Batch Loss: 2.126368999481201\n",
      "Epoch 1040, Loss: 4.146903097629547, Final Batch Loss: 1.1598536968231201\n",
      "Epoch 1041, Loss: 4.32274866104126, Final Batch Loss: 1.3058725595474243\n",
      "Epoch 1042, Loss: 2.906839894130826, Final Batch Loss: 0.00780402310192585\n",
      "Epoch 1043, Loss: 3.6731762290000916, Final Batch Loss: 0.6687608361244202\n",
      "Epoch 1044, Loss: 5.7421727776527405, Final Batch Loss: 2.7677621841430664\n",
      "Epoch 1045, Loss: 4.812552809715271, Final Batch Loss: 1.8772114515304565\n",
      "Epoch 1046, Loss: 4.46064156293869, Final Batch Loss: 1.6075713634490967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1047, Loss: 3.4335097670555115, Final Batch Loss: 0.5133493542671204\n",
      "Epoch 1048, Loss: 3.0539172366261482, Final Batch Loss: 0.06705767661333084\n",
      "Epoch 1049, Loss: 3.2417324632406235, Final Batch Loss: 0.2499864548444748\n",
      "Epoch 1050, Loss: 3.0607917457818985, Final Batch Loss: 0.19279827177524567\n",
      "Epoch 1051, Loss: 3.0699948593974113, Final Batch Loss: 0.10367535799741745\n",
      "Epoch 1052, Loss: 4.406101942062378, Final Batch Loss: 1.4445263147354126\n",
      "Epoch 1053, Loss: 4.839358925819397, Final Batch Loss: 1.9913467168807983\n",
      "Epoch 1054, Loss: 4.944628477096558, Final Batch Loss: 2.0516462326049805\n",
      "Epoch 1055, Loss: 2.866934268735349, Final Batch Loss: 0.005641846917569637\n",
      "Epoch 1056, Loss: 3.266625851392746, Final Batch Loss: 0.2871260941028595\n",
      "Epoch 1057, Loss: 4.711804509162903, Final Batch Loss: 1.6603952646255493\n",
      "Epoch 1058, Loss: 3.7118071913719177, Final Batch Loss: 0.7167040109634399\n",
      "Epoch 1059, Loss: 3.047094441950321, Final Batch Loss: 0.04851726442575455\n",
      "Epoch 1060, Loss: 2.9032544649671763, Final Batch Loss: 0.002599552972242236\n",
      "Epoch 1061, Loss: 3.914681375026703, Final Batch Loss: 0.9296281933784485\n",
      "Epoch 1062, Loss: 5.381101965904236, Final Batch Loss: 2.3922133445739746\n",
      "Epoch 1063, Loss: 3.995984971523285, Final Batch Loss: 1.0091737508773804\n",
      "Epoch 1064, Loss: 3.5407721996307373, Final Batch Loss: 0.6752146482467651\n",
      "Epoch 1065, Loss: 3.2459886372089386, Final Batch Loss: 0.3504815399646759\n",
      "Epoch 1066, Loss: 2.8313832031562924, Final Batch Loss: 0.006183658726513386\n",
      "Epoch 1067, Loss: 4.060187220573425, Final Batch Loss: 1.1870954036712646\n",
      "Epoch 1068, Loss: 4.225084483623505, Final Batch Loss: 1.4056439399719238\n",
      "Epoch 1069, Loss: 4.52392190694809, Final Batch Loss: 1.6184262037277222\n",
      "Epoch 1070, Loss: 3.7763983607292175, Final Batch Loss: 0.8445106148719788\n",
      "Epoch 1071, Loss: 3.432305335998535, Final Batch Loss: 0.5306488871574402\n",
      "Epoch 1072, Loss: 2.9189668018370867, Final Batch Loss: 0.015733512118458748\n",
      "Epoch 1073, Loss: 4.497810244560242, Final Batch Loss: 1.4036060571670532\n",
      "Epoch 1074, Loss: 3.036778524518013, Final Batch Loss: 0.06127817928791046\n",
      "Epoch 1075, Loss: 3.400166094303131, Final Batch Loss: 0.47528183460235596\n",
      "Epoch 1076, Loss: 3.144211933016777, Final Batch Loss: 0.1496056169271469\n",
      "Epoch 1077, Loss: 3.1573336720466614, Final Batch Loss: 0.16357213258743286\n",
      "Epoch 1078, Loss: 4.633922576904297, Final Batch Loss: 1.6459828615188599\n",
      "Epoch 1079, Loss: 3.736660420894623, Final Batch Loss: 0.7633008360862732\n",
      "Epoch 1080, Loss: 6.651770353317261, Final Batch Loss: 3.6447529792785645\n",
      "Epoch 1081, Loss: 5.350777208805084, Final Batch Loss: 2.4473743438720703\n",
      "Epoch 1082, Loss: 4.01968640089035, Final Batch Loss: 1.1058233976364136\n",
      "Epoch 1083, Loss: 4.167621612548828, Final Batch Loss: 1.253827691078186\n",
      "Epoch 1084, Loss: 4.766056418418884, Final Batch Loss: 1.8243374824523926\n",
      "Epoch 1085, Loss: 4.56658262014389, Final Batch Loss: 1.5759638547897339\n",
      "Epoch 1086, Loss: 4.63867849111557, Final Batch Loss: 1.686276912689209\n",
      "Epoch 1087, Loss: 5.694568157196045, Final Batch Loss: 2.7333295345306396\n",
      "Epoch 1088, Loss: 4.97734659910202, Final Batch Loss: 1.9531428813934326\n",
      "Epoch 1089, Loss: 2.8633956264238805, Final Batch Loss: 0.0035539816599339247\n",
      "Epoch 1090, Loss: 4.280185341835022, Final Batch Loss: 1.3138878345489502\n",
      "Epoch 1091, Loss: 3.789826512336731, Final Batch Loss: 0.8737335801124573\n",
      "Epoch 1092, Loss: 2.9729256480932236, Final Batch Loss: 0.08979938924312592\n",
      "Epoch 1093, Loss: 3.3040369153022766, Final Batch Loss: 0.3841218948364258\n",
      "Epoch 1094, Loss: 2.895940087735653, Final Batch Loss: 0.03810533136129379\n",
      "Epoch 1095, Loss: 4.192223012447357, Final Batch Loss: 1.2290666103363037\n",
      "Epoch 1096, Loss: 2.8479442121461034, Final Batch Loss: 0.005570602603256702\n",
      "Epoch 1097, Loss: 2.907834641635418, Final Batch Loss: 0.06882905215024948\n",
      "Epoch 1098, Loss: 4.455858409404755, Final Batch Loss: 1.5357635021209717\n",
      "Epoch 1099, Loss: 3.24260351061821, Final Batch Loss: 0.268270343542099\n",
      "Epoch 1100, Loss: 3.075238138437271, Final Batch Loss: 0.2234475314617157\n",
      "Epoch 1101, Loss: 6.484417617321014, Final Batch Loss: 3.5776405334472656\n",
      "Epoch 1102, Loss: 4.060155093669891, Final Batch Loss: 1.1929503679275513\n",
      "Epoch 1103, Loss: 4.214761912822723, Final Batch Loss: 1.293662190437317\n",
      "Epoch 1104, Loss: 4.55255651473999, Final Batch Loss: 1.5449689626693726\n",
      "Epoch 1105, Loss: 2.84492090344429, Final Batch Loss: 0.07010772824287415\n",
      "Epoch 1106, Loss: 5.892953395843506, Final Batch Loss: 3.0433249473571777\n",
      "Epoch 1107, Loss: 4.757700204849243, Final Batch Loss: 1.7701892852783203\n",
      "Epoch 1108, Loss: 4.411223411560059, Final Batch Loss: 1.2086491584777832\n",
      "Epoch 1109, Loss: 4.193858623504639, Final Batch Loss: 0.7937978506088257\n",
      "Epoch 1110, Loss: 4.721101403236389, Final Batch Loss: 1.4093948602676392\n",
      "Epoch 1111, Loss: 4.143637418746948, Final Batch Loss: 0.9858108758926392\n",
      "Epoch 1112, Loss: 3.9247316122055054, Final Batch Loss: 0.8842964768409729\n",
      "Epoch 1113, Loss: 4.371148943901062, Final Batch Loss: 1.5003265142440796\n",
      "Epoch 1114, Loss: 3.1157457754015923, Final Batch Loss: 0.11745809763669968\n",
      "Epoch 1115, Loss: 3.8865296244621277, Final Batch Loss: 0.8157150745391846\n",
      "Epoch 1116, Loss: 2.907045485917479, Final Batch Loss: 0.006862523499876261\n",
      "Epoch 1117, Loss: 3.746199309825897, Final Batch Loss: 0.8065370917320251\n",
      "Epoch 1118, Loss: 4.282877802848816, Final Batch Loss: 1.3916796445846558\n",
      "Epoch 1119, Loss: 3.136891543865204, Final Batch Loss: 0.204326331615448\n",
      "Epoch 1120, Loss: 4.245719015598297, Final Batch Loss: 1.193596601486206\n",
      "Epoch 1121, Loss: 2.916335066780448, Final Batch Loss: 0.01592840813100338\n",
      "Epoch 1122, Loss: 3.5521585941314697, Final Batch Loss: 0.6302276849746704\n",
      "Epoch 1123, Loss: 2.949460454285145, Final Batch Loss: 0.1092846468091011\n",
      "Epoch 1124, Loss: 3.442289113998413, Final Batch Loss: 0.46416813135147095\n",
      "Epoch 1125, Loss: 3.2360303699970245, Final Batch Loss: 0.3632287085056305\n",
      "Epoch 1126, Loss: 3.931821286678314, Final Batch Loss: 0.9473760724067688\n",
      "Epoch 1127, Loss: 4.765308320522308, Final Batch Loss: 1.7688630819320679\n",
      "Epoch 1128, Loss: 2.988063646480441, Final Batch Loss: 0.029996050521731377\n",
      "Epoch 1129, Loss: 4.55032604932785, Final Batch Loss: 1.5735993385314941\n",
      "Epoch 1130, Loss: 3.344141036272049, Final Batch Loss: 0.3594898283481598\n",
      "Epoch 1131, Loss: 2.9543338119983673, Final Batch Loss: 0.17057844996452332\n",
      "Epoch 1132, Loss: 3.028864026069641, Final Batch Loss: 0.15792089700698853\n",
      "Epoch 1133, Loss: 4.434512674808502, Final Batch Loss: 1.5273947715759277\n",
      "Epoch 1134, Loss: 3.4875014424324036, Final Batch Loss: 0.6963315606117249\n",
      "Epoch 1135, Loss: 3.8321533799171448, Final Batch Loss: 1.0457141399383545\n",
      "Epoch 1136, Loss: 3.3586245477199554, Final Batch Loss: 0.4761368930339813\n",
      "Epoch 1137, Loss: 3.3237338066101074, Final Batch Loss: 0.5712476372718811\n",
      "Epoch 1138, Loss: 3.1325294226408005, Final Batch Loss: 0.2310175746679306\n",
      "Epoch 1139, Loss: 4.432738780975342, Final Batch Loss: 1.520733118057251\n",
      "Epoch 1140, Loss: 2.924084016121924, Final Batch Loss: 0.013167568482458591\n",
      "Epoch 1141, Loss: 3.8411957025527954, Final Batch Loss: 1.037353754043579\n",
      "Epoch 1142, Loss: 3.4683059453964233, Final Batch Loss: 0.5935794115066528\n",
      "Epoch 1143, Loss: 4.181340932846069, Final Batch Loss: 1.3073676824569702\n",
      "Epoch 1144, Loss: 4.12164694070816, Final Batch Loss: 1.2807865142822266\n",
      "Epoch 1145, Loss: 3.036326229572296, Final Batch Loss: 0.12928366661071777\n",
      "Epoch 1146, Loss: 2.762092027813196, Final Batch Loss: 0.025359008461236954\n",
      "Epoch 1147, Loss: 4.664462327957153, Final Batch Loss: 1.7896448373794556\n",
      "Epoch 1148, Loss: 3.783709168434143, Final Batch Loss: 0.92215895652771\n",
      "Epoch 1149, Loss: 2.8997595757246017, Final Batch Loss: 0.0629042237997055\n",
      "Epoch 1150, Loss: 3.8780393600463867, Final Batch Loss: 1.116409182548523\n",
      "Epoch 1151, Loss: 2.8040266800671816, Final Batch Loss: 0.01634635217487812\n",
      "Epoch 1152, Loss: 2.8949634302407503, Final Batch Loss: 0.016257451847195625\n",
      "Epoch 1153, Loss: 4.938352823257446, Final Batch Loss: 2.1220245361328125\n",
      "Epoch 1154, Loss: 3.0075979083776474, Final Batch Loss: 0.18769894540309906\n",
      "Epoch 1155, Loss: 3.833886742591858, Final Batch Loss: 1.057466983795166\n",
      "Epoch 1156, Loss: 2.8701969869434834, Final Batch Loss: 0.04499034956097603\n",
      "Epoch 1157, Loss: 3.3457010984420776, Final Batch Loss: 0.6159793138504028\n",
      "Epoch 1158, Loss: 4.710802316665649, Final Batch Loss: 1.760303020477295\n",
      "Epoch 1159, Loss: 5.0695629715919495, Final Batch Loss: 1.987125039100647\n",
      "Epoch 1160, Loss: 4.065672755241394, Final Batch Loss: 1.0892624855041504\n",
      "Epoch 1161, Loss: 4.2614706158638, Final Batch Loss: 1.2450902462005615\n",
      "Epoch 1162, Loss: 2.9520019590854645, Final Batch Loss: 0.08434602618217468\n",
      "Epoch 1163, Loss: 4.254296600818634, Final Batch Loss: 1.4090189933776855\n",
      "Epoch 1164, Loss: 3.0712841749191284, Final Batch Loss: 0.27069491147994995\n",
      "Epoch 1165, Loss: 3.2612358927726746, Final Batch Loss: 0.45403993129730225\n",
      "Epoch 1166, Loss: 3.83632093667984, Final Batch Loss: 0.9852361679077148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1167, Loss: 3.0447245836257935, Final Batch Loss: 0.3212946653366089\n",
      "Epoch 1168, Loss: 3.607291579246521, Final Batch Loss: 0.8156346082687378\n",
      "Epoch 1169, Loss: 4.1790977120399475, Final Batch Loss: 1.3851511478424072\n",
      "Epoch 1170, Loss: 5.534947514533997, Final Batch Loss: 2.8076529502868652\n",
      "Epoch 1171, Loss: 4.354271233081818, Final Batch Loss: 1.5385841131210327\n",
      "Epoch 1172, Loss: 3.542443037033081, Final Batch Loss: 0.5655801892280579\n",
      "Epoch 1173, Loss: 3.965438485145569, Final Batch Loss: 0.6046551465988159\n",
      "Epoch 1174, Loss: 4.8699493408203125, Final Batch Loss: 1.5282678604125977\n",
      "Epoch 1175, Loss: 6.191198348999023, Final Batch Loss: 3.0540099143981934\n",
      "Epoch 1176, Loss: 3.5192448496818542, Final Batch Loss: 0.5276610851287842\n",
      "Epoch 1177, Loss: 3.672798454761505, Final Batch Loss: 0.5991034507751465\n",
      "Epoch 1178, Loss: 3.492814302444458, Final Batch Loss: 0.5244600176811218\n",
      "Epoch 1179, Loss: 3.1907745599746704, Final Batch Loss: 0.1621648669242859\n",
      "Epoch 1180, Loss: 5.46508252620697, Final Batch Loss: 2.623607873916626\n",
      "Epoch 1181, Loss: 4.405965328216553, Final Batch Loss: 1.5272231101989746\n",
      "Epoch 1182, Loss: 3.03045941144228, Final Batch Loss: 0.09784073382616043\n",
      "Epoch 1183, Loss: 2.8748927786946297, Final Batch Loss: 0.06506694108247757\n",
      "Epoch 1184, Loss: 2.9089218452572823, Final Batch Loss: 0.051222093403339386\n",
      "Epoch 1185, Loss: 4.312682092189789, Final Batch Loss: 1.577621340751648\n",
      "Epoch 1186, Loss: 5.0736677050590515, Final Batch Loss: 2.2251758575439453\n",
      "Epoch 1187, Loss: 3.2187643945217133, Final Batch Loss: 0.4325226843357086\n",
      "Epoch 1188, Loss: 4.166514575481415, Final Batch Loss: 1.387020468711853\n",
      "Epoch 1189, Loss: 4.206933557987213, Final Batch Loss: 1.34457266330719\n",
      "Epoch 1190, Loss: 3.575000047683716, Final Batch Loss: 0.8828181028366089\n",
      "Epoch 1191, Loss: 5.386358141899109, Final Batch Loss: 2.5892345905303955\n",
      "Epoch 1192, Loss: 4.805760502815247, Final Batch Loss: 1.9072973728179932\n",
      "Epoch 1193, Loss: 3.1510289907455444, Final Batch Loss: 0.2611321210861206\n",
      "Epoch 1194, Loss: 5.45932811498642, Final Batch Loss: 2.4908668994903564\n",
      "Epoch 1195, Loss: 2.907595955824945, Final Batch Loss: 0.0007410878897644579\n",
      "Epoch 1196, Loss: 4.05217581987381, Final Batch Loss: 0.9194132089614868\n",
      "Epoch 1197, Loss: 4.452492117881775, Final Batch Loss: 1.2692363262176514\n",
      "Epoch 1198, Loss: 3.8508626222610474, Final Batch Loss: 0.7246263027191162\n",
      "Epoch 1199, Loss: 4.64282089471817, Final Batch Loss: 1.5782856941223145\n",
      "Epoch 1200, Loss: 3.2960095405578613, Final Batch Loss: 0.3054819107055664\n",
      "Epoch 1201, Loss: 4.61809915304184, Final Batch Loss: 1.8751790523529053\n",
      "Epoch 1202, Loss: 3.239456832408905, Final Batch Loss: 0.35252082347869873\n",
      "Epoch 1203, Loss: 4.085586369037628, Final Batch Loss: 1.2693737745285034\n",
      "Epoch 1204, Loss: 3.2305588722229004, Final Batch Loss: 0.4753890633583069\n",
      "Epoch 1205, Loss: 6.490763306617737, Final Batch Loss: 3.659895896911621\n",
      "Epoch 1206, Loss: 5.225526332855225, Final Batch Loss: 2.416675329208374\n",
      "Epoch 1207, Loss: 3.9902616143226624, Final Batch Loss: 1.1472265720367432\n",
      "Epoch 1208, Loss: 3.2128141820430756, Final Batch Loss: 0.39935609698295593\n",
      "Epoch 1209, Loss: 4.674473404884338, Final Batch Loss: 1.7806696891784668\n",
      "Epoch 1210, Loss: 3.1009199917316437, Final Batch Loss: 0.2945050299167633\n",
      "Epoch 1211, Loss: 3.521649956703186, Final Batch Loss: 0.7695525884628296\n",
      "Epoch 1212, Loss: 2.944132875651121, Final Batch Loss: 0.02209300920367241\n",
      "Epoch 1213, Loss: 4.870477676391602, Final Batch Loss: 2.0217132568359375\n",
      "Epoch 1214, Loss: 2.8065624134615064, Final Batch Loss: 0.014479209668934345\n",
      "Epoch 1215, Loss: 3.259826570749283, Final Batch Loss: 0.4116578996181488\n",
      "Epoch 1216, Loss: 4.096277475357056, Final Batch Loss: 1.3546732664108276\n",
      "Epoch 1217, Loss: 2.8082222379744053, Final Batch Loss: 0.03169173374772072\n",
      "Epoch 1218, Loss: 3.0096757411956787, Final Batch Loss: 0.2747705578804016\n",
      "Epoch 1219, Loss: 2.977322995662689, Final Batch Loss: 0.26577913761138916\n",
      "Epoch 1220, Loss: 2.867854841053486, Final Batch Loss: 0.06677693873643875\n",
      "Epoch 1221, Loss: 4.094626367092133, Final Batch Loss: 1.4041129350662231\n",
      "Epoch 1222, Loss: 2.8210309520363808, Final Batch Loss: 0.0908411517739296\n",
      "Epoch 1223, Loss: 3.150273770093918, Final Batch Loss: 0.3786875903606415\n",
      "Epoch 1224, Loss: 2.7644918262958527, Final Batch Loss: 0.030075639486312866\n",
      "Epoch 1225, Loss: 4.062223553657532, Final Batch Loss: 1.2562428712844849\n",
      "Epoch 1226, Loss: 2.8449538126587868, Final Batch Loss: 0.09351644665002823\n",
      "Epoch 1227, Loss: 2.728310115635395, Final Batch Loss: 0.04103261977434158\n",
      "Epoch 1228, Loss: 2.7174985399469733, Final Batch Loss: 0.012442111037671566\n",
      "Epoch 1229, Loss: 2.8245457261800766, Final Batch Loss: 0.22895966470241547\n",
      "Epoch 1230, Loss: 3.240740865468979, Final Batch Loss: 0.4837338626384735\n",
      "Epoch 1231, Loss: 3.057170122861862, Final Batch Loss: 0.3330064117908478\n",
      "Epoch 1232, Loss: 3.592191994190216, Final Batch Loss: 0.8588486909866333\n",
      "Epoch 1233, Loss: 2.6666168713127263, Final Batch Loss: 0.0009616755996830761\n",
      "Epoch 1234, Loss: 2.8371218889951706, Final Batch Loss: 0.09565888345241547\n",
      "Epoch 1235, Loss: 2.7751005617901683, Final Batch Loss: 0.011248859576880932\n",
      "Epoch 1236, Loss: 2.919915944337845, Final Batch Loss: 0.20333519577980042\n",
      "Epoch 1237, Loss: 3.946618139743805, Final Batch Loss: 1.2211952209472656\n",
      "Epoch 1238, Loss: 3.373193681240082, Final Batch Loss: 0.6082009077072144\n",
      "Epoch 1239, Loss: 3.949791193008423, Final Batch Loss: 1.2133722305297852\n",
      "Epoch 1240, Loss: 4.331563651561737, Final Batch Loss: 1.5976215600967407\n",
      "Epoch 1241, Loss: 5.2827542424201965, Final Batch Loss: 2.6322884559631348\n",
      "Epoch 1242, Loss: 5.652972161769867, Final Batch Loss: 2.9433443546295166\n",
      "Epoch 1243, Loss: 3.938891351222992, Final Batch Loss: 1.2830538749694824\n",
      "Epoch 1244, Loss: 3.9038808345794678, Final Batch Loss: 1.2537579536437988\n",
      "Epoch 1245, Loss: 2.8424775525927544, Final Batch Loss: 0.03156041353940964\n",
      "Epoch 1246, Loss: 4.064261496067047, Final Batch Loss: 1.171972632408142\n",
      "Epoch 1247, Loss: 4.426714062690735, Final Batch Loss: 1.6226913928985596\n",
      "Epoch 1248, Loss: 4.725204288959503, Final Batch Loss: 1.8823820352554321\n",
      "Epoch 1249, Loss: 3.2236612141132355, Final Batch Loss: 0.3775083124637604\n",
      "Epoch 1250, Loss: 2.8188329748809338, Final Batch Loss: 0.005629163235425949\n",
      "Epoch 1251, Loss: 3.0065391659736633, Final Batch Loss: 0.13286197185516357\n",
      "Epoch 1252, Loss: 2.915904715657234, Final Batch Loss: 0.10703243315219879\n",
      "Epoch 1253, Loss: 4.690398275852203, Final Batch Loss: 1.9659433364868164\n",
      "Epoch 1254, Loss: 3.1355799436569214, Final Batch Loss: 0.4145064949989319\n",
      "Epoch 1255, Loss: 2.8939207941293716, Final Batch Loss: 0.15032561123371124\n",
      "Epoch 1256, Loss: 2.853225499391556, Final Batch Loss: 0.14515015482902527\n",
      "Epoch 1257, Loss: 4.588745713233948, Final Batch Loss: 1.8325388431549072\n",
      "Epoch 1258, Loss: 2.9025207981467247, Final Batch Loss: 0.09105851501226425\n",
      "Epoch 1259, Loss: 4.260414004325867, Final Batch Loss: 1.4846551418304443\n",
      "Epoch 1260, Loss: 2.7919865902513266, Final Batch Loss: 0.010019248351454735\n",
      "Epoch 1261, Loss: 4.839002847671509, Final Batch Loss: 2.16117525100708\n",
      "Epoch 1262, Loss: 2.7775541245937347, Final Batch Loss: 0.13273045420646667\n",
      "Epoch 1263, Loss: 4.080146551132202, Final Batch Loss: 1.2908854484558105\n",
      "Epoch 1264, Loss: 2.8029488064348698, Final Batch Loss: 0.04610726609826088\n",
      "Epoch 1265, Loss: 3.7655245661735535, Final Batch Loss: 1.0540554523468018\n",
      "Epoch 1266, Loss: 4.904114127159119, Final Batch Loss: 2.1368846893310547\n",
      "Epoch 1267, Loss: 2.7116028629243374, Final Batch Loss: 0.03878581151366234\n",
      "Epoch 1268, Loss: 3.007753610610962, Final Batch Loss: 0.279579222202301\n",
      "Epoch 1269, Loss: 3.6677419543266296, Final Batch Loss: 0.9772597551345825\n",
      "Epoch 1270, Loss: 3.838048219680786, Final Batch Loss: 1.1019713878631592\n",
      "Epoch 1271, Loss: 4.3970577120780945, Final Batch Loss: 1.7818127870559692\n",
      "Epoch 1272, Loss: 3.7782124876976013, Final Batch Loss: 1.2048125267028809\n",
      "Epoch 1273, Loss: 2.8218999803066254, Final Batch Loss: 0.1266135275363922\n",
      "Epoch 1274, Loss: 2.7602052204310894, Final Batch Loss: 0.04177696630358696\n",
      "Epoch 1275, Loss: 3.0752686262130737, Final Batch Loss: 0.25570112466812134\n",
      "Epoch 1276, Loss: 2.881662830710411, Final Batch Loss: 0.17278091609477997\n",
      "Epoch 1277, Loss: 6.201046645641327, Final Batch Loss: 3.493527412414551\n",
      "Epoch 1278, Loss: 3.2099439799785614, Final Batch Loss: 0.40071120858192444\n",
      "Epoch 1279, Loss: 3.0661701699718833, Final Batch Loss: 0.011469616554677486\n",
      "Epoch 1280, Loss: 5.618464946746826, Final Batch Loss: 2.557492256164551\n",
      "Epoch 1281, Loss: 4.650778532028198, Final Batch Loss: 1.7293822765350342\n",
      "Epoch 1282, Loss: 3.0471329540014267, Final Batch Loss: 0.21802584826946259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1283, Loss: 2.9088902783114463, Final Batch Loss: 0.002222926588729024\n",
      "Epoch 1284, Loss: 4.4868916273117065, Final Batch Loss: 1.6214197874069214\n",
      "Epoch 1285, Loss: 3.5550652146339417, Final Batch Loss: 0.7129113674163818\n",
      "Epoch 1286, Loss: 5.253804862499237, Final Batch Loss: 2.448883056640625\n",
      "Epoch 1287, Loss: 4.41400671005249, Final Batch Loss: 1.5070863962173462\n",
      "Epoch 1288, Loss: 5.087161183357239, Final Batch Loss: 2.18269681930542\n",
      "Epoch 1289, Loss: 2.8676480278372765, Final Batch Loss: 0.11225267499685287\n",
      "Epoch 1290, Loss: 3.8520655035972595, Final Batch Loss: 1.036410927772522\n",
      "Epoch 1291, Loss: 3.0948246717453003, Final Batch Loss: 0.14448976516723633\n",
      "Epoch 1292, Loss: 2.8534410186111927, Final Batch Loss: 0.01905002072453499\n",
      "Epoch 1293, Loss: 4.328755021095276, Final Batch Loss: 1.5945453643798828\n",
      "Epoch 1294, Loss: 3.345667004585266, Final Batch Loss: 0.5445407629013062\n",
      "Epoch 1295, Loss: 3.9584280252456665, Final Batch Loss: 1.214902639389038\n",
      "Epoch 1296, Loss: 2.8557824101299047, Final Batch Loss: 0.018158039078116417\n",
      "Epoch 1297, Loss: 3.368431329727173, Final Batch Loss: 0.6731846332550049\n",
      "Epoch 1298, Loss: 4.515326976776123, Final Batch Loss: 1.7688239812850952\n",
      "Epoch 1299, Loss: 5.182409346103668, Final Batch Loss: 2.4517452716827393\n",
      "Epoch 1300, Loss: 5.577213764190674, Final Batch Loss: 2.767038106918335\n",
      "Epoch 1301, Loss: 4.941277980804443, Final Batch Loss: 2.1624739170074463\n",
      "Epoch 1302, Loss: 2.765039735008031, Final Batch Loss: 0.0024188091047108173\n",
      "Epoch 1303, Loss: 5.167815148830414, Final Batch Loss: 2.279467821121216\n",
      "Epoch 1304, Loss: 4.0731571316719055, Final Batch Loss: 1.1912130117416382\n",
      "Epoch 1305, Loss: 3.8345210552215576, Final Batch Loss: 0.8900065422058105\n",
      "Epoch 1306, Loss: 3.0201000720262527, Final Batch Loss: 0.05400671064853668\n",
      "Epoch 1307, Loss: 3.777527630329132, Final Batch Loss: 0.8793087005615234\n",
      "Epoch 1308, Loss: 2.8882850650697947, Final Batch Loss: 0.007197404280304909\n",
      "Epoch 1309, Loss: 2.843842837959528, Final Batch Loss: 0.030341777950525284\n",
      "Epoch 1310, Loss: 4.386227011680603, Final Batch Loss: 1.5208135843276978\n",
      "Epoch 1311, Loss: 4.200471878051758, Final Batch Loss: 1.4070075750350952\n",
      "Epoch 1312, Loss: 4.714467406272888, Final Batch Loss: 1.7596335411071777\n",
      "Epoch 1313, Loss: 4.362011730670929, Final Batch Loss: 1.5997257232666016\n",
      "Epoch 1314, Loss: 3.7462605834007263, Final Batch Loss: 0.9341123700141907\n",
      "Epoch 1315, Loss: 4.480142652988434, Final Batch Loss: 1.5887731313705444\n",
      "Epoch 1316, Loss: 3.0403998121619225, Final Batch Loss: 0.04179205745458603\n",
      "Epoch 1317, Loss: 5.582076191902161, Final Batch Loss: 2.1399660110473633\n",
      "Epoch 1318, Loss: 3.849267899990082, Final Batch Loss: 0.6771484017372131\n",
      "Epoch 1319, Loss: 3.5741671323776245, Final Batch Loss: 0.743482768535614\n",
      "Epoch 1320, Loss: 3.3773787021636963, Final Batch Loss: 0.5377470850944519\n",
      "Epoch 1321, Loss: 3.4011534452438354, Final Batch Loss: 0.5662520527839661\n",
      "Epoch 1322, Loss: 2.785855920985341, Final Batch Loss: 0.015275748446583748\n",
      "Epoch 1323, Loss: 6.614564478397369, Final Batch Loss: 3.9371070861816406\n",
      "Epoch 1324, Loss: 3.046740800142288, Final Batch Loss: 0.3456108868122101\n",
      "Epoch 1325, Loss: 4.205766677856445, Final Batch Loss: 1.4016624689102173\n",
      "Epoch 1326, Loss: 4.045806705951691, Final Batch Loss: 1.253137469291687\n",
      "Epoch 1327, Loss: 4.032884299755096, Final Batch Loss: 1.1340712308883667\n",
      "Epoch 1328, Loss: 2.866391876275884, Final Batch Loss: 0.00020454221521504223\n",
      "Epoch 1329, Loss: 3.300488382577896, Final Batch Loss: 0.4699999988079071\n",
      "Epoch 1330, Loss: 2.8202976547181606, Final Batch Loss: 0.03638773784041405\n",
      "Epoch 1331, Loss: 3.080644726753235, Final Batch Loss: 0.28799712657928467\n",
      "Epoch 1332, Loss: 4.486420273780823, Final Batch Loss: 1.7794280052185059\n",
      "Epoch 1333, Loss: 4.85797506570816, Final Batch Loss: 2.109013080596924\n",
      "Epoch 1334, Loss: 3.9322001338005066, Final Batch Loss: 1.2818957567214966\n",
      "Epoch 1335, Loss: 3.8468297123908997, Final Batch Loss: 1.1392288208007812\n",
      "Epoch 1336, Loss: 3.9187674522399902, Final Batch Loss: 1.0858855247497559\n",
      "Epoch 1337, Loss: 3.797239601612091, Final Batch Loss: 0.9816765785217285\n",
      "Epoch 1338, Loss: 2.8458778164349496, Final Batch Loss: 0.0033128163777291775\n",
      "Epoch 1339, Loss: 4.991744577884674, Final Batch Loss: 2.145329713821411\n",
      "Epoch 1340, Loss: 3.6018753051757812, Final Batch Loss: 0.7862190008163452\n",
      "Epoch 1341, Loss: 3.3496389389038086, Final Batch Loss: 0.5756588578224182\n",
      "Epoch 1342, Loss: 2.965105976909399, Final Batch Loss: 0.0317058227956295\n",
      "Epoch 1343, Loss: 4.351435542106628, Final Batch Loss: 1.5471693277359009\n",
      "Epoch 1344, Loss: 4.223544180393219, Final Batch Loss: 1.5213379859924316\n",
      "Epoch 1345, Loss: 4.204074203968048, Final Batch Loss: 1.5878345966339111\n",
      "Epoch 1346, Loss: 2.662069265730679, Final Batch Loss: 0.002039615996181965\n",
      "Epoch 1347, Loss: 4.116765856742859, Final Batch Loss: 1.422166109085083\n",
      "Epoch 1348, Loss: 4.733714282512665, Final Batch Loss: 2.000359535217285\n",
      "Epoch 1349, Loss: 4.161014378070831, Final Batch Loss: 1.5570287704467773\n",
      "Epoch 1350, Loss: 2.72122564149322, Final Batch Loss: 0.0007426364463753998\n",
      "Epoch 1351, Loss: 4.830846786499023, Final Batch Loss: 2.165271043777466\n",
      "Epoch 1352, Loss: 3.3833807706832886, Final Batch Loss: 0.6564893126487732\n",
      "Epoch 1353, Loss: 4.549731016159058, Final Batch Loss: 1.880313515663147\n",
      "Epoch 1354, Loss: 5.250880539417267, Final Batch Loss: 2.482189655303955\n",
      "Epoch 1355, Loss: 2.8454035967588425, Final Batch Loss: 0.07421217858791351\n",
      "Epoch 1356, Loss: 2.9513588841073215, Final Batch Loss: 0.003956824075430632\n",
      "Epoch 1357, Loss: 3.1354051120579243, Final Batch Loss: 0.03527866676449776\n",
      "Epoch 1358, Loss: 3.4419984221458435, Final Batch Loss: 0.567243754863739\n",
      "Epoch 1359, Loss: 3.2297482192516327, Final Batch Loss: 0.3585973083972931\n",
      "Epoch 1360, Loss: 2.82822822034359, Final Batch Loss: 0.08218972384929657\n",
      "Epoch 1361, Loss: 5.649603366851807, Final Batch Loss: 2.995936393737793\n",
      "Epoch 1362, Loss: 2.89712880179286, Final Batch Loss: 0.02461431547999382\n",
      "Epoch 1363, Loss: 4.647144258022308, Final Batch Loss: 1.4057528972625732\n",
      "Epoch 1364, Loss: 3.5865237712860107, Final Batch Loss: 0.3037220239639282\n",
      "Epoch 1365, Loss: 4.541293442249298, Final Batch Loss: 1.402510643005371\n",
      "Epoch 1366, Loss: 4.207797467708588, Final Batch Loss: 1.2649610042572021\n",
      "Epoch 1367, Loss: 3.0471596717834473, Final Batch Loss: 0.10575079917907715\n",
      "Epoch 1368, Loss: 4.0332624316215515, Final Batch Loss: 1.1755343675613403\n",
      "Epoch 1369, Loss: 3.206986278295517, Final Batch Loss: 0.3404703438282013\n",
      "Epoch 1370, Loss: 3.3460997343063354, Final Batch Loss: 0.5528361201286316\n",
      "Epoch 1371, Loss: 3.010304629802704, Final Batch Loss: 0.29937124252319336\n",
      "Epoch 1372, Loss: 2.893014296889305, Final Batch Loss: 0.2162971943616867\n",
      "Epoch 1373, Loss: 3.2074931263923645, Final Batch Loss: 0.4568323493003845\n",
      "Epoch 1374, Loss: 3.630875885486603, Final Batch Loss: 0.9003064632415771\n",
      "Epoch 1375, Loss: 5.051380217075348, Final Batch Loss: 2.2721736431121826\n",
      "Epoch 1376, Loss: 3.361332356929779, Final Batch Loss: 0.7282335162162781\n",
      "Epoch 1377, Loss: 3.2118040919303894, Final Batch Loss: 0.6369168162345886\n",
      "Epoch 1378, Loss: 3.7057804465293884, Final Batch Loss: 1.0033612251281738\n",
      "Epoch 1379, Loss: 2.9166546165943146, Final Batch Loss: 0.220965176820755\n",
      "Epoch 1380, Loss: 2.874825730919838, Final Batch Loss: 0.15174193680286407\n",
      "Epoch 1381, Loss: 2.681999694555998, Final Batch Loss: 0.021770428866147995\n",
      "Epoch 1382, Loss: 2.803320661187172, Final Batch Loss: 0.1045227199792862\n",
      "Epoch 1383, Loss: 3.8445786237716675, Final Batch Loss: 1.163665771484375\n",
      "Epoch 1384, Loss: 2.9832839369773865, Final Batch Loss: 0.3462331295013428\n",
      "Epoch 1385, Loss: 4.471181929111481, Final Batch Loss: 1.8179371356964111\n",
      "Epoch 1386, Loss: 4.305260002613068, Final Batch Loss: 1.6791199445724487\n",
      "Epoch 1387, Loss: 4.299453377723694, Final Batch Loss: 1.7389177083969116\n",
      "Epoch 1388, Loss: 2.742274584248662, Final Batch Loss: 0.022716881707310677\n",
      "Epoch 1389, Loss: 3.8143356442451477, Final Batch Loss: 1.2680410146713257\n",
      "Epoch 1390, Loss: 2.6933717485517263, Final Batch Loss: 0.016005253419280052\n",
      "Epoch 1391, Loss: 3.558767855167389, Final Batch Loss: 0.9376636147499084\n",
      "Epoch 1392, Loss: 3.424092710018158, Final Batch Loss: 0.6544208526611328\n",
      "Epoch 1393, Loss: 4.308647096157074, Final Batch Loss: 1.130875587463379\n",
      "Epoch 1394, Loss: 5.698601245880127, Final Batch Loss: 2.563272714614868\n",
      "Epoch 1395, Loss: 4.229817092418671, Final Batch Loss: 1.2502763271331787\n",
      "Epoch 1396, Loss: 3.01076877117157, Final Batch Loss: 0.14461129903793335\n",
      "Epoch 1397, Loss: 4.035211086273193, Final Batch Loss: 1.3872748613357544\n",
      "Epoch 1398, Loss: 3.049023985862732, Final Batch Loss: 0.40921252965927124\n",
      "Epoch 1399, Loss: 3.083467423915863, Final Batch Loss: 0.35231631994247437\n",
      "Epoch 1400, Loss: 2.8793508410453796, Final Batch Loss: 0.29894381761550903\n",
      "Epoch 1401, Loss: 4.822629392147064, Final Batch Loss: 2.1064186096191406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1402, Loss: 2.681035164743662, Final Batch Loss: 0.04632717743515968\n",
      "Epoch 1403, Loss: 4.889346361160278, Final Batch Loss: 2.188337802886963\n",
      "Epoch 1404, Loss: 3.352510929107666, Final Batch Loss: 0.7489961981773376\n",
      "Epoch 1405, Loss: 2.7941877990961075, Final Batch Loss: 0.2168930321931839\n",
      "Epoch 1406, Loss: 2.6138235442340374, Final Batch Loss: 0.035667549818754196\n",
      "Epoch 1407, Loss: 4.29113644361496, Final Batch Loss: 1.7236878871917725\n",
      "Epoch 1408, Loss: 3.529057204723358, Final Batch Loss: 0.9812568426132202\n",
      "Epoch 1409, Loss: 3.378867506980896, Final Batch Loss: 0.7621346712112427\n",
      "Epoch 1410, Loss: 3.9888749718666077, Final Batch Loss: 1.3999443054199219\n",
      "Epoch 1411, Loss: 4.786737263202667, Final Batch Loss: 1.9536340236663818\n",
      "Epoch 1412, Loss: 3.523381769657135, Final Batch Loss: 0.8063194155693054\n",
      "Epoch 1413, Loss: 3.6373296976089478, Final Batch Loss: 0.8493200540542603\n",
      "Epoch 1414, Loss: 3.8284061551094055, Final Batch Loss: 1.116832971572876\n",
      "Epoch 1415, Loss: 2.7498881202191114, Final Batch Loss: 0.02848183922469616\n",
      "Epoch 1416, Loss: 2.87538743019104, Final Batch Loss: 0.1322762370109558\n",
      "Epoch 1417, Loss: 3.5607927441596985, Final Batch Loss: 0.678730845451355\n",
      "Epoch 1418, Loss: 4.328619003295898, Final Batch Loss: 1.5654370784759521\n",
      "Epoch 1419, Loss: 4.2091264724731445, Final Batch Loss: 1.551181435585022\n",
      "Epoch 1420, Loss: 3.9257046580314636, Final Batch Loss: 1.224179744720459\n",
      "Epoch 1421, Loss: 4.440591752529144, Final Batch Loss: 1.7061505317687988\n",
      "Epoch 1422, Loss: 4.763684630393982, Final Batch Loss: 2.017603874206543\n",
      "Epoch 1423, Loss: 2.9553150543943048, Final Batch Loss: 0.013466001488268375\n",
      "Epoch 1424, Loss: 4.686794102191925, Final Batch Loss: 1.6999413967132568\n",
      "Epoch 1425, Loss: 2.7533971685916185, Final Batch Loss: 0.004576565697789192\n",
      "Epoch 1426, Loss: 3.879899561405182, Final Batch Loss: 1.1256482601165771\n",
      "Epoch 1427, Loss: 3.4874303340911865, Final Batch Loss: 0.7879015803337097\n",
      "Epoch 1428, Loss: 2.770558461546898, Final Batch Loss: 0.09140901267528534\n",
      "Epoch 1429, Loss: 2.917463332414627, Final Batch Loss: 0.13754257559776306\n",
      "Epoch 1430, Loss: 4.164422810077667, Final Batch Loss: 1.4765870571136475\n",
      "Epoch 1431, Loss: 3.0462814271450043, Final Batch Loss: 0.36204108595848083\n",
      "Epoch 1432, Loss: 2.676610618829727, Final Batch Loss: 0.10907420516014099\n",
      "Epoch 1433, Loss: 4.555448055267334, Final Batch Loss: 1.980993390083313\n",
      "Epoch 1434, Loss: 4.385262906551361, Final Batch Loss: 1.574674129486084\n",
      "Epoch 1435, Loss: 2.973435241729021, Final Batch Loss: 0.04968186095356941\n",
      "Epoch 1436, Loss: 4.685449481010437, Final Batch Loss: 1.5471335649490356\n",
      "Epoch 1437, Loss: 3.7760912775993347, Final Batch Loss: 0.6969783902168274\n",
      "Epoch 1438, Loss: 3.9593504071235657, Final Batch Loss: 1.0936294794082642\n",
      "Epoch 1439, Loss: 4.17901486158371, Final Batch Loss: 1.3733227252960205\n",
      "Epoch 1440, Loss: 4.949993193149567, Final Batch Loss: 2.145702362060547\n",
      "Epoch 1441, Loss: 3.0892189517326187, Final Batch Loss: 0.0002115741081070155\n",
      "Epoch 1442, Loss: 5.335864901542664, Final Batch Loss: 2.1398119926452637\n",
      "Epoch 1443, Loss: 4.6920759081840515, Final Batch Loss: 1.6919662952423096\n",
      "Epoch 1444, Loss: 2.9485179260373116, Final Batch Loss: 0.05740601569414139\n",
      "Epoch 1445, Loss: 5.07424932718277, Final Batch Loss: 2.4072704315185547\n",
      "Epoch 1446, Loss: 4.408899545669556, Final Batch Loss: 1.7646907567977905\n",
      "Epoch 1447, Loss: 2.918262742459774, Final Batch Loss: 0.06461646407842636\n",
      "Epoch 1448, Loss: 4.748616814613342, Final Batch Loss: 1.7995631694793701\n",
      "Epoch 1449, Loss: 4.4736409187316895, Final Batch Loss: 1.6846857070922852\n",
      "Epoch 1450, Loss: 2.7957199485972524, Final Batch Loss: 0.013977495022118092\n",
      "Epoch 1451, Loss: 4.605324864387512, Final Batch Loss: 1.8640639781951904\n",
      "Epoch 1452, Loss: 4.979421496391296, Final Batch Loss: 2.268080472946167\n",
      "Epoch 1453, Loss: 2.9963015019893646, Final Batch Loss: 0.31461259722709656\n",
      "Epoch 1454, Loss: 2.7626682221889496, Final Batch Loss: 0.1886906921863556\n",
      "Epoch 1455, Loss: 4.0074620842933655, Final Batch Loss: 1.331317663192749\n",
      "Epoch 1456, Loss: 4.481818437576294, Final Batch Loss: 1.7742539644241333\n",
      "Epoch 1457, Loss: 2.623749088961631, Final Batch Loss: 0.002411079127341509\n",
      "Epoch 1458, Loss: 2.6226534424349666, Final Batch Loss: 0.014888363890349865\n",
      "Epoch 1459, Loss: 3.0451216101646423, Final Batch Loss: 0.3926593065261841\n",
      "Epoch 1460, Loss: 3.721760928630829, Final Batch Loss: 1.1601027250289917\n",
      "Epoch 1461, Loss: 2.716065138578415, Final Batch Loss: 0.16683432459831238\n",
      "Epoch 1462, Loss: 3.7385054230690002, Final Batch Loss: 1.149813175201416\n",
      "Epoch 1463, Loss: 2.7897294759750366, Final Batch Loss: 0.17264771461486816\n",
      "Epoch 1464, Loss: 4.235055029392242, Final Batch Loss: 1.6666231155395508\n",
      "Epoch 1465, Loss: 3.4831029772758484, Final Batch Loss: 0.7981703877449036\n",
      "Epoch 1466, Loss: 4.392195165157318, Final Batch Loss: 1.9051152467727661\n",
      "Epoch 1467, Loss: 2.8927354216575623, Final Batch Loss: 0.28616559505462646\n",
      "Epoch 1468, Loss: 4.570790231227875, Final Batch Loss: 1.956397533416748\n",
      "Epoch 1469, Loss: 3.380668342113495, Final Batch Loss: 0.770767092704773\n",
      "Epoch 1470, Loss: 2.5668007358908653, Final Batch Loss: 0.009777866303920746\n",
      "Epoch 1471, Loss: 3.8082156777381897, Final Batch Loss: 1.2115753889083862\n",
      "Epoch 1472, Loss: 3.3368489742279053, Final Batch Loss: 0.81470787525177\n",
      "Epoch 1473, Loss: 4.001779198646545, Final Batch Loss: 1.4463722705841064\n",
      "Epoch 1474, Loss: 3.010878950357437, Final Batch Loss: 0.4605421721935272\n",
      "Epoch 1475, Loss: 3.972971200942993, Final Batch Loss: 1.382366418838501\n",
      "Epoch 1476, Loss: 2.7811029702425003, Final Batch Loss: 0.2424636036157608\n",
      "Epoch 1477, Loss: 3.6796464920043945, Final Batch Loss: 1.095677137374878\n",
      "Epoch 1478, Loss: 2.6582056283950806, Final Batch Loss: 0.18210053443908691\n",
      "Epoch 1479, Loss: 3.1963905692100525, Final Batch Loss: 0.5795988440513611\n",
      "Epoch 1480, Loss: 4.334291756153107, Final Batch Loss: 1.761049747467041\n",
      "Epoch 1481, Loss: 2.937826484441757, Final Batch Loss: 0.3469075858592987\n",
      "Epoch 1482, Loss: 2.9602730572223663, Final Batch Loss: 0.42048928141593933\n",
      "Epoch 1483, Loss: 3.9561902284622192, Final Batch Loss: 1.3782235383987427\n",
      "Epoch 1484, Loss: 4.106138646602631, Final Batch Loss: 1.5364124774932861\n",
      "Epoch 1485, Loss: 4.630613446235657, Final Batch Loss: 2.1329145431518555\n",
      "Epoch 1486, Loss: 2.6797986924648285, Final Batch Loss: 0.03713187575340271\n",
      "Epoch 1487, Loss: 4.172365248203278, Final Batch Loss: 1.5128852128982544\n",
      "Epoch 1488, Loss: 2.9932563602924347, Final Batch Loss: 0.4244156777858734\n",
      "Epoch 1489, Loss: 4.272059500217438, Final Batch Loss: 1.6206378936767578\n",
      "Epoch 1490, Loss: 2.7103779390454292, Final Batch Loss: 0.08975023776292801\n",
      "Epoch 1491, Loss: 5.390029430389404, Final Batch Loss: 2.675288677215576\n",
      "Epoch 1492, Loss: 2.6622064504772425, Final Batch Loss: 0.026248076930642128\n",
      "Epoch 1493, Loss: 3.839671313762665, Final Batch Loss: 1.180649995803833\n",
      "Epoch 1494, Loss: 5.268027901649475, Final Batch Loss: 2.6959972381591797\n",
      "Epoch 1495, Loss: 3.5681046843528748, Final Batch Loss: 0.8761407732963562\n",
      "Epoch 1496, Loss: 4.330947935581207, Final Batch Loss: 1.6669034957885742\n",
      "Epoch 1497, Loss: 2.7849376648664474, Final Batch Loss: 0.031597718596458435\n",
      "Epoch 1498, Loss: 2.894283100962639, Final Batch Loss: 0.14556030929088593\n",
      "Epoch 1499, Loss: 2.939184933900833, Final Batch Loss: 0.39518067240715027\n",
      "Epoch 1500, Loss: 2.643096311017871, Final Batch Loss: 0.02156488411128521\n",
      "Epoch 1501, Loss: 4.3780322670936584, Final Batch Loss: 1.6718239784240723\n",
      "Epoch 1502, Loss: 3.8142112493515015, Final Batch Loss: 1.238523006439209\n",
      "Epoch 1503, Loss: 3.927123010158539, Final Batch Loss: 1.452009916305542\n",
      "Epoch 1504, Loss: 2.647425570525229, Final Batch Loss: 0.002832331694662571\n",
      "Epoch 1505, Loss: 2.6362235583364964, Final Batch Loss: 0.05151844397187233\n",
      "Epoch 1506, Loss: 3.7843279242515564, Final Batch Loss: 1.2346999645233154\n",
      "Epoch 1507, Loss: 2.593277543783188, Final Batch Loss: 0.07635977864265442\n",
      "Epoch 1508, Loss: 3.0473785996437073, Final Batch Loss: 0.5108396410942078\n",
      "Epoch 1509, Loss: 2.661929376423359, Final Batch Loss: 0.00911373645067215\n",
      "Epoch 1510, Loss: 2.6465574391186237, Final Batch Loss: 0.057848382741212845\n",
      "Epoch 1511, Loss: 2.748672153800726, Final Batch Loss: 0.05608877167105675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1512, Loss: 2.6091169915744103, Final Batch Loss: 0.0007454953738488257\n",
      "Epoch 1513, Loss: 2.8959470987319946, Final Batch Loss: 0.3945579528808594\n",
      "Epoch 1514, Loss: 2.6652657240629196, Final Batch Loss: 0.0326530784368515\n",
      "Epoch 1515, Loss: 4.312399387359619, Final Batch Loss: 1.8062984943389893\n",
      "Epoch 1516, Loss: 3.180372655391693, Final Batch Loss: 0.6104301810264587\n",
      "Epoch 1517, Loss: 2.714957535266876, Final Batch Loss: 0.21777284145355225\n",
      "Epoch 1518, Loss: 2.5121701206080616, Final Batch Loss: 0.007398113142699003\n",
      "Epoch 1519, Loss: 2.7821247577667236, Final Batch Loss: 0.21407824754714966\n",
      "Epoch 1520, Loss: 2.6736435666680336, Final Batch Loss: 0.09142271429300308\n",
      "Epoch 1521, Loss: 2.6878811717033386, Final Batch Loss: 0.060842037200927734\n",
      "Epoch 1522, Loss: 2.8414660692214966, Final Batch Loss: 0.28169548511505127\n",
      "Epoch 1523, Loss: 3.8696495294570923, Final Batch Loss: 1.3072763681411743\n",
      "Epoch 1524, Loss: 2.566773723985534, Final Batch Loss: 0.0009008163469843566\n",
      "Epoch 1525, Loss: 5.371981263160706, Final Batch Loss: 2.8487002849578857\n",
      "Epoch 1526, Loss: 2.959137499332428, Final Batch Loss: 0.3844957947731018\n",
      "Epoch 1527, Loss: 2.9502710023516556, Final Batch Loss: 0.00012683063687290996\n",
      "Epoch 1528, Loss: 4.1505661606788635, Final Batch Loss: 0.9536675810813904\n",
      "Epoch 1529, Loss: 3.4639639258384705, Final Batch Loss: 0.27831149101257324\n",
      "Epoch 1530, Loss: 3.761024057865143, Final Batch Loss: 0.7341850399971008\n",
      "Epoch 1531, Loss: 5.5412691831588745, Final Batch Loss: 2.640376567840576\n",
      "Epoch 1532, Loss: 3.3167352378368378, Final Batch Loss: 0.4245988428592682\n",
      "Epoch 1533, Loss: 2.989407427608967, Final Batch Loss: 0.035351045429706573\n",
      "Epoch 1534, Loss: 3.7405014634132385, Final Batch Loss: 0.8231295347213745\n",
      "Epoch 1535, Loss: 5.611464619636536, Final Batch Loss: 2.941962242126465\n",
      "Epoch 1536, Loss: 4.085415542125702, Final Batch Loss: 1.3454604148864746\n",
      "Epoch 1537, Loss: 4.2420583963394165, Final Batch Loss: 1.0185519456863403\n",
      "Epoch 1538, Loss: 3.8447299003601074, Final Batch Loss: 0.3479534387588501\n",
      "Epoch 1539, Loss: 4.838932991027832, Final Batch Loss: 1.3635300397872925\n",
      "Epoch 1540, Loss: 4.150154113769531, Final Batch Loss: 0.9181058406829834\n",
      "Epoch 1541, Loss: 4.121306002140045, Final Batch Loss: 1.2102859020233154\n",
      "Epoch 1542, Loss: 5.502768099308014, Final Batch Loss: 2.8551957607269287\n",
      "Epoch 1543, Loss: 3.211098074913025, Final Batch Loss: 0.5274211764335632\n",
      "Epoch 1544, Loss: 3.669433057308197, Final Batch Loss: 0.9676450490951538\n",
      "Epoch 1545, Loss: 2.832885106327012, Final Batch Loss: 0.001073380233719945\n",
      "Epoch 1546, Loss: 3.2690504789352417, Final Batch Loss: 0.36796432733535767\n",
      "Epoch 1547, Loss: 4.923611700534821, Final Batch Loss: 1.924108862876892\n",
      "Epoch 1548, Loss: 4.901686906814575, Final Batch Loss: 2.2026705741882324\n",
      "Epoch 1549, Loss: 3.7164050936698914, Final Batch Loss: 1.1212327480316162\n",
      "Epoch 1550, Loss: 2.763784348964691, Final Batch Loss: 0.08499062061309814\n",
      "Epoch 1551, Loss: 4.0053059458732605, Final Batch Loss: 1.3713884353637695\n",
      "Epoch 1552, Loss: 3.0951027870178223, Final Batch Loss: 0.5196337699890137\n",
      "Epoch 1553, Loss: 2.7333562672138214, Final Batch Loss: 0.18545040488243103\n",
      "Epoch 1554, Loss: 3.8079229593276978, Final Batch Loss: 1.1325329542160034\n",
      "Epoch 1555, Loss: 3.2627367973327637, Final Batch Loss: 0.627651035785675\n",
      "Epoch 1556, Loss: 2.9513085782527924, Final Batch Loss: 0.4412823021411896\n",
      "Epoch 1557, Loss: 3.1999955773353577, Final Batch Loss: 0.6419907808303833\n",
      "Epoch 1558, Loss: 4.47012197971344, Final Batch Loss: 1.8632702827453613\n",
      "Epoch 1559, Loss: 3.843043029308319, Final Batch Loss: 1.0568575859069824\n",
      "Epoch 1560, Loss: 2.7147905295714736, Final Batch Loss: 0.005452997051179409\n",
      "Epoch 1561, Loss: 3.3488277196884155, Final Batch Loss: 0.5824014544487\n",
      "Epoch 1562, Loss: 2.7556764781475067, Final Batch Loss: 0.09255853295326233\n",
      "Epoch 1563, Loss: 2.7172248661518097, Final Batch Loss: 0.12213966250419617\n",
      "Epoch 1564, Loss: 3.0300351977348328, Final Batch Loss: 0.5021787881851196\n",
      "Epoch 1565, Loss: 3.6419291496276855, Final Batch Loss: 1.186301589012146\n",
      "Epoch 1566, Loss: 5.928243815898895, Final Batch Loss: 3.3556017875671387\n",
      "Epoch 1567, Loss: 2.968515157699585, Final Batch Loss: 0.35193711519241333\n",
      "Epoch 1568, Loss: 2.69749476358993, Final Batch Loss: 0.0008236353169195354\n",
      "Epoch 1569, Loss: 2.636133886873722, Final Batch Loss: 0.0056823864579200745\n",
      "Epoch 1570, Loss: 3.0729548633098602, Final Batch Loss: 0.4201570451259613\n",
      "Epoch 1571, Loss: 4.70197069644928, Final Batch Loss: 2.0625529289245605\n",
      "Epoch 1572, Loss: 2.5997063675895333, Final Batch Loss: 0.013700262643396854\n",
      "Epoch 1573, Loss: 3.1859723329544067, Final Batch Loss: 0.7323744893074036\n",
      "Epoch 1574, Loss: 4.868874371051788, Final Batch Loss: 2.4149341583251953\n",
      "Epoch 1575, Loss: 2.677383191883564, Final Batch Loss: 0.11066699773073196\n",
      "Epoch 1576, Loss: 3.644457459449768, Final Batch Loss: 1.1421771049499512\n",
      "Epoch 1577, Loss: 4.18657910823822, Final Batch Loss: 1.6187794208526611\n",
      "Epoch 1578, Loss: 2.6384076103568077, Final Batch Loss: 0.09627100080251694\n",
      "Epoch 1579, Loss: 3.7323142290115356, Final Batch Loss: 1.2681124210357666\n",
      "Epoch 1580, Loss: 4.158813536167145, Final Batch Loss: 1.6910187005996704\n",
      "Epoch 1581, Loss: 2.4942969728726894, Final Batch Loss: 0.0026490141171962023\n",
      "Epoch 1582, Loss: 3.626961827278137, Final Batch Loss: 1.2068160772323608\n",
      "Epoch 1583, Loss: 3.2349719405174255, Final Batch Loss: 0.6281046867370605\n",
      "Epoch 1584, Loss: 3.040412962436676, Final Batch Loss: 0.546635091304779\n",
      "Epoch 1585, Loss: 3.373739242553711, Final Batch Loss: 0.8172921538352966\n",
      "Epoch 1586, Loss: 2.5129097886383533, Final Batch Loss: 0.03957434371113777\n",
      "Epoch 1587, Loss: 3.5019547939300537, Final Batch Loss: 0.9454115033149719\n",
      "Epoch 1588, Loss: 3.612731695175171, Final Batch Loss: 1.1080985069274902\n",
      "Epoch 1589, Loss: 2.6221094727516174, Final Batch Loss: 0.04970329999923706\n",
      "Epoch 1590, Loss: 4.653534352779388, Final Batch Loss: 2.1440367698669434\n",
      "Epoch 1591, Loss: 2.5270485146320425, Final Batch Loss: 0.0007893307483755052\n",
      "Epoch 1592, Loss: 2.5438198633491993, Final Batch Loss: 0.057934898883104324\n",
      "Epoch 1593, Loss: 2.5900271385908127, Final Batch Loss: 0.12445931136608124\n",
      "Epoch 1594, Loss: 3.426882565021515, Final Batch Loss: 1.0024603605270386\n",
      "Epoch 1595, Loss: 2.5742518827319145, Final Batch Loss: 0.07271318882703781\n",
      "Epoch 1596, Loss: 3.190480947494507, Final Batch Loss: 0.7728739380836487\n",
      "Epoch 1597, Loss: 2.617009937763214, Final Batch Loss: 0.06559938192367554\n",
      "Epoch 1598, Loss: 2.640450656414032, Final Batch Loss: 0.16788959503173828\n",
      "Epoch 1599, Loss: 3.655808925628662, Final Batch Loss: 1.1367230415344238\n",
      "Epoch 1600, Loss: 3.754561483860016, Final Batch Loss: 1.2703371047973633\n",
      "Epoch 1601, Loss: 2.737717851996422, Final Batch Loss: 0.19858418405056\n",
      "Epoch 1602, Loss: 3.4279022812843323, Final Batch Loss: 0.980758011341095\n",
      "Epoch 1603, Loss: 3.9240760803222656, Final Batch Loss: 1.3297195434570312\n",
      "Epoch 1604, Loss: 4.427604138851166, Final Batch Loss: 1.8514220714569092\n",
      "Epoch 1605, Loss: 4.224972188472748, Final Batch Loss: 1.7134208679199219\n",
      "Epoch 1606, Loss: 3.4955915808677673, Final Batch Loss: 0.9215875864028931\n",
      "Epoch 1607, Loss: 3.804648697376251, Final Batch Loss: 1.2004976272583008\n",
      "Epoch 1608, Loss: 2.6428337013348937, Final Batch Loss: 0.012202373705804348\n",
      "Epoch 1609, Loss: 3.4781755805015564, Final Batch Loss: 0.9765543341636658\n",
      "Epoch 1610, Loss: 2.718935087323189, Final Batch Loss: 0.15568430721759796\n",
      "Epoch 1611, Loss: 3.1835902333259583, Final Batch Loss: 0.6536092162132263\n",
      "Epoch 1612, Loss: 3.969447433948517, Final Batch Loss: 1.4634044170379639\n",
      "Epoch 1613, Loss: 2.43961226567626, Final Batch Loss: 0.006285067647695541\n",
      "Epoch 1614, Loss: 2.692453034222126, Final Batch Loss: 0.10581804066896439\n",
      "Epoch 1615, Loss: 2.5692477747797966, Final Batch Loss: 0.020971111953258514\n",
      "Epoch 1616, Loss: 3.8104602098464966, Final Batch Loss: 1.2600826025009155\n",
      "Epoch 1617, Loss: 3.905797302722931, Final Batch Loss: 1.47311270236969\n",
      "Epoch 1618, Loss: 3.426633834838867, Final Batch Loss: 0.8633837103843689\n",
      "Epoch 1619, Loss: 2.5962413921952248, Final Batch Loss: 0.12384609133005142\n",
      "Epoch 1620, Loss: 2.6525468677282333, Final Batch Loss: 0.10625891387462616\n",
      "Epoch 1621, Loss: 2.51049623452127, Final Batch Loss: 0.01979384385049343\n",
      "Epoch 1622, Loss: 2.688650608062744, Final Batch Loss: 0.2809743285179138\n",
      "Epoch 1623, Loss: 2.993035674095154, Final Batch Loss: 0.617834210395813\n",
      "Epoch 1624, Loss: 5.638195812702179, Final Batch Loss: 3.2983179092407227\n",
      "Epoch 1625, Loss: 2.482194036245346, Final Batch Loss: 0.09545692801475525\n",
      "Epoch 1626, Loss: 3.0050590336322784, Final Batch Loss: 0.40545669198036194\n",
      "Epoch 1627, Loss: 3.100415974855423, Final Batch Loss: 0.3640764057636261\n",
      "Epoch 1628, Loss: 5.277373373508453, Final Batch Loss: 2.5171265602111816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1629, Loss: 3.9509111642837524, Final Batch Loss: 1.349220633506775\n",
      "Epoch 1630, Loss: 2.7199182957410812, Final Batch Loss: 0.20904777944087982\n",
      "Epoch 1631, Loss: 2.8225926510058343, Final Batch Loss: 0.0005364171229302883\n",
      "Epoch 1632, Loss: 2.9241107180714607, Final Batch Loss: 0.0847354605793953\n",
      "Epoch 1633, Loss: 4.190211713314056, Final Batch Loss: 1.538634181022644\n",
      "Epoch 1634, Loss: 2.611277110874653, Final Batch Loss: 0.08542872220277786\n",
      "Epoch 1635, Loss: 2.656666560098529, Final Batch Loss: 0.0172490905970335\n",
      "Epoch 1636, Loss: 2.7741126269102097, Final Batch Loss: 0.24104700982570648\n",
      "Epoch 1637, Loss: 2.6633377969264984, Final Batch Loss: 0.17871567606925964\n",
      "Epoch 1638, Loss: 2.8960582315921783, Final Batch Loss: 0.4037483036518097\n",
      "Epoch 1639, Loss: 3.396412670612335, Final Batch Loss: 0.976334273815155\n",
      "Epoch 1640, Loss: 3.587950646877289, Final Batch Loss: 1.0501888990402222\n",
      "Epoch 1641, Loss: 2.65231447853148, Final Batch Loss: 0.0010967198759317398\n",
      "Epoch 1642, Loss: 4.594389975070953, Final Batch Loss: 2.1089768409729004\n",
      "Epoch 1643, Loss: 4.021735727787018, Final Batch Loss: 1.5772128105163574\n",
      "Epoch 1644, Loss: 2.4564623050391674, Final Batch Loss: 0.025562148541212082\n",
      "Epoch 1645, Loss: 3.230522036552429, Final Batch Loss: 0.7131248712539673\n",
      "Epoch 1646, Loss: 2.537365562049672, Final Batch Loss: 0.0019795361440628767\n",
      "Epoch 1647, Loss: 4.4941288232803345, Final Batch Loss: 2.1672425270080566\n",
      "Epoch 1648, Loss: 2.8729602098464966, Final Batch Loss: 0.4676010012626648\n",
      "Epoch 1649, Loss: 4.290406048297882, Final Batch Loss: 1.8437726497650146\n",
      "Epoch 1650, Loss: 3.7113518714904785, Final Batch Loss: 1.3011479377746582\n",
      "Epoch 1651, Loss: 3.3597388863563538, Final Batch Loss: 0.8908460736274719\n",
      "Epoch 1652, Loss: 2.5738521367311478, Final Batch Loss: 0.03730989992618561\n",
      "Epoch 1653, Loss: 4.536618947982788, Final Batch Loss: 1.7573679685592651\n",
      "Epoch 1654, Loss: 4.445460140705109, Final Batch Loss: 1.8557891845703125\n",
      "Epoch 1655, Loss: 2.732709012925625, Final Batch Loss: 0.008477769792079926\n",
      "Epoch 1656, Loss: 3.9812352061271667, Final Batch Loss: 1.3883246183395386\n",
      "Epoch 1657, Loss: 4.447897493839264, Final Batch Loss: 1.8206783533096313\n",
      "Epoch 1658, Loss: 2.53865235298872, Final Batch Loss: 0.011650852859020233\n",
      "Epoch 1659, Loss: 2.622843101620674, Final Batch Loss: 0.03245575726032257\n",
      "Epoch 1660, Loss: 3.527834415435791, Final Batch Loss: 1.0638281106948853\n",
      "Epoch 1661, Loss: 4.4993520975112915, Final Batch Loss: 2.012875556945801\n",
      "Epoch 1662, Loss: 3.655627965927124, Final Batch Loss: 1.1555838584899902\n",
      "Epoch 1663, Loss: 4.141046345233917, Final Batch Loss: 1.733947515487671\n",
      "Epoch 1664, Loss: 4.778410911560059, Final Batch Loss: 2.254117965698242\n",
      "Epoch 1665, Loss: 6.411920130252838, Final Batch Loss: 3.9345293045043945\n",
      "Epoch 1666, Loss: 3.1924858689308167, Final Batch Loss: 0.8516454100608826\n",
      "Epoch 1667, Loss: 3.6051741242408752, Final Batch Loss: 1.1543629169464111\n",
      "Epoch 1668, Loss: 2.722422714577988, Final Batch Loss: 0.0022651508916169405\n",
      "Epoch 1669, Loss: 3.3048319816589355, Final Batch Loss: 0.5110464692115784\n",
      "Epoch 1670, Loss: 3.6682794094085693, Final Batch Loss: 1.0768524408340454\n",
      "Epoch 1671, Loss: 3.436212956905365, Final Batch Loss: 0.9575226306915283\n",
      "Epoch 1672, Loss: 2.6447064131498337, Final Batch Loss: 0.1425790637731552\n",
      "Epoch 1673, Loss: 2.717792607843876, Final Batch Loss: 0.07612869888544083\n",
      "Epoch 1674, Loss: 3.4864184856414795, Final Batch Loss: 0.9151624441146851\n",
      "Epoch 1675, Loss: 3.6733327507972717, Final Batch Loss: 1.2564325332641602\n",
      "Epoch 1676, Loss: 2.8721179962158203, Final Batch Loss: 0.38525235652923584\n",
      "Epoch 1677, Loss: 2.668074294924736, Final Batch Loss: 0.12699870765209198\n",
      "Epoch 1678, Loss: 4.440909564495087, Final Batch Loss: 2.005068063735962\n",
      "Epoch 1679, Loss: 2.828355997800827, Final Batch Loss: 0.40985777974128723\n",
      "Epoch 1680, Loss: 3.8198124766349792, Final Batch Loss: 1.278307318687439\n",
      "Epoch 1681, Loss: 4.578540503978729, Final Batch Loss: 2.073784828186035\n",
      "Epoch 1682, Loss: 2.510886059142649, Final Batch Loss: 0.01318721566349268\n",
      "Epoch 1683, Loss: 3.896473526954651, Final Batch Loss: 1.3936811685562134\n",
      "Epoch 1684, Loss: 6.4631675481796265, Final Batch Loss: 3.9300222396850586\n",
      "Epoch 1685, Loss: 3.6677539944648743, Final Batch Loss: 1.13170325756073\n",
      "Epoch 1686, Loss: 3.8251641392707825, Final Batch Loss: 1.225522518157959\n",
      "Epoch 1687, Loss: 4.9336724281311035, Final Batch Loss: 2.186245918273926\n",
      "Epoch 1688, Loss: 3.6559765934944153, Final Batch Loss: 1.0386714935302734\n",
      "Epoch 1689, Loss: 4.668218016624451, Final Batch Loss: 1.9549721479415894\n",
      "Epoch 1690, Loss: 5.491479396820068, Final Batch Loss: 2.945993423461914\n",
      "Epoch 1691, Loss: 2.7774155288934708, Final Batch Loss: 0.22267861664295197\n",
      "Epoch 1692, Loss: 2.5990111008286476, Final Batch Loss: 0.09491727501153946\n",
      "Epoch 1693, Loss: 2.4768392127007246, Final Batch Loss: 0.01362570933997631\n",
      "Epoch 1694, Loss: 2.7244741916656494, Final Batch Loss: 0.18423396348953247\n",
      "Epoch 1695, Loss: 2.596757084131241, Final Batch Loss: 0.09253093600273132\n",
      "Epoch 1696, Loss: 2.6906456276774406, Final Batch Loss: 0.12114047259092331\n",
      "Epoch 1697, Loss: 3.9767662286758423, Final Batch Loss: 1.5254825353622437\n",
      "Epoch 1698, Loss: 3.9083138704299927, Final Batch Loss: 1.443987250328064\n",
      "Epoch 1699, Loss: 3.775319278240204, Final Batch Loss: 1.2566719055175781\n",
      "Epoch 1700, Loss: 2.474217958166264, Final Batch Loss: 0.0016869375249370933\n",
      "Epoch 1701, Loss: 3.2151577472686768, Final Batch Loss: 0.7645679712295532\n",
      "Epoch 1702, Loss: 2.686278782784939, Final Batch Loss: 0.10110440105199814\n",
      "Epoch 1703, Loss: 3.917911946773529, Final Batch Loss: 1.3890928030014038\n",
      "Epoch 1704, Loss: 4.189815044403076, Final Batch Loss: 1.589512586593628\n",
      "Epoch 1705, Loss: 3.5787635445594788, Final Batch Loss: 1.0234113931655884\n",
      "Epoch 1706, Loss: 4.148770391941071, Final Batch Loss: 1.684831142425537\n",
      "Epoch 1707, Loss: 5.088082373142242, Final Batch Loss: 2.6382956504821777\n",
      "Epoch 1708, Loss: 2.4745970964431763, Final Batch Loss: 0.04456585645675659\n",
      "Epoch 1709, Loss: 2.660936936736107, Final Batch Loss: 0.10110925137996674\n",
      "Epoch 1710, Loss: 2.515776090323925, Final Batch Loss: 0.02268611639738083\n",
      "Epoch 1711, Loss: 2.8425110578536987, Final Batch Loss: 0.37457185983657837\n",
      "Epoch 1712, Loss: 2.607199117541313, Final Batch Loss: 0.11038820445537567\n",
      "Epoch 1713, Loss: 4.667734146118164, Final Batch Loss: 2.1835789680480957\n",
      "Epoch 1714, Loss: 2.496215544641018, Final Batch Loss: 0.04698070138692856\n",
      "Epoch 1715, Loss: 2.515133634209633, Final Batch Loss: 0.10826535522937775\n",
      "Epoch 1716, Loss: 3.7279855608940125, Final Batch Loss: 1.2327218055725098\n",
      "Epoch 1717, Loss: 2.477600298821926, Final Batch Loss: 0.021376334130764008\n",
      "Epoch 1718, Loss: 3.5602200627326965, Final Batch Loss: 1.0206174850463867\n",
      "Epoch 1719, Loss: 4.2953930497169495, Final Batch Loss: 1.960032343864441\n",
      "Epoch 1720, Loss: 2.4389635920524597, Final Batch Loss: 0.08413475751876831\n",
      "Epoch 1721, Loss: 3.7251782417297363, Final Batch Loss: 1.2975807189941406\n",
      "Epoch 1722, Loss: 4.945226848125458, Final Batch Loss: 2.5695688724517822\n",
      "Epoch 1723, Loss: 3.8882680535316467, Final Batch Loss: 1.418987512588501\n",
      "Epoch 1724, Loss: 8.40822947025299, Final Batch Loss: 5.871376037597656\n",
      "Epoch 1725, Loss: 2.547493327409029, Final Batch Loss: 0.005574751645326614\n",
      "Epoch 1726, Loss: 3.7429497838020325, Final Batch Loss: 1.2801254987716675\n",
      "Epoch 1727, Loss: 4.1473992466926575, Final Batch Loss: 1.6222071647644043\n",
      "Epoch 1728, Loss: 2.8264548778533936, Final Batch Loss: 0.2937853932380676\n",
      "Epoch 1729, Loss: 3.66545033454895, Final Batch Loss: 1.2135862112045288\n",
      "Epoch 1730, Loss: 4.388503849506378, Final Batch Loss: 1.8654265403747559\n",
      "Epoch 1731, Loss: 2.5977362282574177, Final Batch Loss: 0.050782788544893265\n",
      "Epoch 1732, Loss: 3.789144456386566, Final Batch Loss: 1.227023720741272\n",
      "Epoch 1733, Loss: 3.5377432703971863, Final Batch Loss: 1.041989803314209\n",
      "Epoch 1734, Loss: 3.0324113368988037, Final Batch Loss: 0.6717348694801331\n",
      "Epoch 1735, Loss: 3.377562165260315, Final Batch Loss: 0.8460391163825989\n",
      "Epoch 1736, Loss: 5.3348424434661865, Final Batch Loss: 2.953725814819336\n",
      "Epoch 1737, Loss: 2.498530089855194, Final Batch Loss: 0.010391652584075928\n",
      "Epoch 1738, Loss: 3.329323887825012, Final Batch Loss: 0.7986971139907837\n",
      "Epoch 1739, Loss: 3.6686878204345703, Final Batch Loss: 1.174057960510254\n",
      "Epoch 1740, Loss: 7.2827717661857605, Final Batch Loss: 4.788757801055908\n",
      "Epoch 1741, Loss: 2.729652225971222, Final Batch Loss: 0.13217461109161377\n",
      "Epoch 1742, Loss: 3.5551559925079346, Final Batch Loss: 1.074575662612915\n",
      "Epoch 1743, Loss: 3.350651979446411, Final Batch Loss: 0.8119103312492371\n",
      "Epoch 1744, Loss: 3.7356274724006653, Final Batch Loss: 1.1946423053741455\n",
      "Epoch 1745, Loss: 3.3713483810424805, Final Batch Loss: 0.8095102906227112\n",
      "Epoch 1746, Loss: 2.7766239643096924, Final Batch Loss: 0.3030562996864319\n",
      "Epoch 1747, Loss: 3.341543138027191, Final Batch Loss: 0.8629890084266663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1748, Loss: 2.4845414170995355, Final Batch Loss: 0.014474392868578434\n",
      "Epoch 1749, Loss: 2.5309558040462434, Final Batch Loss: 0.007378233131021261\n",
      "Epoch 1750, Loss: 2.6921309321187437, Final Batch Loss: 0.005912789609283209\n",
      "Epoch 1751, Loss: 5.1012715101242065, Final Batch Loss: 2.4417033195495605\n",
      "Epoch 1752, Loss: 2.732489843852818, Final Batch Loss: 0.007898171432316303\n",
      "Epoch 1753, Loss: 2.785509530454874, Final Batch Loss: 0.017326537519693375\n",
      "Epoch 1754, Loss: 2.76960451900959, Final Batch Loss: 0.09938998520374298\n",
      "Epoch 1755, Loss: 3.7982651591300964, Final Batch Loss: 1.1622467041015625\n",
      "Epoch 1756, Loss: 4.361031711101532, Final Batch Loss: 1.8715460300445557\n",
      "Epoch 1757, Loss: 4.352236568927765, Final Batch Loss: 1.8497833013534546\n",
      "Epoch 1758, Loss: 4.6777671575546265, Final Batch Loss: 2.270306348800659\n",
      "Epoch 1759, Loss: 2.6140687091392465, Final Batch Loss: 0.0007426364463753998\n",
      "Epoch 1760, Loss: 5.224648892879486, Final Batch Loss: 2.4170968532562256\n",
      "Epoch 1761, Loss: 3.1173566579818726, Final Batch Loss: 0.4365553855895996\n",
      "Epoch 1762, Loss: 2.885692276060581, Final Batch Loss: 0.08046630769968033\n",
      "Epoch 1763, Loss: 2.871280938386917, Final Batch Loss: 0.015112310647964478\n",
      "Epoch 1764, Loss: 2.983852069824934, Final Batch Loss: 0.04075763747096062\n",
      "Epoch 1765, Loss: 2.954740360379219, Final Batch Loss: 0.005240157246589661\n",
      "Epoch 1766, Loss: 8.749622583389282, Final Batch Loss: 6.124486923217773\n",
      "Epoch 1767, Loss: 3.7080607414245605, Final Batch Loss: 1.1672309637069702\n",
      "Epoch 1768, Loss: 3.449349105358124, Final Batch Loss: 0.841418445110321\n",
      "Epoch 1769, Loss: 4.695637404918671, Final Batch Loss: 2.177666664123535\n",
      "Epoch 1770, Loss: 3.6292514204978943, Final Batch Loss: 1.1432944536209106\n",
      "Epoch 1771, Loss: 4.689942896366119, Final Batch Loss: 2.160762071609497\n",
      "Epoch 1772, Loss: 4.036570370197296, Final Batch Loss: 1.5191864967346191\n",
      "Epoch 1773, Loss: 2.651145849376917, Final Batch Loss: 0.04602256789803505\n",
      "Epoch 1774, Loss: 2.7280237898230553, Final Batch Loss: 0.07867743819952011\n",
      "Epoch 1775, Loss: 4.272867441177368, Final Batch Loss: 1.7779948711395264\n",
      "Epoch 1776, Loss: 3.83136910200119, Final Batch Loss: 1.28719162940979\n",
      "Epoch 1777, Loss: 3.4786263704299927, Final Batch Loss: 0.9679718017578125\n",
      "Epoch 1778, Loss: 2.6532371193170547, Final Batch Loss: 0.2293349653482437\n",
      "Epoch 1779, Loss: 2.472594477236271, Final Batch Loss: 0.03139684349298477\n",
      "Epoch 1780, Loss: 3.5447083711624146, Final Batch Loss: 1.0924935340881348\n",
      "Epoch 1781, Loss: 2.813432037830353, Final Batch Loss: 0.3774006962776184\n",
      "Epoch 1782, Loss: 3.915767252445221, Final Batch Loss: 1.4688239097595215\n",
      "Epoch 1783, Loss: 2.523220669478178, Final Batch Loss: 0.02859259769320488\n",
      "Epoch 1784, Loss: 2.968601256608963, Final Batch Loss: 0.45208969712257385\n",
      "Epoch 1785, Loss: 2.579893445596099, Final Batch Loss: 0.025761878117918968\n",
      "Epoch 1786, Loss: 3.9497389793395996, Final Batch Loss: 1.4063658714294434\n",
      "Epoch 1787, Loss: 2.592872738838196, Final Batch Loss: 0.07913869619369507\n",
      "Epoch 1788, Loss: 2.5815403070300817, Final Batch Loss: 0.018158039078116417\n",
      "Epoch 1789, Loss: 2.463533179834485, Final Batch Loss: 0.021824317052960396\n",
      "Epoch 1790, Loss: 2.6061069443821907, Final Batch Loss: 0.11842507869005203\n",
      "Epoch 1791, Loss: 2.487188547849655, Final Batch Loss: 0.09279301762580872\n",
      "Epoch 1792, Loss: 2.925360828638077, Final Batch Loss: 0.4717738926410675\n",
      "Epoch 1793, Loss: 2.3355421182350256, Final Batch Loss: 0.00040928093949332833\n",
      "Epoch 1794, Loss: 3.482336640357971, Final Batch Loss: 0.9378756880760193\n",
      "Epoch 1795, Loss: 4.960640549659729, Final Batch Loss: 2.486823558807373\n",
      "Epoch 1796, Loss: 3.3908851742744446, Final Batch Loss: 0.9858720898628235\n",
      "Epoch 1797, Loss: 3.9828551411628723, Final Batch Loss: 1.5353279113769531\n",
      "Epoch 1798, Loss: 3.8385300636291504, Final Batch Loss: 1.3845765590667725\n",
      "Epoch 1799, Loss: 4.351985514163971, Final Batch Loss: 1.7380468845367432\n",
      "Epoch 1800, Loss: 2.8772240355610847, Final Batch Loss: 0.05625522881746292\n",
      "Epoch 1801, Loss: 2.935954913496971, Final Batch Loss: 0.24283955991268158\n",
      "Epoch 1802, Loss: 3.49931263923645, Final Batch Loss: 0.6767088770866394\n",
      "Epoch 1803, Loss: 4.231384336948395, Final Batch Loss: 1.6522057056427002\n",
      "Epoch 1804, Loss: 2.6394539065659046, Final Batch Loss: 0.04345931485295296\n",
      "Epoch 1805, Loss: 3.361865520477295, Final Batch Loss: 0.8702349066734314\n",
      "Epoch 1806, Loss: 3.475762724876404, Final Batch Loss: 1.0352277755737305\n",
      "Epoch 1807, Loss: 4.369175672531128, Final Batch Loss: 1.928652048110962\n",
      "Epoch 1808, Loss: 3.090465545654297, Final Batch Loss: 0.5579946637153625\n",
      "Epoch 1809, Loss: 3.3982510566711426, Final Batch Loss: 0.9427911043167114\n",
      "Epoch 1810, Loss: 2.4480951093137264, Final Batch Loss: 0.030254807323217392\n",
      "Epoch 1811, Loss: 2.9158206582069397, Final Batch Loss: 0.4832479953765869\n",
      "Epoch 1812, Loss: 4.059089660644531, Final Batch Loss: 1.549813985824585\n",
      "Epoch 1813, Loss: 2.4761009719222784, Final Batch Loss: 0.023090412840247154\n",
      "Epoch 1814, Loss: 2.7453563511371613, Final Batch Loss: 0.14676633477210999\n",
      "Epoch 1815, Loss: 2.4848501309752464, Final Batch Loss: 0.08715542405843735\n",
      "Epoch 1816, Loss: 2.799956291913986, Final Batch Loss: 0.48111405968666077\n",
      "Epoch 1817, Loss: 3.1893482208251953, Final Batch Loss: 0.7757941484451294\n",
      "Epoch 1818, Loss: 2.4432719945907593, Final Batch Loss: 0.02959442138671875\n",
      "Epoch 1819, Loss: 3.6210561394691467, Final Batch Loss: 1.2753616571426392\n",
      "Epoch 1820, Loss: 4.791336894035339, Final Batch Loss: 2.378666400909424\n",
      "Epoch 1821, Loss: 4.595968067646027, Final Batch Loss: 2.287677764892578\n",
      "Epoch 1822, Loss: 2.5399131774902344, Final Batch Loss: 0.14978677034378052\n",
      "Epoch 1823, Loss: 3.492048919200897, Final Batch Loss: 1.1110177040100098\n",
      "Epoch 1824, Loss: 2.477114051580429, Final Batch Loss: 0.17293885350227356\n",
      "Epoch 1825, Loss: 3.206909418106079, Final Batch Loss: 0.8747642040252686\n",
      "Epoch 1826, Loss: 2.5413731187582016, Final Batch Loss: 0.19496826827526093\n",
      "Epoch 1827, Loss: 4.936219751834869, Final Batch Loss: 2.5631914138793945\n",
      "Epoch 1828, Loss: 2.429067447781563, Final Batch Loss: 0.10034461319446564\n",
      "Epoch 1829, Loss: 2.4849590733647346, Final Batch Loss: 0.067600317299366\n",
      "Epoch 1830, Loss: 4.410511255264282, Final Batch Loss: 1.9124364852905273\n",
      "Epoch 1831, Loss: 3.0372510850429535, Final Batch Loss: 0.4889998733997345\n",
      "Epoch 1832, Loss: 3.198165714740753, Final Batch Loss: 0.5264260768890381\n",
      "Epoch 1833, Loss: 2.618201568722725, Final Batch Loss: 0.15305007994174957\n",
      "Epoch 1834, Loss: 3.411167860031128, Final Batch Loss: 0.8655182123184204\n",
      "Epoch 1835, Loss: 3.73317289352417, Final Batch Loss: 1.2941612005233765\n",
      "Epoch 1836, Loss: 4.328173220157623, Final Batch Loss: 1.801092505455017\n",
      "Epoch 1837, Loss: 4.275755763053894, Final Batch Loss: 1.8751922845840454\n",
      "Epoch 1838, Loss: 3.008376359939575, Final Batch Loss: 0.5901820063591003\n",
      "Epoch 1839, Loss: 3.350996732711792, Final Batch Loss: 0.9650943279266357\n",
      "Epoch 1840, Loss: 2.875966250896454, Final Batch Loss: 0.31154072284698486\n",
      "Epoch 1841, Loss: 2.4977772179991007, Final Batch Loss: 0.013847941532731056\n",
      "Epoch 1842, Loss: 3.974176049232483, Final Batch Loss: 1.5837767124176025\n",
      "Epoch 1843, Loss: 2.3961400873959064, Final Batch Loss: 0.013182509690523148\n",
      "Epoch 1844, Loss: 4.309499800205231, Final Batch Loss: 1.9276930093765259\n",
      "Epoch 1845, Loss: 2.509119614958763, Final Batch Loss: 0.1280147284269333\n",
      "Epoch 1846, Loss: 2.896741211414337, Final Batch Loss: 0.5376082062721252\n",
      "Epoch 1847, Loss: 2.313035475090146, Final Batch Loss: 0.018133806064724922\n",
      "Epoch 1848, Loss: 2.4110686518251896, Final Batch Loss: 0.05608764663338661\n",
      "Epoch 1849, Loss: 2.508199229836464, Final Batch Loss: 0.13121803104877472\n",
      "Epoch 1850, Loss: 2.413780100643635, Final Batch Loss: 0.07273346930742264\n",
      "Epoch 1851, Loss: 3.831987977027893, Final Batch Loss: 1.4398753643035889\n",
      "Epoch 1852, Loss: 2.3421725295484066, Final Batch Loss: 0.017167653888463974\n",
      "Epoch 1853, Loss: 2.6070799827575684, Final Batch Loss: 0.3096025586128235\n",
      "Epoch 1854, Loss: 3.4047802686691284, Final Batch Loss: 1.106929898262024\n",
      "Epoch 1855, Loss: 2.357816807925701, Final Batch Loss: 0.10282986611127853\n",
      "Epoch 1856, Loss: 3.329260468482971, Final Batch Loss: 1.0094897747039795\n",
      "Epoch 1857, Loss: 2.305887833237648, Final Batch Loss: 0.008084431290626526\n",
      "Epoch 1858, Loss: 4.080448269844055, Final Batch Loss: 1.8035531044006348\n",
      "Epoch 1859, Loss: 3.4009461402893066, Final Batch Loss: 1.067719578742981\n",
      "Epoch 1860, Loss: 2.3657400854863226, Final Batch Loss: 0.007583046797662973\n",
      "Epoch 1861, Loss: 2.4484490528702736, Final Batch Loss: 0.10449222475290298\n",
      "Epoch 1862, Loss: 2.674526333808899, Final Batch Loss: 0.3355066776275635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1863, Loss: 3.030349612236023, Final Batch Loss: 0.7135185599327087\n",
      "Epoch 1864, Loss: 3.536745011806488, Final Batch Loss: 1.1795728206634521\n",
      "Epoch 1865, Loss: 3.0877551436424255, Final Batch Loss: 0.6805907487869263\n",
      "Epoch 1866, Loss: 3.1277427673339844, Final Batch Loss: 0.8065305352210999\n",
      "Epoch 1867, Loss: 2.3709951043128967, Final Batch Loss: 0.002315223217010498\n",
      "Epoch 1868, Loss: 3.0191453099250793, Final Batch Loss: 0.6678485870361328\n",
      "Epoch 1869, Loss: 2.460731402039528, Final Batch Loss: 0.09939517080783844\n",
      "Epoch 1870, Loss: 3.458085060119629, Final Batch Loss: 1.1542500257492065\n",
      "Epoch 1871, Loss: 2.5911042243242264, Final Batch Loss: 0.22689540684223175\n",
      "Epoch 1872, Loss: 3.8790045976638794, Final Batch Loss: 1.4998639822006226\n",
      "Epoch 1873, Loss: 2.420949086546898, Final Batch Loss: 0.1255011409521103\n",
      "Epoch 1874, Loss: 2.3744088374078274, Final Batch Loss: 0.030652817338705063\n",
      "Epoch 1875, Loss: 3.6183246970176697, Final Batch Loss: 1.2408978939056396\n",
      "Epoch 1876, Loss: 2.3400210812687874, Final Batch Loss: 0.07593513280153275\n",
      "Epoch 1877, Loss: 2.4288017600774765, Final Batch Loss: 0.09115330874919891\n",
      "Epoch 1878, Loss: 2.2472401969134808, Final Batch Loss: 0.007223915308713913\n",
      "Epoch 1879, Loss: 3.6684446334838867, Final Batch Loss: 1.358543872833252\n",
      "Epoch 1880, Loss: 2.634790003299713, Final Batch Loss: 0.2285516858100891\n",
      "Epoch 1881, Loss: 4.218819439411163, Final Batch Loss: 1.887390375137329\n",
      "Epoch 1882, Loss: 3.219130754470825, Final Batch Loss: 0.9994545578956604\n",
      "Epoch 1883, Loss: 2.8798928260803223, Final Batch Loss: 0.6414737105369568\n",
      "Epoch 1884, Loss: 2.29116028919816, Final Batch Loss: 0.02695256844162941\n",
      "Epoch 1885, Loss: 3.268240451812744, Final Batch Loss: 0.9548832774162292\n",
      "Epoch 1886, Loss: 2.403806045651436, Final Batch Loss: 0.13155849277973175\n",
      "Epoch 1887, Loss: 4.125643074512482, Final Batch Loss: 1.8187350034713745\n",
      "Epoch 1888, Loss: 3.3921037912368774, Final Batch Loss: 1.0011996030807495\n",
      "Epoch 1889, Loss: 3.216211438179016, Final Batch Loss: 0.8109536170959473\n",
      "Epoch 1890, Loss: 5.039401352405548, Final Batch Loss: 2.712820529937744\n",
      "Epoch 1891, Loss: 2.3124359557405114, Final Batch Loss: 0.01386240217834711\n",
      "Epoch 1892, Loss: 4.667807042598724, Final Batch Loss: 2.280902862548828\n",
      "Epoch 1893, Loss: 3.30636990070343, Final Batch Loss: 0.9425057172775269\n",
      "Epoch 1894, Loss: 3.1911036372184753, Final Batch Loss: 0.8906731009483337\n",
      "Epoch 1895, Loss: 3.8330026865005493, Final Batch Loss: 1.5601511001586914\n",
      "Epoch 1896, Loss: 2.349688522517681, Final Batch Loss: 0.03395520895719528\n",
      "Epoch 1897, Loss: 2.6945067644119263, Final Batch Loss: 0.4413989186286926\n",
      "Epoch 1898, Loss: 2.5038645938038826, Final Batch Loss: 0.11966646462678909\n",
      "Epoch 1899, Loss: 2.698163151741028, Final Batch Loss: 0.28681349754333496\n",
      "Epoch 1900, Loss: 4.2318965792655945, Final Batch Loss: 1.9417951107025146\n",
      "Epoch 1901, Loss: 5.699425518512726, Final Batch Loss: 3.499424695968628\n",
      "Epoch 1902, Loss: 2.639601230621338, Final Batch Loss: 0.22320926189422607\n",
      "Epoch 1903, Loss: 2.8737471951171756, Final Batch Loss: 0.006280447356402874\n",
      "Epoch 1904, Loss: 3.3971189856529236, Final Batch Loss: 0.3273472189903259\n",
      "Epoch 1905, Loss: 5.110449433326721, Final Batch Loss: 2.1231837272644043\n",
      "Epoch 1906, Loss: 2.6758876624517143, Final Batch Loss: 0.006176668684929609\n",
      "Epoch 1907, Loss: 3.0545175671577454, Final Batch Loss: 0.5392022728919983\n",
      "Epoch 1908, Loss: 2.7920950651168823, Final Batch Loss: 0.35402780771255493\n",
      "Epoch 1909, Loss: 2.555972635746002, Final Batch Loss: 0.2075192928314209\n",
      "Epoch 1910, Loss: 2.4209349304437637, Final Batch Loss: 0.006099775433540344\n",
      "Epoch 1911, Loss: 3.8963447213172913, Final Batch Loss: 1.5508822202682495\n",
      "Epoch 1912, Loss: 2.4905464202165604, Final Batch Loss: 0.09237660467624664\n",
      "Epoch 1913, Loss: 4.188209056854248, Final Batch Loss: 1.7488957643508911\n",
      "Epoch 1914, Loss: 3.3001229763031006, Final Batch Loss: 1.020304560661316\n",
      "Epoch 1915, Loss: 3.0065753161907196, Final Batch Loss: 0.4728751480579376\n",
      "Epoch 1916, Loss: 2.773784711956978, Final Batch Loss: 0.1618604212999344\n",
      "Epoch 1917, Loss: 2.8066661655902863, Final Batch Loss: 0.05715283751487732\n",
      "Epoch 1918, Loss: 2.7739913910627365, Final Batch Loss: 0.17213712632656097\n",
      "Epoch 1919, Loss: 2.3811761178076267, Final Batch Loss: 0.013315442949533463\n",
      "Epoch 1920, Loss: 3.1877388954162598, Final Batch Loss: 0.8479480147361755\n",
      "Epoch 1921, Loss: 3.626004219055176, Final Batch Loss: 1.3484482765197754\n",
      "Epoch 1922, Loss: 3.3455971479415894, Final Batch Loss: 0.9853357672691345\n",
      "Epoch 1923, Loss: 3.582897126674652, Final Batch Loss: 1.2358312606811523\n",
      "Epoch 1924, Loss: 2.372013351880014, Final Batch Loss: 0.015128514729440212\n",
      "Epoch 1925, Loss: 2.4249932318925858, Final Batch Loss: 0.06694407761096954\n",
      "Epoch 1926, Loss: 2.442067801952362, Final Batch Loss: 0.14945542812347412\n",
      "Epoch 1927, Loss: 3.1011615991592407, Final Batch Loss: 0.6968410611152649\n",
      "Epoch 1928, Loss: 5.664486050605774, Final Batch Loss: 3.3368184566497803\n",
      "Epoch 1929, Loss: 2.455199308693409, Final Batch Loss: 0.09897620230913162\n",
      "Epoch 1930, Loss: 4.0570725202560425, Final Batch Loss: 1.6409122943878174\n",
      "Epoch 1931, Loss: 2.3161076083779335, Final Batch Loss: 0.05450240522623062\n",
      "Epoch 1932, Loss: 4.587011694908142, Final Batch Loss: 2.192730665206909\n",
      "Epoch 1933, Loss: 3.7905882596969604, Final Batch Loss: 1.391657829284668\n",
      "Epoch 1934, Loss: 2.5083330869674683, Final Batch Loss: 0.2022060751914978\n",
      "Epoch 1935, Loss: 3.7174578309059143, Final Batch Loss: 1.4206089973449707\n",
      "Epoch 1936, Loss: 4.407667458057404, Final Batch Loss: 2.0990149974823\n",
      "Epoch 1937, Loss: 2.371624119579792, Final Batch Loss: 0.07641256600618362\n",
      "Epoch 1938, Loss: 2.440043218433857, Final Batch Loss: 0.06552625447511673\n",
      "Epoch 1939, Loss: 4.422382831573486, Final Batch Loss: 2.0718324184417725\n",
      "Epoch 1940, Loss: 3.3972253799438477, Final Batch Loss: 1.027840256690979\n",
      "Epoch 1941, Loss: 3.308577239513397, Final Batch Loss: 0.9124603271484375\n",
      "Epoch 1942, Loss: 2.429249592125416, Final Batch Loss: 0.0165446475148201\n",
      "Epoch 1943, Loss: 4.425181686878204, Final Batch Loss: 2.0279974937438965\n",
      "Epoch 1944, Loss: 2.3766056336462498, Final Batch Loss: 0.0369192399084568\n",
      "Epoch 1945, Loss: 2.822766363620758, Final Batch Loss: 0.48097139596939087\n",
      "Epoch 1946, Loss: 4.048464119434357, Final Batch Loss: 1.7780840396881104\n",
      "Epoch 1947, Loss: 3.455066978931427, Final Batch Loss: 1.184774398803711\n",
      "Epoch 1948, Loss: 3.4431310892105103, Final Batch Loss: 1.1471949815750122\n",
      "Epoch 1949, Loss: 2.302581612020731, Final Batch Loss: 0.022706393152475357\n",
      "Epoch 1950, Loss: 6.877708852291107, Final Batch Loss: 4.650998115539551\n",
      "Epoch 1951, Loss: 3.770090639591217, Final Batch Loss: 1.3939281702041626\n",
      "Epoch 1952, Loss: 2.625761517614592, Final Batch Loss: 0.00047017011092975736\n",
      "Epoch 1953, Loss: 3.6537309885025024, Final Batch Loss: 1.0350464582443237\n",
      "Epoch 1954, Loss: 2.670289282221347, Final Batch Loss: 0.005518440622836351\n",
      "Epoch 1955, Loss: 3.754431903362274, Final Batch Loss: 1.2082607746124268\n",
      "Epoch 1956, Loss: 4.1615132093429565, Final Batch Loss: 1.6514151096343994\n",
      "Epoch 1957, Loss: 4.543058514595032, Final Batch Loss: 2.0399935245513916\n",
      "Epoch 1958, Loss: 2.4907264932990074, Final Batch Loss: 0.06873979419469833\n",
      "Epoch 1959, Loss: 3.5658008456230164, Final Batch Loss: 1.2500159740447998\n",
      "Epoch 1960, Loss: 3.7860751152038574, Final Batch Loss: 1.3822802305221558\n",
      "Epoch 1961, Loss: 3.455277442932129, Final Batch Loss: 0.9990209341049194\n",
      "Epoch 1962, Loss: 2.4684689611094655, Final Batch Loss: 7.73638384998776e-05\n",
      "Epoch 1963, Loss: 3.307991921901703, Final Batch Loss: 0.9287657141685486\n",
      "Epoch 1964, Loss: 3.3151987195014954, Final Batch Loss: 0.8384668827056885\n",
      "Epoch 1965, Loss: 2.5906808972358704, Final Batch Loss: 0.27864354848861694\n",
      "Epoch 1966, Loss: 2.711250811815262, Final Batch Loss: 0.46680858731269836\n",
      "Epoch 1967, Loss: 3.275603473186493, Final Batch Loss: 0.9883473515510559\n",
      "Epoch 1968, Loss: 2.3412763327360153, Final Batch Loss: 0.07084707915782928\n",
      "Epoch 1969, Loss: 2.933374524116516, Final Batch Loss: 0.624493420124054\n",
      "Epoch 1970, Loss: 2.6682856380939484, Final Batch Loss: 0.322907418012619\n",
      "Epoch 1971, Loss: 6.658579528331757, Final Batch Loss: 4.293917655944824\n",
      "Epoch 1972, Loss: 2.6830974817276, Final Batch Loss: 0.2886201739311218\n",
      "Epoch 1973, Loss: 2.6983631551265717, Final Batch Loss: 0.3072320520877838\n",
      "Epoch 1974, Loss: 4.688071548938751, Final Batch Loss: 2.2975478172302246\n",
      "Epoch 1975, Loss: 3.7799009680747986, Final Batch Loss: 1.4195106029510498\n",
      "Epoch 1976, Loss: 2.755868583917618, Final Batch Loss: 0.33510497212409973\n",
      "Epoch 1977, Loss: 3.3105032444000244, Final Batch Loss: 0.9705560207366943\n",
      "Epoch 1978, Loss: 3.980516731739044, Final Batch Loss: 1.4350372552871704\n",
      "Epoch 1979, Loss: 2.607371300458908, Final Batch Loss: 0.3155958354473114\n",
      "Epoch 1980, Loss: 2.897044360637665, Final Batch Loss: 0.5908385515213013\n",
      "Epoch 1981, Loss: 2.4254889637231827, Final Batch Loss: 0.1624576598405838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1982, Loss: 2.3253594636917114, Final Batch Loss: 0.028347313404083252\n",
      "Epoch 1983, Loss: 2.393308088183403, Final Batch Loss: 0.03424243628978729\n",
      "Epoch 1984, Loss: 2.9188942909240723, Final Batch Loss: 0.6070055365562439\n",
      "Epoch 1985, Loss: 3.5970606207847595, Final Batch Loss: 1.3230081796646118\n",
      "Epoch 1986, Loss: 2.6580488979816437, Final Batch Loss: 0.397471159696579\n",
      "Epoch 1987, Loss: 2.273606405593455, Final Batch Loss: 0.014926181174814701\n",
      "Epoch 1988, Loss: 3.3272184133529663, Final Batch Loss: 1.0774903297424316\n",
      "Epoch 1989, Loss: 2.409635230898857, Final Batch Loss: 0.2350311130285263\n",
      "Epoch 1990, Loss: 2.654497981071472, Final Batch Loss: 0.34808820486068726\n",
      "Epoch 1991, Loss: 2.3783805146813393, Final Batch Loss: 0.11024770885705948\n",
      "Epoch 1992, Loss: 2.795117199420929, Final Batch Loss: 0.5328627824783325\n",
      "Epoch 1993, Loss: 2.2880808487534523, Final Batch Loss: 0.05631539970636368\n",
      "Epoch 1994, Loss: 3.9642120599746704, Final Batch Loss: 1.6325490474700928\n",
      "Epoch 1995, Loss: 3.5550724267959595, Final Batch Loss: 1.3057650327682495\n",
      "Epoch 1996, Loss: 4.195420324802399, Final Batch Loss: 1.9204492568969727\n",
      "Epoch 1997, Loss: 3.715014934539795, Final Batch Loss: 1.4478816986083984\n",
      "Epoch 1998, Loss: 2.3851718455553055, Final Batch Loss: 0.09678275883197784\n",
      "Epoch 1999, Loss: 2.965987741947174, Final Batch Loss: 0.5674740076065063\n",
      "Epoch 2000, Loss: 3.3655970096588135, Final Batch Loss: 0.903007984161377\n",
      "Epoch 2001, Loss: 3.1448670029640198, Final Batch Loss: 0.7512761354446411\n",
      "Epoch 2002, Loss: 3.2333539724349976, Final Batch Loss: 0.750326931476593\n",
      "Epoch 2003, Loss: 2.3880790434777737, Final Batch Loss: 0.0525740347802639\n",
      "Epoch 2004, Loss: 2.706391155719757, Final Batch Loss: 0.33599305152893066\n",
      "Epoch 2005, Loss: 4.160930871963501, Final Batch Loss: 1.8789126873016357\n",
      "Epoch 2006, Loss: 4.136579632759094, Final Batch Loss: 1.8490750789642334\n",
      "Epoch 2007, Loss: 3.438538074493408, Final Batch Loss: 1.1389209032058716\n",
      "Epoch 2008, Loss: 2.9082943201065063, Final Batch Loss: 0.6557366251945496\n",
      "Epoch 2009, Loss: 2.2881801053881645, Final Batch Loss: 0.07576441019773483\n",
      "Epoch 2010, Loss: 2.3765336982905865, Final Batch Loss: 0.00926823541522026\n",
      "Epoch 2011, Loss: 3.194511353969574, Final Batch Loss: 0.9201639890670776\n",
      "Epoch 2012, Loss: 2.4308630526065826, Final Batch Loss: 0.008119314908981323\n",
      "Epoch 2013, Loss: 2.3504747152280743, Final Batch Loss: 3.099436753473128e-06\n",
      "Epoch 2014, Loss: 2.3430360946804285, Final Batch Loss: 0.022475281730294228\n",
      "Epoch 2015, Loss: 2.50297112762928, Final Batch Loss: 0.17145605385303497\n",
      "Epoch 2016, Loss: 2.5756451934576035, Final Batch Loss: 0.22515444457530975\n",
      "Epoch 2017, Loss: 2.3345762975513935, Final Batch Loss: 0.007085908204317093\n",
      "Epoch 2018, Loss: 4.0015334486961365, Final Batch Loss: 1.73236882686615\n",
      "Epoch 2019, Loss: 4.586100935935974, Final Batch Loss: 2.2597241401672363\n",
      "Epoch 2020, Loss: 2.4007315635681152, Final Batch Loss: 0.17209666967391968\n",
      "Epoch 2021, Loss: 3.3680084943771362, Final Batch Loss: 1.101483702659607\n",
      "Epoch 2022, Loss: 2.381151869893074, Final Batch Loss: 0.18437089025974274\n",
      "Epoch 2023, Loss: 3.6728625893592834, Final Batch Loss: 1.434523105621338\n",
      "Epoch 2024, Loss: 2.4600344598293304, Final Batch Loss: 0.21681049466133118\n",
      "Epoch 2025, Loss: 2.741066813468933, Final Batch Loss: 0.5389853119850159\n",
      "Epoch 2026, Loss: 2.9638919830322266, Final Batch Loss: 0.6795688271522522\n",
      "Epoch 2027, Loss: 4.40144270658493, Final Batch Loss: 2.1526241302490234\n",
      "Epoch 2028, Loss: 2.3008444565348327, Final Batch Loss: 0.0060392278246581554\n",
      "Epoch 2029, Loss: 2.3674540407955647, Final Batch Loss: 0.01803569868206978\n",
      "Epoch 2030, Loss: 3.2632404565811157, Final Batch Loss: 0.9929302930831909\n",
      "Epoch 2031, Loss: 4.521677613258362, Final Batch Loss: 2.3301048278808594\n",
      "Epoch 2032, Loss: 2.70540788769722, Final Batch Loss: 0.4368714988231659\n",
      "Epoch 2033, Loss: 2.3946915566921234, Final Batch Loss: 0.17753294110298157\n",
      "Epoch 2034, Loss: 2.2134904935956, Final Batch Loss: 0.008230932056903839\n",
      "Epoch 2035, Loss: 2.770754337310791, Final Batch Loss: 0.5040250420570374\n",
      "Epoch 2036, Loss: 3.780251681804657, Final Batch Loss: 1.4619038105010986\n",
      "Epoch 2037, Loss: 4.510156154632568, Final Batch Loss: 2.154148578643799\n",
      "Epoch 2038, Loss: 2.2417148873209953, Final Batch Loss: 0.009218864142894745\n",
      "Epoch 2039, Loss: 2.4049239456653595, Final Batch Loss: 0.10670837759971619\n",
      "Epoch 2040, Loss: 2.294152192771435, Final Batch Loss: 0.05945282429456711\n",
      "Epoch 2041, Loss: 2.8049015402793884, Final Batch Loss: 0.6124812364578247\n",
      "Epoch 2042, Loss: 2.2666023661440704, Final Batch Loss: 0.00042524831951595843\n",
      "Epoch 2043, Loss: 2.3574307933449745, Final Batch Loss: 0.07757548242807388\n",
      "Epoch 2044, Loss: 2.3658702755346894, Final Batch Loss: 0.01267955545336008\n",
      "Epoch 2045, Loss: 4.260171353816986, Final Batch Loss: 1.869201898574829\n",
      "Epoch 2046, Loss: 2.2686710180714726, Final Batch Loss: 0.014373105950653553\n",
      "Epoch 2047, Loss: 5.900576531887054, Final Batch Loss: 3.702430248260498\n",
      "Epoch 2048, Loss: 3.319425046443939, Final Batch Loss: 1.1424329280853271\n",
      "Epoch 2049, Loss: 2.32144148030784, Final Batch Loss: 0.0004700509598478675\n",
      "Epoch 2050, Loss: 3.135956585407257, Final Batch Loss: 0.830337405204773\n",
      "Epoch 2051, Loss: 2.5212066918611526, Final Batch Loss: 0.2348870187997818\n",
      "Epoch 2052, Loss: 3.053265869617462, Final Batch Loss: 0.7727764844894409\n",
      "Epoch 2053, Loss: 2.9326382875442505, Final Batch Loss: 0.7070620059967041\n",
      "Epoch 2054, Loss: 4.201943039894104, Final Batch Loss: 1.9283676147460938\n",
      "Epoch 2055, Loss: 2.475080907344818, Final Batch Loss: 0.2232566475868225\n",
      "Epoch 2056, Loss: 3.6245328783988953, Final Batch Loss: 1.4043288230895996\n",
      "Epoch 2057, Loss: 3.855627417564392, Final Batch Loss: 1.635703444480896\n",
      "Epoch 2058, Loss: 3.2682764530181885, Final Batch Loss: 1.0080349445343018\n",
      "Epoch 2059, Loss: 3.615189552307129, Final Batch Loss: 1.504281997680664\n",
      "Epoch 2060, Loss: 2.144388562068343, Final Batch Loss: 0.004549747332930565\n",
      "Epoch 2061, Loss: 4.198777854442596, Final Batch Loss: 1.9713572263717651\n",
      "Epoch 2062, Loss: 2.947522759437561, Final Batch Loss: 0.8113395571708679\n",
      "Epoch 2063, Loss: 3.8712887167930603, Final Batch Loss: 1.67424738407135\n",
      "Epoch 2064, Loss: 3.747661828994751, Final Batch Loss: 1.6058850288391113\n",
      "Epoch 2065, Loss: 2.234621360898018, Final Batch Loss: 0.044274166226387024\n",
      "Epoch 2066, Loss: 2.5782291293144226, Final Batch Loss: 0.2860434651374817\n",
      "Epoch 2067, Loss: 2.292559517780319, Final Batch Loss: 0.003548873821273446\n",
      "Epoch 2068, Loss: 2.2420512326061726, Final Batch Loss: 0.018360305577516556\n",
      "Epoch 2069, Loss: 2.6844871640205383, Final Batch Loss: 0.5014660954475403\n",
      "Epoch 2070, Loss: 2.686770439147949, Final Batch Loss: 0.4735639691352844\n",
      "Epoch 2071, Loss: 2.123592682182789, Final Batch Loss: 0.02772659808397293\n",
      "Epoch 2072, Loss: 3.217704176902771, Final Batch Loss: 0.9806427359580994\n",
      "Epoch 2073, Loss: 2.1976045155897737, Final Batch Loss: 0.0005303407087922096\n",
      "Epoch 2074, Loss: 3.892517864704132, Final Batch Loss: 1.7900322675704956\n",
      "Epoch 2075, Loss: 2.196536973118782, Final Batch Loss: 0.04272843897342682\n",
      "Epoch 2076, Loss: 3.4225844144821167, Final Batch Loss: 1.2631944417953491\n",
      "Epoch 2077, Loss: 2.8758230805397034, Final Batch Loss: 0.5809110403060913\n",
      "Epoch 2078, Loss: 2.276161214336753, Final Batch Loss: 0.013085445389151573\n",
      "Epoch 2079, Loss: 3.2018215656280518, Final Batch Loss: 0.9033051133155823\n",
      "Epoch 2080, Loss: 4.1144899725914, Final Batch Loss: 1.7363204956054688\n",
      "Epoch 2081, Loss: 3.384487807750702, Final Batch Loss: 1.0374152660369873\n",
      "Epoch 2082, Loss: 2.666799008846283, Final Batch Loss: 0.3142324686050415\n",
      "Epoch 2083, Loss: 2.675261080265045, Final Batch Loss: 0.2109026312828064\n",
      "Epoch 2084, Loss: 3.678254246711731, Final Batch Loss: 1.124462604522705\n",
      "Epoch 2085, Loss: 2.586886093020439, Final Batch Loss: 0.15811114013195038\n",
      "Epoch 2086, Loss: 3.1613431572914124, Final Batch Loss: 0.7338361740112305\n",
      "Epoch 2087, Loss: 2.5577645301818848, Final Batch Loss: 0.28121083974838257\n",
      "Epoch 2088, Loss: 2.2634792315366212, Final Batch Loss: 0.0003488647344056517\n",
      "Epoch 2089, Loss: 3.8897820115089417, Final Batch Loss: 1.6805349588394165\n",
      "Epoch 2090, Loss: 3.2235376238822937, Final Batch Loss: 1.0007705688476562\n",
      "Epoch 2091, Loss: 5.166626751422882, Final Batch Loss: 2.936830520629883\n",
      "Epoch 2092, Loss: 3.678054451942444, Final Batch Loss: 1.3906972408294678\n",
      "Epoch 2093, Loss: 2.596387803554535, Final Batch Loss: 0.2138516902923584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2094, Loss: 4.5669339299201965, Final Batch Loss: 2.168717622756958\n",
      "Epoch 2095, Loss: 3.5041102170944214, Final Batch Loss: 1.107290506362915\n",
      "Epoch 2096, Loss: 3.6158676743507385, Final Batch Loss: 1.359357476234436\n",
      "Epoch 2097, Loss: 2.899862766265869, Final Batch Loss: 0.5464723706245422\n",
      "Epoch 2098, Loss: 2.3748178482055664, Final Batch Loss: 0.1260644793510437\n",
      "Epoch 2099, Loss: 2.5817794501781464, Final Batch Loss: 0.3191203773021698\n",
      "Epoch 2100, Loss: 2.4960208535194397, Final Batch Loss: 0.30905747413635254\n",
      "Epoch 2101, Loss: 2.2572501227259636, Final Batch Loss: 0.04729991406202316\n",
      "Epoch 2102, Loss: 2.994190037250519, Final Batch Loss: 0.7991217970848083\n",
      "Epoch 2103, Loss: 3.2389906644821167, Final Batch Loss: 1.0895620584487915\n",
      "Epoch 2104, Loss: 2.296174630522728, Final Batch Loss: 0.1286228746175766\n",
      "Epoch 2105, Loss: 2.2909488156437874, Final Batch Loss: 0.11364401131868362\n",
      "Epoch 2106, Loss: 3.849146842956543, Final Batch Loss: 1.6513493061065674\n",
      "Epoch 2107, Loss: 3.6751526594161987, Final Batch Loss: 1.4228578805923462\n",
      "Epoch 2108, Loss: 2.287005551159382, Final Batch Loss: 0.07534802705049515\n",
      "Epoch 2109, Loss: 2.3211659528315067, Final Batch Loss: 0.03194150701165199\n",
      "Epoch 2110, Loss: 5.008522927761078, Final Batch Loss: 2.798034906387329\n",
      "Epoch 2111, Loss: 2.174394081812352, Final Batch Loss: 0.00755346892401576\n",
      "Epoch 2112, Loss: 3.06882506608963, Final Batch Loss: 0.8004466891288757\n",
      "Epoch 2113, Loss: 4.342390060424805, Final Batch Loss: 2.1356334686279297\n",
      "Epoch 2114, Loss: 2.235052129253745, Final Batch Loss: 0.007852872833609581\n",
      "Epoch 2115, Loss: 3.8345829844474792, Final Batch Loss: 1.5894855260849\n",
      "Epoch 2116, Loss: 2.4198002368211746, Final Batch Loss: 0.24948759377002716\n",
      "Epoch 2117, Loss: 2.2449727691709995, Final Batch Loss: 0.017853859812021255\n",
      "Epoch 2118, Loss: 2.983012616634369, Final Batch Loss: 0.7242750525474548\n",
      "Epoch 2119, Loss: 2.8667865991592407, Final Batch Loss: 0.7327375411987305\n",
      "Epoch 2120, Loss: 2.4368917644023895, Final Batch Loss: 0.2628413140773773\n",
      "Epoch 2121, Loss: 2.9286367297172546, Final Batch Loss: 0.7811976671218872\n",
      "Epoch 2122, Loss: 2.2332127820700407, Final Batch Loss: 0.02108795754611492\n",
      "Epoch 2123, Loss: 2.1987921372056007, Final Batch Loss: 0.0006342306733131409\n",
      "Epoch 2124, Loss: 2.215864124475047, Final Batch Loss: 0.0022356535773724318\n",
      "Epoch 2125, Loss: 2.283577434718609, Final Batch Loss: 0.02787720412015915\n",
      "Epoch 2126, Loss: 2.977740228176117, Final Batch Loss: 0.7433788180351257\n",
      "Epoch 2127, Loss: 2.3101069945842028, Final Batch Loss: 0.023384036496281624\n",
      "Epoch 2128, Loss: 2.218709087057505, Final Batch Loss: 0.0005354639724828303\n",
      "Epoch 2129, Loss: 3.03775954246521, Final Batch Loss: 0.825912594795227\n",
      "Epoch 2130, Loss: 4.479304492473602, Final Batch Loss: 2.2686142921447754\n",
      "Epoch 2131, Loss: 2.826320171356201, Final Batch Loss: 0.521904468536377\n",
      "Epoch 2132, Loss: 4.136250555515289, Final Batch Loss: 1.877510905265808\n",
      "Epoch 2133, Loss: 2.2965760231018066, Final Batch Loss: 0.030441343784332275\n",
      "Epoch 2134, Loss: 2.8070640563964844, Final Batch Loss: 0.5299460291862488\n",
      "Epoch 2135, Loss: 2.724834978580475, Final Batch Loss: 0.47855526208877563\n",
      "Epoch 2136, Loss: 2.2398946657776833, Final Batch Loss: 0.07096890360116959\n",
      "Epoch 2137, Loss: 2.545535624027252, Final Batch Loss: 0.21511244773864746\n",
      "Epoch 2138, Loss: 3.72782564163208, Final Batch Loss: 1.3707022666931152\n",
      "Epoch 2139, Loss: 2.3385741433594376, Final Batch Loss: 0.0006425699684768915\n",
      "Epoch 2140, Loss: 2.288318060338497, Final Batch Loss: 0.03261777013540268\n",
      "Epoch 2141, Loss: 4.7516257762908936, Final Batch Loss: 2.4974756240844727\n",
      "Epoch 2142, Loss: 3.0476795434951782, Final Batch Loss: 0.9081790447235107\n",
      "Epoch 2143, Loss: 2.2636546567082405, Final Batch Loss: 0.09371835738420486\n",
      "Epoch 2144, Loss: 2.5381992161273956, Final Batch Loss: 0.27126166224479675\n",
      "Epoch 2145, Loss: 2.8129669427871704, Final Batch Loss: 0.5775686502456665\n",
      "Epoch 2146, Loss: 4.263855397701263, Final Batch Loss: 1.9314191341400146\n",
      "Epoch 2147, Loss: 2.3636544346809387, Final Batch Loss: 0.13217252492904663\n",
      "Epoch 2148, Loss: 3.9234240651130676, Final Batch Loss: 1.7205809354782104\n",
      "Epoch 2149, Loss: 3.26621150970459, Final Batch Loss: 1.009832739830017\n",
      "Epoch 2150, Loss: 2.3079760521650314, Final Batch Loss: 0.22608987987041473\n",
      "Epoch 2151, Loss: 2.8759616017341614, Final Batch Loss: 0.6566730737686157\n",
      "Epoch 2152, Loss: 3.6840600967407227, Final Batch Loss: 1.3094488382339478\n",
      "Epoch 2153, Loss: 2.3473953008651733, Final Batch Loss: 0.11805850267410278\n",
      "Epoch 2154, Loss: 2.9155014753341675, Final Batch Loss: 0.6399486064910889\n",
      "Epoch 2155, Loss: 4.662934064865112, Final Batch Loss: 2.512369155883789\n",
      "Epoch 2156, Loss: 2.2479460909962654, Final Batch Loss: 0.03594566136598587\n",
      "Epoch 2157, Loss: 2.1825452484190464, Final Batch Loss: 0.02213067188858986\n",
      "Epoch 2158, Loss: 5.712935090065002, Final Batch Loss: 3.465155601501465\n",
      "Epoch 2159, Loss: 3.211634576320648, Final Batch Loss: 0.9973198175430298\n",
      "Epoch 2160, Loss: 3.243976354598999, Final Batch Loss: 0.6982200145721436\n",
      "Epoch 2161, Loss: 2.6783512011170387, Final Batch Loss: 0.06963161379098892\n",
      "Epoch 2162, Loss: 2.844641327857971, Final Batch Loss: 0.24526286125183105\n",
      "Epoch 2163, Loss: 5.364696800708771, Final Batch Loss: 3.0674331188201904\n",
      "Epoch 2164, Loss: 2.320107728242874, Final Batch Loss: 0.20652511715888977\n",
      "Epoch 2165, Loss: 2.290918782353401, Final Batch Loss: 0.0011832863092422485\n",
      "Epoch 2166, Loss: 2.2908311933279037, Final Batch Loss: 0.08647821843624115\n",
      "Epoch 2167, Loss: 2.6617336869239807, Final Batch Loss: 0.40420961380004883\n",
      "Epoch 2168, Loss: 2.4571067541837692, Final Batch Loss: 0.20334492623806\n",
      "Epoch 2169, Loss: 2.2437780164182186, Final Batch Loss: 0.004610147327184677\n",
      "Epoch 2170, Loss: 2.1914967615157366, Final Batch Loss: 0.015151293948292732\n",
      "Epoch 2171, Loss: 3.0549798607826233, Final Batch Loss: 0.9372800588607788\n",
      "Epoch 2172, Loss: 3.701066553592682, Final Batch Loss: 1.5808637142181396\n",
      "Epoch 2173, Loss: 3.719865918159485, Final Batch Loss: 1.483262062072754\n",
      "Epoch 2174, Loss: 2.1916357721202075, Final Batch Loss: 0.0007915939204394817\n",
      "Epoch 2175, Loss: 2.1718208491802216, Final Batch Loss: 0.009525090456008911\n",
      "Epoch 2176, Loss: 2.2414496652781963, Final Batch Loss: 0.04991663619875908\n",
      "Epoch 2177, Loss: 2.275792770087719, Final Batch Loss: 0.0691528245806694\n",
      "Epoch 2178, Loss: 2.2828291580080986, Final Batch Loss: 0.05496930330991745\n",
      "Epoch 2179, Loss: 6.216075301170349, Final Batch Loss: 4.056058406829834\n",
      "Epoch 2180, Loss: 2.693813443183899, Final Batch Loss: 0.5133835077285767\n",
      "Epoch 2181, Loss: 3.299389123916626, Final Batch Loss: 1.0150309801101685\n",
      "Epoch 2182, Loss: 2.305053688585758, Final Batch Loss: 0.053010858595371246\n",
      "Epoch 2183, Loss: 2.2782047195360065, Final Batch Loss: 0.010168532840907574\n",
      "Epoch 2184, Loss: 2.2853392139077187, Final Batch Loss: 0.03621254116296768\n",
      "Epoch 2185, Loss: 3.316586136817932, Final Batch Loss: 1.082625389099121\n",
      "Epoch 2186, Loss: 2.5686095654964447, Final Batch Loss: 0.3327346742153168\n",
      "Epoch 2187, Loss: 3.3230097889900208, Final Batch Loss: 1.1878209114074707\n",
      "Epoch 2188, Loss: 2.8715596795082092, Final Batch Loss: 0.6510869860649109\n",
      "Epoch 2189, Loss: 2.4896106123924255, Final Batch Loss: 0.288967490196228\n",
      "Epoch 2190, Loss: 2.531425505876541, Final Batch Loss: 0.28611788153648376\n",
      "Epoch 2191, Loss: 3.7478365302085876, Final Batch Loss: 1.5927600860595703\n",
      "Epoch 2192, Loss: 2.582064598798752, Final Batch Loss: 0.4380081593990326\n",
      "Epoch 2193, Loss: 2.7044562101364136, Final Batch Loss: 0.6095595955848694\n",
      "Epoch 2194, Loss: 2.4275108873844147, Final Batch Loss: 0.27867981791496277\n",
      "Epoch 2195, Loss: 4.659388542175293, Final Batch Loss: 2.4817209243774414\n",
      "Epoch 2196, Loss: 3.248300313949585, Final Batch Loss: 1.2023847103118896\n",
      "Epoch 2197, Loss: 2.149694040417671, Final Batch Loss: 0.013631001114845276\n",
      "Epoch 2198, Loss: 2.23190775513649, Final Batch Loss: 0.1247921884059906\n",
      "Epoch 2199, Loss: 4.284711956977844, Final Batch Loss: 2.122122049331665\n",
      "Epoch 2200, Loss: 2.1919962936080992, Final Batch Loss: 0.0021749907173216343\n",
      "Epoch 2201, Loss: 2.91668564081192, Final Batch Loss: 0.7713716626167297\n",
      "Epoch 2202, Loss: 2.446089893579483, Final Batch Loss: 0.29838863015174866\n",
      "Epoch 2203, Loss: 2.3213458247482777, Final Batch Loss: 0.03826780989766121\n",
      "Epoch 2204, Loss: 2.2845568358898163, Final Batch Loss: 0.022937685251235962\n",
      "Epoch 2205, Loss: 4.435896635055542, Final Batch Loss: 2.219966411590576\n",
      "Epoch 2206, Loss: 5.189586818218231, Final Batch Loss: 2.9424262046813965\n",
      "Epoch 2207, Loss: 3.8080793619155884, Final Batch Loss: 1.5416408777236938\n",
      "Epoch 2208, Loss: 2.555431306362152, Final Batch Loss: 0.305650532245636\n",
      "Epoch 2209, Loss: 2.263524401932955, Final Batch Loss: 0.031237173825502396\n",
      "Epoch 2210, Loss: 3.755196273326874, Final Batch Loss: 1.5322887897491455\n",
      "Epoch 2211, Loss: 2.2902002073824406, Final Batch Loss: 0.05294517055153847\n",
      "Epoch 2212, Loss: 4.03277462720871, Final Batch Loss: 1.7068318128585815\n",
      "Epoch 2213, Loss: 2.446371247933712, Final Batch Loss: 0.0003713871701620519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2214, Loss: 4.457669198513031, Final Batch Loss: 2.0045533180236816\n",
      "Epoch 2215, Loss: 5.679331362247467, Final Batch Loss: 3.216050148010254\n",
      "Epoch 2216, Loss: 4.2536672949790955, Final Batch Loss: 2.0156679153442383\n",
      "Epoch 2217, Loss: 3.3739163875579834, Final Batch Loss: 0.9885082244873047\n",
      "Epoch 2218, Loss: 2.356651857495308, Final Batch Loss: 0.04953189194202423\n",
      "Epoch 2219, Loss: 2.3969360180199146, Final Batch Loss: 0.021486598998308182\n",
      "Epoch 2220, Loss: 2.3200070746243, Final Batch Loss: 0.013779748231172562\n",
      "Epoch 2221, Loss: 2.2163067553192377, Final Batch Loss: 0.017549822106957436\n",
      "Epoch 2222, Loss: 3.9484909176826477, Final Batch Loss: 1.6302682161331177\n",
      "Epoch 2223, Loss: 2.407448247075081, Final Batch Loss: 0.19769896566867828\n",
      "Epoch 2224, Loss: 3.377333641052246, Final Batch Loss: 1.1419273614883423\n",
      "Epoch 2225, Loss: 2.78069144487381, Final Batch Loss: 0.5717321634292603\n",
      "Epoch 2226, Loss: 2.431008219718933, Final Batch Loss: 0.2792326807975769\n",
      "Epoch 2227, Loss: 2.745242476463318, Final Batch Loss: 0.5761785507202148\n",
      "Epoch 2228, Loss: 3.711341381072998, Final Batch Loss: 1.5120291709899902\n",
      "Epoch 2229, Loss: 2.255817534402013, Final Batch Loss: 0.011624576523900032\n",
      "Epoch 2230, Loss: 2.5752663165330887, Final Batch Loss: 0.245535209774971\n",
      "Epoch 2231, Loss: 3.6446638703346252, Final Batch Loss: 1.3637139797210693\n",
      "Epoch 2232, Loss: 2.358046606183052, Final Batch Loss: 0.09549160301685333\n",
      "Epoch 2233, Loss: 3.1511601209640503, Final Batch Loss: 0.9435505867004395\n",
      "Epoch 2234, Loss: 3.798147141933441, Final Batch Loss: 1.6526172161102295\n",
      "Epoch 2235, Loss: 2.095935622230172, Final Batch Loss: 0.0066549573093652725\n",
      "Epoch 2236, Loss: 2.740119218826294, Final Batch Loss: 0.6282016634941101\n",
      "Epoch 2237, Loss: 2.188335444778204, Final Batch Loss: 0.014287438243627548\n",
      "Epoch 2238, Loss: 4.128138363361359, Final Batch Loss: 1.8858555555343628\n",
      "Epoch 2239, Loss: 2.423990771174431, Final Batch Loss: 0.22872363030910492\n",
      "Epoch 2240, Loss: 4.140061855316162, Final Batch Loss: 1.9599530696868896\n",
      "Epoch 2241, Loss: 2.871471405029297, Final Batch Loss: 0.8331691026687622\n",
      "Epoch 2242, Loss: 2.1449114345014095, Final Batch Loss: 0.010698463767766953\n",
      "Epoch 2243, Loss: 2.141201190650463, Final Batch Loss: 0.06384878605604172\n",
      "Epoch 2244, Loss: 2.3135153502225876, Final Batch Loss: 0.1548868864774704\n",
      "Epoch 2245, Loss: 2.1800387743860483, Final Batch Loss: 0.015703821554780006\n",
      "Epoch 2246, Loss: 3.6548752784729004, Final Batch Loss: 1.4498426914215088\n",
      "Epoch 2247, Loss: 2.1868847981095314, Final Batch Loss: 0.01632336527109146\n",
      "Epoch 2248, Loss: 2.181063659489155, Final Batch Loss: 0.03989226371049881\n",
      "Epoch 2249, Loss: 2.27014784142375, Final Batch Loss: 0.017782192677259445\n",
      "Epoch 2250, Loss: 2.1449070214293897, Final Batch Loss: 0.0022526620887219906\n",
      "Epoch 2251, Loss: 2.5579633116722107, Final Batch Loss: 0.48507261276245117\n",
      "Epoch 2252, Loss: 2.074653781717643, Final Batch Loss: 0.00211016065441072\n",
      "Epoch 2253, Loss: 2.145893156528473, Final Batch Loss: 0.032711803913116455\n",
      "Epoch 2254, Loss: 2.4025705754756927, Final Batch Loss: 0.312398761510849\n",
      "Epoch 2255, Loss: 2.2690672129392624, Final Batch Loss: 0.22802434861660004\n",
      "Epoch 2256, Loss: 2.107316937763244, Final Batch Loss: 0.007384386379271746\n",
      "Epoch 2257, Loss: 3.6284472942352295, Final Batch Loss: 1.5981084108352661\n",
      "Epoch 2258, Loss: 2.106612549163401, Final Batch Loss: 0.012452707625925541\n",
      "Epoch 2259, Loss: 2.7446309328079224, Final Batch Loss: 0.6263806223869324\n",
      "Epoch 2260, Loss: 2.607957184314728, Final Batch Loss: 0.5465848445892334\n",
      "Epoch 2261, Loss: 4.283653140068054, Final Batch Loss: 2.2795894145965576\n",
      "Epoch 2262, Loss: 3.4705445766448975, Final Batch Loss: 1.3606864213943481\n",
      "Epoch 2263, Loss: 3.5580790638923645, Final Batch Loss: 1.3485924005508423\n",
      "Epoch 2264, Loss: 2.0987992361187935, Final Batch Loss: 0.022045083343982697\n",
      "Epoch 2265, Loss: 4.5209580063819885, Final Batch Loss: 2.4046292304992676\n",
      "Epoch 2266, Loss: 2.205068130977452, Final Batch Loss: 0.008561690337955952\n",
      "Epoch 2267, Loss: 2.2545074466615915, Final Batch Loss: 0.01045759953558445\n",
      "Epoch 2268, Loss: 2.166157826781273, Final Batch Loss: 0.02165214717388153\n",
      "Epoch 2269, Loss: 2.2129979506134987, Final Batch Loss: 0.03975949436426163\n",
      "Epoch 2270, Loss: 2.0503559841308743, Final Batch Loss: 0.0019760860595852137\n",
      "Epoch 2271, Loss: 2.2470191209577024, Final Batch Loss: 0.0038088648580014706\n",
      "Epoch 2272, Loss: 2.1010733991861343, Final Batch Loss: 0.0266735702753067\n",
      "Epoch 2273, Loss: 2.312478318810463, Final Batch Loss: 0.2104165107011795\n",
      "Epoch 2274, Loss: 2.1630656789056957, Final Batch Loss: 0.004348940681666136\n",
      "Epoch 2275, Loss: 2.0321952700278416, Final Batch Loss: 8.22540732769994e-06\n",
      "Epoch 2276, Loss: 2.9623810052871704, Final Batch Loss: 0.931332528591156\n",
      "Epoch 2277, Loss: 3.071671426296234, Final Batch Loss: 0.9557059407234192\n",
      "Epoch 2278, Loss: 2.074524520430714, Final Batch Loss: 0.006265758071094751\n",
      "Epoch 2279, Loss: 2.057196255074814, Final Batch Loss: 0.0019850090611726046\n",
      "Epoch 2280, Loss: 3.330856502056122, Final Batch Loss: 1.142852783203125\n",
      "Epoch 2281, Loss: 2.311478264629841, Final Batch Loss: 0.09520984441041946\n",
      "Epoch 2282, Loss: 4.238478660583496, Final Batch Loss: 2.1716017723083496\n",
      "Epoch 2283, Loss: 3.08067524433136, Final Batch Loss: 0.8852514624595642\n",
      "Epoch 2284, Loss: 3.4841408729553223, Final Batch Loss: 1.3915960788726807\n",
      "Epoch 2285, Loss: 2.2274557426571846, Final Batch Loss: 0.11262894421815872\n",
      "Epoch 2286, Loss: 3.0615559816360474, Final Batch Loss: 0.963596522808075\n",
      "Epoch 2287, Loss: 2.491505265235901, Final Batch Loss: 0.3560904264450073\n",
      "Epoch 2288, Loss: 2.193793948739767, Final Batch Loss: 0.0030877552926540375\n",
      "Epoch 2289, Loss: 3.406360447406769, Final Batch Loss: 1.3000385761260986\n",
      "Epoch 2290, Loss: 2.072174157947302, Final Batch Loss: 0.011729087680578232\n",
      "Epoch 2291, Loss: 5.323431134223938, Final Batch Loss: 3.1857481002807617\n",
      "Epoch 2292, Loss: 2.361844301223755, Final Batch Loss: 0.17721086740493774\n",
      "Epoch 2293, Loss: 2.250682743033394, Final Batch Loss: 0.0037398652639240026\n",
      "Epoch 2294, Loss: 3.5189496874809265, Final Batch Loss: 1.2967451810836792\n",
      "Epoch 2295, Loss: 2.175935685616878, Final Batch Loss: 5.960446742392378e-06\n",
      "Epoch 2296, Loss: 3.474194347858429, Final Batch Loss: 1.2911560535430908\n",
      "Epoch 2297, Loss: 3.7869327068328857, Final Batch Loss: 1.6999022960662842\n",
      "Epoch 2298, Loss: 2.7090196013450623, Final Batch Loss: 0.7286007404327393\n",
      "Epoch 2299, Loss: 2.5247185230255127, Final Batch Loss: 0.34058260917663574\n",
      "Epoch 2300, Loss: 2.0855780858546495, Final Batch Loss: 0.015884997323155403\n",
      "Epoch 2301, Loss: 2.15377302095294, Final Batch Loss: 0.025032829493284225\n",
      "Epoch 2302, Loss: 2.2439445108175278, Final Batch Loss: 0.18094776570796967\n",
      "Epoch 2303, Loss: 2.051820627762936, Final Batch Loss: 0.0012024560710415244\n",
      "Epoch 2304, Loss: 3.0712167024612427, Final Batch Loss: 0.9348088502883911\n",
      "Epoch 2305, Loss: 3.8980535864830017, Final Batch Loss: 1.8890488147735596\n",
      "Epoch 2306, Loss: 3.5756320357322693, Final Batch Loss: 1.5642420053482056\n",
      "Epoch 2307, Loss: 2.471319079399109, Final Batch Loss: 0.35296106338500977\n",
      "Epoch 2308, Loss: 4.090944766998291, Final Batch Loss: 1.9452812671661377\n",
      "Epoch 2309, Loss: 3.984171211719513, Final Batch Loss: 1.8199552297592163\n",
      "Epoch 2310, Loss: 2.4419281631708145, Final Batch Loss: 0.11469961702823639\n",
      "Epoch 2311, Loss: 2.508646994829178, Final Batch Loss: 0.06470564007759094\n",
      "Epoch 2312, Loss: 3.5284703373908997, Final Batch Loss: 1.0410665273666382\n",
      "Epoch 2313, Loss: 3.2709036469459534, Final Batch Loss: 0.879530131816864\n",
      "Epoch 2314, Loss: 2.865569648856763, Final Batch Loss: 6.735097849741578e-05\n",
      "Epoch 2315, Loss: 2.8303354615345597, Final Batch Loss: 0.00832220260053873\n",
      "Epoch 2316, Loss: 4.7739298939704895, Final Batch Loss: 2.14499568939209\n",
      "Epoch 2317, Loss: 2.38001437112689, Final Batch Loss: 0.047513198107481\n",
      "Epoch 2318, Loss: 3.4252688884735107, Final Batch Loss: 1.1397631168365479\n",
      "Epoch 2319, Loss: 3.6474488973617554, Final Batch Loss: 1.4407687187194824\n",
      "Epoch 2320, Loss: 3.304839611053467, Final Batch Loss: 1.116159439086914\n",
      "Epoch 2321, Loss: 2.588886708021164, Final Batch Loss: 0.35012075304985046\n",
      "Epoch 2322, Loss: 2.22606936423108, Final Batch Loss: 0.004634353797882795\n",
      "Epoch 2323, Loss: 2.566274404525757, Final Batch Loss: 0.2988972067832947\n",
      "Epoch 2324, Loss: 3.4157678484916687, Final Batch Loss: 1.2518410682678223\n",
      "Epoch 2325, Loss: 2.327883195132017, Final Batch Loss: 0.04108376428484917\n",
      "Epoch 2326, Loss: 3.517636239528656, Final Batch Loss: 1.3248478174209595\n",
      "Epoch 2327, Loss: 2.1627922169864178, Final Batch Loss: 0.02071554586291313\n",
      "Epoch 2328, Loss: 3.6326566338539124, Final Batch Loss: 1.4476499557495117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2329, Loss: 2.3984211683273315, Final Batch Loss: 0.15582263469696045\n",
      "Epoch 2330, Loss: 3.1453729271888733, Final Batch Loss: 0.9027717113494873\n",
      "Epoch 2331, Loss: 2.691178560256958, Final Batch Loss: 0.4730347990989685\n",
      "Epoch 2332, Loss: 7.804591953754425, Final Batch Loss: 5.633232593536377\n",
      "Epoch 2333, Loss: 2.948416531085968, Final Batch Loss: 0.7593958973884583\n",
      "Epoch 2334, Loss: 3.764184355735779, Final Batch Loss: 1.4930846691131592\n",
      "Epoch 2335, Loss: 3.4485026597976685, Final Batch Loss: 1.0228283405303955\n",
      "Epoch 2336, Loss: 4.718811631202698, Final Batch Loss: 2.1274378299713135\n",
      "Epoch 2337, Loss: 2.4873381853103638, Final Batch Loss: 0.07500612735748291\n",
      "Epoch 2338, Loss: 3.7180830240249634, Final Batch Loss: 1.3901135921478271\n",
      "Epoch 2339, Loss: 5.671358585357666, Final Batch Loss: 3.328782081604004\n",
      "Epoch 2340, Loss: 2.7761465907096863, Final Batch Loss: 0.46050792932510376\n",
      "Epoch 2341, Loss: 2.5939647257328033, Final Batch Loss: 0.17753013968467712\n",
      "Epoch 2342, Loss: 2.357860680669546, Final Batch Loss: 0.04530256614089012\n",
      "Epoch 2343, Loss: 5.3716564774513245, Final Batch Loss: 3.0754356384277344\n",
      "Epoch 2344, Loss: 2.3299618139863014, Final Batch Loss: 0.031207479536533356\n",
      "Epoch 2345, Loss: 2.513897493481636, Final Batch Loss: 0.17821086943149567\n",
      "Epoch 2346, Loss: 3.261192560195923, Final Batch Loss: 0.7648430466651917\n",
      "Epoch 2347, Loss: 4.264308750629425, Final Batch Loss: 1.8633062839508057\n",
      "Epoch 2348, Loss: 4.254376411437988, Final Batch Loss: 1.8511451482772827\n",
      "Epoch 2349, Loss: 6.3810359835624695, Final Batch Loss: 3.8671655654907227\n",
      "Epoch 2350, Loss: 2.6888450011610985, Final Batch Loss: 0.12322290986776352\n",
      "Epoch 2351, Loss: 3.517940402030945, Final Batch Loss: 1.1232569217681885\n",
      "Epoch 2352, Loss: 2.243455372285098, Final Batch Loss: 0.00712307495996356\n",
      "Epoch 2353, Loss: 3.6759693026542664, Final Batch Loss: 1.434598445892334\n",
      "Epoch 2354, Loss: 2.9409037828445435, Final Batch Loss: 0.6970989108085632\n",
      "Epoch 2355, Loss: 2.225105760124279, Final Batch Loss: 6.8662193370983e-05\n",
      "Epoch 2356, Loss: 2.3670209497213364, Final Batch Loss: 0.1091526597738266\n",
      "Epoch 2357, Loss: 4.22659170627594, Final Batch Loss: 1.9362257719039917\n",
      "Epoch 2358, Loss: 3.6461539268493652, Final Batch Loss: 1.3381538391113281\n",
      "Epoch 2359, Loss: 2.360900692641735, Final Batch Loss: 0.10715388506650925\n",
      "Epoch 2360, Loss: 3.3313145637512207, Final Batch Loss: 1.0181024074554443\n",
      "Epoch 2361, Loss: 2.3821980046341196, Final Batch Loss: 0.0002493547508493066\n",
      "Epoch 2362, Loss: 3.738605499267578, Final Batch Loss: 1.166579008102417\n",
      "Epoch 2363, Loss: 4.354238569736481, Final Batch Loss: 1.858780026435852\n",
      "Epoch 2364, Loss: 2.9182958006858826, Final Batch Loss: 0.5595813393592834\n",
      "Epoch 2365, Loss: 3.679171860218048, Final Batch Loss: 1.457528829574585\n",
      "Epoch 2366, Loss: 3.3662793040275574, Final Batch Loss: 1.1847134828567505\n",
      "Epoch 2367, Loss: 5.128468334674835, Final Batch Loss: 2.8578453063964844\n",
      "Epoch 2368, Loss: 2.7874011397361755, Final Batch Loss: 0.5471936464309692\n",
      "Epoch 2369, Loss: 2.6067696511745453, Final Batch Loss: 0.14715370535850525\n",
      "Epoch 2370, Loss: 3.124979555606842, Final Batch Loss: 0.6291017532348633\n",
      "Epoch 2371, Loss: 3.2229669988155365, Final Batch Loss: 0.39636728167533875\n",
      "Epoch 2372, Loss: 3.7255923748016357, Final Batch Loss: 1.1733773946762085\n",
      "Epoch 2373, Loss: 2.7268148064613342, Final Batch Loss: 0.25411975383758545\n",
      "Epoch 2374, Loss: 2.9915019273757935, Final Batch Loss: 0.5746555924415588\n",
      "Epoch 2375, Loss: 2.48752723634243, Final Batch Loss: 0.17893777787685394\n",
      "Epoch 2376, Loss: 3.517064929008484, Final Batch Loss: 1.2127796411514282\n",
      "Epoch 2377, Loss: 3.0206494331359863, Final Batch Loss: 0.6711956262588501\n",
      "Epoch 2378, Loss: 2.9740822315216064, Final Batch Loss: 0.6765772104263306\n",
      "Epoch 2379, Loss: 2.2603964991867542, Final Batch Loss: 0.0368855781853199\n",
      "Epoch 2380, Loss: 2.3338479287922382, Final Batch Loss: 0.03612803295254707\n",
      "Epoch 2381, Loss: 2.3317074049264193, Final Batch Loss: 0.025383995845913887\n",
      "Epoch 2382, Loss: 3.599151909351349, Final Batch Loss: 1.4797149896621704\n",
      "Epoch 2383, Loss: 3.8281272053718567, Final Batch Loss: 1.5766589641571045\n",
      "Epoch 2384, Loss: 2.7500983476638794, Final Batch Loss: 0.6159068942070007\n",
      "Epoch 2385, Loss: 2.2055126167833805, Final Batch Loss: 0.016720745712518692\n",
      "Epoch 2386, Loss: 4.201910495758057, Final Batch Loss: 2.0599701404571533\n",
      "Epoch 2387, Loss: 2.1028694845736027, Final Batch Loss: 0.05079445615410805\n",
      "Epoch 2388, Loss: 2.173810901120305, Final Batch Loss: 0.011568369343876839\n",
      "Epoch 2389, Loss: 2.1407915391027927, Final Batch Loss: 0.062391284853219986\n",
      "Epoch 2390, Loss: 2.098860438913107, Final Batch Loss: 0.040086861699819565\n",
      "Epoch 2391, Loss: 2.3501551747322083, Final Batch Loss: 0.36990445852279663\n",
      "Epoch 2392, Loss: 2.1633268669247627, Final Batch Loss: 0.11550391465425491\n",
      "Epoch 2393, Loss: 2.0586382138280896, Final Batch Loss: 4.970903682988137e-05\n",
      "Epoch 2394, Loss: 2.0282241720706224, Final Batch Loss: 0.013172509148716927\n",
      "Epoch 2395, Loss: 2.110319849445659, Final Batch Loss: 8.225102646974847e-05\n",
      "Epoch 2396, Loss: 3.3018129467964172, Final Batch Loss: 1.2144877910614014\n",
      "Epoch 2397, Loss: 3.4739474058151245, Final Batch Loss: 1.42362642288208\n",
      "Epoch 2398, Loss: 3.1867709159851074, Final Batch Loss: 1.0861198902130127\n",
      "Epoch 2399, Loss: 2.5503203570842743, Final Batch Loss: 0.3558014929294586\n",
      "Epoch 2400, Loss: 2.763033926486969, Final Batch Loss: 0.6516425013542175\n",
      "Epoch 2401, Loss: 2.7730098366737366, Final Batch Loss: 0.61717689037323\n",
      "Epoch 2402, Loss: 2.125350563786924, Final Batch Loss: 0.013111918233335018\n",
      "Epoch 2403, Loss: 3.9262490272521973, Final Batch Loss: 1.8648308515548706\n",
      "Epoch 2404, Loss: 4.3406864404678345, Final Batch Loss: 2.2755794525146484\n",
      "Epoch 2405, Loss: 3.5519497394561768, Final Batch Loss: 1.3070776462554932\n",
      "Epoch 2406, Loss: 3.9700145721435547, Final Batch Loss: 1.8142106533050537\n",
      "Epoch 2407, Loss: 3.043336033821106, Final Batch Loss: 0.8592987656593323\n",
      "Epoch 2408, Loss: 4.265203356742859, Final Batch Loss: 2.0169713497161865\n",
      "Epoch 2409, Loss: 2.7978495955467224, Final Batch Loss: 0.6375371813774109\n",
      "Epoch 2410, Loss: 2.2838833034038544, Final Batch Loss: 0.13392230868339539\n",
      "Epoch 2411, Loss: 2.2837600111961365, Final Batch Loss: 0.15781879425048828\n",
      "Epoch 2412, Loss: 3.982552707195282, Final Batch Loss: 1.81239914894104\n",
      "Epoch 2413, Loss: 4.253191709518433, Final Batch Loss: 2.025442123413086\n",
      "Epoch 2414, Loss: 2.49469393491745, Final Batch Loss: 0.34844809770584106\n",
      "Epoch 2415, Loss: 3.9123138785362244, Final Batch Loss: 1.6978175640106201\n",
      "Epoch 2416, Loss: 2.7678602933883667, Final Batch Loss: 0.6054940819740295\n",
      "Epoch 2417, Loss: 3.2998135089874268, Final Batch Loss: 1.1834838390350342\n",
      "Epoch 2418, Loss: 2.4744686782360077, Final Batch Loss: 0.4023955166339874\n",
      "Epoch 2419, Loss: 3.165714383125305, Final Batch Loss: 1.041800856590271\n",
      "Epoch 2420, Loss: 2.1835287027060986, Final Batch Loss: 0.04659334942698479\n",
      "Epoch 2421, Loss: 3.31302148103714, Final Batch Loss: 1.2861690521240234\n",
      "Epoch 2422, Loss: 2.8697301149368286, Final Batch Loss: 0.7098986506462097\n",
      "Epoch 2423, Loss: 4.384645938873291, Final Batch Loss: 2.29415225982666\n",
      "Epoch 2424, Loss: 2.139819742180407, Final Batch Loss: 0.014375925995409489\n",
      "Epoch 2425, Loss: 2.177901123650372, Final Batch Loss: 0.013375316746532917\n",
      "Epoch 2426, Loss: 2.7540206909179688, Final Batch Loss: 0.6158328652381897\n",
      "Epoch 2427, Loss: 2.91779226064682, Final Batch Loss: 0.7944061756134033\n",
      "Epoch 2428, Loss: 2.1108499001711607, Final Batch Loss: 0.012352628633379936\n",
      "Epoch 2429, Loss: 2.8923972249031067, Final Batch Loss: 0.7138417959213257\n",
      "Epoch 2430, Loss: 2.257393516600132, Final Batch Loss: 0.00824890285730362\n",
      "Epoch 2431, Loss: 7.125339031219482, Final Batch Loss: 4.965406894683838\n",
      "Epoch 2432, Loss: 2.304570198059082, Final Batch Loss: 0.06444960832595825\n",
      "Epoch 2433, Loss: 2.201344197615981, Final Batch Loss: 0.005603676661849022\n",
      "Epoch 2434, Loss: 2.3225512132048607, Final Batch Loss: 0.083726666867733\n",
      "Epoch 2435, Loss: 2.333243948101881, Final Batch Loss: 0.00018976318824570626\n",
      "Epoch 2436, Loss: 3.3988872170448303, Final Batch Loss: 1.2036091089248657\n",
      "Epoch 2437, Loss: 2.290504589676857, Final Batch Loss: 0.06602407991886139\n",
      "Epoch 2438, Loss: 2.180507622542791, Final Batch Loss: 0.0015292390016838908\n",
      "Epoch 2439, Loss: 2.0979300187900662, Final Batch Loss: 0.007491116411983967\n",
      "Epoch 2440, Loss: 2.21412942558527, Final Batch Loss: 0.043127454817295074\n",
      "Epoch 2441, Loss: 2.1569173093885183, Final Batch Loss: 0.015186166390776634\n",
      "Epoch 2442, Loss: 2.192128948867321, Final Batch Loss: 0.10463105887174606\n",
      "Epoch 2443, Loss: 2.5216508507728577, Final Batch Loss: 0.4526323676109314\n",
      "Epoch 2444, Loss: 2.111823422659654, Final Batch Loss: 0.00018404220463708043\n",
      "Epoch 2445, Loss: 2.1066327020525932, Final Batch Loss: 0.03446900099515915\n",
      "Epoch 2446, Loss: 2.740352749824524, Final Batch Loss: 0.724591851234436\n",
      "Epoch 2447, Loss: 3.850938320159912, Final Batch Loss: 1.710097074508667\n",
      "Epoch 2448, Loss: 2.034294655546546, Final Batch Loss: 0.014597168192267418\n",
      "Epoch 2449, Loss: 2.091937627643347, Final Batch Loss: 0.043087150901556015\n",
      "Epoch 2450, Loss: 2.9450557827949524, Final Batch Loss: 0.8905993700027466\n",
      "Epoch 2451, Loss: 2.115591997280717, Final Batch Loss: 0.016623789444565773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2452, Loss: 4.302641868591309, Final Batch Loss: 2.2116453647613525\n",
      "Epoch 2453, Loss: 2.8339990973472595, Final Batch Loss: 0.7800464630126953\n",
      "Epoch 2454, Loss: 4.0254634618759155, Final Batch Loss: 1.9451454877853394\n",
      "Epoch 2455, Loss: 2.31181663274765, Final Batch Loss: 0.10307550430297852\n",
      "Epoch 2456, Loss: 2.2631172575056553, Final Batch Loss: 0.05284035578370094\n",
      "Epoch 2457, Loss: 3.721957206726074, Final Batch Loss: 1.4920270442962646\n",
      "Epoch 2458, Loss: 2.30211338493973, Final Batch Loss: 0.008271248079836369\n",
      "Epoch 2459, Loss: 2.2473373115062714, Final Batch Loss: 0.0061747729778289795\n",
      "Epoch 2460, Loss: 2.2624152936041355, Final Batch Loss: 0.04726523533463478\n",
      "Epoch 2461, Loss: 3.7789754271507263, Final Batch Loss: 1.7067766189575195\n",
      "Epoch 2462, Loss: 2.59679314494133, Final Batch Loss: 0.4660079777240753\n",
      "Epoch 2463, Loss: 2.8424845337867737, Final Batch Loss: 0.7585723996162415\n",
      "Epoch 2464, Loss: 2.577921986579895, Final Batch Loss: 0.3522570729255676\n",
      "Epoch 2465, Loss: 5.2536816000938416, Final Batch Loss: 3.1525354385375977\n",
      "Epoch 2466, Loss: 2.229633040726185, Final Batch Loss: 0.10569996386766434\n",
      "Epoch 2467, Loss: 4.704930126667023, Final Batch Loss: 2.6377549171447754\n",
      "Epoch 2468, Loss: 2.3680267333984375, Final Batch Loss: 0.12872862815856934\n",
      "Epoch 2469, Loss: 2.9201444387435913, Final Batch Loss: 0.6884856820106506\n",
      "Epoch 2470, Loss: 4.103565514087677, Final Batch Loss: 1.8410569429397583\n",
      "Epoch 2471, Loss: 2.3024319671094418, Final Batch Loss: 0.019222404807806015\n",
      "Epoch 2472, Loss: 4.4984410405159, Final Batch Loss: 2.2959420680999756\n",
      "Epoch 2473, Loss: 2.2865136936306953, Final Batch Loss: 0.08490509539842606\n",
      "Epoch 2474, Loss: 2.715341806411743, Final Batch Loss: 0.5713505744934082\n",
      "Epoch 2475, Loss: 2.4763086140155792, Final Batch Loss: 0.30316677689552307\n",
      "Epoch 2476, Loss: 3.932251989841461, Final Batch Loss: 1.787588357925415\n",
      "Epoch 2477, Loss: 2.1511348369531333, Final Batch Loss: 0.005574870388954878\n",
      "Epoch 2478, Loss: 2.196838727220893, Final Batch Loss: 0.017138710245490074\n",
      "Epoch 2479, Loss: 2.267453368753195, Final Batch Loss: 0.045023854821920395\n",
      "Epoch 2480, Loss: 2.172505460679531, Final Batch Loss: 0.06442513316869736\n",
      "Epoch 2481, Loss: 2.202841766178608, Final Batch Loss: 0.11441481858491898\n",
      "Epoch 2482, Loss: 2.879097580909729, Final Batch Loss: 0.7578075528144836\n",
      "Epoch 2483, Loss: 2.1611470971256495, Final Batch Loss: 0.02210245467722416\n",
      "Epoch 2484, Loss: 2.0978247988969088, Final Batch Loss: 0.018864857032895088\n",
      "Epoch 2485, Loss: 3.9830154180526733, Final Batch Loss: 1.927931308746338\n",
      "Epoch 2486, Loss: 2.4519868791103363, Final Batch Loss: 0.35020384192466736\n",
      "Epoch 2487, Loss: 2.783324718475342, Final Batch Loss: 0.7075096964836121\n",
      "Epoch 2488, Loss: 2.050859463401139, Final Batch Loss: 0.01264412421733141\n",
      "Epoch 2489, Loss: 3.973779320716858, Final Batch Loss: 1.9212654829025269\n",
      "Epoch 2490, Loss: 2.0555860493914224, Final Batch Loss: 0.000316927267704159\n",
      "Epoch 2491, Loss: 2.159062092192471, Final Batch Loss: 0.012408320792019367\n",
      "Epoch 2492, Loss: 2.1448682323098183, Final Batch Loss: 0.11612502485513687\n",
      "Epoch 2493, Loss: 4.047913670539856, Final Batch Loss: 2.0588572025299072\n",
      "Epoch 2494, Loss: 2.3903475403785706, Final Batch Loss: 0.3924131989479065\n",
      "Epoch 2495, Loss: 3.2603927850723267, Final Batch Loss: 1.2045214176177979\n",
      "Epoch 2496, Loss: 2.0618505673483014, Final Batch Loss: 0.010076840408146381\n",
      "Epoch 2497, Loss: 2.149616502225399, Final Batch Loss: 0.10303517431020737\n",
      "Epoch 2498, Loss: 2.762164831161499, Final Batch Loss: 0.7793719172477722\n",
      "Epoch 2499, Loss: 2.1051419153809547, Final Batch Loss: 0.1165260449051857\n",
      "Epoch 2500, Loss: 2.1657682061195374, Final Batch Loss: 0.13435262441635132\n",
      "Epoch 2501, Loss: 2.004603107459843, Final Batch Loss: 0.0071732597425580025\n",
      "Epoch 2502, Loss: 2.0852028355002403, Final Batch Loss: 0.0790724977850914\n",
      "Epoch 2503, Loss: 2.1498272866010666, Final Batch Loss: 0.2006193846464157\n",
      "Epoch 2504, Loss: 2.033188048750162, Final Batch Loss: 0.04555249586701393\n",
      "Epoch 2505, Loss: 2.4661737382411957, Final Batch Loss: 0.3625499904155731\n",
      "Epoch 2506, Loss: 2.1206546798348427, Final Batch Loss: 0.07707575708627701\n",
      "Epoch 2507, Loss: 2.160020187497139, Final Batch Loss: 0.12830258905887604\n",
      "Epoch 2508, Loss: 2.077757313847542, Final Batch Loss: 0.08340944349765778\n",
      "Epoch 2509, Loss: 1.9949737272690982, Final Batch Loss: 0.002842911286279559\n",
      "Epoch 2510, Loss: 3.459470510482788, Final Batch Loss: 1.4304225444793701\n",
      "Epoch 2511, Loss: 2.8257166147232056, Final Batch Loss: 0.7781482934951782\n",
      "Epoch 2512, Loss: 3.4365108609199524, Final Batch Loss: 1.4401177167892456\n",
      "Epoch 2513, Loss: 3.208917737007141, Final Batch Loss: 1.1765332221984863\n",
      "Epoch 2514, Loss: 2.3702172935009003, Final Batch Loss: 0.32868126034736633\n",
      "Epoch 2515, Loss: 2.069290390238166, Final Batch Loss: 0.023699810728430748\n",
      "Epoch 2516, Loss: 2.1995032876729965, Final Batch Loss: 0.1801367849111557\n",
      "Epoch 2517, Loss: 3.8157511353492737, Final Batch Loss: 1.8213849067687988\n",
      "Epoch 2518, Loss: 3.985101282596588, Final Batch Loss: 2.058812141418457\n",
      "Epoch 2519, Loss: 3.351136028766632, Final Batch Loss: 1.2162102460861206\n",
      "Epoch 2520, Loss: 2.620508372783661, Final Batch Loss: 0.601654589176178\n",
      "Epoch 2521, Loss: 2.1137370869982988, Final Batch Loss: 0.000196556793525815\n",
      "Epoch 2522, Loss: 3.248454213142395, Final Batch Loss: 1.1687935590744019\n",
      "Epoch 2523, Loss: 3.119230568408966, Final Batch Loss: 1.0556153059005737\n",
      "Epoch 2524, Loss: 2.7596657276153564, Final Batch Loss: 0.6810265779495239\n",
      "Epoch 2525, Loss: 2.089117366820574, Final Batch Loss: 0.020865578204393387\n",
      "Epoch 2526, Loss: 2.4265987873077393, Final Batch Loss: 0.34772032499313354\n",
      "Epoch 2527, Loss: 2.1840444166446105, Final Batch Loss: 0.0018286664271727204\n",
      "Epoch 2528, Loss: 3.5956308841705322, Final Batch Loss: 1.4783488512039185\n",
      "Epoch 2529, Loss: 2.127216277644038, Final Batch Loss: 0.00875609926879406\n",
      "Epoch 2530, Loss: 3.557334542274475, Final Batch Loss: 1.5082981586456299\n",
      "Epoch 2531, Loss: 2.153833372867666, Final Batch Loss: 0.00130425242241472\n",
      "Epoch 2532, Loss: 5.590312361717224, Final Batch Loss: 3.306352376937866\n",
      "Epoch 2533, Loss: 2.574044644832611, Final Batch Loss: 0.26002371311187744\n",
      "Epoch 2534, Loss: 2.241819116054103, Final Batch Loss: 0.0026386703830212355\n",
      "Epoch 2535, Loss: 3.700897514820099, Final Batch Loss: 1.3717021942138672\n",
      "Epoch 2536, Loss: 2.3908695250283927, Final Batch Loss: 0.0015262633096426725\n",
      "Epoch 2537, Loss: 2.8982653617858887, Final Batch Loss: 0.5837003588676453\n",
      "Epoch 2538, Loss: 2.502697616815567, Final Batch Loss: 0.25430944561958313\n",
      "Epoch 2539, Loss: 2.224681308493018, Final Batch Loss: 0.01219271682202816\n",
      "Epoch 2540, Loss: 2.087068872526288, Final Batch Loss: 0.0080768633633852\n",
      "Epoch 2541, Loss: 2.1749941147863865, Final Batch Loss: 0.03227132931351662\n",
      "Epoch 2542, Loss: 3.774739146232605, Final Batch Loss: 1.7814793586730957\n",
      "Epoch 2543, Loss: 2.4857053458690643, Final Batch Loss: 0.42456796765327454\n",
      "Epoch 2544, Loss: 2.1129013411700726, Final Batch Loss: 0.0415387861430645\n",
      "Epoch 2545, Loss: 2.2332659363746643, Final Batch Loss: 0.13080757856369019\n",
      "Epoch 2546, Loss: 2.052082911832258, Final Batch Loss: 0.0023084438871592283\n",
      "Epoch 2547, Loss: 3.352158308029175, Final Batch Loss: 1.3558865785598755\n",
      "Epoch 2548, Loss: 3.2662410140037537, Final Batch Loss: 1.2496092319488525\n",
      "Epoch 2549, Loss: 2.4147499203681946, Final Batch Loss: 0.3566739559173584\n",
      "Epoch 2550, Loss: 2.2063543051481247, Final Batch Loss: 0.14059458673000336\n",
      "Epoch 2551, Loss: 2.011862174840644, Final Batch Loss: 0.0002951186615973711\n",
      "Epoch 2552, Loss: 2.841658592224121, Final Batch Loss: 0.6834681630134583\n",
      "Epoch 2553, Loss: 3.445640981197357, Final Batch Loss: 1.3208467960357666\n",
      "Epoch 2554, Loss: 2.122449651360512, Final Batch Loss: 0.06773556768894196\n",
      "Epoch 2555, Loss: 2.0555975884199142, Final Batch Loss: 0.06600968539714813\n",
      "Epoch 2556, Loss: 2.052877091569826, Final Batch Loss: 0.0028166405390948057\n",
      "Epoch 2557, Loss: 2.0012806421145797, Final Batch Loss: 0.013921654783189297\n",
      "Epoch 2558, Loss: 2.3550370633602142, Final Batch Loss: 0.3396551311016083\n",
      "Epoch 2559, Loss: 2.3752151131629944, Final Batch Loss: 0.372489869594574\n",
      "Epoch 2560, Loss: 2.0546273663640022, Final Batch Loss: 0.025274045765399933\n",
      "Epoch 2561, Loss: 2.1319401264190674, Final Batch Loss: 0.18084758520126343\n",
      "Epoch 2562, Loss: 2.008931843098253, Final Batch Loss: 0.0052036321721971035\n",
      "Epoch 2563, Loss: 2.1921045035123825, Final Batch Loss: 0.22145698964595795\n",
      "Epoch 2564, Loss: 2.369966447353363, Final Batch Loss: 0.41106534004211426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2565, Loss: 3.988676428794861, Final Batch Loss: 2.0132923126220703\n",
      "Epoch 2566, Loss: 4.483457684516907, Final Batch Loss: 2.499556303024292\n",
      "Epoch 2567, Loss: 2.0561901554465294, Final Batch Loss: 0.08181127160787582\n",
      "Epoch 2568, Loss: 2.131717859796481, Final Batch Loss: 4.2437604861333966e-05\n",
      "Epoch 2569, Loss: 2.2084747552871704, Final Batch Loss: 0.049678683280944824\n",
      "Epoch 2570, Loss: 2.1200141159351915, Final Batch Loss: 0.0016999093350023031\n",
      "Epoch 2571, Loss: 2.1819693171419203, Final Batch Loss: 0.006250950042158365\n",
      "Epoch 2572, Loss: 2.1336638531647623, Final Batch Loss: 0.0027100048027932644\n",
      "Epoch 2573, Loss: 2.1269916277378798, Final Batch Loss: 0.011966465041041374\n",
      "Epoch 2574, Loss: 2.343811810016632, Final Batch Loss: 0.29501795768737793\n",
      "Epoch 2575, Loss: 3.825956344604492, Final Batch Loss: 1.8478286266326904\n",
      "Epoch 2576, Loss: 3.2507766485214233, Final Batch Loss: 1.1792203187942505\n",
      "Epoch 2577, Loss: 3.665291428565979, Final Batch Loss: 1.518007755279541\n",
      "Epoch 2578, Loss: 2.8463388681411743, Final Batch Loss: 0.44657576084136963\n",
      "Epoch 2579, Loss: 2.6134326549363323, Final Batch Loss: 0.0009033175301738083\n",
      "Epoch 2580, Loss: 2.4811756871640682, Final Batch Loss: 0.02550044283270836\n",
      "Epoch 2581, Loss: 2.6560484170913696, Final Batch Loss: 0.3985443115234375\n",
      "Epoch 2582, Loss: 2.9412859678268433, Final Batch Loss: 0.8574206829071045\n",
      "Epoch 2583, Loss: 2.779508948326111, Final Batch Loss: 0.7481436729431152\n",
      "Epoch 2584, Loss: 2.0316409543156624, Final Batch Loss: 0.0720914676785469\n",
      "Epoch 2585, Loss: 2.243695929646492, Final Batch Loss: 0.21026475727558136\n",
      "Epoch 2586, Loss: 2.202100306749344, Final Batch Loss: 0.2424878180027008\n",
      "Epoch 2587, Loss: 2.9535984992980957, Final Batch Loss: 0.8700758218765259\n",
      "Epoch 2588, Loss: 2.0630379915237427, Final Batch Loss: 0.09822332859039307\n",
      "Epoch 2589, Loss: 2.068119991570711, Final Batch Loss: 0.020056772977113724\n",
      "Epoch 2590, Loss: 2.6897764205932617, Final Batch Loss: 0.7042300701141357\n",
      "Epoch 2591, Loss: 1.9634293280541897, Final Batch Loss: 0.01990276202559471\n",
      "Epoch 2592, Loss: 3.6509116291999817, Final Batch Loss: 1.7090550661087036\n",
      "Epoch 2593, Loss: 2.066756136715412, Final Batch Loss: 0.016234345734119415\n",
      "Epoch 2594, Loss: 2.3952497243881226, Final Batch Loss: 0.40513962507247925\n",
      "Epoch 2595, Loss: 2.1865090131759644, Final Batch Loss: 0.10879015922546387\n",
      "Epoch 2596, Loss: 2.282089665532112, Final Batch Loss: 0.2117028385400772\n",
      "Epoch 2597, Loss: 3.652230679988861, Final Batch Loss: 1.6661310195922852\n",
      "Epoch 2598, Loss: 1.9746344275772572, Final Batch Loss: 0.039251167327165604\n",
      "Epoch 2599, Loss: 2.1642679274082184, Final Batch Loss: 0.10232517123222351\n",
      "Epoch 2600, Loss: 2.001709207892418, Final Batch Loss: 0.04467587172985077\n",
      "Epoch 2601, Loss: 2.543887108564377, Final Batch Loss: 0.4837842881679535\n",
      "Epoch 2602, Loss: 2.9432684183120728, Final Batch Loss: 0.8992432951927185\n",
      "Epoch 2603, Loss: 3.2641043066978455, Final Batch Loss: 1.3536834716796875\n",
      "Epoch 2604, Loss: 2.2454694360494614, Final Batch Loss: 0.2393345683813095\n",
      "Epoch 2605, Loss: 4.768123269081116, Final Batch Loss: 2.5075528621673584\n",
      "Epoch 2606, Loss: 2.2291019189869985, Final Batch Loss: 0.0006567466771230102\n",
      "Epoch 2607, Loss: 4.467378318309784, Final Batch Loss: 2.181443214416504\n",
      "Epoch 2608, Loss: 2.245352864265442, Final Batch Loss: 0.11247235536575317\n",
      "Epoch 2609, Loss: 2.4633140563964844, Final Batch Loss: 0.3656077980995178\n",
      "Epoch 2610, Loss: 2.174333732575178, Final Batch Loss: 0.062476735562086105\n",
      "Epoch 2611, Loss: 3.3663238883018494, Final Batch Loss: 1.3978731632232666\n",
      "Epoch 2612, Loss: 1.9849858879260864, Final Batch Loss: 1.2516897186287679e-05\n",
      "Epoch 2613, Loss: 2.011281872575637, Final Batch Loss: 0.0005552418879233301\n",
      "Epoch 2614, Loss: 1.9852622165344656, Final Batch Loss: 0.0005956306122243404\n",
      "Epoch 2615, Loss: 3.9405993223190308, Final Batch Loss: 1.9507904052734375\n",
      "Epoch 2616, Loss: 2.1139100939035416, Final Batch Loss: 0.21577949821949005\n",
      "Epoch 2617, Loss: 2.2171266973018646, Final Batch Loss: 0.3307592570781708\n",
      "Epoch 2618, Loss: 3.500470757484436, Final Batch Loss: 1.600834608078003\n",
      "Epoch 2619, Loss: 1.984129061922431, Final Batch Loss: 0.030263712629675865\n",
      "Epoch 2620, Loss: 1.985437409952283, Final Batch Loss: 0.012190597131848335\n",
      "Epoch 2621, Loss: 3.085508704185486, Final Batch Loss: 1.0683040618896484\n",
      "Epoch 2622, Loss: 2.090721635147929, Final Batch Loss: 0.010198859497904778\n",
      "Epoch 2623, Loss: 2.1307811737060547, Final Batch Loss: 0.12540870904922485\n",
      "Epoch 2624, Loss: 3.894642651081085, Final Batch Loss: 1.8293888568878174\n",
      "Epoch 2625, Loss: 2.324903756380081, Final Batch Loss: 0.2653883993625641\n",
      "Epoch 2626, Loss: 2.6085697412490845, Final Batch Loss: 0.5738152265548706\n",
      "Epoch 2627, Loss: 4.214578032493591, Final Batch Loss: 2.2245075702667236\n",
      "Epoch 2628, Loss: 2.5441761016845703, Final Batch Loss: 0.5171394348144531\n",
      "Epoch 2629, Loss: 1.970943607389927, Final Batch Loss: 0.045333780348300934\n",
      "Epoch 2630, Loss: 4.0665730237960815, Final Batch Loss: 2.109074831008911\n",
      "Epoch 2631, Loss: 2.0824805088341236, Final Batch Loss: 0.04259433224797249\n",
      "Epoch 2632, Loss: 3.1630178689956665, Final Batch Loss: 1.0657954216003418\n",
      "Epoch 2633, Loss: 3.689822018146515, Final Batch Loss: 1.7388670444488525\n",
      "Epoch 2634, Loss: 2.111884105950594, Final Batch Loss: 0.018504489213228226\n",
      "Epoch 2635, Loss: 3.56980437040329, Final Batch Loss: 1.3646297454833984\n",
      "Epoch 2636, Loss: 4.557549476623535, Final Batch Loss: 2.319387197494507\n",
      "Epoch 2637, Loss: 2.620867609977722, Final Batch Loss: 0.5281928777694702\n",
      "Epoch 2638, Loss: 3.339754283428192, Final Batch Loss: 1.2280088663101196\n",
      "Epoch 2639, Loss: 2.083990767598152, Final Batch Loss: 0.04871745407581329\n",
      "Epoch 2640, Loss: 2.08352954685688, Final Batch Loss: 0.046260252594947815\n",
      "Epoch 2641, Loss: 4.0197649002075195, Final Batch Loss: 1.9874696731567383\n",
      "Epoch 2642, Loss: 3.884415090084076, Final Batch Loss: 1.923579216003418\n",
      "Epoch 2643, Loss: 1.945617251098156, Final Batch Loss: 0.008304469287395477\n",
      "Epoch 2644, Loss: 3.07265442609787, Final Batch Loss: 1.1453075408935547\n",
      "Epoch 2645, Loss: 2.2963140308856964, Final Batch Loss: 0.33357420563697815\n",
      "Epoch 2646, Loss: 2.0847662687301636, Final Batch Loss: 0.11265045404434204\n",
      "Epoch 2647, Loss: 5.29412704706192, Final Batch Loss: 3.2863993644714355\n",
      "Epoch 2648, Loss: 2.992397367954254, Final Batch Loss: 0.9636930227279663\n",
      "Epoch 2649, Loss: 3.3841081261634827, Final Batch Loss: 1.2971988916397095\n",
      "Epoch 2650, Loss: 1.9056239677593112, Final Batch Loss: 0.0024178577587008476\n",
      "Epoch 2651, Loss: 3.7120321393013, Final Batch Loss: 1.7983766794204712\n",
      "Epoch 2652, Loss: 2.137084439396858, Final Batch Loss: 0.20993180572986603\n",
      "Epoch 2653, Loss: 1.9102791734039783, Final Batch Loss: 0.009582120925188065\n",
      "Epoch 2654, Loss: 4.505451917648315, Final Batch Loss: 2.487586736679077\n",
      "Epoch 2655, Loss: 2.6599497199058533, Final Batch Loss: 0.7103028893470764\n",
      "Epoch 2656, Loss: 2.0121921729296446, Final Batch Loss: 0.009985847398638725\n",
      "Epoch 2657, Loss: 2.299647554755211, Final Batch Loss: 0.0904843658208847\n",
      "Epoch 2658, Loss: 2.3388702869415283, Final Batch Loss: 0.00248873233795166\n",
      "Epoch 2659, Loss: 2.214398087002337, Final Batch Loss: 0.008473278023302555\n",
      "Epoch 2660, Loss: 3.342908263206482, Final Batch Loss: 1.24489426612854\n",
      "Epoch 2661, Loss: 2.3789598643779755, Final Batch Loss: 0.25832340121269226\n",
      "Epoch 2662, Loss: 3.445659577846527, Final Batch Loss: 1.3547370433807373\n",
      "Epoch 2663, Loss: 2.037268126383424, Final Batch Loss: 0.02223607338964939\n",
      "Epoch 2664, Loss: 2.9062947034835815, Final Batch Loss: 0.8364009857177734\n",
      "Epoch 2665, Loss: 3.791221797466278, Final Batch Loss: 1.6757550239562988\n",
      "Epoch 2666, Loss: 2.0383214809698984, Final Batch Loss: 0.0012559153838083148\n",
      "Epoch 2667, Loss: 5.809684872627258, Final Batch Loss: 3.7193613052368164\n",
      "Epoch 2668, Loss: 2.2143574655056, Final Batch Loss: 0.16265079379081726\n",
      "Epoch 2669, Loss: 2.4726924896240234, Final Batch Loss: 0.34862351417541504\n",
      "Epoch 2670, Loss: 3.60025691986084, Final Batch Loss: 1.3832390308380127\n",
      "Epoch 2671, Loss: 3.543331801891327, Final Batch Loss: 1.3948907852172852\n",
      "Epoch 2672, Loss: 2.3333710283041, Final Batch Loss: 0.20168249309062958\n",
      "Epoch 2673, Loss: 2.5060074031352997, Final Batch Loss: 0.43670913577079773\n",
      "Epoch 2674, Loss: 3.33341383934021, Final Batch Loss: 1.322394609451294\n",
      "Epoch 2675, Loss: 2.0593219585716724, Final Batch Loss: 0.02204729989171028\n",
      "Epoch 2676, Loss: 2.105035964399576, Final Batch Loss: 0.05494064465165138\n",
      "Epoch 2677, Loss: 4.429170668125153, Final Batch Loss: 2.4499123096466064\n",
      "Epoch 2678, Loss: 2.1750804483890533, Final Batch Loss: 0.1993618905544281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2679, Loss: 2.437934458255768, Final Batch Loss: 0.41820818185806274\n",
      "Epoch 2680, Loss: 4.034741282463074, Final Batch Loss: 1.9339988231658936\n",
      "Epoch 2681, Loss: 2.1715711504220963, Final Batch Loss: 0.08114714920520782\n",
      "Epoch 2682, Loss: 2.60491144657135, Final Batch Loss: 0.5570305585861206\n",
      "Epoch 2683, Loss: 2.0527215860784054, Final Batch Loss: 0.04675706848502159\n",
      "Epoch 2684, Loss: 3.5150500535964966, Final Batch Loss: 1.5659971237182617\n",
      "Epoch 2685, Loss: 2.697478771209717, Final Batch Loss: 0.6362580060958862\n",
      "Epoch 2686, Loss: 2.4040322601795197, Final Batch Loss: 0.4520517885684967\n",
      "Epoch 2687, Loss: 2.102758504450321, Final Batch Loss: 0.12037620693445206\n",
      "Epoch 2688, Loss: 3.2390644550323486, Final Batch Loss: 1.277305006980896\n",
      "Epoch 2689, Loss: 2.626687169075012, Final Batch Loss: 0.5656383037567139\n",
      "Epoch 2690, Loss: 2.406361073255539, Final Batch Loss: 0.3742363750934601\n",
      "Epoch 2691, Loss: 2.6916747093200684, Final Batch Loss: 0.638843297958374\n",
      "Epoch 2692, Loss: 3.5049105286598206, Final Batch Loss: 1.488385558128357\n",
      "Epoch 2693, Loss: 2.148269864730537, Final Batch Loss: 0.008213433437049389\n",
      "Epoch 2694, Loss: 3.8567091822624207, Final Batch Loss: 1.6831395626068115\n",
      "Epoch 2695, Loss: 5.73411226272583, Final Batch Loss: 3.693362236022949\n",
      "Epoch 2696, Loss: 2.4018149375915527, Final Batch Loss: 0.2731265425682068\n",
      "Epoch 2697, Loss: 2.7431315183639526, Final Batch Loss: 0.6168742179870605\n",
      "Epoch 2698, Loss: 2.2151086442172527, Final Batch Loss: 0.05197621509432793\n",
      "Epoch 2699, Loss: 4.163224637508392, Final Batch Loss: 2.162374496459961\n",
      "Epoch 2700, Loss: 4.647000849246979, Final Batch Loss: 2.5661673545837402\n",
      "Epoch 2701, Loss: 2.1695617251098156, Final Batch Loss: 0.01942632719874382\n",
      "Epoch 2702, Loss: 2.991700232028961, Final Batch Loss: 0.9139477610588074\n",
      "Epoch 2703, Loss: 2.5523669719696045, Final Batch Loss: 0.5217548608779907\n",
      "Epoch 2704, Loss: 2.844924032688141, Final Batch Loss: 0.8218781352043152\n",
      "Epoch 2705, Loss: 3.016330599784851, Final Batch Loss: 0.9939588308334351\n",
      "Epoch 2706, Loss: 2.885645806789398, Final Batch Loss: 0.7127735018730164\n",
      "Epoch 2707, Loss: 2.406289966776967, Final Batch Loss: 0.0007297713309526443\n",
      "Epoch 2708, Loss: 2.2444849871098995, Final Batch Loss: 0.01806965097784996\n",
      "Epoch 2709, Loss: 2.017279198858887, Final Batch Loss: 0.002186885569244623\n",
      "Epoch 2710, Loss: 3.424077272415161, Final Batch Loss: 1.4103007316589355\n",
      "Epoch 2711, Loss: 2.072568252682686, Final Batch Loss: 0.13519658148288727\n",
      "Epoch 2712, Loss: 2.0631425380661312, Final Batch Loss: 2.9802276912960224e-06\n",
      "Epoch 2713, Loss: 2.0291382428258657, Final Batch Loss: 0.015635168179869652\n",
      "Epoch 2714, Loss: 2.234640449285507, Final Batch Loss: 0.2995370924472809\n",
      "Epoch 2715, Loss: 2.094977706670761, Final Batch Loss: 0.08765712380409241\n",
      "Epoch 2716, Loss: 3.4487839937210083, Final Batch Loss: 1.4467926025390625\n",
      "Epoch 2717, Loss: 3.113815426826477, Final Batch Loss: 1.1329984664916992\n",
      "Epoch 2718, Loss: 3.537116765975952, Final Batch Loss: 1.4946435689926147\n",
      "Epoch 2719, Loss: 2.6447341442108154, Final Batch Loss: 0.6148155331611633\n",
      "Epoch 2720, Loss: 2.1383223086595535, Final Batch Loss: 0.18505986034870148\n",
      "Epoch 2721, Loss: 3.2213515639305115, Final Batch Loss: 1.2302861213684082\n",
      "Epoch 2722, Loss: 2.095521468669176, Final Batch Loss: 0.007661718875169754\n",
      "Epoch 2723, Loss: 4.0754982233047485, Final Batch Loss: 1.9906913042068481\n",
      "Epoch 2724, Loss: 2.004201175644994, Final Batch Loss: 0.018901588395237923\n",
      "Epoch 2725, Loss: 2.2164559066295624, Final Batch Loss: 0.1353343427181244\n",
      "Epoch 2726, Loss: 2.2015810012817383, Final Batch Loss: 0.15565890073776245\n",
      "Epoch 2727, Loss: 3.30551016330719, Final Batch Loss: 1.3716247081756592\n",
      "Epoch 2728, Loss: 3.913375675678253, Final Batch Loss: 1.8480840921401978\n",
      "Epoch 2729, Loss: 1.9382931916043162, Final Batch Loss: 0.010987612418830395\n",
      "Epoch 2730, Loss: 2.149159625172615, Final Batch Loss: 0.16255666315555573\n",
      "Epoch 2731, Loss: 2.282941162586212, Final Batch Loss: 0.28984296321868896\n",
      "Epoch 2732, Loss: 2.1861951239407063, Final Batch Loss: 0.057112980633974075\n",
      "Epoch 2733, Loss: 2.632513642311096, Final Batch Loss: 0.6969659924507141\n",
      "Epoch 2734, Loss: 1.9272088259458542, Final Batch Loss: 0.021504566073417664\n",
      "Epoch 2735, Loss: 2.092001885175705, Final Batch Loss: 0.15284547209739685\n",
      "Epoch 2736, Loss: 1.9950265251100063, Final Batch Loss: 0.05309801921248436\n",
      "Epoch 2737, Loss: 2.324019104242325, Final Batch Loss: 0.38407668471336365\n",
      "Epoch 2738, Loss: 2.2247387766838074, Final Batch Loss: 0.33496683835983276\n",
      "Epoch 2739, Loss: 1.9381157644093037, Final Batch Loss: 0.04036145284771919\n",
      "Epoch 2740, Loss: 1.9605335779488087, Final Batch Loss: 0.06021793559193611\n",
      "Epoch 2741, Loss: 3.2165873646736145, Final Batch Loss: 1.3324350118637085\n",
      "Epoch 2742, Loss: 2.087192714214325, Final Batch Loss: 0.13597166538238525\n",
      "Epoch 2743, Loss: 1.9750556908547878, Final Batch Loss: 0.03402065858244896\n",
      "Epoch 2744, Loss: 1.933650765568018, Final Batch Loss: 0.035750601440668106\n",
      "Epoch 2745, Loss: 2.004624657332897, Final Batch Loss: 0.09804945439100266\n",
      "Epoch 2746, Loss: 4.03188419342041, Final Batch Loss: 2.153494119644165\n",
      "Epoch 2747, Loss: 1.9301045266911387, Final Batch Loss: 0.011169525794684887\n",
      "Epoch 2748, Loss: 1.9489073522854596, Final Batch Loss: 0.001994288759306073\n",
      "Epoch 2749, Loss: 2.606672704219818, Final Batch Loss: 0.6870329976081848\n",
      "Epoch 2750, Loss: 2.334188014268875, Final Batch Loss: 0.40440675616264343\n",
      "Epoch 2751, Loss: 2.8359885215759277, Final Batch Loss: 0.9287633299827576\n",
      "Epoch 2752, Loss: 2.1386205703020096, Final Batch Loss: 0.22692067921161652\n",
      "Epoch 2753, Loss: 1.9962247917428613, Final Batch Loss: 0.0027060816064476967\n",
      "Epoch 2754, Loss: 2.4503125846385956, Final Batch Loss: 0.4878094494342804\n",
      "Epoch 2755, Loss: 2.746468722820282, Final Batch Loss: 0.8592278361320496\n",
      "Epoch 2756, Loss: 3.9409393072128296, Final Batch Loss: 2.053525686264038\n",
      "Epoch 2757, Loss: 1.9294974595541134, Final Batch Loss: 0.0009975224966183305\n",
      "Epoch 2758, Loss: 2.1684428453445435, Final Batch Loss: 0.3208675980567932\n",
      "Epoch 2759, Loss: 2.124514654278755, Final Batch Loss: 0.24087782204151154\n",
      "Epoch 2760, Loss: 1.9111043028533459, Final Batch Loss: 0.007478337734937668\n",
      "Epoch 2761, Loss: 2.132178485393524, Final Batch Loss: 0.20884019136428833\n",
      "Epoch 2762, Loss: 2.026222076267004, Final Batch Loss: 0.01234438642859459\n",
      "Epoch 2763, Loss: 2.03612176887691, Final Batch Loss: 0.010839981958270073\n",
      "Epoch 2764, Loss: 1.9956447035074234, Final Batch Loss: 0.06379778683185577\n",
      "Epoch 2765, Loss: 1.8884771172888577, Final Batch Loss: 0.007511467207223177\n",
      "Epoch 2766, Loss: 2.5892735719680786, Final Batch Loss: 0.644655168056488\n",
      "Epoch 2767, Loss: 2.8573458790779114, Final Batch Loss: 0.993518054485321\n",
      "Epoch 2768, Loss: 2.2583657801151276, Final Batch Loss: 0.31526198983192444\n",
      "Epoch 2769, Loss: 2.0308158844709396, Final Batch Loss: 0.1187504380941391\n",
      "Epoch 2770, Loss: 1.9611360095441341, Final Batch Loss: 0.04266127571463585\n",
      "Epoch 2771, Loss: 2.3038629293441772, Final Batch Loss: 0.3406776785850525\n",
      "Epoch 2772, Loss: 1.9737148508429527, Final Batch Loss: 0.09804912656545639\n",
      "Epoch 2773, Loss: 2.403473734855652, Final Batch Loss: 0.5008916854858398\n",
      "Epoch 2774, Loss: 1.84602912212722, Final Batch Loss: 0.0023830130230635405\n",
      "Epoch 2775, Loss: 2.7766937613487244, Final Batch Loss: 0.9309557676315308\n",
      "Epoch 2776, Loss: 2.8887380361557007, Final Batch Loss: 1.0641889572143555\n",
      "Epoch 2777, Loss: 2.2835913002490997, Final Batch Loss: 0.3933200538158417\n",
      "Epoch 2778, Loss: 2.069921836256981, Final Batch Loss: 0.1738291233778\n",
      "Epoch 2779, Loss: 1.8362842127680779, Final Batch Loss: 0.0165669247508049\n",
      "Epoch 2780, Loss: 3.3441343903541565, Final Batch Loss: 1.4744720458984375\n",
      "Epoch 2781, Loss: 2.163901448249817, Final Batch Loss: 0.1746124029159546\n",
      "Epoch 2782, Loss: 3.5081058144569397, Final Batch Loss: 1.6302809715270996\n",
      "Epoch 2783, Loss: 1.902722549799364, Final Batch Loss: 0.0006724718841724098\n",
      "Epoch 2784, Loss: 2.3515487015247345, Final Batch Loss: 0.37431278824806213\n",
      "Epoch 2785, Loss: 2.709782838821411, Final Batch Loss: 0.8010141849517822\n",
      "Epoch 2786, Loss: 1.9753038845956326, Final Batch Loss: 0.00586443766951561\n",
      "Epoch 2787, Loss: 2.2897324562072754, Final Batch Loss: 0.36927181482315063\n",
      "Epoch 2788, Loss: 2.4343029856681824, Final Batch Loss: 0.5127271413803101\n",
      "Epoch 2789, Loss: 2.033406749367714, Final Batch Loss: 0.21009637415409088\n",
      "Epoch 2790, Loss: 1.8606014531105757, Final Batch Loss: 0.004016784951090813\n",
      "Epoch 2791, Loss: 2.339636266231537, Final Batch Loss: 0.4573482275009155\n",
      "Epoch 2792, Loss: 1.8820156699512154, Final Batch Loss: 0.001939917216077447\n",
      "Epoch 2793, Loss: 1.9353420846164227, Final Batch Loss: 0.037100862711668015\n",
      "Epoch 2794, Loss: 1.8691383972764015, Final Batch Loss: 0.03026799112558365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2795, Loss: 2.0024011480272748, Final Batch Loss: 0.00041154498467221856\n",
      "Epoch 2796, Loss: 3.705736994743347, Final Batch Loss: 1.7831286191940308\n",
      "Epoch 2797, Loss: 1.9065061882138252, Final Batch Loss: 0.004036732017993927\n",
      "Epoch 2798, Loss: 2.227403908967972, Final Batch Loss: 0.40714582800865173\n",
      "Epoch 2799, Loss: 2.5751689672470093, Final Batch Loss: 0.753910481929779\n",
      "Epoch 2800, Loss: 1.8186194298323244, Final Batch Loss: 0.0029283759649842978\n",
      "Epoch 2801, Loss: 1.9269073526374996, Final Batch Loss: 0.0026085893623530865\n",
      "Epoch 2802, Loss: 1.9267435716465116, Final Batch Loss: 0.010566596873104572\n",
      "Epoch 2803, Loss: 1.9067689096555114, Final Batch Loss: 0.010002843104302883\n",
      "Epoch 2804, Loss: 1.860141990298871, Final Batch Loss: 6.735097849741578e-05\n",
      "Epoch 2805, Loss: 2.799724817276001, Final Batch Loss: 0.9727885127067566\n",
      "Epoch 2806, Loss: 1.9014673568308353, Final Batch Loss: 0.00592866912484169\n",
      "Epoch 2807, Loss: 1.9608066261280328, Final Batch Loss: 0.0038220465648919344\n",
      "Epoch 2808, Loss: 2.0760999619960785, Final Batch Loss: 0.17967143654823303\n",
      "Epoch 2809, Loss: 2.955803096294403, Final Batch Loss: 1.0730884075164795\n",
      "Epoch 2810, Loss: 1.9466463774442673, Final Batch Loss: 0.00997970998287201\n",
      "Epoch 2811, Loss: 2.0076986530257273, Final Batch Loss: 6.48477507638745e-05\n",
      "Epoch 2812, Loss: 4.639274716377258, Final Batch Loss: 2.6945319175720215\n",
      "Epoch 2813, Loss: 1.8803897462785244, Final Batch Loss: 0.023887690156698227\n",
      "Epoch 2814, Loss: 1.9949247976765037, Final Batch Loss: 0.011567308567464352\n",
      "Epoch 2815, Loss: 2.267165035009384, Final Batch Loss: 0.3531559407711029\n",
      "Epoch 2816, Loss: 1.8141158185899258, Final Batch Loss: 0.03346836194396019\n",
      "Epoch 2817, Loss: 2.7424159049987793, Final Batch Loss: 0.8599243760108948\n",
      "Epoch 2818, Loss: 1.9026644863188267, Final Batch Loss: 0.040982846170663834\n",
      "Epoch 2819, Loss: 2.6910073161125183, Final Batch Loss: 0.8427525758743286\n",
      "Epoch 2820, Loss: 2.9267881512641907, Final Batch Loss: 1.1032403707504272\n",
      "Epoch 2821, Loss: 1.924848545837449, Final Batch Loss: 0.0001461399078834802\n",
      "Epoch 2822, Loss: 3.0392595529556274, Final Batch Loss: 1.0820749998092651\n",
      "Epoch 2823, Loss: 1.9717890918254852, Final Batch Loss: 0.17185577750205994\n",
      "Epoch 2824, Loss: 1.8243563389405608, Final Batch Loss: 0.007935426197946072\n",
      "Epoch 2825, Loss: 2.075061619281769, Final Batch Loss: 0.2811475992202759\n",
      "Epoch 2826, Loss: 2.613634467124939, Final Batch Loss: 0.7011538147926331\n",
      "Epoch 2827, Loss: 2.027292713522911, Final Batch Loss: 0.24727515876293182\n",
      "Epoch 2828, Loss: 1.8745070688564738, Final Batch Loss: 3.8265450712060556e-05\n",
      "Epoch 2829, Loss: 2.672970950603485, Final Batch Loss: 0.7811989188194275\n",
      "Epoch 2830, Loss: 1.9631225436241948, Final Batch Loss: 7.807903602952138e-05\n",
      "Epoch 2831, Loss: 4.322970688343048, Final Batch Loss: 2.515611410140991\n",
      "Epoch 2832, Loss: 3.3783637285232544, Final Batch Loss: 1.555242896080017\n",
      "Epoch 2833, Loss: 1.9016808606684208, Final Batch Loss: 0.02318534627556801\n",
      "Epoch 2834, Loss: 1.8275768924504519, Final Batch Loss: 0.004507856443524361\n",
      "Epoch 2835, Loss: 3.082123279571533, Final Batch Loss: 1.2531201839447021\n",
      "Epoch 2836, Loss: 1.8377561322413385, Final Batch Loss: 0.003189241047948599\n",
      "Epoch 2837, Loss: 1.9181554540991783, Final Batch Loss: 0.005295179784297943\n",
      "Epoch 2838, Loss: 1.9880887176841497, Final Batch Loss: 0.013396253809332848\n",
      "Epoch 2839, Loss: 3.630735158920288, Final Batch Loss: 1.6933116912841797\n",
      "Epoch 2840, Loss: 1.941760340181645, Final Batch Loss: 0.000205018965061754\n",
      "Epoch 2841, Loss: 1.8983473561238497, Final Batch Loss: 0.0005306981038302183\n",
      "Epoch 2842, Loss: 3.7527634501457214, Final Batch Loss: 1.8801100254058838\n",
      "Epoch 2843, Loss: 3.6164549589157104, Final Batch Loss: 1.7707082033157349\n",
      "Epoch 2844, Loss: 1.8722235700115561, Final Batch Loss: 0.005580916069447994\n",
      "Epoch 2845, Loss: 2.527478814125061, Final Batch Loss: 0.651015043258667\n",
      "Epoch 2846, Loss: 1.8634666055440903, Final Batch Loss: 0.016506537795066833\n",
      "Epoch 2847, Loss: 1.8319936469197273, Final Batch Loss: 0.008423395454883575\n",
      "Epoch 2848, Loss: 3.16216641664505, Final Batch Loss: 1.2908703088760376\n",
      "Epoch 2849, Loss: 1.8908503092825413, Final Batch Loss: 0.02177894487977028\n",
      "Epoch 2850, Loss: 2.2049185931682587, Final Batch Loss: 0.2819118797779083\n",
      "Epoch 2851, Loss: 2.080898866057396, Final Batch Loss: 0.18935202062129974\n",
      "Epoch 2852, Loss: 4.605674147605896, Final Batch Loss: 2.735146999359131\n",
      "Epoch 2853, Loss: 3.6101102232933044, Final Batch Loss: 1.6782164573669434\n",
      "Epoch 2854, Loss: 1.8934888113290071, Final Batch Loss: 0.017107771709561348\n",
      "Epoch 2855, Loss: 2.0065662562847137, Final Batch Loss: 0.19590726494789124\n",
      "Epoch 2856, Loss: 3.0977073311805725, Final Batch Loss: 1.2651653289794922\n",
      "Epoch 2857, Loss: 4.042467355728149, Final Batch Loss: 2.1568551063537598\n",
      "Epoch 2858, Loss: 1.9498622678220272, Final Batch Loss: 0.03972741588950157\n",
      "Epoch 2859, Loss: 2.103865472599864, Final Batch Loss: 0.0029187481850385666\n",
      "Epoch 2860, Loss: 2.6560820937156677, Final Batch Loss: 0.5829622149467468\n",
      "Epoch 2861, Loss: 2.384182557463646, Final Batch Loss: 0.2072814553976059\n",
      "Epoch 2862, Loss: 5.551107943058014, Final Batch Loss: 3.503538131713867\n",
      "Epoch 2863, Loss: 2.102384312544018, Final Batch Loss: 0.000520570669323206\n",
      "Epoch 2864, Loss: 2.757807493209839, Final Batch Loss: 0.7331576347351074\n",
      "Epoch 2865, Loss: 2.051305241882801, Final Batch Loss: 0.06369198113679886\n",
      "Epoch 2866, Loss: 2.7908987402915955, Final Batch Loss: 0.8396906852722168\n",
      "Epoch 2867, Loss: 4.757517635822296, Final Batch Loss: 2.8111472129821777\n",
      "Epoch 2868, Loss: 3.221627414226532, Final Batch Loss: 1.1409924030303955\n",
      "Epoch 2869, Loss: 3.7731271982192993, Final Batch Loss: 1.5646793842315674\n",
      "Epoch 2870, Loss: 2.383256658911705, Final Batch Loss: 0.21175549924373627\n",
      "Epoch 2871, Loss: 2.043250740505755, Final Batch Loss: 0.002887011505663395\n",
      "Epoch 2872, Loss: 2.137880615890026, Final Batch Loss: 0.11800669878721237\n",
      "Epoch 2873, Loss: 2.6044980883598328, Final Batch Loss: 0.6106852293014526\n",
      "Epoch 2874, Loss: 3.7144787907600403, Final Batch Loss: 1.7002387046813965\n",
      "Epoch 2875, Loss: 2.3923544883728027, Final Batch Loss: 0.46437329053878784\n",
      "Epoch 2876, Loss: 2.4042198061943054, Final Batch Loss: 0.3351995348930359\n",
      "Epoch 2877, Loss: 2.195245534181595, Final Batch Loss: 0.27286162972450256\n",
      "Epoch 2878, Loss: 2.0053520631045103, Final Batch Loss: 0.030507368966937065\n",
      "Epoch 2879, Loss: 1.9427556246519089, Final Batch Loss: 0.03819678723812103\n",
      "Epoch 2880, Loss: 2.39447745680809, Final Batch Loss: 0.3805428445339203\n",
      "Epoch 2881, Loss: 2.3318806290626526, Final Batch Loss: 0.37439626455307007\n",
      "Epoch 2882, Loss: 2.196493983268738, Final Batch Loss: 0.2510090470314026\n",
      "Epoch 2883, Loss: 2.206997573375702, Final Batch Loss: 0.18539583683013916\n",
      "Epoch 2884, Loss: 3.4484514594078064, Final Batch Loss: 1.5761585235595703\n",
      "Epoch 2885, Loss: 4.038442075252533, Final Batch Loss: 2.1189892292022705\n",
      "Epoch 2886, Loss: 4.42152738571167, Final Batch Loss: 2.538717269897461\n",
      "Epoch 2887, Loss: 3.3793400526046753, Final Batch Loss: 1.499428153038025\n",
      "Epoch 2888, Loss: 2.097486212849617, Final Batch Loss: 0.12096430361270905\n",
      "Epoch 2889, Loss: 1.9395435131154954, Final Batch Loss: 0.0024291551671922207\n",
      "Epoch 2890, Loss: 2.003510795067996, Final Batch Loss: 0.0032303552143275738\n",
      "Epoch 2891, Loss: 1.9387959836094524, Final Batch Loss: 1.9073304429184645e-05\n",
      "Epoch 2892, Loss: 2.536219358444214, Final Batch Loss: 0.5793061852455139\n",
      "Epoch 2893, Loss: 2.3317200541496277, Final Batch Loss: 0.4296519160270691\n",
      "Epoch 2894, Loss: 2.0609191209077835, Final Batch Loss: 0.15448440611362457\n",
      "Epoch 2895, Loss: 1.9777798657305539, Final Batch Loss: 0.0028246049769222736\n",
      "Epoch 2896, Loss: 4.154855906963348, Final Batch Loss: 2.22867751121521\n",
      "Epoch 2897, Loss: 2.4073185324668884, Final Batch Loss: 0.4414886236190796\n",
      "Epoch 2898, Loss: 1.9134305585175753, Final Batch Loss: 0.019322501495480537\n",
      "Epoch 2899, Loss: 2.237890213727951, Final Batch Loss: 0.2721554934978485\n",
      "Epoch 2900, Loss: 2.1267712861299515, Final Batch Loss: 0.22495104372501373\n",
      "Epoch 2901, Loss: 2.04264809936285, Final Batch Loss: 0.007637940347194672\n",
      "Epoch 2902, Loss: 2.5819031596183777, Final Batch Loss: 0.6485195159912109\n",
      "Epoch 2903, Loss: 2.1147360801696777, Final Batch Loss: 0.2078002691268921\n",
      "Epoch 2904, Loss: 2.10846858471632, Final Batch Loss: 0.12109833210706711\n",
      "Epoch 2905, Loss: 2.6533586382865906, Final Batch Loss: 0.6329837441444397\n",
      "Epoch 2906, Loss: 2.0210507856681943, Final Batch Loss: 0.00939921010285616\n",
      "Epoch 2907, Loss: 4.654272496700287, Final Batch Loss: 2.6105802059173584\n",
      "Epoch 2908, Loss: 2.288225411437452, Final Batch Loss: 0.011936663649976254\n",
      "Epoch 2909, Loss: 3.196869432926178, Final Batch Loss: 1.0391902923583984\n",
      "Epoch 2910, Loss: 2.829743504524231, Final Batch Loss: 0.7560657262802124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2911, Loss: 2.6305575370788574, Final Batch Loss: 0.47892123460769653\n",
      "Epoch 2912, Loss: 2.3114946335554123, Final Batch Loss: 0.11895452439785004\n",
      "Epoch 2913, Loss: 2.211544451653026, Final Batch Loss: 0.001692411839030683\n",
      "Epoch 2914, Loss: 2.6710253953933716, Final Batch Loss: 0.5873504877090454\n",
      "Epoch 2915, Loss: 3.452233374118805, Final Batch Loss: 1.510035514831543\n",
      "Epoch 2916, Loss: 2.812812387943268, Final Batch Loss: 0.8674766421318054\n",
      "Epoch 2917, Loss: 2.7820687890052795, Final Batch Loss: 0.872810959815979\n",
      "Epoch 2918, Loss: 2.7646071910858154, Final Batch Loss: 0.8530040979385376\n",
      "Epoch 2919, Loss: 1.9102362412959337, Final Batch Loss: 0.005698980763554573\n",
      "Epoch 2920, Loss: 2.662462055683136, Final Batch Loss: 0.7396764755249023\n",
      "Epoch 2921, Loss: 2.8147798776626587, Final Batch Loss: 0.9371193051338196\n",
      "Epoch 2922, Loss: 2.7103204131126404, Final Batch Loss: 0.7856108546257019\n",
      "Epoch 2923, Loss: 2.0494853407144547, Final Batch Loss: 0.10739792883396149\n",
      "Epoch 2924, Loss: 1.989179641008377, Final Batch Loss: 0.060158222913742065\n",
      "Epoch 2925, Loss: 2.0740793496370316, Final Batch Loss: 0.17272795736789703\n",
      "Epoch 2926, Loss: 1.8479662956669927, Final Batch Loss: 0.011389357037842274\n",
      "Epoch 2927, Loss: 1.869946918450296, Final Batch Loss: 0.0024070357903838158\n",
      "Epoch 2928, Loss: 2.303388476371765, Final Batch Loss: 0.4056655168533325\n",
      "Epoch 2929, Loss: 3.1118953227996826, Final Batch Loss: 1.266084909439087\n",
      "Epoch 2930, Loss: 1.9918163567781448, Final Batch Loss: 0.06685365736484528\n",
      "Epoch 2931, Loss: 2.1737816813983954, Final Batch Loss: 0.0007070187130011618\n",
      "Epoch 2932, Loss: 2.4915046888636425, Final Batch Loss: 0.001016457681544125\n",
      "Epoch 2933, Loss: 3.6683305501937866, Final Batch Loss: 1.4166982173919678\n",
      "Epoch 2934, Loss: 2.895812153816223, Final Batch Loss: 0.6526048183441162\n",
      "Epoch 2935, Loss: 2.030810620635748, Final Batch Loss: 0.051128651946783066\n",
      "Epoch 2936, Loss: 2.5282845497131348, Final Batch Loss: 0.6130683422088623\n",
      "Epoch 2937, Loss: 2.0322386026382446, Final Batch Loss: 0.11378562450408936\n",
      "Epoch 2938, Loss: 2.7879382371902466, Final Batch Loss: 0.6853768825531006\n",
      "Epoch 2939, Loss: 2.247586250305176, Final Batch Loss: 0.332211971282959\n",
      "Epoch 2940, Loss: 2.0960487332195044, Final Batch Loss: 0.004751106724143028\n",
      "Epoch 2941, Loss: 1.889610973186791, Final Batch Loss: 0.012218859978020191\n",
      "Epoch 2942, Loss: 1.9621390011161566, Final Batch Loss: 0.015675773844122887\n",
      "Epoch 2943, Loss: 1.9401084249839187, Final Batch Loss: 0.012474841438233852\n",
      "Epoch 2944, Loss: 2.5451395511627197, Final Batch Loss: 0.6484040021896362\n",
      "Epoch 2945, Loss: 3.497958242893219, Final Batch Loss: 1.6032984256744385\n",
      "Epoch 2946, Loss: 2.4973509311676025, Final Batch Loss: 0.6605527400970459\n",
      "Epoch 2947, Loss: 3.3576038479804993, Final Batch Loss: 1.4653595685958862\n",
      "Epoch 2948, Loss: 4.107109546661377, Final Batch Loss: 2.199979305267334\n",
      "Epoch 2949, Loss: 1.9701517466455698, Final Batch Loss: 0.008083367720246315\n",
      "Epoch 2950, Loss: 3.000307083129883, Final Batch Loss: 0.9161989092826843\n",
      "Epoch 2951, Loss: 5.294576406478882, Final Batch Loss: 3.220526695251465\n",
      "Epoch 2952, Loss: 2.6080294847488403, Final Batch Loss: 0.6852315664291382\n",
      "Epoch 2953, Loss: 2.3332362044602633, Final Batch Loss: 0.0009274948388338089\n",
      "Epoch 2954, Loss: 2.925879344344139, Final Batch Loss: 0.06873957812786102\n",
      "Epoch 2955, Loss: 3.1887310743200032, Final Batch Loss: 5.125986263010418e-06\n",
      "Epoch 2956, Loss: 6.488027632236481, Final Batch Loss: 3.808058738708496\n",
      "Epoch 2957, Loss: 3.847319483757019, Final Batch Loss: 1.642822027206421\n",
      "Epoch 2958, Loss: 3.724059224128723, Final Batch Loss: 1.602274775505066\n",
      "Epoch 2959, Loss: 2.178370490670204, Final Batch Loss: 0.1733853965997696\n",
      "Epoch 2960, Loss: 2.0554127824143507, Final Batch Loss: 0.0005756151513196528\n",
      "Epoch 2961, Loss: 2.1027505612000823, Final Batch Loss: 0.014403305016458035\n",
      "Epoch 2962, Loss: 3.8844131231307983, Final Batch Loss: 1.630828857421875\n",
      "Epoch 2963, Loss: 4.046365857124329, Final Batch Loss: 1.9943277835845947\n",
      "Epoch 2964, Loss: 1.9577453350648284, Final Batch Loss: 0.015518388710916042\n",
      "Epoch 2965, Loss: 3.8399513959884644, Final Batch Loss: 1.7692737579345703\n",
      "Epoch 2966, Loss: 2.797136604785919, Final Batch Loss: 0.7916904091835022\n",
      "Epoch 2967, Loss: 2.0141566917300224, Final Batch Loss: 0.01683937758207321\n",
      "Epoch 2968, Loss: 1.9794896645471454, Final Batch Loss: 0.015440802089869976\n",
      "Epoch 2969, Loss: 1.9903318844735622, Final Batch Loss: 0.018314775079488754\n",
      "Epoch 2970, Loss: 4.023133635520935, Final Batch Loss: 2.161186695098877\n",
      "Epoch 2971, Loss: 1.9227249082177877, Final Batch Loss: 0.012108629569411278\n",
      "Epoch 2972, Loss: 2.221894234418869, Final Batch Loss: 0.2787156403064728\n",
      "Epoch 2973, Loss: 1.9502281695604324, Final Batch Loss: 0.09092845022678375\n",
      "Epoch 2974, Loss: 1.8764260169118643, Final Batch Loss: 0.029092824086546898\n",
      "Epoch 2975, Loss: 2.193323850631714, Final Batch Loss: 0.3181663751602173\n",
      "Epoch 2976, Loss: 1.860097199678421, Final Batch Loss: 0.03358975052833557\n",
      "Epoch 2977, Loss: 1.901518914848566, Final Batch Loss: 0.03883454576134682\n",
      "Epoch 2978, Loss: 3.4757789969444275, Final Batch Loss: 1.6313554048538208\n",
      "Epoch 2979, Loss: 3.5727370977401733, Final Batch Loss: 1.7059777975082397\n",
      "Epoch 2980, Loss: 2.9612600803375244, Final Batch Loss: 1.2035703659057617\n",
      "Epoch 2981, Loss: 2.0701882541179657, Final Batch Loss: 0.28458794951438904\n",
      "Epoch 2982, Loss: 4.067372143268585, Final Batch Loss: 2.1819260120391846\n",
      "Epoch 2983, Loss: 1.9706459399312735, Final Batch Loss: 0.022619223222136497\n",
      "Epoch 2984, Loss: 1.872997771948576, Final Batch Loss: 0.02914203330874443\n",
      "Epoch 2985, Loss: 3.30189973115921, Final Batch Loss: 1.412246584892273\n",
      "Epoch 2986, Loss: 2.5029378533363342, Final Batch Loss: 0.6502652168273926\n",
      "Epoch 2987, Loss: 1.9831574968993664, Final Batch Loss: 0.05221746489405632\n",
      "Epoch 2988, Loss: 2.6438539028167725, Final Batch Loss: 0.7933211326599121\n",
      "Epoch 2989, Loss: 2.051312029361725, Final Batch Loss: 0.20993471145629883\n",
      "Epoch 2990, Loss: 3.4303457736968994, Final Batch Loss: 1.5278878211975098\n",
      "Epoch 2991, Loss: 2.1156307458877563, Final Batch Loss: 0.2031838297843933\n",
      "Epoch 2992, Loss: 3.11752051115036, Final Batch Loss: 1.1963508129119873\n",
      "Epoch 2993, Loss: 5.510520696640015, Final Batch Loss: 3.6735806465148926\n",
      "Epoch 2994, Loss: 2.4064636826515198, Final Batch Loss: 0.49159950017929077\n",
      "Epoch 2995, Loss: 3.468103349208832, Final Batch Loss: 1.5570695400238037\n",
      "Epoch 2996, Loss: 3.3139967918395996, Final Batch Loss: 1.3982212543487549\n",
      "Epoch 2997, Loss: 3.1360522508621216, Final Batch Loss: 1.2067087888717651\n",
      "Epoch 2998, Loss: 2.0106535106897354, Final Batch Loss: 0.058686450123786926\n",
      "Epoch 2999, Loss: 1.945658219512552, Final Batch Loss: 0.006496382411569357\n",
      "Epoch 3000, Loss: 2.4042526483535767, Final Batch Loss: 0.543873131275177\n",
      "Epoch 3001, Loss: 3.3940226435661316, Final Batch Loss: 1.5106935501098633\n",
      "Epoch 3002, Loss: 3.4877023100852966, Final Batch Loss: 1.6066415309906006\n",
      "Epoch 3003, Loss: 2.334223687648773, Final Batch Loss: 0.4415634274482727\n",
      "Epoch 3004, Loss: 1.7492437957680522, Final Batch Loss: 1.4424220353248529e-05\n",
      "Epoch 3005, Loss: 1.9085828214883804, Final Batch Loss: 0.06294138729572296\n",
      "Epoch 3006, Loss: 1.94894140958786, Final Batch Loss: 0.09208559989929199\n",
      "Epoch 3007, Loss: 2.2312121987342834, Final Batch Loss: 0.44456321001052856\n",
      "Epoch 3008, Loss: 2.02133572101593, Final Batch Loss: 0.15893679857254028\n",
      "Epoch 3009, Loss: 2.274877429008484, Final Batch Loss: 0.4178065061569214\n",
      "Epoch 3010, Loss: 3.8416017293930054, Final Batch Loss: 2.111739158630371\n",
      "Epoch 3011, Loss: 2.679729461669922, Final Batch Loss: 0.7817111611366272\n",
      "Epoch 3012, Loss: 2.110331743955612, Final Batch Loss: 0.2302745282649994\n",
      "Epoch 3013, Loss: 2.9720394015312195, Final Batch Loss: 0.9888710975646973\n",
      "Epoch 3014, Loss: 2.018154304474592, Final Batch Loss: 0.0388057641685009\n",
      "Epoch 3015, Loss: 2.0358683317899704, Final Batch Loss: 0.1584731787443161\n",
      "Epoch 3016, Loss: 1.8413063576444983, Final Batch Loss: 0.004622488282620907\n",
      "Epoch 3017, Loss: 2.0630133152008057, Final Batch Loss: 0.2644956111907959\n",
      "Epoch 3018, Loss: 1.9147669076919556, Final Batch Loss: 0.02501922845840454\n",
      "Epoch 3019, Loss: 2.036796271800995, Final Batch Loss: 0.16411280632019043\n",
      "Epoch 3020, Loss: 1.8591872556135058, Final Batch Loss: 0.010666619054973125\n",
      "Epoch 3021, Loss: 1.9030883847735822, Final Batch Loss: 0.006247514393180609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3022, Loss: 2.5902615785598755, Final Batch Loss: 0.6684213280677795\n",
      "Epoch 3023, Loss: 2.024781122803688, Final Batch Loss: 0.2170829325914383\n",
      "Epoch 3024, Loss: 2.0954605489969254, Final Batch Loss: 0.21254028379917145\n",
      "Epoch 3025, Loss: 2.415709614753723, Final Batch Loss: 0.580798327922821\n",
      "Epoch 3026, Loss: 2.0764726102352142, Final Batch Loss: 0.3303077518939972\n",
      "Epoch 3027, Loss: 1.845852161408402, Final Batch Loss: 0.001447463990189135\n",
      "Epoch 3028, Loss: 3.6594133377075195, Final Batch Loss: 1.872869849205017\n",
      "Epoch 3029, Loss: 2.2592972218990326, Final Batch Loss: 0.47356387972831726\n",
      "Epoch 3030, Loss: 1.8791894223541021, Final Batch Loss: 0.009327879175543785\n",
      "Epoch 3031, Loss: 1.8148550223559141, Final Batch Loss: 0.030343396589159966\n",
      "Epoch 3032, Loss: 1.8270509835583653, Final Batch Loss: 2.634490556374658e-05\n",
      "Epoch 3033, Loss: 1.8982337545603514, Final Batch Loss: 0.012414796277880669\n",
      "Epoch 3034, Loss: 1.8153412910178304, Final Batch Loss: 0.00835388619452715\n",
      "Epoch 3035, Loss: 3.407615900039673, Final Batch Loss: 1.659273624420166\n",
      "Epoch 3036, Loss: 4.05592006444931, Final Batch Loss: 2.2878472805023193\n",
      "Epoch 3037, Loss: 1.8457600400579395, Final Batch Loss: 0.00012146688823122531\n",
      "Epoch 3038, Loss: 2.1408817172050476, Final Batch Loss: 0.21860378980636597\n",
      "Epoch 3039, Loss: 2.1614377573132515, Final Batch Loss: 0.10497242957353592\n",
      "Epoch 3040, Loss: 2.5053320229053497, Final Batch Loss: 0.2585744559764862\n",
      "Epoch 3041, Loss: 2.367070585489273, Final Batch Loss: 0.27219799160957336\n",
      "Epoch 3042, Loss: 2.024693767540157, Final Batch Loss: 0.005671362392604351\n",
      "Epoch 3043, Loss: 2.076253693085164, Final Batch Loss: 0.004244248848408461\n",
      "Epoch 3044, Loss: 2.1922283470630646, Final Batch Loss: 0.32141146063804626\n",
      "Epoch 3045, Loss: 3.748918056488037, Final Batch Loss: 1.929195761680603\n",
      "Epoch 3046, Loss: 1.8935538567602634, Final Batch Loss: 0.05971575155854225\n",
      "Epoch 3047, Loss: 2.2826074063777924, Final Batch Loss: 0.3882679045200348\n",
      "Epoch 3048, Loss: 1.9309233425938146, Final Batch Loss: 1.490105023549404e-05\n",
      "Epoch 3049, Loss: 3.291626453399658, Final Batch Loss: 1.4484081268310547\n",
      "Epoch 3050, Loss: 2.031270310282707, Final Batch Loss: 0.19016925990581512\n",
      "Epoch 3051, Loss: 2.834794819355011, Final Batch Loss: 0.9884098768234253\n",
      "Epoch 3052, Loss: 3.269733965396881, Final Batch Loss: 1.522575855255127\n",
      "Epoch 3053, Loss: 1.9662320464849472, Final Batch Loss: 0.13243184983730316\n",
      "Epoch 3054, Loss: 1.8137989599490538, Final Batch Loss: 0.0018609125399962068\n",
      "Epoch 3055, Loss: 2.1373941898345947, Final Batch Loss: 0.33723658323287964\n",
      "Epoch 3056, Loss: 2.4970996379852295, Final Batch Loss: 0.6848642826080322\n",
      "Epoch 3057, Loss: 2.2306169867515564, Final Batch Loss: 0.41603899002075195\n",
      "Epoch 3058, Loss: 1.8767149671912193, Final Batch Loss: 0.07624026387929916\n",
      "Epoch 3059, Loss: 1.9242589175701141, Final Batch Loss: 0.15096524357795715\n",
      "Epoch 3060, Loss: 3.0885782837867737, Final Batch Loss: 1.2152668237686157\n",
      "Epoch 3061, Loss: 1.8337671753342875, Final Batch Loss: 2.610649426060263e-05\n",
      "Epoch 3062, Loss: 1.87184601649642, Final Batch Loss: 0.04254966601729393\n",
      "Epoch 3063, Loss: 2.0331920385360718, Final Batch Loss: 0.08636319637298584\n",
      "Epoch 3064, Loss: 1.8736925512930611, Final Batch Loss: 0.0002040654799202457\n",
      "Epoch 3065, Loss: 3.2598158717155457, Final Batch Loss: 1.4273585081100464\n",
      "Epoch 3066, Loss: 1.8372598059941083, Final Batch Loss: 0.0036253698635846376\n",
      "Epoch 3067, Loss: 2.0042820870876312, Final Batch Loss: 0.12595590949058533\n",
      "Epoch 3068, Loss: 3.149371027946472, Final Batch Loss: 1.3869507312774658\n",
      "Epoch 3069, Loss: 3.722542643547058, Final Batch Loss: 1.8474328517913818\n",
      "Epoch 3070, Loss: 4.330878734588623, Final Batch Loss: 2.5869596004486084\n",
      "Epoch 3071, Loss: 1.916941411793232, Final Batch Loss: 0.09669304639101028\n",
      "Epoch 3072, Loss: 3.541002631187439, Final Batch Loss: 1.6849517822265625\n",
      "Epoch 3073, Loss: 1.7761846166104078, Final Batch Loss: 0.020934222266077995\n",
      "Epoch 3074, Loss: 1.9861734807491302, Final Batch Loss: 0.12981858849525452\n",
      "Epoch 3075, Loss: 4.465317130088806, Final Batch Loss: 2.7006494998931885\n",
      "Epoch 3076, Loss: 2.3065631985664368, Final Batch Loss: 0.5329392552375793\n",
      "Epoch 3077, Loss: 2.028738021850586, Final Batch Loss: 0.2697673439979553\n",
      "Epoch 3078, Loss: 1.8201567600481212, Final Batch Loss: 0.007411129307001829\n",
      "Epoch 3079, Loss: 1.7917234745546011, Final Batch Loss: 0.00017438798386137933\n",
      "Epoch 3080, Loss: 2.1797002851963043, Final Batch Loss: 0.34990403056144714\n",
      "Epoch 3081, Loss: 1.7885212637484074, Final Batch Loss: 0.0363062359392643\n",
      "Epoch 3082, Loss: 1.746282028965652, Final Batch Loss: 0.006191122345626354\n",
      "Epoch 3083, Loss: 1.7728884816169739, Final Batch Loss: 0.07592993974685669\n",
      "Epoch 3084, Loss: 1.9085455536842346, Final Batch Loss: 0.20735538005828857\n",
      "Epoch 3085, Loss: 3.1975061893463135, Final Batch Loss: 1.361195683479309\n",
      "Epoch 3086, Loss: 1.8330385200679302, Final Batch Loss: 0.008548807352781296\n",
      "Epoch 3087, Loss: 3.0195294618606567, Final Batch Loss: 1.172463297843933\n",
      "Epoch 3088, Loss: 1.918733454309404, Final Batch Loss: 0.007956004701554775\n",
      "Epoch 3089, Loss: 2.0238412767648697, Final Batch Loss: 0.16140921413898468\n",
      "Epoch 3090, Loss: 1.8775080858031288, Final Batch Loss: 0.0011080323019996285\n",
      "Epoch 3091, Loss: 1.799460818147054, Final Batch Loss: 0.00014256415306590497\n",
      "Epoch 3092, Loss: 4.706239819526672, Final Batch Loss: 2.9561212062835693\n",
      "Epoch 3093, Loss: 2.911047399044037, Final Batch Loss: 1.0205966234207153\n",
      "Epoch 3094, Loss: 2.138775199651718, Final Batch Loss: 0.2102881371974945\n",
      "Epoch 3095, Loss: 2.0116508565843105, Final Batch Loss: 0.0015070997178554535\n",
      "Epoch 3096, Loss: 2.7094417214393616, Final Batch Loss: 0.7867579460144043\n",
      "Epoch 3097, Loss: 3.447781026363373, Final Batch Loss: 1.542548656463623\n",
      "Epoch 3098, Loss: 1.9895951971411705, Final Batch Loss: 0.07910432666540146\n",
      "Epoch 3099, Loss: 1.9119146168231964, Final Batch Loss: 0.13760969042778015\n",
      "Epoch 3100, Loss: 2.3712390065193176, Final Batch Loss: 0.5673410892486572\n",
      "Epoch 3101, Loss: 1.8028190694749355, Final Batch Loss: 0.03788900002837181\n",
      "Epoch 3102, Loss: 2.6791952252388, Final Batch Loss: 0.9698352217674255\n",
      "Epoch 3103, Loss: 1.7471304046921432, Final Batch Loss: 0.003870378714054823\n",
      "Epoch 3104, Loss: 2.0662034898996353, Final Batch Loss: 0.2070738822221756\n",
      "Epoch 3105, Loss: 2.983507454395294, Final Batch Loss: 1.2639302015304565\n",
      "Epoch 3106, Loss: 1.9590810686349869, Final Batch Loss: 0.1694970279932022\n",
      "Epoch 3107, Loss: 3.671791195869446, Final Batch Loss: 1.9354982376098633\n",
      "Epoch 3108, Loss: 2.6977494955062866, Final Batch Loss: 0.8671979904174805\n",
      "Epoch 3109, Loss: 2.6062599420547485, Final Batch Loss: 0.7247624397277832\n",
      "Epoch 3110, Loss: 2.5738890171051025, Final Batch Loss: 0.6796643733978271\n",
      "Epoch 3111, Loss: 2.847755789756775, Final Batch Loss: 1.003469467163086\n",
      "Epoch 3112, Loss: 1.851308934390545, Final Batch Loss: 0.07434355467557907\n",
      "Epoch 3113, Loss: 2.0556541085243225, Final Batch Loss: 0.31965160369873047\n",
      "Epoch 3114, Loss: 1.8517237044870853, Final Batch Loss: 0.013459179550409317\n",
      "Epoch 3115, Loss: 6.511217176914215, Final Batch Loss: 4.694082260131836\n",
      "Epoch 3116, Loss: 1.9423388428986073, Final Batch Loss: 0.02839227393269539\n",
      "Epoch 3117, Loss: 1.912039807997644, Final Batch Loss: 0.004500261507928371\n",
      "Epoch 3118, Loss: 2.02230591326952, Final Batch Loss: 0.042221732437610626\n",
      "Epoch 3119, Loss: 3.567921757698059, Final Batch Loss: 1.5252430438995361\n",
      "Epoch 3120, Loss: 2.4520548582077026, Final Batch Loss: 0.5874532461166382\n",
      "Epoch 3121, Loss: 1.8603121670894325, Final Batch Loss: 0.006111386697739363\n",
      "Epoch 3122, Loss: 1.93425852060318, Final Batch Loss: 0.026370346546173096\n",
      "Epoch 3123, Loss: 1.9179895389825106, Final Batch Loss: 0.005324231460690498\n",
      "Epoch 3124, Loss: 1.792450469918549, Final Batch Loss: 0.011189919896423817\n",
      "Epoch 3125, Loss: 1.9715267047286034, Final Batch Loss: 0.0747697576880455\n",
      "Epoch 3126, Loss: 1.8356638569384813, Final Batch Loss: 0.02829122729599476\n",
      "Epoch 3127, Loss: 1.73189483769238, Final Batch Loss: 0.0034887660294771194\n",
      "Epoch 3128, Loss: 1.7996840327978134, Final Batch Loss: 0.039199814200401306\n",
      "Epoch 3129, Loss: 3.700560748577118, Final Batch Loss: 1.8925466537475586\n",
      "Epoch 3130, Loss: 1.7694025970995426, Final Batch Loss: 0.03994655981659889\n",
      "Epoch 3131, Loss: 2.004581943154335, Final Batch Loss: 0.21047987043857574\n",
      "Epoch 3132, Loss: 1.8121906502346974, Final Batch Loss: 5.757642793469131e-05\n",
      "Epoch 3133, Loss: 1.865902341902256, Final Batch Loss: 0.05152682214975357\n",
      "Epoch 3134, Loss: 1.7606646292260848, Final Batch Loss: 0.0005134217790327966\n",
      "Epoch 3135, Loss: 3.2889965772628784, Final Batch Loss: 1.5276844501495361\n",
      "Epoch 3136, Loss: 1.7749653367791325, Final Batch Loss: 0.0034012107644230127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3137, Loss: 2.4530386924743652, Final Batch Loss: 0.5842464566230774\n",
      "Epoch 3138, Loss: 3.633536636829376, Final Batch Loss: 1.8165833950042725\n",
      "Epoch 3139, Loss: 4.5960166454315186, Final Batch Loss: 2.857374668121338\n",
      "Epoch 3140, Loss: 1.8862691819667816, Final Batch Loss: 0.21067765355110168\n",
      "Epoch 3141, Loss: 2.146262675523758, Final Batch Loss: 0.3764001429080963\n",
      "Epoch 3142, Loss: 1.778269751026528, Final Batch Loss: 0.0001829695247579366\n",
      "Epoch 3143, Loss: 1.7904445137828588, Final Batch Loss: 0.024741550907492638\n",
      "Epoch 3144, Loss: 4.111640393733978, Final Batch Loss: 2.426347017288208\n",
      "Epoch 3145, Loss: 1.8493115734308958, Final Batch Loss: 0.017782075330615044\n",
      "Epoch 3146, Loss: 3.197209417819977, Final Batch Loss: 1.416832447052002\n",
      "Epoch 3147, Loss: 1.8878149641677737, Final Batch Loss: 0.009698416106402874\n",
      "Epoch 3148, Loss: 1.8865280747413635, Final Batch Loss: 0.03518855571746826\n",
      "Epoch 3149, Loss: 1.821719104424119, Final Batch Loss: 0.019103938713669777\n",
      "Epoch 3150, Loss: 4.0811052322387695, Final Batch Loss: 2.2603609561920166\n",
      "Epoch 3151, Loss: 2.0085995942354202, Final Batch Loss: 0.18602611124515533\n",
      "Epoch 3152, Loss: 1.8502439484000206, Final Batch Loss: 0.0939345583319664\n",
      "Epoch 3153, Loss: 1.719685334712267, Final Batch Loss: 0.006366092711687088\n",
      "Epoch 3154, Loss: 3.1256994009017944, Final Batch Loss: 1.3089401721954346\n",
      "Epoch 3155, Loss: 1.9294143174774945, Final Batch Loss: 0.006210670340806246\n",
      "Epoch 3156, Loss: 2.5735965371131897, Final Batch Loss: 0.5092052221298218\n",
      "Epoch 3157, Loss: 3.9305906891822815, Final Batch Loss: 1.7843375205993652\n",
      "Epoch 3158, Loss: 2.037639044225216, Final Batch Loss: 0.11884041875600815\n",
      "Epoch 3159, Loss: 2.0537959039211273, Final Batch Loss: 0.26864758133888245\n",
      "Epoch 3160, Loss: 3.1935534477233887, Final Batch Loss: 1.3568485975265503\n",
      "Epoch 3161, Loss: 1.9818374528549612, Final Batch Loss: 0.0068712844513356686\n",
      "Epoch 3162, Loss: 1.9653919441625476, Final Batch Loss: 0.012977193109691143\n",
      "Epoch 3163, Loss: 2.04494309425354, Final Batch Loss: 0.1073870062828064\n",
      "Epoch 3164, Loss: 1.8127411948516965, Final Batch Loss: 0.0010595666244626045\n",
      "Epoch 3165, Loss: 3.1394289135932922, Final Batch Loss: 1.2953518629074097\n",
      "Epoch 3166, Loss: 1.7088101208209991, Final Batch Loss: 0.005180269479751587\n",
      "Epoch 3167, Loss: 1.7844117754430044, Final Batch Loss: 0.0003798478574026376\n",
      "Epoch 3168, Loss: 1.7283249480242375, Final Batch Loss: 0.000350175570929423\n",
      "Epoch 3169, Loss: 1.9218486100435257, Final Batch Loss: 0.1665300875902176\n",
      "Epoch 3170, Loss: 1.8194087101146579, Final Batch Loss: 0.0029702140018343925\n",
      "Epoch 3171, Loss: 2.4453225135803223, Final Batch Loss: 0.7151081562042236\n",
      "Epoch 3172, Loss: 2.6394102573394775, Final Batch Loss: 0.9377985000610352\n",
      "Epoch 3173, Loss: 2.1416978538036346, Final Batch Loss: 0.4177218973636627\n",
      "Epoch 3174, Loss: 2.4835789799690247, Final Batch Loss: 0.7326207756996155\n",
      "Epoch 3175, Loss: 2.435127377510071, Final Batch Loss: 0.6089881658554077\n",
      "Epoch 3176, Loss: 3.8870031237602234, Final Batch Loss: 2.233100175857544\n",
      "Epoch 3177, Loss: 1.7978488579392433, Final Batch Loss: 0.004760123789310455\n",
      "Epoch 3178, Loss: 3.943429410457611, Final Batch Loss: 2.1397838592529297\n",
      "Epoch 3179, Loss: 2.275019586086273, Final Batch Loss: 0.5672397613525391\n",
      "Epoch 3180, Loss: 2.5032469630241394, Final Batch Loss: 0.7551114559173584\n",
      "Epoch 3181, Loss: 2.4585626125335693, Final Batch Loss: 0.6695641875267029\n",
      "Epoch 3182, Loss: 2.9004319310188293, Final Batch Loss: 1.1415791511535645\n",
      "Epoch 3183, Loss: 2.1006058752536774, Final Batch Loss: 0.3127650320529938\n",
      "Epoch 3184, Loss: 3.201634466648102, Final Batch Loss: 1.3625667095184326\n",
      "Epoch 3185, Loss: 3.688251256942749, Final Batch Loss: 1.8490105867385864\n",
      "Epoch 3186, Loss: 1.8021263026457746, Final Batch Loss: 0.00030119650182314217\n",
      "Epoch 3187, Loss: 4.0229644775390625, Final Batch Loss: 2.1800649166107178\n",
      "Epoch 3188, Loss: 2.8118552565574646, Final Batch Loss: 1.0477590560913086\n",
      "Epoch 3189, Loss: 1.7762443422743672, Final Batch Loss: 1.0251946150674485e-05\n",
      "Epoch 3190, Loss: 4.2626259326934814, Final Batch Loss: 2.542415142059326\n",
      "Epoch 3191, Loss: 2.93765926361084, Final Batch Loss: 1.0500530004501343\n",
      "Epoch 3192, Loss: 3.9636465907096863, Final Batch Loss: 1.8848680257797241\n",
      "Epoch 3193, Loss: 2.273329637508141, Final Batch Loss: 0.00027450130437500775\n",
      "Epoch 3194, Loss: 3.5535292625427246, Final Batch Loss: 1.543107509613037\n",
      "Epoch 3195, Loss: 2.015736789442599, Final Batch Loss: 0.012310356833040714\n",
      "Epoch 3196, Loss: 3.714138925075531, Final Batch Loss: 1.7972633838653564\n",
      "Epoch 3197, Loss: 1.9357711412012577, Final Batch Loss: 0.047064635902643204\n",
      "Epoch 3198, Loss: 1.978707268834114, Final Batch Loss: 0.10072614252567291\n",
      "Epoch 3199, Loss: 1.9152696058154106, Final Batch Loss: 0.09331231564283371\n",
      "Epoch 3200, Loss: 1.8119699282106012, Final Batch Loss: 0.0013862771447747946\n",
      "Epoch 3201, Loss: 1.8436490260064602, Final Batch Loss: 0.04487865790724754\n",
      "Epoch 3202, Loss: 3.548314690589905, Final Batch Loss: 1.780066967010498\n",
      "Epoch 3203, Loss: 2.9601778388023376, Final Batch Loss: 1.1906641721725464\n",
      "Epoch 3204, Loss: 1.9032582631953119, Final Batch Loss: 4.172238186583854e-05\n",
      "Epoch 3205, Loss: 4.431656360626221, Final Batch Loss: 1.8461636304855347\n",
      "Epoch 3206, Loss: 2.831354096531868, Final Batch Loss: 0.04586316645145416\n",
      "Epoch 3207, Loss: 3.4927207231521606, Final Batch Loss: 0.7982310652732849\n",
      "Epoch 3208, Loss: 4.439546167850494, Final Batch Loss: 2.2204043865203857\n",
      "Epoch 3209, Loss: 2.2667304165661335, Final Batch Loss: 0.037088800221681595\n",
      "Epoch 3210, Loss: 3.0479425191879272, Final Batch Loss: 1.0505024194717407\n",
      "Epoch 3211, Loss: 1.99100029468309, Final Batch Loss: 2.145764938177308e-06\n",
      "Epoch 3212, Loss: 6.167386412620544, Final Batch Loss: 4.206099033355713\n",
      "Epoch 3213, Loss: 1.900627547474869, Final Batch Loss: 0.00010942813969450071\n",
      "Epoch 3214, Loss: 3.2682870030403137, Final Batch Loss: 1.2680102586746216\n",
      "Epoch 3215, Loss: 3.9283435940742493, Final Batch Loss: 1.9261493682861328\n",
      "Epoch 3216, Loss: 2.1553766261786222, Final Batch Loss: 0.013081444427371025\n",
      "Epoch 3217, Loss: 4.228734195232391, Final Batch Loss: 2.2029428482055664\n",
      "Epoch 3218, Loss: 1.971630692263716, Final Batch Loss: 2.0861407392658293e-05\n",
      "Epoch 3219, Loss: 1.964950080960989, Final Batch Loss: 0.03487938269972801\n",
      "Epoch 3220, Loss: 3.9598345160484314, Final Batch Loss: 1.9288811683654785\n",
      "Epoch 3221, Loss: 2.4633877873420715, Final Batch Loss: 0.39187091588974\n",
      "Epoch 3222, Loss: 2.456748366355896, Final Batch Loss: 0.4368932247161865\n",
      "Epoch 3223, Loss: 3.191737473011017, Final Batch Loss: 1.1353230476379395\n",
      "Epoch 3224, Loss: 2.374410182237625, Final Batch Loss: 0.33235451579093933\n",
      "Epoch 3225, Loss: 2.098241910338402, Final Batch Loss: 0.07807759940624237\n",
      "Epoch 3226, Loss: 4.00223845243454, Final Batch Loss: 2.0033645629882812\n",
      "Epoch 3227, Loss: 2.075969323515892, Final Batch Loss: 0.13047753274440765\n",
      "Epoch 3228, Loss: 3.2881184816360474, Final Batch Loss: 1.4005711078643799\n",
      "Epoch 3229, Loss: 3.1093941926956177, Final Batch Loss: 1.2644437551498413\n",
      "Epoch 3230, Loss: 3.3170037865638733, Final Batch Loss: 1.4395277500152588\n",
      "Epoch 3231, Loss: 1.8354201063048095, Final Batch Loss: 0.0021944984328001738\n",
      "Epoch 3232, Loss: 1.967892237007618, Final Batch Loss: 0.07400662451982498\n",
      "Epoch 3233, Loss: 3.5333299040794373, Final Batch Loss: 1.6272705793380737\n",
      "Epoch 3234, Loss: 3.547645688056946, Final Batch Loss: 1.646371603012085\n",
      "Epoch 3235, Loss: 2.996993362903595, Final Batch Loss: 0.9945635795593262\n",
      "Epoch 3236, Loss: 2.0159463733434677, Final Batch Loss: 0.06894366443157196\n",
      "Epoch 3237, Loss: 2.28617399930954, Final Batch Loss: 0.31076765060424805\n",
      "Epoch 3238, Loss: 1.877714332214964, Final Batch Loss: 8.606540359323844e-05\n",
      "Epoch 3239, Loss: 3.473305583000183, Final Batch Loss: 1.5388225317001343\n",
      "Epoch 3240, Loss: 2.4190784692764282, Final Batch Loss: 0.5740925669670105\n",
      "Epoch 3241, Loss: 3.0650822520256042, Final Batch Loss: 1.2729823589324951\n",
      "Epoch 3242, Loss: 2.3242807388305664, Final Batch Loss: 0.35688889026641846\n",
      "Epoch 3243, Loss: 2.1774168461561203, Final Batch Loss: 0.20435528457164764\n",
      "Epoch 3244, Loss: 3.7488542795181274, Final Batch Loss: 1.8851337432861328\n",
      "Epoch 3245, Loss: 3.3539780974388123, Final Batch Loss: 1.499382734298706\n",
      "Epoch 3246, Loss: 1.971786916255951, Final Batch Loss: 0.022716999053955078\n",
      "Epoch 3247, Loss: 3.9352970719337463, Final Batch Loss: 1.9563053846359253\n",
      "Epoch 3248, Loss: 1.944672955200076, Final Batch Loss: 0.016565633937716484\n",
      "Epoch 3249, Loss: 1.8781285838558688, Final Batch Loss: 9.321732068201527e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3250, Loss: 1.9028167566284537, Final Batch Loss: 0.009309454821050167\n",
      "Epoch 3251, Loss: 3.5088196396827698, Final Batch Loss: 1.642472267150879\n",
      "Epoch 3252, Loss: 1.850280499085784, Final Batch Loss: 0.02361074648797512\n",
      "Epoch 3253, Loss: 1.9936644732952118, Final Batch Loss: 0.20691213011741638\n",
      "Epoch 3254, Loss: 2.795480728149414, Final Batch Loss: 1.013504981994629\n",
      "Epoch 3255, Loss: 3.2916876673698425, Final Batch Loss: 1.4567084312438965\n",
      "Epoch 3256, Loss: 1.7926391926594079, Final Batch Loss: 0.003087636549025774\n",
      "Epoch 3257, Loss: 2.055725984275341, Final Batch Loss: 0.04462742060422897\n",
      "Epoch 3258, Loss: 4.280160486698151, Final Batch Loss: 2.11215877532959\n",
      "Epoch 3259, Loss: 2.0414260388351977, Final Batch Loss: 0.006076788995414972\n",
      "Epoch 3260, Loss: 1.974554356187582, Final Batch Loss: 0.04102861508727074\n",
      "Epoch 3261, Loss: 2.86380273103714, Final Batch Loss: 1.0004494190216064\n",
      "Epoch 3262, Loss: 2.935574531555176, Final Batch Loss: 1.0227794647216797\n",
      "Epoch 3263, Loss: 1.9247164307162166, Final Batch Loss: 0.00604053121060133\n",
      "Epoch 3264, Loss: 1.9221580284647644, Final Batch Loss: 0.003611947875469923\n",
      "Epoch 3265, Loss: 3.5148529410362244, Final Batch Loss: 1.5802334547042847\n",
      "Epoch 3266, Loss: 2.0145609378814697, Final Batch Loss: 0.043236374855041504\n",
      "Epoch 3267, Loss: 2.5035250186920166, Final Batch Loss: 0.5488104820251465\n",
      "Epoch 3268, Loss: 1.8904001209884882, Final Batch Loss: 0.029346952214837074\n",
      "Epoch 3269, Loss: 1.9549127956852317, Final Batch Loss: 0.008237316273152828\n",
      "Epoch 3270, Loss: 2.0008692666888237, Final Batch Loss: 0.06381165236234665\n",
      "Epoch 3271, Loss: 1.848647266626358, Final Batch Loss: 0.07004448771476746\n",
      "Epoch 3272, Loss: 1.9505803659558296, Final Batch Loss: 0.039738185703754425\n",
      "Epoch 3273, Loss: 2.1957852244377136, Final Batch Loss: 0.3251916766166687\n",
      "Epoch 3274, Loss: 2.1214820444583893, Final Batch Loss: 0.33542391657829285\n",
      "Epoch 3275, Loss: 2.0359641015529633, Final Batch Loss: 0.27305087447166443\n",
      "Epoch 3276, Loss: 1.7162242652848363, Final Batch Loss: 0.011608433909714222\n",
      "Epoch 3277, Loss: 2.016723930835724, Final Batch Loss: 0.26438260078430176\n",
      "Epoch 3278, Loss: 2.841820240020752, Final Batch Loss: 0.9976521730422974\n",
      "Epoch 3279, Loss: 1.8265682384371758, Final Batch Loss: 0.00739704817533493\n",
      "Epoch 3280, Loss: 1.8611308741383255, Final Batch Loss: 0.0025201248936355114\n",
      "Epoch 3281, Loss: 1.9348747432231903, Final Batch Loss: 0.1316324770450592\n",
      "Epoch 3282, Loss: 1.9022283405065536, Final Batch Loss: 0.08733163774013519\n",
      "Epoch 3283, Loss: 1.8502968661487103, Final Batch Loss: 0.05713392421603203\n",
      "Epoch 3284, Loss: 1.8260840566363186, Final Batch Loss: 0.00020358874462544918\n",
      "Epoch 3285, Loss: 1.8319059908390045, Final Batch Loss: 0.09085878729820251\n",
      "Epoch 3286, Loss: 1.802595417946577, Final Batch Loss: 0.03991929814219475\n",
      "Epoch 3287, Loss: 1.9325195401906967, Final Batch Loss: 0.19138018786907196\n",
      "Epoch 3288, Loss: 1.7744478791719303, Final Batch Loss: 0.001178285456262529\n",
      "Epoch 3289, Loss: 1.7598427757620811, Final Batch Loss: 0.027102015912532806\n",
      "Epoch 3290, Loss: 2.299927830696106, Final Batch Loss: 0.6046757698059082\n",
      "Epoch 3291, Loss: 2.0620401203632355, Final Batch Loss: 0.21826401352882385\n",
      "Epoch 3292, Loss: 4.337517261505127, Final Batch Loss: 2.531808376312256\n",
      "Epoch 3293, Loss: 2.2399445176124573, Final Batch Loss: 0.5125407576560974\n",
      "Epoch 3294, Loss: 1.7643949268385768, Final Batch Loss: 0.00933449249714613\n",
      "Epoch 3295, Loss: 2.660412907600403, Final Batch Loss: 0.8516319394111633\n",
      "Epoch 3296, Loss: 1.8464568676427007, Final Batch Loss: 0.0037845196202397346\n",
      "Epoch 3297, Loss: 2.2201121151447296, Final Batch Loss: 0.405269593000412\n",
      "Epoch 3298, Loss: 1.7920269367095898, Final Batch Loss: 2.4437606043647975e-05\n",
      "Epoch 3299, Loss: 1.8620274565182626, Final Batch Loss: 0.003702928777784109\n",
      "Epoch 3300, Loss: 3.439690887928009, Final Batch Loss: 1.6044362783432007\n",
      "Epoch 3301, Loss: 1.938136488199234, Final Batch Loss: 0.054677814245224\n",
      "Epoch 3302, Loss: 2.5483797788619995, Final Batch Loss: 0.7089055180549622\n",
      "Epoch 3303, Loss: 2.4420969486236572, Final Batch Loss: 0.6874958872795105\n",
      "Epoch 3304, Loss: 1.8105271845997777, Final Batch Loss: 0.0004011779965367168\n",
      "Epoch 3305, Loss: 1.8132312879315577, Final Batch Loss: 0.0006003961316309869\n",
      "Epoch 3306, Loss: 1.878862839192152, Final Batch Loss: 0.05111018940806389\n",
      "Epoch 3307, Loss: 1.7604198265471496, Final Batch Loss: 0.0008678245940245688\n",
      "Epoch 3308, Loss: 1.8056269007502124, Final Batch Loss: 0.00041869457345455885\n",
      "Epoch 3309, Loss: 1.8531994447112083, Final Batch Loss: 0.11936678737401962\n",
      "Epoch 3310, Loss: 1.8769733905792236, Final Batch Loss: 0.14373302459716797\n",
      "Epoch 3311, Loss: 1.7751969508826733, Final Batch Loss: 0.007588725537061691\n",
      "Epoch 3312, Loss: 3.064488470554352, Final Batch Loss: 1.2626519203186035\n",
      "Epoch 3313, Loss: 1.7985734194517136, Final Batch Loss: 0.07199840247631073\n",
      "Epoch 3314, Loss: 2.0289400219917297, Final Batch Loss: 0.20637470483779907\n",
      "Epoch 3315, Loss: 1.7390675102360547, Final Batch Loss: 0.0011829291470348835\n",
      "Epoch 3316, Loss: 1.8576455637812614, Final Batch Loss: 0.11571364849805832\n",
      "Epoch 3317, Loss: 1.760178167372942, Final Batch Loss: 0.051425594836473465\n",
      "Epoch 3318, Loss: 3.020958662033081, Final Batch Loss: 1.3193354606628418\n",
      "Epoch 3319, Loss: 4.699474811553955, Final Batch Loss: 2.983898162841797\n",
      "Epoch 3320, Loss: 1.6963610511593288, Final Batch Loss: 0.0001658063702052459\n",
      "Epoch 3321, Loss: 1.8270307471975684, Final Batch Loss: 0.010081796906888485\n",
      "Epoch 3322, Loss: 4.341261208057404, Final Batch Loss: 2.409691333770752\n",
      "Epoch 3323, Loss: 5.259241819381714, Final Batch Loss: 3.4228947162628174\n",
      "Epoch 3324, Loss: 2.1860711872577667, Final Batch Loss: 0.26920703053474426\n",
      "Epoch 3325, Loss: 1.8761879950761795, Final Batch Loss: 0.1474197655916214\n",
      "Epoch 3326, Loss: 1.64070388302207, Final Batch Loss: 0.021534200757741928\n",
      "Epoch 3327, Loss: 1.7132272227900103, Final Batch Loss: 0.0009657248156145215\n",
      "Epoch 3328, Loss: 1.7471189042553306, Final Batch Loss: 0.0139742037281394\n",
      "Epoch 3329, Loss: 1.763594813644886, Final Batch Loss: 0.01039094477891922\n",
      "Epoch 3330, Loss: 3.782427430152893, Final Batch Loss: 1.9873631000518799\n",
      "Epoch 3331, Loss: 2.663667857646942, Final Batch Loss: 0.8997873663902283\n",
      "Epoch 3332, Loss: 3.28453928232193, Final Batch Loss: 1.5532628297805786\n",
      "Epoch 3333, Loss: 2.0525029003620148, Final Batch Loss: 0.3685455024242401\n",
      "Epoch 3334, Loss: 1.8189472914673388, Final Batch Loss: 0.006391322705894709\n",
      "Epoch 3335, Loss: 4.563946425914764, Final Batch Loss: 2.7377920150756836\n",
      "Epoch 3336, Loss: 2.167412042617798, Final Batch Loss: 0.4325214624404907\n",
      "Epoch 3337, Loss: 2.1697817146778107, Final Batch Loss: 0.39839938282966614\n",
      "Epoch 3338, Loss: 1.7664771823910996, Final Batch Loss: 0.00045694399159401655\n",
      "Epoch 3339, Loss: 1.7583449512021616, Final Batch Loss: 0.00146246247459203\n",
      "Epoch 3340, Loss: 2.239573895931244, Final Batch Loss: 0.33435964584350586\n",
      "Epoch 3341, Loss: 1.9069751910865307, Final Batch Loss: 0.04309491440653801\n",
      "Epoch 3342, Loss: 2.177439898252487, Final Batch Loss: 0.44926491379737854\n",
      "Epoch 3343, Loss: 2.875427722930908, Final Batch Loss: 1.0583398342132568\n",
      "Epoch 3344, Loss: 3.0188587307929993, Final Batch Loss: 1.256251573562622\n",
      "Epoch 3345, Loss: 1.9327238239347935, Final Batch Loss: 0.034791771322488785\n",
      "Epoch 3346, Loss: 1.7591295534803066, Final Batch Loss: 0.00024637524620629847\n",
      "Epoch 3347, Loss: 1.7518327067664359, Final Batch Loss: 0.00039104922325350344\n",
      "Epoch 3348, Loss: 1.832121754065156, Final Batch Loss: 0.018629813566803932\n",
      "Epoch 3349, Loss: 2.763908416032791, Final Batch Loss: 1.0508533716201782\n",
      "Epoch 3350, Loss: 3.5469748973846436, Final Batch Loss: 1.8067299127578735\n",
      "Epoch 3351, Loss: 3.466680586338043, Final Batch Loss: 1.7842097282409668\n",
      "Epoch 3352, Loss: 1.909147948026657, Final Batch Loss: 0.0655083954334259\n",
      "Epoch 3353, Loss: 1.8066772669553757, Final Batch Loss: 0.18015460669994354\n",
      "Epoch 3354, Loss: 1.6703415135852993, Final Batch Loss: 0.006522319745272398\n",
      "Epoch 3355, Loss: 2.6651300191879272, Final Batch Loss: 0.9020023941993713\n",
      "Epoch 3356, Loss: 1.7277913093221287, Final Batch Loss: 8.34461570775602e-06\n",
      "Epoch 3357, Loss: 1.7281900025482173, Final Batch Loss: 6.747018051100895e-05\n",
      "Epoch 3358, Loss: 1.7819707095623016, Final Batch Loss: 0.16632771492004395\n",
      "Epoch 3359, Loss: 1.741275035077706, Final Batch Loss: 0.00200130813755095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3360, Loss: 1.7242787778377533, Final Batch Loss: 0.06198588013648987\n",
      "Epoch 3361, Loss: 3.957942843437195, Final Batch Loss: 2.265273094177246\n",
      "Epoch 3362, Loss: 1.714010066818446, Final Batch Loss: 0.0061880419962108135\n",
      "Epoch 3363, Loss: 2.1719022691249847, Final Batch Loss: 0.46237173676490784\n",
      "Epoch 3364, Loss: 2.2252860069274902, Final Batch Loss: 0.5015957951545715\n",
      "Epoch 3365, Loss: 1.9424748718738556, Final Batch Loss: 0.12923935055732727\n",
      "Epoch 3366, Loss: 1.8393348827958107, Final Batch Loss: 0.10039237886667252\n",
      "Epoch 3367, Loss: 3.785330057144165, Final Batch Loss: 2.0694572925567627\n",
      "Epoch 3368, Loss: 1.6544868173077703, Final Batch Loss: 0.010127467103302479\n",
      "Epoch 3369, Loss: 2.126274824142456, Final Batch Loss: 0.5048342347145081\n",
      "Epoch 3370, Loss: 3.2617819905281067, Final Batch Loss: 1.487061619758606\n",
      "Epoch 3371, Loss: 1.732861030381173, Final Batch Loss: 0.0042143347673118114\n",
      "Epoch 3372, Loss: 1.96195288002491, Final Batch Loss: 0.24788333475589752\n",
      "Epoch 3373, Loss: 3.220335900783539, Final Batch Loss: 1.4624719619750977\n",
      "Epoch 3374, Loss: 1.9530737698078156, Final Batch Loss: 0.18570062518119812\n",
      "Epoch 3375, Loss: 1.7452884308149805, Final Batch Loss: 4.076874756719917e-05\n",
      "Epoch 3376, Loss: 4.167611300945282, Final Batch Loss: 2.414775848388672\n",
      "Epoch 3377, Loss: 1.7055769674479961, Final Batch Loss: 0.007199179381132126\n",
      "Epoch 3378, Loss: 2.10802099108696, Final Batch Loss: 0.35071852803230286\n",
      "Epoch 3379, Loss: 3.817598819732666, Final Batch Loss: 2.131723165512085\n",
      "Epoch 3380, Loss: 2.0549968481063843, Final Batch Loss: 0.45142069458961487\n",
      "Epoch 3381, Loss: 1.764707250520587, Final Batch Loss: 0.016261322423815727\n",
      "Epoch 3382, Loss: 1.728857676498592, Final Batch Loss: 0.009530286304652691\n",
      "Epoch 3383, Loss: 3.519131302833557, Final Batch Loss: 1.8290492296218872\n",
      "Epoch 3384, Loss: 3.4454009532928467, Final Batch Loss: 1.7406774759292603\n",
      "Epoch 3385, Loss: 3.5609576404094696, Final Batch Loss: 1.7569118738174438\n",
      "Epoch 3386, Loss: 2.789737045764923, Final Batch Loss: 1.1512362957000732\n",
      "Epoch 3387, Loss: 1.759448747994611, Final Batch Loss: 0.0001934579631779343\n",
      "Epoch 3388, Loss: 3.5785237550735474, Final Batch Loss: 1.8394029140472412\n",
      "Epoch 3389, Loss: 3.426162004470825, Final Batch Loss: 1.7346246242523193\n",
      "Epoch 3390, Loss: 1.7737641944549978, Final Batch Loss: 0.004211248364299536\n",
      "Epoch 3391, Loss: 2.7898008227348328, Final Batch Loss: 1.035733699798584\n",
      "Epoch 3392, Loss: 1.828155885450542, Final Batch Loss: 0.00630177091807127\n",
      "Epoch 3393, Loss: 1.8276294292882085, Final Batch Loss: 0.01280937809497118\n",
      "Epoch 3394, Loss: 3.1361278295516968, Final Batch Loss: 1.2508649826049805\n",
      "Epoch 3395, Loss: 2.313661515712738, Final Batch Loss: 0.4187580943107605\n",
      "Epoch 3396, Loss: 1.8743540048599243, Final Batch Loss: 0.036213576793670654\n",
      "Epoch 3397, Loss: 4.67831963300705, Final Batch Loss: 2.8951568603515625\n",
      "Epoch 3398, Loss: 1.7639139844104648, Final Batch Loss: 0.01197164785116911\n",
      "Epoch 3399, Loss: 2.535569429397583, Final Batch Loss: 0.7670902609825134\n",
      "Epoch 3400, Loss: 1.7810761885848478, Final Batch Loss: 9.238292841473594e-05\n",
      "Epoch 3401, Loss: 3.0569188594818115, Final Batch Loss: 1.2860119342803955\n",
      "Epoch 3402, Loss: 2.7177863121032715, Final Batch Loss: 1.000331997871399\n",
      "Epoch 3403, Loss: 2.8292444348335266, Final Batch Loss: 1.1266459226608276\n",
      "Epoch 3404, Loss: 1.8071124060079455, Final Batch Loss: 0.00776285957545042\n",
      "Epoch 3405, Loss: 3.4126734733581543, Final Batch Loss: 1.685829758644104\n",
      "Epoch 3406, Loss: 1.7372194072813727, Final Batch Loss: 0.00066985102603212\n",
      "Epoch 3407, Loss: 1.9217150807380676, Final Batch Loss: 0.23438555002212524\n",
      "Epoch 3408, Loss: 1.7654797579161823, Final Batch Loss: 0.004151179920881987\n",
      "Epoch 3409, Loss: 1.7198937349021435, Final Batch Loss: 0.014094572514295578\n",
      "Epoch 3410, Loss: 3.247929871082306, Final Batch Loss: 1.6046011447906494\n",
      "Epoch 3411, Loss: 1.7286999797215685, Final Batch Loss: 0.0011978124966844916\n",
      "Epoch 3412, Loss: 1.831290066242218, Final Batch Loss: 0.19349756836891174\n",
      "Epoch 3413, Loss: 2.7127971053123474, Final Batch Loss: 0.9854860901832581\n",
      "Epoch 3414, Loss: 3.2246418595314026, Final Batch Loss: 1.563688039779663\n",
      "Epoch 3415, Loss: 1.914088860154152, Final Batch Loss: 0.19753803312778473\n",
      "Epoch 3416, Loss: 1.9989813268184662, Final Batch Loss: 0.248494952917099\n",
      "Epoch 3417, Loss: 1.7979923977545695, Final Batch Loss: 0.00014506718434859067\n",
      "Epoch 3418, Loss: 3.277836799621582, Final Batch Loss: 1.4493975639343262\n",
      "Epoch 3419, Loss: 2.1234205663204193, Final Batch Loss: 0.39815476536750793\n",
      "Epoch 3420, Loss: 2.6648547053337097, Final Batch Loss: 1.0198228359222412\n",
      "Epoch 3421, Loss: 2.6836740970611572, Final Batch Loss: 1.0302437543869019\n",
      "Epoch 3422, Loss: 2.477467954158783, Final Batch Loss: 0.6773478984832764\n",
      "Epoch 3423, Loss: 6.878892779350281, Final Batch Loss: 5.165523052215576\n",
      "Epoch 3424, Loss: 2.0256969779729843, Final Batch Loss: 0.22913865745067596\n",
      "Epoch 3425, Loss: 2.0260837120004, Final Batch Loss: 0.0013337773270905018\n",
      "Epoch 3426, Loss: 3.964249610900879, Final Batch Loss: 1.6313055753707886\n",
      "Epoch 3427, Loss: 3.0287363529205322, Final Batch Loss: 0.7757768034934998\n",
      "Epoch 3428, Loss: 2.1545442640781403, Final Batch Loss: 0.1585201919078827\n",
      "Epoch 3429, Loss: 1.9886312186717987, Final Batch Loss: 0.25334641337394714\n",
      "Epoch 3430, Loss: 1.9386421665549278, Final Batch Loss: 0.12193813174962997\n",
      "Epoch 3431, Loss: 1.8033384643495083, Final Batch Loss: 0.032779987901449203\n",
      "Epoch 3432, Loss: 1.9323477074503899, Final Batch Loss: 0.10934417694807053\n",
      "Epoch 3433, Loss: 1.7538273821701296, Final Batch Loss: 0.0007060657371766865\n",
      "Epoch 3434, Loss: 1.7284697564318776, Final Batch Loss: 0.0005509527400135994\n",
      "Epoch 3435, Loss: 1.8264906406402588, Final Batch Loss: 0.013649463653564453\n",
      "Epoch 3436, Loss: 5.015955567359924, Final Batch Loss: 3.21726393699646\n",
      "Epoch 3437, Loss: 3.0450955629348755, Final Batch Loss: 1.2854055166244507\n",
      "Epoch 3438, Loss: 2.019928827881813, Final Batch Loss: 0.22984720766544342\n",
      "Epoch 3439, Loss: 1.982801079750061, Final Batch Loss: 0.28181737661361694\n",
      "Epoch 3440, Loss: 1.7937146737240255, Final Batch Loss: 0.005371542181819677\n",
      "Epoch 3441, Loss: 3.583970606327057, Final Batch Loss: 1.8141038417816162\n",
      "Epoch 3442, Loss: 1.8004718116717413, Final Batch Loss: 0.0012211493449285626\n",
      "Epoch 3443, Loss: 1.75083080684999, Final Batch Loss: 0.000634111522231251\n",
      "Epoch 3444, Loss: 2.27127867937088, Final Batch Loss: 0.48488283157348633\n",
      "Epoch 3445, Loss: 1.7472351077012718, Final Batch Loss: 0.005896079819649458\n",
      "Epoch 3446, Loss: 2.595094859600067, Final Batch Loss: 0.941472053527832\n",
      "Epoch 3447, Loss: 3.584181785583496, Final Batch Loss: 1.8057329654693604\n",
      "Epoch 3448, Loss: 1.7570201139897108, Final Batch Loss: 0.008325276896357536\n",
      "Epoch 3449, Loss: 1.7409902539220639, Final Batch Loss: 0.00023314618738368154\n",
      "Epoch 3450, Loss: 1.8254058479797095, Final Batch Loss: 0.0009641766082495451\n",
      "Epoch 3451, Loss: 1.821173943579197, Final Batch Loss: 0.08291713148355484\n",
      "Epoch 3452, Loss: 1.8596035912632942, Final Batch Loss: 0.020930953323841095\n",
      "Epoch 3453, Loss: 3.0300199389457703, Final Batch Loss: 1.2456083297729492\n",
      "Epoch 3454, Loss: 3.2543715238571167, Final Batch Loss: 1.523297667503357\n",
      "Epoch 3455, Loss: 3.286931574344635, Final Batch Loss: 1.656350016593933\n",
      "Epoch 3456, Loss: 2.15844464302063, Final Batch Loss: 0.44612443447113037\n",
      "Epoch 3457, Loss: 1.6877550598583184, Final Batch Loss: 0.0007568117580376565\n",
      "Epoch 3458, Loss: 1.7359578005343792, Final Batch Loss: 4.017272294731811e-05\n",
      "Epoch 3459, Loss: 1.7429195841541514, Final Batch Loss: 0.0007457336178049445\n",
      "Epoch 3460, Loss: 1.755961138755083, Final Batch Loss: 0.031066734343767166\n",
      "Epoch 3461, Loss: 1.7201319155283272, Final Batch Loss: 0.003187221009284258\n",
      "Epoch 3462, Loss: 2.7305402159690857, Final Batch Loss: 1.1292208433151245\n",
      "Epoch 3463, Loss: 1.900653287768364, Final Batch Loss: 0.1522819846868515\n",
      "Epoch 3464, Loss: 2.8727753162384033, Final Batch Loss: 1.1369380950927734\n",
      "Epoch 3465, Loss: 2.0382919013500214, Final Batch Loss: 0.300513356924057\n",
      "Epoch 3466, Loss: 1.7935944192686293, Final Batch Loss: 3.707340147229843e-05\n",
      "Epoch 3467, Loss: 3.4898712038993835, Final Batch Loss: 1.7676568031311035\n",
      "Epoch 3468, Loss: 1.7556009981781244, Final Batch Loss: 0.014789232984185219\n",
      "Epoch 3469, Loss: 3.642989218235016, Final Batch Loss: 1.8963881731033325\n",
      "Epoch 3470, Loss: 1.9615140408277512, Final Batch Loss: 0.20821844041347504\n",
      "Epoch 3471, Loss: 1.766999939456582, Final Batch Loss: 0.019441643729805946\n",
      "Epoch 3472, Loss: 1.9709493219852448, Final Batch Loss: 0.16963759064674377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3473, Loss: 2.541895031929016, Final Batch Loss: 0.8340693116188049\n",
      "Epoch 3474, Loss: 2.352040946483612, Final Batch Loss: 0.6484656929969788\n",
      "Epoch 3475, Loss: 1.8154789199325023, Final Batch Loss: 4.482168878894299e-05\n",
      "Epoch 3476, Loss: 2.573688268661499, Final Batch Loss: 0.6658971309661865\n",
      "Epoch 3477, Loss: 2.2074114382267, Final Batch Loss: 0.2250392735004425\n",
      "Epoch 3478, Loss: 4.099733054637909, Final Batch Loss: 1.9783047437667847\n",
      "Epoch 3479, Loss: 3.644010543823242, Final Batch Loss: 1.6088851690292358\n",
      "Epoch 3480, Loss: 2.0882932357490063, Final Batch Loss: 0.04416946694254875\n",
      "Epoch 3481, Loss: 2.116941422224045, Final Batch Loss: 0.21244803071022034\n",
      "Epoch 3482, Loss: 2.105558916926384, Final Batch Loss: 0.24133436381816864\n",
      "Epoch 3483, Loss: 4.07240092754364, Final Batch Loss: 2.267120599746704\n",
      "Epoch 3484, Loss: 2.0793441236019135, Final Batch Loss: 0.24353811144828796\n",
      "Epoch 3485, Loss: 1.8958920985460281, Final Batch Loss: 0.01720269024372101\n",
      "Epoch 3486, Loss: 5.34836733341217, Final Batch Loss: 3.564439058303833\n",
      "Epoch 3487, Loss: 1.940304346382618, Final Batch Loss: 0.09855932742357254\n",
      "Epoch 3488, Loss: 3.5959136486053467, Final Batch Loss: 1.4002622365951538\n",
      "Epoch 3489, Loss: 2.4461345970630646, Final Batch Loss: 0.18363085389137268\n",
      "Epoch 3490, Loss: 3.8245514631271362, Final Batch Loss: 1.7215003967285156\n",
      "Epoch 3491, Loss: 2.1455065459012985, Final Batch Loss: 0.13427112996578217\n",
      "Epoch 3492, Loss: 1.7904394859251624, Final Batch Loss: 3.397406908334233e-05\n",
      "Epoch 3493, Loss: 4.004605770111084, Final Batch Loss: 2.133981943130493\n",
      "Epoch 3494, Loss: 3.0175710320472717, Final Batch Loss: 1.287274718284607\n",
      "Epoch 3495, Loss: 3.5123419165611267, Final Batch Loss: 1.807423710823059\n",
      "Epoch 3496, Loss: 1.8551774770021439, Final Batch Loss: 0.004136696457862854\n",
      "Epoch 3497, Loss: 1.746354573406279, Final Batch Loss: 0.00011419598013162613\n",
      "Epoch 3498, Loss: 3.3699533939361572, Final Batch Loss: 1.509864091873169\n",
      "Epoch 3499, Loss: 2.2568971514701843, Final Batch Loss: 0.47898346185684204\n",
      "Epoch 3500, Loss: 2.4238715171813965, Final Batch Loss: 0.548340916633606\n",
      "Epoch 3501, Loss: 1.771872041746974, Final Batch Loss: 0.025820313021540642\n",
      "Epoch 3502, Loss: 1.8064263972919434, Final Batch Loss: 0.002213886706158519\n",
      "Epoch 3503, Loss: 2.6795616149902344, Final Batch Loss: 0.8299393057823181\n",
      "Epoch 3504, Loss: 1.8387799710035324, Final Batch Loss: 0.1235543042421341\n",
      "Epoch 3505, Loss: 1.9758462458848953, Final Batch Loss: 0.18813557922840118\n",
      "Epoch 3506, Loss: 3.0494763255119324, Final Batch Loss: 1.19899320602417\n",
      "Epoch 3507, Loss: 1.6880373121530283, Final Batch Loss: 0.0002177716523874551\n",
      "Epoch 3508, Loss: 2.3544499278068542, Final Batch Loss: 0.6159303784370422\n",
      "Epoch 3509, Loss: 3.8892136216163635, Final Batch Loss: 2.0753397941589355\n",
      "Epoch 3510, Loss: 1.7528276704251766, Final Batch Loss: 0.03916450962424278\n",
      "Epoch 3511, Loss: 1.7188380891748238, Final Batch Loss: 0.00011395759065635502\n",
      "Epoch 3512, Loss: 1.6912702267291024, Final Batch Loss: 0.001566374790854752\n",
      "Epoch 3513, Loss: 4.164205849170685, Final Batch Loss: 2.391446352005005\n",
      "Epoch 3514, Loss: 2.3058036863803864, Final Batch Loss: 0.6537229418754578\n",
      "Epoch 3515, Loss: 1.8944346606731415, Final Batch Loss: 0.1434853971004486\n",
      "Epoch 3516, Loss: 3.443311035633087, Final Batch Loss: 1.7668280601501465\n",
      "Epoch 3517, Loss: 2.0147842466831207, Final Batch Loss: 0.3251761496067047\n",
      "Epoch 3518, Loss: 1.7425654903054237, Final Batch Loss: 0.11171860247850418\n",
      "Epoch 3519, Loss: 1.8339461088180542, Final Batch Loss: 0.14992499351501465\n",
      "Epoch 3520, Loss: 1.8022414818406105, Final Batch Loss: 0.04179034382104874\n",
      "Epoch 3521, Loss: 1.7378404276096262, Final Batch Loss: 0.0007541911327280104\n",
      "Epoch 3522, Loss: 1.800283744931221, Final Batch Loss: 0.030974164605140686\n",
      "Epoch 3523, Loss: 2.7842562198638916, Final Batch Loss: 1.0257524251937866\n",
      "Epoch 3524, Loss: 2.9208861589431763, Final Batch Loss: 1.2508111000061035\n",
      "Epoch 3525, Loss: 1.7458121795207262, Final Batch Loss: 0.023275496438145638\n",
      "Epoch 3526, Loss: 2.318448781967163, Final Batch Loss: 0.6002255082130432\n",
      "Epoch 3527, Loss: 3.5828012228012085, Final Batch Loss: 1.8158555030822754\n",
      "Epoch 3528, Loss: 1.9219364472446614, Final Batch Loss: 0.00010144196130568162\n",
      "Epoch 3529, Loss: 1.9789721444249153, Final Batch Loss: 0.044382162392139435\n",
      "Epoch 3530, Loss: 3.380555808544159, Final Batch Loss: 1.4249413013458252\n",
      "Epoch 3531, Loss: 2.155692219734192, Final Batch Loss: 0.26673424243927\n",
      "Epoch 3532, Loss: 2.630999267101288, Final Batch Loss: 0.8162594437599182\n",
      "Epoch 3533, Loss: 3.581214427947998, Final Batch Loss: 1.7822260856628418\n",
      "Epoch 3534, Loss: 1.8234251327812672, Final Batch Loss: 0.05890192463994026\n",
      "Epoch 3535, Loss: 1.6873621311970055, Final Batch Loss: 0.004642185289412737\n",
      "Epoch 3536, Loss: 1.7701666932553053, Final Batch Loss: 6.103329360485077e-05\n",
      "Epoch 3537, Loss: 2.1348156332969666, Final Batch Loss: 0.4852985739707947\n",
      "Epoch 3538, Loss: 2.369691163301468, Final Batch Loss: 0.7055187821388245\n",
      "Epoch 3539, Loss: 1.6937599275261164, Final Batch Loss: 0.00922996737062931\n",
      "Epoch 3540, Loss: 1.6884177085012197, Final Batch Loss: 0.006483709439635277\n",
      "Epoch 3541, Loss: 2.372712194919586, Final Batch Loss: 0.8103314638137817\n",
      "Epoch 3542, Loss: 2.203126013278961, Final Batch Loss: 0.5387236475944519\n",
      "Epoch 3543, Loss: 3.876726746559143, Final Batch Loss: 2.2621309757232666\n",
      "Epoch 3544, Loss: 1.7335337065160275, Final Batch Loss: 0.0526936911046505\n",
      "Epoch 3545, Loss: 1.762155331671238, Final Batch Loss: 0.037033893167972565\n",
      "Epoch 3546, Loss: 3.041427254676819, Final Batch Loss: 1.2611087560653687\n",
      "Epoch 3547, Loss: 3.080264925956726, Final Batch Loss: 1.2187331914901733\n",
      "Epoch 3548, Loss: 3.2235193848609924, Final Batch Loss: 1.383563756942749\n",
      "Epoch 3549, Loss: 2.831661641597748, Final Batch Loss: 0.988900363445282\n",
      "Epoch 3550, Loss: 2.0697325691580772, Final Batch Loss: 0.07840534299612045\n",
      "Epoch 3551, Loss: 3.4789769053459167, Final Batch Loss: 1.4301316738128662\n",
      "Epoch 3552, Loss: 2.8972747325897217, Final Batch Loss: 0.9115817546844482\n",
      "Epoch 3553, Loss: 3.469004452228546, Final Batch Loss: 1.6366616487503052\n",
      "Epoch 3554, Loss: 2.9810138940811157, Final Batch Loss: 1.1953023672103882\n",
      "Epoch 3555, Loss: 2.046735852956772, Final Batch Loss: 0.28137901425361633\n",
      "Epoch 3556, Loss: 1.7576644397704513, Final Batch Loss: 6.770858453819528e-05\n",
      "Epoch 3557, Loss: 1.8263453990221024, Final Batch Loss: 0.029322758316993713\n",
      "Epoch 3558, Loss: 2.461245894432068, Final Batch Loss: 0.688326358795166\n",
      "Epoch 3559, Loss: 1.983235478401184, Final Batch Loss: 0.20984667539596558\n",
      "Epoch 3560, Loss: 1.7261286731809378, Final Batch Loss: 0.00913015566766262\n",
      "Epoch 3561, Loss: 3.6374903321266174, Final Batch Loss: 1.9422948360443115\n",
      "Epoch 3562, Loss: 2.6809158325195312, Final Batch Loss: 0.938613772392273\n",
      "Epoch 3563, Loss: 2.176046848297119, Final Batch Loss: 0.37877607345581055\n",
      "Epoch 3564, Loss: 4.5305495262146, Final Batch Loss: 2.2758967876434326\n",
      "Epoch 3565, Loss: 2.6146727800369263, Final Batch Loss: 0.430514395236969\n",
      "Epoch 3566, Loss: 2.141319419257343, Final Batch Loss: 0.003443385474383831\n",
      "Epoch 3567, Loss: 3.0370832085609436, Final Batch Loss: 0.9913676381111145\n",
      "Epoch 3568, Loss: 3.792693793773651, Final Batch Loss: 1.9112144708633423\n",
      "Epoch 3569, Loss: 1.9925337880849838, Final Batch Loss: 0.1424035280942917\n",
      "Epoch 3570, Loss: 1.9615367501974106, Final Batch Loss: 0.13110969960689545\n",
      "Epoch 3571, Loss: 1.7529075912898406, Final Batch Loss: 0.0014111570781096816\n",
      "Epoch 3572, Loss: 2.454865276813507, Final Batch Loss: 0.7356249690055847\n",
      "Epoch 3573, Loss: 1.7103825502563268, Final Batch Loss: 0.003172367112711072\n",
      "Epoch 3574, Loss: 3.289199113845825, Final Batch Loss: 1.5751838684082031\n",
      "Epoch 3575, Loss: 2.634245455265045, Final Batch Loss: 0.890479326248169\n",
      "Epoch 3576, Loss: 3.5215917229652405, Final Batch Loss: 1.811657190322876\n",
      "Epoch 3577, Loss: 1.7744569405913353, Final Batch Loss: 0.11320223659276962\n",
      "Epoch 3578, Loss: 4.199842870235443, Final Batch Loss: 2.4810614585876465\n",
      "Epoch 3579, Loss: 2.861007511615753, Final Batch Loss: 1.1584270000457764\n",
      "Epoch 3580, Loss: 2.5975499749183655, Final Batch Loss: 0.926716685295105\n",
      "Epoch 3581, Loss: 2.038945645093918, Final Batch Loss: 0.27112385630607605\n",
      "Epoch 3582, Loss: 2.589198112487793, Final Batch Loss: 0.9275774955749512\n",
      "Epoch 3583, Loss: 1.8564452640130185, Final Batch Loss: 0.0005790702416561544\n",
      "Epoch 3584, Loss: 2.0349679738283157, Final Batch Loss: 0.2164451628923416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3585, Loss: 1.7018013742635958, Final Batch Loss: 0.0003000047872774303\n",
      "Epoch 3586, Loss: 1.7971371673047543, Final Batch Loss: 0.025100376456975937\n",
      "Epoch 3587, Loss: 1.7365290634334087, Final Batch Loss: 0.028120052069425583\n",
      "Epoch 3588, Loss: 3.2253578901290894, Final Batch Loss: 1.6217718124389648\n",
      "Epoch 3589, Loss: 1.596939907874912, Final Batch Loss: 0.0015135272406041622\n",
      "Epoch 3590, Loss: 1.6372288769343868, Final Batch Loss: 0.0015624469378963113\n",
      "Epoch 3591, Loss: 1.5990034923888743, Final Batch Loss: 0.007726071868091822\n",
      "Epoch 3592, Loss: 1.7597323218360543, Final Batch Loss: 0.012431279756128788\n",
      "Epoch 3593, Loss: 2.829918473958969, Final Batch Loss: 1.1852211952209473\n",
      "Epoch 3594, Loss: 1.6432297523133457, Final Batch Loss: 0.005818098317831755\n",
      "Epoch 3595, Loss: 1.8929796814918518, Final Batch Loss: 0.24006247520446777\n",
      "Epoch 3596, Loss: 1.6973266320419498, Final Batch Loss: 0.0008781867218203843\n",
      "Epoch 3597, Loss: 1.6291043013334274, Final Batch Loss: 0.053259775042533875\n",
      "Epoch 3598, Loss: 2.073240429162979, Final Batch Loss: 0.4447881877422333\n",
      "Epoch 3599, Loss: 1.7130289990454912, Final Batch Loss: 0.010405691340565681\n",
      "Epoch 3600, Loss: 1.5858665914856829, Final Batch Loss: 0.00027783826226368546\n",
      "Epoch 3601, Loss: 4.085047692060471, Final Batch Loss: 2.4519546031951904\n",
      "Epoch 3602, Loss: 1.6590968408272602, Final Batch Loss: 0.0008411445305682719\n",
      "Epoch 3603, Loss: 1.6437408968340605, Final Batch Loss: 0.0032541200052946806\n",
      "Epoch 3604, Loss: 1.7287040799856186, Final Batch Loss: 0.03384135663509369\n",
      "Epoch 3605, Loss: 1.910828322172165, Final Batch Loss: 0.18035539984703064\n",
      "Epoch 3606, Loss: 2.7822834253311157, Final Batch Loss: 1.0872751474380493\n",
      "Epoch 3607, Loss: 1.6635554672393482, Final Batch Loss: 0.00043644916149787605\n",
      "Epoch 3608, Loss: 1.6693680697353557, Final Batch Loss: 0.00019238528329879045\n",
      "Epoch 3609, Loss: 1.6590331916231662, Final Batch Loss: 0.00046230596490204334\n",
      "Epoch 3610, Loss: 3.270480453968048, Final Batch Loss: 1.6229180097579956\n",
      "Epoch 3611, Loss: 3.566410779953003, Final Batch Loss: 1.990354299545288\n",
      "Epoch 3612, Loss: 1.879719614982605, Final Batch Loss: 0.27162912487983704\n",
      "Epoch 3613, Loss: 1.6038639432517812, Final Batch Loss: 0.00015031162183731794\n",
      "Epoch 3614, Loss: 1.70591427013278, Final Batch Loss: 0.03729588910937309\n",
      "Epoch 3615, Loss: 1.622920636087656, Final Batch Loss: 0.03718769922852516\n",
      "Epoch 3616, Loss: 1.6081545165507123, Final Batch Loss: 0.0016305259196087718\n",
      "Epoch 3617, Loss: 1.8990700542926788, Final Batch Loss: 0.3517996668815613\n",
      "Epoch 3618, Loss: 1.6608180690964218, Final Batch Loss: 0.0002485204895492643\n",
      "Epoch 3619, Loss: 3.35330867767334, Final Batch Loss: 1.7117207050323486\n",
      "Epoch 3620, Loss: 1.8099627196788788, Final Batch Loss: 0.14206674695014954\n",
      "Epoch 3621, Loss: 2.833625912666321, Final Batch Loss: 1.1735773086547852\n",
      "Epoch 3622, Loss: 3.2888920307159424, Final Batch Loss: 1.5450482368469238\n",
      "Epoch 3623, Loss: 2.9768160581588745, Final Batch Loss: 1.312814712524414\n",
      "Epoch 3624, Loss: 1.574618561193347, Final Batch Loss: 0.01671840064227581\n",
      "Epoch 3625, Loss: 1.673884678632021, Final Batch Loss: 0.04893656447529793\n",
      "Epoch 3626, Loss: 3.551880866289139, Final Batch Loss: 1.9308037757873535\n",
      "Epoch 3627, Loss: 3.598662555217743, Final Batch Loss: 1.9500454664230347\n",
      "Epoch 3628, Loss: 1.9219585061073303, Final Batch Loss: 0.28416624665260315\n",
      "Epoch 3629, Loss: 2.7380030155181885, Final Batch Loss: 1.0622588396072388\n",
      "Epoch 3630, Loss: 2.030183434486389, Final Batch Loss: 0.2978528141975403\n",
      "Epoch 3631, Loss: 3.359756290912628, Final Batch Loss: 1.6482666730880737\n",
      "Epoch 3632, Loss: 3.0308324098587036, Final Batch Loss: 1.3788458108901978\n",
      "Epoch 3633, Loss: 2.152700126171112, Final Batch Loss: 0.513683021068573\n",
      "Epoch 3634, Loss: 1.6290170978754759, Final Batch Loss: 0.0010517071932554245\n",
      "Epoch 3635, Loss: 2.367264449596405, Final Batch Loss: 0.6908043026924133\n",
      "Epoch 3636, Loss: 1.7911025807261467, Final Batch Loss: 0.08106767386198044\n",
      "Epoch 3637, Loss: 1.721172510740871, Final Batch Loss: 4.088794958079234e-05\n",
      "Epoch 3638, Loss: 1.848357304930687, Final Batch Loss: 0.18266575038433075\n",
      "Epoch 3639, Loss: 1.6553287494461983, Final Batch Loss: 0.003577976254746318\n",
      "Epoch 3640, Loss: 2.5076163709163666, Final Batch Loss: 0.9234014749526978\n",
      "Epoch 3641, Loss: 1.5813796031288803, Final Batch Loss: 0.005739043932408094\n",
      "Epoch 3642, Loss: 1.9141769111156464, Final Batch Loss: 0.13641348481178284\n",
      "Epoch 3643, Loss: 1.7987044751644135, Final Batch Loss: 0.03357580304145813\n",
      "Epoch 3644, Loss: 2.7430748343467712, Final Batch Loss: 0.9965668320655823\n",
      "Epoch 3645, Loss: 1.9320564307272434, Final Batch Loss: 0.05589354410767555\n",
      "Epoch 3646, Loss: 3.815603792667389, Final Batch Loss: 1.7526861429214478\n",
      "Epoch 3647, Loss: 4.979323625564575, Final Batch Loss: 2.9268693923950195\n",
      "Epoch 3648, Loss: 2.150945277302526, Final Batch Loss: 0.0006438804557546973\n",
      "Epoch 3649, Loss: 2.144933521747589, Final Batch Loss: 0.08990824222564697\n",
      "Epoch 3650, Loss: 3.5345550775527954, Final Batch Loss: 1.6164450645446777\n",
      "Epoch 3651, Loss: 4.480700850486755, Final Batch Loss: 2.8001763820648193\n",
      "Epoch 3652, Loss: 1.9763851759544195, Final Batch Loss: 2.13382354559144e-05\n",
      "Epoch 3653, Loss: 4.486453533172607, Final Batch Loss: 1.619621992111206\n",
      "Epoch 3654, Loss: 6.413543283939362, Final Batch Loss: 3.1693320274353027\n",
      "Epoch 3655, Loss: 3.4588288068771362, Final Batch Loss: 0.907959520816803\n",
      "Epoch 3656, Loss: 3.1834793090820312, Final Batch Loss: 1.104495882987976\n",
      "Epoch 3657, Loss: 3.9937930703163147, Final Batch Loss: 1.9979326725006104\n",
      "Epoch 3658, Loss: 2.572736829519272, Final Batch Loss: 0.4367423355579376\n",
      "Epoch 3659, Loss: 3.3307321667671204, Final Batch Loss: 1.1982431411743164\n",
      "Epoch 3660, Loss: 2.4011791525408626, Final Batch Loss: 0.01071580033749342\n",
      "Epoch 3661, Loss: 2.8423598408699036, Final Batch Loss: 0.5137588977813721\n",
      "Epoch 3662, Loss: 3.024217367172241, Final Batch Loss: 0.9908747673034668\n",
      "Epoch 3663, Loss: 2.101121160783805, Final Batch Loss: 0.00041476229671388865\n",
      "Epoch 3664, Loss: 2.054929692298174, Final Batch Loss: 0.032486338168382645\n",
      "Epoch 3665, Loss: 2.9160467982292175, Final Batch Loss: 0.9503844976425171\n",
      "Epoch 3666, Loss: 2.668718695640564, Final Batch Loss: 0.5953885316848755\n",
      "Epoch 3667, Loss: 2.22744420170784, Final Batch Loss: 0.3506084978580475\n",
      "Epoch 3668, Loss: 1.8050078842788935, Final Batch Loss: 0.015027293935418129\n",
      "Epoch 3669, Loss: 2.4158644676208496, Final Batch Loss: 0.45042335987091064\n",
      "Epoch 3670, Loss: 1.8373965648934245, Final Batch Loss: 0.011680427007377148\n",
      "Epoch 3671, Loss: 3.4366544485092163, Final Batch Loss: 1.6456284523010254\n",
      "Epoch 3672, Loss: 3.590528130531311, Final Batch Loss: 1.7360731363296509\n",
      "Epoch 3673, Loss: 2.6943629384040833, Final Batch Loss: 0.906004011631012\n",
      "Epoch 3674, Loss: 1.850804936257191, Final Batch Loss: 0.0003108495147898793\n",
      "Epoch 3675, Loss: 2.1955475509166718, Final Batch Loss: 0.24859657883644104\n",
      "Epoch 3676, Loss: 4.050027847290039, Final Batch Loss: 1.9093551635742188\n",
      "Epoch 3677, Loss: 2.205482678953558, Final Batch Loss: 0.00358118349686265\n",
      "Epoch 3678, Loss: 2.5361483693122864, Final Batch Loss: 0.35614603757858276\n",
      "Epoch 3679, Loss: 2.137474164366722, Final Batch Loss: 0.11947278678417206\n",
      "Epoch 3680, Loss: 6.838159799575806, Final Batch Loss: 4.817205905914307\n",
      "Epoch 3681, Loss: 5.082184135913849, Final Batch Loss: 3.242910861968994\n",
      "Epoch 3682, Loss: 1.8706306861713529, Final Batch Loss: 0.003809933550655842\n",
      "Epoch 3683, Loss: 3.320010721683502, Final Batch Loss: 1.1727265119552612\n",
      "Epoch 3684, Loss: 4.919840335845947, Final Batch Loss: 2.527404546737671\n",
      "Epoch 3685, Loss: 2.237797470879741, Final Batch Loss: 0.0010621865512803197\n",
      "Epoch 3686, Loss: 5.463592350482941, Final Batch Loss: 3.264843225479126\n",
      "Epoch 3687, Loss: 2.2340156976133585, Final Batch Loss: 0.030531996861100197\n",
      "Epoch 3688, Loss: 2.1807329580187798, Final Batch Loss: 0.0040697380900382996\n",
      "Epoch 3689, Loss: 3.1623279452323914, Final Batch Loss: 1.213413953781128\n",
      "Epoch 3690, Loss: 3.579913854598999, Final Batch Loss: 1.663573145866394\n",
      "Epoch 3691, Loss: 2.7989355325698853, Final Batch Loss: 0.865321159362793\n",
      "Epoch 3692, Loss: 2.2207066416740417, Final Batch Loss: 0.361383855342865\n",
      "Epoch 3693, Loss: 2.563095211982727, Final Batch Loss: 0.6904825568199158\n",
      "Epoch 3694, Loss: 1.8564655110239983, Final Batch Loss: 0.08994638174772263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3695, Loss: 2.102522134780884, Final Batch Loss: 0.3026108741760254\n",
      "Epoch 3696, Loss: 2.4513842463493347, Final Batch Loss: 0.7256090641021729\n",
      "Epoch 3697, Loss: 1.8108791643753648, Final Batch Loss: 0.005679541267454624\n",
      "Epoch 3698, Loss: 1.7728001546347514, Final Batch Loss: 0.0018867325270548463\n",
      "Epoch 3699, Loss: 2.538477301597595, Final Batch Loss: 0.7825033068656921\n",
      "Epoch 3700, Loss: 2.310203731060028, Final Batch Loss: 0.44177645444869995\n",
      "Epoch 3701, Loss: 1.8332434110343456, Final Batch Loss: 0.04710159823298454\n",
      "Epoch 3702, Loss: 2.277040958404541, Final Batch Loss: 0.35428839921951294\n",
      "Epoch 3703, Loss: 2.3242098689079285, Final Batch Loss: 0.40333980321884155\n",
      "Epoch 3704, Loss: 1.8760524205863476, Final Batch Loss: 0.043096285313367844\n",
      "Epoch 3705, Loss: 2.11481474339962, Final Batch Loss: 0.2240254133939743\n",
      "Epoch 3706, Loss: 2.759653925895691, Final Batch Loss: 0.9449667930603027\n",
      "Epoch 3707, Loss: 3.2774872183799744, Final Batch Loss: 1.5318506956100464\n",
      "Epoch 3708, Loss: 1.765784593531862, Final Batch Loss: 0.0032821616623550653\n",
      "Epoch 3709, Loss: 3.194833815097809, Final Batch Loss: 1.3559627532958984\n",
      "Epoch 3710, Loss: 1.7013591509312391, Final Batch Loss: 0.004009423777461052\n",
      "Epoch 3711, Loss: 3.0493587851524353, Final Batch Loss: 1.2099103927612305\n",
      "Epoch 3712, Loss: 1.910694658756256, Final Batch Loss: 0.19812989234924316\n",
      "Epoch 3713, Loss: 1.7759359069168568, Final Batch Loss: 0.01611306145787239\n",
      "Epoch 3714, Loss: 1.847494900226593, Final Batch Loss: 0.13058048486709595\n",
      "Epoch 3715, Loss: 3.1225253343582153, Final Batch Loss: 1.3187527656555176\n",
      "Epoch 3716, Loss: 2.4406145811080933, Final Batch Loss: 0.682609498500824\n",
      "Epoch 3717, Loss: 2.6003101468086243, Final Batch Loss: 0.8139945268630981\n",
      "Epoch 3718, Loss: 4.88997095823288, Final Batch Loss: 3.055755138397217\n",
      "Epoch 3719, Loss: 2.1694971919059753, Final Batch Loss: 0.2174960970878601\n",
      "Epoch 3720, Loss: 3.73636531829834, Final Batch Loss: 1.7912335395812988\n",
      "Epoch 3721, Loss: 2.5337228178977966, Final Batch Loss: 0.571247935295105\n",
      "Epoch 3722, Loss: 2.709224820137024, Final Batch Loss: 0.8056675791740417\n",
      "Epoch 3723, Loss: 3.3928005695343018, Final Batch Loss: 1.5672324895858765\n",
      "Epoch 3724, Loss: 1.9624569565057755, Final Batch Loss: 0.21491803228855133\n",
      "Epoch 3725, Loss: 2.2761799693107605, Final Batch Loss: 0.5522456169128418\n",
      "Epoch 3726, Loss: 1.736903004348278, Final Batch Loss: 0.07454561442136765\n",
      "Epoch 3727, Loss: 1.7592364633455873, Final Batch Loss: 0.012118169106543064\n",
      "Epoch 3728, Loss: 2.0958093404769897, Final Batch Loss: 0.28097254037857056\n",
      "Epoch 3729, Loss: 2.6764383912086487, Final Batch Loss: 0.9397439956665039\n",
      "Epoch 3730, Loss: 1.734109716489911, Final Batch Loss: 0.011825459077954292\n",
      "Epoch 3731, Loss: 2.02163565158844, Final Batch Loss: 0.2590670585632324\n",
      "Epoch 3732, Loss: 1.8043173188343644, Final Batch Loss: 0.0008874768391251564\n",
      "Epoch 3733, Loss: 2.391673445701599, Final Batch Loss: 0.7331956624984741\n",
      "Epoch 3734, Loss: 3.56178480386734, Final Batch Loss: 1.935891032218933\n",
      "Epoch 3735, Loss: 1.979262799024582, Final Batch Loss: 0.2541820704936981\n",
      "Epoch 3736, Loss: 3.271012246608734, Final Batch Loss: 1.518898606300354\n",
      "Epoch 3737, Loss: 3.506766200065613, Final Batch Loss: 1.7568472623825073\n",
      "Epoch 3738, Loss: 2.6851545572280884, Final Batch Loss: 0.9928532838821411\n",
      "Epoch 3739, Loss: 1.9825104922056198, Final Batch Loss: 0.1820039600133896\n",
      "Epoch 3740, Loss: 1.8715928243473172, Final Batch Loss: 0.00369900930672884\n",
      "Epoch 3741, Loss: 4.316853404045105, Final Batch Loss: 2.4964494705200195\n",
      "Epoch 3742, Loss: 1.7622812893241644, Final Batch Loss: 0.007389238104224205\n",
      "Epoch 3743, Loss: 1.9211656749248505, Final Batch Loss: 0.19888761639595032\n",
      "Epoch 3744, Loss: 3.293865144252777, Final Batch Loss: 1.5970375537872314\n",
      "Epoch 3745, Loss: 1.7155065387487411, Final Batch Loss: 0.06259678304195404\n",
      "Epoch 3746, Loss: 3.376081347465515, Final Batch Loss: 1.599297046661377\n",
      "Epoch 3747, Loss: 1.797857714816928, Final Batch Loss: 0.02281220071017742\n",
      "Epoch 3748, Loss: 2.7854626774787903, Final Batch Loss: 1.123212218284607\n",
      "Epoch 3749, Loss: 1.641568454564549, Final Batch Loss: 0.001116367639042437\n",
      "Epoch 3750, Loss: 1.6947199357673526, Final Batch Loss: 0.011031354777514935\n",
      "Epoch 3751, Loss: 3.124379813671112, Final Batch Loss: 1.440191388130188\n",
      "Epoch 3752, Loss: 1.6762818707502447, Final Batch Loss: 0.0008440031087957323\n",
      "Epoch 3753, Loss: 2.9142465591430664, Final Batch Loss: 1.2771706581115723\n",
      "Epoch 3754, Loss: 1.6976968282833695, Final Batch Loss: 0.0004379982128739357\n",
      "Epoch 3755, Loss: 1.724830375984311, Final Batch Loss: 0.0017333496361970901\n",
      "Epoch 3756, Loss: 1.66828325483948, Final Batch Loss: 0.008718402124941349\n",
      "Epoch 3757, Loss: 3.478111982345581, Final Batch Loss: 1.7202638387680054\n",
      "Epoch 3758, Loss: 1.6429773038253188, Final Batch Loss: 0.007678753696382046\n",
      "Epoch 3759, Loss: 1.7694577537477016, Final Batch Loss: 0.04280256852507591\n",
      "Epoch 3760, Loss: 3.200456917285919, Final Batch Loss: 1.457736611366272\n",
      "Epoch 3761, Loss: 1.8429534882307053, Final Batch Loss: 0.05717490613460541\n",
      "Epoch 3762, Loss: 2.215759664773941, Final Batch Loss: 0.40172919631004333\n",
      "Epoch 3763, Loss: 2.0189168006181717, Final Batch Loss: 0.2012283354997635\n",
      "Epoch 3764, Loss: 2.0105987787246704, Final Batch Loss: 0.16859352588653564\n",
      "Epoch 3765, Loss: 1.7774630738858832, Final Batch Loss: 9.60780744208023e-05\n",
      "Epoch 3766, Loss: 1.9248170107603073, Final Batch Loss: 0.22604356706142426\n",
      "Epoch 3767, Loss: 2.061777114868164, Final Batch Loss: 0.3226332664489746\n",
      "Epoch 3768, Loss: 1.9530908465385437, Final Batch Loss: 0.1984965205192566\n",
      "Epoch 3769, Loss: 1.7576367165893316, Final Batch Loss: 0.01623540185391903\n",
      "Epoch 3770, Loss: 1.782923050224781, Final Batch Loss: 0.06579507142305374\n",
      "Epoch 3771, Loss: 1.7093345327302814, Final Batch Loss: 0.005501131527125835\n",
      "Epoch 3772, Loss: 2.107903480529785, Final Batch Loss: 0.39986225962638855\n",
      "Epoch 3773, Loss: 1.6735059509519488, Final Batch Loss: 0.0001479277852922678\n",
      "Epoch 3774, Loss: 3.2081262469291687, Final Batch Loss: 1.560417652130127\n",
      "Epoch 3775, Loss: 1.697774093307089, Final Batch Loss: 0.0006287504802457988\n",
      "Epoch 3776, Loss: 4.533851534128189, Final Batch Loss: 2.8884096145629883\n",
      "Epoch 3777, Loss: 3.41023713350296, Final Batch Loss: 1.6420377492904663\n",
      "Epoch 3778, Loss: 2.6707685589790344, Final Batch Loss: 1.0072686672210693\n",
      "Epoch 3779, Loss: 1.6220543435774744, Final Batch Loss: 0.0007652691565454006\n",
      "Epoch 3780, Loss: 3.133792519569397, Final Batch Loss: 1.5003125667572021\n",
      "Epoch 3781, Loss: 2.4909583926200867, Final Batch Loss: 0.8727573156356812\n",
      "Epoch 3782, Loss: 2.427925407886505, Final Batch Loss: 0.7954323291778564\n",
      "Epoch 3783, Loss: 2.962444633245468, Final Batch Loss: 1.3755433559417725\n",
      "Epoch 3784, Loss: 3.229549318552017, Final Batch Loss: 1.6530076265335083\n",
      "Epoch 3785, Loss: 2.024179905653, Final Batch Loss: 0.40535613894462585\n",
      "Epoch 3786, Loss: 1.7173132353345864, Final Batch Loss: 0.0003295593778602779\n",
      "Epoch 3787, Loss: 1.8681877255439758, Final Batch Loss: 0.14498227834701538\n",
      "Epoch 3788, Loss: 1.7425493000100687, Final Batch Loss: 1.9192511899746023e-05\n",
      "Epoch 3789, Loss: 2.6060174703598022, Final Batch Loss: 0.8143735527992249\n",
      "Epoch 3790, Loss: 2.30962735414505, Final Batch Loss: 0.5331000685691833\n",
      "Epoch 3791, Loss: 1.7131338436156511, Final Batch Loss: 0.024710150435566902\n",
      "Epoch 3792, Loss: 3.986509621143341, Final Batch Loss: 2.3005635738372803\n",
      "Epoch 3793, Loss: 2.5745437145233154, Final Batch Loss: 0.8030072450637817\n",
      "Epoch 3794, Loss: 1.7071085721254349, Final Batch Loss: 0.007157281041145325\n",
      "Epoch 3795, Loss: 3.73063862323761, Final Batch Loss: 2.0105974674224854\n",
      "Epoch 3796, Loss: 2.4550294876098633, Final Batch Loss: 0.5807602405548096\n",
      "Epoch 3797, Loss: 2.1185147166252136, Final Batch Loss: 0.277506947517395\n",
      "Epoch 3798, Loss: 2.9459065198898315, Final Batch Loss: 1.149813175201416\n",
      "Epoch 3799, Loss: 1.8640998229384422, Final Batch Loss: 0.060082338750362396\n",
      "Epoch 3800, Loss: 4.017304420471191, Final Batch Loss: 2.228917360305786\n",
      "Epoch 3801, Loss: 2.6196892261505127, Final Batch Loss: 0.6811158061027527\n",
      "Epoch 3802, Loss: 1.8745881635695696, Final Batch Loss: 0.013529511168599129\n",
      "Epoch 3803, Loss: 1.7438395684584975, Final Batch Loss: 0.00948388036340475\n",
      "Epoch 3804, Loss: 3.9356181025505066, Final Batch Loss: 2.1711299419403076\n",
      "Epoch 3805, Loss: 2.1293720304965973, Final Batch Loss: 0.4722450077533722\n",
      "Epoch 3806, Loss: 1.727382117358502, Final Batch Loss: 0.0005005536950193346\n",
      "Epoch 3807, Loss: 3.2284751534461975, Final Batch Loss: 1.4961413145065308\n",
      "Epoch 3808, Loss: 1.6590104759961832, Final Batch Loss: 0.0003271759778726846\n",
      "Epoch 3809, Loss: 3.9139793515205383, Final Batch Loss: 2.1707725524902344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3810, Loss: 1.667299821972847, Final Batch Loss: 0.035893335938453674\n",
      "Epoch 3811, Loss: 1.6478509828448296, Final Batch Loss: 0.043076418340206146\n",
      "Epoch 3812, Loss: 1.919768288731575, Final Batch Loss: 0.21025104820728302\n",
      "Epoch 3813, Loss: 2.3012470602989197, Final Batch Loss: 0.5635896325111389\n",
      "Epoch 3814, Loss: 1.7527275010943413, Final Batch Loss: 0.12142547219991684\n",
      "Epoch 3815, Loss: 2.737372398376465, Final Batch Loss: 1.063612461090088\n",
      "Epoch 3816, Loss: 3.0690011382102966, Final Batch Loss: 1.3906666040420532\n",
      "Epoch 3817, Loss: 2.6782422363758087, Final Batch Loss: 0.9501352310180664\n",
      "Epoch 3818, Loss: 1.7660971275654447, Final Batch Loss: 4.0649541915627196e-05\n",
      "Epoch 3819, Loss: 3.844092845916748, Final Batch Loss: 2.189462900161743\n",
      "Epoch 3820, Loss: 4.777537226676941, Final Batch Loss: 3.0461068153381348\n",
      "Epoch 3821, Loss: 2.1702041625976562, Final Batch Loss: 0.43100792169570923\n",
      "Epoch 3822, Loss: 2.6900240778923035, Final Batch Loss: 1.0165220499038696\n",
      "Epoch 3823, Loss: 2.3914108872413635, Final Batch Loss: 0.7454748153686523\n",
      "Epoch 3824, Loss: 3.4855141043663025, Final Batch Loss: 1.69205641746521\n",
      "Epoch 3825, Loss: 2.955877423286438, Final Batch Loss: 1.226252555847168\n",
      "Epoch 3826, Loss: 1.921623557806015, Final Batch Loss: 0.2565039098262787\n",
      "Epoch 3827, Loss: 1.8476405292749405, Final Batch Loss: 0.07235874235630035\n",
      "Epoch 3828, Loss: 1.8770633954554796, Final Batch Loss: 0.031076671555638313\n",
      "Epoch 3829, Loss: 2.3507535457611084, Final Batch Loss: 0.5602400898933411\n",
      "Epoch 3830, Loss: 1.9871356636285782, Final Batch Loss: 0.20386190712451935\n",
      "Epoch 3831, Loss: 1.7855843156576157, Final Batch Loss: 0.0967085212469101\n",
      "Epoch 3832, Loss: 1.675764910876751, Final Batch Loss: 0.03932245820760727\n",
      "Epoch 3833, Loss: 2.072879284620285, Final Batch Loss: 0.39427897334098816\n",
      "Epoch 3834, Loss: 1.686366726178676, Final Batch Loss: 0.00226788641884923\n",
      "Epoch 3835, Loss: 2.2311943769454956, Final Batch Loss: 0.5376771092414856\n",
      "Epoch 3836, Loss: 4.295871138572693, Final Batch Loss: 2.5947184562683105\n",
      "Epoch 3837, Loss: 1.7038434110581875, Final Batch Loss: 0.016227778047323227\n",
      "Epoch 3838, Loss: 2.409926474094391, Final Batch Loss: 0.6068652868270874\n",
      "Epoch 3839, Loss: 1.795916438481072, Final Batch Loss: 0.00048744716332294047\n",
      "Epoch 3840, Loss: 2.6839290857315063, Final Batch Loss: 0.9310482144355774\n",
      "Epoch 3841, Loss: 2.62588632106781, Final Batch Loss: 0.899333119392395\n",
      "Epoch 3842, Loss: 2.051806479692459, Final Batch Loss: 0.3669048249721527\n",
      "Epoch 3843, Loss: 1.7171218576841056, Final Batch Loss: 0.005991119425743818\n",
      "Epoch 3844, Loss: 1.763818260282278, Final Batch Loss: 0.020604845136404037\n",
      "Epoch 3845, Loss: 1.738118794746697, Final Batch Loss: 0.010737028904259205\n",
      "Epoch 3846, Loss: 1.9522200599312782, Final Batch Loss: 0.06943211704492569\n",
      "Epoch 3847, Loss: 1.74484121799469, Final Batch Loss: 0.032447218894958496\n",
      "Epoch 3848, Loss: 2.24247869849205, Final Batch Loss: 0.67799973487854\n",
      "Epoch 3849, Loss: 2.5937309861183167, Final Batch Loss: 0.8928413391113281\n",
      "Epoch 3850, Loss: 3.1826748847961426, Final Batch Loss: 1.5965139865875244\n",
      "Epoch 3851, Loss: 1.708821220556274, Final Batch Loss: 0.0024953915271908045\n",
      "Epoch 3852, Loss: 2.796282649040222, Final Batch Loss: 1.1953768730163574\n",
      "Epoch 3853, Loss: 2.017579287290573, Final Batch Loss: 0.34156402945518494\n",
      "Epoch 3854, Loss: 1.862269639968872, Final Batch Loss: 0.28575998544692993\n",
      "Epoch 3855, Loss: 1.7493178248399772, Final Batch Loss: 1.0728830375228426e-06\n",
      "Epoch 3856, Loss: 2.2102303206920624, Final Batch Loss: 0.4241507351398468\n",
      "Epoch 3857, Loss: 1.827110182493925, Final Batch Loss: 0.03569389507174492\n",
      "Epoch 3858, Loss: 1.7842821031808853, Final Batch Loss: 0.1194458156824112\n",
      "Epoch 3859, Loss: 1.5798426009714603, Final Batch Loss: 0.04554668441414833\n",
      "Epoch 3860, Loss: 1.7955877408385277, Final Batch Loss: 0.12261294573545456\n",
      "Epoch 3861, Loss: 1.6404495537281036, Final Batch Loss: 0.03886929154396057\n",
      "Epoch 3862, Loss: 1.6654456346295774, Final Batch Loss: 0.006981739308685064\n",
      "Epoch 3863, Loss: 1.613056086236611, Final Batch Loss: 0.0010707604233175516\n",
      "Epoch 3864, Loss: 3.9459104239940643, Final Batch Loss: 2.31095814704895\n",
      "Epoch 3865, Loss: 2.068594127893448, Final Batch Loss: 0.520965039730072\n",
      "Epoch 3866, Loss: 3.710176408290863, Final Batch Loss: 2.112152576446533\n",
      "Epoch 3867, Loss: 1.7901513390243053, Final Batch Loss: 0.03984598442912102\n",
      "Epoch 3868, Loss: 3.0102556943893433, Final Batch Loss: 1.345456838607788\n",
      "Epoch 3869, Loss: 2.8433730006217957, Final Batch Loss: 1.1653844118118286\n",
      "Epoch 3870, Loss: 1.7471710545942187, Final Batch Loss: 0.006311128847301006\n",
      "Epoch 3871, Loss: 2.03559547662735, Final Batch Loss: 0.37571191787719727\n",
      "Epoch 3872, Loss: 1.6945929448120296, Final Batch Loss: 0.006266824435442686\n",
      "Epoch 3873, Loss: 3.132726728916168, Final Batch Loss: 1.3589359521865845\n",
      "Epoch 3874, Loss: 3.022703528404236, Final Batch Loss: 1.3501126766204834\n",
      "Epoch 3875, Loss: 2.169523298740387, Final Batch Loss: 0.54368656873703\n",
      "Epoch 3876, Loss: 1.7166536375880241, Final Batch Loss: 0.047153227031230927\n",
      "Epoch 3877, Loss: 1.7623531147837639, Final Batch Loss: 0.07010339945554733\n",
      "Epoch 3878, Loss: 1.6824537860229611, Final Batch Loss: 0.01318803895264864\n",
      "Epoch 3879, Loss: 3.53788822889328, Final Batch Loss: 1.8458751440048218\n",
      "Epoch 3880, Loss: 1.6732234731316566, Final Batch Loss: 0.0058676376938819885\n",
      "Epoch 3881, Loss: 2.18164199590683, Final Batch Loss: 0.5326098799705505\n",
      "Epoch 3882, Loss: 2.3556519746780396, Final Batch Loss: 0.6693122982978821\n",
      "Epoch 3883, Loss: 1.9969764649868011, Final Batch Loss: 0.3492099940776825\n",
      "Epoch 3884, Loss: 1.6877580829896033, Final Batch Loss: 0.002584571484476328\n",
      "Epoch 3885, Loss: 3.4323024451732635, Final Batch Loss: 1.8217923641204834\n",
      "Epoch 3886, Loss: 1.7691383585333824, Final Batch Loss: 0.07428213208913803\n",
      "Epoch 3887, Loss: 5.176913261413574, Final Batch Loss: 3.467104911804199\n",
      "Epoch 3888, Loss: 1.6108942097052932, Final Batch Loss: 0.0031323200091719627\n",
      "Epoch 3889, Loss: 2.1701644361019135, Final Batch Loss: 0.41373327374458313\n",
      "Epoch 3890, Loss: 2.2072708904743195, Final Batch Loss: 0.4795367419719696\n",
      "Epoch 3891, Loss: 3.1062082648277283, Final Batch Loss: 1.3392115831375122\n",
      "Epoch 3892, Loss: 3.846807062625885, Final Batch Loss: 2.120631456375122\n",
      "Epoch 3893, Loss: 1.6463861825177446, Final Batch Loss: 0.0011130335042253137\n",
      "Epoch 3894, Loss: 2.291283130645752, Final Batch Loss: 0.610222339630127\n",
      "Epoch 3895, Loss: 3.661489248275757, Final Batch Loss: 2.001162052154541\n",
      "Epoch 3896, Loss: 2.9269730746746063, Final Batch Loss: 1.1544208526611328\n",
      "Epoch 3897, Loss: 1.7469186727539636, Final Batch Loss: 0.0008521024719811976\n",
      "Epoch 3898, Loss: 4.7578085064888, Final Batch Loss: 2.719834566116333\n",
      "Epoch 3899, Loss: 2.4677075119689107, Final Batch Loss: 0.007184621877968311\n",
      "Epoch 3900, Loss: 4.324642241001129, Final Batch Loss: 2.1643362045288086\n",
      "Epoch 3901, Loss: 2.1723130829632282, Final Batch Loss: 0.008958149701356888\n",
      "Epoch 3902, Loss: 1.9380500681872945, Final Batch Loss: 0.00040665941196493804\n",
      "Epoch 3903, Loss: 5.021342635154724, Final Batch Loss: 3.199265241622925\n",
      "Epoch 3904, Loss: 1.9302262142300606, Final Batch Loss: 0.09457094222307205\n",
      "Epoch 3905, Loss: 2.0204456076025963, Final Batch Loss: 0.08113857358694077\n",
      "Epoch 3906, Loss: 2.1320343911647797, Final Batch Loss: 0.24546357989311218\n",
      "Epoch 3907, Loss: 3.061264991760254, Final Batch Loss: 1.2437379360198975\n",
      "Epoch 3908, Loss: 1.954803854227066, Final Batch Loss: 0.21870848536491394\n",
      "Epoch 3909, Loss: 1.8783654570579529, Final Batch Loss: 0.06452751159667969\n",
      "Epoch 3910, Loss: 2.1253772377967834, Final Batch Loss: 0.3942389488220215\n",
      "Epoch 3911, Loss: 1.7228323220724633, Final Batch Loss: 9.775113539944869e-06\n",
      "Epoch 3912, Loss: 4.466058313846588, Final Batch Loss: 2.765291213989258\n",
      "Epoch 3913, Loss: 1.7191417813301086, Final Batch Loss: 0.07504284381866455\n",
      "Epoch 3914, Loss: 1.7505021006800234, Final Batch Loss: 0.006190648768097162\n",
      "Epoch 3915, Loss: 1.8794219940900803, Final Batch Loss: 0.18236778676509857\n",
      "Epoch 3916, Loss: 1.6808955073353218, Final Batch Loss: 8.344646857949556e-07\n",
      "Epoch 3917, Loss: 2.3663429617881775, Final Batch Loss: 0.646562397480011\n",
      "Epoch 3918, Loss: 2.7267890572547913, Final Batch Loss: 1.0294020175933838\n",
      "Epoch 3919, Loss: 2.105179399251938, Final Batch Loss: 0.42442893981933594\n",
      "Epoch 3920, Loss: 1.747597562149167, Final Batch Loss: 0.015367435291409492\n",
      "Epoch 3921, Loss: 1.7129529360681772, Final Batch Loss: 0.004506194964051247\n",
      "Epoch 3922, Loss: 1.8321395516395569, Final Batch Loss: 0.15154403448104858\n",
      "Epoch 3923, Loss: 1.8766525983810425, Final Batch Loss: 0.18549537658691406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3924, Loss: 3.1129640340805054, Final Batch Loss: 1.3945393562316895\n",
      "Epoch 3925, Loss: 1.6907110214233398, Final Batch Loss: 0.0\n",
      "Epoch 3926, Loss: 1.5778259169310331, Final Batch Loss: 0.017536116763949394\n",
      "Epoch 3927, Loss: 1.6821434833109379, Final Batch Loss: 0.04197714105248451\n",
      "Epoch 3928, Loss: 1.704603774473071, Final Batch Loss: 0.006177498027682304\n",
      "Epoch 3929, Loss: 2.2019715309143066, Final Batch Loss: 0.46985840797424316\n",
      "Epoch 3930, Loss: 3.7596924006938934, Final Batch Loss: 2.1598949432373047\n",
      "Epoch 3931, Loss: 2.0066868364810944, Final Batch Loss: 0.31707099080085754\n",
      "Epoch 3932, Loss: 1.6523173980094725, Final Batch Loss: 0.0001161031104857102\n",
      "Epoch 3933, Loss: 1.6213484648615122, Final Batch Loss: 0.016153061762452126\n",
      "Epoch 3934, Loss: 3.349399656057358, Final Batch Loss: 1.7973823547363281\n",
      "Epoch 3935, Loss: 1.718825250864029, Final Batch Loss: 0.1055162250995636\n",
      "Epoch 3936, Loss: 2.4813424944877625, Final Batch Loss: 0.7388009428977966\n",
      "Epoch 3937, Loss: 1.6190945748239756, Final Batch Loss: 0.02238401211798191\n",
      "Epoch 3938, Loss: 2.377530097961426, Final Batch Loss: 0.6624040603637695\n",
      "Epoch 3939, Loss: 1.6890986040234566, Final Batch Loss: 0.013596780598163605\n",
      "Epoch 3940, Loss: 1.5495510809123516, Final Batch Loss: 0.017116796225309372\n",
      "Epoch 3941, Loss: 1.5700992384226993, Final Batch Loss: 0.0010604002745822072\n",
      "Epoch 3942, Loss: 1.590456961592281, Final Batch Loss: 4.458328112377785e-05\n",
      "Epoch 3943, Loss: 1.8104682862758636, Final Batch Loss: 0.23331031203269958\n",
      "Epoch 3944, Loss: 1.571630358508628, Final Batch Loss: 1.9311717551317997e-05\n",
      "Epoch 3945, Loss: 1.6966781690716743, Final Batch Loss: 0.111969955265522\n",
      "Epoch 3946, Loss: 1.511094472487457, Final Batch Loss: 0.0002759314374998212\n",
      "Epoch 3947, Loss: 1.7598441392183304, Final Batch Loss: 0.14365001022815704\n",
      "Epoch 3948, Loss: 1.639459267258644, Final Batch Loss: 0.08132882416248322\n",
      "Epoch 3949, Loss: 1.5573366056196392, Final Batch Loss: 0.004160202573984861\n",
      "Epoch 3950, Loss: 1.7309423200786114, Final Batch Loss: 0.060970913618803024\n",
      "Epoch 3951, Loss: 3.2242399156093597, Final Batch Loss: 1.6538053750991821\n",
      "Epoch 3952, Loss: 1.614555150270462, Final Batch Loss: 0.10305818915367126\n",
      "Epoch 3953, Loss: 1.5633323462679982, Final Batch Loss: 0.009276148863136768\n",
      "Epoch 3954, Loss: 3.7534762620925903, Final Batch Loss: 2.1314611434936523\n",
      "Epoch 3955, Loss: 1.6480959951877594, Final Batch Loss: 0.08359378576278687\n",
      "Epoch 3956, Loss: 1.6823082566115772, Final Batch Loss: 5.364403477869928e-06\n",
      "Epoch 3957, Loss: 2.434096574783325, Final Batch Loss: 0.8115558624267578\n",
      "Epoch 3958, Loss: 3.8094932436943054, Final Batch Loss: 2.1663379669189453\n",
      "Epoch 3959, Loss: 3.7725170850753784, Final Batch Loss: 2.164900779724121\n",
      "Epoch 3960, Loss: 1.6689244974404573, Final Batch Loss: 0.00568072684109211\n",
      "Epoch 3961, Loss: 1.7357238680124283, Final Batch Loss: 0.13177908957004547\n",
      "Epoch 3962, Loss: 2.7161747217178345, Final Batch Loss: 1.063382625579834\n",
      "Epoch 3963, Loss: 2.059058517217636, Final Batch Loss: 0.48228147625923157\n",
      "Epoch 3964, Loss: 3.2041681110858917, Final Batch Loss: 1.587090015411377\n",
      "Epoch 3965, Loss: 1.6437840134167345, Final Batch Loss: 7.688703772146255e-05\n",
      "Epoch 3966, Loss: 1.683339757262729, Final Batch Loss: 0.0006262486567720771\n",
      "Epoch 3967, Loss: 3.226413518190384, Final Batch Loss: 1.5807074308395386\n",
      "Epoch 3968, Loss: 1.911518782377243, Final Batch Loss: 0.33799150586128235\n",
      "Epoch 3969, Loss: 1.6536083347164094, Final Batch Loss: 0.0021495348773896694\n",
      "Epoch 3970, Loss: 2.3983992636203766, Final Batch Loss: 0.8444357514381409\n",
      "Epoch 3971, Loss: 2.3558168709278107, Final Batch Loss: 0.7783785462379456\n",
      "Epoch 3972, Loss: 2.0154176354408264, Final Batch Loss: 0.4111837148666382\n",
      "Epoch 3973, Loss: 2.845273733139038, Final Batch Loss: 1.2279577255249023\n",
      "Epoch 3974, Loss: 1.6613908410072327, Final Batch Loss: 0.03449767827987671\n",
      "Epoch 3975, Loss: 2.342407464981079, Final Batch Loss: 0.6969578266143799\n",
      "Epoch 3976, Loss: 1.6699646189808846, Final Batch Loss: 0.004362471401691437\n",
      "Epoch 3977, Loss: 1.885300651192665, Final Batch Loss: 0.16298861801624298\n",
      "Epoch 3978, Loss: 1.946761779487133, Final Batch Loss: 0.11014168709516525\n",
      "Epoch 3979, Loss: 2.1301075220108032, Final Batch Loss: 0.11297887563705444\n",
      "Epoch 3980, Loss: 2.3641364872455597, Final Batch Loss: 0.38989701867103577\n",
      "Epoch 3981, Loss: 3.610151946544647, Final Batch Loss: 1.6646149158477783\n",
      "Epoch 3982, Loss: 2.011787176069447, Final Batch Loss: 1.1205610462639015e-05\n",
      "Epoch 3983, Loss: 3.386804223060608, Final Batch Loss: 1.5939351320266724\n",
      "Epoch 3984, Loss: 2.0240100622177124, Final Batch Loss: 0.27778130769729614\n",
      "Epoch 3985, Loss: 3.0104867219924927, Final Batch Loss: 1.2356737852096558\n",
      "Epoch 3986, Loss: 4.342780977487564, Final Batch Loss: 2.6751811504364014\n",
      "Epoch 3987, Loss: 2.6128830313682556, Final Batch Loss: 0.8115476965904236\n",
      "Epoch 3988, Loss: 4.567736625671387, Final Batch Loss: 2.882615566253662\n",
      "Epoch 3989, Loss: 1.7456880807876587, Final Batch Loss: 0.0598376989364624\n",
      "Epoch 3990, Loss: 1.7256126936990768, Final Batch Loss: 0.0006995138246566057\n",
      "Epoch 3991, Loss: 1.6564544569700956, Final Batch Loss: 0.025023413822054863\n",
      "Epoch 3992, Loss: 6.868581563234329, Final Batch Loss: 5.196405410766602\n",
      "Epoch 3993, Loss: 1.6430076053366065, Final Batch Loss: 0.012023239396512508\n",
      "Epoch 3994, Loss: 2.4981871247291565, Final Batch Loss: 0.7935697436332703\n",
      "Epoch 3995, Loss: 2.382692515850067, Final Batch Loss: 0.6320271492004395\n",
      "Epoch 3996, Loss: 1.6989216804504395, Final Batch Loss: 0.0\n",
      "Epoch 3997, Loss: 2.063474118709564, Final Batch Loss: 0.27395719289779663\n",
      "Epoch 3998, Loss: 1.9083068668842316, Final Batch Loss: 0.1869012415409088\n",
      "Epoch 3999, Loss: 2.833020269870758, Final Batch Loss: 1.0866327285766602\n",
      "Epoch 4000, Loss: 3.5566293001174927, Final Batch Loss: 1.8591371774673462\n",
      "Epoch 4001, Loss: 1.7589262433466502, Final Batch Loss: 0.00024101213784888387\n",
      "Epoch 4002, Loss: 2.3883888721466064, Final Batch Loss: 0.6269773840904236\n",
      "Epoch 4003, Loss: 1.6567855114117265, Final Batch Loss: 0.015116655267775059\n",
      "Epoch 4004, Loss: 1.7990291193127632, Final Batch Loss: 0.08582813292741776\n",
      "Epoch 4005, Loss: 2.31158447265625, Final Batch Loss: 0.7392022013664246\n",
      "Epoch 4006, Loss: 3.2561782598495483, Final Batch Loss: 1.6037135124206543\n",
      "Epoch 4007, Loss: 4.177038490772247, Final Batch Loss: 2.365548849105835\n",
      "Epoch 4008, Loss: 2.914367437362671, Final Batch Loss: 1.1494667530059814\n",
      "Epoch 4009, Loss: 2.2586076259613037, Final Batch Loss: 0.4802631735801697\n",
      "Epoch 4010, Loss: 2.0871324837207794, Final Batch Loss: 0.2982371151447296\n",
      "Epoch 4011, Loss: 3.026930034160614, Final Batch Loss: 1.210508108139038\n",
      "Epoch 4012, Loss: 1.8426357507705688, Final Batch Loss: 0.15547198057174683\n",
      "Epoch 4013, Loss: 2.3723264634609222, Final Batch Loss: 0.48539939522743225\n",
      "Epoch 4014, Loss: 2.2640510499477386, Final Batch Loss: 0.3686007559299469\n",
      "Epoch 4015, Loss: 2.1373656122013927, Final Batch Loss: 0.009078296832740307\n",
      "Epoch 4016, Loss: 2.1906569823622704, Final Batch Loss: 0.0988321527838707\n",
      "Epoch 4017, Loss: 2.3624801635742188, Final Batch Loss: 0.09043252468109131\n",
      "Epoch 4018, Loss: 1.9555771239101887, Final Batch Loss: 0.05300520732998848\n",
      "Epoch 4019, Loss: 1.7416109945625067, Final Batch Loss: 0.008354594931006432\n",
      "Epoch 4020, Loss: 2.08469158411026, Final Batch Loss: 0.3194649815559387\n",
      "Epoch 4021, Loss: 1.8743571042150506, Final Batch Loss: 1.3112935448589269e-05\n",
      "Epoch 4022, Loss: 1.673052485100925, Final Batch Loss: 0.001095886342227459\n",
      "Epoch 4023, Loss: 1.7975256405770779, Final Batch Loss: 0.06239789351820946\n",
      "Epoch 4024, Loss: 1.6558964811265469, Final Batch Loss: 0.026392754167318344\n",
      "Epoch 4025, Loss: 1.7341546412808384, Final Batch Loss: 2.634490556374658e-05\n",
      "Epoch 4026, Loss: 1.6608445235178806, Final Batch Loss: 0.000376034586224705\n",
      "Epoch 4027, Loss: 1.6015027383109555, Final Batch Loss: 0.0010144332190975547\n",
      "Epoch 4028, Loss: 1.627252772450447, Final Batch Loss: 0.04486076533794403\n",
      "Epoch 4029, Loss: 3.1071658730506897, Final Batch Loss: 1.536579966545105\n",
      "Epoch 4030, Loss: 1.9291273057460785, Final Batch Loss: 0.37640276551246643\n",
      "Epoch 4031, Loss: 2.2599580883979797, Final Batch Loss: 0.6099233627319336\n",
      "Epoch 4032, Loss: 2.7912763357162476, Final Batch Loss: 1.1506210565567017\n",
      "Epoch 4033, Loss: 1.5939216314873192, Final Batch Loss: 0.0003455280384514481\n",
      "Epoch 4034, Loss: 1.6492018215358257, Final Batch Loss: 0.04951668903231621\n",
      "Epoch 4035, Loss: 1.8051844090223312, Final Batch Loss: 0.1670815795660019\n",
      "Epoch 4036, Loss: 2.7633076310157776, Final Batch Loss: 1.089009404182434\n",
      "Epoch 4037, Loss: 3.0543302297592163, Final Batch Loss: 1.5191285610198975\n",
      "Epoch 4038, Loss: 2.5733020901679993, Final Batch Loss: 0.8370748162269592\n",
      "Epoch 4039, Loss: 1.6292459941469133, Final Batch Loss: 0.0015061474405229092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4040, Loss: 1.713664773851633, Final Batch Loss: 0.05815953388810158\n",
      "Epoch 4041, Loss: 1.7572960834950209, Final Batch Loss: 0.0250051598995924\n",
      "Epoch 4042, Loss: 3.243346869945526, Final Batch Loss: 1.6337846517562866\n",
      "Epoch 4043, Loss: 1.8082530051469803, Final Batch Loss: 0.17686553299427032\n",
      "Epoch 4044, Loss: 1.593317672610283, Final Batch Loss: 0.05819271504878998\n",
      "Epoch 4045, Loss: 1.6376605164259672, Final Batch Loss: 0.006605101749300957\n",
      "Epoch 4046, Loss: 1.570410366402939, Final Batch Loss: 0.0021296695340424776\n",
      "Epoch 4047, Loss: 1.6859922260046005, Final Batch Loss: 0.06015317142009735\n",
      "Epoch 4048, Loss: 3.487010955810547, Final Batch Loss: 1.8883651494979858\n",
      "Epoch 4049, Loss: 3.2848053872585297, Final Batch Loss: 1.7252020835876465\n",
      "Epoch 4050, Loss: 2.3665086030960083, Final Batch Loss: 0.8573351502418518\n",
      "Epoch 4051, Loss: 1.6549685820937157, Final Batch Loss: 0.10473691672086716\n",
      "Epoch 4052, Loss: 1.677379958331585, Final Batch Loss: 0.02488144487142563\n",
      "Epoch 4053, Loss: 1.853864461183548, Final Batch Loss: 0.25440993905067444\n",
      "Epoch 4054, Loss: 3.0952946543693542, Final Batch Loss: 1.5277154445648193\n",
      "Epoch 4055, Loss: 3.720449447631836, Final Batch Loss: 2.093240976333618\n",
      "Epoch 4056, Loss: 1.8846610486507416, Final Batch Loss: 0.2894134819507599\n",
      "Epoch 4057, Loss: 1.7755958810448647, Final Batch Loss: 0.05853996425867081\n",
      "Epoch 4058, Loss: 1.8184202767442912, Final Batch Loss: 0.001801416976377368\n",
      "Epoch 4059, Loss: 1.7576709240674973, Final Batch Loss: 0.03339157998561859\n",
      "Epoch 4060, Loss: 2.5335575342178345, Final Batch Loss: 0.797305703163147\n",
      "Epoch 4061, Loss: 3.228947699069977, Final Batch Loss: 1.5434645414352417\n",
      "Epoch 4062, Loss: 3.168641299009323, Final Batch Loss: 1.523768663406372\n",
      "Epoch 4063, Loss: 3.8622551560401917, Final Batch Loss: 2.2180378437042236\n",
      "Epoch 4064, Loss: 3.4641356468200684, Final Batch Loss: 1.8521759510040283\n",
      "Epoch 4065, Loss: 3.834902435541153, Final Batch Loss: 2.3369226455688477\n",
      "Epoch 4066, Loss: 2.9878862500190735, Final Batch Loss: 1.4858038425445557\n",
      "Epoch 4067, Loss: 2.3119557201862335, Final Batch Loss: 0.682541012763977\n",
      "Epoch 4068, Loss: 1.6751436591148376, Final Batch Loss: 0.10080665349960327\n",
      "Epoch 4069, Loss: 2.902591973543167, Final Batch Loss: 1.3425185680389404\n",
      "Epoch 4070, Loss: 1.8333468586206436, Final Batch Loss: 0.22008167207241058\n",
      "Epoch 4071, Loss: 1.6278977624606341, Final Batch Loss: 0.0028586022090166807\n",
      "Epoch 4072, Loss: 2.1626356840133667, Final Batch Loss: 0.6043980121612549\n",
      "Epoch 4073, Loss: 1.5928881485015154, Final Batch Loss: 0.014472277835011482\n",
      "Epoch 4074, Loss: 1.874621033668518, Final Batch Loss: 0.33646243810653687\n",
      "Epoch 4075, Loss: 3.6716718673706055, Final Batch Loss: 2.1549878120422363\n",
      "Epoch 4076, Loss: 1.6498035714030266, Final Batch Loss: 0.09273337572813034\n",
      "Epoch 4077, Loss: 2.972444236278534, Final Batch Loss: 1.3580039739608765\n",
      "Epoch 4078, Loss: 1.863937109708786, Final Batch Loss: 0.2662335932254791\n",
      "Epoch 4079, Loss: 1.5633563958108425, Final Batch Loss: 0.032562728971242905\n",
      "Epoch 4080, Loss: 2.127774715423584, Final Batch Loss: 0.515444278717041\n",
      "Epoch 4081, Loss: 1.5935663580221444, Final Batch Loss: 1.156323378381785e-05\n",
      "Epoch 4082, Loss: 3.10124808549881, Final Batch Loss: 1.565467119216919\n",
      "Epoch 4083, Loss: 2.7079902291297913, Final Batch Loss: 1.1593765020370483\n",
      "Epoch 4084, Loss: 3.279074788093567, Final Batch Loss: 1.7031623125076294\n",
      "Epoch 4085, Loss: 1.64514684304595, Final Batch Loss: 0.0348205529153347\n",
      "Epoch 4086, Loss: 2.305383622646332, Final Batch Loss: 0.7009573578834534\n",
      "Epoch 4087, Loss: 1.6503214756958187, Final Batch Loss: 0.007304153870791197\n",
      "Epoch 4088, Loss: 1.5982470344752073, Final Batch Loss: 0.010489804670214653\n",
      "Epoch 4089, Loss: 1.678496964275837, Final Batch Loss: 0.04938984662294388\n",
      "Epoch 4090, Loss: 1.9880107641220093, Final Batch Loss: 0.4800358712673187\n",
      "Epoch 4091, Loss: 1.6467137881190865, Final Batch Loss: 0.00010156115604331717\n",
      "Epoch 4092, Loss: 1.5054064482392278, Final Batch Loss: 0.00020430385484360158\n",
      "Epoch 4093, Loss: 3.1617332100868225, Final Batch Loss: 1.594709038734436\n",
      "Epoch 4094, Loss: 2.8908205926418304, Final Batch Loss: 1.31173837184906\n",
      "Epoch 4095, Loss: 4.31536865234375, Final Batch Loss: 2.749393939971924\n",
      "Epoch 4096, Loss: 2.1277205646038055, Final Batch Loss: 0.4784332811832428\n",
      "Epoch 4097, Loss: 2.5330201387405396, Final Batch Loss: 0.9240829348564148\n",
      "Epoch 4098, Loss: 2.551623523235321, Final Batch Loss: 0.9242458343505859\n",
      "Epoch 4099, Loss: 1.5694499341771007, Final Batch Loss: 0.007930341176688671\n",
      "Epoch 4100, Loss: 1.5901574864983559, Final Batch Loss: 0.020148254930973053\n",
      "Epoch 4101, Loss: 1.5133101139217615, Final Batch Loss: 0.015080371871590614\n",
      "Epoch 4102, Loss: 3.172522723674774, Final Batch Loss: 1.6644256114959717\n",
      "Epoch 4103, Loss: 1.6490233913064003, Final Batch Loss: 0.08321509510278702\n",
      "Epoch 4104, Loss: 1.5830396111123264, Final Batch Loss: 0.005863844882696867\n",
      "Epoch 4105, Loss: 2.479152262210846, Final Batch Loss: 0.8694725036621094\n",
      "Epoch 4106, Loss: 1.6545431847916916, Final Batch Loss: 0.0014484162675216794\n",
      "Epoch 4107, Loss: 1.8104184325784445, Final Batch Loss: 0.014469927176833153\n",
      "Epoch 4108, Loss: 1.7953959628939629, Final Batch Loss: 0.0977654978632927\n",
      "Epoch 4109, Loss: 2.7258903682231903, Final Batch Loss: 1.12994384765625\n",
      "Epoch 4110, Loss: 2.8945314288139343, Final Batch Loss: 1.3679286241531372\n",
      "Epoch 4111, Loss: 1.5389939593151212, Final Batch Loss: 0.007542465813457966\n",
      "Epoch 4112, Loss: 1.6338416188955307, Final Batch Loss: 0.026742979884147644\n",
      "Epoch 4113, Loss: 4.078124076128006, Final Batch Loss: 2.5538947582244873\n",
      "Epoch 4114, Loss: 1.6416472345590591, Final Batch Loss: 0.08474816381931305\n",
      "Epoch 4115, Loss: 1.5773355457931757, Final Batch Loss: 0.025685081258416176\n",
      "Epoch 4116, Loss: 2.588907539844513, Final Batch Loss: 1.0613070726394653\n",
      "Epoch 4117, Loss: 2.360485255718231, Final Batch Loss: 0.7933467626571655\n",
      "Epoch 4118, Loss: 3.9701077938079834, Final Batch Loss: 2.3881115913391113\n",
      "Epoch 4119, Loss: 1.688004031777382, Final Batch Loss: 0.15048755705356598\n",
      "Epoch 4120, Loss: 1.4922222043387592, Final Batch Loss: 0.004236295353621244\n",
      "Epoch 4121, Loss: 2.434486210346222, Final Batch Loss: 0.8751187920570374\n",
      "Epoch 4122, Loss: 1.9489974081516266, Final Batch Loss: 0.3622586131095886\n",
      "Epoch 4123, Loss: 1.8997401893138885, Final Batch Loss: 0.2973547875881195\n",
      "Epoch 4124, Loss: 1.929929792881012, Final Batch Loss: 0.2689833343029022\n",
      "Epoch 4125, Loss: 1.690077326958999, Final Batch Loss: 0.0028380376752465963\n",
      "Epoch 4126, Loss: 2.553775370121002, Final Batch Loss: 0.8692551255226135\n",
      "Epoch 4127, Loss: 1.7047769194468856, Final Batch Loss: 0.01207671221345663\n",
      "Epoch 4128, Loss: 1.7115215938538313, Final Batch Loss: 0.01583489589393139\n",
      "Epoch 4129, Loss: 1.915386825799942, Final Batch Loss: 0.200332373380661\n",
      "Epoch 4130, Loss: 1.7666661767289042, Final Batch Loss: 0.0031240014359354973\n",
      "Epoch 4131, Loss: 3.2938717007637024, Final Batch Loss: 1.6718944311141968\n",
      "Epoch 4132, Loss: 4.033989489078522, Final Batch Loss: 2.3590927124023438\n",
      "Epoch 4133, Loss: 2.468264579772949, Final Batch Loss: 0.822476327419281\n",
      "Epoch 4134, Loss: 2.214520901441574, Final Batch Loss: 0.6129826307296753\n",
      "Epoch 4135, Loss: 3.3311053812503815, Final Batch Loss: 1.7166039943695068\n",
      "Epoch 4136, Loss: 1.8112841844558716, Final Batch Loss: 0.11526226997375488\n",
      "Epoch 4137, Loss: 2.7515898942947388, Final Batch Loss: 1.086012601852417\n",
      "Epoch 4138, Loss: 1.6916785009671003, Final Batch Loss: 0.0029157765675336123\n",
      "Epoch 4139, Loss: 1.6476360445813043, Final Batch Loss: 0.00015078838623594493\n",
      "Epoch 4140, Loss: 1.679659818764776, Final Batch Loss: 0.003075276967138052\n",
      "Epoch 4141, Loss: 1.9519630670547485, Final Batch Loss: 0.3179630637168884\n",
      "Epoch 4142, Loss: 2.3565795719623566, Final Batch Loss: 0.7393068075180054\n",
      "Epoch 4143, Loss: 1.6138667427003384, Final Batch Loss: 0.00858178362250328\n",
      "Epoch 4144, Loss: 1.7075455710291862, Final Batch Loss: 0.07313632220029831\n",
      "Epoch 4145, Loss: 2.0374030768871307, Final Batch Loss: 0.32762208580970764\n",
      "Epoch 4146, Loss: 1.7182489322149195, Final Batch Loss: 0.0009467886411584914\n",
      "Epoch 4147, Loss: 2.05798277258873, Final Batch Loss: 0.2690446376800537\n",
      "Epoch 4148, Loss: 1.6819992027594708, Final Batch Loss: 0.0007768235518597066\n",
      "Epoch 4149, Loss: 3.163506418466568, Final Batch Loss: 1.566017508506775\n",
      "Epoch 4150, Loss: 1.6965212114155293, Final Batch Loss: 0.02771662548184395\n",
      "Epoch 4151, Loss: 1.6186183360405266, Final Batch Loss: 0.0004948345012962818\n",
      "Epoch 4152, Loss: 2.22063672542572, Final Batch Loss: 0.5506250262260437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4153, Loss: 1.5586273040244123, Final Batch Loss: 8.237022848334163e-05\n",
      "Epoch 4154, Loss: 2.0687163174152374, Final Batch Loss: 0.46700355410575867\n",
      "Epoch 4155, Loss: 1.888271152973175, Final Batch Loss: 0.31411099433898926\n",
      "Epoch 4156, Loss: 1.5074263233691454, Final Batch Loss: 0.010922761633992195\n",
      "Epoch 4157, Loss: 1.620272476808168, Final Batch Loss: 0.0017146660247817636\n",
      "Epoch 4158, Loss: 1.6647614315152168, Final Batch Loss: 0.08801022917032242\n",
      "Epoch 4159, Loss: 1.56326262652874, Final Batch Loss: 0.005911722779273987\n",
      "Epoch 4160, Loss: 1.52478945254893, Final Batch Loss: 2.7418097943154862e-06\n",
      "Epoch 4161, Loss: 1.5552693617064506, Final Batch Loss: 0.0006887924391776323\n",
      "Epoch 4162, Loss: 1.5459076464144346, Final Batch Loss: 2.50339189733495e-06\n",
      "Epoch 4163, Loss: 1.6175904124975204, Final Batch Loss: 0.10386191308498383\n",
      "Epoch 4164, Loss: 1.8384406566619873, Final Batch Loss: 0.35744330286979675\n",
      "Epoch 4165, Loss: 1.565329065779224, Final Batch Loss: 0.0019264726433902979\n",
      "Epoch 4166, Loss: 1.5447112635156373, Final Batch Loss: 0.00023779425828251988\n",
      "Epoch 4167, Loss: 1.6569309756159782, Final Batch Loss: 0.12453477829694748\n",
      "Epoch 4168, Loss: 1.5510522183030844, Final Batch Loss: 0.026125794276595116\n",
      "Epoch 4169, Loss: 1.8221451342105865, Final Batch Loss: 0.26175516843795776\n",
      "Epoch 4170, Loss: 1.5300746819702908, Final Batch Loss: 0.0017107388703152537\n",
      "Epoch 4171, Loss: 1.4934442825615406, Final Batch Loss: 0.048449236899614334\n",
      "Epoch 4172, Loss: 1.4386583282612264, Final Batch Loss: 0.006312550511211157\n",
      "Epoch 4173, Loss: 1.5322090678419045, Final Batch Loss: 3.731181277544238e-05\n",
      "Epoch 4174, Loss: 2.0420471131801605, Final Batch Loss: 0.5796486139297485\n",
      "Epoch 4175, Loss: 1.5267697698436677, Final Batch Loss: 0.0024474686942994595\n",
      "Epoch 4176, Loss: 2.7428950667381287, Final Batch Loss: 1.1621778011322021\n",
      "Epoch 4177, Loss: 2.2113426625728607, Final Batch Loss: 0.6513258814811707\n",
      "Epoch 4178, Loss: 2.518581509590149, Final Batch Loss: 0.999082624912262\n",
      "Epoch 4179, Loss: 1.5971927607315592, Final Batch Loss: 0.0006038511055521667\n",
      "Epoch 4180, Loss: 1.691307628992945, Final Batch Loss: 0.0034435042180120945\n",
      "Epoch 4181, Loss: 2.591861456632614, Final Batch Loss: 1.1785953044891357\n",
      "Epoch 4182, Loss: 1.8118705302476883, Final Batch Loss: 0.21219401061534882\n",
      "Epoch 4183, Loss: 3.2253918647766113, Final Batch Loss: 1.6670167446136475\n",
      "Epoch 4184, Loss: 1.52982715703547, Final Batch Loss: 0.007277408614754677\n",
      "Epoch 4185, Loss: 1.5673484280705452, Final Batch Loss: 0.045666731894016266\n",
      "Epoch 4186, Loss: 1.5703221367439255, Final Batch Loss: 0.0019006537040695548\n",
      "Epoch 4187, Loss: 3.137288987636566, Final Batch Loss: 1.6449379920959473\n",
      "Epoch 4188, Loss: 1.495918273274583, Final Batch Loss: 3.611976353568025e-05\n",
      "Epoch 4189, Loss: 2.19070103764534, Final Batch Loss: 0.5649261474609375\n",
      "Epoch 4190, Loss: 2.6606980562210083, Final Batch Loss: 1.0967134237289429\n",
      "Epoch 4191, Loss: 1.9994798302650452, Final Batch Loss: 0.41881972551345825\n",
      "Epoch 4192, Loss: 1.5300478031858802, Final Batch Loss: 0.006359814666211605\n",
      "Epoch 4193, Loss: 3.4124778509140015, Final Batch Loss: 1.887204885482788\n",
      "Epoch 4194, Loss: 1.7963902652263641, Final Batch Loss: 0.28912970423698425\n",
      "Epoch 4195, Loss: 3.7431355714797974, Final Batch Loss: 2.166490077972412\n",
      "Epoch 4196, Loss: 1.7287848747946555, Final Batch Loss: 0.00016842853801790625\n",
      "Epoch 4197, Loss: 1.6259170724079013, Final Batch Loss: 0.006489868275821209\n",
      "Epoch 4198, Loss: 1.9893102645874023, Final Batch Loss: 0.32460933923721313\n",
      "Epoch 4199, Loss: 1.6665686555206776, Final Batch Loss: 0.016554612666368484\n",
      "Epoch 4200, Loss: 2.9429013431072235, Final Batch Loss: 1.1965465545654297\n",
      "Epoch 4201, Loss: 3.6055189967155457, Final Batch Loss: 1.9794679880142212\n",
      "Epoch 4202, Loss: 2.090020179748535, Final Batch Loss: 0.3616129755973816\n",
      "Epoch 4203, Loss: 1.6855007987178396, Final Batch Loss: 6.151010165922344e-05\n",
      "Epoch 4204, Loss: 2.882927119731903, Final Batch Loss: 1.3630294799804688\n",
      "Epoch 4205, Loss: 1.6455542305484414, Final Batch Loss: 0.004770801402628422\n",
      "Epoch 4206, Loss: 2.953745812177658, Final Batch Loss: 1.4138882160186768\n",
      "Epoch 4207, Loss: 2.3221233189105988, Final Batch Loss: 0.736743688583374\n",
      "Epoch 4208, Loss: 1.605920247733593, Final Batch Loss: 0.03232788294553757\n",
      "Epoch 4209, Loss: 1.7196575552225113, Final Batch Loss: 0.11582736670970917\n",
      "Epoch 4210, Loss: 1.609472630545497, Final Batch Loss: 0.004316655918955803\n",
      "Epoch 4211, Loss: 1.734556332230568, Final Batch Loss: 0.027350619435310364\n",
      "Epoch 4212, Loss: 1.599268306978047, Final Batch Loss: 0.006267179735004902\n",
      "Epoch 4213, Loss: 1.6236789822576725, Final Batch Loss: 5.960462772236497e-07\n",
      "Epoch 4214, Loss: 1.4577786289155483, Final Batch Loss: 0.01620044931769371\n",
      "Epoch 4215, Loss: 2.946887820959091, Final Batch Loss: 1.3977024555206299\n",
      "Epoch 4216, Loss: 2.0223494172096252, Final Batch Loss: 0.538587212562561\n",
      "Epoch 4217, Loss: 3.9412297308444977, Final Batch Loss: 2.458808422088623\n",
      "Epoch 4218, Loss: 2.78274267911911, Final Batch Loss: 1.200661540031433\n",
      "Epoch 4219, Loss: 1.945902407169342, Final Batch Loss: 0.42099347710609436\n",
      "Epoch 4220, Loss: 3.27368825674057, Final Batch Loss: 1.7473386526107788\n",
      "Epoch 4221, Loss: 2.0217855870723724, Final Batch Loss: 0.46163210272789\n",
      "Epoch 4222, Loss: 1.5529797069902997, Final Batch Loss: 0.00034731553751043975\n",
      "Epoch 4223, Loss: 1.5027200877625546, Final Batch Loss: 2.861018856492592e-06\n",
      "Epoch 4224, Loss: 1.6228023022413254, Final Batch Loss: 0.06592364609241486\n",
      "Epoch 4225, Loss: 1.516703245230019, Final Batch Loss: 0.0023547085002064705\n",
      "Epoch 4226, Loss: 1.7129551619291306, Final Batch Loss: 0.19024012982845306\n",
      "Epoch 4227, Loss: 1.5348284780047834, Final Batch Loss: 0.005261146929115057\n",
      "Epoch 4228, Loss: 2.1798583567142487, Final Batch Loss: 0.598065197467804\n",
      "Epoch 4229, Loss: 2.606584519147873, Final Batch Loss: 1.1533739566802979\n",
      "Epoch 4230, Loss: 2.8264331817626953, Final Batch Loss: 1.2947437763214111\n",
      "Epoch 4231, Loss: 1.9399215281009674, Final Batch Loss: 0.11606165766716003\n",
      "Epoch 4232, Loss: 1.7101417034864426, Final Batch Loss: 0.014806851744651794\n",
      "Epoch 4233, Loss: 1.5993842696771026, Final Batch Loss: 0.010141155682504177\n",
      "Epoch 4234, Loss: 1.688301044050604, Final Batch Loss: 0.001912789884954691\n",
      "Epoch 4235, Loss: 1.6521153654903173, Final Batch Loss: 0.0052049364894628525\n",
      "Epoch 4236, Loss: 1.6954854195937514, Final Batch Loss: 0.008209413848817348\n",
      "Epoch 4237, Loss: 1.54720225953497, Final Batch Loss: 0.0030574502889066935\n",
      "Epoch 4238, Loss: 1.5536888143979013, Final Batch Loss: 0.004862507339566946\n",
      "Epoch 4239, Loss: 1.9663099944591522, Final Batch Loss: 0.45518484711647034\n",
      "Epoch 4240, Loss: 1.5012330571189523, Final Batch Loss: 0.012981311418116093\n",
      "Epoch 4241, Loss: 5.7805348336696625, Final Batch Loss: 4.279013156890869\n",
      "Epoch 4242, Loss: 1.5564052164554596, Final Batch Loss: 0.03823384642601013\n",
      "Epoch 4243, Loss: 1.616840185597539, Final Batch Loss: 0.016789676621556282\n",
      "Epoch 4244, Loss: 1.8028129041194916, Final Batch Loss: 0.08868846297264099\n",
      "Epoch 4245, Loss: 1.8754508532583714, Final Batch Loss: 0.005127612501382828\n",
      "Epoch 4246, Loss: 2.4131155014038086, Final Batch Loss: 0.6599454283714294\n",
      "Epoch 4247, Loss: 1.6984753771685064, Final Batch Loss: 0.005285100545734167\n",
      "Epoch 4248, Loss: 2.3526859283447266, Final Batch Loss: 0.6053305864334106\n",
      "Epoch 4249, Loss: 1.6569105684757233, Final Batch Loss: 0.09073272347450256\n",
      "Epoch 4250, Loss: 2.9304827451705933, Final Batch Loss: 1.2894399166107178\n",
      "Epoch 4251, Loss: 1.6159078478813171, Final Batch Loss: 0.06053239107131958\n",
      "Epoch 4252, Loss: 1.6032544025219977, Final Batch Loss: 0.0062805661000311375\n",
      "Epoch 4253, Loss: 1.491838320158422, Final Batch Loss: 0.012947185896337032\n",
      "Epoch 4254, Loss: 1.8996621072292328, Final Batch Loss: 0.43836870789527893\n",
      "Epoch 4255, Loss: 1.7293961197137833, Final Batch Loss: 0.09251321852207184\n",
      "Epoch 4256, Loss: 2.7787740528583527, Final Batch Loss: 1.2897206544876099\n",
      "Epoch 4257, Loss: 1.5765872493357165, Final Batch Loss: 0.00014447122521232814\n",
      "Epoch 4258, Loss: 2.0457435846328735, Final Batch Loss: 0.12909990549087524\n",
      "Epoch 4259, Loss: 2.91435968875885, Final Batch Loss: 1.0393767356872559\n",
      "Epoch 4260, Loss: 3.0370596051216125, Final Batch Loss: 1.141188144683838\n",
      "Epoch 4261, Loss: 1.9523054225137457, Final Batch Loss: 0.0010487301042303443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4262, Loss: 3.8438135981559753, Final Batch Loss: 1.923844814300537\n",
      "Epoch 4263, Loss: 1.9898983351886272, Final Batch Loss: 0.016834806650877\n",
      "Epoch 4264, Loss: 3.2982866764068604, Final Batch Loss: 1.426220417022705\n",
      "Epoch 4265, Loss: 1.7365907244384289, Final Batch Loss: 0.02693156525492668\n",
      "Epoch 4266, Loss: 3.209807336330414, Final Batch Loss: 1.44087553024292\n",
      "Epoch 4267, Loss: 2.744883179664612, Final Batch Loss: 0.8982177972793579\n",
      "Epoch 4268, Loss: 3.55411559343338, Final Batch Loss: 1.8302884101867676\n",
      "Epoch 4269, Loss: 1.6633935449644923, Final Batch Loss: 0.002161073498427868\n",
      "Epoch 4270, Loss: 1.9495368599891663, Final Batch Loss: 0.3026590347290039\n",
      "Epoch 4271, Loss: 2.5062862634658813, Final Batch Loss: 0.7438316345214844\n",
      "Epoch 4272, Loss: 1.5927094454309554, Final Batch Loss: 0.00011383838864276186\n",
      "Epoch 4273, Loss: 1.7137693911790848, Final Batch Loss: 0.12399806082248688\n",
      "Epoch 4274, Loss: 2.3413186967372894, Final Batch Loss: 0.7391524314880371\n",
      "Epoch 4275, Loss: 1.7752816677093506, Final Batch Loss: 0.1760164499282837\n",
      "Epoch 4276, Loss: 3.806622564792633, Final Batch Loss: 2.074620246887207\n",
      "Epoch 4277, Loss: 1.7241856008768082, Final Batch Loss: 0.12705214321613312\n",
      "Epoch 4278, Loss: 1.6531167411303613, Final Batch Loss: 0.0004027270770166069\n",
      "Epoch 4279, Loss: 1.6971502589294687, Final Batch Loss: 0.0015636371681466699\n",
      "Epoch 4280, Loss: 2.4477447867393494, Final Batch Loss: 0.7453250288963318\n",
      "Epoch 4281, Loss: 1.6524099808157189, Final Batch Loss: 0.00019870213873218745\n",
      "Epoch 4282, Loss: 1.637443546205759, Final Batch Loss: 0.005677763372659683\n",
      "Epoch 4283, Loss: 4.428470432758331, Final Batch Loss: 2.7844161987304688\n",
      "Epoch 4284, Loss: 1.719931174069643, Final Batch Loss: 0.03669701889157295\n",
      "Epoch 4285, Loss: 1.938064068555832, Final Batch Loss: 0.1771012246608734\n",
      "Epoch 4286, Loss: 1.7686820626258566, Final Batch Loss: 2.3841855067985307e-07\n",
      "Epoch 4287, Loss: 1.7360645178705454, Final Batch Loss: 0.023174280300736427\n",
      "Epoch 4288, Loss: 1.7119814625475556, Final Batch Loss: 0.0034886470530182123\n",
      "Epoch 4289, Loss: 2.4737054109573364, Final Batch Loss: 0.8253836035728455\n",
      "Epoch 4290, Loss: 2.193580448627472, Final Batch Loss: 0.673758327960968\n",
      "Epoch 4291, Loss: 1.7553089410066605, Final Batch Loss: 0.20291225612163544\n",
      "Epoch 4292, Loss: 1.620638133957982, Final Batch Loss: 0.027786077931523323\n",
      "Epoch 4293, Loss: 2.0413545966148376, Final Batch Loss: 0.48868846893310547\n",
      "Epoch 4294, Loss: 2.2762776613235474, Final Batch Loss: 0.6749818325042725\n",
      "Epoch 4295, Loss: 1.5987049623581697, Final Batch Loss: 5.590759246842936e-05\n",
      "Epoch 4296, Loss: 1.7970621809363365, Final Batch Loss: 0.10086851567029953\n",
      "Epoch 4297, Loss: 1.7185521461069584, Final Batch Loss: 0.05514395609498024\n",
      "Epoch 4298, Loss: 2.0202018916606903, Final Batch Loss: 0.35396823287010193\n",
      "Epoch 4299, Loss: 1.7602545022500635, Final Batch Loss: 9.65590606938349e-06\n",
      "Epoch 4300, Loss: 2.8271451890468597, Final Batch Loss: 1.1126987934112549\n",
      "Epoch 4301, Loss: 1.8149663209915161, Final Batch Loss: 0.1217184066772461\n",
      "Epoch 4302, Loss: 3.2242119014263153, Final Batch Loss: 1.636551856994629\n",
      "Epoch 4303, Loss: 1.542155547067523, Final Batch Loss: 0.004966778680682182\n",
      "Epoch 4304, Loss: 1.6471726298264002, Final Batch Loss: 3.6954811548639555e-06\n",
      "Epoch 4305, Loss: 2.9175318479537964, Final Batch Loss: 1.3456203937530518\n",
      "Epoch 4306, Loss: 1.6111202532229072, Final Batch Loss: 3.397406908334233e-05\n",
      "Epoch 4307, Loss: 2.6512109637260437, Final Batch Loss: 1.0957006216049194\n",
      "Epoch 4308, Loss: 1.6818975061178207, Final Batch Loss: 0.14444725215435028\n",
      "Epoch 4309, Loss: 1.6046070285374299, Final Batch Loss: 0.0012343652779236436\n",
      "Epoch 4310, Loss: 1.8701913207769394, Final Batch Loss: 0.20666687190532684\n",
      "Epoch 4311, Loss: 1.5913307646114845, Final Batch Loss: 0.00025245340657420456\n",
      "Epoch 4312, Loss: 1.6225486396933775, Final Batch Loss: 1.2755313036905136e-05\n",
      "Epoch 4313, Loss: 1.9629860520362854, Final Batch Loss: 0.3962007761001587\n",
      "Epoch 4314, Loss: 1.6796892285346985, Final Batch Loss: 0.1492600440979004\n",
      "Epoch 4315, Loss: 1.5963481273502111, Final Batch Loss: 0.015085538849234581\n",
      "Epoch 4316, Loss: 2.89472833275795, Final Batch Loss: 1.3346705436706543\n",
      "Epoch 4317, Loss: 1.8883712440729141, Final Batch Loss: 0.22411401569843292\n",
      "Epoch 4318, Loss: 2.7339934706687927, Final Batch Loss: 1.1629053354263306\n",
      "Epoch 4319, Loss: 1.6297128349542618, Final Batch Loss: 0.017061248421669006\n",
      "Epoch 4320, Loss: 1.7164448238909245, Final Batch Loss: 0.0582028366625309\n",
      "Epoch 4321, Loss: 3.647036910057068, Final Batch Loss: 2.0169737339019775\n",
      "Epoch 4322, Loss: 1.7147780880331993, Final Batch Loss: 0.11267229169607162\n",
      "Epoch 4323, Loss: 1.5944512812420726, Final Batch Loss: 0.008271011523902416\n",
      "Epoch 4324, Loss: 1.7992607951164246, Final Batch Loss: 0.2841123342514038\n",
      "Epoch 4325, Loss: 1.6407801918685436, Final Batch Loss: 0.0494050495326519\n",
      "Epoch 4326, Loss: 1.5804490111768246, Final Batch Loss: 0.05539211258292198\n",
      "Epoch 4327, Loss: 3.341730624437332, Final Batch Loss: 1.676356554031372\n",
      "Epoch 4328, Loss: 2.8542054295539856, Final Batch Loss: 1.2248084545135498\n",
      "Epoch 4329, Loss: 1.5523072108626366, Final Batch Loss: 0.00501849502325058\n",
      "Epoch 4330, Loss: 1.5137911844067276, Final Batch Loss: 0.004244248848408461\n",
      "Epoch 4331, Loss: 1.559461069991812, Final Batch Loss: 0.0018518695142120123\n",
      "Epoch 4332, Loss: 2.306316554546356, Final Batch Loss: 0.6507645845413208\n",
      "Epoch 4333, Loss: 1.45898848453362, Final Batch Loss: 7.986703712958843e-05\n",
      "Epoch 4334, Loss: 2.7134298980236053, Final Batch Loss: 1.2002304792404175\n",
      "Epoch 4335, Loss: 2.59057354927063, Final Batch Loss: 0.9460204839706421\n",
      "Epoch 4336, Loss: 2.3335238695144653, Final Batch Loss: 0.8132060766220093\n",
      "Epoch 4337, Loss: 1.665870615048334, Final Batch Loss: 0.003649481339380145\n",
      "Epoch 4338, Loss: 2.2066812603734434, Final Batch Loss: 0.006659456994384527\n",
      "Epoch 4339, Loss: 2.6868247985839844, Final Batch Loss: 0.5361080765724182\n",
      "Epoch 4340, Loss: 1.7660437524318695, Final Batch Loss: 0.07876822352409363\n",
      "Epoch 4341, Loss: 2.0825133621692657, Final Batch Loss: 0.3387646973133087\n",
      "Epoch 4342, Loss: 2.510430157184601, Final Batch Loss: 0.7031592130661011\n",
      "Epoch 4343, Loss: 2.957194209098816, Final Batch Loss: 1.085192322731018\n",
      "Epoch 4344, Loss: 4.796379208564758, Final Batch Loss: 2.9540348052978516\n",
      "Epoch 4345, Loss: 1.7796713411808014, Final Batch Loss: 0.07536295056343079\n",
      "Epoch 4346, Loss: 2.184605836868286, Final Batch Loss: 0.3799372911453247\n",
      "Epoch 4347, Loss: 2.2339524626731873, Final Batch Loss: 0.3523293137550354\n",
      "Epoch 4348, Loss: 1.9014533117879182, Final Batch Loss: 0.0031072453130036592\n",
      "Epoch 4349, Loss: 1.9487973749637604, Final Batch Loss: 0.0943203866481781\n",
      "Epoch 4350, Loss: 1.745106753485743, Final Batch Loss: 0.0006032554083503783\n",
      "Epoch 4351, Loss: 1.7489603571593761, Final Batch Loss: 0.02153490111231804\n",
      "Epoch 4352, Loss: 1.6562999449670315, Final Batch Loss: 0.01299649104475975\n",
      "Epoch 4353, Loss: 2.673931837081909, Final Batch Loss: 1.0053918361663818\n",
      "Epoch 4354, Loss: 2.3181278705596924, Final Batch Loss: 0.5495225787162781\n",
      "Epoch 4355, Loss: 2.234511789458338, Final Batch Loss: 7.998623186722398e-05\n",
      "Epoch 4356, Loss: 3.3788062930107117, Final Batch Loss: 0.7880623936653137\n",
      "Epoch 4357, Loss: 3.048322021961212, Final Batch Loss: 0.6448290944099426\n",
      "Epoch 4358, Loss: 3.2408390045166016, Final Batch Loss: 1.335875391960144\n",
      "Epoch 4359, Loss: 1.8149758577341117, Final Batch Loss: 1.0728830375228426e-06\n",
      "Epoch 4360, Loss: 2.7595072388648987, Final Batch Loss: 0.922556459903717\n",
      "Epoch 4361, Loss: 1.9200825070874998, Final Batch Loss: 6.97350042173639e-05\n",
      "Epoch 4362, Loss: 1.705461796373129, Final Batch Loss: 0.015102799981832504\n",
      "Epoch 4363, Loss: 3.089119553565979, Final Batch Loss: 1.3874856233596802\n",
      "Epoch 4364, Loss: 1.9501498639583588, Final Batch Loss: 0.20252934098243713\n",
      "Epoch 4365, Loss: 1.7102393406821648, Final Batch Loss: 0.00014220656885299832\n",
      "Epoch 4366, Loss: 1.6207655071139015, Final Batch Loss: 1.4662635294371285e-05\n",
      "Epoch 4367, Loss: 1.6423615724779665, Final Batch Loss: 0.004971167538315058\n",
      "Epoch 4368, Loss: 2.91424822807312, Final Batch Loss: 1.2525922060012817\n",
      "Epoch 4369, Loss: 1.6953968331217766, Final Batch Loss: 0.11253552883863449\n",
      "Epoch 4370, Loss: 2.1757187247276306, Final Batch Loss: 0.5328680872917175\n",
      "Epoch 4371, Loss: 1.6433609673695173, Final Batch Loss: 0.0001003691868390888\n",
      "Epoch 4372, Loss: 1.6721501760184765, Final Batch Loss: 0.01647816225886345\n",
      "Epoch 4373, Loss: 1.6296922117471695, Final Batch Loss: 0.05234678089618683\n",
      "Epoch 4374, Loss: 2.1495935916900635, Final Batch Loss: 0.5584419965744019\n",
      "Epoch 4375, Loss: 2.6478246450424194, Final Batch Loss: 1.1346300840377808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4376, Loss: 1.4998887777327923, Final Batch Loss: 3.576278118089249e-07\n",
      "Epoch 4377, Loss: 3.201707184314728, Final Batch Loss: 1.6271088123321533\n",
      "Epoch 4378, Loss: 1.688354223035276, Final Batch Loss: 0.007343797944486141\n",
      "Epoch 4379, Loss: 1.7533550709486008, Final Batch Loss: 0.14294616878032684\n",
      "Epoch 4380, Loss: 2.3888075053691864, Final Batch Loss: 0.7914301753044128\n",
      "Epoch 4381, Loss: 1.5805845798749942, Final Batch Loss: 0.00026675479602999985\n",
      "Epoch 4382, Loss: 1.5144586572423577, Final Batch Loss: 0.002353043295443058\n",
      "Epoch 4383, Loss: 1.9599295556545258, Final Batch Loss: 0.34910228848457336\n",
      "Epoch 4384, Loss: 1.5642882049723994, Final Batch Loss: 0.00032729512895457447\n",
      "Epoch 4385, Loss: 1.9094629883766174, Final Batch Loss: 0.38914918899536133\n",
      "Epoch 4386, Loss: 1.527536709792912, Final Batch Loss: 0.002796669490635395\n",
      "Epoch 4387, Loss: 3.2966608107089996, Final Batch Loss: 1.803781270980835\n",
      "Epoch 4388, Loss: 1.543598664458841, Final Batch Loss: 0.002646160777658224\n",
      "Epoch 4389, Loss: 1.5399891733550248, Final Batch Loss: 1.07287787614041e-05\n",
      "Epoch 4390, Loss: 1.9699038565158844, Final Batch Loss: 0.3817099928855896\n",
      "Epoch 4391, Loss: 2.1213765740394592, Final Batch Loss: 0.6070362329483032\n",
      "Epoch 4392, Loss: 1.5570913385599852, Final Batch Loss: 0.0173140000551939\n",
      "Epoch 4393, Loss: 1.5143714547157288, Final Batch Loss: 0.035945892333984375\n",
      "Epoch 4394, Loss: 1.5628264439292252, Final Batch Loss: 0.003249604720622301\n",
      "Epoch 4395, Loss: 3.7187288105487823, Final Batch Loss: 2.2266151905059814\n",
      "Epoch 4396, Loss: 5.648993223905563, Final Batch Loss: 4.098968505859375\n",
      "Epoch 4397, Loss: 2.2780789136886597, Final Batch Loss: 0.6561515927314758\n",
      "Epoch 4398, Loss: 2.043673425912857, Final Batch Loss: 0.6131590008735657\n",
      "Epoch 4399, Loss: 1.5692744934931397, Final Batch Loss: 0.005877592600882053\n",
      "Epoch 4400, Loss: 1.9345037937164307, Final Batch Loss: 0.4912678897380829\n",
      "Epoch 4401, Loss: 1.9726298451423645, Final Batch Loss: 0.33159780502319336\n",
      "Epoch 4402, Loss: 1.6269141365773976, Final Batch Loss: 0.00643041031435132\n",
      "Epoch 4403, Loss: 1.5676505416631699, Final Batch Loss: 0.007243916392326355\n",
      "Epoch 4404, Loss: 1.516512026078999, Final Batch Loss: 0.004713614471256733\n",
      "Epoch 4405, Loss: 2.1324162781238556, Final Batch Loss: 0.5612238049507141\n",
      "Epoch 4406, Loss: 2.1363931596279144, Final Batch Loss: 0.6076580882072449\n",
      "Epoch 4407, Loss: 2.348890036344528, Final Batch Loss: 0.7762773036956787\n",
      "Epoch 4408, Loss: 1.9136643707752228, Final Batch Loss: 0.3910615146160126\n",
      "Epoch 4409, Loss: 2.098734676837921, Final Batch Loss: 0.502373218536377\n",
      "Epoch 4410, Loss: 1.6342494962736964, Final Batch Loss: 0.00305174570530653\n",
      "Epoch 4411, Loss: 1.6735930144786835, Final Batch Loss: 0.0\n",
      "Epoch 4412, Loss: 2.982999801635742, Final Batch Loss: 1.409082055091858\n",
      "Epoch 4413, Loss: 1.5204057991484206, Final Batch Loss: 2.0265558760002023e-06\n",
      "Epoch 4414, Loss: 1.8969454020261765, Final Batch Loss: 0.1787412017583847\n",
      "Epoch 4415, Loss: 1.6163642317405902, Final Batch Loss: 0.0005061537376604974\n",
      "Epoch 4416, Loss: 1.5386809478513896, Final Batch Loss: 0.005826986860483885\n",
      "Epoch 4417, Loss: 2.4847037196159363, Final Batch Loss: 0.9235863089561462\n",
      "Epoch 4418, Loss: 1.5791831319220364, Final Batch Loss: 0.0054649715311825275\n",
      "Epoch 4419, Loss: 1.5225265396293253, Final Batch Loss: 0.0011386347468942404\n",
      "Epoch 4420, Loss: 1.9466829895973206, Final Batch Loss: 0.3726848363876343\n",
      "Epoch 4421, Loss: 3.5118817389011383, Final Batch Loss: 1.9355137348175049\n",
      "Epoch 4422, Loss: 2.3385032415390015, Final Batch Loss: 0.7551522254943848\n",
      "Epoch 4423, Loss: 2.1873857975006104, Final Batch Loss: 0.6180411577224731\n",
      "Epoch 4424, Loss: 1.5379243695060723, Final Batch Loss: 0.0006605588714592159\n",
      "Epoch 4425, Loss: 4.7170538902282715, Final Batch Loss: 3.1214094161987305\n",
      "Epoch 4426, Loss: 1.8037335872650146, Final Batch Loss: 0.278518408536911\n",
      "Epoch 4427, Loss: 1.5519371731497813, Final Batch Loss: 0.00031406714697368443\n",
      "Epoch 4428, Loss: 2.813427358865738, Final Batch Loss: 1.1802234649658203\n",
      "Epoch 4429, Loss: 8.345639258623123, Final Batch Loss: 6.759061813354492\n",
      "Epoch 4430, Loss: 1.6268528220243752, Final Batch Loss: 0.002578150946646929\n",
      "Epoch 4431, Loss: 1.7942565195262432, Final Batch Loss: 0.04466618224978447\n",
      "Epoch 4432, Loss: 1.7931729865522357, Final Batch Loss: 0.00023898606013972312\n",
      "Epoch 4433, Loss: 1.7836954146623611, Final Batch Loss: 0.03959508240222931\n",
      "Epoch 4434, Loss: 1.634398888796568, Final Batch Loss: 0.016986828297376633\n",
      "Epoch 4435, Loss: 2.518107622861862, Final Batch Loss: 0.9818859696388245\n",
      "Epoch 4436, Loss: 2.297864258289337, Final Batch Loss: 0.6966992616653442\n",
      "Epoch 4437, Loss: 1.6628330573439598, Final Batch Loss: 0.006402693688869476\n",
      "Epoch 4438, Loss: 1.7007883720798418, Final Batch Loss: 0.0012499623699113727\n",
      "Epoch 4439, Loss: 1.762397212907672, Final Batch Loss: 0.019181592389941216\n",
      "Epoch 4440, Loss: 2.034169554710388, Final Batch Loss: 0.37949255108833313\n",
      "Epoch 4441, Loss: 1.7863889783620834, Final Batch Loss: 0.09605465829372406\n",
      "Epoch 4442, Loss: 1.6335803866370497, Final Batch Loss: 1.7881377516459906e-06\n",
      "Epoch 4443, Loss: 1.5911067053675652, Final Batch Loss: 0.03872939199209213\n",
      "Epoch 4444, Loss: 3.221161901950836, Final Batch Loss: 1.6514215469360352\n",
      "Epoch 4445, Loss: 1.5912482059793547, Final Batch Loss: 0.001624099095351994\n",
      "Epoch 4446, Loss: 1.6104207821190357, Final Batch Loss: 0.03946422412991524\n",
      "Epoch 4447, Loss: 1.7333378791809082, Final Batch Loss: 0.1399827003479004\n",
      "Epoch 4448, Loss: 2.1914660334587097, Final Batch Loss: 0.6844562292098999\n",
      "Epoch 4449, Loss: 2.10165598988533, Final Batch Loss: 0.6295239329338074\n",
      "Epoch 4450, Loss: 1.5426627355627716, Final Batch Loss: 0.006170626264065504\n",
      "Epoch 4451, Loss: 1.6508274376392365, Final Batch Loss: 0.13882336020469666\n",
      "Epoch 4452, Loss: 1.5542303752154112, Final Batch Loss: 0.007596297189593315\n",
      "Epoch 4453, Loss: 1.6326643973588943, Final Batch Loss: 0.14087046682834625\n",
      "Epoch 4454, Loss: 2.922097772359848, Final Batch Loss: 1.4626152515411377\n",
      "Epoch 4455, Loss: 3.3022088408470154, Final Batch Loss: 1.8129297494888306\n",
      "Epoch 4456, Loss: 1.7783275693655014, Final Batch Loss: 0.13493715226650238\n",
      "Epoch 4457, Loss: 1.5799224124639295, Final Batch Loss: 0.0005902693956159055\n",
      "Epoch 4458, Loss: 2.3349187672138214, Final Batch Loss: 0.7527815103530884\n",
      "Epoch 4459, Loss: 1.4775475940841716, Final Batch Loss: 0.00020823694649152458\n",
      "Epoch 4460, Loss: 2.181509405374527, Final Batch Loss: 0.6694780588150024\n",
      "Epoch 4461, Loss: 2.0503415763378143, Final Batch Loss: 0.5162889361381531\n",
      "Epoch 4462, Loss: 1.510336396517232, Final Batch Loss: 0.0035027835983783007\n",
      "Epoch 4463, Loss: 1.5406489393790253, Final Batch Loss: 0.0003389737685211003\n",
      "Epoch 4464, Loss: 2.9451282024383545, Final Batch Loss: 1.4391556978225708\n",
      "Epoch 4465, Loss: 1.6114040013490012, Final Batch Loss: 2.6940935640595853e-05\n",
      "Epoch 4466, Loss: 1.5424855765886605, Final Batch Loss: 0.005997162777930498\n",
      "Epoch 4467, Loss: 1.5087447051191702, Final Batch Loss: 0.0005665604257956147\n",
      "Epoch 4468, Loss: 1.589999278949108, Final Batch Loss: 0.0007839705212973058\n",
      "Epoch 4469, Loss: 1.7614427953958511, Final Batch Loss: 0.22269035875797272\n",
      "Epoch 4470, Loss: 1.5441357027739286, Final Batch Loss: 0.015747828409075737\n",
      "Epoch 4471, Loss: 1.4780498024483677, Final Batch Loss: 0.0004508670826908201\n",
      "Epoch 4472, Loss: 1.718239352107048, Final Batch Loss: 0.1781269758939743\n",
      "Epoch 4473, Loss: 1.4236913323102272, Final Batch Loss: 7.748573807475623e-06\n",
      "Epoch 4474, Loss: 1.52865394577384, Final Batch Loss: 0.016694601625204086\n",
      "Epoch 4475, Loss: 1.6586024314165115, Final Batch Loss: 0.21902905404567719\n",
      "Epoch 4476, Loss: 1.6100060073076747, Final Batch Loss: 0.00041631137719377875\n",
      "Epoch 4477, Loss: 1.4706169067394512, Final Batch Loss: 1.680836794548668e-05\n",
      "Epoch 4478, Loss: 1.5173219963908195, Final Batch Loss: 0.11272075027227402\n",
      "Epoch 4479, Loss: 1.9226717352867126, Final Batch Loss: 0.4925965368747711\n",
      "Epoch 4480, Loss: 1.5234259854478296, Final Batch Loss: 0.0002851079625543207\n",
      "Epoch 4481, Loss: 1.8634401261806488, Final Batch Loss: 0.3737965524196625\n",
      "Epoch 4482, Loss: 1.5269342064857483, Final Batch Loss: 0.04746010899543762\n",
      "Epoch 4483, Loss: 1.5686197057366371, Final Batch Loss: 0.06820390373468399\n",
      "Epoch 4484, Loss: 1.5038145133294165, Final Batch Loss: 0.003020013216882944\n",
      "Epoch 4485, Loss: 1.6492863148450851, Final Batch Loss: 0.18420125544071198\n",
      "Epoch 4486, Loss: 1.4482649881392717, Final Batch Loss: 0.004860134795308113\n",
      "Epoch 4487, Loss: 1.4862657486246462, Final Batch Loss: 1.5139465176616795e-05\n",
      "Epoch 4488, Loss: 1.4171162908896804, Final Batch Loss: 0.004523283801972866\n",
      "Epoch 4489, Loss: 2.041056990623474, Final Batch Loss: 0.5179464221000671\n",
      "Epoch 4490, Loss: 2.3900805711746216, Final Batch Loss: 0.9242870807647705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4491, Loss: 3.192118465900421, Final Batch Loss: 1.7135509252548218\n",
      "Epoch 4492, Loss: 2.8179002702236176, Final Batch Loss: 1.3376483917236328\n",
      "Epoch 4493, Loss: 3.2016141414642334, Final Batch Loss: 1.5594792366027832\n",
      "Epoch 4494, Loss: 6.616860628128052, Final Batch Loss: 4.885774612426758\n",
      "Epoch 4495, Loss: 1.6904127324232832, Final Batch Loss: 0.0005414212355390191\n",
      "Epoch 4496, Loss: 2.437986671924591, Final Batch Loss: 0.8003668189048767\n",
      "Epoch 4497, Loss: 5.527565598487854, Final Batch Loss: 3.798023223876953\n",
      "Epoch 4498, Loss: 1.6961387479677796, Final Batch Loss: 0.006947644986212254\n",
      "Epoch 4499, Loss: 1.6681605763733387, Final Batch Loss: 0.0008927173912525177\n",
      "Epoch 4500, Loss: 1.6807885766029358, Final Batch Loss: 0.07806900143623352\n",
      "Epoch 4501, Loss: 2.885831117630005, Final Batch Loss: 1.1926515102386475\n",
      "Epoch 4502, Loss: 1.8236435241997242, Final Batch Loss: 0.05068828538060188\n",
      "Epoch 4503, Loss: 2.0407804548740387, Final Batch Loss: 0.38013938069343567\n",
      "Epoch 4504, Loss: 1.839719370007515, Final Batch Loss: 0.06364007294178009\n",
      "Epoch 4505, Loss: 2.060847043991089, Final Batch Loss: 0.4577455520629883\n",
      "Epoch 4506, Loss: 2.1172945499420166, Final Batch Loss: 0.4615229368209839\n",
      "Epoch 4507, Loss: 1.6432307239156216, Final Batch Loss: 0.0006182666402310133\n",
      "Epoch 4508, Loss: 1.6277137473225594, Final Batch Loss: 0.11347971111536026\n",
      "Epoch 4509, Loss: 2.9488266110420227, Final Batch Loss: 1.4903650283813477\n",
      "Epoch 4510, Loss: 1.5717535577714443, Final Batch Loss: 0.04278760775923729\n",
      "Epoch 4511, Loss: 2.8527480959892273, Final Batch Loss: 1.3562026023864746\n",
      "Epoch 4512, Loss: 1.7312268763780594, Final Batch Loss: 0.17573119699954987\n",
      "Epoch 4513, Loss: 2.5119507908821106, Final Batch Loss: 0.929927408695221\n",
      "Epoch 4514, Loss: 1.8229804337024689, Final Batch Loss: 0.3431209325790405\n",
      "Epoch 4515, Loss: 2.1512116193771362, Final Batch Loss: 0.5778371691703796\n",
      "Epoch 4516, Loss: 1.653228221948666, Final Batch Loss: 5.245071224635467e-05\n",
      "Epoch 4517, Loss: 1.4882409804486088, Final Batch Loss: 0.00011503035057103261\n",
      "Epoch 4518, Loss: 3.4099884927272797, Final Batch Loss: 1.9525336027145386\n",
      "Epoch 4519, Loss: 1.721577763557434, Final Batch Loss: 0.14893829822540283\n",
      "Epoch 4520, Loss: 3.0067490339279175, Final Batch Loss: 1.4108020067214966\n",
      "Epoch 4521, Loss: 3.3340660333633423, Final Batch Loss: 1.8081530332565308\n",
      "Epoch 4522, Loss: 3.0356861352920532, Final Batch Loss: 1.4028842449188232\n",
      "Epoch 4523, Loss: 1.7240284532308578, Final Batch Loss: 0.016930803656578064\n",
      "Epoch 4524, Loss: 1.7608599294908345, Final Batch Loss: 0.0061373342759907246\n",
      "Epoch 4525, Loss: 1.937150999903679, Final Batch Loss: 0.14461903274059296\n",
      "Epoch 4526, Loss: 2.7943169474601746, Final Batch Loss: 1.0109578371047974\n",
      "Epoch 4527, Loss: 2.2408934831619263, Final Batch Loss: 0.5818667411804199\n",
      "Epoch 4528, Loss: 1.8827569037675858, Final Batch Loss: 0.21676059067249298\n",
      "Epoch 4529, Loss: 3.1209704279899597, Final Batch Loss: 1.445114016532898\n",
      "Epoch 4530, Loss: 1.9323135912418365, Final Batch Loss: 0.3221164047718048\n",
      "Epoch 4531, Loss: 1.682981296442449, Final Batch Loss: 0.004930599592626095\n",
      "Epoch 4532, Loss: 1.8452576100826263, Final Batch Loss: 0.09064170718193054\n",
      "Epoch 4533, Loss: 3.45511531829834, Final Batch Loss: 1.7394849061965942\n",
      "Epoch 4534, Loss: 1.7499588078353554, Final Batch Loss: 0.002185339340940118\n",
      "Epoch 4535, Loss: 1.6901392596773803, Final Batch Loss: 0.007384504657238722\n",
      "Epoch 4536, Loss: 2.9213773012161255, Final Batch Loss: 1.2120951414108276\n",
      "Epoch 4537, Loss: 1.5871044981759042, Final Batch Loss: 0.0013598490040749311\n",
      "Epoch 4538, Loss: 3.272358238697052, Final Batch Loss: 1.4340198040008545\n",
      "Epoch 4539, Loss: 1.6556340053211898, Final Batch Loss: 0.0005647733341902494\n",
      "Epoch 4540, Loss: 1.5828086729161441, Final Batch Loss: 0.006237089168280363\n",
      "Epoch 4541, Loss: 1.6962553476914763, Final Batch Loss: 0.00835093017667532\n",
      "Epoch 4542, Loss: 3.579711139202118, Final Batch Loss: 1.912402868270874\n",
      "Epoch 4543, Loss: 1.615906048566103, Final Batch Loss: 0.027237404137849808\n",
      "Epoch 4544, Loss: 2.4700967669487, Final Batch Loss: 0.8289191722869873\n",
      "Epoch 4545, Loss: 1.6642185192322358, Final Batch Loss: 0.0012650828575715423\n",
      "Epoch 4546, Loss: 1.6580759421922266, Final Batch Loss: 0.0032145516015589237\n",
      "Epoch 4547, Loss: 1.569071687757969, Final Batch Loss: 0.08207563310861588\n",
      "Epoch 4548, Loss: 4.875976890325546, Final Batch Loss: 3.264965295791626\n",
      "Epoch 4549, Loss: 1.9361750185489655, Final Batch Loss: 0.3203071653842926\n",
      "Epoch 4550, Loss: 2.3870189785957336, Final Batch Loss: 0.71738201379776\n",
      "Epoch 4551, Loss: 1.6656200215220451, Final Batch Loss: 0.11578574031591415\n",
      "Epoch 4552, Loss: 1.8372710570693016, Final Batch Loss: 0.04522087424993515\n",
      "Epoch 4553, Loss: 4.255516588687897, Final Batch Loss: 2.4963061809539795\n",
      "Epoch 4554, Loss: 2.5938775539398193, Final Batch Loss: 0.8935835957527161\n",
      "Epoch 4555, Loss: 1.6284146457910538, Final Batch Loss: 0.128861203789711\n",
      "Epoch 4556, Loss: 1.6112819894551649, Final Batch Loss: 5.566918844124302e-05\n",
      "Epoch 4557, Loss: 2.8121694326400757, Final Batch Loss: 1.119096279144287\n",
      "Epoch 4558, Loss: 1.7140750028192997, Final Batch Loss: 0.04917696490883827\n",
      "Epoch 4559, Loss: 1.745479827746749, Final Batch Loss: 0.028181826695799828\n",
      "Epoch 4560, Loss: 1.9394571781158447, Final Batch Loss: 0.2959149479866028\n",
      "Epoch 4561, Loss: 1.5299149784259498, Final Batch Loss: 0.004582736175507307\n",
      "Epoch 4562, Loss: 1.5246332444367, Final Batch Loss: 0.0005164004978723824\n",
      "Epoch 4563, Loss: 2.1658604443073273, Final Batch Loss: 0.6206198334693909\n",
      "Epoch 4564, Loss: 2.1597298979759216, Final Batch Loss: 0.5132363438606262\n",
      "Epoch 4565, Loss: 1.5911110825836658, Final Batch Loss: 0.05941541865468025\n",
      "Epoch 4566, Loss: 1.6227174699306488, Final Batch Loss: 0.07761287689208984\n",
      "Epoch 4567, Loss: 1.750729188323021, Final Batch Loss: 0.21438480913639069\n",
      "Epoch 4568, Loss: 2.6608373820781708, Final Batch Loss: 1.0922703742980957\n",
      "Epoch 4569, Loss: 2.7482370734214783, Final Batch Loss: 1.2402524948120117\n",
      "Epoch 4570, Loss: 1.5118139602418523, Final Batch Loss: 0.00036816971260122955\n",
      "Epoch 4571, Loss: 1.4110134755610488, Final Batch Loss: 0.0005052005290053785\n",
      "Epoch 4572, Loss: 1.5976702198386192, Final Batch Loss: 0.0674247071146965\n",
      "Epoch 4573, Loss: 1.4273983966559172, Final Batch Loss: 0.0014349650591611862\n",
      "Epoch 4574, Loss: 1.581585805863142, Final Batch Loss: 0.03265134617686272\n",
      "Epoch 4575, Loss: 2.4439153373241425, Final Batch Loss: 0.9163593053817749\n",
      "Epoch 4576, Loss: 1.5426470679230988, Final Batch Loss: 0.004595552105456591\n",
      "Epoch 4577, Loss: 1.587226651608944, Final Batch Loss: 0.018372945487499237\n",
      "Epoch 4578, Loss: 2.6949865221977234, Final Batch Loss: 1.1929166316986084\n",
      "Epoch 4579, Loss: 1.5949361789389513, Final Batch Loss: 0.0003812778159044683\n",
      "Epoch 4580, Loss: 1.6077506393194199, Final Batch Loss: 0.09520648419857025\n",
      "Epoch 4581, Loss: 1.5461638160049915, Final Batch Loss: 0.01195492222905159\n",
      "Epoch 4582, Loss: 3.3210352063179016, Final Batch Loss: 1.7091896533966064\n",
      "Epoch 4583, Loss: 2.6052035987377167, Final Batch Loss: 1.0025014877319336\n",
      "Epoch 4584, Loss: 2.0236500203609467, Final Batch Loss: 0.4937792122364044\n",
      "Epoch 4585, Loss: 2.4451164305210114, Final Batch Loss: 0.9355455636978149\n",
      "Epoch 4586, Loss: 3.605963259935379, Final Batch Loss: 1.9886751174926758\n",
      "Epoch 4587, Loss: 1.4695969804051856, Final Batch Loss: 5.6503606174374e-05\n",
      "Epoch 4588, Loss: 2.0528364777565002, Final Batch Loss: 0.5475319027900696\n",
      "Epoch 4589, Loss: 1.6143012940055996, Final Batch Loss: 1.2874520507466514e-05\n",
      "Epoch 4590, Loss: 1.5855845016485546, Final Batch Loss: 0.00019822540343739092\n",
      "Epoch 4591, Loss: 1.4777417052537203, Final Batch Loss: 0.01820533163845539\n",
      "Epoch 4592, Loss: 1.4932358311489224, Final Batch Loss: 0.007530634291470051\n",
      "Epoch 4593, Loss: 1.9585569202899933, Final Batch Loss: 0.4924291968345642\n",
      "Epoch 4594, Loss: 1.538624644264928, Final Batch Loss: 5.364403477869928e-06\n",
      "Epoch 4595, Loss: 1.5524121224880219, Final Batch Loss: 0.08347469568252563\n",
      "Epoch 4596, Loss: 1.520771011710167, Final Batch Loss: 0.0864049643278122\n",
      "Epoch 4597, Loss: 2.6079670190811157, Final Batch Loss: 1.1481738090515137\n",
      "Epoch 4598, Loss: 1.9963496029376984, Final Batch Loss: 0.5240949392318726\n",
      "Epoch 4599, Loss: 1.5473236311227083, Final Batch Loss: 0.019019143655896187\n",
      "Epoch 4600, Loss: 1.4225159846246243, Final Batch Loss: 0.02447938546538353\n",
      "Epoch 4601, Loss: 2.060695707798004, Final Batch Loss: 0.566892147064209\n",
      "Epoch 4602, Loss: 3.689979374408722, Final Batch Loss: 2.1874935626983643\n",
      "Epoch 4603, Loss: 1.4230550696956925, Final Batch Loss: 0.0006191005813889205\n",
      "Epoch 4604, Loss: 1.5220003575086594, Final Batch Loss: 0.009763464331626892\n",
      "Epoch 4605, Loss: 2.7203134894371033, Final Batch Loss: 1.295089840888977\n",
      "Epoch 4606, Loss: 1.4875643700361252, Final Batch Loss: 0.03208686411380768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4607, Loss: 1.577736258506775, Final Batch Loss: 0.04944407939910889\n",
      "Epoch 4608, Loss: 2.5305623412132263, Final Batch Loss: 1.0359145402908325\n",
      "Epoch 4609, Loss: 1.4783464600332081, Final Batch Loss: 0.002816759515553713\n",
      "Epoch 4610, Loss: 1.910438358783722, Final Batch Loss: 0.3666794002056122\n",
      "Epoch 4611, Loss: 1.4952558623626828, Final Batch Loss: 0.010122393257915974\n",
      "Epoch 4612, Loss: 1.5762798264622688, Final Batch Loss: 0.12119264155626297\n",
      "Epoch 4613, Loss: 1.5477404571138322, Final Batch Loss: 0.0038890219293534756\n",
      "Epoch 4614, Loss: 1.8319577872753143, Final Batch Loss: 0.26949170231819153\n",
      "Epoch 4615, Loss: 1.5745856016874313, Final Batch Loss: 0.1103726178407669\n",
      "Epoch 4616, Loss: 1.4710652868961915, Final Batch Loss: 0.0008407871937379241\n",
      "Epoch 4617, Loss: 1.5371640287339687, Final Batch Loss: 0.023129668086767197\n",
      "Epoch 4618, Loss: 2.1025468707084656, Final Batch Loss: 0.6343157887458801\n",
      "Epoch 4619, Loss: 1.8827941119670868, Final Batch Loss: 0.38258296251296997\n",
      "Epoch 4620, Loss: 1.5576636943151243, Final Batch Loss: 0.0008274468709714711\n",
      "Epoch 4621, Loss: 1.8367704153060913, Final Batch Loss: 0.2210090458393097\n",
      "Epoch 4622, Loss: 1.678331857547164, Final Batch Loss: 0.022552674636244774\n",
      "Epoch 4623, Loss: 1.9065495729446411, Final Batch Loss: 0.3744533061981201\n",
      "Epoch 4624, Loss: 2.2914055585861206, Final Batch Loss: 0.6723682284355164\n",
      "Epoch 4625, Loss: 2.4193461537361145, Final Batch Loss: 0.8167273998260498\n",
      "Epoch 4626, Loss: 1.5956327691674232, Final Batch Loss: 0.07118374854326248\n",
      "Epoch 4627, Loss: 1.6943101593642496, Final Batch Loss: 0.0005442806868813932\n",
      "Epoch 4628, Loss: 1.920348882675171, Final Batch Loss: 0.1396215558052063\n",
      "Epoch 4629, Loss: 2.446996510028839, Final Batch Loss: 0.7125769257545471\n",
      "Epoch 4630, Loss: 3.0316407680511475, Final Batch Loss: 1.242510199546814\n",
      "Epoch 4631, Loss: 1.7214582674205303, Final Batch Loss: 0.041607629507780075\n",
      "Epoch 4632, Loss: 1.9508378356695175, Final Batch Loss: 0.19390536844730377\n",
      "Epoch 4633, Loss: 2.683164656162262, Final Batch Loss: 0.8316683769226074\n",
      "Epoch 4634, Loss: 1.8968555792234838, Final Batch Loss: 0.00525687774643302\n",
      "Epoch 4635, Loss: 2.0813348591327667, Final Batch Loss: 0.3561148941516876\n",
      "Epoch 4636, Loss: 2.6721048951148987, Final Batch Loss: 1.0501474142074585\n",
      "Epoch 4637, Loss: 3.73742812871933, Final Batch Loss: 2.0523979663848877\n",
      "Epoch 4638, Loss: 1.7164854150614701, Final Batch Loss: 0.0006394725642167032\n",
      "Epoch 4639, Loss: 1.8381229788064957, Final Batch Loss: 0.23587073385715485\n",
      "Epoch 4640, Loss: 1.5132455226830643, Final Batch Loss: 2.4199192921514623e-05\n",
      "Epoch 4641, Loss: 1.5405230524484068, Final Batch Loss: 0.0021414461079984903\n",
      "Epoch 4642, Loss: 1.5374240137171, Final Batch Loss: 0.0023084438871592283\n",
      "Epoch 4643, Loss: 1.7376355677843094, Final Batch Loss: 0.17301206290721893\n",
      "Epoch 4644, Loss: 1.6079877726733685, Final Batch Loss: 0.03672356531023979\n",
      "Epoch 4645, Loss: 2.887387692928314, Final Batch Loss: 1.4348136186599731\n",
      "Epoch 4646, Loss: 3.7868502736091614, Final Batch Loss: 2.278900623321533\n",
      "Epoch 4647, Loss: 1.5843072828920413, Final Batch Loss: 2.3245540432981215e-05\n",
      "Epoch 4648, Loss: 1.6484555210918188, Final Batch Loss: 0.007536431774497032\n",
      "Epoch 4649, Loss: 3.4752419888973236, Final Batch Loss: 1.896716833114624\n",
      "Epoch 4650, Loss: 1.5988865345716476, Final Batch Loss: 0.05953167378902435\n",
      "Epoch 4651, Loss: 1.540805272758007, Final Batch Loss: 0.014034859836101532\n",
      "Epoch 4652, Loss: 2.9931833744049072, Final Batch Loss: 1.4444313049316406\n",
      "Epoch 4653, Loss: 1.5900648720562458, Final Batch Loss: 0.05811252072453499\n",
      "Epoch 4654, Loss: 1.5588116869330406, Final Batch Loss: 0.016858719289302826\n",
      "Epoch 4655, Loss: 1.9287485182285309, Final Batch Loss: 0.31950464844703674\n",
      "Epoch 4656, Loss: 1.586180923601205, Final Batch Loss: 5.94836674281396e-05\n",
      "Epoch 4657, Loss: 2.6748344600200653, Final Batch Loss: 1.142804741859436\n",
      "Epoch 4658, Loss: 1.9718515574932098, Final Batch Loss: 0.41015711426734924\n",
      "Epoch 4659, Loss: 1.8435100317001343, Final Batch Loss: 0.279766321182251\n",
      "Epoch 4660, Loss: 1.6128444643545663, Final Batch Loss: 7.497983460780233e-05\n",
      "Epoch 4661, Loss: 1.9442605674266815, Final Batch Loss: 0.3306025266647339\n",
      "Epoch 4662, Loss: 1.4927753508090973, Final Batch Loss: 0.03207913041114807\n",
      "Epoch 4663, Loss: 3.1487463414669037, Final Batch Loss: 1.5618942975997925\n",
      "Epoch 4664, Loss: 2.1983896791934967, Final Batch Loss: 0.7118398547172546\n",
      "Epoch 4665, Loss: 2.892002582550049, Final Batch Loss: 1.393050193786621\n",
      "Epoch 4666, Loss: 1.5125016352394596, Final Batch Loss: 0.0018055817345157266\n",
      "Epoch 4667, Loss: 2.641978621482849, Final Batch Loss: 1.192905306816101\n",
      "Epoch 4668, Loss: 1.5024813995696604, Final Batch Loss: 0.0018434212543070316\n",
      "Epoch 4669, Loss: 1.5423583686260827, Final Batch Loss: 4.291525328881107e-06\n",
      "Epoch 4670, Loss: 1.5430790381506085, Final Batch Loss: 0.012100149877369404\n",
      "Epoch 4671, Loss: 1.5190548663958907, Final Batch Loss: 0.005645758472383022\n",
      "Epoch 4672, Loss: 1.4900014099166583, Final Batch Loss: 2.5629668016335927e-05\n",
      "Epoch 4673, Loss: 1.568718060851097, Final Batch Loss: 0.1616574078798294\n",
      "Epoch 4674, Loss: 1.6942509412765503, Final Batch Loss: 0.2820325493812561\n",
      "Epoch 4675, Loss: 5.361876428127289, Final Batch Loss: 3.8928208351135254\n",
      "Epoch 4676, Loss: 2.9873218536376953, Final Batch Loss: 1.5102742910385132\n",
      "Epoch 4677, Loss: 2.5389987230300903, Final Batch Loss: 0.9406260251998901\n",
      "Epoch 4678, Loss: 2.591632664203644, Final Batch Loss: 0.9801002740859985\n",
      "Epoch 4679, Loss: 2.744591534137726, Final Batch Loss: 1.2053996324539185\n",
      "Epoch 4680, Loss: 3.3963303565979004, Final Batch Loss: 1.9360723495483398\n",
      "Epoch 4681, Loss: 2.4550730288028717, Final Batch Loss: 0.9569727778434753\n",
      "Epoch 4682, Loss: 1.6253441032022238, Final Batch Loss: 0.02598131261765957\n",
      "Epoch 4683, Loss: 1.6866647228598595, Final Batch Loss: 0.037872932851314545\n",
      "Epoch 4684, Loss: 2.876875400543213, Final Batch Loss: 1.2360351085662842\n",
      "Epoch 4685, Loss: 1.6178476810268876, Final Batch Loss: 6.079655122448457e-06\n",
      "Epoch 4686, Loss: 3.2007323503494263, Final Batch Loss: 1.6051223278045654\n",
      "Epoch 4687, Loss: 1.8646883442997932, Final Batch Loss: 0.05528349429368973\n",
      "Epoch 4688, Loss: 1.9142810921184719, Final Batch Loss: 0.005860052537173033\n",
      "Epoch 4689, Loss: 2.729431390762329, Final Batch Loss: 0.8426205515861511\n",
      "Epoch 4690, Loss: 2.9819930493831635, Final Batch Loss: 1.2935717105865479\n",
      "Epoch 4691, Loss: 2.082846909761429, Final Batch Loss: 0.2935865819454193\n",
      "Epoch 4692, Loss: 1.701026824535802, Final Batch Loss: 0.0008486483711749315\n",
      "Epoch 4693, Loss: 3.4485594034194946, Final Batch Loss: 1.6780805587768555\n",
      "Epoch 4694, Loss: 1.589096236974001, Final Batch Loss: 0.04485609009861946\n",
      "Epoch 4695, Loss: 1.6983822137117386, Final Batch Loss: 0.15889643132686615\n",
      "Epoch 4696, Loss: 5.346806585788727, Final Batch Loss: 3.7873477935791016\n",
      "Epoch 4697, Loss: 1.587354049726855, Final Batch Loss: 0.0004552758182398975\n",
      "Epoch 4698, Loss: 2.463460326194763, Final Batch Loss: 0.8389861583709717\n",
      "Epoch 4699, Loss: 1.7233846781764441, Final Batch Loss: 1.9311717551317997e-05\n",
      "Epoch 4700, Loss: 1.8999987542629242, Final Batch Loss: 0.3773922622203827\n",
      "Epoch 4701, Loss: 1.8586354106664658, Final Batch Loss: 0.23799510300159454\n",
      "Epoch 4702, Loss: 5.332689791917801, Final Batch Loss: 3.766939163208008\n",
      "Epoch 4703, Loss: 2.7478840053081512, Final Batch Loss: 1.1159433126449585\n",
      "Epoch 4704, Loss: 1.818053126335144, Final Batch Loss: 0.2917178273200989\n",
      "Epoch 4705, Loss: 2.116205185651779, Final Batch Loss: 0.44540390372276306\n",
      "Epoch 4706, Loss: 1.7970496341586113, Final Batch Loss: 0.019778065383434296\n",
      "Epoch 4707, Loss: 1.7744509428739548, Final Batch Loss: 0.06901542842388153\n",
      "Epoch 4708, Loss: 2.3539841771125793, Final Batch Loss: 0.6787182092666626\n",
      "Epoch 4709, Loss: 1.6543960198760033, Final Batch Loss: 0.05973821133375168\n",
      "Epoch 4710, Loss: 1.5608484670519829, Final Batch Loss: 0.08138223737478256\n",
      "Epoch 4711, Loss: 3.7537949979305267, Final Batch Loss: 2.2603607177734375\n",
      "Epoch 4712, Loss: 1.5578227788209915, Final Batch Loss: 0.03371734917163849\n",
      "Epoch 4713, Loss: 1.4680224584881216, Final Batch Loss: 0.0007812308613210917\n",
      "Epoch 4714, Loss: 2.089635491371155, Final Batch Loss: 0.5341421961784363\n",
      "Epoch 4715, Loss: 2.1970004439353943, Final Batch Loss: 0.7131616473197937\n",
      "Epoch 4716, Loss: 3.855742394924164, Final Batch Loss: 2.3430848121643066\n",
      "Epoch 4717, Loss: 1.9730441272258759, Final Batch Loss: 0.5145343542098999\n",
      "Epoch 4718, Loss: 1.480895133689046, Final Batch Loss: 0.01658591814339161\n",
      "Epoch 4719, Loss: 1.5491265431046486, Final Batch Loss: 0.10502759367227554\n",
      "Epoch 4720, Loss: 1.486548214990762, Final Batch Loss: 1.7881233361549675e-05\n",
      "Epoch 4721, Loss: 1.5294978021956922, Final Batch Loss: 1.1086402082582936e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4722, Loss: 2.291076123714447, Final Batch Loss: 0.8346859216690063\n",
      "Epoch 4723, Loss: 1.6142405718564987, Final Batch Loss: 0.1275591105222702\n",
      "Epoch 4724, Loss: 2.0922585129737854, Final Batch Loss: 0.6077709197998047\n",
      "Epoch 4725, Loss: 1.520387778058648, Final Batch Loss: 0.010613778606057167\n",
      "Epoch 4726, Loss: 1.6883578896522522, Final Batch Loss: 0.15205705165863037\n",
      "Epoch 4727, Loss: 3.375294119119644, Final Batch Loss: 1.875974416732788\n",
      "Epoch 4728, Loss: 1.460058628454135, Final Batch Loss: 4.60137271147687e-05\n",
      "Epoch 4729, Loss: 1.5572859607636929, Final Batch Loss: 0.014582600444555283\n",
      "Epoch 4730, Loss: 1.443637311457337, Final Batch Loss: 1.5497195136049413e-06\n",
      "Epoch 4731, Loss: 1.467833032249473, Final Batch Loss: 0.0018799504032358527\n",
      "Epoch 4732, Loss: 1.777338445186615, Final Batch Loss: 0.26549863815307617\n",
      "Epoch 4733, Loss: 1.4532524733513128, Final Batch Loss: 0.0004698126285802573\n",
      "Epoch 4734, Loss: 1.6852223873138428, Final Batch Loss: 0.29233962297439575\n",
      "Epoch 4735, Loss: 1.6978548467159271, Final Batch Loss: 0.2347640097141266\n",
      "Epoch 4736, Loss: 4.4616213738918304, Final Batch Loss: 3.0722103118896484\n",
      "Epoch 4737, Loss: 1.4270365418633446, Final Batch Loss: 0.0009577454766258597\n",
      "Epoch 4738, Loss: 1.5666659697890282, Final Batch Loss: 0.06407835334539413\n",
      "Epoch 4739, Loss: 2.928429275751114, Final Batch Loss: 1.455539345741272\n",
      "Epoch 4740, Loss: 1.5643057078123093, Final Batch Loss: 0.07932426035404205\n",
      "Epoch 4741, Loss: 3.239386796951294, Final Batch Loss: 1.6747028827667236\n",
      "Epoch 4742, Loss: 1.8096821010112762, Final Batch Loss: 0.35175615549087524\n",
      "Epoch 4743, Loss: 1.5253178980201483, Final Batch Loss: 0.02803288958966732\n",
      "Epoch 4744, Loss: 1.8740267157554626, Final Batch Loss: 0.2982026934623718\n",
      "Epoch 4745, Loss: 1.5403614866081625, Final Batch Loss: 0.002292506629601121\n",
      "Epoch 4746, Loss: 1.7267002016305923, Final Batch Loss: 0.21285511553287506\n",
      "Epoch 4747, Loss: 2.3365724682807922, Final Batch Loss: 0.8651471138000488\n",
      "Epoch 4748, Loss: 1.431682437318159, Final Batch Loss: 2.610649426060263e-05\n",
      "Epoch 4749, Loss: 1.491825569421053, Final Batch Loss: 0.013414014130830765\n",
      "Epoch 4750, Loss: 1.4861164623871446, Final Batch Loss: 0.011231177486479282\n",
      "Epoch 4751, Loss: 1.869743824005127, Final Batch Loss: 0.42088446021080017\n",
      "Epoch 4752, Loss: 1.5758597329258919, Final Batch Loss: 0.08147267252206802\n",
      "Epoch 4753, Loss: 1.4243790511973202, Final Batch Loss: 0.005914211738854647\n",
      "Epoch 4754, Loss: 2.4764524400234222, Final Batch Loss: 1.1199826002120972\n",
      "Epoch 4755, Loss: 3.1719966530799866, Final Batch Loss: 1.6804841756820679\n",
      "Epoch 4756, Loss: 2.372583746910095, Final Batch Loss: 0.9732547402381897\n",
      "Epoch 4757, Loss: 1.4780196249485016, Final Batch Loss: 0.013714373111724854\n",
      "Epoch 4758, Loss: 1.521912513053394, Final Batch Loss: 6.496695277746767e-05\n",
      "Epoch 4759, Loss: 1.5194733738898947, Final Batch Loss: 2.3841855067985307e-07\n",
      "Epoch 4760, Loss: 1.7079495415091515, Final Batch Loss: 0.12212320417165756\n",
      "Epoch 4761, Loss: 1.495971102733165, Final Batch Loss: 0.005747933406382799\n",
      "Epoch 4762, Loss: 3.085960805416107, Final Batch Loss: 1.6344505548477173\n",
      "Epoch 4763, Loss: 1.539348464779323, Final Batch Loss: 0.0003947432560380548\n",
      "Epoch 4764, Loss: 1.6834494918584824, Final Batch Loss: 0.07725365459918976\n",
      "Epoch 4765, Loss: 1.7018803977407515, Final Batch Loss: 0.0010333680547773838\n",
      "Epoch 4766, Loss: 2.4303871989250183, Final Batch Loss: 0.7151099443435669\n",
      "Epoch 4767, Loss: 1.7811547368764877, Final Batch Loss: 0.13014309108257294\n",
      "Epoch 4768, Loss: 3.6443411707878113, Final Batch Loss: 2.0384864807128906\n",
      "Epoch 4769, Loss: 1.5063868230208755, Final Batch Loss: 0.013118154369294643\n",
      "Epoch 4770, Loss: 3.178672641515732, Final Batch Loss: 1.6991066932678223\n",
      "Epoch 4771, Loss: 3.0206001698970795, Final Batch Loss: 1.605136513710022\n",
      "Epoch 4772, Loss: 2.9954659938812256, Final Batch Loss: 1.5543711185455322\n",
      "Epoch 4773, Loss: 1.4127657935023308, Final Batch Loss: 0.03499104827642441\n",
      "Epoch 4774, Loss: 3.7952003180980682, Final Batch Loss: 2.3840818405151367\n",
      "Epoch 4775, Loss: 1.4276667337398976, Final Batch Loss: 0.002150724409148097\n",
      "Epoch 4776, Loss: 1.9563533961772919, Final Batch Loss: 0.47397199273109436\n",
      "Epoch 4777, Loss: 2.661433666944504, Final Batch Loss: 1.1514568328857422\n",
      "Epoch 4778, Loss: 1.5543704060837626, Final Batch Loss: 0.0141621557995677\n",
      "Epoch 4779, Loss: 1.8419308364391327, Final Batch Loss: 0.29698556661605835\n",
      "Epoch 4780, Loss: 3.0642005801200867, Final Batch Loss: 1.5876331329345703\n",
      "Epoch 4781, Loss: 1.552860215306282, Final Batch Loss: 0.09307198226451874\n",
      "Epoch 4782, Loss: 3.514046370983124, Final Batch Loss: 1.9634556770324707\n",
      "Epoch 4783, Loss: 2.581467479467392, Final Batch Loss: 1.2310001850128174\n",
      "Epoch 4784, Loss: 1.449420653283596, Final Batch Loss: 0.016434304416179657\n",
      "Epoch 4785, Loss: 1.7886295169591904, Final Batch Loss: 0.15532033145427704\n",
      "Epoch 4786, Loss: 2.266174554824829, Final Batch Loss: 0.3231988549232483\n",
      "Epoch 4787, Loss: 7.462161004543304, Final Batch Loss: 5.581855297088623\n",
      "Epoch 4788, Loss: 3.397670090198517, Final Batch Loss: 1.7694658041000366\n",
      "Epoch 4789, Loss: 1.5728224739432335, Final Batch Loss: 0.03198503702878952\n",
      "Epoch 4790, Loss: 4.139537990093231, Final Batch Loss: 2.4989023208618164\n",
      "Epoch 4791, Loss: 2.392417699098587, Final Batch Loss: 0.8418022394180298\n",
      "Epoch 4792, Loss: 1.683058438822627, Final Batch Loss: 0.01417296938598156\n",
      "Epoch 4793, Loss: 1.7250252366065908, Final Batch Loss: 1.1920928244535389e-07\n",
      "Epoch 4794, Loss: 1.6848922595381737, Final Batch Loss: 0.06173800677061081\n",
      "Epoch 4795, Loss: 1.755252979695797, Final Batch Loss: 0.11676176637411118\n",
      "Epoch 4796, Loss: 1.5159969486994669, Final Batch Loss: 0.0004549183649942279\n",
      "Epoch 4797, Loss: 2.151570737361908, Final Batch Loss: 0.5829958319664001\n",
      "Epoch 4798, Loss: 1.63468486815691, Final Batch Loss: 0.0617167130112648\n",
      "Epoch 4799, Loss: 1.5065814041299745, Final Batch Loss: 0.001901843468658626\n",
      "Epoch 4800, Loss: 1.5096390247927047, Final Batch Loss: 0.0008800924406386912\n",
      "Epoch 4801, Loss: 2.053553730249405, Final Batch Loss: 0.5237377285957336\n",
      "Epoch 4802, Loss: 1.5417588067357428, Final Batch Loss: 0.000770391256082803\n",
      "Epoch 4803, Loss: 1.5087782749906182, Final Batch Loss: 0.012842566706240177\n",
      "Epoch 4804, Loss: 2.514465421438217, Final Batch Loss: 1.008224368095398\n",
      "Epoch 4805, Loss: 1.5035453606396914, Final Batch Loss: 0.005407350137829781\n",
      "Epoch 4806, Loss: 1.8621211349964142, Final Batch Loss: 0.4098231792449951\n",
      "Epoch 4807, Loss: 1.561685036867857, Final Batch Loss: 0.03537992760539055\n",
      "Epoch 4808, Loss: 1.8043768405914307, Final Batch Loss: 0.3455984890460968\n",
      "Epoch 4809, Loss: 2.6611958146095276, Final Batch Loss: 1.1920878887176514\n",
      "Epoch 4810, Loss: 1.4350031595677137, Final Batch Loss: 0.009069083258509636\n",
      "Epoch 4811, Loss: 1.4269720560405403, Final Batch Loss: 0.0012488907668739557\n",
      "Epoch 4812, Loss: 2.8904882073402405, Final Batch Loss: 1.448801875114441\n",
      "Epoch 4813, Loss: 2.714352995157242, Final Batch Loss: 1.3537942171096802\n",
      "Epoch 4814, Loss: 1.3756067951908335, Final Batch Loss: 0.0016613503685221076\n",
      "Epoch 4815, Loss: 4.39339292049408, Final Batch Loss: 2.916001081466675\n",
      "Epoch 4816, Loss: 2.5709247291088104, Final Batch Loss: 1.1444557905197144\n",
      "Epoch 4817, Loss: 4.441013902425766, Final Batch Loss: 2.857212781906128\n",
      "Epoch 4818, Loss: 3.3163439631462097, Final Batch Loss: 1.724648118019104\n",
      "Epoch 4819, Loss: 2.118901088833809, Final Batch Loss: 0.24445612728595734\n",
      "Epoch 4820, Loss: 2.1312450766531583, Final Batch Loss: 2.50339189733495e-06\n",
      "Epoch 4821, Loss: 3.909309208393097, Final Batch Loss: 1.7738252878189087\n",
      "Epoch 4822, Loss: 2.9058291912078857, Final Batch Loss: 0.7881972789764404\n",
      "Epoch 4823, Loss: 2.9612724781036377, Final Batch Loss: 1.0588788986206055\n",
      "Epoch 4824, Loss: 2.462512493133545, Final Batch Loss: 0.6626148223876953\n",
      "Epoch 4825, Loss: 2.120799720287323, Final Batch Loss: 0.45446592569351196\n",
      "Epoch 4826, Loss: 1.8263230323791504, Final Batch Loss: 0.26058992743492126\n",
      "Epoch 4827, Loss: 2.3902136087417603, Final Batch Loss: 0.9338115453720093\n",
      "Epoch 4828, Loss: 1.5021805900032632, Final Batch Loss: 0.0005202132160775363\n",
      "Epoch 4829, Loss: 1.644527718424797, Final Batch Loss: 0.19918636977672577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4830, Loss: 3.4072801172733307, Final Batch Loss: 1.8561813831329346\n",
      "Epoch 4831, Loss: 3.3889695405960083, Final Batch Loss: 1.7825924158096313\n",
      "Epoch 4832, Loss: 3.371537208557129, Final Batch Loss: 1.8015174865722656\n",
      "Epoch 4833, Loss: 3.0101715326309204, Final Batch Loss: 1.5109624862670898\n",
      "Epoch 4834, Loss: 1.6869443580508232, Final Batch Loss: 0.0653320774435997\n",
      "Epoch 4835, Loss: 1.620162027888, Final Batch Loss: 0.011286932043731213\n",
      "Epoch 4836, Loss: 2.5057454705238342, Final Batch Loss: 0.9250546097755432\n",
      "Epoch 4837, Loss: 1.5484149150215671, Final Batch Loss: 3.9457496313843876e-05\n",
      "Epoch 4838, Loss: 1.719757780432701, Final Batch Loss: 0.1959162801504135\n",
      "Epoch 4839, Loss: 1.5932591408491135, Final Batch Loss: 0.0667712539434433\n",
      "Epoch 4840, Loss: 3.62074613571167, Final Batch Loss: 2.1628530025482178\n",
      "Epoch 4841, Loss: 1.509004186897073, Final Batch Loss: 0.0008322112844325602\n",
      "Epoch 4842, Loss: 2.0483208894729614, Final Batch Loss: 0.5309142470359802\n",
      "Epoch 4843, Loss: 1.6831814497709274, Final Batch Loss: 0.19581161439418793\n",
      "Epoch 4844, Loss: 2.6843578219413757, Final Batch Loss: 1.2243932485580444\n",
      "Epoch 4845, Loss: 1.5128293130546808, Final Batch Loss: 0.006818125024437904\n",
      "Epoch 4846, Loss: 3.317142903804779, Final Batch Loss: 1.7947839498519897\n",
      "Epoch 4847, Loss: 1.874444842338562, Final Batch Loss: 0.4535064697265625\n",
      "Epoch 4848, Loss: 2.578438878059387, Final Batch Loss: 1.164858341217041\n",
      "Epoch 4849, Loss: 3.002716064453125, Final Batch Loss: 1.554640293121338\n",
      "Epoch 4850, Loss: 2.054588481783867, Final Batch Loss: 0.17968429625034332\n",
      "Epoch 4851, Loss: 2.108720153570175, Final Batch Loss: 0.13688847422599792\n",
      "Epoch 4852, Loss: 4.095103204250336, Final Batch Loss: 2.055609703063965\n",
      "Epoch 4853, Loss: 1.8590245367959142, Final Batch Loss: 0.008534860797226429\n",
      "Epoch 4854, Loss: 2.7600137591362, Final Batch Loss: 0.9959725141525269\n",
      "Epoch 4855, Loss: 3.195997714996338, Final Batch Loss: 1.4181714057922363\n",
      "Epoch 4856, Loss: 3.008956700563431, Final Batch Loss: 1.4941524267196655\n",
      "Epoch 4857, Loss: 2.3008508682250977, Final Batch Loss: 0.7454845905303955\n",
      "Epoch 4858, Loss: 1.8295105993747711, Final Batch Loss: 0.2665052115917206\n",
      "Epoch 4859, Loss: 1.7574499249458313, Final Batch Loss: 0.25916725397109985\n",
      "Epoch 4860, Loss: 1.573627270758152, Final Batch Loss: 0.020241834223270416\n",
      "Epoch 4861, Loss: 1.4631635546679718, Final Batch Loss: 9.536738616588991e-07\n",
      "Epoch 4862, Loss: 2.513910859823227, Final Batch Loss: 0.9996422529220581\n",
      "Epoch 4863, Loss: 1.4864883120171726, Final Batch Loss: 0.004288643132895231\n",
      "Epoch 4864, Loss: 1.5557157807052135, Final Batch Loss: 0.048757534474134445\n",
      "Epoch 4865, Loss: 1.5766415745019913, Final Batch Loss: 0.07571710646152496\n",
      "Epoch 4866, Loss: 1.5744052026420832, Final Batch Loss: 0.02164829708635807\n",
      "Epoch 4867, Loss: 2.1692127883434296, Final Batch Loss: 0.6303127408027649\n",
      "Epoch 4868, Loss: 3.151760995388031, Final Batch Loss: 1.6841509342193604\n",
      "Epoch 4869, Loss: 1.5287273556459695, Final Batch Loss: 0.0027272433508187532\n",
      "Epoch 4870, Loss: 1.593810349702835, Final Batch Loss: 0.008691221475601196\n",
      "Epoch 4871, Loss: 1.9483453631401062, Final Batch Loss: 0.25580257177352905\n",
      "Epoch 4872, Loss: 1.6881109774112701, Final Batch Loss: 0.024715036153793335\n",
      "Epoch 4873, Loss: 3.2990999817848206, Final Batch Loss: 1.6846394538879395\n",
      "Epoch 4874, Loss: 2.7269634008407593, Final Batch Loss: 1.2032477855682373\n",
      "Epoch 4875, Loss: 2.6202649772167206, Final Batch Loss: 1.0306977033615112\n",
      "Epoch 4876, Loss: 1.7115291031077504, Final Batch Loss: 0.009437826462090015\n",
      "Epoch 4877, Loss: 1.7807749509811401, Final Batch Loss: 0.25817498564720154\n",
      "Epoch 4878, Loss: 1.623485624767909, Final Batch Loss: 6.6756979322235566e-06\n",
      "Epoch 4879, Loss: 1.7376149892807007, Final Batch Loss: 0.1745494306087494\n",
      "Epoch 4880, Loss: 4.182621479034424, Final Batch Loss: 2.646059513092041\n",
      "Epoch 4881, Loss: 1.5989775732159615, Final Batch Loss: 0.055448390543460846\n",
      "Epoch 4882, Loss: 3.541148692369461, Final Batch Loss: 1.9944632053375244\n",
      "Epoch 4883, Loss: 2.1746024191379547, Final Batch Loss: 0.6915708780288696\n",
      "Epoch 4884, Loss: 1.6064795397687703, Final Batch Loss: 0.0023600601125508547\n",
      "Epoch 4885, Loss: 1.6440563471987844, Final Batch Loss: 0.01042893249541521\n",
      "Epoch 4886, Loss: 1.5777875380590558, Final Batch Loss: 0.0013222293928265572\n",
      "Epoch 4887, Loss: 1.7287362068891525, Final Batch Loss: 0.24799655377864838\n",
      "Epoch 4888, Loss: 2.7417379021644592, Final Batch Loss: 1.2509522438049316\n",
      "Epoch 4889, Loss: 1.4231063453480601, Final Batch Loss: 0.006502422504127026\n",
      "Epoch 4890, Loss: 1.4431435279548168, Final Batch Loss: 0.02949662134051323\n",
      "Epoch 4891, Loss: 1.4968168139457703, Final Batch Loss: 0.0483894944190979\n",
      "Epoch 4892, Loss: 1.403939863666892, Final Batch Loss: 0.003060540184378624\n",
      "Epoch 4893, Loss: 1.4463903056457639, Final Batch Loss: 0.012700860388576984\n",
      "Epoch 4894, Loss: 1.7174440324306488, Final Batch Loss: 0.29257985949516296\n",
      "Epoch 4895, Loss: 1.4386327862739563, Final Batch Loss: 0.03951945900917053\n",
      "Epoch 4896, Loss: 1.6305138021707535, Final Batch Loss: 0.2304738312959671\n",
      "Epoch 4897, Loss: 1.3950285841710865, Final Batch Loss: 0.0031461049802601337\n",
      "Epoch 4898, Loss: 1.4361073654145002, Final Batch Loss: 0.028000084683299065\n",
      "Epoch 4899, Loss: 1.4280734460335225, Final Batch Loss: 0.0018209319096058607\n",
      "Epoch 4900, Loss: 1.441416410729289, Final Batch Loss: 0.006580350920557976\n",
      "Epoch 4901, Loss: 1.420919120311737, Final Batch Loss: 0.08221641182899475\n",
      "Epoch 4902, Loss: 1.4050687365233898, Final Batch Loss: 0.021523933857679367\n",
      "Epoch 4903, Loss: 3.3788125813007355, Final Batch Loss: 2.0034375190734863\n",
      "Epoch 4904, Loss: 1.3807565681636333, Final Batch Loss: 0.010241102427244186\n",
      "Epoch 4905, Loss: 1.5371579974889755, Final Batch Loss: 0.20402775704860687\n",
      "Epoch 4906, Loss: 1.3805772475898266, Final Batch Loss: 0.005044352263212204\n",
      "Epoch 4907, Loss: 1.9205156862735748, Final Batch Loss: 0.5318302512168884\n",
      "Epoch 4908, Loss: 1.4808139158412814, Final Batch Loss: 0.008178080432116985\n",
      "Epoch 4909, Loss: 1.4202042835531756, Final Batch Loss: 0.0018254535971209407\n",
      "Epoch 4910, Loss: 2.9605823159217834, Final Batch Loss: 1.4384875297546387\n",
      "Epoch 4911, Loss: 1.5855860710144043, Final Batch Loss: 0.13506275415420532\n",
      "Epoch 4912, Loss: 1.4473826885223389, Final Batch Loss: 0.023767679929733276\n",
      "Epoch 4913, Loss: 1.5786334946751595, Final Batch Loss: 0.02799869328737259\n",
      "Epoch 4914, Loss: 1.455327161878813, Final Batch Loss: 0.0007203606073744595\n",
      "Epoch 4915, Loss: 1.6545111276209354, Final Batch Loss: 0.061129603534936905\n",
      "Epoch 4916, Loss: 2.9932111501693726, Final Batch Loss: 1.564994215965271\n",
      "Epoch 4917, Loss: 1.5421899165958166, Final Batch Loss: 0.01636781357228756\n",
      "Epoch 4918, Loss: 1.6090943664312363, Final Batch Loss: 0.13807041943073273\n",
      "Epoch 4919, Loss: 4.617715001106262, Final Batch Loss: 3.196133613586426\n",
      "Epoch 4920, Loss: 1.409475709311664, Final Batch Loss: 0.001095886342227459\n",
      "Epoch 4921, Loss: 1.5393245443701744, Final Batch Loss: 0.09304895251989365\n",
      "Epoch 4922, Loss: 1.675786405801773, Final Batch Loss: 0.2624714970588684\n",
      "Epoch 4923, Loss: 3.4235448837280273, Final Batch Loss: 1.8988450765609741\n",
      "Epoch 4924, Loss: 1.5867104679346085, Final Batch Loss: 0.04233954846858978\n",
      "Epoch 4925, Loss: 1.5364691242575645, Final Batch Loss: 0.03723042458295822\n",
      "Epoch 4926, Loss: 1.445869121758733, Final Batch Loss: 0.000228140561375767\n",
      "Epoch 4927, Loss: 1.5754454880952835, Final Batch Loss: 0.08435292541980743\n",
      "Epoch 4928, Loss: 1.373502096743323, Final Batch Loss: 0.0015683980891481042\n",
      "Epoch 4929, Loss: 1.467037660535425, Final Batch Loss: 0.005330634769052267\n",
      "Epoch 4930, Loss: 1.386619819328189, Final Batch Loss: 0.025233013555407524\n",
      "Epoch 4931, Loss: 2.2068603932857513, Final Batch Loss: 0.662482500076294\n",
      "Epoch 4932, Loss: 1.3701068108421168, Final Batch Loss: 0.00010442188795423135\n",
      "Epoch 4933, Loss: 1.3945504209259525, Final Batch Loss: 0.001111366436816752\n",
      "Epoch 4934, Loss: 1.3859720850596204, Final Batch Loss: 0.0012897277483716607\n",
      "Epoch 4935, Loss: 1.4604685008525848, Final Batch Loss: 0.052219390869140625\n",
      "Epoch 4936, Loss: 1.5042215436697006, Final Batch Loss: 0.11251859366893768\n",
      "Epoch 4937, Loss: 1.344875977432821, Final Batch Loss: 0.0009284476400353014\n",
      "Epoch 4938, Loss: 1.409277580678463, Final Batch Loss: 0.009308509528636932\n",
      "Epoch 4939, Loss: 1.6374315172433853, Final Batch Loss: 0.1822122186422348\n",
      "Epoch 4940, Loss: 3.2143805623054504, Final Batch Loss: 1.7586708068847656\n",
      "Epoch 4941, Loss: 1.4960070177912712, Final Batch Loss: 0.09079619497060776\n",
      "Epoch 4942, Loss: 2.089812397956848, Final Batch Loss: 0.7314144372940063\n",
      "Epoch 4943, Loss: 2.508374661207199, Final Batch Loss: 1.136018991470337\n",
      "Epoch 4944, Loss: 1.571749433875084, Final Batch Loss: 0.18208344280719757\n",
      "Epoch 4945, Loss: 1.3620647760180873, Final Batch Loss: 7.462222856702283e-05\n",
      "Epoch 4946, Loss: 1.3868001829832792, Final Batch Loss: 0.017700446769595146\n",
      "Epoch 4947, Loss: 1.4620836675167084, Final Batch Loss: 0.09477296471595764\n",
      "Epoch 4948, Loss: 3.0622037053108215, Final Batch Loss: 1.6603764295578003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4949, Loss: 1.4214582731947303, Final Batch Loss: 0.00042500998824834824\n",
      "Epoch 4950, Loss: 1.4079046892002225, Final Batch Loss: 0.01009890902787447\n",
      "Epoch 4951, Loss: 1.7672795057296753, Final Batch Loss: 0.3139195740222931\n",
      "Epoch 4952, Loss: 1.8681931793689728, Final Batch Loss: 0.448967307806015\n",
      "Epoch 4953, Loss: 2.8853592574596405, Final Batch Loss: 1.4647200107574463\n",
      "Epoch 4954, Loss: 1.4399963913547253, Final Batch Loss: 1.8954096958623268e-05\n",
      "Epoch 4955, Loss: 1.5250357426702976, Final Batch Loss: 0.03468722477555275\n",
      "Epoch 4956, Loss: 1.434776388690807, Final Batch Loss: 0.0018941095331683755\n",
      "Epoch 4957, Loss: 1.408200915902853, Final Batch Loss: 0.02883654460310936\n",
      "Epoch 4958, Loss: 2.4705729484558105, Final Batch Loss: 0.9617026448249817\n",
      "Epoch 4959, Loss: 2.144141048192978, Final Batch Loss: 0.7060433030128479\n",
      "Epoch 4960, Loss: 3.233532190322876, Final Batch Loss: 1.8377058506011963\n",
      "Epoch 4961, Loss: 1.6053512143553235, Final Batch Loss: 0.0003313469351269305\n",
      "Epoch 4962, Loss: 1.6520967588294297, Final Batch Loss: 0.003616105066612363\n",
      "Epoch 4963, Loss: 1.5959070082753897, Final Batch Loss: 0.010119324550032616\n",
      "Epoch 4964, Loss: 1.599012978374958, Final Batch Loss: 0.04666832834482193\n",
      "Epoch 4965, Loss: 1.7310807220637798, Final Batch Loss: 0.05210782214999199\n",
      "Epoch 4966, Loss: 1.607014000415802, Final Batch Loss: 0.14892536401748657\n",
      "Epoch 4967, Loss: 2.5958308577537537, Final Batch Loss: 1.1812028884887695\n",
      "Epoch 4968, Loss: 1.3623261200264096, Final Batch Loss: 0.002946917898952961\n",
      "Epoch 4969, Loss: 1.4909080903744325, Final Batch Loss: 0.0016164820408448577\n",
      "Epoch 4970, Loss: 1.9319161474704742, Final Batch Loss: 0.4858036935329437\n",
      "Epoch 4971, Loss: 3.201927602291107, Final Batch Loss: 1.7249959707260132\n",
      "Epoch 4972, Loss: 2.218559741973877, Final Batch Loss: 0.7216395735740662\n",
      "Epoch 4973, Loss: 1.6403783558344003, Final Batch Loss: 1.7046782886609435e-05\n",
      "Epoch 4974, Loss: 1.6641588150523603, Final Batch Loss: 0.0008870004676282406\n",
      "Epoch 4975, Loss: 1.6602869033810066, Final Batch Loss: 8.344646857949556e-07\n",
      "Epoch 4976, Loss: 2.0774762630462646, Final Batch Loss: 0.5085540413856506\n",
      "Epoch 4977, Loss: 2.2752087712287903, Final Batch Loss: 0.6209683418273926\n",
      "Epoch 4978, Loss: 1.63665721565485, Final Batch Loss: 0.07811442762613297\n",
      "Epoch 4979, Loss: 3.1573669016361237, Final Batch Loss: 1.689810872077942\n",
      "Epoch 4980, Loss: 1.5433613201603293, Final Batch Loss: 0.009836181066930294\n",
      "Epoch 4981, Loss: 1.5838128924369812, Final Batch Loss: 0.06615698337554932\n",
      "Epoch 4982, Loss: 1.5028583025559783, Final Batch Loss: 0.008547625504434109\n",
      "Epoch 4983, Loss: 1.593462597578764, Final Batch Loss: 0.0015747062861919403\n",
      "Epoch 4984, Loss: 1.4992453245795332, Final Batch Loss: 0.0009673921740613878\n",
      "Epoch 4985, Loss: 1.540649976581335, Final Batch Loss: 0.010868873447179794\n",
      "Epoch 4986, Loss: 1.435690917656757, Final Batch Loss: 0.0009369036415591836\n",
      "Epoch 4987, Loss: 1.8124126195907593, Final Batch Loss: 0.35328012704849243\n",
      "Epoch 4988, Loss: 1.4468551422469318, Final Batch Loss: 0.0012329365126788616\n",
      "Epoch 4989, Loss: 1.4126051203347743, Final Batch Loss: 0.0036480561830103397\n",
      "Epoch 4990, Loss: 2.363529086112976, Final Batch Loss: 0.9892910718917847\n",
      "Epoch 4991, Loss: 1.530787467956543, Final Batch Loss: 0.1419905126094818\n",
      "Epoch 4992, Loss: 1.4571531005203724, Final Batch Loss: 0.02071087434887886\n",
      "Epoch 4993, Loss: 1.4374129772068045, Final Batch Loss: 4.887569048150908e-06\n",
      "Epoch 4994, Loss: 1.5187181839428376, Final Batch Loss: 0.00024041623692028224\n",
      "Epoch 4995, Loss: 1.500603219436016, Final Batch Loss: 0.000745018885936588\n",
      "Epoch 4996, Loss: 1.4693999290420834, Final Batch Loss: 2.9802276912960224e-06\n",
      "Epoch 4997, Loss: 2.0114191472530365, Final Batch Loss: 0.6566729545593262\n",
      "Epoch 4998, Loss: 1.4750145599246025, Final Batch Loss: 0.05063774436712265\n",
      "Epoch 4999, Loss: 1.8418672382831573, Final Batch Loss: 0.4395730197429657\n",
      "Epoch 5000, Loss: 1.3926325439224456, Final Batch Loss: 1.537788011773955e-05\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0]\n",
      " [ 0  5  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0]\n",
      " [ 0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  3  0  0  4  0  0  0  0  0  3  0  0  1  0  0  2]\n",
      " [ 0  0  0  0  0  0 11  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  2  0  0  4  0  0  2  0  0  2  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 13  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  1  0  0 14  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 11  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  7  0  0  0  0]\n",
      " [ 0  0  3  0  0  1  0  0  2  0  0  0  0  0  3  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 14  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5  0]\n",
      " [ 0  0  3  0  0  0  0  0  0  0  0  0  0  0  3  0  0  2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.96429   0.98182        28\n",
      "           1    1.00000   0.83333   0.90909         6\n",
      "           2    0.35714   0.83333   0.50000         6\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    0.90909   1.00000   0.95238        10\n",
      "           5    0.50000   0.30769   0.38095        13\n",
      "           6    1.00000   1.00000   1.00000        11\n",
      "           7    1.00000   1.00000   1.00000         7\n",
      "           8    0.57143   0.40000   0.47059        10\n",
      "           9    1.00000   1.00000   1.00000        13\n",
      "          10    1.00000   1.00000   1.00000         5\n",
      "          11    0.73684   0.87500   0.80000        16\n",
      "          12    1.00000   1.00000   1.00000        11\n",
      "          13    0.77778   0.87500   0.82353         8\n",
      "          14    0.33333   0.33333   0.33333         9\n",
      "          15    1.00000   1.00000   1.00000        14\n",
      "          16    1.00000   1.00000   1.00000         5\n",
      "          17    0.40000   0.25000   0.30769         8\n",
      "\n",
      "    accuracy                        0.82447       188\n",
      "   macro avg    0.81031   0.81511   0.80330       188\n",
      "weighted avg    0.82798   0.82447   0.81903       188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Fake Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20,000 epochs DEFAULT PARAMETERS FOR THRESHOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (gen): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=109, out_features=80, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=80, out_features=60, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=60, out_features=50, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Linear(in_features=50, out_features=33, bias=True)\n",
       "    (4): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator(z_dim = 109)\n",
    "load_model(gen, \"3 Label 6 Subject GAN Ablation_gen.param\")\n",
    "gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(X_test)\n",
    "latent_vectors = get_noise(size, 100)\n",
    "act_vectors = get_act_matrix(size, 3)\n",
    "usr_vectors = get_usr_matrix(size, 6)\n",
    "\n",
    "fake_labels = []\n",
    "\n",
    "for k in range(size):\n",
    "    if act_vectors[0][k] == 0 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(0)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(1)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(2)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(3)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(4)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(5)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(6)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(7)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(8)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 3:\n",
    "        fake_labels.append(9)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 3:\n",
    "        fake_labels.append(10)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 3:\n",
    "        fake_labels.append(11)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 4:\n",
    "        fake_labels.append(12)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 4:\n",
    "        fake_labels.append(13)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 4:\n",
    "        fake_labels.append(14)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 5:\n",
    "        fake_labels.append(15)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 5:\n",
    "        fake_labels.append(16)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 5:\n",
    "        fake_labels.append(17)\n",
    "        \n",
    "fake_labels = np.asarray(fake_labels)\n",
    "to_gen = torch.cat((latent_vectors, act_vectors[1], usr_vectors[1]), 1)\n",
    "fake_features = gen(to_gen).detach()\n",
    "#fake_features = gen(to_gen).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  4  0  0  0  0  0  0  0  0  0  0  0  5  0  0  0  0]\n",
      " [ 0  0 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 14  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  0  8  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 15  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 14  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        11\n",
      "           1    0.33333   0.44444   0.38095         9\n",
      "           2    1.00000   1.00000   1.00000        11\n",
      "           3    1.00000   1.00000   1.00000        10\n",
      "           4    1.00000   1.00000   1.00000        14\n",
      "           5    1.00000   1.00000   1.00000         6\n",
      "           6    0.00000   0.00000   0.00000         9\n",
      "           7    0.00000   0.00000   0.00000         7\n",
      "           8    0.00000   0.00000   0.00000         8\n",
      "           9    0.47059   0.88889   0.61538         9\n",
      "          10    1.00000   0.40000   0.57143        10\n",
      "          11    0.65217   1.00000   0.78947        15\n",
      "          12    1.00000   1.00000   1.00000        10\n",
      "          13    0.00000   0.00000   0.00000         8\n",
      "          14    0.00000   0.00000   0.00000         5\n",
      "          15    1.00000   1.00000   1.00000        11\n",
      "          16    0.51852   1.00000   0.68293        14\n",
      "          17    1.00000   1.00000   1.00000        21\n",
      "\n",
      "    accuracy                        0.73936       188\n",
      "   macro avg    0.60970   0.65185   0.61334       188\n",
      "weighted avg    0.68233   0.73936   0.69194       188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5, zero_division = 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
