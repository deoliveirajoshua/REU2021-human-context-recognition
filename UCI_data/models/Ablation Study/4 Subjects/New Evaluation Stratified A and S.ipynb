{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '141 tBodyGyro-iqr()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '434 fBodyGyro-max()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 30),\n",
    "            classifier_block(30, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            nn.Linear(15, 12)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "    \n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_10 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_11 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_12 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9, X_10, X_11, X_12))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9) + [9] * len(X_10) + [10] * len(X_11) + [11] * len(X_12)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5, 7]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.422689199447632, Final Batch Loss: 2.453273057937622\n",
      "Epoch 2, Loss: 7.370434284210205, Final Batch Loss: 2.406439781188965\n",
      "Epoch 3, Loss: 7.4301841259002686, Final Batch Loss: 2.477592706680298\n",
      "Epoch 4, Loss: 7.389699459075928, Final Batch Loss: 2.442140579223633\n",
      "Epoch 5, Loss: 7.482944965362549, Final Batch Loss: 2.5409023761749268\n",
      "Epoch 6, Loss: 7.366950750350952, Final Batch Loss: 2.422630786895752\n",
      "Epoch 7, Loss: 7.390000820159912, Final Batch Loss: 2.4611198902130127\n",
      "Epoch 8, Loss: 7.3953564167022705, Final Batch Loss: 2.4677884578704834\n",
      "Epoch 9, Loss: 7.428051710128784, Final Batch Loss: 2.5073204040527344\n",
      "Epoch 10, Loss: 7.333410739898682, Final Batch Loss: 2.425410509109497\n",
      "Epoch 11, Loss: 7.342511892318726, Final Batch Loss: 2.442734479904175\n",
      "Epoch 12, Loss: 7.303533554077148, Final Batch Loss: 2.4173624515533447\n",
      "Epoch 13, Loss: 7.3596882820129395, Final Batch Loss: 2.481877326965332\n",
      "Epoch 14, Loss: 7.262422561645508, Final Batch Loss: 2.404369354248047\n",
      "Epoch 15, Loss: 7.284754276275635, Final Batch Loss: 2.442131757736206\n",
      "Epoch 16, Loss: 7.302757263183594, Final Batch Loss: 2.481419563293457\n",
      "Epoch 17, Loss: 7.220242738723755, Final Batch Loss: 2.416125535964966\n",
      "Epoch 18, Loss: 7.184870958328247, Final Batch Loss: 2.3981428146362305\n",
      "Epoch 19, Loss: 7.178508043289185, Final Batch Loss: 2.420588254928589\n",
      "Epoch 20, Loss: 7.139387130737305, Final Batch Loss: 2.422816038131714\n",
      "Epoch 21, Loss: 7.0264811515808105, Final Batch Loss: 2.329379081726074\n",
      "Epoch 22, Loss: 6.891132593154907, Final Batch Loss: 2.2565011978149414\n",
      "Epoch 23, Loss: 6.8918776512146, Final Batch Loss: 2.2604644298553467\n",
      "Epoch 24, Loss: 6.879136800765991, Final Batch Loss: 2.335211992263794\n",
      "Epoch 25, Loss: 6.664928913116455, Final Batch Loss: 2.1016385555267334\n",
      "Epoch 26, Loss: 6.833937406539917, Final Batch Loss: 2.348541021347046\n",
      "Epoch 27, Loss: 6.563252687454224, Final Batch Loss: 2.0975635051727295\n",
      "Epoch 28, Loss: 6.727755546569824, Final Batch Loss: 2.3346846103668213\n",
      "Epoch 29, Loss: 6.5711305141448975, Final Batch Loss: 2.213364601135254\n",
      "Epoch 30, Loss: 6.531651258468628, Final Batch Loss: 2.164398670196533\n",
      "Epoch 31, Loss: 6.363563060760498, Final Batch Loss: 2.0951502323150635\n",
      "Epoch 32, Loss: 6.326259613037109, Final Batch Loss: 2.077934503555298\n",
      "Epoch 33, Loss: 6.143559694290161, Final Batch Loss: 1.974806785583496\n",
      "Epoch 34, Loss: 6.190644979476929, Final Batch Loss: 2.085636854171753\n",
      "Epoch 35, Loss: 6.174986839294434, Final Batch Loss: 2.1587297916412354\n",
      "Epoch 36, Loss: 5.882985591888428, Final Batch Loss: 1.8527649641036987\n",
      "Epoch 37, Loss: 5.9076210260391235, Final Batch Loss: 2.0224480628967285\n",
      "Epoch 38, Loss: 5.932816982269287, Final Batch Loss: 2.0922701358795166\n",
      "Epoch 39, Loss: 5.707436442375183, Final Batch Loss: 1.9022176265716553\n",
      "Epoch 40, Loss: 5.588330030441284, Final Batch Loss: 1.8837906122207642\n",
      "Epoch 41, Loss: 5.475461959838867, Final Batch Loss: 1.8209800720214844\n",
      "Epoch 42, Loss: 5.46660590171814, Final Batch Loss: 1.8684322834014893\n",
      "Epoch 43, Loss: 5.20095694065094, Final Batch Loss: 1.6582015752792358\n",
      "Epoch 44, Loss: 5.292246341705322, Final Batch Loss: 1.8785057067871094\n",
      "Epoch 45, Loss: 5.010663032531738, Final Batch Loss: 1.5917972326278687\n",
      "Epoch 46, Loss: 5.170665979385376, Final Batch Loss: 1.8216851949691772\n",
      "Epoch 47, Loss: 4.99630868434906, Final Batch Loss: 1.719316005706787\n",
      "Epoch 48, Loss: 5.00612473487854, Final Batch Loss: 1.691860556602478\n",
      "Epoch 49, Loss: 5.040522217750549, Final Batch Loss: 1.7628329992294312\n",
      "Epoch 50, Loss: 4.605277180671692, Final Batch Loss: 1.4314265251159668\n",
      "Epoch 51, Loss: 4.668650984764099, Final Batch Loss: 1.5708847045898438\n",
      "Epoch 52, Loss: 4.753376483917236, Final Batch Loss: 1.6136394739151\n",
      "Epoch 53, Loss: 4.751468300819397, Final Batch Loss: 1.6830897331237793\n",
      "Epoch 54, Loss: 4.633865118026733, Final Batch Loss: 1.476363182067871\n",
      "Epoch 55, Loss: 4.4598952531814575, Final Batch Loss: 1.3569225072860718\n",
      "Epoch 56, Loss: 4.58906102180481, Final Batch Loss: 1.4537832736968994\n",
      "Epoch 57, Loss: 4.435944199562073, Final Batch Loss: 1.4160447120666504\n",
      "Epoch 58, Loss: 4.314985156059265, Final Batch Loss: 1.3952008485794067\n",
      "Epoch 59, Loss: 4.329582214355469, Final Batch Loss: 1.3631925582885742\n",
      "Epoch 60, Loss: 4.500999450683594, Final Batch Loss: 1.6251327991485596\n",
      "Epoch 61, Loss: 4.533112525939941, Final Batch Loss: 1.6144599914550781\n",
      "Epoch 62, Loss: 4.524501204490662, Final Batch Loss: 1.6352064609527588\n",
      "Epoch 63, Loss: 4.515897750854492, Final Batch Loss: 1.5897536277770996\n",
      "Epoch 64, Loss: 4.316757321357727, Final Batch Loss: 1.3704687356948853\n",
      "Epoch 65, Loss: 4.162866115570068, Final Batch Loss: 1.2996939420700073\n",
      "Epoch 66, Loss: 4.321745157241821, Final Batch Loss: 1.4036195278167725\n",
      "Epoch 67, Loss: 4.516865849494934, Final Batch Loss: 1.584938645362854\n",
      "Epoch 68, Loss: 4.223733901977539, Final Batch Loss: 1.3883460760116577\n",
      "Epoch 69, Loss: 4.013250946998596, Final Batch Loss: 1.246629238128662\n",
      "Epoch 70, Loss: 4.081832766532898, Final Batch Loss: 1.3227472305297852\n",
      "Epoch 71, Loss: 4.176811099052429, Final Batch Loss: 1.396018147468567\n",
      "Epoch 72, Loss: 4.083466172218323, Final Batch Loss: 1.2491180896759033\n",
      "Epoch 73, Loss: 4.109009385108948, Final Batch Loss: 1.3697240352630615\n",
      "Epoch 74, Loss: 4.3130412101745605, Final Batch Loss: 1.6384998559951782\n",
      "Epoch 75, Loss: 3.985389232635498, Final Batch Loss: 1.255740761756897\n",
      "Epoch 76, Loss: 3.950656294822693, Final Batch Loss: 1.2832523584365845\n",
      "Epoch 77, Loss: 3.882926344871521, Final Batch Loss: 1.258318543434143\n",
      "Epoch 78, Loss: 4.023699641227722, Final Batch Loss: 1.3376909494400024\n",
      "Epoch 79, Loss: 3.7733551263809204, Final Batch Loss: 1.1237670183181763\n",
      "Epoch 80, Loss: 3.663640022277832, Final Batch Loss: 1.079636812210083\n",
      "Epoch 81, Loss: 3.811399459838867, Final Batch Loss: 1.201073408126831\n",
      "Epoch 82, Loss: 3.6365180015563965, Final Batch Loss: 1.0378936529159546\n",
      "Epoch 83, Loss: 3.878944516181946, Final Batch Loss: 1.3228203058242798\n",
      "Epoch 84, Loss: 3.783735156059265, Final Batch Loss: 1.212264060974121\n",
      "Epoch 85, Loss: 3.7621006965637207, Final Batch Loss: 1.14173424243927\n",
      "Epoch 86, Loss: 3.995736837387085, Final Batch Loss: 1.5145100355148315\n",
      "Epoch 87, Loss: 3.8626967668533325, Final Batch Loss: 1.3133344650268555\n",
      "Epoch 88, Loss: 3.9416030645370483, Final Batch Loss: 1.3046669960021973\n",
      "Epoch 89, Loss: 3.738653779029846, Final Batch Loss: 1.1530905961990356\n",
      "Epoch 90, Loss: 3.6606180667877197, Final Batch Loss: 1.1249006986618042\n",
      "Epoch 91, Loss: 3.6395586729049683, Final Batch Loss: 1.2241857051849365\n",
      "Epoch 92, Loss: 3.5757055282592773, Final Batch Loss: 1.0897575616836548\n",
      "Epoch 93, Loss: 3.9328590631484985, Final Batch Loss: 1.4726721048355103\n",
      "Epoch 94, Loss: 3.4305379390716553, Final Batch Loss: 1.0415153503417969\n",
      "Epoch 95, Loss: 3.756312847137451, Final Batch Loss: 1.3337701559066772\n",
      "Epoch 96, Loss: 4.155653238296509, Final Batch Loss: 1.8149296045303345\n",
      "Epoch 97, Loss: 3.303141176700592, Final Batch Loss: 0.8756627440452576\n",
      "Epoch 98, Loss: 3.9663798809051514, Final Batch Loss: 1.6149860620498657\n",
      "Epoch 99, Loss: 3.547605514526367, Final Batch Loss: 1.235589623451233\n",
      "Epoch 100, Loss: 3.576328158378601, Final Batch Loss: 1.1454294919967651\n",
      "Epoch 101, Loss: 3.5779154300689697, Final Batch Loss: 1.1853578090667725\n",
      "Epoch 102, Loss: 3.602219343185425, Final Batch Loss: 1.306656837463379\n",
      "Epoch 103, Loss: 3.8797357082366943, Final Batch Loss: 1.4928157329559326\n",
      "Epoch 104, Loss: 3.4338717460632324, Final Batch Loss: 1.127769112586975\n",
      "Epoch 105, Loss: 3.4630719423294067, Final Batch Loss: 1.127596378326416\n",
      "Epoch 106, Loss: 3.277680277824402, Final Batch Loss: 1.0350842475891113\n",
      "Epoch 107, Loss: 3.5112597942352295, Final Batch Loss: 1.1906964778900146\n",
      "Epoch 108, Loss: 3.180117428302765, Final Batch Loss: 0.8997368216514587\n",
      "Epoch 109, Loss: 3.6895679235458374, Final Batch Loss: 1.4170386791229248\n",
      "Epoch 110, Loss: 3.570020914077759, Final Batch Loss: 1.2892109155654907\n",
      "Epoch 111, Loss: 3.221145987510681, Final Batch Loss: 1.042853593826294\n",
      "Epoch 112, Loss: 3.432363271713257, Final Batch Loss: 1.1756176948547363\n",
      "Epoch 113, Loss: 3.027050256729126, Final Batch Loss: 0.7652254104614258\n",
      "Epoch 114, Loss: 3.3092663288116455, Final Batch Loss: 1.0276429653167725\n",
      "Epoch 115, Loss: 3.2737258672714233, Final Batch Loss: 1.021763801574707\n",
      "Epoch 116, Loss: 3.331266164779663, Final Batch Loss: 1.1362730264663696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117, Loss: 3.427583336830139, Final Batch Loss: 1.1823242902755737\n",
      "Epoch 118, Loss: 3.2597157955169678, Final Batch Loss: 1.063146710395813\n",
      "Epoch 119, Loss: 3.4876513481140137, Final Batch Loss: 1.3867236375808716\n",
      "Epoch 120, Loss: 3.2262483835220337, Final Batch Loss: 1.0066908597946167\n",
      "Epoch 121, Loss: 3.1837856769561768, Final Batch Loss: 1.025815725326538\n",
      "Epoch 122, Loss: 3.4219497442245483, Final Batch Loss: 1.3123468160629272\n",
      "Epoch 123, Loss: 3.139937400817871, Final Batch Loss: 0.9784226417541504\n",
      "Epoch 124, Loss: 3.1385037899017334, Final Batch Loss: 0.9966672658920288\n",
      "Epoch 125, Loss: 3.267960786819458, Final Batch Loss: 1.139164924621582\n",
      "Epoch 126, Loss: 3.433777928352356, Final Batch Loss: 1.2146730422973633\n",
      "Epoch 127, Loss: 3.080708682537079, Final Batch Loss: 0.9566335082054138\n",
      "Epoch 128, Loss: 3.121229887008667, Final Batch Loss: 1.026881456375122\n",
      "Epoch 129, Loss: 3.207728147506714, Final Batch Loss: 1.1390676498413086\n",
      "Epoch 130, Loss: 3.0669084191322327, Final Batch Loss: 1.0452020168304443\n",
      "Epoch 131, Loss: 3.0424453616142273, Final Batch Loss: 0.9528740048408508\n",
      "Epoch 132, Loss: 3.164432644844055, Final Batch Loss: 1.0688145160675049\n",
      "Epoch 133, Loss: 3.1216750741004944, Final Batch Loss: 0.9919393658638\n",
      "Epoch 134, Loss: 3.337036609649658, Final Batch Loss: 1.224230408668518\n",
      "Epoch 135, Loss: 3.2362509965896606, Final Batch Loss: 1.1914387941360474\n",
      "Epoch 136, Loss: 3.051164388656616, Final Batch Loss: 1.0035014152526855\n",
      "Epoch 137, Loss: 3.3543626070022583, Final Batch Loss: 1.3587003946304321\n",
      "Epoch 138, Loss: 2.866824150085449, Final Batch Loss: 0.8786125183105469\n",
      "Epoch 139, Loss: 3.2023507356643677, Final Batch Loss: 1.1410695314407349\n",
      "Epoch 140, Loss: 2.982712149620056, Final Batch Loss: 0.8510406017303467\n",
      "Epoch 141, Loss: 2.6636714339256287, Final Batch Loss: 0.6928101778030396\n",
      "Epoch 142, Loss: 2.954170525074005, Final Batch Loss: 0.9710097908973694\n",
      "Epoch 143, Loss: 3.066603362560272, Final Batch Loss: 1.046284556388855\n",
      "Epoch 144, Loss: 3.053625166416168, Final Batch Loss: 1.104156255722046\n",
      "Epoch 145, Loss: 2.7425606846809387, Final Batch Loss: 0.6842659115791321\n",
      "Epoch 146, Loss: 3.0192667841911316, Final Batch Loss: 1.1052191257476807\n",
      "Epoch 147, Loss: 3.130699872970581, Final Batch Loss: 1.1852830648422241\n",
      "Epoch 148, Loss: 3.257517457008362, Final Batch Loss: 1.2843043804168701\n",
      "Epoch 149, Loss: 3.052237570285797, Final Batch Loss: 1.1228911876678467\n",
      "Epoch 150, Loss: 2.907302677631378, Final Batch Loss: 0.9273719787597656\n",
      "Epoch 151, Loss: 3.079276204109192, Final Batch Loss: 1.0424773693084717\n",
      "Epoch 152, Loss: 3.1009291410446167, Final Batch Loss: 1.0901780128479004\n",
      "Epoch 153, Loss: 3.090458333492279, Final Batch Loss: 1.1445417404174805\n",
      "Epoch 154, Loss: 3.242908537387848, Final Batch Loss: 1.2467756271362305\n",
      "Epoch 155, Loss: 2.7392152547836304, Final Batch Loss: 0.8540855050086975\n",
      "Epoch 156, Loss: 3.0521554946899414, Final Batch Loss: 1.1195118427276611\n",
      "Epoch 157, Loss: 2.780497133731842, Final Batch Loss: 0.7455598711967468\n",
      "Epoch 158, Loss: 2.822098970413208, Final Batch Loss: 0.831805944442749\n",
      "Epoch 159, Loss: 3.016714572906494, Final Batch Loss: 1.1552938222885132\n",
      "Epoch 160, Loss: 2.980320692062378, Final Batch Loss: 1.1286193132400513\n",
      "Epoch 161, Loss: 3.030214548110962, Final Batch Loss: 1.148460865020752\n",
      "Epoch 162, Loss: 2.9154189825057983, Final Batch Loss: 1.0773894786834717\n",
      "Epoch 163, Loss: 2.7247249484062195, Final Batch Loss: 0.7876878976821899\n",
      "Epoch 164, Loss: 2.7660902738571167, Final Batch Loss: 0.9004675149917603\n",
      "Epoch 165, Loss: 2.880882978439331, Final Batch Loss: 0.9808820486068726\n",
      "Epoch 166, Loss: 2.5948274731636047, Final Batch Loss: 0.6334917545318604\n",
      "Epoch 167, Loss: 2.5765221118927, Final Batch Loss: 0.5928203463554382\n",
      "Epoch 168, Loss: 2.5884082913398743, Final Batch Loss: 0.7006059288978577\n",
      "Epoch 169, Loss: 2.940806269645691, Final Batch Loss: 1.058383584022522\n",
      "Epoch 170, Loss: 2.402982711791992, Final Batch Loss: 0.5956533551216125\n",
      "Epoch 171, Loss: 3.074623465538025, Final Batch Loss: 1.1745623350143433\n",
      "Epoch 172, Loss: 2.9646283388137817, Final Batch Loss: 1.135759949684143\n",
      "Epoch 173, Loss: 2.513576626777649, Final Batch Loss: 0.6629032492637634\n",
      "Epoch 174, Loss: 2.7847066521644592, Final Batch Loss: 0.9190374612808228\n",
      "Epoch 175, Loss: 2.6953577995300293, Final Batch Loss: 0.8391641974449158\n",
      "Epoch 176, Loss: 2.7345520853996277, Final Batch Loss: 0.9128565788269043\n",
      "Epoch 177, Loss: 2.917627453804016, Final Batch Loss: 1.0411368608474731\n",
      "Epoch 178, Loss: 2.6042492389678955, Final Batch Loss: 0.7192593216896057\n",
      "Epoch 179, Loss: 2.815707564353943, Final Batch Loss: 1.030359148979187\n",
      "Epoch 180, Loss: 2.621957242488861, Final Batch Loss: 0.7915072441101074\n",
      "Epoch 181, Loss: 2.8740509152412415, Final Batch Loss: 1.0900036096572876\n",
      "Epoch 182, Loss: 2.6190744638442993, Final Batch Loss: 0.8636298775672913\n",
      "Epoch 183, Loss: 2.758813798427582, Final Batch Loss: 0.9360607266426086\n",
      "Epoch 184, Loss: 2.613791763782501, Final Batch Loss: 0.8359624743461609\n",
      "Epoch 185, Loss: 2.524920880794525, Final Batch Loss: 0.7007222771644592\n",
      "Epoch 186, Loss: 2.6259453296661377, Final Batch Loss: 0.7792767286300659\n",
      "Epoch 187, Loss: 2.7474831342697144, Final Batch Loss: 1.0254286527633667\n",
      "Epoch 188, Loss: 2.940122127532959, Final Batch Loss: 1.1599252223968506\n",
      "Epoch 189, Loss: 2.587131917476654, Final Batch Loss: 0.821821928024292\n",
      "Epoch 190, Loss: 2.8927396535873413, Final Batch Loss: 1.0129296779632568\n",
      "Epoch 191, Loss: 2.4944690465927124, Final Batch Loss: 0.6807883977890015\n",
      "Epoch 192, Loss: 2.933580219745636, Final Batch Loss: 1.0849764347076416\n",
      "Epoch 193, Loss: 2.7305281162261963, Final Batch Loss: 0.8726418018341064\n",
      "Epoch 194, Loss: 2.6280879378318787, Final Batch Loss: 0.8477213978767395\n",
      "Epoch 195, Loss: 2.641394555568695, Final Batch Loss: 0.86532062292099\n",
      "Epoch 196, Loss: 2.6843658089637756, Final Batch Loss: 0.8951126933097839\n",
      "Epoch 197, Loss: 2.5619874596595764, Final Batch Loss: 0.7557308077812195\n",
      "Epoch 198, Loss: 2.6761598587036133, Final Batch Loss: 0.8872873187065125\n",
      "Epoch 199, Loss: 2.3809749484062195, Final Batch Loss: 0.6496239304542542\n",
      "Epoch 200, Loss: 2.6249574422836304, Final Batch Loss: 0.888971209526062\n",
      "Epoch 201, Loss: 2.5137293934822083, Final Batch Loss: 0.7440502643585205\n",
      "Epoch 202, Loss: 2.6429749131202698, Final Batch Loss: 0.837367594242096\n",
      "Epoch 203, Loss: 2.6159064173698425, Final Batch Loss: 0.8791193962097168\n",
      "Epoch 204, Loss: 2.681521415710449, Final Batch Loss: 0.8998867869377136\n",
      "Epoch 205, Loss: 2.5317294001579285, Final Batch Loss: 0.7763205766677856\n",
      "Epoch 206, Loss: 2.5504478812217712, Final Batch Loss: 0.886761486530304\n",
      "Epoch 207, Loss: 2.2936684489250183, Final Batch Loss: 0.5918689370155334\n",
      "Epoch 208, Loss: 2.638441562652588, Final Batch Loss: 0.8973144888877869\n",
      "Epoch 209, Loss: 2.389080762863159, Final Batch Loss: 0.6725786924362183\n",
      "Epoch 210, Loss: 2.5995472073554993, Final Batch Loss: 0.8749597668647766\n",
      "Epoch 211, Loss: 2.5430789589881897, Final Batch Loss: 0.7431286573410034\n",
      "Epoch 212, Loss: 2.7335968613624573, Final Batch Loss: 0.9860101938247681\n",
      "Epoch 213, Loss: 2.5594780445098877, Final Batch Loss: 0.7908166646957397\n",
      "Epoch 214, Loss: 2.6342753171920776, Final Batch Loss: 0.965756893157959\n",
      "Epoch 215, Loss: 2.3985464572906494, Final Batch Loss: 0.7271309494972229\n",
      "Epoch 216, Loss: 2.6215832233428955, Final Batch Loss: 0.9199249744415283\n",
      "Epoch 217, Loss: 2.6047141551971436, Final Batch Loss: 0.9220507740974426\n",
      "Epoch 218, Loss: 2.2977702021598816, Final Batch Loss: 0.5820178389549255\n",
      "Epoch 219, Loss: 2.4795985221862793, Final Batch Loss: 0.6888498663902283\n",
      "Epoch 220, Loss: 2.647653102874756, Final Batch Loss: 0.9610991477966309\n",
      "Epoch 221, Loss: 2.518482029438019, Final Batch Loss: 0.8437641263008118\n",
      "Epoch 222, Loss: 2.705462396144867, Final Batch Loss: 1.1136099100112915\n",
      "Epoch 223, Loss: 2.6836661100387573, Final Batch Loss: 0.967362642288208\n",
      "Epoch 224, Loss: 2.7251198291778564, Final Batch Loss: 1.0241128206253052\n",
      "Epoch 225, Loss: 2.459636628627777, Final Batch Loss: 0.7493327260017395\n",
      "Epoch 226, Loss: 2.2578144669532776, Final Batch Loss: 0.5945740938186646\n",
      "Epoch 227, Loss: 2.4016114473342896, Final Batch Loss: 0.6728664636611938\n",
      "Epoch 228, Loss: 2.687893033027649, Final Batch Loss: 1.0235971212387085\n",
      "Epoch 229, Loss: 2.309961676597595, Final Batch Loss: 0.6291597485542297\n",
      "Epoch 230, Loss: 2.5573567748069763, Final Batch Loss: 0.9067805409431458\n",
      "Epoch 231, Loss: 2.5373940467834473, Final Batch Loss: 0.7881443500518799\n",
      "Epoch 232, Loss: 2.4964370131492615, Final Batch Loss: 0.7792555689811707\n",
      "Epoch 233, Loss: 2.6014475226402283, Final Batch Loss: 1.032417893409729\n",
      "Epoch 234, Loss: 2.6933465003967285, Final Batch Loss: 0.9899070858955383\n",
      "Epoch 235, Loss: 2.604172706604004, Final Batch Loss: 0.955513596534729\n",
      "Epoch 236, Loss: 3.0187403559684753, Final Batch Loss: 1.2940161228179932\n",
      "Epoch 237, Loss: 2.3259516954421997, Final Batch Loss: 0.6673067212104797\n",
      "Epoch 238, Loss: 2.240861415863037, Final Batch Loss: 0.6306895613670349\n",
      "Epoch 239, Loss: 2.592285633087158, Final Batch Loss: 0.7856019735336304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240, Loss: 2.562880039215088, Final Batch Loss: 0.9492486715316772\n",
      "Epoch 241, Loss: 2.4186298847198486, Final Batch Loss: 0.7012877464294434\n",
      "Epoch 242, Loss: 2.501971125602722, Final Batch Loss: 0.9094704985618591\n",
      "Epoch 243, Loss: 2.660338878631592, Final Batch Loss: 1.071550726890564\n",
      "Epoch 244, Loss: 2.515383720397949, Final Batch Loss: 0.9008351564407349\n",
      "Epoch 245, Loss: 2.4574268460273743, Final Batch Loss: 0.7946351170539856\n",
      "Epoch 246, Loss: 2.9826626777648926, Final Batch Loss: 1.2346558570861816\n",
      "Epoch 247, Loss: 2.659198820590973, Final Batch Loss: 0.9632810354232788\n",
      "Epoch 248, Loss: 2.4432127475738525, Final Batch Loss: 0.781440258026123\n",
      "Epoch 249, Loss: 2.620486855506897, Final Batch Loss: 1.0203454494476318\n",
      "Epoch 250, Loss: 2.398397982120514, Final Batch Loss: 0.7455203533172607\n",
      "Epoch 251, Loss: 2.658624231815338, Final Batch Loss: 1.0219988822937012\n",
      "Epoch 252, Loss: 2.513192296028137, Final Batch Loss: 0.8394344449043274\n",
      "Epoch 253, Loss: 2.2527737617492676, Final Batch Loss: 0.569622278213501\n",
      "Epoch 254, Loss: 2.4549731612205505, Final Batch Loss: 0.8745598196983337\n",
      "Epoch 255, Loss: 2.406297206878662, Final Batch Loss: 0.7262675762176514\n",
      "Epoch 256, Loss: 2.3258612751960754, Final Batch Loss: 0.7104600071907043\n",
      "Epoch 257, Loss: 2.487295091152191, Final Batch Loss: 0.8028724193572998\n",
      "Epoch 258, Loss: 2.4563823342323303, Final Batch Loss: 0.8908645510673523\n",
      "Epoch 259, Loss: 2.548595130443573, Final Batch Loss: 0.967645525932312\n",
      "Epoch 260, Loss: 2.5541661977767944, Final Batch Loss: 0.8848793506622314\n",
      "Epoch 261, Loss: 2.295161485671997, Final Batch Loss: 0.7842822074890137\n",
      "Epoch 262, Loss: 2.139851152896881, Final Batch Loss: 0.5300726890563965\n",
      "Epoch 263, Loss: 2.5130571722984314, Final Batch Loss: 0.9055476784706116\n",
      "Epoch 264, Loss: 2.4588010907173157, Final Batch Loss: 0.8401361107826233\n",
      "Epoch 265, Loss: 2.400121510028839, Final Batch Loss: 0.8558349609375\n",
      "Epoch 266, Loss: 2.7554373741149902, Final Batch Loss: 1.0517816543579102\n",
      "Epoch 267, Loss: 2.437755763530731, Final Batch Loss: 0.7512531876564026\n",
      "Epoch 268, Loss: 2.251234710216522, Final Batch Loss: 0.6663232445716858\n",
      "Epoch 269, Loss: 2.485596239566803, Final Batch Loss: 0.8085713386535645\n",
      "Epoch 270, Loss: 2.1994029879570007, Final Batch Loss: 0.5106754899024963\n",
      "Epoch 271, Loss: 2.293259084224701, Final Batch Loss: 0.6592836976051331\n",
      "Epoch 272, Loss: 2.16136372089386, Final Batch Loss: 0.5998197793960571\n",
      "Epoch 273, Loss: 2.3553364872932434, Final Batch Loss: 0.8540173768997192\n",
      "Epoch 274, Loss: 2.091780960559845, Final Batch Loss: 0.5801197290420532\n",
      "Epoch 275, Loss: 2.3710586428642273, Final Batch Loss: 0.8092254996299744\n",
      "Epoch 276, Loss: 2.137945234775543, Final Batch Loss: 0.5533817410469055\n",
      "Epoch 277, Loss: 2.45951908826828, Final Batch Loss: 0.9075477123260498\n",
      "Epoch 278, Loss: 2.461969316005707, Final Batch Loss: 0.8411310315132141\n",
      "Epoch 279, Loss: 2.0625001788139343, Final Batch Loss: 0.5236777067184448\n",
      "Epoch 280, Loss: 2.4305837750434875, Final Batch Loss: 0.8075945973396301\n",
      "Epoch 281, Loss: 2.137051999568939, Final Batch Loss: 0.636111319065094\n",
      "Epoch 282, Loss: 2.385625422000885, Final Batch Loss: 0.8608483672142029\n",
      "Epoch 283, Loss: 2.299832284450531, Final Batch Loss: 0.7914167642593384\n",
      "Epoch 284, Loss: 2.0637342631816864, Final Batch Loss: 0.4995761811733246\n",
      "Epoch 285, Loss: 2.5877429246902466, Final Batch Loss: 1.0108462572097778\n",
      "Epoch 286, Loss: 2.741958439350128, Final Batch Loss: 1.1881660223007202\n",
      "Epoch 287, Loss: 2.511419355869293, Final Batch Loss: 0.9633243083953857\n",
      "Epoch 288, Loss: 2.390189290046692, Final Batch Loss: 0.7825406789779663\n",
      "Epoch 289, Loss: 2.2677212357521057, Final Batch Loss: 0.7139148116111755\n",
      "Epoch 290, Loss: 2.161610782146454, Final Batch Loss: 0.6983872056007385\n",
      "Epoch 291, Loss: 1.8617699444293976, Final Batch Loss: 0.3179732859134674\n",
      "Epoch 292, Loss: 2.4037081003189087, Final Batch Loss: 0.8494270443916321\n",
      "Epoch 293, Loss: 2.2370694279670715, Final Batch Loss: 0.6973548531532288\n",
      "Epoch 294, Loss: 2.3114447593688965, Final Batch Loss: 0.7108028531074524\n",
      "Epoch 295, Loss: 2.5248873233795166, Final Batch Loss: 0.9897783398628235\n",
      "Epoch 296, Loss: 2.3885836005210876, Final Batch Loss: 0.8540514707565308\n",
      "Epoch 297, Loss: 2.2000010013580322, Final Batch Loss: 0.7094627022743225\n",
      "Epoch 298, Loss: 2.1278650760650635, Final Batch Loss: 0.6643756628036499\n",
      "Epoch 299, Loss: 2.2204688787460327, Final Batch Loss: 0.7011528611183167\n",
      "Epoch 300, Loss: 2.2906194925308228, Final Batch Loss: 0.8263908624649048\n",
      "Epoch 301, Loss: 2.3283960819244385, Final Batch Loss: 0.8105530738830566\n",
      "Epoch 302, Loss: 2.4157543778419495, Final Batch Loss: 0.87026447057724\n",
      "Epoch 303, Loss: 2.1091113686561584, Final Batch Loss: 0.5869231224060059\n",
      "Epoch 304, Loss: 2.1398683190345764, Final Batch Loss: 0.5863313674926758\n",
      "Epoch 305, Loss: 2.418447494506836, Final Batch Loss: 0.9174246788024902\n",
      "Epoch 306, Loss: 2.3126585483551025, Final Batch Loss: 0.7713644504547119\n",
      "Epoch 307, Loss: 2.0312790274620056, Final Batch Loss: 0.5801049470901489\n",
      "Epoch 308, Loss: 2.1470894813537598, Final Batch Loss: 0.6212995052337646\n",
      "Epoch 309, Loss: 2.2399410605430603, Final Batch Loss: 0.7287073731422424\n",
      "Epoch 310, Loss: 1.9992884397506714, Final Batch Loss: 0.5595526695251465\n",
      "Epoch 311, Loss: 2.3521469831466675, Final Batch Loss: 0.8610979318618774\n",
      "Epoch 312, Loss: 2.066557824611664, Final Batch Loss: 0.5499993562698364\n",
      "Epoch 313, Loss: 2.335497796535492, Final Batch Loss: 0.8299440741539001\n",
      "Epoch 314, Loss: 2.2436472177505493, Final Batch Loss: 0.7739659547805786\n",
      "Epoch 315, Loss: 2.1307257413864136, Final Batch Loss: 0.6554542183876038\n",
      "Epoch 316, Loss: 2.104981541633606, Final Batch Loss: 0.6286330819129944\n",
      "Epoch 317, Loss: 2.2410532236099243, Final Batch Loss: 0.7742102742195129\n",
      "Epoch 318, Loss: 2.227647542953491, Final Batch Loss: 0.6912420392036438\n",
      "Epoch 319, Loss: 2.4715448021888733, Final Batch Loss: 0.9157576560974121\n",
      "Epoch 320, Loss: 2.1114478707313538, Final Batch Loss: 0.6711353063583374\n",
      "Epoch 321, Loss: 2.41712886095047, Final Batch Loss: 0.827944278717041\n",
      "Epoch 322, Loss: 2.4161030650138855, Final Batch Loss: 0.9386585354804993\n",
      "Epoch 323, Loss: 2.2062541842460632, Final Batch Loss: 0.7718206644058228\n",
      "Epoch 324, Loss: 2.6489946246147156, Final Batch Loss: 1.1576974391937256\n",
      "Epoch 325, Loss: 2.2550455927848816, Final Batch Loss: 0.7815973162651062\n",
      "Epoch 326, Loss: 2.3663430213928223, Final Batch Loss: 0.8957111239433289\n",
      "Epoch 327, Loss: 2.279248058795929, Final Batch Loss: 0.7863585352897644\n",
      "Epoch 328, Loss: 2.511934995651245, Final Batch Loss: 0.8960734009742737\n",
      "Epoch 329, Loss: 2.1639004945755005, Final Batch Loss: 0.6624353528022766\n",
      "Epoch 330, Loss: 2.0339971780776978, Final Batch Loss: 0.6063876152038574\n",
      "Epoch 331, Loss: 2.4158177375793457, Final Batch Loss: 0.9407436847686768\n",
      "Epoch 332, Loss: 2.0832985043525696, Final Batch Loss: 0.6038398146629333\n",
      "Epoch 333, Loss: 2.103465497493744, Final Batch Loss: 0.6627717614173889\n",
      "Epoch 334, Loss: 2.189192235469818, Final Batch Loss: 0.7120350003242493\n",
      "Epoch 335, Loss: 1.9275314509868622, Final Batch Loss: 0.45351138710975647\n",
      "Epoch 336, Loss: 2.177228093147278, Final Batch Loss: 0.6725287437438965\n",
      "Epoch 337, Loss: 2.37820565700531, Final Batch Loss: 0.9556017518043518\n",
      "Epoch 338, Loss: 2.319728434085846, Final Batch Loss: 0.8372930884361267\n",
      "Epoch 339, Loss: 2.1410187482833862, Final Batch Loss: 0.7686406970024109\n",
      "Epoch 340, Loss: 2.2725865244865417, Final Batch Loss: 0.903117835521698\n",
      "Epoch 341, Loss: 2.3433678150177, Final Batch Loss: 0.9155128598213196\n",
      "Epoch 342, Loss: 2.175252377986908, Final Batch Loss: 0.7057185769081116\n",
      "Epoch 343, Loss: 2.3633534908294678, Final Batch Loss: 0.9242889881134033\n",
      "Epoch 344, Loss: 2.5667877793312073, Final Batch Loss: 1.045059323310852\n",
      "Epoch 345, Loss: 2.3126355409622192, Final Batch Loss: 0.896852970123291\n",
      "Epoch 346, Loss: 2.5605046153068542, Final Batch Loss: 1.0832850933074951\n",
      "Epoch 347, Loss: 2.0813282132148743, Final Batch Loss: 0.6694669723510742\n",
      "Epoch 348, Loss: 2.1158955097198486, Final Batch Loss: 0.6450983881950378\n",
      "Epoch 349, Loss: 2.1970648169517517, Final Batch Loss: 0.773568332195282\n",
      "Epoch 350, Loss: 2.1559327840805054, Final Batch Loss: 0.7805936932563782\n",
      "Epoch 351, Loss: 1.8851410150527954, Final Batch Loss: 0.4584360718727112\n",
      "Epoch 352, Loss: 2.1563100814819336, Final Batch Loss: 0.7218438386917114\n",
      "Epoch 353, Loss: 2.0769620537757874, Final Batch Loss: 0.6254191398620605\n",
      "Epoch 354, Loss: 2.057770311832428, Final Batch Loss: 0.585335910320282\n",
      "Epoch 355, Loss: 2.0593326091766357, Final Batch Loss: 0.6733635067939758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 356, Loss: 2.417973220348358, Final Batch Loss: 1.0592554807662964\n",
      "Epoch 357, Loss: 2.0828887820243835, Final Batch Loss: 0.7447357177734375\n",
      "Epoch 358, Loss: 1.9610441327095032, Final Batch Loss: 0.5549768805503845\n",
      "Epoch 359, Loss: 2.0557820200920105, Final Batch Loss: 0.6312133073806763\n",
      "Epoch 360, Loss: 2.0889267325401306, Final Batch Loss: 0.6124946475028992\n",
      "Epoch 361, Loss: 2.090624511241913, Final Batch Loss: 0.688174307346344\n",
      "Epoch 362, Loss: 2.152690351009369, Final Batch Loss: 0.6995115280151367\n",
      "Epoch 363, Loss: 1.762330025434494, Final Batch Loss: 0.3550420105457306\n",
      "Epoch 364, Loss: 2.1940632462501526, Final Batch Loss: 0.8724821209907532\n",
      "Epoch 365, Loss: 1.9057292342185974, Final Batch Loss: 0.5616421103477478\n",
      "Epoch 366, Loss: 2.27535480260849, Final Batch Loss: 0.8767470121383667\n",
      "Epoch 367, Loss: 2.237858235836029, Final Batch Loss: 0.7914186120033264\n",
      "Epoch 368, Loss: 2.1019963026046753, Final Batch Loss: 0.6712498068809509\n",
      "Epoch 369, Loss: 1.8404226005077362, Final Batch Loss: 0.41425821185112\n",
      "Epoch 370, Loss: 2.1320486664772034, Final Batch Loss: 0.678259015083313\n",
      "Epoch 371, Loss: 1.975713074207306, Final Batch Loss: 0.5564081072807312\n",
      "Epoch 372, Loss: 1.8503219783306122, Final Batch Loss: 0.4564865529537201\n",
      "Epoch 373, Loss: 2.0985634326934814, Final Batch Loss: 0.7259466052055359\n",
      "Epoch 374, Loss: 2.0286470651626587, Final Batch Loss: 0.6520342230796814\n",
      "Epoch 375, Loss: 1.8473804593086243, Final Batch Loss: 0.5393623113632202\n",
      "Epoch 376, Loss: 2.2073230147361755, Final Batch Loss: 0.8782405853271484\n",
      "Epoch 377, Loss: 1.911314845085144, Final Batch Loss: 0.5259135365486145\n",
      "Epoch 378, Loss: 2.1309832334518433, Final Batch Loss: 0.7380427122116089\n",
      "Epoch 379, Loss: 2.0214962363243103, Final Batch Loss: 0.6638456583023071\n",
      "Epoch 380, Loss: 2.0007123351097107, Final Batch Loss: 0.607258677482605\n",
      "Epoch 381, Loss: 2.0228113532066345, Final Batch Loss: 0.6840203404426575\n",
      "Epoch 382, Loss: 2.2004395127296448, Final Batch Loss: 0.7902496457099915\n",
      "Epoch 383, Loss: 2.2087015509605408, Final Batch Loss: 0.8551661968231201\n",
      "Epoch 384, Loss: 2.1860931515693665, Final Batch Loss: 0.7894942164421082\n",
      "Epoch 385, Loss: 1.8580827713012695, Final Batch Loss: 0.49212366342544556\n",
      "Epoch 386, Loss: 2.2795433402061462, Final Batch Loss: 0.8415671586990356\n",
      "Epoch 387, Loss: 2.0017884373664856, Final Batch Loss: 0.6518356204032898\n",
      "Epoch 388, Loss: 2.1348479986190796, Final Batch Loss: 0.7819797396659851\n",
      "Epoch 389, Loss: 2.187984049320221, Final Batch Loss: 0.818766713142395\n",
      "Epoch 390, Loss: 2.0464411973953247, Final Batch Loss: 0.6883684992790222\n",
      "Epoch 391, Loss: 1.7227670848369598, Final Batch Loss: 0.4219718277454376\n",
      "Epoch 392, Loss: 2.1097729206085205, Final Batch Loss: 0.739541232585907\n",
      "Epoch 393, Loss: 2.0082210302352905, Final Batch Loss: 0.6077901721000671\n",
      "Epoch 394, Loss: 1.9870561957359314, Final Batch Loss: 0.6593485474586487\n",
      "Epoch 395, Loss: 2.359023094177246, Final Batch Loss: 0.8921861052513123\n",
      "Epoch 396, Loss: 2.1889320611953735, Final Batch Loss: 0.8014375567436218\n",
      "Epoch 397, Loss: 1.9390632510185242, Final Batch Loss: 0.5299773216247559\n",
      "Epoch 398, Loss: 2.1862183809280396, Final Batch Loss: 0.8194450736045837\n",
      "Epoch 399, Loss: 2.246240496635437, Final Batch Loss: 0.9535477161407471\n",
      "Epoch 400, Loss: 1.8533974289894104, Final Batch Loss: 0.5201374888420105\n",
      "Epoch 401, Loss: 2.083670437335968, Final Batch Loss: 0.71152663230896\n",
      "Epoch 402, Loss: 1.9993844628334045, Final Batch Loss: 0.6192976832389832\n",
      "Epoch 403, Loss: 2.0783535838127136, Final Batch Loss: 0.6769230961799622\n",
      "Epoch 404, Loss: 1.8021876513957977, Final Batch Loss: 0.4240742623806\n",
      "Epoch 405, Loss: 2.1060280203819275, Final Batch Loss: 0.7834348082542419\n",
      "Epoch 406, Loss: 2.320124328136444, Final Batch Loss: 1.0169755220413208\n",
      "Epoch 407, Loss: 2.0066969990730286, Final Batch Loss: 0.6373919248580933\n",
      "Epoch 408, Loss: 2.1470048427581787, Final Batch Loss: 0.8137296438217163\n",
      "Epoch 409, Loss: 2.0387828946113586, Final Batch Loss: 0.6778144240379333\n",
      "Epoch 410, Loss: 2.131666362285614, Final Batch Loss: 0.7141048312187195\n",
      "Epoch 411, Loss: 2.0961238741874695, Final Batch Loss: 0.7389817237854004\n",
      "Epoch 412, Loss: 2.1196992993354797, Final Batch Loss: 0.8028342723846436\n",
      "Epoch 413, Loss: 1.9140992760658264, Final Batch Loss: 0.6267091035842896\n",
      "Epoch 414, Loss: 2.0204623341560364, Final Batch Loss: 0.618756115436554\n",
      "Epoch 415, Loss: 2.0864996910095215, Final Batch Loss: 0.6706885099411011\n",
      "Epoch 416, Loss: 1.8786560893058777, Final Batch Loss: 0.5003961324691772\n",
      "Epoch 417, Loss: 2.0917951464653015, Final Batch Loss: 0.7664443254470825\n",
      "Epoch 418, Loss: 2.273930072784424, Final Batch Loss: 0.9534894824028015\n",
      "Epoch 419, Loss: 2.3163739442825317, Final Batch Loss: 1.025333046913147\n",
      "Epoch 420, Loss: 1.8212362825870514, Final Batch Loss: 0.42151203751564026\n",
      "Epoch 421, Loss: 1.942244827747345, Final Batch Loss: 0.6153079867362976\n",
      "Epoch 422, Loss: 1.95133775472641, Final Batch Loss: 0.659238338470459\n",
      "Epoch 423, Loss: 2.1173070669174194, Final Batch Loss: 0.7468535900115967\n",
      "Epoch 424, Loss: 1.8172501623630524, Final Batch Loss: 0.4937996566295624\n",
      "Epoch 425, Loss: 1.7916259467601776, Final Batch Loss: 0.49509599804878235\n",
      "Epoch 426, Loss: 1.7641630470752716, Final Batch Loss: 0.4889906346797943\n",
      "Epoch 427, Loss: 2.0870174765586853, Final Batch Loss: 0.7637301683425903\n",
      "Epoch 428, Loss: 1.8743266463279724, Final Batch Loss: 0.573793888092041\n",
      "Epoch 429, Loss: 1.9726057052612305, Final Batch Loss: 0.6383005380630493\n",
      "Epoch 430, Loss: 2.062889277935028, Final Batch Loss: 0.7451514005661011\n",
      "Epoch 431, Loss: 1.8753042817115784, Final Batch Loss: 0.6426581740379333\n",
      "Epoch 432, Loss: 2.0508078932762146, Final Batch Loss: 0.822985827922821\n",
      "Epoch 433, Loss: 1.6593263447284698, Final Batch Loss: 0.35869744420051575\n",
      "Epoch 434, Loss: 2.0533854365348816, Final Batch Loss: 0.6986513137817383\n",
      "Epoch 435, Loss: 1.714107185602188, Final Batch Loss: 0.48223838210105896\n",
      "Epoch 436, Loss: 1.9967849254608154, Final Batch Loss: 0.6743867993354797\n",
      "Epoch 437, Loss: 2.0659626722335815, Final Batch Loss: 0.7974092960357666\n",
      "Epoch 438, Loss: 1.7980331182479858, Final Batch Loss: 0.5143054127693176\n",
      "Epoch 439, Loss: 2.1283957958221436, Final Batch Loss: 0.8818535804748535\n",
      "Epoch 440, Loss: 2.0269190073013306, Final Batch Loss: 0.7764633893966675\n",
      "Epoch 441, Loss: 1.7378777861595154, Final Batch Loss: 0.5011889934539795\n",
      "Epoch 442, Loss: 2.1955403089523315, Final Batch Loss: 0.9044559597969055\n",
      "Epoch 443, Loss: 1.8748902678489685, Final Batch Loss: 0.49310463666915894\n",
      "Epoch 444, Loss: 1.7998580634593964, Final Batch Loss: 0.4695686399936676\n",
      "Epoch 445, Loss: 1.8664110898971558, Final Batch Loss: 0.5443002581596375\n",
      "Epoch 446, Loss: 2.0535516142845154, Final Batch Loss: 0.7391761541366577\n",
      "Epoch 447, Loss: 1.8518443703651428, Final Batch Loss: 0.5954425930976868\n",
      "Epoch 448, Loss: 2.050688683986664, Final Batch Loss: 0.6878331899642944\n",
      "Epoch 449, Loss: 1.8629297614097595, Final Batch Loss: 0.5801968574523926\n",
      "Epoch 450, Loss: 2.0069406032562256, Final Batch Loss: 0.6880183815956116\n",
      "Epoch 451, Loss: 1.7982168793678284, Final Batch Loss: 0.5944088101387024\n",
      "Epoch 452, Loss: 1.7888460755348206, Final Batch Loss: 0.5681135058403015\n",
      "Epoch 453, Loss: 1.7604734003543854, Final Batch Loss: 0.4946887791156769\n",
      "Epoch 454, Loss: 1.809852123260498, Final Batch Loss: 0.5241055488586426\n",
      "Epoch 455, Loss: 1.7695801854133606, Final Batch Loss: 0.5429262518882751\n",
      "Epoch 456, Loss: 1.8517891764640808, Final Batch Loss: 0.5241857767105103\n",
      "Epoch 457, Loss: 1.8008769154548645, Final Batch Loss: 0.5706995129585266\n",
      "Epoch 458, Loss: 1.9316123723983765, Final Batch Loss: 0.624435305595398\n",
      "Epoch 459, Loss: 1.9070261716842651, Final Batch Loss: 0.6541114449501038\n",
      "Epoch 460, Loss: 1.8630439639091492, Final Batch Loss: 0.5997755527496338\n",
      "Epoch 461, Loss: 1.7989546656608582, Final Batch Loss: 0.4936021566390991\n",
      "Epoch 462, Loss: 1.8557220101356506, Final Batch Loss: 0.567656934261322\n",
      "Epoch 463, Loss: 2.19872784614563, Final Batch Loss: 0.9258421063423157\n",
      "Epoch 464, Loss: 1.8879740238189697, Final Batch Loss: 0.5644089579582214\n",
      "Epoch 465, Loss: 1.9327319264411926, Final Batch Loss: 0.7688749432563782\n",
      "Epoch 466, Loss: 2.1425331830978394, Final Batch Loss: 0.8558315634727478\n",
      "Epoch 467, Loss: 2.0505003333091736, Final Batch Loss: 0.7873204350471497\n",
      "Epoch 468, Loss: 2.2832282185554504, Final Batch Loss: 0.9625819325447083\n",
      "Epoch 469, Loss: 2.171555519104004, Final Batch Loss: 0.8677774667739868\n",
      "Epoch 470, Loss: 1.9514060020446777, Final Batch Loss: 0.6030351519584656\n",
      "Epoch 471, Loss: 1.7244292795658112, Final Batch Loss: 0.47785434126853943\n",
      "Epoch 472, Loss: 1.7205542922019958, Final Batch Loss: 0.48035168647766113\n",
      "Epoch 473, Loss: 2.3196852803230286, Final Batch Loss: 1.122963547706604\n",
      "Epoch 474, Loss: 1.8336860537528992, Final Batch Loss: 0.49614018201828003\n",
      "Epoch 475, Loss: 1.8352922201156616, Final Batch Loss: 0.5897586941719055\n",
      "Epoch 476, Loss: 1.8359747529029846, Final Batch Loss: 0.6059356331825256\n",
      "Epoch 477, Loss: 2.2568430304527283, Final Batch Loss: 0.9347425699234009\n",
      "Epoch 478, Loss: 2.141019105911255, Final Batch Loss: 0.7947328686714172\n",
      "Epoch 479, Loss: 1.7404560148715973, Final Batch Loss: 0.46753421425819397\n",
      "Epoch 480, Loss: 1.9796403050422668, Final Batch Loss: 0.6360496282577515\n",
      "Epoch 481, Loss: 1.9710463881492615, Final Batch Loss: 0.7642027735710144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 482, Loss: 2.0395464301109314, Final Batch Loss: 0.7394116520881653\n",
      "Epoch 483, Loss: 1.8157429099082947, Final Batch Loss: 0.594957172870636\n",
      "Epoch 484, Loss: 1.8317511081695557, Final Batch Loss: 0.6016479730606079\n",
      "Epoch 485, Loss: 2.142364203929901, Final Batch Loss: 0.8083497881889343\n",
      "Epoch 486, Loss: 1.917198657989502, Final Batch Loss: 0.6853881478309631\n",
      "Epoch 487, Loss: 1.8265911936759949, Final Batch Loss: 0.660183310508728\n",
      "Epoch 488, Loss: 2.219134509563446, Final Batch Loss: 0.9851030111312866\n",
      "Epoch 489, Loss: 1.8089088797569275, Final Batch Loss: 0.5741105079650879\n",
      "Epoch 490, Loss: 1.896019160747528, Final Batch Loss: 0.6555838584899902\n",
      "Epoch 491, Loss: 2.1121652722358704, Final Batch Loss: 0.9024850726127625\n",
      "Epoch 492, Loss: 1.6129043996334076, Final Batch Loss: 0.42457345128059387\n",
      "Epoch 493, Loss: 1.6560169160366058, Final Batch Loss: 0.4028647840023041\n",
      "Epoch 494, Loss: 2.494306445121765, Final Batch Loss: 1.186206340789795\n",
      "Epoch 495, Loss: 1.9452713131904602, Final Batch Loss: 0.7110670208930969\n",
      "Epoch 496, Loss: 1.7984449863433838, Final Batch Loss: 0.5410079956054688\n",
      "Epoch 497, Loss: 2.2648428082466125, Final Batch Loss: 0.9923797249794006\n",
      "Epoch 498, Loss: 1.824277400970459, Final Batch Loss: 0.4857499599456787\n",
      "Epoch 499, Loss: 1.9250300526618958, Final Batch Loss: 0.6403835415840149\n",
      "Epoch 500, Loss: 1.9503366351127625, Final Batch Loss: 0.786154568195343\n",
      "Epoch 501, Loss: 1.9492433667182922, Final Batch Loss: 0.6602312326431274\n",
      "Epoch 502, Loss: 2.1667014360427856, Final Batch Loss: 0.9207972288131714\n",
      "Epoch 503, Loss: 1.9566640257835388, Final Batch Loss: 0.6731278300285339\n",
      "Epoch 504, Loss: 1.8205111026763916, Final Batch Loss: 0.5669698119163513\n",
      "Epoch 505, Loss: 2.0304070711135864, Final Batch Loss: 0.7607908844947815\n",
      "Epoch 506, Loss: 1.8131164908409119, Final Batch Loss: 0.5326732397079468\n",
      "Epoch 507, Loss: 2.2225162982940674, Final Batch Loss: 1.0577106475830078\n",
      "Epoch 508, Loss: 1.8399608135223389, Final Batch Loss: 0.6807956099510193\n",
      "Epoch 509, Loss: 1.8642461895942688, Final Batch Loss: 0.6432043313980103\n",
      "Epoch 510, Loss: 1.7669410705566406, Final Batch Loss: 0.5719983577728271\n",
      "Epoch 511, Loss: 1.9065886735916138, Final Batch Loss: 0.6889106035232544\n",
      "Epoch 512, Loss: 1.862110435962677, Final Batch Loss: 0.7107535600662231\n",
      "Epoch 513, Loss: 2.0607088804244995, Final Batch Loss: 0.960113525390625\n",
      "Epoch 514, Loss: 1.7754193544387817, Final Batch Loss: 0.4979207515716553\n",
      "Epoch 515, Loss: 1.7187714874744415, Final Batch Loss: 0.49048247933387756\n",
      "Epoch 516, Loss: 1.9247633814811707, Final Batch Loss: 0.6481096148490906\n",
      "Epoch 517, Loss: 1.7214917838573456, Final Batch Loss: 0.4986492097377777\n",
      "Epoch 518, Loss: 1.8659111857414246, Final Batch Loss: 0.6465829014778137\n",
      "Epoch 519, Loss: 1.7588461637496948, Final Batch Loss: 0.5766550898551941\n",
      "Epoch 520, Loss: 1.4980203807353973, Final Batch Loss: 0.3322022259235382\n",
      "Epoch 521, Loss: 1.7685661315917969, Final Batch Loss: 0.6308749914169312\n",
      "Epoch 522, Loss: 1.8130146861076355, Final Batch Loss: 0.5530847907066345\n",
      "Epoch 523, Loss: 2.0806323289871216, Final Batch Loss: 0.869840145111084\n",
      "Epoch 524, Loss: 1.9246963262557983, Final Batch Loss: 0.6637047529220581\n",
      "Epoch 525, Loss: 1.73150634765625, Final Batch Loss: 0.5674033761024475\n",
      "Epoch 526, Loss: 1.6836429834365845, Final Batch Loss: 0.4896659255027771\n",
      "Epoch 527, Loss: 1.7328004837036133, Final Batch Loss: 0.5332353115081787\n",
      "Epoch 528, Loss: 1.8996761441230774, Final Batch Loss: 0.6381956338882446\n",
      "Epoch 529, Loss: 1.6370328664779663, Final Batch Loss: 0.5373825430870056\n",
      "Epoch 530, Loss: 1.7897915840148926, Final Batch Loss: 0.6386615037918091\n",
      "Epoch 531, Loss: 1.7646341919898987, Final Batch Loss: 0.5740272402763367\n",
      "Epoch 532, Loss: 1.5822027623653412, Final Batch Loss: 0.4497723877429962\n",
      "Epoch 533, Loss: 2.0164989233016968, Final Batch Loss: 0.8186384439468384\n",
      "Epoch 534, Loss: 2.0114575624465942, Final Batch Loss: 0.951474666595459\n",
      "Epoch 535, Loss: 1.654871642589569, Final Batch Loss: 0.4344888925552368\n",
      "Epoch 536, Loss: 1.6408417522907257, Final Batch Loss: 0.3708135783672333\n",
      "Epoch 537, Loss: 2.2965914011001587, Final Batch Loss: 1.1151174306869507\n",
      "Epoch 538, Loss: 1.5644417703151703, Final Batch Loss: 0.37825706601142883\n",
      "Epoch 539, Loss: 1.8717884421348572, Final Batch Loss: 0.5868752002716064\n",
      "Epoch 540, Loss: 1.8015618920326233, Final Batch Loss: 0.5755054354667664\n",
      "Epoch 541, Loss: 1.5482047200202942, Final Batch Loss: 0.42357850074768066\n",
      "Epoch 542, Loss: 1.9062637090682983, Final Batch Loss: 0.7045953273773193\n",
      "Epoch 543, Loss: 1.6308500170707703, Final Batch Loss: 0.45669931173324585\n",
      "Epoch 544, Loss: 1.8137144446372986, Final Batch Loss: 0.5896298885345459\n",
      "Epoch 545, Loss: 1.6098772585391998, Final Batch Loss: 0.43395718932151794\n",
      "Epoch 546, Loss: 1.7696438431739807, Final Batch Loss: 0.5098524689674377\n",
      "Epoch 547, Loss: 1.7971471548080444, Final Batch Loss: 0.6362583041191101\n",
      "Epoch 548, Loss: 1.76437109708786, Final Batch Loss: 0.5433758497238159\n",
      "Epoch 549, Loss: 1.5725137591362, Final Batch Loss: 0.4157456159591675\n",
      "Epoch 550, Loss: 1.8954654932022095, Final Batch Loss: 0.7321636080741882\n",
      "Epoch 551, Loss: 1.5022827088832855, Final Batch Loss: 0.36560097336769104\n",
      "Epoch 552, Loss: 1.5032916069030762, Final Batch Loss: 0.37640273571014404\n",
      "Epoch 553, Loss: 1.6719737946987152, Final Batch Loss: 0.4127630293369293\n",
      "Epoch 554, Loss: 1.529693216085434, Final Batch Loss: 0.3670036494731903\n",
      "Epoch 555, Loss: 1.708413541316986, Final Batch Loss: 0.5257378816604614\n",
      "Epoch 556, Loss: 1.8959365487098694, Final Batch Loss: 0.7079663872718811\n",
      "Epoch 557, Loss: 1.9090266823768616, Final Batch Loss: 0.7480486035346985\n",
      "Epoch 558, Loss: 1.5584389567375183, Final Batch Loss: 0.4104417562484741\n",
      "Epoch 559, Loss: 1.4857588410377502, Final Batch Loss: 0.36290377378463745\n",
      "Epoch 560, Loss: 1.7004472017288208, Final Batch Loss: 0.49167531728744507\n",
      "Epoch 561, Loss: 1.7563591599464417, Final Batch Loss: 0.6663252711296082\n",
      "Epoch 562, Loss: 1.8089015483856201, Final Batch Loss: 0.6780921816825867\n",
      "Epoch 563, Loss: 1.516556292772293, Final Batch Loss: 0.39052000641822815\n",
      "Epoch 564, Loss: 1.599500060081482, Final Batch Loss: 0.5233003497123718\n",
      "Epoch 565, Loss: 1.8161847591400146, Final Batch Loss: 0.6378596425056458\n",
      "Epoch 566, Loss: 1.6278404593467712, Final Batch Loss: 0.5016882419586182\n",
      "Epoch 567, Loss: 1.6735183596611023, Final Batch Loss: 0.5881699323654175\n",
      "Epoch 568, Loss: 1.871727466583252, Final Batch Loss: 0.6816022992134094\n",
      "Epoch 569, Loss: 1.7748398780822754, Final Batch Loss: 0.5965008735656738\n",
      "Epoch 570, Loss: 1.7964988350868225, Final Batch Loss: 0.6069989204406738\n",
      "Epoch 571, Loss: 1.6050856709480286, Final Batch Loss: 0.5205857753753662\n",
      "Epoch 572, Loss: 1.7768092155456543, Final Batch Loss: 0.642423152923584\n",
      "Epoch 573, Loss: 1.5737261176109314, Final Batch Loss: 0.38785725831985474\n",
      "Epoch 574, Loss: 1.7183274626731873, Final Batch Loss: 0.528053343296051\n",
      "Epoch 575, Loss: 1.7662715911865234, Final Batch Loss: 0.582971453666687\n",
      "Epoch 576, Loss: 2.021414577960968, Final Batch Loss: 0.7292402386665344\n",
      "Epoch 577, Loss: 1.5298261046409607, Final Batch Loss: 0.39979320764541626\n",
      "Epoch 578, Loss: 2.1344898343086243, Final Batch Loss: 0.8512893319129944\n",
      "Epoch 579, Loss: 1.6326160728931427, Final Batch Loss: 0.5430964231491089\n",
      "Epoch 580, Loss: 1.7244319021701813, Final Batch Loss: 0.48999038338661194\n",
      "Epoch 581, Loss: 1.6416906714439392, Final Batch Loss: 0.4778515696525574\n",
      "Epoch 582, Loss: 1.6962801218032837, Final Batch Loss: 0.519934356212616\n",
      "Epoch 583, Loss: 1.8029248714447021, Final Batch Loss: 0.7074504494667053\n",
      "Epoch 584, Loss: 1.5891041159629822, Final Batch Loss: 0.503502607345581\n",
      "Epoch 585, Loss: 1.4996545910835266, Final Batch Loss: 0.3288688659667969\n",
      "Epoch 586, Loss: 1.8457483649253845, Final Batch Loss: 0.6425687074661255\n",
      "Epoch 587, Loss: 1.7796338200569153, Final Batch Loss: 0.5472413897514343\n",
      "Epoch 588, Loss: 1.7479471564292908, Final Batch Loss: 0.6568008661270142\n",
      "Epoch 589, Loss: 1.7601131200790405, Final Batch Loss: 0.6149443984031677\n",
      "Epoch 590, Loss: 1.613663136959076, Final Batch Loss: 0.5185789465904236\n",
      "Epoch 591, Loss: 1.5918807089328766, Final Batch Loss: 0.4433639943599701\n",
      "Epoch 592, Loss: 1.5512167811393738, Final Batch Loss: 0.4371350407600403\n",
      "Epoch 593, Loss: 1.414722591638565, Final Batch Loss: 0.3548344671726227\n",
      "Epoch 594, Loss: 1.7569146752357483, Final Batch Loss: 0.627282440662384\n",
      "Epoch 595, Loss: 1.781120777130127, Final Batch Loss: 0.6589359045028687\n",
      "Epoch 596, Loss: 1.4669204652309418, Final Batch Loss: 0.2977618873119354\n",
      "Epoch 597, Loss: 1.8560895919799805, Final Batch Loss: 0.6612515449523926\n",
      "Epoch 598, Loss: 1.6108344793319702, Final Batch Loss: 0.4482332468032837\n",
      "Epoch 599, Loss: 1.5053561627864838, Final Batch Loss: 0.37391558289527893\n",
      "Epoch 600, Loss: 1.5642336010932922, Final Batch Loss: 0.44320136308670044\n",
      "Epoch 601, Loss: 1.9372795224189758, Final Batch Loss: 0.8018434643745422\n",
      "Epoch 602, Loss: 1.4142034649848938, Final Batch Loss: 0.3168933391571045\n",
      "Epoch 603, Loss: 1.903223693370819, Final Batch Loss: 0.8070892691612244\n",
      "Epoch 604, Loss: 1.8183923363685608, Final Batch Loss: 0.7159262299537659\n",
      "Epoch 605, Loss: 1.7935659885406494, Final Batch Loss: 0.6464544534683228\n",
      "Epoch 606, Loss: 1.5555427074432373, Final Batch Loss: 0.45230644941329956\n",
      "Epoch 607, Loss: 1.5532782971858978, Final Batch Loss: 0.43129613995552063\n",
      "Epoch 608, Loss: 1.712724208831787, Final Batch Loss: 0.5838942527770996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 609, Loss: 1.8287878036499023, Final Batch Loss: 0.695806086063385\n",
      "Epoch 610, Loss: 1.5817356705665588, Final Batch Loss: 0.48889291286468506\n",
      "Epoch 611, Loss: 1.5323029458522797, Final Batch Loss: 0.3920856714248657\n",
      "Epoch 612, Loss: 1.9243125915527344, Final Batch Loss: 0.8367767333984375\n",
      "Epoch 613, Loss: 1.6985580325126648, Final Batch Loss: 0.5478437542915344\n",
      "Epoch 614, Loss: 1.6969197988510132, Final Batch Loss: 0.6158385872840881\n",
      "Epoch 615, Loss: 1.7336793839931488, Final Batch Loss: 0.49132201075553894\n",
      "Epoch 616, Loss: 1.583376407623291, Final Batch Loss: 0.47017186880111694\n",
      "Epoch 617, Loss: 1.4979506731033325, Final Batch Loss: 0.2823983430862427\n",
      "Epoch 618, Loss: 1.5014329552650452, Final Batch Loss: 0.43215978145599365\n",
      "Epoch 619, Loss: 1.5380346179008484, Final Batch Loss: 0.4023205637931824\n",
      "Epoch 620, Loss: 1.336203157901764, Final Batch Loss: 0.23602449893951416\n",
      "Epoch 621, Loss: 1.582093894481659, Final Batch Loss: 0.4903782308101654\n",
      "Epoch 622, Loss: 1.7185442447662354, Final Batch Loss: 0.5944371819496155\n",
      "Epoch 623, Loss: 1.623684138059616, Final Batch Loss: 0.4740987718105316\n",
      "Epoch 624, Loss: 1.7778834104537964, Final Batch Loss: 0.632515013217926\n",
      "Epoch 625, Loss: 1.6640772223472595, Final Batch Loss: 0.5439714789390564\n",
      "Epoch 626, Loss: 1.9719120860099792, Final Batch Loss: 0.8367785215377808\n",
      "Epoch 627, Loss: 1.6343282461166382, Final Batch Loss: 0.4908931255340576\n",
      "Epoch 628, Loss: 1.9694628715515137, Final Batch Loss: 0.8524975180625916\n",
      "Epoch 629, Loss: 1.8179570436477661, Final Batch Loss: 0.6819170117378235\n",
      "Epoch 630, Loss: 1.7580685019493103, Final Batch Loss: 0.6470223069190979\n",
      "Epoch 631, Loss: 1.5308702886104584, Final Batch Loss: 0.37262484431266785\n",
      "Epoch 632, Loss: 1.7189770340919495, Final Batch Loss: 0.6265120506286621\n",
      "Epoch 633, Loss: 1.611558437347412, Final Batch Loss: 0.4519783854484558\n",
      "Epoch 634, Loss: 1.6119740903377533, Final Batch Loss: 0.47287288308143616\n",
      "Epoch 635, Loss: 2.0466240644454956, Final Batch Loss: 0.9452030062675476\n",
      "Epoch 636, Loss: 1.6413640975952148, Final Batch Loss: 0.4909299612045288\n",
      "Epoch 637, Loss: 1.662431001663208, Final Batch Loss: 0.5743829607963562\n",
      "Epoch 638, Loss: 1.5768617391586304, Final Batch Loss: 0.4596773386001587\n",
      "Epoch 639, Loss: 1.710324466228485, Final Batch Loss: 0.5858029127120972\n",
      "Epoch 640, Loss: 1.6588937044143677, Final Batch Loss: 0.5401952862739563\n",
      "Epoch 641, Loss: 1.8722966015338898, Final Batch Loss: 0.865323007106781\n",
      "Epoch 642, Loss: 1.4908747673034668, Final Batch Loss: 0.38178426027297974\n",
      "Epoch 643, Loss: 1.6010574698448181, Final Batch Loss: 0.5100643038749695\n",
      "Epoch 644, Loss: 1.778393030166626, Final Batch Loss: 0.6986041069030762\n",
      "Epoch 645, Loss: 1.9067569375038147, Final Batch Loss: 0.7028970718383789\n",
      "Epoch 646, Loss: 1.4476874768733978, Final Batch Loss: 0.3957538604736328\n",
      "Epoch 647, Loss: 1.580430805683136, Final Batch Loss: 0.5181431770324707\n",
      "Epoch 648, Loss: 1.6522348523139954, Final Batch Loss: 0.5713956952095032\n",
      "Epoch 649, Loss: 1.519710659980774, Final Batch Loss: 0.4298703074455261\n",
      "Epoch 650, Loss: 1.5432958602905273, Final Batch Loss: 0.4768073856830597\n",
      "Epoch 651, Loss: 1.7760627269744873, Final Batch Loss: 0.6747277975082397\n",
      "Epoch 652, Loss: 1.4357053637504578, Final Batch Loss: 0.34664005041122437\n",
      "Epoch 653, Loss: 1.5397265255451202, Final Batch Loss: 0.5029388070106506\n",
      "Epoch 654, Loss: 1.4790910482406616, Final Batch Loss: 0.4271624684333801\n",
      "Epoch 655, Loss: 1.7761068940162659, Final Batch Loss: 0.714728832244873\n",
      "Epoch 656, Loss: 1.4223669469356537, Final Batch Loss: 0.27968987822532654\n",
      "Epoch 657, Loss: 1.5848352015018463, Final Batch Loss: 0.515479564666748\n",
      "Epoch 658, Loss: 1.5646655857563019, Final Batch Loss: 0.39911743998527527\n",
      "Epoch 659, Loss: 1.8286317586898804, Final Batch Loss: 0.7047813534736633\n",
      "Epoch 660, Loss: 1.7370569705963135, Final Batch Loss: 0.6566414833068848\n",
      "Epoch 661, Loss: 1.696239948272705, Final Batch Loss: 0.6205449104309082\n",
      "Epoch 662, Loss: 1.5829326510429382, Final Batch Loss: 0.5255926847457886\n",
      "Epoch 663, Loss: 1.5513717234134674, Final Batch Loss: 0.47782716155052185\n",
      "Epoch 664, Loss: 1.3947374522686005, Final Batch Loss: 0.42307332158088684\n",
      "Epoch 665, Loss: 1.743509829044342, Final Batch Loss: 0.6282760500907898\n",
      "Epoch 666, Loss: 1.7687943875789642, Final Batch Loss: 0.6559708118438721\n",
      "Epoch 667, Loss: 1.6227898597717285, Final Batch Loss: 0.5444185733795166\n",
      "Epoch 668, Loss: 1.4895814955234528, Final Batch Loss: 0.4261709451675415\n",
      "Epoch 669, Loss: 1.2911287248134613, Final Batch Loss: 0.1884985864162445\n",
      "Epoch 670, Loss: 1.63468736410141, Final Batch Loss: 0.5922330617904663\n",
      "Epoch 671, Loss: 1.627688467502594, Final Batch Loss: 0.5244195461273193\n",
      "Epoch 672, Loss: 1.6662373542785645, Final Batch Loss: 0.5875056385993958\n",
      "Epoch 673, Loss: 1.4655419886112213, Final Batch Loss: 0.3176191747188568\n",
      "Epoch 674, Loss: 1.63589209318161, Final Batch Loss: 0.5145472884178162\n",
      "Epoch 675, Loss: 1.562085121870041, Final Batch Loss: 0.4942115843296051\n",
      "Epoch 676, Loss: 1.717763364315033, Final Batch Loss: 0.6303160190582275\n",
      "Epoch 677, Loss: 1.66558837890625, Final Batch Loss: 0.6616144180297852\n",
      "Epoch 678, Loss: 1.578859955072403, Final Batch Loss: 0.45113328099250793\n",
      "Epoch 679, Loss: 1.473966807126999, Final Batch Loss: 0.38218268752098083\n",
      "Epoch 680, Loss: 1.44854736328125, Final Batch Loss: 0.3995159864425659\n",
      "Epoch 681, Loss: 1.6594762802124023, Final Batch Loss: 0.5273495316505432\n",
      "Epoch 682, Loss: 1.647606521844864, Final Batch Loss: 0.6564376950263977\n",
      "Epoch 683, Loss: 1.3696064949035645, Final Batch Loss: 0.26178187131881714\n",
      "Epoch 684, Loss: 1.7036890387535095, Final Batch Loss: 0.5656083226203918\n",
      "Epoch 685, Loss: 1.895165115594864, Final Batch Loss: 0.9635342955589294\n",
      "Epoch 686, Loss: 1.4275811612606049, Final Batch Loss: 0.3439078629016876\n",
      "Epoch 687, Loss: 1.6077956557273865, Final Batch Loss: 0.5298529267311096\n",
      "Epoch 688, Loss: 1.715864360332489, Final Batch Loss: 0.70554119348526\n",
      "Epoch 689, Loss: 1.5667962431907654, Final Batch Loss: 0.5596314072608948\n",
      "Epoch 690, Loss: 1.549650490283966, Final Batch Loss: 0.4625280797481537\n",
      "Epoch 691, Loss: 1.566084623336792, Final Batch Loss: 0.4954741597175598\n",
      "Epoch 692, Loss: 1.746342658996582, Final Batch Loss: 0.7148007750511169\n",
      "Epoch 693, Loss: 1.635496973991394, Final Batch Loss: 0.6229597330093384\n",
      "Epoch 694, Loss: 1.222859650850296, Final Batch Loss: 0.15118348598480225\n",
      "Epoch 695, Loss: 1.8116728067398071, Final Batch Loss: 0.6900215148925781\n",
      "Epoch 696, Loss: 1.5874239802360535, Final Batch Loss: 0.5465840101242065\n",
      "Epoch 697, Loss: 1.516320526599884, Final Batch Loss: 0.4266461730003357\n",
      "Epoch 698, Loss: 1.9344367384910583, Final Batch Loss: 0.9028587341308594\n",
      "Epoch 699, Loss: 1.5560667216777802, Final Batch Loss: 0.40569058060646057\n",
      "Epoch 700, Loss: 1.5073028802871704, Final Batch Loss: 0.4679027795791626\n",
      "Epoch 701, Loss: 1.5382547676563263, Final Batch Loss: 0.5159710645675659\n",
      "Epoch 702, Loss: 1.7841735482215881, Final Batch Loss: 0.6793400049209595\n",
      "Epoch 703, Loss: 1.4765708148479462, Final Batch Loss: 0.3547672927379608\n",
      "Epoch 704, Loss: 1.7414445281028748, Final Batch Loss: 0.63515305519104\n",
      "Epoch 705, Loss: 1.766996443271637, Final Batch Loss: 0.598450243473053\n",
      "Epoch 706, Loss: 1.9091125428676605, Final Batch Loss: 0.8025150299072266\n",
      "Epoch 707, Loss: 1.544541209936142, Final Batch Loss: 0.35371509194374084\n",
      "Epoch 708, Loss: 1.7161824405193329, Final Batch Loss: 0.6818917393684387\n",
      "Epoch 709, Loss: 1.5638858377933502, Final Batch Loss: 0.3953551948070526\n",
      "Epoch 710, Loss: 1.4985679686069489, Final Batch Loss: 0.44965147972106934\n",
      "Epoch 711, Loss: 1.4344624280929565, Final Batch Loss: 0.4268799424171448\n",
      "Epoch 712, Loss: 1.7490531206130981, Final Batch Loss: 0.6599798798561096\n",
      "Epoch 713, Loss: 1.4066536128520966, Final Batch Loss: 0.3801146447658539\n",
      "Epoch 714, Loss: 1.7276323735713959, Final Batch Loss: 0.7390637397766113\n",
      "Epoch 715, Loss: 1.8033223152160645, Final Batch Loss: 0.6663002967834473\n",
      "Epoch 716, Loss: 1.4765790402889252, Final Batch Loss: 0.406029611825943\n",
      "Epoch 717, Loss: 1.6787588000297546, Final Batch Loss: 0.620907187461853\n",
      "Epoch 718, Loss: 1.405150443315506, Final Batch Loss: 0.32069095969200134\n",
      "Epoch 719, Loss: 1.523117870092392, Final Batch Loss: 0.5475281476974487\n",
      "Epoch 720, Loss: 1.392088532447815, Final Batch Loss: 0.3626684844493866\n",
      "Epoch 721, Loss: 1.7829254865646362, Final Batch Loss: 0.6790560483932495\n",
      "Epoch 722, Loss: 1.751920223236084, Final Batch Loss: 0.701994001865387\n",
      "Epoch 723, Loss: 1.5756961703300476, Final Batch Loss: 0.5710170269012451\n",
      "Epoch 724, Loss: 1.467868447303772, Final Batch Loss: 0.46135640144348145\n",
      "Epoch 725, Loss: 1.674991637468338, Final Batch Loss: 0.6285533308982849\n",
      "Epoch 726, Loss: 1.8754885792732239, Final Batch Loss: 0.7932766079902649\n",
      "Epoch 727, Loss: 1.4461195766925812, Final Batch Loss: 0.36857208609580994\n",
      "Epoch 728, Loss: 1.426614761352539, Final Batch Loss: 0.35210373997688293\n",
      "Epoch 729, Loss: 1.3542876839637756, Final Batch Loss: 0.2871391773223877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 730, Loss: 1.694777399301529, Final Batch Loss: 0.6352721452713013\n",
      "Epoch 731, Loss: 1.555340439081192, Final Batch Loss: 0.5500826239585876\n",
      "Epoch 732, Loss: 1.6515734791755676, Final Batch Loss: 0.6226084232330322\n",
      "Epoch 733, Loss: 1.5145678222179413, Final Batch Loss: 0.45839396119117737\n",
      "Epoch 734, Loss: 1.3041505217552185, Final Batch Loss: 0.3309305012226105\n",
      "Epoch 735, Loss: 1.6241253018379211, Final Batch Loss: 0.5774451494216919\n",
      "Epoch 736, Loss: 1.6061492562294006, Final Batch Loss: 0.4497816562652588\n",
      "Epoch 737, Loss: 1.49633127450943, Final Batch Loss: 0.4707714319229126\n",
      "Epoch 738, Loss: 1.2911200821399689, Final Batch Loss: 0.33358508348464966\n",
      "Epoch 739, Loss: 1.8983872532844543, Final Batch Loss: 0.891606867313385\n",
      "Epoch 740, Loss: 1.493775486946106, Final Batch Loss: 0.45789995789527893\n",
      "Epoch 741, Loss: 1.6515634059906006, Final Batch Loss: 0.6569679379463196\n",
      "Epoch 742, Loss: 1.3964650630950928, Final Batch Loss: 0.3763352334499359\n",
      "Epoch 743, Loss: 1.6358415484428406, Final Batch Loss: 0.5853120684623718\n",
      "Epoch 744, Loss: 1.356914758682251, Final Batch Loss: 0.3093677759170532\n",
      "Epoch 745, Loss: 1.4960223138332367, Final Batch Loss: 0.5080794095993042\n",
      "Epoch 746, Loss: 1.3352343440055847, Final Batch Loss: 0.2698532044887543\n",
      "Epoch 747, Loss: 1.2492796927690506, Final Batch Loss: 0.2223832756280899\n",
      "Epoch 748, Loss: 1.6267866790294647, Final Batch Loss: 0.5877900123596191\n",
      "Epoch 749, Loss: 1.5570368468761444, Final Batch Loss: 0.4935988485813141\n",
      "Epoch 750, Loss: 1.4799770712852478, Final Batch Loss: 0.43538153171539307\n",
      "Epoch 751, Loss: 1.3580791354179382, Final Batch Loss: 0.2527122497558594\n",
      "Epoch 752, Loss: 1.4781320691108704, Final Batch Loss: 0.4954453110694885\n",
      "Epoch 753, Loss: 1.2997196316719055, Final Batch Loss: 0.28633204102516174\n",
      "Epoch 754, Loss: 1.7115334272384644, Final Batch Loss: 0.6467947959899902\n",
      "Epoch 755, Loss: 1.7563353180885315, Final Batch Loss: 0.6776029467582703\n",
      "Epoch 756, Loss: 1.7726970613002777, Final Batch Loss: 0.6318265199661255\n",
      "Epoch 757, Loss: 1.7330102324485779, Final Batch Loss: 0.6641860604286194\n",
      "Epoch 758, Loss: 1.6334685683250427, Final Batch Loss: 0.536780834197998\n",
      "Epoch 759, Loss: 1.6219279766082764, Final Batch Loss: 0.6181320548057556\n",
      "Epoch 760, Loss: 1.6154495179653168, Final Batch Loss: 0.681008517742157\n",
      "Epoch 761, Loss: 1.6549092233181, Final Batch Loss: 0.6531148552894592\n",
      "Epoch 762, Loss: 1.8456518054008484, Final Batch Loss: 0.8446711301803589\n",
      "Epoch 763, Loss: 1.3518404066562653, Final Batch Loss: 0.34807249903678894\n",
      "Epoch 764, Loss: 1.565621018409729, Final Batch Loss: 0.4342276453971863\n",
      "Epoch 765, Loss: 1.5912836790084839, Final Batch Loss: 0.5931476950645447\n",
      "Epoch 766, Loss: 1.682870090007782, Final Batch Loss: 0.6580589413642883\n",
      "Epoch 767, Loss: 1.5771896839141846, Final Batch Loss: 0.549617350101471\n",
      "Epoch 768, Loss: 1.488383412361145, Final Batch Loss: 0.4891194701194763\n",
      "Epoch 769, Loss: 1.6967541575431824, Final Batch Loss: 0.7193640470504761\n",
      "Epoch 770, Loss: 1.4520245790481567, Final Batch Loss: 0.4662533700466156\n",
      "Epoch 771, Loss: 1.87732994556427, Final Batch Loss: 0.783603310585022\n",
      "Epoch 772, Loss: 1.492015928030014, Final Batch Loss: 0.5290488600730896\n",
      "Epoch 773, Loss: 1.642105609178543, Final Batch Loss: 0.6517637372016907\n",
      "Epoch 774, Loss: 1.7285072803497314, Final Batch Loss: 0.7512920498847961\n",
      "Epoch 775, Loss: 1.5144749879837036, Final Batch Loss: 0.5072024464607239\n",
      "Epoch 776, Loss: 1.5866242051124573, Final Batch Loss: 0.5676271319389343\n",
      "Epoch 777, Loss: 1.7680450677871704, Final Batch Loss: 0.7098255753517151\n",
      "Epoch 778, Loss: 1.5169085264205933, Final Batch Loss: 0.47300851345062256\n",
      "Epoch 779, Loss: 1.2535437643527985, Final Batch Loss: 0.34177789092063904\n",
      "Epoch 780, Loss: 1.6206228137016296, Final Batch Loss: 0.6677679419517517\n",
      "Epoch 781, Loss: 1.6293561160564423, Final Batch Loss: 0.6309043765068054\n",
      "Epoch 782, Loss: 1.32024747133255, Final Batch Loss: 0.32815712690353394\n",
      "Epoch 783, Loss: 1.693262130022049, Final Batch Loss: 0.6745826601982117\n",
      "Epoch 784, Loss: 1.4596756398677826, Final Batch Loss: 0.3885060250759125\n",
      "Epoch 785, Loss: 1.3974820375442505, Final Batch Loss: 0.35913896560668945\n",
      "Epoch 786, Loss: 1.4769298136234283, Final Batch Loss: 0.45412173867225647\n",
      "Epoch 787, Loss: 1.6672242283821106, Final Batch Loss: 0.6600072383880615\n",
      "Epoch 788, Loss: 1.6047653257846832, Final Batch Loss: 0.6340476870536804\n",
      "Epoch 789, Loss: 1.492226004600525, Final Batch Loss: 0.46115559339523315\n",
      "Epoch 790, Loss: 1.516247570514679, Final Batch Loss: 0.5310081839561462\n",
      "Epoch 791, Loss: 1.4212447106838226, Final Batch Loss: 0.48693692684173584\n",
      "Epoch 792, Loss: 1.7721205949783325, Final Batch Loss: 0.6615918278694153\n",
      "Epoch 793, Loss: 1.619019329547882, Final Batch Loss: 0.6229304075241089\n",
      "Epoch 794, Loss: 1.3095831274986267, Final Batch Loss: 0.33534687757492065\n",
      "Epoch 795, Loss: 1.4899874925613403, Final Batch Loss: 0.45675602555274963\n",
      "Epoch 796, Loss: 1.5269810557365417, Final Batch Loss: 0.4879891276359558\n",
      "Epoch 797, Loss: 1.4865498840808868, Final Batch Loss: 0.4290120601654053\n",
      "Epoch 798, Loss: 1.3755144476890564, Final Batch Loss: 0.40941572189331055\n",
      "Epoch 799, Loss: 1.4668236076831818, Final Batch Loss: 0.4505062699317932\n",
      "Epoch 800, Loss: 1.700190544128418, Final Batch Loss: 0.7147784233093262\n",
      "Epoch 801, Loss: 1.4892137050628662, Final Batch Loss: 0.4990449845790863\n",
      "Epoch 802, Loss: 1.5073820650577545, Final Batch Loss: 0.5373215079307556\n",
      "Epoch 803, Loss: 1.644762933254242, Final Batch Loss: 0.6270081996917725\n",
      "Epoch 804, Loss: 1.8215433955192566, Final Batch Loss: 0.884873628616333\n",
      "Epoch 805, Loss: 1.588646113872528, Final Batch Loss: 0.6233761310577393\n",
      "Epoch 806, Loss: 1.5699120461940765, Final Batch Loss: 0.574126124382019\n",
      "Epoch 807, Loss: 1.4900100529193878, Final Batch Loss: 0.5459044575691223\n",
      "Epoch 808, Loss: 1.6566585302352905, Final Batch Loss: 0.6581647992134094\n",
      "Epoch 809, Loss: 1.2556648254394531, Final Batch Loss: 0.3122618794441223\n",
      "Epoch 810, Loss: 1.5749342143535614, Final Batch Loss: 0.4812236726284027\n",
      "Epoch 811, Loss: 1.3700456023216248, Final Batch Loss: 0.3499218821525574\n",
      "Epoch 812, Loss: 1.8214772045612335, Final Batch Loss: 0.8507207632064819\n",
      "Epoch 813, Loss: 1.3673127889633179, Final Batch Loss: 0.3941796123981476\n",
      "Epoch 814, Loss: 1.683982789516449, Final Batch Loss: 0.6004727482795715\n",
      "Epoch 815, Loss: 1.2810487151145935, Final Batch Loss: 0.28423529863357544\n",
      "Epoch 816, Loss: 1.5043367743492126, Final Batch Loss: 0.5354183316230774\n",
      "Epoch 817, Loss: 1.5703154802322388, Final Batch Loss: 0.5538129210472107\n",
      "Epoch 818, Loss: 1.6816082000732422, Final Batch Loss: 0.6504025459289551\n",
      "Epoch 819, Loss: 1.6394095718860626, Final Batch Loss: 0.6973977088928223\n",
      "Epoch 820, Loss: 1.2194808721542358, Final Batch Loss: 0.24308615922927856\n",
      "Epoch 821, Loss: 1.4062775373458862, Final Batch Loss: 0.4452309310436249\n",
      "Epoch 822, Loss: 1.3695865869522095, Final Batch Loss: 0.21372878551483154\n",
      "Epoch 823, Loss: 1.5415865182876587, Final Batch Loss: 0.5696396827697754\n",
      "Epoch 824, Loss: 1.1991568952798843, Final Batch Loss: 0.18231429159641266\n",
      "Epoch 825, Loss: 1.2345875203609467, Final Batch Loss: 0.2807777523994446\n",
      "Epoch 826, Loss: 1.4252780079841614, Final Batch Loss: 0.5049933791160583\n",
      "Epoch 827, Loss: 1.5312415063381195, Final Batch Loss: 0.5744101405143738\n",
      "Epoch 828, Loss: 1.2060849070549011, Final Batch Loss: 0.27732938528060913\n",
      "Epoch 829, Loss: 1.346131980419159, Final Batch Loss: 0.3038747012615204\n",
      "Epoch 830, Loss: 1.3207169473171234, Final Batch Loss: 0.2832402288913727\n",
      "Epoch 831, Loss: 1.4610335230827332, Final Batch Loss: 0.4273580312728882\n",
      "Epoch 832, Loss: 1.5574305951595306, Final Batch Loss: 0.5836836695671082\n",
      "Epoch 833, Loss: 1.421831488609314, Final Batch Loss: 0.5265921354293823\n",
      "Epoch 834, Loss: 1.50152787566185, Final Batch Loss: 0.5322611927986145\n",
      "Epoch 835, Loss: 1.520760327577591, Final Batch Loss: 0.5644572973251343\n",
      "Epoch 836, Loss: 1.4875860214233398, Final Batch Loss: 0.5457212328910828\n",
      "Epoch 837, Loss: 1.404161423444748, Final Batch Loss: 0.49950262904167175\n",
      "Epoch 838, Loss: 1.5046195685863495, Final Batch Loss: 0.5005523562431335\n",
      "Epoch 839, Loss: 1.3604423999786377, Final Batch Loss: 0.45214447379112244\n",
      "Epoch 840, Loss: 1.3949875831604004, Final Batch Loss: 0.4224029779434204\n",
      "Epoch 841, Loss: 1.688620150089264, Final Batch Loss: 0.7153997421264648\n",
      "Epoch 842, Loss: 1.540724903345108, Final Batch Loss: 0.5569201707839966\n",
      "Epoch 843, Loss: 1.380893886089325, Final Batch Loss: 0.4094867408275604\n",
      "Epoch 844, Loss: 1.5621978044509888, Final Batch Loss: 0.5881879925727844\n",
      "Epoch 845, Loss: 1.4609349071979523, Final Batch Loss: 0.4180353283882141\n",
      "Epoch 846, Loss: 1.6107004284858704, Final Batch Loss: 0.5563324093818665\n",
      "Epoch 847, Loss: 1.533263385295868, Final Batch Loss: 0.48531121015548706\n",
      "Epoch 848, Loss: 1.7258004546165466, Final Batch Loss: 0.7977549433708191\n",
      "Epoch 849, Loss: 1.4689655005931854, Final Batch Loss: 0.48318561911582947\n",
      "Epoch 850, Loss: 1.7235222458839417, Final Batch Loss: 0.6500915884971619\n",
      "Epoch 851, Loss: 1.551314115524292, Final Batch Loss: 0.5792499780654907\n",
      "Epoch 852, Loss: 1.304354965686798, Final Batch Loss: 0.388924777507782\n",
      "Epoch 853, Loss: 1.3107602298259735, Final Batch Loss: 0.3364575505256653\n",
      "Epoch 854, Loss: 1.4139881432056427, Final Batch Loss: 0.4927917420864105\n",
      "Epoch 855, Loss: 1.2918994724750519, Final Batch Loss: 0.3658125698566437\n",
      "Epoch 856, Loss: 1.1083647459745407, Final Batch Loss: 0.19322623312473297\n",
      "Epoch 857, Loss: 1.2918302118778229, Final Batch Loss: 0.3055989742279053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 858, Loss: 1.3720582723617554, Final Batch Loss: 0.3816010653972626\n",
      "Epoch 859, Loss: 1.4727087020874023, Final Batch Loss: 0.5346220135688782\n",
      "Epoch 860, Loss: 1.8544379472732544, Final Batch Loss: 0.9150375723838806\n",
      "Epoch 861, Loss: 1.61845064163208, Final Batch Loss: 0.6637498736381531\n",
      "Epoch 862, Loss: 1.4439089000225067, Final Batch Loss: 0.466403603553772\n",
      "Epoch 863, Loss: 1.5414122641086578, Final Batch Loss: 0.5631377696990967\n",
      "Epoch 864, Loss: 1.3867526352405548, Final Batch Loss: 0.3687504529953003\n",
      "Epoch 865, Loss: 1.7593646943569183, Final Batch Loss: 0.7563484311103821\n",
      "Epoch 866, Loss: 1.4595817923545837, Final Batch Loss: 0.5122589468955994\n",
      "Epoch 867, Loss: 1.3742220997810364, Final Batch Loss: 0.3739510476589203\n",
      "Epoch 868, Loss: 1.390528380870819, Final Batch Loss: 0.44268807768821716\n",
      "Epoch 869, Loss: 1.317745327949524, Final Batch Loss: 0.38086092472076416\n",
      "Epoch 870, Loss: 1.4070000052452087, Final Batch Loss: 0.47470414638519287\n",
      "Epoch 871, Loss: 1.328448325395584, Final Batch Loss: 0.33309802412986755\n",
      "Epoch 872, Loss: 1.8548651337623596, Final Batch Loss: 0.8764336109161377\n",
      "Epoch 873, Loss: 1.489084154367447, Final Batch Loss: 0.5438757538795471\n",
      "Epoch 874, Loss: 1.4654269516468048, Final Batch Loss: 0.40733835101127625\n",
      "Epoch 875, Loss: 1.4750877022743225, Final Batch Loss: 0.5608744621276855\n",
      "Epoch 876, Loss: 1.3557099103927612, Final Batch Loss: 0.4522794783115387\n",
      "Epoch 877, Loss: 1.3620002269744873, Final Batch Loss: 0.4477444291114807\n",
      "Epoch 878, Loss: 1.4391820132732391, Final Batch Loss: 0.4662700593471527\n",
      "Epoch 879, Loss: 1.657221794128418, Final Batch Loss: 0.7239893674850464\n",
      "Epoch 880, Loss: 1.2833784818649292, Final Batch Loss: 0.3924848139286041\n",
      "Epoch 881, Loss: 1.5635404288768768, Final Batch Loss: 0.6630333065986633\n",
      "Epoch 882, Loss: 1.3184444606304169, Final Batch Loss: 0.33752933144569397\n",
      "Epoch 883, Loss: 1.284435123205185, Final Batch Loss: 0.3271869719028473\n",
      "Epoch 884, Loss: 1.608963817358017, Final Batch Loss: 0.657010555267334\n",
      "Epoch 885, Loss: 1.7716160714626312, Final Batch Loss: 0.7952558398246765\n",
      "Epoch 886, Loss: 1.413580983877182, Final Batch Loss: 0.4858104884624481\n",
      "Epoch 887, Loss: 1.5680677592754364, Final Batch Loss: 0.6912407875061035\n",
      "Epoch 888, Loss: 1.5815520286560059, Final Batch Loss: 0.6227880716323853\n",
      "Epoch 889, Loss: 1.2956041097640991, Final Batch Loss: 0.3524397611618042\n",
      "Epoch 890, Loss: 1.5207462906837463, Final Batch Loss: 0.5507066249847412\n",
      "Epoch 891, Loss: 1.3485195338726044, Final Batch Loss: 0.3926733136177063\n",
      "Epoch 892, Loss: 1.5402150452136993, Final Batch Loss: 0.5972777605056763\n",
      "Epoch 893, Loss: 1.3821540772914886, Final Batch Loss: 0.40195679664611816\n",
      "Epoch 894, Loss: 1.5598039031028748, Final Batch Loss: 0.5514929890632629\n",
      "Epoch 895, Loss: 1.6120035946369171, Final Batch Loss: 0.6321704387664795\n",
      "Epoch 896, Loss: 1.3243065774440765, Final Batch Loss: 0.3877323269844055\n",
      "Epoch 897, Loss: 1.2246992588043213, Final Batch Loss: 0.2996371388435364\n",
      "Epoch 898, Loss: 1.3249877393245697, Final Batch Loss: 0.4030531346797943\n",
      "Epoch 899, Loss: 1.4224316775798798, Final Batch Loss: 0.4846007823944092\n",
      "Epoch 900, Loss: 1.4365726709365845, Final Batch Loss: 0.5371799468994141\n",
      "Epoch 901, Loss: 1.4488383829593658, Final Batch Loss: 0.48469191789627075\n",
      "Epoch 902, Loss: 1.4144070148468018, Final Batch Loss: 0.39602041244506836\n",
      "Epoch 903, Loss: 1.3074997663497925, Final Batch Loss: 0.2942289113998413\n",
      "Epoch 904, Loss: 1.528952270746231, Final Batch Loss: 0.5430930256843567\n",
      "Epoch 905, Loss: 1.4733611643314362, Final Batch Loss: 0.5369714498519897\n",
      "Epoch 906, Loss: 1.3326425552368164, Final Batch Loss: 0.36268872022628784\n",
      "Epoch 907, Loss: 1.291157066822052, Final Batch Loss: 0.42018991708755493\n",
      "Epoch 908, Loss: 1.4229347109794617, Final Batch Loss: 0.4778977930545807\n",
      "Epoch 909, Loss: 1.4636749625205994, Final Batch Loss: 0.5606436729431152\n",
      "Epoch 910, Loss: 1.3624117374420166, Final Batch Loss: 0.4456784725189209\n",
      "Epoch 911, Loss: 1.3128697276115417, Final Batch Loss: 0.33031725883483887\n",
      "Epoch 912, Loss: 1.2984660863876343, Final Batch Loss: 0.3624633848667145\n",
      "Epoch 913, Loss: 1.4403950572013855, Final Batch Loss: 0.49587783217430115\n",
      "Epoch 914, Loss: 1.2761953473091125, Final Batch Loss: 0.400966078042984\n",
      "Epoch 915, Loss: 1.3043496012687683, Final Batch Loss: 0.43906205892562866\n",
      "Epoch 916, Loss: 1.3655369281768799, Final Batch Loss: 0.40538015961647034\n",
      "Epoch 917, Loss: 1.3929996490478516, Final Batch Loss: 0.4682047367095947\n",
      "Epoch 918, Loss: 1.2523100674152374, Final Batch Loss: 0.3516504764556885\n",
      "Epoch 919, Loss: 1.1451828181743622, Final Batch Loss: 0.2648487389087677\n",
      "Epoch 920, Loss: 1.5548983812332153, Final Batch Loss: 0.5877247452735901\n",
      "Epoch 921, Loss: 1.5216735303401947, Final Batch Loss: 0.5697120428085327\n",
      "Epoch 922, Loss: 1.479502648115158, Final Batch Loss: 0.4261460602283478\n",
      "Epoch 923, Loss: 1.3102790415287018, Final Batch Loss: 0.41085711121559143\n",
      "Epoch 924, Loss: 1.461693823337555, Final Batch Loss: 0.564856767654419\n",
      "Epoch 925, Loss: 1.4762468039989471, Final Batch Loss: 0.5191710591316223\n",
      "Epoch 926, Loss: 1.310018390417099, Final Batch Loss: 0.341365784406662\n",
      "Epoch 927, Loss: 1.3582825064659119, Final Batch Loss: 0.33854272961616516\n",
      "Epoch 928, Loss: 1.6036337614059448, Final Batch Loss: 0.6970000267028809\n",
      "Epoch 929, Loss: 1.3800994455814362, Final Batch Loss: 0.49451160430908203\n",
      "Epoch 930, Loss: 1.4922772347927094, Final Batch Loss: 0.5025058388710022\n",
      "Epoch 931, Loss: 1.3848975002765656, Final Batch Loss: 0.4347812235355377\n",
      "Epoch 932, Loss: 1.342319905757904, Final Batch Loss: 0.37788841128349304\n",
      "Epoch 933, Loss: 1.4489935636520386, Final Batch Loss: 0.487579345703125\n",
      "Epoch 934, Loss: 1.2172963321208954, Final Batch Loss: 0.2582158148288727\n",
      "Epoch 935, Loss: 1.4603402316570282, Final Batch Loss: 0.5417046546936035\n",
      "Epoch 936, Loss: 1.314588338136673, Final Batch Loss: 0.43742260336875916\n",
      "Epoch 937, Loss: 1.3672556579113007, Final Batch Loss: 0.49252796173095703\n",
      "Epoch 938, Loss: 1.2189401388168335, Final Batch Loss: 0.28260812163352966\n",
      "Epoch 939, Loss: 1.6164056360721588, Final Batch Loss: 0.6954411268234253\n",
      "Epoch 940, Loss: 1.2785966396331787, Final Batch Loss: 0.4065243899822235\n",
      "Epoch 941, Loss: 1.2960071861743927, Final Batch Loss: 0.37509894371032715\n",
      "Epoch 942, Loss: 1.3791234195232391, Final Batch Loss: 0.468203604221344\n",
      "Epoch 943, Loss: 1.4116814136505127, Final Batch Loss: 0.4451272487640381\n",
      "Epoch 944, Loss: 1.0861135870218277, Final Batch Loss: 0.2374451607465744\n",
      "Epoch 945, Loss: 1.4506705403327942, Final Batch Loss: 0.5041438937187195\n",
      "Epoch 946, Loss: 1.1015036404132843, Final Batch Loss: 0.21234428882598877\n",
      "Epoch 947, Loss: 1.2005327939987183, Final Batch Loss: 0.2647685408592224\n",
      "Epoch 948, Loss: 1.2995880544185638, Final Batch Loss: 0.3785015940666199\n",
      "Epoch 949, Loss: 1.3408126533031464, Final Batch Loss: 0.38418254256248474\n",
      "Epoch 950, Loss: 1.31043741106987, Final Batch Loss: 0.4125930964946747\n",
      "Epoch 951, Loss: 1.6117810308933258, Final Batch Loss: 0.6813340187072754\n",
      "Epoch 952, Loss: 1.4372956454753876, Final Batch Loss: 0.5104231834411621\n",
      "Epoch 953, Loss: 1.4353851675987244, Final Batch Loss: 0.6104187369346619\n",
      "Epoch 954, Loss: 1.1521191447973251, Final Batch Loss: 0.22489051520824432\n",
      "Epoch 955, Loss: 1.2405501306056976, Final Batch Loss: 0.35923975706100464\n",
      "Epoch 956, Loss: 1.0809374898672104, Final Batch Loss: 0.18022699654102325\n",
      "Epoch 957, Loss: 1.2463029623031616, Final Batch Loss: 0.3886723518371582\n",
      "Epoch 958, Loss: 1.4264767467975616, Final Batch Loss: 0.5619139075279236\n",
      "Epoch 959, Loss: 1.296446532011032, Final Batch Loss: 0.39933836460113525\n",
      "Epoch 960, Loss: 1.260399729013443, Final Batch Loss: 0.3777174949645996\n",
      "Epoch 961, Loss: 1.3243898749351501, Final Batch Loss: 0.4877219498157501\n",
      "Epoch 962, Loss: 1.371077835559845, Final Batch Loss: 0.4732040464878082\n",
      "Epoch 963, Loss: 1.4840112626552582, Final Batch Loss: 0.580847442150116\n",
      "Epoch 964, Loss: 1.3910106122493744, Final Batch Loss: 0.44749316573143005\n",
      "Epoch 965, Loss: 1.3631936311721802, Final Batch Loss: 0.4736490845680237\n",
      "Epoch 966, Loss: 1.2386398017406464, Final Batch Loss: 0.33487218618392944\n",
      "Epoch 967, Loss: 1.437940090894699, Final Batch Loss: 0.5416933298110962\n",
      "Epoch 968, Loss: 1.5524051189422607, Final Batch Loss: 0.6070011258125305\n",
      "Epoch 969, Loss: 1.159640610218048, Final Batch Loss: 0.3555927872657776\n",
      "Epoch 970, Loss: 1.385466307401657, Final Batch Loss: 0.4430241286754608\n",
      "Epoch 971, Loss: 1.4186997413635254, Final Batch Loss: 0.4576737880706787\n",
      "Epoch 972, Loss: 1.1941209435462952, Final Batch Loss: 0.34466731548309326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 973, Loss: 1.392437607049942, Final Batch Loss: 0.5332587957382202\n",
      "Epoch 974, Loss: 1.3357490599155426, Final Batch Loss: 0.43150606751441956\n",
      "Epoch 975, Loss: 1.3109604120254517, Final Batch Loss: 0.467013955116272\n",
      "Epoch 976, Loss: 1.7304259538650513, Final Batch Loss: 0.8549128174781799\n",
      "Epoch 977, Loss: 1.3499277532100677, Final Batch Loss: 0.4616186320781708\n",
      "Epoch 978, Loss: 1.2345487177371979, Final Batch Loss: 0.338696151971817\n",
      "Epoch 979, Loss: 1.4261857867240906, Final Batch Loss: 0.5615670084953308\n",
      "Epoch 980, Loss: 1.2051132321357727, Final Batch Loss: 0.2888028621673584\n",
      "Epoch 981, Loss: 1.4877110421657562, Final Batch Loss: 0.5977770090103149\n",
      "Epoch 982, Loss: 1.527113139629364, Final Batch Loss: 0.5952335596084595\n",
      "Epoch 983, Loss: 1.3453243970870972, Final Batch Loss: 0.4978675842285156\n",
      "Epoch 984, Loss: 1.3012065589427948, Final Batch Loss: 0.3713393807411194\n",
      "Epoch 985, Loss: 1.3842520713806152, Final Batch Loss: 0.451695054769516\n",
      "Epoch 986, Loss: 1.2956447005271912, Final Batch Loss: 0.38172009587287903\n",
      "Epoch 987, Loss: 1.4213781952857971, Final Batch Loss: 0.4739128053188324\n",
      "Epoch 988, Loss: 1.479597419500351, Final Batch Loss: 0.6078291535377502\n",
      "Epoch 989, Loss: 1.2582585215568542, Final Batch Loss: 0.33290091156959534\n",
      "Epoch 990, Loss: 1.3004652857780457, Final Batch Loss: 0.48159369826316833\n",
      "Epoch 991, Loss: 1.7732637226581573, Final Batch Loss: 0.8893859386444092\n",
      "Epoch 992, Loss: 1.5604105293750763, Final Batch Loss: 0.5950844883918762\n",
      "Epoch 993, Loss: 1.3019169867038727, Final Batch Loss: 0.3643958270549774\n",
      "Epoch 994, Loss: 1.5725704729557037, Final Batch Loss: 0.5890870094299316\n",
      "Epoch 995, Loss: 1.309887856245041, Final Batch Loss: 0.35275864601135254\n",
      "Epoch 996, Loss: 1.6275244653224945, Final Batch Loss: 0.6212400794029236\n",
      "Epoch 997, Loss: 1.4006484150886536, Final Batch Loss: 0.44694679975509644\n",
      "Epoch 998, Loss: 1.5689137876033783, Final Batch Loss: 0.6671125292778015\n",
      "Epoch 999, Loss: 1.1323473751544952, Final Batch Loss: 0.21903002262115479\n",
      "Epoch 1000, Loss: 1.3337410688400269, Final Batch Loss: 0.33038559556007385\n",
      "Epoch 1001, Loss: 1.3372011482715607, Final Batch Loss: 0.46776479482650757\n",
      "Epoch 1002, Loss: 1.357304573059082, Final Batch Loss: 0.43239784240722656\n",
      "Epoch 1003, Loss: 1.442144364118576, Final Batch Loss: 0.5854271650314331\n",
      "Epoch 1004, Loss: 1.420246809720993, Final Batch Loss: 0.5653956532478333\n",
      "Epoch 1005, Loss: 1.2124948799610138, Final Batch Loss: 0.3361993134021759\n",
      "Epoch 1006, Loss: 1.1726590991020203, Final Batch Loss: 0.2564721703529358\n",
      "Epoch 1007, Loss: 1.5786974430084229, Final Batch Loss: 0.7139720916748047\n",
      "Epoch 1008, Loss: 1.615872472524643, Final Batch Loss: 0.7134903073310852\n",
      "Epoch 1009, Loss: 1.2238164246082306, Final Batch Loss: 0.3272864818572998\n",
      "Epoch 1010, Loss: 1.0124984681606293, Final Batch Loss: 0.1033068597316742\n",
      "Epoch 1011, Loss: 1.202512949705124, Final Batch Loss: 0.2777091860771179\n",
      "Epoch 1012, Loss: 1.3039062917232513, Final Batch Loss: 0.5039685964584351\n",
      "Epoch 1013, Loss: 1.3200694620609283, Final Batch Loss: 0.43154260516166687\n",
      "Epoch 1014, Loss: 1.1796824634075165, Final Batch Loss: 0.31675276160240173\n",
      "Epoch 1015, Loss: 1.163574904203415, Final Batch Loss: 0.2682522237300873\n",
      "Epoch 1016, Loss: 1.2907284796237946, Final Batch Loss: 0.41956987977027893\n",
      "Epoch 1017, Loss: 1.4651689529418945, Final Batch Loss: 0.5122514963150024\n",
      "Epoch 1018, Loss: 1.0372944325208664, Final Batch Loss: 0.15785710513591766\n",
      "Epoch 1019, Loss: 1.4018889665603638, Final Batch Loss: 0.5023683905601501\n",
      "Epoch 1020, Loss: 1.1489462852478027, Final Batch Loss: 0.2313089668750763\n",
      "Epoch 1021, Loss: 1.0923313200473785, Final Batch Loss: 0.24688559770584106\n",
      "Epoch 1022, Loss: 1.434850424528122, Final Batch Loss: 0.5132332444190979\n",
      "Epoch 1023, Loss: 1.0704410374164581, Final Batch Loss: 0.2705164849758148\n",
      "Epoch 1024, Loss: 1.2601150870323181, Final Batch Loss: 0.3502662181854248\n",
      "Epoch 1025, Loss: 1.2986310124397278, Final Batch Loss: 0.4647887945175171\n",
      "Epoch 1026, Loss: 1.4441660940647125, Final Batch Loss: 0.5560697317123413\n",
      "Epoch 1027, Loss: 1.174140602350235, Final Batch Loss: 0.2554892301559448\n",
      "Epoch 1028, Loss: 1.2193522155284882, Final Batch Loss: 0.3777443468570709\n",
      "Epoch 1029, Loss: 1.1317002475261688, Final Batch Loss: 0.3319168984889984\n",
      "Epoch 1030, Loss: 1.2738274931907654, Final Batch Loss: 0.3809976279735565\n",
      "Epoch 1031, Loss: 1.2598365545272827, Final Batch Loss: 0.3683152198791504\n",
      "Epoch 1032, Loss: 1.2631743550300598, Final Batch Loss: 0.3893236219882965\n",
      "Epoch 1033, Loss: 1.338446468114853, Final Batch Loss: 0.43607065081596375\n",
      "Epoch 1034, Loss: 1.328244537115097, Final Batch Loss: 0.34841132164001465\n",
      "Epoch 1035, Loss: 1.3683243691921234, Final Batch Loss: 0.5097507238388062\n",
      "Epoch 1036, Loss: 1.40313258767128, Final Batch Loss: 0.47091352939605713\n",
      "Epoch 1037, Loss: 1.314681500196457, Final Batch Loss: 0.45120278000831604\n",
      "Epoch 1038, Loss: 1.354532152414322, Final Batch Loss: 0.45850667357444763\n",
      "Epoch 1039, Loss: 1.197367936372757, Final Batch Loss: 0.34183207154273987\n",
      "Epoch 1040, Loss: 1.404432326555252, Final Batch Loss: 0.5198770761489868\n",
      "Epoch 1041, Loss: 1.300880789756775, Final Batch Loss: 0.3003561496734619\n",
      "Epoch 1042, Loss: 1.4330262541770935, Final Batch Loss: 0.547399640083313\n",
      "Epoch 1043, Loss: 1.6081417202949524, Final Batch Loss: 0.7120156288146973\n",
      "Epoch 1044, Loss: 1.4567663371562958, Final Batch Loss: 0.5514329075813293\n",
      "Epoch 1045, Loss: 1.2967403829097748, Final Batch Loss: 0.4860917627811432\n",
      "Epoch 1046, Loss: 1.4354730248451233, Final Batch Loss: 0.6162554621696472\n",
      "Epoch 1047, Loss: 1.5613373517990112, Final Batch Loss: 0.6997842788696289\n",
      "Epoch 1048, Loss: 1.1885677576065063, Final Batch Loss: 0.30352044105529785\n",
      "Epoch 1049, Loss: 1.627640426158905, Final Batch Loss: 0.6843169927597046\n",
      "Epoch 1050, Loss: 1.2797748446464539, Final Batch Loss: 0.38242948055267334\n",
      "Epoch 1051, Loss: 1.2624668180942535, Final Batch Loss: 0.45144426822662354\n",
      "Epoch 1052, Loss: 1.3700379133224487, Final Batch Loss: 0.4538421630859375\n",
      "Epoch 1053, Loss: 1.2931427359580994, Final Batch Loss: 0.3931591808795929\n",
      "Epoch 1054, Loss: 1.0974412858486176, Final Batch Loss: 0.24575263261795044\n",
      "Epoch 1055, Loss: 1.1106974482536316, Final Batch Loss: 0.2979833781719208\n",
      "Epoch 1056, Loss: 1.4882685542106628, Final Batch Loss: 0.6623901724815369\n",
      "Epoch 1057, Loss: 1.3036034107208252, Final Batch Loss: 0.4786001443862915\n",
      "Epoch 1058, Loss: 1.4331148564815521, Final Batch Loss: 0.535502552986145\n",
      "Epoch 1059, Loss: 1.3458694517612457, Final Batch Loss: 0.4308093786239624\n",
      "Epoch 1060, Loss: 1.4195370078086853, Final Batch Loss: 0.5231064558029175\n",
      "Epoch 1061, Loss: 1.179792881011963, Final Batch Loss: 0.31124648451805115\n",
      "Epoch 1062, Loss: 1.2584509253501892, Final Batch Loss: 0.2970769703388214\n",
      "Epoch 1063, Loss: 1.3354196548461914, Final Batch Loss: 0.4429471790790558\n",
      "Epoch 1064, Loss: 1.3108403086662292, Final Batch Loss: 0.46665963530540466\n",
      "Epoch 1065, Loss: 1.2862222492694855, Final Batch Loss: 0.43230414390563965\n",
      "Epoch 1066, Loss: 1.4610049426555634, Final Batch Loss: 0.6208224296569824\n",
      "Epoch 1067, Loss: 1.1854808628559113, Final Batch Loss: 0.343563050031662\n",
      "Epoch 1068, Loss: 1.2515359222888947, Final Batch Loss: 0.33671805262565613\n",
      "Epoch 1069, Loss: 1.1101669669151306, Final Batch Loss: 0.29117727279663086\n",
      "Epoch 1070, Loss: 1.138982117176056, Final Batch Loss: 0.3327987492084503\n",
      "Epoch 1071, Loss: 1.2585347592830658, Final Batch Loss: 0.33325397968292236\n",
      "Epoch 1072, Loss: 1.1791426837444305, Final Batch Loss: 0.3571434020996094\n",
      "Epoch 1073, Loss: 1.1834198236465454, Final Batch Loss: 0.34893476963043213\n",
      "Epoch 1074, Loss: 1.5293648540973663, Final Batch Loss: 0.6972336769104004\n",
      "Epoch 1075, Loss: 1.183018147945404, Final Batch Loss: 0.3363604247570038\n",
      "Epoch 1076, Loss: 1.3441585004329681, Final Batch Loss: 0.4178524911403656\n",
      "Epoch 1077, Loss: 1.3018679320812225, Final Batch Loss: 0.42376774549484253\n",
      "Epoch 1078, Loss: 1.205260992050171, Final Batch Loss: 0.3213418126106262\n",
      "Epoch 1079, Loss: 1.488230288028717, Final Batch Loss: 0.5380385518074036\n",
      "Epoch 1080, Loss: 1.4084046483039856, Final Batch Loss: 0.5875471234321594\n",
      "Epoch 1081, Loss: 1.2734616696834564, Final Batch Loss: 0.4218178689479828\n",
      "Epoch 1082, Loss: 1.2076237499713898, Final Batch Loss: 0.3535274267196655\n",
      "Epoch 1083, Loss: 1.3944397866725922, Final Batch Loss: 0.5629917979240417\n",
      "Epoch 1084, Loss: 1.1534227132797241, Final Batch Loss: 0.3610784709453583\n",
      "Epoch 1085, Loss: 1.2060259580612183, Final Batch Loss: 0.35003137588500977\n",
      "Epoch 1086, Loss: 1.1610473990440369, Final Batch Loss: 0.3254776895046234\n",
      "Epoch 1087, Loss: 1.3887350857257843, Final Batch Loss: 0.49594536423683167\n",
      "Epoch 1088, Loss: 1.188370794057846, Final Batch Loss: 0.3261149525642395\n",
      "Epoch 1089, Loss: 1.2998414933681488, Final Batch Loss: 0.4632949233055115\n",
      "Epoch 1090, Loss: 0.9311043545603752, Final Batch Loss: 0.0922921821475029\n",
      "Epoch 1091, Loss: 1.3557544946670532, Final Batch Loss: 0.4627488851547241\n",
      "Epoch 1092, Loss: 1.3897153735160828, Final Batch Loss: 0.5771340727806091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1093, Loss: 1.3526756763458252, Final Batch Loss: 0.5256412625312805\n",
      "Epoch 1094, Loss: 1.1642618477344513, Final Batch Loss: 0.382880836725235\n",
      "Epoch 1095, Loss: 1.2718059420585632, Final Batch Loss: 0.3897516131401062\n",
      "Epoch 1096, Loss: 1.2866661846637726, Final Batch Loss: 0.42108213901519775\n",
      "Epoch 1097, Loss: 1.577552080154419, Final Batch Loss: 0.692218005657196\n",
      "Epoch 1098, Loss: 1.0949735790491104, Final Batch Loss: 0.20314355194568634\n",
      "Epoch 1099, Loss: 1.2216786444187164, Final Batch Loss: 0.38353976607322693\n",
      "Epoch 1100, Loss: 1.3228722214698792, Final Batch Loss: 0.47072944045066833\n",
      "Epoch 1101, Loss: 1.1646257638931274, Final Batch Loss: 0.3199082612991333\n",
      "Epoch 1102, Loss: 1.3542966842651367, Final Batch Loss: 0.4640105962753296\n",
      "Epoch 1103, Loss: 1.0792827010154724, Final Batch Loss: 0.23732507228851318\n",
      "Epoch 1104, Loss: 1.3085879385471344, Final Batch Loss: 0.48297208547592163\n",
      "Epoch 1105, Loss: 1.340076357126236, Final Batch Loss: 0.4422571659088135\n",
      "Epoch 1106, Loss: 1.0548336803913116, Final Batch Loss: 0.24052348732948303\n",
      "Epoch 1107, Loss: 1.4432567059993744, Final Batch Loss: 0.5645899772644043\n",
      "Epoch 1108, Loss: 1.4163383841514587, Final Batch Loss: 0.5743288993835449\n",
      "Epoch 1109, Loss: 1.133982539176941, Final Batch Loss: 0.28394338488578796\n",
      "Epoch 1110, Loss: 1.1277732253074646, Final Batch Loss: 0.29615724086761475\n",
      "Epoch 1111, Loss: 1.2626273930072784, Final Batch Loss: 0.4211944341659546\n",
      "Epoch 1112, Loss: 1.2274903059005737, Final Batch Loss: 0.4556904435157776\n",
      "Epoch 1113, Loss: 1.1714409291744232, Final Batch Loss: 0.2673994302749634\n",
      "Epoch 1114, Loss: 1.0595405548810959, Final Batch Loss: 0.24900300800800323\n",
      "Epoch 1115, Loss: 1.2640196979045868, Final Batch Loss: 0.48585614562034607\n",
      "Epoch 1116, Loss: 1.1590628027915955, Final Batch Loss: 0.3177977204322815\n",
      "Epoch 1117, Loss: 1.0847921669483185, Final Batch Loss: 0.2577785551548004\n",
      "Epoch 1118, Loss: 1.270917683839798, Final Batch Loss: 0.40096235275268555\n",
      "Epoch 1119, Loss: 1.064041018486023, Final Batch Loss: 0.26488304138183594\n",
      "Epoch 1120, Loss: 1.1183792650699615, Final Batch Loss: 0.255316823720932\n",
      "Epoch 1121, Loss: 1.2306397557258606, Final Batch Loss: 0.388396292924881\n",
      "Epoch 1122, Loss: 1.251442551612854, Final Batch Loss: 0.4525485932826996\n",
      "Epoch 1123, Loss: 1.153201401233673, Final Batch Loss: 0.30715471506118774\n",
      "Epoch 1124, Loss: 1.222863346338272, Final Batch Loss: 0.28611084818840027\n",
      "Epoch 1125, Loss: 1.2674463391304016, Final Batch Loss: 0.41941362619400024\n",
      "Epoch 1126, Loss: 1.2931104600429535, Final Batch Loss: 0.4908808171749115\n",
      "Epoch 1127, Loss: 1.1078838109970093, Final Batch Loss: 0.24267399311065674\n",
      "Epoch 1128, Loss: 1.1516197323799133, Final Batch Loss: 0.3053124248981476\n",
      "Epoch 1129, Loss: 1.2229089736938477, Final Batch Loss: 0.3791242241859436\n",
      "Epoch 1130, Loss: 1.6467507779598236, Final Batch Loss: 0.8296378254890442\n",
      "Epoch 1131, Loss: 1.1533534228801727, Final Batch Loss: 0.3096214830875397\n",
      "Epoch 1132, Loss: 1.5155887305736542, Final Batch Loss: 0.5723411440849304\n",
      "Epoch 1133, Loss: 1.2401647865772247, Final Batch Loss: 0.4418638050556183\n",
      "Epoch 1134, Loss: 1.497967392206192, Final Batch Loss: 0.6626501679420471\n",
      "Epoch 1135, Loss: 1.1997160017490387, Final Batch Loss: 0.38779622316360474\n",
      "Epoch 1136, Loss: 1.467596411705017, Final Batch Loss: 0.6090867519378662\n",
      "Epoch 1137, Loss: 1.153732568025589, Final Batch Loss: 0.27702268958091736\n",
      "Epoch 1138, Loss: 1.2734605371952057, Final Batch Loss: 0.45545336604118347\n",
      "Epoch 1139, Loss: 1.485905408859253, Final Batch Loss: 0.6840704083442688\n",
      "Epoch 1140, Loss: 1.1558886766433716, Final Batch Loss: 0.3405335247516632\n",
      "Epoch 1141, Loss: 1.163793683052063, Final Batch Loss: 0.38815629482269287\n",
      "Epoch 1142, Loss: 1.3857924044132233, Final Batch Loss: 0.5274119973182678\n",
      "Epoch 1143, Loss: 1.1333876252174377, Final Batch Loss: 0.2877834737300873\n",
      "Epoch 1144, Loss: 1.435375154018402, Final Batch Loss: 0.6419433951377869\n",
      "Epoch 1145, Loss: 1.2759220600128174, Final Batch Loss: 0.44367313385009766\n",
      "Epoch 1146, Loss: 1.2327759265899658, Final Batch Loss: 0.3911649286746979\n",
      "Epoch 1147, Loss: 1.237086445093155, Final Batch Loss: 0.4070090353488922\n",
      "Epoch 1148, Loss: 1.1024767458438873, Final Batch Loss: 0.3517363667488098\n",
      "Epoch 1149, Loss: 1.1632418930530548, Final Batch Loss: 0.34051769971847534\n",
      "Epoch 1150, Loss: 1.3444900214672089, Final Batch Loss: 0.5145192742347717\n",
      "Epoch 1151, Loss: 1.1483695209026337, Final Batch Loss: 0.27240556478500366\n",
      "Epoch 1152, Loss: 1.151516079902649, Final Batch Loss: 0.3280446529388428\n",
      "Epoch 1153, Loss: 1.23941171169281, Final Batch Loss: 0.4465867578983307\n",
      "Epoch 1154, Loss: 1.2325429916381836, Final Batch Loss: 0.4082004725933075\n",
      "Epoch 1155, Loss: 1.0111666172742844, Final Batch Loss: 0.2327985018491745\n",
      "Epoch 1156, Loss: 0.8954149484634399, Final Batch Loss: 0.11631837487220764\n",
      "Epoch 1157, Loss: 1.7268150448799133, Final Batch Loss: 0.966161847114563\n",
      "Epoch 1158, Loss: 1.2371917366981506, Final Batch Loss: 0.45543208718299866\n",
      "Epoch 1159, Loss: 1.2866550087928772, Final Batch Loss: 0.4842114746570587\n",
      "Epoch 1160, Loss: 1.2562692761421204, Final Batch Loss: 0.39274436235427856\n",
      "Epoch 1161, Loss: 1.3797930181026459, Final Batch Loss: 0.46982845664024353\n",
      "Epoch 1162, Loss: 1.4022917747497559, Final Batch Loss: 0.6050549745559692\n",
      "Epoch 1163, Loss: 1.242568165063858, Final Batch Loss: 0.4237888753414154\n",
      "Epoch 1164, Loss: 1.2630162239074707, Final Batch Loss: 0.3745311498641968\n",
      "Epoch 1165, Loss: 1.0527220964431763, Final Batch Loss: 0.2586660385131836\n",
      "Epoch 1166, Loss: 1.3397633731365204, Final Batch Loss: 0.5056004524230957\n",
      "Epoch 1167, Loss: 1.319238007068634, Final Batch Loss: 0.5157170295715332\n",
      "Epoch 1168, Loss: 1.2138711810112, Final Batch Loss: 0.33793577551841736\n",
      "Epoch 1169, Loss: 1.153577744960785, Final Batch Loss: 0.2866133749485016\n",
      "Epoch 1170, Loss: 1.5253699719905853, Final Batch Loss: 0.7537574172019958\n",
      "Epoch 1171, Loss: 1.2805355489253998, Final Batch Loss: 0.48665040731430054\n",
      "Epoch 1172, Loss: 0.9924967736005783, Final Batch Loss: 0.18459443747997284\n",
      "Epoch 1173, Loss: 1.1074762642383575, Final Batch Loss: 0.2705927789211273\n",
      "Epoch 1174, Loss: 1.1038355827331543, Final Batch Loss: 0.35980066657066345\n",
      "Epoch 1175, Loss: 1.172207623720169, Final Batch Loss: 0.3493137061595917\n",
      "Epoch 1176, Loss: 1.2922938466072083, Final Batch Loss: 0.44892024993896484\n",
      "Epoch 1177, Loss: 1.098346471786499, Final Batch Loss: 0.26204514503479004\n",
      "Epoch 1178, Loss: 1.1020548045635223, Final Batch Loss: 0.25934115052223206\n",
      "Epoch 1179, Loss: 1.1994665265083313, Final Batch Loss: 0.40520134568214417\n",
      "Epoch 1180, Loss: 1.2260919213294983, Final Batch Loss: 0.4629020690917969\n",
      "Epoch 1181, Loss: 1.0884899199008942, Final Batch Loss: 0.342818945646286\n",
      "Epoch 1182, Loss: 1.120645821094513, Final Batch Loss: 0.3990705907344818\n",
      "Epoch 1183, Loss: 1.3651775419712067, Final Batch Loss: 0.6157016754150391\n",
      "Epoch 1184, Loss: 1.1979567408561707, Final Batch Loss: 0.3913652002811432\n",
      "Epoch 1185, Loss: 1.158872812986374, Final Batch Loss: 0.3416961133480072\n",
      "Epoch 1186, Loss: 1.0295689404010773, Final Batch Loss: 0.18977326154708862\n",
      "Epoch 1187, Loss: 1.058459609746933, Final Batch Loss: 0.2935580015182495\n",
      "Epoch 1188, Loss: 1.2664254903793335, Final Batch Loss: 0.3828272521495819\n",
      "Epoch 1189, Loss: 1.2528528273105621, Final Batch Loss: 0.5199138522148132\n",
      "Epoch 1190, Loss: 1.1770418286323547, Final Batch Loss: 0.3675701916217804\n",
      "Epoch 1191, Loss: 1.2607640326023102, Final Batch Loss: 0.5167995691299438\n",
      "Epoch 1192, Loss: 1.1293087303638458, Final Batch Loss: 0.3312845230102539\n",
      "Epoch 1193, Loss: 1.0560393333435059, Final Batch Loss: 0.2550245225429535\n",
      "Epoch 1194, Loss: 1.1871112883090973, Final Batch Loss: 0.3880261778831482\n",
      "Epoch 1195, Loss: 1.2447691559791565, Final Batch Loss: 0.43392297625541687\n",
      "Epoch 1196, Loss: 1.1480381488800049, Final Batch Loss: 0.35513606667518616\n",
      "Epoch 1197, Loss: 1.1473968923091888, Final Batch Loss: 0.4102326035499573\n",
      "Epoch 1198, Loss: 0.9541365653276443, Final Batch Loss: 0.1933077722787857\n",
      "Epoch 1199, Loss: 1.435426414012909, Final Batch Loss: 0.6835710406303406\n",
      "Epoch 1200, Loss: 1.1137568950653076, Final Batch Loss: 0.3115398585796356\n",
      "Epoch 1201, Loss: 0.99741992354393, Final Batch Loss: 0.24384605884552002\n",
      "Epoch 1202, Loss: 1.1046113073825836, Final Batch Loss: 0.30082693696022034\n",
      "Epoch 1203, Loss: 1.1288147270679474, Final Batch Loss: 0.3882332742214203\n",
      "Epoch 1204, Loss: 1.1466810405254364, Final Batch Loss: 0.27910953760147095\n",
      "Epoch 1205, Loss: 1.32746160030365, Final Batch Loss: 0.49319472908973694\n",
      "Epoch 1206, Loss: 1.2486744821071625, Final Batch Loss: 0.46126872301101685\n",
      "Epoch 1207, Loss: 1.0602720081806183, Final Batch Loss: 0.3018158972263336\n",
      "Epoch 1208, Loss: 1.0251973867416382, Final Batch Loss: 0.2790165841579437\n",
      "Epoch 1209, Loss: 1.1041843891143799, Final Batch Loss: 0.30467191338539124\n",
      "Epoch 1210, Loss: 0.9956239610910416, Final Batch Loss: 0.2196745127439499\n",
      "Epoch 1211, Loss: 1.0965392887592316, Final Batch Loss: 0.3002057373523712\n",
      "Epoch 1212, Loss: 1.0047059655189514, Final Batch Loss: 0.2787691056728363\n",
      "Epoch 1213, Loss: 0.964031308889389, Final Batch Loss: 0.22991293668746948\n",
      "Epoch 1214, Loss: 1.2547517120838165, Final Batch Loss: 0.47715798020362854\n",
      "Epoch 1215, Loss: 1.267717868089676, Final Batch Loss: 0.45328837633132935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1216, Loss: 1.0681704878807068, Final Batch Loss: 0.33621180057525635\n",
      "Epoch 1217, Loss: 1.3925766944885254, Final Batch Loss: 0.5990433692932129\n",
      "Epoch 1218, Loss: 1.0765989422798157, Final Batch Loss: 0.3019552230834961\n",
      "Epoch 1219, Loss: 0.987938866019249, Final Batch Loss: 0.20452041923999786\n",
      "Epoch 1220, Loss: 1.5122277438640594, Final Batch Loss: 0.7350663542747498\n",
      "Epoch 1221, Loss: 0.9425078630447388, Final Batch Loss: 0.13402163982391357\n",
      "Epoch 1222, Loss: 0.9957105070352554, Final Batch Loss: 0.21748186647891998\n",
      "Epoch 1223, Loss: 1.1794600188732147, Final Batch Loss: 0.4242400527000427\n",
      "Epoch 1224, Loss: 1.047179251909256, Final Batch Loss: 0.22906622290611267\n",
      "Epoch 1225, Loss: 1.1269286572933197, Final Batch Loss: 0.29632532596588135\n",
      "Epoch 1226, Loss: 1.0290557742118835, Final Batch Loss: 0.23467448353767395\n",
      "Epoch 1227, Loss: 1.2117841243743896, Final Batch Loss: 0.3262616991996765\n",
      "Epoch 1228, Loss: 1.3011020720005035, Final Batch Loss: 0.4512155055999756\n",
      "Epoch 1229, Loss: 0.998251885175705, Final Batch Loss: 0.2060895562171936\n",
      "Epoch 1230, Loss: 1.1490930914878845, Final Batch Loss: 0.36958831548690796\n",
      "Epoch 1231, Loss: 1.0371582806110382, Final Batch Loss: 0.271318256855011\n",
      "Epoch 1232, Loss: 1.3285931646823883, Final Batch Loss: 0.5234745740890503\n",
      "Epoch 1233, Loss: 1.1770887970924377, Final Batch Loss: 0.4604545533657074\n",
      "Epoch 1234, Loss: 1.0993759334087372, Final Batch Loss: 0.28661614656448364\n",
      "Epoch 1235, Loss: 1.2810314893722534, Final Batch Loss: 0.5314043760299683\n",
      "Epoch 1236, Loss: 1.0636801421642303, Final Batch Loss: 0.29152804613113403\n",
      "Epoch 1237, Loss: 1.0465857684612274, Final Batch Loss: 0.2985585629940033\n",
      "Epoch 1238, Loss: 1.0170025378465652, Final Batch Loss: 0.21794040501117706\n",
      "Epoch 1239, Loss: 1.128420352935791, Final Batch Loss: 0.2633369266986847\n",
      "Epoch 1240, Loss: 0.9816801846027374, Final Batch Loss: 0.2879970073699951\n",
      "Epoch 1241, Loss: 1.0650342404842377, Final Batch Loss: 0.2556684613227844\n",
      "Epoch 1242, Loss: 1.1429874002933502, Final Batch Loss: 0.35883215069770813\n",
      "Epoch 1243, Loss: 1.2239030599594116, Final Batch Loss: 0.397225022315979\n",
      "Epoch 1244, Loss: 1.2068281471729279, Final Batch Loss: 0.44414570927619934\n",
      "Epoch 1245, Loss: 1.1014698445796967, Final Batch Loss: 0.3246682584285736\n",
      "Epoch 1246, Loss: 1.1171380281448364, Final Batch Loss: 0.32977306842803955\n",
      "Epoch 1247, Loss: 1.340719074010849, Final Batch Loss: 0.6288854479789734\n",
      "Epoch 1248, Loss: 1.182787150144577, Final Batch Loss: 0.38333752751350403\n",
      "Epoch 1249, Loss: 1.158158779144287, Final Batch Loss: 0.3257348835468292\n",
      "Epoch 1250, Loss: 1.246206670999527, Final Batch Loss: 0.4912467896938324\n",
      "Epoch 1251, Loss: 1.234832525253296, Final Batch Loss: 0.45665886998176575\n",
      "Epoch 1252, Loss: 1.0645593702793121, Final Batch Loss: 0.3246457278728485\n",
      "Epoch 1253, Loss: 1.458966612815857, Final Batch Loss: 0.7133274078369141\n",
      "Epoch 1254, Loss: 1.3194915354251862, Final Batch Loss: 0.4608480930328369\n",
      "Epoch 1255, Loss: 1.1547304689884186, Final Batch Loss: 0.3566396236419678\n",
      "Epoch 1256, Loss: 1.3431939482688904, Final Batch Loss: 0.560541570186615\n",
      "Epoch 1257, Loss: 1.0507876873016357, Final Batch Loss: 0.32585564255714417\n",
      "Epoch 1258, Loss: 1.2470192313194275, Final Batch Loss: 0.4386725127696991\n",
      "Epoch 1259, Loss: 1.2830176055431366, Final Batch Loss: 0.5608670115470886\n",
      "Epoch 1260, Loss: 0.8837699443101883, Final Batch Loss: 0.18158622086048126\n",
      "Epoch 1261, Loss: 1.1098700612783432, Final Batch Loss: 0.23477624356746674\n",
      "Epoch 1262, Loss: 1.0998842120170593, Final Batch Loss: 0.3557465970516205\n",
      "Epoch 1263, Loss: 1.1315817534923553, Final Batch Loss: 0.40728890895843506\n",
      "Epoch 1264, Loss: 0.9285445958375931, Final Batch Loss: 0.18969418108463287\n",
      "Epoch 1265, Loss: 1.1396580636501312, Final Batch Loss: 0.3792017102241516\n",
      "Epoch 1266, Loss: 0.9825911372900009, Final Batch Loss: 0.23055653274059296\n",
      "Epoch 1267, Loss: 1.0963126420974731, Final Batch Loss: 0.27484726905822754\n",
      "Epoch 1268, Loss: 1.0250617414712906, Final Batch Loss: 0.24664725363254547\n",
      "Epoch 1269, Loss: 1.1927157938480377, Final Batch Loss: 0.38182634115219116\n",
      "Epoch 1270, Loss: 1.0533194541931152, Final Batch Loss: 0.3256678581237793\n",
      "Epoch 1271, Loss: 1.2089332342147827, Final Batch Loss: 0.467124342918396\n",
      "Epoch 1272, Loss: 1.6756472289562225, Final Batch Loss: 0.9153854250907898\n",
      "Epoch 1273, Loss: 1.1605455577373505, Final Batch Loss: 0.4172053039073944\n",
      "Epoch 1274, Loss: 1.1417839229106903, Final Batch Loss: 0.318737655878067\n",
      "Epoch 1275, Loss: 1.2469098567962646, Final Batch Loss: 0.4722014367580414\n",
      "Epoch 1276, Loss: 1.2693239748477936, Final Batch Loss: 0.5209774971008301\n",
      "Epoch 1277, Loss: 1.3103369772434235, Final Batch Loss: 0.5612421035766602\n",
      "Epoch 1278, Loss: 1.093807578086853, Final Batch Loss: 0.31085264682769775\n",
      "Epoch 1279, Loss: 0.9913890957832336, Final Batch Loss: 0.1946568787097931\n",
      "Epoch 1280, Loss: 1.1414838135242462, Final Batch Loss: 0.3750237226486206\n",
      "Epoch 1281, Loss: 1.379226565361023, Final Batch Loss: 0.6138608455657959\n",
      "Epoch 1282, Loss: 1.1516035795211792, Final Batch Loss: 0.41198331117630005\n",
      "Epoch 1283, Loss: 1.2543558776378632, Final Batch Loss: 0.479861855506897\n",
      "Epoch 1284, Loss: 1.0799440741539001, Final Batch Loss: 0.3299095034599304\n",
      "Epoch 1285, Loss: 1.2007111608982086, Final Batch Loss: 0.4213891327381134\n",
      "Epoch 1286, Loss: 1.1336293518543243, Final Batch Loss: 0.3715325593948364\n",
      "Epoch 1287, Loss: 1.5309021770954132, Final Batch Loss: 0.7466070055961609\n",
      "Epoch 1288, Loss: 0.9769862592220306, Final Batch Loss: 0.1924535632133484\n",
      "Epoch 1289, Loss: 1.5152007937431335, Final Batch Loss: 0.8013584017753601\n",
      "Epoch 1290, Loss: 0.9892733842134476, Final Batch Loss: 0.22185398638248444\n",
      "Epoch 1291, Loss: 1.196363091468811, Final Batch Loss: 0.46736833453178406\n",
      "Epoch 1292, Loss: 1.0640000700950623, Final Batch Loss: 0.2893083393573761\n",
      "Epoch 1293, Loss: 1.1163447201251984, Final Batch Loss: 0.37478122115135193\n",
      "Epoch 1294, Loss: 1.1788395941257477, Final Batch Loss: 0.4060187339782715\n",
      "Epoch 1295, Loss: 1.1499923765659332, Final Batch Loss: 0.3893851935863495\n",
      "Epoch 1296, Loss: 0.8834864348173141, Final Batch Loss: 0.12444265186786652\n",
      "Epoch 1297, Loss: 1.0894680619239807, Final Batch Loss: 0.28411632776260376\n",
      "Epoch 1298, Loss: 1.0748765468597412, Final Batch Loss: 0.28788983821868896\n",
      "Epoch 1299, Loss: 1.074342742562294, Final Batch Loss: 0.19304020702838898\n",
      "Epoch 1300, Loss: 1.1882812976837158, Final Batch Loss: 0.36572209000587463\n",
      "Epoch 1301, Loss: 1.1999543905258179, Final Batch Loss: 0.38857579231262207\n",
      "Epoch 1302, Loss: 1.1671321392059326, Final Batch Loss: 0.42029815912246704\n",
      "Epoch 1303, Loss: 0.8916241750121117, Final Batch Loss: 0.11146964877843857\n",
      "Epoch 1304, Loss: 1.1636162102222443, Final Batch Loss: 0.38995516300201416\n",
      "Epoch 1305, Loss: 1.1705167591571808, Final Batch Loss: 0.31339001655578613\n",
      "Epoch 1306, Loss: 1.240529865026474, Final Batch Loss: 0.4866809844970703\n",
      "Epoch 1307, Loss: 1.1842076778411865, Final Batch Loss: 0.4560302495956421\n",
      "Epoch 1308, Loss: 1.144712656736374, Final Batch Loss: 0.38806772232055664\n",
      "Epoch 1309, Loss: 0.9925288558006287, Final Batch Loss: 0.19416090846061707\n",
      "Epoch 1310, Loss: 1.15398508310318, Final Batch Loss: 0.37761181592941284\n",
      "Epoch 1311, Loss: 1.0127013325691223, Final Batch Loss: 0.2737564742565155\n",
      "Epoch 1312, Loss: 1.1211625039577484, Final Batch Loss: 0.3081071972846985\n",
      "Epoch 1313, Loss: 1.061426967382431, Final Batch Loss: 0.33199331164360046\n",
      "Epoch 1314, Loss: 1.0528320372104645, Final Batch Loss: 0.25605228543281555\n",
      "Epoch 1315, Loss: 1.035241037607193, Final Batch Loss: 0.3044760823249817\n",
      "Epoch 1316, Loss: 1.156987577676773, Final Batch Loss: 0.36080479621887207\n",
      "Epoch 1317, Loss: 1.1058833301067352, Final Batch Loss: 0.3225332200527191\n",
      "Epoch 1318, Loss: 1.0425324141979218, Final Batch Loss: 0.28573429584503174\n",
      "Epoch 1319, Loss: 1.3150110244750977, Final Batch Loss: 0.5426468849182129\n",
      "Epoch 1320, Loss: 0.9721474796533585, Final Batch Loss: 0.21497558057308197\n",
      "Epoch 1321, Loss: 1.487383097410202, Final Batch Loss: 0.6470264792442322\n",
      "Epoch 1322, Loss: 0.8954858332872391, Final Batch Loss: 0.11611463129520416\n",
      "Epoch 1323, Loss: 0.9090679734945297, Final Batch Loss: 0.1618933528661728\n",
      "Epoch 1324, Loss: 1.2288168966770172, Final Batch Loss: 0.4619339406490326\n",
      "Epoch 1325, Loss: 1.4933406114578247, Final Batch Loss: 0.7195361256599426\n",
      "Epoch 1326, Loss: 1.0419331789016724, Final Batch Loss: 0.29938408732414246\n",
      "Epoch 1327, Loss: 1.2172271311283112, Final Batch Loss: 0.4743047058582306\n",
      "Epoch 1328, Loss: 1.3360561728477478, Final Batch Loss: 0.586886465549469\n",
      "Epoch 1329, Loss: 1.197191596031189, Final Batch Loss: 0.4917236566543579\n",
      "Epoch 1330, Loss: 1.0546864569187164, Final Batch Loss: 0.33435460925102234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1331, Loss: 1.2738195061683655, Final Batch Loss: 0.47872963547706604\n",
      "Epoch 1332, Loss: 1.0693891942501068, Final Batch Loss: 0.2943528890609741\n",
      "Epoch 1333, Loss: 1.1963447630405426, Final Batch Loss: 0.3185635507106781\n",
      "Epoch 1334, Loss: 1.6331431865692139, Final Batch Loss: 0.7615149617195129\n",
      "Epoch 1335, Loss: 1.2513432800769806, Final Batch Loss: 0.47016850113868713\n",
      "Epoch 1336, Loss: 1.0798958837985992, Final Batch Loss: 0.3680954575538635\n",
      "Epoch 1337, Loss: 1.0685139894485474, Final Batch Loss: 0.31274643540382385\n",
      "Epoch 1338, Loss: 1.0409567952156067, Final Batch Loss: 0.33614248037338257\n",
      "Epoch 1339, Loss: 1.089699000120163, Final Batch Loss: 0.38196054100990295\n",
      "Epoch 1340, Loss: 1.1499045193195343, Final Batch Loss: 0.3685295283794403\n",
      "Epoch 1341, Loss: 1.3167625665664673, Final Batch Loss: 0.530657947063446\n",
      "Epoch 1342, Loss: 1.1613947749137878, Final Batch Loss: 0.3159635365009308\n",
      "Epoch 1343, Loss: 1.1287380158901215, Final Batch Loss: 0.3915284276008606\n",
      "Epoch 1344, Loss: 1.1544014513492584, Final Batch Loss: 0.4146842062473297\n",
      "Epoch 1345, Loss: 1.256696343421936, Final Batch Loss: 0.515380322933197\n",
      "Epoch 1346, Loss: 1.0087306499481201, Final Batch Loss: 0.3062555491924286\n",
      "Epoch 1347, Loss: 0.8970628678798676, Final Batch Loss: 0.1449187994003296\n",
      "Epoch 1348, Loss: 1.1604388654232025, Final Batch Loss: 0.4719057083129883\n",
      "Epoch 1349, Loss: 1.2823034524917603, Final Batch Loss: 0.5297484993934631\n",
      "Epoch 1350, Loss: 0.9703034460544586, Final Batch Loss: 0.30440205335617065\n",
      "Epoch 1351, Loss: 1.3084863722324371, Final Batch Loss: 0.5785384178161621\n",
      "Epoch 1352, Loss: 1.145293951034546, Final Batch Loss: 0.3270761966705322\n",
      "Epoch 1353, Loss: 1.0716272294521332, Final Batch Loss: 0.28507137298583984\n",
      "Epoch 1354, Loss: 0.9125580936670303, Final Batch Loss: 0.15224887430667877\n",
      "Epoch 1355, Loss: 1.3771344125270844, Final Batch Loss: 0.6445363163948059\n",
      "Epoch 1356, Loss: 0.9707825183868408, Final Batch Loss: 0.2523943781852722\n",
      "Epoch 1357, Loss: 1.1914300918579102, Final Batch Loss: 0.4419267177581787\n",
      "Epoch 1358, Loss: 1.030014529824257, Final Batch Loss: 0.24895261228084564\n",
      "Epoch 1359, Loss: 1.0555554032325745, Final Batch Loss: 0.3335202932357788\n",
      "Epoch 1360, Loss: 1.1667210757732391, Final Batch Loss: 0.40742114186286926\n",
      "Epoch 1361, Loss: 1.2164307832717896, Final Batch Loss: 0.4464355707168579\n",
      "Epoch 1362, Loss: 0.9133859127759933, Final Batch Loss: 0.2057800143957138\n",
      "Epoch 1363, Loss: 1.068887621164322, Final Batch Loss: 0.2803579270839691\n",
      "Epoch 1364, Loss: 1.2641267776489258, Final Batch Loss: 0.4602639079093933\n",
      "Epoch 1365, Loss: 1.1267968118190765, Final Batch Loss: 0.37179964780807495\n",
      "Epoch 1366, Loss: 1.0610482394695282, Final Batch Loss: 0.31703248620033264\n",
      "Epoch 1367, Loss: 0.9436896443367004, Final Batch Loss: 0.23174971342086792\n",
      "Epoch 1368, Loss: 1.1493279337882996, Final Batch Loss: 0.40299808979034424\n",
      "Epoch 1369, Loss: 1.1809476613998413, Final Batch Loss: 0.43406912684440613\n",
      "Epoch 1370, Loss: 1.1157650351524353, Final Batch Loss: 0.3561039865016937\n",
      "Epoch 1371, Loss: 1.2892711162567139, Final Batch Loss: 0.4980327785015106\n",
      "Epoch 1372, Loss: 1.214762806892395, Final Batch Loss: 0.505244255065918\n",
      "Epoch 1373, Loss: 0.9965618997812271, Final Batch Loss: 0.22586466372013092\n",
      "Epoch 1374, Loss: 1.221175879240036, Final Batch Loss: 0.4576098918914795\n",
      "Epoch 1375, Loss: 1.0932332873344421, Final Batch Loss: 0.36796319484710693\n",
      "Epoch 1376, Loss: 1.119316190481186, Final Batch Loss: 0.4280402660369873\n",
      "Epoch 1377, Loss: 1.3306280970573425, Final Batch Loss: 0.5769621729850769\n",
      "Epoch 1378, Loss: 1.0933190882205963, Final Batch Loss: 0.3791688084602356\n",
      "Epoch 1379, Loss: 1.0821744501590729, Final Batch Loss: 0.344811350107193\n",
      "Epoch 1380, Loss: 1.1727414727210999, Final Batch Loss: 0.38978227972984314\n",
      "Epoch 1381, Loss: 1.1401151418685913, Final Batch Loss: 0.4026649296283722\n",
      "Epoch 1382, Loss: 1.3931050896644592, Final Batch Loss: 0.5040987133979797\n",
      "Epoch 1383, Loss: 1.186458557844162, Final Batch Loss: 0.3765595555305481\n",
      "Epoch 1384, Loss: 1.1926540732383728, Final Batch Loss: 0.4694826602935791\n",
      "Epoch 1385, Loss: 0.9910185635089874, Final Batch Loss: 0.25140297412872314\n",
      "Epoch 1386, Loss: 0.9211029261350632, Final Batch Loss: 0.1977531760931015\n",
      "Epoch 1387, Loss: 0.9950538128614426, Final Batch Loss: 0.2057715505361557\n",
      "Epoch 1388, Loss: 1.0437159538269043, Final Batch Loss: 0.2517545819282532\n",
      "Epoch 1389, Loss: 0.8465889543294907, Final Batch Loss: 0.15129916369915009\n",
      "Epoch 1390, Loss: 1.0322567820549011, Final Batch Loss: 0.2787536382675171\n",
      "Epoch 1391, Loss: 0.948050931096077, Final Batch Loss: 0.16592462360858917\n",
      "Epoch 1392, Loss: 1.0242516100406647, Final Batch Loss: 0.3325488567352295\n",
      "Epoch 1393, Loss: 1.0709552764892578, Final Batch Loss: 0.2986968159675598\n",
      "Epoch 1394, Loss: 1.491249293088913, Final Batch Loss: 0.7807409763336182\n",
      "Epoch 1395, Loss: 0.9817650318145752, Final Batch Loss: 0.29099753499031067\n",
      "Epoch 1396, Loss: 1.1334739327430725, Final Batch Loss: 0.3863748013973236\n",
      "Epoch 1397, Loss: 0.9108708947896957, Final Batch Loss: 0.2372111827135086\n",
      "Epoch 1398, Loss: 1.168701171875, Final Batch Loss: 0.44356799125671387\n",
      "Epoch 1399, Loss: 1.0644897818565369, Final Batch Loss: 0.3532625138759613\n",
      "Epoch 1400, Loss: 1.0757432878017426, Final Batch Loss: 0.37057217955589294\n",
      "Epoch 1401, Loss: 1.0856949090957642, Final Batch Loss: 0.36986738443374634\n",
      "Epoch 1402, Loss: 1.0493720173835754, Final Batch Loss: 0.3097435235977173\n",
      "Epoch 1403, Loss: 1.1145020127296448, Final Batch Loss: 0.41873693466186523\n",
      "Epoch 1404, Loss: 0.9929726719856262, Final Batch Loss: 0.18472665548324585\n",
      "Epoch 1405, Loss: 1.19078728556633, Final Batch Loss: 0.4835251271724701\n",
      "Epoch 1406, Loss: 1.1302449703216553, Final Batch Loss: 0.3868156969547272\n",
      "Epoch 1407, Loss: 1.1186163127422333, Final Batch Loss: 0.31776517629623413\n",
      "Epoch 1408, Loss: 1.1513626873493195, Final Batch Loss: 0.42220818996429443\n",
      "Epoch 1409, Loss: 0.9529288113117218, Final Batch Loss: 0.25009021162986755\n",
      "Epoch 1410, Loss: 0.9814548492431641, Final Batch Loss: 0.22200143337249756\n",
      "Epoch 1411, Loss: 1.3921102583408356, Final Batch Loss: 0.6716005206108093\n",
      "Epoch 1412, Loss: 0.9002049267292023, Final Batch Loss: 0.20317471027374268\n",
      "Epoch 1413, Loss: 1.205264538526535, Final Batch Loss: 0.46115779876708984\n",
      "Epoch 1414, Loss: 0.8421269059181213, Final Batch Loss: 0.07787331938743591\n",
      "Epoch 1415, Loss: 1.1274307072162628, Final Batch Loss: 0.4045969545841217\n",
      "Epoch 1416, Loss: 1.0424544215202332, Final Batch Loss: 0.30647653341293335\n",
      "Epoch 1417, Loss: 1.0232805907726288, Final Batch Loss: 0.3485637605190277\n",
      "Epoch 1418, Loss: 1.128334492444992, Final Batch Loss: 0.368704617023468\n",
      "Epoch 1419, Loss: 1.100163996219635, Final Batch Loss: 0.33393827080726624\n",
      "Epoch 1420, Loss: 0.885777086019516, Final Batch Loss: 0.2387620508670807\n",
      "Epoch 1421, Loss: 1.0616704523563385, Final Batch Loss: 0.30624717473983765\n",
      "Epoch 1422, Loss: 1.017018347978592, Final Batch Loss: 0.34426549077033997\n",
      "Epoch 1423, Loss: 0.9891925156116486, Final Batch Loss: 0.28600552678108215\n",
      "Epoch 1424, Loss: 1.1915946900844574, Final Batch Loss: 0.4580441117286682\n",
      "Epoch 1425, Loss: 1.1481429636478424, Final Batch Loss: 0.3360181748867035\n",
      "Epoch 1426, Loss: 0.824061706662178, Final Batch Loss: 0.14935694634914398\n",
      "Epoch 1427, Loss: 0.8623074442148209, Final Batch Loss: 0.11155174672603607\n",
      "Epoch 1428, Loss: 1.0261707603931427, Final Batch Loss: 0.3005962073802948\n",
      "Epoch 1429, Loss: 1.0097030103206635, Final Batch Loss: 0.3383728861808777\n",
      "Epoch 1430, Loss: 1.1871883273124695, Final Batch Loss: 0.449394166469574\n",
      "Epoch 1431, Loss: 1.0364956259727478, Final Batch Loss: 0.24338924884796143\n",
      "Epoch 1432, Loss: 1.3498355746269226, Final Batch Loss: 0.6333886981010437\n",
      "Epoch 1433, Loss: 0.97522833943367, Final Batch Loss: 0.22941243648529053\n",
      "Epoch 1434, Loss: 1.150983840227127, Final Batch Loss: 0.3272861838340759\n",
      "Epoch 1435, Loss: 1.1193903684616089, Final Batch Loss: 0.36407801508903503\n",
      "Epoch 1436, Loss: 0.8841265588998795, Final Batch Loss: 0.1691325157880783\n",
      "Epoch 1437, Loss: 1.0881161391735077, Final Batch Loss: 0.39819830656051636\n",
      "Epoch 1438, Loss: 1.0022089779376984, Final Batch Loss: 0.30775579810142517\n",
      "Epoch 1439, Loss: 1.2864149510860443, Final Batch Loss: 0.5207193493843079\n",
      "Epoch 1440, Loss: 1.153429388999939, Final Batch Loss: 0.38277480006217957\n",
      "Epoch 1441, Loss: 0.8658239543437958, Final Batch Loss: 0.16800296306610107\n",
      "Epoch 1442, Loss: 0.9045180231332779, Final Batch Loss: 0.19643668830394745\n",
      "Epoch 1443, Loss: 0.9463946372270584, Final Batch Loss: 0.23914365470409393\n",
      "Epoch 1444, Loss: 1.1667791604995728, Final Batch Loss: 0.4638271927833557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1445, Loss: 1.2099372744560242, Final Batch Loss: 0.47181758284568787\n",
      "Epoch 1446, Loss: 1.106144279241562, Final Batch Loss: 0.3937988579273224\n",
      "Epoch 1447, Loss: 0.9557680189609528, Final Batch Loss: 0.2788529098033905\n",
      "Epoch 1448, Loss: 1.2117484211921692, Final Batch Loss: 0.46225032210350037\n",
      "Epoch 1449, Loss: 0.9584963023662567, Final Batch Loss: 0.23817726969718933\n",
      "Epoch 1450, Loss: 1.0704030096530914, Final Batch Loss: 0.39451390504837036\n",
      "Epoch 1451, Loss: 1.1887764930725098, Final Batch Loss: 0.5106542110443115\n",
      "Epoch 1452, Loss: 1.1955238580703735, Final Batch Loss: 0.4292105734348297\n",
      "Epoch 1453, Loss: 1.3840665817260742, Final Batch Loss: 0.6767339706420898\n",
      "Epoch 1454, Loss: 1.0888154804706573, Final Batch Loss: 0.37336161732673645\n",
      "Epoch 1455, Loss: 1.0281042456626892, Final Batch Loss: 0.2862199544906616\n",
      "Epoch 1456, Loss: 1.1200893819332123, Final Batch Loss: 0.37529635429382324\n",
      "Epoch 1457, Loss: 0.8854223638772964, Final Batch Loss: 0.13135500252246857\n",
      "Epoch 1458, Loss: 1.1585952639579773, Final Batch Loss: 0.45553359389305115\n",
      "Epoch 1459, Loss: 1.0532141327857971, Final Batch Loss: 0.29689717292785645\n",
      "Epoch 1460, Loss: 0.8744731917977333, Final Batch Loss: 0.11162189394235611\n",
      "Epoch 1461, Loss: 0.8892007023096085, Final Batch Loss: 0.2074144333600998\n",
      "Epoch 1462, Loss: 1.23909792304039, Final Batch Loss: 0.49011173844337463\n",
      "Epoch 1463, Loss: 1.0293216109275818, Final Batch Loss: 0.25247707962989807\n",
      "Epoch 1464, Loss: 0.9258827716112137, Final Batch Loss: 0.18089734017848969\n",
      "Epoch 1465, Loss: 1.0485508441925049, Final Batch Loss: 0.3265833854675293\n",
      "Epoch 1466, Loss: 0.8729662746191025, Final Batch Loss: 0.17694799602031708\n",
      "Epoch 1467, Loss: 0.9384822249412537, Final Batch Loss: 0.29828914999961853\n",
      "Epoch 1468, Loss: 1.1119839251041412, Final Batch Loss: 0.45764973759651184\n",
      "Epoch 1469, Loss: 1.1596942842006683, Final Batch Loss: 0.42412954568862915\n",
      "Epoch 1470, Loss: 1.049891322851181, Final Batch Loss: 0.40839728713035583\n",
      "Epoch 1471, Loss: 1.085096001625061, Final Batch Loss: 0.40292778611183167\n",
      "Epoch 1472, Loss: 1.412802129983902, Final Batch Loss: 0.5871099233627319\n",
      "Epoch 1473, Loss: 1.2032452523708344, Final Batch Loss: 0.46480026841163635\n",
      "Epoch 1474, Loss: 1.1589106917381287, Final Batch Loss: 0.42332497239112854\n",
      "Epoch 1475, Loss: 1.327983558177948, Final Batch Loss: 0.6095947623252869\n",
      "Epoch 1476, Loss: 1.2432866096496582, Final Batch Loss: 0.43375271558761597\n",
      "Epoch 1477, Loss: 1.0370323359966278, Final Batch Loss: 0.3023897111415863\n",
      "Epoch 1478, Loss: 0.9912194907665253, Final Batch Loss: 0.28654980659484863\n",
      "Epoch 1479, Loss: 1.0507677793502808, Final Batch Loss: 0.24047976732254028\n",
      "Epoch 1480, Loss: 1.1090415716171265, Final Batch Loss: 0.34657394886016846\n",
      "Epoch 1481, Loss: 1.0815973281860352, Final Batch Loss: 0.32584381103515625\n",
      "Epoch 1482, Loss: 0.9785634279251099, Final Batch Loss: 0.29253944754600525\n",
      "Epoch 1483, Loss: 0.7982407212257385, Final Batch Loss: 0.1387099027633667\n",
      "Epoch 1484, Loss: 1.245775431394577, Final Batch Loss: 0.5281658172607422\n",
      "Epoch 1485, Loss: 0.9975261390209198, Final Batch Loss: 0.3288044333457947\n",
      "Epoch 1486, Loss: 1.1704524159431458, Final Batch Loss: 0.4553816318511963\n",
      "Epoch 1487, Loss: 1.0944530069828033, Final Batch Loss: 0.3435951769351959\n",
      "Epoch 1488, Loss: 0.9569283127784729, Final Batch Loss: 0.18179965019226074\n",
      "Epoch 1489, Loss: 1.3529474139213562, Final Batch Loss: 0.6787111759185791\n",
      "Epoch 1490, Loss: 0.9733063280582428, Final Batch Loss: 0.2738807797431946\n",
      "Epoch 1491, Loss: 1.0975102484226227, Final Batch Loss: 0.4318627417087555\n",
      "Epoch 1492, Loss: 1.2139808237552643, Final Batch Loss: 0.5321096777915955\n",
      "Epoch 1493, Loss: 1.193165272474289, Final Batch Loss: 0.496138334274292\n",
      "Epoch 1494, Loss: 1.061085045337677, Final Batch Loss: 0.33118632435798645\n",
      "Epoch 1495, Loss: 1.1010128557682037, Final Batch Loss: 0.3812783658504486\n",
      "Epoch 1496, Loss: 0.9449723064899445, Final Batch Loss: 0.22346195578575134\n",
      "Epoch 1497, Loss: 1.0004042983055115, Final Batch Loss: 0.3414851129055023\n",
      "Epoch 1498, Loss: 1.1250616014003754, Final Batch Loss: 0.4356066584587097\n",
      "Epoch 1499, Loss: 1.0321752727031708, Final Batch Loss: 0.37490740418434143\n",
      "Epoch 1500, Loss: 1.0726624131202698, Final Batch Loss: 0.34577879309654236\n",
      "Epoch 1501, Loss: 1.0073345601558685, Final Batch Loss: 0.222389817237854\n",
      "Epoch 1502, Loss: 1.2463187277317047, Final Batch Loss: 0.4546951353549957\n",
      "Epoch 1503, Loss: 0.9743454158306122, Final Batch Loss: 0.26080217957496643\n",
      "Epoch 1504, Loss: 1.0266310572624207, Final Batch Loss: 0.2953164577484131\n",
      "Epoch 1505, Loss: 1.0525385737419128, Final Batch Loss: 0.29436320066452026\n",
      "Epoch 1506, Loss: 0.9913584887981415, Final Batch Loss: 0.2840086817741394\n",
      "Epoch 1507, Loss: 1.2323098182678223, Final Batch Loss: 0.5131353139877319\n",
      "Epoch 1508, Loss: 0.9763694703578949, Final Batch Loss: 0.26574021577835083\n",
      "Epoch 1509, Loss: 1.2324409186840057, Final Batch Loss: 0.4938543140888214\n",
      "Epoch 1510, Loss: 1.2203011214733124, Final Batch Loss: 0.4865611791610718\n",
      "Epoch 1511, Loss: 0.9809897541999817, Final Batch Loss: 0.25022801756858826\n",
      "Epoch 1512, Loss: 1.1519999504089355, Final Batch Loss: 0.45253363251686096\n",
      "Epoch 1513, Loss: 1.0291657149791718, Final Batch Loss: 0.35735154151916504\n",
      "Epoch 1514, Loss: 1.121527522802353, Final Batch Loss: 0.44768691062927246\n",
      "Epoch 1515, Loss: 1.085589736700058, Final Batch Loss: 0.36897286772727966\n",
      "Epoch 1516, Loss: 0.8505412489175797, Final Batch Loss: 0.13423390686511993\n",
      "Epoch 1517, Loss: 1.0703017115592957, Final Batch Loss: 0.3794398903846741\n",
      "Epoch 1518, Loss: 0.9244578331708908, Final Batch Loss: 0.24861280620098114\n",
      "Epoch 1519, Loss: 0.8520944863557816, Final Batch Loss: 0.14102773368358612\n",
      "Epoch 1520, Loss: 1.1099766492843628, Final Batch Loss: 0.3861439824104309\n",
      "Epoch 1521, Loss: 1.109232097864151, Final Batch Loss: 0.3077426254749298\n",
      "Epoch 1522, Loss: 0.9680518209934235, Final Batch Loss: 0.3253905177116394\n",
      "Epoch 1523, Loss: 1.012377291917801, Final Batch Loss: 0.2824770510196686\n",
      "Epoch 1524, Loss: 1.1101069450378418, Final Batch Loss: 0.4174984097480774\n",
      "Epoch 1525, Loss: 1.1656056642532349, Final Batch Loss: 0.4613375961780548\n",
      "Epoch 1526, Loss: 1.1678161919116974, Final Batch Loss: 0.4242442846298218\n",
      "Epoch 1527, Loss: 0.9119579046964645, Final Batch Loss: 0.2152888923883438\n",
      "Epoch 1528, Loss: 1.0287406593561172, Final Batch Loss: 0.24979515373706818\n",
      "Epoch 1529, Loss: 1.1786346137523651, Final Batch Loss: 0.39962825179100037\n",
      "Epoch 1530, Loss: 1.0472124218940735, Final Batch Loss: 0.33401888608932495\n",
      "Epoch 1531, Loss: 1.0950569808483124, Final Batch Loss: 0.41710537672042847\n",
      "Epoch 1532, Loss: 1.0468717217445374, Final Batch Loss: 0.33947792649269104\n",
      "Epoch 1533, Loss: 1.0523581206798553, Final Batch Loss: 0.3398098051548004\n",
      "Epoch 1534, Loss: 1.0336833894252777, Final Batch Loss: 0.3052333891391754\n",
      "Epoch 1535, Loss: 1.0075093507766724, Final Batch Loss: 0.27068787813186646\n",
      "Epoch 1536, Loss: 0.9501885175704956, Final Batch Loss: 0.3392986059188843\n",
      "Epoch 1537, Loss: 1.1091515719890594, Final Batch Loss: 0.4265836477279663\n",
      "Epoch 1538, Loss: 1.0039564818143845, Final Batch Loss: 0.24890251457691193\n",
      "Epoch 1539, Loss: 1.0571615397930145, Final Batch Loss: 0.3622029423713684\n",
      "Epoch 1540, Loss: 1.188681960105896, Final Batch Loss: 0.4184908866882324\n",
      "Epoch 1541, Loss: 1.226959615945816, Final Batch Loss: 0.5752321481704712\n",
      "Epoch 1542, Loss: 0.931071400642395, Final Batch Loss: 0.1921387016773224\n",
      "Epoch 1543, Loss: 0.9255289286375046, Final Batch Loss: 0.21077291667461395\n",
      "Epoch 1544, Loss: 1.3556491434574127, Final Batch Loss: 0.6264782547950745\n",
      "Epoch 1545, Loss: 0.8685334771871567, Final Batch Loss: 0.11888886988162994\n",
      "Epoch 1546, Loss: 1.1737233698368073, Final Batch Loss: 0.45100173354148865\n",
      "Epoch 1547, Loss: 1.2584574520587921, Final Batch Loss: 0.4683704972267151\n",
      "Epoch 1548, Loss: 0.8970533907413483, Final Batch Loss: 0.2244696319103241\n",
      "Epoch 1549, Loss: 0.9440953135490417, Final Batch Loss: 0.25110262632369995\n",
      "Epoch 1550, Loss: 0.873961478471756, Final Batch Loss: 0.15508639812469482\n",
      "Epoch 1551, Loss: 0.9186275601387024, Final Batch Loss: 0.2590057849884033\n",
      "Epoch 1552, Loss: 0.9000634998083115, Final Batch Loss: 0.1315491944551468\n",
      "Epoch 1553, Loss: 0.8884498327970505, Final Batch Loss: 0.2202482670545578\n",
      "Epoch 1554, Loss: 1.1615994572639465, Final Batch Loss: 0.3755134344100952\n",
      "Epoch 1555, Loss: 0.9595463424921036, Final Batch Loss: 0.22030915319919586\n",
      "Epoch 1556, Loss: 1.0826570987701416, Final Batch Loss: 0.3305593430995941\n",
      "Epoch 1557, Loss: 0.9111886918544769, Final Batch Loss: 0.1909719705581665\n",
      "Epoch 1558, Loss: 1.0527588725090027, Final Batch Loss: 0.32347699999809265\n",
      "Epoch 1559, Loss: 1.1360561847686768, Final Batch Loss: 0.48766863346099854\n",
      "Epoch 1560, Loss: 0.8141258507966995, Final Batch Loss: 0.1198689192533493\n",
      "Epoch 1561, Loss: 1.0232164561748505, Final Batch Loss: 0.31156283617019653\n",
      "Epoch 1562, Loss: 1.121468961238861, Final Batch Loss: 0.4500962197780609\n",
      "Epoch 1563, Loss: 0.9890616238117218, Final Batch Loss: 0.311419814825058\n",
      "Epoch 1564, Loss: 1.0135727524757385, Final Batch Loss: 0.2412782907485962\n",
      "Epoch 1565, Loss: 0.9466504901647568, Final Batch Loss: 0.24386157095432281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1566, Loss: 0.9627597332000732, Final Batch Loss: 0.290617436170578\n",
      "Epoch 1567, Loss: 0.9286808669567108, Final Batch Loss: 0.22432386875152588\n",
      "Epoch 1568, Loss: 1.1311730742454529, Final Batch Loss: 0.42229196429252625\n",
      "Epoch 1569, Loss: 0.9608745276927948, Final Batch Loss: 0.2505853474140167\n",
      "Epoch 1570, Loss: 1.1316436231136322, Final Batch Loss: 0.42654454708099365\n",
      "Epoch 1571, Loss: 0.8885337114334106, Final Batch Loss: 0.13815072178840637\n",
      "Epoch 1572, Loss: 1.082023411989212, Final Batch Loss: 0.352032870054245\n",
      "Epoch 1573, Loss: 0.9478677809238434, Final Batch Loss: 0.2814532220363617\n",
      "Epoch 1574, Loss: 1.2804005146026611, Final Batch Loss: 0.6003075242042542\n",
      "Epoch 1575, Loss: 0.9347040951251984, Final Batch Loss: 0.2615073323249817\n",
      "Epoch 1576, Loss: 0.9492952674627304, Final Batch Loss: 0.16144908964633942\n",
      "Epoch 1577, Loss: 1.186224102973938, Final Batch Loss: 0.4264138340950012\n",
      "Epoch 1578, Loss: 1.1124165654182434, Final Batch Loss: 0.3713228702545166\n",
      "Epoch 1579, Loss: 1.1654764115810394, Final Batch Loss: 0.5026645064353943\n",
      "Epoch 1580, Loss: 0.9525377452373505, Final Batch Loss: 0.27886518836021423\n",
      "Epoch 1581, Loss: 1.0512171983718872, Final Batch Loss: 0.3254336714744568\n",
      "Epoch 1582, Loss: 1.1239689886569977, Final Batch Loss: 0.4344029724597931\n",
      "Epoch 1583, Loss: 1.0906200110912323, Final Batch Loss: 0.3797808885574341\n",
      "Epoch 1584, Loss: 0.8792479932308197, Final Batch Loss: 0.20483049750328064\n",
      "Epoch 1585, Loss: 0.9705567359924316, Final Batch Loss: 0.22951257228851318\n",
      "Epoch 1586, Loss: 1.1164385080337524, Final Batch Loss: 0.43910127878189087\n",
      "Epoch 1587, Loss: 0.8658746182918549, Final Batch Loss: 0.14397355914115906\n",
      "Epoch 1588, Loss: 1.026244044303894, Final Batch Loss: 0.3547538220882416\n",
      "Epoch 1589, Loss: 0.8320783227682114, Final Batch Loss: 0.1437494307756424\n",
      "Epoch 1590, Loss: 1.1123795807361603, Final Batch Loss: 0.46355971693992615\n",
      "Epoch 1591, Loss: 0.8245822042226791, Final Batch Loss: 0.1856653243303299\n",
      "Epoch 1592, Loss: 1.0779346525669098, Final Batch Loss: 0.4455602467060089\n",
      "Epoch 1593, Loss: 1.0032116770744324, Final Batch Loss: 0.254946768283844\n",
      "Epoch 1594, Loss: 1.2039845287799835, Final Batch Loss: 0.4759696125984192\n",
      "Epoch 1595, Loss: 1.0913480818271637, Final Batch Loss: 0.4265799820423126\n",
      "Epoch 1596, Loss: 0.9308287501335144, Final Batch Loss: 0.2758421003818512\n",
      "Epoch 1597, Loss: 1.0654308199882507, Final Batch Loss: 0.3879581689834595\n",
      "Epoch 1598, Loss: 1.1201414465904236, Final Batch Loss: 0.45398762822151184\n",
      "Epoch 1599, Loss: 1.0554304122924805, Final Batch Loss: 0.3573768138885498\n",
      "Epoch 1600, Loss: 0.9691878855228424, Final Batch Loss: 0.2568104565143585\n",
      "Epoch 1601, Loss: 1.0004366338253021, Final Batch Loss: 0.2814551889896393\n",
      "Epoch 1602, Loss: 0.8422639816999435, Final Batch Loss: 0.15204091370105743\n",
      "Epoch 1603, Loss: 1.2037557661533356, Final Batch Loss: 0.4423317015171051\n",
      "Epoch 1604, Loss: 0.8452249020338058, Final Batch Loss: 0.16688938438892365\n",
      "Epoch 1605, Loss: 1.2259475588798523, Final Batch Loss: 0.522362470626831\n",
      "Epoch 1606, Loss: 1.1451383233070374, Final Batch Loss: 0.4302579462528229\n",
      "Epoch 1607, Loss: 0.8883186429738998, Final Batch Loss: 0.14638833701610565\n",
      "Epoch 1608, Loss: 1.106882929801941, Final Batch Loss: 0.3408600687980652\n",
      "Epoch 1609, Loss: 1.1767869889736176, Final Batch Loss: 0.3948105275630951\n",
      "Epoch 1610, Loss: 1.04616579413414, Final Batch Loss: 0.41350680589675903\n",
      "Epoch 1611, Loss: 0.9056216478347778, Final Batch Loss: 0.20302411913871765\n",
      "Epoch 1612, Loss: 1.1413351595401764, Final Batch Loss: 0.44857367873191833\n",
      "Epoch 1613, Loss: 1.0467389523983002, Final Batch Loss: 0.36730414628982544\n",
      "Epoch 1614, Loss: 0.9619298726320267, Final Batch Loss: 0.22371824085712433\n",
      "Epoch 1615, Loss: 1.0912071764469147, Final Batch Loss: 0.4368871748447418\n",
      "Epoch 1616, Loss: 0.8399747312068939, Final Batch Loss: 0.18055596947669983\n",
      "Epoch 1617, Loss: 0.9594093561172485, Final Batch Loss: 0.33685341477394104\n",
      "Epoch 1618, Loss: 1.1419152915477753, Final Batch Loss: 0.4596163034439087\n",
      "Epoch 1619, Loss: 1.0343135595321655, Final Batch Loss: 0.26809343695640564\n",
      "Epoch 1620, Loss: 0.716933973133564, Final Batch Loss: 0.07980868965387344\n",
      "Epoch 1621, Loss: 1.1537776291370392, Final Batch Loss: 0.4997827708721161\n",
      "Epoch 1622, Loss: 0.7995287328958511, Final Batch Loss: 0.13927651941776276\n",
      "Epoch 1623, Loss: 0.9263456761837006, Final Batch Loss: 0.23750030994415283\n",
      "Epoch 1624, Loss: 1.1629692912101746, Final Batch Loss: 0.45453351736068726\n",
      "Epoch 1625, Loss: 1.1581902205944061, Final Batch Loss: 0.48999354243278503\n",
      "Epoch 1626, Loss: 1.0142498016357422, Final Batch Loss: 0.32838064432144165\n",
      "Epoch 1627, Loss: 1.0884395241737366, Final Batch Loss: 0.4436322748661041\n",
      "Epoch 1628, Loss: 1.025071769952774, Final Batch Loss: 0.323763906955719\n",
      "Epoch 1629, Loss: 0.8686016499996185, Final Batch Loss: 0.21584847569465637\n",
      "Epoch 1630, Loss: 0.9539711773395538, Final Batch Loss: 0.1846955120563507\n",
      "Epoch 1631, Loss: 0.9426670670509338, Final Batch Loss: 0.2516919672489166\n",
      "Epoch 1632, Loss: 1.0670457482337952, Final Batch Loss: 0.33760303258895874\n",
      "Epoch 1633, Loss: 0.9508635103702545, Final Batch Loss: 0.2738005220890045\n",
      "Epoch 1634, Loss: 1.0341397523880005, Final Batch Loss: 0.40391790866851807\n",
      "Epoch 1635, Loss: 1.0543141663074493, Final Batch Loss: 0.3816240429878235\n",
      "Epoch 1636, Loss: 1.08115553855896, Final Batch Loss: 0.4209514260292053\n",
      "Epoch 1637, Loss: 1.0739469230175018, Final Batch Loss: 0.3997727632522583\n",
      "Epoch 1638, Loss: 0.9367787539958954, Final Batch Loss: 0.22942766547203064\n",
      "Epoch 1639, Loss: 0.8865238428115845, Final Batch Loss: 0.17200368642807007\n",
      "Epoch 1640, Loss: 1.0784118473529816, Final Batch Loss: 0.44240832328796387\n",
      "Epoch 1641, Loss: 0.9646039605140686, Final Batch Loss: 0.3792317509651184\n",
      "Epoch 1642, Loss: 0.956904947757721, Final Batch Loss: 0.22194689512252808\n",
      "Epoch 1643, Loss: 0.9246139824390411, Final Batch Loss: 0.2291731834411621\n",
      "Epoch 1644, Loss: 1.0321258306503296, Final Batch Loss: 0.36547157168388367\n",
      "Epoch 1645, Loss: 1.1923139095306396, Final Batch Loss: 0.49482622742652893\n",
      "Epoch 1646, Loss: 0.9562200605869293, Final Batch Loss: 0.2745496928691864\n",
      "Epoch 1647, Loss: 1.2627319991588593, Final Batch Loss: 0.5041377544403076\n",
      "Epoch 1648, Loss: 1.5928370952606201, Final Batch Loss: 0.8867700695991516\n",
      "Epoch 1649, Loss: 0.9212453067302704, Final Batch Loss: 0.19910302758216858\n",
      "Epoch 1650, Loss: 0.8260282874107361, Final Batch Loss: 0.14054030179977417\n",
      "Epoch 1651, Loss: 1.1118353009223938, Final Batch Loss: 0.3695993423461914\n",
      "Epoch 1652, Loss: 1.3066361844539642, Final Batch Loss: 0.6067000031471252\n",
      "Epoch 1653, Loss: 0.927613377571106, Final Batch Loss: 0.29259103536605835\n",
      "Epoch 1654, Loss: 0.8030612617731094, Final Batch Loss: 0.15184490382671356\n",
      "Epoch 1655, Loss: 1.0487140119075775, Final Batch Loss: 0.36335378885269165\n",
      "Epoch 1656, Loss: 1.2772916853427887, Final Batch Loss: 0.617969274520874\n",
      "Epoch 1657, Loss: 0.8912523984909058, Final Batch Loss: 0.16091889142990112\n",
      "Epoch 1658, Loss: 0.9427215456962585, Final Batch Loss: 0.27165237069129944\n",
      "Epoch 1659, Loss: 1.0274869799613953, Final Batch Loss: 0.3646455705165863\n",
      "Epoch 1660, Loss: 1.0394644439220428, Final Batch Loss: 0.3451977074146271\n",
      "Epoch 1661, Loss: 1.2245129942893982, Final Batch Loss: 0.5151857137680054\n",
      "Epoch 1662, Loss: 1.0096642673015594, Final Batch Loss: 0.4111369252204895\n",
      "Epoch 1663, Loss: 1.0194232761859894, Final Batch Loss: 0.30412954092025757\n",
      "Epoch 1664, Loss: 1.0114924013614655, Final Batch Loss: 0.3969866931438446\n",
      "Epoch 1665, Loss: 1.0538206696510315, Final Batch Loss: 0.33644503355026245\n",
      "Epoch 1666, Loss: 0.9819423854351044, Final Batch Loss: 0.3387583792209625\n",
      "Epoch 1667, Loss: 1.0140195786952972, Final Batch Loss: 0.3060806691646576\n",
      "Epoch 1668, Loss: 1.0050975382328033, Final Batch Loss: 0.37153029441833496\n",
      "Epoch 1669, Loss: 1.0504216253757477, Final Batch Loss: 0.3931959271430969\n",
      "Epoch 1670, Loss: 1.027685672044754, Final Batch Loss: 0.31454557180404663\n",
      "Epoch 1671, Loss: 0.8803133368492126, Final Batch Loss: 0.1869623363018036\n",
      "Epoch 1672, Loss: 0.8866450488567352, Final Batch Loss: 0.26944398880004883\n",
      "Epoch 1673, Loss: 1.0889456868171692, Final Batch Loss: 0.44424811005592346\n",
      "Epoch 1674, Loss: 1.1388417184352875, Final Batch Loss: 0.5360226035118103\n",
      "Epoch 1675, Loss: 0.8688182830810547, Final Batch Loss: 0.260206937789917\n",
      "Epoch 1676, Loss: 0.9627525210380554, Final Batch Loss: 0.30366411805152893\n",
      "Epoch 1677, Loss: 1.1057422161102295, Final Batch Loss: 0.4882899820804596\n",
      "Epoch 1678, Loss: 1.2470438480377197, Final Batch Loss: 0.42236050963401794\n",
      "Epoch 1679, Loss: 1.128922551870346, Final Batch Loss: 0.38107359409332275\n",
      "Epoch 1680, Loss: 0.8566510528326035, Final Batch Loss: 0.22187529504299164\n",
      "Epoch 1681, Loss: 0.9154376983642578, Final Batch Loss: 0.24999567866325378\n",
      "Epoch 1682, Loss: 0.8533692359924316, Final Batch Loss: 0.2161371111869812\n",
      "Epoch 1683, Loss: 1.1025078296661377, Final Batch Loss: 0.44595417380332947\n",
      "Epoch 1684, Loss: 1.2909128963947296, Final Batch Loss: 0.5350415110588074\n",
      "Epoch 1685, Loss: 1.117081642150879, Final Batch Loss: 0.4838974177837372\n",
      "Epoch 1686, Loss: 0.9587550759315491, Final Batch Loss: 0.31312504410743713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1687, Loss: 0.7732294201850891, Final Batch Loss: 0.10692530870437622\n",
      "Epoch 1688, Loss: 1.0276593565940857, Final Batch Loss: 0.37743058800697327\n",
      "Epoch 1689, Loss: 0.850088819861412, Final Batch Loss: 0.18218432366847992\n",
      "Epoch 1690, Loss: 0.991087019443512, Final Batch Loss: 0.38724520802497864\n",
      "Epoch 1691, Loss: 0.8439629822969437, Final Batch Loss: 0.2166949361562729\n",
      "Epoch 1692, Loss: 1.451501339673996, Final Batch Loss: 0.7520284056663513\n",
      "Epoch 1693, Loss: 1.1613578796386719, Final Batch Loss: 0.5151432752609253\n",
      "Epoch 1694, Loss: 0.9221366345882416, Final Batch Loss: 0.26660335063934326\n",
      "Epoch 1695, Loss: 1.1842636466026306, Final Batch Loss: 0.4415683150291443\n",
      "Epoch 1696, Loss: 0.982129842042923, Final Batch Loss: 0.32333096861839294\n",
      "Epoch 1697, Loss: 0.923810675740242, Final Batch Loss: 0.23616380989551544\n",
      "Epoch 1698, Loss: 1.0877989530563354, Final Batch Loss: 0.39963752031326294\n",
      "Epoch 1699, Loss: 0.90269535779953, Final Batch Loss: 0.2162226140499115\n",
      "Epoch 1700, Loss: 0.9029141962528229, Final Batch Loss: 0.2418268918991089\n",
      "Epoch 1701, Loss: 1.1764458119869232, Final Batch Loss: 0.5479829907417297\n",
      "Epoch 1702, Loss: 1.0299698114395142, Final Batch Loss: 0.38593968749046326\n",
      "Epoch 1703, Loss: 0.9290149658918381, Final Batch Loss: 0.24013753235340118\n",
      "Epoch 1704, Loss: 1.0585185885429382, Final Batch Loss: 0.4013918936252594\n",
      "Epoch 1705, Loss: 1.208291232585907, Final Batch Loss: 0.520637571811676\n",
      "Epoch 1706, Loss: 0.7158962897956371, Final Batch Loss: 0.05361638590693474\n",
      "Epoch 1707, Loss: 0.8220708966255188, Final Batch Loss: 0.1850045621395111\n",
      "Epoch 1708, Loss: 0.851353257894516, Final Batch Loss: 0.19906628131866455\n",
      "Epoch 1709, Loss: 0.8878041952848434, Final Batch Loss: 0.23242859542369843\n",
      "Epoch 1710, Loss: 0.9845902323722839, Final Batch Loss: 0.34365615248680115\n",
      "Epoch 1711, Loss: 1.0983657240867615, Final Batch Loss: 0.4313555657863617\n",
      "Epoch 1712, Loss: 1.003786325454712, Final Batch Loss: 0.3161543011665344\n",
      "Epoch 1713, Loss: 1.1350470185279846, Final Batch Loss: 0.45224887132644653\n",
      "Epoch 1714, Loss: 1.0727897882461548, Final Batch Loss: 0.3581247329711914\n",
      "Epoch 1715, Loss: 0.8938970416784286, Final Batch Loss: 0.2452961951494217\n",
      "Epoch 1716, Loss: 1.1205209493637085, Final Batch Loss: 0.4933471083641052\n",
      "Epoch 1717, Loss: 0.9334693849086761, Final Batch Loss: 0.27321168780326843\n",
      "Epoch 1718, Loss: 1.058872252702713, Final Batch Loss: 0.31450802087783813\n",
      "Epoch 1719, Loss: 1.2461394667625427, Final Batch Loss: 0.5382542610168457\n",
      "Epoch 1720, Loss: 1.2407070696353912, Final Batch Loss: 0.5673387050628662\n",
      "Epoch 1721, Loss: 0.7463324815034866, Final Batch Loss: 0.13841138780117035\n",
      "Epoch 1722, Loss: 0.8657804429531097, Final Batch Loss: 0.281902939081192\n",
      "Epoch 1723, Loss: 1.0829299986362457, Final Batch Loss: 0.3846243917942047\n",
      "Epoch 1724, Loss: 1.008110672235489, Final Batch Loss: 0.36400318145751953\n",
      "Epoch 1725, Loss: 1.321487545967102, Final Batch Loss: 0.6762781143188477\n",
      "Epoch 1726, Loss: 0.8927093595266342, Final Batch Loss: 0.230515256524086\n",
      "Epoch 1727, Loss: 0.9130728840827942, Final Batch Loss: 0.2758745849132538\n",
      "Epoch 1728, Loss: 0.9618909955024719, Final Batch Loss: 0.2901977300643921\n",
      "Epoch 1729, Loss: 0.9635668396949768, Final Batch Loss: 0.31940165162086487\n",
      "Epoch 1730, Loss: 1.02755206823349, Final Batch Loss: 0.40050992369651794\n",
      "Epoch 1731, Loss: 1.025749921798706, Final Batch Loss: 0.3557814061641693\n",
      "Epoch 1732, Loss: 1.0947953760623932, Final Batch Loss: 0.37296774983406067\n",
      "Epoch 1733, Loss: 0.9895335733890533, Final Batch Loss: 0.33717331290245056\n",
      "Epoch 1734, Loss: 0.9729078412055969, Final Batch Loss: 0.3729589879512787\n",
      "Epoch 1735, Loss: 1.1669056117534637, Final Batch Loss: 0.512077808380127\n",
      "Epoch 1736, Loss: 0.8910723179578781, Final Batch Loss: 0.2073298841714859\n",
      "Epoch 1737, Loss: 0.903173178434372, Final Batch Loss: 0.29419076442718506\n",
      "Epoch 1738, Loss: 0.9044036120176315, Final Batch Loss: 0.21194200217723846\n",
      "Epoch 1739, Loss: 1.1289487481117249, Final Batch Loss: 0.4581609070301056\n",
      "Epoch 1740, Loss: 1.092254400253296, Final Batch Loss: 0.4646177291870117\n",
      "Epoch 1741, Loss: 0.8479994088411331, Final Batch Loss: 0.1419251710176468\n",
      "Epoch 1742, Loss: 0.8393806666135788, Final Batch Loss: 0.1937761753797531\n",
      "Epoch 1743, Loss: 0.9231995642185211, Final Batch Loss: 0.2646035850048065\n",
      "Epoch 1744, Loss: 0.9307514429092407, Final Batch Loss: 0.28573372960090637\n",
      "Epoch 1745, Loss: 0.9595910906791687, Final Batch Loss: 0.2855079472064972\n",
      "Epoch 1746, Loss: 1.0524272918701172, Final Batch Loss: 0.3376651406288147\n",
      "Epoch 1747, Loss: 0.8411138653755188, Final Batch Loss: 0.21290874481201172\n",
      "Epoch 1748, Loss: 1.056537926197052, Final Batch Loss: 0.2910330295562744\n",
      "Epoch 1749, Loss: 0.9813628196716309, Final Batch Loss: 0.3366605341434479\n",
      "Epoch 1750, Loss: 1.007227748632431, Final Batch Loss: 0.3270173966884613\n",
      "Epoch 1751, Loss: 1.2568242847919464, Final Batch Loss: 0.582410991191864\n",
      "Epoch 1752, Loss: 1.0046362578868866, Final Batch Loss: 0.3851684331893921\n",
      "Epoch 1753, Loss: 1.0493171513080597, Final Batch Loss: 0.423989862203598\n",
      "Epoch 1754, Loss: 0.9452211856842041, Final Batch Loss: 0.27673399448394775\n",
      "Epoch 1755, Loss: 1.0972146391868591, Final Batch Loss: 0.3858981430530548\n",
      "Epoch 1756, Loss: 1.1637412011623383, Final Batch Loss: 0.44991397857666016\n",
      "Epoch 1757, Loss: 1.2131017744541168, Final Batch Loss: 0.571611225605011\n",
      "Epoch 1758, Loss: 1.0575771629810333, Final Batch Loss: 0.3964882791042328\n",
      "Epoch 1759, Loss: 0.9597388207912445, Final Batch Loss: 0.3078565299510956\n",
      "Epoch 1760, Loss: 1.1201948523521423, Final Batch Loss: 0.4688943028450012\n",
      "Epoch 1761, Loss: 0.989818662405014, Final Batch Loss: 0.31322750449180603\n",
      "Epoch 1762, Loss: 0.9693538546562195, Final Batch Loss: 0.28045061230659485\n",
      "Epoch 1763, Loss: 1.1746711134910583, Final Batch Loss: 0.51380455493927\n",
      "Epoch 1764, Loss: 0.8432491272687912, Final Batch Loss: 0.22783520817756653\n",
      "Epoch 1765, Loss: 1.095950961112976, Final Batch Loss: 0.4070594906806946\n",
      "Epoch 1766, Loss: 1.1824036836624146, Final Batch Loss: 0.48300236463546753\n",
      "Epoch 1767, Loss: 0.8569798469543457, Final Batch Loss: 0.1012929379940033\n",
      "Epoch 1768, Loss: 1.083446204662323, Final Batch Loss: 0.3553503453731537\n",
      "Epoch 1769, Loss: 0.9189918637275696, Final Batch Loss: 0.2592030167579651\n",
      "Epoch 1770, Loss: 0.9061786383390427, Final Batch Loss: 0.20600686967372894\n",
      "Epoch 1771, Loss: 1.1142301857471466, Final Batch Loss: 0.5202257037162781\n",
      "Epoch 1772, Loss: 0.8343754708766937, Final Batch Loss: 0.23352813720703125\n",
      "Epoch 1773, Loss: 1.2592531740665436, Final Batch Loss: 0.5353434681892395\n",
      "Epoch 1774, Loss: 1.06937974691391, Final Batch Loss: 0.4279645085334778\n",
      "Epoch 1775, Loss: 0.9385619163513184, Final Batch Loss: 0.29567766189575195\n",
      "Epoch 1776, Loss: 0.9330088794231415, Final Batch Loss: 0.2945789396762848\n",
      "Epoch 1777, Loss: 0.8053057789802551, Final Batch Loss: 0.15150481462478638\n",
      "Epoch 1778, Loss: 1.1233634054660797, Final Batch Loss: 0.4701486825942993\n",
      "Epoch 1779, Loss: 1.0083736777305603, Final Batch Loss: 0.2901526093482971\n",
      "Epoch 1780, Loss: 0.8313135802745819, Final Batch Loss: 0.25482824444770813\n",
      "Epoch 1781, Loss: 0.8570138812065125, Final Batch Loss: 0.2142176628112793\n",
      "Epoch 1782, Loss: 0.8718851506710052, Final Batch Loss: 0.2436005175113678\n",
      "Epoch 1783, Loss: 1.2075700461864471, Final Batch Loss: 0.5319253206253052\n",
      "Epoch 1784, Loss: 1.242446392774582, Final Batch Loss: 0.6437535285949707\n",
      "Epoch 1785, Loss: 1.0814342498779297, Final Batch Loss: 0.45929843187332153\n",
      "Epoch 1786, Loss: 0.9363207817077637, Final Batch Loss: 0.2806919813156128\n",
      "Epoch 1787, Loss: 0.9998844861984253, Final Batch Loss: 0.3722178339958191\n",
      "Epoch 1788, Loss: 1.0509616434574127, Final Batch Loss: 0.40423068404197693\n",
      "Epoch 1789, Loss: 1.040235847234726, Final Batch Loss: 0.41166266798973083\n",
      "Epoch 1790, Loss: 1.1881552934646606, Final Batch Loss: 0.5668371319770813\n",
      "Epoch 1791, Loss: 0.8341017514467239, Final Batch Loss: 0.18261800706386566\n",
      "Epoch 1792, Loss: 0.8994452804327011, Final Batch Loss: 0.21074418723583221\n",
      "Epoch 1793, Loss: 1.0618960559368134, Final Batch Loss: 0.41915297508239746\n",
      "Epoch 1794, Loss: 0.8024890869855881, Final Batch Loss: 0.19140948355197906\n",
      "Epoch 1795, Loss: 1.1004750430583954, Final Batch Loss: 0.4344015121459961\n",
      "Epoch 1796, Loss: 0.9734840989112854, Final Batch Loss: 0.31502190232276917\n",
      "Epoch 1797, Loss: 1.0574168860912323, Final Batch Loss: 0.44288069009780884\n",
      "Epoch 1798, Loss: 1.1253574192523956, Final Batch Loss: 0.4582391679286957\n",
      "Epoch 1799, Loss: 0.9533599317073822, Final Batch Loss: 0.27678167819976807\n",
      "Epoch 1800, Loss: 1.072401374578476, Final Batch Loss: 0.3728732466697693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1801, Loss: 1.05596524477005, Final Batch Loss: 0.3878587484359741\n",
      "Epoch 1802, Loss: 0.9640121757984161, Final Batch Loss: 0.3263091742992401\n",
      "Epoch 1803, Loss: 0.8253568112850189, Final Batch Loss: 0.15159016847610474\n",
      "Epoch 1804, Loss: 0.8898563534021378, Final Batch Loss: 0.2040795534849167\n",
      "Epoch 1805, Loss: 0.9433812499046326, Final Batch Loss: 0.3009551763534546\n",
      "Epoch 1806, Loss: 0.8834883868694305, Final Batch Loss: 0.27261030673980713\n",
      "Epoch 1807, Loss: 0.8754012733697891, Final Batch Loss: 0.22507022321224213\n",
      "Epoch 1808, Loss: 1.1540248095989227, Final Batch Loss: 0.5374440550804138\n",
      "Epoch 1809, Loss: 0.9875480532646179, Final Batch Loss: 0.3298966884613037\n",
      "Epoch 1810, Loss: 0.9165343940258026, Final Batch Loss: 0.26173630356788635\n",
      "Epoch 1811, Loss: 0.8819365054368973, Final Batch Loss: 0.216328427195549\n",
      "Epoch 1812, Loss: 0.8774794787168503, Final Batch Loss: 0.1776166707277298\n",
      "Epoch 1813, Loss: 0.8074603974819183, Final Batch Loss: 0.15578266978263855\n",
      "Epoch 1814, Loss: 0.8669591099023819, Final Batch Loss: 0.16528944671154022\n",
      "Epoch 1815, Loss: 0.7580460011959076, Final Batch Loss: 0.15498816967010498\n",
      "Epoch 1816, Loss: 0.8818772435188293, Final Batch Loss: 0.28357818722724915\n",
      "Epoch 1817, Loss: 1.2037046253681183, Final Batch Loss: 0.5862860679626465\n",
      "Epoch 1818, Loss: 0.9249384105205536, Final Batch Loss: 0.309334933757782\n",
      "Epoch 1819, Loss: 1.058664619922638, Final Batch Loss: 0.40433263778686523\n",
      "Epoch 1820, Loss: 1.038437694311142, Final Batch Loss: 0.3609424829483032\n",
      "Epoch 1821, Loss: 1.0032873451709747, Final Batch Loss: 0.33918413519859314\n",
      "Epoch 1822, Loss: 0.9461112916469574, Final Batch Loss: 0.2920544147491455\n",
      "Epoch 1823, Loss: 1.1660336256027222, Final Batch Loss: 0.470121830701828\n",
      "Epoch 1824, Loss: 0.9417357444763184, Final Batch Loss: 0.26555073261260986\n",
      "Epoch 1825, Loss: 0.7405308187007904, Final Batch Loss: 0.126807302236557\n",
      "Epoch 1826, Loss: 1.1314599215984344, Final Batch Loss: 0.4333917796611786\n",
      "Epoch 1827, Loss: 1.2514130771160126, Final Batch Loss: 0.5734530091285706\n",
      "Epoch 1828, Loss: 0.95359867811203, Final Batch Loss: 0.2937048077583313\n",
      "Epoch 1829, Loss: 0.8069191575050354, Final Batch Loss: 0.14352577924728394\n",
      "Epoch 1830, Loss: 0.8543976098299026, Final Batch Loss: 0.23919175565242767\n",
      "Epoch 1831, Loss: 0.9567410349845886, Final Batch Loss: 0.2826874554157257\n",
      "Epoch 1832, Loss: 0.9151550829410553, Final Batch Loss: 0.33351507782936096\n",
      "Epoch 1833, Loss: 0.7827523052692413, Final Batch Loss: 0.15479892492294312\n",
      "Epoch 1834, Loss: 0.843540370464325, Final Batch Loss: 0.20837509632110596\n",
      "Epoch 1835, Loss: 1.1273333728313446, Final Batch Loss: 0.5186114311218262\n",
      "Epoch 1836, Loss: 0.9178198575973511, Final Batch Loss: 0.297906756401062\n",
      "Epoch 1837, Loss: 1.0429862588644028, Final Batch Loss: 0.4356924891471863\n",
      "Epoch 1838, Loss: 0.9091820418834686, Final Batch Loss: 0.3202444016933441\n",
      "Epoch 1839, Loss: 1.086291640996933, Final Batch Loss: 0.4434231221675873\n",
      "Epoch 1840, Loss: 0.9320110976696014, Final Batch Loss: 0.2695830762386322\n",
      "Epoch 1841, Loss: 0.8403609693050385, Final Batch Loss: 0.2071583867073059\n",
      "Epoch 1842, Loss: 0.9442835748195648, Final Batch Loss: 0.32994621992111206\n",
      "Epoch 1843, Loss: 1.0668765008449554, Final Batch Loss: 0.37019437551498413\n",
      "Epoch 1844, Loss: 1.2473840117454529, Final Batch Loss: 0.5867329835891724\n",
      "Epoch 1845, Loss: 0.9899218082427979, Final Batch Loss: 0.3294830024242401\n",
      "Epoch 1846, Loss: 0.9365159422159195, Final Batch Loss: 0.23621635138988495\n",
      "Epoch 1847, Loss: 0.7434650361537933, Final Batch Loss: 0.15734851360321045\n",
      "Epoch 1848, Loss: 0.796651229262352, Final Batch Loss: 0.13150669634342194\n",
      "Epoch 1849, Loss: 0.9928643703460693, Final Batch Loss: 0.35151609778404236\n",
      "Epoch 1850, Loss: 0.909860759973526, Final Batch Loss: 0.32928988337516785\n",
      "Epoch 1851, Loss: 1.2038547098636627, Final Batch Loss: 0.5389430522918701\n",
      "Epoch 1852, Loss: 0.8278311640024185, Final Batch Loss: 0.1817736178636551\n",
      "Epoch 1853, Loss: 0.7766309902071953, Final Batch Loss: 0.09949149936437607\n",
      "Epoch 1854, Loss: 0.9104918539524078, Final Batch Loss: 0.3010372221469879\n",
      "Epoch 1855, Loss: 0.9260013699531555, Final Batch Loss: 0.3270098567008972\n",
      "Epoch 1856, Loss: 0.8758344352245331, Final Batch Loss: 0.2895665466785431\n",
      "Epoch 1857, Loss: 1.1367164254188538, Final Batch Loss: 0.5301969647407532\n",
      "Epoch 1858, Loss: 0.7325367778539658, Final Batch Loss: 0.11761163175106049\n",
      "Epoch 1859, Loss: 0.958564281463623, Final Batch Loss: 0.3013788163661957\n",
      "Epoch 1860, Loss: 0.7780081182718277, Final Batch Loss: 0.17345111072063446\n",
      "Epoch 1861, Loss: 0.8947007358074188, Final Batch Loss: 0.26685628294944763\n",
      "Epoch 1862, Loss: 0.8394975662231445, Final Batch Loss: 0.18654054403305054\n",
      "Epoch 1863, Loss: 0.758221372961998, Final Batch Loss: 0.13904546201229095\n",
      "Epoch 1864, Loss: 1.1428276002407074, Final Batch Loss: 0.5617799162864685\n",
      "Epoch 1865, Loss: 1.2275672852993011, Final Batch Loss: 0.605786144733429\n",
      "Epoch 1866, Loss: 1.0361090004444122, Final Batch Loss: 0.3870735764503479\n",
      "Epoch 1867, Loss: 1.0719119906425476, Final Batch Loss: 0.4454309046268463\n",
      "Epoch 1868, Loss: 0.8141983598470688, Final Batch Loss: 0.2279951125383377\n",
      "Epoch 1869, Loss: 0.8552985787391663, Final Batch Loss: 0.16988858580589294\n",
      "Epoch 1870, Loss: 1.2439014613628387, Final Batch Loss: 0.578234851360321\n",
      "Epoch 1871, Loss: 0.887896716594696, Final Batch Loss: 0.24690386652946472\n",
      "Epoch 1872, Loss: 0.8856223970651627, Final Batch Loss: 0.2483106106519699\n",
      "Epoch 1873, Loss: 0.8353920131921768, Final Batch Loss: 0.20093445479869843\n",
      "Epoch 1874, Loss: 0.7874016463756561, Final Batch Loss: 0.1633150577545166\n",
      "Epoch 1875, Loss: 1.0617367327213287, Final Batch Loss: 0.3915800154209137\n",
      "Epoch 1876, Loss: 0.912669762969017, Final Batch Loss: 0.24683637917041779\n",
      "Epoch 1877, Loss: 0.9705173373222351, Final Batch Loss: 0.2694234251976013\n",
      "Epoch 1878, Loss: 0.8865710198879242, Final Batch Loss: 0.25426965951919556\n",
      "Epoch 1879, Loss: 0.7193703129887581, Final Batch Loss: 0.08676359802484512\n",
      "Epoch 1880, Loss: 0.8826499283313751, Final Batch Loss: 0.34843578934669495\n",
      "Epoch 1881, Loss: 0.8003601580858231, Final Batch Loss: 0.2205507606267929\n",
      "Epoch 1882, Loss: 0.9312739670276642, Final Batch Loss: 0.34245821833610535\n",
      "Epoch 1883, Loss: 0.8123336583375931, Final Batch Loss: 0.13921453058719635\n",
      "Epoch 1884, Loss: 0.8070841580629349, Final Batch Loss: 0.1516057699918747\n",
      "Epoch 1885, Loss: 0.8083485066890717, Final Batch Loss: 0.18633297085762024\n",
      "Epoch 1886, Loss: 0.8722726106643677, Final Batch Loss: 0.2600269913673401\n",
      "Epoch 1887, Loss: 0.9417304843664169, Final Batch Loss: 0.23896275460720062\n",
      "Epoch 1888, Loss: 0.8666377365589142, Final Batch Loss: 0.27543124556541443\n",
      "Epoch 1889, Loss: 0.8375089913606644, Final Batch Loss: 0.2232336550951004\n",
      "Epoch 1890, Loss: 0.9117923676967621, Final Batch Loss: 0.29069316387176514\n",
      "Epoch 1891, Loss: 0.9166823625564575, Final Batch Loss: 0.28453436493873596\n",
      "Epoch 1892, Loss: 0.8171350359916687, Final Batch Loss: 0.1765199601650238\n",
      "Epoch 1893, Loss: 0.8678471446037292, Final Batch Loss: 0.2253931760787964\n",
      "Epoch 1894, Loss: 0.8926867246627808, Final Batch Loss: 0.2010946273803711\n",
      "Epoch 1895, Loss: 0.8076538294553757, Final Batch Loss: 0.23931895196437836\n",
      "Epoch 1896, Loss: 0.8357972800731659, Final Batch Loss: 0.17881149053573608\n",
      "Epoch 1897, Loss: 0.7970692962408066, Final Batch Loss: 0.2182110995054245\n",
      "Epoch 1898, Loss: 0.7522756606340408, Final Batch Loss: 0.17607636749744415\n",
      "Epoch 1899, Loss: 0.8930045664310455, Final Batch Loss: 0.2971396744251251\n",
      "Epoch 1900, Loss: 0.9258198738098145, Final Batch Loss: 0.27121520042419434\n",
      "Epoch 1901, Loss: 0.7830242365598679, Final Batch Loss: 0.2050100713968277\n",
      "Epoch 1902, Loss: 0.8841400742530823, Final Batch Loss: 0.3269321024417877\n",
      "Epoch 1903, Loss: 1.3852224051952362, Final Batch Loss: 0.7358286380767822\n",
      "Epoch 1904, Loss: 0.8583343178033829, Final Batch Loss: 0.24251942336559296\n",
      "Epoch 1905, Loss: 0.8995432257652283, Final Batch Loss: 0.2681749761104584\n",
      "Epoch 1906, Loss: 0.9975904822349548, Final Batch Loss: 0.3441791534423828\n",
      "Epoch 1907, Loss: 0.9692970216274261, Final Batch Loss: 0.35637155175209045\n",
      "Epoch 1908, Loss: 1.2080312371253967, Final Batch Loss: 0.5648865699768066\n",
      "Epoch 1909, Loss: 0.7983975559473038, Final Batch Loss: 0.15330399572849274\n",
      "Epoch 1910, Loss: 0.9730931222438812, Final Batch Loss: 0.3153158128261566\n",
      "Epoch 1911, Loss: 0.7727286517620087, Final Batch Loss: 0.17669042944908142\n",
      "Epoch 1912, Loss: 0.8689924031496048, Final Batch Loss: 0.23302192986011505\n",
      "Epoch 1913, Loss: 1.0308548212051392, Final Batch Loss: 0.39061427116394043\n",
      "Epoch 1914, Loss: 0.8906086832284927, Final Batch Loss: 0.219831183552742\n",
      "Epoch 1915, Loss: 1.001800298690796, Final Batch Loss: 0.38166189193725586\n",
      "Epoch 1916, Loss: 0.8612705767154694, Final Batch Loss: 0.25991567969322205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1917, Loss: 1.2281290292739868, Final Batch Loss: 0.5346934795379639\n",
      "Epoch 1918, Loss: 0.9459481239318848, Final Batch Loss: 0.28031986951828003\n",
      "Epoch 1919, Loss: 0.7907997369766235, Final Batch Loss: 0.17806032299995422\n",
      "Epoch 1920, Loss: 0.9471690058708191, Final Batch Loss: 0.3344317078590393\n",
      "Epoch 1921, Loss: 0.941701740026474, Final Batch Loss: 0.3063453733921051\n",
      "Epoch 1922, Loss: 1.2277179062366486, Final Batch Loss: 0.6009487509727478\n",
      "Epoch 1923, Loss: 0.9642324447631836, Final Batch Loss: 0.3270839750766754\n",
      "Epoch 1924, Loss: 1.0113311409950256, Final Batch Loss: 0.3862890601158142\n",
      "Epoch 1925, Loss: 0.9882045090198517, Final Batch Loss: 0.2668260633945465\n",
      "Epoch 1926, Loss: 0.8426689505577087, Final Batch Loss: 0.25129225850105286\n",
      "Epoch 1927, Loss: 1.3768118619918823, Final Batch Loss: 0.6781738996505737\n",
      "Epoch 1928, Loss: 1.1720945239067078, Final Batch Loss: 0.5629417896270752\n",
      "Epoch 1929, Loss: 0.8641682863235474, Final Batch Loss: 0.26915231347084045\n",
      "Epoch 1930, Loss: 1.0649033188819885, Final Batch Loss: 0.44719043374061584\n",
      "Epoch 1931, Loss: 0.8151433318853378, Final Batch Loss: 0.18745671212673187\n",
      "Epoch 1932, Loss: 1.0347755551338196, Final Batch Loss: 0.3716186583042145\n",
      "Epoch 1933, Loss: 1.0168289542198181, Final Batch Loss: 0.32711219787597656\n",
      "Epoch 1934, Loss: 0.9804748892784119, Final Batch Loss: 0.339959055185318\n",
      "Epoch 1935, Loss: 0.9798312783241272, Final Batch Loss: 0.4091593027114868\n",
      "Epoch 1936, Loss: 1.3048421144485474, Final Batch Loss: 0.6485059261322021\n",
      "Epoch 1937, Loss: 0.9300378262996674, Final Batch Loss: 0.32940733432769775\n",
      "Epoch 1938, Loss: 1.0047253370285034, Final Batch Loss: 0.3934277296066284\n",
      "Epoch 1939, Loss: 0.888076439499855, Final Batch Loss: 0.24788178503513336\n",
      "Epoch 1940, Loss: 0.9506627321243286, Final Batch Loss: 0.24803674221038818\n",
      "Epoch 1941, Loss: 1.107172667980194, Final Batch Loss: 0.43865740299224854\n",
      "Epoch 1942, Loss: 0.805615097284317, Final Batch Loss: 0.15525025129318237\n",
      "Epoch 1943, Loss: 1.0312799513339996, Final Batch Loss: 0.4237092137336731\n",
      "Epoch 1944, Loss: 0.8656564652919769, Final Batch Loss: 0.2593127489089966\n",
      "Epoch 1945, Loss: 0.9533485472202301, Final Batch Loss: 0.300103098154068\n",
      "Epoch 1946, Loss: 0.8914940059185028, Final Batch Loss: 0.27856749296188354\n",
      "Epoch 1947, Loss: 0.9088891446590424, Final Batch Loss: 0.3360746204853058\n",
      "Epoch 1948, Loss: 0.9100761115550995, Final Batch Loss: 0.2845200002193451\n",
      "Epoch 1949, Loss: 0.803482711315155, Final Batch Loss: 0.1709001660346985\n",
      "Epoch 1950, Loss: 0.999643474817276, Final Batch Loss: 0.37276366353034973\n",
      "Epoch 1951, Loss: 0.9245864152908325, Final Batch Loss: 0.39215344190597534\n",
      "Epoch 1952, Loss: 0.9780411720275879, Final Batch Loss: 0.3432888090610504\n",
      "Epoch 1953, Loss: 1.0386704802513123, Final Batch Loss: 0.40196484327316284\n",
      "Epoch 1954, Loss: 0.8994853794574738, Final Batch Loss: 0.27201539278030396\n",
      "Epoch 1955, Loss: 0.8940370082855225, Final Batch Loss: 0.2642616927623749\n",
      "Epoch 1956, Loss: 0.9841707348823547, Final Batch Loss: 0.38292452692985535\n",
      "Epoch 1957, Loss: 1.1528491377830505, Final Batch Loss: 0.5402519106864929\n",
      "Epoch 1958, Loss: 1.0142517983913422, Final Batch Loss: 0.428000807762146\n",
      "Epoch 1959, Loss: 0.8484567701816559, Final Batch Loss: 0.20358318090438843\n",
      "Epoch 1960, Loss: 0.6211405210196972, Final Batch Loss: 0.05491913482546806\n",
      "Epoch 1961, Loss: 1.2724201381206512, Final Batch Loss: 0.6219048500061035\n",
      "Epoch 1962, Loss: 0.7970767617225647, Final Batch Loss: 0.1530226171016693\n",
      "Epoch 1963, Loss: 1.0132436007261276, Final Batch Loss: 0.4170026481151581\n",
      "Epoch 1964, Loss: 0.8275335729122162, Final Batch Loss: 0.16921305656433105\n",
      "Epoch 1965, Loss: 1.1766394674777985, Final Batch Loss: 0.5706067085266113\n",
      "Epoch 1966, Loss: 0.7495133280754089, Final Batch Loss: 0.16600704193115234\n",
      "Epoch 1967, Loss: 1.0149469375610352, Final Batch Loss: 0.3882278501987457\n",
      "Epoch 1968, Loss: 0.711688369512558, Final Batch Loss: 0.12148088216781616\n",
      "Epoch 1969, Loss: 0.9458974003791809, Final Batch Loss: 0.27549004554748535\n",
      "Epoch 1970, Loss: 0.8976688981056213, Final Batch Loss: 0.2858181893825531\n",
      "Epoch 1971, Loss: 0.847282201051712, Final Batch Loss: 0.2523055672645569\n",
      "Epoch 1972, Loss: 0.88346067070961, Final Batch Loss: 0.2501222789287567\n",
      "Epoch 1973, Loss: 1.2655541002750397, Final Batch Loss: 0.6179435849189758\n",
      "Epoch 1974, Loss: 0.8047229498624802, Final Batch Loss: 0.12003014981746674\n",
      "Epoch 1975, Loss: 0.7830154895782471, Final Batch Loss: 0.18209925293922424\n",
      "Epoch 1976, Loss: 0.8876204490661621, Final Batch Loss: 0.274005651473999\n",
      "Epoch 1977, Loss: 0.9319367706775665, Final Batch Loss: 0.3504577577114105\n",
      "Epoch 1978, Loss: 0.9051467180252075, Final Batch Loss: 0.3619607388973236\n",
      "Epoch 1979, Loss: 0.8724134266376495, Final Batch Loss: 0.2942632734775543\n",
      "Epoch 1980, Loss: 0.9181342720985413, Final Batch Loss: 0.3072810769081116\n",
      "Epoch 1981, Loss: 0.8295661509037018, Final Batch Loss: 0.24963074922561646\n",
      "Epoch 1982, Loss: 0.9294676780700684, Final Batch Loss: 0.33652162551879883\n",
      "Epoch 1983, Loss: 0.8254257589578629, Final Batch Loss: 0.20974107086658478\n",
      "Epoch 1984, Loss: 0.8420382440090179, Final Batch Loss: 0.22757399082183838\n",
      "Epoch 1985, Loss: 0.8725237846374512, Final Batch Loss: 0.3004405200481415\n",
      "Epoch 1986, Loss: 0.9095591902732849, Final Batch Loss: 0.29547539353370667\n",
      "Epoch 1987, Loss: 0.9785298109054565, Final Batch Loss: 0.3522244393825531\n",
      "Epoch 1988, Loss: 0.876942977309227, Final Batch Loss: 0.22637848556041718\n",
      "Epoch 1989, Loss: 0.805069163441658, Final Batch Loss: 0.12336395680904388\n",
      "Epoch 1990, Loss: 0.892524391412735, Final Batch Loss: 0.23424196243286133\n",
      "Epoch 1991, Loss: 1.2107035517692566, Final Batch Loss: 0.5331678986549377\n",
      "Epoch 1992, Loss: 0.9860218465328217, Final Batch Loss: 0.3310316801071167\n",
      "Epoch 1993, Loss: 0.7762870788574219, Final Batch Loss: 0.20240670442581177\n",
      "Epoch 1994, Loss: 0.8054143488407135, Final Batch Loss: 0.22189584374427795\n",
      "Epoch 1995, Loss: 0.9329409003257751, Final Batch Loss: 0.24280592799186707\n",
      "Epoch 1996, Loss: 1.0618689358234406, Final Batch Loss: 0.45443007349967957\n",
      "Epoch 1997, Loss: 0.8554367274045944, Final Batch Loss: 0.22694449126720428\n",
      "Epoch 1998, Loss: 0.6987995952367783, Final Batch Loss: 0.11991675198078156\n",
      "Epoch 1999, Loss: 0.9769115149974823, Final Batch Loss: 0.3781512379646301\n",
      "Epoch 2000, Loss: 0.861864447593689, Final Batch Loss: 0.2622665762901306\n",
      "Epoch 2001, Loss: 0.7769582867622375, Final Batch Loss: 0.15569734573364258\n",
      "Epoch 2002, Loss: 1.044306755065918, Final Batch Loss: 0.4699210226535797\n",
      "Epoch 2003, Loss: 0.8638980835676193, Final Batch Loss: 0.24734406173229218\n",
      "Epoch 2004, Loss: 0.7862463146448135, Final Batch Loss: 0.2318473607301712\n",
      "Epoch 2005, Loss: 0.6830848313402385, Final Batch Loss: 0.003740445477887988\n",
      "Epoch 2006, Loss: 0.8217506259679794, Final Batch Loss: 0.21859101951122284\n",
      "Epoch 2007, Loss: 1.0499069392681122, Final Batch Loss: 0.36118701100349426\n",
      "Epoch 2008, Loss: 0.9821138679981232, Final Batch Loss: 0.35887038707733154\n",
      "Epoch 2009, Loss: 1.1810275316238403, Final Batch Loss: 0.6051196455955505\n",
      "Epoch 2010, Loss: 0.736025482416153, Final Batch Loss: 0.15027377009391785\n",
      "Epoch 2011, Loss: 0.7967432737350464, Final Batch Loss: 0.08015921711921692\n",
      "Epoch 2012, Loss: 1.166563093662262, Final Batch Loss: 0.520193874835968\n",
      "Epoch 2013, Loss: 1.0405264496803284, Final Batch Loss: 0.3809483051300049\n",
      "Epoch 2014, Loss: 0.7774413824081421, Final Batch Loss: 0.23167991638183594\n",
      "Epoch 2015, Loss: 1.0957366824150085, Final Batch Loss: 0.47392919659614563\n",
      "Epoch 2016, Loss: 0.8143274635076523, Final Batch Loss: 0.18940488994121552\n",
      "Epoch 2017, Loss: 0.9044114053249359, Final Batch Loss: 0.3035033941268921\n",
      "Epoch 2018, Loss: 0.9595354497432709, Final Batch Loss: 0.3169197142124176\n",
      "Epoch 2019, Loss: 0.6642402336001396, Final Batch Loss: 0.11437001079320908\n",
      "Epoch 2020, Loss: 0.9767182469367981, Final Batch Loss: 0.3806108236312866\n",
      "Epoch 2021, Loss: 1.0433251559734344, Final Batch Loss: 0.3993041515350342\n",
      "Epoch 2022, Loss: 0.788789376616478, Final Batch Loss: 0.1817552000284195\n",
      "Epoch 2023, Loss: 0.8153243511915207, Final Batch Loss: 0.2275453358888626\n",
      "Epoch 2024, Loss: 0.8008404672145844, Final Batch Loss: 0.18649274110794067\n",
      "Epoch 2025, Loss: 1.3088977634906769, Final Batch Loss: 0.7377264499664307\n",
      "Epoch 2026, Loss: 0.7047936618328094, Final Batch Loss: 0.06128743290901184\n",
      "Epoch 2027, Loss: 1.1254521310329437, Final Batch Loss: 0.5292431116104126\n",
      "Epoch 2028, Loss: 0.9590134024620056, Final Batch Loss: 0.3327060639858246\n",
      "Epoch 2029, Loss: 0.7723661810159683, Final Batch Loss: 0.12511961162090302\n",
      "Epoch 2030, Loss: 0.8358648419380188, Final Batch Loss: 0.25557568669319153\n",
      "Epoch 2031, Loss: 0.943024605512619, Final Batch Loss: 0.3635804355144501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2032, Loss: 0.7416348531842232, Final Batch Loss: 0.11695531755685806\n",
      "Epoch 2033, Loss: 0.9067796915769577, Final Batch Loss: 0.24539496004581451\n",
      "Epoch 2034, Loss: 0.847419798374176, Final Batch Loss: 0.25690945982933044\n",
      "Epoch 2035, Loss: 0.9602262675762177, Final Batch Loss: 0.3845573365688324\n",
      "Epoch 2036, Loss: 1.0505026280879974, Final Batch Loss: 0.4036780595779419\n",
      "Epoch 2037, Loss: 1.0237575471401215, Final Batch Loss: 0.37652453780174255\n",
      "Epoch 2038, Loss: 1.0609375834465027, Final Batch Loss: 0.5039337277412415\n",
      "Epoch 2039, Loss: 0.9459520578384399, Final Batch Loss: 0.3483413755893707\n",
      "Epoch 2040, Loss: 0.7322703450918198, Final Batch Loss: 0.13510529696941376\n",
      "Epoch 2041, Loss: 0.7054150700569153, Final Batch Loss: 0.11261296272277832\n",
      "Epoch 2042, Loss: 0.8793597519397736, Final Batch Loss: 0.27489185333251953\n",
      "Epoch 2043, Loss: 1.0635656118392944, Final Batch Loss: 0.47109824419021606\n",
      "Epoch 2044, Loss: 0.9637636244297028, Final Batch Loss: 0.3976792097091675\n",
      "Epoch 2045, Loss: 0.7662872225046158, Final Batch Loss: 0.19629625976085663\n",
      "Epoch 2046, Loss: 1.1906517148017883, Final Batch Loss: 0.5922651290893555\n",
      "Epoch 2047, Loss: 0.6630262359976768, Final Batch Loss: 0.05143814533948898\n",
      "Epoch 2048, Loss: 1.0562866032123566, Final Batch Loss: 0.4592368006706238\n",
      "Epoch 2049, Loss: 0.8240981549024582, Final Batch Loss: 0.24155832827091217\n",
      "Epoch 2050, Loss: 1.0876285135746002, Final Batch Loss: 0.5383129715919495\n",
      "Epoch 2051, Loss: 0.7740522027015686, Final Batch Loss: 0.2074994593858719\n",
      "Epoch 2052, Loss: 1.0648450553417206, Final Batch Loss: 0.44779977202415466\n",
      "Epoch 2053, Loss: 0.9043404459953308, Final Batch Loss: 0.22452494502067566\n",
      "Epoch 2054, Loss: 1.0974971950054169, Final Batch Loss: 0.4167215824127197\n",
      "Epoch 2055, Loss: 1.0323107242584229, Final Batch Loss: 0.4334809184074402\n",
      "Epoch 2056, Loss: 0.7453419715166092, Final Batch Loss: 0.15972809493541718\n",
      "Epoch 2057, Loss: 0.8590834140777588, Final Batch Loss: 0.25428542494773865\n",
      "Epoch 2058, Loss: 0.9932251870632172, Final Batch Loss: 0.4090084135532379\n",
      "Epoch 2059, Loss: 0.9509719014167786, Final Batch Loss: 0.3271351456642151\n",
      "Epoch 2060, Loss: 0.7619742304086685, Final Batch Loss: 0.18335752189159393\n",
      "Epoch 2061, Loss: 0.8203136622905731, Final Batch Loss: 0.26826152205467224\n",
      "Epoch 2062, Loss: 0.926933228969574, Final Batch Loss: 0.3603079319000244\n",
      "Epoch 2063, Loss: 1.0527673661708832, Final Batch Loss: 0.41639652848243713\n",
      "Epoch 2064, Loss: 1.19119393825531, Final Batch Loss: 0.5861140489578247\n",
      "Epoch 2065, Loss: 0.8284896910190582, Final Batch Loss: 0.18497774004936218\n",
      "Epoch 2066, Loss: 0.6866684406995773, Final Batch Loss: 0.14898575842380524\n",
      "Epoch 2067, Loss: 1.0205075144767761, Final Batch Loss: 0.43146073818206787\n",
      "Epoch 2068, Loss: 1.0865212976932526, Final Batch Loss: 0.45640093088150024\n",
      "Epoch 2069, Loss: 0.846727192401886, Final Batch Loss: 0.2684395909309387\n",
      "Epoch 2070, Loss: 0.9644430577754974, Final Batch Loss: 0.31354326009750366\n",
      "Epoch 2071, Loss: 0.8264922350645065, Final Batch Loss: 0.2435287982225418\n",
      "Epoch 2072, Loss: 1.0291935801506042, Final Batch Loss: 0.4119609296321869\n",
      "Epoch 2073, Loss: 0.8976570069789886, Final Batch Loss: 0.34139296412467957\n",
      "Epoch 2074, Loss: 0.8505043238401413, Final Batch Loss: 0.1993127316236496\n",
      "Epoch 2075, Loss: 0.8725798279047012, Final Batch Loss: 0.2078959196805954\n",
      "Epoch 2076, Loss: 0.8806235790252686, Final Batch Loss: 0.26308757066726685\n",
      "Epoch 2077, Loss: 0.8300689160823822, Final Batch Loss: 0.23495307564735413\n",
      "Epoch 2078, Loss: 0.7718413472175598, Final Batch Loss: 0.15344151854515076\n",
      "Epoch 2079, Loss: 1.0734263211488724, Final Batch Loss: 0.5405796766281128\n",
      "Epoch 2080, Loss: 0.9744204878807068, Final Batch Loss: 0.3375353515148163\n",
      "Epoch 2081, Loss: 0.9671397805213928, Final Batch Loss: 0.40798619389533997\n",
      "Epoch 2082, Loss: 1.0557316839694977, Final Batch Loss: 0.4684905409812927\n",
      "Epoch 2083, Loss: 0.8510808497667313, Final Batch Loss: 0.23954157531261444\n",
      "Epoch 2084, Loss: 0.9522458612918854, Final Batch Loss: 0.34633925557136536\n",
      "Epoch 2085, Loss: 0.7942531108856201, Final Batch Loss: 0.20667994022369385\n",
      "Epoch 2086, Loss: 0.7632869631052017, Final Batch Loss: 0.16220040619373322\n",
      "Epoch 2087, Loss: 1.0089799761772156, Final Batch Loss: 0.403963178396225\n",
      "Epoch 2088, Loss: 0.8083019405603409, Final Batch Loss: 0.21064017713069916\n",
      "Epoch 2089, Loss: 0.9380023777484894, Final Batch Loss: 0.3227078914642334\n",
      "Epoch 2090, Loss: 0.8406709432601929, Final Batch Loss: 0.22813814878463745\n",
      "Epoch 2091, Loss: 0.9857634603977203, Final Batch Loss: 0.40165382623672485\n",
      "Epoch 2092, Loss: 0.9499214440584183, Final Batch Loss: 0.37061044573783875\n",
      "Epoch 2093, Loss: 1.0388491451740265, Final Batch Loss: 0.46128252148628235\n",
      "Epoch 2094, Loss: 0.8458952605724335, Final Batch Loss: 0.23389992117881775\n",
      "Epoch 2095, Loss: 1.109436258673668, Final Batch Loss: 0.46052008867263794\n",
      "Epoch 2096, Loss: 0.7801416665315628, Final Batch Loss: 0.19843141734600067\n",
      "Epoch 2097, Loss: 0.8099269568920135, Final Batch Loss: 0.2422119677066803\n",
      "Epoch 2098, Loss: 0.8322069048881531, Final Batch Loss: 0.227335125207901\n",
      "Epoch 2099, Loss: 0.8294935822486877, Final Batch Loss: 0.24748694896697998\n",
      "Epoch 2100, Loss: 0.9837772846221924, Final Batch Loss: 0.4266960620880127\n",
      "Epoch 2101, Loss: 0.7440224587917328, Final Batch Loss: 0.19202426075935364\n",
      "Epoch 2102, Loss: 0.6894319579005241, Final Batch Loss: 0.08585154265165329\n",
      "Epoch 2103, Loss: 0.8141550123691559, Final Batch Loss: 0.27059248089790344\n",
      "Epoch 2104, Loss: 1.1322005987167358, Final Batch Loss: 0.585993766784668\n",
      "Epoch 2105, Loss: 0.9432884454727173, Final Batch Loss: 0.35526156425476074\n",
      "Epoch 2106, Loss: 0.8349476605653763, Final Batch Loss: 0.22448088228702545\n",
      "Epoch 2107, Loss: 0.7455664277076721, Final Batch Loss: 0.09729745984077454\n",
      "Epoch 2108, Loss: 0.9658392667770386, Final Batch Loss: 0.32069844007492065\n",
      "Epoch 2109, Loss: 0.9049620628356934, Final Batch Loss: 0.3207038938999176\n",
      "Epoch 2110, Loss: 0.8753223121166229, Final Batch Loss: 0.3476376235485077\n",
      "Epoch 2111, Loss: 0.9342204034328461, Final Batch Loss: 0.2656770646572113\n",
      "Epoch 2112, Loss: 1.0322263538837433, Final Batch Loss: 0.4341776967048645\n",
      "Epoch 2113, Loss: 1.0812563598155975, Final Batch Loss: 0.5293048024177551\n",
      "Epoch 2114, Loss: 0.9037424623966217, Final Batch Loss: 0.3074730336666107\n",
      "Epoch 2115, Loss: 1.1327885687351227, Final Batch Loss: 0.4956926107406616\n",
      "Epoch 2116, Loss: 0.7815131098031998, Final Batch Loss: 0.17798008024692535\n",
      "Epoch 2117, Loss: 0.8451287597417831, Final Batch Loss: 0.15701664984226227\n",
      "Epoch 2118, Loss: 0.9570760130882263, Final Batch Loss: 0.2999211251735687\n",
      "Epoch 2119, Loss: 0.8634264469146729, Final Batch Loss: 0.33528196811676025\n",
      "Epoch 2120, Loss: 0.7910690754652023, Final Batch Loss: 0.18107463419437408\n",
      "Epoch 2121, Loss: 0.9456742703914642, Final Batch Loss: 0.3085113763809204\n",
      "Epoch 2122, Loss: 0.9450920522212982, Final Batch Loss: 0.34744203090667725\n",
      "Epoch 2123, Loss: 0.8804595619440079, Final Batch Loss: 0.23569191992282867\n",
      "Epoch 2124, Loss: 0.9225322902202606, Final Batch Loss: 0.35321658849716187\n",
      "Epoch 2125, Loss: 0.8101205825805664, Final Batch Loss: 0.2244398146867752\n",
      "Epoch 2126, Loss: 0.9408388733863831, Final Batch Loss: 0.32122278213500977\n",
      "Epoch 2127, Loss: 1.000501036643982, Final Batch Loss: 0.4242967367172241\n",
      "Epoch 2128, Loss: 0.8326130509376526, Final Batch Loss: 0.18860113620758057\n",
      "Epoch 2129, Loss: 0.9039458632469177, Final Batch Loss: 0.2702062726020813\n",
      "Epoch 2130, Loss: 0.7669591754674911, Final Batch Loss: 0.1836208552122116\n",
      "Epoch 2131, Loss: 0.8981361389160156, Final Batch Loss: 0.3100494146347046\n",
      "Epoch 2132, Loss: 0.8914513885974884, Final Batch Loss: 0.2886522710323334\n",
      "Epoch 2133, Loss: 0.8436725586652756, Final Batch Loss: 0.14952151477336884\n",
      "Epoch 2134, Loss: 0.943381667137146, Final Batch Loss: 0.3977172374725342\n",
      "Epoch 2135, Loss: 0.7872136831283569, Final Batch Loss: 0.13396525382995605\n",
      "Epoch 2136, Loss: 0.9586180746555328, Final Batch Loss: 0.36182495951652527\n",
      "Epoch 2137, Loss: 1.0939688980579376, Final Batch Loss: 0.4776730239391327\n",
      "Epoch 2138, Loss: 0.7200873717665672, Final Batch Loss: 0.11584105342626572\n",
      "Epoch 2139, Loss: 0.8899013996124268, Final Batch Loss: 0.2645946443080902\n",
      "Epoch 2140, Loss: 0.8509734869003296, Final Batch Loss: 0.2153691053390503\n",
      "Epoch 2141, Loss: 0.9769439697265625, Final Batch Loss: 0.4099369943141937\n",
      "Epoch 2142, Loss: 0.8671378195285797, Final Batch Loss: 0.31100648641586304\n",
      "Epoch 2143, Loss: 0.8191374689340591, Final Batch Loss: 0.1767028123140335\n",
      "Epoch 2144, Loss: 0.8025910556316376, Final Batch Loss: 0.24139440059661865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2145, Loss: 0.9974685609340668, Final Batch Loss: 0.42198923230171204\n",
      "Epoch 2146, Loss: 0.739850252866745, Final Batch Loss: 0.16012537479400635\n",
      "Epoch 2147, Loss: 0.944758951663971, Final Batch Loss: 0.35085147619247437\n",
      "Epoch 2148, Loss: 0.9487298727035522, Final Batch Loss: 0.32078179717063904\n",
      "Epoch 2149, Loss: 0.8647662252187729, Final Batch Loss: 0.21182690560817719\n",
      "Epoch 2150, Loss: 0.7595887184143066, Final Batch Loss: 0.19066989421844482\n",
      "Epoch 2151, Loss: 1.010737955570221, Final Batch Loss: 0.45977166295051575\n",
      "Epoch 2152, Loss: 0.9052415192127228, Final Batch Loss: 0.3118046522140503\n",
      "Epoch 2153, Loss: 0.8887728452682495, Final Batch Loss: 0.2959557771682739\n",
      "Epoch 2154, Loss: 0.7849022299051285, Final Batch Loss: 0.16722674667835236\n",
      "Epoch 2155, Loss: 0.7757783532142639, Final Batch Loss: 0.15939971804618835\n",
      "Epoch 2156, Loss: 0.9797258079051971, Final Batch Loss: 0.4063040018081665\n",
      "Epoch 2157, Loss: 0.952793151140213, Final Batch Loss: 0.3376120328903198\n",
      "Epoch 2158, Loss: 0.7285975813865662, Final Batch Loss: 0.20158572494983673\n",
      "Epoch 2159, Loss: 1.0980425477027893, Final Batch Loss: 0.5757728219032288\n",
      "Epoch 2160, Loss: 0.7175510674715042, Final Batch Loss: 0.11083434522151947\n",
      "Epoch 2161, Loss: 0.9832748174667358, Final Batch Loss: 0.3240341544151306\n",
      "Epoch 2162, Loss: 0.8332353830337524, Final Batch Loss: 0.26558801531791687\n",
      "Epoch 2163, Loss: 1.1038196980953217, Final Batch Loss: 0.5397279262542725\n",
      "Epoch 2164, Loss: 1.1024858355522156, Final Batch Loss: 0.5557422041893005\n",
      "Epoch 2165, Loss: 1.065144956111908, Final Batch Loss: 0.4674263596534729\n",
      "Epoch 2166, Loss: 1.011348843574524, Final Batch Loss: 0.33198046684265137\n",
      "Epoch 2167, Loss: 0.991540253162384, Final Batch Loss: 0.38365235924720764\n",
      "Epoch 2168, Loss: 0.7022607922554016, Final Batch Loss: 0.08194875717163086\n",
      "Epoch 2169, Loss: 1.0886333882808685, Final Batch Loss: 0.48809829354286194\n",
      "Epoch 2170, Loss: 0.8408838659524918, Final Batch Loss: 0.1919989436864853\n",
      "Epoch 2171, Loss: 0.763875849545002, Final Batch Loss: 0.0991598442196846\n",
      "Epoch 2172, Loss: 1.1116315424442291, Final Batch Loss: 0.47354647517204285\n",
      "Epoch 2173, Loss: 0.7110795080661774, Final Batch Loss: 0.13825014233589172\n",
      "Epoch 2174, Loss: 0.6686334982514381, Final Batch Loss: 0.07978356629610062\n",
      "Epoch 2175, Loss: 0.6788034364581108, Final Batch Loss: 0.10233857482671738\n",
      "Epoch 2176, Loss: 0.8354970961809158, Final Batch Loss: 0.238799050450325\n",
      "Epoch 2177, Loss: 1.1768333464860916, Final Batch Loss: 0.6350786685943604\n",
      "Epoch 2178, Loss: 0.7252880930900574, Final Batch Loss: 0.13610681891441345\n",
      "Epoch 2179, Loss: 0.8311179280281067, Final Batch Loss: 0.25058525800704956\n",
      "Epoch 2180, Loss: 0.8230113983154297, Final Batch Loss: 0.24309790134429932\n",
      "Epoch 2181, Loss: 0.8478405773639679, Final Batch Loss: 0.2963913381099701\n",
      "Epoch 2182, Loss: 0.8342186957597733, Final Batch Loss: 0.23423011600971222\n",
      "Epoch 2183, Loss: 0.8993123024702072, Final Batch Loss: 0.359784871339798\n",
      "Epoch 2184, Loss: 0.8277107179164886, Final Batch Loss: 0.24413254857063293\n",
      "Epoch 2185, Loss: 0.7645902186632156, Final Batch Loss: 0.20071731507778168\n",
      "Epoch 2186, Loss: 0.73807792365551, Final Batch Loss: 0.19104810059070587\n",
      "Epoch 2187, Loss: 0.7679725587368011, Final Batch Loss: 0.24038296937942505\n",
      "Epoch 2188, Loss: 1.096413940191269, Final Batch Loss: 0.5440858602523804\n",
      "Epoch 2189, Loss: 1.153202012181282, Final Batch Loss: 0.6141641736030579\n",
      "Epoch 2190, Loss: 0.884411633014679, Final Batch Loss: 0.2622300386428833\n",
      "Epoch 2191, Loss: 0.7207020074129105, Final Batch Loss: 0.17759765684604645\n",
      "Epoch 2192, Loss: 0.7153930515050888, Final Batch Loss: 0.13873575627803802\n",
      "Epoch 2193, Loss: 0.7782005220651627, Final Batch Loss: 0.21832729876041412\n",
      "Epoch 2194, Loss: 0.8789438605308533, Final Batch Loss: 0.2649165391921997\n",
      "Epoch 2195, Loss: 0.949257880449295, Final Batch Loss: 0.28629961609840393\n",
      "Epoch 2196, Loss: 1.0361305475234985, Final Batch Loss: 0.4851413071155548\n",
      "Epoch 2197, Loss: 0.8256995528936386, Final Batch Loss: 0.2959025800228119\n",
      "Epoch 2198, Loss: 0.8591650575399399, Final Batch Loss: 0.2957424223423004\n",
      "Epoch 2199, Loss: 0.9949614107608795, Final Batch Loss: 0.38759511709213257\n",
      "Epoch 2200, Loss: 1.085106998682022, Final Batch Loss: 0.48859837651252747\n",
      "Epoch 2201, Loss: 0.9313333630561829, Final Batch Loss: 0.33207598328590393\n",
      "Epoch 2202, Loss: 0.8058560788631439, Final Batch Loss: 0.190647691488266\n",
      "Epoch 2203, Loss: 0.9252661168575287, Final Batch Loss: 0.25756680965423584\n",
      "Epoch 2204, Loss: 0.8760634660720825, Final Batch Loss: 0.3447430729866028\n",
      "Epoch 2205, Loss: 1.0502538979053497, Final Batch Loss: 0.45012789964675903\n",
      "Epoch 2206, Loss: 0.9496892988681793, Final Batch Loss: 0.25433483719825745\n",
      "Epoch 2207, Loss: 0.8458854258060455, Final Batch Loss: 0.230500727891922\n",
      "Epoch 2208, Loss: 0.9074291586875916, Final Batch Loss: 0.2757251262664795\n",
      "Epoch 2209, Loss: 0.9075939059257507, Final Batch Loss: 0.3375101387500763\n",
      "Epoch 2210, Loss: 0.8427295684814453, Final Batch Loss: 0.29376161098480225\n",
      "Epoch 2211, Loss: 0.8612070381641388, Final Batch Loss: 0.26716822385787964\n",
      "Epoch 2212, Loss: 0.8004197478294373, Final Batch Loss: 0.19450980424880981\n",
      "Epoch 2213, Loss: 0.7269579470157623, Final Batch Loss: 0.20222365856170654\n",
      "Epoch 2214, Loss: 0.7083936333656311, Final Batch Loss: 0.15714484453201294\n",
      "Epoch 2215, Loss: 0.794725775718689, Final Batch Loss: 0.2519543170928955\n",
      "Epoch 2216, Loss: 0.7588536292314529, Final Batch Loss: 0.17541815340518951\n",
      "Epoch 2217, Loss: 0.7802371829748154, Final Batch Loss: 0.1197107583284378\n",
      "Epoch 2218, Loss: 0.850219339132309, Final Batch Loss: 0.2719661295413971\n",
      "Epoch 2219, Loss: 1.2552635967731476, Final Batch Loss: 0.7513103485107422\n",
      "Epoch 2220, Loss: 0.9303943514823914, Final Batch Loss: 0.35934486985206604\n",
      "Epoch 2221, Loss: 0.8505384027957916, Final Batch Loss: 0.26167723536491394\n",
      "Epoch 2222, Loss: 1.0062212347984314, Final Batch Loss: 0.35115647315979004\n",
      "Epoch 2223, Loss: 0.9787382185459137, Final Batch Loss: 0.3454878032207489\n",
      "Epoch 2224, Loss: 0.7690155357122421, Final Batch Loss: 0.23516331613063812\n",
      "Epoch 2225, Loss: 0.7221974581480026, Final Batch Loss: 0.17447148263454437\n",
      "Epoch 2226, Loss: 0.8239945322275162, Final Batch Loss: 0.27772513031959534\n",
      "Epoch 2227, Loss: 0.7888150215148926, Final Batch Loss: 0.22901499271392822\n",
      "Epoch 2228, Loss: 0.9916470050811768, Final Batch Loss: 0.4074670374393463\n",
      "Epoch 2229, Loss: 0.8717983067035675, Final Batch Loss: 0.2791966497898102\n",
      "Epoch 2230, Loss: 0.7214201092720032, Final Batch Loss: 0.1917363405227661\n",
      "Epoch 2231, Loss: 0.7640652805566788, Final Batch Loss: 0.23550474643707275\n",
      "Epoch 2232, Loss: 0.6987527757883072, Final Batch Loss: 0.1074383407831192\n",
      "Epoch 2233, Loss: 1.1800280809402466, Final Batch Loss: 0.6583638191223145\n",
      "Epoch 2234, Loss: 0.9760404825210571, Final Batch Loss: 0.4086298942565918\n",
      "Epoch 2235, Loss: 0.8707687109708786, Final Batch Loss: 0.26102545857429504\n",
      "Epoch 2236, Loss: 1.362865924835205, Final Batch Loss: 0.8528648018836975\n",
      "Epoch 2237, Loss: 0.8692539632320404, Final Batch Loss: 0.29424038529396057\n",
      "Epoch 2238, Loss: 0.949071615934372, Final Batch Loss: 0.257009893655777\n",
      "Epoch 2239, Loss: 1.143667995929718, Final Batch Loss: 0.44126883149147034\n",
      "Epoch 2240, Loss: 1.0646001100540161, Final Batch Loss: 0.43600645661354065\n",
      "Epoch 2241, Loss: 1.088877111673355, Final Batch Loss: 0.3804711699485779\n",
      "Epoch 2242, Loss: 0.8458667248487473, Final Batch Loss: 0.22411127388477325\n",
      "Epoch 2243, Loss: 0.9331014752388, Final Batch Loss: 0.3241823613643646\n",
      "Epoch 2244, Loss: 0.7307443097233772, Final Batch Loss: 0.10205120593309402\n",
      "Epoch 2245, Loss: 0.814911276102066, Final Batch Loss: 0.29118460416793823\n",
      "Epoch 2246, Loss: 0.8323183953762054, Final Batch Loss: 0.21223273873329163\n",
      "Epoch 2247, Loss: 0.7307044118642807, Final Batch Loss: 0.16799886524677277\n",
      "Epoch 2248, Loss: 0.9773640334606171, Final Batch Loss: 0.43200770020484924\n",
      "Epoch 2249, Loss: 0.8361935317516327, Final Batch Loss: 0.27220433950424194\n",
      "Epoch 2250, Loss: 0.7629332989454269, Final Batch Loss: 0.20840291678905487\n",
      "Epoch 2251, Loss: 0.9277597367763519, Final Batch Loss: 0.3541012704372406\n",
      "Epoch 2252, Loss: 1.0281100571155548, Final Batch Loss: 0.4748522639274597\n",
      "Epoch 2253, Loss: 0.8370414078235626, Final Batch Loss: 0.3056511878967285\n",
      "Epoch 2254, Loss: 0.9939979910850525, Final Batch Loss: 0.38397130370140076\n",
      "Epoch 2255, Loss: 0.8126740902662277, Final Batch Loss: 0.20058055222034454\n",
      "Epoch 2256, Loss: 0.77340167760849, Final Batch Loss: 0.2075401395559311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2257, Loss: 1.0319987535476685, Final Batch Loss: 0.4569546580314636\n",
      "Epoch 2258, Loss: 1.0589493811130524, Final Batch Loss: 0.4721473753452301\n",
      "Epoch 2259, Loss: 0.8102450370788574, Final Batch Loss: 0.2718690037727356\n",
      "Epoch 2260, Loss: 0.8628058433532715, Final Batch Loss: 0.26371803879737854\n",
      "Epoch 2261, Loss: 0.7704997211694717, Final Batch Loss: 0.1695406287908554\n",
      "Epoch 2262, Loss: 0.9514076113700867, Final Batch Loss: 0.3061883747577667\n",
      "Epoch 2263, Loss: 0.7470815032720566, Final Batch Loss: 0.18373803794384003\n",
      "Epoch 2264, Loss: 0.786045253276825, Final Batch Loss: 0.2396688461303711\n",
      "Epoch 2265, Loss: 0.8500897288322449, Final Batch Loss: 0.269037127494812\n",
      "Epoch 2266, Loss: 0.8525951951742172, Final Batch Loss: 0.2253766506910324\n",
      "Epoch 2267, Loss: 0.8780461251735687, Final Batch Loss: 0.26215192675590515\n",
      "Epoch 2268, Loss: 0.8541928827762604, Final Batch Loss: 0.2760196626186371\n",
      "Epoch 2269, Loss: 0.9187756478786469, Final Batch Loss: 0.3132021129131317\n",
      "Epoch 2270, Loss: 0.6936801746487617, Final Batch Loss: 0.08842926472425461\n",
      "Epoch 2271, Loss: 0.8934732377529144, Final Batch Loss: 0.29119813442230225\n",
      "Epoch 2272, Loss: 0.9064372181892395, Final Batch Loss: 0.36128973960876465\n",
      "Epoch 2273, Loss: 0.8774652481079102, Final Batch Loss: 0.26426079869270325\n",
      "Epoch 2274, Loss: 0.7072107195854187, Final Batch Loss: 0.14103832840919495\n",
      "Epoch 2275, Loss: 0.9053488671779633, Final Batch Loss: 0.30000174045562744\n",
      "Epoch 2276, Loss: 0.9455356001853943, Final Batch Loss: 0.3306143283843994\n",
      "Epoch 2277, Loss: 1.1825198233127594, Final Batch Loss: 0.5149455070495605\n",
      "Epoch 2278, Loss: 0.8544637858867645, Final Batch Loss: 0.23002958297729492\n",
      "Epoch 2279, Loss: 0.9079164266586304, Final Batch Loss: 0.29412785172462463\n",
      "Epoch 2280, Loss: 0.7770855724811554, Final Batch Loss: 0.20829331874847412\n",
      "Epoch 2281, Loss: 0.9998221695423126, Final Batch Loss: 0.4034361243247986\n",
      "Epoch 2282, Loss: 1.0505079329013824, Final Batch Loss: 0.30282312631607056\n",
      "Epoch 2283, Loss: 0.8665454685688019, Final Batch Loss: 0.2602793574333191\n",
      "Epoch 2284, Loss: 1.1020166873931885, Final Batch Loss: 0.5182816982269287\n",
      "Epoch 2285, Loss: 1.0757707953453064, Final Batch Loss: 0.41927918791770935\n",
      "Epoch 2286, Loss: 0.77274589240551, Final Batch Loss: 0.2159290760755539\n",
      "Epoch 2287, Loss: 0.9528779983520508, Final Batch Loss: 0.3891448676586151\n",
      "Epoch 2288, Loss: 0.8111889511346817, Final Batch Loss: 0.2511793076992035\n",
      "Epoch 2289, Loss: 0.8116820454597473, Final Batch Loss: 0.20577222108840942\n",
      "Epoch 2290, Loss: 1.0130607783794403, Final Batch Loss: 0.41639718413352966\n",
      "Epoch 2291, Loss: 0.8356498628854752, Final Batch Loss: 0.293443500995636\n",
      "Epoch 2292, Loss: 0.9406556487083435, Final Batch Loss: 0.36523380875587463\n",
      "Epoch 2293, Loss: 0.7243557423353195, Final Batch Loss: 0.13185422122478485\n",
      "Epoch 2294, Loss: 0.876643717288971, Final Batch Loss: 0.35596179962158203\n",
      "Epoch 2295, Loss: 0.7772137820720673, Final Batch Loss: 0.1818823218345642\n",
      "Epoch 2296, Loss: 0.8577191829681396, Final Batch Loss: 0.29212895035743713\n",
      "Epoch 2297, Loss: 0.830113098025322, Final Batch Loss: 0.2456430345773697\n",
      "Epoch 2298, Loss: 0.8518325388431549, Final Batch Loss: 0.26494643092155457\n",
      "Epoch 2299, Loss: 0.9330907762050629, Final Batch Loss: 0.2991463243961334\n",
      "Epoch 2300, Loss: 0.9428559690713882, Final Batch Loss: 0.39164167642593384\n",
      "Epoch 2301, Loss: 0.7500827312469482, Final Batch Loss: 0.19889229536056519\n",
      "Epoch 2302, Loss: 1.0611055493354797, Final Batch Loss: 0.493979811668396\n",
      "Epoch 2303, Loss: 0.7648425549268723, Final Batch Loss: 0.2316703051328659\n",
      "Epoch 2304, Loss: 1.013761192560196, Final Batch Loss: 0.4444865584373474\n",
      "Epoch 2305, Loss: 0.874642550945282, Final Batch Loss: 0.31507381796836853\n",
      "Epoch 2306, Loss: 0.9496200680732727, Final Batch Loss: 0.33365434408187866\n",
      "Epoch 2307, Loss: 0.6832791268825531, Final Batch Loss: 0.1422644555568695\n",
      "Epoch 2308, Loss: 0.8225953578948975, Final Batch Loss: 0.2918759286403656\n",
      "Epoch 2309, Loss: 1.4240783154964447, Final Batch Loss: 0.8559135794639587\n",
      "Epoch 2310, Loss: 0.8219311237335205, Final Batch Loss: 0.3186160922050476\n",
      "Epoch 2311, Loss: 0.839132696390152, Final Batch Loss: 0.26820358633995056\n",
      "Epoch 2312, Loss: 0.8114452511072159, Final Batch Loss: 0.22824154794216156\n",
      "Epoch 2313, Loss: 1.0985680371522903, Final Batch Loss: 0.5583759546279907\n",
      "Epoch 2314, Loss: 1.039687156677246, Final Batch Loss: 0.42363372445106506\n",
      "Epoch 2315, Loss: 0.8656541109085083, Final Batch Loss: 0.31222519278526306\n",
      "Epoch 2316, Loss: 0.9294989705085754, Final Batch Loss: 0.42313694953918457\n",
      "Epoch 2317, Loss: 0.922734946012497, Final Batch Loss: 0.3567996323108673\n",
      "Epoch 2318, Loss: 0.8028614073991776, Final Batch Loss: 0.21008078753948212\n",
      "Epoch 2319, Loss: 0.7440710961818695, Final Batch Loss: 0.17706555128097534\n",
      "Epoch 2320, Loss: 0.7637858092784882, Final Batch Loss: 0.1636146903038025\n",
      "Epoch 2321, Loss: 0.9401682317256927, Final Batch Loss: 0.35536816716194153\n",
      "Epoch 2322, Loss: 0.7293666303157806, Final Batch Loss: 0.17360800504684448\n",
      "Epoch 2323, Loss: 0.9620972573757172, Final Batch Loss: 0.368556946516037\n",
      "Epoch 2324, Loss: 0.828547477722168, Final Batch Loss: 0.255741149187088\n",
      "Epoch 2325, Loss: 0.7622933834791183, Final Batch Loss: 0.1343250423669815\n",
      "Epoch 2326, Loss: 0.8505410254001617, Final Batch Loss: 0.2445220649242401\n",
      "Epoch 2327, Loss: 0.8672916889190674, Final Batch Loss: 0.34402167797088623\n",
      "Epoch 2328, Loss: 0.8636159896850586, Final Batch Loss: 0.2630065977573395\n",
      "Epoch 2329, Loss: 0.774954304099083, Final Batch Loss: 0.18784146010875702\n",
      "Epoch 2330, Loss: 0.8591330945491791, Final Batch Loss: 0.2784510850906372\n",
      "Epoch 2331, Loss: 0.8064048737287521, Final Batch Loss: 0.2753947377204895\n",
      "Epoch 2332, Loss: 0.7244068086147308, Final Batch Loss: 0.15855956077575684\n",
      "Epoch 2333, Loss: 0.6359296664595604, Final Batch Loss: 0.09598144143819809\n",
      "Epoch 2334, Loss: 0.7946569919586182, Final Batch Loss: 0.2510775327682495\n",
      "Epoch 2335, Loss: 0.9056518971920013, Final Batch Loss: 0.3122197091579437\n",
      "Epoch 2336, Loss: 0.8812718689441681, Final Batch Loss: 0.33492738008499146\n",
      "Epoch 2337, Loss: 0.887254074215889, Final Batch Loss: 0.36103543639183044\n",
      "Epoch 2338, Loss: 0.9454766511917114, Final Batch Loss: 0.37442564964294434\n",
      "Epoch 2339, Loss: 0.9929194450378418, Final Batch Loss: 0.4315012991428375\n",
      "Epoch 2340, Loss: 1.0807423293590546, Final Batch Loss: 0.4748581349849701\n",
      "Epoch 2341, Loss: 0.8500439375638962, Final Batch Loss: 0.2752797305583954\n",
      "Epoch 2342, Loss: 0.9229817092418671, Final Batch Loss: 0.38900113105773926\n",
      "Epoch 2343, Loss: 0.9282755255699158, Final Batch Loss: 0.38577041029930115\n",
      "Epoch 2344, Loss: 0.7791792005300522, Final Batch Loss: 0.20547227561473846\n",
      "Epoch 2345, Loss: 0.7861709594726562, Final Batch Loss: 0.20455947518348694\n",
      "Epoch 2346, Loss: 0.7054396495223045, Final Batch Loss: 0.12437649816274643\n",
      "Epoch 2347, Loss: 0.7988807857036591, Final Batch Loss: 0.22190478444099426\n",
      "Epoch 2348, Loss: 0.649101085960865, Final Batch Loss: 0.123884417116642\n",
      "Epoch 2349, Loss: 0.8735319674015045, Final Batch Loss: 0.3132513761520386\n",
      "Epoch 2350, Loss: 0.7623542696237564, Final Batch Loss: 0.2094438523054123\n",
      "Epoch 2351, Loss: 0.8518924415111542, Final Batch Loss: 0.28317928314208984\n",
      "Epoch 2352, Loss: 0.8783566057682037, Final Batch Loss: 0.32011228799819946\n",
      "Epoch 2353, Loss: 0.9316963851451874, Final Batch Loss: 0.3618057668209076\n",
      "Epoch 2354, Loss: 0.9348412156105042, Final Batch Loss: 0.3984179198741913\n",
      "Epoch 2355, Loss: 0.9019382447004318, Final Batch Loss: 0.3578246533870697\n",
      "Epoch 2356, Loss: 0.8295087218284607, Final Batch Loss: 0.25230520963668823\n",
      "Epoch 2357, Loss: 0.7499369233846664, Final Batch Loss: 0.23419351875782013\n",
      "Epoch 2358, Loss: 0.8595709055662155, Final Batch Loss: 0.3494759500026703\n",
      "Epoch 2359, Loss: 0.66938516497612, Final Batch Loss: 0.13234402239322662\n",
      "Epoch 2360, Loss: 0.9409601092338562, Final Batch Loss: 0.37238609790802\n",
      "Epoch 2361, Loss: 0.6997980773448944, Final Batch Loss: 0.1746947318315506\n",
      "Epoch 2362, Loss: 0.8587482869625092, Final Batch Loss: 0.3132893443107605\n",
      "Epoch 2363, Loss: 0.8148351013660431, Final Batch Loss: 0.24153023958206177\n",
      "Epoch 2364, Loss: 0.8630471527576447, Final Batch Loss: 0.20340251922607422\n",
      "Epoch 2365, Loss: 0.7671235799789429, Final Batch Loss: 0.16351497173309326\n",
      "Epoch 2366, Loss: 0.9866451919078827, Final Batch Loss: 0.4003047049045563\n",
      "Epoch 2367, Loss: 0.7865038961172104, Final Batch Loss: 0.223887637257576\n",
      "Epoch 2368, Loss: 0.9071493297815323, Final Batch Loss: 0.3999205231666565\n",
      "Epoch 2369, Loss: 0.8546811193227768, Final Batch Loss: 0.4038008451461792\n",
      "Epoch 2370, Loss: 0.9255813658237457, Final Batch Loss: 0.39229297637939453\n",
      "Epoch 2371, Loss: 0.7031987011432648, Final Batch Loss: 0.15064910054206848\n",
      "Epoch 2372, Loss: 1.025948703289032, Final Batch Loss: 0.5067386031150818\n",
      "Epoch 2373, Loss: 1.1032909452915192, Final Batch Loss: 0.6175071001052856\n",
      "Epoch 2374, Loss: 0.7640837281942368, Final Batch Loss: 0.16654284298419952\n",
      "Epoch 2375, Loss: 0.7354899197816849, Final Batch Loss: 0.18021263182163239\n",
      "Epoch 2376, Loss: 0.7617737203836441, Final Batch Loss: 0.21953700482845306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2377, Loss: 0.9601881504058838, Final Batch Loss: 0.3894157409667969\n",
      "Epoch 2378, Loss: 0.6236057057976723, Final Batch Loss: 0.07852957397699356\n",
      "Epoch 2379, Loss: 1.0260955393314362, Final Batch Loss: 0.45207875967025757\n",
      "Epoch 2380, Loss: 0.7502815276384354, Final Batch Loss: 0.1845868080854416\n",
      "Epoch 2381, Loss: 0.7401277720928192, Final Batch Loss: 0.19586560130119324\n",
      "Epoch 2382, Loss: 0.7274559140205383, Final Batch Loss: 0.18131577968597412\n",
      "Epoch 2383, Loss: 0.8478991687297821, Final Batch Loss: 0.3102954924106598\n",
      "Epoch 2384, Loss: 0.6701523661613464, Final Batch Loss: 0.1270710825920105\n",
      "Epoch 2385, Loss: 0.7667701989412308, Final Batch Loss: 0.2605864703655243\n",
      "Epoch 2386, Loss: 0.6734203994274139, Final Batch Loss: 0.15570394694805145\n",
      "Epoch 2387, Loss: 0.7308508306741714, Final Batch Loss: 0.15969626605510712\n",
      "Epoch 2388, Loss: 1.223045974969864, Final Batch Loss: 0.6921496987342834\n",
      "Epoch 2389, Loss: 0.7468398213386536, Final Batch Loss: 0.1810131072998047\n",
      "Epoch 2390, Loss: 1.094292774796486, Final Batch Loss: 0.5892306566238403\n",
      "Epoch 2391, Loss: 0.7582056224346161, Final Batch Loss: 0.2333221733570099\n",
      "Epoch 2392, Loss: 0.7642114460468292, Final Batch Loss: 0.10483354330062866\n",
      "Epoch 2393, Loss: 0.7970500886440277, Final Batch Loss: 0.2643085718154907\n",
      "Epoch 2394, Loss: 0.737210676074028, Final Batch Loss: 0.20032496750354767\n",
      "Epoch 2395, Loss: 0.6598811820149422, Final Batch Loss: 0.11890486627817154\n",
      "Epoch 2396, Loss: 0.6938628293573856, Final Batch Loss: 0.05863151326775551\n",
      "Epoch 2397, Loss: 0.8317048400640488, Final Batch Loss: 0.24702094495296478\n",
      "Epoch 2398, Loss: 0.8273335695266724, Final Batch Loss: 0.1990974247455597\n",
      "Epoch 2399, Loss: 0.6196324974298477, Final Batch Loss: 0.044410184025764465\n",
      "Epoch 2400, Loss: 0.624822199344635, Final Batch Loss: 0.10810130834579468\n",
      "Epoch 2401, Loss: 0.8656720668077469, Final Batch Loss: 0.28043481707572937\n",
      "Epoch 2402, Loss: 0.9269369691610336, Final Batch Loss: 0.4286061227321625\n",
      "Epoch 2403, Loss: 0.8641092479228973, Final Batch Loss: 0.2633157968521118\n",
      "Epoch 2404, Loss: 0.7357847094535828, Final Batch Loss: 0.18346503376960754\n",
      "Epoch 2405, Loss: 0.7421090006828308, Final Batch Loss: 0.17144089937210083\n",
      "Epoch 2406, Loss: 0.7331743389368057, Final Batch Loss: 0.18094263970851898\n",
      "Epoch 2407, Loss: 0.8724990785121918, Final Batch Loss: 0.3436853885650635\n",
      "Epoch 2408, Loss: 0.8037585020065308, Final Batch Loss: 0.2013843059539795\n",
      "Epoch 2409, Loss: 0.9496516287326813, Final Batch Loss: 0.361497163772583\n",
      "Epoch 2410, Loss: 0.575628362596035, Final Batch Loss: 0.06213458627462387\n",
      "Epoch 2411, Loss: 0.6552012339234352, Final Batch Loss: 0.09749690443277359\n",
      "Epoch 2412, Loss: 0.8220687657594681, Final Batch Loss: 0.2430330067873001\n",
      "Epoch 2413, Loss: 0.8412420302629471, Final Batch Loss: 0.3214334547519684\n",
      "Epoch 2414, Loss: 0.8360615372657776, Final Batch Loss: 0.2602463662624359\n",
      "Epoch 2415, Loss: 0.8443200290203094, Final Batch Loss: 0.26910677552223206\n",
      "Epoch 2416, Loss: 0.9362468123435974, Final Batch Loss: 0.3804953098297119\n",
      "Epoch 2417, Loss: 0.7814081236720085, Final Batch Loss: 0.11939769238233566\n",
      "Epoch 2418, Loss: 0.9040497690439224, Final Batch Loss: 0.3579624593257904\n",
      "Epoch 2419, Loss: 0.6110129952430725, Final Batch Loss: 0.07430341839790344\n",
      "Epoch 2420, Loss: 0.7053915113210678, Final Batch Loss: 0.1588655263185501\n",
      "Epoch 2421, Loss: 0.7361151278018951, Final Batch Loss: 0.20780718326568604\n",
      "Epoch 2422, Loss: 0.9017603099346161, Final Batch Loss: 0.2708944082260132\n",
      "Epoch 2423, Loss: 0.8254841566085815, Final Batch Loss: 0.2302255630493164\n",
      "Epoch 2424, Loss: 0.7989203184843063, Final Batch Loss: 0.208492711186409\n",
      "Epoch 2425, Loss: 1.133233368396759, Final Batch Loss: 0.5559796094894409\n",
      "Epoch 2426, Loss: 0.6271771192550659, Final Batch Loss: 0.15158270299434662\n",
      "Epoch 2427, Loss: 0.6422853395342827, Final Batch Loss: 0.12208300083875656\n",
      "Epoch 2428, Loss: 0.927812933921814, Final Batch Loss: 0.35722771286964417\n",
      "Epoch 2429, Loss: 0.6194268725812435, Final Batch Loss: 0.06228163465857506\n",
      "Epoch 2430, Loss: 0.7412878274917603, Final Batch Loss: 0.19455376267433167\n",
      "Epoch 2431, Loss: 0.8353884071111679, Final Batch Loss: 0.3303662836551666\n",
      "Epoch 2432, Loss: 0.7499517351388931, Final Batch Loss: 0.231679767370224\n",
      "Epoch 2433, Loss: 0.6700489521026611, Final Batch Loss: 0.10443925857543945\n",
      "Epoch 2434, Loss: 1.016280084848404, Final Batch Loss: 0.4444591701030731\n",
      "Epoch 2435, Loss: 0.8632373809814453, Final Batch Loss: 0.29645219445228577\n",
      "Epoch 2436, Loss: 1.1700072586536407, Final Batch Loss: 0.5791285037994385\n",
      "Epoch 2437, Loss: 0.7759076207876205, Final Batch Loss: 0.25874829292297363\n",
      "Epoch 2438, Loss: 0.8579525053501129, Final Batch Loss: 0.2961989939212799\n",
      "Epoch 2439, Loss: 0.8041094392538071, Final Batch Loss: 0.21469222009181976\n",
      "Epoch 2440, Loss: 0.6893925443291664, Final Batch Loss: 0.10680434852838516\n",
      "Epoch 2441, Loss: 0.744109109044075, Final Batch Loss: 0.16603361070156097\n",
      "Epoch 2442, Loss: 1.0041828155517578, Final Batch Loss: 0.45908984541893005\n",
      "Epoch 2443, Loss: 1.0349511206150055, Final Batch Loss: 0.4552246332168579\n",
      "Epoch 2444, Loss: 0.843805119395256, Final Batch Loss: 0.34273266792297363\n",
      "Epoch 2445, Loss: 0.8659079372882843, Final Batch Loss: 0.3199898898601532\n",
      "Epoch 2446, Loss: 0.9250540435314178, Final Batch Loss: 0.36131182312965393\n",
      "Epoch 2447, Loss: 0.7207959294319153, Final Batch Loss: 0.15238887071609497\n",
      "Epoch 2448, Loss: 1.152053415775299, Final Batch Loss: 0.591812789440155\n",
      "Epoch 2449, Loss: 0.7715829014778137, Final Batch Loss: 0.19540603458881378\n",
      "Epoch 2450, Loss: 0.8599203824996948, Final Batch Loss: 0.36159443855285645\n",
      "Epoch 2451, Loss: 0.8036884665489197, Final Batch Loss: 0.23994362354278564\n",
      "Epoch 2452, Loss: 0.8308344185352325, Final Batch Loss: 0.264595627784729\n",
      "Epoch 2453, Loss: 0.7436705455183983, Final Batch Loss: 0.10906826704740524\n",
      "Epoch 2454, Loss: 0.8145497292280197, Final Batch Loss: 0.20867855846881866\n",
      "Epoch 2455, Loss: 0.6962763369083405, Final Batch Loss: 0.13152408599853516\n",
      "Epoch 2456, Loss: 0.7617221176624298, Final Batch Loss: 0.2136058509349823\n",
      "Epoch 2457, Loss: 0.7633350789546967, Final Batch Loss: 0.19730865955352783\n",
      "Epoch 2458, Loss: 0.675294890999794, Final Batch Loss: 0.15323440730571747\n",
      "Epoch 2459, Loss: 0.8888418674468994, Final Batch Loss: 0.28613054752349854\n",
      "Epoch 2460, Loss: 0.983480304479599, Final Batch Loss: 0.4399160146713257\n",
      "Epoch 2461, Loss: 0.8396204560995102, Final Batch Loss: 0.29706692695617676\n",
      "Epoch 2462, Loss: 0.7770102024078369, Final Batch Loss: 0.20444276928901672\n",
      "Epoch 2463, Loss: 0.6665404438972473, Final Batch Loss: 0.12146568298339844\n",
      "Epoch 2464, Loss: 0.6808296591043472, Final Batch Loss: 0.16308844089508057\n",
      "Epoch 2465, Loss: 0.9532020688056946, Final Batch Loss: 0.4055120050907135\n",
      "Epoch 2466, Loss: 0.8389746844768524, Final Batch Loss: 0.3140706717967987\n",
      "Epoch 2467, Loss: 0.9894318878650665, Final Batch Loss: 0.4616330862045288\n",
      "Epoch 2468, Loss: 0.7253029644489288, Final Batch Loss: 0.2188844382762909\n",
      "Epoch 2469, Loss: 0.8141421973705292, Final Batch Loss: 0.25291404128074646\n",
      "Epoch 2470, Loss: 0.851605087518692, Final Batch Loss: 0.32008418440818787\n",
      "Epoch 2471, Loss: 0.839879035949707, Final Batch Loss: 0.2687155306339264\n",
      "Epoch 2472, Loss: 0.794158473610878, Final Batch Loss: 0.17988838255405426\n",
      "Epoch 2473, Loss: 0.9609125852584839, Final Batch Loss: 0.41993969678878784\n",
      "Epoch 2474, Loss: 0.7501552700996399, Final Batch Loss: 0.22606533765792847\n",
      "Epoch 2475, Loss: 0.8529277145862579, Final Batch Loss: 0.33668574690818787\n",
      "Epoch 2476, Loss: 0.7500041872262955, Final Batch Loss: 0.1991966813802719\n",
      "Epoch 2477, Loss: 0.821990042924881, Final Batch Loss: 0.2723632752895355\n",
      "Epoch 2478, Loss: 0.8994444906711578, Final Batch Loss: 0.33154433965682983\n",
      "Epoch 2479, Loss: 0.7498032450675964, Final Batch Loss: 0.1584237515926361\n",
      "Epoch 2480, Loss: 0.7274569571018219, Final Batch Loss: 0.21914798021316528\n",
      "Epoch 2481, Loss: 0.6951718106865883, Final Batch Loss: 0.09511817246675491\n",
      "Epoch 2482, Loss: 0.7879958897829056, Final Batch Loss: 0.2811046838760376\n",
      "Epoch 2483, Loss: 0.9491603970527649, Final Batch Loss: 0.40169885754585266\n",
      "Epoch 2484, Loss: 0.8686785697937012, Final Batch Loss: 0.35781222581863403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2485, Loss: 0.9337812960147858, Final Batch Loss: 0.40475544333457947\n",
      "Epoch 2486, Loss: 0.6497778296470642, Final Batch Loss: 0.09652534127235413\n",
      "Epoch 2487, Loss: 0.7570880502462387, Final Batch Loss: 0.22074882686138153\n",
      "Epoch 2488, Loss: 0.8462866097688675, Final Batch Loss: 0.3139362931251526\n",
      "Epoch 2489, Loss: 0.9142958372831345, Final Batch Loss: 0.37778958678245544\n",
      "Epoch 2490, Loss: 1.139790117740631, Final Batch Loss: 0.5804517865180969\n",
      "Epoch 2491, Loss: 0.7080845832824707, Final Batch Loss: 0.1755635142326355\n",
      "Epoch 2492, Loss: 0.8430342823266983, Final Batch Loss: 0.24844275414943695\n",
      "Epoch 2493, Loss: 0.8399590402841568, Final Batch Loss: 0.3159138560295105\n",
      "Epoch 2494, Loss: 0.9812595844268799, Final Batch Loss: 0.4363543391227722\n",
      "Epoch 2495, Loss: 0.7392563968896866, Final Batch Loss: 0.15719299018383026\n",
      "Epoch 2496, Loss: 0.715187780559063, Final Batch Loss: 0.11442448943853378\n",
      "Epoch 2497, Loss: 0.9120749831199646, Final Batch Loss: 0.4016665816307068\n",
      "Epoch 2498, Loss: 0.6275001615285873, Final Batch Loss: 0.11776885390281677\n",
      "Epoch 2499, Loss: 0.9400933533906937, Final Batch Loss: 0.38160404562950134\n",
      "Epoch 2500, Loss: 0.8753463327884674, Final Batch Loss: 0.35870498418807983\n",
      "Epoch 2501, Loss: 0.5659171417355537, Final Batch Loss: 0.024588800966739655\n",
      "Epoch 2502, Loss: 0.7343999147415161, Final Batch Loss: 0.14833003282546997\n",
      "Epoch 2503, Loss: 0.7309097647666931, Final Batch Loss: 0.18500134348869324\n",
      "Epoch 2504, Loss: 0.8522598743438721, Final Batch Loss: 0.3029843270778656\n",
      "Epoch 2505, Loss: 0.8880356848239899, Final Batch Loss: 0.3175930678844452\n",
      "Epoch 2506, Loss: 0.8114224672317505, Final Batch Loss: 0.28010740876197815\n",
      "Epoch 2507, Loss: 1.1877334117889404, Final Batch Loss: 0.6101248264312744\n",
      "Epoch 2508, Loss: 0.7011921554803848, Final Batch Loss: 0.1109754890203476\n",
      "Epoch 2509, Loss: 0.8508596867322922, Final Batch Loss: 0.3545406460762024\n",
      "Epoch 2510, Loss: 0.7821275591850281, Final Batch Loss: 0.2506183981895447\n",
      "Epoch 2511, Loss: 0.7764938622713089, Final Batch Loss: 0.17715813219547272\n",
      "Epoch 2512, Loss: 0.934255987405777, Final Batch Loss: 0.2996548116207123\n",
      "Epoch 2513, Loss: 0.717683732509613, Final Batch Loss: 0.21138811111450195\n",
      "Epoch 2514, Loss: 0.9032802581787109, Final Batch Loss: 0.33465784788131714\n",
      "Epoch 2515, Loss: 0.7359331846237183, Final Batch Loss: 0.21629539132118225\n",
      "Epoch 2516, Loss: 0.8407418578863144, Final Batch Loss: 0.2595057189464569\n",
      "Epoch 2517, Loss: 0.6887548118829727, Final Batch Loss: 0.19273579120635986\n",
      "Epoch 2518, Loss: 1.048990547657013, Final Batch Loss: 0.4519495666027069\n",
      "Epoch 2519, Loss: 0.7617079317569733, Final Batch Loss: 0.20413395762443542\n",
      "Epoch 2520, Loss: 0.7327003628015518, Final Batch Loss: 0.19028617441654205\n",
      "Epoch 2521, Loss: 0.791761189699173, Final Batch Loss: 0.2666034698486328\n",
      "Epoch 2522, Loss: 0.7001542970538139, Final Batch Loss: 0.08483948558568954\n",
      "Epoch 2523, Loss: 0.8587299883365631, Final Batch Loss: 0.3236443102359772\n",
      "Epoch 2524, Loss: 1.0572636872529984, Final Batch Loss: 0.5391209721565247\n",
      "Epoch 2525, Loss: 0.7615958601236343, Final Batch Loss: 0.19803859293460846\n",
      "Epoch 2526, Loss: 0.8243148475885391, Final Batch Loss: 0.3004651963710785\n",
      "Epoch 2527, Loss: 0.625706173479557, Final Batch Loss: 0.12055488675832748\n",
      "Epoch 2528, Loss: 0.7738111019134521, Final Batch Loss: 0.2301889955997467\n",
      "Epoch 2529, Loss: 0.7350420206785202, Final Batch Loss: 0.23283268511295319\n",
      "Epoch 2530, Loss: 1.1535347402095795, Final Batch Loss: 0.6559610366821289\n",
      "Epoch 2531, Loss: 0.986066460609436, Final Batch Loss: 0.4363897144794464\n",
      "Epoch 2532, Loss: 1.0173447728157043, Final Batch Loss: 0.47126248478889465\n",
      "Epoch 2533, Loss: 0.7836300581693649, Final Batch Loss: 0.14548088610172272\n",
      "Epoch 2534, Loss: 0.8849208950996399, Final Batch Loss: 0.23949509859085083\n",
      "Epoch 2535, Loss: 0.8873270750045776, Final Batch Loss: 0.3336660861968994\n",
      "Epoch 2536, Loss: 1.1000208258628845, Final Batch Loss: 0.545081377029419\n",
      "Epoch 2537, Loss: 0.6689963936805725, Final Batch Loss: 0.13928380608558655\n",
      "Epoch 2538, Loss: 0.5940110981464386, Final Batch Loss: 0.0658918172121048\n",
      "Epoch 2539, Loss: 0.8243029564619064, Final Batch Loss: 0.3041885197162628\n",
      "Epoch 2540, Loss: 0.7715651392936707, Final Batch Loss: 0.2750336527824402\n",
      "Epoch 2541, Loss: 0.6131433546543121, Final Batch Loss: 0.10045471787452698\n",
      "Epoch 2542, Loss: 0.6054580360651016, Final Batch Loss: 0.046828076243400574\n",
      "Epoch 2543, Loss: 0.8956694900989532, Final Batch Loss: 0.34481537342071533\n",
      "Epoch 2544, Loss: 0.8057028949260712, Final Batch Loss: 0.2864467203617096\n",
      "Epoch 2545, Loss: 0.9703257381916046, Final Batch Loss: 0.4432654082775116\n",
      "Epoch 2546, Loss: 0.7917178869247437, Final Batch Loss: 0.2562618553638458\n",
      "Epoch 2547, Loss: 0.8199619054794312, Final Batch Loss: 0.29294416308403015\n",
      "Epoch 2548, Loss: 0.6918039619922638, Final Batch Loss: 0.14741584658622742\n",
      "Epoch 2549, Loss: 0.8211983144283295, Final Batch Loss: 0.3066631257534027\n",
      "Epoch 2550, Loss: 0.8738365024328232, Final Batch Loss: 0.3298557996749878\n",
      "Epoch 2551, Loss: 0.9991387724876404, Final Batch Loss: 0.4716143310070038\n",
      "Epoch 2552, Loss: 1.016240417957306, Final Batch Loss: 0.45380643010139465\n",
      "Epoch 2553, Loss: 0.9416665136814117, Final Batch Loss: 0.37334075570106506\n",
      "Epoch 2554, Loss: 1.0020990073680878, Final Batch Loss: 0.42894721031188965\n",
      "Epoch 2555, Loss: 0.6740658581256866, Final Batch Loss: 0.07094249129295349\n",
      "Epoch 2556, Loss: 0.9405552446842194, Final Batch Loss: 0.3877287805080414\n",
      "Epoch 2557, Loss: 0.7509938627481461, Final Batch Loss: 0.19797904789447784\n",
      "Epoch 2558, Loss: 0.9521797299385071, Final Batch Loss: 0.4623192250728607\n",
      "Epoch 2559, Loss: 0.8201786428689957, Final Batch Loss: 0.2955434322357178\n",
      "Epoch 2560, Loss: 0.8716208040714264, Final Batch Loss: 0.33509963750839233\n",
      "Epoch 2561, Loss: 0.7399740964174271, Final Batch Loss: 0.19496311247348785\n",
      "Epoch 2562, Loss: 0.7684883326292038, Final Batch Loss: 0.2931773066520691\n",
      "Epoch 2563, Loss: 0.7807443588972092, Final Batch Loss: 0.245148703455925\n",
      "Epoch 2564, Loss: 0.7666502296924591, Final Batch Loss: 0.2517749071121216\n",
      "Epoch 2565, Loss: 0.68598672747612, Final Batch Loss: 0.17080268263816833\n",
      "Epoch 2566, Loss: 0.7571127414703369, Final Batch Loss: 0.23428811132907867\n",
      "Epoch 2567, Loss: 0.7505987882614136, Final Batch Loss: 0.17304688692092896\n",
      "Epoch 2568, Loss: 1.03206866979599, Final Batch Loss: 0.5273628234863281\n",
      "Epoch 2569, Loss: 0.6596988588571548, Final Batch Loss: 0.13007918000221252\n",
      "Epoch 2570, Loss: 0.7622911036014557, Final Batch Loss: 0.2787776589393616\n",
      "Epoch 2571, Loss: 0.7464637458324432, Final Batch Loss: 0.21548700332641602\n",
      "Epoch 2572, Loss: 0.6393701806664467, Final Batch Loss: 0.05332139879465103\n",
      "Epoch 2573, Loss: 0.8135401904582977, Final Batch Loss: 0.24182239174842834\n",
      "Epoch 2574, Loss: 1.090020939707756, Final Batch Loss: 0.5851788520812988\n",
      "Epoch 2575, Loss: 0.7485925108194351, Final Batch Loss: 0.1842547208070755\n",
      "Epoch 2576, Loss: 0.7005317658185959, Final Batch Loss: 0.2120918184518814\n",
      "Epoch 2577, Loss: 0.7669144868850708, Final Batch Loss: 0.23674452304840088\n",
      "Epoch 2578, Loss: 0.842970684170723, Final Batch Loss: 0.3320125639438629\n",
      "Epoch 2579, Loss: 0.9969007074832916, Final Batch Loss: 0.46648868918418884\n",
      "Epoch 2580, Loss: 0.7071745097637177, Final Batch Loss: 0.16778722405433655\n",
      "Epoch 2581, Loss: 1.065767526626587, Final Batch Loss: 0.5258801579475403\n",
      "Epoch 2582, Loss: 0.9854907989501953, Final Batch Loss: 0.39860618114471436\n",
      "Epoch 2583, Loss: 0.7877975553274155, Final Batch Loss: 0.22045011818408966\n",
      "Epoch 2584, Loss: 0.7275689244270325, Final Batch Loss: 0.13217103481292725\n",
      "Epoch 2585, Loss: 0.9310341775417328, Final Batch Loss: 0.3640281856060028\n",
      "Epoch 2586, Loss: 0.7458955645561218, Final Batch Loss: 0.22568698227405548\n",
      "Epoch 2587, Loss: 0.6887753009796143, Final Batch Loss: 0.17682914435863495\n",
      "Epoch 2588, Loss: 0.7678193151950836, Final Batch Loss: 0.2406073808670044\n",
      "Epoch 2589, Loss: 0.6577470302581787, Final Batch Loss: 0.13848115503787994\n",
      "Epoch 2590, Loss: 0.7184706926345825, Final Batch Loss: 0.12552791833877563\n",
      "Epoch 2591, Loss: 0.8977012634277344, Final Batch Loss: 0.41462814807891846\n",
      "Epoch 2592, Loss: 0.7741442173719406, Final Batch Loss: 0.23073254525661469\n",
      "Epoch 2593, Loss: 0.8268969804048538, Final Batch Loss: 0.22484390437602997\n",
      "Epoch 2594, Loss: 1.3244401216506958, Final Batch Loss: 0.7642247080802917\n",
      "Epoch 2595, Loss: 0.9241185486316681, Final Batch Loss: 0.3558378517627716\n",
      "Epoch 2596, Loss: 0.7229360789060593, Final Batch Loss: 0.14300356805324554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2597, Loss: 0.9298948347568512, Final Batch Loss: 0.2909964323043823\n",
      "Epoch 2598, Loss: 0.6426554843783379, Final Batch Loss: 0.07798456400632858\n",
      "Epoch 2599, Loss: 0.9679620862007141, Final Batch Loss: 0.38801252841949463\n",
      "Epoch 2600, Loss: 0.9126489460468292, Final Batch Loss: 0.3384473919868469\n",
      "Epoch 2601, Loss: 0.7949886620044708, Final Batch Loss: 0.27570411562919617\n",
      "Epoch 2602, Loss: 0.8485422134399414, Final Batch Loss: 0.31101545691490173\n",
      "Epoch 2603, Loss: 0.9422129392623901, Final Batch Loss: 0.39352554082870483\n",
      "Epoch 2604, Loss: 0.8523370325565338, Final Batch Loss: 0.2761760950088501\n",
      "Epoch 2605, Loss: 0.9912893772125244, Final Batch Loss: 0.4793933629989624\n",
      "Epoch 2606, Loss: 0.7043931484222412, Final Batch Loss: 0.13383041322231293\n",
      "Epoch 2607, Loss: 0.7227675914764404, Final Batch Loss: 0.15976554155349731\n",
      "Epoch 2608, Loss: 1.0138535797595978, Final Batch Loss: 0.39945805072784424\n",
      "Epoch 2609, Loss: 0.8437491953372955, Final Batch Loss: 0.26101863384246826\n",
      "Epoch 2610, Loss: 0.7623997926712036, Final Batch Loss: 0.21253882348537445\n",
      "Epoch 2611, Loss: 0.6193643361330032, Final Batch Loss: 0.04480147361755371\n",
      "Epoch 2612, Loss: 0.7392316311597824, Final Batch Loss: 0.252829909324646\n",
      "Epoch 2613, Loss: 0.9120430052280426, Final Batch Loss: 0.2505955994129181\n",
      "Epoch 2614, Loss: 1.0585331618785858, Final Batch Loss: 0.5108040571212769\n",
      "Epoch 2615, Loss: 0.7077623158693314, Final Batch Loss: 0.23280763626098633\n",
      "Epoch 2616, Loss: 0.6298001036047935, Final Batch Loss: 0.11660156399011612\n",
      "Epoch 2617, Loss: 0.7037414163351059, Final Batch Loss: 0.180284321308136\n",
      "Epoch 2618, Loss: 0.8114666044712067, Final Batch Loss: 0.21490955352783203\n",
      "Epoch 2619, Loss: 0.7465120553970337, Final Batch Loss: 0.20417708158493042\n",
      "Epoch 2620, Loss: 0.6928443908691406, Final Batch Loss: 0.15001672506332397\n",
      "Epoch 2621, Loss: 0.9022493958473206, Final Batch Loss: 0.2926335632801056\n",
      "Epoch 2622, Loss: 1.0863143503665924, Final Batch Loss: 0.5884931087493896\n",
      "Epoch 2623, Loss: 0.714116171002388, Final Batch Loss: 0.18778152763843536\n",
      "Epoch 2624, Loss: 0.8546871095895767, Final Batch Loss: 0.3620218336582184\n",
      "Epoch 2625, Loss: 0.6949855089187622, Final Batch Loss: 0.14685234427452087\n",
      "Epoch 2626, Loss: 0.7163656651973724, Final Batch Loss: 0.16439288854599\n",
      "Epoch 2627, Loss: 0.8386291563510895, Final Batch Loss: 0.2543764114379883\n",
      "Epoch 2628, Loss: 0.647272139787674, Final Batch Loss: 0.14190474152565002\n",
      "Epoch 2629, Loss: 0.8136170506477356, Final Batch Loss: 0.2588290274143219\n",
      "Epoch 2630, Loss: 0.7895957231521606, Final Batch Loss: 0.2981334328651428\n",
      "Epoch 2631, Loss: 0.8219136595726013, Final Batch Loss: 0.27402710914611816\n",
      "Epoch 2632, Loss: 0.7850040048360825, Final Batch Loss: 0.2997004985809326\n",
      "Epoch 2633, Loss: 0.7539787441492081, Final Batch Loss: 0.3030299246311188\n",
      "Epoch 2634, Loss: 0.8124865889549255, Final Batch Loss: 0.30535054206848145\n",
      "Epoch 2635, Loss: 0.7030045837163925, Final Batch Loss: 0.2102263867855072\n",
      "Epoch 2636, Loss: 0.7729958146810532, Final Batch Loss: 0.2829955220222473\n",
      "Epoch 2637, Loss: 0.6656752824783325, Final Batch Loss: 0.1611570417881012\n",
      "Epoch 2638, Loss: 0.7733898758888245, Final Batch Loss: 0.25148195028305054\n",
      "Epoch 2639, Loss: 0.7565418183803558, Final Batch Loss: 0.2520407736301422\n",
      "Epoch 2640, Loss: 0.9058551341295242, Final Batch Loss: 0.370874285697937\n",
      "Epoch 2641, Loss: 0.9418118000030518, Final Batch Loss: 0.3936910927295685\n",
      "Epoch 2642, Loss: 0.7891589105129242, Final Batch Loss: 0.2881665825843811\n",
      "Epoch 2643, Loss: 0.7266522943973541, Final Batch Loss: 0.19300982356071472\n",
      "Epoch 2644, Loss: 0.8446236848831177, Final Batch Loss: 0.35471975803375244\n",
      "Epoch 2645, Loss: 0.6341667324304581, Final Batch Loss: 0.15701763331890106\n",
      "Epoch 2646, Loss: 0.999024361371994, Final Batch Loss: 0.5333781242370605\n",
      "Epoch 2647, Loss: 0.7009360641241074, Final Batch Loss: 0.2162187695503235\n",
      "Epoch 2648, Loss: 0.7293704599142075, Final Batch Loss: 0.2173338681459427\n",
      "Epoch 2649, Loss: 0.89555624127388, Final Batch Loss: 0.4315291941165924\n",
      "Epoch 2650, Loss: 0.6899821013212204, Final Batch Loss: 0.18065261840820312\n",
      "Epoch 2651, Loss: 0.8546339869499207, Final Batch Loss: 0.31830427050590515\n",
      "Epoch 2652, Loss: 0.7716758698225021, Final Batch Loss: 0.22750116884708405\n",
      "Epoch 2653, Loss: 0.5916519984602928, Final Batch Loss: 0.0871329978108406\n",
      "Epoch 2654, Loss: 0.9667825996875763, Final Batch Loss: 0.505401611328125\n",
      "Epoch 2655, Loss: 0.7007875591516495, Final Batch Loss: 0.1571529507637024\n",
      "Epoch 2656, Loss: 0.7508862018585205, Final Batch Loss: 0.21339896321296692\n",
      "Epoch 2657, Loss: 0.8466363847255707, Final Batch Loss: 0.3638252913951874\n",
      "Epoch 2658, Loss: 0.8326504230499268, Final Batch Loss: 0.30624571442604065\n",
      "Epoch 2659, Loss: 1.0711243748664856, Final Batch Loss: 0.48404473066329956\n",
      "Epoch 2660, Loss: 0.8914456367492676, Final Batch Loss: 0.36048659682273865\n",
      "Epoch 2661, Loss: 0.8291293233633041, Final Batch Loss: 0.3167528212070465\n",
      "Epoch 2662, Loss: 0.7225427776575089, Final Batch Loss: 0.20859825611114502\n",
      "Epoch 2663, Loss: 0.9256821274757385, Final Batch Loss: 0.4159358739852905\n",
      "Epoch 2664, Loss: 0.7397869676351547, Final Batch Loss: 0.1562720388174057\n",
      "Epoch 2665, Loss: 0.9398440718650818, Final Batch Loss: 0.42782196402549744\n",
      "Epoch 2666, Loss: 0.715316042304039, Final Batch Loss: 0.22077420353889465\n",
      "Epoch 2667, Loss: 0.8482497036457062, Final Batch Loss: 0.2722897529602051\n",
      "Epoch 2668, Loss: 0.8706230819225311, Final Batch Loss: 0.39754050970077515\n",
      "Epoch 2669, Loss: 0.6210000887513161, Final Batch Loss: 0.11624259501695633\n",
      "Epoch 2670, Loss: 0.8163813203573227, Final Batch Loss: 0.3034555912017822\n",
      "Epoch 2671, Loss: 0.8320669531822205, Final Batch Loss: 0.3112421929836273\n",
      "Epoch 2672, Loss: 1.013433650135994, Final Batch Loss: 0.4403485655784607\n",
      "Epoch 2673, Loss: 0.7568851411342621, Final Batch Loss: 0.18956118822097778\n",
      "Epoch 2674, Loss: 0.7741347998380661, Final Batch Loss: 0.2057502418756485\n",
      "Epoch 2675, Loss: 0.6714812815189362, Final Batch Loss: 0.14066219329833984\n",
      "Epoch 2676, Loss: 0.791748583316803, Final Batch Loss: 0.2828925848007202\n",
      "Epoch 2677, Loss: 0.6963570713996887, Final Batch Loss: 0.15995781123638153\n",
      "Epoch 2678, Loss: 0.7447991967201233, Final Batch Loss: 0.22878536581993103\n",
      "Epoch 2679, Loss: 0.749228224158287, Final Batch Loss: 0.27711451053619385\n",
      "Epoch 2680, Loss: 0.7520686388015747, Final Batch Loss: 0.24444419145584106\n",
      "Epoch 2681, Loss: 0.7572653442621231, Final Batch Loss: 0.26554983854293823\n",
      "Epoch 2682, Loss: 0.6704791337251663, Final Batch Loss: 0.1916874647140503\n",
      "Epoch 2683, Loss: 0.6957508623600006, Final Batch Loss: 0.2350744903087616\n",
      "Epoch 2684, Loss: 0.6663090288639069, Final Batch Loss: 0.11734214425086975\n",
      "Epoch 2685, Loss: 0.6839935630559921, Final Batch Loss: 0.16271214187145233\n",
      "Epoch 2686, Loss: 0.8007915914058685, Final Batch Loss: 0.2605835795402527\n",
      "Epoch 2687, Loss: 0.7687834352254868, Final Batch Loss: 0.1573832482099533\n",
      "Epoch 2688, Loss: 0.7937621921300888, Final Batch Loss: 0.20006684958934784\n",
      "Epoch 2689, Loss: 1.1409934312105179, Final Batch Loss: 0.5842666625976562\n",
      "Epoch 2690, Loss: 0.602427564561367, Final Batch Loss: 0.1027887687087059\n",
      "Epoch 2691, Loss: 0.6060817614197731, Final Batch Loss: 0.11283757537603378\n",
      "Epoch 2692, Loss: 0.7999420464038849, Final Batch Loss: 0.298532098531723\n",
      "Epoch 2693, Loss: 0.7062662690877914, Final Batch Loss: 0.16357101500034332\n",
      "Epoch 2694, Loss: 0.8592560142278671, Final Batch Loss: 0.2595057189464569\n",
      "Epoch 2695, Loss: 0.5864085480570793, Final Batch Loss: 0.10715999454259872\n",
      "Epoch 2696, Loss: 0.9495342075824738, Final Batch Loss: 0.39503830671310425\n",
      "Epoch 2697, Loss: 0.842126652598381, Final Batch Loss: 0.3178417384624481\n",
      "Epoch 2698, Loss: 0.7693390250205994, Final Batch Loss: 0.2824755311012268\n",
      "Epoch 2699, Loss: 0.7919571399688721, Final Batch Loss: 0.2561355531215668\n",
      "Epoch 2700, Loss: 0.9068360179662704, Final Batch Loss: 0.4057638645172119\n",
      "Epoch 2701, Loss: 0.8514683097600937, Final Batch Loss: 0.3910501003265381\n",
      "Epoch 2702, Loss: 0.6632255464792252, Final Batch Loss: 0.16935153305530548\n",
      "Epoch 2703, Loss: 0.6568061113357544, Final Batch Loss: 0.1421738713979721\n",
      "Epoch 2704, Loss: 0.577290914952755, Final Batch Loss: 0.08237221091985703\n",
      "Epoch 2705, Loss: 0.7069927752017975, Final Batch Loss: 0.18978746235370636\n",
      "Epoch 2706, Loss: 0.688762903213501, Final Batch Loss: 0.1848246455192566\n",
      "Epoch 2707, Loss: 0.7091274559497833, Final Batch Loss: 0.18334807455539703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2708, Loss: 0.8477887064218521, Final Batch Loss: 0.28388792276382446\n",
      "Epoch 2709, Loss: 0.6872921288013458, Final Batch Loss: 0.1621076762676239\n",
      "Epoch 2710, Loss: 0.608237199485302, Final Batch Loss: 0.095326729118824\n",
      "Epoch 2711, Loss: 0.6845372468233109, Final Batch Loss: 0.15888479351997375\n",
      "Epoch 2712, Loss: 0.8437630236148834, Final Batch Loss: 0.3838143050670624\n",
      "Epoch 2713, Loss: 0.7528433501720428, Final Batch Loss: 0.2561448812484741\n",
      "Epoch 2714, Loss: 0.8860292434692383, Final Batch Loss: 0.34612393379211426\n",
      "Epoch 2715, Loss: 0.9420123845338821, Final Batch Loss: 0.41443559527397156\n",
      "Epoch 2716, Loss: 0.7735491096973419, Final Batch Loss: 0.23492205142974854\n",
      "Epoch 2717, Loss: 0.8738665282726288, Final Batch Loss: 0.2880353629589081\n",
      "Epoch 2718, Loss: 0.7845214307308197, Final Batch Loss: 0.2179279625415802\n",
      "Epoch 2719, Loss: 1.0261109471321106, Final Batch Loss: 0.5092740058898926\n",
      "Epoch 2720, Loss: 0.7533542513847351, Final Batch Loss: 0.28171247243881226\n",
      "Epoch 2721, Loss: 0.8140412271022797, Final Batch Loss: 0.2809614837169647\n",
      "Epoch 2722, Loss: 1.079738050699234, Final Batch Loss: 0.6073356866836548\n",
      "Epoch 2723, Loss: 0.7899062633514404, Final Batch Loss: 0.24632784724235535\n",
      "Epoch 2724, Loss: 0.6860523372888565, Final Batch Loss: 0.19408120214939117\n",
      "Epoch 2725, Loss: 0.64903324842453, Final Batch Loss: 0.11469277739524841\n",
      "Epoch 2726, Loss: 1.0019719004631042, Final Batch Loss: 0.4720846116542816\n",
      "Epoch 2727, Loss: 0.7673267275094986, Final Batch Loss: 0.30175724625587463\n",
      "Epoch 2728, Loss: 0.695110023021698, Final Batch Loss: 0.15346355736255646\n",
      "Epoch 2729, Loss: 0.9182998836040497, Final Batch Loss: 0.3361796736717224\n",
      "Epoch 2730, Loss: 0.7223565131425858, Final Batch Loss: 0.19546423852443695\n",
      "Epoch 2731, Loss: 0.804688885807991, Final Batch Loss: 0.29167479276657104\n",
      "Epoch 2732, Loss: 0.7178987264633179, Final Batch Loss: 0.1469365358352661\n",
      "Epoch 2733, Loss: 0.7867369204759598, Final Batch Loss: 0.28642067313194275\n",
      "Epoch 2734, Loss: 0.9763846099376678, Final Batch Loss: 0.37134552001953125\n",
      "Epoch 2735, Loss: 0.6826451122760773, Final Batch Loss: 0.1804083287715912\n",
      "Epoch 2736, Loss: 0.7333559989929199, Final Batch Loss: 0.20115654170513153\n",
      "Epoch 2737, Loss: 0.8032087087631226, Final Batch Loss: 0.32535219192504883\n",
      "Epoch 2738, Loss: 0.6208790391683578, Final Batch Loss: 0.14131443202495575\n",
      "Epoch 2739, Loss: 0.7597456723451614, Final Batch Loss: 0.2213045209646225\n",
      "Epoch 2740, Loss: 0.7124571949243546, Final Batch Loss: 0.18033204972743988\n",
      "Epoch 2741, Loss: 0.7949887365102768, Final Batch Loss: 0.1988438218832016\n",
      "Epoch 2742, Loss: 0.737266942858696, Final Batch Loss: 0.23977307975292206\n",
      "Epoch 2743, Loss: 0.7649734616279602, Final Batch Loss: 0.24749517440795898\n",
      "Epoch 2744, Loss: 0.9572926014661789, Final Batch Loss: 0.47797831892967224\n",
      "Epoch 2745, Loss: 0.5872020497918129, Final Batch Loss: 0.12357517331838608\n",
      "Epoch 2746, Loss: 0.7294668108224869, Final Batch Loss: 0.24602189660072327\n",
      "Epoch 2747, Loss: 0.6859458386898041, Final Batch Loss: 0.12928009033203125\n",
      "Epoch 2748, Loss: 0.6828831136226654, Final Batch Loss: 0.1440984010696411\n",
      "Epoch 2749, Loss: 0.6791913509368896, Final Batch Loss: 0.20013177394866943\n",
      "Epoch 2750, Loss: 0.6885064020752907, Final Batch Loss: 0.12399161607027054\n",
      "Epoch 2751, Loss: 0.8063252419233322, Final Batch Loss: 0.24615903198719025\n",
      "Epoch 2752, Loss: 0.84446582198143, Final Batch Loss: 0.3127758800983429\n",
      "Epoch 2753, Loss: 1.0659463107585907, Final Batch Loss: 0.6128162741661072\n",
      "Epoch 2754, Loss: 0.6072583943605423, Final Batch Loss: 0.13336564600467682\n",
      "Epoch 2755, Loss: 0.8522870391607285, Final Batch Loss: 0.3169896900653839\n",
      "Epoch 2756, Loss: 0.5759206339716911, Final Batch Loss: 0.10744637995958328\n",
      "Epoch 2757, Loss: 0.6038978807628155, Final Batch Loss: 0.04796506091952324\n",
      "Epoch 2758, Loss: 0.7232821136713028, Final Batch Loss: 0.08137957751750946\n",
      "Epoch 2759, Loss: 0.6689885705709457, Final Batch Loss: 0.21247677505016327\n",
      "Epoch 2760, Loss: 0.629117526113987, Final Batch Loss: 0.0897732749581337\n",
      "Epoch 2761, Loss: 0.8012660443782806, Final Batch Loss: 0.2862200438976288\n",
      "Epoch 2762, Loss: 0.7160018235445023, Final Batch Loss: 0.20408979058265686\n",
      "Epoch 2763, Loss: 0.7260648608207703, Final Batch Loss: 0.24739617109298706\n",
      "Epoch 2764, Loss: 0.7037810534238815, Final Batch Loss: 0.14275740087032318\n",
      "Epoch 2765, Loss: 0.7343111485242844, Final Batch Loss: 0.2773175537586212\n",
      "Epoch 2766, Loss: 0.9700352549552917, Final Batch Loss: 0.44674623012542725\n",
      "Epoch 2767, Loss: 0.9486167579889297, Final Batch Loss: 0.46744880080223083\n",
      "Epoch 2768, Loss: 1.032656580209732, Final Batch Loss: 0.5253618359565735\n",
      "Epoch 2769, Loss: 0.6882773190736771, Final Batch Loss: 0.1349777728319168\n",
      "Epoch 2770, Loss: 0.8359614908695221, Final Batch Loss: 0.37754151225090027\n",
      "Epoch 2771, Loss: 0.650951474905014, Final Batch Loss: 0.14484620094299316\n",
      "Epoch 2772, Loss: 0.7336565852165222, Final Batch Loss: 0.27634963393211365\n",
      "Epoch 2773, Loss: 0.7428113222122192, Final Batch Loss: 0.2373555600643158\n",
      "Epoch 2774, Loss: 0.7128842398524284, Final Batch Loss: 0.10085656493902206\n",
      "Epoch 2775, Loss: 0.7125266790390015, Final Batch Loss: 0.1333688497543335\n",
      "Epoch 2776, Loss: 0.6488076001405716, Final Batch Loss: 0.10145263373851776\n",
      "Epoch 2777, Loss: 0.8658424913883209, Final Batch Loss: 0.415699303150177\n",
      "Epoch 2778, Loss: 0.7884563505649567, Final Batch Loss: 0.307140976190567\n",
      "Epoch 2779, Loss: 0.7788214236497879, Final Batch Loss: 0.21621058881282806\n",
      "Epoch 2780, Loss: 0.669643223285675, Final Batch Loss: 0.18323388695716858\n",
      "Epoch 2781, Loss: 0.962085023522377, Final Batch Loss: 0.5187602639198303\n",
      "Epoch 2782, Loss: 0.5947074294090271, Final Batch Loss: 0.12700259685516357\n",
      "Epoch 2783, Loss: 0.8469721376895905, Final Batch Loss: 0.3839086592197418\n",
      "Epoch 2784, Loss: 0.7186708003282547, Final Batch Loss: 0.28276538848876953\n",
      "Epoch 2785, Loss: 0.9909012615680695, Final Batch Loss: 0.5013145804405212\n",
      "Epoch 2786, Loss: 0.607605867087841, Final Batch Loss: 0.09447898715734482\n",
      "Epoch 2787, Loss: 0.6065570116043091, Final Batch Loss: 0.06227460503578186\n",
      "Epoch 2788, Loss: 0.8386871814727783, Final Batch Loss: 0.2881505489349365\n",
      "Epoch 2789, Loss: 1.0398851037025452, Final Batch Loss: 0.5380819439888\n",
      "Epoch 2790, Loss: 0.6832715719938278, Final Batch Loss: 0.19905687868595123\n",
      "Epoch 2791, Loss: 0.9341749101877213, Final Batch Loss: 0.4484448730945587\n",
      "Epoch 2792, Loss: 0.6198260933160782, Final Batch Loss: 0.0929834395647049\n",
      "Epoch 2793, Loss: 0.9881640821695328, Final Batch Loss: 0.4389680027961731\n",
      "Epoch 2794, Loss: 0.7899400293827057, Final Batch Loss: 0.25697481632232666\n",
      "Epoch 2795, Loss: 1.0476045161485672, Final Batch Loss: 0.5620089769363403\n",
      "Epoch 2796, Loss: 0.7307924330234528, Final Batch Loss: 0.2389807403087616\n",
      "Epoch 2797, Loss: 0.7355852723121643, Final Batch Loss: 0.2005358636379242\n",
      "Epoch 2798, Loss: 0.6125407218933105, Final Batch Loss: 0.09068627655506134\n",
      "Epoch 2799, Loss: 0.8320200741291046, Final Batch Loss: 0.30517101287841797\n",
      "Epoch 2800, Loss: 0.7786372900009155, Final Batch Loss: 0.2937430143356323\n",
      "Epoch 2801, Loss: 0.6436784267425537, Final Batch Loss: 0.18605411052703857\n",
      "Epoch 2802, Loss: 0.8984823971986771, Final Batch Loss: 0.3816945552825928\n",
      "Epoch 2803, Loss: 0.8262268155813217, Final Batch Loss: 0.23667918145656586\n",
      "Epoch 2804, Loss: 0.832467645406723, Final Batch Loss: 0.3092629909515381\n",
      "Epoch 2805, Loss: 0.7769937068223953, Final Batch Loss: 0.28942611813545227\n",
      "Epoch 2806, Loss: 0.8491513133049011, Final Batch Loss: 0.30021440982818604\n",
      "Epoch 2807, Loss: 0.7800039350986481, Final Batch Loss: 0.2567073404788971\n",
      "Epoch 2808, Loss: 0.5693084299564362, Final Batch Loss: 0.06484603881835938\n",
      "Epoch 2809, Loss: 0.6100067794322968, Final Batch Loss: 0.10720434784889221\n",
      "Epoch 2810, Loss: 0.670862466096878, Final Batch Loss: 0.16786591708660126\n",
      "Epoch 2811, Loss: 0.7680807113647461, Final Batch Loss: 0.30080151557922363\n",
      "Epoch 2812, Loss: 0.934295266866684, Final Batch Loss: 0.46550479531288147\n",
      "Epoch 2813, Loss: 0.6831924319267273, Final Batch Loss: 0.1868942528963089\n",
      "Epoch 2814, Loss: 0.718439519405365, Final Batch Loss: 0.24252364039421082\n",
      "Epoch 2815, Loss: 0.7998824119567871, Final Batch Loss: 0.2581608295440674\n",
      "Epoch 2816, Loss: 0.7899492233991623, Final Batch Loss: 0.2832719683647156\n",
      "Epoch 2817, Loss: 0.7937463074922562, Final Batch Loss: 0.2869372069835663\n",
      "Epoch 2818, Loss: 0.8577341139316559, Final Batch Loss: 0.2866252660751343\n",
      "Epoch 2819, Loss: 0.9427970796823502, Final Batch Loss: 0.4323035776615143\n",
      "Epoch 2820, Loss: 0.757180780172348, Final Batch Loss: 0.296908438205719\n",
      "Epoch 2821, Loss: 0.7248179465532303, Final Batch Loss: 0.19126899540424347\n",
      "Epoch 2822, Loss: 0.7086481750011444, Final Batch Loss: 0.20473679900169373\n",
      "Epoch 2823, Loss: 0.8534474223852158, Final Batch Loss: 0.3618641495704651\n",
      "Epoch 2824, Loss: 0.6669899672269821, Final Batch Loss: 0.15882723033428192\n",
      "Epoch 2825, Loss: 0.6823954433202744, Final Batch Loss: 0.23437514901161194\n",
      "Epoch 2826, Loss: 0.5736732333898544, Final Batch Loss: 0.10336500406265259\n",
      "Epoch 2827, Loss: 0.8729547262191772, Final Batch Loss: 0.42010724544525146\n",
      "Epoch 2828, Loss: 0.6949018985033035, Final Batch Loss: 0.19050396978855133\n",
      "Epoch 2829, Loss: 0.9877817928791046, Final Batch Loss: 0.480487585067749\n",
      "Epoch 2830, Loss: 0.7714165896177292, Final Batch Loss: 0.3037603795528412\n",
      "Epoch 2831, Loss: 0.5650486685335636, Final Batch Loss: 0.052927423268556595\n",
      "Epoch 2832, Loss: 0.6435282379388809, Final Batch Loss: 0.13035158812999725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2833, Loss: 0.9287681877613068, Final Batch Loss: 0.3862733244895935\n",
      "Epoch 2834, Loss: 0.8507439643144608, Final Batch Loss: 0.3537543714046478\n",
      "Epoch 2835, Loss: 0.6937018185853958, Final Batch Loss: 0.20076048374176025\n",
      "Epoch 2836, Loss: 0.7669223845005035, Final Batch Loss: 0.3009271025657654\n",
      "Epoch 2837, Loss: 0.6583608090877533, Final Batch Loss: 0.13887619972229004\n",
      "Epoch 2838, Loss: 0.7172626107931137, Final Batch Loss: 0.18707652390003204\n",
      "Epoch 2839, Loss: 0.7732512503862381, Final Batch Loss: 0.23916567862033844\n",
      "Epoch 2840, Loss: 0.553801141679287, Final Batch Loss: 0.0815945640206337\n",
      "Epoch 2841, Loss: 0.76362045109272, Final Batch Loss: 0.2732555866241455\n",
      "Epoch 2842, Loss: 0.9263963401317596, Final Batch Loss: 0.41328340768814087\n",
      "Epoch 2843, Loss: 0.749264732003212, Final Batch Loss: 0.23197464644908905\n",
      "Epoch 2844, Loss: 1.1020135879516602, Final Batch Loss: 0.5554952025413513\n",
      "Epoch 2845, Loss: 0.7339865416288376, Final Batch Loss: 0.2056828737258911\n",
      "Epoch 2846, Loss: 0.6019569486379623, Final Batch Loss: 0.1341256946325302\n",
      "Epoch 2847, Loss: 0.711523711681366, Final Batch Loss: 0.16171014308929443\n",
      "Epoch 2848, Loss: 0.8067115247249603, Final Batch Loss: 0.31964749097824097\n",
      "Epoch 2849, Loss: 0.8824096322059631, Final Batch Loss: 0.3987394869327545\n",
      "Epoch 2850, Loss: 0.6365635395050049, Final Batch Loss: 0.1628538817167282\n",
      "Epoch 2851, Loss: 0.8561305105686188, Final Batch Loss: 0.3440866470336914\n",
      "Epoch 2852, Loss: 0.843588650226593, Final Batch Loss: 0.3066539466381073\n",
      "Epoch 2853, Loss: 0.6864683330059052, Final Batch Loss: 0.2029731720685959\n",
      "Epoch 2854, Loss: 0.9028875082731247, Final Batch Loss: 0.42259854078292847\n",
      "Epoch 2855, Loss: 0.7012367397546768, Final Batch Loss: 0.23659563064575195\n",
      "Epoch 2856, Loss: 0.8481084853410721, Final Batch Loss: 0.30128106474876404\n",
      "Epoch 2857, Loss: 0.8671037405729294, Final Batch Loss: 0.40344420075416565\n",
      "Epoch 2858, Loss: 0.6610765010118484, Final Batch Loss: 0.19782094657421112\n",
      "Epoch 2859, Loss: 0.6230362951755524, Final Batch Loss: 0.14985938370227814\n",
      "Epoch 2860, Loss: 0.7563413083553314, Final Batch Loss: 0.29737070202827454\n",
      "Epoch 2861, Loss: 0.7704804241657257, Final Batch Loss: 0.26703622937202454\n",
      "Epoch 2862, Loss: 0.7586319446563721, Final Batch Loss: 0.21162903308868408\n",
      "Epoch 2863, Loss: 0.8073137104511261, Final Batch Loss: 0.2624433934688568\n",
      "Epoch 2864, Loss: 0.6985168904066086, Final Batch Loss: 0.2376089096069336\n",
      "Epoch 2865, Loss: 0.8909692466259003, Final Batch Loss: 0.42723163962364197\n",
      "Epoch 2866, Loss: 0.6967664957046509, Final Batch Loss: 0.24392953515052795\n",
      "Epoch 2867, Loss: 0.9566569775342941, Final Batch Loss: 0.44956815242767334\n",
      "Epoch 2868, Loss: 0.6279482021927834, Final Batch Loss: 0.09250359982252121\n",
      "Epoch 2869, Loss: 0.6640349328517914, Final Batch Loss: 0.15977728366851807\n",
      "Epoch 2870, Loss: 0.6337374299764633, Final Batch Loss: 0.19661253690719604\n",
      "Epoch 2871, Loss: 0.8978148400783539, Final Batch Loss: 0.4344384968280792\n",
      "Epoch 2872, Loss: 0.6162940710783005, Final Batch Loss: 0.10862666368484497\n",
      "Epoch 2873, Loss: 0.9553740322589874, Final Batch Loss: 0.4119313359260559\n",
      "Epoch 2874, Loss: 0.7444553971290588, Final Batch Loss: 0.2817647457122803\n",
      "Epoch 2875, Loss: 0.6611878126859665, Final Batch Loss: 0.17510101199150085\n",
      "Epoch 2876, Loss: 0.7646339237689972, Final Batch Loss: 0.21661612391471863\n",
      "Epoch 2877, Loss: 0.7748346328735352, Final Batch Loss: 0.24577251076698303\n",
      "Epoch 2878, Loss: 0.7099508941173553, Final Batch Loss: 0.27771854400634766\n",
      "Epoch 2879, Loss: 0.8553938567638397, Final Batch Loss: 0.33065536618232727\n",
      "Epoch 2880, Loss: 0.8178475648164749, Final Batch Loss: 0.3169492781162262\n",
      "Epoch 2881, Loss: 0.9214191883802414, Final Batch Loss: 0.37148234248161316\n",
      "Epoch 2882, Loss: 0.8311638236045837, Final Batch Loss: 0.34815457463264465\n",
      "Epoch 2883, Loss: 0.958700641989708, Final Batch Loss: 0.4755760431289673\n",
      "Epoch 2884, Loss: 0.6570930927991867, Final Batch Loss: 0.15060043334960938\n",
      "Epoch 2885, Loss: 0.6506691873073578, Final Batch Loss: 0.15326569974422455\n",
      "Epoch 2886, Loss: 0.7144788056612015, Final Batch Loss: 0.2055983543395996\n",
      "Epoch 2887, Loss: 0.74537293612957, Final Batch Loss: 0.20082597434520721\n",
      "Epoch 2888, Loss: 0.6040478311479092, Final Batch Loss: 0.05642331764101982\n",
      "Epoch 2889, Loss: 0.8419080078601837, Final Batch Loss: 0.3189842700958252\n",
      "Epoch 2890, Loss: 0.7555500417947769, Final Batch Loss: 0.23670005798339844\n",
      "Epoch 2891, Loss: 0.6311487704515457, Final Batch Loss: 0.14262130856513977\n",
      "Epoch 2892, Loss: 0.7008901983499527, Final Batch Loss: 0.148692324757576\n",
      "Epoch 2893, Loss: 0.6740130484104156, Final Batch Loss: 0.17374877631664276\n",
      "Epoch 2894, Loss: 0.8851761817932129, Final Batch Loss: 0.3259492814540863\n",
      "Epoch 2895, Loss: 0.6555321514606476, Final Batch Loss: 0.16734561324119568\n",
      "Epoch 2896, Loss: 0.8548913896083832, Final Batch Loss: 0.3824921250343323\n",
      "Epoch 2897, Loss: 0.847040981054306, Final Batch Loss: 0.3860973119735718\n",
      "Epoch 2898, Loss: 0.6499212831258774, Final Batch Loss: 0.1318695992231369\n",
      "Epoch 2899, Loss: 0.7042751908302307, Final Batch Loss: 0.23308834433555603\n",
      "Epoch 2900, Loss: 0.593075692653656, Final Batch Loss: 0.07450893521308899\n",
      "Epoch 2901, Loss: 0.6594622433185577, Final Batch Loss: 0.1461043655872345\n",
      "Epoch 2902, Loss: 0.5692212805151939, Final Batch Loss: 0.034734539687633514\n",
      "Epoch 2903, Loss: 0.7252946496009827, Final Batch Loss: 0.22163031995296478\n",
      "Epoch 2904, Loss: 0.5845490545034409, Final Batch Loss: 0.1076297014951706\n",
      "Epoch 2905, Loss: 0.9998941123485565, Final Batch Loss: 0.49367794394493103\n",
      "Epoch 2906, Loss: 0.6468809992074966, Final Batch Loss: 0.14494113624095917\n",
      "Epoch 2907, Loss: 0.5885627642273903, Final Batch Loss: 0.09226729720830917\n",
      "Epoch 2908, Loss: 0.699935719370842, Final Batch Loss: 0.12648756802082062\n",
      "Epoch 2909, Loss: 0.7860424667596817, Final Batch Loss: 0.30969756841659546\n",
      "Epoch 2910, Loss: 0.598924458026886, Final Batch Loss: 0.13566377758979797\n",
      "Epoch 2911, Loss: 0.8140099048614502, Final Batch Loss: 0.34949541091918945\n",
      "Epoch 2912, Loss: 0.6590933352708817, Final Batch Loss: 0.20752687752246857\n",
      "Epoch 2913, Loss: 0.833958312869072, Final Batch Loss: 0.333209753036499\n",
      "Epoch 2914, Loss: 0.6147130131721497, Final Batch Loss: 0.1759735345840454\n",
      "Epoch 2915, Loss: 0.8125105202198029, Final Batch Loss: 0.30759140849113464\n",
      "Epoch 2916, Loss: 0.6906464993953705, Final Batch Loss: 0.17981462180614471\n",
      "Epoch 2917, Loss: 0.7191576510667801, Final Batch Loss: 0.22693009674549103\n",
      "Epoch 2918, Loss: 0.715406984090805, Final Batch Loss: 0.1580306887626648\n",
      "Epoch 2919, Loss: 0.7837619930505753, Final Batch Loss: 0.25050783157348633\n",
      "Epoch 2920, Loss: 1.1552744954824448, Final Batch Loss: 0.6538590788841248\n",
      "Epoch 2921, Loss: 0.8213462084531784, Final Batch Loss: 0.31595906615257263\n",
      "Epoch 2922, Loss: 0.6747844219207764, Final Batch Loss: 0.19034329056739807\n",
      "Epoch 2923, Loss: 0.7232926040887833, Final Batch Loss: 0.23035691678524017\n",
      "Epoch 2924, Loss: 0.7599026411771774, Final Batch Loss: 0.2858878970146179\n",
      "Epoch 2925, Loss: 0.7360111176967621, Final Batch Loss: 0.23554016649723053\n",
      "Epoch 2926, Loss: 0.5780982673168182, Final Batch Loss: 0.09630133211612701\n",
      "Epoch 2927, Loss: 0.5761793330311775, Final Batch Loss: 0.09219437092542648\n",
      "Epoch 2928, Loss: 0.7539156675338745, Final Batch Loss: 0.23149043321609497\n",
      "Epoch 2929, Loss: 0.5976286381483078, Final Batch Loss: 0.12800472974777222\n",
      "Epoch 2930, Loss: 0.7251179665327072, Final Batch Loss: 0.24430008232593536\n",
      "Epoch 2931, Loss: 0.8419845104217529, Final Batch Loss: 0.27060380578041077\n",
      "Epoch 2932, Loss: 0.9045567810535431, Final Batch Loss: 0.3623453676700592\n",
      "Epoch 2933, Loss: 0.6534086018800735, Final Batch Loss: 0.17815090715885162\n",
      "Epoch 2934, Loss: 0.6736143380403519, Final Batch Loss: 0.24946385622024536\n",
      "Epoch 2935, Loss: 0.8789262771606445, Final Batch Loss: 0.3902703821659088\n",
      "Epoch 2936, Loss: 0.8293581604957581, Final Batch Loss: 0.33621326088905334\n",
      "Epoch 2937, Loss: 0.779145285487175, Final Batch Loss: 0.20029506087303162\n",
      "Epoch 2938, Loss: 0.5665733367204666, Final Batch Loss: 0.07765182852745056\n",
      "Epoch 2939, Loss: 0.6692284792661667, Final Batch Loss: 0.17033211886882782\n",
      "Epoch 2940, Loss: 0.5652112364768982, Final Batch Loss: 0.07184194028377533\n",
      "Epoch 2941, Loss: 0.8407051265239716, Final Batch Loss: 0.36724311113357544\n",
      "Epoch 2942, Loss: 0.6428669542074203, Final Batch Loss: 0.13580693304538727\n",
      "Epoch 2943, Loss: 0.5924873724579811, Final Batch Loss: 0.101033054292202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2944, Loss: 0.6471660137176514, Final Batch Loss: 0.1899459809064865\n",
      "Epoch 2945, Loss: 0.5217247381806374, Final Batch Loss: 0.09137629717588425\n",
      "Epoch 2946, Loss: 0.6053086817264557, Final Batch Loss: 0.18631677329540253\n",
      "Epoch 2947, Loss: 0.5897066667675972, Final Batch Loss: 0.11591539531946182\n",
      "Epoch 2948, Loss: 0.6436981856822968, Final Batch Loss: 0.17956972122192383\n",
      "Epoch 2949, Loss: 0.5861302763223648, Final Batch Loss: 0.12278462946414948\n",
      "Epoch 2950, Loss: 0.6763617247343063, Final Batch Loss: 0.21795383095741272\n",
      "Epoch 2951, Loss: 0.7009316235780716, Final Batch Loss: 0.14776483178138733\n",
      "Epoch 2952, Loss: 0.6953108012676239, Final Batch Loss: 0.21260635554790497\n",
      "Epoch 2953, Loss: 0.7135293930768967, Final Batch Loss: 0.16724614799022675\n",
      "Epoch 2954, Loss: 0.5792003497481346, Final Batch Loss: 0.09401381760835648\n",
      "Epoch 2955, Loss: 0.6675508618354797, Final Batch Loss: 0.14778101444244385\n",
      "Epoch 2956, Loss: 0.7100517302751541, Final Batch Loss: 0.24431584775447845\n",
      "Epoch 2957, Loss: 0.543586254119873, Final Batch Loss: 0.0890343189239502\n",
      "Epoch 2958, Loss: 0.621889054775238, Final Batch Loss: 0.16303610801696777\n",
      "Epoch 2959, Loss: 0.6030469983816147, Final Batch Loss: 0.14083194732666016\n",
      "Epoch 2960, Loss: 0.9956983029842377, Final Batch Loss: 0.41648179292678833\n",
      "Epoch 2961, Loss: 0.6579506993293762, Final Batch Loss: 0.1399858295917511\n",
      "Epoch 2962, Loss: 0.6983833163976669, Final Batch Loss: 0.20448730885982513\n",
      "Epoch 2963, Loss: 0.7430105656385422, Final Batch Loss: 0.29188624024391174\n",
      "Epoch 2964, Loss: 0.8074912428855896, Final Batch Loss: 0.30202674865722656\n",
      "Epoch 2965, Loss: 0.580966517329216, Final Batch Loss: 0.166179820895195\n",
      "Epoch 2966, Loss: 0.6257278397679329, Final Batch Loss: 0.06690063327550888\n",
      "Epoch 2967, Loss: 0.7883520424365997, Final Batch Loss: 0.2856704294681549\n",
      "Epoch 2968, Loss: 0.7085917443037033, Final Batch Loss: 0.19012267887592316\n",
      "Epoch 2969, Loss: 0.7258876115083694, Final Batch Loss: 0.25399249792099\n",
      "Epoch 2970, Loss: 0.6485752090811729, Final Batch Loss: 0.11619653552770615\n",
      "Epoch 2971, Loss: 0.6937084048986435, Final Batch Loss: 0.15941114723682404\n",
      "Epoch 2972, Loss: 0.5985177531838417, Final Batch Loss: 0.10930437594652176\n",
      "Epoch 2973, Loss: 0.8343895971775055, Final Batch Loss: 0.3575958013534546\n",
      "Epoch 2974, Loss: 0.7962937951087952, Final Batch Loss: 0.32744336128234863\n",
      "Epoch 2975, Loss: 0.6197087913751602, Final Batch Loss: 0.12842579185962677\n",
      "Epoch 2976, Loss: 0.625568650662899, Final Batch Loss: 0.07922818511724472\n",
      "Epoch 2977, Loss: 0.6016879081726074, Final Batch Loss: 0.16909831762313843\n",
      "Epoch 2978, Loss: 0.5965358465909958, Final Batch Loss: 0.13179078698158264\n",
      "Epoch 2979, Loss: 0.811532661318779, Final Batch Loss: 0.27349621057510376\n",
      "Epoch 2980, Loss: 0.5629468187689781, Final Batch Loss: 0.10392259806394577\n",
      "Epoch 2981, Loss: 0.7405779510736465, Final Batch Loss: 0.1832057386636734\n",
      "Epoch 2982, Loss: 0.6030663102865219, Final Batch Loss: 0.12402549386024475\n",
      "Epoch 2983, Loss: 0.7065457254648209, Final Batch Loss: 0.24898548424243927\n",
      "Epoch 2984, Loss: 0.7707372605800629, Final Batch Loss: 0.3281765878200531\n",
      "Epoch 2985, Loss: 0.9149008989334106, Final Batch Loss: 0.3910980224609375\n",
      "Epoch 2986, Loss: 0.5625487230718136, Final Batch Loss: 0.043810632079839706\n",
      "Epoch 2987, Loss: 0.7925701141357422, Final Batch Loss: 0.2739946246147156\n",
      "Epoch 2988, Loss: 0.6074051558971405, Final Batch Loss: 0.14834612607955933\n",
      "Epoch 2989, Loss: 0.7221279591321945, Final Batch Loss: 0.24280698597431183\n",
      "Epoch 2990, Loss: 0.6294086128473282, Final Batch Loss: 0.1489894986152649\n",
      "Epoch 2991, Loss: 0.6071783900260925, Final Batch Loss: 0.12444299459457397\n",
      "Epoch 2992, Loss: 0.6223372220993042, Final Batch Loss: 0.09707702696323395\n",
      "Epoch 2993, Loss: 0.7925091087818146, Final Batch Loss: 0.30674752593040466\n",
      "Epoch 2994, Loss: 0.8183728754520416, Final Batch Loss: 0.32737502455711365\n",
      "Epoch 2995, Loss: 0.8943711519241333, Final Batch Loss: 0.3633480966091156\n",
      "Epoch 2996, Loss: 1.0699495524168015, Final Batch Loss: 0.5844280123710632\n",
      "Epoch 2997, Loss: 0.9988906979560852, Final Batch Loss: 0.4686880111694336\n",
      "Epoch 2998, Loss: 0.9646891951560974, Final Batch Loss: 0.4863070547580719\n",
      "Epoch 2999, Loss: 0.7013236582279205, Final Batch Loss: 0.2815499007701874\n",
      "Epoch 3000, Loss: 0.6920408606529236, Final Batch Loss: 0.2275683730840683\n",
      "Epoch 3001, Loss: 0.6339844167232513, Final Batch Loss: 0.1798132359981537\n",
      "Epoch 3002, Loss: 0.531238853931427, Final Batch Loss: 0.04553946852684021\n",
      "Epoch 3003, Loss: 0.7002626806497574, Final Batch Loss: 0.26952099800109863\n",
      "Epoch 3004, Loss: 0.7218389213085175, Final Batch Loss: 0.186894491314888\n",
      "Epoch 3005, Loss: 0.7741217166185379, Final Batch Loss: 0.3084520697593689\n",
      "Epoch 3006, Loss: 0.5806461423635483, Final Batch Loss: 0.1663379669189453\n",
      "Epoch 3007, Loss: 0.6370773315429688, Final Batch Loss: 0.1967056542634964\n",
      "Epoch 3008, Loss: 0.6628381162881851, Final Batch Loss: 0.1549045443534851\n",
      "Epoch 3009, Loss: 0.7781796604394913, Final Batch Loss: 0.34288111329078674\n",
      "Epoch 3010, Loss: 0.6428235024213791, Final Batch Loss: 0.1561124324798584\n",
      "Epoch 3011, Loss: 0.6284811049699783, Final Batch Loss: 0.1757517158985138\n",
      "Epoch 3012, Loss: 0.7624126672744751, Final Batch Loss: 0.328095406293869\n",
      "Epoch 3013, Loss: 0.8959684669971466, Final Batch Loss: 0.4388352930545807\n",
      "Epoch 3014, Loss: 0.6437021493911743, Final Batch Loss: 0.18550652265548706\n",
      "Epoch 3015, Loss: 0.616991862654686, Final Batch Loss: 0.12231069803237915\n",
      "Epoch 3016, Loss: 0.8373562842607498, Final Batch Loss: 0.4203277826309204\n",
      "Epoch 3017, Loss: 0.9255197793245316, Final Batch Loss: 0.40249356627464294\n",
      "Epoch 3018, Loss: 0.6824351698160172, Final Batch Loss: 0.18708135187625885\n",
      "Epoch 3019, Loss: 0.5168412253260612, Final Batch Loss: 0.045370154082775116\n",
      "Epoch 3020, Loss: 0.6348914802074432, Final Batch Loss: 0.16050980985164642\n",
      "Epoch 3021, Loss: 0.7307273894548416, Final Batch Loss: 0.24888527393341064\n",
      "Epoch 3022, Loss: 0.8565536588430405, Final Batch Loss: 0.38425248861312866\n",
      "Epoch 3023, Loss: 0.8955440372228622, Final Batch Loss: 0.33562853932380676\n",
      "Epoch 3024, Loss: 0.690605953335762, Final Batch Loss: 0.2039812058210373\n",
      "Epoch 3025, Loss: 0.7554823905229568, Final Batch Loss: 0.24213369190692902\n",
      "Epoch 3026, Loss: 0.7956973910331726, Final Batch Loss: 0.25062212347984314\n",
      "Epoch 3027, Loss: 0.5856870263814926, Final Batch Loss: 0.15589691698551178\n",
      "Epoch 3028, Loss: 0.6142197549343109, Final Batch Loss: 0.1461312174797058\n",
      "Epoch 3029, Loss: 0.8856159597635269, Final Batch Loss: 0.40368330478668213\n",
      "Epoch 3030, Loss: 1.1141435950994492, Final Batch Loss: 0.654943585395813\n",
      "Epoch 3031, Loss: 0.7918563634157181, Final Batch Loss: 0.31506800651550293\n",
      "Epoch 3032, Loss: 0.6516772955656052, Final Batch Loss: 0.16707119345664978\n",
      "Epoch 3033, Loss: 0.6514084488153458, Final Batch Loss: 0.17051063477993011\n",
      "Epoch 3034, Loss: 0.6939892917871475, Final Batch Loss: 0.15042532980442047\n",
      "Epoch 3035, Loss: 0.6333563476800919, Final Batch Loss: 0.1533852219581604\n",
      "Epoch 3036, Loss: 0.629078283905983, Final Batch Loss: 0.12596772611141205\n",
      "Epoch 3037, Loss: 0.5347055047750473, Final Batch Loss: 0.08731701970100403\n",
      "Epoch 3038, Loss: 0.7373283952474594, Final Batch Loss: 0.2575497627258301\n",
      "Epoch 3039, Loss: 0.5757878869771957, Final Batch Loss: 0.14472874999046326\n",
      "Epoch 3040, Loss: 0.6001683175563812, Final Batch Loss: 0.19583049416542053\n",
      "Epoch 3041, Loss: 0.6095075011253357, Final Batch Loss: 0.14901626110076904\n",
      "Epoch 3042, Loss: 0.6238122433423996, Final Batch Loss: 0.11746580898761749\n",
      "Epoch 3043, Loss: 0.7493450343608856, Final Batch Loss: 0.2133154571056366\n",
      "Epoch 3044, Loss: 0.7711441963911057, Final Batch Loss: 0.31697604060173035\n",
      "Epoch 3045, Loss: 0.599113866686821, Final Batch Loss: 0.10223203897476196\n",
      "Epoch 3046, Loss: 0.7029865086078644, Final Batch Loss: 0.24634727835655212\n",
      "Epoch 3047, Loss: 0.7999021261930466, Final Batch Loss: 0.35257264971733093\n",
      "Epoch 3048, Loss: 0.8676722049713135, Final Batch Loss: 0.38701048493385315\n",
      "Epoch 3049, Loss: 0.6621040254831314, Final Batch Loss: 0.18202517926692963\n",
      "Epoch 3050, Loss: 0.7429382354021072, Final Batch Loss: 0.26143646240234375\n",
      "Epoch 3051, Loss: 0.6482124552130699, Final Batch Loss: 0.12321191281080246\n",
      "Epoch 3052, Loss: 0.9076829105615616, Final Batch Loss: 0.44554755091667175\n",
      "Epoch 3053, Loss: 0.8909883052110672, Final Batch Loss: 0.34213098883628845\n",
      "Epoch 3054, Loss: 0.6988785713911057, Final Batch Loss: 0.1960955113172531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3055, Loss: 0.660769060254097, Final Batch Loss: 0.20015698671340942\n",
      "Epoch 3056, Loss: 0.6135800369083881, Final Batch Loss: 0.04849282279610634\n",
      "Epoch 3057, Loss: 0.9309100657701492, Final Batch Loss: 0.43636584281921387\n",
      "Epoch 3058, Loss: 0.524211797863245, Final Batch Loss: 0.043631527572870255\n",
      "Epoch 3059, Loss: 0.7533138990402222, Final Batch Loss: 0.21818512678146362\n",
      "Epoch 3060, Loss: 0.8639485836029053, Final Batch Loss: 0.3199869692325592\n",
      "Epoch 3061, Loss: 0.6099942773580551, Final Batch Loss: 0.145844504237175\n",
      "Epoch 3062, Loss: 0.7166680246591568, Final Batch Loss: 0.27573370933532715\n",
      "Epoch 3063, Loss: 0.7529661655426025, Final Batch Loss: 0.30958718061447144\n",
      "Epoch 3064, Loss: 0.7393391728401184, Final Batch Loss: 0.2875728905200958\n",
      "Epoch 3065, Loss: 0.8459282219409943, Final Batch Loss: 0.3284030258655548\n",
      "Epoch 3066, Loss: 0.661082997918129, Final Batch Loss: 0.23449769616127014\n",
      "Epoch 3067, Loss: 0.6543296724557877, Final Batch Loss: 0.17601358890533447\n",
      "Epoch 3068, Loss: 0.5406674593687057, Final Batch Loss: 0.12206697463989258\n",
      "Epoch 3069, Loss: 0.8664661943912506, Final Batch Loss: 0.3769565522670746\n",
      "Epoch 3070, Loss: 0.885898619890213, Final Batch Loss: 0.37277957797050476\n",
      "Epoch 3071, Loss: 0.8071911931037903, Final Batch Loss: 0.3547944128513336\n",
      "Epoch 3072, Loss: 0.8385073393583298, Final Batch Loss: 0.3627024292945862\n",
      "Epoch 3073, Loss: 0.8038346767425537, Final Batch Loss: 0.33177581429481506\n",
      "Epoch 3074, Loss: 0.8576182425022125, Final Batch Loss: 0.34530261158943176\n",
      "Epoch 3075, Loss: 1.0691502094268799, Final Batch Loss: 0.46079596877098083\n",
      "Epoch 3076, Loss: 0.7190618216991425, Final Batch Loss: 0.23262786865234375\n",
      "Epoch 3077, Loss: 0.5789458155632019, Final Batch Loss: 0.12973089516162872\n",
      "Epoch 3078, Loss: 0.7064478248357773, Final Batch Loss: 0.2889660596847534\n",
      "Epoch 3079, Loss: 0.6685792207717896, Final Batch Loss: 0.15572138130664825\n",
      "Epoch 3080, Loss: 0.668303057551384, Final Batch Loss: 0.19088433682918549\n",
      "Epoch 3081, Loss: 0.4853031262755394, Final Batch Loss: 0.04076472669839859\n",
      "Epoch 3082, Loss: 0.7015414535999298, Final Batch Loss: 0.20955999195575714\n",
      "Epoch 3083, Loss: 0.8305580765008926, Final Batch Loss: 0.27917298674583435\n",
      "Epoch 3084, Loss: 0.8216101825237274, Final Batch Loss: 0.2901390790939331\n",
      "Epoch 3085, Loss: 0.6768135577440262, Final Batch Loss: 0.21136438846588135\n",
      "Epoch 3086, Loss: 0.9878396391868591, Final Batch Loss: 0.48711323738098145\n",
      "Epoch 3087, Loss: 0.7458484768867493, Final Batch Loss: 0.2069537341594696\n",
      "Epoch 3088, Loss: 0.7331523895263672, Final Batch Loss: 0.21593379974365234\n",
      "Epoch 3089, Loss: 0.7808258384466171, Final Batch Loss: 0.315782755613327\n",
      "Epoch 3090, Loss: 0.8468955606222153, Final Batch Loss: 0.33800992369651794\n",
      "Epoch 3091, Loss: 0.6856607645750046, Final Batch Loss: 0.22332505881786346\n",
      "Epoch 3092, Loss: 0.6004989668726921, Final Batch Loss: 0.10764791816473007\n",
      "Epoch 3093, Loss: 0.8784886598587036, Final Batch Loss: 0.3562421500682831\n",
      "Epoch 3094, Loss: 0.7196305096149445, Final Batch Loss: 0.21670381724834442\n",
      "Epoch 3095, Loss: 0.6122080534696579, Final Batch Loss: 0.08532293140888214\n",
      "Epoch 3096, Loss: 0.7655963003635406, Final Batch Loss: 0.3180774748325348\n",
      "Epoch 3097, Loss: 0.6900033354759216, Final Batch Loss: 0.19257310032844543\n",
      "Epoch 3098, Loss: 0.6532180160284042, Final Batch Loss: 0.11536256968975067\n",
      "Epoch 3099, Loss: 0.7268111556768417, Final Batch Loss: 0.2510035037994385\n",
      "Epoch 3100, Loss: 0.6423545032739639, Final Batch Loss: 0.16496336460113525\n",
      "Epoch 3101, Loss: 0.7746735960245132, Final Batch Loss: 0.2601078152656555\n",
      "Epoch 3102, Loss: 0.7476193159818649, Final Batch Loss: 0.26566097140312195\n",
      "Epoch 3103, Loss: 0.6954413205385208, Final Batch Loss: 0.23588210344314575\n",
      "Epoch 3104, Loss: 0.7467262446880341, Final Batch Loss: 0.19058531522750854\n",
      "Epoch 3105, Loss: 0.6937389820814133, Final Batch Loss: 0.1946762353181839\n",
      "Epoch 3106, Loss: 0.8039203733205795, Final Batch Loss: 0.3251892328262329\n",
      "Epoch 3107, Loss: 0.757814034819603, Final Batch Loss: 0.21623735129833221\n",
      "Epoch 3108, Loss: 0.8927404880523682, Final Batch Loss: 0.4256957769393921\n",
      "Epoch 3109, Loss: 0.7393101900815964, Final Batch Loss: 0.27355462312698364\n",
      "Epoch 3110, Loss: 0.7962425649166107, Final Batch Loss: 0.29865169525146484\n",
      "Epoch 3111, Loss: 0.5718651451170444, Final Batch Loss: 0.05578865483403206\n",
      "Epoch 3112, Loss: 0.8103133291006088, Final Batch Loss: 0.34307441115379333\n",
      "Epoch 3113, Loss: 0.7116077840328217, Final Batch Loss: 0.26019248366355896\n",
      "Epoch 3114, Loss: 0.5554829016327858, Final Batch Loss: 0.09744804352521896\n",
      "Epoch 3115, Loss: 0.5910896807909012, Final Batch Loss: 0.12032534182071686\n",
      "Epoch 3116, Loss: 0.620319053530693, Final Batch Loss: 0.14341363310813904\n",
      "Epoch 3117, Loss: 0.7407041937112808, Final Batch Loss: 0.26989367604255676\n",
      "Epoch 3118, Loss: 0.8147222846746445, Final Batch Loss: 0.3282129466533661\n",
      "Epoch 3119, Loss: 0.725143626332283, Final Batch Loss: 0.19731594622135162\n",
      "Epoch 3120, Loss: 0.6513301283121109, Final Batch Loss: 0.193553626537323\n",
      "Epoch 3121, Loss: 0.6777480691671371, Final Batch Loss: 0.15452523529529572\n",
      "Epoch 3122, Loss: 0.8044378906488419, Final Batch Loss: 0.3609279990196228\n",
      "Epoch 3123, Loss: 0.8604703694581985, Final Batch Loss: 0.4050326645374298\n",
      "Epoch 3124, Loss: 0.7628001421689987, Final Batch Loss: 0.35521432757377625\n",
      "Epoch 3125, Loss: 0.5823458954691887, Final Batch Loss: 0.1062396839261055\n",
      "Epoch 3126, Loss: 0.7148476541042328, Final Batch Loss: 0.2940513491630554\n",
      "Epoch 3127, Loss: 0.8476012498140335, Final Batch Loss: 0.3588928282260895\n",
      "Epoch 3128, Loss: 0.7176379412412643, Final Batch Loss: 0.23879174888134003\n",
      "Epoch 3129, Loss: 0.5816573351621628, Final Batch Loss: 0.13025961816310883\n",
      "Epoch 3130, Loss: 0.6213968172669411, Final Batch Loss: 0.12066376954317093\n",
      "Epoch 3131, Loss: 0.8040890246629715, Final Batch Loss: 0.25980639457702637\n",
      "Epoch 3132, Loss: 0.5917677581310272, Final Batch Loss: 0.15852655470371246\n",
      "Epoch 3133, Loss: 0.7254200428724289, Final Batch Loss: 0.25321125984191895\n",
      "Epoch 3134, Loss: 0.7201767265796661, Final Batch Loss: 0.25081148743629456\n",
      "Epoch 3135, Loss: 0.8200415670871735, Final Batch Loss: 0.2977159023284912\n",
      "Epoch 3136, Loss: 0.6959332823753357, Final Batch Loss: 0.21078546345233917\n",
      "Epoch 3137, Loss: 0.7277327924966812, Final Batch Loss: 0.28659892082214355\n",
      "Epoch 3138, Loss: 0.827171802520752, Final Batch Loss: 0.3253718316555023\n",
      "Epoch 3139, Loss: 0.7350866198539734, Final Batch Loss: 0.2416498214006424\n",
      "Epoch 3140, Loss: 0.6642829328775406, Final Batch Loss: 0.18480367958545685\n",
      "Epoch 3141, Loss: 0.7144313305616379, Final Batch Loss: 0.23553195595741272\n",
      "Epoch 3142, Loss: 0.781220018863678, Final Batch Loss: 0.2142617553472519\n",
      "Epoch 3143, Loss: 0.7645312994718552, Final Batch Loss: 0.23206619918346405\n",
      "Epoch 3144, Loss: 0.7881018221378326, Final Batch Loss: 0.27977827191352844\n",
      "Epoch 3145, Loss: 0.6452143043279648, Final Batch Loss: 0.1866542398929596\n",
      "Epoch 3146, Loss: 0.5988271459937096, Final Batch Loss: 0.11069046705961227\n",
      "Epoch 3147, Loss: 0.7813971191644669, Final Batch Loss: 0.32334789633750916\n",
      "Epoch 3148, Loss: 0.8032415062189102, Final Batch Loss: 0.31636929512023926\n",
      "Epoch 3149, Loss: 0.6648955196142197, Final Batch Loss: 0.21191737055778503\n",
      "Epoch 3150, Loss: 0.8451331853866577, Final Batch Loss: 0.4116980731487274\n",
      "Epoch 3151, Loss: 0.5374634265899658, Final Batch Loss: 0.1011308878660202\n",
      "Epoch 3152, Loss: 0.6578311175107956, Final Batch Loss: 0.22916056215763092\n",
      "Epoch 3153, Loss: 0.6467583924531937, Final Batch Loss: 0.1798435002565384\n",
      "Epoch 3154, Loss: 0.6044879406690598, Final Batch Loss: 0.16643410921096802\n",
      "Epoch 3155, Loss: 0.7921215146780014, Final Batch Loss: 0.24522726237773895\n",
      "Epoch 3156, Loss: 0.7649553269147873, Final Batch Loss: 0.33187782764434814\n",
      "Epoch 3157, Loss: 0.6571511030197144, Final Batch Loss: 0.1638265997171402\n",
      "Epoch 3158, Loss: 0.6956144869327545, Final Batch Loss: 0.266554594039917\n",
      "Epoch 3159, Loss: 0.8069668412208557, Final Batch Loss: 0.3342062830924988\n",
      "Epoch 3160, Loss: 0.6708500981330872, Final Batch Loss: 0.22270111739635468\n",
      "Epoch 3161, Loss: 0.5383618921041489, Final Batch Loss: 0.036856845021247864\n",
      "Epoch 3162, Loss: 0.7871531099081039, Final Batch Loss: 0.3085242211818695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3163, Loss: 0.6230103820562363, Final Batch Loss: 0.14540725946426392\n",
      "Epoch 3164, Loss: 0.7550758123397827, Final Batch Loss: 0.2765933573246002\n",
      "Epoch 3165, Loss: 1.071160152554512, Final Batch Loss: 0.6304405331611633\n",
      "Epoch 3166, Loss: 0.774979829788208, Final Batch Loss: 0.32571008801460266\n",
      "Epoch 3167, Loss: 0.5406152382493019, Final Batch Loss: 0.1046086773276329\n",
      "Epoch 3168, Loss: 0.866615504026413, Final Batch Loss: 0.3996071219444275\n",
      "Epoch 3169, Loss: 0.8211440145969391, Final Batch Loss: 0.38675910234451294\n",
      "Epoch 3170, Loss: 0.8520037233829498, Final Batch Loss: 0.3206358551979065\n",
      "Epoch 3171, Loss: 0.7431668490171432, Final Batch Loss: 0.23666521906852722\n",
      "Epoch 3172, Loss: 0.6912553012371063, Final Batch Loss: 0.16499854624271393\n",
      "Epoch 3173, Loss: 0.8764523267745972, Final Batch Loss: 0.32214781641960144\n",
      "Epoch 3174, Loss: 0.7472219318151474, Final Batch Loss: 0.1873370260000229\n",
      "Epoch 3175, Loss: 0.8050597012042999, Final Batch Loss: 0.32704874873161316\n",
      "Epoch 3176, Loss: 0.662241131067276, Final Batch Loss: 0.20261113345623016\n",
      "Epoch 3177, Loss: 0.8047463297843933, Final Batch Loss: 0.33024129271507263\n",
      "Epoch 3178, Loss: 0.6755243390798569, Final Batch Loss: 0.22822746634483337\n",
      "Epoch 3179, Loss: 0.7165470719337463, Final Batch Loss: 0.1976083666086197\n",
      "Epoch 3180, Loss: 0.7372231483459473, Final Batch Loss: 0.22557954490184784\n",
      "Epoch 3181, Loss: 0.6803949922323227, Final Batch Loss: 0.17363706231117249\n",
      "Epoch 3182, Loss: 1.2807853072881699, Final Batch Loss: 0.7651451826095581\n",
      "Epoch 3183, Loss: 0.7130947411060333, Final Batch Loss: 0.17965799570083618\n",
      "Epoch 3184, Loss: 0.7415028363466263, Final Batch Loss: 0.20652690529823303\n",
      "Epoch 3185, Loss: 0.7865258902311325, Final Batch Loss: 0.3113989233970642\n",
      "Epoch 3186, Loss: 0.49788133054971695, Final Batch Loss: 0.04108845442533493\n",
      "Epoch 3187, Loss: 0.8235096782445908, Final Batch Loss: 0.3756055235862732\n",
      "Epoch 3188, Loss: 0.7034288495779037, Final Batch Loss: 0.15322832763195038\n",
      "Epoch 3189, Loss: 0.8725444972515106, Final Batch Loss: 0.4007507264614105\n",
      "Epoch 3190, Loss: 0.8015388995409012, Final Batch Loss: 0.3092649579048157\n",
      "Epoch 3191, Loss: 0.7645019888877869, Final Batch Loss: 0.23275792598724365\n",
      "Epoch 3192, Loss: 0.5642807558178902, Final Batch Loss: 0.0587286576628685\n",
      "Epoch 3193, Loss: 0.8240687549114227, Final Batch Loss: 0.3595324754714966\n",
      "Epoch 3194, Loss: 0.7983658611774445, Final Batch Loss: 0.35799258947372437\n",
      "Epoch 3195, Loss: 0.7365564703941345, Final Batch Loss: 0.2284117192029953\n",
      "Epoch 3196, Loss: 0.9605935513973236, Final Batch Loss: 0.42169007658958435\n",
      "Epoch 3197, Loss: 0.685018852353096, Final Batch Loss: 0.23220644891262054\n",
      "Epoch 3198, Loss: 0.7942842096090317, Final Batch Loss: 0.34141162037849426\n",
      "Epoch 3199, Loss: 0.7510276585817337, Final Batch Loss: 0.2756703197956085\n",
      "Epoch 3200, Loss: 0.9296440035104752, Final Batch Loss: 0.47029992938041687\n",
      "Epoch 3201, Loss: 0.6028138995170593, Final Batch Loss: 0.14373750984668732\n",
      "Epoch 3202, Loss: 0.7600221335887909, Final Batch Loss: 0.2983093559741974\n",
      "Epoch 3203, Loss: 0.6627754420042038, Final Batch Loss: 0.1789756566286087\n",
      "Epoch 3204, Loss: 0.5599278062582016, Final Batch Loss: 0.11190837621688843\n",
      "Epoch 3205, Loss: 0.8248162865638733, Final Batch Loss: 0.321824312210083\n",
      "Epoch 3206, Loss: 0.9549945890903473, Final Batch Loss: 0.4776473343372345\n",
      "Epoch 3207, Loss: 0.5928197354078293, Final Batch Loss: 0.08991457521915436\n",
      "Epoch 3208, Loss: 0.8361599445343018, Final Batch Loss: 0.3905482292175293\n",
      "Epoch 3209, Loss: 0.7949635982513428, Final Batch Loss: 0.3122604489326477\n",
      "Epoch 3210, Loss: 0.7567891776561737, Final Batch Loss: 0.3146393299102783\n",
      "Epoch 3211, Loss: 0.682405486702919, Final Batch Loss: 0.2242770493030548\n",
      "Epoch 3212, Loss: 0.5901229977607727, Final Batch Loss: 0.12591461837291718\n",
      "Epoch 3213, Loss: 0.5597697347402573, Final Batch Loss: 0.13681961596012115\n",
      "Epoch 3214, Loss: 0.8045494854450226, Final Batch Loss: 0.30650654435157776\n",
      "Epoch 3215, Loss: 0.6256773769855499, Final Batch Loss: 0.1282404661178589\n",
      "Epoch 3216, Loss: 0.6079577803611755, Final Batch Loss: 0.17418687045574188\n",
      "Epoch 3217, Loss: 0.7186020761728287, Final Batch Loss: 0.24047917127609253\n",
      "Epoch 3218, Loss: 0.7449712157249451, Final Batch Loss: 0.24193862080574036\n",
      "Epoch 3219, Loss: 0.8765411525964737, Final Batch Loss: 0.3883674144744873\n",
      "Epoch 3220, Loss: 0.5457081273198128, Final Batch Loss: 0.0747138187289238\n",
      "Epoch 3221, Loss: 0.8815360218286514, Final Batch Loss: 0.35837841033935547\n",
      "Epoch 3222, Loss: 0.7999589294195175, Final Batch Loss: 0.22340500354766846\n",
      "Epoch 3223, Loss: 0.6996828615665436, Final Batch Loss: 0.28110018372535706\n",
      "Epoch 3224, Loss: 0.5343712493777275, Final Batch Loss: 0.04920477420091629\n",
      "Epoch 3225, Loss: 0.6488149017095566, Final Batch Loss: 0.2188478708267212\n",
      "Epoch 3226, Loss: 0.6148319244384766, Final Batch Loss: 0.15345612168312073\n",
      "Epoch 3227, Loss: 0.8181415647268295, Final Batch Loss: 0.3793860375881195\n",
      "Epoch 3228, Loss: 0.7490861713886261, Final Batch Loss: 0.27326369285583496\n",
      "Epoch 3229, Loss: 0.8330867737531662, Final Batch Loss: 0.35974186658859253\n",
      "Epoch 3230, Loss: 0.518852673470974, Final Batch Loss: 0.11815979331731796\n",
      "Epoch 3231, Loss: 0.6080518811941147, Final Batch Loss: 0.1794537454843521\n",
      "Epoch 3232, Loss: 0.6000512540340424, Final Batch Loss: 0.1458493322134018\n",
      "Epoch 3233, Loss: 0.6744949817657471, Final Batch Loss: 0.24032454192638397\n",
      "Epoch 3234, Loss: 1.0284340977668762, Final Batch Loss: 0.5833678245544434\n",
      "Epoch 3235, Loss: 0.5780174881219864, Final Batch Loss: 0.13181571662425995\n",
      "Epoch 3236, Loss: 0.7130668312311172, Final Batch Loss: 0.2851683795452118\n",
      "Epoch 3237, Loss: 0.5539115145802498, Final Batch Loss: 0.08816719800233841\n",
      "Epoch 3238, Loss: 0.5784969180822372, Final Batch Loss: 0.15141522884368896\n",
      "Epoch 3239, Loss: 0.5941494554281235, Final Batch Loss: 0.13921068608760834\n",
      "Epoch 3240, Loss: 0.6361554563045502, Final Batch Loss: 0.1657608449459076\n",
      "Epoch 3241, Loss: 0.49208052828907967, Final Batch Loss: 0.05977398529648781\n",
      "Epoch 3242, Loss: 0.8336533755064011, Final Batch Loss: 0.3272860050201416\n",
      "Epoch 3243, Loss: 0.683611199259758, Final Batch Loss: 0.2543541193008423\n",
      "Epoch 3244, Loss: 0.7131032347679138, Final Batch Loss: 0.17568767070770264\n",
      "Epoch 3245, Loss: 0.5115367472171783, Final Batch Loss: 0.11867548525333405\n",
      "Epoch 3246, Loss: 0.6683446019887924, Final Batch Loss: 0.20050902664661407\n",
      "Epoch 3247, Loss: 0.5653603076934814, Final Batch Loss: 0.14691190421581268\n",
      "Epoch 3248, Loss: 0.8247366547584534, Final Batch Loss: 0.4013911187648773\n",
      "Epoch 3249, Loss: 0.6498973965644836, Final Batch Loss: 0.12532854080200195\n",
      "Epoch 3250, Loss: 0.6409222334623337, Final Batch Loss: 0.2071809321641922\n",
      "Epoch 3251, Loss: 0.684458538889885, Final Batch Loss: 0.13935957849025726\n",
      "Epoch 3252, Loss: 0.6001107171177864, Final Batch Loss: 0.04110348969697952\n",
      "Epoch 3253, Loss: 0.6835182458162308, Final Batch Loss: 0.1962515264749527\n",
      "Epoch 3254, Loss: 0.8627524077892303, Final Batch Loss: 0.37738510966300964\n",
      "Epoch 3255, Loss: 1.1967383921146393, Final Batch Loss: 0.6659456491470337\n",
      "Epoch 3256, Loss: 0.5330763123929501, Final Batch Loss: 0.05873919650912285\n",
      "Epoch 3257, Loss: 0.7137290090322495, Final Batch Loss: 0.20568665862083435\n",
      "Epoch 3258, Loss: 0.7042961269617081, Final Batch Loss: 0.16798363626003265\n",
      "Epoch 3259, Loss: 0.8426015377044678, Final Batch Loss: 0.40251967310905457\n",
      "Epoch 3260, Loss: 0.9610026627779007, Final Batch Loss: 0.4840579330921173\n",
      "Epoch 3261, Loss: 0.6427747309207916, Final Batch Loss: 0.18737073242664337\n",
      "Epoch 3262, Loss: 0.5968630090355873, Final Batch Loss: 0.06628217548131943\n",
      "Epoch 3263, Loss: 0.8507641851902008, Final Batch Loss: 0.34392109513282776\n",
      "Epoch 3264, Loss: 0.5035881996154785, Final Batch Loss: 0.1021685004234314\n",
      "Epoch 3265, Loss: 0.7001400887966156, Final Batch Loss: 0.23603853583335876\n",
      "Epoch 3266, Loss: 0.6574913859367371, Final Batch Loss: 0.21073029935359955\n",
      "Epoch 3267, Loss: 0.680869922041893, Final Batch Loss: 0.21668313443660736\n",
      "Epoch 3268, Loss: 0.5556519478559494, Final Batch Loss: 0.12165124714374542\n",
      "Epoch 3269, Loss: 0.6899731904268265, Final Batch Loss: 0.19628112018108368\n",
      "Epoch 3270, Loss: 0.6924319118261337, Final Batch Loss: 0.27591827511787415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3271, Loss: 0.6126794219017029, Final Batch Loss: 0.10928486287593842\n",
      "Epoch 3272, Loss: 0.6743216663599014, Final Batch Loss: 0.1700499802827835\n",
      "Epoch 3273, Loss: 0.8569033294916153, Final Batch Loss: 0.3245680332183838\n",
      "Epoch 3274, Loss: 0.7810442745685577, Final Batch Loss: 0.27120253443717957\n",
      "Epoch 3275, Loss: 0.6677267998456955, Final Batch Loss: 0.21240659058094025\n",
      "Epoch 3276, Loss: 0.6148251742124557, Final Batch Loss: 0.17413325607776642\n",
      "Epoch 3277, Loss: 0.831088125705719, Final Batch Loss: 0.38033896684646606\n",
      "Epoch 3278, Loss: 0.7361233234405518, Final Batch Loss: 0.31482556462287903\n",
      "Epoch 3279, Loss: 0.6666934192180634, Final Batch Loss: 0.12015527486801147\n",
      "Epoch 3280, Loss: 0.6559696644544601, Final Batch Loss: 0.17265014350414276\n",
      "Epoch 3281, Loss: 0.7740879654884338, Final Batch Loss: 0.2799989879131317\n",
      "Epoch 3282, Loss: 0.7624441981315613, Final Batch Loss: 0.31951460242271423\n",
      "Epoch 3283, Loss: 0.9228871464729309, Final Batch Loss: 0.42979928851127625\n",
      "Epoch 3284, Loss: 0.8888412415981293, Final Batch Loss: 0.4049510955810547\n",
      "Epoch 3285, Loss: 0.5748706310987473, Final Batch Loss: 0.13512513041496277\n",
      "Epoch 3286, Loss: 0.5723486989736557, Final Batch Loss: 0.11399421095848083\n",
      "Epoch 3287, Loss: 1.071537122130394, Final Batch Loss: 0.5891437530517578\n",
      "Epoch 3288, Loss: 0.7531142085790634, Final Batch Loss: 0.23457999527454376\n",
      "Epoch 3289, Loss: 0.5798106864094734, Final Batch Loss: 0.11202212423086166\n",
      "Epoch 3290, Loss: 0.6770100593566895, Final Batch Loss: 0.23585480451583862\n",
      "Epoch 3291, Loss: 0.7099189460277557, Final Batch Loss: 0.25134921073913574\n",
      "Epoch 3292, Loss: 0.6283567845821381, Final Batch Loss: 0.23022131621837616\n",
      "Epoch 3293, Loss: 0.7886733561754227, Final Batch Loss: 0.2910696268081665\n",
      "Epoch 3294, Loss: 0.6269859671592712, Final Batch Loss: 0.18909384310245514\n",
      "Epoch 3295, Loss: 0.7341216653585434, Final Batch Loss: 0.33210256695747375\n",
      "Epoch 3296, Loss: 0.7296573519706726, Final Batch Loss: 0.33959510922431946\n",
      "Epoch 3297, Loss: 0.6012088805437088, Final Batch Loss: 0.19405095279216766\n",
      "Epoch 3298, Loss: 0.6249523013830185, Final Batch Loss: 0.13367797434329987\n",
      "Epoch 3299, Loss: 0.8373805731534958, Final Batch Loss: 0.3354977071285248\n",
      "Epoch 3300, Loss: 0.6821136921644211, Final Batch Loss: 0.2126559019088745\n",
      "Epoch 3301, Loss: 0.7236821502447128, Final Batch Loss: 0.24058027565479279\n",
      "Epoch 3302, Loss: 0.6885097026824951, Final Batch Loss: 0.24769175052642822\n",
      "Epoch 3303, Loss: 0.7881324887275696, Final Batch Loss: 0.2979680299758911\n",
      "Epoch 3304, Loss: 0.7018310129642487, Final Batch Loss: 0.22237925231456757\n",
      "Epoch 3305, Loss: 0.548699289560318, Final Batch Loss: 0.12583129107952118\n",
      "Epoch 3306, Loss: 0.6601166129112244, Final Batch Loss: 0.25847432017326355\n",
      "Epoch 3307, Loss: 0.8170219659805298, Final Batch Loss: 0.35250550508499146\n",
      "Epoch 3308, Loss: 0.7930941879749298, Final Batch Loss: 0.3308117389678955\n",
      "Epoch 3309, Loss: 0.6726841032505035, Final Batch Loss: 0.26152491569519043\n",
      "Epoch 3310, Loss: 0.753990888595581, Final Batch Loss: 0.2550412714481354\n",
      "Epoch 3311, Loss: 0.8011252731084824, Final Batch Loss: 0.36039021611213684\n",
      "Epoch 3312, Loss: 0.654648095369339, Final Batch Loss: 0.09710624814033508\n",
      "Epoch 3313, Loss: 0.7513109892606735, Final Batch Loss: 0.21444977819919586\n",
      "Epoch 3314, Loss: 0.6932414025068283, Final Batch Loss: 0.2086251974105835\n",
      "Epoch 3315, Loss: 0.7497128397226334, Final Batch Loss: 0.3523935377597809\n",
      "Epoch 3316, Loss: 0.6592338234186172, Final Batch Loss: 0.23105569183826447\n",
      "Epoch 3317, Loss: 0.9073852896690369, Final Batch Loss: 0.5168188214302063\n",
      "Epoch 3318, Loss: 0.5291609466075897, Final Batch Loss: 0.0895899087190628\n",
      "Epoch 3319, Loss: 0.6057170778512955, Final Batch Loss: 0.17718839645385742\n",
      "Epoch 3320, Loss: 0.6128838658332825, Final Batch Loss: 0.17163732647895813\n",
      "Epoch 3321, Loss: 0.6972234696149826, Final Batch Loss: 0.26605263352394104\n",
      "Epoch 3322, Loss: 0.5341391563415527, Final Batch Loss: 0.12471765279769897\n",
      "Epoch 3323, Loss: 0.6222102046012878, Final Batch Loss: 0.1590951681137085\n",
      "Epoch 3324, Loss: 0.6312191635370255, Final Batch Loss: 0.1802801638841629\n",
      "Epoch 3325, Loss: 0.6899649053812027, Final Batch Loss: 0.17902226746082306\n",
      "Epoch 3326, Loss: 0.6228781342506409, Final Batch Loss: 0.2005055546760559\n",
      "Epoch 3327, Loss: 0.533871091902256, Final Batch Loss: 0.11386805027723312\n",
      "Epoch 3328, Loss: 0.691105529665947, Final Batch Loss: 0.3010247051715851\n",
      "Epoch 3329, Loss: 0.8177269101142883, Final Batch Loss: 0.3973062336444855\n",
      "Epoch 3330, Loss: 0.5930881500244141, Final Batch Loss: 0.1812019795179367\n",
      "Epoch 3331, Loss: 0.6842739284038544, Final Batch Loss: 0.16218748688697815\n",
      "Epoch 3332, Loss: 0.5847632810473442, Final Batch Loss: 0.10379498451948166\n",
      "Epoch 3333, Loss: 0.5788891389966011, Final Batch Loss: 0.1102527603507042\n",
      "Epoch 3334, Loss: 0.7670762687921524, Final Batch Loss: 0.3030592203140259\n",
      "Epoch 3335, Loss: 0.6450913399457932, Final Batch Loss: 0.20409727096557617\n",
      "Epoch 3336, Loss: 0.6545900851488113, Final Batch Loss: 0.22391454875469208\n",
      "Epoch 3337, Loss: 0.8982759714126587, Final Batch Loss: 0.4514409601688385\n",
      "Epoch 3338, Loss: 0.5398236140608788, Final Batch Loss: 0.11733932048082352\n",
      "Epoch 3339, Loss: 0.8648654073476791, Final Batch Loss: 0.4186551868915558\n",
      "Epoch 3340, Loss: 0.5881527438759804, Final Batch Loss: 0.1007857695221901\n",
      "Epoch 3341, Loss: 0.6103967949748039, Final Batch Loss: 0.11128260940313339\n",
      "Epoch 3342, Loss: 0.7819979190826416, Final Batch Loss: 0.2897354066371918\n",
      "Epoch 3343, Loss: 0.5760525315999985, Final Batch Loss: 0.16750583052635193\n",
      "Epoch 3344, Loss: 1.001260444521904, Final Batch Loss: 0.5258370637893677\n",
      "Epoch 3345, Loss: 0.6422208249568939, Final Batch Loss: 0.17184115946292877\n",
      "Epoch 3346, Loss: 0.5826304107904434, Final Batch Loss: 0.11370903253555298\n",
      "Epoch 3347, Loss: 0.6518914997577667, Final Batch Loss: 0.14576761424541473\n",
      "Epoch 3348, Loss: 0.5890259146690369, Final Batch Loss: 0.13978080451488495\n",
      "Epoch 3349, Loss: 0.7457127869129181, Final Batch Loss: 0.319974809885025\n",
      "Epoch 3350, Loss: 0.5444043278694153, Final Batch Loss: 0.06539827585220337\n",
      "Epoch 3351, Loss: 0.6132580637931824, Final Batch Loss: 0.17632927000522614\n",
      "Epoch 3352, Loss: 0.6031527072191238, Final Batch Loss: 0.18472349643707275\n",
      "Epoch 3353, Loss: 0.6965106874704361, Final Batch Loss: 0.22246350347995758\n",
      "Epoch 3354, Loss: 0.5556300580501556, Final Batch Loss: 0.1369134932756424\n",
      "Epoch 3355, Loss: 0.5000315085053444, Final Batch Loss: 0.056398384273052216\n",
      "Epoch 3356, Loss: 0.9905749559402466, Final Batch Loss: 0.5357200503349304\n",
      "Epoch 3357, Loss: 0.5434808190912008, Final Batch Loss: 0.028892552480101585\n",
      "Epoch 3358, Loss: 0.5164863802492619, Final Batch Loss: 0.037479568272829056\n",
      "Epoch 3359, Loss: 0.7976459264755249, Final Batch Loss: 0.3923304080963135\n",
      "Epoch 3360, Loss: 0.8001847565174103, Final Batch Loss: 0.21684569120407104\n",
      "Epoch 3361, Loss: 0.8565429747104645, Final Batch Loss: 0.3961338698863983\n",
      "Epoch 3362, Loss: 0.6669414788484573, Final Batch Loss: 0.20563171803951263\n",
      "Epoch 3363, Loss: 1.2923717349767685, Final Batch Loss: 0.8271839618682861\n",
      "Epoch 3364, Loss: 0.5780831575393677, Final Batch Loss: 0.14061906933784485\n",
      "Epoch 3365, Loss: 0.5630974173545837, Final Batch Loss: 0.12567825615406036\n",
      "Epoch 3366, Loss: 0.7702718824148178, Final Batch Loss: 0.27026158571243286\n",
      "Epoch 3367, Loss: 0.7954381853342056, Final Batch Loss: 0.3291900157928467\n",
      "Epoch 3368, Loss: 0.8211835473775864, Final Batch Loss: 0.3708987832069397\n",
      "Epoch 3369, Loss: 0.802656352519989, Final Batch Loss: 0.31675273180007935\n",
      "Epoch 3370, Loss: 0.6357709914445877, Final Batch Loss: 0.13757160305976868\n",
      "Epoch 3371, Loss: 0.6955328434705734, Final Batch Loss: 0.15492211282253265\n",
      "Epoch 3372, Loss: 0.7976388335227966, Final Batch Loss: 0.2547254264354706\n",
      "Epoch 3373, Loss: 0.5020693689584732, Final Batch Loss: 0.042994529008865356\n",
      "Epoch 3374, Loss: 0.684657022356987, Final Batch Loss: 0.2828329801559448\n",
      "Epoch 3375, Loss: 0.7272181510925293, Final Batch Loss: 0.24701440334320068\n",
      "Epoch 3376, Loss: 0.8635248094797134, Final Batch Loss: 0.46931809186935425\n",
      "Epoch 3377, Loss: 0.6150884479284286, Final Batch Loss: 0.2107708901166916\n",
      "Epoch 3378, Loss: 0.841194212436676, Final Batch Loss: 0.36599379777908325\n",
      "Epoch 3379, Loss: 0.6376577466726303, Final Batch Loss: 0.18184852600097656\n",
      "Epoch 3380, Loss: 0.591537170112133, Final Batch Loss: 0.053387872874736786\n",
      "Epoch 3381, Loss: 0.4934180825948715, Final Batch Loss: 0.03051532804965973\n",
      "Epoch 3382, Loss: 0.8158973604440689, Final Batch Loss: 0.3434045612812042\n",
      "Epoch 3383, Loss: 0.7262441962957382, Final Batch Loss: 0.20311446487903595\n",
      "Epoch 3384, Loss: 0.698224201798439, Final Batch Loss: 0.16759687662124634\n",
      "Epoch 3385, Loss: 0.8630473762750626, Final Batch Loss: 0.3452996015548706\n",
      "Epoch 3386, Loss: 1.004368782043457, Final Batch Loss: 0.5231753587722778\n",
      "Epoch 3387, Loss: 0.6201226264238358, Final Batch Loss: 0.0920848399400711\n",
      "Epoch 3388, Loss: 0.6948682218790054, Final Batch Loss: 0.22587834298610687\n",
      "Epoch 3389, Loss: 0.6682756543159485, Final Batch Loss: 0.18587499856948853\n",
      "Epoch 3390, Loss: 0.637779712677002, Final Batch Loss: 0.18865691125392914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3391, Loss: 0.7273141890764236, Final Batch Loss: 0.2776334285736084\n",
      "Epoch 3392, Loss: 0.5343258380889893, Final Batch Loss: 0.10829775035381317\n",
      "Epoch 3393, Loss: 0.685840517282486, Final Batch Loss: 0.19728848338127136\n",
      "Epoch 3394, Loss: 0.6909281760454178, Final Batch Loss: 0.1509065181016922\n",
      "Epoch 3395, Loss: 0.5276487097144127, Final Batch Loss: 0.10299880057573318\n",
      "Epoch 3396, Loss: 1.1938514858484268, Final Batch Loss: 0.7374523282051086\n",
      "Epoch 3397, Loss: 0.5218183770775795, Final Batch Loss: 0.09640791267156601\n",
      "Epoch 3398, Loss: 0.7270016819238663, Final Batch Loss: 0.28835415840148926\n",
      "Epoch 3399, Loss: 0.47969263792037964, Final Batch Loss: 0.07184180617332458\n",
      "Epoch 3400, Loss: 0.7389297932386398, Final Batch Loss: 0.27989280223846436\n",
      "Epoch 3401, Loss: 0.7216029018163681, Final Batch Loss: 0.2580735683441162\n",
      "Epoch 3402, Loss: 0.5992400348186493, Final Batch Loss: 0.21314093470573425\n",
      "Epoch 3403, Loss: 0.9829446524381638, Final Batch Loss: 0.5763228535652161\n",
      "Epoch 3404, Loss: 0.634318009018898, Final Batch Loss: 0.1758454293012619\n",
      "Epoch 3405, Loss: 0.5566584318876266, Final Batch Loss: 0.14868886768817902\n",
      "Epoch 3406, Loss: 0.7713190913200378, Final Batch Loss: 0.31977933645248413\n",
      "Epoch 3407, Loss: 0.6803324818611145, Final Batch Loss: 0.25022372603416443\n",
      "Epoch 3408, Loss: 0.5291528031229973, Final Batch Loss: 0.06365617364645004\n",
      "Epoch 3409, Loss: 0.6800735592842102, Final Batch Loss: 0.216363787651062\n",
      "Epoch 3410, Loss: 0.5836028307676315, Final Batch Loss: 0.15017546713352203\n",
      "Epoch 3411, Loss: 0.7634301483631134, Final Batch Loss: 0.32564887404441833\n",
      "Epoch 3412, Loss: 0.869601234793663, Final Batch Loss: 0.4283473491668701\n",
      "Epoch 3413, Loss: 0.6494979709386826, Final Batch Loss: 0.20711654424667358\n",
      "Epoch 3414, Loss: 0.966405838727951, Final Batch Loss: 0.5499574542045593\n",
      "Epoch 3415, Loss: 0.8856120854616165, Final Batch Loss: 0.39727312326431274\n",
      "Epoch 3416, Loss: 0.6001651883125305, Final Batch Loss: 0.1881580799818039\n",
      "Epoch 3417, Loss: 0.8464933335781097, Final Batch Loss: 0.3538157045841217\n",
      "Epoch 3418, Loss: 0.730782613158226, Final Batch Loss: 0.25071075558662415\n",
      "Epoch 3419, Loss: 0.5932058393955231, Final Batch Loss: 0.168585404753685\n",
      "Epoch 3420, Loss: 0.5983659774065018, Final Batch Loss: 0.1592998504638672\n",
      "Epoch 3421, Loss: 0.625047966837883, Final Batch Loss: 0.13621334731578827\n",
      "Epoch 3422, Loss: 0.5571133941411972, Final Batch Loss: 0.15280140936374664\n",
      "Epoch 3423, Loss: 0.899765282869339, Final Batch Loss: 0.46548792719841003\n",
      "Epoch 3424, Loss: 0.650203213095665, Final Batch Loss: 0.2081817090511322\n",
      "Epoch 3425, Loss: 0.7387379109859467, Final Batch Loss: 0.29817211627960205\n",
      "Epoch 3426, Loss: 0.6252093762159348, Final Batch Loss: 0.21727687120437622\n",
      "Epoch 3427, Loss: 0.6837827265262604, Final Batch Loss: 0.22434282302856445\n",
      "Epoch 3428, Loss: 0.6012497246265411, Final Batch Loss: 0.12412041425704956\n",
      "Epoch 3429, Loss: 0.7077968567609787, Final Batch Loss: 0.28286802768707275\n",
      "Epoch 3430, Loss: 0.7734325975179672, Final Batch Loss: 0.31788453459739685\n",
      "Epoch 3431, Loss: 0.5218987427651882, Final Batch Loss: 0.05467111989855766\n",
      "Epoch 3432, Loss: 0.5185419209301472, Final Batch Loss: 0.04669833555817604\n",
      "Epoch 3433, Loss: 0.6088507324457169, Final Batch Loss: 0.15489166975021362\n",
      "Epoch 3434, Loss: 0.7022827565670013, Final Batch Loss: 0.2861897051334381\n",
      "Epoch 3435, Loss: 0.6583350747823715, Final Batch Loss: 0.24624207615852356\n",
      "Epoch 3436, Loss: 0.5960753411054611, Final Batch Loss: 0.18458719551563263\n",
      "Epoch 3437, Loss: 0.7173105776309967, Final Batch Loss: 0.2763214111328125\n",
      "Epoch 3438, Loss: 0.8056198060512543, Final Batch Loss: 0.4122020900249481\n",
      "Epoch 3439, Loss: 0.5526555478572845, Final Batch Loss: 0.1913856416940689\n",
      "Epoch 3440, Loss: 0.7823319137096405, Final Batch Loss: 0.3281462490558624\n",
      "Epoch 3441, Loss: 0.6796917766332626, Final Batch Loss: 0.1811261624097824\n",
      "Epoch 3442, Loss: 0.621918112039566, Final Batch Loss: 0.16184906661510468\n",
      "Epoch 3443, Loss: 0.8523156195878983, Final Batch Loss: 0.4512738287448883\n",
      "Epoch 3444, Loss: 0.5631465688347816, Final Batch Loss: 0.11721775680780411\n",
      "Epoch 3445, Loss: 0.4806883856654167, Final Batch Loss: 0.06246744841337204\n",
      "Epoch 3446, Loss: 0.5975579023361206, Final Batch Loss: 0.1499132513999939\n",
      "Epoch 3447, Loss: 0.5488498508930206, Final Batch Loss: 0.14718075096607208\n",
      "Epoch 3448, Loss: 0.4668752234429121, Final Batch Loss: 0.020429639145731926\n",
      "Epoch 3449, Loss: 0.7262454479932785, Final Batch Loss: 0.3142518699169159\n",
      "Epoch 3450, Loss: 0.7192864269018173, Final Batch Loss: 0.20060984790325165\n",
      "Epoch 3451, Loss: 0.5645409226417542, Final Batch Loss: 0.085966095328331\n",
      "Epoch 3452, Loss: 0.5610777288675308, Final Batch Loss: 0.1483594924211502\n",
      "Epoch 3453, Loss: 0.5713727325201035, Final Batch Loss: 0.16271278262138367\n",
      "Epoch 3454, Loss: 0.6350205987691879, Final Batch Loss: 0.14875228703022003\n",
      "Epoch 3455, Loss: 0.8442241251468658, Final Batch Loss: 0.4458121359348297\n",
      "Epoch 3456, Loss: 0.5640817061066628, Final Batch Loss: 0.10478828102350235\n",
      "Epoch 3457, Loss: 0.5870279669761658, Final Batch Loss: 0.18254245817661285\n",
      "Epoch 3458, Loss: 0.7652802765369415, Final Batch Loss: 0.3228012025356293\n",
      "Epoch 3459, Loss: 0.7120852172374725, Final Batch Loss: 0.32781529426574707\n",
      "Epoch 3460, Loss: 0.6589917242527008, Final Batch Loss: 0.2695837914943695\n",
      "Epoch 3461, Loss: 0.6533070206642151, Final Batch Loss: 0.1735830157995224\n",
      "Epoch 3462, Loss: 0.6623684167861938, Final Batch Loss: 0.23141640424728394\n",
      "Epoch 3463, Loss: 0.6289573907852173, Final Batch Loss: 0.17714661359786987\n",
      "Epoch 3464, Loss: 0.646606370806694, Final Batch Loss: 0.23232342302799225\n",
      "Epoch 3465, Loss: 0.8109009712934494, Final Batch Loss: 0.42180246114730835\n",
      "Epoch 3466, Loss: 0.7730327546596527, Final Batch Loss: 0.3062856197357178\n",
      "Epoch 3467, Loss: 0.5905979573726654, Final Batch Loss: 0.10928153991699219\n",
      "Epoch 3468, Loss: 0.6309656649827957, Final Batch Loss: 0.21425019204616547\n",
      "Epoch 3469, Loss: 0.5213316380977631, Final Batch Loss: 0.09838089346885681\n",
      "Epoch 3470, Loss: 0.814372181892395, Final Batch Loss: 0.3584916889667511\n",
      "Epoch 3471, Loss: 0.640979453921318, Final Batch Loss: 0.2045804113149643\n",
      "Epoch 3472, Loss: 0.6714046150445938, Final Batch Loss: 0.19671165943145752\n",
      "Epoch 3473, Loss: 0.5968760699033737, Final Batch Loss: 0.1642778068780899\n",
      "Epoch 3474, Loss: 0.8527819514274597, Final Batch Loss: 0.4269508123397827\n",
      "Epoch 3475, Loss: 0.7116961181163788, Final Batch Loss: 0.3081614673137665\n",
      "Epoch 3476, Loss: 0.8151376247406006, Final Batch Loss: 0.3405880630016327\n",
      "Epoch 3477, Loss: 0.8404605388641357, Final Batch Loss: 0.31402167677879333\n",
      "Epoch 3478, Loss: 0.7517138794064522, Final Batch Loss: 0.11169732362031937\n",
      "Epoch 3479, Loss: 0.7748019248247147, Final Batch Loss: 0.19498096406459808\n",
      "Epoch 3480, Loss: 1.0356638431549072, Final Batch Loss: 0.578412652015686\n",
      "Epoch 3481, Loss: 0.7597451955080032, Final Batch Loss: 0.24769112467765808\n",
      "Epoch 3482, Loss: 0.7094863504171371, Final Batch Loss: 0.1929568201303482\n",
      "Epoch 3483, Loss: 0.7875991314649582, Final Batch Loss: 0.31805285811424255\n",
      "Epoch 3484, Loss: 0.7509333193302155, Final Batch Loss: 0.2666776180267334\n",
      "Epoch 3485, Loss: 0.7105820775032043, Final Batch Loss: 0.2918574810028076\n",
      "Epoch 3486, Loss: 0.7141534388065338, Final Batch Loss: 0.2615413963794708\n",
      "Epoch 3487, Loss: 0.846172884106636, Final Batch Loss: 0.3633061647415161\n",
      "Epoch 3488, Loss: 0.708957314491272, Final Batch Loss: 0.20157890021800995\n",
      "Epoch 3489, Loss: 0.6031796932220459, Final Batch Loss: 0.14795219898223877\n",
      "Epoch 3490, Loss: 0.43862635642290115, Final Batch Loss: 0.04405028373003006\n",
      "Epoch 3491, Loss: 0.5713256821036339, Final Batch Loss: 0.07492158561944962\n",
      "Epoch 3492, Loss: 0.5136585384607315, Final Batch Loss: 0.07843628525733948\n",
      "Epoch 3493, Loss: 0.7487506121397018, Final Batch Loss: 0.2908497154712677\n",
      "Epoch 3494, Loss: 0.7361618280410767, Final Batch Loss: 0.31373724341392517\n",
      "Epoch 3495, Loss: 0.5630851835012436, Final Batch Loss: 0.17233078181743622\n",
      "Epoch 3496, Loss: 0.5345721319317818, Final Batch Loss: 0.09177815169095993\n",
      "Epoch 3497, Loss: 0.45953864604234695, Final Batch Loss: 0.0665728971362114\n",
      "Epoch 3498, Loss: 0.6409405618906021, Final Batch Loss: 0.14604362845420837\n",
      "Epoch 3499, Loss: 0.7110480666160583, Final Batch Loss: 0.32352888584136963\n",
      "Epoch 3500, Loss: 0.6268600523471832, Final Batch Loss: 0.1854988932609558\n",
      "Epoch 3501, Loss: 0.7913459539413452, Final Batch Loss: 0.3462606370449066\n",
      "Epoch 3502, Loss: 0.7904307991266251, Final Batch Loss: 0.3130761682987213\n",
      "Epoch 3503, Loss: 0.648180365562439, Final Batch Loss: 0.19760526716709137\n",
      "Epoch 3504, Loss: 0.5693008452653885, Final Batch Loss: 0.15154516696929932\n",
      "Epoch 3505, Loss: 0.5758608430624008, Final Batch Loss: 0.15277810394763947\n",
      "Epoch 3506, Loss: 0.4950058236718178, Final Batch Loss: 0.09504542499780655\n",
      "Epoch 3507, Loss: 0.5783188045024872, Final Batch Loss: 0.1764409840106964\n",
      "Epoch 3508, Loss: 0.7440441846847534, Final Batch Loss: 0.26594534516334534\n",
      "Epoch 3509, Loss: 0.5283047258853912, Final Batch Loss: 0.13032877445220947\n",
      "Epoch 3510, Loss: 0.46557147428393364, Final Batch Loss: 0.050617020577192307\n",
      "Epoch 3511, Loss: 0.8351060599088669, Final Batch Loss: 0.3859921395778656\n",
      "Epoch 3512, Loss: 0.8759039044380188, Final Batch Loss: 0.4118606150150299\n",
      "Epoch 3513, Loss: 0.4671811871230602, Final Batch Loss: 0.03337835893034935\n",
      "Epoch 3514, Loss: 0.6435770392417908, Final Batch Loss: 0.2565816044807434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3515, Loss: 0.6334947347640991, Final Batch Loss: 0.1919092833995819\n",
      "Epoch 3516, Loss: 0.5801272243261337, Final Batch Loss: 0.12687267363071442\n",
      "Epoch 3517, Loss: 0.6584372073411942, Final Batch Loss: 0.20680689811706543\n",
      "Epoch 3518, Loss: 0.56999871134758, Final Batch Loss: 0.1428692638874054\n",
      "Epoch 3519, Loss: 0.5099009945988655, Final Batch Loss: 0.08815563470125198\n",
      "Epoch 3520, Loss: 0.636659637093544, Final Batch Loss: 0.23504477739334106\n",
      "Epoch 3521, Loss: 0.7132102996110916, Final Batch Loss: 0.36421918869018555\n",
      "Epoch 3522, Loss: 0.7498257160186768, Final Batch Loss: 0.31156912446022034\n",
      "Epoch 3523, Loss: 0.6076898574829102, Final Batch Loss: 0.17484575510025024\n",
      "Epoch 3524, Loss: 0.6607834547758102, Final Batch Loss: 0.24310210347175598\n",
      "Epoch 3525, Loss: 0.556758638471365, Final Batch Loss: 0.04246861860156059\n",
      "Epoch 3526, Loss: 0.6847429126501083, Final Batch Loss: 0.28089460730552673\n",
      "Epoch 3527, Loss: 0.42303016409277916, Final Batch Loss: 0.05707274004817009\n",
      "Epoch 3528, Loss: 0.6173613369464874, Final Batch Loss: 0.20339453220367432\n",
      "Epoch 3529, Loss: 0.5317071080207825, Final Batch Loss: 0.14591172337532043\n",
      "Epoch 3530, Loss: 0.7620540261268616, Final Batch Loss: 0.3577716648578644\n",
      "Epoch 3531, Loss: 0.5498787686228752, Final Batch Loss: 0.06977704912424088\n",
      "Epoch 3532, Loss: 0.7158204019069672, Final Batch Loss: 0.25161492824554443\n",
      "Epoch 3533, Loss: 0.6619427800178528, Final Batch Loss: 0.25007200241088867\n",
      "Epoch 3534, Loss: 0.7618320286273956, Final Batch Loss: 0.30284005403518677\n",
      "Epoch 3535, Loss: 0.5187972784042358, Final Batch Loss: 0.1636231541633606\n",
      "Epoch 3536, Loss: 0.7577457427978516, Final Batch Loss: 0.2913334369659424\n",
      "Epoch 3537, Loss: 0.8281540423631668, Final Batch Loss: 0.411289244890213\n",
      "Epoch 3538, Loss: 0.7137954384088516, Final Batch Loss: 0.32125210762023926\n",
      "Epoch 3539, Loss: 1.0775315761566162, Final Batch Loss: 0.657739520072937\n",
      "Epoch 3540, Loss: 0.5078864172101021, Final Batch Loss: 0.0632082149386406\n",
      "Epoch 3541, Loss: 0.7433538436889648, Final Batch Loss: 0.2527024447917938\n",
      "Epoch 3542, Loss: 0.9078105837106705, Final Batch Loss: 0.39995312690734863\n",
      "Epoch 3543, Loss: 0.5885789543390274, Final Batch Loss: 0.16523084044456482\n",
      "Epoch 3544, Loss: 0.6036830544471741, Final Batch Loss: 0.09209397435188293\n",
      "Epoch 3545, Loss: 0.7521828263998032, Final Batch Loss: 0.29490214586257935\n",
      "Epoch 3546, Loss: 0.6048109754920006, Final Batch Loss: 0.09762642532587051\n",
      "Epoch 3547, Loss: 0.6274866908788681, Final Batch Loss: 0.17695197463035583\n",
      "Epoch 3548, Loss: 0.7032391428947449, Final Batch Loss: 0.28889694809913635\n",
      "Epoch 3549, Loss: 0.6544228047132492, Final Batch Loss: 0.21289105713367462\n",
      "Epoch 3550, Loss: 0.7189819365739822, Final Batch Loss: 0.24951325356960297\n",
      "Epoch 3551, Loss: 0.7171852141618729, Final Batch Loss: 0.2822840213775635\n",
      "Epoch 3552, Loss: 0.5482316613197327, Final Batch Loss: 0.12630601227283478\n",
      "Epoch 3553, Loss: 0.7357268184423447, Final Batch Loss: 0.27950170636177063\n",
      "Epoch 3554, Loss: 0.6654791235923767, Final Batch Loss: 0.23876315355300903\n",
      "Epoch 3555, Loss: 0.5232193320989609, Final Batch Loss: 0.0939541757106781\n",
      "Epoch 3556, Loss: 0.5790897980332375, Final Batch Loss: 0.08461696654558182\n",
      "Epoch 3557, Loss: 0.5847167819738388, Final Batch Loss: 0.1285882294178009\n",
      "Epoch 3558, Loss: 0.5645853132009506, Final Batch Loss: 0.16923338174819946\n",
      "Epoch 3559, Loss: 0.6110157519578934, Final Batch Loss: 0.1603723168373108\n",
      "Epoch 3560, Loss: 0.5992158204317093, Final Batch Loss: 0.1745528280735016\n",
      "Epoch 3561, Loss: 0.5852368175983429, Final Batch Loss: 0.13068147003650665\n",
      "Epoch 3562, Loss: 0.6412098556756973, Final Batch Loss: 0.18905319273471832\n",
      "Epoch 3563, Loss: 0.871117576956749, Final Batch Loss: 0.45284998416900635\n",
      "Epoch 3564, Loss: 0.8086124956607819, Final Batch Loss: 0.3964378535747528\n",
      "Epoch 3565, Loss: 0.5337982922792435, Final Batch Loss: 0.12701919674873352\n",
      "Epoch 3566, Loss: 0.8741817325353622, Final Batch Loss: 0.5128640532493591\n",
      "Epoch 3567, Loss: 0.6518639028072357, Final Batch Loss: 0.2124522179365158\n",
      "Epoch 3568, Loss: 0.6080090552568436, Final Batch Loss: 0.20763660967350006\n",
      "Epoch 3569, Loss: 0.6708682328462601, Final Batch Loss: 0.21237611770629883\n",
      "Epoch 3570, Loss: 0.5468053817749023, Final Batch Loss: 0.10497745871543884\n",
      "Epoch 3571, Loss: 0.6267841905355453, Final Batch Loss: 0.10048329830169678\n",
      "Epoch 3572, Loss: 0.5827649235725403, Final Batch Loss: 0.14338308572769165\n",
      "Epoch 3573, Loss: 0.588557243347168, Final Batch Loss: 0.19229581952095032\n",
      "Epoch 3574, Loss: 0.9333336800336838, Final Batch Loss: 0.4393613338470459\n",
      "Epoch 3575, Loss: 0.648258700966835, Final Batch Loss: 0.22106993198394775\n",
      "Epoch 3576, Loss: 0.735163077712059, Final Batch Loss: 0.27011018991470337\n",
      "Epoch 3577, Loss: 0.487554207444191, Final Batch Loss: 0.04319162666797638\n",
      "Epoch 3578, Loss: 0.5048673748970032, Final Batch Loss: 0.08215638995170593\n",
      "Epoch 3579, Loss: 0.717927873134613, Final Batch Loss: 0.2827511429786682\n",
      "Epoch 3580, Loss: 0.7187541127204895, Final Batch Loss: 0.3465433716773987\n",
      "Epoch 3581, Loss: 0.6233598291873932, Final Batch Loss: 0.204891175031662\n",
      "Epoch 3582, Loss: 0.5814217925071716, Final Batch Loss: 0.07672557234764099\n",
      "Epoch 3583, Loss: 0.8404565751552582, Final Batch Loss: 0.3836866021156311\n",
      "Epoch 3584, Loss: 0.6006712168455124, Final Batch Loss: 0.13331685960292816\n",
      "Epoch 3585, Loss: 0.6786017268896103, Final Batch Loss: 0.2456156611442566\n",
      "Epoch 3586, Loss: 0.6717973351478577, Final Batch Loss: 0.15827487409114838\n",
      "Epoch 3587, Loss: 0.625436544418335, Final Batch Loss: 0.1999775916337967\n",
      "Epoch 3588, Loss: 0.6118657141923904, Final Batch Loss: 0.21248109638690948\n",
      "Epoch 3589, Loss: 0.8011345714330673, Final Batch Loss: 0.3953092098236084\n",
      "Epoch 3590, Loss: 0.8389019668102264, Final Batch Loss: 0.4297778904438019\n",
      "Epoch 3591, Loss: 0.4858725816011429, Final Batch Loss: 0.028551504015922546\n",
      "Epoch 3592, Loss: 0.7064109295606613, Final Batch Loss: 0.2693367600440979\n",
      "Epoch 3593, Loss: 0.5269803777337074, Final Batch Loss: 0.1159675344824791\n",
      "Epoch 3594, Loss: 0.5823362395167351, Final Batch Loss: 0.12102808803319931\n",
      "Epoch 3595, Loss: 0.5788233876228333, Final Batch Loss: 0.0977594256401062\n",
      "Epoch 3596, Loss: 0.625097244977951, Final Batch Loss: 0.25083106756210327\n",
      "Epoch 3597, Loss: 0.5862051546573639, Final Batch Loss: 0.13393500447273254\n",
      "Epoch 3598, Loss: 0.5324375480413437, Final Batch Loss: 0.1759614497423172\n",
      "Epoch 3599, Loss: 0.48785754293203354, Final Batch Loss: 0.09140900522470474\n",
      "Epoch 3600, Loss: 0.5857740193605423, Final Batch Loss: 0.19015808403491974\n",
      "Epoch 3601, Loss: 0.7828576564788818, Final Batch Loss: 0.3914783298969269\n",
      "Epoch 3602, Loss: 0.5126443766057491, Final Batch Loss: 0.05474689230322838\n",
      "Epoch 3603, Loss: 0.605137050151825, Final Batch Loss: 0.10614730417728424\n",
      "Epoch 3604, Loss: 0.6728977113962173, Final Batch Loss: 0.26248040795326233\n",
      "Epoch 3605, Loss: 0.6275447607040405, Final Batch Loss: 0.18383824825286865\n",
      "Epoch 3606, Loss: 0.7250175178050995, Final Batch Loss: 0.33898088335990906\n",
      "Epoch 3607, Loss: 0.8225081861019135, Final Batch Loss: 0.3005852699279785\n",
      "Epoch 3608, Loss: 0.4883429780602455, Final Batch Loss: 0.059657685458660126\n",
      "Epoch 3609, Loss: 0.759758785367012, Final Batch Loss: 0.27761393785476685\n",
      "Epoch 3610, Loss: 0.5537474751472473, Final Batch Loss: 0.1478511244058609\n",
      "Epoch 3611, Loss: 0.5549695938825607, Final Batch Loss: 0.1384793221950531\n",
      "Epoch 3612, Loss: 0.9029954820871353, Final Batch Loss: 0.39368224143981934\n",
      "Epoch 3613, Loss: 0.5347026586532593, Final Batch Loss: 0.16789095103740692\n",
      "Epoch 3614, Loss: 0.6174569725990295, Final Batch Loss: 0.1802167445421219\n",
      "Epoch 3615, Loss: 0.4703187867999077, Final Batch Loss: 0.09252028912305832\n",
      "Epoch 3616, Loss: 0.6993880867958069, Final Batch Loss: 0.2614190876483917\n",
      "Epoch 3617, Loss: 0.49100442603230476, Final Batch Loss: 0.059110093861818314\n",
      "Epoch 3618, Loss: 0.6878875344991684, Final Batch Loss: 0.23095901310443878\n",
      "Epoch 3619, Loss: 0.5799116417765617, Final Batch Loss: 0.09529159218072891\n",
      "Epoch 3620, Loss: 0.6934186816215515, Final Batch Loss: 0.23607510328292847\n",
      "Epoch 3621, Loss: 0.563850536942482, Final Batch Loss: 0.16034843027591705\n",
      "Epoch 3622, Loss: 0.49354345351457596, Final Batch Loss: 0.11309000104665756\n",
      "Epoch 3623, Loss: 0.5229050517082214, Final Batch Loss: 0.15751445293426514\n",
      "Epoch 3624, Loss: 0.5183798670768738, Final Batch Loss: 0.08989204466342926\n",
      "Epoch 3625, Loss: 0.6667439192533493, Final Batch Loss: 0.13746806979179382\n",
      "Epoch 3626, Loss: 0.6398865580558777, Final Batch Loss: 0.2586214542388916\n",
      "Epoch 3627, Loss: 0.5512115657329559, Final Batch Loss: 0.1814027726650238\n",
      "Epoch 3628, Loss: 0.5464030504226685, Final Batch Loss: 0.10424035787582397\n",
      "Epoch 3629, Loss: 0.5148172453045845, Final Batch Loss: 0.08311396092176437\n",
      "Epoch 3630, Loss: 0.5506586879491806, Final Batch Loss: 0.09655022621154785\n",
      "Epoch 3631, Loss: 0.47752565890550613, Final Batch Loss: 0.0935351774096489\n",
      "Epoch 3632, Loss: 0.7657699733972549, Final Batch Loss: 0.40771642327308655\n",
      "Epoch 3633, Loss: 0.46619047224521637, Final Batch Loss: 0.07547593116760254\n",
      "Epoch 3634, Loss: 0.6446061134338379, Final Batch Loss: 0.2109500914812088\n",
      "Epoch 3635, Loss: 0.6906299144029617, Final Batch Loss: 0.31279048323631287\n",
      "Epoch 3636, Loss: 0.8166615068912506, Final Batch Loss: 0.37040722370147705\n",
      "Epoch 3637, Loss: 0.7488760948181152, Final Batch Loss: 0.36645087599754333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3638, Loss: 0.6323171705007553, Final Batch Loss: 0.2363683581352234\n",
      "Epoch 3639, Loss: 0.4947893023490906, Final Batch Loss: 0.06583411991596222\n",
      "Epoch 3640, Loss: 0.5455367118120193, Final Batch Loss: 0.14285410940647125\n",
      "Epoch 3641, Loss: 0.7591498643159866, Final Batch Loss: 0.33996716141700745\n",
      "Epoch 3642, Loss: 0.6024216860532761, Final Batch Loss: 0.18641912937164307\n",
      "Epoch 3643, Loss: 0.8239225596189499, Final Batch Loss: 0.37876057624816895\n",
      "Epoch 3644, Loss: 0.5082992389798164, Final Batch Loss: 0.12222295254468918\n",
      "Epoch 3645, Loss: 0.8237922936677933, Final Batch Loss: 0.38328877091407776\n",
      "Epoch 3646, Loss: 0.7177295982837677, Final Batch Loss: 0.27759966254234314\n",
      "Epoch 3647, Loss: 0.5404985323548317, Final Batch Loss: 0.11438383907079697\n",
      "Epoch 3648, Loss: 0.7393160164356232, Final Batch Loss: 0.25843939185142517\n",
      "Epoch 3649, Loss: 0.7247281819581985, Final Batch Loss: 0.29294854402542114\n",
      "Epoch 3650, Loss: 0.9814387857913971, Final Batch Loss: 0.5842571258544922\n",
      "Epoch 3651, Loss: 0.514650272205472, Final Batch Loss: 0.025301845744252205\n",
      "Epoch 3652, Loss: 0.5139035955071449, Final Batch Loss: 0.10082683712244034\n",
      "Epoch 3653, Loss: 0.6280799359083176, Final Batch Loss: 0.16208292543888092\n",
      "Epoch 3654, Loss: 0.7238851189613342, Final Batch Loss: 0.24715562164783478\n",
      "Epoch 3655, Loss: 0.6672036498785019, Final Batch Loss: 0.2273334264755249\n",
      "Epoch 3656, Loss: 0.6686013638973236, Final Batch Loss: 0.23751798272132874\n",
      "Epoch 3657, Loss: 0.7135484367609024, Final Batch Loss: 0.28404101729393005\n",
      "Epoch 3658, Loss: 0.4248746056109667, Final Batch Loss: 0.03097735531628132\n",
      "Epoch 3659, Loss: 0.6087866425514221, Final Batch Loss: 0.2126714289188385\n",
      "Epoch 3660, Loss: 0.6088118106126785, Final Batch Loss: 0.1557874232530594\n",
      "Epoch 3661, Loss: 0.529053807258606, Final Batch Loss: 0.04947471618652344\n",
      "Epoch 3662, Loss: 0.6629630625247955, Final Batch Loss: 0.23494012653827667\n",
      "Epoch 3663, Loss: 0.6670209467411041, Final Batch Loss: 0.2581406831741333\n",
      "Epoch 3664, Loss: 0.5118008702993393, Final Batch Loss: 0.049083441495895386\n",
      "Epoch 3665, Loss: 0.4873175770044327, Final Batch Loss: 0.1020166277885437\n",
      "Epoch 3666, Loss: 0.5811038464307785, Final Batch Loss: 0.13637906312942505\n",
      "Epoch 3667, Loss: 0.5316103920340538, Final Batch Loss: 0.118866465985775\n",
      "Epoch 3668, Loss: 0.8278158456087112, Final Batch Loss: 0.4310680329799652\n",
      "Epoch 3669, Loss: 0.5392313078045845, Final Batch Loss: 0.07011296600103378\n",
      "Epoch 3670, Loss: 0.5576313585042953, Final Batch Loss: 0.20128285884857178\n",
      "Epoch 3671, Loss: 0.5474390387535095, Final Batch Loss: 0.1322699934244156\n",
      "Epoch 3672, Loss: 0.734093964099884, Final Batch Loss: 0.3300894796848297\n",
      "Epoch 3673, Loss: 0.619870737195015, Final Batch Loss: 0.15318679809570312\n",
      "Epoch 3674, Loss: 0.4479653351008892, Final Batch Loss: 0.025362011045217514\n",
      "Epoch 3675, Loss: 0.5757668763399124, Final Batch Loss: 0.12163399159908295\n",
      "Epoch 3676, Loss: 0.5341878533363342, Final Batch Loss: 0.16733984649181366\n",
      "Epoch 3677, Loss: 0.5297124087810516, Final Batch Loss: 0.11601719260215759\n",
      "Epoch 3678, Loss: 0.766870990395546, Final Batch Loss: 0.28592902421951294\n",
      "Epoch 3679, Loss: 0.6196040958166122, Final Batch Loss: 0.16449129581451416\n",
      "Epoch 3680, Loss: 0.615744099020958, Final Batch Loss: 0.16614238917827606\n",
      "Epoch 3681, Loss: 0.6543644815683365, Final Batch Loss: 0.16598902642726898\n",
      "Epoch 3682, Loss: 0.5687109529972076, Final Batch Loss: 0.14459288120269775\n",
      "Epoch 3683, Loss: 0.6180227845907211, Final Batch Loss: 0.23092834651470184\n",
      "Epoch 3684, Loss: 0.5551846921443939, Final Batch Loss: 0.17471802234649658\n",
      "Epoch 3685, Loss: 0.8836040496826172, Final Batch Loss: 0.49494126439094543\n",
      "Epoch 3686, Loss: 0.7888697981834412, Final Batch Loss: 0.3077268898487091\n",
      "Epoch 3687, Loss: 0.898938462138176, Final Batch Loss: 0.40158507227897644\n",
      "Epoch 3688, Loss: 0.6392980217933655, Final Batch Loss: 0.15005409717559814\n",
      "Epoch 3689, Loss: 0.8414862006902695, Final Batch Loss: 0.4257172644138336\n",
      "Epoch 3690, Loss: 0.5411602780222893, Final Batch Loss: 0.0552692785859108\n",
      "Epoch 3691, Loss: 0.79166179895401, Final Batch Loss: 0.3760296702384949\n",
      "Epoch 3692, Loss: 0.7815573513507843, Final Batch Loss: 0.37307778000831604\n",
      "Epoch 3693, Loss: 0.4533817358314991, Final Batch Loss: 0.031856972724199295\n",
      "Epoch 3694, Loss: 0.5998339056968689, Final Batch Loss: 0.1887459009885788\n",
      "Epoch 3695, Loss: 0.5779396146535873, Final Batch Loss: 0.1276509314775467\n",
      "Epoch 3696, Loss: 0.7191953361034393, Final Batch Loss: 0.29707425832748413\n",
      "Epoch 3697, Loss: 0.7023592889308929, Final Batch Loss: 0.32622694969177246\n",
      "Epoch 3698, Loss: 0.4987572655081749, Final Batch Loss: 0.08733875304460526\n",
      "Epoch 3699, Loss: 0.48584672436118126, Final Batch Loss: 0.06161392852663994\n",
      "Epoch 3700, Loss: 0.5216691195964813, Final Batch Loss: 0.08129960298538208\n",
      "Epoch 3701, Loss: 0.8196701109409332, Final Batch Loss: 0.34350186586380005\n",
      "Epoch 3702, Loss: 0.516757071018219, Final Batch Loss: 0.08816765248775482\n",
      "Epoch 3703, Loss: 0.5946637392044067, Final Batch Loss: 0.15568961203098297\n",
      "Epoch 3704, Loss: 0.6439128220081329, Final Batch Loss: 0.19876542687416077\n",
      "Epoch 3705, Loss: 0.46242906898260117, Final Batch Loss: 0.04162374883890152\n",
      "Epoch 3706, Loss: 0.7590519487857819, Final Batch Loss: 0.30158671736717224\n",
      "Epoch 3707, Loss: 0.7998606562614441, Final Batch Loss: 0.3270885646343231\n",
      "Epoch 3708, Loss: 0.7341955006122589, Final Batch Loss: 0.23330989480018616\n",
      "Epoch 3709, Loss: 0.5292867422103882, Final Batch Loss: 0.12393325567245483\n",
      "Epoch 3710, Loss: 0.6296022236347198, Final Batch Loss: 0.23177309334278107\n",
      "Epoch 3711, Loss: 0.4837107304483652, Final Batch Loss: 0.029415080323815346\n",
      "Epoch 3712, Loss: 0.5374239608645439, Final Batch Loss: 0.10255273431539536\n",
      "Epoch 3713, Loss: 0.5769705325365067, Final Batch Loss: 0.202152818441391\n",
      "Epoch 3714, Loss: 0.8021063804626465, Final Batch Loss: 0.3657987713813782\n",
      "Epoch 3715, Loss: 0.4607741944491863, Final Batch Loss: 0.05952944979071617\n",
      "Epoch 3716, Loss: 0.700453445315361, Final Batch Loss: 0.3287699818611145\n",
      "Epoch 3717, Loss: 0.5450096130371094, Final Batch Loss: 0.0941915363073349\n",
      "Epoch 3718, Loss: 0.4889886826276779, Final Batch Loss: 0.07278819382190704\n",
      "Epoch 3719, Loss: 0.7967270612716675, Final Batch Loss: 0.4000660181045532\n",
      "Epoch 3720, Loss: 0.5584525465965271, Final Batch Loss: 0.09642083942890167\n",
      "Epoch 3721, Loss: 0.603951632976532, Final Batch Loss: 0.19697457551956177\n",
      "Epoch 3722, Loss: 0.9523684233427048, Final Batch Loss: 0.5484457015991211\n",
      "Epoch 3723, Loss: 1.179139867424965, Final Batch Loss: 0.7497150897979736\n",
      "Epoch 3724, Loss: 0.7052062898874283, Final Batch Loss: 0.28381773829460144\n",
      "Epoch 3725, Loss: 0.6553767621517181, Final Batch Loss: 0.20331066846847534\n",
      "Epoch 3726, Loss: 0.8904557526111603, Final Batch Loss: 0.392453670501709\n",
      "Epoch 3727, Loss: 0.7136411368846893, Final Batch Loss: 0.2148207128047943\n",
      "Epoch 3728, Loss: 0.8801717758178711, Final Batch Loss: 0.4197830855846405\n",
      "Epoch 3729, Loss: 0.6836458146572113, Final Batch Loss: 0.22665292024612427\n",
      "Epoch 3730, Loss: 0.7414907068014145, Final Batch Loss: 0.32572320103645325\n",
      "Epoch 3731, Loss: 0.5452951937913895, Final Batch Loss: 0.10759089887142181\n",
      "Epoch 3732, Loss: 0.4920180216431618, Final Batch Loss: 0.035809360444545746\n",
      "Epoch 3733, Loss: 0.5780539736151695, Final Batch Loss: 0.10128403455018997\n",
      "Epoch 3734, Loss: 0.5724087804555893, Final Batch Loss: 0.18896852433681488\n",
      "Epoch 3735, Loss: 0.6171296685934067, Final Batch Loss: 0.2037559300661087\n",
      "Epoch 3736, Loss: 0.714842677116394, Final Batch Loss: 0.2723681628704071\n",
      "Epoch 3737, Loss: 0.4874614477157593, Final Batch Loss: 0.09269873797893524\n",
      "Epoch 3738, Loss: 0.6207833290100098, Final Batch Loss: 0.23254820704460144\n",
      "Epoch 3739, Loss: 0.5997725129127502, Final Batch Loss: 0.21179401874542236\n",
      "Epoch 3740, Loss: 0.5077289044857025, Final Batch Loss: 0.12898054718971252\n",
      "Epoch 3741, Loss: 0.5521388649940491, Final Batch Loss: 0.19384314119815826\n",
      "Epoch 3742, Loss: 0.5717952102422714, Final Batch Loss: 0.1835487335920334\n",
      "Epoch 3743, Loss: 0.6553348749876022, Final Batch Loss: 0.21826104819774628\n",
      "Epoch 3744, Loss: 0.6286130398511887, Final Batch Loss: 0.2218119204044342\n",
      "Epoch 3745, Loss: 0.4906543716788292, Final Batch Loss: 0.07286388427019119\n",
      "Epoch 3746, Loss: 0.4552991911768913, Final Batch Loss: 0.0532374307513237\n",
      "Epoch 3747, Loss: 0.6039318591356277, Final Batch Loss: 0.19927017390727997\n",
      "Epoch 3748, Loss: 0.5411390364170074, Final Batch Loss: 0.14565961062908173\n",
      "Epoch 3749, Loss: 0.48797598481178284, Final Batch Loss: 0.09692013263702393\n",
      "Epoch 3750, Loss: 0.6974057257175446, Final Batch Loss: 0.2933661639690399\n",
      "Epoch 3751, Loss: 0.46601181104779243, Final Batch Loss: 0.053276438266038895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3752, Loss: 0.6923379451036453, Final Batch Loss: 0.20058666169643402\n",
      "Epoch 3753, Loss: 0.7249422967433929, Final Batch Loss: 0.2598116397857666\n",
      "Epoch 3754, Loss: 0.625459760427475, Final Batch Loss: 0.23094713687896729\n",
      "Epoch 3755, Loss: 0.6886528134346008, Final Batch Loss: 0.2102406769990921\n",
      "Epoch 3756, Loss: 0.4399659959599376, Final Batch Loss: 0.00939312856644392\n",
      "Epoch 3757, Loss: 0.600243091583252, Final Batch Loss: 0.196633443236351\n",
      "Epoch 3758, Loss: 0.6513105183839798, Final Batch Loss: 0.27189159393310547\n",
      "Epoch 3759, Loss: 0.8290119171142578, Final Batch Loss: 0.37486472725868225\n",
      "Epoch 3760, Loss: 0.7363111972808838, Final Batch Loss: 0.2987806797027588\n",
      "Epoch 3761, Loss: 0.7186335176229477, Final Batch Loss: 0.32809343934059143\n",
      "Epoch 3762, Loss: 0.7044022232294083, Final Batch Loss: 0.3076882064342499\n",
      "Epoch 3763, Loss: 0.7282009273767471, Final Batch Loss: 0.3156180679798126\n",
      "Epoch 3764, Loss: 0.6181520074605942, Final Batch Loss: 0.2637532949447632\n",
      "Epoch 3765, Loss: 0.6712273806333542, Final Batch Loss: 0.24797974526882172\n",
      "Epoch 3766, Loss: 0.46393535286188126, Final Batch Loss: 0.10658212751150131\n",
      "Epoch 3767, Loss: 0.6001801490783691, Final Batch Loss: 0.20837944746017456\n",
      "Epoch 3768, Loss: 0.5354574173688889, Final Batch Loss: 0.1338823288679123\n",
      "Epoch 3769, Loss: 0.7105670422315598, Final Batch Loss: 0.2900789678096771\n",
      "Epoch 3770, Loss: 0.5762792527675629, Final Batch Loss: 0.19403158128261566\n",
      "Epoch 3771, Loss: 0.6558243334293365, Final Batch Loss: 0.20677915215492249\n",
      "Epoch 3772, Loss: 0.5893941074609756, Final Batch Loss: 0.17445673048496246\n",
      "Epoch 3773, Loss: 0.5858373492956161, Final Batch Loss: 0.16069543361663818\n",
      "Epoch 3774, Loss: 0.5029489621520042, Final Batch Loss: 0.07332298904657364\n",
      "Epoch 3775, Loss: 0.4754122868180275, Final Batch Loss: 0.0738736167550087\n",
      "Epoch 3776, Loss: 0.5648547857999802, Final Batch Loss: 0.21767692267894745\n",
      "Epoch 3777, Loss: 0.5206473097205162, Final Batch Loss: 0.12478815764188766\n",
      "Epoch 3778, Loss: 0.5003118738532066, Final Batch Loss: 0.11576051265001297\n",
      "Epoch 3779, Loss: 0.5071452260017395, Final Batch Loss: 0.09697863459587097\n",
      "Epoch 3780, Loss: 0.637335941195488, Final Batch Loss: 0.16910743713378906\n",
      "Epoch 3781, Loss: 0.6036729365587234, Final Batch Loss: 0.17806865274906158\n",
      "Epoch 3782, Loss: 0.45608099550008774, Final Batch Loss: 0.044842325150966644\n",
      "Epoch 3783, Loss: 0.665037676692009, Final Batch Loss: 0.34548795223236084\n",
      "Epoch 3784, Loss: 0.5833220034837723, Final Batch Loss: 0.18015146255493164\n",
      "Epoch 3785, Loss: 0.7377935945987701, Final Batch Loss: 0.3044825494289398\n",
      "Epoch 3786, Loss: 0.7050463110208511, Final Batch Loss: 0.27222609519958496\n",
      "Epoch 3787, Loss: 0.5874991565942764, Final Batch Loss: 0.2480638474225998\n",
      "Epoch 3788, Loss: 0.511276125907898, Final Batch Loss: 0.10612934827804565\n",
      "Epoch 3789, Loss: 0.5484708547592163, Final Batch Loss: 0.16532212495803833\n",
      "Epoch 3790, Loss: 0.4905070811510086, Final Batch Loss: 0.14029377698898315\n",
      "Epoch 3791, Loss: 0.8061813116073608, Final Batch Loss: 0.43303343653678894\n",
      "Epoch 3792, Loss: 0.5754125565290451, Final Batch Loss: 0.14809763431549072\n",
      "Epoch 3793, Loss: 0.5419052988290787, Final Batch Loss: 0.19668498635292053\n",
      "Epoch 3794, Loss: 0.49604498594999313, Final Batch Loss: 0.1135469600558281\n",
      "Epoch 3795, Loss: 0.5163768380880356, Final Batch Loss: 0.08748497068881989\n",
      "Epoch 3796, Loss: 0.6774070411920547, Final Batch Loss: 0.3314882516860962\n",
      "Epoch 3797, Loss: 0.3478733319789171, Final Batch Loss: 0.012651296332478523\n",
      "Epoch 3798, Loss: 0.7201722711324692, Final Batch Loss: 0.2751583158969879\n",
      "Epoch 3799, Loss: 0.6148616820573807, Final Batch Loss: 0.1649877429008484\n",
      "Epoch 3800, Loss: 0.6419079005718231, Final Batch Loss: 0.12768226861953735\n",
      "Epoch 3801, Loss: 0.6055260002613068, Final Batch Loss: 0.24560976028442383\n",
      "Epoch 3802, Loss: 0.571023166179657, Final Batch Loss: 0.13441847264766693\n",
      "Epoch 3803, Loss: 0.6703303903341293, Final Batch Loss: 0.24205483496189117\n",
      "Epoch 3804, Loss: 0.5533079877495766, Final Batch Loss: 0.12040571123361588\n",
      "Epoch 3805, Loss: 0.6451497077941895, Final Batch Loss: 0.21270671486854553\n",
      "Epoch 3806, Loss: 0.5581342130899429, Final Batch Loss: 0.08798892796039581\n",
      "Epoch 3807, Loss: 0.40023402497172356, Final Batch Loss: 0.038967784494161606\n",
      "Epoch 3808, Loss: 0.5519285947084427, Final Batch Loss: 0.13992047309875488\n",
      "Epoch 3809, Loss: 0.7004552781581879, Final Batch Loss: 0.33451342582702637\n",
      "Epoch 3810, Loss: 0.5971433371305466, Final Batch Loss: 0.2127680778503418\n",
      "Epoch 3811, Loss: 0.5624938309192657, Final Batch Loss: 0.15430033206939697\n",
      "Epoch 3812, Loss: 0.7707162797451019, Final Batch Loss: 0.34809044003486633\n",
      "Epoch 3813, Loss: 0.5674926191568375, Final Batch Loss: 0.1396888643503189\n",
      "Epoch 3814, Loss: 0.5236930549144745, Final Batch Loss: 0.13372011482715607\n",
      "Epoch 3815, Loss: 0.5029483139514923, Final Batch Loss: 0.09457902610301971\n",
      "Epoch 3816, Loss: 0.6366777867078781, Final Batch Loss: 0.226274311542511\n",
      "Epoch 3817, Loss: 0.4251718819141388, Final Batch Loss: 0.05520431697368622\n",
      "Epoch 3818, Loss: 0.7089845836162567, Final Batch Loss: 0.2802703380584717\n",
      "Epoch 3819, Loss: 0.6032715439796448, Final Batch Loss: 0.15961456298828125\n",
      "Epoch 3820, Loss: 0.5866550654172897, Final Batch Loss: 0.18210169672966003\n",
      "Epoch 3821, Loss: 0.5105353742837906, Final Batch Loss: 0.1737629473209381\n",
      "Epoch 3822, Loss: 0.6115311533212662, Final Batch Loss: 0.163667693734169\n",
      "Epoch 3823, Loss: 0.5334020406007767, Final Batch Loss: 0.07823067903518677\n",
      "Epoch 3824, Loss: 0.48544882237911224, Final Batch Loss: 0.05178748071193695\n",
      "Epoch 3825, Loss: 0.5994377136230469, Final Batch Loss: 0.24050140380859375\n",
      "Epoch 3826, Loss: 0.5306305438280106, Final Batch Loss: 0.2036242038011551\n",
      "Epoch 3827, Loss: 0.6230461150407791, Final Batch Loss: 0.2075873166322708\n",
      "Epoch 3828, Loss: 0.9527560174465179, Final Batch Loss: 0.5775644183158875\n",
      "Epoch 3829, Loss: 0.9450346231460571, Final Batch Loss: 0.45468586683273315\n",
      "Epoch 3830, Loss: 0.9439797848463058, Final Batch Loss: 0.5535754561424255\n",
      "Epoch 3831, Loss: 0.6135625392198563, Final Batch Loss: 0.13590975105762482\n",
      "Epoch 3832, Loss: 0.5899042934179306, Final Batch Loss: 0.19974304735660553\n",
      "Epoch 3833, Loss: 0.4869607090950012, Final Batch Loss: 0.06538426876068115\n",
      "Epoch 3834, Loss: 0.4980598781257868, Final Batch Loss: 0.023156071081757545\n",
      "Epoch 3835, Loss: 0.5944183468818665, Final Batch Loss: 0.20349201560020447\n",
      "Epoch 3836, Loss: 0.7951117306947708, Final Batch Loss: 0.36884045600891113\n",
      "Epoch 3837, Loss: 0.513865053653717, Final Batch Loss: 0.11168883740901947\n",
      "Epoch 3838, Loss: 0.6623275130987167, Final Batch Loss: 0.1627524346113205\n",
      "Epoch 3839, Loss: 0.8010716587305069, Final Batch Loss: 0.3400852084159851\n",
      "Epoch 3840, Loss: 0.7339122593402863, Final Batch Loss: 0.3519010543823242\n",
      "Epoch 3841, Loss: 0.39870886504650116, Final Batch Loss: 0.020938381552696228\n",
      "Epoch 3842, Loss: 0.6846534460783005, Final Batch Loss: 0.35345086455345154\n",
      "Epoch 3843, Loss: 0.6375322639942169, Final Batch Loss: 0.21998243033885956\n",
      "Epoch 3844, Loss: 0.6729459166526794, Final Batch Loss: 0.24027091264724731\n",
      "Epoch 3845, Loss: 0.8601421862840652, Final Batch Loss: 0.37903961539268494\n",
      "Epoch 3846, Loss: 0.4876113384962082, Final Batch Loss: 0.1270182579755783\n",
      "Epoch 3847, Loss: 0.7305616289377213, Final Batch Loss: 0.2806589603424072\n",
      "Epoch 3848, Loss: 0.586728423833847, Final Batch Loss: 0.2551419734954834\n",
      "Epoch 3849, Loss: 0.7392774224281311, Final Batch Loss: 0.3237774968147278\n",
      "Epoch 3850, Loss: 0.5800484716892242, Final Batch Loss: 0.16008199751377106\n",
      "Epoch 3851, Loss: 0.5937123149633408, Final Batch Loss: 0.18747077882289886\n",
      "Epoch 3852, Loss: 0.5583152398467064, Final Batch Loss: 0.09514925628900528\n",
      "Epoch 3853, Loss: 0.8806956708431244, Final Batch Loss: 0.44763779640197754\n",
      "Epoch 3854, Loss: 0.6776018440723419, Final Batch Loss: 0.32706737518310547\n",
      "Epoch 3855, Loss: 0.44938820600509644, Final Batch Loss: 0.08975142240524292\n",
      "Epoch 3856, Loss: 0.5466578304767609, Final Batch Loss: 0.1249052882194519\n",
      "Epoch 3857, Loss: 0.535079799592495, Final Batch Loss: 0.12353990226984024\n",
      "Epoch 3858, Loss: 0.5301656052470207, Final Batch Loss: 0.08352003246545792\n",
      "Epoch 3859, Loss: 0.4420465864241123, Final Batch Loss: 0.03629517927765846\n",
      "Epoch 3860, Loss: 0.5063145011663437, Final Batch Loss: 0.09239095449447632\n",
      "Epoch 3861, Loss: 0.5540594756603241, Final Batch Loss: 0.14103089272975922\n",
      "Epoch 3862, Loss: 0.5283984988927841, Final Batch Loss: 0.09687525033950806\n",
      "Epoch 3863, Loss: 0.5097166374325752, Final Batch Loss: 0.10763361304998398\n",
      "Epoch 3864, Loss: 0.71596759557724, Final Batch Loss: 0.3530057966709137\n",
      "Epoch 3865, Loss: 0.6683493107557297, Final Batch Loss: 0.27252745628356934\n",
      "Epoch 3866, Loss: 0.7239100635051727, Final Batch Loss: 0.21649444103240967\n",
      "Epoch 3867, Loss: 0.7187222838401794, Final Batch Loss: 0.30981162190437317\n",
      "Epoch 3868, Loss: 0.5832290798425674, Final Batch Loss: 0.20532183349132538\n",
      "Epoch 3869, Loss: 0.5223382413387299, Final Batch Loss: 0.07802382111549377\n",
      "Epoch 3870, Loss: 0.6275506317615509, Final Batch Loss: 0.28787556290626526\n",
      "Epoch 3871, Loss: 0.48628006875514984, Final Batch Loss: 0.1297389566898346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3872, Loss: 0.6599958539009094, Final Batch Loss: 0.2797502279281616\n",
      "Epoch 3873, Loss: 0.648091584444046, Final Batch Loss: 0.23762306571006775\n",
      "Epoch 3874, Loss: 0.49113326519727707, Final Batch Loss: 0.12227324396371841\n",
      "Epoch 3875, Loss: 0.4795845001935959, Final Batch Loss: 0.08014334738254547\n",
      "Epoch 3876, Loss: 0.5382847040891647, Final Batch Loss: 0.16012167930603027\n",
      "Epoch 3877, Loss: 0.6338130086660385, Final Batch Loss: 0.20212037861347198\n",
      "Epoch 3878, Loss: 0.6727354228496552, Final Batch Loss: 0.26161471009254456\n",
      "Epoch 3879, Loss: 0.5602077543735504, Final Batch Loss: 0.1358221024274826\n",
      "Epoch 3880, Loss: 0.6175319105386734, Final Batch Loss: 0.20092922449111938\n",
      "Epoch 3881, Loss: 0.8479123115539551, Final Batch Loss: 0.3982731103897095\n",
      "Epoch 3882, Loss: 0.6454980224370956, Final Batch Loss: 0.27149051427841187\n",
      "Epoch 3883, Loss: 0.5297483280301094, Final Batch Loss: 0.09420482069253922\n",
      "Epoch 3884, Loss: 0.637817457318306, Final Batch Loss: 0.15622647106647491\n",
      "Epoch 3885, Loss: 0.4855825752019882, Final Batch Loss: 0.08035223186016083\n",
      "Epoch 3886, Loss: 0.5161124020814896, Final Batch Loss: 0.1578490287065506\n",
      "Epoch 3887, Loss: 0.6471847891807556, Final Batch Loss: 0.27143749594688416\n",
      "Epoch 3888, Loss: 0.5498875230550766, Final Batch Loss: 0.15294896066188812\n",
      "Epoch 3889, Loss: 0.6901167780160904, Final Batch Loss: 0.226396843791008\n",
      "Epoch 3890, Loss: 0.6968119144439697, Final Batch Loss: 0.28448978066444397\n",
      "Epoch 3891, Loss: 0.4195261262357235, Final Batch Loss: 0.01799696311354637\n",
      "Epoch 3892, Loss: 0.4938872903585434, Final Batch Loss: 0.10602380335330963\n",
      "Epoch 3893, Loss: 0.49391333013772964, Final Batch Loss: 0.11013955622911453\n",
      "Epoch 3894, Loss: 0.5284544974565506, Final Batch Loss: 0.11424905061721802\n",
      "Epoch 3895, Loss: 0.6672829687595367, Final Batch Loss: 0.25498858094215393\n",
      "Epoch 3896, Loss: 0.5532100573182106, Final Batch Loss: 0.06331001967191696\n",
      "Epoch 3897, Loss: 0.5418413057923317, Final Batch Loss: 0.060403548181056976\n",
      "Epoch 3898, Loss: 0.48343053460121155, Final Batch Loss: 0.10151416063308716\n",
      "Epoch 3899, Loss: 0.41127917915582657, Final Batch Loss: 0.0665443167090416\n",
      "Epoch 3900, Loss: 0.5617114305496216, Final Batch Loss: 0.15513712167739868\n",
      "Epoch 3901, Loss: 0.6437662243843079, Final Batch Loss: 0.2631593942642212\n",
      "Epoch 3902, Loss: 0.7945144772529602, Final Batch Loss: 0.46183380484580994\n",
      "Epoch 3903, Loss: 0.5404853969812393, Final Batch Loss: 0.16749951243400574\n",
      "Epoch 3904, Loss: 0.6789815723896027, Final Batch Loss: 0.28618261218070984\n",
      "Epoch 3905, Loss: 0.572027713060379, Final Batch Loss: 0.1879126876592636\n",
      "Epoch 3906, Loss: 0.48771901428699493, Final Batch Loss: 0.1013128012418747\n",
      "Epoch 3907, Loss: 0.4614095985889435, Final Batch Loss: 0.028295442461967468\n",
      "Epoch 3908, Loss: 0.7531485110521317, Final Batch Loss: 0.34103575348854065\n",
      "Epoch 3909, Loss: 0.4957387447357178, Final Batch Loss: 0.13337257504463196\n",
      "Epoch 3910, Loss: 0.567473441362381, Final Batch Loss: 0.19204320013523102\n",
      "Epoch 3911, Loss: 0.5927651822566986, Final Batch Loss: 0.21991267800331116\n",
      "Epoch 3912, Loss: 0.6750676482915878, Final Batch Loss: 0.3307779133319855\n",
      "Epoch 3913, Loss: 0.4963431879878044, Final Batch Loss: 0.07853438705205917\n",
      "Epoch 3914, Loss: 0.7765783220529556, Final Batch Loss: 0.3615758419036865\n",
      "Epoch 3915, Loss: 0.8221169114112854, Final Batch Loss: 0.4413706064224243\n",
      "Epoch 3916, Loss: 0.5305366367101669, Final Batch Loss: 0.12617214024066925\n",
      "Epoch 3917, Loss: 0.9694452732801437, Final Batch Loss: 0.6506652235984802\n",
      "Epoch 3918, Loss: 0.7402658760547638, Final Batch Loss: 0.3099038898944855\n",
      "Epoch 3919, Loss: 0.7769294232130051, Final Batch Loss: 0.40172237157821655\n",
      "Epoch 3920, Loss: 0.6232917159795761, Final Batch Loss: 0.1732499897480011\n",
      "Epoch 3921, Loss: 0.8369990587234497, Final Batch Loss: 0.3009099066257477\n",
      "Epoch 3922, Loss: 1.1060190051794052, Final Batch Loss: 0.7542364001274109\n",
      "Epoch 3923, Loss: 0.630543440580368, Final Batch Loss: 0.1405087411403656\n",
      "Epoch 3924, Loss: 0.730843186378479, Final Batch Loss: 0.2848222851753235\n",
      "Epoch 3925, Loss: 0.7170854806900024, Final Batch Loss: 0.31047606468200684\n",
      "Epoch 3926, Loss: 0.5059806853532791, Final Batch Loss: 0.13811764121055603\n",
      "Epoch 3927, Loss: 0.5130065008997917, Final Batch Loss: 0.09099314361810684\n",
      "Epoch 3928, Loss: 0.605056419968605, Final Batch Loss: 0.1822390854358673\n",
      "Epoch 3929, Loss: 0.4915814772248268, Final Batch Loss: 0.07478424161672592\n",
      "Epoch 3930, Loss: 0.5917230099439621, Final Batch Loss: 0.2310609519481659\n",
      "Epoch 3931, Loss: 0.5391906350851059, Final Batch Loss: 0.1799192875623703\n",
      "Epoch 3932, Loss: 0.6818781346082687, Final Batch Loss: 0.27885541319847107\n",
      "Epoch 3933, Loss: 0.5490296334028244, Final Batch Loss: 0.2185945212841034\n",
      "Epoch 3934, Loss: 0.6977009773254395, Final Batch Loss: 0.29063501954078674\n",
      "Epoch 3935, Loss: 0.5884151607751846, Final Batch Loss: 0.1462898552417755\n",
      "Epoch 3936, Loss: 0.5240314602851868, Final Batch Loss: 0.15486906468868256\n",
      "Epoch 3937, Loss: 0.4578310400247574, Final Batch Loss: 0.0738682895898819\n",
      "Epoch 3938, Loss: 0.7650169730186462, Final Batch Loss: 0.32207348942756653\n",
      "Epoch 3939, Loss: 0.5641696602106094, Final Batch Loss: 0.19967781007289886\n",
      "Epoch 3940, Loss: 0.7127613574266434, Final Batch Loss: 0.30624130368232727\n",
      "Epoch 3941, Loss: 0.5661512762308121, Final Batch Loss: 0.10458226501941681\n",
      "Epoch 3942, Loss: 0.5827047228813171, Final Batch Loss: 0.19294650852680206\n",
      "Epoch 3943, Loss: 0.45397472381591797, Final Batch Loss: 0.06828044354915619\n",
      "Epoch 3944, Loss: 0.47907475382089615, Final Batch Loss: 0.08618784695863724\n",
      "Epoch 3945, Loss: 0.5345335602760315, Final Batch Loss: 0.12379962205886841\n",
      "Epoch 3946, Loss: 0.522893913090229, Final Batch Loss: 0.11343634873628616\n",
      "Epoch 3947, Loss: 0.7368831634521484, Final Batch Loss: 0.32440462708473206\n",
      "Epoch 3948, Loss: 0.5835530459880829, Final Batch Loss: 0.2337760478258133\n",
      "Epoch 3949, Loss: 0.5586422383785248, Final Batch Loss: 0.15704213082790375\n",
      "Epoch 3950, Loss: 0.6597913950681686, Final Batch Loss: 0.21925552189350128\n",
      "Epoch 3951, Loss: 0.7713689208030701, Final Batch Loss: 0.29716986417770386\n",
      "Epoch 3952, Loss: 0.48416000604629517, Final Batch Loss: 0.16810593008995056\n",
      "Epoch 3953, Loss: 0.699878916144371, Final Batch Loss: 0.25765690207481384\n",
      "Epoch 3954, Loss: 0.512919008731842, Final Batch Loss: 0.13696886599063873\n",
      "Epoch 3955, Loss: 0.6420386582612991, Final Batch Loss: 0.2838311195373535\n",
      "Epoch 3956, Loss: 0.7068885117769241, Final Batch Loss: 0.24023114144802094\n",
      "Epoch 3957, Loss: 0.5797341093420982, Final Batch Loss: 0.10907501727342606\n",
      "Epoch 3958, Loss: 0.6252707540988922, Final Batch Loss: 0.20064401626586914\n",
      "Epoch 3959, Loss: 0.8856408596038818, Final Batch Loss: 0.5454764366149902\n",
      "Epoch 3960, Loss: 0.5160030946135521, Final Batch Loss: 0.06584171205759048\n",
      "Epoch 3961, Loss: 0.6011447608470917, Final Batch Loss: 0.19312582910060883\n",
      "Epoch 3962, Loss: 0.7110428959131241, Final Batch Loss: 0.2235804796218872\n",
      "Epoch 3963, Loss: 0.6775684058666229, Final Batch Loss: 0.20729200541973114\n",
      "Epoch 3964, Loss: 0.4730803668498993, Final Batch Loss: 0.08561451733112335\n",
      "Epoch 3965, Loss: 0.41992227733135223, Final Batch Loss: 0.0704108327627182\n",
      "Epoch 3966, Loss: 0.47637276351451874, Final Batch Loss: 0.16263501346111298\n",
      "Epoch 3967, Loss: 0.5593005418777466, Final Batch Loss: 0.2097811996936798\n",
      "Epoch 3968, Loss: 0.5048884227871895, Final Batch Loss: 0.11697167903184891\n",
      "Epoch 3969, Loss: 0.5989319682121277, Final Batch Loss: 0.2392200380563736\n",
      "Epoch 3970, Loss: 0.5373998731374741, Final Batch Loss: 0.21802033483982086\n",
      "Epoch 3971, Loss: 0.707070916891098, Final Batch Loss: 0.3408631980419159\n",
      "Epoch 3972, Loss: 0.6619520485401154, Final Batch Loss: 0.27515193819999695\n",
      "Epoch 3973, Loss: 0.39037157967686653, Final Batch Loss: 0.04411398246884346\n",
      "Epoch 3974, Loss: 0.5349075645208359, Final Batch Loss: 0.1636156290769577\n",
      "Epoch 3975, Loss: 0.6034605503082275, Final Batch Loss: 0.22288227081298828\n",
      "Epoch 3976, Loss: 0.5173133313655853, Final Batch Loss: 0.13035711646080017\n",
      "Epoch 3977, Loss: 0.46272196620702744, Final Batch Loss: 0.06905276328325272\n",
      "Epoch 3978, Loss: 0.5211460888385773, Final Batch Loss: 0.039408937096595764\n",
      "Epoch 3979, Loss: 0.711106926202774, Final Batch Loss: 0.2923787534236908\n",
      "Epoch 3980, Loss: 0.8873225897550583, Final Batch Loss: 0.4465065002441406\n",
      "Epoch 3981, Loss: 0.4945639744400978, Final Batch Loss: 0.10589561611413956\n",
      "Epoch 3982, Loss: 0.512758657336235, Final Batch Loss: 0.16859836876392365\n",
      "Epoch 3983, Loss: 0.5551369786262512, Final Batch Loss: 0.18213526904582977\n",
      "Epoch 3984, Loss: 0.5688768178224564, Final Batch Loss: 0.1722022294998169\n",
      "Epoch 3985, Loss: 0.49059338867664337, Final Batch Loss: 0.07108664512634277\n",
      "Epoch 3986, Loss: 0.6113868057727814, Final Batch Loss: 0.23260340094566345\n",
      "Epoch 3987, Loss: 0.7251058220863342, Final Batch Loss: 0.3383607268333435\n",
      "Epoch 3988, Loss: 0.5405450612306595, Final Batch Loss: 0.16955488920211792\n",
      "Epoch 3989, Loss: 0.4761842414736748, Final Batch Loss: 0.07912258058786392\n",
      "Epoch 3990, Loss: 0.5431981533765793, Final Batch Loss: 0.23432312905788422\n",
      "Epoch 3991, Loss: 0.5124307870864868, Final Batch Loss: 0.1511967033147812\n",
      "Epoch 3992, Loss: 0.4663156718015671, Final Batch Loss: 0.1179298609495163\n",
      "Epoch 3993, Loss: 0.5341838449239731, Final Batch Loss: 0.14507727324962616\n",
      "Epoch 3994, Loss: 0.5406383275985718, Final Batch Loss: 0.16716134548187256\n",
      "Epoch 3995, Loss: 0.6806054711341858, Final Batch Loss: 0.2465287446975708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3996, Loss: 0.4960381090641022, Final Batch Loss: 0.12830011546611786\n",
      "Epoch 3997, Loss: 0.5832815915346146, Final Batch Loss: 0.2505137324333191\n",
      "Epoch 3998, Loss: 0.6243915557861328, Final Batch Loss: 0.2522384524345398\n",
      "Epoch 3999, Loss: 0.513291485607624, Final Batch Loss: 0.07200465351343155\n",
      "Epoch 4000, Loss: 0.5384101867675781, Final Batch Loss: 0.12751001119613647\n",
      "Epoch 4001, Loss: 0.4582909196615219, Final Batch Loss: 0.07935914397239685\n",
      "Epoch 4002, Loss: 0.4971904009580612, Final Batch Loss: 0.09754541516304016\n",
      "Epoch 4003, Loss: 0.6150272041559219, Final Batch Loss: 0.23070558905601501\n",
      "Epoch 4004, Loss: 0.5054654777050018, Final Batch Loss: 0.09252206981182098\n",
      "Epoch 4005, Loss: 0.5831520184874535, Final Batch Loss: 0.21703371405601501\n",
      "Epoch 4006, Loss: 0.6666288524866104, Final Batch Loss: 0.26399683952331543\n",
      "Epoch 4007, Loss: 0.7271388918161392, Final Batch Loss: 0.3279735743999481\n",
      "Epoch 4008, Loss: 0.6151700615882874, Final Batch Loss: 0.19610542058944702\n",
      "Epoch 4009, Loss: 0.5210340544581413, Final Batch Loss: 0.08459896594285965\n",
      "Epoch 4010, Loss: 0.6032500714063644, Final Batch Loss: 0.18186897039413452\n",
      "Epoch 4011, Loss: 0.5665382444858551, Final Batch Loss: 0.14124150574207306\n",
      "Epoch 4012, Loss: 0.8001825660467148, Final Batch Loss: 0.3746342062950134\n",
      "Epoch 4013, Loss: 0.5274444743990898, Final Batch Loss: 0.0898570790886879\n",
      "Epoch 4014, Loss: 0.5066662430763245, Final Batch Loss: 0.09194333851337433\n",
      "Epoch 4015, Loss: 0.6334837675094604, Final Batch Loss: 0.13973960280418396\n",
      "Epoch 4016, Loss: 0.4224251750856638, Final Batch Loss: 0.01833748258650303\n",
      "Epoch 4017, Loss: 0.6295730918645859, Final Batch Loss: 0.1786765158176422\n",
      "Epoch 4018, Loss: 0.5645012408494949, Final Batch Loss: 0.09375560283660889\n",
      "Epoch 4019, Loss: 0.6387841254472733, Final Batch Loss: 0.21197494864463806\n",
      "Epoch 4020, Loss: 0.41591373458504677, Final Batch Loss: 0.05484677478671074\n",
      "Epoch 4021, Loss: 1.032703384757042, Final Batch Loss: 0.6463329195976257\n",
      "Epoch 4022, Loss: 0.5137290954589844, Final Batch Loss: 0.14009051024913788\n",
      "Epoch 4023, Loss: 0.6209045797586441, Final Batch Loss: 0.19198596477508545\n",
      "Epoch 4024, Loss: 0.43467904068529606, Final Batch Loss: 0.019303658977150917\n",
      "Epoch 4025, Loss: 0.5158083289861679, Final Batch Loss: 0.08933140337467194\n",
      "Epoch 4026, Loss: 0.9312338083982468, Final Batch Loss: 0.5337154269218445\n",
      "Epoch 4027, Loss: 0.5393274575471878, Final Batch Loss: 0.1807893067598343\n",
      "Epoch 4028, Loss: 0.5021720230579376, Final Batch Loss: 0.119640052318573\n",
      "Epoch 4029, Loss: 1.0436607152223587, Final Batch Loss: 0.5719156861305237\n",
      "Epoch 4030, Loss: 0.5045161694288254, Final Batch Loss: 0.12380506098270416\n",
      "Epoch 4031, Loss: 0.5930054187774658, Final Batch Loss: 0.15308435261249542\n",
      "Epoch 4032, Loss: 0.6322349011898041, Final Batch Loss: 0.10997126996517181\n",
      "Epoch 4033, Loss: 0.6114260405302048, Final Batch Loss: 0.17939171195030212\n",
      "Epoch 4034, Loss: 0.40714075043797493, Final Batch Loss: 0.058705586940050125\n",
      "Epoch 4035, Loss: 0.5474898219108582, Final Batch Loss: 0.14482352137565613\n",
      "Epoch 4036, Loss: 0.7329755872488022, Final Batch Loss: 0.2722572386264801\n",
      "Epoch 4037, Loss: 0.5862323343753815, Final Batch Loss: 0.18914811313152313\n",
      "Epoch 4038, Loss: 0.7480618506669998, Final Batch Loss: 0.3129647970199585\n",
      "Epoch 4039, Loss: 0.5851616859436035, Final Batch Loss: 0.169868603348732\n",
      "Epoch 4040, Loss: 0.5714689046144485, Final Batch Loss: 0.1657097041606903\n",
      "Epoch 4041, Loss: 0.4886373430490494, Final Batch Loss: 0.06989625096321106\n",
      "Epoch 4042, Loss: 0.6326072663068771, Final Batch Loss: 0.22501406073570251\n",
      "Epoch 4043, Loss: 0.7226954698562622, Final Batch Loss: 0.3211570382118225\n",
      "Epoch 4044, Loss: 0.5532848834991455, Final Batch Loss: 0.1856803447008133\n",
      "Epoch 4045, Loss: 0.5450552403926849, Final Batch Loss: 0.12962257862091064\n",
      "Epoch 4046, Loss: 0.7118944078683853, Final Batch Loss: 0.3010299503803253\n",
      "Epoch 4047, Loss: 0.5707492679357529, Final Batch Loss: 0.1154065877199173\n",
      "Epoch 4048, Loss: 0.6332910507917404, Final Batch Loss: 0.229307621717453\n",
      "Epoch 4049, Loss: 0.7512679845094681, Final Batch Loss: 0.2979559004306793\n",
      "Epoch 4050, Loss: 0.6199084222316742, Final Batch Loss: 0.22237099707126617\n",
      "Epoch 4051, Loss: 0.5376444756984711, Final Batch Loss: 0.16863581538200378\n",
      "Epoch 4052, Loss: 0.5451153814792633, Final Batch Loss: 0.18770059943199158\n",
      "Epoch 4053, Loss: 0.4282783232629299, Final Batch Loss: 0.05273480340838432\n",
      "Epoch 4054, Loss: 0.6102858036756516, Final Batch Loss: 0.20124630630016327\n",
      "Epoch 4055, Loss: 0.8257203549146652, Final Batch Loss: 0.4299604594707489\n",
      "Epoch 4056, Loss: 0.42660466209053993, Final Batch Loss: 0.036469269543886185\n",
      "Epoch 4057, Loss: 0.6485397666692734, Final Batch Loss: 0.2740880250930786\n",
      "Epoch 4058, Loss: 0.5144258588552475, Final Batch Loss: 0.09186902642250061\n",
      "Epoch 4059, Loss: 0.7053104341030121, Final Batch Loss: 0.27934709191322327\n",
      "Epoch 4060, Loss: 0.6362768858671188, Final Batch Loss: 0.23214486241340637\n",
      "Epoch 4061, Loss: 0.6948655843734741, Final Batch Loss: 0.3443668484687805\n",
      "Epoch 4062, Loss: 0.6401751488447189, Final Batch Loss: 0.22823283076286316\n",
      "Epoch 4063, Loss: 0.5483334064483643, Final Batch Loss: 0.09718647599220276\n",
      "Epoch 4064, Loss: 0.6067055910825729, Final Batch Loss: 0.15902027487754822\n",
      "Epoch 4065, Loss: 0.69398133456707, Final Batch Loss: 0.3083561658859253\n",
      "Epoch 4066, Loss: 0.567163810133934, Final Batch Loss: 0.1767064929008484\n",
      "Epoch 4067, Loss: 0.46845000237226486, Final Batch Loss: 0.08490783721208572\n",
      "Epoch 4068, Loss: 0.49051057919859886, Final Batch Loss: 0.03455784544348717\n",
      "Epoch 4069, Loss: 0.5719120725989342, Final Batch Loss: 0.10439326614141464\n",
      "Epoch 4070, Loss: 0.47610949724912643, Final Batch Loss: 0.06881733983755112\n",
      "Epoch 4071, Loss: 0.4869067445397377, Final Batch Loss: 0.11228571087121964\n",
      "Epoch 4072, Loss: 0.6039345264434814, Final Batch Loss: 0.17734302580356598\n",
      "Epoch 4073, Loss: 0.5729587376117706, Final Batch Loss: 0.22615721821784973\n",
      "Epoch 4074, Loss: 0.43495097011327744, Final Batch Loss: 0.04858226329088211\n",
      "Epoch 4075, Loss: 0.6622208654880524, Final Batch Loss: 0.29041969776153564\n",
      "Epoch 4076, Loss: 0.5685213059186935, Final Batch Loss: 0.15911386907100677\n",
      "Epoch 4077, Loss: 0.5173989906907082, Final Batch Loss: 0.0987410619854927\n",
      "Epoch 4078, Loss: 0.7027902901172638, Final Batch Loss: 0.2873814105987549\n",
      "Epoch 4079, Loss: 0.5105375796556473, Final Batch Loss: 0.17160458862781525\n",
      "Epoch 4080, Loss: 0.48862141370773315, Final Batch Loss: 0.08558842539787292\n",
      "Epoch 4081, Loss: 0.6614213734865189, Final Batch Loss: 0.2808988094329834\n",
      "Epoch 4082, Loss: 0.6471826285123825, Final Batch Loss: 0.3301272392272949\n",
      "Epoch 4083, Loss: 0.6603540629148483, Final Batch Loss: 0.2916972041130066\n",
      "Epoch 4084, Loss: 0.5811925828456879, Final Batch Loss: 0.19358961284160614\n",
      "Epoch 4085, Loss: 0.5920445621013641, Final Batch Loss: 0.20810313522815704\n",
      "Epoch 4086, Loss: 0.47135486267507076, Final Batch Loss: 0.015406301245093346\n",
      "Epoch 4087, Loss: 0.5649735033512115, Final Batch Loss: 0.17566129565238953\n",
      "Epoch 4088, Loss: 0.7677974849939346, Final Batch Loss: 0.31361421942710876\n",
      "Epoch 4089, Loss: 0.6699793487787247, Final Batch Loss: 0.28198885917663574\n",
      "Epoch 4090, Loss: 0.5165216773748398, Final Batch Loss: 0.19689075648784637\n",
      "Epoch 4091, Loss: 0.4754408374428749, Final Batch Loss: 0.09371796995401382\n",
      "Epoch 4092, Loss: 0.5386794880032539, Final Batch Loss: 0.10583881288766861\n",
      "Epoch 4093, Loss: 0.46419793367385864, Final Batch Loss: 0.08917148411273956\n",
      "Epoch 4094, Loss: 0.392451710999012, Final Batch Loss: 0.06857027858495712\n",
      "Epoch 4095, Loss: 0.7963239848613739, Final Batch Loss: 0.39137497544288635\n",
      "Epoch 4096, Loss: 0.7743751555681229, Final Batch Loss: 0.3954857587814331\n",
      "Epoch 4097, Loss: 0.4556087702512741, Final Batch Loss: 0.09074674546718597\n",
      "Epoch 4098, Loss: 0.4934864491224289, Final Batch Loss: 0.1435481607913971\n",
      "Epoch 4099, Loss: 0.7183866500854492, Final Batch Loss: 0.32868853211402893\n",
      "Epoch 4100, Loss: 0.43619650416076183, Final Batch Loss: 0.021536706015467644\n",
      "Epoch 4101, Loss: 0.5052726827561855, Final Batch Loss: 0.0616009347140789\n",
      "Epoch 4102, Loss: 0.6206836849451065, Final Batch Loss: 0.20063981413841248\n",
      "Epoch 4103, Loss: 0.6633662357926369, Final Batch Loss: 0.28069618344306946\n",
      "Epoch 4104, Loss: 0.5141943544149399, Final Batch Loss: 0.1307407170534134\n",
      "Epoch 4105, Loss: 0.5369242206215858, Final Batch Loss: 0.11044736951589584\n",
      "Epoch 4106, Loss: 0.5493953377008438, Final Batch Loss: 0.19638851284980774\n",
      "Epoch 4107, Loss: 0.9798295050859451, Final Batch Loss: 0.5824865698814392\n",
      "Epoch 4108, Loss: 0.5374040901660919, Final Batch Loss: 0.15971466898918152\n",
      "Epoch 4109, Loss: 0.5076367408037186, Final Batch Loss: 0.04903988540172577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4110, Loss: 0.7022421061992645, Final Batch Loss: 0.3260432481765747\n",
      "Epoch 4111, Loss: 0.4633663520216942, Final Batch Loss: 0.0741979256272316\n",
      "Epoch 4112, Loss: 0.5263407230377197, Final Batch Loss: 0.11577938497066498\n",
      "Epoch 4113, Loss: 0.5283020511269569, Final Batch Loss: 0.08753583580255508\n",
      "Epoch 4114, Loss: 0.9333577901124954, Final Batch Loss: 0.5286405682563782\n",
      "Epoch 4115, Loss: 0.7684154957532883, Final Batch Loss: 0.30569642782211304\n",
      "Epoch 4116, Loss: 0.5987507551908493, Final Batch Loss: 0.16315846145153046\n",
      "Epoch 4117, Loss: 0.5963596105575562, Final Batch Loss: 0.24774140119552612\n",
      "Epoch 4118, Loss: 0.4937485307455063, Final Batch Loss: 0.1434299796819687\n",
      "Epoch 4119, Loss: 0.5275449454784393, Final Batch Loss: 0.1676711142063141\n",
      "Epoch 4120, Loss: 0.4907084032893181, Final Batch Loss: 0.10810714215040207\n",
      "Epoch 4121, Loss: 0.43482664600014687, Final Batch Loss: 0.04598310962319374\n",
      "Epoch 4122, Loss: 0.52860127389431, Final Batch Loss: 0.13769280910491943\n",
      "Epoch 4123, Loss: 0.5429900586605072, Final Batch Loss: 0.12936587631702423\n",
      "Epoch 4124, Loss: 0.4102940019220114, Final Batch Loss: 0.014640172943472862\n",
      "Epoch 4125, Loss: 0.48914287984371185, Final Batch Loss: 0.10060064494609833\n",
      "Epoch 4126, Loss: 0.5496660768985748, Final Batch Loss: 0.18536971509456635\n",
      "Epoch 4127, Loss: 0.738369420170784, Final Batch Loss: 0.36336129903793335\n",
      "Epoch 4128, Loss: 0.7668970972299576, Final Batch Loss: 0.42030075192451477\n",
      "Epoch 4129, Loss: 0.5922575891017914, Final Batch Loss: 0.2209019958972931\n",
      "Epoch 4130, Loss: 0.6514568328857422, Final Batch Loss: 0.2299329787492752\n",
      "Epoch 4131, Loss: 0.5778127014636993, Final Batch Loss: 0.16052061319351196\n",
      "Epoch 4132, Loss: 0.6245295256376266, Final Batch Loss: 0.2525812089443207\n",
      "Epoch 4133, Loss: 0.6473854929208755, Final Batch Loss: 0.3069251775741577\n",
      "Epoch 4134, Loss: 0.5444111526012421, Final Batch Loss: 0.11100691556930542\n",
      "Epoch 4135, Loss: 0.6890573054552078, Final Batch Loss: 0.32229727506637573\n",
      "Epoch 4136, Loss: 0.5346982032060623, Final Batch Loss: 0.12616509199142456\n",
      "Epoch 4137, Loss: 0.5598604083061218, Final Batch Loss: 0.19366058707237244\n",
      "Epoch 4138, Loss: 0.389411024749279, Final Batch Loss: 0.04563704878091812\n",
      "Epoch 4139, Loss: 0.49117233604192734, Final Batch Loss: 0.09579835087060928\n",
      "Epoch 4140, Loss: 0.7231314331293106, Final Batch Loss: 0.39519003033638\n",
      "Epoch 4141, Loss: 0.5148103833198547, Final Batch Loss: 0.13204126060009003\n",
      "Epoch 4142, Loss: 0.4866231232881546, Final Batch Loss: 0.058086007833480835\n",
      "Epoch 4143, Loss: 0.7309924215078354, Final Batch Loss: 0.263313353061676\n",
      "Epoch 4144, Loss: 0.6058783233165741, Final Batch Loss: 0.2433294951915741\n",
      "Epoch 4145, Loss: 0.5160576403141022, Final Batch Loss: 0.15005244314670563\n",
      "Epoch 4146, Loss: 0.48066073656082153, Final Batch Loss: 0.1391775757074356\n",
      "Epoch 4147, Loss: 0.5123650208115578, Final Batch Loss: 0.061569102108478546\n",
      "Epoch 4148, Loss: 0.5997667759656906, Final Batch Loss: 0.14087410271167755\n",
      "Epoch 4149, Loss: 0.550793744623661, Final Batch Loss: 0.08889133483171463\n",
      "Epoch 4150, Loss: 0.8796011656522751, Final Batch Loss: 0.4677470922470093\n",
      "Epoch 4151, Loss: 0.6430467069149017, Final Batch Loss: 0.16690082848072052\n",
      "Epoch 4152, Loss: 0.4928555563092232, Final Batch Loss: 0.11319298297166824\n",
      "Epoch 4153, Loss: 0.7460412979125977, Final Batch Loss: 0.28196805715560913\n",
      "Epoch 4154, Loss: 0.8441061973571777, Final Batch Loss: 0.4523300230503082\n",
      "Epoch 4155, Loss: 0.7976175472140312, Final Batch Loss: 0.43448543548583984\n",
      "Epoch 4156, Loss: 0.40280698984861374, Final Batch Loss: 0.07307196408510208\n",
      "Epoch 4157, Loss: 0.44137895852327347, Final Batch Loss: 0.07381550222635269\n",
      "Epoch 4158, Loss: 0.6364190876483917, Final Batch Loss: 0.2162257730960846\n",
      "Epoch 4159, Loss: 0.5080716982483864, Final Batch Loss: 0.10644520074129105\n",
      "Epoch 4160, Loss: 0.7538944482803345, Final Batch Loss: 0.3773048222064972\n",
      "Epoch 4161, Loss: 0.568391352891922, Final Batch Loss: 0.2494170218706131\n",
      "Epoch 4162, Loss: 0.7189042568206787, Final Batch Loss: 0.3414050042629242\n",
      "Epoch 4163, Loss: 0.5784400105476379, Final Batch Loss: 0.16031937301158905\n",
      "Epoch 4164, Loss: 0.6124518662691116, Final Batch Loss: 0.25590744614601135\n",
      "Epoch 4165, Loss: 0.5868247449398041, Final Batch Loss: 0.13106228411197662\n",
      "Epoch 4166, Loss: 0.5510729551315308, Final Batch Loss: 0.1757240742444992\n",
      "Epoch 4167, Loss: 0.6996091902256012, Final Batch Loss: 0.3060602843761444\n",
      "Epoch 4168, Loss: 0.4366817772388458, Final Batch Loss: 0.07376548647880554\n",
      "Epoch 4169, Loss: 0.44123075902462006, Final Batch Loss: 0.07703034579753876\n",
      "Epoch 4170, Loss: 0.4616951420903206, Final Batch Loss: 0.07156576961278915\n",
      "Epoch 4171, Loss: 0.4967092275619507, Final Batch Loss: 0.09191051125526428\n",
      "Epoch 4172, Loss: 0.37811239808797836, Final Batch Loss: 0.04898815602064133\n",
      "Epoch 4173, Loss: 0.6337500810623169, Final Batch Loss: 0.3187490999698639\n",
      "Epoch 4174, Loss: 0.6492389291524887, Final Batch Loss: 0.30547574162483215\n",
      "Epoch 4175, Loss: 0.5782395303249359, Final Batch Loss: 0.13884961605072021\n",
      "Epoch 4176, Loss: 0.5209142491221428, Final Batch Loss: 0.09196790307760239\n",
      "Epoch 4177, Loss: 0.7114917635917664, Final Batch Loss: 0.2861400246620178\n",
      "Epoch 4178, Loss: 0.4523257836699486, Final Batch Loss: 0.0693875178694725\n",
      "Epoch 4179, Loss: 0.4766290411353111, Final Batch Loss: 0.10696297138929367\n",
      "Epoch 4180, Loss: 0.6537852734327316, Final Batch Loss: 0.26342156529426575\n",
      "Epoch 4181, Loss: 0.5729953497648239, Final Batch Loss: 0.20410025119781494\n",
      "Epoch 4182, Loss: 0.5510675758123398, Final Batch Loss: 0.15100930631160736\n",
      "Epoch 4183, Loss: 0.5185430198907852, Final Batch Loss: 0.17266781628131866\n",
      "Epoch 4184, Loss: 0.4462621286511421, Final Batch Loss: 0.09766104072332382\n",
      "Epoch 4185, Loss: 0.469928041100502, Final Batch Loss: 0.05931274592876434\n",
      "Epoch 4186, Loss: 0.45259247114881873, Final Batch Loss: 0.0023194043897092342\n",
      "Epoch 4187, Loss: 0.5767673403024673, Final Batch Loss: 0.1413344144821167\n",
      "Epoch 4188, Loss: 1.090835228562355, Final Batch Loss: 0.7181650400161743\n",
      "Epoch 4189, Loss: 0.5736826956272125, Final Batch Loss: 0.23695258796215057\n",
      "Epoch 4190, Loss: 0.6126804500818253, Final Batch Loss: 0.18729257583618164\n",
      "Epoch 4191, Loss: 0.7426789700984955, Final Batch Loss: 0.31526169180870056\n",
      "Epoch 4192, Loss: 0.685126930475235, Final Batch Loss: 0.21571846306324005\n",
      "Epoch 4193, Loss: 0.5871101319789886, Final Batch Loss: 0.1594332456588745\n",
      "Epoch 4194, Loss: 0.5084375217556953, Final Batch Loss: 0.11349546164274216\n",
      "Epoch 4195, Loss: 0.7494902461767197, Final Batch Loss: 0.3619336485862732\n",
      "Epoch 4196, Loss: 0.43899355828762054, Final Batch Loss: 0.05669665336608887\n",
      "Epoch 4197, Loss: 0.3736434057354927, Final Batch Loss: 0.0643707886338234\n",
      "Epoch 4198, Loss: 0.47724859043955803, Final Batch Loss: 0.03589382395148277\n",
      "Epoch 4199, Loss: 0.8161459937691689, Final Batch Loss: 0.4745589792728424\n",
      "Epoch 4200, Loss: 0.4627924710512161, Final Batch Loss: 0.10602466762065887\n",
      "Epoch 4201, Loss: 0.6716318577528, Final Batch Loss: 0.32214027643203735\n",
      "Epoch 4202, Loss: 0.8863190859556198, Final Batch Loss: 0.42609530687332153\n",
      "Epoch 4203, Loss: 0.5928334891796112, Final Batch Loss: 0.18423373997211456\n",
      "Epoch 4204, Loss: 0.48075468093156815, Final Batch Loss: 0.10390690714120865\n",
      "Epoch 4205, Loss: 0.5804915577173233, Final Batch Loss: 0.16360798478126526\n",
      "Epoch 4206, Loss: 0.6187594681978226, Final Batch Loss: 0.2198844701051712\n",
      "Epoch 4207, Loss: 0.629785880446434, Final Batch Loss: 0.24377699196338654\n",
      "Epoch 4208, Loss: 0.4624060168862343, Final Batch Loss: 0.0757930800318718\n",
      "Epoch 4209, Loss: 0.4841490611433983, Final Batch Loss: 0.07237005978822708\n",
      "Epoch 4210, Loss: 0.7725348025560379, Final Batch Loss: 0.3501571714878082\n",
      "Epoch 4211, Loss: 0.3976420983672142, Final Batch Loss: 0.07503212988376617\n",
      "Epoch 4212, Loss: 0.39572366140782833, Final Batch Loss: 0.018984830006957054\n",
      "Epoch 4213, Loss: 0.764925166964531, Final Batch Loss: 0.30109015107154846\n",
      "Epoch 4214, Loss: 0.5581942498683929, Final Batch Loss: 0.1763458549976349\n",
      "Epoch 4215, Loss: 0.3921859450638294, Final Batch Loss: 0.039039645344018936\n",
      "Epoch 4216, Loss: 0.5022298693656921, Final Batch Loss: 0.17538705468177795\n",
      "Epoch 4217, Loss: 0.5454482883214951, Final Batch Loss: 0.17144279181957245\n",
      "Epoch 4218, Loss: 0.6295919865369797, Final Batch Loss: 0.24597454071044922\n",
      "Epoch 4219, Loss: 0.5588267967104912, Final Batch Loss: 0.10635799914598465\n",
      "Epoch 4220, Loss: 0.6230478435754776, Final Batch Loss: 0.19707012176513672\n",
      "Epoch 4221, Loss: 0.5057213604450226, Final Batch Loss: 0.12787364423274994\n",
      "Epoch 4222, Loss: 0.5985769629478455, Final Batch Loss: 0.22319607436656952\n",
      "Epoch 4223, Loss: 0.47745317965745926, Final Batch Loss: 0.0868147686123848\n",
      "Epoch 4224, Loss: 0.5648748129606247, Final Batch Loss: 0.1844671666622162\n",
      "Epoch 4225, Loss: 0.7577212899923325, Final Batch Loss: 0.3491196036338806\n",
      "Epoch 4226, Loss: 0.728680744767189, Final Batch Loss: 0.3534773886203766\n",
      "Epoch 4227, Loss: 0.6051941365003586, Final Batch Loss: 0.27060964703559875\n",
      "Epoch 4228, Loss: 0.5212058126926422, Final Batch Loss: 0.10880059003829956\n",
      "Epoch 4229, Loss: 0.8171989470720291, Final Batch Loss: 0.4426630437374115\n",
      "Epoch 4230, Loss: 0.6453239470720291, Final Batch Loss: 0.24835996329784393\n",
      "Epoch 4231, Loss: 0.4592193067073822, Final Batch Loss: 0.10114680230617523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4232, Loss: 0.4234200082719326, Final Batch Loss: 0.03603838011622429\n",
      "Epoch 4233, Loss: 0.6767585873603821, Final Batch Loss: 0.2738448679447174\n",
      "Epoch 4234, Loss: 0.4211859703063965, Final Batch Loss: 0.07243530452251434\n",
      "Epoch 4235, Loss: 0.6635653525590897, Final Batch Loss: 0.2453838437795639\n",
      "Epoch 4236, Loss: 0.5614957064390182, Final Batch Loss: 0.1611149162054062\n",
      "Epoch 4237, Loss: 0.5331888794898987, Final Batch Loss: 0.16161616146564484\n",
      "Epoch 4238, Loss: 0.5996268689632416, Final Batch Loss: 0.2094472199678421\n",
      "Epoch 4239, Loss: 0.3916104771196842, Final Batch Loss: 0.04657130315899849\n",
      "Epoch 4240, Loss: 0.4568770304322243, Final Batch Loss: 0.06643872708082199\n",
      "Epoch 4241, Loss: 0.6357370316982269, Final Batch Loss: 0.2716304659843445\n",
      "Epoch 4242, Loss: 0.6410204917192459, Final Batch Loss: 0.2556261122226715\n",
      "Epoch 4243, Loss: 0.5999271273612976, Final Batch Loss: 0.2832672894001007\n",
      "Epoch 4244, Loss: 0.47542403638362885, Final Batch Loss: 0.12244367599487305\n",
      "Epoch 4245, Loss: 0.5711070895195007, Final Batch Loss: 0.20032675564289093\n",
      "Epoch 4246, Loss: 0.6820880770683289, Final Batch Loss: 0.2586246430873871\n",
      "Epoch 4247, Loss: 0.8371148854494095, Final Batch Loss: 0.3443162143230438\n",
      "Epoch 4248, Loss: 0.5472688376903534, Final Batch Loss: 0.14746475219726562\n",
      "Epoch 4249, Loss: 0.6007292121648788, Final Batch Loss: 0.19912973046302795\n",
      "Epoch 4250, Loss: 0.642210990190506, Final Batch Loss: 0.19889366626739502\n",
      "Epoch 4251, Loss: 0.5290312096476555, Final Batch Loss: 0.1077537015080452\n",
      "Epoch 4252, Loss: 0.7248262614011765, Final Batch Loss: 0.3128754198551178\n",
      "Epoch 4253, Loss: 0.6877751499414444, Final Batch Loss: 0.32570692896842957\n",
      "Epoch 4254, Loss: 0.6831393092870712, Final Batch Loss: 0.31709739565849304\n",
      "Epoch 4255, Loss: 0.49481412023305893, Final Batch Loss: 0.04645027965307236\n",
      "Epoch 4256, Loss: 0.584499254822731, Final Batch Loss: 0.12818022072315216\n",
      "Epoch 4257, Loss: 0.527007058262825, Final Batch Loss: 0.11542774736881256\n",
      "Epoch 4258, Loss: 0.49377769976854324, Final Batch Loss: 0.1036394014954567\n",
      "Epoch 4259, Loss: 0.4593776445835829, Final Batch Loss: 0.023199809715151787\n",
      "Epoch 4260, Loss: 0.4379826784133911, Final Batch Loss: 0.10871060192584991\n",
      "Epoch 4261, Loss: 0.6784090846776962, Final Batch Loss: 0.3402585983276367\n",
      "Epoch 4262, Loss: 0.6559322029352188, Final Batch Loss: 0.3081098198890686\n",
      "Epoch 4263, Loss: 0.6344381719827652, Final Batch Loss: 0.2661864757537842\n",
      "Epoch 4264, Loss: 0.4983042925596237, Final Batch Loss: 0.10127599537372589\n",
      "Epoch 4265, Loss: 0.8129193484783173, Final Batch Loss: 0.4486525058746338\n",
      "Epoch 4266, Loss: 0.6456056833267212, Final Batch Loss: 0.3121178150177002\n",
      "Epoch 4267, Loss: 0.7864125669002533, Final Batch Loss: 0.3231271207332611\n",
      "Epoch 4268, Loss: 0.8128886073827744, Final Batch Loss: 0.3618755638599396\n",
      "Epoch 4269, Loss: 0.5779614523053169, Final Batch Loss: 0.07785008102655411\n",
      "Epoch 4270, Loss: 0.6705451309680939, Final Batch Loss: 0.2330157607793808\n",
      "Epoch 4271, Loss: 0.6610038876533508, Final Batch Loss: 0.206792950630188\n",
      "Epoch 4272, Loss: 0.8262511193752289, Final Batch Loss: 0.416489839553833\n",
      "Epoch 4273, Loss: 0.6287732720375061, Final Batch Loss: 0.1311744600534439\n",
      "Epoch 4274, Loss: 0.6144442111253738, Final Batch Loss: 0.15766304731369019\n",
      "Epoch 4275, Loss: 0.5381429046392441, Final Batch Loss: 0.20245438814163208\n",
      "Epoch 4276, Loss: 0.5141457170248032, Final Batch Loss: 0.11605356633663177\n",
      "Epoch 4277, Loss: 0.579614669084549, Final Batch Loss: 0.11327908933162689\n",
      "Epoch 4278, Loss: 0.49364402890205383, Final Batch Loss: 0.15488339960575104\n",
      "Epoch 4279, Loss: 0.6312556266784668, Final Batch Loss: 0.27990391850471497\n",
      "Epoch 4280, Loss: 0.8127129077911377, Final Batch Loss: 0.38878241181373596\n",
      "Epoch 4281, Loss: 0.5779240131378174, Final Batch Loss: 0.17947830259799957\n",
      "Epoch 4282, Loss: 0.49804526567459106, Final Batch Loss: 0.16743247210979462\n",
      "Epoch 4283, Loss: 0.5053449869155884, Final Batch Loss: 0.1438358873128891\n",
      "Epoch 4284, Loss: 0.6323270350694656, Final Batch Loss: 0.26952195167541504\n",
      "Epoch 4285, Loss: 0.5880566090345383, Final Batch Loss: 0.16468074917793274\n",
      "Epoch 4286, Loss: 0.48421861976385117, Final Batch Loss: 0.09461169689893723\n",
      "Epoch 4287, Loss: 0.49534404277801514, Final Batch Loss: 0.1374024599790573\n",
      "Epoch 4288, Loss: 0.4655473008751869, Final Batch Loss: 0.06503575295209885\n",
      "Epoch 4289, Loss: 0.5707646831870079, Final Batch Loss: 0.11954034119844437\n",
      "Epoch 4290, Loss: 0.5323694199323654, Final Batch Loss: 0.17120307683944702\n",
      "Epoch 4291, Loss: 0.4096251558512449, Final Batch Loss: 0.030925361439585686\n",
      "Epoch 4292, Loss: 0.43753374367952347, Final Batch Loss: 0.040322817862033844\n",
      "Epoch 4293, Loss: 0.5622383505105972, Final Batch Loss: 0.15089313685894012\n",
      "Epoch 4294, Loss: 0.6228253245353699, Final Batch Loss: 0.19624942541122437\n",
      "Epoch 4295, Loss: 0.428732231259346, Final Batch Loss: 0.03305618464946747\n",
      "Epoch 4296, Loss: 0.6115628629922867, Final Batch Loss: 0.23087599873542786\n",
      "Epoch 4297, Loss: 0.5174016952514648, Final Batch Loss: 0.13139279186725616\n",
      "Epoch 4298, Loss: 0.7449240684509277, Final Batch Loss: 0.360884428024292\n",
      "Epoch 4299, Loss: 0.4084162637591362, Final Batch Loss: 0.034370385110378265\n",
      "Epoch 4300, Loss: 1.1171503365039825, Final Batch Loss: 0.6759933233261108\n",
      "Epoch 4301, Loss: 0.5812736451625824, Final Batch Loss: 0.16426336765289307\n",
      "Epoch 4302, Loss: 0.6720968335866928, Final Batch Loss: 0.28327298164367676\n",
      "Epoch 4303, Loss: 0.6984104812145233, Final Batch Loss: 0.2571583390235901\n",
      "Epoch 4304, Loss: 1.9912225753068924, Final Batch Loss: 1.6019580364227295\n",
      "Epoch 4305, Loss: 0.53868268430233, Final Batch Loss: 0.16217133402824402\n",
      "Epoch 4306, Loss: 0.7477210015058517, Final Batch Loss: 0.35667404532432556\n",
      "Epoch 4307, Loss: 0.809079185128212, Final Batch Loss: 0.39886346459388733\n",
      "Epoch 4308, Loss: 0.7690368145704269, Final Batch Loss: 0.3768913745880127\n",
      "Epoch 4309, Loss: 0.8511887937784195, Final Batch Loss: 0.4559447765350342\n",
      "Epoch 4310, Loss: 0.5106068775057793, Final Batch Loss: 0.07262825220823288\n",
      "Epoch 4311, Loss: 0.4533287473022938, Final Batch Loss: 0.03822239115834236\n",
      "Epoch 4312, Loss: 0.6213437169790268, Final Batch Loss: 0.21027660369873047\n",
      "Epoch 4313, Loss: 0.7996217012405396, Final Batch Loss: 0.3753505051136017\n",
      "Epoch 4314, Loss: 0.6125952750444412, Final Batch Loss: 0.2558535933494568\n",
      "Epoch 4315, Loss: 0.5418474227190018, Final Batch Loss: 0.1696605235338211\n",
      "Epoch 4316, Loss: 0.49495019018650055, Final Batch Loss: 0.15709491074085236\n",
      "Epoch 4317, Loss: 0.520693302154541, Final Batch Loss: 0.1344941109418869\n",
      "Epoch 4318, Loss: 0.5393458902835846, Final Batch Loss: 0.05595633387565613\n",
      "Epoch 4319, Loss: 0.6820917576551437, Final Batch Loss: 0.24878062307834625\n",
      "Epoch 4320, Loss: 0.5270252823829651, Final Batch Loss: 0.15231260657310486\n",
      "Epoch 4321, Loss: 0.6465882062911987, Final Batch Loss: 0.3196961283683777\n",
      "Epoch 4322, Loss: 0.6322076469659805, Final Batch Loss: 0.20474682748317719\n",
      "Epoch 4323, Loss: 0.6281186044216156, Final Batch Loss: 0.23397661745548248\n",
      "Epoch 4324, Loss: 0.5297350734472275, Final Batch Loss: 0.146365225315094\n",
      "Epoch 4325, Loss: 0.545435331761837, Final Batch Loss: 0.07330631464719772\n",
      "Epoch 4326, Loss: 0.6154780238866806, Final Batch Loss: 0.1678786277770996\n",
      "Epoch 4327, Loss: 1.2437520623207092, Final Batch Loss: 0.789136528968811\n",
      "Epoch 4328, Loss: 0.5551255345344543, Final Batch Loss: 0.1921570748090744\n",
      "Epoch 4329, Loss: 0.5806264132261276, Final Batch Loss: 0.14880189299583435\n",
      "Epoch 4330, Loss: 0.48017318919301033, Final Batch Loss: 0.056229282170534134\n",
      "Epoch 4331, Loss: 0.536409929394722, Final Batch Loss: 0.15931500494480133\n",
      "Epoch 4332, Loss: 0.5245455652475357, Final Batch Loss: 0.12347866594791412\n",
      "Epoch 4333, Loss: 0.6737409234046936, Final Batch Loss: 0.31970250606536865\n",
      "Epoch 4334, Loss: 0.6963678449392319, Final Batch Loss: 0.19178935885429382\n",
      "Epoch 4335, Loss: 0.5708976835012436, Final Batch Loss: 0.23812909424304962\n",
      "Epoch 4336, Loss: 0.6514826267957687, Final Batch Loss: 0.3171861171722412\n",
      "Epoch 4337, Loss: 0.6919966638088226, Final Batch Loss: 0.3183680474758148\n",
      "Epoch 4338, Loss: 0.5547340959310532, Final Batch Loss: 0.13818359375\n",
      "Epoch 4339, Loss: 0.8374810963869095, Final Batch Loss: 0.47748199105262756\n",
      "Epoch 4340, Loss: 0.7101451903581619, Final Batch Loss: 0.37181830406188965\n",
      "Epoch 4341, Loss: 0.6309549510478973, Final Batch Loss: 0.24093575775623322\n",
      "Epoch 4342, Loss: 0.4987664893269539, Final Batch Loss: 0.11193286627531052\n",
      "Epoch 4343, Loss: 0.48753824457526207, Final Batch Loss: 0.058392416685819626\n",
      "Epoch 4344, Loss: 0.4845152869820595, Final Batch Loss: 0.12016611546278\n",
      "Epoch 4345, Loss: 0.5894706845283508, Final Batch Loss: 0.2725903391838074\n",
      "Epoch 4346, Loss: 0.5063566118478775, Final Batch Loss: 0.14916537702083588\n",
      "Epoch 4347, Loss: 0.6705859899520874, Final Batch Loss: 0.3530714809894562\n",
      "Epoch 4348, Loss: 0.5305878967046738, Final Batch Loss: 0.16289399564266205\n",
      "Epoch 4349, Loss: 0.5421075373888016, Final Batch Loss: 0.1871296912431717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4350, Loss: 0.4920603036880493, Final Batch Loss: 0.11627191305160522\n",
      "Epoch 4351, Loss: 0.40253283455967903, Final Batch Loss: 0.030809108167886734\n",
      "Epoch 4352, Loss: 0.49349016696214676, Final Batch Loss: 0.09571710973978043\n",
      "Epoch 4353, Loss: 0.522857666015625, Final Batch Loss: 0.19436167180538177\n",
      "Epoch 4354, Loss: 0.41978999227285385, Final Batch Loss: 0.09542617946863174\n",
      "Epoch 4355, Loss: 0.5501049160957336, Final Batch Loss: 0.1653590053319931\n",
      "Epoch 4356, Loss: 0.7236985862255096, Final Batch Loss: 0.35623717308044434\n",
      "Epoch 4357, Loss: 0.48712319880723953, Final Batch Loss: 0.08350171893835068\n",
      "Epoch 4358, Loss: 0.5297700613737106, Final Batch Loss: 0.11969852447509766\n",
      "Epoch 4359, Loss: 0.4537564665079117, Final Batch Loss: 0.0815068781375885\n",
      "Epoch 4360, Loss: 0.6604825258255005, Final Batch Loss: 0.2426750808954239\n",
      "Epoch 4361, Loss: 0.5677602887153625, Final Batch Loss: 0.21678279340267181\n",
      "Epoch 4362, Loss: 0.4456496462225914, Final Batch Loss: 0.06474427133798599\n",
      "Epoch 4363, Loss: 0.38470615446567535, Final Batch Loss: 0.050912708044052124\n",
      "Epoch 4364, Loss: 0.48191822320222855, Final Batch Loss: 0.09020795673131943\n",
      "Epoch 4365, Loss: 0.3897348567843437, Final Batch Loss: 0.05845602601766586\n",
      "Epoch 4366, Loss: 0.5338986590504646, Final Batch Loss: 0.07659763842821121\n",
      "Epoch 4367, Loss: 0.41404030099511147, Final Batch Loss: 0.051394689828157425\n",
      "Epoch 4368, Loss: 0.4308263212442398, Final Batch Loss: 0.0757889598608017\n",
      "Epoch 4369, Loss: 0.6690574586391449, Final Batch Loss: 0.3402588665485382\n",
      "Epoch 4370, Loss: 0.43563124164938927, Final Batch Loss: 0.06230355426669121\n",
      "Epoch 4371, Loss: 0.3907402902841568, Final Batch Loss: 0.08053141832351685\n",
      "Epoch 4372, Loss: 0.761718824505806, Final Batch Loss: 0.37788867950439453\n",
      "Epoch 4373, Loss: 0.5167391002178192, Final Batch Loss: 0.12528693675994873\n",
      "Epoch 4374, Loss: 0.8799746334552765, Final Batch Loss: 0.5212829113006592\n",
      "Epoch 4375, Loss: 0.6810699552297592, Final Batch Loss: 0.3138425946235657\n",
      "Epoch 4376, Loss: 0.4911561906337738, Final Batch Loss: 0.13194666802883148\n",
      "Epoch 4377, Loss: 0.5332363694906235, Final Batch Loss: 0.16210700571537018\n",
      "Epoch 4378, Loss: 0.5693719536066055, Final Batch Loss: 0.20555761456489563\n",
      "Epoch 4379, Loss: 0.7483825087547302, Final Batch Loss: 0.42146873474121094\n",
      "Epoch 4380, Loss: 0.607428252696991, Final Batch Loss: 0.21475470066070557\n",
      "Epoch 4381, Loss: 0.5147112160921097, Final Batch Loss: 0.15222004055976868\n",
      "Epoch 4382, Loss: 0.597430869936943, Final Batch Loss: 0.2031996101140976\n",
      "Epoch 4383, Loss: 0.7305386811494827, Final Batch Loss: 0.3410213589668274\n",
      "Epoch 4384, Loss: 0.7650781571865082, Final Batch Loss: 0.3734937608242035\n",
      "Epoch 4385, Loss: 0.7696535587310791, Final Batch Loss: 0.44915544986724854\n",
      "Epoch 4386, Loss: 0.5923798382282257, Final Batch Loss: 0.23432636260986328\n",
      "Epoch 4387, Loss: 0.624897763133049, Final Batch Loss: 0.21347369253635406\n",
      "Epoch 4388, Loss: 0.7114173471927643, Final Batch Loss: 0.37513676285743713\n",
      "Epoch 4389, Loss: 0.4238712228834629, Final Batch Loss: 0.04163059964776039\n",
      "Epoch 4390, Loss: 0.6080109924077988, Final Batch Loss: 0.19157452881336212\n",
      "Epoch 4391, Loss: 0.5834052339196205, Final Batch Loss: 0.25642699003219604\n",
      "Epoch 4392, Loss: 0.45096319913864136, Final Batch Loss: 0.11373306810855865\n",
      "Epoch 4393, Loss: 0.4650956094264984, Final Batch Loss: 0.1130158007144928\n",
      "Epoch 4394, Loss: 0.6032844632863998, Final Batch Loss: 0.21035394072532654\n",
      "Epoch 4395, Loss: 0.6223826706409454, Final Batch Loss: 0.3022061884403229\n",
      "Epoch 4396, Loss: 0.5205100849270821, Final Batch Loss: 0.120940662920475\n",
      "Epoch 4397, Loss: 0.5185654163360596, Final Batch Loss: 0.14251379668712616\n",
      "Epoch 4398, Loss: 0.6171838790178299, Final Batch Loss: 0.23222537338733673\n",
      "Epoch 4399, Loss: 0.541007861495018, Final Batch Loss: 0.18788206577301025\n",
      "Epoch 4400, Loss: 0.5040073990821838, Final Batch Loss: 0.08833342790603638\n",
      "Epoch 4401, Loss: 0.4410596787929535, Final Batch Loss: 0.10233698785305023\n",
      "Epoch 4402, Loss: 0.6372056603431702, Final Batch Loss: 0.2976844906806946\n",
      "Epoch 4403, Loss: 0.5236770659685135, Final Batch Loss: 0.15330369770526886\n",
      "Epoch 4404, Loss: 0.45390889048576355, Final Batch Loss: 0.08772332966327667\n",
      "Epoch 4405, Loss: 0.5654619634151459, Final Batch Loss: 0.17531093955039978\n",
      "Epoch 4406, Loss: 0.4443850964307785, Final Batch Loss: 0.06428241729736328\n",
      "Epoch 4407, Loss: 0.6177641451358795, Final Batch Loss: 0.2854735553264618\n",
      "Epoch 4408, Loss: 0.4605574607849121, Final Batch Loss: 0.09508353471755981\n",
      "Epoch 4409, Loss: 0.656587079167366, Final Batch Loss: 0.24877603352069855\n",
      "Epoch 4410, Loss: 0.4720229357481003, Final Batch Loss: 0.042496174573898315\n",
      "Epoch 4411, Loss: 0.8360433876514435, Final Batch Loss: 0.4212254583835602\n",
      "Epoch 4412, Loss: 0.4707586318254471, Final Batch Loss: 0.12863369286060333\n",
      "Epoch 4413, Loss: 0.6285617351531982, Final Batch Loss: 0.2505010664463043\n",
      "Epoch 4414, Loss: 0.5857173055410385, Final Batch Loss: 0.23916804790496826\n",
      "Epoch 4415, Loss: 0.5792555212974548, Final Batch Loss: 0.2597902715206146\n",
      "Epoch 4416, Loss: 0.6775552555918694, Final Batch Loss: 0.3158108592033386\n",
      "Epoch 4417, Loss: 0.4214779734611511, Final Batch Loss: 0.08770494163036346\n",
      "Epoch 4418, Loss: 0.5109472200274467, Final Batch Loss: 0.08512646704912186\n",
      "Epoch 4419, Loss: 0.43180041015148163, Final Batch Loss: 0.07003651559352875\n",
      "Epoch 4420, Loss: 0.746297299861908, Final Batch Loss: 0.32261112332344055\n",
      "Epoch 4421, Loss: 0.5742392241954803, Final Batch Loss: 0.18862010538578033\n",
      "Epoch 4422, Loss: 0.4727085530757904, Final Batch Loss: 0.12727415561676025\n",
      "Epoch 4423, Loss: 0.4558445066213608, Final Batch Loss: 0.11641405522823334\n",
      "Epoch 4424, Loss: 0.39502789080142975, Final Batch Loss: 0.0600380003452301\n",
      "Epoch 4425, Loss: 0.4455748572945595, Final Batch Loss: 0.1135370209813118\n",
      "Epoch 4426, Loss: 0.7185531705617905, Final Batch Loss: 0.3495943248271942\n",
      "Epoch 4427, Loss: 0.5247896760702133, Final Batch Loss: 0.17211586236953735\n",
      "Epoch 4428, Loss: 0.6618878394365311, Final Batch Loss: 0.29613709449768066\n",
      "Epoch 4429, Loss: 0.7624599188566208, Final Batch Loss: 0.37016183137893677\n",
      "Epoch 4430, Loss: 0.4458479508757591, Final Batch Loss: 0.09464064985513687\n",
      "Epoch 4431, Loss: 0.7072826027870178, Final Batch Loss: 0.3124113976955414\n",
      "Epoch 4432, Loss: 0.7015691250562668, Final Batch Loss: 0.35841307044029236\n",
      "Epoch 4433, Loss: 0.45645423233509064, Final Batch Loss: 0.09253336489200592\n",
      "Epoch 4434, Loss: 0.4859969913959503, Final Batch Loss: 0.07024340331554413\n",
      "Epoch 4435, Loss: 0.5904823988676071, Final Batch Loss: 0.15823376178741455\n",
      "Epoch 4436, Loss: 0.621444821357727, Final Batch Loss: 0.22559337317943573\n",
      "Epoch 4437, Loss: 0.4702432341873646, Final Batch Loss: 0.060540784150362015\n",
      "Epoch 4438, Loss: 0.5493282824754715, Final Batch Loss: 0.22526395320892334\n",
      "Epoch 4439, Loss: 0.8084730505943298, Final Batch Loss: 0.45509228110313416\n",
      "Epoch 4440, Loss: 0.5732022672891617, Final Batch Loss: 0.17052145302295685\n",
      "Epoch 4441, Loss: 0.5543335676193237, Final Batch Loss: 0.150035560131073\n",
      "Epoch 4442, Loss: 0.5554813295602798, Final Batch Loss: 0.14609676599502563\n",
      "Epoch 4443, Loss: 0.5013048425316811, Final Batch Loss: 0.12166775017976761\n",
      "Epoch 4444, Loss: 0.5334662646055222, Final Batch Loss: 0.12133225798606873\n",
      "Epoch 4445, Loss: 0.6188690066337585, Final Batch Loss: 0.28504160046577454\n",
      "Epoch 4446, Loss: 0.5863656550645828, Final Batch Loss: 0.22756707668304443\n",
      "Epoch 4447, Loss: 0.6448094248771667, Final Batch Loss: 0.31736668944358826\n",
      "Epoch 4448, Loss: 0.46017059683799744, Final Batch Loss: 0.11615428328514099\n",
      "Epoch 4449, Loss: 0.45143648982048035, Final Batch Loss: 0.12411530315876007\n",
      "Epoch 4450, Loss: 0.5156666338443756, Final Batch Loss: 0.13833457231521606\n",
      "Epoch 4451, Loss: 0.6533334106206894, Final Batch Loss: 0.27918294072151184\n",
      "Epoch 4452, Loss: 0.4366783872246742, Final Batch Loss: 0.08028239756822586\n",
      "Epoch 4453, Loss: 0.5126577019691467, Final Batch Loss: 0.15736381709575653\n",
      "Epoch 4454, Loss: 0.7896099388599396, Final Batch Loss: 0.42030730843544006\n",
      "Epoch 4455, Loss: 0.5682259649038315, Final Batch Loss: 0.20646081864833832\n",
      "Epoch 4456, Loss: 0.5126625597476959, Final Batch Loss: 0.11718781292438507\n",
      "Epoch 4457, Loss: 0.8883735537528992, Final Batch Loss: 0.4713393747806549\n",
      "Epoch 4458, Loss: 0.48762213438749313, Final Batch Loss: 0.11503567546606064\n",
      "Epoch 4459, Loss: 0.5440838485956192, Final Batch Loss: 0.20366795361042023\n",
      "Epoch 4460, Loss: 0.559712678194046, Final Batch Loss: 0.25026440620422363\n",
      "Epoch 4461, Loss: 0.4400094151496887, Final Batch Loss: 0.05699937045574188\n",
      "Epoch 4462, Loss: 0.6183805912733078, Final Batch Loss: 0.1926785409450531\n",
      "Epoch 4463, Loss: 0.6335161775350571, Final Batch Loss: 0.2645471692085266\n",
      "Epoch 4464, Loss: 0.8252011984586716, Final Batch Loss: 0.45702823996543884\n",
      "Epoch 4465, Loss: 0.5043423846364021, Final Batch Loss: 0.09519656747579575\n",
      "Epoch 4466, Loss: 0.9287448078393936, Final Batch Loss: 0.5109312534332275\n",
      "Epoch 4467, Loss: 0.4411175027489662, Final Batch Loss: 0.08967951685190201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4468, Loss: 0.41845228895545006, Final Batch Loss: 0.046510692685842514\n",
      "Epoch 4469, Loss: 0.5482931733131409, Final Batch Loss: 0.1403018832206726\n",
      "Epoch 4470, Loss: 0.5372059643268585, Final Batch Loss: 0.17591188848018646\n",
      "Epoch 4471, Loss: 0.42055535316467285, Final Batch Loss: 0.07252363860607147\n",
      "Epoch 4472, Loss: 0.4643116518855095, Final Batch Loss: 0.046049974858760834\n",
      "Epoch 4473, Loss: 0.5849869102239609, Final Batch Loss: 0.2092042714357376\n",
      "Epoch 4474, Loss: 0.49206380173563957, Final Batch Loss: 0.03200076147913933\n",
      "Epoch 4475, Loss: 0.562303215265274, Final Batch Loss: 0.18300510942935944\n",
      "Epoch 4476, Loss: 0.5737955719232559, Final Batch Loss: 0.2175697237253189\n",
      "Epoch 4477, Loss: 0.47572020441293716, Final Batch Loss: 0.11665342003107071\n",
      "Epoch 4478, Loss: 0.5000303760170937, Final Batch Loss: 0.0731649324297905\n",
      "Epoch 4479, Loss: 0.45450038462877274, Final Batch Loss: 0.06895139068365097\n",
      "Epoch 4480, Loss: 0.44563233852386475, Final Batch Loss: 0.1088365912437439\n",
      "Epoch 4481, Loss: 0.45906032621860504, Final Batch Loss: 0.06939515471458435\n",
      "Epoch 4482, Loss: 0.503338560461998, Final Batch Loss: 0.14001969993114471\n",
      "Epoch 4483, Loss: 0.5406130105257034, Final Batch Loss: 0.2382696121931076\n",
      "Epoch 4484, Loss: 0.5144932270050049, Final Batch Loss: 0.1805359423160553\n",
      "Epoch 4485, Loss: 0.5887143760919571, Final Batch Loss: 0.19909246265888214\n",
      "Epoch 4486, Loss: 0.5526921898126602, Final Batch Loss: 0.2207684963941574\n",
      "Epoch 4487, Loss: 0.7857393622398376, Final Batch Loss: 0.41678646206855774\n",
      "Epoch 4488, Loss: 0.5791302621364594, Final Batch Loss: 0.29607367515563965\n",
      "Epoch 4489, Loss: 0.5270360708236694, Final Batch Loss: 0.21599608659744263\n",
      "Epoch 4490, Loss: 0.5046316459774971, Final Batch Loss: 0.11499319225549698\n",
      "Epoch 4491, Loss: 0.5728133767843246, Final Batch Loss: 0.18433649837970734\n",
      "Epoch 4492, Loss: 0.7368472367525101, Final Batch Loss: 0.33590665459632874\n",
      "Epoch 4493, Loss: 0.7200984060764313, Final Batch Loss: 0.36500313878059387\n",
      "Epoch 4494, Loss: 0.6098507642745972, Final Batch Loss: 0.2189822942018509\n",
      "Epoch 4495, Loss: 0.5868862792849541, Final Batch Loss: 0.11098184436559677\n",
      "Epoch 4496, Loss: 0.5248451977968216, Final Batch Loss: 0.09761805832386017\n",
      "Epoch 4497, Loss: 0.668457955121994, Final Batch Loss: 0.25981178879737854\n",
      "Epoch 4498, Loss: 0.44365544617176056, Final Batch Loss: 0.06434871256351471\n",
      "Epoch 4499, Loss: 0.5738885551691055, Final Batch Loss: 0.2311609983444214\n",
      "Epoch 4500, Loss: 0.4699564576148987, Final Batch Loss: 0.1189689040184021\n",
      "Epoch 4501, Loss: 0.37934789061546326, Final Batch Loss: 0.041508257389068604\n",
      "Epoch 4502, Loss: 0.5589818209409714, Final Batch Loss: 0.18214261531829834\n",
      "Epoch 4503, Loss: 0.6895506829023361, Final Batch Loss: 0.2672082781791687\n",
      "Epoch 4504, Loss: 0.5722288489341736, Final Batch Loss: 0.18384644389152527\n",
      "Epoch 4505, Loss: 0.5155878216028214, Final Batch Loss: 0.13206425309181213\n",
      "Epoch 4506, Loss: 0.6700102835893631, Final Batch Loss: 0.2784537971019745\n",
      "Epoch 4507, Loss: 0.5173069536685944, Final Batch Loss: 0.15804429352283478\n",
      "Epoch 4508, Loss: 0.5566727966070175, Final Batch Loss: 0.1811092048883438\n",
      "Epoch 4509, Loss: 0.7394148260354996, Final Batch Loss: 0.3572518527507782\n",
      "Epoch 4510, Loss: 0.7221148163080215, Final Batch Loss: 0.2240358591079712\n",
      "Epoch 4511, Loss: 0.5118695944547653, Final Batch Loss: 0.20382852852344513\n",
      "Epoch 4512, Loss: 0.5521222800016403, Final Batch Loss: 0.13223756849765778\n",
      "Epoch 4513, Loss: 0.7161283791065216, Final Batch Loss: 0.34762343764305115\n",
      "Epoch 4514, Loss: 0.5719920545816422, Final Batch Loss: 0.2069733738899231\n",
      "Epoch 4515, Loss: 0.5929794609546661, Final Batch Loss: 0.28809377551078796\n",
      "Epoch 4516, Loss: 0.6977209150791168, Final Batch Loss: 0.34847521781921387\n",
      "Epoch 4517, Loss: 0.45979592204093933, Final Batch Loss: 0.047619059681892395\n",
      "Epoch 4518, Loss: 0.5925254672765732, Final Batch Loss: 0.24339397251605988\n",
      "Epoch 4519, Loss: 0.6528997421264648, Final Batch Loss: 0.2769540846347809\n",
      "Epoch 4520, Loss: 0.3969668559730053, Final Batch Loss: 0.05899697169661522\n",
      "Epoch 4521, Loss: 0.5854772925376892, Final Batch Loss: 0.2145748883485794\n",
      "Epoch 4522, Loss: 0.42759912461042404, Final Batch Loss: 0.10820361226797104\n",
      "Epoch 4523, Loss: 0.5569562315940857, Final Batch Loss: 0.15717744827270508\n",
      "Epoch 4524, Loss: 0.5708744823932648, Final Batch Loss: 0.2466554343700409\n",
      "Epoch 4525, Loss: 0.4267847090959549, Final Batch Loss: 0.08283674716949463\n",
      "Epoch 4526, Loss: 0.6522445529699326, Final Batch Loss: 0.2947620451450348\n",
      "Epoch 4527, Loss: 0.44011715054512024, Final Batch Loss: 0.10040350258350372\n",
      "Epoch 4528, Loss: 0.6687672436237335, Final Batch Loss: 0.270828515291214\n",
      "Epoch 4529, Loss: 0.8315159678459167, Final Batch Loss: 0.4839688241481781\n",
      "Epoch 4530, Loss: 0.7518331110477448, Final Batch Loss: 0.38913872838020325\n",
      "Epoch 4531, Loss: 0.5289682969450951, Final Batch Loss: 0.2207224816083908\n",
      "Epoch 4532, Loss: 0.5546694099903107, Final Batch Loss: 0.16726410388946533\n",
      "Epoch 4533, Loss: 0.5008394047617912, Final Batch Loss: 0.12257999926805496\n",
      "Epoch 4534, Loss: 0.5034880936145782, Final Batch Loss: 0.13952060043811798\n",
      "Epoch 4535, Loss: 0.5658642798662186, Final Batch Loss: 0.21971039474010468\n",
      "Epoch 4536, Loss: 0.44366028159856796, Final Batch Loss: 0.09471604973077774\n",
      "Epoch 4537, Loss: 0.5098739862442017, Final Batch Loss: 0.18560791015625\n",
      "Epoch 4538, Loss: 0.4480733200907707, Final Batch Loss: 0.05266321450471878\n",
      "Epoch 4539, Loss: 0.4605288542807102, Final Batch Loss: 0.056499142199754715\n",
      "Epoch 4540, Loss: 0.5301627218723297, Final Batch Loss: 0.18679213523864746\n",
      "Epoch 4541, Loss: 0.4079878032207489, Final Batch Loss: 0.09288491308689117\n",
      "Epoch 4542, Loss: 0.4215485453605652, Final Batch Loss: 0.08439716696739197\n",
      "Epoch 4543, Loss: 0.5264915302395821, Final Batch Loss: 0.11441924422979355\n",
      "Epoch 4544, Loss: 0.8834184408187866, Final Batch Loss: 0.5313354730606079\n",
      "Epoch 4545, Loss: 0.469802126288414, Final Batch Loss: 0.1329505443572998\n",
      "Epoch 4546, Loss: 0.6231256127357483, Final Batch Loss: 0.2583153247833252\n",
      "Epoch 4547, Loss: 0.6035626232624054, Final Batch Loss: 0.251423180103302\n",
      "Epoch 4548, Loss: 0.4594323858618736, Final Batch Loss: 0.09860029071569443\n",
      "Epoch 4549, Loss: 0.567273810505867, Final Batch Loss: 0.19201810657978058\n",
      "Epoch 4550, Loss: 0.41618467308580875, Final Batch Loss: 0.02618345059454441\n",
      "Epoch 4551, Loss: 0.5008728355169296, Final Batch Loss: 0.11979763209819794\n",
      "Epoch 4552, Loss: 0.6576307117938995, Final Batch Loss: 0.28136447072029114\n",
      "Epoch 4553, Loss: 0.510631613433361, Final Batch Loss: 0.11470886319875717\n",
      "Epoch 4554, Loss: 0.49895821511745453, Final Batch Loss: 0.153668612241745\n",
      "Epoch 4555, Loss: 0.5316586941480637, Final Batch Loss: 0.14708560705184937\n",
      "Epoch 4556, Loss: 0.5299428701400757, Final Batch Loss: 0.17429353296756744\n",
      "Epoch 4557, Loss: 0.47474633157253265, Final Batch Loss: 0.17430618405342102\n",
      "Epoch 4558, Loss: 0.4685444161295891, Final Batch Loss: 0.11903560906648636\n",
      "Epoch 4559, Loss: 0.6504252552986145, Final Batch Loss: 0.326020747423172\n",
      "Epoch 4560, Loss: 0.5119778513908386, Final Batch Loss: 0.11892272531986237\n",
      "Epoch 4561, Loss: 0.6443704813718796, Final Batch Loss: 0.24403858184814453\n",
      "Epoch 4562, Loss: 0.3724750764667988, Final Batch Loss: 0.046476591378450394\n",
      "Epoch 4563, Loss: 0.5496958792209625, Final Batch Loss: 0.15946969389915466\n",
      "Epoch 4564, Loss: 0.5518719255924225, Final Batch Loss: 0.16704024374485016\n",
      "Epoch 4565, Loss: 0.45092638581991196, Final Batch Loss: 0.11532499641180038\n",
      "Epoch 4566, Loss: 0.5987752377986908, Final Batch Loss: 0.32154038548469543\n",
      "Epoch 4567, Loss: 0.6165307909250259, Final Batch Loss: 0.30825793743133545\n",
      "Epoch 4568, Loss: 0.49206705763936043, Final Batch Loss: 0.04890463128685951\n",
      "Epoch 4569, Loss: 0.6007259041070938, Final Batch Loss: 0.19985473155975342\n",
      "Epoch 4570, Loss: 0.42744865268468857, Final Batch Loss: 0.06241524964570999\n",
      "Epoch 4571, Loss: 0.47134368121623993, Final Batch Loss: 0.13859514892101288\n",
      "Epoch 4572, Loss: 0.46530260890722275, Final Batch Loss: 0.0986521914601326\n",
      "Epoch 4573, Loss: 0.4838341549038887, Final Batch Loss: 0.11629190295934677\n",
      "Epoch 4574, Loss: 0.4211477115750313, Final Batch Loss: 0.06938379257917404\n",
      "Epoch 4575, Loss: 0.4505583345890045, Final Batch Loss: 0.10520058870315552\n",
      "Epoch 4576, Loss: 0.5221513658761978, Final Batch Loss: 0.10929162800312042\n",
      "Epoch 4577, Loss: 0.53218574821949, Final Batch Loss: 0.20472677052021027\n",
      "Epoch 4578, Loss: 0.46514076739549637, Final Batch Loss: 0.11399469524621964\n",
      "Epoch 4579, Loss: 0.42222191393375397, Final Batch Loss: 0.09292805194854736\n",
      "Epoch 4580, Loss: 0.6033375710248947, Final Batch Loss: 0.27087506651878357\n",
      "Epoch 4581, Loss: 0.5175463408231735, Final Batch Loss: 0.14064164459705353\n",
      "Epoch 4582, Loss: 0.5623799115419388, Final Batch Loss: 0.1936761885881424\n",
      "Epoch 4583, Loss: 0.6334439814090729, Final Batch Loss: 0.2839829921722412\n",
      "Epoch 4584, Loss: 0.5014290660619736, Final Batch Loss: 0.19556017220020294\n",
      "Epoch 4585, Loss: 0.5940155237913132, Final Batch Loss: 0.29677829146385193\n",
      "Epoch 4586, Loss: 0.6011480987071991, Final Batch Loss: 0.2030400037765503\n",
      "Epoch 4587, Loss: 0.3660249896347523, Final Batch Loss: 0.06088993325829506\n",
      "Epoch 4588, Loss: 0.5948439538478851, Final Batch Loss: 0.2373633086681366\n",
      "Epoch 4589, Loss: 0.49735233187675476, Final Batch Loss: 0.14755766093730927\n",
      "Epoch 4590, Loss: 0.5015334933996201, Final Batch Loss: 0.14188441634178162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4591, Loss: 0.4722956568002701, Final Batch Loss: 0.09191827476024628\n",
      "Epoch 4592, Loss: 0.5601796656847, Final Batch Loss: 0.1453900784254074\n",
      "Epoch 4593, Loss: 0.43695785105228424, Final Batch Loss: 0.06572481989860535\n",
      "Epoch 4594, Loss: 0.3598969653248787, Final Batch Loss: 0.048959411680698395\n",
      "Epoch 4595, Loss: 0.8736111223697662, Final Batch Loss: 0.521377444267273\n",
      "Epoch 4596, Loss: 0.4448918104171753, Final Batch Loss: 0.1390306055545807\n",
      "Epoch 4597, Loss: 0.4267287403345108, Final Batch Loss: 0.09687879681587219\n",
      "Epoch 4598, Loss: 0.6089490205049515, Final Batch Loss: 0.21086464822292328\n",
      "Epoch 4599, Loss: 0.5652153193950653, Final Batch Loss: 0.14944550395011902\n",
      "Epoch 4600, Loss: 0.4155363067984581, Final Batch Loss: 0.06861060112714767\n",
      "Epoch 4601, Loss: 0.6443134546279907, Final Batch Loss: 0.2819133996963501\n",
      "Epoch 4602, Loss: 0.5695138871669769, Final Batch Loss: 0.2505939304828644\n",
      "Epoch 4603, Loss: 0.590709775686264, Final Batch Loss: 0.226286381483078\n",
      "Epoch 4604, Loss: 0.6099193245172501, Final Batch Loss: 0.2079668790102005\n",
      "Epoch 4605, Loss: 0.49969011545181274, Final Batch Loss: 0.10588717460632324\n",
      "Epoch 4606, Loss: 0.7220271229743958, Final Batch Loss: 0.36349132657051086\n",
      "Epoch 4607, Loss: 0.5660006403923035, Final Batch Loss: 0.2352622002363205\n",
      "Epoch 4608, Loss: 0.42363230884075165, Final Batch Loss: 0.09714514017105103\n",
      "Epoch 4609, Loss: 0.46727414429187775, Final Batch Loss: 0.0630926787853241\n",
      "Epoch 4610, Loss: 0.6282307654619217, Final Batch Loss: 0.18813197314739227\n",
      "Epoch 4611, Loss: 0.50444345921278, Final Batch Loss: 0.1095353290438652\n",
      "Epoch 4612, Loss: 0.8039944767951965, Final Batch Loss: 0.48977410793304443\n",
      "Epoch 4613, Loss: 0.5253999978303909, Final Batch Loss: 0.18602068722248077\n",
      "Epoch 4614, Loss: 0.631883978843689, Final Batch Loss: 0.23940856754779816\n",
      "Epoch 4615, Loss: 0.41078126057982445, Final Batch Loss: 0.05306583270430565\n",
      "Epoch 4616, Loss: 0.5032187700271606, Final Batch Loss: 0.11646074056625366\n",
      "Epoch 4617, Loss: 0.44376595318317413, Final Batch Loss: 0.11478772759437561\n",
      "Epoch 4618, Loss: 0.617540568113327, Final Batch Loss: 0.2237718552350998\n",
      "Epoch 4619, Loss: 0.7060797214508057, Final Batch Loss: 0.3807573616504669\n",
      "Epoch 4620, Loss: 0.5615651458501816, Final Batch Loss: 0.20309710502624512\n",
      "Epoch 4621, Loss: 0.5262301415205002, Final Batch Loss: 0.20705780386924744\n",
      "Epoch 4622, Loss: 0.5440321713685989, Final Batch Loss: 0.10321837663650513\n",
      "Epoch 4623, Loss: 0.41519860178232193, Final Batch Loss: 0.0831131562590599\n",
      "Epoch 4624, Loss: 0.5638485252857208, Final Batch Loss: 0.2265247255563736\n",
      "Epoch 4625, Loss: 0.44367270916700363, Final Batch Loss: 0.08473461121320724\n",
      "Epoch 4626, Loss: 0.5352537930011749, Final Batch Loss: 0.13389474153518677\n",
      "Epoch 4627, Loss: 0.5254824310541153, Final Batch Loss: 0.16428637504577637\n",
      "Epoch 4628, Loss: 0.4957361966371536, Final Batch Loss: 0.10558900237083435\n",
      "Epoch 4629, Loss: 0.501149520277977, Final Batch Loss: 0.1378878504037857\n",
      "Epoch 4630, Loss: 0.6468680202960968, Final Batch Loss: 0.32557860016822815\n",
      "Epoch 4631, Loss: 0.4550395756959915, Final Batch Loss: 0.14301162958145142\n",
      "Epoch 4632, Loss: 0.45080138742923737, Final Batch Loss: 0.1151069849729538\n",
      "Epoch 4633, Loss: 0.6210187450051308, Final Batch Loss: 0.3781065344810486\n",
      "Epoch 4634, Loss: 0.5160117968916893, Final Batch Loss: 0.12474333494901657\n",
      "Epoch 4635, Loss: 0.40600984543561935, Final Batch Loss: 0.04658759385347366\n",
      "Epoch 4636, Loss: 0.5614126324653625, Final Batch Loss: 0.175957590341568\n",
      "Epoch 4637, Loss: 0.401388444006443, Final Batch Loss: 0.10813779383897781\n",
      "Epoch 4638, Loss: 0.4642965868115425, Final Batch Loss: 0.11390919238328934\n",
      "Epoch 4639, Loss: 0.3861664831638336, Final Batch Loss: 0.06356213986873627\n",
      "Epoch 4640, Loss: 0.5774774700403214, Final Batch Loss: 0.13559599220752716\n",
      "Epoch 4641, Loss: 0.43367771059274673, Final Batch Loss: 0.08772339671850204\n",
      "Epoch 4642, Loss: 0.6272095739841461, Final Batch Loss: 0.31082504987716675\n",
      "Epoch 4643, Loss: 0.6709415763616562, Final Batch Loss: 0.33256033062934875\n",
      "Epoch 4644, Loss: 0.5506511926651001, Final Batch Loss: 0.22657717764377594\n",
      "Epoch 4645, Loss: 0.5585389137268066, Final Batch Loss: 0.25568634271621704\n",
      "Epoch 4646, Loss: 0.4667842239141464, Final Batch Loss: 0.10765257477760315\n",
      "Epoch 4647, Loss: 0.7741410434246063, Final Batch Loss: 0.26691916584968567\n",
      "Epoch 4648, Loss: 0.6836609095335007, Final Batch Loss: 0.29909560084342957\n",
      "Epoch 4649, Loss: 0.7220956385135651, Final Batch Loss: 0.288257360458374\n",
      "Epoch 4650, Loss: 0.6246640682220459, Final Batch Loss: 0.1956806778907776\n",
      "Epoch 4651, Loss: 0.5118940770626068, Final Batch Loss: 0.167711541056633\n",
      "Epoch 4652, Loss: 0.6191268935799599, Final Batch Loss: 0.1196964904665947\n",
      "Epoch 4653, Loss: 0.597479373216629, Final Batch Loss: 0.1979738175868988\n",
      "Epoch 4654, Loss: 0.5135833472013474, Final Batch Loss: 0.18249663710594177\n",
      "Epoch 4655, Loss: 0.70159712433815, Final Batch Loss: 0.26359909772872925\n",
      "Epoch 4656, Loss: 0.4719935804605484, Final Batch Loss: 0.07596927881240845\n",
      "Epoch 4657, Loss: 0.45893414318561554, Final Batch Loss: 0.1328735649585724\n",
      "Epoch 4658, Loss: 0.698714941740036, Final Batch Loss: 0.38140591979026794\n",
      "Epoch 4659, Loss: 0.6306408494710922, Final Batch Loss: 0.2776237428188324\n",
      "Epoch 4660, Loss: 0.534254178404808, Final Batch Loss: 0.19017691910266876\n",
      "Epoch 4661, Loss: 0.42569006234407425, Final Batch Loss: 0.07892327755689621\n",
      "Epoch 4662, Loss: 0.4978794455528259, Final Batch Loss: 0.18140563368797302\n",
      "Epoch 4663, Loss: 0.692853108048439, Final Batch Loss: 0.31569337844848633\n",
      "Epoch 4664, Loss: 0.46572402119636536, Final Batch Loss: 0.07066884636878967\n",
      "Epoch 4665, Loss: 0.6379232853651047, Final Batch Loss: 0.11655750870704651\n",
      "Epoch 4666, Loss: 0.7094640284776688, Final Batch Loss: 0.24727179110050201\n",
      "Epoch 4667, Loss: 0.4431568346917629, Final Batch Loss: 0.03585001453757286\n",
      "Epoch 4668, Loss: 0.6462192386388779, Final Batch Loss: 0.2954564392566681\n",
      "Epoch 4669, Loss: 0.5907012373209, Final Batch Loss: 0.22030521929264069\n",
      "Epoch 4670, Loss: 0.5466860383749008, Final Batch Loss: 0.17570903897285461\n",
      "Epoch 4671, Loss: 0.48980477452278137, Final Batch Loss: 0.11080700159072876\n",
      "Epoch 4672, Loss: 0.528576448559761, Final Batch Loss: 0.19574323296546936\n",
      "Epoch 4673, Loss: 0.39202011935412884, Final Batch Loss: 0.012692121788859367\n",
      "Epoch 4674, Loss: 0.5799896568059921, Final Batch Loss: 0.1525924652814865\n",
      "Epoch 4675, Loss: 0.52933369576931, Final Batch Loss: 0.1768472045660019\n",
      "Epoch 4676, Loss: 0.6914690136909485, Final Batch Loss: 0.262996107339859\n",
      "Epoch 4677, Loss: 0.5222499072551727, Final Batch Loss: 0.1288246214389801\n",
      "Epoch 4678, Loss: 0.6736743599176407, Final Batch Loss: 0.23463888466358185\n",
      "Epoch 4679, Loss: 0.5461957901716232, Final Batch Loss: 0.11037398874759674\n",
      "Epoch 4680, Loss: 0.6886320114135742, Final Batch Loss: 0.31994175910949707\n",
      "Epoch 4681, Loss: 0.5965005159378052, Final Batch Loss: 0.20996695756912231\n",
      "Epoch 4682, Loss: 0.7076041400432587, Final Batch Loss: 0.35843658447265625\n",
      "Epoch 4683, Loss: 0.5997000187635422, Final Batch Loss: 0.26833194494247437\n",
      "Epoch 4684, Loss: 0.5750167220830917, Final Batch Loss: 0.19695138931274414\n",
      "Epoch 4685, Loss: 0.41832349076867104, Final Batch Loss: 0.061867255717515945\n",
      "Epoch 4686, Loss: 0.6827376335859299, Final Batch Loss: 0.3282898962497711\n",
      "Epoch 4687, Loss: 0.49599431455135345, Final Batch Loss: 0.15827690064907074\n",
      "Epoch 4688, Loss: 0.5278704166412354, Final Batch Loss: 0.15857788920402527\n",
      "Epoch 4689, Loss: 0.5508008226752281, Final Batch Loss: 0.12321484833955765\n",
      "Epoch 4690, Loss: 0.7040124386548996, Final Batch Loss: 0.3435195982456207\n",
      "Epoch 4691, Loss: 0.4041827954351902, Final Batch Loss: 0.0227205790579319\n",
      "Epoch 4692, Loss: 0.43206438422203064, Final Batch Loss: 0.03821179270744324\n",
      "Epoch 4693, Loss: 0.4734662175178528, Final Batch Loss: 0.11893010139465332\n",
      "Epoch 4694, Loss: 0.524131566286087, Final Batch Loss: 0.11512693762779236\n",
      "Epoch 4695, Loss: 0.4267682731151581, Final Batch Loss: 0.09619845449924469\n",
      "Epoch 4696, Loss: 0.5909767299890518, Final Batch Loss: 0.22992779314517975\n",
      "Epoch 4697, Loss: 0.47047487646341324, Final Batch Loss: 0.06269849091768265\n",
      "Epoch 4698, Loss: 0.5364212244749069, Final Batch Loss: 0.133367121219635\n",
      "Epoch 4699, Loss: 0.5146307796239853, Final Batch Loss: 0.19431538879871368\n",
      "Epoch 4700, Loss: 0.5214149951934814, Final Batch Loss: 0.2207237333059311\n",
      "Epoch 4701, Loss: 0.35513111017644405, Final Batch Loss: 0.029810195788741112\n",
      "Epoch 4702, Loss: 0.5459460765123367, Final Batch Loss: 0.18949270248413086\n",
      "Epoch 4703, Loss: 0.3575387950986624, Final Batch Loss: 0.02583322487771511\n",
      "Epoch 4704, Loss: 0.7377568036317825, Final Batch Loss: 0.39370906352996826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4705, Loss: 0.802967980504036, Final Batch Loss: 0.4389391839504242\n",
      "Epoch 4706, Loss: 0.537649855017662, Final Batch Loss: 0.14576134085655212\n",
      "Epoch 4707, Loss: 0.39133111014962196, Final Batch Loss: 0.04225415363907814\n",
      "Epoch 4708, Loss: 0.4885893240571022, Final Batch Loss: 0.0969734713435173\n",
      "Epoch 4709, Loss: 0.4436807781457901, Final Batch Loss: 0.057528287172317505\n",
      "Epoch 4710, Loss: 0.6094308644533157, Final Batch Loss: 0.2046162486076355\n",
      "Epoch 4711, Loss: 0.5493947714567184, Final Batch Loss: 0.059905439615249634\n",
      "Epoch 4712, Loss: 0.7897484302520752, Final Batch Loss: 0.3597220778465271\n",
      "Epoch 4713, Loss: 0.48472652584314346, Final Batch Loss: 0.06436029821634293\n",
      "Epoch 4714, Loss: 0.6612119078636169, Final Batch Loss: 0.26062408089637756\n",
      "Epoch 4715, Loss: 0.5471373349428177, Final Batch Loss: 0.09767836332321167\n",
      "Epoch 4716, Loss: 0.5774001479148865, Final Batch Loss: 0.19543695449829102\n",
      "Epoch 4717, Loss: 0.5505374521017075, Final Batch Loss: 0.15709568560123444\n",
      "Epoch 4718, Loss: 0.5813726633787155, Final Batch Loss: 0.1437138319015503\n",
      "Epoch 4719, Loss: 0.47062067687511444, Final Batch Loss: 0.15149866044521332\n",
      "Epoch 4720, Loss: 0.47879549860954285, Final Batch Loss: 0.15983785688877106\n",
      "Epoch 4721, Loss: 0.5596432983875275, Final Batch Loss: 0.1508278101682663\n",
      "Epoch 4722, Loss: 0.517714187502861, Final Batch Loss: 0.14721627533435822\n",
      "Epoch 4723, Loss: 0.5166446268558502, Final Batch Loss: 0.16736629605293274\n",
      "Epoch 4724, Loss: 0.5795259326696396, Final Batch Loss: 0.21176892518997192\n",
      "Epoch 4725, Loss: 0.6213377118110657, Final Batch Loss: 0.31839948892593384\n",
      "Epoch 4726, Loss: 0.6429408937692642, Final Batch Loss: 0.15718123316764832\n",
      "Epoch 4727, Loss: 0.7097662538290024, Final Batch Loss: 0.29645219445228577\n",
      "Epoch 4728, Loss: 0.7931573837995529, Final Batch Loss: 0.3687978684902191\n",
      "Epoch 4729, Loss: 0.5499322712421417, Final Batch Loss: 0.2473578006029129\n",
      "Epoch 4730, Loss: 0.6027515232563019, Final Batch Loss: 0.26145321130752563\n",
      "Epoch 4731, Loss: 0.513812281191349, Final Batch Loss: 0.11036775261163712\n",
      "Epoch 4732, Loss: 0.4181391969323158, Final Batch Loss: 0.07802433520555496\n",
      "Epoch 4733, Loss: 0.5509541779756546, Final Batch Loss: 0.1609671413898468\n",
      "Epoch 4734, Loss: 0.4258951395750046, Final Batch Loss: 0.09803956747055054\n",
      "Epoch 4735, Loss: 0.4130924195051193, Final Batch Loss: 0.1150805801153183\n",
      "Epoch 4736, Loss: 0.5595697164535522, Final Batch Loss: 0.2329612523317337\n",
      "Epoch 4737, Loss: 0.5557850301265717, Final Batch Loss: 0.17452241480350494\n",
      "Epoch 4738, Loss: 0.7764527797698975, Final Batch Loss: 0.37715959548950195\n",
      "Epoch 4739, Loss: 0.6176558136940002, Final Batch Loss: 0.24684520065784454\n",
      "Epoch 4740, Loss: 0.8521870821714401, Final Batch Loss: 0.49266424775123596\n",
      "Epoch 4741, Loss: 0.5098496675491333, Final Batch Loss: 0.16264718770980835\n",
      "Epoch 4742, Loss: 0.8188116401433945, Final Batch Loss: 0.4629906713962555\n",
      "Epoch 4743, Loss: 0.5235148519277573, Final Batch Loss: 0.09384365379810333\n",
      "Epoch 4744, Loss: 0.6528554707765579, Final Batch Loss: 0.21413803100585938\n",
      "Epoch 4745, Loss: 0.8156398385763168, Final Batch Loss: 0.358712762594223\n",
      "Epoch 4746, Loss: 0.4619380906224251, Final Batch Loss: 0.09712740033864975\n",
      "Epoch 4747, Loss: 0.43504466116428375, Final Batch Loss: 0.07052674889564514\n",
      "Epoch 4748, Loss: 0.5853302627801895, Final Batch Loss: 0.13527333736419678\n",
      "Epoch 4749, Loss: 0.4914931207895279, Final Batch Loss: 0.1637532263994217\n",
      "Epoch 4750, Loss: 0.45039528608322144, Final Batch Loss: 0.11663104593753815\n",
      "Epoch 4751, Loss: 0.5855910778045654, Final Batch Loss: 0.2596183717250824\n",
      "Epoch 4752, Loss: 0.4441038444638252, Final Batch Loss: 0.09681438654661179\n",
      "Epoch 4753, Loss: 0.3957105726003647, Final Batch Loss: 0.040484458208084106\n",
      "Epoch 4754, Loss: 0.5327780842781067, Final Batch Loss: 0.16253556311130524\n",
      "Epoch 4755, Loss: 0.5046284347772598, Final Batch Loss: 0.21201379597187042\n",
      "Epoch 4756, Loss: 0.5804154425859451, Final Batch Loss: 0.24467450380325317\n",
      "Epoch 4757, Loss: 0.5359467267990112, Final Batch Loss: 0.18863151967525482\n",
      "Epoch 4758, Loss: 0.5955885946750641, Final Batch Loss: 0.2406955063343048\n",
      "Epoch 4759, Loss: 0.5387390479445457, Final Batch Loss: 0.09719882160425186\n",
      "Epoch 4760, Loss: 0.43254077434539795, Final Batch Loss: 0.06828339397907257\n",
      "Epoch 4761, Loss: 0.3644988611340523, Final Batch Loss: 0.09412012249231339\n",
      "Epoch 4762, Loss: 0.43186450749635696, Final Batch Loss: 0.08204708248376846\n",
      "Epoch 4763, Loss: 0.4473903477191925, Final Batch Loss: 0.10540205240249634\n",
      "Epoch 4764, Loss: 0.46756331622600555, Final Batch Loss: 0.10032254457473755\n",
      "Epoch 4765, Loss: 0.5592191368341446, Final Batch Loss: 0.1683189868927002\n",
      "Epoch 4766, Loss: 0.6181900203227997, Final Batch Loss: 0.24095818400382996\n",
      "Epoch 4767, Loss: 0.6343584507703781, Final Batch Loss: 0.29037559032440186\n",
      "Epoch 4768, Loss: 0.5726206004619598, Final Batch Loss: 0.15599803626537323\n",
      "Epoch 4769, Loss: 0.6042968481779099, Final Batch Loss: 0.15023522078990936\n",
      "Epoch 4770, Loss: 0.5520653426647186, Final Batch Loss: 0.16000138223171234\n",
      "Epoch 4771, Loss: 0.35401345416903496, Final Batch Loss: 0.05463757738471031\n",
      "Epoch 4772, Loss: 0.5583440661430359, Final Batch Loss: 0.2357160896062851\n",
      "Epoch 4773, Loss: 0.4758507162332535, Final Batch Loss: 0.13303928077220917\n",
      "Epoch 4774, Loss: 0.5341739058494568, Final Batch Loss: 0.23499929904937744\n",
      "Epoch 4775, Loss: 0.5520028173923492, Final Batch Loss: 0.2061861902475357\n",
      "Epoch 4776, Loss: 0.49808166921138763, Final Batch Loss: 0.12189960479736328\n",
      "Epoch 4777, Loss: 0.4587211608886719, Final Batch Loss: 0.13248908519744873\n",
      "Epoch 4778, Loss: 0.9883862286806107, Final Batch Loss: 0.6790047287940979\n",
      "Epoch 4779, Loss: 0.39040757715702057, Final Batch Loss: 0.11820229887962341\n",
      "Epoch 4780, Loss: 0.38984209299087524, Final Batch Loss: 0.05560731887817383\n",
      "Epoch 4781, Loss: 0.527235820889473, Final Batch Loss: 0.2084088772535324\n",
      "Epoch 4782, Loss: 0.611441045999527, Final Batch Loss: 0.19738496840000153\n",
      "Epoch 4783, Loss: 0.4682469666004181, Final Batch Loss: 0.11299371719360352\n",
      "Epoch 4784, Loss: 0.53204645216465, Final Batch Loss: 0.2011476457118988\n",
      "Epoch 4785, Loss: 0.5418675988912582, Final Batch Loss: 0.2003639042377472\n",
      "Epoch 4786, Loss: 0.5082971304655075, Final Batch Loss: 0.15858256816864014\n",
      "Epoch 4787, Loss: 0.6185883581638336, Final Batch Loss: 0.17030252516269684\n",
      "Epoch 4788, Loss: 0.6594074368476868, Final Batch Loss: 0.31689581274986267\n",
      "Epoch 4789, Loss: 0.5930061936378479, Final Batch Loss: 0.22789320349693298\n",
      "Epoch 4790, Loss: 0.7297324240207672, Final Batch Loss: 0.3945150375366211\n",
      "Epoch 4791, Loss: 0.4521426558494568, Final Batch Loss: 0.062408849596977234\n",
      "Epoch 4792, Loss: 0.412224892526865, Final Batch Loss: 0.04270866885781288\n",
      "Epoch 4793, Loss: 0.5467426627874374, Final Batch Loss: 0.1266113519668579\n",
      "Epoch 4794, Loss: 0.6332705467939377, Final Batch Loss: 0.227827250957489\n",
      "Epoch 4795, Loss: 0.5766588896512985, Final Batch Loss: 0.25435179471969604\n",
      "Epoch 4796, Loss: 0.3926785602234304, Final Batch Loss: 0.001333074178546667\n",
      "Epoch 4797, Loss: 0.4815543219447136, Final Batch Loss: 0.0885215625166893\n",
      "Epoch 4798, Loss: 0.6659578830003738, Final Batch Loss: 0.30914047360420227\n",
      "Epoch 4799, Loss: 0.4454292878508568, Final Batch Loss: 0.11697272211313248\n",
      "Epoch 4800, Loss: 0.5605403631925583, Final Batch Loss: 0.2315436601638794\n",
      "Epoch 4801, Loss: 0.7730288654565811, Final Batch Loss: 0.38866162300109863\n",
      "Epoch 4802, Loss: 0.3975759223103523, Final Batch Loss: 0.10501984506845474\n",
      "Epoch 4803, Loss: 0.5435519516468048, Final Batch Loss: 0.20341165363788605\n",
      "Epoch 4804, Loss: 0.45695603638887405, Final Batch Loss: 0.09969951957464218\n",
      "Epoch 4805, Loss: 0.8720735013484955, Final Batch Loss: 0.5556490421295166\n",
      "Epoch 4806, Loss: 0.4055875688791275, Final Batch Loss: 0.06513214111328125\n",
      "Epoch 4807, Loss: 0.9643102288246155, Final Batch Loss: 0.557674765586853\n",
      "Epoch 4808, Loss: 0.5303380638360977, Final Batch Loss: 0.15087205171585083\n",
      "Epoch 4809, Loss: 0.6444723159074783, Final Batch Loss: 0.24924758076667786\n",
      "Epoch 4810, Loss: 0.49593353644013405, Final Batch Loss: 0.017196539789438248\n",
      "Epoch 4811, Loss: 0.43888042867183685, Final Batch Loss: 0.04110746085643768\n",
      "Epoch 4812, Loss: 0.5364352911710739, Final Batch Loss: 0.13644057512283325\n",
      "Epoch 4813, Loss: 0.8116485625505447, Final Batch Loss: 0.46995529532432556\n",
      "Epoch 4814, Loss: 0.5779790580272675, Final Batch Loss: 0.19610032439231873\n",
      "Epoch 4815, Loss: 0.5580154657363892, Final Batch Loss: 0.13209591805934906\n",
      "Epoch 4816, Loss: 0.6279044896364212, Final Batch Loss: 0.30610573291778564\n",
      "Epoch 4817, Loss: 0.3381461873650551, Final Batch Loss: 0.04071381688117981\n",
      "Epoch 4818, Loss: 0.4889170825481415, Final Batch Loss: 0.13471193611621857\n",
      "Epoch 4819, Loss: 0.6693074256181717, Final Batch Loss: 0.3144616186618805\n",
      "Epoch 4820, Loss: 0.6246529817581177, Final Batch Loss: 0.2911069393157959\n",
      "Epoch 4821, Loss: 0.48873044550418854, Final Batch Loss: 0.10902801156044006\n",
      "Epoch 4822, Loss: 0.45948783308267593, Final Batch Loss: 0.08727400749921799\n",
      "Epoch 4823, Loss: 0.4891970157623291, Final Batch Loss: 0.13520865142345428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4824, Loss: 0.4595402777194977, Final Batch Loss: 0.14068925380706787\n",
      "Epoch 4825, Loss: 0.5080630928277969, Final Batch Loss: 0.1394677758216858\n",
      "Epoch 4826, Loss: 0.5646183490753174, Final Batch Loss: 0.1541563719511032\n",
      "Epoch 4827, Loss: 0.6777383238077164, Final Batch Loss: 0.289362370967865\n",
      "Epoch 4828, Loss: 0.7357814759016037, Final Batch Loss: 0.4109489321708679\n",
      "Epoch 4829, Loss: 0.4957343861460686, Final Batch Loss: 0.07853015512228012\n",
      "Epoch 4830, Loss: 0.7675891071557999, Final Batch Loss: 0.4001816213130951\n",
      "Epoch 4831, Loss: 0.49940426647663116, Final Batch Loss: 0.11924347281455994\n",
      "Epoch 4832, Loss: 0.4174622744321823, Final Batch Loss: 0.06989549100399017\n",
      "Epoch 4833, Loss: 0.5525332689285278, Final Batch Loss: 0.16172771155834198\n",
      "Epoch 4834, Loss: 0.5762249380350113, Final Batch Loss: 0.2138686180114746\n",
      "Epoch 4835, Loss: 0.42181286960840225, Final Batch Loss: 0.09344453364610672\n",
      "Epoch 4836, Loss: 0.5289512276649475, Final Batch Loss: 0.1526632457971573\n",
      "Epoch 4837, Loss: 0.4234589859843254, Final Batch Loss: 0.06168099492788315\n",
      "Epoch 4838, Loss: 0.42944712191820145, Final Batch Loss: 0.11955209821462631\n",
      "Epoch 4839, Loss: 0.520908772945404, Final Batch Loss: 0.16533347964286804\n",
      "Epoch 4840, Loss: 0.4134705141186714, Final Batch Loss: 0.07171223312616348\n",
      "Epoch 4841, Loss: 0.9895214140415192, Final Batch Loss: 0.6598501801490784\n",
      "Epoch 4842, Loss: 0.7446721643209457, Final Batch Loss: 0.31796494126319885\n",
      "Epoch 4843, Loss: 0.4996498376131058, Final Batch Loss: 0.13214026391506195\n",
      "Epoch 4844, Loss: 0.4483259618282318, Final Batch Loss: 0.14255954325199127\n",
      "Epoch 4845, Loss: 0.610710620880127, Final Batch Loss: 0.20004983246326447\n",
      "Epoch 4846, Loss: 0.5008947253227234, Final Batch Loss: 0.10336242616176605\n",
      "Epoch 4847, Loss: 0.4351741746068001, Final Batch Loss: 0.08344533294439316\n",
      "Epoch 4848, Loss: 0.5857837349176407, Final Batch Loss: 0.22054408490657806\n",
      "Epoch 4849, Loss: 0.3960586003959179, Final Batch Loss: 0.057416629046201706\n",
      "Epoch 4850, Loss: 0.38187458738684654, Final Batch Loss: 0.05755053088068962\n",
      "Epoch 4851, Loss: 0.5536754429340363, Final Batch Loss: 0.1278471201658249\n",
      "Epoch 4852, Loss: 0.5079707950353622, Final Batch Loss: 0.123530313372612\n",
      "Epoch 4853, Loss: 0.5454475283622742, Final Batch Loss: 0.17294129729270935\n",
      "Epoch 4854, Loss: 0.4561643972992897, Final Batch Loss: 0.1084449514746666\n",
      "Epoch 4855, Loss: 0.4584527835249901, Final Batch Loss: 0.12110205739736557\n",
      "Epoch 4856, Loss: 0.5271661132574081, Final Batch Loss: 0.19170081615447998\n",
      "Epoch 4857, Loss: 0.46857261657714844, Final Batch Loss: 0.1705879122018814\n",
      "Epoch 4858, Loss: 0.3751833587884903, Final Batch Loss: 0.06941358745098114\n",
      "Epoch 4859, Loss: 0.43423257768154144, Final Batch Loss: 0.10250513255596161\n",
      "Epoch 4860, Loss: 0.5020092576742172, Final Batch Loss: 0.15213102102279663\n",
      "Epoch 4861, Loss: 0.46412721276283264, Final Batch Loss: 0.1243850439786911\n",
      "Epoch 4862, Loss: 0.5260498374700546, Final Batch Loss: 0.2053612917661667\n",
      "Epoch 4863, Loss: 0.45445161312818527, Final Batch Loss: 0.11052713543176651\n",
      "Epoch 4864, Loss: 0.49773384630680084, Final Batch Loss: 0.1354169398546219\n",
      "Epoch 4865, Loss: 0.44073110818862915, Final Batch Loss: 0.09517168998718262\n",
      "Epoch 4866, Loss: 0.5491538047790527, Final Batch Loss: 0.24950243532657623\n",
      "Epoch 4867, Loss: 0.5116266012191772, Final Batch Loss: 0.17785866558551788\n",
      "Epoch 4868, Loss: 0.5191570967435837, Final Batch Loss: 0.21257030963897705\n",
      "Epoch 4869, Loss: 0.744408369064331, Final Batch Loss: 0.38675886392593384\n",
      "Epoch 4870, Loss: 0.5253658220171928, Final Batch Loss: 0.10056593269109726\n",
      "Epoch 4871, Loss: 0.710717961192131, Final Batch Loss: 0.38658037781715393\n",
      "Epoch 4872, Loss: 0.479000985622406, Final Batch Loss: 0.15099672973155975\n",
      "Epoch 4873, Loss: 0.4385498911142349, Final Batch Loss: 0.0726187527179718\n",
      "Epoch 4874, Loss: 0.6757477521896362, Final Batch Loss: 0.3266814351081848\n",
      "Epoch 4875, Loss: 0.6479423195123672, Final Batch Loss: 0.3337550759315491\n",
      "Epoch 4876, Loss: 0.5607535094022751, Final Batch Loss: 0.24565215408802032\n",
      "Epoch 4877, Loss: 0.5278544574975967, Final Batch Loss: 0.19395700097084045\n",
      "Epoch 4878, Loss: 0.5422561019659042, Final Batch Loss: 0.16720972955226898\n",
      "Epoch 4879, Loss: 0.37458302453160286, Final Batch Loss: 0.032717231661081314\n",
      "Epoch 4880, Loss: 0.6022903919219971, Final Batch Loss: 0.24700018763542175\n",
      "Epoch 4881, Loss: 0.6561952978372574, Final Batch Loss: 0.33902987837791443\n",
      "Epoch 4882, Loss: 0.46545759588479996, Final Batch Loss: 0.08909914642572403\n",
      "Epoch 4883, Loss: 0.546288788318634, Final Batch Loss: 0.1517203152179718\n",
      "Epoch 4884, Loss: 0.5257395356893539, Final Batch Loss: 0.10378384590148926\n",
      "Epoch 4885, Loss: 0.4184196349233389, Final Batch Loss: 0.026982909068465233\n",
      "Epoch 4886, Loss: 0.6084118187427521, Final Batch Loss: 0.18810799717903137\n",
      "Epoch 4887, Loss: 0.5126880556344986, Final Batch Loss: 0.15983392298221588\n",
      "Epoch 4888, Loss: 0.4416417330503464, Final Batch Loss: 0.11069545149803162\n",
      "Epoch 4889, Loss: 0.6617000848054886, Final Batch Loss: 0.2986437678337097\n",
      "Epoch 4890, Loss: 0.6177691221237183, Final Batch Loss: 0.23547041416168213\n",
      "Epoch 4891, Loss: 0.4847464859485626, Final Batch Loss: 0.09982314705848694\n",
      "Epoch 4892, Loss: 0.7272419780492783, Final Batch Loss: 0.39318251609802246\n",
      "Epoch 4893, Loss: 0.5200052410364151, Final Batch Loss: 0.15432967245578766\n",
      "Epoch 4894, Loss: 0.4371570199728012, Final Batch Loss: 0.042646974325180054\n",
      "Epoch 4895, Loss: 0.4311991538852453, Final Batch Loss: 0.02839878760278225\n",
      "Epoch 4896, Loss: 0.5262778475880623, Final Batch Loss: 0.11979541927576065\n",
      "Epoch 4897, Loss: 0.4586608335375786, Final Batch Loss: 0.09592077881097794\n",
      "Epoch 4898, Loss: 0.445238895714283, Final Batch Loss: 0.1665370762348175\n",
      "Epoch 4899, Loss: 0.40706299245357513, Final Batch Loss: 0.09837913513183594\n",
      "Epoch 4900, Loss: 0.44401611387729645, Final Batch Loss: 0.06571684777736664\n",
      "Epoch 4901, Loss: 0.6731797009706497, Final Batch Loss: 0.41156089305877686\n",
      "Epoch 4902, Loss: 0.6031952649354935, Final Batch Loss: 0.2727551758289337\n",
      "Epoch 4903, Loss: 0.38593703508377075, Final Batch Loss: 0.06958916783332825\n",
      "Epoch 4904, Loss: 0.5325056165456772, Final Batch Loss: 0.1257273554801941\n",
      "Epoch 4905, Loss: 0.38865551352500916, Final Batch Loss: 0.06695003807544708\n",
      "Epoch 4906, Loss: 0.4901819974184036, Final Batch Loss: 0.12561897933483124\n",
      "Epoch 4907, Loss: 0.5875810980796814, Final Batch Loss: 0.14957702159881592\n",
      "Epoch 4908, Loss: 0.3971666768193245, Final Batch Loss: 0.07439415901899338\n",
      "Epoch 4909, Loss: 0.7070545703172684, Final Batch Loss: 0.3567439913749695\n",
      "Epoch 4910, Loss: 0.687972366809845, Final Batch Loss: 0.34336256980895996\n",
      "Epoch 4911, Loss: 0.4724038541316986, Final Batch Loss: 0.11093789339065552\n",
      "Epoch 4912, Loss: 0.7075542658567429, Final Batch Loss: 0.3437102437019348\n",
      "Epoch 4913, Loss: 0.44709113240242004, Final Batch Loss: 0.08734720945358276\n",
      "Epoch 4914, Loss: 0.45318468287587166, Final Batch Loss: 0.060951922088861465\n",
      "Epoch 4915, Loss: 0.4893283024430275, Final Batch Loss: 0.14188361167907715\n",
      "Epoch 4916, Loss: 0.48492448031902313, Final Batch Loss: 0.11876145005226135\n",
      "Epoch 4917, Loss: 0.5455486103892326, Final Batch Loss: 0.11416923254728317\n",
      "Epoch 4918, Loss: 0.41694940626621246, Final Batch Loss: 0.06833860278129578\n",
      "Epoch 4919, Loss: 0.7122288197278976, Final Batch Loss: 0.40634235739707947\n",
      "Epoch 4920, Loss: 0.5856419801712036, Final Batch Loss: 0.23017388582229614\n",
      "Epoch 4921, Loss: 0.3448142781853676, Final Batch Loss: 0.0483347550034523\n",
      "Epoch 4922, Loss: 0.5770543664693832, Final Batch Loss: 0.20538628101348877\n",
      "Epoch 4923, Loss: 0.5936619639396667, Final Batch Loss: 0.19831836223602295\n",
      "Epoch 4924, Loss: 0.39533090591430664, Final Batch Loss: 0.06023617088794708\n",
      "Epoch 4925, Loss: 0.710406094789505, Final Batch Loss: 0.39049091935157776\n",
      "Epoch 4926, Loss: 0.47046617418527603, Final Batch Loss: 0.1166442409157753\n",
      "Epoch 4927, Loss: 0.5961056798696518, Final Batch Loss: 0.2887861132621765\n",
      "Epoch 4928, Loss: 0.4775617942214012, Final Batch Loss: 0.098211370408535\n",
      "Epoch 4929, Loss: 0.5226845741271973, Final Batch Loss: 0.15162238478660583\n",
      "Epoch 4930, Loss: 0.39067453518509865, Final Batch Loss: 0.049635160714387894\n",
      "Epoch 4931, Loss: 0.42583735287189484, Final Batch Loss: 0.07491245865821838\n",
      "Epoch 4932, Loss: 0.5245604515075684, Final Batch Loss: 0.23894324898719788\n",
      "Epoch 4933, Loss: 0.4236997440457344, Final Batch Loss: 0.11231552809476852\n",
      "Epoch 4934, Loss: 0.7523822486400604, Final Batch Loss: 0.3660503923892975\n",
      "Epoch 4935, Loss: 0.4140380993485451, Final Batch Loss: 0.02409534901380539\n",
      "Epoch 4936, Loss: 0.5441556721925735, Final Batch Loss: 0.2109193503856659\n",
      "Epoch 4937, Loss: 0.5631823018193245, Final Batch Loss: 0.2363877296447754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4938, Loss: 0.4067825600504875, Final Batch Loss: 0.07966230064630508\n",
      "Epoch 4939, Loss: 0.3703414648771286, Final Batch Loss: 0.03475138545036316\n",
      "Epoch 4940, Loss: 0.5314431488513947, Final Batch Loss: 0.16947665810585022\n",
      "Epoch 4941, Loss: 0.5673286020755768, Final Batch Loss: 0.27035948634147644\n",
      "Epoch 4942, Loss: 0.7594276368618011, Final Batch Loss: 0.4586440622806549\n",
      "Epoch 4943, Loss: 0.36691969633102417, Final Batch Loss: 0.07721082866191864\n",
      "Epoch 4944, Loss: 0.38941009156405926, Final Batch Loss: 0.02073451690375805\n",
      "Epoch 4945, Loss: 0.3872009553015232, Final Batch Loss: 0.05404757335782051\n",
      "Epoch 4946, Loss: 0.4176507517695427, Final Batch Loss: 0.1159445121884346\n",
      "Epoch 4947, Loss: 0.5686311274766922, Final Batch Loss: 0.22443921864032745\n",
      "Epoch 4948, Loss: 0.44788189977407455, Final Batch Loss: 0.04361575096845627\n",
      "Epoch 4949, Loss: 0.5025646686553955, Final Batch Loss: 0.16353854537010193\n",
      "Epoch 4950, Loss: 0.49135328084230423, Final Batch Loss: 0.11711966246366501\n",
      "Epoch 4951, Loss: 0.7672115713357925, Final Batch Loss: 0.4235813617706299\n",
      "Epoch 4952, Loss: 0.389051117002964, Final Batch Loss: 0.09107182174921036\n",
      "Epoch 4953, Loss: 0.5884930938482285, Final Batch Loss: 0.2549227774143219\n",
      "Epoch 4954, Loss: 0.4846944808959961, Final Batch Loss: 0.15880604088306427\n",
      "Epoch 4955, Loss: 0.43158695846796036, Final Batch Loss: 0.06561961024999619\n",
      "Epoch 4956, Loss: 0.4566978979855776, Final Batch Loss: 0.023840533569455147\n",
      "Epoch 4957, Loss: 0.5200973972678185, Final Batch Loss: 0.0936364158987999\n",
      "Epoch 4958, Loss: 0.6562062203884125, Final Batch Loss: 0.28540706634521484\n",
      "Epoch 4959, Loss: 0.4794730544090271, Final Batch Loss: 0.19377489387989044\n",
      "Epoch 4960, Loss: 0.4197128266096115, Final Batch Loss: 0.1465451866388321\n",
      "Epoch 4961, Loss: 0.4682561308145523, Final Batch Loss: 0.14843110740184784\n",
      "Epoch 4962, Loss: 0.3547701071947813, Final Batch Loss: 0.018959445878863335\n",
      "Epoch 4963, Loss: 0.576612576842308, Final Batch Loss: 0.22203852236270905\n",
      "Epoch 4964, Loss: 0.4523078054189682, Final Batch Loss: 0.11186021566390991\n",
      "Epoch 4965, Loss: 0.4792177230119705, Final Batch Loss: 0.16467750072479248\n",
      "Epoch 4966, Loss: 0.4556990787386894, Final Batch Loss: 0.1141081228852272\n",
      "Epoch 4967, Loss: 0.43011339008808136, Final Batch Loss: 0.1353563666343689\n",
      "Epoch 4968, Loss: 0.4241706281900406, Final Batch Loss: 0.06822606921195984\n",
      "Epoch 4969, Loss: 0.5453996509313583, Final Batch Loss: 0.23588669300079346\n",
      "Epoch 4970, Loss: 0.4458993971347809, Final Batch Loss: 0.08354765176773071\n",
      "Epoch 4971, Loss: 0.5950769186019897, Final Batch Loss: 0.23460209369659424\n",
      "Epoch 4972, Loss: 0.5716084390878677, Final Batch Loss: 0.2054433375597\n",
      "Epoch 4973, Loss: 0.4014880731701851, Final Batch Loss: 0.08874934166669846\n",
      "Epoch 4974, Loss: 0.6307450085878372, Final Batch Loss: 0.2600364089012146\n",
      "Epoch 4975, Loss: 0.75033800303936, Final Batch Loss: 0.4808627665042877\n",
      "Epoch 4976, Loss: 0.6611621975898743, Final Batch Loss: 0.36620983481407166\n",
      "Epoch 4977, Loss: 0.3704891465604305, Final Batch Loss: 0.049145739525556564\n",
      "Epoch 4978, Loss: 0.7330059558153152, Final Batch Loss: 0.43042778968811035\n",
      "Epoch 4979, Loss: 0.7869760394096375, Final Batch Loss: 0.35352328419685364\n",
      "Epoch 4980, Loss: 0.5477451533079147, Final Batch Loss: 0.2175941914319992\n",
      "Epoch 4981, Loss: 0.44294802099466324, Final Batch Loss: 0.10477019846439362\n",
      "Epoch 4982, Loss: 0.4168361909687519, Final Batch Loss: 0.03739911690354347\n",
      "Epoch 4983, Loss: 0.4210584834218025, Final Batch Loss: 0.09343502670526505\n",
      "Epoch 4984, Loss: 0.6780272871255875, Final Batch Loss: 0.27025464177131653\n",
      "Epoch 4985, Loss: 0.46581917256116867, Final Batch Loss: 0.08247902244329453\n",
      "Epoch 4986, Loss: 0.45240288227796555, Final Batch Loss: 0.08817983418703079\n",
      "Epoch 4987, Loss: 0.7191329896450043, Final Batch Loss: 0.3680666983127594\n",
      "Epoch 4988, Loss: 0.5770601481199265, Final Batch Loss: 0.2526695728302002\n",
      "Epoch 4989, Loss: 0.6141529157757759, Final Batch Loss: 0.29698842763900757\n",
      "Epoch 4990, Loss: 0.9639289379119873, Final Batch Loss: 0.5595314502716064\n",
      "Epoch 4991, Loss: 0.5290963798761368, Final Batch Loss: 0.1803630143404007\n",
      "Epoch 4992, Loss: 0.3647169768810272, Final Batch Loss: 0.07097048312425613\n",
      "Epoch 4993, Loss: 0.5336888134479523, Final Batch Loss: 0.16773243248462677\n",
      "Epoch 4994, Loss: 0.4765655919909477, Final Batch Loss: 0.08201629668474197\n",
      "Epoch 4995, Loss: 0.4690244309604168, Final Batch Loss: 0.05944574996829033\n",
      "Epoch 4996, Loss: 0.4170799031853676, Final Batch Loss: 0.07358702272176743\n",
      "Epoch 4997, Loss: 0.5616266131401062, Final Batch Loss: 0.2124508023262024\n",
      "Epoch 4998, Loss: 0.8392482399940491, Final Batch Loss: 0.49694353342056274\n",
      "Epoch 4999, Loss: 0.6494358330965042, Final Batch Loss: 0.21797198057174683\n",
      "Epoch 5000, Loss: 0.9595853686332703, Final Batch Loss: 0.6067986488342285\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 12  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 12  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  8  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  5  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0 11  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  6  0  0  2]\n",
      " [ 0  0  0  0  0  0  0  0  0 12  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        19\n",
      "           1    1.00000   1.00000   1.00000        12\n",
      "           2    1.00000   1.00000   1.00000        12\n",
      "           3    1.00000   1.00000   1.00000        12\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   0.83333   0.90909         6\n",
      "           6    1.00000   1.00000   1.00000        11\n",
      "           7    1.00000   1.00000   1.00000        11\n",
      "           8    0.85714   0.75000   0.80000         8\n",
      "           9    1.00000   1.00000   1.00000        12\n",
      "          10    1.00000   1.00000   1.00000         8\n",
      "          11    0.84615   1.00000   0.91667        11\n",
      "\n",
      "    accuracy                        0.97692       130\n",
      "   macro avg    0.97527   0.96528   0.96881       130\n",
      "weighted avg    0.97819   0.97692   0.97645       130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_1 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_1 = np.zeros(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_2 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_2 = np.ones(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_3 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_3 = np.ones(n_samples) + 1\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_4 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_4 = np.ones(n_samples) + 2\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_5 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_5 = np.ones(n_samples) + 3\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_6 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_6 = np.ones(n_samples) + 4\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_7 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_7 = np.ones(n_samples) + 5\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_8 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_8 = np.ones(n_samples) + 6\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_9 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_9 = np.ones(n_samples) + 7\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U3A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_10 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_10 = np.ones(n_samples) + 8\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U3A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_11 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_11 = np.ones(n_samples) + 9\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U3A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_12 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_12 = np.ones(n_samples) + 10\n",
    "\n",
    "fake_features = np.concatenate((fake_features_1, fake_features_2, fake_features_3, fake_features_4, fake_features_5, fake_features_6,\n",
    "                         fake_features_7, fake_features_8, fake_features_9, fake_features_10, fake_features_11, fake_features_12))\n",
    "fake_labels = np.concatenate((y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9, y_10, y_11, y_12))\n",
    "\n",
    "fake_features = torch.Tensor(fake_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  0  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 1 19  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  0  6  0  0  5  0  0  0]\n",
      " [ 0  0  0 20  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 15  0  2  0  0  0  3  0]\n",
      " [ 0  0  3  0  0  8  0  0  8  0  0  1]\n",
      " [ 0  0  0  0  0  0 20  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 20  0  0  0  0]\n",
      " [ 0  0  0  0  0  4  0  0 10  4  0  2]\n",
      " [ 0  0  0  0  1  0  7  0  0  8  4  0]\n",
      " [ 0  0  0  0  2  0  0  0  0  0 18  0]\n",
      " [ 0  0  0  0  0 14  0  0  2  0  0  4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    0.95000   0.95000   0.95000        20\n",
      "         1.0    1.00000   0.95000   0.97436        20\n",
      "         2.0    0.75000   0.45000   0.56250        20\n",
      "         3.0    1.00000   1.00000   1.00000        20\n",
      "         4.0    0.83333   0.75000   0.78947        20\n",
      "         5.0    0.25000   0.40000   0.30769        20\n",
      "         6.0    0.68966   1.00000   0.81633        20\n",
      "         7.0    1.00000   1.00000   1.00000        20\n",
      "         8.0    0.40000   0.50000   0.44444        20\n",
      "         9.0    0.61538   0.40000   0.48485        20\n",
      "        10.0    0.72000   0.90000   0.80000        20\n",
      "        11.0    0.57143   0.20000   0.29630        20\n",
      "\n",
      "    accuracy                        0.70833       240\n",
      "   macro avg    0.73165   0.70833   0.70216       240\n",
      "weighted avg    0.73165   0.70833   0.70216       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
