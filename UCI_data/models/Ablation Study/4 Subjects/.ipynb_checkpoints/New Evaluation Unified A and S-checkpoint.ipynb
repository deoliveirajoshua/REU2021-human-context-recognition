{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '141 tBodyGyro-iqr()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '434 fBodyGyro-max()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 30),\n",
    "            classifier_block(30, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            nn.Linear(15, 12)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def get_act_matrix(batch_size, a_dim):\n",
    "    indexes = np.random.randint(a_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((len(indexes), indexes.max()+1))\n",
    "    one_hot[np.arange(len(indexes)),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "    \n",
    "def get_usr_matrix(batch_size, u_dim):\n",
    "    indexes = np.random.randint(u_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((indexes.size, indexes.max()+1))\n",
    "    one_hot[np.arange(indexes.size),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Real Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "    \n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_10 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_11 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_12 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9, X_10, X_11, X_12))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9) + [9] * len(X_10) + [10] * len(X_11) + [11] * len(X_12)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5, 7]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.4565019607543945, Final Batch Loss: 2.4861161708831787\n",
      "Epoch 2, Loss: 7.389077186584473, Final Batch Loss: 2.4213759899139404\n",
      "Epoch 3, Loss: 7.438206195831299, Final Batch Loss: 2.476147413253784\n",
      "Epoch 4, Loss: 7.425340890884399, Final Batch Loss: 2.4706366062164307\n",
      "Epoch 5, Loss: 7.407440423965454, Final Batch Loss: 2.455002784729004\n",
      "Epoch 6, Loss: 7.455641031265259, Final Batch Loss: 2.507023572921753\n",
      "Epoch 7, Loss: 7.404532432556152, Final Batch Loss: 2.455768346786499\n",
      "Epoch 8, Loss: 7.409439325332642, Final Batch Loss: 2.464600086212158\n",
      "Epoch 9, Loss: 7.442714214324951, Final Batch Loss: 2.5060012340545654\n",
      "Epoch 10, Loss: 7.377971410751343, Final Batch Loss: 2.4446983337402344\n",
      "Epoch 11, Loss: 7.463738679885864, Final Batch Loss: 2.538994312286377\n",
      "Epoch 12, Loss: 7.381815671920776, Final Batch Loss: 2.4655027389526367\n",
      "Epoch 13, Loss: 7.37760591506958, Final Batch Loss: 2.46934175491333\n",
      "Epoch 14, Loss: 7.332941293716431, Final Batch Loss: 2.4351930618286133\n",
      "Epoch 15, Loss: 7.34438681602478, Final Batch Loss: 2.458721399307251\n",
      "Epoch 16, Loss: 7.240749359130859, Final Batch Loss: 2.364915370941162\n",
      "Epoch 17, Loss: 7.303565263748169, Final Batch Loss: 2.4610092639923096\n",
      "Epoch 18, Loss: 7.226880073547363, Final Batch Loss: 2.4018585681915283\n",
      "Epoch 19, Loss: 7.188782691955566, Final Batch Loss: 2.3875365257263184\n",
      "Epoch 20, Loss: 7.205249547958374, Final Batch Loss: 2.4424026012420654\n",
      "Epoch 21, Loss: 7.027640104293823, Final Batch Loss: 2.2794008255004883\n",
      "Epoch 22, Loss: 7.109628677368164, Final Batch Loss: 2.43454647064209\n",
      "Epoch 23, Loss: 6.919240236282349, Final Batch Loss: 2.2775344848632812\n",
      "Epoch 24, Loss: 6.965949535369873, Final Batch Loss: 2.3954498767852783\n",
      "Epoch 25, Loss: 6.820394515991211, Final Batch Loss: 2.314525842666626\n",
      "Epoch 26, Loss: 6.589109897613525, Final Batch Loss: 2.1787774562835693\n",
      "Epoch 27, Loss: 6.698931694030762, Final Batch Loss: 2.3502261638641357\n",
      "Epoch 28, Loss: 6.472294330596924, Final Batch Loss: 2.2121894359588623\n",
      "Epoch 29, Loss: 6.150477766990662, Final Batch Loss: 1.9526714086532593\n",
      "Epoch 30, Loss: 6.148334741592407, Final Batch Loss: 2.0596864223480225\n",
      "Epoch 31, Loss: 5.95683753490448, Final Batch Loss: 1.9144366979599\n",
      "Epoch 32, Loss: 5.834807753562927, Final Batch Loss: 1.8686021566390991\n",
      "Epoch 33, Loss: 5.910413861274719, Final Batch Loss: 2.025761842727661\n",
      "Epoch 34, Loss: 5.61503529548645, Final Batch Loss: 1.7857598066329956\n",
      "Epoch 35, Loss: 5.425859689712524, Final Batch Loss: 1.653609275817871\n",
      "Epoch 36, Loss: 5.535136580467224, Final Batch Loss: 1.8240524530410767\n",
      "Epoch 37, Loss: 5.332509636878967, Final Batch Loss: 1.6988184452056885\n",
      "Epoch 38, Loss: 5.52235734462738, Final Batch Loss: 1.8944616317749023\n",
      "Epoch 39, Loss: 5.333977937698364, Final Batch Loss: 1.782141089439392\n",
      "Epoch 40, Loss: 5.654858827590942, Final Batch Loss: 2.2187178134918213\n",
      "Epoch 41, Loss: 5.322402119636536, Final Batch Loss: 1.8648946285247803\n",
      "Epoch 42, Loss: 5.064648985862732, Final Batch Loss: 1.6324447393417358\n",
      "Epoch 43, Loss: 5.164356112480164, Final Batch Loss: 1.662936806678772\n",
      "Epoch 44, Loss: 5.041540145874023, Final Batch Loss: 1.6563942432403564\n",
      "Epoch 45, Loss: 5.344210386276245, Final Batch Loss: 2.0040595531463623\n",
      "Epoch 46, Loss: 4.993752717971802, Final Batch Loss: 1.7377026081085205\n",
      "Epoch 47, Loss: 4.92566978931427, Final Batch Loss: 1.5797888040542603\n",
      "Epoch 48, Loss: 4.873543977737427, Final Batch Loss: 1.584855079650879\n",
      "Epoch 49, Loss: 4.828720927238464, Final Batch Loss: 1.5727283954620361\n",
      "Epoch 50, Loss: 4.709892749786377, Final Batch Loss: 1.5042058229446411\n",
      "Epoch 51, Loss: 4.860196590423584, Final Batch Loss: 1.6681199073791504\n",
      "Epoch 52, Loss: 4.967241644859314, Final Batch Loss: 1.7838668823242188\n",
      "Epoch 53, Loss: 4.790434837341309, Final Batch Loss: 1.717654824256897\n",
      "Epoch 54, Loss: 4.580135464668274, Final Batch Loss: 1.4909051656723022\n",
      "Epoch 55, Loss: 4.650963544845581, Final Batch Loss: 1.5334551334381104\n",
      "Epoch 56, Loss: 4.75628399848938, Final Batch Loss: 1.5870757102966309\n",
      "Epoch 57, Loss: 4.7373634576797485, Final Batch Loss: 1.6751121282577515\n",
      "Epoch 58, Loss: 4.378223419189453, Final Batch Loss: 1.3653417825698853\n",
      "Epoch 59, Loss: 4.311815142631531, Final Batch Loss: 1.3806523084640503\n",
      "Epoch 60, Loss: 4.855854511260986, Final Batch Loss: 1.8698439598083496\n",
      "Epoch 61, Loss: 4.530156135559082, Final Batch Loss: 1.5644994974136353\n",
      "Epoch 62, Loss: 4.458091735839844, Final Batch Loss: 1.5237981081008911\n",
      "Epoch 63, Loss: 4.65340781211853, Final Batch Loss: 1.7283538579940796\n",
      "Epoch 64, Loss: 4.412410259246826, Final Batch Loss: 1.4909720420837402\n",
      "Epoch 65, Loss: 4.4108275175094604, Final Batch Loss: 1.52621328830719\n",
      "Epoch 66, Loss: 4.587570905685425, Final Batch Loss: 1.700154185295105\n",
      "Epoch 67, Loss: 4.290881991386414, Final Batch Loss: 1.414049506187439\n",
      "Epoch 68, Loss: 4.185475587844849, Final Batch Loss: 1.2811622619628906\n",
      "Epoch 69, Loss: 4.435303568840027, Final Batch Loss: 1.5418000221252441\n",
      "Epoch 70, Loss: 4.225543856620789, Final Batch Loss: 1.3098158836364746\n",
      "Epoch 71, Loss: 4.3924548625946045, Final Batch Loss: 1.496564269065857\n",
      "Epoch 72, Loss: 4.245163440704346, Final Batch Loss: 1.3377580642700195\n",
      "Epoch 73, Loss: 4.130647540092468, Final Batch Loss: 1.323757529258728\n",
      "Epoch 74, Loss: 4.121723651885986, Final Batch Loss: 1.3549001216888428\n",
      "Epoch 75, Loss: 4.172085762023926, Final Batch Loss: 1.3625696897506714\n",
      "Epoch 76, Loss: 4.195729374885559, Final Batch Loss: 1.3966869115829468\n",
      "Epoch 77, Loss: 3.9113441705703735, Final Batch Loss: 1.1964865922927856\n",
      "Epoch 78, Loss: 4.082396984100342, Final Batch Loss: 1.3350763320922852\n",
      "Epoch 79, Loss: 4.0044249296188354, Final Batch Loss: 1.331491231918335\n",
      "Epoch 80, Loss: 3.939174771308899, Final Batch Loss: 1.2480190992355347\n",
      "Epoch 81, Loss: 4.1333736181259155, Final Batch Loss: 1.498508095741272\n",
      "Epoch 82, Loss: 4.129596590995789, Final Batch Loss: 1.4844903945922852\n",
      "Epoch 83, Loss: 3.9610438346862793, Final Batch Loss: 1.3180946111679077\n",
      "Epoch 84, Loss: 3.755887031555176, Final Batch Loss: 1.185336709022522\n",
      "Epoch 85, Loss: 4.052422761917114, Final Batch Loss: 1.469641923904419\n",
      "Epoch 86, Loss: 3.9909669160842896, Final Batch Loss: 1.4358059167861938\n",
      "Epoch 87, Loss: 4.278068423271179, Final Batch Loss: 1.6762367486953735\n",
      "Epoch 88, Loss: 3.722596287727356, Final Batch Loss: 1.160396933555603\n",
      "Epoch 89, Loss: 3.706748366355896, Final Batch Loss: 1.1553726196289062\n",
      "Epoch 90, Loss: 3.6953970193862915, Final Batch Loss: 1.1377060413360596\n",
      "Epoch 91, Loss: 3.848762273788452, Final Batch Loss: 1.3185664415359497\n",
      "Epoch 92, Loss: 3.783184289932251, Final Batch Loss: 1.3130100965499878\n",
      "Epoch 93, Loss: 3.6380871534347534, Final Batch Loss: 1.1955913305282593\n",
      "Epoch 94, Loss: 3.884889245033264, Final Batch Loss: 1.3607168197631836\n",
      "Epoch 95, Loss: 3.691617250442505, Final Batch Loss: 1.1560165882110596\n",
      "Epoch 96, Loss: 3.616379976272583, Final Batch Loss: 1.2490991353988647\n",
      "Epoch 97, Loss: 3.7351434230804443, Final Batch Loss: 1.2760611772537231\n",
      "Epoch 98, Loss: 3.58031964302063, Final Batch Loss: 1.0847841501235962\n",
      "Epoch 99, Loss: 3.430601954460144, Final Batch Loss: 1.0067516565322876\n",
      "Epoch 100, Loss: 3.578144907951355, Final Batch Loss: 1.158876657485962\n",
      "Epoch 101, Loss: 3.6929075717926025, Final Batch Loss: 1.2868280410766602\n",
      "Epoch 102, Loss: 4.011611580848694, Final Batch Loss: 1.55342435836792\n",
      "Epoch 103, Loss: 3.5410913228988647, Final Batch Loss: 1.1425023078918457\n",
      "Epoch 104, Loss: 3.4247923493385315, Final Batch Loss: 0.999772846698761\n",
      "Epoch 105, Loss: 3.5877199172973633, Final Batch Loss: 1.1438138484954834\n",
      "Epoch 106, Loss: 3.767219662666321, Final Batch Loss: 1.3469889163970947\n",
      "Epoch 107, Loss: 3.7253997325897217, Final Batch Loss: 1.3891398906707764\n",
      "Epoch 108, Loss: 3.428023099899292, Final Batch Loss: 1.0007754564285278\n",
      "Epoch 109, Loss: 3.6257232427597046, Final Batch Loss: 1.2736893892288208\n",
      "Epoch 110, Loss: 3.601676344871521, Final Batch Loss: 1.3220406770706177\n",
      "Epoch 111, Loss: 3.7529098987579346, Final Batch Loss: 1.4550542831420898\n",
      "Epoch 112, Loss: 3.6956028938293457, Final Batch Loss: 1.331114649772644\n",
      "Epoch 113, Loss: 3.3194020986557007, Final Batch Loss: 0.9963116645812988\n",
      "Epoch 114, Loss: 3.5488189458847046, Final Batch Loss: 1.1936872005462646\n",
      "Epoch 115, Loss: 3.4722272157669067, Final Batch Loss: 1.1596314907073975\n",
      "Epoch 116, Loss: 3.4468531608581543, Final Batch Loss: 1.1829544305801392\n",
      "Epoch 117, Loss: 3.4284396171569824, Final Batch Loss: 1.1775743961334229\n",
      "Epoch 118, Loss: 3.280392587184906, Final Batch Loss: 0.9892894625663757\n",
      "Epoch 119, Loss: 3.3350037336349487, Final Batch Loss: 1.0917588472366333\n",
      "Epoch 120, Loss: 3.5775532722473145, Final Batch Loss: 1.3210508823394775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121, Loss: 3.4227999448776245, Final Batch Loss: 1.2566736936569214\n",
      "Epoch 122, Loss: 3.445485472679138, Final Batch Loss: 1.2329602241516113\n",
      "Epoch 123, Loss: 3.1802684664726257, Final Batch Loss: 0.9533560872077942\n",
      "Epoch 124, Loss: 3.2421780824661255, Final Batch Loss: 0.9654021263122559\n",
      "Epoch 125, Loss: 3.391984462738037, Final Batch Loss: 1.1992532014846802\n",
      "Epoch 126, Loss: 3.08952397108078, Final Batch Loss: 0.9206394553184509\n",
      "Epoch 127, Loss: 3.0704381465911865, Final Batch Loss: 0.9501303434371948\n",
      "Epoch 128, Loss: 3.337642788887024, Final Batch Loss: 1.1260581016540527\n",
      "Epoch 129, Loss: 3.213936448097229, Final Batch Loss: 1.022255301475525\n",
      "Epoch 130, Loss: 3.31439745426178, Final Batch Loss: 1.1407952308654785\n",
      "Epoch 131, Loss: 3.3022472858428955, Final Batch Loss: 1.132822036743164\n",
      "Epoch 132, Loss: 3.1774948835372925, Final Batch Loss: 0.9923214912414551\n",
      "Epoch 133, Loss: 3.4500350952148438, Final Batch Loss: 1.2308024168014526\n",
      "Epoch 134, Loss: 3.1075358390808105, Final Batch Loss: 1.0709452629089355\n",
      "Epoch 135, Loss: 3.1036893725395203, Final Batch Loss: 0.9990382790565491\n",
      "Epoch 136, Loss: 3.254343032836914, Final Batch Loss: 1.1102932691574097\n",
      "Epoch 137, Loss: 3.157628655433655, Final Batch Loss: 1.0556784868240356\n",
      "Epoch 138, Loss: 2.9944438338279724, Final Batch Loss: 0.8840619921684265\n",
      "Epoch 139, Loss: 3.1283968687057495, Final Batch Loss: 1.0500969886779785\n",
      "Epoch 140, Loss: 3.0964956879615784, Final Batch Loss: 0.9521284699440002\n",
      "Epoch 141, Loss: 3.3477178812026978, Final Batch Loss: 1.2432537078857422\n",
      "Epoch 142, Loss: 2.975111484527588, Final Batch Loss: 0.8749401569366455\n",
      "Epoch 143, Loss: 3.241259455680847, Final Batch Loss: 1.0749012231826782\n",
      "Epoch 144, Loss: 2.9749233722686768, Final Batch Loss: 0.9267560243606567\n",
      "Epoch 145, Loss: 2.8411874175071716, Final Batch Loss: 0.8108532428741455\n",
      "Epoch 146, Loss: 2.86143559217453, Final Batch Loss: 0.7301148772239685\n",
      "Epoch 147, Loss: 2.959578037261963, Final Batch Loss: 0.8923306465148926\n",
      "Epoch 148, Loss: 3.077297270298004, Final Batch Loss: 0.9720398783683777\n",
      "Epoch 149, Loss: 3.1698367595672607, Final Batch Loss: 1.1498149633407593\n",
      "Epoch 150, Loss: 3.152739644050598, Final Batch Loss: 1.0785231590270996\n",
      "Epoch 151, Loss: 3.1660606265068054, Final Batch Loss: 1.152137279510498\n",
      "Epoch 152, Loss: 3.1036547422409058, Final Batch Loss: 1.081841230392456\n",
      "Epoch 153, Loss: 2.9700927138328552, Final Batch Loss: 0.8967697024345398\n",
      "Epoch 154, Loss: 2.839946210384369, Final Batch Loss: 0.8806636929512024\n",
      "Epoch 155, Loss: 3.007508337497711, Final Batch Loss: 0.9449681639671326\n",
      "Epoch 156, Loss: 3.3334803581237793, Final Batch Loss: 1.2875856161117554\n",
      "Epoch 157, Loss: 3.031272768974304, Final Batch Loss: 1.0934648513793945\n",
      "Epoch 158, Loss: 2.948615550994873, Final Batch Loss: 0.9310550689697266\n",
      "Epoch 159, Loss: 2.896297872066498, Final Batch Loss: 0.9451850056648254\n",
      "Epoch 160, Loss: 2.7077234387397766, Final Batch Loss: 0.701511800289154\n",
      "Epoch 161, Loss: 2.7722837924957275, Final Batch Loss: 0.8837016224861145\n",
      "Epoch 162, Loss: 3.0226266980171204, Final Batch Loss: 1.0939124822616577\n",
      "Epoch 163, Loss: 2.9469682574272156, Final Batch Loss: 1.003074049949646\n",
      "Epoch 164, Loss: 2.9474772214889526, Final Batch Loss: 0.9284752011299133\n",
      "Epoch 165, Loss: 2.6910168528556824, Final Batch Loss: 0.7737480401992798\n",
      "Epoch 166, Loss: 3.002501428127289, Final Batch Loss: 0.9610012173652649\n",
      "Epoch 167, Loss: 3.383144199848175, Final Batch Loss: 1.3854964971542358\n",
      "Epoch 168, Loss: 3.0163450837135315, Final Batch Loss: 1.1213767528533936\n",
      "Epoch 169, Loss: 3.17600017786026, Final Batch Loss: 1.267914891242981\n",
      "Epoch 170, Loss: 2.9736493825912476, Final Batch Loss: 1.0282901525497437\n",
      "Epoch 171, Loss: 2.8641087412834167, Final Batch Loss: 0.8872596621513367\n",
      "Epoch 172, Loss: 2.9072601199150085, Final Batch Loss: 1.0385364294052124\n",
      "Epoch 173, Loss: 2.836052417755127, Final Batch Loss: 0.972123920917511\n",
      "Epoch 174, Loss: 2.7188947796821594, Final Batch Loss: 0.701951801776886\n",
      "Epoch 175, Loss: 2.7264317870140076, Final Batch Loss: 0.7753787636756897\n",
      "Epoch 176, Loss: 2.8885403275489807, Final Batch Loss: 0.9456269145011902\n",
      "Epoch 177, Loss: 2.8396249413490295, Final Batch Loss: 0.8934280872344971\n",
      "Epoch 178, Loss: 2.8995174765586853, Final Batch Loss: 1.0793023109436035\n",
      "Epoch 179, Loss: 2.8968339562416077, Final Batch Loss: 1.0122315883636475\n",
      "Epoch 180, Loss: 2.7638601064682007, Final Batch Loss: 0.9644144177436829\n",
      "Epoch 181, Loss: 2.655015289783478, Final Batch Loss: 0.8663865923881531\n",
      "Epoch 182, Loss: 2.6071730852127075, Final Batch Loss: 0.7049424648284912\n",
      "Epoch 183, Loss: 2.6563042998313904, Final Batch Loss: 0.8083667755126953\n",
      "Epoch 184, Loss: 2.6047475337982178, Final Batch Loss: 0.7431524395942688\n",
      "Epoch 185, Loss: 2.699614942073822, Final Batch Loss: 0.8371674418449402\n",
      "Epoch 186, Loss: 2.8685524463653564, Final Batch Loss: 0.9602826833724976\n",
      "Epoch 187, Loss: 3.129856824874878, Final Batch Loss: 1.2652442455291748\n",
      "Epoch 188, Loss: 2.7087737917900085, Final Batch Loss: 0.9180249571800232\n",
      "Epoch 189, Loss: 2.551625430583954, Final Batch Loss: 0.7268211841583252\n",
      "Epoch 190, Loss: 2.807869851589203, Final Batch Loss: 0.9765713214874268\n",
      "Epoch 191, Loss: 2.519052565097809, Final Batch Loss: 0.6816651225090027\n",
      "Epoch 192, Loss: 2.8701881170272827, Final Batch Loss: 1.0236691236495972\n",
      "Epoch 193, Loss: 2.70705246925354, Final Batch Loss: 0.8752282857894897\n",
      "Epoch 194, Loss: 2.739540755748749, Final Batch Loss: 0.9720749258995056\n",
      "Epoch 195, Loss: 2.5528566241264343, Final Batch Loss: 0.8033295273780823\n",
      "Epoch 196, Loss: 2.509690999984741, Final Batch Loss: 0.7101460099220276\n",
      "Epoch 197, Loss: 2.70116525888443, Final Batch Loss: 0.8584315776824951\n",
      "Epoch 198, Loss: 2.738808333873749, Final Batch Loss: 0.9836236238479614\n",
      "Epoch 199, Loss: 2.5196120142936707, Final Batch Loss: 0.6406807899475098\n",
      "Epoch 200, Loss: 2.617469549179077, Final Batch Loss: 0.831525444984436\n",
      "Epoch 201, Loss: 2.5656603574752808, Final Batch Loss: 0.701033353805542\n",
      "Epoch 202, Loss: 2.440477132797241, Final Batch Loss: 0.6619595289230347\n",
      "Epoch 203, Loss: 2.8414202332496643, Final Batch Loss: 1.0362064838409424\n",
      "Epoch 204, Loss: 2.6197460889816284, Final Batch Loss: 0.8924088478088379\n",
      "Epoch 205, Loss: 2.710617184638977, Final Batch Loss: 0.9935715198516846\n",
      "Epoch 206, Loss: 2.6924237608909607, Final Batch Loss: 0.8583868741989136\n",
      "Epoch 207, Loss: 2.832919120788574, Final Batch Loss: 0.9735773801803589\n",
      "Epoch 208, Loss: 2.6027596592903137, Final Batch Loss: 0.8420244455337524\n",
      "Epoch 209, Loss: 2.6688050031661987, Final Batch Loss: 0.8603206276893616\n",
      "Epoch 210, Loss: 2.4593313336372375, Final Batch Loss: 0.6430264711380005\n",
      "Epoch 211, Loss: 2.607875943183899, Final Batch Loss: 0.8490002155303955\n",
      "Epoch 212, Loss: 2.6060704588890076, Final Batch Loss: 0.8584302663803101\n",
      "Epoch 213, Loss: 2.54341858625412, Final Batch Loss: 0.7614264488220215\n",
      "Epoch 214, Loss: 2.499014377593994, Final Batch Loss: 0.7651655077934265\n",
      "Epoch 215, Loss: 2.437128007411957, Final Batch Loss: 0.606160044670105\n",
      "Epoch 216, Loss: 2.447917938232422, Final Batch Loss: 0.7299403548240662\n",
      "Epoch 217, Loss: 2.5513634085655212, Final Batch Loss: 0.799949049949646\n",
      "Epoch 218, Loss: 2.4276713132858276, Final Batch Loss: 0.682335376739502\n",
      "Epoch 219, Loss: 2.374662160873413, Final Batch Loss: 0.6518720388412476\n",
      "Epoch 220, Loss: 2.7745184302330017, Final Batch Loss: 1.1404845714569092\n",
      "Epoch 221, Loss: 2.8921077847480774, Final Batch Loss: 1.0778411626815796\n",
      "Epoch 222, Loss: 2.7088833451271057, Final Batch Loss: 0.9722545742988586\n",
      "Epoch 223, Loss: 2.6056647300720215, Final Batch Loss: 0.7378256916999817\n",
      "Epoch 224, Loss: 2.6490489840507507, Final Batch Loss: 0.9831127524375916\n",
      "Epoch 225, Loss: 2.4990204572677612, Final Batch Loss: 0.7321868538856506\n",
      "Epoch 226, Loss: 2.228660762310028, Final Batch Loss: 0.5428102612495422\n",
      "Epoch 227, Loss: 2.6448525190353394, Final Batch Loss: 0.874202311038971\n",
      "Epoch 228, Loss: 2.677807092666626, Final Batch Loss: 1.0204585790634155\n",
      "Epoch 229, Loss: 2.4839738607406616, Final Batch Loss: 0.7365131378173828\n",
      "Epoch 230, Loss: 2.83997905254364, Final Batch Loss: 1.2091010808944702\n",
      "Epoch 231, Loss: 2.1853121519088745, Final Batch Loss: 0.5311527848243713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232, Loss: 2.794485032558441, Final Batch Loss: 1.0647879838943481\n",
      "Epoch 233, Loss: 2.5321959853172302, Final Batch Loss: 0.786387026309967\n",
      "Epoch 234, Loss: 1.9754333198070526, Final Batch Loss: 0.3347020447254181\n",
      "Epoch 235, Loss: 2.54498028755188, Final Batch Loss: 0.8374310731887817\n",
      "Epoch 236, Loss: 2.537276029586792, Final Batch Loss: 0.9168505668640137\n",
      "Epoch 237, Loss: 2.389365315437317, Final Batch Loss: 0.6080032587051392\n",
      "Epoch 238, Loss: 2.187043458223343, Final Batch Loss: 0.4386216104030609\n",
      "Epoch 239, Loss: 2.3405430912971497, Final Batch Loss: 0.7358778119087219\n",
      "Epoch 240, Loss: 2.3713969588279724, Final Batch Loss: 0.7175435423851013\n",
      "Epoch 241, Loss: 2.6247350573539734, Final Batch Loss: 0.8955686688423157\n",
      "Epoch 242, Loss: 2.455954134464264, Final Batch Loss: 0.7595963478088379\n",
      "Epoch 243, Loss: 2.3565760254859924, Final Batch Loss: 0.739057719707489\n",
      "Epoch 244, Loss: 2.495826303958893, Final Batch Loss: 0.7519140839576721\n",
      "Epoch 245, Loss: 2.4173234701156616, Final Batch Loss: 0.7014845609664917\n",
      "Epoch 246, Loss: 2.300680458545685, Final Batch Loss: 0.6982264518737793\n",
      "Epoch 247, Loss: 2.685292601585388, Final Batch Loss: 1.113682746887207\n",
      "Epoch 248, Loss: 2.24582302570343, Final Batch Loss: 0.6168826222419739\n",
      "Epoch 249, Loss: 2.2851526737213135, Final Batch Loss: 0.6851014494895935\n",
      "Epoch 250, Loss: 2.7542526721954346, Final Batch Loss: 1.1281930208206177\n",
      "Epoch 251, Loss: 2.677304267883301, Final Batch Loss: 1.047437310218811\n",
      "Epoch 252, Loss: 2.782102584838867, Final Batch Loss: 1.0570096969604492\n",
      "Epoch 253, Loss: 2.5760003328323364, Final Batch Loss: 0.9388948678970337\n",
      "Epoch 254, Loss: 2.3182130455970764, Final Batch Loss: 0.6994571685791016\n",
      "Epoch 255, Loss: 2.319986879825592, Final Batch Loss: 0.6413884162902832\n",
      "Epoch 256, Loss: 2.3717092871665955, Final Batch Loss: 0.733048677444458\n",
      "Epoch 257, Loss: 2.287691354751587, Final Batch Loss: 0.6554189920425415\n",
      "Epoch 258, Loss: 2.548371732234955, Final Batch Loss: 0.9418142437934875\n",
      "Epoch 259, Loss: 2.2695910930633545, Final Batch Loss: 0.7196183800697327\n",
      "Epoch 260, Loss: 2.3471924662590027, Final Batch Loss: 0.7498036623001099\n",
      "Epoch 261, Loss: 2.372160255908966, Final Batch Loss: 0.7866096496582031\n",
      "Epoch 262, Loss: 2.3539315462112427, Final Batch Loss: 0.7161411046981812\n",
      "Epoch 263, Loss: 2.663025736808777, Final Batch Loss: 1.0527700185775757\n",
      "Epoch 264, Loss: 2.5302022099494934, Final Batch Loss: 0.8661054372787476\n",
      "Epoch 265, Loss: 2.674329996109009, Final Batch Loss: 1.0083558559417725\n",
      "Epoch 266, Loss: 2.277311325073242, Final Batch Loss: 0.662848949432373\n",
      "Epoch 267, Loss: 2.714966297149658, Final Batch Loss: 1.1948308944702148\n",
      "Epoch 268, Loss: 2.5518828630447388, Final Batch Loss: 0.8346376419067383\n",
      "Epoch 269, Loss: 2.418688178062439, Final Batch Loss: 0.8158899545669556\n",
      "Epoch 270, Loss: 2.599473237991333, Final Batch Loss: 1.0407365560531616\n",
      "Epoch 271, Loss: 2.363244593143463, Final Batch Loss: 0.75312739610672\n",
      "Epoch 272, Loss: 2.4311896562576294, Final Batch Loss: 0.7729681730270386\n",
      "Epoch 273, Loss: 2.3264160752296448, Final Batch Loss: 0.747795581817627\n",
      "Epoch 274, Loss: 2.5746098160743713, Final Batch Loss: 0.9948337078094482\n",
      "Epoch 275, Loss: 2.261907935142517, Final Batch Loss: 0.7318928837776184\n",
      "Epoch 276, Loss: 2.0744145810604095, Final Batch Loss: 0.47642001509666443\n",
      "Epoch 277, Loss: 2.2448808550834656, Final Batch Loss: 0.6540878415107727\n",
      "Epoch 278, Loss: 2.288788914680481, Final Batch Loss: 0.721767783164978\n",
      "Epoch 279, Loss: 2.691652297973633, Final Batch Loss: 1.1161407232284546\n",
      "Epoch 280, Loss: 2.4099390506744385, Final Batch Loss: 0.8727695941925049\n",
      "Epoch 281, Loss: 2.1719881296157837, Final Batch Loss: 0.6411244869232178\n",
      "Epoch 282, Loss: 2.4660488963127136, Final Batch Loss: 0.9479151368141174\n",
      "Epoch 283, Loss: 2.654091000556946, Final Batch Loss: 1.070128083229065\n",
      "Epoch 284, Loss: 2.394842803478241, Final Batch Loss: 0.6728532910346985\n",
      "Epoch 285, Loss: 2.7047852873802185, Final Batch Loss: 1.128780722618103\n",
      "Epoch 286, Loss: 2.0998578667640686, Final Batch Loss: 0.5250886082649231\n",
      "Epoch 287, Loss: 2.4068097472190857, Final Batch Loss: 0.7581552863121033\n",
      "Epoch 288, Loss: 2.2949891090393066, Final Batch Loss: 0.7592503428459167\n",
      "Epoch 289, Loss: 2.4393463730812073, Final Batch Loss: 0.8770140409469604\n",
      "Epoch 290, Loss: 2.2296048998832703, Final Batch Loss: 0.60784512758255\n",
      "Epoch 291, Loss: 2.429543673992157, Final Batch Loss: 0.7915381789207458\n",
      "Epoch 292, Loss: 2.2247042059898376, Final Batch Loss: 0.645871639251709\n",
      "Epoch 293, Loss: 2.325071930885315, Final Batch Loss: 0.8539327383041382\n",
      "Epoch 294, Loss: 2.3818143010139465, Final Batch Loss: 0.8182892203330994\n",
      "Epoch 295, Loss: 2.8304344415664673, Final Batch Loss: 1.269844889640808\n",
      "Epoch 296, Loss: 2.545384705066681, Final Batch Loss: 0.8972848653793335\n",
      "Epoch 297, Loss: 2.3564544916152954, Final Batch Loss: 0.85170978307724\n",
      "Epoch 298, Loss: 2.476735830307007, Final Batch Loss: 0.9845779538154602\n",
      "Epoch 299, Loss: 2.0639384388923645, Final Batch Loss: 0.566371738910675\n",
      "Epoch 300, Loss: 2.3850786089897156, Final Batch Loss: 0.9595405459403992\n",
      "Epoch 301, Loss: 2.4039533734321594, Final Batch Loss: 0.8528414368629456\n",
      "Epoch 302, Loss: 2.2539199590682983, Final Batch Loss: 0.7651203274726868\n",
      "Epoch 303, Loss: 2.2718430161476135, Final Batch Loss: 0.7655289173126221\n",
      "Epoch 304, Loss: 2.1924046874046326, Final Batch Loss: 0.7360522747039795\n",
      "Epoch 305, Loss: 2.462777853012085, Final Batch Loss: 0.9088860750198364\n",
      "Epoch 306, Loss: 2.1335062384605408, Final Batch Loss: 0.5995185375213623\n",
      "Epoch 307, Loss: 2.1332948803901672, Final Batch Loss: 0.6412956714630127\n",
      "Epoch 308, Loss: 2.6536444425582886, Final Batch Loss: 1.1482908725738525\n",
      "Epoch 309, Loss: 2.391435384750366, Final Batch Loss: 0.9504456520080566\n",
      "Epoch 310, Loss: 2.009257197380066, Final Batch Loss: 0.5146745443344116\n",
      "Epoch 311, Loss: 2.1088406443595886, Final Batch Loss: 0.6295123100280762\n",
      "Epoch 312, Loss: 2.2169456481933594, Final Batch Loss: 0.7583866119384766\n",
      "Epoch 313, Loss: 2.451956808567047, Final Batch Loss: 0.9800990223884583\n",
      "Epoch 314, Loss: 2.353537857532501, Final Batch Loss: 0.8420194387435913\n",
      "Epoch 315, Loss: 2.2907413840293884, Final Batch Loss: 0.7400704026222229\n",
      "Epoch 316, Loss: 2.60668808221817, Final Batch Loss: 1.098044514656067\n",
      "Epoch 317, Loss: 2.1198185086250305, Final Batch Loss: 0.7020111083984375\n",
      "Epoch 318, Loss: 2.245818555355072, Final Batch Loss: 0.7064687609672546\n",
      "Epoch 319, Loss: 2.153067708015442, Final Batch Loss: 0.6602630019187927\n",
      "Epoch 320, Loss: 1.8922529518604279, Final Batch Loss: 0.33324095606803894\n",
      "Epoch 321, Loss: 2.0896336436271667, Final Batch Loss: 0.6198310852050781\n",
      "Epoch 322, Loss: 2.163097083568573, Final Batch Loss: 0.7324795722961426\n",
      "Epoch 323, Loss: 2.580745279788971, Final Batch Loss: 1.1320176124572754\n",
      "Epoch 324, Loss: 2.2546083331108093, Final Batch Loss: 0.7768875360488892\n",
      "Epoch 325, Loss: 2.0309630036354065, Final Batch Loss: 0.6207984089851379\n",
      "Epoch 326, Loss: 2.1032373309135437, Final Batch Loss: 0.6769448518753052\n",
      "Epoch 327, Loss: 2.741090714931488, Final Batch Loss: 1.2000800371170044\n",
      "Epoch 328, Loss: 2.400443434715271, Final Batch Loss: 0.8714033365249634\n",
      "Epoch 329, Loss: 2.31699275970459, Final Batch Loss: 0.8069667220115662\n",
      "Epoch 330, Loss: 2.067376732826233, Final Batch Loss: 0.5767008066177368\n",
      "Epoch 331, Loss: 2.265972852706909, Final Batch Loss: 0.8004483580589294\n",
      "Epoch 332, Loss: 2.219669282436371, Final Batch Loss: 0.6827660799026489\n",
      "Epoch 333, Loss: 2.2357466220855713, Final Batch Loss: 0.787319004535675\n",
      "Epoch 334, Loss: 2.4372013807296753, Final Batch Loss: 0.9463304281234741\n",
      "Epoch 335, Loss: 2.326769471168518, Final Batch Loss: 0.8715821504592896\n",
      "Epoch 336, Loss: 2.2001951336860657, Final Batch Loss: 0.7098159193992615\n",
      "Epoch 337, Loss: 2.17279714345932, Final Batch Loss: 0.7053699493408203\n",
      "Epoch 338, Loss: 2.1738184094429016, Final Batch Loss: 0.699485182762146\n",
      "Epoch 339, Loss: 2.2485249042510986, Final Batch Loss: 0.8581548929214478\n",
      "Epoch 340, Loss: 2.0442557334899902, Final Batch Loss: 0.537492573261261\n",
      "Epoch 341, Loss: 2.056716024875641, Final Batch Loss: 0.6180672645568848\n",
      "Epoch 342, Loss: 1.944797694683075, Final Batch Loss: 0.5657496452331543\n",
      "Epoch 343, Loss: 1.838477462530136, Final Batch Loss: 0.34359797835350037\n",
      "Epoch 344, Loss: 2.2630234360694885, Final Batch Loss: 0.7976576089859009\n",
      "Epoch 345, Loss: 2.1259433031082153, Final Batch Loss: 0.6703649163246155\n",
      "Epoch 346, Loss: 2.040249824523926, Final Batch Loss: 0.5646411776542664\n",
      "Epoch 347, Loss: 2.2661473155021667, Final Batch Loss: 0.9088318347930908\n",
      "Epoch 348, Loss: 2.0107656121253967, Final Batch Loss: 0.5945134162902832\n",
      "Epoch 349, Loss: 2.321538269519806, Final Batch Loss: 0.8729866147041321\n",
      "Epoch 350, Loss: 2.2058739066123962, Final Batch Loss: 0.8107495307922363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 351, Loss: 2.013450562953949, Final Batch Loss: 0.6400045156478882\n",
      "Epoch 352, Loss: 2.1342252492904663, Final Batch Loss: 0.6904661655426025\n",
      "Epoch 353, Loss: 2.0439324378967285, Final Batch Loss: 0.5866657495498657\n",
      "Epoch 354, Loss: 1.9390186667442322, Final Batch Loss: 0.5529820322990417\n",
      "Epoch 355, Loss: 1.9993765950202942, Final Batch Loss: 0.5824297666549683\n",
      "Epoch 356, Loss: 1.7692166864871979, Final Batch Loss: 0.2674235999584198\n",
      "Epoch 357, Loss: 2.4637480974197388, Final Batch Loss: 0.9410055875778198\n",
      "Epoch 358, Loss: 2.076961040496826, Final Batch Loss: 0.7111289501190186\n",
      "Epoch 359, Loss: 2.3985010385513306, Final Batch Loss: 0.9990618824958801\n",
      "Epoch 360, Loss: 1.955083817243576, Final Batch Loss: 0.4821673333644867\n",
      "Epoch 361, Loss: 1.8687787353992462, Final Batch Loss: 0.4787910282611847\n",
      "Epoch 362, Loss: 2.514367163181305, Final Batch Loss: 1.039547324180603\n",
      "Epoch 363, Loss: 1.9633955359458923, Final Batch Loss: 0.6130227446556091\n",
      "Epoch 364, Loss: 2.2619903683662415, Final Batch Loss: 0.8503274321556091\n",
      "Epoch 365, Loss: 2.395860016345978, Final Batch Loss: 0.9584676623344421\n",
      "Epoch 366, Loss: 2.147744834423065, Final Batch Loss: 0.7429023385047913\n",
      "Epoch 367, Loss: 1.9817000031471252, Final Batch Loss: 0.5386216044425964\n",
      "Epoch 368, Loss: 2.1677682995796204, Final Batch Loss: 0.769603431224823\n",
      "Epoch 369, Loss: 2.119981110095978, Final Batch Loss: 0.6985545754432678\n",
      "Epoch 370, Loss: 1.8483517169952393, Final Batch Loss: 0.3274266719818115\n",
      "Epoch 371, Loss: 2.217720091342926, Final Batch Loss: 0.8142110109329224\n",
      "Epoch 372, Loss: 1.9882968068122864, Final Batch Loss: 0.558731198310852\n",
      "Epoch 373, Loss: 2.051180839538574, Final Batch Loss: 0.7060405611991882\n",
      "Epoch 374, Loss: 2.0675244331359863, Final Batch Loss: 0.6868376731872559\n",
      "Epoch 375, Loss: 1.9322441220283508, Final Batch Loss: 0.5428277850151062\n",
      "Epoch 376, Loss: 1.9981442093849182, Final Batch Loss: 0.6127837896347046\n",
      "Epoch 377, Loss: 2.2255818247795105, Final Batch Loss: 0.8956354260444641\n",
      "Epoch 378, Loss: 2.159623920917511, Final Batch Loss: 0.7762154340744019\n",
      "Epoch 379, Loss: 2.233550786972046, Final Batch Loss: 0.8571611642837524\n",
      "Epoch 380, Loss: 2.1687915921211243, Final Batch Loss: 0.7784107327461243\n",
      "Epoch 381, Loss: 2.0961620211601257, Final Batch Loss: 0.7541914582252502\n",
      "Epoch 382, Loss: 2.007174491882324, Final Batch Loss: 0.614306628704071\n",
      "Epoch 383, Loss: 1.855391800403595, Final Batch Loss: 0.5272148847579956\n",
      "Epoch 384, Loss: 2.15049546957016, Final Batch Loss: 0.7441731691360474\n",
      "Epoch 385, Loss: 1.864821970462799, Final Batch Loss: 0.4809456467628479\n",
      "Epoch 386, Loss: 1.798207849264145, Final Batch Loss: 0.3496035635471344\n",
      "Epoch 387, Loss: 2.024893820285797, Final Batch Loss: 0.6142786741256714\n",
      "Epoch 388, Loss: 2.114334464073181, Final Batch Loss: 0.7108991742134094\n",
      "Epoch 389, Loss: 2.1565579771995544, Final Batch Loss: 0.7641832232475281\n",
      "Epoch 390, Loss: 1.9710302352905273, Final Batch Loss: 0.5859963893890381\n",
      "Epoch 391, Loss: 2.0478528141975403, Final Batch Loss: 0.66724693775177\n",
      "Epoch 392, Loss: 2.043120861053467, Final Batch Loss: 0.7178791165351868\n",
      "Epoch 393, Loss: 1.977342426776886, Final Batch Loss: 0.6134630441665649\n",
      "Epoch 394, Loss: 1.9644194841384888, Final Batch Loss: 0.5119436979293823\n",
      "Epoch 395, Loss: 2.3921974301338196, Final Batch Loss: 0.9977100491523743\n",
      "Epoch 396, Loss: 2.3685027360916138, Final Batch Loss: 0.988303542137146\n",
      "Epoch 397, Loss: 2.166788339614868, Final Batch Loss: 0.8064871430397034\n",
      "Epoch 398, Loss: 2.0266076922416687, Final Batch Loss: 0.7097827792167664\n",
      "Epoch 399, Loss: 1.9384326338768005, Final Batch Loss: 0.5542915463447571\n",
      "Epoch 400, Loss: 2.0472365617752075, Final Batch Loss: 0.6111962795257568\n",
      "Epoch 401, Loss: 2.3104625940322876, Final Batch Loss: 0.9325360655784607\n",
      "Epoch 402, Loss: 2.228948950767517, Final Batch Loss: 0.8160895109176636\n",
      "Epoch 403, Loss: 1.7842665314674377, Final Batch Loss: 0.46872711181640625\n",
      "Epoch 404, Loss: 1.9814817905426025, Final Batch Loss: 0.611982524394989\n",
      "Epoch 405, Loss: 2.1019169688224792, Final Batch Loss: 0.7331795692443848\n",
      "Epoch 406, Loss: 2.27471923828125, Final Batch Loss: 0.8565383553504944\n",
      "Epoch 407, Loss: 1.8183015882968903, Final Batch Loss: 0.47592267394065857\n",
      "Epoch 408, Loss: 2.136420786380768, Final Batch Loss: 0.7368419766426086\n",
      "Epoch 409, Loss: 2.0350316166877747, Final Batch Loss: 0.6248693466186523\n",
      "Epoch 410, Loss: 1.8313714563846588, Final Batch Loss: 0.4682946503162384\n",
      "Epoch 411, Loss: 1.6105095446109772, Final Batch Loss: 0.2941564619541168\n",
      "Epoch 412, Loss: 1.858856737613678, Final Batch Loss: 0.5365682244300842\n",
      "Epoch 413, Loss: 2.1330578923225403, Final Batch Loss: 0.7838984131813049\n",
      "Epoch 414, Loss: 1.9428266286849976, Final Batch Loss: 0.5964210629463196\n",
      "Epoch 415, Loss: 1.776327759027481, Final Batch Loss: 0.4796279966831207\n",
      "Epoch 416, Loss: 2.216485857963562, Final Batch Loss: 0.9122361540794373\n",
      "Epoch 417, Loss: 2.2079625129699707, Final Batch Loss: 0.8703622221946716\n",
      "Epoch 418, Loss: 2.1309635043144226, Final Batch Loss: 0.7978807687759399\n",
      "Epoch 419, Loss: 2.06836473941803, Final Batch Loss: 0.7355026602745056\n",
      "Epoch 420, Loss: 1.888524353504181, Final Batch Loss: 0.6049160957336426\n",
      "Epoch 421, Loss: 2.120937407016754, Final Batch Loss: 0.7993360161781311\n",
      "Epoch 422, Loss: 1.9022014141082764, Final Batch Loss: 0.5565385222434998\n",
      "Epoch 423, Loss: 2.02017480134964, Final Batch Loss: 0.6961579918861389\n",
      "Epoch 424, Loss: 2.0609967708587646, Final Batch Loss: 0.7451555132865906\n",
      "Epoch 425, Loss: 1.7825382947921753, Final Batch Loss: 0.42969799041748047\n",
      "Epoch 426, Loss: 2.19792640209198, Final Batch Loss: 0.8613470196723938\n",
      "Epoch 427, Loss: 2.2595866322517395, Final Batch Loss: 0.8955994844436646\n",
      "Epoch 428, Loss: 1.9508141875267029, Final Batch Loss: 0.5698333382606506\n",
      "Epoch 429, Loss: 2.0092723965644836, Final Batch Loss: 0.6792811155319214\n",
      "Epoch 430, Loss: 1.7274475693702698, Final Batch Loss: 0.46632546186447144\n",
      "Epoch 431, Loss: 2.118506669998169, Final Batch Loss: 0.7930822968482971\n",
      "Epoch 432, Loss: 2.053828001022339, Final Batch Loss: 0.7581636905670166\n",
      "Epoch 433, Loss: 2.1838064193725586, Final Batch Loss: 0.850048840045929\n",
      "Epoch 434, Loss: 1.823786973953247, Final Batch Loss: 0.5250792503356934\n",
      "Epoch 435, Loss: 1.786296308040619, Final Batch Loss: 0.5620420575141907\n",
      "Epoch 436, Loss: 1.9909968376159668, Final Batch Loss: 0.6809358596801758\n",
      "Epoch 437, Loss: 1.8891363739967346, Final Batch Loss: 0.6601707935333252\n",
      "Epoch 438, Loss: 2.1153467893600464, Final Batch Loss: 0.7930765748023987\n",
      "Epoch 439, Loss: 1.9540590047836304, Final Batch Loss: 0.6642708778381348\n",
      "Epoch 440, Loss: 2.077739179134369, Final Batch Loss: 0.8487789630889893\n",
      "Epoch 441, Loss: 2.0079821944236755, Final Batch Loss: 0.6635534167289734\n",
      "Epoch 442, Loss: 2.1903855204582214, Final Batch Loss: 0.9110930562019348\n",
      "Epoch 443, Loss: 1.8960824012756348, Final Batch Loss: 0.6299538016319275\n",
      "Epoch 444, Loss: 2.0103076696395874, Final Batch Loss: 0.6862385272979736\n",
      "Epoch 445, Loss: 1.9624227285385132, Final Batch Loss: 0.6876043677330017\n",
      "Epoch 446, Loss: 1.754889965057373, Final Batch Loss: 0.4892348647117615\n",
      "Epoch 447, Loss: 2.224339485168457, Final Batch Loss: 0.9239280819892883\n",
      "Epoch 448, Loss: 1.8707157373428345, Final Batch Loss: 0.5206195116043091\n",
      "Epoch 449, Loss: 1.9696317911148071, Final Batch Loss: 0.7003836631774902\n",
      "Epoch 450, Loss: 1.973856806755066, Final Batch Loss: 0.6328258514404297\n",
      "Epoch 451, Loss: 1.753304898738861, Final Batch Loss: 0.38976311683654785\n",
      "Epoch 452, Loss: 1.8785571455955505, Final Batch Loss: 0.5863378643989563\n",
      "Epoch 453, Loss: 1.7473304867744446, Final Batch Loss: 0.44592851400375366\n",
      "Epoch 454, Loss: 1.89837247133255, Final Batch Loss: 0.5704575181007385\n",
      "Epoch 455, Loss: 2.01396244764328, Final Batch Loss: 0.8144862651824951\n",
      "Epoch 456, Loss: 1.8879247307777405, Final Batch Loss: 0.741232693195343\n",
      "Epoch 457, Loss: 1.897378146648407, Final Batch Loss: 0.5904225707054138\n",
      "Epoch 458, Loss: 1.6824403405189514, Final Batch Loss: 0.36545073986053467\n",
      "Epoch 459, Loss: 2.2958162426948547, Final Batch Loss: 1.044296145439148\n",
      "Epoch 460, Loss: 2.3320208191871643, Final Batch Loss: 1.0282588005065918\n",
      "Epoch 461, Loss: 1.715321034193039, Final Batch Loss: 0.48728570342063904\n",
      "Epoch 462, Loss: 1.7104664146900177, Final Batch Loss: 0.4125784933567047\n",
      "Epoch 463, Loss: 1.9692222476005554, Final Batch Loss: 0.708861768245697\n",
      "Epoch 464, Loss: 2.1243979930877686, Final Batch Loss: 0.8204411864280701\n",
      "Epoch 465, Loss: 1.71729177236557, Final Batch Loss: 0.5192130208015442\n",
      "Epoch 466, Loss: 1.7278798520565033, Final Batch Loss: 0.4748521149158478\n",
      "Epoch 467, Loss: 1.9456570148468018, Final Batch Loss: 0.6774101257324219\n",
      "Epoch 468, Loss: 1.9452374577522278, Final Batch Loss: 0.5898703336715698\n",
      "Epoch 469, Loss: 1.90797621011734, Final Batch Loss: 0.6337477564811707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470, Loss: 1.9586249589920044, Final Batch Loss: 0.6220993399620056\n",
      "Epoch 471, Loss: 1.8814269304275513, Final Batch Loss: 0.5689801573753357\n",
      "Epoch 472, Loss: 1.8783403038978577, Final Batch Loss: 0.5932577848434448\n",
      "Epoch 473, Loss: 2.3281983733177185, Final Batch Loss: 1.061290979385376\n",
      "Epoch 474, Loss: 2.292081296443939, Final Batch Loss: 0.9729374051094055\n",
      "Epoch 475, Loss: 2.084551751613617, Final Batch Loss: 0.7967334985733032\n",
      "Epoch 476, Loss: 1.7897415161132812, Final Batch Loss: 0.5070258975028992\n",
      "Epoch 477, Loss: 1.723101019859314, Final Batch Loss: 0.4327154755592346\n",
      "Epoch 478, Loss: 1.5935910940170288, Final Batch Loss: 0.3171360492706299\n",
      "Epoch 479, Loss: 2.260531723499298, Final Batch Loss: 0.991112470626831\n",
      "Epoch 480, Loss: 1.7151660025119781, Final Batch Loss: 0.4271680414676666\n",
      "Epoch 481, Loss: 1.959616482257843, Final Batch Loss: 0.6650979518890381\n",
      "Epoch 482, Loss: 1.8413841128349304, Final Batch Loss: 0.5836924910545349\n",
      "Epoch 483, Loss: 2.0854429602622986, Final Batch Loss: 0.7651534676551819\n",
      "Epoch 484, Loss: 1.9723578095436096, Final Batch Loss: 0.7663156986236572\n",
      "Epoch 485, Loss: 1.8739609122276306, Final Batch Loss: 0.6334518790245056\n",
      "Epoch 486, Loss: 1.7862573266029358, Final Batch Loss: 0.5716493725776672\n",
      "Epoch 487, Loss: 2.1120458841323853, Final Batch Loss: 0.7609938383102417\n",
      "Epoch 488, Loss: 2.0670876502990723, Final Batch Loss: 0.7569418549537659\n",
      "Epoch 489, Loss: 1.707910269498825, Final Batch Loss: 0.42722371220588684\n",
      "Epoch 490, Loss: 1.7784024477005005, Final Batch Loss: 0.5282678604125977\n",
      "Epoch 491, Loss: 1.8417311310768127, Final Batch Loss: 0.5292691588401794\n",
      "Epoch 492, Loss: 1.6652685105800629, Final Batch Loss: 0.42789724469184875\n",
      "Epoch 493, Loss: 1.8263235092163086, Final Batch Loss: 0.624778687953949\n",
      "Epoch 494, Loss: 1.8864349126815796, Final Batch Loss: 0.5780386924743652\n",
      "Epoch 495, Loss: 1.9421475529670715, Final Batch Loss: 0.6769302487373352\n",
      "Epoch 496, Loss: 1.8176847100257874, Final Batch Loss: 0.5708429217338562\n",
      "Epoch 497, Loss: 1.9874629378318787, Final Batch Loss: 0.6805433630943298\n",
      "Epoch 498, Loss: 1.6717022955417633, Final Batch Loss: 0.4412541091442108\n",
      "Epoch 499, Loss: 1.7551667988300323, Final Batch Loss: 0.4909529387950897\n",
      "Epoch 500, Loss: 1.8765843510627747, Final Batch Loss: 0.5980159640312195\n",
      "Epoch 501, Loss: 1.8563435673713684, Final Batch Loss: 0.6245393753051758\n",
      "Epoch 502, Loss: 1.8821339011192322, Final Batch Loss: 0.6717861890792847\n",
      "Epoch 503, Loss: 1.9145363569259644, Final Batch Loss: 0.6976581811904907\n",
      "Epoch 504, Loss: 1.9717334508895874, Final Batch Loss: 0.6828745007514954\n",
      "Epoch 505, Loss: 1.9041219353675842, Final Batch Loss: 0.6441066265106201\n",
      "Epoch 506, Loss: 1.8100795149803162, Final Batch Loss: 0.5755150318145752\n",
      "Epoch 507, Loss: 1.7700911164283752, Final Batch Loss: 0.5532711148262024\n",
      "Epoch 508, Loss: 2.2506558895111084, Final Batch Loss: 0.9825453758239746\n",
      "Epoch 509, Loss: 1.912036120891571, Final Batch Loss: 0.7593191266059875\n",
      "Epoch 510, Loss: 2.082965910434723, Final Batch Loss: 0.912704586982727\n",
      "Epoch 511, Loss: 1.5362301170825958, Final Batch Loss: 0.32226791977882385\n",
      "Epoch 512, Loss: 1.8245441317558289, Final Batch Loss: 0.6666990518569946\n",
      "Epoch 513, Loss: 1.8473886251449585, Final Batch Loss: 0.5733605623245239\n",
      "Epoch 514, Loss: 1.9926401376724243, Final Batch Loss: 0.6961153149604797\n",
      "Epoch 515, Loss: 2.011653423309326, Final Batch Loss: 0.7124026417732239\n",
      "Epoch 516, Loss: 1.977591335773468, Final Batch Loss: 0.7850333452224731\n",
      "Epoch 517, Loss: 2.0531085729599, Final Batch Loss: 0.884255051612854\n",
      "Epoch 518, Loss: 1.8013461828231812, Final Batch Loss: 0.6371302604675293\n",
      "Epoch 519, Loss: 1.8726540803909302, Final Batch Loss: 0.6954779624938965\n",
      "Epoch 520, Loss: 1.8332094550132751, Final Batch Loss: 0.5773937702178955\n",
      "Epoch 521, Loss: 1.7265308499336243, Final Batch Loss: 0.49463748931884766\n",
      "Epoch 522, Loss: 1.9195550084114075, Final Batch Loss: 0.6596722602844238\n",
      "Epoch 523, Loss: 1.8587801456451416, Final Batch Loss: 0.5852466821670532\n",
      "Epoch 524, Loss: 1.9753530025482178, Final Batch Loss: 0.7265691757202148\n",
      "Epoch 525, Loss: 1.9124637246131897, Final Batch Loss: 0.7194889187812805\n",
      "Epoch 526, Loss: 1.8821513652801514, Final Batch Loss: 0.7020301818847656\n",
      "Epoch 527, Loss: 1.8663274645805359, Final Batch Loss: 0.6208069920539856\n",
      "Epoch 528, Loss: 1.7647212147712708, Final Batch Loss: 0.5005180239677429\n",
      "Epoch 529, Loss: 2.021306872367859, Final Batch Loss: 0.8224403858184814\n",
      "Epoch 530, Loss: 1.7448013722896576, Final Batch Loss: 0.497923344373703\n",
      "Epoch 531, Loss: 1.9507684111595154, Final Batch Loss: 0.6820172667503357\n",
      "Epoch 532, Loss: 1.6516847014427185, Final Batch Loss: 0.46388155221939087\n",
      "Epoch 533, Loss: 1.8040685057640076, Final Batch Loss: 0.5761921405792236\n",
      "Epoch 534, Loss: 1.822245478630066, Final Batch Loss: 0.6415959596633911\n",
      "Epoch 535, Loss: 1.8156402707099915, Final Batch Loss: 0.6501346230506897\n",
      "Epoch 536, Loss: 1.714404046535492, Final Batch Loss: 0.557181179523468\n",
      "Epoch 537, Loss: 1.9098334908485413, Final Batch Loss: 0.7231023907661438\n",
      "Epoch 538, Loss: 1.7172430753707886, Final Batch Loss: 0.5431456565856934\n",
      "Epoch 539, Loss: 1.7532116174697876, Final Batch Loss: 0.5393881797790527\n",
      "Epoch 540, Loss: 1.745078682899475, Final Batch Loss: 0.5656490325927734\n",
      "Epoch 541, Loss: 1.8385308980941772, Final Batch Loss: 0.6062870025634766\n",
      "Epoch 542, Loss: 1.831460416316986, Final Batch Loss: 0.5978560447692871\n",
      "Epoch 543, Loss: 1.9941529631614685, Final Batch Loss: 0.7931501865386963\n",
      "Epoch 544, Loss: 1.6259546279907227, Final Batch Loss: 0.5227409601211548\n",
      "Epoch 545, Loss: 1.6633232831954956, Final Batch Loss: 0.5557624697685242\n",
      "Epoch 546, Loss: 1.7655656933784485, Final Batch Loss: 0.5750289559364319\n",
      "Epoch 547, Loss: 1.8068877458572388, Final Batch Loss: 0.6211285591125488\n",
      "Epoch 548, Loss: 2.0350705981254578, Final Batch Loss: 0.8088012933731079\n",
      "Epoch 549, Loss: 1.9076188206672668, Final Batch Loss: 0.7920553088188171\n",
      "Epoch 550, Loss: 1.6444212794303894, Final Batch Loss: 0.49154603481292725\n",
      "Epoch 551, Loss: 1.9829432964324951, Final Batch Loss: 0.7876680493354797\n",
      "Epoch 552, Loss: 1.5657066106796265, Final Batch Loss: 0.34302836656570435\n",
      "Epoch 553, Loss: 1.7957812547683716, Final Batch Loss: 0.6017181277275085\n",
      "Epoch 554, Loss: 1.734351634979248, Final Batch Loss: 0.5139254927635193\n",
      "Epoch 555, Loss: 1.9597457647323608, Final Batch Loss: 0.7455471158027649\n",
      "Epoch 556, Loss: 1.5904019176959991, Final Batch Loss: 0.3526165783405304\n",
      "Epoch 557, Loss: 1.562960535287857, Final Batch Loss: 0.406992107629776\n",
      "Epoch 558, Loss: 1.8895971775054932, Final Batch Loss: 0.7651308178901672\n",
      "Epoch 559, Loss: 1.6054360270500183, Final Batch Loss: 0.42395979166030884\n",
      "Epoch 560, Loss: 1.5502291321754456, Final Batch Loss: 0.3121349811553955\n",
      "Epoch 561, Loss: 2.3345277905464172, Final Batch Loss: 1.0740211009979248\n",
      "Epoch 562, Loss: 1.6705027222633362, Final Batch Loss: 0.4504406452178955\n",
      "Epoch 563, Loss: 1.8286228775978088, Final Batch Loss: 0.6881386637687683\n",
      "Epoch 564, Loss: 1.8361451625823975, Final Batch Loss: 0.6399998664855957\n",
      "Epoch 565, Loss: 1.8021386861801147, Final Batch Loss: 0.6127973794937134\n",
      "Epoch 566, Loss: 1.754318654537201, Final Batch Loss: 0.5132373571395874\n",
      "Epoch 567, Loss: 2.2882537841796875, Final Batch Loss: 1.0440943241119385\n",
      "Epoch 568, Loss: 1.7888266444206238, Final Batch Loss: 0.563615620136261\n",
      "Epoch 569, Loss: 1.7718692421913147, Final Batch Loss: 0.6319721937179565\n",
      "Epoch 570, Loss: 1.808112919330597, Final Batch Loss: 0.6165032386779785\n",
      "Epoch 571, Loss: 1.6578038930892944, Final Batch Loss: 0.5175952315330505\n",
      "Epoch 572, Loss: 1.8243275880813599, Final Batch Loss: 0.6609014868736267\n",
      "Epoch 573, Loss: 2.0355879068374634, Final Batch Loss: 0.821508526802063\n",
      "Epoch 574, Loss: 1.667521744966507, Final Batch Loss: 0.49031582474708557\n",
      "Epoch 575, Loss: 1.651673138141632, Final Batch Loss: 0.5719396471977234\n",
      "Epoch 576, Loss: 1.5522453486919403, Final Batch Loss: 0.38072988390922546\n",
      "Epoch 577, Loss: 2.035770297050476, Final Batch Loss: 0.7795136570930481\n",
      "Epoch 578, Loss: 1.6172529458999634, Final Batch Loss: 0.42638516426086426\n",
      "Epoch 579, Loss: 1.7231732606887817, Final Batch Loss: 0.6167712211608887\n",
      "Epoch 580, Loss: 1.7070079445838928, Final Batch Loss: 0.5067514181137085\n",
      "Epoch 581, Loss: 1.6697655320167542, Final Batch Loss: 0.5160959959030151\n",
      "Epoch 582, Loss: 1.68455171585083, Final Batch Loss: 0.4837634563446045\n",
      "Epoch 583, Loss: 1.7233851552009583, Final Batch Loss: 0.6232382655143738\n",
      "Epoch 584, Loss: 1.567097544670105, Final Batch Loss: 0.40168553590774536\n",
      "Epoch 585, Loss: 1.835336148738861, Final Batch Loss: 0.7035222053527832\n",
      "Epoch 586, Loss: 1.9619327187538147, Final Batch Loss: 0.8612358570098877\n",
      "Epoch 587, Loss: 1.7319822311401367, Final Batch Loss: 0.5391301512718201\n",
      "Epoch 588, Loss: 2.0043106079101562, Final Batch Loss: 0.7906914353370667\n",
      "Epoch 589, Loss: 1.6604869365692139, Final Batch Loss: 0.5154168009757996\n",
      "Epoch 590, Loss: 1.63632071018219, Final Batch Loss: 0.5728024840354919\n",
      "Epoch 591, Loss: 2.027014195919037, Final Batch Loss: 0.8931482434272766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 592, Loss: 1.69326913356781, Final Batch Loss: 0.4686644673347473\n",
      "Epoch 593, Loss: 1.9077742099761963, Final Batch Loss: 0.7457810640335083\n",
      "Epoch 594, Loss: 1.6730315685272217, Final Batch Loss: 0.4802761673927307\n",
      "Epoch 595, Loss: 1.8142898082733154, Final Batch Loss: 0.6509081125259399\n",
      "Epoch 596, Loss: 1.5487203299999237, Final Batch Loss: 0.3358323276042938\n",
      "Epoch 597, Loss: 1.6283623278141022, Final Batch Loss: 0.4968093931674957\n",
      "Epoch 598, Loss: 1.7959765791893005, Final Batch Loss: 0.6620500683784485\n",
      "Epoch 599, Loss: 1.8955330848693848, Final Batch Loss: 0.7218726277351379\n",
      "Epoch 600, Loss: 1.6426363587379456, Final Batch Loss: 0.45590901374816895\n",
      "Epoch 601, Loss: 1.7832172513008118, Final Batch Loss: 0.6326601505279541\n",
      "Epoch 602, Loss: 1.6320799887180328, Final Batch Loss: 0.4860135018825531\n",
      "Epoch 603, Loss: 1.7057815790176392, Final Batch Loss: 0.6133989691734314\n",
      "Epoch 604, Loss: 1.727394938468933, Final Batch Loss: 0.675423800945282\n",
      "Epoch 605, Loss: 1.6163692772388458, Final Batch Loss: 0.4450395405292511\n",
      "Epoch 606, Loss: 1.7053590714931488, Final Batch Loss: 0.7016323208808899\n",
      "Epoch 607, Loss: 1.5887661576271057, Final Batch Loss: 0.48494088649749756\n",
      "Epoch 608, Loss: 1.3802037835121155, Final Batch Loss: 0.26007598638534546\n",
      "Epoch 609, Loss: 1.646510362625122, Final Batch Loss: 0.4999926686286926\n",
      "Epoch 610, Loss: 1.4563962817192078, Final Batch Loss: 0.35370004177093506\n",
      "Epoch 611, Loss: 1.7515146732330322, Final Batch Loss: 0.6082959175109863\n",
      "Epoch 612, Loss: 1.815474510192871, Final Batch Loss: 0.6650068163871765\n",
      "Epoch 613, Loss: 2.0954525768756866, Final Batch Loss: 1.021468162536621\n",
      "Epoch 614, Loss: 1.5228889286518097, Final Batch Loss: 0.4420885145664215\n",
      "Epoch 615, Loss: 1.8540614247322083, Final Batch Loss: 0.8004786372184753\n",
      "Epoch 616, Loss: 1.5228599905967712, Final Batch Loss: 0.37167054414749146\n",
      "Epoch 617, Loss: 1.4665750861167908, Final Batch Loss: 0.33033740520477295\n",
      "Epoch 618, Loss: 1.6427676677703857, Final Batch Loss: 0.4955942630767822\n",
      "Epoch 619, Loss: 1.751474380493164, Final Batch Loss: 0.640543520450592\n",
      "Epoch 620, Loss: 1.788092851638794, Final Batch Loss: 0.5890510678291321\n",
      "Epoch 621, Loss: 1.8735029101371765, Final Batch Loss: 0.7919490933418274\n",
      "Epoch 622, Loss: 1.8159277439117432, Final Batch Loss: 0.7195981740951538\n",
      "Epoch 623, Loss: 1.4552532732486725, Final Batch Loss: 0.3097623884677887\n",
      "Epoch 624, Loss: 1.5272513628005981, Final Batch Loss: 0.5038387775421143\n",
      "Epoch 625, Loss: 1.5278002619743347, Final Batch Loss: 0.38096117973327637\n",
      "Epoch 626, Loss: 1.4185956716537476, Final Batch Loss: 0.3533669710159302\n",
      "Epoch 627, Loss: 1.66708242893219, Final Batch Loss: 0.5132060050964355\n",
      "Epoch 628, Loss: 1.6903347074985504, Final Batch Loss: 0.6031314730644226\n",
      "Epoch 629, Loss: 1.4769248962402344, Final Batch Loss: 0.3716018795967102\n",
      "Epoch 630, Loss: 1.596803903579712, Final Batch Loss: 0.5619601011276245\n",
      "Epoch 631, Loss: 1.5790461897850037, Final Batch Loss: 0.4534054398536682\n",
      "Epoch 632, Loss: 1.6092402338981628, Final Batch Loss: 0.4434897303581238\n",
      "Epoch 633, Loss: 1.5897080600261688, Final Batch Loss: 0.45486554503440857\n",
      "Epoch 634, Loss: 1.733464777469635, Final Batch Loss: 0.6576656699180603\n",
      "Epoch 635, Loss: 1.5389655232429504, Final Batch Loss: 0.47444063425064087\n",
      "Epoch 636, Loss: 1.9360325932502747, Final Batch Loss: 0.871518611907959\n",
      "Epoch 637, Loss: 1.6442785859107971, Final Batch Loss: 0.5887451767921448\n",
      "Epoch 638, Loss: 1.634126365184784, Final Batch Loss: 0.5536043047904968\n",
      "Epoch 639, Loss: 1.834745615720749, Final Batch Loss: 0.8055293560028076\n",
      "Epoch 640, Loss: 1.4996137022972107, Final Batch Loss: 0.44801557064056396\n",
      "Epoch 641, Loss: 1.75650355219841, Final Batch Loss: 0.7011622786521912\n",
      "Epoch 642, Loss: 1.506219059228897, Final Batch Loss: 0.4933815598487854\n",
      "Epoch 643, Loss: 1.5770684480667114, Final Batch Loss: 0.4489230513572693\n",
      "Epoch 644, Loss: 1.8620031476020813, Final Batch Loss: 0.7858611941337585\n",
      "Epoch 645, Loss: 1.5361900627613068, Final Batch Loss: 0.5100460648536682\n",
      "Epoch 646, Loss: 1.6652277708053589, Final Batch Loss: 0.6041523218154907\n",
      "Epoch 647, Loss: 1.5137935280799866, Final Batch Loss: 0.4914071559906006\n",
      "Epoch 648, Loss: 1.652756154537201, Final Batch Loss: 0.6189112067222595\n",
      "Epoch 649, Loss: 1.6271247267723083, Final Batch Loss: 0.6003116965293884\n",
      "Epoch 650, Loss: 1.6375529766082764, Final Batch Loss: 0.5117233395576477\n",
      "Epoch 651, Loss: 1.5713187158107758, Final Batch Loss: 0.5524381399154663\n",
      "Epoch 652, Loss: 1.3414146602153778, Final Batch Loss: 0.3513627350330353\n",
      "Epoch 653, Loss: 1.7370773553848267, Final Batch Loss: 0.6747587323188782\n",
      "Epoch 654, Loss: 1.530165433883667, Final Batch Loss: 0.48637574911117554\n",
      "Epoch 655, Loss: 1.6822392344474792, Final Batch Loss: 0.6126391887664795\n",
      "Epoch 656, Loss: 1.6897318959236145, Final Batch Loss: 0.573727548122406\n",
      "Epoch 657, Loss: 1.6084394752979279, Final Batch Loss: 0.5312290191650391\n",
      "Epoch 658, Loss: 1.4414118826389313, Final Batch Loss: 0.3702845871448517\n",
      "Epoch 659, Loss: 1.5830793380737305, Final Batch Loss: 0.5643763542175293\n",
      "Epoch 660, Loss: 1.3829432427883148, Final Batch Loss: 0.3284185826778412\n",
      "Epoch 661, Loss: 1.7664502263069153, Final Batch Loss: 0.7339765429496765\n",
      "Epoch 662, Loss: 1.7237000465393066, Final Batch Loss: 0.7172815799713135\n",
      "Epoch 663, Loss: 1.7473688125610352, Final Batch Loss: 0.6733536720275879\n",
      "Epoch 664, Loss: 1.317035973072052, Final Batch Loss: 0.264303982257843\n",
      "Epoch 665, Loss: 1.5349262058734894, Final Batch Loss: 0.40415313839912415\n",
      "Epoch 666, Loss: 1.613504409790039, Final Batch Loss: 0.48996198177337646\n",
      "Epoch 667, Loss: 1.4222634732723236, Final Batch Loss: 0.30438414216041565\n",
      "Epoch 668, Loss: 1.5749196410179138, Final Batch Loss: 0.48608851432800293\n",
      "Epoch 669, Loss: 1.7115437686443329, Final Batch Loss: 0.6024702191352844\n",
      "Epoch 670, Loss: 1.6872218549251556, Final Batch Loss: 0.5955333113670349\n",
      "Epoch 671, Loss: 1.5896522104740143, Final Batch Loss: 0.5722047090530396\n",
      "Epoch 672, Loss: 1.9161283075809479, Final Batch Loss: 0.87896728515625\n",
      "Epoch 673, Loss: 1.4949650168418884, Final Batch Loss: 0.4339481592178345\n",
      "Epoch 674, Loss: 1.757914274930954, Final Batch Loss: 0.759870707988739\n",
      "Epoch 675, Loss: 1.5211279392242432, Final Batch Loss: 0.5024265646934509\n",
      "Epoch 676, Loss: 1.5313284397125244, Final Batch Loss: 0.33493030071258545\n",
      "Epoch 677, Loss: 1.5679837465286255, Final Batch Loss: 0.5913401246070862\n",
      "Epoch 678, Loss: 1.3423165678977966, Final Batch Loss: 0.3597882091999054\n",
      "Epoch 679, Loss: 1.5700410306453705, Final Batch Loss: 0.5619277358055115\n",
      "Epoch 680, Loss: 2.1610052585601807, Final Batch Loss: 1.1249784231185913\n",
      "Epoch 681, Loss: 1.6695837080478668, Final Batch Loss: 0.663646936416626\n",
      "Epoch 682, Loss: 1.7411608397960663, Final Batch Loss: 0.6693836450576782\n",
      "Epoch 683, Loss: 1.7293332815170288, Final Batch Loss: 0.6564461588859558\n",
      "Epoch 684, Loss: 1.5423351228237152, Final Batch Loss: 0.49210813641548157\n",
      "Epoch 685, Loss: 1.5224056839942932, Final Batch Loss: 0.515828013420105\n",
      "Epoch 686, Loss: 1.5988991856575012, Final Batch Loss: 0.533714771270752\n",
      "Epoch 687, Loss: 1.57281893491745, Final Batch Loss: 0.5038341283798218\n",
      "Epoch 688, Loss: 1.4013378024101257, Final Batch Loss: 0.3607572913169861\n",
      "Epoch 689, Loss: 1.8907399773597717, Final Batch Loss: 0.8190646171569824\n",
      "Epoch 690, Loss: 1.6618792712688446, Final Batch Loss: 0.6455386281013489\n",
      "Epoch 691, Loss: 1.7707103788852692, Final Batch Loss: 0.7121159434318542\n",
      "Epoch 692, Loss: 1.485843449831009, Final Batch Loss: 0.4037552773952484\n",
      "Epoch 693, Loss: 1.7410296201705933, Final Batch Loss: 0.6253377199172974\n",
      "Epoch 694, Loss: 1.5293585360050201, Final Batch Loss: 0.5359183549880981\n",
      "Epoch 695, Loss: 1.525962620973587, Final Batch Loss: 0.4869934022426605\n",
      "Epoch 696, Loss: 1.3693954944610596, Final Batch Loss: 0.3725307583808899\n",
      "Epoch 697, Loss: 1.2406886368989944, Final Batch Loss: 0.21222646534442902\n",
      "Epoch 698, Loss: 1.6922774910926819, Final Batch Loss: 0.6474705934524536\n",
      "Epoch 699, Loss: 1.5469931066036224, Final Batch Loss: 0.5349197387695312\n",
      "Epoch 700, Loss: 1.5224530100822449, Final Batch Loss: 0.5385221242904663\n",
      "Epoch 701, Loss: 1.7176767587661743, Final Batch Loss: 0.6855713725090027\n",
      "Epoch 702, Loss: 1.4001114964485168, Final Batch Loss: 0.3784688115119934\n",
      "Epoch 703, Loss: 1.3452421724796295, Final Batch Loss: 0.29661571979522705\n",
      "Epoch 704, Loss: 1.4557097256183624, Final Batch Loss: 0.42568516731262207\n",
      "Epoch 705, Loss: 1.6509652733802795, Final Batch Loss: 0.6315463185310364\n",
      "Epoch 706, Loss: 1.526383101940155, Final Batch Loss: 0.5184346437454224\n",
      "Epoch 707, Loss: 1.8006795048713684, Final Batch Loss: 0.7384862303733826\n",
      "Epoch 708, Loss: 1.5969022512435913, Final Batch Loss: 0.60086989402771\n",
      "Epoch 709, Loss: 1.3908592760562897, Final Batch Loss: 0.40020817518234253\n",
      "Epoch 710, Loss: 1.4337301552295685, Final Batch Loss: 0.42177972197532654\n",
      "Epoch 711, Loss: 1.5888384580612183, Final Batch Loss: 0.6095407009124756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 712, Loss: 1.3604874014854431, Final Batch Loss: 0.3702690601348877\n",
      "Epoch 713, Loss: 1.4938486516475677, Final Batch Loss: 0.5340778231620789\n",
      "Epoch 714, Loss: 1.3767221868038177, Final Batch Loss: 0.41569191217422485\n",
      "Epoch 715, Loss: 1.4216744303703308, Final Batch Loss: 0.4121338427066803\n",
      "Epoch 716, Loss: 1.6443625092506409, Final Batch Loss: 0.6093217730522156\n",
      "Epoch 717, Loss: 1.5765307545661926, Final Batch Loss: 0.5296295285224915\n",
      "Epoch 718, Loss: 1.48597052693367, Final Batch Loss: 0.4985570013523102\n",
      "Epoch 719, Loss: 1.4395391643047333, Final Batch Loss: 0.4265119731426239\n",
      "Epoch 720, Loss: 1.4471289813518524, Final Batch Loss: 0.44619980454444885\n",
      "Epoch 721, Loss: 1.3910677134990692, Final Batch Loss: 0.32238802313804626\n",
      "Epoch 722, Loss: 1.5264371037483215, Final Batch Loss: 0.44726496934890747\n",
      "Epoch 723, Loss: 1.4387918412685394, Final Batch Loss: 0.44875070452690125\n",
      "Epoch 724, Loss: 1.314142107963562, Final Batch Loss: 0.3284420073032379\n",
      "Epoch 725, Loss: 1.567865937948227, Final Batch Loss: 0.5723275542259216\n",
      "Epoch 726, Loss: 1.3839381039142609, Final Batch Loss: 0.30822888016700745\n",
      "Epoch 727, Loss: 1.2976309955120087, Final Batch Loss: 0.33809033036231995\n",
      "Epoch 728, Loss: 1.6194133460521698, Final Batch Loss: 0.6727967858314514\n",
      "Epoch 729, Loss: 1.5713709890842438, Final Batch Loss: 0.4594172537326813\n",
      "Epoch 730, Loss: 1.6200492680072784, Final Batch Loss: 0.6260349750518799\n",
      "Epoch 731, Loss: 1.386653333902359, Final Batch Loss: 0.34776124358177185\n",
      "Epoch 732, Loss: 1.619331419467926, Final Batch Loss: 0.5000408291816711\n",
      "Epoch 733, Loss: 1.72325599193573, Final Batch Loss: 0.6710518002510071\n",
      "Epoch 734, Loss: 1.5379811227321625, Final Batch Loss: 0.4657587707042694\n",
      "Epoch 735, Loss: 1.5264061987400055, Final Batch Loss: 0.4833347499370575\n",
      "Epoch 736, Loss: 1.5987729728221893, Final Batch Loss: 0.6126818656921387\n",
      "Epoch 737, Loss: 1.5904821157455444, Final Batch Loss: 0.6009078025817871\n",
      "Epoch 738, Loss: 1.5994486510753632, Final Batch Loss: 0.6385281682014465\n",
      "Epoch 739, Loss: 1.591730535030365, Final Batch Loss: 0.5392667651176453\n",
      "Epoch 740, Loss: 1.451689213514328, Final Batch Loss: 0.35223814845085144\n",
      "Epoch 741, Loss: 1.563252478837967, Final Batch Loss: 0.5116598010063171\n",
      "Epoch 742, Loss: 1.735697865486145, Final Batch Loss: 0.75846266746521\n",
      "Epoch 743, Loss: 1.6514504551887512, Final Batch Loss: 0.668130099773407\n",
      "Epoch 744, Loss: 1.6386635303497314, Final Batch Loss: 0.5910868048667908\n",
      "Epoch 745, Loss: 1.259448915719986, Final Batch Loss: 0.31405314803123474\n",
      "Epoch 746, Loss: 1.3692525625228882, Final Batch Loss: 0.4184349775314331\n",
      "Epoch 747, Loss: 1.6763975918293, Final Batch Loss: 0.7410406470298767\n",
      "Epoch 748, Loss: 1.3414474725723267, Final Batch Loss: 0.33891427516937256\n",
      "Epoch 749, Loss: 1.4734062552452087, Final Batch Loss: 0.4549741744995117\n",
      "Epoch 750, Loss: 1.501116544008255, Final Batch Loss: 0.4302050471305847\n",
      "Epoch 751, Loss: 1.3497712910175323, Final Batch Loss: 0.37108826637268066\n",
      "Epoch 752, Loss: 1.3284552097320557, Final Batch Loss: 0.36728793382644653\n",
      "Epoch 753, Loss: 1.4257487952709198, Final Batch Loss: 0.4875791072845459\n",
      "Epoch 754, Loss: 1.2423361092805862, Final Batch Loss: 0.24424390494823456\n",
      "Epoch 755, Loss: 1.385281890630722, Final Batch Loss: 0.36025241017341614\n",
      "Epoch 756, Loss: 1.295301467180252, Final Batch Loss: 0.2710612714290619\n",
      "Epoch 757, Loss: 1.5100285410881042, Final Batch Loss: 0.5211890339851379\n",
      "Epoch 758, Loss: 1.76821368932724, Final Batch Loss: 0.7618776559829712\n",
      "Epoch 759, Loss: 1.5365798473358154, Final Batch Loss: 0.6003945469856262\n",
      "Epoch 760, Loss: 1.4314066171646118, Final Batch Loss: 0.48624157905578613\n",
      "Epoch 761, Loss: 1.2746797800064087, Final Batch Loss: 0.23907598853111267\n",
      "Epoch 762, Loss: 1.3673529624938965, Final Batch Loss: 0.37954768538475037\n",
      "Epoch 763, Loss: 1.7550618052482605, Final Batch Loss: 0.8584892749786377\n",
      "Epoch 764, Loss: 1.4908219873905182, Final Batch Loss: 0.5259296298027039\n",
      "Epoch 765, Loss: 1.4523370862007141, Final Batch Loss: 0.47408705949783325\n",
      "Epoch 766, Loss: 1.3316276669502258, Final Batch Loss: 0.3459731638431549\n",
      "Epoch 767, Loss: 1.4055768549442291, Final Batch Loss: 0.41662970185279846\n",
      "Epoch 768, Loss: 1.3147968351840973, Final Batch Loss: 0.2948094606399536\n",
      "Epoch 769, Loss: 1.4077699184417725, Final Batch Loss: 0.45637643337249756\n",
      "Epoch 770, Loss: 1.7576733529567719, Final Batch Loss: 0.7229873538017273\n",
      "Epoch 771, Loss: 1.5564440488815308, Final Batch Loss: 0.517952561378479\n",
      "Epoch 772, Loss: 1.3903773725032806, Final Batch Loss: 0.36803504824638367\n",
      "Epoch 773, Loss: 1.470937728881836, Final Batch Loss: 0.4998473525047302\n",
      "Epoch 774, Loss: 1.4599272012710571, Final Batch Loss: 0.4717617928981781\n",
      "Epoch 775, Loss: 1.5620635747909546, Final Batch Loss: 0.5775818824768066\n",
      "Epoch 776, Loss: 1.4626828730106354, Final Batch Loss: 0.47800299525260925\n",
      "Epoch 777, Loss: 1.3332977890968323, Final Batch Loss: 0.3877466917037964\n",
      "Epoch 778, Loss: 1.3941424489021301, Final Batch Loss: 0.4561882019042969\n",
      "Epoch 779, Loss: 1.3403643071651459, Final Batch Loss: 0.3921935558319092\n",
      "Epoch 780, Loss: 1.5650630295276642, Final Batch Loss: 0.5437032580375671\n",
      "Epoch 781, Loss: 1.4235509634017944, Final Batch Loss: 0.47382017970085144\n",
      "Epoch 782, Loss: 1.487379938364029, Final Batch Loss: 0.46354368329048157\n",
      "Epoch 783, Loss: 1.6170771718025208, Final Batch Loss: 0.6336140036582947\n",
      "Epoch 784, Loss: 1.372173011302948, Final Batch Loss: 0.46335020661354065\n",
      "Epoch 785, Loss: 1.4531191885471344, Final Batch Loss: 0.4794215261936188\n",
      "Epoch 786, Loss: 1.3398564457893372, Final Batch Loss: 0.33882564306259155\n",
      "Epoch 787, Loss: 1.4724541008472443, Final Batch Loss: 0.5214430093765259\n",
      "Epoch 788, Loss: 1.3968779742717743, Final Batch Loss: 0.4180872440338135\n",
      "Epoch 789, Loss: 1.2720949053764343, Final Batch Loss: 0.35491853952407837\n",
      "Epoch 790, Loss: 1.725418508052826, Final Batch Loss: 0.7400105595588684\n",
      "Epoch 791, Loss: 1.3113140165805817, Final Batch Loss: 0.2991583049297333\n",
      "Epoch 792, Loss: 1.4789016544818878, Final Batch Loss: 0.5804355144500732\n",
      "Epoch 793, Loss: 1.434164434671402, Final Batch Loss: 0.44885706901550293\n",
      "Epoch 794, Loss: 1.410724014043808, Final Batch Loss: 0.44283807277679443\n",
      "Epoch 795, Loss: 1.6562772691249847, Final Batch Loss: 0.7581055760383606\n",
      "Epoch 796, Loss: 1.430238962173462, Final Batch Loss: 0.53867107629776\n",
      "Epoch 797, Loss: 1.25181382894516, Final Batch Loss: 0.27401599287986755\n",
      "Epoch 798, Loss: 1.4535593390464783, Final Batch Loss: 0.5170449018478394\n",
      "Epoch 799, Loss: 1.6108624339103699, Final Batch Loss: 0.5777251124382019\n",
      "Epoch 800, Loss: 1.3088005185127258, Final Batch Loss: 0.41032132506370544\n",
      "Epoch 801, Loss: 1.4546626508235931, Final Batch Loss: 0.5150783658027649\n",
      "Epoch 802, Loss: 1.37672558426857, Final Batch Loss: 0.3586253821849823\n",
      "Epoch 803, Loss: 1.3185243904590607, Final Batch Loss: 0.35232090950012207\n",
      "Epoch 804, Loss: 1.423926293849945, Final Batch Loss: 0.49993520975112915\n",
      "Epoch 805, Loss: 1.3323781788349152, Final Batch Loss: 0.29293617606163025\n",
      "Epoch 806, Loss: 1.3879934251308441, Final Batch Loss: 0.4761451780796051\n",
      "Epoch 807, Loss: 1.5461117327213287, Final Batch Loss: 0.5586086511611938\n",
      "Epoch 808, Loss: 1.160602554678917, Final Batch Loss: 0.17071254551410675\n",
      "Epoch 809, Loss: 1.3219445645809174, Final Batch Loss: 0.3636949360370636\n",
      "Epoch 810, Loss: 1.3544965088367462, Final Batch Loss: 0.3619793653488159\n",
      "Epoch 811, Loss: 1.4821965396404266, Final Batch Loss: 0.5095936059951782\n",
      "Epoch 812, Loss: 1.4605906307697296, Final Batch Loss: 0.5350921750068665\n",
      "Epoch 813, Loss: 1.323446363210678, Final Batch Loss: 0.33720654249191284\n",
      "Epoch 814, Loss: 1.5431033968925476, Final Batch Loss: 0.5729027390480042\n",
      "Epoch 815, Loss: 1.4431264102458954, Final Batch Loss: 0.481006920337677\n",
      "Epoch 816, Loss: 1.4401353895664215, Final Batch Loss: 0.440621942281723\n",
      "Epoch 817, Loss: 1.43226957321167, Final Batch Loss: 0.46797290444374084\n",
      "Epoch 818, Loss: 1.4176321923732758, Final Batch Loss: 0.46393460035324097\n",
      "Epoch 819, Loss: 1.2974083721637726, Final Batch Loss: 0.36696919798851013\n",
      "Epoch 820, Loss: 1.6285173296928406, Final Batch Loss: 0.7236757874488831\n",
      "Epoch 821, Loss: 1.768151968717575, Final Batch Loss: 0.8261425495147705\n",
      "Epoch 822, Loss: 1.3386663794517517, Final Batch Loss: 0.3959989845752716\n",
      "Epoch 823, Loss: 1.408192127943039, Final Batch Loss: 0.45080050826072693\n",
      "Epoch 824, Loss: 1.4734108746051788, Final Batch Loss: 0.496677041053772\n",
      "Epoch 825, Loss: 1.298487812280655, Final Batch Loss: 0.30181020498275757\n",
      "Epoch 826, Loss: 1.3580740988254547, Final Batch Loss: 0.38822081685066223\n",
      "Epoch 827, Loss: 1.4496676623821259, Final Batch Loss: 0.4359087646007538\n",
      "Epoch 828, Loss: 1.424761414527893, Final Batch Loss: 0.4724951684474945\n",
      "Epoch 829, Loss: 1.6264725029468536, Final Batch Loss: 0.6796333193778992\n",
      "Epoch 830, Loss: 1.5172504782676697, Final Batch Loss: 0.5701204538345337\n",
      "Epoch 831, Loss: 1.4121816158294678, Final Batch Loss: 0.4319448471069336\n",
      "Epoch 832, Loss: 1.610647737979889, Final Batch Loss: 0.6183082461357117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 833, Loss: 1.470682293176651, Final Batch Loss: 0.5155634880065918\n",
      "Epoch 834, Loss: 1.410218507051468, Final Batch Loss: 0.4960308074951172\n",
      "Epoch 835, Loss: 1.4570623934268951, Final Batch Loss: 0.5344264507293701\n",
      "Epoch 836, Loss: 1.3133151829242706, Final Batch Loss: 0.4102727770805359\n",
      "Epoch 837, Loss: 1.259329617023468, Final Batch Loss: 0.345157265663147\n",
      "Epoch 838, Loss: 1.172047957777977, Final Batch Loss: 0.19939281046390533\n",
      "Epoch 839, Loss: 1.6252447664737701, Final Batch Loss: 0.7337371706962585\n",
      "Epoch 840, Loss: 1.5271019637584686, Final Batch Loss: 0.6065645813941956\n",
      "Epoch 841, Loss: 1.423428624868393, Final Batch Loss: 0.4991820454597473\n",
      "Epoch 842, Loss: 1.611154466867447, Final Batch Loss: 0.7026229500770569\n",
      "Epoch 843, Loss: 1.439673662185669, Final Batch Loss: 0.5549708008766174\n",
      "Epoch 844, Loss: 1.3150800168514252, Final Batch Loss: 0.40172523260116577\n",
      "Epoch 845, Loss: 1.3338307738304138, Final Batch Loss: 0.35686376690864563\n",
      "Epoch 846, Loss: 1.4344549775123596, Final Batch Loss: 0.493746817111969\n",
      "Epoch 847, Loss: 1.2074498236179352, Final Batch Loss: 0.29236894845962524\n",
      "Epoch 848, Loss: 1.2878408432006836, Final Batch Loss: 0.38631966710090637\n",
      "Epoch 849, Loss: 1.3552684485912323, Final Batch Loss: 0.4819788634777069\n",
      "Epoch 850, Loss: 1.2964096665382385, Final Batch Loss: 0.37534070014953613\n",
      "Epoch 851, Loss: 1.4477371573448181, Final Batch Loss: 0.5189045071601868\n",
      "Epoch 852, Loss: 1.5676676630973816, Final Batch Loss: 0.56207674741745\n",
      "Epoch 853, Loss: 1.348350167274475, Final Batch Loss: 0.41175055503845215\n",
      "Epoch 854, Loss: 1.5871738195419312, Final Batch Loss: 0.590570330619812\n",
      "Epoch 855, Loss: 1.3967185318470001, Final Batch Loss: 0.4541555345058441\n",
      "Epoch 856, Loss: 1.1898100078105927, Final Batch Loss: 0.26939359307289124\n",
      "Epoch 857, Loss: 1.1769077777862549, Final Batch Loss: 0.24088352918624878\n",
      "Epoch 858, Loss: 1.6360092759132385, Final Batch Loss: 0.7027831077575684\n",
      "Epoch 859, Loss: 1.284602165222168, Final Batch Loss: 0.4056473672389984\n",
      "Epoch 860, Loss: 1.166415810585022, Final Batch Loss: 0.26060986518859863\n",
      "Epoch 861, Loss: 1.3769388496875763, Final Batch Loss: 0.4493596851825714\n",
      "Epoch 862, Loss: 1.558190405368805, Final Batch Loss: 0.6364209055900574\n",
      "Epoch 863, Loss: 1.5632844865322113, Final Batch Loss: 0.6955267786979675\n",
      "Epoch 864, Loss: 1.3794169127941132, Final Batch Loss: 0.4672963619232178\n",
      "Epoch 865, Loss: 1.3271223902702332, Final Batch Loss: 0.48588651418685913\n",
      "Epoch 866, Loss: 1.705470085144043, Final Batch Loss: 0.7689080238342285\n",
      "Epoch 867, Loss: 1.6052577793598175, Final Batch Loss: 0.6881207823753357\n",
      "Epoch 868, Loss: 1.4338942766189575, Final Batch Loss: 0.5194035768508911\n",
      "Epoch 869, Loss: 1.2441185414791107, Final Batch Loss: 0.3636302053928375\n",
      "Epoch 870, Loss: 1.2767791748046875, Final Batch Loss: 0.3024805188179016\n",
      "Epoch 871, Loss: 1.4190156757831573, Final Batch Loss: 0.4630209803581238\n",
      "Epoch 872, Loss: 1.3075118660926819, Final Batch Loss: 0.3754051923751831\n",
      "Epoch 873, Loss: 1.5575090050697327, Final Batch Loss: 0.6326034665107727\n",
      "Epoch 874, Loss: 1.1852076649665833, Final Batch Loss: 0.2511459290981293\n",
      "Epoch 875, Loss: 1.5022222697734833, Final Batch Loss: 0.5985440015792847\n",
      "Epoch 876, Loss: 1.4955816566944122, Final Batch Loss: 0.5837051272392273\n",
      "Epoch 877, Loss: 1.7190929651260376, Final Batch Loss: 0.7539007067680359\n",
      "Epoch 878, Loss: 1.3157569766044617, Final Batch Loss: 0.4408942759037018\n",
      "Epoch 879, Loss: 1.4178280234336853, Final Batch Loss: 0.49167323112487793\n",
      "Epoch 880, Loss: 1.4463913142681122, Final Batch Loss: 0.5580002069473267\n",
      "Epoch 881, Loss: 1.424323946237564, Final Batch Loss: 0.4710051417350769\n",
      "Epoch 882, Loss: 1.2369378209114075, Final Batch Loss: 0.30948418378829956\n",
      "Epoch 883, Loss: 1.1829861104488373, Final Batch Loss: 0.29419228434562683\n",
      "Epoch 884, Loss: 1.436845600605011, Final Batch Loss: 0.5062170028686523\n",
      "Epoch 885, Loss: 1.4587953686714172, Final Batch Loss: 0.6168568730354309\n",
      "Epoch 886, Loss: 1.4783296883106232, Final Batch Loss: 0.5349742770195007\n",
      "Epoch 887, Loss: 1.212473213672638, Final Batch Loss: 0.30809441208839417\n",
      "Epoch 888, Loss: 1.175243467092514, Final Batch Loss: 0.2842619717121124\n",
      "Epoch 889, Loss: 1.2863819003105164, Final Batch Loss: 0.3476485311985016\n",
      "Epoch 890, Loss: 1.2266547977924347, Final Batch Loss: 0.3173709213733673\n",
      "Epoch 891, Loss: 1.3406277000904083, Final Batch Loss: 0.43429049849510193\n",
      "Epoch 892, Loss: 1.457400619983673, Final Batch Loss: 0.5768887400627136\n",
      "Epoch 893, Loss: 1.2076413035392761, Final Batch Loss: 0.368988573551178\n",
      "Epoch 894, Loss: 1.441274881362915, Final Batch Loss: 0.5707336664199829\n",
      "Epoch 895, Loss: 1.2167710363864899, Final Batch Loss: 0.2675844132900238\n",
      "Epoch 896, Loss: 1.1142206937074661, Final Batch Loss: 0.17130924761295319\n",
      "Epoch 897, Loss: 1.2390974462032318, Final Batch Loss: 0.276327520608902\n",
      "Epoch 898, Loss: 1.1330225467681885, Final Batch Loss: 0.31977033615112305\n",
      "Epoch 899, Loss: 1.5978024899959564, Final Batch Loss: 0.7423309683799744\n",
      "Epoch 900, Loss: 1.413474977016449, Final Batch Loss: 0.5375804305076599\n",
      "Epoch 901, Loss: 1.4526475667953491, Final Batch Loss: 0.615362823009491\n",
      "Epoch 902, Loss: 1.203624665737152, Final Batch Loss: 0.29535984992980957\n",
      "Epoch 903, Loss: 1.368175595998764, Final Batch Loss: 0.49161314964294434\n",
      "Epoch 904, Loss: 1.2173803746700287, Final Batch Loss: 0.3517339825630188\n",
      "Epoch 905, Loss: 1.4460207521915436, Final Batch Loss: 0.5554709434509277\n",
      "Epoch 906, Loss: 1.2302300930023193, Final Batch Loss: 0.32521897554397583\n",
      "Epoch 907, Loss: 1.438709020614624, Final Batch Loss: 0.5177664756774902\n",
      "Epoch 908, Loss: 1.355429083108902, Final Batch Loss: 0.5184253454208374\n",
      "Epoch 909, Loss: 1.432802975177765, Final Batch Loss: 0.5740581750869751\n",
      "Epoch 910, Loss: 1.194847822189331, Final Batch Loss: 0.28826484084129333\n",
      "Epoch 911, Loss: 1.2378262877464294, Final Batch Loss: 0.3601309359073639\n",
      "Epoch 912, Loss: 1.129906713962555, Final Batch Loss: 0.287009596824646\n",
      "Epoch 913, Loss: 1.3033567368984222, Final Batch Loss: 0.3895839750766754\n",
      "Epoch 914, Loss: 1.2982123494148254, Final Batch Loss: 0.39364901185035706\n",
      "Epoch 915, Loss: 1.4929750263690948, Final Batch Loss: 0.5437216758728027\n",
      "Epoch 916, Loss: 1.1932337582111359, Final Batch Loss: 0.2927078604698181\n",
      "Epoch 917, Loss: 1.225274384021759, Final Batch Loss: 0.3856295347213745\n",
      "Epoch 918, Loss: 1.7043496370315552, Final Batch Loss: 0.8051301836967468\n",
      "Epoch 919, Loss: 1.2202226221561432, Final Batch Loss: 0.35612714290618896\n",
      "Epoch 920, Loss: 1.1967706680297852, Final Batch Loss: 0.27235493063926697\n",
      "Epoch 921, Loss: 1.3281264901161194, Final Batch Loss: 0.3845759332180023\n",
      "Epoch 922, Loss: 1.396383285522461, Final Batch Loss: 0.41746437549591064\n",
      "Epoch 923, Loss: 1.4438920617103577, Final Batch Loss: 0.5885001420974731\n",
      "Epoch 924, Loss: 1.3713915348052979, Final Batch Loss: 0.4444940984249115\n",
      "Epoch 925, Loss: 1.2653879523277283, Final Batch Loss: 0.36438682675361633\n",
      "Epoch 926, Loss: 1.48960480093956, Final Batch Loss: 0.5834062099456787\n",
      "Epoch 927, Loss: 1.2899988889694214, Final Batch Loss: 0.38931432366371155\n",
      "Epoch 928, Loss: 1.4044395089149475, Final Batch Loss: 0.543640673160553\n",
      "Epoch 929, Loss: 1.3011931777000427, Final Batch Loss: 0.47659558057785034\n",
      "Epoch 930, Loss: 1.3111414909362793, Final Batch Loss: 0.39374908804893494\n",
      "Epoch 931, Loss: 1.2227162420749664, Final Batch Loss: 0.32605722546577454\n",
      "Epoch 932, Loss: 1.2044402658939362, Final Batch Loss: 0.33176520466804504\n",
      "Epoch 933, Loss: 1.3291839957237244, Final Batch Loss: 0.45088011026382446\n",
      "Epoch 934, Loss: 1.1179611831903458, Final Batch Loss: 0.21369032561779022\n",
      "Epoch 935, Loss: 1.3323457837104797, Final Batch Loss: 0.4655141830444336\n",
      "Epoch 936, Loss: 1.1860661804676056, Final Batch Loss: 0.2650200128555298\n",
      "Epoch 937, Loss: 1.45553857088089, Final Batch Loss: 0.5758739709854126\n",
      "Epoch 938, Loss: 1.1146823465824127, Final Batch Loss: 0.24402299523353577\n",
      "Epoch 939, Loss: 1.3113557398319244, Final Batch Loss: 0.44176506996154785\n",
      "Epoch 940, Loss: 1.5532236993312836, Final Batch Loss: 0.6577457189559937\n",
      "Epoch 941, Loss: 1.4112416207790375, Final Batch Loss: 0.5198901891708374\n",
      "Epoch 942, Loss: 1.29582479596138, Final Batch Loss: 0.304755300283432\n",
      "Epoch 943, Loss: 1.5980590283870697, Final Batch Loss: 0.6900891661643982\n",
      "Epoch 944, Loss: 1.349221408367157, Final Batch Loss: 0.4105369448661804\n",
      "Epoch 945, Loss: 1.3066205978393555, Final Batch Loss: 0.41855451464653015\n",
      "Epoch 946, Loss: 1.1842179000377655, Final Batch Loss: 0.26034975051879883\n",
      "Epoch 947, Loss: 1.235693246126175, Final Batch Loss: 0.37687447667121887\n",
      "Epoch 948, Loss: 1.2790398597717285, Final Batch Loss: 0.32674193382263184\n",
      "Epoch 949, Loss: 1.5949971973896027, Final Batch Loss: 0.6851019859313965\n",
      "Epoch 950, Loss: 1.2891020476818085, Final Batch Loss: 0.4360091984272003\n",
      "Epoch 951, Loss: 1.189358651638031, Final Batch Loss: 0.2648982107639313\n",
      "Epoch 952, Loss: 1.7895171642303467, Final Batch Loss: 0.8132135272026062\n",
      "Epoch 953, Loss: 1.701810598373413, Final Batch Loss: 0.8252975344657898\n",
      "Epoch 954, Loss: 1.5503897070884705, Final Batch Loss: 0.6788169741630554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 955, Loss: 1.5196517407894135, Final Batch Loss: 0.6454699635505676\n",
      "Epoch 956, Loss: 1.4424238502979279, Final Batch Loss: 0.48761168122291565\n",
      "Epoch 957, Loss: 1.413595050573349, Final Batch Loss: 0.39649781584739685\n",
      "Epoch 958, Loss: 1.337997853755951, Final Batch Loss: 0.41922083497047424\n",
      "Epoch 959, Loss: 1.1498874127864838, Final Batch Loss: 0.29849082231521606\n",
      "Epoch 960, Loss: 1.2952357530593872, Final Batch Loss: 0.4441763162612915\n",
      "Epoch 961, Loss: 1.2279143035411835, Final Batch Loss: 0.3579261302947998\n",
      "Epoch 962, Loss: 1.3247837126255035, Final Batch Loss: 0.43515315651893616\n",
      "Epoch 963, Loss: 1.4294835329055786, Final Batch Loss: 0.5881605744361877\n",
      "Epoch 964, Loss: 1.5590317845344543, Final Batch Loss: 0.7682259678840637\n",
      "Epoch 965, Loss: 1.2587236762046814, Final Batch Loss: 0.3943637013435364\n",
      "Epoch 966, Loss: 1.291303664445877, Final Batch Loss: 0.4496114253997803\n",
      "Epoch 967, Loss: 1.4650084972381592, Final Batch Loss: 0.6133897304534912\n",
      "Epoch 968, Loss: 1.3441638350486755, Final Batch Loss: 0.4881401062011719\n",
      "Epoch 969, Loss: 1.366005688905716, Final Batch Loss: 0.3931637704372406\n",
      "Epoch 970, Loss: 1.3530411124229431, Final Batch Loss: 0.4801992177963257\n",
      "Epoch 971, Loss: 1.2024838626384735, Final Batch Loss: 0.313047856092453\n",
      "Epoch 972, Loss: 1.1931922435760498, Final Batch Loss: 0.32433903217315674\n",
      "Epoch 973, Loss: 1.2013388574123383, Final Batch Loss: 0.29763391613960266\n",
      "Epoch 974, Loss: 1.220308780670166, Final Batch Loss: 0.43102067708969116\n",
      "Epoch 975, Loss: 1.2361715734004974, Final Batch Loss: 0.3503623902797699\n",
      "Epoch 976, Loss: 1.0448591113090515, Final Batch Loss: 0.17481917142868042\n",
      "Epoch 977, Loss: 1.394808977842331, Final Batch Loss: 0.517350971698761\n",
      "Epoch 978, Loss: 1.1908231675624847, Final Batch Loss: 0.32099080085754395\n",
      "Epoch 979, Loss: 1.4435154497623444, Final Batch Loss: 0.5860825777053833\n",
      "Epoch 980, Loss: 1.2549658417701721, Final Batch Loss: 0.3623906970024109\n",
      "Epoch 981, Loss: 1.568785160779953, Final Batch Loss: 0.680733323097229\n",
      "Epoch 982, Loss: 1.125634104013443, Final Batch Loss: 0.2546522915363312\n",
      "Epoch 983, Loss: 1.3287633061408997, Final Batch Loss: 0.4255374073982239\n",
      "Epoch 984, Loss: 1.200874924659729, Final Batch Loss: 0.33168044686317444\n",
      "Epoch 985, Loss: 1.311915636062622, Final Batch Loss: 0.448271244764328\n",
      "Epoch 986, Loss: 1.4653927385807037, Final Batch Loss: 0.5950891375541687\n",
      "Epoch 987, Loss: 1.293354868888855, Final Batch Loss: 0.42660677433013916\n",
      "Epoch 988, Loss: 1.3407778441905975, Final Batch Loss: 0.4811801016330719\n",
      "Epoch 989, Loss: 1.4835620522499084, Final Batch Loss: 0.5954629778862\n",
      "Epoch 990, Loss: 1.2954970598220825, Final Batch Loss: 0.43520089983940125\n",
      "Epoch 991, Loss: 1.1394789814949036, Final Batch Loss: 0.30106407403945923\n",
      "Epoch 992, Loss: 1.2155734598636627, Final Batch Loss: 0.3184455931186676\n",
      "Epoch 993, Loss: 1.0915745198726654, Final Batch Loss: 0.27082034945487976\n",
      "Epoch 994, Loss: 1.4308169484138489, Final Batch Loss: 0.6387616991996765\n",
      "Epoch 995, Loss: 1.2746323943138123, Final Batch Loss: 0.46934548020362854\n",
      "Epoch 996, Loss: 1.169596642255783, Final Batch Loss: 0.3361939489841461\n",
      "Epoch 997, Loss: 1.452406257390976, Final Batch Loss: 0.5985099077224731\n",
      "Epoch 998, Loss: 1.3649958670139313, Final Batch Loss: 0.4236847162246704\n",
      "Epoch 999, Loss: 1.323310762643814, Final Batch Loss: 0.40994128584861755\n",
      "Epoch 1000, Loss: 1.2162048816680908, Final Batch Loss: 0.39712560176849365\n",
      "Epoch 1001, Loss: 1.2764054834842682, Final Batch Loss: 0.39969757199287415\n",
      "Epoch 1002, Loss: 1.1116485893726349, Final Batch Loss: 0.27371329069137573\n",
      "Epoch 1003, Loss: 1.2027287483215332, Final Batch Loss: 0.3252297341823578\n",
      "Epoch 1004, Loss: 1.4772080779075623, Final Batch Loss: 0.6282868385314941\n",
      "Epoch 1005, Loss: 1.2158379256725311, Final Batch Loss: 0.358720988035202\n",
      "Epoch 1006, Loss: 1.4759465456008911, Final Batch Loss: 0.5883115530014038\n",
      "Epoch 1007, Loss: 1.2716608047485352, Final Batch Loss: 0.38606879115104675\n",
      "Epoch 1008, Loss: 1.2353553175926208, Final Batch Loss: 0.3890749216079712\n",
      "Epoch 1009, Loss: 1.5035587847232819, Final Batch Loss: 0.6104702353477478\n",
      "Epoch 1010, Loss: 1.3777215480804443, Final Batch Loss: 0.48364904522895813\n",
      "Epoch 1011, Loss: 1.01772952824831, Final Batch Loss: 0.11800882965326309\n",
      "Epoch 1012, Loss: 1.4710992276668549, Final Batch Loss: 0.571693480014801\n",
      "Epoch 1013, Loss: 1.1727913320064545, Final Batch Loss: 0.3353782892227173\n",
      "Epoch 1014, Loss: 1.4613944590091705, Final Batch Loss: 0.5554788708686829\n",
      "Epoch 1015, Loss: 1.0684402137994766, Final Batch Loss: 0.24982188642024994\n",
      "Epoch 1016, Loss: 1.3252843916416168, Final Batch Loss: 0.47333166003227234\n",
      "Epoch 1017, Loss: 1.1533161103725433, Final Batch Loss: 0.33491724729537964\n",
      "Epoch 1018, Loss: 1.2935459613800049, Final Batch Loss: 0.42456215620040894\n",
      "Epoch 1019, Loss: 1.1060735285282135, Final Batch Loss: 0.2628749907016754\n",
      "Epoch 1020, Loss: 1.4351826310157776, Final Batch Loss: 0.5588661432266235\n",
      "Epoch 1021, Loss: 1.3216191232204437, Final Batch Loss: 0.4170774519443512\n",
      "Epoch 1022, Loss: 1.5086931586265564, Final Batch Loss: 0.6269866824150085\n",
      "Epoch 1023, Loss: 1.5492472648620605, Final Batch Loss: 0.7494968771934509\n",
      "Epoch 1024, Loss: 1.0842689126729965, Final Batch Loss: 0.2444298416376114\n",
      "Epoch 1025, Loss: 1.276168167591095, Final Batch Loss: 0.4489743709564209\n",
      "Epoch 1026, Loss: 1.7860144078731537, Final Batch Loss: 0.9680406451225281\n",
      "Epoch 1027, Loss: 1.4004133939743042, Final Batch Loss: 0.5778971910476685\n",
      "Epoch 1028, Loss: 1.0597472488880157, Final Batch Loss: 0.15373989939689636\n",
      "Epoch 1029, Loss: 1.1689241230487823, Final Batch Loss: 0.33908820152282715\n",
      "Epoch 1030, Loss: 1.1770207285881042, Final Batch Loss: 0.3408002555370331\n",
      "Epoch 1031, Loss: 1.1326520442962646, Final Batch Loss: 0.35339459776878357\n",
      "Epoch 1032, Loss: 1.4792298078536987, Final Batch Loss: 0.5956438183784485\n",
      "Epoch 1033, Loss: 1.3165333569049835, Final Batch Loss: 0.424017995595932\n",
      "Epoch 1034, Loss: 1.0804297477006912, Final Batch Loss: 0.17771895229816437\n",
      "Epoch 1035, Loss: 1.1305291652679443, Final Batch Loss: 0.32801055908203125\n",
      "Epoch 1036, Loss: 1.3907392919063568, Final Batch Loss: 0.5637452602386475\n",
      "Epoch 1037, Loss: 1.0445084422826767, Final Batch Loss: 0.19780682027339935\n",
      "Epoch 1038, Loss: 1.1554145216941833, Final Batch Loss: 0.27624282240867615\n",
      "Epoch 1039, Loss: 1.190299153327942, Final Batch Loss: 0.39632731676101685\n",
      "Epoch 1040, Loss: 1.3622413277626038, Final Batch Loss: 0.5341631174087524\n",
      "Epoch 1041, Loss: 1.1126893162727356, Final Batch Loss: 0.29858121275901794\n",
      "Epoch 1042, Loss: 1.4217332601547241, Final Batch Loss: 0.5492119193077087\n",
      "Epoch 1043, Loss: 1.1456717252731323, Final Batch Loss: 0.29892489314079285\n",
      "Epoch 1044, Loss: 1.438256412744522, Final Batch Loss: 0.5660325288772583\n",
      "Epoch 1045, Loss: 1.4149348735809326, Final Batch Loss: 0.5667034983634949\n",
      "Epoch 1046, Loss: 1.174593061208725, Final Batch Loss: 0.4141485393047333\n",
      "Epoch 1047, Loss: 1.2886634469032288, Final Batch Loss: 0.44507941603660583\n",
      "Epoch 1048, Loss: 1.1819699108600616, Final Batch Loss: 0.3821339011192322\n",
      "Epoch 1049, Loss: 1.2171677649021149, Final Batch Loss: 0.34649837017059326\n",
      "Epoch 1050, Loss: 1.3530682027339935, Final Batch Loss: 0.5117797255516052\n",
      "Epoch 1051, Loss: 1.2129081189632416, Final Batch Loss: 0.3397831916809082\n",
      "Epoch 1052, Loss: 1.2112809121608734, Final Batch Loss: 0.35856106877326965\n",
      "Epoch 1053, Loss: 1.0071182698011398, Final Batch Loss: 0.19434170424938202\n",
      "Epoch 1054, Loss: 1.369142860174179, Final Batch Loss: 0.5052632093429565\n",
      "Epoch 1055, Loss: 1.0875879228115082, Final Batch Loss: 0.29071375727653503\n",
      "Epoch 1056, Loss: 1.383143812417984, Final Batch Loss: 0.49204620718955994\n",
      "Epoch 1057, Loss: 1.2814554870128632, Final Batch Loss: 0.5035184025764465\n",
      "Epoch 1058, Loss: 0.9718665331602097, Final Batch Loss: 0.16781948506832123\n",
      "Epoch 1059, Loss: 1.289221704006195, Final Batch Loss: 0.4043790400028229\n",
      "Epoch 1060, Loss: 1.3411490619182587, Final Batch Loss: 0.5231814384460449\n",
      "Epoch 1061, Loss: 1.4965264797210693, Final Batch Loss: 0.6595483422279358\n",
      "Epoch 1062, Loss: 1.3155269026756287, Final Batch Loss: 0.47044408321380615\n",
      "Epoch 1063, Loss: 1.2651533782482147, Final Batch Loss: 0.4371156692504883\n",
      "Epoch 1064, Loss: 1.1636481583118439, Final Batch Loss: 0.3112078309059143\n",
      "Epoch 1065, Loss: 1.5203735530376434, Final Batch Loss: 0.6396691799163818\n",
      "Epoch 1066, Loss: 1.1360127925872803, Final Batch Loss: 0.330612450838089\n",
      "Epoch 1067, Loss: 1.0943401157855988, Final Batch Loss: 0.3032241761684418\n",
      "Epoch 1068, Loss: 1.2568244636058807, Final Batch Loss: 0.46274039149284363\n",
      "Epoch 1069, Loss: 1.7118037045001984, Final Batch Loss: 0.8468418121337891\n",
      "Epoch 1070, Loss: 1.2730748653411865, Final Batch Loss: 0.4582844376564026\n",
      "Epoch 1071, Loss: 1.3809456527233124, Final Batch Loss: 0.48045483231544495\n",
      "Epoch 1072, Loss: 1.4652429521083832, Final Batch Loss: 0.6157875657081604\n",
      "Epoch 1073, Loss: 1.3214580714702606, Final Batch Loss: 0.5908070802688599\n",
      "Epoch 1074, Loss: 1.133448600769043, Final Batch Loss: 0.2789180278778076\n",
      "Epoch 1075, Loss: 1.3689000606536865, Final Batch Loss: 0.5046825408935547\n",
      "Epoch 1076, Loss: 1.319959282875061, Final Batch Loss: 0.4161471724510193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1077, Loss: 1.327939510345459, Final Batch Loss: 0.49086710810661316\n",
      "Epoch 1078, Loss: 1.2435298562049866, Final Batch Loss: 0.3936282992362976\n",
      "Epoch 1079, Loss: 1.0416203290224075, Final Batch Loss: 0.2397276908159256\n",
      "Epoch 1080, Loss: 1.2104957103729248, Final Batch Loss: 0.35244083404541016\n",
      "Epoch 1081, Loss: 1.2080605030059814, Final Batch Loss: 0.33318188786506653\n",
      "Epoch 1082, Loss: 1.347434937953949, Final Batch Loss: 0.48266395926475525\n",
      "Epoch 1083, Loss: 1.235037088394165, Final Batch Loss: 0.3323505222797394\n",
      "Epoch 1084, Loss: 1.4008136689662933, Final Batch Loss: 0.6172818541526794\n",
      "Epoch 1085, Loss: 1.1778971552848816, Final Batch Loss: 0.34654173254966736\n",
      "Epoch 1086, Loss: 1.1345899999141693, Final Batch Loss: 0.35266634821891785\n",
      "Epoch 1087, Loss: 1.4490914940834045, Final Batch Loss: 0.5513786673545837\n",
      "Epoch 1088, Loss: 1.2285272777080536, Final Batch Loss: 0.4093921184539795\n",
      "Epoch 1089, Loss: 1.2693987488746643, Final Batch Loss: 0.4619610011577606\n",
      "Epoch 1090, Loss: 1.6667851209640503, Final Batch Loss: 0.852294921875\n",
      "Epoch 1091, Loss: 1.4809184968471527, Final Batch Loss: 0.706106424331665\n",
      "Epoch 1092, Loss: 1.1709305942058563, Final Batch Loss: 0.345630407333374\n",
      "Epoch 1093, Loss: 1.1987156569957733, Final Batch Loss: 0.39345505833625793\n",
      "Epoch 1094, Loss: 1.1971740126609802, Final Batch Loss: 0.36698004603385925\n",
      "Epoch 1095, Loss: 0.9767656773328781, Final Batch Loss: 0.16177718341350555\n",
      "Epoch 1096, Loss: 1.6376228332519531, Final Batch Loss: 0.8459354043006897\n",
      "Epoch 1097, Loss: 1.431937426328659, Final Batch Loss: 0.6502087712287903\n",
      "Epoch 1098, Loss: 1.2630132734775543, Final Batch Loss: 0.4133601188659668\n",
      "Epoch 1099, Loss: 0.9972282648086548, Final Batch Loss: 0.1888296902179718\n",
      "Epoch 1100, Loss: 1.399819940328598, Final Batch Loss: 0.5838480591773987\n",
      "Epoch 1101, Loss: 0.9219280406832695, Final Batch Loss: 0.09542813152074814\n",
      "Epoch 1102, Loss: 1.192366123199463, Final Batch Loss: 0.3195081055164337\n",
      "Epoch 1103, Loss: 1.2234505116939545, Final Batch Loss: 0.37823954224586487\n",
      "Epoch 1104, Loss: 1.0952160060405731, Final Batch Loss: 0.3822203576564789\n",
      "Epoch 1105, Loss: 1.6179674863815308, Final Batch Loss: 0.7674351334571838\n",
      "Epoch 1106, Loss: 1.1598799526691437, Final Batch Loss: 0.3335716724395752\n",
      "Epoch 1107, Loss: 1.1629884839057922, Final Batch Loss: 0.29290246963500977\n",
      "Epoch 1108, Loss: 1.284883737564087, Final Batch Loss: 0.4682967960834503\n",
      "Epoch 1109, Loss: 1.207068294286728, Final Batch Loss: 0.3805994987487793\n",
      "Epoch 1110, Loss: 1.0790755152702332, Final Batch Loss: 0.29198944568634033\n",
      "Epoch 1111, Loss: 1.1639906466007233, Final Batch Loss: 0.35284584760665894\n",
      "Epoch 1112, Loss: 1.3092558085918427, Final Batch Loss: 0.46121126413345337\n",
      "Epoch 1113, Loss: 1.1471606492996216, Final Batch Loss: 0.33421316742897034\n",
      "Epoch 1114, Loss: 1.2653138637542725, Final Batch Loss: 0.4692726731300354\n",
      "Epoch 1115, Loss: 1.0735205560922623, Final Batch Loss: 0.24665121734142303\n",
      "Epoch 1116, Loss: 1.189904272556305, Final Batch Loss: 0.36998292803764343\n",
      "Epoch 1117, Loss: 1.2699042856693268, Final Batch Loss: 0.43137654662132263\n",
      "Epoch 1118, Loss: 1.2007248997688293, Final Batch Loss: 0.4546712636947632\n",
      "Epoch 1119, Loss: 1.5546443462371826, Final Batch Loss: 0.7771400809288025\n",
      "Epoch 1120, Loss: 1.2464984357357025, Final Batch Loss: 0.4399402141571045\n",
      "Epoch 1121, Loss: 1.28798508644104, Final Batch Loss: 0.502850353717804\n",
      "Epoch 1122, Loss: 1.1246196627616882, Final Batch Loss: 0.294744074344635\n",
      "Epoch 1123, Loss: 1.1050422936677933, Final Batch Loss: 0.23656119406223297\n",
      "Epoch 1124, Loss: 1.2706465125083923, Final Batch Loss: 0.4132969379425049\n",
      "Epoch 1125, Loss: 1.1891147196292877, Final Batch Loss: 0.3502902090549469\n",
      "Epoch 1126, Loss: 1.38381028175354, Final Batch Loss: 0.5392278432846069\n",
      "Epoch 1127, Loss: 1.0774889290332794, Final Batch Loss: 0.27218079566955566\n",
      "Epoch 1128, Loss: 1.068985030055046, Final Batch Loss: 0.11972720921039581\n",
      "Epoch 1129, Loss: 1.2949668765068054, Final Batch Loss: 0.5518882870674133\n",
      "Epoch 1130, Loss: 1.2798821330070496, Final Batch Loss: 0.41838112473487854\n",
      "Epoch 1131, Loss: 0.9865361899137497, Final Batch Loss: 0.14959608018398285\n",
      "Epoch 1132, Loss: 1.3049113154411316, Final Batch Loss: 0.4127088189125061\n",
      "Epoch 1133, Loss: 1.1060782372951508, Final Batch Loss: 0.333509236574173\n",
      "Epoch 1134, Loss: 1.0792436003684998, Final Batch Loss: 0.30933263897895813\n",
      "Epoch 1135, Loss: 0.9463736861944199, Final Batch Loss: 0.1568463295698166\n",
      "Epoch 1136, Loss: 1.2641428112983704, Final Batch Loss: 0.4783361256122589\n",
      "Epoch 1137, Loss: 1.2274974584579468, Final Batch Loss: 0.45718926191329956\n",
      "Epoch 1138, Loss: 1.0761065036058426, Final Batch Loss: 0.23012204468250275\n",
      "Epoch 1139, Loss: 1.298971265554428, Final Batch Loss: 0.4975860118865967\n",
      "Epoch 1140, Loss: 1.2434780597686768, Final Batch Loss: 0.41893652081489563\n",
      "Epoch 1141, Loss: 1.1667732894420624, Final Batch Loss: 0.3476009666919708\n",
      "Epoch 1142, Loss: 1.0448399186134338, Final Batch Loss: 0.2787463665008545\n",
      "Epoch 1143, Loss: 1.097228467464447, Final Batch Loss: 0.2869185209274292\n",
      "Epoch 1144, Loss: 1.1447846591472626, Final Batch Loss: 0.3216497600078583\n",
      "Epoch 1145, Loss: 1.0756325125694275, Final Batch Loss: 0.16877570748329163\n",
      "Epoch 1146, Loss: 1.3183874189853668, Final Batch Loss: 0.3552655577659607\n",
      "Epoch 1147, Loss: 1.2557323575019836, Final Batch Loss: 0.4569404423236847\n",
      "Epoch 1148, Loss: 1.197038620710373, Final Batch Loss: 0.38784995675086975\n",
      "Epoch 1149, Loss: 1.0644730627536774, Final Batch Loss: 0.27856212854385376\n",
      "Epoch 1150, Loss: 1.1967471837997437, Final Batch Loss: 0.39629843831062317\n",
      "Epoch 1151, Loss: 1.0519894659519196, Final Batch Loss: 0.2799355983734131\n",
      "Epoch 1152, Loss: 1.2600487768650055, Final Batch Loss: 0.3991773724555969\n",
      "Epoch 1153, Loss: 1.1136404275894165, Final Batch Loss: 0.35998326539993286\n",
      "Epoch 1154, Loss: 1.2348559498786926, Final Batch Loss: 0.4926447570323944\n",
      "Epoch 1155, Loss: 1.2546307146549225, Final Batch Loss: 0.44225722551345825\n",
      "Epoch 1156, Loss: 1.1551759839057922, Final Batch Loss: 0.3316825032234192\n",
      "Epoch 1157, Loss: 1.1820862889289856, Final Batch Loss: 0.44976022839546204\n",
      "Epoch 1158, Loss: 1.1455650627613068, Final Batch Loss: 0.37415531277656555\n",
      "Epoch 1159, Loss: 1.2281821072101593, Final Batch Loss: 0.3969959020614624\n",
      "Epoch 1160, Loss: 1.0042165219783783, Final Batch Loss: 0.20210683345794678\n",
      "Epoch 1161, Loss: 1.0757098197937012, Final Batch Loss: 0.27098268270492554\n",
      "Epoch 1162, Loss: 1.4027049541473389, Final Batch Loss: 0.6289445161819458\n",
      "Epoch 1163, Loss: 1.091577559709549, Final Batch Loss: 0.2722454369068146\n",
      "Epoch 1164, Loss: 1.227672427892685, Final Batch Loss: 0.4720923900604248\n",
      "Epoch 1165, Loss: 1.3821376860141754, Final Batch Loss: 0.5947648882865906\n",
      "Epoch 1166, Loss: 1.2339237034320831, Final Batch Loss: 0.43653467297554016\n",
      "Epoch 1167, Loss: 1.1409908831119537, Final Batch Loss: 0.38463863730430603\n",
      "Epoch 1168, Loss: 1.2432373464107513, Final Batch Loss: 0.4609355032444\n",
      "Epoch 1169, Loss: 1.189586579799652, Final Batch Loss: 0.34437862038612366\n",
      "Epoch 1170, Loss: 1.0360690504312515, Final Batch Loss: 0.20034559071063995\n",
      "Epoch 1171, Loss: 1.1274564564228058, Final Batch Loss: 0.3996456265449524\n",
      "Epoch 1172, Loss: 1.0395450592041016, Final Batch Loss: 0.33798956871032715\n",
      "Epoch 1173, Loss: 1.048425167798996, Final Batch Loss: 0.32693663239479065\n",
      "Epoch 1174, Loss: 1.1401790082454681, Final Batch Loss: 0.35648706555366516\n",
      "Epoch 1175, Loss: 1.1713500022888184, Final Batch Loss: 0.37260326743125916\n",
      "Epoch 1176, Loss: 1.2881059050559998, Final Batch Loss: 0.5111546516418457\n",
      "Epoch 1177, Loss: 1.207541435956955, Final Batch Loss: 0.410259485244751\n",
      "Epoch 1178, Loss: 1.0973072052001953, Final Batch Loss: 0.3590218424797058\n",
      "Epoch 1179, Loss: 1.0682111084461212, Final Batch Loss: 0.2961837351322174\n",
      "Epoch 1180, Loss: 1.011998936533928, Final Batch Loss: 0.24396152794361115\n",
      "Epoch 1181, Loss: 1.0032827854156494, Final Batch Loss: 0.2748974859714508\n",
      "Epoch 1182, Loss: 1.168996423482895, Final Batch Loss: 0.35003066062927246\n",
      "Epoch 1183, Loss: 1.0262324512004852, Final Batch Loss: 0.25232046842575073\n",
      "Epoch 1184, Loss: 1.1866106986999512, Final Batch Loss: 0.3877309560775757\n",
      "Epoch 1185, Loss: 1.1045725047588348, Final Batch Loss: 0.3024764657020569\n",
      "Epoch 1186, Loss: 1.5351388156414032, Final Batch Loss: 0.7899195551872253\n",
      "Epoch 1187, Loss: 1.2694593667984009, Final Batch Loss: 0.4721169173717499\n",
      "Epoch 1188, Loss: 1.3431285917758942, Final Batch Loss: 0.4908495545387268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1189, Loss: 1.0033676624298096, Final Batch Loss: 0.21760019659996033\n",
      "Epoch 1190, Loss: 1.1708939373493195, Final Batch Loss: 0.34505507349967957\n",
      "Epoch 1191, Loss: 1.4176709055900574, Final Batch Loss: 0.5587289929389954\n",
      "Epoch 1192, Loss: 1.3553505837917328, Final Batch Loss: 0.5813755989074707\n",
      "Epoch 1193, Loss: 1.2929305732250214, Final Batch Loss: 0.4259875416755676\n",
      "Epoch 1194, Loss: 1.1062619090080261, Final Batch Loss: 0.3026600778102875\n",
      "Epoch 1195, Loss: 1.473622888326645, Final Batch Loss: 0.6839519739151001\n",
      "Epoch 1196, Loss: 0.9648405313491821, Final Batch Loss: 0.18951785564422607\n",
      "Epoch 1197, Loss: 1.038310021162033, Final Batch Loss: 0.21991398930549622\n",
      "Epoch 1198, Loss: 1.0288339406251907, Final Batch Loss: 0.22165478765964508\n",
      "Epoch 1199, Loss: 1.2105428874492645, Final Batch Loss: 0.46924805641174316\n",
      "Epoch 1200, Loss: 1.1443953812122345, Final Batch Loss: 0.3593771755695343\n",
      "Epoch 1201, Loss: 1.2912573218345642, Final Batch Loss: 0.5466058850288391\n",
      "Epoch 1202, Loss: 1.2476480901241302, Final Batch Loss: 0.4386945068836212\n",
      "Epoch 1203, Loss: 1.2429615557193756, Final Batch Loss: 0.3815405070781708\n",
      "Epoch 1204, Loss: 1.1095421016216278, Final Batch Loss: 0.3340494632720947\n",
      "Epoch 1205, Loss: 1.302260398864746, Final Batch Loss: 0.5634286999702454\n",
      "Epoch 1206, Loss: 1.2800533771514893, Final Batch Loss: 0.4768648147583008\n",
      "Epoch 1207, Loss: 1.088015764951706, Final Batch Loss: 0.3138481080532074\n",
      "Epoch 1208, Loss: 1.07468980550766, Final Batch Loss: 0.2809349596500397\n",
      "Epoch 1209, Loss: 1.4337317049503326, Final Batch Loss: 0.6472530364990234\n",
      "Epoch 1210, Loss: 1.3090559840202332, Final Batch Loss: 0.5075430274009705\n",
      "Epoch 1211, Loss: 1.2818963825702667, Final Batch Loss: 0.5498896241188049\n",
      "Epoch 1212, Loss: 1.0428337156772614, Final Batch Loss: 0.3114621937274933\n",
      "Epoch 1213, Loss: 1.1718894243240356, Final Batch Loss: 0.4061182141304016\n",
      "Epoch 1214, Loss: 1.1413434445858002, Final Batch Loss: 0.37732580304145813\n",
      "Epoch 1215, Loss: 1.1979165077209473, Final Batch Loss: 0.40921807289123535\n",
      "Epoch 1216, Loss: 1.0544272065162659, Final Batch Loss: 0.25282683968544006\n",
      "Epoch 1217, Loss: 1.4005376100540161, Final Batch Loss: 0.6381001472473145\n",
      "Epoch 1218, Loss: 1.0201036185026169, Final Batch Loss: 0.24102191627025604\n",
      "Epoch 1219, Loss: 1.1390262842178345, Final Batch Loss: 0.40134263038635254\n",
      "Epoch 1220, Loss: 1.2570891678333282, Final Batch Loss: 0.4674917161464691\n",
      "Epoch 1221, Loss: 1.0714457631111145, Final Batch Loss: 0.30552902817726135\n",
      "Epoch 1222, Loss: 1.2127180397510529, Final Batch Loss: 0.39383664727211\n",
      "Epoch 1223, Loss: 1.167201280593872, Final Batch Loss: 0.4028318524360657\n",
      "Epoch 1224, Loss: 1.0336481630802155, Final Batch Loss: 0.17602300643920898\n",
      "Epoch 1225, Loss: 1.2875449657440186, Final Batch Loss: 0.505683958530426\n",
      "Epoch 1226, Loss: 1.0615462362766266, Final Batch Loss: 0.30472052097320557\n",
      "Epoch 1227, Loss: 1.279011756181717, Final Batch Loss: 0.5227274894714355\n",
      "Epoch 1228, Loss: 1.295843482017517, Final Batch Loss: 0.5736441612243652\n",
      "Epoch 1229, Loss: 1.120053619146347, Final Batch Loss: 0.3008134067058563\n",
      "Epoch 1230, Loss: 1.06265190243721, Final Batch Loss: 0.30654042959213257\n",
      "Epoch 1231, Loss: 1.1687120199203491, Final Batch Loss: 0.3523111045360565\n",
      "Epoch 1232, Loss: 1.3951613008975983, Final Batch Loss: 0.6155610084533691\n",
      "Epoch 1233, Loss: 1.3927732706069946, Final Batch Loss: 0.6219676733016968\n",
      "Epoch 1234, Loss: 1.224782407283783, Final Batch Loss: 0.4550916850566864\n",
      "Epoch 1235, Loss: 1.340922862291336, Final Batch Loss: 0.5330913662910461\n",
      "Epoch 1236, Loss: 1.3234741687774658, Final Batch Loss: 0.5716001391410828\n",
      "Epoch 1237, Loss: 1.7480015456676483, Final Batch Loss: 0.9251808524131775\n",
      "Epoch 1238, Loss: 1.1857245564460754, Final Batch Loss: 0.40657392144203186\n",
      "Epoch 1239, Loss: 1.0699264258146286, Final Batch Loss: 0.2419968694448471\n",
      "Epoch 1240, Loss: 1.26883926987648, Final Batch Loss: 0.4551627039909363\n",
      "Epoch 1241, Loss: 1.2460041046142578, Final Batch Loss: 0.4440579116344452\n",
      "Epoch 1242, Loss: 1.3139879703521729, Final Batch Loss: 0.5787686109542847\n",
      "Epoch 1243, Loss: 1.0503402650356293, Final Batch Loss: 0.2895011305809021\n",
      "Epoch 1244, Loss: 1.0870816707611084, Final Batch Loss: 0.30509939789772034\n",
      "Epoch 1245, Loss: 1.3046323359012604, Final Batch Loss: 0.5631874799728394\n",
      "Epoch 1246, Loss: 1.268616646528244, Final Batch Loss: 0.4673186242580414\n",
      "Epoch 1247, Loss: 1.268131583929062, Final Batch Loss: 0.48040103912353516\n",
      "Epoch 1248, Loss: 1.062065601348877, Final Batch Loss: 0.2665202021598816\n",
      "Epoch 1249, Loss: 1.1316710412502289, Final Batch Loss: 0.3395823836326599\n",
      "Epoch 1250, Loss: 1.1593815684318542, Final Batch Loss: 0.3542715311050415\n",
      "Epoch 1251, Loss: 1.2164196074008942, Final Batch Loss: 0.408378928899765\n",
      "Epoch 1252, Loss: 1.1519052684307098, Final Batch Loss: 0.33917149901390076\n",
      "Epoch 1253, Loss: 0.9970896989107132, Final Batch Loss: 0.24714957177639008\n",
      "Epoch 1254, Loss: 0.9579026103019714, Final Batch Loss: 0.18140602111816406\n",
      "Epoch 1255, Loss: 0.9492609202861786, Final Batch Loss: 0.13788416981697083\n",
      "Epoch 1256, Loss: 1.1287421882152557, Final Batch Loss: 0.39382514357566833\n",
      "Epoch 1257, Loss: 1.0254436433315277, Final Batch Loss: 0.31509217619895935\n",
      "Epoch 1258, Loss: 1.0771185159683228, Final Batch Loss: 0.3217625617980957\n",
      "Epoch 1259, Loss: 1.6200754046440125, Final Batch Loss: 0.8621208667755127\n",
      "Epoch 1260, Loss: 0.9667440503835678, Final Batch Loss: 0.2219213992357254\n",
      "Epoch 1261, Loss: 1.2120721638202667, Final Batch Loss: 0.403280109167099\n",
      "Epoch 1262, Loss: 0.9688736945390701, Final Batch Loss: 0.1833977848291397\n",
      "Epoch 1263, Loss: 1.0171993225812912, Final Batch Loss: 0.20314230024814606\n",
      "Epoch 1264, Loss: 1.0024411976337433, Final Batch Loss: 0.2741827964782715\n",
      "Epoch 1265, Loss: 0.9474974721670151, Final Batch Loss: 0.1470039039850235\n",
      "Epoch 1266, Loss: 1.0059879273176193, Final Batch Loss: 0.23950617015361786\n",
      "Epoch 1267, Loss: 1.3075627088546753, Final Batch Loss: 0.5902214050292969\n",
      "Epoch 1268, Loss: 1.0656057000160217, Final Batch Loss: 0.2571868896484375\n",
      "Epoch 1269, Loss: 1.1369003653526306, Final Batch Loss: 0.37311193346977234\n",
      "Epoch 1270, Loss: 1.2111332714557648, Final Batch Loss: 0.48038607835769653\n",
      "Epoch 1271, Loss: 1.300162822008133, Final Batch Loss: 0.5029814839363098\n",
      "Epoch 1272, Loss: 1.419463872909546, Final Batch Loss: 0.6612943410873413\n",
      "Epoch 1273, Loss: 1.2649237513542175, Final Batch Loss: 0.5378205180168152\n",
      "Epoch 1274, Loss: 0.9696706980466843, Final Batch Loss: 0.23773615062236786\n",
      "Epoch 1275, Loss: 1.1879613101482391, Final Batch Loss: 0.41537943482398987\n",
      "Epoch 1276, Loss: 1.2209358215332031, Final Batch Loss: 0.4058963656425476\n",
      "Epoch 1277, Loss: 1.133350819349289, Final Batch Loss: 0.43429625034332275\n",
      "Epoch 1278, Loss: 1.1371729969978333, Final Batch Loss: 0.40023666620254517\n",
      "Epoch 1279, Loss: 1.2245534360408783, Final Batch Loss: 0.4609944224357605\n",
      "Epoch 1280, Loss: 1.3143733143806458, Final Batch Loss: 0.49515753984451294\n",
      "Epoch 1281, Loss: 1.0549399554729462, Final Batch Loss: 0.3016919195652008\n",
      "Epoch 1282, Loss: 1.2546319365501404, Final Batch Loss: 0.5284957885742188\n",
      "Epoch 1283, Loss: 1.186399519443512, Final Batch Loss: 0.4411883056163788\n",
      "Epoch 1284, Loss: 1.322589635848999, Final Batch Loss: 0.5296006202697754\n",
      "Epoch 1285, Loss: 0.9799734950065613, Final Batch Loss: 0.15392562747001648\n",
      "Epoch 1286, Loss: 1.079232633113861, Final Batch Loss: 0.3148881793022156\n",
      "Epoch 1287, Loss: 1.0782967805862427, Final Batch Loss: 0.347007691860199\n",
      "Epoch 1288, Loss: 1.1734531223773956, Final Batch Loss: 0.42645159363746643\n",
      "Epoch 1289, Loss: 1.0686992704868317, Final Batch Loss: 0.30864599347114563\n",
      "Epoch 1290, Loss: 1.322144627571106, Final Batch Loss: 0.572942316532135\n",
      "Epoch 1291, Loss: 1.0816746056079865, Final Batch Loss: 0.3606664538383484\n",
      "Epoch 1292, Loss: 1.132252961397171, Final Batch Loss: 0.3827113211154938\n",
      "Epoch 1293, Loss: 1.0790839791297913, Final Batch Loss: 0.3618153929710388\n",
      "Epoch 1294, Loss: 1.1831400990486145, Final Batch Loss: 0.3944779336452484\n",
      "Epoch 1295, Loss: 1.0072768181562424, Final Batch Loss: 0.21006141602993011\n",
      "Epoch 1296, Loss: 0.9988893568515778, Final Batch Loss: 0.300141841173172\n",
      "Epoch 1297, Loss: 1.503631353378296, Final Batch Loss: 0.7215660214424133\n",
      "Epoch 1298, Loss: 0.8891995251178741, Final Batch Loss: 0.12246385216712952\n",
      "Epoch 1299, Loss: 1.2705802023410797, Final Batch Loss: 0.408173531293869\n",
      "Epoch 1300, Loss: 0.9803193509578705, Final Batch Loss: 0.20081835985183716\n",
      "Epoch 1301, Loss: 1.1584511399269104, Final Batch Loss: 0.36270007491111755\n",
      "Epoch 1302, Loss: 0.8845802545547485, Final Batch Loss: 0.1414267122745514\n",
      "Epoch 1303, Loss: 1.1253085434436798, Final Batch Loss: 0.3579799234867096\n",
      "Epoch 1304, Loss: 1.1867028176784515, Final Batch Loss: 0.4886876046657562\n",
      "Epoch 1305, Loss: 1.1358718872070312, Final Batch Loss: 0.36077681183815\n",
      "Epoch 1306, Loss: 1.2449806332588196, Final Batch Loss: 0.4482700824737549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1307, Loss: 1.3072474300861359, Final Batch Loss: 0.5326068997383118\n",
      "Epoch 1308, Loss: 1.0564275681972504, Final Batch Loss: 0.3283202648162842\n",
      "Epoch 1309, Loss: 1.2168425023555756, Final Batch Loss: 0.48669251799583435\n",
      "Epoch 1310, Loss: 1.3159209489822388, Final Batch Loss: 0.44013726711273193\n",
      "Epoch 1311, Loss: 1.2328728437423706, Final Batch Loss: 0.4863797724246979\n",
      "Epoch 1312, Loss: 1.2568134367465973, Final Batch Loss: 0.4769265055656433\n",
      "Epoch 1313, Loss: 1.5046598017215729, Final Batch Loss: 0.7686900496482849\n",
      "Epoch 1314, Loss: 1.2429843544960022, Final Batch Loss: 0.48695921897888184\n",
      "Epoch 1315, Loss: 1.0970387756824493, Final Batch Loss: 0.3271529972553253\n",
      "Epoch 1316, Loss: 1.1675329208374023, Final Batch Loss: 0.3857846260070801\n",
      "Epoch 1317, Loss: 1.2877053320407867, Final Batch Loss: 0.5722139477729797\n",
      "Epoch 1318, Loss: 1.2907248735427856, Final Batch Loss: 0.5683863162994385\n",
      "Epoch 1319, Loss: 1.4039615988731384, Final Batch Loss: 0.5751543045043945\n",
      "Epoch 1320, Loss: 1.1482629477977753, Final Batch Loss: 0.40419426560401917\n",
      "Epoch 1321, Loss: 1.268056720495224, Final Batch Loss: 0.503495991230011\n",
      "Epoch 1322, Loss: 1.2500993013381958, Final Batch Loss: 0.3573678433895111\n",
      "Epoch 1323, Loss: 1.4120213985443115, Final Batch Loss: 0.626150906085968\n",
      "Epoch 1324, Loss: 1.5457229614257812, Final Batch Loss: 0.7985241413116455\n",
      "Epoch 1325, Loss: 1.156576931476593, Final Batch Loss: 0.4349021315574646\n",
      "Epoch 1326, Loss: 0.9967243075370789, Final Batch Loss: 0.2394842803478241\n",
      "Epoch 1327, Loss: 1.06373131275177, Final Batch Loss: 0.2672950029373169\n",
      "Epoch 1328, Loss: 1.238444983959198, Final Batch Loss: 0.4911157190799713\n",
      "Epoch 1329, Loss: 1.201442688703537, Final Batch Loss: 0.5053048133850098\n",
      "Epoch 1330, Loss: 0.8303724937140942, Final Batch Loss: 0.05225963518023491\n",
      "Epoch 1331, Loss: 0.9812172800302505, Final Batch Loss: 0.1755785495042801\n",
      "Epoch 1332, Loss: 1.1682194769382477, Final Batch Loss: 0.3426498472690582\n",
      "Epoch 1333, Loss: 1.2749693393707275, Final Batch Loss: 0.5361697673797607\n",
      "Epoch 1334, Loss: 1.1496411859989166, Final Batch Loss: 0.42662739753723145\n",
      "Epoch 1335, Loss: 1.1514278054237366, Final Batch Loss: 0.38503965735435486\n",
      "Epoch 1336, Loss: 1.3876575827598572, Final Batch Loss: 0.655433714389801\n",
      "Epoch 1337, Loss: 1.349795162677765, Final Batch Loss: 0.5964205861091614\n",
      "Epoch 1338, Loss: 1.0905272662639618, Final Batch Loss: 0.3176651895046234\n",
      "Epoch 1339, Loss: 1.3926208913326263, Final Batch Loss: 0.6388001441955566\n",
      "Epoch 1340, Loss: 1.0499394834041595, Final Batch Loss: 0.2542380690574646\n",
      "Epoch 1341, Loss: 1.3723740875720978, Final Batch Loss: 0.5779442191123962\n",
      "Epoch 1342, Loss: 1.1119787395000458, Final Batch Loss: 0.3105909526348114\n",
      "Epoch 1343, Loss: 1.2162793576717377, Final Batch Loss: 0.423855185508728\n",
      "Epoch 1344, Loss: 1.1243661642074585, Final Batch Loss: 0.3523337244987488\n",
      "Epoch 1345, Loss: 1.1841970086097717, Final Batch Loss: 0.3757689893245697\n",
      "Epoch 1346, Loss: 0.9790300577878952, Final Batch Loss: 0.14818890392780304\n",
      "Epoch 1347, Loss: 1.5207582116127014, Final Batch Loss: 0.7772412300109863\n",
      "Epoch 1348, Loss: 1.2665057480335236, Final Batch Loss: 0.45411697030067444\n",
      "Epoch 1349, Loss: 1.1124508678913116, Final Batch Loss: 0.34738773107528687\n",
      "Epoch 1350, Loss: 1.4911439418792725, Final Batch Loss: 0.6597760915756226\n",
      "Epoch 1351, Loss: 1.2129135131835938, Final Batch Loss: 0.4204288125038147\n",
      "Epoch 1352, Loss: 1.18008753657341, Final Batch Loss: 0.44509977102279663\n",
      "Epoch 1353, Loss: 1.0364840775728226, Final Batch Loss: 0.24662552773952484\n",
      "Epoch 1354, Loss: 1.1168248653411865, Final Batch Loss: 0.35289478302001953\n",
      "Epoch 1355, Loss: 1.1110838055610657, Final Batch Loss: 0.3527970016002655\n",
      "Epoch 1356, Loss: 1.2867467105388641, Final Batch Loss: 0.5347266793251038\n",
      "Epoch 1357, Loss: 0.9666209369897842, Final Batch Loss: 0.20366744697093964\n",
      "Epoch 1358, Loss: 1.0446027219295502, Final Batch Loss: 0.21600425243377686\n",
      "Epoch 1359, Loss: 1.2458975911140442, Final Batch Loss: 0.5047688484191895\n",
      "Epoch 1360, Loss: 0.9662326723337173, Final Batch Loss: 0.18509097397327423\n",
      "Epoch 1361, Loss: 1.3361214697360992, Final Batch Loss: 0.554543673992157\n",
      "Epoch 1362, Loss: 1.3120313584804535, Final Batch Loss: 0.6094252467155457\n",
      "Epoch 1363, Loss: 1.1217552721500397, Final Batch Loss: 0.379718542098999\n",
      "Epoch 1364, Loss: 1.0223531275987625, Final Batch Loss: 0.23804731667041779\n",
      "Epoch 1365, Loss: 1.2796486616134644, Final Batch Loss: 0.5298965573310852\n",
      "Epoch 1366, Loss: 1.0640651285648346, Final Batch Loss: 0.30215024948120117\n",
      "Epoch 1367, Loss: 1.0298482775688171, Final Batch Loss: 0.2577309012413025\n",
      "Epoch 1368, Loss: 1.1912029385566711, Final Batch Loss: 0.4526269733905792\n",
      "Epoch 1369, Loss: 1.1182045936584473, Final Batch Loss: 0.3689783215522766\n",
      "Epoch 1370, Loss: 1.2440572679042816, Final Batch Loss: 0.5201416015625\n",
      "Epoch 1371, Loss: 1.0074228942394257, Final Batch Loss: 0.28617843985557556\n",
      "Epoch 1372, Loss: 0.9998956322669983, Final Batch Loss: 0.2609477639198303\n",
      "Epoch 1373, Loss: 1.0695156455039978, Final Batch Loss: 0.25879472494125366\n",
      "Epoch 1374, Loss: 1.1788702607154846, Final Batch Loss: 0.3253117799758911\n",
      "Epoch 1375, Loss: 1.1779977977275848, Final Batch Loss: 0.45632195472717285\n",
      "Epoch 1376, Loss: 0.9258743524551392, Final Batch Loss: 0.19117861986160278\n",
      "Epoch 1377, Loss: 1.0538698434829712, Final Batch Loss: 0.3507150113582611\n",
      "Epoch 1378, Loss: 1.0640285015106201, Final Batch Loss: 0.27862414717674255\n",
      "Epoch 1379, Loss: 1.1072704792022705, Final Batch Loss: 0.3583175241947174\n",
      "Epoch 1380, Loss: 1.1324880421161652, Final Batch Loss: 0.41202446818351746\n",
      "Epoch 1381, Loss: 1.280487596988678, Final Batch Loss: 0.5819045305252075\n",
      "Epoch 1382, Loss: 0.9963217973709106, Final Batch Loss: 0.22831404209136963\n",
      "Epoch 1383, Loss: 1.0608238875865936, Final Batch Loss: 0.2892168164253235\n",
      "Epoch 1384, Loss: 1.060313642024994, Final Batch Loss: 0.3329024314880371\n",
      "Epoch 1385, Loss: 0.920043408870697, Final Batch Loss: 0.18405601382255554\n",
      "Epoch 1386, Loss: 1.2766283750534058, Final Batch Loss: 0.5316527485847473\n",
      "Epoch 1387, Loss: 0.9766458719968796, Final Batch Loss: 0.22098900377750397\n",
      "Epoch 1388, Loss: 0.884412944316864, Final Batch Loss: 0.1640876829624176\n",
      "Epoch 1389, Loss: 0.8666855469346046, Final Batch Loss: 0.11717618256807327\n",
      "Epoch 1390, Loss: 1.0650970041751862, Final Batch Loss: 0.37515103816986084\n",
      "Epoch 1391, Loss: 0.9685748815536499, Final Batch Loss: 0.13243630528450012\n",
      "Epoch 1392, Loss: 1.020635962486267, Final Batch Loss: 0.30522316694259644\n",
      "Epoch 1393, Loss: 1.2043257057666779, Final Batch Loss: 0.42113566398620605\n",
      "Epoch 1394, Loss: 1.2977230250835419, Final Batch Loss: 0.623018741607666\n",
      "Epoch 1395, Loss: 1.1487403512001038, Final Batch Loss: 0.39977842569351196\n",
      "Epoch 1396, Loss: 1.0783217549324036, Final Batch Loss: 0.2617966830730438\n",
      "Epoch 1397, Loss: 0.9413514286279678, Final Batch Loss: 0.19950716197490692\n",
      "Epoch 1398, Loss: 1.1506418287754059, Final Batch Loss: 0.40127646923065186\n",
      "Epoch 1399, Loss: 1.0604156255722046, Final Batch Loss: 0.2545192539691925\n",
      "Epoch 1400, Loss: 1.092980831861496, Final Batch Loss: 0.29979705810546875\n",
      "Epoch 1401, Loss: 1.299945056438446, Final Batch Loss: 0.599840521812439\n",
      "Epoch 1402, Loss: 1.132962316274643, Final Batch Loss: 0.34413790702819824\n",
      "Epoch 1403, Loss: 1.1341253221035004, Final Batch Loss: 0.3204629123210907\n",
      "Epoch 1404, Loss: 1.0255561470985413, Final Batch Loss: 0.2276805341243744\n",
      "Epoch 1405, Loss: 1.2245573103427887, Final Batch Loss: 0.4895879924297333\n",
      "Epoch 1406, Loss: 1.0410108864307404, Final Batch Loss: 0.2850516140460968\n",
      "Epoch 1407, Loss: 1.2029812335968018, Final Batch Loss: 0.43917202949523926\n",
      "Epoch 1408, Loss: 0.8710974901914597, Final Batch Loss: 0.15158794820308685\n",
      "Epoch 1409, Loss: 1.268126219511032, Final Batch Loss: 0.5440725088119507\n",
      "Epoch 1410, Loss: 1.1405507922172546, Final Batch Loss: 0.42822572588920593\n",
      "Epoch 1411, Loss: 1.0120996832847595, Final Batch Loss: 0.2791198790073395\n",
      "Epoch 1412, Loss: 0.9255182147026062, Final Batch Loss: 0.13800278306007385\n",
      "Epoch 1413, Loss: 1.0462477803230286, Final Batch Loss: 0.26987364888191223\n",
      "Epoch 1414, Loss: 1.1105308830738068, Final Batch Loss: 0.34951284527778625\n",
      "Epoch 1415, Loss: 1.094337373971939, Final Batch Loss: 0.3274795413017273\n",
      "Epoch 1416, Loss: 1.1445162296295166, Final Batch Loss: 0.3934178054332733\n",
      "Epoch 1417, Loss: 1.2169404923915863, Final Batch Loss: 0.5251734852790833\n",
      "Epoch 1418, Loss: 1.332651436328888, Final Batch Loss: 0.6050707101821899\n",
      "Epoch 1419, Loss: 1.0807271301746368, Final Batch Loss: 0.3056568205356598\n",
      "Epoch 1420, Loss: 1.2600291967391968, Final Batch Loss: 0.4893416464328766\n",
      "Epoch 1421, Loss: 0.9839931726455688, Final Batch Loss: 0.22589918971061707\n",
      "Epoch 1422, Loss: 1.0358807146549225, Final Batch Loss: 0.26684924960136414\n",
      "Epoch 1423, Loss: 1.0321401059627533, Final Batch Loss: 0.33711573481559753\n",
      "Epoch 1424, Loss: 1.1969578862190247, Final Batch Loss: 0.4654006063938141\n",
      "Epoch 1425, Loss: 1.1280468106269836, Final Batch Loss: 0.39463141560554504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1426, Loss: 1.0886410176753998, Final Batch Loss: 0.3556775748729706\n",
      "Epoch 1427, Loss: 0.8918674439191818, Final Batch Loss: 0.14383848011493683\n",
      "Epoch 1428, Loss: 1.1006512939929962, Final Batch Loss: 0.35579389333724976\n",
      "Epoch 1429, Loss: 0.8611798286437988, Final Batch Loss: 0.1306520700454712\n",
      "Epoch 1430, Loss: 1.450405329465866, Final Batch Loss: 0.7800040245056152\n",
      "Epoch 1431, Loss: 1.0535339415073395, Final Batch Loss: 0.34312114119529724\n",
      "Epoch 1432, Loss: 1.0713087916374207, Final Batch Loss: 0.39511123299598694\n",
      "Epoch 1433, Loss: 1.072282463312149, Final Batch Loss: 0.3846088945865631\n",
      "Epoch 1434, Loss: 0.9838964194059372, Final Batch Loss: 0.22699888050556183\n",
      "Epoch 1435, Loss: 1.299311876296997, Final Batch Loss: 0.5551758408546448\n",
      "Epoch 1436, Loss: 1.1112126111984253, Final Batch Loss: 0.43034055829048157\n",
      "Epoch 1437, Loss: 1.1132056415081024, Final Batch Loss: 0.3448439836502075\n",
      "Epoch 1438, Loss: 1.1157569289207458, Final Batch Loss: 0.3878211975097656\n",
      "Epoch 1439, Loss: 1.2158521115779877, Final Batch Loss: 0.48237383365631104\n",
      "Epoch 1440, Loss: 1.0542717278003693, Final Batch Loss: 0.2752332389354706\n",
      "Epoch 1441, Loss: 1.0110186040401459, Final Batch Loss: 0.23410668969154358\n",
      "Epoch 1442, Loss: 1.1434898376464844, Final Batch Loss: 0.365751713514328\n",
      "Epoch 1443, Loss: 0.9862691164016724, Final Batch Loss: 0.28378039598464966\n",
      "Epoch 1444, Loss: 0.9966323375701904, Final Batch Loss: 0.2772635519504547\n",
      "Epoch 1445, Loss: 0.9816279709339142, Final Batch Loss: 0.2501034438610077\n",
      "Epoch 1446, Loss: 1.2611214816570282, Final Batch Loss: 0.5318551063537598\n",
      "Epoch 1447, Loss: 1.1656722128391266, Final Batch Loss: 0.39735931158065796\n",
      "Epoch 1448, Loss: 1.1735844612121582, Final Batch Loss: 0.43828830122947693\n",
      "Epoch 1449, Loss: 1.1301584839820862, Final Batch Loss: 0.3738766610622406\n",
      "Epoch 1450, Loss: 0.900789737701416, Final Batch Loss: 0.18974906206130981\n",
      "Epoch 1451, Loss: 1.0724676251411438, Final Batch Loss: 0.3745013177394867\n",
      "Epoch 1452, Loss: 1.0904096364974976, Final Batch Loss: 0.33929216861724854\n",
      "Epoch 1453, Loss: 1.180794358253479, Final Batch Loss: 0.4817613661289215\n",
      "Epoch 1454, Loss: 1.22227743268013, Final Batch Loss: 0.46572232246398926\n",
      "Epoch 1455, Loss: 1.007790595293045, Final Batch Loss: 0.294565886259079\n",
      "Epoch 1456, Loss: 1.0381490588188171, Final Batch Loss: 0.3177325129508972\n",
      "Epoch 1457, Loss: 1.0740613043308258, Final Batch Loss: 0.34959131479263306\n",
      "Epoch 1458, Loss: 1.0428442358970642, Final Batch Loss: 0.30938491225242615\n",
      "Epoch 1459, Loss: 1.1525484919548035, Final Batch Loss: 0.3358401656150818\n",
      "Epoch 1460, Loss: 1.0370463728904724, Final Batch Loss: 0.29865431785583496\n",
      "Epoch 1461, Loss: 1.232943445444107, Final Batch Loss: 0.4836622178554535\n",
      "Epoch 1462, Loss: 1.1122985184192657, Final Batch Loss: 0.40861228108406067\n",
      "Epoch 1463, Loss: 1.4379336833953857, Final Batch Loss: 0.7237078547477722\n",
      "Epoch 1464, Loss: 1.0783264338970184, Final Batch Loss: 0.360726922750473\n",
      "Epoch 1465, Loss: 1.3184292018413544, Final Batch Loss: 0.5530202984809875\n",
      "Epoch 1466, Loss: 1.366603821516037, Final Batch Loss: 0.6245610117912292\n",
      "Epoch 1467, Loss: 1.1414379477500916, Final Batch Loss: 0.401497483253479\n",
      "Epoch 1468, Loss: 1.2301442921161652, Final Batch Loss: 0.478899210691452\n",
      "Epoch 1469, Loss: 0.9488126784563065, Final Batch Loss: 0.22803811728954315\n",
      "Epoch 1470, Loss: 0.9756467491388321, Final Batch Loss: 0.2140994817018509\n",
      "Epoch 1471, Loss: 1.100069522857666, Final Batch Loss: 0.3167625367641449\n",
      "Epoch 1472, Loss: 1.3564229309558868, Final Batch Loss: 0.6497647762298584\n",
      "Epoch 1473, Loss: 1.02210333943367, Final Batch Loss: 0.39369481801986694\n",
      "Epoch 1474, Loss: 1.0829599797725677, Final Batch Loss: 0.3471369445323944\n",
      "Epoch 1475, Loss: 1.3865036070346832, Final Batch Loss: 0.6513710618019104\n",
      "Epoch 1476, Loss: 1.4357055723667145, Final Batch Loss: 0.6180322766304016\n",
      "Epoch 1477, Loss: 1.1655044853687286, Final Batch Loss: 0.37326759099960327\n",
      "Epoch 1478, Loss: 1.0048446506261826, Final Batch Loss: 0.224986270070076\n",
      "Epoch 1479, Loss: 1.488909661769867, Final Batch Loss: 0.7973182201385498\n",
      "Epoch 1480, Loss: 0.9472353607416153, Final Batch Loss: 0.20508529245853424\n",
      "Epoch 1481, Loss: 1.168378323316574, Final Batch Loss: 0.3771698474884033\n",
      "Epoch 1482, Loss: 1.0746831893920898, Final Batch Loss: 0.313145250082016\n",
      "Epoch 1483, Loss: 1.0817687213420868, Final Batch Loss: 0.3443978428840637\n",
      "Epoch 1484, Loss: 1.1552622318267822, Final Batch Loss: 0.3952553868293762\n",
      "Epoch 1485, Loss: 1.4694213569164276, Final Batch Loss: 0.7276052832603455\n",
      "Epoch 1486, Loss: 0.9242759346961975, Final Batch Loss: 0.25439974665641785\n",
      "Epoch 1487, Loss: 1.3345006704330444, Final Batch Loss: 0.6008095145225525\n",
      "Epoch 1488, Loss: 1.1337231695652008, Final Batch Loss: 0.3395805358886719\n",
      "Epoch 1489, Loss: 1.0847738087177277, Final Batch Loss: 0.2882181704044342\n",
      "Epoch 1490, Loss: 1.1614913046360016, Final Batch Loss: 0.42200204730033875\n",
      "Epoch 1491, Loss: 1.1615249514579773, Final Batch Loss: 0.37302133440971375\n",
      "Epoch 1492, Loss: 1.3588357865810394, Final Batch Loss: 0.5640734434127808\n",
      "Epoch 1493, Loss: 1.1516741514205933, Final Batch Loss: 0.4457016885280609\n",
      "Epoch 1494, Loss: 1.000180795788765, Final Batch Loss: 0.22956876456737518\n",
      "Epoch 1495, Loss: 1.023580178618431, Final Batch Loss: 0.2048637419939041\n",
      "Epoch 1496, Loss: 1.0763221383094788, Final Batch Loss: 0.3178953230381012\n",
      "Epoch 1497, Loss: 1.1323760151863098, Final Batch Loss: 0.45207837224006653\n",
      "Epoch 1498, Loss: 1.3438895046710968, Final Batch Loss: 0.6399143934249878\n",
      "Epoch 1499, Loss: 1.073800265789032, Final Batch Loss: 0.34384387731552124\n",
      "Epoch 1500, Loss: 1.0596045851707458, Final Batch Loss: 0.35030481219291687\n",
      "Epoch 1501, Loss: 1.0147060453891754, Final Batch Loss: 0.29727283120155334\n",
      "Epoch 1502, Loss: 1.230107992887497, Final Batch Loss: 0.48114290833473206\n",
      "Epoch 1503, Loss: 1.3593737185001373, Final Batch Loss: 0.609174907207489\n",
      "Epoch 1504, Loss: 1.1369142830371857, Final Batch Loss: 0.3937946557998657\n",
      "Epoch 1505, Loss: 1.1609237790107727, Final Batch Loss: 0.4404976963996887\n",
      "Epoch 1506, Loss: 1.2504958808422089, Final Batch Loss: 0.5336502194404602\n",
      "Epoch 1507, Loss: 0.98133984208107, Final Batch Loss: 0.3001752197742462\n",
      "Epoch 1508, Loss: 0.9520838260650635, Final Batch Loss: 0.25258007645606995\n",
      "Epoch 1509, Loss: 1.1419637501239777, Final Batch Loss: 0.40022537112236023\n",
      "Epoch 1510, Loss: 1.0731447637081146, Final Batch Loss: 0.3100672662258148\n",
      "Epoch 1511, Loss: 1.0934820473194122, Final Batch Loss: 0.3254144787788391\n",
      "Epoch 1512, Loss: 0.9041619151830673, Final Batch Loss: 0.1375662237405777\n",
      "Epoch 1513, Loss: 0.967359721660614, Final Batch Loss: 0.23834529519081116\n",
      "Epoch 1514, Loss: 1.0384317338466644, Final Batch Loss: 0.3513966500759125\n",
      "Epoch 1515, Loss: 1.140795737504959, Final Batch Loss: 0.42485055327415466\n",
      "Epoch 1516, Loss: 0.8623411655426025, Final Batch Loss: 0.13956156373023987\n",
      "Epoch 1517, Loss: 1.1977192163467407, Final Batch Loss: 0.5142037272453308\n",
      "Epoch 1518, Loss: 1.0109971165657043, Final Batch Loss: 0.26546359062194824\n",
      "Epoch 1519, Loss: 1.371379315853119, Final Batch Loss: 0.6588816046714783\n",
      "Epoch 1520, Loss: 1.0692890584468842, Final Batch Loss: 0.3280109763145447\n",
      "Epoch 1521, Loss: 1.3384251594543457, Final Batch Loss: 0.6658914089202881\n",
      "Epoch 1522, Loss: 1.022766798734665, Final Batch Loss: 0.30909204483032227\n",
      "Epoch 1523, Loss: 1.126232922077179, Final Batch Loss: 0.41935035586357117\n",
      "Epoch 1524, Loss: 1.2931036949157715, Final Batch Loss: 0.584705650806427\n",
      "Epoch 1525, Loss: 1.2519217729568481, Final Batch Loss: 0.5267031192779541\n",
      "Epoch 1526, Loss: 1.253115713596344, Final Batch Loss: 0.5740008354187012\n",
      "Epoch 1527, Loss: 1.2432846128940582, Final Batch Loss: 0.48992976546287537\n",
      "Epoch 1528, Loss: 1.1302233338356018, Final Batch Loss: 0.38195499777793884\n",
      "Epoch 1529, Loss: 1.3412328958511353, Final Batch Loss: 0.5954707264900208\n",
      "Epoch 1530, Loss: 1.180073767900467, Final Batch Loss: 0.4308098256587982\n",
      "Epoch 1531, Loss: 1.1779989302158356, Final Batch Loss: 0.34057241678237915\n",
      "Epoch 1532, Loss: 1.2675900757312775, Final Batch Loss: 0.4652453064918518\n",
      "Epoch 1533, Loss: 1.4151176810264587, Final Batch Loss: 0.6656942367553711\n",
      "Epoch 1534, Loss: 1.1453303694725037, Final Batch Loss: 0.4308604598045349\n",
      "Epoch 1535, Loss: 1.0169631838798523, Final Batch Loss: 0.2513653337955475\n",
      "Epoch 1536, Loss: 1.1300379633903503, Final Batch Loss: 0.4085143506526947\n",
      "Epoch 1537, Loss: 1.2103529274463654, Final Batch Loss: 0.505079448223114\n",
      "Epoch 1538, Loss: 1.1092773079872131, Final Batch Loss: 0.37704014778137207\n",
      "Epoch 1539, Loss: 1.0756753981113434, Final Batch Loss: 0.3457384705543518\n",
      "Epoch 1540, Loss: 1.0863847136497498, Final Batch Loss: 0.3939470648765564\n",
      "Epoch 1541, Loss: 1.2501662373542786, Final Batch Loss: 0.5352448225021362\n",
      "Epoch 1542, Loss: 1.1355568170547485, Final Batch Loss: 0.4095076620578766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1543, Loss: 0.9781745821237564, Final Batch Loss: 0.24896584451198578\n",
      "Epoch 1544, Loss: 1.123850405216217, Final Batch Loss: 0.48651623725891113\n",
      "Epoch 1545, Loss: 0.9339016377925873, Final Batch Loss: 0.15915656089782715\n",
      "Epoch 1546, Loss: 0.9819650053977966, Final Batch Loss: 0.3043406009674072\n",
      "Epoch 1547, Loss: 1.1937699615955353, Final Batch Loss: 0.5062872171401978\n",
      "Epoch 1548, Loss: 1.192251741886139, Final Batch Loss: 0.5070564150810242\n",
      "Epoch 1549, Loss: 1.196245789527893, Final Batch Loss: 0.47511935234069824\n",
      "Epoch 1550, Loss: 1.0190660655498505, Final Batch Loss: 0.26497703790664673\n",
      "Epoch 1551, Loss: 1.0737581253051758, Final Batch Loss: 0.32042476534843445\n",
      "Epoch 1552, Loss: 1.064243197441101, Final Batch Loss: 0.30275893211364746\n",
      "Epoch 1553, Loss: 0.9886417239904404, Final Batch Loss: 0.22771702706813812\n",
      "Epoch 1554, Loss: 0.9609031677246094, Final Batch Loss: 0.1938348114490509\n",
      "Epoch 1555, Loss: 0.8493582606315613, Final Batch Loss: 0.17192232608795166\n",
      "Epoch 1556, Loss: 1.1823605597019196, Final Batch Loss: 0.46179646253585815\n",
      "Epoch 1557, Loss: 0.8929314017295837, Final Batch Loss: 0.13750040531158447\n",
      "Epoch 1558, Loss: 1.010300412774086, Final Batch Loss: 0.19392363727092743\n",
      "Epoch 1559, Loss: 1.3631888628005981, Final Batch Loss: 0.6489289402961731\n",
      "Epoch 1560, Loss: 1.1273268163204193, Final Batch Loss: 0.3473757803440094\n",
      "Epoch 1561, Loss: 1.1520540714263916, Final Batch Loss: 0.3323025405406952\n",
      "Epoch 1562, Loss: 1.0967436730861664, Final Batch Loss: 0.3545812964439392\n",
      "Epoch 1563, Loss: 1.1027314066886902, Final Batch Loss: 0.36834272742271423\n",
      "Epoch 1564, Loss: 1.0520831048488617, Final Batch Loss: 0.34600216150283813\n",
      "Epoch 1565, Loss: 1.1912395060062408, Final Batch Loss: 0.4982442259788513\n",
      "Epoch 1566, Loss: 1.025086134672165, Final Batch Loss: 0.3196641504764557\n",
      "Epoch 1567, Loss: 1.134747952222824, Final Batch Loss: 0.3376544415950775\n",
      "Epoch 1568, Loss: 1.3462021052837372, Final Batch Loss: 0.6692134141921997\n",
      "Epoch 1569, Loss: 0.9914779663085938, Final Batch Loss: 0.3497035503387451\n",
      "Epoch 1570, Loss: 1.0760931968688965, Final Batch Loss: 0.3772641718387604\n",
      "Epoch 1571, Loss: 1.4351181089878082, Final Batch Loss: 0.6668042540550232\n",
      "Epoch 1572, Loss: 0.9382401406764984, Final Batch Loss: 0.1571282148361206\n",
      "Epoch 1573, Loss: 1.04552361369133, Final Batch Loss: 0.29639652371406555\n",
      "Epoch 1574, Loss: 0.9769321978092194, Final Batch Loss: 0.26364612579345703\n",
      "Epoch 1575, Loss: 1.1310722827911377, Final Batch Loss: 0.41668573021888733\n",
      "Epoch 1576, Loss: 0.9728402495384216, Final Batch Loss: 0.2628936171531677\n",
      "Epoch 1577, Loss: 1.159733921289444, Final Batch Loss: 0.4969196319580078\n",
      "Epoch 1578, Loss: 1.1729744374752045, Final Batch Loss: 0.4759555160999298\n",
      "Epoch 1579, Loss: 1.0685932040214539, Final Batch Loss: 0.3738720118999481\n",
      "Epoch 1580, Loss: 1.1327236890792847, Final Batch Loss: 0.45440974831581116\n",
      "Epoch 1581, Loss: 0.9942449331283569, Final Batch Loss: 0.3282923996448517\n",
      "Epoch 1582, Loss: 1.2372568249702454, Final Batch Loss: 0.510668158531189\n",
      "Epoch 1583, Loss: 1.1709076762199402, Final Batch Loss: 0.43671026825904846\n",
      "Epoch 1584, Loss: 1.0116121172904968, Final Batch Loss: 0.3198811411857605\n",
      "Epoch 1585, Loss: 0.974263608455658, Final Batch Loss: 0.293878972530365\n",
      "Epoch 1586, Loss: 1.1946517825126648, Final Batch Loss: 0.4308070242404938\n",
      "Epoch 1587, Loss: 1.0005447268486023, Final Batch Loss: 0.3299688398838043\n",
      "Epoch 1588, Loss: 1.0682151019573212, Final Batch Loss: 0.40439119935035706\n",
      "Epoch 1589, Loss: 1.0899384319782257, Final Batch Loss: 0.3730928599834442\n",
      "Epoch 1590, Loss: 1.132691204547882, Final Batch Loss: 0.4623635411262512\n",
      "Epoch 1591, Loss: 1.1659891605377197, Final Batch Loss: 0.4662967920303345\n",
      "Epoch 1592, Loss: 0.9577695280313492, Final Batch Loss: 0.20621980726718903\n",
      "Epoch 1593, Loss: 1.1448426842689514, Final Batch Loss: 0.4577547311782837\n",
      "Epoch 1594, Loss: 0.9306512326002121, Final Batch Loss: 0.17428095638751984\n",
      "Epoch 1595, Loss: 1.0908340811729431, Final Batch Loss: 0.3162950575351715\n",
      "Epoch 1596, Loss: 0.839668832719326, Final Batch Loss: 0.10139169543981552\n",
      "Epoch 1597, Loss: 0.9052960425615311, Final Batch Loss: 0.2164638489484787\n",
      "Epoch 1598, Loss: 1.0011383891105652, Final Batch Loss: 0.30362293124198914\n",
      "Epoch 1599, Loss: 1.1515181362628937, Final Batch Loss: 0.48583918809890747\n",
      "Epoch 1600, Loss: 1.501941204071045, Final Batch Loss: 0.8254483342170715\n",
      "Epoch 1601, Loss: 1.3818794786930084, Final Batch Loss: 0.7021211385726929\n",
      "Epoch 1602, Loss: 1.211402267217636, Final Batch Loss: 0.5085956454277039\n",
      "Epoch 1603, Loss: 1.1200162172317505, Final Batch Loss: 0.36970385909080505\n",
      "Epoch 1604, Loss: 0.9121606051921844, Final Batch Loss: 0.24467134475708008\n",
      "Epoch 1605, Loss: 1.1105629801750183, Final Batch Loss: 0.41862353682518005\n",
      "Epoch 1606, Loss: 0.8557655066251755, Final Batch Loss: 0.18518038094043732\n",
      "Epoch 1607, Loss: 1.209245890378952, Final Batch Loss: 0.49749454855918884\n",
      "Epoch 1608, Loss: 1.1537949740886688, Final Batch Loss: 0.42779532074928284\n",
      "Epoch 1609, Loss: 1.327185720205307, Final Batch Loss: 0.6017805337905884\n",
      "Epoch 1610, Loss: 0.8424244523048401, Final Batch Loss: 0.13499394059181213\n",
      "Epoch 1611, Loss: 0.9699970185756683, Final Batch Loss: 0.2384379804134369\n",
      "Epoch 1612, Loss: 1.0083202719688416, Final Batch Loss: 0.2753206193447113\n",
      "Epoch 1613, Loss: 0.9373731166124344, Final Batch Loss: 0.24325866997241974\n",
      "Epoch 1614, Loss: 0.8764142245054245, Final Batch Loss: 0.14107532799243927\n",
      "Epoch 1615, Loss: 0.9965786933898926, Final Batch Loss: 0.2991340756416321\n",
      "Epoch 1616, Loss: 1.055100679397583, Final Batch Loss: 0.35328009724617004\n",
      "Epoch 1617, Loss: 1.1914234459400177, Final Batch Loss: 0.48725831508636475\n",
      "Epoch 1618, Loss: 0.9046208411455154, Final Batch Loss: 0.2027975171804428\n",
      "Epoch 1619, Loss: 1.0511511862277985, Final Batch Loss: 0.2602316737174988\n",
      "Epoch 1620, Loss: 1.2116482555866241, Final Batch Loss: 0.5508975386619568\n",
      "Epoch 1621, Loss: 1.0852270722389221, Final Batch Loss: 0.3383796215057373\n",
      "Epoch 1622, Loss: 1.1922650337219238, Final Batch Loss: 0.443189412355423\n",
      "Epoch 1623, Loss: 1.0639047920703888, Final Batch Loss: 0.3378070592880249\n",
      "Epoch 1624, Loss: 1.0372142642736435, Final Batch Loss: 0.3902917802333832\n",
      "Epoch 1625, Loss: 0.861831471323967, Final Batch Loss: 0.18544287979602814\n",
      "Epoch 1626, Loss: 1.1691333055496216, Final Batch Loss: 0.4482763111591339\n",
      "Epoch 1627, Loss: 1.0208755731582642, Final Batch Loss: 0.29755064845085144\n",
      "Epoch 1628, Loss: 0.9235952198505402, Final Batch Loss: 0.21574798226356506\n",
      "Epoch 1629, Loss: 1.1644275784492493, Final Batch Loss: 0.45010140538215637\n",
      "Epoch 1630, Loss: 0.9397778660058975, Final Batch Loss: 0.18733163177967072\n",
      "Epoch 1631, Loss: 0.8316302075982094, Final Batch Loss: 0.09595892578363419\n",
      "Epoch 1632, Loss: 1.3090953826904297, Final Batch Loss: 0.6349854469299316\n",
      "Epoch 1633, Loss: 1.2382100820541382, Final Batch Loss: 0.5887720584869385\n",
      "Epoch 1634, Loss: 1.0247196853160858, Final Batch Loss: 0.36368271708488464\n",
      "Epoch 1635, Loss: 0.8741442710161209, Final Batch Loss: 0.13550229370594025\n",
      "Epoch 1636, Loss: 1.1581939458847046, Final Batch Loss: 0.4918766915798187\n",
      "Epoch 1637, Loss: 1.014590710401535, Final Batch Loss: 0.2926943004131317\n",
      "Epoch 1638, Loss: 1.0858001410961151, Final Batch Loss: 0.39857617020606995\n",
      "Epoch 1639, Loss: 0.8442714512348175, Final Batch Loss: 0.2035732865333557\n",
      "Epoch 1640, Loss: 1.066971868276596, Final Batch Loss: 0.386793315410614\n",
      "Epoch 1641, Loss: 1.2614993155002594, Final Batch Loss: 0.5076444745063782\n",
      "Epoch 1642, Loss: 1.0878014862537384, Final Batch Loss: 0.3739122450351715\n",
      "Epoch 1643, Loss: 1.0756225287914276, Final Batch Loss: 0.4269673824310303\n",
      "Epoch 1644, Loss: 0.9440052509307861, Final Batch Loss: 0.30314475297927856\n",
      "Epoch 1645, Loss: 0.9901417195796967, Final Batch Loss: 0.24535658955574036\n",
      "Epoch 1646, Loss: 0.9629440009593964, Final Batch Loss: 0.28638049960136414\n",
      "Epoch 1647, Loss: 0.9242972433567047, Final Batch Loss: 0.19441717863082886\n",
      "Epoch 1648, Loss: 1.0171045362949371, Final Batch Loss: 0.3370402455329895\n",
      "Epoch 1649, Loss: 1.0693096220493317, Final Batch Loss: 0.3720019459724426\n",
      "Epoch 1650, Loss: 1.062576562166214, Final Batch Loss: 0.36540400981903076\n",
      "Epoch 1651, Loss: 0.8442625254392624, Final Batch Loss: 0.1489076167345047\n",
      "Epoch 1652, Loss: 1.0004664361476898, Final Batch Loss: 0.340328574180603\n",
      "Epoch 1653, Loss: 1.1213466823101044, Final Batch Loss: 0.43684300780296326\n",
      "Epoch 1654, Loss: 0.9748062789440155, Final Batch Loss: 0.2912294864654541\n",
      "Epoch 1655, Loss: 1.039090096950531, Final Batch Loss: 0.351252943277359\n",
      "Epoch 1656, Loss: 0.7779775932431221, Final Batch Loss: 0.09639520198106766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1657, Loss: 1.108250081539154, Final Batch Loss: 0.45247143507003784\n",
      "Epoch 1658, Loss: 1.037992775440216, Final Batch Loss: 0.3540152609348297\n",
      "Epoch 1659, Loss: 0.849588006734848, Final Batch Loss: 0.20581045746803284\n",
      "Epoch 1660, Loss: 0.9612293243408203, Final Batch Loss: 0.2810381352901459\n",
      "Epoch 1661, Loss: 1.117911696434021, Final Batch Loss: 0.3671298921108246\n",
      "Epoch 1662, Loss: 0.897187665104866, Final Batch Loss: 0.1610742062330246\n",
      "Epoch 1663, Loss: 0.9651276171207428, Final Batch Loss: 0.2909499406814575\n",
      "Epoch 1664, Loss: 0.8987091556191444, Final Batch Loss: 0.09982345253229141\n",
      "Epoch 1665, Loss: 0.9692788273096085, Final Batch Loss: 0.23493270576000214\n",
      "Epoch 1666, Loss: 0.9971007853746414, Final Batch Loss: 0.20761381089687347\n",
      "Epoch 1667, Loss: 1.2252178192138672, Final Batch Loss: 0.47054392099380493\n",
      "Epoch 1668, Loss: 0.8620901256799698, Final Batch Loss: 0.16827274858951569\n",
      "Epoch 1669, Loss: 0.941439300775528, Final Batch Loss: 0.25566697120666504\n",
      "Epoch 1670, Loss: 0.949794739484787, Final Batch Loss: 0.25667133927345276\n",
      "Epoch 1671, Loss: 0.8251690715551376, Final Batch Loss: 0.16716624796390533\n",
      "Epoch 1672, Loss: 0.9247903376817703, Final Batch Loss: 0.20530156791210175\n",
      "Epoch 1673, Loss: 0.8835092484951019, Final Batch Loss: 0.25439533591270447\n",
      "Epoch 1674, Loss: 1.2944517731666565, Final Batch Loss: 0.6296431422233582\n",
      "Epoch 1675, Loss: 1.0208321809768677, Final Batch Loss: 0.3058433532714844\n",
      "Epoch 1676, Loss: 1.029182106256485, Final Batch Loss: 0.4033697247505188\n",
      "Epoch 1677, Loss: 1.0985642671585083, Final Batch Loss: 0.3912706673145294\n",
      "Epoch 1678, Loss: 0.786779023706913, Final Batch Loss: 0.07390288263559341\n",
      "Epoch 1679, Loss: 1.3857838809490204, Final Batch Loss: 0.7347747683525085\n",
      "Epoch 1680, Loss: 0.724719911813736, Final Batch Loss: 0.08726230263710022\n",
      "Epoch 1681, Loss: 0.8647055476903915, Final Batch Loss: 0.1718171089887619\n",
      "Epoch 1682, Loss: 1.108214646577835, Final Batch Loss: 0.4645439386367798\n",
      "Epoch 1683, Loss: 1.0442352592945099, Final Batch Loss: 0.37890687584877014\n",
      "Epoch 1684, Loss: 1.0085417926311493, Final Batch Loss: 0.362654447555542\n",
      "Epoch 1685, Loss: 1.1123453974723816, Final Batch Loss: 0.39160963892936707\n",
      "Epoch 1686, Loss: 1.3273782432079315, Final Batch Loss: 0.6738479733467102\n",
      "Epoch 1687, Loss: 1.0649559497833252, Final Batch Loss: 0.4110261797904968\n",
      "Epoch 1688, Loss: 1.0906921327114105, Final Batch Loss: 0.3690723478794098\n",
      "Epoch 1689, Loss: 0.8384042903780937, Final Batch Loss: 0.12282710522413254\n",
      "Epoch 1690, Loss: 0.8893336057662964, Final Batch Loss: 0.19194841384887695\n",
      "Epoch 1691, Loss: 0.9849918782711029, Final Batch Loss: 0.3618677854537964\n",
      "Epoch 1692, Loss: 0.9425594508647919, Final Batch Loss: 0.21600687503814697\n",
      "Epoch 1693, Loss: 0.950302392244339, Final Batch Loss: 0.25057119131088257\n",
      "Epoch 1694, Loss: 1.0258301794528961, Final Batch Loss: 0.3418775200843811\n",
      "Epoch 1695, Loss: 1.1432759165763855, Final Batch Loss: 0.4643236994743347\n",
      "Epoch 1696, Loss: 0.8405665904283524, Final Batch Loss: 0.15245120227336884\n",
      "Epoch 1697, Loss: 1.1689953207969666, Final Batch Loss: 0.43956008553504944\n",
      "Epoch 1698, Loss: 1.1673931777477264, Final Batch Loss: 0.5062353014945984\n",
      "Epoch 1699, Loss: 1.066200077533722, Final Batch Loss: 0.43758273124694824\n",
      "Epoch 1700, Loss: 1.0810519754886627, Final Batch Loss: 0.37546494603157043\n",
      "Epoch 1701, Loss: 0.9097757190465927, Final Batch Loss: 0.19482235610485077\n",
      "Epoch 1702, Loss: 1.032162606716156, Final Batch Loss: 0.36444929242134094\n",
      "Epoch 1703, Loss: 0.8687492907047272, Final Batch Loss: 0.18099749088287354\n",
      "Epoch 1704, Loss: 1.0727594494819641, Final Batch Loss: 0.3772636651992798\n",
      "Epoch 1705, Loss: 0.9556495547294617, Final Batch Loss: 0.26851633191108704\n",
      "Epoch 1706, Loss: 1.1811327040195465, Final Batch Loss: 0.49561992287635803\n",
      "Epoch 1707, Loss: 0.8513820171356201, Final Batch Loss: 0.1536208987236023\n",
      "Epoch 1708, Loss: 1.0157269537448883, Final Batch Loss: 0.30373042821884155\n",
      "Epoch 1709, Loss: 1.170771211385727, Final Batch Loss: 0.5061876177787781\n",
      "Epoch 1710, Loss: 0.8765828460454941, Final Batch Loss: 0.1862015575170517\n",
      "Epoch 1711, Loss: 0.8696069121360779, Final Batch Loss: 0.18845224380493164\n",
      "Epoch 1712, Loss: 0.9491884410381317, Final Batch Loss: 0.32536056637763977\n",
      "Epoch 1713, Loss: 1.1177982687950134, Final Batch Loss: 0.38336971402168274\n",
      "Epoch 1714, Loss: 0.983890175819397, Final Batch Loss: 0.29222509264945984\n",
      "Epoch 1715, Loss: 0.8832318633794785, Final Batch Loss: 0.22359104454517365\n",
      "Epoch 1716, Loss: 1.513277679681778, Final Batch Loss: 0.8191680312156677\n",
      "Epoch 1717, Loss: 1.146798998117447, Final Batch Loss: 0.46703314781188965\n",
      "Epoch 1718, Loss: 0.9171105176210403, Final Batch Loss: 0.21269483864307404\n",
      "Epoch 1719, Loss: 1.1351976096630096, Final Batch Loss: 0.36437180638313293\n",
      "Epoch 1720, Loss: 0.9766943007707596, Final Batch Loss: 0.22812674939632416\n",
      "Epoch 1721, Loss: 1.1996488571166992, Final Batch Loss: 0.4483872354030609\n",
      "Epoch 1722, Loss: 1.421250194311142, Final Batch Loss: 0.7423293590545654\n",
      "Epoch 1723, Loss: 1.261447936296463, Final Batch Loss: 0.5345466732978821\n",
      "Epoch 1724, Loss: 1.2026644349098206, Final Batch Loss: 0.5000892877578735\n",
      "Epoch 1725, Loss: 1.2026024460792542, Final Batch Loss: 0.5064794421195984\n",
      "Epoch 1726, Loss: 0.8504278138279915, Final Batch Loss: 0.10224417597055435\n",
      "Epoch 1727, Loss: 1.0284452736377716, Final Batch Loss: 0.35879603028297424\n",
      "Epoch 1728, Loss: 0.8161951005458832, Final Batch Loss: 0.09830135107040405\n",
      "Epoch 1729, Loss: 0.9599508792161942, Final Batch Loss: 0.1798228770494461\n",
      "Epoch 1730, Loss: 1.0611565113067627, Final Batch Loss: 0.3086619973182678\n",
      "Epoch 1731, Loss: 1.0350672006607056, Final Batch Loss: 0.3165229558944702\n",
      "Epoch 1732, Loss: 1.0748347342014313, Final Batch Loss: 0.33646872639656067\n",
      "Epoch 1733, Loss: 0.986348420381546, Final Batch Loss: 0.26022663712501526\n",
      "Epoch 1734, Loss: 0.9633467197418213, Final Batch Loss: 0.25922641158103943\n",
      "Epoch 1735, Loss: 1.2268538177013397, Final Batch Loss: 0.5876265168190002\n",
      "Epoch 1736, Loss: 1.291866809129715, Final Batch Loss: 0.6444732546806335\n",
      "Epoch 1737, Loss: 1.1239073276519775, Final Batch Loss: 0.4504610002040863\n",
      "Epoch 1738, Loss: 0.9219769537448883, Final Batch Loss: 0.26972496509552\n",
      "Epoch 1739, Loss: 0.9556458741426468, Final Batch Loss: 0.21424360573291779\n",
      "Epoch 1740, Loss: 0.8945542573928833, Final Batch Loss: 0.23486760258674622\n",
      "Epoch 1741, Loss: 1.0063604712486267, Final Batch Loss: 0.34056729078292847\n",
      "Epoch 1742, Loss: 0.8751722425222397, Final Batch Loss: 0.2130868285894394\n",
      "Epoch 1743, Loss: 1.001315176486969, Final Batch Loss: 0.289813756942749\n",
      "Epoch 1744, Loss: 1.036584347486496, Final Batch Loss: 0.37921664118766785\n",
      "Epoch 1745, Loss: 1.1259118914604187, Final Batch Loss: 0.4527420103549957\n",
      "Epoch 1746, Loss: 0.7840152084827423, Final Batch Loss: 0.1378086507320404\n",
      "Epoch 1747, Loss: 1.008145660161972, Final Batch Loss: 0.21803298592567444\n",
      "Epoch 1748, Loss: 1.018218219280243, Final Batch Loss: 0.352883905172348\n",
      "Epoch 1749, Loss: 1.0261763334274292, Final Batch Loss: 0.425997793674469\n",
      "Epoch 1750, Loss: 1.0493594706058502, Final Batch Loss: 0.3774200677871704\n",
      "Epoch 1751, Loss: 0.9056466072797775, Final Batch Loss: 0.2291654795408249\n",
      "Epoch 1752, Loss: 0.7417838722467422, Final Batch Loss: 0.09808219969272614\n",
      "Epoch 1753, Loss: 1.3876836001873016, Final Batch Loss: 0.6848752498626709\n",
      "Epoch 1754, Loss: 1.0098548233509064, Final Batch Loss: 0.3094217777252197\n",
      "Epoch 1755, Loss: 1.0567394495010376, Final Batch Loss: 0.38626089692115784\n",
      "Epoch 1756, Loss: 0.9168397635221481, Final Batch Loss: 0.21486054360866547\n",
      "Epoch 1757, Loss: 0.9793764054775238, Final Batch Loss: 0.3207266330718994\n",
      "Epoch 1758, Loss: 0.9600089490413666, Final Batch Loss: 0.30070772767066956\n",
      "Epoch 1759, Loss: 1.0266207456588745, Final Batch Loss: 0.3872342109680176\n",
      "Epoch 1760, Loss: 1.0120451748371124, Final Batch Loss: 0.3010028302669525\n",
      "Epoch 1761, Loss: 0.9243116676807404, Final Batch Loss: 0.2915503978729248\n",
      "Epoch 1762, Loss: 0.9903257191181183, Final Batch Loss: 0.33094096183776855\n",
      "Epoch 1763, Loss: 1.2077638804912567, Final Batch Loss: 0.4767274856567383\n",
      "Epoch 1764, Loss: 1.2938542068004608, Final Batch Loss: 0.533772885799408\n",
      "Epoch 1765, Loss: 1.0191029012203217, Final Batch Loss: 0.22837603092193604\n",
      "Epoch 1766, Loss: 1.1648043990135193, Final Batch Loss: 0.47517329454421997\n",
      "Epoch 1767, Loss: 0.8077601194381714, Final Batch Loss: 0.12413105368614197\n",
      "Epoch 1768, Loss: 1.036029040813446, Final Batch Loss: 0.33432283997535706\n",
      "Epoch 1769, Loss: 1.2155743837356567, Final Batch Loss: 0.4816634953022003\n",
      "Epoch 1770, Loss: 0.9288920760154724, Final Batch Loss: 0.2484036684036255\n",
      "Epoch 1771, Loss: 1.1737131774425507, Final Batch Loss: 0.5331610441207886\n",
      "Epoch 1772, Loss: 0.9570403695106506, Final Batch Loss: 0.2949642539024353\n",
      "Epoch 1773, Loss: 1.1415860056877136, Final Batch Loss: 0.464678019285202\n",
      "Epoch 1774, Loss: 1.3530399799346924, Final Batch Loss: 0.556170642375946\n",
      "Epoch 1775, Loss: 1.0689474642276764, Final Batch Loss: 0.36578577756881714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1776, Loss: 0.999047726392746, Final Batch Loss: 0.3387008011341095\n",
      "Epoch 1777, Loss: 1.06328547000885, Final Batch Loss: 0.3669305741786957\n",
      "Epoch 1778, Loss: 1.1429382860660553, Final Batch Loss: 0.4587636888027191\n",
      "Epoch 1779, Loss: 1.090624064207077, Final Batch Loss: 0.4002494215965271\n",
      "Epoch 1780, Loss: 1.1227708756923676, Final Batch Loss: 0.40253809094429016\n",
      "Epoch 1781, Loss: 1.1464858651161194, Final Batch Loss: 0.43445447087287903\n",
      "Epoch 1782, Loss: 0.9722220599651337, Final Batch Loss: 0.2966083586215973\n",
      "Epoch 1783, Loss: 1.048597365617752, Final Batch Loss: 0.3950671851634979\n",
      "Epoch 1784, Loss: 1.1810934245586395, Final Batch Loss: 0.4999314248561859\n",
      "Epoch 1785, Loss: 0.8284646570682526, Final Batch Loss: 0.19209325313568115\n",
      "Epoch 1786, Loss: 1.060299426317215, Final Batch Loss: 0.3852297365665436\n",
      "Epoch 1787, Loss: 1.2161397337913513, Final Batch Loss: 0.5260770916938782\n",
      "Epoch 1788, Loss: 1.308099627494812, Final Batch Loss: 0.6306250691413879\n",
      "Epoch 1789, Loss: 1.009910136461258, Final Batch Loss: 0.3419434726238251\n",
      "Epoch 1790, Loss: 1.0020890533924103, Final Batch Loss: 0.3311775028705597\n",
      "Epoch 1791, Loss: 0.791076548397541, Final Batch Loss: 0.10933036357164383\n",
      "Epoch 1792, Loss: 1.260958194732666, Final Batch Loss: 0.5217725038528442\n",
      "Epoch 1793, Loss: 0.8090847730636597, Final Batch Loss: 0.14302411675453186\n",
      "Epoch 1794, Loss: 1.0064390897750854, Final Batch Loss: 0.3228667676448822\n",
      "Epoch 1795, Loss: 1.0206339955329895, Final Batch Loss: 0.354962557554245\n",
      "Epoch 1796, Loss: 1.0101763010025024, Final Batch Loss: 0.3048662841320038\n",
      "Epoch 1797, Loss: 0.8388234823942184, Final Batch Loss: 0.15817804634571075\n",
      "Epoch 1798, Loss: 1.1745598018169403, Final Batch Loss: 0.510403573513031\n",
      "Epoch 1799, Loss: 1.1125994622707367, Final Batch Loss: 0.4238261282444\n",
      "Epoch 1800, Loss: 0.9643571078777313, Final Batch Loss: 0.2717893123626709\n",
      "Epoch 1801, Loss: 0.8782205581665039, Final Batch Loss: 0.16844740509986877\n",
      "Epoch 1802, Loss: 1.065874844789505, Final Batch Loss: 0.35522952675819397\n",
      "Epoch 1803, Loss: 1.1581657528877258, Final Batch Loss: 0.45848599076271057\n",
      "Epoch 1804, Loss: 0.981233686208725, Final Batch Loss: 0.30450645089149475\n",
      "Epoch 1805, Loss: 1.177492469549179, Final Batch Loss: 0.49682068824768066\n",
      "Epoch 1806, Loss: 1.189547598361969, Final Batch Loss: 0.5329168438911438\n",
      "Epoch 1807, Loss: 0.9356307685375214, Final Batch Loss: 0.24778318405151367\n",
      "Epoch 1808, Loss: 0.9689063429832458, Final Batch Loss: 0.2690807580947876\n",
      "Epoch 1809, Loss: 0.893807515501976, Final Batch Loss: 0.2440348118543625\n",
      "Epoch 1810, Loss: 1.1162714064121246, Final Batch Loss: 0.3674733638763428\n",
      "Epoch 1811, Loss: 1.2919122278690338, Final Batch Loss: 0.5396003723144531\n",
      "Epoch 1812, Loss: 0.9921923279762268, Final Batch Loss: 0.3466624617576599\n",
      "Epoch 1813, Loss: 0.7574620842933655, Final Batch Loss: 0.13561856746673584\n",
      "Epoch 1814, Loss: 1.119149386882782, Final Batch Loss: 0.4708636999130249\n",
      "Epoch 1815, Loss: 0.8832982331514359, Final Batch Loss: 0.18945766985416412\n",
      "Epoch 1816, Loss: 1.4305636584758759, Final Batch Loss: 0.7561119794845581\n",
      "Epoch 1817, Loss: 0.8526698350906372, Final Batch Loss: 0.1877737045288086\n",
      "Epoch 1818, Loss: 1.1026036739349365, Final Batch Loss: 0.392142653465271\n",
      "Epoch 1819, Loss: 0.8939044028520584, Final Batch Loss: 0.22033177316188812\n",
      "Epoch 1820, Loss: 1.133583515882492, Final Batch Loss: 0.47353053092956543\n",
      "Epoch 1821, Loss: 0.9688010811805725, Final Batch Loss: 0.2675972282886505\n",
      "Epoch 1822, Loss: 1.1181144714355469, Final Batch Loss: 0.4380781352519989\n",
      "Epoch 1823, Loss: 0.8362542539834976, Final Batch Loss: 0.18089599907398224\n",
      "Epoch 1824, Loss: 0.9189711660146713, Final Batch Loss: 0.22468991577625275\n",
      "Epoch 1825, Loss: 0.9389583617448807, Final Batch Loss: 0.20132113993167877\n",
      "Epoch 1826, Loss: 1.0645881295204163, Final Batch Loss: 0.4070289433002472\n",
      "Epoch 1827, Loss: 0.8273616284132004, Final Batch Loss: 0.17666281759738922\n",
      "Epoch 1828, Loss: 1.1750641465187073, Final Batch Loss: 0.5006750226020813\n",
      "Epoch 1829, Loss: 0.823238268494606, Final Batch Loss: 0.14740018546581268\n",
      "Epoch 1830, Loss: 1.1566834151744843, Final Batch Loss: 0.49698683619499207\n",
      "Epoch 1831, Loss: 0.8987535089254379, Final Batch Loss: 0.22818733751773834\n",
      "Epoch 1832, Loss: 0.8172738626599312, Final Batch Loss: 0.08331380039453506\n",
      "Epoch 1833, Loss: 0.9962725341320038, Final Batch Loss: 0.37089499831199646\n",
      "Epoch 1834, Loss: 1.0827002227306366, Final Batch Loss: 0.41553622484207153\n",
      "Epoch 1835, Loss: 0.9535268545150757, Final Batch Loss: 0.3025601804256439\n",
      "Epoch 1836, Loss: 1.0945837497711182, Final Batch Loss: 0.4168689548969269\n",
      "Epoch 1837, Loss: 0.896935299038887, Final Batch Loss: 0.21734260022640228\n",
      "Epoch 1838, Loss: 0.9605660438537598, Final Batch Loss: 0.28973159193992615\n",
      "Epoch 1839, Loss: 0.8820894509553909, Final Batch Loss: 0.22985194623470306\n",
      "Epoch 1840, Loss: 1.1962802410125732, Final Batch Loss: 0.563812255859375\n",
      "Epoch 1841, Loss: 0.9555620551109314, Final Batch Loss: 0.30641594529151917\n",
      "Epoch 1842, Loss: 0.9485572278499603, Final Batch Loss: 0.29534491896629333\n",
      "Epoch 1843, Loss: 1.0783511996269226, Final Batch Loss: 0.4194657802581787\n",
      "Epoch 1844, Loss: 0.9047567844390869, Final Batch Loss: 0.2863103151321411\n",
      "Epoch 1845, Loss: 1.0928490459918976, Final Batch Loss: 0.4115450978279114\n",
      "Epoch 1846, Loss: 1.0976413488388062, Final Batch Loss: 0.46872344613075256\n",
      "Epoch 1847, Loss: 0.983537882566452, Final Batch Loss: 0.26193276047706604\n",
      "Epoch 1848, Loss: 0.7298877984285355, Final Batch Loss: 0.08176775276660919\n",
      "Epoch 1849, Loss: 1.2445264756679535, Final Batch Loss: 0.5863851308822632\n",
      "Epoch 1850, Loss: 0.9502860307693481, Final Batch Loss: 0.2881167531013489\n",
      "Epoch 1851, Loss: 1.1019562184810638, Final Batch Loss: 0.40069207549095154\n",
      "Epoch 1852, Loss: 0.9014374762773514, Final Batch Loss: 0.2345908135175705\n",
      "Epoch 1853, Loss: 0.8114315643906593, Final Batch Loss: 0.09916362911462784\n",
      "Epoch 1854, Loss: 0.9649849832057953, Final Batch Loss: 0.26406264305114746\n",
      "Epoch 1855, Loss: 0.8067693263292313, Final Batch Loss: 0.15717200934886932\n",
      "Epoch 1856, Loss: 0.8016788810491562, Final Batch Loss: 0.15161152184009552\n",
      "Epoch 1857, Loss: 0.9033088386058807, Final Batch Loss: 0.26276835799217224\n",
      "Epoch 1858, Loss: 0.9635307788848877, Final Batch Loss: 0.27960261702537537\n",
      "Epoch 1859, Loss: 0.9071677327156067, Final Batch Loss: 0.197717547416687\n",
      "Epoch 1860, Loss: 1.0398712754249573, Final Batch Loss: 0.36459586024284363\n",
      "Epoch 1861, Loss: 1.083722859621048, Final Batch Loss: 0.4267139434814453\n",
      "Epoch 1862, Loss: 0.9983211755752563, Final Batch Loss: 0.32180365920066833\n",
      "Epoch 1863, Loss: 0.9129252433776855, Final Batch Loss: 0.21264874935150146\n",
      "Epoch 1864, Loss: 1.3130198419094086, Final Batch Loss: 0.6281055808067322\n",
      "Epoch 1865, Loss: 0.9298811703920364, Final Batch Loss: 0.24173204600811005\n",
      "Epoch 1866, Loss: 1.032573252916336, Final Batch Loss: 0.3995679020881653\n",
      "Epoch 1867, Loss: 0.7958466708660126, Final Batch Loss: 0.15781459212303162\n",
      "Epoch 1868, Loss: 0.8484464585781097, Final Batch Loss: 0.23618030548095703\n",
      "Epoch 1869, Loss: 1.0656794309616089, Final Batch Loss: 0.42599818110466003\n",
      "Epoch 1870, Loss: 1.1145817935466766, Final Batch Loss: 0.5260510444641113\n",
      "Epoch 1871, Loss: 1.0123257637023926, Final Batch Loss: 0.3524995744228363\n",
      "Epoch 1872, Loss: 0.9190201759338379, Final Batch Loss: 0.19969090819358826\n",
      "Epoch 1873, Loss: 1.0612528622150421, Final Batch Loss: 0.3827478587627411\n",
      "Epoch 1874, Loss: 0.776535764336586, Final Batch Loss: 0.1423444300889969\n",
      "Epoch 1875, Loss: 1.4089215993881226, Final Batch Loss: 0.746959388256073\n",
      "Epoch 1876, Loss: 0.8903657495975494, Final Batch Loss: 0.2455138862133026\n",
      "Epoch 1877, Loss: 0.9872590899467468, Final Batch Loss: 0.31700599193573\n",
      "Epoch 1878, Loss: 0.875340461730957, Final Batch Loss: 0.22370103001594543\n",
      "Epoch 1879, Loss: 0.8813336789608002, Final Batch Loss: 0.26953020691871643\n",
      "Epoch 1880, Loss: 0.7885916978120804, Final Batch Loss: 0.1526857167482376\n",
      "Epoch 1881, Loss: 0.831233337521553, Final Batch Loss: 0.21128137409687042\n",
      "Epoch 1882, Loss: 0.9228959083557129, Final Batch Loss: 0.23522692918777466\n",
      "Epoch 1883, Loss: 1.1975158751010895, Final Batch Loss: 0.4955748915672302\n",
      "Epoch 1884, Loss: 1.1622955203056335, Final Batch Loss: 0.473846435546875\n",
      "Epoch 1885, Loss: 1.2453479170799255, Final Batch Loss: 0.537280797958374\n",
      "Epoch 1886, Loss: 1.1332115828990936, Final Batch Loss: 0.4389866292476654\n",
      "Epoch 1887, Loss: 0.8906924426555634, Final Batch Loss: 0.21769267320632935\n",
      "Epoch 1888, Loss: 0.8114533722400665, Final Batch Loss: 0.13289153575897217\n",
      "Epoch 1889, Loss: 1.0571293532848358, Final Batch Loss: 0.40858742594718933\n",
      "Epoch 1890, Loss: 0.9790478944778442, Final Batch Loss: 0.3153623342514038\n",
      "Epoch 1891, Loss: 0.9828528761863708, Final Batch Loss: 0.2877729535102844\n",
      "Epoch 1892, Loss: 1.0243248641490936, Final Batch Loss: 0.2961844503879547\n",
      "Epoch 1893, Loss: 1.0768166780471802, Final Batch Loss: 0.4522475600242615\n",
      "Epoch 1894, Loss: 0.9223936796188354, Final Batch Loss: 0.2150689959526062\n",
      "Epoch 1895, Loss: 1.1161567568778992, Final Batch Loss: 0.43575596809387207\n",
      "Epoch 1896, Loss: 1.2231040596961975, Final Batch Loss: 0.5000348091125488\n",
      "Epoch 1897, Loss: 1.0691604614257812, Final Batch Loss: 0.37037691473960876\n",
      "Epoch 1898, Loss: 1.0610140562057495, Final Batch Loss: 0.4395260512828827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1899, Loss: 1.098759114742279, Final Batch Loss: 0.46602606773376465\n",
      "Epoch 1900, Loss: 0.8945351392030716, Final Batch Loss: 0.22643350064754486\n",
      "Epoch 1901, Loss: 0.9954308569431305, Final Batch Loss: 0.34090903401374817\n",
      "Epoch 1902, Loss: 0.7488192543387413, Final Batch Loss: 0.09345542639493942\n",
      "Epoch 1903, Loss: 0.9275643527507782, Final Batch Loss: 0.27908995747566223\n",
      "Epoch 1904, Loss: 0.9271690249443054, Final Batch Loss: 0.26929569244384766\n",
      "Epoch 1905, Loss: 0.929902657866478, Final Batch Loss: 0.24700461328029633\n",
      "Epoch 1906, Loss: 0.9039938598871231, Final Batch Loss: 0.23308812081813812\n",
      "Epoch 1907, Loss: 1.0299493968486786, Final Batch Loss: 0.4231427311897278\n",
      "Epoch 1908, Loss: 0.9743348360061646, Final Batch Loss: 0.358271062374115\n",
      "Epoch 1909, Loss: 1.0555935204029083, Final Batch Loss: 0.46534863114356995\n",
      "Epoch 1910, Loss: 1.1056521236896515, Final Batch Loss: 0.47161152958869934\n",
      "Epoch 1911, Loss: 0.9457659721374512, Final Batch Loss: 0.29069676995277405\n",
      "Epoch 1912, Loss: 0.9771220982074738, Final Batch Loss: 0.2839704155921936\n",
      "Epoch 1913, Loss: 0.9874722957611084, Final Batch Loss: 0.251660019159317\n",
      "Epoch 1914, Loss: 0.7658648490905762, Final Batch Loss: 0.11060374975204468\n",
      "Epoch 1915, Loss: 0.9282641112804413, Final Batch Loss: 0.22144287824630737\n",
      "Epoch 1916, Loss: 0.9170175343751907, Final Batch Loss: 0.24365465342998505\n",
      "Epoch 1917, Loss: 0.9283820390701294, Final Batch Loss: 0.25010478496551514\n",
      "Epoch 1918, Loss: 1.0190504491329193, Final Batch Loss: 0.3085814118385315\n",
      "Epoch 1919, Loss: 0.9647542536258698, Final Batch Loss: 0.3255540728569031\n",
      "Epoch 1920, Loss: 1.595903903245926, Final Batch Loss: 0.9014037251472473\n",
      "Epoch 1921, Loss: 0.9195116460323334, Final Batch Loss: 0.22493189573287964\n",
      "Epoch 1922, Loss: 0.9312726259231567, Final Batch Loss: 0.2545890808105469\n",
      "Epoch 1923, Loss: 0.8503168672323227, Final Batch Loss: 0.24963851273059845\n",
      "Epoch 1924, Loss: 0.962079256772995, Final Batch Loss: 0.38653048872947693\n",
      "Epoch 1925, Loss: 0.9515145421028137, Final Batch Loss: 0.3248061239719391\n",
      "Epoch 1926, Loss: 0.9614119231700897, Final Batch Loss: 0.31148311495780945\n",
      "Epoch 1927, Loss: 0.917753130197525, Final Batch Loss: 0.23371058702468872\n",
      "Epoch 1928, Loss: 0.7235895693302155, Final Batch Loss: 0.11006942391395569\n",
      "Epoch 1929, Loss: 0.9464169144630432, Final Batch Loss: 0.3034127950668335\n",
      "Epoch 1930, Loss: 0.8563547879457474, Final Batch Loss: 0.23845939338207245\n",
      "Epoch 1931, Loss: 0.9317469447851181, Final Batch Loss: 0.2429191917181015\n",
      "Epoch 1932, Loss: 0.9011062383651733, Final Batch Loss: 0.23655617237091064\n",
      "Epoch 1933, Loss: 0.9260306060314178, Final Batch Loss: 0.3123343288898468\n",
      "Epoch 1934, Loss: 1.1627265810966492, Final Batch Loss: 0.5083315968513489\n",
      "Epoch 1935, Loss: 0.9024359881877899, Final Batch Loss: 0.2239038050174713\n",
      "Epoch 1936, Loss: 1.0298342406749725, Final Batch Loss: 0.3555222153663635\n",
      "Epoch 1937, Loss: 1.1166719794273376, Final Batch Loss: 0.5063086152076721\n",
      "Epoch 1938, Loss: 0.9697274267673492, Final Batch Loss: 0.2407439947128296\n",
      "Epoch 1939, Loss: 0.8598553985357285, Final Batch Loss: 0.19104652106761932\n",
      "Epoch 1940, Loss: 0.9706758558750153, Final Batch Loss: 0.2917179465293884\n",
      "Epoch 1941, Loss: 1.365875482559204, Final Batch Loss: 0.7261494398117065\n",
      "Epoch 1942, Loss: 0.8879182785749435, Final Batch Loss: 0.232451394200325\n",
      "Epoch 1943, Loss: 1.1010031402111053, Final Batch Loss: 0.47064319252967834\n",
      "Epoch 1944, Loss: 0.8869978189468384, Final Batch Loss: 0.2799055576324463\n",
      "Epoch 1945, Loss: 0.9638534784317017, Final Batch Loss: 0.3351944386959076\n",
      "Epoch 1946, Loss: 0.9241911768913269, Final Batch Loss: 0.2822573781013489\n",
      "Epoch 1947, Loss: 1.1286175549030304, Final Batch Loss: 0.47773727774620056\n",
      "Epoch 1948, Loss: 0.9813063740730286, Final Batch Loss: 0.31636443734169006\n",
      "Epoch 1949, Loss: 0.8781681209802628, Final Batch Loss: 0.24636422097682953\n",
      "Epoch 1950, Loss: 0.8726765811443329, Final Batch Loss: 0.2310391664505005\n",
      "Epoch 1951, Loss: 0.9341477006673813, Final Batch Loss: 0.1988181620836258\n",
      "Epoch 1952, Loss: 1.09903284907341, Final Batch Loss: 0.4128042161464691\n",
      "Epoch 1953, Loss: 1.1336564719676971, Final Batch Loss: 0.4163791537284851\n",
      "Epoch 1954, Loss: 0.9991993308067322, Final Batch Loss: 0.35110238194465637\n",
      "Epoch 1955, Loss: 1.004914551973343, Final Batch Loss: 0.3418189585208893\n",
      "Epoch 1956, Loss: 0.9620982110500336, Final Batch Loss: 0.30522775650024414\n",
      "Epoch 1957, Loss: 0.9438643157482147, Final Batch Loss: 0.2930435240268707\n",
      "Epoch 1958, Loss: 1.0008767247200012, Final Batch Loss: 0.34228503704071045\n",
      "Epoch 1959, Loss: 0.9499138593673706, Final Batch Loss: 0.2543003261089325\n",
      "Epoch 1960, Loss: 1.0400810539722443, Final Batch Loss: 0.42620834708213806\n",
      "Epoch 1961, Loss: 1.087016761302948, Final Batch Loss: 0.44128045439720154\n",
      "Epoch 1962, Loss: 0.7412276044487953, Final Batch Loss: 0.10017786175012589\n",
      "Epoch 1963, Loss: 0.8337605893611908, Final Batch Loss: 0.16295617818832397\n",
      "Epoch 1964, Loss: 0.7967537641525269, Final Batch Loss: 0.1477598249912262\n",
      "Epoch 1965, Loss: 1.0799376666545868, Final Batch Loss: 0.4034593105316162\n",
      "Epoch 1966, Loss: 0.9163653999567032, Final Batch Loss: 0.24944157898426056\n",
      "Epoch 1967, Loss: 1.010799616575241, Final Batch Loss: 0.4141826033592224\n",
      "Epoch 1968, Loss: 0.9305987656116486, Final Batch Loss: 0.25526952743530273\n",
      "Epoch 1969, Loss: 0.9876314699649811, Final Batch Loss: 0.39326292276382446\n",
      "Epoch 1970, Loss: 0.8337818682193756, Final Batch Loss: 0.18218660354614258\n",
      "Epoch 1971, Loss: 0.9211238026618958, Final Batch Loss: 0.2302454710006714\n",
      "Epoch 1972, Loss: 0.947939932346344, Final Batch Loss: 0.26565590500831604\n",
      "Epoch 1973, Loss: 0.8499316573143005, Final Batch Loss: 0.20438355207443237\n",
      "Epoch 1974, Loss: 1.1740018725395203, Final Batch Loss: 0.5823270082473755\n",
      "Epoch 1975, Loss: 1.1639058589935303, Final Batch Loss: 0.5220810770988464\n",
      "Epoch 1976, Loss: 0.8046103268861771, Final Batch Loss: 0.20320118963718414\n",
      "Epoch 1977, Loss: 1.040571004152298, Final Batch Loss: 0.32118910551071167\n",
      "Epoch 1978, Loss: 0.7706578820943832, Final Batch Loss: 0.15569566190242767\n",
      "Epoch 1979, Loss: 1.0581333935260773, Final Batch Loss: 0.4402121603488922\n",
      "Epoch 1980, Loss: 1.0562532246112823, Final Batch Loss: 0.42673221230506897\n",
      "Epoch 1981, Loss: 0.8652899265289307, Final Batch Loss: 0.22251176834106445\n",
      "Epoch 1982, Loss: 0.82129967212677, Final Batch Loss: 0.17801231145858765\n",
      "Epoch 1983, Loss: 0.9626080095767975, Final Batch Loss: 0.32038846611976624\n",
      "Epoch 1984, Loss: 1.0745303630828857, Final Batch Loss: 0.44126951694488525\n",
      "Epoch 1985, Loss: 0.889803558588028, Final Batch Loss: 0.2086123526096344\n",
      "Epoch 1986, Loss: 0.8772990256547928, Final Batch Loss: 0.20759694278240204\n",
      "Epoch 1987, Loss: 0.7612411975860596, Final Batch Loss: 0.0625045895576477\n",
      "Epoch 1988, Loss: 0.9229983389377594, Final Batch Loss: 0.25543540716171265\n",
      "Epoch 1989, Loss: 0.9234868884086609, Final Batch Loss: 0.29387375712394714\n",
      "Epoch 1990, Loss: 1.0954889059066772, Final Batch Loss: 0.44963154196739197\n",
      "Epoch 1991, Loss: 1.1687211394309998, Final Batch Loss: 0.5736318230628967\n",
      "Epoch 1992, Loss: 0.9576884806156158, Final Batch Loss: 0.2642500400543213\n",
      "Epoch 1993, Loss: 1.0938531756401062, Final Batch Loss: 0.39325594902038574\n",
      "Epoch 1994, Loss: 0.9341705143451691, Final Batch Loss: 0.31149330735206604\n",
      "Epoch 1995, Loss: 0.9127324521541595, Final Batch Loss: 0.2545190751552582\n",
      "Epoch 1996, Loss: 0.8957457095384598, Final Batch Loss: 0.23314981162548065\n",
      "Epoch 1997, Loss: 0.8747721165418625, Final Batch Loss: 0.21657924354076385\n",
      "Epoch 1998, Loss: 0.9632954299449921, Final Batch Loss: 0.34997156262397766\n",
      "Epoch 1999, Loss: 0.8271067440509796, Final Batch Loss: 0.18701297044754028\n",
      "Epoch 2000, Loss: 0.7883646339178085, Final Batch Loss: 0.13241632282733917\n",
      "Epoch 2001, Loss: 0.9959171712398529, Final Batch Loss: 0.44159141182899475\n",
      "Epoch 2002, Loss: 0.9108474254608154, Final Batch Loss: 0.332807332277298\n",
      "Epoch 2003, Loss: 0.9339105188846588, Final Batch Loss: 0.284447580575943\n",
      "Epoch 2004, Loss: 0.7998429983854294, Final Batch Loss: 0.18121959269046783\n",
      "Epoch 2005, Loss: 0.7559413909912109, Final Batch Loss: 0.13617631793022156\n",
      "Epoch 2006, Loss: 1.0721597075462341, Final Batch Loss: 0.4251477122306824\n",
      "Epoch 2007, Loss: 1.061082363128662, Final Batch Loss: 0.4044544994831085\n",
      "Epoch 2008, Loss: 0.895791620016098, Final Batch Loss: 0.3229529857635498\n",
      "Epoch 2009, Loss: 0.9526269137859344, Final Batch Loss: 0.27758434414863586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2010, Loss: 1.0023800432682037, Final Batch Loss: 0.3258413076400757\n",
      "Epoch 2011, Loss: 0.9074434340000153, Final Batch Loss: 0.3210393190383911\n",
      "Epoch 2012, Loss: 1.033296376466751, Final Batch Loss: 0.3540628254413605\n",
      "Epoch 2013, Loss: 1.2299789190292358, Final Batch Loss: 0.5615839958190918\n",
      "Epoch 2014, Loss: 0.8750750422477722, Final Batch Loss: 0.311292439699173\n",
      "Epoch 2015, Loss: 0.9550526142120361, Final Batch Loss: 0.28797677159309387\n",
      "Epoch 2016, Loss: 1.1707079112529755, Final Batch Loss: 0.4937925934791565\n",
      "Epoch 2017, Loss: 1.1209132373332977, Final Batch Loss: 0.5005034804344177\n",
      "Epoch 2018, Loss: 0.9892841279506683, Final Batch Loss: 0.32010963559150696\n",
      "Epoch 2019, Loss: 1.0433287620544434, Final Batch Loss: 0.45932769775390625\n",
      "Epoch 2020, Loss: 0.9082143306732178, Final Batch Loss: 0.23144114017486572\n",
      "Epoch 2021, Loss: 1.1437968015670776, Final Batch Loss: 0.5468708276748657\n",
      "Epoch 2022, Loss: 1.0117157101631165, Final Batch Loss: 0.40263205766677856\n",
      "Epoch 2023, Loss: 0.9156447947025299, Final Batch Loss: 0.2830416262149811\n",
      "Epoch 2024, Loss: 0.8707118928432465, Final Batch Loss: 0.23817026615142822\n",
      "Epoch 2025, Loss: 0.8350958973169327, Final Batch Loss: 0.21274907886981964\n",
      "Epoch 2026, Loss: 1.178058922290802, Final Batch Loss: 0.5048531889915466\n",
      "Epoch 2027, Loss: 1.0322960317134857, Final Batch Loss: 0.4113193452358246\n",
      "Epoch 2028, Loss: 1.0182775259017944, Final Batch Loss: 0.4254675805568695\n",
      "Epoch 2029, Loss: 0.8897290676832199, Final Batch Loss: 0.2223370224237442\n",
      "Epoch 2030, Loss: 1.1157205998897552, Final Batch Loss: 0.45084160566329956\n",
      "Epoch 2031, Loss: 1.0009508728981018, Final Batch Loss: 0.3257140815258026\n",
      "Epoch 2032, Loss: 1.1096708178520203, Final Batch Loss: 0.40857645869255066\n",
      "Epoch 2033, Loss: 0.8677572160959244, Final Batch Loss: 0.23515360057353973\n",
      "Epoch 2034, Loss: 1.1455073952674866, Final Batch Loss: 0.5200687050819397\n",
      "Epoch 2035, Loss: 1.0769841969013214, Final Batch Loss: 0.4404052495956421\n",
      "Epoch 2036, Loss: 0.909026712179184, Final Batch Loss: 0.23319008946418762\n",
      "Epoch 2037, Loss: 1.0167569518089294, Final Batch Loss: 0.3574696183204651\n",
      "Epoch 2038, Loss: 0.9982709884643555, Final Batch Loss: 0.3917459547519684\n",
      "Epoch 2039, Loss: 0.8739034682512283, Final Batch Loss: 0.21878741681575775\n",
      "Epoch 2040, Loss: 0.8266589269042015, Final Batch Loss: 0.11956688016653061\n",
      "Epoch 2041, Loss: 1.0272734463214874, Final Batch Loss: 0.32634520530700684\n",
      "Epoch 2042, Loss: 0.8393548429012299, Final Batch Loss: 0.17748981714248657\n",
      "Epoch 2043, Loss: 0.9712638556957245, Final Batch Loss: 0.33834555745124817\n",
      "Epoch 2044, Loss: 0.9384328126907349, Final Batch Loss: 0.31464916467666626\n",
      "Epoch 2045, Loss: 0.9112532734870911, Final Batch Loss: 0.318150132894516\n",
      "Epoch 2046, Loss: 0.8924823999404907, Final Batch Loss: 0.1913505494594574\n",
      "Epoch 2047, Loss: 0.794693797826767, Final Batch Loss: 0.1522480845451355\n",
      "Epoch 2048, Loss: 0.8541330844163895, Final Batch Loss: 0.213698610663414\n",
      "Epoch 2049, Loss: 0.834098219871521, Final Batch Loss: 0.17443308234214783\n",
      "Epoch 2050, Loss: 0.8658161610364914, Final Batch Loss: 0.21008707582950592\n",
      "Epoch 2051, Loss: 0.721820205450058, Final Batch Loss: 0.09050756692886353\n",
      "Epoch 2052, Loss: 0.7793890982866287, Final Batch Loss: 0.11800940334796906\n",
      "Epoch 2053, Loss: 0.8315413147211075, Final Batch Loss: 0.16224567592144012\n",
      "Epoch 2054, Loss: 1.003387987613678, Final Batch Loss: 0.39644500613212585\n",
      "Epoch 2055, Loss: 0.8575232923030853, Final Batch Loss: 0.23678123950958252\n",
      "Epoch 2056, Loss: 0.755271390080452, Final Batch Loss: 0.13913507759571075\n",
      "Epoch 2057, Loss: 0.7925185710191727, Final Batch Loss: 0.18422041833400726\n",
      "Epoch 2058, Loss: 0.7299855127930641, Final Batch Loss: 0.11438164860010147\n",
      "Epoch 2059, Loss: 0.8729812502861023, Final Batch Loss: 0.29408711194992065\n",
      "Epoch 2060, Loss: 1.1192832291126251, Final Batch Loss: 0.4853058457374573\n",
      "Epoch 2061, Loss: 0.8918425291776657, Final Batch Loss: 0.23815222084522247\n",
      "Epoch 2062, Loss: 1.0841033458709717, Final Batch Loss: 0.484942227602005\n",
      "Epoch 2063, Loss: 1.1763297319412231, Final Batch Loss: 0.5441131591796875\n",
      "Epoch 2064, Loss: 1.0631054639816284, Final Batch Loss: 0.366925448179245\n",
      "Epoch 2065, Loss: 0.7190231755375862, Final Batch Loss: 0.1216488853096962\n",
      "Epoch 2066, Loss: 1.0048553943634033, Final Batch Loss: 0.4090440273284912\n",
      "Epoch 2067, Loss: 1.1020084619522095, Final Batch Loss: 0.4770647883415222\n",
      "Epoch 2068, Loss: 0.9642941653728485, Final Batch Loss: 0.36760005354881287\n",
      "Epoch 2069, Loss: 0.9631657600402832, Final Batch Loss: 0.31373727321624756\n",
      "Epoch 2070, Loss: 0.9785511195659637, Final Batch Loss: 0.2725791037082672\n",
      "Epoch 2071, Loss: 1.0066052973270416, Final Batch Loss: 0.3910413682460785\n",
      "Epoch 2072, Loss: 0.8707998991012573, Final Batch Loss: 0.28313857316970825\n",
      "Epoch 2073, Loss: 0.8911586701869965, Final Batch Loss: 0.3162209987640381\n",
      "Epoch 2074, Loss: 1.0408716201782227, Final Batch Loss: 0.44648823142051697\n",
      "Epoch 2075, Loss: 0.8045722246170044, Final Batch Loss: 0.18709182739257812\n",
      "Epoch 2076, Loss: 0.8345156311988831, Final Batch Loss: 0.14524614810943604\n",
      "Epoch 2077, Loss: 0.8368543237447739, Final Batch Loss: 0.213770791888237\n",
      "Epoch 2078, Loss: 0.7948406636714935, Final Batch Loss: 0.1929636299610138\n",
      "Epoch 2079, Loss: 0.6962860897183418, Final Batch Loss: 0.09238048642873764\n",
      "Epoch 2080, Loss: 1.0227373838424683, Final Batch Loss: 0.38249969482421875\n",
      "Epoch 2081, Loss: 0.8661108613014221, Final Batch Loss: 0.27742382884025574\n",
      "Epoch 2082, Loss: 1.2335540056228638, Final Batch Loss: 0.5044676661491394\n",
      "Epoch 2083, Loss: 0.9752148389816284, Final Batch Loss: 0.3004842698574066\n",
      "Epoch 2084, Loss: 0.8305232673883438, Final Batch Loss: 0.24241964519023895\n",
      "Epoch 2085, Loss: 1.0401474237442017, Final Batch Loss: 0.41367095708847046\n",
      "Epoch 2086, Loss: 0.9467577040195465, Final Batch Loss: 0.32543060183525085\n",
      "Epoch 2087, Loss: 1.2604258954524994, Final Batch Loss: 0.6066642999649048\n",
      "Epoch 2088, Loss: 0.7424288168549538, Final Batch Loss: 0.09614301472902298\n",
      "Epoch 2089, Loss: 0.9728302657604218, Final Batch Loss: 0.29154637455940247\n",
      "Epoch 2090, Loss: 1.2560476660728455, Final Batch Loss: 0.619390606880188\n",
      "Epoch 2091, Loss: 0.8327203989028931, Final Batch Loss: 0.2267698049545288\n",
      "Epoch 2092, Loss: 0.9642318785190582, Final Batch Loss: 0.33458855748176575\n",
      "Epoch 2093, Loss: 0.888105034828186, Final Batch Loss: 0.2930174171924591\n",
      "Epoch 2094, Loss: 0.9078203737735748, Final Batch Loss: 0.3195997178554535\n",
      "Epoch 2095, Loss: 1.1186615228652954, Final Batch Loss: 0.42963066697120667\n",
      "Epoch 2096, Loss: 1.045286625623703, Final Batch Loss: 0.33796700835227966\n",
      "Epoch 2097, Loss: 1.2648201882839203, Final Batch Loss: 0.6007237434387207\n",
      "Epoch 2098, Loss: 0.9918926656246185, Final Batch Loss: 0.38442859053611755\n",
      "Epoch 2099, Loss: 0.9053578674793243, Final Batch Loss: 0.2888915538787842\n",
      "Epoch 2100, Loss: 0.8036447316408157, Final Batch Loss: 0.17569364607334137\n",
      "Epoch 2101, Loss: 0.8005409687757492, Final Batch Loss: 0.19201289117336273\n",
      "Epoch 2102, Loss: 0.9758911430835724, Final Batch Loss: 0.2991168797016144\n",
      "Epoch 2103, Loss: 0.8076105415821075, Final Batch Loss: 0.16388395428657532\n",
      "Epoch 2104, Loss: 1.0013209283351898, Final Batch Loss: 0.33165621757507324\n",
      "Epoch 2105, Loss: 1.1159250438213348, Final Batch Loss: 0.45655742287635803\n",
      "Epoch 2106, Loss: 0.7715937793254852, Final Batch Loss: 0.12936294078826904\n",
      "Epoch 2107, Loss: 0.8815750479698181, Final Batch Loss: 0.22072139382362366\n",
      "Epoch 2108, Loss: 0.99442058801651, Final Batch Loss: 0.2994844913482666\n",
      "Epoch 2109, Loss: 1.0177308022975922, Final Batch Loss: 0.35189685225486755\n",
      "Epoch 2110, Loss: 0.9651150405406952, Final Batch Loss: 0.3281831741333008\n",
      "Epoch 2111, Loss: 0.9502394199371338, Final Batch Loss: 0.336662232875824\n",
      "Epoch 2112, Loss: 0.9889784753322601, Final Batch Loss: 0.35324329137802124\n",
      "Epoch 2113, Loss: 1.023633986711502, Final Batch Loss: 0.44576188921928406\n",
      "Epoch 2114, Loss: 0.929862380027771, Final Batch Loss: 0.3025513291358948\n",
      "Epoch 2115, Loss: 0.7968975901603699, Final Batch Loss: 0.18357646465301514\n",
      "Epoch 2116, Loss: 1.0282629430294037, Final Batch Loss: 0.37484994530677795\n",
      "Epoch 2117, Loss: 0.8754232227802277, Final Batch Loss: 0.2707068920135498\n",
      "Epoch 2118, Loss: 1.068738728761673, Final Batch Loss: 0.36189642548561096\n",
      "Epoch 2119, Loss: 1.0597766041755676, Final Batch Loss: 0.3800116181373596\n",
      "Epoch 2120, Loss: 1.017086684703827, Final Batch Loss: 0.28670987486839294\n",
      "Epoch 2121, Loss: 0.7873853594064713, Final Batch Loss: 0.16494174301624298\n",
      "Epoch 2122, Loss: 0.8852066695690155, Final Batch Loss: 0.2642805576324463\n",
      "Epoch 2123, Loss: 0.9015519618988037, Final Batch Loss: 0.2523927092552185\n",
      "Epoch 2124, Loss: 0.8557787984609604, Final Batch Loss: 0.23650340735912323\n",
      "Epoch 2125, Loss: 0.842577338218689, Final Batch Loss: 0.21733251214027405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2126, Loss: 0.8856310546398163, Final Batch Loss: 0.2943977117538452\n",
      "Epoch 2127, Loss: 0.946929931640625, Final Batch Loss: 0.30498093366622925\n",
      "Epoch 2128, Loss: 0.8235775530338287, Final Batch Loss: 0.1806895136833191\n",
      "Epoch 2129, Loss: 0.9599853456020355, Final Batch Loss: 0.35935330390930176\n",
      "Epoch 2130, Loss: 0.8452766537666321, Final Batch Loss: 0.2881045639514923\n",
      "Epoch 2131, Loss: 0.9231488257646561, Final Batch Loss: 0.2760295867919922\n",
      "Epoch 2132, Loss: 1.254666954278946, Final Batch Loss: 0.5867373943328857\n",
      "Epoch 2133, Loss: 0.9430772662162781, Final Batch Loss: 0.3261212110519409\n",
      "Epoch 2134, Loss: 0.8134166151285172, Final Batch Loss: 0.13324479758739471\n",
      "Epoch 2135, Loss: 0.9981550574302673, Final Batch Loss: 0.27747735381126404\n",
      "Epoch 2136, Loss: 0.8837332874536514, Final Batch Loss: 0.23411087691783905\n",
      "Epoch 2137, Loss: 0.7530854493379593, Final Batch Loss: 0.08916573226451874\n",
      "Epoch 2138, Loss: 0.9725283086299896, Final Batch Loss: 0.34148702025413513\n",
      "Epoch 2139, Loss: 0.7551240921020508, Final Batch Loss: 0.09439355134963989\n",
      "Epoch 2140, Loss: 0.7699261754751205, Final Batch Loss: 0.1854950338602066\n",
      "Epoch 2141, Loss: 1.0326935946941376, Final Batch Loss: 0.39259758591651917\n",
      "Epoch 2142, Loss: 0.7349814176559448, Final Batch Loss: 0.16277441382408142\n",
      "Epoch 2143, Loss: 0.8346278518438339, Final Batch Loss: 0.19973395764827728\n",
      "Epoch 2144, Loss: 0.73573337495327, Final Batch Loss: 0.10440404713153839\n",
      "Epoch 2145, Loss: 1.3122460842132568, Final Batch Loss: 0.6789409518241882\n",
      "Epoch 2146, Loss: 0.9223956167697906, Final Batch Loss: 0.2615630328655243\n",
      "Epoch 2147, Loss: 0.9036080241203308, Final Batch Loss: 0.26726987957954407\n",
      "Epoch 2148, Loss: 1.071400448679924, Final Batch Loss: 0.511719822883606\n",
      "Epoch 2149, Loss: 1.0044192671775818, Final Batch Loss: 0.34648317098617554\n",
      "Epoch 2150, Loss: 1.1529002785682678, Final Batch Loss: 0.5455134510993958\n",
      "Epoch 2151, Loss: 1.023733139038086, Final Batch Loss: 0.4025648236274719\n",
      "Epoch 2152, Loss: 0.7815997898578644, Final Batch Loss: 0.20135891437530518\n",
      "Epoch 2153, Loss: 1.1222525238990784, Final Batch Loss: 0.48093098402023315\n",
      "Epoch 2154, Loss: 0.7117869630455971, Final Batch Loss: 0.11947069317102432\n",
      "Epoch 2155, Loss: 0.7618949264287949, Final Batch Loss: 0.1382308453321457\n",
      "Epoch 2156, Loss: 1.132867157459259, Final Batch Loss: 0.5125724673271179\n",
      "Epoch 2157, Loss: 1.2581281065940857, Final Batch Loss: 0.6434681415557861\n",
      "Epoch 2158, Loss: 0.9792773425579071, Final Batch Loss: 0.3138974606990814\n",
      "Epoch 2159, Loss: 0.8061390221118927, Final Batch Loss: 0.16266992688179016\n",
      "Epoch 2160, Loss: 0.8657098114490509, Final Batch Loss: 0.18508702516555786\n",
      "Epoch 2161, Loss: 1.1600300669670105, Final Batch Loss: 0.48609408736228943\n",
      "Epoch 2162, Loss: 0.7686297222971916, Final Batch Loss: 0.07286529988050461\n",
      "Epoch 2163, Loss: 0.9641480445861816, Final Batch Loss: 0.33078253269195557\n",
      "Epoch 2164, Loss: 0.8505071103572845, Final Batch Loss: 0.20929023623466492\n",
      "Epoch 2165, Loss: 0.93143430352211, Final Batch Loss: 0.27087393403053284\n",
      "Epoch 2166, Loss: 0.9388857483863831, Final Batch Loss: 0.3122275769710541\n",
      "Epoch 2167, Loss: 0.8558768332004547, Final Batch Loss: 0.19843730330467224\n",
      "Epoch 2168, Loss: 1.0597925186157227, Final Batch Loss: 0.3894767761230469\n",
      "Epoch 2169, Loss: 0.8630626052618027, Final Batch Loss: 0.21433712542057037\n",
      "Epoch 2170, Loss: 0.9293708801269531, Final Batch Loss: 0.31497064232826233\n",
      "Epoch 2171, Loss: 1.1952287256717682, Final Batch Loss: 0.547770082950592\n",
      "Epoch 2172, Loss: 0.7806212455034256, Final Batch Loss: 0.1853945404291153\n",
      "Epoch 2173, Loss: 0.6997997164726257, Final Batch Loss: 0.07932466268539429\n",
      "Epoch 2174, Loss: 0.8648146390914917, Final Batch Loss: 0.21837010979652405\n",
      "Epoch 2175, Loss: 1.0606228113174438, Final Batch Loss: 0.42694225907325745\n",
      "Epoch 2176, Loss: 0.9429330378770828, Final Batch Loss: 0.24753181636333466\n",
      "Epoch 2177, Loss: 0.9383314847946167, Final Batch Loss: 0.2930189073085785\n",
      "Epoch 2178, Loss: 1.060301661491394, Final Batch Loss: 0.4463345408439636\n",
      "Epoch 2179, Loss: 0.8657017201185226, Final Batch Loss: 0.19529415667057037\n",
      "Epoch 2180, Loss: 1.0622678101062775, Final Batch Loss: 0.4383356273174286\n",
      "Epoch 2181, Loss: 0.7185113728046417, Final Batch Loss: 0.08728200197219849\n",
      "Epoch 2182, Loss: 1.079601138830185, Final Batch Loss: 0.4637705087661743\n",
      "Epoch 2183, Loss: 1.1267249286174774, Final Batch Loss: 0.5045374035835266\n",
      "Epoch 2184, Loss: 1.0526228845119476, Final Batch Loss: 0.38449999690055847\n",
      "Epoch 2185, Loss: 0.997511476278305, Final Batch Loss: 0.3024822175502777\n",
      "Epoch 2186, Loss: 1.4813209772109985, Final Batch Loss: 0.8063675165176392\n",
      "Epoch 2187, Loss: 0.8825173676013947, Final Batch Loss: 0.23367232084274292\n",
      "Epoch 2188, Loss: 1.1792683899402618, Final Batch Loss: 0.48687970638275146\n",
      "Epoch 2189, Loss: 1.117115169763565, Final Batch Loss: 0.46031373739242554\n",
      "Epoch 2190, Loss: 1.3876973390579224, Final Batch Loss: 0.7309497594833374\n",
      "Epoch 2191, Loss: 1.0712158381938934, Final Batch Loss: 0.350141316652298\n",
      "Epoch 2192, Loss: 1.1045950949192047, Final Batch Loss: 0.33638501167297363\n",
      "Epoch 2193, Loss: 1.0464646518230438, Final Batch Loss: 0.3242131471633911\n",
      "Epoch 2194, Loss: 1.0410573184490204, Final Batch Loss: 0.3868715465068817\n",
      "Epoch 2195, Loss: 0.9025338292121887, Final Batch Loss: 0.231317400932312\n",
      "Epoch 2196, Loss: 0.9519239068031311, Final Batch Loss: 0.3073986768722534\n",
      "Epoch 2197, Loss: 0.9472770392894745, Final Batch Loss: 0.3068176805973053\n",
      "Epoch 2198, Loss: 0.8169941753149033, Final Batch Loss: 0.18462683260440826\n",
      "Epoch 2199, Loss: 0.8297742158174515, Final Batch Loss: 0.18698002398014069\n",
      "Epoch 2200, Loss: 1.279057651758194, Final Batch Loss: 0.6757581830024719\n",
      "Epoch 2201, Loss: 0.8708286583423615, Final Batch Loss: 0.24950569868087769\n",
      "Epoch 2202, Loss: 0.8509578257799149, Final Batch Loss: 0.1715538054704666\n",
      "Epoch 2203, Loss: 0.955091118812561, Final Batch Loss: 0.2867249548435211\n",
      "Epoch 2204, Loss: 0.9747115075588226, Final Batch Loss: 0.3099328875541687\n",
      "Epoch 2205, Loss: 0.9188208281993866, Final Batch Loss: 0.3230699598789215\n",
      "Epoch 2206, Loss: 1.1072719395160675, Final Batch Loss: 0.43792763352394104\n",
      "Epoch 2207, Loss: 1.0308069586753845, Final Batch Loss: 0.4340641498565674\n",
      "Epoch 2208, Loss: 0.8115950673818588, Final Batch Loss: 0.17564798891544342\n",
      "Epoch 2209, Loss: 0.970127135515213, Final Batch Loss: 0.32032206654548645\n",
      "Epoch 2210, Loss: 0.8145349621772766, Final Batch Loss: 0.14711341261863708\n",
      "Epoch 2211, Loss: 0.8993010073900223, Final Batch Loss: 0.21451683342456818\n",
      "Epoch 2212, Loss: 0.7605081498622894, Final Batch Loss: 0.15860915184020996\n",
      "Epoch 2213, Loss: 0.8769934773445129, Final Batch Loss: 0.2506135404109955\n",
      "Epoch 2214, Loss: 0.9598083794116974, Final Batch Loss: 0.258541077375412\n",
      "Epoch 2215, Loss: 0.8238657116889954, Final Batch Loss: 0.14867129921913147\n",
      "Epoch 2216, Loss: 1.0349506735801697, Final Batch Loss: 0.4121038019657135\n",
      "Epoch 2217, Loss: 1.0095860362052917, Final Batch Loss: 0.2969447374343872\n",
      "Epoch 2218, Loss: 0.8627498149871826, Final Batch Loss: 0.17923861742019653\n",
      "Epoch 2219, Loss: 0.9826556146144867, Final Batch Loss: 0.38411274552345276\n",
      "Epoch 2220, Loss: 0.8837522119283676, Final Batch Loss: 0.23104308545589447\n",
      "Epoch 2221, Loss: 0.7841270565986633, Final Batch Loss: 0.1563197374343872\n",
      "Epoch 2222, Loss: 1.1284285187721252, Final Batch Loss: 0.5100530385971069\n",
      "Epoch 2223, Loss: 1.2004248797893524, Final Batch Loss: 0.5439965724945068\n",
      "Epoch 2224, Loss: 0.9550763070583344, Final Batch Loss: 0.3398616313934326\n",
      "Epoch 2225, Loss: 1.0162318348884583, Final Batch Loss: 0.36311590671539307\n",
      "Epoch 2226, Loss: 0.9583878666162491, Final Batch Loss: 0.35873010754585266\n",
      "Epoch 2227, Loss: 0.9279962182044983, Final Batch Loss: 0.25036829710006714\n",
      "Epoch 2228, Loss: 0.6829900741577148, Final Batch Loss: 0.09506607055664062\n",
      "Epoch 2229, Loss: 0.7857300490140915, Final Batch Loss: 0.19686464965343475\n",
      "Epoch 2230, Loss: 0.8945460319519043, Final Batch Loss: 0.2304908037185669\n",
      "Epoch 2231, Loss: 0.9220045506954193, Final Batch Loss: 0.2982776463031769\n",
      "Epoch 2232, Loss: 0.8268316239118576, Final Batch Loss: 0.23964433372020721\n",
      "Epoch 2233, Loss: 0.9091368913650513, Final Batch Loss: 0.31735217571258545\n",
      "Epoch 2234, Loss: 0.9277262091636658, Final Batch Loss: 0.257826566696167\n",
      "Epoch 2235, Loss: 0.8572603464126587, Final Batch Loss: 0.2607269585132599\n",
      "Epoch 2236, Loss: 0.9245056211948395, Final Batch Loss: 0.2851709723472595\n",
      "Epoch 2237, Loss: 1.0144389867782593, Final Batch Loss: 0.386214941740036\n",
      "Epoch 2238, Loss: 0.8869543224573135, Final Batch Loss: 0.20735199749469757\n",
      "Epoch 2239, Loss: 0.9710065126419067, Final Batch Loss: 0.36142271757125854\n",
      "Epoch 2240, Loss: 0.8119163811206818, Final Batch Loss: 0.225874125957489\n",
      "Epoch 2241, Loss: 0.8695244789123535, Final Batch Loss: 0.2545090615749359\n",
      "Epoch 2242, Loss: 0.9999150335788727, Final Batch Loss: 0.4003569185733795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2243, Loss: 0.8824934512376785, Final Batch Loss: 0.20333199203014374\n",
      "Epoch 2244, Loss: 0.7185371741652489, Final Batch Loss: 0.07885619252920151\n",
      "Epoch 2245, Loss: 1.0554279386997223, Final Batch Loss: 0.4776233732700348\n",
      "Epoch 2246, Loss: 0.8044615685939789, Final Batch Loss: 0.1719551384449005\n",
      "Epoch 2247, Loss: 0.8556760549545288, Final Batch Loss: 0.265796422958374\n",
      "Epoch 2248, Loss: 1.0426585972309113, Final Batch Loss: 0.47507792711257935\n",
      "Epoch 2249, Loss: 0.8782760500907898, Final Batch Loss: 0.2841080129146576\n",
      "Epoch 2250, Loss: 1.2228443324565887, Final Batch Loss: 0.6355757117271423\n",
      "Epoch 2251, Loss: 0.9021815061569214, Final Batch Loss: 0.2861860394477844\n",
      "Epoch 2252, Loss: 0.7787950038909912, Final Batch Loss: 0.17020517587661743\n",
      "Epoch 2253, Loss: 0.9115378856658936, Final Batch Loss: 0.2748991549015045\n",
      "Epoch 2254, Loss: 0.9260956794023514, Final Batch Loss: 0.31032925844192505\n",
      "Epoch 2255, Loss: 0.8460886478424072, Final Batch Loss: 0.17811930179595947\n",
      "Epoch 2256, Loss: 0.9399904310703278, Final Batch Loss: 0.3599797785282135\n",
      "Epoch 2257, Loss: 1.2280403971672058, Final Batch Loss: 0.5700793266296387\n",
      "Epoch 2258, Loss: 0.9765790402889252, Final Batch Loss: 0.37462908029556274\n",
      "Epoch 2259, Loss: 0.8781276494264603, Final Batch Loss: 0.2334592193365097\n",
      "Epoch 2260, Loss: 1.0824564397335052, Final Batch Loss: 0.48353180289268494\n",
      "Epoch 2261, Loss: 1.1020360589027405, Final Batch Loss: 0.5265656113624573\n",
      "Epoch 2262, Loss: 0.789808452129364, Final Batch Loss: 0.18203729391098022\n",
      "Epoch 2263, Loss: 1.0798551738262177, Final Batch Loss: 0.493055135011673\n",
      "Epoch 2264, Loss: 0.8103213012218475, Final Batch Loss: 0.2306116819381714\n",
      "Epoch 2265, Loss: 0.8868415951728821, Final Batch Loss: 0.21176302433013916\n",
      "Epoch 2266, Loss: 0.8749038875102997, Final Batch Loss: 0.28399574756622314\n",
      "Epoch 2267, Loss: 0.985633134841919, Final Batch Loss: 0.3641760051250458\n",
      "Epoch 2268, Loss: 0.772638812661171, Final Batch Loss: 0.13714776933193207\n",
      "Epoch 2269, Loss: 0.9996184706687927, Final Batch Loss: 0.40939033031463623\n",
      "Epoch 2270, Loss: 1.0706084072589874, Final Batch Loss: 0.46159884333610535\n",
      "Epoch 2271, Loss: 1.3406501114368439, Final Batch Loss: 0.7354459166526794\n",
      "Epoch 2272, Loss: 0.9226559102535248, Final Batch Loss: 0.3101142942905426\n",
      "Epoch 2273, Loss: 0.8963059186935425, Final Batch Loss: 0.2501726448535919\n",
      "Epoch 2274, Loss: 0.9781510233879089, Final Batch Loss: 0.28782182931900024\n",
      "Epoch 2275, Loss: 0.9700944423675537, Final Batch Loss: 0.3121491074562073\n",
      "Epoch 2276, Loss: 1.236115276813507, Final Batch Loss: 0.5922186374664307\n",
      "Epoch 2277, Loss: 0.816394567489624, Final Batch Loss: 0.2670750021934509\n",
      "Epoch 2278, Loss: 0.843692809343338, Final Batch Loss: 0.25918272137641907\n",
      "Epoch 2279, Loss: 0.8080504685640335, Final Batch Loss: 0.22257669270038605\n",
      "Epoch 2280, Loss: 1.0844904482364655, Final Batch Loss: 0.45967572927474976\n",
      "Epoch 2281, Loss: 1.060712993144989, Final Batch Loss: 0.4760049879550934\n",
      "Epoch 2282, Loss: 0.9390503168106079, Final Batch Loss: 0.35781294107437134\n",
      "Epoch 2283, Loss: 0.7482854276895523, Final Batch Loss: 0.1248883455991745\n",
      "Epoch 2284, Loss: 1.2811389863491058, Final Batch Loss: 0.6702715158462524\n",
      "Epoch 2285, Loss: 1.0017180442810059, Final Batch Loss: 0.3769247531890869\n",
      "Epoch 2286, Loss: 0.7409685626626015, Final Batch Loss: 0.1194513812661171\n",
      "Epoch 2287, Loss: 0.9764230847358704, Final Batch Loss: 0.2677113115787506\n",
      "Epoch 2288, Loss: 1.1564505100250244, Final Batch Loss: 0.44514575600624084\n",
      "Epoch 2289, Loss: 1.0988064110279083, Final Batch Loss: 0.42064085602760315\n",
      "Epoch 2290, Loss: 0.9472375810146332, Final Batch Loss: 0.2450650930404663\n",
      "Epoch 2291, Loss: 0.8332305550575256, Final Batch Loss: 0.20515978336334229\n",
      "Epoch 2292, Loss: 0.9477823078632355, Final Batch Loss: 0.2545090317726135\n",
      "Epoch 2293, Loss: 0.9340331256389618, Final Batch Loss: 0.3168233335018158\n",
      "Epoch 2294, Loss: 0.8907154500484467, Final Batch Loss: 0.23281636834144592\n",
      "Epoch 2295, Loss: 1.215512067079544, Final Batch Loss: 0.505556583404541\n",
      "Epoch 2296, Loss: 0.9019849747419357, Final Batch Loss: 0.23483173549175262\n",
      "Epoch 2297, Loss: 0.9845253229141235, Final Batch Loss: 0.364041268825531\n",
      "Epoch 2298, Loss: 0.9423723220825195, Final Batch Loss: 0.2805740535259247\n",
      "Epoch 2299, Loss: 0.7210348397493362, Final Batch Loss: 0.1040024608373642\n",
      "Epoch 2300, Loss: 0.8584406077861786, Final Batch Loss: 0.19799092411994934\n",
      "Epoch 2301, Loss: 0.8897573351860046, Final Batch Loss: 0.1821655035018921\n",
      "Epoch 2302, Loss: 1.0995646715164185, Final Batch Loss: 0.4939769506454468\n",
      "Epoch 2303, Loss: 1.0586541593074799, Final Batch Loss: 0.4527348577976227\n",
      "Epoch 2304, Loss: 0.9239106178283691, Final Batch Loss: 0.34192630648612976\n",
      "Epoch 2305, Loss: 0.9260613322257996, Final Batch Loss: 0.2820447087287903\n",
      "Epoch 2306, Loss: 0.955591082572937, Final Batch Loss: 0.2450825273990631\n",
      "Epoch 2307, Loss: 0.8825488984584808, Final Batch Loss: 0.2440568208694458\n",
      "Epoch 2308, Loss: 1.0271639227867126, Final Batch Loss: 0.38527464866638184\n",
      "Epoch 2309, Loss: 0.9117204546928406, Final Batch Loss: 0.2691481113433838\n",
      "Epoch 2310, Loss: 0.9853789210319519, Final Batch Loss: 0.41746172308921814\n",
      "Epoch 2311, Loss: 1.1517949402332306, Final Batch Loss: 0.5493610501289368\n",
      "Epoch 2312, Loss: 0.9125283360481262, Final Batch Loss: 0.30570706725120544\n",
      "Epoch 2313, Loss: 0.8925852179527283, Final Batch Loss: 0.2646161913871765\n",
      "Epoch 2314, Loss: 0.8257173299789429, Final Batch Loss: 0.23544055223464966\n",
      "Epoch 2315, Loss: 1.0811522006988525, Final Batch Loss: 0.5141599774360657\n",
      "Epoch 2316, Loss: 1.1400465369224548, Final Batch Loss: 0.5222782492637634\n",
      "Epoch 2317, Loss: 0.9722371995449066, Final Batch Loss: 0.3394556939601898\n",
      "Epoch 2318, Loss: 0.857273742556572, Final Batch Loss: 0.22697429358959198\n",
      "Epoch 2319, Loss: 0.8529202342033386, Final Batch Loss: 0.2571943998336792\n",
      "Epoch 2320, Loss: 0.868694007396698, Final Batch Loss: 0.31950369477272034\n",
      "Epoch 2321, Loss: 1.0198011696338654, Final Batch Loss: 0.3784817159175873\n",
      "Epoch 2322, Loss: 0.795330598950386, Final Batch Loss: 0.19385991990566254\n",
      "Epoch 2323, Loss: 0.8959556221961975, Final Batch Loss: 0.25108370184898376\n",
      "Epoch 2324, Loss: 0.9179529845714569, Final Batch Loss: 0.30066385865211487\n",
      "Epoch 2325, Loss: 0.9749819338321686, Final Batch Loss: 0.37697121500968933\n",
      "Epoch 2326, Loss: 0.880316287279129, Final Batch Loss: 0.25276848673820496\n",
      "Epoch 2327, Loss: 0.7491879537701607, Final Batch Loss: 0.0935807004570961\n",
      "Epoch 2328, Loss: 0.7108137682080269, Final Batch Loss: 0.1083960011601448\n",
      "Epoch 2329, Loss: 0.8300690203905106, Final Batch Loss: 0.24146299064159393\n",
      "Epoch 2330, Loss: 0.6425185091793537, Final Batch Loss: 0.06197552755475044\n",
      "Epoch 2331, Loss: 0.8794564008712769, Final Batch Loss: 0.26223623752593994\n",
      "Epoch 2332, Loss: 0.9166508316993713, Final Batch Loss: 0.31324025988578796\n",
      "Epoch 2333, Loss: 0.877968817949295, Final Batch Loss: 0.2834313213825226\n",
      "Epoch 2334, Loss: 0.7348490878939629, Final Batch Loss: 0.11669566482305527\n",
      "Epoch 2335, Loss: 1.1199321299791336, Final Batch Loss: 0.5448518991470337\n",
      "Epoch 2336, Loss: 0.9078536033630371, Final Batch Loss: 0.30283641815185547\n",
      "Epoch 2337, Loss: 0.980114072561264, Final Batch Loss: 0.35512974858283997\n",
      "Epoch 2338, Loss: 0.9994934797286987, Final Batch Loss: 0.337251752614975\n",
      "Epoch 2339, Loss: 1.1242187321186066, Final Batch Loss: 0.49898824095726013\n",
      "Epoch 2340, Loss: 0.8620359897613525, Final Batch Loss: 0.2675785422325134\n",
      "Epoch 2341, Loss: 0.8545208275318146, Final Batch Loss: 0.25399044156074524\n",
      "Epoch 2342, Loss: 0.9795683026313782, Final Batch Loss: 0.3462289273738861\n",
      "Epoch 2343, Loss: 0.9205208420753479, Final Batch Loss: 0.33506640791893005\n",
      "Epoch 2344, Loss: 1.0391745865345001, Final Batch Loss: 0.40586909651756287\n",
      "Epoch 2345, Loss: 0.9105145335197449, Final Batch Loss: 0.2919847369194031\n",
      "Epoch 2346, Loss: 0.8960200846195221, Final Batch Loss: 0.3288514316082001\n",
      "Epoch 2347, Loss: 0.8611888587474823, Final Batch Loss: 0.28142789006233215\n",
      "Epoch 2348, Loss: 0.8962804675102234, Final Batch Loss: 0.3007718324661255\n",
      "Epoch 2349, Loss: 0.8348197340965271, Final Batch Loss: 0.20451176166534424\n",
      "Epoch 2350, Loss: 1.041243702173233, Final Batch Loss: 0.4630208909511566\n",
      "Epoch 2351, Loss: 1.2736343145370483, Final Batch Loss: 0.6523903012275696\n",
      "Epoch 2352, Loss: 0.8264667242765427, Final Batch Loss: 0.15211735665798187\n",
      "Epoch 2353, Loss: 0.8177797943353653, Final Batch Loss: 0.2217186838388443\n",
      "Epoch 2354, Loss: 0.756371408700943, Final Batch Loss: 0.18064336478710175\n",
      "Epoch 2355, Loss: 0.9523024260997772, Final Batch Loss: 0.3941464424133301\n",
      "Epoch 2356, Loss: 0.9563677310943604, Final Batch Loss: 0.3477836549282074\n",
      "Epoch 2357, Loss: 1.0199486017227173, Final Batch Loss: 0.4287097752094269\n",
      "Epoch 2358, Loss: 1.0849554240703583, Final Batch Loss: 0.5076043009757996\n",
      "Epoch 2359, Loss: 0.8397201299667358, Final Batch Loss: 0.1822405457496643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2360, Loss: 1.1010383665561676, Final Batch Loss: 0.4809664785861969\n",
      "Epoch 2361, Loss: 0.7933000177145004, Final Batch Loss: 0.14285831153392792\n",
      "Epoch 2362, Loss: 0.8179675936698914, Final Batch Loss: 0.21190056204795837\n",
      "Epoch 2363, Loss: 1.0377165973186493, Final Batch Loss: 0.3377109169960022\n",
      "Epoch 2364, Loss: 0.8057158142328262, Final Batch Loss: 0.1809232085943222\n",
      "Epoch 2365, Loss: 1.0201101303100586, Final Batch Loss: 0.3408176302909851\n",
      "Epoch 2366, Loss: 0.9748915731906891, Final Batch Loss: 0.3424778878688812\n",
      "Epoch 2367, Loss: 1.0796456038951874, Final Batch Loss: 0.5236622095108032\n",
      "Epoch 2368, Loss: 0.7053753510117531, Final Batch Loss: 0.10026810318231583\n",
      "Epoch 2369, Loss: 0.9230621457099915, Final Batch Loss: 0.27927398681640625\n",
      "Epoch 2370, Loss: 1.013007491827011, Final Batch Loss: 0.4656338393688202\n",
      "Epoch 2371, Loss: 0.8486962467432022, Final Batch Loss: 0.18248872458934784\n",
      "Epoch 2372, Loss: 0.890102744102478, Final Batch Loss: 0.3268655836582184\n",
      "Epoch 2373, Loss: 0.7870567888021469, Final Batch Loss: 0.19994671642780304\n",
      "Epoch 2374, Loss: 1.1604986488819122, Final Batch Loss: 0.5192061066627502\n",
      "Epoch 2375, Loss: 0.9731201827526093, Final Batch Loss: 0.33986327052116394\n",
      "Epoch 2376, Loss: 1.1028560400009155, Final Batch Loss: 0.5284861922264099\n",
      "Epoch 2377, Loss: 0.7271477058529854, Final Batch Loss: 0.10465738922357559\n",
      "Epoch 2378, Loss: 0.7010574340820312, Final Batch Loss: 0.13620111346244812\n",
      "Epoch 2379, Loss: 1.0567214488983154, Final Batch Loss: 0.4445807933807373\n",
      "Epoch 2380, Loss: 0.8059912025928497, Final Batch Loss: 0.23428502678871155\n",
      "Epoch 2381, Loss: 0.8756866157054901, Final Batch Loss: 0.27144479751586914\n",
      "Epoch 2382, Loss: 1.135213851928711, Final Batch Loss: 0.5201262831687927\n",
      "Epoch 2383, Loss: 0.8493912518024445, Final Batch Loss: 0.2697181701660156\n",
      "Epoch 2384, Loss: 0.8014081865549088, Final Batch Loss: 0.20592756569385529\n",
      "Epoch 2385, Loss: 0.7984661906957626, Final Batch Loss: 0.1767602413892746\n",
      "Epoch 2386, Loss: 0.8446689546108246, Final Batch Loss: 0.2517922520637512\n",
      "Epoch 2387, Loss: 1.0346072912216187, Final Batch Loss: 0.42485612630844116\n",
      "Epoch 2388, Loss: 0.8139211088418961, Final Batch Loss: 0.201189324259758\n",
      "Epoch 2389, Loss: 0.8351987451314926, Final Batch Loss: 0.24702922999858856\n",
      "Epoch 2390, Loss: 0.9428536593914032, Final Batch Loss: 0.298587441444397\n",
      "Epoch 2391, Loss: 0.7902671843767166, Final Batch Loss: 0.17770324647426605\n",
      "Epoch 2392, Loss: 0.9874396622180939, Final Batch Loss: 0.3880601227283478\n",
      "Epoch 2393, Loss: 1.0624413788318634, Final Batch Loss: 0.4862266480922699\n",
      "Epoch 2394, Loss: 1.0612220466136932, Final Batch Loss: 0.456668496131897\n",
      "Epoch 2395, Loss: 0.8034297227859497, Final Batch Loss: 0.22485487163066864\n",
      "Epoch 2396, Loss: 0.897250160574913, Final Batch Loss: 0.2108743041753769\n",
      "Epoch 2397, Loss: 1.0026375949382782, Final Batch Loss: 0.40706923604011536\n",
      "Epoch 2398, Loss: 0.7130001038312912, Final Batch Loss: 0.09134270250797272\n",
      "Epoch 2399, Loss: 0.8998009562492371, Final Batch Loss: 0.26342588663101196\n",
      "Epoch 2400, Loss: 0.7125833034515381, Final Batch Loss: 0.08198779821395874\n",
      "Epoch 2401, Loss: 0.9213381111621857, Final Batch Loss: 0.28901055455207825\n",
      "Epoch 2402, Loss: 0.8577811121940613, Final Batch Loss: 0.2814635932445526\n",
      "Epoch 2403, Loss: 0.9254309833049774, Final Batch Loss: 0.29159820079803467\n",
      "Epoch 2404, Loss: 0.9581810832023621, Final Batch Loss: 0.35212260484695435\n",
      "Epoch 2405, Loss: 0.7123098075389862, Final Batch Loss: 0.15573060512542725\n",
      "Epoch 2406, Loss: 0.8723515868186951, Final Batch Loss: 0.3090924024581909\n",
      "Epoch 2407, Loss: 0.9414070248603821, Final Batch Loss: 0.3390166759490967\n",
      "Epoch 2408, Loss: 0.9276778101921082, Final Batch Loss: 0.3286961317062378\n",
      "Epoch 2409, Loss: 0.9034210741519928, Final Batch Loss: 0.34245118498802185\n",
      "Epoch 2410, Loss: 0.8545534610748291, Final Batch Loss: 0.26252278685569763\n",
      "Epoch 2411, Loss: 0.7566156685352325, Final Batch Loss: 0.1516655683517456\n",
      "Epoch 2412, Loss: 0.7835545390844345, Final Batch Loss: 0.17682890594005585\n",
      "Epoch 2413, Loss: 0.9618133902549744, Final Batch Loss: 0.28997334837913513\n",
      "Epoch 2414, Loss: 0.7238020151853561, Final Batch Loss: 0.18633630871772766\n",
      "Epoch 2415, Loss: 0.9133705198764801, Final Batch Loss: 0.27520620822906494\n",
      "Epoch 2416, Loss: 0.7429975867271423, Final Batch Loss: 0.15935829281806946\n",
      "Epoch 2417, Loss: 0.8728788793087006, Final Batch Loss: 0.2951565086841583\n",
      "Epoch 2418, Loss: 0.637247309088707, Final Batch Loss: 0.06962917745113373\n",
      "Epoch 2419, Loss: 0.844557523727417, Final Batch Loss: 0.21524298191070557\n",
      "Epoch 2420, Loss: 0.7306627482175827, Final Batch Loss: 0.16185206174850464\n",
      "Epoch 2421, Loss: 0.8121758103370667, Final Batch Loss: 0.20289301872253418\n",
      "Epoch 2422, Loss: 1.0938446521759033, Final Batch Loss: 0.45791301131248474\n",
      "Epoch 2423, Loss: 0.7014254108071327, Final Batch Loss: 0.11054528504610062\n",
      "Epoch 2424, Loss: 0.8727312833070755, Final Batch Loss: 0.2448262721300125\n",
      "Epoch 2425, Loss: 0.7331801652908325, Final Batch Loss: 0.1844690442085266\n",
      "Epoch 2426, Loss: 0.8886033892631531, Final Batch Loss: 0.274706095457077\n",
      "Epoch 2427, Loss: 1.215172290802002, Final Batch Loss: 0.5764158368110657\n",
      "Epoch 2428, Loss: 0.7862844467163086, Final Batch Loss: 0.22557923197746277\n",
      "Epoch 2429, Loss: 0.6938332095742226, Final Batch Loss: 0.08493340760469437\n",
      "Epoch 2430, Loss: 0.691385954618454, Final Batch Loss: 0.14013847708702087\n",
      "Epoch 2431, Loss: 0.8265252709388733, Final Batch Loss: 0.21847441792488098\n",
      "Epoch 2432, Loss: 0.959283709526062, Final Batch Loss: 0.370121568441391\n",
      "Epoch 2433, Loss: 0.7138496041297913, Final Batch Loss: 0.07884201407432556\n",
      "Epoch 2434, Loss: 0.7839861959218979, Final Batch Loss: 0.1380414217710495\n",
      "Epoch 2435, Loss: 0.9304235577583313, Final Batch Loss: 0.36805081367492676\n",
      "Epoch 2436, Loss: 0.6965866684913635, Final Batch Loss: 0.10922718048095703\n",
      "Epoch 2437, Loss: 1.146419107913971, Final Batch Loss: 0.4927047789096832\n",
      "Epoch 2438, Loss: 1.0109995603561401, Final Batch Loss: 0.37334832549095154\n",
      "Epoch 2439, Loss: 0.9726341366767883, Final Batch Loss: 0.32256537675857544\n",
      "Epoch 2440, Loss: 0.9741600751876831, Final Batch Loss: 0.36635932326316833\n",
      "Epoch 2441, Loss: 0.9629745483398438, Final Batch Loss: 0.34472399950027466\n",
      "Epoch 2442, Loss: 0.6666861549019814, Final Batch Loss: 0.055752284824848175\n",
      "Epoch 2443, Loss: 1.0179704427719116, Final Batch Loss: 0.3420673608779907\n",
      "Epoch 2444, Loss: 0.8662323653697968, Final Batch Loss: 0.2640852928161621\n",
      "Epoch 2445, Loss: 0.9762628674507141, Final Batch Loss: 0.3572031855583191\n",
      "Epoch 2446, Loss: 0.9028613567352295, Final Batch Loss: 0.283130407333374\n",
      "Epoch 2447, Loss: 0.945573091506958, Final Batch Loss: 0.33801034092903137\n",
      "Epoch 2448, Loss: 0.8582093119621277, Final Batch Loss: 0.28660622239112854\n",
      "Epoch 2449, Loss: 0.7972007691860199, Final Batch Loss: 0.22110334038734436\n",
      "Epoch 2450, Loss: 0.9901767671108246, Final Batch Loss: 0.39286279678344727\n",
      "Epoch 2451, Loss: 0.8711878508329391, Final Batch Loss: 0.30802273750305176\n",
      "Epoch 2452, Loss: 1.0986292958259583, Final Batch Loss: 0.5277549624443054\n",
      "Epoch 2453, Loss: 0.9943224191665649, Final Batch Loss: 0.4067663252353668\n",
      "Epoch 2454, Loss: 0.8763995170593262, Final Batch Loss: 0.2613537013530731\n",
      "Epoch 2455, Loss: 0.7251449078321457, Final Batch Loss: 0.16844217479228973\n",
      "Epoch 2456, Loss: 0.8743855953216553, Final Batch Loss: 0.3162411153316498\n",
      "Epoch 2457, Loss: 0.7952063977718353, Final Batch Loss: 0.18478679656982422\n",
      "Epoch 2458, Loss: 1.008976399898529, Final Batch Loss: 0.39643433690071106\n",
      "Epoch 2459, Loss: 0.916837066411972, Final Batch Loss: 0.31985166668891907\n",
      "Epoch 2460, Loss: 0.8497900366783142, Final Batch Loss: 0.2587794363498688\n",
      "Epoch 2461, Loss: 0.9056546092033386, Final Batch Loss: 0.318630576133728\n",
      "Epoch 2462, Loss: 0.9935057461261749, Final Batch Loss: 0.3132859170436859\n",
      "Epoch 2463, Loss: 0.889935165643692, Final Batch Loss: 0.304135262966156\n",
      "Epoch 2464, Loss: 0.813266396522522, Final Batch Loss: 0.2268339991569519\n",
      "Epoch 2465, Loss: 0.9105101823806763, Final Batch Loss: 0.2958123981952667\n",
      "Epoch 2466, Loss: 0.856056272983551, Final Batch Loss: 0.26146966218948364\n",
      "Epoch 2467, Loss: 0.8760564923286438, Final Batch Loss: 0.2734549641609192\n",
      "Epoch 2468, Loss: 0.8082716166973114, Final Batch Loss: 0.18612569570541382\n",
      "Epoch 2469, Loss: 1.0996739268302917, Final Batch Loss: 0.5244932174682617\n",
      "Epoch 2470, Loss: 1.0133167505264282, Final Batch Loss: 0.392607182264328\n",
      "Epoch 2471, Loss: 0.9223799705505371, Final Batch Loss: 0.2719031274318695\n",
      "Epoch 2472, Loss: 0.864488035440445, Final Batch Loss: 0.24486219882965088\n",
      "Epoch 2473, Loss: 0.7837477698922157, Final Batch Loss: 0.10946998745203018\n",
      "Epoch 2474, Loss: 0.8797335624694824, Final Batch Loss: 0.2714575529098511\n",
      "Epoch 2475, Loss: 0.9827459156513214, Final Batch Loss: 0.35762348771095276\n",
      "Epoch 2476, Loss: 0.9337393939495087, Final Batch Loss: 0.3468113839626312\n",
      "Epoch 2477, Loss: 0.9939009845256805, Final Batch Loss: 0.4631986618041992\n",
      "Epoch 2478, Loss: 0.8015313148498535, Final Batch Loss: 0.19672003388404846\n",
      "Epoch 2479, Loss: 1.1156948506832123, Final Batch Loss: 0.4847797155380249\n",
      "Epoch 2480, Loss: 0.7861435413360596, Final Batch Loss: 0.2258758544921875\n",
      "Epoch 2481, Loss: 0.8682321757078171, Final Batch Loss: 0.19994144141674042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2482, Loss: 0.7287871763110161, Final Batch Loss: 0.10394265502691269\n",
      "Epoch 2483, Loss: 0.7548899576067924, Final Batch Loss: 0.12145914882421494\n",
      "Epoch 2484, Loss: 0.6576360166072845, Final Batch Loss: 0.06529697775840759\n",
      "Epoch 2485, Loss: 0.9332756102085114, Final Batch Loss: 0.2721741497516632\n",
      "Epoch 2486, Loss: 1.065201848745346, Final Batch Loss: 0.4572789967060089\n",
      "Epoch 2487, Loss: 1.0808401107788086, Final Batch Loss: 0.5223912596702576\n",
      "Epoch 2488, Loss: 0.8447131365537643, Final Batch Loss: 0.19633762538433075\n",
      "Epoch 2489, Loss: 1.289351299405098, Final Batch Loss: 0.6371640563011169\n",
      "Epoch 2490, Loss: 0.8591054081916809, Final Batch Loss: 0.2510230243206024\n",
      "Epoch 2491, Loss: 1.101508766412735, Final Batch Loss: 0.46394509077072144\n",
      "Epoch 2492, Loss: 0.9817136526107788, Final Batch Loss: 0.3067033588886261\n",
      "Epoch 2493, Loss: 0.9801949858665466, Final Batch Loss: 0.34230878949165344\n",
      "Epoch 2494, Loss: 0.8991856575012207, Final Batch Loss: 0.28951096534729004\n",
      "Epoch 2495, Loss: 0.8523276001214981, Final Batch Loss: 0.22119130194187164\n",
      "Epoch 2496, Loss: 0.9445319175720215, Final Batch Loss: 0.3458252251148224\n",
      "Epoch 2497, Loss: 0.7605925351381302, Final Batch Loss: 0.16586928069591522\n",
      "Epoch 2498, Loss: 0.9631982743740082, Final Batch Loss: 0.34833693504333496\n",
      "Epoch 2499, Loss: 1.1037323772907257, Final Batch Loss: 0.5066908597946167\n",
      "Epoch 2500, Loss: 0.8608845472335815, Final Batch Loss: 0.2649712860584259\n",
      "Epoch 2501, Loss: 0.8166614770889282, Final Batch Loss: 0.22695061564445496\n",
      "Epoch 2502, Loss: 0.8738133758306503, Final Batch Loss: 0.3012003004550934\n",
      "Epoch 2503, Loss: 0.8765464127063751, Final Batch Loss: 0.2876700460910797\n",
      "Epoch 2504, Loss: 1.1321921944618225, Final Batch Loss: 0.5017139911651611\n",
      "Epoch 2505, Loss: 0.8262377232313156, Final Batch Loss: 0.19297046959400177\n",
      "Epoch 2506, Loss: 0.9261756241321564, Final Batch Loss: 0.3152408301830292\n",
      "Epoch 2507, Loss: 0.911257416009903, Final Batch Loss: 0.33423733711242676\n",
      "Epoch 2508, Loss: 0.9792226850986481, Final Batch Loss: 0.42739832401275635\n",
      "Epoch 2509, Loss: 0.9513230323791504, Final Batch Loss: 0.32920095324516296\n",
      "Epoch 2510, Loss: 0.9594212472438812, Final Batch Loss: 0.37762725353240967\n",
      "Epoch 2511, Loss: 0.6365437805652618, Final Batch Loss: 0.025947093963623047\n",
      "Epoch 2512, Loss: 0.9431631565093994, Final Batch Loss: 0.40320369601249695\n",
      "Epoch 2513, Loss: 0.9557631611824036, Final Batch Loss: 0.4050309658050537\n",
      "Epoch 2514, Loss: 0.7512658536434174, Final Batch Loss: 0.1510550081729889\n",
      "Epoch 2515, Loss: 0.8616054952144623, Final Batch Loss: 0.2695777416229248\n",
      "Epoch 2516, Loss: 0.9910183846950531, Final Batch Loss: 0.3421851098537445\n",
      "Epoch 2517, Loss: 0.8948837518692017, Final Batch Loss: 0.2970218360424042\n",
      "Epoch 2518, Loss: 0.8846801817417145, Final Batch Loss: 0.28714439272880554\n",
      "Epoch 2519, Loss: 0.9855810701847076, Final Batch Loss: 0.38909029960632324\n",
      "Epoch 2520, Loss: 0.8427111655473709, Final Batch Loss: 0.21875710785388947\n",
      "Epoch 2521, Loss: 0.7622194290161133, Final Batch Loss: 0.18488729000091553\n",
      "Epoch 2522, Loss: 0.9518628716468811, Final Batch Loss: 0.30086594820022583\n",
      "Epoch 2523, Loss: 1.2679668962955475, Final Batch Loss: 0.7053371667861938\n",
      "Epoch 2524, Loss: 0.9485285580158234, Final Batch Loss: 0.36919716000556946\n",
      "Epoch 2525, Loss: 0.8611021488904953, Final Batch Loss: 0.22883076965808868\n",
      "Epoch 2526, Loss: 0.7636321038007736, Final Batch Loss: 0.0847240537405014\n",
      "Epoch 2527, Loss: 0.7343232482671738, Final Batch Loss: 0.1851530522108078\n",
      "Epoch 2528, Loss: 0.9585140943527222, Final Batch Loss: 0.340778648853302\n",
      "Epoch 2529, Loss: 0.8867630362510681, Final Batch Loss: 0.33333727717399597\n",
      "Epoch 2530, Loss: 0.6910145208239555, Final Batch Loss: 0.12423526495695114\n",
      "Epoch 2531, Loss: 0.6867701932787895, Final Batch Loss: 0.12282692641019821\n",
      "Epoch 2532, Loss: 0.8644890785217285, Final Batch Loss: 0.2881377339363098\n",
      "Epoch 2533, Loss: 0.701155811548233, Final Batch Loss: 0.12200182676315308\n",
      "Epoch 2534, Loss: 0.8219419568777084, Final Batch Loss: 0.2353716939687729\n",
      "Epoch 2535, Loss: 0.875710129737854, Final Batch Loss: 0.26708149909973145\n",
      "Epoch 2536, Loss: 0.8689106404781342, Final Batch Loss: 0.2686057984828949\n",
      "Epoch 2537, Loss: 0.7660608738660812, Final Batch Loss: 0.21513722836971283\n",
      "Epoch 2538, Loss: 0.7306161597371101, Final Batch Loss: 0.12075985223054886\n",
      "Epoch 2539, Loss: 0.7829593271017075, Final Batch Loss: 0.1716606169939041\n",
      "Epoch 2540, Loss: 0.9432315826416016, Final Batch Loss: 0.32176560163497925\n",
      "Epoch 2541, Loss: 0.8385555148124695, Final Batch Loss: 0.2666590213775635\n",
      "Epoch 2542, Loss: 0.8537364304065704, Final Batch Loss: 0.28831472992897034\n",
      "Epoch 2543, Loss: 0.7089737355709076, Final Batch Loss: 0.1587284654378891\n",
      "Epoch 2544, Loss: 0.7786772102117538, Final Batch Loss: 0.24622872471809387\n",
      "Epoch 2545, Loss: 0.6449983194470406, Final Batch Loss: 0.10709542781114578\n",
      "Epoch 2546, Loss: 0.9215975403785706, Final Batch Loss: 0.2444193959236145\n",
      "Epoch 2547, Loss: 0.732597678899765, Final Batch Loss: 0.15406924486160278\n",
      "Epoch 2548, Loss: 0.904086172580719, Final Batch Loss: 0.39054057002067566\n",
      "Epoch 2549, Loss: 0.7540989816188812, Final Batch Loss: 0.23028090596199036\n",
      "Epoch 2550, Loss: 0.7895502895116806, Final Batch Loss: 0.1760723739862442\n",
      "Epoch 2551, Loss: 0.9435760974884033, Final Batch Loss: 0.3265005648136139\n",
      "Epoch 2552, Loss: 1.1551107168197632, Final Batch Loss: 0.5840308666229248\n",
      "Epoch 2553, Loss: 0.9320792555809021, Final Batch Loss: 0.3744712769985199\n",
      "Epoch 2554, Loss: 1.026980847120285, Final Batch Loss: 0.38656488060951233\n",
      "Epoch 2555, Loss: 1.0290194302797318, Final Batch Loss: 0.4659043252468109\n",
      "Epoch 2556, Loss: 0.9549650549888611, Final Batch Loss: 0.3920396864414215\n",
      "Epoch 2557, Loss: 1.011429101228714, Final Batch Loss: 0.4615744948387146\n",
      "Epoch 2558, Loss: 0.8528297245502472, Final Batch Loss: 0.26248714327812195\n",
      "Epoch 2559, Loss: 0.9527873694896698, Final Batch Loss: 0.37169620394706726\n",
      "Epoch 2560, Loss: 0.8173279166221619, Final Batch Loss: 0.1802390217781067\n",
      "Epoch 2561, Loss: 0.84730364382267, Final Batch Loss: 0.21775703132152557\n",
      "Epoch 2562, Loss: 0.6790980398654938, Final Batch Loss: 0.10827487707138062\n",
      "Epoch 2563, Loss: 1.1736379712820053, Final Batch Loss: 0.6035228371620178\n",
      "Epoch 2564, Loss: 0.9647556245326996, Final Batch Loss: 0.39866775274276733\n",
      "Epoch 2565, Loss: 1.0212460458278656, Final Batch Loss: 0.4430732727050781\n",
      "Epoch 2566, Loss: 0.7643014937639236, Final Batch Loss: 0.13030953705310822\n",
      "Epoch 2567, Loss: 0.8483564257621765, Final Batch Loss: 0.2561899721622467\n",
      "Epoch 2568, Loss: 0.8709159940481186, Final Batch Loss: 0.24533869326114655\n",
      "Epoch 2569, Loss: 0.7427003979682922, Final Batch Loss: 0.18881812691688538\n",
      "Epoch 2570, Loss: 0.7867879718542099, Final Batch Loss: 0.20165659487247467\n",
      "Epoch 2571, Loss: 0.7893277704715729, Final Batch Loss: 0.2267732322216034\n",
      "Epoch 2572, Loss: 1.0522600263357162, Final Batch Loss: 0.5031026601791382\n",
      "Epoch 2573, Loss: 0.8585200607776642, Final Batch Loss: 0.26507604122161865\n",
      "Epoch 2574, Loss: 0.7174871563911438, Final Batch Loss: 0.15090709924697876\n",
      "Epoch 2575, Loss: 0.8504552245140076, Final Batch Loss: 0.29546868801116943\n",
      "Epoch 2576, Loss: 0.8405850231647491, Final Batch Loss: 0.24428147077560425\n",
      "Epoch 2577, Loss: 0.671321377158165, Final Batch Loss: 0.15417397022247314\n",
      "Epoch 2578, Loss: 0.8797256648540497, Final Batch Loss: 0.29227837920188904\n",
      "Epoch 2579, Loss: 0.8107869327068329, Final Batch Loss: 0.23893293738365173\n",
      "Epoch 2580, Loss: 0.7858996093273163, Final Batch Loss: 0.20188969373703003\n",
      "Epoch 2581, Loss: 0.8427857756614685, Final Batch Loss: 0.2383103370666504\n",
      "Epoch 2582, Loss: 0.6841600760817528, Final Batch Loss: 0.11083327978849411\n",
      "Epoch 2583, Loss: 0.8721807599067688, Final Batch Loss: 0.29317811131477356\n",
      "Epoch 2584, Loss: 0.9445943534374237, Final Batch Loss: 0.3928793966770172\n",
      "Epoch 2585, Loss: 0.9768224358558655, Final Batch Loss: 0.34836503863334656\n",
      "Epoch 2586, Loss: 0.9436619877815247, Final Batch Loss: 0.3267108201980591\n",
      "Epoch 2587, Loss: 0.880763977766037, Final Batch Loss: 0.21050229668617249\n",
      "Epoch 2588, Loss: 0.9473627209663391, Final Batch Loss: 0.3622942864894867\n",
      "Epoch 2589, Loss: 0.7708852738142014, Final Batch Loss: 0.17666570842266083\n",
      "Epoch 2590, Loss: 0.8195017874240875, Final Batch Loss: 0.2055947184562683\n",
      "Epoch 2591, Loss: 0.8719452917575836, Final Batch Loss: 0.24020081758499146\n",
      "Epoch 2592, Loss: 1.0701401233673096, Final Batch Loss: 0.45041215419769287\n",
      "Epoch 2593, Loss: 0.6745984293520451, Final Batch Loss: 0.01576128974556923\n",
      "Epoch 2594, Loss: 0.856489896774292, Final Batch Loss: 0.28880882263183594\n",
      "Epoch 2595, Loss: 0.86830934882164, Final Batch Loss: 0.276714563369751\n",
      "Epoch 2596, Loss: 0.7772392928600311, Final Batch Loss: 0.17110270261764526\n",
      "Epoch 2597, Loss: 0.7299935519695282, Final Batch Loss: 0.13924962282180786\n",
      "Epoch 2598, Loss: 0.7965736091136932, Final Batch Loss: 0.2510462701320648\n",
      "Epoch 2599, Loss: 1.0483518540859222, Final Batch Loss: 0.41324496269226074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2600, Loss: 0.8517942726612091, Final Batch Loss: 0.1962651014328003\n",
      "Epoch 2601, Loss: 0.8611461073160172, Final Batch Loss: 0.2293388694524765\n",
      "Epoch 2602, Loss: 0.953361451625824, Final Batch Loss: 0.3202393651008606\n",
      "Epoch 2603, Loss: 0.8008095175027847, Final Batch Loss: 0.22267703711986542\n",
      "Epoch 2604, Loss: 0.8831939697265625, Final Batch Loss: 0.30656394362449646\n",
      "Epoch 2605, Loss: 0.9122813045978546, Final Batch Loss: 0.32236000895500183\n",
      "Epoch 2606, Loss: 0.8485393077135086, Final Batch Loss: 0.23573489487171173\n",
      "Epoch 2607, Loss: 0.8496896475553513, Final Batch Loss: 0.239632710814476\n",
      "Epoch 2608, Loss: 1.0434464663267136, Final Batch Loss: 0.4523519277572632\n",
      "Epoch 2609, Loss: 0.9079917967319489, Final Batch Loss: 0.3423028886318207\n",
      "Epoch 2610, Loss: 0.9311398863792419, Final Batch Loss: 0.3240046203136444\n",
      "Epoch 2611, Loss: 0.840794712305069, Final Batch Loss: 0.27000927925109863\n",
      "Epoch 2612, Loss: 0.9736138582229614, Final Batch Loss: 0.44666510820388794\n",
      "Epoch 2613, Loss: 0.8835629820823669, Final Batch Loss: 0.3255535066127777\n",
      "Epoch 2614, Loss: 0.8004533499479294, Final Batch Loss: 0.23089750111103058\n",
      "Epoch 2615, Loss: 0.9257282614707947, Final Batch Loss: 0.34695982933044434\n",
      "Epoch 2616, Loss: 1.0911635160446167, Final Batch Loss: 0.47229868173599243\n",
      "Epoch 2617, Loss: 0.798215702176094, Final Batch Loss: 0.16016630828380585\n",
      "Epoch 2618, Loss: 0.7471656203269958, Final Batch Loss: 0.136989563703537\n",
      "Epoch 2619, Loss: 0.9465552568435669, Final Batch Loss: 0.3891778886318207\n",
      "Epoch 2620, Loss: 0.9201538562774658, Final Batch Loss: 0.38517656922340393\n",
      "Epoch 2621, Loss: 0.8119855970144272, Final Batch Loss: 0.19451485574245453\n",
      "Epoch 2622, Loss: 1.204285740852356, Final Batch Loss: 0.6234593987464905\n",
      "Epoch 2623, Loss: 0.9485123306512833, Final Batch Loss: 0.38890737295150757\n",
      "Epoch 2624, Loss: 0.8514505624771118, Final Batch Loss: 0.2618032395839691\n",
      "Epoch 2625, Loss: 0.5867365505546331, Final Batch Loss: 0.023315628990530968\n",
      "Epoch 2626, Loss: 0.8352920114994049, Final Batch Loss: 0.26464083790779114\n",
      "Epoch 2627, Loss: 1.1514672935009003, Final Batch Loss: 0.6086335778236389\n",
      "Epoch 2628, Loss: 1.0194737017154694, Final Batch Loss: 0.43066033720970154\n",
      "Epoch 2629, Loss: 1.2394968569278717, Final Batch Loss: 0.6035193800926208\n",
      "Epoch 2630, Loss: 0.8781193196773529, Final Batch Loss: 0.32534536719322205\n",
      "Epoch 2631, Loss: 0.8352538496255875, Final Batch Loss: 0.22473569214344025\n",
      "Epoch 2632, Loss: 0.8409681767225266, Final Batch Loss: 0.24704651534557343\n",
      "Epoch 2633, Loss: 0.8597237169742584, Final Batch Loss: 0.2331010401248932\n",
      "Epoch 2634, Loss: 0.9872546494007111, Final Batch Loss: 0.38138094544410706\n",
      "Epoch 2635, Loss: 0.7714421302080154, Final Batch Loss: 0.14233176410198212\n",
      "Epoch 2636, Loss: 0.9436016380786896, Final Batch Loss: 0.39934495091438293\n",
      "Epoch 2637, Loss: 0.9103690683841705, Final Batch Loss: 0.21951010823249817\n",
      "Epoch 2638, Loss: 0.7991849631071091, Final Batch Loss: 0.15018193423748016\n",
      "Epoch 2639, Loss: 0.7347440123558044, Final Batch Loss: 0.1876884400844574\n",
      "Epoch 2640, Loss: 1.0471699982881546, Final Batch Loss: 0.4866362512111664\n",
      "Epoch 2641, Loss: 0.8070072829723358, Final Batch Loss: 0.224979430437088\n",
      "Epoch 2642, Loss: 0.8204053640365601, Final Batch Loss: 0.25617873668670654\n",
      "Epoch 2643, Loss: 0.7217243909835815, Final Batch Loss: 0.1748623549938202\n",
      "Epoch 2644, Loss: 0.9690997302532196, Final Batch Loss: 0.38481196761131287\n",
      "Epoch 2645, Loss: 1.0222092270851135, Final Batch Loss: 0.41381749510765076\n",
      "Epoch 2646, Loss: 0.8210123628377914, Final Batch Loss: 0.19316606223583221\n",
      "Epoch 2647, Loss: 1.4964242279529572, Final Batch Loss: 0.893783688545227\n",
      "Epoch 2648, Loss: 0.8391451835632324, Final Batch Loss: 0.29836711287498474\n",
      "Epoch 2649, Loss: 0.6599237397313118, Final Batch Loss: 0.12041137367486954\n",
      "Epoch 2650, Loss: 0.7762496471405029, Final Batch Loss: 0.1706368625164032\n",
      "Epoch 2651, Loss: 1.032520592212677, Final Batch Loss: 0.42462649941444397\n",
      "Epoch 2652, Loss: 1.0445259809494019, Final Batch Loss: 0.37511202692985535\n",
      "Epoch 2653, Loss: 0.9536695331335068, Final Batch Loss: 0.38050445914268494\n",
      "Epoch 2654, Loss: 0.9997479319572449, Final Batch Loss: 0.38799935579299927\n",
      "Epoch 2655, Loss: 1.0304858684539795, Final Batch Loss: 0.38655081391334534\n",
      "Epoch 2656, Loss: 0.7786742895841599, Final Batch Loss: 0.17120499908924103\n",
      "Epoch 2657, Loss: 1.0101736932992935, Final Batch Loss: 0.3638252913951874\n",
      "Epoch 2658, Loss: 0.8780915141105652, Final Batch Loss: 0.2789159119129181\n",
      "Epoch 2659, Loss: 0.9218749403953552, Final Batch Loss: 0.3463362753391266\n",
      "Epoch 2660, Loss: 0.9975190907716751, Final Batch Loss: 0.4371142089366913\n",
      "Epoch 2661, Loss: 0.8404955863952637, Final Batch Loss: 0.3140060603618622\n",
      "Epoch 2662, Loss: 1.0040754079818726, Final Batch Loss: 0.43755125999450684\n",
      "Epoch 2663, Loss: 0.8814509212970734, Final Batch Loss: 0.3109579086303711\n",
      "Epoch 2664, Loss: 0.8845472633838654, Final Batch Loss: 0.310389906167984\n",
      "Epoch 2665, Loss: 0.9723015129566193, Final Batch Loss: 0.37422290444374084\n",
      "Epoch 2666, Loss: 1.0424512326717377, Final Batch Loss: 0.3893851637840271\n",
      "Epoch 2667, Loss: 0.9625952243804932, Final Batch Loss: 0.335501104593277\n",
      "Epoch 2668, Loss: 0.8060673773288727, Final Batch Loss: 0.20190316438674927\n",
      "Epoch 2669, Loss: 0.8080673813819885, Final Batch Loss: 0.22817647457122803\n",
      "Epoch 2670, Loss: 0.8355205953121185, Final Batch Loss: 0.274374395608902\n",
      "Epoch 2671, Loss: 1.0773634314537048, Final Batch Loss: 0.4085513949394226\n",
      "Epoch 2672, Loss: 0.8800525516271591, Final Batch Loss: 0.34967100620269775\n",
      "Epoch 2673, Loss: 0.724874921143055, Final Batch Loss: 0.07552150636911392\n",
      "Epoch 2674, Loss: 0.9649504125118256, Final Batch Loss: 0.3093221187591553\n",
      "Epoch 2675, Loss: 0.8601897656917572, Final Batch Loss: 0.29462379217147827\n",
      "Epoch 2676, Loss: 1.0040740072727203, Final Batch Loss: 0.36866647005081177\n",
      "Epoch 2677, Loss: 1.0581075847148895, Final Batch Loss: 0.457841157913208\n",
      "Epoch 2678, Loss: 1.061057984828949, Final Batch Loss: 0.5443589687347412\n",
      "Epoch 2679, Loss: 0.7652502804994583, Final Batch Loss: 0.19415085017681122\n",
      "Epoch 2680, Loss: 0.8027555495500565, Final Batch Loss: 0.22023506462574005\n",
      "Epoch 2681, Loss: 0.8426178097724915, Final Batch Loss: 0.2694965600967407\n",
      "Epoch 2682, Loss: 0.8716882914304733, Final Batch Loss: 0.32154473662376404\n",
      "Epoch 2683, Loss: 0.9894028007984161, Final Batch Loss: 0.4551028609275818\n",
      "Epoch 2684, Loss: 0.7963742911815643, Final Batch Loss: 0.19255799055099487\n",
      "Epoch 2685, Loss: 0.8695124685764313, Final Batch Loss: 0.25455209612846375\n",
      "Epoch 2686, Loss: 0.9829627275466919, Final Batch Loss: 0.4232493042945862\n",
      "Epoch 2687, Loss: 0.768937461078167, Final Batch Loss: 0.12202750891447067\n",
      "Epoch 2688, Loss: 0.7194715142250061, Final Batch Loss: 0.12172907590866089\n",
      "Epoch 2689, Loss: 0.7831169068813324, Final Batch Loss: 0.19969666004180908\n",
      "Epoch 2690, Loss: 0.8235820531845093, Final Batch Loss: 0.23992013931274414\n",
      "Epoch 2691, Loss: 0.8333385288715363, Final Batch Loss: 0.25563785433769226\n",
      "Epoch 2692, Loss: 0.7851191312074661, Final Batch Loss: 0.21783943474292755\n",
      "Epoch 2693, Loss: 1.1703855693340302, Final Batch Loss: 0.5774760842323303\n",
      "Epoch 2694, Loss: 0.8162166923284531, Final Batch Loss: 0.2789108157157898\n",
      "Epoch 2695, Loss: 0.8789724707603455, Final Batch Loss: 0.34007468819618225\n",
      "Epoch 2696, Loss: 0.9875421226024628, Final Batch Loss: 0.3348359763622284\n",
      "Epoch 2697, Loss: 0.8709062784910202, Final Batch Loss: 0.2978496253490448\n",
      "Epoch 2698, Loss: 0.8633386641740799, Final Batch Loss: 0.23519302904605865\n",
      "Epoch 2699, Loss: 0.9250471889972687, Final Batch Loss: 0.3090042471885681\n",
      "Epoch 2700, Loss: 1.1436402201652527, Final Batch Loss: 0.591799795627594\n",
      "Epoch 2701, Loss: 0.8079694360494614, Final Batch Loss: 0.22442957758903503\n",
      "Epoch 2702, Loss: 0.8622513562440872, Final Batch Loss: 0.3167940676212311\n",
      "Epoch 2703, Loss: 0.6885115504264832, Final Batch Loss: 0.13915878534317017\n",
      "Epoch 2704, Loss: 1.0304335951805115, Final Batch Loss: 0.45002785325050354\n",
      "Epoch 2705, Loss: 1.103756383061409, Final Batch Loss: 0.5762575268745422\n",
      "Epoch 2706, Loss: 0.7423805743455887, Final Batch Loss: 0.19022388756275177\n",
      "Epoch 2707, Loss: 0.7966810315847397, Final Batch Loss: 0.2247326821088791\n",
      "Epoch 2708, Loss: 0.8817371129989624, Final Batch Loss: 0.34243109822273254\n",
      "Epoch 2709, Loss: 0.9200582504272461, Final Batch Loss: 0.33575424551963806\n",
      "Epoch 2710, Loss: 0.8209940195083618, Final Batch Loss: 0.26106777787208557\n",
      "Epoch 2711, Loss: 0.8337447941303253, Final Batch Loss: 0.25048771500587463\n",
      "Epoch 2712, Loss: 0.7611466944217682, Final Batch Loss: 0.1999799609184265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2713, Loss: 0.7922983169555664, Final Batch Loss: 0.19024226069450378\n",
      "Epoch 2714, Loss: 0.7512511014938354, Final Batch Loss: 0.20038560032844543\n",
      "Epoch 2715, Loss: 0.712373822927475, Final Batch Loss: 0.18002277612686157\n",
      "Epoch 2716, Loss: 0.7197156250476837, Final Batch Loss: 0.13272106647491455\n",
      "Epoch 2717, Loss: 0.9699328541755676, Final Batch Loss: 0.4227825403213501\n",
      "Epoch 2718, Loss: 0.8042860180139542, Final Batch Loss: 0.22984589636325836\n",
      "Epoch 2719, Loss: 0.7919436395168304, Final Batch Loss: 0.2446940839290619\n",
      "Epoch 2720, Loss: 0.9166964590549469, Final Batch Loss: 0.2729980945587158\n",
      "Epoch 2721, Loss: 0.8505858331918716, Final Batch Loss: 0.23091883957386017\n",
      "Epoch 2722, Loss: 0.7229273468255997, Final Batch Loss: 0.1724899560213089\n",
      "Epoch 2723, Loss: 0.9324217736721039, Final Batch Loss: 0.307070791721344\n",
      "Epoch 2724, Loss: 0.8014902025461197, Final Batch Loss: 0.2236594408750534\n",
      "Epoch 2725, Loss: 0.6241913065314293, Final Batch Loss: 0.07046540826559067\n",
      "Epoch 2726, Loss: 0.7084387540817261, Final Batch Loss: 0.18372994661331177\n",
      "Epoch 2727, Loss: 0.7297684401273727, Final Batch Loss: 0.18950505554676056\n",
      "Epoch 2728, Loss: 0.9249527156352997, Final Batch Loss: 0.34508267045021057\n",
      "Epoch 2729, Loss: 0.9732975661754608, Final Batch Loss: 0.41800403594970703\n",
      "Epoch 2730, Loss: 0.85360187292099, Final Batch Loss: 0.24274349212646484\n",
      "Epoch 2731, Loss: 0.8017463982105255, Final Batch Loss: 0.25485527515411377\n",
      "Epoch 2732, Loss: 0.8724995851516724, Final Batch Loss: 0.30929458141326904\n",
      "Epoch 2733, Loss: 0.8737001866102219, Final Batch Loss: 0.3504350185394287\n",
      "Epoch 2734, Loss: 0.918275773525238, Final Batch Loss: 0.35982808470726013\n",
      "Epoch 2735, Loss: 0.6413891538977623, Final Batch Loss: 0.08494629710912704\n",
      "Epoch 2736, Loss: 0.9428430646657944, Final Batch Loss: 0.3730422854423523\n",
      "Epoch 2737, Loss: 0.8029675334692001, Final Batch Loss: 0.23170016705989838\n",
      "Epoch 2738, Loss: 0.8251201510429382, Final Batch Loss: 0.27852264046669006\n",
      "Epoch 2739, Loss: 1.086011826992035, Final Batch Loss: 0.45761606097221375\n",
      "Epoch 2740, Loss: 0.9107571244239807, Final Batch Loss: 0.3551403284072876\n",
      "Epoch 2741, Loss: 0.8357061594724655, Final Batch Loss: 0.24432574212551117\n",
      "Epoch 2742, Loss: 0.9578830301761627, Final Batch Loss: 0.39685195684432983\n",
      "Epoch 2743, Loss: 0.9343234002590179, Final Batch Loss: 0.35849833488464355\n",
      "Epoch 2744, Loss: 0.7875435501337051, Final Batch Loss: 0.21650071442127228\n",
      "Epoch 2745, Loss: 1.002104789018631, Final Batch Loss: 0.4278540313243866\n",
      "Epoch 2746, Loss: 0.7786477208137512, Final Batch Loss: 0.2334516942501068\n",
      "Epoch 2747, Loss: 0.901958167552948, Final Batch Loss: 0.2805400490760803\n",
      "Epoch 2748, Loss: 0.795117050409317, Final Batch Loss: 0.24091602861881256\n",
      "Epoch 2749, Loss: 0.8091623783111572, Final Batch Loss: 0.21631014347076416\n",
      "Epoch 2750, Loss: 0.8110052943229675, Final Batch Loss: 0.16238108277320862\n",
      "Epoch 2751, Loss: 0.8152576982975006, Final Batch Loss: 0.205539733171463\n",
      "Epoch 2752, Loss: 0.8080843985080719, Final Batch Loss: 0.2596229612827301\n",
      "Epoch 2753, Loss: 0.6269826889038086, Final Batch Loss: 0.10179215669631958\n",
      "Epoch 2754, Loss: 1.0650022327899933, Final Batch Loss: 0.39783886075019836\n",
      "Epoch 2755, Loss: 0.84683758020401, Final Batch Loss: 0.2589576840400696\n",
      "Epoch 2756, Loss: 0.861682578921318, Final Batch Loss: 0.21872110664844513\n",
      "Epoch 2757, Loss: 0.7125638052821159, Final Batch Loss: 0.12384020537137985\n",
      "Epoch 2758, Loss: 0.7957049906253815, Final Batch Loss: 0.2115502655506134\n",
      "Epoch 2759, Loss: 1.0623555183410645, Final Batch Loss: 0.5214219093322754\n",
      "Epoch 2760, Loss: 0.6334861330688, Final Batch Loss: 0.05547712370753288\n",
      "Epoch 2761, Loss: 0.9223388135433197, Final Batch Loss: 0.38144758343696594\n",
      "Epoch 2762, Loss: 0.8208616822957993, Final Batch Loss: 0.23739071190357208\n",
      "Epoch 2763, Loss: 0.7215411365032196, Final Batch Loss: 0.15108805894851685\n",
      "Epoch 2764, Loss: 0.8922906368970871, Final Batch Loss: 0.3104776442050934\n",
      "Epoch 2765, Loss: 0.7915609627962112, Final Batch Loss: 0.2956489622592926\n",
      "Epoch 2766, Loss: 0.7662733048200607, Final Batch Loss: 0.2143220156431198\n",
      "Epoch 2767, Loss: 0.7442649304866791, Final Batch Loss: 0.2185002565383911\n",
      "Epoch 2768, Loss: 0.8508898913860321, Final Batch Loss: 0.27274420857429504\n",
      "Epoch 2769, Loss: 0.8188298940658569, Final Batch Loss: 0.228666752576828\n",
      "Epoch 2770, Loss: 0.8842579126358032, Final Batch Loss: 0.3369530737400055\n",
      "Epoch 2771, Loss: 0.9041344523429871, Final Batch Loss: 0.32360589504241943\n",
      "Epoch 2772, Loss: 0.6483354195952415, Final Batch Loss: 0.10787680000066757\n",
      "Epoch 2773, Loss: 0.7405377179384232, Final Batch Loss: 0.14561879634857178\n",
      "Epoch 2774, Loss: 0.8744229823350906, Final Batch Loss: 0.34890538454055786\n",
      "Epoch 2775, Loss: 0.8306700140237808, Final Batch Loss: 0.22395969927310944\n",
      "Epoch 2776, Loss: 0.8688365817070007, Final Batch Loss: 0.29842209815979004\n",
      "Epoch 2777, Loss: 0.726347804069519, Final Batch Loss: 0.19727735221385956\n",
      "Epoch 2778, Loss: 0.8699502944946289, Final Batch Loss: 0.33585992455482483\n",
      "Epoch 2779, Loss: 0.8417691439390182, Final Batch Loss: 0.3093603551387787\n",
      "Epoch 2780, Loss: 0.8123993128538132, Final Batch Loss: 0.2587987780570984\n",
      "Epoch 2781, Loss: 0.7983405739068985, Final Batch Loss: 0.23071853816509247\n",
      "Epoch 2782, Loss: 0.7791105359792709, Final Batch Loss: 0.19776366651058197\n",
      "Epoch 2783, Loss: 0.7234824895858765, Final Batch Loss: 0.17599181830883026\n",
      "Epoch 2784, Loss: 0.8920139074325562, Final Batch Loss: 0.35338109731674194\n",
      "Epoch 2785, Loss: 0.8330381512641907, Final Batch Loss: 0.26919862627983093\n",
      "Epoch 2786, Loss: 0.8657468557357788, Final Batch Loss: 0.27630624175071716\n",
      "Epoch 2787, Loss: 0.8457770645618439, Final Batch Loss: 0.3178074359893799\n",
      "Epoch 2788, Loss: 0.773360937833786, Final Batch Loss: 0.19213294982910156\n",
      "Epoch 2789, Loss: 0.6518668830394745, Final Batch Loss: 0.1314483880996704\n",
      "Epoch 2790, Loss: 0.8288890719413757, Final Batch Loss: 0.3373505473136902\n",
      "Epoch 2791, Loss: 0.8805481195449829, Final Batch Loss: 0.19306159019470215\n",
      "Epoch 2792, Loss: 0.6486377939581871, Final Batch Loss: 0.10243277996778488\n",
      "Epoch 2793, Loss: 0.833505392074585, Final Batch Loss: 0.3259548246860504\n",
      "Epoch 2794, Loss: 0.9449854791164398, Final Batch Loss: 0.3541300296783447\n",
      "Epoch 2795, Loss: 0.7572746574878693, Final Batch Loss: 0.22702951729297638\n",
      "Epoch 2796, Loss: 0.9375590980052948, Final Batch Loss: 0.4185980558395386\n",
      "Epoch 2797, Loss: 0.8263587653636932, Final Batch Loss: 0.29069897532463074\n",
      "Epoch 2798, Loss: 0.7024181559681892, Final Batch Loss: 0.12455800920724869\n",
      "Epoch 2799, Loss: 0.8111404925584793, Final Batch Loss: 0.1806732565164566\n",
      "Epoch 2800, Loss: 0.7033710032701492, Final Batch Loss: 0.1688758134841919\n",
      "Epoch 2801, Loss: 0.8752586245536804, Final Batch Loss: 0.3266982138156891\n",
      "Epoch 2802, Loss: 0.8674115389585495, Final Batch Loss: 0.3163709342479706\n",
      "Epoch 2803, Loss: 0.8994832932949066, Final Batch Loss: 0.35208308696746826\n",
      "Epoch 2804, Loss: 0.7861237823963165, Final Batch Loss: 0.2762693762779236\n",
      "Epoch 2805, Loss: 0.7818047255277634, Final Batch Loss: 0.23157157003879547\n",
      "Epoch 2806, Loss: 0.7804866880178452, Final Batch Loss: 0.18124808371067047\n",
      "Epoch 2807, Loss: 0.818350613117218, Final Batch Loss: 0.26854944229125977\n",
      "Epoch 2808, Loss: 0.7641573697328568, Final Batch Loss: 0.24594879150390625\n",
      "Epoch 2809, Loss: 0.8781686425209045, Final Batch Loss: 0.33926934003829956\n",
      "Epoch 2810, Loss: 0.9613012671470642, Final Batch Loss: 0.39849191904067993\n",
      "Epoch 2811, Loss: 0.9713286608457565, Final Batch Loss: 0.43991294503211975\n",
      "Epoch 2812, Loss: 0.7405316829681396, Final Batch Loss: 0.19468560814857483\n",
      "Epoch 2813, Loss: 0.6539369449019432, Final Batch Loss: 0.1017465814948082\n",
      "Epoch 2814, Loss: 1.0420317351818085, Final Batch Loss: 0.4901333749294281\n",
      "Epoch 2815, Loss: 0.7343444377183914, Final Batch Loss: 0.17110669612884521\n",
      "Epoch 2816, Loss: 0.9243370890617371, Final Batch Loss: 0.40705934166908264\n",
      "Epoch 2817, Loss: 0.9677147567272186, Final Batch Loss: 0.3935612738132477\n",
      "Epoch 2818, Loss: 0.7697503566741943, Final Batch Loss: 0.18977826833724976\n",
      "Epoch 2819, Loss: 0.9538141787052155, Final Batch Loss: 0.37609970569610596\n",
      "Epoch 2820, Loss: 0.928644210100174, Final Batch Loss: 0.28724202513694763\n",
      "Epoch 2821, Loss: 0.964247077703476, Final Batch Loss: 0.39330634474754333\n",
      "Epoch 2822, Loss: 0.7636227607727051, Final Batch Loss: 0.20015037059783936\n",
      "Epoch 2823, Loss: 0.9797698557376862, Final Batch Loss: 0.38944390416145325\n",
      "Epoch 2824, Loss: 0.8978957831859589, Final Batch Loss: 0.332206666469574\n",
      "Epoch 2825, Loss: 1.133458897471428, Final Batch Loss: 0.6314301490783691\n",
      "Epoch 2826, Loss: 0.796853557229042, Final Batch Loss: 0.24440626800060272\n",
      "Epoch 2827, Loss: 0.8054908812046051, Final Batch Loss: 0.24983900785446167\n",
      "Epoch 2828, Loss: 0.746324747800827, Final Batch Loss: 0.16132128238677979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2829, Loss: 0.7986854314804077, Final Batch Loss: 0.2257419377565384\n",
      "Epoch 2830, Loss: 0.8265009820461273, Final Batch Loss: 0.3238225281238556\n",
      "Epoch 2831, Loss: 0.8302194476127625, Final Batch Loss: 0.2920842170715332\n",
      "Epoch 2832, Loss: 0.9012007713317871, Final Batch Loss: 0.31864821910858154\n",
      "Epoch 2833, Loss: 0.8674830794334412, Final Batch Loss: 0.3217345178127289\n",
      "Epoch 2834, Loss: 0.7971169948577881, Final Batch Loss: 0.19839772582054138\n",
      "Epoch 2835, Loss: 0.967263787984848, Final Batch Loss: 0.38336828351020813\n",
      "Epoch 2836, Loss: 1.1331034302711487, Final Batch Loss: 0.5244491696357727\n",
      "Epoch 2837, Loss: 0.9036588668823242, Final Batch Loss: 0.27979686856269836\n",
      "Epoch 2838, Loss: 0.6576612070202827, Final Batch Loss: 0.11829794198274612\n",
      "Epoch 2839, Loss: 0.7130696624517441, Final Batch Loss: 0.16657720506191254\n",
      "Epoch 2840, Loss: 0.9332888424396515, Final Batch Loss: 0.3685648441314697\n",
      "Epoch 2841, Loss: 0.9775343239307404, Final Batch Loss: 0.3868221938610077\n",
      "Epoch 2842, Loss: 0.7656892389059067, Final Batch Loss: 0.22094352543354034\n",
      "Epoch 2843, Loss: 0.850986123085022, Final Batch Loss: 0.3367868661880493\n",
      "Epoch 2844, Loss: 0.9805439114570618, Final Batch Loss: 0.4031480848789215\n",
      "Epoch 2845, Loss: 0.7239961475133896, Final Batch Loss: 0.22680877149105072\n",
      "Epoch 2846, Loss: 0.807368278503418, Final Batch Loss: 0.27471670508384705\n",
      "Epoch 2847, Loss: 0.7422133684158325, Final Batch Loss: 0.19968189299106598\n",
      "Epoch 2848, Loss: 0.76292484998703, Final Batch Loss: 0.18601438403129578\n",
      "Epoch 2849, Loss: 0.6810147911310196, Final Batch Loss: 0.13440661132335663\n",
      "Epoch 2850, Loss: 0.7872616499662399, Final Batch Loss: 0.23716454207897186\n",
      "Epoch 2851, Loss: 0.8054229915142059, Final Batch Loss: 0.3098502457141876\n",
      "Epoch 2852, Loss: 0.8121284544467926, Final Batch Loss: 0.29914483428001404\n",
      "Epoch 2853, Loss: 0.8549239039421082, Final Batch Loss: 0.3155009150505066\n",
      "Epoch 2854, Loss: 0.7534276098012924, Final Batch Loss: 0.21761484444141388\n",
      "Epoch 2855, Loss: 0.8159877210855484, Final Batch Loss: 0.22488875687122345\n",
      "Epoch 2856, Loss: 0.9597432166337967, Final Batch Loss: 0.35676759481430054\n",
      "Epoch 2857, Loss: 0.9363624006509781, Final Batch Loss: 0.4104212820529938\n",
      "Epoch 2858, Loss: 0.7608592808246613, Final Batch Loss: 0.24034595489501953\n",
      "Epoch 2859, Loss: 0.6089223176240921, Final Batch Loss: 0.08916103839874268\n",
      "Epoch 2860, Loss: 0.7798717468976974, Final Batch Loss: 0.20195995271205902\n",
      "Epoch 2861, Loss: 0.8097157776355743, Final Batch Loss: 0.27693530917167664\n",
      "Epoch 2862, Loss: 1.0465185642242432, Final Batch Loss: 0.4901706278324127\n",
      "Epoch 2863, Loss: 1.0810886770486832, Final Batch Loss: 0.5862975120544434\n",
      "Epoch 2864, Loss: 0.6997480690479279, Final Batch Loss: 0.14487501978874207\n",
      "Epoch 2865, Loss: 0.6981505304574966, Final Batch Loss: 0.16661186516284943\n",
      "Epoch 2866, Loss: 0.8666210174560547, Final Batch Loss: 0.3107229769229889\n",
      "Epoch 2867, Loss: 0.6315927654504776, Final Batch Loss: 0.05750627815723419\n",
      "Epoch 2868, Loss: 0.9716322422027588, Final Batch Loss: 0.4198870360851288\n",
      "Epoch 2869, Loss: 0.8368241786956787, Final Batch Loss: 0.3365233242511749\n",
      "Epoch 2870, Loss: 0.7718421518802643, Final Batch Loss: 0.22238104045391083\n",
      "Epoch 2871, Loss: 0.9126074612140656, Final Batch Loss: 0.39815831184387207\n",
      "Epoch 2872, Loss: 0.6731677502393723, Final Batch Loss: 0.08993728458881378\n",
      "Epoch 2873, Loss: 0.980463057756424, Final Batch Loss: 0.4838005006313324\n",
      "Epoch 2874, Loss: 1.0722289979457855, Final Batch Loss: 0.4742169678211212\n",
      "Epoch 2875, Loss: 0.9122342616319656, Final Batch Loss: 0.3956674635410309\n",
      "Epoch 2876, Loss: 0.7955894023180008, Final Batch Loss: 0.1769019514322281\n",
      "Epoch 2877, Loss: 0.9193872511386871, Final Batch Loss: 0.3686775863170624\n",
      "Epoch 2878, Loss: 0.7966939061880112, Final Batch Loss: 0.23384834825992584\n",
      "Epoch 2879, Loss: 0.8410035669803619, Final Batch Loss: 0.3194039762020111\n",
      "Epoch 2880, Loss: 0.8698374927043915, Final Batch Loss: 0.32558879256248474\n",
      "Epoch 2881, Loss: 0.8905893266201019, Final Batch Loss: 0.3651452660560608\n",
      "Epoch 2882, Loss: 0.8354530036449432, Final Batch Loss: 0.2452031672000885\n",
      "Epoch 2883, Loss: 0.6207818388938904, Final Batch Loss: 0.08244015276432037\n",
      "Epoch 2884, Loss: 0.8632750511169434, Final Batch Loss: 0.3432958424091339\n",
      "Epoch 2885, Loss: 0.9693166315555573, Final Batch Loss: 0.4108676612377167\n",
      "Epoch 2886, Loss: 0.8574654161930084, Final Batch Loss: 0.2667941153049469\n",
      "Epoch 2887, Loss: 0.8245991766452789, Final Batch Loss: 0.2505568563938141\n",
      "Epoch 2888, Loss: 0.8239630311727524, Final Batch Loss: 0.2364092320203781\n",
      "Epoch 2889, Loss: 0.9156095385551453, Final Batch Loss: 0.3299623131752014\n",
      "Epoch 2890, Loss: 0.9230208396911621, Final Batch Loss: 0.3625922203063965\n",
      "Epoch 2891, Loss: 0.9179077446460724, Final Batch Loss: 0.36229023337364197\n",
      "Epoch 2892, Loss: 0.7310424596071243, Final Batch Loss: 0.1641233116388321\n",
      "Epoch 2893, Loss: 0.9466807842254639, Final Batch Loss: 0.3265102803707123\n",
      "Epoch 2894, Loss: 0.7803058177232742, Final Batch Loss: 0.2209637314081192\n",
      "Epoch 2895, Loss: 0.9196877032518387, Final Batch Loss: 0.38208818435668945\n",
      "Epoch 2896, Loss: 0.6648750454187393, Final Batch Loss: 0.09290696680545807\n",
      "Epoch 2897, Loss: 0.7457416504621506, Final Batch Loss: 0.17223702371120453\n",
      "Epoch 2898, Loss: 0.9409647285938263, Final Batch Loss: 0.3702717423439026\n",
      "Epoch 2899, Loss: 0.7927521914243698, Final Batch Loss: 0.23550774157047272\n",
      "Epoch 2900, Loss: 0.6615199744701385, Final Batch Loss: 0.14675801992416382\n",
      "Epoch 2901, Loss: 0.742479145526886, Final Batch Loss: 0.12591850757598877\n",
      "Epoch 2902, Loss: 0.9411884248256683, Final Batch Loss: 0.3800007402896881\n",
      "Epoch 2903, Loss: 0.9213647842407227, Final Batch Loss: 0.37156978249549866\n",
      "Epoch 2904, Loss: 0.8134374022483826, Final Batch Loss: 0.30173683166503906\n",
      "Epoch 2905, Loss: 0.8464865684509277, Final Batch Loss: 0.3482646048069\n",
      "Epoch 2906, Loss: 0.8265104591846466, Final Batch Loss: 0.325209379196167\n",
      "Epoch 2907, Loss: 0.7252960354089737, Final Batch Loss: 0.17858485877513885\n",
      "Epoch 2908, Loss: 0.7740295976400375, Final Batch Loss: 0.21201185882091522\n",
      "Epoch 2909, Loss: 0.7653715163469315, Final Batch Loss: 0.2393520325422287\n",
      "Epoch 2910, Loss: 0.8527243137359619, Final Batch Loss: 0.3974669277667999\n",
      "Epoch 2911, Loss: 0.7789818793535233, Final Batch Loss: 0.2152281552553177\n",
      "Epoch 2912, Loss: 0.6150644198060036, Final Batch Loss: 0.0795455351471901\n",
      "Epoch 2913, Loss: 0.9563065469264984, Final Batch Loss: 0.3873401880264282\n",
      "Epoch 2914, Loss: 0.6306580603122711, Final Batch Loss: 0.09158670902252197\n",
      "Epoch 2915, Loss: 0.9838176667690277, Final Batch Loss: 0.3928546905517578\n",
      "Epoch 2916, Loss: 0.7194828242063522, Final Batch Loss: 0.1532200425863266\n",
      "Epoch 2917, Loss: 0.8302799761295319, Final Batch Loss: 0.23823803663253784\n",
      "Epoch 2918, Loss: 0.7975048273801804, Final Batch Loss: 0.23200607299804688\n",
      "Epoch 2919, Loss: 0.6872464120388031, Final Batch Loss: 0.11757701635360718\n",
      "Epoch 2920, Loss: 0.8304088711738586, Final Batch Loss: 0.3065902590751648\n",
      "Epoch 2921, Loss: 0.9819045066833496, Final Batch Loss: 0.427774041891098\n",
      "Epoch 2922, Loss: 0.845266580581665, Final Batch Loss: 0.2875206470489502\n",
      "Epoch 2923, Loss: 0.8286169767379761, Final Batch Loss: 0.2933371067047119\n",
      "Epoch 2924, Loss: 0.8784957528114319, Final Batch Loss: 0.36124154925346375\n",
      "Epoch 2925, Loss: 0.9714685827493668, Final Batch Loss: 0.459559828042984\n",
      "Epoch 2926, Loss: 0.8494714796543121, Final Batch Loss: 0.2874487340450287\n",
      "Epoch 2927, Loss: 0.6091540791094303, Final Batch Loss: 0.055738065391778946\n",
      "Epoch 2928, Loss: 0.8252816498279572, Final Batch Loss: 0.3217627704143524\n",
      "Epoch 2929, Loss: 0.7616256773471832, Final Batch Loss: 0.2383408099412918\n",
      "Epoch 2930, Loss: 0.8513276875019073, Final Batch Loss: 0.3133236765861511\n",
      "Epoch 2931, Loss: 0.9513867795467377, Final Batch Loss: 0.40811195969581604\n",
      "Epoch 2932, Loss: 0.870572417974472, Final Batch Loss: 0.3744274079799652\n",
      "Epoch 2933, Loss: 0.8211666345596313, Final Batch Loss: 0.32666367292404175\n",
      "Epoch 2934, Loss: 0.6283437758684158, Final Batch Loss: 0.08733999729156494\n",
      "Epoch 2935, Loss: 1.2557792663574219, Final Batch Loss: 0.7123615741729736\n",
      "Epoch 2936, Loss: 0.71531543135643, Final Batch Loss: 0.12519443035125732\n",
      "Epoch 2937, Loss: 0.8921989798545837, Final Batch Loss: 0.282195508480072\n",
      "Epoch 2938, Loss: 0.7241940498352051, Final Batch Loss: 0.20435090363025665\n",
      "Epoch 2939, Loss: 1.020231008529663, Final Batch Loss: 0.5147243738174438\n",
      "Epoch 2940, Loss: 0.7377185374498367, Final Batch Loss: 0.20576347410678864\n",
      "Epoch 2941, Loss: 0.9223066866397858, Final Batch Loss: 0.40543219447135925\n",
      "Epoch 2942, Loss: 0.8218785226345062, Final Batch Loss: 0.2720482349395752\n",
      "Epoch 2943, Loss: 0.8869668245315552, Final Batch Loss: 0.27765345573425293\n",
      "Epoch 2944, Loss: 1.3457958102226257, Final Batch Loss: 0.7293215394020081\n",
      "Epoch 2945, Loss: 0.8185775578022003, Final Batch Loss: 0.2993496358394623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2946, Loss: 0.9766724109649658, Final Batch Loss: 0.4138809144496918\n",
      "Epoch 2947, Loss: 0.8352666199207306, Final Batch Loss: 0.20803844928741455\n",
      "Epoch 2948, Loss: 0.8263255655765533, Final Batch Loss: 0.2866247594356537\n",
      "Epoch 2949, Loss: 0.8261346966028214, Final Batch Loss: 0.2255275696516037\n",
      "Epoch 2950, Loss: 0.6416044682264328, Final Batch Loss: 0.11376157402992249\n",
      "Epoch 2951, Loss: 0.8547063767910004, Final Batch Loss: 0.2491612434387207\n",
      "Epoch 2952, Loss: 0.8996249437332153, Final Batch Loss: 0.3909396231174469\n",
      "Epoch 2953, Loss: 0.923609122633934, Final Batch Loss: 0.4571853578090668\n",
      "Epoch 2954, Loss: 0.7013723254203796, Final Batch Loss: 0.1525944024324417\n",
      "Epoch 2955, Loss: 0.7492153346538544, Final Batch Loss: 0.19270360469818115\n",
      "Epoch 2956, Loss: 0.6969158947467804, Final Batch Loss: 0.1281145215034485\n",
      "Epoch 2957, Loss: 0.8129420578479767, Final Batch Loss: 0.2916302978992462\n",
      "Epoch 2958, Loss: 0.6357437968254089, Final Batch Loss: 0.1270289272069931\n",
      "Epoch 2959, Loss: 0.8982508331537247, Final Batch Loss: 0.3622095584869385\n",
      "Epoch 2960, Loss: 0.8059300184249878, Final Batch Loss: 0.26240524649620056\n",
      "Epoch 2961, Loss: 0.9132403433322906, Final Batch Loss: 0.4098065495491028\n",
      "Epoch 2962, Loss: 0.5996980741620064, Final Batch Loss: 0.11335910111665726\n",
      "Epoch 2963, Loss: 0.9451791048049927, Final Batch Loss: 0.2973673343658447\n",
      "Epoch 2964, Loss: 0.8700239360332489, Final Batch Loss: 0.2629404366016388\n",
      "Epoch 2965, Loss: 0.8340440392494202, Final Batch Loss: 0.2522810995578766\n",
      "Epoch 2966, Loss: 0.7679187208414078, Final Batch Loss: 0.19379623234272003\n",
      "Epoch 2967, Loss: 0.6702174097299576, Final Batch Loss: 0.1419469565153122\n",
      "Epoch 2968, Loss: 0.6383914053440094, Final Batch Loss: 0.1280754953622818\n",
      "Epoch 2969, Loss: 0.762066513299942, Final Batch Loss: 0.23283331096172333\n",
      "Epoch 2970, Loss: 0.6962218582630157, Final Batch Loss: 0.15359768271446228\n",
      "Epoch 2971, Loss: 0.8575012683868408, Final Batch Loss: 0.29904797673225403\n",
      "Epoch 2972, Loss: 0.8941716551780701, Final Batch Loss: 0.2972826361656189\n",
      "Epoch 2973, Loss: 0.8534523248672485, Final Batch Loss: 0.3431871235370636\n",
      "Epoch 2974, Loss: 0.6545169502496719, Final Batch Loss: 0.07781295478343964\n",
      "Epoch 2975, Loss: 0.6906347647309303, Final Batch Loss: 0.12479079514741898\n",
      "Epoch 2976, Loss: 0.8609844446182251, Final Batch Loss: 0.32269370555877686\n",
      "Epoch 2977, Loss: 0.9186780750751495, Final Batch Loss: 0.31839719414711\n",
      "Epoch 2978, Loss: 1.0926775336265564, Final Batch Loss: 0.5926593542098999\n",
      "Epoch 2979, Loss: 0.9040515124797821, Final Batch Loss: 0.36674436926841736\n",
      "Epoch 2980, Loss: 0.816252127289772, Final Batch Loss: 0.2963085174560547\n",
      "Epoch 2981, Loss: 0.8254894614219666, Final Batch Loss: 0.25806131958961487\n",
      "Epoch 2982, Loss: 0.6660587787628174, Final Batch Loss: 0.1532701998949051\n",
      "Epoch 2983, Loss: 0.8986223936080933, Final Batch Loss: 0.37523359060287476\n",
      "Epoch 2984, Loss: 0.998498260974884, Final Batch Loss: 0.45266255736351013\n",
      "Epoch 2985, Loss: 0.8303424119949341, Final Batch Loss: 0.2805510461330414\n",
      "Epoch 2986, Loss: 0.6772759705781937, Final Batch Loss: 0.167485311627388\n",
      "Epoch 2987, Loss: 0.8491698503494263, Final Batch Loss: 0.2864789366722107\n",
      "Epoch 2988, Loss: 0.848219484090805, Final Batch Loss: 0.2922017574310303\n",
      "Epoch 2989, Loss: 0.6273287832736969, Final Batch Loss: 0.08513027429580688\n",
      "Epoch 2990, Loss: 0.8171399682760239, Final Batch Loss: 0.3416103422641754\n",
      "Epoch 2991, Loss: 0.6854834109544754, Final Batch Loss: 0.14258703589439392\n",
      "Epoch 2992, Loss: 0.8445535004138947, Final Batch Loss: 0.268642395734787\n",
      "Epoch 2993, Loss: 0.7165978401899338, Final Batch Loss: 0.215547114610672\n",
      "Epoch 2994, Loss: 0.6774060875177383, Final Batch Loss: 0.15532700717449188\n",
      "Epoch 2995, Loss: 1.1486346125602722, Final Batch Loss: 0.5754415392875671\n",
      "Epoch 2996, Loss: 0.8388387560844421, Final Batch Loss: 0.3168387711048126\n",
      "Epoch 2997, Loss: 0.8318542242050171, Final Batch Loss: 0.2717553675174713\n",
      "Epoch 2998, Loss: 0.8825213760137558, Final Batch Loss: 0.3645888864994049\n",
      "Epoch 2999, Loss: 0.77455173432827, Final Batch Loss: 0.21203778684139252\n",
      "Epoch 3000, Loss: 0.7478196620941162, Final Batch Loss: 0.2189473807811737\n",
      "Epoch 3001, Loss: 0.8577791899442673, Final Batch Loss: 0.29729533195495605\n",
      "Epoch 3002, Loss: 0.8917061388492584, Final Batch Loss: 0.4022311866283417\n",
      "Epoch 3003, Loss: 0.7061753273010254, Final Batch Loss: 0.1329779028892517\n",
      "Epoch 3004, Loss: 0.6871839761734009, Final Batch Loss: 0.1330091953277588\n",
      "Epoch 3005, Loss: 0.6522360593080521, Final Batch Loss: 0.1398577243089676\n",
      "Epoch 3006, Loss: 0.839596152305603, Final Batch Loss: 0.27429044246673584\n",
      "Epoch 3007, Loss: 0.9917120039463043, Final Batch Loss: 0.4456717371940613\n",
      "Epoch 3008, Loss: 0.6520240157842636, Final Batch Loss: 0.14157378673553467\n",
      "Epoch 3009, Loss: 0.9879061728715897, Final Batch Loss: 0.4865666925907135\n",
      "Epoch 3010, Loss: 0.7704805135726929, Final Batch Loss: 0.18500247597694397\n",
      "Epoch 3011, Loss: 0.8234219700098038, Final Batch Loss: 0.22840873897075653\n",
      "Epoch 3012, Loss: 0.72697314620018, Final Batch Loss: 0.12981170415878296\n",
      "Epoch 3013, Loss: 0.6788023561239243, Final Batch Loss: 0.18489223718643188\n",
      "Epoch 3014, Loss: 0.8825719058513641, Final Batch Loss: 0.26355957984924316\n",
      "Epoch 3015, Loss: 0.7022960335016251, Final Batch Loss: 0.1891898661851883\n",
      "Epoch 3016, Loss: 0.7223777770996094, Final Batch Loss: 0.21385692059993744\n",
      "Epoch 3017, Loss: 0.7007906585931778, Final Batch Loss: 0.1926102340221405\n",
      "Epoch 3018, Loss: 0.870874673128128, Final Batch Loss: 0.3500920534133911\n",
      "Epoch 3019, Loss: 0.7231108099222183, Final Batch Loss: 0.12767280638217926\n",
      "Epoch 3020, Loss: 0.7417697459459305, Final Batch Loss: 0.22433802485466003\n",
      "Epoch 3021, Loss: 0.8933051228523254, Final Batch Loss: 0.3823358714580536\n",
      "Epoch 3022, Loss: 0.7984408289194107, Final Batch Loss: 0.28554797172546387\n",
      "Epoch 3023, Loss: 0.9105014204978943, Final Batch Loss: 0.4430408477783203\n",
      "Epoch 3024, Loss: 0.9580685794353485, Final Batch Loss: 0.44842594861984253\n",
      "Epoch 3025, Loss: 0.7161399871110916, Final Batch Loss: 0.18457473814487457\n",
      "Epoch 3026, Loss: 0.7852326035499573, Final Batch Loss: 0.2618337571620941\n",
      "Epoch 3027, Loss: 0.9002093970775604, Final Batch Loss: 0.28192320466041565\n",
      "Epoch 3028, Loss: 0.802042156457901, Final Batch Loss: 0.2622685134410858\n",
      "Epoch 3029, Loss: 0.9489052146673203, Final Batch Loss: 0.469536691904068\n",
      "Epoch 3030, Loss: 0.8952114880084991, Final Batch Loss: 0.334610253572464\n",
      "Epoch 3031, Loss: 0.7280349284410477, Final Batch Loss: 0.20217832922935486\n",
      "Epoch 3032, Loss: 0.6898660957813263, Final Batch Loss: 0.11201852560043335\n",
      "Epoch 3033, Loss: 0.8641999661922455, Final Batch Loss: 0.28292128443717957\n",
      "Epoch 3034, Loss: 0.6957881301641464, Final Batch Loss: 0.1516890674829483\n",
      "Epoch 3035, Loss: 0.8548998534679413, Final Batch Loss: 0.3499521315097809\n",
      "Epoch 3036, Loss: 0.9703165590763092, Final Batch Loss: 0.44403302669525146\n",
      "Epoch 3037, Loss: 0.7922022342681885, Final Batch Loss: 0.22748345136642456\n",
      "Epoch 3038, Loss: 0.8352397680282593, Final Batch Loss: 0.322936475276947\n",
      "Epoch 3039, Loss: 0.9972035586833954, Final Batch Loss: 0.46211931109428406\n",
      "Epoch 3040, Loss: 1.0575875788927078, Final Batch Loss: 0.5850062966346741\n",
      "Epoch 3041, Loss: 0.8240628689527512, Final Batch Loss: 0.31808432936668396\n",
      "Epoch 3042, Loss: 0.821371853351593, Final Batch Loss: 0.3136419355869293\n",
      "Epoch 3043, Loss: 0.8187418580055237, Final Batch Loss: 0.2870180904865265\n",
      "Epoch 3044, Loss: 0.8097918629646301, Final Batch Loss: 0.2638972997665405\n",
      "Epoch 3045, Loss: 0.7229718565940857, Final Batch Loss: 0.20019115507602692\n",
      "Epoch 3046, Loss: 0.8998549282550812, Final Batch Loss: 0.34073105454444885\n",
      "Epoch 3047, Loss: 0.6733741760253906, Final Batch Loss: 0.1187845766544342\n",
      "Epoch 3048, Loss: 0.7124975323677063, Final Batch Loss: 0.18461890518665314\n",
      "Epoch 3049, Loss: 0.8066929280757904, Final Batch Loss: 0.23055651783943176\n",
      "Epoch 3050, Loss: 0.8997573703527451, Final Batch Loss: 0.4233402907848358\n",
      "Epoch 3051, Loss: 0.9156773239374161, Final Batch Loss: 0.44071364402770996\n",
      "Epoch 3052, Loss: 0.7916164696216583, Final Batch Loss: 0.2471693754196167\n",
      "Epoch 3053, Loss: 0.6427700147032738, Final Batch Loss: 0.10774274915456772\n",
      "Epoch 3054, Loss: 0.6517540514469147, Final Batch Loss: 0.09306302666664124\n",
      "Epoch 3055, Loss: 0.6224097460508347, Final Batch Loss: 0.1362442523241043\n",
      "Epoch 3056, Loss: 0.7805149555206299, Final Batch Loss: 0.28168827295303345\n",
      "Epoch 3057, Loss: 0.7877122759819031, Final Batch Loss: 0.21629378199577332\n",
      "Epoch 3058, Loss: 0.7546916753053665, Final Batch Loss: 0.18693308532238007\n",
      "Epoch 3059, Loss: 1.1367416083812714, Final Batch Loss: 0.6101790070533752\n",
      "Epoch 3060, Loss: 1.0490143597126007, Final Batch Loss: 0.5711280107498169\n",
      "Epoch 3061, Loss: 0.692666694521904, Final Batch Loss: 0.16342517733573914\n",
      "Epoch 3062, Loss: 0.7246058434247971, Final Batch Loss: 0.15721632540225983\n",
      "Epoch 3063, Loss: 0.8259509205818176, Final Batch Loss: 0.30190029740333557\n",
      "Epoch 3064, Loss: 0.9158312678337097, Final Batch Loss: 0.3722141683101654\n",
      "Epoch 3065, Loss: 0.5698669143021107, Final Batch Loss: 0.048103708773851395\n",
      "Epoch 3066, Loss: 0.7306857109069824, Final Batch Loss: 0.2235884964466095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3067, Loss: 0.9311368018388748, Final Batch Loss: 0.4305274486541748\n",
      "Epoch 3068, Loss: 0.7672970294952393, Final Batch Loss: 0.23007729649543762\n",
      "Epoch 3069, Loss: 0.6982380896806717, Final Batch Loss: 0.1378360539674759\n",
      "Epoch 3070, Loss: 0.8561746180057526, Final Batch Loss: 0.29651060700416565\n",
      "Epoch 3071, Loss: 0.7223459780216217, Final Batch Loss: 0.16953924298286438\n",
      "Epoch 3072, Loss: 1.036016821861267, Final Batch Loss: 0.4312819540500641\n",
      "Epoch 3073, Loss: 0.7841869741678238, Final Batch Loss: 0.2331835776567459\n",
      "Epoch 3074, Loss: 0.7253727316856384, Final Batch Loss: 0.17035400867462158\n",
      "Epoch 3075, Loss: 0.6849194467067719, Final Batch Loss: 0.18391551077365875\n",
      "Epoch 3076, Loss: 0.9486301839351654, Final Batch Loss: 0.41053107380867004\n",
      "Epoch 3077, Loss: 0.8439301252365112, Final Batch Loss: 0.26103028655052185\n",
      "Epoch 3078, Loss: 0.7493157535791397, Final Batch Loss: 0.19571852684020996\n",
      "Epoch 3079, Loss: 0.7116900533437729, Final Batch Loss: 0.18325042724609375\n",
      "Epoch 3080, Loss: 0.9069580137729645, Final Batch Loss: 0.3406243920326233\n",
      "Epoch 3081, Loss: 0.7399590015411377, Final Batch Loss: 0.18016552925109863\n",
      "Epoch 3082, Loss: 0.6829807907342911, Final Batch Loss: 0.210490420460701\n",
      "Epoch 3083, Loss: 0.6852331906557083, Final Batch Loss: 0.19489461183547974\n",
      "Epoch 3084, Loss: 0.8229378461837769, Final Batch Loss: 0.30625224113464355\n",
      "Epoch 3085, Loss: 0.8500884622335434, Final Batch Loss: 0.3667244017124176\n",
      "Epoch 3086, Loss: 0.5711133107542992, Final Batch Loss: 0.04600527137517929\n",
      "Epoch 3087, Loss: 0.7143905311822891, Final Batch Loss: 0.1667499989271164\n",
      "Epoch 3088, Loss: 0.7869942486286163, Final Batch Loss: 0.28403404355049133\n",
      "Epoch 3089, Loss: 0.7325143665075302, Final Batch Loss: 0.16092057526111603\n",
      "Epoch 3090, Loss: 0.948713630437851, Final Batch Loss: 0.4267585277557373\n",
      "Epoch 3091, Loss: 0.6559206992387772, Final Batch Loss: 0.13584712147712708\n",
      "Epoch 3092, Loss: 0.6199736893177032, Final Batch Loss: 0.0929211974143982\n",
      "Epoch 3093, Loss: 0.8932361900806427, Final Batch Loss: 0.36291906237602234\n",
      "Epoch 3094, Loss: 0.7979537844657898, Final Batch Loss: 0.2938712537288666\n",
      "Epoch 3095, Loss: 0.603853814303875, Final Batch Loss: 0.06793827563524246\n",
      "Epoch 3096, Loss: 0.7816487103700638, Final Batch Loss: 0.24457554519176483\n",
      "Epoch 3097, Loss: 1.0121148824691772, Final Batch Loss: 0.5290160775184631\n",
      "Epoch 3098, Loss: 0.8510590940713882, Final Batch Loss: 0.3163004219532013\n",
      "Epoch 3099, Loss: 0.5942170545458794, Final Batch Loss: 0.11870966106653214\n",
      "Epoch 3100, Loss: 0.8743433058261871, Final Batch Loss: 0.40841588377952576\n",
      "Epoch 3101, Loss: 0.7003430426120758, Final Batch Loss: 0.14692382514476776\n",
      "Epoch 3102, Loss: 0.644286185503006, Final Batch Loss: 0.12612339854240417\n",
      "Epoch 3103, Loss: 1.1138269007205963, Final Batch Loss: 0.5355334281921387\n",
      "Epoch 3104, Loss: 1.0147518664598465, Final Batch Loss: 0.47994884848594666\n",
      "Epoch 3105, Loss: 0.6627011299133301, Final Batch Loss: 0.10464334487915039\n",
      "Epoch 3106, Loss: 0.6915861815214157, Final Batch Loss: 0.19411751627922058\n",
      "Epoch 3107, Loss: 0.723198652267456, Final Batch Loss: 0.17409616708755493\n",
      "Epoch 3108, Loss: 0.9540096521377563, Final Batch Loss: 0.4201923906803131\n",
      "Epoch 3109, Loss: 0.9292930066585541, Final Batch Loss: 0.44802725315093994\n",
      "Epoch 3110, Loss: 0.8237863183021545, Final Batch Loss: 0.2578083574771881\n",
      "Epoch 3111, Loss: 0.8065442889928818, Final Batch Loss: 0.2862710952758789\n",
      "Epoch 3112, Loss: 0.7131365984678268, Final Batch Loss: 0.13604554533958435\n",
      "Epoch 3113, Loss: 0.8340858072042465, Final Batch Loss: 0.34487637877464294\n",
      "Epoch 3114, Loss: 0.7599182575941086, Final Batch Loss: 0.24766673147678375\n",
      "Epoch 3115, Loss: 0.789768636226654, Final Batch Loss: 0.29621875286102295\n",
      "Epoch 3116, Loss: 0.7290990650653839, Final Batch Loss: 0.1890225112438202\n",
      "Epoch 3117, Loss: 0.7361880391836166, Final Batch Loss: 0.19485950469970703\n",
      "Epoch 3118, Loss: 0.7957184165716171, Final Batch Loss: 0.29698482155799866\n",
      "Epoch 3119, Loss: 0.6063040792942047, Final Batch Loss: 0.13386206328868866\n",
      "Epoch 3120, Loss: 0.8262918442487717, Final Batch Loss: 0.27079012989997864\n",
      "Epoch 3121, Loss: 0.7913123667240143, Final Batch Loss: 0.2892782688140869\n",
      "Epoch 3122, Loss: 1.0354707092046738, Final Batch Loss: 0.5643699765205383\n",
      "Epoch 3123, Loss: 0.9193995743989944, Final Batch Loss: 0.37583401799201965\n",
      "Epoch 3124, Loss: 0.6546249762177467, Final Batch Loss: 0.09321898967027664\n",
      "Epoch 3125, Loss: 0.8312422037124634, Final Batch Loss: 0.2313176691532135\n",
      "Epoch 3126, Loss: 0.8330763578414917, Final Batch Loss: 0.2882092595100403\n",
      "Epoch 3127, Loss: 0.8035954833030701, Final Batch Loss: 0.2525595724582672\n",
      "Epoch 3128, Loss: 0.8144568502902985, Final Batch Loss: 0.32718589901924133\n",
      "Epoch 3129, Loss: 0.7724534422159195, Final Batch Loss: 0.31301552057266235\n",
      "Epoch 3130, Loss: 0.7137229740619659, Final Batch Loss: 0.22127686440944672\n",
      "Epoch 3131, Loss: 0.914595291018486, Final Batch Loss: 0.4367063045501709\n",
      "Epoch 3132, Loss: 0.7621331363916397, Final Batch Loss: 0.3321809470653534\n",
      "Epoch 3133, Loss: 0.9318313151597977, Final Batch Loss: 0.4678354263305664\n",
      "Epoch 3134, Loss: 0.637343019247055, Final Batch Loss: 0.09286776185035706\n",
      "Epoch 3135, Loss: 0.8438863009214401, Final Batch Loss: 0.2912820875644684\n",
      "Epoch 3136, Loss: 0.8730353564023972, Final Batch Loss: 0.384651243686676\n",
      "Epoch 3137, Loss: 0.7364160418510437, Final Batch Loss: 0.18738406896591187\n",
      "Epoch 3138, Loss: 0.979668378829956, Final Batch Loss: 0.4944804906845093\n",
      "Epoch 3139, Loss: 1.107764720916748, Final Batch Loss: 0.5245407223701477\n",
      "Epoch 3140, Loss: 1.1623413264751434, Final Batch Loss: 0.6310378313064575\n",
      "Epoch 3141, Loss: 0.6874175071716309, Final Batch Loss: 0.17462444305419922\n",
      "Epoch 3142, Loss: 0.8277298808097839, Final Batch Loss: 0.26325520873069763\n",
      "Epoch 3143, Loss: 0.9572133421897888, Final Batch Loss: 0.4240202009677887\n",
      "Epoch 3144, Loss: 0.7035212218761444, Final Batch Loss: 0.1604943871498108\n",
      "Epoch 3145, Loss: 0.8487133085727692, Final Batch Loss: 0.28915509581565857\n",
      "Epoch 3146, Loss: 0.6840795204043388, Final Batch Loss: 0.0712377056479454\n",
      "Epoch 3147, Loss: 0.6540544927120209, Final Batch Loss: 0.1230589747428894\n",
      "Epoch 3148, Loss: 0.9056102186441422, Final Batch Loss: 0.40417158603668213\n",
      "Epoch 3149, Loss: 0.6580778658390045, Final Batch Loss: 0.13537001609802246\n",
      "Epoch 3150, Loss: 0.7811707258224487, Final Batch Loss: 0.23165366053581238\n",
      "Epoch 3151, Loss: 0.8810080289840698, Final Batch Loss: 0.30311083793640137\n",
      "Epoch 3152, Loss: 0.7425823956727982, Final Batch Loss: 0.2714957594871521\n",
      "Epoch 3153, Loss: 0.846358984708786, Final Batch Loss: 0.2587392330169678\n",
      "Epoch 3154, Loss: 0.663304477930069, Final Batch Loss: 0.1380806565284729\n",
      "Epoch 3155, Loss: 0.9461757242679596, Final Batch Loss: 0.4479053318500519\n",
      "Epoch 3156, Loss: 0.7427733391523361, Final Batch Loss: 0.24482929706573486\n",
      "Epoch 3157, Loss: 0.8612908273935318, Final Batch Loss: 0.21910543739795685\n",
      "Epoch 3158, Loss: 0.80476213991642, Final Batch Loss: 0.29909756779670715\n",
      "Epoch 3159, Loss: 1.0452215075492859, Final Batch Loss: 0.4691753387451172\n",
      "Epoch 3160, Loss: 0.8644269108772278, Final Batch Loss: 0.2743130624294281\n",
      "Epoch 3161, Loss: 1.022230252623558, Final Batch Loss: 0.4980509281158447\n",
      "Epoch 3162, Loss: 0.8088738769292831, Final Batch Loss: 0.29253140091896057\n",
      "Epoch 3163, Loss: 1.1550522148609161, Final Batch Loss: 0.6756039261817932\n",
      "Epoch 3164, Loss: 0.9289950430393219, Final Batch Loss: 0.41305723786354065\n",
      "Epoch 3165, Loss: 0.6154088824987411, Final Batch Loss: 0.09702928364276886\n",
      "Epoch 3166, Loss: 0.65015809237957, Final Batch Loss: 0.19107015430927277\n",
      "Epoch 3167, Loss: 0.7088440805673599, Final Batch Loss: 0.1485857218503952\n",
      "Epoch 3168, Loss: 0.7788346111774445, Final Batch Loss: 0.23486188054084778\n",
      "Epoch 3169, Loss: 0.8352421373128891, Final Batch Loss: 0.3046480119228363\n",
      "Epoch 3170, Loss: 0.7074621617794037, Final Batch Loss: 0.1918102502822876\n",
      "Epoch 3171, Loss: 0.7662083655595779, Final Batch Loss: 0.22588293254375458\n",
      "Epoch 3172, Loss: 0.7373882532119751, Final Batch Loss: 0.1907581388950348\n",
      "Epoch 3173, Loss: 0.7698726058006287, Final Batch Loss: 0.26781004667282104\n",
      "Epoch 3174, Loss: 0.6795075684785843, Final Batch Loss: 0.17724590003490448\n",
      "Epoch 3175, Loss: 0.6654687374830246, Final Batch Loss: 0.15919923782348633\n",
      "Epoch 3176, Loss: 0.882265031337738, Final Batch Loss: 0.3253363072872162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3177, Loss: 0.7319049835205078, Final Batch Loss: 0.23427267372608185\n",
      "Epoch 3178, Loss: 0.6392709463834763, Final Batch Loss: 0.09469030797481537\n",
      "Epoch 3179, Loss: 0.6834024041891098, Final Batch Loss: 0.1962609738111496\n",
      "Epoch 3180, Loss: 0.9016111791133881, Final Batch Loss: 0.37527787685394287\n",
      "Epoch 3181, Loss: 0.7301263958215714, Final Batch Loss: 0.19668127596378326\n",
      "Epoch 3182, Loss: 0.6003816947340965, Final Batch Loss: 0.08025433868169785\n",
      "Epoch 3183, Loss: 0.7605661004781723, Final Batch Loss: 0.2151780128479004\n",
      "Epoch 3184, Loss: 1.1863479614257812, Final Batch Loss: 0.5228530764579773\n",
      "Epoch 3185, Loss: 0.6960562467575073, Final Batch Loss: 0.18054234981536865\n",
      "Epoch 3186, Loss: 0.7748894095420837, Final Batch Loss: 0.2209843099117279\n",
      "Epoch 3187, Loss: 0.9999923408031464, Final Batch Loss: 0.48609742522239685\n",
      "Epoch 3188, Loss: 0.7719481140375137, Final Batch Loss: 0.17113400995731354\n",
      "Epoch 3189, Loss: 0.7109338045120239, Final Batch Loss: 0.20037555694580078\n",
      "Epoch 3190, Loss: 0.7612140625715256, Final Batch Loss: 0.1797686666250229\n",
      "Epoch 3191, Loss: 1.1616705060005188, Final Batch Loss: 0.5636255145072937\n",
      "Epoch 3192, Loss: 0.6207803972065449, Final Batch Loss: 0.037679124623537064\n",
      "Epoch 3193, Loss: 0.9579833745956421, Final Batch Loss: 0.4168666899204254\n",
      "Epoch 3194, Loss: 0.7199640274047852, Final Batch Loss: 0.16222740709781647\n",
      "Epoch 3195, Loss: 1.042521357536316, Final Batch Loss: 0.5167579650878906\n",
      "Epoch 3196, Loss: 0.6866279393434525, Final Batch Loss: 0.18641330301761627\n",
      "Epoch 3197, Loss: 0.9068581759929657, Final Batch Loss: 0.3503704369068146\n",
      "Epoch 3198, Loss: 0.9284018278121948, Final Batch Loss: 0.40806248784065247\n",
      "Epoch 3199, Loss: 0.9182766377925873, Final Batch Loss: 0.35106730461120605\n",
      "Epoch 3200, Loss: 0.5787213481962681, Final Batch Loss: 0.059830013662576675\n",
      "Epoch 3201, Loss: 0.709210991859436, Final Batch Loss: 0.22503581643104553\n",
      "Epoch 3202, Loss: 0.7559110075235367, Final Batch Loss: 0.2596346139907837\n",
      "Epoch 3203, Loss: 0.6729933470487595, Final Batch Loss: 0.1428213268518448\n",
      "Epoch 3204, Loss: 0.9892641603946686, Final Batch Loss: 0.42928415536880493\n",
      "Epoch 3205, Loss: 0.9235163927078247, Final Batch Loss: 0.35009223222732544\n",
      "Epoch 3206, Loss: 0.9255406260490417, Final Batch Loss: 0.41240033507347107\n",
      "Epoch 3207, Loss: 0.6726475283503532, Final Batch Loss: 0.10250376909971237\n",
      "Epoch 3208, Loss: 0.9322824478149414, Final Batch Loss: 0.3253738582134247\n",
      "Epoch 3209, Loss: 0.6813947707414627, Final Batch Loss: 0.15939120948314667\n",
      "Epoch 3210, Loss: 0.8755453824996948, Final Batch Loss: 0.31491193175315857\n",
      "Epoch 3211, Loss: 0.7830047160387039, Final Batch Loss: 0.23631657660007477\n",
      "Epoch 3212, Loss: 0.9675078690052032, Final Batch Loss: 0.48175400495529175\n",
      "Epoch 3213, Loss: 0.751363068819046, Final Batch Loss: 0.28170037269592285\n",
      "Epoch 3214, Loss: 0.8210244923830032, Final Batch Loss: 0.3262544572353363\n",
      "Epoch 3215, Loss: 0.6112556755542755, Final Batch Loss: 0.13595251739025116\n",
      "Epoch 3216, Loss: 1.0623089224100113, Final Batch Loss: 0.5514667630195618\n",
      "Epoch 3217, Loss: 0.8110422790050507, Final Batch Loss: 0.27129513025283813\n",
      "Epoch 3218, Loss: 0.7123811841011047, Final Batch Loss: 0.22074632346630096\n",
      "Epoch 3219, Loss: 0.9069503545761108, Final Batch Loss: 0.31948474049568176\n",
      "Epoch 3220, Loss: 0.8391092717647552, Final Batch Loss: 0.28337615728378296\n",
      "Epoch 3221, Loss: 0.8271149694919586, Final Batch Loss: 0.2814619839191437\n",
      "Epoch 3222, Loss: 0.6852037310600281, Final Batch Loss: 0.16009224951267242\n",
      "Epoch 3223, Loss: 0.8623300492763519, Final Batch Loss: 0.27311640977859497\n",
      "Epoch 3224, Loss: 0.7702544182538986, Final Batch Loss: 0.17927534878253937\n",
      "Epoch 3225, Loss: 0.7395387440919876, Final Batch Loss: 0.2589428424835205\n",
      "Epoch 3226, Loss: 0.7369480431079865, Final Batch Loss: 0.1971612274646759\n",
      "Epoch 3227, Loss: 0.9341522455215454, Final Batch Loss: 0.37979352474212646\n",
      "Epoch 3228, Loss: 0.7617432773113251, Final Batch Loss: 0.1749344766139984\n",
      "Epoch 3229, Loss: 0.7780480533838272, Final Batch Loss: 0.24005042016506195\n",
      "Epoch 3230, Loss: 0.7253594696521759, Final Batch Loss: 0.1875927746295929\n",
      "Epoch 3231, Loss: 0.6087166666984558, Final Batch Loss: 0.08398982882499695\n",
      "Epoch 3232, Loss: 0.6755193620920181, Final Batch Loss: 0.20594187080860138\n",
      "Epoch 3233, Loss: 0.985176756978035, Final Batch Loss: 0.4443104863166809\n",
      "Epoch 3234, Loss: 0.6047683507204056, Final Batch Loss: 0.0944925844669342\n",
      "Epoch 3235, Loss: 0.9113262295722961, Final Batch Loss: 0.4125484526157379\n",
      "Epoch 3236, Loss: 0.8997104167938232, Final Batch Loss: 0.3191048800945282\n",
      "Epoch 3237, Loss: 0.7043413519859314, Final Batch Loss: 0.2115854024887085\n",
      "Epoch 3238, Loss: 0.5471852198243141, Final Batch Loss: 0.044744886457920074\n",
      "Epoch 3239, Loss: 0.9186983406543732, Final Batch Loss: 0.40804219245910645\n",
      "Epoch 3240, Loss: 0.7205613255500793, Final Batch Loss: 0.2215055376291275\n",
      "Epoch 3241, Loss: 0.7955043613910675, Final Batch Loss: 0.31255990266799927\n",
      "Epoch 3242, Loss: 0.8944582939147949, Final Batch Loss: 0.34429168701171875\n",
      "Epoch 3243, Loss: 0.7340727299451828, Final Batch Loss: 0.2788451313972473\n",
      "Epoch 3244, Loss: 0.6594374626874924, Final Batch Loss: 0.16713950037956238\n",
      "Epoch 3245, Loss: 1.040180116891861, Final Batch Loss: 0.537487268447876\n",
      "Epoch 3246, Loss: 0.7369538396596909, Final Batch Loss: 0.1912640482187271\n",
      "Epoch 3247, Loss: 0.719828873872757, Final Batch Loss: 0.23817792534828186\n",
      "Epoch 3248, Loss: 0.5879981070756912, Final Batch Loss: 0.09264318645000458\n",
      "Epoch 3249, Loss: 0.9218193590641022, Final Batch Loss: 0.3989228308200836\n",
      "Epoch 3250, Loss: 0.7859409153461456, Final Batch Loss: 0.2837579846382141\n",
      "Epoch 3251, Loss: 0.6435743868350983, Final Batch Loss: 0.2156791388988495\n",
      "Epoch 3252, Loss: 0.6649633795022964, Final Batch Loss: 0.2021743804216385\n",
      "Epoch 3253, Loss: 0.6732476055622101, Final Batch Loss: 0.13329842686653137\n",
      "Epoch 3254, Loss: 0.5934282951056957, Final Batch Loss: 0.047471415251493454\n",
      "Epoch 3255, Loss: 0.8039731085300446, Final Batch Loss: 0.30983319878578186\n",
      "Epoch 3256, Loss: 0.668052151799202, Final Batch Loss: 0.16950945556163788\n",
      "Epoch 3257, Loss: 0.9137569665908813, Final Batch Loss: 0.32682666182518005\n",
      "Epoch 3258, Loss: 0.9412392675876617, Final Batch Loss: 0.446094810962677\n",
      "Epoch 3259, Loss: 0.7538749575614929, Final Batch Loss: 0.27122369408607483\n",
      "Epoch 3260, Loss: 0.8831076323986053, Final Batch Loss: 0.3181569576263428\n",
      "Epoch 3261, Loss: 0.6911556571722031, Final Batch Loss: 0.14670050144195557\n",
      "Epoch 3262, Loss: 1.0933886766433716, Final Batch Loss: 0.4998236298561096\n",
      "Epoch 3263, Loss: 1.0966213047504425, Final Batch Loss: 0.5279280543327332\n",
      "Epoch 3264, Loss: 0.6532327979803085, Final Batch Loss: 0.12648387253284454\n",
      "Epoch 3265, Loss: 0.8376378118991852, Final Batch Loss: 0.3456716239452362\n",
      "Epoch 3266, Loss: 0.9237977266311646, Final Batch Loss: 0.3950325548648834\n",
      "Epoch 3267, Loss: 0.8425301611423492, Final Batch Loss: 0.38979682326316833\n",
      "Epoch 3268, Loss: 0.6991505771875381, Final Batch Loss: 0.13622336089611053\n",
      "Epoch 3269, Loss: 0.8222255408763885, Final Batch Loss: 0.27073410153388977\n",
      "Epoch 3270, Loss: 0.7817578166723251, Final Batch Loss: 0.26279202103614807\n",
      "Epoch 3271, Loss: 0.6441726014018059, Final Batch Loss: 0.11535253375768661\n",
      "Epoch 3272, Loss: 0.648212119936943, Final Batch Loss: 0.1272403746843338\n",
      "Epoch 3273, Loss: 0.656402587890625, Final Batch Loss: 0.1472684144973755\n",
      "Epoch 3274, Loss: 0.6358237266540527, Final Batch Loss: 0.13859127461910248\n",
      "Epoch 3275, Loss: 0.7471300959587097, Final Batch Loss: 0.24928350746631622\n",
      "Epoch 3276, Loss: 0.6893302202224731, Final Batch Loss: 0.18363744020462036\n",
      "Epoch 3277, Loss: 0.7640441805124283, Final Batch Loss: 0.3047125041484833\n",
      "Epoch 3278, Loss: 0.7597948908805847, Final Batch Loss: 0.25494229793548584\n",
      "Epoch 3279, Loss: 0.5559485778212547, Final Batch Loss: 0.0987681970000267\n",
      "Epoch 3280, Loss: 0.6776682883501053, Final Batch Loss: 0.1997673660516739\n",
      "Epoch 3281, Loss: 0.6777366548776627, Final Batch Loss: 0.17157971858978271\n",
      "Epoch 3282, Loss: 0.6944196075201035, Final Batch Loss: 0.16580627858638763\n",
      "Epoch 3283, Loss: 0.6955128610134125, Final Batch Loss: 0.16559553146362305\n",
      "Epoch 3284, Loss: 0.8903491348028183, Final Batch Loss: 0.3394555449485779\n",
      "Epoch 3285, Loss: 0.863720491528511, Final Batch Loss: 0.40391406416893005\n",
      "Epoch 3286, Loss: 1.0465373992919922, Final Batch Loss: 0.4872249662876129\n",
      "Epoch 3287, Loss: 0.8076192289590836, Final Batch Loss: 0.2761428952217102\n",
      "Epoch 3288, Loss: 0.6817820370197296, Final Batch Loss: 0.15573370456695557\n",
      "Epoch 3289, Loss: 0.8185102343559265, Final Batch Loss: 0.24984455108642578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3290, Loss: 0.8878923952579498, Final Batch Loss: 0.35783565044403076\n",
      "Epoch 3291, Loss: 0.8664964437484741, Final Batch Loss: 0.2962164878845215\n",
      "Epoch 3292, Loss: 0.9827928245067596, Final Batch Loss: 0.4372592270374298\n",
      "Epoch 3293, Loss: 0.8848735392093658, Final Batch Loss: 0.3505192995071411\n",
      "Epoch 3294, Loss: 0.7757997959852219, Final Batch Loss: 0.14820213615894318\n",
      "Epoch 3295, Loss: 0.8817481994628906, Final Batch Loss: 0.28171420097351074\n",
      "Epoch 3296, Loss: 0.7847360968589783, Final Batch Loss: 0.27537021040916443\n",
      "Epoch 3297, Loss: 0.8463933020830154, Final Batch Loss: 0.3186052143573761\n",
      "Epoch 3298, Loss: 0.7798597663640976, Final Batch Loss: 0.20850487053394318\n",
      "Epoch 3299, Loss: 1.0512772798538208, Final Batch Loss: 0.4234544336795807\n",
      "Epoch 3300, Loss: 1.0380082428455353, Final Batch Loss: 0.4794350564479828\n",
      "Epoch 3301, Loss: 0.9044687151908875, Final Batch Loss: 0.3708811104297638\n",
      "Epoch 3302, Loss: 0.7701655477285385, Final Batch Loss: 0.29093989729881287\n",
      "Epoch 3303, Loss: 0.7588173002004623, Final Batch Loss: 0.19109822809696198\n",
      "Epoch 3304, Loss: 0.951391339302063, Final Batch Loss: 0.43781059980392456\n",
      "Epoch 3305, Loss: 0.5929474383592606, Final Batch Loss: 0.06570054590702057\n",
      "Epoch 3306, Loss: 0.7848132401704788, Final Batch Loss: 0.21590030193328857\n",
      "Epoch 3307, Loss: 0.9108277857303619, Final Batch Loss: 0.39408788084983826\n",
      "Epoch 3308, Loss: 0.9644650220870972, Final Batch Loss: 0.5133329033851624\n",
      "Epoch 3309, Loss: 1.1048288345336914, Final Batch Loss: 0.5932023525238037\n",
      "Epoch 3310, Loss: 0.7413136661052704, Final Batch Loss: 0.17258203029632568\n",
      "Epoch 3311, Loss: 0.6560539901256561, Final Batch Loss: 0.16728611290454865\n",
      "Epoch 3312, Loss: 0.7937840521335602, Final Batch Loss: 0.2545311450958252\n",
      "Epoch 3313, Loss: 0.6077978983521461, Final Batch Loss: 0.09850362688302994\n",
      "Epoch 3314, Loss: 0.8788802325725555, Final Batch Loss: 0.3232000172138214\n",
      "Epoch 3315, Loss: 0.789165586233139, Final Batch Loss: 0.24724039435386658\n",
      "Epoch 3316, Loss: 0.8113102465867996, Final Batch Loss: 0.31084778904914856\n",
      "Epoch 3317, Loss: 0.6219197511672974, Final Batch Loss: 0.10115018486976624\n",
      "Epoch 3318, Loss: 0.8106283098459244, Final Batch Loss: 0.18039311468601227\n",
      "Epoch 3319, Loss: 0.683998316526413, Final Batch Loss: 0.1751689612865448\n",
      "Epoch 3320, Loss: 0.9117919653654099, Final Batch Loss: 0.4284617304801941\n",
      "Epoch 3321, Loss: 0.8444314897060394, Final Batch Loss: 0.31798145174980164\n",
      "Epoch 3322, Loss: 0.6808480024337769, Final Batch Loss: 0.1456274390220642\n",
      "Epoch 3323, Loss: 0.9813266396522522, Final Batch Loss: 0.44544389843940735\n",
      "Epoch 3324, Loss: 0.5523737072944641, Final Batch Loss: 0.10321110486984253\n",
      "Epoch 3325, Loss: 1.0464503467082977, Final Batch Loss: 0.41560426354408264\n",
      "Epoch 3326, Loss: 0.9309449791908264, Final Batch Loss: 0.2957098186016083\n",
      "Epoch 3327, Loss: 0.7171220183372498, Final Batch Loss: 0.1449120044708252\n",
      "Epoch 3328, Loss: 0.7722181528806686, Final Batch Loss: 0.22297929227352142\n",
      "Epoch 3329, Loss: 0.6057382673025131, Final Batch Loss: 0.13058657944202423\n",
      "Epoch 3330, Loss: 0.7189149111509323, Final Batch Loss: 0.20778365433216095\n",
      "Epoch 3331, Loss: 0.8802588880062103, Final Batch Loss: 0.3608208894729614\n",
      "Epoch 3332, Loss: 0.9007797837257385, Final Batch Loss: 0.37573298811912537\n",
      "Epoch 3333, Loss: 0.7239465117454529, Final Batch Loss: 0.22069740295410156\n",
      "Epoch 3334, Loss: 0.5881141852587461, Final Batch Loss: 0.02925117127597332\n",
      "Epoch 3335, Loss: 0.6051803454756737, Final Batch Loss: 0.06457389146089554\n",
      "Epoch 3336, Loss: 0.6476320773363113, Final Batch Loss: 0.18466316163539886\n",
      "Epoch 3337, Loss: 1.0239053964614868, Final Batch Loss: 0.5311223864555359\n",
      "Epoch 3338, Loss: 0.7929239124059677, Final Batch Loss: 0.2425529807806015\n",
      "Epoch 3339, Loss: 0.6769963949918747, Final Batch Loss: 0.12923848628997803\n",
      "Epoch 3340, Loss: 0.7305375784635544, Final Batch Loss: 0.19325309991836548\n",
      "Epoch 3341, Loss: 0.6314619481563568, Final Batch Loss: 0.14208173751831055\n",
      "Epoch 3342, Loss: 0.6680378168821335, Final Batch Loss: 0.16073288023471832\n",
      "Epoch 3343, Loss: 0.9115976691246033, Final Batch Loss: 0.38562536239624023\n",
      "Epoch 3344, Loss: 0.814360961318016, Final Batch Loss: 0.33947524428367615\n",
      "Epoch 3345, Loss: 0.8291458189487457, Final Batch Loss: 0.3751479983329773\n",
      "Epoch 3346, Loss: 0.8861400783061981, Final Batch Loss: 0.36788225173950195\n",
      "Epoch 3347, Loss: 0.6970562040805817, Final Batch Loss: 0.22512029111385345\n",
      "Epoch 3348, Loss: 0.5451547838747501, Final Batch Loss: 0.039769578725099564\n",
      "Epoch 3349, Loss: 0.7902838438749313, Final Batch Loss: 0.30375537276268005\n",
      "Epoch 3350, Loss: 0.6403336375951767, Final Batch Loss: 0.14912356436252594\n",
      "Epoch 3351, Loss: 0.8783250600099564, Final Batch Loss: 0.3645975589752197\n",
      "Epoch 3352, Loss: 0.8399228453636169, Final Batch Loss: 0.26111480593681335\n",
      "Epoch 3353, Loss: 0.6542415469884872, Final Batch Loss: 0.12665991485118866\n",
      "Epoch 3354, Loss: 0.8382626473903656, Final Batch Loss: 0.4113251864910126\n",
      "Epoch 3355, Loss: 0.8213950544595718, Final Batch Loss: 0.28701820969581604\n",
      "Epoch 3356, Loss: 0.771414265036583, Final Batch Loss: 0.2933126986026764\n",
      "Epoch 3357, Loss: 0.7427040934562683, Final Batch Loss: 0.2618008553981781\n",
      "Epoch 3358, Loss: 0.9191395789384842, Final Batch Loss: 0.43566638231277466\n",
      "Epoch 3359, Loss: 0.7983283698558807, Final Batch Loss: 0.34241756796836853\n",
      "Epoch 3360, Loss: 0.7001601159572601, Final Batch Loss: 0.20252111554145813\n",
      "Epoch 3361, Loss: 0.7071200460195541, Final Batch Loss: 0.21384957432746887\n",
      "Epoch 3362, Loss: 0.8214758038520813, Final Batch Loss: 0.30343392491340637\n",
      "Epoch 3363, Loss: 0.7976647466421127, Final Batch Loss: 0.26869580149650574\n",
      "Epoch 3364, Loss: 0.8502413183450699, Final Batch Loss: 0.35407865047454834\n",
      "Epoch 3365, Loss: 0.6328438520431519, Final Batch Loss: 0.1349472999572754\n",
      "Epoch 3366, Loss: 0.9042193442583084, Final Batch Loss: 0.46999430656433105\n",
      "Epoch 3367, Loss: 0.842978224158287, Final Batch Loss: 0.3652893006801605\n",
      "Epoch 3368, Loss: 0.7058280259370804, Final Batch Loss: 0.18254925310611725\n",
      "Epoch 3369, Loss: 0.7503825277090073, Final Batch Loss: 0.2916647791862488\n",
      "Epoch 3370, Loss: 0.6493208408355713, Final Batch Loss: 0.14231976866722107\n",
      "Epoch 3371, Loss: 0.6563450545072556, Final Batch Loss: 0.19087201356887817\n",
      "Epoch 3372, Loss: 0.8851766884326935, Final Batch Loss: 0.31211796402931213\n",
      "Epoch 3373, Loss: 0.8779296725988388, Final Batch Loss: 0.35888323187828064\n",
      "Epoch 3374, Loss: 0.6600110530853271, Final Batch Loss: 0.1960623413324356\n",
      "Epoch 3375, Loss: 0.748829111456871, Final Batch Loss: 0.25894656777381897\n",
      "Epoch 3376, Loss: 0.6160133630037308, Final Batch Loss: 0.10349543392658234\n",
      "Epoch 3377, Loss: 0.7864310294389725, Final Batch Loss: 0.24828855693340302\n",
      "Epoch 3378, Loss: 0.6742547452449799, Final Batch Loss: 0.19283367693424225\n",
      "Epoch 3379, Loss: 0.7163288593292236, Final Batch Loss: 0.2150920331478119\n",
      "Epoch 3380, Loss: 0.7491814941167831, Final Batch Loss: 0.21979178488254547\n",
      "Epoch 3381, Loss: 0.7044198364019394, Final Batch Loss: 0.2335309535264969\n",
      "Epoch 3382, Loss: 0.6880141198635101, Final Batch Loss: 0.16509167850017548\n",
      "Epoch 3383, Loss: 0.6778903901576996, Final Batch Loss: 0.1996440291404724\n",
      "Epoch 3384, Loss: 0.6872597634792328, Final Batch Loss: 0.16584864258766174\n",
      "Epoch 3385, Loss: 0.6178322061896324, Final Batch Loss: 0.10630645602941513\n",
      "Epoch 3386, Loss: 0.7373556941747665, Final Batch Loss: 0.2528141736984253\n",
      "Epoch 3387, Loss: 0.7148455679416656, Final Batch Loss: 0.20768067240715027\n",
      "Epoch 3388, Loss: 0.9284656643867493, Final Batch Loss: 0.4392486810684204\n",
      "Epoch 3389, Loss: 0.5359343737363815, Final Batch Loss: 0.039776310324668884\n",
      "Epoch 3390, Loss: 0.7056296467781067, Final Batch Loss: 0.15596330165863037\n",
      "Epoch 3391, Loss: 0.697415828704834, Final Batch Loss: 0.19019711017608643\n",
      "Epoch 3392, Loss: 0.7423934936523438, Final Batch Loss: 0.23906289041042328\n",
      "Epoch 3393, Loss: 1.1666553020477295, Final Batch Loss: 0.6983227729797363\n",
      "Epoch 3394, Loss: 0.7606411725282669, Final Batch Loss: 0.23204036056995392\n",
      "Epoch 3395, Loss: 0.7661597430706024, Final Batch Loss: 0.308283269405365\n",
      "Epoch 3396, Loss: 0.7750645577907562, Final Batch Loss: 0.28627628087997437\n",
      "Epoch 3397, Loss: 0.6325435191392899, Final Batch Loss: 0.1161491870880127\n",
      "Epoch 3398, Loss: 0.7914020121097565, Final Batch Loss: 0.30583298206329346\n",
      "Epoch 3399, Loss: 0.6112246513366699, Final Batch Loss: 0.19864769279956818\n",
      "Epoch 3400, Loss: 0.8604491800069809, Final Batch Loss: 0.42255812883377075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3401, Loss: 0.7305516600608826, Final Batch Loss: 0.24094219505786896\n",
      "Epoch 3402, Loss: 0.8716254234313965, Final Batch Loss: 0.38255947828292847\n",
      "Epoch 3403, Loss: 0.8406840711832047, Final Batch Loss: 0.3336879312992096\n",
      "Epoch 3404, Loss: 0.8857594430446625, Final Batch Loss: 0.3916034400463104\n",
      "Epoch 3405, Loss: 0.8800168037414551, Final Batch Loss: 0.410142183303833\n",
      "Epoch 3406, Loss: 0.7102246284484863, Final Batch Loss: 0.21058343350887299\n",
      "Epoch 3407, Loss: 0.7103155255317688, Final Batch Loss: 0.20099954307079315\n",
      "Epoch 3408, Loss: 0.6404886320233345, Final Batch Loss: 0.030745841562747955\n",
      "Epoch 3409, Loss: 0.7727188467979431, Final Batch Loss: 0.18819014728069305\n",
      "Epoch 3410, Loss: 0.8326431512832642, Final Batch Loss: 0.32724979519844055\n",
      "Epoch 3411, Loss: 0.6620810627937317, Final Batch Loss: 0.1745223104953766\n",
      "Epoch 3412, Loss: 0.6688921004533768, Final Batch Loss: 0.13614626228809357\n",
      "Epoch 3413, Loss: 0.889753669500351, Final Batch Loss: 0.4018881916999817\n",
      "Epoch 3414, Loss: 0.6205137148499489, Final Batch Loss: 0.06532341986894608\n",
      "Epoch 3415, Loss: 0.8494882434606552, Final Batch Loss: 0.3737831711769104\n",
      "Epoch 3416, Loss: 0.7725676447153091, Final Batch Loss: 0.2874767780303955\n",
      "Epoch 3417, Loss: 0.7833863496780396, Final Batch Loss: 0.2794758677482605\n",
      "Epoch 3418, Loss: 0.7915671020746231, Final Batch Loss: 0.30019354820251465\n",
      "Epoch 3419, Loss: 0.796292319893837, Final Batch Loss: 0.30911436676979065\n",
      "Epoch 3420, Loss: 0.5817616507411003, Final Batch Loss: 0.09092787653207779\n",
      "Epoch 3421, Loss: 0.9475264996290207, Final Batch Loss: 0.4757411479949951\n",
      "Epoch 3422, Loss: 0.8841732442378998, Final Batch Loss: 0.3879075348377228\n",
      "Epoch 3423, Loss: 0.5948697403073311, Final Batch Loss: 0.11846456676721573\n",
      "Epoch 3424, Loss: 0.6505648493766785, Final Batch Loss: 0.1352042704820633\n",
      "Epoch 3425, Loss: 0.996734231710434, Final Batch Loss: 0.5738657712936401\n",
      "Epoch 3426, Loss: 0.6396589130163193, Final Batch Loss: 0.20149274170398712\n",
      "Epoch 3427, Loss: 0.748118668794632, Final Batch Loss: 0.3073709011077881\n",
      "Epoch 3428, Loss: 0.8219587653875351, Final Batch Loss: 0.35306552052497864\n",
      "Epoch 3429, Loss: 0.7418948113918304, Final Batch Loss: 0.29228317737579346\n",
      "Epoch 3430, Loss: 0.8646724820137024, Final Batch Loss: 0.3501449525356293\n",
      "Epoch 3431, Loss: 0.8742657601833344, Final Batch Loss: 0.44711950421333313\n",
      "Epoch 3432, Loss: 0.7035831660032272, Final Batch Loss: 0.23592038452625275\n",
      "Epoch 3433, Loss: 0.772039607167244, Final Batch Loss: 0.26319050788879395\n",
      "Epoch 3434, Loss: 0.5997467190027237, Final Batch Loss: 0.1691807508468628\n",
      "Epoch 3435, Loss: 0.8278404027223587, Final Batch Loss: 0.3441452085971832\n",
      "Epoch 3436, Loss: 0.6820297539234161, Final Batch Loss: 0.20688705146312714\n",
      "Epoch 3437, Loss: 0.7778986096382141, Final Batch Loss: 0.272516667842865\n",
      "Epoch 3438, Loss: 0.8599816560745239, Final Batch Loss: 0.3430221974849701\n",
      "Epoch 3439, Loss: 0.635285884141922, Final Batch Loss: 0.19674940407276154\n",
      "Epoch 3440, Loss: 0.560367152094841, Final Batch Loss: 0.08010907471179962\n",
      "Epoch 3441, Loss: 0.8280397206544876, Final Batch Loss: 0.34690821170806885\n",
      "Epoch 3442, Loss: 0.7184733599424362, Final Batch Loss: 0.2574802339076996\n",
      "Epoch 3443, Loss: 0.5985232517123222, Final Batch Loss: 0.07140650600194931\n",
      "Epoch 3444, Loss: 0.7287065386772156, Final Batch Loss: 0.23077231645584106\n",
      "Epoch 3445, Loss: 0.7405738830566406, Final Batch Loss: 0.3051973879337311\n",
      "Epoch 3446, Loss: 0.678125336766243, Final Batch Loss: 0.17969566583633423\n",
      "Epoch 3447, Loss: 0.8840891718864441, Final Batch Loss: 0.3933943510055542\n",
      "Epoch 3448, Loss: 0.7322765439748764, Final Batch Loss: 0.2591732144355774\n",
      "Epoch 3449, Loss: 0.7077370434999466, Final Batch Loss: 0.21224942803382874\n",
      "Epoch 3450, Loss: 0.7124883830547333, Final Batch Loss: 0.2757764458656311\n",
      "Epoch 3451, Loss: 0.6694521903991699, Final Batch Loss: 0.2105705291032791\n",
      "Epoch 3452, Loss: 0.7926615923643112, Final Batch Loss: 0.31121015548706055\n",
      "Epoch 3453, Loss: 0.8376370668411255, Final Batch Loss: 0.3851495087146759\n",
      "Epoch 3454, Loss: 0.711598813533783, Final Batch Loss: 0.2200552076101303\n",
      "Epoch 3455, Loss: 0.7489098906517029, Final Batch Loss: 0.2936548590660095\n",
      "Epoch 3456, Loss: 0.7691667377948761, Final Batch Loss: 0.2782166302204132\n",
      "Epoch 3457, Loss: 0.8354443162679672, Final Batch Loss: 0.29192328453063965\n",
      "Epoch 3458, Loss: 0.654022291302681, Final Batch Loss: 0.1573389768600464\n",
      "Epoch 3459, Loss: 0.8235568851232529, Final Batch Loss: 0.33131149411201477\n",
      "Epoch 3460, Loss: 0.6984921842813492, Final Batch Loss: 0.16268233954906464\n",
      "Epoch 3461, Loss: 1.0609550178050995, Final Batch Loss: 0.48958051204681396\n",
      "Epoch 3462, Loss: 0.7783297896385193, Final Batch Loss: 0.20979097485542297\n",
      "Epoch 3463, Loss: 0.7057113796472549, Final Batch Loss: 0.12805549800395966\n",
      "Epoch 3464, Loss: 0.8388729691505432, Final Batch Loss: 0.2555074989795685\n",
      "Epoch 3465, Loss: 0.9503570646047592, Final Batch Loss: 0.38644298911094666\n",
      "Epoch 3466, Loss: 0.9849551618099213, Final Batch Loss: 0.38148778676986694\n",
      "Epoch 3467, Loss: 0.9362251162528992, Final Batch Loss: 0.3566015660762787\n",
      "Epoch 3468, Loss: 0.6373263671994209, Final Batch Loss: 0.09636559337377548\n",
      "Epoch 3469, Loss: 0.7235975712537766, Final Batch Loss: 0.19783847033977509\n",
      "Epoch 3470, Loss: 0.5214478150010109, Final Batch Loss: 0.0827498510479927\n",
      "Epoch 3471, Loss: 0.5797743201255798, Final Batch Loss: 0.05801853537559509\n",
      "Epoch 3472, Loss: 0.6274394541978836, Final Batch Loss: 0.13164564967155457\n",
      "Epoch 3473, Loss: 0.6480377912521362, Final Batch Loss: 0.20920881628990173\n",
      "Epoch 3474, Loss: 0.8388586640357971, Final Batch Loss: 0.3089214563369751\n",
      "Epoch 3475, Loss: 0.7486806362867355, Final Batch Loss: 0.28037282824516296\n",
      "Epoch 3476, Loss: 0.7123071253299713, Final Batch Loss: 0.25834888219833374\n",
      "Epoch 3477, Loss: 0.6562031581997871, Final Batch Loss: 0.12094659358263016\n",
      "Epoch 3478, Loss: 0.7100136876106262, Final Batch Loss: 0.2002606838941574\n",
      "Epoch 3479, Loss: 0.7217198461294174, Final Batch Loss: 0.2496025413274765\n",
      "Epoch 3480, Loss: 0.5077723786234856, Final Batch Loss: 0.057241834700107574\n",
      "Epoch 3481, Loss: 0.8616516441106796, Final Batch Loss: 0.3698213994503021\n",
      "Epoch 3482, Loss: 1.0551108568906784, Final Batch Loss: 0.5767552852630615\n",
      "Epoch 3483, Loss: 0.7623536586761475, Final Batch Loss: 0.31054314970970154\n",
      "Epoch 3484, Loss: 0.889045313000679, Final Batch Loss: 0.4250141382217407\n",
      "Epoch 3485, Loss: 0.7945984303951263, Final Batch Loss: 0.29892203211784363\n",
      "Epoch 3486, Loss: 0.8611626029014587, Final Batch Loss: 0.38415712118148804\n",
      "Epoch 3487, Loss: 0.6068411841988564, Final Batch Loss: 0.1176728829741478\n",
      "Epoch 3488, Loss: 0.7412675470113754, Final Batch Loss: 0.24436025321483612\n",
      "Epoch 3489, Loss: 0.7161671072244644, Final Batch Loss: 0.2646496295928955\n",
      "Epoch 3490, Loss: 0.5688867047429085, Final Batch Loss: 0.11605217307806015\n",
      "Epoch 3491, Loss: 0.825321838259697, Final Batch Loss: 0.26499491930007935\n",
      "Epoch 3492, Loss: 0.8358524590730667, Final Batch Loss: 0.3789951801300049\n",
      "Epoch 3493, Loss: 0.7484442591667175, Final Batch Loss: 0.20178329944610596\n",
      "Epoch 3494, Loss: 0.8255386650562286, Final Batch Loss: 0.32630884647369385\n",
      "Epoch 3495, Loss: 0.673318088054657, Final Batch Loss: 0.21798257529735565\n",
      "Epoch 3496, Loss: 0.8211240023374557, Final Batch Loss: 0.33674564957618713\n",
      "Epoch 3497, Loss: 0.9086375534534454, Final Batch Loss: 0.422355055809021\n",
      "Epoch 3498, Loss: 0.768129974603653, Final Batch Loss: 0.28826960921287537\n",
      "Epoch 3499, Loss: 0.8755777776241302, Final Batch Loss: 0.37983080744743347\n",
      "Epoch 3500, Loss: 0.6599573045969009, Final Batch Loss: 0.16831065714359283\n",
      "Epoch 3501, Loss: 0.8546472042798996, Final Batch Loss: 0.39702171087265015\n",
      "Epoch 3502, Loss: 0.589234322309494, Final Batch Loss: 0.1677759736776352\n",
      "Epoch 3503, Loss: 0.8134457916021347, Final Batch Loss: 0.26522108912467957\n",
      "Epoch 3504, Loss: 0.7335793077945709, Final Batch Loss: 0.2676726281642914\n",
      "Epoch 3505, Loss: 0.6045148372650146, Final Batch Loss: 0.15700943768024445\n",
      "Epoch 3506, Loss: 0.6547103226184845, Final Batch Loss: 0.1765841245651245\n",
      "Epoch 3507, Loss: 0.7880823165178299, Final Batch Loss: 0.32708802819252014\n",
      "Epoch 3508, Loss: 0.8681206107139587, Final Batch Loss: 0.4142371416091919\n",
      "Epoch 3509, Loss: 0.5679541677236557, Final Batch Loss: 0.08890682458877563\n",
      "Epoch 3510, Loss: 0.7139600515365601, Final Batch Loss: 0.20503582060337067\n",
      "Epoch 3511, Loss: 0.717487022280693, Final Batch Loss: 0.2566079795360565\n",
      "Epoch 3512, Loss: 0.7472918778657913, Final Batch Loss: 0.2776094377040863\n",
      "Epoch 3513, Loss: 0.6614552289247513, Final Batch Loss: 0.21790966391563416\n",
      "Epoch 3514, Loss: 0.6564418524503708, Final Batch Loss: 0.15973135828971863\n",
      "Epoch 3515, Loss: 0.6334347426891327, Final Batch Loss: 0.203715518116951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3516, Loss: 0.6431522220373154, Final Batch Loss: 0.13253211975097656\n",
      "Epoch 3517, Loss: 0.6400966942310333, Final Batch Loss: 0.19423317909240723\n",
      "Epoch 3518, Loss: 0.8467066287994385, Final Batch Loss: 0.3785759508609772\n",
      "Epoch 3519, Loss: 0.7116797268390656, Final Batch Loss: 0.27240756154060364\n",
      "Epoch 3520, Loss: 0.6737679243087769, Final Batch Loss: 0.23493964970111847\n",
      "Epoch 3521, Loss: 0.6306958794593811, Final Batch Loss: 0.22320497035980225\n",
      "Epoch 3522, Loss: 1.0845575630664825, Final Batch Loss: 0.5876431465148926\n",
      "Epoch 3523, Loss: 0.6405893862247467, Final Batch Loss: 0.15823495388031006\n",
      "Epoch 3524, Loss: 0.7646029591560364, Final Batch Loss: 0.3078666627407074\n",
      "Epoch 3525, Loss: 1.1008691042661667, Final Batch Loss: 0.6338726282119751\n",
      "Epoch 3526, Loss: 0.6536338925361633, Final Batch Loss: 0.1424720287322998\n",
      "Epoch 3527, Loss: 0.7964730560779572, Final Batch Loss: 0.298841655254364\n",
      "Epoch 3528, Loss: 0.7892763763666153, Final Batch Loss: 0.31586551666259766\n",
      "Epoch 3529, Loss: 0.7916382402181625, Final Batch Loss: 0.31232357025146484\n",
      "Epoch 3530, Loss: 0.7803971022367477, Final Batch Loss: 0.2719368636608124\n",
      "Epoch 3531, Loss: 0.7710990309715271, Final Batch Loss: 0.28502145409584045\n",
      "Epoch 3532, Loss: 0.6112344488501549, Final Batch Loss: 0.11734021455049515\n",
      "Epoch 3533, Loss: 0.6057871878147125, Final Batch Loss: 0.16102449595928192\n",
      "Epoch 3534, Loss: 0.8265741318464279, Final Batch Loss: 0.3527197539806366\n",
      "Epoch 3535, Loss: 0.9002539813518524, Final Batch Loss: 0.4538016617298126\n",
      "Epoch 3536, Loss: 1.2254324704408646, Final Batch Loss: 0.6744564175605774\n",
      "Epoch 3537, Loss: 0.5423100851476192, Final Batch Loss: 0.060080718249082565\n",
      "Epoch 3538, Loss: 0.7600095719099045, Final Batch Loss: 0.24394471943378448\n",
      "Epoch 3539, Loss: 0.5979279577732086, Final Batch Loss: 0.1410331279039383\n",
      "Epoch 3540, Loss: 0.7730769515037537, Final Batch Loss: 0.22042736411094666\n",
      "Epoch 3541, Loss: 0.7113714814186096, Final Batch Loss: 0.1550120860338211\n",
      "Epoch 3542, Loss: 0.840101033449173, Final Batch Loss: 0.35410791635513306\n",
      "Epoch 3543, Loss: 0.6871339231729507, Final Batch Loss: 0.241718590259552\n",
      "Epoch 3544, Loss: 0.6793446987867355, Final Batch Loss: 0.21507100760936737\n",
      "Epoch 3545, Loss: 1.1540345549583435, Final Batch Loss: 0.6371816992759705\n",
      "Epoch 3546, Loss: 0.6222137063741684, Final Batch Loss: 0.15139566361904144\n",
      "Epoch 3547, Loss: 0.751750037074089, Final Batch Loss: 0.28350818157196045\n",
      "Epoch 3548, Loss: 0.9630241394042969, Final Batch Loss: 0.5372391939163208\n",
      "Epoch 3549, Loss: 0.7332402765750885, Final Batch Loss: 0.2475358545780182\n",
      "Epoch 3550, Loss: 0.6877778321504593, Final Batch Loss: 0.15529006719589233\n",
      "Epoch 3551, Loss: 0.7746656537055969, Final Batch Loss: 0.2634749114513397\n",
      "Epoch 3552, Loss: 0.7442036718130112, Final Batch Loss: 0.24407008290290833\n",
      "Epoch 3553, Loss: 0.8631938099861145, Final Batch Loss: 0.34838318824768066\n",
      "Epoch 3554, Loss: 0.7517283260822296, Final Batch Loss: 0.2650642395019531\n",
      "Epoch 3555, Loss: 0.5040065124630928, Final Batch Loss: 0.03595038503408432\n",
      "Epoch 3556, Loss: 0.5798652917146683, Final Batch Loss: 0.1507817506790161\n",
      "Epoch 3557, Loss: 0.5792075991630554, Final Batch Loss: 0.0825505256652832\n",
      "Epoch 3558, Loss: 0.6652439087629318, Final Batch Loss: 0.19768846035003662\n",
      "Epoch 3559, Loss: 0.605270117521286, Final Batch Loss: 0.09618246555328369\n",
      "Epoch 3560, Loss: 0.7389270961284637, Final Batch Loss: 0.2694413661956787\n",
      "Epoch 3561, Loss: 0.7700983136892319, Final Batch Loss: 0.34794163703918457\n",
      "Epoch 3562, Loss: 0.6941636502742767, Final Batch Loss: 0.1994423121213913\n",
      "Epoch 3563, Loss: 0.5557151958346367, Final Batch Loss: 0.06787488609552383\n",
      "Epoch 3564, Loss: 0.6192984580993652, Final Batch Loss: 0.15271905064582825\n",
      "Epoch 3565, Loss: 0.8112637102603912, Final Batch Loss: 0.3936234414577484\n",
      "Epoch 3566, Loss: 1.0702706277370453, Final Batch Loss: 0.5961863398551941\n",
      "Epoch 3567, Loss: 0.8014252036809921, Final Batch Loss: 0.28300267457962036\n",
      "Epoch 3568, Loss: 0.711212232708931, Final Batch Loss: 0.1664944291114807\n",
      "Epoch 3569, Loss: 0.8231417834758759, Final Batch Loss: 0.29694458842277527\n",
      "Epoch 3570, Loss: 0.7166004478931427, Final Batch Loss: 0.14493882656097412\n",
      "Epoch 3571, Loss: 0.7418008744716644, Final Batch Loss: 0.308307409286499\n",
      "Epoch 3572, Loss: 0.861750066280365, Final Batch Loss: 0.40145057439804077\n",
      "Epoch 3573, Loss: 0.5620980486273766, Final Batch Loss: 0.10979471355676651\n",
      "Epoch 3574, Loss: 0.7901945859193802, Final Batch Loss: 0.2811664640903473\n",
      "Epoch 3575, Loss: 0.7296537607908249, Final Batch Loss: 0.18734289705753326\n",
      "Epoch 3576, Loss: 0.7534274309873581, Final Batch Loss: 0.28362852334976196\n",
      "Epoch 3577, Loss: 0.9396240562200546, Final Batch Loss: 0.4701921045780182\n",
      "Epoch 3578, Loss: 0.5210214368999004, Final Batch Loss: 0.03185413405299187\n",
      "Epoch 3579, Loss: 0.6303415074944496, Final Batch Loss: 0.12214332073926926\n",
      "Epoch 3580, Loss: 0.6556960493326187, Final Batch Loss: 0.21639412641525269\n",
      "Epoch 3581, Loss: 0.7962624132633209, Final Batch Loss: 0.3023439347743988\n",
      "Epoch 3582, Loss: 0.63962222635746, Final Batch Loss: 0.20444484055042267\n",
      "Epoch 3583, Loss: 0.6179700195789337, Final Batch Loss: 0.11855539679527283\n",
      "Epoch 3584, Loss: 0.6166386753320694, Final Batch Loss: 0.18929247558116913\n",
      "Epoch 3585, Loss: 0.8119545727968216, Final Batch Loss: 0.28393688797950745\n",
      "Epoch 3586, Loss: 0.6085094660520554, Final Batch Loss: 0.12687446177005768\n",
      "Epoch 3587, Loss: 0.7342073619365692, Final Batch Loss: 0.2745892107486725\n",
      "Epoch 3588, Loss: 0.7790928483009338, Final Batch Loss: 0.3475591540336609\n",
      "Epoch 3589, Loss: 0.5809038877487183, Final Batch Loss: 0.14792387187480927\n",
      "Epoch 3590, Loss: 0.6428763270378113, Final Batch Loss: 0.18302465975284576\n",
      "Epoch 3591, Loss: 0.7159710377454758, Final Batch Loss: 0.23480796813964844\n",
      "Epoch 3592, Loss: 0.5399527922272682, Final Batch Loss: 0.05053495615720749\n",
      "Epoch 3593, Loss: 0.5633878707885742, Final Batch Loss: 0.1223917156457901\n",
      "Epoch 3594, Loss: 0.5529653578996658, Final Batch Loss: 0.1321561336517334\n",
      "Epoch 3595, Loss: 0.7639547139406204, Final Batch Loss: 0.29139190912246704\n",
      "Epoch 3596, Loss: 0.7339500784873962, Final Batch Loss: 0.2891012132167816\n",
      "Epoch 3597, Loss: 0.7220702171325684, Final Batch Loss: 0.23774124681949615\n",
      "Epoch 3598, Loss: 0.671274408698082, Final Batch Loss: 0.20780794322490692\n",
      "Epoch 3599, Loss: 0.7262319624423981, Final Batch Loss: 0.30638185143470764\n",
      "Epoch 3600, Loss: 0.9339958727359772, Final Batch Loss: 0.4265879988670349\n",
      "Epoch 3601, Loss: 0.5790124014019966, Final Batch Loss: 0.10524822026491165\n",
      "Epoch 3602, Loss: 0.9473118335008621, Final Batch Loss: 0.5300388336181641\n",
      "Epoch 3603, Loss: 0.6129976809024811, Final Batch Loss: 0.18183155357837677\n",
      "Epoch 3604, Loss: 0.6780241578817368, Final Batch Loss: 0.24492117762565613\n",
      "Epoch 3605, Loss: 0.5951374471187592, Final Batch Loss: 0.16178031265735626\n",
      "Epoch 3606, Loss: 0.8045721054077148, Final Batch Loss: 0.2830807864665985\n",
      "Epoch 3607, Loss: 0.5941282659769058, Final Batch Loss: 0.11171215772628784\n",
      "Epoch 3608, Loss: 0.5863111615180969, Final Batch Loss: 0.1566207855939865\n",
      "Epoch 3609, Loss: 0.5822114795446396, Final Batch Loss: 0.07672303915023804\n",
      "Epoch 3610, Loss: 0.6701643615961075, Final Batch Loss: 0.13466580212116241\n",
      "Epoch 3611, Loss: 0.5130372755229473, Final Batch Loss: 0.05810023471713066\n",
      "Epoch 3612, Loss: 0.7321593761444092, Final Batch Loss: 0.3141954839229584\n",
      "Epoch 3613, Loss: 0.5930762887001038, Final Batch Loss: 0.12723612785339355\n",
      "Epoch 3614, Loss: 0.6968187689781189, Final Batch Loss: 0.25857433676719666\n",
      "Epoch 3615, Loss: 1.043250396847725, Final Batch Loss: 0.56065434217453\n",
      "Epoch 3616, Loss: 0.72129225730896, Final Batch Loss: 0.2122538983821869\n",
      "Epoch 3617, Loss: 0.7412122637033463, Final Batch Loss: 0.2867835760116577\n",
      "Epoch 3618, Loss: 0.7890718132257462, Final Batch Loss: 0.2837705612182617\n",
      "Epoch 3619, Loss: 0.7425331920385361, Final Batch Loss: 0.2860755920410156\n",
      "Epoch 3620, Loss: 0.6115856319665909, Final Batch Loss: 0.15318498015403748\n",
      "Epoch 3621, Loss: 0.587571881711483, Final Batch Loss: 0.09412752836942673\n",
      "Epoch 3622, Loss: 0.9081721901893616, Final Batch Loss: 0.38197001814842224\n",
      "Epoch 3623, Loss: 0.6555454432964325, Final Batch Loss: 0.21138788759708405\n",
      "Epoch 3624, Loss: 0.7699612677097321, Final Batch Loss: 0.27966657280921936\n",
      "Epoch 3625, Loss: 1.0553479045629501, Final Batch Loss: 0.6244444847106934\n",
      "Epoch 3626, Loss: 0.56940708309412, Final Batch Loss: 0.052024953067302704\n",
      "Epoch 3627, Loss: 0.6324068754911423, Final Batch Loss: 0.16877448558807373\n",
      "Epoch 3628, Loss: 0.6275614202022552, Final Batch Loss: 0.14867402613162994\n",
      "Epoch 3629, Loss: 0.7769234031438828, Final Batch Loss: 0.32661470770835876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3630, Loss: 0.6479271203279495, Final Batch Loss: 0.1936616152524948\n",
      "Epoch 3631, Loss: 0.738362729549408, Final Batch Loss: 0.19200153648853302\n",
      "Epoch 3632, Loss: 0.8849326521158218, Final Batch Loss: 0.3996552526950836\n",
      "Epoch 3633, Loss: 0.8290478438138962, Final Batch Loss: 0.33053556084632874\n",
      "Epoch 3634, Loss: 0.6312662214040756, Final Batch Loss: 0.1832883656024933\n",
      "Epoch 3635, Loss: 0.8813779652118683, Final Batch Loss: 0.4020674526691437\n",
      "Epoch 3636, Loss: 0.6315519362688065, Final Batch Loss: 0.15371590852737427\n",
      "Epoch 3637, Loss: 0.5731405019760132, Final Batch Loss: 0.11220456659793854\n",
      "Epoch 3638, Loss: 0.6604117602109909, Final Batch Loss: 0.1851205825805664\n",
      "Epoch 3639, Loss: 0.7579231858253479, Final Batch Loss: 0.32229405641555786\n",
      "Epoch 3640, Loss: 0.7862879782915115, Final Batch Loss: 0.33708661794662476\n",
      "Epoch 3641, Loss: 0.6512503623962402, Final Batch Loss: 0.2217947244644165\n",
      "Epoch 3642, Loss: 0.6377875953912735, Final Batch Loss: 0.25834909081459045\n",
      "Epoch 3643, Loss: 0.5327386483550072, Final Batch Loss: 0.07812342792749405\n",
      "Epoch 3644, Loss: 0.7447703778743744, Final Batch Loss: 0.2947894036769867\n",
      "Epoch 3645, Loss: 0.5105191767215729, Final Batch Loss: 0.03545014560222626\n",
      "Epoch 3646, Loss: 0.6226753443479538, Final Batch Loss: 0.16352087259292603\n",
      "Epoch 3647, Loss: 0.9020745158195496, Final Batch Loss: 0.3811764121055603\n",
      "Epoch 3648, Loss: 0.684639573097229, Final Batch Loss: 0.22655975818634033\n",
      "Epoch 3649, Loss: 0.598426029086113, Final Batch Loss: 0.1715007871389389\n",
      "Epoch 3650, Loss: 0.5832809507846832, Final Batch Loss: 0.13651226460933685\n",
      "Epoch 3651, Loss: 0.5723584145307541, Final Batch Loss: 0.16194701194763184\n",
      "Epoch 3652, Loss: 0.6801557540893555, Final Batch Loss: 0.2799850106239319\n",
      "Epoch 3653, Loss: 0.5391674712300301, Final Batch Loss: 0.12187308818101883\n",
      "Epoch 3654, Loss: 0.6692057847976685, Final Batch Loss: 0.199372798204422\n",
      "Epoch 3655, Loss: 0.6176222413778305, Final Batch Loss: 0.1686786562204361\n",
      "Epoch 3656, Loss: 0.6975828111171722, Final Batch Loss: 0.21279297769069672\n",
      "Epoch 3657, Loss: 0.8384197503328323, Final Batch Loss: 0.42757606506347656\n",
      "Epoch 3658, Loss: 0.7046497613191605, Final Batch Loss: 0.17893201112747192\n",
      "Epoch 3659, Loss: 0.5199355110526085, Final Batch Loss: 0.0614088699221611\n",
      "Epoch 3660, Loss: 0.7488355934619904, Final Batch Loss: 0.3004569113254547\n",
      "Epoch 3661, Loss: 0.6623301953077316, Final Batch Loss: 0.24375152587890625\n",
      "Epoch 3662, Loss: 0.8746604919433594, Final Batch Loss: 0.4142886996269226\n",
      "Epoch 3663, Loss: 0.7340336292982101, Final Batch Loss: 0.28914016485214233\n",
      "Epoch 3664, Loss: 0.6018072962760925, Final Batch Loss: 0.15837301313877106\n",
      "Epoch 3665, Loss: 0.7162146717309952, Final Batch Loss: 0.22660130262374878\n",
      "Epoch 3666, Loss: 0.7328508347272873, Final Batch Loss: 0.282467246055603\n",
      "Epoch 3667, Loss: 0.7392323166131973, Final Batch Loss: 0.27389124035835266\n",
      "Epoch 3668, Loss: 0.6555621325969696, Final Batch Loss: 0.21776939928531647\n",
      "Epoch 3669, Loss: 0.5077502429485321, Final Batch Loss: 0.11840511858463287\n",
      "Epoch 3670, Loss: 0.8660536408424377, Final Batch Loss: 0.3908188045024872\n",
      "Epoch 3671, Loss: 1.1073915660381317, Final Batch Loss: 0.656563937664032\n",
      "Epoch 3672, Loss: 0.6411172300577164, Final Batch Loss: 0.18732158839702606\n",
      "Epoch 3673, Loss: 0.768404945731163, Final Batch Loss: 0.224980428814888\n",
      "Epoch 3674, Loss: 0.6706029921770096, Final Batch Loss: 0.245143860578537\n",
      "Epoch 3675, Loss: 0.6612350940704346, Final Batch Loss: 0.23451903462409973\n",
      "Epoch 3676, Loss: 0.47799677588045597, Final Batch Loss: 0.029081666842103004\n",
      "Epoch 3677, Loss: 0.6884578168392181, Final Batch Loss: 0.19793349504470825\n",
      "Epoch 3678, Loss: 0.5685459822416306, Final Batch Loss: 0.11852732300758362\n",
      "Epoch 3679, Loss: 0.6770641505718231, Final Batch Loss: 0.21206627786159515\n",
      "Epoch 3680, Loss: 0.774088129401207, Final Batch Loss: 0.30231600999832153\n",
      "Epoch 3681, Loss: 0.59077587723732, Final Batch Loss: 0.11517022550106049\n",
      "Epoch 3682, Loss: 0.6458982974290848, Final Batch Loss: 0.16671280562877655\n",
      "Epoch 3683, Loss: 0.5357694178819656, Final Batch Loss: 0.11478489637374878\n",
      "Epoch 3684, Loss: 0.823571965098381, Final Batch Loss: 0.34956932067871094\n",
      "Epoch 3685, Loss: 0.5050069019198418, Final Batch Loss: 0.03590039163827896\n",
      "Epoch 3686, Loss: 0.7236265391111374, Final Batch Loss: 0.2674768269062042\n",
      "Epoch 3687, Loss: 0.8368227928876877, Final Batch Loss: 0.3025553226470947\n",
      "Epoch 3688, Loss: 0.7975997626781464, Final Batch Loss: 0.35347050428390503\n",
      "Epoch 3689, Loss: 0.7359045892953873, Final Batch Loss: 0.2843998670578003\n",
      "Epoch 3690, Loss: 0.637944296002388, Final Batch Loss: 0.15467213094234467\n",
      "Epoch 3691, Loss: 1.427389681339264, Final Batch Loss: 0.9165721535682678\n",
      "Epoch 3692, Loss: 0.6677639782428741, Final Batch Loss: 0.2143782377243042\n",
      "Epoch 3693, Loss: 0.6715218722820282, Final Batch Loss: 0.21767649054527283\n",
      "Epoch 3694, Loss: 0.7323264181613922, Final Batch Loss: 0.22884926199913025\n",
      "Epoch 3695, Loss: 0.5616026371717453, Final Batch Loss: 0.07925781607627869\n",
      "Epoch 3696, Loss: 0.767632782459259, Final Batch Loss: 0.28123846650123596\n",
      "Epoch 3697, Loss: 0.6470850706100464, Final Batch Loss: 0.20461107790470123\n",
      "Epoch 3698, Loss: 0.5539261698722839, Final Batch Loss: 0.12771017849445343\n",
      "Epoch 3699, Loss: 0.7762133628129959, Final Batch Loss: 0.3451899588108063\n",
      "Epoch 3700, Loss: 0.6915962547063828, Final Batch Loss: 0.2504251003265381\n",
      "Epoch 3701, Loss: 0.8725141137838364, Final Batch Loss: 0.47560766339302063\n",
      "Epoch 3702, Loss: 0.7399078756570816, Final Batch Loss: 0.3087290823459625\n",
      "Epoch 3703, Loss: 0.6745044887065887, Final Batch Loss: 0.20910730957984924\n",
      "Epoch 3704, Loss: 0.6693389713764191, Final Batch Loss: 0.2707228362560272\n",
      "Epoch 3705, Loss: 0.5786276161670685, Final Batch Loss: 0.14884419739246368\n",
      "Epoch 3706, Loss: 0.7424547672271729, Final Batch Loss: 0.35899391770362854\n",
      "Epoch 3707, Loss: 0.5088975206017494, Final Batch Loss: 0.08111371845006943\n",
      "Epoch 3708, Loss: 0.697903960943222, Final Batch Loss: 0.23997053503990173\n",
      "Epoch 3709, Loss: 0.7533762156963348, Final Batch Loss: 0.3087405562400818\n",
      "Epoch 3710, Loss: 0.648263543844223, Final Batch Loss: 0.15653161704540253\n",
      "Epoch 3711, Loss: 0.4838141221553087, Final Batch Loss: 0.025078685954213142\n",
      "Epoch 3712, Loss: 0.6960039436817169, Final Batch Loss: 0.2109719067811966\n",
      "Epoch 3713, Loss: 0.5538541525602341, Final Batch Loss: 0.1517491489648819\n",
      "Epoch 3714, Loss: 0.6610109061002731, Final Batch Loss: 0.19945180416107178\n",
      "Epoch 3715, Loss: 0.6263038963079453, Final Batch Loss: 0.1386093646287918\n",
      "Epoch 3716, Loss: 0.7731131762266159, Final Batch Loss: 0.27991539239883423\n",
      "Epoch 3717, Loss: 0.6498515903949738, Final Batch Loss: 0.22281727194786072\n",
      "Epoch 3718, Loss: 0.6435070484876633, Final Batch Loss: 0.277653306722641\n",
      "Epoch 3719, Loss: 0.6863761842250824, Final Batch Loss: 0.2619495689868927\n",
      "Epoch 3720, Loss: 0.7212400138378143, Final Batch Loss: 0.2900906503200531\n",
      "Epoch 3721, Loss: 0.5723061710596085, Final Batch Loss: 0.08382683992385864\n",
      "Epoch 3722, Loss: 0.6441985368728638, Final Batch Loss: 0.19157755374908447\n",
      "Epoch 3723, Loss: 0.7773626148700714, Final Batch Loss: 0.3261190354824066\n",
      "Epoch 3724, Loss: 0.4849620833992958, Final Batch Loss: 0.07986412197351456\n",
      "Epoch 3725, Loss: 0.5180722251534462, Final Batch Loss: 0.07617578655481339\n",
      "Epoch 3726, Loss: 0.7568514347076416, Final Batch Loss: 0.296810507774353\n",
      "Epoch 3727, Loss: 0.695098340511322, Final Batch Loss: 0.2428639829158783\n",
      "Epoch 3728, Loss: 0.6288111656904221, Final Batch Loss: 0.1986033320426941\n",
      "Epoch 3729, Loss: 0.9919895678758621, Final Batch Loss: 0.5863503217697144\n",
      "Epoch 3730, Loss: 0.599260538816452, Final Batch Loss: 0.14598853886127472\n",
      "Epoch 3731, Loss: 0.6561727225780487, Final Batch Loss: 0.2110990285873413\n",
      "Epoch 3732, Loss: 0.881594717502594, Final Batch Loss: 0.4173681139945984\n",
      "Epoch 3733, Loss: 0.6046544313430786, Final Batch Loss: 0.11641043424606323\n",
      "Epoch 3734, Loss: 0.5188247859477997, Final Batch Loss: 0.06387381255626678\n",
      "Epoch 3735, Loss: 0.6564086377620697, Final Batch Loss: 0.2190961092710495\n",
      "Epoch 3736, Loss: 0.5038411468267441, Final Batch Loss: 0.04419359564781189\n",
      "Epoch 3737, Loss: 0.7676205784082413, Final Batch Loss: 0.3441517651081085\n",
      "Epoch 3738, Loss: 0.5838833451271057, Final Batch Loss: 0.14416497945785522\n",
      "Epoch 3739, Loss: 0.994145467877388, Final Batch Loss: 0.5624801516532898\n",
      "Epoch 3740, Loss: 0.58439502120018, Final Batch Loss: 0.159636452794075\n",
      "Epoch 3741, Loss: 0.6915683895349503, Final Batch Loss: 0.18665236234664917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3742, Loss: 0.633259043097496, Final Batch Loss: 0.17907744646072388\n",
      "Epoch 3743, Loss: 0.6526750326156616, Final Batch Loss: 0.2080174684524536\n",
      "Epoch 3744, Loss: 0.511554341763258, Final Batch Loss: 0.04418199881911278\n",
      "Epoch 3745, Loss: 0.7221812158823013, Final Batch Loss: 0.29003533720970154\n",
      "Epoch 3746, Loss: 0.5195733085274696, Final Batch Loss: 0.11437172442674637\n",
      "Epoch 3747, Loss: 0.7160371392965317, Final Batch Loss: 0.3032110929489136\n",
      "Epoch 3748, Loss: 0.8443601429462433, Final Batch Loss: 0.3452191948890686\n",
      "Epoch 3749, Loss: 0.7785419523715973, Final Batch Loss: 0.3137674331665039\n",
      "Epoch 3750, Loss: 0.6182039529085159, Final Batch Loss: 0.15443511307239532\n",
      "Epoch 3751, Loss: 0.5237366929650307, Final Batch Loss: 0.09726611524820328\n",
      "Epoch 3752, Loss: 0.684269443154335, Final Batch Loss: 0.16557912528514862\n",
      "Epoch 3753, Loss: 0.9467489570379257, Final Batch Loss: 0.49013394117355347\n",
      "Epoch 3754, Loss: 0.6784857511520386, Final Batch Loss: 0.21284981071949005\n",
      "Epoch 3755, Loss: 0.8336305767297745, Final Batch Loss: 0.37700504064559937\n",
      "Epoch 3756, Loss: 0.5803209841251373, Final Batch Loss: 0.15487511456012726\n",
      "Epoch 3757, Loss: 0.6575042009353638, Final Batch Loss: 0.15592928230762482\n",
      "Epoch 3758, Loss: 0.6271679103374481, Final Batch Loss: 0.18672342598438263\n",
      "Epoch 3759, Loss: 0.8843813985586166, Final Batch Loss: 0.3694184422492981\n",
      "Epoch 3760, Loss: 0.9518714845180511, Final Batch Loss: 0.4757332503795624\n",
      "Epoch 3761, Loss: 0.9091472625732422, Final Batch Loss: 0.44701290130615234\n",
      "Epoch 3762, Loss: 0.7876336425542831, Final Batch Loss: 0.3460158109664917\n",
      "Epoch 3763, Loss: 0.7302769273519516, Final Batch Loss: 0.30005913972854614\n",
      "Epoch 3764, Loss: 0.6852893978357315, Final Batch Loss: 0.23963242769241333\n",
      "Epoch 3765, Loss: 0.6377759128808975, Final Batch Loss: 0.19018031656742096\n",
      "Epoch 3766, Loss: 0.6814925819635391, Final Batch Loss: 0.24933724105358124\n",
      "Epoch 3767, Loss: 0.9581785351037979, Final Batch Loss: 0.48228639364242554\n",
      "Epoch 3768, Loss: 0.7552659064531326, Final Batch Loss: 0.301944762468338\n",
      "Epoch 3769, Loss: 0.6574314832687378, Final Batch Loss: 0.23280397057533264\n",
      "Epoch 3770, Loss: 0.8035868555307388, Final Batch Loss: 0.3223188519477844\n",
      "Epoch 3771, Loss: 0.59913769364357, Final Batch Loss: 0.16525118052959442\n",
      "Epoch 3772, Loss: 0.7997839450836182, Final Batch Loss: 0.2929777503013611\n",
      "Epoch 3773, Loss: 0.479633416980505, Final Batch Loss: 0.0311351977288723\n",
      "Epoch 3774, Loss: 0.7501761317253113, Final Batch Loss: 0.2308691442012787\n",
      "Epoch 3775, Loss: 0.5473312437534332, Final Batch Loss: 0.10024726390838623\n",
      "Epoch 3776, Loss: 0.6921377331018448, Final Batch Loss: 0.22269509732723236\n",
      "Epoch 3777, Loss: 0.6180218607187271, Final Batch Loss: 0.12282007932662964\n",
      "Epoch 3778, Loss: 0.592472493648529, Final Batch Loss: 0.14136646687984467\n",
      "Epoch 3779, Loss: 0.7870007157325745, Final Batch Loss: 0.34763002395629883\n",
      "Epoch 3780, Loss: 0.5741000920534134, Final Batch Loss: 0.2136775255203247\n",
      "Epoch 3781, Loss: 0.8490061163902283, Final Batch Loss: 0.3957144618034363\n",
      "Epoch 3782, Loss: 0.7572441399097443, Final Batch Loss: 0.355373740196228\n",
      "Epoch 3783, Loss: 0.5414621159434319, Final Batch Loss: 0.10201118141412735\n",
      "Epoch 3784, Loss: 0.5904029309749603, Final Batch Loss: 0.13704071938991547\n",
      "Epoch 3785, Loss: 0.9978352636098862, Final Batch Loss: 0.5411733984947205\n",
      "Epoch 3786, Loss: 0.5640052407979965, Final Batch Loss: 0.06542536616325378\n",
      "Epoch 3787, Loss: 0.7888577729463577, Final Batch Loss: 0.36087143421173096\n",
      "Epoch 3788, Loss: 0.6364933252334595, Final Batch Loss: 0.15751731395721436\n",
      "Epoch 3789, Loss: 0.5934507548809052, Final Batch Loss: 0.1306387037038803\n",
      "Epoch 3790, Loss: 1.0744407176971436, Final Batch Loss: 0.620710551738739\n",
      "Epoch 3791, Loss: 0.6563026905059814, Final Batch Loss: 0.24601386487483978\n",
      "Epoch 3792, Loss: 0.4851316213607788, Final Batch Loss: 0.09246332943439484\n",
      "Epoch 3793, Loss: 0.532251663506031, Final Batch Loss: 0.1140657588839531\n",
      "Epoch 3794, Loss: 0.6243169605731964, Final Batch Loss: 0.17276473343372345\n",
      "Epoch 3795, Loss: 0.7275238782167435, Final Batch Loss: 0.2845892906188965\n",
      "Epoch 3796, Loss: 1.188768982887268, Final Batch Loss: 0.6856825351715088\n",
      "Epoch 3797, Loss: 0.7052255719900131, Final Batch Loss: 0.3032206892967224\n",
      "Epoch 3798, Loss: 0.771791085600853, Final Batch Loss: 0.30473431944847107\n",
      "Epoch 3799, Loss: 0.6727284640073776, Final Batch Loss: 0.24434079229831696\n",
      "Epoch 3800, Loss: 0.6770135760307312, Final Batch Loss: 0.21366746723651886\n",
      "Epoch 3801, Loss: 0.6513179242610931, Final Batch Loss: 0.1878497153520584\n",
      "Epoch 3802, Loss: 0.6294335573911667, Final Batch Loss: 0.14533160626888275\n",
      "Epoch 3803, Loss: 0.7195223420858383, Final Batch Loss: 0.24280567467212677\n",
      "Epoch 3804, Loss: 0.8715061694383621, Final Batch Loss: 0.40222278237342834\n",
      "Epoch 3805, Loss: 0.625295914709568, Final Batch Loss: 0.07583902031183243\n",
      "Epoch 3806, Loss: 0.6920633316040039, Final Batch Loss: 0.22180163860321045\n",
      "Epoch 3807, Loss: 0.6550247818231583, Final Batch Loss: 0.15416693687438965\n",
      "Epoch 3808, Loss: 0.677222803235054, Final Batch Loss: 0.2588762938976288\n",
      "Epoch 3809, Loss: 0.6969715505838394, Final Batch Loss: 0.18281570076942444\n",
      "Epoch 3810, Loss: 0.6215391457080841, Final Batch Loss: 0.17269207537174225\n",
      "Epoch 3811, Loss: 0.5953569039702415, Final Batch Loss: 0.10320232063531876\n",
      "Epoch 3812, Loss: 0.6097463965415955, Final Batch Loss: 0.15422317385673523\n",
      "Epoch 3813, Loss: 0.7973256409168243, Final Batch Loss: 0.325306236743927\n",
      "Epoch 3814, Loss: 0.5131391286849976, Final Batch Loss: 0.12723664939403534\n",
      "Epoch 3815, Loss: 0.6607708632946014, Final Batch Loss: 0.23113521933555603\n",
      "Epoch 3816, Loss: 0.5753362253308296, Final Batch Loss: 0.09819620102643967\n",
      "Epoch 3817, Loss: 0.6260369420051575, Final Batch Loss: 0.18655557930469513\n",
      "Epoch 3818, Loss: 0.6817762851715088, Final Batch Loss: 0.2394643872976303\n",
      "Epoch 3819, Loss: 0.6513279527425766, Final Batch Loss: 0.2517987787723541\n",
      "Epoch 3820, Loss: 0.5202322974801064, Final Batch Loss: 0.06529729813337326\n",
      "Epoch 3821, Loss: 0.5356238186359406, Final Batch Loss: 0.12787574529647827\n",
      "Epoch 3822, Loss: 0.7132432162761688, Final Batch Loss: 0.29877132177352905\n",
      "Epoch 3823, Loss: 0.7177916020154953, Final Batch Loss: 0.22727662324905396\n",
      "Epoch 3824, Loss: 0.5637980699539185, Final Batch Loss: 0.08134832978248596\n",
      "Epoch 3825, Loss: 0.6459984183311462, Final Batch Loss: 0.2051296979188919\n",
      "Epoch 3826, Loss: 0.7459616512060165, Final Batch Loss: 0.33934614062309265\n",
      "Epoch 3827, Loss: 0.7183799892663956, Final Batch Loss: 0.2714998126029968\n",
      "Epoch 3828, Loss: 0.7725444883108139, Final Batch Loss: 0.3847469389438629\n",
      "Epoch 3829, Loss: 0.6423188745975494, Final Batch Loss: 0.2382139414548874\n",
      "Epoch 3830, Loss: 0.4949568696320057, Final Batch Loss: 0.060911986976861954\n",
      "Epoch 3831, Loss: 0.6332081109285355, Final Batch Loss: 0.16830021142959595\n",
      "Epoch 3832, Loss: 0.6701792776584625, Final Batch Loss: 0.18217378854751587\n",
      "Epoch 3833, Loss: 0.7692527770996094, Final Batch Loss: 0.34680795669555664\n",
      "Epoch 3834, Loss: 0.57625313103199, Final Batch Loss: 0.1182558685541153\n",
      "Epoch 3835, Loss: 0.6744037866592407, Final Batch Loss: 0.2158767580986023\n",
      "Epoch 3836, Loss: 0.6553577780723572, Final Batch Loss: 0.1940382570028305\n",
      "Epoch 3837, Loss: 0.48826005309820175, Final Batch Loss: 0.06125394254922867\n",
      "Epoch 3838, Loss: 0.5737246721982956, Final Batch Loss: 0.08424140512943268\n",
      "Epoch 3839, Loss: 0.7640877813100815, Final Batch Loss: 0.339051753282547\n",
      "Epoch 3840, Loss: 0.5396278128027916, Final Batch Loss: 0.07810584455728531\n",
      "Epoch 3841, Loss: 0.6604198962450027, Final Batch Loss: 0.19265471398830414\n",
      "Epoch 3842, Loss: 0.7004361003637314, Final Batch Loss: 0.2523444592952728\n",
      "Epoch 3843, Loss: 0.6252194195985794, Final Batch Loss: 0.22684919834136963\n",
      "Epoch 3844, Loss: 0.6691327542066574, Final Batch Loss: 0.14090505242347717\n",
      "Epoch 3845, Loss: 0.6884990781545639, Final Batch Loss: 0.2324947863817215\n",
      "Epoch 3846, Loss: 0.5440687611699104, Final Batch Loss: 0.05971931666135788\n",
      "Epoch 3847, Loss: 0.6469488739967346, Final Batch Loss: 0.1918841004371643\n",
      "Epoch 3848, Loss: 0.5147229954600334, Final Batch Loss: 0.06474687904119492\n",
      "Epoch 3849, Loss: 0.5344859510660172, Final Batch Loss: 0.07574021816253662\n",
      "Epoch 3850, Loss: 0.7156799733638763, Final Batch Loss: 0.31214436888694763\n",
      "Epoch 3851, Loss: 0.6574450880289078, Final Batch Loss: 0.2317395657300949\n",
      "Epoch 3852, Loss: 0.8061986565589905, Final Batch Loss: 0.3937520384788513\n",
      "Epoch 3853, Loss: 0.5819898098707199, Final Batch Loss: 0.12628288567066193\n",
      "Epoch 3854, Loss: 0.5601354092359543, Final Batch Loss: 0.14013506472110748\n",
      "Epoch 3855, Loss: 0.4901478663086891, Final Batch Loss: 0.11379633098840714\n",
      "Epoch 3856, Loss: 0.5165538266301155, Final Batch Loss: 0.08291687816381454\n",
      "Epoch 3857, Loss: 0.5657372921705246, Final Batch Loss: 0.06822410225868225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3858, Loss: 0.6597162038087845, Final Batch Loss: 0.25431329011917114\n",
      "Epoch 3859, Loss: 0.524184450507164, Final Batch Loss: 0.14318561553955078\n",
      "Epoch 3860, Loss: 0.5418142080307007, Final Batch Loss: 0.12881122529506683\n",
      "Epoch 3861, Loss: 0.5440384075045586, Final Batch Loss: 0.06838717311620712\n",
      "Epoch 3862, Loss: 0.6635257303714752, Final Batch Loss: 0.2333843857049942\n",
      "Epoch 3863, Loss: 0.8360437154769897, Final Batch Loss: 0.3638150095939636\n",
      "Epoch 3864, Loss: 0.4700668603181839, Final Batch Loss: 0.08844655752182007\n",
      "Epoch 3865, Loss: 0.5250285938382149, Final Batch Loss: 0.10915718227624893\n",
      "Epoch 3866, Loss: 0.5490574166178703, Final Batch Loss: 0.12200135737657547\n",
      "Epoch 3867, Loss: 0.5974601656198502, Final Batch Loss: 0.20010441541671753\n",
      "Epoch 3868, Loss: 0.7112933099269867, Final Batch Loss: 0.27528947591781616\n",
      "Epoch 3869, Loss: 0.852959156036377, Final Batch Loss: 0.4778936207294464\n",
      "Epoch 3870, Loss: 0.4892452843487263, Final Batch Loss: 0.03653492406010628\n",
      "Epoch 3871, Loss: 0.842337429523468, Final Batch Loss: 0.43689095973968506\n",
      "Epoch 3872, Loss: 0.6382411420345306, Final Batch Loss: 0.16884355247020721\n",
      "Epoch 3873, Loss: 0.7592370361089706, Final Batch Loss: 0.3562432527542114\n",
      "Epoch 3874, Loss: 0.7452458292245865, Final Batch Loss: 0.3625221252441406\n",
      "Epoch 3875, Loss: 0.8510283827781677, Final Batch Loss: 0.4041208326816559\n",
      "Epoch 3876, Loss: 0.6577576696872711, Final Batch Loss: 0.23966260254383087\n",
      "Epoch 3877, Loss: 0.8027036488056183, Final Batch Loss: 0.2995356619358063\n",
      "Epoch 3878, Loss: 0.676605224609375, Final Batch Loss: 0.21545358002185822\n",
      "Epoch 3879, Loss: 0.5488810539245605, Final Batch Loss: 0.07574847340583801\n",
      "Epoch 3880, Loss: 0.4945249892771244, Final Batch Loss: 0.0364631749689579\n",
      "Epoch 3881, Loss: 0.66904316842556, Final Batch Loss: 0.21813268959522247\n",
      "Epoch 3882, Loss: 0.6663352698087692, Final Batch Loss: 0.2914595305919647\n",
      "Epoch 3883, Loss: 0.822334498167038, Final Batch Loss: 0.2588592767715454\n",
      "Epoch 3884, Loss: 0.6395933330059052, Final Batch Loss: 0.18690159916877747\n",
      "Epoch 3885, Loss: 0.8724457174539566, Final Batch Loss: 0.4736871123313904\n",
      "Epoch 3886, Loss: 0.6835023760795593, Final Batch Loss: 0.24103568494319916\n",
      "Epoch 3887, Loss: 0.7951002568006516, Final Batch Loss: 0.3151983916759491\n",
      "Epoch 3888, Loss: 0.6034338176250458, Final Batch Loss: 0.14196838438510895\n",
      "Epoch 3889, Loss: 0.6382723599672318, Final Batch Loss: 0.18246415257453918\n",
      "Epoch 3890, Loss: 0.6994239985942841, Final Batch Loss: 0.2791430652141571\n",
      "Epoch 3891, Loss: 0.7750505805015564, Final Batch Loss: 0.28178635239601135\n",
      "Epoch 3892, Loss: 0.5741521865129471, Final Batch Loss: 0.1617809385061264\n",
      "Epoch 3893, Loss: 0.46939558535814285, Final Batch Loss: 0.06994358450174332\n",
      "Epoch 3894, Loss: 0.4942142553627491, Final Batch Loss: 0.06220226362347603\n",
      "Epoch 3895, Loss: 0.6185292750597, Final Batch Loss: 0.12891776859760284\n",
      "Epoch 3896, Loss: 0.5411489903926849, Final Batch Loss: 0.13397543132305145\n",
      "Epoch 3897, Loss: 0.5717317759990692, Final Batch Loss: 0.17967751622200012\n",
      "Epoch 3898, Loss: 0.7816718518733978, Final Batch Loss: 0.30492913722991943\n",
      "Epoch 3899, Loss: 0.7061383277177811, Final Batch Loss: 0.2423163205385208\n",
      "Epoch 3900, Loss: 0.7050705254077911, Final Batch Loss: 0.32337066531181335\n",
      "Epoch 3901, Loss: 0.7256670147180557, Final Batch Loss: 0.3122827708721161\n",
      "Epoch 3902, Loss: 0.7142945677042007, Final Batch Loss: 0.3081144392490387\n",
      "Epoch 3903, Loss: 0.6742769777774811, Final Batch Loss: 0.2601773142814636\n",
      "Epoch 3904, Loss: 0.48496653139591217, Final Batch Loss: 0.08883723616600037\n",
      "Epoch 3905, Loss: 0.5260895788669586, Final Batch Loss: 0.09617754817008972\n",
      "Epoch 3906, Loss: 0.9258101284503937, Final Batch Loss: 0.4663691818714142\n",
      "Epoch 3907, Loss: 0.778589516878128, Final Batch Loss: 0.30917954444885254\n",
      "Epoch 3908, Loss: 0.5555440187454224, Final Batch Loss: 0.10395261645317078\n",
      "Epoch 3909, Loss: 0.8159093409776688, Final Batch Loss: 0.3633049726486206\n",
      "Epoch 3910, Loss: 0.6185387820005417, Final Batch Loss: 0.20464912056922913\n",
      "Epoch 3911, Loss: 0.525559738278389, Final Batch Loss: 0.1451500952243805\n",
      "Epoch 3912, Loss: 0.5285746604204178, Final Batch Loss: 0.15236952900886536\n",
      "Epoch 3913, Loss: 0.6008538901805878, Final Batch Loss: 0.1793055534362793\n",
      "Epoch 3914, Loss: 0.5839809030294418, Final Batch Loss: 0.12945139408111572\n",
      "Epoch 3915, Loss: 0.6951552182435989, Final Batch Loss: 0.23830784857273102\n",
      "Epoch 3916, Loss: 0.5808016508817673, Final Batch Loss: 0.14745119214057922\n",
      "Epoch 3917, Loss: 0.609557181596756, Final Batch Loss: 0.18336577713489532\n",
      "Epoch 3918, Loss: 0.5440959855914116, Final Batch Loss: 0.06982619315385818\n",
      "Epoch 3919, Loss: 0.6384184882044792, Final Batch Loss: 0.12452145665884018\n",
      "Epoch 3920, Loss: 0.5407205000519753, Final Batch Loss: 0.11640425771474838\n",
      "Epoch 3921, Loss: 0.8534232825040817, Final Batch Loss: 0.4234106242656708\n",
      "Epoch 3922, Loss: 0.5433234423398972, Final Batch Loss: 0.09645785391330719\n",
      "Epoch 3923, Loss: 0.5698221027851105, Final Batch Loss: 0.1508495956659317\n",
      "Epoch 3924, Loss: 0.5702630430459976, Final Batch Loss: 0.16482867300510406\n",
      "Epoch 3925, Loss: 0.7721938490867615, Final Batch Loss: 0.3744332492351532\n",
      "Epoch 3926, Loss: 0.8377488851547241, Final Batch Loss: 0.37837350368499756\n",
      "Epoch 3927, Loss: 0.519438698887825, Final Batch Loss: 0.15377065539360046\n",
      "Epoch 3928, Loss: 0.5832035690546036, Final Batch Loss: 0.1954279989004135\n",
      "Epoch 3929, Loss: 0.48766617476940155, Final Batch Loss: 0.06953415274620056\n",
      "Epoch 3930, Loss: 0.6499170362949371, Final Batch Loss: 0.2206610143184662\n",
      "Epoch 3931, Loss: 0.5340746939182281, Final Batch Loss: 0.1612216830253601\n",
      "Epoch 3932, Loss: 0.7339017689228058, Final Batch Loss: 0.2752816677093506\n",
      "Epoch 3933, Loss: 0.6940916329622269, Final Batch Loss: 0.2750340402126312\n",
      "Epoch 3934, Loss: 0.5492428541183472, Final Batch Loss: 0.16632656753063202\n",
      "Epoch 3935, Loss: 0.6135021150112152, Final Batch Loss: 0.20311640202999115\n",
      "Epoch 3936, Loss: 0.7619934678077698, Final Batch Loss: 0.21844977140426636\n",
      "Epoch 3937, Loss: 0.8020851314067841, Final Batch Loss: 0.30012619495391846\n",
      "Epoch 3938, Loss: 0.7816423624753952, Final Batch Loss: 0.35534071922302246\n",
      "Epoch 3939, Loss: 0.5768947303295135, Final Batch Loss: 0.13473820686340332\n",
      "Epoch 3940, Loss: 1.2518858462572098, Final Batch Loss: 0.7614832520484924\n",
      "Epoch 3941, Loss: 0.5723641365766525, Final Batch Loss: 0.1215103417634964\n",
      "Epoch 3942, Loss: 0.5692253708839417, Final Batch Loss: 0.10353508591651917\n",
      "Epoch 3943, Loss: 0.5693169832229614, Final Batch Loss: 0.10791446268558502\n",
      "Epoch 3944, Loss: 0.8719555884599686, Final Batch Loss: 0.42638081312179565\n",
      "Epoch 3945, Loss: 0.5013667643070221, Final Batch Loss: 0.11460845172405243\n",
      "Epoch 3946, Loss: 0.6069628745317459, Final Batch Loss: 0.1538507342338562\n",
      "Epoch 3947, Loss: 0.7569615989923477, Final Batch Loss: 0.3269002437591553\n",
      "Epoch 3948, Loss: 0.6918980926275253, Final Batch Loss: 0.30564165115356445\n",
      "Epoch 3949, Loss: 0.7231404036283493, Final Batch Loss: 0.3102019727230072\n",
      "Epoch 3950, Loss: 0.6889867037534714, Final Batch Loss: 0.2760521471500397\n",
      "Epoch 3951, Loss: 0.5985954403877258, Final Batch Loss: 0.11269520223140717\n",
      "Epoch 3952, Loss: 0.7298540621995926, Final Batch Loss: 0.3036823868751526\n",
      "Epoch 3953, Loss: 0.6243177503347397, Final Batch Loss: 0.09806108474731445\n",
      "Epoch 3954, Loss: 0.9015036374330521, Final Batch Loss: 0.47778797149658203\n",
      "Epoch 3955, Loss: 0.5531181991100311, Final Batch Loss: 0.15166570246219635\n",
      "Epoch 3956, Loss: 0.5445410460233688, Final Batch Loss: 0.14622770249843597\n",
      "Epoch 3957, Loss: 0.546981155872345, Final Batch Loss: 0.14484304189682007\n",
      "Epoch 3958, Loss: 0.5578944534063339, Final Batch Loss: 0.10842473804950714\n",
      "Epoch 3959, Loss: 0.4957408607006073, Final Batch Loss: 0.05708341300487518\n",
      "Epoch 3960, Loss: 0.6033253371715546, Final Batch Loss: 0.1821870356798172\n",
      "Epoch 3961, Loss: 0.7216061353683472, Final Batch Loss: 0.3075904846191406\n",
      "Epoch 3962, Loss: 0.7972747683525085, Final Batch Loss: 0.35419124364852905\n",
      "Epoch 3963, Loss: 0.5738144069910049, Final Batch Loss: 0.13263647258281708\n",
      "Epoch 3964, Loss: 0.49782784655690193, Final Batch Loss: 0.051705602556467056\n",
      "Epoch 3965, Loss: 0.7994833737611771, Final Batch Loss: 0.29456958174705505\n",
      "Epoch 3966, Loss: 0.5780426412820816, Final Batch Loss: 0.11984854936599731\n",
      "Epoch 3967, Loss: 0.7281673550605774, Final Batch Loss: 0.2334458976984024\n",
      "Epoch 3968, Loss: 0.6668004989624023, Final Batch Loss: 0.13564087450504303\n",
      "Epoch 3969, Loss: 0.8082273751497269, Final Batch Loss: 0.370016485452652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3970, Loss: 0.5936843901872635, Final Batch Loss: 0.1425592303276062\n",
      "Epoch 3971, Loss: 0.7270622402429581, Final Batch Loss: 0.3307855427265167\n",
      "Epoch 3972, Loss: 0.5215021371841431, Final Batch Loss: 0.15313905477523804\n",
      "Epoch 3973, Loss: 0.5197661370038986, Final Batch Loss: 0.11665581166744232\n",
      "Epoch 3974, Loss: 0.5908165574073792, Final Batch Loss: 0.134630024433136\n",
      "Epoch 3975, Loss: 0.7196303158998489, Final Batch Loss: 0.27026620507240295\n",
      "Epoch 3976, Loss: 0.5470132529735565, Final Batch Loss: 0.1292308270931244\n",
      "Epoch 3977, Loss: 0.5021747574210167, Final Batch Loss: 0.1161159947514534\n",
      "Epoch 3978, Loss: 0.6060133129358292, Final Batch Loss: 0.19156566262245178\n",
      "Epoch 3979, Loss: 0.6127110123634338, Final Batch Loss: 0.21392063796520233\n",
      "Epoch 3980, Loss: 0.6344237625598907, Final Batch Loss: 0.19933557510375977\n",
      "Epoch 3981, Loss: 0.6575249582529068, Final Batch Loss: 0.20155520737171173\n",
      "Epoch 3982, Loss: 0.6747868061065674, Final Batch Loss: 0.2656401991844177\n",
      "Epoch 3983, Loss: 0.5654401257634163, Final Batch Loss: 0.11020579189062119\n",
      "Epoch 3984, Loss: 0.5751779526472092, Final Batch Loss: 0.13774392008781433\n",
      "Epoch 3985, Loss: 0.5148275569081306, Final Batch Loss: 0.09261643141508102\n",
      "Epoch 3986, Loss: 0.632559597492218, Final Batch Loss: 0.23562850058078766\n",
      "Epoch 3987, Loss: 0.8199319392442703, Final Batch Loss: 0.3885074853897095\n",
      "Epoch 3988, Loss: 0.4448637627065182, Final Batch Loss: 0.06056119129061699\n",
      "Epoch 3989, Loss: 0.5198003500699997, Final Batch Loss: 0.11397583782672882\n",
      "Epoch 3990, Loss: 0.6776552647352219, Final Batch Loss: 0.27762076258659363\n",
      "Epoch 3991, Loss: 0.763872891664505, Final Batch Loss: 0.2908321022987366\n",
      "Epoch 3992, Loss: 0.8536455184221268, Final Batch Loss: 0.47906213998794556\n",
      "Epoch 3993, Loss: 0.6252810657024384, Final Batch Loss: 0.24797332286834717\n",
      "Epoch 3994, Loss: 0.579188272356987, Final Batch Loss: 0.19264757633209229\n",
      "Epoch 3995, Loss: 0.7223519831895828, Final Batch Loss: 0.29549849033355713\n",
      "Epoch 3996, Loss: 0.828423023223877, Final Batch Loss: 0.3891866207122803\n",
      "Epoch 3997, Loss: 0.49346550554037094, Final Batch Loss: 0.05806385725736618\n",
      "Epoch 3998, Loss: 0.6330658942461014, Final Batch Loss: 0.2382364273071289\n",
      "Epoch 3999, Loss: 0.5560105815529823, Final Batch Loss: 0.10416131466627121\n",
      "Epoch 4000, Loss: 0.4699135944247246, Final Batch Loss: 0.08880219608545303\n",
      "Epoch 4001, Loss: 0.7055360376834869, Final Batch Loss: 0.3425849974155426\n",
      "Epoch 4002, Loss: 0.4646936282515526, Final Batch Loss: 0.03230343014001846\n",
      "Epoch 4003, Loss: 0.5967071205377579, Final Batch Loss: 0.1946963518857956\n",
      "Epoch 4004, Loss: 0.6849549859762192, Final Batch Loss: 0.25298556685447693\n",
      "Epoch 4005, Loss: 1.053962528705597, Final Batch Loss: 0.6776348352432251\n",
      "Epoch 4006, Loss: 0.6854985356330872, Final Batch Loss: 0.25632381439208984\n",
      "Epoch 4007, Loss: 0.768757164478302, Final Batch Loss: 0.378366082906723\n",
      "Epoch 4008, Loss: 0.6217347681522369, Final Batch Loss: 0.22461645305156708\n",
      "Epoch 4009, Loss: 0.6469827741384506, Final Batch Loss: 0.19843700528144836\n",
      "Epoch 4010, Loss: 0.5623568445444107, Final Batch Loss: 0.14983539283275604\n",
      "Epoch 4011, Loss: 0.5907955095171928, Final Batch Loss: 0.08832652121782303\n",
      "Epoch 4012, Loss: 0.6676708906888962, Final Batch Loss: 0.18648572266101837\n",
      "Epoch 4013, Loss: 0.6228235363960266, Final Batch Loss: 0.23816709220409393\n",
      "Epoch 4014, Loss: 0.6834642142057419, Final Batch Loss: 0.20274336636066437\n",
      "Epoch 4015, Loss: 0.5884095579385757, Final Batch Loss: 0.14788925647735596\n",
      "Epoch 4016, Loss: 0.8579162806272507, Final Batch Loss: 0.4537642002105713\n",
      "Epoch 4017, Loss: 0.6097535490989685, Final Batch Loss: 0.1907031089067459\n",
      "Epoch 4018, Loss: 0.7258834838867188, Final Batch Loss: 0.3086179196834564\n",
      "Epoch 4019, Loss: 0.758019283413887, Final Batch Loss: 0.3211272358894348\n",
      "Epoch 4020, Loss: 0.8288049101829529, Final Batch Loss: 0.42333245277404785\n",
      "Epoch 4021, Loss: 0.5344727784395218, Final Batch Loss: 0.17277094721794128\n",
      "Epoch 4022, Loss: 0.8139190673828125, Final Batch Loss: 0.3756415843963623\n",
      "Epoch 4023, Loss: 0.6091655492782593, Final Batch Loss: 0.16557036340236664\n",
      "Epoch 4024, Loss: 0.6775432229042053, Final Batch Loss: 0.2894715666770935\n",
      "Epoch 4025, Loss: 0.8138881325721741, Final Batch Loss: 0.38296446204185486\n",
      "Epoch 4026, Loss: 0.632887065410614, Final Batch Loss: 0.2721264362335205\n",
      "Epoch 4027, Loss: 0.5036605596542358, Final Batch Loss: 0.08051049709320068\n",
      "Epoch 4028, Loss: 0.5653624087572098, Final Batch Loss: 0.173763707280159\n",
      "Epoch 4029, Loss: 0.5696156620979309, Final Batch Loss: 0.1412501037120819\n",
      "Epoch 4030, Loss: 0.584711566567421, Final Batch Loss: 0.19095651805400848\n",
      "Epoch 4031, Loss: 0.562068447470665, Final Batch Loss: 0.14487972855567932\n",
      "Epoch 4032, Loss: 0.5686454325914383, Final Batch Loss: 0.1774154156446457\n",
      "Epoch 4033, Loss: 0.5071223974227905, Final Batch Loss: 0.11601607501506805\n",
      "Epoch 4034, Loss: 0.5035789459943771, Final Batch Loss: 0.09651525318622589\n",
      "Epoch 4035, Loss: 0.4697456583380699, Final Batch Loss: 0.12025963515043259\n",
      "Epoch 4036, Loss: 0.5415826812386513, Final Batch Loss: 0.11227092891931534\n",
      "Epoch 4037, Loss: 0.5873454213142395, Final Batch Loss: 0.22186201810836792\n",
      "Epoch 4038, Loss: 0.5084025785326958, Final Batch Loss: 0.11792663484811783\n",
      "Epoch 4039, Loss: 0.508459784090519, Final Batch Loss: 0.11080243438482285\n",
      "Epoch 4040, Loss: 0.6080359816551208, Final Batch Loss: 0.10784190893173218\n",
      "Epoch 4041, Loss: 0.5477489084005356, Final Batch Loss: 0.1519274264574051\n",
      "Epoch 4042, Loss: 0.6610962152481079, Final Batch Loss: 0.24644909799098969\n",
      "Epoch 4043, Loss: 0.5297921597957611, Final Batch Loss: 0.09835249185562134\n",
      "Epoch 4044, Loss: 0.5298477932810783, Final Batch Loss: 0.10295633226633072\n",
      "Epoch 4045, Loss: 0.5751152634620667, Final Batch Loss: 0.1484493613243103\n",
      "Epoch 4046, Loss: 0.5969980359077454, Final Batch Loss: 0.2453327178955078\n",
      "Epoch 4047, Loss: 0.5760909542441368, Final Batch Loss: 0.11626579612493515\n",
      "Epoch 4048, Loss: 0.6245311349630356, Final Batch Loss: 0.2540803849697113\n",
      "Epoch 4049, Loss: 0.40191085264086723, Final Batch Loss: 0.04437611624598503\n",
      "Epoch 4050, Loss: 0.46758825704455376, Final Batch Loss: 0.045258622616529465\n",
      "Epoch 4051, Loss: 0.6864386051893234, Final Batch Loss: 0.32904887199401855\n",
      "Epoch 4052, Loss: 0.5099095031619072, Final Batch Loss: 0.08062728494405746\n",
      "Epoch 4053, Loss: 0.6490350514650345, Final Batch Loss: 0.27804797887802124\n",
      "Epoch 4054, Loss: 0.6094813495874405, Final Batch Loss: 0.1704397350549698\n",
      "Epoch 4055, Loss: 0.6609016060829163, Final Batch Loss: 0.2265605926513672\n",
      "Epoch 4056, Loss: 0.5946140289306641, Final Batch Loss: 0.1770334243774414\n",
      "Epoch 4057, Loss: 0.674674317240715, Final Batch Loss: 0.25226396322250366\n",
      "Epoch 4058, Loss: 0.6573956906795502, Final Batch Loss: 0.16965271532535553\n",
      "Epoch 4059, Loss: 0.669767215847969, Final Batch Loss: 0.2559358477592468\n",
      "Epoch 4060, Loss: 0.8112025558948517, Final Batch Loss: 0.38170313835144043\n",
      "Epoch 4061, Loss: 0.623352661728859, Final Batch Loss: 0.2161756306886673\n",
      "Epoch 4062, Loss: 0.6820417195558548, Final Batch Loss: 0.2466667890548706\n",
      "Epoch 4063, Loss: 0.7117170542478561, Final Batch Loss: 0.2594049274921417\n",
      "Epoch 4064, Loss: 0.5665136873722076, Final Batch Loss: 0.2052193582057953\n",
      "Epoch 4065, Loss: 0.6618211567401886, Final Batch Loss: 0.16794486343860626\n",
      "Epoch 4066, Loss: 0.8728149384260178, Final Batch Loss: 0.4959237575531006\n",
      "Epoch 4067, Loss: 0.6760558634996414, Final Batch Loss: 0.315784215927124\n",
      "Epoch 4068, Loss: 0.7186537086963654, Final Batch Loss: 0.23550650477409363\n",
      "Epoch 4069, Loss: 0.610141932964325, Final Batch Loss: 0.24315012991428375\n",
      "Epoch 4070, Loss: 0.5016830116510391, Final Batch Loss: 0.06469994783401489\n",
      "Epoch 4071, Loss: 0.5460958033800125, Final Batch Loss: 0.10309891402721405\n",
      "Epoch 4072, Loss: 0.6888371706008911, Final Batch Loss: 0.2958594262599945\n",
      "Epoch 4073, Loss: 0.646407887339592, Final Batch Loss: 0.16954120993614197\n",
      "Epoch 4074, Loss: 0.5167764946818352, Final Batch Loss: 0.0670788511633873\n",
      "Epoch 4075, Loss: 0.587923213839531, Final Batch Loss: 0.17760667204856873\n",
      "Epoch 4076, Loss: 0.5325035825371742, Final Batch Loss: 0.09008551388978958\n",
      "Epoch 4077, Loss: 0.4945762827992439, Final Batch Loss: 0.11701414734125137\n",
      "Epoch 4078, Loss: 0.5928502082824707, Final Batch Loss: 0.2108813375234604\n",
      "Epoch 4079, Loss: 0.7128220498561859, Final Batch Loss: 0.28650546073913574\n",
      "Epoch 4080, Loss: 0.5761919170618057, Final Batch Loss: 0.12905403971672058\n",
      "Epoch 4081, Loss: 0.6049992889165878, Final Batch Loss: 0.24925066530704498\n",
      "Epoch 4082, Loss: 0.7054678052663803, Final Batch Loss: 0.25058379769325256\n",
      "Epoch 4083, Loss: 0.5126067399978638, Final Batch Loss: 0.14273451268672943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4084, Loss: 0.6174231469631195, Final Batch Loss: 0.20735017955303192\n",
      "Epoch 4085, Loss: 0.5742910802364349, Final Batch Loss: 0.2105216085910797\n",
      "Epoch 4086, Loss: 0.5239660292863846, Final Batch Loss: 0.10913443565368652\n",
      "Epoch 4087, Loss: 0.5193252637982368, Final Batch Loss: 0.07174823433160782\n",
      "Epoch 4088, Loss: 0.6738804280757904, Final Batch Loss: 0.29326650500297546\n",
      "Epoch 4089, Loss: 0.5158053413033485, Final Batch Loss: 0.1107678934931755\n",
      "Epoch 4090, Loss: 0.5972528755664825, Final Batch Loss: 0.20591427385807037\n",
      "Epoch 4091, Loss: 1.2633021771907806, Final Batch Loss: 0.7702118754386902\n",
      "Epoch 4092, Loss: 0.5933397561311722, Final Batch Loss: 0.1508931666612625\n",
      "Epoch 4093, Loss: 0.5085155144333839, Final Batch Loss: 0.09638649970293045\n",
      "Epoch 4094, Loss: 0.9105749875307083, Final Batch Loss: 0.5196416974067688\n",
      "Epoch 4095, Loss: 0.5651775002479553, Final Batch Loss: 0.1636379510164261\n",
      "Epoch 4096, Loss: 0.5489993989467621, Final Batch Loss: 0.12810300290584564\n",
      "Epoch 4097, Loss: 0.5204723477363586, Final Batch Loss: 0.09538595378398895\n",
      "Epoch 4098, Loss: 0.4586259238421917, Final Batch Loss: 0.04704883322119713\n",
      "Epoch 4099, Loss: 0.4216684550046921, Final Batch Loss: 0.07315607368946075\n",
      "Epoch 4100, Loss: 0.515995591878891, Final Batch Loss: 0.11577397584915161\n",
      "Epoch 4101, Loss: 0.4519058670848608, Final Batch Loss: 0.02490243874490261\n",
      "Epoch 4102, Loss: 0.731969878077507, Final Batch Loss: 0.37025246024131775\n",
      "Epoch 4103, Loss: 0.5459014177322388, Final Batch Loss: 0.1406279355287552\n",
      "Epoch 4104, Loss: 0.44394709542393684, Final Batch Loss: 0.05160244181752205\n",
      "Epoch 4105, Loss: 0.6166940182447433, Final Batch Loss: 0.18101592361927032\n",
      "Epoch 4106, Loss: 0.6138522624969482, Final Batch Loss: 0.19038981199264526\n",
      "Epoch 4107, Loss: 0.4779592379927635, Final Batch Loss: 0.0792907252907753\n",
      "Epoch 4108, Loss: 0.6098677664995193, Final Batch Loss: 0.26826468110084534\n",
      "Epoch 4109, Loss: 0.5863970071077347, Final Batch Loss: 0.19827552139759064\n",
      "Epoch 4110, Loss: 0.7275514900684357, Final Batch Loss: 0.3056328594684601\n",
      "Epoch 4111, Loss: 0.6563044637441635, Final Batch Loss: 0.2584575414657593\n",
      "Epoch 4112, Loss: 0.6098615229129791, Final Batch Loss: 0.250173956155777\n",
      "Epoch 4113, Loss: 0.5993173569440842, Final Batch Loss: 0.16066458821296692\n",
      "Epoch 4114, Loss: 0.6370357275009155, Final Batch Loss: 0.23985327780246735\n",
      "Epoch 4115, Loss: 0.6301800757646561, Final Batch Loss: 0.23695258796215057\n",
      "Epoch 4116, Loss: 0.5675741583108902, Final Batch Loss: 0.14516916871070862\n",
      "Epoch 4117, Loss: 0.6842773705720901, Final Batch Loss: 0.2385774701833725\n",
      "Epoch 4118, Loss: 0.5025009289383888, Final Batch Loss: 0.07342729717493057\n",
      "Epoch 4119, Loss: 0.7918614894151688, Final Batch Loss: 0.37911391258239746\n",
      "Epoch 4120, Loss: 0.7327065169811249, Final Batch Loss: 0.26343801617622375\n",
      "Epoch 4121, Loss: 0.7384616732597351, Final Batch Loss: 0.35042837262153625\n",
      "Epoch 4122, Loss: 0.6455980092287064, Final Batch Loss: 0.27054300904273987\n",
      "Epoch 4123, Loss: 0.6837640106678009, Final Batch Loss: 0.2763753831386566\n",
      "Epoch 4124, Loss: 0.46426502615213394, Final Batch Loss: 0.0528925284743309\n",
      "Epoch 4125, Loss: 0.6290760785341263, Final Batch Loss: 0.23434430360794067\n",
      "Epoch 4126, Loss: 0.7070614248514175, Final Batch Loss: 0.27703869342803955\n",
      "Epoch 4127, Loss: 0.5544042885303497, Final Batch Loss: 0.17580485343933105\n",
      "Epoch 4128, Loss: 0.5937867015600204, Final Batch Loss: 0.1679438054561615\n",
      "Epoch 4129, Loss: 0.6700267493724823, Final Batch Loss: 0.23904390633106232\n",
      "Epoch 4130, Loss: 0.631358191370964, Final Batch Loss: 0.17358063161373138\n",
      "Epoch 4131, Loss: 0.7815429717302322, Final Batch Loss: 0.3220502436161041\n",
      "Epoch 4132, Loss: 0.7690694630146027, Final Batch Loss: 0.3522462844848633\n",
      "Epoch 4133, Loss: 0.6189511269330978, Final Batch Loss: 0.21134471893310547\n",
      "Epoch 4134, Loss: 0.5995647609233856, Final Batch Loss: 0.21934692561626434\n",
      "Epoch 4135, Loss: 0.5139489620923996, Final Batch Loss: 0.13699278235435486\n",
      "Epoch 4136, Loss: 0.5823243260383606, Final Batch Loss: 0.1962658166885376\n",
      "Epoch 4137, Loss: 0.47044508904218674, Final Batch Loss: 0.0838427022099495\n",
      "Epoch 4138, Loss: 0.5554174333810806, Final Batch Loss: 0.17674995958805084\n",
      "Epoch 4139, Loss: 0.5941024124622345, Final Batch Loss: 0.20734621584415436\n",
      "Epoch 4140, Loss: 0.6266947984695435, Final Batch Loss: 0.2558192312717438\n",
      "Epoch 4141, Loss: 0.7343286573886871, Final Batch Loss: 0.27236151695251465\n",
      "Epoch 4142, Loss: 0.9922242760658264, Final Batch Loss: 0.5453174114227295\n",
      "Epoch 4143, Loss: 0.7318337112665176, Final Batch Loss: 0.31674736738204956\n",
      "Epoch 4144, Loss: 0.458259753882885, Final Batch Loss: 0.0969613716006279\n",
      "Epoch 4145, Loss: 0.7861323952674866, Final Batch Loss: 0.33913785219192505\n",
      "Epoch 4146, Loss: 0.6165056973695755, Final Batch Loss: 0.2044840008020401\n",
      "Epoch 4147, Loss: 0.7284576445817947, Final Batch Loss: 0.2876809239387512\n",
      "Epoch 4148, Loss: 0.5737139731645584, Final Batch Loss: 0.1771109700202942\n",
      "Epoch 4149, Loss: 0.7292566150426865, Final Batch Loss: 0.35305947065353394\n",
      "Epoch 4150, Loss: 0.8502485603094101, Final Batch Loss: 0.3765375316143036\n",
      "Epoch 4151, Loss: 0.544606126844883, Final Batch Loss: 0.06969795376062393\n",
      "Epoch 4152, Loss: 0.44965963438153267, Final Batch Loss: 0.034641627222299576\n",
      "Epoch 4153, Loss: 0.6260232031345367, Final Batch Loss: 0.22319118678569794\n",
      "Epoch 4154, Loss: 0.7506747841835022, Final Batch Loss: 0.30506643652915955\n",
      "Epoch 4155, Loss: 0.6342158615589142, Final Batch Loss: 0.2561008632183075\n",
      "Epoch 4156, Loss: 0.4783160984516144, Final Batch Loss: 0.13180598616600037\n",
      "Epoch 4157, Loss: 0.7304679304361343, Final Batch Loss: 0.32285696268081665\n",
      "Epoch 4158, Loss: 0.5778602063655853, Final Batch Loss: 0.2024441808462143\n",
      "Epoch 4159, Loss: 0.5291502922773361, Final Batch Loss: 0.15634383261203766\n",
      "Epoch 4160, Loss: 0.4756874516606331, Final Batch Loss: 0.06297192722558975\n",
      "Epoch 4161, Loss: 0.571520134806633, Final Batch Loss: 0.17059564590454102\n",
      "Epoch 4162, Loss: 0.5890282988548279, Final Batch Loss: 0.17946834862232208\n",
      "Epoch 4163, Loss: 0.6534571945667267, Final Batch Loss: 0.21972548961639404\n",
      "Epoch 4164, Loss: 0.508630283176899, Final Batch Loss: 0.08361727744340897\n",
      "Epoch 4165, Loss: 0.5754049271345139, Final Batch Loss: 0.18064452707767487\n",
      "Epoch 4166, Loss: 0.5960736572742462, Final Batch Loss: 0.2509422302246094\n",
      "Epoch 4167, Loss: 0.48451612144708633, Final Batch Loss: 0.11334972828626633\n",
      "Epoch 4168, Loss: 0.4824618250131607, Final Batch Loss: 0.1334083378314972\n",
      "Epoch 4169, Loss: 0.47478944808244705, Final Batch Loss: 0.08218870311975479\n",
      "Epoch 4170, Loss: 0.5587762296199799, Final Batch Loss: 0.18993046879768372\n",
      "Epoch 4171, Loss: 0.5090114325284958, Final Batch Loss: 0.10982020199298859\n",
      "Epoch 4172, Loss: 0.7338141947984695, Final Batch Loss: 0.3161538243293762\n",
      "Epoch 4173, Loss: 0.6797849386930466, Final Batch Loss: 0.2625534236431122\n",
      "Epoch 4174, Loss: 0.4885946214199066, Final Batch Loss: 0.03398051857948303\n",
      "Epoch 4175, Loss: 0.6899368017911911, Final Batch Loss: 0.27585887908935547\n",
      "Epoch 4176, Loss: 0.609001874923706, Final Batch Loss: 0.22382420301437378\n",
      "Epoch 4177, Loss: 0.7504153251647949, Final Batch Loss: 0.4159446358680725\n",
      "Epoch 4178, Loss: 0.5021906718611717, Final Batch Loss: 0.12179035693407059\n",
      "Epoch 4179, Loss: 0.44750242680311203, Final Batch Loss: 0.12023492902517319\n",
      "Epoch 4180, Loss: 0.6878801137208939, Final Batch Loss: 0.2367812991142273\n",
      "Epoch 4181, Loss: 0.5304756984114647, Final Batch Loss: 0.07486722618341446\n",
      "Epoch 4182, Loss: 0.461160272359848, Final Batch Loss: 0.10720513761043549\n",
      "Epoch 4183, Loss: 0.5894556492567062, Final Batch Loss: 0.20677651464939117\n",
      "Epoch 4184, Loss: 0.6663976460695267, Final Batch Loss: 0.2817109525203705\n",
      "Epoch 4185, Loss: 0.5129296258091927, Final Batch Loss: 0.10111580044031143\n",
      "Epoch 4186, Loss: 0.6300166696310043, Final Batch Loss: 0.29194942116737366\n",
      "Epoch 4187, Loss: 0.5528993308544159, Final Batch Loss: 0.1794358640909195\n",
      "Epoch 4188, Loss: 0.6223046332597733, Final Batch Loss: 0.26551553606987\n",
      "Epoch 4189, Loss: 0.6499227434396744, Final Batch Loss: 0.25747647881507874\n",
      "Epoch 4190, Loss: 0.48111411929130554, Final Batch Loss: 0.07493472099304199\n",
      "Epoch 4191, Loss: 0.5064200162887573, Final Batch Loss: 0.13901038467884064\n",
      "Epoch 4192, Loss: 0.913128450512886, Final Batch Loss: 0.5705109238624573\n",
      "Epoch 4193, Loss: 0.5173172056674957, Final Batch Loss: 0.06858845055103302\n",
      "Epoch 4194, Loss: 0.5668502151966095, Final Batch Loss: 0.18698325753211975\n",
      "Epoch 4195, Loss: 0.5266745984554291, Final Batch Loss: 0.11918497085571289\n",
      "Epoch 4196, Loss: 0.45489683002233505, Final Batch Loss: 0.06401241570711136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4197, Loss: 0.5483764261007309, Final Batch Loss: 0.17152859270572662\n",
      "Epoch 4198, Loss: 0.728663370013237, Final Batch Loss: 0.3838988244533539\n",
      "Epoch 4199, Loss: 0.47582298517227173, Final Batch Loss: 0.08023695647716522\n",
      "Epoch 4200, Loss: 0.5734476894140244, Final Batch Loss: 0.13507631421089172\n",
      "Epoch 4201, Loss: 0.6143457740545273, Final Batch Loss: 0.2782410681247711\n",
      "Epoch 4202, Loss: 0.6864828020334244, Final Batch Loss: 0.24334684014320374\n",
      "Epoch 4203, Loss: 0.5776292532682419, Final Batch Loss: 0.1797875165939331\n",
      "Epoch 4204, Loss: 0.5585960149765015, Final Batch Loss: 0.20221047103405\n",
      "Epoch 4205, Loss: 0.507631927728653, Final Batch Loss: 0.13588744401931763\n",
      "Epoch 4206, Loss: 0.4793340861797333, Final Batch Loss: 0.1280404031276703\n",
      "Epoch 4207, Loss: 0.7463009506464005, Final Batch Loss: 0.30311068892478943\n",
      "Epoch 4208, Loss: 0.4312754385173321, Final Batch Loss: 0.05276454612612724\n",
      "Epoch 4209, Loss: 0.9732839316129684, Final Batch Loss: 0.6139168739318848\n",
      "Epoch 4210, Loss: 0.6507645100355148, Final Batch Loss: 0.21020254492759705\n",
      "Epoch 4211, Loss: 0.5717693418264389, Final Batch Loss: 0.18009532988071442\n",
      "Epoch 4212, Loss: 0.6030481159687042, Final Batch Loss: 0.20787683129310608\n",
      "Epoch 4213, Loss: 0.6689744293689728, Final Batch Loss: 0.22782260179519653\n",
      "Epoch 4214, Loss: 0.5147926211357117, Final Batch Loss: 0.09040799736976624\n",
      "Epoch 4215, Loss: 0.6190816909074783, Final Batch Loss: 0.18674536049365997\n",
      "Epoch 4216, Loss: 0.5624604672193527, Final Batch Loss: 0.13430257141590118\n",
      "Epoch 4217, Loss: 0.533823736011982, Final Batch Loss: 0.111284039914608\n",
      "Epoch 4218, Loss: 0.8414631485939026, Final Batch Loss: 0.40828266739845276\n",
      "Epoch 4219, Loss: 0.6822595447301865, Final Batch Loss: 0.27942705154418945\n",
      "Epoch 4220, Loss: 0.461517795920372, Final Batch Loss: 0.07711514830589294\n",
      "Epoch 4221, Loss: 0.7097897976636887, Final Batch Loss: 0.3093673586845398\n",
      "Epoch 4222, Loss: 0.5575241297483444, Final Batch Loss: 0.15466979146003723\n",
      "Epoch 4223, Loss: 0.48608139902353287, Final Batch Loss: 0.025101251900196075\n",
      "Epoch 4224, Loss: 0.554992213845253, Final Batch Loss: 0.11960774660110474\n",
      "Epoch 4225, Loss: 0.4492923393845558, Final Batch Loss: 0.06372179836034775\n",
      "Epoch 4226, Loss: 0.5833591371774673, Final Batch Loss: 0.18355678021907806\n",
      "Epoch 4227, Loss: 0.5187277942895889, Final Batch Loss: 0.08340361714363098\n",
      "Epoch 4228, Loss: 0.44742531701922417, Final Batch Loss: 0.05543289706110954\n",
      "Epoch 4229, Loss: 0.4143703579902649, Final Batch Loss: 0.027502581477165222\n",
      "Epoch 4230, Loss: 0.4607226401567459, Final Batch Loss: 0.09200219810009003\n",
      "Epoch 4231, Loss: 0.5245116800069809, Final Batch Loss: 0.14014768600463867\n",
      "Epoch 4232, Loss: 0.4091687295585871, Final Batch Loss: 0.028477126732468605\n",
      "Epoch 4233, Loss: 0.553575411438942, Final Batch Loss: 0.2254970222711563\n",
      "Epoch 4234, Loss: 0.47933322191238403, Final Batch Loss: 0.09954813122749329\n",
      "Epoch 4235, Loss: 0.4895472154021263, Final Batch Loss: 0.07416991144418716\n",
      "Epoch 4236, Loss: 0.5483983308076859, Final Batch Loss: 0.15176640450954437\n",
      "Epoch 4237, Loss: 0.5293549001216888, Final Batch Loss: 0.15529778599739075\n",
      "Epoch 4238, Loss: 0.399259589612484, Final Batch Loss: 0.04085887223482132\n",
      "Epoch 4239, Loss: 0.48504436016082764, Final Batch Loss: 0.12677785754203796\n",
      "Epoch 4240, Loss: 0.5209665149450302, Final Batch Loss: 0.14186805486679077\n",
      "Epoch 4241, Loss: 0.6741019189357758, Final Batch Loss: 0.3010447025299072\n",
      "Epoch 4242, Loss: 0.579307422041893, Final Batch Loss: 0.1400315910577774\n",
      "Epoch 4243, Loss: 0.5807594805955887, Final Batch Loss: 0.13899007439613342\n",
      "Epoch 4244, Loss: 0.6891117542982101, Final Batch Loss: 0.28247779607772827\n",
      "Epoch 4245, Loss: 0.5712302923202515, Final Batch Loss: 0.18516047298908234\n",
      "Epoch 4246, Loss: 0.5947103798389435, Final Batch Loss: 0.14611856639385223\n",
      "Epoch 4247, Loss: 0.4692387357354164, Final Batch Loss: 0.08005679398775101\n",
      "Epoch 4248, Loss: 0.584155797958374, Final Batch Loss: 0.17334498465061188\n",
      "Epoch 4249, Loss: 0.4829036816954613, Final Batch Loss: 0.11304908245801926\n",
      "Epoch 4250, Loss: 0.6068730354309082, Final Batch Loss: 0.23941858112812042\n",
      "Epoch 4251, Loss: 0.5433243066072464, Final Batch Loss: 0.14873600006103516\n",
      "Epoch 4252, Loss: 0.6943075507879257, Final Batch Loss: 0.3269157409667969\n",
      "Epoch 4253, Loss: 0.5233077555894852, Final Batch Loss: 0.14809222519397736\n",
      "Epoch 4254, Loss: 0.7263263761997223, Final Batch Loss: 0.31505274772644043\n",
      "Epoch 4255, Loss: 0.45578113943338394, Final Batch Loss: 0.08363459259271622\n",
      "Epoch 4256, Loss: 0.565206840634346, Final Batch Loss: 0.1591404676437378\n",
      "Epoch 4257, Loss: 0.5991325378417969, Final Batch Loss: 0.271813303232193\n",
      "Epoch 4258, Loss: 0.4698711112141609, Final Batch Loss: 0.11635050922632217\n",
      "Epoch 4259, Loss: 0.4136818051338196, Final Batch Loss: 0.05473990738391876\n",
      "Epoch 4260, Loss: 0.686980202794075, Final Batch Loss: 0.28789958357810974\n",
      "Epoch 4261, Loss: 0.6787222027778625, Final Batch Loss: 0.3082723617553711\n",
      "Epoch 4262, Loss: 0.46976299583911896, Final Batch Loss: 0.07886391878128052\n",
      "Epoch 4263, Loss: 0.5850528627634048, Final Batch Loss: 0.2141667902469635\n",
      "Epoch 4264, Loss: 0.6274295002222061, Final Batch Loss: 0.17114634811878204\n",
      "Epoch 4265, Loss: 0.6447103321552277, Final Batch Loss: 0.22306907176971436\n",
      "Epoch 4266, Loss: 0.5884076654911041, Final Batch Loss: 0.1251654326915741\n",
      "Epoch 4267, Loss: 0.4827481210231781, Final Batch Loss: 0.1283286064863205\n",
      "Epoch 4268, Loss: 0.4673147350549698, Final Batch Loss: 0.1025126576423645\n",
      "Epoch 4269, Loss: 0.5834795087575912, Final Batch Loss: 0.2141665667295456\n",
      "Epoch 4270, Loss: 0.4795592799782753, Final Batch Loss: 0.08950763195753098\n",
      "Epoch 4271, Loss: 0.5641433000564575, Final Batch Loss: 0.13514263927936554\n",
      "Epoch 4272, Loss: 0.4634874165058136, Final Batch Loss: 0.06972712278366089\n",
      "Epoch 4273, Loss: 0.645364448428154, Final Batch Loss: 0.21876220405101776\n",
      "Epoch 4274, Loss: 0.501300036907196, Final Batch Loss: 0.11419445276260376\n",
      "Epoch 4275, Loss: 0.41070762276649475, Final Batch Loss: 0.031577616930007935\n",
      "Epoch 4276, Loss: 0.5589350759983063, Final Batch Loss: 0.06861244142055511\n",
      "Epoch 4277, Loss: 0.44332652539014816, Final Batch Loss: 0.06603416055440903\n",
      "Epoch 4278, Loss: 0.5917652547359467, Final Batch Loss: 0.2090538591146469\n",
      "Epoch 4279, Loss: 0.3757522199302912, Final Batch Loss: 0.017579609528183937\n",
      "Epoch 4280, Loss: 0.7130230814218521, Final Batch Loss: 0.20929476618766785\n",
      "Epoch 4281, Loss: 0.6204283684492111, Final Batch Loss: 0.1972266584634781\n",
      "Epoch 4282, Loss: 0.5873015522956848, Final Batch Loss: 0.16180972754955292\n",
      "Epoch 4283, Loss: 0.5090971142053604, Final Batch Loss: 0.12645280361175537\n",
      "Epoch 4284, Loss: 0.5731485038995743, Final Batch Loss: 0.17463067173957825\n",
      "Epoch 4285, Loss: 0.6452916264533997, Final Batch Loss: 0.14942717552185059\n",
      "Epoch 4286, Loss: 0.5969090759754181, Final Batch Loss: 0.13198202848434448\n",
      "Epoch 4287, Loss: 0.4953277260065079, Final Batch Loss: 0.13395829498767853\n",
      "Epoch 4288, Loss: 0.9575749337673187, Final Batch Loss: 0.5287540555000305\n",
      "Epoch 4289, Loss: 0.5304087921977043, Final Batch Loss: 0.08863923698663712\n",
      "Epoch 4290, Loss: 0.49052657932043076, Final Batch Loss: 0.0999017283320427\n",
      "Epoch 4291, Loss: 0.6849986016750336, Final Batch Loss: 0.2921016216278076\n",
      "Epoch 4292, Loss: 0.4912417232990265, Final Batch Loss: 0.1348089724779129\n",
      "Epoch 4293, Loss: 0.5185920000076294, Final Batch Loss: 0.0964234471321106\n",
      "Epoch 4294, Loss: 0.84172023832798, Final Batch Loss: 0.39760035276412964\n",
      "Epoch 4295, Loss: 0.5796005576848984, Final Batch Loss: 0.20701685547828674\n",
      "Epoch 4296, Loss: 0.6217064559459686, Final Batch Loss: 0.2593694031238556\n",
      "Epoch 4297, Loss: 0.7308664917945862, Final Batch Loss: 0.3220708668231964\n",
      "Epoch 4298, Loss: 0.747483104467392, Final Batch Loss: 0.315263032913208\n",
      "Epoch 4299, Loss: 0.5252775847911835, Final Batch Loss: 0.14159494638442993\n",
      "Epoch 4300, Loss: 0.4837242439389229, Final Batch Loss: 0.06228546053171158\n",
      "Epoch 4301, Loss: 0.48786623775959015, Final Batch Loss: 0.1160174310207367\n",
      "Epoch 4302, Loss: 0.5050192177295685, Final Batch Loss: 0.16265398263931274\n",
      "Epoch 4303, Loss: 0.6059152483940125, Final Batch Loss: 0.23695288598537445\n",
      "Epoch 4304, Loss: 0.6816840618848801, Final Batch Loss: 0.268047958612442\n",
      "Epoch 4305, Loss: 0.656083270907402, Final Batch Loss: 0.22400858998298645\n",
      "Epoch 4306, Loss: 0.40100062591955066, Final Batch Loss: 0.00758517486974597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4307, Loss: 0.4894314408302307, Final Batch Loss: 0.11500094830989838\n",
      "Epoch 4308, Loss: 0.4789011999964714, Final Batch Loss: 0.0657973513007164\n",
      "Epoch 4309, Loss: 0.848340317606926, Final Batch Loss: 0.42620524764060974\n",
      "Epoch 4310, Loss: 0.6246431618928909, Final Batch Loss: 0.21894267201423645\n",
      "Epoch 4311, Loss: 0.8040285259485245, Final Batch Loss: 0.40319862961769104\n",
      "Epoch 4312, Loss: 0.6013142168521881, Final Batch Loss: 0.16713784635066986\n",
      "Epoch 4313, Loss: 0.6347603648900986, Final Batch Loss: 0.1744089126586914\n",
      "Epoch 4314, Loss: 0.4634440205991268, Final Batch Loss: 0.05876928195357323\n",
      "Epoch 4315, Loss: 0.49626919627189636, Final Batch Loss: 0.10305289924144745\n",
      "Epoch 4316, Loss: 0.41270124539732933, Final Batch Loss: 0.05007728561758995\n",
      "Epoch 4317, Loss: 0.6513612568378448, Final Batch Loss: 0.27391552925109863\n",
      "Epoch 4318, Loss: 0.4934235215187073, Final Batch Loss: 0.09852755069732666\n",
      "Epoch 4319, Loss: 0.5462040603160858, Final Batch Loss: 0.17124469578266144\n",
      "Epoch 4320, Loss: 0.5255774781107903, Final Batch Loss: 0.10220984369516373\n",
      "Epoch 4321, Loss: 0.7527004331350327, Final Batch Loss: 0.35875242948532104\n",
      "Epoch 4322, Loss: 0.501817062497139, Final Batch Loss: 0.10970240831375122\n",
      "Epoch 4323, Loss: 0.5511163771152496, Final Batch Loss: 0.17495311796665192\n",
      "Epoch 4324, Loss: 0.5606435239315033, Final Batch Loss: 0.15354984998703003\n",
      "Epoch 4325, Loss: 0.4168494679033756, Final Batch Loss: 0.03140485659241676\n",
      "Epoch 4326, Loss: 0.542506992816925, Final Batch Loss: 0.17502446472644806\n",
      "Epoch 4327, Loss: 0.47835513949394226, Final Batch Loss: 0.14356540143489838\n",
      "Epoch 4328, Loss: 0.6365639865398407, Final Batch Loss: 0.17633669078350067\n",
      "Epoch 4329, Loss: 0.6525306105613708, Final Batch Loss: 0.13229821622371674\n",
      "Epoch 4330, Loss: 0.5763019770383835, Final Batch Loss: 0.13614626228809357\n",
      "Epoch 4331, Loss: 0.5810214877128601, Final Batch Loss: 0.15378618240356445\n",
      "Epoch 4332, Loss: 0.6215357780456543, Final Batch Loss: 0.19543324410915375\n",
      "Epoch 4333, Loss: 0.5229038000106812, Final Batch Loss: 0.1275157928466797\n",
      "Epoch 4334, Loss: 0.9294538497924805, Final Batch Loss: 0.516202449798584\n",
      "Epoch 4335, Loss: 0.5339947417378426, Final Batch Loss: 0.10889079421758652\n",
      "Epoch 4336, Loss: 0.6211765855550766, Final Batch Loss: 0.17658290266990662\n",
      "Epoch 4337, Loss: 0.6657249480485916, Final Batch Loss: 0.22279511392116547\n",
      "Epoch 4338, Loss: 0.5898144394159317, Final Batch Loss: 0.19069847464561462\n",
      "Epoch 4339, Loss: 0.4420717731118202, Final Batch Loss: 0.07512908428907394\n",
      "Epoch 4340, Loss: 0.5932583808898926, Final Batch Loss: 0.1493881791830063\n",
      "Epoch 4341, Loss: 0.7856128662824631, Final Batch Loss: 0.3656095564365387\n",
      "Epoch 4342, Loss: 0.3771016113460064, Final Batch Loss: 0.03131831809878349\n",
      "Epoch 4343, Loss: 0.468413881957531, Final Batch Loss: 0.06590283662080765\n",
      "Epoch 4344, Loss: 0.5722568929195404, Final Batch Loss: 0.22114881873130798\n",
      "Epoch 4345, Loss: 0.44695159047842026, Final Batch Loss: 0.013001225888729095\n",
      "Epoch 4346, Loss: 0.609919860959053, Final Batch Loss: 0.21739698946475983\n",
      "Epoch 4347, Loss: 1.0888247191905975, Final Batch Loss: 0.7098393440246582\n",
      "Epoch 4348, Loss: 0.6675038784742355, Final Batch Loss: 0.31430238485336304\n",
      "Epoch 4349, Loss: 0.6154139786958694, Final Batch Loss: 0.1926499605178833\n",
      "Epoch 4350, Loss: 0.7004137188196182, Final Batch Loss: 0.3393895626068115\n",
      "Epoch 4351, Loss: 0.5105775035917759, Final Batch Loss: 0.05298759415745735\n",
      "Epoch 4352, Loss: 0.6846366226673126, Final Batch Loss: 0.2792983949184418\n",
      "Epoch 4353, Loss: 0.7164416313171387, Final Batch Loss: 0.24625790119171143\n",
      "Epoch 4354, Loss: 0.5889330208301544, Final Batch Loss: 0.1610490381717682\n",
      "Epoch 4355, Loss: 0.4571413919329643, Final Batch Loss: 0.09540628641843796\n",
      "Epoch 4356, Loss: 0.7343956232070923, Final Batch Loss: 0.329520046710968\n",
      "Epoch 4357, Loss: 0.7382950782775879, Final Batch Loss: 0.34865841269493103\n",
      "Epoch 4358, Loss: 0.6235625743865967, Final Batch Loss: 0.1686612218618393\n",
      "Epoch 4359, Loss: 0.5599572733044624, Final Batch Loss: 0.10962673276662827\n",
      "Epoch 4360, Loss: 0.4775748811662197, Final Batch Loss: 0.05810112878680229\n",
      "Epoch 4361, Loss: 0.7715540677309036, Final Batch Loss: 0.3168184459209442\n",
      "Epoch 4362, Loss: 0.80259969830513, Final Batch Loss: 0.41314059495925903\n",
      "Epoch 4363, Loss: 0.4268762655556202, Final Batch Loss: 0.036430809646844864\n",
      "Epoch 4364, Loss: 0.6276975721120834, Final Batch Loss: 0.24217037856578827\n",
      "Epoch 4365, Loss: 0.4170483574271202, Final Batch Loss: 0.10233426839113235\n",
      "Epoch 4366, Loss: 0.723090648651123, Final Batch Loss: 0.26851901412010193\n",
      "Epoch 4367, Loss: 0.5639201700687408, Final Batch Loss: 0.14571265876293182\n",
      "Epoch 4368, Loss: 0.5457238703966141, Final Batch Loss: 0.14361584186553955\n",
      "Epoch 4369, Loss: 0.5094892680644989, Final Batch Loss: 0.1404879242181778\n",
      "Epoch 4370, Loss: 0.5174128636717796, Final Batch Loss: 0.10529568046331406\n",
      "Epoch 4371, Loss: 0.5741018503904343, Final Batch Loss: 0.1683170646429062\n",
      "Epoch 4372, Loss: 0.5728903859853745, Final Batch Loss: 0.13213828206062317\n",
      "Epoch 4373, Loss: 0.5636779516935349, Final Batch Loss: 0.17392712831497192\n",
      "Epoch 4374, Loss: 0.5561328530311584, Final Batch Loss: 0.15802811086177826\n",
      "Epoch 4375, Loss: 0.5329065471887589, Final Batch Loss: 0.16159100830554962\n",
      "Epoch 4376, Loss: 0.48219896852970123, Final Batch Loss: 0.14656610786914825\n",
      "Epoch 4377, Loss: 0.48204009234905243, Final Batch Loss: 0.1292460411787033\n",
      "Epoch 4378, Loss: 0.451435424387455, Final Batch Loss: 0.09583204239606857\n",
      "Epoch 4379, Loss: 0.46373918652534485, Final Batch Loss: 0.10416620969772339\n",
      "Epoch 4380, Loss: 0.592633068561554, Final Batch Loss: 0.20941460132598877\n",
      "Epoch 4381, Loss: 0.41711629927158356, Final Batch Loss: 0.038381338119506836\n",
      "Epoch 4382, Loss: 0.7089312523603439, Final Batch Loss: 0.3076364994049072\n",
      "Epoch 4383, Loss: 0.6188794523477554, Final Batch Loss: 0.2196999341249466\n",
      "Epoch 4384, Loss: 0.4522254541516304, Final Batch Loss: 0.0671801045536995\n",
      "Epoch 4385, Loss: 0.6214020252227783, Final Batch Loss: 0.2551943361759186\n",
      "Epoch 4386, Loss: 0.46415963768959045, Final Batch Loss: 0.12454000115394592\n",
      "Epoch 4387, Loss: 0.873240128159523, Final Batch Loss: 0.4680345356464386\n",
      "Epoch 4388, Loss: 0.7467970550060272, Final Batch Loss: 0.3447627127170563\n",
      "Epoch 4389, Loss: 0.7208268642425537, Final Batch Loss: 0.38753488659858704\n",
      "Epoch 4390, Loss: 0.5209380388259888, Final Batch Loss: 0.11837427318096161\n",
      "Epoch 4391, Loss: 0.6326537877321243, Final Batch Loss: 0.20855091512203217\n",
      "Epoch 4392, Loss: 0.7706222832202911, Final Batch Loss: 0.4070062041282654\n",
      "Epoch 4393, Loss: 0.8424807786941528, Final Batch Loss: 0.4516700506210327\n",
      "Epoch 4394, Loss: 0.6936443150043488, Final Batch Loss: 0.2973369061946869\n",
      "Epoch 4395, Loss: 0.5696281269192696, Final Batch Loss: 0.11823604255914688\n",
      "Epoch 4396, Loss: 0.6184581033885479, Final Batch Loss: 0.05671639367938042\n",
      "Epoch 4397, Loss: 0.6414733901619911, Final Batch Loss: 0.09144432097673416\n",
      "Epoch 4398, Loss: 0.7709620147943497, Final Batch Loss: 0.24817001819610596\n",
      "Epoch 4399, Loss: 0.5400091484189034, Final Batch Loss: 0.04468735307455063\n",
      "Epoch 4400, Loss: 0.5973097681999207, Final Batch Loss: 0.13454608619213104\n",
      "Epoch 4401, Loss: 0.5728327035903931, Final Batch Loss: 0.12564633786678314\n",
      "Epoch 4402, Loss: 0.5104713141918182, Final Batch Loss: 0.12984682619571686\n",
      "Epoch 4403, Loss: 0.6432237774133682, Final Batch Loss: 0.2108711153268814\n",
      "Epoch 4404, Loss: 0.5462778136134148, Final Batch Loss: 0.10771343857049942\n",
      "Epoch 4405, Loss: 0.6131787449121475, Final Batch Loss: 0.21365132927894592\n",
      "Epoch 4406, Loss: 0.552596241235733, Final Batch Loss: 0.08622805774211884\n",
      "Epoch 4407, Loss: 0.5972619652748108, Final Batch Loss: 0.18048273026943207\n",
      "Epoch 4408, Loss: 0.6575175374746323, Final Batch Loss: 0.1664518415927887\n",
      "Epoch 4409, Loss: 0.5105574950575829, Final Batch Loss: 0.08815912157297134\n",
      "Epoch 4410, Loss: 0.7345005720853806, Final Batch Loss: 0.36002644896507263\n",
      "Epoch 4411, Loss: 0.5860657542943954, Final Batch Loss: 0.19703052937984467\n",
      "Epoch 4412, Loss: 0.4258123077452183, Final Batch Loss: 0.04699491336941719\n",
      "Epoch 4413, Loss: 0.37772439420223236, Final Batch Loss: 0.06970113515853882\n",
      "Epoch 4414, Loss: 0.505023330450058, Final Batch Loss: 0.09767156839370728\n",
      "Epoch 4415, Loss: 0.5079039111733437, Final Batch Loss: 0.08878814429044724\n",
      "Epoch 4416, Loss: 0.6846552491188049, Final Batch Loss: 0.3210258483886719\n",
      "Epoch 4417, Loss: 0.634245365858078, Final Batch Loss: 0.2136480212211609\n",
      "Epoch 4418, Loss: 0.62106953561306, Final Batch Loss: 0.20170828700065613\n",
      "Epoch 4419, Loss: 0.5390868484973907, Final Batch Loss: 0.15048213303089142\n",
      "Epoch 4420, Loss: 0.49244776368141174, Final Batch Loss: 0.11620068550109863\n",
      "Epoch 4421, Loss: 0.5511951148509979, Final Batch Loss: 0.16169215738773346\n",
      "Epoch 4422, Loss: 0.44338487833738327, Final Batch Loss: 0.10369990020990372\n",
      "Epoch 4423, Loss: 0.7924033403396606, Final Batch Loss: 0.3407785892486572\n",
      "Epoch 4424, Loss: 0.6235599964857101, Final Batch Loss: 0.21832463145256042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4425, Loss: 0.5287074595689774, Final Batch Loss: 0.13511493802070618\n",
      "Epoch 4426, Loss: 0.44024666398763657, Final Batch Loss: 0.04455230385065079\n",
      "Epoch 4427, Loss: 0.7444811910390854, Final Batch Loss: 0.37708723545074463\n",
      "Epoch 4428, Loss: 0.4318287558853626, Final Batch Loss: 0.036633748561143875\n",
      "Epoch 4429, Loss: 0.4125948026776314, Final Batch Loss: 0.09051666408777237\n",
      "Epoch 4430, Loss: 0.568734660744667, Final Batch Loss: 0.17854177951812744\n",
      "Epoch 4431, Loss: 0.7086790204048157, Final Batch Loss: 0.3134894371032715\n",
      "Epoch 4432, Loss: 0.5167838633060455, Final Batch Loss: 0.06841737031936646\n",
      "Epoch 4433, Loss: 0.5869929939508438, Final Batch Loss: 0.11341238021850586\n",
      "Epoch 4434, Loss: 0.6243516206741333, Final Batch Loss: 0.22112002968788147\n",
      "Epoch 4435, Loss: 0.5310191363096237, Final Batch Loss: 0.1451830118894577\n",
      "Epoch 4436, Loss: 0.5999242812395096, Final Batch Loss: 0.17663414776325226\n",
      "Epoch 4437, Loss: 0.9344068467617035, Final Batch Loss: 0.5608299374580383\n",
      "Epoch 4438, Loss: 0.6695583611726761, Final Batch Loss: 0.2727074921131134\n",
      "Epoch 4439, Loss: 0.7003536522388458, Final Batch Loss: 0.24587491154670715\n",
      "Epoch 4440, Loss: 0.5380170047283173, Final Batch Loss: 0.13276240229606628\n",
      "Epoch 4441, Loss: 0.5325423926115036, Final Batch Loss: 0.07561983168125153\n",
      "Epoch 4442, Loss: 0.4576020799577236, Final Batch Loss: 0.05784369632601738\n",
      "Epoch 4443, Loss: 0.45949438214302063, Final Batch Loss: 0.12512607872486115\n",
      "Epoch 4444, Loss: 0.7234016954898834, Final Batch Loss: 0.2802027761936188\n",
      "Epoch 4445, Loss: 0.3558257296681404, Final Batch Loss: 0.018786706030368805\n",
      "Epoch 4446, Loss: 0.6517422646284103, Final Batch Loss: 0.21470940113067627\n",
      "Epoch 4447, Loss: 0.64532370865345, Final Batch Loss: 0.2214576005935669\n",
      "Epoch 4448, Loss: 0.5342898368835449, Final Batch Loss: 0.160884827375412\n",
      "Epoch 4449, Loss: 0.6923702955245972, Final Batch Loss: 0.2073550522327423\n",
      "Epoch 4450, Loss: 0.6743998825550079, Final Batch Loss: 0.2769014537334442\n",
      "Epoch 4451, Loss: 0.6053225547075272, Final Batch Loss: 0.24086005985736847\n",
      "Epoch 4452, Loss: 0.5334620252251625, Final Batch Loss: 0.11103982478380203\n",
      "Epoch 4453, Loss: 0.8053508847951889, Final Batch Loss: 0.40364551544189453\n",
      "Epoch 4454, Loss: 0.5970030874013901, Final Batch Loss: 0.1916823387145996\n",
      "Epoch 4455, Loss: 0.7424329668283463, Final Batch Loss: 0.3594273626804352\n",
      "Epoch 4456, Loss: 0.5322206616401672, Final Batch Loss: 0.17279358208179474\n",
      "Epoch 4457, Loss: 0.4357273168861866, Final Batch Loss: 0.02347661927342415\n",
      "Epoch 4458, Loss: 0.7673746198415756, Final Batch Loss: 0.3382367193698883\n",
      "Epoch 4459, Loss: 0.5002967864274979, Final Batch Loss: 0.11699730157852173\n",
      "Epoch 4460, Loss: 0.8466375023126602, Final Batch Loss: 0.47912004590034485\n",
      "Epoch 4461, Loss: 0.5514016300439835, Final Batch Loss: 0.16769398748874664\n",
      "Epoch 4462, Loss: 0.5337437987327576, Final Batch Loss: 0.15004631876945496\n",
      "Epoch 4463, Loss: 0.7313034385442734, Final Batch Loss: 0.37464383244514465\n",
      "Epoch 4464, Loss: 0.7089827507734299, Final Batch Loss: 0.35697585344314575\n",
      "Epoch 4465, Loss: 0.8284555971622467, Final Batch Loss: 0.4788486063480377\n",
      "Epoch 4466, Loss: 0.4564952030777931, Final Batch Loss: 0.10500190407037735\n",
      "Epoch 4467, Loss: 0.4359780214726925, Final Batch Loss: 0.04612406715750694\n",
      "Epoch 4468, Loss: 0.5126200914382935, Final Batch Loss: 0.14164572954177856\n",
      "Epoch 4469, Loss: 0.8893001973628998, Final Batch Loss: 0.5117558240890503\n",
      "Epoch 4470, Loss: 0.5420316755771637, Final Batch Loss: 0.1914146989583969\n",
      "Epoch 4471, Loss: 0.6459029018878937, Final Batch Loss: 0.266254723072052\n",
      "Epoch 4472, Loss: 0.6370338201522827, Final Batch Loss: 0.14514395594596863\n",
      "Epoch 4473, Loss: 0.6569152176380157, Final Batch Loss: 0.27104151248931885\n",
      "Epoch 4474, Loss: 0.4553569257259369, Final Batch Loss: 0.10562270879745483\n",
      "Epoch 4475, Loss: 0.38434283062815666, Final Batch Loss: 0.04850047454237938\n",
      "Epoch 4476, Loss: 0.6725289821624756, Final Batch Loss: 0.32804185152053833\n",
      "Epoch 4477, Loss: 0.6395092606544495, Final Batch Loss: 0.22512772679328918\n",
      "Epoch 4478, Loss: 0.6640434265136719, Final Batch Loss: 0.2774903476238251\n",
      "Epoch 4479, Loss: 0.6343739628791809, Final Batch Loss: 0.2608301639556885\n",
      "Epoch 4480, Loss: 0.6573128700256348, Final Batch Loss: 0.2689533829689026\n",
      "Epoch 4481, Loss: 0.49143846333026886, Final Batch Loss: 0.08678334951400757\n",
      "Epoch 4482, Loss: 0.5820383876562119, Final Batch Loss: 0.12814439833164215\n",
      "Epoch 4483, Loss: 0.42230475693941116, Final Batch Loss: 0.06574656814336777\n",
      "Epoch 4484, Loss: 0.7216002494096756, Final Batch Loss: 0.37130141258239746\n",
      "Epoch 4485, Loss: 0.45217424631118774, Final Batch Loss: 0.10930067300796509\n",
      "Epoch 4486, Loss: 0.6047497093677521, Final Batch Loss: 0.2538330852985382\n",
      "Epoch 4487, Loss: 0.8381740152835846, Final Batch Loss: 0.36090853810310364\n",
      "Epoch 4488, Loss: 0.4189156536012888, Final Batch Loss: 0.02635323442518711\n",
      "Epoch 4489, Loss: 0.5862902253866196, Final Batch Loss: 0.18396160006523132\n",
      "Epoch 4490, Loss: 0.7753283828496933, Final Batch Loss: 0.4187926650047302\n",
      "Epoch 4491, Loss: 0.573884591460228, Final Batch Loss: 0.1979832649230957\n",
      "Epoch 4492, Loss: 0.4752413332462311, Final Batch Loss: 0.12619821727275848\n",
      "Epoch 4493, Loss: 0.5582432895898819, Final Batch Loss: 0.17333556711673737\n",
      "Epoch 4494, Loss: 0.3628018479794264, Final Batch Loss: 0.026587022468447685\n",
      "Epoch 4495, Loss: 0.5727986246347427, Final Batch Loss: 0.17966535687446594\n",
      "Epoch 4496, Loss: 0.5202088057994843, Final Batch Loss: 0.12956750392913818\n",
      "Epoch 4497, Loss: 0.7212440073490143, Final Batch Loss: 0.37130188941955566\n",
      "Epoch 4498, Loss: 0.47396212816238403, Final Batch Loss: 0.12985478341579437\n",
      "Epoch 4499, Loss: 0.583676353096962, Final Batch Loss: 0.2000006139278412\n",
      "Epoch 4500, Loss: 0.4868773818016052, Final Batch Loss: 0.14535127580165863\n",
      "Epoch 4501, Loss: 0.6113657802343369, Final Batch Loss: 0.23881354928016663\n",
      "Epoch 4502, Loss: 0.5248434916138649, Final Batch Loss: 0.08651379495859146\n",
      "Epoch 4503, Loss: 0.48443643003702164, Final Batch Loss: 0.06453192979097366\n",
      "Epoch 4504, Loss: 0.5256234258413315, Final Batch Loss: 0.195694699883461\n",
      "Epoch 4505, Loss: 0.6769081950187683, Final Batch Loss: 0.2692939043045044\n",
      "Epoch 4506, Loss: 0.5115790814161301, Final Batch Loss: 0.1667049676179886\n",
      "Epoch 4507, Loss: 0.8875757455825806, Final Batch Loss: 0.4390234649181366\n",
      "Epoch 4508, Loss: 0.6959015727043152, Final Batch Loss: 0.27189236879348755\n",
      "Epoch 4509, Loss: 0.5290060639381409, Final Batch Loss: 0.1867859959602356\n",
      "Epoch 4510, Loss: 0.6397563964128494, Final Batch Loss: 0.22409102320671082\n",
      "Epoch 4511, Loss: 0.658525139093399, Final Batch Loss: 0.23448173701763153\n",
      "Epoch 4512, Loss: 0.8799217194318771, Final Batch Loss: 0.4616799056529999\n",
      "Epoch 4513, Loss: 0.5621622204780579, Final Batch Loss: 0.15423309803009033\n",
      "Epoch 4514, Loss: 0.5351751297712326, Final Batch Loss: 0.1482372134923935\n",
      "Epoch 4515, Loss: 0.5345189422369003, Final Batch Loss: 0.16044844686985016\n",
      "Epoch 4516, Loss: 0.4695489853620529, Final Batch Loss: 0.09752151370048523\n",
      "Epoch 4517, Loss: 0.8138585537672043, Final Batch Loss: 0.3680785000324249\n",
      "Epoch 4518, Loss: 0.5371862798929214, Final Batch Loss: 0.16563516855239868\n",
      "Epoch 4519, Loss: 0.8423815220594406, Final Batch Loss: 0.4211379587650299\n",
      "Epoch 4520, Loss: 0.6613462567329407, Final Batch Loss: 0.2961232662200928\n",
      "Epoch 4521, Loss: 0.6538548469543457, Final Batch Loss: 0.17924030125141144\n",
      "Epoch 4522, Loss: 0.48902489989995956, Final Batch Loss: 0.10194585472345352\n",
      "Epoch 4523, Loss: 0.6153185814619064, Final Batch Loss: 0.31593194603919983\n",
      "Epoch 4524, Loss: 0.5411957576870918, Final Batch Loss: 0.12448134273290634\n",
      "Epoch 4525, Loss: 0.4829106032848358, Final Batch Loss: 0.0942649245262146\n",
      "Epoch 4526, Loss: 0.5652926564216614, Final Batch Loss: 0.16553270816802979\n",
      "Epoch 4527, Loss: 0.7153027951717377, Final Batch Loss: 0.27121710777282715\n",
      "Epoch 4528, Loss: 0.6357893198728561, Final Batch Loss: 0.28331130743026733\n",
      "Epoch 4529, Loss: 0.6058572083711624, Final Batch Loss: 0.24795998632907867\n",
      "Epoch 4530, Loss: 0.42590005695819855, Final Batch Loss: 0.06357020139694214\n",
      "Epoch 4531, Loss: 0.733721986413002, Final Batch Loss: 0.2796058654785156\n",
      "Epoch 4532, Loss: 0.5000983476638794, Final Batch Loss: 0.10029931366443634\n",
      "Epoch 4533, Loss: 0.439705565571785, Final Batch Loss: 0.10062102973461151\n",
      "Epoch 4534, Loss: 0.4878881201148033, Final Batch Loss: 0.08733541518449783\n",
      "Epoch 4535, Loss: 0.5472683012485504, Final Batch Loss: 0.16638806462287903\n",
      "Epoch 4536, Loss: 0.4221121519804001, Final Batch Loss: 0.017486751079559326\n",
      "Epoch 4537, Loss: 0.4712955951690674, Final Batch Loss: 0.041374653577804565\n",
      "Epoch 4538, Loss: 0.5410743653774261, Final Batch Loss: 0.12906277179718018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4539, Loss: 0.5666025131940842, Final Batch Loss: 0.22004473209381104\n",
      "Epoch 4540, Loss: 0.5003958716988564, Final Batch Loss: 0.11072828620672226\n",
      "Epoch 4541, Loss: 0.4873794764280319, Final Batch Loss: 0.12752194702625275\n",
      "Epoch 4542, Loss: 0.5282069146633148, Final Batch Loss: 0.15377716720104218\n",
      "Epoch 4543, Loss: 0.4560009315609932, Final Batch Loss: 0.0675114169716835\n",
      "Epoch 4544, Loss: 0.4628240242600441, Final Batch Loss: 0.0927400216460228\n",
      "Epoch 4545, Loss: 0.601320207118988, Final Batch Loss: 0.25336387753486633\n",
      "Epoch 4546, Loss: 0.65164715051651, Final Batch Loss: 0.31512442231178284\n",
      "Epoch 4547, Loss: 0.45374777913093567, Final Batch Loss: 0.10328014194965363\n",
      "Epoch 4548, Loss: 0.47137850522994995, Final Batch Loss: 0.12404219806194305\n",
      "Epoch 4549, Loss: 0.6847534030675888, Final Batch Loss: 0.32900160551071167\n",
      "Epoch 4550, Loss: 0.627078041434288, Final Batch Loss: 0.25666651129722595\n",
      "Epoch 4551, Loss: 0.5005881190299988, Final Batch Loss: 0.06389287114143372\n",
      "Epoch 4552, Loss: 0.5011831596493721, Final Batch Loss: 0.07715854793787003\n",
      "Epoch 4553, Loss: 0.483317032456398, Final Batch Loss: 0.07805092632770538\n",
      "Epoch 4554, Loss: 0.46687405928969383, Final Batch Loss: 0.04600546881556511\n",
      "Epoch 4555, Loss: 0.501577153801918, Final Batch Loss: 0.13113127648830414\n",
      "Epoch 4556, Loss: 0.49203697592020035, Final Batch Loss: 0.10952777415513992\n",
      "Epoch 4557, Loss: 0.6755473166704178, Final Batch Loss: 0.2805887758731842\n",
      "Epoch 4558, Loss: 0.49539223313331604, Final Batch Loss: 0.18365275859832764\n",
      "Epoch 4559, Loss: 0.4854530468583107, Final Batch Loss: 0.12164292484521866\n",
      "Epoch 4560, Loss: 0.45722825080156326, Final Batch Loss: 0.0887731984257698\n",
      "Epoch 4561, Loss: 0.45450932905077934, Final Batch Loss: 0.060717400163412094\n",
      "Epoch 4562, Loss: 0.6139686554670334, Final Batch Loss: 0.2347944974899292\n",
      "Epoch 4563, Loss: 0.4474194720387459, Final Batch Loss: 0.07675764709711075\n",
      "Epoch 4564, Loss: 0.6224379986524582, Final Batch Loss: 0.28437650203704834\n",
      "Epoch 4565, Loss: 0.8273943066596985, Final Batch Loss: 0.47835254669189453\n",
      "Epoch 4566, Loss: 0.6393503397703171, Final Batch Loss: 0.2746284306049347\n",
      "Epoch 4567, Loss: 0.5241061002016068, Final Batch Loss: 0.21529097855091095\n",
      "Epoch 4568, Loss: 0.799997091293335, Final Batch Loss: 0.3974158465862274\n",
      "Epoch 4569, Loss: 0.4845335930585861, Final Batch Loss: 0.13699254393577576\n",
      "Epoch 4570, Loss: 0.5923207253217697, Final Batch Loss: 0.24451853334903717\n",
      "Epoch 4571, Loss: 0.7734141200780869, Final Batch Loss: 0.4484584629535675\n",
      "Epoch 4572, Loss: 0.4550768956542015, Final Batch Loss: 0.09278454631567001\n",
      "Epoch 4573, Loss: 0.485546737909317, Final Batch Loss: 0.14538292586803436\n",
      "Epoch 4574, Loss: 0.5922442674636841, Final Batch Loss: 0.22250458598136902\n",
      "Epoch 4575, Loss: 0.5717601329088211, Final Batch Loss: 0.20784565806388855\n",
      "Epoch 4576, Loss: 0.5755162537097931, Final Batch Loss: 0.18245820701122284\n",
      "Epoch 4577, Loss: 0.4163550175726414, Final Batch Loss: 0.045736540108919144\n",
      "Epoch 4578, Loss: 0.46082358807325363, Final Batch Loss: 0.06364535540342331\n",
      "Epoch 4579, Loss: 0.43695493042469025, Final Batch Loss: 0.09280315041542053\n",
      "Epoch 4580, Loss: 0.5166010558605194, Final Batch Loss: 0.1289350688457489\n",
      "Epoch 4581, Loss: 0.5723987966775894, Final Batch Loss: 0.18415994942188263\n",
      "Epoch 4582, Loss: 0.7822135239839554, Final Batch Loss: 0.47190260887145996\n",
      "Epoch 4583, Loss: 0.4763132855296135, Final Batch Loss: 0.12418530136346817\n",
      "Epoch 4584, Loss: 0.4639514610171318, Final Batch Loss: 0.08977095037698746\n",
      "Epoch 4585, Loss: 1.1973279863595963, Final Batch Loss: 0.8654074668884277\n",
      "Epoch 4586, Loss: 0.8608688861131668, Final Batch Loss: 0.4673731327056885\n",
      "Epoch 4587, Loss: 0.4755754619836807, Final Batch Loss: 0.11501136422157288\n",
      "Epoch 4588, Loss: 0.5569081604480743, Final Batch Loss: 0.17384088039398193\n",
      "Epoch 4589, Loss: 0.7615992426872253, Final Batch Loss: 0.4205458164215088\n",
      "Epoch 4590, Loss: 0.5992940217256546, Final Batch Loss: 0.10693860054016113\n",
      "Epoch 4591, Loss: 0.5158947557210922, Final Batch Loss: 0.07621400058269501\n",
      "Epoch 4592, Loss: 0.34556941082701087, Final Batch Loss: 0.005006799008697271\n",
      "Epoch 4593, Loss: 0.4428695961833, Final Batch Loss: 0.0834278091788292\n",
      "Epoch 4594, Loss: 0.5591114908456802, Final Batch Loss: 0.16107617318630219\n",
      "Epoch 4595, Loss: 0.5918048918247223, Final Batch Loss: 0.23848530650138855\n",
      "Epoch 4596, Loss: 0.634535126388073, Final Batch Loss: 0.12241359800100327\n",
      "Epoch 4597, Loss: 0.5676902681589127, Final Batch Loss: 0.2151441127061844\n",
      "Epoch 4598, Loss: 0.6029285043478012, Final Batch Loss: 0.2602333128452301\n",
      "Epoch 4599, Loss: 0.5712164789438248, Final Batch Loss: 0.18768250942230225\n",
      "Epoch 4600, Loss: 0.6078510582447052, Final Batch Loss: 0.18307094275951385\n",
      "Epoch 4601, Loss: 0.613054022192955, Final Batch Loss: 0.20874083042144775\n",
      "Epoch 4602, Loss: 0.8587358593940735, Final Batch Loss: 0.4720783829689026\n",
      "Epoch 4603, Loss: 0.520769864320755, Final Batch Loss: 0.11052586138248444\n",
      "Epoch 4604, Loss: 0.7032253742218018, Final Batch Loss: 0.28535300493240356\n",
      "Epoch 4605, Loss: 0.7271124124526978, Final Batch Loss: 0.2870642840862274\n",
      "Epoch 4606, Loss: 0.6986285895109177, Final Batch Loss: 0.2662999629974365\n",
      "Epoch 4607, Loss: 0.588595062494278, Final Batch Loss: 0.1952444165945053\n",
      "Epoch 4608, Loss: 0.7041315138339996, Final Batch Loss: 0.37005165219306946\n",
      "Epoch 4609, Loss: 0.5072237625718117, Final Batch Loss: 0.0872446671128273\n",
      "Epoch 4610, Loss: 0.5043167620897293, Final Batch Loss: 0.08354970812797546\n",
      "Epoch 4611, Loss: 0.42720459029078484, Final Batch Loss: 0.05033678933978081\n",
      "Epoch 4612, Loss: 0.6482461988925934, Final Batch Loss: 0.27090564370155334\n",
      "Epoch 4613, Loss: 0.4887814223766327, Final Batch Loss: 0.16163672506809235\n",
      "Epoch 4614, Loss: 0.5500500053167343, Final Batch Loss: 0.1306459754705429\n",
      "Epoch 4615, Loss: 0.7340595722198486, Final Batch Loss: 0.4261586666107178\n",
      "Epoch 4616, Loss: 0.7577141374349594, Final Batch Loss: 0.3995853364467621\n",
      "Epoch 4617, Loss: 0.4987114369869232, Final Batch Loss: 0.14212381839752197\n",
      "Epoch 4618, Loss: 0.4544781632721424, Final Batch Loss: 0.06215915456414223\n",
      "Epoch 4619, Loss: 0.6052663698792458, Final Batch Loss: 0.2602658271789551\n",
      "Epoch 4620, Loss: 0.4815170243382454, Final Batch Loss: 0.05337008088827133\n",
      "Epoch 4621, Loss: 0.5240690857172012, Final Batch Loss: 0.11505919694900513\n",
      "Epoch 4622, Loss: 0.5863228440284729, Final Batch Loss: 0.1611916869878769\n",
      "Epoch 4623, Loss: 0.6209099888801575, Final Batch Loss: 0.23325102031230927\n",
      "Epoch 4624, Loss: 0.5997208505868912, Final Batch Loss: 0.21186593174934387\n",
      "Epoch 4625, Loss: 0.6267292648553848, Final Batch Loss: 0.1356673240661621\n",
      "Epoch 4626, Loss: 0.5287032723426819, Final Batch Loss: 0.12917347252368927\n",
      "Epoch 4627, Loss: 0.4866335168480873, Final Batch Loss: 0.11375889927148819\n",
      "Epoch 4628, Loss: 0.53987155854702, Final Batch Loss: 0.16812419891357422\n",
      "Epoch 4629, Loss: 0.5784174054861069, Final Batch Loss: 0.2074350267648697\n",
      "Epoch 4630, Loss: 0.5128507502377033, Final Batch Loss: 0.03551337495446205\n",
      "Epoch 4631, Loss: 0.42448150739073753, Final Batch Loss: 0.05963350459933281\n",
      "Epoch 4632, Loss: 0.7573081105947495, Final Batch Loss: 0.409361332654953\n",
      "Epoch 4633, Loss: 0.5164816826581955, Final Batch Loss: 0.1756761521100998\n",
      "Epoch 4634, Loss: 0.4821080267429352, Final Batch Loss: 0.08200214803218842\n",
      "Epoch 4635, Loss: 0.4981831908226013, Final Batch Loss: 0.15121245384216309\n",
      "Epoch 4636, Loss: 0.5617889985442162, Final Batch Loss: 0.12195541709661484\n",
      "Epoch 4637, Loss: 0.6071282774209976, Final Batch Loss: 0.23258575797080994\n",
      "Epoch 4638, Loss: 0.6856184750795364, Final Batch Loss: 0.3210271894931793\n",
      "Epoch 4639, Loss: 0.7331884056329727, Final Batch Loss: 0.3792791962623596\n",
      "Epoch 4640, Loss: 0.7723073661327362, Final Batch Loss: 0.39264774322509766\n",
      "Epoch 4641, Loss: 0.6921495646238327, Final Batch Loss: 0.30092400312423706\n",
      "Epoch 4642, Loss: 0.5361209064722061, Final Batch Loss: 0.16893370449543\n",
      "Epoch 4643, Loss: 0.612402617931366, Final Batch Loss: 0.17742158472537994\n",
      "Epoch 4644, Loss: 0.4561907984316349, Final Batch Loss: 0.04304927960038185\n",
      "Epoch 4645, Loss: 0.9033446609973907, Final Batch Loss: 0.5095148682594299\n",
      "Epoch 4646, Loss: 0.45348791778087616, Final Batch Loss: 0.10831937193870544\n",
      "Epoch 4647, Loss: 0.5184295028448105, Final Batch Loss: 0.14957286417484283\n",
      "Epoch 4648, Loss: 0.7336639612913132, Final Batch Loss: 0.354893296957016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4649, Loss: 0.6126088798046112, Final Batch Loss: 0.2501172721385956\n",
      "Epoch 4650, Loss: 0.46036243438720703, Final Batch Loss: 0.08489848673343658\n",
      "Epoch 4651, Loss: 0.4388889893889427, Final Batch Loss: 0.06672964245080948\n",
      "Epoch 4652, Loss: 0.5758350044488907, Final Batch Loss: 0.2345191091299057\n",
      "Epoch 4653, Loss: 0.5879097431898117, Final Batch Loss: 0.24848179519176483\n",
      "Epoch 4654, Loss: 0.5226708203554153, Final Batch Loss: 0.12284795939922333\n",
      "Epoch 4655, Loss: 0.6108836829662323, Final Batch Loss: 0.16355466842651367\n",
      "Epoch 4656, Loss: 0.7377432882785797, Final Batch Loss: 0.36987796425819397\n",
      "Epoch 4657, Loss: 0.5062555149197578, Final Batch Loss: 0.10905729979276657\n",
      "Epoch 4658, Loss: 0.5136840343475342, Final Batch Loss: 0.1542578637599945\n",
      "Epoch 4659, Loss: 0.6591385900974274, Final Batch Loss: 0.30231955647468567\n",
      "Epoch 4660, Loss: 0.5002055317163467, Final Batch Loss: 0.16202254593372345\n",
      "Epoch 4661, Loss: 0.4195764120668173, Final Batch Loss: 0.027504777535796165\n",
      "Epoch 4662, Loss: 0.5740696489810944, Final Batch Loss: 0.17825578153133392\n",
      "Epoch 4663, Loss: 0.4389483332633972, Final Batch Loss: 0.14005202054977417\n",
      "Epoch 4664, Loss: 0.4739583507180214, Final Batch Loss: 0.08894846588373184\n",
      "Epoch 4665, Loss: 0.6417267173528671, Final Batch Loss: 0.28631889820098877\n",
      "Epoch 4666, Loss: 0.5117654949426651, Final Batch Loss: 0.1675390750169754\n",
      "Epoch 4667, Loss: 0.4632617086172104, Final Batch Loss: 0.11331583559513092\n",
      "Epoch 4668, Loss: 0.5181702524423599, Final Batch Loss: 0.17256119847297668\n",
      "Epoch 4669, Loss: 0.6309797316789627, Final Batch Loss: 0.2172824889421463\n",
      "Epoch 4670, Loss: 0.41723255813121796, Final Batch Loss: 0.06494523584842682\n",
      "Epoch 4671, Loss: 0.5466515570878983, Final Batch Loss: 0.1385127156972885\n",
      "Epoch 4672, Loss: 0.570996031165123, Final Batch Loss: 0.2062370330095291\n",
      "Epoch 4673, Loss: 0.6460253596305847, Final Batch Loss: 0.21431122720241547\n",
      "Epoch 4674, Loss: 0.4175688847899437, Final Batch Loss: 0.08278758078813553\n",
      "Epoch 4675, Loss: 0.5644609332084656, Final Batch Loss: 0.20714612305164337\n",
      "Epoch 4676, Loss: 0.5124434530735016, Final Batch Loss: 0.12633126974105835\n",
      "Epoch 4677, Loss: 0.5909788608551025, Final Batch Loss: 0.19671039283275604\n",
      "Epoch 4678, Loss: 0.5799912512302399, Final Batch Loss: 0.2138305902481079\n",
      "Epoch 4679, Loss: 0.5681470781564713, Final Batch Loss: 0.21182556450366974\n",
      "Epoch 4680, Loss: 0.5031141191720963, Final Batch Loss: 0.14420604705810547\n",
      "Epoch 4681, Loss: 0.5602942705154419, Final Batch Loss: 0.17823266983032227\n",
      "Epoch 4682, Loss: 0.4476251155138016, Final Batch Loss: 0.10917265713214874\n",
      "Epoch 4683, Loss: 0.5054585859179497, Final Batch Loss: 0.0870395228266716\n",
      "Epoch 4684, Loss: 0.5589845478534698, Final Batch Loss: 0.21770671010017395\n",
      "Epoch 4685, Loss: 0.5620732009410858, Final Batch Loss: 0.21221895515918732\n",
      "Epoch 4686, Loss: 0.35882172733545303, Final Batch Loss: 0.06797672063112259\n",
      "Epoch 4687, Loss: 0.6156230866909027, Final Batch Loss: 0.2752218544483185\n",
      "Epoch 4688, Loss: 0.4369572326540947, Final Batch Loss: 0.08744873851537704\n",
      "Epoch 4689, Loss: 0.5256233662366867, Final Batch Loss: 0.1489696502685547\n",
      "Epoch 4690, Loss: 0.5978971719741821, Final Batch Loss: 0.23352205753326416\n",
      "Epoch 4691, Loss: 0.6139586120843887, Final Batch Loss: 0.2424755096435547\n",
      "Epoch 4692, Loss: 0.737309530377388, Final Batch Loss: 0.3073521554470062\n",
      "Epoch 4693, Loss: 0.4714096784591675, Final Batch Loss: 0.12179669737815857\n",
      "Epoch 4694, Loss: 0.5805807560682297, Final Batch Loss: 0.1569410115480423\n",
      "Epoch 4695, Loss: 0.5477412566542625, Final Batch Loss: 0.083114854991436\n",
      "Epoch 4696, Loss: 0.5834895074367523, Final Batch Loss: 0.17981432378292084\n",
      "Epoch 4697, Loss: 0.7498584091663361, Final Batch Loss: 0.283160924911499\n",
      "Epoch 4698, Loss: 0.4502670615911484, Final Batch Loss: 0.07004109025001526\n",
      "Epoch 4699, Loss: 0.7965612560510635, Final Batch Loss: 0.40922147035598755\n",
      "Epoch 4700, Loss: 0.521265372633934, Final Batch Loss: 0.1371246576309204\n",
      "Epoch 4701, Loss: 0.5079966634511948, Final Batch Loss: 0.14203672111034393\n",
      "Epoch 4702, Loss: 0.4404957890510559, Final Batch Loss: 0.1252276450395584\n",
      "Epoch 4703, Loss: 0.6637735664844513, Final Batch Loss: 0.27474692463874817\n",
      "Epoch 4704, Loss: 0.9236799329519272, Final Batch Loss: 0.609036922454834\n",
      "Epoch 4705, Loss: 0.4704112187027931, Final Batch Loss: 0.05913301557302475\n",
      "Epoch 4706, Loss: 0.5921689420938492, Final Batch Loss: 0.22948427498340607\n",
      "Epoch 4707, Loss: 0.7180716842412949, Final Batch Loss: 0.3229588568210602\n",
      "Epoch 4708, Loss: 0.5397870987653732, Final Batch Loss: 0.17115074396133423\n",
      "Epoch 4709, Loss: 0.8089953064918518, Final Batch Loss: 0.37463048100471497\n",
      "Epoch 4710, Loss: 0.5344867929816246, Final Batch Loss: 0.11622505635023117\n",
      "Epoch 4711, Loss: 0.6772691309452057, Final Batch Loss: 0.29596710205078125\n",
      "Epoch 4712, Loss: 0.46805470436811447, Final Batch Loss: 0.08823420852422714\n",
      "Epoch 4713, Loss: 0.4522615745663643, Final Batch Loss: 0.09308091551065445\n",
      "Epoch 4714, Loss: 0.5401046201586723, Final Batch Loss: 0.12349895387887955\n",
      "Epoch 4715, Loss: 0.4595668613910675, Final Batch Loss: 0.14370501041412354\n",
      "Epoch 4716, Loss: 0.4146311990916729, Final Batch Loss: 0.0434735082089901\n",
      "Epoch 4717, Loss: 0.6354714184999466, Final Batch Loss: 0.23744791746139526\n",
      "Epoch 4718, Loss: 0.5636615008115768, Final Batch Loss: 0.20240573585033417\n",
      "Epoch 4719, Loss: 0.49948497116565704, Final Batch Loss: 0.2001984566450119\n",
      "Epoch 4720, Loss: 0.43614058941602707, Final Batch Loss: 0.09807690232992172\n",
      "Epoch 4721, Loss: 0.5015625208616257, Final Batch Loss: 0.12837839126586914\n",
      "Epoch 4722, Loss: 0.5420747101306915, Final Batch Loss: 0.19822333753108978\n",
      "Epoch 4723, Loss: 0.46231909096241, Final Batch Loss: 0.14623986184597015\n",
      "Epoch 4724, Loss: 0.6348258256912231, Final Batch Loss: 0.26864680647850037\n",
      "Epoch 4725, Loss: 0.551180973649025, Final Batch Loss: 0.2259257733821869\n",
      "Epoch 4726, Loss: 0.4270515777170658, Final Batch Loss: 0.042815614491701126\n",
      "Epoch 4727, Loss: 0.450299471616745, Final Batch Loss: 0.10470855236053467\n",
      "Epoch 4728, Loss: 0.5017580613493919, Final Batch Loss: 0.044062308967113495\n",
      "Epoch 4729, Loss: 0.5276139825582504, Final Batch Loss: 0.1927240788936615\n",
      "Epoch 4730, Loss: 0.3923235237598419, Final Batch Loss: 0.08963710069656372\n",
      "Epoch 4731, Loss: 0.6430594772100449, Final Batch Loss: 0.2755849063396454\n",
      "Epoch 4732, Loss: 0.43930570036172867, Final Batch Loss: 0.10519324988126755\n",
      "Epoch 4733, Loss: 0.8212067186832428, Final Batch Loss: 0.49348267912864685\n",
      "Epoch 4734, Loss: 0.3805679567158222, Final Batch Loss: 0.04877692833542824\n",
      "Epoch 4735, Loss: 0.4570551887154579, Final Batch Loss: 0.05242546647787094\n",
      "Epoch 4736, Loss: 0.4277718886733055, Final Batch Loss: 0.07547552138566971\n",
      "Epoch 4737, Loss: 0.9785832017660141, Final Batch Loss: 0.5652799606323242\n",
      "Epoch 4738, Loss: 0.4488597437739372, Final Batch Loss: 0.023091979324817657\n",
      "Epoch 4739, Loss: 0.5783070176839828, Final Batch Loss: 0.26532429456710815\n",
      "Epoch 4740, Loss: 0.4773876667022705, Final Batch Loss: 0.078783318400383\n",
      "Epoch 4741, Loss: 0.5769065767526627, Final Batch Loss: 0.1901831477880478\n",
      "Epoch 4742, Loss: 0.6352578997612, Final Batch Loss: 0.2982694208621979\n",
      "Epoch 4743, Loss: 0.4918260872364044, Final Batch Loss: 0.10868114233016968\n",
      "Epoch 4744, Loss: 0.7075866758823395, Final Batch Loss: 0.3331793248653412\n",
      "Epoch 4745, Loss: 0.48096462339162827, Final Batch Loss: 0.09761584550142288\n",
      "Epoch 4746, Loss: 0.5724117606878281, Final Batch Loss: 0.15208183228969574\n",
      "Epoch 4747, Loss: 0.6675011813640594, Final Batch Loss: 0.2706044018268585\n",
      "Epoch 4748, Loss: 0.5966123342514038, Final Batch Loss: 0.2084803283214569\n",
      "Epoch 4749, Loss: 0.43683332204818726, Final Batch Loss: 0.12686558067798615\n",
      "Epoch 4750, Loss: 0.40893837064504623, Final Batch Loss: 0.07989779859781265\n",
      "Epoch 4751, Loss: 0.6936797052621841, Final Batch Loss: 0.3081638216972351\n",
      "Epoch 4752, Loss: 0.4605475813150406, Final Batch Loss: 0.05334755778312683\n",
      "Epoch 4753, Loss: 0.5557657033205032, Final Batch Loss: 0.21263214945793152\n",
      "Epoch 4754, Loss: 0.6512620896100998, Final Batch Loss: 0.2571525573730469\n",
      "Epoch 4755, Loss: 0.4144265055656433, Final Batch Loss: 0.04705357551574707\n",
      "Epoch 4756, Loss: 0.4433104321360588, Final Batch Loss: 0.1096382662653923\n",
      "Epoch 4757, Loss: 0.6086703389883041, Final Batch Loss: 0.20024985074996948\n",
      "Epoch 4758, Loss: 0.4391927495598793, Final Batch Loss: 0.05193515866994858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4759, Loss: 0.7410129308700562, Final Batch Loss: 0.3482312560081482\n",
      "Epoch 4760, Loss: 0.7254333198070526, Final Batch Loss: 0.3496089577674866\n",
      "Epoch 4761, Loss: 0.43782223016023636, Final Batch Loss: 0.08625680953264236\n",
      "Epoch 4762, Loss: 0.5555392354726791, Final Batch Loss: 0.19565466046333313\n",
      "Epoch 4763, Loss: 0.5398659259080887, Final Batch Loss: 0.14686493575572968\n",
      "Epoch 4764, Loss: 0.5527344048023224, Final Batch Loss: 0.1711265593767166\n",
      "Epoch 4765, Loss: 0.46629560366272926, Final Batch Loss: 0.04795316234230995\n",
      "Epoch 4766, Loss: 0.5304825454950333, Final Batch Loss: 0.18546436727046967\n",
      "Epoch 4767, Loss: 0.6671982258558273, Final Batch Loss: 0.22338338196277618\n",
      "Epoch 4768, Loss: 0.7168437242507935, Final Batch Loss: 0.3494672477245331\n",
      "Epoch 4769, Loss: 0.6112695634365082, Final Batch Loss: 0.22594642639160156\n",
      "Epoch 4770, Loss: 0.5022335574030876, Final Batch Loss: 0.08229503780603409\n",
      "Epoch 4771, Loss: 0.5304052531719208, Final Batch Loss: 0.15894407033920288\n",
      "Epoch 4772, Loss: 0.5505278259515762, Final Batch Loss: 0.21893513202667236\n",
      "Epoch 4773, Loss: 0.6466575562953949, Final Batch Loss: 0.22443309426307678\n",
      "Epoch 4774, Loss: 0.3906601369380951, Final Batch Loss: 0.03687463700771332\n",
      "Epoch 4775, Loss: 0.5700001120567322, Final Batch Loss: 0.19819645583629608\n",
      "Epoch 4776, Loss: 0.68631611764431, Final Batch Loss: 0.2902723252773285\n",
      "Epoch 4777, Loss: 0.5939226895570755, Final Batch Loss: 0.30296507477760315\n",
      "Epoch 4778, Loss: 0.4958034083247185, Final Batch Loss: 0.08548321574926376\n",
      "Epoch 4779, Loss: 0.4262237437069416, Final Batch Loss: 0.06177276745438576\n",
      "Epoch 4780, Loss: 0.474747933447361, Final Batch Loss: 0.06318401545286179\n",
      "Epoch 4781, Loss: 0.4397849813103676, Final Batch Loss: 0.10599815100431442\n",
      "Epoch 4782, Loss: 0.5017949268221855, Final Batch Loss: 0.13707859814167023\n",
      "Epoch 4783, Loss: 0.6880462169647217, Final Batch Loss: 0.35230422019958496\n",
      "Epoch 4784, Loss: 0.47542494535446167, Final Batch Loss: 0.12008801102638245\n",
      "Epoch 4785, Loss: 0.44856009632349014, Final Batch Loss: 0.11526165157556534\n",
      "Epoch 4786, Loss: 0.7328206449747086, Final Batch Loss: 0.3912142813205719\n",
      "Epoch 4787, Loss: 0.42983270809054375, Final Batch Loss: 0.05239085480570793\n",
      "Epoch 4788, Loss: 0.5971451252698898, Final Batch Loss: 0.23740969598293304\n",
      "Epoch 4789, Loss: 0.4237986356019974, Final Batch Loss: 0.04952266812324524\n",
      "Epoch 4790, Loss: 0.5308178812265396, Final Batch Loss: 0.1183476448059082\n",
      "Epoch 4791, Loss: 0.7628394216299057, Final Batch Loss: 0.38574886322021484\n",
      "Epoch 4792, Loss: 0.5316133722662926, Final Batch Loss: 0.08045636862516403\n",
      "Epoch 4793, Loss: 0.5197449326515198, Final Batch Loss: 0.1273069679737091\n",
      "Epoch 4794, Loss: 0.5576050281524658, Final Batch Loss: 0.23354926705360413\n",
      "Epoch 4795, Loss: 0.43121786788105965, Final Batch Loss: 0.04468433931469917\n",
      "Epoch 4796, Loss: 0.5399462282657623, Final Batch Loss: 0.18033473193645477\n",
      "Epoch 4797, Loss: 0.6156693398952484, Final Batch Loss: 0.2266332507133484\n",
      "Epoch 4798, Loss: 0.49585359543561935, Final Batch Loss: 0.09559699147939682\n",
      "Epoch 4799, Loss: 0.6092731654644012, Final Batch Loss: 0.2960818111896515\n",
      "Epoch 4800, Loss: 0.9979625791311264, Final Batch Loss: 0.5924484729766846\n",
      "Epoch 4801, Loss: 0.7597959190607071, Final Batch Loss: 0.38692083954811096\n",
      "Epoch 4802, Loss: 0.41691117361187935, Final Batch Loss: 0.05529767647385597\n",
      "Epoch 4803, Loss: 0.4053399618715048, Final Batch Loss: 0.018830841407179832\n",
      "Epoch 4804, Loss: 0.6438704282045364, Final Batch Loss: 0.2134314626455307\n",
      "Epoch 4805, Loss: 0.6920683830976486, Final Batch Loss: 0.2904890477657318\n",
      "Epoch 4806, Loss: 0.5005976632237434, Final Batch Loss: 0.08182968944311142\n",
      "Epoch 4807, Loss: 0.4880620166659355, Final Batch Loss: 0.10497929900884628\n",
      "Epoch 4808, Loss: 0.44187361001968384, Final Batch Loss: 0.10732753574848175\n",
      "Epoch 4809, Loss: 0.5538721680641174, Final Batch Loss: 0.19088661670684814\n",
      "Epoch 4810, Loss: 0.4384598881006241, Final Batch Loss: 0.14661499857902527\n",
      "Epoch 4811, Loss: 0.43105873465538025, Final Batch Loss: 0.12944668531417847\n",
      "Epoch 4812, Loss: 0.4156852439045906, Final Batch Loss: 0.00821339339017868\n",
      "Epoch 4813, Loss: 0.6758321076631546, Final Batch Loss: 0.3299002945423126\n",
      "Epoch 4814, Loss: 0.4845923036336899, Final Batch Loss: 0.14960327744483948\n",
      "Epoch 4815, Loss: 0.37213200330734253, Final Batch Loss: 0.03082577884197235\n",
      "Epoch 4816, Loss: 0.5115661472082138, Final Batch Loss: 0.14115044474601746\n",
      "Epoch 4817, Loss: 0.4315387196838856, Final Batch Loss: 0.060304705053567886\n",
      "Epoch 4818, Loss: 0.4217129461467266, Final Batch Loss: 0.04068688675761223\n",
      "Epoch 4819, Loss: 0.5242105275392532, Final Batch Loss: 0.1667090505361557\n",
      "Epoch 4820, Loss: 0.629968672990799, Final Batch Loss: 0.28630512952804565\n",
      "Epoch 4821, Loss: 0.7800164371728897, Final Batch Loss: 0.4057924747467041\n",
      "Epoch 4822, Loss: 0.4501555263996124, Final Batch Loss: 0.13957186043262482\n",
      "Epoch 4823, Loss: 0.682874783873558, Final Batch Loss: 0.2977478504180908\n",
      "Epoch 4824, Loss: 0.5708902776241302, Final Batch Loss: 0.17392663657665253\n",
      "Epoch 4825, Loss: 0.35232561035081744, Final Batch Loss: 0.0026291529648005962\n",
      "Epoch 4826, Loss: 0.5401254445314407, Final Batch Loss: 0.12857790291309357\n",
      "Epoch 4827, Loss: 0.47801757976412773, Final Batch Loss: 0.04366114363074303\n",
      "Epoch 4828, Loss: 0.43411659449338913, Final Batch Loss: 0.09856105595827103\n",
      "Epoch 4829, Loss: 0.6167484670877457, Final Batch Loss: 0.21540407836437225\n",
      "Epoch 4830, Loss: 0.4140380471944809, Final Batch Loss: 0.09464149177074432\n",
      "Epoch 4831, Loss: 0.3968020901083946, Final Batch Loss: 0.10101545602083206\n",
      "Epoch 4832, Loss: 0.701855018734932, Final Batch Loss: 0.28616562485694885\n",
      "Epoch 4833, Loss: 0.4704108089208603, Final Batch Loss: 0.1321907788515091\n",
      "Epoch 4834, Loss: 0.46812527626752853, Final Batch Loss: 0.09105818718671799\n",
      "Epoch 4835, Loss: 0.5030302181839943, Final Batch Loss: 0.10097416490316391\n",
      "Epoch 4836, Loss: 0.6418120712041855, Final Batch Loss: 0.2804090976715088\n",
      "Epoch 4837, Loss: 0.5998156666755676, Final Batch Loss: 0.24915479123592377\n",
      "Epoch 4838, Loss: 0.47810717672109604, Final Batch Loss: 0.05405997484922409\n",
      "Epoch 4839, Loss: 0.3554142117500305, Final Batch Loss: 0.05294983088970184\n",
      "Epoch 4840, Loss: 0.6410011947154999, Final Batch Loss: 0.21724480390548706\n",
      "Epoch 4841, Loss: 0.5329508930444717, Final Batch Loss: 0.16062068939208984\n",
      "Epoch 4842, Loss: 0.4190666824579239, Final Batch Loss: 0.06874126195907593\n",
      "Epoch 4843, Loss: 0.36438191682100296, Final Batch Loss: 0.06857787817716599\n",
      "Epoch 4844, Loss: 0.34785641357302666, Final Batch Loss: 0.01847025379538536\n",
      "Epoch 4845, Loss: 0.3825461007654667, Final Batch Loss: 0.053029511123895645\n",
      "Epoch 4846, Loss: 0.48449045419692993, Final Batch Loss: 0.18468725681304932\n",
      "Epoch 4847, Loss: 0.47070638835430145, Final Batch Loss: 0.16990642249584198\n",
      "Epoch 4848, Loss: 0.5892306715250015, Final Batch Loss: 0.24549169838428497\n",
      "Epoch 4849, Loss: 0.5513732433319092, Final Batch Loss: 0.24076028168201447\n",
      "Epoch 4850, Loss: 0.4873892366886139, Final Batch Loss: 0.1424381136894226\n",
      "Epoch 4851, Loss: 0.49411991983652115, Final Batch Loss: 0.11309731751680374\n",
      "Epoch 4852, Loss: 0.5361167937517166, Final Batch Loss: 0.12232765555381775\n",
      "Epoch 4853, Loss: 0.4212889298796654, Final Batch Loss: 0.11989255994558334\n",
      "Epoch 4854, Loss: 0.5745956152677536, Final Batch Loss: 0.18315066397190094\n",
      "Epoch 4855, Loss: 0.48862021416425705, Final Batch Loss: 0.08465870469808578\n",
      "Epoch 4856, Loss: 0.40421348810195923, Final Batch Loss: 0.07052141427993774\n",
      "Epoch 4857, Loss: 0.4027981497347355, Final Batch Loss: 0.04261276498436928\n",
      "Epoch 4858, Loss: 0.5891187787055969, Final Batch Loss: 0.21372370421886444\n",
      "Epoch 4859, Loss: 0.5354447886347771, Final Batch Loss: 0.11471603065729141\n",
      "Epoch 4860, Loss: 0.3887885734438896, Final Batch Loss: 0.06870736926794052\n",
      "Epoch 4861, Loss: 0.5116835981607437, Final Batch Loss: 0.13064807653427124\n",
      "Epoch 4862, Loss: 0.44991061836481094, Final Batch Loss: 0.10940251499414444\n",
      "Epoch 4863, Loss: 0.5159759968519211, Final Batch Loss: 0.1392228752374649\n",
      "Epoch 4864, Loss: 0.733745738863945, Final Batch Loss: 0.3459036350250244\n",
      "Epoch 4865, Loss: 0.6482051908969879, Final Batch Loss: 0.2920416295528412\n",
      "Epoch 4866, Loss: 0.545487642288208, Final Batch Loss: 0.1400342434644699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4867, Loss: 0.5132587775588036, Final Batch Loss: 0.10961037129163742\n",
      "Epoch 4868, Loss: 0.3832574337720871, Final Batch Loss: 0.06140872836112976\n",
      "Epoch 4869, Loss: 0.48988211154937744, Final Batch Loss: 0.11283731460571289\n",
      "Epoch 4870, Loss: 0.4494621455669403, Final Batch Loss: 0.06993342936038971\n",
      "Epoch 4871, Loss: 0.8435440957546234, Final Batch Loss: 0.47234559059143066\n",
      "Epoch 4872, Loss: 0.6603234112262726, Final Batch Loss: 0.25995898246765137\n",
      "Epoch 4873, Loss: 0.4013790972530842, Final Batch Loss: 0.04509866610169411\n",
      "Epoch 4874, Loss: 0.4695611819624901, Final Batch Loss: 0.1002378985285759\n",
      "Epoch 4875, Loss: 0.557216003537178, Final Batch Loss: 0.18812207877635956\n",
      "Epoch 4876, Loss: 0.5817863792181015, Final Batch Loss: 0.2120371162891388\n",
      "Epoch 4877, Loss: 0.6012236475944519, Final Batch Loss: 0.1722569763660431\n",
      "Epoch 4878, Loss: 0.474753275513649, Final Batch Loss: 0.13140808045864105\n",
      "Epoch 4879, Loss: 0.482304185628891, Final Batch Loss: 0.13370653986930847\n",
      "Epoch 4880, Loss: 0.6272889077663422, Final Batch Loss: 0.2250334769487381\n",
      "Epoch 4881, Loss: 0.44135456532239914, Final Batch Loss: 0.09259632974863052\n",
      "Epoch 4882, Loss: 0.5149320960044861, Final Batch Loss: 0.15180201828479767\n",
      "Epoch 4883, Loss: 0.4127463214099407, Final Batch Loss: 0.04743201658129692\n",
      "Epoch 4884, Loss: 0.5952665507793427, Final Batch Loss: 0.20134596526622772\n",
      "Epoch 4885, Loss: 0.46107859164476395, Final Batch Loss: 0.07126874476671219\n",
      "Epoch 4886, Loss: 0.6124057024717331, Final Batch Loss: 0.33709296584129333\n",
      "Epoch 4887, Loss: 0.5825424790382385, Final Batch Loss: 0.23355308175086975\n",
      "Epoch 4888, Loss: 0.41917096823453903, Final Batch Loss: 0.07462269812822342\n",
      "Epoch 4889, Loss: 0.41040364652872086, Final Batch Loss: 0.07124354690313339\n",
      "Epoch 4890, Loss: 0.6321954727172852, Final Batch Loss: 0.2836900055408478\n",
      "Epoch 4891, Loss: 0.5636417865753174, Final Batch Loss: 0.22567103803157806\n",
      "Epoch 4892, Loss: 0.4764462932944298, Final Batch Loss: 0.09475038200616837\n",
      "Epoch 4893, Loss: 0.5619456022977829, Final Batch Loss: 0.2023983746767044\n",
      "Epoch 4894, Loss: 0.5057417079806328, Final Batch Loss: 0.10721028596162796\n",
      "Epoch 4895, Loss: 0.7055195868015289, Final Batch Loss: 0.3722745180130005\n",
      "Epoch 4896, Loss: 0.46698662638664246, Final Batch Loss: 0.15295851230621338\n",
      "Epoch 4897, Loss: 0.4140942618250847, Final Batch Loss: 0.09045777469873428\n",
      "Epoch 4898, Loss: 0.4767811596393585, Final Batch Loss: 0.12518762052059174\n",
      "Epoch 4899, Loss: 0.4799730032682419, Final Batch Loss: 0.14073491096496582\n",
      "Epoch 4900, Loss: 0.4611310362815857, Final Batch Loss: 0.1268639862537384\n",
      "Epoch 4901, Loss: 0.40095068514347076, Final Batch Loss: 0.11111409962177277\n",
      "Epoch 4902, Loss: 0.48035381734371185, Final Batch Loss: 0.170765221118927\n",
      "Epoch 4903, Loss: 0.4170301780104637, Final Batch Loss: 0.11092666536569595\n",
      "Epoch 4904, Loss: 0.45651736855506897, Final Batch Loss: 0.1335633546113968\n",
      "Epoch 4905, Loss: 0.5546269416809082, Final Batch Loss: 0.20928889513015747\n",
      "Epoch 4906, Loss: 0.35863849334418774, Final Batch Loss: 0.02614174596965313\n",
      "Epoch 4907, Loss: 0.5926555544137955, Final Batch Loss: 0.18910610675811768\n",
      "Epoch 4908, Loss: 0.5461105406284332, Final Batch Loss: 0.14069949090480804\n",
      "Epoch 4909, Loss: 0.6348815858364105, Final Batch Loss: 0.2378503382205963\n",
      "Epoch 4910, Loss: 0.6013168841600418, Final Batch Loss: 0.2972368001937866\n",
      "Epoch 4911, Loss: 0.4798117130994797, Final Batch Loss: 0.1508762389421463\n",
      "Epoch 4912, Loss: 0.6890023797750473, Final Batch Loss: 0.3565342426300049\n",
      "Epoch 4913, Loss: 0.46788935363292694, Final Batch Loss: 0.08355079591274261\n",
      "Epoch 4914, Loss: 0.5118825435638428, Final Batch Loss: 0.17067165672779083\n",
      "Epoch 4915, Loss: 0.3732814621180296, Final Batch Loss: 0.004010943695902824\n",
      "Epoch 4916, Loss: 0.5065957903862, Final Batch Loss: 0.17784452438354492\n",
      "Epoch 4917, Loss: 0.6353843957185745, Final Batch Loss: 0.24973279237747192\n",
      "Epoch 4918, Loss: 0.6342328935861588, Final Batch Loss: 0.297848641872406\n",
      "Epoch 4919, Loss: 0.789393275976181, Final Batch Loss: 0.4354919493198395\n",
      "Epoch 4920, Loss: 0.5795659422874451, Final Batch Loss: 0.21955254673957825\n",
      "Epoch 4921, Loss: 0.48040346801280975, Final Batch Loss: 0.17940516769886017\n",
      "Epoch 4922, Loss: 0.6421890407800674, Final Batch Loss: 0.2788955271244049\n",
      "Epoch 4923, Loss: 0.6424565315246582, Final Batch Loss: 0.27254006266593933\n",
      "Epoch 4924, Loss: 0.6377561539411545, Final Batch Loss: 0.28557926416397095\n",
      "Epoch 4925, Loss: 0.49081847816705704, Final Batch Loss: 0.09383723884820938\n",
      "Epoch 4926, Loss: 0.5910268872976303, Final Batch Loss: 0.2485618144273758\n",
      "Epoch 4927, Loss: 0.7281605005264282, Final Batch Loss: 0.3207012712955475\n",
      "Epoch 4928, Loss: 0.7452754229307175, Final Batch Loss: 0.3074668347835541\n",
      "Epoch 4929, Loss: 0.42465343326330185, Final Batch Loss: 0.0794849768280983\n",
      "Epoch 4930, Loss: 0.43264389783143997, Final Batch Loss: 0.11917123943567276\n",
      "Epoch 4931, Loss: 0.6548096537590027, Final Batch Loss: 0.24610097706317902\n",
      "Epoch 4932, Loss: 0.6825388818979263, Final Batch Loss: 0.4044630527496338\n",
      "Epoch 4933, Loss: 0.48333820700645447, Final Batch Loss: 0.08233539760112762\n",
      "Epoch 4934, Loss: 0.49697670340538025, Final Batch Loss: 0.17270632088184357\n",
      "Epoch 4935, Loss: 0.498623788356781, Final Batch Loss: 0.12175688147544861\n",
      "Epoch 4936, Loss: 0.7033775299787521, Final Batch Loss: 0.38551318645477295\n",
      "Epoch 4937, Loss: 0.5227635353803635, Final Batch Loss: 0.1574247181415558\n",
      "Epoch 4938, Loss: 0.4129892736673355, Final Batch Loss: 0.08108009397983551\n",
      "Epoch 4939, Loss: 0.6093036532402039, Final Batch Loss: 0.1985311061143875\n",
      "Epoch 4940, Loss: 0.42429962009191513, Final Batch Loss: 0.1073724702000618\n",
      "Epoch 4941, Loss: 0.696181058883667, Final Batch Loss: 0.3457756042480469\n",
      "Epoch 4942, Loss: 0.5477099269628525, Final Batch Loss: 0.1667339950799942\n",
      "Epoch 4943, Loss: 0.5289955735206604, Final Batch Loss: 0.1120094358921051\n",
      "Epoch 4944, Loss: 0.41406404972076416, Final Batch Loss: 0.09664380550384521\n",
      "Epoch 4945, Loss: 0.38994260132312775, Final Batch Loss: 0.02757759392261505\n",
      "Epoch 4946, Loss: 0.6576088070869446, Final Batch Loss: 0.3327983021736145\n",
      "Epoch 4947, Loss: 0.6881430447101593, Final Batch Loss: 0.3302154541015625\n",
      "Epoch 4948, Loss: 0.5120989754796028, Final Batch Loss: 0.11206851154565811\n",
      "Epoch 4949, Loss: 0.49620968848466873, Final Batch Loss: 0.11781316250562668\n",
      "Epoch 4950, Loss: 0.5494644492864609, Final Batch Loss: 0.1919318288564682\n",
      "Epoch 4951, Loss: 0.4780034273862839, Final Batch Loss: 0.17181231081485748\n",
      "Epoch 4952, Loss: 0.5706905722618103, Final Batch Loss: 0.1724967509508133\n",
      "Epoch 4953, Loss: 0.48060742020606995, Final Batch Loss: 0.16050229966640472\n",
      "Epoch 4954, Loss: 0.4184345081448555, Final Batch Loss: 0.0386807844042778\n",
      "Epoch 4955, Loss: 0.5359576120972633, Final Batch Loss: 0.12138710170984268\n",
      "Epoch 4956, Loss: 0.3690633922815323, Final Batch Loss: 0.027133524417877197\n",
      "Epoch 4957, Loss: 0.4142023026943207, Final Batch Loss: 0.06659656763076782\n",
      "Epoch 4958, Loss: 0.6320199221372604, Final Batch Loss: 0.2746317386627197\n",
      "Epoch 4959, Loss: 0.5163793563842773, Final Batch Loss: 0.1790386140346527\n",
      "Epoch 4960, Loss: 0.41307375580072403, Final Batch Loss: 0.041685618460178375\n",
      "Epoch 4961, Loss: 0.5813554376363754, Final Batch Loss: 0.2777106761932373\n",
      "Epoch 4962, Loss: 1.1233498305082321, Final Batch Loss: 0.8024712800979614\n",
      "Epoch 4963, Loss: 0.7179632931947708, Final Batch Loss: 0.3579848110675812\n",
      "Epoch 4964, Loss: 0.39818014949560165, Final Batch Loss: 0.11125076562166214\n",
      "Epoch 4965, Loss: 0.4551924914121628, Final Batch Loss: 0.07143700122833252\n",
      "Epoch 4966, Loss: 0.47005072981119156, Final Batch Loss: 0.07325159758329391\n",
      "Epoch 4967, Loss: 0.7187431007623672, Final Batch Loss: 0.33415573835372925\n",
      "Epoch 4968, Loss: 0.49636150896549225, Final Batch Loss: 0.15612338483333588\n",
      "Epoch 4969, Loss: 0.5476634353399277, Final Batch Loss: 0.14340564608573914\n",
      "Epoch 4970, Loss: 0.736285924911499, Final Batch Loss: 0.4025801420211792\n",
      "Epoch 4971, Loss: 0.8061558604240417, Final Batch Loss: 0.333556592464447\n",
      "Epoch 4972, Loss: 0.48173272609710693, Final Batch Loss: 0.09540161490440369\n",
      "Epoch 4973, Loss: 0.4410518780350685, Final Batch Loss: 0.06518571823835373\n",
      "Epoch 4974, Loss: 0.437735952436924, Final Batch Loss: 0.10794699937105179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4975, Loss: 0.540269136428833, Final Batch Loss: 0.1484324187040329\n",
      "Epoch 4976, Loss: 0.48319507390260696, Final Batch Loss: 0.08595595508813858\n",
      "Epoch 4977, Loss: 0.5825638920068741, Final Batch Loss: 0.16121725738048553\n",
      "Epoch 4978, Loss: 0.6094497814774513, Final Batch Loss: 0.2919441759586334\n",
      "Epoch 4979, Loss: 0.610106498003006, Final Batch Loss: 0.21196182072162628\n",
      "Epoch 4980, Loss: 0.7131725549697876, Final Batch Loss: 0.2602177560329437\n",
      "Epoch 4981, Loss: 0.5090555250644684, Final Batch Loss: 0.1526234894990921\n",
      "Epoch 4982, Loss: 0.449469979852438, Final Batch Loss: 0.0513029508292675\n",
      "Epoch 4983, Loss: 0.43913131207227707, Final Batch Loss: 0.11312951892614365\n",
      "Epoch 4984, Loss: 0.584770455956459, Final Batch Loss: 0.287514328956604\n",
      "Epoch 4985, Loss: 0.5496720671653748, Final Batch Loss: 0.1941850632429123\n",
      "Epoch 4986, Loss: 0.40753748267889023, Final Batch Loss: 0.06299030035734177\n",
      "Epoch 4987, Loss: 0.7426247596740723, Final Batch Loss: 0.4283389449119568\n",
      "Epoch 4988, Loss: 0.4192238971590996, Final Batch Loss: 0.0642215833067894\n",
      "Epoch 4989, Loss: 0.3721051290631294, Final Batch Loss: 0.04659781605005264\n",
      "Epoch 4990, Loss: 0.5351880937814713, Final Batch Loss: 0.18872281908988953\n",
      "Epoch 4991, Loss: 0.5428479611873627, Final Batch Loss: 0.17044873535633087\n",
      "Epoch 4992, Loss: 0.4096638783812523, Final Batch Loss: 0.04591209441423416\n",
      "Epoch 4993, Loss: 0.5823901146650314, Final Batch Loss: 0.30581966042518616\n",
      "Epoch 4994, Loss: 0.43254952132701874, Final Batch Loss: 0.09837949275970459\n",
      "Epoch 4995, Loss: 0.6030162423849106, Final Batch Loss: 0.2476305216550827\n",
      "Epoch 4996, Loss: 0.5615298897027969, Final Batch Loss: 0.2775666117668152\n",
      "Epoch 4997, Loss: 0.7444089949131012, Final Batch Loss: 0.4400881826877594\n",
      "Epoch 4998, Loss: 0.6448443830013275, Final Batch Loss: 0.2628154754638672\n",
      "Epoch 4999, Loss: 0.6279133856296539, Final Batch Loss: 0.287841260433197\n",
      "Epoch 5000, Loss: 0.5433986335992813, Final Batch Loss: 0.177903413772583\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 14  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  4  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 14  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  7  0  0  0  0  0  0  0]\n",
      " [ 0  0  4  0  0  2  0  0  2  0  0  1]\n",
      " [ 0  0  0  0  1  1 11  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  7  0  0  0  0]\n",
      " [ 0  0  0  0  0  7  0  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 15  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        17\n",
      "           1    1.00000   1.00000   1.00000        14\n",
      "           2    0.50000   1.00000   0.66667         4\n",
      "           3    1.00000   0.93333   0.96552        15\n",
      "           4    0.77778   1.00000   0.87500         7\n",
      "           5    0.20000   0.22222   0.21053         9\n",
      "           6    1.00000   0.84615   0.91667        13\n",
      "           7    1.00000   1.00000   1.00000         7\n",
      "           8    0.80000   0.53333   0.64000        15\n",
      "           9    1.00000   1.00000   1.00000        15\n",
      "          10    1.00000   1.00000   1.00000         6\n",
      "          11    0.88889   1.00000   0.94118         8\n",
      "\n",
      "    accuracy                        0.86923       130\n",
      "   macro avg    0.84722   0.87792   0.85130       130\n",
      "weighted avg    0.88735   0.86923   0.87089       130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Fake Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20,000 epochs DEFAULT PARAMETERS FOR THRESHOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (gen): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=107, out_features=80, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=80, out_features=60, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=60, out_features=50, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Linear(in_features=50, out_features=37, bias=True)\n",
       "    (4): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator(z_dim = 107)\n",
    "load_model(gen, \"3 Label 4 Subject GAN Ablation_gen.param\")\n",
    "gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(X_test)\n",
    "latent_vectors = get_noise(size, 100)\n",
    "act_vectors = get_act_matrix(size, 3)\n",
    "usr_vectors = get_usr_matrix(size, 4)\n",
    "\n",
    "fake_labels = []\n",
    "\n",
    "for k in range(size):\n",
    "    if act_vectors[0][k] == 0 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(0)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(1)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(2)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(3)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(4)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(5)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(6)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(7)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(8)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 3:\n",
    "        fake_labels.append(9)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 3:\n",
    "        fake_labels.append(10)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 3:\n",
    "        fake_labels.append(11)\n",
    "        \n",
    "fake_labels = np.asarray(fake_labels)\n",
    "to_gen = torch.cat((latent_vectors, act_vectors[1], usr_vectors[1]), 1)\n",
    "fake_features = gen(to_gen).detach()\n",
    "#fake_features = gen(to_gen).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 11  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 15  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 10  0  0  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  2  0  0  3]\n",
      " [ 0  0  0  0  0  0 12  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  5  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  2  0  0 13  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        14\n",
      "           1    1.00000   1.00000   1.00000        12\n",
      "           2    1.00000   1.00000   1.00000        11\n",
      "           3    1.00000   1.00000   1.00000        15\n",
      "           4    1.00000   0.90909   0.95238        11\n",
      "           5    0.00000   0.00000   0.00000         5\n",
      "           6    1.00000   1.00000   1.00000        12\n",
      "           7    0.71429   1.00000   0.83333         5\n",
      "           8    0.00000   0.00000   0.00000        10\n",
      "           9    1.00000   1.00000   1.00000        10\n",
      "          10    0.92857   0.86667   0.89655        15\n",
      "          11    0.76923   1.00000   0.86957        10\n",
      "\n",
      "    accuracy                        0.86154       130\n",
      "   macro avg    0.78434   0.81465   0.79599       130\n",
      "weighted avg    0.84763   0.86154   0.85221       130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
