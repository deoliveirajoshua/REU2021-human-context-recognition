{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "\n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.405383586883545, Final Batch Loss: 2.201852798461914\n",
      "Epoch 2, Loss: 4.404971361160278, Final Batch Loss: 2.2026233673095703\n",
      "Epoch 3, Loss: 4.397129535675049, Final Batch Loss: 2.1937124729156494\n",
      "Epoch 4, Loss: 4.392755746841431, Final Batch Loss: 2.199692726135254\n",
      "Epoch 5, Loss: 4.386135578155518, Final Batch Loss: 2.1832103729248047\n",
      "Epoch 6, Loss: 4.387117147445679, Final Batch Loss: 2.196585178375244\n",
      "Epoch 7, Loss: 4.375082015991211, Final Batch Loss: 2.1840686798095703\n",
      "Epoch 8, Loss: 4.371187448501587, Final Batch Loss: 2.1790261268615723\n",
      "Epoch 9, Loss: 4.3661839962005615, Final Batch Loss: 2.173917055130005\n",
      "Epoch 10, Loss: 4.3590428829193115, Final Batch Loss: 2.1791534423828125\n",
      "Epoch 11, Loss: 4.351651430130005, Final Batch Loss: 2.1725096702575684\n",
      "Epoch 12, Loss: 4.3387885093688965, Final Batch Loss: 2.162135124206543\n",
      "Epoch 13, Loss: 4.331377029418945, Final Batch Loss: 2.1641385555267334\n",
      "Epoch 14, Loss: 4.320014715194702, Final Batch Loss: 2.1608285903930664\n",
      "Epoch 15, Loss: 4.303890943527222, Final Batch Loss: 2.1571080684661865\n",
      "Epoch 16, Loss: 4.291188955307007, Final Batch Loss: 2.1568429470062256\n",
      "Epoch 17, Loss: 4.260818719863892, Final Batch Loss: 2.1287686824798584\n",
      "Epoch 18, Loss: 4.254968643188477, Final Batch Loss: 2.1325507164001465\n",
      "Epoch 19, Loss: 4.220748662948608, Final Batch Loss: 2.1001203060150146\n",
      "Epoch 20, Loss: 4.202219009399414, Final Batch Loss: 2.097886562347412\n",
      "Epoch 21, Loss: 4.185516357421875, Final Batch Loss: 2.0943071842193604\n",
      "Epoch 22, Loss: 4.145824909210205, Final Batch Loss: 2.0651113986968994\n",
      "Epoch 23, Loss: 4.133363485336304, Final Batch Loss: 2.078066349029541\n",
      "Epoch 24, Loss: 4.063478708267212, Final Batch Loss: 2.0096778869628906\n",
      "Epoch 25, Loss: 4.060854196548462, Final Batch Loss: 2.0227768421173096\n",
      "Epoch 26, Loss: 4.02914023399353, Final Batch Loss: 2.02943754196167\n",
      "Epoch 27, Loss: 3.9881794452667236, Final Batch Loss: 1.9851815700531006\n",
      "Epoch 28, Loss: 3.9552472829818726, Final Batch Loss: 1.9765806198120117\n",
      "Epoch 29, Loss: 3.893885016441345, Final Batch Loss: 1.942198634147644\n",
      "Epoch 30, Loss: 3.865604281425476, Final Batch Loss: 1.9402152299880981\n",
      "Epoch 31, Loss: 3.825024366378784, Final Batch Loss: 1.9033927917480469\n",
      "Epoch 32, Loss: 3.763070225715637, Final Batch Loss: 1.8605053424835205\n",
      "Epoch 33, Loss: 3.7197086811065674, Final Batch Loss: 1.8722455501556396\n",
      "Epoch 34, Loss: 3.71358060836792, Final Batch Loss: 1.8879069089889526\n",
      "Epoch 35, Loss: 3.659753441810608, Final Batch Loss: 1.8355491161346436\n",
      "Epoch 36, Loss: 3.616026997566223, Final Batch Loss: 1.7699532508850098\n",
      "Epoch 37, Loss: 3.5374979972839355, Final Batch Loss: 1.7285561561584473\n",
      "Epoch 38, Loss: 3.5361056327819824, Final Batch Loss: 1.7696278095245361\n",
      "Epoch 39, Loss: 3.545248866081238, Final Batch Loss: 1.8248969316482544\n",
      "Epoch 40, Loss: 3.4512282609939575, Final Batch Loss: 1.7390285730361938\n",
      "Epoch 41, Loss: 3.3825511932373047, Final Batch Loss: 1.719124674797058\n",
      "Epoch 42, Loss: 3.3547195196151733, Final Batch Loss: 1.6963577270507812\n",
      "Epoch 43, Loss: 3.28294038772583, Final Batch Loss: 1.622254729270935\n",
      "Epoch 44, Loss: 3.2654982805252075, Final Batch Loss: 1.6060324907302856\n",
      "Epoch 45, Loss: 3.1932811737060547, Final Batch Loss: 1.5867255926132202\n",
      "Epoch 46, Loss: 3.130821108818054, Final Batch Loss: 1.5447142124176025\n",
      "Epoch 47, Loss: 3.092327356338501, Final Batch Loss: 1.5021015405654907\n",
      "Epoch 48, Loss: 3.0047603845596313, Final Batch Loss: 1.5063951015472412\n",
      "Epoch 49, Loss: 2.9764404296875, Final Batch Loss: 1.4415464401245117\n",
      "Epoch 50, Loss: 2.982679843902588, Final Batch Loss: 1.5151433944702148\n",
      "Epoch 51, Loss: 2.9238260984420776, Final Batch Loss: 1.504016399383545\n",
      "Epoch 52, Loss: 2.9154105186462402, Final Batch Loss: 1.4755909442901611\n",
      "Epoch 53, Loss: 2.85966420173645, Final Batch Loss: 1.3975480794906616\n",
      "Epoch 54, Loss: 2.8078057765960693, Final Batch Loss: 1.408368706703186\n",
      "Epoch 55, Loss: 2.7603553533554077, Final Batch Loss: 1.3664042949676514\n",
      "Epoch 56, Loss: 2.7925623655319214, Final Batch Loss: 1.3941373825073242\n",
      "Epoch 57, Loss: 2.7123361825942993, Final Batch Loss: 1.404461145401001\n",
      "Epoch 58, Loss: 2.709755301475525, Final Batch Loss: 1.3207381963729858\n",
      "Epoch 59, Loss: 2.622528314590454, Final Batch Loss: 1.3335496187210083\n",
      "Epoch 60, Loss: 2.6278361082077026, Final Batch Loss: 1.3185412883758545\n",
      "Epoch 61, Loss: 2.6469814777374268, Final Batch Loss: 1.3660995960235596\n",
      "Epoch 62, Loss: 2.583437919616699, Final Batch Loss: 1.2876735925674438\n",
      "Epoch 63, Loss: 2.5283042192459106, Final Batch Loss: 1.2264175415039062\n",
      "Epoch 64, Loss: 2.6183313131332397, Final Batch Loss: 1.2815595865249634\n",
      "Epoch 65, Loss: 2.5476391315460205, Final Batch Loss: 1.2853970527648926\n",
      "Epoch 66, Loss: 2.5417360067367554, Final Batch Loss: 1.2791166305541992\n",
      "Epoch 67, Loss: 2.4819291830062866, Final Batch Loss: 1.183213710784912\n",
      "Epoch 68, Loss: 2.5028258562088013, Final Batch Loss: 1.2434178590774536\n",
      "Epoch 69, Loss: 2.5386635065078735, Final Batch Loss: 1.269097924232483\n",
      "Epoch 70, Loss: 2.4549975395202637, Final Batch Loss: 1.2032864093780518\n",
      "Epoch 71, Loss: 2.4264150857925415, Final Batch Loss: 1.2390172481536865\n",
      "Epoch 72, Loss: 2.4847830533981323, Final Batch Loss: 1.2257356643676758\n",
      "Epoch 73, Loss: 2.4775959253311157, Final Batch Loss: 1.2056834697723389\n",
      "Epoch 74, Loss: 2.430105447769165, Final Batch Loss: 1.2240883111953735\n",
      "Epoch 75, Loss: 2.460474371910095, Final Batch Loss: 1.2451063394546509\n",
      "Epoch 76, Loss: 2.389818787574768, Final Batch Loss: 1.2010968923568726\n",
      "Epoch 77, Loss: 2.419793128967285, Final Batch Loss: 1.1749974489212036\n",
      "Epoch 78, Loss: 2.439544439315796, Final Batch Loss: 1.235525369644165\n",
      "Epoch 79, Loss: 2.3515381813049316, Final Batch Loss: 1.1769065856933594\n",
      "Epoch 80, Loss: 2.4220783710479736, Final Batch Loss: 1.1988599300384521\n",
      "Epoch 81, Loss: 2.4126570224761963, Final Batch Loss: 1.2444621324539185\n",
      "Epoch 82, Loss: 2.330819010734558, Final Batch Loss: 1.162988305091858\n",
      "Epoch 83, Loss: 2.3647814989089966, Final Batch Loss: 1.1341724395751953\n",
      "Epoch 84, Loss: 2.303493857383728, Final Batch Loss: 1.1726505756378174\n",
      "Epoch 85, Loss: 2.3605374097824097, Final Batch Loss: 1.1687567234039307\n",
      "Epoch 86, Loss: 2.3832132816314697, Final Batch Loss: 1.1625436544418335\n",
      "Epoch 87, Loss: 2.3044912815093994, Final Batch Loss: 1.1379170417785645\n",
      "Epoch 88, Loss: 2.3550184965133667, Final Batch Loss: 1.1524614095687866\n",
      "Epoch 89, Loss: 2.2935034036636353, Final Batch Loss: 1.095660924911499\n",
      "Epoch 90, Loss: 2.3223732709884644, Final Batch Loss: 1.2094956636428833\n",
      "Epoch 91, Loss: 2.2614388465881348, Final Batch Loss: 1.1028741598129272\n",
      "Epoch 92, Loss: 2.3883697986602783, Final Batch Loss: 1.2313308715820312\n",
      "Epoch 93, Loss: 2.278858780860901, Final Batch Loss: 1.2011542320251465\n",
      "Epoch 94, Loss: 2.311692237854004, Final Batch Loss: 1.1386626958847046\n",
      "Epoch 95, Loss: 2.2802940607070923, Final Batch Loss: 1.1647270917892456\n",
      "Epoch 96, Loss: 2.2281497716903687, Final Batch Loss: 1.0382397174835205\n",
      "Epoch 97, Loss: 2.233020782470703, Final Batch Loss: 1.0879995822906494\n",
      "Epoch 98, Loss: 2.260562539100647, Final Batch Loss: 1.0961579084396362\n",
      "Epoch 99, Loss: 2.238555431365967, Final Batch Loss: 1.1151185035705566\n",
      "Epoch 100, Loss: 2.2130976915359497, Final Batch Loss: 1.0778515338897705\n",
      "Epoch 101, Loss: 2.239717483520508, Final Batch Loss: 1.1030768156051636\n",
      "Epoch 102, Loss: 2.2694525718688965, Final Batch Loss: 1.1699055433273315\n",
      "Epoch 103, Loss: 2.1835747957229614, Final Batch Loss: 1.0601513385772705\n",
      "Epoch 104, Loss: 2.274611711502075, Final Batch Loss: 1.1526820659637451\n",
      "Epoch 105, Loss: 2.200469493865967, Final Batch Loss: 1.1278685331344604\n",
      "Epoch 106, Loss: 2.234267830848694, Final Batch Loss: 1.0671939849853516\n",
      "Epoch 107, Loss: 2.1855809688568115, Final Batch Loss: 1.0557916164398193\n",
      "Epoch 108, Loss: 2.1866376399993896, Final Batch Loss: 1.0570755004882812\n",
      "Epoch 109, Loss: 2.1568281650543213, Final Batch Loss: 1.077754259109497\n",
      "Epoch 110, Loss: 2.1410973072052, Final Batch Loss: 1.0838416814804077\n",
      "Epoch 111, Loss: 2.144243001937866, Final Batch Loss: 1.0988203287124634\n",
      "Epoch 112, Loss: 2.197993755340576, Final Batch Loss: 1.0598894357681274\n",
      "Epoch 113, Loss: 2.179400324821472, Final Batch Loss: 1.080406904220581\n",
      "Epoch 114, Loss: 2.1506248712539673, Final Batch Loss: 1.0619903802871704\n",
      "Epoch 115, Loss: 2.1871691942214966, Final Batch Loss: 1.1239088773727417\n",
      "Epoch 116, Loss: 2.1416406631469727, Final Batch Loss: 1.0567129850387573\n",
      "Epoch 117, Loss: 2.126489758491516, Final Batch Loss: 1.0483033657073975\n",
      "Epoch 118, Loss: 2.1952624320983887, Final Batch Loss: 1.09260094165802\n",
      "Epoch 119, Loss: 2.2101417779922485, Final Batch Loss: 1.1146528720855713\n",
      "Epoch 120, Loss: 2.1371498107910156, Final Batch Loss: 1.0427451133728027\n",
      "Epoch 121, Loss: 2.1692250967025757, Final Batch Loss: 1.1515209674835205\n",
      "Epoch 122, Loss: 2.1361173391342163, Final Batch Loss: 1.0514410734176636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123, Loss: 2.16373610496521, Final Batch Loss: 1.0867087841033936\n",
      "Epoch 124, Loss: 2.1806094646453857, Final Batch Loss: 1.1069432497024536\n",
      "Epoch 125, Loss: 2.1150310039520264, Final Batch Loss: 1.0048061609268188\n",
      "Epoch 126, Loss: 2.1067310571670532, Final Batch Loss: 1.0936949253082275\n",
      "Epoch 127, Loss: 2.115157961845398, Final Batch Loss: 1.0440527200698853\n",
      "Epoch 128, Loss: 2.037533760070801, Final Batch Loss: 0.998384952545166\n",
      "Epoch 129, Loss: 2.109804391860962, Final Batch Loss: 1.0228476524353027\n",
      "Epoch 130, Loss: 2.1053478717803955, Final Batch Loss: 1.0636951923370361\n",
      "Epoch 131, Loss: 2.069844365119934, Final Batch Loss: 1.0057666301727295\n",
      "Epoch 132, Loss: 2.0486834049224854, Final Batch Loss: 1.0357784032821655\n",
      "Epoch 133, Loss: 2.1233309507369995, Final Batch Loss: 1.0404165983200073\n",
      "Epoch 134, Loss: 2.080194115638733, Final Batch Loss: 1.0207065343856812\n",
      "Epoch 135, Loss: 2.132301688194275, Final Batch Loss: 1.1161409616470337\n",
      "Epoch 136, Loss: 2.106094002723694, Final Batch Loss: 1.0242748260498047\n",
      "Epoch 137, Loss: 2.0980772972106934, Final Batch Loss: 1.0405815839767456\n",
      "Epoch 138, Loss: 2.0463852882385254, Final Batch Loss: 1.0135815143585205\n",
      "Epoch 139, Loss: 2.0292803049087524, Final Batch Loss: 1.0151233673095703\n",
      "Epoch 140, Loss: 2.0411031246185303, Final Batch Loss: 1.0364360809326172\n",
      "Epoch 141, Loss: 1.978978157043457, Final Batch Loss: 0.9712920188903809\n",
      "Epoch 142, Loss: 2.112017273902893, Final Batch Loss: 1.0609186887741089\n",
      "Epoch 143, Loss: 2.0267962217330933, Final Batch Loss: 0.9325134754180908\n",
      "Epoch 144, Loss: 2.051663041114807, Final Batch Loss: 1.0186597108840942\n",
      "Epoch 145, Loss: 2.082073748111725, Final Batch Loss: 1.0889722108840942\n",
      "Epoch 146, Loss: 2.0221047401428223, Final Batch Loss: 1.0076960325241089\n",
      "Epoch 147, Loss: 2.0002440214157104, Final Batch Loss: 1.0395519733428955\n",
      "Epoch 148, Loss: 2.017783045768738, Final Batch Loss: 0.975103497505188\n",
      "Epoch 149, Loss: 1.9939444065093994, Final Batch Loss: 0.9589499235153198\n",
      "Epoch 150, Loss: 2.0662227272987366, Final Batch Loss: 1.0822525024414062\n",
      "Epoch 151, Loss: 2.0268527269363403, Final Batch Loss: 1.0063010454177856\n",
      "Epoch 152, Loss: 1.9833278059959412, Final Batch Loss: 1.0128918886184692\n",
      "Epoch 153, Loss: 1.9963539242744446, Final Batch Loss: 1.022158145904541\n",
      "Epoch 154, Loss: 2.0351486206054688, Final Batch Loss: 1.0410679578781128\n",
      "Epoch 155, Loss: 1.9463613033294678, Final Batch Loss: 0.9487178325653076\n",
      "Epoch 156, Loss: 2.003193736076355, Final Batch Loss: 0.9891029596328735\n",
      "Epoch 157, Loss: 2.0110732913017273, Final Batch Loss: 1.0203393697738647\n",
      "Epoch 158, Loss: 1.9997376799583435, Final Batch Loss: 1.055238127708435\n",
      "Epoch 159, Loss: 2.0086554288864136, Final Batch Loss: 1.0649323463439941\n",
      "Epoch 160, Loss: 1.9949359893798828, Final Batch Loss: 1.0573810338974\n",
      "Epoch 161, Loss: 1.9854381680488586, Final Batch Loss: 0.9967235922813416\n",
      "Epoch 162, Loss: 1.980501651763916, Final Batch Loss: 0.9996625185012817\n",
      "Epoch 163, Loss: 1.877092719078064, Final Batch Loss: 0.939430296421051\n",
      "Epoch 164, Loss: 1.9247812032699585, Final Batch Loss: 0.9687032103538513\n",
      "Epoch 165, Loss: 1.9643203020095825, Final Batch Loss: 0.9743373394012451\n",
      "Epoch 166, Loss: 1.964487075805664, Final Batch Loss: 0.9781466126441956\n",
      "Epoch 167, Loss: 1.9739100933074951, Final Batch Loss: 0.9763031005859375\n",
      "Epoch 168, Loss: 1.9080105423927307, Final Batch Loss: 0.9145011901855469\n",
      "Epoch 169, Loss: 1.9259438514709473, Final Batch Loss: 0.9322049021720886\n",
      "Epoch 170, Loss: 1.9264767169952393, Final Batch Loss: 0.9754823446273804\n",
      "Epoch 171, Loss: 1.8810805082321167, Final Batch Loss: 0.9229863882064819\n",
      "Epoch 172, Loss: 1.9101512432098389, Final Batch Loss: 0.9681804180145264\n",
      "Epoch 173, Loss: 1.9150601625442505, Final Batch Loss: 0.9898214340209961\n",
      "Epoch 174, Loss: 1.8422884941101074, Final Batch Loss: 0.9182578325271606\n",
      "Epoch 175, Loss: 1.8359190225601196, Final Batch Loss: 0.9146486520767212\n",
      "Epoch 176, Loss: 1.8828678131103516, Final Batch Loss: 0.9284282922744751\n",
      "Epoch 177, Loss: 1.8470879793167114, Final Batch Loss: 0.9069616794586182\n",
      "Epoch 178, Loss: 1.87321138381958, Final Batch Loss: 0.9483212828636169\n",
      "Epoch 179, Loss: 1.8149268627166748, Final Batch Loss: 0.9019656181335449\n",
      "Epoch 180, Loss: 1.8289219737052917, Final Batch Loss: 0.8881434202194214\n",
      "Epoch 181, Loss: 1.853126347064972, Final Batch Loss: 0.9277046918869019\n",
      "Epoch 182, Loss: 1.8420740365982056, Final Batch Loss: 0.9096950888633728\n",
      "Epoch 183, Loss: 1.8381720781326294, Final Batch Loss: 0.886372983455658\n",
      "Epoch 184, Loss: 1.7703102231025696, Final Batch Loss: 0.9241796135902405\n",
      "Epoch 185, Loss: 1.8132892847061157, Final Batch Loss: 0.9074848294258118\n",
      "Epoch 186, Loss: 1.8329365253448486, Final Batch Loss: 0.9369414448738098\n",
      "Epoch 187, Loss: 1.727713406085968, Final Batch Loss: 0.8685952425003052\n",
      "Epoch 188, Loss: 1.715838611125946, Final Batch Loss: 0.8458836078643799\n",
      "Epoch 189, Loss: 1.7486311793327332, Final Batch Loss: 0.8638294339179993\n",
      "Epoch 190, Loss: 1.708129346370697, Final Batch Loss: 0.835280179977417\n",
      "Epoch 191, Loss: 1.7653242945671082, Final Batch Loss: 0.8396417498588562\n",
      "Epoch 192, Loss: 1.648739218711853, Final Batch Loss: 0.7948136329650879\n",
      "Epoch 193, Loss: 1.7063198685646057, Final Batch Loss: 0.8732263445854187\n",
      "Epoch 194, Loss: 1.6628267168998718, Final Batch Loss: 0.8302009701728821\n",
      "Epoch 195, Loss: 1.6833413243293762, Final Batch Loss: 0.8553670048713684\n",
      "Epoch 196, Loss: 1.6629550457000732, Final Batch Loss: 0.8435684442520142\n",
      "Epoch 197, Loss: 1.6528591513633728, Final Batch Loss: 0.8539111614227295\n",
      "Epoch 198, Loss: 1.6599360704421997, Final Batch Loss: 0.7756866812705994\n",
      "Epoch 199, Loss: 1.6878083944320679, Final Batch Loss: 0.8511611223220825\n",
      "Epoch 200, Loss: 1.6391401886940002, Final Batch Loss: 0.7827062606811523\n",
      "Epoch 201, Loss: 1.612529218196869, Final Batch Loss: 0.8124570846557617\n",
      "Epoch 202, Loss: 1.7029243111610413, Final Batch Loss: 0.8503248691558838\n",
      "Epoch 203, Loss: 1.5775750279426575, Final Batch Loss: 0.8320700526237488\n",
      "Epoch 204, Loss: 1.5626316666603088, Final Batch Loss: 0.7509179711341858\n",
      "Epoch 205, Loss: 1.5902721285820007, Final Batch Loss: 0.7833977341651917\n",
      "Epoch 206, Loss: 1.547235906124115, Final Batch Loss: 0.7799819707870483\n",
      "Epoch 207, Loss: 1.5268127918243408, Final Batch Loss: 0.7650807499885559\n",
      "Epoch 208, Loss: 1.616645634174347, Final Batch Loss: 0.8183289766311646\n",
      "Epoch 209, Loss: 1.4947327971458435, Final Batch Loss: 0.737411379814148\n",
      "Epoch 210, Loss: 1.4839792847633362, Final Batch Loss: 0.7190853953361511\n",
      "Epoch 211, Loss: 1.507161259651184, Final Batch Loss: 0.7288567423820496\n",
      "Epoch 212, Loss: 1.4934043884277344, Final Batch Loss: 0.7232433557510376\n",
      "Epoch 213, Loss: 1.5092117190361023, Final Batch Loss: 0.8023524880409241\n",
      "Epoch 214, Loss: 1.5165104269981384, Final Batch Loss: 0.7717801332473755\n",
      "Epoch 215, Loss: 1.4024333953857422, Final Batch Loss: 0.6365237236022949\n",
      "Epoch 216, Loss: 1.4604855179786682, Final Batch Loss: 0.7279096841812134\n",
      "Epoch 217, Loss: 1.48303884267807, Final Batch Loss: 0.7737875580787659\n",
      "Epoch 218, Loss: 1.4036611914634705, Final Batch Loss: 0.6531803011894226\n",
      "Epoch 219, Loss: 1.5016976594924927, Final Batch Loss: 0.8299221992492676\n",
      "Epoch 220, Loss: 1.430027723312378, Final Batch Loss: 0.6767787933349609\n",
      "Epoch 221, Loss: 1.3635582327842712, Final Batch Loss: 0.684778094291687\n",
      "Epoch 222, Loss: 1.5503461360931396, Final Batch Loss: 0.8244674801826477\n",
      "Epoch 223, Loss: 1.3983681201934814, Final Batch Loss: 0.6790849566459656\n",
      "Epoch 224, Loss: 1.3607287406921387, Final Batch Loss: 0.6299747228622437\n",
      "Epoch 225, Loss: 1.388138473033905, Final Batch Loss: 0.7023063898086548\n",
      "Epoch 226, Loss: 1.3944289684295654, Final Batch Loss: 0.683578610420227\n",
      "Epoch 227, Loss: 1.3329434990882874, Final Batch Loss: 0.6833961009979248\n",
      "Epoch 228, Loss: 1.3763974905014038, Final Batch Loss: 0.6794464588165283\n",
      "Epoch 229, Loss: 1.4083111882209778, Final Batch Loss: 0.7755754590034485\n",
      "Epoch 230, Loss: 1.395678162574768, Final Batch Loss: 0.700915515422821\n",
      "Epoch 231, Loss: 1.4187570214271545, Final Batch Loss: 0.6937093734741211\n",
      "Epoch 232, Loss: 1.3275914192199707, Final Batch Loss: 0.6427658796310425\n",
      "Epoch 233, Loss: 1.3574497699737549, Final Batch Loss: 0.7027284502983093\n",
      "Epoch 234, Loss: 1.3160937428474426, Final Batch Loss: 0.6360321044921875\n",
      "Epoch 235, Loss: 1.3774038553237915, Final Batch Loss: 0.7228871583938599\n",
      "Epoch 236, Loss: 1.3044649362564087, Final Batch Loss: 0.6383777260780334\n",
      "Epoch 237, Loss: 1.299817144870758, Final Batch Loss: 0.633510410785675\n",
      "Epoch 238, Loss: 1.4051820635795593, Final Batch Loss: 0.7081478834152222\n",
      "Epoch 239, Loss: 1.3943896889686584, Final Batch Loss: 0.7342731952667236\n",
      "Epoch 240, Loss: 1.3163775205612183, Final Batch Loss: 0.6567878723144531\n",
      "Epoch 241, Loss: 1.3737143278121948, Final Batch Loss: 0.6763485074043274\n",
      "Epoch 242, Loss: 1.3063865303993225, Final Batch Loss: 0.6766223907470703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 243, Loss: 1.3491519093513489, Final Batch Loss: 0.6715535521507263\n",
      "Epoch 244, Loss: 1.3487282395362854, Final Batch Loss: 0.7018105983734131\n",
      "Epoch 245, Loss: 1.3051897883415222, Final Batch Loss: 0.7078912854194641\n",
      "Epoch 246, Loss: 1.2488600015640259, Final Batch Loss: 0.6031080484390259\n",
      "Epoch 247, Loss: 1.289712131023407, Final Batch Loss: 0.6095854043960571\n",
      "Epoch 248, Loss: 1.3782225847244263, Final Batch Loss: 0.7180049419403076\n",
      "Epoch 249, Loss: 1.2415176033973694, Final Batch Loss: 0.5920885801315308\n",
      "Epoch 250, Loss: 1.2607640027999878, Final Batch Loss: 0.6517081260681152\n",
      "Epoch 251, Loss: 1.34029221534729, Final Batch Loss: 0.6223857402801514\n",
      "Epoch 252, Loss: 1.294925332069397, Final Batch Loss: 0.684981107711792\n",
      "Epoch 253, Loss: 1.2807279825210571, Final Batch Loss: 0.6880307793617249\n",
      "Epoch 254, Loss: 1.3136653900146484, Final Batch Loss: 0.6613491177558899\n",
      "Epoch 255, Loss: 1.2571028470993042, Final Batch Loss: 0.6051924228668213\n",
      "Epoch 256, Loss: 1.2539365887641907, Final Batch Loss: 0.5836001038551331\n",
      "Epoch 257, Loss: 1.2884321808815002, Final Batch Loss: 0.7285367250442505\n",
      "Epoch 258, Loss: 1.232395350933075, Final Batch Loss: 0.6244206428527832\n",
      "Epoch 259, Loss: 1.3423314094543457, Final Batch Loss: 0.7335295677185059\n",
      "Epoch 260, Loss: 1.2128397822380066, Final Batch Loss: 0.6330527663230896\n",
      "Epoch 261, Loss: 1.1768280267715454, Final Batch Loss: 0.5347500443458557\n",
      "Epoch 262, Loss: 1.2172970175743103, Final Batch Loss: 0.6418519616127014\n",
      "Epoch 263, Loss: 1.1959736943244934, Final Batch Loss: 0.5721072554588318\n",
      "Epoch 264, Loss: 1.1960926055908203, Final Batch Loss: 0.6478192806243896\n",
      "Epoch 265, Loss: 1.225230872631073, Final Batch Loss: 0.6004093885421753\n",
      "Epoch 266, Loss: 1.3390859365463257, Final Batch Loss: 0.7539503574371338\n",
      "Epoch 267, Loss: 1.2828673720359802, Final Batch Loss: 0.6873612999916077\n",
      "Epoch 268, Loss: 1.259789228439331, Final Batch Loss: 0.6356004476547241\n",
      "Epoch 269, Loss: 1.222090482711792, Final Batch Loss: 0.6700092554092407\n",
      "Epoch 270, Loss: 1.2287176251411438, Final Batch Loss: 0.6373306512832642\n",
      "Epoch 271, Loss: 1.1834112405776978, Final Batch Loss: 0.5309606790542603\n",
      "Epoch 272, Loss: 1.161482572555542, Final Batch Loss: 0.5274713635444641\n",
      "Epoch 273, Loss: 1.2508674263954163, Final Batch Loss: 0.6599563956260681\n",
      "Epoch 274, Loss: 1.2154120206832886, Final Batch Loss: 0.5852076411247253\n",
      "Epoch 275, Loss: 1.2442802786827087, Final Batch Loss: 0.6151460409164429\n",
      "Epoch 276, Loss: 1.1907511949539185, Final Batch Loss: 0.5946530699729919\n",
      "Epoch 277, Loss: 1.126055896282196, Final Batch Loss: 0.530519425868988\n",
      "Epoch 278, Loss: 1.225630760192871, Final Batch Loss: 0.6328753232955933\n",
      "Epoch 279, Loss: 1.2595409154891968, Final Batch Loss: 0.6122129559516907\n",
      "Epoch 280, Loss: 1.2378641366958618, Final Batch Loss: 0.6603169441223145\n",
      "Epoch 281, Loss: 1.1576327681541443, Final Batch Loss: 0.5592419505119324\n",
      "Epoch 282, Loss: 1.207285463809967, Final Batch Loss: 0.6290748119354248\n",
      "Epoch 283, Loss: 1.25123530626297, Final Batch Loss: 0.6526413559913635\n",
      "Epoch 284, Loss: 1.218749761581421, Final Batch Loss: 0.6303938627243042\n",
      "Epoch 285, Loss: 1.1948845982551575, Final Batch Loss: 0.6026780009269714\n",
      "Epoch 286, Loss: 1.1925439238548279, Final Batch Loss: 0.5676923394203186\n",
      "Epoch 287, Loss: 1.165699303150177, Final Batch Loss: 0.6207412481307983\n",
      "Epoch 288, Loss: 1.1299272775650024, Final Batch Loss: 0.5058243870735168\n",
      "Epoch 289, Loss: 1.1569800078868866, Final Batch Loss: 0.49334052205085754\n",
      "Epoch 290, Loss: 1.1780078411102295, Final Batch Loss: 0.6191757917404175\n",
      "Epoch 291, Loss: 1.2021406292915344, Final Batch Loss: 0.5890083312988281\n",
      "Epoch 292, Loss: 1.1549214720726013, Final Batch Loss: 0.5653784871101379\n",
      "Epoch 293, Loss: 1.094584196805954, Final Batch Loss: 0.49359962344169617\n",
      "Epoch 294, Loss: 1.2109209299087524, Final Batch Loss: 0.6194136738777161\n",
      "Epoch 295, Loss: 1.1216115951538086, Final Batch Loss: 0.5231379866600037\n",
      "Epoch 296, Loss: 1.0887009501457214, Final Batch Loss: 0.5885032415390015\n",
      "Epoch 297, Loss: 1.1754581928253174, Final Batch Loss: 0.5670266151428223\n",
      "Epoch 298, Loss: 1.1807217001914978, Final Batch Loss: 0.6495361328125\n",
      "Epoch 299, Loss: 1.0979490876197815, Final Batch Loss: 0.5203308463096619\n",
      "Epoch 300, Loss: 1.1435638666152954, Final Batch Loss: 0.5919207334518433\n",
      "Epoch 301, Loss: 1.0942370295524597, Final Batch Loss: 0.5533954501152039\n",
      "Epoch 302, Loss: 1.1203349828720093, Final Batch Loss: 0.5610184073448181\n",
      "Epoch 303, Loss: 1.1196748614311218, Final Batch Loss: 0.543786883354187\n",
      "Epoch 304, Loss: 1.1420754194259644, Final Batch Loss: 0.5792246460914612\n",
      "Epoch 305, Loss: 1.1848831176757812, Final Batch Loss: 0.549635112285614\n",
      "Epoch 306, Loss: 1.0650621056556702, Final Batch Loss: 0.5039147734642029\n",
      "Epoch 307, Loss: 1.0525527596473694, Final Batch Loss: 0.5451768040657043\n",
      "Epoch 308, Loss: 1.1569900512695312, Final Batch Loss: 0.5673860907554626\n",
      "Epoch 309, Loss: 1.077818214893341, Final Batch Loss: 0.5280164480209351\n",
      "Epoch 310, Loss: 1.1585770845413208, Final Batch Loss: 0.6200652122497559\n",
      "Epoch 311, Loss: 1.0753206312656403, Final Batch Loss: 0.49420174956321716\n",
      "Epoch 312, Loss: 1.0691567659378052, Final Batch Loss: 0.5260657072067261\n",
      "Epoch 313, Loss: 1.0605546236038208, Final Batch Loss: 0.5195581912994385\n",
      "Epoch 314, Loss: 1.0501846671104431, Final Batch Loss: 0.5323010087013245\n",
      "Epoch 315, Loss: 1.1204975843429565, Final Batch Loss: 0.5857954025268555\n",
      "Epoch 316, Loss: 1.1601717472076416, Final Batch Loss: 0.5178923606872559\n",
      "Epoch 317, Loss: 1.1413851380348206, Final Batch Loss: 0.5755144953727722\n",
      "Epoch 318, Loss: 1.0968098044395447, Final Batch Loss: 0.5453402996063232\n",
      "Epoch 319, Loss: 1.0648603439331055, Final Batch Loss: 0.5169005393981934\n",
      "Epoch 320, Loss: 1.0815096497535706, Final Batch Loss: 0.5612123012542725\n",
      "Epoch 321, Loss: 1.1099154353141785, Final Batch Loss: 0.5345361828804016\n",
      "Epoch 322, Loss: 1.0745137333869934, Final Batch Loss: 0.5935807824134827\n",
      "Epoch 323, Loss: 1.1100753545761108, Final Batch Loss: 0.5505325198173523\n",
      "Epoch 324, Loss: 1.1213239431381226, Final Batch Loss: 0.5717745423316956\n",
      "Epoch 325, Loss: 1.0143145620822906, Final Batch Loss: 0.4953317940235138\n",
      "Epoch 326, Loss: 0.9927563071250916, Final Batch Loss: 0.4868171811103821\n",
      "Epoch 327, Loss: 1.0414042472839355, Final Batch Loss: 0.4658812880516052\n",
      "Epoch 328, Loss: 0.9929540157318115, Final Batch Loss: 0.46601027250289917\n",
      "Epoch 329, Loss: 1.0388730466365814, Final Batch Loss: 0.4793201982975006\n",
      "Epoch 330, Loss: 1.0831924676895142, Final Batch Loss: 0.5445356965065002\n",
      "Epoch 331, Loss: 1.0725824236869812, Final Batch Loss: 0.5665743350982666\n",
      "Epoch 332, Loss: 1.0944215059280396, Final Batch Loss: 0.5277270674705505\n",
      "Epoch 333, Loss: 1.1060121655464172, Final Batch Loss: 0.5548098683357239\n",
      "Epoch 334, Loss: 1.0770894289016724, Final Batch Loss: 0.5742000937461853\n",
      "Epoch 335, Loss: 1.0472496449947357, Final Batch Loss: 0.42723795771598816\n",
      "Epoch 336, Loss: 1.1107550859451294, Final Batch Loss: 0.6073482036590576\n",
      "Epoch 337, Loss: 1.0526407957077026, Final Batch Loss: 0.5025299191474915\n",
      "Epoch 338, Loss: 1.0832058787345886, Final Batch Loss: 0.5689578652381897\n",
      "Epoch 339, Loss: 1.1103145480155945, Final Batch Loss: 0.5887972712516785\n",
      "Epoch 340, Loss: 1.0660446286201477, Final Batch Loss: 0.5239917039871216\n",
      "Epoch 341, Loss: 1.1041142344474792, Final Batch Loss: 0.5750435590744019\n",
      "Epoch 342, Loss: 1.0296516418457031, Final Batch Loss: 0.500003457069397\n",
      "Epoch 343, Loss: 1.0435124933719635, Final Batch Loss: 0.543999195098877\n",
      "Epoch 344, Loss: 1.0727670788764954, Final Batch Loss: 0.5501936078071594\n",
      "Epoch 345, Loss: 1.0185433626174927, Final Batch Loss: 0.49163609743118286\n",
      "Epoch 346, Loss: 1.1126896739006042, Final Batch Loss: 0.6067796349525452\n",
      "Epoch 347, Loss: 1.0736015439033508, Final Batch Loss: 0.5524201393127441\n",
      "Epoch 348, Loss: 1.0159704387187958, Final Batch Loss: 0.521598219871521\n",
      "Epoch 349, Loss: 1.0405416190624237, Final Batch Loss: 0.5529497265815735\n",
      "Epoch 350, Loss: 0.9932117462158203, Final Batch Loss: 0.4464453458786011\n",
      "Epoch 351, Loss: 1.0830065310001373, Final Batch Loss: 0.6040450930595398\n",
      "Epoch 352, Loss: 1.0403542518615723, Final Batch Loss: 0.4700830578804016\n",
      "Epoch 353, Loss: 1.0019962787628174, Final Batch Loss: 0.5124610662460327\n",
      "Epoch 354, Loss: 1.035729557275772, Final Batch Loss: 0.5423884987831116\n",
      "Epoch 355, Loss: 1.0311731696128845, Final Batch Loss: 0.48570936918258667\n",
      "Epoch 356, Loss: 1.0185489058494568, Final Batch Loss: 0.4833981394767761\n",
      "Epoch 357, Loss: 1.084432601928711, Final Batch Loss: 0.5451813340187073\n",
      "Epoch 358, Loss: 1.0295758247375488, Final Batch Loss: 0.5487712025642395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 359, Loss: 0.947395384311676, Final Batch Loss: 0.49294471740722656\n",
      "Epoch 360, Loss: 1.0405330657958984, Final Batch Loss: 0.5228453278541565\n",
      "Epoch 361, Loss: 0.9886625409126282, Final Batch Loss: 0.4741535186767578\n",
      "Epoch 362, Loss: 1.0340560972690582, Final Batch Loss: 0.558902382850647\n",
      "Epoch 363, Loss: 1.008003056049347, Final Batch Loss: 0.46768879890441895\n",
      "Epoch 364, Loss: 1.0136909484863281, Final Batch Loss: 0.5202088356018066\n",
      "Epoch 365, Loss: 1.0068312287330627, Final Batch Loss: 0.5022833943367004\n",
      "Epoch 366, Loss: 1.0891092419624329, Final Batch Loss: 0.5681456327438354\n",
      "Epoch 367, Loss: 1.0092397034168243, Final Batch Loss: 0.5496039390563965\n",
      "Epoch 368, Loss: 0.9576459228992462, Final Batch Loss: 0.4691949486732483\n",
      "Epoch 369, Loss: 0.9433106780052185, Final Batch Loss: 0.4356922507286072\n",
      "Epoch 370, Loss: 1.041974514722824, Final Batch Loss: 0.5449028015136719\n",
      "Epoch 371, Loss: 0.9306096732616425, Final Batch Loss: 0.45407456159591675\n",
      "Epoch 372, Loss: 0.9348950982093811, Final Batch Loss: 0.4126654267311096\n",
      "Epoch 373, Loss: 0.9814417958259583, Final Batch Loss: 0.4722138047218323\n",
      "Epoch 374, Loss: 0.9063242673873901, Final Batch Loss: 0.4858907461166382\n",
      "Epoch 375, Loss: 0.9471488893032074, Final Batch Loss: 0.49721717834472656\n",
      "Epoch 376, Loss: 0.9913720786571503, Final Batch Loss: 0.5324040651321411\n",
      "Epoch 377, Loss: 0.9937039017677307, Final Batch Loss: 0.4783245921134949\n",
      "Epoch 378, Loss: 0.9068763852119446, Final Batch Loss: 0.46184587478637695\n",
      "Epoch 379, Loss: 0.9731670320034027, Final Batch Loss: 0.47122183442115784\n",
      "Epoch 380, Loss: 0.9841782748699188, Final Batch Loss: 0.5147253274917603\n",
      "Epoch 381, Loss: 0.968652069568634, Final Batch Loss: 0.520544171333313\n",
      "Epoch 382, Loss: 1.0070434510707855, Final Batch Loss: 0.5186697244644165\n",
      "Epoch 383, Loss: 0.9171182513237, Final Batch Loss: 0.42301130294799805\n",
      "Epoch 384, Loss: 1.0150047838687897, Final Batch Loss: 0.5322629809379578\n",
      "Epoch 385, Loss: 1.0130410194396973, Final Batch Loss: 0.503991425037384\n",
      "Epoch 386, Loss: 0.9744984805583954, Final Batch Loss: 0.5224359631538391\n",
      "Epoch 387, Loss: 0.9814243018627167, Final Batch Loss: 0.5255458354949951\n",
      "Epoch 388, Loss: 0.8624558746814728, Final Batch Loss: 0.3450517952442169\n",
      "Epoch 389, Loss: 1.0234520733356476, Final Batch Loss: 0.4946671426296234\n",
      "Epoch 390, Loss: 0.9763042032718658, Final Batch Loss: 0.5120992064476013\n",
      "Epoch 391, Loss: 0.9276717603206635, Final Batch Loss: 0.42671796679496765\n",
      "Epoch 392, Loss: 0.8878267705440521, Final Batch Loss: 0.3951902687549591\n",
      "Epoch 393, Loss: 0.92000412940979, Final Batch Loss: 0.4081277847290039\n",
      "Epoch 394, Loss: 0.9445280730724335, Final Batch Loss: 0.4515499770641327\n",
      "Epoch 395, Loss: 0.9333014190196991, Final Batch Loss: 0.46867993474006653\n",
      "Epoch 396, Loss: 0.9447349607944489, Final Batch Loss: 0.48035550117492676\n",
      "Epoch 397, Loss: 0.9924921989440918, Final Batch Loss: 0.4595203399658203\n",
      "Epoch 398, Loss: 1.0102701783180237, Final Batch Loss: 0.5182682871818542\n",
      "Epoch 399, Loss: 0.9024826288223267, Final Batch Loss: 0.4096919596195221\n",
      "Epoch 400, Loss: 0.9195505976676941, Final Batch Loss: 0.4424777925014496\n",
      "Epoch 401, Loss: 0.9208597242832184, Final Batch Loss: 0.46364375948905945\n",
      "Epoch 402, Loss: 0.8970434069633484, Final Batch Loss: 0.4033106863498688\n",
      "Epoch 403, Loss: 0.9446373283863068, Final Batch Loss: 0.47250428795814514\n",
      "Epoch 404, Loss: 0.9403258562088013, Final Batch Loss: 0.47078460454940796\n",
      "Epoch 405, Loss: 0.907255083322525, Final Batch Loss: 0.43464791774749756\n",
      "Epoch 406, Loss: 0.9193964302539825, Final Batch Loss: 0.4444126486778259\n",
      "Epoch 407, Loss: 0.9177522659301758, Final Batch Loss: 0.41423821449279785\n",
      "Epoch 408, Loss: 0.8957692086696625, Final Batch Loss: 0.4138745367527008\n",
      "Epoch 409, Loss: 0.9114925563335419, Final Batch Loss: 0.4528293311595917\n",
      "Epoch 410, Loss: 0.9604587852954865, Final Batch Loss: 0.5141907334327698\n",
      "Epoch 411, Loss: 0.9434491693973541, Final Batch Loss: 0.5107753276824951\n",
      "Epoch 412, Loss: 0.9279288053512573, Final Batch Loss: 0.45873570442199707\n",
      "Epoch 413, Loss: 0.9270049333572388, Final Batch Loss: 0.45236921310424805\n",
      "Epoch 414, Loss: 0.8533939719200134, Final Batch Loss: 0.3631236255168915\n",
      "Epoch 415, Loss: 0.8775071203708649, Final Batch Loss: 0.4467834532260895\n",
      "Epoch 416, Loss: 1.006407380104065, Final Batch Loss: 0.5870246887207031\n",
      "Epoch 417, Loss: 0.976001650094986, Final Batch Loss: 0.4540157616138458\n",
      "Epoch 418, Loss: 0.9204166233539581, Final Batch Loss: 0.43096014857292175\n",
      "Epoch 419, Loss: 0.9075718522071838, Final Batch Loss: 0.4703352153301239\n",
      "Epoch 420, Loss: 0.9377830624580383, Final Batch Loss: 0.4501693546772003\n",
      "Epoch 421, Loss: 0.8726295530796051, Final Batch Loss: 0.40644362568855286\n",
      "Epoch 422, Loss: 0.8563114106655121, Final Batch Loss: 0.3999404311180115\n",
      "Epoch 423, Loss: 0.8523678481578827, Final Batch Loss: 0.37439337372779846\n",
      "Epoch 424, Loss: 0.8803589940071106, Final Batch Loss: 0.39148902893066406\n",
      "Epoch 425, Loss: 0.8590553998947144, Final Batch Loss: 0.44352245330810547\n",
      "Epoch 426, Loss: 0.9141056537628174, Final Batch Loss: 0.4201118052005768\n",
      "Epoch 427, Loss: 0.8779027760028839, Final Batch Loss: 0.4081244170665741\n",
      "Epoch 428, Loss: 0.8652647435665131, Final Batch Loss: 0.36340150237083435\n",
      "Epoch 429, Loss: 0.8937983810901642, Final Batch Loss: 0.45460832118988037\n",
      "Epoch 430, Loss: 0.8624420762062073, Final Batch Loss: 0.45449739694595337\n",
      "Epoch 431, Loss: 0.883989155292511, Final Batch Loss: 0.40698572993278503\n",
      "Epoch 432, Loss: 0.898827999830246, Final Batch Loss: 0.4215408265590668\n",
      "Epoch 433, Loss: 0.99125537276268, Final Batch Loss: 0.5825363993644714\n",
      "Epoch 434, Loss: 0.840589851140976, Final Batch Loss: 0.41397908329963684\n",
      "Epoch 435, Loss: 0.831640362739563, Final Batch Loss: 0.3864932954311371\n",
      "Epoch 436, Loss: 0.9079359173774719, Final Batch Loss: 0.45772141218185425\n",
      "Epoch 437, Loss: 0.90328449010849, Final Batch Loss: 0.47511857748031616\n",
      "Epoch 438, Loss: 0.8560076653957367, Final Batch Loss: 0.38388705253601074\n",
      "Epoch 439, Loss: 0.8208765387535095, Final Batch Loss: 0.42738503217697144\n",
      "Epoch 440, Loss: 0.8913404643535614, Final Batch Loss: 0.4944228231906891\n",
      "Epoch 441, Loss: 0.8464014232158661, Final Batch Loss: 0.4528626799583435\n",
      "Epoch 442, Loss: 0.8824450969696045, Final Batch Loss: 0.4446731507778168\n",
      "Epoch 443, Loss: 0.8928394317626953, Final Batch Loss: 0.4785177409648895\n",
      "Epoch 444, Loss: 0.8549636006355286, Final Batch Loss: 0.3846020996570587\n",
      "Epoch 445, Loss: 0.8750013709068298, Final Batch Loss: 0.4458097517490387\n",
      "Epoch 446, Loss: 0.9213385283946991, Final Batch Loss: 0.49092724919319153\n",
      "Epoch 447, Loss: 0.8375117182731628, Final Batch Loss: 0.3880218267440796\n",
      "Epoch 448, Loss: 0.9072588682174683, Final Batch Loss: 0.5004857778549194\n",
      "Epoch 449, Loss: 0.9066002070903778, Final Batch Loss: 0.4227002263069153\n",
      "Epoch 450, Loss: 0.8414893746376038, Final Batch Loss: 0.41321173310279846\n",
      "Epoch 451, Loss: 0.88348788022995, Final Batch Loss: 0.4059877395629883\n",
      "Epoch 452, Loss: 0.8664754331111908, Final Batch Loss: 0.4155995547771454\n",
      "Epoch 453, Loss: 0.904787003993988, Final Batch Loss: 0.4466067850589752\n",
      "Epoch 454, Loss: 0.8063205778598785, Final Batch Loss: 0.4040951132774353\n",
      "Epoch 455, Loss: 0.8594397604465485, Final Batch Loss: 0.4012850224971771\n",
      "Epoch 456, Loss: 0.8465867042541504, Final Batch Loss: 0.40112683176994324\n",
      "Epoch 457, Loss: 0.8243498206138611, Final Batch Loss: 0.3981119394302368\n",
      "Epoch 458, Loss: 0.8238196074962616, Final Batch Loss: 0.40760472416877747\n",
      "Epoch 459, Loss: 0.8178367912769318, Final Batch Loss: 0.4123336374759674\n",
      "Epoch 460, Loss: 0.8770604729652405, Final Batch Loss: 0.45719778537750244\n",
      "Epoch 461, Loss: 0.8084254562854767, Final Batch Loss: 0.3935849964618683\n",
      "Epoch 462, Loss: 0.8716155886650085, Final Batch Loss: 0.44731539487838745\n",
      "Epoch 463, Loss: 0.8152352273464203, Final Batch Loss: 0.4137285053730011\n",
      "Epoch 464, Loss: 0.8369797766208649, Final Batch Loss: 0.40414124727249146\n",
      "Epoch 465, Loss: 0.9162398874759674, Final Batch Loss: 0.4640706479549408\n",
      "Epoch 466, Loss: 0.8309705853462219, Final Batch Loss: 0.4095251262187958\n",
      "Epoch 467, Loss: 0.8902813196182251, Final Batch Loss: 0.4603532552719116\n",
      "Epoch 468, Loss: 0.8394343554973602, Final Batch Loss: 0.4174885153770447\n",
      "Epoch 469, Loss: 0.7607637345790863, Final Batch Loss: 0.3733062148094177\n",
      "Epoch 470, Loss: 0.8123498558998108, Final Batch Loss: 0.37041255831718445\n",
      "Epoch 471, Loss: 0.8025075793266296, Final Batch Loss: 0.36809873580932617\n",
      "Epoch 472, Loss: 0.8382833302021027, Final Batch Loss: 0.37853774428367615\n",
      "Epoch 473, Loss: 0.8350960612297058, Final Batch Loss: 0.41633737087249756\n",
      "Epoch 474, Loss: 0.7960322499275208, Final Batch Loss: 0.3553400933742523\n",
      "Epoch 475, Loss: 0.8021215498447418, Final Batch Loss: 0.393717885017395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 476, Loss: 0.7760559022426605, Final Batch Loss: 0.40046581625938416\n",
      "Epoch 477, Loss: 0.8552139699459076, Final Batch Loss: 0.45569777488708496\n",
      "Epoch 478, Loss: 0.8262907266616821, Final Batch Loss: 0.39175987243652344\n",
      "Epoch 479, Loss: 0.8263705670833588, Final Batch Loss: 0.42783889174461365\n",
      "Epoch 480, Loss: 0.8242186009883881, Final Batch Loss: 0.3677403926849365\n",
      "Epoch 481, Loss: 0.7964741885662079, Final Batch Loss: 0.41721993684768677\n",
      "Epoch 482, Loss: 0.8120981156826019, Final Batch Loss: 0.39794355630874634\n",
      "Epoch 483, Loss: 0.8047182559967041, Final Batch Loss: 0.37830671668052673\n",
      "Epoch 484, Loss: 0.8218025863170624, Final Batch Loss: 0.39098623394966125\n",
      "Epoch 485, Loss: 0.8763949573040009, Final Batch Loss: 0.43904078006744385\n",
      "Epoch 486, Loss: 0.815151572227478, Final Batch Loss: 0.3647186756134033\n",
      "Epoch 487, Loss: 0.8206508457660675, Final Batch Loss: 0.4235013723373413\n",
      "Epoch 488, Loss: 0.7957474589347839, Final Batch Loss: 0.3994097113609314\n",
      "Epoch 489, Loss: 0.7709183990955353, Final Batch Loss: 0.4043119549751282\n",
      "Epoch 490, Loss: 0.816396564245224, Final Batch Loss: 0.37252259254455566\n",
      "Epoch 491, Loss: 0.7873692214488983, Final Batch Loss: 0.34777095913887024\n",
      "Epoch 492, Loss: 0.8154727220535278, Final Batch Loss: 0.37892261147499084\n",
      "Epoch 493, Loss: 0.7902380228042603, Final Batch Loss: 0.3156297206878662\n",
      "Epoch 494, Loss: 0.8268721103668213, Final Batch Loss: 0.39217421412467957\n",
      "Epoch 495, Loss: 0.8140005469322205, Final Batch Loss: 0.4508272409439087\n",
      "Epoch 496, Loss: 0.89727583527565, Final Batch Loss: 0.5165274143218994\n",
      "Epoch 497, Loss: 0.8639400899410248, Final Batch Loss: 0.4297221302986145\n",
      "Epoch 498, Loss: 0.8511328101158142, Final Batch Loss: 0.4272964894771576\n",
      "Epoch 499, Loss: 0.8243842720985413, Final Batch Loss: 0.4512673318386078\n",
      "Epoch 500, Loss: 0.8113555610179901, Final Batch Loss: 0.4161403775215149\n",
      "Epoch 501, Loss: 0.8253077268600464, Final Batch Loss: 0.42213505506515503\n",
      "Epoch 502, Loss: 0.8142988681793213, Final Batch Loss: 0.439472496509552\n",
      "Epoch 503, Loss: 0.8325052559375763, Final Batch Loss: 0.45300203561782837\n",
      "Epoch 504, Loss: 0.7330153584480286, Final Batch Loss: 0.362774133682251\n",
      "Epoch 505, Loss: 0.7707025706768036, Final Batch Loss: 0.33714303374290466\n",
      "Epoch 506, Loss: 0.8183109164237976, Final Batch Loss: 0.3898017704486847\n",
      "Epoch 507, Loss: 0.817058652639389, Final Batch Loss: 0.454678475856781\n",
      "Epoch 508, Loss: 0.7259833216667175, Final Batch Loss: 0.318776935338974\n",
      "Epoch 509, Loss: 0.7838178277015686, Final Batch Loss: 0.3991501033306122\n",
      "Epoch 510, Loss: 0.780542254447937, Final Batch Loss: 0.4067859649658203\n",
      "Epoch 511, Loss: 0.7890954315662384, Final Batch Loss: 0.39631590247154236\n",
      "Epoch 512, Loss: 0.7865724861621857, Final Batch Loss: 0.4122668206691742\n",
      "Epoch 513, Loss: 0.7589655220508575, Final Batch Loss: 0.3598416745662689\n",
      "Epoch 514, Loss: 0.855843186378479, Final Batch Loss: 0.428349107503891\n",
      "Epoch 515, Loss: 0.7761331498622894, Final Batch Loss: 0.41186827421188354\n",
      "Epoch 516, Loss: 0.7158119380474091, Final Batch Loss: 0.3648257851600647\n",
      "Epoch 517, Loss: 0.7966861724853516, Final Batch Loss: 0.41755732893943787\n",
      "Epoch 518, Loss: 0.7407517433166504, Final Batch Loss: 0.31679147481918335\n",
      "Epoch 519, Loss: 0.8356598317623138, Final Batch Loss: 0.46294236183166504\n",
      "Epoch 520, Loss: 0.7473678290843964, Final Batch Loss: 0.30956149101257324\n",
      "Epoch 521, Loss: 0.7661784887313843, Final Batch Loss: 0.430679589509964\n",
      "Epoch 522, Loss: 0.7553841769695282, Final Batch Loss: 0.3909353017807007\n",
      "Epoch 523, Loss: 0.807341456413269, Final Batch Loss: 0.3927081227302551\n",
      "Epoch 524, Loss: 0.8135027289390564, Final Batch Loss: 0.417176753282547\n",
      "Epoch 525, Loss: 0.789529412984848, Final Batch Loss: 0.39697402715682983\n",
      "Epoch 526, Loss: 0.782609611749649, Final Batch Loss: 0.38885191082954407\n",
      "Epoch 527, Loss: 0.8299678266048431, Final Batch Loss: 0.38293910026550293\n",
      "Epoch 528, Loss: 0.878860205411911, Final Batch Loss: 0.5333496928215027\n",
      "Epoch 529, Loss: 0.782249242067337, Final Batch Loss: 0.3943803906440735\n",
      "Epoch 530, Loss: 0.7417502403259277, Final Batch Loss: 0.38803786039352417\n",
      "Epoch 531, Loss: 0.7941329479217529, Final Batch Loss: 0.4009271264076233\n",
      "Epoch 532, Loss: 0.7362925708293915, Final Batch Loss: 0.3209685981273651\n",
      "Epoch 533, Loss: 0.8272538781166077, Final Batch Loss: 0.4010145962238312\n",
      "Epoch 534, Loss: 0.8091957867145538, Final Batch Loss: 0.3700815737247467\n",
      "Epoch 535, Loss: 0.7055580615997314, Final Batch Loss: 0.35332003235816956\n",
      "Epoch 536, Loss: 0.693436324596405, Final Batch Loss: 0.2981893718242645\n",
      "Epoch 537, Loss: 0.7377604842185974, Final Batch Loss: 0.3567078411579132\n",
      "Epoch 538, Loss: 0.783491462469101, Final Batch Loss: 0.4305099844932556\n",
      "Epoch 539, Loss: 0.744505912065506, Final Batch Loss: 0.35303738713264465\n",
      "Epoch 540, Loss: 0.7443259656429291, Final Batch Loss: 0.363025963306427\n",
      "Epoch 541, Loss: 0.7444848716259003, Final Batch Loss: 0.38082802295684814\n",
      "Epoch 542, Loss: 0.7952763140201569, Final Batch Loss: 0.3852459490299225\n",
      "Epoch 543, Loss: 0.7352778911590576, Final Batch Loss: 0.36396968364715576\n",
      "Epoch 544, Loss: 0.7931416630744934, Final Batch Loss: 0.391751766204834\n",
      "Epoch 545, Loss: 0.8083230257034302, Final Batch Loss: 0.4118405878543854\n",
      "Epoch 546, Loss: 0.7598262429237366, Final Batch Loss: 0.3450496196746826\n",
      "Epoch 547, Loss: 0.779891848564148, Final Batch Loss: 0.37201550602912903\n",
      "Epoch 548, Loss: 0.7277904748916626, Final Batch Loss: 0.3555014133453369\n",
      "Epoch 549, Loss: 0.7686697542667389, Final Batch Loss: 0.40313225984573364\n",
      "Epoch 550, Loss: 0.7467591166496277, Final Batch Loss: 0.3920787572860718\n",
      "Epoch 551, Loss: 0.8235809206962585, Final Batch Loss: 0.4493231177330017\n",
      "Epoch 552, Loss: 0.8002387583255768, Final Batch Loss: 0.41291680932044983\n",
      "Epoch 553, Loss: 0.8216128349304199, Final Batch Loss: 0.45023977756500244\n",
      "Epoch 554, Loss: 0.7245083749294281, Final Batch Loss: 0.3411734402179718\n",
      "Epoch 555, Loss: 0.8736377954483032, Final Batch Loss: 0.5117449760437012\n",
      "Epoch 556, Loss: 0.7938342392444611, Final Batch Loss: 0.4161570966243744\n",
      "Epoch 557, Loss: 0.7415725588798523, Final Batch Loss: 0.3879527449607849\n",
      "Epoch 558, Loss: 0.8252676427364349, Final Batch Loss: 0.4409514367580414\n",
      "Epoch 559, Loss: 0.751752495765686, Final Batch Loss: 0.38822218775749207\n",
      "Epoch 560, Loss: 0.723951131105423, Final Batch Loss: 0.3363521099090576\n",
      "Epoch 561, Loss: 0.7401263117790222, Final Batch Loss: 0.34564775228500366\n",
      "Epoch 562, Loss: 0.7481646537780762, Final Batch Loss: 0.333699494600296\n",
      "Epoch 563, Loss: 0.776790976524353, Final Batch Loss: 0.40817782282829285\n",
      "Epoch 564, Loss: 0.7762868702411652, Final Batch Loss: 0.43478694558143616\n",
      "Epoch 565, Loss: 0.7542915642261505, Final Batch Loss: 0.4023621082305908\n",
      "Epoch 566, Loss: 0.7397432029247284, Final Batch Loss: 0.37675240635871887\n",
      "Epoch 567, Loss: 0.7723738551139832, Final Batch Loss: 0.3828118145465851\n",
      "Epoch 568, Loss: 0.8237792551517487, Final Batch Loss: 0.4354189932346344\n",
      "Epoch 569, Loss: 0.7585417926311493, Final Batch Loss: 0.40195366740226746\n",
      "Epoch 570, Loss: 0.7071285545825958, Final Batch Loss: 0.3291817605495453\n",
      "Epoch 571, Loss: 0.7001598477363586, Final Batch Loss: 0.3038683235645294\n",
      "Epoch 572, Loss: 0.7352767288684845, Final Batch Loss: 0.33547261357307434\n",
      "Epoch 573, Loss: 0.7298330962657928, Final Batch Loss: 0.36394044756889343\n",
      "Epoch 574, Loss: 0.7155875861644745, Final Batch Loss: 0.2831285893917084\n",
      "Epoch 575, Loss: 0.7345897555351257, Final Batch Loss: 0.3722493350505829\n",
      "Epoch 576, Loss: 0.7748317718505859, Final Batch Loss: 0.375292032957077\n",
      "Epoch 577, Loss: 0.6950284540653229, Final Batch Loss: 0.31568291783332825\n",
      "Epoch 578, Loss: 0.8317791521549225, Final Batch Loss: 0.44122448563575745\n",
      "Epoch 579, Loss: 0.7276755571365356, Final Batch Loss: 0.32758828997612\n",
      "Epoch 580, Loss: 0.7460183799266815, Final Batch Loss: 0.3252926766872406\n",
      "Epoch 581, Loss: 0.7597982585430145, Final Batch Loss: 0.36827218532562256\n",
      "Epoch 582, Loss: 0.6681775152683258, Final Batch Loss: 0.29619890451431274\n",
      "Epoch 583, Loss: 0.7651773393154144, Final Batch Loss: 0.4315987527370453\n",
      "Epoch 584, Loss: 0.7161771357059479, Final Batch Loss: 0.30485767126083374\n",
      "Epoch 585, Loss: 0.7330937683582306, Final Batch Loss: 0.41483747959136963\n",
      "Epoch 586, Loss: 0.6841349005699158, Final Batch Loss: 0.32250645756721497\n",
      "Epoch 587, Loss: 0.7466537952423096, Final Batch Loss: 0.3633327782154083\n",
      "Epoch 588, Loss: 0.6754548847675323, Final Batch Loss: 0.33924445509910583\n",
      "Epoch 589, Loss: 0.7619315385818481, Final Batch Loss: 0.38771116733551025\n",
      "Epoch 590, Loss: 0.7202842831611633, Final Batch Loss: 0.2813723683357239\n",
      "Epoch 591, Loss: 0.7336200177669525, Final Batch Loss: 0.3619552552700043\n",
      "Epoch 592, Loss: 0.7472780048847198, Final Batch Loss: 0.3737923800945282\n",
      "Epoch 593, Loss: 0.7425658404827118, Final Batch Loss: 0.3600347936153412\n",
      "Epoch 594, Loss: 0.7416148781776428, Final Batch Loss: 0.36444079875946045\n",
      "Epoch 595, Loss: 0.6878325343132019, Final Batch Loss: 0.30452942848205566\n",
      "Epoch 596, Loss: 0.7524697184562683, Final Batch Loss: 0.3818163275718689\n",
      "Epoch 597, Loss: 0.7054603099822998, Final Batch Loss: 0.3512640595436096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 598, Loss: 0.7329679727554321, Final Batch Loss: 0.3210848271846771\n",
      "Epoch 599, Loss: 0.7669959962368011, Final Batch Loss: 0.31579428911209106\n",
      "Epoch 600, Loss: 0.797245442867279, Final Batch Loss: 0.4073101580142975\n",
      "Epoch 601, Loss: 0.7410597801208496, Final Batch Loss: 0.3632783591747284\n",
      "Epoch 602, Loss: 0.7446835041046143, Final Batch Loss: 0.3871619403362274\n",
      "Epoch 603, Loss: 0.7679025828838348, Final Batch Loss: 0.39043593406677246\n",
      "Epoch 604, Loss: 0.698123961687088, Final Batch Loss: 0.31763729453086853\n",
      "Epoch 605, Loss: 0.7651661336421967, Final Batch Loss: 0.41162940859794617\n",
      "Epoch 606, Loss: 0.7688064575195312, Final Batch Loss: 0.3598730266094208\n",
      "Epoch 607, Loss: 0.7622868120670319, Final Batch Loss: 0.3788635730743408\n",
      "Epoch 608, Loss: 0.7620730996131897, Final Batch Loss: 0.36696434020996094\n",
      "Epoch 609, Loss: 0.7291241884231567, Final Batch Loss: 0.3399078845977783\n",
      "Epoch 610, Loss: 0.6701287925243378, Final Batch Loss: 0.32325077056884766\n",
      "Epoch 611, Loss: 0.7344743013381958, Final Batch Loss: 0.3379655182361603\n",
      "Epoch 612, Loss: 0.7340889275074005, Final Batch Loss: 0.33592021465301514\n",
      "Epoch 613, Loss: 0.6875901222229004, Final Batch Loss: 0.31268635392189026\n",
      "Epoch 614, Loss: 0.7398104071617126, Final Batch Loss: 0.3735363483428955\n",
      "Epoch 615, Loss: 0.7049795687198639, Final Batch Loss: 0.33606991171836853\n",
      "Epoch 616, Loss: 0.7057949602603912, Final Batch Loss: 0.2892848253250122\n",
      "Epoch 617, Loss: 0.7547588050365448, Final Batch Loss: 0.3608233630657196\n",
      "Epoch 618, Loss: 0.7068778276443481, Final Batch Loss: 0.36033543944358826\n",
      "Epoch 619, Loss: 0.7171460390090942, Final Batch Loss: 0.33826443552970886\n",
      "Epoch 620, Loss: 0.6858446896076202, Final Batch Loss: 0.3267478048801422\n",
      "Epoch 621, Loss: 0.7233180403709412, Final Batch Loss: 0.37565121054649353\n",
      "Epoch 622, Loss: 0.7796693444252014, Final Batch Loss: 0.40641364455223083\n",
      "Epoch 623, Loss: 0.7079518735408783, Final Batch Loss: 0.3879886865615845\n",
      "Epoch 624, Loss: 0.7586726546287537, Final Batch Loss: 0.3969419300556183\n",
      "Epoch 625, Loss: 0.7247903048992157, Final Batch Loss: 0.3400244116783142\n",
      "Epoch 626, Loss: 0.7219618558883667, Final Batch Loss: 0.4050290882587433\n",
      "Epoch 627, Loss: 0.6921298801898956, Final Batch Loss: 0.3331921100616455\n",
      "Epoch 628, Loss: 0.7704454064369202, Final Batch Loss: 0.3831036686897278\n",
      "Epoch 629, Loss: 0.6733931005001068, Final Batch Loss: 0.2985541522502899\n",
      "Epoch 630, Loss: 0.6995261013507843, Final Batch Loss: 0.31731337308883667\n",
      "Epoch 631, Loss: 0.7259673476219177, Final Batch Loss: 0.3721577823162079\n",
      "Epoch 632, Loss: 0.7335415184497833, Final Batch Loss: 0.37823542952537537\n",
      "Epoch 633, Loss: 0.6681480705738068, Final Batch Loss: 0.3249862492084503\n",
      "Epoch 634, Loss: 0.7128065228462219, Final Batch Loss: 0.4030834436416626\n",
      "Epoch 635, Loss: 0.6911988258361816, Final Batch Loss: 0.32864144444465637\n",
      "Epoch 636, Loss: 0.7022882103919983, Final Batch Loss: 0.3709905743598938\n",
      "Epoch 637, Loss: 0.6890372633934021, Final Batch Loss: 0.35204243659973145\n",
      "Epoch 638, Loss: 0.6930339336395264, Final Batch Loss: 0.32267603278160095\n",
      "Epoch 639, Loss: 0.6782904267311096, Final Batch Loss: 0.33881324529647827\n",
      "Epoch 640, Loss: 0.7403891682624817, Final Batch Loss: 0.3587196171283722\n",
      "Epoch 641, Loss: 0.7763918936252594, Final Batch Loss: 0.4717080891132355\n",
      "Epoch 642, Loss: 0.7218190729618073, Final Batch Loss: 0.37987568974494934\n",
      "Epoch 643, Loss: 0.6446473896503448, Final Batch Loss: 0.2724689841270447\n",
      "Epoch 644, Loss: 0.6541178226470947, Final Batch Loss: 0.3645210266113281\n",
      "Epoch 645, Loss: 0.709247887134552, Final Batch Loss: 0.3173968195915222\n",
      "Epoch 646, Loss: 0.7324942946434021, Final Batch Loss: 0.35929831862449646\n",
      "Epoch 647, Loss: 0.7276276051998138, Final Batch Loss: 0.3504805266857147\n",
      "Epoch 648, Loss: 0.7228967249393463, Final Batch Loss: 0.3400408625602722\n",
      "Epoch 649, Loss: 0.7146991491317749, Final Batch Loss: 0.3444441556930542\n",
      "Epoch 650, Loss: 0.6801382601261139, Final Batch Loss: 0.3296067416667938\n",
      "Epoch 651, Loss: 0.6894874274730682, Final Batch Loss: 0.3517909646034241\n",
      "Epoch 652, Loss: 0.7520803511142731, Final Batch Loss: 0.4153023362159729\n",
      "Epoch 653, Loss: 0.6890683770179749, Final Batch Loss: 0.3074369430541992\n",
      "Epoch 654, Loss: 0.7031495869159698, Final Batch Loss: 0.3494889438152313\n",
      "Epoch 655, Loss: 0.7372283041477203, Final Batch Loss: 0.38567668199539185\n",
      "Epoch 656, Loss: 0.7343052625656128, Final Batch Loss: 0.39339685440063477\n",
      "Epoch 657, Loss: 0.7657492458820343, Final Batch Loss: 0.40028300881385803\n",
      "Epoch 658, Loss: 0.6702279150485992, Final Batch Loss: 0.3130273222923279\n",
      "Epoch 659, Loss: 0.656316488981247, Final Batch Loss: 0.30411043763160706\n",
      "Epoch 660, Loss: 0.6766802668571472, Final Batch Loss: 0.32875561714172363\n",
      "Epoch 661, Loss: 0.7179840505123138, Final Batch Loss: 0.37353256344795227\n",
      "Epoch 662, Loss: 0.6940656900405884, Final Batch Loss: 0.3654468059539795\n",
      "Epoch 663, Loss: 0.6965858936309814, Final Batch Loss: 0.31370630860328674\n",
      "Epoch 664, Loss: 0.7747543156147003, Final Batch Loss: 0.3399924635887146\n",
      "Epoch 665, Loss: 0.7104010283946991, Final Batch Loss: 0.38952699303627014\n",
      "Epoch 666, Loss: 0.723704606294632, Final Batch Loss: 0.3296389877796173\n",
      "Epoch 667, Loss: 0.7108733355998993, Final Batch Loss: 0.3663081228733063\n",
      "Epoch 668, Loss: 0.7137185335159302, Final Batch Loss: 0.3574996888637543\n",
      "Epoch 669, Loss: 0.6888615489006042, Final Batch Loss: 0.30134278535842896\n",
      "Epoch 670, Loss: 0.7117734849452972, Final Batch Loss: 0.37702077627182007\n",
      "Epoch 671, Loss: 0.7724936902523041, Final Batch Loss: 0.44060465693473816\n",
      "Epoch 672, Loss: 0.7071296274662018, Final Batch Loss: 0.2840366065502167\n",
      "Epoch 673, Loss: 0.6513940989971161, Final Batch Loss: 0.2952640950679779\n",
      "Epoch 674, Loss: 0.6988086104393005, Final Batch Loss: 0.34326285123825073\n",
      "Epoch 675, Loss: 0.7712488174438477, Final Batch Loss: 0.43396803736686707\n",
      "Epoch 676, Loss: 0.686182975769043, Final Batch Loss: 0.32384151220321655\n",
      "Epoch 677, Loss: 0.6557548642158508, Final Batch Loss: 0.34236767888069153\n",
      "Epoch 678, Loss: 0.6872403919696808, Final Batch Loss: 0.3523406982421875\n",
      "Epoch 679, Loss: 0.6449768841266632, Final Batch Loss: 0.34482771158218384\n",
      "Epoch 680, Loss: 0.6411408185958862, Final Batch Loss: 0.3003103733062744\n",
      "Epoch 681, Loss: 0.7172620296478271, Final Batch Loss: 0.39381369948387146\n",
      "Epoch 682, Loss: 0.7304939329624176, Final Batch Loss: 0.3781628906726837\n",
      "Epoch 683, Loss: 0.657229483127594, Final Batch Loss: 0.312170147895813\n",
      "Epoch 684, Loss: 0.6896577179431915, Final Batch Loss: 0.3448657989501953\n",
      "Epoch 685, Loss: 0.6810406446456909, Final Batch Loss: 0.34516939520835876\n",
      "Epoch 686, Loss: 0.6896534264087677, Final Batch Loss: 0.34039783477783203\n",
      "Epoch 687, Loss: 0.6893095076084137, Final Batch Loss: 0.35213735699653625\n",
      "Epoch 688, Loss: 0.7040091156959534, Final Batch Loss: 0.3564492464065552\n",
      "Epoch 689, Loss: 0.6039172410964966, Final Batch Loss: 0.26971834897994995\n",
      "Epoch 690, Loss: 0.7070635557174683, Final Batch Loss: 0.3531518578529358\n",
      "Epoch 691, Loss: 0.6563059985637665, Final Batch Loss: 0.2876671850681305\n",
      "Epoch 692, Loss: 0.6702547371387482, Final Batch Loss: 0.34804290533065796\n",
      "Epoch 693, Loss: 0.6990804076194763, Final Batch Loss: 0.3557986915111542\n",
      "Epoch 694, Loss: 0.6512498557567596, Final Batch Loss: 0.31300219893455505\n",
      "Epoch 695, Loss: 0.6951946020126343, Final Batch Loss: 0.311496764421463\n",
      "Epoch 696, Loss: 0.7283574938774109, Final Batch Loss: 0.42250049114227295\n",
      "Epoch 697, Loss: 0.68717360496521, Final Batch Loss: 0.33554407954216003\n",
      "Epoch 698, Loss: 0.6758927702903748, Final Batch Loss: 0.34042349457740784\n",
      "Epoch 699, Loss: 0.6739601492881775, Final Batch Loss: 0.339922159910202\n",
      "Epoch 700, Loss: 0.6758506894111633, Final Batch Loss: 0.3096356689929962\n",
      "Epoch 701, Loss: 0.7157142460346222, Final Batch Loss: 0.3536628484725952\n",
      "Epoch 702, Loss: 0.7027429044246674, Final Batch Loss: 0.36091628670692444\n",
      "Epoch 703, Loss: 0.6900884509086609, Final Batch Loss: 0.3596988916397095\n",
      "Epoch 704, Loss: 0.6822437345981598, Final Batch Loss: 0.3382315933704376\n",
      "Epoch 705, Loss: 0.7007359266281128, Final Batch Loss: 0.38891667127609253\n",
      "Epoch 706, Loss: 0.6885802745819092, Final Batch Loss: 0.39686185121536255\n",
      "Epoch 707, Loss: 0.6775928735733032, Final Batch Loss: 0.3223394751548767\n",
      "Epoch 708, Loss: 0.6747887134552002, Final Batch Loss: 0.2683170437812805\n",
      "Epoch 709, Loss: 0.6790608763694763, Final Batch Loss: 0.3420308828353882\n",
      "Epoch 710, Loss: 0.7454459369182587, Final Batch Loss: 0.3770543038845062\n",
      "Epoch 711, Loss: 0.6719643771648407, Final Batch Loss: 0.39071688055992126\n",
      "Epoch 712, Loss: 0.7161315381526947, Final Batch Loss: 0.3600938022136688\n",
      "Epoch 713, Loss: 0.718326061964035, Final Batch Loss: 0.40917879343032837\n",
      "Epoch 714, Loss: 0.7063867449760437, Final Batch Loss: 0.34230875968933105\n",
      "Epoch 715, Loss: 0.6873022019863129, Final Batch Loss: 0.33321061730384827\n",
      "Epoch 716, Loss: 0.6833157241344452, Final Batch Loss: 0.3624856770038605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 717, Loss: 0.676110714673996, Final Batch Loss: 0.3328125476837158\n",
      "Epoch 718, Loss: 0.6926352679729462, Final Batch Loss: 0.37575098872184753\n",
      "Epoch 719, Loss: 0.6916863322257996, Final Batch Loss: 0.370829313993454\n",
      "Epoch 720, Loss: 0.7124918401241302, Final Batch Loss: 0.3355335295200348\n",
      "Epoch 721, Loss: 0.708938866853714, Final Batch Loss: 0.3999345302581787\n",
      "Epoch 722, Loss: 0.7129819989204407, Final Batch Loss: 0.3823632299900055\n",
      "Epoch 723, Loss: 0.653776079416275, Final Batch Loss: 0.33573001623153687\n",
      "Epoch 724, Loss: 0.7022353410720825, Final Batch Loss: 0.3566330671310425\n",
      "Epoch 725, Loss: 0.6613897532224655, Final Batch Loss: 0.248664990067482\n",
      "Epoch 726, Loss: 0.6810433566570282, Final Batch Loss: 0.317440003156662\n",
      "Epoch 727, Loss: 0.6686355769634247, Final Batch Loss: 0.3429781198501587\n",
      "Epoch 728, Loss: 0.6184869706630707, Final Batch Loss: 0.2911582887172699\n",
      "Epoch 729, Loss: 0.6757216155529022, Final Batch Loss: 0.3177899122238159\n",
      "Epoch 730, Loss: 0.62405925989151, Final Batch Loss: 0.2664470076560974\n",
      "Epoch 731, Loss: 0.6899189352989197, Final Batch Loss: 0.3592723309993744\n",
      "Epoch 732, Loss: 0.7496534287929535, Final Batch Loss: 0.41697898507118225\n",
      "Epoch 733, Loss: 0.6598747372627258, Final Batch Loss: 0.2795485556125641\n",
      "Epoch 734, Loss: 0.665981650352478, Final Batch Loss: 0.28266504406929016\n",
      "Epoch 735, Loss: 0.6543663144111633, Final Batch Loss: 0.3704448342323303\n",
      "Epoch 736, Loss: 0.6547987163066864, Final Batch Loss: 0.30433180928230286\n",
      "Epoch 737, Loss: 0.6975735723972321, Final Batch Loss: 0.3632982075214386\n",
      "Epoch 738, Loss: 0.7062366902828217, Final Batch Loss: 0.37751510739326477\n",
      "Epoch 739, Loss: 0.614602655172348, Final Batch Loss: 0.2882458567619324\n",
      "Epoch 740, Loss: 0.6264925301074982, Final Batch Loss: 0.28515884280204773\n",
      "Epoch 741, Loss: 0.6720966398715973, Final Batch Loss: 0.3306454122066498\n",
      "Epoch 742, Loss: 0.71272012591362, Final Batch Loss: 0.3376467823982239\n",
      "Epoch 743, Loss: 0.7010882794857025, Final Batch Loss: 0.42724519968032837\n",
      "Epoch 744, Loss: 0.6388445496559143, Final Batch Loss: 0.3176882863044739\n",
      "Epoch 745, Loss: 0.6733058989048004, Final Batch Loss: 0.38143131136894226\n",
      "Epoch 746, Loss: 0.6942231059074402, Final Batch Loss: 0.35188591480255127\n",
      "Epoch 747, Loss: 0.7004864811897278, Final Batch Loss: 0.3411373496055603\n",
      "Epoch 748, Loss: 0.693938285112381, Final Batch Loss: 0.3288998603820801\n",
      "Epoch 749, Loss: 0.6750410497188568, Final Batch Loss: 0.376758873462677\n",
      "Epoch 750, Loss: 0.6362818777561188, Final Batch Loss: 0.3210350573062897\n",
      "Epoch 751, Loss: 0.620024710893631, Final Batch Loss: 0.34996044635772705\n",
      "Epoch 752, Loss: 0.6390878558158875, Final Batch Loss: 0.27782493829727173\n",
      "Epoch 753, Loss: 0.6808478832244873, Final Batch Loss: 0.3099205791950226\n",
      "Epoch 754, Loss: 0.6616891622543335, Final Batch Loss: 0.2979969382286072\n",
      "Epoch 755, Loss: 0.6541313827037811, Final Batch Loss: 0.31887000799179077\n",
      "Epoch 756, Loss: 0.7010435461997986, Final Batch Loss: 0.3481804430484772\n",
      "Epoch 757, Loss: 0.656152606010437, Final Batch Loss: 0.32627445459365845\n",
      "Epoch 758, Loss: 0.7267390191555023, Final Batch Loss: 0.3728616535663605\n",
      "Epoch 759, Loss: 0.6560819447040558, Final Batch Loss: 0.354591965675354\n",
      "Epoch 760, Loss: 0.6592092514038086, Final Batch Loss: 0.3263578414916992\n",
      "Epoch 761, Loss: 0.7206559777259827, Final Batch Loss: 0.39903679490089417\n",
      "Epoch 762, Loss: 0.6400600075721741, Final Batch Loss: 0.327343225479126\n",
      "Epoch 763, Loss: 0.6948544681072235, Final Batch Loss: 0.3517066538333893\n",
      "Epoch 764, Loss: 0.6592009663581848, Final Batch Loss: 0.34361743927001953\n",
      "Epoch 765, Loss: 0.6661630272865295, Final Batch Loss: 0.36677098274230957\n",
      "Epoch 766, Loss: 0.6979973912239075, Final Batch Loss: 0.35199466347694397\n",
      "Epoch 767, Loss: 0.6595882475376129, Final Batch Loss: 0.30725932121276855\n",
      "Epoch 768, Loss: 0.6315845549106598, Final Batch Loss: 0.25320184230804443\n",
      "Epoch 769, Loss: 0.6764335334300995, Final Batch Loss: 0.3752983808517456\n",
      "Epoch 770, Loss: 0.6577903628349304, Final Batch Loss: 0.30987828969955444\n",
      "Epoch 771, Loss: 0.6462113857269287, Final Batch Loss: 0.29702460765838623\n",
      "Epoch 772, Loss: 0.6117072105407715, Final Batch Loss: 0.25995302200317383\n",
      "Epoch 773, Loss: 0.6561844348907471, Final Batch Loss: 0.31406551599502563\n",
      "Epoch 774, Loss: 0.6321188509464264, Final Batch Loss: 0.2863144874572754\n",
      "Epoch 775, Loss: 0.6840603947639465, Final Batch Loss: 0.3462753891944885\n",
      "Epoch 776, Loss: 0.7186231017112732, Final Batch Loss: 0.34697479009628296\n",
      "Epoch 777, Loss: 0.6342114210128784, Final Batch Loss: 0.31397080421447754\n",
      "Epoch 778, Loss: 0.6366563439369202, Final Batch Loss: 0.3212626576423645\n",
      "Epoch 779, Loss: 0.6538185775279999, Final Batch Loss: 0.30574560165405273\n",
      "Epoch 780, Loss: 0.6059056520462036, Final Batch Loss: 0.3034802973270416\n",
      "Epoch 781, Loss: 0.6601805984973907, Final Batch Loss: 0.3320864140987396\n",
      "Epoch 782, Loss: 0.6312173008918762, Final Batch Loss: 0.30323490500450134\n",
      "Epoch 783, Loss: 0.6481422781944275, Final Batch Loss: 0.32467398047447205\n",
      "Epoch 784, Loss: 0.6302029490470886, Final Batch Loss: 0.305759459733963\n",
      "Epoch 785, Loss: 0.6213598549365997, Final Batch Loss: 0.29789263010025024\n",
      "Epoch 786, Loss: 0.5912440717220306, Final Batch Loss: 0.2923142611980438\n",
      "Epoch 787, Loss: 0.655038982629776, Final Batch Loss: 0.3184531629085541\n",
      "Epoch 788, Loss: 0.6613416969776154, Final Batch Loss: 0.30445438623428345\n",
      "Epoch 789, Loss: 0.6212041974067688, Final Batch Loss: 0.2758696675300598\n",
      "Epoch 790, Loss: 0.6615565121173859, Final Batch Loss: 0.3450833559036255\n",
      "Epoch 791, Loss: 0.7105551362037659, Final Batch Loss: 0.37076857686042786\n",
      "Epoch 792, Loss: 0.7047997117042542, Final Batch Loss: 0.34404394030570984\n",
      "Epoch 793, Loss: 0.6848379969596863, Final Batch Loss: 0.32797449827194214\n",
      "Epoch 794, Loss: 0.7059643268585205, Final Batch Loss: 0.30054378509521484\n",
      "Epoch 795, Loss: 0.645758867263794, Final Batch Loss: 0.3699834644794464\n",
      "Epoch 796, Loss: 0.6880171895027161, Final Batch Loss: 0.30586618185043335\n",
      "Epoch 797, Loss: 0.660841166973114, Final Batch Loss: 0.3426975905895233\n",
      "Epoch 798, Loss: 0.6249343156814575, Final Batch Loss: 0.30487769842147827\n",
      "Epoch 799, Loss: 0.6976996958255768, Final Batch Loss: 0.3500559628009796\n",
      "Epoch 800, Loss: 0.6631724238395691, Final Batch Loss: 0.2901085317134857\n",
      "Epoch 801, Loss: 0.7489349544048309, Final Batch Loss: 0.41322779655456543\n",
      "Epoch 802, Loss: 0.6246463060379028, Final Batch Loss: 0.30841293931007385\n",
      "Epoch 803, Loss: 0.6189592778682709, Final Batch Loss: 0.3066883087158203\n",
      "Epoch 804, Loss: 0.6817097663879395, Final Batch Loss: 0.36345311999320984\n",
      "Epoch 805, Loss: 0.6366564333438873, Final Batch Loss: 0.30450600385665894\n",
      "Epoch 806, Loss: 0.6298282146453857, Final Batch Loss: 0.29071009159088135\n",
      "Epoch 807, Loss: 0.6476273834705353, Final Batch Loss: 0.3146429657936096\n",
      "Epoch 808, Loss: 0.640990674495697, Final Batch Loss: 0.3154304027557373\n",
      "Epoch 809, Loss: 0.6329742968082428, Final Batch Loss: 0.33273279666900635\n",
      "Epoch 810, Loss: 0.6536889970302582, Final Batch Loss: 0.31681811809539795\n",
      "Epoch 811, Loss: 0.6105013489723206, Final Batch Loss: 0.3068370223045349\n",
      "Epoch 812, Loss: 0.6878911256790161, Final Batch Loss: 0.27778318524360657\n",
      "Epoch 813, Loss: 0.6118586659431458, Final Batch Loss: 0.3094465434551239\n",
      "Epoch 814, Loss: 0.6087575554847717, Final Batch Loss: 0.2641903758049011\n",
      "Epoch 815, Loss: 0.6178252398967743, Final Batch Loss: 0.3070682883262634\n",
      "Epoch 816, Loss: 0.6175583004951477, Final Batch Loss: 0.32077980041503906\n",
      "Epoch 817, Loss: 0.6483640074729919, Final Batch Loss: 0.30047017335891724\n",
      "Epoch 818, Loss: 0.6600844860076904, Final Batch Loss: 0.3290160000324249\n",
      "Epoch 819, Loss: 0.6442062854766846, Final Batch Loss: 0.272894948720932\n",
      "Epoch 820, Loss: 0.679303765296936, Final Batch Loss: 0.2977486550807953\n",
      "Epoch 821, Loss: 0.6732268929481506, Final Batch Loss: 0.35388892889022827\n",
      "Epoch 822, Loss: 0.6555521786212921, Final Batch Loss: 0.32876476645469666\n",
      "Epoch 823, Loss: 0.673433780670166, Final Batch Loss: 0.37642258405685425\n",
      "Epoch 824, Loss: 0.6524703800678253, Final Batch Loss: 0.29538124799728394\n",
      "Epoch 825, Loss: 0.7597168385982513, Final Batch Loss: 0.39466577768325806\n",
      "Epoch 826, Loss: 0.6220625340938568, Final Batch Loss: 0.34232547879219055\n",
      "Epoch 827, Loss: 0.6115875691175461, Final Batch Loss: 0.24704666435718536\n",
      "Epoch 828, Loss: 0.6281781196594238, Final Batch Loss: 0.3178402781486511\n",
      "Epoch 829, Loss: 0.6269543170928955, Final Batch Loss: 0.3265816867351532\n",
      "Epoch 830, Loss: 0.6618245244026184, Final Batch Loss: 0.35106414556503296\n",
      "Epoch 831, Loss: 0.5757760405540466, Final Batch Loss: 0.24641519784927368\n",
      "Epoch 832, Loss: 0.7035540342330933, Final Batch Loss: 0.3380572199821472\n",
      "Epoch 833, Loss: 0.6192406415939331, Final Batch Loss: 0.2961730360984802\n",
      "Epoch 834, Loss: 0.6371096670627594, Final Batch Loss: 0.36702290177345276\n",
      "Epoch 835, Loss: 0.6573505997657776, Final Batch Loss: 0.37716150283813477\n",
      "Epoch 836, Loss: 0.6340053975582123, Final Batch Loss: 0.3276166319847107\n",
      "Epoch 837, Loss: 0.632210910320282, Final Batch Loss: 0.2986631691455841\n",
      "Epoch 838, Loss: 0.6182344555854797, Final Batch Loss: 0.35549813508987427\n",
      "Epoch 839, Loss: 0.6138340830802917, Final Batch Loss: 0.3146308958530426\n",
      "Epoch 840, Loss: 0.6641372740268707, Final Batch Loss: 0.4102481007575989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 841, Loss: 0.6906668543815613, Final Batch Loss: 0.33011066913604736\n",
      "Epoch 842, Loss: 0.6516812741756439, Final Batch Loss: 0.3215903639793396\n",
      "Epoch 843, Loss: 0.6225244104862213, Final Batch Loss: 0.2877311110496521\n",
      "Epoch 844, Loss: 0.68944451212883, Final Batch Loss: 0.33105990290641785\n",
      "Epoch 845, Loss: 0.5861771702766418, Final Batch Loss: 0.31433144211769104\n",
      "Epoch 846, Loss: 0.5973281264305115, Final Batch Loss: 0.31704702973365784\n",
      "Epoch 847, Loss: 0.640621691942215, Final Batch Loss: 0.28865429759025574\n",
      "Epoch 848, Loss: 0.651847243309021, Final Batch Loss: 0.3301696479320526\n",
      "Epoch 849, Loss: 0.6484724879264832, Final Batch Loss: 0.3307799696922302\n",
      "Epoch 850, Loss: 0.6124627888202667, Final Batch Loss: 0.31609711050987244\n",
      "Epoch 851, Loss: 0.6400063633918762, Final Batch Loss: 0.3232375979423523\n",
      "Epoch 852, Loss: 0.6630488634109497, Final Batch Loss: 0.3554117679595947\n",
      "Epoch 853, Loss: 0.6754636168479919, Final Batch Loss: 0.37497755885124207\n",
      "Epoch 854, Loss: 0.6201711297035217, Final Batch Loss: 0.34344491362571716\n",
      "Epoch 855, Loss: 0.6493380069732666, Final Batch Loss: 0.33983030915260315\n",
      "Epoch 856, Loss: 0.6695795655250549, Final Batch Loss: 0.38764601945877075\n",
      "Epoch 857, Loss: 0.6465642750263214, Final Batch Loss: 0.305080384016037\n",
      "Epoch 858, Loss: 0.6001742780208588, Final Batch Loss: 0.27327394485473633\n",
      "Epoch 859, Loss: 0.5805865973234177, Final Batch Loss: 0.23922313749790192\n",
      "Epoch 860, Loss: 0.6239687502384186, Final Batch Loss: 0.25937801599502563\n",
      "Epoch 861, Loss: 0.6301103830337524, Final Batch Loss: 0.3371028006076813\n",
      "Epoch 862, Loss: 0.6253406405448914, Final Batch Loss: 0.31155678629875183\n",
      "Epoch 863, Loss: 0.6272740364074707, Final Batch Loss: 0.29484203457832336\n",
      "Epoch 864, Loss: 0.6494691371917725, Final Batch Loss: 0.33886390924453735\n",
      "Epoch 865, Loss: 0.6027327477931976, Final Batch Loss: 0.283467173576355\n",
      "Epoch 866, Loss: 0.6167959272861481, Final Batch Loss: 0.28636249899864197\n",
      "Epoch 867, Loss: 0.5986863374710083, Final Batch Loss: 0.2950318157672882\n",
      "Epoch 868, Loss: 0.5599917471408844, Final Batch Loss: 0.25042852759361267\n",
      "Epoch 869, Loss: 0.6256219744682312, Final Batch Loss: 0.3397314250469208\n",
      "Epoch 870, Loss: 0.6270741820335388, Final Batch Loss: 0.3445579409599304\n",
      "Epoch 871, Loss: 0.6462386548519135, Final Batch Loss: 0.3468312621116638\n",
      "Epoch 872, Loss: 0.5742526650428772, Final Batch Loss: 0.279432088136673\n",
      "Epoch 873, Loss: 0.644744336605072, Final Batch Loss: 0.326856404542923\n",
      "Epoch 874, Loss: 0.6240502893924713, Final Batch Loss: 0.3093976378440857\n",
      "Epoch 875, Loss: 0.616375058889389, Final Batch Loss: 0.3368369936943054\n",
      "Epoch 876, Loss: 0.5584366172552109, Final Batch Loss: 0.2341325730085373\n",
      "Epoch 877, Loss: 0.6031648814678192, Final Batch Loss: 0.2698368728160858\n",
      "Epoch 878, Loss: 0.6000663340091705, Final Batch Loss: 0.2593047320842743\n",
      "Epoch 879, Loss: 0.6367787420749664, Final Batch Loss: 0.3261907994747162\n",
      "Epoch 880, Loss: 0.6354039311408997, Final Batch Loss: 0.3393523693084717\n",
      "Epoch 881, Loss: 0.6257416605949402, Final Batch Loss: 0.30697351694107056\n",
      "Epoch 882, Loss: 0.6442130208015442, Final Batch Loss: 0.35941389203071594\n",
      "Epoch 883, Loss: 0.5790001451969147, Final Batch Loss: 0.2769738435745239\n",
      "Epoch 884, Loss: 0.6113930940628052, Final Batch Loss: 0.32456788420677185\n",
      "Epoch 885, Loss: 0.5989092886447906, Final Batch Loss: 0.294019490480423\n",
      "Epoch 886, Loss: 0.6169301569461823, Final Batch Loss: 0.30938366055488586\n",
      "Epoch 887, Loss: 0.6628957986831665, Final Batch Loss: 0.38554754853248596\n",
      "Epoch 888, Loss: 0.6514212191104889, Final Batch Loss: 0.3684261739253998\n",
      "Epoch 889, Loss: 0.607719749212265, Final Batch Loss: 0.29249897599220276\n",
      "Epoch 890, Loss: 0.6355547308921814, Final Batch Loss: 0.32456663250923157\n",
      "Epoch 891, Loss: 0.5939337611198425, Final Batch Loss: 0.30930250883102417\n",
      "Epoch 892, Loss: 0.6537628471851349, Final Batch Loss: 0.37529608607292175\n",
      "Epoch 893, Loss: 0.5581333935260773, Final Batch Loss: 0.2725986838340759\n",
      "Epoch 894, Loss: 0.6496586799621582, Final Batch Loss: 0.37373584508895874\n",
      "Epoch 895, Loss: 0.555044412612915, Final Batch Loss: 0.2861103415489197\n",
      "Epoch 896, Loss: 0.6640342473983765, Final Batch Loss: 0.3018207550048828\n",
      "Epoch 897, Loss: 0.692458987236023, Final Batch Loss: 0.38970527052879333\n",
      "Epoch 898, Loss: 0.5845625102519989, Final Batch Loss: 0.30883678793907166\n",
      "Epoch 899, Loss: 0.6670628190040588, Final Batch Loss: 0.373887836933136\n",
      "Epoch 900, Loss: 0.6734813749790192, Final Batch Loss: 0.33960428833961487\n",
      "Epoch 901, Loss: 0.6350630521774292, Final Batch Loss: 0.3489759564399719\n",
      "Epoch 902, Loss: 0.6432079076766968, Final Batch Loss: 0.3468913435935974\n",
      "Epoch 903, Loss: 0.6371628046035767, Final Batch Loss: 0.33657094836235046\n",
      "Epoch 904, Loss: 0.5876602530479431, Final Batch Loss: 0.267692506313324\n",
      "Epoch 905, Loss: 0.6181802153587341, Final Batch Loss: 0.2824203670024872\n",
      "Epoch 906, Loss: 0.623686820268631, Final Batch Loss: 0.3123527467250824\n",
      "Epoch 907, Loss: 0.5787763595581055, Final Batch Loss: 0.30539098381996155\n",
      "Epoch 908, Loss: 0.6080020070075989, Final Batch Loss: 0.2962745130062103\n",
      "Epoch 909, Loss: 0.5842060744762421, Final Batch Loss: 0.2810267508029938\n",
      "Epoch 910, Loss: 0.656080424785614, Final Batch Loss: 0.34989652037620544\n",
      "Epoch 911, Loss: 0.6172344088554382, Final Batch Loss: 0.34079670906066895\n",
      "Epoch 912, Loss: 0.5988965630531311, Final Batch Loss: 0.3103320002555847\n",
      "Epoch 913, Loss: 0.664550393819809, Final Batch Loss: 0.36185500025749207\n",
      "Epoch 914, Loss: 0.6771647930145264, Final Batch Loss: 0.37030303478240967\n",
      "Epoch 915, Loss: 0.630767285823822, Final Batch Loss: 0.308838814496994\n",
      "Epoch 916, Loss: 0.5977337062358856, Final Batch Loss: 0.2882455587387085\n",
      "Epoch 917, Loss: 0.6224308013916016, Final Batch Loss: 0.2711924612522125\n",
      "Epoch 918, Loss: 0.638393759727478, Final Batch Loss: 0.33670374751091003\n",
      "Epoch 919, Loss: 0.6142743825912476, Final Batch Loss: 0.3435027599334717\n",
      "Epoch 920, Loss: 0.6105928421020508, Final Batch Loss: 0.27280235290527344\n",
      "Epoch 921, Loss: 0.5706393420696259, Final Batch Loss: 0.2960214614868164\n",
      "Epoch 922, Loss: 0.6473964750766754, Final Batch Loss: 0.3600054681301117\n",
      "Epoch 923, Loss: 0.5780000686645508, Final Batch Loss: 0.25689828395843506\n",
      "Epoch 924, Loss: 0.5982859432697296, Final Batch Loss: 0.2718972861766815\n",
      "Epoch 925, Loss: 0.6319579482078552, Final Batch Loss: 0.34308138489723206\n",
      "Epoch 926, Loss: 0.645597368478775, Final Batch Loss: 0.314503014087677\n",
      "Epoch 927, Loss: 0.613834410905838, Final Batch Loss: 0.3016836941242218\n",
      "Epoch 928, Loss: 0.6011587083339691, Final Batch Loss: 0.3283270001411438\n",
      "Epoch 929, Loss: 0.6095897853374481, Final Batch Loss: 0.30057042837142944\n",
      "Epoch 930, Loss: 0.5943849682807922, Final Batch Loss: 0.3191053569316864\n",
      "Epoch 931, Loss: 0.5866512060165405, Final Batch Loss: 0.28093421459198\n",
      "Epoch 932, Loss: 0.5811115205287933, Final Batch Loss: 0.27206993103027344\n",
      "Epoch 933, Loss: 0.5950334668159485, Final Batch Loss: 0.281563937664032\n",
      "Epoch 934, Loss: 0.6564210057258606, Final Batch Loss: 0.2927013337612152\n",
      "Epoch 935, Loss: 0.6142938435077667, Final Batch Loss: 0.3369337022304535\n",
      "Epoch 936, Loss: 0.5548076331615448, Final Batch Loss: 0.26497483253479004\n",
      "Epoch 937, Loss: 0.575867623090744, Final Batch Loss: 0.28173497319221497\n",
      "Epoch 938, Loss: 0.598914235830307, Final Batch Loss: 0.27124765515327454\n",
      "Epoch 939, Loss: 0.6309520304203033, Final Batch Loss: 0.3470281660556793\n",
      "Epoch 940, Loss: 0.6142342984676361, Final Batch Loss: 0.31490397453308105\n",
      "Epoch 941, Loss: 0.6206128597259521, Final Batch Loss: 0.3293299078941345\n",
      "Epoch 942, Loss: 0.6107802540063858, Final Batch Loss: 0.24631531536579132\n",
      "Epoch 943, Loss: 0.6269001066684723, Final Batch Loss: 0.28532278537750244\n",
      "Epoch 944, Loss: 0.6468273103237152, Final Batch Loss: 0.33189669251441956\n",
      "Epoch 945, Loss: 0.5904496908187866, Final Batch Loss: 0.3536205291748047\n",
      "Epoch 946, Loss: 0.5780234038829803, Final Batch Loss: 0.26068079471588135\n",
      "Epoch 947, Loss: 0.5703603029251099, Final Batch Loss: 0.26782989501953125\n",
      "Epoch 948, Loss: 0.6619380116462708, Final Batch Loss: 0.3955981433391571\n",
      "Epoch 949, Loss: 0.581875741481781, Final Batch Loss: 0.2871988117694855\n",
      "Epoch 950, Loss: 0.621539980173111, Final Batch Loss: 0.33440473675727844\n",
      "Epoch 951, Loss: 0.6769856512546539, Final Batch Loss: 0.36955952644348145\n",
      "Epoch 952, Loss: 0.5711320340633392, Final Batch Loss: 0.25820499658584595\n",
      "Epoch 953, Loss: 0.6042416989803314, Final Batch Loss: 0.29381561279296875\n",
      "Epoch 954, Loss: 0.5832414925098419, Final Batch Loss: 0.3100733757019043\n",
      "Epoch 955, Loss: 0.5928449332714081, Final Batch Loss: 0.28288206458091736\n",
      "Epoch 956, Loss: 0.6127108037471771, Final Batch Loss: 0.29069918394088745\n",
      "Epoch 957, Loss: 0.5958929359912872, Final Batch Loss: 0.2958201766014099\n",
      "Epoch 958, Loss: 0.5497556775808334, Final Batch Loss: 0.244239941239357\n",
      "Epoch 959, Loss: 0.6354145109653473, Final Batch Loss: 0.3480466306209564\n",
      "Epoch 960, Loss: 0.624714583158493, Final Batch Loss: 0.3159120976924896\n",
      "Epoch 961, Loss: 0.5921923518180847, Final Batch Loss: 0.29249945282936096\n",
      "Epoch 962, Loss: 0.5768890976905823, Final Batch Loss: 0.2645493745803833\n",
      "Epoch 963, Loss: 0.571079432964325, Final Batch Loss: 0.28444239497184753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 964, Loss: 0.6576653122901917, Final Batch Loss: 0.362948477268219\n",
      "Epoch 965, Loss: 0.563542902469635, Final Batch Loss: 0.288621187210083\n",
      "Epoch 966, Loss: 0.6129373013973236, Final Batch Loss: 0.3150067925453186\n",
      "Epoch 967, Loss: 0.5884330868721008, Final Batch Loss: 0.28760766983032227\n",
      "Epoch 968, Loss: 0.5749130845069885, Final Batch Loss: 0.29351040720939636\n",
      "Epoch 969, Loss: 0.6462326049804688, Final Batch Loss: 0.33209776878356934\n",
      "Epoch 970, Loss: 0.5768168270587921, Final Batch Loss: 0.2729337215423584\n",
      "Epoch 971, Loss: 0.5985918939113617, Final Batch Loss: 0.2883758246898651\n",
      "Epoch 972, Loss: 0.5678388774394989, Final Batch Loss: 0.30003947019577026\n",
      "Epoch 973, Loss: 0.6074956357479095, Final Batch Loss: 0.3227660655975342\n",
      "Epoch 974, Loss: 0.646001398563385, Final Batch Loss: 0.3662005364894867\n",
      "Epoch 975, Loss: 0.6150206625461578, Final Batch Loss: 0.33811062574386597\n",
      "Epoch 976, Loss: 0.6551516950130463, Final Batch Loss: 0.34432676434516907\n",
      "Epoch 977, Loss: 0.5479016900062561, Final Batch Loss: 0.2678975760936737\n",
      "Epoch 978, Loss: 0.6228627264499664, Final Batch Loss: 0.3557480573654175\n",
      "Epoch 979, Loss: 0.6241761744022369, Final Batch Loss: 0.2832314670085907\n",
      "Epoch 980, Loss: 0.5865717232227325, Final Batch Loss: 0.31592509150505066\n",
      "Epoch 981, Loss: 0.6564130485057831, Final Batch Loss: 0.3289964497089386\n",
      "Epoch 982, Loss: 0.6548470258712769, Final Batch Loss: 0.34770891070365906\n",
      "Epoch 983, Loss: 0.5838001668453217, Final Batch Loss: 0.25856009125709534\n",
      "Epoch 984, Loss: 0.6018701791763306, Final Batch Loss: 0.3130571246147156\n",
      "Epoch 985, Loss: 0.6109622716903687, Final Batch Loss: 0.3449763357639313\n",
      "Epoch 986, Loss: 0.5877249240875244, Final Batch Loss: 0.2729269564151764\n",
      "Epoch 987, Loss: 0.5901569724082947, Final Batch Loss: 0.27464956045150757\n",
      "Epoch 988, Loss: 0.5597870051860809, Final Batch Loss: 0.28662824630737305\n",
      "Epoch 989, Loss: 0.5899820029735565, Final Batch Loss: 0.2721376419067383\n",
      "Epoch 990, Loss: 0.5888844132423401, Final Batch Loss: 0.2750581204891205\n",
      "Epoch 991, Loss: 0.5732603371143341, Final Batch Loss: 0.2560797333717346\n",
      "Epoch 992, Loss: 0.524896651506424, Final Batch Loss: 0.19505944848060608\n",
      "Epoch 993, Loss: 0.6062052249908447, Final Batch Loss: 0.3048763871192932\n",
      "Epoch 994, Loss: 0.5616403222084045, Final Batch Loss: 0.2616714835166931\n",
      "Epoch 995, Loss: 0.6139822006225586, Final Batch Loss: 0.30138665437698364\n",
      "Epoch 996, Loss: 0.5790314078330994, Final Batch Loss: 0.2737734615802765\n",
      "Epoch 997, Loss: 0.5971770286560059, Final Batch Loss: 0.3011839687824249\n",
      "Epoch 998, Loss: 0.5524046719074249, Final Batch Loss: 0.2533032298088074\n",
      "Epoch 999, Loss: 0.5770017206668854, Final Batch Loss: 0.2877484858036041\n",
      "Epoch 1000, Loss: 0.5660976469516754, Final Batch Loss: 0.3116362392902374\n",
      "Epoch 1001, Loss: 0.5685687959194183, Final Batch Loss: 0.29062873125076294\n",
      "Epoch 1002, Loss: 0.5458365976810455, Final Batch Loss: 0.28605228662490845\n",
      "Epoch 1003, Loss: 0.5991168916225433, Final Batch Loss: 0.2823622524738312\n",
      "Epoch 1004, Loss: 0.5797315537929535, Final Batch Loss: 0.2918544113636017\n",
      "Epoch 1005, Loss: 0.6051752269268036, Final Batch Loss: 0.337872177362442\n",
      "Epoch 1006, Loss: 0.6243649423122406, Final Batch Loss: 0.31706708669662476\n",
      "Epoch 1007, Loss: 0.5489901602268219, Final Batch Loss: 0.2391553819179535\n",
      "Epoch 1008, Loss: 0.6223046779632568, Final Batch Loss: 0.29318565130233765\n",
      "Epoch 1009, Loss: 0.5566619485616684, Final Batch Loss: 0.24458624422550201\n",
      "Epoch 1010, Loss: 0.6084311902523041, Final Batch Loss: 0.3015509247779846\n",
      "Epoch 1011, Loss: 0.5947598218917847, Final Batch Loss: 0.29642438888549805\n",
      "Epoch 1012, Loss: 0.6433990597724915, Final Batch Loss: 0.34604567289352417\n",
      "Epoch 1013, Loss: 0.5483265519142151, Final Batch Loss: 0.2532465159893036\n",
      "Epoch 1014, Loss: 0.5959679484367371, Final Batch Loss: 0.25762417912483215\n",
      "Epoch 1015, Loss: 0.5728619694709778, Final Batch Loss: 0.2930298447608948\n",
      "Epoch 1016, Loss: 0.604360044002533, Final Batch Loss: 0.32001572847366333\n",
      "Epoch 1017, Loss: 0.5693748295307159, Final Batch Loss: 0.2514585554599762\n",
      "Epoch 1018, Loss: 0.5765836834907532, Final Batch Loss: 0.26689594984054565\n",
      "Epoch 1019, Loss: 0.5667381584644318, Final Batch Loss: 0.2739664316177368\n",
      "Epoch 1020, Loss: 0.5596679002046585, Final Batch Loss: 0.24587245285511017\n",
      "Epoch 1021, Loss: 0.5456157922744751, Final Batch Loss: 0.2820815443992615\n",
      "Epoch 1022, Loss: 0.6083701252937317, Final Batch Loss: 0.3285202980041504\n",
      "Epoch 1023, Loss: 0.6306110322475433, Final Batch Loss: 0.3164415657520294\n",
      "Epoch 1024, Loss: 0.5568240880966187, Final Batch Loss: 0.2452835738658905\n",
      "Epoch 1025, Loss: 0.5526417195796967, Final Batch Loss: 0.26864802837371826\n",
      "Epoch 1026, Loss: 0.5645507872104645, Final Batch Loss: 0.27264946699142456\n",
      "Epoch 1027, Loss: 0.4926730841398239, Final Batch Loss: 0.21613721549510956\n",
      "Epoch 1028, Loss: 0.6369704306125641, Final Batch Loss: 0.3202032148838043\n",
      "Epoch 1029, Loss: 0.5890912413597107, Final Batch Loss: 0.3249613642692566\n",
      "Epoch 1030, Loss: 0.6045740842819214, Final Batch Loss: 0.2967700660228729\n",
      "Epoch 1031, Loss: 0.5568770468235016, Final Batch Loss: 0.3057717978954315\n",
      "Epoch 1032, Loss: 0.6411856412887573, Final Batch Loss: 0.33718234300613403\n",
      "Epoch 1033, Loss: 0.5704012215137482, Final Batch Loss: 0.3036493957042694\n",
      "Epoch 1034, Loss: 0.5145134627819061, Final Batch Loss: 0.25211605429649353\n",
      "Epoch 1035, Loss: 0.5583552122116089, Final Batch Loss: 0.2808306813240051\n",
      "Epoch 1036, Loss: 0.5744779706001282, Final Batch Loss: 0.2961735129356384\n",
      "Epoch 1037, Loss: 0.5941615104675293, Final Batch Loss: 0.32123202085494995\n",
      "Epoch 1038, Loss: 0.6738376021385193, Final Batch Loss: 0.31262895464897156\n",
      "Epoch 1039, Loss: 0.5351967513561249, Final Batch Loss: 0.26158586144447327\n",
      "Epoch 1040, Loss: 0.6332905292510986, Final Batch Loss: 0.30091989040374756\n",
      "Epoch 1041, Loss: 0.5737970769405365, Final Batch Loss: 0.26890185475349426\n",
      "Epoch 1042, Loss: 0.5942780375480652, Final Batch Loss: 0.2698606848716736\n",
      "Epoch 1043, Loss: 0.5890394747257233, Final Batch Loss: 0.30856338143348694\n",
      "Epoch 1044, Loss: 0.616669625043869, Final Batch Loss: 0.3534839153289795\n",
      "Epoch 1045, Loss: 0.5831657946109772, Final Batch Loss: 0.3080010712146759\n",
      "Epoch 1046, Loss: 0.6131263375282288, Final Batch Loss: 0.2972823679447174\n",
      "Epoch 1047, Loss: 0.5594992637634277, Final Batch Loss: 0.2937138080596924\n",
      "Epoch 1048, Loss: 0.5542663484811783, Final Batch Loss: 0.2456575483083725\n",
      "Epoch 1049, Loss: 0.548259973526001, Final Batch Loss: 0.259219229221344\n",
      "Epoch 1050, Loss: 0.5561794191598892, Final Batch Loss: 0.24410299956798553\n",
      "Epoch 1051, Loss: 0.5402652472257614, Final Batch Loss: 0.29068079590797424\n",
      "Epoch 1052, Loss: 0.5985866785049438, Final Batch Loss: 0.28709420561790466\n",
      "Epoch 1053, Loss: 0.5755223631858826, Final Batch Loss: 0.2958301901817322\n",
      "Epoch 1054, Loss: 0.5217174589633942, Final Batch Loss: 0.266987144947052\n",
      "Epoch 1055, Loss: 0.5740211009979248, Final Batch Loss: 0.2835372984409332\n",
      "Epoch 1056, Loss: 0.5622891336679459, Final Batch Loss: 0.3319181203842163\n",
      "Epoch 1057, Loss: 0.5800350904464722, Final Batch Loss: 0.2566882371902466\n",
      "Epoch 1058, Loss: 0.5497922003269196, Final Batch Loss: 0.29153507947921753\n",
      "Epoch 1059, Loss: 0.5883121192455292, Final Batch Loss: 0.3158322870731354\n",
      "Epoch 1060, Loss: 0.5526032745838165, Final Batch Loss: 0.29636916518211365\n",
      "Epoch 1061, Loss: 0.5055422633886337, Final Batch Loss: 0.2430097907781601\n",
      "Epoch 1062, Loss: 0.6040223240852356, Final Batch Loss: 0.3476240336894989\n",
      "Epoch 1063, Loss: 0.602749228477478, Final Batch Loss: 0.30593106150627136\n",
      "Epoch 1064, Loss: 0.5725277960300446, Final Batch Loss: 0.20963835716247559\n",
      "Epoch 1065, Loss: 0.5315703004598618, Final Batch Loss: 0.29118072986602783\n",
      "Epoch 1066, Loss: 0.5601260960102081, Final Batch Loss: 0.30722758173942566\n",
      "Epoch 1067, Loss: 0.5878287851810455, Final Batch Loss: 0.3256264626979828\n",
      "Epoch 1068, Loss: 0.5629896223545074, Final Batch Loss: 0.2886064946651459\n",
      "Epoch 1069, Loss: 0.5490540713071823, Final Batch Loss: 0.30206501483917236\n",
      "Epoch 1070, Loss: 0.6347537338733673, Final Batch Loss: 0.30977755784988403\n",
      "Epoch 1071, Loss: 0.5208818018436432, Final Batch Loss: 0.23486992716789246\n",
      "Epoch 1072, Loss: 0.5278139710426331, Final Batch Loss: 0.22917434573173523\n",
      "Epoch 1073, Loss: 0.588638424873352, Final Batch Loss: 0.3193398416042328\n",
      "Epoch 1074, Loss: 0.5605347156524658, Final Batch Loss: 0.29445475339889526\n",
      "Epoch 1075, Loss: 0.5781672298908234, Final Batch Loss: 0.2967332601547241\n",
      "Epoch 1076, Loss: 0.5340297222137451, Final Batch Loss: 0.24636223912239075\n",
      "Epoch 1077, Loss: 0.4918534904718399, Final Batch Loss: 0.20707546174526215\n",
      "Epoch 1078, Loss: 0.6350922882556915, Final Batch Loss: 0.3722217381000519\n",
      "Epoch 1079, Loss: 0.5937864780426025, Final Batch Loss: 0.3176984190940857\n",
      "Epoch 1080, Loss: 0.5747760534286499, Final Batch Loss: 0.30917930603027344\n",
      "Epoch 1081, Loss: 0.5334597527980804, Final Batch Loss: 0.26006409525871277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1082, Loss: 0.5955763161182404, Final Batch Loss: 0.29283633828163147\n",
      "Epoch 1083, Loss: 0.5313222706317902, Final Batch Loss: 0.21465083956718445\n",
      "Epoch 1084, Loss: 0.5273217111825943, Final Batch Loss: 0.2475840300321579\n",
      "Epoch 1085, Loss: 0.5753757655620575, Final Batch Loss: 0.3089960217475891\n",
      "Epoch 1086, Loss: 0.6011050045490265, Final Batch Loss: 0.33584123849868774\n",
      "Epoch 1087, Loss: 0.5262244641780853, Final Batch Loss: 0.24653398990631104\n",
      "Epoch 1088, Loss: 0.569614589214325, Final Batch Loss: 0.2700100839138031\n",
      "Epoch 1089, Loss: 0.5403076410293579, Final Batch Loss: 0.2743074595928192\n",
      "Epoch 1090, Loss: 0.5741243660449982, Final Batch Loss: 0.3107058107852936\n",
      "Epoch 1091, Loss: 0.607093095779419, Final Batch Loss: 0.3704446852207184\n",
      "Epoch 1092, Loss: 0.49645787477493286, Final Batch Loss: 0.20155462622642517\n",
      "Epoch 1093, Loss: 0.5717143714427948, Final Batch Loss: 0.2787785828113556\n",
      "Epoch 1094, Loss: 0.519912987947464, Final Batch Loss: 0.24572071433067322\n",
      "Epoch 1095, Loss: 0.5912136137485504, Final Batch Loss: 0.2917584776878357\n",
      "Epoch 1096, Loss: 0.5417903661727905, Final Batch Loss: 0.2402704954147339\n",
      "Epoch 1097, Loss: 0.6098645627498627, Final Batch Loss: 0.2503051459789276\n",
      "Epoch 1098, Loss: 0.570961207151413, Final Batch Loss: 0.32032155990600586\n",
      "Epoch 1099, Loss: 0.5309441834688187, Final Batch Loss: 0.29167088866233826\n",
      "Epoch 1100, Loss: 0.5700232982635498, Final Batch Loss: 0.31705427169799805\n",
      "Epoch 1101, Loss: 0.5053201615810394, Final Batch Loss: 0.225691556930542\n",
      "Epoch 1102, Loss: 0.5658917129039764, Final Batch Loss: 0.31334638595581055\n",
      "Epoch 1103, Loss: 0.5356011986732483, Final Batch Loss: 0.24176129698753357\n",
      "Epoch 1104, Loss: 0.5017025172710419, Final Batch Loss: 0.2204953134059906\n",
      "Epoch 1105, Loss: 0.5011616945266724, Final Batch Loss: 0.22203904390335083\n",
      "Epoch 1106, Loss: 0.5540102124214172, Final Batch Loss: 0.2121320366859436\n",
      "Epoch 1107, Loss: 0.5173618644475937, Final Batch Loss: 0.2278270572423935\n",
      "Epoch 1108, Loss: 0.5693272799253464, Final Batch Loss: 0.3438372313976288\n",
      "Epoch 1109, Loss: 0.5080882608890533, Final Batch Loss: 0.22298112511634827\n",
      "Epoch 1110, Loss: 0.4930681437253952, Final Batch Loss: 0.2416880577802658\n",
      "Epoch 1111, Loss: 0.5290414839982986, Final Batch Loss: 0.22948746383190155\n",
      "Epoch 1112, Loss: 0.48442769050598145, Final Batch Loss: 0.25139984488487244\n",
      "Epoch 1113, Loss: 0.5220517516136169, Final Batch Loss: 0.2684210538864136\n",
      "Epoch 1114, Loss: 0.49398931860923767, Final Batch Loss: 0.22100910544395447\n",
      "Epoch 1115, Loss: 0.543722927570343, Final Batch Loss: 0.2814582288265228\n",
      "Epoch 1116, Loss: 0.5215978622436523, Final Batch Loss: 0.252280592918396\n",
      "Epoch 1117, Loss: 0.5617251992225647, Final Batch Loss: 0.2664440870285034\n",
      "Epoch 1118, Loss: 0.5191623568534851, Final Batch Loss: 0.28174343705177307\n",
      "Epoch 1119, Loss: 0.5496338456869125, Final Batch Loss: 0.307754784822464\n",
      "Epoch 1120, Loss: 0.5528163909912109, Final Batch Loss: 0.2900749146938324\n",
      "Epoch 1121, Loss: 0.6196989119052887, Final Batch Loss: 0.3432132303714752\n",
      "Epoch 1122, Loss: 0.5258685648441315, Final Batch Loss: 0.254494309425354\n",
      "Epoch 1123, Loss: 0.46199940145015717, Final Batch Loss: 0.23161126673221588\n",
      "Epoch 1124, Loss: 0.48918361961841583, Final Batch Loss: 0.24235157668590546\n",
      "Epoch 1125, Loss: 0.589353084564209, Final Batch Loss: 0.3289731442928314\n",
      "Epoch 1126, Loss: 0.49592380225658417, Final Batch Loss: 0.24541319906711578\n",
      "Epoch 1127, Loss: 0.5010146796703339, Final Batch Loss: 0.24510255455970764\n",
      "Epoch 1128, Loss: 0.5520113557577133, Final Batch Loss: 0.32428085803985596\n",
      "Epoch 1129, Loss: 0.5203719437122345, Final Batch Loss: 0.26980552077293396\n",
      "Epoch 1130, Loss: 0.5740363597869873, Final Batch Loss: 0.30171796679496765\n",
      "Epoch 1131, Loss: 0.5262869298458099, Final Batch Loss: 0.29925867915153503\n",
      "Epoch 1132, Loss: 0.5306964814662933, Final Batch Loss: 0.25579291582107544\n",
      "Epoch 1133, Loss: 0.531311571598053, Final Batch Loss: 0.2593287229537964\n",
      "Epoch 1134, Loss: 0.4761497974395752, Final Batch Loss: 0.210499107837677\n",
      "Epoch 1135, Loss: 0.5704325139522552, Final Batch Loss: 0.31176409125328064\n",
      "Epoch 1136, Loss: 0.5365851372480392, Final Batch Loss: 0.23916254937648773\n",
      "Epoch 1137, Loss: 0.5033043920993805, Final Batch Loss: 0.23034530878067017\n",
      "Epoch 1138, Loss: 0.5225947797298431, Final Batch Loss: 0.27770015597343445\n",
      "Epoch 1139, Loss: 0.5300465822219849, Final Batch Loss: 0.3051503002643585\n",
      "Epoch 1140, Loss: 0.546748548746109, Final Batch Loss: 0.28341957926750183\n",
      "Epoch 1141, Loss: 0.5263493955135345, Final Batch Loss: 0.28439149260520935\n",
      "Epoch 1142, Loss: 0.5162865817546844, Final Batch Loss: 0.2535879909992218\n",
      "Epoch 1143, Loss: 0.49622897803783417, Final Batch Loss: 0.22896261513233185\n",
      "Epoch 1144, Loss: 0.5122817605733871, Final Batch Loss: 0.24576152861118317\n",
      "Epoch 1145, Loss: 0.5089279115200043, Final Batch Loss: 0.22983011603355408\n",
      "Epoch 1146, Loss: 0.536505788564682, Final Batch Loss: 0.2732875645160675\n",
      "Epoch 1147, Loss: 0.5518677532672882, Final Batch Loss: 0.26148104667663574\n",
      "Epoch 1148, Loss: 0.5179484188556671, Final Batch Loss: 0.2653304636478424\n",
      "Epoch 1149, Loss: 0.4365151822566986, Final Batch Loss: 0.17681053280830383\n",
      "Epoch 1150, Loss: 0.5279575288295746, Final Batch Loss: 0.27130427956581116\n",
      "Epoch 1151, Loss: 0.5198626220226288, Final Batch Loss: 0.25200778245925903\n",
      "Epoch 1152, Loss: 0.622408539056778, Final Batch Loss: 0.3115452229976654\n",
      "Epoch 1153, Loss: 0.49916838109493256, Final Batch Loss: 0.21671105921268463\n",
      "Epoch 1154, Loss: 0.514396995306015, Final Batch Loss: 0.287157267332077\n",
      "Epoch 1155, Loss: 0.5744516253471375, Final Batch Loss: 0.3135311007499695\n",
      "Epoch 1156, Loss: 0.5197755545377731, Final Batch Loss: 0.2806263864040375\n",
      "Epoch 1157, Loss: 0.5537613034248352, Final Batch Loss: 0.291307657957077\n",
      "Epoch 1158, Loss: 0.46357934176921844, Final Batch Loss: 0.1938624531030655\n",
      "Epoch 1159, Loss: 0.5160528868436813, Final Batch Loss: 0.23642154037952423\n",
      "Epoch 1160, Loss: 0.5145358741283417, Final Batch Loss: 0.25393205881118774\n",
      "Epoch 1161, Loss: 0.5235619097948074, Final Batch Loss: 0.27941375970840454\n",
      "Epoch 1162, Loss: 0.5251443684101105, Final Batch Loss: 0.2513748109340668\n",
      "Epoch 1163, Loss: 0.5402967035770416, Final Batch Loss: 0.2813771367073059\n",
      "Epoch 1164, Loss: 0.4723179340362549, Final Batch Loss: 0.2173614799976349\n",
      "Epoch 1165, Loss: 0.4289867430925369, Final Batch Loss: 0.18867577612400055\n",
      "Epoch 1166, Loss: 0.4737062305212021, Final Batch Loss: 0.20982442796230316\n",
      "Epoch 1167, Loss: 0.501184269785881, Final Batch Loss: 0.21990035474300385\n",
      "Epoch 1168, Loss: 0.4710538536310196, Final Batch Loss: 0.21041305363178253\n",
      "Epoch 1169, Loss: 0.5563036799430847, Final Batch Loss: 0.2710014283657074\n",
      "Epoch 1170, Loss: 0.5018312036991119, Final Batch Loss: 0.23686140775680542\n",
      "Epoch 1171, Loss: 0.5094152987003326, Final Batch Loss: 0.23096641898155212\n",
      "Epoch 1172, Loss: 0.5374592542648315, Final Batch Loss: 0.25488555431365967\n",
      "Epoch 1173, Loss: 0.5149642676115036, Final Batch Loss: 0.20632632076740265\n",
      "Epoch 1174, Loss: 0.5116007030010223, Final Batch Loss: 0.25674256682395935\n",
      "Epoch 1175, Loss: 0.49473705887794495, Final Batch Loss: 0.25919443368911743\n",
      "Epoch 1176, Loss: 0.5555470883846283, Final Batch Loss: 0.2905047833919525\n",
      "Epoch 1177, Loss: 0.4897189885377884, Final Batch Loss: 0.21136574447155\n",
      "Epoch 1178, Loss: 0.44253218173980713, Final Batch Loss: 0.17470723390579224\n",
      "Epoch 1179, Loss: 0.5195427536964417, Final Batch Loss: 0.26559022068977356\n",
      "Epoch 1180, Loss: 0.5578321516513824, Final Batch Loss: 0.27863258123397827\n",
      "Epoch 1181, Loss: 0.4651961177587509, Final Batch Loss: 0.2186869978904724\n",
      "Epoch 1182, Loss: 0.5120185911655426, Final Batch Loss: 0.22278448939323425\n",
      "Epoch 1183, Loss: 0.4859553426504135, Final Batch Loss: 0.2740676999092102\n",
      "Epoch 1184, Loss: 0.49828630685806274, Final Batch Loss: 0.23570877313613892\n",
      "Epoch 1185, Loss: 0.47695839405059814, Final Batch Loss: 0.24225832521915436\n",
      "Epoch 1186, Loss: 0.48500582575798035, Final Batch Loss: 0.23295438289642334\n",
      "Epoch 1187, Loss: 0.47908225655555725, Final Batch Loss: 0.24714940786361694\n",
      "Epoch 1188, Loss: 0.5309396386146545, Final Batch Loss: 0.26947593688964844\n",
      "Epoch 1189, Loss: 0.5349640846252441, Final Batch Loss: 0.2864834666252136\n",
      "Epoch 1190, Loss: 0.48918791115283966, Final Batch Loss: 0.23673535883426666\n",
      "Epoch 1191, Loss: 0.524827167391777, Final Batch Loss: 0.301829069852829\n",
      "Epoch 1192, Loss: 0.4392654597759247, Final Batch Loss: 0.19922855496406555\n",
      "Epoch 1193, Loss: 0.4693525582551956, Final Batch Loss: 0.27216559648513794\n",
      "Epoch 1194, Loss: 0.5255030989646912, Final Batch Loss: 0.26982393860816956\n",
      "Epoch 1195, Loss: 0.5026281177997589, Final Batch Loss: 0.24148178100585938\n",
      "Epoch 1196, Loss: 0.5189088881015778, Final Batch Loss: 0.3014334440231323\n",
      "Epoch 1197, Loss: 0.5220670253038406, Final Batch Loss: 0.29306474328041077\n",
      "Epoch 1198, Loss: 0.4561435580253601, Final Batch Loss: 0.2288772463798523\n",
      "Epoch 1199, Loss: 0.4532925486564636, Final Batch Loss: 0.2105875313282013\n",
      "Epoch 1200, Loss: 0.4830133467912674, Final Batch Loss: 0.24907845258712769\n",
      "Epoch 1201, Loss: 0.5131659358739853, Final Batch Loss: 0.2779809832572937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1202, Loss: 0.5036330670118332, Final Batch Loss: 0.2236863225698471\n",
      "Epoch 1203, Loss: 0.5057631582021713, Final Batch Loss: 0.24447600543498993\n",
      "Epoch 1204, Loss: 0.4727899879217148, Final Batch Loss: 0.21164347231388092\n",
      "Epoch 1205, Loss: 0.48248347640037537, Final Batch Loss: 0.23304492235183716\n",
      "Epoch 1206, Loss: 0.5176097601652145, Final Batch Loss: 0.24126867949962616\n",
      "Epoch 1207, Loss: 0.49063245952129364, Final Batch Loss: 0.2430250197649002\n",
      "Epoch 1208, Loss: 0.5138804316520691, Final Batch Loss: 0.32137832045555115\n",
      "Epoch 1209, Loss: 0.5032356530427933, Final Batch Loss: 0.28741681575775146\n",
      "Epoch 1210, Loss: 0.4366537630558014, Final Batch Loss: 0.20144054293632507\n",
      "Epoch 1211, Loss: 0.4813944101333618, Final Batch Loss: 0.23567087948322296\n",
      "Epoch 1212, Loss: 0.47637489438056946, Final Batch Loss: 0.23606686294078827\n",
      "Epoch 1213, Loss: 0.5510933846235275, Final Batch Loss: 0.33204638957977295\n",
      "Epoch 1214, Loss: 0.45483456552028656, Final Batch Loss: 0.17665649950504303\n",
      "Epoch 1215, Loss: 0.47416362166404724, Final Batch Loss: 0.24040009081363678\n",
      "Epoch 1216, Loss: 0.45898501574993134, Final Batch Loss: 0.24972712993621826\n",
      "Epoch 1217, Loss: 0.4921029806137085, Final Batch Loss: 0.26153478026390076\n",
      "Epoch 1218, Loss: 0.4904846251010895, Final Batch Loss: 0.2630959451198578\n",
      "Epoch 1219, Loss: 0.5363449156284332, Final Batch Loss: 0.2779655456542969\n",
      "Epoch 1220, Loss: 0.5240306109189987, Final Batch Loss: 0.23931674659252167\n",
      "Epoch 1221, Loss: 0.5160293281078339, Final Batch Loss: 0.2624472677707672\n",
      "Epoch 1222, Loss: 0.6127247512340546, Final Batch Loss: 0.3823578655719757\n",
      "Epoch 1223, Loss: 0.49469539523124695, Final Batch Loss: 0.2769818603992462\n",
      "Epoch 1224, Loss: 0.49397052824497223, Final Batch Loss: 0.24288524687290192\n",
      "Epoch 1225, Loss: 0.47751933336257935, Final Batch Loss: 0.2532165050506592\n",
      "Epoch 1226, Loss: 0.47998081147670746, Final Batch Loss: 0.24155892431735992\n",
      "Epoch 1227, Loss: 0.5359558612108231, Final Batch Loss: 0.29722529649734497\n",
      "Epoch 1228, Loss: 0.49007265269756317, Final Batch Loss: 0.23452509939670563\n",
      "Epoch 1229, Loss: 0.4832429736852646, Final Batch Loss: 0.20910592377185822\n",
      "Epoch 1230, Loss: 0.5300453752279282, Final Batch Loss: 0.31996360421180725\n",
      "Epoch 1231, Loss: 0.5071974098682404, Final Batch Loss: 0.23317798972129822\n",
      "Epoch 1232, Loss: 0.4750065505504608, Final Batch Loss: 0.2460727095603943\n",
      "Epoch 1233, Loss: 0.5158372223377228, Final Batch Loss: 0.26434990763664246\n",
      "Epoch 1234, Loss: 0.4737320840358734, Final Batch Loss: 0.23103854060173035\n",
      "Epoch 1235, Loss: 0.4944489598274231, Final Batch Loss: 0.215477854013443\n",
      "Epoch 1236, Loss: 0.5507837235927582, Final Batch Loss: 0.2871788740158081\n",
      "Epoch 1237, Loss: 0.5096132904291153, Final Batch Loss: 0.22165630757808685\n",
      "Epoch 1238, Loss: 0.5496348440647125, Final Batch Loss: 0.3128621280193329\n",
      "Epoch 1239, Loss: 0.5183024704456329, Final Batch Loss: 0.31577378511428833\n",
      "Epoch 1240, Loss: 0.49424877762794495, Final Batch Loss: 0.23357713222503662\n",
      "Epoch 1241, Loss: 0.4676561653614044, Final Batch Loss: 0.21046879887580872\n",
      "Epoch 1242, Loss: 0.5014151483774185, Final Batch Loss: 0.2941948473453522\n",
      "Epoch 1243, Loss: 0.48351652920246124, Final Batch Loss: 0.22927500307559967\n",
      "Epoch 1244, Loss: 0.4869573563337326, Final Batch Loss: 0.2493448257446289\n",
      "Epoch 1245, Loss: 0.5118865519762039, Final Batch Loss: 0.2806123197078705\n",
      "Epoch 1246, Loss: 0.4633276015520096, Final Batch Loss: 0.22030559182167053\n",
      "Epoch 1247, Loss: 0.4244091212749481, Final Batch Loss: 0.1955592781305313\n",
      "Epoch 1248, Loss: 0.4673776924610138, Final Batch Loss: 0.2812904417514801\n",
      "Epoch 1249, Loss: 0.4399821609258652, Final Batch Loss: 0.18483714759349823\n",
      "Epoch 1250, Loss: 0.46124257147312164, Final Batch Loss: 0.2485780268907547\n",
      "Epoch 1251, Loss: 0.4659668356180191, Final Batch Loss: 0.20927934348583221\n",
      "Epoch 1252, Loss: 0.47325946390628815, Final Batch Loss: 0.26602882146835327\n",
      "Epoch 1253, Loss: 0.45160360634326935, Final Batch Loss: 0.20947588980197906\n",
      "Epoch 1254, Loss: 0.48780712485313416, Final Batch Loss: 0.27926865220069885\n",
      "Epoch 1255, Loss: 0.5120173394680023, Final Batch Loss: 0.2779355049133301\n",
      "Epoch 1256, Loss: 0.513158991932869, Final Batch Loss: 0.30389395356178284\n",
      "Epoch 1257, Loss: 0.49639858305454254, Final Batch Loss: 0.2534240484237671\n",
      "Epoch 1258, Loss: 0.43206703662872314, Final Batch Loss: 0.23523789644241333\n",
      "Epoch 1259, Loss: 0.4967736601829529, Final Batch Loss: 0.2932380735874176\n",
      "Epoch 1260, Loss: 0.488838866353035, Final Batch Loss: 0.2274337261915207\n",
      "Epoch 1261, Loss: 0.4809318333864212, Final Batch Loss: 0.23167888820171356\n",
      "Epoch 1262, Loss: 0.48146311938762665, Final Batch Loss: 0.19073913991451263\n",
      "Epoch 1263, Loss: 0.4781315177679062, Final Batch Loss: 0.2671307325363159\n",
      "Epoch 1264, Loss: 0.47415444254875183, Final Batch Loss: 0.2639369070529938\n",
      "Epoch 1265, Loss: 0.4761706292629242, Final Batch Loss: 0.22941112518310547\n",
      "Epoch 1266, Loss: 0.4878508895635605, Final Batch Loss: 0.2540462613105774\n",
      "Epoch 1267, Loss: 0.6106139421463013, Final Batch Loss: 0.24574604630470276\n",
      "Epoch 1268, Loss: 0.46775953471660614, Final Batch Loss: 0.2310081273317337\n",
      "Epoch 1269, Loss: 0.47112880647182465, Final Batch Loss: 0.2575954794883728\n",
      "Epoch 1270, Loss: 0.47783251106739044, Final Batch Loss: 0.2774546146392822\n",
      "Epoch 1271, Loss: 0.44643597304821014, Final Batch Loss: 0.20763519406318665\n",
      "Epoch 1272, Loss: 0.47207027673721313, Final Batch Loss: 0.2388952076435089\n",
      "Epoch 1273, Loss: 0.5000004023313522, Final Batch Loss: 0.25480836629867554\n",
      "Epoch 1274, Loss: 0.47652968764305115, Final Batch Loss: 0.23302851617336273\n",
      "Epoch 1275, Loss: 0.42438672482967377, Final Batch Loss: 0.2201513648033142\n",
      "Epoch 1276, Loss: 0.5433074533939362, Final Batch Loss: 0.30137988924980164\n",
      "Epoch 1277, Loss: 0.5467592626810074, Final Batch Loss: 0.3079380691051483\n",
      "Epoch 1278, Loss: 0.48658038675785065, Final Batch Loss: 0.22010232508182526\n",
      "Epoch 1279, Loss: 0.4293563514947891, Final Batch Loss: 0.21253015100955963\n",
      "Epoch 1280, Loss: 0.5469646155834198, Final Batch Loss: 0.28811758756637573\n",
      "Epoch 1281, Loss: 0.4417297840118408, Final Batch Loss: 0.21704885363578796\n",
      "Epoch 1282, Loss: 0.5134540796279907, Final Batch Loss: 0.27588459849357605\n",
      "Epoch 1283, Loss: 0.512880951166153, Final Batch Loss: 0.24558675289154053\n",
      "Epoch 1284, Loss: 0.4689453989267349, Final Batch Loss: 0.2286405712366104\n",
      "Epoch 1285, Loss: 0.4768364876508713, Final Batch Loss: 0.25196030735969543\n",
      "Epoch 1286, Loss: 0.4318206161260605, Final Batch Loss: 0.2308565229177475\n",
      "Epoch 1287, Loss: 0.5090386122465134, Final Batch Loss: 0.3006819486618042\n",
      "Epoch 1288, Loss: 0.5105831623077393, Final Batch Loss: 0.28318849205970764\n",
      "Epoch 1289, Loss: 0.47458675503730774, Final Batch Loss: 0.23294594883918762\n",
      "Epoch 1290, Loss: 0.4599542170763016, Final Batch Loss: 0.21355123817920685\n",
      "Epoch 1291, Loss: 0.444127693772316, Final Batch Loss: 0.19119201600551605\n",
      "Epoch 1292, Loss: 0.4940732270479202, Final Batch Loss: 0.2415706366300583\n",
      "Epoch 1293, Loss: 0.45248985290527344, Final Batch Loss: 0.2397368848323822\n",
      "Epoch 1294, Loss: 0.4252535402774811, Final Batch Loss: 0.19997018575668335\n",
      "Epoch 1295, Loss: 0.44164353609085083, Final Batch Loss: 0.19528701901435852\n",
      "Epoch 1296, Loss: 0.530186116695404, Final Batch Loss: 0.2529189884662628\n",
      "Epoch 1297, Loss: 0.5441349446773529, Final Batch Loss: 0.31879884004592896\n",
      "Epoch 1298, Loss: 0.4887438863515854, Final Batch Loss: 0.2193480283021927\n",
      "Epoch 1299, Loss: 0.48468638956546783, Final Batch Loss: 0.2617337107658386\n",
      "Epoch 1300, Loss: 0.4688354432582855, Final Batch Loss: 0.19078460335731506\n",
      "Epoch 1301, Loss: 0.4170105904340744, Final Batch Loss: 0.2061157077550888\n",
      "Epoch 1302, Loss: 0.5292324870824814, Final Batch Loss: 0.29087090492248535\n",
      "Epoch 1303, Loss: 0.432685062289238, Final Batch Loss: 0.2024070918560028\n",
      "Epoch 1304, Loss: 0.4235697388648987, Final Batch Loss: 0.22149702906608582\n",
      "Epoch 1305, Loss: 0.4806980937719345, Final Batch Loss: 0.2727985680103302\n",
      "Epoch 1306, Loss: 0.44953393936157227, Final Batch Loss: 0.20256108045578003\n",
      "Epoch 1307, Loss: 0.43294908106327057, Final Batch Loss: 0.17705003917217255\n",
      "Epoch 1308, Loss: 0.44836463034152985, Final Batch Loss: 0.21212218701839447\n",
      "Epoch 1309, Loss: 0.43617402017116547, Final Batch Loss: 0.21443846821784973\n",
      "Epoch 1310, Loss: 0.5559605956077576, Final Batch Loss: 0.3060981333255768\n",
      "Epoch 1311, Loss: 0.40860210359096527, Final Batch Loss: 0.17954477667808533\n",
      "Epoch 1312, Loss: 0.4400731325149536, Final Batch Loss: 0.24674095213413239\n",
      "Epoch 1313, Loss: 0.501462385058403, Final Batch Loss: 0.2866111695766449\n",
      "Epoch 1314, Loss: 0.5525142997503281, Final Batch Loss: 0.3095560371875763\n",
      "Epoch 1315, Loss: 0.4430503100156784, Final Batch Loss: 0.2042223960161209\n",
      "Epoch 1316, Loss: 0.47611844539642334, Final Batch Loss: 0.2205960750579834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1317, Loss: 0.4803590774536133, Final Batch Loss: 0.24486012756824493\n",
      "Epoch 1318, Loss: 0.5881526172161102, Final Batch Loss: 0.33793002367019653\n",
      "Epoch 1319, Loss: 0.5471141487360001, Final Batch Loss: 0.3115690350532532\n",
      "Epoch 1320, Loss: 0.5069041699171066, Final Batch Loss: 0.23869235813617706\n",
      "Epoch 1321, Loss: 0.4562698006629944, Final Batch Loss: 0.23117713630199432\n",
      "Epoch 1322, Loss: 0.4718591570854187, Final Batch Loss: 0.2515173554420471\n",
      "Epoch 1323, Loss: 0.4556483328342438, Final Batch Loss: 0.23111197352409363\n",
      "Epoch 1324, Loss: 0.45689819753170013, Final Batch Loss: 0.21298453211784363\n",
      "Epoch 1325, Loss: 0.49932761490345, Final Batch Loss: 0.2325221747159958\n",
      "Epoch 1326, Loss: 0.44151847064495087, Final Batch Loss: 0.2130751758813858\n",
      "Epoch 1327, Loss: 0.47335657477378845, Final Batch Loss: 0.1852284073829651\n",
      "Epoch 1328, Loss: 0.43302588164806366, Final Batch Loss: 0.1844225823879242\n",
      "Epoch 1329, Loss: 0.5164273232221603, Final Batch Loss: 0.2676512897014618\n",
      "Epoch 1330, Loss: 0.4767475575208664, Final Batch Loss: 0.28406932950019836\n",
      "Epoch 1331, Loss: 0.44838424026966095, Final Batch Loss: 0.2669171392917633\n",
      "Epoch 1332, Loss: 0.41630907356739044, Final Batch Loss: 0.17207123339176178\n",
      "Epoch 1333, Loss: 0.49881018698215485, Final Batch Loss: 0.22734899818897247\n",
      "Epoch 1334, Loss: 0.4476338028907776, Final Batch Loss: 0.22208456695079803\n",
      "Epoch 1335, Loss: 0.43383340537548065, Final Batch Loss: 0.21142421662807465\n",
      "Epoch 1336, Loss: 0.46728572249412537, Final Batch Loss: 0.2332972288131714\n",
      "Epoch 1337, Loss: 0.4715272933244705, Final Batch Loss: 0.21900932490825653\n",
      "Epoch 1338, Loss: 0.46203136444091797, Final Batch Loss: 0.26541668176651\n",
      "Epoch 1339, Loss: 0.42295874655246735, Final Batch Loss: 0.21864472329616547\n",
      "Epoch 1340, Loss: 0.483583927154541, Final Batch Loss: 0.26761186122894287\n",
      "Epoch 1341, Loss: 0.46168848872184753, Final Batch Loss: 0.26134902238845825\n",
      "Epoch 1342, Loss: 0.48013150691986084, Final Batch Loss: 0.27077051997184753\n",
      "Epoch 1343, Loss: 0.38384418189525604, Final Batch Loss: 0.15178371965885162\n",
      "Epoch 1344, Loss: 0.4852773994207382, Final Batch Loss: 0.21414823830127716\n",
      "Epoch 1345, Loss: 0.48083552718162537, Final Batch Loss: 0.23761031031608582\n",
      "Epoch 1346, Loss: 0.43096742033958435, Final Batch Loss: 0.1976274847984314\n",
      "Epoch 1347, Loss: 0.45841488242149353, Final Batch Loss: 0.20333516597747803\n",
      "Epoch 1348, Loss: 0.44027088582515717, Final Batch Loss: 0.21892978250980377\n",
      "Epoch 1349, Loss: 0.4358474612236023, Final Batch Loss: 0.2307390421628952\n",
      "Epoch 1350, Loss: 0.4583204537630081, Final Batch Loss: 0.23613591492176056\n",
      "Epoch 1351, Loss: 0.48173828423023224, Final Batch Loss: 0.23997339606285095\n",
      "Epoch 1352, Loss: 0.524617612361908, Final Batch Loss: 0.27701395750045776\n",
      "Epoch 1353, Loss: 0.42998582124710083, Final Batch Loss: 0.22277818620204926\n",
      "Epoch 1354, Loss: 0.4423362761735916, Final Batch Loss: 0.18505661189556122\n",
      "Epoch 1355, Loss: 0.45943063497543335, Final Batch Loss: 0.2522954046726227\n",
      "Epoch 1356, Loss: 0.44152992963790894, Final Batch Loss: 0.24013379216194153\n",
      "Epoch 1357, Loss: 0.4276537746191025, Final Batch Loss: 0.22783979773521423\n",
      "Epoch 1358, Loss: 0.5123421251773834, Final Batch Loss: 0.24898168444633484\n",
      "Epoch 1359, Loss: 0.4441801607608795, Final Batch Loss: 0.20205512642860413\n",
      "Epoch 1360, Loss: 0.44739238917827606, Final Batch Loss: 0.19023512303829193\n",
      "Epoch 1361, Loss: 0.44147783517837524, Final Batch Loss: 0.21939294040203094\n",
      "Epoch 1362, Loss: 0.44539256393909454, Final Batch Loss: 0.2356363981962204\n",
      "Epoch 1363, Loss: 0.4420786052942276, Final Batch Loss: 0.17468248307704926\n",
      "Epoch 1364, Loss: 0.4803072065114975, Final Batch Loss: 0.25049710273742676\n",
      "Epoch 1365, Loss: 0.4838067889213562, Final Batch Loss: 0.21361172199249268\n",
      "Epoch 1366, Loss: 0.4109325408935547, Final Batch Loss: 0.18159323930740356\n",
      "Epoch 1367, Loss: 0.3971172571182251, Final Batch Loss: 0.22217966616153717\n",
      "Epoch 1368, Loss: 0.4339979887008667, Final Batch Loss: 0.18320953845977783\n",
      "Epoch 1369, Loss: 0.4495103806257248, Final Batch Loss: 0.2355419099330902\n",
      "Epoch 1370, Loss: 0.43286021053791046, Final Batch Loss: 0.2012455314397812\n",
      "Epoch 1371, Loss: 0.448030561208725, Final Batch Loss: 0.191316157579422\n",
      "Epoch 1372, Loss: 0.4718579053878784, Final Batch Loss: 0.2179708182811737\n",
      "Epoch 1373, Loss: 0.4832148849964142, Final Batch Loss: 0.26639747619628906\n",
      "Epoch 1374, Loss: 0.44420026242733, Final Batch Loss: 0.2472442388534546\n",
      "Epoch 1375, Loss: 0.4582175463438034, Final Batch Loss: 0.20202822983264923\n",
      "Epoch 1376, Loss: 0.5238597244024277, Final Batch Loss: 0.21673880517482758\n",
      "Epoch 1377, Loss: 0.4751913696527481, Final Batch Loss: 0.2711883783340454\n",
      "Epoch 1378, Loss: 0.4303308129310608, Final Batch Loss: 0.21920093894004822\n",
      "Epoch 1379, Loss: 0.4114307314157486, Final Batch Loss: 0.20898331701755524\n",
      "Epoch 1380, Loss: 0.47922301292419434, Final Batch Loss: 0.19154518842697144\n",
      "Epoch 1381, Loss: 0.521789088845253, Final Batch Loss: 0.2759568989276886\n",
      "Epoch 1382, Loss: 0.4598318189382553, Final Batch Loss: 0.2230350524187088\n",
      "Epoch 1383, Loss: 0.4802653044462204, Final Batch Loss: 0.25738897919654846\n",
      "Epoch 1384, Loss: 0.4223008006811142, Final Batch Loss: 0.20590756833553314\n",
      "Epoch 1385, Loss: 0.46136702597141266, Final Batch Loss: 0.21315738558769226\n",
      "Epoch 1386, Loss: 0.4382695108652115, Final Batch Loss: 0.19360624253749847\n",
      "Epoch 1387, Loss: 0.4890855550765991, Final Batch Loss: 0.22285765409469604\n",
      "Epoch 1388, Loss: 0.4148944467306137, Final Batch Loss: 0.1596461981534958\n",
      "Epoch 1389, Loss: 0.4427168667316437, Final Batch Loss: 0.2533167600631714\n",
      "Epoch 1390, Loss: 0.5060945749282837, Final Batch Loss: 0.2774304151535034\n",
      "Epoch 1391, Loss: 0.4382352828979492, Final Batch Loss: 0.16040396690368652\n",
      "Epoch 1392, Loss: 0.4418889731168747, Final Batch Loss: 0.23042412102222443\n",
      "Epoch 1393, Loss: 0.46293579041957855, Final Batch Loss: 0.24136590957641602\n",
      "Epoch 1394, Loss: 0.4533866047859192, Final Batch Loss: 0.22845134139060974\n",
      "Epoch 1395, Loss: 0.46834245324134827, Final Batch Loss: 0.22891250252723694\n",
      "Epoch 1396, Loss: 0.38883034884929657, Final Batch Loss: 0.16482460498809814\n",
      "Epoch 1397, Loss: 0.5637158006429672, Final Batch Loss: 0.23043756186962128\n",
      "Epoch 1398, Loss: 0.4857989102602005, Final Batch Loss: 0.23998117446899414\n",
      "Epoch 1399, Loss: 0.44146260619163513, Final Batch Loss: 0.21170365810394287\n",
      "Epoch 1400, Loss: 0.4724583029747009, Final Batch Loss: 0.2394394874572754\n",
      "Epoch 1401, Loss: 0.43380777537822723, Final Batch Loss: 0.23992542922496796\n",
      "Epoch 1402, Loss: 0.39115384221076965, Final Batch Loss: 0.18031442165374756\n",
      "Epoch 1403, Loss: 0.42087796330451965, Final Batch Loss: 0.19077792763710022\n",
      "Epoch 1404, Loss: 0.3877784013748169, Final Batch Loss: 0.15324071049690247\n",
      "Epoch 1405, Loss: 0.43944941461086273, Final Batch Loss: 0.242300882935524\n",
      "Epoch 1406, Loss: 0.49523311853408813, Final Batch Loss: 0.27161380648612976\n",
      "Epoch 1407, Loss: 0.4808548539876938, Final Batch Loss: 0.2178928107023239\n",
      "Epoch 1408, Loss: 0.47505122423171997, Final Batch Loss: 0.24248094856739044\n",
      "Epoch 1409, Loss: 0.4313184320926666, Final Batch Loss: 0.16588065028190613\n",
      "Epoch 1410, Loss: 0.4105199873447418, Final Batch Loss: 0.18275205790996552\n",
      "Epoch 1411, Loss: 0.46612825989723206, Final Batch Loss: 0.2375701665878296\n",
      "Epoch 1412, Loss: 0.45099788904190063, Final Batch Loss: 0.26667433977127075\n",
      "Epoch 1413, Loss: 0.46185754239559174, Final Batch Loss: 0.2203436642885208\n",
      "Epoch 1414, Loss: 0.40858572721481323, Final Batch Loss: 0.15044772624969482\n",
      "Epoch 1415, Loss: 0.39105290174484253, Final Batch Loss: 0.18927644193172455\n",
      "Epoch 1416, Loss: 0.5126186013221741, Final Batch Loss: 0.24832552671432495\n",
      "Epoch 1417, Loss: 0.42914068698883057, Final Batch Loss: 0.2596282660961151\n",
      "Epoch 1418, Loss: 0.4619627296924591, Final Batch Loss: 0.2378978580236435\n",
      "Epoch 1419, Loss: 0.4907841384410858, Final Batch Loss: 0.2840895652770996\n",
      "Epoch 1420, Loss: 0.4361754357814789, Final Batch Loss: 0.20922943949699402\n",
      "Epoch 1421, Loss: 0.4957532435655594, Final Batch Loss: 0.2781844139099121\n",
      "Epoch 1422, Loss: 0.43237443268299103, Final Batch Loss: 0.20983490347862244\n",
      "Epoch 1423, Loss: 0.4779292941093445, Final Batch Loss: 0.2883670926094055\n",
      "Epoch 1424, Loss: 0.4456140697002411, Final Batch Loss: 0.2610582113265991\n",
      "Epoch 1425, Loss: 0.45859645307064056, Final Batch Loss: 0.2427079826593399\n",
      "Epoch 1426, Loss: 0.42341992259025574, Final Batch Loss: 0.19174480438232422\n",
      "Epoch 1427, Loss: 0.41551992297172546, Final Batch Loss: 0.20787101984024048\n",
      "Epoch 1428, Loss: 0.5069432705640793, Final Batch Loss: 0.22299961745738983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1429, Loss: 0.4378417730331421, Final Batch Loss: 0.21792465448379517\n",
      "Epoch 1430, Loss: 0.4307289272546768, Final Batch Loss: 0.18924371898174286\n",
      "Epoch 1431, Loss: 0.412350133061409, Final Batch Loss: 0.2000623494386673\n",
      "Epoch 1432, Loss: 0.45358526706695557, Final Batch Loss: 0.24678421020507812\n",
      "Epoch 1433, Loss: 0.4398535490036011, Final Batch Loss: 0.24326801300048828\n",
      "Epoch 1434, Loss: 0.5095548182725906, Final Batch Loss: 0.27174803614616394\n",
      "Epoch 1435, Loss: 0.4278007745742798, Final Batch Loss: 0.16992124915122986\n",
      "Epoch 1436, Loss: 0.4652377814054489, Final Batch Loss: 0.2635095715522766\n",
      "Epoch 1437, Loss: 0.4533974379301071, Final Batch Loss: 0.253273069858551\n",
      "Epoch 1438, Loss: 0.47836145758628845, Final Batch Loss: 0.2818261981010437\n",
      "Epoch 1439, Loss: 0.43674927949905396, Final Batch Loss: 0.23341356217861176\n",
      "Epoch 1440, Loss: 0.4512365162372589, Final Batch Loss: 0.25785571336746216\n",
      "Epoch 1441, Loss: 0.4565623998641968, Final Batch Loss: 0.2261732518672943\n",
      "Epoch 1442, Loss: 0.4673028290271759, Final Batch Loss: 0.2584391236305237\n",
      "Epoch 1443, Loss: 0.47712475061416626, Final Batch Loss: 0.29314863681793213\n",
      "Epoch 1444, Loss: 0.4339408278465271, Final Batch Loss: 0.2453637421131134\n",
      "Epoch 1445, Loss: 0.4481821209192276, Final Batch Loss: 0.21648262441158295\n",
      "Epoch 1446, Loss: 0.38898153603076935, Final Batch Loss: 0.17129631340503693\n",
      "Epoch 1447, Loss: 0.40334634482860565, Final Batch Loss: 0.1636190563440323\n",
      "Epoch 1448, Loss: 0.4111694395542145, Final Batch Loss: 0.20051035284996033\n",
      "Epoch 1449, Loss: 0.4550068974494934, Final Batch Loss: 0.23210559785366058\n",
      "Epoch 1450, Loss: 0.45324715971946716, Final Batch Loss: 0.22650174796581268\n",
      "Epoch 1451, Loss: 0.4265095442533493, Final Batch Loss: 0.2577078938484192\n",
      "Epoch 1452, Loss: 0.4391038715839386, Final Batch Loss: 0.1912023276090622\n",
      "Epoch 1453, Loss: 0.3826489895582199, Final Batch Loss: 0.1720537394285202\n",
      "Epoch 1454, Loss: 0.473490908741951, Final Batch Loss: 0.19805078208446503\n",
      "Epoch 1455, Loss: 0.5011176764965057, Final Batch Loss: 0.2650396525859833\n",
      "Epoch 1456, Loss: 0.42521679401397705, Final Batch Loss: 0.20888197422027588\n",
      "Epoch 1457, Loss: 0.3898313641548157, Final Batch Loss: 0.1814679354429245\n",
      "Epoch 1458, Loss: 0.40650783479213715, Final Batch Loss: 0.18783557415008545\n",
      "Epoch 1459, Loss: 0.4414995163679123, Final Batch Loss: 0.23468448221683502\n",
      "Epoch 1460, Loss: 0.4044077694416046, Final Batch Loss: 0.19231963157653809\n",
      "Epoch 1461, Loss: 0.4836236834526062, Final Batch Loss: 0.23913690447807312\n",
      "Epoch 1462, Loss: 0.39471159875392914, Final Batch Loss: 0.1743249148130417\n",
      "Epoch 1463, Loss: 0.4446285218000412, Final Batch Loss: 0.20964886248111725\n",
      "Epoch 1464, Loss: 0.3797318935394287, Final Batch Loss: 0.16785210371017456\n",
      "Epoch 1465, Loss: 0.47254884243011475, Final Batch Loss: 0.2422412931919098\n",
      "Epoch 1466, Loss: 0.41633112728595734, Final Batch Loss: 0.20579594373703003\n",
      "Epoch 1467, Loss: 0.4554515928030014, Final Batch Loss: 0.2183576375246048\n",
      "Epoch 1468, Loss: 0.4333549737930298, Final Batch Loss: 0.21814857423305511\n",
      "Epoch 1469, Loss: 0.4708341509103775, Final Batch Loss: 0.22570565342903137\n",
      "Epoch 1470, Loss: 0.4438374936580658, Final Batch Loss: 0.20909039676189423\n",
      "Epoch 1471, Loss: 0.43682466447353363, Final Batch Loss: 0.2469736933708191\n",
      "Epoch 1472, Loss: 0.44111907482147217, Final Batch Loss: 0.21551306545734406\n",
      "Epoch 1473, Loss: 0.44190867245197296, Final Batch Loss: 0.21455442905426025\n",
      "Epoch 1474, Loss: 0.418218269944191, Final Batch Loss: 0.2125484049320221\n",
      "Epoch 1475, Loss: 0.42633143067359924, Final Batch Loss: 0.21973969042301178\n",
      "Epoch 1476, Loss: 0.4565022438764572, Final Batch Loss: 0.20282600820064545\n",
      "Epoch 1477, Loss: 0.41534657776355743, Final Batch Loss: 0.18965508043766022\n",
      "Epoch 1478, Loss: 0.3895266354084015, Final Batch Loss: 0.17529310286045074\n",
      "Epoch 1479, Loss: 0.4065075069665909, Final Batch Loss: 0.1987808793783188\n",
      "Epoch 1480, Loss: 0.44387659430503845, Final Batch Loss: 0.22856347262859344\n",
      "Epoch 1481, Loss: 0.5488663017749786, Final Batch Loss: 0.33754247426986694\n",
      "Epoch 1482, Loss: 0.42935454845428467, Final Batch Loss: 0.1764470338821411\n",
      "Epoch 1483, Loss: 0.41263270378112793, Final Batch Loss: 0.19347286224365234\n",
      "Epoch 1484, Loss: 0.4329358637332916, Final Batch Loss: 0.2022707611322403\n",
      "Epoch 1485, Loss: 0.4655087739229202, Final Batch Loss: 0.22582542896270752\n",
      "Epoch 1486, Loss: 0.34563300758600235, Final Batch Loss: 0.12036990374326706\n",
      "Epoch 1487, Loss: 0.4572420120239258, Final Batch Loss: 0.21460562944412231\n",
      "Epoch 1488, Loss: 0.45760323107242584, Final Batch Loss: 0.18092657625675201\n",
      "Epoch 1489, Loss: 0.366379052400589, Final Batch Loss: 0.15812011063098907\n",
      "Epoch 1490, Loss: 0.38339242339134216, Final Batch Loss: 0.2268688678741455\n",
      "Epoch 1491, Loss: 0.44342583417892456, Final Batch Loss: 0.24946215748786926\n",
      "Epoch 1492, Loss: 0.43614646792411804, Final Batch Loss: 0.22704507410526276\n",
      "Epoch 1493, Loss: 0.3924986273050308, Final Batch Loss: 0.16622774302959442\n",
      "Epoch 1494, Loss: 0.40161360800266266, Final Batch Loss: 0.1937415897846222\n",
      "Epoch 1495, Loss: 0.3761804699897766, Final Batch Loss: 0.15396785736083984\n",
      "Epoch 1496, Loss: 0.43400780856609344, Final Batch Loss: 0.2285921275615692\n",
      "Epoch 1497, Loss: 0.4823736548423767, Final Batch Loss: 0.3058478832244873\n",
      "Epoch 1498, Loss: 0.42064931988716125, Final Batch Loss: 0.20194581151008606\n",
      "Epoch 1499, Loss: 0.3921205848455429, Final Batch Loss: 0.18078501522541046\n",
      "Epoch 1500, Loss: 0.39839006960392, Final Batch Loss: 0.20605036616325378\n",
      "Epoch 1501, Loss: 0.42688049376010895, Final Batch Loss: 0.20505526661872864\n",
      "Epoch 1502, Loss: 0.43572457134723663, Final Batch Loss: 0.2203143835067749\n",
      "Epoch 1503, Loss: 0.4559936821460724, Final Batch Loss: 0.2316422164440155\n",
      "Epoch 1504, Loss: 0.4235273003578186, Final Batch Loss: 0.22892695665359497\n",
      "Epoch 1505, Loss: 0.4248199015855789, Final Batch Loss: 0.19428551197052002\n",
      "Epoch 1506, Loss: 0.4146261215209961, Final Batch Loss: 0.21698157489299774\n",
      "Epoch 1507, Loss: 0.4252665340900421, Final Batch Loss: 0.1865023523569107\n",
      "Epoch 1508, Loss: 0.4102605730295181, Final Batch Loss: 0.18877264857292175\n",
      "Epoch 1509, Loss: 0.4710208475589752, Final Batch Loss: 0.2669582664966583\n",
      "Epoch 1510, Loss: 0.4614518731832504, Final Batch Loss: 0.23577649891376495\n",
      "Epoch 1511, Loss: 0.4335371106863022, Final Batch Loss: 0.23349516093730927\n",
      "Epoch 1512, Loss: 0.40566420555114746, Final Batch Loss: 0.20184464752674103\n",
      "Epoch 1513, Loss: 0.3653195649385452, Final Batch Loss: 0.15293221175670624\n",
      "Epoch 1514, Loss: 0.4159706085920334, Final Batch Loss: 0.2046532928943634\n",
      "Epoch 1515, Loss: 0.42141902446746826, Final Batch Loss: 0.21351350843906403\n",
      "Epoch 1516, Loss: 0.4336325377225876, Final Batch Loss: 0.22211603820323944\n",
      "Epoch 1517, Loss: 0.4506339132785797, Final Batch Loss: 0.24122175574302673\n",
      "Epoch 1518, Loss: 0.40569038689136505, Final Batch Loss: 0.15378092229366302\n",
      "Epoch 1519, Loss: 0.43577560782432556, Final Batch Loss: 0.23459993302822113\n",
      "Epoch 1520, Loss: 0.4563338905572891, Final Batch Loss: 0.2860182225704193\n",
      "Epoch 1521, Loss: 0.45088672637939453, Final Batch Loss: 0.28372472524642944\n",
      "Epoch 1522, Loss: 0.4014939218759537, Final Batch Loss: 0.1661394089460373\n",
      "Epoch 1523, Loss: 0.3962680697441101, Final Batch Loss: 0.18192999064922333\n",
      "Epoch 1524, Loss: 0.3800704926252365, Final Batch Loss: 0.18221968412399292\n",
      "Epoch 1525, Loss: 0.43286025524139404, Final Batch Loss: 0.26405563950538635\n",
      "Epoch 1526, Loss: 0.4268295019865036, Final Batch Loss: 0.18637113273143768\n",
      "Epoch 1527, Loss: 0.4031202793121338, Final Batch Loss: 0.2189868688583374\n",
      "Epoch 1528, Loss: 0.41940124332904816, Final Batch Loss: 0.2139555662870407\n",
      "Epoch 1529, Loss: 0.4018462598323822, Final Batch Loss: 0.2054743468761444\n",
      "Epoch 1530, Loss: 0.41325466334819794, Final Batch Loss: 0.186399444937706\n",
      "Epoch 1531, Loss: 0.4902456998825073, Final Batch Loss: 0.2715704143047333\n",
      "Epoch 1532, Loss: 0.4712204784154892, Final Batch Loss: 0.2799440622329712\n",
      "Epoch 1533, Loss: 0.4547160863876343, Final Batch Loss: 0.24213378131389618\n",
      "Epoch 1534, Loss: 0.412654310464859, Final Batch Loss: 0.16855749487876892\n",
      "Epoch 1535, Loss: 0.3752375543117523, Final Batch Loss: 0.1450532078742981\n",
      "Epoch 1536, Loss: 0.43766437470912933, Final Batch Loss: 0.22628076374530792\n",
      "Epoch 1537, Loss: 0.43349334597587585, Final Batch Loss: 0.2329169362783432\n",
      "Epoch 1538, Loss: 0.4287397712469101, Final Batch Loss: 0.2384636551141739\n",
      "Epoch 1539, Loss: 0.392883762717247, Final Batch Loss: 0.18132764101028442\n",
      "Epoch 1540, Loss: 0.42920002341270447, Final Batch Loss: 0.22898685932159424\n",
      "Epoch 1541, Loss: 0.42893558740615845, Final Batch Loss: 0.21075360476970673\n",
      "Epoch 1542, Loss: 0.43837058544158936, Final Batch Loss: 0.1980864405632019\n",
      "Epoch 1543, Loss: 0.43707482516765594, Final Batch Loss: 0.20025157928466797\n",
      "Epoch 1544, Loss: 0.41892698407173157, Final Batch Loss: 0.26746079325675964\n",
      "Epoch 1545, Loss: 0.3911997377872467, Final Batch Loss: 0.19102871417999268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1546, Loss: 0.3660410940647125, Final Batch Loss: 0.16026124358177185\n",
      "Epoch 1547, Loss: 0.41694582998752594, Final Batch Loss: 0.20974214375019073\n",
      "Epoch 1548, Loss: 0.43651778995990753, Final Batch Loss: 0.23380713164806366\n",
      "Epoch 1549, Loss: 0.38210031390190125, Final Batch Loss: 0.18789264559745789\n",
      "Epoch 1550, Loss: 0.42522139847278595, Final Batch Loss: 0.17893795669078827\n",
      "Epoch 1551, Loss: 0.343214675784111, Final Batch Loss: 0.1520630121231079\n",
      "Epoch 1552, Loss: 0.4116399884223938, Final Batch Loss: 0.1945217102766037\n",
      "Epoch 1553, Loss: 0.43445251882076263, Final Batch Loss: 0.2035227119922638\n",
      "Epoch 1554, Loss: 0.37302812933921814, Final Batch Loss: 0.16659553349018097\n",
      "Epoch 1555, Loss: 0.43343931436538696, Final Batch Loss: 0.23221983015537262\n",
      "Epoch 1556, Loss: 0.4031130075454712, Final Batch Loss: 0.22848136723041534\n",
      "Epoch 1557, Loss: 0.3515184074640274, Final Batch Loss: 0.1329258382320404\n",
      "Epoch 1558, Loss: 0.47800779342651367, Final Batch Loss: 0.2742162048816681\n",
      "Epoch 1559, Loss: 0.41578736901283264, Final Batch Loss: 0.24530917406082153\n",
      "Epoch 1560, Loss: 0.36919769644737244, Final Batch Loss: 0.1973181515932083\n",
      "Epoch 1561, Loss: 0.41822291910648346, Final Batch Loss: 0.16326354444026947\n",
      "Epoch 1562, Loss: 0.5070632994174957, Final Batch Loss: 0.2745828330516815\n",
      "Epoch 1563, Loss: 0.4010802358388901, Final Batch Loss: 0.1828562319278717\n",
      "Epoch 1564, Loss: 0.4700342267751694, Final Batch Loss: 0.205910786986351\n",
      "Epoch 1565, Loss: 0.505770206451416, Final Batch Loss: 0.27513590455055237\n",
      "Epoch 1566, Loss: 0.4083339273929596, Final Batch Loss: 0.19737455248832703\n",
      "Epoch 1567, Loss: 0.3958429992198944, Final Batch Loss: 0.19291554391384125\n",
      "Epoch 1568, Loss: 0.4235279858112335, Final Batch Loss: 0.21161969006061554\n",
      "Epoch 1569, Loss: 0.41313911974430084, Final Batch Loss: 0.19628942012786865\n",
      "Epoch 1570, Loss: 0.43573370575904846, Final Batch Loss: 0.252005010843277\n",
      "Epoch 1571, Loss: 0.3780800998210907, Final Batch Loss: 0.2063719481229782\n",
      "Epoch 1572, Loss: 0.3800358772277832, Final Batch Loss: 0.19155970215797424\n",
      "Epoch 1573, Loss: 0.3686040788888931, Final Batch Loss: 0.17062020301818848\n",
      "Epoch 1574, Loss: 0.4397412836551666, Final Batch Loss: 0.2128133326768875\n",
      "Epoch 1575, Loss: 0.38042522966861725, Final Batch Loss: 0.17261798679828644\n",
      "Epoch 1576, Loss: 0.4218568950891495, Final Batch Loss: 0.22548192739486694\n",
      "Epoch 1577, Loss: 0.3578554838895798, Final Batch Loss: 0.13452863693237305\n",
      "Epoch 1578, Loss: 0.44327691197395325, Final Batch Loss: 0.21098686754703522\n",
      "Epoch 1579, Loss: 0.4280046671628952, Final Batch Loss: 0.21861973404884338\n",
      "Epoch 1580, Loss: 0.4188394844532013, Final Batch Loss: 0.19157497584819794\n",
      "Epoch 1581, Loss: 0.45883309841156006, Final Batch Loss: 0.2448599487543106\n",
      "Epoch 1582, Loss: 0.4340359717607498, Final Batch Loss: 0.24639472365379333\n",
      "Epoch 1583, Loss: 0.4386059194803238, Final Batch Loss: 0.26210346817970276\n",
      "Epoch 1584, Loss: 0.4730779081583023, Final Batch Loss: 0.22349509596824646\n",
      "Epoch 1585, Loss: 0.4364507645368576, Final Batch Loss: 0.1783929020166397\n",
      "Epoch 1586, Loss: 0.43426433205604553, Final Batch Loss: 0.2232692539691925\n",
      "Epoch 1587, Loss: 0.40262436866760254, Final Batch Loss: 0.19777990877628326\n",
      "Epoch 1588, Loss: 0.4345798045396805, Final Batch Loss: 0.25485438108444214\n",
      "Epoch 1589, Loss: 0.405176505446434, Final Batch Loss: 0.17192240059375763\n",
      "Epoch 1590, Loss: 0.3601182699203491, Final Batch Loss: 0.17166438698768616\n",
      "Epoch 1591, Loss: 0.4029073566198349, Final Batch Loss: 0.16352157294750214\n",
      "Epoch 1592, Loss: 0.39934688806533813, Final Batch Loss: 0.2060045748949051\n",
      "Epoch 1593, Loss: 0.40289904177188873, Final Batch Loss: 0.18198585510253906\n",
      "Epoch 1594, Loss: 0.38267381489276886, Final Batch Loss: 0.17604917287826538\n",
      "Epoch 1595, Loss: 0.35827215015888214, Final Batch Loss: 0.19239221513271332\n",
      "Epoch 1596, Loss: 0.3924899995326996, Final Batch Loss: 0.21052710711956024\n",
      "Epoch 1597, Loss: 0.3494844138622284, Final Batch Loss: 0.17656050622463226\n",
      "Epoch 1598, Loss: 0.43792301416397095, Final Batch Loss: 0.2298244833946228\n",
      "Epoch 1599, Loss: 0.4630470424890518, Final Batch Loss: 0.2522156238555908\n",
      "Epoch 1600, Loss: 0.4005514085292816, Final Batch Loss: 0.20093779265880585\n",
      "Epoch 1601, Loss: 0.4753413051366806, Final Batch Loss: 0.23161038756370544\n",
      "Epoch 1602, Loss: 0.4264707714319229, Final Batch Loss: 0.2183946669101715\n",
      "Epoch 1603, Loss: 0.410088375210762, Final Batch Loss: 0.2057964950799942\n",
      "Epoch 1604, Loss: 0.4035976678133011, Final Batch Loss: 0.18124379217624664\n",
      "Epoch 1605, Loss: 0.39579160511493683, Final Batch Loss: 0.21874552965164185\n",
      "Epoch 1606, Loss: 0.33715589344501495, Final Batch Loss: 0.1719697266817093\n",
      "Epoch 1607, Loss: 0.4307316243648529, Final Batch Loss: 0.1943378895521164\n",
      "Epoch 1608, Loss: 0.43599873781204224, Final Batch Loss: 0.2207150161266327\n",
      "Epoch 1609, Loss: 0.42745035886764526, Final Batch Loss: 0.1922668069601059\n",
      "Epoch 1610, Loss: 0.35072584450244904, Final Batch Loss: 0.12540088593959808\n",
      "Epoch 1611, Loss: 0.38797156512737274, Final Batch Loss: 0.1886104941368103\n",
      "Epoch 1612, Loss: 0.3680980056524277, Final Batch Loss: 0.1892731785774231\n",
      "Epoch 1613, Loss: 0.4105895906686783, Final Batch Loss: 0.2168930470943451\n",
      "Epoch 1614, Loss: 0.4061315506696701, Final Batch Loss: 0.2152160406112671\n",
      "Epoch 1615, Loss: 0.3984052240848541, Final Batch Loss: 0.2088751196861267\n",
      "Epoch 1616, Loss: 0.41702409088611603, Final Batch Loss: 0.2244490087032318\n",
      "Epoch 1617, Loss: 0.49336476624011993, Final Batch Loss: 0.27434009313583374\n",
      "Epoch 1618, Loss: 0.4153820127248764, Final Batch Loss: 0.1989906281232834\n",
      "Epoch 1619, Loss: 0.408321738243103, Final Batch Loss: 0.2116679698228836\n",
      "Epoch 1620, Loss: 0.44931577146053314, Final Batch Loss: 0.24562950432300568\n",
      "Epoch 1621, Loss: 0.4030143767595291, Final Batch Loss: 0.18378250300884247\n",
      "Epoch 1622, Loss: 0.3773510605096817, Final Batch Loss: 0.17635071277618408\n",
      "Epoch 1623, Loss: 0.3943008780479431, Final Batch Loss: 0.17095841467380524\n",
      "Epoch 1624, Loss: 0.4135063588619232, Final Batch Loss: 0.18932481110095978\n",
      "Epoch 1625, Loss: 0.39657869935035706, Final Batch Loss: 0.19835905730724335\n",
      "Epoch 1626, Loss: 0.379023015499115, Final Batch Loss: 0.20234191417694092\n",
      "Epoch 1627, Loss: 0.44495467841625214, Final Batch Loss: 0.1886456161737442\n",
      "Epoch 1628, Loss: 0.41753925383090973, Final Batch Loss: 0.22046656906604767\n",
      "Epoch 1629, Loss: 0.3888872116804123, Final Batch Loss: 0.17412057518959045\n",
      "Epoch 1630, Loss: 0.34626056253910065, Final Batch Loss: 0.17967069149017334\n",
      "Epoch 1631, Loss: 0.3586430996656418, Final Batch Loss: 0.13575713336467743\n",
      "Epoch 1632, Loss: 0.4000726044178009, Final Batch Loss: 0.18465393781661987\n",
      "Epoch 1633, Loss: 0.4208372086286545, Final Batch Loss: 0.1925334483385086\n",
      "Epoch 1634, Loss: 0.42787861824035645, Final Batch Loss: 0.2654687762260437\n",
      "Epoch 1635, Loss: 0.4132477790117264, Final Batch Loss: 0.25641217827796936\n",
      "Epoch 1636, Loss: 0.39705584943294525, Final Batch Loss: 0.15057902038097382\n",
      "Epoch 1637, Loss: 0.4265447407960892, Final Batch Loss: 0.21608233451843262\n",
      "Epoch 1638, Loss: 0.4171564280986786, Final Batch Loss: 0.20370487868785858\n",
      "Epoch 1639, Loss: 0.39064280688762665, Final Batch Loss: 0.22510042786598206\n",
      "Epoch 1640, Loss: 0.39981862902641296, Final Batch Loss: 0.17894722521305084\n",
      "Epoch 1641, Loss: 0.41544096171855927, Final Batch Loss: 0.17517048120498657\n",
      "Epoch 1642, Loss: 0.4677911847829819, Final Batch Loss: 0.23711369931697845\n",
      "Epoch 1643, Loss: 0.429649218916893, Final Batch Loss: 0.23146823048591614\n",
      "Epoch 1644, Loss: 0.3988155722618103, Final Batch Loss: 0.16750074923038483\n",
      "Epoch 1645, Loss: 0.43893948197364807, Final Batch Loss: 0.21118009090423584\n",
      "Epoch 1646, Loss: 0.3727816641330719, Final Batch Loss: 0.16198937594890594\n",
      "Epoch 1647, Loss: 0.4071761220693588, Final Batch Loss: 0.21413350105285645\n",
      "Epoch 1648, Loss: 0.3521711975336075, Final Batch Loss: 0.16014932096004486\n",
      "Epoch 1649, Loss: 0.398690402507782, Final Batch Loss: 0.19746223092079163\n",
      "Epoch 1650, Loss: 0.4178968816995621, Final Batch Loss: 0.19722887873649597\n",
      "Epoch 1651, Loss: 0.44000864028930664, Final Batch Loss: 0.25473952293395996\n",
      "Epoch 1652, Loss: 0.3937782943248749, Final Batch Loss: 0.20357060432434082\n",
      "Epoch 1653, Loss: 0.4223593473434448, Final Batch Loss: 0.20732848346233368\n",
      "Epoch 1654, Loss: 0.42016276717185974, Final Batch Loss: 0.24644672870635986\n",
      "Epoch 1655, Loss: 0.48976774513721466, Final Batch Loss: 0.2516932189464569\n",
      "Epoch 1656, Loss: 0.4091515392065048, Final Batch Loss: 0.1744885891675949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1657, Loss: 0.379342183470726, Final Batch Loss: 0.18634797632694244\n",
      "Epoch 1658, Loss: 0.4163886308670044, Final Batch Loss: 0.20381219685077667\n",
      "Epoch 1659, Loss: 0.39608387649059296, Final Batch Loss: 0.206992045044899\n",
      "Epoch 1660, Loss: 0.45783035457134247, Final Batch Loss: 0.25552278757095337\n",
      "Epoch 1661, Loss: 0.37432409822940826, Final Batch Loss: 0.20231759548187256\n",
      "Epoch 1662, Loss: 0.41149212419986725, Final Batch Loss: 0.19499509036540985\n",
      "Epoch 1663, Loss: 0.38836395740509033, Final Batch Loss: 0.2274249643087387\n",
      "Epoch 1664, Loss: 0.42926572263240814, Final Batch Loss: 0.19978538155555725\n",
      "Epoch 1665, Loss: 0.4747316390275955, Final Batch Loss: 0.2879489064216614\n",
      "Epoch 1666, Loss: 0.4578157067298889, Final Batch Loss: 0.2833527624607086\n",
      "Epoch 1667, Loss: 0.3794577568769455, Final Batch Loss: 0.1785607933998108\n",
      "Epoch 1668, Loss: 0.39226219058036804, Final Batch Loss: 0.1977984607219696\n",
      "Epoch 1669, Loss: 0.37103454768657684, Final Batch Loss: 0.17581920325756073\n",
      "Epoch 1670, Loss: 0.4023599773645401, Final Batch Loss: 0.1912846565246582\n",
      "Epoch 1671, Loss: 0.37914973497390747, Final Batch Loss: 0.16254790127277374\n",
      "Epoch 1672, Loss: 0.3847135603427887, Final Batch Loss: 0.19540685415267944\n",
      "Epoch 1673, Loss: 0.3881566822528839, Final Batch Loss: 0.18543019890785217\n",
      "Epoch 1674, Loss: 0.37737348675727844, Final Batch Loss: 0.19000931084156036\n",
      "Epoch 1675, Loss: 0.4253979027271271, Final Batch Loss: 0.23269139230251312\n",
      "Epoch 1676, Loss: 0.44493813812732697, Final Batch Loss: 0.22738079726696014\n",
      "Epoch 1677, Loss: 0.4373983144760132, Final Batch Loss: 0.2549780011177063\n",
      "Epoch 1678, Loss: 0.35799720883369446, Final Batch Loss: 0.1767992228269577\n",
      "Epoch 1679, Loss: 0.4207251965999603, Final Batch Loss: 0.18314707279205322\n",
      "Epoch 1680, Loss: 0.4087977260351181, Final Batch Loss: 0.1968533992767334\n",
      "Epoch 1681, Loss: 0.3845009356737137, Final Batch Loss: 0.18730568885803223\n",
      "Epoch 1682, Loss: 0.41056759655475616, Final Batch Loss: 0.22579535841941833\n",
      "Epoch 1683, Loss: 0.42073291540145874, Final Batch Loss: 0.21831992268562317\n",
      "Epoch 1684, Loss: 0.4439697265625, Final Batch Loss: 0.21237200498580933\n",
      "Epoch 1685, Loss: 0.4209277778863907, Final Batch Loss: 0.1776760220527649\n",
      "Epoch 1686, Loss: 0.42063507437705994, Final Batch Loss: 0.2047663927078247\n",
      "Epoch 1687, Loss: 0.41094259917736053, Final Batch Loss: 0.18831461668014526\n",
      "Epoch 1688, Loss: 0.3910083919763565, Final Batch Loss: 0.20671123266220093\n",
      "Epoch 1689, Loss: 0.33509835600852966, Final Batch Loss: 0.12993034720420837\n",
      "Epoch 1690, Loss: 0.38404811918735504, Final Batch Loss: 0.1851184070110321\n",
      "Epoch 1691, Loss: 0.43857981264591217, Final Batch Loss: 0.2104877233505249\n",
      "Epoch 1692, Loss: 0.41604380309581757, Final Batch Loss: 0.21227851510047913\n",
      "Epoch 1693, Loss: 0.3902684450149536, Final Batch Loss: 0.17961783707141876\n",
      "Epoch 1694, Loss: 0.41254258155822754, Final Batch Loss: 0.191367968916893\n",
      "Epoch 1695, Loss: 0.38025327026844025, Final Batch Loss: 0.2119053304195404\n",
      "Epoch 1696, Loss: 0.37265540659427643, Final Batch Loss: 0.15437546372413635\n",
      "Epoch 1697, Loss: 0.40215858817100525, Final Batch Loss: 0.21595443785190582\n",
      "Epoch 1698, Loss: 0.45416389405727386, Final Batch Loss: 0.2626449763774872\n",
      "Epoch 1699, Loss: 0.41250285506248474, Final Batch Loss: 0.21014970541000366\n",
      "Epoch 1700, Loss: 0.39503826200962067, Final Batch Loss: 0.21176527440547943\n",
      "Epoch 1701, Loss: 0.40622885525226593, Final Batch Loss: 0.19625066220760345\n",
      "Epoch 1702, Loss: 0.4267723262310028, Final Batch Loss: 0.24668821692466736\n",
      "Epoch 1703, Loss: 0.39586499333381653, Final Batch Loss: 0.2028108537197113\n",
      "Epoch 1704, Loss: 0.4296405762434006, Final Batch Loss: 0.22414153814315796\n",
      "Epoch 1705, Loss: 0.3930445909500122, Final Batch Loss: 0.2029111385345459\n",
      "Epoch 1706, Loss: 0.42885439097881317, Final Batch Loss: 0.2550477385520935\n",
      "Epoch 1707, Loss: 0.4171299934387207, Final Batch Loss: 0.20338591933250427\n",
      "Epoch 1708, Loss: 0.3860672414302826, Final Batch Loss: 0.18241626024246216\n",
      "Epoch 1709, Loss: 0.3622968941926956, Final Batch Loss: 0.15188202261924744\n",
      "Epoch 1710, Loss: 0.4082633852958679, Final Batch Loss: 0.21701698005199432\n",
      "Epoch 1711, Loss: 0.40624605119228363, Final Batch Loss: 0.20842938125133514\n",
      "Epoch 1712, Loss: 0.3899371027946472, Final Batch Loss: 0.13679054379463196\n",
      "Epoch 1713, Loss: 0.42211002111434937, Final Batch Loss: 0.23788627982139587\n",
      "Epoch 1714, Loss: 0.39675000309944153, Final Batch Loss: 0.19245317578315735\n",
      "Epoch 1715, Loss: 0.36788052320480347, Final Batch Loss: 0.21221038699150085\n",
      "Epoch 1716, Loss: 0.40839429199695587, Final Batch Loss: 0.22740495204925537\n",
      "Epoch 1717, Loss: 0.43836236000061035, Final Batch Loss: 0.22400608658790588\n",
      "Epoch 1718, Loss: 0.44432516396045685, Final Batch Loss: 0.2749733030796051\n",
      "Epoch 1719, Loss: 0.42601704597473145, Final Batch Loss: 0.21153512597084045\n",
      "Epoch 1720, Loss: 0.4076210707426071, Final Batch Loss: 0.2123243659734726\n",
      "Epoch 1721, Loss: 0.38925690948963165, Final Batch Loss: 0.207954540848732\n",
      "Epoch 1722, Loss: 0.4575207084417343, Final Batch Loss: 0.262919157743454\n",
      "Epoch 1723, Loss: 0.5344863533973694, Final Batch Loss: 0.295504629611969\n",
      "Epoch 1724, Loss: 0.40523561835289, Final Batch Loss: 0.19983406364917755\n",
      "Epoch 1725, Loss: 0.35654670000076294, Final Batch Loss: 0.19067586958408356\n",
      "Epoch 1726, Loss: 0.4172092229127884, Final Batch Loss: 0.19051265716552734\n",
      "Epoch 1727, Loss: 0.39221931993961334, Final Batch Loss: 0.1863294392824173\n",
      "Epoch 1728, Loss: 0.38413970172405243, Final Batch Loss: 0.21162353456020355\n",
      "Epoch 1729, Loss: 0.3995443433523178, Final Batch Loss: 0.19639404118061066\n",
      "Epoch 1730, Loss: 0.3461446911096573, Final Batch Loss: 0.19884167611598969\n",
      "Epoch 1731, Loss: 0.39763985574245453, Final Batch Loss: 0.18319952487945557\n",
      "Epoch 1732, Loss: 0.4049202799797058, Final Batch Loss: 0.19611690938472748\n",
      "Epoch 1733, Loss: 0.4426151067018509, Final Batch Loss: 0.23238025605678558\n",
      "Epoch 1734, Loss: 0.4111335128545761, Final Batch Loss: 0.21436071395874023\n",
      "Epoch 1735, Loss: 0.40185172855854034, Final Batch Loss: 0.19350799918174744\n",
      "Epoch 1736, Loss: 0.4483891874551773, Final Batch Loss: 0.25135311484336853\n",
      "Epoch 1737, Loss: 0.4124719798564911, Final Batch Loss: 0.21781596541404724\n",
      "Epoch 1738, Loss: 0.3740345239639282, Final Batch Loss: 0.17811813950538635\n",
      "Epoch 1739, Loss: 0.4033544212579727, Final Batch Loss: 0.19411344826221466\n",
      "Epoch 1740, Loss: 0.3820345550775528, Final Batch Loss: 0.17140579223632812\n",
      "Epoch 1741, Loss: 0.3779389411211014, Final Batch Loss: 0.1932229995727539\n",
      "Epoch 1742, Loss: 0.39667727053165436, Final Batch Loss: 0.2335282415151596\n",
      "Epoch 1743, Loss: 0.37489208579063416, Final Batch Loss: 0.1377520114183426\n",
      "Epoch 1744, Loss: 0.38805387914180756, Final Batch Loss: 0.16284437477588654\n",
      "Epoch 1745, Loss: 0.3677489310503006, Final Batch Loss: 0.2062758207321167\n",
      "Epoch 1746, Loss: 0.39213405549526215, Final Batch Loss: 0.18832804262638092\n",
      "Epoch 1747, Loss: 0.39340323209762573, Final Batch Loss: 0.21293289959430695\n",
      "Epoch 1748, Loss: 0.3667406141757965, Final Batch Loss: 0.16310137510299683\n",
      "Epoch 1749, Loss: 0.3810570389032364, Final Batch Loss: 0.1989573836326599\n",
      "Epoch 1750, Loss: 0.4609847217798233, Final Batch Loss: 0.240976944565773\n",
      "Epoch 1751, Loss: 0.40206632018089294, Final Batch Loss: 0.1986141800880432\n",
      "Epoch 1752, Loss: 0.4382401257753372, Final Batch Loss: 0.2155986875295639\n",
      "Epoch 1753, Loss: 0.4419904351234436, Final Batch Loss: 0.2335117906332016\n",
      "Epoch 1754, Loss: 0.36006778478622437, Final Batch Loss: 0.15698310732841492\n",
      "Epoch 1755, Loss: 0.38868145644664764, Final Batch Loss: 0.1990879327058792\n",
      "Epoch 1756, Loss: 0.42092761397361755, Final Batch Loss: 0.23200687766075134\n",
      "Epoch 1757, Loss: 0.4042065441608429, Final Batch Loss: 0.19672778248786926\n",
      "Epoch 1758, Loss: 0.39041027426719666, Final Batch Loss: 0.22239373624324799\n",
      "Epoch 1759, Loss: 0.4093187749385834, Final Batch Loss: 0.22629308700561523\n",
      "Epoch 1760, Loss: 0.347355917096138, Final Batch Loss: 0.16293023526668549\n",
      "Epoch 1761, Loss: 0.4253176748752594, Final Batch Loss: 0.23448656499385834\n",
      "Epoch 1762, Loss: 0.4128052741289139, Final Batch Loss: 0.17853379249572754\n",
      "Epoch 1763, Loss: 0.35516466200351715, Final Batch Loss: 0.15681667625904083\n",
      "Epoch 1764, Loss: 0.3227451592683792, Final Batch Loss: 0.14520572125911713\n",
      "Epoch 1765, Loss: 0.4180140197277069, Final Batch Loss: 0.21679532527923584\n",
      "Epoch 1766, Loss: 0.39112167060375214, Final Batch Loss: 0.14658178389072418\n",
      "Epoch 1767, Loss: 0.34617458283901215, Final Batch Loss: 0.16057749092578888\n",
      "Epoch 1768, Loss: 0.39274728298187256, Final Batch Loss: 0.22099880874156952\n",
      "Epoch 1769, Loss: 0.3533608466386795, Final Batch Loss: 0.1217288225889206\n",
      "Epoch 1770, Loss: 0.3926808387041092, Final Batch Loss: 0.22076450288295746\n",
      "Epoch 1771, Loss: 0.39757759869098663, Final Batch Loss: 0.20510007441043854\n",
      "Epoch 1772, Loss: 0.36804501712322235, Final Batch Loss: 0.18627814948558807\n",
      "Epoch 1773, Loss: 0.40457984805107117, Final Batch Loss: 0.25544336438179016\n",
      "Epoch 1774, Loss: 0.39666974544525146, Final Batch Loss: 0.20378641784191132\n",
      "Epoch 1775, Loss: 0.3804956078529358, Final Batch Loss: 0.1963137835264206\n",
      "Epoch 1776, Loss: 0.4522244930267334, Final Batch Loss: 0.19555184245109558\n",
      "Epoch 1777, Loss: 0.3718338757753372, Final Batch Loss: 0.1966341733932495\n",
      "Epoch 1778, Loss: 0.4172462075948715, Final Batch Loss: 0.22438186407089233\n",
      "Epoch 1779, Loss: 0.3737899661064148, Final Batch Loss: 0.19017638266086578\n",
      "Epoch 1780, Loss: 0.44237908720970154, Final Batch Loss: 0.24773845076560974\n",
      "Epoch 1781, Loss: 0.4057973176240921, Final Batch Loss: 0.2127297967672348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1782, Loss: 0.4156502038240433, Final Batch Loss: 0.20051714777946472\n",
      "Epoch 1783, Loss: 0.39239539206027985, Final Batch Loss: 0.20024265348911285\n",
      "Epoch 1784, Loss: 0.40003354847431183, Final Batch Loss: 0.1744750589132309\n",
      "Epoch 1785, Loss: 0.3804933428764343, Final Batch Loss: 0.16701816022396088\n",
      "Epoch 1786, Loss: 0.3534284234046936, Final Batch Loss: 0.164138063788414\n",
      "Epoch 1787, Loss: 0.4700161963701248, Final Batch Loss: 0.24203693866729736\n",
      "Epoch 1788, Loss: 0.4696824997663498, Final Batch Loss: 0.28200405836105347\n",
      "Epoch 1789, Loss: 0.3458070456981659, Final Batch Loss: 0.1645999401807785\n",
      "Epoch 1790, Loss: 0.36151446402072906, Final Batch Loss: 0.1814284473657608\n",
      "Epoch 1791, Loss: 0.4197349101305008, Final Batch Loss: 0.23073001205921173\n",
      "Epoch 1792, Loss: 0.39528313279151917, Final Batch Loss: 0.1946929693222046\n",
      "Epoch 1793, Loss: 0.4051108956336975, Final Batch Loss: 0.21479350328445435\n",
      "Epoch 1794, Loss: 0.37485797703266144, Final Batch Loss: 0.1831158995628357\n",
      "Epoch 1795, Loss: 0.508851945400238, Final Batch Loss: 0.2871829867362976\n",
      "Epoch 1796, Loss: 0.4436018168926239, Final Batch Loss: 0.196798175573349\n",
      "Epoch 1797, Loss: 0.4094354510307312, Final Batch Loss: 0.19685348868370056\n",
      "Epoch 1798, Loss: 0.41250768303871155, Final Batch Loss: 0.2032398134469986\n",
      "Epoch 1799, Loss: 0.5047614723443985, Final Batch Loss: 0.308490514755249\n",
      "Epoch 1800, Loss: 0.4058746099472046, Final Batch Loss: 0.22414985299110413\n",
      "Epoch 1801, Loss: 0.4045312702655792, Final Batch Loss: 0.15959033370018005\n",
      "Epoch 1802, Loss: 0.410533145070076, Final Batch Loss: 0.1887076050043106\n",
      "Epoch 1803, Loss: 0.45065937936306, Final Batch Loss: 0.2222282737493515\n",
      "Epoch 1804, Loss: 0.393057718873024, Final Batch Loss: 0.17688538134098053\n",
      "Epoch 1805, Loss: 0.3995656669139862, Final Batch Loss: 0.22335317730903625\n",
      "Epoch 1806, Loss: 0.37627387046813965, Final Batch Loss: 0.19147230684757233\n",
      "Epoch 1807, Loss: 0.42023782432079315, Final Batch Loss: 0.21503308415412903\n",
      "Epoch 1808, Loss: 0.3722948580980301, Final Batch Loss: 0.17412222921848297\n",
      "Epoch 1809, Loss: 0.4215654581785202, Final Batch Loss: 0.2006199061870575\n",
      "Epoch 1810, Loss: 0.3901492804288864, Final Batch Loss: 0.2187044769525528\n",
      "Epoch 1811, Loss: 0.4334679991006851, Final Batch Loss: 0.25795385241508484\n",
      "Epoch 1812, Loss: 0.3690745085477829, Final Batch Loss: 0.1794549822807312\n",
      "Epoch 1813, Loss: 0.3552638292312622, Final Batch Loss: 0.19381959736347198\n",
      "Epoch 1814, Loss: 0.37368933856487274, Final Batch Loss: 0.1812053918838501\n",
      "Epoch 1815, Loss: 0.3555501252412796, Final Batch Loss: 0.1814287006855011\n",
      "Epoch 1816, Loss: 0.3962371200323105, Final Batch Loss: 0.20568746328353882\n",
      "Epoch 1817, Loss: 0.4384554475545883, Final Batch Loss: 0.21388964354991913\n",
      "Epoch 1818, Loss: 0.4089672565460205, Final Batch Loss: 0.2134532481431961\n",
      "Epoch 1819, Loss: 0.41664785146713257, Final Batch Loss: 0.24591977894306183\n",
      "Epoch 1820, Loss: 0.3815809041261673, Final Batch Loss: 0.19340594112873077\n",
      "Epoch 1821, Loss: 0.40246863663196564, Final Batch Loss: 0.19346904754638672\n",
      "Epoch 1822, Loss: 0.4068840742111206, Final Batch Loss: 0.2461075782775879\n",
      "Epoch 1823, Loss: 0.388291135430336, Final Batch Loss: 0.2244740128517151\n",
      "Epoch 1824, Loss: 0.3533196747303009, Final Batch Loss: 0.18043558299541473\n",
      "Epoch 1825, Loss: 0.34837737679481506, Final Batch Loss: 0.16843311488628387\n",
      "Epoch 1826, Loss: 0.38059327006340027, Final Batch Loss: 0.22303174436092377\n",
      "Epoch 1827, Loss: 0.41351351141929626, Final Batch Loss: 0.22869831323623657\n",
      "Epoch 1828, Loss: 0.361061692237854, Final Batch Loss: 0.1654321849346161\n",
      "Epoch 1829, Loss: 0.35577264428138733, Final Batch Loss: 0.159981831908226\n",
      "Epoch 1830, Loss: 0.3826755881309509, Final Batch Loss: 0.2226482778787613\n",
      "Epoch 1831, Loss: 0.48933736979961395, Final Batch Loss: 0.23141111433506012\n",
      "Epoch 1832, Loss: 0.37153221666812897, Final Batch Loss: 0.18036802113056183\n",
      "Epoch 1833, Loss: 0.4083506017923355, Final Batch Loss: 0.21872171759605408\n",
      "Epoch 1834, Loss: 0.379208043217659, Final Batch Loss: 0.16297626495361328\n",
      "Epoch 1835, Loss: 0.42978210747241974, Final Batch Loss: 0.1918591558933258\n",
      "Epoch 1836, Loss: 0.388005793094635, Final Batch Loss: 0.21270816028118134\n",
      "Epoch 1837, Loss: 0.40859903395175934, Final Batch Loss: 0.18969304859638214\n",
      "Epoch 1838, Loss: 0.4244508892297745, Final Batch Loss: 0.21249936521053314\n",
      "Epoch 1839, Loss: 0.35032743215560913, Final Batch Loss: 0.13545441627502441\n",
      "Epoch 1840, Loss: 0.3788253217935562, Final Batch Loss: 0.16944019496440887\n",
      "Epoch 1841, Loss: 0.4110957235097885, Final Batch Loss: 0.22953768074512482\n",
      "Epoch 1842, Loss: 0.37078244984149933, Final Batch Loss: 0.20846320688724518\n",
      "Epoch 1843, Loss: 0.3732879012823105, Final Batch Loss: 0.17931681871414185\n",
      "Epoch 1844, Loss: 0.4044378846883774, Final Batch Loss: 0.22491399943828583\n",
      "Epoch 1845, Loss: 0.40769870579242706, Final Batch Loss: 0.20457611978054047\n",
      "Epoch 1846, Loss: 0.47507937252521515, Final Batch Loss: 0.20917977392673492\n",
      "Epoch 1847, Loss: 0.4014638215303421, Final Batch Loss: 0.19644685089588165\n",
      "Epoch 1848, Loss: 0.3652304708957672, Final Batch Loss: 0.16005410254001617\n",
      "Epoch 1849, Loss: 0.5475777834653854, Final Batch Loss: 0.3758361339569092\n",
      "Epoch 1850, Loss: 0.4483136981725693, Final Batch Loss: 0.24674494564533234\n",
      "Epoch 1851, Loss: 0.3256237655878067, Final Batch Loss: 0.16052250564098358\n",
      "Epoch 1852, Loss: 0.3528287261724472, Final Batch Loss: 0.18060564994812012\n",
      "Epoch 1853, Loss: 0.4452837258577347, Final Batch Loss: 0.20188991725444794\n",
      "Epoch 1854, Loss: 0.36017170548439026, Final Batch Loss: 0.1657971888780594\n",
      "Epoch 1855, Loss: 0.36693961918354034, Final Batch Loss: 0.1493385285139084\n",
      "Epoch 1856, Loss: 0.3828679621219635, Final Batch Loss: 0.17634780704975128\n",
      "Epoch 1857, Loss: 0.37542974948883057, Final Batch Loss: 0.1525646299123764\n",
      "Epoch 1858, Loss: 0.39588166773319244, Final Batch Loss: 0.19785159826278687\n",
      "Epoch 1859, Loss: 0.35169529914855957, Final Batch Loss: 0.15455347299575806\n",
      "Epoch 1860, Loss: 0.492905855178833, Final Batch Loss: 0.2823004424571991\n",
      "Epoch 1861, Loss: 0.4261798858642578, Final Batch Loss: 0.19349221885204315\n",
      "Epoch 1862, Loss: 0.42450790107250214, Final Batch Loss: 0.20837236940860748\n",
      "Epoch 1863, Loss: 0.3693064898252487, Final Batch Loss: 0.20103763043880463\n",
      "Epoch 1864, Loss: 0.3864792287349701, Final Batch Loss: 0.1903352439403534\n",
      "Epoch 1865, Loss: 0.35621193051338196, Final Batch Loss: 0.18351395428180695\n",
      "Epoch 1866, Loss: 0.43486468493938446, Final Batch Loss: 0.23277537524700165\n",
      "Epoch 1867, Loss: 0.40949632227420807, Final Batch Loss: 0.24821731448173523\n",
      "Epoch 1868, Loss: 0.3611770123243332, Final Batch Loss: 0.18721459805965424\n",
      "Epoch 1869, Loss: 0.38328325748443604, Final Batch Loss: 0.19840291142463684\n",
      "Epoch 1870, Loss: 0.4154416173696518, Final Batch Loss: 0.1723250448703766\n",
      "Epoch 1871, Loss: 0.3686757832765579, Final Batch Loss: 0.1584315150976181\n",
      "Epoch 1872, Loss: 0.40556105971336365, Final Batch Loss: 0.18363822996616364\n",
      "Epoch 1873, Loss: 0.359188511967659, Final Batch Loss: 0.1581507921218872\n",
      "Epoch 1874, Loss: 0.42801475524902344, Final Batch Loss: 0.18349015712738037\n",
      "Epoch 1875, Loss: 0.3884298950433731, Final Batch Loss: 0.17207303643226624\n",
      "Epoch 1876, Loss: 0.35963839292526245, Final Batch Loss: 0.16574296355247498\n",
      "Epoch 1877, Loss: 0.42876604199409485, Final Batch Loss: 0.23201878368854523\n",
      "Epoch 1878, Loss: 0.3647083342075348, Final Batch Loss: 0.16313548386096954\n",
      "Epoch 1879, Loss: 0.3848790377378464, Final Batch Loss: 0.1692373901605606\n",
      "Epoch 1880, Loss: 0.36075322329998016, Final Batch Loss: 0.14459937810897827\n",
      "Epoch 1881, Loss: 0.3921705484390259, Final Batch Loss: 0.1757575273513794\n",
      "Epoch 1882, Loss: 0.3947601467370987, Final Batch Loss: 0.19822147488594055\n",
      "Epoch 1883, Loss: 0.35241663455963135, Final Batch Loss: 0.1926896721124649\n",
      "Epoch 1884, Loss: 0.41849571466445923, Final Batch Loss: 0.2617712616920471\n",
      "Epoch 1885, Loss: 0.38023556768894196, Final Batch Loss: 0.15635886788368225\n",
      "Epoch 1886, Loss: 0.41045403480529785, Final Batch Loss: 0.2067287266254425\n",
      "Epoch 1887, Loss: 0.38847582042217255, Final Batch Loss: 0.20378418266773224\n",
      "Epoch 1888, Loss: 0.3927416205406189, Final Batch Loss: 0.25429782271385193\n",
      "Epoch 1889, Loss: 0.3909199833869934, Final Batch Loss: 0.20795245468616486\n",
      "Epoch 1890, Loss: 0.39229150116443634, Final Batch Loss: 0.22872021794319153\n",
      "Epoch 1891, Loss: 0.36687731742858887, Final Batch Loss: 0.19186733663082123\n",
      "Epoch 1892, Loss: 0.36938363313674927, Final Batch Loss: 0.16042068600654602\n",
      "Epoch 1893, Loss: 0.4026358872652054, Final Batch Loss: 0.2270849049091339\n",
      "Epoch 1894, Loss: 0.36525867879390717, Final Batch Loss: 0.18289954960346222\n",
      "Epoch 1895, Loss: 0.41288070380687714, Final Batch Loss: 0.23905062675476074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1896, Loss: 0.39475902915000916, Final Batch Loss: 0.2188711315393448\n",
      "Epoch 1897, Loss: 0.4208374470472336, Final Batch Loss: 0.24608135223388672\n",
      "Epoch 1898, Loss: 0.4171300679445267, Final Batch Loss: 0.231849804520607\n",
      "Epoch 1899, Loss: 0.3552597016096115, Final Batch Loss: 0.15303169190883636\n",
      "Epoch 1900, Loss: 0.3617332726716995, Final Batch Loss: 0.19979186356067657\n",
      "Epoch 1901, Loss: 0.37959863245487213, Final Batch Loss: 0.1837749481201172\n",
      "Epoch 1902, Loss: 0.33294877409935, Final Batch Loss: 0.15321114659309387\n",
      "Epoch 1903, Loss: 0.35197198390960693, Final Batch Loss: 0.14375315606594086\n",
      "Epoch 1904, Loss: 0.3922778069972992, Final Batch Loss: 0.2059064656496048\n",
      "Epoch 1905, Loss: 0.37390105426311493, Final Batch Loss: 0.1820574700832367\n",
      "Epoch 1906, Loss: 0.40585507452487946, Final Batch Loss: 0.19844909012317657\n",
      "Epoch 1907, Loss: 0.3341357707977295, Final Batch Loss: 0.1533413976430893\n",
      "Epoch 1908, Loss: 0.34956300258636475, Final Batch Loss: 0.15163055062294006\n",
      "Epoch 1909, Loss: 0.38561226427555084, Final Batch Loss: 0.16369257867336273\n",
      "Epoch 1910, Loss: 0.3523814529180527, Final Batch Loss: 0.16829834878444672\n",
      "Epoch 1911, Loss: 0.37926459312438965, Final Batch Loss: 0.18788757920265198\n",
      "Epoch 1912, Loss: 0.36242711544036865, Final Batch Loss: 0.1574174463748932\n",
      "Epoch 1913, Loss: 0.3764438033103943, Final Batch Loss: 0.18833620846271515\n",
      "Epoch 1914, Loss: 0.3826253265142441, Final Batch Loss: 0.21164005994796753\n",
      "Epoch 1915, Loss: 0.35657796263694763, Final Batch Loss: 0.19844205677509308\n",
      "Epoch 1916, Loss: 0.4344484806060791, Final Batch Loss: 0.26115360856056213\n",
      "Epoch 1917, Loss: 0.3970666825771332, Final Batch Loss: 0.2113475501537323\n",
      "Epoch 1918, Loss: 0.3908837139606476, Final Batch Loss: 0.185152068734169\n",
      "Epoch 1919, Loss: 0.39309997856616974, Final Batch Loss: 0.22720855474472046\n",
      "Epoch 1920, Loss: 0.3864963501691818, Final Batch Loss: 0.1686524599790573\n",
      "Epoch 1921, Loss: 0.3648476302623749, Final Batch Loss: 0.19190457463264465\n",
      "Epoch 1922, Loss: 0.34288132190704346, Final Batch Loss: 0.17756958305835724\n",
      "Epoch 1923, Loss: 0.34469449520111084, Final Batch Loss: 0.15676254034042358\n",
      "Epoch 1924, Loss: 0.3422956317663193, Final Batch Loss: 0.16209860146045685\n",
      "Epoch 1925, Loss: 0.39189571142196655, Final Batch Loss: 0.17545931041240692\n",
      "Epoch 1926, Loss: 0.4576348066329956, Final Batch Loss: 0.2766912579536438\n",
      "Epoch 1927, Loss: 0.3987840712070465, Final Batch Loss: 0.1769266426563263\n",
      "Epoch 1928, Loss: 0.39392584562301636, Final Batch Loss: 0.21676619350910187\n",
      "Epoch 1929, Loss: 0.35688459873199463, Final Batch Loss: 0.1801084280014038\n",
      "Epoch 1930, Loss: 0.4092409312725067, Final Batch Loss: 0.1678435206413269\n",
      "Epoch 1931, Loss: 0.38513147830963135, Final Batch Loss: 0.18888378143310547\n",
      "Epoch 1932, Loss: 0.41257332265377045, Final Batch Loss: 0.22310663759708405\n",
      "Epoch 1933, Loss: 0.35693956911563873, Final Batch Loss: 0.15640883147716522\n",
      "Epoch 1934, Loss: 0.3740234225988388, Final Batch Loss: 0.18929021060466766\n",
      "Epoch 1935, Loss: 0.3827999532222748, Final Batch Loss: 0.2264644056558609\n",
      "Epoch 1936, Loss: 0.40817035734653473, Final Batch Loss: 0.21328897774219513\n",
      "Epoch 1937, Loss: 0.4562629759311676, Final Batch Loss: 0.24162828922271729\n",
      "Epoch 1938, Loss: 0.3764594793319702, Final Batch Loss: 0.1697886735200882\n",
      "Epoch 1939, Loss: 0.3873389959335327, Final Batch Loss: 0.214478999376297\n",
      "Epoch 1940, Loss: 0.30499230325222015, Final Batch Loss: 0.12775936722755432\n",
      "Epoch 1941, Loss: 0.3369440883398056, Final Batch Loss: 0.15831081569194794\n",
      "Epoch 1942, Loss: 0.3954080790281296, Final Batch Loss: 0.20940032601356506\n",
      "Epoch 1943, Loss: 0.3413569778203964, Final Batch Loss: 0.16715441644191742\n",
      "Epoch 1944, Loss: 0.38458067178726196, Final Batch Loss: 0.1862737238407135\n",
      "Epoch 1945, Loss: 0.3982580304145813, Final Batch Loss: 0.21813230216503143\n",
      "Epoch 1946, Loss: 0.3889554291963577, Final Batch Loss: 0.1760784238576889\n",
      "Epoch 1947, Loss: 0.40657296776771545, Final Batch Loss: 0.2517644464969635\n",
      "Epoch 1948, Loss: 0.369772344827652, Final Batch Loss: 0.18086226284503937\n",
      "Epoch 1949, Loss: 0.37796401232481003, Final Batch Loss: 0.1241903230547905\n",
      "Epoch 1950, Loss: 0.3876335322856903, Final Batch Loss: 0.1880236566066742\n",
      "Epoch 1951, Loss: 0.35685910284519196, Final Batch Loss: 0.16671250760555267\n",
      "Epoch 1952, Loss: 0.34479621052742004, Final Batch Loss: 0.14791613817214966\n",
      "Epoch 1953, Loss: 0.39847178757190704, Final Batch Loss: 0.18346966803073883\n",
      "Epoch 1954, Loss: 0.3609945774078369, Final Batch Loss: 0.17315006256103516\n",
      "Epoch 1955, Loss: 0.3733574002981186, Final Batch Loss: 0.1774100810289383\n",
      "Epoch 1956, Loss: 0.37832941114902496, Final Batch Loss: 0.15450552105903625\n",
      "Epoch 1957, Loss: 0.4072204977273941, Final Batch Loss: 0.20433983206748962\n",
      "Epoch 1958, Loss: 0.3686482906341553, Final Batch Loss: 0.19727006554603577\n",
      "Epoch 1959, Loss: 0.3955349028110504, Final Batch Loss: 0.16997796297073364\n",
      "Epoch 1960, Loss: 0.3637971580028534, Final Batch Loss: 0.20766721665859222\n",
      "Epoch 1961, Loss: 0.3883001357316971, Final Batch Loss: 0.19690610468387604\n",
      "Epoch 1962, Loss: 0.3970586955547333, Final Batch Loss: 0.1929801106452942\n",
      "Epoch 1963, Loss: 0.3877428323030472, Final Batch Loss: 0.16638308763504028\n",
      "Epoch 1964, Loss: 0.4169406145811081, Final Batch Loss: 0.19112540781497955\n",
      "Epoch 1965, Loss: 0.3856358379125595, Final Batch Loss: 0.19159995019435883\n",
      "Epoch 1966, Loss: 0.4309517443180084, Final Batch Loss: 0.23111335933208466\n",
      "Epoch 1967, Loss: 0.44253475964069366, Final Batch Loss: 0.1616142839193344\n",
      "Epoch 1968, Loss: 0.4284197986125946, Final Batch Loss: 0.19192376732826233\n",
      "Epoch 1969, Loss: 0.41165126860141754, Final Batch Loss: 0.21626512706279755\n",
      "Epoch 1970, Loss: 0.3488852232694626, Final Batch Loss: 0.17538346350193024\n",
      "Epoch 1971, Loss: 0.36379288136959076, Final Batch Loss: 0.18477988243103027\n",
      "Epoch 1972, Loss: 0.35791829228401184, Final Batch Loss: 0.1780044287443161\n",
      "Epoch 1973, Loss: 0.3784994035959244, Final Batch Loss: 0.19890883564949036\n",
      "Epoch 1974, Loss: 0.3507048934698105, Final Batch Loss: 0.14538004994392395\n",
      "Epoch 1975, Loss: 0.35327406227588654, Final Batch Loss: 0.16812856495380402\n",
      "Epoch 1976, Loss: 0.39215798676013947, Final Batch Loss: 0.22618107497692108\n",
      "Epoch 1977, Loss: 0.38417187333106995, Final Batch Loss: 0.20757398009300232\n",
      "Epoch 1978, Loss: 0.41848428547382355, Final Batch Loss: 0.24596892297267914\n",
      "Epoch 1979, Loss: 0.38900038599967957, Final Batch Loss: 0.1628120243549347\n",
      "Epoch 1980, Loss: 0.38312968611717224, Final Batch Loss: 0.19339072704315186\n",
      "Epoch 1981, Loss: 0.4250102639198303, Final Batch Loss: 0.2248564511537552\n",
      "Epoch 1982, Loss: 0.3451457768678665, Final Batch Loss: 0.1543956995010376\n",
      "Epoch 1983, Loss: 0.35911522805690765, Final Batch Loss: 0.17138351500034332\n",
      "Epoch 1984, Loss: 0.40966740250587463, Final Batch Loss: 0.21983753144741058\n",
      "Epoch 1985, Loss: 0.3898570388555527, Final Batch Loss: 0.23779451847076416\n",
      "Epoch 1986, Loss: 0.38184474408626556, Final Batch Loss: 0.20511269569396973\n",
      "Epoch 1987, Loss: 0.420084148645401, Final Batch Loss: 0.25018760561943054\n",
      "Epoch 1988, Loss: 0.3706427216529846, Final Batch Loss: 0.1695103943347931\n",
      "Epoch 1989, Loss: 0.3636210262775421, Final Batch Loss: 0.1899677962064743\n",
      "Epoch 1990, Loss: 0.39701472222805023, Final Batch Loss: 0.17851583659648895\n",
      "Epoch 1991, Loss: 0.36735640466213226, Final Batch Loss: 0.17464101314544678\n",
      "Epoch 1992, Loss: 0.4163576364517212, Final Batch Loss: 0.24256330728530884\n",
      "Epoch 1993, Loss: 0.3939944952726364, Final Batch Loss: 0.18271751701831818\n",
      "Epoch 1994, Loss: 0.46246635913848877, Final Batch Loss: 0.2665857970714569\n",
      "Epoch 1995, Loss: 0.3969990909099579, Final Batch Loss: 0.1975947469472885\n",
      "Epoch 1996, Loss: 0.4205639511346817, Final Batch Loss: 0.23587661981582642\n",
      "Epoch 1997, Loss: 0.3621821254491806, Final Batch Loss: 0.16202127933502197\n",
      "Epoch 1998, Loss: 0.3613826334476471, Final Batch Loss: 0.1796998232603073\n",
      "Epoch 1999, Loss: 0.34974393248558044, Final Batch Loss: 0.16666395962238312\n",
      "Epoch 2000, Loss: 0.3673659712076187, Final Batch Loss: 0.21679101884365082\n",
      "Epoch 2001, Loss: 0.3819393962621689, Final Batch Loss: 0.16106368601322174\n",
      "Epoch 2002, Loss: 0.37857353687286377, Final Batch Loss: 0.1938144564628601\n",
      "Epoch 2003, Loss: 0.373548224568367, Final Batch Loss: 0.20718003809452057\n",
      "Epoch 2004, Loss: 0.3597675561904907, Final Batch Loss: 0.13943475484848022\n",
      "Epoch 2005, Loss: 0.393889918923378, Final Batch Loss: 0.2506283223628998\n",
      "Epoch 2006, Loss: 0.35806991159915924, Final Batch Loss: 0.18166570365428925\n",
      "Epoch 2007, Loss: 0.3806949555873871, Final Batch Loss: 0.13305488228797913\n",
      "Epoch 2008, Loss: 0.35375770926475525, Final Batch Loss: 0.17254795134067535\n",
      "Epoch 2009, Loss: 0.3589005619287491, Final Batch Loss: 0.21062642335891724\n",
      "Epoch 2010, Loss: 0.3854641318321228, Final Batch Loss: 0.20859237015247345\n",
      "Epoch 2011, Loss: 0.3654964119195938, Final Batch Loss: 0.2185760736465454\n",
      "Epoch 2012, Loss: 0.4184275418519974, Final Batch Loss: 0.23435385525226593\n",
      "Epoch 2013, Loss: 0.3518667370080948, Final Batch Loss: 0.1753842830657959\n",
      "Epoch 2014, Loss: 0.34909434616565704, Final Batch Loss: 0.1668217033147812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2015, Loss: 0.3828375041484833, Final Batch Loss: 0.22339282929897308\n",
      "Epoch 2016, Loss: 0.37890230119228363, Final Batch Loss: 0.20106814801692963\n",
      "Epoch 2017, Loss: 0.3811458498239517, Final Batch Loss: 0.21109886467456818\n",
      "Epoch 2018, Loss: 0.39866216480731964, Final Batch Loss: 0.20837390422821045\n",
      "Epoch 2019, Loss: 0.35925647616386414, Final Batch Loss: 0.18457408249378204\n",
      "Epoch 2020, Loss: 0.3591877222061157, Final Batch Loss: 0.195654958486557\n",
      "Epoch 2021, Loss: 0.3281182050704956, Final Batch Loss: 0.16353750228881836\n",
      "Epoch 2022, Loss: 0.3334057480096817, Final Batch Loss: 0.18633055686950684\n",
      "Epoch 2023, Loss: 0.3517027050256729, Final Batch Loss: 0.1624138206243515\n",
      "Epoch 2024, Loss: 0.3642050325870514, Final Batch Loss: 0.17255574464797974\n",
      "Epoch 2025, Loss: 0.3971520960330963, Final Batch Loss: 0.2031158059835434\n",
      "Epoch 2026, Loss: 0.36359551548957825, Final Batch Loss: 0.15791262686252594\n",
      "Epoch 2027, Loss: 0.36994755268096924, Final Batch Loss: 0.20536117255687714\n",
      "Epoch 2028, Loss: 0.36804480850696564, Final Batch Loss: 0.20014536380767822\n",
      "Epoch 2029, Loss: 0.36854907870292664, Final Batch Loss: 0.17677433788776398\n",
      "Epoch 2030, Loss: 0.373559907078743, Final Batch Loss: 0.22058546543121338\n",
      "Epoch 2031, Loss: 0.367990180850029, Final Batch Loss: 0.17685772478580475\n",
      "Epoch 2032, Loss: 0.42100299894809723, Final Batch Loss: 0.21259798109531403\n",
      "Epoch 2033, Loss: 0.3712565004825592, Final Batch Loss: 0.18980322778224945\n",
      "Epoch 2034, Loss: 0.37502457201480865, Final Batch Loss: 0.21418575942516327\n",
      "Epoch 2035, Loss: 0.3593900352716446, Final Batch Loss: 0.18221107125282288\n",
      "Epoch 2036, Loss: 0.3750631809234619, Final Batch Loss: 0.22430098056793213\n",
      "Epoch 2037, Loss: 0.3391842842102051, Final Batch Loss: 0.1663924902677536\n",
      "Epoch 2038, Loss: 0.3688115328550339, Final Batch Loss: 0.18338505923748016\n",
      "Epoch 2039, Loss: 0.4008255749940872, Final Batch Loss: 0.1920601725578308\n",
      "Epoch 2040, Loss: 0.3696191757917404, Final Batch Loss: 0.19212354719638824\n",
      "Epoch 2041, Loss: 0.35094670951366425, Final Batch Loss: 0.15963469445705414\n",
      "Epoch 2042, Loss: 0.3925291448831558, Final Batch Loss: 0.225236177444458\n",
      "Epoch 2043, Loss: 0.3707508444786072, Final Batch Loss: 0.184261754155159\n",
      "Epoch 2044, Loss: 0.34577564895153046, Final Batch Loss: 0.17758293449878693\n",
      "Epoch 2045, Loss: 0.36695845425128937, Final Batch Loss: 0.18867738544940948\n",
      "Epoch 2046, Loss: 0.32207438349723816, Final Batch Loss: 0.16900959610939026\n",
      "Epoch 2047, Loss: 0.36606959998607635, Final Batch Loss: 0.13671155273914337\n",
      "Epoch 2048, Loss: 0.4633137881755829, Final Batch Loss: 0.23198167979717255\n",
      "Epoch 2049, Loss: 0.3582194447517395, Final Batch Loss: 0.15800495445728302\n",
      "Epoch 2050, Loss: 0.3593584895133972, Final Batch Loss: 0.17923741042613983\n",
      "Epoch 2051, Loss: 0.34751781821250916, Final Batch Loss: 0.19008967280387878\n",
      "Epoch 2052, Loss: 0.3416772037744522, Final Batch Loss: 0.14875513315200806\n",
      "Epoch 2053, Loss: 0.3417692631483078, Final Batch Loss: 0.17126910388469696\n",
      "Epoch 2054, Loss: 0.3553842008113861, Final Batch Loss: 0.14001236855983734\n",
      "Epoch 2055, Loss: 0.3742901682853699, Final Batch Loss: 0.18065346777439117\n",
      "Epoch 2056, Loss: 0.3693094253540039, Final Batch Loss: 0.18258751928806305\n",
      "Epoch 2057, Loss: 0.37898804247379303, Final Batch Loss: 0.19063161313533783\n",
      "Epoch 2058, Loss: 0.33776310086250305, Final Batch Loss: 0.14260900020599365\n",
      "Epoch 2059, Loss: 0.35191449522972107, Final Batch Loss: 0.1486477553844452\n",
      "Epoch 2060, Loss: 0.34043703973293304, Final Batch Loss: 0.17111797630786896\n",
      "Epoch 2061, Loss: 0.32613255083560944, Final Batch Loss: 0.17841409146785736\n",
      "Epoch 2062, Loss: 0.3860023468732834, Final Batch Loss: 0.19216415286064148\n",
      "Epoch 2063, Loss: 0.36068369448184967, Final Batch Loss: 0.14418865740299225\n",
      "Epoch 2064, Loss: 0.35344311594963074, Final Batch Loss: 0.19047245383262634\n",
      "Epoch 2065, Loss: 0.37449294328689575, Final Batch Loss: 0.21053650975227356\n",
      "Epoch 2066, Loss: 0.306978315114975, Final Batch Loss: 0.13163380324840546\n",
      "Epoch 2067, Loss: 0.3743283599615097, Final Batch Loss: 0.19939717650413513\n",
      "Epoch 2068, Loss: 0.3876965492963791, Final Batch Loss: 0.19935806095600128\n",
      "Epoch 2069, Loss: 0.3439086526632309, Final Batch Loss: 0.16355933248996735\n",
      "Epoch 2070, Loss: 0.3722132742404938, Final Batch Loss: 0.18662911653518677\n",
      "Epoch 2071, Loss: 0.37993697822093964, Final Batch Loss: 0.20154070854187012\n",
      "Epoch 2072, Loss: 0.4291868209838867, Final Batch Loss: 0.2328789383172989\n",
      "Epoch 2073, Loss: 0.3252676725387573, Final Batch Loss: 0.18863791227340698\n",
      "Epoch 2074, Loss: 0.31973303854465485, Final Batch Loss: 0.13952548801898956\n",
      "Epoch 2075, Loss: 0.3350536972284317, Final Batch Loss: 0.16645587980747223\n",
      "Epoch 2076, Loss: 0.3494378328323364, Final Batch Loss: 0.18877939879894257\n",
      "Epoch 2077, Loss: 0.48232021927833557, Final Batch Loss: 0.2510635256767273\n",
      "Epoch 2078, Loss: 0.3919299691915512, Final Batch Loss: 0.21054384112358093\n",
      "Epoch 2079, Loss: 0.4218176007270813, Final Batch Loss: 0.22538526356220245\n",
      "Epoch 2080, Loss: 0.3382264971733093, Final Batch Loss: 0.15943250060081482\n",
      "Epoch 2081, Loss: 0.3903394043445587, Final Batch Loss: 0.21331915259361267\n",
      "Epoch 2082, Loss: 0.3685854524374008, Final Batch Loss: 0.22719188034534454\n",
      "Epoch 2083, Loss: 0.3527577519416809, Final Batch Loss: 0.2007783055305481\n",
      "Epoch 2084, Loss: 0.32639677822589874, Final Batch Loss: 0.16593357920646667\n",
      "Epoch 2085, Loss: 0.43190664052963257, Final Batch Loss: 0.215933695435524\n",
      "Epoch 2086, Loss: 0.38122832775115967, Final Batch Loss: 0.19986291229724884\n",
      "Epoch 2087, Loss: 0.4237738400697708, Final Batch Loss: 0.2424667477607727\n",
      "Epoch 2088, Loss: 0.3228745311498642, Final Batch Loss: 0.15102723240852356\n",
      "Epoch 2089, Loss: 0.3997235894203186, Final Batch Loss: 0.2121492177248001\n",
      "Epoch 2090, Loss: 0.36392372846603394, Final Batch Loss: 0.23151633143424988\n",
      "Epoch 2091, Loss: 0.34545472264289856, Final Batch Loss: 0.15839843451976776\n",
      "Epoch 2092, Loss: 0.36675944924354553, Final Batch Loss: 0.19082781672477722\n",
      "Epoch 2093, Loss: 0.3606507033109665, Final Batch Loss: 0.20347873866558075\n",
      "Epoch 2094, Loss: 0.41578909754753113, Final Batch Loss: 0.2241598665714264\n",
      "Epoch 2095, Loss: 0.3220936805009842, Final Batch Loss: 0.15794017910957336\n",
      "Epoch 2096, Loss: 0.3393055945634842, Final Batch Loss: 0.1607503890991211\n",
      "Epoch 2097, Loss: 0.35125158727169037, Final Batch Loss: 0.14809107780456543\n",
      "Epoch 2098, Loss: 0.36555832624435425, Final Batch Loss: 0.18282251060009003\n",
      "Epoch 2099, Loss: 0.4135979115962982, Final Batch Loss: 0.23067225515842438\n",
      "Epoch 2100, Loss: 0.37295962870121, Final Batch Loss: 0.22280053794384003\n",
      "Epoch 2101, Loss: 0.35523515939712524, Final Batch Loss: 0.18239296972751617\n",
      "Epoch 2102, Loss: 0.35099413990974426, Final Batch Loss: 0.1856682300567627\n",
      "Epoch 2103, Loss: 0.39467479288578033, Final Batch Loss: 0.19045200943946838\n",
      "Epoch 2104, Loss: 0.40769706666469574, Final Batch Loss: 0.2090163230895996\n",
      "Epoch 2105, Loss: 0.4228377938270569, Final Batch Loss: 0.27030783891677856\n",
      "Epoch 2106, Loss: 0.37766560912132263, Final Batch Loss: 0.2149350345134735\n",
      "Epoch 2107, Loss: 0.3643299639225006, Final Batch Loss: 0.19863027334213257\n",
      "Epoch 2108, Loss: 0.41134020686149597, Final Batch Loss: 0.2249981313943863\n",
      "Epoch 2109, Loss: 0.4047199785709381, Final Batch Loss: 0.22846949100494385\n",
      "Epoch 2110, Loss: 0.3519820421934128, Final Batch Loss: 0.18325254321098328\n",
      "Epoch 2111, Loss: 0.4406578540802002, Final Batch Loss: 0.256210058927536\n",
      "Epoch 2112, Loss: 0.3751887083053589, Final Batch Loss: 0.1758262813091278\n",
      "Epoch 2113, Loss: 0.359390988945961, Final Batch Loss: 0.16723203659057617\n",
      "Epoch 2114, Loss: 0.3653154671192169, Final Batch Loss: 0.17642360925674438\n",
      "Epoch 2115, Loss: 0.3595676124095917, Final Batch Loss: 0.17116831243038177\n",
      "Epoch 2116, Loss: 0.34723207354545593, Final Batch Loss: 0.21037299931049347\n",
      "Epoch 2117, Loss: 0.3035135269165039, Final Batch Loss: 0.12956365942955017\n",
      "Epoch 2118, Loss: 0.36639492213726044, Final Batch Loss: 0.16664062440395355\n",
      "Epoch 2119, Loss: 0.3527804762125015, Final Batch Loss: 0.15332596004009247\n",
      "Epoch 2120, Loss: 0.2791331931948662, Final Batch Loss: 0.12375000864267349\n",
      "Epoch 2121, Loss: 0.331782728433609, Final Batch Loss: 0.15167221426963806\n",
      "Epoch 2122, Loss: 0.48467445373535156, Final Batch Loss: 0.3231092393398285\n",
      "Epoch 2123, Loss: 0.41139887273311615, Final Batch Loss: 0.23478302359580994\n",
      "Epoch 2124, Loss: 0.3861902207136154, Final Batch Loss: 0.21285493671894073\n",
      "Epoch 2125, Loss: 0.3906518220901489, Final Batch Loss: 0.18675528466701508\n",
      "Epoch 2126, Loss: 0.34893904626369476, Final Batch Loss: 0.1948595941066742\n",
      "Epoch 2127, Loss: 0.3386068195104599, Final Batch Loss: 0.18236848711967468\n",
      "Epoch 2128, Loss: 0.39264191687107086, Final Batch Loss: 0.18321910500526428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2129, Loss: 0.3286880850791931, Final Batch Loss: 0.16213181614875793\n",
      "Epoch 2130, Loss: 0.37723371386528015, Final Batch Loss: 0.20525197684764862\n",
      "Epoch 2131, Loss: 0.36516259610652924, Final Batch Loss: 0.1991746723651886\n",
      "Epoch 2132, Loss: 0.39001311361789703, Final Batch Loss: 0.17297588288784027\n",
      "Epoch 2133, Loss: 0.33956940472126007, Final Batch Loss: 0.16798816621303558\n",
      "Epoch 2134, Loss: 0.35772040486335754, Final Batch Loss: 0.16779008507728577\n",
      "Epoch 2135, Loss: 0.35265833139419556, Final Batch Loss: 0.19163016974925995\n",
      "Epoch 2136, Loss: 0.3532397150993347, Final Batch Loss: 0.1696133017539978\n",
      "Epoch 2137, Loss: 0.3708485960960388, Final Batch Loss: 0.22426192462444305\n",
      "Epoch 2138, Loss: 0.41909365355968475, Final Batch Loss: 0.2296706736087799\n",
      "Epoch 2139, Loss: 0.33043980598449707, Final Batch Loss: 0.11850586533546448\n",
      "Epoch 2140, Loss: 0.3387155383825302, Final Batch Loss: 0.16026853024959564\n",
      "Epoch 2141, Loss: 0.33221809566020966, Final Batch Loss: 0.1577475666999817\n",
      "Epoch 2142, Loss: 0.3861055374145508, Final Batch Loss: 0.2266777753829956\n",
      "Epoch 2143, Loss: 0.43336963653564453, Final Batch Loss: 0.25383004546165466\n",
      "Epoch 2144, Loss: 0.3738992065191269, Final Batch Loss: 0.17896755039691925\n",
      "Epoch 2145, Loss: 0.3899937868118286, Final Batch Loss: 0.21817901730537415\n",
      "Epoch 2146, Loss: 0.3747529834508896, Final Batch Loss: 0.2065243273973465\n",
      "Epoch 2147, Loss: 0.32448433339595795, Final Batch Loss: 0.15480808913707733\n",
      "Epoch 2148, Loss: 0.3850472867488861, Final Batch Loss: 0.1857621967792511\n",
      "Epoch 2149, Loss: 0.39626625180244446, Final Batch Loss: 0.13899266719818115\n",
      "Epoch 2150, Loss: 0.3485783636569977, Final Batch Loss: 0.16645397245883942\n",
      "Epoch 2151, Loss: 0.3787366896867752, Final Batch Loss: 0.18762612342834473\n",
      "Epoch 2152, Loss: 0.37430526316165924, Final Batch Loss: 0.22215452790260315\n",
      "Epoch 2153, Loss: 0.389094740152359, Final Batch Loss: 0.21972887217998505\n",
      "Epoch 2154, Loss: 0.41952621936798096, Final Batch Loss: 0.20520800352096558\n",
      "Epoch 2155, Loss: 0.3618557006120682, Final Batch Loss: 0.16877572238445282\n",
      "Epoch 2156, Loss: 0.3895793855190277, Final Batch Loss: 0.18379385769367218\n",
      "Epoch 2157, Loss: 0.32544295489788055, Final Batch Loss: 0.15157034993171692\n",
      "Epoch 2158, Loss: 0.3733954280614853, Final Batch Loss: 0.18008263409137726\n",
      "Epoch 2159, Loss: 0.3353463113307953, Final Batch Loss: 0.1462390124797821\n",
      "Epoch 2160, Loss: 0.3748362809419632, Final Batch Loss: 0.18372568488121033\n",
      "Epoch 2161, Loss: 0.3842523843050003, Final Batch Loss: 0.1859135627746582\n",
      "Epoch 2162, Loss: 0.347191259264946, Final Batch Loss: 0.12838110327720642\n",
      "Epoch 2163, Loss: 0.36024613678455353, Final Batch Loss: 0.19605201482772827\n",
      "Epoch 2164, Loss: 0.34265561401844025, Final Batch Loss: 0.1849929392337799\n",
      "Epoch 2165, Loss: 0.35605600476264954, Final Batch Loss: 0.18126852810382843\n",
      "Epoch 2166, Loss: 0.3332480788230896, Final Batch Loss: 0.18190927803516388\n",
      "Epoch 2167, Loss: 0.3968244343996048, Final Batch Loss: 0.16731585562229156\n",
      "Epoch 2168, Loss: 0.37050436437129974, Final Batch Loss: 0.17739491164684296\n",
      "Epoch 2169, Loss: 0.3868080824613571, Final Batch Loss: 0.18640202283859253\n",
      "Epoch 2170, Loss: 0.33905237913131714, Final Batch Loss: 0.16884122788906097\n",
      "Epoch 2171, Loss: 0.3345580995082855, Final Batch Loss: 0.19348281621932983\n",
      "Epoch 2172, Loss: 0.3724164068698883, Final Batch Loss: 0.14248481392860413\n",
      "Epoch 2173, Loss: 0.325675830245018, Final Batch Loss: 0.14702750742435455\n",
      "Epoch 2174, Loss: 0.3708500564098358, Final Batch Loss: 0.18020035326480865\n",
      "Epoch 2175, Loss: 0.32406435906887054, Final Batch Loss: 0.1888493001461029\n",
      "Epoch 2176, Loss: 0.3516848087310791, Final Batch Loss: 0.17105454206466675\n",
      "Epoch 2177, Loss: 0.3662036955356598, Final Batch Loss: 0.16962680220603943\n",
      "Epoch 2178, Loss: 0.33314673602581024, Final Batch Loss: 0.14627274870872498\n",
      "Epoch 2179, Loss: 0.37990929186344147, Final Batch Loss: 0.18925313651561737\n",
      "Epoch 2180, Loss: 0.39771638810634613, Final Batch Loss: 0.20621077716350555\n",
      "Epoch 2181, Loss: 0.33628807961940765, Final Batch Loss: 0.15164439380168915\n",
      "Epoch 2182, Loss: 0.3483790010213852, Final Batch Loss: 0.15677368640899658\n",
      "Epoch 2183, Loss: 0.31596437096595764, Final Batch Loss: 0.1250375509262085\n",
      "Epoch 2184, Loss: 0.3590765744447708, Final Batch Loss: 0.17147380113601685\n",
      "Epoch 2185, Loss: 0.3526439517736435, Final Batch Loss: 0.14391307532787323\n",
      "Epoch 2186, Loss: 0.38813187181949615, Final Batch Loss: 0.19837193191051483\n",
      "Epoch 2187, Loss: 0.35936543345451355, Final Batch Loss: 0.19908250868320465\n",
      "Epoch 2188, Loss: 0.36685018241405487, Final Batch Loss: 0.16826891899108887\n",
      "Epoch 2189, Loss: 0.43631140887737274, Final Batch Loss: 0.22334234416484833\n",
      "Epoch 2190, Loss: 0.3565661162137985, Final Batch Loss: 0.20277263224124908\n",
      "Epoch 2191, Loss: 0.3103557974100113, Final Batch Loss: 0.1521671563386917\n",
      "Epoch 2192, Loss: 0.37041424214839935, Final Batch Loss: 0.1837431937456131\n",
      "Epoch 2193, Loss: 0.34025049209594727, Final Batch Loss: 0.16687767207622528\n",
      "Epoch 2194, Loss: 0.3846048414707184, Final Batch Loss: 0.19427505135536194\n",
      "Epoch 2195, Loss: 0.3217107057571411, Final Batch Loss: 0.16060908138751984\n",
      "Epoch 2196, Loss: 0.3830915689468384, Final Batch Loss: 0.20267988741397858\n",
      "Epoch 2197, Loss: 0.34999440610408783, Final Batch Loss: 0.16788332164287567\n",
      "Epoch 2198, Loss: 0.366771936416626, Final Batch Loss: 0.16588740050792694\n",
      "Epoch 2199, Loss: 0.35877472162246704, Final Batch Loss: 0.19827790558338165\n",
      "Epoch 2200, Loss: 0.3466707468032837, Final Batch Loss: 0.18646274507045746\n",
      "Epoch 2201, Loss: 0.3113955855369568, Final Batch Loss: 0.1686442643404007\n",
      "Epoch 2202, Loss: 0.34433022141456604, Final Batch Loss: 0.16093440353870392\n",
      "Epoch 2203, Loss: 0.3807138651609421, Final Batch Loss: 0.2356712967157364\n",
      "Epoch 2204, Loss: 0.4046272337436676, Final Batch Loss: 0.23708143830299377\n",
      "Epoch 2205, Loss: 0.33154940605163574, Final Batch Loss: 0.1657300591468811\n",
      "Epoch 2206, Loss: 0.34210821986198425, Final Batch Loss: 0.16064824163913727\n",
      "Epoch 2207, Loss: 0.3245486617088318, Final Batch Loss: 0.1724158078432083\n",
      "Epoch 2208, Loss: 0.3500294089317322, Final Batch Loss: 0.14320388436317444\n",
      "Epoch 2209, Loss: 0.34388257563114166, Final Batch Loss: 0.15555019676685333\n",
      "Epoch 2210, Loss: 0.3338918536901474, Final Batch Loss: 0.1490441858768463\n",
      "Epoch 2211, Loss: 0.3007906675338745, Final Batch Loss: 0.14516839385032654\n",
      "Epoch 2212, Loss: 0.37712128460407257, Final Batch Loss: 0.20120500028133392\n",
      "Epoch 2213, Loss: 0.3643455058336258, Final Batch Loss: 0.18842549622058868\n",
      "Epoch 2214, Loss: 0.4523477852344513, Final Batch Loss: 0.2245018035173416\n",
      "Epoch 2215, Loss: 0.3219209313392639, Final Batch Loss: 0.13974185287952423\n",
      "Epoch 2216, Loss: 0.38261936604976654, Final Batch Loss: 0.2228548526763916\n",
      "Epoch 2217, Loss: 0.35388922691345215, Final Batch Loss: 0.1511501520872116\n",
      "Epoch 2218, Loss: 0.35421738028526306, Final Batch Loss: 0.1872827112674713\n",
      "Epoch 2219, Loss: 0.3628930598497391, Final Batch Loss: 0.1803070306777954\n",
      "Epoch 2220, Loss: 0.332511767745018, Final Batch Loss: 0.17804957926273346\n",
      "Epoch 2221, Loss: 0.4396604001522064, Final Batch Loss: 0.2239305078983307\n",
      "Epoch 2222, Loss: 0.3779880851507187, Final Batch Loss: 0.17640100419521332\n",
      "Epoch 2223, Loss: 0.33993421494960785, Final Batch Loss: 0.1691916137933731\n",
      "Epoch 2224, Loss: 0.3268328756093979, Final Batch Loss: 0.11029614508152008\n",
      "Epoch 2225, Loss: 0.37808749079704285, Final Batch Loss: 0.22388510406017303\n",
      "Epoch 2226, Loss: 0.3339944928884506, Final Batch Loss: 0.13345736265182495\n",
      "Epoch 2227, Loss: 0.33969953656196594, Final Batch Loss: 0.13606658577919006\n",
      "Epoch 2228, Loss: 0.384159117937088, Final Batch Loss: 0.20750245451927185\n",
      "Epoch 2229, Loss: 0.3446638137102127, Final Batch Loss: 0.19537405669689178\n",
      "Epoch 2230, Loss: 0.37056055665016174, Final Batch Loss: 0.18158744275569916\n",
      "Epoch 2231, Loss: 0.320997029542923, Final Batch Loss: 0.12581750750541687\n",
      "Epoch 2232, Loss: 0.36793622374534607, Final Batch Loss: 0.2244369387626648\n",
      "Epoch 2233, Loss: 0.3482608497142792, Final Batch Loss: 0.2036123275756836\n",
      "Epoch 2234, Loss: 0.30938920378685, Final Batch Loss: 0.1608371138572693\n",
      "Epoch 2235, Loss: 0.34090515971183777, Final Batch Loss: 0.1367211937904358\n",
      "Epoch 2236, Loss: 0.40295231342315674, Final Batch Loss: 0.2072819322347641\n",
      "Epoch 2237, Loss: 0.33868037164211273, Final Batch Loss: 0.15422384440898895\n",
      "Epoch 2238, Loss: 0.33059580624103546, Final Batch Loss: 0.14744281768798828\n",
      "Epoch 2239, Loss: 0.3518126904964447, Final Batch Loss: 0.16408784687519073\n",
      "Epoch 2240, Loss: 0.3618377596139908, Final Batch Loss: 0.22751399874687195\n",
      "Epoch 2241, Loss: 0.38636237382888794, Final Batch Loss: 0.1508341133594513\n",
      "Epoch 2242, Loss: 0.35639964044094086, Final Batch Loss: 0.18050146102905273\n",
      "Epoch 2243, Loss: 0.3774895668029785, Final Batch Loss: 0.24545186758041382\n",
      "Epoch 2244, Loss: 0.3530438095331192, Final Batch Loss: 0.17738023400306702\n",
      "Epoch 2245, Loss: 0.3556308299303055, Final Batch Loss: 0.20978465676307678\n",
      "Epoch 2246, Loss: 0.39354631304740906, Final Batch Loss: 0.2437400072813034\n",
      "Epoch 2247, Loss: 0.314952552318573, Final Batch Loss: 0.15196342766284943\n",
      "Epoch 2248, Loss: 0.3704013228416443, Final Batch Loss: 0.19121204316616058\n",
      "Epoch 2249, Loss: 0.37811756134033203, Final Batch Loss: 0.2530863285064697\n",
      "Epoch 2250, Loss: 0.3273889273405075, Final Batch Loss: 0.15424372255802155\n",
      "Epoch 2251, Loss: 0.34476371109485626, Final Batch Loss: 0.1879880279302597\n",
      "Epoch 2252, Loss: 0.3418983221054077, Final Batch Loss: 0.1690085381269455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2253, Loss: 0.42636509239673615, Final Batch Loss: 0.2124304622411728\n",
      "Epoch 2254, Loss: 0.3963274210691452, Final Batch Loss: 0.2391105741262436\n",
      "Epoch 2255, Loss: 0.3589065223932266, Final Batch Loss: 0.22388826310634613\n",
      "Epoch 2256, Loss: 0.315976545214653, Final Batch Loss: 0.12893661856651306\n",
      "Epoch 2257, Loss: 0.3345620036125183, Final Batch Loss: 0.18924401700496674\n",
      "Epoch 2258, Loss: 0.38741882145404816, Final Batch Loss: 0.1906892955303192\n",
      "Epoch 2259, Loss: 0.3802456110715866, Final Batch Loss: 0.17138436436653137\n",
      "Epoch 2260, Loss: 0.3128080368041992, Final Batch Loss: 0.14335986971855164\n",
      "Epoch 2261, Loss: 0.3758934438228607, Final Batch Loss: 0.2292204648256302\n",
      "Epoch 2262, Loss: 0.3168957382440567, Final Batch Loss: 0.15437984466552734\n",
      "Epoch 2263, Loss: 0.34675949811935425, Final Batch Loss: 0.15050393342971802\n",
      "Epoch 2264, Loss: 0.32487280666828156, Final Batch Loss: 0.16146878898143768\n",
      "Epoch 2265, Loss: 0.3273180276155472, Final Batch Loss: 0.17218820750713348\n",
      "Epoch 2266, Loss: 0.400993674993515, Final Batch Loss: 0.21824145317077637\n",
      "Epoch 2267, Loss: 0.36681343615055084, Final Batch Loss: 0.17450039088726044\n",
      "Epoch 2268, Loss: 0.34645697474479675, Final Batch Loss: 0.19403061270713806\n",
      "Epoch 2269, Loss: 0.3302713483572006, Final Batch Loss: 0.14934013783931732\n",
      "Epoch 2270, Loss: 0.308094322681427, Final Batch Loss: 0.1580958217382431\n",
      "Epoch 2271, Loss: 0.30122019350528717, Final Batch Loss: 0.14272689819335938\n",
      "Epoch 2272, Loss: 0.3399842232465744, Final Batch Loss: 0.14322537183761597\n",
      "Epoch 2273, Loss: 0.35326890647411346, Final Batch Loss: 0.17841887474060059\n",
      "Epoch 2274, Loss: 0.3370679020881653, Final Batch Loss: 0.17763108015060425\n",
      "Epoch 2275, Loss: 0.32483766973018646, Final Batch Loss: 0.14943328499794006\n",
      "Epoch 2276, Loss: 0.36349551379680634, Final Batch Loss: 0.21169184148311615\n",
      "Epoch 2277, Loss: 0.39493417739868164, Final Batch Loss: 0.23093637824058533\n",
      "Epoch 2278, Loss: 0.3991582989692688, Final Batch Loss: 0.20413167774677277\n",
      "Epoch 2279, Loss: 0.4912891834974289, Final Batch Loss: 0.328233927488327\n",
      "Epoch 2280, Loss: 0.39696598052978516, Final Batch Loss: 0.1974116414785385\n",
      "Epoch 2281, Loss: 0.3793586641550064, Final Batch Loss: 0.14085835218429565\n",
      "Epoch 2282, Loss: 0.3423188477754593, Final Batch Loss: 0.1905626803636551\n",
      "Epoch 2283, Loss: 0.35528118908405304, Final Batch Loss: 0.19186298549175262\n",
      "Epoch 2284, Loss: 0.3662533313035965, Final Batch Loss: 0.1866263896226883\n",
      "Epoch 2285, Loss: 0.3478600084781647, Final Batch Loss: 0.1818687617778778\n",
      "Epoch 2286, Loss: 0.39572054147720337, Final Batch Loss: 0.21258589625358582\n",
      "Epoch 2287, Loss: 0.3463229387998581, Final Batch Loss: 0.18482036888599396\n",
      "Epoch 2288, Loss: 0.31053920835256577, Final Batch Loss: 0.12044728547334671\n",
      "Epoch 2289, Loss: 0.31486715376377106, Final Batch Loss: 0.1571189910173416\n",
      "Epoch 2290, Loss: 0.33231034874916077, Final Batch Loss: 0.15801775455474854\n",
      "Epoch 2291, Loss: 0.304768830537796, Final Batch Loss: 0.13477151095867157\n",
      "Epoch 2292, Loss: 0.31932608783245087, Final Batch Loss: 0.18566285073757172\n",
      "Epoch 2293, Loss: 0.3584994524717331, Final Batch Loss: 0.2136160433292389\n",
      "Epoch 2294, Loss: 0.30626438558101654, Final Batch Loss: 0.1401519924402237\n",
      "Epoch 2295, Loss: 0.3548305630683899, Final Batch Loss: 0.17701590061187744\n",
      "Epoch 2296, Loss: 0.3461157977581024, Final Batch Loss: 0.19011181592941284\n",
      "Epoch 2297, Loss: 0.3517388105392456, Final Batch Loss: 0.14969758689403534\n",
      "Epoch 2298, Loss: 0.3493943512439728, Final Batch Loss: 0.14606012403964996\n",
      "Epoch 2299, Loss: 0.32772430777549744, Final Batch Loss: 0.15293999016284943\n",
      "Epoch 2300, Loss: 0.36625999212265015, Final Batch Loss: 0.17896652221679688\n",
      "Epoch 2301, Loss: 0.3462565690279007, Final Batch Loss: 0.17388948798179626\n",
      "Epoch 2302, Loss: 0.3321181982755661, Final Batch Loss: 0.15045402944087982\n",
      "Epoch 2303, Loss: 0.31334054470062256, Final Batch Loss: 0.17403607070446014\n",
      "Epoch 2304, Loss: 0.3360123634338379, Final Batch Loss: 0.16688917577266693\n",
      "Epoch 2305, Loss: 0.33812758326530457, Final Batch Loss: 0.15937510132789612\n",
      "Epoch 2306, Loss: 0.36796873807907104, Final Batch Loss: 0.20704232156276703\n",
      "Epoch 2307, Loss: 0.32453568279743195, Final Batch Loss: 0.14614222943782806\n",
      "Epoch 2308, Loss: 0.3496154844760895, Final Batch Loss: 0.16864341497421265\n",
      "Epoch 2309, Loss: 0.34727387875318527, Final Batch Loss: 0.1069471463561058\n",
      "Epoch 2310, Loss: 0.3229410946369171, Final Batch Loss: 0.13692963123321533\n",
      "Epoch 2311, Loss: 0.32639072835445404, Final Batch Loss: 0.16669109463691711\n",
      "Epoch 2312, Loss: 0.3087947964668274, Final Batch Loss: 0.12327185273170471\n",
      "Epoch 2313, Loss: 0.32503776252269745, Final Batch Loss: 0.14448586106300354\n",
      "Epoch 2314, Loss: 0.353606715798378, Final Batch Loss: 0.21576781570911407\n",
      "Epoch 2315, Loss: 0.3919232487678528, Final Batch Loss: 0.16504304111003876\n",
      "Epoch 2316, Loss: 0.32136237621307373, Final Batch Loss: 0.16637608408927917\n",
      "Epoch 2317, Loss: 0.3764425367116928, Final Batch Loss: 0.1658380776643753\n",
      "Epoch 2318, Loss: 0.346599280834198, Final Batch Loss: 0.18707481026649475\n",
      "Epoch 2319, Loss: 0.3263706564903259, Final Batch Loss: 0.15425707399845123\n",
      "Epoch 2320, Loss: 0.3180609792470932, Final Batch Loss: 0.16190674901008606\n",
      "Epoch 2321, Loss: 0.3040374368429184, Final Batch Loss: 0.1376696527004242\n",
      "Epoch 2322, Loss: 0.31024275720119476, Final Batch Loss: 0.15206235647201538\n",
      "Epoch 2323, Loss: 0.35253144800662994, Final Batch Loss: 0.19037988781929016\n",
      "Epoch 2324, Loss: 0.35234948992729187, Final Batch Loss: 0.2002319097518921\n",
      "Epoch 2325, Loss: 0.31537093222141266, Final Batch Loss: 0.17670956254005432\n",
      "Epoch 2326, Loss: 0.3906332403421402, Final Batch Loss: 0.188332661986351\n",
      "Epoch 2327, Loss: 0.3359646499156952, Final Batch Loss: 0.19107723236083984\n",
      "Epoch 2328, Loss: 0.3507656902074814, Final Batch Loss: 0.19419683516025543\n",
      "Epoch 2329, Loss: 0.3647676706314087, Final Batch Loss: 0.1716821789741516\n",
      "Epoch 2330, Loss: 0.3580337166786194, Final Batch Loss: 0.21493449807167053\n",
      "Epoch 2331, Loss: 0.3283745273947716, Final Batch Loss: 0.124358169734478\n",
      "Epoch 2332, Loss: 0.319352850317955, Final Batch Loss: 0.18542957305908203\n",
      "Epoch 2333, Loss: 0.33586183190345764, Final Batch Loss: 0.18305902183055878\n",
      "Epoch 2334, Loss: 0.3715551048517227, Final Batch Loss: 0.202736034989357\n",
      "Epoch 2335, Loss: 0.30010001361370087, Final Batch Loss: 0.12827414274215698\n",
      "Epoch 2336, Loss: 0.3446137607097626, Final Batch Loss: 0.19319403171539307\n",
      "Epoch 2337, Loss: 0.35758639872074127, Final Batch Loss: 0.17484188079833984\n",
      "Epoch 2338, Loss: 0.3790348321199417, Final Batch Loss: 0.1855974644422531\n",
      "Epoch 2339, Loss: 0.37804582715034485, Final Batch Loss: 0.20611995458602905\n",
      "Epoch 2340, Loss: 0.3462134003639221, Final Batch Loss: 0.1742066890001297\n",
      "Epoch 2341, Loss: 0.33320508897304535, Final Batch Loss: 0.1454700082540512\n",
      "Epoch 2342, Loss: 0.36525771021842957, Final Batch Loss: 0.14052478969097137\n",
      "Epoch 2343, Loss: 0.27984417229890823, Final Batch Loss: 0.12198842316865921\n",
      "Epoch 2344, Loss: 0.31734855473041534, Final Batch Loss: 0.14883939921855927\n",
      "Epoch 2345, Loss: 0.33927811682224274, Final Batch Loss: 0.16145142912864685\n",
      "Epoch 2346, Loss: 0.3087994456291199, Final Batch Loss: 0.18205665051937103\n",
      "Epoch 2347, Loss: 0.3182613253593445, Final Batch Loss: 0.1870814859867096\n",
      "Epoch 2348, Loss: 0.35309264063835144, Final Batch Loss: 0.21511238813400269\n",
      "Epoch 2349, Loss: 0.40258388221263885, Final Batch Loss: 0.23491878807544708\n",
      "Epoch 2350, Loss: 0.3443804383277893, Final Batch Loss: 0.20133642852306366\n",
      "Epoch 2351, Loss: 0.35408009588718414, Final Batch Loss: 0.2033187448978424\n",
      "Epoch 2352, Loss: 0.3122483491897583, Final Batch Loss: 0.14237438142299652\n",
      "Epoch 2353, Loss: 0.31609781086444855, Final Batch Loss: 0.13587510585784912\n",
      "Epoch 2354, Loss: 0.3647449463605881, Final Batch Loss: 0.16304154694080353\n",
      "Epoch 2355, Loss: 0.3477419465780258, Final Batch Loss: 0.1355726718902588\n",
      "Epoch 2356, Loss: 0.3766191005706787, Final Batch Loss: 0.17569801211357117\n",
      "Epoch 2357, Loss: 0.44041769206523895, Final Batch Loss: 0.250350683927536\n",
      "Epoch 2358, Loss: 0.3225252032279968, Final Batch Loss: 0.1759943664073944\n",
      "Epoch 2359, Loss: 0.30899886786937714, Final Batch Loss: 0.15554271638393402\n",
      "Epoch 2360, Loss: 0.384340837597847, Final Batch Loss: 0.2299278974533081\n",
      "Epoch 2361, Loss: 0.3165276050567627, Final Batch Loss: 0.1308467537164688\n",
      "Epoch 2362, Loss: 0.349824920296669, Final Batch Loss: 0.17173580825328827\n",
      "Epoch 2363, Loss: 0.4355628341436386, Final Batch Loss: 0.26079070568084717\n",
      "Epoch 2364, Loss: 0.3198872059583664, Final Batch Loss: 0.14202262461185455\n",
      "Epoch 2365, Loss: 0.30969616770744324, Final Batch Loss: 0.1468810737133026\n",
      "Epoch 2366, Loss: 0.3004726991057396, Final Batch Loss: 0.12046035379171371\n",
      "Epoch 2367, Loss: 0.3027525320649147, Final Batch Loss: 0.12358389049768448\n",
      "Epoch 2368, Loss: 0.3740905225276947, Final Batch Loss: 0.19050025939941406\n",
      "Epoch 2369, Loss: 0.3258269280195236, Final Batch Loss: 0.14799919724464417\n",
      "Epoch 2370, Loss: 0.33562780916690826, Final Batch Loss: 0.16693231463432312\n",
      "Epoch 2371, Loss: 0.3281232863664627, Final Batch Loss: 0.14251668751239777\n",
      "Epoch 2372, Loss: 0.4250381290912628, Final Batch Loss: 0.21216051280498505\n",
      "Epoch 2373, Loss: 0.3267906457185745, Final Batch Loss: 0.14565224945545197\n",
      "Epoch 2374, Loss: 0.403992623090744, Final Batch Loss: 0.1773768961429596\n",
      "Epoch 2375, Loss: 0.33605584502220154, Final Batch Loss: 0.17487424612045288\n",
      "Epoch 2376, Loss: 0.403300017118454, Final Batch Loss: 0.21576009690761566\n",
      "Epoch 2377, Loss: 0.34349967539310455, Final Batch Loss: 0.21968446671962738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2378, Loss: 0.3557873070240021, Final Batch Loss: 0.1957055777311325\n",
      "Epoch 2379, Loss: 0.3611147403717041, Final Batch Loss: 0.18575480580329895\n",
      "Epoch 2380, Loss: 0.36102986335754395, Final Batch Loss: 0.17730671167373657\n",
      "Epoch 2381, Loss: 0.3503023236989975, Final Batch Loss: 0.15184727311134338\n",
      "Epoch 2382, Loss: 0.3571242094039917, Final Batch Loss: 0.17795954644680023\n",
      "Epoch 2383, Loss: 0.28698787093162537, Final Batch Loss: 0.13012440502643585\n",
      "Epoch 2384, Loss: 0.3765202462673187, Final Batch Loss: 0.21819967031478882\n",
      "Epoch 2385, Loss: 0.4528294652700424, Final Batch Loss: 0.2720189690589905\n",
      "Epoch 2386, Loss: 0.3471640795469284, Final Batch Loss: 0.18620353937149048\n",
      "Epoch 2387, Loss: 0.3708163946866989, Final Batch Loss: 0.2104005515575409\n",
      "Epoch 2388, Loss: 0.4093029946088791, Final Batch Loss: 0.24665769934654236\n",
      "Epoch 2389, Loss: 0.3401893824338913, Final Batch Loss: 0.13499896228313446\n",
      "Epoch 2390, Loss: 0.30856236815452576, Final Batch Loss: 0.13729889690876007\n",
      "Epoch 2391, Loss: 0.343438521027565, Final Batch Loss: 0.1572473794221878\n",
      "Epoch 2392, Loss: 0.30083276331424713, Final Batch Loss: 0.1273091733455658\n",
      "Epoch 2393, Loss: 0.36387230455875397, Final Batch Loss: 0.16102047264575958\n",
      "Epoch 2394, Loss: 0.38647550344467163, Final Batch Loss: 0.2487802803516388\n",
      "Epoch 2395, Loss: 0.3939586579799652, Final Batch Loss: 0.18560343980789185\n",
      "Epoch 2396, Loss: 0.37802037596702576, Final Batch Loss: 0.18804535269737244\n",
      "Epoch 2397, Loss: 0.29295670986175537, Final Batch Loss: 0.15869195759296417\n",
      "Epoch 2398, Loss: 0.3097015470266342, Final Batch Loss: 0.1335904747247696\n",
      "Epoch 2399, Loss: 0.3304968327283859, Final Batch Loss: 0.17024238407611847\n",
      "Epoch 2400, Loss: 0.32051487267017365, Final Batch Loss: 0.15054601430892944\n",
      "Epoch 2401, Loss: 0.3929198980331421, Final Batch Loss: 0.2327568680047989\n",
      "Epoch 2402, Loss: 0.38252460956573486, Final Batch Loss: 0.1915886402130127\n",
      "Epoch 2403, Loss: 0.3216594308614731, Final Batch Loss: 0.12737804651260376\n",
      "Epoch 2404, Loss: 0.35985787212848663, Final Batch Loss: 0.19385087490081787\n",
      "Epoch 2405, Loss: 0.34179578721523285, Final Batch Loss: 0.1881442368030548\n",
      "Epoch 2406, Loss: 0.35349489748477936, Final Batch Loss: 0.17229005694389343\n",
      "Epoch 2407, Loss: 0.34263062477111816, Final Batch Loss: 0.16308750212192535\n",
      "Epoch 2408, Loss: 0.3267534524202347, Final Batch Loss: 0.17472024261951447\n",
      "Epoch 2409, Loss: 0.3200068920850754, Final Batch Loss: 0.14098435640335083\n",
      "Epoch 2410, Loss: 0.3011048585176468, Final Batch Loss: 0.15727974474430084\n",
      "Epoch 2411, Loss: 0.37732988595962524, Final Batch Loss: 0.21316228806972504\n",
      "Epoch 2412, Loss: 0.3763122111558914, Final Batch Loss: 0.20764261484146118\n",
      "Epoch 2413, Loss: 0.3408433049917221, Final Batch Loss: 0.1660018265247345\n",
      "Epoch 2414, Loss: 0.3572828769683838, Final Batch Loss: 0.21439768373966217\n",
      "Epoch 2415, Loss: 0.37416860461235046, Final Batch Loss: 0.16274864971637726\n",
      "Epoch 2416, Loss: 0.35321713984012604, Final Batch Loss: 0.18353328108787537\n",
      "Epoch 2417, Loss: 0.3434170037508011, Final Batch Loss: 0.15853264927864075\n",
      "Epoch 2418, Loss: 0.317077100276947, Final Batch Loss: 0.13652817904949188\n",
      "Epoch 2419, Loss: 0.3396625369787216, Final Batch Loss: 0.1856374442577362\n",
      "Epoch 2420, Loss: 0.2927917540073395, Final Batch Loss: 0.13396425545215607\n",
      "Epoch 2421, Loss: 0.3001265153288841, Final Batch Loss: 0.10309629887342453\n",
      "Epoch 2422, Loss: 0.34980371594429016, Final Batch Loss: 0.18567515909671783\n",
      "Epoch 2423, Loss: 0.3604010343551636, Final Batch Loss: 0.19007818400859833\n",
      "Epoch 2424, Loss: 0.3037170469760895, Final Batch Loss: 0.13796044886112213\n",
      "Epoch 2425, Loss: 0.30588632822036743, Final Batch Loss: 0.13185399770736694\n",
      "Epoch 2426, Loss: 0.36035194993019104, Final Batch Loss: 0.1761806756258011\n",
      "Epoch 2427, Loss: 0.39726829528808594, Final Batch Loss: 0.19288669526576996\n",
      "Epoch 2428, Loss: 0.36952507495880127, Final Batch Loss: 0.2012706696987152\n",
      "Epoch 2429, Loss: 0.4164634048938751, Final Batch Loss: 0.2390599399805069\n",
      "Epoch 2430, Loss: 0.29618558287620544, Final Batch Loss: 0.15910328924655914\n",
      "Epoch 2431, Loss: 0.2884436547756195, Final Batch Loss: 0.10850335657596588\n",
      "Epoch 2432, Loss: 0.3677421659231186, Final Batch Loss: 0.19241926074028015\n",
      "Epoch 2433, Loss: 0.3237699717283249, Final Batch Loss: 0.1625150442123413\n",
      "Epoch 2434, Loss: 0.3439782112836838, Final Batch Loss: 0.15926524996757507\n",
      "Epoch 2435, Loss: 0.35758641362190247, Final Batch Loss: 0.15041697025299072\n",
      "Epoch 2436, Loss: 0.3221176713705063, Final Batch Loss: 0.12791401147842407\n",
      "Epoch 2437, Loss: 0.33013418316841125, Final Batch Loss: 0.16234643757343292\n",
      "Epoch 2438, Loss: 0.2958409786224365, Final Batch Loss: 0.13569779694080353\n",
      "Epoch 2439, Loss: 0.3641621842980385, Final Batch Loss: 0.11905152350664139\n",
      "Epoch 2440, Loss: 0.3205246776342392, Final Batch Loss: 0.14807896316051483\n",
      "Epoch 2441, Loss: 0.4545949548482895, Final Batch Loss: 0.21083000302314758\n",
      "Epoch 2442, Loss: 0.3536040633916855, Final Batch Loss: 0.17403662204742432\n",
      "Epoch 2443, Loss: 0.3331052362918854, Final Batch Loss: 0.16150997579097748\n",
      "Epoch 2444, Loss: 0.31354743242263794, Final Batch Loss: 0.16709399223327637\n",
      "Epoch 2445, Loss: 0.3083879351615906, Final Batch Loss: 0.142884761095047\n",
      "Epoch 2446, Loss: 0.30590197443962097, Final Batch Loss: 0.11834092438220978\n",
      "Epoch 2447, Loss: 0.32013241946697235, Final Batch Loss: 0.14258743822574615\n",
      "Epoch 2448, Loss: 0.35463255643844604, Final Batch Loss: 0.1627230942249298\n",
      "Epoch 2449, Loss: 0.27215124666690826, Final Batch Loss: 0.13298557698726654\n",
      "Epoch 2450, Loss: 0.2878437936306, Final Batch Loss: 0.14835689961910248\n",
      "Epoch 2451, Loss: 0.3419262021780014, Final Batch Loss: 0.1320696771144867\n",
      "Epoch 2452, Loss: 0.31056684255599976, Final Batch Loss: 0.1583012491464615\n",
      "Epoch 2453, Loss: 0.34285594522953033, Final Batch Loss: 0.18952392041683197\n",
      "Epoch 2454, Loss: 0.3594685047864914, Final Batch Loss: 0.168582022190094\n",
      "Epoch 2455, Loss: 0.33986298739910126, Final Batch Loss: 0.19828450679779053\n",
      "Epoch 2456, Loss: 0.3732556849718094, Final Batch Loss: 0.16746695339679718\n",
      "Epoch 2457, Loss: 0.3815993517637253, Final Batch Loss: 0.1735278218984604\n",
      "Epoch 2458, Loss: 0.34081970155239105, Final Batch Loss: 0.208375483751297\n",
      "Epoch 2459, Loss: 0.3394022434949875, Final Batch Loss: 0.16273809969425201\n",
      "Epoch 2460, Loss: 0.38579852879047394, Final Batch Loss: 0.23587666451931\n",
      "Epoch 2461, Loss: 0.3343481123447418, Final Batch Loss: 0.17384715378284454\n",
      "Epoch 2462, Loss: 0.3056824207305908, Final Batch Loss: 0.143330380320549\n",
      "Epoch 2463, Loss: 0.32730646431446075, Final Batch Loss: 0.14498300850391388\n",
      "Epoch 2464, Loss: 0.35087670385837555, Final Batch Loss: 0.2003406584262848\n",
      "Epoch 2465, Loss: 0.2945217341184616, Final Batch Loss: 0.12571018934249878\n",
      "Epoch 2466, Loss: 0.3757203370332718, Final Batch Loss: 0.18141207098960876\n",
      "Epoch 2467, Loss: 0.33500292897224426, Final Batch Loss: 0.1552862524986267\n",
      "Epoch 2468, Loss: 0.365740567445755, Final Batch Loss: 0.18531736731529236\n",
      "Epoch 2469, Loss: 0.29010042548179626, Final Batch Loss: 0.1459411382675171\n",
      "Epoch 2470, Loss: 0.35136954486370087, Final Batch Loss: 0.17957280576229095\n",
      "Epoch 2471, Loss: 0.3548525422811508, Final Batch Loss: 0.17845776677131653\n",
      "Epoch 2472, Loss: 0.4429541528224945, Final Batch Loss: 0.26151755452156067\n",
      "Epoch 2473, Loss: 0.3044864386320114, Final Batch Loss: 0.1555960476398468\n",
      "Epoch 2474, Loss: 0.2897924333810806, Final Batch Loss: 0.14150801301002502\n",
      "Epoch 2475, Loss: 0.3494824320077896, Final Batch Loss: 0.18787211179733276\n",
      "Epoch 2476, Loss: 0.29862837493419647, Final Batch Loss: 0.1436420977115631\n",
      "Epoch 2477, Loss: 0.3363915681838989, Final Batch Loss: 0.16093659400939941\n",
      "Epoch 2478, Loss: 0.3457861989736557, Final Batch Loss: 0.18362130224704742\n",
      "Epoch 2479, Loss: 0.34382908046245575, Final Batch Loss: 0.15630197525024414\n",
      "Epoch 2480, Loss: 0.3772452771663666, Final Batch Loss: 0.1972646713256836\n",
      "Epoch 2481, Loss: 0.3769930452108383, Final Batch Loss: 0.2257547676563263\n",
      "Epoch 2482, Loss: 0.37627433240413666, Final Batch Loss: 0.22992272675037384\n",
      "Epoch 2483, Loss: 0.32669582962989807, Final Batch Loss: 0.1471666395664215\n",
      "Epoch 2484, Loss: 0.36498212814331055, Final Batch Loss: 0.16359665989875793\n",
      "Epoch 2485, Loss: 0.33227504789829254, Final Batch Loss: 0.16624365746974945\n",
      "Epoch 2486, Loss: 0.33090540766716003, Final Batch Loss: 0.17201773822307587\n",
      "Epoch 2487, Loss: 0.3305136635899544, Final Batch Loss: 0.20586201548576355\n",
      "Epoch 2488, Loss: 0.39104948937892914, Final Batch Loss: 0.23471671342849731\n",
      "Epoch 2489, Loss: 0.2954389601945877, Final Batch Loss: 0.11657986044883728\n",
      "Epoch 2490, Loss: 0.36474497616291046, Final Batch Loss: 0.1847795844078064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2491, Loss: 0.30419012904167175, Final Batch Loss: 0.1340862363576889\n",
      "Epoch 2492, Loss: 0.3273294270038605, Final Batch Loss: 0.15858705341815948\n",
      "Epoch 2493, Loss: 0.3642527535557747, Final Batch Loss: 0.1138460710644722\n",
      "Epoch 2494, Loss: 0.3578610122203827, Final Batch Loss: 0.16110600531101227\n",
      "Epoch 2495, Loss: 0.35807928442955017, Final Batch Loss: 0.19609037041664124\n",
      "Epoch 2496, Loss: 0.30406560003757477, Final Batch Loss: 0.14915773272514343\n",
      "Epoch 2497, Loss: 0.4141508489847183, Final Batch Loss: 0.2536931037902832\n",
      "Epoch 2498, Loss: 0.2935754060745239, Final Batch Loss: 0.1510220170021057\n",
      "Epoch 2499, Loss: 0.340973362326622, Final Batch Loss: 0.1540333330631256\n",
      "Epoch 2500, Loss: 0.3488122671842575, Final Batch Loss: 0.1629776805639267\n",
      "Epoch 2501, Loss: 0.3927835375070572, Final Batch Loss: 0.2121380865573883\n",
      "Epoch 2502, Loss: 0.3460036665201187, Final Batch Loss: 0.17215412855148315\n",
      "Epoch 2503, Loss: 0.34614184498786926, Final Batch Loss: 0.1695781946182251\n",
      "Epoch 2504, Loss: 0.4092811495065689, Final Batch Loss: 0.21753686666488647\n",
      "Epoch 2505, Loss: 0.3163992017507553, Final Batch Loss: 0.16562512516975403\n",
      "Epoch 2506, Loss: 0.31144870817661285, Final Batch Loss: 0.14545495808124542\n",
      "Epoch 2507, Loss: 0.2994370460510254, Final Batch Loss: 0.13774944841861725\n",
      "Epoch 2508, Loss: 0.32076340913772583, Final Batch Loss: 0.17993605136871338\n",
      "Epoch 2509, Loss: 0.3370000272989273, Final Batch Loss: 0.16735607385635376\n",
      "Epoch 2510, Loss: 0.35314369201660156, Final Batch Loss: 0.19816537201404572\n",
      "Epoch 2511, Loss: 0.3714771419763565, Final Batch Loss: 0.19820550084114075\n",
      "Epoch 2512, Loss: 0.2997833639383316, Final Batch Loss: 0.13001149892807007\n",
      "Epoch 2513, Loss: 0.36900995671749115, Final Batch Loss: 0.2031240165233612\n",
      "Epoch 2514, Loss: 0.3211755156517029, Final Batch Loss: 0.15138068795204163\n",
      "Epoch 2515, Loss: 0.36321042478084564, Final Batch Loss: 0.1936093270778656\n",
      "Epoch 2516, Loss: 0.3243377208709717, Final Batch Loss: 0.16338784992694855\n",
      "Epoch 2517, Loss: 0.3549996167421341, Final Batch Loss: 0.19214662909507751\n",
      "Epoch 2518, Loss: 0.3155084401369095, Final Batch Loss: 0.15114706754684448\n",
      "Epoch 2519, Loss: 0.3075522482395172, Final Batch Loss: 0.15913879871368408\n",
      "Epoch 2520, Loss: 0.37055113911628723, Final Batch Loss: 0.20241908729076385\n",
      "Epoch 2521, Loss: 0.3159725219011307, Final Batch Loss: 0.1570872813463211\n",
      "Epoch 2522, Loss: 0.3548920154571533, Final Batch Loss: 0.19398508965969086\n",
      "Epoch 2523, Loss: 0.3134278208017349, Final Batch Loss: 0.15106503665447235\n",
      "Epoch 2524, Loss: 0.33328375220298767, Final Batch Loss: 0.1908375322818756\n",
      "Epoch 2525, Loss: 0.29790225625038147, Final Batch Loss: 0.12708623707294464\n",
      "Epoch 2526, Loss: 0.3599269539117813, Final Batch Loss: 0.17777486145496368\n",
      "Epoch 2527, Loss: 0.3488758057355881, Final Batch Loss: 0.17515985667705536\n",
      "Epoch 2528, Loss: 0.32662828266620636, Final Batch Loss: 0.17387214303016663\n",
      "Epoch 2529, Loss: 0.32898789644241333, Final Batch Loss: 0.17343218624591827\n",
      "Epoch 2530, Loss: 0.35811468958854675, Final Batch Loss: 0.21023721992969513\n",
      "Epoch 2531, Loss: 0.30607323348522186, Final Batch Loss: 0.14705762267112732\n",
      "Epoch 2532, Loss: 0.29130882024765015, Final Batch Loss: 0.15541613101959229\n",
      "Epoch 2533, Loss: 0.3570259362459183, Final Batch Loss: 0.1583739072084427\n",
      "Epoch 2534, Loss: 0.3011927455663681, Final Batch Loss: 0.1582675725221634\n",
      "Epoch 2535, Loss: 0.29933425784111023, Final Batch Loss: 0.16348934173583984\n",
      "Epoch 2536, Loss: 0.28757748007774353, Final Batch Loss: 0.13045530021190643\n",
      "Epoch 2537, Loss: 0.32337477803230286, Final Batch Loss: 0.1719491332769394\n",
      "Epoch 2538, Loss: 0.2841865196824074, Final Batch Loss: 0.12149878591299057\n",
      "Epoch 2539, Loss: 0.28956589102745056, Final Batch Loss: 0.12380540370941162\n",
      "Epoch 2540, Loss: 0.30921532958745956, Final Batch Loss: 0.11622593551874161\n",
      "Epoch 2541, Loss: 0.29748503863811493, Final Batch Loss: 0.1558746099472046\n",
      "Epoch 2542, Loss: 0.365473672747612, Final Batch Loss: 0.17326517403125763\n",
      "Epoch 2543, Loss: 0.3186686336994171, Final Batch Loss: 0.14791730046272278\n",
      "Epoch 2544, Loss: 0.3741123378276825, Final Batch Loss: 0.20975719392299652\n",
      "Epoch 2545, Loss: 0.32318463921546936, Final Batch Loss: 0.11774563789367676\n",
      "Epoch 2546, Loss: 0.34350332617759705, Final Batch Loss: 0.19882026314735413\n",
      "Epoch 2547, Loss: 0.2972447872161865, Final Batch Loss: 0.14585527777671814\n",
      "Epoch 2548, Loss: 0.4721725285053253, Final Batch Loss: 0.3055776357650757\n",
      "Epoch 2549, Loss: 0.273386150598526, Final Batch Loss: 0.12605275213718414\n",
      "Epoch 2550, Loss: 0.2754417359828949, Final Batch Loss: 0.11629746854305267\n",
      "Epoch 2551, Loss: 0.3807792365550995, Final Batch Loss: 0.17769761383533478\n",
      "Epoch 2552, Loss: 0.3195393234491348, Final Batch Loss: 0.1845458596944809\n",
      "Epoch 2553, Loss: 0.35608330368995667, Final Batch Loss: 0.1999364048242569\n",
      "Epoch 2554, Loss: 0.3280535638332367, Final Batch Loss: 0.17581506073474884\n",
      "Epoch 2555, Loss: 0.2971179932355881, Final Batch Loss: 0.16261865198612213\n",
      "Epoch 2556, Loss: 0.39130957424640656, Final Batch Loss: 0.22113384306430817\n",
      "Epoch 2557, Loss: 0.3353966623544693, Final Batch Loss: 0.1895209103822708\n",
      "Epoch 2558, Loss: 0.2651953920722008, Final Batch Loss: 0.10408886522054672\n",
      "Epoch 2559, Loss: 0.31935645639896393, Final Batch Loss: 0.1494915932416916\n",
      "Epoch 2560, Loss: 0.34163254499435425, Final Batch Loss: 0.19341133534908295\n",
      "Epoch 2561, Loss: 0.3074115961790085, Final Batch Loss: 0.1286916732788086\n",
      "Epoch 2562, Loss: 0.30316415429115295, Final Batch Loss: 0.16360068321228027\n",
      "Epoch 2563, Loss: 0.29532311856746674, Final Batch Loss: 0.1345633566379547\n",
      "Epoch 2564, Loss: 0.32793234288692474, Final Batch Loss: 0.13427408039569855\n",
      "Epoch 2565, Loss: 0.303419753909111, Final Batch Loss: 0.1353345513343811\n",
      "Epoch 2566, Loss: 0.33209750056266785, Final Batch Loss: 0.1732974350452423\n",
      "Epoch 2567, Loss: 0.32654939591884613, Final Batch Loss: 0.19854216277599335\n",
      "Epoch 2568, Loss: 0.3722964972257614, Final Batch Loss: 0.20322252810001373\n",
      "Epoch 2569, Loss: 0.2857116684317589, Final Batch Loss: 0.09915677458047867\n",
      "Epoch 2570, Loss: 0.2989332377910614, Final Batch Loss: 0.13231050968170166\n",
      "Epoch 2571, Loss: 0.37657344341278076, Final Batch Loss: 0.2153032273054123\n",
      "Epoch 2572, Loss: 0.3174538165330887, Final Batch Loss: 0.135379821062088\n",
      "Epoch 2573, Loss: 0.37566427886486053, Final Batch Loss: 0.1892920583486557\n",
      "Epoch 2574, Loss: 0.33849121630191803, Final Batch Loss: 0.16699525713920593\n",
      "Epoch 2575, Loss: 0.35210874676704407, Final Batch Loss: 0.1652948409318924\n",
      "Epoch 2576, Loss: 0.303518608212471, Final Batch Loss: 0.16784000396728516\n",
      "Epoch 2577, Loss: 0.3376266658306122, Final Batch Loss: 0.1656484603881836\n",
      "Epoch 2578, Loss: 0.4240863472223282, Final Batch Loss: 0.260343462228775\n",
      "Epoch 2579, Loss: 0.26518913358449936, Final Batch Loss: 0.11866868287324905\n",
      "Epoch 2580, Loss: 0.3415854871273041, Final Batch Loss: 0.1877780258655548\n",
      "Epoch 2581, Loss: 0.3363770693540573, Final Batch Loss: 0.18890967965126038\n",
      "Epoch 2582, Loss: 0.28970620781183243, Final Batch Loss: 0.10615754872560501\n",
      "Epoch 2583, Loss: 0.275896891951561, Final Batch Loss: 0.12527355551719666\n",
      "Epoch 2584, Loss: 0.32279373705387115, Final Batch Loss: 0.18201707303524017\n",
      "Epoch 2585, Loss: 0.37234263122081757, Final Batch Loss: 0.22016167640686035\n",
      "Epoch 2586, Loss: 0.3608088493347168, Final Batch Loss: 0.16806362569332123\n",
      "Epoch 2587, Loss: 0.3707289844751358, Final Batch Loss: 0.20527510344982147\n",
      "Epoch 2588, Loss: 0.30259935557842255, Final Batch Loss: 0.1522672325372696\n",
      "Epoch 2589, Loss: 0.2859497144818306, Final Batch Loss: 0.10673520714044571\n",
      "Epoch 2590, Loss: 0.33741411566734314, Final Batch Loss: 0.19443482160568237\n",
      "Epoch 2591, Loss: 0.31091129779815674, Final Batch Loss: 0.1275782287120819\n",
      "Epoch 2592, Loss: 0.3457670360803604, Final Batch Loss: 0.1989976465702057\n",
      "Epoch 2593, Loss: 0.31030137836933136, Final Batch Loss: 0.1784907579421997\n",
      "Epoch 2594, Loss: 0.3227832168340683, Final Batch Loss: 0.16793721914291382\n",
      "Epoch 2595, Loss: 0.2925315946340561, Final Batch Loss: 0.15413512289524078\n",
      "Epoch 2596, Loss: 0.278232678771019, Final Batch Loss: 0.12597119808197021\n",
      "Epoch 2597, Loss: 0.363191619515419, Final Batch Loss: 0.22832299768924713\n",
      "Epoch 2598, Loss: 0.39035023748874664, Final Batch Loss: 0.20258396863937378\n",
      "Epoch 2599, Loss: 0.30449745059013367, Final Batch Loss: 0.1486791968345642\n",
      "Epoch 2600, Loss: 0.3224337249994278, Final Batch Loss: 0.18249428272247314\n",
      "Epoch 2601, Loss: 0.34041111171245575, Final Batch Loss: 0.14687831699848175\n",
      "Epoch 2602, Loss: 0.32412686944007874, Final Batch Loss: 0.13018743693828583\n",
      "Epoch 2603, Loss: 0.2929275184869766, Final Batch Loss: 0.1444818079471588\n",
      "Epoch 2604, Loss: 0.27124717831611633, Final Batch Loss: 0.14956459403038025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2605, Loss: 0.3136689215898514, Final Batch Loss: 0.1522037237882614\n",
      "Epoch 2606, Loss: 0.2846416234970093, Final Batch Loss: 0.15800389647483826\n",
      "Epoch 2607, Loss: 0.29484933614730835, Final Batch Loss: 0.1333126723766327\n",
      "Epoch 2608, Loss: 0.3514336943626404, Final Batch Loss: 0.16706733405590057\n",
      "Epoch 2609, Loss: 0.29346175491809845, Final Batch Loss: 0.1184028685092926\n",
      "Epoch 2610, Loss: 0.3164459317922592, Final Batch Loss: 0.17974025011062622\n",
      "Epoch 2611, Loss: 0.29343897104263306, Final Batch Loss: 0.14144138991832733\n",
      "Epoch 2612, Loss: 0.29071737080812454, Final Batch Loss: 0.1110943928360939\n",
      "Epoch 2613, Loss: 0.2858709841966629, Final Batch Loss: 0.15583330392837524\n",
      "Epoch 2614, Loss: 0.30195842683315277, Final Batch Loss: 0.1407989263534546\n",
      "Epoch 2615, Loss: 0.3083568811416626, Final Batch Loss: 0.176319882273674\n",
      "Epoch 2616, Loss: 0.3709997981786728, Final Batch Loss: 0.20891772210597992\n",
      "Epoch 2617, Loss: 0.32208098471164703, Final Batch Loss: 0.14764545857906342\n",
      "Epoch 2618, Loss: 0.3005148321390152, Final Batch Loss: 0.15853764116764069\n",
      "Epoch 2619, Loss: 0.33381596207618713, Final Batch Loss: 0.18982520699501038\n",
      "Epoch 2620, Loss: 0.3284602463245392, Final Batch Loss: 0.12058506906032562\n",
      "Epoch 2621, Loss: 0.2996489256620407, Final Batch Loss: 0.14442148804664612\n",
      "Epoch 2622, Loss: 0.30597715079784393, Final Batch Loss: 0.11278612911701202\n",
      "Epoch 2623, Loss: 0.4283774346113205, Final Batch Loss: 0.2927130460739136\n",
      "Epoch 2624, Loss: 0.26720526814460754, Final Batch Loss: 0.14054107666015625\n",
      "Epoch 2625, Loss: 0.33940985798835754, Final Batch Loss: 0.17611737549304962\n",
      "Epoch 2626, Loss: 0.2962770462036133, Final Batch Loss: 0.14279039204120636\n",
      "Epoch 2627, Loss: 0.3271096497774124, Final Batch Loss: 0.16636338829994202\n",
      "Epoch 2628, Loss: 0.32212090492248535, Final Batch Loss: 0.1756107658147812\n",
      "Epoch 2629, Loss: 0.27853792160749435, Final Batch Loss: 0.12065952271223068\n",
      "Epoch 2630, Loss: 0.317639485001564, Final Batch Loss: 0.1368120014667511\n",
      "Epoch 2631, Loss: 0.3572565168142319, Final Batch Loss: 0.2167191207408905\n",
      "Epoch 2632, Loss: 0.386672779917717, Final Batch Loss: 0.2209351509809494\n",
      "Epoch 2633, Loss: 0.3983694761991501, Final Batch Loss: 0.20043453574180603\n",
      "Epoch 2634, Loss: 0.30163052678108215, Final Batch Loss: 0.17248006165027618\n",
      "Epoch 2635, Loss: 0.4148361086845398, Final Batch Loss: 0.22532552480697632\n",
      "Epoch 2636, Loss: 0.3181508928537369, Final Batch Loss: 0.17266857624053955\n",
      "Epoch 2637, Loss: 0.26633958518505096, Final Batch Loss: 0.12064322829246521\n",
      "Epoch 2638, Loss: 0.3221592754125595, Final Batch Loss: 0.16104921698570251\n",
      "Epoch 2639, Loss: 0.3250717520713806, Final Batch Loss: 0.1757972240447998\n",
      "Epoch 2640, Loss: 0.317594975233078, Final Batch Loss: 0.14242491126060486\n",
      "Epoch 2641, Loss: 0.2956743687391281, Final Batch Loss: 0.100867360830307\n",
      "Epoch 2642, Loss: 0.27690014988183975, Final Batch Loss: 0.12171024829149246\n",
      "Epoch 2643, Loss: 0.3405075669288635, Final Batch Loss: 0.14203980565071106\n",
      "Epoch 2644, Loss: 0.2896982580423355, Final Batch Loss: 0.1291915327310562\n",
      "Epoch 2645, Loss: 0.3123276084661484, Final Batch Loss: 0.12550076842308044\n",
      "Epoch 2646, Loss: 0.33507785201072693, Final Batch Loss: 0.17780253291130066\n",
      "Epoch 2647, Loss: 0.2751842364668846, Final Batch Loss: 0.11955327540636063\n",
      "Epoch 2648, Loss: 0.3125259280204773, Final Batch Loss: 0.1674855500459671\n",
      "Epoch 2649, Loss: 0.3128321170806885, Final Batch Loss: 0.19741229712963104\n",
      "Epoch 2650, Loss: 0.34488634765148163, Final Batch Loss: 0.16135655343532562\n",
      "Epoch 2651, Loss: 0.26401475071907043, Final Batch Loss: 0.12826243042945862\n",
      "Epoch 2652, Loss: 0.35681456327438354, Final Batch Loss: 0.20235420763492584\n",
      "Epoch 2653, Loss: 0.2790849432349205, Final Batch Loss: 0.12228208035230637\n",
      "Epoch 2654, Loss: 0.31668969988822937, Final Batch Loss: 0.12096627056598663\n",
      "Epoch 2655, Loss: 0.344379186630249, Final Batch Loss: 0.1928473562002182\n",
      "Epoch 2656, Loss: 0.34555500745773315, Final Batch Loss: 0.18368937075138092\n",
      "Epoch 2657, Loss: 0.27394285798072815, Final Batch Loss: 0.13891656696796417\n",
      "Epoch 2658, Loss: 0.3640729784965515, Final Batch Loss: 0.1707196980714798\n",
      "Epoch 2659, Loss: 0.37604352831840515, Final Batch Loss: 0.15478681027889252\n",
      "Epoch 2660, Loss: 0.4346786439418793, Final Batch Loss: 0.28848880529403687\n",
      "Epoch 2661, Loss: 0.30095070600509644, Final Batch Loss: 0.15402206778526306\n",
      "Epoch 2662, Loss: 0.3561038374900818, Final Batch Loss: 0.19702737033367157\n",
      "Epoch 2663, Loss: 0.34655623137950897, Final Batch Loss: 0.20551586151123047\n",
      "Epoch 2664, Loss: 0.2680407613515854, Final Batch Loss: 0.12321582436561584\n",
      "Epoch 2665, Loss: 0.3390556871891022, Final Batch Loss: 0.18885958194732666\n",
      "Epoch 2666, Loss: 0.3490109145641327, Final Batch Loss: 0.15902020037174225\n",
      "Epoch 2667, Loss: 0.30282092094421387, Final Batch Loss: 0.1549009382724762\n",
      "Epoch 2668, Loss: 0.36993587017059326, Final Batch Loss: 0.21518853306770325\n",
      "Epoch 2669, Loss: 0.32653625309467316, Final Batch Loss: 0.19815696775913239\n",
      "Epoch 2670, Loss: 0.29547305405139923, Final Batch Loss: 0.14257824420928955\n",
      "Epoch 2671, Loss: 0.3296528309583664, Final Batch Loss: 0.16932512819766998\n",
      "Epoch 2672, Loss: 0.3305756002664566, Final Batch Loss: 0.1594715118408203\n",
      "Epoch 2673, Loss: 0.3164900690317154, Final Batch Loss: 0.13778673112392426\n",
      "Epoch 2674, Loss: 0.29454832524061203, Final Batch Loss: 0.12248929589986801\n",
      "Epoch 2675, Loss: 0.27806316316127777, Final Batch Loss: 0.14128261804580688\n",
      "Epoch 2676, Loss: 0.28806284815073013, Final Batch Loss: 0.11695777624845505\n",
      "Epoch 2677, Loss: 0.2887706458568573, Final Batch Loss: 0.14151853322982788\n",
      "Epoch 2678, Loss: 0.35288241505622864, Final Batch Loss: 0.19839483499526978\n",
      "Epoch 2679, Loss: 0.283053383231163, Final Batch Loss: 0.1599605530500412\n",
      "Epoch 2680, Loss: 0.3095496743917465, Final Batch Loss: 0.16158755123615265\n",
      "Epoch 2681, Loss: 0.2927847057580948, Final Batch Loss: 0.14703142642974854\n",
      "Epoch 2682, Loss: 0.2821003347635269, Final Batch Loss: 0.1341969221830368\n",
      "Epoch 2683, Loss: 0.32375963032245636, Final Batch Loss: 0.17263740301132202\n",
      "Epoch 2684, Loss: 0.25761543959379196, Final Batch Loss: 0.11604275554418564\n",
      "Epoch 2685, Loss: 0.30036818981170654, Final Batch Loss: 0.16992254555225372\n",
      "Epoch 2686, Loss: 0.2920026481151581, Final Batch Loss: 0.19151632487773895\n",
      "Epoch 2687, Loss: 0.28975917398929596, Final Batch Loss: 0.15135742723941803\n",
      "Epoch 2688, Loss: 0.2925528362393379, Final Batch Loss: 0.1166561171412468\n",
      "Epoch 2689, Loss: 0.3147982060909271, Final Batch Loss: 0.15232425928115845\n",
      "Epoch 2690, Loss: 0.3471491038799286, Final Batch Loss: 0.19652336835861206\n",
      "Epoch 2691, Loss: 0.2535960525274277, Final Batch Loss: 0.13090892136096954\n",
      "Epoch 2692, Loss: 0.2829732894897461, Final Batch Loss: 0.17198899388313293\n",
      "Epoch 2693, Loss: 0.29488229751586914, Final Batch Loss: 0.09082631766796112\n",
      "Epoch 2694, Loss: 0.31527867913246155, Final Batch Loss: 0.18044403195381165\n",
      "Epoch 2695, Loss: 0.31410451233386993, Final Batch Loss: 0.14876611530780792\n",
      "Epoch 2696, Loss: 0.30276261270046234, Final Batch Loss: 0.13279569149017334\n",
      "Epoch 2697, Loss: 0.3047127276659012, Final Batch Loss: 0.14654886722564697\n",
      "Epoch 2698, Loss: 0.28043514490127563, Final Batch Loss: 0.1167968213558197\n",
      "Epoch 2699, Loss: 0.3303357809782028, Final Batch Loss: 0.17706020176410675\n",
      "Epoch 2700, Loss: 0.3864680230617523, Final Batch Loss: 0.21452774107456207\n",
      "Epoch 2701, Loss: 0.2846681550145149, Final Batch Loss: 0.1619328260421753\n",
      "Epoch 2702, Loss: 0.25813663750886917, Final Batch Loss: 0.09707588702440262\n",
      "Epoch 2703, Loss: 0.27520258724689484, Final Batch Loss: 0.12684516608715057\n",
      "Epoch 2704, Loss: 0.27188974618911743, Final Batch Loss: 0.11828793585300446\n",
      "Epoch 2705, Loss: 0.42077501118183136, Final Batch Loss: 0.26644057035446167\n",
      "Epoch 2706, Loss: 0.31390658020973206, Final Batch Loss: 0.14051461219787598\n",
      "Epoch 2707, Loss: 0.3433362916111946, Final Batch Loss: 0.11711379140615463\n",
      "Epoch 2708, Loss: 0.33105967938899994, Final Batch Loss: 0.15227121114730835\n",
      "Epoch 2709, Loss: 0.336078017950058, Final Batch Loss: 0.1862473487854004\n",
      "Epoch 2710, Loss: 0.3232444226741791, Final Batch Loss: 0.163059264421463\n",
      "Epoch 2711, Loss: 0.3846909701824188, Final Batch Loss: 0.23187360167503357\n",
      "Epoch 2712, Loss: 0.28659293055534363, Final Batch Loss: 0.150650754570961\n",
      "Epoch 2713, Loss: 0.3587506264448166, Final Batch Loss: 0.2007681429386139\n",
      "Epoch 2714, Loss: 0.2786797732114792, Final Batch Loss: 0.14011840522289276\n",
      "Epoch 2715, Loss: 0.30043846368789673, Final Batch Loss: 0.1625329852104187\n",
      "Epoch 2716, Loss: 0.3168140947818756, Final Batch Loss: 0.15774869918823242\n",
      "Epoch 2717, Loss: 0.3279173821210861, Final Batch Loss: 0.13451513648033142\n",
      "Epoch 2718, Loss: 0.3296455293893814, Final Batch Loss: 0.1682613492012024\n",
      "Epoch 2719, Loss: 0.3004349321126938, Final Batch Loss: 0.17143964767456055\n",
      "Epoch 2720, Loss: 0.31331767141819, Final Batch Loss: 0.1601656675338745\n",
      "Epoch 2721, Loss: 0.26927168667316437, Final Batch Loss: 0.1327599585056305\n",
      "Epoch 2722, Loss: 0.2660318911075592, Final Batch Loss: 0.09741349518299103\n",
      "Epoch 2723, Loss: 0.32920362055301666, Final Batch Loss: 0.1516711562871933\n",
      "Epoch 2724, Loss: 0.3449975550174713, Final Batch Loss: 0.13290371000766754\n",
      "Epoch 2725, Loss: 0.2545160725712776, Final Batch Loss: 0.09775414317846298\n",
      "Epoch 2726, Loss: 0.32687386870384216, Final Batch Loss: 0.17054162919521332\n",
      "Epoch 2727, Loss: 0.37204085290431976, Final Batch Loss: 0.17545102536678314\n",
      "Epoch 2728, Loss: 0.30155594646930695, Final Batch Loss: 0.1498694270849228\n",
      "Epoch 2729, Loss: 0.28966034948825836, Final Batch Loss: 0.14082665741443634\n",
      "Epoch 2730, Loss: 0.3364372104406357, Final Batch Loss: 0.1912592202425003\n",
      "Epoch 2731, Loss: 0.29505176842212677, Final Batch Loss: 0.12241896986961365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2732, Loss: 0.3139031082391739, Final Batch Loss: 0.17237018048763275\n",
      "Epoch 2733, Loss: 0.2852979153394699, Final Batch Loss: 0.13707177340984344\n",
      "Epoch 2734, Loss: 0.30479151010513306, Final Batch Loss: 0.15849433839321136\n",
      "Epoch 2735, Loss: 0.3165445774793625, Final Batch Loss: 0.15001894533634186\n",
      "Epoch 2736, Loss: 0.32708707451820374, Final Batch Loss: 0.16259640455245972\n",
      "Epoch 2737, Loss: 0.2577630504965782, Final Batch Loss: 0.11076559871435165\n",
      "Epoch 2738, Loss: 0.28966808319091797, Final Batch Loss: 0.14730067551136017\n",
      "Epoch 2739, Loss: 0.25605764240026474, Final Batch Loss: 0.13577942550182343\n",
      "Epoch 2740, Loss: 0.3294578269124031, Final Batch Loss: 0.21065755188465118\n",
      "Epoch 2741, Loss: 0.2617403641343117, Final Batch Loss: 0.13792869448661804\n",
      "Epoch 2742, Loss: 0.35508738458156586, Final Batch Loss: 0.1766684502363205\n",
      "Epoch 2743, Loss: 0.2892974242568016, Final Batch Loss: 0.18026337027549744\n",
      "Epoch 2744, Loss: 0.2752833068370819, Final Batch Loss: 0.13254554569721222\n",
      "Epoch 2745, Loss: 0.30331145226955414, Final Batch Loss: 0.16386778652668\n",
      "Epoch 2746, Loss: 0.32150107622146606, Final Batch Loss: 0.14200127124786377\n",
      "Epoch 2747, Loss: 0.2789571285247803, Final Batch Loss: 0.13981018960475922\n",
      "Epoch 2748, Loss: 0.24335303157567978, Final Batch Loss: 0.09665492922067642\n",
      "Epoch 2749, Loss: 0.27955319732427597, Final Batch Loss: 0.12351033836603165\n",
      "Epoch 2750, Loss: 0.33207277953624725, Final Batch Loss: 0.16783198714256287\n",
      "Epoch 2751, Loss: 0.24720969796180725, Final Batch Loss: 0.12044766545295715\n",
      "Epoch 2752, Loss: 0.33902551233768463, Final Batch Loss: 0.18323002755641937\n",
      "Epoch 2753, Loss: 0.2547970935702324, Final Batch Loss: 0.12002084404230118\n",
      "Epoch 2754, Loss: 0.2828218936920166, Final Batch Loss: 0.11764875054359436\n",
      "Epoch 2755, Loss: 0.31680111587047577, Final Batch Loss: 0.16847823560237885\n",
      "Epoch 2756, Loss: 0.3316020667552948, Final Batch Loss: 0.20234201848506927\n",
      "Epoch 2757, Loss: 0.2545834109187126, Final Batch Loss: 0.1097455844283104\n",
      "Epoch 2758, Loss: 0.3151269853115082, Final Batch Loss: 0.13601098954677582\n",
      "Epoch 2759, Loss: 0.29178038239479065, Final Batch Loss: 0.1726192981004715\n",
      "Epoch 2760, Loss: 0.2824571654200554, Final Batch Loss: 0.18149995803833008\n",
      "Epoch 2761, Loss: 0.27956460416316986, Final Batch Loss: 0.17779189348220825\n",
      "Epoch 2762, Loss: 0.2998836189508438, Final Batch Loss: 0.14740793406963348\n",
      "Epoch 2763, Loss: 0.33336296677589417, Final Batch Loss: 0.18469688296318054\n",
      "Epoch 2764, Loss: 0.26703672856092453, Final Batch Loss: 0.11315562576055527\n",
      "Epoch 2765, Loss: 0.2815392315387726, Final Batch Loss: 0.13125042617321014\n",
      "Epoch 2766, Loss: 0.28412363678216934, Final Batch Loss: 0.12100066989660263\n",
      "Epoch 2767, Loss: 0.2927372455596924, Final Batch Loss: 0.14906099438667297\n",
      "Epoch 2768, Loss: 0.26108095049858093, Final Batch Loss: 0.1322997361421585\n",
      "Epoch 2769, Loss: 0.2985941469669342, Final Batch Loss: 0.11889687180519104\n",
      "Epoch 2770, Loss: 0.2752334028482437, Final Batch Loss: 0.11990639567375183\n",
      "Epoch 2771, Loss: 0.28503550589084625, Final Batch Loss: 0.1482847034931183\n",
      "Epoch 2772, Loss: 0.28338543325662613, Final Batch Loss: 0.1585865318775177\n",
      "Epoch 2773, Loss: 0.30584652721881866, Final Batch Loss: 0.1466834545135498\n",
      "Epoch 2774, Loss: 0.3322716951370239, Final Batch Loss: 0.16446852684020996\n",
      "Epoch 2775, Loss: 0.29607293009757996, Final Batch Loss: 0.1300436109304428\n",
      "Epoch 2776, Loss: 0.3213580399751663, Final Batch Loss: 0.14341853559017181\n",
      "Epoch 2777, Loss: 0.312300905585289, Final Batch Loss: 0.17010793089866638\n",
      "Epoch 2778, Loss: 0.2783384323120117, Final Batch Loss: 0.14013603329658508\n",
      "Epoch 2779, Loss: 0.30606842041015625, Final Batch Loss: 0.17119622230529785\n",
      "Epoch 2780, Loss: 0.292418897151947, Final Batch Loss: 0.1301003098487854\n",
      "Epoch 2781, Loss: 0.27228202670812607, Final Batch Loss: 0.10432051867246628\n",
      "Epoch 2782, Loss: 0.2670452445745468, Final Batch Loss: 0.12198033928871155\n",
      "Epoch 2783, Loss: 0.2911040484905243, Final Batch Loss: 0.11774507164955139\n",
      "Epoch 2784, Loss: 0.3050849288702011, Final Batch Loss: 0.1483611762523651\n",
      "Epoch 2785, Loss: 0.24313543736934662, Final Batch Loss: 0.08636918663978577\n",
      "Epoch 2786, Loss: 0.24517906457185745, Final Batch Loss: 0.11689253896474838\n",
      "Epoch 2787, Loss: 0.26283134520053864, Final Batch Loss: 0.1531728357076645\n",
      "Epoch 2788, Loss: 0.329131618142128, Final Batch Loss: 0.1287710815668106\n",
      "Epoch 2789, Loss: 0.25295648723840714, Final Batch Loss: 0.11015959829092026\n",
      "Epoch 2790, Loss: 0.29031386226415634, Final Batch Loss: 0.11370114237070084\n",
      "Epoch 2791, Loss: 0.3615281879901886, Final Batch Loss: 0.2068191021680832\n",
      "Epoch 2792, Loss: 0.3802323043346405, Final Batch Loss: 0.2281319797039032\n",
      "Epoch 2793, Loss: 0.34885162115097046, Final Batch Loss: 0.1802932173013687\n",
      "Epoch 2794, Loss: 0.34847258776426315, Final Batch Loss: 0.2276792675256729\n",
      "Epoch 2795, Loss: 0.2883061021566391, Final Batch Loss: 0.133427232503891\n",
      "Epoch 2796, Loss: 0.26950059831142426, Final Batch Loss: 0.14054355025291443\n",
      "Epoch 2797, Loss: 0.333550363779068, Final Batch Loss: 0.13407737016677856\n",
      "Epoch 2798, Loss: 0.30195046961307526, Final Batch Loss: 0.13705004751682281\n",
      "Epoch 2799, Loss: 0.3226466029882431, Final Batch Loss: 0.1649618148803711\n",
      "Epoch 2800, Loss: 0.3136647045612335, Final Batch Loss: 0.1616923063993454\n",
      "Epoch 2801, Loss: 0.2905217781662941, Final Batch Loss: 0.16989845037460327\n",
      "Epoch 2802, Loss: 0.28952617943286896, Final Batch Loss: 0.14854680001735687\n",
      "Epoch 2803, Loss: 0.30501987040042877, Final Batch Loss: 0.17121373116970062\n",
      "Epoch 2804, Loss: 0.3009139746427536, Final Batch Loss: 0.1686742603778839\n",
      "Epoch 2805, Loss: 0.2693065255880356, Final Batch Loss: 0.12416937947273254\n",
      "Epoch 2806, Loss: 0.2913135141134262, Final Batch Loss: 0.12691588699817657\n",
      "Epoch 2807, Loss: 0.29505065083503723, Final Batch Loss: 0.1419333666563034\n",
      "Epoch 2808, Loss: 0.2963918447494507, Final Batch Loss: 0.15508034825325012\n",
      "Epoch 2809, Loss: 0.32022301852703094, Final Batch Loss: 0.16236284375190735\n",
      "Epoch 2810, Loss: 0.33443404734134674, Final Batch Loss: 0.19896602630615234\n",
      "Epoch 2811, Loss: 0.319042906165123, Final Batch Loss: 0.1847817450761795\n",
      "Epoch 2812, Loss: 0.28514234721660614, Final Batch Loss: 0.1280442774295807\n",
      "Epoch 2813, Loss: 0.2548340708017349, Final Batch Loss: 0.13554412126541138\n",
      "Epoch 2814, Loss: 0.303057461977005, Final Batch Loss: 0.1580241322517395\n",
      "Epoch 2815, Loss: 0.3267466872930527, Final Batch Loss: 0.1550518423318863\n",
      "Epoch 2816, Loss: 0.27217962592840195, Final Batch Loss: 0.15834051370620728\n",
      "Epoch 2817, Loss: 0.34219639003276825, Final Batch Loss: 0.18053162097930908\n",
      "Epoch 2818, Loss: 0.3230200782418251, Final Batch Loss: 0.20431426167488098\n",
      "Epoch 2819, Loss: 0.28297190368175507, Final Batch Loss: 0.1498335301876068\n",
      "Epoch 2820, Loss: 0.2814690098166466, Final Batch Loss: 0.15935029089450836\n",
      "Epoch 2821, Loss: 0.25688527524471283, Final Batch Loss: 0.11066405475139618\n",
      "Epoch 2822, Loss: 0.2550911605358124, Final Batch Loss: 0.107149139046669\n",
      "Epoch 2823, Loss: 0.25778303295373917, Final Batch Loss: 0.12254460901021957\n",
      "Epoch 2824, Loss: 0.35846783220767975, Final Batch Loss: 0.17052502930164337\n",
      "Epoch 2825, Loss: 0.2981018051505089, Final Batch Loss: 0.18469880521297455\n",
      "Epoch 2826, Loss: 0.28617067635059357, Final Batch Loss: 0.13242746889591217\n",
      "Epoch 2827, Loss: 0.2933379262685776, Final Batch Loss: 0.1488128900527954\n",
      "Epoch 2828, Loss: 0.2947108745574951, Final Batch Loss: 0.13664311170578003\n",
      "Epoch 2829, Loss: 0.2718532830476761, Final Batch Loss: 0.12717539072036743\n",
      "Epoch 2830, Loss: 0.2685174196958542, Final Batch Loss: 0.1384555846452713\n",
      "Epoch 2831, Loss: 0.41583143174648285, Final Batch Loss: 0.2769816815853119\n",
      "Epoch 2832, Loss: 0.2843865007162094, Final Batch Loss: 0.1407700628042221\n",
      "Epoch 2833, Loss: 0.3419008105993271, Final Batch Loss: 0.19737881422042847\n",
      "Epoch 2834, Loss: 0.3032947778701782, Final Batch Loss: 0.14497345685958862\n",
      "Epoch 2835, Loss: 0.28498779982328415, Final Batch Loss: 0.12026432901620865\n",
      "Epoch 2836, Loss: 0.3253321796655655, Final Batch Loss: 0.14221109449863434\n",
      "Epoch 2837, Loss: 0.30339011549949646, Final Batch Loss: 0.13387231528759003\n",
      "Epoch 2838, Loss: 0.3634660094976425, Final Batch Loss: 0.21460691094398499\n",
      "Epoch 2839, Loss: 0.30392059683799744, Final Batch Loss: 0.16799618303775787\n",
      "Epoch 2840, Loss: 0.2902170717716217, Final Batch Loss: 0.15554021298885345\n",
      "Epoch 2841, Loss: 0.2531799450516701, Final Batch Loss: 0.10442238301038742\n",
      "Epoch 2842, Loss: 0.2955458015203476, Final Batch Loss: 0.14685671031475067\n",
      "Epoch 2843, Loss: 0.2604561820626259, Final Batch Loss: 0.10690820962190628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2844, Loss: 0.2337978109717369, Final Batch Loss: 0.1204199492931366\n",
      "Epoch 2845, Loss: 0.2885045111179352, Final Batch Loss: 0.13056008517742157\n",
      "Epoch 2846, Loss: 0.2739282697439194, Final Batch Loss: 0.13260185718536377\n",
      "Epoch 2847, Loss: 0.2597743794322014, Final Batch Loss: 0.11784828454256058\n",
      "Epoch 2848, Loss: 0.29591136425733566, Final Batch Loss: 0.18971338868141174\n",
      "Epoch 2849, Loss: 0.29583221673965454, Final Batch Loss: 0.15046216547489166\n",
      "Epoch 2850, Loss: 0.24166344106197357, Final Batch Loss: 0.07994581758975983\n",
      "Epoch 2851, Loss: 0.30129027366638184, Final Batch Loss: 0.15176768600940704\n",
      "Epoch 2852, Loss: 0.320721372961998, Final Batch Loss: 0.14801707863807678\n",
      "Epoch 2853, Loss: 0.3048391491174698, Final Batch Loss: 0.16770793497562408\n",
      "Epoch 2854, Loss: 0.2867302745580673, Final Batch Loss: 0.13456493616104126\n",
      "Epoch 2855, Loss: 0.30972687900066376, Final Batch Loss: 0.15948878228664398\n",
      "Epoch 2856, Loss: 0.24975159019231796, Final Batch Loss: 0.11590921133756638\n",
      "Epoch 2857, Loss: 0.26348234713077545, Final Batch Loss: 0.13802117109298706\n",
      "Epoch 2858, Loss: 0.2836160510778427, Final Batch Loss: 0.14320452511310577\n",
      "Epoch 2859, Loss: 0.2900574505329132, Final Batch Loss: 0.13038049638271332\n",
      "Epoch 2860, Loss: 0.28629370033741, Final Batch Loss: 0.10854412615299225\n",
      "Epoch 2861, Loss: 0.2954472303390503, Final Batch Loss: 0.14096176624298096\n",
      "Epoch 2862, Loss: 0.2878119498491287, Final Batch Loss: 0.14690032601356506\n",
      "Epoch 2863, Loss: 0.27502962946891785, Final Batch Loss: 0.12580347061157227\n",
      "Epoch 2864, Loss: 0.302036851644516, Final Batch Loss: 0.17759929597377777\n",
      "Epoch 2865, Loss: 0.29157058894634247, Final Batch Loss: 0.13904304802417755\n",
      "Epoch 2866, Loss: 0.29230761528015137, Final Batch Loss: 0.1567361205816269\n",
      "Epoch 2867, Loss: 0.2811214327812195, Final Batch Loss: 0.1196412742137909\n",
      "Epoch 2868, Loss: 0.2879796177148819, Final Batch Loss: 0.1427241861820221\n",
      "Epoch 2869, Loss: 0.2605542093515396, Final Batch Loss: 0.14006149768829346\n",
      "Epoch 2870, Loss: 0.2580689862370491, Final Batch Loss: 0.12450937181711197\n",
      "Epoch 2871, Loss: 0.23909644037485123, Final Batch Loss: 0.12554875016212463\n",
      "Epoch 2872, Loss: 0.31592319905757904, Final Batch Loss: 0.1396680772304535\n",
      "Epoch 2873, Loss: 0.3064702898263931, Final Batch Loss: 0.16184170544147491\n",
      "Epoch 2874, Loss: 0.2429950162768364, Final Batch Loss: 0.1321599930524826\n",
      "Epoch 2875, Loss: 0.2976827025413513, Final Batch Loss: 0.13959430158138275\n",
      "Epoch 2876, Loss: 0.2708044946193695, Final Batch Loss: 0.15219466388225555\n",
      "Epoch 2877, Loss: 0.3130199760198593, Final Batch Loss: 0.14361129701137543\n",
      "Epoch 2878, Loss: 0.21088732779026031, Final Batch Loss: 0.12451274693012238\n",
      "Epoch 2879, Loss: 0.23691731691360474, Final Batch Loss: 0.11778642237186432\n",
      "Epoch 2880, Loss: 0.3099991977214813, Final Batch Loss: 0.16715040802955627\n",
      "Epoch 2881, Loss: 0.28687460720539093, Final Batch Loss: 0.14492963254451752\n",
      "Epoch 2882, Loss: 0.2623014450073242, Final Batch Loss: 0.1460934281349182\n",
      "Epoch 2883, Loss: 0.2655271589756012, Final Batch Loss: 0.10163149237632751\n",
      "Epoch 2884, Loss: 0.3097008466720581, Final Batch Loss: 0.17573420703411102\n",
      "Epoch 2885, Loss: 0.34082575142383575, Final Batch Loss: 0.19593782722949982\n",
      "Epoch 2886, Loss: 0.2893235608935356, Final Batch Loss: 0.17730745673179626\n",
      "Epoch 2887, Loss: 0.29989902675151825, Final Batch Loss: 0.15721307694911957\n",
      "Epoch 2888, Loss: 0.39782488346099854, Final Batch Loss: 0.2195528745651245\n",
      "Epoch 2889, Loss: 0.2644312083721161, Final Batch Loss: 0.10182175040245056\n",
      "Epoch 2890, Loss: 0.3076707422733307, Final Batch Loss: 0.16160649061203003\n",
      "Epoch 2891, Loss: 0.2561880871653557, Final Batch Loss: 0.11605840176343918\n",
      "Epoch 2892, Loss: 0.29301004111766815, Final Batch Loss: 0.1530075967311859\n",
      "Epoch 2893, Loss: 0.22509948164224625, Final Batch Loss: 0.1012185662984848\n",
      "Epoch 2894, Loss: 0.24445032328367233, Final Batch Loss: 0.10717570036649704\n",
      "Epoch 2895, Loss: 0.2744778096675873, Final Batch Loss: 0.1450778990983963\n",
      "Epoch 2896, Loss: 0.2852378860116005, Final Batch Loss: 0.09650439769029617\n",
      "Epoch 2897, Loss: 0.2838739603757858, Final Batch Loss: 0.1534312516450882\n",
      "Epoch 2898, Loss: 0.2851240783929825, Final Batch Loss: 0.1479097157716751\n",
      "Epoch 2899, Loss: 0.3123372942209244, Final Batch Loss: 0.20200282335281372\n",
      "Epoch 2900, Loss: 0.25401420146226883, Final Batch Loss: 0.10680755227804184\n",
      "Epoch 2901, Loss: 0.25653523206710815, Final Batch Loss: 0.1241973340511322\n",
      "Epoch 2902, Loss: 0.2851681709289551, Final Batch Loss: 0.14644378423690796\n",
      "Epoch 2903, Loss: 0.29748204350471497, Final Batch Loss: 0.15607309341430664\n",
      "Epoch 2904, Loss: 0.382197842001915, Final Batch Loss: 0.19676780700683594\n",
      "Epoch 2905, Loss: 0.26354440301656723, Final Batch Loss: 0.15948307514190674\n",
      "Epoch 2906, Loss: 0.2501673996448517, Final Batch Loss: 0.11092205345630646\n",
      "Epoch 2907, Loss: 0.3309958726167679, Final Batch Loss: 0.1617099791765213\n",
      "Epoch 2908, Loss: 0.2630181089043617, Final Batch Loss: 0.14550234377384186\n",
      "Epoch 2909, Loss: 0.26385293155908585, Final Batch Loss: 0.1466672420501709\n",
      "Epoch 2910, Loss: 0.28346288204193115, Final Batch Loss: 0.13243749737739563\n",
      "Epoch 2911, Loss: 0.2630116790533066, Final Batch Loss: 0.14800257980823517\n",
      "Epoch 2912, Loss: 0.2864399403333664, Final Batch Loss: 0.1723361611366272\n",
      "Epoch 2913, Loss: 0.2842871695756912, Final Batch Loss: 0.15319262444972992\n",
      "Epoch 2914, Loss: 0.2784719616174698, Final Batch Loss: 0.14497685432434082\n",
      "Epoch 2915, Loss: 0.3060823976993561, Final Batch Loss: 0.1333216428756714\n",
      "Epoch 2916, Loss: 0.33757174015045166, Final Batch Loss: 0.1448564976453781\n",
      "Epoch 2917, Loss: 0.271073654294014, Final Batch Loss: 0.13284988701343536\n",
      "Epoch 2918, Loss: 0.3109554201364517, Final Batch Loss: 0.14627324044704437\n",
      "Epoch 2919, Loss: 0.25084055960178375, Final Batch Loss: 0.10539361834526062\n",
      "Epoch 2920, Loss: 0.2697027251124382, Final Batch Loss: 0.11556490510702133\n",
      "Epoch 2921, Loss: 0.26489444077014923, Final Batch Loss: 0.12394312024116516\n",
      "Epoch 2922, Loss: 0.3095850348472595, Final Batch Loss: 0.12921176850795746\n",
      "Epoch 2923, Loss: 0.33770596981048584, Final Batch Loss: 0.19729989767074585\n",
      "Epoch 2924, Loss: 0.27614179253578186, Final Batch Loss: 0.10502974689006805\n",
      "Epoch 2925, Loss: 0.2381427213549614, Final Batch Loss: 0.11740170419216156\n",
      "Epoch 2926, Loss: 0.24818754196166992, Final Batch Loss: 0.08942310512065887\n",
      "Epoch 2927, Loss: 0.22586821764707565, Final Batch Loss: 0.11688558012247086\n",
      "Epoch 2928, Loss: 0.3015143945813179, Final Batch Loss: 0.1821225881576538\n",
      "Epoch 2929, Loss: 0.3022616356611252, Final Batch Loss: 0.16991162300109863\n",
      "Epoch 2930, Loss: 0.31428736448287964, Final Batch Loss: 0.14377814531326294\n",
      "Epoch 2931, Loss: 0.28925395011901855, Final Batch Loss: 0.13884279131889343\n",
      "Epoch 2932, Loss: 0.25819481909275055, Final Batch Loss: 0.12137261033058167\n",
      "Epoch 2933, Loss: 0.30656325817108154, Final Batch Loss: 0.15687969326972961\n",
      "Epoch 2934, Loss: 0.27067938446998596, Final Batch Loss: 0.11533854901790619\n",
      "Epoch 2935, Loss: 0.24192140996456146, Final Batch Loss: 0.14248469471931458\n",
      "Epoch 2936, Loss: 0.29412081837654114, Final Batch Loss: 0.12936335802078247\n",
      "Epoch 2937, Loss: 0.2819179818034172, Final Batch Loss: 0.11276549845933914\n",
      "Epoch 2938, Loss: 0.32240407168865204, Final Batch Loss: 0.1266249120235443\n",
      "Epoch 2939, Loss: 0.3016401082277298, Final Batch Loss: 0.18165381252765656\n",
      "Epoch 2940, Loss: 0.25137505680322647, Final Batch Loss: 0.10359134525060654\n",
      "Epoch 2941, Loss: 0.2906750738620758, Final Batch Loss: 0.15712693333625793\n",
      "Epoch 2942, Loss: 0.3091359883546829, Final Batch Loss: 0.1809064894914627\n",
      "Epoch 2943, Loss: 0.25336892902851105, Final Batch Loss: 0.12519893050193787\n",
      "Epoch 2944, Loss: 0.2796874940395355, Final Batch Loss: 0.15258444845676422\n",
      "Epoch 2945, Loss: 0.2695535868406296, Final Batch Loss: 0.12900248169898987\n",
      "Epoch 2946, Loss: 0.2836278825998306, Final Batch Loss: 0.1373259574174881\n",
      "Epoch 2947, Loss: 0.29376785457134247, Final Batch Loss: 0.1289059817790985\n",
      "Epoch 2948, Loss: 0.2246408462524414, Final Batch Loss: 0.13551484048366547\n",
      "Epoch 2949, Loss: 0.2616647481918335, Final Batch Loss: 0.1112876683473587\n",
      "Epoch 2950, Loss: 0.28082917630672455, Final Batch Loss: 0.1480720341205597\n",
      "Epoch 2951, Loss: 0.26393362879753113, Final Batch Loss: 0.12155982851982117\n",
      "Epoch 2952, Loss: 0.2566498890519142, Final Batch Loss: 0.10492292791604996\n",
      "Epoch 2953, Loss: 0.2831079438328743, Final Batch Loss: 0.17198528349399567\n",
      "Epoch 2954, Loss: 0.2865937724709511, Final Batch Loss: 0.18277151882648468\n",
      "Epoch 2955, Loss: 0.34114399552345276, Final Batch Loss: 0.1884782910346985\n",
      "Epoch 2956, Loss: 0.2537633776664734, Final Batch Loss: 0.12184201180934906\n",
      "Epoch 2957, Loss: 0.26643946021795273, Final Batch Loss: 0.12378797680139542\n",
      "Epoch 2958, Loss: 0.2947308421134949, Final Batch Loss: 0.15500880777835846\n",
      "Epoch 2959, Loss: 0.3151954710483551, Final Batch Loss: 0.1600804328918457\n",
      "Epoch 2960, Loss: 0.31774476170539856, Final Batch Loss: 0.15832741558551788\n",
      "Epoch 2961, Loss: 0.2776701897382736, Final Batch Loss: 0.1660122573375702\n",
      "Epoch 2962, Loss: 0.28296755254268646, Final Batch Loss: 0.15459683537483215\n",
      "Epoch 2963, Loss: 0.25731587409973145, Final Batch Loss: 0.11342829465866089\n",
      "Epoch 2964, Loss: 0.292146772146225, Final Batch Loss: 0.13917075097560883\n",
      "Epoch 2965, Loss: 0.2558113634586334, Final Batch Loss: 0.10702905058860779\n",
      "Epoch 2966, Loss: 0.27138476073741913, Final Batch Loss: 0.14114494621753693\n",
      "Epoch 2967, Loss: 0.2659783810377121, Final Batch Loss: 0.13424809277057648\n",
      "Epoch 2968, Loss: 0.22646577656269073, Final Batch Loss: 0.10217688977718353\n",
      "Epoch 2969, Loss: 0.36598365008831024, Final Batch Loss: 0.2189365029335022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2970, Loss: 0.2305535078048706, Final Batch Loss: 0.12427987903356552\n",
      "Epoch 2971, Loss: 0.2641454190015793, Final Batch Loss: 0.13574033975601196\n",
      "Epoch 2972, Loss: 0.27072232961654663, Final Batch Loss: 0.1513647884130478\n",
      "Epoch 2973, Loss: 0.26630476117134094, Final Batch Loss: 0.137608140707016\n",
      "Epoch 2974, Loss: 0.2473583221435547, Final Batch Loss: 0.1177661269903183\n",
      "Epoch 2975, Loss: 0.2797813042998314, Final Batch Loss: 0.09962110966444016\n",
      "Epoch 2976, Loss: 0.2740304321050644, Final Batch Loss: 0.11948814988136292\n",
      "Epoch 2977, Loss: 0.3188185691833496, Final Batch Loss: 0.14866575598716736\n",
      "Epoch 2978, Loss: 0.27397794276475906, Final Batch Loss: 0.11715371161699295\n",
      "Epoch 2979, Loss: 0.25248730927705765, Final Batch Loss: 0.133290097117424\n",
      "Epoch 2980, Loss: 0.26274484395980835, Final Batch Loss: 0.12093973159790039\n",
      "Epoch 2981, Loss: 0.2782032862305641, Final Batch Loss: 0.17698164284229279\n",
      "Epoch 2982, Loss: 0.21434377133846283, Final Batch Loss: 0.08075487613677979\n",
      "Epoch 2983, Loss: 0.23975705355405807, Final Batch Loss: 0.09261811524629593\n",
      "Epoch 2984, Loss: 0.2244659662246704, Final Batch Loss: 0.11082976311445236\n",
      "Epoch 2985, Loss: 0.26545609533786774, Final Batch Loss: 0.15077728033065796\n",
      "Epoch 2986, Loss: 0.31078463047742844, Final Batch Loss: 0.19209688901901245\n",
      "Epoch 2987, Loss: 0.2090991884469986, Final Batch Loss: 0.08811087906360626\n",
      "Epoch 2988, Loss: 0.2975742518901825, Final Batch Loss: 0.12810927629470825\n",
      "Epoch 2989, Loss: 0.2670513466000557, Final Batch Loss: 0.12451568990945816\n",
      "Epoch 2990, Loss: 0.2692222073674202, Final Batch Loss: 0.14846061170101166\n",
      "Epoch 2991, Loss: 0.2841191440820694, Final Batch Loss: 0.15135584771633148\n",
      "Epoch 2992, Loss: 0.2668835371732712, Final Batch Loss: 0.13637828826904297\n",
      "Epoch 2993, Loss: 0.251593716442585, Final Batch Loss: 0.1384289413690567\n",
      "Epoch 2994, Loss: 0.2072913870215416, Final Batch Loss: 0.07434656471014023\n",
      "Epoch 2995, Loss: 0.2762518972158432, Final Batch Loss: 0.15613611042499542\n",
      "Epoch 2996, Loss: 0.2240627035498619, Final Batch Loss: 0.09434490650892258\n",
      "Epoch 2997, Loss: 0.27463939785957336, Final Batch Loss: 0.12548081576824188\n",
      "Epoch 2998, Loss: 0.28135786950588226, Final Batch Loss: 0.15935100615024567\n",
      "Epoch 2999, Loss: 0.2480287179350853, Final Batch Loss: 0.1193111464381218\n",
      "Epoch 3000, Loss: 0.31436845660209656, Final Batch Loss: 0.18663722276687622\n",
      "Epoch 3001, Loss: 0.2705804407596588, Final Batch Loss: 0.13861286640167236\n",
      "Epoch 3002, Loss: 0.2763879746198654, Final Batch Loss: 0.13935713469982147\n",
      "Epoch 3003, Loss: 0.2960512042045593, Final Batch Loss: 0.18338419497013092\n",
      "Epoch 3004, Loss: 0.2864892780780792, Final Batch Loss: 0.1290108561515808\n",
      "Epoch 3005, Loss: 0.31338778138160706, Final Batch Loss: 0.14350703358650208\n",
      "Epoch 3006, Loss: 0.2764337807893753, Final Batch Loss: 0.13961951434612274\n",
      "Epoch 3007, Loss: 0.2259095087647438, Final Batch Loss: 0.08584310859441757\n",
      "Epoch 3008, Loss: 0.3411359339952469, Final Batch Loss: 0.1892584264278412\n",
      "Epoch 3009, Loss: 0.22431664913892746, Final Batch Loss: 0.1121128648519516\n",
      "Epoch 3010, Loss: 0.2770811915397644, Final Batch Loss: 0.17873527109622955\n",
      "Epoch 3011, Loss: 0.28640611469745636, Final Batch Loss: 0.14594730734825134\n",
      "Epoch 3012, Loss: 0.2678529620170593, Final Batch Loss: 0.12522132694721222\n",
      "Epoch 3013, Loss: 0.2679065614938736, Final Batch Loss: 0.1342533826828003\n",
      "Epoch 3014, Loss: 0.3373353034257889, Final Batch Loss: 0.15417395532131195\n",
      "Epoch 3015, Loss: 0.28288915753364563, Final Batch Loss: 0.14025890827178955\n",
      "Epoch 3016, Loss: 0.28528110682964325, Final Batch Loss: 0.14703971147537231\n",
      "Epoch 3017, Loss: 0.21934345364570618, Final Batch Loss: 0.10057797282934189\n",
      "Epoch 3018, Loss: 0.25168487429618835, Final Batch Loss: 0.1198553591966629\n",
      "Epoch 3019, Loss: 0.24129758030176163, Final Batch Loss: 0.11841735243797302\n",
      "Epoch 3020, Loss: 0.2960622236132622, Final Batch Loss: 0.09816896170377731\n",
      "Epoch 3021, Loss: 0.25090672820806503, Final Batch Loss: 0.12469253689050674\n",
      "Epoch 3022, Loss: 0.2595461755990982, Final Batch Loss: 0.14754578471183777\n",
      "Epoch 3023, Loss: 0.3448805510997772, Final Batch Loss: 0.1806427538394928\n",
      "Epoch 3024, Loss: 0.25396323949098587, Final Batch Loss: 0.1374744325876236\n",
      "Epoch 3025, Loss: 0.2127484381198883, Final Batch Loss: 0.09719207137823105\n",
      "Epoch 3026, Loss: 0.230757437646389, Final Batch Loss: 0.09680471569299698\n",
      "Epoch 3027, Loss: 0.23784982413053513, Final Batch Loss: 0.10391519218683243\n",
      "Epoch 3028, Loss: 0.2312093898653984, Final Batch Loss: 0.07947120815515518\n",
      "Epoch 3029, Loss: 0.2515471279621124, Final Batch Loss: 0.12315541505813599\n",
      "Epoch 3030, Loss: 0.2519334629178047, Final Batch Loss: 0.13186416029930115\n",
      "Epoch 3031, Loss: 0.2685225233435631, Final Batch Loss: 0.14446324110031128\n",
      "Epoch 3032, Loss: 0.2560446560382843, Final Batch Loss: 0.11680501699447632\n",
      "Epoch 3033, Loss: 0.25969530642032623, Final Batch Loss: 0.16034209728240967\n",
      "Epoch 3034, Loss: 0.2685723230242729, Final Batch Loss: 0.16247917711734772\n",
      "Epoch 3035, Loss: 0.2546795457601547, Final Batch Loss: 0.10233542323112488\n",
      "Epoch 3036, Loss: 0.2593993619084358, Final Batch Loss: 0.12060489505529404\n",
      "Epoch 3037, Loss: 0.24552276730537415, Final Batch Loss: 0.13837476074695587\n",
      "Epoch 3038, Loss: 0.26160595566034317, Final Batch Loss: 0.16016621887683868\n",
      "Epoch 3039, Loss: 0.29461267590522766, Final Batch Loss: 0.15838152170181274\n",
      "Epoch 3040, Loss: 0.2517828866839409, Final Batch Loss: 0.12718960642814636\n",
      "Epoch 3041, Loss: 0.32531674206256866, Final Batch Loss: 0.1395667940378189\n",
      "Epoch 3042, Loss: 0.2547661066055298, Final Batch Loss: 0.11578792333602905\n",
      "Epoch 3043, Loss: 0.27306152135133743, Final Batch Loss: 0.15341100096702576\n",
      "Epoch 3044, Loss: 0.2598554342985153, Final Batch Loss: 0.13538341224193573\n",
      "Epoch 3045, Loss: 0.22540738433599472, Final Batch Loss: 0.11838413774967194\n",
      "Epoch 3046, Loss: 0.29056644439697266, Final Batch Loss: 0.16521361470222473\n",
      "Epoch 3047, Loss: 0.2594621926546097, Final Batch Loss: 0.13081470131874084\n",
      "Epoch 3048, Loss: 0.2792944014072418, Final Batch Loss: 0.11185647547245026\n",
      "Epoch 3049, Loss: 0.25646139681339264, Final Batch Loss: 0.12104804813861847\n",
      "Epoch 3050, Loss: 0.31263959407806396, Final Batch Loss: 0.13778279721736908\n",
      "Epoch 3051, Loss: 0.2694520726799965, Final Batch Loss: 0.16076502203941345\n",
      "Epoch 3052, Loss: 0.23573055118322372, Final Batch Loss: 0.13227899372577667\n",
      "Epoch 3053, Loss: 0.24612189084291458, Final Batch Loss: 0.118862085044384\n",
      "Epoch 3054, Loss: 0.2438744679093361, Final Batch Loss: 0.1323792189359665\n",
      "Epoch 3055, Loss: 0.27665098011493683, Final Batch Loss: 0.1726003736257553\n",
      "Epoch 3056, Loss: 0.3073713034391403, Final Batch Loss: 0.18897804617881775\n",
      "Epoch 3057, Loss: 0.25457046926021576, Final Batch Loss: 0.15013188123703003\n",
      "Epoch 3058, Loss: 0.293237641453743, Final Batch Loss: 0.16056020557880402\n",
      "Epoch 3059, Loss: 0.26197436451911926, Final Batch Loss: 0.12763182818889618\n",
      "Epoch 3060, Loss: 0.20521272718906403, Final Batch Loss: 0.06515398621559143\n",
      "Epoch 3061, Loss: 0.2810294032096863, Final Batch Loss: 0.11616094410419464\n",
      "Epoch 3062, Loss: 0.2420089691877365, Final Batch Loss: 0.10559229552745819\n",
      "Epoch 3063, Loss: 0.24337252974510193, Final Batch Loss: 0.13360808789730072\n",
      "Epoch 3064, Loss: 0.23792579770088196, Final Batch Loss: 0.10564500093460083\n",
      "Epoch 3065, Loss: 0.25879161059856415, Final Batch Loss: 0.13165464997291565\n",
      "Epoch 3066, Loss: 0.2933463156223297, Final Batch Loss: 0.16904257237911224\n",
      "Epoch 3067, Loss: 0.23805753141641617, Final Batch Loss: 0.10745876282453537\n",
      "Epoch 3068, Loss: 0.3569694757461548, Final Batch Loss: 0.23248626291751862\n",
      "Epoch 3069, Loss: 0.25854355096817017, Final Batch Loss: 0.15364313125610352\n",
      "Epoch 3070, Loss: 0.24379955977201462, Final Batch Loss: 0.10668075829744339\n",
      "Epoch 3071, Loss: 0.23797392845153809, Final Batch Loss: 0.12355668097734451\n",
      "Epoch 3072, Loss: 0.2399892956018448, Final Batch Loss: 0.12030262500047684\n",
      "Epoch 3073, Loss: 0.291794016957283, Final Batch Loss: 0.19573520123958588\n",
      "Epoch 3074, Loss: 0.2835245728492737, Final Batch Loss: 0.13245150446891785\n",
      "Epoch 3075, Loss: 0.267956480383873, Final Batch Loss: 0.12742629647254944\n",
      "Epoch 3076, Loss: 0.27530208230018616, Final Batch Loss: 0.14493092894554138\n",
      "Epoch 3077, Loss: 0.23884087055921555, Final Batch Loss: 0.13683664798736572\n",
      "Epoch 3078, Loss: 0.23719308525323868, Final Batch Loss: 0.09483889490365982\n",
      "Epoch 3079, Loss: 0.26848936080932617, Final Batch Loss: 0.13170698285102844\n",
      "Epoch 3080, Loss: 0.21986408531665802, Final Batch Loss: 0.08727292716503143\n",
      "Epoch 3081, Loss: 0.26763761043548584, Final Batch Loss: 0.13657283782958984\n",
      "Epoch 3082, Loss: 0.2302895113825798, Final Batch Loss: 0.11762002855539322\n",
      "Epoch 3083, Loss: 0.3333093374967575, Final Batch Loss: 0.14785991609096527\n",
      "Epoch 3084, Loss: 0.26727286726236343, Final Batch Loss: 0.14447148144245148\n",
      "Epoch 3085, Loss: 0.23100442439317703, Final Batch Loss: 0.11969762295484543\n",
      "Epoch 3086, Loss: 0.2549597918987274, Final Batch Loss: 0.12704385817050934\n",
      "Epoch 3087, Loss: 0.3097180128097534, Final Batch Loss: 0.2050023376941681\n",
      "Epoch 3088, Loss: 0.23657844215631485, Final Batch Loss: 0.11949271708726883\n",
      "Epoch 3089, Loss: 0.24920561909675598, Final Batch Loss: 0.12766772508621216\n",
      "Epoch 3090, Loss: 0.2708647698163986, Final Batch Loss: 0.14490363001823425\n",
      "Epoch 3091, Loss: 0.25221433490514755, Final Batch Loss: 0.12904198467731476\n",
      "Epoch 3092, Loss: 0.2555644363164902, Final Batch Loss: 0.1362403929233551\n",
      "Epoch 3093, Loss: 0.3179496005177498, Final Batch Loss: 0.11999595910310745\n",
      "Epoch 3094, Loss: 0.2533806189894676, Final Batch Loss: 0.1109158918261528\n",
      "Epoch 3095, Loss: 0.2570333182811737, Final Batch Loss: 0.0971677303314209\n",
      "Epoch 3096, Loss: 0.2701430469751358, Final Batch Loss: 0.144367977976799\n",
      "Epoch 3097, Loss: 0.3231539651751518, Final Batch Loss: 0.21273355185985565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3098, Loss: 0.29420197010040283, Final Batch Loss: 0.1603989601135254\n",
      "Epoch 3099, Loss: 0.31184864044189453, Final Batch Loss: 0.14249156415462494\n",
      "Epoch 3100, Loss: 0.27438458800315857, Final Batch Loss: 0.1064101904630661\n",
      "Epoch 3101, Loss: 0.24986745417118073, Final Batch Loss: 0.13832706212997437\n",
      "Epoch 3102, Loss: 0.24846019595861435, Final Batch Loss: 0.1301662027835846\n",
      "Epoch 3103, Loss: 0.24514739215373993, Final Batch Loss: 0.10307548940181732\n",
      "Epoch 3104, Loss: 0.24299422651529312, Final Batch Loss: 0.14127613604068756\n",
      "Epoch 3105, Loss: 0.2878580689430237, Final Batch Loss: 0.1467197835445404\n",
      "Epoch 3106, Loss: 0.2558753937482834, Final Batch Loss: 0.14271102845668793\n",
      "Epoch 3107, Loss: 0.2599771320819855, Final Batch Loss: 0.11369386315345764\n",
      "Epoch 3108, Loss: 0.2741273269057274, Final Batch Loss: 0.10575204342603683\n",
      "Epoch 3109, Loss: 0.22328174859285355, Final Batch Loss: 0.11557888984680176\n",
      "Epoch 3110, Loss: 0.22392994910478592, Final Batch Loss: 0.10007259994745255\n",
      "Epoch 3111, Loss: 0.26858406513929367, Final Batch Loss: 0.12113694101572037\n",
      "Epoch 3112, Loss: 0.30012358725070953, Final Batch Loss: 0.12377969920635223\n",
      "Epoch 3113, Loss: 0.25175652652978897, Final Batch Loss: 0.11255427449941635\n",
      "Epoch 3114, Loss: 0.2653576135635376, Final Batch Loss: 0.1348240226507187\n",
      "Epoch 3115, Loss: 0.2894609197974205, Final Batch Loss: 0.11142375320196152\n",
      "Epoch 3116, Loss: 0.23765429854393005, Final Batch Loss: 0.11503749340772629\n",
      "Epoch 3117, Loss: 0.2818004637956619, Final Batch Loss: 0.13503189384937286\n",
      "Epoch 3118, Loss: 0.22996465116739273, Final Batch Loss: 0.11335780471563339\n",
      "Epoch 3119, Loss: 0.287135511636734, Final Batch Loss: 0.14983052015304565\n",
      "Epoch 3120, Loss: 0.2032533586025238, Final Batch Loss: 0.08187965303659439\n",
      "Epoch 3121, Loss: 0.2718832194805145, Final Batch Loss: 0.15090329945087433\n",
      "Epoch 3122, Loss: 0.2607361599802971, Final Batch Loss: 0.14404425024986267\n",
      "Epoch 3123, Loss: 0.24025660753250122, Final Batch Loss: 0.12473904341459274\n",
      "Epoch 3124, Loss: 0.28896960616111755, Final Batch Loss: 0.1504254788160324\n",
      "Epoch 3125, Loss: 0.2416481226682663, Final Batch Loss: 0.13169248402118683\n",
      "Epoch 3126, Loss: 0.23211710155010223, Final Batch Loss: 0.13788554072380066\n",
      "Epoch 3127, Loss: 0.21626847237348557, Final Batch Loss: 0.10586538165807724\n",
      "Epoch 3128, Loss: 0.29501328617334366, Final Batch Loss: 0.11190547794103622\n",
      "Epoch 3129, Loss: 0.24950885772705078, Final Batch Loss: 0.1200421005487442\n",
      "Epoch 3130, Loss: 0.28012488037347794, Final Batch Loss: 0.09960994869470596\n",
      "Epoch 3131, Loss: 0.21782360970973969, Final Batch Loss: 0.1076691597700119\n",
      "Epoch 3132, Loss: 0.2725159674882889, Final Batch Loss: 0.15378372371196747\n",
      "Epoch 3133, Loss: 0.28559283912181854, Final Batch Loss: 0.12521517276763916\n",
      "Epoch 3134, Loss: 0.2676430642604828, Final Batch Loss: 0.13453762233257294\n",
      "Epoch 3135, Loss: 0.31606291234493256, Final Batch Loss: 0.14575384557247162\n",
      "Epoch 3136, Loss: 0.22958195954561234, Final Batch Loss: 0.11590469628572464\n",
      "Epoch 3137, Loss: 0.23661968857049942, Final Batch Loss: 0.11590364575386047\n",
      "Epoch 3138, Loss: 0.26621198654174805, Final Batch Loss: 0.1293926239013672\n",
      "Epoch 3139, Loss: 0.25856826454401016, Final Batch Loss: 0.14622581005096436\n",
      "Epoch 3140, Loss: 0.24721717834472656, Final Batch Loss: 0.12531794607639313\n",
      "Epoch 3141, Loss: 0.27645162492990494, Final Batch Loss: 0.17275254428386688\n",
      "Epoch 3142, Loss: 0.28736765682697296, Final Batch Loss: 0.14912277460098267\n",
      "Epoch 3143, Loss: 0.31938573718070984, Final Batch Loss: 0.15228083729743958\n",
      "Epoch 3144, Loss: 0.24588651955127716, Final Batch Loss: 0.11251258850097656\n",
      "Epoch 3145, Loss: 0.25720250606536865, Final Batch Loss: 0.1264275312423706\n",
      "Epoch 3146, Loss: 0.2598682940006256, Final Batch Loss: 0.1225816011428833\n",
      "Epoch 3147, Loss: 0.28582894802093506, Final Batch Loss: 0.13249702751636505\n",
      "Epoch 3148, Loss: 0.2980812191963196, Final Batch Loss: 0.18184322118759155\n",
      "Epoch 3149, Loss: 0.29461053013801575, Final Batch Loss: 0.1282837688922882\n",
      "Epoch 3150, Loss: 0.25983165204524994, Final Batch Loss: 0.13865521550178528\n",
      "Epoch 3151, Loss: 0.25323306769132614, Final Batch Loss: 0.12888213992118835\n",
      "Epoch 3152, Loss: 0.21965737640857697, Final Batch Loss: 0.10832688212394714\n",
      "Epoch 3153, Loss: 0.24964267760515213, Final Batch Loss: 0.10309799760580063\n",
      "Epoch 3154, Loss: 0.2834617793560028, Final Batch Loss: 0.13697469234466553\n",
      "Epoch 3155, Loss: 0.26702867448329926, Final Batch Loss: 0.139840766787529\n",
      "Epoch 3156, Loss: 0.21987982839345932, Final Batch Loss: 0.09123214334249496\n",
      "Epoch 3157, Loss: 0.28369176387786865, Final Batch Loss: 0.16467337310314178\n",
      "Epoch 3158, Loss: 0.30095483362674713, Final Batch Loss: 0.12561990320682526\n",
      "Epoch 3159, Loss: 0.3346804529428482, Final Batch Loss: 0.15772897005081177\n",
      "Epoch 3160, Loss: 0.28733840584754944, Final Batch Loss: 0.1373896300792694\n",
      "Epoch 3161, Loss: 0.3103838413953781, Final Batch Loss: 0.19350439310073853\n",
      "Epoch 3162, Loss: 0.2349763959646225, Final Batch Loss: 0.13544638454914093\n",
      "Epoch 3163, Loss: 0.28371375799179077, Final Batch Loss: 0.15601162612438202\n",
      "Epoch 3164, Loss: 0.30096622556447983, Final Batch Loss: 0.1806560456752777\n",
      "Epoch 3165, Loss: 0.2641728147864342, Final Batch Loss: 0.14257194101810455\n",
      "Epoch 3166, Loss: 0.26115983724594116, Final Batch Loss: 0.10741835832595825\n",
      "Epoch 3167, Loss: 0.2641219273209572, Final Batch Loss: 0.1631045788526535\n",
      "Epoch 3168, Loss: 0.2583833932876587, Final Batch Loss: 0.13261881470680237\n",
      "Epoch 3169, Loss: 0.2882773205637932, Final Batch Loss: 0.16636556386947632\n",
      "Epoch 3170, Loss: 0.24040300399065018, Final Batch Loss: 0.13471239805221558\n",
      "Epoch 3171, Loss: 0.2556748762726784, Final Batch Loss: 0.09917175024747849\n",
      "Epoch 3172, Loss: 0.28805993497371674, Final Batch Loss: 0.13441301882266998\n",
      "Epoch 3173, Loss: 0.23319677263498306, Final Batch Loss: 0.11713553220033646\n",
      "Epoch 3174, Loss: 0.2989223748445511, Final Batch Loss: 0.11778691411018372\n",
      "Epoch 3175, Loss: 0.2514844834804535, Final Batch Loss: 0.14173853397369385\n",
      "Epoch 3176, Loss: 0.2489345669746399, Final Batch Loss: 0.1191929280757904\n",
      "Epoch 3177, Loss: 0.23503968864679337, Final Batch Loss: 0.10735643655061722\n",
      "Epoch 3178, Loss: 0.21626531332731247, Final Batch Loss: 0.0966024398803711\n",
      "Epoch 3179, Loss: 0.27129486203193665, Final Batch Loss: 0.14957770705223083\n",
      "Epoch 3180, Loss: 0.32868528366088867, Final Batch Loss: 0.15809568762779236\n",
      "Epoch 3181, Loss: 0.27547700703144073, Final Batch Loss: 0.14690746366977692\n",
      "Epoch 3182, Loss: 0.241189144551754, Final Batch Loss: 0.11477095633745193\n",
      "Epoch 3183, Loss: 0.2866852656006813, Final Batch Loss: 0.11454006284475327\n",
      "Epoch 3184, Loss: 0.2934756278991699, Final Batch Loss: 0.15488187968730927\n",
      "Epoch 3185, Loss: 0.22395356744527817, Final Batch Loss: 0.12352851033210754\n",
      "Epoch 3186, Loss: 0.24724441766738892, Final Batch Loss: 0.131887286901474\n",
      "Epoch 3187, Loss: 0.2512591853737831, Final Batch Loss: 0.10248466581106186\n",
      "Epoch 3188, Loss: 0.23500128835439682, Final Batch Loss: 0.13778716325759888\n",
      "Epoch 3189, Loss: 0.25029652565717697, Final Batch Loss: 0.13952265679836273\n",
      "Epoch 3190, Loss: 0.27196503430604935, Final Batch Loss: 0.14877866208553314\n",
      "Epoch 3191, Loss: 0.3584231287240982, Final Batch Loss: 0.21767202019691467\n",
      "Epoch 3192, Loss: 0.2188824862241745, Final Batch Loss: 0.11647511273622513\n",
      "Epoch 3193, Loss: 0.23407067358493805, Final Batch Loss: 0.12259805202484131\n",
      "Epoch 3194, Loss: 0.24386167526245117, Final Batch Loss: 0.1552201807498932\n",
      "Epoch 3195, Loss: 0.1948406994342804, Final Batch Loss: 0.08975080400705338\n",
      "Epoch 3196, Loss: 0.2721334397792816, Final Batch Loss: 0.10794752836227417\n",
      "Epoch 3197, Loss: 0.2469692975282669, Final Batch Loss: 0.11896315217018127\n",
      "Epoch 3198, Loss: 0.29544976353645325, Final Batch Loss: 0.13700665533542633\n",
      "Epoch 3199, Loss: 0.3356063961982727, Final Batch Loss: 0.19993144273757935\n",
      "Epoch 3200, Loss: 0.2616164907813072, Final Batch Loss: 0.16072750091552734\n",
      "Epoch 3201, Loss: 0.3072138577699661, Final Batch Loss: 0.19821509718894958\n",
      "Epoch 3202, Loss: 0.2596338093280792, Final Batch Loss: 0.1105833351612091\n",
      "Epoch 3203, Loss: 0.30286500602960587, Final Batch Loss: 0.18722933530807495\n",
      "Epoch 3204, Loss: 0.21234624087810516, Final Batch Loss: 0.09940155595541\n",
      "Epoch 3205, Loss: 0.2528339698910713, Final Batch Loss: 0.1085529550909996\n",
      "Epoch 3206, Loss: 0.2359720915555954, Final Batch Loss: 0.1479138880968094\n",
      "Epoch 3207, Loss: 0.2578719034790993, Final Batch Loss: 0.13507016003131866\n",
      "Epoch 3208, Loss: 0.3074226379394531, Final Batch Loss: 0.14879582822322845\n",
      "Epoch 3209, Loss: 0.3009233921766281, Final Batch Loss: 0.17053276300430298\n",
      "Epoch 3210, Loss: 0.3333708941936493, Final Batch Loss: 0.16300614178180695\n",
      "Epoch 3211, Loss: 0.2114078626036644, Final Batch Loss: 0.08813528716564178\n",
      "Epoch 3212, Loss: 0.22055324912071228, Final Batch Loss: 0.11282938718795776\n",
      "Epoch 3213, Loss: 0.237403504550457, Final Batch Loss: 0.11405275017023087\n",
      "Epoch 3214, Loss: 0.23126216977834702, Final Batch Loss: 0.08797302097082138\n",
      "Epoch 3215, Loss: 0.24658644944429398, Final Batch Loss: 0.14826376736164093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3216, Loss: 0.21535959094762802, Final Batch Loss: 0.0787290558218956\n",
      "Epoch 3217, Loss: 0.25551218539476395, Final Batch Loss: 0.11934848874807358\n",
      "Epoch 3218, Loss: 0.3018571734428406, Final Batch Loss: 0.14288437366485596\n",
      "Epoch 3219, Loss: 0.22982949763536453, Final Batch Loss: 0.10126448422670364\n",
      "Epoch 3220, Loss: 0.2727808430790901, Final Batch Loss: 0.10941248387098312\n",
      "Epoch 3221, Loss: 0.18996308743953705, Final Batch Loss: 0.07351969182491302\n",
      "Epoch 3222, Loss: 0.29571542143821716, Final Batch Loss: 0.13195323944091797\n",
      "Epoch 3223, Loss: 0.3180798590183258, Final Batch Loss: 0.18323029577732086\n",
      "Epoch 3224, Loss: 0.26325100660324097, Final Batch Loss: 0.12583160400390625\n",
      "Epoch 3225, Loss: 0.22714097797870636, Final Batch Loss: 0.11330381035804749\n",
      "Epoch 3226, Loss: 0.2724307030439377, Final Batch Loss: 0.14632056653499603\n",
      "Epoch 3227, Loss: 0.24347363412380219, Final Batch Loss: 0.10695330798625946\n",
      "Epoch 3228, Loss: 0.2590608075261116, Final Batch Loss: 0.1699606329202652\n",
      "Epoch 3229, Loss: 0.26540670543909073, Final Batch Loss: 0.14715449512004852\n",
      "Epoch 3230, Loss: 0.36333876848220825, Final Batch Loss: 0.18617835640907288\n",
      "Epoch 3231, Loss: 0.21334991604089737, Final Batch Loss: 0.101971335709095\n",
      "Epoch 3232, Loss: 0.1983209103345871, Final Batch Loss: 0.09181351214647293\n",
      "Epoch 3233, Loss: 0.23824501782655716, Final Batch Loss: 0.09468723088502884\n",
      "Epoch 3234, Loss: 0.24530509859323502, Final Batch Loss: 0.1304480880498886\n",
      "Epoch 3235, Loss: 0.2865423113107681, Final Batch Loss: 0.14468346536159515\n",
      "Epoch 3236, Loss: 0.2374402955174446, Final Batch Loss: 0.11261346936225891\n",
      "Epoch 3237, Loss: 0.26784294843673706, Final Batch Loss: 0.13117654621601105\n",
      "Epoch 3238, Loss: 0.25211191922426224, Final Batch Loss: 0.12234882265329361\n",
      "Epoch 3239, Loss: 0.2601569667458534, Final Batch Loss: 0.1552734076976776\n",
      "Epoch 3240, Loss: 0.2466064617037773, Final Batch Loss: 0.11899714916944504\n",
      "Epoch 3241, Loss: 0.2090751677751541, Final Batch Loss: 0.1176193356513977\n",
      "Epoch 3242, Loss: 0.2788507789373398, Final Batch Loss: 0.1420747935771942\n",
      "Epoch 3243, Loss: 0.26893242448568344, Final Batch Loss: 0.1560702919960022\n",
      "Epoch 3244, Loss: 0.23340027779340744, Final Batch Loss: 0.1129470244050026\n",
      "Epoch 3245, Loss: 0.24842090904712677, Final Batch Loss: 0.1147213727235794\n",
      "Epoch 3246, Loss: 0.2567734867334366, Final Batch Loss: 0.12755008041858673\n",
      "Epoch 3247, Loss: 0.23225785046815872, Final Batch Loss: 0.13916361331939697\n",
      "Epoch 3248, Loss: 0.2627294510602951, Final Batch Loss: 0.14147701859474182\n",
      "Epoch 3249, Loss: 0.28279533982276917, Final Batch Loss: 0.13599641621112823\n",
      "Epoch 3250, Loss: 0.23983841389417648, Final Batch Loss: 0.12161475419998169\n",
      "Epoch 3251, Loss: 0.28548139333724976, Final Batch Loss: 0.16100125014781952\n",
      "Epoch 3252, Loss: 0.26091718673706055, Final Batch Loss: 0.14334532618522644\n",
      "Epoch 3253, Loss: 0.21411942690610886, Final Batch Loss: 0.09600580483675003\n",
      "Epoch 3254, Loss: 0.2613557279109955, Final Batch Loss: 0.14049047231674194\n",
      "Epoch 3255, Loss: 0.24506281316280365, Final Batch Loss: 0.11617331206798553\n",
      "Epoch 3256, Loss: 0.1876511052250862, Final Batch Loss: 0.07591170072555542\n",
      "Epoch 3257, Loss: 0.2586375027894974, Final Batch Loss: 0.13848759233951569\n",
      "Epoch 3258, Loss: 0.2820351719856262, Final Batch Loss: 0.14399589598178864\n",
      "Epoch 3259, Loss: 0.24684158712625504, Final Batch Loss: 0.1373225301504135\n",
      "Epoch 3260, Loss: 0.242742620408535, Final Batch Loss: 0.12597373127937317\n",
      "Epoch 3261, Loss: 0.2893781289458275, Final Batch Loss: 0.17086052894592285\n",
      "Epoch 3262, Loss: 0.2728804498910904, Final Batch Loss: 0.10853436589241028\n",
      "Epoch 3263, Loss: 0.26634517312049866, Final Batch Loss: 0.10338419675827026\n",
      "Epoch 3264, Loss: 0.28973914682865143, Final Batch Loss: 0.09127789735794067\n",
      "Epoch 3265, Loss: 0.20948900282382965, Final Batch Loss: 0.10918457806110382\n",
      "Epoch 3266, Loss: 0.2566322982311249, Final Batch Loss: 0.1253126859664917\n",
      "Epoch 3267, Loss: 0.248509980738163, Final Batch Loss: 0.10569778829813004\n",
      "Epoch 3268, Loss: 0.28242718428373337, Final Batch Loss: 0.18943649530410767\n",
      "Epoch 3269, Loss: 0.3146751672029495, Final Batch Loss: 0.18713536858558655\n",
      "Epoch 3270, Loss: 0.2448248639702797, Final Batch Loss: 0.1182500496506691\n",
      "Epoch 3271, Loss: 0.28111206740140915, Final Batch Loss: 0.17060379683971405\n",
      "Epoch 3272, Loss: 0.22717420756816864, Final Batch Loss: 0.10232487320899963\n",
      "Epoch 3273, Loss: 0.2552799582481384, Final Batch Loss: 0.14017640054225922\n",
      "Epoch 3274, Loss: 0.30129601061344147, Final Batch Loss: 0.16361047327518463\n",
      "Epoch 3275, Loss: 0.26741746068000793, Final Batch Loss: 0.14500625431537628\n",
      "Epoch 3276, Loss: 0.2666499391198158, Final Batch Loss: 0.15319174528121948\n",
      "Epoch 3277, Loss: 0.2654557228088379, Final Batch Loss: 0.14316843450069427\n",
      "Epoch 3278, Loss: 0.2117239534854889, Final Batch Loss: 0.07636064291000366\n",
      "Epoch 3279, Loss: 0.23748420923948288, Final Batch Loss: 0.10640909522771835\n",
      "Epoch 3280, Loss: 0.23321852087974548, Final Batch Loss: 0.12054409831762314\n",
      "Epoch 3281, Loss: 0.3331824690103531, Final Batch Loss: 0.20585773885250092\n",
      "Epoch 3282, Loss: 0.28517964482307434, Final Batch Loss: 0.179026797413826\n",
      "Epoch 3283, Loss: 0.2571514993906021, Final Batch Loss: 0.14168743789196014\n",
      "Epoch 3284, Loss: 0.2899024188518524, Final Batch Loss: 0.13597917556762695\n",
      "Epoch 3285, Loss: 0.28595634549856186, Final Batch Loss: 0.18113841116428375\n",
      "Epoch 3286, Loss: 0.21837002038955688, Final Batch Loss: 0.07727640867233276\n",
      "Epoch 3287, Loss: 0.25778745859861374, Final Batch Loss: 0.14891839027404785\n",
      "Epoch 3288, Loss: 0.2912948802113533, Final Batch Loss: 0.17072512209415436\n",
      "Epoch 3289, Loss: 0.28980495035648346, Final Batch Loss: 0.16267289221286774\n",
      "Epoch 3290, Loss: 0.2405996471643448, Final Batch Loss: 0.1277153640985489\n",
      "Epoch 3291, Loss: 0.2261148765683174, Final Batch Loss: 0.11576154083013535\n",
      "Epoch 3292, Loss: 0.2516968250274658, Final Batch Loss: 0.12956203520298004\n",
      "Epoch 3293, Loss: 0.2705914303660393, Final Batch Loss: 0.16633199155330658\n",
      "Epoch 3294, Loss: 0.22555671632289886, Final Batch Loss: 0.10879389941692352\n",
      "Epoch 3295, Loss: 0.20722917467355728, Final Batch Loss: 0.10629063099622726\n",
      "Epoch 3296, Loss: 0.26886483281850815, Final Batch Loss: 0.15352238714694977\n",
      "Epoch 3297, Loss: 0.22908363491296768, Final Batch Loss: 0.08150770515203476\n",
      "Epoch 3298, Loss: 0.3168087303638458, Final Batch Loss: 0.1434182971715927\n",
      "Epoch 3299, Loss: 0.2724742814898491, Final Batch Loss: 0.14792872965335846\n",
      "Epoch 3300, Loss: 0.20962556451559067, Final Batch Loss: 0.06501074880361557\n",
      "Epoch 3301, Loss: 0.26096319407224655, Final Batch Loss: 0.14851047098636627\n",
      "Epoch 3302, Loss: 0.2539312392473221, Final Batch Loss: 0.11664268374443054\n",
      "Epoch 3303, Loss: 0.22114457190036774, Final Batch Loss: 0.11093944311141968\n",
      "Epoch 3304, Loss: 0.2679474800825119, Final Batch Loss: 0.1430301070213318\n",
      "Epoch 3305, Loss: 0.21382832527160645, Final Batch Loss: 0.09229276329278946\n",
      "Epoch 3306, Loss: 0.3174455761909485, Final Batch Loss: 0.1514233946800232\n",
      "Epoch 3307, Loss: 0.2525963559746742, Final Batch Loss: 0.155660018324852\n",
      "Epoch 3308, Loss: 0.27011600136756897, Final Batch Loss: 0.12224164605140686\n",
      "Epoch 3309, Loss: 0.2644466161727905, Final Batch Loss: 0.1528552770614624\n",
      "Epoch 3310, Loss: 0.2762286067008972, Final Batch Loss: 0.1620243638753891\n",
      "Epoch 3311, Loss: 0.2886578440666199, Final Batch Loss: 0.1278640478849411\n",
      "Epoch 3312, Loss: 0.2751503735780716, Final Batch Loss: 0.1356622725725174\n",
      "Epoch 3313, Loss: 0.2881533205509186, Final Batch Loss: 0.16346296668052673\n",
      "Epoch 3314, Loss: 0.3090475723147392, Final Batch Loss: 0.18728092312812805\n",
      "Epoch 3315, Loss: 0.30472661554813385, Final Batch Loss: 0.16983775794506073\n",
      "Epoch 3316, Loss: 0.195398211479187, Final Batch Loss: 0.09950807690620422\n",
      "Epoch 3317, Loss: 0.22971343994140625, Final Batch Loss: 0.11466790735721588\n",
      "Epoch 3318, Loss: 0.26653970777988434, Final Batch Loss: 0.14089389145374298\n",
      "Epoch 3319, Loss: 0.2659260332584381, Final Batch Loss: 0.13974320888519287\n",
      "Epoch 3320, Loss: 0.20173438638448715, Final Batch Loss: 0.08188667893409729\n",
      "Epoch 3321, Loss: 0.2215292826294899, Final Batch Loss: 0.09513462334871292\n",
      "Epoch 3322, Loss: 0.2773546651005745, Final Batch Loss: 0.11597288399934769\n",
      "Epoch 3323, Loss: 0.24638938158750534, Final Batch Loss: 0.1386658102273941\n",
      "Epoch 3324, Loss: 0.258812315762043, Final Batch Loss: 0.11914848536252975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3325, Loss: 0.251257948577404, Final Batch Loss: 0.12076743692159653\n",
      "Epoch 3326, Loss: 0.3317928612232208, Final Batch Loss: 0.18837209045886993\n",
      "Epoch 3327, Loss: 0.2026056945323944, Final Batch Loss: 0.12109028548002243\n",
      "Epoch 3328, Loss: 0.2528756558895111, Final Batch Loss: 0.1319226324558258\n",
      "Epoch 3329, Loss: 0.2693665996193886, Final Batch Loss: 0.1488291621208191\n",
      "Epoch 3330, Loss: 0.25847794115543365, Final Batch Loss: 0.13976715505123138\n",
      "Epoch 3331, Loss: 0.2546581104397774, Final Batch Loss: 0.08408258110284805\n",
      "Epoch 3332, Loss: 0.23761235922574997, Final Batch Loss: 0.10406646877527237\n",
      "Epoch 3333, Loss: 0.2684561759233475, Final Batch Loss: 0.13530340790748596\n",
      "Epoch 3334, Loss: 0.22594548016786575, Final Batch Loss: 0.10578084737062454\n",
      "Epoch 3335, Loss: 0.21433934569358826, Final Batch Loss: 0.08014795184135437\n",
      "Epoch 3336, Loss: 0.25125790387392044, Final Batch Loss: 0.1533234566450119\n",
      "Epoch 3337, Loss: 0.25883473455905914, Final Batch Loss: 0.12137724459171295\n",
      "Epoch 3338, Loss: 0.2516399398446083, Final Batch Loss: 0.13922472298145294\n",
      "Epoch 3339, Loss: 0.24450936540961266, Final Batch Loss: 0.060248102992773056\n",
      "Epoch 3340, Loss: 0.25594042241573334, Final Batch Loss: 0.09498322010040283\n",
      "Epoch 3341, Loss: 0.221139058470726, Final Batch Loss: 0.11550293117761612\n",
      "Epoch 3342, Loss: 0.2595975771546364, Final Batch Loss: 0.10630553215742111\n",
      "Epoch 3343, Loss: 0.2822297364473343, Final Batch Loss: 0.2006445974111557\n",
      "Epoch 3344, Loss: 0.2488565295934677, Final Batch Loss: 0.14972218871116638\n",
      "Epoch 3345, Loss: 0.267987959086895, Final Batch Loss: 0.11529224365949631\n",
      "Epoch 3346, Loss: 0.25779367983341217, Final Batch Loss: 0.1207210123538971\n",
      "Epoch 3347, Loss: 0.2403424084186554, Final Batch Loss: 0.0835929811000824\n",
      "Epoch 3348, Loss: 0.2819294333457947, Final Batch Loss: 0.1489139348268509\n",
      "Epoch 3349, Loss: 0.2305963933467865, Final Batch Loss: 0.1424858421087265\n",
      "Epoch 3350, Loss: 0.16412727162241936, Final Batch Loss: 0.04592746123671532\n",
      "Epoch 3351, Loss: 0.2553675025701523, Final Batch Loss: 0.11755229532718658\n",
      "Epoch 3352, Loss: 0.23658441752195358, Final Batch Loss: 0.10848469287157059\n",
      "Epoch 3353, Loss: 0.2769288197159767, Final Batch Loss: 0.15480276942253113\n",
      "Epoch 3354, Loss: 0.2203003466129303, Final Batch Loss: 0.10553324967622757\n",
      "Epoch 3355, Loss: 0.18539465963840485, Final Batch Loss: 0.08678717166185379\n",
      "Epoch 3356, Loss: 0.21703293919563293, Final Batch Loss: 0.077040895819664\n",
      "Epoch 3357, Loss: 0.24922280758619308, Final Batch Loss: 0.14178141951560974\n",
      "Epoch 3358, Loss: 0.229206845164299, Final Batch Loss: 0.1300680786371231\n",
      "Epoch 3359, Loss: 0.20411114394664764, Final Batch Loss: 0.095317542552948\n",
      "Epoch 3360, Loss: 0.23931865394115448, Final Batch Loss: 0.10483777523040771\n",
      "Epoch 3361, Loss: 0.21222838014364243, Final Batch Loss: 0.1160786971449852\n",
      "Epoch 3362, Loss: 0.2649134397506714, Final Batch Loss: 0.15220214426517487\n",
      "Epoch 3363, Loss: 0.22539978474378586, Final Batch Loss: 0.11290908604860306\n",
      "Epoch 3364, Loss: 0.2622168958187103, Final Batch Loss: 0.15498760342597961\n",
      "Epoch 3365, Loss: 0.2139487937092781, Final Batch Loss: 0.11824754625558853\n",
      "Epoch 3366, Loss: 0.24461611360311508, Final Batch Loss: 0.1156037375330925\n",
      "Epoch 3367, Loss: 0.2933100089430809, Final Batch Loss: 0.17841677367687225\n",
      "Epoch 3368, Loss: 0.23148585110902786, Final Batch Loss: 0.12445472925901413\n",
      "Epoch 3369, Loss: 0.2858062833547592, Final Batch Loss: 0.14311130344867706\n",
      "Epoch 3370, Loss: 0.3158971443772316, Final Batch Loss: 0.10981272906064987\n",
      "Epoch 3371, Loss: 0.23524628579616547, Final Batch Loss: 0.09198139607906342\n",
      "Epoch 3372, Loss: 0.2791140079498291, Final Batch Loss: 0.14267799258232117\n",
      "Epoch 3373, Loss: 0.24332349747419357, Final Batch Loss: 0.1255384385585785\n",
      "Epoch 3374, Loss: 0.23665162920951843, Final Batch Loss: 0.13676820695400238\n",
      "Epoch 3375, Loss: 0.2673647701740265, Final Batch Loss: 0.12046805024147034\n",
      "Epoch 3376, Loss: 0.2140144556760788, Final Batch Loss: 0.09830216318368912\n",
      "Epoch 3377, Loss: 0.2894167900085449, Final Batch Loss: 0.12645193934440613\n",
      "Epoch 3378, Loss: 0.24231649935245514, Final Batch Loss: 0.12083200365304947\n",
      "Epoch 3379, Loss: 0.23949133604764938, Final Batch Loss: 0.08994892984628677\n",
      "Epoch 3380, Loss: 0.2431902512907982, Final Batch Loss: 0.13429191708564758\n",
      "Epoch 3381, Loss: 0.2559836506843567, Final Batch Loss: 0.145174041390419\n",
      "Epoch 3382, Loss: 0.2098931446671486, Final Batch Loss: 0.11131076514720917\n",
      "Epoch 3383, Loss: 0.2525852844119072, Final Batch Loss: 0.11778546124696732\n",
      "Epoch 3384, Loss: 0.22308985888957977, Final Batch Loss: 0.08333539962768555\n",
      "Epoch 3385, Loss: 0.2353254184126854, Final Batch Loss: 0.08545439690351486\n",
      "Epoch 3386, Loss: 0.218806192278862, Final Batch Loss: 0.1250760406255722\n",
      "Epoch 3387, Loss: 0.28097015619277954, Final Batch Loss: 0.1531773954629898\n",
      "Epoch 3388, Loss: 0.2073514610528946, Final Batch Loss: 0.10146509855985641\n",
      "Epoch 3389, Loss: 0.23491153120994568, Final Batch Loss: 0.08417613804340363\n",
      "Epoch 3390, Loss: 0.2144293487071991, Final Batch Loss: 0.09243295341730118\n",
      "Epoch 3391, Loss: 0.2234906256198883, Final Batch Loss: 0.09641243517398834\n",
      "Epoch 3392, Loss: 0.22007419914007187, Final Batch Loss: 0.1056201159954071\n",
      "Epoch 3393, Loss: 0.26545093953609467, Final Batch Loss: 0.14607255160808563\n",
      "Epoch 3394, Loss: 0.26427289843559265, Final Batch Loss: 0.11884909868240356\n",
      "Epoch 3395, Loss: 0.26264244318008423, Final Batch Loss: 0.17259928584098816\n",
      "Epoch 3396, Loss: 0.2853095456957817, Final Batch Loss: 0.1878824084997177\n",
      "Epoch 3397, Loss: 0.3028261661529541, Final Batch Loss: 0.15002164244651794\n",
      "Epoch 3398, Loss: 0.2392127364873886, Final Batch Loss: 0.11132550239562988\n",
      "Epoch 3399, Loss: 0.2523423582315445, Final Batch Loss: 0.11749657988548279\n",
      "Epoch 3400, Loss: 0.24998529255390167, Final Batch Loss: 0.1084427535533905\n",
      "Epoch 3401, Loss: 0.29935456812381744, Final Batch Loss: 0.19201093912124634\n",
      "Epoch 3402, Loss: 0.2554258853197098, Final Batch Loss: 0.11662083864212036\n",
      "Epoch 3403, Loss: 0.22905011475086212, Final Batch Loss: 0.1378513127565384\n",
      "Epoch 3404, Loss: 0.23671665787696838, Final Batch Loss: 0.10791443288326263\n",
      "Epoch 3405, Loss: 0.21607888489961624, Final Batch Loss: 0.07377871125936508\n",
      "Epoch 3406, Loss: 0.23683709651231766, Final Batch Loss: 0.12616828083992004\n",
      "Epoch 3407, Loss: 0.30511850118637085, Final Batch Loss: 0.20219719409942627\n",
      "Epoch 3408, Loss: 0.18940692394971848, Final Batch Loss: 0.09052668511867523\n",
      "Epoch 3409, Loss: 0.25106725841760635, Final Batch Loss: 0.12363088876008987\n",
      "Epoch 3410, Loss: 0.24561630934476852, Final Batch Loss: 0.12885315716266632\n",
      "Epoch 3411, Loss: 0.25236310064792633, Final Batch Loss: 0.1385101079940796\n",
      "Epoch 3412, Loss: 0.22420219331979752, Final Batch Loss: 0.10668639093637466\n",
      "Epoch 3413, Loss: 0.22958700358867645, Final Batch Loss: 0.11409252136945724\n",
      "Epoch 3414, Loss: 0.23468193411827087, Final Batch Loss: 0.14127513766288757\n",
      "Epoch 3415, Loss: 0.24996836483478546, Final Batch Loss: 0.1263776570558548\n",
      "Epoch 3416, Loss: 0.24799370020627975, Final Batch Loss: 0.14191778004169464\n",
      "Epoch 3417, Loss: 0.24778635054826736, Final Batch Loss: 0.137657031416893\n",
      "Epoch 3418, Loss: 0.26544239372015, Final Batch Loss: 0.17789492011070251\n",
      "Epoch 3419, Loss: 0.2575627267360687, Final Batch Loss: 0.14996546506881714\n",
      "Epoch 3420, Loss: 0.22300967574119568, Final Batch Loss: 0.12155032157897949\n",
      "Epoch 3421, Loss: 0.24904140830039978, Final Batch Loss: 0.12754468619823456\n",
      "Epoch 3422, Loss: 0.23626475036144257, Final Batch Loss: 0.1346375048160553\n",
      "Epoch 3423, Loss: 0.3125748038291931, Final Batch Loss: 0.15225645899772644\n",
      "Epoch 3424, Loss: 0.24365386366844177, Final Batch Loss: 0.12240346521139145\n",
      "Epoch 3425, Loss: 0.22002407163381577, Final Batch Loss: 0.10668481141328812\n",
      "Epoch 3426, Loss: 0.2751743048429489, Final Batch Loss: 0.14709517359733582\n",
      "Epoch 3427, Loss: 0.25622357428073883, Final Batch Loss: 0.14993752539157867\n",
      "Epoch 3428, Loss: 0.2630769610404968, Final Batch Loss: 0.12919849157333374\n",
      "Epoch 3429, Loss: 0.27940334379673004, Final Batch Loss: 0.12971119582653046\n",
      "Epoch 3430, Loss: 0.22155261784791946, Final Batch Loss: 0.101016566157341\n",
      "Epoch 3431, Loss: 0.28996361792087555, Final Batch Loss: 0.14318551123142242\n",
      "Epoch 3432, Loss: 0.19811547547578812, Final Batch Loss: 0.0775015726685524\n",
      "Epoch 3433, Loss: 0.23071268945932388, Final Batch Loss: 0.11643379926681519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3434, Loss: 0.20153772085905075, Final Batch Loss: 0.10183528065681458\n",
      "Epoch 3435, Loss: 0.25367075949907303, Final Batch Loss: 0.15389877557754517\n",
      "Epoch 3436, Loss: 0.3387371152639389, Final Batch Loss: 0.15284091234207153\n",
      "Epoch 3437, Loss: 0.2610730677843094, Final Batch Loss: 0.11957691609859467\n",
      "Epoch 3438, Loss: 0.25446347892284393, Final Batch Loss: 0.12355631589889526\n",
      "Epoch 3439, Loss: 0.22959070652723312, Final Batch Loss: 0.11812257021665573\n",
      "Epoch 3440, Loss: 0.22183354198932648, Final Batch Loss: 0.12606628239154816\n",
      "Epoch 3441, Loss: 0.22472868114709854, Final Batch Loss: 0.08226452022790909\n",
      "Epoch 3442, Loss: 0.2198115736246109, Final Batch Loss: 0.07231065630912781\n",
      "Epoch 3443, Loss: 0.197606660425663, Final Batch Loss: 0.09987127035856247\n",
      "Epoch 3444, Loss: 0.2902669087052345, Final Batch Loss: 0.20062141120433807\n",
      "Epoch 3445, Loss: 0.26199765503406525, Final Batch Loss: 0.13964587450027466\n",
      "Epoch 3446, Loss: 0.2463138848543167, Final Batch Loss: 0.12165281176567078\n",
      "Epoch 3447, Loss: 0.2902255579829216, Final Batch Loss: 0.17008689045906067\n",
      "Epoch 3448, Loss: 0.23213818669319153, Final Batch Loss: 0.08841848373413086\n",
      "Epoch 3449, Loss: 0.24201036989688873, Final Batch Loss: 0.13096588850021362\n",
      "Epoch 3450, Loss: 0.2509119138121605, Final Batch Loss: 0.12820680439472198\n",
      "Epoch 3451, Loss: 0.2775043398141861, Final Batch Loss: 0.14722949266433716\n",
      "Epoch 3452, Loss: 0.2135242335498333, Final Batch Loss: 0.05987430736422539\n",
      "Epoch 3453, Loss: 0.22916138172149658, Final Batch Loss: 0.11257987469434738\n",
      "Epoch 3454, Loss: 0.26159126311540604, Final Batch Loss: 0.14461615681648254\n",
      "Epoch 3455, Loss: 0.19951574504375458, Final Batch Loss: 0.07515069842338562\n",
      "Epoch 3456, Loss: 0.4216349273920059, Final Batch Loss: 0.18776604533195496\n",
      "Epoch 3457, Loss: 0.21500445157289505, Final Batch Loss: 0.09596419334411621\n",
      "Epoch 3458, Loss: 0.24281232059001923, Final Batch Loss: 0.13111542165279388\n",
      "Epoch 3459, Loss: 0.18332519382238388, Final Batch Loss: 0.08422572165727615\n",
      "Epoch 3460, Loss: 0.2777668535709381, Final Batch Loss: 0.14502140879631042\n",
      "Epoch 3461, Loss: 0.19836543500423431, Final Batch Loss: 0.10774772614240646\n",
      "Epoch 3462, Loss: 0.21880259364843369, Final Batch Loss: 0.09403583407402039\n",
      "Epoch 3463, Loss: 0.21961184591054916, Final Batch Loss: 0.09814702719449997\n",
      "Epoch 3464, Loss: 0.2648690342903137, Final Batch Loss: 0.13328537344932556\n",
      "Epoch 3465, Loss: 0.26408053934574127, Final Batch Loss: 0.12871786952018738\n",
      "Epoch 3466, Loss: 0.21211320906877518, Final Batch Loss: 0.11550082266330719\n",
      "Epoch 3467, Loss: 0.2759680524468422, Final Batch Loss: 0.1192498728632927\n",
      "Epoch 3468, Loss: 0.25056567043066025, Final Batch Loss: 0.10559683293104172\n",
      "Epoch 3469, Loss: 0.2185584455728531, Final Batch Loss: 0.12156101316213608\n",
      "Epoch 3470, Loss: 0.26413583755493164, Final Batch Loss: 0.1697445958852768\n",
      "Epoch 3471, Loss: 0.19351645559072495, Final Batch Loss: 0.06698817759752274\n",
      "Epoch 3472, Loss: 0.2584776133298874, Final Batch Loss: 0.1693383753299713\n",
      "Epoch 3473, Loss: 0.21425926685333252, Final Batch Loss: 0.09585612267255783\n",
      "Epoch 3474, Loss: 0.25661594420671463, Final Batch Loss: 0.13316857814788818\n",
      "Epoch 3475, Loss: 0.25418829172849655, Final Batch Loss: 0.14804868400096893\n",
      "Epoch 3476, Loss: 0.23705516010522842, Final Batch Loss: 0.12692256271839142\n",
      "Epoch 3477, Loss: 0.25766175985336304, Final Batch Loss: 0.13247916102409363\n",
      "Epoch 3478, Loss: 0.2707323879003525, Final Batch Loss: 0.13131248950958252\n",
      "Epoch 3479, Loss: 0.26469945907592773, Final Batch Loss: 0.15870417654514313\n",
      "Epoch 3480, Loss: 0.2601126581430435, Final Batch Loss: 0.13232386112213135\n",
      "Epoch 3481, Loss: 0.22218649089336395, Final Batch Loss: 0.13169732689857483\n",
      "Epoch 3482, Loss: 0.2012011781334877, Final Batch Loss: 0.1196756586432457\n",
      "Epoch 3483, Loss: 0.22074677050113678, Final Batch Loss: 0.06775571405887604\n",
      "Epoch 3484, Loss: 0.2952199727296829, Final Batch Loss: 0.14313040673732758\n",
      "Epoch 3485, Loss: 0.24041447788476944, Final Batch Loss: 0.1449291855096817\n",
      "Epoch 3486, Loss: 0.24222951382398605, Final Batch Loss: 0.13314388692378998\n",
      "Epoch 3487, Loss: 0.2683752477169037, Final Batch Loss: 0.14090567827224731\n",
      "Epoch 3488, Loss: 0.22389903664588928, Final Batch Loss: 0.11330723762512207\n",
      "Epoch 3489, Loss: 0.20249027758836746, Final Batch Loss: 0.07328494638204575\n",
      "Epoch 3490, Loss: 0.2319164201617241, Final Batch Loss: 0.08470969647169113\n",
      "Epoch 3491, Loss: 0.22466926276683807, Final Batch Loss: 0.11778625100851059\n",
      "Epoch 3492, Loss: 0.20746441930532455, Final Batch Loss: 0.1006486639380455\n",
      "Epoch 3493, Loss: 0.2588529959321022, Final Batch Loss: 0.1397850513458252\n",
      "Epoch 3494, Loss: 0.22113168984651566, Final Batch Loss: 0.1238003820180893\n",
      "Epoch 3495, Loss: 0.2326270043849945, Final Batch Loss: 0.12870743870735168\n",
      "Epoch 3496, Loss: 0.22013715654611588, Final Batch Loss: 0.10547944158315659\n",
      "Epoch 3497, Loss: 0.2945319414138794, Final Batch Loss: 0.13065995275974274\n",
      "Epoch 3498, Loss: 0.22004826366901398, Final Batch Loss: 0.0979967713356018\n",
      "Epoch 3499, Loss: 0.24098187685012817, Final Batch Loss: 0.12338120490312576\n",
      "Epoch 3500, Loss: 0.23561781644821167, Final Batch Loss: 0.14873842895030975\n",
      "Epoch 3501, Loss: 0.23345379531383514, Final Batch Loss: 0.12062662839889526\n",
      "Epoch 3502, Loss: 0.2605276554822922, Final Batch Loss: 0.14480189979076385\n",
      "Epoch 3503, Loss: 0.1932799518108368, Final Batch Loss: 0.0646771639585495\n",
      "Epoch 3504, Loss: 0.2048826366662979, Final Batch Loss: 0.09775841981172562\n",
      "Epoch 3505, Loss: 0.2537231892347336, Final Batch Loss: 0.11030687391757965\n",
      "Epoch 3506, Loss: 0.2524551600217819, Final Batch Loss: 0.1272624135017395\n",
      "Epoch 3507, Loss: 0.29145923256874084, Final Batch Loss: 0.13825395703315735\n",
      "Epoch 3508, Loss: 0.2531055882573128, Final Batch Loss: 0.1404375284910202\n",
      "Epoch 3509, Loss: 0.20595797151327133, Final Batch Loss: 0.10359793156385422\n",
      "Epoch 3510, Loss: 0.22116509079933167, Final Batch Loss: 0.09625677764415741\n",
      "Epoch 3511, Loss: 0.2782081067562103, Final Batch Loss: 0.13207390904426575\n",
      "Epoch 3512, Loss: 0.2606777101755142, Final Batch Loss: 0.11900930106639862\n",
      "Epoch 3513, Loss: 0.16388149559497833, Final Batch Loss: 0.07958895713090897\n",
      "Epoch 3514, Loss: 0.2712102010846138, Final Batch Loss: 0.12200964242219925\n",
      "Epoch 3515, Loss: 0.21595865488052368, Final Batch Loss: 0.09753482788801193\n",
      "Epoch 3516, Loss: 0.22265221923589706, Final Batch Loss: 0.10209678113460541\n",
      "Epoch 3517, Loss: 0.23294156789779663, Final Batch Loss: 0.11469380557537079\n",
      "Epoch 3518, Loss: 0.19948019832372665, Final Batch Loss: 0.08606435358524323\n",
      "Epoch 3519, Loss: 0.2067924365401268, Final Batch Loss: 0.09689684957265854\n",
      "Epoch 3520, Loss: 0.22904318571090698, Final Batch Loss: 0.11180146038532257\n",
      "Epoch 3521, Loss: 0.17501767724752426, Final Batch Loss: 0.07645868510007858\n",
      "Epoch 3522, Loss: 0.20309270918369293, Final Batch Loss: 0.08055586367845535\n",
      "Epoch 3523, Loss: 0.1837724894285202, Final Batch Loss: 0.1056799665093422\n",
      "Epoch 3524, Loss: 0.2003798708319664, Final Batch Loss: 0.06783352047204971\n",
      "Epoch 3525, Loss: 0.1964312642812729, Final Batch Loss: 0.07287272065877914\n",
      "Epoch 3526, Loss: 0.2357916161417961, Final Batch Loss: 0.11514680832624435\n",
      "Epoch 3527, Loss: 0.23163266479969025, Final Batch Loss: 0.1473500281572342\n",
      "Epoch 3528, Loss: 0.19145220518112183, Final Batch Loss: 0.07098299264907837\n",
      "Epoch 3529, Loss: 0.18701300024986267, Final Batch Loss: 0.08619274199008942\n",
      "Epoch 3530, Loss: 0.20582768321037292, Final Batch Loss: 0.11864560842514038\n",
      "Epoch 3531, Loss: 0.1911369115114212, Final Batch Loss: 0.06881681829690933\n",
      "Epoch 3532, Loss: 0.22893690317869186, Final Batch Loss: 0.10463256388902664\n",
      "Epoch 3533, Loss: 0.22167206555604935, Final Batch Loss: 0.08532526344060898\n",
      "Epoch 3534, Loss: 0.22865602374076843, Final Batch Loss: 0.1089530885219574\n",
      "Epoch 3535, Loss: 0.2815774902701378, Final Batch Loss: 0.12450291961431503\n",
      "Epoch 3536, Loss: 0.243943490087986, Final Batch Loss: 0.12264015525579453\n",
      "Epoch 3537, Loss: 0.20934955030679703, Final Batch Loss: 0.10261251777410507\n",
      "Epoch 3538, Loss: 0.191279336810112, Final Batch Loss: 0.09198246896266937\n",
      "Epoch 3539, Loss: 0.20635830610990524, Final Batch Loss: 0.0806516632437706\n",
      "Epoch 3540, Loss: 0.2126971334218979, Final Batch Loss: 0.10084742307662964\n",
      "Epoch 3541, Loss: 0.22206110507249832, Final Batch Loss: 0.11965091526508331\n",
      "Epoch 3542, Loss: 0.22079501301050186, Final Batch Loss: 0.10120762884616852\n",
      "Epoch 3543, Loss: 0.19988106191158295, Final Batch Loss: 0.10161580890417099\n",
      "Epoch 3544, Loss: 0.2490679919719696, Final Batch Loss: 0.1657356172800064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3545, Loss: 0.23432979732751846, Final Batch Loss: 0.10077882558107376\n",
      "Epoch 3546, Loss: 0.2751755565404892, Final Batch Loss: 0.11433568596839905\n",
      "Epoch 3547, Loss: 0.2823723554611206, Final Batch Loss: 0.15540999174118042\n",
      "Epoch 3548, Loss: 0.24199625849723816, Final Batch Loss: 0.12700510025024414\n",
      "Epoch 3549, Loss: 0.23996064066886902, Final Batch Loss: 0.08392110466957092\n",
      "Epoch 3550, Loss: 0.25412578135728836, Final Batch Loss: 0.1571531444787979\n",
      "Epoch 3551, Loss: 0.2478538081049919, Final Batch Loss: 0.11273466795682907\n",
      "Epoch 3552, Loss: 0.21482879668474197, Final Batch Loss: 0.07743258029222488\n",
      "Epoch 3553, Loss: 0.20020924508571625, Final Batch Loss: 0.09100516140460968\n",
      "Epoch 3554, Loss: 0.22504182904958725, Final Batch Loss: 0.12873755395412445\n",
      "Epoch 3555, Loss: 0.18935251981019974, Final Batch Loss: 0.09732484817504883\n",
      "Epoch 3556, Loss: 0.23201368004083633, Final Batch Loss: 0.1372395157814026\n",
      "Epoch 3557, Loss: 0.22134442627429962, Final Batch Loss: 0.11087923496961594\n",
      "Epoch 3558, Loss: 0.253265842795372, Final Batch Loss: 0.1180361807346344\n",
      "Epoch 3559, Loss: 0.2650597244501114, Final Batch Loss: 0.10554876923561096\n",
      "Epoch 3560, Loss: 0.235197514295578, Final Batch Loss: 0.12105315178632736\n",
      "Epoch 3561, Loss: 0.2658417373895645, Final Batch Loss: 0.09056763350963593\n",
      "Epoch 3562, Loss: 0.19785121828317642, Final Batch Loss: 0.05438608676195145\n",
      "Epoch 3563, Loss: 0.2670123651623726, Final Batch Loss: 0.10168247669935226\n",
      "Epoch 3564, Loss: 0.21364765614271164, Final Batch Loss: 0.09294166415929794\n",
      "Epoch 3565, Loss: 0.24318984150886536, Final Batch Loss: 0.1381053924560547\n",
      "Epoch 3566, Loss: 0.24950524419546127, Final Batch Loss: 0.10442595928907394\n",
      "Epoch 3567, Loss: 0.32994405925273895, Final Batch Loss: 0.2293536365032196\n",
      "Epoch 3568, Loss: 0.19152090698480606, Final Batch Loss: 0.10046399384737015\n",
      "Epoch 3569, Loss: 0.2186482921242714, Final Batch Loss: 0.08076716214418411\n",
      "Epoch 3570, Loss: 0.23506424576044083, Final Batch Loss: 0.11199057102203369\n",
      "Epoch 3571, Loss: 0.24191582202911377, Final Batch Loss: 0.10964074730873108\n",
      "Epoch 3572, Loss: 0.25401168316602707, Final Batch Loss: 0.1406892091035843\n",
      "Epoch 3573, Loss: 0.1972629353404045, Final Batch Loss: 0.0731101706624031\n",
      "Epoch 3574, Loss: 0.23272264748811722, Final Batch Loss: 0.08781195431947708\n",
      "Epoch 3575, Loss: 0.21148139983415604, Final Batch Loss: 0.11581072211265564\n",
      "Epoch 3576, Loss: 0.2254924550652504, Final Batch Loss: 0.11193464696407318\n",
      "Epoch 3577, Loss: 0.29272978007793427, Final Batch Loss: 0.140310600399971\n",
      "Epoch 3578, Loss: 0.22958873212337494, Final Batch Loss: 0.10139337182044983\n",
      "Epoch 3579, Loss: 0.180900726467371, Final Batch Loss: 0.04483950510621071\n",
      "Epoch 3580, Loss: 0.2606488987803459, Final Batch Loss: 0.13831163942813873\n",
      "Epoch 3581, Loss: 0.2359057515859604, Final Batch Loss: 0.09212096035480499\n",
      "Epoch 3582, Loss: 0.20582248270511627, Final Batch Loss: 0.09590151160955429\n",
      "Epoch 3583, Loss: 0.23054609447717667, Final Batch Loss: 0.11179358512163162\n",
      "Epoch 3584, Loss: 0.23206797242164612, Final Batch Loss: 0.10887949168682098\n",
      "Epoch 3585, Loss: 0.18834951519966125, Final Batch Loss: 0.06884007900953293\n",
      "Epoch 3586, Loss: 0.2512352988123894, Final Batch Loss: 0.11892644315958023\n",
      "Epoch 3587, Loss: 0.1957734376192093, Final Batch Loss: 0.10778278857469559\n",
      "Epoch 3588, Loss: 0.19286996126174927, Final Batch Loss: 0.09668636322021484\n",
      "Epoch 3589, Loss: 0.28306031227111816, Final Batch Loss: 0.12914568185806274\n",
      "Epoch 3590, Loss: 0.20609112828969955, Final Batch Loss: 0.09221482276916504\n",
      "Epoch 3591, Loss: 0.18656469136476517, Final Batch Loss: 0.0817490965127945\n",
      "Epoch 3592, Loss: 0.16596955433487892, Final Batch Loss: 0.05014655366539955\n",
      "Epoch 3593, Loss: 0.21818939596414566, Final Batch Loss: 0.09553948789834976\n",
      "Epoch 3594, Loss: 0.21521620452404022, Final Batch Loss: 0.11746851354837418\n",
      "Epoch 3595, Loss: 0.24962838739156723, Final Batch Loss: 0.1106165274977684\n",
      "Epoch 3596, Loss: 0.20744752883911133, Final Batch Loss: 0.10414843261241913\n",
      "Epoch 3597, Loss: 0.22359589487314224, Final Batch Loss: 0.12451335042715073\n",
      "Epoch 3598, Loss: 0.21381989121437073, Final Batch Loss: 0.10718802362680435\n",
      "Epoch 3599, Loss: 0.23135582357645035, Final Batch Loss: 0.09612420946359634\n",
      "Epoch 3600, Loss: 0.2093740552663803, Final Batch Loss: 0.09302351623773575\n",
      "Epoch 3601, Loss: 0.26980292797088623, Final Batch Loss: 0.14203648269176483\n",
      "Epoch 3602, Loss: 0.21620755642652512, Final Batch Loss: 0.11668126285076141\n",
      "Epoch 3603, Loss: 0.21138478070497513, Final Batch Loss: 0.11858268082141876\n",
      "Epoch 3604, Loss: 0.1872364953160286, Final Batch Loss: 0.08442278951406479\n",
      "Epoch 3605, Loss: 0.2323959544301033, Final Batch Loss: 0.1260484904050827\n",
      "Epoch 3606, Loss: 0.19124821573495865, Final Batch Loss: 0.11598718166351318\n",
      "Epoch 3607, Loss: 0.22888441383838654, Final Batch Loss: 0.1019742339849472\n",
      "Epoch 3608, Loss: 0.230122372508049, Final Batch Loss: 0.11392033100128174\n",
      "Epoch 3609, Loss: 0.20751648396253586, Final Batch Loss: 0.11002372205257416\n",
      "Epoch 3610, Loss: 0.34286292642354965, Final Batch Loss: 0.22515249252319336\n",
      "Epoch 3611, Loss: 0.24300649762153625, Final Batch Loss: 0.12421224266290665\n",
      "Epoch 3612, Loss: 0.22972801327705383, Final Batch Loss: 0.08239704370498657\n",
      "Epoch 3613, Loss: 0.26595117151737213, Final Batch Loss: 0.16532380878925323\n",
      "Epoch 3614, Loss: 0.23103846609592438, Final Batch Loss: 0.11343725025653839\n",
      "Epoch 3615, Loss: 0.22627361863851547, Final Batch Loss: 0.14362426102161407\n",
      "Epoch 3616, Loss: 0.23664775490760803, Final Batch Loss: 0.14256241917610168\n",
      "Epoch 3617, Loss: 0.18263758718967438, Final Batch Loss: 0.08541842550039291\n",
      "Epoch 3618, Loss: 0.2700998857617378, Final Batch Loss: 0.17936956882476807\n",
      "Epoch 3619, Loss: 0.26929399371147156, Final Batch Loss: 0.14376726746559143\n",
      "Epoch 3620, Loss: 0.190839484333992, Final Batch Loss: 0.08425111323595047\n",
      "Epoch 3621, Loss: 0.1952212229371071, Final Batch Loss: 0.07420343160629272\n",
      "Epoch 3622, Loss: 0.2638026177883148, Final Batch Loss: 0.17381170392036438\n",
      "Epoch 3623, Loss: 0.2319488674402237, Final Batch Loss: 0.12973013520240784\n",
      "Epoch 3624, Loss: 0.20573531836271286, Final Batch Loss: 0.08972489088773727\n",
      "Epoch 3625, Loss: 0.2558494806289673, Final Batch Loss: 0.15752114355564117\n",
      "Epoch 3626, Loss: 0.2603246718645096, Final Batch Loss: 0.1632920801639557\n",
      "Epoch 3627, Loss: 0.27112920582294464, Final Batch Loss: 0.1745106279850006\n",
      "Epoch 3628, Loss: 0.25754785537719727, Final Batch Loss: 0.09418238699436188\n",
      "Epoch 3629, Loss: 0.27920931577682495, Final Batch Loss: 0.12581533193588257\n",
      "Epoch 3630, Loss: 0.1805277243256569, Final Batch Loss: 0.07226536422967911\n",
      "Epoch 3631, Loss: 0.23547609150409698, Final Batch Loss: 0.14921395480632782\n",
      "Epoch 3632, Loss: 0.25343814492225647, Final Batch Loss: 0.1398935317993164\n",
      "Epoch 3633, Loss: 0.22848961502313614, Final Batch Loss: 0.10289206355810165\n",
      "Epoch 3634, Loss: 0.20559639483690262, Final Batch Loss: 0.09104886651039124\n",
      "Epoch 3635, Loss: 0.23430189490318298, Final Batch Loss: 0.10890607535839081\n",
      "Epoch 3636, Loss: 0.25061678141355515, Final Batch Loss: 0.09366051107645035\n",
      "Epoch 3637, Loss: 0.21674557030200958, Final Batch Loss: 0.10071287304162979\n",
      "Epoch 3638, Loss: 0.2166026085615158, Final Batch Loss: 0.09773459285497665\n",
      "Epoch 3639, Loss: 0.25194210559129715, Final Batch Loss: 0.1412942111492157\n",
      "Epoch 3640, Loss: 0.21649185568094254, Final Batch Loss: 0.0980706587433815\n",
      "Epoch 3641, Loss: 0.25387581437826157, Final Batch Loss: 0.07693704217672348\n",
      "Epoch 3642, Loss: 0.31672966480255127, Final Batch Loss: 0.08987733721733093\n",
      "Epoch 3643, Loss: 0.23600446432828903, Final Batch Loss: 0.15043391287326813\n",
      "Epoch 3644, Loss: 0.205189511179924, Final Batch Loss: 0.09295027703046799\n",
      "Epoch 3645, Loss: 0.2229534238576889, Final Batch Loss: 0.14582088589668274\n",
      "Epoch 3646, Loss: 0.23203634470701218, Final Batch Loss: 0.10048940032720566\n",
      "Epoch 3647, Loss: 0.160996213555336, Final Batch Loss: 0.08155754953622818\n",
      "Epoch 3648, Loss: 0.2435566633939743, Final Batch Loss: 0.11485880613327026\n",
      "Epoch 3649, Loss: 0.28366613388061523, Final Batch Loss: 0.1460384726524353\n",
      "Epoch 3650, Loss: 0.22692251205444336, Final Batch Loss: 0.07572302222251892\n",
      "Epoch 3651, Loss: 0.23320116847753525, Final Batch Loss: 0.07873978465795517\n",
      "Epoch 3652, Loss: 0.23780874907970428, Final Batch Loss: 0.11314123868942261\n",
      "Epoch 3653, Loss: 0.2085881009697914, Final Batch Loss: 0.10898973792791367\n",
      "Epoch 3654, Loss: 0.20949017256498337, Final Batch Loss: 0.09868458658456802\n",
      "Epoch 3655, Loss: 0.25792641937732697, Final Batch Loss: 0.1312187910079956\n",
      "Epoch 3656, Loss: 0.21108855307102203, Final Batch Loss: 0.11699008196592331\n",
      "Epoch 3657, Loss: 0.29847878962755203, Final Batch Loss: 0.18231935799121857\n",
      "Epoch 3658, Loss: 0.2180473655462265, Final Batch Loss: 0.12255445122718811\n",
      "Epoch 3659, Loss: 0.23232337087392807, Final Batch Loss: 0.15493562817573547\n",
      "Epoch 3660, Loss: 0.20896603167057037, Final Batch Loss: 0.08160041272640228\n",
      "Epoch 3661, Loss: 0.26794150471687317, Final Batch Loss: 0.14114727079868317\n",
      "Epoch 3662, Loss: 0.22849323600530624, Final Batch Loss: 0.12106245756149292\n",
      "Epoch 3663, Loss: 0.20190199464559555, Final Batch Loss: 0.10045421123504639\n",
      "Epoch 3664, Loss: 0.22149139642715454, Final Batch Loss: 0.11284684389829636\n",
      "Epoch 3665, Loss: 0.2420884594321251, Final Batch Loss: 0.12100952863693237\n",
      "Epoch 3666, Loss: 0.2396717220544815, Final Batch Loss: 0.1479232758283615\n",
      "Epoch 3667, Loss: 0.23229163140058517, Final Batch Loss: 0.13749006390571594\n",
      "Epoch 3668, Loss: 0.2045237272977829, Final Batch Loss: 0.12367619574069977\n",
      "Epoch 3669, Loss: 0.23521514236927032, Final Batch Loss: 0.1068812906742096\n",
      "Epoch 3670, Loss: 0.23812421411275864, Final Batch Loss: 0.0918971374630928\n",
      "Epoch 3671, Loss: 0.18996884673833847, Final Batch Loss: 0.06972307711839676\n",
      "Epoch 3672, Loss: 0.21115897595882416, Final Batch Loss: 0.08926347643136978\n",
      "Epoch 3673, Loss: 0.1851232722401619, Final Batch Loss: 0.08368565142154694\n",
      "Epoch 3674, Loss: 0.26463185995817184, Final Batch Loss: 0.15632963180541992\n",
      "Epoch 3675, Loss: 0.20784123986959457, Final Batch Loss: 0.10173419117927551\n",
      "Epoch 3676, Loss: 0.24443669617176056, Final Batch Loss: 0.1105927973985672\n",
      "Epoch 3677, Loss: 0.23780519515275955, Final Batch Loss: 0.10724975913763046\n",
      "Epoch 3678, Loss: 0.24167447537183762, Final Batch Loss: 0.1075395718216896\n",
      "Epoch 3679, Loss: 0.24554453045129776, Final Batch Loss: 0.10250788182020187\n",
      "Epoch 3680, Loss: 0.23860342800617218, Final Batch Loss: 0.1230282410979271\n",
      "Epoch 3681, Loss: 0.23093581199645996, Final Batch Loss: 0.13349953293800354\n",
      "Epoch 3682, Loss: 0.24889487773180008, Final Batch Loss: 0.1489705741405487\n",
      "Epoch 3683, Loss: 0.251196525990963, Final Batch Loss: 0.1163896843791008\n",
      "Epoch 3684, Loss: 0.198017917573452, Final Batch Loss: 0.10165658593177795\n",
      "Epoch 3685, Loss: 0.25825081765651703, Final Batch Loss: 0.14373737573623657\n",
      "Epoch 3686, Loss: 0.222761370241642, Final Batch Loss: 0.06247875839471817\n",
      "Epoch 3687, Loss: 0.2114475518465042, Final Batch Loss: 0.10740909725427628\n",
      "Epoch 3688, Loss: 0.21704339981079102, Final Batch Loss: 0.105098195374012\n",
      "Epoch 3689, Loss: 0.25193092226982117, Final Batch Loss: 0.1355903148651123\n",
      "Epoch 3690, Loss: 0.22211258113384247, Final Batch Loss: 0.10704559832811356\n",
      "Epoch 3691, Loss: 0.19669917225837708, Final Batch Loss: 0.08986735343933105\n",
      "Epoch 3692, Loss: 0.19009603559970856, Final Batch Loss: 0.11513475328683853\n",
      "Epoch 3693, Loss: 0.2462075650691986, Final Batch Loss: 0.12398616969585419\n",
      "Epoch 3694, Loss: 0.2410036399960518, Final Batch Loss: 0.12357278168201447\n",
      "Epoch 3695, Loss: 0.2359834983944893, Final Batch Loss: 0.11704852432012558\n",
      "Epoch 3696, Loss: 0.23530936986207962, Final Batch Loss: 0.12556612491607666\n",
      "Epoch 3697, Loss: 0.22720733284950256, Final Batch Loss: 0.13399779796600342\n",
      "Epoch 3698, Loss: 0.18703648447990417, Final Batch Loss: 0.08514861762523651\n",
      "Epoch 3699, Loss: 0.21875406056642532, Final Batch Loss: 0.11365611851215363\n",
      "Epoch 3700, Loss: 0.20216498523950577, Final Batch Loss: 0.08848778158426285\n",
      "Epoch 3701, Loss: 0.1757553443312645, Final Batch Loss: 0.07309188693761826\n",
      "Epoch 3702, Loss: 0.18538720905780792, Final Batch Loss: 0.08316369354724884\n",
      "Epoch 3703, Loss: 0.24779000133275986, Final Batch Loss: 0.12371084094047546\n",
      "Epoch 3704, Loss: 0.23416722565889359, Final Batch Loss: 0.1455957442522049\n",
      "Epoch 3705, Loss: 0.19368775933980942, Final Batch Loss: 0.0819137692451477\n",
      "Epoch 3706, Loss: 0.20330849289894104, Final Batch Loss: 0.09254956990480423\n",
      "Epoch 3707, Loss: 0.23549608886241913, Final Batch Loss: 0.11632809042930603\n",
      "Epoch 3708, Loss: 0.2174067348241806, Final Batch Loss: 0.08914606273174286\n",
      "Epoch 3709, Loss: 0.1882702261209488, Final Batch Loss: 0.10422194004058838\n",
      "Epoch 3710, Loss: 0.22649043053388596, Final Batch Loss: 0.12400577962398529\n",
      "Epoch 3711, Loss: 0.20717138051986694, Final Batch Loss: 0.08845625817775726\n",
      "Epoch 3712, Loss: 0.28796668350696564, Final Batch Loss: 0.10425770282745361\n",
      "Epoch 3713, Loss: 0.20175856351852417, Final Batch Loss: 0.07649785280227661\n",
      "Epoch 3714, Loss: 0.2551126405596733, Final Batch Loss: 0.13120047748088837\n",
      "Epoch 3715, Loss: 0.23075130581855774, Final Batch Loss: 0.14422908425331116\n",
      "Epoch 3716, Loss: 0.24925944209098816, Final Batch Loss: 0.1491134762763977\n",
      "Epoch 3717, Loss: 0.20791435986757278, Final Batch Loss: 0.10083851963281631\n",
      "Epoch 3718, Loss: 0.19849766790866852, Final Batch Loss: 0.09153162688016891\n",
      "Epoch 3719, Loss: 0.23983241617679596, Final Batch Loss: 0.11422859132289886\n",
      "Epoch 3720, Loss: 0.22656352072954178, Final Batch Loss: 0.09918081015348434\n",
      "Epoch 3721, Loss: 0.19986601918935776, Final Batch Loss: 0.06257084757089615\n",
      "Epoch 3722, Loss: 0.17553385347127914, Final Batch Loss: 0.08874273300170898\n",
      "Epoch 3723, Loss: 0.2078741490840912, Final Batch Loss: 0.0961889699101448\n",
      "Epoch 3724, Loss: 0.20745163410902023, Final Batch Loss: 0.11889547109603882\n",
      "Epoch 3725, Loss: 0.24383100867271423, Final Batch Loss: 0.1264488697052002\n",
      "Epoch 3726, Loss: 0.23309968411922455, Final Batch Loss: 0.15301652252674103\n",
      "Epoch 3727, Loss: 0.2435671091079712, Final Batch Loss: 0.1040182113647461\n",
      "Epoch 3728, Loss: 0.20694956928491592, Final Batch Loss: 0.11074696481227875\n",
      "Epoch 3729, Loss: 0.21099595725536346, Final Batch Loss: 0.12577636539936066\n",
      "Epoch 3730, Loss: 0.20415829122066498, Final Batch Loss: 0.07915788888931274\n",
      "Epoch 3731, Loss: 0.23470386117696762, Final Batch Loss: 0.12849706411361694\n",
      "Epoch 3732, Loss: 0.23844774067401886, Final Batch Loss: 0.09072990715503693\n",
      "Epoch 3733, Loss: 0.17626956850290298, Final Batch Loss: 0.07699116319417953\n",
      "Epoch 3734, Loss: 0.2539573162794113, Final Batch Loss: 0.12709492444992065\n",
      "Epoch 3735, Loss: 0.2759027034044266, Final Batch Loss: 0.15050087869167328\n",
      "Epoch 3736, Loss: 0.17342662066221237, Final Batch Loss: 0.08808083832263947\n",
      "Epoch 3737, Loss: 0.206161230802536, Final Batch Loss: 0.10386084020137787\n",
      "Epoch 3738, Loss: 0.19671156257390976, Final Batch Loss: 0.09285847842693329\n",
      "Epoch 3739, Loss: 0.3171975463628769, Final Batch Loss: 0.17292281985282898\n",
      "Epoch 3740, Loss: 0.22481992095708847, Final Batch Loss: 0.07923442870378494\n",
      "Epoch 3741, Loss: 0.15334905683994293, Final Batch Loss: 0.04387231916189194\n",
      "Epoch 3742, Loss: 0.28296664357185364, Final Batch Loss: 0.1446298211812973\n",
      "Epoch 3743, Loss: 0.257967971265316, Final Batch Loss: 0.1359373927116394\n",
      "Epoch 3744, Loss: 0.3294171690940857, Final Batch Loss: 0.1441660076379776\n",
      "Epoch 3745, Loss: 0.231529138982296, Final Batch Loss: 0.1034492775797844\n",
      "Epoch 3746, Loss: 0.2081071138381958, Final Batch Loss: 0.11331546306610107\n",
      "Epoch 3747, Loss: 0.2010400965809822, Final Batch Loss: 0.11093680560588837\n",
      "Epoch 3748, Loss: 0.20152787864208221, Final Batch Loss: 0.1312628835439682\n",
      "Epoch 3749, Loss: 0.2000773921608925, Final Batch Loss: 0.11492816358804703\n",
      "Epoch 3750, Loss: 0.20665086805820465, Final Batch Loss: 0.1290564090013504\n",
      "Epoch 3751, Loss: 0.24107227474451065, Final Batch Loss: 0.10980742424726486\n",
      "Epoch 3752, Loss: 0.22168628871440887, Final Batch Loss: 0.1124190017580986\n",
      "Epoch 3753, Loss: 0.2081328108906746, Final Batch Loss: 0.08711008727550507\n",
      "Epoch 3754, Loss: 0.17585457861423492, Final Batch Loss: 0.10977582633495331\n",
      "Epoch 3755, Loss: 0.20107337087392807, Final Batch Loss: 0.11774004995822906\n",
      "Epoch 3756, Loss: 0.20387596637010574, Final Batch Loss: 0.09894684702157974\n",
      "Epoch 3757, Loss: 0.20873380452394485, Final Batch Loss: 0.11376720666885376\n",
      "Epoch 3758, Loss: 0.22073917835950851, Final Batch Loss: 0.10966408997774124\n",
      "Epoch 3759, Loss: 0.19836322218179703, Final Batch Loss: 0.0829097256064415\n",
      "Epoch 3760, Loss: 0.2314281389117241, Final Batch Loss: 0.1147538498044014\n",
      "Epoch 3761, Loss: 0.17125075310468674, Final Batch Loss: 0.06728480756282806\n",
      "Epoch 3762, Loss: 0.182501882314682, Final Batch Loss: 0.08761927485466003\n",
      "Epoch 3763, Loss: 0.16319353505969048, Final Batch Loss: 0.061476368457078934\n",
      "Epoch 3764, Loss: 0.16888776794075966, Final Batch Loss: 0.04860522970557213\n",
      "Epoch 3765, Loss: 0.21566720306873322, Final Batch Loss: 0.1518779695034027\n",
      "Epoch 3766, Loss: 0.1838962584733963, Final Batch Loss: 0.09765942394733429\n",
      "Epoch 3767, Loss: 0.26176363229751587, Final Batch Loss: 0.13492298126220703\n",
      "Epoch 3768, Loss: 0.19215383380651474, Final Batch Loss: 0.09061182290315628\n",
      "Epoch 3769, Loss: 0.20932012051343918, Final Batch Loss: 0.0965917780995369\n",
      "Epoch 3770, Loss: 0.2281312793493271, Final Batch Loss: 0.11643103510141373\n",
      "Epoch 3771, Loss: 0.19260577112436295, Final Batch Loss: 0.0904921293258667\n",
      "Epoch 3772, Loss: 0.1930946409702301, Final Batch Loss: 0.09541132301092148\n",
      "Epoch 3773, Loss: 0.22825604304671288, Final Batch Loss: 0.059696730226278305\n",
      "Epoch 3774, Loss: 0.2568591758608818, Final Batch Loss: 0.13991974294185638\n",
      "Epoch 3775, Loss: 0.27032530307769775, Final Batch Loss: 0.12292249500751495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3776, Loss: 0.23482000082731247, Final Batch Loss: 0.09083466976881027\n",
      "Epoch 3777, Loss: 0.24226637184619904, Final Batch Loss: 0.08533470332622528\n",
      "Epoch 3778, Loss: 0.2556327059864998, Final Batch Loss: 0.12373334914445877\n",
      "Epoch 3779, Loss: 0.1995682492852211, Final Batch Loss: 0.10702472925186157\n",
      "Epoch 3780, Loss: 0.2512274459004402, Final Batch Loss: 0.126548632979393\n",
      "Epoch 3781, Loss: 0.18875477463006973, Final Batch Loss: 0.09253957867622375\n",
      "Epoch 3782, Loss: 0.2021939903497696, Final Batch Loss: 0.10894997417926788\n",
      "Epoch 3783, Loss: 0.15133381634950638, Final Batch Loss: 0.07129762321710587\n",
      "Epoch 3784, Loss: 0.32315730303525925, Final Batch Loss: 0.11351151019334793\n",
      "Epoch 3785, Loss: 0.22095833718776703, Final Batch Loss: 0.11730878055095673\n",
      "Epoch 3786, Loss: 0.2216934859752655, Final Batch Loss: 0.12906070053577423\n",
      "Epoch 3787, Loss: 0.18211913853883743, Final Batch Loss: 0.07301843911409378\n",
      "Epoch 3788, Loss: 0.22390015423297882, Final Batch Loss: 0.08743374049663544\n",
      "Epoch 3789, Loss: 0.21602914482355118, Final Batch Loss: 0.08319105952978134\n",
      "Epoch 3790, Loss: 0.21230920404195786, Final Batch Loss: 0.11396761238574982\n",
      "Epoch 3791, Loss: 0.20992755144834518, Final Batch Loss: 0.1226772665977478\n",
      "Epoch 3792, Loss: 0.1784816011786461, Final Batch Loss: 0.0756588727235794\n",
      "Epoch 3793, Loss: 0.19904054701328278, Final Batch Loss: 0.10435274988412857\n",
      "Epoch 3794, Loss: 0.20231056958436966, Final Batch Loss: 0.12417768687009811\n",
      "Epoch 3795, Loss: 0.25346802920103073, Final Batch Loss: 0.10428149253129959\n",
      "Epoch 3796, Loss: 0.24619659781455994, Final Batch Loss: 0.12702403962612152\n",
      "Epoch 3797, Loss: 0.22431859374046326, Final Batch Loss: 0.11623366922140121\n",
      "Epoch 3798, Loss: 0.34448397159576416, Final Batch Loss: 0.16658581793308258\n",
      "Epoch 3799, Loss: 0.2113794982433319, Final Batch Loss: 0.11096959561109543\n",
      "Epoch 3800, Loss: 0.2644621282815933, Final Batch Loss: 0.1676165759563446\n",
      "Epoch 3801, Loss: 0.19856875389814377, Final Batch Loss: 0.1169244796037674\n",
      "Epoch 3802, Loss: 0.19675516337156296, Final Batch Loss: 0.07892102003097534\n",
      "Epoch 3803, Loss: 0.21061882376670837, Final Batch Loss: 0.09778356552124023\n",
      "Epoch 3804, Loss: 0.1899726465344429, Final Batch Loss: 0.08668588101863861\n",
      "Epoch 3805, Loss: 0.22216399013996124, Final Batch Loss: 0.07661867141723633\n",
      "Epoch 3806, Loss: 0.23736820369958878, Final Batch Loss: 0.13620904088020325\n",
      "Epoch 3807, Loss: 0.26602770388126373, Final Batch Loss: 0.17306238412857056\n",
      "Epoch 3808, Loss: 0.21523843705654144, Final Batch Loss: 0.13590453565120697\n",
      "Epoch 3809, Loss: 0.23053237795829773, Final Batch Loss: 0.09438401460647583\n",
      "Epoch 3810, Loss: 0.22437530755996704, Final Batch Loss: 0.11223532259464264\n",
      "Epoch 3811, Loss: 0.2080182507634163, Final Batch Loss: 0.0996246263384819\n",
      "Epoch 3812, Loss: 0.26585205644369125, Final Batch Loss: 0.18070095777511597\n",
      "Epoch 3813, Loss: 0.28862880170345306, Final Batch Loss: 0.17169810831546783\n",
      "Epoch 3814, Loss: 0.21854817122220993, Final Batch Loss: 0.08375712484121323\n",
      "Epoch 3815, Loss: 0.27883831411600113, Final Batch Loss: 0.15814615786075592\n",
      "Epoch 3816, Loss: 0.2426406294107437, Final Batch Loss: 0.11507868766784668\n",
      "Epoch 3817, Loss: 0.2002432867884636, Final Batch Loss: 0.0984317809343338\n",
      "Epoch 3818, Loss: 0.23916008323431015, Final Batch Loss: 0.12608204782009125\n",
      "Epoch 3819, Loss: 0.1831849440932274, Final Batch Loss: 0.07613266259431839\n",
      "Epoch 3820, Loss: 0.2192160040140152, Final Batch Loss: 0.11483005434274673\n",
      "Epoch 3821, Loss: 0.22872888296842575, Final Batch Loss: 0.12581844627857208\n",
      "Epoch 3822, Loss: 0.2006630375981331, Final Batch Loss: 0.10453258454799652\n",
      "Epoch 3823, Loss: 0.15813174843788147, Final Batch Loss: 0.03912947326898575\n",
      "Epoch 3824, Loss: 0.20969846099615097, Final Batch Loss: 0.08616624027490616\n",
      "Epoch 3825, Loss: 0.20156855136156082, Final Batch Loss: 0.12791672348976135\n",
      "Epoch 3826, Loss: 0.2447371557354927, Final Batch Loss: 0.09964992851018906\n",
      "Epoch 3827, Loss: 0.25398949533700943, Final Batch Loss: 0.14052128791809082\n",
      "Epoch 3828, Loss: 0.22047123312950134, Final Batch Loss: 0.1269010454416275\n",
      "Epoch 3829, Loss: 0.24771037697792053, Final Batch Loss: 0.11798198521137238\n",
      "Epoch 3830, Loss: 0.2244211509823799, Final Batch Loss: 0.13516102731227875\n",
      "Epoch 3831, Loss: 0.20964466035366058, Final Batch Loss: 0.07055337727069855\n",
      "Epoch 3832, Loss: 0.1854155883193016, Final Batch Loss: 0.07834862917661667\n",
      "Epoch 3833, Loss: 0.23245622962713242, Final Batch Loss: 0.06587988883256912\n",
      "Epoch 3834, Loss: 0.1956087127327919, Final Batch Loss: 0.10380440205335617\n",
      "Epoch 3835, Loss: 0.20032333582639694, Final Batch Loss: 0.09061262756586075\n",
      "Epoch 3836, Loss: 0.1907196044921875, Final Batch Loss: 0.10330774635076523\n",
      "Epoch 3837, Loss: 0.27638480067253113, Final Batch Loss: 0.1801043301820755\n",
      "Epoch 3838, Loss: 0.19992157816886902, Final Batch Loss: 0.11203455924987793\n",
      "Epoch 3839, Loss: 0.2088564708828926, Final Batch Loss: 0.10106544196605682\n",
      "Epoch 3840, Loss: 0.24742431938648224, Final Batch Loss: 0.1648244708776474\n",
      "Epoch 3841, Loss: 0.19672353565692902, Final Batch Loss: 0.08271992206573486\n",
      "Epoch 3842, Loss: 0.2686930149793625, Final Batch Loss: 0.1300465613603592\n",
      "Epoch 3843, Loss: 0.18280958384275436, Final Batch Loss: 0.09954673796892166\n",
      "Epoch 3844, Loss: 0.15746790170669556, Final Batch Loss: 0.08785783499479294\n",
      "Epoch 3845, Loss: 0.25087714195251465, Final Batch Loss: 0.15422676503658295\n",
      "Epoch 3846, Loss: 0.2553919032216072, Final Batch Loss: 0.11745349317789078\n",
      "Epoch 3847, Loss: 0.2700572684407234, Final Batch Loss: 0.08448857814073563\n",
      "Epoch 3848, Loss: 0.18779461830854416, Final Batch Loss: 0.08414566516876221\n",
      "Epoch 3849, Loss: 0.17478377372026443, Final Batch Loss: 0.07440869510173798\n",
      "Epoch 3850, Loss: 0.2086062654852867, Final Batch Loss: 0.11963354796171188\n",
      "Epoch 3851, Loss: 0.1607547029852867, Final Batch Loss: 0.09408456087112427\n",
      "Epoch 3852, Loss: 0.1784927323460579, Final Batch Loss: 0.1032070517539978\n",
      "Epoch 3853, Loss: 0.2162776067852974, Final Batch Loss: 0.1129176989197731\n",
      "Epoch 3854, Loss: 0.19796022772789001, Final Batch Loss: 0.10903678834438324\n",
      "Epoch 3855, Loss: 0.22291471809148788, Final Batch Loss: 0.11104711145162582\n",
      "Epoch 3856, Loss: 0.20834100246429443, Final Batch Loss: 0.14231516420841217\n",
      "Epoch 3857, Loss: 0.22317088395357132, Final Batch Loss: 0.12013006210327148\n",
      "Epoch 3858, Loss: 0.15678412467241287, Final Batch Loss: 0.07357253134250641\n",
      "Epoch 3859, Loss: 0.20443885028362274, Final Batch Loss: 0.08985602110624313\n",
      "Epoch 3860, Loss: 0.23054751008749008, Final Batch Loss: 0.14605507254600525\n",
      "Epoch 3861, Loss: 0.18360770493745804, Final Batch Loss: 0.10533491522073746\n",
      "Epoch 3862, Loss: 0.2566596046090126, Final Batch Loss: 0.16556371748447418\n",
      "Epoch 3863, Loss: 0.2556948885321617, Final Batch Loss: 0.08275685459375381\n",
      "Epoch 3864, Loss: 0.20622141659259796, Final Batch Loss: 0.12284623831510544\n",
      "Epoch 3865, Loss: 0.35622239112854004, Final Batch Loss: 0.19134655594825745\n",
      "Epoch 3866, Loss: 0.19470595195889473, Final Batch Loss: 0.057263392955064774\n",
      "Epoch 3867, Loss: 0.13960553333163261, Final Batch Loss: 0.05567357316613197\n",
      "Epoch 3868, Loss: 0.21600105613470078, Final Batch Loss: 0.08697891980409622\n",
      "Epoch 3869, Loss: 0.19886623322963715, Final Batch Loss: 0.08137932419776917\n",
      "Epoch 3870, Loss: 0.1527130976319313, Final Batch Loss: 0.06746497005224228\n",
      "Epoch 3871, Loss: 0.18102539330720901, Final Batch Loss: 0.08369223028421402\n",
      "Epoch 3872, Loss: 0.21765802055597305, Final Batch Loss: 0.10180668532848358\n",
      "Epoch 3873, Loss: 0.20500364899635315, Final Batch Loss: 0.10919564217329025\n",
      "Epoch 3874, Loss: 0.16817496716976166, Final Batch Loss: 0.04029776155948639\n",
      "Epoch 3875, Loss: 0.19913432747125626, Final Batch Loss: 0.09354986995458603\n",
      "Epoch 3876, Loss: 0.19205674529075623, Final Batch Loss: 0.10631563514471054\n",
      "Epoch 3877, Loss: 0.18094158917665482, Final Batch Loss: 0.0840357095003128\n",
      "Epoch 3878, Loss: 0.22722133994102478, Final Batch Loss: 0.12199793756008148\n",
      "Epoch 3879, Loss: 0.20080819725990295, Final Batch Loss: 0.10440512746572495\n",
      "Epoch 3880, Loss: 0.18502408266067505, Final Batch Loss: 0.10563787817955017\n",
      "Epoch 3881, Loss: 0.1428094580769539, Final Batch Loss: 0.08842357993125916\n",
      "Epoch 3882, Loss: 0.16157299280166626, Final Batch Loss: 0.07879766821861267\n",
      "Epoch 3883, Loss: 0.15295137465000153, Final Batch Loss: 0.07847528159618378\n",
      "Epoch 3884, Loss: 0.15820830687880516, Final Batch Loss: 0.03709257021546364\n",
      "Epoch 3885, Loss: 0.24623272567987442, Final Batch Loss: 0.1481233835220337\n",
      "Epoch 3886, Loss: 0.23779891431331635, Final Batch Loss: 0.10264226794242859\n",
      "Epoch 3887, Loss: 0.38228031247854233, Final Batch Loss: 0.2859457731246948\n",
      "Epoch 3888, Loss: 0.19060425460338593, Final Batch Loss: 0.12550659477710724\n",
      "Epoch 3889, Loss: 0.20358716696500778, Final Batch Loss: 0.08176707476377487\n",
      "Epoch 3890, Loss: 0.3290642201900482, Final Batch Loss: 0.13364307582378387\n",
      "Epoch 3891, Loss: 0.19471649825572968, Final Batch Loss: 0.10729484260082245\n",
      "Epoch 3892, Loss: 0.23190516233444214, Final Batch Loss: 0.13497093319892883\n",
      "Epoch 3893, Loss: 0.188136987388134, Final Batch Loss: 0.10315947234630585\n",
      "Epoch 3894, Loss: 0.23820149153470993, Final Batch Loss: 0.12830428779125214\n",
      "Epoch 3895, Loss: 0.2018224075436592, Final Batch Loss: 0.07302982360124588\n",
      "Epoch 3896, Loss: 0.1941896677017212, Final Batch Loss: 0.06644076108932495\n",
      "Epoch 3897, Loss: 0.232033409178257, Final Batch Loss: 0.06597497314214706\n",
      "Epoch 3898, Loss: 0.2143740877509117, Final Batch Loss: 0.09190807491540909\n",
      "Epoch 3899, Loss: 0.197997584939003, Final Batch Loss: 0.08764731138944626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3900, Loss: 0.22807227820158005, Final Batch Loss: 0.1470881700515747\n",
      "Epoch 3901, Loss: 0.1730392947793007, Final Batch Loss: 0.10321097820997238\n",
      "Epoch 3902, Loss: 0.1652780920267105, Final Batch Loss: 0.0877435952425003\n",
      "Epoch 3903, Loss: 0.1490199714899063, Final Batch Loss: 0.06288829445838928\n",
      "Epoch 3904, Loss: 0.1712675541639328, Final Batch Loss: 0.09387903660535812\n",
      "Epoch 3905, Loss: 0.23533806204795837, Final Batch Loss: 0.13200683891773224\n",
      "Epoch 3906, Loss: 0.21481002867221832, Final Batch Loss: 0.1060030460357666\n",
      "Epoch 3907, Loss: 0.19013847410678864, Final Batch Loss: 0.09596613794565201\n",
      "Epoch 3908, Loss: 0.20038828998804092, Final Batch Loss: 0.10747615247964859\n",
      "Epoch 3909, Loss: 0.21837328374385834, Final Batch Loss: 0.10684351623058319\n",
      "Epoch 3910, Loss: 0.21095232665538788, Final Batch Loss: 0.1185905709862709\n",
      "Epoch 3911, Loss: 0.19822002947330475, Final Batch Loss: 0.07756795734167099\n",
      "Epoch 3912, Loss: 0.22255872935056686, Final Batch Loss: 0.14062580466270447\n",
      "Epoch 3913, Loss: 0.19480504840612411, Final Batch Loss: 0.1120886355638504\n",
      "Epoch 3914, Loss: 0.19853577762842178, Final Batch Loss: 0.08516082912683487\n",
      "Epoch 3915, Loss: 0.2517557665705681, Final Batch Loss: 0.10651210695505142\n",
      "Epoch 3916, Loss: 0.21938980370759964, Final Batch Loss: 0.1235092282295227\n",
      "Epoch 3917, Loss: 0.1925053372979164, Final Batch Loss: 0.08595933020114899\n",
      "Epoch 3918, Loss: 0.201505646109581, Final Batch Loss: 0.11252325773239136\n",
      "Epoch 3919, Loss: 0.21913409233093262, Final Batch Loss: 0.13939650356769562\n",
      "Epoch 3920, Loss: 0.21542978286743164, Final Batch Loss: 0.12036000192165375\n",
      "Epoch 3921, Loss: 0.20505212247371674, Final Batch Loss: 0.10158691555261612\n",
      "Epoch 3922, Loss: 0.17909304797649384, Final Batch Loss: 0.07603579014539719\n",
      "Epoch 3923, Loss: 0.24107305705547333, Final Batch Loss: 0.09935618937015533\n",
      "Epoch 3924, Loss: 0.20760312676429749, Final Batch Loss: 0.12582258880138397\n",
      "Epoch 3925, Loss: 0.22735807672142982, Final Batch Loss: 0.1658908873796463\n",
      "Epoch 3926, Loss: 0.1623297929763794, Final Batch Loss: 0.07025156915187836\n",
      "Epoch 3927, Loss: 0.2167607545852661, Final Batch Loss: 0.11371055990457535\n",
      "Epoch 3928, Loss: 0.2225583866238594, Final Batch Loss: 0.12339439988136292\n",
      "Epoch 3929, Loss: 0.2319433093070984, Final Batch Loss: 0.1080600693821907\n",
      "Epoch 3930, Loss: 0.20523474365472794, Final Batch Loss: 0.08123405277729034\n",
      "Epoch 3931, Loss: 0.17974896356463432, Final Batch Loss: 0.04802508279681206\n",
      "Epoch 3932, Loss: 0.31867194920778275, Final Batch Loss: 0.21836085617542267\n",
      "Epoch 3933, Loss: 0.2820059135556221, Final Batch Loss: 0.2009880393743515\n",
      "Epoch 3934, Loss: 0.22547129541635513, Final Batch Loss: 0.12612544000148773\n",
      "Epoch 3935, Loss: 0.21609968692064285, Final Batch Loss: 0.1272668093442917\n",
      "Epoch 3936, Loss: 0.17352350801229477, Final Batch Loss: 0.07000017166137695\n",
      "Epoch 3937, Loss: 0.23514267057180405, Final Batch Loss: 0.16454991698265076\n",
      "Epoch 3938, Loss: 0.17828233540058136, Final Batch Loss: 0.08229073137044907\n",
      "Epoch 3939, Loss: 0.20231462270021439, Final Batch Loss: 0.07155609875917435\n",
      "Epoch 3940, Loss: 0.17566370964050293, Final Batch Loss: 0.05974031239748001\n",
      "Epoch 3941, Loss: 0.23071250319480896, Final Batch Loss: 0.14738032221794128\n",
      "Epoch 3942, Loss: 0.19000626355409622, Final Batch Loss: 0.047303251922130585\n",
      "Epoch 3943, Loss: 0.19861463457345963, Final Batch Loss: 0.10009049624204636\n",
      "Epoch 3944, Loss: 0.2510376125574112, Final Batch Loss: 0.1238010823726654\n",
      "Epoch 3945, Loss: 0.14423152059316635, Final Batch Loss: 0.07067608833312988\n",
      "Epoch 3946, Loss: 0.22685256600379944, Final Batch Loss: 0.12146725505590439\n",
      "Epoch 3947, Loss: 0.16621161997318268, Final Batch Loss: 0.08579347282648087\n",
      "Epoch 3948, Loss: 0.20099987834692, Final Batch Loss: 0.07616819441318512\n",
      "Epoch 3949, Loss: 0.18864791095256805, Final Batch Loss: 0.09660585969686508\n",
      "Epoch 3950, Loss: 0.17800843715667725, Final Batch Loss: 0.06670910120010376\n",
      "Epoch 3951, Loss: 0.19606145471334457, Final Batch Loss: 0.0963050127029419\n",
      "Epoch 3952, Loss: 0.1789744757115841, Final Batch Loss: 0.05202076956629753\n",
      "Epoch 3953, Loss: 0.23732516169548035, Final Batch Loss: 0.11044643819332123\n",
      "Epoch 3954, Loss: 0.26686837524175644, Final Batch Loss: 0.12475169450044632\n",
      "Epoch 3955, Loss: 0.21506904065608978, Final Batch Loss: 0.13361264765262604\n",
      "Epoch 3956, Loss: 0.306731715798378, Final Batch Loss: 0.16143426299095154\n",
      "Epoch 3957, Loss: 0.22232316434383392, Final Batch Loss: 0.1321278065443039\n",
      "Epoch 3958, Loss: 0.19412343949079514, Final Batch Loss: 0.09872359037399292\n",
      "Epoch 3959, Loss: 0.187889464199543, Final Batch Loss: 0.08325052261352539\n",
      "Epoch 3960, Loss: 0.18803951889276505, Final Batch Loss: 0.07472660392522812\n",
      "Epoch 3961, Loss: 0.2095717266201973, Final Batch Loss: 0.08883456885814667\n",
      "Epoch 3962, Loss: 0.18267366290092468, Final Batch Loss: 0.12184367328882217\n",
      "Epoch 3963, Loss: 0.1807294487953186, Final Batch Loss: 0.08296343684196472\n",
      "Epoch 3964, Loss: 0.19401517510414124, Final Batch Loss: 0.11773078143596649\n",
      "Epoch 3965, Loss: 0.2007693201303482, Final Batch Loss: 0.07964841276407242\n",
      "Epoch 3966, Loss: 0.22098030149936676, Final Batch Loss: 0.08205926418304443\n",
      "Epoch 3967, Loss: 0.22764049470424652, Final Batch Loss: 0.09617288410663605\n",
      "Epoch 3968, Loss: 0.17571114748716354, Final Batch Loss: 0.10684973746538162\n",
      "Epoch 3969, Loss: 0.23998792469501495, Final Batch Loss: 0.14665384590625763\n",
      "Epoch 3970, Loss: 0.19903238862752914, Final Batch Loss: 0.12131325155496597\n",
      "Epoch 3971, Loss: 0.22165746986865997, Final Batch Loss: 0.09844857454299927\n",
      "Epoch 3972, Loss: 0.2626491114497185, Final Batch Loss: 0.1182488426566124\n",
      "Epoch 3973, Loss: 0.23181775957345963, Final Batch Loss: 0.1396513283252716\n",
      "Epoch 3974, Loss: 0.21496769785881042, Final Batch Loss: 0.10789913684129715\n",
      "Epoch 3975, Loss: 0.17112190276384354, Final Batch Loss: 0.07836806774139404\n",
      "Epoch 3976, Loss: 0.23819541931152344, Final Batch Loss: 0.10585516691207886\n",
      "Epoch 3977, Loss: 0.15536625683307648, Final Batch Loss: 0.06089334934949875\n",
      "Epoch 3978, Loss: 0.24344609677791595, Final Batch Loss: 0.12843120098114014\n",
      "Epoch 3979, Loss: 0.1665346324443817, Final Batch Loss: 0.07198041677474976\n",
      "Epoch 3980, Loss: 0.20710737258195877, Final Batch Loss: 0.09551196545362473\n",
      "Epoch 3981, Loss: 0.18510498851537704, Final Batch Loss: 0.07445569336414337\n",
      "Epoch 3982, Loss: 0.1895904392004013, Final Batch Loss: 0.11113763600587845\n",
      "Epoch 3983, Loss: 0.17127758637070656, Final Batch Loss: 0.06050950661301613\n",
      "Epoch 3984, Loss: 0.21748656034469604, Final Batch Loss: 0.13188888132572174\n",
      "Epoch 3985, Loss: 0.2542976588010788, Final Batch Loss: 0.15832531452178955\n",
      "Epoch 3986, Loss: 0.15608026087284088, Final Batch Loss: 0.0700395256280899\n",
      "Epoch 3987, Loss: 0.2891125828027725, Final Batch Loss: 0.15653833746910095\n",
      "Epoch 3988, Loss: 0.20021650940179825, Final Batch Loss: 0.11520663648843765\n",
      "Epoch 3989, Loss: 0.24842208623886108, Final Batch Loss: 0.12788937985897064\n",
      "Epoch 3990, Loss: 0.21867574751377106, Final Batch Loss: 0.15557199716567993\n",
      "Epoch 3991, Loss: 0.21853110939264297, Final Batch Loss: 0.10813938081264496\n",
      "Epoch 3992, Loss: 0.2115316167473793, Final Batch Loss: 0.10354773700237274\n",
      "Epoch 3993, Loss: 0.2455100491642952, Final Batch Loss: 0.13398756086826324\n",
      "Epoch 3994, Loss: 0.22310669720172882, Final Batch Loss: 0.11471905559301376\n",
      "Epoch 3995, Loss: 0.21972905844449997, Final Batch Loss: 0.11724543571472168\n",
      "Epoch 3996, Loss: 0.24636465311050415, Final Batch Loss: 0.12730056047439575\n",
      "Epoch 3997, Loss: 0.1874033808708191, Final Batch Loss: 0.09969719499349594\n",
      "Epoch 3998, Loss: 0.3014124631881714, Final Batch Loss: 0.1974256932735443\n",
      "Epoch 3999, Loss: 0.19845393300056458, Final Batch Loss: 0.07123559713363647\n",
      "Epoch 4000, Loss: 0.21499848365783691, Final Batch Loss: 0.12408462911844254\n",
      "Epoch 4001, Loss: 0.20866762846708298, Final Batch Loss: 0.10089540481567383\n",
      "Epoch 4002, Loss: 0.19484081864356995, Final Batch Loss: 0.0997198224067688\n",
      "Epoch 4003, Loss: 0.23175079375505447, Final Batch Loss: 0.12327715754508972\n",
      "Epoch 4004, Loss: 0.22338546812534332, Final Batch Loss: 0.08981244266033173\n",
      "Epoch 4005, Loss: 0.21306513994932175, Final Batch Loss: 0.12738342583179474\n",
      "Epoch 4006, Loss: 0.2670549303293228, Final Batch Loss: 0.13726156949996948\n",
      "Epoch 4007, Loss: 0.20364469289779663, Final Batch Loss: 0.1118454858660698\n",
      "Epoch 4008, Loss: 0.19988131523132324, Final Batch Loss: 0.08256834745407104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4009, Loss: 0.17964540421962738, Final Batch Loss: 0.06012243032455444\n",
      "Epoch 4010, Loss: 0.17815786972641945, Final Batch Loss: 0.0602787621319294\n",
      "Epoch 4011, Loss: 0.2637683004140854, Final Batch Loss: 0.14667947590351105\n",
      "Epoch 4012, Loss: 0.2379181683063507, Final Batch Loss: 0.12251855432987213\n",
      "Epoch 4013, Loss: 0.15573103725910187, Final Batch Loss: 0.07319442182779312\n",
      "Epoch 4014, Loss: 0.24187103658914566, Final Batch Loss: 0.15272030234336853\n",
      "Epoch 4015, Loss: 0.18263741582632065, Final Batch Loss: 0.10917633771896362\n",
      "Epoch 4016, Loss: 0.1778787150979042, Final Batch Loss: 0.08625223487615585\n",
      "Epoch 4017, Loss: 0.25221724063158035, Final Batch Loss: 0.12255241721868515\n",
      "Epoch 4018, Loss: 0.18606849759817123, Final Batch Loss: 0.10994015634059906\n",
      "Epoch 4019, Loss: 0.24097780883312225, Final Batch Loss: 0.17189517617225647\n",
      "Epoch 4020, Loss: 0.19370289891958237, Final Batch Loss: 0.09877985715866089\n",
      "Epoch 4021, Loss: 0.21372640877962112, Final Batch Loss: 0.12219180911779404\n",
      "Epoch 4022, Loss: 0.23624807596206665, Final Batch Loss: 0.16596156358718872\n",
      "Epoch 4023, Loss: 0.2227458730340004, Final Batch Loss: 0.10803836584091187\n",
      "Epoch 4024, Loss: 0.26072604954242706, Final Batch Loss: 0.12709525227546692\n",
      "Epoch 4025, Loss: 0.21590914577245712, Final Batch Loss: 0.1109483540058136\n",
      "Epoch 4026, Loss: 0.17308764159679413, Final Batch Loss: 0.10209895670413971\n",
      "Epoch 4027, Loss: 0.2788270637392998, Final Batch Loss: 0.1570122241973877\n",
      "Epoch 4028, Loss: 0.19855382293462753, Final Batch Loss: 0.1447942852973938\n",
      "Epoch 4029, Loss: 0.5143948048353195, Final Batch Loss: 0.38239097595214844\n",
      "Epoch 4030, Loss: 0.18335316330194473, Final Batch Loss: 0.07455146312713623\n",
      "Epoch 4031, Loss: 0.14482346177101135, Final Batch Loss: 0.08018531650304794\n",
      "Epoch 4032, Loss: 0.21413040906190872, Final Batch Loss: 0.1090841218829155\n",
      "Epoch 4033, Loss: 0.15082292258739471, Final Batch Loss: 0.07666625827550888\n",
      "Epoch 4034, Loss: 0.19707456231117249, Final Batch Loss: 0.09758153557777405\n",
      "Epoch 4035, Loss: 0.16654086858034134, Final Batch Loss: 0.07814445346593857\n",
      "Epoch 4036, Loss: 0.2452724203467369, Final Batch Loss: 0.16809223592281342\n",
      "Epoch 4037, Loss: 0.20189599692821503, Final Batch Loss: 0.08041924983263016\n",
      "Epoch 4038, Loss: 0.21289026737213135, Final Batch Loss: 0.13645052909851074\n",
      "Epoch 4039, Loss: 0.19677919894456863, Final Batch Loss: 0.1301715075969696\n",
      "Epoch 4040, Loss: 0.21053413301706314, Final Batch Loss: 0.1125601977109909\n",
      "Epoch 4041, Loss: 0.2079831138253212, Final Batch Loss: 0.09728652983903885\n",
      "Epoch 4042, Loss: 0.17551933228969574, Final Batch Loss: 0.07273043692111969\n",
      "Epoch 4043, Loss: 0.18722959607839584, Final Batch Loss: 0.09723865240812302\n",
      "Epoch 4044, Loss: 0.20582710951566696, Final Batch Loss: 0.1343819946050644\n",
      "Epoch 4045, Loss: 0.17699836939573288, Final Batch Loss: 0.07922244071960449\n",
      "Epoch 4046, Loss: 0.1868080571293831, Final Batch Loss: 0.11642803996801376\n",
      "Epoch 4047, Loss: 0.1471824049949646, Final Batch Loss: 0.06514366716146469\n",
      "Epoch 4048, Loss: 0.21015546470880508, Final Batch Loss: 0.0850757583975792\n",
      "Epoch 4049, Loss: 0.17508576065301895, Final Batch Loss: 0.08584626019001007\n",
      "Epoch 4050, Loss: 0.18077529966831207, Final Batch Loss: 0.1054607555270195\n",
      "Epoch 4051, Loss: 0.1354094259440899, Final Batch Loss: 0.08842318505048752\n",
      "Epoch 4052, Loss: 0.17104501277208328, Final Batch Loss: 0.07129062712192535\n",
      "Epoch 4053, Loss: 0.16829270496964455, Final Batch Loss: 0.05975540354847908\n",
      "Epoch 4054, Loss: 0.19769303500652313, Final Batch Loss: 0.09885071963071823\n",
      "Epoch 4055, Loss: 0.18415675312280655, Final Batch Loss: 0.04616231471300125\n",
      "Epoch 4056, Loss: 0.19707456231117249, Final Batch Loss: 0.14085964858531952\n",
      "Epoch 4057, Loss: 0.1720433086156845, Final Batch Loss: 0.06785595417022705\n",
      "Epoch 4058, Loss: 0.1774856112897396, Final Batch Loss: 0.0539124570786953\n",
      "Epoch 4059, Loss: 0.1787610799074173, Final Batch Loss: 0.07947225868701935\n",
      "Epoch 4060, Loss: 0.22670698165893555, Final Batch Loss: 0.12741562724113464\n",
      "Epoch 4061, Loss: 0.21839317679405212, Final Batch Loss: 0.10055636614561081\n",
      "Epoch 4062, Loss: 0.1798778548836708, Final Batch Loss: 0.08553478866815567\n",
      "Epoch 4063, Loss: 0.15380731225013733, Final Batch Loss: 0.09997960925102234\n",
      "Epoch 4064, Loss: 0.21241460740566254, Final Batch Loss: 0.12876743078231812\n",
      "Epoch 4065, Loss: 0.16459761187434196, Final Batch Loss: 0.05914444103837013\n",
      "Epoch 4066, Loss: 0.16010216251015663, Final Batch Loss: 0.056698936969041824\n",
      "Epoch 4067, Loss: 0.24641621112823486, Final Batch Loss: 0.1093536913394928\n",
      "Epoch 4068, Loss: 0.2220781370997429, Final Batch Loss: 0.12982112169265747\n",
      "Epoch 4069, Loss: 0.2234962284564972, Final Batch Loss: 0.1280675232410431\n",
      "Epoch 4070, Loss: 0.2024148665368557, Final Batch Loss: 0.05719376727938652\n",
      "Epoch 4071, Loss: 0.15454549342393875, Final Batch Loss: 0.04648144543170929\n",
      "Epoch 4072, Loss: 0.1732824333012104, Final Batch Loss: 0.049457091838121414\n",
      "Epoch 4073, Loss: 0.16009240597486496, Final Batch Loss: 0.06340020895004272\n",
      "Epoch 4074, Loss: 0.18429285287857056, Final Batch Loss: 0.08533650636672974\n",
      "Epoch 4075, Loss: 0.2911047786474228, Final Batch Loss: 0.1598622351884842\n",
      "Epoch 4076, Loss: 0.20308753848075867, Final Batch Loss: 0.1097107082605362\n",
      "Epoch 4077, Loss: 0.23938675969839096, Final Batch Loss: 0.15321633219718933\n",
      "Epoch 4078, Loss: 0.21324031800031662, Final Batch Loss: 0.08646503835916519\n",
      "Epoch 4079, Loss: 0.19198962301015854, Final Batch Loss: 0.08984438329935074\n",
      "Epoch 4080, Loss: 0.2091434821486473, Final Batch Loss: 0.09823977947235107\n",
      "Epoch 4081, Loss: 0.2338990643620491, Final Batch Loss: 0.11348927766084671\n",
      "Epoch 4082, Loss: 0.19374803453683853, Final Batch Loss: 0.10016123205423355\n",
      "Epoch 4083, Loss: 0.2972949370741844, Final Batch Loss: 0.20906563103199005\n",
      "Epoch 4084, Loss: 0.2563747763633728, Final Batch Loss: 0.12936295568943024\n",
      "Epoch 4085, Loss: 0.19513314217329025, Final Batch Loss: 0.0900716558098793\n",
      "Epoch 4086, Loss: 0.2466837242245674, Final Batch Loss: 0.1427897959947586\n",
      "Epoch 4087, Loss: 0.2818400636315346, Final Batch Loss: 0.09286830574274063\n",
      "Epoch 4088, Loss: 0.20076622068881989, Final Batch Loss: 0.08705908805131912\n",
      "Epoch 4089, Loss: 0.22969164699316025, Final Batch Loss: 0.14747433364391327\n",
      "Epoch 4090, Loss: 0.2378806173801422, Final Batch Loss: 0.14202041923999786\n",
      "Epoch 4091, Loss: 0.18044714629650116, Final Batch Loss: 0.08038599789142609\n",
      "Epoch 4092, Loss: 0.1653342843055725, Final Batch Loss: 0.09747160226106644\n",
      "Epoch 4093, Loss: 0.1593300998210907, Final Batch Loss: 0.08775188028812408\n",
      "Epoch 4094, Loss: 0.20085780322551727, Final Batch Loss: 0.09655669331550598\n",
      "Epoch 4095, Loss: 0.22826159000396729, Final Batch Loss: 0.09977491199970245\n",
      "Epoch 4096, Loss: 0.25762127339839935, Final Batch Loss: 0.1680893748998642\n",
      "Epoch 4097, Loss: 0.18516036123037338, Final Batch Loss: 0.11567659676074982\n",
      "Epoch 4098, Loss: 0.19005169719457626, Final Batch Loss: 0.07593926042318344\n",
      "Epoch 4099, Loss: 0.19539909809827805, Final Batch Loss: 0.06318039447069168\n",
      "Epoch 4100, Loss: 0.21684765070676804, Final Batch Loss: 0.1239193007349968\n",
      "Epoch 4101, Loss: 0.33857812732458115, Final Batch Loss: 0.22382666170597076\n",
      "Epoch 4102, Loss: 0.20134399831295013, Final Batch Loss: 0.11708037555217743\n",
      "Epoch 4103, Loss: 0.22401398420333862, Final Batch Loss: 0.12105798721313477\n",
      "Epoch 4104, Loss: 0.2018837183713913, Final Batch Loss: 0.12755459547042847\n",
      "Epoch 4105, Loss: 0.27687834948301315, Final Batch Loss: 0.09527737647294998\n",
      "Epoch 4106, Loss: 0.17092018574476242, Final Batch Loss: 0.09188269078731537\n",
      "Epoch 4107, Loss: 0.16995833814144135, Final Batch Loss: 0.10015492141246796\n",
      "Epoch 4108, Loss: 0.21412160247564316, Final Batch Loss: 0.14701950550079346\n",
      "Epoch 4109, Loss: 0.2919901907444, Final Batch Loss: 0.1343686878681183\n",
      "Epoch 4110, Loss: 0.20746447145938873, Final Batch Loss: 0.09568899869918823\n",
      "Epoch 4111, Loss: 0.17429687827825546, Final Batch Loss: 0.0660221129655838\n",
      "Epoch 4112, Loss: 0.17708730325102806, Final Batch Loss: 0.06058194860816002\n",
      "Epoch 4113, Loss: 0.16866328567266464, Final Batch Loss: 0.08453609049320221\n",
      "Epoch 4114, Loss: 0.16531699895858765, Final Batch Loss: 0.09396810084581375\n",
      "Epoch 4115, Loss: 0.20379869639873505, Final Batch Loss: 0.07132159173488617\n",
      "Epoch 4116, Loss: 0.16561757028102875, Final Batch Loss: 0.07284001260995865\n",
      "Epoch 4117, Loss: 0.19578394293785095, Final Batch Loss: 0.07351744920015335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4118, Loss: 0.19589845836162567, Final Batch Loss: 0.07265040278434753\n",
      "Epoch 4119, Loss: 0.16439197212457657, Final Batch Loss: 0.08739820122718811\n",
      "Epoch 4120, Loss: 0.20331959426403046, Final Batch Loss: 0.12835852801799774\n",
      "Epoch 4121, Loss: 0.13758688792586327, Final Batch Loss: 0.048539940267801285\n",
      "Epoch 4122, Loss: 0.1706216186285019, Final Batch Loss: 0.08520564436912537\n",
      "Epoch 4123, Loss: 0.22187252342700958, Final Batch Loss: 0.11162786185741425\n",
      "Epoch 4124, Loss: 0.192483052611351, Final Batch Loss: 0.12807001173496246\n",
      "Epoch 4125, Loss: 0.12958795577287674, Final Batch Loss: 0.053772613406181335\n",
      "Epoch 4126, Loss: 0.18765641003847122, Final Batch Loss: 0.059323765337467194\n",
      "Epoch 4127, Loss: 0.16646165400743484, Final Batch Loss: 0.052291885018348694\n",
      "Epoch 4128, Loss: 0.29139668494462967, Final Batch Loss: 0.17217691242694855\n",
      "Epoch 4129, Loss: 0.17044084519147873, Final Batch Loss: 0.09736552834510803\n",
      "Epoch 4130, Loss: 0.18749909102916718, Final Batch Loss: 0.08089371025562286\n",
      "Epoch 4131, Loss: 0.21059231460094452, Final Batch Loss: 0.08309924602508545\n",
      "Epoch 4132, Loss: 0.17209475487470627, Final Batch Loss: 0.08661391586065292\n",
      "Epoch 4133, Loss: 0.24153891950845718, Final Batch Loss: 0.15158505737781525\n",
      "Epoch 4134, Loss: 0.17581194639205933, Final Batch Loss: 0.1001247838139534\n",
      "Epoch 4135, Loss: 0.22435811161994934, Final Batch Loss: 0.07081469893455505\n",
      "Epoch 4136, Loss: 0.18752846121788025, Final Batch Loss: 0.08795060962438583\n",
      "Epoch 4137, Loss: 0.23045656830072403, Final Batch Loss: 0.13248522579669952\n",
      "Epoch 4138, Loss: 0.22770535200834274, Final Batch Loss: 0.12191938608884811\n",
      "Epoch 4139, Loss: 0.17602071911096573, Final Batch Loss: 0.0625876933336258\n",
      "Epoch 4140, Loss: 0.20251426100730896, Final Batch Loss: 0.07440702617168427\n",
      "Epoch 4141, Loss: 0.2874923646450043, Final Batch Loss: 0.1861712634563446\n",
      "Epoch 4142, Loss: 0.19463004916906357, Final Batch Loss: 0.08792977035045624\n",
      "Epoch 4143, Loss: 0.17233993113040924, Final Batch Loss: 0.08840665221214294\n",
      "Epoch 4144, Loss: 0.20899352431297302, Final Batch Loss: 0.0741799920797348\n",
      "Epoch 4145, Loss: 0.19379891455173492, Final Batch Loss: 0.11616794764995575\n",
      "Epoch 4146, Loss: 0.18384642899036407, Final Batch Loss: 0.10324405133724213\n",
      "Epoch 4147, Loss: 0.2070835679769516, Final Batch Loss: 0.10528641939163208\n",
      "Epoch 4148, Loss: 0.20621546357870102, Final Batch Loss: 0.09421339631080627\n",
      "Epoch 4149, Loss: 0.18925175815820694, Final Batch Loss: 0.08975265175104141\n",
      "Epoch 4150, Loss: 0.15079541504383087, Final Batch Loss: 0.0859375074505806\n",
      "Epoch 4151, Loss: 0.16943155601620674, Final Batch Loss: 0.10766603797674179\n",
      "Epoch 4152, Loss: 0.1985279768705368, Final Batch Loss: 0.1241152361035347\n",
      "Epoch 4153, Loss: 0.1552881859242916, Final Batch Loss: 0.09323977679014206\n",
      "Epoch 4154, Loss: 0.20681403577327728, Final Batch Loss: 0.10979034751653671\n",
      "Epoch 4155, Loss: 0.2349783256649971, Final Batch Loss: 0.12192226946353912\n",
      "Epoch 4156, Loss: 0.24878936260938644, Final Batch Loss: 0.17173665761947632\n",
      "Epoch 4157, Loss: 0.16075654327869415, Final Batch Loss: 0.06491658091545105\n",
      "Epoch 4158, Loss: 0.19611072540283203, Final Batch Loss: 0.11860930174589157\n",
      "Epoch 4159, Loss: 0.1553848721086979, Final Batch Loss: 0.09424947202205658\n",
      "Epoch 4160, Loss: 0.15878164023160934, Final Batch Loss: 0.07428329437971115\n",
      "Epoch 4161, Loss: 0.15108317136764526, Final Batch Loss: 0.09732011705636978\n",
      "Epoch 4162, Loss: 0.1494675651192665, Final Batch Loss: 0.08505617827177048\n",
      "Epoch 4163, Loss: 0.17445078492164612, Final Batch Loss: 0.09762515127658844\n",
      "Epoch 4164, Loss: 0.144714817404747, Final Batch Loss: 0.08959618955850601\n",
      "Epoch 4165, Loss: 0.1870015263557434, Final Batch Loss: 0.08927350491285324\n",
      "Epoch 4166, Loss: 0.17324061691761017, Final Batch Loss: 0.06759984791278839\n",
      "Epoch 4167, Loss: 0.1801753044128418, Final Batch Loss: 0.10395672172307968\n",
      "Epoch 4168, Loss: 0.16729054600000381, Final Batch Loss: 0.0690603032708168\n",
      "Epoch 4169, Loss: 0.2563302144408226, Final Batch Loss: 0.14750084280967712\n",
      "Epoch 4170, Loss: 0.16922979056835175, Final Batch Loss: 0.06803687661886215\n",
      "Epoch 4171, Loss: 0.16319865733385086, Final Batch Loss: 0.07710354030132294\n",
      "Epoch 4172, Loss: 0.1747712343931198, Final Batch Loss: 0.0881035327911377\n",
      "Epoch 4173, Loss: 0.19924332946538925, Final Batch Loss: 0.11548758298158646\n",
      "Epoch 4174, Loss: 0.21403523534536362, Final Batch Loss: 0.10721681267023087\n",
      "Epoch 4175, Loss: 0.20711741596460342, Final Batch Loss: 0.12083729356527328\n",
      "Epoch 4176, Loss: 0.2305615171790123, Final Batch Loss: 0.15613839030265808\n",
      "Epoch 4177, Loss: 0.1211836040019989, Final Batch Loss: 0.03771578520536423\n",
      "Epoch 4178, Loss: 0.2134007290005684, Final Batch Loss: 0.12318150699138641\n",
      "Epoch 4179, Loss: 0.18333499878644943, Final Batch Loss: 0.07330045849084854\n",
      "Epoch 4180, Loss: 0.25984442979097366, Final Batch Loss: 0.15469199419021606\n",
      "Epoch 4181, Loss: 0.1464701108634472, Final Batch Loss: 0.06127509847283363\n",
      "Epoch 4182, Loss: 0.21527905762195587, Final Batch Loss: 0.15877477824687958\n",
      "Epoch 4183, Loss: 0.34282660484313965, Final Batch Loss: 0.18614503741264343\n",
      "Epoch 4184, Loss: 0.18146086484193802, Final Batch Loss: 0.10214465856552124\n",
      "Epoch 4185, Loss: 0.19918808341026306, Final Batch Loss: 0.13578759133815765\n",
      "Epoch 4186, Loss: 0.14988116174936295, Final Batch Loss: 0.0672321543097496\n",
      "Epoch 4187, Loss: 0.16986393183469772, Final Batch Loss: 0.1016467809677124\n",
      "Epoch 4188, Loss: 0.22477802634239197, Final Batch Loss: 0.14211075007915497\n",
      "Epoch 4189, Loss: 0.19016773253679276, Final Batch Loss: 0.10618829727172852\n",
      "Epoch 4190, Loss: 0.16096169874072075, Final Batch Loss: 0.06174660846590996\n",
      "Epoch 4191, Loss: 0.1555737443268299, Final Batch Loss: 0.033987876027822495\n",
      "Epoch 4192, Loss: 0.18813161551952362, Final Batch Loss: 0.08560847491025925\n",
      "Epoch 4193, Loss: 0.17972620576620102, Final Batch Loss: 0.09677650034427643\n",
      "Epoch 4194, Loss: 0.18973909318447113, Final Batch Loss: 0.11781349033117294\n",
      "Epoch 4195, Loss: 0.2154914289712906, Final Batch Loss: 0.11472971737384796\n",
      "Epoch 4196, Loss: 0.19510258734226227, Final Batch Loss: 0.1168530136346817\n",
      "Epoch 4197, Loss: 0.1407787799835205, Final Batch Loss: 0.06058592349290848\n",
      "Epoch 4198, Loss: 0.19686224311590195, Final Batch Loss: 0.113555908203125\n",
      "Epoch 4199, Loss: 0.17789499461650848, Final Batch Loss: 0.06643243134021759\n",
      "Epoch 4200, Loss: 0.18763568252325058, Final Batch Loss: 0.10851657390594482\n",
      "Epoch 4201, Loss: 0.18113336712121964, Final Batch Loss: 0.07660216838121414\n",
      "Epoch 4202, Loss: 0.15282977372407913, Final Batch Loss: 0.08198940008878708\n",
      "Epoch 4203, Loss: 0.16571513563394547, Final Batch Loss: 0.05146109312772751\n",
      "Epoch 4204, Loss: 0.1874052882194519, Final Batch Loss: 0.0920790582895279\n",
      "Epoch 4205, Loss: 0.19037310406565666, Final Batch Loss: 0.13188014924526215\n",
      "Epoch 4206, Loss: 0.28703881055116653, Final Batch Loss: 0.16437119245529175\n",
      "Epoch 4207, Loss: 0.18860717117786407, Final Batch Loss: 0.069712795317173\n",
      "Epoch 4208, Loss: 0.18371516466140747, Final Batch Loss: 0.11296185106039047\n",
      "Epoch 4209, Loss: 0.19665814191102982, Final Batch Loss: 0.05220816284418106\n",
      "Epoch 4210, Loss: 0.1742415726184845, Final Batch Loss: 0.059297166764736176\n",
      "Epoch 4211, Loss: 0.19874992966651917, Final Batch Loss: 0.09084273874759674\n",
      "Epoch 4212, Loss: 0.1861443817615509, Final Batch Loss: 0.08971083909273148\n",
      "Epoch 4213, Loss: 0.1308533437550068, Final Batch Loss: 0.05772336944937706\n",
      "Epoch 4214, Loss: 0.21895893663167953, Final Batch Loss: 0.11556858569383621\n",
      "Epoch 4215, Loss: 0.17898567020893097, Final Batch Loss: 0.08776078373193741\n",
      "Epoch 4216, Loss: 0.17945131659507751, Final Batch Loss: 0.07206720113754272\n",
      "Epoch 4217, Loss: 0.22951003164052963, Final Batch Loss: 0.13825562596321106\n",
      "Epoch 4218, Loss: 0.14964989572763443, Final Batch Loss: 0.0822521522641182\n",
      "Epoch 4219, Loss: 0.2094041109085083, Final Batch Loss: 0.1053691878914833\n",
      "Epoch 4220, Loss: 0.15672046691179276, Final Batch Loss: 0.0927027016878128\n",
      "Epoch 4221, Loss: 0.17469193041324615, Final Batch Loss: 0.06648577749729156\n",
      "Epoch 4222, Loss: 0.18392404168844223, Final Batch Loss: 0.11728596687316895\n",
      "Epoch 4223, Loss: 0.16395001113414764, Final Batch Loss: 0.08347512781620026\n",
      "Epoch 4224, Loss: 0.29923922568559647, Final Batch Loss: 0.08923660963773727\n",
      "Epoch 4225, Loss: 0.22783682495355606, Final Batch Loss: 0.17313560843467712\n",
      "Epoch 4226, Loss: 0.19068999588489532, Final Batch Loss: 0.10237930715084076\n",
      "Epoch 4227, Loss: 0.1638655588030815, Final Batch Loss: 0.08824219554662704\n",
      "Epoch 4228, Loss: 0.2728816717863083, Final Batch Loss: 0.14887182414531708\n",
      "Epoch 4229, Loss: 0.1860809400677681, Final Batch Loss: 0.06952298432588577\n",
      "Epoch 4230, Loss: 0.1672431230545044, Final Batch Loss: 0.09398437291383743\n",
      "Epoch 4231, Loss: 0.1755324900150299, Final Batch Loss: 0.11038020998239517\n",
      "Epoch 4232, Loss: 0.1617213636636734, Final Batch Loss: 0.080047108232975\n",
      "Epoch 4233, Loss: 0.17424976080656052, Final Batch Loss: 0.07862678170204163\n",
      "Epoch 4234, Loss: 0.14916065335273743, Final Batch Loss: 0.08216898143291473\n",
      "Epoch 4235, Loss: 0.17008230090141296, Final Batch Loss: 0.08100301772356033\n",
      "Epoch 4236, Loss: 0.2180710732936859, Final Batch Loss: 0.14347383379936218\n",
      "Epoch 4237, Loss: 0.2095058262348175, Final Batch Loss: 0.14490921795368195\n",
      "Epoch 4238, Loss: 0.15969029068946838, Final Batch Loss: 0.09597952663898468\n",
      "Epoch 4239, Loss: 0.15851852297782898, Final Batch Loss: 0.0568251833319664\n",
      "Epoch 4240, Loss: 0.1764492467045784, Final Batch Loss: 0.08926708251237869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4241, Loss: 0.18030499666929245, Final Batch Loss: 0.06506501138210297\n",
      "Epoch 4242, Loss: 0.21602606028318405, Final Batch Loss: 0.14758946001529694\n",
      "Epoch 4243, Loss: 0.17790398746728897, Final Batch Loss: 0.06559667736291885\n",
      "Epoch 4244, Loss: 0.2258848324418068, Final Batch Loss: 0.09653174132108688\n",
      "Epoch 4245, Loss: 0.19533754885196686, Final Batch Loss: 0.0707625150680542\n",
      "Epoch 4246, Loss: 0.17729148268699646, Final Batch Loss: 0.10166065394878387\n",
      "Epoch 4247, Loss: 0.15870164334774017, Final Batch Loss: 0.04849865287542343\n",
      "Epoch 4248, Loss: 0.1547006219625473, Final Batch Loss: 0.07142869383096695\n",
      "Epoch 4249, Loss: 0.14529204368591309, Final Batch Loss: 0.06368949264287949\n",
      "Epoch 4250, Loss: 0.2928409203886986, Final Batch Loss: 0.2066233605146408\n",
      "Epoch 4251, Loss: 0.20301764458417892, Final Batch Loss: 0.11894957721233368\n",
      "Epoch 4252, Loss: 0.20323941856622696, Final Batch Loss: 0.09391601383686066\n",
      "Epoch 4253, Loss: 0.16508352011442184, Final Batch Loss: 0.08512818068265915\n",
      "Epoch 4254, Loss: 0.16730695217847824, Final Batch Loss: 0.05926284193992615\n",
      "Epoch 4255, Loss: 0.2141769453883171, Final Batch Loss: 0.12343670427799225\n",
      "Epoch 4256, Loss: 0.17375939339399338, Final Batch Loss: 0.07598660886287689\n",
      "Epoch 4257, Loss: 0.16038839146494865, Final Batch Loss: 0.0605657584965229\n",
      "Epoch 4258, Loss: 0.2076607048511505, Final Batch Loss: 0.10038383305072784\n",
      "Epoch 4259, Loss: 0.183244951069355, Final Batch Loss: 0.07256034761667252\n",
      "Epoch 4260, Loss: 0.16589269042015076, Final Batch Loss: 0.08969727158546448\n",
      "Epoch 4261, Loss: 0.1845647096633911, Final Batch Loss: 0.09832387417554855\n",
      "Epoch 4262, Loss: 0.12920083478093147, Final Batch Loss: 0.04810737445950508\n",
      "Epoch 4263, Loss: 0.20963140577077866, Final Batch Loss: 0.0917116031050682\n",
      "Epoch 4264, Loss: 0.17073475569486618, Final Batch Loss: 0.08667749166488647\n",
      "Epoch 4265, Loss: 0.1778377890586853, Final Batch Loss: 0.09787970781326294\n",
      "Epoch 4266, Loss: 0.1635158434510231, Final Batch Loss: 0.07201334834098816\n",
      "Epoch 4267, Loss: 0.15817522257566452, Final Batch Loss: 0.07595537602901459\n",
      "Epoch 4268, Loss: 0.16758230328559875, Final Batch Loss: 0.06816479563713074\n",
      "Epoch 4269, Loss: 0.16008451581001282, Final Batch Loss: 0.0884510949254036\n",
      "Epoch 4270, Loss: 0.21601545065641403, Final Batch Loss: 0.12394452095031738\n",
      "Epoch 4271, Loss: 0.16621879488229752, Final Batch Loss: 0.09895899146795273\n",
      "Epoch 4272, Loss: 0.19252464920282364, Final Batch Loss: 0.09736420214176178\n",
      "Epoch 4273, Loss: 0.18559317290782928, Final Batch Loss: 0.08475299924612045\n",
      "Epoch 4274, Loss: 0.1283053196966648, Final Batch Loss: 0.05764526501297951\n",
      "Epoch 4275, Loss: 0.15560658276081085, Final Batch Loss: 0.06925506144762039\n",
      "Epoch 4276, Loss: 0.18776793777942657, Final Batch Loss: 0.10275204479694366\n",
      "Epoch 4277, Loss: 0.12169798091053963, Final Batch Loss: 0.05654268339276314\n",
      "Epoch 4278, Loss: 0.1532808542251587, Final Batch Loss: 0.10312176495790482\n",
      "Epoch 4279, Loss: 0.21411743015050888, Final Batch Loss: 0.11767502874135971\n",
      "Epoch 4280, Loss: 0.2789781987667084, Final Batch Loss: 0.19352000951766968\n",
      "Epoch 4281, Loss: 0.17190932482481003, Final Batch Loss: 0.1294546127319336\n",
      "Epoch 4282, Loss: 0.18747233599424362, Final Batch Loss: 0.11051119863986969\n",
      "Epoch 4283, Loss: 0.1516019105911255, Final Batch Loss: 0.03956102579832077\n",
      "Epoch 4284, Loss: 0.15162615850567818, Final Batch Loss: 0.0899176374077797\n",
      "Epoch 4285, Loss: 0.24710384756326675, Final Batch Loss: 0.14598345756530762\n",
      "Epoch 4286, Loss: 0.1537371389567852, Final Batch Loss: 0.05139431729912758\n",
      "Epoch 4287, Loss: 0.17428168654441833, Final Batch Loss: 0.06941550970077515\n",
      "Epoch 4288, Loss: 0.2300877869129181, Final Batch Loss: 0.122298963367939\n",
      "Epoch 4289, Loss: 0.20395753532648087, Final Batch Loss: 0.11435424536466599\n",
      "Epoch 4290, Loss: 0.22947562485933304, Final Batch Loss: 0.128880575299263\n",
      "Epoch 4291, Loss: 0.1775849238038063, Final Batch Loss: 0.09600446373224258\n",
      "Epoch 4292, Loss: 0.23253994435071945, Final Batch Loss: 0.14299733936786652\n",
      "Epoch 4293, Loss: 0.1955113261938095, Final Batch Loss: 0.13015958666801453\n",
      "Epoch 4294, Loss: 0.20419842004776, Final Batch Loss: 0.08593717962503433\n",
      "Epoch 4295, Loss: 0.1611308455467224, Final Batch Loss: 0.06562800705432892\n",
      "Epoch 4296, Loss: 0.15738704055547714, Final Batch Loss: 0.07332128286361694\n",
      "Epoch 4297, Loss: 0.18469392508268356, Final Batch Loss: 0.05607835203409195\n",
      "Epoch 4298, Loss: 0.2483019009232521, Final Batch Loss: 0.1508401334285736\n",
      "Epoch 4299, Loss: 0.1744319275021553, Final Batch Loss: 0.06932657957077026\n",
      "Epoch 4300, Loss: 0.2773975357413292, Final Batch Loss: 0.19498950242996216\n",
      "Epoch 4301, Loss: 0.18273060768842697, Final Batch Loss: 0.09410089254379272\n",
      "Epoch 4302, Loss: 0.19768063724040985, Final Batch Loss: 0.0861765518784523\n",
      "Epoch 4303, Loss: 0.14138257503509521, Final Batch Loss: 0.05050842463970184\n",
      "Epoch 4304, Loss: 0.19315364956855774, Final Batch Loss: 0.11278755962848663\n",
      "Epoch 4305, Loss: 0.19714796543121338, Final Batch Loss: 0.09361838549375534\n",
      "Epoch 4306, Loss: 0.17511068284511566, Final Batch Loss: 0.08600518852472305\n",
      "Epoch 4307, Loss: 0.27136146277189255, Final Batch Loss: 0.2061336785554886\n",
      "Epoch 4308, Loss: 0.1965818628668785, Final Batch Loss: 0.08055663108825684\n",
      "Epoch 4309, Loss: 0.13933980464935303, Final Batch Loss: 0.07864721119403839\n",
      "Epoch 4310, Loss: 0.14511756971478462, Final Batch Loss: 0.09756182134151459\n",
      "Epoch 4311, Loss: 0.16626520454883575, Final Batch Loss: 0.09627048671245575\n",
      "Epoch 4312, Loss: 0.16411767899990082, Final Batch Loss: 0.06721741706132889\n",
      "Epoch 4313, Loss: 0.18922943621873856, Final Batch Loss: 0.10218235105276108\n",
      "Epoch 4314, Loss: 0.23184557259082794, Final Batch Loss: 0.1410530060529709\n",
      "Epoch 4315, Loss: 0.16724784672260284, Final Batch Loss: 0.08651643991470337\n",
      "Epoch 4316, Loss: 0.22018448263406754, Final Batch Loss: 0.11598483473062515\n",
      "Epoch 4317, Loss: 0.15134023129940033, Final Batch Loss: 0.05344388633966446\n",
      "Epoch 4318, Loss: 0.21900322288274765, Final Batch Loss: 0.16703879833221436\n",
      "Epoch 4319, Loss: 0.1584584265947342, Final Batch Loss: 0.0727199837565422\n",
      "Epoch 4320, Loss: 0.19660857319831848, Final Batch Loss: 0.11806132644414902\n",
      "Epoch 4321, Loss: 0.14329810440540314, Final Batch Loss: 0.08961400389671326\n",
      "Epoch 4322, Loss: 0.12507390603423119, Final Batch Loss: 0.0425775982439518\n",
      "Epoch 4323, Loss: 0.123064324259758, Final Batch Loss: 0.03872734308242798\n",
      "Epoch 4324, Loss: 0.19616565108299255, Final Batch Loss: 0.10920466482639313\n",
      "Epoch 4325, Loss: 0.2011014074087143, Final Batch Loss: 0.09691484272480011\n",
      "Epoch 4326, Loss: 0.211566761136055, Final Batch Loss: 0.13911442458629608\n",
      "Epoch 4327, Loss: 0.1788095086812973, Final Batch Loss: 0.10359231382608414\n",
      "Epoch 4328, Loss: 0.13260838761925697, Final Batch Loss: 0.05057958886027336\n",
      "Epoch 4329, Loss: 0.16927764937281609, Final Batch Loss: 0.12139194458723068\n",
      "Epoch 4330, Loss: 0.24055032432079315, Final Batch Loss: 0.14363797008991241\n",
      "Epoch 4331, Loss: 0.19739722460508347, Final Batch Loss: 0.1132887750864029\n",
      "Epoch 4332, Loss: 0.21694790571928024, Final Batch Loss: 0.14127002656459808\n",
      "Epoch 4333, Loss: 0.24277277290821075, Final Batch Loss: 0.1689586192369461\n",
      "Epoch 4334, Loss: 0.17683842033147812, Final Batch Loss: 0.09153895080089569\n",
      "Epoch 4335, Loss: 0.13213462755084038, Final Batch Loss: 0.04444864019751549\n",
      "Epoch 4336, Loss: 0.19809464365243912, Final Batch Loss: 0.07192113250494003\n",
      "Epoch 4337, Loss: 0.1779564544558525, Final Batch Loss: 0.0839306190609932\n",
      "Epoch 4338, Loss: 0.20796577632427216, Final Batch Loss: 0.1158638745546341\n",
      "Epoch 4339, Loss: 0.14758466184139252, Final Batch Loss: 0.07723630964756012\n",
      "Epoch 4340, Loss: 0.18940362334251404, Final Batch Loss: 0.0851341262459755\n",
      "Epoch 4341, Loss: 0.16628814861178398, Final Batch Loss: 0.053117360919713974\n",
      "Epoch 4342, Loss: 0.15083372592926025, Final Batch Loss: 0.05541028827428818\n",
      "Epoch 4343, Loss: 0.17100493609905243, Final Batch Loss: 0.09468111395835876\n",
      "Epoch 4344, Loss: 0.15900354832410812, Final Batch Loss: 0.0922602117061615\n",
      "Epoch 4345, Loss: 0.1594133824110031, Final Batch Loss: 0.09067151695489883\n",
      "Epoch 4346, Loss: 0.13462993130087852, Final Batch Loss: 0.0486132837831974\n",
      "Epoch 4347, Loss: 0.1504393108189106, Final Batch Loss: 0.09109463542699814\n",
      "Epoch 4348, Loss: 0.15816599130630493, Final Batch Loss: 0.06854034960269928\n",
      "Epoch 4349, Loss: 0.14917218312621117, Final Batch Loss: 0.047339122742414474\n",
      "Epoch 4350, Loss: 0.274959459900856, Final Batch Loss: 0.19412440061569214\n",
      "Epoch 4351, Loss: 0.13812895119190216, Final Batch Loss: 0.06408149003982544\n",
      "Epoch 4352, Loss: 0.2943572849035263, Final Batch Loss: 0.05790784955024719\n",
      "Epoch 4353, Loss: 0.15451211854815483, Final Batch Loss: 0.047939885407686234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4354, Loss: 0.18295805156230927, Final Batch Loss: 0.07906189560890198\n",
      "Epoch 4355, Loss: 0.14353962242603302, Final Batch Loss: 0.06654088944196701\n",
      "Epoch 4356, Loss: 0.12318816408514977, Final Batch Loss: 0.07392694801092148\n",
      "Epoch 4357, Loss: 0.13000472635030746, Final Batch Loss: 0.046797625720500946\n",
      "Epoch 4358, Loss: 0.15270842984318733, Final Batch Loss: 0.04102542623877525\n",
      "Epoch 4359, Loss: 0.19171208143234253, Final Batch Loss: 0.11403964459896088\n",
      "Epoch 4360, Loss: 0.14418574422597885, Final Batch Loss: 0.07181010395288467\n",
      "Epoch 4361, Loss: 0.143182672560215, Final Batch Loss: 0.06150759011507034\n",
      "Epoch 4362, Loss: 0.19959016889333725, Final Batch Loss: 0.07545842230319977\n",
      "Epoch 4363, Loss: 0.1662382110953331, Final Batch Loss: 0.08429886400699615\n",
      "Epoch 4364, Loss: 0.17268356680870056, Final Batch Loss: 0.09120890498161316\n",
      "Epoch 4365, Loss: 0.12463247030973434, Final Batch Loss: 0.06752572953701019\n",
      "Epoch 4366, Loss: 0.12221105024218559, Final Batch Loss: 0.05184303596615791\n",
      "Epoch 4367, Loss: 0.17626029253005981, Final Batch Loss: 0.09280537813901901\n",
      "Epoch 4368, Loss: 0.1787775158882141, Final Batch Loss: 0.08112232387065887\n",
      "Epoch 4369, Loss: 0.15521462261676788, Final Batch Loss: 0.07090003788471222\n",
      "Epoch 4370, Loss: 0.1480799801647663, Final Batch Loss: 0.05686334893107414\n",
      "Epoch 4371, Loss: 0.20425482094287872, Final Batch Loss: 0.11722689121961594\n",
      "Epoch 4372, Loss: 0.16346993297338486, Final Batch Loss: 0.07959160208702087\n",
      "Epoch 4373, Loss: 0.2051583081483841, Final Batch Loss: 0.135946124792099\n",
      "Epoch 4374, Loss: 0.18197543174028397, Final Batch Loss: 0.07617964595556259\n",
      "Epoch 4375, Loss: 0.17977889627218246, Final Batch Loss: 0.09394940733909607\n",
      "Epoch 4376, Loss: 0.15273915976285934, Final Batch Loss: 0.08768441528081894\n",
      "Epoch 4377, Loss: 0.13317304104566574, Final Batch Loss: 0.07714254409074783\n",
      "Epoch 4378, Loss: 0.18198170512914658, Final Batch Loss: 0.08273498713970184\n",
      "Epoch 4379, Loss: 0.20611923933029175, Final Batch Loss: 0.08218255639076233\n",
      "Epoch 4380, Loss: 0.1644018366932869, Final Batch Loss: 0.09445144236087799\n",
      "Epoch 4381, Loss: 0.19958225637674332, Final Batch Loss: 0.0964023694396019\n",
      "Epoch 4382, Loss: 0.19758129864931107, Final Batch Loss: 0.12735691666603088\n",
      "Epoch 4383, Loss: 0.18844321370124817, Final Batch Loss: 0.08807212114334106\n",
      "Epoch 4384, Loss: 0.16958355158567429, Final Batch Loss: 0.0946820080280304\n",
      "Epoch 4385, Loss: 0.19891205430030823, Final Batch Loss: 0.06261061131954193\n",
      "Epoch 4386, Loss: 0.1268906258046627, Final Batch Loss: 0.0695546492934227\n",
      "Epoch 4387, Loss: 0.1711101159453392, Final Batch Loss: 0.08253339678049088\n",
      "Epoch 4388, Loss: 0.2554512768983841, Final Batch Loss: 0.17714159190654755\n",
      "Epoch 4389, Loss: 0.13190440088510513, Final Batch Loss: 0.06914377212524414\n",
      "Epoch 4390, Loss: 0.19881518930196762, Final Batch Loss: 0.07926255464553833\n",
      "Epoch 4391, Loss: 0.2001044675707817, Final Batch Loss: 0.13630062341690063\n",
      "Epoch 4392, Loss: 0.2053883746266365, Final Batch Loss: 0.1404464989900589\n",
      "Epoch 4393, Loss: 0.1370059698820114, Final Batch Loss: 0.07655427604913712\n",
      "Epoch 4394, Loss: 0.16156966984272003, Final Batch Loss: 0.05701104551553726\n",
      "Epoch 4395, Loss: 0.1507508121430874, Final Batch Loss: 0.09621302783489227\n",
      "Epoch 4396, Loss: 0.13370321691036224, Final Batch Loss: 0.05415178835391998\n",
      "Epoch 4397, Loss: 0.186519056558609, Final Batch Loss: 0.057246968150138855\n",
      "Epoch 4398, Loss: 0.1708102598786354, Final Batch Loss: 0.08781073242425919\n",
      "Epoch 4399, Loss: 0.2614675313234329, Final Batch Loss: 0.14766350388526917\n",
      "Epoch 4400, Loss: 0.15219463407993317, Final Batch Loss: 0.08458079397678375\n",
      "Epoch 4401, Loss: 0.2334049716591835, Final Batch Loss: 0.15940500795841217\n",
      "Epoch 4402, Loss: 0.11270421743392944, Final Batch Loss: 0.04325278103351593\n",
      "Epoch 4403, Loss: 0.16453273594379425, Final Batch Loss: 0.05745179206132889\n",
      "Epoch 4404, Loss: 0.16606851667165756, Final Batch Loss: 0.1079372838139534\n",
      "Epoch 4405, Loss: 0.2548803985118866, Final Batch Loss: 0.1585172414779663\n",
      "Epoch 4406, Loss: 0.17716677486896515, Final Batch Loss: 0.09328319132328033\n",
      "Epoch 4407, Loss: 0.15401611104607582, Final Batch Loss: 0.06205093488097191\n",
      "Epoch 4408, Loss: 0.168966893106699, Final Batch Loss: 0.05552584305405617\n",
      "Epoch 4409, Loss: 0.12166450545191765, Final Batch Loss: 0.06886430829763412\n",
      "Epoch 4410, Loss: 0.15648268908262253, Final Batch Loss: 0.06741660833358765\n",
      "Epoch 4411, Loss: 0.164434514939785, Final Batch Loss: 0.07525887340307236\n",
      "Epoch 4412, Loss: 0.15047194063663483, Final Batch Loss: 0.03863611817359924\n",
      "Epoch 4413, Loss: 0.14121751114726067, Final Batch Loss: 0.048015888780355453\n",
      "Epoch 4414, Loss: 0.15200691297650337, Final Batch Loss: 0.10108528286218643\n",
      "Epoch 4415, Loss: 0.16287462413311005, Final Batch Loss: 0.09324897825717926\n",
      "Epoch 4416, Loss: 0.18152231723070145, Final Batch Loss: 0.10535865277051926\n",
      "Epoch 4417, Loss: 0.185002401471138, Final Batch Loss: 0.07975433021783829\n",
      "Epoch 4418, Loss: 0.2053680345416069, Final Batch Loss: 0.09111233055591583\n",
      "Epoch 4419, Loss: 0.1260811910033226, Final Batch Loss: 0.06288393586874008\n",
      "Epoch 4420, Loss: 0.19346198439598083, Final Batch Loss: 0.09511096775531769\n",
      "Epoch 4421, Loss: 0.21720445156097412, Final Batch Loss: 0.12959207594394684\n",
      "Epoch 4422, Loss: 0.13561082258820534, Final Batch Loss: 0.052317481487989426\n",
      "Epoch 4423, Loss: 0.16306403279304504, Final Batch Loss: 0.060842372477054596\n",
      "Epoch 4424, Loss: 0.21026401221752167, Final Batch Loss: 0.08007967472076416\n",
      "Epoch 4425, Loss: 0.18316398561000824, Final Batch Loss: 0.08368253707885742\n",
      "Epoch 4426, Loss: 0.17648059874773026, Final Batch Loss: 0.06710241734981537\n",
      "Epoch 4427, Loss: 0.27143336087465286, Final Batch Loss: 0.18827609717845917\n",
      "Epoch 4428, Loss: 0.15224067121744156, Final Batch Loss: 0.065400131046772\n",
      "Epoch 4429, Loss: 0.19105810672044754, Final Batch Loss: 0.07094373553991318\n",
      "Epoch 4430, Loss: 0.17691560089588165, Final Batch Loss: 0.07701528072357178\n",
      "Epoch 4431, Loss: 0.26398415118455887, Final Batch Loss: 0.14406554400920868\n",
      "Epoch 4432, Loss: 0.1818993017077446, Final Batch Loss: 0.06846510618925095\n",
      "Epoch 4433, Loss: 0.20696987956762314, Final Batch Loss: 0.12470728158950806\n",
      "Epoch 4434, Loss: 0.19261040538549423, Final Batch Loss: 0.11208191514015198\n",
      "Epoch 4435, Loss: 0.21440277248620987, Final Batch Loss: 0.11514180898666382\n",
      "Epoch 4436, Loss: 0.20254388451576233, Final Batch Loss: 0.0862782821059227\n",
      "Epoch 4437, Loss: 0.2098684161901474, Final Batch Loss: 0.1247580498456955\n",
      "Epoch 4438, Loss: 0.2500467598438263, Final Batch Loss: 0.13009609282016754\n",
      "Epoch 4439, Loss: 0.1368226669728756, Final Batch Loss: 0.04394857957959175\n",
      "Epoch 4440, Loss: 0.19707725197076797, Final Batch Loss: 0.09636600315570831\n",
      "Epoch 4441, Loss: 0.20799291878938675, Final Batch Loss: 0.13763034343719482\n",
      "Epoch 4442, Loss: 0.22369712963700294, Final Batch Loss: 0.16454605758190155\n",
      "Epoch 4443, Loss: 0.14792605489492416, Final Batch Loss: 0.08251144737005234\n",
      "Epoch 4444, Loss: 0.16855356469750404, Final Batch Loss: 0.10834410041570663\n",
      "Epoch 4445, Loss: 0.1489667370915413, Final Batch Loss: 0.08549883216619492\n",
      "Epoch 4446, Loss: 0.21300438046455383, Final Batch Loss: 0.13452088832855225\n",
      "Epoch 4447, Loss: 0.2068924605846405, Final Batch Loss: 0.13013017177581787\n",
      "Epoch 4448, Loss: 0.2560257688164711, Final Batch Loss: 0.11930982023477554\n",
      "Epoch 4449, Loss: 0.20440995693206787, Final Batch Loss: 0.12387067824602127\n",
      "Epoch 4450, Loss: 0.16990766674280167, Final Batch Loss: 0.1071392148733139\n",
      "Epoch 4451, Loss: 0.12949204817414284, Final Batch Loss: 0.053849730640649796\n",
      "Epoch 4452, Loss: 0.16132072731852531, Final Batch Loss: 0.04998597875237465\n",
      "Epoch 4453, Loss: 0.2106829360127449, Final Batch Loss: 0.12471500039100647\n",
      "Epoch 4454, Loss: 0.1582917720079422, Final Batch Loss: 0.08704931288957596\n",
      "Epoch 4455, Loss: 0.12458391860127449, Final Batch Loss: 0.05229989066720009\n",
      "Epoch 4456, Loss: 0.22310757637023926, Final Batch Loss: 0.09763722121715546\n",
      "Epoch 4457, Loss: 0.17004620283842087, Final Batch Loss: 0.07771029323339462\n",
      "Epoch 4458, Loss: 0.16683420538902283, Final Batch Loss: 0.09721536189317703\n",
      "Epoch 4459, Loss: 0.21706829965114594, Final Batch Loss: 0.09060467779636383\n",
      "Epoch 4460, Loss: 0.154899500310421, Final Batch Loss: 0.09444386512041092\n",
      "Epoch 4461, Loss: 0.14492513611912727, Final Batch Loss: 0.10953117161989212\n",
      "Epoch 4462, Loss: 0.16395475715398788, Final Batch Loss: 0.06407344341278076\n",
      "Epoch 4463, Loss: 0.17815835773944855, Final Batch Loss: 0.0912540853023529\n",
      "Epoch 4464, Loss: 0.17323320358991623, Final Batch Loss: 0.08587517589330673\n",
      "Epoch 4465, Loss: 0.1848892793059349, Final Batch Loss: 0.09335510432720184\n",
      "Epoch 4466, Loss: 0.1524803340435028, Final Batch Loss: 0.05779305100440979\n",
      "Epoch 4467, Loss: 0.1590796411037445, Final Batch Loss: 0.09215740114450455\n",
      "Epoch 4468, Loss: 0.12993013113737106, Final Batch Loss: 0.037581197917461395\n",
      "Epoch 4469, Loss: 0.1724114939570427, Final Batch Loss: 0.09972706437110901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4470, Loss: 0.14255448803305626, Final Batch Loss: 0.04920829460024834\n",
      "Epoch 4471, Loss: 0.19450385123491287, Final Batch Loss: 0.1204606145620346\n",
      "Epoch 4472, Loss: 0.22957174479961395, Final Batch Loss: 0.1442234069108963\n",
      "Epoch 4473, Loss: 0.25230585038661957, Final Batch Loss: 0.17990992963314056\n",
      "Epoch 4474, Loss: 0.15023069828748703, Final Batch Loss: 0.06703385710716248\n",
      "Epoch 4475, Loss: 0.1526176556944847, Final Batch Loss: 0.08165154606103897\n",
      "Epoch 4476, Loss: 0.19596213474869728, Final Batch Loss: 0.054882828146219254\n",
      "Epoch 4477, Loss: 0.14351696521043777, Final Batch Loss: 0.07179120928049088\n",
      "Epoch 4478, Loss: 0.22895076870918274, Final Batch Loss: 0.14246204495429993\n",
      "Epoch 4479, Loss: 0.19330213218927383, Final Batch Loss: 0.11444970220327377\n",
      "Epoch 4480, Loss: 0.1571601927280426, Final Batch Loss: 0.0782027468085289\n",
      "Epoch 4481, Loss: 0.24813978374004364, Final Batch Loss: 0.11990785598754883\n",
      "Epoch 4482, Loss: 0.17987757176160812, Final Batch Loss: 0.07823309302330017\n",
      "Epoch 4483, Loss: 0.2165496051311493, Final Batch Loss: 0.1372409164905548\n",
      "Epoch 4484, Loss: 0.21796444058418274, Final Batch Loss: 0.0754178911447525\n",
      "Epoch 4485, Loss: 0.16954166442155838, Final Batch Loss: 0.07913117855787277\n",
      "Epoch 4486, Loss: 0.16223515570163727, Final Batch Loss: 0.07952288538217545\n",
      "Epoch 4487, Loss: 0.13418807089328766, Final Batch Loss: 0.049969784915447235\n",
      "Epoch 4488, Loss: 0.1475725658237934, Final Batch Loss: 0.05154665187001228\n",
      "Epoch 4489, Loss: 0.2773973420262337, Final Batch Loss: 0.1124921664595604\n",
      "Epoch 4490, Loss: 0.16748743876814842, Final Batch Loss: 0.10613670200109482\n",
      "Epoch 4491, Loss: 0.16444694995880127, Final Batch Loss: 0.09181364625692368\n",
      "Epoch 4492, Loss: 0.21710436046123505, Final Batch Loss: 0.14849433302879333\n",
      "Epoch 4493, Loss: 0.22942300140857697, Final Batch Loss: 0.08596579730510712\n",
      "Epoch 4494, Loss: 0.15189548581838608, Final Batch Loss: 0.08913733810186386\n",
      "Epoch 4495, Loss: 0.15210659056901932, Final Batch Loss: 0.07143557071685791\n",
      "Epoch 4496, Loss: 0.14701702445745468, Final Batch Loss: 0.054079338908195496\n",
      "Epoch 4497, Loss: 0.1480250358581543, Final Batch Loss: 0.07332497835159302\n",
      "Epoch 4498, Loss: 0.35497766733169556, Final Batch Loss: 0.13274455070495605\n",
      "Epoch 4499, Loss: 0.20525550097227097, Final Batch Loss: 0.1227792501449585\n",
      "Epoch 4500, Loss: 0.18449602276086807, Final Batch Loss: 0.08663653582334518\n",
      "Epoch 4501, Loss: 0.12299429625272751, Final Batch Loss: 0.04604065418243408\n",
      "Epoch 4502, Loss: 0.21128661930561066, Final Batch Loss: 0.1384182721376419\n",
      "Epoch 4503, Loss: 0.15225819870829582, Final Batch Loss: 0.05718641355633736\n",
      "Epoch 4504, Loss: 0.15426620095968246, Final Batch Loss: 0.08050799369812012\n",
      "Epoch 4505, Loss: 0.19251778721809387, Final Batch Loss: 0.11981358379125595\n",
      "Epoch 4506, Loss: 0.14193075150251389, Final Batch Loss: 0.0758536234498024\n",
      "Epoch 4507, Loss: 0.1886911690235138, Final Batch Loss: 0.1044430211186409\n",
      "Epoch 4508, Loss: 0.20932333916425705, Final Batch Loss: 0.1473185420036316\n",
      "Epoch 4509, Loss: 0.1810186207294464, Final Batch Loss: 0.07622234523296356\n",
      "Epoch 4510, Loss: 0.19988924264907837, Final Batch Loss: 0.10174331814050674\n",
      "Epoch 4511, Loss: 0.15711775422096252, Final Batch Loss: 0.0827050730586052\n",
      "Epoch 4512, Loss: 0.14415019005537033, Final Batch Loss: 0.06625840067863464\n",
      "Epoch 4513, Loss: 0.2840156555175781, Final Batch Loss: 0.19006004929542542\n",
      "Epoch 4514, Loss: 0.20824570208787918, Final Batch Loss: 0.10037694126367569\n",
      "Epoch 4515, Loss: 0.25621838867664337, Final Batch Loss: 0.16463474929332733\n",
      "Epoch 4516, Loss: 0.20984730869531631, Final Batch Loss: 0.10355263203382492\n",
      "Epoch 4517, Loss: 0.1609601229429245, Final Batch Loss: 0.05761031061410904\n",
      "Epoch 4518, Loss: 0.17743225395679474, Final Batch Loss: 0.08650892972946167\n",
      "Epoch 4519, Loss: 0.1553434208035469, Final Batch Loss: 0.07589510828256607\n",
      "Epoch 4520, Loss: 0.23091697692871094, Final Batch Loss: 0.14624978601932526\n",
      "Epoch 4521, Loss: 0.16397449374198914, Final Batch Loss: 0.06837119162082672\n",
      "Epoch 4522, Loss: 0.13299977034330368, Final Batch Loss: 0.057884328067302704\n",
      "Epoch 4523, Loss: 0.1944541484117508, Final Batch Loss: 0.09982132911682129\n",
      "Epoch 4524, Loss: 0.11925021931529045, Final Batch Loss: 0.04291437193751335\n",
      "Epoch 4525, Loss: 0.21148807555437088, Final Batch Loss: 0.12360060214996338\n",
      "Epoch 4526, Loss: 0.19559892266988754, Final Batch Loss: 0.10301879048347473\n",
      "Epoch 4527, Loss: 0.16213392466306686, Final Batch Loss: 0.09058531373739243\n",
      "Epoch 4528, Loss: 0.2106984630227089, Final Batch Loss: 0.12525063753128052\n",
      "Epoch 4529, Loss: 0.14490750432014465, Final Batch Loss: 0.0706375315785408\n",
      "Epoch 4530, Loss: 0.19466793537139893, Final Batch Loss: 0.07609300315380096\n",
      "Epoch 4531, Loss: 0.1439441442489624, Final Batch Loss: 0.06410285085439682\n",
      "Epoch 4532, Loss: 0.16602089256048203, Final Batch Loss: 0.07811803370714188\n",
      "Epoch 4533, Loss: 0.1714242659509182, Final Batch Loss: 0.12149889767169952\n",
      "Epoch 4534, Loss: 0.2049308493733406, Final Batch Loss: 0.09019823372364044\n",
      "Epoch 4535, Loss: 0.18324529379606247, Final Batch Loss: 0.08239059895277023\n",
      "Epoch 4536, Loss: 0.15808605402708054, Final Batch Loss: 0.09065685421228409\n",
      "Epoch 4537, Loss: 0.17652080953121185, Final Batch Loss: 0.09668045490980148\n",
      "Epoch 4538, Loss: 0.20861756801605225, Final Batch Loss: 0.10127783566713333\n",
      "Epoch 4539, Loss: 0.15290134400129318, Final Batch Loss: 0.07053205370903015\n",
      "Epoch 4540, Loss: 0.15204595774412155, Final Batch Loss: 0.07621689140796661\n",
      "Epoch 4541, Loss: 0.18297559022903442, Final Batch Loss: 0.09320754557847977\n",
      "Epoch 4542, Loss: 0.1767103523015976, Final Batch Loss: 0.0788026824593544\n",
      "Epoch 4543, Loss: 0.1436922401189804, Final Batch Loss: 0.0577864870429039\n",
      "Epoch 4544, Loss: 0.1940469592809677, Final Batch Loss: 0.08097078651189804\n",
      "Epoch 4545, Loss: 0.13086359202861786, Final Batch Loss: 0.07031842321157455\n",
      "Epoch 4546, Loss: 0.14567779749631882, Final Batch Loss: 0.06765724718570709\n",
      "Epoch 4547, Loss: 0.15729594975709915, Final Batch Loss: 0.07998885214328766\n",
      "Epoch 4548, Loss: 0.22283387929201126, Final Batch Loss: 0.16462482511997223\n",
      "Epoch 4549, Loss: 0.12948525696992874, Final Batch Loss: 0.0555657297372818\n",
      "Epoch 4550, Loss: 0.17609189078211784, Final Batch Loss: 0.050443317741155624\n",
      "Epoch 4551, Loss: 0.16034873574972153, Final Batch Loss: 0.09363573789596558\n",
      "Epoch 4552, Loss: 0.2037457562983036, Final Batch Loss: 0.14952664077281952\n",
      "Epoch 4553, Loss: 0.1889055371284485, Final Batch Loss: 0.07491988688707352\n",
      "Epoch 4554, Loss: 0.1100178137421608, Final Batch Loss: 0.040880657732486725\n",
      "Epoch 4555, Loss: 0.15313770622015, Final Batch Loss: 0.07916520535945892\n",
      "Epoch 4556, Loss: 0.18373272567987442, Final Batch Loss: 0.08267957717180252\n",
      "Epoch 4557, Loss: 0.21276429295539856, Final Batch Loss: 0.11391707509756088\n",
      "Epoch 4558, Loss: 0.22726130485534668, Final Batch Loss: 0.15139654278755188\n",
      "Epoch 4559, Loss: 0.27263542264699936, Final Batch Loss: 0.09475699812173843\n",
      "Epoch 4560, Loss: 0.15879316627979279, Final Batch Loss: 0.07287389785051346\n",
      "Epoch 4561, Loss: 0.2096916064620018, Final Batch Loss: 0.08425579220056534\n",
      "Epoch 4562, Loss: 0.27767330408096313, Final Batch Loss: 0.16823045909404755\n",
      "Epoch 4563, Loss: 0.1322740539908409, Final Batch Loss: 0.05703224241733551\n",
      "Epoch 4564, Loss: 0.20980090647935867, Final Batch Loss: 0.07878298312425613\n",
      "Epoch 4565, Loss: 0.17359251156449318, Final Batch Loss: 0.05135738477110863\n",
      "Epoch 4566, Loss: 0.15979214757680893, Final Batch Loss: 0.06906136870384216\n",
      "Epoch 4567, Loss: 0.1892145499587059, Final Batch Loss: 0.07653830200433731\n",
      "Epoch 4568, Loss: 0.19039972126483917, Final Batch Loss: 0.09651156514883041\n",
      "Epoch 4569, Loss: 0.2292574718594551, Final Batch Loss: 0.1798037737607956\n",
      "Epoch 4570, Loss: 0.15015779808163643, Final Batch Loss: 0.04706203565001488\n",
      "Epoch 4571, Loss: 0.17490384355187416, Final Batch Loss: 0.03733818605542183\n",
      "Epoch 4572, Loss: 0.1634880155324936, Final Batch Loss: 0.09541317820549011\n",
      "Epoch 4573, Loss: 0.1734398603439331, Final Batch Loss: 0.0941057875752449\n",
      "Epoch 4574, Loss: 0.2666412591934204, Final Batch Loss: 0.1290818750858307\n",
      "Epoch 4575, Loss: 0.22768522799015045, Final Batch Loss: 0.1634315550327301\n",
      "Epoch 4576, Loss: 0.12952870875597, Final Batch Loss: 0.043779537081718445\n",
      "Epoch 4577, Loss: 0.20060689002275467, Final Batch Loss: 0.11370330303907394\n",
      "Epoch 4578, Loss: 0.17986000329256058, Final Batch Loss: 0.105117067694664\n",
      "Epoch 4579, Loss: 0.13058678060770035, Final Batch Loss: 0.058367617428302765\n",
      "Epoch 4580, Loss: 0.14615806937217712, Final Batch Loss: 0.08134415745735168\n",
      "Epoch 4581, Loss: 0.14140034466981888, Final Batch Loss: 0.06999583542346954\n",
      "Epoch 4582, Loss: 0.19952642917633057, Final Batch Loss: 0.09717316180467606\n",
      "Epoch 4583, Loss: 0.08931397460401058, Final Batch Loss: 0.021041689440608025\n",
      "Epoch 4584, Loss: 0.1718766838312149, Final Batch Loss: 0.1092347800731659\n",
      "Epoch 4585, Loss: 0.15046928077936172, Final Batch Loss: 0.07018031924962997\n",
      "Epoch 4586, Loss: 0.16040844470262527, Final Batch Loss: 0.05491029471158981\n",
      "Epoch 4587, Loss: 0.15033841878175735, Final Batch Loss: 0.08316860347986221\n",
      "Epoch 4588, Loss: 0.15470338612794876, Final Batch Loss: 0.08555026352405548\n",
      "Epoch 4589, Loss: 0.17268521338701248, Final Batch Loss: 0.09079186618328094\n",
      "Epoch 4590, Loss: 0.1579199992120266, Final Batch Loss: 0.10671808570623398\n",
      "Epoch 4591, Loss: 0.17624697089195251, Final Batch Loss: 0.07755579054355621\n",
      "Epoch 4592, Loss: 0.1834636926651001, Final Batch Loss: 0.10451758652925491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4593, Loss: 0.11673470586538315, Final Batch Loss: 0.04883313179016113\n",
      "Epoch 4594, Loss: 0.15350691974163055, Final Batch Loss: 0.09974122792482376\n",
      "Epoch 4595, Loss: 0.11055083945393562, Final Batch Loss: 0.04727049544453621\n",
      "Epoch 4596, Loss: 0.13253312185406685, Final Batch Loss: 0.053667787462472916\n",
      "Epoch 4597, Loss: 0.15431886166334152, Final Batch Loss: 0.07080867141485214\n",
      "Epoch 4598, Loss: 0.16896100342273712, Final Batch Loss: 0.12041766941547394\n",
      "Epoch 4599, Loss: 0.18921852111816406, Final Batch Loss: 0.09963759034872055\n",
      "Epoch 4600, Loss: 0.15376626700162888, Final Batch Loss: 0.08331137895584106\n",
      "Epoch 4601, Loss: 0.16090212762355804, Final Batch Loss: 0.0657995343208313\n",
      "Epoch 4602, Loss: 0.20507652312517166, Final Batch Loss: 0.09178425371646881\n",
      "Epoch 4603, Loss: 0.14998957514762878, Final Batch Loss: 0.06905068457126617\n",
      "Epoch 4604, Loss: 0.17875579744577408, Final Batch Loss: 0.08742253482341766\n",
      "Epoch 4605, Loss: 0.25381308794021606, Final Batch Loss: 0.1691056489944458\n",
      "Epoch 4606, Loss: 0.19357989728450775, Final Batch Loss: 0.08053972572088242\n",
      "Epoch 4607, Loss: 0.21467941999435425, Final Batch Loss: 0.09894347190856934\n",
      "Epoch 4608, Loss: 0.20270579308271408, Final Batch Loss: 0.10666648298501968\n",
      "Epoch 4609, Loss: 0.22329266369342804, Final Batch Loss: 0.105849489569664\n",
      "Epoch 4610, Loss: 0.12548093870282173, Final Batch Loss: 0.06116838380694389\n",
      "Epoch 4611, Loss: 0.2570577338337898, Final Batch Loss: 0.12011172622442245\n",
      "Epoch 4612, Loss: 0.17849274724721909, Final Batch Loss: 0.08517015725374222\n",
      "Epoch 4613, Loss: 0.17059027403593063, Final Batch Loss: 0.07054075598716736\n",
      "Epoch 4614, Loss: 0.14100036025047302, Final Batch Loss: 0.07254785299301147\n",
      "Epoch 4615, Loss: 0.19390908628702164, Final Batch Loss: 0.09366025775671005\n",
      "Epoch 4616, Loss: 0.21888573467731476, Final Batch Loss: 0.10085003077983856\n",
      "Epoch 4617, Loss: 0.14461210742592812, Final Batch Loss: 0.05523829534649849\n",
      "Epoch 4618, Loss: 0.1666191816329956, Final Batch Loss: 0.08119265735149384\n",
      "Epoch 4619, Loss: 0.2150561884045601, Final Batch Loss: 0.0747193768620491\n",
      "Epoch 4620, Loss: 0.20099014043807983, Final Batch Loss: 0.07543443143367767\n",
      "Epoch 4621, Loss: 0.18557633459568024, Final Batch Loss: 0.09748338907957077\n",
      "Epoch 4622, Loss: 0.1275932490825653, Final Batch Loss: 0.046653106808662415\n",
      "Epoch 4623, Loss: 0.17520150542259216, Final Batch Loss: 0.11113156378269196\n",
      "Epoch 4624, Loss: 0.15334679186344147, Final Batch Loss: 0.06891584396362305\n",
      "Epoch 4625, Loss: 0.1515103504061699, Final Batch Loss: 0.08844905346632004\n",
      "Epoch 4626, Loss: 0.16980848461389542, Final Batch Loss: 0.07908178120851517\n",
      "Epoch 4627, Loss: 0.16660011559724808, Final Batch Loss: 0.07183964550495148\n",
      "Epoch 4628, Loss: 0.16573844850063324, Final Batch Loss: 0.10310511291027069\n",
      "Epoch 4629, Loss: 0.1879589781165123, Final Batch Loss: 0.05100899189710617\n",
      "Epoch 4630, Loss: 0.11250617727637291, Final Batch Loss: 0.053075239062309265\n",
      "Epoch 4631, Loss: 0.1936618685722351, Final Batch Loss: 0.09377843886613846\n",
      "Epoch 4632, Loss: 0.1710531897842884, Final Batch Loss: 0.1197790578007698\n",
      "Epoch 4633, Loss: 0.11976420506834984, Final Batch Loss: 0.06308452039957047\n",
      "Epoch 4634, Loss: 0.18722324818372726, Final Batch Loss: 0.12280105799436569\n",
      "Epoch 4635, Loss: 0.1423758715391159, Final Batch Loss: 0.07562653720378876\n",
      "Epoch 4636, Loss: 0.16345545276999474, Final Batch Loss: 0.11160770058631897\n",
      "Epoch 4637, Loss: 0.1585020199418068, Final Batch Loss: 0.06905177235603333\n",
      "Epoch 4638, Loss: 0.1937551014125347, Final Batch Loss: 0.05810841545462608\n",
      "Epoch 4639, Loss: 0.15924369543790817, Final Batch Loss: 0.06375649571418762\n",
      "Epoch 4640, Loss: 0.13283691555261612, Final Batch Loss: 0.046656668186187744\n",
      "Epoch 4641, Loss: 0.10978003218770027, Final Batch Loss: 0.04046470299363136\n",
      "Epoch 4642, Loss: 0.1475699432194233, Final Batch Loss: 0.05883163586258888\n",
      "Epoch 4643, Loss: 0.21171946823596954, Final Batch Loss: 0.1423150897026062\n",
      "Epoch 4644, Loss: 0.14045656844973564, Final Batch Loss: 0.05042493715882301\n",
      "Epoch 4645, Loss: 0.20964358747005463, Final Batch Loss: 0.13690973818302155\n",
      "Epoch 4646, Loss: 0.1460217870771885, Final Batch Loss: 0.0578533299267292\n",
      "Epoch 4647, Loss: 0.13434064015746117, Final Batch Loss: 0.05873638764023781\n",
      "Epoch 4648, Loss: 0.14209968596696854, Final Batch Loss: 0.06516087800264359\n",
      "Epoch 4649, Loss: 0.17523310706019402, Final Batch Loss: 0.0378815196454525\n",
      "Epoch 4650, Loss: 0.16285045444965363, Final Batch Loss: 0.07375981658697128\n",
      "Epoch 4651, Loss: 0.1700197011232376, Final Batch Loss: 0.06832729279994965\n",
      "Epoch 4652, Loss: 0.1502162404358387, Final Batch Loss: 0.04431694373488426\n",
      "Epoch 4653, Loss: 0.19172965735197067, Final Batch Loss: 0.11532433331012726\n",
      "Epoch 4654, Loss: 0.21129801124334335, Final Batch Loss: 0.11671159416437149\n",
      "Epoch 4655, Loss: 0.18916559219360352, Final Batch Loss: 0.07773163914680481\n",
      "Epoch 4656, Loss: 0.1658881977200508, Final Batch Loss: 0.08384720236063004\n",
      "Epoch 4657, Loss: 0.16651246696710587, Final Batch Loss: 0.123876191675663\n",
      "Epoch 4658, Loss: 0.22407517582178116, Final Batch Loss: 0.14807990193367004\n",
      "Epoch 4659, Loss: 0.1481412872672081, Final Batch Loss: 0.0947975143790245\n",
      "Epoch 4660, Loss: 0.19631340354681015, Final Batch Loss: 0.10291844606399536\n",
      "Epoch 4661, Loss: 0.14262137562036514, Final Batch Loss: 0.07714255154132843\n",
      "Epoch 4662, Loss: 0.1723071038722992, Final Batch Loss: 0.10069900751113892\n",
      "Epoch 4663, Loss: 0.19294510036706924, Final Batch Loss: 0.11309646815061569\n",
      "Epoch 4664, Loss: 0.18820519745349884, Final Batch Loss: 0.1000111773610115\n",
      "Epoch 4665, Loss: 0.17111440747976303, Final Batch Loss: 0.08351696282625198\n",
      "Epoch 4666, Loss: 0.17689165472984314, Final Batch Loss: 0.08481434732675552\n",
      "Epoch 4667, Loss: 0.1866072528064251, Final Batch Loss: 0.1279897689819336\n",
      "Epoch 4668, Loss: 0.16369765624403954, Final Batch Loss: 0.06009149178862572\n",
      "Epoch 4669, Loss: 0.18396925181150436, Final Batch Loss: 0.11830455809831619\n",
      "Epoch 4670, Loss: 0.1826988011598587, Final Batch Loss: 0.0672372505068779\n",
      "Epoch 4671, Loss: 0.1101839505136013, Final Batch Loss: 0.06348095089197159\n",
      "Epoch 4672, Loss: 0.1417432241141796, Final Batch Loss: 0.05627559497952461\n",
      "Epoch 4673, Loss: 0.18891942873597145, Final Batch Loss: 0.060393307358026505\n",
      "Epoch 4674, Loss: 0.16938066482543945, Final Batch Loss: 0.1188015416264534\n",
      "Epoch 4675, Loss: 0.21014565229415894, Final Batch Loss: 0.15464173257350922\n",
      "Epoch 4676, Loss: 0.14945093169808388, Final Batch Loss: 0.06134839728474617\n",
      "Epoch 4677, Loss: 0.20178038626909256, Final Batch Loss: 0.09939958900213242\n",
      "Epoch 4678, Loss: 0.12782736495137215, Final Batch Loss: 0.05994775518774986\n",
      "Epoch 4679, Loss: 0.20867466181516647, Final Batch Loss: 0.07620299607515335\n",
      "Epoch 4680, Loss: 0.17237036675214767, Final Batch Loss: 0.0909421518445015\n",
      "Epoch 4681, Loss: 0.22801266610622406, Final Batch Loss: 0.15471088886260986\n",
      "Epoch 4682, Loss: 0.16434050351381302, Final Batch Loss: 0.06298493593931198\n",
      "Epoch 4683, Loss: 0.1636633574962616, Final Batch Loss: 0.09393160790205002\n",
      "Epoch 4684, Loss: 0.11692026630043983, Final Batch Loss: 0.02578629180788994\n",
      "Epoch 4685, Loss: 0.12843723967671394, Final Batch Loss: 0.04284103587269783\n",
      "Epoch 4686, Loss: 0.18254081904888153, Final Batch Loss: 0.11760758608579636\n",
      "Epoch 4687, Loss: 0.16678790003061295, Final Batch Loss: 0.09973568469285965\n",
      "Epoch 4688, Loss: 0.15197648108005524, Final Batch Loss: 0.08716128766536713\n",
      "Epoch 4689, Loss: 0.1957562491297722, Final Batch Loss: 0.12012854218482971\n",
      "Epoch 4690, Loss: 0.142383623868227, Final Batch Loss: 0.04811331257224083\n",
      "Epoch 4691, Loss: 0.15703926980495453, Final Batch Loss: 0.0903906375169754\n",
      "Epoch 4692, Loss: 0.1857098937034607, Final Batch Loss: 0.08697181195020676\n",
      "Epoch 4693, Loss: 0.1587069034576416, Final Batch Loss: 0.08346983045339584\n",
      "Epoch 4694, Loss: 0.12665259465575218, Final Batch Loss: 0.0559091754257679\n",
      "Epoch 4695, Loss: 0.2068924978375435, Final Batch Loss: 0.11339128017425537\n",
      "Epoch 4696, Loss: 0.15833065286278725, Final Batch Loss: 0.1037321388721466\n",
      "Epoch 4697, Loss: 0.16269095242023468, Final Batch Loss: 0.08064647763967514\n",
      "Epoch 4698, Loss: 0.13529837504029274, Final Batch Loss: 0.060895342379808426\n",
      "Epoch 4699, Loss: 0.22125232964754105, Final Batch Loss: 0.10829629004001617\n",
      "Epoch 4700, Loss: 0.17977756261825562, Final Batch Loss: 0.06381329149007797\n",
      "Epoch 4701, Loss: 0.1808723658323288, Final Batch Loss: 0.07715759426355362\n",
      "Epoch 4702, Loss: 0.14483820647001266, Final Batch Loss: 0.07903854548931122\n",
      "Epoch 4703, Loss: 0.14836182445287704, Final Batch Loss: 0.07654985785484314\n",
      "Epoch 4704, Loss: 0.14533912390470505, Final Batch Loss: 0.08776577562093735\n",
      "Epoch 4705, Loss: 0.19541256874799728, Final Batch Loss: 0.12049711495637894\n",
      "Epoch 4706, Loss: 0.19763898849487305, Final Batch Loss: 0.1150171086192131\n",
      "Epoch 4707, Loss: 0.12988333404064178, Final Batch Loss: 0.062198199331760406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4708, Loss: 0.22748444229364395, Final Batch Loss: 0.1305888444185257\n",
      "Epoch 4709, Loss: 0.22401010990142822, Final Batch Loss: 0.13456548750400543\n",
      "Epoch 4710, Loss: 0.24083013087511063, Final Batch Loss: 0.11914472281932831\n",
      "Epoch 4711, Loss: 0.13701097667217255, Final Batch Loss: 0.04174748808145523\n",
      "Epoch 4712, Loss: 0.195309616625309, Final Batch Loss: 0.10109879821538925\n",
      "Epoch 4713, Loss: 0.15643525123596191, Final Batch Loss: 0.1023208498954773\n",
      "Epoch 4714, Loss: 0.17175952345132828, Final Batch Loss: 0.07954490184783936\n",
      "Epoch 4715, Loss: 0.10783959180116653, Final Batch Loss: 0.04645441100001335\n",
      "Epoch 4716, Loss: 0.14648494124412537, Final Batch Loss: 0.06731335073709488\n",
      "Epoch 4717, Loss: 0.1265597678720951, Final Batch Loss: 0.06660699844360352\n",
      "Epoch 4718, Loss: 0.13975906744599342, Final Batch Loss: 0.03972857818007469\n",
      "Epoch 4719, Loss: 0.15681647509336472, Final Batch Loss: 0.061849094927310944\n",
      "Epoch 4720, Loss: 0.16585589572787285, Final Batch Loss: 0.061109308153390884\n",
      "Epoch 4721, Loss: 0.14869272708892822, Final Batch Loss: 0.08961287885904312\n",
      "Epoch 4722, Loss: 0.2043803706765175, Final Batch Loss: 0.12544555962085724\n",
      "Epoch 4723, Loss: 0.2006632387638092, Final Batch Loss: 0.1107940524816513\n",
      "Epoch 4724, Loss: 0.25516556948423386, Final Batch Loss: 0.1446475088596344\n",
      "Epoch 4725, Loss: 0.12538742274045944, Final Batch Loss: 0.06513472646474838\n",
      "Epoch 4726, Loss: 0.14611774310469627, Final Batch Loss: 0.08930249512195587\n",
      "Epoch 4727, Loss: 0.12600553035736084, Final Batch Loss: 0.06527402251958847\n",
      "Epoch 4728, Loss: 0.16889969259500504, Final Batch Loss: 0.09993419796228409\n",
      "Epoch 4729, Loss: 0.190525621175766, Final Batch Loss: 0.08029501140117645\n",
      "Epoch 4730, Loss: 0.10706717148423195, Final Batch Loss: 0.052723146975040436\n",
      "Epoch 4731, Loss: 0.15027671307325363, Final Batch Loss: 0.07208135724067688\n",
      "Epoch 4732, Loss: 0.13328664749860764, Final Batch Loss: 0.08576715737581253\n",
      "Epoch 4733, Loss: 0.14661132544279099, Final Batch Loss: 0.07889226078987122\n",
      "Epoch 4734, Loss: 0.13810711354017258, Final Batch Loss: 0.05812138319015503\n",
      "Epoch 4735, Loss: 0.11534914374351501, Final Batch Loss: 0.04487571865320206\n",
      "Epoch 4736, Loss: 0.1598019003868103, Final Batch Loss: 0.07277421653270721\n",
      "Epoch 4737, Loss: 0.18233711272478104, Final Batch Loss: 0.120571069419384\n",
      "Epoch 4738, Loss: 0.162441648542881, Final Batch Loss: 0.06064710021018982\n",
      "Epoch 4739, Loss: 0.22267866879701614, Final Batch Loss: 0.17132636904716492\n",
      "Epoch 4740, Loss: 0.1624894142150879, Final Batch Loss: 0.12214641273021698\n",
      "Epoch 4741, Loss: 0.1007479764521122, Final Batch Loss: 0.05776792764663696\n",
      "Epoch 4742, Loss: 0.125193040817976, Final Batch Loss: 0.05288241431117058\n",
      "Epoch 4743, Loss: 0.13566117733716965, Final Batch Loss: 0.040134526789188385\n",
      "Epoch 4744, Loss: 0.1402362436056137, Final Batch Loss: 0.07080953568220139\n",
      "Epoch 4745, Loss: 0.13486843183636665, Final Batch Loss: 0.0546063669025898\n",
      "Epoch 4746, Loss: 0.15136577934026718, Final Batch Loss: 0.06902270019054413\n",
      "Epoch 4747, Loss: 0.3830036520957947, Final Batch Loss: 0.12143799662590027\n",
      "Epoch 4748, Loss: 0.21863510459661484, Final Batch Loss: 0.1120418906211853\n",
      "Epoch 4749, Loss: 0.12862670421600342, Final Batch Loss: 0.044791631400585175\n",
      "Epoch 4750, Loss: 0.1446232870221138, Final Batch Loss: 0.0770973265171051\n",
      "Epoch 4751, Loss: 0.11482465639710426, Final Batch Loss: 0.05184893682599068\n",
      "Epoch 4752, Loss: 0.12411459162831306, Final Batch Loss: 0.06909329444169998\n",
      "Epoch 4753, Loss: 0.18252911418676376, Final Batch Loss: 0.10771656781435013\n",
      "Epoch 4754, Loss: 0.1989043653011322, Final Batch Loss: 0.097505584359169\n",
      "Epoch 4755, Loss: 0.1504364088177681, Final Batch Loss: 0.07050837576389313\n",
      "Epoch 4756, Loss: 0.15016897954046726, Final Batch Loss: 0.030934298411011696\n",
      "Epoch 4757, Loss: 0.09756036475300789, Final Batch Loss: 0.040395066142082214\n",
      "Epoch 4758, Loss: 0.18460506945848465, Final Batch Loss: 0.057418935000896454\n",
      "Epoch 4759, Loss: 0.17279961705207825, Final Batch Loss: 0.06760681420564651\n",
      "Epoch 4760, Loss: 0.12417362257838249, Final Batch Loss: 0.04818214103579521\n",
      "Epoch 4761, Loss: 0.1704433411359787, Final Batch Loss: 0.08698473125696182\n",
      "Epoch 4762, Loss: 0.13177987933158875, Final Batch Loss: 0.033728472888469696\n",
      "Epoch 4763, Loss: 0.16652336716651917, Final Batch Loss: 0.07583647221326828\n",
      "Epoch 4764, Loss: 0.16490775346755981, Final Batch Loss: 0.08486156910657883\n",
      "Epoch 4765, Loss: 0.1270461156964302, Final Batch Loss: 0.06348592042922974\n",
      "Epoch 4766, Loss: 0.13727612048387527, Final Batch Loss: 0.04823371767997742\n",
      "Epoch 4767, Loss: 0.16360020264983177, Final Batch Loss: 0.04463126137852669\n",
      "Epoch 4768, Loss: 0.20468208193778992, Final Batch Loss: 0.09690859913825989\n",
      "Epoch 4769, Loss: 0.1560993269085884, Final Batch Loss: 0.08835694193840027\n",
      "Epoch 4770, Loss: 0.12768522650003433, Final Batch Loss: 0.06990096718072891\n",
      "Epoch 4771, Loss: 0.10902506113052368, Final Batch Loss: 0.051711369305849075\n",
      "Epoch 4772, Loss: 0.1965452879667282, Final Batch Loss: 0.0779309794306755\n",
      "Epoch 4773, Loss: 0.13975055143237114, Final Batch Loss: 0.029525335878133774\n",
      "Epoch 4774, Loss: 0.11989077553153038, Final Batch Loss: 0.05137183889746666\n",
      "Epoch 4775, Loss: 0.13532590866088867, Final Batch Loss: 0.06594549119472504\n",
      "Epoch 4776, Loss: 0.11894841864705086, Final Batch Loss: 0.060859739780426025\n",
      "Epoch 4777, Loss: 0.1762119084596634, Final Batch Loss: 0.07795268297195435\n",
      "Epoch 4778, Loss: 0.12396775186061859, Final Batch Loss: 0.05278448015451431\n",
      "Epoch 4779, Loss: 0.16717825084924698, Final Batch Loss: 0.08666783571243286\n",
      "Epoch 4780, Loss: 0.14925671741366386, Final Batch Loss: 0.06139354780316353\n",
      "Epoch 4781, Loss: 0.16457394137978554, Final Batch Loss: 0.04352085664868355\n",
      "Epoch 4782, Loss: 0.2202930524945259, Final Batch Loss: 0.14767439663410187\n",
      "Epoch 4783, Loss: 0.13905200362205505, Final Batch Loss: 0.06802736222743988\n",
      "Epoch 4784, Loss: 0.1941601186990738, Final Batch Loss: 0.06547604501247406\n",
      "Epoch 4785, Loss: 0.1814873069524765, Final Batch Loss: 0.13274338841438293\n",
      "Epoch 4786, Loss: 0.1272287331521511, Final Batch Loss: 0.05345160886645317\n",
      "Epoch 4787, Loss: 0.13661395013332367, Final Batch Loss: 0.04090245068073273\n",
      "Epoch 4788, Loss: 0.1330593079328537, Final Batch Loss: 0.06502937525510788\n",
      "Epoch 4789, Loss: 0.12894506379961967, Final Batch Loss: 0.04865366593003273\n",
      "Epoch 4790, Loss: 0.1200822964310646, Final Batch Loss: 0.06842892616987228\n",
      "Epoch 4791, Loss: 0.1589958518743515, Final Batch Loss: 0.08709217607975006\n",
      "Epoch 4792, Loss: 0.27829062193632126, Final Batch Loss: 0.09490140527486801\n",
      "Epoch 4793, Loss: 0.15076417475938797, Final Batch Loss: 0.06431972980499268\n",
      "Epoch 4794, Loss: 0.12156246230006218, Final Batch Loss: 0.05933952331542969\n",
      "Epoch 4795, Loss: 0.17512596398591995, Final Batch Loss: 0.11447548121213913\n",
      "Epoch 4796, Loss: 0.14752255007624626, Final Batch Loss: 0.0574171282351017\n",
      "Epoch 4797, Loss: 0.16052667051553726, Final Batch Loss: 0.09476252645254135\n",
      "Epoch 4798, Loss: 0.14336493611335754, Final Batch Loss: 0.056778550148010254\n",
      "Epoch 4799, Loss: 0.11348814517259598, Final Batch Loss: 0.04578012228012085\n",
      "Epoch 4800, Loss: 0.1447145715355873, Final Batch Loss: 0.07689899951219559\n",
      "Epoch 4801, Loss: 0.16950196027755737, Final Batch Loss: 0.07015613466501236\n",
      "Epoch 4802, Loss: 0.1286596953868866, Final Batch Loss: 0.05023161321878433\n",
      "Epoch 4803, Loss: 0.16885020956397057, Final Batch Loss: 0.11778129637241364\n",
      "Epoch 4804, Loss: 0.17990850657224655, Final Batch Loss: 0.09857979416847229\n",
      "Epoch 4805, Loss: 0.1541781798005104, Final Batch Loss: 0.09518105536699295\n",
      "Epoch 4806, Loss: 0.22843168675899506, Final Batch Loss: 0.16348502039909363\n",
      "Epoch 4807, Loss: 0.12920358031988144, Final Batch Loss: 0.03935215622186661\n",
      "Epoch 4808, Loss: 0.19054310768842697, Final Batch Loss: 0.11773872375488281\n",
      "Epoch 4809, Loss: 0.192672997713089, Final Batch Loss: 0.0599711537361145\n",
      "Epoch 4810, Loss: 0.14093409478664398, Final Batch Loss: 0.06102455407381058\n",
      "Epoch 4811, Loss: 0.12627290189266205, Final Batch Loss: 0.07321026921272278\n",
      "Epoch 4812, Loss: 0.16084721684455872, Final Batch Loss: 0.07027705013751984\n",
      "Epoch 4813, Loss: 0.1585342437028885, Final Batch Loss: 0.0690649151802063\n",
      "Epoch 4814, Loss: 0.15710491687059402, Final Batch Loss: 0.09541679173707962\n",
      "Epoch 4815, Loss: 0.14023437350988388, Final Batch Loss: 0.06419336050748825\n",
      "Epoch 4816, Loss: 0.12750187888741493, Final Batch Loss: 0.07138905674219131\n",
      "Epoch 4817, Loss: 0.11484052985906601, Final Batch Loss: 0.05925116315484047\n",
      "Epoch 4818, Loss: 0.14049749076366425, Final Batch Loss: 0.048680469393730164\n",
      "Epoch 4819, Loss: 0.14209597557783127, Final Batch Loss: 0.06353898346424103\n",
      "Epoch 4820, Loss: 0.1250198632478714, Final Batch Loss: 0.06761212646961212\n",
      "Epoch 4821, Loss: 0.19157055765390396, Final Batch Loss: 0.0941537395119667\n",
      "Epoch 4822, Loss: 0.2318240851163864, Final Batch Loss: 0.137294739484787\n",
      "Epoch 4823, Loss: 0.16209881007671356, Final Batch Loss: 0.09899543225765228\n",
      "Epoch 4824, Loss: 0.1493276096880436, Final Batch Loss: 0.09299913793802261\n",
      "Epoch 4825, Loss: 0.1326267570257187, Final Batch Loss: 0.06622746586799622\n",
      "Epoch 4826, Loss: 0.21998664736747742, Final Batch Loss: 0.12311360985040665\n",
      "Epoch 4827, Loss: 0.13822028785943985, Final Batch Loss: 0.06684471666812897\n",
      "Epoch 4828, Loss: 0.1953054554760456, Final Batch Loss: 0.14178195595741272\n",
      "Epoch 4829, Loss: 0.13996296003460884, Final Batch Loss: 0.04517063871026039\n",
      "Epoch 4830, Loss: 0.1500202938914299, Final Batch Loss: 0.0664496123790741\n",
      "Epoch 4831, Loss: 0.15638048946857452, Final Batch Loss: 0.06249653548002243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4832, Loss: 0.1701137125492096, Final Batch Loss: 0.13305124640464783\n",
      "Epoch 4833, Loss: 0.15067971497774124, Final Batch Loss: 0.048723623156547546\n",
      "Epoch 4834, Loss: 0.12941860035061836, Final Batch Loss: 0.05713193491101265\n",
      "Epoch 4835, Loss: 0.1709076724946499, Final Batch Loss: 0.05307774618268013\n",
      "Epoch 4836, Loss: 0.15513763576745987, Final Batch Loss: 0.09255702048540115\n",
      "Epoch 4837, Loss: 0.12741341069340706, Final Batch Loss: 0.0462990440428257\n",
      "Epoch 4838, Loss: 0.2634905204176903, Final Batch Loss: 0.2044958472251892\n",
      "Epoch 4839, Loss: 0.1707545444369316, Final Batch Loss: 0.09536179900169373\n",
      "Epoch 4840, Loss: 0.12183723226189613, Final Batch Loss: 0.0617724172770977\n",
      "Epoch 4841, Loss: 0.22956060618162155, Final Batch Loss: 0.13422372937202454\n",
      "Epoch 4842, Loss: 0.1542653739452362, Final Batch Loss: 0.0943942442536354\n",
      "Epoch 4843, Loss: 0.18272069841623306, Final Batch Loss: 0.09638616442680359\n",
      "Epoch 4844, Loss: 0.169177807867527, Final Batch Loss: 0.07204621285200119\n",
      "Epoch 4845, Loss: 0.1413177102804184, Final Batch Loss: 0.07078707218170166\n",
      "Epoch 4846, Loss: 0.13538504391908646, Final Batch Loss: 0.055720992386341095\n",
      "Epoch 4847, Loss: 0.28017063438892365, Final Batch Loss: 0.11632005870342255\n",
      "Epoch 4848, Loss: 0.12366443499922752, Final Batch Loss: 0.05312589183449745\n",
      "Epoch 4849, Loss: 0.1990191414952278, Final Batch Loss: 0.10868184268474579\n",
      "Epoch 4850, Loss: 0.14613229781389236, Final Batch Loss: 0.04160761833190918\n",
      "Epoch 4851, Loss: 0.23200491070747375, Final Batch Loss: 0.15817312896251678\n",
      "Epoch 4852, Loss: 0.23512198776006699, Final Batch Loss: 0.11101587861776352\n",
      "Epoch 4853, Loss: 0.2207147255539894, Final Batch Loss: 0.08962609618902206\n",
      "Epoch 4854, Loss: 0.3460255563259125, Final Batch Loss: 0.1726173311471939\n",
      "Epoch 4855, Loss: 0.16789034754037857, Final Batch Loss: 0.07072893530130386\n",
      "Epoch 4856, Loss: 0.2466764599084854, Final Batch Loss: 0.07133130729198456\n",
      "Epoch 4857, Loss: 0.23620464652776718, Final Batch Loss: 0.16131995618343353\n",
      "Epoch 4858, Loss: 0.22186002880334854, Final Batch Loss: 0.11271758377552032\n",
      "Epoch 4859, Loss: 0.1914946399629116, Final Batch Loss: 0.04899298772215843\n",
      "Epoch 4860, Loss: 0.2109765037894249, Final Batch Loss: 0.11038843542337418\n",
      "Epoch 4861, Loss: 0.18920107930898666, Final Batch Loss: 0.09813925623893738\n",
      "Epoch 4862, Loss: 0.28143465518951416, Final Batch Loss: 0.15577079355716705\n",
      "Epoch 4863, Loss: 0.20848321914672852, Final Batch Loss: 0.06512464582920074\n",
      "Epoch 4864, Loss: 0.16270502656698227, Final Batch Loss: 0.07454042881727219\n",
      "Epoch 4865, Loss: 0.21380964666604996, Final Batch Loss: 0.08373145014047623\n",
      "Epoch 4866, Loss: 0.23180241137742996, Final Batch Loss: 0.1171044185757637\n",
      "Epoch 4867, Loss: 0.19008871167898178, Final Batch Loss: 0.08232121914625168\n",
      "Epoch 4868, Loss: 0.23660440742969513, Final Batch Loss: 0.14055666327476501\n",
      "Epoch 4869, Loss: 0.16928771138191223, Final Batch Loss: 0.06268778443336487\n",
      "Epoch 4870, Loss: 0.16242582350969315, Final Batch Loss: 0.0646652951836586\n",
      "Epoch 4871, Loss: 0.15447017550468445, Final Batch Loss: 0.07853702455759048\n",
      "Epoch 4872, Loss: 0.13303271681070328, Final Batch Loss: 0.08292671293020248\n",
      "Epoch 4873, Loss: 0.12243392691016197, Final Batch Loss: 0.05003737285733223\n",
      "Epoch 4874, Loss: 0.14773715287446976, Final Batch Loss: 0.06867644935846329\n",
      "Epoch 4875, Loss: 0.17844048887491226, Final Batch Loss: 0.08731510490179062\n",
      "Epoch 4876, Loss: 0.12197966873645782, Final Batch Loss: 0.04622911661863327\n",
      "Epoch 4877, Loss: 0.22774704545736313, Final Batch Loss: 0.08506279438734055\n",
      "Epoch 4878, Loss: 0.23045066744089127, Final Batch Loss: 0.10716881603002548\n",
      "Epoch 4879, Loss: 0.17271948978304863, Final Batch Loss: 0.04790661111474037\n",
      "Epoch 4880, Loss: 0.148090697824955, Final Batch Loss: 0.05948137491941452\n",
      "Epoch 4881, Loss: 0.1921003982424736, Final Batch Loss: 0.09256037324666977\n",
      "Epoch 4882, Loss: 0.2978350445628166, Final Batch Loss: 0.22806398570537567\n",
      "Epoch 4883, Loss: 0.12504839524626732, Final Batch Loss: 0.038953643292188644\n",
      "Epoch 4884, Loss: 0.1608663573861122, Final Batch Loss: 0.05155104398727417\n",
      "Epoch 4885, Loss: 0.1703781858086586, Final Batch Loss: 0.10729210078716278\n",
      "Epoch 4886, Loss: 0.1508624404668808, Final Batch Loss: 0.07802645117044449\n",
      "Epoch 4887, Loss: 0.17177168279886246, Final Batch Loss: 0.0989353209733963\n",
      "Epoch 4888, Loss: 0.21403638273477554, Final Batch Loss: 0.09106884151697159\n",
      "Epoch 4889, Loss: 0.16689752042293549, Final Batch Loss: 0.0833127498626709\n",
      "Epoch 4890, Loss: 0.14714785665273666, Final Batch Loss: 0.04823009669780731\n",
      "Epoch 4891, Loss: 0.17505380511283875, Final Batch Loss: 0.05533524602651596\n",
      "Epoch 4892, Loss: 0.1324773207306862, Final Batch Loss: 0.05536555498838425\n",
      "Epoch 4893, Loss: 0.21183092147111893, Final Batch Loss: 0.12899106740951538\n",
      "Epoch 4894, Loss: 0.19131207466125488, Final Batch Loss: 0.11899272352457047\n",
      "Epoch 4895, Loss: 0.18594175577163696, Final Batch Loss: 0.07533181458711624\n",
      "Epoch 4896, Loss: 0.19521228969097137, Final Batch Loss: 0.10927951335906982\n",
      "Epoch 4897, Loss: 0.18020090460777283, Final Batch Loss: 0.08522789180278778\n",
      "Epoch 4898, Loss: 0.12356527894735336, Final Batch Loss: 0.047220341861248016\n",
      "Epoch 4899, Loss: 0.1617501825094223, Final Batch Loss: 0.08469288051128387\n",
      "Epoch 4900, Loss: 0.17865002527832985, Final Batch Loss: 0.05859241262078285\n",
      "Epoch 4901, Loss: 0.1630503460764885, Final Batch Loss: 0.049669839441776276\n",
      "Epoch 4902, Loss: 0.1954391598701477, Final Batch Loss: 0.10446969419717789\n",
      "Epoch 4903, Loss: 0.24745091050863266, Final Batch Loss: 0.10621369630098343\n",
      "Epoch 4904, Loss: 0.1781240701675415, Final Batch Loss: 0.09463123977184296\n",
      "Epoch 4905, Loss: 0.18164695799350739, Final Batch Loss: 0.102553591132164\n",
      "Epoch 4906, Loss: 0.1579616293311119, Final Batch Loss: 0.09035696089267731\n",
      "Epoch 4907, Loss: 0.24021806567907333, Final Batch Loss: 0.17227859795093536\n",
      "Epoch 4908, Loss: 0.1374247893691063, Final Batch Loss: 0.056597113609313965\n",
      "Epoch 4909, Loss: 0.18718761950731277, Final Batch Loss: 0.10769382119178772\n",
      "Epoch 4910, Loss: 0.17560527846217155, Final Batch Loss: 0.13977938890457153\n",
      "Epoch 4911, Loss: 0.15167304128408432, Final Batch Loss: 0.076283298432827\n",
      "Epoch 4912, Loss: 0.17167701944708824, Final Batch Loss: 0.051308345049619675\n",
      "Epoch 4913, Loss: 0.11477167904376984, Final Batch Loss: 0.04742562770843506\n",
      "Epoch 4914, Loss: 0.16522076725959778, Final Batch Loss: 0.05939236283302307\n",
      "Epoch 4915, Loss: 0.16398967057466507, Final Batch Loss: 0.0737495869398117\n",
      "Epoch 4916, Loss: 0.16225259006023407, Final Batch Loss: 0.06578417867422104\n",
      "Epoch 4917, Loss: 0.20373379439115524, Final Batch Loss: 0.12398797273635864\n",
      "Epoch 4918, Loss: 0.12304902076721191, Final Batch Loss: 0.036927491426467896\n",
      "Epoch 4919, Loss: 0.12312984094023705, Final Batch Loss: 0.05718013271689415\n",
      "Epoch 4920, Loss: 0.11400185152888298, Final Batch Loss: 0.03835071250796318\n",
      "Epoch 4921, Loss: 0.17875873297452927, Final Batch Loss: 0.07814834266901016\n",
      "Epoch 4922, Loss: 0.15816691517829895, Final Batch Loss: 0.09270188957452774\n",
      "Epoch 4923, Loss: 0.1278529167175293, Final Batch Loss: 0.05681786686182022\n",
      "Epoch 4924, Loss: 0.13091344013810158, Final Batch Loss: 0.0601532943546772\n",
      "Epoch 4925, Loss: 0.146550714969635, Final Batch Loss: 0.0928909182548523\n",
      "Epoch 4926, Loss: 0.14609690755605698, Final Batch Loss: 0.07465887814760208\n",
      "Epoch 4927, Loss: 0.16049397736787796, Final Batch Loss: 0.060201890766620636\n",
      "Epoch 4928, Loss: 0.13588542491197586, Final Batch Loss: 0.06753671169281006\n",
      "Epoch 4929, Loss: 0.15260744839906693, Final Batch Loss: 0.03530127555131912\n",
      "Epoch 4930, Loss: 0.1497846469283104, Final Batch Loss: 0.07817916572093964\n",
      "Epoch 4931, Loss: 0.1726876050233841, Final Batch Loss: 0.07663377374410629\n",
      "Epoch 4932, Loss: 0.17269311845302582, Final Batch Loss: 0.08258068561553955\n",
      "Epoch 4933, Loss: 0.18588374555110931, Final Batch Loss: 0.08546584844589233\n",
      "Epoch 4934, Loss: 0.18142451345920563, Final Batch Loss: 0.09037454426288605\n",
      "Epoch 4935, Loss: 0.1984909176826477, Final Batch Loss: 0.10447568446397781\n",
      "Epoch 4936, Loss: 0.1615530252456665, Final Batch Loss: 0.08236008137464523\n",
      "Epoch 4937, Loss: 0.14718138054013252, Final Batch Loss: 0.10764742642641068\n",
      "Epoch 4938, Loss: 0.18631822988390923, Final Batch Loss: 0.05696400627493858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4939, Loss: 0.11461512371897697, Final Batch Loss: 0.05780003219842911\n",
      "Epoch 4940, Loss: 0.16652341932058334, Final Batch Loss: 0.10630413144826889\n",
      "Epoch 4941, Loss: 0.1531054452061653, Final Batch Loss: 0.08459313213825226\n",
      "Epoch 4942, Loss: 0.167655348777771, Final Batch Loss: 0.04760632663965225\n",
      "Epoch 4943, Loss: 0.14165503904223442, Final Batch Loss: 0.052018191665410995\n",
      "Epoch 4944, Loss: 0.12760454416275024, Final Batch Loss: 0.06867600977420807\n",
      "Epoch 4945, Loss: 0.20927802473306656, Final Batch Loss: 0.06973565369844437\n",
      "Epoch 4946, Loss: 0.14549430087208748, Final Batch Loss: 0.06043146923184395\n",
      "Epoch 4947, Loss: 0.12264504283666611, Final Batch Loss: 0.07052907347679138\n",
      "Epoch 4948, Loss: 0.14170612394809723, Final Batch Loss: 0.06407158821821213\n",
      "Epoch 4949, Loss: 0.09648694470524788, Final Batch Loss: 0.04169980809092522\n",
      "Epoch 4950, Loss: 0.16252537444233894, Final Batch Loss: 0.11130280792713165\n",
      "Epoch 4951, Loss: 0.19197626411914825, Final Batch Loss: 0.06219252943992615\n",
      "Epoch 4952, Loss: 0.16363022476434708, Final Batch Loss: 0.07177317142486572\n",
      "Epoch 4953, Loss: 0.21156634390354156, Final Batch Loss: 0.1087762713432312\n",
      "Epoch 4954, Loss: 0.14884621649980545, Final Batch Loss: 0.0771244689822197\n",
      "Epoch 4955, Loss: 0.17076104879379272, Final Batch Loss: 0.10974450409412384\n",
      "Epoch 4956, Loss: 0.16103610768914223, Final Batch Loss: 0.1076858639717102\n",
      "Epoch 4957, Loss: 0.18745241314172745, Final Batch Loss: 0.11109078675508499\n",
      "Epoch 4958, Loss: 0.1888817846775055, Final Batch Loss: 0.09299436211585999\n",
      "Epoch 4959, Loss: 0.11427220329642296, Final Batch Loss: 0.03294573351740837\n",
      "Epoch 4960, Loss: 0.12401985377073288, Final Batch Loss: 0.06755976378917694\n",
      "Epoch 4961, Loss: 0.17019279301166534, Final Batch Loss: 0.11015255004167557\n",
      "Epoch 4962, Loss: 0.1439317911863327, Final Batch Loss: 0.08229336142539978\n",
      "Epoch 4963, Loss: 0.12891987338662148, Final Batch Loss: 0.07373760640621185\n",
      "Epoch 4964, Loss: 0.122945137321949, Final Batch Loss: 0.07667388021945953\n",
      "Epoch 4965, Loss: 0.1335923746228218, Final Batch Loss: 0.06979546695947647\n",
      "Epoch 4966, Loss: 0.13908206671476364, Final Batch Loss: 0.05574814975261688\n",
      "Epoch 4967, Loss: 0.17120396345853806, Final Batch Loss: 0.03321171551942825\n",
      "Epoch 4968, Loss: 0.10217995196580887, Final Batch Loss: 0.036759063601493835\n",
      "Epoch 4969, Loss: 0.12874682247638702, Final Batch Loss: 0.07408437132835388\n",
      "Epoch 4970, Loss: 0.2672718018293381, Final Batch Loss: 0.19470646977424622\n",
      "Epoch 4971, Loss: 0.1367669329047203, Final Batch Loss: 0.04716343432664871\n",
      "Epoch 4972, Loss: 0.1149015985429287, Final Batch Loss: 0.04458462819457054\n",
      "Epoch 4973, Loss: 0.13172048330307007, Final Batch Loss: 0.08721891045570374\n",
      "Epoch 4974, Loss: 0.14342773333191872, Final Batch Loss: 0.098509281873703\n",
      "Epoch 4975, Loss: 0.16881567239761353, Final Batch Loss: 0.11639479547739029\n",
      "Epoch 4976, Loss: 0.14430587366223335, Final Batch Loss: 0.09652841091156006\n",
      "Epoch 4977, Loss: 0.12005334347486496, Final Batch Loss: 0.07026617974042892\n",
      "Epoch 4978, Loss: 0.1561433970928192, Final Batch Loss: 0.07747988402843475\n",
      "Epoch 4979, Loss: 0.2104054130613804, Final Batch Loss: 0.1548311561346054\n",
      "Epoch 4980, Loss: 0.1488826423883438, Final Batch Loss: 0.09240511804819107\n",
      "Epoch 4981, Loss: 0.13403258845210075, Final Batch Loss: 0.057115767151117325\n",
      "Epoch 4982, Loss: 0.1009046919643879, Final Batch Loss: 0.04701223969459534\n",
      "Epoch 4983, Loss: 0.16444944590330124, Final Batch Loss: 0.07799773663282394\n",
      "Epoch 4984, Loss: 0.11640814319252968, Final Batch Loss: 0.047100309282541275\n",
      "Epoch 4985, Loss: 0.16465404629707336, Final Batch Loss: 0.10859019309282303\n",
      "Epoch 4986, Loss: 0.1303638033568859, Final Batch Loss: 0.05762328580021858\n",
      "Epoch 4987, Loss: 0.13920940458774567, Final Batch Loss: 0.05054480582475662\n",
      "Epoch 4988, Loss: 0.1359548382461071, Final Batch Loss: 0.05770832672715187\n",
      "Epoch 4989, Loss: 0.13386734202504158, Final Batch Loss: 0.07946950197219849\n",
      "Epoch 4990, Loss: 0.20452341437339783, Final Batch Loss: 0.10016700625419617\n",
      "Epoch 4991, Loss: 0.12085551768541336, Final Batch Loss: 0.06387116014957428\n",
      "Epoch 4992, Loss: 0.13717757910490036, Final Batch Loss: 0.0437869131565094\n",
      "Epoch 4993, Loss: 0.1538015902042389, Final Batch Loss: 0.08457954227924347\n",
      "Epoch 4994, Loss: 0.09627244994044304, Final Batch Loss: 0.05608624964952469\n",
      "Epoch 4995, Loss: 0.1417691633105278, Final Batch Loss: 0.09020966291427612\n",
      "Epoch 4996, Loss: 0.0984213761985302, Final Batch Loss: 0.0367949903011322\n",
      "Epoch 4997, Loss: 0.0874703861773014, Final Batch Loss: 0.03809839114546776\n",
      "Epoch 4998, Loss: 0.11554213240742683, Final Batch Loss: 0.06502053886651993\n",
      "Epoch 4999, Loss: 0.1807394027709961, Final Batch Loss: 0.10859331488609314\n",
      "Epoch 5000, Loss: 0.12876933813095093, Final Batch Loss: 0.06680230051279068\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  0  0  0  0  0  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  0  0  0  0  0]\n",
      " [ 0  0  0 13  0  0  0  0  0]\n",
      " [ 0  0  0  0  9  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0]\n",
      " [ 0  0  0  0  0  0  0  0 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        19\n",
      "           1    1.00000   1.00000   1.00000         7\n",
      "           2    1.00000   1.00000   1.00000         7\n",
      "           3    1.00000   1.00000   1.00000        13\n",
      "           4    1.00000   1.00000   1.00000         9\n",
      "           5    1.00000   1.00000   1.00000        10\n",
      "           6    1.00000   1.00000   1.00000        10\n",
      "           7    1.00000   1.00000   1.00000        12\n",
      "           8    1.00000   1.00000   1.00000        13\n",
      "\n",
      "    accuracy                        1.00000       100\n",
      "   macro avg    1.00000   1.00000   1.00000       100\n",
      "weighted avg    1.00000   1.00000   1.00000       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A0 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_1 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_1 = np.zeros(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A1 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_2 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_2 = np.ones(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A2 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_3 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_3 = np.ones(n_samples) + 1\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A0 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_4 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_4 = np.ones(n_samples) + 2\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A1 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_5 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_5 = np.ones(n_samples) + 3\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A2 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_6 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_6 = np.ones(n_samples) + 4\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A0 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_7 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_7 = np.ones(n_samples) + 5\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A1 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_8 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_8 = np.ones(n_samples) + 6\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A2 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_9 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_9 = np.ones(n_samples) + 7\n",
    "\n",
    "fake_features = np.concatenate((fake_features_1, fake_features_2, fake_features_3, fake_features_4, fake_features_5, fake_features_6,\n",
    "                         fake_features_7, fake_features_8, fake_features_9))\n",
    "fake_labels = np.concatenate((y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9))\n",
    "\n",
    "fake_features = torch.Tensor(fake_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  0  0  0  0  0  0  0  0]\n",
      " [ 0 20  0  0  0  0  0  0  0]\n",
      " [ 0  0 19  0  0  0  0  0  1]\n",
      " [ 0  0  0 20  0  0  0  0  0]\n",
      " [ 0  0  0  2 18  0  0  0  0]\n",
      " [ 0  0  1  0  0 17  0  0  2]\n",
      " [ 0  0  0  9  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0 20  0]\n",
      " [ 0  0  0  0  0  6  0  0 14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    1.00000   1.00000   1.00000        20\n",
      "         1.0    1.00000   1.00000   1.00000        20\n",
      "         2.0    0.95000   0.95000   0.95000        20\n",
      "         3.0    0.64516   1.00000   0.78431        20\n",
      "         4.0    1.00000   0.90000   0.94737        20\n",
      "         5.0    0.73913   0.85000   0.79070        20\n",
      "         6.0    1.00000   0.55000   0.70968        20\n",
      "         7.0    1.00000   1.00000   1.00000        20\n",
      "         8.0    0.82353   0.70000   0.75676        20\n",
      "\n",
      "    accuracy                        0.88333       180\n",
      "   macro avg    0.90642   0.88333   0.88209       180\n",
      "weighted avg    0.90642   0.88333   0.88209       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
