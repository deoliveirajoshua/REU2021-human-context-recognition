{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '141 tBodyGyro-iqr()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '434 fBodyGyro-max()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>42 tGravityAcc-mean()-Y</th>\n",
       "      <th>43 tGravityAcc-mean()-Z</th>\n",
       "      <th>51 tGravityAcc-max()-Y</th>\n",
       "      <th>52 tGravityAcc-max()-Z</th>\n",
       "      <th>54 tGravityAcc-min()-Y</th>\n",
       "      <th>55 tGravityAcc-min()-Z</th>\n",
       "      <th>56 tGravityAcc-sma()</th>\n",
       "      <th>58 tGravityAcc-energy()-Y</th>\n",
       "      <th>59 tGravityAcc-energy()-Z</th>\n",
       "      <th>128 tBodyGyro-mad()-Y</th>\n",
       "      <th>...</th>\n",
       "      <th>282 fBodyAcc-energy()-X</th>\n",
       "      <th>303 fBodyAcc-bandsEnergy()-1,8</th>\n",
       "      <th>311 fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <th>315 fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <th>382 fBodyAccJerk-bandsEnergy()-1,8</th>\n",
       "      <th>504 fBodyAccMag-std()</th>\n",
       "      <th>505 fBodyAccMag-mad()</th>\n",
       "      <th>509 fBodyAccMag-energy()</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.140840</td>\n",
       "      <td>0.115375</td>\n",
       "      <td>-0.161265</td>\n",
       "      <td>0.124660</td>\n",
       "      <td>-0.123213</td>\n",
       "      <td>0.056483</td>\n",
       "      <td>-0.375426</td>\n",
       "      <td>-0.970905</td>\n",
       "      <td>-0.975510</td>\n",
       "      <td>-0.976353</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.956134</td>\n",
       "      <td>-0.948870</td>\n",
       "      <td>-0.998285</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.141551</td>\n",
       "      <td>0.109379</td>\n",
       "      <td>-0.161343</td>\n",
       "      <td>0.122586</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.383430</td>\n",
       "      <td>-0.970583</td>\n",
       "      <td>-0.978500</td>\n",
       "      <td>-0.989038</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.975866</td>\n",
       "      <td>-0.975777</td>\n",
       "      <td>-0.999472</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.142010</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.094566</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.401602</td>\n",
       "      <td>-0.970368</td>\n",
       "      <td>-0.981672</td>\n",
       "      <td>-0.994122</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.999972</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.989015</td>\n",
       "      <td>-0.985594</td>\n",
       "      <td>-0.999807</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.143976</td>\n",
       "      <td>0.099850</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.093425</td>\n",
       "      <td>-0.121336</td>\n",
       "      <td>0.095753</td>\n",
       "      <td>-0.400278</td>\n",
       "      <td>-0.969400</td>\n",
       "      <td>-0.982420</td>\n",
       "      <td>-0.993142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999975</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-0.986742</td>\n",
       "      <td>-0.983524</td>\n",
       "      <td>-0.999770</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.148750</td>\n",
       "      <td>0.094486</td>\n",
       "      <td>-0.166786</td>\n",
       "      <td>0.091682</td>\n",
       "      <td>-0.121834</td>\n",
       "      <td>0.094059</td>\n",
       "      <td>-0.400477</td>\n",
       "      <td>-0.967051</td>\n",
       "      <td>-0.984363</td>\n",
       "      <td>-0.992542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-0.990063</td>\n",
       "      <td>-0.992324</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>-0.222004</td>\n",
       "      <td>-0.039492</td>\n",
       "      <td>-0.214233</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.071977</td>\n",
       "      <td>-0.405132</td>\n",
       "      <td>-0.918375</td>\n",
       "      <td>-0.995193</td>\n",
       "      <td>0.065142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.674230</td>\n",
       "      <td>-0.684177</td>\n",
       "      <td>-0.666429</td>\n",
       "      <td>-0.668164</td>\n",
       "      <td>-0.839256</td>\n",
       "      <td>-0.232600</td>\n",
       "      <td>-0.007392</td>\n",
       "      <td>-0.584282</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>-0.242054</td>\n",
       "      <td>-0.039863</td>\n",
       "      <td>-0.231477</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.358934</td>\n",
       "      <td>-0.902880</td>\n",
       "      <td>-0.995151</td>\n",
       "      <td>0.091791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.705580</td>\n",
       "      <td>-0.726986</td>\n",
       "      <td>-0.704444</td>\n",
       "      <td>-0.705435</td>\n",
       "      <td>-0.854278</td>\n",
       "      <td>-0.275373</td>\n",
       "      <td>-0.172448</td>\n",
       "      <td>-0.632536</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>-0.236950</td>\n",
       "      <td>-0.026805</td>\n",
       "      <td>-0.249134</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.216004</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.377025</td>\n",
       "      <td>-0.907561</td>\n",
       "      <td>-0.995450</td>\n",
       "      <td>0.170686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.692379</td>\n",
       "      <td>-0.655263</td>\n",
       "      <td>-0.674515</td>\n",
       "      <td>-0.684729</td>\n",
       "      <td>-0.815380</td>\n",
       "      <td>-0.220288</td>\n",
       "      <td>-0.216074</td>\n",
       "      <td>-0.641170</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>-0.233230</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>-0.244267</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.210542</td>\n",
       "      <td>-0.040009</td>\n",
       "      <td>-0.440050</td>\n",
       "      <td>-0.910648</td>\n",
       "      <td>-0.998824</td>\n",
       "      <td>0.178939</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.693098</td>\n",
       "      <td>-0.643425</td>\n",
       "      <td>-0.677215</td>\n",
       "      <td>-0.685088</td>\n",
       "      <td>-0.822905</td>\n",
       "      <td>-0.234539</td>\n",
       "      <td>-0.220443</td>\n",
       "      <td>-0.663579</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>-0.233292</td>\n",
       "      <td>-0.020954</td>\n",
       "      <td>-0.240956</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.212149</td>\n",
       "      <td>-0.047491</td>\n",
       "      <td>-0.432003</td>\n",
       "      <td>-0.910579</td>\n",
       "      <td>-0.998144</td>\n",
       "      <td>-0.073681</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.731037</td>\n",
       "      <td>-0.709495</td>\n",
       "      <td>-0.728519</td>\n",
       "      <td>-0.727441</td>\n",
       "      <td>-0.834215</td>\n",
       "      <td>-0.342670</td>\n",
       "      <td>-0.146649</td>\n",
       "      <td>-0.698087</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      42 tGravityAcc-mean()-Y  43 tGravityAcc-mean()-Z  \\\n",
       "0                   -0.140840                 0.115375   \n",
       "1                   -0.141551                 0.109379   \n",
       "2                   -0.142010                 0.101884   \n",
       "3                   -0.143976                 0.099850   \n",
       "4                   -0.148750                 0.094486   \n",
       "...                       ...                      ...   \n",
       "7347                -0.222004                -0.039492   \n",
       "7348                -0.242054                -0.039863   \n",
       "7349                -0.236950                -0.026805   \n",
       "7350                -0.233230                -0.004984   \n",
       "7351                -0.233292                -0.020954   \n",
       "\n",
       "      51 tGravityAcc-max()-Y  52 tGravityAcc-max()-Z  54 tGravityAcc-min()-Y  \\\n",
       "0                  -0.161265                0.124660               -0.123213   \n",
       "1                  -0.161343                0.122586               -0.114893   \n",
       "2                  -0.163711                0.094566               -0.114893   \n",
       "3                  -0.163711                0.093425               -0.121336   \n",
       "4                  -0.166786                0.091682               -0.121834   \n",
       "...                      ...                     ...                     ...   \n",
       "7347               -0.214233               -0.016391               -0.234998   \n",
       "7348               -0.231477               -0.016391               -0.234998   \n",
       "7349               -0.249134                0.024684               -0.216004   \n",
       "7350               -0.244267                0.024684               -0.210542   \n",
       "7351               -0.240956                0.003031               -0.212149   \n",
       "\n",
       "      55 tGravityAcc-min()-Z  56 tGravityAcc-sma()  58 tGravityAcc-energy()-Y  \\\n",
       "0                   0.056483             -0.375426                  -0.970905   \n",
       "1                   0.102764             -0.383430                  -0.970583   \n",
       "2                   0.102764             -0.401602                  -0.970368   \n",
       "3                   0.095753             -0.400278                  -0.969400   \n",
       "4                   0.094059             -0.400477                  -0.967051   \n",
       "...                      ...                   ...                        ...   \n",
       "7347               -0.071977             -0.405132                  -0.918375   \n",
       "7348               -0.068919             -0.358934                  -0.902880   \n",
       "7349               -0.068919             -0.377025                  -0.907561   \n",
       "7350               -0.040009             -0.440050                  -0.910648   \n",
       "7351               -0.047491             -0.432003                  -0.910579   \n",
       "\n",
       "      59 tGravityAcc-energy()-Z  128 tBodyGyro-mad()-Y  ...  \\\n",
       "0                     -0.975510              -0.976353  ...   \n",
       "1                     -0.978500              -0.989038  ...   \n",
       "2                     -0.981672              -0.994122  ...   \n",
       "3                     -0.982420              -0.993142  ...   \n",
       "4                     -0.984363              -0.992542  ...   \n",
       "...                         ...                    ...  ...   \n",
       "7347                  -0.995193               0.065142  ...   \n",
       "7348                  -0.995151               0.091791  ...   \n",
       "7349                  -0.995450               0.170686  ...   \n",
       "7350                  -0.998824               0.178939  ...   \n",
       "7351                  -0.998144              -0.073681  ...   \n",
       "\n",
       "      282 fBodyAcc-energy()-X  303 fBodyAcc-bandsEnergy()-1,8  \\\n",
       "0                   -0.999968                       -0.999963   \n",
       "1                   -0.999991                       -0.999996   \n",
       "2                   -0.999969                       -0.999989   \n",
       "3                   -0.999975                       -0.999989   \n",
       "4                   -0.999990                       -0.999994   \n",
       "...                       ...                             ...   \n",
       "7347                -0.674230                       -0.684177   \n",
       "7348                -0.705580                       -0.726986   \n",
       "7349                -0.692379                       -0.655263   \n",
       "7350                -0.693098                       -0.643425   \n",
       "7351                -0.731037                       -0.709495   \n",
       "\n",
       "      311 fBodyAcc-bandsEnergy()-1,16  315 fBodyAcc-bandsEnergy()-1,24  \\\n",
       "0                           -0.999969                        -0.999971   \n",
       "1                           -0.999994                        -0.999992   \n",
       "2                           -0.999983                        -0.999972   \n",
       "3                           -0.999986                        -0.999977   \n",
       "4                           -0.999993                        -0.999991   \n",
       "...                               ...                              ...   \n",
       "7347                        -0.666429                        -0.668164   \n",
       "7348                        -0.704444                        -0.705435   \n",
       "7349                        -0.674515                        -0.684729   \n",
       "7350                        -0.677215                        -0.685088   \n",
       "7351                        -0.728519                        -0.727441   \n",
       "\n",
       "      382 fBodyAccJerk-bandsEnergy()-1,8  504 fBodyAccMag-std()  \\\n",
       "0                              -0.999986              -0.956134   \n",
       "1                              -0.999996              -0.975866   \n",
       "2                              -0.999994              -0.989015   \n",
       "3                              -0.999998              -0.986742   \n",
       "4                              -0.999995              -0.990063   \n",
       "...                                  ...                    ...   \n",
       "7347                           -0.839256              -0.232600   \n",
       "7348                           -0.854278              -0.275373   \n",
       "7349                           -0.815380              -0.220288   \n",
       "7350                           -0.822905              -0.234539   \n",
       "7351                           -0.834215              -0.342670   \n",
       "\n",
       "      505 fBodyAccMag-mad()  509 fBodyAccMag-energy()  Subject  Activity  \n",
       "0                 -0.948870                 -0.998285        1         5  \n",
       "1                 -0.975777                 -0.999472        1         5  \n",
       "2                 -0.985594                 -0.999807        1         5  \n",
       "3                 -0.983524                 -0.999770        1         5  \n",
       "4                 -0.992324                 -0.999873        1         5  \n",
       "...                     ...                       ...      ...       ...  \n",
       "7347              -0.007392                 -0.584282       30         2  \n",
       "7348              -0.172448                 -0.632536       30         2  \n",
       "7349              -0.216074                 -0.641170       30         2  \n",
       "7350              -0.220443                 -0.663579       30         2  \n",
       "7351              -0.146649                 -0.698087       30         2  \n",
       "\n",
       "[7352 rows x 39 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_names = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "train_column_names = train_names.values.tolist()\n",
    "train_column_names = [k for row in train_column_names for k in row]\n",
    "\n",
    "train_data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None)\n",
    "train_data.columns = train_column_names\n",
    "\n",
    "### Single dataframe column\n",
    "\n",
    "y_train = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "y_train.columns = ['Subject']\n",
    "\n",
    "y_train_activity = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "y_train_activity.columns = ['Activity']\n",
    "\n",
    "X_train_1 = train_data[sub_features]\n",
    "X_train_2 = train_data[act_features]\n",
    "X_train_data = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "X_train_data = pd.concat([X_train_data, y_train, y_train_activity], axis = 1)\n",
    "X_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_data[(X_train_data['Subject'].isin([1, 3, 5, 7])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,:-2].values\n",
    "y_train = X_train_data[(X_train_data['Subject'].isin([1, 3, 5, 7])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,-2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y_train)):\n",
    "    if y_train[k] == 1:\n",
    "        y_train[k] = 0\n",
    "    elif y_train[k] == 3:\n",
    "        y_train[k] = 1\n",
    "    elif y_train[k] == 5:\n",
    "        y_train[k] = 2\n",
    "    else:\n",
    "        y_train[k] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 30),\n",
    "            classifier_block(30, 20),\n",
    "            classifier_block(20, 20),\n",
    "            classifier_block(20, 10),\n",
    "            nn.Linear(10, 4)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.163192510604858, Final Batch Loss: 1.3853248357772827\n",
      "Epoch 2, Loss: 4.163917779922485, Final Batch Loss: 1.388655424118042\n",
      "Epoch 3, Loss: 4.169805645942688, Final Batch Loss: 1.4020342826843262\n",
      "Epoch 4, Loss: 4.155314564704895, Final Batch Loss: 1.3812613487243652\n",
      "Epoch 5, Loss: 4.155547618865967, Final Batch Loss: 1.3898611068725586\n",
      "Epoch 6, Loss: 4.139104604721069, Final Batch Loss: 1.372504711151123\n",
      "Epoch 7, Loss: 4.1508142948150635, Final Batch Loss: 1.3882583379745483\n",
      "Epoch 8, Loss: 4.149568676948547, Final Batch Loss: 1.3921146392822266\n",
      "Epoch 9, Loss: 4.131005644798279, Final Batch Loss: 1.3700803518295288\n",
      "Epoch 10, Loss: 4.137405514717102, Final Batch Loss: 1.3789788484573364\n",
      "Epoch 11, Loss: 4.117838144302368, Final Batch Loss: 1.3619083166122437\n",
      "Epoch 12, Loss: 4.120696187019348, Final Batch Loss: 1.3629494905471802\n",
      "Epoch 13, Loss: 4.124494314193726, Final Batch Loss: 1.3772684335708618\n",
      "Epoch 14, Loss: 4.11756956577301, Final Batch Loss: 1.372423529624939\n",
      "Epoch 15, Loss: 4.123033046722412, Final Batch Loss: 1.3818365335464478\n",
      "Epoch 16, Loss: 4.099458336830139, Final Batch Loss: 1.3623645305633545\n",
      "Epoch 17, Loss: 4.075745582580566, Final Batch Loss: 1.3424763679504395\n",
      "Epoch 18, Loss: 4.091838240623474, Final Batch Loss: 1.3677574396133423\n",
      "Epoch 19, Loss: 4.056906342506409, Final Batch Loss: 1.3370972871780396\n",
      "Epoch 20, Loss: 4.052496433258057, Final Batch Loss: 1.3481651544570923\n",
      "Epoch 21, Loss: 4.054921865463257, Final Batch Loss: 1.3607852458953857\n",
      "Epoch 22, Loss: 3.991451859474182, Final Batch Loss: 1.3065637350082397\n",
      "Epoch 23, Loss: 3.9932620525360107, Final Batch Loss: 1.3331494331359863\n",
      "Epoch 24, Loss: 3.964608907699585, Final Batch Loss: 1.3210697174072266\n",
      "Epoch 25, Loss: 3.916274905204773, Final Batch Loss: 1.297650933265686\n",
      "Epoch 26, Loss: 3.888715624809265, Final Batch Loss: 1.2927089929580688\n",
      "Epoch 27, Loss: 3.825470447540283, Final Batch Loss: 1.2787282466888428\n",
      "Epoch 28, Loss: 3.7803635597229004, Final Batch Loss: 1.258003830909729\n",
      "Epoch 29, Loss: 3.672320246696472, Final Batch Loss: 1.1987578868865967\n",
      "Epoch 30, Loss: 3.669072151184082, Final Batch Loss: 1.2500725984573364\n",
      "Epoch 31, Loss: 3.5549392700195312, Final Batch Loss: 1.1682175397872925\n",
      "Epoch 32, Loss: 3.464626908302307, Final Batch Loss: 1.1109899282455444\n",
      "Epoch 33, Loss: 3.323870062828064, Final Batch Loss: 1.0433779954910278\n",
      "Epoch 34, Loss: 3.381692886352539, Final Batch Loss: 1.1015863418579102\n",
      "Epoch 35, Loss: 3.2810884714126587, Final Batch Loss: 1.034455418586731\n",
      "Epoch 36, Loss: 3.2687315940856934, Final Batch Loss: 1.0702126026153564\n",
      "Epoch 37, Loss: 3.166415572166443, Final Batch Loss: 1.0103816986083984\n",
      "Epoch 38, Loss: 3.18089759349823, Final Batch Loss: 1.0847291946411133\n",
      "Epoch 39, Loss: 3.0422383546829224, Final Batch Loss: 0.9744013547897339\n",
      "Epoch 40, Loss: 2.97564560174942, Final Batch Loss: 0.9390228986740112\n",
      "Epoch 41, Loss: 3.1084883213043213, Final Batch Loss: 1.1012746095657349\n",
      "Epoch 42, Loss: 2.8829076886177063, Final Batch Loss: 0.9582352042198181\n",
      "Epoch 43, Loss: 2.9128944277763367, Final Batch Loss: 0.9819681644439697\n",
      "Epoch 44, Loss: 2.8691678643226624, Final Batch Loss: 1.0000724792480469\n",
      "Epoch 45, Loss: 2.830375909805298, Final Batch Loss: 0.9320077300071716\n",
      "Epoch 46, Loss: 2.6431251764297485, Final Batch Loss: 0.7969716191291809\n",
      "Epoch 47, Loss: 2.7576549649238586, Final Batch Loss: 0.9641242027282715\n",
      "Epoch 48, Loss: 2.635965645313263, Final Batch Loss: 0.8480059504508972\n",
      "Epoch 49, Loss: 2.67234343290329, Final Batch Loss: 0.9174968600273132\n",
      "Epoch 50, Loss: 2.7026111483573914, Final Batch Loss: 0.9644057154655457\n",
      "Epoch 51, Loss: 2.5013654232025146, Final Batch Loss: 0.7862316966056824\n",
      "Epoch 52, Loss: 2.666735827922821, Final Batch Loss: 0.9648414254188538\n",
      "Epoch 53, Loss: 2.6251545548439026, Final Batch Loss: 0.8922761082649231\n",
      "Epoch 54, Loss: 2.6484490036964417, Final Batch Loss: 1.0356031656265259\n",
      "Epoch 55, Loss: 2.5008367896080017, Final Batch Loss: 0.8450775146484375\n",
      "Epoch 56, Loss: 2.4207160472869873, Final Batch Loss: 0.7916903495788574\n",
      "Epoch 57, Loss: 2.5455791354179382, Final Batch Loss: 0.9001138806343079\n",
      "Epoch 58, Loss: 2.4264150261878967, Final Batch Loss: 0.8428066372871399\n",
      "Epoch 59, Loss: 2.3552306294441223, Final Batch Loss: 0.7395763993263245\n",
      "Epoch 60, Loss: 2.42513769865036, Final Batch Loss: 0.8787872195243835\n",
      "Epoch 61, Loss: 2.3678305745124817, Final Batch Loss: 0.7983017563819885\n",
      "Epoch 62, Loss: 2.221455693244934, Final Batch Loss: 0.7069650292396545\n",
      "Epoch 63, Loss: 2.2566272020339966, Final Batch Loss: 0.717843234539032\n",
      "Epoch 64, Loss: 2.0556840300559998, Final Batch Loss: 0.5273576378822327\n",
      "Epoch 65, Loss: 2.263829827308655, Final Batch Loss: 0.7944138646125793\n",
      "Epoch 66, Loss: 2.1688849925994873, Final Batch Loss: 0.6619204878807068\n",
      "Epoch 67, Loss: 2.366317331790924, Final Batch Loss: 0.8396917581558228\n",
      "Epoch 68, Loss: 2.142732262611389, Final Batch Loss: 0.6151437759399414\n",
      "Epoch 69, Loss: 2.1356794834136963, Final Batch Loss: 0.6686999797821045\n",
      "Epoch 70, Loss: 2.394090175628662, Final Batch Loss: 0.9430846571922302\n",
      "Epoch 71, Loss: 2.1099199056625366, Final Batch Loss: 0.6813871264457703\n",
      "Epoch 72, Loss: 2.1302456855773926, Final Batch Loss: 0.6958439946174622\n",
      "Epoch 73, Loss: 2.0890746116638184, Final Batch Loss: 0.6949856281280518\n",
      "Epoch 74, Loss: 2.0689725279808044, Final Batch Loss: 0.6115081310272217\n",
      "Epoch 75, Loss: 2.013788163661957, Final Batch Loss: 0.6321620345115662\n",
      "Epoch 76, Loss: 2.087886929512024, Final Batch Loss: 0.7553761005401611\n",
      "Epoch 77, Loss: 2.086944878101349, Final Batch Loss: 0.6080880761146545\n",
      "Epoch 78, Loss: 1.9981447458267212, Final Batch Loss: 0.5891923308372498\n",
      "Epoch 79, Loss: 2.199113190174103, Final Batch Loss: 0.8268479108810425\n",
      "Epoch 80, Loss: 2.0910850167274475, Final Batch Loss: 0.6821786761283875\n",
      "Epoch 81, Loss: 2.2128167152404785, Final Batch Loss: 0.7915553450584412\n",
      "Epoch 82, Loss: 1.9117247462272644, Final Batch Loss: 0.5609865784645081\n",
      "Epoch 83, Loss: 2.0263912677764893, Final Batch Loss: 0.6433427333831787\n",
      "Epoch 84, Loss: 2.0427417159080505, Final Batch Loss: 0.7058306932449341\n",
      "Epoch 85, Loss: 2.0172204971313477, Final Batch Loss: 0.6854272484779358\n",
      "Epoch 86, Loss: 2.141596019268036, Final Batch Loss: 0.8193157315254211\n",
      "Epoch 87, Loss: 2.005758821964264, Final Batch Loss: 0.7450535297393799\n",
      "Epoch 88, Loss: 1.9275259375572205, Final Batch Loss: 0.5413770079612732\n",
      "Epoch 89, Loss: 1.955319344997406, Final Batch Loss: 0.6608871221542358\n",
      "Epoch 90, Loss: 1.8844892382621765, Final Batch Loss: 0.6439105272293091\n",
      "Epoch 91, Loss: 1.8162325024604797, Final Batch Loss: 0.5107908844947815\n",
      "Epoch 92, Loss: 1.932966709136963, Final Batch Loss: 0.5925154685974121\n",
      "Epoch 93, Loss: 1.9572625160217285, Final Batch Loss: 0.664075493812561\n",
      "Epoch 94, Loss: 1.9341083765029907, Final Batch Loss: 0.7075676918029785\n",
      "Epoch 95, Loss: 1.8581778407096863, Final Batch Loss: 0.6224586367607117\n",
      "Epoch 96, Loss: 1.9656562209129333, Final Batch Loss: 0.7076115608215332\n",
      "Epoch 97, Loss: 1.9726049900054932, Final Batch Loss: 0.76739102602005\n",
      "Epoch 98, Loss: 1.870123565196991, Final Batch Loss: 0.6318018436431885\n",
      "Epoch 99, Loss: 1.9266242980957031, Final Batch Loss: 0.7145235538482666\n",
      "Epoch 100, Loss: 1.7996670007705688, Final Batch Loss: 0.5714054703712463\n",
      "Epoch 101, Loss: 1.76741224527359, Final Batch Loss: 0.5299131274223328\n",
      "Epoch 102, Loss: 1.7963404059410095, Final Batch Loss: 0.5698791742324829\n",
      "Epoch 103, Loss: 1.7916635274887085, Final Batch Loss: 0.5995771288871765\n",
      "Epoch 104, Loss: 1.7304141521453857, Final Batch Loss: 0.5074698328971863\n",
      "Epoch 105, Loss: 1.7374216318130493, Final Batch Loss: 0.5159310102462769\n",
      "Epoch 106, Loss: 1.729718029499054, Final Batch Loss: 0.5323710441589355\n",
      "Epoch 107, Loss: 1.8941431045532227, Final Batch Loss: 0.6095907688140869\n",
      "Epoch 108, Loss: 1.6124008297920227, Final Batch Loss: 0.4601750373840332\n",
      "Epoch 109, Loss: 1.8230433464050293, Final Batch Loss: 0.5493795275688171\n",
      "Epoch 110, Loss: 1.7537562251091003, Final Batch Loss: 0.5237874984741211\n",
      "Epoch 111, Loss: 1.7347734570503235, Final Batch Loss: 0.572789192199707\n",
      "Epoch 112, Loss: 1.9829062819480896, Final Batch Loss: 0.8645570874214172\n",
      "Epoch 113, Loss: 1.8646259307861328, Final Batch Loss: 0.7326299548149109\n",
      "Epoch 114, Loss: 1.8930965662002563, Final Batch Loss: 0.7091041207313538\n",
      "Epoch 115, Loss: 1.6236295402050018, Final Batch Loss: 0.49149003624916077\n",
      "Epoch 116, Loss: 1.6378863453865051, Final Batch Loss: 0.4602068066596985\n",
      "Epoch 117, Loss: 1.7291222214698792, Final Batch Loss: 0.6155226230621338\n",
      "Epoch 118, Loss: 1.6069195866584778, Final Batch Loss: 0.45510292053222656\n",
      "Epoch 119, Loss: 1.6455759406089783, Final Batch Loss: 0.5183236598968506\n",
      "Epoch 120, Loss: 1.6919502019882202, Final Batch Loss: 0.5593847632408142\n",
      "Epoch 121, Loss: 1.6374576687812805, Final Batch Loss: 0.5602596402168274\n",
      "Epoch 122, Loss: 1.6149618029594421, Final Batch Loss: 0.5303505063056946\n",
      "Epoch 123, Loss: 1.6814422309398651, Final Batch Loss: 0.6123515367507935\n",
      "Epoch 124, Loss: 1.5783582627773285, Final Batch Loss: 0.4464165270328522\n",
      "Epoch 125, Loss: 1.6763171553611755, Final Batch Loss: 0.5593477487564087\n",
      "Epoch 126, Loss: 1.6482468843460083, Final Batch Loss: 0.5481793880462646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127, Loss: 1.5309062004089355, Final Batch Loss: 0.40959763526916504\n",
      "Epoch 128, Loss: 1.7171286344528198, Final Batch Loss: 0.6563536524772644\n",
      "Epoch 129, Loss: 1.4834814071655273, Final Batch Loss: 0.42456698417663574\n",
      "Epoch 130, Loss: 1.5645470321178436, Final Batch Loss: 0.4665113389492035\n",
      "Epoch 131, Loss: 1.6056340038776398, Final Batch Loss: 0.5739187002182007\n",
      "Epoch 132, Loss: 1.6466050744056702, Final Batch Loss: 0.5455926060676575\n",
      "Epoch 133, Loss: 1.526097148656845, Final Batch Loss: 0.5066728591918945\n",
      "Epoch 134, Loss: 1.5178958773612976, Final Batch Loss: 0.37063759565353394\n",
      "Epoch 135, Loss: 1.482317179441452, Final Batch Loss: 0.4835520088672638\n",
      "Epoch 136, Loss: 1.5095603466033936, Final Batch Loss: 0.4977472722530365\n",
      "Epoch 137, Loss: 1.5245345532894135, Final Batch Loss: 0.47429463267326355\n",
      "Epoch 138, Loss: 1.5619279444217682, Final Batch Loss: 0.5626880526542664\n",
      "Epoch 139, Loss: 1.573928952217102, Final Batch Loss: 0.584017276763916\n",
      "Epoch 140, Loss: 1.4654308259487152, Final Batch Loss: 0.368021160364151\n",
      "Epoch 141, Loss: 1.50376358628273, Final Batch Loss: 0.5081267356872559\n",
      "Epoch 142, Loss: 1.5170487463474274, Final Batch Loss: 0.5046886801719666\n",
      "Epoch 143, Loss: 1.6977497935295105, Final Batch Loss: 0.6890053153038025\n",
      "Epoch 144, Loss: 1.4428188502788544, Final Batch Loss: 0.416136234998703\n",
      "Epoch 145, Loss: 1.4990076124668121, Final Batch Loss: 0.45149967074394226\n",
      "Epoch 146, Loss: 1.5400947630405426, Final Batch Loss: 0.5549954771995544\n",
      "Epoch 147, Loss: 1.6133641004562378, Final Batch Loss: 0.603327751159668\n",
      "Epoch 148, Loss: 1.464234620332718, Final Batch Loss: 0.38911810517311096\n",
      "Epoch 149, Loss: 1.4443785846233368, Final Batch Loss: 0.4726123809814453\n",
      "Epoch 150, Loss: 1.3940660059452057, Final Batch Loss: 0.45928648114204407\n",
      "Epoch 151, Loss: 1.5347267985343933, Final Batch Loss: 0.606002688407898\n",
      "Epoch 152, Loss: 1.5264193415641785, Final Batch Loss: 0.5435280799865723\n",
      "Epoch 153, Loss: 1.5499996542930603, Final Batch Loss: 0.5958086848258972\n",
      "Epoch 154, Loss: 1.4519015550613403, Final Batch Loss: 0.516844630241394\n",
      "Epoch 155, Loss: 1.5426499247550964, Final Batch Loss: 0.5520111322402954\n",
      "Epoch 156, Loss: 1.4829194843769073, Final Batch Loss: 0.4597274959087372\n",
      "Epoch 157, Loss: 1.4226278364658356, Final Batch Loss: 0.4227488934993744\n",
      "Epoch 158, Loss: 1.6101633608341217, Final Batch Loss: 0.5918256044387817\n",
      "Epoch 159, Loss: 1.4665262401103973, Final Batch Loss: 0.4954213500022888\n",
      "Epoch 160, Loss: 1.3450627028942108, Final Batch Loss: 0.35303083062171936\n",
      "Epoch 161, Loss: 1.2228624820709229, Final Batch Loss: 0.32174140214920044\n",
      "Epoch 162, Loss: 1.4520753920078278, Final Batch Loss: 0.4718587398529053\n",
      "Epoch 163, Loss: 1.4457366168498993, Final Batch Loss: 0.5165619254112244\n",
      "Epoch 164, Loss: 1.3322674930095673, Final Batch Loss: 0.3992334008216858\n",
      "Epoch 165, Loss: 1.6155714690685272, Final Batch Loss: 0.6699708700180054\n",
      "Epoch 166, Loss: 1.2789704501628876, Final Batch Loss: 0.368645042181015\n",
      "Epoch 167, Loss: 1.5118650197982788, Final Batch Loss: 0.4948469400405884\n",
      "Epoch 168, Loss: 1.5050300657749176, Final Batch Loss: 0.5419513583183289\n",
      "Epoch 169, Loss: 1.444243460893631, Final Batch Loss: 0.5226041078567505\n",
      "Epoch 170, Loss: 1.4401005506515503, Final Batch Loss: 0.5068209767341614\n",
      "Epoch 171, Loss: 1.398180067539215, Final Batch Loss: 0.4378034174442291\n",
      "Epoch 172, Loss: 1.4034995436668396, Final Batch Loss: 0.516252875328064\n",
      "Epoch 173, Loss: 1.365850955247879, Final Batch Loss: 0.46041786670684814\n",
      "Epoch 174, Loss: 1.3603273332118988, Final Batch Loss: 0.4754590392112732\n",
      "Epoch 175, Loss: 1.2629591822624207, Final Batch Loss: 0.3952980935573578\n",
      "Epoch 176, Loss: 1.3757024109363556, Final Batch Loss: 0.4898063838481903\n",
      "Epoch 177, Loss: 1.3983641862869263, Final Batch Loss: 0.4860709309577942\n",
      "Epoch 178, Loss: 1.0769383013248444, Final Batch Loss: 0.2425243854522705\n",
      "Epoch 179, Loss: 1.3844291269779205, Final Batch Loss: 0.45721688866615295\n",
      "Epoch 180, Loss: 1.3561077117919922, Final Batch Loss: 0.3955289423465729\n",
      "Epoch 181, Loss: 1.3334053754806519, Final Batch Loss: 0.45727241039276123\n",
      "Epoch 182, Loss: 1.2737396955490112, Final Batch Loss: 0.39794689416885376\n",
      "Epoch 183, Loss: 1.4249856770038605, Final Batch Loss: 0.5536479353904724\n",
      "Epoch 184, Loss: 1.3281938433647156, Final Batch Loss: 0.42886751890182495\n",
      "Epoch 185, Loss: 1.2965621948242188, Final Batch Loss: 0.4722725749015808\n",
      "Epoch 186, Loss: 1.365769386291504, Final Batch Loss: 0.4633776843547821\n",
      "Epoch 187, Loss: 1.3112500309944153, Final Batch Loss: 0.4094048738479614\n",
      "Epoch 188, Loss: 1.3229486346244812, Final Batch Loss: 0.4246523678302765\n",
      "Epoch 189, Loss: 1.3792214095592499, Final Batch Loss: 0.5606548190116882\n",
      "Epoch 190, Loss: 1.2459052205085754, Final Batch Loss: 0.4049023985862732\n",
      "Epoch 191, Loss: 1.3850207328796387, Final Batch Loss: 0.46995124220848083\n",
      "Epoch 192, Loss: 1.222767412662506, Final Batch Loss: 0.34787440299987793\n",
      "Epoch 193, Loss: 1.2825000584125519, Final Batch Loss: 0.391682505607605\n",
      "Epoch 194, Loss: 1.3438090682029724, Final Batch Loss: 0.47100406885147095\n",
      "Epoch 195, Loss: 1.48664128780365, Final Batch Loss: 0.6690893769264221\n",
      "Epoch 196, Loss: 1.3637646436691284, Final Batch Loss: 0.5088750720024109\n",
      "Epoch 197, Loss: 1.381936401128769, Final Batch Loss: 0.5311052203178406\n",
      "Epoch 198, Loss: 1.2736103534698486, Final Batch Loss: 0.4489303529262543\n",
      "Epoch 199, Loss: 1.3632445633411407, Final Batch Loss: 0.5749585628509521\n",
      "Epoch 200, Loss: 1.3497649133205414, Final Batch Loss: 0.5426416397094727\n",
      "Epoch 201, Loss: 1.2793385684490204, Final Batch Loss: 0.4849143624305725\n",
      "Epoch 202, Loss: 1.1755012273788452, Final Batch Loss: 0.36165720224380493\n",
      "Epoch 203, Loss: 1.3626385629177094, Final Batch Loss: 0.43450507521629333\n",
      "Epoch 204, Loss: 1.4416703581809998, Final Batch Loss: 0.5955352187156677\n",
      "Epoch 205, Loss: 1.2202135622501373, Final Batch Loss: 0.3825918436050415\n",
      "Epoch 206, Loss: 1.3006977438926697, Final Batch Loss: 0.5027066469192505\n",
      "Epoch 207, Loss: 1.2947219014167786, Final Batch Loss: 0.46947452425956726\n",
      "Epoch 208, Loss: 1.528209239244461, Final Batch Loss: 0.7459011673927307\n",
      "Epoch 209, Loss: 1.2825309932231903, Final Batch Loss: 0.3729550838470459\n",
      "Epoch 210, Loss: 1.3456910252571106, Final Batch Loss: 0.5329302549362183\n",
      "Epoch 211, Loss: 1.0898293852806091, Final Batch Loss: 0.2626674771308899\n",
      "Epoch 212, Loss: 1.1626306772232056, Final Batch Loss: 0.3622625470161438\n",
      "Epoch 213, Loss: 1.2072069644927979, Final Batch Loss: 0.3017697334289551\n",
      "Epoch 214, Loss: 1.2794944941997528, Final Batch Loss: 0.4090275764465332\n",
      "Epoch 215, Loss: 1.180967926979065, Final Batch Loss: 0.3328979015350342\n",
      "Epoch 216, Loss: 1.232179194688797, Final Batch Loss: 0.471623033285141\n",
      "Epoch 217, Loss: 1.1767777502536774, Final Batch Loss: 0.413741797208786\n",
      "Epoch 218, Loss: 1.1050347983837128, Final Batch Loss: 0.31168133020401\n",
      "Epoch 219, Loss: 1.2113055884838104, Final Batch Loss: 0.3798247277736664\n",
      "Epoch 220, Loss: 1.1187686920166016, Final Batch Loss: 0.3100697100162506\n",
      "Epoch 221, Loss: 1.126186192035675, Final Batch Loss: 0.39044180512428284\n",
      "Epoch 222, Loss: 1.1744053661823273, Final Batch Loss: 0.4049559235572815\n",
      "Epoch 223, Loss: 1.018715426325798, Final Batch Loss: 0.21335549652576447\n",
      "Epoch 224, Loss: 1.3485287129878998, Final Batch Loss: 0.5900107622146606\n",
      "Epoch 225, Loss: 1.2453445196151733, Final Batch Loss: 0.4922667145729065\n",
      "Epoch 226, Loss: 1.1779942214488983, Final Batch Loss: 0.41756176948547363\n",
      "Epoch 227, Loss: 1.1521095633506775, Final Batch Loss: 0.41034388542175293\n",
      "Epoch 228, Loss: 1.271944522857666, Final Batch Loss: 0.46466976404190063\n",
      "Epoch 229, Loss: 1.1747559010982513, Final Batch Loss: 0.37508314847946167\n",
      "Epoch 230, Loss: 1.1521027982234955, Final Batch Loss: 0.347405344247818\n",
      "Epoch 231, Loss: 1.1566393375396729, Final Batch Loss: 0.36912813782691956\n",
      "Epoch 232, Loss: 1.0355899631977081, Final Batch Loss: 0.18605974316596985\n",
      "Epoch 233, Loss: 1.0661642253398895, Final Batch Loss: 0.25936028361320496\n",
      "Epoch 234, Loss: 1.2047699093818665, Final Batch Loss: 0.3791727125644684\n",
      "Epoch 235, Loss: 1.175373911857605, Final Batch Loss: 0.41761016845703125\n",
      "Epoch 236, Loss: 1.1194636821746826, Final Batch Loss: 0.31831321120262146\n",
      "Epoch 237, Loss: 1.0755775272846222, Final Batch Loss: 0.3280940353870392\n",
      "Epoch 238, Loss: 1.1812217235565186, Final Batch Loss: 0.3779415786266327\n",
      "Epoch 239, Loss: 1.150797814130783, Final Batch Loss: 0.41454988718032837\n",
      "Epoch 240, Loss: 1.053210824728012, Final Batch Loss: 0.310382217168808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241, Loss: 1.11497163772583, Final Batch Loss: 0.3649607300758362\n",
      "Epoch 242, Loss: 1.2348156571388245, Final Batch Loss: 0.5337626934051514\n",
      "Epoch 243, Loss: 1.2753673493862152, Final Batch Loss: 0.5441527366638184\n",
      "Epoch 244, Loss: 1.2113663256168365, Final Batch Loss: 0.46116387844085693\n",
      "Epoch 245, Loss: 1.039446622133255, Final Batch Loss: 0.29069384932518005\n",
      "Epoch 246, Loss: 1.2323348820209503, Final Batch Loss: 0.44137775897979736\n",
      "Epoch 247, Loss: 1.1655654311180115, Final Batch Loss: 0.39712658524513245\n",
      "Epoch 248, Loss: 1.0260225534439087, Final Batch Loss: 0.24452295899391174\n",
      "Epoch 249, Loss: 1.2543549239635468, Final Batch Loss: 0.38253891468048096\n",
      "Epoch 250, Loss: 1.0910275280475616, Final Batch Loss: 0.38632938265800476\n",
      "Epoch 251, Loss: 1.0906485319137573, Final Batch Loss: 0.30692198872566223\n",
      "Epoch 252, Loss: 1.133418470621109, Final Batch Loss: 0.4239406883716583\n",
      "Epoch 253, Loss: 1.1282213926315308, Final Batch Loss: 0.3646372854709625\n",
      "Epoch 254, Loss: 1.2577354311943054, Final Batch Loss: 0.5500665903091431\n",
      "Epoch 255, Loss: 1.123578429222107, Final Batch Loss: 0.4375550448894501\n",
      "Epoch 256, Loss: 1.1291804313659668, Final Batch Loss: 0.405486136674881\n",
      "Epoch 257, Loss: 1.118491291999817, Final Batch Loss: 0.3787335455417633\n",
      "Epoch 258, Loss: 0.9833920300006866, Final Batch Loss: 0.19860291481018066\n",
      "Epoch 259, Loss: 1.0663664042949677, Final Batch Loss: 0.36016303300857544\n",
      "Epoch 260, Loss: 1.0228630304336548, Final Batch Loss: 0.2538495659828186\n",
      "Epoch 261, Loss: 0.9486368894577026, Final Batch Loss: 0.20151785016059875\n",
      "Epoch 262, Loss: 1.1900665760040283, Final Batch Loss: 0.4362736642360687\n",
      "Epoch 263, Loss: 1.0314043760299683, Final Batch Loss: 0.28333669900894165\n",
      "Epoch 264, Loss: 1.0994007587432861, Final Batch Loss: 0.40862372517585754\n",
      "Epoch 265, Loss: 1.0274601876735687, Final Batch Loss: 0.2116025686264038\n",
      "Epoch 266, Loss: 1.0126871168613434, Final Batch Loss: 0.28043293952941895\n",
      "Epoch 267, Loss: 0.9611681401729584, Final Batch Loss: 0.18073439598083496\n",
      "Epoch 268, Loss: 0.9432360529899597, Final Batch Loss: 0.1902487874031067\n",
      "Epoch 269, Loss: 1.0973994135856628, Final Batch Loss: 0.35928237438201904\n",
      "Epoch 270, Loss: 1.0369356870651245, Final Batch Loss: 0.29410266876220703\n",
      "Epoch 271, Loss: 1.016104906797409, Final Batch Loss: 0.26411333680152893\n",
      "Epoch 272, Loss: 0.9977487027645111, Final Batch Loss: 0.23644694685935974\n",
      "Epoch 273, Loss: 1.2205674350261688, Final Batch Loss: 0.4112747013568878\n",
      "Epoch 274, Loss: 1.0487800538539886, Final Batch Loss: 0.2903103828430176\n",
      "Epoch 275, Loss: 1.116555392742157, Final Batch Loss: 0.39534538984298706\n",
      "Epoch 276, Loss: 1.227716475725174, Final Batch Loss: 0.537876546382904\n",
      "Epoch 277, Loss: 1.199397325515747, Final Batch Loss: 0.476950466632843\n",
      "Epoch 278, Loss: 1.0755977630615234, Final Batch Loss: 0.3465481996536255\n",
      "Epoch 279, Loss: 1.0837434828281403, Final Batch Loss: 0.3614886403083801\n",
      "Epoch 280, Loss: 1.1861795783042908, Final Batch Loss: 0.41867685317993164\n",
      "Epoch 281, Loss: 1.002871721982956, Final Batch Loss: 0.3140016794204712\n",
      "Epoch 282, Loss: 1.1373707950115204, Final Batch Loss: 0.4402948021888733\n",
      "Epoch 283, Loss: 0.9297482967376709, Final Batch Loss: 0.25005197525024414\n",
      "Epoch 284, Loss: 1.0160738825798035, Final Batch Loss: 0.29517853260040283\n",
      "Epoch 285, Loss: 0.9740328192710876, Final Batch Loss: 0.3188149333000183\n",
      "Epoch 286, Loss: 0.9453617632389069, Final Batch Loss: 0.29656654596328735\n",
      "Epoch 287, Loss: 0.9551699012517929, Final Batch Loss: 0.2388431578874588\n",
      "Epoch 288, Loss: 1.0423880219459534, Final Batch Loss: 0.36202552914619446\n",
      "Epoch 289, Loss: 0.926491379737854, Final Batch Loss: 0.25380563735961914\n",
      "Epoch 290, Loss: 1.0661301910877228, Final Batch Loss: 0.39506006240844727\n",
      "Epoch 291, Loss: 1.1097815036773682, Final Batch Loss: 0.39729514718055725\n",
      "Epoch 292, Loss: 1.036482334136963, Final Batch Loss: 0.3603881001472473\n",
      "Epoch 293, Loss: 1.0145752131938934, Final Batch Loss: 0.34160131216049194\n",
      "Epoch 294, Loss: 0.9738013446331024, Final Batch Loss: 0.2763408422470093\n",
      "Epoch 295, Loss: 1.0420667827129364, Final Batch Loss: 0.3592665493488312\n",
      "Epoch 296, Loss: 0.9328728318214417, Final Batch Loss: 0.25136253237724304\n",
      "Epoch 297, Loss: 1.026580274105072, Final Batch Loss: 0.33981162309646606\n",
      "Epoch 298, Loss: 1.0422331094741821, Final Batch Loss: 0.32277369499206543\n",
      "Epoch 299, Loss: 1.1207798421382904, Final Batch Loss: 0.42967820167541504\n",
      "Epoch 300, Loss: 1.0627801716327667, Final Batch Loss: 0.3514193594455719\n",
      "Epoch 301, Loss: 1.1578547656536102, Final Batch Loss: 0.4284905791282654\n",
      "Epoch 302, Loss: 1.080306589603424, Final Batch Loss: 0.4256066083908081\n",
      "Epoch 303, Loss: 1.0734409987926483, Final Batch Loss: 0.41763702034950256\n",
      "Epoch 304, Loss: 1.138109266757965, Final Batch Loss: 0.5223607420921326\n",
      "Epoch 305, Loss: 1.1178773939609528, Final Batch Loss: 0.45897993445396423\n",
      "Epoch 306, Loss: 1.0704677402973175, Final Batch Loss: 0.40931883454322815\n",
      "Epoch 307, Loss: 0.9360851049423218, Final Batch Loss: 0.27491483092308044\n",
      "Epoch 308, Loss: 1.0349481999874115, Final Batch Loss: 0.31733834743499756\n",
      "Epoch 309, Loss: 0.9648242592811584, Final Batch Loss: 0.26459023356437683\n",
      "Epoch 310, Loss: 1.0060790181159973, Final Batch Loss: 0.299296498298645\n",
      "Epoch 311, Loss: 0.9230614602565765, Final Batch Loss: 0.2993956208229065\n",
      "Epoch 312, Loss: 0.8240382820367813, Final Batch Loss: 0.12270893156528473\n",
      "Epoch 313, Loss: 0.9804290533065796, Final Batch Loss: 0.3334342837333679\n",
      "Epoch 314, Loss: 1.0733061730861664, Final Batch Loss: 0.4662396013736725\n",
      "Epoch 315, Loss: 0.9275461435317993, Final Batch Loss: 0.2899658977985382\n",
      "Epoch 316, Loss: 0.9481091201305389, Final Batch Loss: 0.27760398387908936\n",
      "Epoch 317, Loss: 1.0062742829322815, Final Batch Loss: 0.3479233682155609\n",
      "Epoch 318, Loss: 1.0659695267677307, Final Batch Loss: 0.42134714126586914\n",
      "Epoch 319, Loss: 0.9754469394683838, Final Batch Loss: 0.3277263641357422\n",
      "Epoch 320, Loss: 0.9392392635345459, Final Batch Loss: 0.26092493534088135\n",
      "Epoch 321, Loss: 0.9429467916488647, Final Batch Loss: 0.264413058757782\n",
      "Epoch 322, Loss: 0.9141847789287567, Final Batch Loss: 0.2658703327178955\n",
      "Epoch 323, Loss: 0.8730131089687347, Final Batch Loss: 0.260245144367218\n",
      "Epoch 324, Loss: 1.0834775865077972, Final Batch Loss: 0.4470231533050537\n",
      "Epoch 325, Loss: 1.0242527425289154, Final Batch Loss: 0.43050283193588257\n",
      "Epoch 326, Loss: 1.0101132690906525, Final Batch Loss: 0.3367169201374054\n",
      "Epoch 327, Loss: 0.9681822955608368, Final Batch Loss: 0.268693208694458\n",
      "Epoch 328, Loss: 0.9677205979824066, Final Batch Loss: 0.3123250901699066\n",
      "Epoch 329, Loss: 1.082281917333603, Final Batch Loss: 0.46197858452796936\n",
      "Epoch 330, Loss: 0.995306134223938, Final Batch Loss: 0.36592817306518555\n",
      "Epoch 331, Loss: 0.9614118337631226, Final Batch Loss: 0.2682259678840637\n",
      "Epoch 332, Loss: 1.0251555144786835, Final Batch Loss: 0.41737979650497437\n",
      "Epoch 333, Loss: 0.9142083823680878, Final Batch Loss: 0.34310317039489746\n",
      "Epoch 334, Loss: 0.9958840608596802, Final Batch Loss: 0.22898733615875244\n",
      "Epoch 335, Loss: 0.9069557785987854, Final Batch Loss: 0.2265506386756897\n",
      "Epoch 336, Loss: 0.9685500264167786, Final Batch Loss: 0.3763941526412964\n",
      "Epoch 337, Loss: 0.9344439208507538, Final Batch Loss: 0.2762511968612671\n",
      "Epoch 338, Loss: 0.9239010810852051, Final Batch Loss: 0.321546733379364\n",
      "Epoch 339, Loss: 0.9687754809856415, Final Batch Loss: 0.33575353026390076\n",
      "Epoch 340, Loss: 0.970269501209259, Final Batch Loss: 0.3763459026813507\n",
      "Epoch 341, Loss: 1.062758356332779, Final Batch Loss: 0.42162656784057617\n",
      "Epoch 342, Loss: 0.963150829076767, Final Batch Loss: 0.3370615541934967\n",
      "Epoch 343, Loss: 0.8986169993877411, Final Batch Loss: 0.2400982677936554\n",
      "Epoch 344, Loss: 0.907774806022644, Final Batch Loss: 0.318574994802475\n",
      "Epoch 345, Loss: 0.7488275319337845, Final Batch Loss: 0.13901282846927643\n",
      "Epoch 346, Loss: 1.0563459694385529, Final Batch Loss: 0.4088478982448578\n",
      "Epoch 347, Loss: 0.9262563586235046, Final Batch Loss: 0.3262581527233124\n",
      "Epoch 348, Loss: 0.995564877986908, Final Batch Loss: 0.327095627784729\n",
      "Epoch 349, Loss: 0.8566729128360748, Final Batch Loss: 0.2449415922164917\n",
      "Epoch 350, Loss: 1.0158580243587494, Final Batch Loss: 0.3975594937801361\n",
      "Epoch 351, Loss: 0.9640325903892517, Final Batch Loss: 0.3089732825756073\n",
      "Epoch 352, Loss: 1.0240417122840881, Final Batch Loss: 0.4257432520389557\n",
      "Epoch 353, Loss: 0.8329922705888748, Final Batch Loss: 0.22444231808185577\n",
      "Epoch 354, Loss: 0.8115149587392807, Final Batch Loss: 0.22366206347942352\n",
      "Epoch 355, Loss: 0.9977177679538727, Final Batch Loss: 0.3660144507884979\n",
      "Epoch 356, Loss: 0.9722408652305603, Final Batch Loss: 0.40490400791168213\n",
      "Epoch 357, Loss: 0.9313337802886963, Final Batch Loss: 0.3374943137168884\n",
      "Epoch 358, Loss: 0.867218941450119, Final Batch Loss: 0.26657921075820923\n",
      "Epoch 359, Loss: 0.8212069571018219, Final Batch Loss: 0.253978431224823\n",
      "Epoch 360, Loss: 1.00371852517128, Final Batch Loss: 0.42808374762535095\n",
      "Epoch 361, Loss: 0.9166765213012695, Final Batch Loss: 0.3127295970916748\n",
      "Epoch 362, Loss: 0.9487856030464172, Final Batch Loss: 0.38035520911216736\n",
      "Epoch 363, Loss: 0.7492175847291946, Final Batch Loss: 0.13772137463092804\n",
      "Epoch 364, Loss: 0.8229118138551712, Final Batch Loss: 0.22926677763462067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 365, Loss: 0.8514433652162552, Final Batch Loss: 0.21984802186489105\n",
      "Epoch 366, Loss: 0.8303072154521942, Final Batch Loss: 0.2666773796081543\n",
      "Epoch 367, Loss: 0.7549078315496445, Final Batch Loss: 0.1429011970758438\n",
      "Epoch 368, Loss: 0.8618405759334564, Final Batch Loss: 0.25233104825019836\n",
      "Epoch 369, Loss: 0.802330881357193, Final Batch Loss: 0.2377701699733734\n",
      "Epoch 370, Loss: 0.9972930550575256, Final Batch Loss: 0.40086960792541504\n",
      "Epoch 371, Loss: 0.9489540159702301, Final Batch Loss: 0.4053913652896881\n",
      "Epoch 372, Loss: 0.925661563873291, Final Batch Loss: 0.34764406085014343\n",
      "Epoch 373, Loss: 0.947147786617279, Final Batch Loss: 0.35562843084335327\n",
      "Epoch 374, Loss: 0.835265189409256, Final Batch Loss: 0.23004886507987976\n",
      "Epoch 375, Loss: 1.0345060229301453, Final Batch Loss: 0.4672456383705139\n",
      "Epoch 376, Loss: 0.9603141695261002, Final Batch Loss: 0.38984590768814087\n",
      "Epoch 377, Loss: 0.865890383720398, Final Batch Loss: 0.28136464953422546\n",
      "Epoch 378, Loss: 0.9294881224632263, Final Batch Loss: 0.3706601560115814\n",
      "Epoch 379, Loss: 0.7788836508989334, Final Batch Loss: 0.21988914906978607\n",
      "Epoch 380, Loss: 0.9918846487998962, Final Batch Loss: 0.4243978261947632\n",
      "Epoch 381, Loss: 0.8775245249271393, Final Batch Loss: 0.28816723823547363\n",
      "Epoch 382, Loss: 0.7621573060750961, Final Batch Loss: 0.1901550143957138\n",
      "Epoch 383, Loss: 0.836839109659195, Final Batch Loss: 0.23879960179328918\n",
      "Epoch 384, Loss: 0.8991060554981232, Final Batch Loss: 0.32658419013023376\n",
      "Epoch 385, Loss: 0.9934225082397461, Final Batch Loss: 0.37035825848579407\n",
      "Epoch 386, Loss: 0.8228543400764465, Final Batch Loss: 0.28579625487327576\n",
      "Epoch 387, Loss: 0.934044748544693, Final Batch Loss: 0.3233084976673126\n",
      "Epoch 388, Loss: 0.7898043990135193, Final Batch Loss: 0.21889258921146393\n",
      "Epoch 389, Loss: 0.8576059639453888, Final Batch Loss: 0.25548166036605835\n",
      "Epoch 390, Loss: 0.936815470457077, Final Batch Loss: 0.402709037065506\n",
      "Epoch 391, Loss: 0.8601915240287781, Final Batch Loss: 0.32263830304145813\n",
      "Epoch 392, Loss: 0.901788517832756, Final Batch Loss: 0.38712483644485474\n",
      "Epoch 393, Loss: 0.9301843643188477, Final Batch Loss: 0.35218140482902527\n",
      "Epoch 394, Loss: 1.0338312685489655, Final Batch Loss: 0.41286954283714294\n",
      "Epoch 395, Loss: 0.7640399485826492, Final Batch Loss: 0.1534724086523056\n",
      "Epoch 396, Loss: 1.015572965145111, Final Batch Loss: 0.47344592213630676\n",
      "Epoch 397, Loss: 0.8320316225290298, Final Batch Loss: 0.20484976470470428\n",
      "Epoch 398, Loss: 1.026243656873703, Final Batch Loss: 0.4291699230670929\n",
      "Epoch 399, Loss: 0.8890527784824371, Final Batch Loss: 0.26768696308135986\n",
      "Epoch 400, Loss: 0.7719662934541702, Final Batch Loss: 0.1471986323595047\n",
      "Epoch 401, Loss: 0.8688273429870605, Final Batch Loss: 0.28386640548706055\n",
      "Epoch 402, Loss: 0.9295267164707184, Final Batch Loss: 0.4118300676345825\n",
      "Epoch 403, Loss: 0.8525658845901489, Final Batch Loss: 0.2922253906726837\n",
      "Epoch 404, Loss: 0.8596979677677155, Final Batch Loss: 0.2957223355770111\n",
      "Epoch 405, Loss: 0.815174326300621, Final Batch Loss: 0.22717685997486115\n",
      "Epoch 406, Loss: 0.8102070987224579, Final Batch Loss: 0.2182970941066742\n",
      "Epoch 407, Loss: 0.8694949746131897, Final Batch Loss: 0.3042192757129669\n",
      "Epoch 408, Loss: 0.8351688981056213, Final Batch Loss: 0.2833438515663147\n",
      "Epoch 409, Loss: 0.7789549678564072, Final Batch Loss: 0.22077633440494537\n",
      "Epoch 410, Loss: 0.8813584744930267, Final Batch Loss: 0.3068534731864929\n",
      "Epoch 411, Loss: 0.9335138499736786, Final Batch Loss: 0.3922384977340698\n",
      "Epoch 412, Loss: 0.8536321222782135, Final Batch Loss: 0.32311075925827026\n",
      "Epoch 413, Loss: 0.9342606663703918, Final Batch Loss: 0.3670836091041565\n",
      "Epoch 414, Loss: 0.8771184384822845, Final Batch Loss: 0.29890894889831543\n",
      "Epoch 415, Loss: 0.73088438808918, Final Batch Loss: 0.2277403026819229\n",
      "Epoch 416, Loss: 0.898242324590683, Final Batch Loss: 0.31117865443229675\n",
      "Epoch 417, Loss: 0.8721785992383957, Final Batch Loss: 0.29244109988212585\n",
      "Epoch 418, Loss: 0.958543211221695, Final Batch Loss: 0.44273215532302856\n",
      "Epoch 419, Loss: 0.8676003217697144, Final Batch Loss: 0.31893405318260193\n",
      "Epoch 420, Loss: 0.7609154284000397, Final Batch Loss: 0.24105067551136017\n",
      "Epoch 421, Loss: 0.9423255622386932, Final Batch Loss: 0.4339425265789032\n",
      "Epoch 422, Loss: 0.7041109651327133, Final Batch Loss: 0.16344083845615387\n",
      "Epoch 423, Loss: 0.8268229067325592, Final Batch Loss: 0.28584226965904236\n",
      "Epoch 424, Loss: 0.7617323100566864, Final Batch Loss: 0.21394383907318115\n",
      "Epoch 425, Loss: 0.8297993689775467, Final Batch Loss: 0.3123507797718048\n",
      "Epoch 426, Loss: 0.721207469701767, Final Batch Loss: 0.24121105670928955\n",
      "Epoch 427, Loss: 0.8356878459453583, Final Batch Loss: 0.25233593583106995\n",
      "Epoch 428, Loss: 0.9750123172998428, Final Batch Loss: 0.4618396759033203\n",
      "Epoch 429, Loss: 0.8106762766838074, Final Batch Loss: 0.2561998665332794\n",
      "Epoch 430, Loss: 0.8669080287218094, Final Batch Loss: 0.3968425691127777\n",
      "Epoch 431, Loss: 0.7857531756162643, Final Batch Loss: 0.19761936366558075\n",
      "Epoch 432, Loss: 0.75412617623806, Final Batch Loss: 0.22611598670482635\n",
      "Epoch 433, Loss: 0.7760912925004959, Final Batch Loss: 0.2641218900680542\n",
      "Epoch 434, Loss: 0.818458080291748, Final Batch Loss: 0.2539481520652771\n",
      "Epoch 435, Loss: 0.8648959100246429, Final Batch Loss: 0.3104698061943054\n",
      "Epoch 436, Loss: 0.9038232564926147, Final Batch Loss: 0.3247848451137543\n",
      "Epoch 437, Loss: 0.759691059589386, Final Batch Loss: 0.18913021683692932\n",
      "Epoch 438, Loss: 0.8380576968193054, Final Batch Loss: 0.2571249306201935\n",
      "Epoch 439, Loss: 0.8193553686141968, Final Batch Loss: 0.30446574091911316\n",
      "Epoch 440, Loss: 0.7291405946016312, Final Batch Loss: 0.1890942007303238\n",
      "Epoch 441, Loss: 0.751191258430481, Final Batch Loss: 0.2738637924194336\n",
      "Epoch 442, Loss: 0.8152882158756256, Final Batch Loss: 0.24277937412261963\n",
      "Epoch 443, Loss: 0.7771495878696442, Final Batch Loss: 0.23941436409950256\n",
      "Epoch 444, Loss: 0.8208159953355789, Final Batch Loss: 0.22599224746227264\n",
      "Epoch 445, Loss: 0.7806178033351898, Final Batch Loss: 0.27280211448669434\n",
      "Epoch 446, Loss: 0.8947426080703735, Final Batch Loss: 0.3703661561012268\n",
      "Epoch 447, Loss: 0.7771810293197632, Final Batch Loss: 0.2341006100177765\n",
      "Epoch 448, Loss: 0.8201125860214233, Final Batch Loss: 0.28661057353019714\n",
      "Epoch 449, Loss: 0.7564317137002945, Final Batch Loss: 0.27530959248542786\n",
      "Epoch 450, Loss: 0.7317793965339661, Final Batch Loss: 0.22046704590320587\n",
      "Epoch 451, Loss: 0.9114949405193329, Final Batch Loss: 0.35000869631767273\n",
      "Epoch 452, Loss: 0.739270955324173, Final Batch Loss: 0.2158936858177185\n",
      "Epoch 453, Loss: 0.8909016847610474, Final Batch Loss: 0.382488876581192\n",
      "Epoch 454, Loss: 0.8429481983184814, Final Batch Loss: 0.2921365201473236\n",
      "Epoch 455, Loss: 0.7862096428871155, Final Batch Loss: 0.27245965600013733\n",
      "Epoch 456, Loss: 0.8194540590047836, Final Batch Loss: 0.23183639347553253\n",
      "Epoch 457, Loss: 0.8331569135189056, Final Batch Loss: 0.2494882494211197\n",
      "Epoch 458, Loss: 0.7752307802438736, Final Batch Loss: 0.2598468065261841\n",
      "Epoch 459, Loss: 0.6814929246902466, Final Batch Loss: 0.16111616790294647\n",
      "Epoch 460, Loss: 0.7291231006383896, Final Batch Loss: 0.193686842918396\n",
      "Epoch 461, Loss: 0.7342306524515152, Final Batch Loss: 0.27805331349372864\n",
      "Epoch 462, Loss: 0.8293081372976303, Final Batch Loss: 0.3380013406276703\n",
      "Epoch 463, Loss: 0.7066137790679932, Final Batch Loss: 0.21018454432487488\n",
      "Epoch 464, Loss: 0.8565823882818222, Final Batch Loss: 0.34553104639053345\n",
      "Epoch 465, Loss: 0.7771872580051422, Final Batch Loss: 0.2611611783504486\n",
      "Epoch 466, Loss: 0.7001805603504181, Final Batch Loss: 0.2060704082250595\n",
      "Epoch 467, Loss: 0.7006643563508987, Final Batch Loss: 0.22345991432666779\n",
      "Epoch 468, Loss: 0.7182932496070862, Final Batch Loss: 0.2078368216753006\n",
      "Epoch 469, Loss: 0.8564321100711823, Final Batch Loss: 0.3607965111732483\n",
      "Epoch 470, Loss: 0.7836490273475647, Final Batch Loss: 0.20918428897857666\n",
      "Epoch 471, Loss: 0.6707116216421127, Final Batch Loss: 0.12225918471813202\n",
      "Epoch 472, Loss: 0.8275431543588638, Final Batch Loss: 0.22678185999393463\n",
      "Epoch 473, Loss: 0.8123680651187897, Final Batch Loss: 0.3301123380661011\n",
      "Epoch 474, Loss: 0.6288889646530151, Final Batch Loss: 0.08755424618721008\n",
      "Epoch 475, Loss: 0.7576838433742523, Final Batch Loss: 0.24383851885795593\n",
      "Epoch 476, Loss: 0.8612069934606552, Final Batch Loss: 0.39315009117126465\n",
      "Epoch 477, Loss: 0.7308579832315445, Final Batch Loss: 0.22787770628929138\n",
      "Epoch 478, Loss: 0.7037735879421234, Final Batch Loss: 0.18289630115032196\n",
      "Epoch 479, Loss: 0.6989494264125824, Final Batch Loss: 0.20850855112075806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480, Loss: 0.7088683992624283, Final Batch Loss: 0.191515251994133\n",
      "Epoch 481, Loss: 0.7120664119720459, Final Batch Loss: 0.21514452993869781\n",
      "Epoch 482, Loss: 0.8442355692386627, Final Batch Loss: 0.32875528931617737\n",
      "Epoch 483, Loss: 0.8241088092327118, Final Batch Loss: 0.3522135317325592\n",
      "Epoch 484, Loss: 0.820291057229042, Final Batch Loss: 0.3565729260444641\n",
      "Epoch 485, Loss: 0.7700406312942505, Final Batch Loss: 0.3332785367965698\n",
      "Epoch 486, Loss: 0.7733951956033707, Final Batch Loss: 0.3072960376739502\n",
      "Epoch 487, Loss: 0.6477548032999039, Final Batch Loss: 0.11350031197071075\n",
      "Epoch 488, Loss: 0.7177956402301788, Final Batch Loss: 0.26562196016311646\n",
      "Epoch 489, Loss: 0.7417635172605515, Final Batch Loss: 0.2656919062137604\n",
      "Epoch 490, Loss: 0.8355164974927902, Final Batch Loss: 0.34739524126052856\n",
      "Epoch 491, Loss: 0.6871214359998703, Final Batch Loss: 0.16557875275611877\n",
      "Epoch 492, Loss: 0.7260439842939377, Final Batch Loss: 0.2796822190284729\n",
      "Epoch 493, Loss: 0.8239239305257797, Final Batch Loss: 0.3501375615596771\n",
      "Epoch 494, Loss: 0.8844583332538605, Final Batch Loss: 0.37017810344696045\n",
      "Epoch 495, Loss: 0.7884299755096436, Final Batch Loss: 0.2652994394302368\n",
      "Epoch 496, Loss: 0.7804322242736816, Final Batch Loss: 0.3072894513607025\n",
      "Epoch 497, Loss: 0.6530726477503777, Final Batch Loss: 0.11879245191812515\n",
      "Epoch 498, Loss: 0.7065128684043884, Final Batch Loss: 0.23026268184185028\n",
      "Epoch 499, Loss: 0.8218685239553452, Final Batch Loss: 0.32670268416404724\n",
      "Epoch 500, Loss: 0.819066122174263, Final Batch Loss: 0.33623528480529785\n",
      "Epoch 501, Loss: 0.8752011358737946, Final Batch Loss: 0.36702069640159607\n",
      "Epoch 502, Loss: 0.7578513324260712, Final Batch Loss: 0.21883341670036316\n",
      "Epoch 503, Loss: 0.6637283712625504, Final Batch Loss: 0.16238239407539368\n",
      "Epoch 504, Loss: 0.7363074719905853, Final Batch Loss: 0.24403181672096252\n",
      "Epoch 505, Loss: 0.6334036290645599, Final Batch Loss: 0.14203347265720367\n",
      "Epoch 506, Loss: 0.7157331705093384, Final Batch Loss: 0.19882383942604065\n",
      "Epoch 507, Loss: 0.6790146976709366, Final Batch Loss: 0.1633114367723465\n",
      "Epoch 508, Loss: 0.6713720709085464, Final Batch Loss: 0.1411721557378769\n",
      "Epoch 509, Loss: 0.7783080786466599, Final Batch Loss: 0.3220139443874359\n",
      "Epoch 510, Loss: 0.6795224696397781, Final Batch Loss: 0.19793401658535004\n",
      "Epoch 511, Loss: 0.6947893351316452, Final Batch Loss: 0.18485763669013977\n",
      "Epoch 512, Loss: 0.6784223020076752, Final Batch Loss: 0.1822032481431961\n",
      "Epoch 513, Loss: 0.6995840519666672, Final Batch Loss: 0.20009209215641022\n",
      "Epoch 514, Loss: 0.7508808523416519, Final Batch Loss: 0.276981383562088\n",
      "Epoch 515, Loss: 0.8154177665710449, Final Batch Loss: 0.2912115156650543\n",
      "Epoch 516, Loss: 0.8456124365329742, Final Batch Loss: 0.3819196820259094\n",
      "Epoch 517, Loss: 0.769538164138794, Final Batch Loss: 0.2900308668613434\n",
      "Epoch 518, Loss: 0.6709142029285431, Final Batch Loss: 0.1904497742652893\n",
      "Epoch 519, Loss: 0.7402724623680115, Final Batch Loss: 0.2748342454433441\n",
      "Epoch 520, Loss: 0.709530308842659, Final Batch Loss: 0.21511466801166534\n",
      "Epoch 521, Loss: 0.6695422977209091, Final Batch Loss: 0.2030360847711563\n",
      "Epoch 522, Loss: 0.6470070779323578, Final Batch Loss: 0.2318209409713745\n",
      "Epoch 523, Loss: 0.6279596835374832, Final Batch Loss: 0.19943250715732574\n",
      "Epoch 524, Loss: 0.6963194012641907, Final Batch Loss: 0.18594394624233246\n",
      "Epoch 525, Loss: 0.715143546462059, Final Batch Loss: 0.2965356409549713\n",
      "Epoch 526, Loss: 0.6130736097693443, Final Batch Loss: 0.11046727746725082\n",
      "Epoch 527, Loss: 0.8260586857795715, Final Batch Loss: 0.38005775213241577\n",
      "Epoch 528, Loss: 0.6829865574836731, Final Batch Loss: 0.1728004813194275\n",
      "Epoch 529, Loss: 0.6819805055856705, Final Batch Loss: 0.2233302891254425\n",
      "Epoch 530, Loss: 0.7475901544094086, Final Batch Loss: 0.26148730516433716\n",
      "Epoch 531, Loss: 0.6909336596727371, Final Batch Loss: 0.2101733237504959\n",
      "Epoch 532, Loss: 0.6326308399438858, Final Batch Loss: 0.16177856922149658\n",
      "Epoch 533, Loss: 0.6742354184389114, Final Batch Loss: 0.2041175365447998\n",
      "Epoch 534, Loss: 0.6704303324222565, Final Batch Loss: 0.2722052037715912\n",
      "Epoch 535, Loss: 0.7321318089962006, Final Batch Loss: 0.2782427966594696\n",
      "Epoch 536, Loss: 0.7276410460472107, Final Batch Loss: 0.23870578408241272\n",
      "Epoch 537, Loss: 0.6420935094356537, Final Batch Loss: 0.19809941947460175\n",
      "Epoch 538, Loss: 0.7487000524997711, Final Batch Loss: 0.2290726900100708\n",
      "Epoch 539, Loss: 0.6984261125326157, Final Batch Loss: 0.22945205867290497\n",
      "Epoch 540, Loss: 0.6557343155145645, Final Batch Loss: 0.17489278316497803\n",
      "Epoch 541, Loss: 0.7725715786218643, Final Batch Loss: 0.24931244552135468\n",
      "Epoch 542, Loss: 0.6674345284700394, Final Batch Loss: 0.2604716122150421\n",
      "Epoch 543, Loss: 0.6000234931707382, Final Batch Loss: 0.12817123532295227\n",
      "Epoch 544, Loss: 0.6770072430372238, Final Batch Loss: 0.27163946628570557\n",
      "Epoch 545, Loss: 0.6673858463764191, Final Batch Loss: 0.21169611811637878\n",
      "Epoch 546, Loss: 0.7798654735088348, Final Batch Loss: 0.3046708106994629\n",
      "Epoch 547, Loss: 0.6895679086446762, Final Batch Loss: 0.28983521461486816\n",
      "Epoch 548, Loss: 0.6012440770864487, Final Batch Loss: 0.16851453483104706\n",
      "Epoch 549, Loss: 0.6473519653081894, Final Batch Loss: 0.20169952511787415\n",
      "Epoch 550, Loss: 0.641277626156807, Final Batch Loss: 0.19054025411605835\n",
      "Epoch 551, Loss: 0.6040018498897552, Final Batch Loss: 0.1995529681444168\n",
      "Epoch 552, Loss: 0.6447397917509079, Final Batch Loss: 0.16157467663288116\n",
      "Epoch 553, Loss: 0.7211823910474777, Final Batch Loss: 0.31187406182289124\n",
      "Epoch 554, Loss: 0.6872913837432861, Final Batch Loss: 0.2639273405075073\n",
      "Epoch 555, Loss: 0.6474033147096634, Final Batch Loss: 0.17249150574207306\n",
      "Epoch 556, Loss: 0.6441534757614136, Final Batch Loss: 0.2484344094991684\n",
      "Epoch 557, Loss: 0.6867594867944717, Final Batch Loss: 0.23110918700695038\n",
      "Epoch 558, Loss: 0.7124008983373642, Final Batch Loss: 0.2480747550725937\n",
      "Epoch 559, Loss: 0.7239419668912888, Final Batch Loss: 0.29442542791366577\n",
      "Epoch 560, Loss: 0.5662940740585327, Final Batch Loss: 0.15055321156978607\n",
      "Epoch 561, Loss: 0.6667793244123459, Final Batch Loss: 0.2468259036540985\n",
      "Epoch 562, Loss: 0.786468967795372, Final Batch Loss: 0.31990423798561096\n",
      "Epoch 563, Loss: 0.6546807587146759, Final Batch Loss: 0.2386673241853714\n",
      "Epoch 564, Loss: 0.516526035964489, Final Batch Loss: 0.1130618080496788\n",
      "Epoch 565, Loss: 0.738164484500885, Final Batch Loss: 0.2987108528614044\n",
      "Epoch 566, Loss: 0.6455263644456863, Final Batch Loss: 0.17514102160930634\n",
      "Epoch 567, Loss: 0.7538851946592331, Final Batch Loss: 0.27372628450393677\n",
      "Epoch 568, Loss: 0.6858861893415451, Final Batch Loss: 0.25856390595436096\n",
      "Epoch 569, Loss: 0.6895082145929337, Final Batch Loss: 0.2388731688261032\n",
      "Epoch 570, Loss: 0.5272255688905716, Final Batch Loss: 0.11512593924999237\n",
      "Epoch 571, Loss: 0.6409289985895157, Final Batch Loss: 0.17498113214969635\n",
      "Epoch 572, Loss: 0.6509717255830765, Final Batch Loss: 0.15137699246406555\n",
      "Epoch 573, Loss: 0.6710689514875412, Final Batch Loss: 0.2668398320674896\n",
      "Epoch 574, Loss: 0.8324175477027893, Final Batch Loss: 0.4178781807422638\n",
      "Epoch 575, Loss: 0.6068412736058235, Final Batch Loss: 0.09144353121519089\n",
      "Epoch 576, Loss: 0.6300867199897766, Final Batch Loss: 0.21893151104450226\n",
      "Epoch 577, Loss: 0.5596935898065567, Final Batch Loss: 0.13213728368282318\n",
      "Epoch 578, Loss: 0.6514467298984528, Final Batch Loss: 0.2536073625087738\n",
      "Epoch 579, Loss: 0.7346847057342529, Final Batch Loss: 0.26232782006263733\n",
      "Epoch 580, Loss: 0.564351812005043, Final Batch Loss: 0.14518287777900696\n",
      "Epoch 581, Loss: 0.6937237977981567, Final Batch Loss: 0.24711285531520844\n",
      "Epoch 582, Loss: 0.643589124083519, Final Batch Loss: 0.1872473955154419\n",
      "Epoch 583, Loss: 0.6415305435657501, Final Batch Loss: 0.24166376888751984\n",
      "Epoch 584, Loss: 0.6325906813144684, Final Batch Loss: 0.20025812089443207\n",
      "Epoch 585, Loss: 0.5746912062168121, Final Batch Loss: 0.1436890959739685\n",
      "Epoch 586, Loss: 0.6106688678264618, Final Batch Loss: 0.1623573750257492\n",
      "Epoch 587, Loss: 0.6493404656648636, Final Batch Loss: 0.28121551871299744\n",
      "Epoch 588, Loss: 0.698950782418251, Final Batch Loss: 0.29163858294487\n",
      "Epoch 589, Loss: 0.6888006180524826, Final Batch Loss: 0.2565288543701172\n",
      "Epoch 590, Loss: 0.8340467661619186, Final Batch Loss: 0.4348720908164978\n",
      "Epoch 591, Loss: 0.6968210637569427, Final Batch Loss: 0.28020161390304565\n",
      "Epoch 592, Loss: 0.6732082664966583, Final Batch Loss: 0.21062317490577698\n",
      "Epoch 593, Loss: 0.68036288022995, Final Batch Loss: 0.234334334731102\n",
      "Epoch 594, Loss: 0.5441609174013138, Final Batch Loss: 0.14624512195587158\n",
      "Epoch 595, Loss: 0.5257847309112549, Final Batch Loss: 0.10148826241493225\n",
      "Epoch 596, Loss: 0.6489051878452301, Final Batch Loss: 0.20238681137561798\n",
      "Epoch 597, Loss: 0.7822165638208389, Final Batch Loss: 0.3518214225769043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 598, Loss: 0.6135405153036118, Final Batch Loss: 0.17294299602508545\n",
      "Epoch 599, Loss: 0.6430089771747589, Final Batch Loss: 0.18926768004894257\n",
      "Epoch 600, Loss: 0.7669867277145386, Final Batch Loss: 0.3097980320453644\n",
      "Epoch 601, Loss: 0.7003531008958817, Final Batch Loss: 0.25706082582473755\n",
      "Epoch 602, Loss: 0.6271276623010635, Final Batch Loss: 0.20688433945178986\n",
      "Epoch 603, Loss: 0.565690390765667, Final Batch Loss: 0.11190872639417648\n",
      "Epoch 604, Loss: 0.6992286294698715, Final Batch Loss: 0.3252296447753906\n",
      "Epoch 605, Loss: 0.5436657518148422, Final Batch Loss: 0.15008661150932312\n",
      "Epoch 606, Loss: 0.6521046757698059, Final Batch Loss: 0.255662739276886\n",
      "Epoch 607, Loss: 0.7409503012895584, Final Batch Loss: 0.3249925673007965\n",
      "Epoch 608, Loss: 0.5748376399278641, Final Batch Loss: 0.1464090496301651\n",
      "Epoch 609, Loss: 0.6101416647434235, Final Batch Loss: 0.2073969691991806\n",
      "Epoch 610, Loss: 0.6166054457426071, Final Batch Loss: 0.20577487349510193\n",
      "Epoch 611, Loss: 0.581684023141861, Final Batch Loss: 0.16678868234157562\n",
      "Epoch 612, Loss: 0.6055966019630432, Final Batch Loss: 0.1824180781841278\n",
      "Epoch 613, Loss: 0.6197849363088608, Final Batch Loss: 0.1842770278453827\n",
      "Epoch 614, Loss: 0.6223068088293076, Final Batch Loss: 0.17648093402385712\n",
      "Epoch 615, Loss: 0.6252382546663284, Final Batch Loss: 0.20187941193580627\n",
      "Epoch 616, Loss: 0.6957542598247528, Final Batch Loss: 0.27806565165519714\n",
      "Epoch 617, Loss: 0.6120879650115967, Final Batch Loss: 0.2070193588733673\n",
      "Epoch 618, Loss: 0.6454600840806961, Final Batch Loss: 0.22961968183517456\n",
      "Epoch 619, Loss: 0.5560580492019653, Final Batch Loss: 0.18320250511169434\n",
      "Epoch 620, Loss: 0.5438138395547867, Final Batch Loss: 0.1858910620212555\n",
      "Epoch 621, Loss: 0.6400369852781296, Final Batch Loss: 0.24672384560108185\n",
      "Epoch 622, Loss: 0.5476209819316864, Final Batch Loss: 0.16556315124034882\n",
      "Epoch 623, Loss: 0.6509340107440948, Final Batch Loss: 0.2428482472896576\n",
      "Epoch 624, Loss: 0.650000125169754, Final Batch Loss: 0.26180750131607056\n",
      "Epoch 625, Loss: 0.46843845397233963, Final Batch Loss: 0.11265061050653458\n",
      "Epoch 626, Loss: 0.6239398717880249, Final Batch Loss: 0.25432300567626953\n",
      "Epoch 627, Loss: 0.6099079251289368, Final Batch Loss: 0.1831696778535843\n",
      "Epoch 628, Loss: 0.7306711673736572, Final Batch Loss: 0.33236926794052124\n",
      "Epoch 629, Loss: 0.5337745249271393, Final Batch Loss: 0.17973120510578156\n",
      "Epoch 630, Loss: 0.6071177423000336, Final Batch Loss: 0.17812471091747284\n",
      "Epoch 631, Loss: 0.641472265124321, Final Batch Loss: 0.24149058759212494\n",
      "Epoch 632, Loss: 0.6490631252527237, Final Batch Loss: 0.18932320177555084\n",
      "Epoch 633, Loss: 0.6393276453018188, Final Batch Loss: 0.21592821180820465\n",
      "Epoch 634, Loss: 0.6465691775083542, Final Batch Loss: 0.22688141465187073\n",
      "Epoch 635, Loss: 0.5460159033536911, Final Batch Loss: 0.1836080700159073\n",
      "Epoch 636, Loss: 0.5887110829353333, Final Batch Loss: 0.19650577008724213\n",
      "Epoch 637, Loss: 0.5580066591501236, Final Batch Loss: 0.12580741941928864\n",
      "Epoch 638, Loss: 0.6791952401399612, Final Batch Loss: 0.28229644894599915\n",
      "Epoch 639, Loss: 0.5162228122353554, Final Batch Loss: 0.104899100959301\n",
      "Epoch 640, Loss: 0.5361525937914848, Final Batch Loss: 0.10176148265600204\n",
      "Epoch 641, Loss: 0.5546361953020096, Final Batch Loss: 0.1853887140750885\n",
      "Epoch 642, Loss: 0.49997711181640625, Final Batch Loss: 0.0978049784898758\n",
      "Epoch 643, Loss: 0.5550262928009033, Final Batch Loss: 0.14342665672302246\n",
      "Epoch 644, Loss: 0.7287165224552155, Final Batch Loss: 0.24848763644695282\n",
      "Epoch 645, Loss: 0.5781108886003494, Final Batch Loss: 0.16845475137233734\n",
      "Epoch 646, Loss: 0.5733401998877525, Final Batch Loss: 0.10683939605951309\n",
      "Epoch 647, Loss: 0.6148430556058884, Final Batch Loss: 0.18161596357822418\n",
      "Epoch 648, Loss: 0.5005787834525108, Final Batch Loss: 0.12097427994012833\n",
      "Epoch 649, Loss: 0.6215981394052505, Final Batch Loss: 0.2097569853067398\n",
      "Epoch 650, Loss: 0.6289866268634796, Final Batch Loss: 0.20912189781665802\n",
      "Epoch 651, Loss: 0.6594686508178711, Final Batch Loss: 0.26653727889060974\n",
      "Epoch 652, Loss: 0.6573408544063568, Final Batch Loss: 0.25254642963409424\n",
      "Epoch 653, Loss: 0.6943637579679489, Final Batch Loss: 0.2815438508987427\n",
      "Epoch 654, Loss: 0.6686695963144302, Final Batch Loss: 0.23328275978565216\n",
      "Epoch 655, Loss: 0.5425706952810287, Final Batch Loss: 0.14992545545101166\n",
      "Epoch 656, Loss: 0.4779597222805023, Final Batch Loss: 0.09101617336273193\n",
      "Epoch 657, Loss: 0.5040392726659775, Final Batch Loss: 0.13571369647979736\n",
      "Epoch 658, Loss: 0.5616641193628311, Final Batch Loss: 0.19653372466564178\n",
      "Epoch 659, Loss: 0.5103392824530602, Final Batch Loss: 0.08392661064863205\n",
      "Epoch 660, Loss: 0.5336311906576157, Final Batch Loss: 0.12530633807182312\n",
      "Epoch 661, Loss: 0.6493009179830551, Final Batch Loss: 0.20912432670593262\n",
      "Epoch 662, Loss: 0.4994588792324066, Final Batch Loss: 0.12796756625175476\n",
      "Epoch 663, Loss: 0.5067957639694214, Final Batch Loss: 0.12956702709197998\n",
      "Epoch 664, Loss: 0.6013563722372055, Final Batch Loss: 0.27314746379852295\n",
      "Epoch 665, Loss: 0.5257880687713623, Final Batch Loss: 0.14531594514846802\n",
      "Epoch 666, Loss: 0.5305563136935234, Final Batch Loss: 0.12375905364751816\n",
      "Epoch 667, Loss: 0.5396189019083977, Final Batch Loss: 0.10869386047124863\n",
      "Epoch 668, Loss: 0.5752019435167313, Final Batch Loss: 0.21103130280971527\n",
      "Epoch 669, Loss: 0.7123330533504486, Final Batch Loss: 0.35336071252822876\n",
      "Epoch 670, Loss: 0.5533772557973862, Final Batch Loss: 0.1757887303829193\n",
      "Epoch 671, Loss: 0.4942110776901245, Final Batch Loss: 0.1094164103269577\n",
      "Epoch 672, Loss: 0.5819952934980392, Final Batch Loss: 0.19031430780887604\n",
      "Epoch 673, Loss: 0.5821041017770767, Final Batch Loss: 0.23288966715335846\n",
      "Epoch 674, Loss: 0.5869335532188416, Final Batch Loss: 0.20105814933776855\n",
      "Epoch 675, Loss: 0.6272750794887543, Final Batch Loss: 0.18424350023269653\n",
      "Epoch 676, Loss: 0.5459039807319641, Final Batch Loss: 0.156731978058815\n",
      "Epoch 677, Loss: 0.5737496465444565, Final Batch Loss: 0.19484877586364746\n",
      "Epoch 678, Loss: 0.6414443999528885, Final Batch Loss: 0.22230874001979828\n",
      "Epoch 679, Loss: 0.6121154427528381, Final Batch Loss: 0.2488071471452713\n",
      "Epoch 680, Loss: 0.5089510530233383, Final Batch Loss: 0.08278769254684448\n",
      "Epoch 681, Loss: 0.5842443704605103, Final Batch Loss: 0.22043433785438538\n",
      "Epoch 682, Loss: 0.5409920811653137, Final Batch Loss: 0.08403235673904419\n",
      "Epoch 683, Loss: 0.666756883263588, Final Batch Loss: 0.23691505193710327\n",
      "Epoch 684, Loss: 0.5367996096611023, Final Batch Loss: 0.19592206180095673\n",
      "Epoch 685, Loss: 0.6182409822940826, Final Batch Loss: 0.19845257699489594\n",
      "Epoch 686, Loss: 0.5541222095489502, Final Batch Loss: 0.17805993556976318\n",
      "Epoch 687, Loss: 0.6039995700120926, Final Batch Loss: 0.24055097997188568\n",
      "Epoch 688, Loss: 0.6220117211341858, Final Batch Loss: 0.30444565415382385\n",
      "Epoch 689, Loss: 0.5387313514947891, Final Batch Loss: 0.12963832914829254\n",
      "Epoch 690, Loss: 0.5429238677024841, Final Batch Loss: 0.14438797533512115\n",
      "Epoch 691, Loss: 0.6516157239675522, Final Batch Loss: 0.3059319853782654\n",
      "Epoch 692, Loss: 0.6706409752368927, Final Batch Loss: 0.2987823188304901\n",
      "Epoch 693, Loss: 0.4876009225845337, Final Batch Loss: 0.08659645915031433\n",
      "Epoch 694, Loss: 0.5922297537326813, Final Batch Loss: 0.24579913914203644\n",
      "Epoch 695, Loss: 0.6146349012851715, Final Batch Loss: 0.22375556826591492\n",
      "Epoch 696, Loss: 0.5391241163015366, Final Batch Loss: 0.18794064223766327\n",
      "Epoch 697, Loss: 0.49057047069072723, Final Batch Loss: 0.15267536044120789\n",
      "Epoch 698, Loss: 0.6275709718465805, Final Batch Loss: 0.2193206399679184\n",
      "Epoch 699, Loss: 0.5246400386095047, Final Batch Loss: 0.1635304093360901\n",
      "Epoch 700, Loss: 0.5964777320623398, Final Batch Loss: 0.19151340425014496\n",
      "Epoch 701, Loss: 0.6789573431015015, Final Batch Loss: 0.3249471187591553\n",
      "Epoch 702, Loss: 0.5254464149475098, Final Batch Loss: 0.15074557065963745\n",
      "Epoch 703, Loss: 0.5175569951534271, Final Batch Loss: 0.13434572517871857\n",
      "Epoch 704, Loss: 0.5252440869808197, Final Batch Loss: 0.17629237473011017\n",
      "Epoch 705, Loss: 0.6199270635843277, Final Batch Loss: 0.2331235706806183\n",
      "Epoch 706, Loss: 0.5573653429746628, Final Batch Loss: 0.21435505151748657\n",
      "Epoch 707, Loss: 0.5884522348642349, Final Batch Loss: 0.20136643946170807\n",
      "Epoch 708, Loss: 0.5001770630478859, Final Batch Loss: 0.10563502460718155\n",
      "Epoch 709, Loss: 0.7057381123304367, Final Batch Loss: 0.31576234102249146\n",
      "Epoch 710, Loss: 0.6536060720682144, Final Batch Loss: 0.27252018451690674\n",
      "Epoch 711, Loss: 0.5460516214370728, Final Batch Loss: 0.17119009792804718\n",
      "Epoch 712, Loss: 0.5254029631614685, Final Batch Loss: 0.17868314683437347\n",
      "Epoch 713, Loss: 0.5573141574859619, Final Batch Loss: 0.1253235787153244\n",
      "Epoch 714, Loss: 0.5283505544066429, Final Batch Loss: 0.11803198605775833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 715, Loss: 0.5628006458282471, Final Batch Loss: 0.19048219919204712\n",
      "Epoch 716, Loss: 0.5815851986408234, Final Batch Loss: 0.21166501939296722\n",
      "Epoch 717, Loss: 0.45964987576007843, Final Batch Loss: 0.12552271783351898\n",
      "Epoch 718, Loss: 0.48202408850193024, Final Batch Loss: 0.13052816689014435\n",
      "Epoch 719, Loss: 0.5481448024511337, Final Batch Loss: 0.16890765726566315\n",
      "Epoch 720, Loss: 0.507511094212532, Final Batch Loss: 0.14751452207565308\n",
      "Epoch 721, Loss: 0.5014183819293976, Final Batch Loss: 0.12531286478042603\n",
      "Epoch 722, Loss: 0.5105067640542984, Final Batch Loss: 0.15428534150123596\n",
      "Epoch 723, Loss: 0.5202032327651978, Final Batch Loss: 0.147780179977417\n",
      "Epoch 724, Loss: 0.5837198048830032, Final Batch Loss: 0.22590430080890656\n",
      "Epoch 725, Loss: 0.6028874516487122, Final Batch Loss: 0.21125434339046478\n",
      "Epoch 726, Loss: 0.7477106750011444, Final Batch Loss: 0.36005812883377075\n",
      "Epoch 727, Loss: 0.5328394323587418, Final Batch Loss: 0.16208797693252563\n",
      "Epoch 728, Loss: 0.5626577734947205, Final Batch Loss: 0.25159260630607605\n",
      "Epoch 729, Loss: 0.5733581483364105, Final Batch Loss: 0.23655708134174347\n",
      "Epoch 730, Loss: 0.4689817428588867, Final Batch Loss: 0.09838491678237915\n",
      "Epoch 731, Loss: 0.5149839371442795, Final Batch Loss: 0.16537335515022278\n",
      "Epoch 732, Loss: 0.6283462792634964, Final Batch Loss: 0.257352352142334\n",
      "Epoch 733, Loss: 0.603695273399353, Final Batch Loss: 0.13999521732330322\n",
      "Epoch 734, Loss: 0.6038711220026016, Final Batch Loss: 0.23128367960453033\n",
      "Epoch 735, Loss: 0.5463184714317322, Final Batch Loss: 0.1974053680896759\n",
      "Epoch 736, Loss: 0.5378792583942413, Final Batch Loss: 0.21036987006664276\n",
      "Epoch 737, Loss: 0.5837876945734024, Final Batch Loss: 0.1895199418067932\n",
      "Epoch 738, Loss: 0.567370131611824, Final Batch Loss: 0.21606674790382385\n",
      "Epoch 739, Loss: 0.47954676300287247, Final Batch Loss: 0.1238330826163292\n",
      "Epoch 740, Loss: 0.4737468659877777, Final Batch Loss: 0.10857963562011719\n",
      "Epoch 741, Loss: 0.5456878244876862, Final Batch Loss: 0.19903221726417542\n",
      "Epoch 742, Loss: 0.5925592631101608, Final Batch Loss: 0.24192045629024506\n",
      "Epoch 743, Loss: 0.5306543558835983, Final Batch Loss: 0.1749839186668396\n",
      "Epoch 744, Loss: 0.5334026217460632, Final Batch Loss: 0.22312264144420624\n",
      "Epoch 745, Loss: 0.447805255651474, Final Batch Loss: 0.1022055596113205\n",
      "Epoch 746, Loss: 0.5093397498130798, Final Batch Loss: 0.14363627135753632\n",
      "Epoch 747, Loss: 0.5262177288532257, Final Batch Loss: 0.19125854969024658\n",
      "Epoch 748, Loss: 0.4695636034011841, Final Batch Loss: 0.16784317791461945\n",
      "Epoch 749, Loss: 0.45967939496040344, Final Batch Loss: 0.11193521320819855\n",
      "Epoch 750, Loss: 0.622976541519165, Final Batch Loss: 0.295305460691452\n",
      "Epoch 751, Loss: 0.502592071890831, Final Batch Loss: 0.15353938937187195\n",
      "Epoch 752, Loss: 0.4489324167370796, Final Batch Loss: 0.12449810653924942\n",
      "Epoch 753, Loss: 0.4645070880651474, Final Batch Loss: 0.1388973891735077\n",
      "Epoch 754, Loss: 0.6247269213199615, Final Batch Loss: 0.29434898495674133\n",
      "Epoch 755, Loss: 0.5541637986898422, Final Batch Loss: 0.20760293304920197\n",
      "Epoch 756, Loss: 0.4856499433517456, Final Batch Loss: 0.14869022369384766\n",
      "Epoch 757, Loss: 0.561105340719223, Final Batch Loss: 0.2681178152561188\n",
      "Epoch 758, Loss: 0.5455977618694305, Final Batch Loss: 0.15304772555828094\n",
      "Epoch 759, Loss: 0.5524853467941284, Final Batch Loss: 0.18973298370838165\n",
      "Epoch 760, Loss: 0.49437248706817627, Final Batch Loss: 0.17337027192115784\n",
      "Epoch 761, Loss: 0.4550818130373955, Final Batch Loss: 0.11633450537919998\n",
      "Epoch 762, Loss: 0.500463679432869, Final Batch Loss: 0.16359063982963562\n",
      "Epoch 763, Loss: 0.38611380755901337, Final Batch Loss: 0.07134278118610382\n",
      "Epoch 764, Loss: 0.4948088228702545, Final Batch Loss: 0.16208487749099731\n",
      "Epoch 765, Loss: 0.5019789934158325, Final Batch Loss: 0.16875837743282318\n",
      "Epoch 766, Loss: 0.5877905339002609, Final Batch Loss: 0.2662258446216583\n",
      "Epoch 767, Loss: 0.6713401675224304, Final Batch Loss: 0.3573659360408783\n",
      "Epoch 768, Loss: 0.572309672832489, Final Batch Loss: 0.24117609858512878\n",
      "Epoch 769, Loss: 0.4020441547036171, Final Batch Loss: 0.08059263974428177\n",
      "Epoch 770, Loss: 0.6300350278615952, Final Batch Loss: 0.2998742163181305\n",
      "Epoch 771, Loss: 0.5839589089155197, Final Batch Loss: 0.19512037932872772\n",
      "Epoch 772, Loss: 0.5118912160396576, Final Batch Loss: 0.1124899834394455\n",
      "Epoch 773, Loss: 0.7276743650436401, Final Batch Loss: 0.3314859867095947\n",
      "Epoch 774, Loss: 0.436364084482193, Final Batch Loss: 0.1077309250831604\n",
      "Epoch 775, Loss: 0.4990421384572983, Final Batch Loss: 0.16496314108371735\n",
      "Epoch 776, Loss: 0.4466603100299835, Final Batch Loss: 0.12719960510730743\n",
      "Epoch 777, Loss: 0.48819924890995026, Final Batch Loss: 0.12990917265415192\n",
      "Epoch 778, Loss: 0.5041681528091431, Final Batch Loss: 0.16431549191474915\n",
      "Epoch 779, Loss: 0.6174347847700119, Final Batch Loss: 0.20903630554676056\n",
      "Epoch 780, Loss: 0.483625665307045, Final Batch Loss: 0.1308024674654007\n",
      "Epoch 781, Loss: 0.5802884995937347, Final Batch Loss: 0.2140723466873169\n",
      "Epoch 782, Loss: 0.4930168241262436, Final Batch Loss: 0.1877754032611847\n",
      "Epoch 783, Loss: 0.5467792749404907, Final Batch Loss: 0.209924578666687\n",
      "Epoch 784, Loss: 0.5481447875499725, Final Batch Loss: 0.21904344856739044\n",
      "Epoch 785, Loss: 0.5264794528484344, Final Batch Loss: 0.1547197848558426\n",
      "Epoch 786, Loss: 0.5933356136083603, Final Batch Loss: 0.2698977589607239\n",
      "Epoch 787, Loss: 0.6179405450820923, Final Batch Loss: 0.284991055727005\n",
      "Epoch 788, Loss: 0.4404713064432144, Final Batch Loss: 0.09487222135066986\n",
      "Epoch 789, Loss: 0.5814589709043503, Final Batch Loss: 0.21532130241394043\n",
      "Epoch 790, Loss: 0.46882228553295135, Final Batch Loss: 0.12631964683532715\n",
      "Epoch 791, Loss: 0.5246381610631943, Final Batch Loss: 0.1825869381427765\n",
      "Epoch 792, Loss: 0.5481773167848587, Final Batch Loss: 0.23653191328048706\n",
      "Epoch 793, Loss: 0.5093812942504883, Final Batch Loss: 0.15020151436328888\n",
      "Epoch 794, Loss: 0.4511686712503433, Final Batch Loss: 0.07861225306987762\n",
      "Epoch 795, Loss: 0.5673473924398422, Final Batch Loss: 0.1796831488609314\n",
      "Epoch 796, Loss: 0.5693396031856537, Final Batch Loss: 0.2680661380290985\n",
      "Epoch 797, Loss: 0.5264784097671509, Final Batch Loss: 0.21096011996269226\n",
      "Epoch 798, Loss: 0.4942961633205414, Final Batch Loss: 0.1416713446378708\n",
      "Epoch 799, Loss: 0.4753955602645874, Final Batch Loss: 0.15575450658798218\n",
      "Epoch 800, Loss: 0.5187303274869919, Final Batch Loss: 0.23171058297157288\n",
      "Epoch 801, Loss: 0.5004135966300964, Final Batch Loss: 0.20229631662368774\n",
      "Epoch 802, Loss: 0.44939011335372925, Final Batch Loss: 0.16380007565021515\n",
      "Epoch 803, Loss: 0.551562562584877, Final Batch Loss: 0.22715075314044952\n",
      "Epoch 804, Loss: 0.48034168779850006, Final Batch Loss: 0.12791644036769867\n",
      "Epoch 805, Loss: 0.5543854534626007, Final Batch Loss: 0.1811395138502121\n",
      "Epoch 806, Loss: 0.6098871678113937, Final Batch Loss: 0.23630531132221222\n",
      "Epoch 807, Loss: 0.4532238841056824, Final Batch Loss: 0.13601158559322357\n",
      "Epoch 808, Loss: 0.4177681803703308, Final Batch Loss: 0.08055922389030457\n",
      "Epoch 809, Loss: 0.7158361226320267, Final Batch Loss: 0.38685721158981323\n",
      "Epoch 810, Loss: 0.5562071949243546, Final Batch Loss: 0.2639947235584259\n",
      "Epoch 811, Loss: 0.528623953461647, Final Batch Loss: 0.2046390026807785\n",
      "Epoch 812, Loss: 0.4797496497631073, Final Batch Loss: 0.1442517787218094\n",
      "Epoch 813, Loss: 0.4405582398176193, Final Batch Loss: 0.12929582595825195\n",
      "Epoch 814, Loss: 0.5032088309526443, Final Batch Loss: 0.16354312002658844\n",
      "Epoch 815, Loss: 0.5555070042610168, Final Batch Loss: 0.16656149923801422\n",
      "Epoch 816, Loss: 0.5218878090381622, Final Batch Loss: 0.2294733077287674\n",
      "Epoch 817, Loss: 0.4609294533729553, Final Batch Loss: 0.12900610268115997\n",
      "Epoch 818, Loss: 0.47206199169158936, Final Batch Loss: 0.1512635201215744\n",
      "Epoch 819, Loss: 0.36734798178076744, Final Batch Loss: 0.059736739844083786\n",
      "Epoch 820, Loss: 0.46179598569869995, Final Batch Loss: 0.14365315437316895\n",
      "Epoch 821, Loss: 0.4062390401959419, Final Batch Loss: 0.09745199233293533\n",
      "Epoch 822, Loss: 0.4409620016813278, Final Batch Loss: 0.13520829379558563\n",
      "Epoch 823, Loss: 0.3778436556458473, Final Batch Loss: 0.08833970874547958\n",
      "Epoch 824, Loss: 0.5870248377323151, Final Batch Loss: 0.21257871389389038\n",
      "Epoch 825, Loss: 0.5055962949991226, Final Batch Loss: 0.2024489939212799\n",
      "Epoch 826, Loss: 0.5188476294279099, Final Batch Loss: 0.1830461174249649\n",
      "Epoch 827, Loss: 0.47714968025684357, Final Batch Loss: 0.15582944452762604\n",
      "Epoch 828, Loss: 0.44297248870134354, Final Batch Loss: 0.08986502140760422\n",
      "Epoch 829, Loss: 0.5254490822553635, Final Batch Loss: 0.13458669185638428\n",
      "Epoch 830, Loss: 0.4263289347290993, Final Batch Loss: 0.11486030369997025\n",
      "Epoch 831, Loss: 0.4195239618420601, Final Batch Loss: 0.10057096928358078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 832, Loss: 0.4434042274951935, Final Batch Loss: 0.12084606289863586\n",
      "Epoch 833, Loss: 0.4371773302555084, Final Batch Loss: 0.14208576083183289\n",
      "Epoch 834, Loss: 0.4438712149858475, Final Batch Loss: 0.08147796988487244\n",
      "Epoch 835, Loss: 0.504413515329361, Final Batch Loss: 0.17269256711006165\n",
      "Epoch 836, Loss: 0.47299744188785553, Final Batch Loss: 0.1729438602924347\n",
      "Epoch 837, Loss: 0.66666679084301, Final Batch Loss: 0.3455222547054291\n",
      "Epoch 838, Loss: 0.36278045922517776, Final Batch Loss: 0.06796547025442123\n",
      "Epoch 839, Loss: 0.4101683497428894, Final Batch Loss: 0.08168646693229675\n",
      "Epoch 840, Loss: 0.44978785514831543, Final Batch Loss: 0.15573713183403015\n",
      "Epoch 841, Loss: 0.452222540974617, Final Batch Loss: 0.16247060894966125\n",
      "Epoch 842, Loss: 0.3891838565468788, Final Batch Loss: 0.0657605305314064\n",
      "Epoch 843, Loss: 0.4211288094520569, Final Batch Loss: 0.15132594108581543\n",
      "Epoch 844, Loss: 0.5036428421735764, Final Batch Loss: 0.19558049738407135\n",
      "Epoch 845, Loss: 0.46775637567043304, Final Batch Loss: 0.14037834107875824\n",
      "Epoch 846, Loss: 0.44333672523498535, Final Batch Loss: 0.15333020687103271\n",
      "Epoch 847, Loss: 0.43605756759643555, Final Batch Loss: 0.11041772365570068\n",
      "Epoch 848, Loss: 0.36586469411849976, Final Batch Loss: 0.09178312867879868\n",
      "Epoch 849, Loss: 0.49231089651584625, Final Batch Loss: 0.1993400603532791\n",
      "Epoch 850, Loss: 0.5516926646232605, Final Batch Loss: 0.2120368778705597\n",
      "Epoch 851, Loss: 0.461145356297493, Final Batch Loss: 0.18585681915283203\n",
      "Epoch 852, Loss: 0.4665466845035553, Final Batch Loss: 0.14574293792247772\n",
      "Epoch 853, Loss: 0.4903789311647415, Final Batch Loss: 0.1479731947183609\n",
      "Epoch 854, Loss: 0.5220641791820526, Final Batch Loss: 0.16033969819545746\n",
      "Epoch 855, Loss: 0.4727484881877899, Final Batch Loss: 0.18205608427524567\n",
      "Epoch 856, Loss: 0.5248234122991562, Final Batch Loss: 0.23616895079612732\n",
      "Epoch 857, Loss: 0.5319570899009705, Final Batch Loss: 0.2046482414007187\n",
      "Epoch 858, Loss: 0.5146996676921844, Final Batch Loss: 0.19458836317062378\n",
      "Epoch 859, Loss: 0.4022638127207756, Final Batch Loss: 0.11088874191045761\n",
      "Epoch 860, Loss: 0.4324929267168045, Final Batch Loss: 0.13092418015003204\n",
      "Epoch 861, Loss: 0.4987301826477051, Final Batch Loss: 0.2187402993440628\n",
      "Epoch 862, Loss: 0.34370483085513115, Final Batch Loss: 0.04401956871151924\n",
      "Epoch 863, Loss: 0.465082585811615, Final Batch Loss: 0.1711433082818985\n",
      "Epoch 864, Loss: 0.42989830672740936, Final Batch Loss: 0.12983281910419464\n",
      "Epoch 865, Loss: 0.6766853928565979, Final Batch Loss: 0.3576645255088806\n",
      "Epoch 866, Loss: 0.47885364294052124, Final Batch Loss: 0.1781015396118164\n",
      "Epoch 867, Loss: 0.38579394668340683, Final Batch Loss: 0.08573175221681595\n",
      "Epoch 868, Loss: 0.5212062299251556, Final Batch Loss: 0.16865192353725433\n",
      "Epoch 869, Loss: 0.5248831808567047, Final Batch Loss: 0.17591576278209686\n",
      "Epoch 870, Loss: 0.4105682373046875, Final Batch Loss: 0.13731136918067932\n",
      "Epoch 871, Loss: 0.5095919519662857, Final Batch Loss: 0.16971346735954285\n",
      "Epoch 872, Loss: 0.43420524522662163, Final Batch Loss: 0.043726976960897446\n",
      "Epoch 873, Loss: 0.359497532248497, Final Batch Loss: 0.08938999474048615\n",
      "Epoch 874, Loss: 0.4445706605911255, Final Batch Loss: 0.1028001457452774\n",
      "Epoch 875, Loss: 0.5004966855049133, Final Batch Loss: 0.20321126282215118\n",
      "Epoch 876, Loss: 0.48831114172935486, Final Batch Loss: 0.2118767350912094\n",
      "Epoch 877, Loss: 0.3445240147411823, Final Batch Loss: 0.049486998468637466\n",
      "Epoch 878, Loss: 0.47896362841129303, Final Batch Loss: 0.17992377281188965\n",
      "Epoch 879, Loss: 0.40690668672323227, Final Batch Loss: 0.0513269379734993\n",
      "Epoch 880, Loss: 0.5453154593706131, Final Batch Loss: 0.20159068703651428\n",
      "Epoch 881, Loss: 0.5022555291652679, Final Batch Loss: 0.20345225930213928\n",
      "Epoch 882, Loss: 0.6354546695947647, Final Batch Loss: 0.2957271933555603\n",
      "Epoch 883, Loss: 0.4571078270673752, Final Batch Loss: 0.1414710283279419\n",
      "Epoch 884, Loss: 0.5399908870458603, Final Batch Loss: 0.2023112028837204\n",
      "Epoch 885, Loss: 0.5048961639404297, Final Batch Loss: 0.19398999214172363\n",
      "Epoch 886, Loss: 0.5166885703802109, Final Batch Loss: 0.16917917132377625\n",
      "Epoch 887, Loss: 0.43188609182834625, Final Batch Loss: 0.13670293986797333\n",
      "Epoch 888, Loss: 0.44841768592596054, Final Batch Loss: 0.17706039547920227\n",
      "Epoch 889, Loss: 0.40139680355787277, Final Batch Loss: 0.11279437690973282\n",
      "Epoch 890, Loss: 0.41843198239803314, Final Batch Loss: 0.12891238927841187\n",
      "Epoch 891, Loss: 0.5451740771532059, Final Batch Loss: 0.25289401412010193\n",
      "Epoch 892, Loss: 0.40167638659477234, Final Batch Loss: 0.08242771029472351\n",
      "Epoch 893, Loss: 0.4732504189014435, Final Batch Loss: 0.13135936856269836\n",
      "Epoch 894, Loss: 0.40579604357481003, Final Batch Loss: 0.10647798329591751\n",
      "Epoch 895, Loss: 0.4016362279653549, Final Batch Loss: 0.06976890563964844\n",
      "Epoch 896, Loss: 0.47259658575057983, Final Batch Loss: 0.1744193434715271\n",
      "Epoch 897, Loss: 0.38849274814128876, Final Batch Loss: 0.0861593633890152\n",
      "Epoch 898, Loss: 0.49798576533794403, Final Batch Loss: 0.12623168528079987\n",
      "Epoch 899, Loss: 0.500797912478447, Final Batch Loss: 0.20329511165618896\n",
      "Epoch 900, Loss: 0.4966808781027794, Final Batch Loss: 0.11419191211462021\n",
      "Epoch 901, Loss: 0.529754102230072, Final Batch Loss: 0.2515115439891815\n",
      "Epoch 902, Loss: 0.42500579357147217, Final Batch Loss: 0.14844654500484467\n",
      "Epoch 903, Loss: 0.5241811722517014, Final Batch Loss: 0.24620048701763153\n",
      "Epoch 904, Loss: 0.39199312776327133, Final Batch Loss: 0.10835821181535721\n",
      "Epoch 905, Loss: 0.502772644162178, Final Batch Loss: 0.15037985146045685\n",
      "Epoch 906, Loss: 0.4559459239244461, Final Batch Loss: 0.1555132418870926\n",
      "Epoch 907, Loss: 0.386658176779747, Final Batch Loss: 0.08985082805156708\n",
      "Epoch 908, Loss: 0.34475889801979065, Final Batch Loss: 0.05798271298408508\n",
      "Epoch 909, Loss: 0.42188121378421783, Final Batch Loss: 0.13562192022800446\n",
      "Epoch 910, Loss: 0.4389781951904297, Final Batch Loss: 0.1706913560628891\n",
      "Epoch 911, Loss: 0.4317774474620819, Final Batch Loss: 0.1534598171710968\n",
      "Epoch 912, Loss: 0.38528768718242645, Final Batch Loss: 0.11767597496509552\n",
      "Epoch 913, Loss: 0.4300411641597748, Final Batch Loss: 0.12676067650318146\n",
      "Epoch 914, Loss: 0.5870422869920731, Final Batch Loss: 0.23036114871501923\n",
      "Epoch 915, Loss: 0.4898745268583298, Final Batch Loss: 0.15287275612354279\n",
      "Epoch 916, Loss: 0.514670193195343, Final Batch Loss: 0.18318860232830048\n",
      "Epoch 917, Loss: 0.4589604139328003, Final Batch Loss: 0.18132303655147552\n",
      "Epoch 918, Loss: 0.5402529835700989, Final Batch Loss: 0.24929150938987732\n",
      "Epoch 919, Loss: 0.43015167117118835, Final Batch Loss: 0.07901132106781006\n",
      "Epoch 920, Loss: 0.4603852406144142, Final Batch Loss: 0.17099988460540771\n",
      "Epoch 921, Loss: 0.4618913233280182, Final Batch Loss: 0.10136391222476959\n",
      "Epoch 922, Loss: 0.4869818240404129, Final Batch Loss: 0.15966196358203888\n",
      "Epoch 923, Loss: 0.45956040918827057, Final Batch Loss: 0.13026942312717438\n",
      "Epoch 924, Loss: 0.45649632811546326, Final Batch Loss: 0.1413625329732895\n",
      "Epoch 925, Loss: 0.4532461240887642, Final Batch Loss: 0.11874697357416153\n",
      "Epoch 926, Loss: 0.491106815636158, Final Batch Loss: 0.19752825796604156\n",
      "Epoch 927, Loss: 0.40397581458091736, Final Batch Loss: 0.11464272439479828\n",
      "Epoch 928, Loss: 0.39739255607128143, Final Batch Loss: 0.12323673069477081\n",
      "Epoch 929, Loss: 0.5222610384225845, Final Batch Loss: 0.2276650220155716\n",
      "Epoch 930, Loss: 0.4509551525115967, Final Batch Loss: 0.16053733229637146\n",
      "Epoch 931, Loss: 0.3870485723018646, Final Batch Loss: 0.09253944456577301\n",
      "Epoch 932, Loss: 0.41922132670879364, Final Batch Loss: 0.08910726010799408\n",
      "Epoch 933, Loss: 0.3773135095834732, Final Batch Loss: 0.06922875344753265\n",
      "Epoch 934, Loss: 0.4356251731514931, Final Batch Loss: 0.16593869030475616\n",
      "Epoch 935, Loss: 0.43494749069213867, Final Batch Loss: 0.1263955682516098\n",
      "Epoch 936, Loss: 0.42027999460697174, Final Batch Loss: 0.14961004257202148\n",
      "Epoch 937, Loss: 0.4053517282009125, Final Batch Loss: 0.16088102757930756\n",
      "Epoch 938, Loss: 0.40140485018491745, Final Batch Loss: 0.16408415138721466\n",
      "Epoch 939, Loss: 0.521445170044899, Final Batch Loss: 0.17993421852588654\n",
      "Epoch 940, Loss: 0.4616224095225334, Final Batch Loss: 0.10038728266954422\n",
      "Epoch 941, Loss: 0.48515719920396805, Final Batch Loss: 0.21775369346141815\n",
      "Epoch 942, Loss: 0.3794691190123558, Final Batch Loss: 0.10543201118707657\n",
      "Epoch 943, Loss: 0.498039186000824, Final Batch Loss: 0.2521243989467621\n",
      "Epoch 944, Loss: 0.4479716494679451, Final Batch Loss: 0.1049332246184349\n",
      "Epoch 945, Loss: 0.42897334694862366, Final Batch Loss: 0.1330977976322174\n",
      "Epoch 946, Loss: 0.3923753350973129, Final Batch Loss: 0.12054307758808136\n",
      "Epoch 947, Loss: 0.35915348678827286, Final Batch Loss: 0.08841177076101303\n",
      "Epoch 948, Loss: 0.32295297645032406, Final Batch Loss: 0.0251066442579031\n",
      "Epoch 949, Loss: 0.37841831892728806, Final Batch Loss: 0.10620943456888199\n",
      "Epoch 950, Loss: 0.3490798994898796, Final Batch Loss: 0.06578392535448074\n",
      "Epoch 951, Loss: 0.4121588468551636, Final Batch Loss: 0.14096646010875702\n",
      "Epoch 952, Loss: 0.4455096945166588, Final Batch Loss: 0.18876655399799347\n",
      "Epoch 953, Loss: 0.3774164691567421, Final Batch Loss: 0.1052735447883606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 954, Loss: 0.40872761607170105, Final Batch Loss: 0.13866133987903595\n",
      "Epoch 955, Loss: 0.5162342190742493, Final Batch Loss: 0.229261115193367\n",
      "Epoch 956, Loss: 0.429170623421669, Final Batch Loss: 0.09100803732872009\n",
      "Epoch 957, Loss: 0.4568279907107353, Final Batch Loss: 0.2298946976661682\n",
      "Epoch 958, Loss: 0.4412830173969269, Final Batch Loss: 0.12973037362098694\n",
      "Epoch 959, Loss: 0.46445470303297043, Final Batch Loss: 0.20588386058807373\n",
      "Epoch 960, Loss: 0.36065009236335754, Final Batch Loss: 0.093110091984272\n",
      "Epoch 961, Loss: 0.4170301556587219, Final Batch Loss: 0.11200970411300659\n",
      "Epoch 962, Loss: 0.46668848395347595, Final Batch Loss: 0.22896860539913177\n",
      "Epoch 963, Loss: 0.4771255627274513, Final Batch Loss: 0.23725546896457672\n",
      "Epoch 964, Loss: 0.41497335582971573, Final Batch Loss: 0.033323101699352264\n",
      "Epoch 965, Loss: 0.4348517879843712, Final Batch Loss: 0.1453874558210373\n",
      "Epoch 966, Loss: 0.46969670057296753, Final Batch Loss: 0.1474175900220871\n",
      "Epoch 967, Loss: 0.4354446232318878, Final Batch Loss: 0.129325732588768\n",
      "Epoch 968, Loss: 0.410566009581089, Final Batch Loss: 0.10098034888505936\n",
      "Epoch 969, Loss: 0.42942488193511963, Final Batch Loss: 0.20331637561321259\n",
      "Epoch 970, Loss: 0.43067365884780884, Final Batch Loss: 0.14471258223056793\n",
      "Epoch 971, Loss: 0.4430813193321228, Final Batch Loss: 0.14890357851982117\n",
      "Epoch 972, Loss: 0.4493402987718582, Final Batch Loss: 0.18175403773784637\n",
      "Epoch 973, Loss: 0.5167325437068939, Final Batch Loss: 0.16829536855220795\n",
      "Epoch 974, Loss: 0.3973446786403656, Final Batch Loss: 0.15396109223365784\n",
      "Epoch 975, Loss: 0.3726658448576927, Final Batch Loss: 0.1062551960349083\n",
      "Epoch 976, Loss: 0.5038119107484818, Final Batch Loss: 0.204928919672966\n",
      "Epoch 977, Loss: 0.47930020838975906, Final Batch Loss: 0.19572508335113525\n",
      "Epoch 978, Loss: 0.4408607631921768, Final Batch Loss: 0.12608326971530914\n",
      "Epoch 979, Loss: 0.48293353617191315, Final Batch Loss: 0.1769893318414688\n",
      "Epoch 980, Loss: 0.4685962647199631, Final Batch Loss: 0.1863580048084259\n",
      "Epoch 981, Loss: 0.4034666046500206, Final Batch Loss: 0.1532549113035202\n",
      "Epoch 982, Loss: 0.37589262425899506, Final Batch Loss: 0.08333766460418701\n",
      "Epoch 983, Loss: 0.41961801797151566, Final Batch Loss: 0.08913909643888474\n",
      "Epoch 984, Loss: 0.3924502581357956, Final Batch Loss: 0.0971699059009552\n",
      "Epoch 985, Loss: 0.4383189529180527, Final Batch Loss: 0.13342095911502838\n",
      "Epoch 986, Loss: 0.4901510775089264, Final Batch Loss: 0.14915831387043\n",
      "Epoch 987, Loss: 0.4566844254732132, Final Batch Loss: 0.18659795820713043\n",
      "Epoch 988, Loss: 0.4109395816922188, Final Batch Loss: 0.09545011073350906\n",
      "Epoch 989, Loss: 0.3338964432477951, Final Batch Loss: 0.08718787133693695\n",
      "Epoch 990, Loss: 0.4073934629559517, Final Batch Loss: 0.13258853554725647\n",
      "Epoch 991, Loss: 0.33963122218847275, Final Batch Loss: 0.08561472594738007\n",
      "Epoch 992, Loss: 0.33744219690561295, Final Batch Loss: 0.08504621684551239\n",
      "Epoch 993, Loss: 0.33473533019423485, Final Batch Loss: 0.04748646542429924\n",
      "Epoch 994, Loss: 0.3226821944117546, Final Batch Loss: 0.05846170336008072\n",
      "Epoch 995, Loss: 0.4436217471957207, Final Batch Loss: 0.19386142492294312\n",
      "Epoch 996, Loss: 0.336263507604599, Final Batch Loss: 0.06040893495082855\n",
      "Epoch 997, Loss: 0.41517817229032516, Final Batch Loss: 0.08113227784633636\n",
      "Epoch 998, Loss: 0.41906195878982544, Final Batch Loss: 0.09625367820262909\n",
      "Epoch 999, Loss: 0.33697422966361046, Final Batch Loss: 0.048179496079683304\n",
      "Epoch 1000, Loss: 0.3335627391934395, Final Batch Loss: 0.08770523220300674\n",
      "Epoch 1001, Loss: 0.42730899155139923, Final Batch Loss: 0.07423990219831467\n",
      "Epoch 1002, Loss: 0.46407005190849304, Final Batch Loss: 0.20653022825717926\n",
      "Epoch 1003, Loss: 0.5659810677170753, Final Batch Loss: 0.3025331199169159\n",
      "Epoch 1004, Loss: 0.4626914709806442, Final Batch Loss: 0.13815093040466309\n",
      "Epoch 1005, Loss: 0.3850924074649811, Final Batch Loss: 0.09374940395355225\n",
      "Epoch 1006, Loss: 0.5356453433632851, Final Batch Loss: 0.26138296723365784\n",
      "Epoch 1007, Loss: 0.482004813849926, Final Batch Loss: 0.1885915994644165\n",
      "Epoch 1008, Loss: 0.45991721749305725, Final Batch Loss: 0.1691804975271225\n",
      "Epoch 1009, Loss: 0.3350091353058815, Final Batch Loss: 0.06134063005447388\n",
      "Epoch 1010, Loss: 0.37216032296419144, Final Batch Loss: 0.13441024720668793\n",
      "Epoch 1011, Loss: 0.3962584212422371, Final Batch Loss: 0.09365741163492203\n",
      "Epoch 1012, Loss: 0.4502881318330765, Final Batch Loss: 0.1663343906402588\n",
      "Epoch 1013, Loss: 0.4328354150056839, Final Batch Loss: 0.1292126476764679\n",
      "Epoch 1014, Loss: 0.45254795253276825, Final Batch Loss: 0.1444065123796463\n",
      "Epoch 1015, Loss: 0.46574321389198303, Final Batch Loss: 0.17685295641422272\n",
      "Epoch 1016, Loss: 0.3610401675105095, Final Batch Loss: 0.09729969501495361\n",
      "Epoch 1017, Loss: 0.47177161276340485, Final Batch Loss: 0.21217957139015198\n",
      "Epoch 1018, Loss: 0.34309305995702744, Final Batch Loss: 0.07719308882951736\n",
      "Epoch 1019, Loss: 0.4113961458206177, Final Batch Loss: 0.13225053250789642\n",
      "Epoch 1020, Loss: 0.34271829947829247, Final Batch Loss: 0.06093968078494072\n",
      "Epoch 1021, Loss: 0.29589283280074596, Final Batch Loss: 0.026284461840987206\n",
      "Epoch 1022, Loss: 0.40381262451410294, Final Batch Loss: 0.15563853085041046\n",
      "Epoch 1023, Loss: 0.39671770483255386, Final Batch Loss: 0.11641456931829453\n",
      "Epoch 1024, Loss: 0.4019688442349434, Final Batch Loss: 0.11378639191389084\n",
      "Epoch 1025, Loss: 0.3265652246773243, Final Batch Loss: 0.058982376009225845\n",
      "Epoch 1026, Loss: 0.38218123465776443, Final Batch Loss: 0.14330747723579407\n",
      "Epoch 1027, Loss: 0.4119797796010971, Final Batch Loss: 0.09008213877677917\n",
      "Epoch 1028, Loss: 0.467151015996933, Final Batch Loss: 0.17472223937511444\n",
      "Epoch 1029, Loss: 0.4600815698504448, Final Batch Loss: 0.214278444647789\n",
      "Epoch 1030, Loss: 0.4099099412560463, Final Batch Loss: 0.1714903712272644\n",
      "Epoch 1031, Loss: 0.36964505165815353, Final Batch Loss: 0.10287903994321823\n",
      "Epoch 1032, Loss: 0.40320418030023575, Final Batch Loss: 0.1783059984445572\n",
      "Epoch 1033, Loss: 0.31501899287104607, Final Batch Loss: 0.040441740304231644\n",
      "Epoch 1034, Loss: 0.35046299546957016, Final Batch Loss: 0.10533245652914047\n",
      "Epoch 1035, Loss: 0.4965002238750458, Final Batch Loss: 0.1717555820941925\n",
      "Epoch 1036, Loss: 0.42627889662981033, Final Batch Loss: 0.12263744324445724\n",
      "Epoch 1037, Loss: 0.4459201991558075, Final Batch Loss: 0.15631604194641113\n",
      "Epoch 1038, Loss: 0.5244194492697716, Final Batch Loss: 0.2538468837738037\n",
      "Epoch 1039, Loss: 0.44825249910354614, Final Batch Loss: 0.20536299049854279\n",
      "Epoch 1040, Loss: 0.442803256213665, Final Batch Loss: 0.16034872829914093\n",
      "Epoch 1041, Loss: 0.4342300817370415, Final Batch Loss: 0.13326044380664825\n",
      "Epoch 1042, Loss: 0.35820772498846054, Final Batch Loss: 0.0632036104798317\n",
      "Epoch 1043, Loss: 0.3966454938054085, Final Batch Loss: 0.1073470339179039\n",
      "Epoch 1044, Loss: 0.3863167464733124, Final Batch Loss: 0.13838829100131989\n",
      "Epoch 1045, Loss: 0.35759493708610535, Final Batch Loss: 0.10247743874788284\n",
      "Epoch 1046, Loss: 0.38578464090824127, Final Batch Loss: 0.14386577904224396\n",
      "Epoch 1047, Loss: 0.4156505763530731, Final Batch Loss: 0.11656923592090607\n",
      "Epoch 1048, Loss: 0.4098120257258415, Final Batch Loss: 0.06569603830575943\n",
      "Epoch 1049, Loss: 0.6192298829555511, Final Batch Loss: 0.3598713278770447\n",
      "Epoch 1050, Loss: 0.4565217047929764, Final Batch Loss: 0.10227896273136139\n",
      "Epoch 1051, Loss: 0.49463681876659393, Final Batch Loss: 0.19459159672260284\n",
      "Epoch 1052, Loss: 0.400151364505291, Final Batch Loss: 0.14478452503681183\n",
      "Epoch 1053, Loss: 0.39282071590423584, Final Batch Loss: 0.11024676263332367\n",
      "Epoch 1054, Loss: 0.3867134153842926, Final Batch Loss: 0.11934611201286316\n",
      "Epoch 1055, Loss: 0.37967053055763245, Final Batch Loss: 0.11776809394359589\n",
      "Epoch 1056, Loss: 0.5250612199306488, Final Batch Loss: 0.2642185091972351\n",
      "Epoch 1057, Loss: 0.44661109894514084, Final Batch Loss: 0.12407951802015305\n",
      "Epoch 1058, Loss: 0.38603465259075165, Final Batch Loss: 0.14635546505451202\n",
      "Epoch 1059, Loss: 0.4128073453903198, Final Batch Loss: 0.10505698621273041\n",
      "Epoch 1060, Loss: 0.3898920342326164, Final Batch Loss: 0.09872554987668991\n",
      "Epoch 1061, Loss: 0.5033066868782043, Final Batch Loss: 0.21142123639583588\n",
      "Epoch 1062, Loss: 0.3837827518582344, Final Batch Loss: 0.12506964802742004\n",
      "Epoch 1063, Loss: 0.47331565618515015, Final Batch Loss: 0.16985847055912018\n",
      "Epoch 1064, Loss: 0.33289235085248947, Final Batch Loss: 0.07178332656621933\n",
      "Epoch 1065, Loss: 0.35826829075813293, Final Batch Loss: 0.07955712080001831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1066, Loss: 0.5177869349718094, Final Batch Loss: 0.21835488080978394\n",
      "Epoch 1067, Loss: 0.4487474858760834, Final Batch Loss: 0.16199785470962524\n",
      "Epoch 1068, Loss: 0.43118542432785034, Final Batch Loss: 0.16314662992954254\n",
      "Epoch 1069, Loss: 0.3507612571120262, Final Batch Loss: 0.09524396806955338\n",
      "Epoch 1070, Loss: 0.4046688824892044, Final Batch Loss: 0.12631376087665558\n",
      "Epoch 1071, Loss: 0.5063397288322449, Final Batch Loss: 0.1806638240814209\n",
      "Epoch 1072, Loss: 0.5187766700983047, Final Batch Loss: 0.21771447360515594\n",
      "Epoch 1073, Loss: 0.47999054193496704, Final Batch Loss: 0.18036328256130219\n",
      "Epoch 1074, Loss: 0.49224861711263657, Final Batch Loss: 0.18922916054725647\n",
      "Epoch 1075, Loss: 0.44240567088127136, Final Batch Loss: 0.11330090463161469\n",
      "Epoch 1076, Loss: 0.3795495927333832, Final Batch Loss: 0.09954400360584259\n",
      "Epoch 1077, Loss: 0.5155976116657257, Final Batch Loss: 0.18942934274673462\n",
      "Epoch 1078, Loss: 0.39975952357053757, Final Batch Loss: 0.1426623910665512\n",
      "Epoch 1079, Loss: 0.42133573442697525, Final Batch Loss: 0.12148801237344742\n",
      "Epoch 1080, Loss: 0.38426078855991364, Final Batch Loss: 0.11745893955230713\n",
      "Epoch 1081, Loss: 0.3627527728676796, Final Batch Loss: 0.11966914683580399\n",
      "Epoch 1082, Loss: 0.4562416449189186, Final Batch Loss: 0.19652357697486877\n",
      "Epoch 1083, Loss: 0.32609057798981667, Final Batch Loss: 0.06179136410355568\n",
      "Epoch 1084, Loss: 0.33130843192338943, Final Batch Loss: 0.07499723881483078\n",
      "Epoch 1085, Loss: 0.40508051961660385, Final Batch Loss: 0.14369027316570282\n",
      "Epoch 1086, Loss: 0.36468424648046494, Final Batch Loss: 0.07973607629537582\n",
      "Epoch 1087, Loss: 0.35749249160289764, Final Batch Loss: 0.0898129791021347\n",
      "Epoch 1088, Loss: 0.35107878781855106, Final Batch Loss: 0.029116788879036903\n",
      "Epoch 1089, Loss: 0.33748260885477066, Final Batch Loss: 0.06942833214998245\n",
      "Epoch 1090, Loss: 0.43784884363412857, Final Batch Loss: 0.23006753623485565\n",
      "Epoch 1091, Loss: 0.3614904507994652, Final Batch Loss: 0.10123395919799805\n",
      "Epoch 1092, Loss: 0.43993666768074036, Final Batch Loss: 0.20508380234241486\n",
      "Epoch 1093, Loss: 0.35826507955789566, Final Batch Loss: 0.1504596769809723\n",
      "Epoch 1094, Loss: 0.3552059605717659, Final Batch Loss: 0.1205478310585022\n",
      "Epoch 1095, Loss: 0.3524825870990753, Final Batch Loss: 0.09399662166833878\n",
      "Epoch 1096, Loss: 0.342859148979187, Final Batch Loss: 0.08615434914827347\n",
      "Epoch 1097, Loss: 0.39181970059871674, Final Batch Loss: 0.14642158150672913\n",
      "Epoch 1098, Loss: 0.2745707407593727, Final Batch Loss: 0.0493481308221817\n",
      "Epoch 1099, Loss: 0.4008956700563431, Final Batch Loss: 0.16027069091796875\n",
      "Epoch 1100, Loss: 0.3690239414572716, Final Batch Loss: 0.07596586644649506\n",
      "Epoch 1101, Loss: 0.3658231273293495, Final Batch Loss: 0.07505124807357788\n",
      "Epoch 1102, Loss: 0.3526744470000267, Final Batch Loss: 0.0996299460530281\n",
      "Epoch 1103, Loss: 0.3690200299024582, Final Batch Loss: 0.0919918417930603\n",
      "Epoch 1104, Loss: 0.3926985412836075, Final Batch Loss: 0.12594705820083618\n",
      "Epoch 1105, Loss: 0.38285131752491, Final Batch Loss: 0.1312600076198578\n",
      "Epoch 1106, Loss: 0.37122657150030136, Final Batch Loss: 0.11552359908819199\n",
      "Epoch 1107, Loss: 0.36112459003925323, Final Batch Loss: 0.11409454792737961\n",
      "Epoch 1108, Loss: 0.4141404926776886, Final Batch Loss: 0.10284000635147095\n",
      "Epoch 1109, Loss: 0.35000254958868027, Final Batch Loss: 0.07446645945310593\n",
      "Epoch 1110, Loss: 0.32600830867886543, Final Batch Loss: 0.0489879734814167\n",
      "Epoch 1111, Loss: 0.3095940425992012, Final Batch Loss: 0.07000204920768738\n",
      "Epoch 1112, Loss: 0.3908892944455147, Final Batch Loss: 0.09218213707208633\n",
      "Epoch 1113, Loss: 0.3471079245209694, Final Batch Loss: 0.13247933983802795\n",
      "Epoch 1114, Loss: 0.5184298306703568, Final Batch Loss: 0.18791836500167847\n",
      "Epoch 1115, Loss: 0.41152021288871765, Final Batch Loss: 0.17849956452846527\n",
      "Epoch 1116, Loss: 0.35519546270370483, Final Batch Loss: 0.0664115697145462\n",
      "Epoch 1117, Loss: 0.3102540075778961, Final Batch Loss: 0.051192425191402435\n",
      "Epoch 1118, Loss: 0.35211942344903946, Final Batch Loss: 0.0677172914147377\n",
      "Epoch 1119, Loss: 0.35560139268636703, Final Batch Loss: 0.07934259623289108\n",
      "Epoch 1120, Loss: 0.45323655009269714, Final Batch Loss: 0.1457340121269226\n",
      "Epoch 1121, Loss: 0.549720048904419, Final Batch Loss: 0.31517234444618225\n",
      "Epoch 1122, Loss: 0.4566758722066879, Final Batch Loss: 0.18148934841156006\n",
      "Epoch 1123, Loss: 0.4894203692674637, Final Batch Loss: 0.24410174787044525\n",
      "Epoch 1124, Loss: 0.5131574720144272, Final Batch Loss: 0.2197737842798233\n",
      "Epoch 1125, Loss: 0.47077739238739014, Final Batch Loss: 0.20222054421901703\n",
      "Epoch 1126, Loss: 0.4486822113394737, Final Batch Loss: 0.1619606912136078\n",
      "Epoch 1127, Loss: 0.4433068484067917, Final Batch Loss: 0.17277689278125763\n",
      "Epoch 1128, Loss: 0.340642474591732, Final Batch Loss: 0.09064707159996033\n",
      "Epoch 1129, Loss: 0.4077467769384384, Final Batch Loss: 0.11886116117238998\n",
      "Epoch 1130, Loss: 0.5169777423143387, Final Batch Loss: 0.2418084293603897\n",
      "Epoch 1131, Loss: 0.33383888006210327, Final Batch Loss: 0.11069965362548828\n",
      "Epoch 1132, Loss: 0.40703529864549637, Final Batch Loss: 0.11328522115945816\n",
      "Epoch 1133, Loss: 0.3522249385714531, Final Batch Loss: 0.10290525108575821\n",
      "Epoch 1134, Loss: 0.4026026651263237, Final Batch Loss: 0.14793290197849274\n",
      "Epoch 1135, Loss: 0.3571924641728401, Final Batch Loss: 0.12404600530862808\n",
      "Epoch 1136, Loss: 0.34363583475351334, Final Batch Loss: 0.035090379416942596\n",
      "Epoch 1137, Loss: 0.41829314082860947, Final Batch Loss: 0.16723819077014923\n",
      "Epoch 1138, Loss: 0.3638855963945389, Final Batch Loss: 0.11991162598133087\n",
      "Epoch 1139, Loss: 0.3244936093688011, Final Batch Loss: 0.09144897758960724\n",
      "Epoch 1140, Loss: 0.4628191888332367, Final Batch Loss: 0.22369228303432465\n",
      "Epoch 1141, Loss: 0.508744515478611, Final Batch Loss: 0.2301727533340454\n",
      "Epoch 1142, Loss: 0.3965837210416794, Final Batch Loss: 0.12260043621063232\n",
      "Epoch 1143, Loss: 0.5209798961877823, Final Batch Loss: 0.19848932325839996\n",
      "Epoch 1144, Loss: 0.32769446820020676, Final Batch Loss: 0.06483330577611923\n",
      "Epoch 1145, Loss: 0.6104554980993271, Final Batch Loss: 0.291708379983902\n",
      "Epoch 1146, Loss: 0.382551871240139, Final Batch Loss: 0.08590879291296005\n",
      "Epoch 1147, Loss: 0.36454562842845917, Final Batch Loss: 0.12689261138439178\n",
      "Epoch 1148, Loss: 0.576125830411911, Final Batch Loss: 0.2835094630718231\n",
      "Epoch 1149, Loss: 0.36015816777944565, Final Batch Loss: 0.08120275288820267\n",
      "Epoch 1150, Loss: 0.4082656130194664, Final Batch Loss: 0.0917036309838295\n",
      "Epoch 1151, Loss: 0.412568062543869, Final Batch Loss: 0.11752040684223175\n",
      "Epoch 1152, Loss: 0.4110855832695961, Final Batch Loss: 0.1434904783964157\n",
      "Epoch 1153, Loss: 0.4068569466471672, Final Batch Loss: 0.18313559889793396\n",
      "Epoch 1154, Loss: 0.40035321563482285, Final Batch Loss: 0.10090523213148117\n",
      "Epoch 1155, Loss: 0.3943474069237709, Final Batch Loss: 0.1371956318616867\n",
      "Epoch 1156, Loss: 0.2739611715078354, Final Batch Loss: 0.06113595515489578\n",
      "Epoch 1157, Loss: 0.332710437476635, Final Batch Loss: 0.11272110044956207\n",
      "Epoch 1158, Loss: 0.30623824149370193, Final Batch Loss: 0.09271275997161865\n",
      "Epoch 1159, Loss: 0.42256222665309906, Final Batch Loss: 0.13936316967010498\n",
      "Epoch 1160, Loss: 0.4705984592437744, Final Batch Loss: 0.1680586338043213\n",
      "Epoch 1161, Loss: 0.4537540376186371, Final Batch Loss: 0.20430037379264832\n",
      "Epoch 1162, Loss: 0.5211331844329834, Final Batch Loss: 0.13095414638519287\n",
      "Epoch 1163, Loss: 0.4366869777441025, Final Batch Loss: 0.08875793218612671\n",
      "Epoch 1164, Loss: 0.46495720744132996, Final Batch Loss: 0.17142535746097565\n",
      "Epoch 1165, Loss: 0.4890556335449219, Final Batch Loss: 0.15432685613632202\n",
      "Epoch 1166, Loss: 0.47766150534152985, Final Batch Loss: 0.1723061352968216\n",
      "Epoch 1167, Loss: 0.4560391306877136, Final Batch Loss: 0.17124538123607635\n",
      "Epoch 1168, Loss: 0.4201880097389221, Final Batch Loss: 0.1268356293439865\n",
      "Epoch 1169, Loss: 0.3933699131011963, Final Batch Loss: 0.1055336594581604\n",
      "Epoch 1170, Loss: 0.44141505658626556, Final Batch Loss: 0.16641025245189667\n",
      "Epoch 1171, Loss: 0.4712100327014923, Final Batch Loss: 0.1859494149684906\n",
      "Epoch 1172, Loss: 0.6020402610301971, Final Batch Loss: 0.25827261805534363\n",
      "Epoch 1173, Loss: 0.3813229575753212, Final Batch Loss: 0.10128364711999893\n",
      "Epoch 1174, Loss: 0.5565826818346977, Final Batch Loss: 0.2832888662815094\n",
      "Epoch 1175, Loss: 0.31886621564626694, Final Batch Loss: 0.08382194489240646\n",
      "Epoch 1176, Loss: 0.3854895234107971, Final Batch Loss: 0.1416047215461731\n",
      "Epoch 1177, Loss: 0.35852085798978806, Final Batch Loss: 0.10534705966711044\n",
      "Epoch 1178, Loss: 0.3846321776509285, Final Batch Loss: 0.07022909820079803\n",
      "Epoch 1179, Loss: 0.2997049316763878, Final Batch Loss: 0.06345240771770477\n",
      "Epoch 1180, Loss: 0.4706375151872635, Final Batch Loss: 0.12730638682842255\n",
      "Epoch 1181, Loss: 0.3140682503581047, Final Batch Loss: 0.06582081317901611\n",
      "Epoch 1182, Loss: 0.3974284529685974, Final Batch Loss: 0.1527843028306961\n",
      "Epoch 1183, Loss: 0.3816564902663231, Final Batch Loss: 0.15140686929225922\n",
      "Epoch 1184, Loss: 0.3263501897454262, Final Batch Loss: 0.06916756927967072\n",
      "Epoch 1185, Loss: 0.37038810551166534, Final Batch Loss: 0.16655278205871582\n",
      "Epoch 1186, Loss: 0.29075248539447784, Final Batch Loss: 0.07350575923919678\n",
      "Epoch 1187, Loss: 0.3551393151283264, Final Batch Loss: 0.10097906738519669\n",
      "Epoch 1188, Loss: 0.3306349217891693, Final Batch Loss: 0.09114338457584381\n",
      "Epoch 1189, Loss: 0.3644179701805115, Final Batch Loss: 0.13719959557056427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1190, Loss: 0.3864392936229706, Final Batch Loss: 0.08336356282234192\n",
      "Epoch 1191, Loss: 0.42163917422294617, Final Batch Loss: 0.15004871785640717\n",
      "Epoch 1192, Loss: 0.44700218737125397, Final Batch Loss: 0.11503034830093384\n",
      "Epoch 1193, Loss: 0.4194766581058502, Final Batch Loss: 0.08154062926769257\n",
      "Epoch 1194, Loss: 0.48609940707683563, Final Batch Loss: 0.24458259344100952\n",
      "Epoch 1195, Loss: 0.5076680406928062, Final Batch Loss: 0.22333474457263947\n",
      "Epoch 1196, Loss: 0.403497114777565, Final Batch Loss: 0.13781926035881042\n",
      "Epoch 1197, Loss: 0.38038428872823715, Final Batch Loss: 0.15303243696689606\n",
      "Epoch 1198, Loss: 0.34176746010780334, Final Batch Loss: 0.0938619077205658\n",
      "Epoch 1199, Loss: 0.36189041286706924, Final Batch Loss: 0.11239384859800339\n",
      "Epoch 1200, Loss: 0.4708366394042969, Final Batch Loss: 0.20782634615898132\n",
      "Epoch 1201, Loss: 0.3019945062696934, Final Batch Loss: 0.0394413135945797\n",
      "Epoch 1202, Loss: 0.3798021599650383, Final Batch Loss: 0.1316203624010086\n",
      "Epoch 1203, Loss: 0.40159910917282104, Final Batch Loss: 0.07848630845546722\n",
      "Epoch 1204, Loss: 0.4654393196105957, Final Batch Loss: 0.1684292107820511\n",
      "Epoch 1205, Loss: 0.34944768995046616, Final Batch Loss: 0.08498657494783401\n",
      "Epoch 1206, Loss: 0.31312618777155876, Final Batch Loss: 0.04280019924044609\n",
      "Epoch 1207, Loss: 0.37170322239398956, Final Batch Loss: 0.10138816386461258\n",
      "Epoch 1208, Loss: 0.4489300698041916, Final Batch Loss: 0.2109077423810959\n",
      "Epoch 1209, Loss: 0.3923433944582939, Final Batch Loss: 0.1424752175807953\n",
      "Epoch 1210, Loss: 0.4319540783762932, Final Batch Loss: 0.20717863738536835\n",
      "Epoch 1211, Loss: 0.360441192984581, Final Batch Loss: 0.08945442736148834\n",
      "Epoch 1212, Loss: 0.29932204633951187, Final Batch Loss: 0.058884523808956146\n",
      "Epoch 1213, Loss: 0.4853202700614929, Final Batch Loss: 0.07578502595424652\n",
      "Epoch 1214, Loss: 0.30923688411712646, Final Batch Loss: 0.04802478849887848\n",
      "Epoch 1215, Loss: 0.3987829089164734, Final Batch Loss: 0.11301256716251373\n",
      "Epoch 1216, Loss: 0.46914923936128616, Final Batch Loss: 0.25140634179115295\n",
      "Epoch 1217, Loss: 0.42009518295526505, Final Batch Loss: 0.18467633426189423\n",
      "Epoch 1218, Loss: 0.49730972200632095, Final Batch Loss: 0.23849382996559143\n",
      "Epoch 1219, Loss: 0.43904367834329605, Final Batch Loss: 0.1913074553012848\n",
      "Epoch 1220, Loss: 0.34817924350500107, Final Batch Loss: 0.12050516158342361\n",
      "Epoch 1221, Loss: 0.46192483603954315, Final Batch Loss: 0.20662719011306763\n",
      "Epoch 1222, Loss: 0.32325736060738564, Final Batch Loss: 0.06137050315737724\n",
      "Epoch 1223, Loss: 0.4307815879583359, Final Batch Loss: 0.13700006902217865\n",
      "Epoch 1224, Loss: 0.38034676015377045, Final Batch Loss: 0.134685680270195\n",
      "Epoch 1225, Loss: 0.3737177923321724, Final Batch Loss: 0.12136531621217728\n",
      "Epoch 1226, Loss: 0.3677341267466545, Final Batch Loss: 0.10536238551139832\n",
      "Epoch 1227, Loss: 0.40883001685142517, Final Batch Loss: 0.16353300213813782\n",
      "Epoch 1228, Loss: 0.41125985980033875, Final Batch Loss: 0.1566438227891922\n",
      "Epoch 1229, Loss: 0.5550678968429565, Final Batch Loss: 0.31729593873023987\n",
      "Epoch 1230, Loss: 0.3269577696919441, Final Batch Loss: 0.08906180411577225\n",
      "Epoch 1231, Loss: 0.35908234864473343, Final Batch Loss: 0.10388973355293274\n",
      "Epoch 1232, Loss: 0.42354851961135864, Final Batch Loss: 0.138352632522583\n",
      "Epoch 1233, Loss: 0.3492431119084358, Final Batch Loss: 0.10570492595434189\n",
      "Epoch 1234, Loss: 0.3376837223768234, Final Batch Loss: 0.10297661274671555\n",
      "Epoch 1235, Loss: 0.4271465614438057, Final Batch Loss: 0.20082314312458038\n",
      "Epoch 1236, Loss: 0.3400473967194557, Final Batch Loss: 0.11136198788881302\n",
      "Epoch 1237, Loss: 0.3074411600828171, Final Batch Loss: 0.08349012583494186\n",
      "Epoch 1238, Loss: 0.3556577116250992, Final Batch Loss: 0.10837891697883606\n",
      "Epoch 1239, Loss: 0.28536834567785263, Final Batch Loss: 0.08335535228252411\n",
      "Epoch 1240, Loss: 0.3957231789827347, Final Batch Loss: 0.14937300980091095\n",
      "Epoch 1241, Loss: 0.46427131444215775, Final Batch Loss: 0.24814839661121368\n",
      "Epoch 1242, Loss: 0.37373070418834686, Final Batch Loss: 0.1517014503479004\n",
      "Epoch 1243, Loss: 0.3140459209680557, Final Batch Loss: 0.07520593702793121\n",
      "Epoch 1244, Loss: 0.3464820832014084, Final Batch Loss: 0.097295381128788\n",
      "Epoch 1245, Loss: 0.32567986100912094, Final Batch Loss: 0.07615985721349716\n",
      "Epoch 1246, Loss: 0.25578878819942474, Final Batch Loss: 0.032730892300605774\n",
      "Epoch 1247, Loss: 0.3720850497484207, Final Batch Loss: 0.13004352152347565\n",
      "Epoch 1248, Loss: 0.3792359158396721, Final Batch Loss: 0.1374494731426239\n",
      "Epoch 1249, Loss: 0.26064982265233994, Final Batch Loss: 0.04601152241230011\n",
      "Epoch 1250, Loss: 0.30481088906526566, Final Batch Loss: 0.08726541697978973\n",
      "Epoch 1251, Loss: 0.3740372285246849, Final Batch Loss: 0.1153416857123375\n",
      "Epoch 1252, Loss: 0.3400013744831085, Final Batch Loss: 0.07438860088586807\n",
      "Epoch 1253, Loss: 0.3339719995856285, Final Batch Loss: 0.09571607410907745\n",
      "Epoch 1254, Loss: 0.37709079682826996, Final Batch Loss: 0.1471141278743744\n",
      "Epoch 1255, Loss: 0.40359751135110855, Final Batch Loss: 0.16463221609592438\n",
      "Epoch 1256, Loss: 0.35557180643081665, Final Batch Loss: 0.13907083868980408\n",
      "Epoch 1257, Loss: 0.3471435010433197, Final Batch Loss: 0.07683488726615906\n",
      "Epoch 1258, Loss: 0.2461727224290371, Final Batch Loss: 0.032655101269483566\n",
      "Epoch 1259, Loss: 0.3696983605623245, Final Batch Loss: 0.13011017441749573\n",
      "Epoch 1260, Loss: 0.33836057782173157, Final Batch Loss: 0.10119054466485977\n",
      "Epoch 1261, Loss: 0.3223867565393448, Final Batch Loss: 0.0678274855017662\n",
      "Epoch 1262, Loss: 0.3223571330308914, Final Batch Loss: 0.11235181987285614\n",
      "Epoch 1263, Loss: 0.32189977914094925, Final Batch Loss: 0.07899732142686844\n",
      "Epoch 1264, Loss: 0.3921555429697037, Final Batch Loss: 0.12029974162578583\n",
      "Epoch 1265, Loss: 0.33258239179849625, Final Batch Loss: 0.07294823229312897\n",
      "Epoch 1266, Loss: 0.33778011053800583, Final Batch Loss: 0.08187176287174225\n",
      "Epoch 1267, Loss: 0.43229294568300247, Final Batch Loss: 0.1696842759847641\n",
      "Epoch 1268, Loss: 0.42368151247501373, Final Batch Loss: 0.22377048432826996\n",
      "Epoch 1269, Loss: 0.337335929274559, Final Batch Loss: 0.14093008637428284\n",
      "Epoch 1270, Loss: 0.38392728567123413, Final Batch Loss: 0.17180278897285461\n",
      "Epoch 1271, Loss: 0.33225246518850327, Final Batch Loss: 0.11120452731847763\n",
      "Epoch 1272, Loss: 0.3768039792776108, Final Batch Loss: 0.16799940168857574\n",
      "Epoch 1273, Loss: 0.3132524937391281, Final Batch Loss: 0.09125951677560806\n",
      "Epoch 1274, Loss: 0.3422854244709015, Final Batch Loss: 0.140476793050766\n",
      "Epoch 1275, Loss: 0.26040973141789436, Final Batch Loss: 0.044954027980566025\n",
      "Epoch 1276, Loss: 0.32033000886440277, Final Batch Loss: 0.0851469486951828\n",
      "Epoch 1277, Loss: 0.5280768796801567, Final Batch Loss: 0.28396621346473694\n",
      "Epoch 1278, Loss: 0.3851068764925003, Final Batch Loss: 0.09392806887626648\n",
      "Epoch 1279, Loss: 0.32906924188137054, Final Batch Loss: 0.0859316736459732\n",
      "Epoch 1280, Loss: 0.46753090620040894, Final Batch Loss: 0.16277556121349335\n",
      "Epoch 1281, Loss: 0.5343666672706604, Final Batch Loss: 0.30423790216445923\n",
      "Epoch 1282, Loss: 0.38088005781173706, Final Batch Loss: 0.10625113546848297\n",
      "Epoch 1283, Loss: 0.3909735158085823, Final Batch Loss: 0.15328626334667206\n",
      "Epoch 1284, Loss: 0.6085586994886398, Final Batch Loss: 0.419231653213501\n",
      "Epoch 1285, Loss: 0.3936465308070183, Final Batch Loss: 0.17854681611061096\n",
      "Epoch 1286, Loss: 0.4615221545100212, Final Batch Loss: 0.0937306359410286\n",
      "Epoch 1287, Loss: 0.33564530313014984, Final Batch Loss: 0.09704174101352692\n",
      "Epoch 1288, Loss: 0.4588596001267433, Final Batch Loss: 0.19688346982002258\n",
      "Epoch 1289, Loss: 0.33203529566526413, Final Batch Loss: 0.11122821271419525\n",
      "Epoch 1290, Loss: 0.5010267123579979, Final Batch Loss: 0.27514806389808655\n",
      "Epoch 1291, Loss: 0.4134836122393608, Final Batch Loss: 0.20765839517116547\n",
      "Epoch 1292, Loss: 0.5110724344849586, Final Batch Loss: 0.2719242572784424\n",
      "Epoch 1293, Loss: 0.26336112432181835, Final Batch Loss: 0.023016495630145073\n",
      "Epoch 1294, Loss: 0.44089871644973755, Final Batch Loss: 0.1597922146320343\n",
      "Epoch 1295, Loss: 0.3352779597043991, Final Batch Loss: 0.10692625492811203\n",
      "Epoch 1296, Loss: 0.392364464700222, Final Batch Loss: 0.11222321540117264\n",
      "Epoch 1297, Loss: 0.4263211339712143, Final Batch Loss: 0.1135326623916626\n",
      "Epoch 1298, Loss: 0.28818345814943314, Final Batch Loss: 0.07596446573734283\n",
      "Epoch 1299, Loss: 0.39237820357084274, Final Batch Loss: 0.15684297680854797\n",
      "Epoch 1300, Loss: 0.3325636088848114, Final Batch Loss: 0.11713185906410217\n",
      "Epoch 1301, Loss: 0.2931375056505203, Final Batch Loss: 0.07938975095748901\n",
      "Epoch 1302, Loss: 0.29233622550964355, Final Batch Loss: 0.05514955520629883\n",
      "Epoch 1303, Loss: 0.3775842860341072, Final Batch Loss: 0.14458250999450684\n",
      "Epoch 1304, Loss: 0.37321043014526367, Final Batch Loss: 0.13368545472621918\n",
      "Epoch 1305, Loss: 0.6126086413860321, Final Batch Loss: 0.3697032034397125\n",
      "Epoch 1306, Loss: 0.6695487946271896, Final Batch Loss: 0.45544758439064026\n",
      "Epoch 1307, Loss: 0.37909039855003357, Final Batch Loss: 0.14374159276485443\n",
      "Epoch 1308, Loss: 0.3523770645260811, Final Batch Loss: 0.08926000446081161\n",
      "Epoch 1309, Loss: 0.32646460831165314, Final Batch Loss: 0.132956862449646\n",
      "Epoch 1310, Loss: 0.355328306555748, Final Batch Loss: 0.11394031345844269\n",
      "Epoch 1311, Loss: 0.4373045265674591, Final Batch Loss: 0.245524600148201\n",
      "Epoch 1312, Loss: 0.32929128408432007, Final Batch Loss: 0.10769630968570709\n",
      "Epoch 1313, Loss: 0.3767279237508774, Final Batch Loss: 0.13704915344715118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1314, Loss: 0.3019815534353256, Final Batch Loss: 0.06896735727787018\n",
      "Epoch 1315, Loss: 0.5063342973589897, Final Batch Loss: 0.2925243675708771\n",
      "Epoch 1316, Loss: 0.30690669640898705, Final Batch Loss: 0.035299692302942276\n",
      "Epoch 1317, Loss: 0.46557585895061493, Final Batch Loss: 0.11164352297782898\n",
      "Epoch 1318, Loss: 0.45985330641269684, Final Batch Loss: 0.16596117615699768\n",
      "Epoch 1319, Loss: 0.39288168400526047, Final Batch Loss: 0.13127225637435913\n",
      "Epoch 1320, Loss: 0.31872690469026566, Final Batch Loss: 0.08131466805934906\n",
      "Epoch 1321, Loss: 0.4485913887619972, Final Batch Loss: 0.2257751226425171\n",
      "Epoch 1322, Loss: 0.3746313825249672, Final Batch Loss: 0.10731193423271179\n",
      "Epoch 1323, Loss: 0.4055466428399086, Final Batch Loss: 0.19410310685634613\n",
      "Epoch 1324, Loss: 0.3614419512450695, Final Batch Loss: 0.09968691319227219\n",
      "Epoch 1325, Loss: 0.4118941128253937, Final Batch Loss: 0.15135794878005981\n",
      "Epoch 1326, Loss: 0.3975658640265465, Final Batch Loss: 0.18150444328784943\n",
      "Epoch 1327, Loss: 0.32788344845175743, Final Batch Loss: 0.060992587357759476\n",
      "Epoch 1328, Loss: 0.34121617674827576, Final Batch Loss: 0.06797957420349121\n",
      "Epoch 1329, Loss: 0.39671020954847336, Final Batch Loss: 0.1958654224872589\n",
      "Epoch 1330, Loss: 0.36960913985967636, Final Batch Loss: 0.10228501260280609\n",
      "Epoch 1331, Loss: 0.33615946769714355, Final Batch Loss: 0.08223747462034225\n",
      "Epoch 1332, Loss: 0.2771269828081131, Final Batch Loss: 0.06814727932214737\n",
      "Epoch 1333, Loss: 0.4103527069091797, Final Batch Loss: 0.21241627633571625\n",
      "Epoch 1334, Loss: 0.29526853561401367, Final Batch Loss: 0.10178392380475998\n",
      "Epoch 1335, Loss: 0.37932586669921875, Final Batch Loss: 0.15196798741817474\n",
      "Epoch 1336, Loss: 0.38724086433649063, Final Batch Loss: 0.19601210951805115\n",
      "Epoch 1337, Loss: 0.36222875863313675, Final Batch Loss: 0.15019084513187408\n",
      "Epoch 1338, Loss: 0.3180680274963379, Final Batch Loss: 0.09959713369607925\n",
      "Epoch 1339, Loss: 0.30544374138116837, Final Batch Loss: 0.060423627495765686\n",
      "Epoch 1340, Loss: 0.3922398090362549, Final Batch Loss: 0.1828475445508957\n",
      "Epoch 1341, Loss: 0.3540610447525978, Final Batch Loss: 0.11145561188459396\n",
      "Epoch 1342, Loss: 0.3926708251237869, Final Batch Loss: 0.14773768186569214\n",
      "Epoch 1343, Loss: 0.36127113550901413, Final Batch Loss: 0.13319075107574463\n",
      "Epoch 1344, Loss: 0.33947262167930603, Final Batch Loss: 0.11956886947154999\n",
      "Epoch 1345, Loss: 0.39918624609708786, Final Batch Loss: 0.12180187553167343\n",
      "Epoch 1346, Loss: 0.3475147783756256, Final Batch Loss: 0.10445725917816162\n",
      "Epoch 1347, Loss: 0.40958499163389206, Final Batch Loss: 0.20794062316417694\n",
      "Epoch 1348, Loss: 0.5345900431275368, Final Batch Loss: 0.27557554841041565\n",
      "Epoch 1349, Loss: 0.2650792673230171, Final Batch Loss: 0.09449253976345062\n",
      "Epoch 1350, Loss: 0.34287603199481964, Final Batch Loss: 0.08108946681022644\n",
      "Epoch 1351, Loss: 0.36706846952438354, Final Batch Loss: 0.13261070847511292\n",
      "Epoch 1352, Loss: 0.3119514435529709, Final Batch Loss: 0.10970117896795273\n",
      "Epoch 1353, Loss: 0.2731487788259983, Final Batch Loss: 0.05191070958971977\n",
      "Epoch 1354, Loss: 0.322641484439373, Final Batch Loss: 0.10560350865125656\n",
      "Epoch 1355, Loss: 0.31807541847229004, Final Batch Loss: 0.07864420115947723\n",
      "Epoch 1356, Loss: 0.3356657102704048, Final Batch Loss: 0.03970950096845627\n",
      "Epoch 1357, Loss: 0.3300814554095268, Final Batch Loss: 0.12762434780597687\n",
      "Epoch 1358, Loss: 0.32333483546972275, Final Batch Loss: 0.096729576587677\n",
      "Epoch 1359, Loss: 0.29984234273433685, Final Batch Loss: 0.06467415392398834\n",
      "Epoch 1360, Loss: 0.48252008110284805, Final Batch Loss: 0.28502157330513\n",
      "Epoch 1361, Loss: 0.3615255132317543, Final Batch Loss: 0.15615758299827576\n",
      "Epoch 1362, Loss: 0.28889013826847076, Final Batch Loss: 0.0842394158244133\n",
      "Epoch 1363, Loss: 0.3034142330288887, Final Batch Loss: 0.11910585314035416\n",
      "Epoch 1364, Loss: 0.40610427409410477, Final Batch Loss: 0.14657272398471832\n",
      "Epoch 1365, Loss: 0.42344358563423157, Final Batch Loss: 0.1753845363855362\n",
      "Epoch 1366, Loss: 0.4494405537843704, Final Batch Loss: 0.21965444087982178\n",
      "Epoch 1367, Loss: 0.4900503382086754, Final Batch Loss: 0.27510300278663635\n",
      "Epoch 1368, Loss: 0.3083491623401642, Final Batch Loss: 0.06930052489042282\n",
      "Epoch 1369, Loss: 0.40253928303718567, Final Batch Loss: 0.19068777561187744\n",
      "Epoch 1370, Loss: 0.35281282663345337, Final Batch Loss: 0.10122386366128922\n",
      "Epoch 1371, Loss: 0.29428812861442566, Final Batch Loss: 0.06962243467569351\n",
      "Epoch 1372, Loss: 0.4634476527571678, Final Batch Loss: 0.21854178607463837\n",
      "Epoch 1373, Loss: 0.345609575510025, Final Batch Loss: 0.08430414646863937\n",
      "Epoch 1374, Loss: 0.28008993715047836, Final Batch Loss: 0.07010942697525024\n",
      "Epoch 1375, Loss: 0.440047487616539, Final Batch Loss: 0.1867210566997528\n",
      "Epoch 1376, Loss: 0.34888210892677307, Final Batch Loss: 0.1309497058391571\n",
      "Epoch 1377, Loss: 0.3866880536079407, Final Batch Loss: 0.10368742048740387\n",
      "Epoch 1378, Loss: 0.3798484280705452, Final Batch Loss: 0.12887567281723022\n",
      "Epoch 1379, Loss: 0.29482677578926086, Final Batch Loss: 0.049608536064624786\n",
      "Epoch 1380, Loss: 0.3079136162996292, Final Batch Loss: 0.09036918729543686\n",
      "Epoch 1381, Loss: 0.49389324337244034, Final Batch Loss: 0.2845867872238159\n",
      "Epoch 1382, Loss: 0.4282371625304222, Final Batch Loss: 0.16376253962516785\n",
      "Epoch 1383, Loss: 0.2926790937781334, Final Batch Loss: 0.057931460440158844\n",
      "Epoch 1384, Loss: 0.3658255413174629, Final Batch Loss: 0.10607317090034485\n",
      "Epoch 1385, Loss: 0.3764830306172371, Final Batch Loss: 0.1489144265651703\n",
      "Epoch 1386, Loss: 0.3149729073047638, Final Batch Loss: 0.08691658079624176\n",
      "Epoch 1387, Loss: 0.37891510128974915, Final Batch Loss: 0.08856746554374695\n",
      "Epoch 1388, Loss: 0.3078051060438156, Final Batch Loss: 0.0954766646027565\n",
      "Epoch 1389, Loss: 0.40981895476579666, Final Batch Loss: 0.2115359902381897\n",
      "Epoch 1390, Loss: 0.30930699221789837, Final Batch Loss: 0.02887008897960186\n",
      "Epoch 1391, Loss: 0.3995589315891266, Final Batch Loss: 0.17027217149734497\n",
      "Epoch 1392, Loss: 0.2944086417555809, Final Batch Loss: 0.09263143688440323\n",
      "Epoch 1393, Loss: 0.3207503631711006, Final Batch Loss: 0.09392262250185013\n",
      "Epoch 1394, Loss: 0.3105296194553375, Final Batch Loss: 0.10385597497224808\n",
      "Epoch 1395, Loss: 0.3657723590731621, Final Batch Loss: 0.09666182845830917\n",
      "Epoch 1396, Loss: 0.2849167138338089, Final Batch Loss: 0.07865426689386368\n",
      "Epoch 1397, Loss: 0.3644518777728081, Final Batch Loss: 0.13877154886722565\n",
      "Epoch 1398, Loss: 0.3060771133750677, Final Batch Loss: 0.02800438366830349\n",
      "Epoch 1399, Loss: 0.3473653942346573, Final Batch Loss: 0.14040632545948029\n",
      "Epoch 1400, Loss: 0.3380471393465996, Final Batch Loss: 0.10455043613910675\n",
      "Epoch 1401, Loss: 0.3166475221514702, Final Batch Loss: 0.10165265947580338\n",
      "Epoch 1402, Loss: 0.3789527043700218, Final Batch Loss: 0.17859897017478943\n",
      "Epoch 1403, Loss: 0.31623993813991547, Final Batch Loss: 0.05506128817796707\n",
      "Epoch 1404, Loss: 0.3302830010652542, Final Batch Loss: 0.10681804269552231\n",
      "Epoch 1405, Loss: 0.26821302622556686, Final Batch Loss: 0.06046789139509201\n",
      "Epoch 1406, Loss: 0.3106973245739937, Final Batch Loss: 0.0889153778553009\n",
      "Epoch 1407, Loss: 0.24653445929288864, Final Batch Loss: 0.045642271637916565\n",
      "Epoch 1408, Loss: 0.4795420244336128, Final Batch Loss: 0.20939135551452637\n",
      "Epoch 1409, Loss: 0.38052187860012054, Final Batch Loss: 0.17556068301200867\n",
      "Epoch 1410, Loss: 0.30561166256666183, Final Batch Loss: 0.10277201235294342\n",
      "Epoch 1411, Loss: 0.3132818713784218, Final Batch Loss: 0.10075563192367554\n",
      "Epoch 1412, Loss: 0.5458621010184288, Final Batch Loss: 0.2845616936683655\n",
      "Epoch 1413, Loss: 0.41729529201984406, Final Batch Loss: 0.12304041534662247\n",
      "Epoch 1414, Loss: 0.306530624628067, Final Batch Loss: 0.061386361718177795\n",
      "Epoch 1415, Loss: 0.3002804070711136, Final Batch Loss: 0.1110004261136055\n",
      "Epoch 1416, Loss: 0.46458569914102554, Final Batch Loss: 0.25604286789894104\n",
      "Epoch 1417, Loss: 0.5144829899072647, Final Batch Loss: 0.2596856951713562\n",
      "Epoch 1418, Loss: 0.26516854763031006, Final Batch Loss: 0.04134032875299454\n",
      "Epoch 1419, Loss: 0.4442712664604187, Final Batch Loss: 0.19123688340187073\n",
      "Epoch 1420, Loss: 0.42986251413822174, Final Batch Loss: 0.21908718347549438\n",
      "Epoch 1421, Loss: 0.34873513132333755, Final Batch Loss: 0.11848586052656174\n",
      "Epoch 1422, Loss: 0.3154153451323509, Final Batch Loss: 0.1179376021027565\n",
      "Epoch 1423, Loss: 0.3810376599431038, Final Batch Loss: 0.11694871634244919\n",
      "Epoch 1424, Loss: 0.48785676807165146, Final Batch Loss: 0.24104346334934235\n",
      "Epoch 1425, Loss: 0.2923775762319565, Final Batch Loss: 0.08493070304393768\n",
      "Epoch 1426, Loss: 0.3969605192542076, Final Batch Loss: 0.14745773375034332\n",
      "Epoch 1427, Loss: 0.35344649106264114, Final Batch Loss: 0.12728522717952728\n",
      "Epoch 1428, Loss: 0.3537168577313423, Final Batch Loss: 0.12325450778007507\n",
      "Epoch 1429, Loss: 0.37518396228551865, Final Batch Loss: 0.20290507376194\n",
      "Epoch 1430, Loss: 0.33917470276355743, Final Batch Loss: 0.1259227693080902\n",
      "Epoch 1431, Loss: 0.2903508432209492, Final Batch Loss: 0.057421404868364334\n",
      "Epoch 1432, Loss: 0.28858673572540283, Final Batch Loss: 0.08471530675888062\n",
      "Epoch 1433, Loss: 0.2820783145725727, Final Batch Loss: 0.05469285324215889\n",
      "Epoch 1434, Loss: 0.28248993307352066, Final Batch Loss: 0.06156306713819504\n",
      "Epoch 1435, Loss: 0.3375879228115082, Final Batch Loss: 0.11809522658586502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1436, Loss: 0.3425758555531502, Final Batch Loss: 0.13563308119773865\n",
      "Epoch 1437, Loss: 0.29805128276348114, Final Batch Loss: 0.08270218968391418\n",
      "Epoch 1438, Loss: 0.3546149432659149, Final Batch Loss: 0.1524503529071808\n",
      "Epoch 1439, Loss: 0.3311871364712715, Final Batch Loss: 0.15555478632450104\n",
      "Epoch 1440, Loss: 0.35957983136177063, Final Batch Loss: 0.06984935700893402\n",
      "Epoch 1441, Loss: 0.2710200995206833, Final Batch Loss: 0.06500855088233948\n",
      "Epoch 1442, Loss: 0.33863507211208344, Final Batch Loss: 0.14523018896579742\n",
      "Epoch 1443, Loss: 0.3721437305212021, Final Batch Loss: 0.15114319324493408\n",
      "Epoch 1444, Loss: 0.46208057552576065, Final Batch Loss: 0.24684736132621765\n",
      "Epoch 1445, Loss: 0.2604212202131748, Final Batch Loss: 0.04076465591788292\n",
      "Epoch 1446, Loss: 0.3721156045794487, Final Batch Loss: 0.1560569405555725\n",
      "Epoch 1447, Loss: 0.2989102154970169, Final Batch Loss: 0.07493564486503601\n",
      "Epoch 1448, Loss: 0.3556099757552147, Final Batch Loss: 0.1119878888130188\n",
      "Epoch 1449, Loss: 0.32135815918445587, Final Batch Loss: 0.13054732978343964\n",
      "Epoch 1450, Loss: 0.3255724310874939, Final Batch Loss: 0.07613284140825272\n",
      "Epoch 1451, Loss: 0.35722896456718445, Final Batch Loss: 0.1311345249414444\n",
      "Epoch 1452, Loss: 0.3353399857878685, Final Batch Loss: 0.09224400669336319\n",
      "Epoch 1453, Loss: 0.28931745886802673, Final Batch Loss: 0.10511507838964462\n",
      "Epoch 1454, Loss: 0.2954059913754463, Final Batch Loss: 0.08140040189027786\n",
      "Epoch 1455, Loss: 0.2667434774339199, Final Batch Loss: 0.0574142225086689\n",
      "Epoch 1456, Loss: 0.24379487335681915, Final Batch Loss: 0.040079519152641296\n",
      "Epoch 1457, Loss: 0.2846953719854355, Final Batch Loss: 0.09708251059055328\n",
      "Epoch 1458, Loss: 0.35010921210050583, Final Batch Loss: 0.1524571031332016\n",
      "Epoch 1459, Loss: 0.2792936637997627, Final Batch Loss: 0.0632409080862999\n",
      "Epoch 1460, Loss: 0.3297503665089607, Final Batch Loss: 0.11620044708251953\n",
      "Epoch 1461, Loss: 0.3707141801714897, Final Batch Loss: 0.1432047039270401\n",
      "Epoch 1462, Loss: 0.25067524798214436, Final Batch Loss: 0.028594331815838814\n",
      "Epoch 1463, Loss: 0.36950066685676575, Final Batch Loss: 0.1496206820011139\n",
      "Epoch 1464, Loss: 0.2979310564696789, Final Batch Loss: 0.04970766231417656\n",
      "Epoch 1465, Loss: 0.29067497700452805, Final Batch Loss: 0.06883742660284042\n",
      "Epoch 1466, Loss: 0.4280412420630455, Final Batch Loss: 0.16000616550445557\n",
      "Epoch 1467, Loss: 0.4211396649479866, Final Batch Loss: 0.164664626121521\n",
      "Epoch 1468, Loss: 0.2524654418230057, Final Batch Loss: 0.03720654547214508\n",
      "Epoch 1469, Loss: 0.34311460703611374, Final Batch Loss: 0.1305055469274521\n",
      "Epoch 1470, Loss: 0.49412911385297775, Final Batch Loss: 0.2565995752811432\n",
      "Epoch 1471, Loss: 0.27003202587366104, Final Batch Loss: 0.06799665838479996\n",
      "Epoch 1472, Loss: 0.29949766024947166, Final Batch Loss: 0.055192913860082626\n",
      "Epoch 1473, Loss: 0.3658172935247421, Final Batch Loss: 0.1413038969039917\n",
      "Epoch 1474, Loss: 0.30489441752433777, Final Batch Loss: 0.08326420933008194\n",
      "Epoch 1475, Loss: 0.2742730900645256, Final Batch Loss: 0.07382988184690475\n",
      "Epoch 1476, Loss: 0.28582434356212616, Final Batch Loss: 0.050013452768325806\n",
      "Epoch 1477, Loss: 0.35549742728471756, Final Batch Loss: 0.1423066109418869\n",
      "Epoch 1478, Loss: 0.3005896769464016, Final Batch Loss: 0.0450623445212841\n",
      "Epoch 1479, Loss: 0.2886897101998329, Final Batch Loss: 0.09263276308774948\n",
      "Epoch 1480, Loss: 0.32363883405923843, Final Batch Loss: 0.07297279685735703\n",
      "Epoch 1481, Loss: 0.28472211956977844, Final Batch Loss: 0.04875362664461136\n",
      "Epoch 1482, Loss: 0.23300039768218994, Final Batch Loss: 0.09135279059410095\n",
      "Epoch 1483, Loss: 0.3164643570780754, Final Batch Loss: 0.080323226749897\n",
      "Epoch 1484, Loss: 0.39276984333992004, Final Batch Loss: 0.1637500375509262\n",
      "Epoch 1485, Loss: 0.3937646225094795, Final Batch Loss: 0.1625896543264389\n",
      "Epoch 1486, Loss: 0.3104540854692459, Final Batch Loss: 0.10162480920553207\n",
      "Epoch 1487, Loss: 0.36990227550268173, Final Batch Loss: 0.16468633711338043\n",
      "Epoch 1488, Loss: 0.29042498767375946, Final Batch Loss: 0.08799364417791367\n",
      "Epoch 1489, Loss: 0.35137564688920975, Final Batch Loss: 0.12224186956882477\n",
      "Epoch 1490, Loss: 0.4459458664059639, Final Batch Loss: 0.18880335986614227\n",
      "Epoch 1491, Loss: 0.5201809033751488, Final Batch Loss: 0.27979642152786255\n",
      "Epoch 1492, Loss: 0.3228533938527107, Final Batch Loss: 0.11805441230535507\n",
      "Epoch 1493, Loss: 0.31768207252025604, Final Batch Loss: 0.0902198925614357\n",
      "Epoch 1494, Loss: 0.25773119926452637, Final Batch Loss: 0.049522772431373596\n",
      "Epoch 1495, Loss: 0.2913566678762436, Final Batch Loss: 0.09757225960493088\n",
      "Epoch 1496, Loss: 0.3621668443083763, Final Batch Loss: 0.06590735167264938\n",
      "Epoch 1497, Loss: 0.3535124659538269, Final Batch Loss: 0.14980673789978027\n",
      "Epoch 1498, Loss: 0.3338901624083519, Final Batch Loss: 0.09717199206352234\n",
      "Epoch 1499, Loss: 0.3354267403483391, Final Batch Loss: 0.0963607057929039\n",
      "Epoch 1500, Loss: 0.3079288974404335, Final Batch Loss: 0.09244309365749359\n",
      "Epoch 1501, Loss: 0.35305220633745193, Final Batch Loss: 0.12097923457622528\n",
      "Epoch 1502, Loss: 0.3666117712855339, Final Batch Loss: 0.15613578259944916\n",
      "Epoch 1503, Loss: 0.2727111242711544, Final Batch Loss: 0.056730564683675766\n",
      "Epoch 1504, Loss: 0.31365177035331726, Final Batch Loss: 0.09754561632871628\n",
      "Epoch 1505, Loss: 0.2988794967532158, Final Batch Loss: 0.12617142498493195\n",
      "Epoch 1506, Loss: 0.33044077455997467, Final Batch Loss: 0.12265271693468094\n",
      "Epoch 1507, Loss: 0.3649350628256798, Final Batch Loss: 0.1518353968858719\n",
      "Epoch 1508, Loss: 0.4489419385790825, Final Batch Loss: 0.2462659329175949\n",
      "Epoch 1509, Loss: 0.28934579342603683, Final Batch Loss: 0.060358926653862\n",
      "Epoch 1510, Loss: 0.31801271438598633, Final Batch Loss: 0.11603496223688126\n",
      "Epoch 1511, Loss: 0.3288953825831413, Final Batch Loss: 0.07458274811506271\n",
      "Epoch 1512, Loss: 0.2417694330215454, Final Batch Loss: 0.03862443566322327\n",
      "Epoch 1513, Loss: 0.28772926330566406, Final Batch Loss: 0.07846154272556305\n",
      "Epoch 1514, Loss: 0.3773445263504982, Final Batch Loss: 0.113841213285923\n",
      "Epoch 1515, Loss: 0.29310207813978195, Final Batch Loss: 0.08942017704248428\n",
      "Epoch 1516, Loss: 0.32673631608486176, Final Batch Loss: 0.11842385679483414\n",
      "Epoch 1517, Loss: 0.3158849775791168, Final Batch Loss: 0.12496139854192734\n",
      "Epoch 1518, Loss: 0.251040767878294, Final Batch Loss: 0.049475718289613724\n",
      "Epoch 1519, Loss: 0.38528812676668167, Final Batch Loss: 0.15723727643489838\n",
      "Epoch 1520, Loss: 0.3039374500513077, Final Batch Loss: 0.1278807818889618\n",
      "Epoch 1521, Loss: 0.320235438644886, Final Batch Loss: 0.06098250299692154\n",
      "Epoch 1522, Loss: 0.22873249650001526, Final Batch Loss: 0.06568285077810287\n",
      "Epoch 1523, Loss: 0.32123346626758575, Final Batch Loss: 0.07211001962423325\n",
      "Epoch 1524, Loss: 0.43937593698501587, Final Batch Loss: 0.25086909532546997\n",
      "Epoch 1525, Loss: 0.39479486644268036, Final Batch Loss: 0.2154712677001953\n",
      "Epoch 1526, Loss: 0.3718329146504402, Final Batch Loss: 0.15710556507110596\n",
      "Epoch 1527, Loss: 0.4201488718390465, Final Batch Loss: 0.19789305329322815\n",
      "Epoch 1528, Loss: 0.33358143270015717, Final Batch Loss: 0.09199308604001999\n",
      "Epoch 1529, Loss: 0.38376233726739883, Final Batch Loss: 0.15249617397785187\n",
      "Epoch 1530, Loss: 0.5013713166117668, Final Batch Loss: 0.28000015020370483\n",
      "Epoch 1531, Loss: 0.3150847628712654, Final Batch Loss: 0.10650231689214706\n",
      "Epoch 1532, Loss: 0.3468034118413925, Final Batch Loss: 0.06663026660680771\n",
      "Epoch 1533, Loss: 0.3478690832853317, Final Batch Loss: 0.12286373972892761\n",
      "Epoch 1534, Loss: 0.25557801127433777, Final Batch Loss: 0.04384654015302658\n",
      "Epoch 1535, Loss: 0.3113189935684204, Final Batch Loss: 0.11404287815093994\n",
      "Epoch 1536, Loss: 0.3049701973795891, Final Batch Loss: 0.12899750471115112\n",
      "Epoch 1537, Loss: 0.44023237377405167, Final Batch Loss: 0.22853009402751923\n",
      "Epoch 1538, Loss: 0.333428755402565, Final Batch Loss: 0.16283880174160004\n",
      "Epoch 1539, Loss: 0.29654739052057266, Final Batch Loss: 0.08586335927248001\n",
      "Epoch 1540, Loss: 0.43626775592565536, Final Batch Loss: 0.22369803488254547\n",
      "Epoch 1541, Loss: 0.39515160024166107, Final Batch Loss: 0.1369742900133133\n",
      "Epoch 1542, Loss: 0.4422750845551491, Final Batch Loss: 0.16416506469249725\n",
      "Epoch 1543, Loss: 0.3237428739666939, Final Batch Loss: 0.09928329288959503\n",
      "Epoch 1544, Loss: 0.41165049374103546, Final Batch Loss: 0.13773146271705627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1545, Loss: 0.44085727632045746, Final Batch Loss: 0.2300540953874588\n",
      "Epoch 1546, Loss: 0.26464758813381195, Final Batch Loss: 0.07485129684209824\n",
      "Epoch 1547, Loss: 0.28306519985198975, Final Batch Loss: 0.07827159017324448\n",
      "Epoch 1548, Loss: 0.3569992780685425, Final Batch Loss: 0.15370608866214752\n",
      "Epoch 1549, Loss: 0.2741916924715042, Final Batch Loss: 0.11153624206781387\n",
      "Epoch 1550, Loss: 0.3961452767252922, Final Batch Loss: 0.1649797260761261\n",
      "Epoch 1551, Loss: 0.3851531893014908, Final Batch Loss: 0.16301757097244263\n",
      "Epoch 1552, Loss: 0.36051470041275024, Final Batch Loss: 0.08646432310342789\n",
      "Epoch 1553, Loss: 0.32245582342147827, Final Batch Loss: 0.10823944211006165\n",
      "Epoch 1554, Loss: 0.31473342329263687, Final Batch Loss: 0.08689279854297638\n",
      "Epoch 1555, Loss: 0.28937504440546036, Final Batch Loss: 0.09610820561647415\n",
      "Epoch 1556, Loss: 0.3480159863829613, Final Batch Loss: 0.08323962241411209\n",
      "Epoch 1557, Loss: 0.4169033542275429, Final Batch Loss: 0.14003853499889374\n",
      "Epoch 1558, Loss: 0.37103578448295593, Final Batch Loss: 0.11585712432861328\n",
      "Epoch 1559, Loss: 0.2921168729662895, Final Batch Loss: 0.0638798251748085\n",
      "Epoch 1560, Loss: 0.43497972935438156, Final Batch Loss: 0.18310317397117615\n",
      "Epoch 1561, Loss: 0.31801700592041016, Final Batch Loss: 0.10264928638935089\n",
      "Epoch 1562, Loss: 0.3438140079379082, Final Batch Loss: 0.08416678011417389\n",
      "Epoch 1563, Loss: 0.505348689854145, Final Batch Loss: 0.25874337553977966\n",
      "Epoch 1564, Loss: 0.34630338847637177, Final Batch Loss: 0.1547972857952118\n",
      "Epoch 1565, Loss: 0.3657241687178612, Final Batch Loss: 0.1624162644147873\n",
      "Epoch 1566, Loss: 0.36513497680425644, Final Batch Loss: 0.12700752913951874\n",
      "Epoch 1567, Loss: 0.344763346016407, Final Batch Loss: 0.07770126312971115\n",
      "Epoch 1568, Loss: 0.2890576049685478, Final Batch Loss: 0.09856997430324554\n",
      "Epoch 1569, Loss: 0.38723234087228775, Final Batch Loss: 0.14614878594875336\n",
      "Epoch 1570, Loss: 0.289831317961216, Final Batch Loss: 0.10294023901224136\n",
      "Epoch 1571, Loss: 0.43110211938619614, Final Batch Loss: 0.20343154668807983\n",
      "Epoch 1572, Loss: 0.26915332302451134, Final Batch Loss: 0.04069479927420616\n",
      "Epoch 1573, Loss: 0.34440571814775467, Final Batch Loss: 0.04910215735435486\n",
      "Epoch 1574, Loss: 0.2625795155763626, Final Batch Loss: 0.04656185209751129\n",
      "Epoch 1575, Loss: 0.33453653007745743, Final Batch Loss: 0.08183299750089645\n",
      "Epoch 1576, Loss: 0.24417505413293839, Final Batch Loss: 0.04162457585334778\n",
      "Epoch 1577, Loss: 0.3329189494252205, Final Batch Loss: 0.14428846538066864\n",
      "Epoch 1578, Loss: 0.3973992168903351, Final Batch Loss: 0.13423189520835876\n",
      "Epoch 1579, Loss: 0.3165167197585106, Final Batch Loss: 0.0661480650305748\n",
      "Epoch 1580, Loss: 0.2772776931524277, Final Batch Loss: 0.09183162450790405\n",
      "Epoch 1581, Loss: 0.26194147765636444, Final Batch Loss: 0.0760565996170044\n",
      "Epoch 1582, Loss: 0.38472285121679306, Final Batch Loss: 0.13692696392536163\n",
      "Epoch 1583, Loss: 0.3036709651350975, Final Batch Loss: 0.07673607766628265\n",
      "Epoch 1584, Loss: 0.3459237813949585, Final Batch Loss: 0.1488160938024521\n",
      "Epoch 1585, Loss: 0.334117129445076, Final Batch Loss: 0.14146926999092102\n",
      "Epoch 1586, Loss: 0.27012310177087784, Final Batch Loss: 0.03376633673906326\n",
      "Epoch 1587, Loss: 0.28780217468738556, Final Batch Loss: 0.07779637724161148\n",
      "Epoch 1588, Loss: 0.34159015864133835, Final Batch Loss: 0.1441899985074997\n",
      "Epoch 1589, Loss: 0.32179904729127884, Final Batch Loss: 0.08201383799314499\n",
      "Epoch 1590, Loss: 0.2491823174059391, Final Batch Loss: 0.061425503343343735\n",
      "Epoch 1591, Loss: 0.3536633029580116, Final Batch Loss: 0.15039071440696716\n",
      "Epoch 1592, Loss: 0.259157232940197, Final Batch Loss: 0.07637505233287811\n",
      "Epoch 1593, Loss: 0.23420605435967445, Final Batch Loss: 0.027139481157064438\n",
      "Epoch 1594, Loss: 0.28533119708299637, Final Batch Loss: 0.06306750327348709\n",
      "Epoch 1595, Loss: 0.3421616554260254, Final Batch Loss: 0.11124738305807114\n",
      "Epoch 1596, Loss: 0.4781924933195114, Final Batch Loss: 0.22865283489227295\n",
      "Epoch 1597, Loss: 0.25968868285417557, Final Batch Loss: 0.0758630782365799\n",
      "Epoch 1598, Loss: 0.2835020422935486, Final Batch Loss: 0.09076251834630966\n",
      "Epoch 1599, Loss: 0.3153369650244713, Final Batch Loss: 0.11394783854484558\n",
      "Epoch 1600, Loss: 0.34699738025665283, Final Batch Loss: 0.121645987033844\n",
      "Epoch 1601, Loss: 0.2755655460059643, Final Batch Loss: 0.05980510637164116\n",
      "Epoch 1602, Loss: 0.3985932990908623, Final Batch Loss: 0.192522794008255\n",
      "Epoch 1603, Loss: 0.2958463132381439, Final Batch Loss: 0.07352346181869507\n",
      "Epoch 1604, Loss: 0.2296185977756977, Final Batch Loss: 0.046267736703157425\n",
      "Epoch 1605, Loss: 0.3287448063492775, Final Batch Loss: 0.12300378829240799\n",
      "Epoch 1606, Loss: 0.2655932940542698, Final Batch Loss: 0.04905705526471138\n",
      "Epoch 1607, Loss: 0.37927036732435226, Final Batch Loss: 0.16874760389328003\n",
      "Epoch 1608, Loss: 0.2755686677992344, Final Batch Loss: 0.05160729959607124\n",
      "Epoch 1609, Loss: 0.32957444339990616, Final Batch Loss: 0.1454041749238968\n",
      "Epoch 1610, Loss: 0.24694347754120827, Final Batch Loss: 0.046617541462183\n",
      "Epoch 1611, Loss: 0.3210585415363312, Final Batch Loss: 0.06565054506063461\n",
      "Epoch 1612, Loss: 0.2445685863494873, Final Batch Loss: 0.04910709708929062\n",
      "Epoch 1613, Loss: 0.24888776615262032, Final Batch Loss: 0.04791807755827904\n",
      "Epoch 1614, Loss: 0.3265029415488243, Final Batch Loss: 0.09890641272068024\n",
      "Epoch 1615, Loss: 0.3408035561442375, Final Batch Loss: 0.12113318592309952\n",
      "Epoch 1616, Loss: 0.3651513010263443, Final Batch Loss: 0.05666276067495346\n",
      "Epoch 1617, Loss: 0.2833678126335144, Final Batch Loss: 0.08126525580883026\n",
      "Epoch 1618, Loss: 0.39814039319753647, Final Batch Loss: 0.15313875675201416\n",
      "Epoch 1619, Loss: 0.33815573155879974, Final Batch Loss: 0.11431550234556198\n",
      "Epoch 1620, Loss: 0.2503701448440552, Final Batch Loss: 0.04244919866323471\n",
      "Epoch 1621, Loss: 0.36540261656045914, Final Batch Loss: 0.12977737188339233\n",
      "Epoch 1622, Loss: 0.31378285586833954, Final Batch Loss: 0.1466847062110901\n",
      "Epoch 1623, Loss: 0.3641728311777115, Final Batch Loss: 0.17321212589740753\n",
      "Epoch 1624, Loss: 0.3406556472182274, Final Batch Loss: 0.11582001298666\n",
      "Epoch 1625, Loss: 0.3423023298382759, Final Batch Loss: 0.12203890085220337\n",
      "Epoch 1626, Loss: 0.35906900838017464, Final Batch Loss: 0.22377783060073853\n",
      "Epoch 1627, Loss: 0.27491314709186554, Final Batch Loss: 0.058355122804641724\n",
      "Epoch 1628, Loss: 0.38974305987358093, Final Batch Loss: 0.12984974682331085\n",
      "Epoch 1629, Loss: 0.29655397683382034, Final Batch Loss: 0.11006779968738556\n",
      "Epoch 1630, Loss: 0.30875319242477417, Final Batch Loss: 0.08770377188920975\n",
      "Epoch 1631, Loss: 0.404424823820591, Final Batch Loss: 0.21232032775878906\n",
      "Epoch 1632, Loss: 0.2889639735221863, Final Batch Loss: 0.09633859246969223\n",
      "Epoch 1633, Loss: 0.3542727008461952, Final Batch Loss: 0.19081535935401917\n",
      "Epoch 1634, Loss: 0.28852515667676926, Final Batch Loss: 0.06606700271368027\n",
      "Epoch 1635, Loss: 0.2874794825911522, Final Batch Loss: 0.07651913166046143\n",
      "Epoch 1636, Loss: 0.33823489397764206, Final Batch Loss: 0.10262826085090637\n",
      "Epoch 1637, Loss: 0.21149510517716408, Final Batch Loss: 0.03555512800812721\n",
      "Epoch 1638, Loss: 0.33144432306289673, Final Batch Loss: 0.08350164443254471\n",
      "Epoch 1639, Loss: 0.3222949281334877, Final Batch Loss: 0.100913867354393\n",
      "Epoch 1640, Loss: 0.34659841656684875, Final Batch Loss: 0.15367363393306732\n",
      "Epoch 1641, Loss: 0.25230175256729126, Final Batch Loss: 0.06680604815483093\n",
      "Epoch 1642, Loss: 0.31525617837905884, Final Batch Loss: 0.12980429828166962\n",
      "Epoch 1643, Loss: 0.23960956186056137, Final Batch Loss: 0.06605551391839981\n",
      "Epoch 1644, Loss: 0.2616969421505928, Final Batch Loss: 0.07011962682008743\n",
      "Epoch 1645, Loss: 0.34538768231868744, Final Batch Loss: 0.16233083605766296\n",
      "Epoch 1646, Loss: 0.3323109932243824, Final Batch Loss: 0.14621546864509583\n",
      "Epoch 1647, Loss: 0.29587895423173904, Final Batch Loss: 0.04434773325920105\n",
      "Epoch 1648, Loss: 0.3024117946624756, Final Batch Loss: 0.0986318364739418\n",
      "Epoch 1649, Loss: 0.27013537287712097, Final Batch Loss: 0.07867773622274399\n",
      "Epoch 1650, Loss: 0.27689124271273613, Final Batch Loss: 0.048433076590299606\n",
      "Epoch 1651, Loss: 0.3226528689265251, Final Batch Loss: 0.07230047136545181\n",
      "Epoch 1652, Loss: 0.23687174543738365, Final Batch Loss: 0.02166563645005226\n",
      "Epoch 1653, Loss: 0.24196584150195122, Final Batch Loss: 0.026191938668489456\n",
      "Epoch 1654, Loss: 0.27939076721668243, Final Batch Loss: 0.05470207333564758\n",
      "Epoch 1655, Loss: 0.2699156515300274, Final Batch Loss: 0.04527422413229942\n",
      "Epoch 1656, Loss: 0.25285959243774414, Final Batch Loss: 0.05935046821832657\n",
      "Epoch 1657, Loss: 0.3443780466914177, Final Batch Loss: 0.10644105076789856\n",
      "Epoch 1658, Loss: 0.2662975788116455, Final Batch Loss: 0.06783831119537354\n",
      "Epoch 1659, Loss: 0.40949348360300064, Final Batch Loss: 0.20101141929626465\n",
      "Epoch 1660, Loss: 0.3303651288151741, Final Batch Loss: 0.12504668533802032\n",
      "Epoch 1661, Loss: 0.2688717469573021, Final Batch Loss: 0.08899993449449539\n",
      "Epoch 1662, Loss: 0.267037995159626, Final Batch Loss: 0.0656658485531807\n",
      "Epoch 1663, Loss: 0.3345496281981468, Final Batch Loss: 0.1239539235830307\n",
      "Epoch 1664, Loss: 0.2380146011710167, Final Batch Loss: 0.07349248230457306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1665, Loss: 0.3311505988240242, Final Batch Loss: 0.12195918709039688\n",
      "Epoch 1666, Loss: 0.2772604003548622, Final Batch Loss: 0.11616647243499756\n",
      "Epoch 1667, Loss: 0.32766518741846085, Final Batch Loss: 0.09697865694761276\n",
      "Epoch 1668, Loss: 0.20026988815516233, Final Batch Loss: 0.011312308721244335\n",
      "Epoch 1669, Loss: 0.25507551804184914, Final Batch Loss: 0.04518095776438713\n",
      "Epoch 1670, Loss: 0.2556705102324486, Final Batch Loss: 0.0773693323135376\n",
      "Epoch 1671, Loss: 0.21554884314537048, Final Batch Loss: 0.052257880568504333\n",
      "Epoch 1672, Loss: 0.37675295770168304, Final Batch Loss: 0.16595274209976196\n",
      "Epoch 1673, Loss: 0.3482403978705406, Final Batch Loss: 0.07197705656290054\n",
      "Epoch 1674, Loss: 0.30054399371147156, Final Batch Loss: 0.1402548998594284\n",
      "Epoch 1675, Loss: 0.28607355803251266, Final Batch Loss: 0.04768642783164978\n",
      "Epoch 1676, Loss: 0.19433629140257835, Final Batch Loss: 0.03777990862727165\n",
      "Epoch 1677, Loss: 0.32823870331048965, Final Batch Loss: 0.19144409894943237\n",
      "Epoch 1678, Loss: 0.28926245123147964, Final Batch Loss: 0.10774411261081696\n",
      "Epoch 1679, Loss: 0.2734478935599327, Final Batch Loss: 0.0921463668346405\n",
      "Epoch 1680, Loss: 0.33694902062416077, Final Batch Loss: 0.17069904506206512\n",
      "Epoch 1681, Loss: 0.4031353071331978, Final Batch Loss: 0.23326940834522247\n",
      "Epoch 1682, Loss: 0.2600775435566902, Final Batch Loss: 0.07634327560663223\n",
      "Epoch 1683, Loss: 0.25012078136205673, Final Batch Loss: 0.06024419516324997\n",
      "Epoch 1684, Loss: 0.2719600051641464, Final Batch Loss: 0.08307947963476181\n",
      "Epoch 1685, Loss: 0.22644945606589317, Final Batch Loss: 0.057507243007421494\n",
      "Epoch 1686, Loss: 0.34127703309059143, Final Batch Loss: 0.15141360461711884\n",
      "Epoch 1687, Loss: 0.4086957648396492, Final Batch Loss: 0.26459410786628723\n",
      "Epoch 1688, Loss: 0.3994492068886757, Final Batch Loss: 0.2390727549791336\n",
      "Epoch 1689, Loss: 0.33961961418390274, Final Batch Loss: 0.10478000342845917\n",
      "Epoch 1690, Loss: 0.3260830119252205, Final Batch Loss: 0.10891301929950714\n",
      "Epoch 1691, Loss: 0.3194688782095909, Final Batch Loss: 0.14870542287826538\n",
      "Epoch 1692, Loss: 0.29202360659837723, Final Batch Loss: 0.07473433017730713\n",
      "Epoch 1693, Loss: 0.3615337684750557, Final Batch Loss: 0.15698374807834625\n",
      "Epoch 1694, Loss: 0.33878781646490097, Final Batch Loss: 0.1034308522939682\n",
      "Epoch 1695, Loss: 0.24942921474575996, Final Batch Loss: 0.05487387254834175\n",
      "Epoch 1696, Loss: 0.31614692509174347, Final Batch Loss: 0.13448171317577362\n",
      "Epoch 1697, Loss: 0.2568141296505928, Final Batch Loss: 0.056879349052906036\n",
      "Epoch 1698, Loss: 0.39746593683958054, Final Batch Loss: 0.20396322011947632\n",
      "Epoch 1699, Loss: 0.3388219475746155, Final Batch Loss: 0.09730991721153259\n",
      "Epoch 1700, Loss: 0.28086430579423904, Final Batch Loss: 0.029993489384651184\n",
      "Epoch 1701, Loss: 0.32715020328760147, Final Batch Loss: 0.10178007185459137\n",
      "Epoch 1702, Loss: 0.32704807072877884, Final Batch Loss: 0.14582012593746185\n",
      "Epoch 1703, Loss: 0.25056998431682587, Final Batch Loss: 0.07723212987184525\n",
      "Epoch 1704, Loss: 0.22671783342957497, Final Batch Loss: 0.06065750494599342\n",
      "Epoch 1705, Loss: 0.2636612802743912, Final Batch Loss: 0.07114317268133163\n",
      "Epoch 1706, Loss: 0.2486458197236061, Final Batch Loss: 0.07755012810230255\n",
      "Epoch 1707, Loss: 0.22456244379281998, Final Batch Loss: 0.05498627573251724\n",
      "Epoch 1708, Loss: 0.2985449805855751, Final Batch Loss: 0.08327760547399521\n",
      "Epoch 1709, Loss: 0.43427838385105133, Final Batch Loss: 0.2855418920516968\n",
      "Epoch 1710, Loss: 0.26627179235219955, Final Batch Loss: 0.09881097823381424\n",
      "Epoch 1711, Loss: 0.35409168899059296, Final Batch Loss: 0.136540025472641\n",
      "Epoch 1712, Loss: 0.33808883279561996, Final Batch Loss: 0.09030288457870483\n",
      "Epoch 1713, Loss: 0.3336791843175888, Final Batch Loss: 0.129873126745224\n",
      "Epoch 1714, Loss: 0.3851681426167488, Final Batch Loss: 0.15867526829242706\n",
      "Epoch 1715, Loss: 0.32059214264154434, Final Batch Loss: 0.13046735525131226\n",
      "Epoch 1716, Loss: 0.4383717253804207, Final Batch Loss: 0.20321926474571228\n",
      "Epoch 1717, Loss: 0.269100159406662, Final Batch Loss: 0.047027334570884705\n",
      "Epoch 1718, Loss: 0.5882738456130028, Final Batch Loss: 0.32704320549964905\n",
      "Epoch 1719, Loss: 0.32023046165704727, Final Batch Loss: 0.06460417807102203\n",
      "Epoch 1720, Loss: 0.5848044976592064, Final Batch Loss: 0.3223109841346741\n",
      "Epoch 1721, Loss: 0.35506684705615044, Final Batch Loss: 0.057270776480436325\n",
      "Epoch 1722, Loss: 0.2961415946483612, Final Batch Loss: 0.02834923565387726\n",
      "Epoch 1723, Loss: 0.4614338129758835, Final Batch Loss: 0.1296030580997467\n",
      "Epoch 1724, Loss: 0.30258888751268387, Final Batch Loss: 0.0524565652012825\n",
      "Epoch 1725, Loss: 0.27243901789188385, Final Batch Loss: 0.07318497449159622\n",
      "Epoch 1726, Loss: 0.4068172574043274, Final Batch Loss: 0.11995524168014526\n",
      "Epoch 1727, Loss: 0.35245195776224136, Final Batch Loss: 0.13930290937423706\n",
      "Epoch 1728, Loss: 0.3932063654065132, Final Batch Loss: 0.15843376517295837\n",
      "Epoch 1729, Loss: 0.3793547451496124, Final Batch Loss: 0.21676449477672577\n",
      "Epoch 1730, Loss: 0.2641359195113182, Final Batch Loss: 0.07240632176399231\n",
      "Epoch 1731, Loss: 0.37292299419641495, Final Batch Loss: 0.15865692496299744\n",
      "Epoch 1732, Loss: 0.2401970587670803, Final Batch Loss: 0.053223010152578354\n",
      "Epoch 1733, Loss: 0.27813833951950073, Final Batch Loss: 0.08045122027397156\n",
      "Epoch 1734, Loss: 0.36970940977334976, Final Batch Loss: 0.08369877934455872\n",
      "Epoch 1735, Loss: 0.32489113509655, Final Batch Loss: 0.1447996199131012\n",
      "Epoch 1736, Loss: 0.3675150200724602, Final Batch Loss: 0.12115103006362915\n",
      "Epoch 1737, Loss: 0.33216747641563416, Final Batch Loss: 0.09757789224386215\n",
      "Epoch 1738, Loss: 0.26827261596918106, Final Batch Loss: 0.09149833768606186\n",
      "Epoch 1739, Loss: 0.3956741914153099, Final Batch Loss: 0.19666291773319244\n",
      "Epoch 1740, Loss: 0.30194612592458725, Final Batch Loss: 0.08521357923746109\n",
      "Epoch 1741, Loss: 0.4054642543196678, Final Batch Loss: 0.16816310584545135\n",
      "Epoch 1742, Loss: 0.24812154099345207, Final Batch Loss: 0.05422201380133629\n",
      "Epoch 1743, Loss: 0.224751777946949, Final Batch Loss: 0.04965536296367645\n",
      "Epoch 1744, Loss: 0.3180340528488159, Final Batch Loss: 0.13170607388019562\n",
      "Epoch 1745, Loss: 0.2707931026816368, Final Batch Loss: 0.05229843407869339\n",
      "Epoch 1746, Loss: 0.41740573197603226, Final Batch Loss: 0.24821513891220093\n",
      "Epoch 1747, Loss: 0.3204300031065941, Final Batch Loss: 0.16175825893878937\n",
      "Epoch 1748, Loss: 0.30800022929906845, Final Batch Loss: 0.11661404371261597\n",
      "Epoch 1749, Loss: 0.3305530473589897, Final Batch Loss: 0.16444265842437744\n",
      "Epoch 1750, Loss: 0.31318310648202896, Final Batch Loss: 0.10330060124397278\n",
      "Epoch 1751, Loss: 0.23376187682151794, Final Batch Loss: 0.07794229686260223\n",
      "Epoch 1752, Loss: 0.27843381464481354, Final Batch Loss: 0.08557312190532684\n",
      "Epoch 1753, Loss: 0.3482399135828018, Final Batch Loss: 0.1869102120399475\n",
      "Epoch 1754, Loss: 0.32205595821142197, Final Batch Loss: 0.10157762467861176\n",
      "Epoch 1755, Loss: 0.3122863918542862, Final Batch Loss: 0.06472750753164291\n",
      "Epoch 1756, Loss: 0.1973271332681179, Final Batch Loss: 0.040627431124448776\n",
      "Epoch 1757, Loss: 0.3112660199403763, Final Batch Loss: 0.09593850374221802\n",
      "Epoch 1758, Loss: 0.40157323330640793, Final Batch Loss: 0.24432404339313507\n",
      "Epoch 1759, Loss: 0.2071877010166645, Final Batch Loss: 0.04590494558215141\n",
      "Epoch 1760, Loss: 0.29147089272737503, Final Batch Loss: 0.1361057013273239\n",
      "Epoch 1761, Loss: 0.3078973963856697, Final Batch Loss: 0.0758146345615387\n",
      "Epoch 1762, Loss: 0.3076200410723686, Final Batch Loss: 0.0652642697095871\n",
      "Epoch 1763, Loss: 0.3355378657579422, Final Batch Loss: 0.1204879954457283\n",
      "Epoch 1764, Loss: 0.3219915181398392, Final Batch Loss: 0.0691889151930809\n",
      "Epoch 1765, Loss: 0.22799233347177505, Final Batch Loss: 0.04385899007320404\n",
      "Epoch 1766, Loss: 0.3725798428058624, Final Batch Loss: 0.1588810533285141\n",
      "Epoch 1767, Loss: 0.3131644129753113, Final Batch Loss: 0.13497518002986908\n",
      "Epoch 1768, Loss: 0.39067260175943375, Final Batch Loss: 0.17188599705696106\n",
      "Epoch 1769, Loss: 0.2371545396745205, Final Batch Loss: 0.03785807266831398\n",
      "Epoch 1770, Loss: 0.35304857045412064, Final Batch Loss: 0.1363406479358673\n",
      "Epoch 1771, Loss: 0.24158061109483242, Final Batch Loss: 0.03077671490609646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1772, Loss: 0.2743995785713196, Final Batch Loss: 0.046530984342098236\n",
      "Epoch 1773, Loss: 0.27043795585632324, Final Batch Loss: 0.09151102602481842\n",
      "Epoch 1774, Loss: 0.2329522743821144, Final Batch Loss: 0.055455103516578674\n",
      "Epoch 1775, Loss: 0.25681228935718536, Final Batch Loss: 0.08906405419111252\n",
      "Epoch 1776, Loss: 0.291760865598917, Final Batch Loss: 0.14720983803272247\n",
      "Epoch 1777, Loss: 0.33797387033700943, Final Batch Loss: 0.10318645089864731\n",
      "Epoch 1778, Loss: 0.2113122958689928, Final Batch Loss: 0.029834525659680367\n",
      "Epoch 1779, Loss: 0.22416288778185844, Final Batch Loss: 0.05093881115317345\n",
      "Epoch 1780, Loss: 0.2675596885383129, Final Batch Loss: 0.06145697459578514\n",
      "Epoch 1781, Loss: 0.2718813680112362, Final Batch Loss: 0.01878950372338295\n",
      "Epoch 1782, Loss: 0.2873598486185074, Final Batch Loss: 0.12764683365821838\n",
      "Epoch 1783, Loss: 0.2653651162981987, Final Batch Loss: 0.09831073135137558\n",
      "Epoch 1784, Loss: 0.25690001249313354, Final Batch Loss: 0.06312794983386993\n",
      "Epoch 1785, Loss: 0.3440082520246506, Final Batch Loss: 0.07769419997930527\n",
      "Epoch 1786, Loss: 0.2650175839662552, Final Batch Loss: 0.09444452822208405\n",
      "Epoch 1787, Loss: 0.4976588189601898, Final Batch Loss: 0.2687085270881653\n",
      "Epoch 1788, Loss: 0.317453071475029, Final Batch Loss: 0.08017328381538391\n",
      "Epoch 1789, Loss: 0.37088220566511154, Final Batch Loss: 0.1036391630768776\n",
      "Epoch 1790, Loss: 0.3359861336648464, Final Batch Loss: 0.057608712464571\n",
      "Epoch 1791, Loss: 0.3275250047445297, Final Batch Loss: 0.1720135658979416\n",
      "Epoch 1792, Loss: 0.4476972818374634, Final Batch Loss: 0.21986427903175354\n",
      "Epoch 1793, Loss: 0.26458889991045, Final Batch Loss: 0.038394130766391754\n",
      "Epoch 1794, Loss: 0.43177448958158493, Final Batch Loss: 0.25080859661102295\n",
      "Epoch 1795, Loss: 0.24099553376436234, Final Batch Loss: 0.022558555006980896\n",
      "Epoch 1796, Loss: 0.3541385680437088, Final Batch Loss: 0.145673006772995\n",
      "Epoch 1797, Loss: 0.20039553195238113, Final Batch Loss: 0.06571289151906967\n",
      "Epoch 1798, Loss: 0.28668899834156036, Final Batch Loss: 0.11373645812273026\n",
      "Epoch 1799, Loss: 0.2770683169364929, Final Batch Loss: 0.08294995874166489\n",
      "Epoch 1800, Loss: 0.2984733581542969, Final Batch Loss: 0.10540789365768433\n",
      "Epoch 1801, Loss: 0.4059278145432472, Final Batch Loss: 0.21755467355251312\n",
      "Epoch 1802, Loss: 0.3876207247376442, Final Batch Loss: 0.2162822037935257\n",
      "Epoch 1803, Loss: 0.3477453365921974, Final Batch Loss: 0.08996877819299698\n",
      "Epoch 1804, Loss: 0.36712709814310074, Final Batch Loss: 0.1315881460905075\n",
      "Epoch 1805, Loss: 0.27561143040657043, Final Batch Loss: 0.0876941904425621\n",
      "Epoch 1806, Loss: 0.31253109127283096, Final Batch Loss: 0.11166766285896301\n",
      "Epoch 1807, Loss: 0.27444010972976685, Final Batch Loss: 0.09152360260486603\n",
      "Epoch 1808, Loss: 0.2881932035088539, Final Batch Loss: 0.07833611965179443\n",
      "Epoch 1809, Loss: 0.26761840283870697, Final Batch Loss: 0.11507604271173477\n",
      "Epoch 1810, Loss: 0.23991689831018448, Final Batch Loss: 0.07793740928173065\n",
      "Epoch 1811, Loss: 0.41552113741636276, Final Batch Loss: 0.21795257925987244\n",
      "Epoch 1812, Loss: 0.3100699409842491, Final Batch Loss: 0.15177108347415924\n",
      "Epoch 1813, Loss: 0.26732608675956726, Final Batch Loss: 0.08822038024663925\n",
      "Epoch 1814, Loss: 0.47015102952718735, Final Batch Loss: 0.2281724214553833\n",
      "Epoch 1815, Loss: 0.2473081611096859, Final Batch Loss: 0.09207608550786972\n",
      "Epoch 1816, Loss: 0.25737376138567924, Final Batch Loss: 0.060557130724191666\n",
      "Epoch 1817, Loss: 0.35095006972551346, Final Batch Loss: 0.12182068079710007\n",
      "Epoch 1818, Loss: 0.2888426259160042, Final Batch Loss: 0.12313669919967651\n",
      "Epoch 1819, Loss: 0.2898368015885353, Final Batch Loss: 0.09116420894861221\n",
      "Epoch 1820, Loss: 0.210863646119833, Final Batch Loss: 0.03375052288174629\n",
      "Epoch 1821, Loss: 0.2681497484445572, Final Batch Loss: 0.06779569387435913\n",
      "Epoch 1822, Loss: 0.22258297353982925, Final Batch Loss: 0.02989848703145981\n",
      "Epoch 1823, Loss: 0.22548016905784607, Final Batch Loss: 0.03931041806936264\n",
      "Epoch 1824, Loss: 0.2109406739473343, Final Batch Loss: 0.06487636268138885\n",
      "Epoch 1825, Loss: 0.22658267058432102, Final Batch Loss: 0.0277716014534235\n",
      "Epoch 1826, Loss: 0.2810148224234581, Final Batch Loss: 0.10074878484010696\n",
      "Epoch 1827, Loss: 0.32296300679445267, Final Batch Loss: 0.1377546340227127\n",
      "Epoch 1828, Loss: 0.25860172510147095, Final Batch Loss: 0.06732765585184097\n",
      "Epoch 1829, Loss: 0.24798511154949665, Final Batch Loss: 0.02522597648203373\n",
      "Epoch 1830, Loss: 0.19560280069708824, Final Batch Loss: 0.05315418168902397\n",
      "Epoch 1831, Loss: 0.25812336057424545, Final Batch Loss: 0.09600074589252472\n",
      "Epoch 1832, Loss: 0.271853543817997, Final Batch Loss: 0.07182221114635468\n",
      "Epoch 1833, Loss: 0.29068198800086975, Final Batch Loss: 0.09752634167671204\n",
      "Epoch 1834, Loss: 0.36419936269521713, Final Batch Loss: 0.11315399408340454\n",
      "Epoch 1835, Loss: 0.3078201189637184, Final Batch Loss: 0.10338250547647476\n",
      "Epoch 1836, Loss: 0.21884805709123611, Final Batch Loss: 0.02407459169626236\n",
      "Epoch 1837, Loss: 0.3249773308634758, Final Batch Loss: 0.1582372635602951\n",
      "Epoch 1838, Loss: 0.20765741355717182, Final Batch Loss: 0.03038753755390644\n",
      "Epoch 1839, Loss: 0.31444285809993744, Final Batch Loss: 0.15528473258018494\n",
      "Epoch 1840, Loss: 0.2438337653875351, Final Batch Loss: 0.05985508859157562\n",
      "Epoch 1841, Loss: 0.2632868718355894, Final Batch Loss: 0.030503636226058006\n",
      "Epoch 1842, Loss: 0.28942956775426865, Final Batch Loss: 0.06993486732244492\n",
      "Epoch 1843, Loss: 0.24696678668260574, Final Batch Loss: 0.04588017612695694\n",
      "Epoch 1844, Loss: 0.3259804919362068, Final Batch Loss: 0.10218439996242523\n",
      "Epoch 1845, Loss: 0.2406822368502617, Final Batch Loss: 0.09390771389007568\n",
      "Epoch 1846, Loss: 0.236417755484581, Final Batch Loss: 0.0680866539478302\n",
      "Epoch 1847, Loss: 0.2646486237645149, Final Batch Loss: 0.08089286088943481\n",
      "Epoch 1848, Loss: 0.25322211906313896, Final Batch Loss: 0.06738535314798355\n",
      "Epoch 1849, Loss: 0.1892392858862877, Final Batch Loss: 0.025618799030780792\n",
      "Epoch 1850, Loss: 0.412597693502903, Final Batch Loss: 0.11011858284473419\n",
      "Epoch 1851, Loss: 0.3521991893649101, Final Batch Loss: 0.16150909662246704\n",
      "Epoch 1852, Loss: 0.19233126752078533, Final Batch Loss: 0.029379820451140404\n",
      "Epoch 1853, Loss: 0.29709500446915627, Final Batch Loss: 0.05936156585812569\n",
      "Epoch 1854, Loss: 0.28381042182445526, Final Batch Loss: 0.08946895599365234\n",
      "Epoch 1855, Loss: 0.24924392998218536, Final Batch Loss: 0.08364039659500122\n",
      "Epoch 1856, Loss: 0.29622066766023636, Final Batch Loss: 0.13512250781059265\n",
      "Epoch 1857, Loss: 0.24127742648124695, Final Batch Loss: 0.05915757268667221\n",
      "Epoch 1858, Loss: 0.28742382675409317, Final Batch Loss: 0.06461893022060394\n",
      "Epoch 1859, Loss: 0.22548586502671242, Final Batch Loss: 0.05380852892994881\n",
      "Epoch 1860, Loss: 0.3252669721841812, Final Batch Loss: 0.16139483451843262\n",
      "Epoch 1861, Loss: 0.2619696706533432, Final Batch Loss: 0.06673665344715118\n",
      "Epoch 1862, Loss: 0.28946399688720703, Final Batch Loss: 0.08571196347475052\n",
      "Epoch 1863, Loss: 0.23173638805747032, Final Batch Loss: 0.043349217623472214\n",
      "Epoch 1864, Loss: 0.39321278035640717, Final Batch Loss: 0.13206256926059723\n",
      "Epoch 1865, Loss: 0.2799685038626194, Final Batch Loss: 0.11997076869010925\n",
      "Epoch 1866, Loss: 0.22699039801955223, Final Batch Loss: 0.06730588525533676\n",
      "Epoch 1867, Loss: 0.2385091632604599, Final Batch Loss: 0.07823510468006134\n",
      "Epoch 1868, Loss: 0.27463478595018387, Final Batch Loss: 0.11421605944633484\n",
      "Epoch 1869, Loss: 0.24699275568127632, Final Batch Loss: 0.10604865849018097\n",
      "Epoch 1870, Loss: 0.19973376765847206, Final Batch Loss: 0.048179205507040024\n",
      "Epoch 1871, Loss: 0.2116078995168209, Final Batch Loss: 0.10092407464981079\n",
      "Epoch 1872, Loss: 0.26642127335071564, Final Batch Loss: 0.06947634369134903\n",
      "Epoch 1873, Loss: 0.2890353947877884, Final Batch Loss: 0.11683877557516098\n",
      "Epoch 1874, Loss: 0.321399949491024, Final Batch Loss: 0.12250382453203201\n",
      "Epoch 1875, Loss: 0.2706238850951195, Final Batch Loss: 0.09281700104475021\n",
      "Epoch 1876, Loss: 0.22846874594688416, Final Batch Loss: 0.0434611514210701\n",
      "Epoch 1877, Loss: 0.20265938341617584, Final Batch Loss: 0.0628516748547554\n",
      "Epoch 1878, Loss: 0.18202029168605804, Final Batch Loss: 0.035443659871816635\n",
      "Epoch 1879, Loss: 0.24396787583827972, Final Batch Loss: 0.09529154747724533\n",
      "Epoch 1880, Loss: 0.3412574380636215, Final Batch Loss: 0.18764974176883698\n",
      "Epoch 1881, Loss: 0.23814335465431213, Final Batch Loss: 0.08176837116479874\n",
      "Epoch 1882, Loss: 0.1886620968580246, Final Batch Loss: 0.05713304877281189\n",
      "Epoch 1883, Loss: 0.25937506556510925, Final Batch Loss: 0.09933299571275711\n",
      "Epoch 1884, Loss: 0.21523701772093773, Final Batch Loss: 0.06437742710113525\n",
      "Epoch 1885, Loss: 0.27773457020521164, Final Batch Loss: 0.09061157703399658\n",
      "Epoch 1886, Loss: 0.25399383157491684, Final Batch Loss: 0.08523400872945786\n",
      "Epoch 1887, Loss: 0.2044784314930439, Final Batch Loss: 0.021373886615037918\n",
      "Epoch 1888, Loss: 0.49388856440782547, Final Batch Loss: 0.24199725687503815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1889, Loss: 0.2475951686501503, Final Batch Loss: 0.07092037051916122\n",
      "Epoch 1890, Loss: 0.3589361384510994, Final Batch Loss: 0.203648641705513\n",
      "Epoch 1891, Loss: 0.32985685765743256, Final Batch Loss: 0.10089437663555145\n",
      "Epoch 1892, Loss: 0.3935793340206146, Final Batch Loss: 0.15499131381511688\n",
      "Epoch 1893, Loss: 0.2424207255244255, Final Batch Loss: 0.06218883395195007\n",
      "Epoch 1894, Loss: 0.3249431177973747, Final Batch Loss: 0.09640274196863174\n",
      "Epoch 1895, Loss: 0.2203887216746807, Final Batch Loss: 0.08029205352067947\n",
      "Epoch 1896, Loss: 0.26068640500307083, Final Batch Loss: 0.0723315179347992\n",
      "Epoch 1897, Loss: 0.29435334354639053, Final Batch Loss: 0.08156409859657288\n",
      "Epoch 1898, Loss: 0.27029960229992867, Final Batch Loss: 0.11276435852050781\n",
      "Epoch 1899, Loss: 0.2709136828780174, Final Batch Loss: 0.07998542487621307\n",
      "Epoch 1900, Loss: 0.21126898750662804, Final Batch Loss: 0.05209580436348915\n",
      "Epoch 1901, Loss: 0.2039736472070217, Final Batch Loss: 0.04091361537575722\n",
      "Epoch 1902, Loss: 0.20539093762636185, Final Batch Loss: 0.039498455822467804\n",
      "Epoch 1903, Loss: 0.25455154851078987, Final Batch Loss: 0.04877646639943123\n",
      "Epoch 1904, Loss: 0.19943512231111526, Final Batch Loss: 0.045078784227371216\n",
      "Epoch 1905, Loss: 0.3204524889588356, Final Batch Loss: 0.12545271217823029\n",
      "Epoch 1906, Loss: 0.25905875116586685, Final Batch Loss: 0.08277606964111328\n",
      "Epoch 1907, Loss: 0.2241213619709015, Final Batch Loss: 0.039061304181814194\n",
      "Epoch 1908, Loss: 0.31178734079003334, Final Batch Loss: 0.16927748918533325\n",
      "Epoch 1909, Loss: 0.23662494122982025, Final Batch Loss: 0.06660572439432144\n",
      "Epoch 1910, Loss: 0.26663707941770554, Final Batch Loss: 0.09031688421964645\n",
      "Epoch 1911, Loss: 0.18302759155631065, Final Batch Loss: 0.05504119023680687\n",
      "Epoch 1912, Loss: 0.21215805783867836, Final Batch Loss: 0.06084910407662392\n",
      "Epoch 1913, Loss: 0.26252947747707367, Final Batch Loss: 0.06073163449764252\n",
      "Epoch 1914, Loss: 0.19083081185817719, Final Batch Loss: 0.06503124535083771\n",
      "Epoch 1915, Loss: 0.20649924874305725, Final Batch Loss: 0.05558577552437782\n",
      "Epoch 1916, Loss: 0.34288735687732697, Final Batch Loss: 0.14601950347423553\n",
      "Epoch 1917, Loss: 0.27059969305992126, Final Batch Loss: 0.0849233865737915\n",
      "Epoch 1918, Loss: 0.28499632328748703, Final Batch Loss: 0.09513775259256363\n",
      "Epoch 1919, Loss: 0.2424532137811184, Final Batch Loss: 0.04251164197921753\n",
      "Epoch 1920, Loss: 0.20370205119252205, Final Batch Loss: 0.04402916505932808\n",
      "Epoch 1921, Loss: 0.2932257503271103, Final Batch Loss: 0.07862292975187302\n",
      "Epoch 1922, Loss: 0.34029364585876465, Final Batch Loss: 0.17937608063220978\n",
      "Epoch 1923, Loss: 0.2624150961637497, Final Batch Loss: 0.08599867671728134\n",
      "Epoch 1924, Loss: 0.2797925993800163, Final Batch Loss: 0.11908614635467529\n",
      "Epoch 1925, Loss: 0.2972710430622101, Final Batch Loss: 0.1349678635597229\n",
      "Epoch 1926, Loss: 0.2286849170923233, Final Batch Loss: 0.07109035551548004\n",
      "Epoch 1927, Loss: 0.2409626916050911, Final Batch Loss: 0.05851670354604721\n",
      "Epoch 1928, Loss: 0.20165350660681725, Final Batch Loss: 0.0376787930727005\n",
      "Epoch 1929, Loss: 0.25242096185684204, Final Batch Loss: 0.06386192888021469\n",
      "Epoch 1930, Loss: 0.2127474620938301, Final Batch Loss: 0.02901049703359604\n",
      "Epoch 1931, Loss: 0.36927057057619095, Final Batch Loss: 0.17324493825435638\n",
      "Epoch 1932, Loss: 0.284568153321743, Final Batch Loss: 0.0570097342133522\n",
      "Epoch 1933, Loss: 0.36376751214265823, Final Batch Loss: 0.12036173045635223\n",
      "Epoch 1934, Loss: 0.25968504324555397, Final Batch Loss: 0.04031062498688698\n",
      "Epoch 1935, Loss: 0.23790926858782768, Final Batch Loss: 0.10562842339277267\n",
      "Epoch 1936, Loss: 0.2531459182500839, Final Batch Loss: 0.08516179770231247\n",
      "Epoch 1937, Loss: 0.31432459875941277, Final Batch Loss: 0.14142650365829468\n",
      "Epoch 1938, Loss: 0.2572973296046257, Final Batch Loss: 0.08593617379665375\n",
      "Epoch 1939, Loss: 0.30633264034986496, Final Batch Loss: 0.14194746315479279\n",
      "Epoch 1940, Loss: 0.2972100228071213, Final Batch Loss: 0.10490258783102036\n",
      "Epoch 1941, Loss: 0.2531379237771034, Final Batch Loss: 0.09112861007452011\n",
      "Epoch 1942, Loss: 0.23888228833675385, Final Batch Loss: 0.05440653860569\n",
      "Epoch 1943, Loss: 0.25206998735666275, Final Batch Loss: 0.10219211131334305\n",
      "Epoch 1944, Loss: 0.27914945036172867, Final Batch Loss: 0.11631704866886139\n",
      "Epoch 1945, Loss: 0.2643178328871727, Final Batch Loss: 0.10502900183200836\n",
      "Epoch 1946, Loss: 0.27754441648721695, Final Batch Loss: 0.10546628385782242\n",
      "Epoch 1947, Loss: 0.24689508974552155, Final Batch Loss: 0.10620030015707016\n",
      "Epoch 1948, Loss: 0.34781746566295624, Final Batch Loss: 0.11180383712053299\n",
      "Epoch 1949, Loss: 0.38752327114343643, Final Batch Loss: 0.2176537662744522\n",
      "Epoch 1950, Loss: 0.20137223973870277, Final Batch Loss: 0.07692156732082367\n",
      "Epoch 1951, Loss: 0.3668230324983597, Final Batch Loss: 0.1395777463912964\n",
      "Epoch 1952, Loss: 0.3087150976061821, Final Batch Loss: 0.09134648740291595\n",
      "Epoch 1953, Loss: 0.34428635239601135, Final Batch Loss: 0.14297914505004883\n",
      "Epoch 1954, Loss: 0.2929565906524658, Final Batch Loss: 0.09849816560745239\n",
      "Epoch 1955, Loss: 0.25754866749048233, Final Batch Loss: 0.040078260004520416\n",
      "Epoch 1956, Loss: 0.4394032657146454, Final Batch Loss: 0.1230497732758522\n",
      "Epoch 1957, Loss: 0.2678573578596115, Final Batch Loss: 0.07193267345428467\n",
      "Epoch 1958, Loss: 0.3137839734554291, Final Batch Loss: 0.08775871247053146\n",
      "Epoch 1959, Loss: 0.2400287901982665, Final Batch Loss: 0.015223241411149502\n",
      "Epoch 1960, Loss: 0.3191419243812561, Final Batch Loss: 0.1283518224954605\n",
      "Epoch 1961, Loss: 0.2926603779196739, Final Batch Loss: 0.0944734588265419\n",
      "Epoch 1962, Loss: 0.35269206017255783, Final Batch Loss: 0.1252027004957199\n",
      "Epoch 1963, Loss: 0.2759895883500576, Final Batch Loss: 0.057632241398096085\n",
      "Epoch 1964, Loss: 0.3357331305742264, Final Batch Loss: 0.13208472728729248\n",
      "Epoch 1965, Loss: 0.34923501312732697, Final Batch Loss: 0.17393261194229126\n",
      "Epoch 1966, Loss: 0.277433380484581, Final Batch Loss: 0.16489197313785553\n",
      "Epoch 1967, Loss: 0.376555472612381, Final Batch Loss: 0.15899795293807983\n",
      "Epoch 1968, Loss: 0.2686363756656647, Final Batch Loss: 0.06868243962526321\n",
      "Epoch 1969, Loss: 0.20611263811588287, Final Batch Loss: 0.0797058716416359\n",
      "Epoch 1970, Loss: 0.32341960817575455, Final Batch Loss: 0.15736623108386993\n",
      "Epoch 1971, Loss: 0.3023306503891945, Final Batch Loss: 0.10141332447528839\n",
      "Epoch 1972, Loss: 0.3051498532295227, Final Batch Loss: 0.12530888617038727\n",
      "Epoch 1973, Loss: 0.32006342709064484, Final Batch Loss: 0.15104980766773224\n",
      "Epoch 1974, Loss: 0.22701162844896317, Final Batch Loss: 0.05690440163016319\n",
      "Epoch 1975, Loss: 0.2844752259552479, Final Batch Loss: 0.051648031920194626\n",
      "Epoch 1976, Loss: 0.3955315575003624, Final Batch Loss: 0.19091568887233734\n",
      "Epoch 1977, Loss: 0.23105647414922714, Final Batch Loss: 0.04111433029174805\n",
      "Epoch 1978, Loss: 0.22826622426509857, Final Batch Loss: 0.061373382806777954\n",
      "Epoch 1979, Loss: 0.2539179176092148, Final Batch Loss: 0.08081263303756714\n",
      "Epoch 1980, Loss: 0.3250267654657364, Final Batch Loss: 0.150039404630661\n",
      "Epoch 1981, Loss: 0.20619194954633713, Final Batch Loss: 0.04709067940711975\n",
      "Epoch 1982, Loss: 0.25674495100975037, Final Batch Loss: 0.09729058295488358\n",
      "Epoch 1983, Loss: 0.22130024433135986, Final Batch Loss: 0.03584897518157959\n",
      "Epoch 1984, Loss: 0.3134456127882004, Final Batch Loss: 0.1471244841814041\n",
      "Epoch 1985, Loss: 0.3329917639493942, Final Batch Loss: 0.13770918548107147\n",
      "Epoch 1986, Loss: 0.20854418724775314, Final Batch Loss: 0.02760826051235199\n",
      "Epoch 1987, Loss: 0.2275291308760643, Final Batch Loss: 0.0632648840546608\n",
      "Epoch 1988, Loss: 0.21193184703588486, Final Batch Loss: 0.06930539011955261\n",
      "Epoch 1989, Loss: 0.23329553753137589, Final Batch Loss: 0.09394998103380203\n",
      "Epoch 1990, Loss: 0.3029365912079811, Final Batch Loss: 0.1317751556634903\n",
      "Epoch 1991, Loss: 0.15723109245300293, Final Batch Loss: 0.03901255875825882\n",
      "Epoch 1992, Loss: 0.183932077139616, Final Batch Loss: 0.04352259263396263\n",
      "Epoch 1993, Loss: 0.29068171977996826, Final Batch Loss: 0.13096089661121368\n",
      "Epoch 1994, Loss: 0.1859589833766222, Final Batch Loss: 0.024207191541790962\n",
      "Epoch 1995, Loss: 0.2433149665594101, Final Batch Loss: 0.023922324180603027\n",
      "Epoch 1996, Loss: 0.20362461730837822, Final Batch Loss: 0.038666654378175735\n",
      "Epoch 1997, Loss: 0.20812938921153545, Final Batch Loss: 0.0192220788449049\n",
      "Epoch 1998, Loss: 0.21117805503308773, Final Batch Loss: 0.028836240991950035\n",
      "Epoch 1999, Loss: 0.3319786489009857, Final Batch Loss: 0.10895262658596039\n",
      "Epoch 2000, Loss: 0.2996858283877373, Final Batch Loss: 0.05426977574825287\n",
      "Epoch 2001, Loss: 0.21914512664079666, Final Batch Loss: 0.06290040910243988\n",
      "Epoch 2002, Loss: 0.234099380671978, Final Batch Loss: 0.090692900121212\n",
      "Epoch 2003, Loss: 0.17007359117269516, Final Batch Loss: 0.03818764537572861\n",
      "Epoch 2004, Loss: 0.23876746743917465, Final Batch Loss: 0.06622971594333649\n",
      "Epoch 2005, Loss: 0.2625780329108238, Final Batch Loss: 0.1136322095990181\n",
      "Epoch 2006, Loss: 0.25196803361177444, Final Batch Loss: 0.10590442270040512\n",
      "Epoch 2007, Loss: 0.34894082695245743, Final Batch Loss: 0.18459419906139374\n",
      "Epoch 2008, Loss: 0.3395877778530121, Final Batch Loss: 0.16191455721855164\n",
      "Epoch 2009, Loss: 0.26394955068826675, Final Batch Loss: 0.07031314074993134\n",
      "Epoch 2010, Loss: 0.38374288380146027, Final Batch Loss: 0.2300184816122055\n",
      "Epoch 2011, Loss: 0.21318841725587845, Final Batch Loss: 0.0496242418885231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2012, Loss: 0.31509024649858475, Final Batch Loss: 0.08330514281988144\n",
      "Epoch 2013, Loss: 0.22400564700365067, Final Batch Loss: 0.0681893602013588\n",
      "Epoch 2014, Loss: 0.411967933177948, Final Batch Loss: 0.20190152525901794\n",
      "Epoch 2015, Loss: 0.27254086546599865, Final Batch Loss: 0.025568386539816856\n",
      "Epoch 2016, Loss: 0.17852997221052647, Final Batch Loss: 0.02608138509094715\n",
      "Epoch 2017, Loss: 0.21607529930770397, Final Batch Loss: 0.024358848109841347\n",
      "Epoch 2018, Loss: 0.24565956741571426, Final Batch Loss: 0.09192253649234772\n",
      "Epoch 2019, Loss: 0.25808506831526756, Final Batch Loss: 0.07585060596466064\n",
      "Epoch 2020, Loss: 0.3125079721212387, Final Batch Loss: 0.12256259471178055\n",
      "Epoch 2021, Loss: 0.21637221053242683, Final Batch Loss: 0.08521370589733124\n",
      "Epoch 2022, Loss: 0.27941495180130005, Final Batch Loss: 0.1342107057571411\n",
      "Epoch 2023, Loss: 0.3032873570919037, Final Batch Loss: 0.0727003738284111\n",
      "Epoch 2024, Loss: 0.28199533373117447, Final Batch Loss: 0.09146139770746231\n",
      "Epoch 2025, Loss: 0.2839543968439102, Final Batch Loss: 0.07197564840316772\n",
      "Epoch 2026, Loss: 0.22538870200514793, Final Batch Loss: 0.04971693828701973\n",
      "Epoch 2027, Loss: 0.16837622597813606, Final Batch Loss: 0.04106993228197098\n",
      "Epoch 2028, Loss: 0.23343243077397346, Final Batch Loss: 0.06862287223339081\n",
      "Epoch 2029, Loss: 0.27045863494277, Final Batch Loss: 0.12661443650722504\n",
      "Epoch 2030, Loss: 0.22815970331430435, Final Batch Loss: 0.07518404722213745\n",
      "Epoch 2031, Loss: 0.3760470524430275, Final Batch Loss: 0.19646790623664856\n",
      "Epoch 2032, Loss: 0.28490665555000305, Final Batch Loss: 0.11232805997133255\n",
      "Epoch 2033, Loss: 0.24555955827236176, Final Batch Loss: 0.07963716983795166\n",
      "Epoch 2034, Loss: 0.4816981479525566, Final Batch Loss: 0.304122656583786\n",
      "Epoch 2035, Loss: 0.2356388419866562, Final Batch Loss: 0.08077146857976913\n",
      "Epoch 2036, Loss: 0.2060786671936512, Final Batch Loss: 0.019891154021024704\n",
      "Epoch 2037, Loss: 0.22422780841588974, Final Batch Loss: 0.06371335685253143\n",
      "Epoch 2038, Loss: 0.2913771867752075, Final Batch Loss: 0.08568727970123291\n",
      "Epoch 2039, Loss: 0.2109430767595768, Final Batch Loss: 0.044464558362960815\n",
      "Epoch 2040, Loss: 0.28456463664770126, Final Batch Loss: 0.08273166418075562\n",
      "Epoch 2041, Loss: 0.1622137613594532, Final Batch Loss: 0.021242361515760422\n",
      "Epoch 2042, Loss: 0.24186237156391144, Final Batch Loss: 0.08180361241102219\n",
      "Epoch 2043, Loss: 0.23303811997175217, Final Batch Loss: 0.0683303102850914\n",
      "Epoch 2044, Loss: 0.2741686962544918, Final Batch Loss: 0.10097045451402664\n",
      "Epoch 2045, Loss: 0.18597302213311195, Final Batch Loss: 0.05357816815376282\n",
      "Epoch 2046, Loss: 0.2735012024641037, Final Batch Loss: 0.09058239310979843\n",
      "Epoch 2047, Loss: 0.14959215000271797, Final Batch Loss: 0.02663186937570572\n",
      "Epoch 2048, Loss: 0.18998904526233673, Final Batch Loss: 0.03172443062067032\n",
      "Epoch 2049, Loss: 0.14310823194682598, Final Batch Loss: 0.02673865295946598\n",
      "Epoch 2050, Loss: 0.16091116145253181, Final Batch Loss: 0.03129712864756584\n",
      "Epoch 2051, Loss: 0.18613027781248093, Final Batch Loss: 0.04814399033784866\n",
      "Epoch 2052, Loss: 0.20053314417600632, Final Batch Loss: 0.06896433979272842\n",
      "Epoch 2053, Loss: 0.2046748511493206, Final Batch Loss: 0.03663863241672516\n",
      "Epoch 2054, Loss: 0.25916146859526634, Final Batch Loss: 0.05509645864367485\n",
      "Epoch 2055, Loss: 0.2791312411427498, Final Batch Loss: 0.12268291413784027\n",
      "Epoch 2056, Loss: 0.20876548066735268, Final Batch Loss: 0.04994304105639458\n",
      "Epoch 2057, Loss: 0.286786288022995, Final Batch Loss: 0.1654721051454544\n",
      "Epoch 2058, Loss: 0.21019648760557175, Final Batch Loss: 0.029125958681106567\n",
      "Epoch 2059, Loss: 0.42988988757133484, Final Batch Loss: 0.2052682787179947\n",
      "Epoch 2060, Loss: 0.26875903457403183, Final Batch Loss: 0.12224383652210236\n",
      "Epoch 2061, Loss: 0.30115651711821556, Final Batch Loss: 0.12403862178325653\n",
      "Epoch 2062, Loss: 0.17315837740898132, Final Batch Loss: 0.04114239662885666\n",
      "Epoch 2063, Loss: 0.22310266643762589, Final Batch Loss: 0.05357106029987335\n",
      "Epoch 2064, Loss: 0.26208868622779846, Final Batch Loss: 0.08120556175708771\n",
      "Epoch 2065, Loss: 0.24951130896806717, Final Batch Loss: 0.12227103114128113\n",
      "Epoch 2066, Loss: 0.28669896721839905, Final Batch Loss: 0.07878480106592178\n",
      "Epoch 2067, Loss: 0.29514639638364315, Final Batch Loss: 0.1348477005958557\n",
      "Epoch 2068, Loss: 0.26142458990216255, Final Batch Loss: 0.13509148359298706\n",
      "Epoch 2069, Loss: 0.19926466420292854, Final Batch Loss: 0.07587507367134094\n",
      "Epoch 2070, Loss: 0.18393713608384132, Final Batch Loss: 0.03710126504302025\n",
      "Epoch 2071, Loss: 0.21729669347405434, Final Batch Loss: 0.08536528795957565\n",
      "Epoch 2072, Loss: 0.2465333268046379, Final Batch Loss: 0.0654146745800972\n",
      "Epoch 2073, Loss: 0.18178675323724747, Final Batch Loss: 0.01384936273097992\n",
      "Epoch 2074, Loss: 0.20472846552729607, Final Batch Loss: 0.027393747121095657\n",
      "Epoch 2075, Loss: 0.23824097961187363, Final Batch Loss: 0.05420845001935959\n",
      "Epoch 2076, Loss: 0.37320712953805923, Final Batch Loss: 0.27789580821990967\n",
      "Epoch 2077, Loss: 0.4650004208087921, Final Batch Loss: 0.25616246461868286\n",
      "Epoch 2078, Loss: 0.4107734337449074, Final Batch Loss: 0.16975471377372742\n",
      "Epoch 2079, Loss: 0.32467688620090485, Final Batch Loss: 0.1533229500055313\n",
      "Epoch 2080, Loss: 0.19600088708102703, Final Batch Loss: 0.028100615367293358\n",
      "Epoch 2081, Loss: 0.24783359467983246, Final Batch Loss: 0.09546367824077606\n",
      "Epoch 2082, Loss: 0.2238328494131565, Final Batch Loss: 0.038502778857946396\n",
      "Epoch 2083, Loss: 0.18756103515625, Final Batch Loss: 0.05803200602531433\n",
      "Epoch 2084, Loss: 0.17589664459228516, Final Batch Loss: 0.03849485516548157\n",
      "Epoch 2085, Loss: 0.34191620722413063, Final Batch Loss: 0.2187773734331131\n",
      "Epoch 2086, Loss: 0.2030271515250206, Final Batch Loss: 0.058026332408189774\n",
      "Epoch 2087, Loss: 0.3002004399895668, Final Batch Loss: 0.08515993505716324\n",
      "Epoch 2088, Loss: 0.23837320506572723, Final Batch Loss: 0.0748600959777832\n",
      "Epoch 2089, Loss: 0.20308462157845497, Final Batch Loss: 0.05030214041471481\n",
      "Epoch 2090, Loss: 0.1822434589266777, Final Batch Loss: 0.03575163334608078\n",
      "Epoch 2091, Loss: 0.1874297298491001, Final Batch Loss: 0.04641750454902649\n",
      "Epoch 2092, Loss: 0.2292916513979435, Final Batch Loss: 0.03843182697892189\n",
      "Epoch 2093, Loss: 0.2603137865662575, Final Batch Loss: 0.09778597205877304\n",
      "Epoch 2094, Loss: 0.3187950402498245, Final Batch Loss: 0.1571187525987625\n",
      "Epoch 2095, Loss: 0.35260818153619766, Final Batch Loss: 0.19373269379138947\n",
      "Epoch 2096, Loss: 0.19176264107227325, Final Batch Loss: 0.049969520419836044\n",
      "Epoch 2097, Loss: 0.23266273364424706, Final Batch Loss: 0.06175558641552925\n",
      "Epoch 2098, Loss: 0.2438666895031929, Final Batch Loss: 0.07622001320123672\n",
      "Epoch 2099, Loss: 0.30637626349925995, Final Batch Loss: 0.11441123485565186\n",
      "Epoch 2100, Loss: 0.3074866272509098, Final Batch Loss: 0.19826433062553406\n",
      "Epoch 2101, Loss: 0.2452111393213272, Final Batch Loss: 0.04937213659286499\n",
      "Epoch 2102, Loss: 0.1954360418021679, Final Batch Loss: 0.07524596899747849\n",
      "Epoch 2103, Loss: 0.206657525151968, Final Batch Loss: 0.05669199302792549\n",
      "Epoch 2104, Loss: 0.16311099752783775, Final Batch Loss: 0.03997698798775673\n",
      "Epoch 2105, Loss: 0.1831481046974659, Final Batch Loss: 0.03873199224472046\n",
      "Epoch 2106, Loss: 0.19816084578633308, Final Batch Loss: 0.05596383288502693\n",
      "Epoch 2107, Loss: 0.20040526520460844, Final Batch Loss: 0.014048251323401928\n",
      "Epoch 2108, Loss: 0.13614042475819588, Final Batch Loss: 0.03235867619514465\n",
      "Epoch 2109, Loss: 0.296265110373497, Final Batch Loss: 0.12607353925704956\n",
      "Epoch 2110, Loss: 0.21545147150754929, Final Batch Loss: 0.05204935744404793\n",
      "Epoch 2111, Loss: 0.20041068270802498, Final Batch Loss: 0.04635116085410118\n",
      "Epoch 2112, Loss: 0.24369612336158752, Final Batch Loss: 0.08272234350442886\n",
      "Epoch 2113, Loss: 0.17113809660077095, Final Batch Loss: 0.025059033185243607\n",
      "Epoch 2114, Loss: 0.18694624304771423, Final Batch Loss: 0.031105712056159973\n",
      "Epoch 2115, Loss: 0.25501156225800514, Final Batch Loss: 0.07601381093263626\n",
      "Epoch 2116, Loss: 0.24577711522579193, Final Batch Loss: 0.11999063193798065\n",
      "Epoch 2117, Loss: 0.12571037281304598, Final Batch Loss: 0.013805513270199299\n",
      "Epoch 2118, Loss: 0.2271830439567566, Final Batch Loss: 0.05621205270290375\n",
      "Epoch 2119, Loss: 0.19422991015017033, Final Batch Loss: 0.021973991766572\n",
      "Epoch 2120, Loss: 0.19270027708262205, Final Batch Loss: 0.015040810219943523\n",
      "Epoch 2121, Loss: 0.15389100834727287, Final Batch Loss: 0.03252686560153961\n",
      "Epoch 2122, Loss: 0.1484521683305502, Final Batch Loss: 0.021350188180804253\n",
      "Epoch 2123, Loss: 0.20707060396671295, Final Batch Loss: 0.04886183887720108\n",
      "Epoch 2124, Loss: 0.20096499845385551, Final Batch Loss: 0.09277034550905228\n",
      "Epoch 2125, Loss: 0.25029826164245605, Final Batch Loss: 0.056251250207424164\n",
      "Epoch 2126, Loss: 0.21259700134396553, Final Batch Loss: 0.10460091382265091\n",
      "Epoch 2127, Loss: 0.21493669226765633, Final Batch Loss: 0.03422510251402855\n",
      "Epoch 2128, Loss: 0.18538139387965202, Final Batch Loss: 0.0774572566151619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2129, Loss: 0.4650842510163784, Final Batch Loss: 0.3163939118385315\n",
      "Epoch 2130, Loss: 0.19635134562849998, Final Batch Loss: 0.07749228179454803\n",
      "Epoch 2131, Loss: 0.17274896427989006, Final Batch Loss: 0.01251387968659401\n",
      "Epoch 2132, Loss: 0.30065038800239563, Final Batch Loss: 0.11962906271219254\n",
      "Epoch 2133, Loss: 0.2156880646944046, Final Batch Loss: 0.05110885947942734\n",
      "Epoch 2134, Loss: 0.3492325134575367, Final Batch Loss: 0.19751814007759094\n",
      "Epoch 2135, Loss: 0.19637463241815567, Final Batch Loss: 0.06787629425525665\n",
      "Epoch 2136, Loss: 0.27900052815675735, Final Batch Loss: 0.17049290239810944\n",
      "Epoch 2137, Loss: 0.23945266380906105, Final Batch Loss: 0.05050881579518318\n",
      "Epoch 2138, Loss: 0.3426516652107239, Final Batch Loss: 0.22471125423908234\n",
      "Epoch 2139, Loss: 0.25901835411787033, Final Batch Loss: 0.14053687453269958\n",
      "Epoch 2140, Loss: 0.1859991643577814, Final Batch Loss: 0.02144143171608448\n",
      "Epoch 2141, Loss: 0.3260103687644005, Final Batch Loss: 0.10565156489610672\n",
      "Epoch 2142, Loss: 0.19788466021418571, Final Batch Loss: 0.04740355908870697\n",
      "Epoch 2143, Loss: 0.39324671030044556, Final Batch Loss: 0.137971892952919\n",
      "Epoch 2144, Loss: 0.26278056204319, Final Batch Loss: 0.07577092945575714\n",
      "Epoch 2145, Loss: 0.25648361444473267, Final Batch Loss: 0.10180605202913284\n",
      "Epoch 2146, Loss: 0.18085720762610435, Final Batch Loss: 0.0300760455429554\n",
      "Epoch 2147, Loss: 0.27879934571683407, Final Batch Loss: 0.025097711011767387\n",
      "Epoch 2148, Loss: 0.21060139313340187, Final Batch Loss: 0.04611409455537796\n",
      "Epoch 2149, Loss: 0.2727774940431118, Final Batch Loss: 0.15117929875850677\n",
      "Epoch 2150, Loss: 0.36341099441051483, Final Batch Loss: 0.13480792939662933\n",
      "Epoch 2151, Loss: 0.3247075453400612, Final Batch Loss: 0.10230206698179245\n",
      "Epoch 2152, Loss: 0.3151204362511635, Final Batch Loss: 0.08811905980110168\n",
      "Epoch 2153, Loss: 0.22411296889185905, Final Batch Loss: 0.06925491988658905\n",
      "Epoch 2154, Loss: 0.29313917458057404, Final Batch Loss: 0.12229632586240768\n",
      "Epoch 2155, Loss: 0.23441536724567413, Final Batch Loss: 0.06863745301961899\n",
      "Epoch 2156, Loss: 0.23740210384130478, Final Batch Loss: 0.07203903049230576\n",
      "Epoch 2157, Loss: 0.1847267486155033, Final Batch Loss: 0.044731225818395615\n",
      "Epoch 2158, Loss: 0.20395193994045258, Final Batch Loss: 0.041659481823444366\n",
      "Epoch 2159, Loss: 0.19724825769662857, Final Batch Loss: 0.05310661345720291\n",
      "Epoch 2160, Loss: 0.2662321776151657, Final Batch Loss: 0.11913400888442993\n",
      "Epoch 2161, Loss: 0.15589388273656368, Final Batch Loss: 0.01192503236234188\n",
      "Epoch 2162, Loss: 0.22333846241235733, Final Batch Loss: 0.09197978675365448\n",
      "Epoch 2163, Loss: 0.2628061808645725, Final Batch Loss: 0.08705785125494003\n",
      "Epoch 2164, Loss: 0.2483796551823616, Final Batch Loss: 0.09570890665054321\n",
      "Epoch 2165, Loss: 0.21585314720869064, Final Batch Loss: 0.05918501317501068\n",
      "Epoch 2166, Loss: 0.19223612174391747, Final Batch Loss: 0.03964858874678612\n",
      "Epoch 2167, Loss: 0.2806854099035263, Final Batch Loss: 0.16015984117984772\n",
      "Epoch 2168, Loss: 0.20058564096689224, Final Batch Loss: 0.034658342599868774\n",
      "Epoch 2169, Loss: 0.20099401473999023, Final Batch Loss: 0.0663909986615181\n",
      "Epoch 2170, Loss: 0.30701781436800957, Final Batch Loss: 0.142331063747406\n",
      "Epoch 2171, Loss: 0.26100511476397514, Final Batch Loss: 0.11890172213315964\n",
      "Epoch 2172, Loss: 0.29798029363155365, Final Batch Loss: 0.08636011928319931\n",
      "Epoch 2173, Loss: 0.2063279002904892, Final Batch Loss: 0.06909283250570297\n",
      "Epoch 2174, Loss: 0.262845940887928, Final Batch Loss: 0.09126701205968857\n",
      "Epoch 2175, Loss: 0.30240121856331825, Final Batch Loss: 0.059785012155771255\n",
      "Epoch 2176, Loss: 0.3022969514131546, Final Batch Loss: 0.12707655131816864\n",
      "Epoch 2177, Loss: 0.240836838260293, Final Batch Loss: 0.018341975286602974\n",
      "Epoch 2178, Loss: 0.23067989386618137, Final Batch Loss: 0.021430807188153267\n",
      "Epoch 2179, Loss: 0.20384452119469643, Final Batch Loss: 0.08028249442577362\n",
      "Epoch 2180, Loss: 0.23409424722194672, Final Batch Loss: 0.048147059977054596\n",
      "Epoch 2181, Loss: 0.21178869158029556, Final Batch Loss: 0.06726820766925812\n",
      "Epoch 2182, Loss: 0.25543351471424103, Final Batch Loss: 0.10321452468633652\n",
      "Epoch 2183, Loss: 0.18092241138219833, Final Batch Loss: 0.04037657380104065\n",
      "Epoch 2184, Loss: 0.16070447862148285, Final Batch Loss: 0.038917623460292816\n",
      "Epoch 2185, Loss: 0.2031680904328823, Final Batch Loss: 0.02456183359026909\n",
      "Epoch 2186, Loss: 0.1657124161720276, Final Batch Loss: 0.03890610858798027\n",
      "Epoch 2187, Loss: 0.180769395083189, Final Batch Loss: 0.03221554309129715\n",
      "Epoch 2188, Loss: 0.2033277340233326, Final Batch Loss: 0.062469232827425\n",
      "Epoch 2189, Loss: 0.12721576541662216, Final Batch Loss: 0.03691653534770012\n",
      "Epoch 2190, Loss: 0.2871702276170254, Final Batch Loss: 0.16778519749641418\n",
      "Epoch 2191, Loss: 0.19634466618299484, Final Batch Loss: 0.06879286468029022\n",
      "Epoch 2192, Loss: 0.16914020106196404, Final Batch Loss: 0.03195016458630562\n",
      "Epoch 2193, Loss: 0.2249823659658432, Final Batch Loss: 0.13198700547218323\n",
      "Epoch 2194, Loss: 0.1937055103480816, Final Batch Loss: 0.0769212394952774\n",
      "Epoch 2195, Loss: 0.33122754842042923, Final Batch Loss: 0.16242949664592743\n",
      "Epoch 2196, Loss: 0.2732538804411888, Final Batch Loss: 0.13210664689540863\n",
      "Epoch 2197, Loss: 0.182433245703578, Final Batch Loss: 0.018992433324456215\n",
      "Epoch 2198, Loss: 0.23257578164339066, Final Batch Loss: 0.03071754425764084\n",
      "Epoch 2199, Loss: 0.16848397627472878, Final Batch Loss: 0.03463223949074745\n",
      "Epoch 2200, Loss: 0.18235612660646439, Final Batch Loss: 0.046820078045129776\n",
      "Epoch 2201, Loss: 0.18618565797805786, Final Batch Loss: 0.05777287855744362\n",
      "Epoch 2202, Loss: 0.21949983574450016, Final Batch Loss: 0.02609177492558956\n",
      "Epoch 2203, Loss: 0.24510378390550613, Final Batch Loss: 0.14927569031715393\n",
      "Epoch 2204, Loss: 0.2485051080584526, Final Batch Loss: 0.13525515794754028\n",
      "Epoch 2205, Loss: 0.19620467349886894, Final Batch Loss: 0.06549802422523499\n",
      "Epoch 2206, Loss: 0.20618532598018646, Final Batch Loss: 0.06262392550706863\n",
      "Epoch 2207, Loss: 0.15774184837937355, Final Batch Loss: 0.03680923581123352\n",
      "Epoch 2208, Loss: 0.2620505765080452, Final Batch Loss: 0.09583957493305206\n",
      "Epoch 2209, Loss: 0.18061891943216324, Final Batch Loss: 0.0447489358484745\n",
      "Epoch 2210, Loss: 0.1614729855209589, Final Batch Loss: 0.025165202096104622\n",
      "Epoch 2211, Loss: 0.21627573668956757, Final Batch Loss: 0.07459995150566101\n",
      "Epoch 2212, Loss: 0.31899653375148773, Final Batch Loss: 0.20352937281131744\n",
      "Epoch 2213, Loss: 0.26763223111629486, Final Batch Loss: 0.09180440753698349\n",
      "Epoch 2214, Loss: 0.32505927234888077, Final Batch Loss: 0.14312610030174255\n",
      "Epoch 2215, Loss: 0.14938335493206978, Final Batch Loss: 0.02176298201084137\n",
      "Epoch 2216, Loss: 0.26108548045158386, Final Batch Loss: 0.08220310509204865\n",
      "Epoch 2217, Loss: 0.20797713473439217, Final Batch Loss: 0.013962287455797195\n",
      "Epoch 2218, Loss: 0.28316760063171387, Final Batch Loss: 0.0943303108215332\n",
      "Epoch 2219, Loss: 0.3833319917321205, Final Batch Loss: 0.18534155189990997\n",
      "Epoch 2220, Loss: 0.2136593572795391, Final Batch Loss: 0.07604391127824783\n",
      "Epoch 2221, Loss: 0.21741708368062973, Final Batch Loss: 0.06518667936325073\n",
      "Epoch 2222, Loss: 0.17606057599186897, Final Batch Loss: 0.030589137226343155\n",
      "Epoch 2223, Loss: 0.13900556415319443, Final Batch Loss: 0.038885265588760376\n",
      "Epoch 2224, Loss: 0.19681977666914463, Final Batch Loss: 0.027580363675951958\n",
      "Epoch 2225, Loss: 0.16877028718590736, Final Batch Loss: 0.053127072751522064\n",
      "Epoch 2226, Loss: 0.2533295229077339, Final Batch Loss: 0.1708543747663498\n",
      "Epoch 2227, Loss: 0.2050669975578785, Final Batch Loss: 0.07489454746246338\n",
      "Epoch 2228, Loss: 0.1440208274871111, Final Batch Loss: 0.02750278078019619\n",
      "Epoch 2229, Loss: 0.18569187819957733, Final Batch Loss: 0.03184853494167328\n",
      "Epoch 2230, Loss: 0.15433188807219267, Final Batch Loss: 0.014017677865922451\n",
      "Epoch 2231, Loss: 0.23545174300670624, Final Batch Loss: 0.043246544897556305\n",
      "Epoch 2232, Loss: 0.3287133425474167, Final Batch Loss: 0.14759454131126404\n",
      "Epoch 2233, Loss: 0.20884975045919418, Final Batch Loss: 0.05232646316289902\n",
      "Epoch 2234, Loss: 0.2596758231520653, Final Batch Loss: 0.08052819967269897\n",
      "Epoch 2235, Loss: 0.26981909573078156, Final Batch Loss: 0.12696518003940582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2236, Loss: 0.22718797624111176, Final Batch Loss: 0.04996919631958008\n",
      "Epoch 2237, Loss: 0.20925331115722656, Final Batch Loss: 0.021656282246112823\n",
      "Epoch 2238, Loss: 0.1656672414392233, Final Batch Loss: 0.01140953041613102\n",
      "Epoch 2239, Loss: 0.16672702878713608, Final Batch Loss: 0.05204001069068909\n",
      "Epoch 2240, Loss: 0.21694054454565048, Final Batch Loss: 0.04846944659948349\n",
      "Epoch 2241, Loss: 0.19009065628051758, Final Batch Loss: 0.054915715008974075\n",
      "Epoch 2242, Loss: 0.23852160573005676, Final Batch Loss: 0.12607929110527039\n",
      "Epoch 2243, Loss: 0.1892783958464861, Final Batch Loss: 0.014438623562455177\n",
      "Epoch 2244, Loss: 0.1898702085018158, Final Batch Loss: 0.055203963071107864\n",
      "Epoch 2245, Loss: 0.13442379981279373, Final Batch Loss: 0.0020737983286380768\n",
      "Epoch 2246, Loss: 0.24656328558921814, Final Batch Loss: 0.09927915781736374\n",
      "Epoch 2247, Loss: 0.30037448555231094, Final Batch Loss: 0.13737398386001587\n",
      "Epoch 2248, Loss: 0.2120601423084736, Final Batch Loss: 0.0536615364253521\n",
      "Epoch 2249, Loss: 0.1942342408001423, Final Batch Loss: 0.07816746085882187\n",
      "Epoch 2250, Loss: 0.21686632186174393, Final Batch Loss: 0.07031399756669998\n",
      "Epoch 2251, Loss: 0.23103875666856766, Final Batch Loss: 0.10473596304655075\n",
      "Epoch 2252, Loss: 0.21828066557645798, Final Batch Loss: 0.0444575771689415\n",
      "Epoch 2253, Loss: 0.25647964142262936, Final Batch Loss: 0.03124062530696392\n",
      "Epoch 2254, Loss: 0.19647415727376938, Final Batch Loss: 0.05834278464317322\n",
      "Epoch 2255, Loss: 0.21955376863479614, Final Batch Loss: 0.0735560730099678\n",
      "Epoch 2256, Loss: 0.17015997506678104, Final Batch Loss: 0.029593544080853462\n",
      "Epoch 2257, Loss: 0.2200324349105358, Final Batch Loss: 0.024228829890489578\n",
      "Epoch 2258, Loss: 0.2315259501338005, Final Batch Loss: 0.1090884581208229\n",
      "Epoch 2259, Loss: 0.1921212114393711, Final Batch Loss: 0.06525205820798874\n",
      "Epoch 2260, Loss: 0.28211458027362823, Final Batch Loss: 0.1377677172422409\n",
      "Epoch 2261, Loss: 0.20290223136544228, Final Batch Loss: 0.07588972896337509\n",
      "Epoch 2262, Loss: 0.18143264204263687, Final Batch Loss: 0.05757153779268265\n",
      "Epoch 2263, Loss: 0.3245140053331852, Final Batch Loss: 0.12907686829566956\n",
      "Epoch 2264, Loss: 0.20822791010141373, Final Batch Loss: 0.07084908336400986\n",
      "Epoch 2265, Loss: 0.19201580993831158, Final Batch Loss: 0.021114496514201164\n",
      "Epoch 2266, Loss: 0.2589430958032608, Final Batch Loss: 0.10490039736032486\n",
      "Epoch 2267, Loss: 0.2067215908318758, Final Batch Loss: 0.030685940757393837\n",
      "Epoch 2268, Loss: 0.2493935115635395, Final Batch Loss: 0.0562061183154583\n",
      "Epoch 2269, Loss: 0.2778262123465538, Final Batch Loss: 0.11958371847867966\n",
      "Epoch 2270, Loss: 0.23770765960216522, Final Batch Loss: 0.09243606775999069\n",
      "Epoch 2271, Loss: 0.22456013783812523, Final Batch Loss: 0.0613887645304203\n",
      "Epoch 2272, Loss: 0.4452556222677231, Final Batch Loss: 0.24688343703746796\n",
      "Epoch 2273, Loss: 0.2741607055068016, Final Batch Loss: 0.12303397059440613\n",
      "Epoch 2274, Loss: 0.18867061287164688, Final Batch Loss: 0.04048730805516243\n",
      "Epoch 2275, Loss: 0.24544475972652435, Final Batch Loss: 0.09964707493782043\n",
      "Epoch 2276, Loss: 0.15300661697983742, Final Batch Loss: 0.03285842388868332\n",
      "Epoch 2277, Loss: 0.34113384410738945, Final Batch Loss: 0.17470066249370575\n",
      "Epoch 2278, Loss: 0.2476390339434147, Final Batch Loss: 0.0793549120426178\n",
      "Epoch 2279, Loss: 0.21757909283041954, Final Batch Loss: 0.04328704997897148\n",
      "Epoch 2280, Loss: 0.2774367332458496, Final Batch Loss: 0.1163015142083168\n",
      "Epoch 2281, Loss: 0.21104690432548523, Final Batch Loss: 0.04829038679599762\n",
      "Epoch 2282, Loss: 0.20454102009534836, Final Batch Loss: 0.06873314827680588\n",
      "Epoch 2283, Loss: 0.13069393392652273, Final Batch Loss: 0.014560974203050137\n",
      "Epoch 2284, Loss: 0.17031806241720915, Final Batch Loss: 0.007983672432601452\n",
      "Epoch 2285, Loss: 0.171156607568264, Final Batch Loss: 0.05492643639445305\n",
      "Epoch 2286, Loss: 0.24105611443519592, Final Batch Loss: 0.12327292561531067\n",
      "Epoch 2287, Loss: 0.14239712059497833, Final Batch Loss: 0.03280213847756386\n",
      "Epoch 2288, Loss: 0.2512873373925686, Final Batch Loss: 0.11517315357923508\n",
      "Epoch 2289, Loss: 0.25809551030397415, Final Batch Loss: 0.14824749529361725\n",
      "Epoch 2290, Loss: 0.20739703625440598, Final Batch Loss: 0.07898224890232086\n",
      "Epoch 2291, Loss: 0.2566331662237644, Final Batch Loss: 0.12148024886846542\n",
      "Epoch 2292, Loss: 0.16949906013906002, Final Batch Loss: 0.03026779554784298\n",
      "Epoch 2293, Loss: 0.16849185526371002, Final Batch Loss: 0.02205614745616913\n",
      "Epoch 2294, Loss: 0.19057423435151577, Final Batch Loss: 0.021130284294486046\n",
      "Epoch 2295, Loss: 0.21877792850136757, Final Batch Loss: 0.11561212688684464\n",
      "Epoch 2296, Loss: 0.147734384983778, Final Batch Loss: 0.0530196875333786\n",
      "Epoch 2297, Loss: 0.2509015277028084, Final Batch Loss: 0.11825322359800339\n",
      "Epoch 2298, Loss: 0.13307781517505646, Final Batch Loss: 0.01854632794857025\n",
      "Epoch 2299, Loss: 0.22957739979028702, Final Batch Loss: 0.07228457927703857\n",
      "Epoch 2300, Loss: 0.17551717162132263, Final Batch Loss: 0.05387762188911438\n",
      "Epoch 2301, Loss: 0.16022517904639244, Final Batch Loss: 0.04479241371154785\n",
      "Epoch 2302, Loss: 0.2414986528456211, Final Batch Loss: 0.10940563678741455\n",
      "Epoch 2303, Loss: 0.2559339478611946, Final Batch Loss: 0.05903305858373642\n",
      "Epoch 2304, Loss: 0.14786922186613083, Final Batch Loss: 0.021739095449447632\n",
      "Epoch 2305, Loss: 0.19600604102015495, Final Batch Loss: 0.09989060461521149\n",
      "Epoch 2306, Loss: 0.22453651949763298, Final Batch Loss: 0.09250310808420181\n",
      "Epoch 2307, Loss: 0.15762828662991524, Final Batch Loss: 0.05733010172843933\n",
      "Epoch 2308, Loss: 0.18111707642674446, Final Batch Loss: 0.04312007874250412\n",
      "Epoch 2309, Loss: 0.19828665256500244, Final Batch Loss: 0.07513859122991562\n",
      "Epoch 2310, Loss: 0.1545627899467945, Final Batch Loss: 0.04363710805773735\n",
      "Epoch 2311, Loss: 0.28008396178483963, Final Batch Loss: 0.11354626715183258\n",
      "Epoch 2312, Loss: 0.14878613501787186, Final Batch Loss: 0.03484896197915077\n",
      "Epoch 2313, Loss: 0.2656971290707588, Final Batch Loss: 0.1697664111852646\n",
      "Epoch 2314, Loss: 0.19848430901765823, Final Batch Loss: 0.0371750071644783\n",
      "Epoch 2315, Loss: 0.2632489986717701, Final Batch Loss: 0.11941668391227722\n",
      "Epoch 2316, Loss: 0.16352621093392372, Final Batch Loss: 0.05823567509651184\n",
      "Epoch 2317, Loss: 0.16575074195861816, Final Batch Loss: 0.03764055669307709\n",
      "Epoch 2318, Loss: 0.2297307513654232, Final Batch Loss: 0.08686553686857224\n",
      "Epoch 2319, Loss: 0.2075827345252037, Final Batch Loss: 0.03159526735544205\n",
      "Epoch 2320, Loss: 0.3101900964975357, Final Batch Loss: 0.1545158326625824\n",
      "Epoch 2321, Loss: 0.21240691095590591, Final Batch Loss: 0.06526888906955719\n",
      "Epoch 2322, Loss: 0.22240496426820755, Final Batch Loss: 0.13154757022857666\n",
      "Epoch 2323, Loss: 0.11040481645613909, Final Batch Loss: 0.013190180994570255\n",
      "Epoch 2324, Loss: 0.20643533766269684, Final Batch Loss: 0.08666308969259262\n",
      "Epoch 2325, Loss: 0.19770149886608124, Final Batch Loss: 0.06431197375059128\n",
      "Epoch 2326, Loss: 0.3444942645728588, Final Batch Loss: 0.2247656136751175\n",
      "Epoch 2327, Loss: 0.27541838586330414, Final Batch Loss: 0.14850060641765594\n",
      "Epoch 2328, Loss: 0.20998413860797882, Final Batch Loss: 0.05430540814995766\n",
      "Epoch 2329, Loss: 0.14695321768522263, Final Batch Loss: 0.019918661564588547\n",
      "Epoch 2330, Loss: 0.23090660944581032, Final Batch Loss: 0.10664408653974533\n",
      "Epoch 2331, Loss: 0.2205735445022583, Final Batch Loss: 0.06627459824085236\n",
      "Epoch 2332, Loss: 0.21869883686304092, Final Batch Loss: 0.0393623523414135\n",
      "Epoch 2333, Loss: 0.2462383657693863, Final Batch Loss: 0.13080771267414093\n",
      "Epoch 2334, Loss: 0.13500518910586834, Final Batch Loss: 0.030360756441950798\n",
      "Epoch 2335, Loss: 0.24392425268888474, Final Batch Loss: 0.09648220986127853\n",
      "Epoch 2336, Loss: 0.17056048288941383, Final Batch Loss: 0.06391969323158264\n",
      "Epoch 2337, Loss: 0.24832958728075027, Final Batch Loss: 0.10376611351966858\n",
      "Epoch 2338, Loss: 0.2172035463154316, Final Batch Loss: 0.05295110121369362\n",
      "Epoch 2339, Loss: 0.17051199823617935, Final Batch Loss: 0.047660090029239655\n",
      "Epoch 2340, Loss: 0.15260536456480622, Final Batch Loss: 0.004439378622919321\n",
      "Epoch 2341, Loss: 0.15406448021531105, Final Batch Loss: 0.048827141523361206\n",
      "Epoch 2342, Loss: 0.28143302351236343, Final Batch Loss: 0.13665130734443665\n",
      "Epoch 2343, Loss: 0.3155531510710716, Final Batch Loss: 0.182205468416214\n",
      "Epoch 2344, Loss: 0.16739080473780632, Final Batch Loss: 0.028256669640541077\n",
      "Epoch 2345, Loss: 0.2436479739844799, Final Batch Loss: 0.1382230520248413\n",
      "Epoch 2346, Loss: 0.19226771220564842, Final Batch Loss: 0.07977046817541122\n",
      "Epoch 2347, Loss: 0.15271950140595436, Final Batch Loss: 0.04356212541460991\n",
      "Epoch 2348, Loss: 0.35980626940727234, Final Batch Loss: 0.1615820676088333\n",
      "Epoch 2349, Loss: 0.34377315640449524, Final Batch Loss: 0.13079796731472015\n",
      "Epoch 2350, Loss: 0.21453706547617912, Final Batch Loss: 0.03367796912789345\n",
      "Epoch 2351, Loss: 0.2504762150347233, Final Batch Loss: 0.045030903071165085\n",
      "Epoch 2352, Loss: 0.25817711278796196, Final Batch Loss: 0.054480310529470444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2353, Loss: 0.3629884570837021, Final Batch Loss: 0.21064762771129608\n",
      "Epoch 2354, Loss: 0.16159388422966003, Final Batch Loss: 0.06035201624035835\n",
      "Epoch 2355, Loss: 0.19738059490919113, Final Batch Loss: 0.039541974663734436\n",
      "Epoch 2356, Loss: 0.16136628575623035, Final Batch Loss: 0.02345753274857998\n",
      "Epoch 2357, Loss: 0.16537223756313324, Final Batch Loss: 0.047988664358854294\n",
      "Epoch 2358, Loss: 0.2360384538769722, Final Batch Loss: 0.10411398857831955\n",
      "Epoch 2359, Loss: 0.19813904725015163, Final Batch Loss: 0.11980405449867249\n",
      "Epoch 2360, Loss: 0.21388160064816475, Final Batch Loss: 0.1287631243467331\n",
      "Epoch 2361, Loss: 0.17658735811710358, Final Batch Loss: 0.045513562858104706\n",
      "Epoch 2362, Loss: 0.24765341728925705, Final Batch Loss: 0.07196033746004105\n",
      "Epoch 2363, Loss: 0.15140093117952347, Final Batch Loss: 0.030531637370586395\n",
      "Epoch 2364, Loss: 0.2199634499847889, Final Batch Loss: 0.07604435086250305\n",
      "Epoch 2365, Loss: 0.2727822810411453, Final Batch Loss: 0.11585411429405212\n",
      "Epoch 2366, Loss: 0.27037536539137363, Final Batch Loss: 0.09088566154241562\n",
      "Epoch 2367, Loss: 0.2004101760685444, Final Batch Loss: 0.0858277678489685\n",
      "Epoch 2368, Loss: 0.3345313146710396, Final Batch Loss: 0.1309186965227127\n",
      "Epoch 2369, Loss: 0.22913327813148499, Final Batch Loss: 0.12822255492210388\n",
      "Epoch 2370, Loss: 0.21211982518434525, Final Batch Loss: 0.06245800852775574\n",
      "Epoch 2371, Loss: 0.255991667509079, Final Batch Loss: 0.06667371094226837\n",
      "Epoch 2372, Loss: 0.33928027749061584, Final Batch Loss: 0.09974279254674911\n",
      "Epoch 2373, Loss: 0.23132891207933426, Final Batch Loss: 0.06791984289884567\n",
      "Epoch 2374, Loss: 0.31223737075924873, Final Batch Loss: 0.11095088720321655\n",
      "Epoch 2375, Loss: 0.16685493662953377, Final Batch Loss: 0.03429794684052467\n",
      "Epoch 2376, Loss: 0.3100680559873581, Final Batch Loss: 0.12089027464389801\n",
      "Epoch 2377, Loss: 0.14664647355675697, Final Batch Loss: 0.06245352700352669\n",
      "Epoch 2378, Loss: 0.19941923767328262, Final Batch Loss: 0.060454368591308594\n",
      "Epoch 2379, Loss: 0.22146540507674217, Final Batch Loss: 0.044227104634046555\n",
      "Epoch 2380, Loss: 0.21834926679730415, Final Batch Loss: 0.03305068239569664\n",
      "Epoch 2381, Loss: 0.15450971759855747, Final Batch Loss: 0.022401606664061546\n",
      "Epoch 2382, Loss: 0.17104347795248032, Final Batch Loss: 0.013629622757434845\n",
      "Epoch 2383, Loss: 0.20073742792010307, Final Batch Loss: 0.07525826245546341\n",
      "Epoch 2384, Loss: 0.17649134248495102, Final Batch Loss: 0.05606807395815849\n",
      "Epoch 2385, Loss: 0.20577086135745049, Final Batch Loss: 0.019462082535028458\n",
      "Epoch 2386, Loss: 0.2120009995996952, Final Batch Loss: 0.0952099934220314\n",
      "Epoch 2387, Loss: 0.16669823974370956, Final Batch Loss: 0.04878058284521103\n",
      "Epoch 2388, Loss: 0.1927098073065281, Final Batch Loss: 0.0939873605966568\n",
      "Epoch 2389, Loss: 0.10862317122519016, Final Batch Loss: 0.015687720850110054\n",
      "Epoch 2390, Loss: 0.1451113810762763, Final Batch Loss: 0.0023669442161917686\n",
      "Epoch 2391, Loss: 0.21178004145622253, Final Batch Loss: 0.10157767683267593\n",
      "Epoch 2392, Loss: 0.3294479176402092, Final Batch Loss: 0.20402482151985168\n",
      "Epoch 2393, Loss: 0.29293814301490784, Final Batch Loss: 0.11315145343542099\n",
      "Epoch 2394, Loss: 0.3401070758700371, Final Batch Loss: 0.151539608836174\n",
      "Epoch 2395, Loss: 0.1270524114370346, Final Batch Loss: 0.010037139058113098\n",
      "Epoch 2396, Loss: 0.19841867685317993, Final Batch Loss: 0.022398993372917175\n",
      "Epoch 2397, Loss: 0.10877967067062855, Final Batch Loss: 0.03177332133054733\n",
      "Epoch 2398, Loss: 0.2254357412457466, Final Batch Loss: 0.054180920124053955\n",
      "Epoch 2399, Loss: 0.2141458224505186, Final Batch Loss: 0.028926702216267586\n",
      "Epoch 2400, Loss: 0.1684691533446312, Final Batch Loss: 0.05713169276714325\n",
      "Epoch 2401, Loss: 0.2848430350422859, Final Batch Loss: 0.1720941960811615\n",
      "Epoch 2402, Loss: 0.16255655139684677, Final Batch Loss: 0.04288622736930847\n",
      "Epoch 2403, Loss: 0.17271510884165764, Final Batch Loss: 0.05231057107448578\n",
      "Epoch 2404, Loss: 0.2497016154229641, Final Batch Loss: 0.09410098940134048\n",
      "Epoch 2405, Loss: 0.2367248311638832, Final Batch Loss: 0.13998374342918396\n",
      "Epoch 2406, Loss: 0.16714791394770145, Final Batch Loss: 0.012644128873944283\n",
      "Epoch 2407, Loss: 0.19918173551559448, Final Batch Loss: 0.05728054419159889\n",
      "Epoch 2408, Loss: 0.1414186879992485, Final Batch Loss: 0.04169731214642525\n",
      "Epoch 2409, Loss: 0.2845984995365143, Final Batch Loss: 0.18298177421092987\n",
      "Epoch 2410, Loss: 0.15083754807710648, Final Batch Loss: 0.04301689192652702\n",
      "Epoch 2411, Loss: 0.24768781661987305, Final Batch Loss: 0.08441436290740967\n",
      "Epoch 2412, Loss: 0.16358093172311783, Final Batch Loss: 0.05148052051663399\n",
      "Epoch 2413, Loss: 0.2893490567803383, Final Batch Loss: 0.13232164084911346\n",
      "Epoch 2414, Loss: 0.1993774063885212, Final Batch Loss: 0.04890957474708557\n",
      "Epoch 2415, Loss: 0.19668646156787872, Final Batch Loss: 0.015454649925231934\n",
      "Epoch 2416, Loss: 0.27687037736177444, Final Batch Loss: 0.17020247876644135\n",
      "Epoch 2417, Loss: 0.25864697620272636, Final Batch Loss: 0.1636279970407486\n",
      "Epoch 2418, Loss: 0.2544563375413418, Final Batch Loss: 0.09431593865156174\n",
      "Epoch 2419, Loss: 0.16260994970798492, Final Batch Loss: 0.0422833189368248\n",
      "Epoch 2420, Loss: 0.1899929065257311, Final Batch Loss: 0.02106173150241375\n",
      "Epoch 2421, Loss: 0.12347524054348469, Final Batch Loss: 0.013420889154076576\n",
      "Epoch 2422, Loss: 0.22417466342449188, Final Batch Loss: 0.07666092365980148\n",
      "Epoch 2423, Loss: 0.17759904079139233, Final Batch Loss: 0.030213503167033195\n",
      "Epoch 2424, Loss: 0.42414863780140877, Final Batch Loss: 0.29080498218536377\n",
      "Epoch 2425, Loss: 0.20956266298890114, Final Batch Loss: 0.06892108172178268\n",
      "Epoch 2426, Loss: 0.16138839721679688, Final Batch Loss: 0.022822335362434387\n",
      "Epoch 2427, Loss: 0.31821779906749725, Final Batch Loss: 0.16415587067604065\n",
      "Epoch 2428, Loss: 0.16465522721409798, Final Batch Loss: 0.04580453038215637\n",
      "Epoch 2429, Loss: 0.16789653152227402, Final Batch Loss: 0.02421734854578972\n",
      "Epoch 2430, Loss: 0.19783272221684456, Final Batch Loss: 0.09875957667827606\n",
      "Epoch 2431, Loss: 0.12567511294037104, Final Batch Loss: 0.013790418393909931\n",
      "Epoch 2432, Loss: 0.1864249836653471, Final Batch Loss: 0.022458186373114586\n",
      "Epoch 2433, Loss: 0.18581871129572392, Final Batch Loss: 0.011254752054810524\n",
      "Epoch 2434, Loss: 0.24435662478208542, Final Batch Loss: 0.08474793285131454\n",
      "Epoch 2435, Loss: 0.22053451091051102, Final Batch Loss: 0.06419998407363892\n",
      "Epoch 2436, Loss: 0.17464620620012283, Final Batch Loss: 0.04151067137718201\n",
      "Epoch 2437, Loss: 0.2226659134030342, Final Batch Loss: 0.08597586303949356\n",
      "Epoch 2438, Loss: 0.12390203401446342, Final Batch Loss: 0.0183042474091053\n",
      "Epoch 2439, Loss: 0.10418691439554095, Final Batch Loss: 0.0071985418908298016\n",
      "Epoch 2440, Loss: 0.27592823281884193, Final Batch Loss: 0.14825822412967682\n",
      "Epoch 2441, Loss: 0.1807055063545704, Final Batch Loss: 0.035445380955934525\n",
      "Epoch 2442, Loss: 0.15794367715716362, Final Batch Loss: 0.025243602693080902\n",
      "Epoch 2443, Loss: 0.2814484164118767, Final Batch Loss: 0.10256415605545044\n",
      "Epoch 2444, Loss: 0.3579879030585289, Final Batch Loss: 0.2081092745065689\n",
      "Epoch 2445, Loss: 0.13125908188521862, Final Batch Loss: 0.017850665375590324\n",
      "Epoch 2446, Loss: 0.2526145987212658, Final Batch Loss: 0.14598660171031952\n",
      "Epoch 2447, Loss: 0.3144630938768387, Final Batch Loss: 0.16895847022533417\n",
      "Epoch 2448, Loss: 0.2848965674638748, Final Batch Loss: 0.07532262802124023\n",
      "Epoch 2449, Loss: 0.19878370687365532, Final Batch Loss: 0.0522741936147213\n",
      "Epoch 2450, Loss: 0.15691538155078888, Final Batch Loss: 0.03288726136088371\n",
      "Epoch 2451, Loss: 0.2757516838610172, Final Batch Loss: 0.12546002864837646\n",
      "Epoch 2452, Loss: 0.15360742062330246, Final Batch Loss: 0.028588809072971344\n",
      "Epoch 2453, Loss: 0.1641473025083542, Final Batch Loss: 0.05600357428193092\n",
      "Epoch 2454, Loss: 0.20783383399248123, Final Batch Loss: 0.11768875271081924\n",
      "Epoch 2455, Loss: 0.18189836293458939, Final Batch Loss: 0.06040867790579796\n",
      "Epoch 2456, Loss: 0.3141682296991348, Final Batch Loss: 0.1698092222213745\n",
      "Epoch 2457, Loss: 0.19403104111552238, Final Batch Loss: 0.05500010401010513\n",
      "Epoch 2458, Loss: 0.17121047712862492, Final Batch Loss: 0.020121412351727486\n",
      "Epoch 2459, Loss: 0.3551560565829277, Final Batch Loss: 0.21123771369457245\n",
      "Epoch 2460, Loss: 0.14075937494635582, Final Batch Loss: 0.03308361768722534\n",
      "Epoch 2461, Loss: 0.2868608236312866, Final Batch Loss: 0.10062213987112045\n",
      "Epoch 2462, Loss: 0.1692419219762087, Final Batch Loss: 0.025723638013005257\n",
      "Epoch 2463, Loss: 0.27324915677309036, Final Batch Loss: 0.10235599428415298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2464, Loss: 0.18144497647881508, Final Batch Loss: 0.03544163703918457\n",
      "Epoch 2465, Loss: 0.1900781663134694, Final Batch Loss: 0.012783479876816273\n",
      "Epoch 2466, Loss: 0.22626113891601562, Final Batch Loss: 0.09876639395952225\n",
      "Epoch 2467, Loss: 0.1633838638663292, Final Batch Loss: 0.04898802936077118\n",
      "Epoch 2468, Loss: 0.2394465133547783, Final Batch Loss: 0.10357178747653961\n",
      "Epoch 2469, Loss: 0.21558959409594536, Final Batch Loss: 0.08367311209440231\n",
      "Epoch 2470, Loss: 0.2175532728433609, Final Batch Loss: 0.07291226834058762\n",
      "Epoch 2471, Loss: 0.2208448387682438, Final Batch Loss: 0.06444136053323746\n",
      "Epoch 2472, Loss: 0.17403587326407433, Final Batch Loss: 0.04585743322968483\n",
      "Epoch 2473, Loss: 0.19554978609085083, Final Batch Loss: 0.06693757325410843\n",
      "Epoch 2474, Loss: 0.15252472832798958, Final Batch Loss: 0.040675945580005646\n",
      "Epoch 2475, Loss: 0.1737125664949417, Final Batch Loss: 0.020192451775074005\n",
      "Epoch 2476, Loss: 0.20207328349351883, Final Batch Loss: 0.041089192032814026\n",
      "Epoch 2477, Loss: 0.2161257192492485, Final Batch Loss: 0.050429873168468475\n",
      "Epoch 2478, Loss: 0.1767675131559372, Final Batch Loss: 0.0837383046746254\n",
      "Epoch 2479, Loss: 0.11187866982072592, Final Batch Loss: 0.006811893545091152\n",
      "Epoch 2480, Loss: 0.18204529955983162, Final Batch Loss: 0.04504409804940224\n",
      "Epoch 2481, Loss: 0.17350853607058525, Final Batch Loss: 0.046223729848861694\n",
      "Epoch 2482, Loss: 0.13703852519392967, Final Batch Loss: 0.014698546379804611\n",
      "Epoch 2483, Loss: 0.17362762987613678, Final Batch Loss: 0.025637947022914886\n",
      "Epoch 2484, Loss: 0.1478700153529644, Final Batch Loss: 0.0452226921916008\n",
      "Epoch 2485, Loss: 0.3355155326426029, Final Batch Loss: 0.23308071494102478\n",
      "Epoch 2486, Loss: 0.16164212115108967, Final Batch Loss: 0.022814208641648293\n",
      "Epoch 2487, Loss: 0.12972807697951794, Final Batch Loss: 0.020498191937804222\n",
      "Epoch 2488, Loss: 0.18332406133413315, Final Batch Loss: 0.06095008924603462\n",
      "Epoch 2489, Loss: 0.26573893427848816, Final Batch Loss: 0.10519677400588989\n",
      "Epoch 2490, Loss: 0.12767047993838787, Final Batch Loss: 0.011135922744870186\n",
      "Epoch 2491, Loss: 0.2813498303294182, Final Batch Loss: 0.09480582922697067\n",
      "Epoch 2492, Loss: 0.1931045656092465, Final Batch Loss: 0.006357506383210421\n",
      "Epoch 2493, Loss: 0.23812663555145264, Final Batch Loss: 0.0805840864777565\n",
      "Epoch 2494, Loss: 0.1959380842745304, Final Batch Loss: 0.07746831327676773\n",
      "Epoch 2495, Loss: 0.1718415841460228, Final Batch Loss: 0.05678655207157135\n",
      "Epoch 2496, Loss: 0.21438398212194443, Final Batch Loss: 0.07044879347085953\n",
      "Epoch 2497, Loss: 0.15611127205193043, Final Batch Loss: 0.025904780253767967\n",
      "Epoch 2498, Loss: 0.12280423194169998, Final Batch Loss: 0.01914946734905243\n",
      "Epoch 2499, Loss: 0.13220253586769104, Final Batch Loss: 0.035906486213207245\n",
      "Epoch 2500, Loss: 0.13045964390039444, Final Batch Loss: 0.016490601003170013\n",
      "Epoch 2501, Loss: 0.185398381203413, Final Batch Loss: 0.051448386162519455\n",
      "Epoch 2502, Loss: 0.19225719571113586, Final Batch Loss: 0.018811017274856567\n",
      "Epoch 2503, Loss: 0.2367924004793167, Final Batch Loss: 0.09739720821380615\n",
      "Epoch 2504, Loss: 0.14922795817255974, Final Batch Loss: 0.06919115781784058\n",
      "Epoch 2505, Loss: 0.17835663259029388, Final Batch Loss: 0.08437320590019226\n",
      "Epoch 2506, Loss: 0.23899266868829727, Final Batch Loss: 0.06361667066812515\n",
      "Epoch 2507, Loss: 0.21049759164452553, Final Batch Loss: 0.08131294697523117\n",
      "Epoch 2508, Loss: 0.19559389725327492, Final Batch Loss: 0.04293741285800934\n",
      "Epoch 2509, Loss: 0.1912815347313881, Final Batch Loss: 0.10142455250024796\n",
      "Epoch 2510, Loss: 0.2119424007833004, Final Batch Loss: 0.052759330719709396\n",
      "Epoch 2511, Loss: 0.19634284265339375, Final Batch Loss: 0.09774694591760635\n",
      "Epoch 2512, Loss: 0.1957523189485073, Final Batch Loss: 0.048775091767311096\n",
      "Epoch 2513, Loss: 0.24076804518699646, Final Batch Loss: 0.09412263333797455\n",
      "Epoch 2514, Loss: 0.20918653532862663, Final Batch Loss: 0.017182957381010056\n",
      "Epoch 2515, Loss: 0.24898894131183624, Final Batch Loss: 0.16247308254241943\n",
      "Epoch 2516, Loss: 0.24668332934379578, Final Batch Loss: 0.09107181429862976\n",
      "Epoch 2517, Loss: 0.15537417121231556, Final Batch Loss: 0.07092468440532684\n",
      "Epoch 2518, Loss: 0.1171708032488823, Final Batch Loss: 0.025257103145122528\n",
      "Epoch 2519, Loss: 0.16568740084767342, Final Batch Loss: 0.04328959435224533\n",
      "Epoch 2520, Loss: 0.18150658905506134, Final Batch Loss: 0.07746821641921997\n",
      "Epoch 2521, Loss: 0.18829181790351868, Final Batch Loss: 0.09353073686361313\n",
      "Epoch 2522, Loss: 0.1533953696489334, Final Batch Loss: 0.050441451370716095\n",
      "Epoch 2523, Loss: 0.2178138680756092, Final Batch Loss: 0.11639077216386795\n",
      "Epoch 2524, Loss: 0.15105110593140125, Final Batch Loss: 0.02462182007730007\n",
      "Epoch 2525, Loss: 0.19067326188087463, Final Batch Loss: 0.056380659341812134\n",
      "Epoch 2526, Loss: 0.3237873986363411, Final Batch Loss: 0.2554912269115448\n",
      "Epoch 2527, Loss: 0.19678755663335323, Final Batch Loss: 0.10502471774816513\n",
      "Epoch 2528, Loss: 0.1601603226736188, Final Batch Loss: 0.012629029341042042\n",
      "Epoch 2529, Loss: 0.24808750301599503, Final Batch Loss: 0.14074239134788513\n",
      "Epoch 2530, Loss: 0.1691175363957882, Final Batch Loss: 0.033915985375642776\n",
      "Epoch 2531, Loss: 0.1982006672769785, Final Batch Loss: 0.0057225096970796585\n",
      "Epoch 2532, Loss: 0.2244078814983368, Final Batch Loss: 0.08004578202962875\n",
      "Epoch 2533, Loss: 0.1828976795077324, Final Batch Loss: 0.03851260989904404\n",
      "Epoch 2534, Loss: 0.1834445372223854, Final Batch Loss: 0.029666773974895477\n",
      "Epoch 2535, Loss: 0.2492261417210102, Final Batch Loss: 0.10422410070896149\n",
      "Epoch 2536, Loss: 0.2013782113790512, Final Batch Loss: 0.11254625022411346\n",
      "Epoch 2537, Loss: 0.26181986927986145, Final Batch Loss: 0.11312514543533325\n",
      "Epoch 2538, Loss: 0.22785209119319916, Final Batch Loss: 0.06491977721452713\n",
      "Epoch 2539, Loss: 0.1435001753270626, Final Batch Loss: 0.03475074842572212\n",
      "Epoch 2540, Loss: 0.17701463401317596, Final Batch Loss: 0.05182565003633499\n",
      "Epoch 2541, Loss: 0.2788776382803917, Final Batch Loss: 0.08319755643606186\n",
      "Epoch 2542, Loss: 0.25725578516721725, Final Batch Loss: 0.10907728224992752\n",
      "Epoch 2543, Loss: 0.16294627077877522, Final Batch Loss: 0.012558484449982643\n",
      "Epoch 2544, Loss: 0.1800878532230854, Final Batch Loss: 0.03261924907565117\n",
      "Epoch 2545, Loss: 0.21661441028118134, Final Batch Loss: 0.08136947453022003\n",
      "Epoch 2546, Loss: 0.3174620270729065, Final Batch Loss: 0.10906222462654114\n",
      "Epoch 2547, Loss: 0.14467287063598633, Final Batch Loss: 0.014023348689079285\n",
      "Epoch 2548, Loss: 0.17206783220171928, Final Batch Loss: 0.06455754488706589\n",
      "Epoch 2549, Loss: 0.13222810626029968, Final Batch Loss: 0.044844940304756165\n",
      "Epoch 2550, Loss: 0.104258568957448, Final Batch Loss: 0.015996495261788368\n",
      "Epoch 2551, Loss: 0.17883994430303574, Final Batch Loss: 0.06823242455720901\n",
      "Epoch 2552, Loss: 0.16911540180444717, Final Batch Loss: 0.03646139055490494\n",
      "Epoch 2553, Loss: 0.177496500313282, Final Batch Loss: 0.042767200618982315\n",
      "Epoch 2554, Loss: 0.15738634392619133, Final Batch Loss: 0.05567362159490585\n",
      "Epoch 2555, Loss: 0.25153474509716034, Final Batch Loss: 0.0523470938205719\n",
      "Epoch 2556, Loss: 0.19081323221325874, Final Batch Loss: 0.07204849272966385\n",
      "Epoch 2557, Loss: 0.23864448815584183, Final Batch Loss: 0.09745127707719803\n",
      "Epoch 2558, Loss: 0.14773183688521385, Final Batch Loss: 0.03705410286784172\n",
      "Epoch 2559, Loss: 0.1555069498717785, Final Batch Loss: 0.034912437200546265\n",
      "Epoch 2560, Loss: 0.15329374372959137, Final Batch Loss: 0.03811362758278847\n",
      "Epoch 2561, Loss: 0.12984726205468178, Final Batch Loss: 0.03829348459839821\n",
      "Epoch 2562, Loss: 0.16165855899453163, Final Batch Loss: 0.06188291311264038\n",
      "Epoch 2563, Loss: 0.1507259439677, Final Batch Loss: 0.030673032626509666\n",
      "Epoch 2564, Loss: 0.09583803825080395, Final Batch Loss: 0.024353602901101112\n",
      "Epoch 2565, Loss: 0.19259952381253242, Final Batch Loss: 0.07943771034479141\n",
      "Epoch 2566, Loss: 0.11736002191901207, Final Batch Loss: 0.03567824512720108\n",
      "Epoch 2567, Loss: 0.19074076786637306, Final Batch Loss: 0.04064341261982918\n",
      "Epoch 2568, Loss: 0.18615109100937843, Final Batch Loss: 0.10738688707351685\n",
      "Epoch 2569, Loss: 0.14419739693403244, Final Batch Loss: 0.03005433827638626\n",
      "Epoch 2570, Loss: 0.22300420328974724, Final Batch Loss: 0.0476030595600605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2571, Loss: 0.1308146845549345, Final Batch Loss: 0.03791382908821106\n",
      "Epoch 2572, Loss: 0.10986100137233734, Final Batch Loss: 0.021283496171236038\n",
      "Epoch 2573, Loss: 0.20850838720798492, Final Batch Loss: 0.10423802584409714\n",
      "Epoch 2574, Loss: 0.2156769149005413, Final Batch Loss: 0.12836457788944244\n",
      "Epoch 2575, Loss: 0.2951259985566139, Final Batch Loss: 0.12947233021259308\n",
      "Epoch 2576, Loss: 0.289105124771595, Final Batch Loss: 0.12791448831558228\n",
      "Epoch 2577, Loss: 0.16475342586636543, Final Batch Loss: 0.04930529743432999\n",
      "Epoch 2578, Loss: 0.2325051687657833, Final Batch Loss: 0.05766160413622856\n",
      "Epoch 2579, Loss: 0.1695771161466837, Final Batch Loss: 0.025586767122149467\n",
      "Epoch 2580, Loss: 0.2605939246714115, Final Batch Loss: 0.10817140340805054\n",
      "Epoch 2581, Loss: 0.11100332718342543, Final Batch Loss: 0.009003783576190472\n",
      "Epoch 2582, Loss: 0.2709154635667801, Final Batch Loss: 0.12908875942230225\n",
      "Epoch 2583, Loss: 0.1883784607052803, Final Batch Loss: 0.03760164976119995\n",
      "Epoch 2584, Loss: 0.25171156600117683, Final Batch Loss: 0.1001976951956749\n",
      "Epoch 2585, Loss: 0.2129220925271511, Final Batch Loss: 0.08678560703992844\n",
      "Epoch 2586, Loss: 0.13426741026341915, Final Batch Loss: 0.027462845668196678\n",
      "Epoch 2587, Loss: 0.24686867743730545, Final Batch Loss: 0.14612548053264618\n",
      "Epoch 2588, Loss: 0.16454484686255455, Final Batch Loss: 0.014509279280900955\n",
      "Epoch 2589, Loss: 0.28540895879268646, Final Batch Loss: 0.10209190100431442\n",
      "Epoch 2590, Loss: 0.19413997605443, Final Batch Loss: 0.022567562758922577\n",
      "Epoch 2591, Loss: 0.21226763725280762, Final Batch Loss: 0.03775321692228317\n",
      "Epoch 2592, Loss: 0.15434419456869364, Final Batch Loss: 0.011160631664097309\n",
      "Epoch 2593, Loss: 0.21972332149744034, Final Batch Loss: 0.026716701686382294\n",
      "Epoch 2594, Loss: 0.26784636825323105, Final Batch Loss: 0.15087057650089264\n",
      "Epoch 2595, Loss: 0.18551387265324593, Final Batch Loss: 0.06315762549638748\n",
      "Epoch 2596, Loss: 0.2876618653535843, Final Batch Loss: 0.20869657397270203\n",
      "Epoch 2597, Loss: 0.2561929002404213, Final Batch Loss: 0.0994315966963768\n",
      "Epoch 2598, Loss: 0.20492787659168243, Final Batch Loss: 0.07935454696416855\n",
      "Epoch 2599, Loss: 0.1709449477493763, Final Batch Loss: 0.04324288293719292\n",
      "Epoch 2600, Loss: 0.3096291460096836, Final Batch Loss: 0.16178108751773834\n",
      "Epoch 2601, Loss: 0.2831530384719372, Final Batch Loss: 0.17971599102020264\n",
      "Epoch 2602, Loss: 0.14666130393743515, Final Batch Loss: 0.04693961516022682\n",
      "Epoch 2603, Loss: 0.2354212999343872, Final Batch Loss: 0.082970030605793\n",
      "Epoch 2604, Loss: 0.16149117797613144, Final Batch Loss: 0.03683701902627945\n",
      "Epoch 2605, Loss: 0.13223229162395, Final Batch Loss: 0.02198759652674198\n",
      "Epoch 2606, Loss: 0.17467985674738884, Final Batch Loss: 0.08040735125541687\n",
      "Epoch 2607, Loss: 0.18811200931668282, Final Batch Loss: 0.0883297547698021\n",
      "Epoch 2608, Loss: 0.18628467991948128, Final Batch Loss: 0.0725865364074707\n",
      "Epoch 2609, Loss: 0.21498510986566544, Final Batch Loss: 0.10220561921596527\n",
      "Epoch 2610, Loss: 0.12863991037011147, Final Batch Loss: 0.03386538103222847\n",
      "Epoch 2611, Loss: 0.15490390732884407, Final Batch Loss: 0.05884347856044769\n",
      "Epoch 2612, Loss: 0.19839848577976227, Final Batch Loss: 0.02717595547437668\n",
      "Epoch 2613, Loss: 0.2075527124106884, Final Batch Loss: 0.08351781219244003\n",
      "Epoch 2614, Loss: 0.19725284725427628, Final Batch Loss: 0.03749462962150574\n",
      "Epoch 2615, Loss: 0.23336943984031677, Final Batch Loss: 0.1076979860663414\n",
      "Epoch 2616, Loss: 0.2914542853832245, Final Batch Loss: 0.0918278619647026\n",
      "Epoch 2617, Loss: 0.18993518501520157, Final Batch Loss: 0.05578584223985672\n",
      "Epoch 2618, Loss: 0.21152738109230995, Final Batch Loss: 0.03247455880045891\n",
      "Epoch 2619, Loss: 0.16816463321447372, Final Batch Loss: 0.06746654957532883\n",
      "Epoch 2620, Loss: 0.11215225048363209, Final Batch Loss: 0.021240398287773132\n",
      "Epoch 2621, Loss: 0.1717609316110611, Final Batch Loss: 0.0365864560008049\n",
      "Epoch 2622, Loss: 0.1643092930316925, Final Batch Loss: 0.05153772979974747\n",
      "Epoch 2623, Loss: 0.2534850388765335, Final Batch Loss: 0.13624818623065948\n",
      "Epoch 2624, Loss: 0.14911826234310865, Final Batch Loss: 0.012453156523406506\n",
      "Epoch 2625, Loss: 0.17252683266997337, Final Batch Loss: 0.06426218897104263\n",
      "Epoch 2626, Loss: 0.12752617802470922, Final Batch Loss: 0.011066320352256298\n",
      "Epoch 2627, Loss: 0.19085901975631714, Final Batch Loss: 0.07544630020856857\n",
      "Epoch 2628, Loss: 0.1322891004383564, Final Batch Loss: 0.047170449048280716\n",
      "Epoch 2629, Loss: 0.2713000401854515, Final Batch Loss: 0.10845723748207092\n",
      "Epoch 2630, Loss: 0.19554222002625465, Final Batch Loss: 0.07028231769800186\n",
      "Epoch 2631, Loss: 0.1657610423862934, Final Batch Loss: 0.0482262521982193\n",
      "Epoch 2632, Loss: 0.22499480471014977, Final Batch Loss: 0.11798033863306046\n",
      "Epoch 2633, Loss: 0.2083263285458088, Final Batch Loss: 0.09238769859075546\n",
      "Epoch 2634, Loss: 0.30449681729078293, Final Batch Loss: 0.14700879156589508\n",
      "Epoch 2635, Loss: 0.177113126963377, Final Batch Loss: 0.025914642959833145\n",
      "Epoch 2636, Loss: 0.14391400665044785, Final Batch Loss: 0.05793006345629692\n",
      "Epoch 2637, Loss: 0.142838504165411, Final Batch Loss: 0.017619045451283455\n",
      "Epoch 2638, Loss: 0.2423253208398819, Final Batch Loss: 0.0704595223069191\n",
      "Epoch 2639, Loss: 0.24393746256828308, Final Batch Loss: 0.07938829064369202\n",
      "Epoch 2640, Loss: 0.22695055603981018, Final Batch Loss: 0.123952716588974\n",
      "Epoch 2641, Loss: 0.13496974855661392, Final Batch Loss: 0.02485552430152893\n",
      "Epoch 2642, Loss: 0.19075538218021393, Final Batch Loss: 0.03658124431967735\n",
      "Epoch 2643, Loss: 0.13041015155613422, Final Batch Loss: 0.022408926859498024\n",
      "Epoch 2644, Loss: 0.25025445967912674, Final Batch Loss: 0.1050228625535965\n",
      "Epoch 2645, Loss: 0.2615373507142067, Final Batch Loss: 0.10887054353952408\n",
      "Epoch 2646, Loss: 0.14029054250568151, Final Batch Loss: 0.01090079452842474\n",
      "Epoch 2647, Loss: 0.2644244134426117, Final Batch Loss: 0.13381552696228027\n",
      "Epoch 2648, Loss: 0.19253227300941944, Final Batch Loss: 0.023998646065592766\n",
      "Epoch 2649, Loss: 0.17709890939295292, Final Batch Loss: 0.025060391053557396\n",
      "Epoch 2650, Loss: 0.26147470995783806, Final Batch Loss: 0.12212294340133667\n",
      "Epoch 2651, Loss: 0.21454836800694466, Final Batch Loss: 0.10213857889175415\n",
      "Epoch 2652, Loss: 0.09134502150118351, Final Batch Loss: 0.0150582455098629\n",
      "Epoch 2653, Loss: 0.1902513951063156, Final Batch Loss: 0.04400688409805298\n",
      "Epoch 2654, Loss: 0.20193464681506157, Final Batch Loss: 0.10764402151107788\n",
      "Epoch 2655, Loss: 0.2099439911544323, Final Batch Loss: 0.10026542842388153\n",
      "Epoch 2656, Loss: 0.16656025871634483, Final Batch Loss: 0.03429032489657402\n",
      "Epoch 2657, Loss: 0.3060588799417019, Final Batch Loss: 0.17999587953090668\n",
      "Epoch 2658, Loss: 0.19471141695976257, Final Batch Loss: 0.03884771466255188\n",
      "Epoch 2659, Loss: 0.16901415213942528, Final Batch Loss: 0.02736828103661537\n",
      "Epoch 2660, Loss: 0.21302856877446175, Final Batch Loss: 0.047135572880506516\n",
      "Epoch 2661, Loss: 0.1565298018977046, Final Batch Loss: 0.012990246526896954\n",
      "Epoch 2662, Loss: 0.22157226502895355, Final Batch Loss: 0.0715629979968071\n",
      "Epoch 2663, Loss: 0.145800294354558, Final Batch Loss: 0.019850490614771843\n",
      "Epoch 2664, Loss: 0.19139821827411652, Final Batch Loss: 0.09486455470323563\n",
      "Epoch 2665, Loss: 0.299519669264555, Final Batch Loss: 0.06997279822826385\n",
      "Epoch 2666, Loss: 0.17421762645244598, Final Batch Loss: 0.05620186775922775\n",
      "Epoch 2667, Loss: 0.17988340556621552, Final Batch Loss: 0.07255752384662628\n",
      "Epoch 2668, Loss: 0.15905934944748878, Final Batch Loss: 0.01899687945842743\n",
      "Epoch 2669, Loss: 0.1836671456694603, Final Batch Loss: 0.05819312855601311\n",
      "Epoch 2670, Loss: 0.202872134745121, Final Batch Loss: 0.11104118078947067\n",
      "Epoch 2671, Loss: 0.10099447146058083, Final Batch Loss: 0.008686967194080353\n",
      "Epoch 2672, Loss: 0.37055033817887306, Final Batch Loss: 0.23700332641601562\n",
      "Epoch 2673, Loss: 0.3095777556300163, Final Batch Loss: 0.18923376500606537\n",
      "Epoch 2674, Loss: 0.23615766316652298, Final Batch Loss: 0.07446953654289246\n",
      "Epoch 2675, Loss: 0.16891177743673325, Final Batch Loss: 0.03315117955207825\n",
      "Epoch 2676, Loss: 0.17460095509886742, Final Batch Loss: 0.07361610978841782\n",
      "Epoch 2677, Loss: 0.2082463540136814, Final Batch Loss: 0.051774684339761734\n",
      "Epoch 2678, Loss: 0.15684458427131176, Final Batch Loss: 0.022769203409552574\n",
      "Epoch 2679, Loss: 0.16731183975934982, Final Batch Loss: 0.022173557430505753\n",
      "Epoch 2680, Loss: 0.22060876339673996, Final Batch Loss: 0.10588675737380981\n",
      "Epoch 2681, Loss: 0.13506662007421255, Final Batch Loss: 0.012711959891021252\n",
      "Epoch 2682, Loss: 0.12313625682145357, Final Batch Loss: 0.0072802649810910225\n",
      "Epoch 2683, Loss: 0.21495644003152847, Final Batch Loss: 0.1170077994465828\n",
      "Epoch 2684, Loss: 0.17609739303588867, Final Batch Loss: 0.056749120354652405\n",
      "Epoch 2685, Loss: 0.18730583786964417, Final Batch Loss: 0.037269674241542816\n",
      "Epoch 2686, Loss: 0.17132654786109924, Final Batch Loss: 0.05534650757908821\n",
      "Epoch 2687, Loss: 0.1714821755886078, Final Batch Loss: 0.043283555656671524\n",
      "Epoch 2688, Loss: 0.1779189482331276, Final Batch Loss: 0.04931189492344856\n",
      "Epoch 2689, Loss: 0.1430260557681322, Final Batch Loss: 0.02745058201253414\n",
      "Epoch 2690, Loss: 0.14163165539503098, Final Batch Loss: 0.014111682772636414\n",
      "Epoch 2691, Loss: 0.19472415000200272, Final Batch Loss: 0.059407979249954224\n",
      "Epoch 2692, Loss: 0.16886792331933975, Final Batch Loss: 0.05110475420951843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2693, Loss: 0.20357858017086983, Final Batch Loss: 0.033703554421663284\n",
      "Epoch 2694, Loss: 0.22777629643678665, Final Batch Loss: 0.0922262966632843\n",
      "Epoch 2695, Loss: 0.15739703178405762, Final Batch Loss: 0.03349120169878006\n",
      "Epoch 2696, Loss: 0.258427657186985, Final Batch Loss: 0.06074340641498566\n",
      "Epoch 2697, Loss: 0.15988543257117271, Final Batch Loss: 0.05306155979633331\n",
      "Epoch 2698, Loss: 0.2746080681681633, Final Batch Loss: 0.08321354538202286\n",
      "Epoch 2699, Loss: 0.26001638174057007, Final Batch Loss: 0.06889639794826508\n",
      "Epoch 2700, Loss: 0.2515605017542839, Final Batch Loss: 0.08910395205020905\n",
      "Epoch 2701, Loss: 0.2892506346106529, Final Batch Loss: 0.0669221356511116\n",
      "Epoch 2702, Loss: 0.16136044636368752, Final Batch Loss: 0.044442251324653625\n",
      "Epoch 2703, Loss: 0.13795707374811172, Final Batch Loss: 0.013638950884342194\n",
      "Epoch 2704, Loss: 0.29910364374518394, Final Batch Loss: 0.18601155281066895\n",
      "Epoch 2705, Loss: 0.15878230705857277, Final Batch Loss: 0.045360662043094635\n",
      "Epoch 2706, Loss: 0.17679957300424576, Final Batch Loss: 0.03598896786570549\n",
      "Epoch 2707, Loss: 0.1339233573526144, Final Batch Loss: 0.02094525657594204\n",
      "Epoch 2708, Loss: 0.21127477660775185, Final Batch Loss: 0.0893082544207573\n",
      "Epoch 2709, Loss: 0.18952568992972374, Final Batch Loss: 0.09819991886615753\n",
      "Epoch 2710, Loss: 0.16162434220314026, Final Batch Loss: 0.03978617116808891\n",
      "Epoch 2711, Loss: 0.1606595478951931, Final Batch Loss: 0.031508710235357285\n",
      "Epoch 2712, Loss: 0.12630034796893597, Final Batch Loss: 0.05389631912112236\n",
      "Epoch 2713, Loss: 0.13647007569670677, Final Batch Loss: 0.03961508348584175\n",
      "Epoch 2714, Loss: 0.2836410775780678, Final Batch Loss: 0.11581964045763016\n",
      "Epoch 2715, Loss: 0.13418354466557503, Final Batch Loss: 0.044777680188417435\n",
      "Epoch 2716, Loss: 0.1616285890340805, Final Batch Loss: 0.030258983373641968\n",
      "Epoch 2717, Loss: 0.260420348495245, Final Batch Loss: 0.0765618085861206\n",
      "Epoch 2718, Loss: 0.16870585456490517, Final Batch Loss: 0.01762104034423828\n",
      "Epoch 2719, Loss: 0.1805130783468485, Final Batch Loss: 0.019119994714856148\n",
      "Epoch 2720, Loss: 0.12520550936460495, Final Batch Loss: 0.006875883787870407\n",
      "Epoch 2721, Loss: 0.20292650163173676, Final Batch Loss: 0.07284893095493317\n",
      "Epoch 2722, Loss: 0.25602997466921806, Final Batch Loss: 0.14631982147693634\n",
      "Epoch 2723, Loss: 0.28540288284420967, Final Batch Loss: 0.12887521088123322\n",
      "Epoch 2724, Loss: 0.204729612916708, Final Batch Loss: 0.05698637291789055\n",
      "Epoch 2725, Loss: 0.2598859742283821, Final Batch Loss: 0.07370832562446594\n",
      "Epoch 2726, Loss: 0.2179902195930481, Final Batch Loss: 0.06930399686098099\n",
      "Epoch 2727, Loss: 0.2600998505949974, Final Batch Loss: 0.06652075052261353\n",
      "Epoch 2728, Loss: 0.20352475717663765, Final Batch Loss: 0.0710398480296135\n",
      "Epoch 2729, Loss: 0.14803990721702576, Final Batch Loss: 0.013013225048780441\n",
      "Epoch 2730, Loss: 0.234440378844738, Final Batch Loss: 0.08298973739147186\n",
      "Epoch 2731, Loss: 0.27160947024822235, Final Batch Loss: 0.1342305839061737\n",
      "Epoch 2732, Loss: 0.35339903086423874, Final Batch Loss: 0.205180823802948\n",
      "Epoch 2733, Loss: 0.2051243558526039, Final Batch Loss: 0.04457603394985199\n",
      "Epoch 2734, Loss: 0.1816158890724182, Final Batch Loss: 0.04447110369801521\n",
      "Epoch 2735, Loss: 0.3530653342604637, Final Batch Loss: 0.22021804749965668\n",
      "Epoch 2736, Loss: 0.13675627298653126, Final Batch Loss: 0.02283179573714733\n",
      "Epoch 2737, Loss: 0.22947901487350464, Final Batch Loss: 0.08490360528230667\n",
      "Epoch 2738, Loss: 0.2081935666501522, Final Batch Loss: 0.056851934641599655\n",
      "Epoch 2739, Loss: 0.5298631116747856, Final Batch Loss: 0.3637217581272125\n",
      "Epoch 2740, Loss: 0.20212735794484615, Final Batch Loss: 0.030434032902121544\n",
      "Epoch 2741, Loss: 0.2434796839952469, Final Batch Loss: 0.046601198613643646\n",
      "Epoch 2742, Loss: 0.21748453006148338, Final Batch Loss: 0.0456855334341526\n",
      "Epoch 2743, Loss: 0.3899318315088749, Final Batch Loss: 0.24508865177631378\n",
      "Epoch 2744, Loss: 0.15871503576636314, Final Batch Loss: 0.07550080865621567\n",
      "Epoch 2745, Loss: 0.1672192569822073, Final Batch Loss: 0.022321460768580437\n",
      "Epoch 2746, Loss: 0.13460533320903778, Final Batch Loss: 0.010734204202890396\n",
      "Epoch 2747, Loss: 0.15344542264938354, Final Batch Loss: 0.04731452837586403\n",
      "Epoch 2748, Loss: 0.15883459895849228, Final Batch Loss: 0.06333598494529724\n",
      "Epoch 2749, Loss: 0.17921779304742813, Final Batch Loss: 0.06931620836257935\n",
      "Epoch 2750, Loss: 0.16611028276383877, Final Batch Loss: 0.023067785426974297\n",
      "Epoch 2751, Loss: 0.20817576721310616, Final Batch Loss: 0.05473706126213074\n",
      "Epoch 2752, Loss: 0.10530989244580269, Final Batch Loss: 0.010269597172737122\n",
      "Epoch 2753, Loss: 0.3078444190323353, Final Batch Loss: 0.1545545905828476\n",
      "Epoch 2754, Loss: 0.14460410550236702, Final Batch Loss: 0.03021090477705002\n",
      "Epoch 2755, Loss: 0.2924905903637409, Final Batch Loss: 0.15101248025894165\n",
      "Epoch 2756, Loss: 0.14891471341252327, Final Batch Loss: 0.0548931248486042\n",
      "Epoch 2757, Loss: 0.23712639138102531, Final Batch Loss: 0.043332766741514206\n",
      "Epoch 2758, Loss: 0.21437888033688068, Final Batch Loss: 0.02267024852335453\n",
      "Epoch 2759, Loss: 0.3081699162721634, Final Batch Loss: 0.08739819377660751\n",
      "Epoch 2760, Loss: 0.22731616720557213, Final Batch Loss: 0.07845732569694519\n",
      "Epoch 2761, Loss: 0.16767244786024094, Final Batch Loss: 0.025515593588352203\n",
      "Epoch 2762, Loss: 0.24386848509311676, Final Batch Loss: 0.10825124382972717\n",
      "Epoch 2763, Loss: 0.212620722129941, Final Batch Loss: 0.10024155676364899\n",
      "Epoch 2764, Loss: 0.21224291250109673, Final Batch Loss: 0.051340531557798386\n",
      "Epoch 2765, Loss: 0.15243791788816452, Final Batch Loss: 0.04119507595896721\n",
      "Epoch 2766, Loss: 0.1467768270522356, Final Batch Loss: 0.01441785879433155\n",
      "Epoch 2767, Loss: 0.19548826664686203, Final Batch Loss: 0.07225773483514786\n",
      "Epoch 2768, Loss: 0.1724412776529789, Final Batch Loss: 0.03564438223838806\n",
      "Epoch 2769, Loss: 0.14940376952290535, Final Batch Loss: 0.03926049545407295\n",
      "Epoch 2770, Loss: 0.20970617607235909, Final Batch Loss: 0.08368749171495438\n",
      "Epoch 2771, Loss: 0.1927124671638012, Final Batch Loss: 0.05846313014626503\n",
      "Epoch 2772, Loss: 0.16395949199795723, Final Batch Loss: 0.06608130782842636\n",
      "Epoch 2773, Loss: 0.1359931044280529, Final Batch Loss: 0.038398776203393936\n",
      "Epoch 2774, Loss: 0.1865951493382454, Final Batch Loss: 0.07696522772312164\n",
      "Epoch 2775, Loss: 0.2743571661412716, Final Batch Loss: 0.11089076101779938\n",
      "Epoch 2776, Loss: 0.16850604861974716, Final Batch Loss: 0.05243358761072159\n",
      "Epoch 2777, Loss: 0.21483160182833672, Final Batch Loss: 0.07938431948423386\n",
      "Epoch 2778, Loss: 0.2461537979543209, Final Batch Loss: 0.1085522398352623\n",
      "Epoch 2779, Loss: 0.2507915124297142, Final Batch Loss: 0.09876172989606857\n",
      "Epoch 2780, Loss: 0.10393968038260937, Final Batch Loss: 0.02180771715939045\n",
      "Epoch 2781, Loss: 0.2490890547633171, Final Batch Loss: 0.10456700623035431\n",
      "Epoch 2782, Loss: 0.24111611396074295, Final Batch Loss: 0.13400587439537048\n",
      "Epoch 2783, Loss: 0.2181089147925377, Final Batch Loss: 0.06213746219873428\n",
      "Epoch 2784, Loss: 0.21049311012029648, Final Batch Loss: 0.04960063099861145\n",
      "Epoch 2785, Loss: 0.17043159157037735, Final Batch Loss: 0.039266858249902725\n",
      "Epoch 2786, Loss: 0.3242412395775318, Final Batch Loss: 0.20861472189426422\n",
      "Epoch 2787, Loss: 0.16833128780126572, Final Batch Loss: 0.04134797677397728\n",
      "Epoch 2788, Loss: 0.13701897393912077, Final Batch Loss: 0.007521730847656727\n",
      "Epoch 2789, Loss: 0.1549178659915924, Final Batch Loss: 0.05376296117901802\n",
      "Epoch 2790, Loss: 0.17614599037915468, Final Batch Loss: 0.01319547463208437\n",
      "Epoch 2791, Loss: 0.19302693381905556, Final Batch Loss: 0.08369545638561249\n",
      "Epoch 2792, Loss: 0.283439539372921, Final Batch Loss: 0.07549112290143967\n",
      "Epoch 2793, Loss: 0.20400093868374825, Final Batch Loss: 0.07445547729730606\n",
      "Epoch 2794, Loss: 0.21819375827908516, Final Batch Loss: 0.09153464436531067\n",
      "Epoch 2795, Loss: 0.20217776112258434, Final Batch Loss: 0.01811726950109005\n",
      "Epoch 2796, Loss: 0.19465340673923492, Final Batch Loss: 0.07978355884552002\n",
      "Epoch 2797, Loss: 0.27781867794692516, Final Batch Loss: 0.19075298309326172\n",
      "Epoch 2798, Loss: 0.1890326365828514, Final Batch Loss: 0.06860347092151642\n",
      "Epoch 2799, Loss: 0.28275712952017784, Final Batch Loss: 0.12904052436351776\n",
      "Epoch 2800, Loss: 0.18729614466428757, Final Batch Loss: 0.048785630613565445\n",
      "Epoch 2801, Loss: 0.16048533469438553, Final Batch Loss: 0.05625666305422783\n",
      "Epoch 2802, Loss: 0.14396845176815987, Final Batch Loss: 0.04414118453860283\n",
      "Epoch 2803, Loss: 0.14004502817988396, Final Batch Loss: 0.049116384238004684\n",
      "Epoch 2804, Loss: 0.13382533192634583, Final Batch Loss: 0.03420647606253624\n",
      "Epoch 2805, Loss: 0.1611090525984764, Final Batch Loss: 0.06209190562367439\n",
      "Epoch 2806, Loss: 0.21361256390810013, Final Batch Loss: 0.08165773749351501\n",
      "Epoch 2807, Loss: 0.16394146345555782, Final Batch Loss: 0.0292374175041914\n",
      "Epoch 2808, Loss: 0.15985540486872196, Final Batch Loss: 0.004041032865643501\n",
      "Epoch 2809, Loss: 0.21198006346821785, Final Batch Loss: 0.03823753073811531\n",
      "Epoch 2810, Loss: 0.14418779872357845, Final Batch Loss: 0.03858301788568497\n",
      "Epoch 2811, Loss: 0.20899061113595963, Final Batch Loss: 0.05828665569424629\n",
      "Epoch 2812, Loss: 0.1314159743487835, Final Batch Loss: 0.03203713521361351\n",
      "Epoch 2813, Loss: 0.14793389290571213, Final Batch Loss: 0.06255748867988586\n",
      "Epoch 2814, Loss: 0.16508549451828003, Final Batch Loss: 0.02187022566795349\n",
      "Epoch 2815, Loss: 0.17294199019670486, Final Batch Loss: 0.04799932241439819\n",
      "Epoch 2816, Loss: 0.12574718613177538, Final Batch Loss: 0.014767986722290516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2817, Loss: 0.21397369354963303, Final Batch Loss: 0.07001315802335739\n",
      "Epoch 2818, Loss: 0.10785995796322823, Final Batch Loss: 0.02369997277855873\n",
      "Epoch 2819, Loss: 0.4257045593112707, Final Batch Loss: 0.3549671471118927\n",
      "Epoch 2820, Loss: 0.18166473507881165, Final Batch Loss: 0.05625796318054199\n",
      "Epoch 2821, Loss: 0.19083783403038979, Final Batch Loss: 0.0885007232427597\n",
      "Epoch 2822, Loss: 0.12536438554525375, Final Batch Loss: 0.0252668559551239\n",
      "Epoch 2823, Loss: 0.1801365353167057, Final Batch Loss: 0.056410547345876694\n",
      "Epoch 2824, Loss: 0.28522989153862, Final Batch Loss: 0.09823665767908096\n",
      "Epoch 2825, Loss: 0.13953985087573528, Final Batch Loss: 0.01694602333009243\n",
      "Epoch 2826, Loss: 0.10727726854383945, Final Batch Loss: 0.018192777410149574\n",
      "Epoch 2827, Loss: 0.2180548496544361, Final Batch Loss: 0.04246297851204872\n",
      "Epoch 2828, Loss: 0.16978133842349052, Final Batch Loss: 0.062722347676754\n",
      "Epoch 2829, Loss: 0.1671437993645668, Final Batch Loss: 0.07630497962236404\n",
      "Epoch 2830, Loss: 0.13109188992530107, Final Batch Loss: 0.008096040226519108\n",
      "Epoch 2831, Loss: 0.2901197597384453, Final Batch Loss: 0.20169712603092194\n",
      "Epoch 2832, Loss: 0.15541239455342293, Final Batch Loss: 0.044237375259399414\n",
      "Epoch 2833, Loss: 0.14174052327871323, Final Batch Loss: 0.03403731808066368\n",
      "Epoch 2834, Loss: 0.18128476664423943, Final Batch Loss: 0.027282368391752243\n",
      "Epoch 2835, Loss: 0.22285263985395432, Final Batch Loss: 0.14328159391880035\n",
      "Epoch 2836, Loss: 0.11300753429532051, Final Batch Loss: 0.030681923031806946\n",
      "Epoch 2837, Loss: 0.1202399767935276, Final Batch Loss: 0.03233427181839943\n",
      "Epoch 2838, Loss: 0.1873280480504036, Final Batch Loss: 0.03006085753440857\n",
      "Epoch 2839, Loss: 0.23085222020745277, Final Batch Loss: 0.10607709735631943\n",
      "Epoch 2840, Loss: 0.15251313894987106, Final Batch Loss: 0.0690908432006836\n",
      "Epoch 2841, Loss: 0.10194774530827999, Final Batch Loss: 0.010874474421143532\n",
      "Epoch 2842, Loss: 0.20862749963998795, Final Batch Loss: 0.020847782492637634\n",
      "Epoch 2843, Loss: 0.1845962591469288, Final Batch Loss: 0.07348277419805527\n",
      "Epoch 2844, Loss: 0.15162665955722332, Final Batch Loss: 0.08676869422197342\n",
      "Epoch 2845, Loss: 0.1160392016172409, Final Batch Loss: 0.016103293746709824\n",
      "Epoch 2846, Loss: 0.2090408094227314, Final Batch Loss: 0.11617729067802429\n",
      "Epoch 2847, Loss: 0.14510959712788463, Final Batch Loss: 0.0030924784950912\n",
      "Epoch 2848, Loss: 0.2818046249449253, Final Batch Loss: 0.16362658143043518\n",
      "Epoch 2849, Loss: 0.20837481319904327, Final Batch Loss: 0.06371767073869705\n",
      "Epoch 2850, Loss: 0.1288173459470272, Final Batch Loss: 0.015479393303394318\n",
      "Epoch 2851, Loss: 0.2770754247903824, Final Batch Loss: 0.12360718101263046\n",
      "Epoch 2852, Loss: 0.174483273178339, Final Batch Loss: 0.05032956227660179\n",
      "Epoch 2853, Loss: 0.2284756489098072, Final Batch Loss: 0.08634737133979797\n",
      "Epoch 2854, Loss: 0.18923600763082504, Final Batch Loss: 0.01063574105501175\n",
      "Epoch 2855, Loss: 0.13631164003163576, Final Batch Loss: 0.011977297253906727\n",
      "Epoch 2856, Loss: 0.24331603199243546, Final Batch Loss: 0.15490680932998657\n",
      "Epoch 2857, Loss: 0.1186674740165472, Final Batch Loss: 0.021195633336901665\n",
      "Epoch 2858, Loss: 0.11380878649652004, Final Batch Loss: 0.041394662111997604\n",
      "Epoch 2859, Loss: 0.19859707728028297, Final Batch Loss: 0.10561195015907288\n",
      "Epoch 2860, Loss: 0.14511315524578094, Final Batch Loss: 0.011074546724557877\n",
      "Epoch 2861, Loss: 0.09515775088220835, Final Batch Loss: 0.009213731624186039\n",
      "Epoch 2862, Loss: 0.1927503366023302, Final Batch Loss: 0.056835103780031204\n",
      "Epoch 2863, Loss: 0.20546488091349602, Final Batch Loss: 0.09472271054983139\n",
      "Epoch 2864, Loss: 0.1548024546355009, Final Batch Loss: 0.09125971049070358\n",
      "Epoch 2865, Loss: 0.11754312738776207, Final Batch Loss: 0.012466669082641602\n",
      "Epoch 2866, Loss: 0.10470985528081656, Final Batch Loss: 0.007579262368381023\n",
      "Epoch 2867, Loss: 0.1274778926745057, Final Batch Loss: 0.013944768346846104\n",
      "Epoch 2868, Loss: 0.10026739072054625, Final Batch Loss: 0.01484882365912199\n",
      "Epoch 2869, Loss: 0.15306968614459038, Final Batch Loss: 0.05766644328832626\n",
      "Epoch 2870, Loss: 0.09998902305960655, Final Batch Loss: 0.018642518669366837\n",
      "Epoch 2871, Loss: 0.2140529453754425, Final Batch Loss: 0.1028093472123146\n",
      "Epoch 2872, Loss: 0.13908881321549416, Final Batch Loss: 0.033671677112579346\n",
      "Epoch 2873, Loss: 0.08540205843746662, Final Batch Loss: 0.016623839735984802\n",
      "Epoch 2874, Loss: 0.10935638472437859, Final Batch Loss: 0.010390061885118484\n",
      "Epoch 2875, Loss: 0.1356184333562851, Final Batch Loss: 0.04985513538122177\n",
      "Epoch 2876, Loss: 0.1334170512855053, Final Batch Loss: 0.018274672329425812\n",
      "Epoch 2877, Loss: 0.11977860517799854, Final Batch Loss: 0.008831271901726723\n",
      "Epoch 2878, Loss: 0.18988285958766937, Final Batch Loss: 0.03991814702749252\n",
      "Epoch 2879, Loss: 0.13842048682272434, Final Batch Loss: 0.04630501568317413\n",
      "Epoch 2880, Loss: 0.11502649169415236, Final Batch Loss: 0.006661104969680309\n",
      "Epoch 2881, Loss: 0.09092285484075546, Final Batch Loss: 0.017656754702329636\n",
      "Epoch 2882, Loss: 0.16848336346447468, Final Batch Loss: 0.07609973847866058\n",
      "Epoch 2883, Loss: 0.15266604535281658, Final Batch Loss: 0.02738582156598568\n",
      "Epoch 2884, Loss: 0.2774854600429535, Final Batch Loss: 0.11543995141983032\n",
      "Epoch 2885, Loss: 0.24505630135536194, Final Batch Loss: 0.10265161842107773\n",
      "Epoch 2886, Loss: 0.1498592235147953, Final Batch Loss: 0.035232819616794586\n",
      "Epoch 2887, Loss: 0.19893058016896248, Final Batch Loss: 0.11774137616157532\n",
      "Epoch 2888, Loss: 0.21583929285407066, Final Batch Loss: 0.10577337443828583\n",
      "Epoch 2889, Loss: 0.31603096425533295, Final Batch Loss: 0.15389323234558105\n",
      "Epoch 2890, Loss: 0.2549322284758091, Final Batch Loss: 0.16285140812397003\n",
      "Epoch 2891, Loss: 0.14650395140051842, Final Batch Loss: 0.039749518036842346\n",
      "Epoch 2892, Loss: 0.26047753915190697, Final Batch Loss: 0.12408451735973358\n",
      "Epoch 2893, Loss: 0.21775345876812935, Final Batch Loss: 0.07820244878530502\n",
      "Epoch 2894, Loss: 0.16277486644685268, Final Batch Loss: 0.019495872780680656\n",
      "Epoch 2895, Loss: 0.1974548175930977, Final Batch Loss: 0.06993241608142853\n",
      "Epoch 2896, Loss: 0.18517757020890713, Final Batch Loss: 0.10744322836399078\n",
      "Epoch 2897, Loss: 0.10405188240110874, Final Batch Loss: 0.029647966846823692\n",
      "Epoch 2898, Loss: 0.2290268260985613, Final Batch Loss: 0.14950856566429138\n",
      "Epoch 2899, Loss: 0.23521197959780693, Final Batch Loss: 0.05180950090289116\n",
      "Epoch 2900, Loss: 0.20665382966399193, Final Batch Loss: 0.07564635574817657\n",
      "Epoch 2901, Loss: 0.12355747073888779, Final Batch Loss: 0.023794319480657578\n",
      "Epoch 2902, Loss: 0.25924910604953766, Final Batch Loss: 0.16372837126255035\n",
      "Epoch 2903, Loss: 0.23427723720669746, Final Batch Loss: 0.10456392168998718\n",
      "Epoch 2904, Loss: 0.28983085602521896, Final Batch Loss: 0.16396169364452362\n",
      "Epoch 2905, Loss: 0.10808306839317083, Final Batch Loss: 0.007656211964786053\n",
      "Epoch 2906, Loss: 0.11454915069043636, Final Batch Loss: 0.01865583099424839\n",
      "Epoch 2907, Loss: 0.17146685160696507, Final Batch Loss: 0.017909446731209755\n",
      "Epoch 2908, Loss: 0.16092163510620594, Final Batch Loss: 0.020793119445443153\n",
      "Epoch 2909, Loss: 0.21393564715981483, Final Batch Loss: 0.08561456948518753\n",
      "Epoch 2910, Loss: 0.12527842447161674, Final Batch Loss: 0.01977270469069481\n",
      "Epoch 2911, Loss: 0.13727822434157133, Final Batch Loss: 0.00783103983849287\n",
      "Epoch 2912, Loss: 0.16590530658140779, Final Batch Loss: 0.005149016622453928\n",
      "Epoch 2913, Loss: 0.18460006080567837, Final Batch Loss: 0.024754052981734276\n",
      "Epoch 2914, Loss: 0.15259187668561935, Final Batch Loss: 0.044111382216215134\n",
      "Epoch 2915, Loss: 0.1811334751546383, Final Batch Loss: 0.05539393052458763\n",
      "Epoch 2916, Loss: 0.33142470195889473, Final Batch Loss: 0.20528975129127502\n",
      "Epoch 2917, Loss: 0.11246742121875286, Final Batch Loss: 0.014231089502573013\n",
      "Epoch 2918, Loss: 0.16410377528518438, Final Batch Loss: 0.008625387214124203\n",
      "Epoch 2919, Loss: 0.14511550962924957, Final Batch Loss: 0.06222584843635559\n",
      "Epoch 2920, Loss: 0.10699671879410744, Final Batch Loss: 0.03478364273905754\n",
      "Epoch 2921, Loss: 0.15074755251407623, Final Batch Loss: 0.007211137562990189\n",
      "Epoch 2922, Loss: 0.2623323164880276, Final Batch Loss: 0.15536361932754517\n",
      "Epoch 2923, Loss: 0.13730917498469353, Final Batch Loss: 0.051955677568912506\n",
      "Epoch 2924, Loss: 0.2116629183292389, Final Batch Loss: 0.14483505487442017\n",
      "Epoch 2925, Loss: 0.17796539142727852, Final Batch Loss: 0.05670987814664841\n",
      "Epoch 2926, Loss: 0.18436067551374435, Final Batch Loss: 0.036582134664058685\n",
      "Epoch 2927, Loss: 0.15844158641994, Final Batch Loss: 0.012907179072499275\n",
      "Epoch 2928, Loss: 0.115001967176795, Final Batch Loss: 0.02480950392782688\n",
      "Epoch 2929, Loss: 0.15327857434749603, Final Batch Loss: 0.05044911801815033\n",
      "Epoch 2930, Loss: 0.11707759462296963, Final Batch Loss: 0.04939616098999977\n",
      "Epoch 2931, Loss: 0.1635232213884592, Final Batch Loss: 0.053790587931871414\n",
      "Epoch 2932, Loss: 0.14060968160629272, Final Batch Loss: 0.046746186912059784\n",
      "Epoch 2933, Loss: 0.14506877213716507, Final Batch Loss: 0.04908040910959244\n",
      "Epoch 2934, Loss: 0.12505868077278137, Final Batch Loss: 0.008792504668235779\n",
      "Epoch 2935, Loss: 0.11181987822055817, Final Batch Loss: 0.03217370808124542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2936, Loss: 0.16270726919174194, Final Batch Loss: 0.06516315788030624\n",
      "Epoch 2937, Loss: 0.09885748103260994, Final Batch Loss: 0.017136264592409134\n",
      "Epoch 2938, Loss: 0.10451210848987103, Final Batch Loss: 0.028026657178997993\n",
      "Epoch 2939, Loss: 0.13851769361644983, Final Batch Loss: 0.07355717569589615\n",
      "Epoch 2940, Loss: 0.12759385630488396, Final Batch Loss: 0.027551323175430298\n",
      "Epoch 2941, Loss: 0.13010257855057716, Final Batch Loss: 0.0437600277364254\n",
      "Epoch 2942, Loss: 0.129634290933609, Final Batch Loss: 0.03928640857338905\n",
      "Epoch 2943, Loss: 0.24246972799301147, Final Batch Loss: 0.09050273150205612\n",
      "Epoch 2944, Loss: 0.09662328101694584, Final Batch Loss: 0.011612201109528542\n",
      "Epoch 2945, Loss: 0.21852987259626389, Final Batch Loss: 0.08066781610250473\n",
      "Epoch 2946, Loss: 0.18321868404746056, Final Batch Loss: 0.07851296663284302\n",
      "Epoch 2947, Loss: 0.3230462521314621, Final Batch Loss: 0.12749046087265015\n",
      "Epoch 2948, Loss: 0.1618153676390648, Final Batch Loss: 0.05964724346995354\n",
      "Epoch 2949, Loss: 0.16944113746285439, Final Batch Loss: 0.05093131959438324\n",
      "Epoch 2950, Loss: 0.15997644886374474, Final Batch Loss: 0.03702615946531296\n",
      "Epoch 2951, Loss: 0.24573669955134392, Final Batch Loss: 0.08841744810342789\n",
      "Epoch 2952, Loss: 0.24812959879636765, Final Batch Loss: 0.1594175100326538\n",
      "Epoch 2953, Loss: 0.107141999527812, Final Batch Loss: 0.028561590239405632\n",
      "Epoch 2954, Loss: 0.16522002592682838, Final Batch Loss: 0.07489845901727676\n",
      "Epoch 2955, Loss: 0.1882792003452778, Final Batch Loss: 0.05554604530334473\n",
      "Epoch 2956, Loss: 0.09802412101998925, Final Batch Loss: 0.005744019988924265\n",
      "Epoch 2957, Loss: 0.11105550359934568, Final Batch Loss: 0.011624935083091259\n",
      "Epoch 2958, Loss: 0.15592282265424728, Final Batch Loss: 0.049477580934762955\n",
      "Epoch 2959, Loss: 0.16542550548911095, Final Batch Loss: 0.05814781412482262\n",
      "Epoch 2960, Loss: 0.20225262269377708, Final Batch Loss: 0.11268777400255203\n",
      "Epoch 2961, Loss: 0.2030325010418892, Final Batch Loss: 0.11293553560972214\n",
      "Epoch 2962, Loss: 0.18522415682673454, Final Batch Loss: 0.058455973863601685\n",
      "Epoch 2963, Loss: 0.19285660423338413, Final Batch Loss: 0.03082928992807865\n",
      "Epoch 2964, Loss: 0.15921635180711746, Final Batch Loss: 0.045648377388715744\n",
      "Epoch 2965, Loss: 0.24582963436841965, Final Batch Loss: 0.1216447651386261\n",
      "Epoch 2966, Loss: 0.18132715299725533, Final Batch Loss: 0.0539822094142437\n",
      "Epoch 2967, Loss: 0.1717747375369072, Final Batch Loss: 0.07966829091310501\n",
      "Epoch 2968, Loss: 0.14212340116500854, Final Batch Loss: 0.027188092470169067\n",
      "Epoch 2969, Loss: 0.153666571713984, Final Batch Loss: 0.006105405278503895\n",
      "Epoch 2970, Loss: 0.16262773238122463, Final Batch Loss: 0.011751165613532066\n",
      "Epoch 2971, Loss: 0.13269896991550922, Final Batch Loss: 0.01993747241795063\n",
      "Epoch 2972, Loss: 0.14403145015239716, Final Batch Loss: 0.0315803587436676\n",
      "Epoch 2973, Loss: 0.1692209430038929, Final Batch Loss: 0.034603167325258255\n",
      "Epoch 2974, Loss: 0.16235926654189825, Final Batch Loss: 0.004753752611577511\n",
      "Epoch 2975, Loss: 0.2503444477915764, Final Batch Loss: 0.11488029360771179\n",
      "Epoch 2976, Loss: 0.16760775819420815, Final Batch Loss: 0.06628737598657608\n",
      "Epoch 2977, Loss: 0.22972305864095688, Final Batch Loss: 0.14485256373882294\n",
      "Epoch 2978, Loss: 0.1878357194364071, Final Batch Loss: 0.07472169399261475\n",
      "Epoch 2979, Loss: 0.21388660185039043, Final Batch Loss: 0.02773815207183361\n",
      "Epoch 2980, Loss: 0.21310119703412056, Final Batch Loss: 0.10580110549926758\n",
      "Epoch 2981, Loss: 0.1427879687398672, Final Batch Loss: 0.017926936969161034\n",
      "Epoch 2982, Loss: 0.18961909133940935, Final Batch Loss: 0.014332820661365986\n",
      "Epoch 2983, Loss: 0.19924350827932358, Final Batch Loss: 0.05388842523097992\n",
      "Epoch 2984, Loss: 0.1531260870397091, Final Batch Loss: 0.058090999722480774\n",
      "Epoch 2985, Loss: 0.17607136070728302, Final Batch Loss: 0.03997517749667168\n",
      "Epoch 2986, Loss: 0.23512635007500648, Final Batch Loss: 0.11048613488674164\n",
      "Epoch 2987, Loss: 0.2879583090543747, Final Batch Loss: 0.10708393901586533\n",
      "Epoch 2988, Loss: 0.15105461329221725, Final Batch Loss: 0.06175120174884796\n",
      "Epoch 2989, Loss: 0.2519701747223735, Final Batch Loss: 0.012624437920749187\n",
      "Epoch 2990, Loss: 0.1865866854786873, Final Batch Loss: 0.051975857466459274\n",
      "Epoch 2991, Loss: 0.16422337666153908, Final Batch Loss: 0.06716758012771606\n",
      "Epoch 2992, Loss: 0.18299616128206253, Final Batch Loss: 0.03180208057165146\n",
      "Epoch 2993, Loss: 0.183706384152174, Final Batch Loss: 0.07151611894369125\n",
      "Epoch 2994, Loss: 0.13222424127161503, Final Batch Loss: 0.07783740758895874\n",
      "Epoch 2995, Loss: 0.0997685007750988, Final Batch Loss: 0.0236679520457983\n",
      "Epoch 2996, Loss: 0.16596026718616486, Final Batch Loss: 0.02183084562420845\n",
      "Epoch 2997, Loss: 0.10957273282110691, Final Batch Loss: 0.037818703800439835\n",
      "Epoch 2998, Loss: 0.11154481396079063, Final Batch Loss: 0.03826707601547241\n",
      "Epoch 2999, Loss: 0.11872121319174767, Final Batch Loss: 0.024838386103510857\n",
      "Epoch 3000, Loss: 0.14924756065011024, Final Batch Loss: 0.02482406049966812\n",
      "Epoch 3001, Loss: 0.11003907956182957, Final Batch Loss: 0.02092498354613781\n",
      "Epoch 3002, Loss: 0.10833017714321613, Final Batch Loss: 0.014724845066666603\n",
      "Epoch 3003, Loss: 0.2303822599351406, Final Batch Loss: 0.10180820524692535\n",
      "Epoch 3004, Loss: 0.21075029857456684, Final Batch Loss: 0.027147220447659492\n",
      "Epoch 3005, Loss: 0.07771012233570218, Final Batch Loss: 0.006916603539139032\n",
      "Epoch 3006, Loss: 0.15745922178030014, Final Batch Loss: 0.0333174504339695\n",
      "Epoch 3007, Loss: 0.1677371747791767, Final Batch Loss: 0.06072504445910454\n",
      "Epoch 3008, Loss: 0.22645999863743782, Final Batch Loss: 0.04822476580739021\n",
      "Epoch 3009, Loss: 0.1400253362953663, Final Batch Loss: 0.02667984366416931\n",
      "Epoch 3010, Loss: 0.12998248636722565, Final Batch Loss: 0.024694863706827164\n",
      "Epoch 3011, Loss: 0.10169149003922939, Final Batch Loss: 0.01096259243786335\n",
      "Epoch 3012, Loss: 0.10579064674675465, Final Batch Loss: 0.044108796864748\n",
      "Epoch 3013, Loss: 0.17442865297198296, Final Batch Loss: 0.09747274965047836\n",
      "Epoch 3014, Loss: 0.20412692800164223, Final Batch Loss: 0.051449116319417953\n",
      "Epoch 3015, Loss: 0.17391206324100494, Final Batch Loss: 0.02552124112844467\n",
      "Epoch 3016, Loss: 0.2258724458515644, Final Batch Loss: 0.10600782185792923\n",
      "Epoch 3017, Loss: 0.17740175500512123, Final Batch Loss: 0.06394265592098236\n",
      "Epoch 3018, Loss: 0.1786915920674801, Final Batch Loss: 0.018725048750638962\n",
      "Epoch 3019, Loss: 0.13869532197713852, Final Batch Loss: 0.03432217612862587\n",
      "Epoch 3020, Loss: 0.12008209154009819, Final Batch Loss: 0.03917384520173073\n",
      "Epoch 3021, Loss: 0.18611819297075272, Final Batch Loss: 0.10862082988023758\n",
      "Epoch 3022, Loss: 0.11553128808736801, Final Batch Loss: 0.019588852301239967\n",
      "Epoch 3023, Loss: 0.15103551372885704, Final Batch Loss: 0.048859983682632446\n",
      "Epoch 3024, Loss: 0.1536246631294489, Final Batch Loss: 0.021012509241700172\n",
      "Epoch 3025, Loss: 0.13727898336946964, Final Batch Loss: 0.010904217138886452\n",
      "Epoch 3026, Loss: 0.12709494307637215, Final Batch Loss: 0.014090832322835922\n",
      "Epoch 3027, Loss: 0.17859606258571148, Final Batch Loss: 0.09806159883737564\n",
      "Epoch 3028, Loss: 0.1741921380162239, Final Batch Loss: 0.06882674247026443\n",
      "Epoch 3029, Loss: 0.12833788990974426, Final Batch Loss: 0.01999184489250183\n",
      "Epoch 3030, Loss: 0.13646480441093445, Final Batch Loss: 0.06754090636968613\n",
      "Epoch 3031, Loss: 0.16862471774220467, Final Batch Loss: 0.06268531084060669\n",
      "Epoch 3032, Loss: 0.1711500659584999, Final Batch Loss: 0.08329208940267563\n",
      "Epoch 3033, Loss: 0.15779048204421997, Final Batch Loss: 0.028330199420452118\n",
      "Epoch 3034, Loss: 0.240165326744318, Final Batch Loss: 0.07549282908439636\n",
      "Epoch 3035, Loss: 0.12569008395075798, Final Batch Loss: 0.011822085827589035\n",
      "Epoch 3036, Loss: 0.13734009489417076, Final Batch Loss: 0.03746940940618515\n",
      "Epoch 3037, Loss: 0.2026461884379387, Final Batch Loss: 0.07367262244224548\n",
      "Epoch 3038, Loss: 0.13782407902181149, Final Batch Loss: 0.011288804933428764\n",
      "Epoch 3039, Loss: 0.07307399623095989, Final Batch Loss: 0.020705148577690125\n",
      "Epoch 3040, Loss: 0.09465548535808921, Final Batch Loss: 0.0037360801361501217\n",
      "Epoch 3041, Loss: 0.13404501788318157, Final Batch Loss: 0.03553461655974388\n",
      "Epoch 3042, Loss: 0.13662265427410603, Final Batch Loss: 0.05367972329258919\n",
      "Epoch 3043, Loss: 0.11588949710130692, Final Batch Loss: 0.06878160685300827\n",
      "Epoch 3044, Loss: 0.1596270464360714, Final Batch Loss: 0.04199262708425522\n",
      "Epoch 3045, Loss: 0.21516041830182076, Final Batch Loss: 0.03483743593096733\n",
      "Epoch 3046, Loss: 0.14053934812545776, Final Batch Loss: 0.03264947608113289\n",
      "Epoch 3047, Loss: 0.12185649573802948, Final Batch Loss: 0.01150752604007721\n",
      "Epoch 3048, Loss: 0.09710082784295082, Final Batch Loss: 0.004406314343214035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3049, Loss: 0.11311790067702532, Final Batch Loss: 0.01438517589122057\n",
      "Epoch 3050, Loss: 0.18236548453569412, Final Batch Loss: 0.03611690551042557\n",
      "Epoch 3051, Loss: 0.1242936309427023, Final Batch Loss: 0.049005523324012756\n",
      "Epoch 3052, Loss: 0.1430584117770195, Final Batch Loss: 0.05279082432389259\n",
      "Epoch 3053, Loss: 0.16015220060944557, Final Batch Loss: 0.041436586529016495\n",
      "Epoch 3054, Loss: 0.15071851201355457, Final Batch Loss: 0.01330680213868618\n",
      "Epoch 3055, Loss: 0.09617222705855966, Final Batch Loss: 0.0037930035032331944\n",
      "Epoch 3056, Loss: 0.32483363151550293, Final Batch Loss: 0.15649087727069855\n",
      "Epoch 3057, Loss: 0.1846972182393074, Final Batch Loss: 0.0784638449549675\n",
      "Epoch 3058, Loss: 0.13142619095742702, Final Batch Loss: 0.026003653183579445\n",
      "Epoch 3059, Loss: 0.18572913482785225, Final Batch Loss: 0.06117470562458038\n",
      "Epoch 3060, Loss: 0.13496615551412106, Final Batch Loss: 0.009916631504893303\n",
      "Epoch 3061, Loss: 0.1930893361568451, Final Batch Loss: 0.0556030310690403\n",
      "Epoch 3062, Loss: 0.16533729620277882, Final Batch Loss: 0.06477384269237518\n",
      "Epoch 3063, Loss: 0.12534013576805592, Final Batch Loss: 0.01598559506237507\n",
      "Epoch 3064, Loss: 0.13918739277869463, Final Batch Loss: 0.011134074069559574\n",
      "Epoch 3065, Loss: 0.18748359382152557, Final Batch Loss: 0.0814422070980072\n",
      "Epoch 3066, Loss: 0.173614963889122, Final Batch Loss: 0.08178746700286865\n",
      "Epoch 3067, Loss: 0.156528877094388, Final Batch Loss: 0.08926030993461609\n",
      "Epoch 3068, Loss: 0.4090728387236595, Final Batch Loss: 0.27273160219192505\n",
      "Epoch 3069, Loss: 0.1627198662608862, Final Batch Loss: 0.017585603520274162\n",
      "Epoch 3070, Loss: 0.4134761430323124, Final Batch Loss: 0.2684396207332611\n",
      "Epoch 3071, Loss: 0.16431695222854614, Final Batch Loss: 0.05305333808064461\n",
      "Epoch 3072, Loss: 0.24409440159797668, Final Batch Loss: 0.03228512406349182\n",
      "Epoch 3073, Loss: 0.3223046064376831, Final Batch Loss: 0.16923877596855164\n",
      "Epoch 3074, Loss: 0.3273007422685623, Final Batch Loss: 0.12092787027359009\n",
      "Epoch 3075, Loss: 0.22980138286948204, Final Batch Loss: 0.043768566101789474\n",
      "Epoch 3076, Loss: 0.15315375477075577, Final Batch Loss: 0.03582882881164551\n",
      "Epoch 3077, Loss: 0.15743683278560638, Final Batch Loss: 0.022794175893068314\n",
      "Epoch 3078, Loss: 0.11714491061866283, Final Batch Loss: 0.010944558307528496\n",
      "Epoch 3079, Loss: 0.2577487491071224, Final Batch Loss: 0.13895826041698456\n",
      "Epoch 3080, Loss: 0.2312232218682766, Final Batch Loss: 0.08421924710273743\n",
      "Epoch 3081, Loss: 0.18317508324980736, Final Batch Loss: 0.05826951935887337\n",
      "Epoch 3082, Loss: 0.1915901079773903, Final Batch Loss: 0.0827709287405014\n",
      "Epoch 3083, Loss: 0.3028864711523056, Final Batch Loss: 0.07955264300107956\n",
      "Epoch 3084, Loss: 0.13878240063786507, Final Batch Loss: 0.033665549010038376\n",
      "Epoch 3085, Loss: 0.11169135943055153, Final Batch Loss: 0.04609445855021477\n",
      "Epoch 3086, Loss: 0.2791517749428749, Final Batch Loss: 0.11645283550024033\n",
      "Epoch 3087, Loss: 0.15444112895056605, Final Batch Loss: 0.004678997676819563\n",
      "Epoch 3088, Loss: 0.1830135565251112, Final Batch Loss: 0.023334546014666557\n",
      "Epoch 3089, Loss: 0.15244398266077042, Final Batch Loss: 0.046301499009132385\n",
      "Epoch 3090, Loss: 0.32618679478764534, Final Batch Loss: 0.20134733617305756\n",
      "Epoch 3091, Loss: 0.16777285188436508, Final Batch Loss: 0.0688445121049881\n",
      "Epoch 3092, Loss: 0.2946946993470192, Final Batch Loss: 0.13164089620113373\n",
      "Epoch 3093, Loss: 0.11426308285444975, Final Batch Loss: 0.012062548659741879\n",
      "Epoch 3094, Loss: 0.3075835034251213, Final Batch Loss: 0.13710278272628784\n",
      "Epoch 3095, Loss: 0.2510915659368038, Final Batch Loss: 0.020988840609788895\n",
      "Epoch 3096, Loss: 0.1982782781124115, Final Batch Loss: 0.06800054758787155\n",
      "Epoch 3097, Loss: 0.15302824974060059, Final Batch Loss: 0.024960599839687347\n",
      "Epoch 3098, Loss: 0.25220488011837006, Final Batch Loss: 0.09546229243278503\n",
      "Epoch 3099, Loss: 0.29594988375902176, Final Batch Loss: 0.10140997171401978\n",
      "Epoch 3100, Loss: 0.15918195992708206, Final Batch Loss: 0.037402525544166565\n",
      "Epoch 3101, Loss: 0.20327603816986084, Final Batch Loss: 0.044224467128515244\n",
      "Epoch 3102, Loss: 0.21338123083114624, Final Batch Loss: 0.07333365082740784\n",
      "Epoch 3103, Loss: 0.11210023611783981, Final Batch Loss: 0.028949977830052376\n",
      "Epoch 3104, Loss: 0.10633040964603424, Final Batch Loss: 0.01111270859837532\n",
      "Epoch 3105, Loss: 0.1036017294973135, Final Batch Loss: 0.017572319135069847\n",
      "Epoch 3106, Loss: 0.09564663097262383, Final Batch Loss: 0.012690313160419464\n",
      "Epoch 3107, Loss: 0.0908465450629592, Final Batch Loss: 0.00904680322855711\n",
      "Epoch 3108, Loss: 0.1261637583374977, Final Batch Loss: 0.04310134798288345\n",
      "Epoch 3109, Loss: 0.13182809576392174, Final Batch Loss: 0.03602767363190651\n",
      "Epoch 3110, Loss: 0.15977103263139725, Final Batch Loss: 0.03602050617337227\n",
      "Epoch 3111, Loss: 0.16745207272469997, Final Batch Loss: 0.013235358521342278\n",
      "Epoch 3112, Loss: 0.15193294547498226, Final Batch Loss: 0.029189296066761017\n",
      "Epoch 3113, Loss: 0.12121716234833002, Final Batch Loss: 0.006715242750942707\n",
      "Epoch 3114, Loss: 0.19724074751138687, Final Batch Loss: 0.127971351146698\n",
      "Epoch 3115, Loss: 0.16761967539787292, Final Batch Loss: 0.07275141775608063\n",
      "Epoch 3116, Loss: 0.12086068093776703, Final Batch Loss: 0.016780979931354523\n",
      "Epoch 3117, Loss: 0.10869697481393814, Final Batch Loss: 0.009399808943271637\n",
      "Epoch 3118, Loss: 0.1031791977584362, Final Batch Loss: 0.01730673387646675\n",
      "Epoch 3119, Loss: 0.10901770135387778, Final Batch Loss: 0.006639746483415365\n",
      "Epoch 3120, Loss: 0.13048235140740871, Final Batch Loss: 0.06941792368888855\n",
      "Epoch 3121, Loss: 0.11563649587333202, Final Batch Loss: 0.0042082201689481735\n",
      "Epoch 3122, Loss: 0.12827349826693535, Final Batch Loss: 0.026208337396383286\n",
      "Epoch 3123, Loss: 0.11703774333000183, Final Batch Loss: 0.05421961843967438\n",
      "Epoch 3124, Loss: 0.12412318587303162, Final Batch Loss: 0.01906483620405197\n",
      "Epoch 3125, Loss: 0.17556868493556976, Final Batch Loss: 0.08296434581279755\n",
      "Epoch 3126, Loss: 0.3419087678194046, Final Batch Loss: 0.2058485597372055\n",
      "Epoch 3127, Loss: 0.29228442162275314, Final Batch Loss: 0.1326213926076889\n",
      "Epoch 3128, Loss: 0.1341050323098898, Final Batch Loss: 0.017508160322904587\n",
      "Epoch 3129, Loss: 0.2668237164616585, Final Batch Loss: 0.10438619554042816\n",
      "Epoch 3130, Loss: 0.18621979281306267, Final Batch Loss: 0.03492274135351181\n",
      "Epoch 3131, Loss: 0.1514708586037159, Final Batch Loss: 0.061309587210416794\n",
      "Epoch 3132, Loss: 0.2594999745488167, Final Batch Loss: 0.11515889316797256\n",
      "Epoch 3133, Loss: 0.23071640357375145, Final Batch Loss: 0.16185517609119415\n",
      "Epoch 3134, Loss: 0.23457935824990273, Final Batch Loss: 0.05592923238873482\n",
      "Epoch 3135, Loss: 0.29810018092393875, Final Batch Loss: 0.13135585188865662\n",
      "Epoch 3136, Loss: 0.11963977944105864, Final Batch Loss: 0.009115572087466717\n",
      "Epoch 3137, Loss: 0.12824881821870804, Final Batch Loss: 0.020321235060691833\n",
      "Epoch 3138, Loss: 0.18961238116025925, Final Batch Loss: 0.0493573434650898\n",
      "Epoch 3139, Loss: 0.1721494011580944, Final Batch Loss: 0.07449856400489807\n",
      "Epoch 3140, Loss: 0.1863216608762741, Final Batch Loss: 0.0605069063603878\n",
      "Epoch 3141, Loss: 0.16049796901643276, Final Batch Loss: 0.022567076608538628\n",
      "Epoch 3142, Loss: 0.12313298508524895, Final Batch Loss: 0.04307785630226135\n",
      "Epoch 3143, Loss: 0.13343729823827744, Final Batch Loss: 0.04017815738916397\n",
      "Epoch 3144, Loss: 0.16888312622904778, Final Batch Loss: 0.06357292085886002\n",
      "Epoch 3145, Loss: 0.10715414211153984, Final Batch Loss: 0.022844582796096802\n",
      "Epoch 3146, Loss: 0.17044377326965332, Final Batch Loss: 0.07638794183731079\n",
      "Epoch 3147, Loss: 0.23786183446645737, Final Batch Loss: 0.11481054872274399\n",
      "Epoch 3148, Loss: 0.2435862496495247, Final Batch Loss: 0.13254931569099426\n",
      "Epoch 3149, Loss: 0.1005558930337429, Final Batch Loss: 0.014733187854290009\n",
      "Epoch 3150, Loss: 0.11509275063872337, Final Batch Loss: 0.03018195554614067\n",
      "Epoch 3151, Loss: 0.11658053379505873, Final Batch Loss: 0.009686320088803768\n",
      "Epoch 3152, Loss: 0.1204876508563757, Final Batch Loss: 0.02057134546339512\n",
      "Epoch 3153, Loss: 0.190085144713521, Final Batch Loss: 0.11835048347711563\n",
      "Epoch 3154, Loss: 0.09318692795932293, Final Batch Loss: 0.009655846282839775\n",
      "Epoch 3155, Loss: 0.11878591775894165, Final Batch Loss: 0.022233713418245316\n",
      "Epoch 3156, Loss: 0.14294127188622952, Final Batch Loss: 0.06367895007133484\n",
      "Epoch 3157, Loss: 0.09885280020534992, Final Batch Loss: 0.02016729675233364\n",
      "Epoch 3158, Loss: 0.12777095660567284, Final Batch Loss: 0.04180383309721947\n",
      "Epoch 3159, Loss: 0.11911188252270222, Final Batch Loss: 0.017645573243498802\n",
      "Epoch 3160, Loss: 0.12797854654490948, Final Batch Loss: 0.04533861577510834\n",
      "Epoch 3161, Loss: 0.20831838622689247, Final Batch Loss: 0.060898225754499435\n",
      "Epoch 3162, Loss: 0.1042116079479456, Final Batch Loss: 0.04175879433751106\n",
      "Epoch 3163, Loss: 0.13127943500876427, Final Batch Loss: 0.038492776453495026\n",
      "Epoch 3164, Loss: 0.14119873195886612, Final Batch Loss: 0.0464358814060688\n",
      "Epoch 3165, Loss: 0.1547834910452366, Final Batch Loss: 0.047646865248680115\n",
      "Epoch 3166, Loss: 0.23230717703700066, Final Batch Loss: 0.09422606229782104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3167, Loss: 0.12617507576942444, Final Batch Loss: 0.03602677956223488\n",
      "Epoch 3168, Loss: 0.07225678209215403, Final Batch Loss: 0.004132435657083988\n",
      "Epoch 3169, Loss: 0.1952984556555748, Final Batch Loss: 0.11470713466405869\n",
      "Epoch 3170, Loss: 0.13922821171581745, Final Batch Loss: 0.028332745656371117\n",
      "Epoch 3171, Loss: 0.16274132579565048, Final Batch Loss: 0.04131611809134483\n",
      "Epoch 3172, Loss: 0.22312356531620026, Final Batch Loss: 0.04411021247506142\n",
      "Epoch 3173, Loss: 0.10137726226821542, Final Batch Loss: 0.004828075412660837\n",
      "Epoch 3174, Loss: 0.24724702909588814, Final Batch Loss: 0.13259683549404144\n",
      "Epoch 3175, Loss: 0.13343925215303898, Final Batch Loss: 0.023468898609280586\n",
      "Epoch 3176, Loss: 0.31042196601629257, Final Batch Loss: 0.14154991507530212\n",
      "Epoch 3177, Loss: 0.21536246314644814, Final Batch Loss: 0.12134573608636856\n",
      "Epoch 3178, Loss: 0.11886314302682877, Final Batch Loss: 0.03695465996861458\n",
      "Epoch 3179, Loss: 0.13651343807578087, Final Batch Loss: 0.02296849712729454\n",
      "Epoch 3180, Loss: 0.11705001257359982, Final Batch Loss: 0.02631126157939434\n",
      "Epoch 3181, Loss: 0.15337495878338814, Final Batch Loss: 0.06764042377471924\n",
      "Epoch 3182, Loss: 0.09777223132550716, Final Batch Loss: 0.013596789911389351\n",
      "Epoch 3183, Loss: 0.19360585883259773, Final Batch Loss: 0.08113761991262436\n",
      "Epoch 3184, Loss: 0.11924132332205772, Final Batch Loss: 0.02882857248187065\n",
      "Epoch 3185, Loss: 0.09820758737623692, Final Batch Loss: 0.027338597923517227\n",
      "Epoch 3186, Loss: 0.20627765357494354, Final Batch Loss: 0.11142610013484955\n",
      "Epoch 3187, Loss: 0.24742740392684937, Final Batch Loss: 0.1477418690919876\n",
      "Epoch 3188, Loss: 0.07695277011953294, Final Batch Loss: 0.0037988496478646994\n",
      "Epoch 3189, Loss: 0.18500951677560806, Final Batch Loss: 0.03911549970507622\n",
      "Epoch 3190, Loss: 0.13740337453782558, Final Batch Loss: 0.08317851275205612\n",
      "Epoch 3191, Loss: 0.10129333101212978, Final Batch Loss: 0.021674832329154015\n",
      "Epoch 3192, Loss: 0.09672034438699484, Final Batch Loss: 0.012841473333537579\n",
      "Epoch 3193, Loss: 0.13900112733244896, Final Batch Loss: 0.05138976499438286\n",
      "Epoch 3194, Loss: 0.10134564340114594, Final Batch Loss: 0.01415703073143959\n",
      "Epoch 3195, Loss: 0.08717984892427921, Final Batch Loss: 0.017684461548924446\n",
      "Epoch 3196, Loss: 0.11563818529248238, Final Batch Loss: 0.022948238998651505\n",
      "Epoch 3197, Loss: 0.1544496901333332, Final Batch Loss: 0.030961450189352036\n",
      "Epoch 3198, Loss: 0.1223247442394495, Final Batch Loss: 0.049601007252931595\n",
      "Epoch 3199, Loss: 0.12904678843915462, Final Batch Loss: 0.06290940195322037\n",
      "Epoch 3200, Loss: 0.1264454871416092, Final Batch Loss: 0.03129652887582779\n",
      "Epoch 3201, Loss: 0.16079297102987766, Final Batch Loss: 0.056029826402664185\n",
      "Epoch 3202, Loss: 0.1095463614910841, Final Batch Loss: 0.023406917229294777\n",
      "Epoch 3203, Loss: 0.12906178506091237, Final Batch Loss: 0.007612655404955149\n",
      "Epoch 3204, Loss: 0.2505716532468796, Final Batch Loss: 0.10171130299568176\n",
      "Epoch 3205, Loss: 0.12209099903702736, Final Batch Loss: 0.01855335757136345\n",
      "Epoch 3206, Loss: 0.10398120619356632, Final Batch Loss: 0.0451970212161541\n",
      "Epoch 3207, Loss: 0.32584570348262787, Final Batch Loss: 0.05405642092227936\n",
      "Epoch 3208, Loss: 0.12495485041290522, Final Batch Loss: 0.013375313021242619\n",
      "Epoch 3209, Loss: 0.1239675460383296, Final Batch Loss: 0.01208904292434454\n",
      "Epoch 3210, Loss: 0.22113680839538574, Final Batch Loss: 0.05190235376358032\n",
      "Epoch 3211, Loss: 0.11312523856759071, Final Batch Loss: 0.03448290750384331\n",
      "Epoch 3212, Loss: 0.2641587406396866, Final Batch Loss: 0.13096089661121368\n",
      "Epoch 3213, Loss: 0.17905570939183235, Final Batch Loss: 0.044537127017974854\n",
      "Epoch 3214, Loss: 0.1568555235862732, Final Batch Loss: 0.041997939348220825\n",
      "Epoch 3215, Loss: 0.15860570222139359, Final Batch Loss: 0.0191195011138916\n",
      "Epoch 3216, Loss: 0.1639498807489872, Final Batch Loss: 0.08061888813972473\n",
      "Epoch 3217, Loss: 0.20137616991996765, Final Batch Loss: 0.037709563970565796\n",
      "Epoch 3218, Loss: 0.13596429862082005, Final Batch Loss: 0.03443339094519615\n",
      "Epoch 3219, Loss: 0.22191205248236656, Final Batch Loss: 0.10976364463567734\n",
      "Epoch 3220, Loss: 0.12218988128006458, Final Batch Loss: 0.024913936853408813\n",
      "Epoch 3221, Loss: 0.12239141389727592, Final Batch Loss: 0.020305972546339035\n",
      "Epoch 3222, Loss: 0.1311420639976859, Final Batch Loss: 0.015314600430428982\n",
      "Epoch 3223, Loss: 0.18211560510098934, Final Batch Loss: 0.09507548809051514\n",
      "Epoch 3224, Loss: 0.1761392392218113, Final Batch Loss: 0.03469139337539673\n",
      "Epoch 3225, Loss: 0.19087783619761467, Final Batch Loss: 0.029117044061422348\n",
      "Epoch 3226, Loss: 0.18238424882292747, Final Batch Loss: 0.10228274762630463\n",
      "Epoch 3227, Loss: 0.1423533009365201, Final Batch Loss: 0.014888585545122623\n",
      "Epoch 3228, Loss: 0.263489980250597, Final Batch Loss: 0.14464977383613586\n",
      "Epoch 3229, Loss: 0.20314153656363487, Final Batch Loss: 0.07117053866386414\n",
      "Epoch 3230, Loss: 0.21831127256155014, Final Batch Loss: 0.043317172676324844\n",
      "Epoch 3231, Loss: 0.2375670187175274, Final Batch Loss: 0.1081581711769104\n",
      "Epoch 3232, Loss: 0.15346483141183853, Final Batch Loss: 0.02763499692082405\n",
      "Epoch 3233, Loss: 0.23948300629854202, Final Batch Loss: 0.0604732409119606\n",
      "Epoch 3234, Loss: 0.21468180790543556, Final Batch Loss: 0.06394078582525253\n",
      "Epoch 3235, Loss: 0.2274005487561226, Final Batch Loss: 0.06390351057052612\n",
      "Epoch 3236, Loss: 0.4327124357223511, Final Batch Loss: 0.2895069420337677\n",
      "Epoch 3237, Loss: 0.14299484342336655, Final Batch Loss: 0.05784272029995918\n",
      "Epoch 3238, Loss: 0.3120790868997574, Final Batch Loss: 0.16214871406555176\n",
      "Epoch 3239, Loss: 0.2397315874695778, Final Batch Loss: 0.05270896852016449\n",
      "Epoch 3240, Loss: 0.13686218485236168, Final Batch Loss: 0.01737261563539505\n",
      "Epoch 3241, Loss: 0.1874827966094017, Final Batch Loss: 0.07158500701189041\n",
      "Epoch 3242, Loss: 0.16220179572701454, Final Batch Loss: 0.03421957418322563\n",
      "Epoch 3243, Loss: 0.12755172979086637, Final Batch Loss: 0.014385241083800793\n",
      "Epoch 3244, Loss: 0.13734290562570095, Final Batch Loss: 0.008104296401143074\n",
      "Epoch 3245, Loss: 0.11954030767083168, Final Batch Loss: 0.011400386691093445\n",
      "Epoch 3246, Loss: 0.24785607680678368, Final Batch Loss: 0.1151789128780365\n",
      "Epoch 3247, Loss: 0.10296089015901089, Final Batch Loss: 0.036083728075027466\n",
      "Epoch 3248, Loss: 0.1333727315068245, Final Batch Loss: 0.008352212607860565\n",
      "Epoch 3249, Loss: 0.08754821447655559, Final Batch Loss: 0.007136236410588026\n",
      "Epoch 3250, Loss: 0.11809313297271729, Final Batch Loss: 0.017028026282787323\n",
      "Epoch 3251, Loss: 0.14545217528939247, Final Batch Loss: 0.050019003450870514\n",
      "Epoch 3252, Loss: 0.06583096738904715, Final Batch Loss: 0.017367340624332428\n",
      "Epoch 3253, Loss: 0.10696599073708057, Final Batch Loss: 0.034957632422447205\n",
      "Epoch 3254, Loss: 0.155399139970541, Final Batch Loss: 0.063546322286129\n",
      "Epoch 3255, Loss: 0.11164255440235138, Final Batch Loss: 0.02531057968735695\n",
      "Epoch 3256, Loss: 0.15399983897805214, Final Batch Loss: 0.03829445689916611\n",
      "Epoch 3257, Loss: 0.09991532098501921, Final Batch Loss: 0.01246145274490118\n",
      "Epoch 3258, Loss: 0.08115647407248616, Final Batch Loss: 0.00363538833335042\n",
      "Epoch 3259, Loss: 0.20812706649303436, Final Batch Loss: 0.12329360842704773\n",
      "Epoch 3260, Loss: 0.11488507874310017, Final Batch Loss: 0.044871680438518524\n",
      "Epoch 3261, Loss: 0.26938873901963234, Final Batch Loss: 0.11794647574424744\n",
      "Epoch 3262, Loss: 0.17861858010292053, Final Batch Loss: 0.0740102007985115\n",
      "Epoch 3263, Loss: 0.15476380288600922, Final Batch Loss: 0.034712087363004684\n",
      "Epoch 3264, Loss: 0.18186787143349648, Final Batch Loss: 0.023724708706140518\n",
      "Epoch 3265, Loss: 0.10564610734581947, Final Batch Loss: 0.017022915184497833\n",
      "Epoch 3266, Loss: 0.4472954384982586, Final Batch Loss: 0.37316223978996277\n",
      "Epoch 3267, Loss: 0.083881551399827, Final Batch Loss: 0.016466336324810982\n",
      "Epoch 3268, Loss: 0.06995926797389984, Final Batch Loss: 0.016699956730008125\n",
      "Epoch 3269, Loss: 0.1984558440744877, Final Batch Loss: 0.09070069342851639\n",
      "Epoch 3270, Loss: 0.134177902713418, Final Batch Loss: 0.06462030857801437\n",
      "Epoch 3271, Loss: 0.14201652258634567, Final Batch Loss: 0.03644094243645668\n",
      "Epoch 3272, Loss: 0.12444625794887543, Final Batch Loss: 0.02250150591135025\n",
      "Epoch 3273, Loss: 0.14582328125834465, Final Batch Loss: 0.0346609465777874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3274, Loss: 0.15410581231117249, Final Batch Loss: 0.047109127044677734\n",
      "Epoch 3275, Loss: 0.22465937584638596, Final Batch Loss: 0.11727466434240341\n",
      "Epoch 3276, Loss: 0.26141270250082016, Final Batch Loss: 0.06058137118816376\n",
      "Epoch 3277, Loss: 0.1883739661425352, Final Batch Loss: 0.12428907305002213\n",
      "Epoch 3278, Loss: 0.187717005610466, Final Batch Loss: 0.09684943407773972\n",
      "Epoch 3279, Loss: 0.14995808992534876, Final Batch Loss: 0.012374901212751865\n",
      "Epoch 3280, Loss: 0.1193355843424797, Final Batch Loss: 0.04080849513411522\n",
      "Epoch 3281, Loss: 0.10936278477311134, Final Batch Loss: 0.022982750087976456\n",
      "Epoch 3282, Loss: 0.0657883882522583, Final Batch Loss: 0.004642672836780548\n",
      "Epoch 3283, Loss: 0.18321789801120758, Final Batch Loss: 0.05663427338004112\n",
      "Epoch 3284, Loss: 0.11265608295798302, Final Batch Loss: 0.03854081407189369\n",
      "Epoch 3285, Loss: 0.12455128971487284, Final Batch Loss: 0.010741339065134525\n",
      "Epoch 3286, Loss: 0.12031678110361099, Final Batch Loss: 0.042004987597465515\n",
      "Epoch 3287, Loss: 0.1026057917624712, Final Batch Loss: 0.030132347717881203\n",
      "Epoch 3288, Loss: 0.08506954740732908, Final Batch Loss: 0.034235987812280655\n",
      "Epoch 3289, Loss: 0.07498728297650814, Final Batch Loss: 0.019518662244081497\n",
      "Epoch 3290, Loss: 0.15088007971644402, Final Batch Loss: 0.06723413616418839\n",
      "Epoch 3291, Loss: 0.23363321647047997, Final Batch Loss: 0.10298284143209457\n",
      "Epoch 3292, Loss: 0.14063281379640102, Final Batch Loss: 0.0486542247235775\n",
      "Epoch 3293, Loss: 0.13730674609541893, Final Batch Loss: 0.03843168541789055\n",
      "Epoch 3294, Loss: 0.11854498088359833, Final Batch Loss: 0.04733030125498772\n",
      "Epoch 3295, Loss: 0.11442924500443041, Final Batch Loss: 0.003277981886640191\n",
      "Epoch 3296, Loss: 0.2975190132856369, Final Batch Loss: 0.1903878152370453\n",
      "Epoch 3297, Loss: 0.11762580834329128, Final Batch Loss: 0.021676594391465187\n",
      "Epoch 3298, Loss: 0.11163099762052298, Final Batch Loss: 0.010452407412230968\n",
      "Epoch 3299, Loss: 0.13638837356120348, Final Batch Loss: 0.010755118913948536\n",
      "Epoch 3300, Loss: 0.08172773942351341, Final Batch Loss: 0.0053118746727705\n",
      "Epoch 3301, Loss: 0.1270573865622282, Final Batch Loss: 0.05152062326669693\n",
      "Epoch 3302, Loss: 0.1138385571539402, Final Batch Loss: 0.015707772225141525\n",
      "Epoch 3303, Loss: 0.1562853530049324, Final Batch Loss: 0.0519428625702858\n",
      "Epoch 3304, Loss: 0.08065236546099186, Final Batch Loss: 0.01805819757282734\n",
      "Epoch 3305, Loss: 0.12429172080010176, Final Batch Loss: 0.011090575717389584\n",
      "Epoch 3306, Loss: 0.14116616919636726, Final Batch Loss: 0.03984246030449867\n",
      "Epoch 3307, Loss: 0.08992507122457027, Final Batch Loss: 0.010678326711058617\n",
      "Epoch 3308, Loss: 0.25231996551156044, Final Batch Loss: 0.16270576417446136\n",
      "Epoch 3309, Loss: 0.13178998976945877, Final Batch Loss: 0.05992453545331955\n",
      "Epoch 3310, Loss: 0.09953078255057335, Final Batch Loss: 0.031080426648259163\n",
      "Epoch 3311, Loss: 0.24407587945461273, Final Batch Loss: 0.08637355268001556\n",
      "Epoch 3312, Loss: 0.09768037311732769, Final Batch Loss: 0.01097673736512661\n",
      "Epoch 3313, Loss: 0.10335346311330795, Final Batch Loss: 0.01751835271716118\n",
      "Epoch 3314, Loss: 0.14265945181250572, Final Batch Loss: 0.04501338675618172\n",
      "Epoch 3315, Loss: 0.16469438932836056, Final Batch Loss: 0.08997462689876556\n",
      "Epoch 3316, Loss: 0.15526104345917702, Final Batch Loss: 0.019309740513563156\n",
      "Epoch 3317, Loss: 0.12497084960341454, Final Batch Loss: 0.022879069671034813\n",
      "Epoch 3318, Loss: 0.2645578682422638, Final Batch Loss: 0.06464867293834686\n",
      "Epoch 3319, Loss: 0.1307605244219303, Final Batch Loss: 0.01760241389274597\n",
      "Epoch 3320, Loss: 0.17992783896625042, Final Batch Loss: 0.022513655945658684\n",
      "Epoch 3321, Loss: 0.12768820486962795, Final Batch Loss: 0.022604288533329964\n",
      "Epoch 3322, Loss: 0.24316057562828064, Final Batch Loss: 0.1502055674791336\n",
      "Epoch 3323, Loss: 0.15641985088586807, Final Batch Loss: 0.03303087130188942\n",
      "Epoch 3324, Loss: 0.2616294175386429, Final Batch Loss: 0.0878707692027092\n",
      "Epoch 3325, Loss: 0.1798897236585617, Final Batch Loss: 0.08069033920764923\n",
      "Epoch 3326, Loss: 0.20048977062106133, Final Batch Loss: 0.040961138904094696\n",
      "Epoch 3327, Loss: 0.18972336500883102, Final Batch Loss: 0.06029728055000305\n",
      "Epoch 3328, Loss: 0.23535360395908356, Final Batch Loss: 0.08419169485569\n",
      "Epoch 3329, Loss: 0.1456800401210785, Final Batch Loss: 0.047343816608190536\n",
      "Epoch 3330, Loss: 0.2703641690313816, Final Batch Loss: 0.17602349817752838\n",
      "Epoch 3331, Loss: 0.3551449850201607, Final Batch Loss: 0.16062010824680328\n",
      "Epoch 3332, Loss: 0.5162938460707664, Final Batch Loss: 0.2571823000907898\n",
      "Epoch 3333, Loss: 0.2033875249326229, Final Batch Loss: 0.048726607114076614\n",
      "Epoch 3334, Loss: 0.2698036655783653, Final Batch Loss: 0.0709478110074997\n",
      "Epoch 3335, Loss: 0.12018181756138802, Final Batch Loss: 0.01161324605345726\n",
      "Epoch 3336, Loss: 0.20430801436305046, Final Batch Loss: 0.018319617956876755\n",
      "Epoch 3337, Loss: 0.14191012363880873, Final Batch Loss: 0.004564437083899975\n",
      "Epoch 3338, Loss: 0.1517854779958725, Final Batch Loss: 0.06195884943008423\n",
      "Epoch 3339, Loss: 0.19182085245847702, Final Batch Loss: 0.0872274786233902\n",
      "Epoch 3340, Loss: 0.22030144557356834, Final Batch Loss: 0.0691957026720047\n",
      "Epoch 3341, Loss: 0.13447799161076546, Final Batch Loss: 0.02618269994854927\n",
      "Epoch 3342, Loss: 0.19259627163410187, Final Batch Loss: 0.07867702841758728\n",
      "Epoch 3343, Loss: 0.3250979036092758, Final Batch Loss: 0.13997666537761688\n",
      "Epoch 3344, Loss: 0.19173607230186462, Final Batch Loss: 0.0475522056221962\n",
      "Epoch 3345, Loss: 0.10993666853755713, Final Batch Loss: 0.007915164344012737\n",
      "Epoch 3346, Loss: 0.27603763341903687, Final Batch Loss: 0.12355940788984299\n",
      "Epoch 3347, Loss: 0.14988169074058533, Final Batch Loss: 0.05202484130859375\n",
      "Epoch 3348, Loss: 0.13186550419777632, Final Batch Loss: 0.06501695513725281\n",
      "Epoch 3349, Loss: 0.1601279005408287, Final Batch Loss: 0.05865766853094101\n",
      "Epoch 3350, Loss: 0.37123825401067734, Final Batch Loss: 0.27039211988449097\n",
      "Epoch 3351, Loss: 0.21241975016891956, Final Batch Loss: 0.02593334950506687\n",
      "Epoch 3352, Loss: 0.16250144690275192, Final Batch Loss: 0.040965232998132706\n",
      "Epoch 3353, Loss: 0.23288235813379288, Final Batch Loss: 0.07212142646312714\n",
      "Epoch 3354, Loss: 0.13724862970411777, Final Batch Loss: 0.01669437251985073\n",
      "Epoch 3355, Loss: 0.13658598810434341, Final Batch Loss: 0.021280601620674133\n",
      "Epoch 3356, Loss: 0.40521984174847603, Final Batch Loss: 0.26703232526779175\n",
      "Epoch 3357, Loss: 0.16452466696500778, Final Batch Loss: 0.014366038143634796\n",
      "Epoch 3358, Loss: 0.17659341543912888, Final Batch Loss: 0.04269559681415558\n",
      "Epoch 3359, Loss: 0.17363078147172928, Final Batch Loss: 0.08266711980104446\n",
      "Epoch 3360, Loss: 0.11370065249502659, Final Batch Loss: 0.04711706191301346\n",
      "Epoch 3361, Loss: 0.15484867617487907, Final Batch Loss: 0.0856163278222084\n",
      "Epoch 3362, Loss: 0.13154139555990696, Final Batch Loss: 0.021592991426587105\n",
      "Epoch 3363, Loss: 0.1344243921339512, Final Batch Loss: 0.0642152950167656\n",
      "Epoch 3364, Loss: 0.1526511386036873, Final Batch Loss: 0.05951057747006416\n",
      "Epoch 3365, Loss: 0.1465546116232872, Final Batch Loss: 0.05225616693496704\n",
      "Epoch 3366, Loss: 0.08722325414419174, Final Batch Loss: 0.03738019987940788\n",
      "Epoch 3367, Loss: 0.22084770537912846, Final Batch Loss: 0.13216827809810638\n",
      "Epoch 3368, Loss: 0.10022588260471821, Final Batch Loss: 0.028423508629202843\n",
      "Epoch 3369, Loss: 0.1391890924423933, Final Batch Loss: 0.01611850969493389\n",
      "Epoch 3370, Loss: 0.1760258451104164, Final Batch Loss: 0.05275953188538551\n",
      "Epoch 3371, Loss: 0.14831061474978924, Final Batch Loss: 0.01860448159277439\n",
      "Epoch 3372, Loss: 0.14527293667197227, Final Batch Loss: 0.04885697364807129\n",
      "Epoch 3373, Loss: 0.12863273359835148, Final Batch Loss: 0.005912167951464653\n",
      "Epoch 3374, Loss: 0.1518838508054614, Final Batch Loss: 0.008058986626565456\n",
      "Epoch 3375, Loss: 0.10723717510700226, Final Batch Loss: 0.02372736483812332\n",
      "Epoch 3376, Loss: 0.19227633252739906, Final Batch Loss: 0.09480667114257812\n",
      "Epoch 3377, Loss: 0.12237292155623436, Final Batch Loss: 0.03408422693610191\n",
      "Epoch 3378, Loss: 0.12586134485900402, Final Batch Loss: 0.01948450319468975\n",
      "Epoch 3379, Loss: 0.14002449996769428, Final Batch Loss: 0.061713751405477524\n",
      "Epoch 3380, Loss: 0.09965208172798157, Final Batch Loss: 0.03583260998129845\n",
      "Epoch 3381, Loss: 0.11038882937282324, Final Batch Loss: 0.015262403525412083\n",
      "Epoch 3382, Loss: 0.08724745735526085, Final Batch Loss: 0.02003398910164833\n",
      "Epoch 3383, Loss: 0.1633772887289524, Final Batch Loss: 0.079887256026268\n",
      "Epoch 3384, Loss: 0.25776782631874084, Final Batch Loss: 0.04038155823945999\n",
      "Epoch 3385, Loss: 0.0850503146648407, Final Batch Loss: 0.01684553176164627\n",
      "Epoch 3386, Loss: 0.12967929430305958, Final Batch Loss: 0.016698023304343224\n",
      "Epoch 3387, Loss: 0.10843981802463531, Final Batch Loss: 0.03327694535255432\n",
      "Epoch 3388, Loss: 0.08342262543737888, Final Batch Loss: 0.014696428552269936\n",
      "Epoch 3389, Loss: 0.10480145551264286, Final Batch Loss: 0.020021740347146988\n",
      "Epoch 3390, Loss: 0.0887395404279232, Final Batch Loss: 0.006410803645849228\n",
      "Epoch 3391, Loss: 0.10155652277171612, Final Batch Loss: 0.03842121735215187\n",
      "Epoch 3392, Loss: 0.10551843512803316, Final Batch Loss: 0.008498420007526875\n",
      "Epoch 3393, Loss: 0.10330535098910332, Final Batch Loss: 0.03671732544898987\n",
      "Epoch 3394, Loss: 0.08649939391762018, Final Batch Loss: 0.01281148660928011\n",
      "Epoch 3395, Loss: 0.18443729728460312, Final Batch Loss: 0.021246202290058136\n",
      "Epoch 3396, Loss: 0.18575473874807358, Final Batch Loss: 0.047808390110731125\n",
      "Epoch 3397, Loss: 0.2576463297009468, Final Batch Loss: 0.08547152578830719\n",
      "Epoch 3398, Loss: 0.123329296708107, Final Batch Loss: 0.04560888558626175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3399, Loss: 0.1596747376024723, Final Batch Loss: 0.03821722790598869\n",
      "Epoch 3400, Loss: 0.13262399286031723, Final Batch Loss: 0.02911551296710968\n",
      "Epoch 3401, Loss: 0.16389892250299454, Final Batch Loss: 0.0681307390332222\n",
      "Epoch 3402, Loss: 0.24918080121278763, Final Batch Loss: 0.12364871799945831\n",
      "Epoch 3403, Loss: 0.09556084871292114, Final Batch Loss: 0.013203557580709457\n",
      "Epoch 3404, Loss: 0.21977616101503372, Final Batch Loss: 0.017855338752269745\n",
      "Epoch 3405, Loss: 0.12542006000876427, Final Batch Loss: 0.007962333038449287\n",
      "Epoch 3406, Loss: 0.11563809774816036, Final Batch Loss: 0.013279711827635765\n",
      "Epoch 3407, Loss: 0.14417094085365534, Final Batch Loss: 0.008615669794380665\n",
      "Epoch 3408, Loss: 0.12503383215516806, Final Batch Loss: 0.013094167225062847\n",
      "Epoch 3409, Loss: 0.20833280682563782, Final Batch Loss: 0.10960938036441803\n",
      "Epoch 3410, Loss: 0.13697154819965363, Final Batch Loss: 0.01900029182434082\n",
      "Epoch 3411, Loss: 0.11508064717054367, Final Batch Loss: 0.03302108496427536\n",
      "Epoch 3412, Loss: 0.18035098910331726, Final Batch Loss: 0.05189918354153633\n",
      "Epoch 3413, Loss: 0.11563296429812908, Final Batch Loss: 0.022076420485973358\n",
      "Epoch 3414, Loss: 0.4235174097120762, Final Batch Loss: 0.34848347306251526\n",
      "Epoch 3415, Loss: 0.2335675172507763, Final Batch Loss: 0.09783578664064407\n",
      "Epoch 3416, Loss: 0.3443658724427223, Final Batch Loss: 0.15692353248596191\n",
      "Epoch 3417, Loss: 0.5761478953063488, Final Batch Loss: 0.4312059283256531\n",
      "Epoch 3418, Loss: 0.22126369923353195, Final Batch Loss: 0.11613856256008148\n",
      "Epoch 3419, Loss: 0.33430370688438416, Final Batch Loss: 0.14784123003482819\n",
      "Epoch 3420, Loss: 0.24974051862955093, Final Batch Loss: 0.07336579263210297\n",
      "Epoch 3421, Loss: 0.24699627608060837, Final Batch Loss: 0.07529890537261963\n",
      "Epoch 3422, Loss: 0.27620328962802887, Final Batch Loss: 0.09080656617879868\n",
      "Epoch 3423, Loss: 0.2635086178779602, Final Batch Loss: 0.10117289423942566\n",
      "Epoch 3424, Loss: 0.15148990601301193, Final Batch Loss: 0.015496280044317245\n",
      "Epoch 3425, Loss: 0.2572401110082865, Final Batch Loss: 0.1599263697862625\n",
      "Epoch 3426, Loss: 0.16533639281988144, Final Batch Loss: 0.034416306763887405\n",
      "Epoch 3427, Loss: 0.14652564749121666, Final Batch Loss: 0.03456301614642143\n",
      "Epoch 3428, Loss: 0.09270639531314373, Final Batch Loss: 0.009529715403914452\n",
      "Epoch 3429, Loss: 0.218430794775486, Final Batch Loss: 0.08809246122837067\n",
      "Epoch 3430, Loss: 0.11002400144934654, Final Batch Loss: 0.013950500637292862\n",
      "Epoch 3431, Loss: 0.13420098274946213, Final Batch Loss: 0.03936530649662018\n",
      "Epoch 3432, Loss: 0.1393768060952425, Final Batch Loss: 0.08228668570518494\n",
      "Epoch 3433, Loss: 0.12099307775497437, Final Batch Loss: 0.04433509334921837\n",
      "Epoch 3434, Loss: 0.13922137767076492, Final Batch Loss: 0.06685050576925278\n",
      "Epoch 3435, Loss: 0.11248662322759628, Final Batch Loss: 0.03022640198469162\n",
      "Epoch 3436, Loss: 0.16431604325771332, Final Batch Loss: 0.05243487283587456\n",
      "Epoch 3437, Loss: 0.13069979194551706, Final Batch Loss: 0.010023673065006733\n",
      "Epoch 3438, Loss: 0.1272602528333664, Final Batch Loss: 0.06106926500797272\n",
      "Epoch 3439, Loss: 0.09420393407344818, Final Batch Loss: 0.03142646327614784\n",
      "Epoch 3440, Loss: 0.08837033994495869, Final Batch Loss: 0.01607474870979786\n",
      "Epoch 3441, Loss: 0.11860297434031963, Final Batch Loss: 0.06262999773025513\n",
      "Epoch 3442, Loss: 0.19558367878198624, Final Batch Loss: 0.13262324035167694\n",
      "Epoch 3443, Loss: 0.12189386039972305, Final Batch Loss: 0.011334624141454697\n",
      "Epoch 3444, Loss: 0.17883862555027008, Final Batch Loss: 0.033446673303842545\n",
      "Epoch 3445, Loss: 0.1390887312591076, Final Batch Loss: 0.04718849062919617\n",
      "Epoch 3446, Loss: 0.1188780595548451, Final Batch Loss: 0.00379691319540143\n",
      "Epoch 3447, Loss: 0.1474906373769045, Final Batch Loss: 0.047357864677906036\n",
      "Epoch 3448, Loss: 0.17004449106752872, Final Batch Loss: 0.026304611936211586\n",
      "Epoch 3449, Loss: 0.11693968065083027, Final Batch Loss: 0.018739523366093636\n",
      "Epoch 3450, Loss: 0.13279050216078758, Final Batch Loss: 0.0339188426733017\n",
      "Epoch 3451, Loss: 0.10834913328289986, Final Batch Loss: 0.038767389953136444\n",
      "Epoch 3452, Loss: 0.06584594864398241, Final Batch Loss: 0.009444757364690304\n",
      "Epoch 3453, Loss: 0.11435569263994694, Final Batch Loss: 0.023985378444194794\n",
      "Epoch 3454, Loss: 0.21161218360066414, Final Batch Loss: 0.08052223175764084\n",
      "Epoch 3455, Loss: 0.12431024573743343, Final Batch Loss: 0.01934867724776268\n",
      "Epoch 3456, Loss: 0.2687299959361553, Final Batch Loss: 0.14162977039813995\n",
      "Epoch 3457, Loss: 0.22633021138608456, Final Batch Loss: 0.026528222486376762\n",
      "Epoch 3458, Loss: 0.14113329723477364, Final Batch Loss: 0.05742613971233368\n",
      "Epoch 3459, Loss: 0.12174747511744499, Final Batch Loss: 0.016446858644485474\n",
      "Epoch 3460, Loss: 0.14630908891558647, Final Batch Loss: 0.026240669190883636\n",
      "Epoch 3461, Loss: 0.10929016768932343, Final Batch Loss: 0.025363735854625702\n",
      "Epoch 3462, Loss: 0.09491638094186783, Final Batch Loss: 0.008981253951787949\n",
      "Epoch 3463, Loss: 0.15848230570554733, Final Batch Loss: 0.054639123380184174\n",
      "Epoch 3464, Loss: 0.08839831012301147, Final Batch Loss: 0.0024566061329096556\n",
      "Epoch 3465, Loss: 0.14896737411618233, Final Batch Loss: 0.046571601182222366\n",
      "Epoch 3466, Loss: 0.11508909985423088, Final Batch Loss: 0.03374752774834633\n",
      "Epoch 3467, Loss: 0.22644099034368992, Final Batch Loss: 0.017794525250792503\n",
      "Epoch 3468, Loss: 0.056827944703400135, Final Batch Loss: 0.0064697181805968285\n",
      "Epoch 3469, Loss: 0.05951818637549877, Final Batch Loss: 0.019093038514256477\n",
      "Epoch 3470, Loss: 0.06393972411751747, Final Batch Loss: 0.01934828795492649\n",
      "Epoch 3471, Loss: 0.08337854593992233, Final Batch Loss: 0.01994462125003338\n",
      "Epoch 3472, Loss: 0.1519133597612381, Final Batch Loss: 0.030847832560539246\n",
      "Epoch 3473, Loss: 0.11199003458023071, Final Batch Loss: 0.040855567902326584\n",
      "Epoch 3474, Loss: 0.17482204735279083, Final Batch Loss: 0.030713971704244614\n",
      "Epoch 3475, Loss: 0.07740713097155094, Final Batch Loss: 0.02432141825556755\n",
      "Epoch 3476, Loss: 0.08562341332435608, Final Batch Loss: 0.03200206160545349\n",
      "Epoch 3477, Loss: 0.09261906240135431, Final Batch Loss: 0.012807083316147327\n",
      "Epoch 3478, Loss: 0.0912220012396574, Final Batch Loss: 0.044181421399116516\n",
      "Epoch 3479, Loss: 0.08616933412849903, Final Batch Loss: 0.014018204063177109\n",
      "Epoch 3480, Loss: 0.05299275740981102, Final Batch Loss: 0.004393300041556358\n",
      "Epoch 3481, Loss: 0.12272443808615208, Final Batch Loss: 0.050387192517519\n",
      "Epoch 3482, Loss: 0.15343879908323288, Final Batch Loss: 0.062477122992277145\n",
      "Epoch 3483, Loss: 0.13141515105962753, Final Batch Loss: 0.023887209594249725\n",
      "Epoch 3484, Loss: 0.18550192937254906, Final Batch Loss: 0.1156250536441803\n",
      "Epoch 3485, Loss: 0.18183835595846176, Final Batch Loss: 0.08488278090953827\n",
      "Epoch 3486, Loss: 0.14694585651159286, Final Batch Loss: 0.08425404131412506\n",
      "Epoch 3487, Loss: 0.14524893648922443, Final Batch Loss: 0.027565525844693184\n",
      "Epoch 3488, Loss: 0.07090591080486774, Final Batch Loss: 0.022179758176207542\n",
      "Epoch 3489, Loss: 0.11507730558514595, Final Batch Loss: 0.018454868346452713\n",
      "Epoch 3490, Loss: 0.13329028338193893, Final Batch Loss: 0.03368961438536644\n",
      "Epoch 3491, Loss: 0.15600815415382385, Final Batch Loss: 0.06935694813728333\n",
      "Epoch 3492, Loss: 0.07257714308798313, Final Batch Loss: 0.018588684499263763\n",
      "Epoch 3493, Loss: 0.1532755345106125, Final Batch Loss: 0.07444792985916138\n",
      "Epoch 3494, Loss: 0.13817328214645386, Final Batch Loss: 0.07540564239025116\n",
      "Epoch 3495, Loss: 0.10691029392182827, Final Batch Loss: 0.047128066420555115\n",
      "Epoch 3496, Loss: 0.11349966935813427, Final Batch Loss: 0.06165416166186333\n",
      "Epoch 3497, Loss: 0.10666979383677244, Final Batch Loss: 0.004878371022641659\n",
      "Epoch 3498, Loss: 0.1171455830335617, Final Batch Loss: 0.008641228079795837\n",
      "Epoch 3499, Loss: 0.1265874244272709, Final Batch Loss: 0.054407451301813126\n",
      "Epoch 3500, Loss: 0.1029110923409462, Final Batch Loss: 0.0576401986181736\n",
      "Epoch 3501, Loss: 0.18622317165136337, Final Batch Loss: 0.1264815330505371\n",
      "Epoch 3502, Loss: 0.1208646148443222, Final Batch Loss: 0.0305902399122715\n",
      "Epoch 3503, Loss: 0.0938316322863102, Final Batch Loss: 0.012696720659732819\n",
      "Epoch 3504, Loss: 0.11013023369014263, Final Batch Loss: 0.048820096999406815\n",
      "Epoch 3505, Loss: 0.15485092252492905, Final Batch Loss: 0.0386110357940197\n",
      "Epoch 3506, Loss: 0.13763807713985443, Final Batch Loss: 0.06404437869787216\n",
      "Epoch 3507, Loss: 0.0961693599820137, Final Batch Loss: 0.029839908704161644\n",
      "Epoch 3508, Loss: 0.14780998975038528, Final Batch Loss: 0.026980213820934296\n",
      "Epoch 3509, Loss: 0.11747894575819373, Final Batch Loss: 0.006909513380378485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3510, Loss: 0.14635552279651165, Final Batch Loss: 0.01741356961429119\n",
      "Epoch 3511, Loss: 0.13990294467657804, Final Batch Loss: 0.09740561246871948\n",
      "Epoch 3512, Loss: 0.15389136224985123, Final Batch Loss: 0.03749510273337364\n",
      "Epoch 3513, Loss: 0.19007484428584576, Final Batch Loss: 0.025182506069540977\n",
      "Epoch 3514, Loss: 0.23136307671666145, Final Batch Loss: 0.1408902406692505\n",
      "Epoch 3515, Loss: 0.09344757162034512, Final Batch Loss: 0.04949551820755005\n",
      "Epoch 3516, Loss: 0.22395269572734833, Final Batch Loss: 0.03227812796831131\n",
      "Epoch 3517, Loss: 0.13686428591609, Final Batch Loss: 0.03242108225822449\n",
      "Epoch 3518, Loss: 0.14574667438864708, Final Batch Loss: 0.023063156753778458\n",
      "Epoch 3519, Loss: 0.18881062045693398, Final Batch Loss: 0.054395437240600586\n",
      "Epoch 3520, Loss: 0.07370841130614281, Final Batch Loss: 0.009795960038900375\n",
      "Epoch 3521, Loss: 0.27120573818683624, Final Batch Loss: 0.060235291719436646\n",
      "Epoch 3522, Loss: 0.1340593844652176, Final Batch Loss: 0.04185470938682556\n",
      "Epoch 3523, Loss: 0.24892234057188034, Final Batch Loss: 0.16365940868854523\n",
      "Epoch 3524, Loss: 0.2175779789686203, Final Batch Loss: 0.03697305917739868\n",
      "Epoch 3525, Loss: 0.27650468377396464, Final Batch Loss: 0.0068280878476798534\n",
      "Epoch 3526, Loss: 0.2135055512189865, Final Batch Loss: 0.07455004751682281\n",
      "Epoch 3527, Loss: 0.21113968268036842, Final Batch Loss: 0.11827472597360611\n",
      "Epoch 3528, Loss: 0.25933245196938515, Final Batch Loss: 0.1347423940896988\n",
      "Epoch 3529, Loss: 0.11760133877396584, Final Batch Loss: 0.05500645563006401\n",
      "Epoch 3530, Loss: 0.16246547363698483, Final Batch Loss: 0.06345818191766739\n",
      "Epoch 3531, Loss: 0.1532230991870165, Final Batch Loss: 0.06445100158452988\n",
      "Epoch 3532, Loss: 0.1783664207905531, Final Batch Loss: 0.01920594833791256\n",
      "Epoch 3533, Loss: 0.31400833278894424, Final Batch Loss: 0.1587226837873459\n",
      "Epoch 3534, Loss: 0.1828855499625206, Final Batch Loss: 0.043252892792224884\n",
      "Epoch 3535, Loss: 0.2299736812710762, Final Batch Loss: 0.11296036094427109\n",
      "Epoch 3536, Loss: 0.18271870352327824, Final Batch Loss: 0.02156163938343525\n",
      "Epoch 3537, Loss: 0.22994652390480042, Final Batch Loss: 0.10397516191005707\n",
      "Epoch 3538, Loss: 0.11524954065680504, Final Batch Loss: 0.023757033050060272\n",
      "Epoch 3539, Loss: 0.14034057036042213, Final Batch Loss: 0.0194949209690094\n",
      "Epoch 3540, Loss: 0.2857067249715328, Final Batch Loss: 0.15930335223674774\n",
      "Epoch 3541, Loss: 0.27636992931365967, Final Batch Loss: 0.1923549920320511\n",
      "Epoch 3542, Loss: 0.1365436092019081, Final Batch Loss: 0.049317583441734314\n",
      "Epoch 3543, Loss: 0.22971607744693756, Final Batch Loss: 0.09456273913383484\n",
      "Epoch 3544, Loss: 0.14984769374132156, Final Batch Loss: 0.057914603501558304\n",
      "Epoch 3545, Loss: 0.13937865011394024, Final Batch Loss: 0.0112044308334589\n",
      "Epoch 3546, Loss: 0.238966703414917, Final Batch Loss: 0.1213512122631073\n",
      "Epoch 3547, Loss: 0.307003416121006, Final Batch Loss: 0.21055303514003754\n",
      "Epoch 3548, Loss: 0.2194296456873417, Final Batch Loss: 0.033545371145009995\n",
      "Epoch 3549, Loss: 0.29597102105617523, Final Batch Loss: 0.1688021868467331\n",
      "Epoch 3550, Loss: 0.20356610044836998, Final Batch Loss: 0.040781039744615555\n",
      "Epoch 3551, Loss: 0.15505606308579445, Final Batch Loss: 0.05633513629436493\n",
      "Epoch 3552, Loss: 0.1430025529116392, Final Batch Loss: 0.010862423107028008\n",
      "Epoch 3553, Loss: 0.10125292837619781, Final Batch Loss: 0.01797652803361416\n",
      "Epoch 3554, Loss: 0.08413328602910042, Final Batch Loss: 0.020582307130098343\n",
      "Epoch 3555, Loss: 0.09556392952799797, Final Batch Loss: 0.0212227925658226\n",
      "Epoch 3556, Loss: 0.14145623706281185, Final Batch Loss: 0.08956262469291687\n",
      "Epoch 3557, Loss: 0.13636099733412266, Final Batch Loss: 0.0820198804140091\n",
      "Epoch 3558, Loss: 0.13662028312683105, Final Batch Loss: 0.054672639816999435\n",
      "Epoch 3559, Loss: 0.08351861499249935, Final Batch Loss: 0.014557676389813423\n",
      "Epoch 3560, Loss: 0.12448031641542912, Final Batch Loss: 0.043117959052324295\n",
      "Epoch 3561, Loss: 0.13623046875, Final Batch Loss: 0.038624078035354614\n",
      "Epoch 3562, Loss: 0.1581168919801712, Final Batch Loss: 0.05224412679672241\n",
      "Epoch 3563, Loss: 0.11290545202791691, Final Batch Loss: 0.051967110484838486\n",
      "Epoch 3564, Loss: 0.10575513262301683, Final Batch Loss: 0.013721664436161518\n",
      "Epoch 3565, Loss: 0.21584774553775787, Final Batch Loss: 0.08449289202690125\n",
      "Epoch 3566, Loss: 0.17892674542963505, Final Batch Loss: 0.1155928447842598\n",
      "Epoch 3567, Loss: 0.08459958340972662, Final Batch Loss: 0.004928472451865673\n",
      "Epoch 3568, Loss: 0.11484212335199118, Final Batch Loss: 0.01377141010016203\n",
      "Epoch 3569, Loss: 0.1971081756055355, Final Batch Loss: 0.02763107419013977\n",
      "Epoch 3570, Loss: 0.10956835374236107, Final Batch Loss: 0.043830834329128265\n",
      "Epoch 3571, Loss: 0.07393480557948351, Final Batch Loss: 0.017350448295474052\n",
      "Epoch 3572, Loss: 0.2049555778503418, Final Batch Loss: 0.08865121752023697\n",
      "Epoch 3573, Loss: 0.15256225690245628, Final Batch Loss: 0.06007660925388336\n",
      "Epoch 3574, Loss: 0.18674665316939354, Final Batch Loss: 0.11277860403060913\n",
      "Epoch 3575, Loss: 0.118747279047966, Final Batch Loss: 0.026104211807250977\n",
      "Epoch 3576, Loss: 0.2955036461353302, Final Batch Loss: 0.21193744242191315\n",
      "Epoch 3577, Loss: 0.1252508731558919, Final Batch Loss: 0.014393971301615238\n",
      "Epoch 3578, Loss: 0.201073182746768, Final Batch Loss: 0.02112635225057602\n",
      "Epoch 3579, Loss: 0.18368754535913467, Final Batch Loss: 0.04333502799272537\n",
      "Epoch 3580, Loss: 0.12691163085401058, Final Batch Loss: 0.026699963957071304\n",
      "Epoch 3581, Loss: 0.20263714715838432, Final Batch Loss: 0.02383594587445259\n",
      "Epoch 3582, Loss: 0.1618594378232956, Final Batch Loss: 0.045402687042951584\n",
      "Epoch 3583, Loss: 0.1448867917060852, Final Batch Loss: 0.04938807711005211\n",
      "Epoch 3584, Loss: 0.13636468350887299, Final Batch Loss: 0.055288802832365036\n",
      "Epoch 3585, Loss: 0.16058076545596123, Final Batch Loss: 0.08463114500045776\n",
      "Epoch 3586, Loss: 0.08654430136084557, Final Batch Loss: 0.014369156211614609\n",
      "Epoch 3587, Loss: 0.18305687978863716, Final Batch Loss: 0.053211286664009094\n",
      "Epoch 3588, Loss: 0.11474809423089027, Final Batch Loss: 0.05972284451127052\n",
      "Epoch 3589, Loss: 0.16791416890919209, Final Batch Loss: 0.09050096571445465\n",
      "Epoch 3590, Loss: 0.11019124649465084, Final Batch Loss: 0.021997155621647835\n",
      "Epoch 3591, Loss: 0.15943123027682304, Final Batch Loss: 0.07239111512899399\n",
      "Epoch 3592, Loss: 0.13586232997477055, Final Batch Loss: 0.028159676119685173\n",
      "Epoch 3593, Loss: 0.14789290726184845, Final Batch Loss: 0.04598274454474449\n",
      "Epoch 3594, Loss: 0.17164688184857368, Final Batch Loss: 0.05581380054354668\n",
      "Epoch 3595, Loss: 0.1211616788059473, Final Batch Loss: 0.027844557538628578\n",
      "Epoch 3596, Loss: 0.11830868199467659, Final Batch Loss: 0.02269730158150196\n",
      "Epoch 3597, Loss: 0.20771797373890877, Final Batch Loss: 0.11015525460243225\n",
      "Epoch 3598, Loss: 0.15827887691557407, Final Batch Loss: 0.010499542579054832\n",
      "Epoch 3599, Loss: 0.17225267365574837, Final Batch Loss: 0.05572838336229324\n",
      "Epoch 3600, Loss: 0.08419516030699015, Final Batch Loss: 0.009572695009410381\n",
      "Epoch 3601, Loss: 0.11358969286084175, Final Batch Loss: 0.035345498472452164\n",
      "Epoch 3602, Loss: 0.13298356719315052, Final Batch Loss: 0.026962878182530403\n",
      "Epoch 3603, Loss: 0.10962737165391445, Final Batch Loss: 0.0198721494525671\n",
      "Epoch 3604, Loss: 0.09616819210350513, Final Batch Loss: 0.03424525633454323\n",
      "Epoch 3605, Loss: 0.1614494789391756, Final Batch Loss: 0.07650782912969589\n",
      "Epoch 3606, Loss: 0.3081918340176344, Final Batch Loss: 0.14893308281898499\n",
      "Epoch 3607, Loss: 0.15414706990122795, Final Batch Loss: 0.042688339948654175\n",
      "Epoch 3608, Loss: 0.18926758132874966, Final Batch Loss: 0.10675590485334396\n",
      "Epoch 3609, Loss: 0.10038992017507553, Final Batch Loss: 0.019210372120141983\n",
      "Epoch 3610, Loss: 0.2471604235470295, Final Batch Loss: 0.1018352285027504\n",
      "Epoch 3611, Loss: 0.11081023328006268, Final Batch Loss: 0.054108209908008575\n",
      "Epoch 3612, Loss: 0.11080807819962502, Final Batch Loss: 0.0481090173125267\n",
      "Epoch 3613, Loss: 0.15098528936505318, Final Batch Loss: 0.07490761578083038\n",
      "Epoch 3614, Loss: 0.19185479916632175, Final Batch Loss: 0.14305555820465088\n",
      "Epoch 3615, Loss: 0.1016443520784378, Final Batch Loss: 0.04467693343758583\n",
      "Epoch 3616, Loss: 0.13018297031521797, Final Batch Loss: 0.02277349680662155\n",
      "Epoch 3617, Loss: 0.20292611420154572, Final Batch Loss: 0.08015437424182892\n",
      "Epoch 3618, Loss: 0.12364963628351688, Final Batch Loss: 0.02769731543958187\n",
      "Epoch 3619, Loss: 0.10447065345942974, Final Batch Loss: 0.02589525654911995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3620, Loss: 0.15793277136981487, Final Batch Loss: 0.06196828559041023\n",
      "Epoch 3621, Loss: 0.16811706498265266, Final Batch Loss: 0.05822473391890526\n",
      "Epoch 3622, Loss: 0.1253836341202259, Final Batch Loss: 0.03683844953775406\n",
      "Epoch 3623, Loss: 0.12777134776115417, Final Batch Loss: 0.031864557415246964\n",
      "Epoch 3624, Loss: 0.10931254643946886, Final Batch Loss: 0.008667814545333385\n",
      "Epoch 3625, Loss: 0.10982987470924854, Final Batch Loss: 0.06219480559229851\n",
      "Epoch 3626, Loss: 0.15877487137913704, Final Batch Loss: 0.048926714807748795\n",
      "Epoch 3627, Loss: 0.10154297202825546, Final Batch Loss: 0.02776939794421196\n",
      "Epoch 3628, Loss: 0.1181036364287138, Final Batch Loss: 0.01948746107518673\n",
      "Epoch 3629, Loss: 0.11165955662727356, Final Batch Loss: 0.05095890909433365\n",
      "Epoch 3630, Loss: 0.07291263621300459, Final Batch Loss: 0.01220486406236887\n",
      "Epoch 3631, Loss: 0.06526410765945911, Final Batch Loss: 0.019278770312666893\n",
      "Epoch 3632, Loss: 0.10024209320545197, Final Batch Loss: 0.01974339410662651\n",
      "Epoch 3633, Loss: 0.20281017199158669, Final Batch Loss: 0.07108017057180405\n",
      "Epoch 3634, Loss: 0.13678117538802326, Final Batch Loss: 0.0012358918320387602\n",
      "Epoch 3635, Loss: 0.1086918544024229, Final Batch Loss: 0.018081242218613625\n",
      "Epoch 3636, Loss: 0.06541576888412237, Final Batch Loss: 0.012351728044450283\n",
      "Epoch 3637, Loss: 0.10988430865108967, Final Batch Loss: 0.030011868104338646\n",
      "Epoch 3638, Loss: 0.15851705893874168, Final Batch Loss: 0.06894291937351227\n",
      "Epoch 3639, Loss: 0.20553833805024624, Final Batch Loss: 0.11773382127285004\n",
      "Epoch 3640, Loss: 0.3982314020395279, Final Batch Loss: 0.23953241109848022\n",
      "Epoch 3641, Loss: 0.1359143778681755, Final Batch Loss: 0.0398191437125206\n",
      "Epoch 3642, Loss: 0.14504304435104132, Final Batch Loss: 0.012634954415261745\n",
      "Epoch 3643, Loss: 0.12590477149933577, Final Batch Loss: 0.010321295820176601\n",
      "Epoch 3644, Loss: 0.10309377498924732, Final Batch Loss: 0.016208941116929054\n",
      "Epoch 3645, Loss: 0.08336116746068001, Final Batch Loss: 0.020106922835111618\n",
      "Epoch 3646, Loss: 0.17644888907670975, Final Batch Loss: 0.1169644370675087\n",
      "Epoch 3647, Loss: 0.09076209366321564, Final Batch Loss: 0.029494691640138626\n",
      "Epoch 3648, Loss: 0.12258645298425108, Final Batch Loss: 0.0015255414182320237\n",
      "Epoch 3649, Loss: 0.19368481263518333, Final Batch Loss: 0.07404360920190811\n",
      "Epoch 3650, Loss: 0.10983937606215477, Final Batch Loss: 0.027358010411262512\n",
      "Epoch 3651, Loss: 0.16434404253959656, Final Batch Loss: 0.03670434653759003\n",
      "Epoch 3652, Loss: 0.1099572042003274, Final Batch Loss: 0.012093932367861271\n",
      "Epoch 3653, Loss: 0.13572580181062222, Final Batch Loss: 0.06344768404960632\n",
      "Epoch 3654, Loss: 0.10017629712820053, Final Batch Loss: 0.014703162014484406\n",
      "Epoch 3655, Loss: 0.09852061234414577, Final Batch Loss: 0.01900944858789444\n",
      "Epoch 3656, Loss: 0.28769050911068916, Final Batch Loss: 0.20257137715816498\n",
      "Epoch 3657, Loss: 0.17250376008450985, Final Batch Loss: 0.11121737957000732\n",
      "Epoch 3658, Loss: 0.17700792476534843, Final Batch Loss: 0.06744970381259918\n",
      "Epoch 3659, Loss: 0.2695501521229744, Final Batch Loss: 0.17104418575763702\n",
      "Epoch 3660, Loss: 0.14768010191619396, Final Batch Loss: 0.07863488793373108\n",
      "Epoch 3661, Loss: 0.15704813227057457, Final Batch Loss: 0.060571011155843735\n",
      "Epoch 3662, Loss: 0.1582685001194477, Final Batch Loss: 0.09586276859045029\n",
      "Epoch 3663, Loss: 0.08113692759070545, Final Batch Loss: 0.0013888931134715676\n",
      "Epoch 3664, Loss: 0.17378101870417595, Final Batch Loss: 0.022838830947875977\n",
      "Epoch 3665, Loss: 0.13850888167507946, Final Batch Loss: 0.0032189779449254274\n",
      "Epoch 3666, Loss: 0.10482549853622913, Final Batch Loss: 0.046780239790678024\n",
      "Epoch 3667, Loss: 0.07414111262187362, Final Batch Loss: 0.004827649798244238\n",
      "Epoch 3668, Loss: 0.1751568801701069, Final Batch Loss: 0.030222248286008835\n",
      "Epoch 3669, Loss: 0.1831923071295023, Final Batch Loss: 0.1151362881064415\n",
      "Epoch 3670, Loss: 0.11202612519264221, Final Batch Loss: 0.051713671535253525\n",
      "Epoch 3671, Loss: 0.1797659955918789, Final Batch Loss: 0.08169763535261154\n",
      "Epoch 3672, Loss: 0.1321188099682331, Final Batch Loss: 0.05701642856001854\n",
      "Epoch 3673, Loss: 0.06861116667278111, Final Batch Loss: 0.0032141816336661577\n",
      "Epoch 3674, Loss: 0.1330603864043951, Final Batch Loss: 0.061846256256103516\n",
      "Epoch 3675, Loss: 0.0880956556648016, Final Batch Loss: 0.026396656408905983\n",
      "Epoch 3676, Loss: 0.07730072131380439, Final Batch Loss: 0.006350965704768896\n",
      "Epoch 3677, Loss: 0.17357881367206573, Final Batch Loss: 0.09470979869365692\n",
      "Epoch 3678, Loss: 0.16560467332601547, Final Batch Loss: 0.04862930625677109\n",
      "Epoch 3679, Loss: 0.1807524971663952, Final Batch Loss: 0.0916827917098999\n",
      "Epoch 3680, Loss: 0.14529409259557724, Final Batch Loss: 0.03775540366768837\n",
      "Epoch 3681, Loss: 0.1248114351183176, Final Batch Loss: 0.0641980692744255\n",
      "Epoch 3682, Loss: 0.09063287824392319, Final Batch Loss: 0.018029727041721344\n",
      "Epoch 3683, Loss: 0.10884201526641846, Final Batch Loss: 0.028999334201216698\n",
      "Epoch 3684, Loss: 0.15680530481040478, Final Batch Loss: 0.07439666986465454\n",
      "Epoch 3685, Loss: 0.10034348419867456, Final Batch Loss: 0.002344988053664565\n",
      "Epoch 3686, Loss: 0.13119102455675602, Final Batch Loss: 0.048391640186309814\n",
      "Epoch 3687, Loss: 0.2898169085383415, Final Batch Loss: 0.11386585235595703\n",
      "Epoch 3688, Loss: 0.10043788701295853, Final Batch Loss: 0.03391483426094055\n",
      "Epoch 3689, Loss: 0.23378175869584084, Final Batch Loss: 0.10483398288488388\n",
      "Epoch 3690, Loss: 0.1172163225710392, Final Batch Loss: 0.0334736667573452\n",
      "Epoch 3691, Loss: 0.1716073751449585, Final Batch Loss: 0.07205993682146072\n",
      "Epoch 3692, Loss: 0.10881411843001842, Final Batch Loss: 0.019012371078133583\n",
      "Epoch 3693, Loss: 0.1167725594714284, Final Batch Loss: 0.01315962802618742\n",
      "Epoch 3694, Loss: 0.14636192843317986, Final Batch Loss: 0.018025510013103485\n",
      "Epoch 3695, Loss: 0.08117795456200838, Final Batch Loss: 0.006287979893386364\n",
      "Epoch 3696, Loss: 0.09247360657900572, Final Batch Loss: 0.00276076328009367\n",
      "Epoch 3697, Loss: 0.19193601422011852, Final Batch Loss: 0.12107846885919571\n",
      "Epoch 3698, Loss: 0.06510984268970788, Final Batch Loss: 0.003263249294832349\n",
      "Epoch 3699, Loss: 0.10364832170307636, Final Batch Loss: 0.021816348657011986\n",
      "Epoch 3700, Loss: 0.14935539290308952, Final Batch Loss: 0.041733913123607635\n",
      "Epoch 3701, Loss: 0.08789350092411041, Final Batch Loss: 0.016886405646800995\n",
      "Epoch 3702, Loss: 0.1253940686583519, Final Batch Loss: 0.06565210968255997\n",
      "Epoch 3703, Loss: 0.14007099717855453, Final Batch Loss: 0.0636000856757164\n",
      "Epoch 3704, Loss: 0.09407064784318209, Final Batch Loss: 0.005341348238289356\n",
      "Epoch 3705, Loss: 0.13278540968894958, Final Batch Loss: 0.03220856562256813\n",
      "Epoch 3706, Loss: 0.09564163908362389, Final Batch Loss: 0.031106213107705116\n",
      "Epoch 3707, Loss: 0.2362946942448616, Final Batch Loss: 0.12555505335330963\n",
      "Epoch 3708, Loss: 0.07177530974149704, Final Batch Loss: 0.019744737073779106\n",
      "Epoch 3709, Loss: 0.22040605172514915, Final Batch Loss: 0.14642083644866943\n",
      "Epoch 3710, Loss: 0.1542612388730049, Final Batch Loss: 0.039672233164310455\n",
      "Epoch 3711, Loss: 0.22419433668255806, Final Batch Loss: 0.1430196613073349\n",
      "Epoch 3712, Loss: 0.19992094114422798, Final Batch Loss: 0.10391645133495331\n",
      "Epoch 3713, Loss: 0.2525556981563568, Final Batch Loss: 0.06592343747615814\n",
      "Epoch 3714, Loss: 0.12219649460166693, Final Batch Loss: 0.048500221222639084\n",
      "Epoch 3715, Loss: 0.3365064822137356, Final Batch Loss: 0.2192959189414978\n",
      "Epoch 3716, Loss: 0.13414486031979322, Final Batch Loss: 0.011174323968589306\n",
      "Epoch 3717, Loss: 0.2490425817668438, Final Batch Loss: 0.12269426137208939\n",
      "Epoch 3718, Loss: 0.10850687325000763, Final Batch Loss: 0.005469534546136856\n",
      "Epoch 3719, Loss: 0.2051074206829071, Final Batch Loss: 0.04656217247247696\n",
      "Epoch 3720, Loss: 0.12200543656945229, Final Batch Loss: 0.03203220292925835\n",
      "Epoch 3721, Loss: 0.26399942487478256, Final Batch Loss: 0.10940459370613098\n",
      "Epoch 3722, Loss: 0.12614616379141808, Final Batch Loss: 0.014769628643989563\n",
      "Epoch 3723, Loss: 0.13821377232670784, Final Batch Loss: 0.0333479680120945\n",
      "Epoch 3724, Loss: 0.16668525710701942, Final Batch Loss: 0.051089752465486526\n",
      "Epoch 3725, Loss: 0.1433108188211918, Final Batch Loss: 0.037431955337524414\n",
      "Epoch 3726, Loss: 0.1079195886850357, Final Batch Loss: 0.019548051059246063\n",
      "Epoch 3727, Loss: 0.22046605125069618, Final Batch Loss: 0.08059600740671158\n",
      "Epoch 3728, Loss: 0.10289409290999174, Final Batch Loss: 0.011816105805337429\n",
      "Epoch 3729, Loss: 0.18272734060883522, Final Batch Loss: 0.10890047252178192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3730, Loss: 0.1914617270231247, Final Batch Loss: 0.09007370471954346\n",
      "Epoch 3731, Loss: 0.19754127971827984, Final Batch Loss: 0.04257217049598694\n",
      "Epoch 3732, Loss: 0.12463479954749346, Final Batch Loss: 0.008676507510244846\n",
      "Epoch 3733, Loss: 0.11053571291267872, Final Batch Loss: 0.010520333424210548\n",
      "Epoch 3734, Loss: 0.11515051126480103, Final Batch Loss: 0.015733662992715836\n",
      "Epoch 3735, Loss: 0.11849198490381241, Final Batch Loss: 0.012118332087993622\n",
      "Epoch 3736, Loss: 0.1045694719068706, Final Batch Loss: 0.006048526149243116\n",
      "Epoch 3737, Loss: 0.14078201353549957, Final Batch Loss: 0.06860189884901047\n",
      "Epoch 3738, Loss: 0.1149339322000742, Final Batch Loss: 0.009448213502764702\n",
      "Epoch 3739, Loss: 0.07653979258611798, Final Batch Loss: 0.005544275511056185\n",
      "Epoch 3740, Loss: 0.1333802193403244, Final Batch Loss: 0.0508260540664196\n",
      "Epoch 3741, Loss: 0.12924708798527718, Final Batch Loss: 0.0473981611430645\n",
      "Epoch 3742, Loss: 0.08815742284059525, Final Batch Loss: 0.01712822914123535\n",
      "Epoch 3743, Loss: 0.1146894246339798, Final Batch Loss: 0.029875699430704117\n",
      "Epoch 3744, Loss: 0.10226778499782085, Final Batch Loss: 0.022412436082959175\n",
      "Epoch 3745, Loss: 0.129318967461586, Final Batch Loss: 0.05230206996202469\n",
      "Epoch 3746, Loss: 0.18683718889951706, Final Batch Loss: 0.04849426448345184\n",
      "Epoch 3747, Loss: 0.0903636310249567, Final Batch Loss: 0.03006463125348091\n",
      "Epoch 3748, Loss: 0.07653892040252686, Final Batch Loss: 0.009988414123654366\n",
      "Epoch 3749, Loss: 0.09572201035916805, Final Batch Loss: 0.008953051641583443\n",
      "Epoch 3750, Loss: 0.12842545472085476, Final Batch Loss: 0.06334716081619263\n",
      "Epoch 3751, Loss: 0.09073825925588608, Final Batch Loss: 0.020534036681056023\n",
      "Epoch 3752, Loss: 0.13113939762115479, Final Batch Loss: 0.030861590057611465\n",
      "Epoch 3753, Loss: 0.25348521023988724, Final Batch Loss: 0.1747593730688095\n",
      "Epoch 3754, Loss: 0.10427739284932613, Final Batch Loss: 0.04303251951932907\n",
      "Epoch 3755, Loss: 0.1276915892958641, Final Batch Loss: 0.005766790360212326\n",
      "Epoch 3756, Loss: 0.15447436925023794, Final Batch Loss: 0.01086686085909605\n",
      "Epoch 3757, Loss: 0.13733875378966331, Final Batch Loss: 0.04448061063885689\n",
      "Epoch 3758, Loss: 0.19293617829680443, Final Batch Loss: 0.04964243248105049\n",
      "Epoch 3759, Loss: 0.14633648842573166, Final Batch Loss: 0.06431891024112701\n",
      "Epoch 3760, Loss: 0.08535089157521725, Final Batch Loss: 0.033661454916000366\n",
      "Epoch 3761, Loss: 0.1250508576631546, Final Batch Loss: 0.04465654864907265\n",
      "Epoch 3762, Loss: 0.09429928846657276, Final Batch Loss: 0.006414571776986122\n",
      "Epoch 3763, Loss: 0.09317885339260101, Final Batch Loss: 0.025178294628858566\n",
      "Epoch 3764, Loss: 0.19370116293430328, Final Batch Loss: 0.015829455107450485\n",
      "Epoch 3765, Loss: 0.09585358761250973, Final Batch Loss: 0.015438223257660866\n",
      "Epoch 3766, Loss: 0.09947091154754162, Final Batch Loss: 0.03276616334915161\n",
      "Epoch 3767, Loss: 0.09806694462895393, Final Batch Loss: 0.03213740512728691\n",
      "Epoch 3768, Loss: 0.10268680192530155, Final Batch Loss: 0.020226845517754555\n",
      "Epoch 3769, Loss: 0.11611555144190788, Final Batch Loss: 0.029288863763213158\n",
      "Epoch 3770, Loss: 0.0953943282365799, Final Batch Loss: 0.04447150602936745\n",
      "Epoch 3771, Loss: 0.2098567970097065, Final Batch Loss: 0.09887845814228058\n",
      "Epoch 3772, Loss: 0.14610714092850685, Final Batch Loss: 0.015250597149133682\n",
      "Epoch 3773, Loss: 0.12426051683723927, Final Batch Loss: 0.02112877182662487\n",
      "Epoch 3774, Loss: 0.09256007755175233, Final Batch Loss: 0.005450217518955469\n",
      "Epoch 3775, Loss: 0.08665253547951579, Final Batch Loss: 0.0038827168755233288\n",
      "Epoch 3776, Loss: 0.29477824084460735, Final Batch Loss: 0.19305913150310516\n",
      "Epoch 3777, Loss: 0.09351269155740738, Final Batch Loss: 0.03896167129278183\n",
      "Epoch 3778, Loss: 0.12158572487533092, Final Batch Loss: 0.0687638372182846\n",
      "Epoch 3779, Loss: 0.09796678880229592, Final Batch Loss: 0.003984688315540552\n",
      "Epoch 3780, Loss: 0.1316591575741768, Final Batch Loss: 0.03510938584804535\n",
      "Epoch 3781, Loss: 0.1257706880569458, Final Batch Loss: 0.04396525397896767\n",
      "Epoch 3782, Loss: 0.2521534599363804, Final Batch Loss: 0.12828050553798676\n",
      "Epoch 3783, Loss: 0.16948093473911285, Final Batch Loss: 0.0187126062810421\n",
      "Epoch 3784, Loss: 0.18557114526629448, Final Batch Loss: 0.0431038960814476\n",
      "Epoch 3785, Loss: 0.22298311069607735, Final Batch Loss: 0.07238412648439407\n",
      "Epoch 3786, Loss: 0.11225447803735733, Final Batch Loss: 0.02787507325410843\n",
      "Epoch 3787, Loss: 0.11950119584798813, Final Batch Loss: 0.046993937343358994\n",
      "Epoch 3788, Loss: 0.12255398742854595, Final Batch Loss: 0.008505979552865028\n",
      "Epoch 3789, Loss: 0.13882958516478539, Final Batch Loss: 0.022922296077013016\n",
      "Epoch 3790, Loss: 0.11573934555053711, Final Batch Loss: 0.040706146508455276\n",
      "Epoch 3791, Loss: 0.12910374253988266, Final Batch Loss: 0.050412435084581375\n",
      "Epoch 3792, Loss: 0.08349330956116319, Final Batch Loss: 0.005785331595689058\n",
      "Epoch 3793, Loss: 0.1152793038636446, Final Batch Loss: 0.028717396780848503\n",
      "Epoch 3794, Loss: 0.3871202915906906, Final Batch Loss: 0.26706698536872864\n",
      "Epoch 3795, Loss: 0.12609298713505268, Final Batch Loss: 0.028133684769272804\n",
      "Epoch 3796, Loss: 0.13059568591415882, Final Batch Loss: 0.021614713594317436\n",
      "Epoch 3797, Loss: 0.07940973527729511, Final Batch Loss: 0.022992122918367386\n",
      "Epoch 3798, Loss: 0.15533001720905304, Final Batch Loss: 0.03328419849276543\n",
      "Epoch 3799, Loss: 0.14481435157358646, Final Batch Loss: 0.025717800483107567\n",
      "Epoch 3800, Loss: 0.0896360045298934, Final Batch Loss: 0.017454443499445915\n",
      "Epoch 3801, Loss: 0.08486093953251839, Final Batch Loss: 0.034167394042015076\n",
      "Epoch 3802, Loss: 0.08899323455989361, Final Batch Loss: 0.032826099544763565\n",
      "Epoch 3803, Loss: 0.11861684173345566, Final Batch Loss: 0.014588382095098495\n",
      "Epoch 3804, Loss: 0.09405877254903316, Final Batch Loss: 0.015461215749382973\n",
      "Epoch 3805, Loss: 0.1672450602054596, Final Batch Loss: 0.0794236809015274\n",
      "Epoch 3806, Loss: 0.15405175741761923, Final Batch Loss: 0.008066839538514614\n",
      "Epoch 3807, Loss: 0.14568495377898216, Final Batch Loss: 0.04833034798502922\n",
      "Epoch 3808, Loss: 0.1510102078318596, Final Batch Loss: 0.07659824937582016\n",
      "Epoch 3809, Loss: 0.1051874402910471, Final Batch Loss: 0.05273255705833435\n",
      "Epoch 3810, Loss: 0.08560128882527351, Final Batch Loss: 0.026944803074002266\n",
      "Epoch 3811, Loss: 0.19449199456721544, Final Batch Loss: 0.11578544974327087\n",
      "Epoch 3812, Loss: 0.0913922656327486, Final Batch Loss: 0.01569925807416439\n",
      "Epoch 3813, Loss: 0.467522818595171, Final Batch Loss: 0.38163700699806213\n",
      "Epoch 3814, Loss: 0.11314254999160767, Final Batch Loss: 0.04813401401042938\n",
      "Epoch 3815, Loss: 0.11589302495121956, Final Batch Loss: 0.058830130845308304\n",
      "Epoch 3816, Loss: 0.07600461598485708, Final Batch Loss: 0.011847850866615772\n",
      "Epoch 3817, Loss: 0.13517597317695618, Final Batch Loss: 0.03369715437293053\n",
      "Epoch 3818, Loss: 0.1743660271167755, Final Batch Loss: 0.04644780606031418\n",
      "Epoch 3819, Loss: 0.20905323699116707, Final Batch Loss: 0.045927416533231735\n",
      "Epoch 3820, Loss: 0.10984849417582154, Final Batch Loss: 0.006906351540237665\n",
      "Epoch 3821, Loss: 0.19356350973248482, Final Batch Loss: 0.0749262347817421\n",
      "Epoch 3822, Loss: 0.13442586734890938, Final Batch Loss: 0.06579194217920303\n",
      "Epoch 3823, Loss: 0.09746684692800045, Final Batch Loss: 0.012368055060505867\n",
      "Epoch 3824, Loss: 0.1411597914993763, Final Batch Loss: 0.019537027925252914\n",
      "Epoch 3825, Loss: 0.15078708343207836, Final Batch Loss: 0.06826021522283554\n",
      "Epoch 3826, Loss: 0.13937734439969063, Final Batch Loss: 0.03872798755764961\n",
      "Epoch 3827, Loss: 0.10072208568453789, Final Batch Loss: 0.00833044946193695\n",
      "Epoch 3828, Loss: 0.20588262006640434, Final Batch Loss: 0.12920048832893372\n",
      "Epoch 3829, Loss: 0.2471054382622242, Final Batch Loss: 0.08230176568031311\n",
      "Epoch 3830, Loss: 0.1368245929479599, Final Batch Loss: 0.01225830614566803\n",
      "Epoch 3831, Loss: 0.20710931345820427, Final Batch Loss: 0.09616470336914062\n",
      "Epoch 3832, Loss: 0.32623255997896194, Final Batch Loss: 0.201134592294693\n",
      "Epoch 3833, Loss: 0.11967157945036888, Final Batch Loss: 0.016654860228300095\n",
      "Epoch 3834, Loss: 0.11715973913669586, Final Batch Loss: 0.013461954891681671\n",
      "Epoch 3835, Loss: 0.15430478751659393, Final Batch Loss: 0.0871252715587616\n",
      "Epoch 3836, Loss: 0.17030769772827625, Final Batch Loss: 0.10612282156944275\n",
      "Epoch 3837, Loss: 0.07992658810690045, Final Batch Loss: 0.007388208527117968\n",
      "Epoch 3838, Loss: 0.09648019447922707, Final Batch Loss: 0.014933912083506584\n",
      "Epoch 3839, Loss: 0.10115607804618776, Final Batch Loss: 0.0025211346801370382\n",
      "Epoch 3840, Loss: 0.1478923331014812, Final Batch Loss: 0.002067017834633589\n",
      "Epoch 3841, Loss: 0.08396817184984684, Final Batch Loss: 0.030461303889751434\n",
      "Epoch 3842, Loss: 0.12527734600007534, Final Batch Loss: 0.012047059834003448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3843, Loss: 0.12606072705239058, Final Batch Loss: 0.014499532990157604\n",
      "Epoch 3844, Loss: 0.09320921637117863, Final Batch Loss: 0.02179759554564953\n",
      "Epoch 3845, Loss: 0.07588676735758781, Final Batch Loss: 0.028508352115750313\n",
      "Epoch 3846, Loss: 0.0691111427731812, Final Batch Loss: 0.004517706576734781\n",
      "Epoch 3847, Loss: 0.0790142584592104, Final Batch Loss: 0.02239263243973255\n",
      "Epoch 3848, Loss: 0.09975808300077915, Final Batch Loss: 0.020475653931498528\n",
      "Epoch 3849, Loss: 0.1314647663384676, Final Batch Loss: 0.0544121079146862\n",
      "Epoch 3850, Loss: 0.11246561072766781, Final Batch Loss: 0.02881358377635479\n",
      "Epoch 3851, Loss: 0.09503921214491129, Final Batch Loss: 0.00924674142152071\n",
      "Epoch 3852, Loss: 0.15421726554632187, Final Batch Loss: 0.06338579952716827\n",
      "Epoch 3853, Loss: 0.1510734222829342, Final Batch Loss: 0.025323718786239624\n",
      "Epoch 3854, Loss: 0.12160382326692343, Final Batch Loss: 0.004940963350236416\n",
      "Epoch 3855, Loss: 0.19674527272582054, Final Batch Loss: 0.1134662926197052\n",
      "Epoch 3856, Loss: 0.14783011004328728, Final Batch Loss: 0.06491487473249435\n",
      "Epoch 3857, Loss: 0.1332375556230545, Final Batch Loss: 0.05734812095761299\n",
      "Epoch 3858, Loss: 0.12436317838728428, Final Batch Loss: 0.055801406502723694\n",
      "Epoch 3859, Loss: 0.18446683418005705, Final Batch Loss: 0.025583837181329727\n",
      "Epoch 3860, Loss: 0.09415881521999836, Final Batch Loss: 0.02056719921529293\n",
      "Epoch 3861, Loss: 0.10448373481631279, Final Batch Loss: 0.01734760031104088\n",
      "Epoch 3862, Loss: 0.1989866252988577, Final Batch Loss: 0.1456281840801239\n",
      "Epoch 3863, Loss: 0.06805731076747179, Final Batch Loss: 0.013619760982692242\n",
      "Epoch 3864, Loss: 0.08891214150935411, Final Batch Loss: 0.015119249932467937\n",
      "Epoch 3865, Loss: 0.3589516095817089, Final Batch Loss: 0.26684707403182983\n",
      "Epoch 3866, Loss: 0.14214514940977097, Final Batch Loss: 0.08928261697292328\n",
      "Epoch 3867, Loss: 0.14140445413067937, Final Batch Loss: 0.007584814447909594\n",
      "Epoch 3868, Loss: 0.14344138652086258, Final Batch Loss: 0.03449594974517822\n",
      "Epoch 3869, Loss: 0.10114117432385683, Final Batch Loss: 0.008834508247673512\n",
      "Epoch 3870, Loss: 0.10918464884161949, Final Batch Loss: 0.04140513017773628\n",
      "Epoch 3871, Loss: 0.17412143014371395, Final Batch Loss: 0.03367653489112854\n",
      "Epoch 3872, Loss: 0.14873217418789864, Final Batch Loss: 0.03772762045264244\n",
      "Epoch 3873, Loss: 0.1293986402451992, Final Batch Loss: 0.05914386734366417\n",
      "Epoch 3874, Loss: 0.20441788248717785, Final Batch Loss: 0.142628014087677\n",
      "Epoch 3875, Loss: 0.1311752237379551, Final Batch Loss: 0.04373330622911453\n",
      "Epoch 3876, Loss: 0.14959925040602684, Final Batch Loss: 0.03628918156027794\n",
      "Epoch 3877, Loss: 0.11449193302541971, Final Batch Loss: 0.014205546118319035\n",
      "Epoch 3878, Loss: 0.07647529803216457, Final Batch Loss: 0.03865252062678337\n",
      "Epoch 3879, Loss: 0.13596136681735516, Final Batch Loss: 0.0506478026509285\n",
      "Epoch 3880, Loss: 0.11011706106364727, Final Batch Loss: 0.0385873019695282\n",
      "Epoch 3881, Loss: 0.11940671317279339, Final Batch Loss: 0.01678597927093506\n",
      "Epoch 3882, Loss: 0.18616928346455097, Final Batch Loss: 0.12160616368055344\n",
      "Epoch 3883, Loss: 0.0634735501371324, Final Batch Loss: 0.006516748573631048\n",
      "Epoch 3884, Loss: 0.08964474499225616, Final Batch Loss: 0.01653629168868065\n",
      "Epoch 3885, Loss: 0.05568039417266846, Final Batch Loss: 0.016615059226751328\n",
      "Epoch 3886, Loss: 0.08500708267092705, Final Batch Loss: 0.010353684425354004\n",
      "Epoch 3887, Loss: 0.074733623303473, Final Batch Loss: 0.009795707650482655\n",
      "Epoch 3888, Loss: 0.1253850143402815, Final Batch Loss: 0.011662030592560768\n",
      "Epoch 3889, Loss: 0.14468207955360413, Final Batch Loss: 0.07974956184625626\n",
      "Epoch 3890, Loss: 0.08855588687583804, Final Batch Loss: 0.00579112721607089\n",
      "Epoch 3891, Loss: 0.12957780435681343, Final Batch Loss: 0.06168840453028679\n",
      "Epoch 3892, Loss: 0.10757648199796677, Final Batch Loss: 0.03394012898206711\n",
      "Epoch 3893, Loss: 0.10903220064938068, Final Batch Loss: 0.0219491608440876\n",
      "Epoch 3894, Loss: 0.1265327068977058, Final Batch Loss: 0.005852974485605955\n",
      "Epoch 3895, Loss: 0.07749411556869745, Final Batch Loss: 0.010981258936226368\n",
      "Epoch 3896, Loss: 0.12483979016542435, Final Batch Loss: 0.01360834576189518\n",
      "Epoch 3897, Loss: 0.14286857657134533, Final Batch Loss: 0.0743735134601593\n",
      "Epoch 3898, Loss: 0.22890722379088402, Final Batch Loss: 0.10924959182739258\n",
      "Epoch 3899, Loss: 0.11326522496528924, Final Batch Loss: 0.0022745889145880938\n",
      "Epoch 3900, Loss: 0.20084844157099724, Final Batch Loss: 0.08002179116010666\n",
      "Epoch 3901, Loss: 0.17979250848293304, Final Batch Loss: 0.07558134198188782\n",
      "Epoch 3902, Loss: 0.1914648488163948, Final Batch Loss: 0.09904105961322784\n",
      "Epoch 3903, Loss: 0.0828271983191371, Final Batch Loss: 0.004309500567615032\n",
      "Epoch 3904, Loss: 0.08565706945955753, Final Batch Loss: 0.015566162765026093\n",
      "Epoch 3905, Loss: 0.06599806994199753, Final Batch Loss: 0.00830567255616188\n",
      "Epoch 3906, Loss: 0.1582662994042039, Final Batch Loss: 0.10739519447088242\n",
      "Epoch 3907, Loss: 0.10418837517499924, Final Batch Loss: 0.027295716106891632\n",
      "Epoch 3908, Loss: 0.08811942022293806, Final Batch Loss: 0.007594254799187183\n",
      "Epoch 3909, Loss: 0.07160921394824982, Final Batch Loss: 0.007269533351063728\n",
      "Epoch 3910, Loss: 0.08779325056821108, Final Batch Loss: 0.012486395426094532\n",
      "Epoch 3911, Loss: 0.12901002541184425, Final Batch Loss: 0.05221567302942276\n",
      "Epoch 3912, Loss: 0.09994873031973839, Final Batch Loss: 0.023023933172225952\n",
      "Epoch 3913, Loss: 0.1339464671909809, Final Batch Loss: 0.06682996451854706\n",
      "Epoch 3914, Loss: 0.10236297734081745, Final Batch Loss: 0.04509931430220604\n",
      "Epoch 3915, Loss: 0.1363946571946144, Final Batch Loss: 0.03743254765868187\n",
      "Epoch 3916, Loss: 0.135851938277483, Final Batch Loss: 0.029380429536104202\n",
      "Epoch 3917, Loss: 0.07034259662032127, Final Batch Loss: 0.016763372346758842\n",
      "Epoch 3918, Loss: 0.20034973323345184, Final Batch Loss: 0.0756896510720253\n",
      "Epoch 3919, Loss: 0.10376538336277008, Final Batch Loss: 0.029028646647930145\n",
      "Epoch 3920, Loss: 0.10525556840002537, Final Batch Loss: 0.0431487038731575\n",
      "Epoch 3921, Loss: 0.1034873928874731, Final Batch Loss: 0.04325714707374573\n",
      "Epoch 3922, Loss: 0.1360476128757, Final Batch Loss: 0.01709553599357605\n",
      "Epoch 3923, Loss: 0.14145305566489697, Final Batch Loss: 0.07892226427793503\n",
      "Epoch 3924, Loss: 0.10011052805930376, Final Batch Loss: 0.004742910154163837\n",
      "Epoch 3925, Loss: 0.32535187900066376, Final Batch Loss: 0.10741374641656876\n",
      "Epoch 3926, Loss: 0.25910790264606476, Final Batch Loss: 0.08693145960569382\n",
      "Epoch 3927, Loss: 0.13267549313604832, Final Batch Loss: 0.018537132069468498\n",
      "Epoch 3928, Loss: 0.1279066512361169, Final Batch Loss: 0.009477936662733555\n",
      "Epoch 3929, Loss: 0.09778325073421001, Final Batch Loss: 0.010755982249975204\n",
      "Epoch 3930, Loss: 0.0892163225216791, Final Batch Loss: 0.0004997932119295001\n",
      "Epoch 3931, Loss: 0.09066490642726421, Final Batch Loss: 0.018990101292729378\n",
      "Epoch 3932, Loss: 0.16117828339338303, Final Batch Loss: 0.030705198645591736\n",
      "Epoch 3933, Loss: 0.11313891224563122, Final Batch Loss: 0.05744628235697746\n",
      "Epoch 3934, Loss: 0.06052119750529528, Final Batch Loss: 0.023572435602545738\n",
      "Epoch 3935, Loss: 0.09268135018646717, Final Batch Loss: 0.01908908598124981\n",
      "Epoch 3936, Loss: 0.1430029422044754, Final Batch Loss: 0.07077991962432861\n",
      "Epoch 3937, Loss: 0.1365148527547717, Final Batch Loss: 0.01229227613657713\n",
      "Epoch 3938, Loss: 0.19620632007718086, Final Batch Loss: 0.08721061050891876\n",
      "Epoch 3939, Loss: 0.06747797504067421, Final Batch Loss: 0.01245245710015297\n",
      "Epoch 3940, Loss: 0.14425893500447273, Final Batch Loss: 0.0497514046728611\n",
      "Epoch 3941, Loss: 0.1413568388670683, Final Batch Loss: 0.013542288914322853\n",
      "Epoch 3942, Loss: 0.12330513820052147, Final Batch Loss: 0.04525715112686157\n",
      "Epoch 3943, Loss: 0.12273347936570644, Final Batch Loss: 0.02621903456747532\n",
      "Epoch 3944, Loss: 0.15954158827662468, Final Batch Loss: 0.020623192191123962\n",
      "Epoch 3945, Loss: 0.1913800910115242, Final Batch Loss: 0.10081686824560165\n",
      "Epoch 3946, Loss: 0.17678728885948658, Final Batch Loss: 0.08405076712369919\n",
      "Epoch 3947, Loss: 0.17116598784923553, Final Batch Loss: 0.04655027389526367\n",
      "Epoch 3948, Loss: 0.17035187780857086, Final Batch Loss: 0.04158632457256317\n",
      "Epoch 3949, Loss: 0.11232060566544533, Final Batch Loss: 0.03896601125597954\n",
      "Epoch 3950, Loss: 0.18813933059573174, Final Batch Loss: 0.038376741111278534\n",
      "Epoch 3951, Loss: 0.11657245829701424, Final Batch Loss: 0.01466558501124382\n",
      "Epoch 3952, Loss: 0.16509240865707397, Final Batch Loss: 0.05554167926311493\n",
      "Epoch 3953, Loss: 0.13301977887749672, Final Batch Loss: 0.03937533497810364\n",
      "Epoch 3954, Loss: 0.11789080128073692, Final Batch Loss: 0.05565115436911583\n",
      "Epoch 3955, Loss: 0.2471250332891941, Final Batch Loss: 0.1434142142534256\n",
      "Epoch 3956, Loss: 0.11729009076952934, Final Batch Loss: 0.03791562840342522\n",
      "Epoch 3957, Loss: 0.13007671758532524, Final Batch Loss: 0.009232603013515472\n",
      "Epoch 3958, Loss: 0.1885473681613803, Final Batch Loss: 0.010101000778377056\n",
      "Epoch 3959, Loss: 0.11141708120703697, Final Batch Loss: 0.027581296861171722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3960, Loss: 0.21026857942342758, Final Batch Loss: 0.08828401565551758\n",
      "Epoch 3961, Loss: 0.24155128002166748, Final Batch Loss: 0.07783535122871399\n",
      "Epoch 3962, Loss: 0.12630425300449133, Final Batch Loss: 0.013473800383508205\n",
      "Epoch 3963, Loss: 0.12818889692425728, Final Batch Loss: 0.033899880945682526\n",
      "Epoch 3964, Loss: 0.2047284170985222, Final Batch Loss: 0.07976722717285156\n",
      "Epoch 3965, Loss: 0.22640816122293472, Final Batch Loss: 0.045304298400878906\n",
      "Epoch 3966, Loss: 0.4059164673089981, Final Batch Loss: 0.2353346198797226\n",
      "Epoch 3967, Loss: 0.13916314020752907, Final Batch Loss: 0.030575979501008987\n",
      "Epoch 3968, Loss: 0.13098790682852268, Final Batch Loss: 0.021479902788996696\n",
      "Epoch 3969, Loss: 0.25514938682317734, Final Batch Loss: 0.11478334665298462\n",
      "Epoch 3970, Loss: 0.2154845893383026, Final Batch Loss: 0.07789404690265656\n",
      "Epoch 3971, Loss: 0.15628967247903347, Final Batch Loss: 0.02255110628902912\n",
      "Epoch 3972, Loss: 0.14999105222523212, Final Batch Loss: 0.06067914888262749\n",
      "Epoch 3973, Loss: 0.13256898149847984, Final Batch Loss: 0.029315046966075897\n",
      "Epoch 3974, Loss: 0.10216590389609337, Final Batch Loss: 0.025718282908201218\n",
      "Epoch 3975, Loss: 0.19756568782031536, Final Batch Loss: 0.11005287617444992\n",
      "Epoch 3976, Loss: 0.1380164623260498, Final Batch Loss: 0.06506786495447159\n",
      "Epoch 3977, Loss: 0.09977802447974682, Final Batch Loss: 0.008550481870770454\n",
      "Epoch 3978, Loss: 0.13729843869805336, Final Batch Loss: 0.06875645369291306\n",
      "Epoch 3979, Loss: 0.11095046252012253, Final Batch Loss: 0.03559395670890808\n",
      "Epoch 3980, Loss: 0.14000514149665833, Final Batch Loss: 0.056088559329509735\n",
      "Epoch 3981, Loss: 0.1886950619518757, Final Batch Loss: 0.06099994108080864\n",
      "Epoch 3982, Loss: 0.061834572930820286, Final Batch Loss: 0.001733047072775662\n",
      "Epoch 3983, Loss: 0.13732223212718964, Final Batch Loss: 0.08044745773077011\n",
      "Epoch 3984, Loss: 0.11602472886443138, Final Batch Loss: 0.03782526031136513\n",
      "Epoch 3985, Loss: 0.10978275537490845, Final Batch Loss: 0.022948451340198517\n",
      "Epoch 3986, Loss: 0.11395279550924897, Final Batch Loss: 0.007671153638511896\n",
      "Epoch 3987, Loss: 0.17539693042635918, Final Batch Loss: 0.09440988302230835\n",
      "Epoch 3988, Loss: 0.21077901311218739, Final Batch Loss: 0.14591819047927856\n",
      "Epoch 3989, Loss: 0.22107608243823051, Final Batch Loss: 0.07930796593427658\n",
      "Epoch 3990, Loss: 0.1340859830379486, Final Batch Loss: 0.06197412684559822\n",
      "Epoch 3991, Loss: 0.1401491453871131, Final Batch Loss: 0.01458622794598341\n",
      "Epoch 3992, Loss: 0.15605884790420532, Final Batch Loss: 0.01611156389117241\n",
      "Epoch 3993, Loss: 0.15637117251753807, Final Batch Loss: 0.07385019958019257\n",
      "Epoch 3994, Loss: 0.130600580945611, Final Batch Loss: 0.029422366991639137\n",
      "Epoch 3995, Loss: 0.19721068628132343, Final Batch Loss: 0.028876060619950294\n",
      "Epoch 3996, Loss: 0.1595776416361332, Final Batch Loss: 0.0335419625043869\n",
      "Epoch 3997, Loss: 0.16377926617860794, Final Batch Loss: 0.05810880661010742\n",
      "Epoch 3998, Loss: 0.1690380945801735, Final Batch Loss: 0.041853904724121094\n",
      "Epoch 3999, Loss: 0.1403016410768032, Final Batch Loss: 0.03459060564637184\n",
      "Epoch 4000, Loss: 0.13544374704360962, Final Batch Loss: 0.040199000388383865\n",
      "Epoch 4001, Loss: 0.11516487412154675, Final Batch Loss: 0.02345370687544346\n",
      "Epoch 4002, Loss: 0.09549006819725037, Final Batch Loss: 0.030077926814556122\n",
      "Epoch 4003, Loss: 0.130560458637774, Final Batch Loss: 0.04346755892038345\n",
      "Epoch 4004, Loss: 0.10853291675448418, Final Batch Loss: 0.020520340651273727\n",
      "Epoch 4005, Loss: 0.10640584118664265, Final Batch Loss: 0.017921937629580498\n",
      "Epoch 4006, Loss: 0.1263434924185276, Final Batch Loss: 0.025515371933579445\n",
      "Epoch 4007, Loss: 0.08130490314215422, Final Batch Loss: 0.050387971103191376\n",
      "Epoch 4008, Loss: 0.28597157448530197, Final Batch Loss: 0.18431933224201202\n",
      "Epoch 4009, Loss: 0.13170613534748554, Final Batch Loss: 0.06919652223587036\n",
      "Epoch 4010, Loss: 0.12059644237160683, Final Batch Loss: 0.054377682507038116\n",
      "Epoch 4011, Loss: 0.1379067413508892, Final Batch Loss: 0.016831006854772568\n",
      "Epoch 4012, Loss: 0.219033595174551, Final Batch Loss: 0.023716792464256287\n",
      "Epoch 4013, Loss: 0.0911177508533001, Final Batch Loss: 0.0323452390730381\n",
      "Epoch 4014, Loss: 0.09529435820877552, Final Batch Loss: 0.01653365045785904\n",
      "Epoch 4015, Loss: 0.1312086097896099, Final Batch Loss: 0.06053302809596062\n",
      "Epoch 4016, Loss: 0.1277230642735958, Final Batch Loss: 0.031485799700021744\n",
      "Epoch 4017, Loss: 0.11588004231452942, Final Batch Loss: 0.02824074774980545\n",
      "Epoch 4018, Loss: 0.08527208399027586, Final Batch Loss: 0.014809704385697842\n",
      "Epoch 4019, Loss: 0.15154284611344337, Final Batch Loss: 0.071335069835186\n",
      "Epoch 4020, Loss: 0.07746195420622826, Final Batch Loss: 0.01353621855378151\n",
      "Epoch 4021, Loss: 0.16600194945931435, Final Batch Loss: 0.07357297837734222\n",
      "Epoch 4022, Loss: 0.08582191355526447, Final Batch Loss: 0.030228596180677414\n",
      "Epoch 4023, Loss: 0.09740418940782547, Final Batch Loss: 0.028076447546482086\n",
      "Epoch 4024, Loss: 0.08311395696364343, Final Batch Loss: 0.001006044214591384\n",
      "Epoch 4025, Loss: 0.06573078222572803, Final Batch Loss: 0.027446433901786804\n",
      "Epoch 4026, Loss: 0.1071150777861476, Final Batch Loss: 0.0065375035628676414\n",
      "Epoch 4027, Loss: 0.13609621301293373, Final Batch Loss: 0.0626860186457634\n",
      "Epoch 4028, Loss: 0.09831412881612778, Final Batch Loss: 0.013575747609138489\n",
      "Epoch 4029, Loss: 0.07966476678848267, Final Batch Loss: 0.003631826490163803\n",
      "Epoch 4030, Loss: 0.2926788590848446, Final Batch Loss: 0.20230217278003693\n",
      "Epoch 4031, Loss: 0.13906172662973404, Final Batch Loss: 0.058885131031274796\n",
      "Epoch 4032, Loss: 0.2103884257376194, Final Batch Loss: 0.10638502240180969\n",
      "Epoch 4033, Loss: 0.08731183037161827, Final Batch Loss: 0.03553337603807449\n",
      "Epoch 4034, Loss: 0.09321234188973904, Final Batch Loss: 0.024121327325701714\n",
      "Epoch 4035, Loss: 0.1428827829658985, Final Batch Loss: 0.014492228627204895\n",
      "Epoch 4036, Loss: 0.08797287568449974, Final Batch Loss: 0.009501366876065731\n",
      "Epoch 4037, Loss: 0.08019520342350006, Final Batch Loss: 0.003644406795501709\n",
      "Epoch 4038, Loss: 0.11275775916874409, Final Batch Loss: 0.01675121858716011\n",
      "Epoch 4039, Loss: 0.13741699419915676, Final Batch Loss: 0.004907289519906044\n",
      "Epoch 4040, Loss: 0.0926555935293436, Final Batch Loss: 0.027520013973116875\n",
      "Epoch 4041, Loss: 0.100680872797966, Final Batch Loss: 0.05030284821987152\n",
      "Epoch 4042, Loss: 0.17330595292150974, Final Batch Loss: 0.11616478860378265\n",
      "Epoch 4043, Loss: 0.17283005639910698, Final Batch Loss: 0.06698352843523026\n",
      "Epoch 4044, Loss: 0.1054826732724905, Final Batch Loss: 0.05245519056916237\n",
      "Epoch 4045, Loss: 0.06757005956023932, Final Batch Loss: 0.012458301149308681\n",
      "Epoch 4046, Loss: 0.0851235892623663, Final Batch Loss: 0.019708823412656784\n",
      "Epoch 4047, Loss: 0.11138071306049824, Final Batch Loss: 0.04653240367770195\n",
      "Epoch 4048, Loss: 0.05835707765072584, Final Batch Loss: 0.013700885698199272\n",
      "Epoch 4049, Loss: 0.06592908501625061, Final Batch Loss: 0.008016856387257576\n",
      "Epoch 4050, Loss: 0.16473153606057167, Final Batch Loss: 0.1064465194940567\n",
      "Epoch 4051, Loss: 0.1825788989663124, Final Batch Loss: 0.08149011433124542\n",
      "Epoch 4052, Loss: 0.13338389992713928, Final Batch Loss: 0.037547316402196884\n",
      "Epoch 4053, Loss: 0.17955075204372406, Final Batch Loss: 0.033531397581100464\n",
      "Epoch 4054, Loss: 0.15456924866884947, Final Batch Loss: 0.0950978696346283\n",
      "Epoch 4055, Loss: 0.21056895703077316, Final Batch Loss: 0.11502226442098618\n",
      "Epoch 4056, Loss: 0.2294783741235733, Final Batch Loss: 0.13134804368019104\n",
      "Epoch 4057, Loss: 0.11998598836362362, Final Batch Loss: 0.03525865823030472\n",
      "Epoch 4058, Loss: 0.15000474825501442, Final Batch Loss: 0.0649486556649208\n",
      "Epoch 4059, Loss: 0.172715462744236, Final Batch Loss: 0.10228332132101059\n",
      "Epoch 4060, Loss: 0.17456810176372528, Final Batch Loss: 0.04526529461145401\n",
      "Epoch 4061, Loss: 0.15721788257360458, Final Batch Loss: 0.026138335466384888\n",
      "Epoch 4062, Loss: 0.20655012503266335, Final Batch Loss: 0.08152544498443604\n",
      "Epoch 4063, Loss: 0.3502664174884558, Final Batch Loss: 0.2431347370147705\n",
      "Epoch 4064, Loss: 0.23970090597867966, Final Batch Loss: 0.08777104318141937\n",
      "Epoch 4065, Loss: 0.25173165276646614, Final Batch Loss: 0.1071983352303505\n",
      "Epoch 4066, Loss: 0.40872374549508095, Final Batch Loss: 0.3002341687679291\n",
      "Epoch 4067, Loss: 0.1525830365717411, Final Batch Loss: 0.023205146193504333\n",
      "Epoch 4068, Loss: 0.2695365250110626, Final Batch Loss: 0.12267869710922241\n",
      "Epoch 4069, Loss: 0.17846589908003807, Final Batch Loss: 0.03216831013560295\n",
      "Epoch 4070, Loss: 0.15821432322263718, Final Batch Loss: 0.014385826885700226\n",
      "Epoch 4071, Loss: 0.1179419457912445, Final Batch Loss: 0.017715346068143845\n",
      "Epoch 4072, Loss: 0.2213355004787445, Final Batch Loss: 0.08778853714466095\n",
      "Epoch 4073, Loss: 0.15358863770961761, Final Batch Loss: 0.043434955179691315\n",
      "Epoch 4074, Loss: 0.11989432945847511, Final Batch Loss: 0.035219836980104446\n",
      "Epoch 4075, Loss: 0.0946282371878624, Final Batch Loss: 0.01050836592912674\n",
      "Epoch 4076, Loss: 0.16438938677310944, Final Batch Loss: 0.03141837567090988\n",
      "Epoch 4077, Loss: 0.1560153253376484, Final Batch Loss: 0.04821581393480301\n",
      "Epoch 4078, Loss: 0.07310244999825954, Final Batch Loss: 0.021940993145108223\n",
      "Epoch 4079, Loss: 0.12775322515517473, Final Batch Loss: 0.01278758142143488\n",
      "Epoch 4080, Loss: 0.08576565003022552, Final Batch Loss: 0.006720548029989004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4081, Loss: 0.08609113469719887, Final Batch Loss: 0.010058481246232986\n",
      "Epoch 4082, Loss: 0.1641860157251358, Final Batch Loss: 0.07376547157764435\n",
      "Epoch 4083, Loss: 0.07920099142938852, Final Batch Loss: 0.023718522861599922\n",
      "Epoch 4084, Loss: 0.08737368136644363, Final Batch Loss: 0.005411352962255478\n",
      "Epoch 4085, Loss: 0.07532528787851334, Final Batch Loss: 0.006781395524740219\n",
      "Epoch 4086, Loss: 0.145167276263237, Final Batch Loss: 0.08008544892072678\n",
      "Epoch 4087, Loss: 0.18826931342482567, Final Batch Loss: 0.051865097135305405\n",
      "Epoch 4088, Loss: 0.14807683788239956, Final Batch Loss: 0.009265558794140816\n",
      "Epoch 4089, Loss: 0.08458737656474113, Final Batch Loss: 0.03253621235489845\n",
      "Epoch 4090, Loss: 0.13058041222393513, Final Batch Loss: 0.041795678436756134\n",
      "Epoch 4091, Loss: 0.12130752578377724, Final Batch Loss: 0.0539875365793705\n",
      "Epoch 4092, Loss: 0.10006234608590603, Final Batch Loss: 0.053689077496528625\n",
      "Epoch 4093, Loss: 0.12008020840585232, Final Batch Loss: 0.062209438532590866\n",
      "Epoch 4094, Loss: 0.10887597687542439, Final Batch Loss: 0.03353219851851463\n",
      "Epoch 4095, Loss: 0.14034130843356252, Final Batch Loss: 0.00560013996437192\n",
      "Epoch 4096, Loss: 0.08125906810164452, Final Batch Loss: 0.011522991582751274\n",
      "Epoch 4097, Loss: 0.09351899102330208, Final Batch Loss: 0.023094262927770615\n",
      "Epoch 4098, Loss: 0.11236696969717741, Final Batch Loss: 0.009218274615705013\n",
      "Epoch 4099, Loss: 0.12232672236859798, Final Batch Loss: 0.08089157193899155\n",
      "Epoch 4100, Loss: 0.06817775731906295, Final Batch Loss: 0.005261776503175497\n",
      "Epoch 4101, Loss: 0.11763165518641472, Final Batch Loss: 0.06883134692907333\n",
      "Epoch 4102, Loss: 0.13439450412988663, Final Batch Loss: 0.05468839779496193\n",
      "Epoch 4103, Loss: 0.20265076449140906, Final Batch Loss: 0.006688260938972235\n",
      "Epoch 4104, Loss: 0.10243021883070469, Final Batch Loss: 0.012021408416330814\n",
      "Epoch 4105, Loss: 0.09233338385820389, Final Batch Loss: 0.02033296972513199\n",
      "Epoch 4106, Loss: 0.11042822152376175, Final Batch Loss: 0.026832574978470802\n",
      "Epoch 4107, Loss: 0.07447641715407372, Final Batch Loss: 0.02225468121469021\n",
      "Epoch 4108, Loss: 0.11692236177623272, Final Batch Loss: 0.04552192613482475\n",
      "Epoch 4109, Loss: 0.12251516059041023, Final Batch Loss: 0.03887764737010002\n",
      "Epoch 4110, Loss: 0.13259280659258366, Final Batch Loss: 0.05130578204989433\n",
      "Epoch 4111, Loss: 0.08194074546918273, Final Batch Loss: 0.0065360418520867825\n",
      "Epoch 4112, Loss: 0.18114861845970154, Final Batch Loss: 0.06607942283153534\n",
      "Epoch 4113, Loss: 0.11283456906676292, Final Batch Loss: 0.027102597057819366\n",
      "Epoch 4114, Loss: 0.13638588786125183, Final Batch Loss: 0.015184342861175537\n",
      "Epoch 4115, Loss: 0.1227369150146842, Final Batch Loss: 0.04242486506700516\n",
      "Epoch 4116, Loss: 0.1367112398147583, Final Batch Loss: 0.07671716809272766\n",
      "Epoch 4117, Loss: 0.10239410493522882, Final Batch Loss: 0.011804667301476002\n",
      "Epoch 4118, Loss: 0.19980550929903984, Final Batch Loss: 0.09000543504953384\n",
      "Epoch 4119, Loss: 0.1054520532488823, Final Batch Loss: 0.035768210887908936\n",
      "Epoch 4120, Loss: 0.22371812164783478, Final Batch Loss: 0.171317458152771\n",
      "Epoch 4121, Loss: 0.29132477194070816, Final Batch Loss: 0.20189844071865082\n",
      "Epoch 4122, Loss: 0.06557368393987417, Final Batch Loss: 0.014462095685303211\n",
      "Epoch 4123, Loss: 0.14588372223079205, Final Batch Loss: 0.01699288748204708\n",
      "Epoch 4124, Loss: 0.22966502606868744, Final Batch Loss: 0.11698425561189651\n",
      "Epoch 4125, Loss: 0.2731592580676079, Final Batch Loss: 0.17573006451129913\n",
      "Epoch 4126, Loss: 0.10944160213693976, Final Batch Loss: 0.003075615968555212\n",
      "Epoch 4127, Loss: 0.2154238373041153, Final Batch Loss: 0.1287626326084137\n",
      "Epoch 4128, Loss: 0.1617727056145668, Final Batch Loss: 0.03689083084464073\n",
      "Epoch 4129, Loss: 0.15462956205010414, Final Batch Loss: 0.039747610688209534\n",
      "Epoch 4130, Loss: 0.1291618924587965, Final Batch Loss: 0.007495163008570671\n",
      "Epoch 4131, Loss: 0.3563130423426628, Final Batch Loss: 0.14640937745571136\n",
      "Epoch 4132, Loss: 0.1252127606421709, Final Batch Loss: 0.02843027003109455\n",
      "Epoch 4133, Loss: 0.12621713429689407, Final Batch Loss: 0.021109361201524734\n",
      "Epoch 4134, Loss: 0.08941961824893951, Final Batch Loss: 0.026982562616467476\n",
      "Epoch 4135, Loss: 0.11334100924432278, Final Batch Loss: 0.03962346166372299\n",
      "Epoch 4136, Loss: 0.10231949016451836, Final Batch Loss: 0.029375627636909485\n",
      "Epoch 4137, Loss: 0.0885536796413362, Final Batch Loss: 0.007732202764600515\n",
      "Epoch 4138, Loss: 0.25599973648786545, Final Batch Loss: 0.18352562189102173\n",
      "Epoch 4139, Loss: 0.07323353458195925, Final Batch Loss: 0.014378107152879238\n",
      "Epoch 4140, Loss: 0.1730173397809267, Final Batch Loss: 0.09452734142541885\n",
      "Epoch 4141, Loss: 0.15616321749985218, Final Batch Loss: 0.074334055185318\n",
      "Epoch 4142, Loss: 0.16272196732461452, Final Batch Loss: 0.02088896371424198\n",
      "Epoch 4143, Loss: 0.15315785352140665, Final Batch Loss: 0.015347530134022236\n",
      "Epoch 4144, Loss: 0.21141741052269936, Final Batch Loss: 0.061245594173669815\n",
      "Epoch 4145, Loss: 0.14754983130842447, Final Batch Loss: 0.009515327401459217\n",
      "Epoch 4146, Loss: 0.09115451015532017, Final Batch Loss: 0.00867103599011898\n",
      "Epoch 4147, Loss: 0.13093061745166779, Final Batch Loss: 0.029591653496026993\n",
      "Epoch 4148, Loss: 0.2636728137731552, Final Batch Loss: 0.10566423088312149\n",
      "Epoch 4149, Loss: 0.3669609762728214, Final Batch Loss: 0.2551473081111908\n",
      "Epoch 4150, Loss: 0.11988349817693233, Final Batch Loss: 0.013271057978272438\n",
      "Epoch 4151, Loss: 0.16996600106358528, Final Batch Loss: 0.03148035705089569\n",
      "Epoch 4152, Loss: 0.33440857380628586, Final Batch Loss: 0.1579420417547226\n",
      "Epoch 4153, Loss: 0.21616648137569427, Final Batch Loss: 0.05702508985996246\n",
      "Epoch 4154, Loss: 0.19062285125255585, Final Batch Loss: 0.020866230130195618\n",
      "Epoch 4155, Loss: 0.17037931457161903, Final Batch Loss: 0.041942209005355835\n",
      "Epoch 4156, Loss: 0.2245831899344921, Final Batch Loss: 0.032609328627586365\n",
      "Epoch 4157, Loss: 0.17576858773827553, Final Batch Loss: 0.041979070752859116\n",
      "Epoch 4158, Loss: 0.22589895874261856, Final Batch Loss: 0.09425938129425049\n",
      "Epoch 4159, Loss: 0.1892842873930931, Final Batch Loss: 0.05527138710021973\n",
      "Epoch 4160, Loss: 0.26343522034585476, Final Batch Loss: 0.04234033077955246\n",
      "Epoch 4161, Loss: 0.11774438247084618, Final Batch Loss: 0.039753325283527374\n",
      "Epoch 4162, Loss: 0.13911208882927895, Final Batch Loss: 0.047336362302303314\n",
      "Epoch 4163, Loss: 0.11576305516064167, Final Batch Loss: 0.02036040462553501\n",
      "Epoch 4164, Loss: 0.1851583905518055, Final Batch Loss: 0.08533423393964767\n",
      "Epoch 4165, Loss: 0.3137100301682949, Final Batch Loss: 0.22695107758045197\n",
      "Epoch 4166, Loss: 0.1698291413486004, Final Batch Loss: 0.04849817231297493\n",
      "Epoch 4167, Loss: 0.13348716869950294, Final Batch Loss: 0.03167449310421944\n",
      "Epoch 4168, Loss: 0.08425724506378174, Final Batch Loss: 0.015756400302052498\n",
      "Epoch 4169, Loss: 0.10587290767580271, Final Batch Loss: 0.008578845299780369\n",
      "Epoch 4170, Loss: 0.12620601803064346, Final Batch Loss: 0.06937876343727112\n",
      "Epoch 4171, Loss: 0.09395759180188179, Final Batch Loss: 0.03066820092499256\n",
      "Epoch 4172, Loss: 0.0857145395129919, Final Batch Loss: 0.017461072653532028\n",
      "Epoch 4173, Loss: 0.08947257697582245, Final Batch Loss: 0.00954076275229454\n",
      "Epoch 4174, Loss: 0.13708539307117462, Final Batch Loss: 0.03245816379785538\n",
      "Epoch 4175, Loss: 0.08165484946221113, Final Batch Loss: 0.012081637047231197\n",
      "Epoch 4176, Loss: 0.12943102978169918, Final Batch Loss: 0.018118588253855705\n",
      "Epoch 4177, Loss: 0.11609963327646255, Final Batch Loss: 0.05424389988183975\n",
      "Epoch 4178, Loss: 0.12431097030639648, Final Batch Loss: 0.055119775235652924\n",
      "Epoch 4179, Loss: 0.08180989231914282, Final Batch Loss: 0.012034325860440731\n",
      "Epoch 4180, Loss: 0.13554047420620918, Final Batch Loss: 0.011948999017477036\n",
      "Epoch 4181, Loss: 0.13296133745461702, Final Batch Loss: 0.0037760334089398384\n",
      "Epoch 4182, Loss: 0.11036747135221958, Final Batch Loss: 0.029824862256646156\n",
      "Epoch 4183, Loss: 0.09324419405311346, Final Batch Loss: 0.009426097385585308\n",
      "Epoch 4184, Loss: 0.11329945363104343, Final Batch Loss: 0.027315771207213402\n",
      "Epoch 4185, Loss: 0.10533254966139793, Final Batch Loss: 0.03695063292980194\n",
      "Epoch 4186, Loss: 0.15355408564209938, Final Batch Loss: 0.04959815740585327\n",
      "Epoch 4187, Loss: 0.11263084039092064, Final Batch Loss: 0.0467587485909462\n",
      "Epoch 4188, Loss: 0.09692401066422462, Final Batch Loss: 0.015222592279314995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4189, Loss: 0.08742520399391651, Final Batch Loss: 0.023258492350578308\n",
      "Epoch 4190, Loss: 0.10962632857263088, Final Batch Loss: 0.021305205300450325\n",
      "Epoch 4191, Loss: 0.1620963281020522, Final Batch Loss: 0.11214638501405716\n",
      "Epoch 4192, Loss: 0.09178153518587351, Final Batch Loss: 0.011182577349245548\n",
      "Epoch 4193, Loss: 0.06857053760904819, Final Batch Loss: 0.0008492154302075505\n",
      "Epoch 4194, Loss: 0.06098389159888029, Final Batch Loss: 0.007384759373962879\n",
      "Epoch 4195, Loss: 0.06703265104442835, Final Batch Loss: 0.025251442566514015\n",
      "Epoch 4196, Loss: 0.1259039007127285, Final Batch Loss: 0.03880460932850838\n",
      "Epoch 4197, Loss: 0.11132807098329067, Final Batch Loss: 0.012101562693715096\n",
      "Epoch 4198, Loss: 0.05910382512956858, Final Batch Loss: 0.014374521560966969\n",
      "Epoch 4199, Loss: 0.0656534368172288, Final Batch Loss: 0.014925685711205006\n",
      "Epoch 4200, Loss: 0.13541517220437527, Final Batch Loss: 0.02854425273835659\n",
      "Epoch 4201, Loss: 0.0827042181044817, Final Batch Loss: 0.020281000062823296\n",
      "Epoch 4202, Loss: 0.11196755431592464, Final Batch Loss: 0.055891647934913635\n",
      "Epoch 4203, Loss: 0.0815760288387537, Final Batch Loss: 0.023605776950716972\n",
      "Epoch 4204, Loss: 0.09494498372077942, Final Batch Loss: 0.015488751232624054\n",
      "Epoch 4205, Loss: 0.09062957763671875, Final Batch Loss: 0.031746674329042435\n",
      "Epoch 4206, Loss: 0.036516839638352394, Final Batch Loss: 0.008028846234083176\n",
      "Epoch 4207, Loss: 0.135753083974123, Final Batch Loss: 0.027591537684202194\n",
      "Epoch 4208, Loss: 0.06428147526457906, Final Batch Loss: 0.003260201308876276\n",
      "Epoch 4209, Loss: 0.05726097384467721, Final Batch Loss: 0.006843397859483957\n",
      "Epoch 4210, Loss: 0.24194558709859848, Final Batch Loss: 0.043673571199178696\n",
      "Epoch 4211, Loss: 0.09415666293352842, Final Batch Loss: 0.014959954656660557\n",
      "Epoch 4212, Loss: 0.09341313317418098, Final Batch Loss: 0.03498699516057968\n",
      "Epoch 4213, Loss: 0.10429144091904163, Final Batch Loss: 0.014421438798308372\n",
      "Epoch 4214, Loss: 0.11407520622015, Final Batch Loss: 0.033031661063432693\n",
      "Epoch 4215, Loss: 0.05143307335674763, Final Batch Loss: 0.018043169751763344\n",
      "Epoch 4216, Loss: 0.11197786964476109, Final Batch Loss: 0.04578094556927681\n",
      "Epoch 4217, Loss: 0.11545475013554096, Final Batch Loss: 0.024520132690668106\n",
      "Epoch 4218, Loss: 0.12235809862613678, Final Batch Loss: 0.04475734755396843\n",
      "Epoch 4219, Loss: 0.08595577254891396, Final Batch Loss: 0.03995257616043091\n",
      "Epoch 4220, Loss: 0.10178432427346706, Final Batch Loss: 0.027224021032452583\n",
      "Epoch 4221, Loss: 0.09109851391986012, Final Batch Loss: 0.004663592670112848\n",
      "Epoch 4222, Loss: 0.094434579834342, Final Batch Loss: 0.022630060091614723\n",
      "Epoch 4223, Loss: 0.13964004069566727, Final Batch Loss: 0.05753341689705849\n",
      "Epoch 4224, Loss: 0.13999748788774014, Final Batch Loss: 0.03064720518887043\n",
      "Epoch 4225, Loss: 0.19411830231547356, Final Batch Loss: 0.08372776955366135\n",
      "Epoch 4226, Loss: 0.08102643117308617, Final Batch Loss: 0.012393221259117126\n",
      "Epoch 4227, Loss: 0.10419393703341484, Final Batch Loss: 0.01657189056277275\n",
      "Epoch 4228, Loss: 0.12447462603449821, Final Batch Loss: 0.043129757046699524\n",
      "Epoch 4229, Loss: 0.15204628556966782, Final Batch Loss: 0.0639636293053627\n",
      "Epoch 4230, Loss: 0.09319995157420635, Final Batch Loss: 0.023193294182419777\n",
      "Epoch 4231, Loss: 0.07181175169534981, Final Batch Loss: 0.0020317540038377047\n",
      "Epoch 4232, Loss: 0.39573707431554794, Final Batch Loss: 0.2588147521018982\n",
      "Epoch 4233, Loss: 0.14473575353622437, Final Batch Loss: 0.029137950390577316\n",
      "Epoch 4234, Loss: 0.29158251732587814, Final Batch Loss: 0.1242942065000534\n",
      "Epoch 4235, Loss: 0.21587222814559937, Final Batch Loss: 0.07615374773740768\n",
      "Epoch 4236, Loss: 0.14259140565991402, Final Batch Loss: 0.045037172734737396\n",
      "Epoch 4237, Loss: 0.19516966119408607, Final Batch Loss: 0.10195339471101761\n",
      "Epoch 4238, Loss: 0.14851197972893715, Final Batch Loss: 0.07607095688581467\n",
      "Epoch 4239, Loss: 0.3251447267830372, Final Batch Loss: 0.1967848837375641\n",
      "Epoch 4240, Loss: 0.11958141624927521, Final Batch Loss: 0.01740909367799759\n",
      "Epoch 4241, Loss: 0.44167305529117584, Final Batch Loss: 0.11861276626586914\n",
      "Epoch 4242, Loss: 0.34378141909837723, Final Batch Loss: 0.17589916288852692\n",
      "Epoch 4243, Loss: 0.22742648841813207, Final Batch Loss: 0.0057590720243752\n",
      "Epoch 4244, Loss: 0.3019000291824341, Final Batch Loss: 0.09486797451972961\n",
      "Epoch 4245, Loss: 0.19149094633758068, Final Batch Loss: 0.027453644201159477\n",
      "Epoch 4246, Loss: 0.1762308795005083, Final Batch Loss: 0.0158663559705019\n",
      "Epoch 4247, Loss: 0.16438328102231026, Final Batch Loss: 0.0762547180056572\n",
      "Epoch 4248, Loss: 0.2486782856285572, Final Batch Loss: 0.06318256258964539\n",
      "Epoch 4249, Loss: 0.1817747838795185, Final Batch Loss: 0.05864018201828003\n",
      "Epoch 4250, Loss: 0.17434190586209297, Final Batch Loss: 0.06613828241825104\n",
      "Epoch 4251, Loss: 0.1687597893178463, Final Batch Loss: 0.04792157933115959\n",
      "Epoch 4252, Loss: 0.1888537034392357, Final Batch Loss: 0.07138007879257202\n",
      "Epoch 4253, Loss: 0.19173899665474892, Final Batch Loss: 0.04545014351606369\n",
      "Epoch 4254, Loss: 0.19561018981039524, Final Batch Loss: 0.02603873796761036\n",
      "Epoch 4255, Loss: 0.16182438470423222, Final Batch Loss: 0.021548883989453316\n",
      "Epoch 4256, Loss: 0.14959917217493057, Final Batch Loss: 0.059266842901706696\n",
      "Epoch 4257, Loss: 0.15158133395016193, Final Batch Loss: 0.019413849338889122\n",
      "Epoch 4258, Loss: 0.13017786666750908, Final Batch Loss: 0.02169221267104149\n",
      "Epoch 4259, Loss: 0.15658001229166985, Final Batch Loss: 0.01656695082783699\n",
      "Epoch 4260, Loss: 0.19319941103458405, Final Batch Loss: 0.09190133213996887\n",
      "Epoch 4261, Loss: 0.19025727733969688, Final Batch Loss: 0.029909085482358932\n",
      "Epoch 4262, Loss: 0.12070181034505367, Final Batch Loss: 0.016484176740050316\n",
      "Epoch 4263, Loss: 0.16946041584014893, Final Batch Loss: 0.05215129628777504\n",
      "Epoch 4264, Loss: 0.08199379313737154, Final Batch Loss: 0.013530085794627666\n",
      "Epoch 4265, Loss: 0.14860989898443222, Final Batch Loss: 0.031751904636621475\n",
      "Epoch 4266, Loss: 0.16236943751573563, Final Batch Loss: 0.08910375833511353\n",
      "Epoch 4267, Loss: 0.22521333768963814, Final Batch Loss: 0.1422639936208725\n",
      "Epoch 4268, Loss: 0.17194005474448204, Final Batch Loss: 0.07134035229682922\n",
      "Epoch 4269, Loss: 0.2993812747299671, Final Batch Loss: 0.16252531111240387\n",
      "Epoch 4270, Loss: 0.1507934257388115, Final Batch Loss: 0.043848391622304916\n",
      "Epoch 4271, Loss: 0.15614137798547745, Final Batch Loss: 0.06338333338499069\n",
      "Epoch 4272, Loss: 0.21583443507552147, Final Batch Loss: 0.08351974189281464\n",
      "Epoch 4273, Loss: 0.16570052318274975, Final Batch Loss: 0.02691086195409298\n",
      "Epoch 4274, Loss: 0.17287711426615715, Final Batch Loss: 0.04288238659501076\n",
      "Epoch 4275, Loss: 0.1084489207714796, Final Batch Loss: 0.02157730422914028\n",
      "Epoch 4276, Loss: 0.15871895756572485, Final Batch Loss: 0.011737181805074215\n",
      "Epoch 4277, Loss: 0.18304076045751572, Final Batch Loss: 0.061242006719112396\n",
      "Epoch 4278, Loss: 0.19677499309182167, Final Batch Loss: 0.08208611607551575\n",
      "Epoch 4279, Loss: 0.09039759822189808, Final Batch Loss: 0.008534129709005356\n",
      "Epoch 4280, Loss: 0.10484002437442541, Final Batch Loss: 0.046487465500831604\n",
      "Epoch 4281, Loss: 0.1321092899888754, Final Batch Loss: 0.026575302705168724\n",
      "Epoch 4282, Loss: 0.09165444411337376, Final Batch Loss: 0.044560980051755905\n",
      "Epoch 4283, Loss: 0.12761885300278664, Final Batch Loss: 0.05000677332282066\n",
      "Epoch 4284, Loss: 0.11853731330484152, Final Batch Loss: 0.014121928252279758\n",
      "Epoch 4285, Loss: 0.13562269881367683, Final Batch Loss: 0.03675493225455284\n",
      "Epoch 4286, Loss: 0.17974099144339561, Final Batch Loss: 0.006048675626516342\n",
      "Epoch 4287, Loss: 0.1867302469909191, Final Batch Loss: 0.08485323190689087\n",
      "Epoch 4288, Loss: 0.2500521969050169, Final Batch Loss: 0.1735331267118454\n",
      "Epoch 4289, Loss: 0.19581057503819466, Final Batch Loss: 0.037971191108226776\n",
      "Epoch 4290, Loss: 0.16280628740787506, Final Batch Loss: 0.04403237625956535\n",
      "Epoch 4291, Loss: 0.14161369390785694, Final Batch Loss: 0.00973699800670147\n",
      "Epoch 4292, Loss: 0.21824376098811626, Final Batch Loss: 0.02236468903720379\n",
      "Epoch 4293, Loss: 0.12971085123717785, Final Batch Loss: 0.016264325007796288\n",
      "Epoch 4294, Loss: 0.14626405574381351, Final Batch Loss: 0.04830082133412361\n",
      "Epoch 4295, Loss: 0.09222333878278732, Final Batch Loss: 0.028751268982887268\n",
      "Epoch 4296, Loss: 0.10973876342177391, Final Batch Loss: 0.0469689778983593\n",
      "Epoch 4297, Loss: 0.10720320418477058, Final Batch Loss: 0.027702782303094864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4298, Loss: 0.16920888796448708, Final Batch Loss: 0.04323672130703926\n",
      "Epoch 4299, Loss: 0.2108420617878437, Final Batch Loss: 0.09007986634969711\n",
      "Epoch 4300, Loss: 0.1550117675215006, Final Batch Loss: 0.022932859137654305\n",
      "Epoch 4301, Loss: 0.11682114377617836, Final Batch Loss: 0.044759657233953476\n",
      "Epoch 4302, Loss: 0.20489195734262466, Final Batch Loss: 0.09899596869945526\n",
      "Epoch 4303, Loss: 0.22760205157101154, Final Batch Loss: 0.14522962272167206\n",
      "Epoch 4304, Loss: 0.2302800491452217, Final Batch Loss: 0.07767975330352783\n",
      "Epoch 4305, Loss: 0.15811142697930336, Final Batch Loss: 0.042273957282304764\n",
      "Epoch 4306, Loss: 0.19597618095576763, Final Batch Loss: 0.019144875928759575\n",
      "Epoch 4307, Loss: 0.14029900170862675, Final Batch Loss: 0.019878646358847618\n",
      "Epoch 4308, Loss: 0.23435954749584198, Final Batch Loss: 0.08390849828720093\n",
      "Epoch 4309, Loss: 0.21985256485641003, Final Batch Loss: 0.0742315798997879\n",
      "Epoch 4310, Loss: 0.12950717844069004, Final Batch Loss: 0.013625452294945717\n",
      "Epoch 4311, Loss: 0.25931544229388237, Final Batch Loss: 0.13160540163516998\n",
      "Epoch 4312, Loss: 0.18818815425038338, Final Batch Loss: 0.07229238748550415\n",
      "Epoch 4313, Loss: 0.19034690782427788, Final Batch Loss: 0.06380384415388107\n",
      "Epoch 4314, Loss: 0.22644534334540367, Final Batch Loss: 0.045035917311906815\n",
      "Epoch 4315, Loss: 0.10124183259904385, Final Batch Loss: 0.010673338547348976\n",
      "Epoch 4316, Loss: 0.2459290325641632, Final Batch Loss: 0.1335054636001587\n",
      "Epoch 4317, Loss: 0.18438170850276947, Final Batch Loss: 0.0510355681180954\n",
      "Epoch 4318, Loss: 0.17732271924614906, Final Batch Loss: 0.038237832486629486\n",
      "Epoch 4319, Loss: 0.12839857302606106, Final Batch Loss: 0.01680227555334568\n",
      "Epoch 4320, Loss: 0.11808488517999649, Final Batch Loss: 0.01851242408156395\n",
      "Epoch 4321, Loss: 0.13446996174752712, Final Batch Loss: 0.014582289382815361\n",
      "Epoch 4322, Loss: 0.10040131397545338, Final Batch Loss: 0.0101862121373415\n",
      "Epoch 4323, Loss: 0.17042128648608923, Final Batch Loss: 0.01259638275951147\n",
      "Epoch 4324, Loss: 0.10046986117959023, Final Batch Loss: 0.009048201143741608\n",
      "Epoch 4325, Loss: 0.1282156202942133, Final Batch Loss: 0.025900868698954582\n",
      "Epoch 4326, Loss: 0.12246015667915344, Final Batch Loss: 0.05551794543862343\n",
      "Epoch 4327, Loss: 0.16810142621397972, Final Batch Loss: 0.04936583340167999\n",
      "Epoch 4328, Loss: 0.09816623292863369, Final Batch Loss: 0.012713616713881493\n",
      "Epoch 4329, Loss: 0.08009047247469425, Final Batch Loss: 0.02361910417675972\n",
      "Epoch 4330, Loss: 0.17722789198160172, Final Batch Loss: 0.07366097718477249\n",
      "Epoch 4331, Loss: 0.18978973478078842, Final Batch Loss: 0.14441892504692078\n",
      "Epoch 4332, Loss: 0.35993070900440216, Final Batch Loss: 0.27504345774650574\n",
      "Epoch 4333, Loss: 0.1466850887518376, Final Batch Loss: 0.002892638323828578\n",
      "Epoch 4334, Loss: 0.15806834027171135, Final Batch Loss: 0.06378962099552155\n",
      "Epoch 4335, Loss: 0.08862442523241043, Final Batch Loss: 0.016421694308519363\n",
      "Epoch 4336, Loss: 0.11249777302145958, Final Batch Loss: 0.019057396799325943\n",
      "Epoch 4337, Loss: 0.10020014084875584, Final Batch Loss: 0.039587877690792084\n",
      "Epoch 4338, Loss: 0.10576075874269009, Final Batch Loss: 0.008659930899739265\n",
      "Epoch 4339, Loss: 0.19759424775838852, Final Batch Loss: 0.09099214524030685\n",
      "Epoch 4340, Loss: 0.06906490446999669, Final Batch Loss: 0.0038276822306215763\n",
      "Epoch 4341, Loss: 0.18763167783617973, Final Batch Loss: 0.051662132143974304\n",
      "Epoch 4342, Loss: 0.17519094422459602, Final Batch Loss: 0.04302245378494263\n",
      "Epoch 4343, Loss: 0.15930136293172836, Final Batch Loss: 0.07349500805139542\n",
      "Epoch 4344, Loss: 0.1569911539554596, Final Batch Loss: 0.047754209488630295\n",
      "Epoch 4345, Loss: 0.21518496796488762, Final Batch Loss: 0.08883830904960632\n",
      "Epoch 4346, Loss: 0.22811005637049675, Final Batch Loss: 0.13454864919185638\n",
      "Epoch 4347, Loss: 0.12463115900754929, Final Batch Loss: 0.033794645220041275\n",
      "Epoch 4348, Loss: 0.14865879155695438, Final Batch Loss: 0.06167164072394371\n",
      "Epoch 4349, Loss: 0.19982613623142242, Final Batch Loss: 0.06316470354795456\n",
      "Epoch 4350, Loss: 0.15553429163992405, Final Batch Loss: 0.050780292600393295\n",
      "Epoch 4351, Loss: 0.12443858478218317, Final Batch Loss: 0.011061755008995533\n",
      "Epoch 4352, Loss: 0.3096415139734745, Final Batch Loss: 0.2197897881269455\n",
      "Epoch 4353, Loss: 0.1535002700984478, Final Batch Loss: 0.05390886589884758\n",
      "Epoch 4354, Loss: 0.16634957492351532, Final Batch Loss: 0.07787683606147766\n",
      "Epoch 4355, Loss: 0.19292502850294113, Final Batch Loss: 0.08433667570352554\n",
      "Epoch 4356, Loss: 0.1553663555532694, Final Batch Loss: 0.0734785944223404\n",
      "Epoch 4357, Loss: 0.2824929654598236, Final Batch Loss: 0.16672298312187195\n",
      "Epoch 4358, Loss: 0.13251514919102192, Final Batch Loss: 0.033200573176145554\n",
      "Epoch 4359, Loss: 0.20726313814520836, Final Batch Loss: 0.08044189214706421\n",
      "Epoch 4360, Loss: 0.16166585311293602, Final Batch Loss: 0.02292792871594429\n",
      "Epoch 4361, Loss: 0.18444619327783585, Final Batch Loss: 0.07673486322164536\n",
      "Epoch 4362, Loss: 0.24162530153989792, Final Batch Loss: 0.0549798384308815\n",
      "Epoch 4363, Loss: 0.1820201240479946, Final Batch Loss: 0.043818727135658264\n",
      "Epoch 4364, Loss: 0.19759994000196457, Final Batch Loss: 0.07105142623186111\n",
      "Epoch 4365, Loss: 0.17894769087433815, Final Batch Loss: 0.07309454679489136\n",
      "Epoch 4366, Loss: 0.18259741365909576, Final Batch Loss: 0.07860565930604935\n",
      "Epoch 4367, Loss: 0.12850212585180998, Final Batch Loss: 0.011029760353267193\n",
      "Epoch 4368, Loss: 0.2542678713798523, Final Batch Loss: 0.07421409338712692\n",
      "Epoch 4369, Loss: 0.18130914494395256, Final Batch Loss: 0.06655491143465042\n",
      "Epoch 4370, Loss: 0.13297923281788826, Final Batch Loss: 0.01916595548391342\n",
      "Epoch 4371, Loss: 0.24626288563013077, Final Batch Loss: 0.17077070474624634\n",
      "Epoch 4372, Loss: 0.1406224798411131, Final Batch Loss: 0.01774410717189312\n",
      "Epoch 4373, Loss: 0.1063476987183094, Final Batch Loss: 0.020916011184453964\n",
      "Epoch 4374, Loss: 0.1541259717196226, Final Batch Loss: 0.016499711200594902\n",
      "Epoch 4375, Loss: 0.2337720189243555, Final Batch Loss: 0.12271516770124435\n",
      "Epoch 4376, Loss: 0.13075575977563858, Final Batch Loss: 0.05091622471809387\n",
      "Epoch 4377, Loss: 0.20447643473744392, Final Batch Loss: 0.02898455411195755\n",
      "Epoch 4378, Loss: 0.1442982666194439, Final Batch Loss: 0.04977916553616524\n",
      "Epoch 4379, Loss: 0.20763245597481728, Final Batch Loss: 0.11344755440950394\n",
      "Epoch 4380, Loss: 0.11560649145394564, Final Batch Loss: 0.010871157981455326\n",
      "Epoch 4381, Loss: 0.15068523958325386, Final Batch Loss: 0.03873956575989723\n",
      "Epoch 4382, Loss: 0.13054683059453964, Final Batch Loss: 0.016444552689790726\n",
      "Epoch 4383, Loss: 0.15084770135581493, Final Batch Loss: 0.016058405861258507\n",
      "Epoch 4384, Loss: 0.11325044929981232, Final Batch Loss: 0.019105970859527588\n",
      "Epoch 4385, Loss: 0.1428508274257183, Final Batch Loss: 0.033044565469026566\n",
      "Epoch 4386, Loss: 0.36549612134695053, Final Batch Loss: 0.27793845534324646\n",
      "Epoch 4387, Loss: 0.14147000573575497, Final Batch Loss: 0.028136225417256355\n",
      "Epoch 4388, Loss: 0.2536984831094742, Final Batch Loss: 0.08263418078422546\n",
      "Epoch 4389, Loss: 0.30760862305760384, Final Batch Loss: 0.17014992237091064\n",
      "Epoch 4390, Loss: 0.12199493870139122, Final Batch Loss: 0.03358808159828186\n",
      "Epoch 4391, Loss: 0.10164721589535475, Final Batch Loss: 0.015056592412292957\n",
      "Epoch 4392, Loss: 0.15712475776672363, Final Batch Loss: 0.07805092632770538\n",
      "Epoch 4393, Loss: 0.1476379632949829, Final Batch Loss: 0.05161336809396744\n",
      "Epoch 4394, Loss: 0.19875207170844078, Final Batch Loss: 0.0711313784122467\n",
      "Epoch 4395, Loss: 0.16533847525715828, Final Batch Loss: 0.06546106189489365\n",
      "Epoch 4396, Loss: 0.12959089688956738, Final Batch Loss: 0.02446996606886387\n",
      "Epoch 4397, Loss: 0.16269753500819206, Final Batch Loss: 0.04602666571736336\n",
      "Epoch 4398, Loss: 0.06090084323659539, Final Batch Loss: 0.004160842392593622\n",
      "Epoch 4399, Loss: 0.08846157230436802, Final Batch Loss: 0.012738296762108803\n",
      "Epoch 4400, Loss: 0.12973075918853283, Final Batch Loss: 0.049990858882665634\n",
      "Epoch 4401, Loss: 0.09737719036638737, Final Batch Loss: 0.05062378570437431\n",
      "Epoch 4402, Loss: 0.10112206637859344, Final Batch Loss: 0.025719908997416496\n",
      "Epoch 4403, Loss: 0.1366681894287467, Final Batch Loss: 0.013681064359843731\n",
      "Epoch 4404, Loss: 0.19574154540896416, Final Batch Loss: 0.1195073053240776\n",
      "Epoch 4405, Loss: 0.11944842338562012, Final Batch Loss: 0.024328365921974182\n",
      "Epoch 4406, Loss: 0.08042452856898308, Final Batch Loss: 0.018375366926193237\n",
      "Epoch 4407, Loss: 0.1569641623646021, Final Batch Loss: 0.015934010967612267\n",
      "Epoch 4408, Loss: 0.1928372159600258, Final Batch Loss: 0.11449235677719116\n",
      "Epoch 4409, Loss: 0.08702947711572051, Final Batch Loss: 0.007806563284248114\n",
      "Epoch 4410, Loss: 0.16236811503767967, Final Batch Loss: 0.04934724047780037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4411, Loss: 0.2353760451078415, Final Batch Loss: 0.10401426255702972\n",
      "Epoch 4412, Loss: 0.09643420949578285, Final Batch Loss: 0.01112312451004982\n",
      "Epoch 4413, Loss: 0.10811179503798485, Final Batch Loss: 0.035502348095178604\n",
      "Epoch 4414, Loss: 0.09728465043008327, Final Batch Loss: 0.016112379729747772\n",
      "Epoch 4415, Loss: 0.09753248654305935, Final Batch Loss: 0.03237675875425339\n",
      "Epoch 4416, Loss: 0.11203583516180515, Final Batch Loss: 0.03363020345568657\n",
      "Epoch 4417, Loss: 0.081866305321455, Final Batch Loss: 0.003504108637571335\n",
      "Epoch 4418, Loss: 0.19555481150746346, Final Batch Loss: 0.1142963394522667\n",
      "Epoch 4419, Loss: 0.1266203736886382, Final Batch Loss: 0.007428484968841076\n",
      "Epoch 4420, Loss: 0.16158127784729004, Final Batch Loss: 0.04308393597602844\n",
      "Epoch 4421, Loss: 0.10412420704960823, Final Batch Loss: 0.017057612538337708\n",
      "Epoch 4422, Loss: 0.14248071145266294, Final Batch Loss: 0.0117912283167243\n",
      "Epoch 4423, Loss: 0.16965126246213913, Final Batch Loss: 0.03755367174744606\n",
      "Epoch 4424, Loss: 0.19080793857574463, Final Batch Loss: 0.06445804983377457\n",
      "Epoch 4425, Loss: 0.11119137238711119, Final Batch Loss: 0.00782619696110487\n",
      "Epoch 4426, Loss: 0.17605431005358696, Final Batch Loss: 0.029929261654615402\n",
      "Epoch 4427, Loss: 0.08450539782643318, Final Batch Loss: 0.008913235738873482\n",
      "Epoch 4428, Loss: 0.18027843534946442, Final Batch Loss: 0.10405546426773071\n",
      "Epoch 4429, Loss: 0.24671050161123276, Final Batch Loss: 0.18285800516605377\n",
      "Epoch 4430, Loss: 0.16335857659578323, Final Batch Loss: 0.09604904800653458\n",
      "Epoch 4431, Loss: 0.10694845393300056, Final Batch Loss: 0.04695483297109604\n",
      "Epoch 4432, Loss: 0.20898354053497314, Final Batch Loss: 0.030422411859035492\n",
      "Epoch 4433, Loss: 0.24017922580242157, Final Batch Loss: 0.09476028382778168\n",
      "Epoch 4434, Loss: 0.29762205481529236, Final Batch Loss: 0.14940142631530762\n",
      "Epoch 4435, Loss: 0.1632653884589672, Final Batch Loss: 0.0629129633307457\n",
      "Epoch 4436, Loss: 0.15389459021389484, Final Batch Loss: 0.05855283513665199\n",
      "Epoch 4437, Loss: 0.10195568948984146, Final Batch Loss: 0.018365757539868355\n",
      "Epoch 4438, Loss: 0.12665454111993313, Final Batch Loss: 0.01936156116425991\n",
      "Epoch 4439, Loss: 0.2568277567625046, Final Batch Loss: 0.16999031603336334\n",
      "Epoch 4440, Loss: 0.1596144586801529, Final Batch Loss: 0.04244854301214218\n",
      "Epoch 4441, Loss: 0.11167766340076923, Final Batch Loss: 0.019430914893746376\n",
      "Epoch 4442, Loss: 0.11762169376015663, Final Batch Loss: 0.01967822015285492\n",
      "Epoch 4443, Loss: 0.1516462042927742, Final Batch Loss: 0.04423469677567482\n",
      "Epoch 4444, Loss: 0.09519504941999912, Final Batch Loss: 0.00950176827609539\n",
      "Epoch 4445, Loss: 0.1704398225992918, Final Batch Loss: 0.08052199333906174\n",
      "Epoch 4446, Loss: 0.17398832365870476, Final Batch Loss: 0.044963933527469635\n",
      "Epoch 4447, Loss: 0.12547224387526512, Final Batch Loss: 0.04363042116165161\n",
      "Epoch 4448, Loss: 0.12774521298706532, Final Batch Loss: 0.015729499980807304\n",
      "Epoch 4449, Loss: 0.1329285092651844, Final Batch Loss: 0.01841914653778076\n",
      "Epoch 4450, Loss: 0.22242549061775208, Final Batch Loss: 0.08861183375120163\n",
      "Epoch 4451, Loss: 0.10887489281594753, Final Batch Loss: 0.029501881450414658\n",
      "Epoch 4452, Loss: 0.1533481697551906, Final Batch Loss: 0.0018042284063994884\n",
      "Epoch 4453, Loss: 0.12472018413245678, Final Batch Loss: 0.041973501443862915\n",
      "Epoch 4454, Loss: 0.1304473914206028, Final Batch Loss: 0.07023622840642929\n",
      "Epoch 4455, Loss: 0.08498457632958889, Final Batch Loss: 0.009683610871434212\n",
      "Epoch 4456, Loss: 0.15117856860160828, Final Batch Loss: 0.03287695348262787\n",
      "Epoch 4457, Loss: 0.19340265169739723, Final Batch Loss: 0.07752563059329987\n",
      "Epoch 4458, Loss: 0.07588280737400055, Final Batch Loss: 0.008299624547362328\n",
      "Epoch 4459, Loss: 0.1513532791286707, Final Batch Loss: 0.02274639718234539\n",
      "Epoch 4460, Loss: 0.15346974693238735, Final Batch Loss: 0.023039167746901512\n",
      "Epoch 4461, Loss: 0.11554655060172081, Final Batch Loss: 0.01823057234287262\n",
      "Epoch 4462, Loss: 0.1709688901901245, Final Batch Loss: 0.05036621168255806\n",
      "Epoch 4463, Loss: 0.0771801508963108, Final Batch Loss: 0.0030750278383493423\n",
      "Epoch 4464, Loss: 0.1930614486336708, Final Batch Loss: 0.07907022535800934\n",
      "Epoch 4465, Loss: 0.11742203496396542, Final Batch Loss: 0.042156659066677094\n",
      "Epoch 4466, Loss: 0.2393489107489586, Final Batch Loss: 0.10260258615016937\n",
      "Epoch 4467, Loss: 0.21146713383495808, Final Batch Loss: 0.12947429716587067\n",
      "Epoch 4468, Loss: 0.14222600311040878, Final Batch Loss: 0.024339105933904648\n",
      "Epoch 4469, Loss: 0.10393881797790527, Final Batch Loss: 0.021130118519067764\n",
      "Epoch 4470, Loss: 0.2184683345258236, Final Batch Loss: 0.1251068115234375\n",
      "Epoch 4471, Loss: 0.1337022939696908, Final Batch Loss: 0.012097776867449284\n",
      "Epoch 4472, Loss: 0.17836099863052368, Final Batch Loss: 0.047915127128362656\n",
      "Epoch 4473, Loss: 0.12451176904141903, Final Batch Loss: 0.02871839888393879\n",
      "Epoch 4474, Loss: 0.11691968515515327, Final Batch Loss: 0.02869730442762375\n",
      "Epoch 4475, Loss: 0.08258014847524464, Final Batch Loss: 0.002945204498246312\n",
      "Epoch 4476, Loss: 0.15094387903809547, Final Batch Loss: 0.06483180075883865\n",
      "Epoch 4477, Loss: 0.1554546356201172, Final Batch Loss: 0.08629821240901947\n",
      "Epoch 4478, Loss: 0.187909796833992, Final Batch Loss: 0.09682773053646088\n",
      "Epoch 4479, Loss: 0.14554952830076218, Final Batch Loss: 0.04767124354839325\n",
      "Epoch 4480, Loss: 0.18976465985178947, Final Batch Loss: 0.14354221522808075\n",
      "Epoch 4481, Loss: 0.10221184976398945, Final Batch Loss: 0.024808254092931747\n",
      "Epoch 4482, Loss: 0.13713449612259865, Final Batch Loss: 0.05982528626918793\n",
      "Epoch 4483, Loss: 0.11033293232321739, Final Batch Loss: 0.0295175202190876\n",
      "Epoch 4484, Loss: 0.0910396771505475, Final Batch Loss: 0.006690287031233311\n",
      "Epoch 4485, Loss: 0.15557293221354485, Final Batch Loss: 0.02956899255514145\n",
      "Epoch 4486, Loss: 0.11562499962747097, Final Batch Loss: 0.018341174349188805\n",
      "Epoch 4487, Loss: 0.1070535508915782, Final Batch Loss: 0.008419188670814037\n",
      "Epoch 4488, Loss: 0.12500178068876266, Final Batch Loss: 0.010817372240126133\n",
      "Epoch 4489, Loss: 0.12449347227811813, Final Batch Loss: 0.049089837819337845\n",
      "Epoch 4490, Loss: 0.09644349059090018, Final Batch Loss: 0.0016701123677194118\n",
      "Epoch 4491, Loss: 0.05902490857988596, Final Batch Loss: 0.01122854184359312\n",
      "Epoch 4492, Loss: 0.10056816600263119, Final Batch Loss: 0.05328630283474922\n",
      "Epoch 4493, Loss: 0.2408706359565258, Final Batch Loss: 0.08888384699821472\n",
      "Epoch 4494, Loss: 0.1273157075047493, Final Batch Loss: 0.024561818689107895\n",
      "Epoch 4495, Loss: 0.05521681159734726, Final Batch Loss: 0.017528504133224487\n",
      "Epoch 4496, Loss: 0.07113184779882431, Final Batch Loss: 0.01882880926132202\n",
      "Epoch 4497, Loss: 0.13889473862946033, Final Batch Loss: 0.04743164777755737\n",
      "Epoch 4498, Loss: 0.11001030914485455, Final Batch Loss: 0.01866723783314228\n",
      "Epoch 4499, Loss: 0.12694047018885612, Final Batch Loss: 0.010010797530412674\n",
      "Epoch 4500, Loss: 0.07613718695938587, Final Batch Loss: 0.02248280681669712\n",
      "Epoch 4501, Loss: 0.067795948125422, Final Batch Loss: 0.0039192261174321175\n",
      "Epoch 4502, Loss: 0.12691698409616947, Final Batch Loss: 0.05818183347582817\n",
      "Epoch 4503, Loss: 0.12394825182855129, Final Batch Loss: 0.01075361855328083\n",
      "Epoch 4504, Loss: 0.06542534567415714, Final Batch Loss: 0.011700326576828957\n",
      "Epoch 4505, Loss: 0.07882472965866327, Final Batch Loss: 0.02696279063820839\n",
      "Epoch 4506, Loss: 0.07964430563151836, Final Batch Loss: 0.02489287778735161\n",
      "Epoch 4507, Loss: 0.07703577540814877, Final Batch Loss: 0.004987964406609535\n",
      "Epoch 4508, Loss: 0.05518117197789252, Final Batch Loss: 0.00304011651314795\n",
      "Epoch 4509, Loss: 0.14987817034125328, Final Batch Loss: 0.0742468535900116\n",
      "Epoch 4510, Loss: 0.07008362095803022, Final Batch Loss: 0.013272023759782314\n",
      "Epoch 4511, Loss: 0.0761971864849329, Final Batch Loss: 0.012220047414302826\n",
      "Epoch 4512, Loss: 0.05571848014369607, Final Batch Loss: 0.007151613477617502\n",
      "Epoch 4513, Loss: 0.21073775179684162, Final Batch Loss: 0.06129084527492523\n",
      "Epoch 4514, Loss: 0.20844305492937565, Final Batch Loss: 0.15782320499420166\n",
      "Epoch 4515, Loss: 0.06250432506203651, Final Batch Loss: 0.0027596335858106613\n",
      "Epoch 4516, Loss: 0.14930159226059914, Final Batch Loss: 0.043156299740076065\n",
      "Epoch 4517, Loss: 0.08556302823126316, Final Batch Loss: 0.012139128521084785\n",
      "Epoch 4518, Loss: 0.11625615879893303, Final Batch Loss: 0.019414301961660385\n",
      "Epoch 4519, Loss: 0.1097096661105752, Final Batch Loss: 0.06458792835474014\n",
      "Epoch 4520, Loss: 0.22393249347805977, Final Batch Loss: 0.13135214149951935\n",
      "Epoch 4521, Loss: 0.12323904596269131, Final Batch Loss: 0.030603451654314995\n",
      "Epoch 4522, Loss: 0.31041072122752666, Final Batch Loss: 0.20859947800636292\n",
      "Epoch 4523, Loss: 0.15036389604210854, Final Batch Loss: 0.0448995903134346\n",
      "Epoch 4524, Loss: 0.10751509107649326, Final Batch Loss: 0.027586406096816063\n",
      "Epoch 4525, Loss: 0.2303604856133461, Final Batch Loss: 0.12661176919937134\n",
      "Epoch 4526, Loss: 0.16224273294210434, Final Batch Loss: 0.04711819812655449\n",
      "Epoch 4527, Loss: 0.17251431196928024, Final Batch Loss: 0.09953487664461136\n",
      "Epoch 4528, Loss: 0.10471238940954208, Final Batch Loss: 0.016951467841863632\n",
      "Epoch 4529, Loss: 0.26306405290961266, Final Batch Loss: 0.1517074555158615\n",
      "Epoch 4530, Loss: 0.15921652875840664, Final Batch Loss: 0.10613468289375305\n",
      "Epoch 4531, Loss: 0.12875543907284737, Final Batch Loss: 0.026072971522808075\n",
      "Epoch 4532, Loss: 0.12965748459100723, Final Batch Loss: 0.019191712141036987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4533, Loss: 0.10954490210860968, Final Batch Loss: 0.008570275269448757\n",
      "Epoch 4534, Loss: 0.07105005159974098, Final Batch Loss: 0.0317276306450367\n",
      "Epoch 4535, Loss: 0.09484140574932098, Final Batch Loss: 0.03576917201280594\n",
      "Epoch 4536, Loss: 0.12065913900732994, Final Batch Loss: 0.014945223927497864\n",
      "Epoch 4537, Loss: 0.2561112307012081, Final Batch Loss: 0.16741780936717987\n",
      "Epoch 4538, Loss: 0.1245235875248909, Final Batch Loss: 0.06045191362500191\n",
      "Epoch 4539, Loss: 0.08757234830409288, Final Batch Loss: 0.01010905671864748\n",
      "Epoch 4540, Loss: 0.04938191547989845, Final Batch Loss: 0.01564963534474373\n",
      "Epoch 4541, Loss: 0.22916167601943016, Final Batch Loss: 0.14223220944404602\n",
      "Epoch 4542, Loss: 0.11220552306622267, Final Batch Loss: 0.009413447231054306\n",
      "Epoch 4543, Loss: 0.10751988738775253, Final Batch Loss: 0.01942349597811699\n",
      "Epoch 4544, Loss: 0.13146967813372612, Final Batch Loss: 0.050106458365917206\n",
      "Epoch 4545, Loss: 0.09446103870868683, Final Batch Loss: 0.027177520096302032\n",
      "Epoch 4546, Loss: 0.18610451184213161, Final Batch Loss: 0.07846996188163757\n",
      "Epoch 4547, Loss: 0.17940634861588478, Final Batch Loss: 0.07569646090269089\n",
      "Epoch 4548, Loss: 0.10392669215798378, Final Batch Loss: 0.045902326703071594\n",
      "Epoch 4549, Loss: 0.10433765314519405, Final Batch Loss: 0.043532539159059525\n",
      "Epoch 4550, Loss: 0.09551689866930246, Final Batch Loss: 0.015425545163452625\n",
      "Epoch 4551, Loss: 0.09671473875641823, Final Batch Loss: 0.040450844913721085\n",
      "Epoch 4552, Loss: 0.06307063810527325, Final Batch Loss: 0.01324901171028614\n",
      "Epoch 4553, Loss: 0.21424926817417145, Final Batch Loss: 0.15136368572711945\n",
      "Epoch 4554, Loss: 0.16743102297186852, Final Batch Loss: 0.019986603409051895\n",
      "Epoch 4555, Loss: 0.3372103637084365, Final Batch Loss: 0.009122847579419613\n",
      "Epoch 4556, Loss: 0.1890351139008999, Final Batch Loss: 0.03486413508653641\n",
      "Epoch 4557, Loss: 0.15684160217642784, Final Batch Loss: 0.035943713039159775\n",
      "Epoch 4558, Loss: 0.1874178722500801, Final Batch Loss: 0.05715953931212425\n",
      "Epoch 4559, Loss: 0.12055308185517788, Final Batch Loss: 0.02624400146305561\n",
      "Epoch 4560, Loss: 0.17203234136104584, Final Batch Loss: 0.0388694629073143\n",
      "Epoch 4561, Loss: 0.1375925000756979, Final Batch Loss: 0.017520790919661522\n",
      "Epoch 4562, Loss: 0.22190554067492485, Final Batch Loss: 0.12837378680706024\n",
      "Epoch 4563, Loss: 0.11642247438430786, Final Batch Loss: 0.04244920611381531\n",
      "Epoch 4564, Loss: 0.07815112173557281, Final Batch Loss: 0.019206706434488297\n",
      "Epoch 4565, Loss: 0.15680725872516632, Final Batch Loss: 0.05010766535997391\n",
      "Epoch 4566, Loss: 0.11378785036504269, Final Batch Loss: 0.05188579112291336\n",
      "Epoch 4567, Loss: 0.08744164276868105, Final Batch Loss: 0.00942834373563528\n",
      "Epoch 4568, Loss: 0.09333986602723598, Final Batch Loss: 0.008604710921645164\n",
      "Epoch 4569, Loss: 0.09905796684324741, Final Batch Loss: 0.04229183867573738\n",
      "Epoch 4570, Loss: 0.22116154059767723, Final Batch Loss: 0.1046396866440773\n",
      "Epoch 4571, Loss: 0.08870839513838291, Final Batch Loss: 0.018912097439169884\n",
      "Epoch 4572, Loss: 0.08603142201900482, Final Batch Loss: 0.020120898261666298\n",
      "Epoch 4573, Loss: 0.11319360323250294, Final Batch Loss: 0.03418540954589844\n",
      "Epoch 4574, Loss: 0.08723416784778237, Final Batch Loss: 0.005724480841308832\n",
      "Epoch 4575, Loss: 0.09245732054114342, Final Batch Loss: 0.01310315914452076\n",
      "Epoch 4576, Loss: 0.184156296774745, Final Batch Loss: 0.11240608245134354\n",
      "Epoch 4577, Loss: 0.16448793560266495, Final Batch Loss: 0.08974548429250717\n",
      "Epoch 4578, Loss: 0.31227903068065643, Final Batch Loss: 0.2114797830581665\n",
      "Epoch 4579, Loss: 0.1008628320414573, Final Batch Loss: 0.003096354426816106\n",
      "Epoch 4580, Loss: 0.1661697719246149, Final Batch Loss: 0.03224802017211914\n",
      "Epoch 4581, Loss: 0.13405394554138184, Final Batch Loss: 0.04730439558625221\n",
      "Epoch 4582, Loss: 0.08997776452451944, Final Batch Loss: 0.006917349062860012\n",
      "Epoch 4583, Loss: 0.16753565706312656, Final Batch Loss: 0.1154070720076561\n",
      "Epoch 4584, Loss: 0.08033985458314419, Final Batch Loss: 0.008535472676157951\n",
      "Epoch 4585, Loss: 0.17450638487935066, Final Batch Loss: 0.023309048265218735\n",
      "Epoch 4586, Loss: 0.09104737266898155, Final Batch Loss: 0.03796520456671715\n",
      "Epoch 4587, Loss: 0.11034127231687307, Final Batch Loss: 0.0067544421181082726\n",
      "Epoch 4588, Loss: 0.12461094930768013, Final Batch Loss: 0.04169831424951553\n",
      "Epoch 4589, Loss: 0.06700360868126154, Final Batch Loss: 0.007468598894774914\n",
      "Epoch 4590, Loss: 0.19675235822796822, Final Batch Loss: 0.1205226480960846\n",
      "Epoch 4591, Loss: 0.12861644476652145, Final Batch Loss: 0.015901703387498856\n",
      "Epoch 4592, Loss: 0.22540894523262978, Final Batch Loss: 0.1803688108921051\n",
      "Epoch 4593, Loss: 0.10114015638828278, Final Batch Loss: 0.022464532405138016\n",
      "Epoch 4594, Loss: 0.12428314611315727, Final Batch Loss: 0.04915183037519455\n",
      "Epoch 4595, Loss: 0.1435372233390808, Final Batch Loss: 0.0597824826836586\n",
      "Epoch 4596, Loss: 0.06408463045954704, Final Batch Loss: 0.01049206405878067\n",
      "Epoch 4597, Loss: 0.15422166883945465, Final Batch Loss: 0.06869344413280487\n",
      "Epoch 4598, Loss: 0.1156271742656827, Final Batch Loss: 0.01502369437366724\n",
      "Epoch 4599, Loss: 0.12210256233811378, Final Batch Loss: 0.02439464069902897\n",
      "Epoch 4600, Loss: 0.27267681062221527, Final Batch Loss: 0.10490807145833969\n",
      "Epoch 4601, Loss: 0.31353330612182617, Final Batch Loss: 0.23123976588249207\n",
      "Epoch 4602, Loss: 0.07213307823985815, Final Batch Loss: 0.004570300690829754\n",
      "Epoch 4603, Loss: 0.0663915229961276, Final Batch Loss: 0.006309204734861851\n",
      "Epoch 4604, Loss: 0.0754255261272192, Final Batch Loss: 0.014282938092947006\n",
      "Epoch 4605, Loss: 0.12150165252387524, Final Batch Loss: 0.028063280507922173\n",
      "Epoch 4606, Loss: 0.09858074132353067, Final Batch Loss: 0.012022466398775578\n",
      "Epoch 4607, Loss: 0.09636214189231396, Final Batch Loss: 0.020322095602750778\n",
      "Epoch 4608, Loss: 0.0844515198841691, Final Batch Loss: 0.005456690676510334\n",
      "Epoch 4609, Loss: 0.15380388498306274, Final Batch Loss: 0.10525553673505783\n",
      "Epoch 4610, Loss: 0.13828101567924023, Final Batch Loss: 0.01260986365377903\n",
      "Epoch 4611, Loss: 0.13594479486346245, Final Batch Loss: 0.01858251914381981\n",
      "Epoch 4612, Loss: 0.162962194532156, Final Batch Loss: 0.05060373991727829\n",
      "Epoch 4613, Loss: 0.22396274656057358, Final Batch Loss: 0.06866747885942459\n",
      "Epoch 4614, Loss: 0.11087484285235405, Final Batch Loss: 0.042153969407081604\n",
      "Epoch 4615, Loss: 0.10758372861891985, Final Batch Loss: 0.015455258078873158\n",
      "Epoch 4616, Loss: 0.1264809798449278, Final Batch Loss: 0.021109243854880333\n",
      "Epoch 4617, Loss: 0.16276690177619457, Final Batch Loss: 0.02465597353875637\n",
      "Epoch 4618, Loss: 0.30061687901616096, Final Batch Loss: 0.1961703896522522\n",
      "Epoch 4619, Loss: 0.15401266515254974, Final Batch Loss: 0.053376197814941406\n",
      "Epoch 4620, Loss: 0.07472437154501677, Final Batch Loss: 0.015491371043026447\n",
      "Epoch 4621, Loss: 0.11313911527395248, Final Batch Loss: 0.03650335222482681\n",
      "Epoch 4622, Loss: 0.1404847726225853, Final Batch Loss: 0.03220178186893463\n",
      "Epoch 4623, Loss: 0.13458659872412682, Final Batch Loss: 0.0565635971724987\n",
      "Epoch 4624, Loss: 0.28739673271775246, Final Batch Loss: 0.19099749624729156\n",
      "Epoch 4625, Loss: 0.18654442951083183, Final Batch Loss: 0.06551416963338852\n",
      "Epoch 4626, Loss: 0.12055440619587898, Final Batch Loss: 0.031814344227313995\n",
      "Epoch 4627, Loss: 0.0885008848272264, Final Batch Loss: 0.005583654623478651\n",
      "Epoch 4628, Loss: 0.24262015148997307, Final Batch Loss: 0.13416321575641632\n",
      "Epoch 4629, Loss: 0.1516937054693699, Final Batch Loss: 0.028855368494987488\n",
      "Epoch 4630, Loss: 0.12071815319359303, Final Batch Loss: 0.017748797312378883\n",
      "Epoch 4631, Loss: 0.22512494400143623, Final Batch Loss: 0.04778082296252251\n",
      "Epoch 4632, Loss: 0.15434209629893303, Final Batch Loss: 0.03948966786265373\n",
      "Epoch 4633, Loss: 0.19219433888792992, Final Batch Loss: 0.03017279878258705\n",
      "Epoch 4634, Loss: 0.1229543131776154, Final Batch Loss: 0.007252753246575594\n",
      "Epoch 4635, Loss: 0.09465248649939895, Final Batch Loss: 0.006926884409040213\n",
      "Epoch 4636, Loss: 0.12315505091100931, Final Batch Loss: 0.008191042579710484\n",
      "Epoch 4637, Loss: 0.11179220769554377, Final Batch Loss: 0.0099656255915761\n",
      "Epoch 4638, Loss: 0.08878329396247864, Final Batch Loss: 0.04023171588778496\n",
      "Epoch 4639, Loss: 0.05548224365338683, Final Batch Loss: 0.0053864712826907635\n",
      "Epoch 4640, Loss: 0.19766896218061447, Final Batch Loss: 0.049493126571178436\n",
      "Epoch 4641, Loss: 0.1341775651089847, Final Batch Loss: 0.0059751044027507305\n",
      "Epoch 4642, Loss: 0.1625225879251957, Final Batch Loss: 0.049028005450963974\n",
      "Epoch 4643, Loss: 0.04738685814663768, Final Batch Loss: 0.004995378199964762\n",
      "Epoch 4644, Loss: 0.0909626130014658, Final Batch Loss: 0.04330570995807648\n",
      "Epoch 4645, Loss: 0.09869218617677689, Final Batch Loss: 0.05228208005428314\n",
      "Epoch 4646, Loss: 0.11531450971961021, Final Batch Loss: 0.02743713930249214\n",
      "Epoch 4647, Loss: 0.1322483979165554, Final Batch Loss: 0.06209837272763252\n",
      "Epoch 4648, Loss: 0.13529806025326252, Final Batch Loss: 0.05611005797982216\n",
      "Epoch 4649, Loss: 0.1107924859970808, Final Batch Loss: 0.012731188908219337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4650, Loss: 0.09959514997899532, Final Batch Loss: 0.01897544227540493\n",
      "Epoch 4651, Loss: 0.10388157516717911, Final Batch Loss: 0.03673877939581871\n",
      "Epoch 4652, Loss: 0.09351854398846626, Final Batch Loss: 0.019908152520656586\n",
      "Epoch 4653, Loss: 0.09083060873672366, Final Batch Loss: 0.007632391061633825\n",
      "Epoch 4654, Loss: 0.07820331305265427, Final Batch Loss: 0.021640794351696968\n",
      "Epoch 4655, Loss: 0.12105175666511059, Final Batch Loss: 0.030274370685219765\n",
      "Epoch 4656, Loss: 0.08229839382693172, Final Batch Loss: 0.005178756546229124\n",
      "Epoch 4657, Loss: 0.11421946249902248, Final Batch Loss: 0.034535329788923264\n",
      "Epoch 4658, Loss: 0.18342971056699753, Final Batch Loss: 0.09653130918741226\n",
      "Epoch 4659, Loss: 0.16132361069321632, Final Batch Loss: 0.1115938350558281\n",
      "Epoch 4660, Loss: 0.14681394584476948, Final Batch Loss: 0.0838630273938179\n",
      "Epoch 4661, Loss: 0.12693798914551735, Final Batch Loss: 0.022323068231344223\n",
      "Epoch 4662, Loss: 0.12306972593069077, Final Batch Loss: 0.04539807513356209\n",
      "Epoch 4663, Loss: 0.1131218820810318, Final Batch Loss: 0.054363418370485306\n",
      "Epoch 4664, Loss: 0.0883677490055561, Final Batch Loss: 0.010348793119192123\n",
      "Epoch 4665, Loss: 0.1485326550900936, Final Batch Loss: 0.09582137316465378\n",
      "Epoch 4666, Loss: 0.07160902116447687, Final Batch Loss: 0.009531793184578419\n",
      "Epoch 4667, Loss: 0.061865911819040775, Final Batch Loss: 0.01630689762532711\n",
      "Epoch 4668, Loss: 0.10146244801580906, Final Batch Loss: 0.008915042504668236\n",
      "Epoch 4669, Loss: 0.09994826838374138, Final Batch Loss: 0.052926890552043915\n",
      "Epoch 4670, Loss: 0.05945838941261172, Final Batch Loss: 0.007044794503599405\n",
      "Epoch 4671, Loss: 0.06366517767310143, Final Batch Loss: 0.01902739703655243\n",
      "Epoch 4672, Loss: 0.07391731068491936, Final Batch Loss: 0.025605451315641403\n",
      "Epoch 4673, Loss: 0.06834164075553417, Final Batch Loss: 0.03158832713961601\n",
      "Epoch 4674, Loss: 0.0577514823526144, Final Batch Loss: 0.0082173440605402\n",
      "Epoch 4675, Loss: 0.050348808988928795, Final Batch Loss: 0.007151361554861069\n",
      "Epoch 4676, Loss: 0.0788978012278676, Final Batch Loss: 0.033137328922748566\n",
      "Epoch 4677, Loss: 0.09366302564740181, Final Batch Loss: 0.033917222172021866\n",
      "Epoch 4678, Loss: 0.16491231881082058, Final Batch Loss: 0.11017076671123505\n",
      "Epoch 4679, Loss: 0.09187646023929119, Final Batch Loss: 0.02321784943342209\n",
      "Epoch 4680, Loss: 0.08598294109106064, Final Batch Loss: 0.04350590705871582\n",
      "Epoch 4681, Loss: 0.0744721507653594, Final Batch Loss: 0.03403814136981964\n",
      "Epoch 4682, Loss: 0.14068834483623505, Final Batch Loss: 0.03587915003299713\n",
      "Epoch 4683, Loss: 0.133362028747797, Final Batch Loss: 0.04200064763426781\n",
      "Epoch 4684, Loss: 0.08751614764332771, Final Batch Loss: 0.026634210720658302\n",
      "Epoch 4685, Loss: 0.10794439539313316, Final Batch Loss: 0.06388087570667267\n",
      "Epoch 4686, Loss: 0.06931938696652651, Final Batch Loss: 0.010193848051130772\n",
      "Epoch 4687, Loss: 0.15907876193523407, Final Batch Loss: 0.04889693111181259\n",
      "Epoch 4688, Loss: 0.13129456155002117, Final Batch Loss: 0.06731941550970078\n",
      "Epoch 4689, Loss: 0.08585173636674881, Final Batch Loss: 0.018908383324742317\n",
      "Epoch 4690, Loss: 0.10205290466547012, Final Batch Loss: 0.021595966070890427\n",
      "Epoch 4691, Loss: 0.12593562435358763, Final Batch Loss: 0.004260505549609661\n",
      "Epoch 4692, Loss: 0.2533777132630348, Final Batch Loss: 0.10475218296051025\n",
      "Epoch 4693, Loss: 0.16591577790677547, Final Batch Loss: 0.10832633078098297\n",
      "Epoch 4694, Loss: 0.25716960802674294, Final Batch Loss: 0.10858747363090515\n",
      "Epoch 4695, Loss: 0.10622235760092735, Final Batch Loss: 0.010327417403459549\n",
      "Epoch 4696, Loss: 0.10460735764354467, Final Batch Loss: 0.0050187138840556145\n",
      "Epoch 4697, Loss: 0.1917954832315445, Final Batch Loss: 0.11148156225681305\n",
      "Epoch 4698, Loss: 0.09981582127511501, Final Batch Loss: 0.0096061360090971\n",
      "Epoch 4699, Loss: 0.10228827223181725, Final Batch Loss: 0.029393091797828674\n",
      "Epoch 4700, Loss: 0.19964720401912928, Final Batch Loss: 0.1600881665945053\n",
      "Epoch 4701, Loss: 0.12611206620931625, Final Batch Loss: 0.009947661310434341\n",
      "Epoch 4702, Loss: 0.23161274939775467, Final Batch Loss: 0.06684117019176483\n",
      "Epoch 4703, Loss: 0.1386087890714407, Final Batch Loss: 0.013263063505291939\n",
      "Epoch 4704, Loss: 0.16174077056348324, Final Batch Loss: 0.030340513214468956\n",
      "Epoch 4705, Loss: 0.08333596214652061, Final Batch Loss: 0.021154440939426422\n",
      "Epoch 4706, Loss: 0.07588935224339366, Final Batch Loss: 0.005442179273813963\n",
      "Epoch 4707, Loss: 0.20669355615973473, Final Batch Loss: 0.11870303004980087\n",
      "Epoch 4708, Loss: 0.16132821515202522, Final Batch Loss: 0.08660339564085007\n",
      "Epoch 4709, Loss: 0.10211501363664865, Final Batch Loss: 0.003697180189192295\n",
      "Epoch 4710, Loss: 0.11874532513320446, Final Batch Loss: 0.019692620262503624\n",
      "Epoch 4711, Loss: 0.08163960743695498, Final Batch Loss: 0.02283722348511219\n",
      "Epoch 4712, Loss: 0.07465964183211327, Final Batch Loss: 0.012269904837012291\n",
      "Epoch 4713, Loss: 0.22694561630487442, Final Batch Loss: 0.11129067093133926\n",
      "Epoch 4714, Loss: 0.1291920430958271, Final Batch Loss: 0.05231492593884468\n",
      "Epoch 4715, Loss: 0.22181355953216553, Final Batch Loss: 0.10168831050395966\n",
      "Epoch 4716, Loss: 0.08736251294612885, Final Batch Loss: 0.015309594571590424\n",
      "Epoch 4717, Loss: 0.3855890668928623, Final Batch Loss: 0.24290043115615845\n",
      "Epoch 4718, Loss: 0.15893260017037392, Final Batch Loss: 0.05435321852564812\n",
      "Epoch 4719, Loss: 0.2347605675458908, Final Batch Loss: 0.13536949455738068\n",
      "Epoch 4720, Loss: 0.15952181816101074, Final Batch Loss: 0.023966018110513687\n",
      "Epoch 4721, Loss: 0.11024115420877934, Final Batch Loss: 0.028292598202824593\n",
      "Epoch 4722, Loss: 0.08386941161006689, Final Batch Loss: 0.011407439596951008\n",
      "Epoch 4723, Loss: 0.06872650515288115, Final Batch Loss: 0.008266265504062176\n",
      "Epoch 4724, Loss: 0.0883986297994852, Final Batch Loss: 0.018852513283491135\n",
      "Epoch 4725, Loss: 0.10435376688838005, Final Batch Loss: 0.03522452339529991\n",
      "Epoch 4726, Loss: 0.16241204924881458, Final Batch Loss: 0.10460016131401062\n",
      "Epoch 4727, Loss: 0.1114053213968873, Final Batch Loss: 0.013501911424100399\n",
      "Epoch 4728, Loss: 0.23910045251250267, Final Batch Loss: 0.13367199897766113\n",
      "Epoch 4729, Loss: 0.18393987230956554, Final Batch Loss: 0.007326675578951836\n",
      "Epoch 4730, Loss: 0.392782773822546, Final Batch Loss: 0.22706075012683868\n",
      "Epoch 4731, Loss: 0.15344059467315674, Final Batch Loss: 0.01961854100227356\n",
      "Epoch 4732, Loss: 0.22642923705279827, Final Batch Loss: 0.14995594322681427\n",
      "Epoch 4733, Loss: 0.1371107380837202, Final Batch Loss: 0.08007659018039703\n",
      "Epoch 4734, Loss: 0.17763595283031464, Final Batch Loss: 0.011744670569896698\n",
      "Epoch 4735, Loss: 0.20001951977610588, Final Batch Loss: 0.036500003188848495\n",
      "Epoch 4736, Loss: 0.12461503595113754, Final Batch Loss: 0.020360983908176422\n",
      "Epoch 4737, Loss: 0.834819070994854, Final Batch Loss: 0.6428006291389465\n",
      "Epoch 4738, Loss: 0.1367843635380268, Final Batch Loss: 0.03037548065185547\n",
      "Epoch 4739, Loss: 0.4501991905272007, Final Batch Loss: 0.37676575779914856\n",
      "Epoch 4740, Loss: 0.1589680276811123, Final Batch Loss: 0.020936384797096252\n",
      "Epoch 4741, Loss: 0.2138882800936699, Final Batch Loss: 0.06198611855506897\n",
      "Epoch 4742, Loss: 0.23265598341822624, Final Batch Loss: 0.03309964761137962\n",
      "Epoch 4743, Loss: 0.33127231150865555, Final Batch Loss: 0.1779959499835968\n",
      "Epoch 4744, Loss: 0.17617569118738174, Final Batch Loss: 0.05561191961169243\n",
      "Epoch 4745, Loss: 0.18324338644742966, Final Batch Loss: 0.08244575560092926\n",
      "Epoch 4746, Loss: 0.16425933130085468, Final Batch Loss: 0.05720296874642372\n",
      "Epoch 4747, Loss: 0.14668406080454588, Final Batch Loss: 0.010381459258496761\n",
      "Epoch 4748, Loss: 0.1967979185283184, Final Batch Loss: 0.10005169361829758\n",
      "Epoch 4749, Loss: 0.11644581146538258, Final Batch Loss: 0.022588534280657768\n",
      "Epoch 4750, Loss: 0.13871260732412338, Final Batch Loss: 0.019346069544553757\n",
      "Epoch 4751, Loss: 0.15989181771874428, Final Batch Loss: 0.05470825731754303\n",
      "Epoch 4752, Loss: 0.2099953517317772, Final Batch Loss: 0.09135407209396362\n",
      "Epoch 4753, Loss: 0.25264066085219383, Final Batch Loss: 0.12817253172397614\n",
      "Epoch 4754, Loss: 0.1553452704101801, Final Batch Loss: 0.0641946792602539\n",
      "Epoch 4755, Loss: 0.30475443601608276, Final Batch Loss: 0.1289457082748413\n",
      "Epoch 4756, Loss: 0.19695528596639633, Final Batch Loss: 0.06859753280878067\n",
      "Epoch 4757, Loss: 0.16417278815060854, Final Batch Loss: 0.01418313104659319\n",
      "Epoch 4758, Loss: 0.08710608445107937, Final Batch Loss: 0.011528437957167625\n",
      "Epoch 4759, Loss: 0.19085760787129402, Final Batch Loss: 0.023288175463676453\n",
      "Epoch 4760, Loss: 0.18623444065451622, Final Batch Loss: 0.06220962479710579\n",
      "Epoch 4761, Loss: 0.2376367151737213, Final Batch Loss: 0.14413057267665863\n",
      "Epoch 4762, Loss: 0.10217046225443482, Final Batch Loss: 0.007520389277487993\n",
      "Epoch 4763, Loss: 0.12823904305696487, Final Batch Loss: 0.03284582495689392\n",
      "Epoch 4764, Loss: 0.11931943520903587, Final Batch Loss: 0.0188412144780159\n",
      "Epoch 4765, Loss: 0.10358637385070324, Final Batch Loss: 0.05884839594364166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4766, Loss: 0.07940975949168205, Final Batch Loss: 0.0075972676277160645\n",
      "Epoch 4767, Loss: 0.11794987320899963, Final Batch Loss: 0.047170836478471756\n",
      "Epoch 4768, Loss: 0.16224193200469017, Final Batch Loss: 0.05473565682768822\n",
      "Epoch 4769, Loss: 0.059819936752319336, Final Batch Loss: 0.012741722166538239\n",
      "Epoch 4770, Loss: 0.17323077842593193, Final Batch Loss: 0.017502274364233017\n",
      "Epoch 4771, Loss: 0.13359717652201653, Final Batch Loss: 0.06021573394536972\n",
      "Epoch 4772, Loss: 0.12982672452926636, Final Batch Loss: 0.07628525793552399\n",
      "Epoch 4773, Loss: 0.1619954090565443, Final Batch Loss: 0.10790449380874634\n",
      "Epoch 4774, Loss: 0.08512704074382782, Final Batch Loss: 0.0466393381357193\n",
      "Epoch 4775, Loss: 0.17310315184295177, Final Batch Loss: 0.049114130437374115\n",
      "Epoch 4776, Loss: 0.11314835958182812, Final Batch Loss: 0.00973672978579998\n",
      "Epoch 4777, Loss: 0.1256401026621461, Final Batch Loss: 0.008290995843708515\n",
      "Epoch 4778, Loss: 0.07649495080113411, Final Batch Loss: 0.009604530408978462\n",
      "Epoch 4779, Loss: 0.09465604368597269, Final Batch Loss: 0.01028516236692667\n",
      "Epoch 4780, Loss: 0.09671662840992212, Final Batch Loss: 0.02125634253025055\n",
      "Epoch 4781, Loss: 0.42984987795352936, Final Batch Loss: 0.33427104353904724\n",
      "Epoch 4782, Loss: 0.06980135198682547, Final Batch Loss: 0.012462557293474674\n",
      "Epoch 4783, Loss: 0.13208393566310406, Final Batch Loss: 0.05474837124347687\n",
      "Epoch 4784, Loss: 0.07539141923189163, Final Batch Loss: 0.023647498339414597\n",
      "Epoch 4785, Loss: 0.1338766124099493, Final Batch Loss: 0.0732680931687355\n",
      "Epoch 4786, Loss: 0.0617474103346467, Final Batch Loss: 0.01366749033331871\n",
      "Epoch 4787, Loss: 0.07119174022227526, Final Batch Loss: 0.02324874885380268\n",
      "Epoch 4788, Loss: 0.08405930735170841, Final Batch Loss: 0.017657535150647163\n",
      "Epoch 4789, Loss: 0.055366589687764645, Final Batch Loss: 0.011350302956998348\n",
      "Epoch 4790, Loss: 0.07179861702024937, Final Batch Loss: 0.022590195760130882\n",
      "Epoch 4791, Loss: 0.08956320211291313, Final Batch Loss: 0.01854594424366951\n",
      "Epoch 4792, Loss: 0.15164967626333237, Final Batch Loss: 0.05572836101055145\n",
      "Epoch 4793, Loss: 0.07854345999658108, Final Batch Loss: 0.035944219678640366\n",
      "Epoch 4794, Loss: 0.07355594635009766, Final Batch Loss: 0.019218409433960915\n",
      "Epoch 4795, Loss: 0.13068327400833368, Final Batch Loss: 0.07910311222076416\n",
      "Epoch 4796, Loss: 0.12057109363377094, Final Batch Loss: 0.02450103871524334\n",
      "Epoch 4797, Loss: 0.1774232964962721, Final Batch Loss: 0.029987195506691933\n",
      "Epoch 4798, Loss: 0.11553903575986624, Final Batch Loss: 0.01453464012593031\n",
      "Epoch 4799, Loss: 0.10103910602629185, Final Batch Loss: 0.024243099614977837\n",
      "Epoch 4800, Loss: 0.1541192475706339, Final Batch Loss: 0.017450271174311638\n",
      "Epoch 4801, Loss: 0.07224497594870627, Final Batch Loss: 0.0022340419236570597\n",
      "Epoch 4802, Loss: 0.10134307108819485, Final Batch Loss: 0.057129278779029846\n",
      "Epoch 4803, Loss: 0.09346340224146843, Final Batch Loss: 0.024046320468187332\n",
      "Epoch 4804, Loss: 0.10158700123429298, Final Batch Loss: 0.05595996975898743\n",
      "Epoch 4805, Loss: 0.10469618812203407, Final Batch Loss: 0.039976462721824646\n",
      "Epoch 4806, Loss: 0.18921970576047897, Final Batch Loss: 0.06486218422651291\n",
      "Epoch 4807, Loss: 0.11966284457594156, Final Batch Loss: 0.00876274798065424\n",
      "Epoch 4808, Loss: 0.14222079096361995, Final Batch Loss: 0.0035350206308066845\n",
      "Epoch 4809, Loss: 0.11871145479381084, Final Batch Loss: 0.024203510954976082\n",
      "Epoch 4810, Loss: 0.12136957421898842, Final Batch Loss: 0.03215176612138748\n",
      "Epoch 4811, Loss: 0.12336240708827972, Final Batch Loss: 0.032060690224170685\n",
      "Epoch 4812, Loss: 0.21740972623229027, Final Batch Loss: 0.07235419750213623\n",
      "Epoch 4813, Loss: 0.11808546399697661, Final Batch Loss: 0.006524705793708563\n",
      "Epoch 4814, Loss: 0.1406619492918253, Final Batch Loss: 0.08031260967254639\n",
      "Epoch 4815, Loss: 0.1372626330703497, Final Batch Loss: 0.024059223011136055\n",
      "Epoch 4816, Loss: 0.14314104616641998, Final Batch Loss: 0.05428752303123474\n",
      "Epoch 4817, Loss: 0.22443585097789764, Final Batch Loss: 0.11716563254594803\n",
      "Epoch 4818, Loss: 0.2048255242407322, Final Batch Loss: 0.09730545431375504\n",
      "Epoch 4819, Loss: 0.2838159240782261, Final Batch Loss: 0.17406490445137024\n",
      "Epoch 4820, Loss: 0.21983933448791504, Final Batch Loss: 0.03984420746564865\n",
      "Epoch 4821, Loss: 0.10600222833454609, Final Batch Loss: 0.007504871115088463\n",
      "Epoch 4822, Loss: 0.18533311411738396, Final Batch Loss: 0.08383884280920029\n",
      "Epoch 4823, Loss: 0.13550077006220818, Final Batch Loss: 0.013122417032718658\n",
      "Epoch 4824, Loss: 0.12338196858763695, Final Batch Loss: 0.014189036563038826\n",
      "Epoch 4825, Loss: 0.08554334845393896, Final Batch Loss: 0.010087392292916775\n",
      "Epoch 4826, Loss: 0.09209014289081097, Final Batch Loss: 0.04059797525405884\n",
      "Epoch 4827, Loss: 0.09255503863096237, Final Batch Loss: 0.012823531404137611\n",
      "Epoch 4828, Loss: 0.0912912655621767, Final Batch Loss: 0.026961341500282288\n",
      "Epoch 4829, Loss: 0.08283847756683826, Final Batch Loss: 0.016144242137670517\n",
      "Epoch 4830, Loss: 0.11920392885804176, Final Batch Loss: 0.05784768983721733\n",
      "Epoch 4831, Loss: 0.1453334167599678, Final Batch Loss: 0.09722228348255157\n",
      "Epoch 4832, Loss: 0.10485939122736454, Final Batch Loss: 0.046625856310129166\n",
      "Epoch 4833, Loss: 0.12044405424967408, Final Batch Loss: 0.006588163319975138\n",
      "Epoch 4834, Loss: 0.18810540763661265, Final Batch Loss: 0.006454552989453077\n",
      "Epoch 4835, Loss: 0.12941678427159786, Final Batch Loss: 0.023944607004523277\n",
      "Epoch 4836, Loss: 0.19119993224740028, Final Batch Loss: 0.09299060702323914\n",
      "Epoch 4837, Loss: 0.2890396937727928, Final Batch Loss: 0.1789589822292328\n",
      "Epoch 4838, Loss: 0.15864513162523508, Final Batch Loss: 0.012672939337790012\n",
      "Epoch 4839, Loss: 0.11326421424746513, Final Batch Loss: 0.035761766135692596\n",
      "Epoch 4840, Loss: 0.13260543160140514, Final Batch Loss: 0.06591587513685226\n",
      "Epoch 4841, Loss: 0.15108121000230312, Final Batch Loss: 0.01533483900129795\n",
      "Epoch 4842, Loss: 0.2092226855456829, Final Batch Loss: 0.029638122767210007\n",
      "Epoch 4843, Loss: 0.29766465723514557, Final Batch Loss: 0.1369788646697998\n",
      "Epoch 4844, Loss: 0.12311480473726988, Final Batch Loss: 0.011190827004611492\n",
      "Epoch 4845, Loss: 0.2308432087302208, Final Batch Loss: 0.0712609514594078\n",
      "Epoch 4846, Loss: 0.12634672410786152, Final Batch Loss: 0.023171478882431984\n",
      "Epoch 4847, Loss: 0.13271943107247353, Final Batch Loss: 0.04925041273236275\n",
      "Epoch 4848, Loss: 0.14783897250890732, Final Batch Loss: 0.08451373130083084\n",
      "Epoch 4849, Loss: 0.11402360862120986, Final Batch Loss: 0.007587125059217215\n",
      "Epoch 4850, Loss: 0.09982009930536151, Final Batch Loss: 0.005199290346354246\n",
      "Epoch 4851, Loss: 0.10685188323259354, Final Batch Loss: 0.021380819380283356\n",
      "Epoch 4852, Loss: 0.147584218531847, Final Batch Loss: 0.059856656938791275\n",
      "Epoch 4853, Loss: 0.23214197531342506, Final Batch Loss: 0.16163955628871918\n",
      "Epoch 4854, Loss: 0.07643129304051399, Final Batch Loss: 0.01800013706088066\n",
      "Epoch 4855, Loss: 0.15503054670989513, Final Batch Loss: 0.07562509179115295\n",
      "Epoch 4856, Loss: 0.18942907080054283, Final Batch Loss: 0.10420713573694229\n",
      "Epoch 4857, Loss: 0.16998338885605335, Final Batch Loss: 0.024229304865002632\n",
      "Epoch 4858, Loss: 0.1328069381415844, Final Batch Loss: 0.0525793731212616\n",
      "Epoch 4859, Loss: 0.23713844642043114, Final Batch Loss: 0.035666149109601974\n",
      "Epoch 4860, Loss: 0.1207706406712532, Final Batch Loss: 0.01608600839972496\n",
      "Epoch 4861, Loss: 0.12790829129517078, Final Batch Loss: 0.0311827901750803\n",
      "Epoch 4862, Loss: 0.13030977919697762, Final Batch Loss: 0.03745988756418228\n",
      "Epoch 4863, Loss: 0.15048929676413536, Final Batch Loss: 0.04532766342163086\n",
      "Epoch 4864, Loss: 0.1396595500409603, Final Batch Loss: 0.03710167109966278\n",
      "Epoch 4865, Loss: 0.14490010775625706, Final Batch Loss: 0.06308533251285553\n",
      "Epoch 4866, Loss: 0.06780882552266121, Final Batch Loss: 0.010545382276177406\n",
      "Epoch 4867, Loss: 0.10520150512456894, Final Batch Loss: 0.0612693689763546\n",
      "Epoch 4868, Loss: 0.07353657716885209, Final Batch Loss: 0.004303349647670984\n",
      "Epoch 4869, Loss: 0.10797020699828863, Final Batch Loss: 0.003061125986278057\n",
      "Epoch 4870, Loss: 0.13170432299375534, Final Batch Loss: 0.008567016571760178\n",
      "Epoch 4871, Loss: 0.13042315281927586, Final Batch Loss: 0.04586196318268776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4872, Loss: 0.10458717308938503, Final Batch Loss: 0.035798460245132446\n",
      "Epoch 4873, Loss: 0.10531735979020596, Final Batch Loss: 0.018685806542634964\n",
      "Epoch 4874, Loss: 0.10133611783385277, Final Batch Loss: 0.03173014521598816\n",
      "Epoch 4875, Loss: 0.07269449532032013, Final Batch Loss: 0.0040741488337516785\n",
      "Epoch 4876, Loss: 0.0872178990393877, Final Batch Loss: 0.00956420972943306\n",
      "Epoch 4877, Loss: 0.09249521885067225, Final Batch Loss: 0.012097508646547794\n",
      "Epoch 4878, Loss: 0.16466258093714714, Final Batch Loss: 0.08178495615720749\n",
      "Epoch 4879, Loss: 0.13819731026887894, Final Batch Loss: 0.0760008692741394\n",
      "Epoch 4880, Loss: 0.10094166174530983, Final Batch Loss: 0.008753729984164238\n",
      "Epoch 4881, Loss: 0.18538904190063477, Final Batch Loss: 0.04613592475652695\n",
      "Epoch 4882, Loss: 0.14202013611793518, Final Batch Loss: 0.020144030451774597\n",
      "Epoch 4883, Loss: 0.21263916045427322, Final Batch Loss: 0.09551239758729935\n",
      "Epoch 4884, Loss: 0.18050651997327805, Final Batch Loss: 0.0911419615149498\n",
      "Epoch 4885, Loss: 0.09502830822020769, Final Batch Loss: 0.008200504817068577\n",
      "Epoch 4886, Loss: 0.18919513002038002, Final Batch Loss: 0.021913278847932816\n",
      "Epoch 4887, Loss: 0.1904301904141903, Final Batch Loss: 0.03590204939246178\n",
      "Epoch 4888, Loss: 0.23576714284718037, Final Batch Loss: 0.0806061178445816\n",
      "Epoch 4889, Loss: 0.11228206288069487, Final Batch Loss: 0.013107660226523876\n",
      "Epoch 4890, Loss: 0.28764662332832813, Final Batch Loss: 0.026921967044472694\n",
      "Epoch 4891, Loss: 0.2217053472995758, Final Batch Loss: 0.07092571258544922\n",
      "Epoch 4892, Loss: 0.24616756662726402, Final Batch Loss: 0.11232587695121765\n",
      "Epoch 4893, Loss: 0.1750285066664219, Final Batch Loss: 0.09069979190826416\n",
      "Epoch 4894, Loss: 0.21415941044688225, Final Batch Loss: 0.059243712574243546\n",
      "Epoch 4895, Loss: 0.09664017613977194, Final Batch Loss: 0.014656069688498974\n",
      "Epoch 4896, Loss: 0.13437147438526154, Final Batch Loss: 0.029513392597436905\n",
      "Epoch 4897, Loss: 0.1927376240491867, Final Batch Loss: 0.04291704297065735\n",
      "Epoch 4898, Loss: 0.14844434708356857, Final Batch Loss: 0.04518042504787445\n",
      "Epoch 4899, Loss: 0.12897112220525742, Final Batch Loss: 0.06726866960525513\n",
      "Epoch 4900, Loss: 0.11828514374792576, Final Batch Loss: 0.042437594383955\n",
      "Epoch 4901, Loss: 0.09016034752130508, Final Batch Loss: 0.010668592527508736\n",
      "Epoch 4902, Loss: 0.13811107352375984, Final Batch Loss: 0.05508248507976532\n",
      "Epoch 4903, Loss: 0.0925753996707499, Final Batch Loss: 0.005290715489536524\n",
      "Epoch 4904, Loss: 0.15174649097025394, Final Batch Loss: 0.01772843860089779\n",
      "Epoch 4905, Loss: 0.14086034521460533, Final Batch Loss: 0.0398770309984684\n",
      "Epoch 4906, Loss: 0.13998777605593204, Final Batch Loss: 0.026274075731635094\n",
      "Epoch 4907, Loss: 0.16107746213674545, Final Batch Loss: 0.06903805583715439\n",
      "Epoch 4908, Loss: 0.08366250060498714, Final Batch Loss: 0.03168255835771561\n",
      "Epoch 4909, Loss: 0.08510992303490639, Final Batch Loss: 0.016461383551359177\n",
      "Epoch 4910, Loss: 0.06334086693823338, Final Batch Loss: 0.011127915233373642\n",
      "Epoch 4911, Loss: 0.12339402362704277, Final Batch Loss: 0.04213978722691536\n",
      "Epoch 4912, Loss: 0.05315965507179499, Final Batch Loss: 0.019725337624549866\n",
      "Epoch 4913, Loss: 0.12254391796886921, Final Batch Loss: 0.005689157173037529\n",
      "Epoch 4914, Loss: 0.13667774014174938, Final Batch Loss: 0.0777212381362915\n",
      "Epoch 4915, Loss: 0.08499150536954403, Final Batch Loss: 0.039341576397418976\n",
      "Epoch 4916, Loss: 0.1035833666101098, Final Batch Loss: 0.01180191058665514\n",
      "Epoch 4917, Loss: 0.1694319909438491, Final Batch Loss: 0.014092019759118557\n",
      "Epoch 4918, Loss: 0.12682169303297997, Final Batch Loss: 0.05954338610172272\n",
      "Epoch 4919, Loss: 0.14823749288916588, Final Batch Loss: 0.049342527985572815\n",
      "Epoch 4920, Loss: 0.0976403933018446, Final Batch Loss: 0.023489033803343773\n",
      "Epoch 4921, Loss: 0.0883836317807436, Final Batch Loss: 0.02159442938864231\n",
      "Epoch 4922, Loss: 0.10232199355959892, Final Batch Loss: 0.03287716954946518\n",
      "Epoch 4923, Loss: 0.09463629312813282, Final Batch Loss: 0.017616011202335358\n",
      "Epoch 4924, Loss: 0.1324811987578869, Final Batch Loss: 0.05170075222849846\n",
      "Epoch 4925, Loss: 0.14863250777125359, Final Batch Loss: 0.055493809282779694\n",
      "Epoch 4926, Loss: 0.09833402745425701, Final Batch Loss: 0.009426690638065338\n",
      "Epoch 4927, Loss: 0.29472504183650017, Final Batch Loss: 0.22196564078330994\n",
      "Epoch 4928, Loss: 0.15168046578764915, Final Batch Loss: 0.0537930391728878\n",
      "Epoch 4929, Loss: 0.2031216323375702, Final Batch Loss: 0.08496453613042831\n",
      "Epoch 4930, Loss: 0.14770764485001564, Final Batch Loss: 0.019956499338150024\n",
      "Epoch 4931, Loss: 0.09978712163865566, Final Batch Loss: 0.026693424209952354\n",
      "Epoch 4932, Loss: 0.1625581681728363, Final Batch Loss: 0.04622114449739456\n",
      "Epoch 4933, Loss: 0.2618940435349941, Final Batch Loss: 0.12628574669361115\n",
      "Epoch 4934, Loss: 0.12540965527296066, Final Batch Loss: 0.03460410609841347\n",
      "Epoch 4935, Loss: 0.1881500594317913, Final Batch Loss: 0.06721936166286469\n",
      "Epoch 4936, Loss: 0.1238556019961834, Final Batch Loss: 0.039949823170900345\n",
      "Epoch 4937, Loss: 0.182586083188653, Final Batch Loss: 0.05093614384531975\n",
      "Epoch 4938, Loss: 0.2653983049094677, Final Batch Loss: 0.13151489198207855\n",
      "Epoch 4939, Loss: 0.132074112072587, Final Batch Loss: 0.03448482230305672\n",
      "Epoch 4940, Loss: 0.17618787288665771, Final Batch Loss: 0.010189473628997803\n",
      "Epoch 4941, Loss: 0.20827362313866615, Final Batch Loss: 0.045398417860269547\n",
      "Epoch 4942, Loss: 0.18712466955184937, Final Batch Loss: 0.08558716624975204\n",
      "Epoch 4943, Loss: 0.11339985765516758, Final Batch Loss: 0.032374948263168335\n",
      "Epoch 4944, Loss: 0.1261466247960925, Final Batch Loss: 0.010416715405881405\n",
      "Epoch 4945, Loss: 0.15873911418020725, Final Batch Loss: 0.0201710257679224\n",
      "Epoch 4946, Loss: 0.1900936234742403, Final Batch Loss: 0.09427744895219803\n",
      "Epoch 4947, Loss: 0.13856626860797405, Final Batch Loss: 0.01422998495399952\n",
      "Epoch 4948, Loss: 0.25022439658641815, Final Batch Loss: 0.11859934031963348\n",
      "Epoch 4949, Loss: 0.17232387140393257, Final Batch Loss: 0.01597566530108452\n",
      "Epoch 4950, Loss: 0.1398441046476364, Final Batch Loss: 0.05210641771554947\n",
      "Epoch 4951, Loss: 0.09872108325362206, Final Batch Loss: 0.035639435052871704\n",
      "Epoch 4952, Loss: 0.1626787008717656, Final Batch Loss: 0.009145633317530155\n",
      "Epoch 4953, Loss: 0.17426876723766327, Final Batch Loss: 0.05850255861878395\n",
      "Epoch 4954, Loss: 0.0855381628498435, Final Batch Loss: 0.008203304372727871\n",
      "Epoch 4955, Loss: 0.09930535405874252, Final Batch Loss: 0.01743382215499878\n",
      "Epoch 4956, Loss: 0.17012366466224194, Final Batch Loss: 0.08551396429538727\n",
      "Epoch 4957, Loss: 0.0846079383045435, Final Batch Loss: 0.01642828993499279\n",
      "Epoch 4958, Loss: 0.24530096724629402, Final Batch Loss: 0.14175336062908173\n",
      "Epoch 4959, Loss: 0.07714122720062733, Final Batch Loss: 0.014550454914569855\n",
      "Epoch 4960, Loss: 0.21699927188456059, Final Batch Loss: 0.15524883568286896\n",
      "Epoch 4961, Loss: 0.14626729860901833, Final Batch Loss: 0.05003867298364639\n",
      "Epoch 4962, Loss: 0.1244281493127346, Final Batch Loss: 0.03146098181605339\n",
      "Epoch 4963, Loss: 0.11452431324869394, Final Batch Loss: 0.015252110548317432\n",
      "Epoch 4964, Loss: 0.14910683408379555, Final Batch Loss: 0.05139562487602234\n",
      "Epoch 4965, Loss: 0.0909571535885334, Final Batch Loss: 0.03308999910950661\n",
      "Epoch 4966, Loss: 0.10331100784242153, Final Batch Loss: 0.056015487760305405\n",
      "Epoch 4967, Loss: 0.2160232812166214, Final Batch Loss: 0.06871914118528366\n",
      "Epoch 4968, Loss: 0.09423859044909477, Final Batch Loss: 0.05076593905687332\n",
      "Epoch 4969, Loss: 0.10646007023751736, Final Batch Loss: 0.017875375226140022\n",
      "Epoch 4970, Loss: 0.1930142231285572, Final Batch Loss: 0.08084646612405777\n",
      "Epoch 4971, Loss: 0.19228930491954088, Final Batch Loss: 0.005252276547253132\n",
      "Epoch 4972, Loss: 0.12093466706573963, Final Batch Loss: 0.07268021255731583\n",
      "Epoch 4973, Loss: 0.10484418831765652, Final Batch Loss: 0.04342213645577431\n",
      "Epoch 4974, Loss: 0.0970688909292221, Final Batch Loss: 0.026601793244481087\n",
      "Epoch 4975, Loss: 0.08432825095951557, Final Batch Loss: 0.020621374249458313\n",
      "Epoch 4976, Loss: 0.0967545798048377, Final Batch Loss: 0.010245063342154026\n",
      "Epoch 4977, Loss: 0.13619432598352432, Final Batch Loss: 0.047227706760168076\n",
      "Epoch 4978, Loss: 0.07894717901945114, Final Batch Loss: 0.02270999550819397\n",
      "Epoch 4979, Loss: 0.183971855789423, Final Batch Loss: 0.08267854899168015\n",
      "Epoch 4980, Loss: 0.10764824040234089, Final Batch Loss: 0.017927134409546852\n",
      "Epoch 4981, Loss: 0.11522293835878372, Final Batch Loss: 0.04539323225617409\n",
      "Epoch 4982, Loss: 0.09988588653504848, Final Batch Loss: 0.010123217478394508\n",
      "Epoch 4983, Loss: 0.12656535021960735, Final Batch Loss: 0.010046517476439476\n",
      "Epoch 4984, Loss: 0.08662191219627857, Final Batch Loss: 0.006492314860224724\n",
      "Epoch 4985, Loss: 0.15263580158352852, Final Batch Loss: 0.0517394058406353\n",
      "Epoch 4986, Loss: 0.09710590541362762, Final Batch Loss: 0.040644921362400055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4987, Loss: 0.10421226918697357, Final Batch Loss: 0.04616590216755867\n",
      "Epoch 4988, Loss: 0.10304143466055393, Final Batch Loss: 0.029843376949429512\n",
      "Epoch 4989, Loss: 0.11155742593109608, Final Batch Loss: 0.03595145419239998\n",
      "Epoch 4990, Loss: 0.09357530158013105, Final Batch Loss: 0.004461041651666164\n",
      "Epoch 4991, Loss: 0.06465634889900684, Final Batch Loss: 0.0010751914232969284\n",
      "Epoch 4992, Loss: 0.07434330321848392, Final Batch Loss: 0.036835215985774994\n",
      "Epoch 4993, Loss: 0.09200823679566383, Final Batch Loss: 0.025701172649860382\n",
      "Epoch 4994, Loss: 0.07301194034516811, Final Batch Loss: 0.02124515175819397\n",
      "Epoch 4995, Loss: 0.11305319610983133, Final Batch Loss: 0.011439171619713306\n",
      "Epoch 4996, Loss: 0.11069613695144653, Final Batch Loss: 0.042166899889707565\n",
      "Epoch 4997, Loss: 0.26408980041742325, Final Batch Loss: 0.18732710182666779\n",
      "Epoch 4998, Loss: 0.10636193491518497, Final Batch Loss: 0.014463657513260841\n",
      "Epoch 4999, Loss: 0.08302875026129186, Final Batch Loss: 0.0016543546225875616\n",
      "Epoch 5000, Loss: 0.09850128553807735, Final Batch Loss: 0.04192553460597992\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21  1  0  0]\n",
      " [ 0 22  0  1]\n",
      " [ 0  1 28  0]\n",
      " [ 0  0  0 24]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.955     0.977        22\n",
      "           1      0.917     0.957     0.936        23\n",
      "           2      1.000     0.966     0.982        29\n",
      "           3      0.960     1.000     0.980        24\n",
      "\n",
      "    accuracy                          0.969        98\n",
      "   macro avg      0.969     0.969     0.969        98\n",
      "weighted avg      0.971     0.969     0.970        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'../../../saved_models/UCI 4 User Classifier Ablation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
