{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '90 tBodyAccJerk-max()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 40),\n",
    "            classifier_block(40, 30),\n",
    "            classifier_block(30, 25),\n",
    "            classifier_block(25, 20),\n",
    "            nn.Linear(20, 21)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def get_act_matrix(batch_size, a_dim):\n",
    "    indexes = np.random.randint(a_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((len(indexes), indexes.max()+1))\n",
    "    one_hot[np.arange(len(indexes)),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "    \n",
    "def get_usr_matrix(batch_size, u_dim):\n",
    "    indexes = np.random.randint(u_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((indexes.size, indexes.max()+1))\n",
    "    one_hot[np.arange(indexes.size),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Real Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "    \n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_10 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_11 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_12 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_13 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_14 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_15 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_16 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_17 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_18 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_19 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_20 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_21 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9, X_10, X_11, X_12, X_13, X_14, X_15, X_16, X_17, X_18, X_19, X_20, X_21))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9) + [9] * len(X_10) + [10] * len(X_11) + [11] * len(X_12) + [12] * len(X_13) + [13] * len(X_14) + [14] * len(X_15) + [15] * len(X_16) + [16] * len(X_17) + [17] * len(X_18) + [18] * len(X_19) + [19] * len(X_20) + [20] * len(X_21)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5, 7, 8, 11, 14]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 12.233332395553589, Final Batch Loss: 3.0618832111358643\n",
      "Epoch 2, Loss: 12.213953971862793, Final Batch Loss: 3.0347023010253906\n",
      "Epoch 3, Loss: 12.213301181793213, Final Batch Loss: 3.047750949859619\n",
      "Epoch 4, Loss: 12.19971489906311, Final Batch Loss: 3.0395970344543457\n",
      "Epoch 5, Loss: 12.189426183700562, Final Batch Loss: 3.0559213161468506\n",
      "Epoch 6, Loss: 12.165900945663452, Final Batch Loss: 3.048717737197876\n",
      "Epoch 7, Loss: 12.13656210899353, Final Batch Loss: 3.037327527999878\n",
      "Epoch 8, Loss: 12.075984716415405, Final Batch Loss: 2.9997315406799316\n",
      "Epoch 9, Loss: 12.004512310028076, Final Batch Loss: 2.9855034351348877\n",
      "Epoch 10, Loss: 11.918658971786499, Final Batch Loss: 2.9668917655944824\n",
      "Epoch 11, Loss: 11.808464050292969, Final Batch Loss: 2.948636054992676\n",
      "Epoch 12, Loss: 11.671109437942505, Final Batch Loss: 2.9222023487091064\n",
      "Epoch 13, Loss: 11.510560750961304, Final Batch Loss: 2.8589224815368652\n",
      "Epoch 14, Loss: 11.307159900665283, Final Batch Loss: 2.8106188774108887\n",
      "Epoch 15, Loss: 11.147449016571045, Final Batch Loss: 2.776812791824341\n",
      "Epoch 16, Loss: 10.933979511260986, Final Batch Loss: 2.719609022140503\n",
      "Epoch 17, Loss: 10.773064851760864, Final Batch Loss: 2.7136545181274414\n",
      "Epoch 18, Loss: 10.556228399276733, Final Batch Loss: 2.6019208431243896\n",
      "Epoch 19, Loss: 10.360880136489868, Final Batch Loss: 2.612764835357666\n",
      "Epoch 20, Loss: 10.13774037361145, Final Batch Loss: 2.526214361190796\n",
      "Epoch 21, Loss: 9.91224718093872, Final Batch Loss: 2.483599901199341\n",
      "Epoch 22, Loss: 9.721633911132812, Final Batch Loss: 2.39937424659729\n",
      "Epoch 23, Loss: 9.635059833526611, Final Batch Loss: 2.4295544624328613\n",
      "Epoch 24, Loss: 9.223325490951538, Final Batch Loss: 2.283053398132324\n",
      "Epoch 25, Loss: 9.114059925079346, Final Batch Loss: 2.224902391433716\n",
      "Epoch 26, Loss: 8.98681926727295, Final Batch Loss: 2.267642021179199\n",
      "Epoch 27, Loss: 8.79992151260376, Final Batch Loss: 2.16030216217041\n",
      "Epoch 28, Loss: 8.620383977890015, Final Batch Loss: 2.095893621444702\n",
      "Epoch 29, Loss: 8.601583480834961, Final Batch Loss: 2.140082836151123\n",
      "Epoch 30, Loss: 8.36514949798584, Final Batch Loss: 2.1281769275665283\n",
      "Epoch 31, Loss: 8.36140489578247, Final Batch Loss: 2.0946338176727295\n",
      "Epoch 32, Loss: 8.219671249389648, Final Batch Loss: 2.0607213973999023\n",
      "Epoch 33, Loss: 8.147862672805786, Final Batch Loss: 2.0315942764282227\n",
      "Epoch 34, Loss: 7.948106646537781, Final Batch Loss: 1.9259028434753418\n",
      "Epoch 35, Loss: 7.8655112981796265, Final Batch Loss: 1.9541795253753662\n",
      "Epoch 36, Loss: 7.814798712730408, Final Batch Loss: 1.914336085319519\n",
      "Epoch 37, Loss: 7.838833689689636, Final Batch Loss: 1.9923397302627563\n",
      "Epoch 38, Loss: 7.712552070617676, Final Batch Loss: 1.9847450256347656\n",
      "Epoch 39, Loss: 7.633401274681091, Final Batch Loss: 1.9009016752243042\n",
      "Epoch 40, Loss: 7.557577967643738, Final Batch Loss: 1.880919337272644\n",
      "Epoch 41, Loss: 7.515761375427246, Final Batch Loss: 1.8718944787979126\n",
      "Epoch 42, Loss: 7.4859795570373535, Final Batch Loss: 1.875394344329834\n",
      "Epoch 43, Loss: 7.445566892623901, Final Batch Loss: 1.7912951707839966\n",
      "Epoch 44, Loss: 7.52598512172699, Final Batch Loss: 1.8491283655166626\n",
      "Epoch 45, Loss: 7.440426588058472, Final Batch Loss: 1.8408236503601074\n",
      "Epoch 46, Loss: 7.235719323158264, Final Batch Loss: 1.7974255084991455\n",
      "Epoch 47, Loss: 7.282399773597717, Final Batch Loss: 1.8715903759002686\n",
      "Epoch 48, Loss: 7.165680766105652, Final Batch Loss: 1.7604002952575684\n",
      "Epoch 49, Loss: 7.160825252532959, Final Batch Loss: 1.727475643157959\n",
      "Epoch 50, Loss: 7.095421671867371, Final Batch Loss: 1.7703717947006226\n",
      "Epoch 51, Loss: 7.019883394241333, Final Batch Loss: 1.7299643754959106\n",
      "Epoch 52, Loss: 6.875574946403503, Final Batch Loss: 1.6315025091171265\n",
      "Epoch 53, Loss: 6.851716756820679, Final Batch Loss: 1.7333673238754272\n",
      "Epoch 54, Loss: 6.8491092920303345, Final Batch Loss: 1.7215361595153809\n",
      "Epoch 55, Loss: 6.778794527053833, Final Batch Loss: 1.6559635400772095\n",
      "Epoch 56, Loss: 6.827436923980713, Final Batch Loss: 1.751175880432129\n",
      "Epoch 57, Loss: 6.578319191932678, Final Batch Loss: 1.6167271137237549\n",
      "Epoch 58, Loss: 6.637238383293152, Final Batch Loss: 1.6743072271347046\n",
      "Epoch 59, Loss: 6.5818785429000854, Final Batch Loss: 1.70280122756958\n",
      "Epoch 60, Loss: 6.519680380821228, Final Batch Loss: 1.6453295946121216\n",
      "Epoch 61, Loss: 6.459103465080261, Final Batch Loss: 1.5825132131576538\n",
      "Epoch 62, Loss: 6.427488327026367, Final Batch Loss: 1.5732250213623047\n",
      "Epoch 63, Loss: 6.401220679283142, Final Batch Loss: 1.5783827304840088\n",
      "Epoch 64, Loss: 6.269659757614136, Final Batch Loss: 1.5805860757827759\n",
      "Epoch 65, Loss: 6.320911884307861, Final Batch Loss: 1.5450650453567505\n",
      "Epoch 66, Loss: 6.216360092163086, Final Batch Loss: 1.5454223155975342\n",
      "Epoch 67, Loss: 6.206945419311523, Final Batch Loss: 1.6697806119918823\n",
      "Epoch 68, Loss: 6.088351011276245, Final Batch Loss: 1.4645024538040161\n",
      "Epoch 69, Loss: 6.210944771766663, Final Batch Loss: 1.5111984014511108\n",
      "Epoch 70, Loss: 5.96099853515625, Final Batch Loss: 1.404911756515503\n",
      "Epoch 71, Loss: 5.998984098434448, Final Batch Loss: 1.5745643377304077\n",
      "Epoch 72, Loss: 5.869898200035095, Final Batch Loss: 1.417864203453064\n",
      "Epoch 73, Loss: 5.909180164337158, Final Batch Loss: 1.4459292888641357\n",
      "Epoch 74, Loss: 6.055767774581909, Final Batch Loss: 1.5602898597717285\n",
      "Epoch 75, Loss: 5.957862734794617, Final Batch Loss: 1.6171185970306396\n",
      "Epoch 76, Loss: 5.8295371532440186, Final Batch Loss: 1.4143849611282349\n",
      "Epoch 77, Loss: 5.870718955993652, Final Batch Loss: 1.518572449684143\n",
      "Epoch 78, Loss: 5.748739123344421, Final Batch Loss: 1.40185546875\n",
      "Epoch 79, Loss: 5.691742181777954, Final Batch Loss: 1.3613334894180298\n",
      "Epoch 80, Loss: 5.627295255661011, Final Batch Loss: 1.468959927558899\n",
      "Epoch 81, Loss: 5.588830351829529, Final Batch Loss: 1.4821053743362427\n",
      "Epoch 82, Loss: 5.58843469619751, Final Batch Loss: 1.391160249710083\n",
      "Epoch 83, Loss: 5.5140745639801025, Final Batch Loss: 1.3474187850952148\n",
      "Epoch 84, Loss: 5.715192794799805, Final Batch Loss: 1.4751790761947632\n",
      "Epoch 85, Loss: 5.511465072631836, Final Batch Loss: 1.3507612943649292\n",
      "Epoch 86, Loss: 5.45320463180542, Final Batch Loss: 1.2970350980758667\n",
      "Epoch 87, Loss: 5.620551228523254, Final Batch Loss: 1.4376718997955322\n",
      "Epoch 88, Loss: 5.3952343463897705, Final Batch Loss: 1.298741340637207\n",
      "Epoch 89, Loss: 5.414766311645508, Final Batch Loss: 1.3200620412826538\n",
      "Epoch 90, Loss: 5.451682090759277, Final Batch Loss: 1.492432713508606\n",
      "Epoch 91, Loss: 5.286925435066223, Final Batch Loss: 1.2621521949768066\n",
      "Epoch 92, Loss: 5.438369631767273, Final Batch Loss: 1.381861686706543\n",
      "Epoch 93, Loss: 5.249330282211304, Final Batch Loss: 1.3149558305740356\n",
      "Epoch 94, Loss: 5.286231279373169, Final Batch Loss: 1.3781167268753052\n",
      "Epoch 95, Loss: 5.229800462722778, Final Batch Loss: 1.3085417747497559\n",
      "Epoch 96, Loss: 5.206061124801636, Final Batch Loss: 1.361937165260315\n",
      "Epoch 97, Loss: 5.063109040260315, Final Batch Loss: 1.1575028896331787\n",
      "Epoch 98, Loss: 5.038052082061768, Final Batch Loss: 1.154191255569458\n",
      "Epoch 99, Loss: 5.178477883338928, Final Batch Loss: 1.2916065454483032\n",
      "Epoch 100, Loss: 5.187752604484558, Final Batch Loss: 1.2583755254745483\n",
      "Epoch 101, Loss: 5.087465286254883, Final Batch Loss: 1.3105453252792358\n",
      "Epoch 102, Loss: 4.976737022399902, Final Batch Loss: 1.2468202114105225\n",
      "Epoch 103, Loss: 4.869332671165466, Final Batch Loss: 1.2231875658035278\n",
      "Epoch 104, Loss: 4.954302787780762, Final Batch Loss: 1.2663792371749878\n",
      "Epoch 105, Loss: 4.997256636619568, Final Batch Loss: 1.2369675636291504\n",
      "Epoch 106, Loss: 5.04140031337738, Final Batch Loss: 1.2992535829544067\n",
      "Epoch 107, Loss: 4.817623138427734, Final Batch Loss: 1.2295176982879639\n",
      "Epoch 108, Loss: 4.930703520774841, Final Batch Loss: 1.2485308647155762\n",
      "Epoch 109, Loss: 4.824706554412842, Final Batch Loss: 1.1480677127838135\n",
      "Epoch 110, Loss: 4.6749327182769775, Final Batch Loss: 1.135728359222412\n",
      "Epoch 111, Loss: 4.8291462659835815, Final Batch Loss: 1.1524074077606201\n",
      "Epoch 112, Loss: 4.668598413467407, Final Batch Loss: 1.1160234212875366\n",
      "Epoch 113, Loss: 4.720720410346985, Final Batch Loss: 1.224410057067871\n",
      "Epoch 114, Loss: 4.777919173240662, Final Batch Loss: 1.1539669036865234\n",
      "Epoch 115, Loss: 4.690564513206482, Final Batch Loss: 1.0742748975753784\n",
      "Epoch 116, Loss: 4.582078456878662, Final Batch Loss: 1.1092511415481567\n",
      "Epoch 117, Loss: 4.749253869056702, Final Batch Loss: 1.201134204864502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118, Loss: 4.698841691017151, Final Batch Loss: 1.1599957942962646\n",
      "Epoch 119, Loss: 4.677254557609558, Final Batch Loss: 1.112173080444336\n",
      "Epoch 120, Loss: 4.6663841009140015, Final Batch Loss: 1.154649019241333\n",
      "Epoch 121, Loss: 4.712216258049011, Final Batch Loss: 1.2512235641479492\n",
      "Epoch 122, Loss: 4.568700432777405, Final Batch Loss: 1.09285569190979\n",
      "Epoch 123, Loss: 4.531087875366211, Final Batch Loss: 1.0430294275283813\n",
      "Epoch 124, Loss: 4.609476923942566, Final Batch Loss: 1.1612225770950317\n",
      "Epoch 125, Loss: 4.297021567821503, Final Batch Loss: 0.9713675379753113\n",
      "Epoch 126, Loss: 4.484549283981323, Final Batch Loss: 1.1157035827636719\n",
      "Epoch 127, Loss: 4.626560211181641, Final Batch Loss: 1.2059450149536133\n",
      "Epoch 128, Loss: 4.420087456703186, Final Batch Loss: 1.0262075662612915\n",
      "Epoch 129, Loss: 4.590576887130737, Final Batch Loss: 1.086307406425476\n",
      "Epoch 130, Loss: 4.46967077255249, Final Batch Loss: 1.009466528892517\n",
      "Epoch 131, Loss: 4.458341002464294, Final Batch Loss: 1.1011346578598022\n",
      "Epoch 132, Loss: 4.514702796936035, Final Batch Loss: 1.1447722911834717\n",
      "Epoch 133, Loss: 4.479093313217163, Final Batch Loss: 1.094313144683838\n",
      "Epoch 134, Loss: 4.478922128677368, Final Batch Loss: 1.1945273876190186\n",
      "Epoch 135, Loss: 4.495187163352966, Final Batch Loss: 1.152316927909851\n",
      "Epoch 136, Loss: 4.417783141136169, Final Batch Loss: 1.1655850410461426\n",
      "Epoch 137, Loss: 4.437249541282654, Final Batch Loss: 1.1263436079025269\n",
      "Epoch 138, Loss: 4.210357844829559, Final Batch Loss: 1.062720537185669\n",
      "Epoch 139, Loss: 4.302653074264526, Final Batch Loss: 1.0530322790145874\n",
      "Epoch 140, Loss: 4.327593684196472, Final Batch Loss: 1.0817166566848755\n",
      "Epoch 141, Loss: 4.369099020957947, Final Batch Loss: 0.9958010911941528\n",
      "Epoch 142, Loss: 4.413065433502197, Final Batch Loss: 1.0278537273406982\n",
      "Epoch 143, Loss: 4.24322235584259, Final Batch Loss: 1.0564465522766113\n",
      "Epoch 144, Loss: 4.278724670410156, Final Batch Loss: 1.0002297163009644\n",
      "Epoch 145, Loss: 4.343631386756897, Final Batch Loss: 1.141825556755066\n",
      "Epoch 146, Loss: 4.185676097869873, Final Batch Loss: 1.052253007888794\n",
      "Epoch 147, Loss: 4.2460445165634155, Final Batch Loss: 1.080382227897644\n",
      "Epoch 148, Loss: 4.156057000160217, Final Batch Loss: 1.0440934896469116\n",
      "Epoch 149, Loss: 4.112358033657074, Final Batch Loss: 0.9531489014625549\n",
      "Epoch 150, Loss: 4.197739839553833, Final Batch Loss: 1.0615323781967163\n",
      "Epoch 151, Loss: 4.311526656150818, Final Batch Loss: 1.1830755472183228\n",
      "Epoch 152, Loss: 4.193668842315674, Final Batch Loss: 0.9343348741531372\n",
      "Epoch 153, Loss: 4.1743505001068115, Final Batch Loss: 0.9894033670425415\n",
      "Epoch 154, Loss: 4.136135578155518, Final Batch Loss: 0.9934781789779663\n",
      "Epoch 155, Loss: 4.184058308601379, Final Batch Loss: 1.0047208070755005\n",
      "Epoch 156, Loss: 4.149807393550873, Final Batch Loss: 1.0508813858032227\n",
      "Epoch 157, Loss: 4.320858895778656, Final Batch Loss: 1.2014915943145752\n",
      "Epoch 158, Loss: 4.194243907928467, Final Batch Loss: 1.0251048803329468\n",
      "Epoch 159, Loss: 4.159818947315216, Final Batch Loss: 0.9689809679985046\n",
      "Epoch 160, Loss: 4.102053165435791, Final Batch Loss: 0.9675495028495789\n",
      "Epoch 161, Loss: 4.090949714183807, Final Batch Loss: 0.9043024182319641\n",
      "Epoch 162, Loss: 4.178938925266266, Final Batch Loss: 0.9942187666893005\n",
      "Epoch 163, Loss: 3.976659059524536, Final Batch Loss: 0.9646005034446716\n",
      "Epoch 164, Loss: 4.103838562965393, Final Batch Loss: 1.0723276138305664\n",
      "Epoch 165, Loss: 4.1204705238342285, Final Batch Loss: 1.003513216972351\n",
      "Epoch 166, Loss: 4.022861540317535, Final Batch Loss: 0.8942140340805054\n",
      "Epoch 167, Loss: 4.182673752307892, Final Batch Loss: 1.0863184928894043\n",
      "Epoch 168, Loss: 4.04718691110611, Final Batch Loss: 1.0427663326263428\n",
      "Epoch 169, Loss: 4.013648629188538, Final Batch Loss: 1.0415551662445068\n",
      "Epoch 170, Loss: 4.110763907432556, Final Batch Loss: 1.059275507926941\n",
      "Epoch 171, Loss: 3.841442346572876, Final Batch Loss: 0.8507398366928101\n",
      "Epoch 172, Loss: 4.069179117679596, Final Batch Loss: 0.9525317549705505\n",
      "Epoch 173, Loss: 4.072038173675537, Final Batch Loss: 1.125253677368164\n",
      "Epoch 174, Loss: 4.02763032913208, Final Batch Loss: 1.0603889226913452\n",
      "Epoch 175, Loss: 3.9988269209861755, Final Batch Loss: 1.0320600271224976\n",
      "Epoch 176, Loss: 3.996181309223175, Final Batch Loss: 0.9958974719047546\n",
      "Epoch 177, Loss: 3.9236888885498047, Final Batch Loss: 0.9097906947135925\n",
      "Epoch 178, Loss: 3.8657965064048767, Final Batch Loss: 1.042075276374817\n",
      "Epoch 179, Loss: 3.978041112422943, Final Batch Loss: 1.0287086963653564\n",
      "Epoch 180, Loss: 3.92555832862854, Final Batch Loss: 0.9493982791900635\n",
      "Epoch 181, Loss: 3.9136025309562683, Final Batch Loss: 1.0256847143173218\n",
      "Epoch 182, Loss: 3.876130163669586, Final Batch Loss: 0.9225101470947266\n",
      "Epoch 183, Loss: 3.9660521149635315, Final Batch Loss: 1.0796613693237305\n",
      "Epoch 184, Loss: 3.9219769835472107, Final Batch Loss: 1.0500320196151733\n",
      "Epoch 185, Loss: 3.8142475485801697, Final Batch Loss: 0.9003471732139587\n",
      "Epoch 186, Loss: 3.9603129029273987, Final Batch Loss: 0.9180312752723694\n",
      "Epoch 187, Loss: 3.932115375995636, Final Batch Loss: 1.0436047315597534\n",
      "Epoch 188, Loss: 3.8910112380981445, Final Batch Loss: 0.9145826101303101\n",
      "Epoch 189, Loss: 3.989815890789032, Final Batch Loss: 1.080936312675476\n",
      "Epoch 190, Loss: 3.750351071357727, Final Batch Loss: 0.9255024790763855\n",
      "Epoch 191, Loss: 3.9817188382148743, Final Batch Loss: 1.049332857131958\n",
      "Epoch 192, Loss: 3.8610225319862366, Final Batch Loss: 1.0162919759750366\n",
      "Epoch 193, Loss: 3.85096138715744, Final Batch Loss: 0.9119318127632141\n",
      "Epoch 194, Loss: 3.91392719745636, Final Batch Loss: 0.9481661319732666\n",
      "Epoch 195, Loss: 3.78305184841156, Final Batch Loss: 0.9538097381591797\n",
      "Epoch 196, Loss: 3.7111074924468994, Final Batch Loss: 0.8510134220123291\n",
      "Epoch 197, Loss: 3.864789664745331, Final Batch Loss: 1.000033974647522\n",
      "Epoch 198, Loss: 3.810118019580841, Final Batch Loss: 0.9223621487617493\n",
      "Epoch 199, Loss: 3.7808703184127808, Final Batch Loss: 0.9993396401405334\n",
      "Epoch 200, Loss: 3.7114633917808533, Final Batch Loss: 0.9450804591178894\n",
      "Epoch 201, Loss: 3.8492188453674316, Final Batch Loss: 0.9496377110481262\n",
      "Epoch 202, Loss: 3.86584734916687, Final Batch Loss: 1.0676183700561523\n",
      "Epoch 203, Loss: 3.690753161907196, Final Batch Loss: 0.8760027289390564\n",
      "Epoch 204, Loss: 3.727959096431732, Final Batch Loss: 0.8592965006828308\n",
      "Epoch 205, Loss: 3.714740753173828, Final Batch Loss: 0.8683655261993408\n",
      "Epoch 206, Loss: 3.7837385535240173, Final Batch Loss: 0.9938685297966003\n",
      "Epoch 207, Loss: 3.656868636608124, Final Batch Loss: 0.8291752934455872\n",
      "Epoch 208, Loss: 3.7267876863479614, Final Batch Loss: 0.9175623655319214\n",
      "Epoch 209, Loss: 3.7362119555473328, Final Batch Loss: 0.934564471244812\n",
      "Epoch 210, Loss: 3.8302457332611084, Final Batch Loss: 0.9933563470840454\n",
      "Epoch 211, Loss: 3.712804436683655, Final Batch Loss: 0.9358864426612854\n",
      "Epoch 212, Loss: 3.721686542034149, Final Batch Loss: 0.9025677442550659\n",
      "Epoch 213, Loss: 3.760945200920105, Final Batch Loss: 0.9142866730690002\n",
      "Epoch 214, Loss: 3.8137258291244507, Final Batch Loss: 0.8619070053100586\n",
      "Epoch 215, Loss: 3.486772060394287, Final Batch Loss: 0.7841872572898865\n",
      "Epoch 216, Loss: 3.6130442023277283, Final Batch Loss: 0.8443918824195862\n",
      "Epoch 217, Loss: 3.56576806306839, Final Batch Loss: 0.8614267706871033\n",
      "Epoch 218, Loss: 3.5611987113952637, Final Batch Loss: 0.8723176717758179\n",
      "Epoch 219, Loss: 3.8604755997657776, Final Batch Loss: 1.1095956563949585\n",
      "Epoch 220, Loss: 3.555049777030945, Final Batch Loss: 0.8169021010398865\n",
      "Epoch 221, Loss: 3.6148468255996704, Final Batch Loss: 1.003990650177002\n",
      "Epoch 222, Loss: 3.51282799243927, Final Batch Loss: 0.862638533115387\n",
      "Epoch 223, Loss: 3.6027888655662537, Final Batch Loss: 0.9308056831359863\n",
      "Epoch 224, Loss: 3.610497832298279, Final Batch Loss: 0.8605759739875793\n",
      "Epoch 225, Loss: 3.64641273021698, Final Batch Loss: 0.9715955257415771\n",
      "Epoch 226, Loss: 3.6475008130073547, Final Batch Loss: 0.9271693825721741\n",
      "Epoch 227, Loss: 3.5842334032058716, Final Batch Loss: 0.9300616979598999\n",
      "Epoch 228, Loss: 3.538380205631256, Final Batch Loss: 0.7458555698394775\n",
      "Epoch 229, Loss: 3.611595034599304, Final Batch Loss: 0.9364815950393677\n",
      "Epoch 230, Loss: 3.5720373392105103, Final Batch Loss: 0.864713191986084\n",
      "Epoch 231, Loss: 3.592146933078766, Final Batch Loss: 0.930128812789917\n",
      "Epoch 232, Loss: 3.432104527950287, Final Batch Loss: 0.8263145685195923\n",
      "Epoch 233, Loss: 3.57039737701416, Final Batch Loss: 0.8563327193260193\n",
      "Epoch 234, Loss: 3.77641624212265, Final Batch Loss: 1.110388994216919\n",
      "Epoch 235, Loss: 3.5531347393989563, Final Batch Loss: 0.8553991317749023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236, Loss: 3.5752620100975037, Final Batch Loss: 0.834503710269928\n",
      "Epoch 237, Loss: 3.4958959221839905, Final Batch Loss: 0.8297495245933533\n",
      "Epoch 238, Loss: 3.5615339279174805, Final Batch Loss: 0.9187162518501282\n",
      "Epoch 239, Loss: 3.4417473673820496, Final Batch Loss: 0.7822977304458618\n",
      "Epoch 240, Loss: 3.5136115550994873, Final Batch Loss: 0.9147074818611145\n",
      "Epoch 241, Loss: 3.5589849948883057, Final Batch Loss: 0.87595134973526\n",
      "Epoch 242, Loss: 3.582333207130432, Final Batch Loss: 0.9950692057609558\n",
      "Epoch 243, Loss: 3.5375319123268127, Final Batch Loss: 0.926461398601532\n",
      "Epoch 244, Loss: 3.4872779846191406, Final Batch Loss: 0.9068548679351807\n",
      "Epoch 245, Loss: 3.4987675547599792, Final Batch Loss: 0.8405829668045044\n",
      "Epoch 246, Loss: 3.425139904022217, Final Batch Loss: 0.7193805575370789\n",
      "Epoch 247, Loss: 3.609040379524231, Final Batch Loss: 1.044350266456604\n",
      "Epoch 248, Loss: 3.4547483921051025, Final Batch Loss: 0.8686450719833374\n",
      "Epoch 249, Loss: 3.3499684929847717, Final Batch Loss: 0.7659621238708496\n",
      "Epoch 250, Loss: 3.576947271823883, Final Batch Loss: 0.9421840906143188\n",
      "Epoch 251, Loss: 3.455421209335327, Final Batch Loss: 0.8728842735290527\n",
      "Epoch 252, Loss: 3.504177749156952, Final Batch Loss: 0.8219298720359802\n",
      "Epoch 253, Loss: 3.3773916363716125, Final Batch Loss: 0.9113709330558777\n",
      "Epoch 254, Loss: 3.5582988262176514, Final Batch Loss: 0.7841898798942566\n",
      "Epoch 255, Loss: 3.4574971795082092, Final Batch Loss: 0.9152131676673889\n",
      "Epoch 256, Loss: 3.570588231086731, Final Batch Loss: 0.9132711291313171\n",
      "Epoch 257, Loss: 3.475194036960602, Final Batch Loss: 0.8902445435523987\n",
      "Epoch 258, Loss: 3.340701997280121, Final Batch Loss: 0.8172679543495178\n",
      "Epoch 259, Loss: 3.4218791127204895, Final Batch Loss: 0.8276877999305725\n",
      "Epoch 260, Loss: 3.4283607602119446, Final Batch Loss: 0.7868971824645996\n",
      "Epoch 261, Loss: 3.384209394454956, Final Batch Loss: 0.8331005573272705\n",
      "Epoch 262, Loss: 3.338269591331482, Final Batch Loss: 0.935910701751709\n",
      "Epoch 263, Loss: 3.379586100578308, Final Batch Loss: 0.895846962928772\n",
      "Epoch 264, Loss: 3.28705894947052, Final Batch Loss: 0.8133127689361572\n",
      "Epoch 265, Loss: 3.484729290008545, Final Batch Loss: 0.8394683599472046\n",
      "Epoch 266, Loss: 3.3158751130104065, Final Batch Loss: 0.8742658495903015\n",
      "Epoch 267, Loss: 3.3618884682655334, Final Batch Loss: 0.7410213947296143\n",
      "Epoch 268, Loss: 3.3469672799110413, Final Batch Loss: 0.7589104771614075\n",
      "Epoch 269, Loss: 3.3065847754478455, Final Batch Loss: 0.8453595042228699\n",
      "Epoch 270, Loss: 3.4303576350212097, Final Batch Loss: 0.8064512014389038\n",
      "Epoch 271, Loss: 3.342606484889984, Final Batch Loss: 0.8425365686416626\n",
      "Epoch 272, Loss: 3.4787721633911133, Final Batch Loss: 0.8972824215888977\n",
      "Epoch 273, Loss: 3.2203201055526733, Final Batch Loss: 0.8066085577011108\n",
      "Epoch 274, Loss: 3.320878267288208, Final Batch Loss: 0.7156934142112732\n",
      "Epoch 275, Loss: 3.261323034763336, Final Batch Loss: 0.7336232662200928\n",
      "Epoch 276, Loss: 3.378324329853058, Final Batch Loss: 0.9225395321846008\n",
      "Epoch 277, Loss: 3.3152526021003723, Final Batch Loss: 0.8438278436660767\n",
      "Epoch 278, Loss: 3.3783783316612244, Final Batch Loss: 0.865646243095398\n",
      "Epoch 279, Loss: 3.2585065960884094, Final Batch Loss: 0.7441408038139343\n",
      "Epoch 280, Loss: 3.442467749118805, Final Batch Loss: 0.9172815680503845\n",
      "Epoch 281, Loss: 3.3211257457733154, Final Batch Loss: 0.8789677023887634\n",
      "Epoch 282, Loss: 3.287368416786194, Final Batch Loss: 0.8193853497505188\n",
      "Epoch 283, Loss: 3.3741642832756042, Final Batch Loss: 0.8568425178527832\n",
      "Epoch 284, Loss: 3.191508710384369, Final Batch Loss: 0.7434253096580505\n",
      "Epoch 285, Loss: 3.229159355163574, Final Batch Loss: 0.7550768852233887\n",
      "Epoch 286, Loss: 3.3076353669166565, Final Batch Loss: 0.7780827283859253\n",
      "Epoch 287, Loss: 3.231417238712311, Final Batch Loss: 0.8885601162910461\n",
      "Epoch 288, Loss: 3.158125400543213, Final Batch Loss: 0.7977356910705566\n",
      "Epoch 289, Loss: 3.4296745657920837, Final Batch Loss: 0.9648807644844055\n",
      "Epoch 290, Loss: 3.205754816532135, Final Batch Loss: 0.7569085359573364\n",
      "Epoch 291, Loss: 3.325605809688568, Final Batch Loss: 0.772611141204834\n",
      "Epoch 292, Loss: 3.2123231887817383, Final Batch Loss: 0.7767641544342041\n",
      "Epoch 293, Loss: 3.226425528526306, Final Batch Loss: 0.8995035290718079\n",
      "Epoch 294, Loss: 3.182327091693878, Final Batch Loss: 0.801703155040741\n",
      "Epoch 295, Loss: 3.3467472791671753, Final Batch Loss: 0.8823758959770203\n",
      "Epoch 296, Loss: 3.1551084518432617, Final Batch Loss: 0.7972531914710999\n",
      "Epoch 297, Loss: 3.1368337869644165, Final Batch Loss: 0.6535314321517944\n",
      "Epoch 298, Loss: 3.194394111633301, Final Batch Loss: 0.8806076645851135\n",
      "Epoch 299, Loss: 3.2221937775611877, Final Batch Loss: 0.7729267477989197\n",
      "Epoch 300, Loss: 3.3892423510551453, Final Batch Loss: 0.8859862089157104\n",
      "Epoch 301, Loss: 3.1338230967521667, Final Batch Loss: 0.858540415763855\n",
      "Epoch 302, Loss: 3.3364028930664062, Final Batch Loss: 0.887986421585083\n",
      "Epoch 303, Loss: 3.2400192618370056, Final Batch Loss: 0.8045439124107361\n",
      "Epoch 304, Loss: 3.18693608045578, Final Batch Loss: 0.7457484602928162\n",
      "Epoch 305, Loss: 3.19181889295578, Final Batch Loss: 0.8695733547210693\n",
      "Epoch 306, Loss: 3.278540790081024, Final Batch Loss: 0.7950220704078674\n",
      "Epoch 307, Loss: 3.1905452013015747, Final Batch Loss: 0.7524715662002563\n",
      "Epoch 308, Loss: 3.1144018173217773, Final Batch Loss: 0.7335386872291565\n",
      "Epoch 309, Loss: 3.3546591997146606, Final Batch Loss: 0.8409587144851685\n",
      "Epoch 310, Loss: 3.2694039940834045, Final Batch Loss: 0.9184172749519348\n",
      "Epoch 311, Loss: 3.079616367816925, Final Batch Loss: 0.8428592681884766\n",
      "Epoch 312, Loss: 3.2966511249542236, Final Batch Loss: 0.8500186800956726\n",
      "Epoch 313, Loss: 3.1394070386886597, Final Batch Loss: 0.7657106518745422\n",
      "Epoch 314, Loss: 3.2685970664024353, Final Batch Loss: 0.785068690776825\n",
      "Epoch 315, Loss: 3.1880510449409485, Final Batch Loss: 0.7911072969436646\n",
      "Epoch 316, Loss: 3.0917791724205017, Final Batch Loss: 0.8101459741592407\n",
      "Epoch 317, Loss: 3.2593623399734497, Final Batch Loss: 0.8159810900688171\n",
      "Epoch 318, Loss: 3.043773889541626, Final Batch Loss: 0.6776226758956909\n",
      "Epoch 319, Loss: 2.9947603940963745, Final Batch Loss: 0.6297521591186523\n",
      "Epoch 320, Loss: 2.9795793294906616, Final Batch Loss: 0.65971440076828\n",
      "Epoch 321, Loss: 3.085858106613159, Final Batch Loss: 0.8302342891693115\n",
      "Epoch 322, Loss: 3.084091007709503, Final Batch Loss: 0.7577769160270691\n",
      "Epoch 323, Loss: 3.1360543966293335, Final Batch Loss: 0.7347334623336792\n",
      "Epoch 324, Loss: 3.29844206571579, Final Batch Loss: 0.9407225251197815\n",
      "Epoch 325, Loss: 2.904952883720398, Final Batch Loss: 0.6694220900535583\n",
      "Epoch 326, Loss: 3.0516764521598816, Final Batch Loss: 0.7839066982269287\n",
      "Epoch 327, Loss: 3.0303526520729065, Final Batch Loss: 0.7237051725387573\n",
      "Epoch 328, Loss: 3.2494850158691406, Final Batch Loss: 0.7812579274177551\n",
      "Epoch 329, Loss: 3.1100751757621765, Final Batch Loss: 0.6883401274681091\n",
      "Epoch 330, Loss: 3.048733174800873, Final Batch Loss: 0.688345193862915\n",
      "Epoch 331, Loss: 2.9704549908638, Final Batch Loss: 0.7946761846542358\n",
      "Epoch 332, Loss: 3.0860816836357117, Final Batch Loss: 0.7634502053260803\n",
      "Epoch 333, Loss: 3.044154167175293, Final Batch Loss: 0.7427929639816284\n",
      "Epoch 334, Loss: 2.861633062362671, Final Batch Loss: 0.7131754159927368\n",
      "Epoch 335, Loss: 3.0285508036613464, Final Batch Loss: 0.7224355340003967\n",
      "Epoch 336, Loss: 3.150568902492523, Final Batch Loss: 0.7557315826416016\n",
      "Epoch 337, Loss: 3.0030083656311035, Final Batch Loss: 0.7724736928939819\n",
      "Epoch 338, Loss: 3.003707230091095, Final Batch Loss: 0.7532424926757812\n",
      "Epoch 339, Loss: 3.0134053230285645, Final Batch Loss: 0.8170446157455444\n",
      "Epoch 340, Loss: 3.077389419078827, Final Batch Loss: 0.7218748927116394\n",
      "Epoch 341, Loss: 2.947932779788971, Final Batch Loss: 0.7007361054420471\n",
      "Epoch 342, Loss: 2.9899837970733643, Final Batch Loss: 0.7062285542488098\n",
      "Epoch 343, Loss: 2.980587422847748, Final Batch Loss: 0.7270371913909912\n",
      "Epoch 344, Loss: 2.9263638854026794, Final Batch Loss: 0.7272606492042542\n",
      "Epoch 345, Loss: 2.918372094631195, Final Batch Loss: 0.7176821231842041\n",
      "Epoch 346, Loss: 3.160480558872223, Final Batch Loss: 0.7530285716056824\n",
      "Epoch 347, Loss: 2.9392163157463074, Final Batch Loss: 0.6020641922950745\n",
      "Epoch 348, Loss: 3.1065430641174316, Final Batch Loss: 0.8081802725791931\n",
      "Epoch 349, Loss: 3.052386999130249, Final Batch Loss: 0.7596725225448608\n",
      "Epoch 350, Loss: 2.978605628013611, Final Batch Loss: 0.7224133610725403\n",
      "Epoch 351, Loss: 2.943203032016754, Final Batch Loss: 0.7348688840866089\n",
      "Epoch 352, Loss: 3.1656485199928284, Final Batch Loss: 0.8322970867156982\n",
      "Epoch 353, Loss: 2.9602497816085815, Final Batch Loss: 0.8048107624053955\n",
      "Epoch 354, Loss: 3.075564920902252, Final Batch Loss: 0.774930477142334\n",
      "Epoch 355, Loss: 2.8988757729530334, Final Batch Loss: 0.6710941791534424\n",
      "Epoch 356, Loss: 2.899336099624634, Final Batch Loss: 0.6858973503112793\n",
      "Epoch 357, Loss: 2.877678632736206, Final Batch Loss: 0.6370911002159119\n",
      "Epoch 358, Loss: 2.9288360476493835, Final Batch Loss: 0.8248797655105591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 359, Loss: 2.912096679210663, Final Batch Loss: 0.6039761900901794\n",
      "Epoch 360, Loss: 2.9687530994415283, Final Batch Loss: 0.8059918880462646\n",
      "Epoch 361, Loss: 3.047074556350708, Final Batch Loss: 0.7167794704437256\n",
      "Epoch 362, Loss: 2.916670560836792, Final Batch Loss: 0.6941214203834534\n",
      "Epoch 363, Loss: 2.9280834197998047, Final Batch Loss: 0.8540814518928528\n",
      "Epoch 364, Loss: 2.8668729662895203, Final Batch Loss: 0.7752936482429504\n",
      "Epoch 365, Loss: 2.8994336128234863, Final Batch Loss: 0.7381933927536011\n",
      "Epoch 366, Loss: 2.894223213195801, Final Batch Loss: 0.7058090567588806\n",
      "Epoch 367, Loss: 2.9976886510849, Final Batch Loss: 0.8636389374732971\n",
      "Epoch 368, Loss: 2.8940106630325317, Final Batch Loss: 0.6122897267341614\n",
      "Epoch 369, Loss: 2.823961079120636, Final Batch Loss: 0.7077443599700928\n",
      "Epoch 370, Loss: 2.779483735561371, Final Batch Loss: 0.633307695388794\n",
      "Epoch 371, Loss: 2.952241361141205, Final Batch Loss: 0.7157540321350098\n",
      "Epoch 372, Loss: 2.797391951084137, Final Batch Loss: 0.7027120590209961\n",
      "Epoch 373, Loss: 2.8344610929489136, Final Batch Loss: 0.6347802877426147\n",
      "Epoch 374, Loss: 2.8854339718818665, Final Batch Loss: 0.797770619392395\n",
      "Epoch 375, Loss: 2.860612690448761, Final Batch Loss: 0.7740000486373901\n",
      "Epoch 376, Loss: 2.879621744155884, Final Batch Loss: 0.7218185067176819\n",
      "Epoch 377, Loss: 2.949873983860016, Final Batch Loss: 0.6895224452018738\n",
      "Epoch 378, Loss: 2.8435550928115845, Final Batch Loss: 0.6837877631187439\n",
      "Epoch 379, Loss: 2.974730432033539, Final Batch Loss: 0.7061447501182556\n",
      "Epoch 380, Loss: 2.8698055148124695, Final Batch Loss: 0.6911678314208984\n",
      "Epoch 381, Loss: 2.7819916009902954, Final Batch Loss: 0.6379060745239258\n",
      "Epoch 382, Loss: 2.9576937556266785, Final Batch Loss: 0.729401707649231\n",
      "Epoch 383, Loss: 2.8614941835403442, Final Batch Loss: 0.6403078436851501\n",
      "Epoch 384, Loss: 2.7590702772140503, Final Batch Loss: 0.6657967567443848\n",
      "Epoch 385, Loss: 2.8044243454933167, Final Batch Loss: 0.7134812474250793\n",
      "Epoch 386, Loss: 2.725915253162384, Final Batch Loss: 0.638275146484375\n",
      "Epoch 387, Loss: 2.7511282563209534, Final Batch Loss: 0.5523234605789185\n",
      "Epoch 388, Loss: 2.817107379436493, Final Batch Loss: 0.6335596442222595\n",
      "Epoch 389, Loss: 2.9236114621162415, Final Batch Loss: 0.7913756370544434\n",
      "Epoch 390, Loss: 2.857555568218231, Final Batch Loss: 0.6826889514923096\n",
      "Epoch 391, Loss: 2.708699345588684, Final Batch Loss: 0.6656376123428345\n",
      "Epoch 392, Loss: 2.8752565383911133, Final Batch Loss: 0.7064304947853088\n",
      "Epoch 393, Loss: 2.940285384654999, Final Batch Loss: 0.7899928689002991\n",
      "Epoch 394, Loss: 2.811211585998535, Final Batch Loss: 0.6821405291557312\n",
      "Epoch 395, Loss: 2.864965319633484, Final Batch Loss: 0.7454625964164734\n",
      "Epoch 396, Loss: 2.7624889612197876, Final Batch Loss: 0.68145751953125\n",
      "Epoch 397, Loss: 2.8527126908302307, Final Batch Loss: 0.6339506506919861\n",
      "Epoch 398, Loss: 2.9404848217964172, Final Batch Loss: 0.7896211743354797\n",
      "Epoch 399, Loss: 2.725704550743103, Final Batch Loss: 0.6666308641433716\n",
      "Epoch 400, Loss: 2.729084074497223, Final Batch Loss: 0.6745138168334961\n",
      "Epoch 401, Loss: 2.894697427749634, Final Batch Loss: 0.6745892763137817\n",
      "Epoch 402, Loss: 2.6548821926116943, Final Batch Loss: 0.6064693331718445\n",
      "Epoch 403, Loss: 2.663084030151367, Final Batch Loss: 0.8271324038505554\n",
      "Epoch 404, Loss: 2.94231116771698, Final Batch Loss: 0.8042613863945007\n",
      "Epoch 405, Loss: 2.945107579231262, Final Batch Loss: 0.8222646117210388\n",
      "Epoch 406, Loss: 2.800225853919983, Final Batch Loss: 0.6878697872161865\n",
      "Epoch 407, Loss: 2.698260247707367, Final Batch Loss: 0.6133708357810974\n",
      "Epoch 408, Loss: 2.9438682198524475, Final Batch Loss: 0.8136808276176453\n",
      "Epoch 409, Loss: 2.7264564037323, Final Batch Loss: 0.7057784199714661\n",
      "Epoch 410, Loss: 2.769664168357849, Final Batch Loss: 0.710806667804718\n",
      "Epoch 411, Loss: 2.779573976993561, Final Batch Loss: 0.7186645865440369\n",
      "Epoch 412, Loss: 2.7578792572021484, Final Batch Loss: 0.6983195543289185\n",
      "Epoch 413, Loss: 2.6729095578193665, Final Batch Loss: 0.6035050749778748\n",
      "Epoch 414, Loss: 2.6587095260620117, Final Batch Loss: 0.5806740522384644\n",
      "Epoch 415, Loss: 2.732124388217926, Final Batch Loss: 0.7288050651550293\n",
      "Epoch 416, Loss: 2.8736844658851624, Final Batch Loss: 0.7421233654022217\n",
      "Epoch 417, Loss: 2.642153799533844, Final Batch Loss: 0.5046427249908447\n",
      "Epoch 418, Loss: 2.6727705001831055, Final Batch Loss: 0.6716838479042053\n",
      "Epoch 419, Loss: 2.701602041721344, Final Batch Loss: 0.6752403974533081\n",
      "Epoch 420, Loss: 2.6590378880500793, Final Batch Loss: 0.6098817586898804\n",
      "Epoch 421, Loss: 2.747456729412079, Final Batch Loss: 0.7203998565673828\n",
      "Epoch 422, Loss: 2.702472507953644, Final Batch Loss: 0.6678692102432251\n",
      "Epoch 423, Loss: 2.7099245190620422, Final Batch Loss: 0.7055644392967224\n",
      "Epoch 424, Loss: 2.634792447090149, Final Batch Loss: 0.6405636072158813\n",
      "Epoch 425, Loss: 2.7280927300453186, Final Batch Loss: 0.6796681880950928\n",
      "Epoch 426, Loss: 2.692536234855652, Final Batch Loss: 0.752697229385376\n",
      "Epoch 427, Loss: 2.6945531964302063, Final Batch Loss: 0.7059274315834045\n",
      "Epoch 428, Loss: 2.67383474111557, Final Batch Loss: 0.7264152765274048\n",
      "Epoch 429, Loss: 2.569755434989929, Final Batch Loss: 0.5983940362930298\n",
      "Epoch 430, Loss: 2.4767670333385468, Final Batch Loss: 0.46062591671943665\n",
      "Epoch 431, Loss: 2.550732970237732, Final Batch Loss: 0.6275064945220947\n",
      "Epoch 432, Loss: 2.65704882144928, Final Batch Loss: 0.5827206373214722\n",
      "Epoch 433, Loss: 2.6801897287368774, Final Batch Loss: 0.6714364886283875\n",
      "Epoch 434, Loss: 2.67606920003891, Final Batch Loss: 0.7240110039710999\n",
      "Epoch 435, Loss: 2.6030153632164, Final Batch Loss: 0.6643356680870056\n",
      "Epoch 436, Loss: 2.606686234474182, Final Batch Loss: 0.5687187910079956\n",
      "Epoch 437, Loss: 2.627554476261139, Final Batch Loss: 0.6019245982170105\n",
      "Epoch 438, Loss: 2.767345130443573, Final Batch Loss: 0.6277679204940796\n",
      "Epoch 439, Loss: 2.5492295622825623, Final Batch Loss: 0.5095399618148804\n",
      "Epoch 440, Loss: 2.5120072960853577, Final Batch Loss: 0.546627402305603\n",
      "Epoch 441, Loss: 2.668053984642029, Final Batch Loss: 0.7082629203796387\n",
      "Epoch 442, Loss: 2.6013843417167664, Final Batch Loss: 0.646792471408844\n",
      "Epoch 443, Loss: 2.7353638410568237, Final Batch Loss: 0.757437527179718\n",
      "Epoch 444, Loss: 2.6820746660232544, Final Batch Loss: 0.6612547636032104\n",
      "Epoch 445, Loss: 2.6476691365242004, Final Batch Loss: 0.7223531007766724\n",
      "Epoch 446, Loss: 2.5669690370559692, Final Batch Loss: 0.5732440948486328\n",
      "Epoch 447, Loss: 2.544977843761444, Final Batch Loss: 0.46202635765075684\n",
      "Epoch 448, Loss: 2.5885706543922424, Final Batch Loss: 0.7649734020233154\n",
      "Epoch 449, Loss: 2.646254062652588, Final Batch Loss: 0.68892902135849\n",
      "Epoch 450, Loss: 2.505187690258026, Final Batch Loss: 0.644883930683136\n",
      "Epoch 451, Loss: 2.603223979473114, Final Batch Loss: 0.583809494972229\n",
      "Epoch 452, Loss: 2.81453275680542, Final Batch Loss: 0.7960190773010254\n",
      "Epoch 453, Loss: 2.604446589946747, Final Batch Loss: 0.6092926263809204\n",
      "Epoch 454, Loss: 2.7471208572387695, Final Batch Loss: 0.6816896796226501\n",
      "Epoch 455, Loss: 2.6691944003105164, Final Batch Loss: 0.7378432750701904\n",
      "Epoch 456, Loss: 2.639441668987274, Final Batch Loss: 0.684133768081665\n",
      "Epoch 457, Loss: 2.590796113014221, Final Batch Loss: 0.636353075504303\n",
      "Epoch 458, Loss: 2.687127411365509, Final Batch Loss: 0.692569375038147\n",
      "Epoch 459, Loss: 2.4875917434692383, Final Batch Loss: 0.5857747197151184\n",
      "Epoch 460, Loss: 2.7037733793258667, Final Batch Loss: 0.8092638254165649\n",
      "Epoch 461, Loss: 2.6371684670448303, Final Batch Loss: 0.6584672331809998\n",
      "Epoch 462, Loss: 2.507489800453186, Final Batch Loss: 0.5790334939956665\n",
      "Epoch 463, Loss: 2.7204630970954895, Final Batch Loss: 0.700995147228241\n",
      "Epoch 464, Loss: 2.7190722227096558, Final Batch Loss: 0.8163272738456726\n",
      "Epoch 465, Loss: 2.5312699675559998, Final Batch Loss: 0.5870376229286194\n",
      "Epoch 466, Loss: 2.6412917375564575, Final Batch Loss: 0.6209421753883362\n",
      "Epoch 467, Loss: 2.663437306880951, Final Batch Loss: 0.693030834197998\n",
      "Epoch 468, Loss: 2.516739070415497, Final Batch Loss: 0.6078702807426453\n",
      "Epoch 469, Loss: 2.593526065349579, Final Batch Loss: 0.6604754328727722\n",
      "Epoch 470, Loss: 2.6107850074768066, Final Batch Loss: 0.6873175501823425\n",
      "Epoch 471, Loss: 2.5159409642219543, Final Batch Loss: 0.5873185992240906\n",
      "Epoch 472, Loss: 2.5639805793762207, Final Batch Loss: 0.6139166951179504\n",
      "Epoch 473, Loss: 2.5202041268348694, Final Batch Loss: 0.672248125076294\n",
      "Epoch 474, Loss: 2.435407340526581, Final Batch Loss: 0.6028898358345032\n",
      "Epoch 475, Loss: 2.566434860229492, Final Batch Loss: 0.6418983340263367\n",
      "Epoch 476, Loss: 2.5603612661361694, Final Batch Loss: 0.6515687108039856\n",
      "Epoch 477, Loss: 2.5231552720069885, Final Batch Loss: 0.6411764621734619\n",
      "Epoch 478, Loss: 2.6321460008621216, Final Batch Loss: 0.669971764087677\n",
      "Epoch 479, Loss: 2.6288938522338867, Final Batch Loss: 0.7536440491676331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480, Loss: 2.604950726032257, Final Batch Loss: 0.682703971862793\n",
      "Epoch 481, Loss: 2.528316915035248, Final Batch Loss: 0.747729480266571\n",
      "Epoch 482, Loss: 2.6874207258224487, Final Batch Loss: 0.6960917115211487\n",
      "Epoch 483, Loss: 2.5010933876037598, Final Batch Loss: 0.7156448364257812\n",
      "Epoch 484, Loss: 2.51448518037796, Final Batch Loss: 0.5594602823257446\n",
      "Epoch 485, Loss: 2.573279321193695, Final Batch Loss: 0.5329327583312988\n",
      "Epoch 486, Loss: 2.396823674440384, Final Batch Loss: 0.4650556147098541\n",
      "Epoch 487, Loss: 2.5239654779434204, Final Batch Loss: 0.5148017406463623\n",
      "Epoch 488, Loss: 2.596328854560852, Final Batch Loss: 0.8449313640594482\n",
      "Epoch 489, Loss: 2.5615434646606445, Final Batch Loss: 0.599432110786438\n",
      "Epoch 490, Loss: 2.5551674365997314, Final Batch Loss: 0.7122249007225037\n",
      "Epoch 491, Loss: 2.4480600357055664, Final Batch Loss: 0.5687416791915894\n",
      "Epoch 492, Loss: 2.400842010974884, Final Batch Loss: 0.6103988289833069\n",
      "Epoch 493, Loss: 2.3949485421180725, Final Batch Loss: 0.5950844883918762\n",
      "Epoch 494, Loss: 2.570080280303955, Final Batch Loss: 0.6659940481185913\n",
      "Epoch 495, Loss: 2.4775021076202393, Final Batch Loss: 0.5376464128494263\n",
      "Epoch 496, Loss: 2.400988817214966, Final Batch Loss: 0.5136167407035828\n",
      "Epoch 497, Loss: 2.518373489379883, Final Batch Loss: 0.6167095303535461\n",
      "Epoch 498, Loss: 2.4342997670173645, Final Batch Loss: 0.5941227078437805\n",
      "Epoch 499, Loss: 2.473597228527069, Final Batch Loss: 0.6314175724983215\n",
      "Epoch 500, Loss: 2.420020878314972, Final Batch Loss: 0.623953640460968\n",
      "Epoch 501, Loss: 2.514000952243805, Final Batch Loss: 0.6009505987167358\n",
      "Epoch 502, Loss: 2.5289067029953003, Final Batch Loss: 0.6429347395896912\n",
      "Epoch 503, Loss: 2.3830710649490356, Final Batch Loss: 0.5405761003494263\n",
      "Epoch 504, Loss: 2.5362749695777893, Final Batch Loss: 0.5734302997589111\n",
      "Epoch 505, Loss: 2.537194013595581, Final Batch Loss: 0.581920325756073\n",
      "Epoch 506, Loss: 2.527002692222595, Final Batch Loss: 0.6617565751075745\n",
      "Epoch 507, Loss: 2.4875702261924744, Final Batch Loss: 0.6594513654708862\n",
      "Epoch 508, Loss: 2.477118492126465, Final Batch Loss: 0.6778051853179932\n",
      "Epoch 509, Loss: 2.4689825773239136, Final Batch Loss: 0.543877124786377\n",
      "Epoch 510, Loss: 2.5031312704086304, Final Batch Loss: 0.6229308247566223\n",
      "Epoch 511, Loss: 2.4032060503959656, Final Batch Loss: 0.5604352355003357\n",
      "Epoch 512, Loss: 2.425989270210266, Final Batch Loss: 0.6783926486968994\n",
      "Epoch 513, Loss: 2.522060990333557, Final Batch Loss: 0.6743442416191101\n",
      "Epoch 514, Loss: 2.6096118092536926, Final Batch Loss: 0.7245739102363586\n",
      "Epoch 515, Loss: 2.3632526993751526, Final Batch Loss: 0.5840606689453125\n",
      "Epoch 516, Loss: 2.5006015300750732, Final Batch Loss: 0.5598953366279602\n",
      "Epoch 517, Loss: 2.4496156573295593, Final Batch Loss: 0.5987415909767151\n",
      "Epoch 518, Loss: 2.4210236072540283, Final Batch Loss: 0.5507941246032715\n",
      "Epoch 519, Loss: 2.442469835281372, Final Batch Loss: 0.5513617992401123\n",
      "Epoch 520, Loss: 2.4827505350112915, Final Batch Loss: 0.7005264759063721\n",
      "Epoch 521, Loss: 2.4693207144737244, Final Batch Loss: 0.5504942536354065\n",
      "Epoch 522, Loss: 2.5552675127983093, Final Batch Loss: 0.6671201586723328\n",
      "Epoch 523, Loss: 2.3798587322235107, Final Batch Loss: 0.6433637738227844\n",
      "Epoch 524, Loss: 2.4030794501304626, Final Batch Loss: 0.5419197678565979\n",
      "Epoch 525, Loss: 2.561112642288208, Final Batch Loss: 0.6071339845657349\n",
      "Epoch 526, Loss: 2.483921468257904, Final Batch Loss: 0.6252952218055725\n",
      "Epoch 527, Loss: 2.3711547255516052, Final Batch Loss: 0.5197213292121887\n",
      "Epoch 528, Loss: 2.388623356819153, Final Batch Loss: 0.5271227955818176\n",
      "Epoch 529, Loss: 2.3399239778518677, Final Batch Loss: 0.5855420827865601\n",
      "Epoch 530, Loss: 2.5077494978904724, Final Batch Loss: 0.5968576669692993\n",
      "Epoch 531, Loss: 2.541056990623474, Final Batch Loss: 0.6509419679641724\n",
      "Epoch 532, Loss: 2.3979406654834747, Final Batch Loss: 0.49756088852882385\n",
      "Epoch 533, Loss: 2.3483774960041046, Final Batch Loss: 0.47350701689720154\n",
      "Epoch 534, Loss: 2.4283193945884705, Final Batch Loss: 0.6725281476974487\n",
      "Epoch 535, Loss: 2.455195188522339, Final Batch Loss: 0.7483586072921753\n",
      "Epoch 536, Loss: 2.362841248512268, Final Batch Loss: 0.6048986315727234\n",
      "Epoch 537, Loss: 2.4458799362182617, Final Batch Loss: 0.5450802445411682\n",
      "Epoch 538, Loss: 2.5024073123931885, Final Batch Loss: 0.6290429830551147\n",
      "Epoch 539, Loss: 2.387326955795288, Final Batch Loss: 0.5235987305641174\n",
      "Epoch 540, Loss: 2.365422785282135, Final Batch Loss: 0.5818724036216736\n",
      "Epoch 541, Loss: 2.3712287545204163, Final Batch Loss: 0.5531190633773804\n",
      "Epoch 542, Loss: 2.3812748193740845, Final Batch Loss: 0.5846956968307495\n",
      "Epoch 543, Loss: 2.438275933265686, Final Batch Loss: 0.6306841969490051\n",
      "Epoch 544, Loss: 2.25471168756485, Final Batch Loss: 0.5316708087921143\n",
      "Epoch 545, Loss: 2.3811609148979187, Final Batch Loss: 0.5278095006942749\n",
      "Epoch 546, Loss: 2.2620128989219666, Final Batch Loss: 0.500780463218689\n",
      "Epoch 547, Loss: 2.483763635158539, Final Batch Loss: 0.6403564214706421\n",
      "Epoch 548, Loss: 2.3404954075813293, Final Batch Loss: 0.6089420318603516\n",
      "Epoch 549, Loss: 2.3452240228652954, Final Batch Loss: 0.5520907044410706\n",
      "Epoch 550, Loss: 2.332319438457489, Final Batch Loss: 0.6037967205047607\n",
      "Epoch 551, Loss: 2.378328561782837, Final Batch Loss: 0.6014649271965027\n",
      "Epoch 552, Loss: 2.4008920788764954, Final Batch Loss: 0.5969035029411316\n",
      "Epoch 553, Loss: 2.2915831804275513, Final Batch Loss: 0.5604507327079773\n",
      "Epoch 554, Loss: 2.3828529715538025, Final Batch Loss: 0.5431776642799377\n",
      "Epoch 555, Loss: 2.4506166577339172, Final Batch Loss: 0.5245026350021362\n",
      "Epoch 556, Loss: 2.290773034095764, Final Batch Loss: 0.5715485215187073\n",
      "Epoch 557, Loss: 2.434155583381653, Final Batch Loss: 0.6832941770553589\n",
      "Epoch 558, Loss: 2.38181334733963, Final Batch Loss: 0.5693836808204651\n",
      "Epoch 559, Loss: 2.3745797276496887, Final Batch Loss: 0.6550415754318237\n",
      "Epoch 560, Loss: 2.380570888519287, Final Batch Loss: 0.6421087384223938\n",
      "Epoch 561, Loss: 2.345361888408661, Final Batch Loss: 0.6079857349395752\n",
      "Epoch 562, Loss: 2.403400421142578, Final Batch Loss: 0.5426418781280518\n",
      "Epoch 563, Loss: 2.398090124130249, Final Batch Loss: 0.5267380475997925\n",
      "Epoch 564, Loss: 2.2970759868621826, Final Batch Loss: 0.5235506892204285\n",
      "Epoch 565, Loss: 2.406073033809662, Final Batch Loss: 0.6405283212661743\n",
      "Epoch 566, Loss: 2.2263005077838898, Final Batch Loss: 0.46704939007759094\n",
      "Epoch 567, Loss: 2.4019476175308228, Final Batch Loss: 0.6069138050079346\n",
      "Epoch 568, Loss: 2.3193580508232117, Final Batch Loss: 0.6129071712493896\n",
      "Epoch 569, Loss: 2.1095230281352997, Final Batch Loss: 0.37538182735443115\n",
      "Epoch 570, Loss: 2.231095016002655, Final Batch Loss: 0.5904474258422852\n",
      "Epoch 571, Loss: 2.452249586582184, Final Batch Loss: 0.645717978477478\n",
      "Epoch 572, Loss: 2.3826727867126465, Final Batch Loss: 0.6079572439193726\n",
      "Epoch 573, Loss: 2.311507225036621, Final Batch Loss: 0.5888665914535522\n",
      "Epoch 574, Loss: 2.329099714756012, Final Batch Loss: 0.6012431383132935\n",
      "Epoch 575, Loss: 2.310333490371704, Final Batch Loss: 0.5795115232467651\n",
      "Epoch 576, Loss: 2.305657684803009, Final Batch Loss: 0.6224319338798523\n",
      "Epoch 577, Loss: 2.4718042612075806, Final Batch Loss: 0.687004804611206\n",
      "Epoch 578, Loss: 2.1956869065761566, Final Batch Loss: 0.4746114909648895\n",
      "Epoch 579, Loss: 2.3646315336227417, Final Batch Loss: 0.5715888142585754\n",
      "Epoch 580, Loss: 2.284467875957489, Final Batch Loss: 0.5551789999008179\n",
      "Epoch 581, Loss: 2.239919424057007, Final Batch Loss: 0.6256153583526611\n",
      "Epoch 582, Loss: 2.3296344876289368, Final Batch Loss: 0.6041544079780579\n",
      "Epoch 583, Loss: 2.359335482120514, Final Batch Loss: 0.7258485555648804\n",
      "Epoch 584, Loss: 2.248892843723297, Final Batch Loss: 0.4915892481803894\n",
      "Epoch 585, Loss: 2.2876596450805664, Final Batch Loss: 0.5977534651756287\n",
      "Epoch 586, Loss: 2.2878627479076385, Final Batch Loss: 0.6094285845756531\n",
      "Epoch 587, Loss: 2.330874741077423, Final Batch Loss: 0.680823802947998\n",
      "Epoch 588, Loss: 2.1903014183044434, Final Batch Loss: 0.603352963924408\n",
      "Epoch 589, Loss: 2.4113531708717346, Final Batch Loss: 0.6520927548408508\n",
      "Epoch 590, Loss: 2.297246277332306, Final Batch Loss: 0.6679040193557739\n",
      "Epoch 591, Loss: 2.277705490589142, Final Batch Loss: 0.6604048609733582\n",
      "Epoch 592, Loss: 2.226285934448242, Final Batch Loss: 0.4999353885650635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 593, Loss: 2.3362648487091064, Final Batch Loss: 0.6479887366294861\n",
      "Epoch 594, Loss: 2.2089000940322876, Final Batch Loss: 0.5803834795951843\n",
      "Epoch 595, Loss: 2.2883410155773163, Final Batch Loss: 0.6052349209785461\n",
      "Epoch 596, Loss: 2.3689993619918823, Final Batch Loss: 0.6247851848602295\n",
      "Epoch 597, Loss: 2.1770834624767303, Final Batch Loss: 0.4610605537891388\n",
      "Epoch 598, Loss: 2.237056791782379, Final Batch Loss: 0.532462477684021\n",
      "Epoch 599, Loss: 2.2794530987739563, Final Batch Loss: 0.5516117811203003\n",
      "Epoch 600, Loss: 2.2316310703754425, Final Batch Loss: 0.4930417239665985\n",
      "Epoch 601, Loss: 2.2294393181800842, Final Batch Loss: 0.5573912262916565\n",
      "Epoch 602, Loss: 2.330112099647522, Final Batch Loss: 0.6500775814056396\n",
      "Epoch 603, Loss: 2.2140761017799377, Final Batch Loss: 0.5752606987953186\n",
      "Epoch 604, Loss: 2.236133873462677, Final Batch Loss: 0.5846093893051147\n",
      "Epoch 605, Loss: 2.317681133747101, Final Batch Loss: 0.6823776960372925\n",
      "Epoch 606, Loss: 2.3837989270687103, Final Batch Loss: 0.6983838677406311\n",
      "Epoch 607, Loss: 2.285755753517151, Final Batch Loss: 0.572240948677063\n",
      "Epoch 608, Loss: 2.3034108877182007, Final Batch Loss: 0.616402804851532\n",
      "Epoch 609, Loss: 2.2935396432876587, Final Batch Loss: 0.49300169944763184\n",
      "Epoch 610, Loss: 2.1772809624671936, Final Batch Loss: 0.4981597661972046\n",
      "Epoch 611, Loss: 2.2968520522117615, Final Batch Loss: 0.6128712892532349\n",
      "Epoch 612, Loss: 2.2621540427207947, Final Batch Loss: 0.6061335206031799\n",
      "Epoch 613, Loss: 2.376625895500183, Final Batch Loss: 0.5782291889190674\n",
      "Epoch 614, Loss: 2.244434654712677, Final Batch Loss: 0.5048390030860901\n",
      "Epoch 615, Loss: 2.2220849990844727, Final Batch Loss: 0.5984266996383667\n",
      "Epoch 616, Loss: 2.1218188107013702, Final Batch Loss: 0.47464731335639954\n",
      "Epoch 617, Loss: 2.162487894296646, Final Batch Loss: 0.6318575143814087\n",
      "Epoch 618, Loss: 2.1937320828437805, Final Batch Loss: 0.569824755191803\n",
      "Epoch 619, Loss: 2.2660756707191467, Final Batch Loss: 0.6465701460838318\n",
      "Epoch 620, Loss: 2.2851656675338745, Final Batch Loss: 0.6664210557937622\n",
      "Epoch 621, Loss: 2.3162362575531006, Final Batch Loss: 0.5262872576713562\n",
      "Epoch 622, Loss: 2.2591075003147125, Final Batch Loss: 0.5779388546943665\n",
      "Epoch 623, Loss: 2.2484917640686035, Final Batch Loss: 0.5536617636680603\n",
      "Epoch 624, Loss: 2.234645962715149, Final Batch Loss: 0.5864701867103577\n",
      "Epoch 625, Loss: 2.15887188911438, Final Batch Loss: 0.5723984241485596\n",
      "Epoch 626, Loss: 2.1918137669563293, Final Batch Loss: 0.5265071392059326\n",
      "Epoch 627, Loss: 2.1832315325737, Final Batch Loss: 0.5657051205635071\n",
      "Epoch 628, Loss: 2.2767584323883057, Final Batch Loss: 0.5741192102432251\n",
      "Epoch 629, Loss: 2.2806522250175476, Final Batch Loss: 0.5871111154556274\n",
      "Epoch 630, Loss: 2.272084593772888, Final Batch Loss: 0.5905402898788452\n",
      "Epoch 631, Loss: 2.1792572140693665, Final Batch Loss: 0.5598440170288086\n",
      "Epoch 632, Loss: 2.177687883377075, Final Batch Loss: 0.6083380579948425\n",
      "Epoch 633, Loss: 2.189159482717514, Final Batch Loss: 0.47251585125923157\n",
      "Epoch 634, Loss: 2.2348111271858215, Final Batch Loss: 0.5279108285903931\n",
      "Epoch 635, Loss: 2.276257574558258, Final Batch Loss: 0.6362446546554565\n",
      "Epoch 636, Loss: 2.156195282936096, Final Batch Loss: 0.5035090446472168\n",
      "Epoch 637, Loss: 2.325175166130066, Final Batch Loss: 0.6772035360336304\n",
      "Epoch 638, Loss: 2.119585007429123, Final Batch Loss: 0.44617024064064026\n",
      "Epoch 639, Loss: 2.153622627258301, Final Batch Loss: 0.4914764165878296\n",
      "Epoch 640, Loss: 2.1955132484436035, Final Batch Loss: 0.5071194171905518\n",
      "Epoch 641, Loss: 2.20062655210495, Final Batch Loss: 0.5710837244987488\n",
      "Epoch 642, Loss: 2.0005096793174744, Final Batch Loss: 0.48287519812583923\n",
      "Epoch 643, Loss: 2.2165393233299255, Final Batch Loss: 0.6458187103271484\n",
      "Epoch 644, Loss: 2.1187165081501007, Final Batch Loss: 0.5431832075119019\n",
      "Epoch 645, Loss: 2.0931158363819122, Final Batch Loss: 0.4797689616680145\n",
      "Epoch 646, Loss: 2.2421414852142334, Final Batch Loss: 0.6487619876861572\n",
      "Epoch 647, Loss: 2.141452133655548, Final Batch Loss: 0.5086597204208374\n",
      "Epoch 648, Loss: 2.1502456963062286, Final Batch Loss: 0.6033424735069275\n",
      "Epoch 649, Loss: 2.267735540866852, Final Batch Loss: 0.6201384663581848\n",
      "Epoch 650, Loss: 2.1317652463912964, Final Batch Loss: 0.5695362687110901\n",
      "Epoch 651, Loss: 2.180236577987671, Final Batch Loss: 0.5389509201049805\n",
      "Epoch 652, Loss: 2.1857519149780273, Final Batch Loss: 0.5832200050354004\n",
      "Epoch 653, Loss: 2.2787009477615356, Final Batch Loss: 0.5175678730010986\n",
      "Epoch 654, Loss: 2.1095745265483856, Final Batch Loss: 0.5239236354827881\n",
      "Epoch 655, Loss: 2.1837320625782013, Final Batch Loss: 0.44094178080558777\n",
      "Epoch 656, Loss: 2.1162282526493073, Final Batch Loss: 0.5328744649887085\n",
      "Epoch 657, Loss: 2.2618881464004517, Final Batch Loss: 0.6120463609695435\n",
      "Epoch 658, Loss: 2.2204343676567078, Final Batch Loss: 0.5265377163887024\n",
      "Epoch 659, Loss: 2.1216780245304108, Final Batch Loss: 0.46206268668174744\n",
      "Epoch 660, Loss: 2.148340106010437, Final Batch Loss: 0.45675724744796753\n",
      "Epoch 661, Loss: 2.1272398233413696, Final Batch Loss: 0.5570198893547058\n",
      "Epoch 662, Loss: 2.178354024887085, Final Batch Loss: 0.551976203918457\n",
      "Epoch 663, Loss: 2.1638264656066895, Final Batch Loss: 0.5277129411697388\n",
      "Epoch 664, Loss: 2.2873489558696747, Final Batch Loss: 0.6526636481285095\n",
      "Epoch 665, Loss: 2.2548713088035583, Final Batch Loss: 0.7074515223503113\n",
      "Epoch 666, Loss: 2.1162338256835938, Final Batch Loss: 0.5509521961212158\n",
      "Epoch 667, Loss: 2.0855196118354797, Final Batch Loss: 0.5706002116203308\n",
      "Epoch 668, Loss: 1.9638454020023346, Final Batch Loss: 0.46504852175712585\n",
      "Epoch 669, Loss: 2.050064116716385, Final Batch Loss: 0.47764852643013\n",
      "Epoch 670, Loss: 2.1275524497032166, Final Batch Loss: 0.47119003534317017\n",
      "Epoch 671, Loss: 2.1320641934871674, Final Batch Loss: 0.5863825678825378\n",
      "Epoch 672, Loss: 2.1562837064266205, Final Batch Loss: 0.5928057432174683\n",
      "Epoch 673, Loss: 2.1837462782859802, Final Batch Loss: 0.48111873865127563\n",
      "Epoch 674, Loss: 2.2200878858566284, Final Batch Loss: 0.588117778301239\n",
      "Epoch 675, Loss: 2.0373935103416443, Final Batch Loss: 0.531059205532074\n",
      "Epoch 676, Loss: 2.193498194217682, Final Batch Loss: 0.567929744720459\n",
      "Epoch 677, Loss: 2.138181060552597, Final Batch Loss: 0.4748808443546295\n",
      "Epoch 678, Loss: 2.2659959197044373, Final Batch Loss: 0.6134271621704102\n",
      "Epoch 679, Loss: 2.089788168668747, Final Batch Loss: 0.5318160057067871\n",
      "Epoch 680, Loss: 2.163390874862671, Final Batch Loss: 0.5102294087409973\n",
      "Epoch 681, Loss: 2.2590757608413696, Final Batch Loss: 0.5279204845428467\n",
      "Epoch 682, Loss: 2.1331765949726105, Final Batch Loss: 0.4883166253566742\n",
      "Epoch 683, Loss: 2.061067521572113, Final Batch Loss: 0.5103636980056763\n",
      "Epoch 684, Loss: 2.057187706232071, Final Batch Loss: 0.5396476984024048\n",
      "Epoch 685, Loss: 2.042722225189209, Final Batch Loss: 0.563420295715332\n",
      "Epoch 686, Loss: 2.1086626648902893, Final Batch Loss: 0.526423454284668\n",
      "Epoch 687, Loss: 2.1745097637176514, Final Batch Loss: 0.5804951190948486\n",
      "Epoch 688, Loss: 2.0843931436538696, Final Batch Loss: 0.519922137260437\n",
      "Epoch 689, Loss: 2.0140028595924377, Final Batch Loss: 0.5728365182876587\n",
      "Epoch 690, Loss: 2.1650939881801605, Final Batch Loss: 0.5562840700149536\n",
      "Epoch 691, Loss: 2.0564759373664856, Final Batch Loss: 0.5412177443504333\n",
      "Epoch 692, Loss: 2.085197687149048, Final Batch Loss: 0.5085764527320862\n",
      "Epoch 693, Loss: 2.163721591234207, Final Batch Loss: 0.5609080791473389\n",
      "Epoch 694, Loss: 2.2791459560394287, Final Batch Loss: 0.6717914938926697\n",
      "Epoch 695, Loss: 2.0663799345493317, Final Batch Loss: 0.4353252649307251\n",
      "Epoch 696, Loss: 2.043027877807617, Final Batch Loss: 0.49546390771865845\n",
      "Epoch 697, Loss: 2.1843058466911316, Final Batch Loss: 0.5049161314964294\n",
      "Epoch 698, Loss: 2.107142060995102, Final Batch Loss: 0.5638748407363892\n",
      "Epoch 699, Loss: 2.0734749734401703, Final Batch Loss: 0.5583945512771606\n",
      "Epoch 700, Loss: 2.2412395775318146, Final Batch Loss: 0.6115308403968811\n",
      "Epoch 701, Loss: 2.2700551748275757, Final Batch Loss: 0.6734852194786072\n",
      "Epoch 702, Loss: 2.025678515434265, Final Batch Loss: 0.4371854364871979\n",
      "Epoch 703, Loss: 2.134409010410309, Final Batch Loss: 0.577595591545105\n",
      "Epoch 704, Loss: 2.103665590286255, Final Batch Loss: 0.508001446723938\n",
      "Epoch 705, Loss: 2.08316233754158, Final Batch Loss: 0.48136892914772034\n",
      "Epoch 706, Loss: 2.2026787400245667, Final Batch Loss: 0.538654088973999\n",
      "Epoch 707, Loss: 2.08521831035614, Final Batch Loss: 0.5182490944862366\n",
      "Epoch 708, Loss: 2.061661422252655, Final Batch Loss: 0.5188546776771545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 709, Loss: 2.1349990367889404, Final Batch Loss: 0.5629603862762451\n",
      "Epoch 710, Loss: 2.089241921901703, Final Batch Loss: 0.5360115170478821\n",
      "Epoch 711, Loss: 2.0413854718208313, Final Batch Loss: 0.4893907308578491\n",
      "Epoch 712, Loss: 2.1089271903038025, Final Batch Loss: 0.42911219596862793\n",
      "Epoch 713, Loss: 2.0622985661029816, Final Batch Loss: 0.49544963240623474\n",
      "Epoch 714, Loss: 1.9340402781963348, Final Batch Loss: 0.49511653184890747\n",
      "Epoch 715, Loss: 2.0824490785598755, Final Batch Loss: 0.628005862236023\n",
      "Epoch 716, Loss: 2.017549604177475, Final Batch Loss: 0.4751424491405487\n",
      "Epoch 717, Loss: 1.951129287481308, Final Batch Loss: 0.4926178753376007\n",
      "Epoch 718, Loss: 2.019603908061981, Final Batch Loss: 0.4563901126384735\n",
      "Epoch 719, Loss: 2.14053612947464, Final Batch Loss: 0.6069933772087097\n",
      "Epoch 720, Loss: 2.146846652030945, Final Batch Loss: 0.5611493587493896\n",
      "Epoch 721, Loss: 2.0314164757728577, Final Batch Loss: 0.43914562463760376\n",
      "Epoch 722, Loss: 2.135922372341156, Final Batch Loss: 0.4663374423980713\n",
      "Epoch 723, Loss: 2.06877937912941, Final Batch Loss: 0.4900933802127838\n",
      "Epoch 724, Loss: 2.2084795236587524, Final Batch Loss: 0.5702697038650513\n",
      "Epoch 725, Loss: 2.0360741019248962, Final Batch Loss: 0.45841091871261597\n",
      "Epoch 726, Loss: 2.0262550115585327, Final Batch Loss: 0.4643126130104065\n",
      "Epoch 727, Loss: 2.160276710987091, Final Batch Loss: 0.5683399438858032\n",
      "Epoch 728, Loss: 2.086041122674942, Final Batch Loss: 0.5441802144050598\n",
      "Epoch 729, Loss: 2.1255997717380524, Final Batch Loss: 0.5780487656593323\n",
      "Epoch 730, Loss: 2.231369733810425, Final Batch Loss: 0.557853102684021\n",
      "Epoch 731, Loss: 1.9507322907447815, Final Batch Loss: 0.575503945350647\n",
      "Epoch 732, Loss: 2.1716510355472565, Final Batch Loss: 0.6707722544670105\n",
      "Epoch 733, Loss: 1.9238021075725555, Final Batch Loss: 0.4782180190086365\n",
      "Epoch 734, Loss: 1.9197121858596802, Final Batch Loss: 0.44112905859947205\n",
      "Epoch 735, Loss: 2.1641667783260345, Final Batch Loss: 0.5393926501274109\n",
      "Epoch 736, Loss: 2.1992789804935455, Final Batch Loss: 0.6345840692520142\n",
      "Epoch 737, Loss: 2.050044059753418, Final Batch Loss: 0.5724590420722961\n",
      "Epoch 738, Loss: 2.1405006051063538, Final Batch Loss: 0.5909942388534546\n",
      "Epoch 739, Loss: 1.9525335729122162, Final Batch Loss: 0.542622983455658\n",
      "Epoch 740, Loss: 1.9971205592155457, Final Batch Loss: 0.48039722442626953\n",
      "Epoch 741, Loss: 2.103638857603073, Final Batch Loss: 0.5277239680290222\n",
      "Epoch 742, Loss: 2.0836388170719147, Final Batch Loss: 0.5761324763298035\n",
      "Epoch 743, Loss: 2.0363181829452515, Final Batch Loss: 0.5147280693054199\n",
      "Epoch 744, Loss: 1.9699470400810242, Final Batch Loss: 0.4961870610713959\n",
      "Epoch 745, Loss: 2.055238217115402, Final Batch Loss: 0.5363779664039612\n",
      "Epoch 746, Loss: 2.0188779532909393, Final Batch Loss: 0.534390389919281\n",
      "Epoch 747, Loss: 2.251881092786789, Final Batch Loss: 0.6164238452911377\n",
      "Epoch 748, Loss: 1.9591780602931976, Final Batch Loss: 0.40164169669151306\n",
      "Epoch 749, Loss: 2.079534500837326, Final Batch Loss: 0.5380014181137085\n",
      "Epoch 750, Loss: 2.016660749912262, Final Batch Loss: 0.48675337433815\n",
      "Epoch 751, Loss: 2.018642872571945, Final Batch Loss: 0.4620395302772522\n",
      "Epoch 752, Loss: 1.9419649839401245, Final Batch Loss: 0.42680415511131287\n",
      "Epoch 753, Loss: 2.099861294031143, Final Batch Loss: 0.5749396681785583\n",
      "Epoch 754, Loss: 1.9949783980846405, Final Batch Loss: 0.4960814118385315\n",
      "Epoch 755, Loss: 2.1640470921993256, Final Batch Loss: 0.5826449990272522\n",
      "Epoch 756, Loss: 2.0077004730701447, Final Batch Loss: 0.5105545520782471\n",
      "Epoch 757, Loss: 1.9662222266197205, Final Batch Loss: 0.5291693210601807\n",
      "Epoch 758, Loss: 2.0071537792682648, Final Batch Loss: 0.46074023842811584\n",
      "Epoch 759, Loss: 1.9181634485721588, Final Batch Loss: 0.517063558101654\n",
      "Epoch 760, Loss: 2.092058151960373, Final Batch Loss: 0.4761493504047394\n",
      "Epoch 761, Loss: 1.9169239103794098, Final Batch Loss: 0.3974306285381317\n",
      "Epoch 762, Loss: 2.061684936285019, Final Batch Loss: 0.5042831897735596\n",
      "Epoch 763, Loss: 2.0620998442173004, Final Batch Loss: 0.5642023086547852\n",
      "Epoch 764, Loss: 1.8350083231925964, Final Batch Loss: 0.42682236433029175\n",
      "Epoch 765, Loss: 1.9234678745269775, Final Batch Loss: 0.4397701621055603\n",
      "Epoch 766, Loss: 1.791424810886383, Final Batch Loss: 0.4214620888233185\n",
      "Epoch 767, Loss: 2.0636879205703735, Final Batch Loss: 0.5012128949165344\n",
      "Epoch 768, Loss: 2.019705682992935, Final Batch Loss: 0.5990467071533203\n",
      "Epoch 769, Loss: 2.0243768393993378, Final Batch Loss: 0.5948163270950317\n",
      "Epoch 770, Loss: 2.0607150197029114, Final Batch Loss: 0.4597073793411255\n",
      "Epoch 771, Loss: 2.0709275901317596, Final Batch Loss: 0.5112302899360657\n",
      "Epoch 772, Loss: 2.131430745124817, Final Batch Loss: 0.5921934843063354\n",
      "Epoch 773, Loss: 1.9576554894447327, Final Batch Loss: 0.4692937731742859\n",
      "Epoch 774, Loss: 1.9623367488384247, Final Batch Loss: 0.588649570941925\n",
      "Epoch 775, Loss: 1.917502224445343, Final Batch Loss: 0.4439578950405121\n",
      "Epoch 776, Loss: 1.8733809292316437, Final Batch Loss: 0.430497407913208\n",
      "Epoch 777, Loss: 1.8574461340904236, Final Batch Loss: 0.527240514755249\n",
      "Epoch 778, Loss: 1.959082543849945, Final Batch Loss: 0.4569760262966156\n",
      "Epoch 779, Loss: 2.007942110300064, Final Batch Loss: 0.48540833592414856\n",
      "Epoch 780, Loss: 2.009397655725479, Final Batch Loss: 0.5483276844024658\n",
      "Epoch 781, Loss: 1.9971565306186676, Final Batch Loss: 0.5064677596092224\n",
      "Epoch 782, Loss: 1.9200759530067444, Final Batch Loss: 0.5071865916252136\n",
      "Epoch 783, Loss: 1.8613640666007996, Final Batch Loss: 0.4570114314556122\n",
      "Epoch 784, Loss: 1.9202583134174347, Final Batch Loss: 0.4929892420768738\n",
      "Epoch 785, Loss: 1.9397447109222412, Final Batch Loss: 0.5040169358253479\n",
      "Epoch 786, Loss: 1.9398018419742584, Final Batch Loss: 0.4493393003940582\n",
      "Epoch 787, Loss: 1.9149331748485565, Final Batch Loss: 0.44374796748161316\n",
      "Epoch 788, Loss: 1.917620837688446, Final Batch Loss: 0.4976992905139923\n",
      "Epoch 789, Loss: 1.9628265798091888, Final Batch Loss: 0.556339681148529\n",
      "Epoch 790, Loss: 1.942068636417389, Final Batch Loss: 0.5122795701026917\n",
      "Epoch 791, Loss: 1.9757429659366608, Final Batch Loss: 0.4820592999458313\n",
      "Epoch 792, Loss: 2.070855677127838, Final Batch Loss: 0.5551871061325073\n",
      "Epoch 793, Loss: 1.9633313119411469, Final Batch Loss: 0.4514204263687134\n",
      "Epoch 794, Loss: 1.8957476019859314, Final Batch Loss: 0.5183935761451721\n",
      "Epoch 795, Loss: 1.9579831063747406, Final Batch Loss: 0.40301668643951416\n",
      "Epoch 796, Loss: 1.9369146525859833, Final Batch Loss: 0.4754345417022705\n",
      "Epoch 797, Loss: 1.9844803512096405, Final Batch Loss: 0.5104178190231323\n",
      "Epoch 798, Loss: 2.0504140853881836, Final Batch Loss: 0.4973769783973694\n",
      "Epoch 799, Loss: 1.9488039016723633, Final Batch Loss: 0.4714246988296509\n",
      "Epoch 800, Loss: 2.04996919631958, Final Batch Loss: 0.5461286306381226\n",
      "Epoch 801, Loss: 1.9096566140651703, Final Batch Loss: 0.4995025396347046\n",
      "Epoch 802, Loss: 1.881371945142746, Final Batch Loss: 0.4844411611557007\n",
      "Epoch 803, Loss: 1.9120053350925446, Final Batch Loss: 0.43726664781570435\n",
      "Epoch 804, Loss: 2.0470277965068817, Final Batch Loss: 0.5273013710975647\n",
      "Epoch 805, Loss: 1.9198005199432373, Final Batch Loss: 0.47374361753463745\n",
      "Epoch 806, Loss: 1.9518687427043915, Final Batch Loss: 0.5480960011482239\n",
      "Epoch 807, Loss: 1.818310022354126, Final Batch Loss: 0.3839849531650543\n",
      "Epoch 808, Loss: 1.9926532208919525, Final Batch Loss: 0.5163512229919434\n",
      "Epoch 809, Loss: 1.875479519367218, Final Batch Loss: 0.40298736095428467\n",
      "Epoch 810, Loss: 1.9289846122264862, Final Batch Loss: 0.4092220366001129\n",
      "Epoch 811, Loss: 1.92826709151268, Final Batch Loss: 0.404789537191391\n",
      "Epoch 812, Loss: 2.0311180353164673, Final Batch Loss: 0.5190207958221436\n",
      "Epoch 813, Loss: 2.064532160758972, Final Batch Loss: 0.5609073042869568\n",
      "Epoch 814, Loss: 1.931709736585617, Final Batch Loss: 0.5086487531661987\n",
      "Epoch 815, Loss: 1.9855910241603851, Final Batch Loss: 0.48821696639060974\n",
      "Epoch 816, Loss: 1.888759732246399, Final Batch Loss: 0.4705195426940918\n",
      "Epoch 817, Loss: 2.0092495679855347, Final Batch Loss: 0.4984573423862457\n",
      "Epoch 818, Loss: 1.8155744969844818, Final Batch Loss: 0.37176960706710815\n",
      "Epoch 819, Loss: 1.8500046133995056, Final Batch Loss: 0.5283128619194031\n",
      "Epoch 820, Loss: 1.828613817691803, Final Batch Loss: 0.48572131991386414\n",
      "Epoch 821, Loss: 1.9702558815479279, Final Batch Loss: 0.4442480802536011\n",
      "Epoch 822, Loss: 2.0016639828681946, Final Batch Loss: 0.5103757381439209\n",
      "Epoch 823, Loss: 1.859634280204773, Final Batch Loss: 0.40191754698753357\n",
      "Epoch 824, Loss: 1.9552035331726074, Final Batch Loss: 0.49272865056991577\n",
      "Epoch 825, Loss: 1.9677481949329376, Final Batch Loss: 0.48383793234825134\n",
      "Epoch 826, Loss: 1.8303985297679901, Final Batch Loss: 0.4426545798778534\n",
      "Epoch 827, Loss: 1.827176958322525, Final Batch Loss: 0.40912729501724243\n",
      "Epoch 828, Loss: 1.8674439489841461, Final Batch Loss: 0.4379558265209198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 829, Loss: 1.9563463628292084, Final Batch Loss: 0.4543167054653168\n",
      "Epoch 830, Loss: 1.8073405027389526, Final Batch Loss: 0.40083256363868713\n",
      "Epoch 831, Loss: 1.8923964500427246, Final Batch Loss: 0.42097803950309753\n",
      "Epoch 832, Loss: 1.830840289592743, Final Batch Loss: 0.47074002027511597\n",
      "Epoch 833, Loss: 1.8360697627067566, Final Batch Loss: 0.46356329321861267\n",
      "Epoch 834, Loss: 1.9790136218070984, Final Batch Loss: 0.4506000578403473\n",
      "Epoch 835, Loss: 1.9132872819900513, Final Batch Loss: 0.46302345395088196\n",
      "Epoch 836, Loss: 1.957951843738556, Final Batch Loss: 0.42697903513908386\n",
      "Epoch 837, Loss: 1.934652417898178, Final Batch Loss: 0.4619993567466736\n",
      "Epoch 838, Loss: 2.022989183664322, Final Batch Loss: 0.6305065751075745\n",
      "Epoch 839, Loss: 1.9012122750282288, Final Batch Loss: 0.5561623573303223\n",
      "Epoch 840, Loss: 1.9716505408287048, Final Batch Loss: 0.5752500295639038\n",
      "Epoch 841, Loss: 1.8620086908340454, Final Batch Loss: 0.4136582612991333\n",
      "Epoch 842, Loss: 1.9435869455337524, Final Batch Loss: 0.4363267719745636\n",
      "Epoch 843, Loss: 1.7380748093128204, Final Batch Loss: 0.34365639090538025\n",
      "Epoch 844, Loss: 1.9163607954978943, Final Batch Loss: 0.5474231243133545\n",
      "Epoch 845, Loss: 1.8981044590473175, Final Batch Loss: 0.36834537982940674\n",
      "Epoch 846, Loss: 1.7563067078590393, Final Batch Loss: 0.44095098972320557\n",
      "Epoch 847, Loss: 1.8592596650123596, Final Batch Loss: 0.3628023862838745\n",
      "Epoch 848, Loss: 1.7923602163791656, Final Batch Loss: 0.3917452394962311\n",
      "Epoch 849, Loss: 1.789076566696167, Final Batch Loss: 0.40523019433021545\n",
      "Epoch 850, Loss: 1.842874437570572, Final Batch Loss: 0.44525811076164246\n",
      "Epoch 851, Loss: 1.910603016614914, Final Batch Loss: 0.5513557195663452\n",
      "Epoch 852, Loss: 1.8101802468299866, Final Batch Loss: 0.3875410854816437\n",
      "Epoch 853, Loss: 2.00428506731987, Final Batch Loss: 0.5782890915870667\n",
      "Epoch 854, Loss: 1.992427945137024, Final Batch Loss: 0.5064724087715149\n",
      "Epoch 855, Loss: 1.8410929143428802, Final Batch Loss: 0.4251934587955475\n",
      "Epoch 856, Loss: 1.8410874903202057, Final Batch Loss: 0.48812970519065857\n",
      "Epoch 857, Loss: 1.743420422077179, Final Batch Loss: 0.4038756191730499\n",
      "Epoch 858, Loss: 1.8213455975055695, Final Batch Loss: 0.489963561296463\n",
      "Epoch 859, Loss: 1.850059688091278, Final Batch Loss: 0.5452234745025635\n",
      "Epoch 860, Loss: 1.9308484196662903, Final Batch Loss: 0.40347498655319214\n",
      "Epoch 861, Loss: 1.983637809753418, Final Batch Loss: 0.5329716801643372\n",
      "Epoch 862, Loss: 1.8013764917850494, Final Batch Loss: 0.37466415762901306\n",
      "Epoch 863, Loss: 1.6732327938079834, Final Batch Loss: 0.36211085319519043\n",
      "Epoch 864, Loss: 1.7584856450557709, Final Batch Loss: 0.3732846677303314\n",
      "Epoch 865, Loss: 1.807295709848404, Final Batch Loss: 0.37991175055503845\n",
      "Epoch 866, Loss: 1.8731604218482971, Final Batch Loss: 0.5721255540847778\n",
      "Epoch 867, Loss: 1.7766660749912262, Final Batch Loss: 0.4248337149620056\n",
      "Epoch 868, Loss: 1.8531665205955505, Final Batch Loss: 0.500761091709137\n",
      "Epoch 869, Loss: 1.812354564666748, Final Batch Loss: 0.4495687186717987\n",
      "Epoch 870, Loss: 1.8977258503437042, Final Batch Loss: 0.4609736204147339\n",
      "Epoch 871, Loss: 1.886072963476181, Final Batch Loss: 0.5373771786689758\n",
      "Epoch 872, Loss: 1.8736901581287384, Final Batch Loss: 0.5084902048110962\n",
      "Epoch 873, Loss: 1.7796463072299957, Final Batch Loss: 0.4588488042354584\n",
      "Epoch 874, Loss: 1.924930214881897, Final Batch Loss: 0.5311830043792725\n",
      "Epoch 875, Loss: 1.880300372838974, Final Batch Loss: 0.5766510963439941\n",
      "Epoch 876, Loss: 1.8967073559761047, Final Batch Loss: 0.5096364617347717\n",
      "Epoch 877, Loss: 1.92979696393013, Final Batch Loss: 0.48111096024513245\n",
      "Epoch 878, Loss: 1.6855885088443756, Final Batch Loss: 0.33487120270729065\n",
      "Epoch 879, Loss: 1.8333563208580017, Final Batch Loss: 0.4803427457809448\n",
      "Epoch 880, Loss: 1.744393914937973, Final Batch Loss: 0.4143245220184326\n",
      "Epoch 881, Loss: 1.778906226158142, Final Batch Loss: 0.34815341234207153\n",
      "Epoch 882, Loss: 1.8472287058830261, Final Batch Loss: 0.43850356340408325\n",
      "Epoch 883, Loss: 1.791610211133957, Final Batch Loss: 0.33937838673591614\n",
      "Epoch 884, Loss: 2.055887132883072, Final Batch Loss: 0.571355402469635\n",
      "Epoch 885, Loss: 1.9142667949199677, Final Batch Loss: 0.5349956154823303\n",
      "Epoch 886, Loss: 1.9083138406276703, Final Batch Loss: 0.5880458950996399\n",
      "Epoch 887, Loss: 1.8528368175029755, Final Batch Loss: 0.5256347060203552\n",
      "Epoch 888, Loss: 1.905039370059967, Final Batch Loss: 0.5885760188102722\n",
      "Epoch 889, Loss: 1.7645659148693085, Final Batch Loss: 0.44418883323669434\n",
      "Epoch 890, Loss: 2.001518964767456, Final Batch Loss: 0.5678683519363403\n",
      "Epoch 891, Loss: 1.8244278132915497, Final Batch Loss: 0.5376966595649719\n",
      "Epoch 892, Loss: 2.0828032195568085, Final Batch Loss: 0.5394434332847595\n",
      "Epoch 893, Loss: 1.8057496547698975, Final Batch Loss: 0.4192977249622345\n",
      "Epoch 894, Loss: 1.7830362617969513, Final Batch Loss: 0.4534258544445038\n",
      "Epoch 895, Loss: 1.878497302532196, Final Batch Loss: 0.37994951009750366\n",
      "Epoch 896, Loss: 1.8729457557201385, Final Batch Loss: 0.47000494599342346\n",
      "Epoch 897, Loss: 1.919526755809784, Final Batch Loss: 0.3843286335468292\n",
      "Epoch 898, Loss: 1.802497148513794, Final Batch Loss: 0.47998762130737305\n",
      "Epoch 899, Loss: 1.7762145698070526, Final Batch Loss: 0.48876431584358215\n",
      "Epoch 900, Loss: 1.8047496676445007, Final Batch Loss: 0.5229821801185608\n",
      "Epoch 901, Loss: 1.9244875609874725, Final Batch Loss: 0.5308182239532471\n",
      "Epoch 902, Loss: 2.0190696716308594, Final Batch Loss: 0.5318873524665833\n",
      "Epoch 903, Loss: 1.9037113189697266, Final Batch Loss: 0.538588285446167\n",
      "Epoch 904, Loss: 1.658613532781601, Final Batch Loss: 0.37056007981300354\n",
      "Epoch 905, Loss: 1.9539209306240082, Final Batch Loss: 0.5250537991523743\n",
      "Epoch 906, Loss: 1.8355564177036285, Final Batch Loss: 0.36318519711494446\n",
      "Epoch 907, Loss: 1.8925427198410034, Final Batch Loss: 0.5553479194641113\n",
      "Epoch 908, Loss: 1.7925134003162384, Final Batch Loss: 0.3845316171646118\n",
      "Epoch 909, Loss: 1.839048445224762, Final Batch Loss: 0.5244593024253845\n",
      "Epoch 910, Loss: 1.853282481431961, Final Batch Loss: 0.5626876354217529\n",
      "Epoch 911, Loss: 1.8812488317489624, Final Batch Loss: 0.4699333608150482\n",
      "Epoch 912, Loss: 1.800499439239502, Final Batch Loss: 0.4195244312286377\n",
      "Epoch 913, Loss: 1.8617273271083832, Final Batch Loss: 0.44091176986694336\n",
      "Epoch 914, Loss: 1.9010920226573944, Final Batch Loss: 0.4592020809650421\n",
      "Epoch 915, Loss: 1.8336267173290253, Final Batch Loss: 0.4529380798339844\n",
      "Epoch 916, Loss: 1.8849270641803741, Final Batch Loss: 0.48271554708480835\n",
      "Epoch 917, Loss: 1.767703652381897, Final Batch Loss: 0.4005451798439026\n",
      "Epoch 918, Loss: 1.7360205054283142, Final Batch Loss: 0.3780398666858673\n",
      "Epoch 919, Loss: 1.7293529212474823, Final Batch Loss: 0.4516611099243164\n",
      "Epoch 920, Loss: 1.761679083108902, Final Batch Loss: 0.4301299750804901\n",
      "Epoch 921, Loss: 1.8464659452438354, Final Batch Loss: 0.45536720752716064\n",
      "Epoch 922, Loss: 1.849903643131256, Final Batch Loss: 0.4478131830692291\n",
      "Epoch 923, Loss: 1.79935622215271, Final Batch Loss: 0.41532114148139954\n",
      "Epoch 924, Loss: 1.825963020324707, Final Batch Loss: 0.475753515958786\n",
      "Epoch 925, Loss: 1.9241425096988678, Final Batch Loss: 0.47450125217437744\n",
      "Epoch 926, Loss: 1.8937532603740692, Final Batch Loss: 0.4932299852371216\n",
      "Epoch 927, Loss: 1.907386600971222, Final Batch Loss: 0.5221081972122192\n",
      "Epoch 928, Loss: 1.6483447849750519, Final Batch Loss: 0.40750718116760254\n",
      "Epoch 929, Loss: 1.785471796989441, Final Batch Loss: 0.45574599504470825\n",
      "Epoch 930, Loss: 1.7229737341403961, Final Batch Loss: 0.44018223881721497\n",
      "Epoch 931, Loss: 1.8172511160373688, Final Batch Loss: 0.47614786028862\n",
      "Epoch 932, Loss: 1.7069206833839417, Final Batch Loss: 0.33087602257728577\n",
      "Epoch 933, Loss: 1.8605175018310547, Final Batch Loss: 0.4924161732196808\n",
      "Epoch 934, Loss: 1.7474903166294098, Final Batch Loss: 0.39368122816085815\n",
      "Epoch 935, Loss: 1.733854979276657, Final Batch Loss: 0.3487186133861542\n",
      "Epoch 936, Loss: 1.9102563560009003, Final Batch Loss: 0.6505418419837952\n",
      "Epoch 937, Loss: 1.7359962165355682, Final Batch Loss: 0.38426199555397034\n",
      "Epoch 938, Loss: 1.782272607088089, Final Batch Loss: 0.4175904095172882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 939, Loss: 1.7091864049434662, Final Batch Loss: 0.44734933972358704\n",
      "Epoch 940, Loss: 1.8075524866580963, Final Batch Loss: 0.4466835856437683\n",
      "Epoch 941, Loss: 1.9134099185466766, Final Batch Loss: 0.5314103364944458\n",
      "Epoch 942, Loss: 1.7138963639736176, Final Batch Loss: 0.45292210578918457\n",
      "Epoch 943, Loss: 1.7971899509429932, Final Batch Loss: 0.49270927906036377\n",
      "Epoch 944, Loss: 1.8190406560897827, Final Batch Loss: 0.3603323698043823\n",
      "Epoch 945, Loss: 1.7624720633029938, Final Batch Loss: 0.4366745948791504\n",
      "Epoch 946, Loss: 1.8043701648712158, Final Batch Loss: 0.4545115828514099\n",
      "Epoch 947, Loss: 1.9932920932769775, Final Batch Loss: 0.5708891153335571\n",
      "Epoch 948, Loss: 1.7026067674160004, Final Batch Loss: 0.3895443081855774\n",
      "Epoch 949, Loss: 1.8155658543109894, Final Batch Loss: 0.3674766421318054\n",
      "Epoch 950, Loss: 1.7306508123874664, Final Batch Loss: 0.5097714066505432\n",
      "Epoch 951, Loss: 1.8594937026500702, Final Batch Loss: 0.45180198550224304\n",
      "Epoch 952, Loss: 1.7500296235084534, Final Batch Loss: 0.440875381231308\n",
      "Epoch 953, Loss: 1.8672813475131989, Final Batch Loss: 0.5021670460700989\n",
      "Epoch 954, Loss: 1.7508841753005981, Final Batch Loss: 0.5325598120689392\n",
      "Epoch 955, Loss: 1.6314317882061005, Final Batch Loss: 0.41513434052467346\n",
      "Epoch 956, Loss: 1.7722275257110596, Final Batch Loss: 0.4504501223564148\n",
      "Epoch 957, Loss: 1.826064556837082, Final Batch Loss: 0.4952329397201538\n",
      "Epoch 958, Loss: 1.5609138309955597, Final Batch Loss: 0.29991838335990906\n",
      "Epoch 959, Loss: 1.7085278928279877, Final Batch Loss: 0.4212333559989929\n",
      "Epoch 960, Loss: 1.8700751960277557, Final Batch Loss: 0.49452653527259827\n",
      "Epoch 961, Loss: 1.7403933703899384, Final Batch Loss: 0.43006888031959534\n",
      "Epoch 962, Loss: 1.8527368605136871, Final Batch Loss: 0.4740501344203949\n",
      "Epoch 963, Loss: 1.7130563855171204, Final Batch Loss: 0.47116807103157043\n",
      "Epoch 964, Loss: 1.6648867726325989, Final Batch Loss: 0.4049418568611145\n",
      "Epoch 965, Loss: 1.8313344717025757, Final Batch Loss: 0.4697321653366089\n",
      "Epoch 966, Loss: 1.7419622242450714, Final Batch Loss: 0.4168173372745514\n",
      "Epoch 967, Loss: 1.7652819454669952, Final Batch Loss: 0.47552356123924255\n",
      "Epoch 968, Loss: 1.7515546083450317, Final Batch Loss: 0.45209458470344543\n",
      "Epoch 969, Loss: 1.9779845774173737, Final Batch Loss: 0.48731714487075806\n",
      "Epoch 970, Loss: 1.7094193994998932, Final Batch Loss: 0.4231703281402588\n",
      "Epoch 971, Loss: 1.8334654867649078, Final Batch Loss: 0.5445311069488525\n",
      "Epoch 972, Loss: 1.681008368730545, Final Batch Loss: 0.4471026360988617\n",
      "Epoch 973, Loss: 1.7937228381633759, Final Batch Loss: 0.48202136158943176\n",
      "Epoch 974, Loss: 1.6704235076904297, Final Batch Loss: 0.4332265853881836\n",
      "Epoch 975, Loss: 1.632485419511795, Final Batch Loss: 0.4080381691455841\n",
      "Epoch 976, Loss: 1.8714455962181091, Final Batch Loss: 0.45565295219421387\n",
      "Epoch 977, Loss: 1.6957784593105316, Final Batch Loss: 0.4104485511779785\n",
      "Epoch 978, Loss: 1.8297433853149414, Final Batch Loss: 0.45263683795928955\n",
      "Epoch 979, Loss: 1.8034319579601288, Final Batch Loss: 0.5189485549926758\n",
      "Epoch 980, Loss: 1.7242348492145538, Final Batch Loss: 0.4749199151992798\n",
      "Epoch 981, Loss: 1.6725522577762604, Final Batch Loss: 0.4261494576931\n",
      "Epoch 982, Loss: 1.8120663464069366, Final Batch Loss: 0.47255590558052063\n",
      "Epoch 983, Loss: 1.8845434486865997, Final Batch Loss: 0.6074665188789368\n",
      "Epoch 984, Loss: 1.8913043141365051, Final Batch Loss: 0.5996589064598083\n",
      "Epoch 985, Loss: 1.76134791970253, Final Batch Loss: 0.5102137923240662\n",
      "Epoch 986, Loss: 1.8165786862373352, Final Batch Loss: 0.46986448764801025\n",
      "Epoch 987, Loss: 1.7947575151920319, Final Batch Loss: 0.4994165599346161\n",
      "Epoch 988, Loss: 1.78329735994339, Final Batch Loss: 0.3776741027832031\n",
      "Epoch 989, Loss: 1.7257623970508575, Final Batch Loss: 0.4839935302734375\n",
      "Epoch 990, Loss: 1.78952094912529, Final Batch Loss: 0.49956443905830383\n",
      "Epoch 991, Loss: 1.6631750762462616, Final Batch Loss: 0.36656537652015686\n",
      "Epoch 992, Loss: 1.742288500070572, Final Batch Loss: 0.4411022365093231\n",
      "Epoch 993, Loss: 1.7438377141952515, Final Batch Loss: 0.4069870710372925\n",
      "Epoch 994, Loss: 1.8231031596660614, Final Batch Loss: 0.4560698866844177\n",
      "Epoch 995, Loss: 1.688750982284546, Final Batch Loss: 0.4137732684612274\n",
      "Epoch 996, Loss: 1.7117149531841278, Final Batch Loss: 0.4445342719554901\n",
      "Epoch 997, Loss: 1.7765697538852692, Final Batch Loss: 0.49296602606773376\n",
      "Epoch 998, Loss: 1.7236751019954681, Final Batch Loss: 0.4227602481842041\n",
      "Epoch 999, Loss: 1.7799443304538727, Final Batch Loss: 0.49433740973472595\n",
      "Epoch 1000, Loss: 1.8658308386802673, Final Batch Loss: 0.5152655839920044\n",
      "Epoch 1001, Loss: 1.831746220588684, Final Batch Loss: 0.5473491549491882\n",
      "Epoch 1002, Loss: 1.7747084200382233, Final Batch Loss: 0.5318112373352051\n",
      "Epoch 1003, Loss: 1.8050744235515594, Final Batch Loss: 0.3980099558830261\n",
      "Epoch 1004, Loss: 1.6900657713413239, Final Batch Loss: 0.4518737494945526\n",
      "Epoch 1005, Loss: 1.7778920531272888, Final Batch Loss: 0.474903404712677\n",
      "Epoch 1006, Loss: 1.8354298174381256, Final Batch Loss: 0.5656653046607971\n",
      "Epoch 1007, Loss: 1.8002362251281738, Final Batch Loss: 0.48714688420295715\n",
      "Epoch 1008, Loss: 1.7091719210147858, Final Batch Loss: 0.4386644661426544\n",
      "Epoch 1009, Loss: 1.7462158203125, Final Batch Loss: 0.41371163725852966\n",
      "Epoch 1010, Loss: 1.685819149017334, Final Batch Loss: 0.4325748383998871\n",
      "Epoch 1011, Loss: 1.75120609998703, Final Batch Loss: 0.46263888478279114\n",
      "Epoch 1012, Loss: 1.6825618743896484, Final Batch Loss: 0.3820781409740448\n",
      "Epoch 1013, Loss: 1.6847889721393585, Final Batch Loss: 0.4453297555446625\n",
      "Epoch 1014, Loss: 1.6532703936100006, Final Batch Loss: 0.3158113360404968\n",
      "Epoch 1015, Loss: 1.5993894636631012, Final Batch Loss: 0.2963009476661682\n",
      "Epoch 1016, Loss: 1.7066642940044403, Final Batch Loss: 0.4129534363746643\n",
      "Epoch 1017, Loss: 1.7220600843429565, Final Batch Loss: 0.4078027009963989\n",
      "Epoch 1018, Loss: 1.620504766702652, Final Batch Loss: 0.2878488600254059\n",
      "Epoch 1019, Loss: 1.7259420454502106, Final Batch Loss: 0.3490052819252014\n",
      "Epoch 1020, Loss: 1.6365676522254944, Final Batch Loss: 0.3821617662906647\n",
      "Epoch 1021, Loss: 1.6201629340648651, Final Batch Loss: 0.30460232496261597\n",
      "Epoch 1022, Loss: 1.6814292669296265, Final Batch Loss: 0.42904379963874817\n",
      "Epoch 1023, Loss: 1.7609488666057587, Final Batch Loss: 0.5028167366981506\n",
      "Epoch 1024, Loss: 1.5437730848789215, Final Batch Loss: 0.44665655493736267\n",
      "Epoch 1025, Loss: 1.5799424350261688, Final Batch Loss: 0.377413809299469\n",
      "Epoch 1026, Loss: 1.7406468391418457, Final Batch Loss: 0.38944730162620544\n",
      "Epoch 1027, Loss: 1.6214812397956848, Final Batch Loss: 0.3759201765060425\n",
      "Epoch 1028, Loss: 1.6293493509292603, Final Batch Loss: 0.44785720109939575\n",
      "Epoch 1029, Loss: 1.73787122964859, Final Batch Loss: 0.367841511964798\n",
      "Epoch 1030, Loss: 1.6198760271072388, Final Batch Loss: 0.40403929352760315\n",
      "Epoch 1031, Loss: 1.6786980032920837, Final Batch Loss: 0.4018650949001312\n",
      "Epoch 1032, Loss: 1.8326174318790436, Final Batch Loss: 0.5083828568458557\n",
      "Epoch 1033, Loss: 1.7277574837207794, Final Batch Loss: 0.41847842931747437\n",
      "Epoch 1034, Loss: 1.6732279658317566, Final Batch Loss: 0.365617573261261\n",
      "Epoch 1035, Loss: 1.723187416791916, Final Batch Loss: 0.5858138203620911\n",
      "Epoch 1036, Loss: 1.7971579432487488, Final Batch Loss: 0.4866643249988556\n",
      "Epoch 1037, Loss: 1.7366158068180084, Final Batch Loss: 0.45554980635643005\n",
      "Epoch 1038, Loss: 1.7637902200222015, Final Batch Loss: 0.44546255469322205\n",
      "Epoch 1039, Loss: 1.6808936595916748, Final Batch Loss: 0.4411763846874237\n",
      "Epoch 1040, Loss: 1.7321900725364685, Final Batch Loss: 0.43253976106643677\n",
      "Epoch 1041, Loss: 1.7276567816734314, Final Batch Loss: 0.4459781348705292\n",
      "Epoch 1042, Loss: 1.6095623970031738, Final Batch Loss: 0.38067224621772766\n",
      "Epoch 1043, Loss: 1.663680762052536, Final Batch Loss: 0.4249154329299927\n",
      "Epoch 1044, Loss: 1.6428872346878052, Final Batch Loss: 0.3064739406108856\n",
      "Epoch 1045, Loss: 1.7914677262306213, Final Batch Loss: 0.5022667050361633\n",
      "Epoch 1046, Loss: 1.8288846611976624, Final Batch Loss: 0.6655305027961731\n",
      "Epoch 1047, Loss: 1.5484375059604645, Final Batch Loss: 0.3597966730594635\n",
      "Epoch 1048, Loss: 1.752470314502716, Final Batch Loss: 0.430269330739975\n",
      "Epoch 1049, Loss: 1.7077097296714783, Final Batch Loss: 0.511316180229187\n",
      "Epoch 1050, Loss: 1.6875751316547394, Final Batch Loss: 0.3823093771934509\n",
      "Epoch 1051, Loss: 1.627698302268982, Final Batch Loss: 0.5189058184623718\n",
      "Epoch 1052, Loss: 1.6366827189922333, Final Batch Loss: 0.4092300236225128\n",
      "Epoch 1053, Loss: 1.5472747683525085, Final Batch Loss: 0.42454633116722107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1054, Loss: 1.631069928407669, Final Batch Loss: 0.37769171595573425\n",
      "Epoch 1055, Loss: 1.6989712119102478, Final Batch Loss: 0.3936980366706848\n",
      "Epoch 1056, Loss: 1.654195874929428, Final Batch Loss: 0.4031747877597809\n",
      "Epoch 1057, Loss: 1.5774421393871307, Final Batch Loss: 0.3942711353302002\n",
      "Epoch 1058, Loss: 1.6944229900836945, Final Batch Loss: 0.40940892696380615\n",
      "Epoch 1059, Loss: 1.7330299317836761, Final Batch Loss: 0.4498690962791443\n",
      "Epoch 1060, Loss: 1.5354251861572266, Final Batch Loss: 0.36089393496513367\n",
      "Epoch 1061, Loss: 1.7667548060417175, Final Batch Loss: 0.4260311424732208\n",
      "Epoch 1062, Loss: 1.6723062694072723, Final Batch Loss: 0.384891152381897\n",
      "Epoch 1063, Loss: 1.7443055510520935, Final Batch Loss: 0.4568955898284912\n",
      "Epoch 1064, Loss: 1.7091373205184937, Final Batch Loss: 0.5090287923812866\n",
      "Epoch 1065, Loss: 1.7265967428684235, Final Batch Loss: 0.378781259059906\n",
      "Epoch 1066, Loss: 1.6970050930976868, Final Batch Loss: 0.37631258368492126\n",
      "Epoch 1067, Loss: 1.6819398999214172, Final Batch Loss: 0.41493216156959534\n",
      "Epoch 1068, Loss: 1.721552461385727, Final Batch Loss: 0.49941375851631165\n",
      "Epoch 1069, Loss: 1.6888568699359894, Final Batch Loss: 0.35411497950553894\n",
      "Epoch 1070, Loss: 1.6887382566928864, Final Batch Loss: 0.48275840282440186\n",
      "Epoch 1071, Loss: 1.5913135409355164, Final Batch Loss: 0.341866135597229\n",
      "Epoch 1072, Loss: 1.6594160795211792, Final Batch Loss: 0.3979748785495758\n",
      "Epoch 1073, Loss: 1.6483932733535767, Final Batch Loss: 0.42533791065216064\n",
      "Epoch 1074, Loss: 1.5915599763393402, Final Batch Loss: 0.37135976552963257\n",
      "Epoch 1075, Loss: 1.5927792191505432, Final Batch Loss: 0.40448662638664246\n",
      "Epoch 1076, Loss: 1.62765434384346, Final Batch Loss: 0.4344061017036438\n",
      "Epoch 1077, Loss: 1.6522852778434753, Final Batch Loss: 0.44423818588256836\n",
      "Epoch 1078, Loss: 1.7175653278827667, Final Batch Loss: 0.48676833510398865\n",
      "Epoch 1079, Loss: 1.6450133323669434, Final Batch Loss: 0.4599688649177551\n",
      "Epoch 1080, Loss: 1.7141596972942352, Final Batch Loss: 0.524684488773346\n",
      "Epoch 1081, Loss: 1.7470982372760773, Final Batch Loss: 0.5012449622154236\n",
      "Epoch 1082, Loss: 1.6646197438240051, Final Batch Loss: 0.4516303837299347\n",
      "Epoch 1083, Loss: 1.7016580700874329, Final Batch Loss: 0.44723832607269287\n",
      "Epoch 1084, Loss: 1.6463810503482819, Final Batch Loss: 0.41522160172462463\n",
      "Epoch 1085, Loss: 1.6618690192699432, Final Batch Loss: 0.33960962295532227\n",
      "Epoch 1086, Loss: 1.5953075289726257, Final Batch Loss: 0.4091154932975769\n",
      "Epoch 1087, Loss: 1.5789310038089752, Final Batch Loss: 0.3354935050010681\n",
      "Epoch 1088, Loss: 1.660604178905487, Final Batch Loss: 0.33174657821655273\n",
      "Epoch 1089, Loss: 1.4688133001327515, Final Batch Loss: 0.3213704228401184\n",
      "Epoch 1090, Loss: 1.7069708108901978, Final Batch Loss: 0.46661582589149475\n",
      "Epoch 1091, Loss: 1.7493669092655182, Final Batch Loss: 0.464326411485672\n",
      "Epoch 1092, Loss: 1.6944101750850677, Final Batch Loss: 0.40391549468040466\n",
      "Epoch 1093, Loss: 1.6043416559696198, Final Batch Loss: 0.41763734817504883\n",
      "Epoch 1094, Loss: 1.6402015089988708, Final Batch Loss: 0.4721108078956604\n",
      "Epoch 1095, Loss: 1.5038662254810333, Final Batch Loss: 0.3346472382545471\n",
      "Epoch 1096, Loss: 1.593184471130371, Final Batch Loss: 0.44270145893096924\n",
      "Epoch 1097, Loss: 1.5418635606765747, Final Batch Loss: 0.3493442237377167\n",
      "Epoch 1098, Loss: 1.6679563522338867, Final Batch Loss: 0.4451795220375061\n",
      "Epoch 1099, Loss: 1.7850054800510406, Final Batch Loss: 0.5874515175819397\n",
      "Epoch 1100, Loss: 1.737338274717331, Final Batch Loss: 0.5006961822509766\n",
      "Epoch 1101, Loss: 1.6193931698799133, Final Batch Loss: 0.3217855393886566\n",
      "Epoch 1102, Loss: 1.7075719237327576, Final Batch Loss: 0.3800782561302185\n",
      "Epoch 1103, Loss: 1.6099948585033417, Final Batch Loss: 0.4231399595737457\n",
      "Epoch 1104, Loss: 1.5510355234146118, Final Batch Loss: 0.420301616191864\n",
      "Epoch 1105, Loss: 1.5819260776042938, Final Batch Loss: 0.39688640832901\n",
      "Epoch 1106, Loss: 1.726679414510727, Final Batch Loss: 0.5963638424873352\n",
      "Epoch 1107, Loss: 1.582995355129242, Final Batch Loss: 0.34119105339050293\n",
      "Epoch 1108, Loss: 1.6784582734107971, Final Batch Loss: 0.40124478936195374\n",
      "Epoch 1109, Loss: 1.6067249476909637, Final Batch Loss: 0.42969685792922974\n",
      "Epoch 1110, Loss: 1.5579251945018768, Final Batch Loss: 0.445029079914093\n",
      "Epoch 1111, Loss: 1.5821947753429413, Final Batch Loss: 0.4743647873401642\n",
      "Epoch 1112, Loss: 1.585836261510849, Final Batch Loss: 0.35407257080078125\n",
      "Epoch 1113, Loss: 1.6461181044578552, Final Batch Loss: 0.40453803539276123\n",
      "Epoch 1114, Loss: 1.593673199415207, Final Batch Loss: 0.4106922745704651\n",
      "Epoch 1115, Loss: 1.5619406998157501, Final Batch Loss: 0.360842764377594\n",
      "Epoch 1116, Loss: 1.7313181459903717, Final Batch Loss: 0.5168139934539795\n",
      "Epoch 1117, Loss: 1.722661167383194, Final Batch Loss: 0.43872734904289246\n",
      "Epoch 1118, Loss: 1.5943134129047394, Final Batch Loss: 0.35290083289146423\n",
      "Epoch 1119, Loss: 1.5477403700351715, Final Batch Loss: 0.3941729962825775\n",
      "Epoch 1120, Loss: 1.5362186431884766, Final Batch Loss: 0.38088783621788025\n",
      "Epoch 1121, Loss: 1.5473733842372894, Final Batch Loss: 0.3588888645172119\n",
      "Epoch 1122, Loss: 1.5193357169628143, Final Batch Loss: 0.37505674362182617\n",
      "Epoch 1123, Loss: 1.6189900636672974, Final Batch Loss: 0.3864049017429352\n",
      "Epoch 1124, Loss: 1.6139095723628998, Final Batch Loss: 0.35734254121780396\n",
      "Epoch 1125, Loss: 1.6830598413944244, Final Batch Loss: 0.5094374418258667\n",
      "Epoch 1126, Loss: 1.6988061368465424, Final Batch Loss: 0.46887022256851196\n",
      "Epoch 1127, Loss: 1.4648529291152954, Final Batch Loss: 0.25947362184524536\n",
      "Epoch 1128, Loss: 1.6198075115680695, Final Batch Loss: 0.40405529737472534\n",
      "Epoch 1129, Loss: 1.5103321373462677, Final Batch Loss: 0.39817798137664795\n",
      "Epoch 1130, Loss: 1.5748018324375153, Final Batch Loss: 0.35407954454421997\n",
      "Epoch 1131, Loss: 1.5217025876045227, Final Batch Loss: 0.3399789035320282\n",
      "Epoch 1132, Loss: 1.5607159435749054, Final Batch Loss: 0.38941869139671326\n",
      "Epoch 1133, Loss: 1.600699782371521, Final Batch Loss: 0.3895440101623535\n",
      "Epoch 1134, Loss: 1.5018585920333862, Final Batch Loss: 0.43832680583000183\n",
      "Epoch 1135, Loss: 1.3929017186164856, Final Batch Loss: 0.32193323969841003\n",
      "Epoch 1136, Loss: 1.6961786448955536, Final Batch Loss: 0.44178175926208496\n",
      "Epoch 1137, Loss: 1.616825520992279, Final Batch Loss: 0.4051898121833801\n",
      "Epoch 1138, Loss: 1.629514753818512, Final Batch Loss: 0.405463308095932\n",
      "Epoch 1139, Loss: 1.5870618522167206, Final Batch Loss: 0.33670902252197266\n",
      "Epoch 1140, Loss: 1.639142245054245, Final Batch Loss: 0.40815338492393494\n",
      "Epoch 1141, Loss: 1.5689031183719635, Final Batch Loss: 0.3901529610157013\n",
      "Epoch 1142, Loss: 1.7266297936439514, Final Batch Loss: 0.5317313075065613\n",
      "Epoch 1143, Loss: 1.53728386759758, Final Batch Loss: 0.38669347763061523\n",
      "Epoch 1144, Loss: 1.5703602731227875, Final Batch Loss: 0.2934142053127289\n",
      "Epoch 1145, Loss: 1.5646783113479614, Final Batch Loss: 0.42341962456703186\n",
      "Epoch 1146, Loss: 1.6886870861053467, Final Batch Loss: 0.48166242241859436\n",
      "Epoch 1147, Loss: 1.6360701620578766, Final Batch Loss: 0.4608946442604065\n",
      "Epoch 1148, Loss: 1.6074248552322388, Final Batch Loss: 0.4665943682193756\n",
      "Epoch 1149, Loss: 1.5221923291683197, Final Batch Loss: 0.31441888213157654\n",
      "Epoch 1150, Loss: 1.4798888266086578, Final Batch Loss: 0.30360040068626404\n",
      "Epoch 1151, Loss: 1.5881073772907257, Final Batch Loss: 0.37012502551078796\n",
      "Epoch 1152, Loss: 1.5821859240531921, Final Batch Loss: 0.40339916944503784\n",
      "Epoch 1153, Loss: 1.500151515007019, Final Batch Loss: 0.3710821568965912\n",
      "Epoch 1154, Loss: 1.7056861817836761, Final Batch Loss: 0.4988648593425751\n",
      "Epoch 1155, Loss: 1.6595428586006165, Final Batch Loss: 0.3551786243915558\n",
      "Epoch 1156, Loss: 1.5840108394622803, Final Batch Loss: 0.43001705408096313\n",
      "Epoch 1157, Loss: 1.6106118857860565, Final Batch Loss: 0.4045172333717346\n",
      "Epoch 1158, Loss: 1.612827092409134, Final Batch Loss: 0.4115964472293854\n",
      "Epoch 1159, Loss: 1.5372680723667145, Final Batch Loss: 0.3448604643344879\n",
      "Epoch 1160, Loss: 1.6405394673347473, Final Batch Loss: 0.44349220395088196\n",
      "Epoch 1161, Loss: 1.6267507374286652, Final Batch Loss: 0.41718822717666626\n",
      "Epoch 1162, Loss: 1.594346284866333, Final Batch Loss: 0.3761873245239258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1163, Loss: 1.5790298283100128, Final Batch Loss: 0.41334107518196106\n",
      "Epoch 1164, Loss: 1.5244792103767395, Final Batch Loss: 0.3078263998031616\n",
      "Epoch 1165, Loss: 1.6363479495048523, Final Batch Loss: 0.28856998682022095\n",
      "Epoch 1166, Loss: 1.6462036967277527, Final Batch Loss: 0.5127888917922974\n",
      "Epoch 1167, Loss: 1.5766706466674805, Final Batch Loss: 0.36233553290367126\n",
      "Epoch 1168, Loss: 1.566380649805069, Final Batch Loss: 0.3895244300365448\n",
      "Epoch 1169, Loss: 1.5973795056343079, Final Batch Loss: 0.4182776212692261\n",
      "Epoch 1170, Loss: 1.5747813284397125, Final Batch Loss: 0.33371037244796753\n",
      "Epoch 1171, Loss: 1.4617098420858383, Final Batch Loss: 0.2499818354845047\n",
      "Epoch 1172, Loss: 1.5906071662902832, Final Batch Loss: 0.3963920474052429\n",
      "Epoch 1173, Loss: 1.4628554582595825, Final Batch Loss: 0.3986073136329651\n",
      "Epoch 1174, Loss: 1.645782232284546, Final Batch Loss: 0.40276971459388733\n",
      "Epoch 1175, Loss: 1.5535263419151306, Final Batch Loss: 0.4161882996559143\n",
      "Epoch 1176, Loss: 1.5186458826065063, Final Batch Loss: 0.3509751856327057\n",
      "Epoch 1177, Loss: 1.3823846280574799, Final Batch Loss: 0.25165703892707825\n",
      "Epoch 1178, Loss: 1.4840021133422852, Final Batch Loss: 0.3350876569747925\n",
      "Epoch 1179, Loss: 1.6026796102523804, Final Batch Loss: 0.37986069917678833\n",
      "Epoch 1180, Loss: 1.5751768052577972, Final Batch Loss: 0.35977229475975037\n",
      "Epoch 1181, Loss: 1.4334688484668732, Final Batch Loss: 0.3198091983795166\n",
      "Epoch 1182, Loss: 1.5236747860908508, Final Batch Loss: 0.425947368144989\n",
      "Epoch 1183, Loss: 1.5628750026226044, Final Batch Loss: 0.39514267444610596\n",
      "Epoch 1184, Loss: 1.7518298029899597, Final Batch Loss: 0.4128408133983612\n",
      "Epoch 1185, Loss: 1.6561231911182404, Final Batch Loss: 0.44908371567726135\n",
      "Epoch 1186, Loss: 1.522593080997467, Final Batch Loss: 0.39051830768585205\n",
      "Epoch 1187, Loss: 1.4932571947574615, Final Batch Loss: 0.4371486008167267\n",
      "Epoch 1188, Loss: 1.6284191608428955, Final Batch Loss: 0.5138837099075317\n",
      "Epoch 1189, Loss: 1.4164372980594635, Final Batch Loss: 0.31232935190200806\n",
      "Epoch 1190, Loss: 1.5007721185684204, Final Batch Loss: 0.3467305898666382\n",
      "Epoch 1191, Loss: 1.623506784439087, Final Batch Loss: 0.44767701625823975\n",
      "Epoch 1192, Loss: 1.5184446275234222, Final Batch Loss: 0.43066608905792236\n",
      "Epoch 1193, Loss: 1.5579447448253632, Final Batch Loss: 0.48895370960235596\n",
      "Epoch 1194, Loss: 1.515741765499115, Final Batch Loss: 0.29963254928588867\n",
      "Epoch 1195, Loss: 1.5350687801837921, Final Batch Loss: 0.3404180109500885\n",
      "Epoch 1196, Loss: 1.6477170586585999, Final Batch Loss: 0.38780489563941956\n",
      "Epoch 1197, Loss: 1.5565716922283173, Final Batch Loss: 0.41037866473197937\n",
      "Epoch 1198, Loss: 1.6180350482463837, Final Batch Loss: 0.41448748111724854\n",
      "Epoch 1199, Loss: 1.5841762721538544, Final Batch Loss: 0.42253735661506653\n",
      "Epoch 1200, Loss: 1.5554336607456207, Final Batch Loss: 0.3241559863090515\n",
      "Epoch 1201, Loss: 1.497301608324051, Final Batch Loss: 0.3510523736476898\n",
      "Epoch 1202, Loss: 1.498976707458496, Final Batch Loss: 0.3410944640636444\n",
      "Epoch 1203, Loss: 1.5498861372470856, Final Batch Loss: 0.4965505599975586\n",
      "Epoch 1204, Loss: 1.5572645366191864, Final Batch Loss: 0.3707638084888458\n",
      "Epoch 1205, Loss: 1.465558260679245, Final Batch Loss: 0.26868540048599243\n",
      "Epoch 1206, Loss: 1.5939199924468994, Final Batch Loss: 0.44680625200271606\n",
      "Epoch 1207, Loss: 1.7243447601795197, Final Batch Loss: 0.5721348524093628\n",
      "Epoch 1208, Loss: 1.4093624949455261, Final Batch Loss: 0.32474178075790405\n",
      "Epoch 1209, Loss: 1.541001707315445, Final Batch Loss: 0.3618360459804535\n",
      "Epoch 1210, Loss: 1.4914307296276093, Final Batch Loss: 0.27343541383743286\n",
      "Epoch 1211, Loss: 1.5915146172046661, Final Batch Loss: 0.3714236319065094\n",
      "Epoch 1212, Loss: 1.5186746716499329, Final Batch Loss: 0.33059898018836975\n",
      "Epoch 1213, Loss: 1.4509957134723663, Final Batch Loss: 0.3090006113052368\n",
      "Epoch 1214, Loss: 1.478907585144043, Final Batch Loss: 0.3047826290130615\n",
      "Epoch 1215, Loss: 1.495294988155365, Final Batch Loss: 0.3040745258331299\n",
      "Epoch 1216, Loss: 1.6812997162342072, Final Batch Loss: 0.41170641779899597\n",
      "Epoch 1217, Loss: 1.5325498580932617, Final Batch Loss: 0.37180623412132263\n",
      "Epoch 1218, Loss: 1.7078396379947662, Final Batch Loss: 0.5628721117973328\n",
      "Epoch 1219, Loss: 1.5836407542228699, Final Batch Loss: 0.456133633852005\n",
      "Epoch 1220, Loss: 1.5009557902812958, Final Batch Loss: 0.38452422618865967\n",
      "Epoch 1221, Loss: 1.5384352803230286, Final Batch Loss: 0.3771430253982544\n",
      "Epoch 1222, Loss: 1.6278401613235474, Final Batch Loss: 0.4537161588668823\n",
      "Epoch 1223, Loss: 1.4008455574512482, Final Batch Loss: 0.29497185349464417\n",
      "Epoch 1224, Loss: 1.6378629505634308, Final Batch Loss: 0.4594515562057495\n",
      "Epoch 1225, Loss: 1.444886326789856, Final Batch Loss: 0.4265376031398773\n",
      "Epoch 1226, Loss: 1.4703108966350555, Final Batch Loss: 0.32723039388656616\n",
      "Epoch 1227, Loss: 1.4742583930492401, Final Batch Loss: 0.4437176585197449\n",
      "Epoch 1228, Loss: 1.5661356747150421, Final Batch Loss: 0.3558705747127533\n",
      "Epoch 1229, Loss: 1.3513042628765106, Final Batch Loss: 0.2960249185562134\n",
      "Epoch 1230, Loss: 1.5364610254764557, Final Batch Loss: 0.4333925247192383\n",
      "Epoch 1231, Loss: 1.544533610343933, Final Batch Loss: 0.4210260808467865\n",
      "Epoch 1232, Loss: 1.540202260017395, Final Batch Loss: 0.3665739595890045\n",
      "Epoch 1233, Loss: 1.4730601608753204, Final Batch Loss: 0.4326310455799103\n",
      "Epoch 1234, Loss: 1.4272116124629974, Final Batch Loss: 0.32308241724967957\n",
      "Epoch 1235, Loss: 1.508086383342743, Final Batch Loss: 0.41906672716140747\n",
      "Epoch 1236, Loss: 1.4968747198581696, Final Batch Loss: 0.3698892593383789\n",
      "Epoch 1237, Loss: 1.5405172109603882, Final Batch Loss: 0.44694703817367554\n",
      "Epoch 1238, Loss: 1.4650976657867432, Final Batch Loss: 0.40786418318748474\n",
      "Epoch 1239, Loss: 1.5036742091178894, Final Batch Loss: 0.3770243227481842\n",
      "Epoch 1240, Loss: 1.5174777805805206, Final Batch Loss: 0.3659741282463074\n",
      "Epoch 1241, Loss: 1.4217728674411774, Final Batch Loss: 0.3881882429122925\n",
      "Epoch 1242, Loss: 1.4298344850540161, Final Batch Loss: 0.34160223603248596\n",
      "Epoch 1243, Loss: 1.5395438969135284, Final Batch Loss: 0.38765987753868103\n",
      "Epoch 1244, Loss: 1.5848954916000366, Final Batch Loss: 0.5030729174613953\n",
      "Epoch 1245, Loss: 1.446596473455429, Final Batch Loss: 0.4097699224948883\n",
      "Epoch 1246, Loss: 1.5660670697689056, Final Batch Loss: 0.4560440480709076\n",
      "Epoch 1247, Loss: 1.527037650346756, Final Batch Loss: 0.41232916712760925\n",
      "Epoch 1248, Loss: 1.4380742609500885, Final Batch Loss: 0.3295626640319824\n",
      "Epoch 1249, Loss: 1.450654149055481, Final Batch Loss: 0.3375682234764099\n",
      "Epoch 1250, Loss: 1.6867759823799133, Final Batch Loss: 0.4763704836368561\n",
      "Epoch 1251, Loss: 1.4363558888435364, Final Batch Loss: 0.3512013852596283\n",
      "Epoch 1252, Loss: 1.5305727124214172, Final Batch Loss: 0.346866637468338\n",
      "Epoch 1253, Loss: 1.686947077512741, Final Batch Loss: 0.4647270441055298\n",
      "Epoch 1254, Loss: 1.5234260857105255, Final Batch Loss: 0.3918216824531555\n",
      "Epoch 1255, Loss: 1.5089041590690613, Final Batch Loss: 0.3592075705528259\n",
      "Epoch 1256, Loss: 1.4321216344833374, Final Batch Loss: 0.3587645888328552\n",
      "Epoch 1257, Loss: 1.4940738081932068, Final Batch Loss: 0.39319756627082825\n",
      "Epoch 1258, Loss: 1.3975293636322021, Final Batch Loss: 0.30839765071868896\n",
      "Epoch 1259, Loss: 1.3779291212558746, Final Batch Loss: 0.3117416799068451\n",
      "Epoch 1260, Loss: 1.4443947970867157, Final Batch Loss: 0.32530277967453003\n",
      "Epoch 1261, Loss: 1.4722404181957245, Final Batch Loss: 0.3804709017276764\n",
      "Epoch 1262, Loss: 1.5939289927482605, Final Batch Loss: 0.4078403115272522\n",
      "Epoch 1263, Loss: 1.4995323419570923, Final Batch Loss: 0.3501076102256775\n",
      "Epoch 1264, Loss: 1.4016587138175964, Final Batch Loss: 0.2769775688648224\n",
      "Epoch 1265, Loss: 1.4859235882759094, Final Batch Loss: 0.3570484519004822\n",
      "Epoch 1266, Loss: 1.5120867192745209, Final Batch Loss: 0.36742714047431946\n",
      "Epoch 1267, Loss: 1.4439354240894318, Final Batch Loss: 0.3553037643432617\n",
      "Epoch 1268, Loss: 1.5150383710861206, Final Batch Loss: 0.3624327480792999\n",
      "Epoch 1269, Loss: 1.3408257365226746, Final Batch Loss: 0.276517778635025\n",
      "Epoch 1270, Loss: 1.464389979839325, Final Batch Loss: 0.4062446057796478\n",
      "Epoch 1271, Loss: 1.6456060409545898, Final Batch Loss: 0.440991073846817\n",
      "Epoch 1272, Loss: 1.4922227263450623, Final Batch Loss: 0.33689406514167786\n",
      "Epoch 1273, Loss: 1.378398358821869, Final Batch Loss: 0.3247087895870209\n",
      "Epoch 1274, Loss: 1.4782350659370422, Final Batch Loss: 0.43458858132362366\n",
      "Epoch 1275, Loss: 1.4544087052345276, Final Batch Loss: 0.40958207845687866\n",
      "Epoch 1276, Loss: 1.4420866668224335, Final Batch Loss: 0.3891642689704895\n",
      "Epoch 1277, Loss: 1.572656363248825, Final Batch Loss: 0.4287708103656769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1278, Loss: 1.508245974779129, Final Batch Loss: 0.3825157582759857\n",
      "Epoch 1279, Loss: 1.499863177537918, Final Batch Loss: 0.4503103494644165\n",
      "Epoch 1280, Loss: 1.6022680699825287, Final Batch Loss: 0.4393494725227356\n",
      "Epoch 1281, Loss: 1.5988104939460754, Final Batch Loss: 0.4998704195022583\n",
      "Epoch 1282, Loss: 1.419845312833786, Final Batch Loss: 0.2525591552257538\n",
      "Epoch 1283, Loss: 1.4665622115135193, Final Batch Loss: 0.36172500252723694\n",
      "Epoch 1284, Loss: 1.4329343438148499, Final Batch Loss: 0.3431859016418457\n",
      "Epoch 1285, Loss: 1.46715047955513, Final Batch Loss: 0.33599844574928284\n",
      "Epoch 1286, Loss: 1.452584445476532, Final Batch Loss: 0.3860221803188324\n",
      "Epoch 1287, Loss: 1.419712781906128, Final Batch Loss: 0.3601854145526886\n",
      "Epoch 1288, Loss: 1.5402754545211792, Final Batch Loss: 0.4438112676143646\n",
      "Epoch 1289, Loss: 1.4384129047393799, Final Batch Loss: 0.3371511995792389\n",
      "Epoch 1290, Loss: 1.649865835905075, Final Batch Loss: 0.3537713289260864\n",
      "Epoch 1291, Loss: 1.4765889644622803, Final Batch Loss: 0.29769960045814514\n",
      "Epoch 1292, Loss: 1.446974754333496, Final Batch Loss: 0.4054807424545288\n",
      "Epoch 1293, Loss: 1.310495138168335, Final Batch Loss: 0.29797178506851196\n",
      "Epoch 1294, Loss: 1.4876195192337036, Final Batch Loss: 0.3730180561542511\n",
      "Epoch 1295, Loss: 1.4136331677436829, Final Batch Loss: 0.27160364389419556\n",
      "Epoch 1296, Loss: 1.5461061596870422, Final Batch Loss: 0.3947513997554779\n",
      "Epoch 1297, Loss: 1.3897156119346619, Final Batch Loss: 0.3383377492427826\n",
      "Epoch 1298, Loss: 1.3329998403787613, Final Batch Loss: 0.24648036062717438\n",
      "Epoch 1299, Loss: 1.4957951307296753, Final Batch Loss: 0.3389599621295929\n",
      "Epoch 1300, Loss: 1.5174430906772614, Final Batch Loss: 0.2842986285686493\n",
      "Epoch 1301, Loss: 1.4322198033332825, Final Batch Loss: 0.3615017235279083\n",
      "Epoch 1302, Loss: 1.3682176768779755, Final Batch Loss: 0.3362025022506714\n",
      "Epoch 1303, Loss: 1.423673301935196, Final Batch Loss: 0.3371865749359131\n",
      "Epoch 1304, Loss: 1.4191067516803741, Final Batch Loss: 0.2680529057979584\n",
      "Epoch 1305, Loss: 1.4480767846107483, Final Batch Loss: 0.33706119656562805\n",
      "Epoch 1306, Loss: 1.3861893713474274, Final Batch Loss: 0.33084970712661743\n",
      "Epoch 1307, Loss: 1.3972806930541992, Final Batch Loss: 0.29332756996154785\n",
      "Epoch 1308, Loss: 1.5099404752254486, Final Batch Loss: 0.39025095105171204\n",
      "Epoch 1309, Loss: 1.4885283410549164, Final Batch Loss: 0.33278772234916687\n",
      "Epoch 1310, Loss: 1.417550951242447, Final Batch Loss: 0.3491400182247162\n",
      "Epoch 1311, Loss: 1.4750846922397614, Final Batch Loss: 0.39614662528038025\n",
      "Epoch 1312, Loss: 1.4500755667686462, Final Batch Loss: 0.38605207204818726\n",
      "Epoch 1313, Loss: 1.458574801683426, Final Batch Loss: 0.2976338863372803\n",
      "Epoch 1314, Loss: 1.4354774355888367, Final Batch Loss: 0.3344813585281372\n",
      "Epoch 1315, Loss: 1.4721560776233673, Final Batch Loss: 0.3726828992366791\n",
      "Epoch 1316, Loss: 1.6208424270153046, Final Batch Loss: 0.5872801542282104\n",
      "Epoch 1317, Loss: 1.2714973390102386, Final Batch Loss: 0.2577178478240967\n",
      "Epoch 1318, Loss: 1.41305473446846, Final Batch Loss: 0.32231444120407104\n",
      "Epoch 1319, Loss: 1.3112530559301376, Final Batch Loss: 0.24848292768001556\n",
      "Epoch 1320, Loss: 1.5106048583984375, Final Batch Loss: 0.4003009498119354\n",
      "Epoch 1321, Loss: 1.475833922624588, Final Batch Loss: 0.3391094505786896\n",
      "Epoch 1322, Loss: 1.4446660578250885, Final Batch Loss: 0.37668561935424805\n",
      "Epoch 1323, Loss: 1.5020315647125244, Final Batch Loss: 0.42827358841896057\n",
      "Epoch 1324, Loss: 1.5005460679531097, Final Batch Loss: 0.349384069442749\n",
      "Epoch 1325, Loss: 1.480653554201126, Final Batch Loss: 0.3708321452140808\n",
      "Epoch 1326, Loss: 1.5512354373931885, Final Batch Loss: 0.4159862995147705\n",
      "Epoch 1327, Loss: 1.3820635378360748, Final Batch Loss: 0.3431701064109802\n",
      "Epoch 1328, Loss: 1.5542593002319336, Final Batch Loss: 0.4722936749458313\n",
      "Epoch 1329, Loss: 1.3723204433918, Final Batch Loss: 0.29683464765548706\n",
      "Epoch 1330, Loss: 1.5133936405181885, Final Batch Loss: 0.33807000517845154\n",
      "Epoch 1331, Loss: 1.3364777266979218, Final Batch Loss: 0.30413204431533813\n",
      "Epoch 1332, Loss: 1.4807914793491364, Final Batch Loss: 0.30534371733665466\n",
      "Epoch 1333, Loss: 1.4766605198383331, Final Batch Loss: 0.39193761348724365\n",
      "Epoch 1334, Loss: 1.47428297996521, Final Batch Loss: 0.35838860273361206\n",
      "Epoch 1335, Loss: 1.3358855843544006, Final Batch Loss: 0.2775956094264984\n",
      "Epoch 1336, Loss: 1.4357452988624573, Final Batch Loss: 0.4300585091114044\n",
      "Epoch 1337, Loss: 1.4248791337013245, Final Batch Loss: 0.24161076545715332\n",
      "Epoch 1338, Loss: 1.2539498507976532, Final Batch Loss: 0.273409366607666\n",
      "Epoch 1339, Loss: 1.3869831562042236, Final Batch Loss: 0.33643293380737305\n",
      "Epoch 1340, Loss: 1.3465486764907837, Final Batch Loss: 0.2302672266960144\n",
      "Epoch 1341, Loss: 1.4772911071777344, Final Batch Loss: 0.36853882670402527\n",
      "Epoch 1342, Loss: 1.5686941742897034, Final Batch Loss: 0.43891507387161255\n",
      "Epoch 1343, Loss: 1.3543061912059784, Final Batch Loss: 0.3239637315273285\n",
      "Epoch 1344, Loss: 1.4537469148635864, Final Batch Loss: 0.34636735916137695\n",
      "Epoch 1345, Loss: 1.5563644468784332, Final Batch Loss: 0.39283493161201477\n",
      "Epoch 1346, Loss: 1.3501857221126556, Final Batch Loss: 0.33640095591545105\n",
      "Epoch 1347, Loss: 1.5564341843128204, Final Batch Loss: 0.39739131927490234\n",
      "Epoch 1348, Loss: 1.3740824460983276, Final Batch Loss: 0.3403547704219818\n",
      "Epoch 1349, Loss: 1.3434776365756989, Final Batch Loss: 0.3846808671951294\n",
      "Epoch 1350, Loss: 1.3644916713237762, Final Batch Loss: 0.34096768498420715\n",
      "Epoch 1351, Loss: 1.3868105113506317, Final Batch Loss: 0.3136390149593353\n",
      "Epoch 1352, Loss: 1.3463196456432343, Final Batch Loss: 0.26222285628318787\n",
      "Epoch 1353, Loss: 1.532492309808731, Final Batch Loss: 0.4945785105228424\n",
      "Epoch 1354, Loss: 1.3848888874053955, Final Batch Loss: 0.3654076159000397\n",
      "Epoch 1355, Loss: 1.4369539022445679, Final Batch Loss: 0.39992985129356384\n",
      "Epoch 1356, Loss: 1.4164093434810638, Final Batch Loss: 0.33049073815345764\n",
      "Epoch 1357, Loss: 1.3606143295764923, Final Batch Loss: 0.35687556862831116\n",
      "Epoch 1358, Loss: 1.3652144074440002, Final Batch Loss: 0.40983846783638\n",
      "Epoch 1359, Loss: 1.3812482953071594, Final Batch Loss: 0.31345054507255554\n",
      "Epoch 1360, Loss: 1.3043533563613892, Final Batch Loss: 0.23373964428901672\n",
      "Epoch 1361, Loss: 1.360499620437622, Final Batch Loss: 0.3108479082584381\n",
      "Epoch 1362, Loss: 1.3922185003757477, Final Batch Loss: 0.3567178547382355\n",
      "Epoch 1363, Loss: 1.5081955194473267, Final Batch Loss: 0.42508870363235474\n",
      "Epoch 1364, Loss: 1.4371587336063385, Final Batch Loss: 0.3334268629550934\n",
      "Epoch 1365, Loss: 1.4531524181365967, Final Batch Loss: 0.4260723888874054\n",
      "Epoch 1366, Loss: 1.359504610300064, Final Batch Loss: 0.30353090167045593\n",
      "Epoch 1367, Loss: 1.4770126044750214, Final Batch Loss: 0.40014219284057617\n",
      "Epoch 1368, Loss: 1.4460631608963013, Final Batch Loss: 0.40355318784713745\n",
      "Epoch 1369, Loss: 1.3465254604816437, Final Batch Loss: 0.2835550308227539\n",
      "Epoch 1370, Loss: 1.5303267538547516, Final Batch Loss: 0.36495068669319153\n",
      "Epoch 1371, Loss: 1.478459358215332, Final Batch Loss: 0.3505605161190033\n",
      "Epoch 1372, Loss: 1.4044820964336395, Final Batch Loss: 0.3550769090652466\n",
      "Epoch 1373, Loss: 1.6130905151367188, Final Batch Loss: 0.3448653817176819\n",
      "Epoch 1374, Loss: 1.409828543663025, Final Batch Loss: 0.43881309032440186\n",
      "Epoch 1375, Loss: 1.4943228662014008, Final Batch Loss: 0.4567507803440094\n",
      "Epoch 1376, Loss: 1.264590471982956, Final Batch Loss: 0.2363167703151703\n",
      "Epoch 1377, Loss: 1.3699843287467957, Final Batch Loss: 0.2596372961997986\n",
      "Epoch 1378, Loss: 1.3161360919475555, Final Batch Loss: 0.2952674925327301\n",
      "Epoch 1379, Loss: 1.4265679717063904, Final Batch Loss: 0.36723271012306213\n",
      "Epoch 1380, Loss: 1.506255954504013, Final Batch Loss: 0.44090741872787476\n",
      "Epoch 1381, Loss: 1.4189490973949432, Final Batch Loss: 0.36299827694892883\n",
      "Epoch 1382, Loss: 1.3840532004833221, Final Batch Loss: 0.37850698828697205\n",
      "Epoch 1383, Loss: 1.348686158657074, Final Batch Loss: 0.37582695484161377\n",
      "Epoch 1384, Loss: 1.3962957859039307, Final Batch Loss: 0.336068332195282\n",
      "Epoch 1385, Loss: 1.3955639600753784, Final Batch Loss: 0.279615581035614\n",
      "Epoch 1386, Loss: 1.3679615259170532, Final Batch Loss: 0.3421401381492615\n",
      "Epoch 1387, Loss: 1.4802215993404388, Final Batch Loss: 0.35884010791778564\n",
      "Epoch 1388, Loss: 1.3532882630825043, Final Batch Loss: 0.3680824041366577\n",
      "Epoch 1389, Loss: 1.3939781188964844, Final Batch Loss: 0.42330142855644226\n",
      "Epoch 1390, Loss: 1.5418547093868256, Final Batch Loss: 0.3967510759830475\n",
      "Epoch 1391, Loss: 1.4809794127941132, Final Batch Loss: 0.31228283047676086\n",
      "Epoch 1392, Loss: 1.3198190778493881, Final Batch Loss: 0.2376585453748703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1393, Loss: 1.2579872906208038, Final Batch Loss: 0.22807058691978455\n",
      "Epoch 1394, Loss: 1.5054311752319336, Final Batch Loss: 0.3511565327644348\n",
      "Epoch 1395, Loss: 1.3808437585830688, Final Batch Loss: 0.3548218309879303\n",
      "Epoch 1396, Loss: 1.3963981568813324, Final Batch Loss: 0.3151536285877228\n",
      "Epoch 1397, Loss: 1.4271415770053864, Final Batch Loss: 0.4462703466415405\n",
      "Epoch 1398, Loss: 1.3550719618797302, Final Batch Loss: 0.32501667737960815\n",
      "Epoch 1399, Loss: 1.375457912683487, Final Batch Loss: 0.30814090371131897\n",
      "Epoch 1400, Loss: 1.4177883565425873, Final Batch Loss: 0.4195784628391266\n",
      "Epoch 1401, Loss: 1.3363745510578156, Final Batch Loss: 0.3733515739440918\n",
      "Epoch 1402, Loss: 1.4081531167030334, Final Batch Loss: 0.3626898527145386\n",
      "Epoch 1403, Loss: 1.2037738263607025, Final Batch Loss: 0.2673141360282898\n",
      "Epoch 1404, Loss: 1.4174632728099823, Final Batch Loss: 0.29176247119903564\n",
      "Epoch 1405, Loss: 1.5487779676914215, Final Batch Loss: 0.4239804148674011\n",
      "Epoch 1406, Loss: 1.3952943682670593, Final Batch Loss: 0.30870574712753296\n",
      "Epoch 1407, Loss: 1.4022590219974518, Final Batch Loss: 0.32157713174819946\n",
      "Epoch 1408, Loss: 1.4240289330482483, Final Batch Loss: 0.3711581528186798\n",
      "Epoch 1409, Loss: 1.3834813833236694, Final Batch Loss: 0.38494428992271423\n",
      "Epoch 1410, Loss: 1.461330085992813, Final Batch Loss: 0.28584596514701843\n",
      "Epoch 1411, Loss: 1.441057711839676, Final Batch Loss: 0.3644699156284332\n",
      "Epoch 1412, Loss: 1.4045711159706116, Final Batch Loss: 0.38246768712997437\n",
      "Epoch 1413, Loss: 1.371669352054596, Final Batch Loss: 0.369088351726532\n",
      "Epoch 1414, Loss: 1.4261256158351898, Final Batch Loss: 0.43359270691871643\n",
      "Epoch 1415, Loss: 1.2856676280498505, Final Batch Loss: 0.3199758231639862\n",
      "Epoch 1416, Loss: 1.4335740506649017, Final Batch Loss: 0.30295947194099426\n",
      "Epoch 1417, Loss: 1.34281387925148, Final Batch Loss: 0.33613985776901245\n",
      "Epoch 1418, Loss: 1.5059555768966675, Final Batch Loss: 0.33998391032218933\n",
      "Epoch 1419, Loss: 1.296542912721634, Final Batch Loss: 0.38452643156051636\n",
      "Epoch 1420, Loss: 1.4438965916633606, Final Batch Loss: 0.41764846444129944\n",
      "Epoch 1421, Loss: 1.3725860118865967, Final Batch Loss: 0.339954674243927\n",
      "Epoch 1422, Loss: 1.2956064343452454, Final Batch Loss: 0.2817000448703766\n",
      "Epoch 1423, Loss: 1.2902432978153229, Final Batch Loss: 0.32469284534454346\n",
      "Epoch 1424, Loss: 1.4114934206008911, Final Batch Loss: 0.38401558995246887\n",
      "Epoch 1425, Loss: 1.369959145784378, Final Batch Loss: 0.3415548503398895\n",
      "Epoch 1426, Loss: 1.516949862241745, Final Batch Loss: 0.4727271795272827\n",
      "Epoch 1427, Loss: 1.3535398542881012, Final Batch Loss: 0.37798911333084106\n",
      "Epoch 1428, Loss: 1.3146840333938599, Final Batch Loss: 0.23514389991760254\n",
      "Epoch 1429, Loss: 1.457360565662384, Final Batch Loss: 0.4326837658882141\n",
      "Epoch 1430, Loss: 1.4349396228790283, Final Batch Loss: 0.34841740131378174\n",
      "Epoch 1431, Loss: 1.360292375087738, Final Batch Loss: 0.28747662901878357\n",
      "Epoch 1432, Loss: 1.3105696439743042, Final Batch Loss: 0.3421536087989807\n",
      "Epoch 1433, Loss: 1.3587416112422943, Final Batch Loss: 0.3593863248825073\n",
      "Epoch 1434, Loss: 1.2735496759414673, Final Batch Loss: 0.3193458020687103\n",
      "Epoch 1435, Loss: 1.4098863303661346, Final Batch Loss: 0.3714459240436554\n",
      "Epoch 1436, Loss: 1.3005602955818176, Final Batch Loss: 0.2928760051727295\n",
      "Epoch 1437, Loss: 1.338555783033371, Final Batch Loss: 0.38612720370292664\n",
      "Epoch 1438, Loss: 1.3127872943878174, Final Batch Loss: 0.3307342529296875\n",
      "Epoch 1439, Loss: 1.396928608417511, Final Batch Loss: 0.2844081521034241\n",
      "Epoch 1440, Loss: 1.4509266912937164, Final Batch Loss: 0.3431394398212433\n",
      "Epoch 1441, Loss: 1.4392448365688324, Final Batch Loss: 0.42611461877822876\n",
      "Epoch 1442, Loss: 1.4215974509716034, Final Batch Loss: 0.2986898422241211\n",
      "Epoch 1443, Loss: 1.4096950590610504, Final Batch Loss: 0.40813589096069336\n",
      "Epoch 1444, Loss: 1.3706536293029785, Final Batch Loss: 0.3708276152610779\n",
      "Epoch 1445, Loss: 1.3719717264175415, Final Batch Loss: 0.30243980884552\n",
      "Epoch 1446, Loss: 1.4365518391132355, Final Batch Loss: 0.3425939083099365\n",
      "Epoch 1447, Loss: 1.465153157711029, Final Batch Loss: 0.4036322236061096\n",
      "Epoch 1448, Loss: 1.3967450261116028, Final Batch Loss: 0.3217388689517975\n",
      "Epoch 1449, Loss: 1.3770026862621307, Final Batch Loss: 0.46346983313560486\n",
      "Epoch 1450, Loss: 1.4175950288772583, Final Batch Loss: 0.33893483877182007\n",
      "Epoch 1451, Loss: 1.3460544347763062, Final Batch Loss: 0.2786847651004791\n",
      "Epoch 1452, Loss: 1.4515917897224426, Final Batch Loss: 0.38944944739341736\n",
      "Epoch 1453, Loss: 1.2736000418663025, Final Batch Loss: 0.27997255325317383\n",
      "Epoch 1454, Loss: 1.3577968180179596, Final Batch Loss: 0.35478562116622925\n",
      "Epoch 1455, Loss: 1.2597390413284302, Final Batch Loss: 0.3309261202812195\n",
      "Epoch 1456, Loss: 1.2373153865337372, Final Batch Loss: 0.3017962574958801\n",
      "Epoch 1457, Loss: 1.303184688091278, Final Batch Loss: 0.29030415415763855\n",
      "Epoch 1458, Loss: 1.5452097058296204, Final Batch Loss: 0.4687942862510681\n",
      "Epoch 1459, Loss: 1.3387117683887482, Final Batch Loss: 0.3399629592895508\n",
      "Epoch 1460, Loss: 1.2982861399650574, Final Batch Loss: 0.2984048128128052\n",
      "Epoch 1461, Loss: 1.3654559254646301, Final Batch Loss: 0.2795516848564148\n",
      "Epoch 1462, Loss: 1.37624192237854, Final Batch Loss: 0.31377163529396057\n",
      "Epoch 1463, Loss: 1.4059028625488281, Final Batch Loss: 0.3833482265472412\n",
      "Epoch 1464, Loss: 1.3985689282417297, Final Batch Loss: 0.4020032286643982\n",
      "Epoch 1465, Loss: 1.3375470638275146, Final Batch Loss: 0.33175185322761536\n",
      "Epoch 1466, Loss: 1.3949390649795532, Final Batch Loss: 0.27526921033859253\n",
      "Epoch 1467, Loss: 1.4245922565460205, Final Batch Loss: 0.3579108417034149\n",
      "Epoch 1468, Loss: 1.3603187799453735, Final Batch Loss: 0.3317641019821167\n",
      "Epoch 1469, Loss: 1.4226824939250946, Final Batch Loss: 0.33652743697166443\n",
      "Epoch 1470, Loss: 1.2369916290044785, Final Batch Loss: 0.24670059978961945\n",
      "Epoch 1471, Loss: 1.3763792216777802, Final Batch Loss: 0.3824283480644226\n",
      "Epoch 1472, Loss: 1.2510299384593964, Final Batch Loss: 0.2617701590061188\n",
      "Epoch 1473, Loss: 1.34697887301445, Final Batch Loss: 0.4016922414302826\n",
      "Epoch 1474, Loss: 1.33160899579525, Final Batch Loss: 0.2413218766450882\n",
      "Epoch 1475, Loss: 1.415211796760559, Final Batch Loss: 0.26144930720329285\n",
      "Epoch 1476, Loss: 1.4102430641651154, Final Batch Loss: 0.35602131485939026\n",
      "Epoch 1477, Loss: 1.343976229429245, Final Batch Loss: 0.3273584246635437\n",
      "Epoch 1478, Loss: 1.3083134591579437, Final Batch Loss: 0.35805773735046387\n",
      "Epoch 1479, Loss: 1.417950987815857, Final Batch Loss: 0.5004642009735107\n",
      "Epoch 1480, Loss: 1.3544389009475708, Final Batch Loss: 0.31110960245132446\n",
      "Epoch 1481, Loss: 1.423155665397644, Final Batch Loss: 0.40738603472709656\n",
      "Epoch 1482, Loss: 1.2849544137716293, Final Batch Loss: 0.22936080396175385\n",
      "Epoch 1483, Loss: 1.363633394241333, Final Batch Loss: 0.3584239184856415\n",
      "Epoch 1484, Loss: 1.262494057416916, Final Batch Loss: 0.2559220492839813\n",
      "Epoch 1485, Loss: 1.4765908122062683, Final Batch Loss: 0.36923283338546753\n",
      "Epoch 1486, Loss: 1.3014247119426727, Final Batch Loss: 0.3125409781932831\n",
      "Epoch 1487, Loss: 1.2598926424980164, Final Batch Loss: 0.32803189754486084\n",
      "Epoch 1488, Loss: 1.3462856113910675, Final Batch Loss: 0.399819016456604\n",
      "Epoch 1489, Loss: 1.3258675932884216, Final Batch Loss: 0.3546292781829834\n",
      "Epoch 1490, Loss: 1.3296714425086975, Final Batch Loss: 0.3579106628894806\n",
      "Epoch 1491, Loss: 1.2579482793807983, Final Batch Loss: 0.2917011082172394\n",
      "Epoch 1492, Loss: 1.365017592906952, Final Batch Loss: 0.3125987946987152\n",
      "Epoch 1493, Loss: 1.3084251582622528, Final Batch Loss: 0.26136139035224915\n",
      "Epoch 1494, Loss: 1.369865357875824, Final Batch Loss: 0.3885643184185028\n",
      "Epoch 1495, Loss: 1.389039009809494, Final Batch Loss: 0.29259684681892395\n",
      "Epoch 1496, Loss: 1.3132804334163666, Final Batch Loss: 0.31299498677253723\n",
      "Epoch 1497, Loss: 1.3827470242977142, Final Batch Loss: 0.35671666264533997\n",
      "Epoch 1498, Loss: 1.2769738733768463, Final Batch Loss: 0.27202746272087097\n",
      "Epoch 1499, Loss: 1.3502189218997955, Final Batch Loss: 0.4030040502548218\n",
      "Epoch 1500, Loss: 1.3125158548355103, Final Batch Loss: 0.30692580342292786\n",
      "Epoch 1501, Loss: 1.4552165269851685, Final Batch Loss: 0.36853495240211487\n",
      "Epoch 1502, Loss: 1.2492454648017883, Final Batch Loss: 0.2825900614261627\n",
      "Epoch 1503, Loss: 1.337802678346634, Final Batch Loss: 0.28590476512908936\n",
      "Epoch 1504, Loss: 1.3391542434692383, Final Batch Loss: 0.33612269163131714\n",
      "Epoch 1505, Loss: 1.3339265286922455, Final Batch Loss: 0.3541819751262665\n",
      "Epoch 1506, Loss: 1.411144882440567, Final Batch Loss: 0.39253348112106323\n",
      "Epoch 1507, Loss: 1.4215969145298004, Final Batch Loss: 0.45564115047454834\n",
      "Epoch 1508, Loss: 1.2925816476345062, Final Batch Loss: 0.31335651874542236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1509, Loss: 1.2983070015907288, Final Batch Loss: 0.4194644093513489\n",
      "Epoch 1510, Loss: 1.3332290947437286, Final Batch Loss: 0.34394219517707825\n",
      "Epoch 1511, Loss: 1.3262234032154083, Final Batch Loss: 0.2826007008552551\n",
      "Epoch 1512, Loss: 1.3966149389743805, Final Batch Loss: 0.4560718834400177\n",
      "Epoch 1513, Loss: 1.3668961226940155, Final Batch Loss: 0.3781610429286957\n",
      "Epoch 1514, Loss: 1.2158215045928955, Final Batch Loss: 0.2698648273944855\n",
      "Epoch 1515, Loss: 1.3535643815994263, Final Batch Loss: 0.2899298369884491\n",
      "Epoch 1516, Loss: 1.2856312096118927, Final Batch Loss: 0.29099422693252563\n",
      "Epoch 1517, Loss: 1.2907030880451202, Final Batch Loss: 0.3585217595100403\n",
      "Epoch 1518, Loss: 1.279444932937622, Final Batch Loss: 0.2703173756599426\n",
      "Epoch 1519, Loss: 1.279944658279419, Final Batch Loss: 0.3528183400630951\n",
      "Epoch 1520, Loss: 1.5512376725673676, Final Batch Loss: 0.5709919929504395\n",
      "Epoch 1521, Loss: 1.3553167879581451, Final Batch Loss: 0.3050765097141266\n",
      "Epoch 1522, Loss: 1.3086805939674377, Final Batch Loss: 0.2748109996318817\n",
      "Epoch 1523, Loss: 1.3728817105293274, Final Batch Loss: 0.43368861079216003\n",
      "Epoch 1524, Loss: 1.3469165563583374, Final Batch Loss: 0.3441982567310333\n",
      "Epoch 1525, Loss: 1.270534336566925, Final Batch Loss: 0.29649442434310913\n",
      "Epoch 1526, Loss: 1.364074319601059, Final Batch Loss: 0.3046638071537018\n",
      "Epoch 1527, Loss: 1.320884346961975, Final Batch Loss: 0.38255801796913147\n",
      "Epoch 1528, Loss: 1.299007773399353, Final Batch Loss: 0.309347927570343\n",
      "Epoch 1529, Loss: 1.4085436463356018, Final Batch Loss: 0.43844273686408997\n",
      "Epoch 1530, Loss: 1.3585988581180573, Final Batch Loss: 0.3839230239391327\n",
      "Epoch 1531, Loss: 1.3764152526855469, Final Batch Loss: 0.3740605115890503\n",
      "Epoch 1532, Loss: 1.2479648292064667, Final Batch Loss: 0.2958504557609558\n",
      "Epoch 1533, Loss: 1.3903047144412994, Final Batch Loss: 0.4047342538833618\n",
      "Epoch 1534, Loss: 1.3232356309890747, Final Batch Loss: 0.37877997756004333\n",
      "Epoch 1535, Loss: 1.3395141661167145, Final Batch Loss: 0.3255319595336914\n",
      "Epoch 1536, Loss: 1.413489043712616, Final Batch Loss: 0.4184418320655823\n",
      "Epoch 1537, Loss: 1.266430914402008, Final Batch Loss: 0.26828083395957947\n",
      "Epoch 1538, Loss: 1.2658759355545044, Final Batch Loss: 0.320949912071228\n",
      "Epoch 1539, Loss: 1.3181885480880737, Final Batch Loss: 0.3308778405189514\n",
      "Epoch 1540, Loss: 1.3947548270225525, Final Batch Loss: 0.3846440017223358\n",
      "Epoch 1541, Loss: 1.4009425938129425, Final Batch Loss: 0.34245750308036804\n",
      "Epoch 1542, Loss: 1.3395405113697052, Final Batch Loss: 0.3018626868724823\n",
      "Epoch 1543, Loss: 1.2388725876808167, Final Batch Loss: 0.2763608694076538\n",
      "Epoch 1544, Loss: 1.4725687801837921, Final Batch Loss: 0.3717929422855377\n",
      "Epoch 1545, Loss: 1.4294072985649109, Final Batch Loss: 0.3870339095592499\n",
      "Epoch 1546, Loss: 1.2166043519973755, Final Batch Loss: 0.2856098711490631\n",
      "Epoch 1547, Loss: 1.245207041501999, Final Batch Loss: 0.31561318039894104\n",
      "Epoch 1548, Loss: 1.26810884475708, Final Batch Loss: 0.3341348171234131\n",
      "Epoch 1549, Loss: 1.4898382127285004, Final Batch Loss: 0.3305108845233917\n",
      "Epoch 1550, Loss: 1.531588226556778, Final Batch Loss: 0.45122528076171875\n",
      "Epoch 1551, Loss: 1.3442604541778564, Final Batch Loss: 0.37071943283081055\n",
      "Epoch 1552, Loss: 1.256822556257248, Final Batch Loss: 0.3555237352848053\n",
      "Epoch 1553, Loss: 1.2535722851753235, Final Batch Loss: 0.30255672335624695\n",
      "Epoch 1554, Loss: 1.303063452243805, Final Batch Loss: 0.36580243706703186\n",
      "Epoch 1555, Loss: 1.272196650505066, Final Batch Loss: 0.3337717056274414\n",
      "Epoch 1556, Loss: 1.3105957806110382, Final Batch Loss: 0.2578917145729065\n",
      "Epoch 1557, Loss: 1.3838477432727814, Final Batch Loss: 0.3887023329734802\n",
      "Epoch 1558, Loss: 1.2756749242544174, Final Batch Loss: 0.238008514046669\n",
      "Epoch 1559, Loss: 1.2792614698410034, Final Batch Loss: 0.3209376335144043\n",
      "Epoch 1560, Loss: 1.2847916185855865, Final Batch Loss: 0.3682577908039093\n",
      "Epoch 1561, Loss: 1.3294817507266998, Final Batch Loss: 0.4027213454246521\n",
      "Epoch 1562, Loss: 1.3759549856185913, Final Batch Loss: 0.38831689953804016\n",
      "Epoch 1563, Loss: 1.2793899476528168, Final Batch Loss: 0.313318133354187\n",
      "Epoch 1564, Loss: 1.2571347653865814, Final Batch Loss: 0.30799323320388794\n",
      "Epoch 1565, Loss: 1.3165371716022491, Final Batch Loss: 0.3764511048793793\n",
      "Epoch 1566, Loss: 1.2674891352653503, Final Batch Loss: 0.3318525552749634\n",
      "Epoch 1567, Loss: 1.2545206248760223, Final Batch Loss: 0.26115772128105164\n",
      "Epoch 1568, Loss: 1.2536174058914185, Final Batch Loss: 0.3271886706352234\n",
      "Epoch 1569, Loss: 1.2809429466724396, Final Batch Loss: 0.33723193407058716\n",
      "Epoch 1570, Loss: 1.3315855264663696, Final Batch Loss: 0.41158822178840637\n",
      "Epoch 1571, Loss: 1.4640943706035614, Final Batch Loss: 0.41928771138191223\n",
      "Epoch 1572, Loss: 1.2665467858314514, Final Batch Loss: 0.3526592552661896\n",
      "Epoch 1573, Loss: 1.3620672225952148, Final Batch Loss: 0.33830389380455017\n",
      "Epoch 1574, Loss: 1.3405217826366425, Final Batch Loss: 0.329115092754364\n",
      "Epoch 1575, Loss: 1.3359046876430511, Final Batch Loss: 0.3972591161727905\n",
      "Epoch 1576, Loss: 1.3666149973869324, Final Batch Loss: 0.3347204327583313\n",
      "Epoch 1577, Loss: 1.3054594695568085, Final Batch Loss: 0.3089551329612732\n",
      "Epoch 1578, Loss: 1.3218809366226196, Final Batch Loss: 0.3393823206424713\n",
      "Epoch 1579, Loss: 1.2777986228466034, Final Batch Loss: 0.25904276967048645\n",
      "Epoch 1580, Loss: 1.3221259415149689, Final Batch Loss: 0.3811768889427185\n",
      "Epoch 1581, Loss: 1.317610740661621, Final Batch Loss: 0.3481763005256653\n",
      "Epoch 1582, Loss: 1.3137106895446777, Final Batch Loss: 0.29753872752189636\n",
      "Epoch 1583, Loss: 1.2541793137788773, Final Batch Loss: 0.21495138108730316\n",
      "Epoch 1584, Loss: 1.3409690260887146, Final Batch Loss: 0.4037091135978699\n",
      "Epoch 1585, Loss: 1.3706459701061249, Final Batch Loss: 0.36717984080314636\n",
      "Epoch 1586, Loss: 1.3971045315265656, Final Batch Loss: 0.359605073928833\n",
      "Epoch 1587, Loss: 1.2374391555786133, Final Batch Loss: 0.3349650204181671\n",
      "Epoch 1588, Loss: 1.2722366452217102, Final Batch Loss: 0.2343865931034088\n",
      "Epoch 1589, Loss: 1.3572327196598053, Final Batch Loss: 0.47371706366539\n",
      "Epoch 1590, Loss: 1.3573282659053802, Final Batch Loss: 0.36957046389579773\n",
      "Epoch 1591, Loss: 1.2559215128421783, Final Batch Loss: 0.27333927154541016\n",
      "Epoch 1592, Loss: 1.2688474655151367, Final Batch Loss: 0.2581871747970581\n",
      "Epoch 1593, Loss: 1.2053554058074951, Final Batch Loss: 0.29094985127449036\n",
      "Epoch 1594, Loss: 1.2788733839988708, Final Batch Loss: 0.3329925835132599\n",
      "Epoch 1595, Loss: 1.2548968195915222, Final Batch Loss: 0.3523104190826416\n",
      "Epoch 1596, Loss: 1.344212681055069, Final Batch Loss: 0.3545686900615692\n",
      "Epoch 1597, Loss: 1.2633881270885468, Final Batch Loss: 0.3554134666919708\n",
      "Epoch 1598, Loss: 1.3537444174289703, Final Batch Loss: 0.2929588258266449\n",
      "Epoch 1599, Loss: 1.3643273413181305, Final Batch Loss: 0.3413689136505127\n",
      "Epoch 1600, Loss: 1.2337380051612854, Final Batch Loss: 0.2940695285797119\n",
      "Epoch 1601, Loss: 1.232155591249466, Final Batch Loss: 0.3277651071548462\n",
      "Epoch 1602, Loss: 1.327979326248169, Final Batch Loss: 0.28505444526672363\n",
      "Epoch 1603, Loss: 1.2662458717823029, Final Batch Loss: 0.35298556089401245\n",
      "Epoch 1604, Loss: 1.2028062492609024, Final Batch Loss: 0.20320002734661102\n",
      "Epoch 1605, Loss: 1.2373831570148468, Final Batch Loss: 0.32210543751716614\n",
      "Epoch 1606, Loss: 1.2841865122318268, Final Batch Loss: 0.30171769857406616\n",
      "Epoch 1607, Loss: 1.3114926218986511, Final Batch Loss: 0.27992403507232666\n",
      "Epoch 1608, Loss: 1.2677312195301056, Final Batch Loss: 0.3684052526950836\n",
      "Epoch 1609, Loss: 1.3819203674793243, Final Batch Loss: 0.3942777216434479\n",
      "Epoch 1610, Loss: 1.314182460308075, Final Batch Loss: 0.3116871416568756\n",
      "Epoch 1611, Loss: 1.2021370828151703, Final Batch Loss: 0.29784050583839417\n",
      "Epoch 1612, Loss: 1.191732257604599, Final Batch Loss: 0.28762659430503845\n",
      "Epoch 1613, Loss: 1.27937912940979, Final Batch Loss: 0.3402100205421448\n",
      "Epoch 1614, Loss: 1.3111131489276886, Final Batch Loss: 0.37002575397491455\n",
      "Epoch 1615, Loss: 1.3200863301753998, Final Batch Loss: 0.3918653726577759\n",
      "Epoch 1616, Loss: 1.2619093507528305, Final Batch Loss: 0.16104243695735931\n",
      "Epoch 1617, Loss: 1.4118581116199493, Final Batch Loss: 0.45519354939460754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1618, Loss: 1.276725858449936, Final Batch Loss: 0.2455584704875946\n",
      "Epoch 1619, Loss: 1.3467244803905487, Final Batch Loss: 0.33981382846832275\n",
      "Epoch 1620, Loss: 1.3494274318218231, Final Batch Loss: 0.2950887978076935\n",
      "Epoch 1621, Loss: 1.3734923601150513, Final Batch Loss: 0.2844175398349762\n",
      "Epoch 1622, Loss: 1.2469312846660614, Final Batch Loss: 0.2512040138244629\n",
      "Epoch 1623, Loss: 1.260559618473053, Final Batch Loss: 0.2840365171432495\n",
      "Epoch 1624, Loss: 1.33988156914711, Final Batch Loss: 0.36100447177886963\n",
      "Epoch 1625, Loss: 1.1631300300359726, Final Batch Loss: 0.3296755850315094\n",
      "Epoch 1626, Loss: 1.2147355675697327, Final Batch Loss: 0.3015711307525635\n",
      "Epoch 1627, Loss: 1.1688214838504791, Final Batch Loss: 0.2721986174583435\n",
      "Epoch 1628, Loss: 1.1833476722240448, Final Batch Loss: 0.2931167483329773\n",
      "Epoch 1629, Loss: 1.357517033815384, Final Batch Loss: 0.4023972749710083\n",
      "Epoch 1630, Loss: 1.2605552673339844, Final Batch Loss: 0.3401653468608856\n",
      "Epoch 1631, Loss: 1.2689486294984818, Final Batch Loss: 0.21515493094921112\n",
      "Epoch 1632, Loss: 1.2662110924720764, Final Batch Loss: 0.3603951334953308\n",
      "Epoch 1633, Loss: 1.3011643588542938, Final Batch Loss: 0.36263585090637207\n",
      "Epoch 1634, Loss: 1.237385779619217, Final Batch Loss: 0.34535521268844604\n",
      "Epoch 1635, Loss: 1.2753890454769135, Final Batch Loss: 0.2952815890312195\n",
      "Epoch 1636, Loss: 1.238265261054039, Final Batch Loss: 0.4180523157119751\n",
      "Epoch 1637, Loss: 1.326248288154602, Final Batch Loss: 0.2735224664211273\n",
      "Epoch 1638, Loss: 1.2846434116363525, Final Batch Loss: 0.3458494246006012\n",
      "Epoch 1639, Loss: 1.321693778038025, Final Batch Loss: 0.34973955154418945\n",
      "Epoch 1640, Loss: 1.2067570388317108, Final Batch Loss: 0.29848018288612366\n",
      "Epoch 1641, Loss: 1.2825647294521332, Final Batch Loss: 0.29690465331077576\n",
      "Epoch 1642, Loss: 1.2557635009288788, Final Batch Loss: 0.2919534146785736\n",
      "Epoch 1643, Loss: 1.1877200901508331, Final Batch Loss: 0.275820255279541\n",
      "Epoch 1644, Loss: 1.3662950694561005, Final Batch Loss: 0.32364189624786377\n",
      "Epoch 1645, Loss: 1.096659779548645, Final Batch Loss: 0.2181469202041626\n",
      "Epoch 1646, Loss: 1.2027798891067505, Final Batch Loss: 0.2593829333782196\n",
      "Epoch 1647, Loss: 1.27187579870224, Final Batch Loss: 0.2524086833000183\n",
      "Epoch 1648, Loss: 1.1828010380268097, Final Batch Loss: 0.3011242151260376\n",
      "Epoch 1649, Loss: 1.2936581075191498, Final Batch Loss: 0.3164489269256592\n",
      "Epoch 1650, Loss: 1.23749577999115, Final Batch Loss: 0.31720539927482605\n",
      "Epoch 1651, Loss: 1.214667171239853, Final Batch Loss: 0.3676905333995819\n",
      "Epoch 1652, Loss: 1.1742725372314453, Final Batch Loss: 0.24046766757965088\n",
      "Epoch 1653, Loss: 1.182213455438614, Final Batch Loss: 0.30413317680358887\n",
      "Epoch 1654, Loss: 1.1777458935976028, Final Batch Loss: 0.25634583830833435\n",
      "Epoch 1655, Loss: 1.3058431446552277, Final Batch Loss: 0.4110252857208252\n",
      "Epoch 1656, Loss: 1.1889349520206451, Final Batch Loss: 0.27847468852996826\n",
      "Epoch 1657, Loss: 1.210768312215805, Final Batch Loss: 0.2602289915084839\n",
      "Epoch 1658, Loss: 1.3082517236471176, Final Batch Loss: 0.42301151156425476\n",
      "Epoch 1659, Loss: 1.5240049958229065, Final Batch Loss: 0.4345254898071289\n",
      "Epoch 1660, Loss: 1.3080752789974213, Final Batch Loss: 0.3793022632598877\n",
      "Epoch 1661, Loss: 1.3089538514614105, Final Batch Loss: 0.3908689618110657\n",
      "Epoch 1662, Loss: 1.2810004353523254, Final Batch Loss: 0.36076900362968445\n",
      "Epoch 1663, Loss: 1.3423795998096466, Final Batch Loss: 0.4180706739425659\n",
      "Epoch 1664, Loss: 1.2790234088897705, Final Batch Loss: 0.3469589054584503\n",
      "Epoch 1665, Loss: 1.2920074462890625, Final Batch Loss: 0.3719247877597809\n",
      "Epoch 1666, Loss: 1.2469311952590942, Final Batch Loss: 0.3008634150028229\n",
      "Epoch 1667, Loss: 1.3700686693191528, Final Batch Loss: 0.33670303225517273\n",
      "Epoch 1668, Loss: 1.3623750805854797, Final Batch Loss: 0.28171682357788086\n",
      "Epoch 1669, Loss: 1.3367946445941925, Final Batch Loss: 0.345873087644577\n",
      "Epoch 1670, Loss: 1.2008094042539597, Final Batch Loss: 0.2443038672208786\n",
      "Epoch 1671, Loss: 1.2177462428808212, Final Batch Loss: 0.2449328750371933\n",
      "Epoch 1672, Loss: 1.2477732598781586, Final Batch Loss: 0.3315984606742859\n",
      "Epoch 1673, Loss: 1.1768791675567627, Final Batch Loss: 0.3125351071357727\n",
      "Epoch 1674, Loss: 1.1887039542198181, Final Batch Loss: 0.29257068037986755\n",
      "Epoch 1675, Loss: 1.2225531190633774, Final Batch Loss: 0.22071076929569244\n",
      "Epoch 1676, Loss: 1.3407829254865646, Final Batch Loss: 0.4152831435203552\n",
      "Epoch 1677, Loss: 1.2272914946079254, Final Batch Loss: 0.31764400005340576\n",
      "Epoch 1678, Loss: 1.154077485203743, Final Batch Loss: 0.2618664503097534\n",
      "Epoch 1679, Loss: 1.3315826058387756, Final Batch Loss: 0.3847579061985016\n",
      "Epoch 1680, Loss: 1.3373567759990692, Final Batch Loss: 0.29526466131210327\n",
      "Epoch 1681, Loss: 1.127536341547966, Final Batch Loss: 0.2775709629058838\n",
      "Epoch 1682, Loss: 1.4068515598773956, Final Batch Loss: 0.4195947051048279\n",
      "Epoch 1683, Loss: 1.244897112250328, Final Batch Loss: 0.2324208766222\n",
      "Epoch 1684, Loss: 1.1725765764713287, Final Batch Loss: 0.2989584803581238\n",
      "Epoch 1685, Loss: 1.446698546409607, Final Batch Loss: 0.36206164956092834\n",
      "Epoch 1686, Loss: 1.177993968129158, Final Batch Loss: 0.21856720745563507\n",
      "Epoch 1687, Loss: 1.1754200905561447, Final Batch Loss: 0.20794333517551422\n",
      "Epoch 1688, Loss: 1.3274719417095184, Final Batch Loss: 0.382407546043396\n",
      "Epoch 1689, Loss: 1.3667585849761963, Final Batch Loss: 0.32008183002471924\n",
      "Epoch 1690, Loss: 1.0995559841394424, Final Batch Loss: 0.22230584919452667\n",
      "Epoch 1691, Loss: 1.3613111078739166, Final Batch Loss: 0.34820011258125305\n",
      "Epoch 1692, Loss: 1.2514535784721375, Final Batch Loss: 0.2724911868572235\n",
      "Epoch 1693, Loss: 1.2073597609996796, Final Batch Loss: 0.2157437801361084\n",
      "Epoch 1694, Loss: 1.3176834881305695, Final Batch Loss: 0.4268939793109894\n",
      "Epoch 1695, Loss: 1.2995238453149796, Final Batch Loss: 0.30166372656822205\n",
      "Epoch 1696, Loss: 1.2290703654289246, Final Batch Loss: 0.3089260458946228\n",
      "Epoch 1697, Loss: 1.137534737586975, Final Batch Loss: 0.29338338971138\n",
      "Epoch 1698, Loss: 1.2990855872631073, Final Batch Loss: 0.30859196186065674\n",
      "Epoch 1699, Loss: 1.3462630212306976, Final Batch Loss: 0.3833135664463043\n",
      "Epoch 1700, Loss: 1.3535857796669006, Final Batch Loss: 0.36997097730636597\n",
      "Epoch 1701, Loss: 1.1754435449838638, Final Batch Loss: 0.240020290017128\n",
      "Epoch 1702, Loss: 1.2225061357021332, Final Batch Loss: 0.35660162568092346\n",
      "Epoch 1703, Loss: 1.3164438903331757, Final Batch Loss: 0.33593180775642395\n",
      "Epoch 1704, Loss: 1.187598317861557, Final Batch Loss: 0.29030632972717285\n",
      "Epoch 1705, Loss: 1.1637926697731018, Final Batch Loss: 0.2925340533256531\n",
      "Epoch 1706, Loss: 1.2495612800121307, Final Batch Loss: 0.2975371181964874\n",
      "Epoch 1707, Loss: 1.2586493492126465, Final Batch Loss: 0.31548887491226196\n",
      "Epoch 1708, Loss: 1.3133662641048431, Final Batch Loss: 0.3103431165218353\n",
      "Epoch 1709, Loss: 1.2050729393959045, Final Batch Loss: 0.3191634714603424\n",
      "Epoch 1710, Loss: 1.1794200539588928, Final Batch Loss: 0.27787572145462036\n",
      "Epoch 1711, Loss: 1.163090005517006, Final Batch Loss: 0.24518166482448578\n",
      "Epoch 1712, Loss: 1.225504457950592, Final Batch Loss: 0.31127044558525085\n",
      "Epoch 1713, Loss: 1.2550844848155975, Final Batch Loss: 0.3158468008041382\n",
      "Epoch 1714, Loss: 1.2534705102443695, Final Batch Loss: 0.33836933970451355\n",
      "Epoch 1715, Loss: 1.3022756576538086, Final Batch Loss: 0.30722129344940186\n",
      "Epoch 1716, Loss: 1.2516842782497406, Final Batch Loss: 0.3517725467681885\n",
      "Epoch 1717, Loss: 1.2074929475784302, Final Batch Loss: 0.300235390663147\n",
      "Epoch 1718, Loss: 1.3589676320552826, Final Batch Loss: 0.31933629512786865\n",
      "Epoch 1719, Loss: 1.2179560363292694, Final Batch Loss: 0.29271939396858215\n",
      "Epoch 1720, Loss: 1.363347202539444, Final Batch Loss: 0.4151538610458374\n",
      "Epoch 1721, Loss: 1.1082210093736649, Final Batch Loss: 0.24219925701618195\n",
      "Epoch 1722, Loss: 1.1918227672576904, Final Batch Loss: 0.27165400981903076\n",
      "Epoch 1723, Loss: 1.271715760231018, Final Batch Loss: 0.2945784032344818\n",
      "Epoch 1724, Loss: 1.1450694501399994, Final Batch Loss: 0.2707626223564148\n",
      "Epoch 1725, Loss: 1.173867180943489, Final Batch Loss: 0.2818348705768585\n",
      "Epoch 1726, Loss: 1.2881692051887512, Final Batch Loss: 0.2651509940624237\n",
      "Epoch 1727, Loss: 1.3109809756278992, Final Batch Loss: 0.37838518619537354\n",
      "Epoch 1728, Loss: 1.1403912901878357, Final Batch Loss: 0.3072268068790436\n",
      "Epoch 1729, Loss: 1.281051129102707, Final Batch Loss: 0.24768131971359253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1730, Loss: 1.21982941031456, Final Batch Loss: 0.31324657797813416\n",
      "Epoch 1731, Loss: 1.1622167229652405, Final Batch Loss: 0.25214719772338867\n",
      "Epoch 1732, Loss: 1.206113576889038, Final Batch Loss: 0.29707059264183044\n",
      "Epoch 1733, Loss: 1.1315299570560455, Final Batch Loss: 0.22443026304244995\n",
      "Epoch 1734, Loss: 1.1912576407194138, Final Batch Loss: 0.2672812342643738\n",
      "Epoch 1735, Loss: 1.2040081322193146, Final Batch Loss: 0.25254884362220764\n",
      "Epoch 1736, Loss: 1.2466617822647095, Final Batch Loss: 0.298519104719162\n",
      "Epoch 1737, Loss: 1.2413937747478485, Final Batch Loss: 0.2926304042339325\n",
      "Epoch 1738, Loss: 1.1836809515953064, Final Batch Loss: 0.3449168801307678\n",
      "Epoch 1739, Loss: 1.2419263422489166, Final Batch Loss: 0.3994089961051941\n",
      "Epoch 1740, Loss: 1.249837338924408, Final Batch Loss: 0.4224082827568054\n",
      "Epoch 1741, Loss: 1.0774898529052734, Final Batch Loss: 0.19332224130630493\n",
      "Epoch 1742, Loss: 1.1775627434253693, Final Batch Loss: 0.1907193958759308\n",
      "Epoch 1743, Loss: 1.2681462466716766, Final Batch Loss: 0.3195795714855194\n",
      "Epoch 1744, Loss: 1.2417123168706894, Final Batch Loss: 0.30875396728515625\n",
      "Epoch 1745, Loss: 1.2677533626556396, Final Batch Loss: 0.38689059019088745\n",
      "Epoch 1746, Loss: 1.315438836812973, Final Batch Loss: 0.34024927020072937\n",
      "Epoch 1747, Loss: 1.2049054205417633, Final Batch Loss: 0.30199140310287476\n",
      "Epoch 1748, Loss: 1.3089919686317444, Final Batch Loss: 0.4452033042907715\n",
      "Epoch 1749, Loss: 1.1211087256669998, Final Batch Loss: 0.2543773949146271\n",
      "Epoch 1750, Loss: 1.2306050062179565, Final Batch Loss: 0.3829335570335388\n",
      "Epoch 1751, Loss: 1.091359481215477, Final Batch Loss: 0.23172985017299652\n",
      "Epoch 1752, Loss: 1.251858651638031, Final Batch Loss: 0.39988330006599426\n",
      "Epoch 1753, Loss: 1.2122998535633087, Final Batch Loss: 0.3469544053077698\n",
      "Epoch 1754, Loss: 1.228413999080658, Final Batch Loss: 0.3253735303878784\n",
      "Epoch 1755, Loss: 1.172611579298973, Final Batch Loss: 0.30291736125946045\n",
      "Epoch 1756, Loss: 1.1598773896694183, Final Batch Loss: 0.29797831177711487\n",
      "Epoch 1757, Loss: 1.2415226697921753, Final Batch Loss: 0.2926103174686432\n",
      "Epoch 1758, Loss: 1.3575454652309418, Final Batch Loss: 0.43645790219306946\n",
      "Epoch 1759, Loss: 1.2661146521568298, Final Batch Loss: 0.32095450162887573\n",
      "Epoch 1760, Loss: 1.339875340461731, Final Batch Loss: 0.4180788993835449\n",
      "Epoch 1761, Loss: 1.345567673444748, Final Batch Loss: 0.3237859904766083\n",
      "Epoch 1762, Loss: 1.2939480543136597, Final Batch Loss: 0.32223817706108093\n",
      "Epoch 1763, Loss: 1.2449613511562347, Final Batch Loss: 0.26175743341445923\n",
      "Epoch 1764, Loss: 1.2560770213603973, Final Batch Loss: 0.22672978043556213\n",
      "Epoch 1765, Loss: 1.1436685770750046, Final Batch Loss: 0.3289567530155182\n",
      "Epoch 1766, Loss: 1.165678709745407, Final Batch Loss: 0.2739955484867096\n",
      "Epoch 1767, Loss: 1.2703472077846527, Final Batch Loss: 0.318191796541214\n",
      "Epoch 1768, Loss: 1.2287582755088806, Final Batch Loss: 0.27716317772865295\n",
      "Epoch 1769, Loss: 1.2895424664020538, Final Batch Loss: 0.33059602975845337\n",
      "Epoch 1770, Loss: 1.390768587589264, Final Batch Loss: 0.4031479060649872\n",
      "Epoch 1771, Loss: 1.1557014286518097, Final Batch Loss: 0.2843238413333893\n",
      "Epoch 1772, Loss: 1.250810593366623, Final Batch Loss: 0.30593788623809814\n",
      "Epoch 1773, Loss: 1.3359170258045197, Final Batch Loss: 0.36003559827804565\n",
      "Epoch 1774, Loss: 1.0685786604881287, Final Batch Loss: 0.2193504124879837\n",
      "Epoch 1775, Loss: 1.3727230727672577, Final Batch Loss: 0.32783424854278564\n",
      "Epoch 1776, Loss: 1.086530178785324, Final Batch Loss: 0.24408628046512604\n",
      "Epoch 1777, Loss: 1.209159642457962, Final Batch Loss: 0.32111606001853943\n",
      "Epoch 1778, Loss: 1.188102588057518, Final Batch Loss: 0.2290773242712021\n",
      "Epoch 1779, Loss: 1.183982565999031, Final Batch Loss: 0.3417145311832428\n",
      "Epoch 1780, Loss: 1.1631451547145844, Final Batch Loss: 0.2833911180496216\n",
      "Epoch 1781, Loss: 1.2209542095661163, Final Batch Loss: 0.3306070566177368\n",
      "Epoch 1782, Loss: 1.1237434148788452, Final Batch Loss: 0.27649635076522827\n",
      "Epoch 1783, Loss: 1.1834628283977509, Final Batch Loss: 0.34489935636520386\n",
      "Epoch 1784, Loss: 1.2244071513414383, Final Batch Loss: 0.28468039631843567\n",
      "Epoch 1785, Loss: 1.2810907810926437, Final Batch Loss: 0.3554736077785492\n",
      "Epoch 1786, Loss: 1.2725254595279694, Final Batch Loss: 0.32882699370384216\n",
      "Epoch 1787, Loss: 1.430252581834793, Final Batch Loss: 0.4242311716079712\n",
      "Epoch 1788, Loss: 1.1750834286212921, Final Batch Loss: 0.2791486382484436\n",
      "Epoch 1789, Loss: 1.1131508201360703, Final Batch Loss: 0.22121970355510712\n",
      "Epoch 1790, Loss: 1.2483745515346527, Final Batch Loss: 0.32429227232933044\n",
      "Epoch 1791, Loss: 1.185517817735672, Final Batch Loss: 0.29452261328697205\n",
      "Epoch 1792, Loss: 1.2055655419826508, Final Batch Loss: 0.30499356985092163\n",
      "Epoch 1793, Loss: 1.3524550795555115, Final Batch Loss: 0.27491655945777893\n",
      "Epoch 1794, Loss: 1.3314162492752075, Final Batch Loss: 0.32632383704185486\n",
      "Epoch 1795, Loss: 1.3266324400901794, Final Batch Loss: 0.2973763048648834\n",
      "Epoch 1796, Loss: 1.1779184937477112, Final Batch Loss: 0.3016103506088257\n",
      "Epoch 1797, Loss: 1.1799329668283463, Final Batch Loss: 0.29108086228370667\n",
      "Epoch 1798, Loss: 1.1840493977069855, Final Batch Loss: 0.2939944565296173\n",
      "Epoch 1799, Loss: 1.226815551519394, Final Batch Loss: 0.28873705863952637\n",
      "Epoch 1800, Loss: 1.2110709249973297, Final Batch Loss: 0.2870114743709564\n",
      "Epoch 1801, Loss: 1.1496029496192932, Final Batch Loss: 0.29445207118988037\n",
      "Epoch 1802, Loss: 1.1446459889411926, Final Batch Loss: 0.2753496468067169\n",
      "Epoch 1803, Loss: 1.1664806008338928, Final Batch Loss: 0.2543236315250397\n",
      "Epoch 1804, Loss: 1.280480444431305, Final Batch Loss: 0.32940495014190674\n",
      "Epoch 1805, Loss: 1.289550632238388, Final Batch Loss: 0.34536778926849365\n",
      "Epoch 1806, Loss: 1.0911145508289337, Final Batch Loss: 0.25925731658935547\n",
      "Epoch 1807, Loss: 1.2081401646137238, Final Batch Loss: 0.3087531328201294\n",
      "Epoch 1808, Loss: 1.1525856405496597, Final Batch Loss: 0.27241265773773193\n",
      "Epoch 1809, Loss: 1.176573008298874, Final Batch Loss: 0.25435569882392883\n",
      "Epoch 1810, Loss: 1.2092671394348145, Final Batch Loss: 0.293005108833313\n",
      "Epoch 1811, Loss: 1.301782876253128, Final Batch Loss: 0.4026498794555664\n",
      "Epoch 1812, Loss: 1.1845372319221497, Final Batch Loss: 0.35666248202323914\n",
      "Epoch 1813, Loss: 1.1298258602619171, Final Batch Loss: 0.2799347937107086\n",
      "Epoch 1814, Loss: 1.325608879327774, Final Batch Loss: 0.4047473669052124\n",
      "Epoch 1815, Loss: 1.2051404118537903, Final Batch Loss: 0.28310704231262207\n",
      "Epoch 1816, Loss: 1.1639204621315002, Final Batch Loss: 0.33059272170066833\n",
      "Epoch 1817, Loss: 1.1848806142807007, Final Batch Loss: 0.3778056502342224\n",
      "Epoch 1818, Loss: 1.18988698720932, Final Batch Loss: 0.28572988510131836\n",
      "Epoch 1819, Loss: 1.2665563225746155, Final Batch Loss: 0.3237469494342804\n",
      "Epoch 1820, Loss: 1.2356545180082321, Final Batch Loss: 0.36808282136917114\n",
      "Epoch 1821, Loss: 1.1776992976665497, Final Batch Loss: 0.2652534544467926\n",
      "Epoch 1822, Loss: 1.2333641648292542, Final Batch Loss: 0.38983258605003357\n",
      "Epoch 1823, Loss: 1.2873805165290833, Final Batch Loss: 0.4155067801475525\n",
      "Epoch 1824, Loss: 1.101190835237503, Final Batch Loss: 0.2579194903373718\n",
      "Epoch 1825, Loss: 1.1701789200305939, Final Batch Loss: 0.2830957770347595\n",
      "Epoch 1826, Loss: 1.270691841840744, Final Batch Loss: 0.27198848128318787\n",
      "Epoch 1827, Loss: 1.2390480637550354, Final Batch Loss: 0.3022620379924774\n",
      "Epoch 1828, Loss: 1.2245712727308273, Final Batch Loss: 0.337734192609787\n",
      "Epoch 1829, Loss: 1.3254200518131256, Final Batch Loss: 0.34943756461143494\n",
      "Epoch 1830, Loss: 1.1385135799646378, Final Batch Loss: 0.21841628849506378\n",
      "Epoch 1831, Loss: 1.4128874242305756, Final Batch Loss: 0.46615859866142273\n",
      "Epoch 1832, Loss: 1.2426928132772446, Final Batch Loss: 0.45922723412513733\n",
      "Epoch 1833, Loss: 1.1705156564712524, Final Batch Loss: 0.26151371002197266\n",
      "Epoch 1834, Loss: 1.2487362027168274, Final Batch Loss: 0.36580684781074524\n",
      "Epoch 1835, Loss: 1.335776150226593, Final Batch Loss: 0.30819451808929443\n",
      "Epoch 1836, Loss: 1.1741827130317688, Final Batch Loss: 0.1894439160823822\n",
      "Epoch 1837, Loss: 1.1901921778917313, Final Batch Loss: 0.3614312708377838\n",
      "Epoch 1838, Loss: 1.3324079513549805, Final Batch Loss: 0.4015607535839081\n",
      "Epoch 1839, Loss: 1.2256125807762146, Final Batch Loss: 0.3091993033885956\n",
      "Epoch 1840, Loss: 1.227693259716034, Final Batch Loss: 0.3027492165565491\n",
      "Epoch 1841, Loss: 1.2677390575408936, Final Batch Loss: 0.2951904237270355\n",
      "Epoch 1842, Loss: 1.3297461569309235, Final Batch Loss: 0.3897233307361603\n",
      "Epoch 1843, Loss: 1.1559660881757736, Final Batch Loss: 0.2800147831439972\n",
      "Epoch 1844, Loss: 1.160859763622284, Final Batch Loss: 0.29258474707603455\n",
      "Epoch 1845, Loss: 1.2567913830280304, Final Batch Loss: 0.302223801612854\n",
      "Epoch 1846, Loss: 1.190583735704422, Final Batch Loss: 0.3026767671108246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1847, Loss: 1.171823799610138, Final Batch Loss: 0.3658939599990845\n",
      "Epoch 1848, Loss: 1.1947399079799652, Final Batch Loss: 0.2983073890209198\n",
      "Epoch 1849, Loss: 1.2211551666259766, Final Batch Loss: 0.3222426772117615\n",
      "Epoch 1850, Loss: 1.1979619562625885, Final Batch Loss: 0.3098447024822235\n",
      "Epoch 1851, Loss: 1.1901240646839142, Final Batch Loss: 0.29101648926734924\n",
      "Epoch 1852, Loss: 1.190134733915329, Final Batch Loss: 0.3245391547679901\n",
      "Epoch 1853, Loss: 1.2843464016914368, Final Batch Loss: 0.42695602774620056\n",
      "Epoch 1854, Loss: 1.133267879486084, Final Batch Loss: 0.26344218850135803\n",
      "Epoch 1855, Loss: 1.1688359379768372, Final Batch Loss: 0.33006373047828674\n",
      "Epoch 1856, Loss: 1.207933485507965, Final Batch Loss: 0.32548975944519043\n",
      "Epoch 1857, Loss: 1.080199882388115, Final Batch Loss: 0.2325538545846939\n",
      "Epoch 1858, Loss: 1.097073271870613, Final Batch Loss: 0.29440414905548096\n",
      "Epoch 1859, Loss: 1.1782735586166382, Final Batch Loss: 0.26429012417793274\n",
      "Epoch 1860, Loss: 1.1993815302848816, Final Batch Loss: 0.3462107181549072\n",
      "Epoch 1861, Loss: 1.1005453318357468, Final Batch Loss: 0.20247472822666168\n",
      "Epoch 1862, Loss: 1.1859480738639832, Final Batch Loss: 0.32103559374809265\n",
      "Epoch 1863, Loss: 1.2260345816612244, Final Batch Loss: 0.3129531443119049\n",
      "Epoch 1864, Loss: 1.3321231007575989, Final Batch Loss: 0.37929534912109375\n",
      "Epoch 1865, Loss: 1.236413687467575, Final Batch Loss: 0.39348044991493225\n",
      "Epoch 1866, Loss: 1.2120436429977417, Final Batch Loss: 0.3229714632034302\n",
      "Epoch 1867, Loss: 1.239201843738556, Final Batch Loss: 0.2501087188720703\n",
      "Epoch 1868, Loss: 1.2874934673309326, Final Batch Loss: 0.412015825510025\n",
      "Epoch 1869, Loss: 1.0475384891033173, Final Batch Loss: 0.2135939598083496\n",
      "Epoch 1870, Loss: 1.143066257238388, Final Batch Loss: 0.2694391906261444\n",
      "Epoch 1871, Loss: 1.1207825690507889, Final Batch Loss: 0.2557622194290161\n",
      "Epoch 1872, Loss: 1.1924996972084045, Final Batch Loss: 0.32892197370529175\n",
      "Epoch 1873, Loss: 1.1961848735809326, Final Batch Loss: 0.3529476225376129\n",
      "Epoch 1874, Loss: 1.2251910865306854, Final Batch Loss: 0.27463799715042114\n",
      "Epoch 1875, Loss: 1.2126376777887344, Final Batch Loss: 0.34202560782432556\n",
      "Epoch 1876, Loss: 1.2939100861549377, Final Batch Loss: 0.42539331316947937\n",
      "Epoch 1877, Loss: 1.2305988669395447, Final Batch Loss: 0.30709517002105713\n",
      "Epoch 1878, Loss: 1.144781231880188, Final Batch Loss: 0.2564238905906677\n",
      "Epoch 1879, Loss: 1.1403090804815292, Final Batch Loss: 0.3384082615375519\n",
      "Epoch 1880, Loss: 1.213662177324295, Final Batch Loss: 0.2532340884208679\n",
      "Epoch 1881, Loss: 1.1302830278873444, Final Batch Loss: 0.21742787957191467\n",
      "Epoch 1882, Loss: 1.217276930809021, Final Batch Loss: 0.33651795983314514\n",
      "Epoch 1883, Loss: 1.1218978613615036, Final Batch Loss: 0.3190499544143677\n",
      "Epoch 1884, Loss: 1.3530730307102203, Final Batch Loss: 0.3963065445423126\n",
      "Epoch 1885, Loss: 1.1509812772274017, Final Batch Loss: 0.281819224357605\n",
      "Epoch 1886, Loss: 1.1045354902744293, Final Batch Loss: 0.21763378381729126\n",
      "Epoch 1887, Loss: 1.3584172427654266, Final Batch Loss: 0.42022204399108887\n",
      "Epoch 1888, Loss: 1.0607302784919739, Final Batch Loss: 0.17119133472442627\n",
      "Epoch 1889, Loss: 1.0828988701105118, Final Batch Loss: 0.2835412323474884\n",
      "Epoch 1890, Loss: 1.2626480460166931, Final Batch Loss: 0.2608868479728699\n",
      "Epoch 1891, Loss: 1.2022009193897247, Final Batch Loss: 0.3269483745098114\n",
      "Epoch 1892, Loss: 1.11056849360466, Final Batch Loss: 0.29351019859313965\n",
      "Epoch 1893, Loss: 1.2490991204977036, Final Batch Loss: 0.3609060049057007\n",
      "Epoch 1894, Loss: 1.135323479771614, Final Batch Loss: 0.26865652203559875\n",
      "Epoch 1895, Loss: 1.1118814945220947, Final Batch Loss: 0.1786506474018097\n",
      "Epoch 1896, Loss: 1.19628044962883, Final Batch Loss: 0.31053581833839417\n",
      "Epoch 1897, Loss: 1.1220672577619553, Final Batch Loss: 0.23377983272075653\n",
      "Epoch 1898, Loss: 1.0813858211040497, Final Batch Loss: 0.22996506094932556\n",
      "Epoch 1899, Loss: 1.190262109041214, Final Batch Loss: 0.3239039182662964\n",
      "Epoch 1900, Loss: 1.1641187369823456, Final Batch Loss: 0.20650410652160645\n",
      "Epoch 1901, Loss: 1.165635421872139, Final Batch Loss: 0.3348703384399414\n",
      "Epoch 1902, Loss: 1.1263764947652817, Final Batch Loss: 0.2614120841026306\n",
      "Epoch 1903, Loss: 1.1832840740680695, Final Batch Loss: 0.3302856683731079\n",
      "Epoch 1904, Loss: 1.1014851778745651, Final Batch Loss: 0.32175105810165405\n",
      "Epoch 1905, Loss: 1.1085814535617828, Final Batch Loss: 0.24470260739326477\n",
      "Epoch 1906, Loss: 1.012156292796135, Final Batch Loss: 0.21426771581172943\n",
      "Epoch 1907, Loss: 1.188397154211998, Final Batch Loss: 0.3492231070995331\n",
      "Epoch 1908, Loss: 1.1588745564222336, Final Batch Loss: 0.2415405660867691\n",
      "Epoch 1909, Loss: 1.2519001960754395, Final Batch Loss: 0.3599300980567932\n",
      "Epoch 1910, Loss: 1.076773315668106, Final Batch Loss: 0.24177512526512146\n",
      "Epoch 1911, Loss: 1.1951844096183777, Final Batch Loss: 0.37113162875175476\n",
      "Epoch 1912, Loss: 1.050222471356392, Final Batch Loss: 0.19678963720798492\n",
      "Epoch 1913, Loss: 1.1024588644504547, Final Batch Loss: 0.2874870002269745\n",
      "Epoch 1914, Loss: 1.1931363195180893, Final Batch Loss: 0.41567695140838623\n",
      "Epoch 1915, Loss: 1.182598888874054, Final Batch Loss: 0.32129013538360596\n",
      "Epoch 1916, Loss: 1.3761530816555023, Final Batch Loss: 0.4558928310871124\n",
      "Epoch 1917, Loss: 1.2136952579021454, Final Batch Loss: 0.32298731803894043\n",
      "Epoch 1918, Loss: 1.2367454767227173, Final Batch Loss: 0.30796152353286743\n",
      "Epoch 1919, Loss: 1.201964482665062, Final Batch Loss: 0.328945130109787\n",
      "Epoch 1920, Loss: 1.1650698482990265, Final Batch Loss: 0.24621877074241638\n",
      "Epoch 1921, Loss: 1.172775074839592, Final Batch Loss: 0.3754196763038635\n",
      "Epoch 1922, Loss: 1.1718857735395432, Final Batch Loss: 0.2217823714017868\n",
      "Epoch 1923, Loss: 1.277459442615509, Final Batch Loss: 0.275724321603775\n",
      "Epoch 1924, Loss: 1.0498973578214645, Final Batch Loss: 0.2627772390842438\n",
      "Epoch 1925, Loss: 1.243697702884674, Final Batch Loss: 0.33972272276878357\n",
      "Epoch 1926, Loss: 1.1836472749710083, Final Batch Loss: 0.31087103486061096\n",
      "Epoch 1927, Loss: 1.0515892952680588, Final Batch Loss: 0.25754258036613464\n",
      "Epoch 1928, Loss: 1.1074343770742416, Final Batch Loss: 0.26717695593833923\n",
      "Epoch 1929, Loss: 1.1352624595165253, Final Batch Loss: 0.3096546530723572\n",
      "Epoch 1930, Loss: 1.1443271338939667, Final Batch Loss: 0.21894289553165436\n",
      "Epoch 1931, Loss: 1.2207601368427277, Final Batch Loss: 0.3039700984954834\n",
      "Epoch 1932, Loss: 1.185298591852188, Final Batch Loss: 0.2982577085494995\n",
      "Epoch 1933, Loss: 1.160552516579628, Final Batch Loss: 0.27831512689590454\n",
      "Epoch 1934, Loss: 1.1775648593902588, Final Batch Loss: 0.27535510063171387\n",
      "Epoch 1935, Loss: 1.2764926850795746, Final Batch Loss: 0.41293543577194214\n",
      "Epoch 1936, Loss: 0.9894606620073318, Final Batch Loss: 0.25726935267448425\n",
      "Epoch 1937, Loss: 1.1812859773635864, Final Batch Loss: 0.27726060152053833\n",
      "Epoch 1938, Loss: 1.2852067351341248, Final Batch Loss: 0.3154696524143219\n",
      "Epoch 1939, Loss: 1.117845505475998, Final Batch Loss: 0.26436153054237366\n",
      "Epoch 1940, Loss: 1.1596099138259888, Final Batch Loss: 0.31562498211860657\n",
      "Epoch 1941, Loss: 1.2102767825126648, Final Batch Loss: 0.3513791561126709\n",
      "Epoch 1942, Loss: 1.1510440707206726, Final Batch Loss: 0.2511991560459137\n",
      "Epoch 1943, Loss: 1.1079146414995193, Final Batch Loss: 0.23954473435878754\n",
      "Epoch 1944, Loss: 1.1525023579597473, Final Batch Loss: 0.27872368693351746\n",
      "Epoch 1945, Loss: 1.1796367466449738, Final Batch Loss: 0.34342408180236816\n",
      "Epoch 1946, Loss: 1.2877396643161774, Final Batch Loss: 0.4372390806674957\n",
      "Epoch 1947, Loss: 1.1653006374835968, Final Batch Loss: 0.26948240399360657\n",
      "Epoch 1948, Loss: 1.2368050813674927, Final Batch Loss: 0.3590129017829895\n",
      "Epoch 1949, Loss: 1.2290101051330566, Final Batch Loss: 0.32652467489242554\n",
      "Epoch 1950, Loss: 1.2267222106456757, Final Batch Loss: 0.3527766466140747\n",
      "Epoch 1951, Loss: 1.1233371198177338, Final Batch Loss: 0.24544960260391235\n",
      "Epoch 1952, Loss: 1.0022308081388474, Final Batch Loss: 0.1900802105665207\n",
      "Epoch 1953, Loss: 1.2310424149036407, Final Batch Loss: 0.3466980755329132\n",
      "Epoch 1954, Loss: 1.3068114519119263, Final Batch Loss: 0.35263022780418396\n",
      "Epoch 1955, Loss: 1.284834623336792, Final Batch Loss: 0.43625032901763916\n",
      "Epoch 1956, Loss: 1.2270280420780182, Final Batch Loss: 0.3075212836265564\n",
      "Epoch 1957, Loss: 1.1871653944253922, Final Batch Loss: 0.2106315642595291\n",
      "Epoch 1958, Loss: 1.1213929057121277, Final Batch Loss: 0.2804400324821472\n",
      "Epoch 1959, Loss: 1.2307137548923492, Final Batch Loss: 0.3148622512817383\n",
      "Epoch 1960, Loss: 1.1711864173412323, Final Batch Loss: 0.34017929434776306\n",
      "Epoch 1961, Loss: 1.1896179020404816, Final Batch Loss: 0.2712801396846771\n",
      "Epoch 1962, Loss: 1.1894734799861908, Final Batch Loss: 0.3436959385871887\n",
      "Epoch 1963, Loss: 1.1490075588226318, Final Batch Loss: 0.24113717675209045\n",
      "Epoch 1964, Loss: 1.0674352794885635, Final Batch Loss: 0.2949240207672119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1965, Loss: 1.1935808658599854, Final Batch Loss: 0.3457079529762268\n",
      "Epoch 1966, Loss: 1.2991037666797638, Final Batch Loss: 0.3302418887615204\n",
      "Epoch 1967, Loss: 1.306849867105484, Final Batch Loss: 0.3722410798072815\n",
      "Epoch 1968, Loss: 1.1372565031051636, Final Batch Loss: 0.2405708134174347\n",
      "Epoch 1969, Loss: 1.191614419221878, Final Batch Loss: 0.3034266233444214\n",
      "Epoch 1970, Loss: 1.1032797545194626, Final Batch Loss: 0.31303372979164124\n",
      "Epoch 1971, Loss: 1.1939961165189743, Final Batch Loss: 0.2223469763994217\n",
      "Epoch 1972, Loss: 1.2051593661308289, Final Batch Loss: 0.33999332785606384\n",
      "Epoch 1973, Loss: 1.1986853629350662, Final Batch Loss: 0.18210159242153168\n",
      "Epoch 1974, Loss: 1.189076989889145, Final Batch Loss: 0.3333881199359894\n",
      "Epoch 1975, Loss: 1.169198453426361, Final Batch Loss: 0.4005577862262726\n",
      "Epoch 1976, Loss: 1.1886871457099915, Final Batch Loss: 0.34502413868904114\n",
      "Epoch 1977, Loss: 1.086362898349762, Final Batch Loss: 0.27396097779273987\n",
      "Epoch 1978, Loss: 1.0500771403312683, Final Batch Loss: 0.2997637391090393\n",
      "Epoch 1979, Loss: 1.0769532471895218, Final Batch Loss: 0.2843322455883026\n",
      "Epoch 1980, Loss: 1.2070278525352478, Final Batch Loss: 0.29869967699050903\n",
      "Epoch 1981, Loss: 1.1889367699623108, Final Batch Loss: 0.3272889256477356\n",
      "Epoch 1982, Loss: 1.155087798833847, Final Batch Loss: 0.3056299090385437\n",
      "Epoch 1983, Loss: 1.1481749266386032, Final Batch Loss: 0.30149179697036743\n",
      "Epoch 1984, Loss: 1.189544677734375, Final Batch Loss: 0.33278483152389526\n",
      "Epoch 1985, Loss: 1.1559307426214218, Final Batch Loss: 0.2442578822374344\n",
      "Epoch 1986, Loss: 1.2903039157390594, Final Batch Loss: 0.3152916133403778\n",
      "Epoch 1987, Loss: 1.1611589193344116, Final Batch Loss: 0.30713173747062683\n",
      "Epoch 1988, Loss: 1.1951042264699936, Final Batch Loss: 0.40226027369499207\n",
      "Epoch 1989, Loss: 1.0516655147075653, Final Batch Loss: 0.2751295566558838\n",
      "Epoch 1990, Loss: 1.2078116536140442, Final Batch Loss: 0.45620468258857727\n",
      "Epoch 1991, Loss: 1.1964490115642548, Final Batch Loss: 0.280521959066391\n",
      "Epoch 1992, Loss: 1.1852252185344696, Final Batch Loss: 0.32250556349754333\n",
      "Epoch 1993, Loss: 1.1730299592018127, Final Batch Loss: 0.29477766156196594\n",
      "Epoch 1994, Loss: 1.1201607286930084, Final Batch Loss: 0.2896513044834137\n",
      "Epoch 1995, Loss: 1.2151224613189697, Final Batch Loss: 0.3426520824432373\n",
      "Epoch 1996, Loss: 1.1382458209991455, Final Batch Loss: 0.2296673059463501\n",
      "Epoch 1997, Loss: 1.1776171624660492, Final Batch Loss: 0.28387999534606934\n",
      "Epoch 1998, Loss: 1.0578226298093796, Final Batch Loss: 0.2445637434720993\n",
      "Epoch 1999, Loss: 1.1969089806079865, Final Batch Loss: 0.30979102849960327\n",
      "Epoch 2000, Loss: 1.0557405054569244, Final Batch Loss: 0.25187888741493225\n",
      "Epoch 2001, Loss: 1.016516998410225, Final Batch Loss: 0.18873031437397003\n",
      "Epoch 2002, Loss: 1.2133724093437195, Final Batch Loss: 0.24993759393692017\n",
      "Epoch 2003, Loss: 1.062811478972435, Final Batch Loss: 0.22732575237751007\n",
      "Epoch 2004, Loss: 1.114744946360588, Final Batch Loss: 0.22553332149982452\n",
      "Epoch 2005, Loss: 1.128297820687294, Final Batch Loss: 0.29058924317359924\n",
      "Epoch 2006, Loss: 1.0964433550834656, Final Batch Loss: 0.25158241391181946\n",
      "Epoch 2007, Loss: 1.2100679874420166, Final Batch Loss: 0.3197687268257141\n",
      "Epoch 2008, Loss: 1.2122639566659927, Final Batch Loss: 0.24524076282978058\n",
      "Epoch 2009, Loss: 1.057303100824356, Final Batch Loss: 0.2585279047489166\n",
      "Epoch 2010, Loss: 1.1335256546735764, Final Batch Loss: 0.21428434550762177\n",
      "Epoch 2011, Loss: 1.1265438944101334, Final Batch Loss: 0.2953515648841858\n",
      "Epoch 2012, Loss: 1.0710048973560333, Final Batch Loss: 0.2708100378513336\n",
      "Epoch 2013, Loss: 1.1791448444128036, Final Batch Loss: 0.21979622542858124\n",
      "Epoch 2014, Loss: 1.2111275643110275, Final Batch Loss: 0.34841981530189514\n",
      "Epoch 2015, Loss: 1.1697946935892105, Final Batch Loss: 0.280109703540802\n",
      "Epoch 2016, Loss: 1.1901449710130692, Final Batch Loss: 0.24581997096538544\n",
      "Epoch 2017, Loss: 1.1756073087453842, Final Batch Loss: 0.33951127529144287\n",
      "Epoch 2018, Loss: 1.0806131660938263, Final Batch Loss: 0.27935436367988586\n",
      "Epoch 2019, Loss: 1.094538927078247, Final Batch Loss: 0.26328083872795105\n",
      "Epoch 2020, Loss: 1.0140228867530823, Final Batch Loss: 0.21397213637828827\n",
      "Epoch 2021, Loss: 1.1724548041820526, Final Batch Loss: 0.30679982900619507\n",
      "Epoch 2022, Loss: 1.0410258322954178, Final Batch Loss: 0.20903749763965607\n",
      "Epoch 2023, Loss: 1.1941912472248077, Final Batch Loss: 0.31549692153930664\n",
      "Epoch 2024, Loss: 1.0474383682012558, Final Batch Loss: 0.2444867044687271\n",
      "Epoch 2025, Loss: 1.1167408227920532, Final Batch Loss: 0.27288076281547546\n",
      "Epoch 2026, Loss: 1.086255744099617, Final Batch Loss: 0.29340940713882446\n",
      "Epoch 2027, Loss: 1.0992475301027298, Final Batch Loss: 0.20993109047412872\n",
      "Epoch 2028, Loss: 1.1119037717580795, Final Batch Loss: 0.23303984105587006\n",
      "Epoch 2029, Loss: 1.0665218085050583, Final Batch Loss: 0.25903788208961487\n",
      "Epoch 2030, Loss: 1.0325957983732224, Final Batch Loss: 0.24197664856910706\n",
      "Epoch 2031, Loss: 1.1284984946250916, Final Batch Loss: 0.28869232535362244\n",
      "Epoch 2032, Loss: 1.1748171150684357, Final Batch Loss: 0.34051939845085144\n",
      "Epoch 2033, Loss: 1.1424921751022339, Final Batch Loss: 0.2724389135837555\n",
      "Epoch 2034, Loss: 1.1022857129573822, Final Batch Loss: 0.3697274923324585\n",
      "Epoch 2035, Loss: 1.1463193595409393, Final Batch Loss: 0.38271602988243103\n",
      "Epoch 2036, Loss: 1.0468786805868149, Final Batch Loss: 0.28812193870544434\n",
      "Epoch 2037, Loss: 1.1042229235172272, Final Batch Loss: 0.22108179330825806\n",
      "Epoch 2038, Loss: 1.0995435267686844, Final Batch Loss: 0.16416017711162567\n",
      "Epoch 2039, Loss: 1.202177882194519, Final Batch Loss: 0.39334791898727417\n",
      "Epoch 2040, Loss: 1.1182942241430283, Final Batch Loss: 0.2341902107000351\n",
      "Epoch 2041, Loss: 1.0946089327335358, Final Batch Loss: 0.32329025864601135\n",
      "Epoch 2042, Loss: 1.2244715094566345, Final Batch Loss: 0.34326159954071045\n",
      "Epoch 2043, Loss: 1.1769033074378967, Final Batch Loss: 0.29852578043937683\n",
      "Epoch 2044, Loss: 1.0200348049402237, Final Batch Loss: 0.25025662779808044\n",
      "Epoch 2045, Loss: 0.9898387491703033, Final Batch Loss: 0.23150403797626495\n",
      "Epoch 2046, Loss: 1.2188068330287933, Final Batch Loss: 0.36882612109184265\n",
      "Epoch 2047, Loss: 1.0880554020404816, Final Batch Loss: 0.2725647985935211\n",
      "Epoch 2048, Loss: 1.1276773512363434, Final Batch Loss: 0.3032371401786804\n",
      "Epoch 2049, Loss: 1.066188395023346, Final Batch Loss: 0.3237229287624359\n",
      "Epoch 2050, Loss: 1.0451762676239014, Final Batch Loss: 0.2989085912704468\n",
      "Epoch 2051, Loss: 1.029050588607788, Final Batch Loss: 0.2740301191806793\n",
      "Epoch 2052, Loss: 1.1065658926963806, Final Batch Loss: 0.25212806463241577\n",
      "Epoch 2053, Loss: 1.1337384283542633, Final Batch Loss: 0.30085518956184387\n",
      "Epoch 2054, Loss: 1.0782224535942078, Final Batch Loss: 0.3159414827823639\n",
      "Epoch 2055, Loss: 1.0930733382701874, Final Batch Loss: 0.2876395881175995\n",
      "Epoch 2056, Loss: 1.261972337961197, Final Batch Loss: 0.41739749908447266\n",
      "Epoch 2057, Loss: 1.2096053808927536, Final Batch Loss: 0.3366730511188507\n",
      "Epoch 2058, Loss: 1.260577529668808, Final Batch Loss: 0.2758292555809021\n",
      "Epoch 2059, Loss: 1.1007118374109268, Final Batch Loss: 0.23567070066928864\n",
      "Epoch 2060, Loss: 1.0223406851291656, Final Batch Loss: 0.19776231050491333\n",
      "Epoch 2061, Loss: 1.1496757119894028, Final Batch Loss: 0.21454750001430511\n",
      "Epoch 2062, Loss: 1.0865696519613266, Final Batch Loss: 0.2041611224412918\n",
      "Epoch 2063, Loss: 1.0429599583148956, Final Batch Loss: 0.22364097833633423\n",
      "Epoch 2064, Loss: 1.1002197563648224, Final Batch Loss: 0.27226442098617554\n",
      "Epoch 2065, Loss: 1.1274430453777313, Final Batch Loss: 0.2478671371936798\n",
      "Epoch 2066, Loss: 1.1073927581310272, Final Batch Loss: 0.3198308050632477\n",
      "Epoch 2067, Loss: 1.0670208930969238, Final Batch Loss: 0.2746044099330902\n",
      "Epoch 2068, Loss: 1.1361761689186096, Final Batch Loss: 0.3654455542564392\n",
      "Epoch 2069, Loss: 1.1200617402791977, Final Batch Loss: 0.2388858199119568\n",
      "Epoch 2070, Loss: 1.0994700640439987, Final Batch Loss: 0.26248699426651\n",
      "Epoch 2071, Loss: 1.1753675490617752, Final Batch Loss: 0.344624400138855\n",
      "Epoch 2072, Loss: 1.260835587978363, Final Batch Loss: 0.2631334364414215\n",
      "Epoch 2073, Loss: 1.0556444823741913, Final Batch Loss: 0.2146468311548233\n",
      "Epoch 2074, Loss: 1.0304157137870789, Final Batch Loss: 0.25645169615745544\n",
      "Epoch 2075, Loss: 1.1078397780656815, Final Batch Loss: 0.28941354155540466\n",
      "Epoch 2076, Loss: 1.1272221058607101, Final Batch Loss: 0.3790375888347626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2077, Loss: 1.1064829677343369, Final Batch Loss: 0.37560218572616577\n",
      "Epoch 2078, Loss: 1.1302912384271622, Final Batch Loss: 0.3802802860736847\n",
      "Epoch 2079, Loss: 1.0999396443367004, Final Batch Loss: 0.2293320596218109\n",
      "Epoch 2080, Loss: 1.1192714720964432, Final Batch Loss: 0.31315916776657104\n",
      "Epoch 2081, Loss: 0.9955030679702759, Final Batch Loss: 0.17505046725273132\n",
      "Epoch 2082, Loss: 1.1776880323886871, Final Batch Loss: 0.4107125401496887\n",
      "Epoch 2083, Loss: 1.0054108202457428, Final Batch Loss: 0.3070022761821747\n",
      "Epoch 2084, Loss: 1.1563032269477844, Final Batch Loss: 0.3196943700313568\n",
      "Epoch 2085, Loss: 1.1499724984169006, Final Batch Loss: 0.3370051980018616\n",
      "Epoch 2086, Loss: 1.1664460003376007, Final Batch Loss: 0.2367645800113678\n",
      "Epoch 2087, Loss: 1.069944128394127, Final Batch Loss: 0.26610639691352844\n",
      "Epoch 2088, Loss: 0.9661027193069458, Final Batch Loss: 0.17123812437057495\n",
      "Epoch 2089, Loss: 1.083542361855507, Final Batch Loss: 0.27809762954711914\n",
      "Epoch 2090, Loss: 0.9913991838693619, Final Batch Loss: 0.23617899417877197\n",
      "Epoch 2091, Loss: 1.1313072443008423, Final Batch Loss: 0.2396571934223175\n",
      "Epoch 2092, Loss: 1.1889844834804535, Final Batch Loss: 0.2800864577293396\n",
      "Epoch 2093, Loss: 1.140341192483902, Final Batch Loss: 0.2608538568019867\n",
      "Epoch 2094, Loss: 1.028152957558632, Final Batch Loss: 0.27010098099708557\n",
      "Epoch 2095, Loss: 1.1899570524692535, Final Batch Loss: 0.35752683877944946\n",
      "Epoch 2096, Loss: 1.1513793766498566, Final Batch Loss: 0.32095757126808167\n",
      "Epoch 2097, Loss: 1.0779576301574707, Final Batch Loss: 0.26043039560317993\n",
      "Epoch 2098, Loss: 1.1259895861148834, Final Batch Loss: 0.2767997682094574\n",
      "Epoch 2099, Loss: 1.1286550164222717, Final Batch Loss: 0.26857900619506836\n",
      "Epoch 2100, Loss: 1.1368261575698853, Final Batch Loss: 0.2802354097366333\n",
      "Epoch 2101, Loss: 1.0658659785985947, Final Batch Loss: 0.24827857315540314\n",
      "Epoch 2102, Loss: 1.0667699724435806, Final Batch Loss: 0.293014258146286\n",
      "Epoch 2103, Loss: 1.1631416380405426, Final Batch Loss: 0.3504972457885742\n",
      "Epoch 2104, Loss: 1.0899310559034348, Final Batch Loss: 0.245177760720253\n",
      "Epoch 2105, Loss: 1.027018278837204, Final Batch Loss: 0.18933039903640747\n",
      "Epoch 2106, Loss: 1.1339563727378845, Final Batch Loss: 0.31717541813850403\n",
      "Epoch 2107, Loss: 1.0296478867530823, Final Batch Loss: 0.2757461667060852\n",
      "Epoch 2108, Loss: 1.0481070131063461, Final Batch Loss: 0.2706076204776764\n",
      "Epoch 2109, Loss: 1.0021833181381226, Final Batch Loss: 0.25371628999710083\n",
      "Epoch 2110, Loss: 1.0119811743497849, Final Batch Loss: 0.30606192350387573\n",
      "Epoch 2111, Loss: 1.0895779728889465, Final Batch Loss: 0.24698218703269958\n",
      "Epoch 2112, Loss: 1.1894921958446503, Final Batch Loss: 0.3082612454891205\n",
      "Epoch 2113, Loss: 1.0266877710819244, Final Batch Loss: 0.2098669707775116\n",
      "Epoch 2114, Loss: 1.0956351906061172, Final Batch Loss: 0.24986174702644348\n",
      "Epoch 2115, Loss: 1.169024795293808, Final Batch Loss: 0.27646785974502563\n",
      "Epoch 2116, Loss: 1.085577368736267, Final Batch Loss: 0.21931374073028564\n",
      "Epoch 2117, Loss: 1.0795518904924393, Final Batch Loss: 0.2408534735441208\n",
      "Epoch 2118, Loss: 1.061018392443657, Final Batch Loss: 0.23061959445476532\n",
      "Epoch 2119, Loss: 1.1104743629693985, Final Batch Loss: 0.2742162346839905\n",
      "Epoch 2120, Loss: 1.1285199671983719, Final Batch Loss: 0.28164592385292053\n",
      "Epoch 2121, Loss: 1.202539086341858, Final Batch Loss: 0.3407242000102997\n",
      "Epoch 2122, Loss: 1.142214223742485, Final Batch Loss: 0.2204851657152176\n",
      "Epoch 2123, Loss: 1.053384080529213, Final Batch Loss: 0.288351446390152\n",
      "Epoch 2124, Loss: 1.0857644230127335, Final Batch Loss: 0.3420088589191437\n",
      "Epoch 2125, Loss: 1.0949194133281708, Final Batch Loss: 0.2811132073402405\n",
      "Epoch 2126, Loss: 1.0141733437776566, Final Batch Loss: 0.24436631798744202\n",
      "Epoch 2127, Loss: 0.9319971799850464, Final Batch Loss: 0.1954820603132248\n",
      "Epoch 2128, Loss: 1.078534796833992, Final Batch Loss: 0.2701137661933899\n",
      "Epoch 2129, Loss: 1.0139164179563522, Final Batch Loss: 0.32327190041542053\n",
      "Epoch 2130, Loss: 1.0896658599376678, Final Batch Loss: 0.23753216862678528\n",
      "Epoch 2131, Loss: 1.1325684934854507, Final Batch Loss: 0.3342602550983429\n",
      "Epoch 2132, Loss: 1.1543711572885513, Final Batch Loss: 0.34539100527763367\n",
      "Epoch 2133, Loss: 1.068821832537651, Final Batch Loss: 0.23017209768295288\n",
      "Epoch 2134, Loss: 1.0990528613328934, Final Batch Loss: 0.23210135102272034\n",
      "Epoch 2135, Loss: 1.1306935548782349, Final Batch Loss: 0.288104772567749\n",
      "Epoch 2136, Loss: 1.0447910279035568, Final Batch Loss: 0.25695502758026123\n",
      "Epoch 2137, Loss: 0.9940694570541382, Final Batch Loss: 0.29188650846481323\n",
      "Epoch 2138, Loss: 1.1553976237773895, Final Batch Loss: 0.27410975098609924\n",
      "Epoch 2139, Loss: 1.055764615535736, Final Batch Loss: 0.249423086643219\n",
      "Epoch 2140, Loss: 1.075486570596695, Final Batch Loss: 0.23915863037109375\n",
      "Epoch 2141, Loss: 1.103225439786911, Final Batch Loss: 0.3457500636577606\n",
      "Epoch 2142, Loss: 1.226437196135521, Final Batch Loss: 0.3949926495552063\n",
      "Epoch 2143, Loss: 1.1320030987262726, Final Batch Loss: 0.24397528171539307\n",
      "Epoch 2144, Loss: 1.1795481443405151, Final Batch Loss: 0.2556775212287903\n",
      "Epoch 2145, Loss: 1.0699926167726517, Final Batch Loss: 0.288116455078125\n",
      "Epoch 2146, Loss: 1.1119200438261032, Final Batch Loss: 0.2741124927997589\n",
      "Epoch 2147, Loss: 1.0153545588254929, Final Batch Loss: 0.27274489402770996\n",
      "Epoch 2148, Loss: 1.1568260043859482, Final Batch Loss: 0.3324502408504486\n",
      "Epoch 2149, Loss: 1.0612186044454575, Final Batch Loss: 0.24148210883140564\n",
      "Epoch 2150, Loss: 1.0826351046562195, Final Batch Loss: 0.29910820722579956\n",
      "Epoch 2151, Loss: 1.1198561787605286, Final Batch Loss: 0.2797238230705261\n",
      "Epoch 2152, Loss: 1.0824096947908401, Final Batch Loss: 0.19104965031147003\n",
      "Epoch 2153, Loss: 1.0964218378067017, Final Batch Loss: 0.31198784708976746\n",
      "Epoch 2154, Loss: 1.269321769475937, Final Batch Loss: 0.2882255017757416\n",
      "Epoch 2155, Loss: 1.0624951720237732, Final Batch Loss: 0.2195245325565338\n",
      "Epoch 2156, Loss: 1.0725929290056229, Final Batch Loss: 0.293284147977829\n",
      "Epoch 2157, Loss: 1.0192997008562088, Final Batch Loss: 0.2577730119228363\n",
      "Epoch 2158, Loss: 1.031104937195778, Final Batch Loss: 0.23113484680652618\n",
      "Epoch 2159, Loss: 1.153484970331192, Final Batch Loss: 0.2830469608306885\n",
      "Epoch 2160, Loss: 1.0839336216449738, Final Batch Loss: 0.2942555248737335\n",
      "Epoch 2161, Loss: 0.9982636719942093, Final Batch Loss: 0.24959713220596313\n",
      "Epoch 2162, Loss: 1.1359333395957947, Final Batch Loss: 0.32811006903648376\n",
      "Epoch 2163, Loss: 1.0854685306549072, Final Batch Loss: 0.267535924911499\n",
      "Epoch 2164, Loss: 1.1695314943790436, Final Batch Loss: 0.3079572021961212\n",
      "Epoch 2165, Loss: 1.1597976684570312, Final Batch Loss: 0.274226576089859\n",
      "Epoch 2166, Loss: 1.1380689144134521, Final Batch Loss: 0.283067911863327\n",
      "Epoch 2167, Loss: 1.0659385472536087, Final Batch Loss: 0.3231502175331116\n",
      "Epoch 2168, Loss: 1.1001940220594406, Final Batch Loss: 0.22518624365329742\n",
      "Epoch 2169, Loss: 1.0527852773666382, Final Batch Loss: 0.27334463596343994\n",
      "Epoch 2170, Loss: 0.9947404563426971, Final Batch Loss: 0.235803484916687\n",
      "Epoch 2171, Loss: 1.15198615193367, Final Batch Loss: 0.32657694816589355\n",
      "Epoch 2172, Loss: 1.0856529623270035, Final Batch Loss: 0.361491858959198\n",
      "Epoch 2173, Loss: 1.1365472227334976, Final Batch Loss: 0.40500888228416443\n",
      "Epoch 2174, Loss: 1.0429694652557373, Final Batch Loss: 0.23952943086624146\n",
      "Epoch 2175, Loss: 1.0304510295391083, Final Batch Loss: 0.20817193388938904\n",
      "Epoch 2176, Loss: 1.0449627190828323, Final Batch Loss: 0.205977663397789\n",
      "Epoch 2177, Loss: 1.0571590214967728, Final Batch Loss: 0.18675895035266876\n",
      "Epoch 2178, Loss: 1.1339826434850693, Final Batch Loss: 0.2846883237361908\n",
      "Epoch 2179, Loss: 1.0287862867116928, Final Batch Loss: 0.27272266149520874\n",
      "Epoch 2180, Loss: 0.9818692058324814, Final Batch Loss: 0.276002436876297\n",
      "Epoch 2181, Loss: 1.129647672176361, Final Batch Loss: 0.31187430024147034\n",
      "Epoch 2182, Loss: 1.0741902142763138, Final Batch Loss: 0.3222024738788605\n",
      "Epoch 2183, Loss: 1.1168343126773834, Final Batch Loss: 0.2962267994880676\n",
      "Epoch 2184, Loss: 1.0962066352367401, Final Batch Loss: 0.25561976432800293\n",
      "Epoch 2185, Loss: 1.18613800406456, Final Batch Loss: 0.31565138697624207\n",
      "Epoch 2186, Loss: 1.1089704632759094, Final Batch Loss: 0.29621902108192444\n",
      "Epoch 2187, Loss: 1.109521821141243, Final Batch Loss: 0.26932960748672485\n",
      "Epoch 2188, Loss: 1.1390839368104935, Final Batch Loss: 0.38453975319862366\n",
      "Epoch 2189, Loss: 1.022321194410324, Final Batch Loss: 0.17075079679489136\n",
      "Epoch 2190, Loss: 1.060519978404045, Final Batch Loss: 0.2985582947731018\n",
      "Epoch 2191, Loss: 1.014203891158104, Final Batch Loss: 0.24578119814395905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2192, Loss: 0.9810749739408493, Final Batch Loss: 0.22734931111335754\n",
      "Epoch 2193, Loss: 1.2452434301376343, Final Batch Loss: 0.28788653016090393\n",
      "Epoch 2194, Loss: 1.1435818821191788, Final Batch Loss: 0.353736937046051\n",
      "Epoch 2195, Loss: 0.9977386146783829, Final Batch Loss: 0.23077447712421417\n",
      "Epoch 2196, Loss: 1.0390779674053192, Final Batch Loss: 0.2860211431980133\n",
      "Epoch 2197, Loss: 1.0800763219594955, Final Batch Loss: 0.24847327172756195\n",
      "Epoch 2198, Loss: 1.080582395195961, Final Batch Loss: 0.1903351992368698\n",
      "Epoch 2199, Loss: 1.0377936959266663, Final Batch Loss: 0.26209187507629395\n",
      "Epoch 2200, Loss: 1.1033525466918945, Final Batch Loss: 0.20766830444335938\n",
      "Epoch 2201, Loss: 1.246446281671524, Final Batch Loss: 0.36092761158943176\n",
      "Epoch 2202, Loss: 1.0645992159843445, Final Batch Loss: 0.22056931257247925\n",
      "Epoch 2203, Loss: 1.0433247834444046, Final Batch Loss: 0.25373589992523193\n",
      "Epoch 2204, Loss: 0.9289336055517197, Final Batch Loss: 0.2097027450799942\n",
      "Epoch 2205, Loss: 1.105515956878662, Final Batch Loss: 0.3040885031223297\n",
      "Epoch 2206, Loss: 1.1711588501930237, Final Batch Loss: 0.3390524983406067\n",
      "Epoch 2207, Loss: 1.0690057575702667, Final Batch Loss: 0.228005051612854\n",
      "Epoch 2208, Loss: 1.1858812272548676, Final Batch Loss: 0.28185197710990906\n",
      "Epoch 2209, Loss: 1.0017586201429367, Final Batch Loss: 0.23646685481071472\n",
      "Epoch 2210, Loss: 1.102740466594696, Final Batch Loss: 0.30613481998443604\n",
      "Epoch 2211, Loss: 1.029326394200325, Final Batch Loss: 0.27618056535720825\n",
      "Epoch 2212, Loss: 1.080901026725769, Final Batch Loss: 0.25258198380470276\n",
      "Epoch 2213, Loss: 1.2921465635299683, Final Batch Loss: 0.31213200092315674\n",
      "Epoch 2214, Loss: 1.1298254132270813, Final Batch Loss: 0.32454854249954224\n",
      "Epoch 2215, Loss: 1.0780651271343231, Final Batch Loss: 0.22955620288848877\n",
      "Epoch 2216, Loss: 1.0857765078544617, Final Batch Loss: 0.2824018895626068\n",
      "Epoch 2217, Loss: 1.2681437730789185, Final Batch Loss: 0.38469523191452026\n",
      "Epoch 2218, Loss: 1.1546297073364258, Final Batch Loss: 0.27630048990249634\n",
      "Epoch 2219, Loss: 1.169522687792778, Final Batch Loss: 0.2969786524772644\n",
      "Epoch 2220, Loss: 1.095369428396225, Final Batch Loss: 0.2822149097919464\n",
      "Epoch 2221, Loss: 1.0541361421346664, Final Batch Loss: 0.2750793397426605\n",
      "Epoch 2222, Loss: 1.3423004448413849, Final Batch Loss: 0.4210161566734314\n",
      "Epoch 2223, Loss: 1.1921310424804688, Final Batch Loss: 0.35146355628967285\n",
      "Epoch 2224, Loss: 1.075010135769844, Final Batch Loss: 0.24409161508083344\n",
      "Epoch 2225, Loss: 1.0850247591733932, Final Batch Loss: 0.212079256772995\n",
      "Epoch 2226, Loss: 1.1532586514949799, Final Batch Loss: 0.17445293068885803\n",
      "Epoch 2227, Loss: 1.0484206527471542, Final Batch Loss: 0.27359673380851746\n",
      "Epoch 2228, Loss: 1.1057766377925873, Final Batch Loss: 0.23822379112243652\n",
      "Epoch 2229, Loss: 1.1386877596378326, Final Batch Loss: 0.2778574228286743\n",
      "Epoch 2230, Loss: 1.0988320857286453, Final Batch Loss: 0.2942053973674774\n",
      "Epoch 2231, Loss: 1.1280547380447388, Final Batch Loss: 0.27974551916122437\n",
      "Epoch 2232, Loss: 1.0262726992368698, Final Batch Loss: 0.298348993062973\n",
      "Epoch 2233, Loss: 1.1269369274377823, Final Batch Loss: 0.3115338385105133\n",
      "Epoch 2234, Loss: 1.0130237936973572, Final Batch Loss: 0.21842344105243683\n",
      "Epoch 2235, Loss: 1.143764242529869, Final Batch Loss: 0.23929914832115173\n",
      "Epoch 2236, Loss: 1.0981519222259521, Final Batch Loss: 0.2710687220096588\n",
      "Epoch 2237, Loss: 1.0867916643619537, Final Batch Loss: 0.26041465997695923\n",
      "Epoch 2238, Loss: 1.0611950010061264, Final Batch Loss: 0.30829182267189026\n",
      "Epoch 2239, Loss: 0.9870395064353943, Final Batch Loss: 0.25779521465301514\n",
      "Epoch 2240, Loss: 1.0623485743999481, Final Batch Loss: 0.24016332626342773\n",
      "Epoch 2241, Loss: 1.1090527474880219, Final Batch Loss: 0.29032519459724426\n",
      "Epoch 2242, Loss: 1.073790803551674, Final Batch Loss: 0.20707140862941742\n",
      "Epoch 2243, Loss: 1.0321744531393051, Final Batch Loss: 0.24810728430747986\n",
      "Epoch 2244, Loss: 1.0748165398836136, Final Batch Loss: 0.29234427213668823\n",
      "Epoch 2245, Loss: 0.9692428559064865, Final Batch Loss: 0.18750257790088654\n",
      "Epoch 2246, Loss: 0.9217780232429504, Final Batch Loss: 0.1746463030576706\n",
      "Epoch 2247, Loss: 1.0574368983507156, Final Batch Loss: 0.25850528478622437\n",
      "Epoch 2248, Loss: 1.028842881321907, Final Batch Loss: 0.23568786680698395\n",
      "Epoch 2249, Loss: 1.0791551917791367, Final Batch Loss: 0.2867787778377533\n",
      "Epoch 2250, Loss: 1.1270219832658768, Final Batch Loss: 0.3454141318798065\n",
      "Epoch 2251, Loss: 1.0567044019699097, Final Batch Loss: 0.2388598918914795\n",
      "Epoch 2252, Loss: 1.0279549211263657, Final Batch Loss: 0.2574409544467926\n",
      "Epoch 2253, Loss: 1.0590994209051132, Final Batch Loss: 0.27971017360687256\n",
      "Epoch 2254, Loss: 1.0654157102108002, Final Batch Loss: 0.2979048192501068\n",
      "Epoch 2255, Loss: 1.0622669011354446, Final Batch Loss: 0.2715527415275574\n",
      "Epoch 2256, Loss: 1.1335967630147934, Final Batch Loss: 0.3138825595378876\n",
      "Epoch 2257, Loss: 1.0125270336866379, Final Batch Loss: 0.22133487462997437\n",
      "Epoch 2258, Loss: 1.0535889565944672, Final Batch Loss: 0.1844157725572586\n",
      "Epoch 2259, Loss: 0.9578759670257568, Final Batch Loss: 0.2024327963590622\n",
      "Epoch 2260, Loss: 1.1737428605556488, Final Batch Loss: 0.30921638011932373\n",
      "Epoch 2261, Loss: 1.08200803399086, Final Batch Loss: 0.25866129994392395\n",
      "Epoch 2262, Loss: 1.0609669089317322, Final Batch Loss: 0.34790685772895813\n",
      "Epoch 2263, Loss: 1.0020655244588852, Final Batch Loss: 0.3004130423069\n",
      "Epoch 2264, Loss: 0.9192613661289215, Final Batch Loss: 0.18770699203014374\n",
      "Epoch 2265, Loss: 1.0424048006534576, Final Batch Loss: 0.3004129230976105\n",
      "Epoch 2266, Loss: 1.1064241379499435, Final Batch Loss: 0.25107747316360474\n",
      "Epoch 2267, Loss: 1.066731482744217, Final Batch Loss: 0.2512274384498596\n",
      "Epoch 2268, Loss: 1.0544763207435608, Final Batch Loss: 0.27893462777137756\n",
      "Epoch 2269, Loss: 1.0274814069271088, Final Batch Loss: 0.20455928146839142\n",
      "Epoch 2270, Loss: 1.2095257937908173, Final Batch Loss: 0.349400132894516\n",
      "Epoch 2271, Loss: 1.128575623035431, Final Batch Loss: 0.3736255168914795\n",
      "Epoch 2272, Loss: 1.0572043657302856, Final Batch Loss: 0.23638401925563812\n",
      "Epoch 2273, Loss: 0.9469520598649979, Final Batch Loss: 0.24250762164592743\n",
      "Epoch 2274, Loss: 1.0333324670791626, Final Batch Loss: 0.24168270826339722\n",
      "Epoch 2275, Loss: 1.061523199081421, Final Batch Loss: 0.2841230630874634\n",
      "Epoch 2276, Loss: 0.9936410635709763, Final Batch Loss: 0.24225041270256042\n",
      "Epoch 2277, Loss: 1.0642760694026947, Final Batch Loss: 0.2559460401535034\n",
      "Epoch 2278, Loss: 1.011747270822525, Final Batch Loss: 0.21023163199424744\n",
      "Epoch 2279, Loss: 1.0246047377586365, Final Batch Loss: 0.24489575624465942\n",
      "Epoch 2280, Loss: 1.004183441400528, Final Batch Loss: 0.2451801747083664\n",
      "Epoch 2281, Loss: 0.9740097969770432, Final Batch Loss: 0.22645217180252075\n",
      "Epoch 2282, Loss: 1.1153409779071808, Final Batch Loss: 0.28878483176231384\n",
      "Epoch 2283, Loss: 1.08537957072258, Final Batch Loss: 0.3063610792160034\n",
      "Epoch 2284, Loss: 1.1333554089069366, Final Batch Loss: 0.3197646737098694\n",
      "Epoch 2285, Loss: 1.0714618116617203, Final Batch Loss: 0.2658986449241638\n",
      "Epoch 2286, Loss: 0.9733048975467682, Final Batch Loss: 0.21510621905326843\n",
      "Epoch 2287, Loss: 1.1583974808454514, Final Batch Loss: 0.37731999158859253\n",
      "Epoch 2288, Loss: 1.0025003850460052, Final Batch Loss: 0.16657495498657227\n",
      "Epoch 2289, Loss: 1.136537253856659, Final Batch Loss: 0.26822200417518616\n",
      "Epoch 2290, Loss: 0.9982064515352249, Final Batch Loss: 0.2348211258649826\n",
      "Epoch 2291, Loss: 1.0591059923171997, Final Batch Loss: 0.2805139124393463\n",
      "Epoch 2292, Loss: 1.256805658340454, Final Batch Loss: 0.3489755690097809\n",
      "Epoch 2293, Loss: 0.979606032371521, Final Batch Loss: 0.17598079144954681\n",
      "Epoch 2294, Loss: 1.2134207487106323, Final Batch Loss: 0.43019232153892517\n",
      "Epoch 2295, Loss: 1.0111596286296844, Final Batch Loss: 0.28373631834983826\n",
      "Epoch 2296, Loss: 1.1752428710460663, Final Batch Loss: 0.28800761699676514\n",
      "Epoch 2297, Loss: 1.1293188333511353, Final Batch Loss: 0.262484073638916\n",
      "Epoch 2298, Loss: 1.0133835673332214, Final Batch Loss: 0.18530693650245667\n",
      "Epoch 2299, Loss: 1.0364003032445908, Final Batch Loss: 0.3628009855747223\n",
      "Epoch 2300, Loss: 1.1757542788982391, Final Batch Loss: 0.3141593933105469\n",
      "Epoch 2301, Loss: 1.1210245341062546, Final Batch Loss: 0.31136879324913025\n",
      "Epoch 2302, Loss: 0.9236063957214355, Final Batch Loss: 0.1604883074760437\n",
      "Epoch 2303, Loss: 0.945361852645874, Final Batch Loss: 0.23435837030410767\n",
      "Epoch 2304, Loss: 1.069920614361763, Final Batch Loss: 0.2028566300868988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2305, Loss: 1.0804753452539444, Final Batch Loss: 0.22718383371829987\n",
      "Epoch 2306, Loss: 1.1249172687530518, Final Batch Loss: 0.2672739028930664\n",
      "Epoch 2307, Loss: 1.0572274923324585, Final Batch Loss: 0.28759488463401794\n",
      "Epoch 2308, Loss: 1.110117107629776, Final Batch Loss: 0.3221757113933563\n",
      "Epoch 2309, Loss: 1.033198818564415, Final Batch Loss: 0.255679190158844\n",
      "Epoch 2310, Loss: 1.1401866525411606, Final Batch Loss: 0.35843101143836975\n",
      "Epoch 2311, Loss: 0.9821776896715164, Final Batch Loss: 0.2224532663822174\n",
      "Epoch 2312, Loss: 0.9477530270814896, Final Batch Loss: 0.20894832909107208\n",
      "Epoch 2313, Loss: 1.053358256816864, Final Batch Loss: 0.2577294707298279\n",
      "Epoch 2314, Loss: 0.9691120386123657, Final Batch Loss: 0.22571566700935364\n",
      "Epoch 2315, Loss: 1.2404529750347137, Final Batch Loss: 0.4351038932800293\n",
      "Epoch 2316, Loss: 1.1009010076522827, Final Batch Loss: 0.2624824643135071\n",
      "Epoch 2317, Loss: 1.0754258632659912, Final Batch Loss: 0.29650208353996277\n",
      "Epoch 2318, Loss: 1.1032711267471313, Final Batch Loss: 0.21613194048404694\n",
      "Epoch 2319, Loss: 1.0677356868982315, Final Batch Loss: 0.2026238888502121\n",
      "Epoch 2320, Loss: 0.9690410494804382, Final Batch Loss: 0.22894640266895294\n",
      "Epoch 2321, Loss: 1.0186499953269958, Final Batch Loss: 0.24591048061847687\n",
      "Epoch 2322, Loss: 1.1897214949131012, Final Batch Loss: 0.3384280800819397\n",
      "Epoch 2323, Loss: 1.0165498852729797, Final Batch Loss: 0.24969272315502167\n",
      "Epoch 2324, Loss: 1.0477170050144196, Final Batch Loss: 0.2308647334575653\n",
      "Epoch 2325, Loss: 1.0151048302650452, Final Batch Loss: 0.24551844596862793\n",
      "Epoch 2326, Loss: 0.9640211015939713, Final Batch Loss: 0.20983998477458954\n",
      "Epoch 2327, Loss: 1.072602465748787, Final Batch Loss: 0.2901512086391449\n",
      "Epoch 2328, Loss: 1.1586026698350906, Final Batch Loss: 0.33757156133651733\n",
      "Epoch 2329, Loss: 1.0176839530467987, Final Batch Loss: 0.27850887179374695\n",
      "Epoch 2330, Loss: 0.8893784731626511, Final Batch Loss: 0.17495197057724\n",
      "Epoch 2331, Loss: 1.1164264231920242, Final Batch Loss: 0.2829654812812805\n",
      "Epoch 2332, Loss: 0.8706301897764206, Final Batch Loss: 0.14684568345546722\n",
      "Epoch 2333, Loss: 0.9829365760087967, Final Batch Loss: 0.20410801470279694\n",
      "Epoch 2334, Loss: 0.9948473870754242, Final Batch Loss: 0.19299975037574768\n",
      "Epoch 2335, Loss: 1.0549292117357254, Final Batch Loss: 0.2480313926935196\n",
      "Epoch 2336, Loss: 0.995928481221199, Final Batch Loss: 0.19545191526412964\n",
      "Epoch 2337, Loss: 1.0164830535650253, Final Batch Loss: 0.23715992271900177\n",
      "Epoch 2338, Loss: 1.2063294649124146, Final Batch Loss: 0.3731366991996765\n",
      "Epoch 2339, Loss: 1.104015201330185, Final Batch Loss: 0.22866290807724\n",
      "Epoch 2340, Loss: 1.1788477003574371, Final Batch Loss: 0.3743981719017029\n",
      "Epoch 2341, Loss: 1.0638856440782547, Final Batch Loss: 0.30564868450164795\n",
      "Epoch 2342, Loss: 1.0857020318508148, Final Batch Loss: 0.2656105160713196\n",
      "Epoch 2343, Loss: 1.143989935517311, Final Batch Loss: 0.3071520924568176\n",
      "Epoch 2344, Loss: 1.078362300992012, Final Batch Loss: 0.3287089467048645\n",
      "Epoch 2345, Loss: 1.0073534846305847, Final Batch Loss: 0.2665817439556122\n",
      "Epoch 2346, Loss: 1.1374692916870117, Final Batch Loss: 0.29317042231559753\n",
      "Epoch 2347, Loss: 1.0698572397232056, Final Batch Loss: 0.25448545813560486\n",
      "Epoch 2348, Loss: 0.9561533182859421, Final Batch Loss: 0.19656974077224731\n",
      "Epoch 2349, Loss: 1.0555708706378937, Final Batch Loss: 0.2895687520503998\n",
      "Epoch 2350, Loss: 0.9813229441642761, Final Batch Loss: 0.20424038171768188\n",
      "Epoch 2351, Loss: 1.0254773795604706, Final Batch Loss: 0.18577495217323303\n",
      "Epoch 2352, Loss: 1.080943837761879, Final Batch Loss: 0.3379627764225006\n",
      "Epoch 2353, Loss: 1.0371122509241104, Final Batch Loss: 0.2553156018257141\n",
      "Epoch 2354, Loss: 0.9338290095329285, Final Batch Loss: 0.19844520092010498\n",
      "Epoch 2355, Loss: 1.0516241639852524, Final Batch Loss: 0.2407878339290619\n",
      "Epoch 2356, Loss: 1.054461881518364, Final Batch Loss: 0.24840180575847626\n",
      "Epoch 2357, Loss: 1.05485500395298, Final Batch Loss: 0.29491257667541504\n",
      "Epoch 2358, Loss: 1.0077387541532516, Final Batch Loss: 0.25655004382133484\n",
      "Epoch 2359, Loss: 1.1174574941396713, Final Batch Loss: 0.20114372670650482\n",
      "Epoch 2360, Loss: 1.0036184340715408, Final Batch Loss: 0.17898422479629517\n",
      "Epoch 2361, Loss: 1.072218805551529, Final Batch Loss: 0.260898619890213\n",
      "Epoch 2362, Loss: 0.9651127010583878, Final Batch Loss: 0.25775063037872314\n",
      "Epoch 2363, Loss: 1.0499494671821594, Final Batch Loss: 0.2365460991859436\n",
      "Epoch 2364, Loss: 1.0197545140981674, Final Batch Loss: 0.25344324111938477\n",
      "Epoch 2365, Loss: 1.0036208033561707, Final Batch Loss: 0.25014472007751465\n",
      "Epoch 2366, Loss: 1.0138289779424667, Final Batch Loss: 0.23564107716083527\n",
      "Epoch 2367, Loss: 0.9454618543386459, Final Batch Loss: 0.22488859295845032\n",
      "Epoch 2368, Loss: 0.9961449056863785, Final Batch Loss: 0.24502797424793243\n",
      "Epoch 2369, Loss: 1.0971587002277374, Final Batch Loss: 0.2527037560939789\n",
      "Epoch 2370, Loss: 1.0620446503162384, Final Batch Loss: 0.21319884061813354\n",
      "Epoch 2371, Loss: 0.9541172087192535, Final Batch Loss: 0.22035174071788788\n",
      "Epoch 2372, Loss: 0.9723634570837021, Final Batch Loss: 0.2043077051639557\n",
      "Epoch 2373, Loss: 1.0392192006111145, Final Batch Loss: 0.19367149472236633\n",
      "Epoch 2374, Loss: 1.0659969598054886, Final Batch Loss: 0.30011093616485596\n",
      "Epoch 2375, Loss: 0.9578264504671097, Final Batch Loss: 0.259113609790802\n",
      "Epoch 2376, Loss: 1.1566442847251892, Final Batch Loss: 0.30007046461105347\n",
      "Epoch 2377, Loss: 0.9786715060472488, Final Batch Loss: 0.29050305485725403\n",
      "Epoch 2378, Loss: 1.154684603214264, Final Batch Loss: 0.27085456252098083\n",
      "Epoch 2379, Loss: 0.9741016179323196, Final Batch Loss: 0.22189202904701233\n",
      "Epoch 2380, Loss: 1.149097353219986, Final Batch Loss: 0.27592119574546814\n",
      "Epoch 2381, Loss: 1.1981227397918701, Final Batch Loss: 0.39304131269454956\n",
      "Epoch 2382, Loss: 1.0054883360862732, Final Batch Loss: 0.3126017153263092\n",
      "Epoch 2383, Loss: 0.9702480286359787, Final Batch Loss: 0.23974184691905975\n",
      "Epoch 2384, Loss: 1.0607916861772537, Final Batch Loss: 0.21674014627933502\n",
      "Epoch 2385, Loss: 1.0833192616701126, Final Batch Loss: 0.33241114020347595\n",
      "Epoch 2386, Loss: 1.0166287273168564, Final Batch Loss: 0.23126128315925598\n",
      "Epoch 2387, Loss: 0.9986371845006943, Final Batch Loss: 0.23857346177101135\n",
      "Epoch 2388, Loss: 0.9574320614337921, Final Batch Loss: 0.1748022437095642\n",
      "Epoch 2389, Loss: 0.9932399094104767, Final Batch Loss: 0.24674347043037415\n",
      "Epoch 2390, Loss: 1.0213202238082886, Final Batch Loss: 0.25677794218063354\n",
      "Epoch 2391, Loss: 1.1211474388837814, Final Batch Loss: 0.30462926626205444\n",
      "Epoch 2392, Loss: 1.183075800538063, Final Batch Loss: 0.30273938179016113\n",
      "Epoch 2393, Loss: 1.2326137125492096, Final Batch Loss: 0.47139862179756165\n",
      "Epoch 2394, Loss: 1.068803831934929, Final Batch Loss: 0.33643728494644165\n",
      "Epoch 2395, Loss: 0.9771945476531982, Final Batch Loss: 0.181161031126976\n",
      "Epoch 2396, Loss: 1.1322204023599625, Final Batch Loss: 0.23594041168689728\n",
      "Epoch 2397, Loss: 1.0470336377620697, Final Batch Loss: 0.2241750955581665\n",
      "Epoch 2398, Loss: 1.1072790324687958, Final Batch Loss: 0.2364143282175064\n",
      "Epoch 2399, Loss: 0.9820897579193115, Final Batch Loss: 0.26181504130363464\n",
      "Epoch 2400, Loss: 1.063409224152565, Final Batch Loss: 0.2245642989873886\n",
      "Epoch 2401, Loss: 1.0442590117454529, Final Batch Loss: 0.2647463083267212\n",
      "Epoch 2402, Loss: 1.058696672320366, Final Batch Loss: 0.30253079533576965\n",
      "Epoch 2403, Loss: 1.0539458692073822, Final Batch Loss: 0.1847328096628189\n",
      "Epoch 2404, Loss: 1.001624271273613, Final Batch Loss: 0.23124226927757263\n",
      "Epoch 2405, Loss: 0.9668962955474854, Final Batch Loss: 0.21064873039722443\n",
      "Epoch 2406, Loss: 1.0346883833408356, Final Batch Loss: 0.27056774497032166\n",
      "Epoch 2407, Loss: 1.0719786435365677, Final Batch Loss: 0.26326873898506165\n",
      "Epoch 2408, Loss: 0.9601549953222275, Final Batch Loss: 0.21694792807102203\n",
      "Epoch 2409, Loss: 0.8943097442388535, Final Batch Loss: 0.19113029539585114\n",
      "Epoch 2410, Loss: 0.9832773655653, Final Batch Loss: 0.23212240636348724\n",
      "Epoch 2411, Loss: 1.0016721487045288, Final Batch Loss: 0.2868000566959381\n",
      "Epoch 2412, Loss: 1.1728834062814713, Final Batch Loss: 0.4200747311115265\n",
      "Epoch 2413, Loss: 1.0239998996257782, Final Batch Loss: 0.2228143811225891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2414, Loss: 1.156296581029892, Final Batch Loss: 0.37432584166526794\n",
      "Epoch 2415, Loss: 1.0664803832769394, Final Batch Loss: 0.2837996780872345\n",
      "Epoch 2416, Loss: 1.17837955057621, Final Batch Loss: 0.34393179416656494\n",
      "Epoch 2417, Loss: 1.0686412751674652, Final Batch Loss: 0.2823784649372101\n",
      "Epoch 2418, Loss: 1.021787017583847, Final Batch Loss: 0.31066012382507324\n",
      "Epoch 2419, Loss: 1.0962984263896942, Final Batch Loss: 0.262249231338501\n",
      "Epoch 2420, Loss: 0.9342750310897827, Final Batch Loss: 0.2458827644586563\n",
      "Epoch 2421, Loss: 0.956953689455986, Final Batch Loss: 0.2395033836364746\n",
      "Epoch 2422, Loss: 1.0084394663572311, Final Batch Loss: 0.22182568907737732\n",
      "Epoch 2423, Loss: 0.9479959905147552, Final Batch Loss: 0.21979868412017822\n",
      "Epoch 2424, Loss: 1.0536170601844788, Final Batch Loss: 0.3506234288215637\n",
      "Epoch 2425, Loss: 0.9806940853595734, Final Batch Loss: 0.24929489195346832\n",
      "Epoch 2426, Loss: 0.9994959980249405, Final Batch Loss: 0.2652088701725006\n",
      "Epoch 2427, Loss: 1.014174446463585, Final Batch Loss: 0.3147560656070709\n",
      "Epoch 2428, Loss: 0.9677255898714066, Final Batch Loss: 0.24261318147182465\n",
      "Epoch 2429, Loss: 0.9913250803947449, Final Batch Loss: 0.26054316759109497\n",
      "Epoch 2430, Loss: 1.0918318778276443, Final Batch Loss: 0.22896267473697662\n",
      "Epoch 2431, Loss: 1.0028091073036194, Final Batch Loss: 0.23507535457611084\n",
      "Epoch 2432, Loss: 0.9900245070457458, Final Batch Loss: 0.27228453755378723\n",
      "Epoch 2433, Loss: 1.0234990566968918, Final Batch Loss: 0.23106199502944946\n",
      "Epoch 2434, Loss: 1.1279280632734299, Final Batch Loss: 0.30933329463005066\n",
      "Epoch 2435, Loss: 1.1290728747844696, Final Batch Loss: 0.28326913714408875\n",
      "Epoch 2436, Loss: 1.0115263164043427, Final Batch Loss: 0.15255579352378845\n",
      "Epoch 2437, Loss: 1.0381670445203781, Final Batch Loss: 0.1943332999944687\n",
      "Epoch 2438, Loss: 1.0088757127523422, Final Batch Loss: 0.2795064151287079\n",
      "Epoch 2439, Loss: 1.0413431525230408, Final Batch Loss: 0.2671005427837372\n",
      "Epoch 2440, Loss: 1.0391860604286194, Final Batch Loss: 0.27147722244262695\n",
      "Epoch 2441, Loss: 1.0363899022340775, Final Batch Loss: 0.32004672288894653\n",
      "Epoch 2442, Loss: 0.9238360822200775, Final Batch Loss: 0.1688215285539627\n",
      "Epoch 2443, Loss: 1.106904774904251, Final Batch Loss: 0.3178980052471161\n",
      "Epoch 2444, Loss: 1.0724590420722961, Final Batch Loss: 0.2805807888507843\n",
      "Epoch 2445, Loss: 1.0795983374118805, Final Batch Loss: 0.3360595107078552\n",
      "Epoch 2446, Loss: 1.0414324551820755, Final Batch Loss: 0.28633373975753784\n",
      "Epoch 2447, Loss: 0.9310596585273743, Final Batch Loss: 0.21022239327430725\n",
      "Epoch 2448, Loss: 1.0731124579906464, Final Batch Loss: 0.2620454430580139\n",
      "Epoch 2449, Loss: 1.1126497983932495, Final Batch Loss: 0.32558804750442505\n",
      "Epoch 2450, Loss: 0.9900421351194382, Final Batch Loss: 0.21743907034397125\n",
      "Epoch 2451, Loss: 1.033349797129631, Final Batch Loss: 0.27835556864738464\n",
      "Epoch 2452, Loss: 1.0106899440288544, Final Batch Loss: 0.27287328243255615\n",
      "Epoch 2453, Loss: 1.0533130019903183, Final Batch Loss: 0.2057233601808548\n",
      "Epoch 2454, Loss: 1.0362636893987656, Final Batch Loss: 0.24065624177455902\n",
      "Epoch 2455, Loss: 1.0337401181459427, Final Batch Loss: 0.20431940257549286\n",
      "Epoch 2456, Loss: 1.0191406309604645, Final Batch Loss: 0.2357061356306076\n",
      "Epoch 2457, Loss: 1.0849792659282684, Final Batch Loss: 0.26710426807403564\n",
      "Epoch 2458, Loss: 1.0622886717319489, Final Batch Loss: 0.21030740439891815\n",
      "Epoch 2459, Loss: 0.9999742805957794, Final Batch Loss: 0.24347446858882904\n",
      "Epoch 2460, Loss: 0.914642795920372, Final Batch Loss: 0.19469274580478668\n",
      "Epoch 2461, Loss: 0.9897365421056747, Final Batch Loss: 0.29212281107902527\n",
      "Epoch 2462, Loss: 0.9661094844341278, Final Batch Loss: 0.23234760761260986\n",
      "Epoch 2463, Loss: 1.0210382342338562, Final Batch Loss: 0.23838715255260468\n",
      "Epoch 2464, Loss: 0.918367326259613, Final Batch Loss: 0.15670140087604523\n",
      "Epoch 2465, Loss: 1.034471720457077, Final Batch Loss: 0.21028098464012146\n",
      "Epoch 2466, Loss: 0.9245125353336334, Final Batch Loss: 0.23034407198429108\n",
      "Epoch 2467, Loss: 0.9882896691560745, Final Batch Loss: 0.2520591616630554\n",
      "Epoch 2468, Loss: 1.172420620918274, Final Batch Loss: 0.3622092008590698\n",
      "Epoch 2469, Loss: 0.9402345716953278, Final Batch Loss: 0.2141074538230896\n",
      "Epoch 2470, Loss: 1.0413874238729477, Final Batch Loss: 0.2944278120994568\n",
      "Epoch 2471, Loss: 1.0734897702932358, Final Batch Loss: 0.169513538479805\n",
      "Epoch 2472, Loss: 0.97917540371418, Final Batch Loss: 0.2299659699201584\n",
      "Epoch 2473, Loss: 1.0625943839550018, Final Batch Loss: 0.2385248839855194\n",
      "Epoch 2474, Loss: 0.9749402403831482, Final Batch Loss: 0.32716530561447144\n",
      "Epoch 2475, Loss: 0.9439928531646729, Final Batch Loss: 0.2411876618862152\n",
      "Epoch 2476, Loss: 1.0349215120077133, Final Batch Loss: 0.317499577999115\n",
      "Epoch 2477, Loss: 0.9649361670017242, Final Batch Loss: 0.25432589650154114\n",
      "Epoch 2478, Loss: 0.914982259273529, Final Batch Loss: 0.19458453357219696\n",
      "Epoch 2479, Loss: 1.0030284374952316, Final Batch Loss: 0.34351298213005066\n",
      "Epoch 2480, Loss: 1.0185462832450867, Final Batch Loss: 0.2800351679325104\n",
      "Epoch 2481, Loss: 1.14353808760643, Final Batch Loss: 0.33627015352249146\n",
      "Epoch 2482, Loss: 0.9936191290616989, Final Batch Loss: 0.22596637904644012\n",
      "Epoch 2483, Loss: 1.0236059576272964, Final Batch Loss: 0.21386070549488068\n",
      "Epoch 2484, Loss: 1.0358715653419495, Final Batch Loss: 0.25151944160461426\n",
      "Epoch 2485, Loss: 0.9489271342754364, Final Batch Loss: 0.1544632464647293\n",
      "Epoch 2486, Loss: 0.9770997017621994, Final Batch Loss: 0.3040701448917389\n",
      "Epoch 2487, Loss: 1.0291684120893478, Final Batch Loss: 0.2667895555496216\n",
      "Epoch 2488, Loss: 1.0413683503866196, Final Batch Loss: 0.186456099152565\n",
      "Epoch 2489, Loss: 1.0699395835399628, Final Batch Loss: 0.2747424840927124\n",
      "Epoch 2490, Loss: 0.9771412163972855, Final Batch Loss: 0.21092388033866882\n",
      "Epoch 2491, Loss: 1.0610441267490387, Final Batch Loss: 0.2660239338874817\n",
      "Epoch 2492, Loss: 0.9568241089582443, Final Batch Loss: 0.19061928987503052\n",
      "Epoch 2493, Loss: 1.1112020164728165, Final Batch Loss: 0.32740747928619385\n",
      "Epoch 2494, Loss: 0.9686843007802963, Final Batch Loss: 0.25640761852264404\n",
      "Epoch 2495, Loss: 1.0245315879583359, Final Batch Loss: 0.3337888717651367\n",
      "Epoch 2496, Loss: 0.9555449485778809, Final Batch Loss: 0.1915709227323532\n",
      "Epoch 2497, Loss: 1.0770309269428253, Final Batch Loss: 0.3536333441734314\n",
      "Epoch 2498, Loss: 0.9126599729061127, Final Batch Loss: 0.21938282251358032\n",
      "Epoch 2499, Loss: 0.9692331999540329, Final Batch Loss: 0.1863514631986618\n",
      "Epoch 2500, Loss: 1.0332556664943695, Final Batch Loss: 0.2788713574409485\n",
      "Epoch 2501, Loss: 0.9343130886554718, Final Batch Loss: 0.1565948724746704\n",
      "Epoch 2502, Loss: 0.954525426030159, Final Batch Loss: 0.19667895138263702\n",
      "Epoch 2503, Loss: 1.0100092887878418, Final Batch Loss: 0.26851046085357666\n",
      "Epoch 2504, Loss: 0.998100534081459, Final Batch Loss: 0.33120888471603394\n",
      "Epoch 2505, Loss: 0.9507477581501007, Final Batch Loss: 0.2504042387008667\n",
      "Epoch 2506, Loss: 1.0088167935609818, Final Batch Loss: 0.2381768673658371\n",
      "Epoch 2507, Loss: 1.0271482914686203, Final Batch Loss: 0.26736685633659363\n",
      "Epoch 2508, Loss: 1.0874720960855484, Final Batch Loss: 0.29396581649780273\n",
      "Epoch 2509, Loss: 1.0815551429986954, Final Batch Loss: 0.24950377643108368\n",
      "Epoch 2510, Loss: 0.9231871068477631, Final Batch Loss: 0.2471582591533661\n",
      "Epoch 2511, Loss: 1.0982247591018677, Final Batch Loss: 0.2806718945503235\n",
      "Epoch 2512, Loss: 1.1265182048082352, Final Batch Loss: 0.35496577620506287\n",
      "Epoch 2513, Loss: 0.9511354118585587, Final Batch Loss: 0.23249034583568573\n",
      "Epoch 2514, Loss: 0.9339404255151749, Final Batch Loss: 0.1839161068201065\n",
      "Epoch 2515, Loss: 0.9941801428794861, Final Batch Loss: 0.2969572842121124\n",
      "Epoch 2516, Loss: 0.8814364522695541, Final Batch Loss: 0.18454134464263916\n",
      "Epoch 2517, Loss: 0.9507206678390503, Final Batch Loss: 0.23465676605701447\n",
      "Epoch 2518, Loss: 0.9279590249061584, Final Batch Loss: 0.2059834748506546\n",
      "Epoch 2519, Loss: 1.0190586894750595, Final Batch Loss: 0.22604824602603912\n",
      "Epoch 2520, Loss: 1.126428335905075, Final Batch Loss: 0.3202722668647766\n",
      "Epoch 2521, Loss: 1.1312056183815002, Final Batch Loss: 0.37035250663757324\n",
      "Epoch 2522, Loss: 0.9192475378513336, Final Batch Loss: 0.25803399085998535\n",
      "Epoch 2523, Loss: 0.9620572030544281, Final Batch Loss: 0.22491934895515442\n",
      "Epoch 2524, Loss: 0.9396121501922607, Final Batch Loss: 0.23677676916122437\n",
      "Epoch 2525, Loss: 0.9122993350028992, Final Batch Loss: 0.21930643916130066\n",
      "Epoch 2526, Loss: 1.0081892311573029, Final Batch Loss: 0.25004732608795166\n",
      "Epoch 2527, Loss: 0.9538708627223969, Final Batch Loss: 0.18916741013526917\n",
      "Epoch 2528, Loss: 1.054872676730156, Final Batch Loss: 0.24353943765163422\n",
      "Epoch 2529, Loss: 0.9819031357765198, Final Batch Loss: 0.22694575786590576\n",
      "Epoch 2530, Loss: 0.9370664358139038, Final Batch Loss: 0.15215830504894257\n",
      "Epoch 2531, Loss: 0.9770342260599136, Final Batch Loss: 0.27190014719963074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2532, Loss: 1.0031526535749435, Final Batch Loss: 0.1851927936077118\n",
      "Epoch 2533, Loss: 1.0283146649599075, Final Batch Loss: 0.27462175488471985\n",
      "Epoch 2534, Loss: 1.0098763406276703, Final Batch Loss: 0.26635095477104187\n",
      "Epoch 2535, Loss: 0.9941925704479218, Final Batch Loss: 0.24454206228256226\n",
      "Epoch 2536, Loss: 1.0714965909719467, Final Batch Loss: 0.2556731700897217\n",
      "Epoch 2537, Loss: 0.8838133066892624, Final Batch Loss: 0.20082035660743713\n",
      "Epoch 2538, Loss: 1.0350473821163177, Final Batch Loss: 0.24604225158691406\n",
      "Epoch 2539, Loss: 1.0228653401136398, Final Batch Loss: 0.34235629439353943\n",
      "Epoch 2540, Loss: 0.9825363308191299, Final Batch Loss: 0.2284199297428131\n",
      "Epoch 2541, Loss: 0.905736654996872, Final Batch Loss: 0.14771920442581177\n",
      "Epoch 2542, Loss: 0.9387829750776291, Final Batch Loss: 0.21825754642486572\n",
      "Epoch 2543, Loss: 0.9207461029291153, Final Batch Loss: 0.17371848225593567\n",
      "Epoch 2544, Loss: 1.1113253235816956, Final Batch Loss: 0.33201003074645996\n",
      "Epoch 2545, Loss: 1.01029571890831, Final Batch Loss: 0.15972551703453064\n",
      "Epoch 2546, Loss: 1.02505761384964, Final Batch Loss: 0.21942093968391418\n",
      "Epoch 2547, Loss: 0.9767019748687744, Final Batch Loss: 0.2748224437236786\n",
      "Epoch 2548, Loss: 1.073665052652359, Final Batch Loss: 0.2934440076351166\n",
      "Epoch 2549, Loss: 1.0131332725286484, Final Batch Loss: 0.2629700303077698\n",
      "Epoch 2550, Loss: 0.9289166629314423, Final Batch Loss: 0.2229880392551422\n",
      "Epoch 2551, Loss: 1.1428914219141006, Final Batch Loss: 0.3183381259441376\n",
      "Epoch 2552, Loss: 0.947987362742424, Final Batch Loss: 0.15332041680812836\n",
      "Epoch 2553, Loss: 1.084140881896019, Final Batch Loss: 0.3250857889652252\n",
      "Epoch 2554, Loss: 1.0009121596813202, Final Batch Loss: 0.2948072552680969\n",
      "Epoch 2555, Loss: 0.9397137761116028, Final Batch Loss: 0.21959662437438965\n",
      "Epoch 2556, Loss: 0.9310572594404221, Final Batch Loss: 0.2537650763988495\n",
      "Epoch 2557, Loss: 0.9707309454679489, Final Batch Loss: 0.27139338850975037\n",
      "Epoch 2558, Loss: 0.9155949652194977, Final Batch Loss: 0.23837290704250336\n",
      "Epoch 2559, Loss: 0.9326017796993256, Final Batch Loss: 0.30887553095817566\n",
      "Epoch 2560, Loss: 0.9140234291553497, Final Batch Loss: 0.2329997569322586\n",
      "Epoch 2561, Loss: 1.153418704867363, Final Batch Loss: 0.32378873229026794\n",
      "Epoch 2562, Loss: 1.1127545833587646, Final Batch Loss: 0.3366050720214844\n",
      "Epoch 2563, Loss: 1.0223050266504288, Final Batch Loss: 0.3071976900100708\n",
      "Epoch 2564, Loss: 1.0432735830545425, Final Batch Loss: 0.27648597955703735\n",
      "Epoch 2565, Loss: 0.9774601012468338, Final Batch Loss: 0.3157602846622467\n",
      "Epoch 2566, Loss: 0.9477321207523346, Final Batch Loss: 0.19936363399028778\n",
      "Epoch 2567, Loss: 0.9345387816429138, Final Batch Loss: 0.23237015306949615\n",
      "Epoch 2568, Loss: 1.0840712040662766, Final Batch Loss: 0.2682991921901703\n",
      "Epoch 2569, Loss: 0.9216108322143555, Final Batch Loss: 0.19591498374938965\n",
      "Epoch 2570, Loss: 1.1541408151388168, Final Batch Loss: 0.34265533089637756\n",
      "Epoch 2571, Loss: 0.9671019613742828, Final Batch Loss: 0.214588463306427\n",
      "Epoch 2572, Loss: 0.949771910905838, Final Batch Loss: 0.266135036945343\n",
      "Epoch 2573, Loss: 0.9852424263954163, Final Batch Loss: 0.24809032678604126\n",
      "Epoch 2574, Loss: 1.0240953713655472, Final Batch Loss: 0.2777419090270996\n",
      "Epoch 2575, Loss: 0.9528772383928299, Final Batch Loss: 0.23381125926971436\n",
      "Epoch 2576, Loss: 0.997059166431427, Final Batch Loss: 0.20527535676956177\n",
      "Epoch 2577, Loss: 0.9885946661233902, Final Batch Loss: 0.23735906183719635\n",
      "Epoch 2578, Loss: 0.9786204844713211, Final Batch Loss: 0.2514748275279999\n",
      "Epoch 2579, Loss: 0.8734776526689529, Final Batch Loss: 0.22606530785560608\n",
      "Epoch 2580, Loss: 1.0133277624845505, Final Batch Loss: 0.26882562041282654\n",
      "Epoch 2581, Loss: 0.9929987341165543, Final Batch Loss: 0.26586419343948364\n",
      "Epoch 2582, Loss: 1.058957502245903, Final Batch Loss: 0.3496776819229126\n",
      "Epoch 2583, Loss: 1.062203660607338, Final Batch Loss: 0.30513447523117065\n",
      "Epoch 2584, Loss: 1.0801470577716827, Final Batch Loss: 0.2831011116504669\n",
      "Epoch 2585, Loss: 1.0553174316883087, Final Batch Loss: 0.215256005525589\n",
      "Epoch 2586, Loss: 1.0196061730384827, Final Batch Loss: 0.26404738426208496\n",
      "Epoch 2587, Loss: 0.9595839828252792, Final Batch Loss: 0.20436914265155792\n",
      "Epoch 2588, Loss: 1.3682377487421036, Final Batch Loss: 0.5939421653747559\n",
      "Epoch 2589, Loss: 0.8157843947410583, Final Batch Loss: 0.10750710964202881\n",
      "Epoch 2590, Loss: 1.0287374258041382, Final Batch Loss: 0.2610231041908264\n",
      "Epoch 2591, Loss: 1.024569034576416, Final Batch Loss: 0.3057232201099396\n",
      "Epoch 2592, Loss: 0.9234059005975723, Final Batch Loss: 0.23887264728546143\n",
      "Epoch 2593, Loss: 1.1243526637554169, Final Batch Loss: 0.4199863374233246\n",
      "Epoch 2594, Loss: 1.0188018381595612, Final Batch Loss: 0.18652290105819702\n",
      "Epoch 2595, Loss: 1.046031653881073, Final Batch Loss: 0.3374404311180115\n",
      "Epoch 2596, Loss: 0.9122658371925354, Final Batch Loss: 0.17237062752246857\n",
      "Epoch 2597, Loss: 1.0126090347766876, Final Batch Loss: 0.21873115003108978\n",
      "Epoch 2598, Loss: 1.0469385981559753, Final Batch Loss: 0.2911267578601837\n",
      "Epoch 2599, Loss: 1.0104504078626633, Final Batch Loss: 0.31798452138900757\n",
      "Epoch 2600, Loss: 0.9842635095119476, Final Batch Loss: 0.26785537600517273\n",
      "Epoch 2601, Loss: 1.0029675364494324, Final Batch Loss: 0.23068131506443024\n",
      "Epoch 2602, Loss: 1.1145630776882172, Final Batch Loss: 0.30301329493522644\n",
      "Epoch 2603, Loss: 1.0320463627576828, Final Batch Loss: 0.21996968984603882\n",
      "Epoch 2604, Loss: 0.9770092517137527, Final Batch Loss: 0.2729837894439697\n",
      "Epoch 2605, Loss: 1.0089082568883896, Final Batch Loss: 0.22835758328437805\n",
      "Epoch 2606, Loss: 0.8820119202136993, Final Batch Loss: 0.204448863863945\n",
      "Epoch 2607, Loss: 0.9832668453454971, Final Batch Loss: 0.35358908772468567\n",
      "Epoch 2608, Loss: 0.8930075019598007, Final Batch Loss: 0.1714828908443451\n",
      "Epoch 2609, Loss: 1.096122369170189, Final Batch Loss: 0.3358965218067169\n",
      "Epoch 2610, Loss: 1.0494955331087112, Final Batch Loss: 0.24780555069446564\n",
      "Epoch 2611, Loss: 0.9861437082290649, Final Batch Loss: 0.20296545326709747\n",
      "Epoch 2612, Loss: 0.9449852257966995, Final Batch Loss: 0.2127964049577713\n",
      "Epoch 2613, Loss: 1.0605835169553757, Final Batch Loss: 0.33534544706344604\n",
      "Epoch 2614, Loss: 1.1219051629304886, Final Batch Loss: 0.14833013713359833\n",
      "Epoch 2615, Loss: 1.0166793316602707, Final Batch Loss: 0.19294072687625885\n",
      "Epoch 2616, Loss: 0.9964984059333801, Final Batch Loss: 0.20553305745124817\n",
      "Epoch 2617, Loss: 0.9751518815755844, Final Batch Loss: 0.19420282542705536\n",
      "Epoch 2618, Loss: 0.9736318439245224, Final Batch Loss: 0.23228973150253296\n",
      "Epoch 2619, Loss: 0.9524655342102051, Final Batch Loss: 0.2333187758922577\n",
      "Epoch 2620, Loss: 0.9606228172779083, Final Batch Loss: 0.2118549644947052\n",
      "Epoch 2621, Loss: 1.0931379050016403, Final Batch Loss: 0.3137396275997162\n",
      "Epoch 2622, Loss: 0.9903739243745804, Final Batch Loss: 0.20823925733566284\n",
      "Epoch 2623, Loss: 1.0434803366661072, Final Batch Loss: 0.4056176245212555\n",
      "Epoch 2624, Loss: 0.8920245319604874, Final Batch Loss: 0.21220363676548004\n",
      "Epoch 2625, Loss: 0.993645578622818, Final Batch Loss: 0.18766705691814423\n",
      "Epoch 2626, Loss: 1.0465613454580307, Final Batch Loss: 0.3012084662914276\n",
      "Epoch 2627, Loss: 1.1382821798324585, Final Batch Loss: 0.30844148993492126\n",
      "Epoch 2628, Loss: 0.9304332882165909, Final Batch Loss: 0.20077215135097504\n",
      "Epoch 2629, Loss: 1.0074659138917923, Final Batch Loss: 0.2636794149875641\n",
      "Epoch 2630, Loss: 1.0012058466672897, Final Batch Loss: 0.28238123655319214\n",
      "Epoch 2631, Loss: 1.075881153345108, Final Batch Loss: 0.33905652165412903\n",
      "Epoch 2632, Loss: 1.075077623128891, Final Batch Loss: 0.2479308545589447\n",
      "Epoch 2633, Loss: 1.064343273639679, Final Batch Loss: 0.25211724638938904\n",
      "Epoch 2634, Loss: 0.8987559080123901, Final Batch Loss: 0.18416205048561096\n",
      "Epoch 2635, Loss: 0.9927328526973724, Final Batch Loss: 0.21371203660964966\n",
      "Epoch 2636, Loss: 0.972010925412178, Final Batch Loss: 0.23023253679275513\n",
      "Epoch 2637, Loss: 0.9204089790582657, Final Batch Loss: 0.22213244438171387\n",
      "Epoch 2638, Loss: 0.8924425542354584, Final Batch Loss: 0.1891385018825531\n",
      "Epoch 2639, Loss: 0.9837898015975952, Final Batch Loss: 0.26001212000846863\n",
      "Epoch 2640, Loss: 0.8942671120166779, Final Batch Loss: 0.23212893307209015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2641, Loss: 1.0205335468053818, Final Batch Loss: 0.2564835548400879\n",
      "Epoch 2642, Loss: 0.8474623262882233, Final Batch Loss: 0.1675814837217331\n",
      "Epoch 2643, Loss: 0.8338806480169296, Final Batch Loss: 0.2061227411031723\n",
      "Epoch 2644, Loss: 1.0179252475500107, Final Batch Loss: 0.28315839171409607\n",
      "Epoch 2645, Loss: 1.0369476228952408, Final Batch Loss: 0.3036796450614929\n",
      "Epoch 2646, Loss: 1.010381504893303, Final Batch Loss: 0.30329960584640503\n",
      "Epoch 2647, Loss: 0.8482538014650345, Final Batch Loss: 0.15477143228054047\n",
      "Epoch 2648, Loss: 1.0181952714920044, Final Batch Loss: 0.27454930543899536\n",
      "Epoch 2649, Loss: 1.0851696729660034, Final Batch Loss: 0.30978432297706604\n",
      "Epoch 2650, Loss: 0.9032420963048935, Final Batch Loss: 0.17990007996559143\n",
      "Epoch 2651, Loss: 0.9335588067770004, Final Batch Loss: 0.1921030730009079\n",
      "Epoch 2652, Loss: 0.9385126531124115, Final Batch Loss: 0.23217949271202087\n",
      "Epoch 2653, Loss: 1.00566066801548, Final Batch Loss: 0.1999441236257553\n",
      "Epoch 2654, Loss: 1.0801704823970795, Final Batch Loss: 0.273953914642334\n",
      "Epoch 2655, Loss: 0.977207601070404, Final Batch Loss: 0.362764447927475\n",
      "Epoch 2656, Loss: 1.122577041387558, Final Batch Loss: 0.35943323373794556\n",
      "Epoch 2657, Loss: 0.9060264527797699, Final Batch Loss: 0.2363392412662506\n",
      "Epoch 2658, Loss: 0.985490545630455, Final Batch Loss: 0.3013323247432709\n",
      "Epoch 2659, Loss: 0.9340951591730118, Final Batch Loss: 0.20991547405719757\n",
      "Epoch 2660, Loss: 0.902524009346962, Final Batch Loss: 0.19391323626041412\n",
      "Epoch 2661, Loss: 1.0523981600999832, Final Batch Loss: 0.2928004860877991\n",
      "Epoch 2662, Loss: 0.9856163710355759, Final Batch Loss: 0.2363678365945816\n",
      "Epoch 2663, Loss: 1.059939131140709, Final Batch Loss: 0.3100205063819885\n",
      "Epoch 2664, Loss: 1.0039895921945572, Final Batch Loss: 0.2894711494445801\n",
      "Epoch 2665, Loss: 1.0364989638328552, Final Batch Loss: 0.2601739764213562\n",
      "Epoch 2666, Loss: 0.9445793926715851, Final Batch Loss: 0.2657841145992279\n",
      "Epoch 2667, Loss: 0.9377218186855316, Final Batch Loss: 0.26391926407814026\n",
      "Epoch 2668, Loss: 0.9105775505304337, Final Batch Loss: 0.24073031544685364\n",
      "Epoch 2669, Loss: 0.9725501239299774, Final Batch Loss: 0.2608274519443512\n",
      "Epoch 2670, Loss: 0.9964154064655304, Final Batch Loss: 0.2192596197128296\n",
      "Epoch 2671, Loss: 1.0443024188280106, Final Batch Loss: 0.24546781182289124\n",
      "Epoch 2672, Loss: 0.9884187430143356, Final Batch Loss: 0.2709149718284607\n",
      "Epoch 2673, Loss: 0.9129879176616669, Final Batch Loss: 0.23609080910682678\n",
      "Epoch 2674, Loss: 0.9119361340999603, Final Batch Loss: 0.2163611650466919\n",
      "Epoch 2675, Loss: 0.8977950662374496, Final Batch Loss: 0.24322862923145294\n",
      "Epoch 2676, Loss: 0.9173982292413712, Final Batch Loss: 0.20465430617332458\n",
      "Epoch 2677, Loss: 0.9399325549602509, Final Batch Loss: 0.2433064579963684\n",
      "Epoch 2678, Loss: 0.9928201138973236, Final Batch Loss: 0.2802949845790863\n",
      "Epoch 2679, Loss: 0.938245490193367, Final Batch Loss: 0.23752552270889282\n",
      "Epoch 2680, Loss: 1.0628255903720856, Final Batch Loss: 0.28837764263153076\n",
      "Epoch 2681, Loss: 1.0645090192556381, Final Batch Loss: 0.3890252709388733\n",
      "Epoch 2682, Loss: 1.0454392433166504, Final Batch Loss: 0.21350586414337158\n",
      "Epoch 2683, Loss: 0.9855885654687881, Final Batch Loss: 0.24396714568138123\n",
      "Epoch 2684, Loss: 0.9257315695285797, Final Batch Loss: 0.21099932491779327\n",
      "Epoch 2685, Loss: 0.8607613295316696, Final Batch Loss: 0.1787160038948059\n",
      "Epoch 2686, Loss: 1.0480935275554657, Final Batch Loss: 0.26749560236930847\n",
      "Epoch 2687, Loss: 0.9595770090818405, Final Batch Loss: 0.18684189021587372\n",
      "Epoch 2688, Loss: 0.9360605031251907, Final Batch Loss: 0.2558484375476837\n",
      "Epoch 2689, Loss: 1.005955994129181, Final Batch Loss: 0.23283974826335907\n",
      "Epoch 2690, Loss: 0.9934239387512207, Final Batch Loss: 0.2895226776599884\n",
      "Epoch 2691, Loss: 0.9080559015274048, Final Batch Loss: 0.22930602729320526\n",
      "Epoch 2692, Loss: 0.9463831931352615, Final Batch Loss: 0.18906603753566742\n",
      "Epoch 2693, Loss: 0.8750732839107513, Final Batch Loss: 0.2116854041814804\n",
      "Epoch 2694, Loss: 1.0197579711675644, Final Batch Loss: 0.21128429472446442\n",
      "Epoch 2695, Loss: 0.9724436551332474, Final Batch Loss: 0.2607637047767639\n",
      "Epoch 2696, Loss: 0.9198455065488815, Final Batch Loss: 0.24988502264022827\n",
      "Epoch 2697, Loss: 0.9984455108642578, Final Batch Loss: 0.33726736903190613\n",
      "Epoch 2698, Loss: 0.9371377974748611, Final Batch Loss: 0.24136313796043396\n",
      "Epoch 2699, Loss: 0.9123589396476746, Final Batch Loss: 0.2622664272785187\n",
      "Epoch 2700, Loss: 1.0479747503995895, Final Batch Loss: 0.2756118178367615\n",
      "Epoch 2701, Loss: 0.9043787717819214, Final Batch Loss: 0.19452664256095886\n",
      "Epoch 2702, Loss: 1.0236826688051224, Final Batch Loss: 0.21576814353466034\n",
      "Epoch 2703, Loss: 0.9528607577085495, Final Batch Loss: 0.21463531255722046\n",
      "Epoch 2704, Loss: 1.0150244235992432, Final Batch Loss: 0.28195324540138245\n",
      "Epoch 2705, Loss: 0.9454219788312912, Final Batch Loss: 0.18908193707466125\n",
      "Epoch 2706, Loss: 0.919426754117012, Final Batch Loss: 0.2807442247867584\n",
      "Epoch 2707, Loss: 0.9734691828489304, Final Batch Loss: 0.31236565113067627\n",
      "Epoch 2708, Loss: 0.9711867719888687, Final Batch Loss: 0.23606064915657043\n",
      "Epoch 2709, Loss: 0.9386744797229767, Final Batch Loss: 0.25915342569351196\n",
      "Epoch 2710, Loss: 1.0139418989419937, Final Batch Loss: 0.24093495309352875\n",
      "Epoch 2711, Loss: 0.9661240726709366, Final Batch Loss: 0.22310909628868103\n",
      "Epoch 2712, Loss: 0.9011799544095993, Final Batch Loss: 0.21914230287075043\n",
      "Epoch 2713, Loss: 1.0225419104099274, Final Batch Loss: 0.29863762855529785\n",
      "Epoch 2714, Loss: 0.9567577540874481, Final Batch Loss: 0.1931949257850647\n",
      "Epoch 2715, Loss: 0.9387180507183075, Final Batch Loss: 0.205862894654274\n",
      "Epoch 2716, Loss: 0.9429893046617508, Final Batch Loss: 0.2184719443321228\n",
      "Epoch 2717, Loss: 0.8843135088682175, Final Batch Loss: 0.1986895650625229\n",
      "Epoch 2718, Loss: 0.9832571744918823, Final Batch Loss: 0.2932690382003784\n",
      "Epoch 2719, Loss: 0.8856201097369194, Final Batch Loss: 0.11852019280195236\n",
      "Epoch 2720, Loss: 0.9485479295253754, Final Batch Loss: 0.21341857314109802\n",
      "Epoch 2721, Loss: 1.0037805885076523, Final Batch Loss: 0.2110116183757782\n",
      "Epoch 2722, Loss: 0.9141047596931458, Final Batch Loss: 0.1341690719127655\n",
      "Epoch 2723, Loss: 0.949767529964447, Final Batch Loss: 0.27023550868034363\n",
      "Epoch 2724, Loss: 0.983312264084816, Final Batch Loss: 0.24778686463832855\n",
      "Epoch 2725, Loss: 1.1463302671909332, Final Batch Loss: 0.362698495388031\n",
      "Epoch 2726, Loss: 0.9687202870845795, Final Batch Loss: 0.1607048362493515\n",
      "Epoch 2727, Loss: 1.0691886693239212, Final Batch Loss: 0.1773584932088852\n",
      "Epoch 2728, Loss: 1.002616047859192, Final Batch Loss: 0.2702656388282776\n",
      "Epoch 2729, Loss: 1.0446978062391281, Final Batch Loss: 0.3502224385738373\n",
      "Epoch 2730, Loss: 0.970979630947113, Final Batch Loss: 0.2363034188747406\n",
      "Epoch 2731, Loss: 1.026167780160904, Final Batch Loss: 0.2924302816390991\n",
      "Epoch 2732, Loss: 0.9574805349111557, Final Batch Loss: 0.21881051361560822\n",
      "Epoch 2733, Loss: 0.9745312184095383, Final Batch Loss: 0.27457964420318604\n",
      "Epoch 2734, Loss: 1.0002886652946472, Final Batch Loss: 0.298351913690567\n",
      "Epoch 2735, Loss: 0.912290558218956, Final Batch Loss: 0.2114034742116928\n",
      "Epoch 2736, Loss: 1.0115898698568344, Final Batch Loss: 0.34408602118492126\n",
      "Epoch 2737, Loss: 0.9824153929948807, Final Batch Loss: 0.2357015162706375\n",
      "Epoch 2738, Loss: 1.0139405876398087, Final Batch Loss: 0.3018106520175934\n",
      "Epoch 2739, Loss: 0.9546218514442444, Final Batch Loss: 0.2432374358177185\n",
      "Epoch 2740, Loss: 0.8799107074737549, Final Batch Loss: 0.2733258008956909\n",
      "Epoch 2741, Loss: 0.886240541934967, Final Batch Loss: 0.2159816324710846\n",
      "Epoch 2742, Loss: 1.0361327230930328, Final Batch Loss: 0.24692557752132416\n",
      "Epoch 2743, Loss: 0.9906690120697021, Final Batch Loss: 0.23124751448631287\n",
      "Epoch 2744, Loss: 1.0065511763095856, Final Batch Loss: 0.21877576410770416\n",
      "Epoch 2745, Loss: 0.9319294393062592, Final Batch Loss: 0.2111571729183197\n",
      "Epoch 2746, Loss: 1.0127605348825455, Final Batch Loss: 0.2563365697860718\n",
      "Epoch 2747, Loss: 0.9483609795570374, Final Batch Loss: 0.2514314651489258\n",
      "Epoch 2748, Loss: 0.9500273317098618, Final Batch Loss: 0.27240949869155884\n",
      "Epoch 2749, Loss: 0.9670335799455643, Final Batch Loss: 0.31406134366989136\n",
      "Epoch 2750, Loss: 0.9582894593477249, Final Batch Loss: 0.2620682120323181\n",
      "Epoch 2751, Loss: 1.0568802207708359, Final Batch Loss: 0.24152632057666779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2752, Loss: 0.961842492222786, Final Batch Loss: 0.16703610122203827\n",
      "Epoch 2753, Loss: 1.000541478395462, Final Batch Loss: 0.26771292090415955\n",
      "Epoch 2754, Loss: 0.9270349442958832, Final Batch Loss: 0.19766438007354736\n",
      "Epoch 2755, Loss: 0.9048165529966354, Final Batch Loss: 0.16552163660526276\n",
      "Epoch 2756, Loss: 0.9195852428674698, Final Batch Loss: 0.2428138107061386\n",
      "Epoch 2757, Loss: 1.0203864127397537, Final Batch Loss: 0.23665013909339905\n",
      "Epoch 2758, Loss: 0.9676743894815445, Final Batch Loss: 0.190299391746521\n",
      "Epoch 2759, Loss: 0.8400699570775032, Final Batch Loss: 0.12251298874616623\n",
      "Epoch 2760, Loss: 0.9682050943374634, Final Batch Loss: 0.27545586228370667\n",
      "Epoch 2761, Loss: 1.0314185619354248, Final Batch Loss: 0.2936912178993225\n",
      "Epoch 2762, Loss: 1.0172606706619263, Final Batch Loss: 0.26151975989341736\n",
      "Epoch 2763, Loss: 0.9084202796220779, Final Batch Loss: 0.203753262758255\n",
      "Epoch 2764, Loss: 1.0036988705396652, Final Batch Loss: 0.27402934432029724\n",
      "Epoch 2765, Loss: 0.9192289561033249, Final Batch Loss: 0.27628445625305176\n",
      "Epoch 2766, Loss: 0.9738445430994034, Final Batch Loss: 0.19610466063022614\n",
      "Epoch 2767, Loss: 0.9135820865631104, Final Batch Loss: 0.1878480166196823\n",
      "Epoch 2768, Loss: 1.1040391623973846, Final Batch Loss: 0.2816658616065979\n",
      "Epoch 2769, Loss: 0.937811866402626, Final Batch Loss: 0.17405548691749573\n",
      "Epoch 2770, Loss: 0.8482683449983597, Final Batch Loss: 0.1629299819469452\n",
      "Epoch 2771, Loss: 1.0204042196273804, Final Batch Loss: 0.29653266072273254\n",
      "Epoch 2772, Loss: 0.9524745643138885, Final Batch Loss: 0.24061743915081024\n",
      "Epoch 2773, Loss: 0.8715601414442062, Final Batch Loss: 0.27404484152793884\n",
      "Epoch 2774, Loss: 1.0524444282054901, Final Batch Loss: 0.35533490777015686\n",
      "Epoch 2775, Loss: 0.9064143151044846, Final Batch Loss: 0.23949895799160004\n",
      "Epoch 2776, Loss: 0.9492204040288925, Final Batch Loss: 0.1872522383928299\n",
      "Epoch 2777, Loss: 1.066015362739563, Final Batch Loss: 0.3176631033420563\n",
      "Epoch 2778, Loss: 0.8939691036939621, Final Batch Loss: 0.2030339539051056\n",
      "Epoch 2779, Loss: 0.8989925682544708, Final Batch Loss: 0.17421981692314148\n",
      "Epoch 2780, Loss: 0.858349084854126, Final Batch Loss: 0.20028157532215118\n",
      "Epoch 2781, Loss: 0.9592168480157852, Final Batch Loss: 0.24629426002502441\n",
      "Epoch 2782, Loss: 0.9137856066226959, Final Batch Loss: 0.21526023745536804\n",
      "Epoch 2783, Loss: 0.8571147322654724, Final Batch Loss: 0.17236538231372833\n",
      "Epoch 2784, Loss: 0.8905031830072403, Final Batch Loss: 0.26069867610931396\n",
      "Epoch 2785, Loss: 0.8831616938114166, Final Batch Loss: 0.22180864214897156\n",
      "Epoch 2786, Loss: 0.9240129292011261, Final Batch Loss: 0.24716486036777496\n",
      "Epoch 2787, Loss: 0.9936019480228424, Final Batch Loss: 0.23259086906909943\n",
      "Epoch 2788, Loss: 0.929476797580719, Final Batch Loss: 0.2589728832244873\n",
      "Epoch 2789, Loss: 1.0051335096359253, Final Batch Loss: 0.2191104292869568\n",
      "Epoch 2790, Loss: 1.0103673338890076, Final Batch Loss: 0.33510345220565796\n",
      "Epoch 2791, Loss: 0.9396537095308304, Final Batch Loss: 0.24798856675624847\n",
      "Epoch 2792, Loss: 0.9972104728221893, Final Batch Loss: 0.3560067117214203\n",
      "Epoch 2793, Loss: 1.0031960904598236, Final Batch Loss: 0.25789839029312134\n",
      "Epoch 2794, Loss: 0.8539995402097702, Final Batch Loss: 0.18543513119220734\n",
      "Epoch 2795, Loss: 1.060114786028862, Final Batch Loss: 0.2777719497680664\n",
      "Epoch 2796, Loss: 0.887901708483696, Final Batch Loss: 0.13207153975963593\n",
      "Epoch 2797, Loss: 0.979600116610527, Final Batch Loss: 0.24492594599723816\n",
      "Epoch 2798, Loss: 0.9483287185430527, Final Batch Loss: 0.23366498947143555\n",
      "Epoch 2799, Loss: 1.0128585547208786, Final Batch Loss: 0.31170976161956787\n",
      "Epoch 2800, Loss: 1.040061578154564, Final Batch Loss: 0.21234558522701263\n",
      "Epoch 2801, Loss: 0.9143827855587006, Final Batch Loss: 0.27591392397880554\n",
      "Epoch 2802, Loss: 0.9918330758810043, Final Batch Loss: 0.2519424259662628\n",
      "Epoch 2803, Loss: 1.0022859424352646, Final Batch Loss: 0.36257073283195496\n",
      "Epoch 2804, Loss: 0.9216368943452835, Final Batch Loss: 0.14034907519817352\n",
      "Epoch 2805, Loss: 0.920386865735054, Final Batch Loss: 0.20905478298664093\n",
      "Epoch 2806, Loss: 0.9817247092723846, Final Batch Loss: 0.3028419315814972\n",
      "Epoch 2807, Loss: 0.9163174331188202, Final Batch Loss: 0.14930960536003113\n",
      "Epoch 2808, Loss: 0.875478208065033, Final Batch Loss: 0.20379556715488434\n",
      "Epoch 2809, Loss: 0.9567137509584427, Final Batch Loss: 0.2289685308933258\n",
      "Epoch 2810, Loss: 1.0208161175251007, Final Batch Loss: 0.2208775281906128\n",
      "Epoch 2811, Loss: 0.9660377204418182, Final Batch Loss: 0.19030241668224335\n",
      "Epoch 2812, Loss: 0.9681348204612732, Final Batch Loss: 0.29758700728416443\n",
      "Epoch 2813, Loss: 0.9966263622045517, Final Batch Loss: 0.2427392452955246\n",
      "Epoch 2814, Loss: 0.9534681886434555, Final Batch Loss: 0.18393735587596893\n",
      "Epoch 2815, Loss: 1.0662381500005722, Final Batch Loss: 0.305876225233078\n",
      "Epoch 2816, Loss: 0.9837451130151749, Final Batch Loss: 0.2367260903120041\n",
      "Epoch 2817, Loss: 0.9910116344690323, Final Batch Loss: 0.3501588702201843\n",
      "Epoch 2818, Loss: 0.9427372515201569, Final Batch Loss: 0.19854195415973663\n",
      "Epoch 2819, Loss: 0.8441218733787537, Final Batch Loss: 0.192172110080719\n",
      "Epoch 2820, Loss: 0.957413837313652, Final Batch Loss: 0.3006924092769623\n",
      "Epoch 2821, Loss: 0.9957181811332703, Final Batch Loss: 0.3016155958175659\n",
      "Epoch 2822, Loss: 1.0386310070753098, Final Batch Loss: 0.239711731672287\n",
      "Epoch 2823, Loss: 1.0298434495925903, Final Batch Loss: 0.2459724247455597\n",
      "Epoch 2824, Loss: 0.9816965907812119, Final Batch Loss: 0.20824876427650452\n",
      "Epoch 2825, Loss: 0.9446178525686264, Final Batch Loss: 0.2314804494380951\n",
      "Epoch 2826, Loss: 0.9804943650960922, Final Batch Loss: 0.21803903579711914\n",
      "Epoch 2827, Loss: 1.1168846487998962, Final Batch Loss: 0.3292796015739441\n",
      "Epoch 2828, Loss: 0.9066973626613617, Final Batch Loss: 0.2350333333015442\n",
      "Epoch 2829, Loss: 0.9802975356578827, Final Batch Loss: 0.25665050745010376\n",
      "Epoch 2830, Loss: 0.9917441010475159, Final Batch Loss: 0.18892121315002441\n",
      "Epoch 2831, Loss: 0.9173426479101181, Final Batch Loss: 0.19070462882518768\n",
      "Epoch 2832, Loss: 0.869131863117218, Final Batch Loss: 0.19607362151145935\n",
      "Epoch 2833, Loss: 1.2132208049297333, Final Batch Loss: 0.4594810903072357\n",
      "Epoch 2834, Loss: 0.8728431910276413, Final Batch Loss: 0.20798088610172272\n",
      "Epoch 2835, Loss: 0.8850396126508713, Final Batch Loss: 0.23590144515037537\n",
      "Epoch 2836, Loss: 0.9562523066997528, Final Batch Loss: 0.17856061458587646\n",
      "Epoch 2837, Loss: 0.9427022188901901, Final Batch Loss: 0.29950350522994995\n",
      "Epoch 2838, Loss: 0.9183346480131149, Final Batch Loss: 0.1949198842048645\n",
      "Epoch 2839, Loss: 0.9219433814287186, Final Batch Loss: 0.24385780096054077\n",
      "Epoch 2840, Loss: 0.9587564170360565, Final Batch Loss: 0.20561370253562927\n",
      "Epoch 2841, Loss: 0.9836577326059341, Final Batch Loss: 0.2927021384239197\n",
      "Epoch 2842, Loss: 0.901944175362587, Final Batch Loss: 0.20294584333896637\n",
      "Epoch 2843, Loss: 0.894072026014328, Final Batch Loss: 0.1736595779657364\n",
      "Epoch 2844, Loss: 1.015774667263031, Final Batch Loss: 0.27051445841789246\n",
      "Epoch 2845, Loss: 1.0620711594820023, Final Batch Loss: 0.2830691933631897\n",
      "Epoch 2846, Loss: 1.0343904793262482, Final Batch Loss: 0.3390345871448517\n",
      "Epoch 2847, Loss: 0.8501170575618744, Final Batch Loss: 0.17324021458625793\n",
      "Epoch 2848, Loss: 0.9383205771446228, Final Batch Loss: 0.2679469585418701\n",
      "Epoch 2849, Loss: 1.0602431148290634, Final Batch Loss: 0.2951298952102661\n",
      "Epoch 2850, Loss: 0.9395903572440147, Final Batch Loss: 0.124552421271801\n",
      "Epoch 2851, Loss: 1.1155623346567154, Final Batch Loss: 0.3365985155105591\n",
      "Epoch 2852, Loss: 0.9219450950622559, Final Batch Loss: 0.16581958532333374\n",
      "Epoch 2853, Loss: 0.9708314836025238, Final Batch Loss: 0.20055364072322845\n",
      "Epoch 2854, Loss: 0.9211351871490479, Final Batch Loss: 0.19978547096252441\n",
      "Epoch 2855, Loss: 1.0213374644517899, Final Batch Loss: 0.3121355473995209\n",
      "Epoch 2856, Loss: 1.0011643469333649, Final Batch Loss: 0.21647262573242188\n",
      "Epoch 2857, Loss: 0.9274070560932159, Final Batch Loss: 0.17429415881633759\n",
      "Epoch 2858, Loss: 0.8014330267906189, Final Batch Loss: 0.15334419906139374\n",
      "Epoch 2859, Loss: 1.032028391957283, Final Batch Loss: 0.32485029101371765\n",
      "Epoch 2860, Loss: 0.8957255035638809, Final Batch Loss: 0.16716119647026062\n",
      "Epoch 2861, Loss: 1.0081892311573029, Final Batch Loss: 0.30845001339912415\n",
      "Epoch 2862, Loss: 0.9958703219890594, Final Batch Loss: 0.28662535548210144\n",
      "Epoch 2863, Loss: 0.9301949292421341, Final Batch Loss: 0.21499872207641602\n",
      "Epoch 2864, Loss: 0.8488946706056595, Final Batch Loss: 0.153779998421669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2865, Loss: 0.9055959284305573, Final Batch Loss: 0.13343991339206696\n",
      "Epoch 2866, Loss: 0.9931622743606567, Final Batch Loss: 0.3164735734462738\n",
      "Epoch 2867, Loss: 0.8983224630355835, Final Batch Loss: 0.18775013089179993\n",
      "Epoch 2868, Loss: 0.9568829834461212, Final Batch Loss: 0.2752327620983124\n",
      "Epoch 2869, Loss: 0.9392874836921692, Final Batch Loss: 0.22745919227600098\n",
      "Epoch 2870, Loss: 0.9801304489374161, Final Batch Loss: 0.23960807919502258\n",
      "Epoch 2871, Loss: 1.0058914870023727, Final Batch Loss: 0.2669244110584259\n",
      "Epoch 2872, Loss: 0.8935917913913727, Final Batch Loss: 0.22078463435173035\n",
      "Epoch 2873, Loss: 0.9920122921466827, Final Batch Loss: 0.24189019203186035\n",
      "Epoch 2874, Loss: 0.8814467191696167, Final Batch Loss: 0.1956697255373001\n",
      "Epoch 2875, Loss: 0.9443814009428024, Final Batch Loss: 0.19495034217834473\n",
      "Epoch 2876, Loss: 0.9031165540218353, Final Batch Loss: 0.23237143456935883\n",
      "Epoch 2877, Loss: 0.8768322020769119, Final Batch Loss: 0.21620458364486694\n",
      "Epoch 2878, Loss: 0.9756151139736176, Final Batch Loss: 0.2465454787015915\n",
      "Epoch 2879, Loss: 1.0327650904655457, Final Batch Loss: 0.23365776240825653\n",
      "Epoch 2880, Loss: 1.0230736881494522, Final Batch Loss: 0.21710257232189178\n",
      "Epoch 2881, Loss: 0.8939027786254883, Final Batch Loss: 0.2551264762878418\n",
      "Epoch 2882, Loss: 0.941151887178421, Final Batch Loss: 0.21355077624320984\n",
      "Epoch 2883, Loss: 0.8796397149562836, Final Batch Loss: 0.21388213336467743\n",
      "Epoch 2884, Loss: 1.035210832953453, Final Batch Loss: 0.2676825225353241\n",
      "Epoch 2885, Loss: 0.9687358438968658, Final Batch Loss: 0.26517388224601746\n",
      "Epoch 2886, Loss: 0.9658094197511673, Final Batch Loss: 0.2381228357553482\n",
      "Epoch 2887, Loss: 1.0720477551221848, Final Batch Loss: 0.2741822898387909\n",
      "Epoch 2888, Loss: 0.8931012451648712, Final Batch Loss: 0.19263584911823273\n",
      "Epoch 2889, Loss: 0.9094095379114151, Final Batch Loss: 0.242086261510849\n",
      "Epoch 2890, Loss: 0.8830337673425674, Final Batch Loss: 0.1588437408208847\n",
      "Epoch 2891, Loss: 0.9418818205595016, Final Batch Loss: 0.31825679540634155\n",
      "Epoch 2892, Loss: 0.8147805631160736, Final Batch Loss: 0.1811893731355667\n",
      "Epoch 2893, Loss: 1.097678691148758, Final Batch Loss: 0.3606628477573395\n",
      "Epoch 2894, Loss: 0.935784786939621, Final Batch Loss: 0.22683188319206238\n",
      "Epoch 2895, Loss: 0.8882765173912048, Final Batch Loss: 0.12401467561721802\n",
      "Epoch 2896, Loss: 0.9941739588975906, Final Batch Loss: 0.2883177101612091\n",
      "Epoch 2897, Loss: 1.0200573652982712, Final Batch Loss: 0.282261461019516\n",
      "Epoch 2898, Loss: 1.1194902062416077, Final Batch Loss: 0.25826767086982727\n",
      "Epoch 2899, Loss: 0.9792592525482178, Final Batch Loss: 0.28811579942703247\n",
      "Epoch 2900, Loss: 0.9307073354721069, Final Batch Loss: 0.13724908232688904\n",
      "Epoch 2901, Loss: 0.9313498139381409, Final Batch Loss: 0.2834457755088806\n",
      "Epoch 2902, Loss: 1.0356206595897675, Final Batch Loss: 0.26747560501098633\n",
      "Epoch 2903, Loss: 0.9793968498706818, Final Batch Loss: 0.24245290458202362\n",
      "Epoch 2904, Loss: 0.9930226802825928, Final Batch Loss: 0.2801676392555237\n",
      "Epoch 2905, Loss: 0.7557119876146317, Final Batch Loss: 0.13241985440254211\n",
      "Epoch 2906, Loss: 0.9254816770553589, Final Batch Loss: 0.19749893248081207\n",
      "Epoch 2907, Loss: 0.9162684082984924, Final Batch Loss: 0.23085853457450867\n",
      "Epoch 2908, Loss: 0.9071419537067413, Final Batch Loss: 0.2756207585334778\n",
      "Epoch 2909, Loss: 0.9842752516269684, Final Batch Loss: 0.30134111642837524\n",
      "Epoch 2910, Loss: 0.9216208010911942, Final Batch Loss: 0.2288881540298462\n",
      "Epoch 2911, Loss: 0.8835811614990234, Final Batch Loss: 0.1303282082080841\n",
      "Epoch 2912, Loss: 1.0087757408618927, Final Batch Loss: 0.24338406324386597\n",
      "Epoch 2913, Loss: 0.984144315123558, Final Batch Loss: 0.19558624923229218\n",
      "Epoch 2914, Loss: 0.907841756939888, Final Batch Loss: 0.22970016300678253\n",
      "Epoch 2915, Loss: 0.9425288736820221, Final Batch Loss: 0.21454693377017975\n",
      "Epoch 2916, Loss: 0.8631677329540253, Final Batch Loss: 0.18398211896419525\n",
      "Epoch 2917, Loss: 1.008933186531067, Final Batch Loss: 0.2940775454044342\n",
      "Epoch 2918, Loss: 0.8574100732803345, Final Batch Loss: 0.16654394567012787\n",
      "Epoch 2919, Loss: 0.9321426749229431, Final Batch Loss: 0.3038482666015625\n",
      "Epoch 2920, Loss: 1.0330778509378433, Final Batch Loss: 0.30925866961479187\n",
      "Epoch 2921, Loss: 0.9604713618755341, Final Batch Loss: 0.21782130002975464\n",
      "Epoch 2922, Loss: 0.9329761564731598, Final Batch Loss: 0.26867854595184326\n",
      "Epoch 2923, Loss: 1.0943472683429718, Final Batch Loss: 0.23718124628067017\n",
      "Epoch 2924, Loss: 0.8691748380661011, Final Batch Loss: 0.19784419238567352\n",
      "Epoch 2925, Loss: 1.0388421565294266, Final Batch Loss: 0.36991026997566223\n",
      "Epoch 2926, Loss: 1.0006014108657837, Final Batch Loss: 0.22797337174415588\n",
      "Epoch 2927, Loss: 0.9119703620672226, Final Batch Loss: 0.23329687118530273\n",
      "Epoch 2928, Loss: 1.0404152274131775, Final Batch Loss: 0.24426911771297455\n",
      "Epoch 2929, Loss: 0.9367368817329407, Final Batch Loss: 0.1947001963853836\n",
      "Epoch 2930, Loss: 1.0291516929864883, Final Batch Loss: 0.30438658595085144\n",
      "Epoch 2931, Loss: 1.0191220194101334, Final Batch Loss: 0.31937548518180847\n",
      "Epoch 2932, Loss: 0.8166108280420303, Final Batch Loss: 0.18270725011825562\n",
      "Epoch 2933, Loss: 1.0179201662540436, Final Batch Loss: 0.28475165367126465\n",
      "Epoch 2934, Loss: 1.0299651771783829, Final Batch Loss: 0.28305917978286743\n",
      "Epoch 2935, Loss: 0.910364180803299, Final Batch Loss: 0.23679618537425995\n",
      "Epoch 2936, Loss: 1.0187548696994781, Final Batch Loss: 0.22674560546875\n",
      "Epoch 2937, Loss: 0.865520253777504, Final Batch Loss: 0.17631372809410095\n",
      "Epoch 2938, Loss: 0.8526579141616821, Final Batch Loss: 0.1963176429271698\n",
      "Epoch 2939, Loss: 0.9108832776546478, Final Batch Loss: 0.22794552147388458\n",
      "Epoch 2940, Loss: 0.8984916508197784, Final Batch Loss: 0.23239469528198242\n",
      "Epoch 2941, Loss: 0.8947395980358124, Final Batch Loss: 0.17128349840641022\n",
      "Epoch 2942, Loss: 0.8727642297744751, Final Batch Loss: 0.23528283834457397\n",
      "Epoch 2943, Loss: 0.7822182178497314, Final Batch Loss: 0.1258404552936554\n",
      "Epoch 2944, Loss: 0.9612825363874435, Final Batch Loss: 0.24529944360256195\n",
      "Epoch 2945, Loss: 0.8712152242660522, Final Batch Loss: 0.13478927314281464\n",
      "Epoch 2946, Loss: 0.9984843581914902, Final Batch Loss: 0.2868654429912567\n",
      "Epoch 2947, Loss: 0.9927020370960236, Final Batch Loss: 0.2886784076690674\n",
      "Epoch 2948, Loss: 0.8521832227706909, Final Batch Loss: 0.19309480488300323\n",
      "Epoch 2949, Loss: 0.9878833442926407, Final Batch Loss: 0.2574620842933655\n",
      "Epoch 2950, Loss: 0.9579874128103256, Final Batch Loss: 0.23910970985889435\n",
      "Epoch 2951, Loss: 1.001879096031189, Final Batch Loss: 0.2453230619430542\n",
      "Epoch 2952, Loss: 0.9242384135723114, Final Batch Loss: 0.19373902678489685\n",
      "Epoch 2953, Loss: 0.8934915363788605, Final Batch Loss: 0.24893827736377716\n",
      "Epoch 2954, Loss: 0.9463353902101517, Final Batch Loss: 0.25403067469596863\n",
      "Epoch 2955, Loss: 1.0789159536361694, Final Batch Loss: 0.23779702186584473\n",
      "Epoch 2956, Loss: 0.9543066918849945, Final Batch Loss: 0.24817875027656555\n",
      "Epoch 2957, Loss: 0.8568915873765945, Final Batch Loss: 0.17563240230083466\n",
      "Epoch 2958, Loss: 1.024713709950447, Final Batch Loss: 0.3734988868236542\n",
      "Epoch 2959, Loss: 0.9815142005681992, Final Batch Loss: 0.17731203138828278\n",
      "Epoch 2960, Loss: 0.9521632194519043, Final Batch Loss: 0.20887507498264313\n",
      "Epoch 2961, Loss: 0.9582885503768921, Final Batch Loss: 0.15132713317871094\n",
      "Epoch 2962, Loss: 1.0110129714012146, Final Batch Loss: 0.21881312131881714\n",
      "Epoch 2963, Loss: 0.9466600120067596, Final Batch Loss: 0.19578278064727783\n",
      "Epoch 2964, Loss: 0.8957467824220657, Final Batch Loss: 0.19308418035507202\n",
      "Epoch 2965, Loss: 0.92166768014431, Final Batch Loss: 0.2340545505285263\n",
      "Epoch 2966, Loss: 0.9811515957117081, Final Batch Loss: 0.28676122426986694\n",
      "Epoch 2967, Loss: 0.806867703795433, Final Batch Loss: 0.20063169300556183\n",
      "Epoch 2968, Loss: 0.8999328762292862, Final Batch Loss: 0.23727382719516754\n",
      "Epoch 2969, Loss: 1.0530730038881302, Final Batch Loss: 0.2108762264251709\n",
      "Epoch 2970, Loss: 0.9032602310180664, Final Batch Loss: 0.21784311532974243\n",
      "Epoch 2971, Loss: 0.8318596035242081, Final Batch Loss: 0.19064195454120636\n",
      "Epoch 2972, Loss: 0.8908185809850693, Final Batch Loss: 0.1701091080904007\n",
      "Epoch 2973, Loss: 0.9039078205823898, Final Batch Loss: 0.1957409828901291\n",
      "Epoch 2974, Loss: 0.9484247714281082, Final Batch Loss: 0.24243713915348053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2975, Loss: 0.8537309169769287, Final Batch Loss: 0.2516128420829773\n",
      "Epoch 2976, Loss: 0.8748435527086258, Final Batch Loss: 0.238599494099617\n",
      "Epoch 2977, Loss: 1.0264575332403183, Final Batch Loss: 0.27620601654052734\n",
      "Epoch 2978, Loss: 0.973309263586998, Final Batch Loss: 0.3271159529685974\n",
      "Epoch 2979, Loss: 0.9900061637163162, Final Batch Loss: 0.23348796367645264\n",
      "Epoch 2980, Loss: 0.9210378080606461, Final Batch Loss: 0.23311066627502441\n",
      "Epoch 2981, Loss: 0.9328132271766663, Final Batch Loss: 0.26626476645469666\n",
      "Epoch 2982, Loss: 0.9811854958534241, Final Batch Loss: 0.19253148138523102\n",
      "Epoch 2983, Loss: 1.0487482398748398, Final Batch Loss: 0.3233921527862549\n",
      "Epoch 2984, Loss: 1.0270491242408752, Final Batch Loss: 0.2738848924636841\n",
      "Epoch 2985, Loss: 1.002372071146965, Final Batch Loss: 0.2866996228694916\n",
      "Epoch 2986, Loss: 0.8944871872663498, Final Batch Loss: 0.13861674070358276\n",
      "Epoch 2987, Loss: 0.9007477164268494, Final Batch Loss: 0.2129938155412674\n",
      "Epoch 2988, Loss: 1.00344617664814, Final Batch Loss: 0.2555219829082489\n",
      "Epoch 2989, Loss: 0.8288827240467072, Final Batch Loss: 0.22384074330329895\n",
      "Epoch 2990, Loss: 0.8668297678232193, Final Batch Loss: 0.21053390204906464\n",
      "Epoch 2991, Loss: 1.0390143245458603, Final Batch Loss: 0.3615838289260864\n",
      "Epoch 2992, Loss: 0.8956245481967926, Final Batch Loss: 0.2960403263568878\n",
      "Epoch 2993, Loss: 0.9271066337823868, Final Batch Loss: 0.17812302708625793\n",
      "Epoch 2994, Loss: 0.8931403458118439, Final Batch Loss: 0.2454572319984436\n",
      "Epoch 2995, Loss: 0.9755447059869766, Final Batch Loss: 0.27952054142951965\n",
      "Epoch 2996, Loss: 1.0998038947582245, Final Batch Loss: 0.33659258484840393\n",
      "Epoch 2997, Loss: 0.9540079683065414, Final Batch Loss: 0.2906913459300995\n",
      "Epoch 2998, Loss: 0.9045991599559784, Final Batch Loss: 0.17758798599243164\n",
      "Epoch 2999, Loss: 1.0064255148172379, Final Batch Loss: 0.2773597240447998\n",
      "Epoch 3000, Loss: 1.0368714928627014, Final Batch Loss: 0.2850450277328491\n",
      "Epoch 3001, Loss: 0.8669605702161789, Final Batch Loss: 0.18460650742053986\n",
      "Epoch 3002, Loss: 1.0096054822206497, Final Batch Loss: 0.27996501326560974\n",
      "Epoch 3003, Loss: 0.9751682281494141, Final Batch Loss: 0.3126915395259857\n",
      "Epoch 3004, Loss: 0.9604797661304474, Final Batch Loss: 0.279006689786911\n",
      "Epoch 3005, Loss: 0.9639780521392822, Final Batch Loss: 0.23804599046707153\n",
      "Epoch 3006, Loss: 0.8591227829456329, Final Batch Loss: 0.21737085282802582\n",
      "Epoch 3007, Loss: 0.8812251687049866, Final Batch Loss: 0.2618638873100281\n",
      "Epoch 3008, Loss: 1.0292140692472458, Final Batch Loss: 0.33449193835258484\n",
      "Epoch 3009, Loss: 0.8639813363552094, Final Batch Loss: 0.2292245626449585\n",
      "Epoch 3010, Loss: 0.8857771456241608, Final Batch Loss: 0.2567893862724304\n",
      "Epoch 3011, Loss: 0.9983762949705124, Final Batch Loss: 0.24013438820838928\n",
      "Epoch 3012, Loss: 1.0082612186670303, Final Batch Loss: 0.18671362102031708\n",
      "Epoch 3013, Loss: 0.8881961405277252, Final Batch Loss: 0.2116023600101471\n",
      "Epoch 3014, Loss: 0.9086702913045883, Final Batch Loss: 0.23030774295330048\n",
      "Epoch 3015, Loss: 0.8694784045219421, Final Batch Loss: 0.24356165528297424\n",
      "Epoch 3016, Loss: 0.8371055126190186, Final Batch Loss: 0.2020624876022339\n",
      "Epoch 3017, Loss: 0.9048551023006439, Final Batch Loss: 0.2845747470855713\n",
      "Epoch 3018, Loss: 0.9269536733627319, Final Batch Loss: 0.2913646399974823\n",
      "Epoch 3019, Loss: 0.9206915497779846, Final Batch Loss: 0.2648516297340393\n",
      "Epoch 3020, Loss: 0.9304348528385162, Final Batch Loss: 0.25810885429382324\n",
      "Epoch 3021, Loss: 0.8747518956661224, Final Batch Loss: 0.17928898334503174\n",
      "Epoch 3022, Loss: 0.9351222217082977, Final Batch Loss: 0.23330117762088776\n",
      "Epoch 3023, Loss: 0.8654132932424545, Final Batch Loss: 0.19701874256134033\n",
      "Epoch 3024, Loss: 0.8861826211214066, Final Batch Loss: 0.21794797480106354\n",
      "Epoch 3025, Loss: 0.864593580365181, Final Batch Loss: 0.23040850460529327\n",
      "Epoch 3026, Loss: 0.8705220967531204, Final Batch Loss: 0.22862835228443146\n",
      "Epoch 3027, Loss: 0.8919275850057602, Final Batch Loss: 0.17470821738243103\n",
      "Epoch 3028, Loss: 0.9721731245517731, Final Batch Loss: 0.2670869827270508\n",
      "Epoch 3029, Loss: 0.9246029406785965, Final Batch Loss: 0.28810447454452515\n",
      "Epoch 3030, Loss: 0.9039724320173264, Final Batch Loss: 0.2136732041835785\n",
      "Epoch 3031, Loss: 0.8797150254249573, Final Batch Loss: 0.1910490244626999\n",
      "Epoch 3032, Loss: 0.8401404619216919, Final Batch Loss: 0.16044344007968903\n",
      "Epoch 3033, Loss: 0.8724369257688522, Final Batch Loss: 0.14073139429092407\n",
      "Epoch 3034, Loss: 0.814081460237503, Final Batch Loss: 0.14228172600269318\n",
      "Epoch 3035, Loss: 0.9234267473220825, Final Batch Loss: 0.266271710395813\n",
      "Epoch 3036, Loss: 0.9387301951646805, Final Batch Loss: 0.21280311048030853\n",
      "Epoch 3037, Loss: 0.7925385236740112, Final Batch Loss: 0.20520590245723724\n",
      "Epoch 3038, Loss: 0.8725313246250153, Final Batch Loss: 0.1956293135881424\n",
      "Epoch 3039, Loss: 1.017601639032364, Final Batch Loss: 0.24969415366649628\n",
      "Epoch 3040, Loss: 0.8116192519664764, Final Batch Loss: 0.17972546815872192\n",
      "Epoch 3041, Loss: 0.975463330745697, Final Batch Loss: 0.277417927980423\n",
      "Epoch 3042, Loss: 1.0430355072021484, Final Batch Loss: 0.2271319031715393\n",
      "Epoch 3043, Loss: 0.8968474268913269, Final Batch Loss: 0.22842037677764893\n",
      "Epoch 3044, Loss: 0.9725449830293655, Final Batch Loss: 0.31974080204963684\n",
      "Epoch 3045, Loss: 0.9097758382558823, Final Batch Loss: 0.1501132845878601\n",
      "Epoch 3046, Loss: 1.0511686503887177, Final Batch Loss: 0.2529924213886261\n",
      "Epoch 3047, Loss: 0.9399025291204453, Final Batch Loss: 0.26857513189315796\n",
      "Epoch 3048, Loss: 0.9651521593332291, Final Batch Loss: 0.27409031987190247\n",
      "Epoch 3049, Loss: 0.9246529340744019, Final Batch Loss: 0.1939159482717514\n",
      "Epoch 3050, Loss: 0.8747020959854126, Final Batch Loss: 0.15684960782527924\n",
      "Epoch 3051, Loss: 0.8866532295942307, Final Batch Loss: 0.12286381423473358\n",
      "Epoch 3052, Loss: 0.8222077786922455, Final Batch Loss: 0.17147846519947052\n",
      "Epoch 3053, Loss: 0.9409828633069992, Final Batch Loss: 0.3280276358127594\n",
      "Epoch 3054, Loss: 0.8136893510818481, Final Batch Loss: 0.23256903886795044\n",
      "Epoch 3055, Loss: 0.8563555479049683, Final Batch Loss: 0.21790054440498352\n",
      "Epoch 3056, Loss: 0.883318156003952, Final Batch Loss: 0.22684170305728912\n",
      "Epoch 3057, Loss: 0.8583542108535767, Final Batch Loss: 0.15175995230674744\n",
      "Epoch 3058, Loss: 1.0973232239484787, Final Batch Loss: 0.3646375834941864\n",
      "Epoch 3059, Loss: 1.0201930403709412, Final Batch Loss: 0.3282229006290436\n",
      "Epoch 3060, Loss: 0.8010722696781158, Final Batch Loss: 0.15283770859241486\n",
      "Epoch 3061, Loss: 0.9465154707431793, Final Batch Loss: 0.2466013878583908\n",
      "Epoch 3062, Loss: 1.0475834161043167, Final Batch Loss: 0.29472801089286804\n",
      "Epoch 3063, Loss: 0.8031399846076965, Final Batch Loss: 0.14860491454601288\n",
      "Epoch 3064, Loss: 0.9894327819347382, Final Batch Loss: 0.1896684318780899\n",
      "Epoch 3065, Loss: 0.957161471247673, Final Batch Loss: 0.2708381116390228\n",
      "Epoch 3066, Loss: 0.8201262354850769, Final Batch Loss: 0.1808711141347885\n",
      "Epoch 3067, Loss: 0.9812709242105484, Final Batch Loss: 0.36018985509872437\n",
      "Epoch 3068, Loss: 0.8577189147472382, Final Batch Loss: 0.17246437072753906\n",
      "Epoch 3069, Loss: 0.8342890292406082, Final Batch Loss: 0.18789678812026978\n",
      "Epoch 3070, Loss: 0.8158538490533829, Final Batch Loss: 0.19814170897006989\n",
      "Epoch 3071, Loss: 0.7996560335159302, Final Batch Loss: 0.18763643503189087\n",
      "Epoch 3072, Loss: 0.8246272802352905, Final Batch Loss: 0.22304639220237732\n",
      "Epoch 3073, Loss: 0.8959771692752838, Final Batch Loss: 0.31942006945610046\n",
      "Epoch 3074, Loss: 1.0303179025650024, Final Batch Loss: 0.31171679496765137\n",
      "Epoch 3075, Loss: 0.9294355362653732, Final Batch Loss: 0.24759234488010406\n",
      "Epoch 3076, Loss: 0.8870095610618591, Final Batch Loss: 0.17640459537506104\n",
      "Epoch 3077, Loss: 0.9031471014022827, Final Batch Loss: 0.21716724336147308\n",
      "Epoch 3078, Loss: 0.9653906226158142, Final Batch Loss: 0.2414560168981552\n",
      "Epoch 3079, Loss: 0.9038622677326202, Final Batch Loss: 0.19765102863311768\n",
      "Epoch 3080, Loss: 1.032699778676033, Final Batch Loss: 0.23348526656627655\n",
      "Epoch 3081, Loss: 0.9158379882574081, Final Batch Loss: 0.2304895520210266\n",
      "Epoch 3082, Loss: 0.9977517575025558, Final Batch Loss: 0.2442145049571991\n",
      "Epoch 3083, Loss: 0.9772472083568573, Final Batch Loss: 0.2639600336551666\n",
      "Epoch 3084, Loss: 1.0180280655622482, Final Batch Loss: 0.2331860512495041\n",
      "Epoch 3085, Loss: 0.9140507280826569, Final Batch Loss: 0.19100463390350342\n",
      "Epoch 3086, Loss: 0.9238637536764145, Final Batch Loss: 0.18275204300880432\n",
      "Epoch 3087, Loss: 0.9032086282968521, Final Batch Loss: 0.2244834303855896\n",
      "Epoch 3088, Loss: 0.919595867395401, Final Batch Loss: 0.19267146289348602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3089, Loss: 0.9382287263870239, Final Batch Loss: 0.3362310528755188\n",
      "Epoch 3090, Loss: 0.7770169526338577, Final Batch Loss: 0.17837920784950256\n",
      "Epoch 3091, Loss: 0.809233158826828, Final Batch Loss: 0.15714094042778015\n",
      "Epoch 3092, Loss: 0.9218316674232483, Final Batch Loss: 0.1927669793367386\n",
      "Epoch 3093, Loss: 0.8360482454299927, Final Batch Loss: 0.22847945988178253\n",
      "Epoch 3094, Loss: 0.9612899869680405, Final Batch Loss: 0.29611143469810486\n",
      "Epoch 3095, Loss: 1.0101988315582275, Final Batch Loss: 0.2645364999771118\n",
      "Epoch 3096, Loss: 0.9515356123447418, Final Batch Loss: 0.23127329349517822\n",
      "Epoch 3097, Loss: 1.0120864659547806, Final Batch Loss: 0.26980745792388916\n",
      "Epoch 3098, Loss: 0.8713834136724472, Final Batch Loss: 0.2458084374666214\n",
      "Epoch 3099, Loss: 0.9954452514648438, Final Batch Loss: 0.23088958859443665\n",
      "Epoch 3100, Loss: 0.8778524100780487, Final Batch Loss: 0.24722544848918915\n",
      "Epoch 3101, Loss: 0.8648380041122437, Final Batch Loss: 0.2507557272911072\n",
      "Epoch 3102, Loss: 0.9288074523210526, Final Batch Loss: 0.2558436989784241\n",
      "Epoch 3103, Loss: 0.8839414119720459, Final Batch Loss: 0.2673027515411377\n",
      "Epoch 3104, Loss: 0.913720041513443, Final Batch Loss: 0.2320660948753357\n",
      "Epoch 3105, Loss: 0.8912247270345688, Final Batch Loss: 0.22419992089271545\n",
      "Epoch 3106, Loss: 0.9003645032644272, Final Batch Loss: 0.17832288146018982\n",
      "Epoch 3107, Loss: 0.8083042800426483, Final Batch Loss: 0.2032192200422287\n",
      "Epoch 3108, Loss: 0.9656966030597687, Final Batch Loss: 0.2747214734554291\n",
      "Epoch 3109, Loss: 0.9740187525749207, Final Batch Loss: 0.22947143018245697\n",
      "Epoch 3110, Loss: 0.8585071563720703, Final Batch Loss: 0.2074856162071228\n",
      "Epoch 3111, Loss: 0.8905623853206635, Final Batch Loss: 0.23337653279304504\n",
      "Epoch 3112, Loss: 0.9006857573986053, Final Batch Loss: 0.22199532389640808\n",
      "Epoch 3113, Loss: 0.8543791472911835, Final Batch Loss: 0.19132661819458008\n",
      "Epoch 3114, Loss: 0.8911740332841873, Final Batch Loss: 0.17137746512889862\n",
      "Epoch 3115, Loss: 0.9263672083616257, Final Batch Loss: 0.2667737603187561\n",
      "Epoch 3116, Loss: 0.8486110717058182, Final Batch Loss: 0.16532063484191895\n",
      "Epoch 3117, Loss: 0.9767937660217285, Final Batch Loss: 0.19953079521656036\n",
      "Epoch 3118, Loss: 1.0374182909727097, Final Batch Loss: 0.28747084736824036\n",
      "Epoch 3119, Loss: 0.9091614931821823, Final Batch Loss: 0.24601885676383972\n",
      "Epoch 3120, Loss: 1.0408727675676346, Final Batch Loss: 0.28186485171318054\n",
      "Epoch 3121, Loss: 0.8514681458473206, Final Batch Loss: 0.20140968263149261\n",
      "Epoch 3122, Loss: 0.8431676626205444, Final Batch Loss: 0.21198056638240814\n",
      "Epoch 3123, Loss: 0.9079834520816803, Final Batch Loss: 0.15999957919120789\n",
      "Epoch 3124, Loss: 0.945573702454567, Final Batch Loss: 0.3050082325935364\n",
      "Epoch 3125, Loss: 0.9246660321950912, Final Batch Loss: 0.20973755419254303\n",
      "Epoch 3126, Loss: 0.9553996175527573, Final Batch Loss: 0.2210146188735962\n",
      "Epoch 3127, Loss: 1.0316144078969955, Final Batch Loss: 0.20652997493743896\n",
      "Epoch 3128, Loss: 0.8478939682245255, Final Batch Loss: 0.14992491900920868\n",
      "Epoch 3129, Loss: 0.9797149449586868, Final Batch Loss: 0.2878044545650482\n",
      "Epoch 3130, Loss: 0.9881947636604309, Final Batch Loss: 0.2622162699699402\n",
      "Epoch 3131, Loss: 0.8650226593017578, Final Batch Loss: 0.1853014975786209\n",
      "Epoch 3132, Loss: 0.9935639500617981, Final Batch Loss: 0.26148855686187744\n",
      "Epoch 3133, Loss: 0.7944754660129547, Final Batch Loss: 0.22333136200904846\n",
      "Epoch 3134, Loss: 0.9263804107904434, Final Batch Loss: 0.21092694997787476\n",
      "Epoch 3135, Loss: 1.0792254507541656, Final Batch Loss: 0.3167315125465393\n",
      "Epoch 3136, Loss: 0.9500354081392288, Final Batch Loss: 0.285196989774704\n",
      "Epoch 3137, Loss: 0.9020328521728516, Final Batch Loss: 0.13985396921634674\n",
      "Epoch 3138, Loss: 1.0636659562587738, Final Batch Loss: 0.23025187849998474\n",
      "Epoch 3139, Loss: 0.9193387776613235, Final Batch Loss: 0.2070164680480957\n",
      "Epoch 3140, Loss: 0.9567176401615143, Final Batch Loss: 0.25439396500587463\n",
      "Epoch 3141, Loss: 1.0312909483909607, Final Batch Loss: 0.24203528463840485\n",
      "Epoch 3142, Loss: 0.994544044137001, Final Batch Loss: 0.30282363295555115\n",
      "Epoch 3143, Loss: 0.8265262842178345, Final Batch Loss: 0.20914749801158905\n",
      "Epoch 3144, Loss: 0.9597768485546112, Final Batch Loss: 0.24068474769592285\n",
      "Epoch 3145, Loss: 0.9031499326229095, Final Batch Loss: 0.22556667029857635\n",
      "Epoch 3146, Loss: 0.8191081285476685, Final Batch Loss: 0.13869397342205048\n",
      "Epoch 3147, Loss: 0.8904842734336853, Final Batch Loss: 0.18069058656692505\n",
      "Epoch 3148, Loss: 1.171107366681099, Final Batch Loss: 0.4221295714378357\n",
      "Epoch 3149, Loss: 0.9945555329322815, Final Batch Loss: 0.19476786255836487\n",
      "Epoch 3150, Loss: 0.8464302271604538, Final Batch Loss: 0.1300138682126999\n",
      "Epoch 3151, Loss: 0.965248316526413, Final Batch Loss: 0.2629438638687134\n",
      "Epoch 3152, Loss: 0.8342419117689133, Final Batch Loss: 0.13832518458366394\n",
      "Epoch 3153, Loss: 0.9235890507698059, Final Batch Loss: 0.23825617134571075\n",
      "Epoch 3154, Loss: 0.8952805250883102, Final Batch Loss: 0.2229325920343399\n",
      "Epoch 3155, Loss: 0.8719967603683472, Final Batch Loss: 0.18430019915103912\n",
      "Epoch 3156, Loss: 0.9252220094203949, Final Batch Loss: 0.26845672726631165\n",
      "Epoch 3157, Loss: 0.8666989952325821, Final Batch Loss: 0.20238390564918518\n",
      "Epoch 3158, Loss: 0.9224742203950882, Final Batch Loss: 0.24523627758026123\n",
      "Epoch 3159, Loss: 0.9622726738452911, Final Batch Loss: 0.2396508902311325\n",
      "Epoch 3160, Loss: 0.8931307196617126, Final Batch Loss: 0.21453596651554108\n",
      "Epoch 3161, Loss: 0.8430919349193573, Final Batch Loss: 0.2419770061969757\n",
      "Epoch 3162, Loss: 0.8675480335950851, Final Batch Loss: 0.177830770611763\n",
      "Epoch 3163, Loss: 0.8699386864900589, Final Batch Loss: 0.18489792943000793\n",
      "Epoch 3164, Loss: 0.8569792658090591, Final Batch Loss: 0.1675766259431839\n",
      "Epoch 3165, Loss: 0.917548269033432, Final Batch Loss: 0.2763558328151703\n",
      "Epoch 3166, Loss: 0.8915160447359085, Final Batch Loss: 0.25147557258605957\n",
      "Epoch 3167, Loss: 0.8557838499546051, Final Batch Loss: 0.20883122086524963\n",
      "Epoch 3168, Loss: 0.9181138575077057, Final Batch Loss: 0.2643987536430359\n",
      "Epoch 3169, Loss: 0.8941498547792435, Final Batch Loss: 0.2429225742816925\n",
      "Epoch 3170, Loss: 0.8933055996894836, Final Batch Loss: 0.27898111939430237\n",
      "Epoch 3171, Loss: 0.7415531873703003, Final Batch Loss: 0.12188376486301422\n",
      "Epoch 3172, Loss: 0.978497102856636, Final Batch Loss: 0.3298206031322479\n",
      "Epoch 3173, Loss: 0.8899608999490738, Final Batch Loss: 0.2121361643075943\n",
      "Epoch 3174, Loss: 0.8718335032463074, Final Batch Loss: 0.17641259729862213\n",
      "Epoch 3175, Loss: 0.8435745984315872, Final Batch Loss: 0.21213538944721222\n",
      "Epoch 3176, Loss: 0.8764530569314957, Final Batch Loss: 0.1942029893398285\n",
      "Epoch 3177, Loss: 0.8646603971719742, Final Batch Loss: 0.22391028702259064\n",
      "Epoch 3178, Loss: 0.8301525413990021, Final Batch Loss: 0.19795256853103638\n",
      "Epoch 3179, Loss: 0.9070002883672714, Final Batch Loss: 0.2441910207271576\n",
      "Epoch 3180, Loss: 0.9246750026941299, Final Batch Loss: 0.18484127521514893\n",
      "Epoch 3181, Loss: 0.9839550852775574, Final Batch Loss: 0.303514301776886\n",
      "Epoch 3182, Loss: 0.9750984162092209, Final Batch Loss: 0.27006110548973083\n",
      "Epoch 3183, Loss: 0.9328176230192184, Final Batch Loss: 0.19372329115867615\n",
      "Epoch 3184, Loss: 0.9057790786027908, Final Batch Loss: 0.23010143637657166\n",
      "Epoch 3185, Loss: 0.8824548870325089, Final Batch Loss: 0.202505961060524\n",
      "Epoch 3186, Loss: 0.9533718824386597, Final Batch Loss: 0.24444088339805603\n",
      "Epoch 3187, Loss: 0.929987221956253, Final Batch Loss: 0.20064689218997955\n",
      "Epoch 3188, Loss: 0.8541472405195236, Final Batch Loss: 0.1791234314441681\n",
      "Epoch 3189, Loss: 0.9318517744541168, Final Batch Loss: 0.2674829661846161\n",
      "Epoch 3190, Loss: 0.8572302460670471, Final Batch Loss: 0.21341128647327423\n",
      "Epoch 3191, Loss: 0.9304636567831039, Final Batch Loss: 0.2747219502925873\n",
      "Epoch 3192, Loss: 0.9153707176446915, Final Batch Loss: 0.23017780482769012\n",
      "Epoch 3193, Loss: 0.8826219886541367, Final Batch Loss: 0.18886975944042206\n",
      "Epoch 3194, Loss: 1.0461446195840836, Final Batch Loss: 0.22789843380451202\n",
      "Epoch 3195, Loss: 0.9840907603502274, Final Batch Loss: 0.28843510150909424\n",
      "Epoch 3196, Loss: 0.9775624573230743, Final Batch Loss: 0.24607224762439728\n",
      "Epoch 3197, Loss: 0.8130670934915543, Final Batch Loss: 0.10135743021965027\n",
      "Epoch 3198, Loss: 0.8783353269100189, Final Batch Loss: 0.23165461421012878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3199, Loss: 0.8714417666196823, Final Batch Loss: 0.25560060143470764\n",
      "Epoch 3200, Loss: 0.8787247389554977, Final Batch Loss: 0.2882094383239746\n",
      "Epoch 3201, Loss: 0.942981019616127, Final Batch Loss: 0.168968066573143\n",
      "Epoch 3202, Loss: 0.8696235418319702, Final Batch Loss: 0.18061432242393494\n",
      "Epoch 3203, Loss: 0.9039217382669449, Final Batch Loss: 0.21739287674427032\n",
      "Epoch 3204, Loss: 0.9971814155578613, Final Batch Loss: 0.3782045245170593\n",
      "Epoch 3205, Loss: 0.9225217401981354, Final Batch Loss: 0.20449933409690857\n",
      "Epoch 3206, Loss: 0.8790398240089417, Final Batch Loss: 0.23388099670410156\n",
      "Epoch 3207, Loss: 0.8566240817308426, Final Batch Loss: 0.17363256216049194\n",
      "Epoch 3208, Loss: 0.9428450465202332, Final Batch Loss: 0.1973314881324768\n",
      "Epoch 3209, Loss: 1.0266814082860947, Final Batch Loss: 0.3044261634349823\n",
      "Epoch 3210, Loss: 0.9105624705553055, Final Batch Loss: 0.21598397195339203\n",
      "Epoch 3211, Loss: 0.9268667995929718, Final Batch Loss: 0.23139216005802155\n",
      "Epoch 3212, Loss: 0.8641851544380188, Final Batch Loss: 0.20029209554195404\n",
      "Epoch 3213, Loss: 0.8907536268234253, Final Batch Loss: 0.23710201680660248\n",
      "Epoch 3214, Loss: 0.8720420449972153, Final Batch Loss: 0.24554479122161865\n",
      "Epoch 3215, Loss: 0.9375955313444138, Final Batch Loss: 0.25613078474998474\n",
      "Epoch 3216, Loss: 0.8122691959142685, Final Batch Loss: 0.1595170795917511\n",
      "Epoch 3217, Loss: 0.8162382543087006, Final Batch Loss: 0.14113187789916992\n",
      "Epoch 3218, Loss: 0.9265273213386536, Final Batch Loss: 0.22554972767829895\n",
      "Epoch 3219, Loss: 0.9452254772186279, Final Batch Loss: 0.2257799208164215\n",
      "Epoch 3220, Loss: 0.8503442257642746, Final Batch Loss: 0.1807139813899994\n",
      "Epoch 3221, Loss: 0.9513764828443527, Final Batch Loss: 0.32714736461639404\n",
      "Epoch 3222, Loss: 1.0043187737464905, Final Batch Loss: 0.23539553582668304\n",
      "Epoch 3223, Loss: 0.9302562773227692, Final Batch Loss: 0.18383058905601501\n",
      "Epoch 3224, Loss: 1.0128901898860931, Final Batch Loss: 0.2998582124710083\n",
      "Epoch 3225, Loss: 0.9927714765071869, Final Batch Loss: 0.3109126091003418\n",
      "Epoch 3226, Loss: 0.8638200759887695, Final Batch Loss: 0.21655361354351044\n",
      "Epoch 3227, Loss: 0.9299277365207672, Final Batch Loss: 0.1936512142419815\n",
      "Epoch 3228, Loss: 1.0321197360754013, Final Batch Loss: 0.3617638349533081\n",
      "Epoch 3229, Loss: 0.8050132095813751, Final Batch Loss: 0.12350259721279144\n",
      "Epoch 3230, Loss: 0.9756705015897751, Final Batch Loss: 0.27679547667503357\n",
      "Epoch 3231, Loss: 0.8260695189237595, Final Batch Loss: 0.1789761781692505\n",
      "Epoch 3232, Loss: 0.9288165122270584, Final Batch Loss: 0.24112898111343384\n",
      "Epoch 3233, Loss: 0.9002357125282288, Final Batch Loss: 0.324419766664505\n",
      "Epoch 3234, Loss: 0.91243976354599, Final Batch Loss: 0.17970190942287445\n",
      "Epoch 3235, Loss: 0.8021021634340286, Final Batch Loss: 0.12967729568481445\n",
      "Epoch 3236, Loss: 0.8997462689876556, Final Batch Loss: 0.21009311079978943\n",
      "Epoch 3237, Loss: 0.9249099791049957, Final Batch Loss: 0.18504908680915833\n",
      "Epoch 3238, Loss: 0.8827762007713318, Final Batch Loss: 0.2528325021266937\n",
      "Epoch 3239, Loss: 1.032500684261322, Final Batch Loss: 0.35932087898254395\n",
      "Epoch 3240, Loss: 0.9246459007263184, Final Batch Loss: 0.22313660383224487\n",
      "Epoch 3241, Loss: 0.8930913358926773, Final Batch Loss: 0.20338588953018188\n",
      "Epoch 3242, Loss: 0.9826885461807251, Final Batch Loss: 0.29235538840293884\n",
      "Epoch 3243, Loss: 1.0342600643634796, Final Batch Loss: 0.3314758241176605\n",
      "Epoch 3244, Loss: 0.8515496999025345, Final Batch Loss: 0.240706205368042\n",
      "Epoch 3245, Loss: 0.8696497976779938, Final Batch Loss: 0.14150230586528778\n",
      "Epoch 3246, Loss: 0.952510803937912, Final Batch Loss: 0.2168431282043457\n",
      "Epoch 3247, Loss: 0.9727613925933838, Final Batch Loss: 0.2934160828590393\n",
      "Epoch 3248, Loss: 0.9181303381919861, Final Batch Loss: 0.27554038166999817\n",
      "Epoch 3249, Loss: 0.880372166633606, Final Batch Loss: 0.2208268940448761\n",
      "Epoch 3250, Loss: 0.9226882308721542, Final Batch Loss: 0.24438269436359406\n",
      "Epoch 3251, Loss: 0.8056764975190163, Final Batch Loss: 0.11899033933877945\n",
      "Epoch 3252, Loss: 0.822713166475296, Final Batch Loss: 0.19616998732089996\n",
      "Epoch 3253, Loss: 0.9054194390773773, Final Batch Loss: 0.2411099076271057\n",
      "Epoch 3254, Loss: 0.808727577328682, Final Batch Loss: 0.23512090742588043\n",
      "Epoch 3255, Loss: 0.8473013490438461, Final Batch Loss: 0.22216804325580597\n",
      "Epoch 3256, Loss: 0.8676907271146774, Final Batch Loss: 0.2091708779335022\n",
      "Epoch 3257, Loss: 0.9438588321208954, Final Batch Loss: 0.22796186804771423\n",
      "Epoch 3258, Loss: 0.8349253088235855, Final Batch Loss: 0.16866621375083923\n",
      "Epoch 3259, Loss: 0.9259209632873535, Final Batch Loss: 0.21475601196289062\n",
      "Epoch 3260, Loss: 0.9177577197551727, Final Batch Loss: 0.28200963139533997\n",
      "Epoch 3261, Loss: 0.8891406357288361, Final Batch Loss: 0.2854481637477875\n",
      "Epoch 3262, Loss: 0.879443809390068, Final Batch Loss: 0.2673529088497162\n",
      "Epoch 3263, Loss: 0.781949445605278, Final Batch Loss: 0.20404072105884552\n",
      "Epoch 3264, Loss: 0.8673136383295059, Final Batch Loss: 0.23710662126541138\n",
      "Epoch 3265, Loss: 0.9848207384347916, Final Batch Loss: 0.3002811372280121\n",
      "Epoch 3266, Loss: 1.015778437256813, Final Batch Loss: 0.20550759136676788\n",
      "Epoch 3267, Loss: 0.9019370079040527, Final Batch Loss: 0.2469441294670105\n",
      "Epoch 3268, Loss: 0.9341759532690048, Final Batch Loss: 0.21520070731639862\n",
      "Epoch 3269, Loss: 0.8957115113735199, Final Batch Loss: 0.2297985702753067\n",
      "Epoch 3270, Loss: 0.8926259875297546, Final Batch Loss: 0.20760288834571838\n",
      "Epoch 3271, Loss: 0.9662095159292221, Final Batch Loss: 0.20160456001758575\n",
      "Epoch 3272, Loss: 0.755479484796524, Final Batch Loss: 0.14050762355327606\n",
      "Epoch 3273, Loss: 0.8627665787935257, Final Batch Loss: 0.13813598453998566\n",
      "Epoch 3274, Loss: 0.9631605744361877, Final Batch Loss: 0.2799011766910553\n",
      "Epoch 3275, Loss: 0.8784322887659073, Final Batch Loss: 0.288877934217453\n",
      "Epoch 3276, Loss: 0.8680156916379929, Final Batch Loss: 0.1858021765947342\n",
      "Epoch 3277, Loss: 0.8768672347068787, Final Batch Loss: 0.23766225576400757\n",
      "Epoch 3278, Loss: 0.9416460394859314, Final Batch Loss: 0.2165725827217102\n",
      "Epoch 3279, Loss: 0.9728171825408936, Final Batch Loss: 0.16101107001304626\n",
      "Epoch 3280, Loss: 0.8745540380477905, Final Batch Loss: 0.18420758843421936\n",
      "Epoch 3281, Loss: 0.9057421237230301, Final Batch Loss: 0.23654374480247498\n",
      "Epoch 3282, Loss: 0.9595223963260651, Final Batch Loss: 0.23166199028491974\n",
      "Epoch 3283, Loss: 0.8915242552757263, Final Batch Loss: 0.24458494782447815\n",
      "Epoch 3284, Loss: 0.9916135519742966, Final Batch Loss: 0.16830427944660187\n",
      "Epoch 3285, Loss: 0.876071110367775, Final Batch Loss: 0.1999639868736267\n",
      "Epoch 3286, Loss: 0.8976682871580124, Final Batch Loss: 0.2646452486515045\n",
      "Epoch 3287, Loss: 0.8891562819480896, Final Batch Loss: 0.2241925448179245\n",
      "Epoch 3288, Loss: 1.0293628126382828, Final Batch Loss: 0.32775259017944336\n",
      "Epoch 3289, Loss: 0.8516464531421661, Final Batch Loss: 0.1814735233783722\n",
      "Epoch 3290, Loss: 0.9910536557435989, Final Batch Loss: 0.35609740018844604\n",
      "Epoch 3291, Loss: 0.8934836834669113, Final Batch Loss: 0.24944950640201569\n",
      "Epoch 3292, Loss: 0.9306907951831818, Final Batch Loss: 0.29343530535697937\n",
      "Epoch 3293, Loss: 0.8079204708337784, Final Batch Loss: 0.14541083574295044\n",
      "Epoch 3294, Loss: 0.9173466265201569, Final Batch Loss: 0.20692291855812073\n",
      "Epoch 3295, Loss: 0.8083814382553101, Final Batch Loss: 0.19764050841331482\n",
      "Epoch 3296, Loss: 0.7990865260362625, Final Batch Loss: 0.14818881452083588\n",
      "Epoch 3297, Loss: 1.0570964366197586, Final Batch Loss: 0.36976781487464905\n",
      "Epoch 3298, Loss: 0.767317071557045, Final Batch Loss: 0.17346803843975067\n",
      "Epoch 3299, Loss: 0.8856590539216995, Final Batch Loss: 0.23268507421016693\n",
      "Epoch 3300, Loss: 0.8553610295057297, Final Batch Loss: 0.2849228084087372\n",
      "Epoch 3301, Loss: 0.943502888083458, Final Batch Loss: 0.19632452726364136\n",
      "Epoch 3302, Loss: 0.8105591088533401, Final Batch Loss: 0.22117076814174652\n",
      "Epoch 3303, Loss: 0.8188609629869461, Final Batch Loss: 0.19785955548286438\n",
      "Epoch 3304, Loss: 0.9169939309358597, Final Batch Loss: 0.3241826891899109\n",
      "Epoch 3305, Loss: 0.7985176295042038, Final Batch Loss: 0.21067079901695251\n",
      "Epoch 3306, Loss: 0.8412097245454788, Final Batch Loss: 0.19406798481941223\n",
      "Epoch 3307, Loss: 0.8990778177976608, Final Batch Loss: 0.22649115324020386\n",
      "Epoch 3308, Loss: 0.9191251546144485, Final Batch Loss: 0.25456833839416504\n",
      "Epoch 3309, Loss: 0.9156248271465302, Final Batch Loss: 0.23986279964447021\n",
      "Epoch 3310, Loss: 0.7544922679662704, Final Batch Loss: 0.19654938578605652\n",
      "Epoch 3311, Loss: 0.890996441245079, Final Batch Loss: 0.2208089679479599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3312, Loss: 0.8827892243862152, Final Batch Loss: 0.2221081405878067\n",
      "Epoch 3313, Loss: 0.9325339049100876, Final Batch Loss: 0.1819418966770172\n",
      "Epoch 3314, Loss: 0.9762844145298004, Final Batch Loss: 0.21587298810482025\n",
      "Epoch 3315, Loss: 0.9602854996919632, Final Batch Loss: 0.2558448910713196\n",
      "Epoch 3316, Loss: 0.8843145370483398, Final Batch Loss: 0.19010475277900696\n",
      "Epoch 3317, Loss: 0.8142568022012711, Final Batch Loss: 0.1871432363986969\n",
      "Epoch 3318, Loss: 0.7717925012111664, Final Batch Loss: 0.16891326010227203\n",
      "Epoch 3319, Loss: 0.9032381772994995, Final Batch Loss: 0.2528320252895355\n",
      "Epoch 3320, Loss: 0.8465839326381683, Final Batch Loss: 0.22838804125785828\n",
      "Epoch 3321, Loss: 0.7981197237968445, Final Batch Loss: 0.23389855027198792\n",
      "Epoch 3322, Loss: 0.7895005494356155, Final Batch Loss: 0.17471390962600708\n",
      "Epoch 3323, Loss: 1.0131622403860092, Final Batch Loss: 0.3693508505821228\n",
      "Epoch 3324, Loss: 0.8779487907886505, Final Batch Loss: 0.25418350100517273\n",
      "Epoch 3325, Loss: 0.9764807969331741, Final Batch Loss: 0.2706131637096405\n",
      "Epoch 3326, Loss: 0.8639434576034546, Final Batch Loss: 0.1517333984375\n",
      "Epoch 3327, Loss: 0.8428362607955933, Final Batch Loss: 0.26202192902565\n",
      "Epoch 3328, Loss: 0.892375722527504, Final Batch Loss: 0.297067254781723\n",
      "Epoch 3329, Loss: 0.9036137759685516, Final Batch Loss: 0.14040887355804443\n",
      "Epoch 3330, Loss: 0.8754302710294724, Final Batch Loss: 0.20692843198776245\n",
      "Epoch 3331, Loss: 0.8559268862009048, Final Batch Loss: 0.21181027591228485\n",
      "Epoch 3332, Loss: 0.737941786646843, Final Batch Loss: 0.1016184389591217\n",
      "Epoch 3333, Loss: 0.8518351018428802, Final Batch Loss: 0.20117996633052826\n",
      "Epoch 3334, Loss: 0.884040430188179, Final Batch Loss: 0.20489516854286194\n",
      "Epoch 3335, Loss: 0.8711373209953308, Final Batch Loss: 0.18757617473602295\n",
      "Epoch 3336, Loss: 0.9083687663078308, Final Batch Loss: 0.23279504477977753\n",
      "Epoch 3337, Loss: 0.9719288498163223, Final Batch Loss: 0.23128078877925873\n",
      "Epoch 3338, Loss: 0.768244132399559, Final Batch Loss: 0.21939776837825775\n",
      "Epoch 3339, Loss: 1.0048148930072784, Final Batch Loss: 0.32478055357933044\n",
      "Epoch 3340, Loss: 0.789095863699913, Final Batch Loss: 0.13550324738025665\n",
      "Epoch 3341, Loss: 0.9502522945404053, Final Batch Loss: 0.2597741186618805\n",
      "Epoch 3342, Loss: 0.8833699524402618, Final Batch Loss: 0.1959959864616394\n",
      "Epoch 3343, Loss: 0.7884174585342407, Final Batch Loss: 0.2164682298898697\n",
      "Epoch 3344, Loss: 0.9198070913553238, Final Batch Loss: 0.20788824558258057\n",
      "Epoch 3345, Loss: 0.9055258631706238, Final Batch Loss: 0.25804874300956726\n",
      "Epoch 3346, Loss: 0.8602880537509918, Final Batch Loss: 0.2175024002790451\n",
      "Epoch 3347, Loss: 0.8415617942810059, Final Batch Loss: 0.15962989628314972\n",
      "Epoch 3348, Loss: 0.8986894339323044, Final Batch Loss: 0.29066184163093567\n",
      "Epoch 3349, Loss: 0.799401193857193, Final Batch Loss: 0.1746639609336853\n",
      "Epoch 3350, Loss: 0.899641752243042, Final Batch Loss: 0.30662041902542114\n",
      "Epoch 3351, Loss: 0.8304275125265121, Final Batch Loss: 0.21283291280269623\n",
      "Epoch 3352, Loss: 0.8546374589204788, Final Batch Loss: 0.20568588376045227\n",
      "Epoch 3353, Loss: 0.9818010926246643, Final Batch Loss: 0.3018776774406433\n",
      "Epoch 3354, Loss: 0.8528209924697876, Final Batch Loss: 0.20568673312664032\n",
      "Epoch 3355, Loss: 0.7942707389593124, Final Batch Loss: 0.2481643408536911\n",
      "Epoch 3356, Loss: 1.027719721198082, Final Batch Loss: 0.2996487021446228\n",
      "Epoch 3357, Loss: 0.8658119142055511, Final Batch Loss: 0.23761267960071564\n",
      "Epoch 3358, Loss: 0.9613542556762695, Final Batch Loss: 0.2242286205291748\n",
      "Epoch 3359, Loss: 0.8418854475021362, Final Batch Loss: 0.14796718955039978\n",
      "Epoch 3360, Loss: 0.8685728013515472, Final Batch Loss: 0.23427923023700714\n",
      "Epoch 3361, Loss: 0.8103902563452721, Final Batch Loss: 0.11035824567079544\n",
      "Epoch 3362, Loss: 0.8840105533599854, Final Batch Loss: 0.2123241126537323\n",
      "Epoch 3363, Loss: 0.9382117241621017, Final Batch Loss: 0.2953560948371887\n",
      "Epoch 3364, Loss: 0.8676856160163879, Final Batch Loss: 0.27194127440452576\n",
      "Epoch 3365, Loss: 0.9655698984861374, Final Batch Loss: 0.37063834071159363\n",
      "Epoch 3366, Loss: 0.9823626577854156, Final Batch Loss: 0.24019195139408112\n",
      "Epoch 3367, Loss: 0.9320867657661438, Final Batch Loss: 0.19858744740486145\n",
      "Epoch 3368, Loss: 0.9494060277938843, Final Batch Loss: 0.2627182602882385\n",
      "Epoch 3369, Loss: 0.8608632981777191, Final Batch Loss: 0.2069307267665863\n",
      "Epoch 3370, Loss: 0.8019344955682755, Final Batch Loss: 0.1493428498506546\n",
      "Epoch 3371, Loss: 0.8736406117677689, Final Batch Loss: 0.19589510560035706\n",
      "Epoch 3372, Loss: 0.8327266573905945, Final Batch Loss: 0.23066970705986023\n",
      "Epoch 3373, Loss: 0.8262728452682495, Final Batch Loss: 0.24689166247844696\n",
      "Epoch 3374, Loss: 0.8936676681041718, Final Batch Loss: 0.28881531953811646\n",
      "Epoch 3375, Loss: 0.7995593547821045, Final Batch Loss: 0.20453932881355286\n",
      "Epoch 3376, Loss: 0.9412126392126083, Final Batch Loss: 0.28711873292922974\n",
      "Epoch 3377, Loss: 0.8129861801862717, Final Batch Loss: 0.15570010244846344\n",
      "Epoch 3378, Loss: 0.8677181452512741, Final Batch Loss: 0.1648997813463211\n",
      "Epoch 3379, Loss: 0.7667343467473984, Final Batch Loss: 0.14427302777767181\n",
      "Epoch 3380, Loss: 0.9764818996191025, Final Batch Loss: 0.2620042860507965\n",
      "Epoch 3381, Loss: 0.8097816407680511, Final Batch Loss: 0.17030717432498932\n",
      "Epoch 3382, Loss: 0.8866685479879379, Final Batch Loss: 0.27404844760894775\n",
      "Epoch 3383, Loss: 0.9270612001419067, Final Batch Loss: 0.23870539665222168\n",
      "Epoch 3384, Loss: 0.8082597702741623, Final Batch Loss: 0.215524822473526\n",
      "Epoch 3385, Loss: 0.8844511061906815, Final Batch Loss: 0.21022897958755493\n",
      "Epoch 3386, Loss: 0.8780671507120132, Final Batch Loss: 0.20009933412075043\n",
      "Epoch 3387, Loss: 0.8740132302045822, Final Batch Loss: 0.21490876376628876\n",
      "Epoch 3388, Loss: 0.8393436521291733, Final Batch Loss: 0.20869247615337372\n",
      "Epoch 3389, Loss: 0.8683976531028748, Final Batch Loss: 0.21362371742725372\n",
      "Epoch 3390, Loss: 0.7999860793352127, Final Batch Loss: 0.2532723546028137\n",
      "Epoch 3391, Loss: 0.9177198857069016, Final Batch Loss: 0.19954051077365875\n",
      "Epoch 3392, Loss: 0.8386946320533752, Final Batch Loss: 0.20080579817295074\n",
      "Epoch 3393, Loss: 0.9538418799638748, Final Batch Loss: 0.24124391376972198\n",
      "Epoch 3394, Loss: 0.938120499253273, Final Batch Loss: 0.19553320109844208\n",
      "Epoch 3395, Loss: 0.8654201179742813, Final Batch Loss: 0.14082039892673492\n",
      "Epoch 3396, Loss: 0.8132198303937912, Final Batch Loss: 0.1545189619064331\n",
      "Epoch 3397, Loss: 0.8721197545528412, Final Batch Loss: 0.20984919369220734\n",
      "Epoch 3398, Loss: 0.9483500272035599, Final Batch Loss: 0.2932206392288208\n",
      "Epoch 3399, Loss: 0.8058568686246872, Final Batch Loss: 0.20377907156944275\n",
      "Epoch 3400, Loss: 0.9135806858539581, Final Batch Loss: 0.24844741821289062\n",
      "Epoch 3401, Loss: 0.778576210141182, Final Batch Loss: 0.2010040283203125\n",
      "Epoch 3402, Loss: 0.8519487828016281, Final Batch Loss: 0.19395771622657776\n",
      "Epoch 3403, Loss: 0.823366567492485, Final Batch Loss: 0.21762533485889435\n",
      "Epoch 3404, Loss: 0.871928259730339, Final Batch Loss: 0.20953942835330963\n",
      "Epoch 3405, Loss: 0.9961971491575241, Final Batch Loss: 0.19606666266918182\n",
      "Epoch 3406, Loss: 0.9002644568681717, Final Batch Loss: 0.24448585510253906\n",
      "Epoch 3407, Loss: 0.8432864546775818, Final Batch Loss: 0.20168179273605347\n",
      "Epoch 3408, Loss: 0.9163806885480881, Final Batch Loss: 0.2572791576385498\n",
      "Epoch 3409, Loss: 0.8569513410329819, Final Batch Loss: 0.23734043538570404\n",
      "Epoch 3410, Loss: 0.8108777850866318, Final Batch Loss: 0.16499072313308716\n",
      "Epoch 3411, Loss: 0.9601845592260361, Final Batch Loss: 0.2502180337905884\n",
      "Epoch 3412, Loss: 0.8547932356595993, Final Batch Loss: 0.16612063348293304\n",
      "Epoch 3413, Loss: 1.0444722175598145, Final Batch Loss: 0.2709088623523712\n",
      "Epoch 3414, Loss: 0.8784012198448181, Final Batch Loss: 0.22390761971473694\n",
      "Epoch 3415, Loss: 0.9127528667449951, Final Batch Loss: 0.18709419667720795\n",
      "Epoch 3416, Loss: 0.8555367738008499, Final Batch Loss: 0.17511984705924988\n",
      "Epoch 3417, Loss: 0.8691433072090149, Final Batch Loss: 0.1877332329750061\n",
      "Epoch 3418, Loss: 0.851333498954773, Final Batch Loss: 0.1262885332107544\n",
      "Epoch 3419, Loss: 0.7965948432683945, Final Batch Loss: 0.21739453077316284\n",
      "Epoch 3420, Loss: 0.8318235278129578, Final Batch Loss: 0.20003165304660797\n",
      "Epoch 3421, Loss: 0.8940238505601883, Final Batch Loss: 0.21050141751766205\n",
      "Epoch 3422, Loss: 0.8269753158092499, Final Batch Loss: 0.23429694771766663\n",
      "Epoch 3423, Loss: 0.9147318303585052, Final Batch Loss: 0.23176075518131256\n",
      "Epoch 3424, Loss: 0.7617640048265457, Final Batch Loss: 0.1981332153081894\n",
      "Epoch 3425, Loss: 0.7827954441308975, Final Batch Loss: 0.1631801575422287\n",
      "Epoch 3426, Loss: 0.7739562839269638, Final Batch Loss: 0.19529135525226593\n",
      "Epoch 3427, Loss: 0.7775570005178452, Final Batch Loss: 0.212681382894516\n",
      "Epoch 3428, Loss: 0.9161932319402695, Final Batch Loss: 0.21660351753234863\n",
      "Epoch 3429, Loss: 0.9153603166341782, Final Batch Loss: 0.2713804244995117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3430, Loss: 0.8675270080566406, Final Batch Loss: 0.26067110896110535\n",
      "Epoch 3431, Loss: 0.9047722518444061, Final Batch Loss: 0.2828562557697296\n",
      "Epoch 3432, Loss: 0.9588387757539749, Final Batch Loss: 0.3023928701877594\n",
      "Epoch 3433, Loss: 1.0864993631839752, Final Batch Loss: 0.26413851976394653\n",
      "Epoch 3434, Loss: 0.8914002776145935, Final Batch Loss: 0.16598904132843018\n",
      "Epoch 3435, Loss: 0.7932939380407333, Final Batch Loss: 0.1920429766178131\n",
      "Epoch 3436, Loss: 0.8551091998815536, Final Batch Loss: 0.1944247931241989\n",
      "Epoch 3437, Loss: 0.8274282515048981, Final Batch Loss: 0.16757741570472717\n",
      "Epoch 3438, Loss: 0.8996100127696991, Final Batch Loss: 0.26577192544937134\n",
      "Epoch 3439, Loss: 0.9214640408754349, Final Batch Loss: 0.19529563188552856\n",
      "Epoch 3440, Loss: 0.9100984185934067, Final Batch Loss: 0.3106110990047455\n",
      "Epoch 3441, Loss: 0.9264786541461945, Final Batch Loss: 0.2781352400779724\n",
      "Epoch 3442, Loss: 0.7644638419151306, Final Batch Loss: 0.19262102246284485\n",
      "Epoch 3443, Loss: 0.8022875636816025, Final Batch Loss: 0.16754736006259918\n",
      "Epoch 3444, Loss: 0.8633186519145966, Final Batch Loss: 0.18881377577781677\n",
      "Epoch 3445, Loss: 0.9047361761331558, Final Batch Loss: 0.20293809473514557\n",
      "Epoch 3446, Loss: 0.9614438265562057, Final Batch Loss: 0.25655972957611084\n",
      "Epoch 3447, Loss: 0.9587205946445465, Final Batch Loss: 0.25037774443626404\n",
      "Epoch 3448, Loss: 0.9459113329648972, Final Batch Loss: 0.26056116819381714\n",
      "Epoch 3449, Loss: 0.897928923368454, Final Batch Loss: 0.27679187059402466\n",
      "Epoch 3450, Loss: 0.8640641272068024, Final Batch Loss: 0.2188417613506317\n",
      "Epoch 3451, Loss: 0.8011529296636581, Final Batch Loss: 0.2248471975326538\n",
      "Epoch 3452, Loss: 0.9360818862915039, Final Batch Loss: 0.3301221430301666\n",
      "Epoch 3453, Loss: 0.9081468731164932, Final Batch Loss: 0.24824711680412292\n",
      "Epoch 3454, Loss: 0.8672226965427399, Final Batch Loss: 0.25254684686660767\n",
      "Epoch 3455, Loss: 0.9506488293409348, Final Batch Loss: 0.24880750477313995\n",
      "Epoch 3456, Loss: 0.9429555833339691, Final Batch Loss: 0.2498755306005478\n",
      "Epoch 3457, Loss: 0.8762791454792023, Final Batch Loss: 0.18643905222415924\n",
      "Epoch 3458, Loss: 0.8346519768238068, Final Batch Loss: 0.15116918087005615\n",
      "Epoch 3459, Loss: 0.933526024222374, Final Batch Loss: 0.26179173588752747\n",
      "Epoch 3460, Loss: 0.8628588318824768, Final Batch Loss: 0.2319600135087967\n",
      "Epoch 3461, Loss: 0.8906300067901611, Final Batch Loss: 0.2062986046075821\n",
      "Epoch 3462, Loss: 0.7969585359096527, Final Batch Loss: 0.20850935578346252\n",
      "Epoch 3463, Loss: 0.8306625336408615, Final Batch Loss: 0.20715990662574768\n",
      "Epoch 3464, Loss: 0.8319224864244461, Final Batch Loss: 0.21790729463100433\n",
      "Epoch 3465, Loss: 0.8587746471166611, Final Batch Loss: 0.19473691284656525\n",
      "Epoch 3466, Loss: 0.9730543047189713, Final Batch Loss: 0.2667402923107147\n",
      "Epoch 3467, Loss: 0.8509885668754578, Final Batch Loss: 0.22245146334171295\n",
      "Epoch 3468, Loss: 0.7644160985946655, Final Batch Loss: 0.19432127475738525\n",
      "Epoch 3469, Loss: 0.8751995861530304, Final Batch Loss: 0.23030129075050354\n",
      "Epoch 3470, Loss: 0.8624727129936218, Final Batch Loss: 0.19805526733398438\n",
      "Epoch 3471, Loss: 0.8874110579490662, Final Batch Loss: 0.296446830034256\n",
      "Epoch 3472, Loss: 0.9350107163190842, Final Batch Loss: 0.21491046249866486\n",
      "Epoch 3473, Loss: 0.8417444676160812, Final Batch Loss: 0.2260829359292984\n",
      "Epoch 3474, Loss: 0.8553245812654495, Final Batch Loss: 0.26698923110961914\n",
      "Epoch 3475, Loss: 0.7405080497264862, Final Batch Loss: 0.1708693653345108\n",
      "Epoch 3476, Loss: 0.7721354365348816, Final Batch Loss: 0.11163435876369476\n",
      "Epoch 3477, Loss: 0.7406946793198586, Final Batch Loss: 0.11447707563638687\n",
      "Epoch 3478, Loss: 1.020917296409607, Final Batch Loss: 0.29143643379211426\n",
      "Epoch 3479, Loss: 0.891062393784523, Final Batch Loss: 0.16936814785003662\n",
      "Epoch 3480, Loss: 0.9603153765201569, Final Batch Loss: 0.25108665227890015\n",
      "Epoch 3481, Loss: 0.8428023904561996, Final Batch Loss: 0.18042446672916412\n",
      "Epoch 3482, Loss: 0.839920163154602, Final Batch Loss: 0.19501890242099762\n",
      "Epoch 3483, Loss: 0.8886807709932327, Final Batch Loss: 0.23056170344352722\n",
      "Epoch 3484, Loss: 0.8481632322072983, Final Batch Loss: 0.25026804208755493\n",
      "Epoch 3485, Loss: 0.9759268313646317, Final Batch Loss: 0.21827954053878784\n",
      "Epoch 3486, Loss: 0.8925870805978775, Final Batch Loss: 0.22835806012153625\n",
      "Epoch 3487, Loss: 0.8363008052110672, Final Batch Loss: 0.19782724976539612\n",
      "Epoch 3488, Loss: 0.8217316567897797, Final Batch Loss: 0.16408197581768036\n",
      "Epoch 3489, Loss: 0.8274652510881424, Final Batch Loss: 0.18481308221817017\n",
      "Epoch 3490, Loss: 0.8202021718025208, Final Batch Loss: 0.20433078706264496\n",
      "Epoch 3491, Loss: 0.8584093898534775, Final Batch Loss: 0.21031427383422852\n",
      "Epoch 3492, Loss: 0.8833404779434204, Final Batch Loss: 0.21956877410411835\n",
      "Epoch 3493, Loss: 0.8591941595077515, Final Batch Loss: 0.17274513840675354\n",
      "Epoch 3494, Loss: 0.8822893649339676, Final Batch Loss: 0.289662629365921\n",
      "Epoch 3495, Loss: 0.8680075854063034, Final Batch Loss: 0.21734721958637238\n",
      "Epoch 3496, Loss: 0.8219582438468933, Final Batch Loss: 0.22403138875961304\n",
      "Epoch 3497, Loss: 0.8583784997463226, Final Batch Loss: 0.18585573136806488\n",
      "Epoch 3498, Loss: 0.8329936414957047, Final Batch Loss: 0.19787397980690002\n",
      "Epoch 3499, Loss: 0.8412622809410095, Final Batch Loss: 0.2539040148258209\n",
      "Epoch 3500, Loss: 0.9089554399251938, Final Batch Loss: 0.1700240522623062\n",
      "Epoch 3501, Loss: 0.877204105257988, Final Batch Loss: 0.2612721025943756\n",
      "Epoch 3502, Loss: 0.9204198122024536, Final Batch Loss: 0.27207502722740173\n",
      "Epoch 3503, Loss: 0.9258729666471481, Final Batch Loss: 0.2473580241203308\n",
      "Epoch 3504, Loss: 0.8590863794088364, Final Batch Loss: 0.21124112606048584\n",
      "Epoch 3505, Loss: 0.8349079191684723, Final Batch Loss: 0.2215702086687088\n",
      "Epoch 3506, Loss: 0.8111309856176376, Final Batch Loss: 0.2286989986896515\n",
      "Epoch 3507, Loss: 0.8082238435745239, Final Batch Loss: 0.19871288537979126\n",
      "Epoch 3508, Loss: 0.7769167199730873, Final Batch Loss: 0.12221547216176987\n",
      "Epoch 3509, Loss: 0.8953743726015091, Final Batch Loss: 0.2639393210411072\n",
      "Epoch 3510, Loss: 0.9086285680532455, Final Batch Loss: 0.23971951007843018\n",
      "Epoch 3511, Loss: 0.9077743142843246, Final Batch Loss: 0.19194680452346802\n",
      "Epoch 3512, Loss: 0.8758405596017838, Final Batch Loss: 0.24575406312942505\n",
      "Epoch 3513, Loss: 0.9145805090665817, Final Batch Loss: 0.26179584860801697\n",
      "Epoch 3514, Loss: 0.8876840174198151, Final Batch Loss: 0.2809934914112091\n",
      "Epoch 3515, Loss: 0.7921116948127747, Final Batch Loss: 0.17979751527309418\n",
      "Epoch 3516, Loss: 0.8706022799015045, Final Batch Loss: 0.20931857824325562\n",
      "Epoch 3517, Loss: 0.7795346677303314, Final Batch Loss: 0.1654960662126541\n",
      "Epoch 3518, Loss: 0.9816841185092926, Final Batch Loss: 0.2936941981315613\n",
      "Epoch 3519, Loss: 0.8299117088317871, Final Batch Loss: 0.20689408481121063\n",
      "Epoch 3520, Loss: 0.9833201169967651, Final Batch Loss: 0.26485154032707214\n",
      "Epoch 3521, Loss: 0.8323968350887299, Final Batch Loss: 0.22390833497047424\n",
      "Epoch 3522, Loss: 0.7758069932460785, Final Batch Loss: 0.22410425543785095\n",
      "Epoch 3523, Loss: 0.9325035363435745, Final Batch Loss: 0.22404760122299194\n",
      "Epoch 3524, Loss: 0.9461265504360199, Final Batch Loss: 0.26705363392829895\n",
      "Epoch 3525, Loss: 0.8662898391485214, Final Batch Loss: 0.18976759910583496\n",
      "Epoch 3526, Loss: 0.9008655101060867, Final Batch Loss: 0.29119500517845154\n",
      "Epoch 3527, Loss: 0.7652297168970108, Final Batch Loss: 0.19878681004047394\n",
      "Epoch 3528, Loss: 0.8231246769428253, Final Batch Loss: 0.20392291247844696\n",
      "Epoch 3529, Loss: 0.8619634360074997, Final Batch Loss: 0.17297685146331787\n",
      "Epoch 3530, Loss: 0.9601920396089554, Final Batch Loss: 0.39443525671958923\n",
      "Epoch 3531, Loss: 0.8331809192895889, Final Batch Loss: 0.1649475246667862\n",
      "Epoch 3532, Loss: 0.9205042719841003, Final Batch Loss: 0.2940656244754791\n",
      "Epoch 3533, Loss: 0.8644872605800629, Final Batch Loss: 0.20962442457675934\n",
      "Epoch 3534, Loss: 0.8949267566204071, Final Batch Loss: 0.2714255154132843\n",
      "Epoch 3535, Loss: 0.9939786791801453, Final Batch Loss: 0.2467428296804428\n",
      "Epoch 3536, Loss: 0.8253863006830215, Final Batch Loss: 0.2523750066757202\n",
      "Epoch 3537, Loss: 0.8314112573862076, Final Batch Loss: 0.16608667373657227\n",
      "Epoch 3538, Loss: 0.9199106842279434, Final Batch Loss: 0.17476129531860352\n",
      "Epoch 3539, Loss: 0.8121821880340576, Final Batch Loss: 0.17593632638454437\n",
      "Epoch 3540, Loss: 0.8359362930059433, Final Batch Loss: 0.21931055188179016\n",
      "Epoch 3541, Loss: 1.0174438655376434, Final Batch Loss: 0.3332509696483612\n",
      "Epoch 3542, Loss: 0.7569170147180557, Final Batch Loss: 0.2098720520734787\n",
      "Epoch 3543, Loss: 0.9170574843883514, Final Batch Loss: 0.2776538133621216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3544, Loss: 0.8633039146661758, Final Batch Loss: 0.24733562767505646\n",
      "Epoch 3545, Loss: 0.8593568354845047, Final Batch Loss: 0.18108543753623962\n",
      "Epoch 3546, Loss: 0.9682154655456543, Final Batch Loss: 0.27566036581993103\n",
      "Epoch 3547, Loss: 0.8655034601688385, Final Batch Loss: 0.21717746555805206\n",
      "Epoch 3548, Loss: 0.8834343701601028, Final Batch Loss: 0.17850351333618164\n",
      "Epoch 3549, Loss: 0.8677487671375275, Final Batch Loss: 0.21774698793888092\n",
      "Epoch 3550, Loss: 0.9615660607814789, Final Batch Loss: 0.2755320966243744\n",
      "Epoch 3551, Loss: 0.8708124607801437, Final Batch Loss: 0.16007782518863678\n",
      "Epoch 3552, Loss: 0.8703029751777649, Final Batch Loss: 0.2574625015258789\n",
      "Epoch 3553, Loss: 0.8232491314411163, Final Batch Loss: 0.20101334154605865\n",
      "Epoch 3554, Loss: 0.856653556227684, Final Batch Loss: 0.196630597114563\n",
      "Epoch 3555, Loss: 0.8243339657783508, Final Batch Loss: 0.16729506850242615\n",
      "Epoch 3556, Loss: 0.8336522728204727, Final Batch Loss: 0.21727077662944794\n",
      "Epoch 3557, Loss: 0.9151683002710342, Final Batch Loss: 0.2469128519296646\n",
      "Epoch 3558, Loss: 0.8258334547281265, Final Batch Loss: 0.15150851011276245\n",
      "Epoch 3559, Loss: 0.8607135862112045, Final Batch Loss: 0.20744529366493225\n",
      "Epoch 3560, Loss: 0.8747295886278152, Final Batch Loss: 0.18766120076179504\n",
      "Epoch 3561, Loss: 0.8280057907104492, Final Batch Loss: 0.20338237285614014\n",
      "Epoch 3562, Loss: 0.930765762925148, Final Batch Loss: 0.2644166648387909\n",
      "Epoch 3563, Loss: 0.8536133319139481, Final Batch Loss: 0.1353987157344818\n",
      "Epoch 3564, Loss: 0.9107893109321594, Final Batch Loss: 0.24954648315906525\n",
      "Epoch 3565, Loss: 0.9510597288608551, Final Batch Loss: 0.23503653705120087\n",
      "Epoch 3566, Loss: 0.7455427944660187, Final Batch Loss: 0.12841227650642395\n",
      "Epoch 3567, Loss: 0.9050458669662476, Final Batch Loss: 0.2844645380973816\n",
      "Epoch 3568, Loss: 0.8640812337398529, Final Batch Loss: 0.22683174908161163\n",
      "Epoch 3569, Loss: 0.9635867923498154, Final Batch Loss: 0.21432270109653473\n",
      "Epoch 3570, Loss: 0.8699698895215988, Final Batch Loss: 0.16288980841636658\n",
      "Epoch 3571, Loss: 0.9168023020029068, Final Batch Loss: 0.16367901861667633\n",
      "Epoch 3572, Loss: 0.9035423099994659, Final Batch Loss: 0.29097431898117065\n",
      "Epoch 3573, Loss: 0.850237175822258, Final Batch Loss: 0.26132887601852417\n",
      "Epoch 3574, Loss: 0.8434123694896698, Final Batch Loss: 0.15129144489765167\n",
      "Epoch 3575, Loss: 0.7856264412403107, Final Batch Loss: 0.17565274238586426\n",
      "Epoch 3576, Loss: 0.8388043642044067, Final Batch Loss: 0.1872136890888214\n",
      "Epoch 3577, Loss: 0.811479851603508, Final Batch Loss: 0.21797162294387817\n",
      "Epoch 3578, Loss: 0.8429383784532547, Final Batch Loss: 0.19338324666023254\n",
      "Epoch 3579, Loss: 0.9681994467973709, Final Batch Loss: 0.2493860125541687\n",
      "Epoch 3580, Loss: 0.799357682466507, Final Batch Loss: 0.20735183358192444\n",
      "Epoch 3581, Loss: 0.9558701664209366, Final Batch Loss: 0.2702970802783966\n",
      "Epoch 3582, Loss: 0.8349183350801468, Final Batch Loss: 0.16499339044094086\n",
      "Epoch 3583, Loss: 0.8427807539701462, Final Batch Loss: 0.20772850513458252\n",
      "Epoch 3584, Loss: 0.91105917096138, Final Batch Loss: 0.2693707048892975\n",
      "Epoch 3585, Loss: 0.8725566864013672, Final Batch Loss: 0.15204809606075287\n",
      "Epoch 3586, Loss: 0.8340020179748535, Final Batch Loss: 0.1633949875831604\n",
      "Epoch 3587, Loss: 0.8722939342260361, Final Batch Loss: 0.15678934752941132\n",
      "Epoch 3588, Loss: 0.8602950572967529, Final Batch Loss: 0.23183350265026093\n",
      "Epoch 3589, Loss: 0.9328588098287582, Final Batch Loss: 0.1769702434539795\n",
      "Epoch 3590, Loss: 0.7333585545420647, Final Batch Loss: 0.07653873413801193\n",
      "Epoch 3591, Loss: 0.9067566096782684, Final Batch Loss: 0.29085415601730347\n",
      "Epoch 3592, Loss: 0.8677363991737366, Final Batch Loss: 0.20328399538993835\n",
      "Epoch 3593, Loss: 0.8060881048440933, Final Batch Loss: 0.16795817017555237\n",
      "Epoch 3594, Loss: 0.8968102931976318, Final Batch Loss: 0.20155981183052063\n",
      "Epoch 3595, Loss: 0.8123142421245575, Final Batch Loss: 0.17459292709827423\n",
      "Epoch 3596, Loss: 0.8822196871042252, Final Batch Loss: 0.20202572643756866\n",
      "Epoch 3597, Loss: 0.9580198228359222, Final Batch Loss: 0.2988549470901489\n",
      "Epoch 3598, Loss: 0.8955722153186798, Final Batch Loss: 0.1835111379623413\n",
      "Epoch 3599, Loss: 0.9059384018182755, Final Batch Loss: 0.27773869037628174\n",
      "Epoch 3600, Loss: 0.8836636692285538, Final Batch Loss: 0.18486608564853668\n",
      "Epoch 3601, Loss: 0.9822118133306503, Final Batch Loss: 0.37320446968078613\n",
      "Epoch 3602, Loss: 0.8558632135391235, Final Batch Loss: 0.19748274981975555\n",
      "Epoch 3603, Loss: 0.9425143301486969, Final Batch Loss: 0.2657906413078308\n",
      "Epoch 3604, Loss: 0.9155715554952621, Final Batch Loss: 0.3139864504337311\n",
      "Epoch 3605, Loss: 0.8558907955884933, Final Batch Loss: 0.19894306361675262\n",
      "Epoch 3606, Loss: 0.8759885728359222, Final Batch Loss: 0.18204399943351746\n",
      "Epoch 3607, Loss: 0.7987322807312012, Final Batch Loss: 0.17876951396465302\n",
      "Epoch 3608, Loss: 0.8547778725624084, Final Batch Loss: 0.2035602331161499\n",
      "Epoch 3609, Loss: 0.8126614987850189, Final Batch Loss: 0.19356988370418549\n",
      "Epoch 3610, Loss: 0.8248788416385651, Final Batch Loss: 0.17247220873832703\n",
      "Epoch 3611, Loss: 0.8818569481372833, Final Batch Loss: 0.2731453478336334\n",
      "Epoch 3612, Loss: 0.7531554400920868, Final Batch Loss: 0.18275198340415955\n",
      "Epoch 3613, Loss: 0.7855039685964584, Final Batch Loss: 0.18360641598701477\n",
      "Epoch 3614, Loss: 0.8956415504217148, Final Batch Loss: 0.2701338827610016\n",
      "Epoch 3615, Loss: 0.8207402229309082, Final Batch Loss: 0.24846337735652924\n",
      "Epoch 3616, Loss: 0.8756484985351562, Final Batch Loss: 0.19677037000656128\n",
      "Epoch 3617, Loss: 0.8427572697401047, Final Batch Loss: 0.1886584460735321\n",
      "Epoch 3618, Loss: 0.8705026805400848, Final Batch Loss: 0.2242332398891449\n",
      "Epoch 3619, Loss: 0.959516704082489, Final Batch Loss: 0.28185659646987915\n",
      "Epoch 3620, Loss: 0.7991481870412827, Final Batch Loss: 0.21500816941261292\n",
      "Epoch 3621, Loss: 0.860236868262291, Final Batch Loss: 0.18937832117080688\n",
      "Epoch 3622, Loss: 0.8013866245746613, Final Batch Loss: 0.28616875410079956\n",
      "Epoch 3623, Loss: 0.8749110698699951, Final Batch Loss: 0.1459950953722\n",
      "Epoch 3624, Loss: 0.9328547567129135, Final Batch Loss: 0.1854885369539261\n",
      "Epoch 3625, Loss: 0.7792407423257828, Final Batch Loss: 0.21553446352481842\n",
      "Epoch 3626, Loss: 0.803008496761322, Final Batch Loss: 0.1472579687833786\n",
      "Epoch 3627, Loss: 0.735915020108223, Final Batch Loss: 0.17396067082881927\n",
      "Epoch 3628, Loss: 0.9498709738254547, Final Batch Loss: 0.28220340609550476\n",
      "Epoch 3629, Loss: 0.797135666012764, Final Batch Loss: 0.18378837406635284\n",
      "Epoch 3630, Loss: 0.7051765620708466, Final Batch Loss: 0.17844244837760925\n",
      "Epoch 3631, Loss: 0.8288186490535736, Final Batch Loss: 0.13073888421058655\n",
      "Epoch 3632, Loss: 0.742800161242485, Final Batch Loss: 0.16400663554668427\n",
      "Epoch 3633, Loss: 0.8259105682373047, Final Batch Loss: 0.18222570419311523\n",
      "Epoch 3634, Loss: 0.9219536930322647, Final Batch Loss: 0.284942090511322\n",
      "Epoch 3635, Loss: 0.9104906767606735, Final Batch Loss: 0.28798091411590576\n",
      "Epoch 3636, Loss: 0.8966781198978424, Final Batch Loss: 0.25152915716171265\n",
      "Epoch 3637, Loss: 0.9687387198209763, Final Batch Loss: 0.22116553783416748\n",
      "Epoch 3638, Loss: 0.8867224901914597, Final Batch Loss: 0.15933950245380402\n",
      "Epoch 3639, Loss: 0.9801898151636124, Final Batch Loss: 0.1567133069038391\n",
      "Epoch 3640, Loss: 0.7607990354299545, Final Batch Loss: 0.19487829506397247\n",
      "Epoch 3641, Loss: 0.8664306402206421, Final Batch Loss: 0.20559942722320557\n",
      "Epoch 3642, Loss: 0.8801359534263611, Final Batch Loss: 0.24971430003643036\n",
      "Epoch 3643, Loss: 0.7923535853624344, Final Batch Loss: 0.1672033816576004\n",
      "Epoch 3644, Loss: 0.888177752494812, Final Batch Loss: 0.18425233662128448\n",
      "Epoch 3645, Loss: 0.8229336738586426, Final Batch Loss: 0.27782419323921204\n",
      "Epoch 3646, Loss: 0.8160980045795441, Final Batch Loss: 0.24912172555923462\n",
      "Epoch 3647, Loss: 0.7473271936178207, Final Batch Loss: 0.16443243622779846\n",
      "Epoch 3648, Loss: 0.8344992697238922, Final Batch Loss: 0.24915467202663422\n",
      "Epoch 3649, Loss: 0.7965594232082367, Final Batch Loss: 0.22160688042640686\n",
      "Epoch 3650, Loss: 0.8151719570159912, Final Batch Loss: 0.21663115918636322\n",
      "Epoch 3651, Loss: 0.8425964564085007, Final Batch Loss: 0.25341907143592834\n",
      "Epoch 3652, Loss: 0.9096227139234543, Final Batch Loss: 0.2998518645763397\n",
      "Epoch 3653, Loss: 0.9295523166656494, Final Batch Loss: 0.3066772520542145\n",
      "Epoch 3654, Loss: 0.854637622833252, Final Batch Loss: 0.15913130342960358\n",
      "Epoch 3655, Loss: 0.7806873768568039, Final Batch Loss: 0.21142731606960297\n",
      "Epoch 3656, Loss: 0.8612584322690964, Final Batch Loss: 0.32145965099334717\n",
      "Epoch 3657, Loss: 0.8834030032157898, Final Batch Loss: 0.27109137177467346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3658, Loss: 0.9523391127586365, Final Batch Loss: 0.3515715003013611\n",
      "Epoch 3659, Loss: 0.8558930307626724, Final Batch Loss: 0.19012145698070526\n",
      "Epoch 3660, Loss: 0.8438646644353867, Final Batch Loss: 0.206222802400589\n",
      "Epoch 3661, Loss: 0.8588914275169373, Final Batch Loss: 0.24512025713920593\n",
      "Epoch 3662, Loss: 0.8238535523414612, Final Batch Loss: 0.2354126274585724\n",
      "Epoch 3663, Loss: 0.8427722007036209, Final Batch Loss: 0.17165647447109222\n",
      "Epoch 3664, Loss: 0.8225376904010773, Final Batch Loss: 0.164991095662117\n",
      "Epoch 3665, Loss: 0.8586521446704865, Final Batch Loss: 0.2174040675163269\n",
      "Epoch 3666, Loss: 0.9005732089281082, Final Batch Loss: 0.2654038965702057\n",
      "Epoch 3667, Loss: 0.873676672577858, Final Batch Loss: 0.21757477521896362\n",
      "Epoch 3668, Loss: 0.8395347744226456, Final Batch Loss: 0.1447208821773529\n",
      "Epoch 3669, Loss: 0.8800843507051468, Final Batch Loss: 0.18139863014221191\n",
      "Epoch 3670, Loss: 0.8256434202194214, Final Batch Loss: 0.1794939935207367\n",
      "Epoch 3671, Loss: 0.8792054727673531, Final Batch Loss: 0.11599411815404892\n",
      "Epoch 3672, Loss: 0.8910372853279114, Final Batch Loss: 0.2047763615846634\n",
      "Epoch 3673, Loss: 0.767333522439003, Final Batch Loss: 0.14488111436367035\n",
      "Epoch 3674, Loss: 0.9032168239355087, Final Batch Loss: 0.25043752789497375\n",
      "Epoch 3675, Loss: 0.842060461640358, Final Batch Loss: 0.2123163789510727\n",
      "Epoch 3676, Loss: 0.7993974387645721, Final Batch Loss: 0.1589375138282776\n",
      "Epoch 3677, Loss: 0.8073747605085373, Final Batch Loss: 0.16712410748004913\n",
      "Epoch 3678, Loss: 0.8803243339061737, Final Batch Loss: 0.22845451533794403\n",
      "Epoch 3679, Loss: 0.8993176966905594, Final Batch Loss: 0.20240704715251923\n",
      "Epoch 3680, Loss: 0.84687839448452, Final Batch Loss: 0.15193118155002594\n",
      "Epoch 3681, Loss: 0.899843767285347, Final Batch Loss: 0.22257797420024872\n",
      "Epoch 3682, Loss: 0.9878144562244415, Final Batch Loss: 0.27222397923469543\n",
      "Epoch 3683, Loss: 0.8490615338087082, Final Batch Loss: 0.18703773617744446\n",
      "Epoch 3684, Loss: 0.8780454248189926, Final Batch Loss: 0.2824268639087677\n",
      "Epoch 3685, Loss: 0.6626691371202469, Final Batch Loss: 0.1400631070137024\n",
      "Epoch 3686, Loss: 0.8406065851449966, Final Batch Loss: 0.24374143779277802\n",
      "Epoch 3687, Loss: 0.8475096374750137, Final Batch Loss: 0.2932737171649933\n",
      "Epoch 3688, Loss: 0.7739790081977844, Final Batch Loss: 0.17487768828868866\n",
      "Epoch 3689, Loss: 0.8918096423149109, Final Batch Loss: 0.29486986994743347\n",
      "Epoch 3690, Loss: 0.8862840384244919, Final Batch Loss: 0.23625287413597107\n",
      "Epoch 3691, Loss: 0.793900690972805, Final Batch Loss: 0.18913258612155914\n",
      "Epoch 3692, Loss: 0.838612973690033, Final Batch Loss: 0.17249225080013275\n",
      "Epoch 3693, Loss: 0.9053047001361847, Final Batch Loss: 0.2892989218235016\n",
      "Epoch 3694, Loss: 0.8681343346834183, Final Batch Loss: 0.2686860263347626\n",
      "Epoch 3695, Loss: 0.9434555023908615, Final Batch Loss: 0.3431398272514343\n",
      "Epoch 3696, Loss: 0.8326941877603531, Final Batch Loss: 0.2890535295009613\n",
      "Epoch 3697, Loss: 0.825798898935318, Final Batch Loss: 0.2118908017873764\n",
      "Epoch 3698, Loss: 0.9898402541875839, Final Batch Loss: 0.29069623351097107\n",
      "Epoch 3699, Loss: 0.8599791675806046, Final Batch Loss: 0.24755141139030457\n",
      "Epoch 3700, Loss: 0.925363302230835, Final Batch Loss: 0.2561725080013275\n",
      "Epoch 3701, Loss: 0.8143248707056046, Final Batch Loss: 0.16045831143856049\n",
      "Epoch 3702, Loss: 0.8907849937677383, Final Batch Loss: 0.25315356254577637\n",
      "Epoch 3703, Loss: 0.9579391330480576, Final Batch Loss: 0.2681165337562561\n",
      "Epoch 3704, Loss: 0.7453746646642685, Final Batch Loss: 0.13208810985088348\n",
      "Epoch 3705, Loss: 0.7661099880933762, Final Batch Loss: 0.1472802609205246\n",
      "Epoch 3706, Loss: 0.7606099992990494, Final Batch Loss: 0.2274499386548996\n",
      "Epoch 3707, Loss: 0.8398188054561615, Final Batch Loss: 0.25307244062423706\n",
      "Epoch 3708, Loss: 0.8102503418922424, Final Batch Loss: 0.27426695823669434\n",
      "Epoch 3709, Loss: 0.7992645800113678, Final Batch Loss: 0.1754326969385147\n",
      "Epoch 3710, Loss: 0.907034620642662, Final Batch Loss: 0.30623945593833923\n",
      "Epoch 3711, Loss: 0.7769513428211212, Final Batch Loss: 0.18030738830566406\n",
      "Epoch 3712, Loss: 0.8561850786209106, Final Batch Loss: 0.18478307127952576\n",
      "Epoch 3713, Loss: 1.004680559039116, Final Batch Loss: 0.28953298926353455\n",
      "Epoch 3714, Loss: 0.9030829221010208, Final Batch Loss: 0.1988537311553955\n",
      "Epoch 3715, Loss: 0.8708354234695435, Final Batch Loss: 0.19306747615337372\n",
      "Epoch 3716, Loss: 0.747135803103447, Final Batch Loss: 0.0658918172121048\n",
      "Epoch 3717, Loss: 0.8735630661249161, Final Batch Loss: 0.2414797693490982\n",
      "Epoch 3718, Loss: 0.846451073884964, Final Batch Loss: 0.18335214257240295\n",
      "Epoch 3719, Loss: 0.9609076976776123, Final Batch Loss: 0.2791946232318878\n",
      "Epoch 3720, Loss: 0.8158509731292725, Final Batch Loss: 0.1706259846687317\n",
      "Epoch 3721, Loss: 1.0086174756288528, Final Batch Loss: 0.18958818912506104\n",
      "Epoch 3722, Loss: 0.916398286819458, Final Batch Loss: 0.17979466915130615\n",
      "Epoch 3723, Loss: 0.8695523142814636, Final Batch Loss: 0.2410195916891098\n",
      "Epoch 3724, Loss: 0.9511301070451736, Final Batch Loss: 0.30531081557273865\n",
      "Epoch 3725, Loss: 1.0058941692113876, Final Batch Loss: 0.3344082534313202\n",
      "Epoch 3726, Loss: 0.7826714515686035, Final Batch Loss: 0.12605760991573334\n",
      "Epoch 3727, Loss: 0.8592531234025955, Final Batch Loss: 0.15331730246543884\n",
      "Epoch 3728, Loss: 0.8015573024749756, Final Batch Loss: 0.11564859747886658\n",
      "Epoch 3729, Loss: 0.8474840968847275, Final Batch Loss: 0.17904597520828247\n",
      "Epoch 3730, Loss: 0.8417466133832932, Final Batch Loss: 0.25928303599357605\n",
      "Epoch 3731, Loss: 0.7580203413963318, Final Batch Loss: 0.14889433979988098\n",
      "Epoch 3732, Loss: 0.9276321679353714, Final Batch Loss: 0.22460095584392548\n",
      "Epoch 3733, Loss: 0.8920840471982956, Final Batch Loss: 0.21799330413341522\n",
      "Epoch 3734, Loss: 0.8334317654371262, Final Batch Loss: 0.19455914199352264\n",
      "Epoch 3735, Loss: 0.889366626739502, Final Batch Loss: 0.24186939001083374\n",
      "Epoch 3736, Loss: 0.8508815467357635, Final Batch Loss: 0.21190299093723297\n",
      "Epoch 3737, Loss: 0.7495924681425095, Final Batch Loss: 0.17645259201526642\n",
      "Epoch 3738, Loss: 0.8062954992055893, Final Batch Loss: 0.19276465475559235\n",
      "Epoch 3739, Loss: 0.9159557968378067, Final Batch Loss: 0.2422279268503189\n",
      "Epoch 3740, Loss: 0.8019068986177444, Final Batch Loss: 0.19030606746673584\n",
      "Epoch 3741, Loss: 0.8759360462427139, Final Batch Loss: 0.21882013976573944\n",
      "Epoch 3742, Loss: 0.7971026599407196, Final Batch Loss: 0.15857592225074768\n",
      "Epoch 3743, Loss: 0.8393572866916656, Final Batch Loss: 0.18163760006427765\n",
      "Epoch 3744, Loss: 0.8467071652412415, Final Batch Loss: 0.19453005492687225\n",
      "Epoch 3745, Loss: 0.7646117955446243, Final Batch Loss: 0.21675296127796173\n",
      "Epoch 3746, Loss: 0.8330540060997009, Final Batch Loss: 0.13818177580833435\n",
      "Epoch 3747, Loss: 0.7848747968673706, Final Batch Loss: 0.1520729511976242\n",
      "Epoch 3748, Loss: 0.7810229361057281, Final Batch Loss: 0.16116008162498474\n",
      "Epoch 3749, Loss: 0.8841921836137772, Final Batch Loss: 0.2568691670894623\n",
      "Epoch 3750, Loss: 0.7764842510223389, Final Batch Loss: 0.15031112730503082\n",
      "Epoch 3751, Loss: 0.8410562127828598, Final Batch Loss: 0.16088666021823883\n",
      "Epoch 3752, Loss: 0.7753870487213135, Final Batch Loss: 0.151045024394989\n",
      "Epoch 3753, Loss: 0.8824557363986969, Final Batch Loss: 0.22767610847949982\n",
      "Epoch 3754, Loss: 0.7504572570323944, Final Batch Loss: 0.15052834153175354\n",
      "Epoch 3755, Loss: 0.817274734377861, Final Batch Loss: 0.23297640681266785\n",
      "Epoch 3756, Loss: 0.7979619950056076, Final Batch Loss: 0.19996322691440582\n",
      "Epoch 3757, Loss: 0.7188833057880402, Final Batch Loss: 0.20729446411132812\n",
      "Epoch 3758, Loss: 0.8972375690937042, Final Batch Loss: 0.19182029366493225\n",
      "Epoch 3759, Loss: 0.7643354535102844, Final Batch Loss: 0.2501254081726074\n",
      "Epoch 3760, Loss: 0.9040722697973251, Final Batch Loss: 0.25551721453666687\n",
      "Epoch 3761, Loss: 0.851056694984436, Final Batch Loss: 0.309474915266037\n",
      "Epoch 3762, Loss: 0.8339026272296906, Final Batch Loss: 0.15254995226860046\n",
      "Epoch 3763, Loss: 0.9305578619241714, Final Batch Loss: 0.35059604048728943\n",
      "Epoch 3764, Loss: 0.8426711857318878, Final Batch Loss: 0.1713077574968338\n",
      "Epoch 3765, Loss: 0.9747381061315536, Final Batch Loss: 0.3458860218524933\n",
      "Epoch 3766, Loss: 0.8606274724006653, Final Batch Loss: 0.23211736977100372\n",
      "Epoch 3767, Loss: 0.8297692686319351, Final Batch Loss: 0.20244893431663513\n",
      "Epoch 3768, Loss: 0.7372680008411407, Final Batch Loss: 0.1502283364534378\n",
      "Epoch 3769, Loss: 0.7978773415088654, Final Batch Loss: 0.17968815565109253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3770, Loss: 0.7859028279781342, Final Batch Loss: 0.16339178383350372\n",
      "Epoch 3771, Loss: 0.6907720416784286, Final Batch Loss: 0.15424570441246033\n",
      "Epoch 3772, Loss: 0.821938082575798, Final Batch Loss: 0.1154937744140625\n",
      "Epoch 3773, Loss: 0.7599291652441025, Final Batch Loss: 0.1781480461359024\n",
      "Epoch 3774, Loss: 0.8677796870470047, Final Batch Loss: 0.264166921377182\n",
      "Epoch 3775, Loss: 0.9158768206834793, Final Batch Loss: 0.24001401662826538\n",
      "Epoch 3776, Loss: 0.9259315878152847, Final Batch Loss: 0.25520116090774536\n",
      "Epoch 3777, Loss: 0.8919269740581512, Final Batch Loss: 0.2294810712337494\n",
      "Epoch 3778, Loss: 0.8445201218128204, Final Batch Loss: 0.20746523141860962\n",
      "Epoch 3779, Loss: 1.0116876661777496, Final Batch Loss: 0.3115551769733429\n",
      "Epoch 3780, Loss: 0.9221636801958084, Final Batch Loss: 0.2060009390115738\n",
      "Epoch 3781, Loss: 0.8489518761634827, Final Batch Loss: 0.2960943281650543\n",
      "Epoch 3782, Loss: 0.796099528670311, Final Batch Loss: 0.19745390117168427\n",
      "Epoch 3783, Loss: 0.8295296728610992, Final Batch Loss: 0.15687872469425201\n",
      "Epoch 3784, Loss: 0.9195854812860489, Final Batch Loss: 0.19636233150959015\n",
      "Epoch 3785, Loss: 0.7989907711744308, Final Batch Loss: 0.20584675669670105\n",
      "Epoch 3786, Loss: 0.8556499779224396, Final Batch Loss: 0.249604269862175\n",
      "Epoch 3787, Loss: 0.8955250680446625, Final Batch Loss: 0.2850910723209381\n",
      "Epoch 3788, Loss: 0.7461863309144974, Final Batch Loss: 0.17160753905773163\n",
      "Epoch 3789, Loss: 0.7848426997661591, Final Batch Loss: 0.17225569486618042\n",
      "Epoch 3790, Loss: 0.8081088066101074, Final Batch Loss: 0.23510466516017914\n",
      "Epoch 3791, Loss: 0.9033854007720947, Final Batch Loss: 0.17074812948703766\n",
      "Epoch 3792, Loss: 0.9386655390262604, Final Batch Loss: 0.2871406674385071\n",
      "Epoch 3793, Loss: 0.7819764614105225, Final Batch Loss: 0.19958673417568207\n",
      "Epoch 3794, Loss: 0.814346194267273, Final Batch Loss: 0.21184125542640686\n",
      "Epoch 3795, Loss: 0.6923027858138084, Final Batch Loss: 0.1921258121728897\n",
      "Epoch 3796, Loss: 0.8830772787332535, Final Batch Loss: 0.19919629395008087\n",
      "Epoch 3797, Loss: 0.8090431988239288, Final Batch Loss: 0.19770553708076477\n",
      "Epoch 3798, Loss: 0.8183944225311279, Final Batch Loss: 0.21533812582492828\n",
      "Epoch 3799, Loss: 0.8174383491277695, Final Batch Loss: 0.25071969628334045\n",
      "Epoch 3800, Loss: 0.8090561032295227, Final Batch Loss: 0.19332514703273773\n",
      "Epoch 3801, Loss: 0.9207818061113358, Final Batch Loss: 0.2875361144542694\n",
      "Epoch 3802, Loss: 0.8591863363981247, Final Batch Loss: 0.2586224377155304\n",
      "Epoch 3803, Loss: 0.8366459459066391, Final Batch Loss: 0.2428906261920929\n",
      "Epoch 3804, Loss: 0.8250348716974258, Final Batch Loss: 0.1921231597661972\n",
      "Epoch 3805, Loss: 0.833081841468811, Final Batch Loss: 0.1566062569618225\n",
      "Epoch 3806, Loss: 0.8810357749462128, Final Batch Loss: 0.25569888949394226\n",
      "Epoch 3807, Loss: 0.8073274046182632, Final Batch Loss: 0.16749458014965057\n",
      "Epoch 3808, Loss: 0.923225536942482, Final Batch Loss: 0.22264651954174042\n",
      "Epoch 3809, Loss: 0.9091428965330124, Final Batch Loss: 0.20288097858428955\n",
      "Epoch 3810, Loss: 0.9108436703681946, Final Batch Loss: 0.2615434229373932\n",
      "Epoch 3811, Loss: 0.7759168148040771, Final Batch Loss: 0.23346097767353058\n",
      "Epoch 3812, Loss: 1.0002567321062088, Final Batch Loss: 0.3286169171333313\n",
      "Epoch 3813, Loss: 0.8244474083185196, Final Batch Loss: 0.2025243490934372\n",
      "Epoch 3814, Loss: 0.8996333330869675, Final Batch Loss: 0.20021702349185944\n",
      "Epoch 3815, Loss: 0.940237283706665, Final Batch Loss: 0.23601432144641876\n",
      "Epoch 3816, Loss: 0.9277321100234985, Final Batch Loss: 0.20588789880275726\n",
      "Epoch 3817, Loss: 0.7608715742826462, Final Batch Loss: 0.18522070348262787\n",
      "Epoch 3818, Loss: 0.8664082139730453, Final Batch Loss: 0.18943645060062408\n",
      "Epoch 3819, Loss: 0.7615138068795204, Final Batch Loss: 0.11535394936800003\n",
      "Epoch 3820, Loss: 0.8733072429895401, Final Batch Loss: 0.23717351257801056\n",
      "Epoch 3821, Loss: 0.8676576465368271, Final Batch Loss: 0.22092905640602112\n",
      "Epoch 3822, Loss: 0.8905224055051804, Final Batch Loss: 0.2570120096206665\n",
      "Epoch 3823, Loss: 0.8346034735441208, Final Batch Loss: 0.14793890714645386\n",
      "Epoch 3824, Loss: 0.8553684204816818, Final Batch Loss: 0.26031163334846497\n",
      "Epoch 3825, Loss: 0.7934702187776566, Final Batch Loss: 0.1832287460565567\n",
      "Epoch 3826, Loss: 0.8165847510099411, Final Batch Loss: 0.18374918401241302\n",
      "Epoch 3827, Loss: 0.8782391101121902, Final Batch Loss: 0.23236168920993805\n",
      "Epoch 3828, Loss: 0.9174115210771561, Final Batch Loss: 0.20768459141254425\n",
      "Epoch 3829, Loss: 0.8173245191574097, Final Batch Loss: 0.19509254395961761\n",
      "Epoch 3830, Loss: 0.8742927312850952, Final Batch Loss: 0.23869332671165466\n",
      "Epoch 3831, Loss: 0.8099381029605865, Final Batch Loss: 0.2152526080608368\n",
      "Epoch 3832, Loss: 0.73124960064888, Final Batch Loss: 0.13141952455043793\n",
      "Epoch 3833, Loss: 0.8839755654335022, Final Batch Loss: 0.23593918979167938\n",
      "Epoch 3834, Loss: 0.829023540019989, Final Batch Loss: 0.2762336730957031\n",
      "Epoch 3835, Loss: 0.9619904458522797, Final Batch Loss: 0.20205064117908478\n",
      "Epoch 3836, Loss: 0.8326574265956879, Final Batch Loss: 0.24072103202342987\n",
      "Epoch 3837, Loss: 0.8444928079843521, Final Batch Loss: 0.28317850828170776\n",
      "Epoch 3838, Loss: 0.7192049771547318, Final Batch Loss: 0.10518880188465118\n",
      "Epoch 3839, Loss: 0.8045800030231476, Final Batch Loss: 0.24602729082107544\n",
      "Epoch 3840, Loss: 0.783382847905159, Final Batch Loss: 0.2354954481124878\n",
      "Epoch 3841, Loss: 0.8987565189599991, Final Batch Loss: 0.2774887681007385\n",
      "Epoch 3842, Loss: 0.8043440729379654, Final Batch Loss: 0.18714654445648193\n",
      "Epoch 3843, Loss: 0.7890996038913727, Final Batch Loss: 0.2093149572610855\n",
      "Epoch 3844, Loss: 0.6877264231443405, Final Batch Loss: 0.1517692506313324\n",
      "Epoch 3845, Loss: 0.7499949187040329, Final Batch Loss: 0.23117581009864807\n",
      "Epoch 3846, Loss: 0.8362500965595245, Final Batch Loss: 0.17816397547721863\n",
      "Epoch 3847, Loss: 0.8509715050458908, Final Batch Loss: 0.2241002321243286\n",
      "Epoch 3848, Loss: 0.916364774107933, Final Batch Loss: 0.35230737924575806\n",
      "Epoch 3849, Loss: 0.7528620511293411, Final Batch Loss: 0.1423124074935913\n",
      "Epoch 3850, Loss: 0.9904132038354874, Final Batch Loss: 0.2359561324119568\n",
      "Epoch 3851, Loss: 0.7524449378252029, Final Batch Loss: 0.14623631536960602\n",
      "Epoch 3852, Loss: 0.7699753046035767, Final Batch Loss: 0.23014889657497406\n",
      "Epoch 3853, Loss: 0.8284081965684891, Final Batch Loss: 0.27468350529670715\n",
      "Epoch 3854, Loss: 0.86673603951931, Final Batch Loss: 0.2783007025718689\n",
      "Epoch 3855, Loss: 0.8740485161542892, Final Batch Loss: 0.24374094605445862\n",
      "Epoch 3856, Loss: 0.8244456350803375, Final Batch Loss: 0.24463775753974915\n",
      "Epoch 3857, Loss: 0.8835444301366806, Final Batch Loss: 0.23113931715488434\n",
      "Epoch 3858, Loss: 0.7612595856189728, Final Batch Loss: 0.16612079739570618\n",
      "Epoch 3859, Loss: 0.8535688817501068, Final Batch Loss: 0.14606167376041412\n",
      "Epoch 3860, Loss: 0.8768928945064545, Final Batch Loss: 0.24917155504226685\n",
      "Epoch 3861, Loss: 0.7056988179683685, Final Batch Loss: 0.16534174978733063\n",
      "Epoch 3862, Loss: 0.8967824727296829, Final Batch Loss: 0.3270286023616791\n",
      "Epoch 3863, Loss: 0.9282465428113937, Final Batch Loss: 0.17305110394954681\n",
      "Epoch 3864, Loss: 0.8803867846727371, Final Batch Loss: 0.23806239664554596\n",
      "Epoch 3865, Loss: 0.8288937211036682, Final Batch Loss: 0.22355148196220398\n",
      "Epoch 3866, Loss: 0.7974298596382141, Final Batch Loss: 0.18623073399066925\n",
      "Epoch 3867, Loss: 0.7442937791347504, Final Batch Loss: 0.1755838841199875\n",
      "Epoch 3868, Loss: 0.7914932370185852, Final Batch Loss: 0.20550088584423065\n",
      "Epoch 3869, Loss: 0.8820120394229889, Final Batch Loss: 0.29302021861076355\n",
      "Epoch 3870, Loss: 0.9020743519067764, Final Batch Loss: 0.26689374446868896\n",
      "Epoch 3871, Loss: 0.8197916150093079, Final Batch Loss: 0.22623836994171143\n",
      "Epoch 3872, Loss: 0.8969188928604126, Final Batch Loss: 0.28415054082870483\n",
      "Epoch 3873, Loss: 0.8185132592916489, Final Batch Loss: 0.2009635865688324\n",
      "Epoch 3874, Loss: 0.7482753247022629, Final Batch Loss: 0.21882937848567963\n",
      "Epoch 3875, Loss: 0.9279494285583496, Final Batch Loss: 0.276725709438324\n",
      "Epoch 3876, Loss: 0.8207387775182724, Final Batch Loss: 0.18047870695590973\n",
      "Epoch 3877, Loss: 0.845024049282074, Final Batch Loss: 0.1918410360813141\n",
      "Epoch 3878, Loss: 0.9182776808738708, Final Batch Loss: 0.28209036588668823\n",
      "Epoch 3879, Loss: 0.7644698917865753, Final Batch Loss: 0.14852066338062286\n",
      "Epoch 3880, Loss: 0.8237072229385376, Final Batch Loss: 0.12635192275047302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3881, Loss: 0.896800309419632, Final Batch Loss: 0.2657497525215149\n",
      "Epoch 3882, Loss: 0.8567282557487488, Final Batch Loss: 0.12625610828399658\n",
      "Epoch 3883, Loss: 0.8357811272144318, Final Batch Loss: 0.30096718668937683\n",
      "Epoch 3884, Loss: 0.844030112028122, Final Batch Loss: 0.3409058153629303\n",
      "Epoch 3885, Loss: 0.8543970584869385, Final Batch Loss: 0.24715454876422882\n",
      "Epoch 3886, Loss: 1.0335042923688889, Final Batch Loss: 0.2345544546842575\n",
      "Epoch 3887, Loss: 0.7742043435573578, Final Batch Loss: 0.14979639649391174\n",
      "Epoch 3888, Loss: 0.8654814809560776, Final Batch Loss: 0.29246222972869873\n",
      "Epoch 3889, Loss: 0.8387381881475449, Final Batch Loss: 0.22228385508060455\n",
      "Epoch 3890, Loss: 0.7735909521579742, Final Batch Loss: 0.17870967090129852\n",
      "Epoch 3891, Loss: 0.8218897581100464, Final Batch Loss: 0.18885496258735657\n",
      "Epoch 3892, Loss: 0.9299386143684387, Final Batch Loss: 0.24468664824962616\n",
      "Epoch 3893, Loss: 0.7747306227684021, Final Batch Loss: 0.15565316379070282\n",
      "Epoch 3894, Loss: 0.8343889862298965, Final Batch Loss: 0.21512484550476074\n",
      "Epoch 3895, Loss: 0.8623728305101395, Final Batch Loss: 0.21177424490451813\n",
      "Epoch 3896, Loss: 0.8223944902420044, Final Batch Loss: 0.21799716353416443\n",
      "Epoch 3897, Loss: 0.874868631362915, Final Batch Loss: 0.3063114881515503\n",
      "Epoch 3898, Loss: 0.8174943178892136, Final Batch Loss: 0.20190782845020294\n",
      "Epoch 3899, Loss: 0.8155659437179565, Final Batch Loss: 0.19141973555088043\n",
      "Epoch 3900, Loss: 0.7466291785240173, Final Batch Loss: 0.13482370972633362\n",
      "Epoch 3901, Loss: 0.9425714313983917, Final Batch Loss: 0.24369391798973083\n",
      "Epoch 3902, Loss: 0.8727984726428986, Final Batch Loss: 0.18586285412311554\n",
      "Epoch 3903, Loss: 0.855861023068428, Final Batch Loss: 0.25961557030677795\n",
      "Epoch 3904, Loss: 0.8100835978984833, Final Batch Loss: 0.15021531283855438\n",
      "Epoch 3905, Loss: 0.8556661307811737, Final Batch Loss: 0.21795518696308136\n",
      "Epoch 3906, Loss: 0.8705029189586639, Final Batch Loss: 0.2922225296497345\n",
      "Epoch 3907, Loss: 0.7900087237358093, Final Batch Loss: 0.17103955149650574\n",
      "Epoch 3908, Loss: 0.8214569538831711, Final Batch Loss: 0.21999259293079376\n",
      "Epoch 3909, Loss: 0.8945901393890381, Final Batch Loss: 0.25605589151382446\n",
      "Epoch 3910, Loss: 0.8147391676902771, Final Batch Loss: 0.1769566833972931\n",
      "Epoch 3911, Loss: 0.8928958773612976, Final Batch Loss: 0.29880738258361816\n",
      "Epoch 3912, Loss: 0.787105530500412, Final Batch Loss: 0.23699522018432617\n",
      "Epoch 3913, Loss: 0.9285048246383667, Final Batch Loss: 0.38796380162239075\n",
      "Epoch 3914, Loss: 0.8610136955976486, Final Batch Loss: 0.23710644245147705\n",
      "Epoch 3915, Loss: 0.907473012804985, Final Batch Loss: 0.1632387489080429\n",
      "Epoch 3916, Loss: 0.7505161166191101, Final Batch Loss: 0.20450559258460999\n",
      "Epoch 3917, Loss: 0.8357130140066147, Final Batch Loss: 0.22865615785121918\n",
      "Epoch 3918, Loss: 0.9019551128149033, Final Batch Loss: 0.193308487534523\n",
      "Epoch 3919, Loss: 0.7816582471132278, Final Batch Loss: 0.23559555411338806\n",
      "Epoch 3920, Loss: 0.8807564228773117, Final Batch Loss: 0.23878991603851318\n",
      "Epoch 3921, Loss: 0.8351121991872787, Final Batch Loss: 0.29390427470207214\n",
      "Epoch 3922, Loss: 0.9979049563407898, Final Batch Loss: 0.21910686790943146\n",
      "Epoch 3923, Loss: 0.834224596619606, Final Batch Loss: 0.27533015608787537\n",
      "Epoch 3924, Loss: 0.6911352127790451, Final Batch Loss: 0.16282297670841217\n",
      "Epoch 3925, Loss: 0.8123908936977386, Final Batch Loss: 0.19947603344917297\n",
      "Epoch 3926, Loss: 0.8249230086803436, Final Batch Loss: 0.2156991809606552\n",
      "Epoch 3927, Loss: 0.815499559044838, Final Batch Loss: 0.18207557499408722\n",
      "Epoch 3928, Loss: 0.8062142878770828, Final Batch Loss: 0.22424949705600739\n",
      "Epoch 3929, Loss: 0.8802007883787155, Final Batch Loss: 0.14798608422279358\n",
      "Epoch 3930, Loss: 0.8523560166358948, Final Batch Loss: 0.24266473948955536\n",
      "Epoch 3931, Loss: 0.7107246518135071, Final Batch Loss: 0.13219939172267914\n",
      "Epoch 3932, Loss: 0.8546785861253738, Final Batch Loss: 0.27233508229255676\n",
      "Epoch 3933, Loss: 0.7882460355758667, Final Batch Loss: 0.24084223806858063\n",
      "Epoch 3934, Loss: 0.7503193914890289, Final Batch Loss: 0.1961755007505417\n",
      "Epoch 3935, Loss: 0.8366355895996094, Final Batch Loss: 0.21241065859794617\n",
      "Epoch 3936, Loss: 0.8514424413442612, Final Batch Loss: 0.2674393355846405\n",
      "Epoch 3937, Loss: 0.7722594439983368, Final Batch Loss: 0.19456779956817627\n",
      "Epoch 3938, Loss: 0.9286962598562241, Final Batch Loss: 0.17996451258659363\n",
      "Epoch 3939, Loss: 0.8419954478740692, Final Batch Loss: 0.24374179542064667\n",
      "Epoch 3940, Loss: 0.8388659060001373, Final Batch Loss: 0.21438105404376984\n",
      "Epoch 3941, Loss: 0.8119668662548065, Final Batch Loss: 0.13809247314929962\n",
      "Epoch 3942, Loss: 0.8948716968297958, Final Batch Loss: 0.26819494366645813\n",
      "Epoch 3943, Loss: 0.8467097133398056, Final Batch Loss: 0.24021665751934052\n",
      "Epoch 3944, Loss: 0.8435445576906204, Final Batch Loss: 0.1806059330701828\n",
      "Epoch 3945, Loss: 0.8159750401973724, Final Batch Loss: 0.20979571342468262\n",
      "Epoch 3946, Loss: 0.9590407460927963, Final Batch Loss: 0.29416102170944214\n",
      "Epoch 3947, Loss: 0.7644531726837158, Final Batch Loss: 0.15792377293109894\n",
      "Epoch 3948, Loss: 0.7857772260904312, Final Batch Loss: 0.2018023431301117\n",
      "Epoch 3949, Loss: 0.9091037958860397, Final Batch Loss: 0.19832342863082886\n",
      "Epoch 3950, Loss: 0.8688148260116577, Final Batch Loss: 0.28886714577674866\n",
      "Epoch 3951, Loss: 0.910442590713501, Final Batch Loss: 0.2332618683576584\n",
      "Epoch 3952, Loss: 0.7402832210063934, Final Batch Loss: 0.20012718439102173\n",
      "Epoch 3953, Loss: 0.8529932498931885, Final Batch Loss: 0.1873539388179779\n",
      "Epoch 3954, Loss: 0.8129495829343796, Final Batch Loss: 0.18788890540599823\n",
      "Epoch 3955, Loss: 0.925372526049614, Final Batch Loss: 0.20509327948093414\n",
      "Epoch 3956, Loss: 0.9335521906614304, Final Batch Loss: 0.2739505171775818\n",
      "Epoch 3957, Loss: 0.8688306510448456, Final Batch Loss: 0.2123972475528717\n",
      "Epoch 3958, Loss: 0.8791266232728958, Final Batch Loss: 0.21898336708545685\n",
      "Epoch 3959, Loss: 0.8821090459823608, Final Batch Loss: 0.2195655107498169\n",
      "Epoch 3960, Loss: 0.8222167491912842, Final Batch Loss: 0.3151540160179138\n",
      "Epoch 3961, Loss: 0.895066425204277, Final Batch Loss: 0.23735187947750092\n",
      "Epoch 3962, Loss: 0.8628373593091965, Final Batch Loss: 0.23037147521972656\n",
      "Epoch 3963, Loss: 0.9205126017332077, Final Batch Loss: 0.1418459713459015\n",
      "Epoch 3964, Loss: 0.7447150349617004, Final Batch Loss: 0.16477420926094055\n",
      "Epoch 3965, Loss: 0.7868981659412384, Final Batch Loss: 0.17930857837200165\n",
      "Epoch 3966, Loss: 0.7678609937429428, Final Batch Loss: 0.15926146507263184\n",
      "Epoch 3967, Loss: 0.8023880273103714, Final Batch Loss: 0.16842082142829895\n",
      "Epoch 3968, Loss: 0.8369164764881134, Final Batch Loss: 0.23351649940013885\n",
      "Epoch 3969, Loss: 0.7446286305785179, Final Batch Loss: 0.12296221405267715\n",
      "Epoch 3970, Loss: 0.8604235649108887, Final Batch Loss: 0.2630000710487366\n",
      "Epoch 3971, Loss: 1.021374523639679, Final Batch Loss: 0.2423403412103653\n",
      "Epoch 3972, Loss: 0.9015215933322906, Final Batch Loss: 0.19979172945022583\n",
      "Epoch 3973, Loss: 0.8147421032190323, Final Batch Loss: 0.22062291204929352\n",
      "Epoch 3974, Loss: 0.8284973949193954, Final Batch Loss: 0.17848114669322968\n",
      "Epoch 3975, Loss: 0.779232069849968, Final Batch Loss: 0.18855763971805573\n",
      "Epoch 3976, Loss: 0.8791359066963196, Final Batch Loss: 0.19082939624786377\n",
      "Epoch 3977, Loss: 0.8487515449523926, Final Batch Loss: 0.2712061107158661\n",
      "Epoch 3978, Loss: 0.8544431924819946, Final Batch Loss: 0.16831299662590027\n",
      "Epoch 3979, Loss: 0.8719605952501297, Final Batch Loss: 0.1947309523820877\n",
      "Epoch 3980, Loss: 0.850144699215889, Final Batch Loss: 0.2573282718658447\n",
      "Epoch 3981, Loss: 0.7428066581487656, Final Batch Loss: 0.17956368625164032\n",
      "Epoch 3982, Loss: 0.867326557636261, Final Batch Loss: 0.14523230493068695\n",
      "Epoch 3983, Loss: 0.8330043256282806, Final Batch Loss: 0.2595830261707306\n",
      "Epoch 3984, Loss: 0.8847877234220505, Final Batch Loss: 0.28368693590164185\n",
      "Epoch 3985, Loss: 0.8995741903781891, Final Batch Loss: 0.31419822573661804\n",
      "Epoch 3986, Loss: 0.9647467732429504, Final Batch Loss: 0.3029431700706482\n",
      "Epoch 3987, Loss: 0.8784531801939011, Final Batch Loss: 0.261039137840271\n",
      "Epoch 3988, Loss: 0.7539079636335373, Final Batch Loss: 0.12518572807312012\n",
      "Epoch 3989, Loss: 0.8014147728681564, Final Batch Loss: 0.16396158933639526\n",
      "Epoch 3990, Loss: 0.8068684786558151, Final Batch Loss: 0.19380074739456177\n",
      "Epoch 3991, Loss: 0.9681993126869202, Final Batch Loss: 0.25421783328056335\n",
      "Epoch 3992, Loss: 0.7228076159954071, Final Batch Loss: 0.16635635495185852\n",
      "Epoch 3993, Loss: 0.6822639405727386, Final Batch Loss: 0.17423886060714722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3994, Loss: 0.7598345577716827, Final Batch Loss: 0.18983183801174164\n",
      "Epoch 3995, Loss: 0.7846847325563431, Final Batch Loss: 0.2330498993396759\n",
      "Epoch 3996, Loss: 0.8780447393655777, Final Batch Loss: 0.19049760699272156\n",
      "Epoch 3997, Loss: 0.8731092065572739, Final Batch Loss: 0.2882844805717468\n",
      "Epoch 3998, Loss: 0.8772238343954086, Final Batch Loss: 0.21754583716392517\n",
      "Epoch 3999, Loss: 0.8453424572944641, Final Batch Loss: 0.23597724735736847\n",
      "Epoch 4000, Loss: 0.7159246206283569, Final Batch Loss: 0.14148490130901337\n",
      "Epoch 4001, Loss: 0.7678609490394592, Final Batch Loss: 0.1484544426202774\n",
      "Epoch 4002, Loss: 0.8928678631782532, Final Batch Loss: 0.2873627841472626\n",
      "Epoch 4003, Loss: 0.7623073905706406, Final Batch Loss: 0.1850063055753708\n",
      "Epoch 4004, Loss: 0.8425126373767853, Final Batch Loss: 0.20047307014465332\n",
      "Epoch 4005, Loss: 0.9514031708240509, Final Batch Loss: 0.21586570143699646\n",
      "Epoch 4006, Loss: 0.8864817768335342, Final Batch Loss: 0.30153346061706543\n",
      "Epoch 4007, Loss: 0.7155112400650978, Final Batch Loss: 0.1082831546664238\n",
      "Epoch 4008, Loss: 0.9652050584554672, Final Batch Loss: 0.1995318979024887\n",
      "Epoch 4009, Loss: 0.8013832122087479, Final Batch Loss: 0.16384710371494293\n",
      "Epoch 4010, Loss: 0.860446885228157, Final Batch Loss: 0.17957176268100739\n",
      "Epoch 4011, Loss: 0.8988810926675797, Final Batch Loss: 0.29628732800483704\n",
      "Epoch 4012, Loss: 0.8899793028831482, Final Batch Loss: 0.2555660903453827\n",
      "Epoch 4013, Loss: 0.8309999108314514, Final Batch Loss: 0.2480979561805725\n",
      "Epoch 4014, Loss: 0.8342670798301697, Final Batch Loss: 0.14960448443889618\n",
      "Epoch 4015, Loss: 0.9156652092933655, Final Batch Loss: 0.22366216778755188\n",
      "Epoch 4016, Loss: 0.8677205294370651, Final Batch Loss: 0.16158334910869598\n",
      "Epoch 4017, Loss: 0.8328429311513901, Final Batch Loss: 0.20504428446292877\n",
      "Epoch 4018, Loss: 0.8230001926422119, Final Batch Loss: 0.22726011276245117\n",
      "Epoch 4019, Loss: 0.7531218975782394, Final Batch Loss: 0.14860215783119202\n",
      "Epoch 4020, Loss: 0.8020352423191071, Final Batch Loss: 0.20144668221473694\n",
      "Epoch 4021, Loss: 0.9165401607751846, Final Batch Loss: 0.277485728263855\n",
      "Epoch 4022, Loss: 0.8234370946884155, Final Batch Loss: 0.18174739181995392\n",
      "Epoch 4023, Loss: 0.891962081193924, Final Batch Loss: 0.1939784437417984\n",
      "Epoch 4024, Loss: 0.8752770870923996, Final Batch Loss: 0.1597737818956375\n",
      "Epoch 4025, Loss: 1.0089314132928848, Final Batch Loss: 0.35825878381729126\n",
      "Epoch 4026, Loss: 0.7961137145757675, Final Batch Loss: 0.2013290524482727\n",
      "Epoch 4027, Loss: 0.933506965637207, Final Batch Loss: 0.2995435297489166\n",
      "Epoch 4028, Loss: 0.8813849538564682, Final Batch Loss: 0.2241843342781067\n",
      "Epoch 4029, Loss: 0.7518577501177788, Final Batch Loss: 0.12101564556360245\n",
      "Epoch 4030, Loss: 0.7429503798484802, Final Batch Loss: 0.09582023322582245\n",
      "Epoch 4031, Loss: 0.8752292096614838, Final Batch Loss: 0.28740084171295166\n",
      "Epoch 4032, Loss: 0.79726842045784, Final Batch Loss: 0.19494454562664032\n",
      "Epoch 4033, Loss: 0.7416292130947113, Final Batch Loss: 0.1689671277999878\n",
      "Epoch 4034, Loss: 0.7420202642679214, Final Batch Loss: 0.20469292998313904\n",
      "Epoch 4035, Loss: 0.8628797978162766, Final Batch Loss: 0.23493967950344086\n",
      "Epoch 4036, Loss: 0.8633265346288681, Final Batch Loss: 0.250981867313385\n",
      "Epoch 4037, Loss: 0.7270359992980957, Final Batch Loss: 0.2313617318868637\n",
      "Epoch 4038, Loss: 0.751796543598175, Final Batch Loss: 0.1710316687822342\n",
      "Epoch 4039, Loss: 0.7677674740552902, Final Batch Loss: 0.23255570232868195\n",
      "Epoch 4040, Loss: 0.82670459151268, Final Batch Loss: 0.15505415201187134\n",
      "Epoch 4041, Loss: 0.8638688623905182, Final Batch Loss: 0.2922765612602234\n",
      "Epoch 4042, Loss: 0.7834220230579376, Final Batch Loss: 0.19727976620197296\n",
      "Epoch 4043, Loss: 0.7261459082365036, Final Batch Loss: 0.14783337712287903\n",
      "Epoch 4044, Loss: 0.818072184920311, Final Batch Loss: 0.24855375289916992\n",
      "Epoch 4045, Loss: 0.8866554796695709, Final Batch Loss: 0.2547203004360199\n",
      "Epoch 4046, Loss: 0.8783371895551682, Final Batch Loss: 0.213202565908432\n",
      "Epoch 4047, Loss: 0.8318759649991989, Final Batch Loss: 0.23498158156871796\n",
      "Epoch 4048, Loss: 0.8050958812236786, Final Batch Loss: 0.18572694063186646\n",
      "Epoch 4049, Loss: 0.6631911173462868, Final Batch Loss: 0.11980105191469193\n",
      "Epoch 4050, Loss: 0.8805858045816422, Final Batch Loss: 0.24670107662677765\n",
      "Epoch 4051, Loss: 0.888968825340271, Final Batch Loss: 0.24208809435367584\n",
      "Epoch 4052, Loss: 0.8384435921907425, Final Batch Loss: 0.22014352679252625\n",
      "Epoch 4053, Loss: 0.8873354643583298, Final Batch Loss: 0.17322422564029694\n",
      "Epoch 4054, Loss: 0.8051938414573669, Final Batch Loss: 0.14733953773975372\n",
      "Epoch 4055, Loss: 0.8700001388788223, Final Batch Loss: 0.1686907559633255\n",
      "Epoch 4056, Loss: 0.8513957411050797, Final Batch Loss: 0.21341848373413086\n",
      "Epoch 4057, Loss: 0.8745758980512619, Final Batch Loss: 0.1961083859205246\n",
      "Epoch 4058, Loss: 0.8780397474765778, Final Batch Loss: 0.19978825747966766\n",
      "Epoch 4059, Loss: 1.0474058389663696, Final Batch Loss: 0.3413906991481781\n",
      "Epoch 4060, Loss: 0.8928069919347763, Final Batch Loss: 0.24070727825164795\n",
      "Epoch 4061, Loss: 0.8462694883346558, Final Batch Loss: 0.23865966498851776\n",
      "Epoch 4062, Loss: 0.7476754635572433, Final Batch Loss: 0.1731061488389969\n",
      "Epoch 4063, Loss: 0.8596840798854828, Final Batch Loss: 0.26139891147613525\n",
      "Epoch 4064, Loss: 0.8408212065696716, Final Batch Loss: 0.28885897994041443\n",
      "Epoch 4065, Loss: 0.7935062646865845, Final Batch Loss: 0.21827828884124756\n",
      "Epoch 4066, Loss: 0.8101193606853485, Final Batch Loss: 0.2030138075351715\n",
      "Epoch 4067, Loss: 0.838650107383728, Final Batch Loss: 0.15222279727458954\n",
      "Epoch 4068, Loss: 0.7807822227478027, Final Batch Loss: 0.18387305736541748\n",
      "Epoch 4069, Loss: 0.795048400759697, Final Batch Loss: 0.12047992646694183\n",
      "Epoch 4070, Loss: 0.7957870662212372, Final Batch Loss: 0.22996951639652252\n",
      "Epoch 4071, Loss: 0.7801738232374191, Final Batch Loss: 0.1581309288740158\n",
      "Epoch 4072, Loss: 0.8361174911260605, Final Batch Loss: 0.19371965527534485\n",
      "Epoch 4073, Loss: 0.9452760070562363, Final Batch Loss: 0.2540464401245117\n",
      "Epoch 4074, Loss: 0.7503916919231415, Final Batch Loss: 0.1494922786951065\n",
      "Epoch 4075, Loss: 0.7741126716136932, Final Batch Loss: 0.26898977160453796\n",
      "Epoch 4076, Loss: 0.7283129245042801, Final Batch Loss: 0.13449229300022125\n",
      "Epoch 4077, Loss: 0.7381896823644638, Final Batch Loss: 0.1567995548248291\n",
      "Epoch 4078, Loss: 0.7812623828649521, Final Batch Loss: 0.17171579599380493\n",
      "Epoch 4079, Loss: 0.8350168317556381, Final Batch Loss: 0.12461118400096893\n",
      "Epoch 4080, Loss: 0.7300106734037399, Final Batch Loss: 0.16106025874614716\n",
      "Epoch 4081, Loss: 0.7396557033061981, Final Batch Loss: 0.13572081923484802\n",
      "Epoch 4082, Loss: 0.7851290255784988, Final Batch Loss: 0.157836452126503\n",
      "Epoch 4083, Loss: 0.7948612123727798, Final Batch Loss: 0.21404562890529633\n",
      "Epoch 4084, Loss: 0.8428511917591095, Final Batch Loss: 0.18502527475357056\n",
      "Epoch 4085, Loss: 0.8271917998790741, Final Batch Loss: 0.2770158350467682\n",
      "Epoch 4086, Loss: 0.8005286455154419, Final Batch Loss: 0.20979057252407074\n",
      "Epoch 4087, Loss: 0.8019933551549911, Final Batch Loss: 0.15281417965888977\n",
      "Epoch 4088, Loss: 0.8381533771753311, Final Batch Loss: 0.2887929677963257\n",
      "Epoch 4089, Loss: 1.0009497553110123, Final Batch Loss: 0.3211369812488556\n",
      "Epoch 4090, Loss: 0.8404721617698669, Final Batch Loss: 0.27174752950668335\n",
      "Epoch 4091, Loss: 0.8413380831480026, Final Batch Loss: 0.29903051257133484\n",
      "Epoch 4092, Loss: 0.7658441364765167, Final Batch Loss: 0.20137470960617065\n",
      "Epoch 4093, Loss: 0.8782418966293335, Final Batch Loss: 0.31531229615211487\n",
      "Epoch 4094, Loss: 0.9734878987073898, Final Batch Loss: 0.19988103210926056\n",
      "Epoch 4095, Loss: 0.8208775073289871, Final Batch Loss: 0.214424729347229\n",
      "Epoch 4096, Loss: 0.7269002944231033, Final Batch Loss: 0.1263202577829361\n",
      "Epoch 4097, Loss: 0.7390036731958389, Final Batch Loss: 0.16734373569488525\n",
      "Epoch 4098, Loss: 0.8146917074918747, Final Batch Loss: 0.24518321454524994\n",
      "Epoch 4099, Loss: 0.7732102423906326, Final Batch Loss: 0.12554048001766205\n",
      "Epoch 4100, Loss: 0.8336410373449326, Final Batch Loss: 0.2536547780036926\n",
      "Epoch 4101, Loss: 0.791303887963295, Final Batch Loss: 0.149339959025383\n",
      "Epoch 4102, Loss: 0.804119274020195, Final Batch Loss: 0.16277264058589935\n",
      "Epoch 4103, Loss: 0.8374303132295609, Final Batch Loss: 0.20258522033691406\n",
      "Epoch 4104, Loss: 0.84139484167099, Final Batch Loss: 0.2429802417755127\n",
      "Epoch 4105, Loss: 0.7959030866622925, Final Batch Loss: 0.19931575655937195\n",
      "Epoch 4106, Loss: 0.8378099799156189, Final Batch Loss: 0.14075469970703125\n",
      "Epoch 4107, Loss: 0.6991414949297905, Final Batch Loss: 0.1142791137099266\n",
      "Epoch 4108, Loss: 0.9477128088474274, Final Batch Loss: 0.3056756258010864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4109, Loss: 0.8765981644392014, Final Batch Loss: 0.24731014668941498\n",
      "Epoch 4110, Loss: 0.844117060303688, Final Batch Loss: 0.20585288107395172\n",
      "Epoch 4111, Loss: 0.8191174417734146, Final Batch Loss: 0.1696373075246811\n",
      "Epoch 4112, Loss: 0.8198423683643341, Final Batch Loss: 0.19764205813407898\n",
      "Epoch 4113, Loss: 0.8531225472688675, Final Batch Loss: 0.20862430334091187\n",
      "Epoch 4114, Loss: 0.8185591101646423, Final Batch Loss: 0.1694781333208084\n",
      "Epoch 4115, Loss: 0.86611607670784, Final Batch Loss: 0.15750077366828918\n",
      "Epoch 4116, Loss: 0.7410265356302261, Final Batch Loss: 0.1484515219926834\n",
      "Epoch 4117, Loss: 0.8413120657205582, Final Batch Loss: 0.1906139850616455\n",
      "Epoch 4118, Loss: 0.7674901485443115, Final Batch Loss: 0.13664644956588745\n",
      "Epoch 4119, Loss: 0.8567418605089188, Final Batch Loss: 0.1911351978778839\n",
      "Epoch 4120, Loss: 0.9184966534376144, Final Batch Loss: 0.22734509408473969\n",
      "Epoch 4121, Loss: 0.7968709915876389, Final Batch Loss: 0.21732200682163239\n",
      "Epoch 4122, Loss: 0.8678060173988342, Final Batch Loss: 0.20888619124889374\n",
      "Epoch 4123, Loss: 0.8453024625778198, Final Batch Loss: 0.10965178906917572\n",
      "Epoch 4124, Loss: 0.8212378323078156, Final Batch Loss: 0.26136016845703125\n",
      "Epoch 4125, Loss: 0.9119886755943298, Final Batch Loss: 0.25930023193359375\n",
      "Epoch 4126, Loss: 0.7400515526533127, Final Batch Loss: 0.17839404940605164\n",
      "Epoch 4127, Loss: 0.8371984213590622, Final Batch Loss: 0.19162951409816742\n",
      "Epoch 4128, Loss: 0.9020333290100098, Final Batch Loss: 0.2282177358865738\n",
      "Epoch 4129, Loss: 0.8748765289783478, Final Batch Loss: 0.15742547810077667\n",
      "Epoch 4130, Loss: 0.8113828301429749, Final Batch Loss: 0.22534270584583282\n",
      "Epoch 4131, Loss: 0.8626943230628967, Final Batch Loss: 0.18087778985500336\n",
      "Epoch 4132, Loss: 0.7814919799566269, Final Batch Loss: 0.17108818888664246\n",
      "Epoch 4133, Loss: 0.7657011598348618, Final Batch Loss: 0.18754595518112183\n",
      "Epoch 4134, Loss: 0.806869775056839, Final Batch Loss: 0.1822924166917801\n",
      "Epoch 4135, Loss: 0.7082820683717728, Final Batch Loss: 0.1676819771528244\n",
      "Epoch 4136, Loss: 0.7516499161720276, Final Batch Loss: 0.20137256383895874\n",
      "Epoch 4137, Loss: 0.7666170448064804, Final Batch Loss: 0.22483006119728088\n",
      "Epoch 4138, Loss: 0.8291249424219131, Final Batch Loss: 0.24854008853435516\n",
      "Epoch 4139, Loss: 0.7887611389160156, Final Batch Loss: 0.1696886420249939\n",
      "Epoch 4140, Loss: 0.7739588916301727, Final Batch Loss: 0.1180620938539505\n",
      "Epoch 4141, Loss: 0.8222387433052063, Final Batch Loss: 0.2152697890996933\n",
      "Epoch 4142, Loss: 0.8094318956136703, Final Batch Loss: 0.18428172171115875\n",
      "Epoch 4143, Loss: 0.8081271946430206, Final Batch Loss: 0.17235364019870758\n",
      "Epoch 4144, Loss: 0.7681053876876831, Final Batch Loss: 0.14219245314598083\n",
      "Epoch 4145, Loss: 0.7809098064899445, Final Batch Loss: 0.13039088249206543\n",
      "Epoch 4146, Loss: 0.7365753129124641, Final Batch Loss: 0.09900525957345963\n",
      "Epoch 4147, Loss: 0.7789312601089478, Final Batch Loss: 0.17525503039360046\n",
      "Epoch 4148, Loss: 0.7566040009260178, Final Batch Loss: 0.2537308633327484\n",
      "Epoch 4149, Loss: 0.7538397759199142, Final Batch Loss: 0.1816456913948059\n",
      "Epoch 4150, Loss: 0.8135718405246735, Final Batch Loss: 0.14554819464683533\n",
      "Epoch 4151, Loss: 0.7339333593845367, Final Batch Loss: 0.08391089737415314\n",
      "Epoch 4152, Loss: 0.7587894946336746, Final Batch Loss: 0.16881895065307617\n",
      "Epoch 4153, Loss: 0.8216035813093185, Final Batch Loss: 0.1740921139717102\n",
      "Epoch 4154, Loss: 0.8077176958322525, Final Batch Loss: 0.23626501858234406\n",
      "Epoch 4155, Loss: 0.7634789943695068, Final Batch Loss: 0.2313837856054306\n",
      "Epoch 4156, Loss: 0.7846077978610992, Final Batch Loss: 0.2567831873893738\n",
      "Epoch 4157, Loss: 0.7523936480283737, Final Batch Loss: 0.17422664165496826\n",
      "Epoch 4158, Loss: 0.8879255503416061, Final Batch Loss: 0.2843068540096283\n",
      "Epoch 4159, Loss: 0.9284238815307617, Final Batch Loss: 0.19961683452129364\n",
      "Epoch 4160, Loss: 0.7337007373571396, Final Batch Loss: 0.17888452112674713\n",
      "Epoch 4161, Loss: 0.8695746809244156, Final Batch Loss: 0.25037670135498047\n",
      "Epoch 4162, Loss: 0.8273778855800629, Final Batch Loss: 0.18057018518447876\n",
      "Epoch 4163, Loss: 0.8213214725255966, Final Batch Loss: 0.23483821749687195\n",
      "Epoch 4164, Loss: 0.8830992877483368, Final Batch Loss: 0.2980767488479614\n",
      "Epoch 4165, Loss: 0.8751471489667892, Final Batch Loss: 0.25292137265205383\n",
      "Epoch 4166, Loss: 0.8621202111244202, Final Batch Loss: 0.200577050447464\n",
      "Epoch 4167, Loss: 0.7507271766662598, Final Batch Loss: 0.19732937216758728\n",
      "Epoch 4168, Loss: 0.8335472196340561, Final Batch Loss: 0.22139067947864532\n",
      "Epoch 4169, Loss: 0.7210521548986435, Final Batch Loss: 0.18099386990070343\n",
      "Epoch 4170, Loss: 0.8225980699062347, Final Batch Loss: 0.212799534201622\n",
      "Epoch 4171, Loss: 1.0045675337314606, Final Batch Loss: 0.38497430086135864\n",
      "Epoch 4172, Loss: 0.8996817320585251, Final Batch Loss: 0.23085235059261322\n",
      "Epoch 4173, Loss: 0.6873831897974014, Final Batch Loss: 0.13101129233837128\n",
      "Epoch 4174, Loss: 0.7088145911693573, Final Batch Loss: 0.15192776918411255\n",
      "Epoch 4175, Loss: 0.8794007897377014, Final Batch Loss: 0.2310844510793686\n",
      "Epoch 4176, Loss: 0.7871686518192291, Final Batch Loss: 0.1736086905002594\n",
      "Epoch 4177, Loss: 0.742308959364891, Final Batch Loss: 0.1456260085105896\n",
      "Epoch 4178, Loss: 0.9310692101716995, Final Batch Loss: 0.2929419279098511\n",
      "Epoch 4179, Loss: 0.8644768595695496, Final Batch Loss: 0.2134629637002945\n",
      "Epoch 4180, Loss: 0.7782852947711945, Final Batch Loss: 0.17864464223384857\n",
      "Epoch 4181, Loss: 0.9044243097305298, Final Batch Loss: 0.26880791783332825\n",
      "Epoch 4182, Loss: 0.76225645840168, Final Batch Loss: 0.15712007880210876\n",
      "Epoch 4183, Loss: 0.7925160825252533, Final Batch Loss: 0.17736691236495972\n",
      "Epoch 4184, Loss: 0.8107145428657532, Final Batch Loss: 0.18493902683258057\n",
      "Epoch 4185, Loss: 1.034921020269394, Final Batch Loss: 0.3730989992618561\n",
      "Epoch 4186, Loss: 0.7221002578735352, Final Batch Loss: 0.1511181890964508\n",
      "Epoch 4187, Loss: 0.7390105873346329, Final Batch Loss: 0.20233941078186035\n",
      "Epoch 4188, Loss: 0.7773554474115372, Final Batch Loss: 0.14334048330783844\n",
      "Epoch 4189, Loss: 0.7562121152877808, Final Batch Loss: 0.21073132753372192\n",
      "Epoch 4190, Loss: 0.7224850505590439, Final Batch Loss: 0.193783700466156\n",
      "Epoch 4191, Loss: 0.9241016209125519, Final Batch Loss: 0.23020130395889282\n",
      "Epoch 4192, Loss: 0.8373084515333176, Final Batch Loss: 0.2008376121520996\n",
      "Epoch 4193, Loss: 0.8757350742816925, Final Batch Loss: 0.26037392020225525\n",
      "Epoch 4194, Loss: 0.7905194461345673, Final Batch Loss: 0.24009005725383759\n",
      "Epoch 4195, Loss: 0.7108594328165054, Final Batch Loss: 0.19777829945087433\n",
      "Epoch 4196, Loss: 0.7741173952817917, Final Batch Loss: 0.1844482719898224\n",
      "Epoch 4197, Loss: 0.904323011636734, Final Batch Loss: 0.24489650130271912\n",
      "Epoch 4198, Loss: 0.8025717735290527, Final Batch Loss: 0.1597595512866974\n",
      "Epoch 4199, Loss: 0.8189651817083359, Final Batch Loss: 0.21524065732955933\n",
      "Epoch 4200, Loss: 0.8440227657556534, Final Batch Loss: 0.21275971829891205\n",
      "Epoch 4201, Loss: 0.7090532928705215, Final Batch Loss: 0.19752241671085358\n",
      "Epoch 4202, Loss: 0.8367371112108231, Final Batch Loss: 0.1852620393037796\n",
      "Epoch 4203, Loss: 0.916686475276947, Final Batch Loss: 0.2421233057975769\n",
      "Epoch 4204, Loss: 0.7445538565516472, Final Batch Loss: 0.11141297966241837\n",
      "Epoch 4205, Loss: 0.7866475731134415, Final Batch Loss: 0.20393933355808258\n",
      "Epoch 4206, Loss: 0.8071854412555695, Final Batch Loss: 0.1792164146900177\n",
      "Epoch 4207, Loss: 1.011011317372322, Final Batch Loss: 0.3925643861293793\n",
      "Epoch 4208, Loss: 0.9235405325889587, Final Batch Loss: 0.307315856218338\n",
      "Epoch 4209, Loss: 0.7325585186481476, Final Batch Loss: 0.15182679891586304\n",
      "Epoch 4210, Loss: 0.7534259110689163, Final Batch Loss: 0.12162154912948608\n",
      "Epoch 4211, Loss: 0.9436433166265488, Final Batch Loss: 0.30296263098716736\n",
      "Epoch 4212, Loss: 0.6987283229827881, Final Batch Loss: 0.16291718184947968\n",
      "Epoch 4213, Loss: 0.786223366856575, Final Batch Loss: 0.22413930296897888\n",
      "Epoch 4214, Loss: 0.9295280277729034, Final Batch Loss: 0.16368937492370605\n",
      "Epoch 4215, Loss: 0.7508318573236465, Final Batch Loss: 0.16072990000247955\n",
      "Epoch 4216, Loss: 0.8124856948852539, Final Batch Loss: 0.21784032881259918\n",
      "Epoch 4217, Loss: 0.894133985042572, Final Batch Loss: 0.2079811990261078\n",
      "Epoch 4218, Loss: 0.7856850847601891, Final Batch Loss: 0.09943006187677383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4219, Loss: 0.8275857269763947, Final Batch Loss: 0.2031540870666504\n",
      "Epoch 4220, Loss: 0.7345279008150101, Final Batch Loss: 0.16439147293567657\n",
      "Epoch 4221, Loss: 0.9087780117988586, Final Batch Loss: 0.2840431332588196\n",
      "Epoch 4222, Loss: 0.9451910853385925, Final Batch Loss: 0.2385772466659546\n",
      "Epoch 4223, Loss: 0.8692936897277832, Final Batch Loss: 0.2292487472295761\n",
      "Epoch 4224, Loss: 0.9754549413919449, Final Batch Loss: 0.2671952545642853\n",
      "Epoch 4225, Loss: 0.883508488535881, Final Batch Loss: 0.2856799364089966\n",
      "Epoch 4226, Loss: 0.7509662210941315, Final Batch Loss: 0.23092901706695557\n",
      "Epoch 4227, Loss: 0.8086834847927094, Final Batch Loss: 0.222713902592659\n",
      "Epoch 4228, Loss: 0.8999847918748856, Final Batch Loss: 0.29463887214660645\n",
      "Epoch 4229, Loss: 0.8649639338254929, Final Batch Loss: 0.2606908679008484\n",
      "Epoch 4230, Loss: 0.8056792169809341, Final Batch Loss: 0.18238092958927155\n",
      "Epoch 4231, Loss: 0.8632761240005493, Final Batch Loss: 0.20735090970993042\n",
      "Epoch 4232, Loss: 0.8436932116746902, Final Batch Loss: 0.26866793632507324\n",
      "Epoch 4233, Loss: 0.9089585542678833, Final Batch Loss: 0.22528952360153198\n",
      "Epoch 4234, Loss: 0.8200652450323105, Final Batch Loss: 0.18684107065200806\n",
      "Epoch 4235, Loss: 0.8696227222681046, Final Batch Loss: 0.2580963373184204\n",
      "Epoch 4236, Loss: 0.8708020895719528, Final Batch Loss: 0.28915736079216003\n",
      "Epoch 4237, Loss: 0.7815905064344406, Final Batch Loss: 0.19099627435207367\n",
      "Epoch 4238, Loss: 0.7860042303800583, Final Batch Loss: 0.21408407390117645\n",
      "Epoch 4239, Loss: 0.8623536080121994, Final Batch Loss: 0.21833468973636627\n",
      "Epoch 4240, Loss: 0.8619884699583054, Final Batch Loss: 0.18751931190490723\n",
      "Epoch 4241, Loss: 0.7983018010854721, Final Batch Loss: 0.19345587491989136\n",
      "Epoch 4242, Loss: 0.7653533071279526, Final Batch Loss: 0.2354920208454132\n",
      "Epoch 4243, Loss: 0.6973102390766144, Final Batch Loss: 0.17212052643299103\n",
      "Epoch 4244, Loss: 0.7681362628936768, Final Batch Loss: 0.21990881860256195\n",
      "Epoch 4245, Loss: 0.7653122991323471, Final Batch Loss: 0.21833419799804688\n",
      "Epoch 4246, Loss: 0.8017733842134476, Final Batch Loss: 0.14383530616760254\n",
      "Epoch 4247, Loss: 0.7708594650030136, Final Batch Loss: 0.1342439502477646\n",
      "Epoch 4248, Loss: 0.7116553783416748, Final Batch Loss: 0.15422706305980682\n",
      "Epoch 4249, Loss: 0.8914264440536499, Final Batch Loss: 0.25255608558654785\n",
      "Epoch 4250, Loss: 0.8525032699108124, Final Batch Loss: 0.24441835284233093\n",
      "Epoch 4251, Loss: 0.9749923199415207, Final Batch Loss: 0.2609648108482361\n",
      "Epoch 4252, Loss: 0.7736676782369614, Final Batch Loss: 0.17792350053787231\n",
      "Epoch 4253, Loss: 0.8471424132585526, Final Batch Loss: 0.22292548418045044\n",
      "Epoch 4254, Loss: 0.8268096446990967, Final Batch Loss: 0.2127307802438736\n",
      "Epoch 4255, Loss: 0.7962291538715363, Final Batch Loss: 0.2087385207414627\n",
      "Epoch 4256, Loss: 0.8160726130008698, Final Batch Loss: 0.23203535377979279\n",
      "Epoch 4257, Loss: 0.824950322508812, Final Batch Loss: 0.21042299270629883\n",
      "Epoch 4258, Loss: 0.8378890752792358, Final Batch Loss: 0.22746777534484863\n",
      "Epoch 4259, Loss: 0.8115811496973038, Final Batch Loss: 0.11305899918079376\n",
      "Epoch 4260, Loss: 0.7222210839390755, Final Batch Loss: 0.12290722876787186\n",
      "Epoch 4261, Loss: 0.8548136204481125, Final Batch Loss: 0.14989259839057922\n",
      "Epoch 4262, Loss: 0.8396281599998474, Final Batch Loss: 0.2505815327167511\n",
      "Epoch 4263, Loss: 0.8425038009881973, Final Batch Loss: 0.1844913810491562\n",
      "Epoch 4264, Loss: 0.7576316595077515, Final Batch Loss: 0.15844230353832245\n",
      "Epoch 4265, Loss: 0.8012914210557938, Final Batch Loss: 0.1712353229522705\n",
      "Epoch 4266, Loss: 0.8508273065090179, Final Batch Loss: 0.2789527475833893\n",
      "Epoch 4267, Loss: 0.7818253487348557, Final Batch Loss: 0.17982839047908783\n",
      "Epoch 4268, Loss: 0.8017848283052444, Final Batch Loss: 0.18734753131866455\n",
      "Epoch 4269, Loss: 0.7980355769395828, Final Batch Loss: 0.22133944928646088\n",
      "Epoch 4270, Loss: 0.8278971016407013, Final Batch Loss: 0.23797304928302765\n",
      "Epoch 4271, Loss: 0.7664772123098373, Final Batch Loss: 0.20140919089317322\n",
      "Epoch 4272, Loss: 0.8128377795219421, Final Batch Loss: 0.18485233187675476\n",
      "Epoch 4273, Loss: 0.7681419998407364, Final Batch Loss: 0.23834684491157532\n",
      "Epoch 4274, Loss: 0.8693576008081436, Final Batch Loss: 0.23684728145599365\n",
      "Epoch 4275, Loss: 0.7771607488393784, Final Batch Loss: 0.24635857343673706\n",
      "Epoch 4276, Loss: 0.8715081959962845, Final Batch Loss: 0.24057328701019287\n",
      "Epoch 4277, Loss: 0.8321711421012878, Final Batch Loss: 0.2755297124385834\n",
      "Epoch 4278, Loss: 0.7686245441436768, Final Batch Loss: 0.21666039526462555\n",
      "Epoch 4279, Loss: 0.8469161093235016, Final Batch Loss: 0.22473163902759552\n",
      "Epoch 4280, Loss: 0.7324112728238106, Final Batch Loss: 0.08992358297109604\n",
      "Epoch 4281, Loss: 0.7944417744874954, Final Batch Loss: 0.2425190657377243\n",
      "Epoch 4282, Loss: 0.7906919866800308, Final Batch Loss: 0.18511256575584412\n",
      "Epoch 4283, Loss: 0.8917526602745056, Final Batch Loss: 0.2298194169998169\n",
      "Epoch 4284, Loss: 0.7024517804384232, Final Batch Loss: 0.12825563549995422\n",
      "Epoch 4285, Loss: 0.7323549836874008, Final Batch Loss: 0.13960589468479156\n",
      "Epoch 4286, Loss: 0.8931035548448563, Final Batch Loss: 0.3068159520626068\n",
      "Epoch 4287, Loss: 0.8172038644552231, Final Batch Loss: 0.27092593908309937\n",
      "Epoch 4288, Loss: 0.8442188948392868, Final Batch Loss: 0.1937861144542694\n",
      "Epoch 4289, Loss: 0.6702643483877182, Final Batch Loss: 0.1642380654811859\n",
      "Epoch 4290, Loss: 0.8375221192836761, Final Batch Loss: 0.17524336278438568\n",
      "Epoch 4291, Loss: 0.8522381037473679, Final Batch Loss: 0.18395504355430603\n",
      "Epoch 4292, Loss: 0.8208255618810654, Final Batch Loss: 0.28017154335975647\n",
      "Epoch 4293, Loss: 0.7727906852960587, Final Batch Loss: 0.20671671628952026\n",
      "Epoch 4294, Loss: 0.8793200701475143, Final Batch Loss: 0.16537539660930634\n",
      "Epoch 4295, Loss: 0.8677243143320084, Final Batch Loss: 0.30604785680770874\n",
      "Epoch 4296, Loss: 0.8194027245044708, Final Batch Loss: 0.2066420167684555\n",
      "Epoch 4297, Loss: 0.8065906167030334, Final Batch Loss: 0.24791362881660461\n",
      "Epoch 4298, Loss: 0.9707746356725693, Final Batch Loss: 0.24997647106647491\n",
      "Epoch 4299, Loss: 0.719867929816246, Final Batch Loss: 0.18475806713104248\n",
      "Epoch 4300, Loss: 0.7294575423002243, Final Batch Loss: 0.14126037061214447\n",
      "Epoch 4301, Loss: 0.7420964688062668, Final Batch Loss: 0.14730356633663177\n",
      "Epoch 4302, Loss: 0.864696815609932, Final Batch Loss: 0.24185609817504883\n",
      "Epoch 4303, Loss: 0.7735599726438522, Final Batch Loss: 0.28986746072769165\n",
      "Epoch 4304, Loss: 0.8583829700946808, Final Batch Loss: 0.15957993268966675\n",
      "Epoch 4305, Loss: 0.8189999014139175, Final Batch Loss: 0.20211836695671082\n",
      "Epoch 4306, Loss: 0.8430863320827484, Final Batch Loss: 0.2616860568523407\n",
      "Epoch 4307, Loss: 0.7826821506023407, Final Batch Loss: 0.16957837343215942\n",
      "Epoch 4308, Loss: 0.8637876063585281, Final Batch Loss: 0.26910871267318726\n",
      "Epoch 4309, Loss: 0.8458449989557266, Final Batch Loss: 0.2162461280822754\n",
      "Epoch 4310, Loss: 0.7749984562397003, Final Batch Loss: 0.21455596387386322\n",
      "Epoch 4311, Loss: 0.7548969089984894, Final Batch Loss: 0.13152040541172028\n",
      "Epoch 4312, Loss: 0.7867517173290253, Final Batch Loss: 0.1836191713809967\n",
      "Epoch 4313, Loss: 0.9048542082309723, Final Batch Loss: 0.21655303239822388\n",
      "Epoch 4314, Loss: 0.6997409984469414, Final Batch Loss: 0.08667712658643723\n",
      "Epoch 4315, Loss: 0.7182343602180481, Final Batch Loss: 0.18014803528785706\n",
      "Epoch 4316, Loss: 0.8753748834133148, Final Batch Loss: 0.27928411960601807\n",
      "Epoch 4317, Loss: 0.7771704792976379, Final Batch Loss: 0.1797000765800476\n",
      "Epoch 4318, Loss: 1.1604626923799515, Final Batch Loss: 0.47098854184150696\n",
      "Epoch 4319, Loss: 0.9639461785554886, Final Batch Loss: 0.3223610818386078\n",
      "Epoch 4320, Loss: 0.7337794750928879, Final Batch Loss: 0.16530869901180267\n",
      "Epoch 4321, Loss: 0.823029637336731, Final Batch Loss: 0.19911547005176544\n",
      "Epoch 4322, Loss: 0.8982701748609543, Final Batch Loss: 0.31542840600013733\n",
      "Epoch 4323, Loss: 0.7147720009088516, Final Batch Loss: 0.12623560428619385\n",
      "Epoch 4324, Loss: 0.8023474961519241, Final Batch Loss: 0.1407034993171692\n",
      "Epoch 4325, Loss: 0.8971605896949768, Final Batch Loss: 0.25959596037864685\n",
      "Epoch 4326, Loss: 0.796917513012886, Final Batch Loss: 0.18695679306983948\n",
      "Epoch 4327, Loss: 0.7080256193876266, Final Batch Loss: 0.1426089107990265\n",
      "Epoch 4328, Loss: 0.8461648374795914, Final Batch Loss: 0.25860267877578735\n",
      "Epoch 4329, Loss: 0.929429829120636, Final Batch Loss: 0.26891979575157166\n",
      "Epoch 4330, Loss: 0.8645644932985306, Final Batch Loss: 0.22180010378360748\n",
      "Epoch 4331, Loss: 0.8790221810340881, Final Batch Loss: 0.18128854036331177\n",
      "Epoch 4332, Loss: 0.7682091444730759, Final Batch Loss: 0.16448703408241272\n",
      "Epoch 4333, Loss: 0.8153901696205139, Final Batch Loss: 0.22056138515472412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4334, Loss: 0.813982829451561, Final Batch Loss: 0.2547842264175415\n",
      "Epoch 4335, Loss: 0.8067512959241867, Final Batch Loss: 0.1724250316619873\n",
      "Epoch 4336, Loss: 0.779552772641182, Final Batch Loss: 0.14031462371349335\n",
      "Epoch 4337, Loss: 0.7582770884037018, Final Batch Loss: 0.21113531291484833\n",
      "Epoch 4338, Loss: 0.7556063532829285, Final Batch Loss: 0.18892641365528107\n",
      "Epoch 4339, Loss: 0.7910087704658508, Final Batch Loss: 0.16042682528495789\n",
      "Epoch 4340, Loss: 0.8096951693296432, Final Batch Loss: 0.20165924727916718\n",
      "Epoch 4341, Loss: 0.7862327247858047, Final Batch Loss: 0.19687420129776\n",
      "Epoch 4342, Loss: 0.7667343020439148, Final Batch Loss: 0.1429566591978073\n",
      "Epoch 4343, Loss: 0.7516976296901703, Final Batch Loss: 0.12819485366344452\n",
      "Epoch 4344, Loss: 0.8876600414514542, Final Batch Loss: 0.329197496175766\n",
      "Epoch 4345, Loss: 0.7775493413209915, Final Batch Loss: 0.1532488912343979\n",
      "Epoch 4346, Loss: 0.8178679347038269, Final Batch Loss: 0.17985749244689941\n",
      "Epoch 4347, Loss: 0.8448864668607712, Final Batch Loss: 0.2845053970813751\n",
      "Epoch 4348, Loss: 0.8449108153581619, Final Batch Loss: 0.24729801714420319\n",
      "Epoch 4349, Loss: 0.7834853082895279, Final Batch Loss: 0.24145784974098206\n",
      "Epoch 4350, Loss: 0.7888145446777344, Final Batch Loss: 0.1634008288383484\n",
      "Epoch 4351, Loss: 0.655297227203846, Final Batch Loss: 0.12427740544080734\n",
      "Epoch 4352, Loss: 0.7425273507833481, Final Batch Loss: 0.2151615023612976\n",
      "Epoch 4353, Loss: 0.6952284723520279, Final Batch Loss: 0.1419256627559662\n",
      "Epoch 4354, Loss: 0.9163980931043625, Final Batch Loss: 0.2936515808105469\n",
      "Epoch 4355, Loss: 0.7355707883834839, Final Batch Loss: 0.15675878524780273\n",
      "Epoch 4356, Loss: 0.8725845366716385, Final Batch Loss: 0.23712590336799622\n",
      "Epoch 4357, Loss: 0.6920454055070877, Final Batch Loss: 0.14727093279361725\n",
      "Epoch 4358, Loss: 0.7075011879205704, Final Batch Loss: 0.18870913982391357\n",
      "Epoch 4359, Loss: 0.7593910247087479, Final Batch Loss: 0.16596169769763947\n",
      "Epoch 4360, Loss: 0.7844209372997284, Final Batch Loss: 0.17799371480941772\n",
      "Epoch 4361, Loss: 0.9069092273712158, Final Batch Loss: 0.22117945551872253\n",
      "Epoch 4362, Loss: 0.7677834779024124, Final Batch Loss: 0.16598846018314362\n",
      "Epoch 4363, Loss: 0.7999064475297928, Final Batch Loss: 0.16569973528385162\n",
      "Epoch 4364, Loss: 0.7977586090564728, Final Batch Loss: 0.21800382435321808\n",
      "Epoch 4365, Loss: 0.7307703495025635, Final Batch Loss: 0.1388096660375595\n",
      "Epoch 4366, Loss: 0.7758499830961227, Final Batch Loss: 0.20367781817913055\n",
      "Epoch 4367, Loss: 0.7153051644563675, Final Batch Loss: 0.20851552486419678\n",
      "Epoch 4368, Loss: 0.6936547607183456, Final Batch Loss: 0.12254397571086884\n",
      "Epoch 4369, Loss: 0.8037816286087036, Final Batch Loss: 0.2068808376789093\n",
      "Epoch 4370, Loss: 0.8577422499656677, Final Batch Loss: 0.23973441123962402\n",
      "Epoch 4371, Loss: 0.7693933323025703, Final Batch Loss: 0.2815104126930237\n",
      "Epoch 4372, Loss: 0.7585915774106979, Final Batch Loss: 0.18044766783714294\n",
      "Epoch 4373, Loss: 0.8745824545621872, Final Batch Loss: 0.387177973985672\n",
      "Epoch 4374, Loss: 0.8152494132518768, Final Batch Loss: 0.21160802245140076\n",
      "Epoch 4375, Loss: 0.7804017513990402, Final Batch Loss: 0.25503191351890564\n",
      "Epoch 4376, Loss: 0.8878283351659775, Final Batch Loss: 0.29005226492881775\n",
      "Epoch 4377, Loss: 0.7640091776847839, Final Batch Loss: 0.19873137772083282\n",
      "Epoch 4378, Loss: 0.8563474640250206, Final Batch Loss: 0.25130388140678406\n",
      "Epoch 4379, Loss: 0.760845422744751, Final Batch Loss: 0.1754840761423111\n",
      "Epoch 4380, Loss: 0.7039106786251068, Final Batch Loss: 0.1388179063796997\n",
      "Epoch 4381, Loss: 0.9896478056907654, Final Batch Loss: 0.29436764121055603\n",
      "Epoch 4382, Loss: 0.7692202031612396, Final Batch Loss: 0.18866823613643646\n",
      "Epoch 4383, Loss: 0.8593956232070923, Final Batch Loss: 0.2202022671699524\n",
      "Epoch 4384, Loss: 0.7734434902667999, Final Batch Loss: 0.22677652537822723\n",
      "Epoch 4385, Loss: 0.7567054331302643, Final Batch Loss: 0.1767977774143219\n",
      "Epoch 4386, Loss: 0.8157895058393478, Final Batch Loss: 0.18980389833450317\n",
      "Epoch 4387, Loss: 0.8479630500078201, Final Batch Loss: 0.2161523997783661\n",
      "Epoch 4388, Loss: 0.7660299986600876, Final Batch Loss: 0.12540368735790253\n",
      "Epoch 4389, Loss: 0.7055470943450928, Final Batch Loss: 0.1497066617012024\n",
      "Epoch 4390, Loss: 0.7536223381757736, Final Batch Loss: 0.17597812414169312\n",
      "Epoch 4391, Loss: 0.8168247789144516, Final Batch Loss: 0.13585609197616577\n",
      "Epoch 4392, Loss: 0.9119980931282043, Final Batch Loss: 0.27571725845336914\n",
      "Epoch 4393, Loss: 0.822207972407341, Final Batch Loss: 0.20739726722240448\n",
      "Epoch 4394, Loss: 0.7834520936012268, Final Batch Loss: 0.16670551896095276\n",
      "Epoch 4395, Loss: 0.7193857729434967, Final Batch Loss: 0.10565014183521271\n",
      "Epoch 4396, Loss: 0.646784670650959, Final Batch Loss: 0.15170539915561676\n",
      "Epoch 4397, Loss: 0.8213848024606705, Final Batch Loss: 0.23677325248718262\n",
      "Epoch 4398, Loss: 0.7560324370861053, Final Batch Loss: 0.2855512201786041\n",
      "Epoch 4399, Loss: 0.9163616746664047, Final Batch Loss: 0.17108798027038574\n",
      "Epoch 4400, Loss: 0.7051537483930588, Final Batch Loss: 0.12099403142929077\n",
      "Epoch 4401, Loss: 0.7746784538030624, Final Batch Loss: 0.21996065974235535\n",
      "Epoch 4402, Loss: 0.7906747460365295, Final Batch Loss: 0.16562657058238983\n",
      "Epoch 4403, Loss: 0.8464280515909195, Final Batch Loss: 0.29689568281173706\n",
      "Epoch 4404, Loss: 0.7686079144477844, Final Batch Loss: 0.24679553508758545\n",
      "Epoch 4405, Loss: 0.846181258559227, Final Batch Loss: 0.26305726170539856\n",
      "Epoch 4406, Loss: 0.7507804781198502, Final Batch Loss: 0.26273173093795776\n",
      "Epoch 4407, Loss: 0.7934142649173737, Final Batch Loss: 0.28133073449134827\n",
      "Epoch 4408, Loss: 0.867376446723938, Final Batch Loss: 0.18847884237766266\n",
      "Epoch 4409, Loss: 0.7501348108053207, Final Batch Loss: 0.20396468043327332\n",
      "Epoch 4410, Loss: 0.7112610936164856, Final Batch Loss: 0.19728723168373108\n",
      "Epoch 4411, Loss: 0.742202565073967, Final Batch Loss: 0.1305486559867859\n",
      "Epoch 4412, Loss: 0.7850429713726044, Final Batch Loss: 0.24923056364059448\n",
      "Epoch 4413, Loss: 0.7561554908752441, Final Batch Loss: 0.1259341686964035\n",
      "Epoch 4414, Loss: 0.8183273524045944, Final Batch Loss: 0.23643584549427032\n",
      "Epoch 4415, Loss: 0.8826293647289276, Final Batch Loss: 0.16122683882713318\n",
      "Epoch 4416, Loss: 0.710176482796669, Final Batch Loss: 0.1829153448343277\n",
      "Epoch 4417, Loss: 0.9109053760766983, Final Batch Loss: 0.28211459517478943\n",
      "Epoch 4418, Loss: 0.8366634845733643, Final Batch Loss: 0.16928799450397491\n",
      "Epoch 4419, Loss: 0.7635251134634018, Final Batch Loss: 0.21986690163612366\n",
      "Epoch 4420, Loss: 0.7509882748126984, Final Batch Loss: 0.1647139936685562\n",
      "Epoch 4421, Loss: 0.7540149539709091, Final Batch Loss: 0.14129988849163055\n",
      "Epoch 4422, Loss: 0.7789626270532608, Final Batch Loss: 0.23235422372817993\n",
      "Epoch 4423, Loss: 0.7523032873868942, Final Batch Loss: 0.20082026720046997\n",
      "Epoch 4424, Loss: 0.7436568886041641, Final Batch Loss: 0.1694476157426834\n",
      "Epoch 4425, Loss: 0.7249044924974442, Final Batch Loss: 0.23042166233062744\n",
      "Epoch 4426, Loss: 0.7606914937496185, Final Batch Loss: 0.16220828890800476\n",
      "Epoch 4427, Loss: 0.8009675294160843, Final Batch Loss: 0.2217998504638672\n",
      "Epoch 4428, Loss: 0.807116225361824, Final Batch Loss: 0.24679750204086304\n",
      "Epoch 4429, Loss: 0.7932133972644806, Final Batch Loss: 0.29810962080955505\n",
      "Epoch 4430, Loss: 0.7610559165477753, Final Batch Loss: 0.17539936304092407\n",
      "Epoch 4431, Loss: 0.7777125984430313, Final Batch Loss: 0.1367875635623932\n",
      "Epoch 4432, Loss: 0.6362188458442688, Final Batch Loss: 0.12966148555278778\n",
      "Epoch 4433, Loss: 0.7985202372074127, Final Batch Loss: 0.17852060496807098\n",
      "Epoch 4434, Loss: 0.7926989048719406, Final Batch Loss: 0.19368846714496613\n",
      "Epoch 4435, Loss: 0.6515243947505951, Final Batch Loss: 0.14639687538146973\n",
      "Epoch 4436, Loss: 0.7113911211490631, Final Batch Loss: 0.13638359308242798\n",
      "Epoch 4437, Loss: 0.845749020576477, Final Batch Loss: 0.25679638981819153\n",
      "Epoch 4438, Loss: 0.7129026502370834, Final Batch Loss: 0.19703984260559082\n",
      "Epoch 4439, Loss: 0.7895323485136032, Final Batch Loss: 0.24923253059387207\n",
      "Epoch 4440, Loss: 0.7678364664316177, Final Batch Loss: 0.1636698842048645\n",
      "Epoch 4441, Loss: 0.8411637395620346, Final Batch Loss: 0.13305166363716125\n",
      "Epoch 4442, Loss: 0.7190252393484116, Final Batch Loss: 0.17719405889511108\n",
      "Epoch 4443, Loss: 0.7224125862121582, Final Batch Loss: 0.20391537249088287\n",
      "Epoch 4444, Loss: 0.8016096651554108, Final Batch Loss: 0.15066298842430115\n",
      "Epoch 4445, Loss: 0.8250076174736023, Final Batch Loss: 0.19351209700107574\n",
      "Epoch 4446, Loss: 0.873768150806427, Final Batch Loss: 0.21690064668655396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4447, Loss: 0.8742592036724091, Final Batch Loss: 0.22309380769729614\n",
      "Epoch 4448, Loss: 0.8138376921415329, Final Batch Loss: 0.1811475157737732\n",
      "Epoch 4449, Loss: 0.9606448113918304, Final Batch Loss: 0.20933376252651215\n",
      "Epoch 4450, Loss: 0.7613934725522995, Final Batch Loss: 0.14825142920017242\n",
      "Epoch 4451, Loss: 0.794733002781868, Final Batch Loss: 0.23942941427230835\n",
      "Epoch 4452, Loss: 0.757598340511322, Final Batch Loss: 0.13675615191459656\n",
      "Epoch 4453, Loss: 0.9409362822771072, Final Batch Loss: 0.3826952278614044\n",
      "Epoch 4454, Loss: 0.8342868387699127, Final Batch Loss: 0.20676352083683014\n",
      "Epoch 4455, Loss: 0.8686467409133911, Final Batch Loss: 0.2737444043159485\n",
      "Epoch 4456, Loss: 0.8858194202184677, Final Batch Loss: 0.30078810453414917\n",
      "Epoch 4457, Loss: 0.9184727072715759, Final Batch Loss: 0.28276902437210083\n",
      "Epoch 4458, Loss: 0.8688316494226456, Final Batch Loss: 0.2450047731399536\n",
      "Epoch 4459, Loss: 0.8876108229160309, Final Batch Loss: 0.25547078251838684\n",
      "Epoch 4460, Loss: 0.8936829566955566, Final Batch Loss: 0.18839000165462494\n",
      "Epoch 4461, Loss: 0.7876513302326202, Final Batch Loss: 0.16984513401985168\n",
      "Epoch 4462, Loss: 0.8673127442598343, Final Batch Loss: 0.2871197760105133\n",
      "Epoch 4463, Loss: 0.7667667418718338, Final Batch Loss: 0.21325159072875977\n",
      "Epoch 4464, Loss: 0.9338811486959457, Final Batch Loss: 0.33914071321487427\n",
      "Epoch 4465, Loss: 0.784393846988678, Final Batch Loss: 0.16468867659568787\n",
      "Epoch 4466, Loss: 0.8138511329889297, Final Batch Loss: 0.12799721956253052\n",
      "Epoch 4467, Loss: 0.7105181813240051, Final Batch Loss: 0.22864466905593872\n",
      "Epoch 4468, Loss: 0.8290409296751022, Final Batch Loss: 0.2312198430299759\n",
      "Epoch 4469, Loss: 0.8325925767421722, Final Batch Loss: 0.21661797165870667\n",
      "Epoch 4470, Loss: 0.728546142578125, Final Batch Loss: 0.17240652441978455\n",
      "Epoch 4471, Loss: 0.7798290252685547, Final Batch Loss: 0.1823882907629013\n",
      "Epoch 4472, Loss: 0.8992651104927063, Final Batch Loss: 0.22960549592971802\n",
      "Epoch 4473, Loss: 0.7239954769611359, Final Batch Loss: 0.1667393445968628\n",
      "Epoch 4474, Loss: 0.8505087047815323, Final Batch Loss: 0.256872296333313\n",
      "Epoch 4475, Loss: 0.8236134350299835, Final Batch Loss: 0.1459936499595642\n",
      "Epoch 4476, Loss: 0.8278001844882965, Final Batch Loss: 0.2779574394226074\n",
      "Epoch 4477, Loss: 0.8075830936431885, Final Batch Loss: 0.19954267144203186\n",
      "Epoch 4478, Loss: 0.9195805191993713, Final Batch Loss: 0.259939968585968\n",
      "Epoch 4479, Loss: 0.7945510894060135, Final Batch Loss: 0.1625722497701645\n",
      "Epoch 4480, Loss: 0.7423956990242004, Final Batch Loss: 0.15451127290725708\n",
      "Epoch 4481, Loss: 0.8016809448599815, Final Batch Loss: 0.07388041168451309\n",
      "Epoch 4482, Loss: 0.7156947702169418, Final Batch Loss: 0.12724432349205017\n",
      "Epoch 4483, Loss: 0.8733633011579514, Final Batch Loss: 0.19306465983390808\n",
      "Epoch 4484, Loss: 0.765358179807663, Final Batch Loss: 0.1896170675754547\n",
      "Epoch 4485, Loss: 0.8586570769548416, Final Batch Loss: 0.1613779366016388\n",
      "Epoch 4486, Loss: 0.6953897327184677, Final Batch Loss: 0.12637151777744293\n",
      "Epoch 4487, Loss: 0.7029018551111221, Final Batch Loss: 0.15912729501724243\n",
      "Epoch 4488, Loss: 0.7149438560009003, Final Batch Loss: 0.10995474457740784\n",
      "Epoch 4489, Loss: 0.720110148191452, Final Batch Loss: 0.17312251031398773\n",
      "Epoch 4490, Loss: 0.7319163084030151, Final Batch Loss: 0.13879349827766418\n",
      "Epoch 4491, Loss: 0.6981144696474075, Final Batch Loss: 0.17867811024188995\n",
      "Epoch 4492, Loss: 0.7125338613986969, Final Batch Loss: 0.19514033198356628\n",
      "Epoch 4493, Loss: 0.793705552816391, Final Batch Loss: 0.14662416279315948\n",
      "Epoch 4494, Loss: 0.7254328429698944, Final Batch Loss: 0.12233833968639374\n",
      "Epoch 4495, Loss: 0.7331596314907074, Final Batch Loss: 0.15041115880012512\n",
      "Epoch 4496, Loss: 0.8008656650781631, Final Batch Loss: 0.19286732375621796\n",
      "Epoch 4497, Loss: 0.9825959354639053, Final Batch Loss: 0.3273407518863678\n",
      "Epoch 4498, Loss: 0.7770619541406631, Final Batch Loss: 0.161176860332489\n",
      "Epoch 4499, Loss: 0.7835530042648315, Final Batch Loss: 0.18214784562587738\n",
      "Epoch 4500, Loss: 0.7783213108778, Final Batch Loss: 0.23450732231140137\n",
      "Epoch 4501, Loss: 0.7348999530076981, Final Batch Loss: 0.17089810967445374\n",
      "Epoch 4502, Loss: 0.7869526147842407, Final Batch Loss: 0.18924592435359955\n",
      "Epoch 4503, Loss: 0.8231737464666367, Final Batch Loss: 0.2214287966489792\n",
      "Epoch 4504, Loss: 0.696927547454834, Final Batch Loss: 0.17732813954353333\n",
      "Epoch 4505, Loss: 0.8141636997461319, Final Batch Loss: 0.21250887215137482\n",
      "Epoch 4506, Loss: 0.740906685590744, Final Batch Loss: 0.18083719909191132\n",
      "Epoch 4507, Loss: 0.8225822150707245, Final Batch Loss: 0.1911628544330597\n",
      "Epoch 4508, Loss: 0.6950587332248688, Final Batch Loss: 0.13344401121139526\n",
      "Epoch 4509, Loss: 0.6378748789429665, Final Batch Loss: 0.10544472187757492\n",
      "Epoch 4510, Loss: 0.7250673472881317, Final Batch Loss: 0.1849258989095688\n",
      "Epoch 4511, Loss: 0.7691370248794556, Final Batch Loss: 0.1723855882883072\n",
      "Epoch 4512, Loss: 0.7608004957437515, Final Batch Loss: 0.2115410566329956\n",
      "Epoch 4513, Loss: 0.9089528024196625, Final Batch Loss: 0.19810880720615387\n",
      "Epoch 4514, Loss: 0.8378156870603561, Final Batch Loss: 0.24179860949516296\n",
      "Epoch 4515, Loss: 0.7875165939331055, Final Batch Loss: 0.2823271155357361\n",
      "Epoch 4516, Loss: 0.703506588935852, Final Batch Loss: 0.1943465769290924\n",
      "Epoch 4517, Loss: 0.7636107951402664, Final Batch Loss: 0.22342847287654877\n",
      "Epoch 4518, Loss: 0.7609892040491104, Final Batch Loss: 0.15184511244297028\n",
      "Epoch 4519, Loss: 0.8201942145824432, Final Batch Loss: 0.2350679337978363\n",
      "Epoch 4520, Loss: 0.7703681737184525, Final Batch Loss: 0.19191357493400574\n",
      "Epoch 4521, Loss: 0.7553050965070724, Final Batch Loss: 0.1637820452451706\n",
      "Epoch 4522, Loss: 0.7030665278434753, Final Batch Loss: 0.13186879456043243\n",
      "Epoch 4523, Loss: 0.7895503491163254, Final Batch Loss: 0.27066770195961\n",
      "Epoch 4524, Loss: 0.6608582064509392, Final Batch Loss: 0.10687150806188583\n",
      "Epoch 4525, Loss: 0.75898277759552, Final Batch Loss: 0.12896183133125305\n",
      "Epoch 4526, Loss: 0.7828063368797302, Final Batch Loss: 0.13744911551475525\n",
      "Epoch 4527, Loss: 0.7443997114896774, Final Batch Loss: 0.19053834676742554\n",
      "Epoch 4528, Loss: 0.8069179654121399, Final Batch Loss: 0.185371994972229\n",
      "Epoch 4529, Loss: 0.8406027033925056, Final Batch Loss: 0.28690192103385925\n",
      "Epoch 4530, Loss: 0.6969215571880341, Final Batch Loss: 0.13650518655776978\n",
      "Epoch 4531, Loss: 0.7815398871898651, Final Batch Loss: 0.16861321032047272\n",
      "Epoch 4532, Loss: 0.7761434614658356, Final Batch Loss: 0.18823103606700897\n",
      "Epoch 4533, Loss: 0.7389025986194611, Final Batch Loss: 0.17327776551246643\n",
      "Epoch 4534, Loss: 0.8669248968362808, Final Batch Loss: 0.2957709729671478\n",
      "Epoch 4535, Loss: 0.7374926507472992, Final Batch Loss: 0.17490194737911224\n",
      "Epoch 4536, Loss: 0.9088849276304245, Final Batch Loss: 0.28240761160850525\n",
      "Epoch 4537, Loss: 0.6978897452354431, Final Batch Loss: 0.17011748254299164\n",
      "Epoch 4538, Loss: 0.704229474067688, Final Batch Loss: 0.16580705344676971\n",
      "Epoch 4539, Loss: 0.81899094581604, Final Batch Loss: 0.20170600712299347\n",
      "Epoch 4540, Loss: 0.7030898332595825, Final Batch Loss: 0.1747017204761505\n",
      "Epoch 4541, Loss: 0.7385813444852829, Final Batch Loss: 0.13010036945343018\n",
      "Epoch 4542, Loss: 0.6654037982225418, Final Batch Loss: 0.15099696815013885\n",
      "Epoch 4543, Loss: 0.7593030482530594, Final Batch Loss: 0.16974090039730072\n",
      "Epoch 4544, Loss: 0.8555079847574234, Final Batch Loss: 0.300092488527298\n",
      "Epoch 4545, Loss: 0.8072407245635986, Final Batch Loss: 0.2772722542285919\n",
      "Epoch 4546, Loss: 0.8176631331443787, Final Batch Loss: 0.19992242753505707\n",
      "Epoch 4547, Loss: 0.7067293748259544, Final Batch Loss: 0.10519642382860184\n",
      "Epoch 4548, Loss: 0.6914346143603325, Final Batch Loss: 0.12145379930734634\n",
      "Epoch 4549, Loss: 0.8718914538621902, Final Batch Loss: 0.24281081557273865\n",
      "Epoch 4550, Loss: 0.8415431976318359, Final Batch Loss: 0.20164132118225098\n",
      "Epoch 4551, Loss: 0.7002493143081665, Final Batch Loss: 0.22128017246723175\n",
      "Epoch 4552, Loss: 0.7765383422374725, Final Batch Loss: 0.18624398112297058\n",
      "Epoch 4553, Loss: 0.8043177872896194, Final Batch Loss: 0.19349405169487\n",
      "Epoch 4554, Loss: 0.7868908047676086, Final Batch Loss: 0.18972812592983246\n",
      "Epoch 4555, Loss: 0.7611126750707626, Final Batch Loss: 0.22065399587154388\n",
      "Epoch 4556, Loss: 0.713026374578476, Final Batch Loss: 0.1796930432319641\n",
      "Epoch 4557, Loss: 0.7911268472671509, Final Batch Loss: 0.1395827680826187\n",
      "Epoch 4558, Loss: 0.8358744531869888, Final Batch Loss: 0.2186293751001358\n",
      "Epoch 4559, Loss: 0.7356464713811874, Final Batch Loss: 0.15816421806812286\n",
      "Epoch 4560, Loss: 0.7774605751037598, Final Batch Loss: 0.22885794937610626\n",
      "Epoch 4561, Loss: 0.6840161681175232, Final Batch Loss: 0.19418765604496002\n",
      "Epoch 4562, Loss: 0.827645368874073, Final Batch Loss: 0.22136209905147552\n",
      "Epoch 4563, Loss: 0.8296458572149277, Final Batch Loss: 0.2772938907146454\n",
      "Epoch 4564, Loss: 0.7750034034252167, Final Batch Loss: 0.15174691379070282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4565, Loss: 0.8197595477104187, Final Batch Loss: 0.25639650225639343\n",
      "Epoch 4566, Loss: 0.8221640735864639, Final Batch Loss: 0.17306913435459137\n",
      "Epoch 4567, Loss: 0.7961457669734955, Final Batch Loss: 0.21120429039001465\n",
      "Epoch 4568, Loss: 0.7474959641695023, Final Batch Loss: 0.1809263676404953\n",
      "Epoch 4569, Loss: 0.7962273806333542, Final Batch Loss: 0.16799749433994293\n",
      "Epoch 4570, Loss: 0.8190481960773468, Final Batch Loss: 0.16000892221927643\n",
      "Epoch 4571, Loss: 0.7006601542234421, Final Batch Loss: 0.23649248480796814\n",
      "Epoch 4572, Loss: 0.8473779857158661, Final Batch Loss: 0.21555456519126892\n",
      "Epoch 4573, Loss: 0.8595103025436401, Final Batch Loss: 0.23863893747329712\n",
      "Epoch 4574, Loss: 0.9058763980865479, Final Batch Loss: 0.2066964954137802\n",
      "Epoch 4575, Loss: 0.7539674639701843, Final Batch Loss: 0.17412042617797852\n",
      "Epoch 4576, Loss: 0.7101278007030487, Final Batch Loss: 0.1776084005832672\n",
      "Epoch 4577, Loss: 0.8429161310195923, Final Batch Loss: 0.20391975343227386\n",
      "Epoch 4578, Loss: 0.7913927882909775, Final Batch Loss: 0.21175062656402588\n",
      "Epoch 4579, Loss: 0.7762877494096756, Final Batch Loss: 0.19779691100120544\n",
      "Epoch 4580, Loss: 0.7976670563220978, Final Batch Loss: 0.2077760100364685\n",
      "Epoch 4581, Loss: 0.8271229416131973, Final Batch Loss: 0.18805187940597534\n",
      "Epoch 4582, Loss: 0.8441535234451294, Final Batch Loss: 0.23196595907211304\n",
      "Epoch 4583, Loss: 0.8241675943136215, Final Batch Loss: 0.1711091548204422\n",
      "Epoch 4584, Loss: 0.8316061794757843, Final Batch Loss: 0.21351000666618347\n",
      "Epoch 4585, Loss: 0.7346107512712479, Final Batch Loss: 0.1282324194908142\n",
      "Epoch 4586, Loss: 0.7240607291460037, Final Batch Loss: 0.2092035561800003\n",
      "Epoch 4587, Loss: 0.7468278557062149, Final Batch Loss: 0.17997922003269196\n",
      "Epoch 4588, Loss: 0.7277878820896149, Final Batch Loss: 0.15870943665504456\n",
      "Epoch 4589, Loss: 0.689315989613533, Final Batch Loss: 0.14556443691253662\n",
      "Epoch 4590, Loss: 0.8352798968553543, Final Batch Loss: 0.16303317248821259\n",
      "Epoch 4591, Loss: 0.7845417410135269, Final Batch Loss: 0.17791451513767242\n",
      "Epoch 4592, Loss: 0.763388529419899, Final Batch Loss: 0.2208612859249115\n",
      "Epoch 4593, Loss: 0.7375534623861313, Final Batch Loss: 0.18714837729930878\n",
      "Epoch 4594, Loss: 0.6826477199792862, Final Batch Loss: 0.13866284489631653\n",
      "Epoch 4595, Loss: 0.677672266960144, Final Batch Loss: 0.1300228387117386\n",
      "Epoch 4596, Loss: 0.7627448886632919, Final Batch Loss: 0.16124427318572998\n",
      "Epoch 4597, Loss: 0.8190377503633499, Final Batch Loss: 0.2680768072605133\n",
      "Epoch 4598, Loss: 0.8152882605791092, Final Batch Loss: 0.18001416325569153\n",
      "Epoch 4599, Loss: 0.813229501247406, Final Batch Loss: 0.21879494190216064\n",
      "Epoch 4600, Loss: 0.7738762199878693, Final Batch Loss: 0.1652274876832962\n",
      "Epoch 4601, Loss: 0.6234201192855835, Final Batch Loss: 0.16131140291690826\n",
      "Epoch 4602, Loss: 0.708082839846611, Final Batch Loss: 0.1844528317451477\n",
      "Epoch 4603, Loss: 0.7427973449230194, Final Batch Loss: 0.14219065010547638\n",
      "Epoch 4604, Loss: 0.708419144153595, Final Batch Loss: 0.16491658985614777\n",
      "Epoch 4605, Loss: 0.7574845254421234, Final Batch Loss: 0.2052345722913742\n",
      "Epoch 4606, Loss: 0.84610515832901, Final Batch Loss: 0.20549564063549042\n",
      "Epoch 4607, Loss: 0.756925567984581, Final Batch Loss: 0.18367460370063782\n",
      "Epoch 4608, Loss: 0.6390542611479759, Final Batch Loss: 0.08161050826311111\n",
      "Epoch 4609, Loss: 0.7056706994771957, Final Batch Loss: 0.12556438148021698\n",
      "Epoch 4610, Loss: 0.7786595970392227, Final Batch Loss: 0.24152232706546783\n",
      "Epoch 4611, Loss: 0.6750274449586868, Final Batch Loss: 0.09166871011257172\n",
      "Epoch 4612, Loss: 0.8028006851673126, Final Batch Loss: 0.2530604302883148\n",
      "Epoch 4613, Loss: 0.79234579205513, Final Batch Loss: 0.25442588329315186\n",
      "Epoch 4614, Loss: 0.7501863166689873, Final Batch Loss: 0.22011137008666992\n",
      "Epoch 4615, Loss: 0.8250303864479065, Final Batch Loss: 0.24023158848285675\n",
      "Epoch 4616, Loss: 0.7688802182674408, Final Batch Loss: 0.18048331141471863\n",
      "Epoch 4617, Loss: 0.7405729740858078, Final Batch Loss: 0.1834867149591446\n",
      "Epoch 4618, Loss: 0.7967815846204758, Final Batch Loss: 0.21916420757770538\n",
      "Epoch 4619, Loss: 0.7930157482624054, Final Batch Loss: 0.20712848007678986\n",
      "Epoch 4620, Loss: 0.7601251304149628, Final Batch Loss: 0.2072172313928604\n",
      "Epoch 4621, Loss: 0.8001714050769806, Final Batch Loss: 0.28119954466819763\n",
      "Epoch 4622, Loss: 0.8916238993406296, Final Batch Loss: 0.3377615809440613\n",
      "Epoch 4623, Loss: 0.679904043674469, Final Batch Loss: 0.21239133179187775\n",
      "Epoch 4624, Loss: 0.7887639850378036, Final Batch Loss: 0.3009045720100403\n",
      "Epoch 4625, Loss: 0.7500519156455994, Final Batch Loss: 0.15389132499694824\n",
      "Epoch 4626, Loss: 0.8232972174882889, Final Batch Loss: 0.2462933510541916\n",
      "Epoch 4627, Loss: 0.8300517350435257, Final Batch Loss: 0.23910453915596008\n",
      "Epoch 4628, Loss: 0.6280020996928215, Final Batch Loss: 0.07553032785654068\n",
      "Epoch 4629, Loss: 0.8023717403411865, Final Batch Loss: 0.1890561282634735\n",
      "Epoch 4630, Loss: 0.7632157951593399, Final Batch Loss: 0.20435932278633118\n",
      "Epoch 4631, Loss: 0.8242976814508438, Final Batch Loss: 0.1448258012533188\n",
      "Epoch 4632, Loss: 0.7243874818086624, Final Batch Loss: 0.15360762178897858\n",
      "Epoch 4633, Loss: 0.7862420827150345, Final Batch Loss: 0.24510985612869263\n",
      "Epoch 4634, Loss: 0.6420855969190598, Final Batch Loss: 0.13956031203269958\n",
      "Epoch 4635, Loss: 0.8475374132394791, Final Batch Loss: 0.21424461901187897\n",
      "Epoch 4636, Loss: 0.7356864213943481, Final Batch Loss: 0.19883917272090912\n",
      "Epoch 4637, Loss: 0.6993284672498703, Final Batch Loss: 0.1639990359544754\n",
      "Epoch 4638, Loss: 0.8326181173324585, Final Batch Loss: 0.20773430168628693\n",
      "Epoch 4639, Loss: 0.7796859443187714, Final Batch Loss: 0.2523844838142395\n",
      "Epoch 4640, Loss: 0.6844784468412399, Final Batch Loss: 0.0915021151304245\n",
      "Epoch 4641, Loss: 0.7997052073478699, Final Batch Loss: 0.1796277016401291\n",
      "Epoch 4642, Loss: 0.8129986375570297, Final Batch Loss: 0.19523829221725464\n",
      "Epoch 4643, Loss: 0.8026418089866638, Final Batch Loss: 0.27256280183792114\n",
      "Epoch 4644, Loss: 0.7673744559288025, Final Batch Loss: 0.18855437636375427\n",
      "Epoch 4645, Loss: 0.8096147775650024, Final Batch Loss: 0.24206502735614777\n",
      "Epoch 4646, Loss: 0.8179989010095596, Final Batch Loss: 0.16206230223178864\n",
      "Epoch 4647, Loss: 0.7011795938014984, Final Batch Loss: 0.13194815814495087\n",
      "Epoch 4648, Loss: 0.8659831285476685, Final Batch Loss: 0.22177165746688843\n",
      "Epoch 4649, Loss: 0.7521812915802002, Final Batch Loss: 0.14609450101852417\n",
      "Epoch 4650, Loss: 0.7344146519899368, Final Batch Loss: 0.15367349982261658\n",
      "Epoch 4651, Loss: 0.8428880572319031, Final Batch Loss: 0.25974488258361816\n",
      "Epoch 4652, Loss: 0.7961363345384598, Final Batch Loss: 0.1446170210838318\n",
      "Epoch 4653, Loss: 0.834084764122963, Final Batch Loss: 0.25593301653862\n",
      "Epoch 4654, Loss: 0.660455659031868, Final Batch Loss: 0.15945813059806824\n",
      "Epoch 4655, Loss: 0.6799260675907135, Final Batch Loss: 0.13946878910064697\n",
      "Epoch 4656, Loss: 0.7933496832847595, Final Batch Loss: 0.27408167719841003\n",
      "Epoch 4657, Loss: 0.766310840845108, Final Batch Loss: 0.1631489247083664\n",
      "Epoch 4658, Loss: 0.7373688742518425, Final Batch Loss: 0.161226287484169\n",
      "Epoch 4659, Loss: 0.7417027652263641, Final Batch Loss: 0.17765553295612335\n",
      "Epoch 4660, Loss: 0.734251081943512, Final Batch Loss: 0.24167943000793457\n",
      "Epoch 4661, Loss: 0.6815435886383057, Final Batch Loss: 0.20275382697582245\n",
      "Epoch 4662, Loss: 0.6853387653827667, Final Batch Loss: 0.12300775945186615\n",
      "Epoch 4663, Loss: 0.8634272515773773, Final Batch Loss: 0.2817578613758087\n",
      "Epoch 4664, Loss: 0.8341452479362488, Final Batch Loss: 0.21473585069179535\n",
      "Epoch 4665, Loss: 0.8008212745189667, Final Batch Loss: 0.236328586935997\n",
      "Epoch 4666, Loss: 0.7690470665693283, Final Batch Loss: 0.20054838061332703\n",
      "Epoch 4667, Loss: 0.7322088181972504, Final Batch Loss: 0.16734161972999573\n",
      "Epoch 4668, Loss: 0.8342502117156982, Final Batch Loss: 0.26719754934310913\n",
      "Epoch 4669, Loss: 0.8415282368659973, Final Batch Loss: 0.15832971036434174\n",
      "Epoch 4670, Loss: 0.8436853289604187, Final Batch Loss: 0.23498596251010895\n",
      "Epoch 4671, Loss: 0.8642061650753021, Final Batch Loss: 0.25071367621421814\n",
      "Epoch 4672, Loss: 0.7475945800542831, Final Batch Loss: 0.16395039856433868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4673, Loss: 0.6858913898468018, Final Batch Loss: 0.13335703313350677\n",
      "Epoch 4674, Loss: 0.728084072470665, Final Batch Loss: 0.21144968271255493\n",
      "Epoch 4675, Loss: 0.6807893812656403, Final Batch Loss: 0.15831395983695984\n",
      "Epoch 4676, Loss: 0.8370793163776398, Final Batch Loss: 0.24066224694252014\n",
      "Epoch 4677, Loss: 0.841180294752121, Final Batch Loss: 0.2238043248653412\n",
      "Epoch 4678, Loss: 0.7054988592863083, Final Batch Loss: 0.1433410495519638\n",
      "Epoch 4679, Loss: 0.8925461620092392, Final Batch Loss: 0.19752137362957\n",
      "Epoch 4680, Loss: 0.7727579772472382, Final Batch Loss: 0.1681729406118393\n",
      "Epoch 4681, Loss: 0.8347740918397903, Final Batch Loss: 0.2667687237262726\n",
      "Epoch 4682, Loss: 0.7885138541460037, Final Batch Loss: 0.2254902720451355\n",
      "Epoch 4683, Loss: 0.7892804741859436, Final Batch Loss: 0.1823621243238449\n",
      "Epoch 4684, Loss: 0.7072395980358124, Final Batch Loss: 0.16574683785438538\n",
      "Epoch 4685, Loss: 0.8663592636585236, Final Batch Loss: 0.17516566812992096\n",
      "Epoch 4686, Loss: 0.8288247883319855, Final Batch Loss: 0.2556314468383789\n",
      "Epoch 4687, Loss: 0.770475834608078, Final Batch Loss: 0.15382076799869537\n",
      "Epoch 4688, Loss: 0.7469828873872757, Final Batch Loss: 0.18134552240371704\n",
      "Epoch 4689, Loss: 0.7660896182060242, Final Batch Loss: 0.21111303567886353\n",
      "Epoch 4690, Loss: 0.6946590840816498, Final Batch Loss: 0.1704995185136795\n",
      "Epoch 4691, Loss: 0.7281692028045654, Final Batch Loss: 0.1592540442943573\n",
      "Epoch 4692, Loss: 0.8289226442575455, Final Batch Loss: 0.22255994379520416\n",
      "Epoch 4693, Loss: 0.6977633237838745, Final Batch Loss: 0.18965885043144226\n",
      "Epoch 4694, Loss: 0.7801016494631767, Final Batch Loss: 0.1055024191737175\n",
      "Epoch 4695, Loss: 0.9082290083169937, Final Batch Loss: 0.3711894452571869\n",
      "Epoch 4696, Loss: 0.8342460840940475, Final Batch Loss: 0.33460819721221924\n",
      "Epoch 4697, Loss: 0.8460985273122787, Final Batch Loss: 0.23353873193264008\n",
      "Epoch 4698, Loss: 0.8364430665969849, Final Batch Loss: 0.16892701387405396\n",
      "Epoch 4699, Loss: 0.821247011423111, Final Batch Loss: 0.3555215299129486\n",
      "Epoch 4700, Loss: 0.7398067265748978, Final Batch Loss: 0.24067291617393494\n",
      "Epoch 4701, Loss: 0.9131622463464737, Final Batch Loss: 0.1784936785697937\n",
      "Epoch 4702, Loss: 0.8021920919418335, Final Batch Loss: 0.17229609191417694\n",
      "Epoch 4703, Loss: 0.8994184583425522, Final Batch Loss: 0.2703445553779602\n",
      "Epoch 4704, Loss: 0.6929308921098709, Final Batch Loss: 0.16406390070915222\n",
      "Epoch 4705, Loss: 0.7609381675720215, Final Batch Loss: 0.13192309439182281\n",
      "Epoch 4706, Loss: 0.6802919209003448, Final Batch Loss: 0.1542273610830307\n",
      "Epoch 4707, Loss: 0.7471597492694855, Final Batch Loss: 0.1750616580247879\n",
      "Epoch 4708, Loss: 0.6309792846441269, Final Batch Loss: 0.11532923579216003\n",
      "Epoch 4709, Loss: 0.7962412089109421, Final Batch Loss: 0.2019822895526886\n",
      "Epoch 4710, Loss: 0.8314820975065231, Final Batch Loss: 0.2627115547657013\n",
      "Epoch 4711, Loss: 0.7209613770246506, Final Batch Loss: 0.13004595041275024\n",
      "Epoch 4712, Loss: 0.7788375020027161, Final Batch Loss: 0.14406472444534302\n",
      "Epoch 4713, Loss: 1.0157127231359482, Final Batch Loss: 0.30040353536605835\n",
      "Epoch 4714, Loss: 0.8629997968673706, Final Batch Loss: 0.21083177626132965\n",
      "Epoch 4715, Loss: 0.7230852842330933, Final Batch Loss: 0.11311028897762299\n",
      "Epoch 4716, Loss: 0.837662547826767, Final Batch Loss: 0.19377641379833221\n",
      "Epoch 4717, Loss: 0.7458602339029312, Final Batch Loss: 0.17569999396800995\n",
      "Epoch 4718, Loss: 0.7291540205478668, Final Batch Loss: 0.13155966997146606\n",
      "Epoch 4719, Loss: 0.7961751520633698, Final Batch Loss: 0.1924942433834076\n",
      "Epoch 4720, Loss: 0.8043545335531235, Final Batch Loss: 0.24035629630088806\n",
      "Epoch 4721, Loss: 0.8818007707595825, Final Batch Loss: 0.22336570918560028\n",
      "Epoch 4722, Loss: 0.8837069869041443, Final Batch Loss: 0.33379632234573364\n",
      "Epoch 4723, Loss: 0.6513730362057686, Final Batch Loss: 0.1232127770781517\n",
      "Epoch 4724, Loss: 0.7185964435338974, Final Batch Loss: 0.20717234909534454\n",
      "Epoch 4725, Loss: 0.7104239165782928, Final Batch Loss: 0.15981416404247284\n",
      "Epoch 4726, Loss: 0.7894885838031769, Final Batch Loss: 0.21827270090579987\n",
      "Epoch 4727, Loss: 0.7697326391935349, Final Batch Loss: 0.14256112277507782\n",
      "Epoch 4728, Loss: 0.7800731956958771, Final Batch Loss: 0.15954329073429108\n",
      "Epoch 4729, Loss: 0.7213912010192871, Final Batch Loss: 0.14613649249076843\n",
      "Epoch 4730, Loss: 0.8670411705970764, Final Batch Loss: 0.22029554843902588\n",
      "Epoch 4731, Loss: 0.7459924668073654, Final Batch Loss: 0.16558043658733368\n",
      "Epoch 4732, Loss: 0.7344105094671249, Final Batch Loss: 0.19253353774547577\n",
      "Epoch 4733, Loss: 0.7876168191432953, Final Batch Loss: 0.22677087783813477\n",
      "Epoch 4734, Loss: 0.8201988935470581, Final Batch Loss: 0.19549834728240967\n",
      "Epoch 4735, Loss: 0.7435766309499741, Final Batch Loss: 0.21650473773479462\n",
      "Epoch 4736, Loss: 0.8691020011901855, Final Batch Loss: 0.22938963770866394\n",
      "Epoch 4737, Loss: 0.8121481239795685, Final Batch Loss: 0.23068992793560028\n",
      "Epoch 4738, Loss: 0.6538794487714767, Final Batch Loss: 0.12160836160182953\n",
      "Epoch 4739, Loss: 0.7128907889127731, Final Batch Loss: 0.18419817090034485\n",
      "Epoch 4740, Loss: 0.7200775146484375, Final Batch Loss: 0.14323319494724274\n",
      "Epoch 4741, Loss: 0.8481801599264145, Final Batch Loss: 0.22756947576999664\n",
      "Epoch 4742, Loss: 0.8398668020963669, Final Batch Loss: 0.2572677731513977\n",
      "Epoch 4743, Loss: 0.6770715713500977, Final Batch Loss: 0.16546304523944855\n",
      "Epoch 4744, Loss: 0.7789608389139175, Final Batch Loss: 0.20239028334617615\n",
      "Epoch 4745, Loss: 0.7549698352813721, Final Batch Loss: 0.15540842711925507\n",
      "Epoch 4746, Loss: 0.8636850565671921, Final Batch Loss: 0.22032961249351501\n",
      "Epoch 4747, Loss: 0.7313759624958038, Final Batch Loss: 0.15565168857574463\n",
      "Epoch 4748, Loss: 0.6868572235107422, Final Batch Loss: 0.15321728587150574\n",
      "Epoch 4749, Loss: 0.8198121786117554, Final Batch Loss: 0.15033361315727234\n",
      "Epoch 4750, Loss: 0.741327553987503, Final Batch Loss: 0.15831676125526428\n",
      "Epoch 4751, Loss: 0.5856660082936287, Final Batch Loss: 0.09444770961999893\n",
      "Epoch 4752, Loss: 0.7364672124385834, Final Batch Loss: 0.12157554924488068\n",
      "Epoch 4753, Loss: 0.7478538900613785, Final Batch Loss: 0.2092292755842209\n",
      "Epoch 4754, Loss: 0.8708540350198746, Final Batch Loss: 0.26029062271118164\n",
      "Epoch 4755, Loss: 0.6610079854726791, Final Batch Loss: 0.20758506655693054\n",
      "Epoch 4756, Loss: 0.7923683822154999, Final Batch Loss: 0.16117514669895172\n",
      "Epoch 4757, Loss: 0.836153969168663, Final Batch Loss: 0.22258727252483368\n",
      "Epoch 4758, Loss: 0.8449519127607346, Final Batch Loss: 0.2299276441335678\n",
      "Epoch 4759, Loss: 0.8121711164712906, Final Batch Loss: 0.16216494143009186\n",
      "Epoch 4760, Loss: 0.757927730679512, Final Batch Loss: 0.1873963326215744\n",
      "Epoch 4761, Loss: 0.9114935994148254, Final Batch Loss: 0.185219869017601\n",
      "Epoch 4762, Loss: 0.9198069125413895, Final Batch Loss: 0.2254249006509781\n",
      "Epoch 4763, Loss: 0.9651270359754562, Final Batch Loss: 0.27705007791519165\n",
      "Epoch 4764, Loss: 0.6945116519927979, Final Batch Loss: 0.12852026522159576\n",
      "Epoch 4765, Loss: 0.7604334503412247, Final Batch Loss: 0.23965871334075928\n",
      "Epoch 4766, Loss: 0.901499330997467, Final Batch Loss: 0.21052630245685577\n",
      "Epoch 4767, Loss: 0.782268762588501, Final Batch Loss: 0.22635509073734283\n",
      "Epoch 4768, Loss: 0.8748680651187897, Final Batch Loss: 0.17024876177310944\n",
      "Epoch 4769, Loss: 0.7156993001699448, Final Batch Loss: 0.17723503708839417\n",
      "Epoch 4770, Loss: 0.9813608974218369, Final Batch Loss: 0.2775544226169586\n",
      "Epoch 4771, Loss: 0.87480528652668, Final Batch Loss: 0.18412882089614868\n",
      "Epoch 4772, Loss: 0.7962853908538818, Final Batch Loss: 0.20516330003738403\n",
      "Epoch 4773, Loss: 0.7771421745419502, Final Batch Loss: 0.11312120407819748\n",
      "Epoch 4774, Loss: 0.8091181367635727, Final Batch Loss: 0.20269277691841125\n",
      "Epoch 4775, Loss: 0.8358655422925949, Final Batch Loss: 0.18673644959926605\n",
      "Epoch 4776, Loss: 0.9035730808973312, Final Batch Loss: 0.26045650243759155\n",
      "Epoch 4777, Loss: 0.8513495773077011, Final Batch Loss: 0.2227318286895752\n",
      "Epoch 4778, Loss: 0.7667953372001648, Final Batch Loss: 0.19575932621955872\n",
      "Epoch 4779, Loss: 0.7117244750261307, Final Batch Loss: 0.18052923679351807\n",
      "Epoch 4780, Loss: 0.7960606068372726, Final Batch Loss: 0.1659523993730545\n",
      "Epoch 4781, Loss: 0.636914923787117, Final Batch Loss: 0.17835280299186707\n",
      "Epoch 4782, Loss: 0.7833143323659897, Final Batch Loss: 0.20692014694213867\n",
      "Epoch 4783, Loss: 0.8381288349628448, Final Batch Loss: 0.2628571689128876\n",
      "Epoch 4784, Loss: 0.88346366584301, Final Batch Loss: 0.2285732924938202\n",
      "Epoch 4785, Loss: 0.7885966598987579, Final Batch Loss: 0.23207715153694153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4786, Loss: 0.7130503803491592, Final Batch Loss: 0.15755538642406464\n",
      "Epoch 4787, Loss: 0.820206418633461, Final Batch Loss: 0.18659652769565582\n",
      "Epoch 4788, Loss: 0.7470470815896988, Final Batch Loss: 0.1487952321767807\n",
      "Epoch 4789, Loss: 0.6370075345039368, Final Batch Loss: 0.12558525800704956\n",
      "Epoch 4790, Loss: 0.6706331893801689, Final Batch Loss: 0.11607753485441208\n",
      "Epoch 4791, Loss: 0.883887991309166, Final Batch Loss: 0.30394428968429565\n",
      "Epoch 4792, Loss: 0.7617763429880142, Final Batch Loss: 0.12748165428638458\n",
      "Epoch 4793, Loss: 0.7847517132759094, Final Batch Loss: 0.18554653227329254\n",
      "Epoch 4794, Loss: 0.7364758998155594, Final Batch Loss: 0.1372622847557068\n",
      "Epoch 4795, Loss: 0.794413149356842, Final Batch Loss: 0.2283088117837906\n",
      "Epoch 4796, Loss: 0.8554303795099258, Final Batch Loss: 0.169374480843544\n",
      "Epoch 4797, Loss: 0.7444017827510834, Final Batch Loss: 0.18277089297771454\n",
      "Epoch 4798, Loss: 0.6836745589971542, Final Batch Loss: 0.20513896644115448\n",
      "Epoch 4799, Loss: 0.8100919649004936, Final Batch Loss: 0.3199635148048401\n",
      "Epoch 4800, Loss: 0.6854331195354462, Final Batch Loss: 0.1554870307445526\n",
      "Epoch 4801, Loss: 0.7709889262914658, Final Batch Loss: 0.17762050032615662\n",
      "Epoch 4802, Loss: 0.8081384152173996, Final Batch Loss: 0.2536105215549469\n",
      "Epoch 4803, Loss: 0.8176142871379852, Final Batch Loss: 0.1417665332555771\n",
      "Epoch 4804, Loss: 0.8249654173851013, Final Batch Loss: 0.21305552124977112\n",
      "Epoch 4805, Loss: 0.6389606669545174, Final Batch Loss: 0.11104374378919601\n",
      "Epoch 4806, Loss: 0.8111724406480789, Final Batch Loss: 0.25044041872024536\n",
      "Epoch 4807, Loss: 0.7478418052196503, Final Batch Loss: 0.18612198531627655\n",
      "Epoch 4808, Loss: 0.8486705422401428, Final Batch Loss: 0.21108223497867584\n",
      "Epoch 4809, Loss: 0.8272639065980911, Final Batch Loss: 0.24324004352092743\n",
      "Epoch 4810, Loss: 0.6483428627252579, Final Batch Loss: 0.16022588312625885\n",
      "Epoch 4811, Loss: 0.7858918011188507, Final Batch Loss: 0.2640744745731354\n",
      "Epoch 4812, Loss: 0.7244277223944664, Final Batch Loss: 0.11210408061742783\n",
      "Epoch 4813, Loss: 0.787890613079071, Final Batch Loss: 0.20983265340328217\n",
      "Epoch 4814, Loss: 0.792953297495842, Final Batch Loss: 0.1777247041463852\n",
      "Epoch 4815, Loss: 0.7192845791578293, Final Batch Loss: 0.23205363750457764\n",
      "Epoch 4816, Loss: 0.7609228938817978, Final Batch Loss: 0.1309722363948822\n",
      "Epoch 4817, Loss: 0.7474417686462402, Final Batch Loss: 0.18665723502635956\n",
      "Epoch 4818, Loss: 0.7621446028351784, Final Batch Loss: 0.09770750254392624\n",
      "Epoch 4819, Loss: 0.8531540185213089, Final Batch Loss: 0.3510003685951233\n",
      "Epoch 4820, Loss: 0.7690856009721756, Final Batch Loss: 0.18926163017749786\n",
      "Epoch 4821, Loss: 0.7736643999814987, Final Batch Loss: 0.2499774992465973\n",
      "Epoch 4822, Loss: 0.7592928409576416, Final Batch Loss: 0.21577392518520355\n",
      "Epoch 4823, Loss: 0.7820641398429871, Final Batch Loss: 0.21806783974170685\n",
      "Epoch 4824, Loss: 0.82661072909832, Final Batch Loss: 0.20319800078868866\n",
      "Epoch 4825, Loss: 0.751359835267067, Final Batch Loss: 0.1505553275346756\n",
      "Epoch 4826, Loss: 0.8933972269296646, Final Batch Loss: 0.266207754611969\n",
      "Epoch 4827, Loss: 0.8573736846446991, Final Batch Loss: 0.17807349562644958\n",
      "Epoch 4828, Loss: 0.7181790992617607, Final Batch Loss: 0.21829259395599365\n",
      "Epoch 4829, Loss: 0.6984589397907257, Final Batch Loss: 0.1389700174331665\n",
      "Epoch 4830, Loss: 0.776018813252449, Final Batch Loss: 0.1422659158706665\n",
      "Epoch 4831, Loss: 0.7848329991102219, Final Batch Loss: 0.19961531460285187\n",
      "Epoch 4832, Loss: 0.7790191918611526, Final Batch Loss: 0.1973116397857666\n",
      "Epoch 4833, Loss: 0.7550236135721207, Final Batch Loss: 0.1497756391763687\n",
      "Epoch 4834, Loss: 0.8240202814340591, Final Batch Loss: 0.18551291525363922\n",
      "Epoch 4835, Loss: 0.6701022088527679, Final Batch Loss: 0.15572252869606018\n",
      "Epoch 4836, Loss: 0.8059914410114288, Final Batch Loss: 0.19866789877414703\n",
      "Epoch 4837, Loss: 0.665556401014328, Final Batch Loss: 0.15193484723567963\n",
      "Epoch 4838, Loss: 0.7366458624601364, Final Batch Loss: 0.14730143547058105\n",
      "Epoch 4839, Loss: 0.7771065831184387, Final Batch Loss: 0.17822739481925964\n",
      "Epoch 4840, Loss: 0.8449049890041351, Final Batch Loss: 0.2470095455646515\n",
      "Epoch 4841, Loss: 0.8637047111988068, Final Batch Loss: 0.22325144708156586\n",
      "Epoch 4842, Loss: 0.6750150099396706, Final Batch Loss: 0.12457599490880966\n",
      "Epoch 4843, Loss: 0.620951384305954, Final Batch Loss: 0.13704299926757812\n",
      "Epoch 4844, Loss: 0.5997172594070435, Final Batch Loss: 0.12585553526878357\n",
      "Epoch 4845, Loss: 0.7056551575660706, Final Batch Loss: 0.2411046326160431\n",
      "Epoch 4846, Loss: 0.7365408390760422, Final Batch Loss: 0.17525434494018555\n",
      "Epoch 4847, Loss: 0.7807978838682175, Final Batch Loss: 0.17081692814826965\n",
      "Epoch 4848, Loss: 0.7030466943979263, Final Batch Loss: 0.12679696083068848\n",
      "Epoch 4849, Loss: 0.7792176604270935, Final Batch Loss: 0.1547691822052002\n",
      "Epoch 4850, Loss: 0.8353392481803894, Final Batch Loss: 0.2467203438282013\n",
      "Epoch 4851, Loss: 0.7563449293375015, Final Batch Loss: 0.1620764583349228\n",
      "Epoch 4852, Loss: 0.7832745462656021, Final Batch Loss: 0.18920953571796417\n",
      "Epoch 4853, Loss: 0.7230812683701515, Final Batch Loss: 0.12303466349840164\n",
      "Epoch 4854, Loss: 0.7415781617164612, Final Batch Loss: 0.14718681573867798\n",
      "Epoch 4855, Loss: 0.7335568219423294, Final Batch Loss: 0.18427689373493195\n",
      "Epoch 4856, Loss: 0.7545072585344315, Final Batch Loss: 0.16373376548290253\n",
      "Epoch 4857, Loss: 0.746306449174881, Final Batch Loss: 0.1794402301311493\n",
      "Epoch 4858, Loss: 0.6798201203346252, Final Batch Loss: 0.17982658743858337\n",
      "Epoch 4859, Loss: 0.8110428303480148, Final Batch Loss: 0.1281675100326538\n",
      "Epoch 4860, Loss: 0.6584650799632072, Final Batch Loss: 0.1030443087220192\n",
      "Epoch 4861, Loss: 0.752352312207222, Final Batch Loss: 0.22453126311302185\n",
      "Epoch 4862, Loss: 0.8814755082130432, Final Batch Loss: 0.2441474050283432\n",
      "Epoch 4863, Loss: 0.6898617893457413, Final Batch Loss: 0.13139435648918152\n",
      "Epoch 4864, Loss: 0.8234949856996536, Final Batch Loss: 0.22229203581809998\n",
      "Epoch 4865, Loss: 0.7833625972270966, Final Batch Loss: 0.2188885509967804\n",
      "Epoch 4866, Loss: 0.6933995336294174, Final Batch Loss: 0.1809224635362625\n",
      "Epoch 4867, Loss: 0.7498842924833298, Final Batch Loss: 0.18681836128234863\n",
      "Epoch 4868, Loss: 0.7449421137571335, Final Batch Loss: 0.15114149451255798\n",
      "Epoch 4869, Loss: 0.7323278337717056, Final Batch Loss: 0.21594060957431793\n",
      "Epoch 4870, Loss: 0.6861321553587914, Final Batch Loss: 0.11019367724657059\n",
      "Epoch 4871, Loss: 0.8473255038261414, Final Batch Loss: 0.2764207124710083\n",
      "Epoch 4872, Loss: 0.7737907022237778, Final Batch Loss: 0.1580953449010849\n",
      "Epoch 4873, Loss: 0.8176307380199432, Final Batch Loss: 0.24809569120407104\n",
      "Epoch 4874, Loss: 0.6851833611726761, Final Batch Loss: 0.17866796255111694\n",
      "Epoch 4875, Loss: 0.7604502439498901, Final Batch Loss: 0.21299122273921967\n",
      "Epoch 4876, Loss: 0.8212096989154816, Final Batch Loss: 0.2883899211883545\n",
      "Epoch 4877, Loss: 0.7901318073272705, Final Batch Loss: 0.19323547184467316\n",
      "Epoch 4878, Loss: 0.8654302656650543, Final Batch Loss: 0.25213658809661865\n",
      "Epoch 4879, Loss: 0.9046567231416702, Final Batch Loss: 0.29216131567955017\n",
      "Epoch 4880, Loss: 0.7695567905902863, Final Batch Loss: 0.2094472497701645\n",
      "Epoch 4881, Loss: 0.8674870729446411, Final Batch Loss: 0.2363658845424652\n",
      "Epoch 4882, Loss: 0.7947670817375183, Final Batch Loss: 0.15059897303581238\n",
      "Epoch 4883, Loss: 0.7682730704545975, Final Batch Loss: 0.19168314337730408\n",
      "Epoch 4884, Loss: 0.8063536286354065, Final Batch Loss: 0.22288689017295837\n",
      "Epoch 4885, Loss: 0.713390126824379, Final Batch Loss: 0.15350426733493805\n",
      "Epoch 4886, Loss: 0.7842804938554764, Final Batch Loss: 0.2416689097881317\n",
      "Epoch 4887, Loss: 0.7323649972677231, Final Batch Loss: 0.1387505829334259\n",
      "Epoch 4888, Loss: 0.8104190528392792, Final Batch Loss: 0.18742309510707855\n",
      "Epoch 4889, Loss: 0.8093194663524628, Final Batch Loss: 0.14865456521511078\n",
      "Epoch 4890, Loss: 0.6683942526578903, Final Batch Loss: 0.15684393048286438\n",
      "Epoch 4891, Loss: 0.7465092241764069, Final Batch Loss: 0.16574053466320038\n",
      "Epoch 4892, Loss: 0.797453299164772, Final Batch Loss: 0.15200702846050262\n",
      "Epoch 4893, Loss: 0.7491968870162964, Final Batch Loss: 0.2036527693271637\n",
      "Epoch 4894, Loss: 0.7935611009597778, Final Batch Loss: 0.23220409452915192\n",
      "Epoch 4895, Loss: 0.8783627152442932, Final Batch Loss: 0.2108822464942932\n",
      "Epoch 4896, Loss: 0.8260304182767868, Final Batch Loss: 0.268004447221756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4897, Loss: 0.6965188533067703, Final Batch Loss: 0.17931023240089417\n",
      "Epoch 4898, Loss: 0.8298775553703308, Final Batch Loss: 0.24044086039066315\n",
      "Epoch 4899, Loss: 0.6869909763336182, Final Batch Loss: 0.16733165085315704\n",
      "Epoch 4900, Loss: 0.8044253438711166, Final Batch Loss: 0.18309861421585083\n",
      "Epoch 4901, Loss: 0.8681163191795349, Final Batch Loss: 0.16053734719753265\n",
      "Epoch 4902, Loss: 0.7759074717760086, Final Batch Loss: 0.29072487354278564\n",
      "Epoch 4903, Loss: 0.7508832961320877, Final Batch Loss: 0.20749405026435852\n",
      "Epoch 4904, Loss: 0.7851029485464096, Final Batch Loss: 0.19196248054504395\n",
      "Epoch 4905, Loss: 0.7341313511133194, Final Batch Loss: 0.17210440337657928\n",
      "Epoch 4906, Loss: 0.7424214035272598, Final Batch Loss: 0.21976277232170105\n",
      "Epoch 4907, Loss: 0.9466035664081573, Final Batch Loss: 0.2737135589122772\n",
      "Epoch 4908, Loss: 0.7988851219415665, Final Batch Loss: 0.215914785861969\n",
      "Epoch 4909, Loss: 0.7831306755542755, Final Batch Loss: 0.18221580982208252\n",
      "Epoch 4910, Loss: 0.7469821274280548, Final Batch Loss: 0.279491126537323\n",
      "Epoch 4911, Loss: 0.7234825640916824, Final Batch Loss: 0.17323517799377441\n",
      "Epoch 4912, Loss: 0.7352961301803589, Final Batch Loss: 0.16552360355854034\n",
      "Epoch 4913, Loss: 0.7461743950843811, Final Batch Loss: 0.16729214787483215\n",
      "Epoch 4914, Loss: 0.6777473017573357, Final Batch Loss: 0.12110991030931473\n",
      "Epoch 4915, Loss: 0.8185206800699234, Final Batch Loss: 0.2692303955554962\n",
      "Epoch 4916, Loss: 0.8271780014038086, Final Batch Loss: 0.20719441771507263\n",
      "Epoch 4917, Loss: 0.7264042347669601, Final Batch Loss: 0.15767505764961243\n",
      "Epoch 4918, Loss: 0.7671225965023041, Final Batch Loss: 0.24834953248500824\n",
      "Epoch 4919, Loss: 0.8014081418514252, Final Batch Loss: 0.1542486995458603\n",
      "Epoch 4920, Loss: 0.8307262063026428, Final Batch Loss: 0.21395014226436615\n",
      "Epoch 4921, Loss: 0.6952204704284668, Final Batch Loss: 0.13261230289936066\n",
      "Epoch 4922, Loss: 0.7436670660972595, Final Batch Loss: 0.17663338780403137\n",
      "Epoch 4923, Loss: 0.7235689610242844, Final Batch Loss: 0.19139544665813446\n",
      "Epoch 4924, Loss: 0.7246157079935074, Final Batch Loss: 0.14497463405132294\n",
      "Epoch 4925, Loss: 0.7726467251777649, Final Batch Loss: 0.19138628244400024\n",
      "Epoch 4926, Loss: 0.689601942896843, Final Batch Loss: 0.1884191483259201\n",
      "Epoch 4927, Loss: 0.8466747999191284, Final Batch Loss: 0.24024023115634918\n",
      "Epoch 4928, Loss: 0.8142889440059662, Final Batch Loss: 0.18879562616348267\n",
      "Epoch 4929, Loss: 0.8243173807859421, Final Batch Loss: 0.2859283685684204\n",
      "Epoch 4930, Loss: 0.8558817654848099, Final Batch Loss: 0.19496804475784302\n",
      "Epoch 4931, Loss: 0.6440857797861099, Final Batch Loss: 0.17896687984466553\n",
      "Epoch 4932, Loss: 0.6362250447273254, Final Batch Loss: 0.1545436531305313\n",
      "Epoch 4933, Loss: 0.7278197109699249, Final Batch Loss: 0.15343759953975677\n",
      "Epoch 4934, Loss: 0.96964530646801, Final Batch Loss: 0.2627551555633545\n",
      "Epoch 4935, Loss: 0.762270137667656, Final Batch Loss: 0.19707833230495453\n",
      "Epoch 4936, Loss: 0.7189379632472992, Final Batch Loss: 0.15833646059036255\n",
      "Epoch 4937, Loss: 0.7732097059488297, Final Batch Loss: 0.2315165102481842\n",
      "Epoch 4938, Loss: 0.8281253576278687, Final Batch Loss: 0.1969182938337326\n",
      "Epoch 4939, Loss: 0.7140473872423172, Final Batch Loss: 0.2219758778810501\n",
      "Epoch 4940, Loss: 0.8494459986686707, Final Batch Loss: 0.3256138861179352\n",
      "Epoch 4941, Loss: 0.7752225250005722, Final Batch Loss: 0.298353374004364\n",
      "Epoch 4942, Loss: 0.9165909588336945, Final Batch Loss: 0.17668595910072327\n",
      "Epoch 4943, Loss: 0.7929135411977768, Final Batch Loss: 0.19731713831424713\n",
      "Epoch 4944, Loss: 0.8489270210266113, Final Batch Loss: 0.3257441222667694\n",
      "Epoch 4945, Loss: 0.7188560217618942, Final Batch Loss: 0.19136358797550201\n",
      "Epoch 4946, Loss: 0.778457909822464, Final Batch Loss: 0.15911667048931122\n",
      "Epoch 4947, Loss: 0.7565192580223083, Final Batch Loss: 0.17349368333816528\n",
      "Epoch 4948, Loss: 0.8066941648721695, Final Batch Loss: 0.22900506854057312\n",
      "Epoch 4949, Loss: 0.703540712594986, Final Batch Loss: 0.14089348912239075\n",
      "Epoch 4950, Loss: 0.6531815528869629, Final Batch Loss: 0.1652858704328537\n",
      "Epoch 4951, Loss: 0.6561260968446732, Final Batch Loss: 0.1341826319694519\n",
      "Epoch 4952, Loss: 0.7078206390142441, Final Batch Loss: 0.1813891977071762\n",
      "Epoch 4953, Loss: 0.8292185962200165, Final Batch Loss: 0.19922739267349243\n",
      "Epoch 4954, Loss: 0.7588775157928467, Final Batch Loss: 0.2480316311120987\n",
      "Epoch 4955, Loss: 0.7251593172550201, Final Batch Loss: 0.2269514501094818\n",
      "Epoch 4956, Loss: 0.7791377753019333, Final Batch Loss: 0.18947111070156097\n",
      "Epoch 4957, Loss: 0.8563425093889236, Final Batch Loss: 0.21659013628959656\n",
      "Epoch 4958, Loss: 0.730473205447197, Final Batch Loss: 0.0819491297006607\n",
      "Epoch 4959, Loss: 0.8990372866392136, Final Batch Loss: 0.16811665892601013\n",
      "Epoch 4960, Loss: 0.826118528842926, Final Batch Loss: 0.2091638147830963\n",
      "Epoch 4961, Loss: 0.6539982259273529, Final Batch Loss: 0.12455163896083832\n",
      "Epoch 4962, Loss: 0.8180811852216721, Final Batch Loss: 0.2202429622411728\n",
      "Epoch 4963, Loss: 0.9104325473308563, Final Batch Loss: 0.32041338086128235\n",
      "Epoch 4964, Loss: 0.7954208850860596, Final Batch Loss: 0.18752475082874298\n",
      "Epoch 4965, Loss: 0.713209331035614, Final Batch Loss: 0.2234150618314743\n",
      "Epoch 4966, Loss: 0.7978314608335495, Final Batch Loss: 0.14577637612819672\n",
      "Epoch 4967, Loss: 0.7533779293298721, Final Batch Loss: 0.13833223283290863\n",
      "Epoch 4968, Loss: 0.7617897689342499, Final Batch Loss: 0.17306505143642426\n",
      "Epoch 4969, Loss: 0.7985535562038422, Final Batch Loss: 0.20969054102897644\n",
      "Epoch 4970, Loss: 0.6884540617465973, Final Batch Loss: 0.16980406641960144\n",
      "Epoch 4971, Loss: 0.7844651490449905, Final Batch Loss: 0.23506951332092285\n",
      "Epoch 4972, Loss: 0.7097120434045792, Final Batch Loss: 0.13840937614440918\n",
      "Epoch 4973, Loss: 0.8226874321699142, Final Batch Loss: 0.21123720705509186\n",
      "Epoch 4974, Loss: 0.8638132363557816, Final Batch Loss: 0.1865844577550888\n",
      "Epoch 4975, Loss: 0.70340596139431, Final Batch Loss: 0.1924964040517807\n",
      "Epoch 4976, Loss: 0.7654382884502411, Final Batch Loss: 0.16629719734191895\n",
      "Epoch 4977, Loss: 0.7384242117404938, Final Batch Loss: 0.14460988342761993\n",
      "Epoch 4978, Loss: 0.8622331321239471, Final Batch Loss: 0.23678764700889587\n",
      "Epoch 4979, Loss: 0.7873253226280212, Final Batch Loss: 0.24456578493118286\n",
      "Epoch 4980, Loss: 0.8261159062385559, Final Batch Loss: 0.191241055727005\n",
      "Epoch 4981, Loss: 0.7802369296550751, Final Batch Loss: 0.1603786200284958\n",
      "Epoch 4982, Loss: 0.804654061794281, Final Batch Loss: 0.15113991498947144\n",
      "Epoch 4983, Loss: 0.7120850533246994, Final Batch Loss: 0.15726399421691895\n",
      "Epoch 4984, Loss: 0.9321388006210327, Final Batch Loss: 0.26954782009124756\n",
      "Epoch 4985, Loss: 0.6323912888765335, Final Batch Loss: 0.10772642493247986\n",
      "Epoch 4986, Loss: 0.7447045743465424, Final Batch Loss: 0.1811981350183487\n",
      "Epoch 4987, Loss: 0.656362920999527, Final Batch Loss: 0.15125931799411774\n",
      "Epoch 4988, Loss: 0.7365928143262863, Final Batch Loss: 0.1741824746131897\n",
      "Epoch 4989, Loss: 0.8695165067911148, Final Batch Loss: 0.16058094799518585\n",
      "Epoch 4990, Loss: 0.7936861664056778, Final Batch Loss: 0.13649655878543854\n",
      "Epoch 4991, Loss: 0.8271719366312027, Final Batch Loss: 0.15449568629264832\n",
      "Epoch 4992, Loss: 0.7399405539035797, Final Batch Loss: 0.1534462422132492\n",
      "Epoch 4993, Loss: 0.6998895108699799, Final Batch Loss: 0.20810829102993011\n",
      "Epoch 4994, Loss: 0.8722510039806366, Final Batch Loss: 0.34484589099884033\n",
      "Epoch 4995, Loss: 0.674827940762043, Final Batch Loss: 0.18809659779071808\n",
      "Epoch 4996, Loss: 0.6843615919351578, Final Batch Loss: 0.15938135981559753\n",
      "Epoch 4997, Loss: 0.6668080687522888, Final Batch Loss: 0.16364534199237823\n",
      "Epoch 4998, Loss: 0.6446132510900497, Final Batch Loss: 0.14677974581718445\n",
      "Epoch 4999, Loss: 0.6826738715171814, Final Batch Loss: 0.19608117640018463\n",
      "Epoch 5000, Loss: 1.0144269168376923, Final Batch Loss: 0.4472353458404541\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  0 15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  7  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  5  0  0  2  0  0  0  0  0  4  0  0  1  0  0  1]\n",
      " [ 0  0  0  0  0  0 10  1  0  0  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  4  0  0  0  0  0  1  0  0  2  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0 13  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 11  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  3]\n",
      " [ 1  0  0  0  1  0  0  0  0  0  0  0  4  0  0  0  0  0  1  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0]\n",
      " [ 0  0  3  0  0  0  0  0  0  0  0  1  0  0  3  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 14  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  1  0  0  0  0  0  0  0  0 11  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 13  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.95455   1.00000   0.97674        21\n",
      "           1    0.88889   1.00000   0.94118         8\n",
      "           2    0.33333   0.66667   0.44444         3\n",
      "           3    1.00000   1.00000   1.00000        15\n",
      "           4    0.87500   0.87500   0.87500         8\n",
      "           5    0.83333   0.35714   0.50000        14\n",
      "           6    1.00000   0.83333   0.90909        12\n",
      "           7    0.88889   1.00000   0.94118         8\n",
      "           8    0.57143   0.50000   0.53333         8\n",
      "           9    1.00000   1.00000   1.00000        13\n",
      "          10    0.91667   1.00000   0.95652        11\n",
      "          11    0.83333   0.62500   0.71429         8\n",
      "          12    1.00000   0.57143   0.72727         7\n",
      "          13    0.85714   0.85714   0.85714         7\n",
      "          14    0.37500   0.42857   0.40000         7\n",
      "          15    1.00000   1.00000   1.00000        14\n",
      "          16    1.00000   1.00000   1.00000         8\n",
      "          17    0.73333   0.84615   0.78571        13\n",
      "          18    0.92857   1.00000   0.96296        13\n",
      "          19    1.00000   1.00000   1.00000        11\n",
      "          20    0.68750   1.00000   0.81481        11\n",
      "\n",
      "    accuracy                        0.86364       220\n",
      "   macro avg    0.84176   0.83621   0.82570       220\n",
      "weighted avg    0.87749   0.86364   0.85865       220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.train()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Fake Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20,000 epochs DEFAULT PARAMETERS FOR THRESHOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (gen): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=110, out_features=80, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=80, out_features=60, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=60, out_features=50, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Linear(in_features=50, out_features=33, bias=True)\n",
       "    (4): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator(z_dim = 110)\n",
    "load_model(gen, \"3 Label 7 Subject GAN Ablation_gen.param\")\n",
    "gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(X_test)\n",
    "latent_vectors = get_noise(size, 100)\n",
    "act_vectors = get_act_matrix(size, 3)\n",
    "usr_vectors = get_usr_matrix(size, 7)\n",
    "\n",
    "fake_labels = []\n",
    "\n",
    "for k in range(size):\n",
    "    if act_vectors[0][k] == 0 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(0)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(1)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(2)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(3)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(4)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(5)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(6)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(7)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(8)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 3:\n",
    "        fake_labels.append(9)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 3:\n",
    "        fake_labels.append(10)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 3:\n",
    "        fake_labels.append(11)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 4:\n",
    "        fake_labels.append(12)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 4:\n",
    "        fake_labels.append(13)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 4:\n",
    "        fake_labels.append(14)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 5:\n",
    "        fake_labels.append(15)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 5:\n",
    "        fake_labels.append(16)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 5:\n",
    "        fake_labels.append(17)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 6:\n",
    "        fake_labels.append(18)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 6:\n",
    "        fake_labels.append(19)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 6:\n",
    "        fake_labels.append(20)\n",
    "        \n",
    "fake_labels = np.asarray(fake_labels)\n",
    "to_gen = torch.cat((latent_vectors, act_vectors[1], usr_vectors[1]), 1)\n",
    "fake_features = gen(to_gen).detach()\n",
    "#fake_features = gen(to_gen).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  8  0  0  4  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 11  0  0  0  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 14  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.33333   1.00000   0.50000         5\n",
      "           1    1.00000   1.00000   1.00000        17\n",
      "           2    1.00000   1.00000   1.00000        17\n",
      "           3    0.55000   1.00000   0.70968        11\n",
      "           4    1.00000   1.00000   1.00000        11\n",
      "           5    1.00000   1.00000   1.00000        12\n",
      "           6    0.00000   0.00000   0.00000         9\n",
      "           7    0.00000   0.00000   0.00000         8\n",
      "           8    1.00000   1.00000   1.00000         9\n",
      "           9    0.00000   0.00000   0.00000        10\n",
      "          10    0.33333   0.33333   0.33333        12\n",
      "          11    1.00000   1.00000   1.00000        11\n",
      "          12    0.00000   0.00000   0.00000        10\n",
      "          13    1.00000   1.00000   1.00000         9\n",
      "          14    1.00000   1.00000   1.00000         9\n",
      "          15    1.00000   1.00000   1.00000        10\n",
      "          16    1.00000   1.00000   1.00000        14\n",
      "          17    1.00000   1.00000   1.00000         5\n",
      "          18    1.00000   1.00000   1.00000        11\n",
      "          19    1.00000   1.00000   1.00000         7\n",
      "          20    1.00000   1.00000   1.00000        13\n",
      "\n",
      "    accuracy                        0.79545       220\n",
      "   macro avg    0.72460   0.77778   0.74014       220\n",
      "weighted avg    0.75780   0.79545   0.76957       220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5, zero_division = 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
