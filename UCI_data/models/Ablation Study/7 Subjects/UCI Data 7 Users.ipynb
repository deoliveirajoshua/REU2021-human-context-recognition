{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '90 tBodyAccJerk-max()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>42 tGravityAcc-mean()-Y</th>\n",
       "      <th>43 tGravityAcc-mean()-Z</th>\n",
       "      <th>51 tGravityAcc-max()-Y</th>\n",
       "      <th>52 tGravityAcc-max()-Z</th>\n",
       "      <th>54 tGravityAcc-min()-Y</th>\n",
       "      <th>55 tGravityAcc-min()-Z</th>\n",
       "      <th>56 tGravityAcc-sma()</th>\n",
       "      <th>58 tGravityAcc-energy()-Y</th>\n",
       "      <th>59 tGravityAcc-energy()-Z</th>\n",
       "      <th>475 fBodyGyro-bandsEnergy()-1,8</th>\n",
       "      <th>...</th>\n",
       "      <th>282 fBodyAcc-energy()-X</th>\n",
       "      <th>303 fBodyAcc-bandsEnergy()-1,8</th>\n",
       "      <th>311 fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <th>315 fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <th>382 fBodyAccJerk-bandsEnergy()-1,8</th>\n",
       "      <th>504 fBodyAccMag-std()</th>\n",
       "      <th>505 fBodyAccMag-mad()</th>\n",
       "      <th>509 fBodyAccMag-energy()</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.140840</td>\n",
       "      <td>0.115375</td>\n",
       "      <td>-0.161265</td>\n",
       "      <td>0.124660</td>\n",
       "      <td>-0.123213</td>\n",
       "      <td>0.056483</td>\n",
       "      <td>-0.375426</td>\n",
       "      <td>-0.970905</td>\n",
       "      <td>-0.975510</td>\n",
       "      <td>-0.999454</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.956134</td>\n",
       "      <td>-0.948870</td>\n",
       "      <td>-0.998285</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.141551</td>\n",
       "      <td>0.109379</td>\n",
       "      <td>-0.161343</td>\n",
       "      <td>0.122586</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.383430</td>\n",
       "      <td>-0.970583</td>\n",
       "      <td>-0.978500</td>\n",
       "      <td>-0.999856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.975866</td>\n",
       "      <td>-0.975777</td>\n",
       "      <td>-0.999472</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.142010</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.094566</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.401602</td>\n",
       "      <td>-0.970368</td>\n",
       "      <td>-0.981672</td>\n",
       "      <td>-0.999954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.999972</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.989015</td>\n",
       "      <td>-0.985594</td>\n",
       "      <td>-0.999807</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.143976</td>\n",
       "      <td>0.099850</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.093425</td>\n",
       "      <td>-0.121336</td>\n",
       "      <td>0.095753</td>\n",
       "      <td>-0.400278</td>\n",
       "      <td>-0.969400</td>\n",
       "      <td>-0.982420</td>\n",
       "      <td>-0.999931</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999975</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-0.986742</td>\n",
       "      <td>-0.983524</td>\n",
       "      <td>-0.999770</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.148750</td>\n",
       "      <td>0.094486</td>\n",
       "      <td>-0.166786</td>\n",
       "      <td>0.091682</td>\n",
       "      <td>-0.121834</td>\n",
       "      <td>0.094059</td>\n",
       "      <td>-0.400477</td>\n",
       "      <td>-0.967051</td>\n",
       "      <td>-0.984363</td>\n",
       "      <td>-0.999926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-0.990063</td>\n",
       "      <td>-0.992324</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>-0.222004</td>\n",
       "      <td>-0.039492</td>\n",
       "      <td>-0.214233</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.071977</td>\n",
       "      <td>-0.405132</td>\n",
       "      <td>-0.918375</td>\n",
       "      <td>-0.995193</td>\n",
       "      <td>-0.053258</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.674230</td>\n",
       "      <td>-0.684177</td>\n",
       "      <td>-0.666429</td>\n",
       "      <td>-0.668164</td>\n",
       "      <td>-0.839256</td>\n",
       "      <td>-0.232600</td>\n",
       "      <td>-0.007392</td>\n",
       "      <td>-0.584282</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>-0.242054</td>\n",
       "      <td>-0.039863</td>\n",
       "      <td>-0.231477</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.358934</td>\n",
       "      <td>-0.902880</td>\n",
       "      <td>-0.995151</td>\n",
       "      <td>-0.029411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.705580</td>\n",
       "      <td>-0.726986</td>\n",
       "      <td>-0.704444</td>\n",
       "      <td>-0.705435</td>\n",
       "      <td>-0.854278</td>\n",
       "      <td>-0.275373</td>\n",
       "      <td>-0.172448</td>\n",
       "      <td>-0.632536</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>-0.236950</td>\n",
       "      <td>-0.026805</td>\n",
       "      <td>-0.249134</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.216004</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.377025</td>\n",
       "      <td>-0.907561</td>\n",
       "      <td>-0.995450</td>\n",
       "      <td>0.161404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.692379</td>\n",
       "      <td>-0.655263</td>\n",
       "      <td>-0.674515</td>\n",
       "      <td>-0.684729</td>\n",
       "      <td>-0.815380</td>\n",
       "      <td>-0.220288</td>\n",
       "      <td>-0.216074</td>\n",
       "      <td>-0.641170</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>-0.233230</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>-0.244267</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.210542</td>\n",
       "      <td>-0.040009</td>\n",
       "      <td>-0.440050</td>\n",
       "      <td>-0.910648</td>\n",
       "      <td>-0.998824</td>\n",
       "      <td>0.193585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.693098</td>\n",
       "      <td>-0.643425</td>\n",
       "      <td>-0.677215</td>\n",
       "      <td>-0.685088</td>\n",
       "      <td>-0.822905</td>\n",
       "      <td>-0.234539</td>\n",
       "      <td>-0.220443</td>\n",
       "      <td>-0.663579</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>-0.233292</td>\n",
       "      <td>-0.020954</td>\n",
       "      <td>-0.240956</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.212149</td>\n",
       "      <td>-0.047491</td>\n",
       "      <td>-0.432003</td>\n",
       "      <td>-0.910579</td>\n",
       "      <td>-0.998144</td>\n",
       "      <td>-0.129277</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.731037</td>\n",
       "      <td>-0.709495</td>\n",
       "      <td>-0.728519</td>\n",
       "      <td>-0.727441</td>\n",
       "      <td>-0.834215</td>\n",
       "      <td>-0.342670</td>\n",
       "      <td>-0.146649</td>\n",
       "      <td>-0.698087</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      42 tGravityAcc-mean()-Y  43 tGravityAcc-mean()-Z  \\\n",
       "0                   -0.140840                 0.115375   \n",
       "1                   -0.141551                 0.109379   \n",
       "2                   -0.142010                 0.101884   \n",
       "3                   -0.143976                 0.099850   \n",
       "4                   -0.148750                 0.094486   \n",
       "...                       ...                      ...   \n",
       "7347                -0.222004                -0.039492   \n",
       "7348                -0.242054                -0.039863   \n",
       "7349                -0.236950                -0.026805   \n",
       "7350                -0.233230                -0.004984   \n",
       "7351                -0.233292                -0.020954   \n",
       "\n",
       "      51 tGravityAcc-max()-Y  52 tGravityAcc-max()-Z  54 tGravityAcc-min()-Y  \\\n",
       "0                  -0.161265                0.124660               -0.123213   \n",
       "1                  -0.161343                0.122586               -0.114893   \n",
       "2                  -0.163711                0.094566               -0.114893   \n",
       "3                  -0.163711                0.093425               -0.121336   \n",
       "4                  -0.166786                0.091682               -0.121834   \n",
       "...                      ...                     ...                     ...   \n",
       "7347               -0.214233               -0.016391               -0.234998   \n",
       "7348               -0.231477               -0.016391               -0.234998   \n",
       "7349               -0.249134                0.024684               -0.216004   \n",
       "7350               -0.244267                0.024684               -0.210542   \n",
       "7351               -0.240956                0.003031               -0.212149   \n",
       "\n",
       "      55 tGravityAcc-min()-Z  56 tGravityAcc-sma()  58 tGravityAcc-energy()-Y  \\\n",
       "0                   0.056483             -0.375426                  -0.970905   \n",
       "1                   0.102764             -0.383430                  -0.970583   \n",
       "2                   0.102764             -0.401602                  -0.970368   \n",
       "3                   0.095753             -0.400278                  -0.969400   \n",
       "4                   0.094059             -0.400477                  -0.967051   \n",
       "...                      ...                   ...                        ...   \n",
       "7347               -0.071977             -0.405132                  -0.918375   \n",
       "7348               -0.068919             -0.358934                  -0.902880   \n",
       "7349               -0.068919             -0.377025                  -0.907561   \n",
       "7350               -0.040009             -0.440050                  -0.910648   \n",
       "7351               -0.047491             -0.432003                  -0.910579   \n",
       "\n",
       "      59 tGravityAcc-energy()-Z  475 fBodyGyro-bandsEnergy()-1,8  ...  \\\n",
       "0                     -0.975510                        -0.999454  ...   \n",
       "1                     -0.978500                        -0.999856  ...   \n",
       "2                     -0.981672                        -0.999954  ...   \n",
       "3                     -0.982420                        -0.999931  ...   \n",
       "4                     -0.984363                        -0.999926  ...   \n",
       "...                         ...                              ...  ...   \n",
       "7347                  -0.995193                        -0.053258  ...   \n",
       "7348                  -0.995151                        -0.029411  ...   \n",
       "7349                  -0.995450                         0.161404  ...   \n",
       "7350                  -0.998824                         0.193585  ...   \n",
       "7351                  -0.998144                        -0.129277  ...   \n",
       "\n",
       "      282 fBodyAcc-energy()-X  303 fBodyAcc-bandsEnergy()-1,8  \\\n",
       "0                   -0.999968                       -0.999963   \n",
       "1                   -0.999991                       -0.999996   \n",
       "2                   -0.999969                       -0.999989   \n",
       "3                   -0.999975                       -0.999989   \n",
       "4                   -0.999990                       -0.999994   \n",
       "...                       ...                             ...   \n",
       "7347                -0.674230                       -0.684177   \n",
       "7348                -0.705580                       -0.726986   \n",
       "7349                -0.692379                       -0.655263   \n",
       "7350                -0.693098                       -0.643425   \n",
       "7351                -0.731037                       -0.709495   \n",
       "\n",
       "      311 fBodyAcc-bandsEnergy()-1,16  315 fBodyAcc-bandsEnergy()-1,24  \\\n",
       "0                           -0.999969                        -0.999971   \n",
       "1                           -0.999994                        -0.999992   \n",
       "2                           -0.999983                        -0.999972   \n",
       "3                           -0.999986                        -0.999977   \n",
       "4                           -0.999993                        -0.999991   \n",
       "...                               ...                              ...   \n",
       "7347                        -0.666429                        -0.668164   \n",
       "7348                        -0.704444                        -0.705435   \n",
       "7349                        -0.674515                        -0.684729   \n",
       "7350                        -0.677215                        -0.685088   \n",
       "7351                        -0.728519                        -0.727441   \n",
       "\n",
       "      382 fBodyAccJerk-bandsEnergy()-1,8  504 fBodyAccMag-std()  \\\n",
       "0                              -0.999986              -0.956134   \n",
       "1                              -0.999996              -0.975866   \n",
       "2                              -0.999994              -0.989015   \n",
       "3                              -0.999998              -0.986742   \n",
       "4                              -0.999995              -0.990063   \n",
       "...                                  ...                    ...   \n",
       "7347                           -0.839256              -0.232600   \n",
       "7348                           -0.854278              -0.275373   \n",
       "7349                           -0.815380              -0.220288   \n",
       "7350                           -0.822905              -0.234539   \n",
       "7351                           -0.834215              -0.342670   \n",
       "\n",
       "      505 fBodyAccMag-mad()  509 fBodyAccMag-energy()  Subject  Activity  \n",
       "0                 -0.948870                 -0.998285        1         5  \n",
       "1                 -0.975777                 -0.999472        1         5  \n",
       "2                 -0.985594                 -0.999807        1         5  \n",
       "3                 -0.983524                 -0.999770        1         5  \n",
       "4                 -0.992324                 -0.999873        1         5  \n",
       "...                     ...                       ...      ...       ...  \n",
       "7347              -0.007392                 -0.584282       30         2  \n",
       "7348              -0.172448                 -0.632536       30         2  \n",
       "7349              -0.216074                 -0.641170       30         2  \n",
       "7350              -0.220443                 -0.663579       30         2  \n",
       "7351              -0.146649                 -0.698087       30         2  \n",
       "\n",
       "[7352 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_names = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "train_column_names = train_names.values.tolist()\n",
    "train_column_names = [k for row in train_column_names for k in row]\n",
    "\n",
    "train_data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None)\n",
    "train_data.columns = train_column_names\n",
    "\n",
    "### Single dataframe column\n",
    "\n",
    "y_train = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "y_train.columns = ['Subject']\n",
    "\n",
    "y_train_activity = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "y_train_activity.columns = ['Activity']\n",
    "\n",
    "X_train_1 = train_data[sub_features]\n",
    "X_train_2 = train_data[act_features]\n",
    "X_train_data = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "X_train_data = pd.concat([X_train_data, y_train, y_train_activity], axis = 1)\n",
    "X_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_data[(X_train_data['Subject'].isin([1, 3, 5, 7, 8, 11, 14])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,:-2].values\n",
    "y_train = X_train_data[(X_train_data['Subject'].isin([1, 3, 5, 7, 8, 11, 14])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,-2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y_train)):\n",
    "    if y_train[k] == 1:\n",
    "        y_train[k] = 0\n",
    "    elif y_train[k] == 3:\n",
    "        y_train[k] = 1\n",
    "    elif y_train[k] == 5:\n",
    "        y_train[k] = 2\n",
    "    elif y_train[k] == 7:\n",
    "        y_train[k] = 3\n",
    "    elif y_train[k] == 8:\n",
    "        y_train[k] = 4\n",
    "    elif y_train[k] == 11:\n",
    "        y_train[k] = 5\n",
    "    else:\n",
    "        y_train[k] = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 30),\n",
    "            classifier_block(30, 20),\n",
    "            classifier_block(20, 20),\n",
    "            classifier_block(20, 10),\n",
    "            nn.Linear(10, 7)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 7500\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.889272928237915, Final Batch Loss: 1.9630135297775269\n",
      "Epoch 2, Loss: 7.882872819900513, Final Batch Loss: 1.9513298273086548\n",
      "Epoch 3, Loss: 7.873667001724243, Final Batch Loss: 1.9647146463394165\n",
      "Epoch 4, Loss: 7.87217903137207, Final Batch Loss: 1.9902414083480835\n",
      "Epoch 5, Loss: 7.856863856315613, Final Batch Loss: 1.973312258720398\n",
      "Epoch 6, Loss: 7.844617962837219, Final Batch Loss: 1.960126280784607\n",
      "Epoch 7, Loss: 7.842557311058044, Final Batch Loss: 1.9713786840438843\n",
      "Epoch 8, Loss: 7.820404529571533, Final Batch Loss: 1.9441825151443481\n",
      "Epoch 9, Loss: 7.818103194236755, Final Batch Loss: 1.9560298919677734\n",
      "Epoch 10, Loss: 7.804453372955322, Final Batch Loss: 1.9504005908966064\n",
      "Epoch 11, Loss: 7.789745569229126, Final Batch Loss: 1.9353642463684082\n",
      "Epoch 12, Loss: 7.782794237136841, Final Batch Loss: 1.9382673501968384\n",
      "Epoch 13, Loss: 7.76604688167572, Final Batch Loss: 1.9407285451889038\n",
      "Epoch 14, Loss: 7.753812193870544, Final Batch Loss: 1.9334181547164917\n",
      "Epoch 15, Loss: 7.720077037811279, Final Batch Loss: 1.927754521369934\n",
      "Epoch 16, Loss: 7.700739860534668, Final Batch Loss: 1.925072431564331\n",
      "Epoch 17, Loss: 7.660950422286987, Final Batch Loss: 1.9132764339447021\n",
      "Epoch 18, Loss: 7.628545045852661, Final Batch Loss: 1.899306058883667\n",
      "Epoch 19, Loss: 7.586483001708984, Final Batch Loss: 1.8885666131973267\n",
      "Epoch 20, Loss: 7.517386436462402, Final Batch Loss: 1.8844987154006958\n",
      "Epoch 21, Loss: 7.43041205406189, Final Batch Loss: 1.8326812982559204\n",
      "Epoch 22, Loss: 7.3424893617630005, Final Batch Loss: 1.8068023920059204\n",
      "Epoch 23, Loss: 7.241539835929871, Final Batch Loss: 1.8400582075119019\n",
      "Epoch 24, Loss: 7.168332099914551, Final Batch Loss: 1.7949029207229614\n",
      "Epoch 25, Loss: 7.005237340927124, Final Batch Loss: 1.7126288414001465\n",
      "Epoch 26, Loss: 6.922096848487854, Final Batch Loss: 1.6600334644317627\n",
      "Epoch 27, Loss: 6.760210275650024, Final Batch Loss: 1.660353660583496\n",
      "Epoch 28, Loss: 6.73373007774353, Final Batch Loss: 1.7105135917663574\n",
      "Epoch 29, Loss: 6.564121723175049, Final Batch Loss: 1.5857940912246704\n",
      "Epoch 30, Loss: 6.532883286476135, Final Batch Loss: 1.6213494539260864\n",
      "Epoch 31, Loss: 6.469409823417664, Final Batch Loss: 1.6134141683578491\n",
      "Epoch 32, Loss: 6.385164260864258, Final Batch Loss: 1.5722413063049316\n",
      "Epoch 33, Loss: 6.270657420158386, Final Batch Loss: 1.5365341901779175\n",
      "Epoch 34, Loss: 6.235918760299683, Final Batch Loss: 1.5225253105163574\n",
      "Epoch 35, Loss: 6.183615684509277, Final Batch Loss: 1.561519742012024\n",
      "Epoch 36, Loss: 6.0930070877075195, Final Batch Loss: 1.5169624090194702\n",
      "Epoch 37, Loss: 6.057110905647278, Final Batch Loss: 1.5600931644439697\n",
      "Epoch 38, Loss: 5.937283158302307, Final Batch Loss: 1.4546188116073608\n",
      "Epoch 39, Loss: 5.947372674942017, Final Batch Loss: 1.4741116762161255\n",
      "Epoch 40, Loss: 5.903543472290039, Final Batch Loss: 1.5027856826782227\n",
      "Epoch 41, Loss: 5.740759372711182, Final Batch Loss: 1.404024600982666\n",
      "Epoch 42, Loss: 5.7625651359558105, Final Batch Loss: 1.448618769645691\n",
      "Epoch 43, Loss: 5.6990615129470825, Final Batch Loss: 1.3554614782333374\n",
      "Epoch 44, Loss: 5.651854157447815, Final Batch Loss: 1.3667491674423218\n",
      "Epoch 45, Loss: 5.649994492530823, Final Batch Loss: 1.4665844440460205\n",
      "Epoch 46, Loss: 5.481769800186157, Final Batch Loss: 1.3393844366073608\n",
      "Epoch 47, Loss: 5.474562406539917, Final Batch Loss: 1.374466896057129\n",
      "Epoch 48, Loss: 5.394808053970337, Final Batch Loss: 1.3062477111816406\n",
      "Epoch 49, Loss: 5.48538863658905, Final Batch Loss: 1.4338432550430298\n",
      "Epoch 50, Loss: 5.379194140434265, Final Batch Loss: 1.3756924867630005\n",
      "Epoch 51, Loss: 5.354034185409546, Final Batch Loss: 1.3205931186676025\n",
      "Epoch 52, Loss: 5.220241189002991, Final Batch Loss: 1.166124939918518\n",
      "Epoch 53, Loss: 5.239479422569275, Final Batch Loss: 1.3336882591247559\n",
      "Epoch 54, Loss: 5.2794517278671265, Final Batch Loss: 1.3628573417663574\n",
      "Epoch 55, Loss: 5.305261850357056, Final Batch Loss: 1.2888916730880737\n",
      "Epoch 56, Loss: 5.078777194023132, Final Batch Loss: 1.2575396299362183\n",
      "Epoch 57, Loss: 5.076536059379578, Final Batch Loss: 1.2284550666809082\n",
      "Epoch 58, Loss: 5.080223202705383, Final Batch Loss: 1.283835768699646\n",
      "Epoch 59, Loss: 5.106658577919006, Final Batch Loss: 1.3436414003372192\n",
      "Epoch 60, Loss: 4.999886512756348, Final Batch Loss: 1.208117127418518\n",
      "Epoch 61, Loss: 4.808506369590759, Final Batch Loss: 1.168152928352356\n",
      "Epoch 62, Loss: 4.851481199264526, Final Batch Loss: 1.1145436763763428\n",
      "Epoch 63, Loss: 4.952390789985657, Final Batch Loss: 1.218045949935913\n",
      "Epoch 64, Loss: 4.768692493438721, Final Batch Loss: 1.1675989627838135\n",
      "Epoch 65, Loss: 4.770725846290588, Final Batch Loss: 1.1609838008880615\n",
      "Epoch 66, Loss: 4.7045464515686035, Final Batch Loss: 1.0957880020141602\n",
      "Epoch 67, Loss: 4.74752414226532, Final Batch Loss: 1.1867058277130127\n",
      "Epoch 68, Loss: 4.656365394592285, Final Batch Loss: 1.229710340499878\n",
      "Epoch 69, Loss: 4.602820038795471, Final Batch Loss: 1.1188452243804932\n",
      "Epoch 70, Loss: 4.6391414403915405, Final Batch Loss: 1.143790364265442\n",
      "Epoch 71, Loss: 4.602115988731384, Final Batch Loss: 1.2224375009536743\n",
      "Epoch 72, Loss: 4.620126008987427, Final Batch Loss: 1.115924596786499\n",
      "Epoch 73, Loss: 4.476685881614685, Final Batch Loss: 1.0435611009597778\n",
      "Epoch 74, Loss: 4.34533965587616, Final Batch Loss: 1.0158768892288208\n",
      "Epoch 75, Loss: 4.44593870639801, Final Batch Loss: 1.2081773281097412\n",
      "Epoch 76, Loss: 4.467121362686157, Final Batch Loss: 1.1072567701339722\n",
      "Epoch 77, Loss: 4.442631006240845, Final Batch Loss: 1.1002671718597412\n",
      "Epoch 78, Loss: 4.468148708343506, Final Batch Loss: 1.1610236167907715\n",
      "Epoch 79, Loss: 4.262113928794861, Final Batch Loss: 1.1286375522613525\n",
      "Epoch 80, Loss: 4.292071342468262, Final Batch Loss: 1.040778636932373\n",
      "Epoch 81, Loss: 4.260576844215393, Final Batch Loss: 1.0276364088058472\n",
      "Epoch 82, Loss: 4.162674069404602, Final Batch Loss: 0.998385488986969\n",
      "Epoch 83, Loss: 4.182005405426025, Final Batch Loss: 1.044081211090088\n",
      "Epoch 84, Loss: 4.049330830574036, Final Batch Loss: 0.9185377955436707\n",
      "Epoch 85, Loss: 4.286551475524902, Final Batch Loss: 1.0551621913909912\n",
      "Epoch 86, Loss: 4.179983913898468, Final Batch Loss: 1.1555442810058594\n",
      "Epoch 87, Loss: 4.2315033078193665, Final Batch Loss: 0.9782640337944031\n",
      "Epoch 88, Loss: 4.045152425765991, Final Batch Loss: 0.9755508303642273\n",
      "Epoch 89, Loss: 4.005789577960968, Final Batch Loss: 0.9337146282196045\n",
      "Epoch 90, Loss: 4.003733396530151, Final Batch Loss: 0.9942898750305176\n",
      "Epoch 91, Loss: 3.9525803327560425, Final Batch Loss: 0.9856270551681519\n",
      "Epoch 92, Loss: 3.9235764741897583, Final Batch Loss: 1.023216724395752\n",
      "Epoch 93, Loss: 4.045567691326141, Final Batch Loss: 1.0223525762557983\n",
      "Epoch 94, Loss: 4.066674768924713, Final Batch Loss: 0.958266019821167\n",
      "Epoch 95, Loss: 3.948301374912262, Final Batch Loss: 0.8701848387718201\n",
      "Epoch 96, Loss: 3.956385910511017, Final Batch Loss: 1.0113188028335571\n",
      "Epoch 97, Loss: 4.006233215332031, Final Batch Loss: 1.0096251964569092\n",
      "Epoch 98, Loss: 3.823792338371277, Final Batch Loss: 0.856089174747467\n",
      "Epoch 99, Loss: 3.919238030910492, Final Batch Loss: 0.9965417981147766\n",
      "Epoch 100, Loss: 3.7996559143066406, Final Batch Loss: 0.9762837290763855\n",
      "Epoch 101, Loss: 3.994893789291382, Final Batch Loss: 1.0935238599777222\n",
      "Epoch 102, Loss: 3.7713266611099243, Final Batch Loss: 0.895618200302124\n",
      "Epoch 103, Loss: 3.869544267654419, Final Batch Loss: 1.0026930570602417\n",
      "Epoch 104, Loss: 3.787033796310425, Final Batch Loss: 0.9440029859542847\n",
      "Epoch 105, Loss: 3.7205055356025696, Final Batch Loss: 0.8386482000350952\n",
      "Epoch 106, Loss: 3.7389973402023315, Final Batch Loss: 0.9610271453857422\n",
      "Epoch 107, Loss: 3.725943088531494, Final Batch Loss: 0.8871020674705505\n",
      "Epoch 108, Loss: 3.6642220616340637, Final Batch Loss: 0.9430331587791443\n",
      "Epoch 109, Loss: 3.7168079614639282, Final Batch Loss: 0.78863525390625\n",
      "Epoch 110, Loss: 3.536747634410858, Final Batch Loss: 0.946891188621521\n",
      "Epoch 111, Loss: 3.743376135826111, Final Batch Loss: 0.8984432220458984\n",
      "Epoch 112, Loss: 3.6083606481552124, Final Batch Loss: 0.809317946434021\n",
      "Epoch 113, Loss: 3.6102054119110107, Final Batch Loss: 0.9010144472122192\n",
      "Epoch 114, Loss: 3.564235270023346, Final Batch Loss: 0.843441367149353\n",
      "Epoch 115, Loss: 3.4451295137405396, Final Batch Loss: 0.8266810178756714\n",
      "Epoch 116, Loss: 3.598597466945648, Final Batch Loss: 0.9174282550811768\n",
      "Epoch 117, Loss: 3.569966673851013, Final Batch Loss: 0.90981525182724\n",
      "Epoch 118, Loss: 3.534757137298584, Final Batch Loss: 0.8235909342765808\n",
      "Epoch 119, Loss: 3.5124926567077637, Final Batch Loss: 0.8830797076225281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120, Loss: 3.4280693531036377, Final Batch Loss: 0.8457850217819214\n",
      "Epoch 121, Loss: 3.437911033630371, Final Batch Loss: 0.8105810880661011\n",
      "Epoch 122, Loss: 3.6064494252204895, Final Batch Loss: 0.9781050086021423\n",
      "Epoch 123, Loss: 3.4404177069664, Final Batch Loss: 0.8175289630889893\n",
      "Epoch 124, Loss: 3.503205955028534, Final Batch Loss: 0.9579083919525146\n",
      "Epoch 125, Loss: 3.4593867659568787, Final Batch Loss: 0.8820235133171082\n",
      "Epoch 126, Loss: 3.3678683042526245, Final Batch Loss: 0.885668933391571\n",
      "Epoch 127, Loss: 3.428358018398285, Final Batch Loss: 0.8805322051048279\n",
      "Epoch 128, Loss: 3.426828920841217, Final Batch Loss: 0.794929027557373\n",
      "Epoch 129, Loss: 3.269774377346039, Final Batch Loss: 0.8032711744308472\n",
      "Epoch 130, Loss: 3.3577959537506104, Final Batch Loss: 0.7771664261817932\n",
      "Epoch 131, Loss: 3.305810332298279, Final Batch Loss: 0.8457049131393433\n",
      "Epoch 132, Loss: 3.3835904598236084, Final Batch Loss: 0.8684773445129395\n",
      "Epoch 133, Loss: 3.437007188796997, Final Batch Loss: 0.9409433603286743\n",
      "Epoch 134, Loss: 3.4083120226860046, Final Batch Loss: 0.9074459075927734\n",
      "Epoch 135, Loss: 3.3770174980163574, Final Batch Loss: 0.8582687973976135\n",
      "Epoch 136, Loss: 3.413824141025543, Final Batch Loss: 0.7827600240707397\n",
      "Epoch 137, Loss: 3.1872134804725647, Final Batch Loss: 0.862525463104248\n",
      "Epoch 138, Loss: 3.3293160796165466, Final Batch Loss: 0.8592067956924438\n",
      "Epoch 139, Loss: 3.3346393704414368, Final Batch Loss: 0.8705978989601135\n",
      "Epoch 140, Loss: 3.464535713195801, Final Batch Loss: 1.0343204736709595\n",
      "Epoch 141, Loss: 3.3079754114151, Final Batch Loss: 0.7072885632514954\n",
      "Epoch 142, Loss: 3.1445468068122864, Final Batch Loss: 0.7805189490318298\n",
      "Epoch 143, Loss: 3.341087818145752, Final Batch Loss: 0.9055162072181702\n",
      "Epoch 144, Loss: 3.2357791662216187, Final Batch Loss: 0.8548781871795654\n",
      "Epoch 145, Loss: 3.182580888271332, Final Batch Loss: 0.726165771484375\n",
      "Epoch 146, Loss: 3.192773938179016, Final Batch Loss: 0.7410461902618408\n",
      "Epoch 147, Loss: 3.1615650057792664, Final Batch Loss: 0.8026881217956543\n",
      "Epoch 148, Loss: 3.2152510285377502, Final Batch Loss: 0.7437077760696411\n",
      "Epoch 149, Loss: 3.23930686712265, Final Batch Loss: 0.7943022847175598\n",
      "Epoch 150, Loss: 3.3157333731651306, Final Batch Loss: 0.8354310393333435\n",
      "Epoch 151, Loss: 3.1185205578804016, Final Batch Loss: 0.7532728314399719\n",
      "Epoch 152, Loss: 3.170245409011841, Final Batch Loss: 0.9016405940055847\n",
      "Epoch 153, Loss: 3.2983343601226807, Final Batch Loss: 0.8398149013519287\n",
      "Epoch 154, Loss: 3.1470037698745728, Final Batch Loss: 0.7564821243286133\n",
      "Epoch 155, Loss: 3.1400261521339417, Final Batch Loss: 0.8590794801712036\n",
      "Epoch 156, Loss: 3.1319579482078552, Final Batch Loss: 0.7847136855125427\n",
      "Epoch 157, Loss: 3.267080247402191, Final Batch Loss: 0.832222044467926\n",
      "Epoch 158, Loss: 3.194712519645691, Final Batch Loss: 0.8276515603065491\n",
      "Epoch 159, Loss: 3.106618046760559, Final Batch Loss: 0.7591222524642944\n",
      "Epoch 160, Loss: 3.0487524271011353, Final Batch Loss: 0.8408411741256714\n",
      "Epoch 161, Loss: 3.030826151371002, Final Batch Loss: 0.7258022427558899\n",
      "Epoch 162, Loss: 3.2447136640548706, Final Batch Loss: 0.8944831490516663\n",
      "Epoch 163, Loss: 3.132826864719391, Final Batch Loss: 0.7621372938156128\n",
      "Epoch 164, Loss: 3.025221586227417, Final Batch Loss: 0.8128290772438049\n",
      "Epoch 165, Loss: 2.98969829082489, Final Batch Loss: 0.7460089325904846\n",
      "Epoch 166, Loss: 3.1069899201393127, Final Batch Loss: 0.7785468697547913\n",
      "Epoch 167, Loss: 3.08066463470459, Final Batch Loss: 0.7378539443016052\n",
      "Epoch 168, Loss: 2.91610985994339, Final Batch Loss: 0.7348424792289734\n",
      "Epoch 169, Loss: 2.8650678396224976, Final Batch Loss: 0.7617439031600952\n",
      "Epoch 170, Loss: 3.279361069202423, Final Batch Loss: 0.8554491400718689\n",
      "Epoch 171, Loss: 3.0576812624931335, Final Batch Loss: 0.7987183928489685\n",
      "Epoch 172, Loss: 2.934997081756592, Final Batch Loss: 0.6840168237686157\n",
      "Epoch 173, Loss: 3.040723741054535, Final Batch Loss: 0.7056268453598022\n",
      "Epoch 174, Loss: 2.978695511817932, Final Batch Loss: 0.792823314666748\n",
      "Epoch 175, Loss: 3.0834275484085083, Final Batch Loss: 0.8747600317001343\n",
      "Epoch 176, Loss: 2.9551026225090027, Final Batch Loss: 0.6420682072639465\n",
      "Epoch 177, Loss: 2.9560051560401917, Final Batch Loss: 0.739372730255127\n",
      "Epoch 178, Loss: 2.8725114464759827, Final Batch Loss: 0.7680051922798157\n",
      "Epoch 179, Loss: 3.002445638179779, Final Batch Loss: 0.7767657041549683\n",
      "Epoch 180, Loss: 2.8642274737358093, Final Batch Loss: 0.7116203904151917\n",
      "Epoch 181, Loss: 2.9357773065567017, Final Batch Loss: 0.7932754158973694\n",
      "Epoch 182, Loss: 2.978638708591461, Final Batch Loss: 0.7576586604118347\n",
      "Epoch 183, Loss: 2.930001974105835, Final Batch Loss: 0.7212387919425964\n",
      "Epoch 184, Loss: 3.047654390335083, Final Batch Loss: 0.765980064868927\n",
      "Epoch 185, Loss: 3.059470474720001, Final Batch Loss: 0.7082278728485107\n",
      "Epoch 186, Loss: 2.916681408882141, Final Batch Loss: 0.7711136937141418\n",
      "Epoch 187, Loss: 2.966328263282776, Final Batch Loss: 0.7840864658355713\n",
      "Epoch 188, Loss: 2.9510160088539124, Final Batch Loss: 0.7531115412712097\n",
      "Epoch 189, Loss: 2.901447594165802, Final Batch Loss: 0.7411499619483948\n",
      "Epoch 190, Loss: 3.046121299266815, Final Batch Loss: 0.8520075082778931\n",
      "Epoch 191, Loss: 3.070712089538574, Final Batch Loss: 0.8095051050186157\n",
      "Epoch 192, Loss: 2.852627217769623, Final Batch Loss: 0.6526600122451782\n",
      "Epoch 193, Loss: 2.981217086315155, Final Batch Loss: 0.8134310245513916\n",
      "Epoch 194, Loss: 2.881538689136505, Final Batch Loss: 0.725050687789917\n",
      "Epoch 195, Loss: 2.7803446650505066, Final Batch Loss: 0.711727499961853\n",
      "Epoch 196, Loss: 2.970460295677185, Final Batch Loss: 0.7655153274536133\n",
      "Epoch 197, Loss: 2.7760753631591797, Final Batch Loss: 0.6383178234100342\n",
      "Epoch 198, Loss: 2.893544018268585, Final Batch Loss: 0.6735220551490784\n",
      "Epoch 199, Loss: 2.996445894241333, Final Batch Loss: 0.7602161765098572\n",
      "Epoch 200, Loss: 2.8166343569755554, Final Batch Loss: 0.7491554021835327\n",
      "Epoch 201, Loss: 2.704777479171753, Final Batch Loss: 0.6082960963249207\n",
      "Epoch 202, Loss: 2.8329243063926697, Final Batch Loss: 0.7950400710105896\n",
      "Epoch 203, Loss: 2.9734537601470947, Final Batch Loss: 0.7275700569152832\n",
      "Epoch 204, Loss: 2.8206050395965576, Final Batch Loss: 0.753649115562439\n",
      "Epoch 205, Loss: 2.7945640087127686, Final Batch Loss: 0.7031252384185791\n",
      "Epoch 206, Loss: 2.74949049949646, Final Batch Loss: 0.670524537563324\n",
      "Epoch 207, Loss: 2.929734468460083, Final Batch Loss: 0.7227020859718323\n",
      "Epoch 208, Loss: 2.8604602217674255, Final Batch Loss: 0.8144100904464722\n",
      "Epoch 209, Loss: 2.8876172304153442, Final Batch Loss: 0.7852628827095032\n",
      "Epoch 210, Loss: 2.7597858905792236, Final Batch Loss: 0.7204006314277649\n",
      "Epoch 211, Loss: 2.687461256980896, Final Batch Loss: 0.5432049036026001\n",
      "Epoch 212, Loss: 2.7652127146720886, Final Batch Loss: 0.6085551977157593\n",
      "Epoch 213, Loss: 2.9797310829162598, Final Batch Loss: 0.8936388492584229\n",
      "Epoch 214, Loss: 2.9285911917686462, Final Batch Loss: 0.7335530519485474\n",
      "Epoch 215, Loss: 2.6276338696479797, Final Batch Loss: 0.6347699165344238\n",
      "Epoch 216, Loss: 2.80183869600296, Final Batch Loss: 0.6863657236099243\n",
      "Epoch 217, Loss: 2.7805854082107544, Final Batch Loss: 0.6590621471405029\n",
      "Epoch 218, Loss: 2.75026273727417, Final Batch Loss: 0.7176761031150818\n",
      "Epoch 219, Loss: 2.9458261728286743, Final Batch Loss: 0.7613285779953003\n",
      "Epoch 220, Loss: 2.735154926776886, Final Batch Loss: 0.6707198023796082\n",
      "Epoch 221, Loss: 2.816886067390442, Final Batch Loss: 0.7854052782058716\n",
      "Epoch 222, Loss: 2.790920078754425, Final Batch Loss: 0.6111918091773987\n",
      "Epoch 223, Loss: 2.8174381256103516, Final Batch Loss: 0.678451418876648\n",
      "Epoch 224, Loss: 2.68283611536026, Final Batch Loss: 0.668979287147522\n",
      "Epoch 225, Loss: 2.6386555433273315, Final Batch Loss: 0.6800005435943604\n",
      "Epoch 226, Loss: 2.732843577861786, Final Batch Loss: 0.7033841609954834\n",
      "Epoch 227, Loss: 2.8331732749938965, Final Batch Loss: 0.793448805809021\n",
      "Epoch 228, Loss: 2.727061927318573, Final Batch Loss: 0.6903616786003113\n",
      "Epoch 229, Loss: 2.6981314420700073, Final Batch Loss: 0.6286612153053284\n",
      "Epoch 230, Loss: 2.8003591895103455, Final Batch Loss: 0.7838844656944275\n",
      "Epoch 231, Loss: 2.70387202501297, Final Batch Loss: 0.6264227628707886\n",
      "Epoch 232, Loss: 2.7165744304656982, Final Batch Loss: 0.6273447871208191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233, Loss: 2.6868160367012024, Final Batch Loss: 0.6749964952468872\n",
      "Epoch 234, Loss: 2.631333589553833, Final Batch Loss: 0.6276511549949646\n",
      "Epoch 235, Loss: 2.8009387850761414, Final Batch Loss: 0.7107123732566833\n",
      "Epoch 236, Loss: 2.678276240825653, Final Batch Loss: 0.6903511881828308\n",
      "Epoch 237, Loss: 2.8013543486595154, Final Batch Loss: 0.6532015204429626\n",
      "Epoch 238, Loss: 2.7967625856399536, Final Batch Loss: 0.6859715580940247\n",
      "Epoch 239, Loss: 2.7645405530929565, Final Batch Loss: 0.7492620348930359\n",
      "Epoch 240, Loss: 2.7915069460868835, Final Batch Loss: 0.670077919960022\n",
      "Epoch 241, Loss: 2.6962634921073914, Final Batch Loss: 0.7121970057487488\n",
      "Epoch 242, Loss: 2.78131639957428, Final Batch Loss: 0.6781213283538818\n",
      "Epoch 243, Loss: 2.7412105202674866, Final Batch Loss: 0.6931127905845642\n",
      "Epoch 244, Loss: 2.6576226353645325, Final Batch Loss: 0.7055301666259766\n",
      "Epoch 245, Loss: 2.7427111864089966, Final Batch Loss: 0.7680819034576416\n",
      "Epoch 246, Loss: 2.528203070163727, Final Batch Loss: 0.5765498876571655\n",
      "Epoch 247, Loss: 2.673037052154541, Final Batch Loss: 0.7065308690071106\n",
      "Epoch 248, Loss: 2.710019588470459, Final Batch Loss: 0.6974020600318909\n",
      "Epoch 249, Loss: 2.621819853782654, Final Batch Loss: 0.6815937757492065\n",
      "Epoch 250, Loss: 2.6105876564979553, Final Batch Loss: 0.7482126355171204\n",
      "Epoch 251, Loss: 2.6445453763008118, Final Batch Loss: 0.7256554961204529\n",
      "Epoch 252, Loss: 2.7018423080444336, Final Batch Loss: 0.6456223130226135\n",
      "Epoch 253, Loss: 2.6983092427253723, Final Batch Loss: 0.6093449592590332\n",
      "Epoch 254, Loss: 2.629831850528717, Final Batch Loss: 0.6603751182556152\n",
      "Epoch 255, Loss: 2.5516436100006104, Final Batch Loss: 0.6283573508262634\n",
      "Epoch 256, Loss: 2.572292149066925, Final Batch Loss: 0.5646485686302185\n",
      "Epoch 257, Loss: 2.6087785363197327, Final Batch Loss: 0.7328206896781921\n",
      "Epoch 258, Loss: 2.6383427381515503, Final Batch Loss: 0.736310601234436\n",
      "Epoch 259, Loss: 2.5625770688056946, Final Batch Loss: 0.5832467079162598\n",
      "Epoch 260, Loss: 2.6158117055892944, Final Batch Loss: 0.5690781474113464\n",
      "Epoch 261, Loss: 2.587896406650543, Final Batch Loss: 0.6051899790763855\n",
      "Epoch 262, Loss: 2.670677423477173, Final Batch Loss: 0.5829876065254211\n",
      "Epoch 263, Loss: 2.637238621711731, Final Batch Loss: 0.636593759059906\n",
      "Epoch 264, Loss: 2.505893647670746, Final Batch Loss: 0.6477153301239014\n",
      "Epoch 265, Loss: 2.4703909754753113, Final Batch Loss: 0.6007027626037598\n",
      "Epoch 266, Loss: 2.590855300426483, Final Batch Loss: 0.6919894218444824\n",
      "Epoch 267, Loss: 2.604597806930542, Final Batch Loss: 0.7492148280143738\n",
      "Epoch 268, Loss: 2.597104847431183, Final Batch Loss: 0.6592586040496826\n",
      "Epoch 269, Loss: 2.526711165904999, Final Batch Loss: 0.6867528557777405\n",
      "Epoch 270, Loss: 2.591374635696411, Final Batch Loss: 0.7199362516403198\n",
      "Epoch 271, Loss: 2.6265520453453064, Final Batch Loss: 0.6620043516159058\n",
      "Epoch 272, Loss: 2.6699519753456116, Final Batch Loss: 0.6627865433692932\n",
      "Epoch 273, Loss: 2.5181310176849365, Final Batch Loss: 0.6515669226646423\n",
      "Epoch 274, Loss: 2.581404209136963, Final Batch Loss: 0.664320170879364\n",
      "Epoch 275, Loss: 2.571688413619995, Final Batch Loss: 0.6168279051780701\n",
      "Epoch 276, Loss: 2.463052272796631, Final Batch Loss: 0.6064088940620422\n",
      "Epoch 277, Loss: 2.593044936656952, Final Batch Loss: 0.5996052622795105\n",
      "Epoch 278, Loss: 2.510105073451996, Final Batch Loss: 0.6200602650642395\n",
      "Epoch 279, Loss: 2.50488018989563, Final Batch Loss: 0.649540901184082\n",
      "Epoch 280, Loss: 2.4787031412124634, Final Batch Loss: 0.5703955292701721\n",
      "Epoch 281, Loss: 2.661699414253235, Final Batch Loss: 0.6506522297859192\n",
      "Epoch 282, Loss: 2.588103175163269, Final Batch Loss: 0.6831336617469788\n",
      "Epoch 283, Loss: 2.4316205382347107, Final Batch Loss: 0.5639543533325195\n",
      "Epoch 284, Loss: 2.495801270008087, Final Batch Loss: 0.5709352493286133\n",
      "Epoch 285, Loss: 2.470003604888916, Final Batch Loss: 0.4779253602027893\n",
      "Epoch 286, Loss: 2.5644033551216125, Final Batch Loss: 0.6403259038925171\n",
      "Epoch 287, Loss: 2.5519458055496216, Final Batch Loss: 0.6105558276176453\n",
      "Epoch 288, Loss: 2.507324695587158, Final Batch Loss: 0.5376974940299988\n",
      "Epoch 289, Loss: 2.5139554738998413, Final Batch Loss: 0.6478325128555298\n",
      "Epoch 290, Loss: 2.5221961736679077, Final Batch Loss: 0.6740508675575256\n",
      "Epoch 291, Loss: 2.4967769384384155, Final Batch Loss: 0.6599277853965759\n",
      "Epoch 292, Loss: 2.5526427626609802, Final Batch Loss: 0.7396124601364136\n",
      "Epoch 293, Loss: 2.5908027291297913, Final Batch Loss: 0.557367742061615\n",
      "Epoch 294, Loss: 2.396103858947754, Final Batch Loss: 0.6230880618095398\n",
      "Epoch 295, Loss: 2.5124098658561707, Final Batch Loss: 0.6469207406044006\n",
      "Epoch 296, Loss: 2.4986871480941772, Final Batch Loss: 0.6286346316337585\n",
      "Epoch 297, Loss: 2.279326856136322, Final Batch Loss: 0.5026620030403137\n",
      "Epoch 298, Loss: 2.4356513619422913, Final Batch Loss: 0.6881678104400635\n",
      "Epoch 299, Loss: 2.3602762818336487, Final Batch Loss: 0.5501365661621094\n",
      "Epoch 300, Loss: 2.646096348762512, Final Batch Loss: 0.6155793070793152\n",
      "Epoch 301, Loss: 2.45967298746109, Final Batch Loss: 0.6021623611450195\n",
      "Epoch 302, Loss: 2.3928678035736084, Final Batch Loss: 0.5967565774917603\n",
      "Epoch 303, Loss: 2.553701639175415, Final Batch Loss: 0.6986449956893921\n",
      "Epoch 304, Loss: 2.4531115293502808, Final Batch Loss: 0.6104254126548767\n",
      "Epoch 305, Loss: 2.581054389476776, Final Batch Loss: 0.6809697151184082\n",
      "Epoch 306, Loss: 2.4761924147605896, Final Batch Loss: 0.5208902955055237\n",
      "Epoch 307, Loss: 2.3994526267051697, Final Batch Loss: 0.6301161646842957\n",
      "Epoch 308, Loss: 2.3754696249961853, Final Batch Loss: 0.5319238305091858\n",
      "Epoch 309, Loss: 2.5182453989982605, Final Batch Loss: 0.711261510848999\n",
      "Epoch 310, Loss: 2.478858709335327, Final Batch Loss: 0.6707402467727661\n",
      "Epoch 311, Loss: 2.5162265300750732, Final Batch Loss: 0.5500986576080322\n",
      "Epoch 312, Loss: 2.5199337005615234, Final Batch Loss: 0.6606245636940002\n",
      "Epoch 313, Loss: 2.43300199508667, Final Batch Loss: 0.6077497601509094\n",
      "Epoch 314, Loss: 2.5033186078071594, Final Batch Loss: 0.660044252872467\n",
      "Epoch 315, Loss: 2.471505045890808, Final Batch Loss: 0.6012924909591675\n",
      "Epoch 316, Loss: 2.466783583164215, Final Batch Loss: 0.633055567741394\n",
      "Epoch 317, Loss: 2.4350555539131165, Final Batch Loss: 0.6127216219902039\n",
      "Epoch 318, Loss: 2.437500834465027, Final Batch Loss: 0.5323205590248108\n",
      "Epoch 319, Loss: 2.350969970226288, Final Batch Loss: 0.6281359791755676\n",
      "Epoch 320, Loss: 2.4170746207237244, Final Batch Loss: 0.6072996258735657\n",
      "Epoch 321, Loss: 2.520920991897583, Final Batch Loss: 0.6649513244628906\n",
      "Epoch 322, Loss: 2.3956178426742554, Final Batch Loss: 0.6714758276939392\n",
      "Epoch 323, Loss: 2.4146058559417725, Final Batch Loss: 0.5733808279037476\n",
      "Epoch 324, Loss: 2.3618516325950623, Final Batch Loss: 0.5094078183174133\n",
      "Epoch 325, Loss: 2.417693614959717, Final Batch Loss: 0.713780403137207\n",
      "Epoch 326, Loss: 2.457633376121521, Final Batch Loss: 0.6891968846321106\n",
      "Epoch 327, Loss: 2.2597302198410034, Final Batch Loss: 0.5394257307052612\n",
      "Epoch 328, Loss: 2.29974502325058, Final Batch Loss: 0.616550087928772\n",
      "Epoch 329, Loss: 2.442403495311737, Final Batch Loss: 0.5731122493743896\n",
      "Epoch 330, Loss: 2.402110993862152, Final Batch Loss: 0.5862563252449036\n",
      "Epoch 331, Loss: 2.391182839870453, Final Batch Loss: 0.6773061156272888\n",
      "Epoch 332, Loss: 2.387586295604706, Final Batch Loss: 0.5982988476753235\n",
      "Epoch 333, Loss: 2.2857887148857117, Final Batch Loss: 0.5269840359687805\n",
      "Epoch 334, Loss: 2.419495940208435, Final Batch Loss: 0.6025938391685486\n",
      "Epoch 335, Loss: 2.3067378997802734, Final Batch Loss: 0.520223081111908\n",
      "Epoch 336, Loss: 2.422783613204956, Final Batch Loss: 0.6054821610450745\n",
      "Epoch 337, Loss: 2.338062286376953, Final Batch Loss: 0.5387474894523621\n",
      "Epoch 338, Loss: 2.321289360523224, Final Batch Loss: 0.6260032653808594\n",
      "Epoch 339, Loss: 2.3401142358779907, Final Batch Loss: 0.6197147369384766\n",
      "Epoch 340, Loss: 2.5002354979515076, Final Batch Loss: 0.6896683573722839\n",
      "Epoch 341, Loss: 2.328412890434265, Final Batch Loss: 0.5072190165519714\n",
      "Epoch 342, Loss: 2.425596535205841, Final Batch Loss: 0.6370297074317932\n",
      "Epoch 343, Loss: 2.385668933391571, Final Batch Loss: 0.5965697169303894\n",
      "Epoch 344, Loss: 2.3428844213485718, Final Batch Loss: 0.5453564524650574\n",
      "Epoch 345, Loss: 2.224785566329956, Final Batch Loss: 0.5146121978759766\n",
      "Epoch 346, Loss: 2.3900741934776306, Final Batch Loss: 0.5941092371940613\n",
      "Epoch 347, Loss: 2.2963171005249023, Final Batch Loss: 0.633652925491333\n",
      "Epoch 348, Loss: 2.3957695364952087, Final Batch Loss: 0.6103217005729675\n",
      "Epoch 349, Loss: 2.3975790143013, Final Batch Loss: 0.6224121451377869\n",
      "Epoch 350, Loss: 2.469086170196533, Final Batch Loss: 0.6700943112373352\n",
      "Epoch 351, Loss: 2.2957164645195007, Final Batch Loss: 0.5762091875076294\n",
      "Epoch 352, Loss: 2.414133667945862, Final Batch Loss: 0.5608457922935486\n",
      "Epoch 353, Loss: 2.3918005228042603, Final Batch Loss: 0.6232481598854065\n",
      "Epoch 354, Loss: 2.4948874711990356, Final Batch Loss: 0.7631887197494507\n",
      "Epoch 355, Loss: 2.3387551307678223, Final Batch Loss: 0.625005304813385\n",
      "Epoch 356, Loss: 2.3187479376792908, Final Batch Loss: 0.5222153067588806\n",
      "Epoch 357, Loss: 2.303086221218109, Final Batch Loss: 0.5700746178627014\n",
      "Epoch 358, Loss: 2.358370006084442, Final Batch Loss: 0.7006009221076965\n",
      "Epoch 359, Loss: 2.3514915108680725, Final Batch Loss: 0.6343114376068115\n",
      "Epoch 360, Loss: 2.341018557548523, Final Batch Loss: 0.6975839138031006\n",
      "Epoch 361, Loss: 2.3433493077754974, Final Batch Loss: 0.6279563307762146\n",
      "Epoch 362, Loss: 2.335523396730423, Final Batch Loss: 0.6695241332054138\n",
      "Epoch 363, Loss: 2.3197643756866455, Final Batch Loss: 0.5863163471221924\n",
      "Epoch 364, Loss: 2.3233586251735687, Final Batch Loss: 0.5632381439208984\n",
      "Epoch 365, Loss: 2.303966701030731, Final Batch Loss: 0.5623102188110352\n",
      "Epoch 366, Loss: 2.2826480865478516, Final Batch Loss: 0.5868121385574341\n",
      "Epoch 367, Loss: 2.404752731323242, Final Batch Loss: 0.598740816116333\n",
      "Epoch 368, Loss: 2.2418513894081116, Final Batch Loss: 0.5252177119255066\n",
      "Epoch 369, Loss: 2.365290343761444, Final Batch Loss: 0.5983773469924927\n",
      "Epoch 370, Loss: 2.256860852241516, Final Batch Loss: 0.5959252715110779\n",
      "Epoch 371, Loss: 2.3874385356903076, Final Batch Loss: 0.6778850555419922\n",
      "Epoch 372, Loss: 2.2815983593463898, Final Batch Loss: 0.7082481980323792\n",
      "Epoch 373, Loss: 2.288052201271057, Final Batch Loss: 0.6923106908798218\n",
      "Epoch 374, Loss: 2.1661689281463623, Final Batch Loss: 0.518687903881073\n",
      "Epoch 375, Loss: 2.340578079223633, Final Batch Loss: 0.5542298555374146\n",
      "Epoch 376, Loss: 2.1583272218704224, Final Batch Loss: 0.5400108695030212\n",
      "Epoch 377, Loss: 2.308114230632782, Final Batch Loss: 0.5835075974464417\n",
      "Epoch 378, Loss: 2.2614814043045044, Final Batch Loss: 0.5504229068756104\n",
      "Epoch 379, Loss: 2.287666976451874, Final Batch Loss: 0.6468785405158997\n",
      "Epoch 380, Loss: 2.2370150983333588, Final Batch Loss: 0.647578775882721\n",
      "Epoch 381, Loss: 2.2715413570404053, Final Batch Loss: 0.6369267702102661\n",
      "Epoch 382, Loss: 2.3025405406951904, Final Batch Loss: 0.5472961664199829\n",
      "Epoch 383, Loss: 2.2877436876296997, Final Batch Loss: 0.5384812355041504\n",
      "Epoch 384, Loss: 2.2667035460472107, Final Batch Loss: 0.506496787071228\n",
      "Epoch 385, Loss: 2.298877239227295, Final Batch Loss: 0.5861770510673523\n",
      "Epoch 386, Loss: 2.4127365946769714, Final Batch Loss: 0.6290239691734314\n",
      "Epoch 387, Loss: 2.445413589477539, Final Batch Loss: 0.6865921020507812\n",
      "Epoch 388, Loss: 2.285849094390869, Final Batch Loss: 0.5846675038337708\n",
      "Epoch 389, Loss: 2.288646399974823, Final Batch Loss: 0.5130264759063721\n",
      "Epoch 390, Loss: 2.176910638809204, Final Batch Loss: 0.45811372995376587\n",
      "Epoch 391, Loss: 2.2458019256591797, Final Batch Loss: 0.563693106174469\n",
      "Epoch 392, Loss: 2.268958628177643, Final Batch Loss: 0.564609706401825\n",
      "Epoch 393, Loss: 2.353423774242401, Final Batch Loss: 0.6901234984397888\n",
      "Epoch 394, Loss: 2.3105266094207764, Final Batch Loss: 0.5865657925605774\n",
      "Epoch 395, Loss: 2.328532636165619, Final Batch Loss: 0.5879428386688232\n",
      "Epoch 396, Loss: 2.2704334259033203, Final Batch Loss: 0.5554050803184509\n",
      "Epoch 397, Loss: 2.229665994644165, Final Batch Loss: 0.4896607995033264\n",
      "Epoch 398, Loss: 2.2589606046676636, Final Batch Loss: 0.5087974667549133\n",
      "Epoch 399, Loss: 2.2183374762535095, Final Batch Loss: 0.5819392204284668\n",
      "Epoch 400, Loss: 2.211505949497223, Final Batch Loss: 0.5574163794517517\n",
      "Epoch 401, Loss: 2.257144331932068, Final Batch Loss: 0.4595656394958496\n",
      "Epoch 402, Loss: 2.238481283187866, Final Batch Loss: 0.5630031228065491\n",
      "Epoch 403, Loss: 2.2793365716934204, Final Batch Loss: 0.6154239177703857\n",
      "Epoch 404, Loss: 2.286136209964752, Final Batch Loss: 0.5186779499053955\n",
      "Epoch 405, Loss: 2.315659999847412, Final Batch Loss: 0.5498365163803101\n",
      "Epoch 406, Loss: 2.2978063225746155, Final Batch Loss: 0.5951767563819885\n",
      "Epoch 407, Loss: 2.368297040462494, Final Batch Loss: 0.6665592789649963\n",
      "Epoch 408, Loss: 2.165423631668091, Final Batch Loss: 0.44546228647232056\n",
      "Epoch 409, Loss: 2.1939732432365417, Final Batch Loss: 0.5137559175491333\n",
      "Epoch 410, Loss: 2.1591867804527283, Final Batch Loss: 0.5580794811248779\n",
      "Epoch 411, Loss: 2.318755269050598, Final Batch Loss: 0.6132436394691467\n",
      "Epoch 412, Loss: 2.2329289317131042, Final Batch Loss: 0.5459829568862915\n",
      "Epoch 413, Loss: 2.128721833229065, Final Batch Loss: 0.5897818207740784\n",
      "Epoch 414, Loss: 2.2857866883277893, Final Batch Loss: 0.5444915294647217\n",
      "Epoch 415, Loss: 2.160635381937027, Final Batch Loss: 0.48256394267082214\n",
      "Epoch 416, Loss: 2.151228368282318, Final Batch Loss: 0.5172671675682068\n",
      "Epoch 417, Loss: 2.122914731502533, Final Batch Loss: 0.5021974444389343\n",
      "Epoch 418, Loss: 2.2929581105709076, Final Batch Loss: 0.5964939594268799\n",
      "Epoch 419, Loss: 2.195114314556122, Final Batch Loss: 0.5195696353912354\n",
      "Epoch 420, Loss: 2.3341720700263977, Final Batch Loss: 0.6512672305107117\n",
      "Epoch 421, Loss: 2.155000865459442, Final Batch Loss: 0.5620079636573792\n",
      "Epoch 422, Loss: 2.1826659440994263, Final Batch Loss: 0.5004985332489014\n",
      "Epoch 423, Loss: 2.143776386976242, Final Batch Loss: 0.4970476031303406\n",
      "Epoch 424, Loss: 2.2178994715213776, Final Batch Loss: 0.6388987898826599\n",
      "Epoch 425, Loss: 2.1643083095550537, Final Batch Loss: 0.5755723118782043\n",
      "Epoch 426, Loss: 2.1792721152305603, Final Batch Loss: 0.48731404542922974\n",
      "Epoch 427, Loss: 2.235312521457672, Final Batch Loss: 0.6804611086845398\n",
      "Epoch 428, Loss: 2.208152413368225, Final Batch Loss: 0.5734624266624451\n",
      "Epoch 429, Loss: 2.2220804691314697, Final Batch Loss: 0.6163884997367859\n",
      "Epoch 430, Loss: 2.2204793095588684, Final Batch Loss: 0.5784661769866943\n",
      "Epoch 431, Loss: 2.136468231678009, Final Batch Loss: 0.5092602968215942\n",
      "Epoch 432, Loss: 2.1732259392738342, Final Batch Loss: 0.5153729915618896\n",
      "Epoch 433, Loss: 2.1012026369571686, Final Batch Loss: 0.522506058216095\n",
      "Epoch 434, Loss: 2.2892690300941467, Final Batch Loss: 0.642607569694519\n",
      "Epoch 435, Loss: 2.277305841445923, Final Batch Loss: 0.5577415227890015\n",
      "Epoch 436, Loss: 2.3220131397247314, Final Batch Loss: 0.6378371715545654\n",
      "Epoch 437, Loss: 2.2029477953910828, Final Batch Loss: 0.5464314222335815\n",
      "Epoch 438, Loss: 2.1922547519207, Final Batch Loss: 0.5167496800422668\n",
      "Epoch 439, Loss: 2.22465643286705, Final Batch Loss: 0.5685524940490723\n",
      "Epoch 440, Loss: 2.170954316854477, Final Batch Loss: 0.5555089116096497\n",
      "Epoch 441, Loss: 2.0901436507701874, Final Batch Loss: 0.4433697760105133\n",
      "Epoch 442, Loss: 2.1957273483276367, Final Batch Loss: 0.5632781386375427\n",
      "Epoch 443, Loss: 2.152569502592087, Final Batch Loss: 0.4913769066333771\n",
      "Epoch 444, Loss: 2.207254648208618, Final Batch Loss: 0.5600651502609253\n",
      "Epoch 445, Loss: 2.129979729652405, Final Batch Loss: 0.5115118622779846\n",
      "Epoch 446, Loss: 2.160465896129608, Final Batch Loss: 0.4320441484451294\n",
      "Epoch 447, Loss: 2.1693376004695892, Final Batch Loss: 0.44807931780815125\n",
      "Epoch 448, Loss: 2.1967456936836243, Final Batch Loss: 0.5929334163665771\n",
      "Epoch 449, Loss: 2.17799374461174, Final Batch Loss: 0.5777707099914551\n",
      "Epoch 450, Loss: 2.12784543633461, Final Batch Loss: 0.4826515018939972\n",
      "Epoch 451, Loss: 2.1201499700546265, Final Batch Loss: 0.4996327757835388\n",
      "Epoch 452, Loss: 2.1983977556228638, Final Batch Loss: 0.5434296131134033\n",
      "Epoch 453, Loss: 2.1681710183620453, Final Batch Loss: 0.5271292924880981\n",
      "Epoch 454, Loss: 2.1543880105018616, Final Batch Loss: 0.6420535445213318\n",
      "Epoch 455, Loss: 2.2527291774749756, Final Batch Loss: 0.5428959727287292\n",
      "Epoch 456, Loss: 2.1237388253211975, Final Batch Loss: 0.5192582607269287\n",
      "Epoch 457, Loss: 2.094045639038086, Final Batch Loss: 0.5382617712020874\n",
      "Epoch 458, Loss: 2.1587721705436707, Final Batch Loss: 0.5310603380203247\n",
      "Epoch 459, Loss: 2.1013006269931793, Final Batch Loss: 0.5619187951087952\n",
      "Epoch 460, Loss: 2.1451685428619385, Final Batch Loss: 0.5778632760047913\n",
      "Epoch 461, Loss: 2.1954479217529297, Final Batch Loss: 0.5074936151504517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 462, Loss: 2.1255030035972595, Final Batch Loss: 0.5103380680084229\n",
      "Epoch 463, Loss: 2.1357336342334747, Final Batch Loss: 0.5996646881103516\n",
      "Epoch 464, Loss: 2.0075706839561462, Final Batch Loss: 0.44062814116477966\n",
      "Epoch 465, Loss: 2.046993464231491, Final Batch Loss: 0.48246920108795166\n",
      "Epoch 466, Loss: 2.2315720319747925, Final Batch Loss: 0.5085380673408508\n",
      "Epoch 467, Loss: 2.1083309948444366, Final Batch Loss: 0.49768078327178955\n",
      "Epoch 468, Loss: 2.1156634986400604, Final Batch Loss: 0.5866759419441223\n",
      "Epoch 469, Loss: 2.0545939803123474, Final Batch Loss: 0.46277105808258057\n",
      "Epoch 470, Loss: 2.1000480353832245, Final Batch Loss: 0.6672779321670532\n",
      "Epoch 471, Loss: 2.1147424578666687, Final Batch Loss: 0.5777517557144165\n",
      "Epoch 472, Loss: 2.163838803768158, Final Batch Loss: 0.5083230137825012\n",
      "Epoch 473, Loss: 2.078349381685257, Final Batch Loss: 0.6043638586997986\n",
      "Epoch 474, Loss: 2.1315010488033295, Final Batch Loss: 0.5906540751457214\n",
      "Epoch 475, Loss: 2.0315520763397217, Final Batch Loss: 0.49949368834495544\n",
      "Epoch 476, Loss: 2.1133585572242737, Final Batch Loss: 0.5138108134269714\n",
      "Epoch 477, Loss: 2.0706300735473633, Final Batch Loss: 0.5132333636283875\n",
      "Epoch 478, Loss: 2.186674952507019, Final Batch Loss: 0.6483922004699707\n",
      "Epoch 479, Loss: 2.2085816860198975, Final Batch Loss: 0.5878875851631165\n",
      "Epoch 480, Loss: 2.111690431833267, Final Batch Loss: 0.6063408851623535\n",
      "Epoch 481, Loss: 2.0573863089084625, Final Batch Loss: 0.4578739106655121\n",
      "Epoch 482, Loss: 2.0341060757637024, Final Batch Loss: 0.49873775243759155\n",
      "Epoch 483, Loss: 2.200644850730896, Final Batch Loss: 0.5369333624839783\n",
      "Epoch 484, Loss: 2.0053076446056366, Final Batch Loss: 0.5112288594245911\n",
      "Epoch 485, Loss: 2.017997980117798, Final Batch Loss: 0.4297489821910858\n",
      "Epoch 486, Loss: 1.9879490435123444, Final Batch Loss: 0.4999307096004486\n",
      "Epoch 487, Loss: 2.152667224407196, Final Batch Loss: 0.47568583488464355\n",
      "Epoch 488, Loss: 2.1042489409446716, Final Batch Loss: 0.43676817417144775\n",
      "Epoch 489, Loss: 2.0364505648612976, Final Batch Loss: 0.44352987408638\n",
      "Epoch 490, Loss: 2.031991720199585, Final Batch Loss: 0.4784001410007477\n",
      "Epoch 491, Loss: 2.0820367634296417, Final Batch Loss: 0.5552838444709778\n",
      "Epoch 492, Loss: 2.2414914965629578, Final Batch Loss: 0.5947733521461487\n",
      "Epoch 493, Loss: 2.1996328830718994, Final Batch Loss: 0.5690741539001465\n",
      "Epoch 494, Loss: 2.1482006907463074, Final Batch Loss: 0.5496329069137573\n",
      "Epoch 495, Loss: 2.09994375705719, Final Batch Loss: 0.5260241031646729\n",
      "Epoch 496, Loss: 2.066483348608017, Final Batch Loss: 0.5504943132400513\n",
      "Epoch 497, Loss: 2.1219629049301147, Final Batch Loss: 0.5620986819267273\n",
      "Epoch 498, Loss: 2.0685127675533295, Final Batch Loss: 0.617967426776886\n",
      "Epoch 499, Loss: 2.058118313550949, Final Batch Loss: 0.556465208530426\n",
      "Epoch 500, Loss: 2.1538748145103455, Final Batch Loss: 0.5618203282356262\n",
      "Epoch 501, Loss: 2.0944012105464935, Final Batch Loss: 0.5936635732650757\n",
      "Epoch 502, Loss: 2.024435132741928, Final Batch Loss: 0.5693140029907227\n",
      "Epoch 503, Loss: 2.1565245985984802, Final Batch Loss: 0.600556492805481\n",
      "Epoch 504, Loss: 2.155767560005188, Final Batch Loss: 0.5398030281066895\n",
      "Epoch 505, Loss: 2.082996815443039, Final Batch Loss: 0.4835430085659027\n",
      "Epoch 506, Loss: 2.094123989343643, Final Batch Loss: 0.45390769839286804\n",
      "Epoch 507, Loss: 2.082971900701523, Final Batch Loss: 0.48885366320610046\n",
      "Epoch 508, Loss: 2.045244663953781, Final Batch Loss: 0.5676349401473999\n",
      "Epoch 509, Loss: 2.084915518760681, Final Batch Loss: 0.44291365146636963\n",
      "Epoch 510, Loss: 2.0173069834709167, Final Batch Loss: 0.5665116906166077\n",
      "Epoch 511, Loss: 2.0711009800434113, Final Batch Loss: 0.48415371775627136\n",
      "Epoch 512, Loss: 2.060282289981842, Final Batch Loss: 0.5188280344009399\n",
      "Epoch 513, Loss: 2.041357308626175, Final Batch Loss: 0.586040735244751\n",
      "Epoch 514, Loss: 2.1572776436805725, Final Batch Loss: 0.5790711641311646\n",
      "Epoch 515, Loss: 1.9929152727127075, Final Batch Loss: 0.4175918400287628\n",
      "Epoch 516, Loss: 2.0672072172164917, Final Batch Loss: 0.5316575765609741\n",
      "Epoch 517, Loss: 2.128209203481674, Final Batch Loss: 0.6259321570396423\n",
      "Epoch 518, Loss: 2.149900496006012, Final Batch Loss: 0.50190269947052\n",
      "Epoch 519, Loss: 2.062110960483551, Final Batch Loss: 0.5491220951080322\n",
      "Epoch 520, Loss: 2.0956805050373077, Final Batch Loss: 0.6177074909210205\n",
      "Epoch 521, Loss: 2.0453750789165497, Final Batch Loss: 0.43545421957969666\n",
      "Epoch 522, Loss: 2.092733919620514, Final Batch Loss: 0.5120216608047485\n",
      "Epoch 523, Loss: 2.172827363014221, Final Batch Loss: 0.6190776824951172\n",
      "Epoch 524, Loss: 2.0849626660346985, Final Batch Loss: 0.46503353118896484\n",
      "Epoch 525, Loss: 2.010696619749069, Final Batch Loss: 0.48845669627189636\n",
      "Epoch 526, Loss: 2.1263126134872437, Final Batch Loss: 0.4589501619338989\n",
      "Epoch 527, Loss: 2.129084289073944, Final Batch Loss: 0.5365453362464905\n",
      "Epoch 528, Loss: 2.0160485208034515, Final Batch Loss: 0.5025471448898315\n",
      "Epoch 529, Loss: 2.110794335603714, Final Batch Loss: 0.5851843953132629\n",
      "Epoch 530, Loss: 2.0606658160686493, Final Batch Loss: 0.5418605208396912\n",
      "Epoch 531, Loss: 2.0402159690856934, Final Batch Loss: 0.6145038604736328\n",
      "Epoch 532, Loss: 1.9945800304412842, Final Batch Loss: 0.5487757921218872\n",
      "Epoch 533, Loss: 1.9499090015888214, Final Batch Loss: 0.49764561653137207\n",
      "Epoch 534, Loss: 2.0660192370414734, Final Batch Loss: 0.49540647864341736\n",
      "Epoch 535, Loss: 2.078417181968689, Final Batch Loss: 0.49684494733810425\n",
      "Epoch 536, Loss: 2.1518657505512238, Final Batch Loss: 0.4789990484714508\n",
      "Epoch 537, Loss: 1.9484790563583374, Final Batch Loss: 0.46561118960380554\n",
      "Epoch 538, Loss: 2.037333518266678, Final Batch Loss: 0.45140859484672546\n",
      "Epoch 539, Loss: 2.076581060886383, Final Batch Loss: 0.5002650022506714\n",
      "Epoch 540, Loss: 1.935545176267624, Final Batch Loss: 0.4603387713432312\n",
      "Epoch 541, Loss: 2.0782045125961304, Final Batch Loss: 0.5268585681915283\n",
      "Epoch 542, Loss: 2.085719257593155, Final Batch Loss: 0.5715106129646301\n",
      "Epoch 543, Loss: 2.042665868997574, Final Batch Loss: 0.5304946303367615\n",
      "Epoch 544, Loss: 2.0535008311271667, Final Batch Loss: 0.62354975938797\n",
      "Epoch 545, Loss: 2.044317364692688, Final Batch Loss: 0.5056546926498413\n",
      "Epoch 546, Loss: 1.9660447537899017, Final Batch Loss: 0.47584378719329834\n",
      "Epoch 547, Loss: 2.0448231995105743, Final Batch Loss: 0.550707221031189\n",
      "Epoch 548, Loss: 1.9821707606315613, Final Batch Loss: 0.5087512731552124\n",
      "Epoch 549, Loss: 2.0739468932151794, Final Batch Loss: 0.5251383185386658\n",
      "Epoch 550, Loss: 2.0083954632282257, Final Batch Loss: 0.5393410325050354\n",
      "Epoch 551, Loss: 1.9961363077163696, Final Batch Loss: 0.5073652267456055\n",
      "Epoch 552, Loss: 1.9955806732177734, Final Batch Loss: 0.5270670056343079\n",
      "Epoch 553, Loss: 2.0040593445301056, Final Batch Loss: 0.4293507933616638\n",
      "Epoch 554, Loss: 2.1009711921215057, Final Batch Loss: 0.547684907913208\n",
      "Epoch 555, Loss: 1.9039698541164398, Final Batch Loss: 0.4592452049255371\n",
      "Epoch 556, Loss: 2.122648924589157, Final Batch Loss: 0.6439889073371887\n",
      "Epoch 557, Loss: 2.0257659554481506, Final Batch Loss: 0.5164565443992615\n",
      "Epoch 558, Loss: 2.075390338897705, Final Batch Loss: 0.41590094566345215\n",
      "Epoch 559, Loss: 1.9286290109157562, Final Batch Loss: 0.4002746045589447\n",
      "Epoch 560, Loss: 1.925551325082779, Final Batch Loss: 0.47501665353775024\n",
      "Epoch 561, Loss: 2.0590960681438446, Final Batch Loss: 0.5207756757736206\n",
      "Epoch 562, Loss: 1.9300417304039001, Final Batch Loss: 0.4236467778682709\n",
      "Epoch 563, Loss: 2.0010862052440643, Final Batch Loss: 0.4821430742740631\n",
      "Epoch 564, Loss: 1.972538024187088, Final Batch Loss: 0.3986842930316925\n",
      "Epoch 565, Loss: 1.9134386777877808, Final Batch Loss: 0.44014325737953186\n",
      "Epoch 566, Loss: 1.9882216453552246, Final Batch Loss: 0.5000900626182556\n",
      "Epoch 567, Loss: 2.193908452987671, Final Batch Loss: 0.625567615032196\n",
      "Epoch 568, Loss: 2.056973934173584, Final Batch Loss: 0.5123575329780579\n",
      "Epoch 569, Loss: 1.9801820516586304, Final Batch Loss: 0.5032286047935486\n",
      "Epoch 570, Loss: 2.0093601644039154, Final Batch Loss: 0.4870920181274414\n",
      "Epoch 571, Loss: 2.080971360206604, Final Batch Loss: 0.5211164355278015\n",
      "Epoch 572, Loss: 2.082792431116104, Final Batch Loss: 0.5361255407333374\n",
      "Epoch 573, Loss: 2.0586548447608948, Final Batch Loss: 0.4867516756057739\n",
      "Epoch 574, Loss: 2.051988273859024, Final Batch Loss: 0.5248192548751831\n",
      "Epoch 575, Loss: 1.9372137784957886, Final Batch Loss: 0.5167530179023743\n",
      "Epoch 576, Loss: 2.0875095427036285, Final Batch Loss: 0.5091938376426697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 577, Loss: 1.9920955896377563, Final Batch Loss: 0.5039730072021484\n",
      "Epoch 578, Loss: 1.95815771818161, Final Batch Loss: 0.44961658120155334\n",
      "Epoch 579, Loss: 1.9727870523929596, Final Batch Loss: 0.558763325214386\n",
      "Epoch 580, Loss: 2.0291627049446106, Final Batch Loss: 0.5376506447792053\n",
      "Epoch 581, Loss: 1.9611070454120636, Final Batch Loss: 0.4711107909679413\n",
      "Epoch 582, Loss: 1.8962903320789337, Final Batch Loss: 0.44964390993118286\n",
      "Epoch 583, Loss: 2.0431722700595856, Final Batch Loss: 0.519787073135376\n",
      "Epoch 584, Loss: 1.922258347272873, Final Batch Loss: 0.4998340308666229\n",
      "Epoch 585, Loss: 2.012185573577881, Final Batch Loss: 0.5060771703720093\n",
      "Epoch 586, Loss: 1.9714904427528381, Final Batch Loss: 0.4978199601173401\n",
      "Epoch 587, Loss: 2.081865817308426, Final Batch Loss: 0.4493766725063324\n",
      "Epoch 588, Loss: 1.9679621756076813, Final Batch Loss: 0.5127089619636536\n",
      "Epoch 589, Loss: 1.910868525505066, Final Batch Loss: 0.4320753216743469\n",
      "Epoch 590, Loss: 1.9916283190250397, Final Batch Loss: 0.5530356764793396\n",
      "Epoch 591, Loss: 1.9868074357509613, Final Batch Loss: 0.5053333044052124\n",
      "Epoch 592, Loss: 1.8709765374660492, Final Batch Loss: 0.4052960276603699\n",
      "Epoch 593, Loss: 2.0024398863315582, Final Batch Loss: 0.45978114008903503\n",
      "Epoch 594, Loss: 2.078671306371689, Final Batch Loss: 0.5431527495384216\n",
      "Epoch 595, Loss: 1.9700265228748322, Final Batch Loss: 0.48636895418167114\n",
      "Epoch 596, Loss: 1.9608040750026703, Final Batch Loss: 0.508699357509613\n",
      "Epoch 597, Loss: 1.9753319919109344, Final Batch Loss: 0.49089545011520386\n",
      "Epoch 598, Loss: 2.010067880153656, Final Batch Loss: 0.5357099175453186\n",
      "Epoch 599, Loss: 1.9658661484718323, Final Batch Loss: 0.5713099241256714\n",
      "Epoch 600, Loss: 2.0094820857048035, Final Batch Loss: 0.5334415435791016\n",
      "Epoch 601, Loss: 1.8466692566871643, Final Batch Loss: 0.36883461475372314\n",
      "Epoch 602, Loss: 2.0812937021255493, Final Batch Loss: 0.5880394577980042\n",
      "Epoch 603, Loss: 1.9188953042030334, Final Batch Loss: 0.48299291729927063\n",
      "Epoch 604, Loss: 1.985561341047287, Final Batch Loss: 0.53135746717453\n",
      "Epoch 605, Loss: 1.9564370810985565, Final Batch Loss: 0.4685880243778229\n",
      "Epoch 606, Loss: 2.07046240568161, Final Batch Loss: 0.47400781512260437\n",
      "Epoch 607, Loss: 1.960997849702835, Final Batch Loss: 0.476328581571579\n",
      "Epoch 608, Loss: 1.9619528651237488, Final Batch Loss: 0.5373721122741699\n",
      "Epoch 609, Loss: 1.917118638753891, Final Batch Loss: 0.46763232350349426\n",
      "Epoch 610, Loss: 2.050717979669571, Final Batch Loss: 0.47877323627471924\n",
      "Epoch 611, Loss: 1.9079298079013824, Final Batch Loss: 0.4545685052871704\n",
      "Epoch 612, Loss: 1.9689017534255981, Final Batch Loss: 0.540266215801239\n",
      "Epoch 613, Loss: 1.968269407749176, Final Batch Loss: 0.5062601566314697\n",
      "Epoch 614, Loss: 1.9221910238265991, Final Batch Loss: 0.5026743412017822\n",
      "Epoch 615, Loss: 1.9072479605674744, Final Batch Loss: 0.46036872267723083\n",
      "Epoch 616, Loss: 1.9427663683891296, Final Batch Loss: 0.5291163921356201\n",
      "Epoch 617, Loss: 1.932358831167221, Final Batch Loss: 0.43894433975219727\n",
      "Epoch 618, Loss: 2.1020407676696777, Final Batch Loss: 0.5331629514694214\n",
      "Epoch 619, Loss: 2.086127758026123, Final Batch Loss: 0.6347503662109375\n",
      "Epoch 620, Loss: 1.9383468329906464, Final Batch Loss: 0.4662787914276123\n",
      "Epoch 621, Loss: 1.989519715309143, Final Batch Loss: 0.4564976394176483\n",
      "Epoch 622, Loss: 1.9110713601112366, Final Batch Loss: 0.37375885248184204\n",
      "Epoch 623, Loss: 1.8493501842021942, Final Batch Loss: 0.39625000953674316\n",
      "Epoch 624, Loss: 1.9493618309497833, Final Batch Loss: 0.45511019229888916\n",
      "Epoch 625, Loss: 1.9891504049301147, Final Batch Loss: 0.48874038457870483\n",
      "Epoch 626, Loss: 1.8638760447502136, Final Batch Loss: 0.4721619188785553\n",
      "Epoch 627, Loss: 2.0108634531497955, Final Batch Loss: 0.45704761147499084\n",
      "Epoch 628, Loss: 1.9851210415363312, Final Batch Loss: 0.4900665283203125\n",
      "Epoch 629, Loss: 1.9652656316757202, Final Batch Loss: 0.5755079388618469\n",
      "Epoch 630, Loss: 1.8454944789409637, Final Batch Loss: 0.4088270664215088\n",
      "Epoch 631, Loss: 1.8935836553573608, Final Batch Loss: 0.47145307064056396\n",
      "Epoch 632, Loss: 1.883742481470108, Final Batch Loss: 0.352009654045105\n",
      "Epoch 633, Loss: 1.852792650461197, Final Batch Loss: 0.37317389249801636\n",
      "Epoch 634, Loss: 1.9100894927978516, Final Batch Loss: 0.512050986289978\n",
      "Epoch 635, Loss: 1.9601748287677765, Final Batch Loss: 0.5716583132743835\n",
      "Epoch 636, Loss: 1.8499557375907898, Final Batch Loss: 0.43838605284690857\n",
      "Epoch 637, Loss: 1.8161123394966125, Final Batch Loss: 0.4252545237541199\n",
      "Epoch 638, Loss: 1.9485613703727722, Final Batch Loss: 0.5174548625946045\n",
      "Epoch 639, Loss: 1.878936231136322, Final Batch Loss: 0.3824659585952759\n",
      "Epoch 640, Loss: 1.9242566227912903, Final Batch Loss: 0.43941614031791687\n",
      "Epoch 641, Loss: 1.8635976612567902, Final Batch Loss: 0.38121724128723145\n",
      "Epoch 642, Loss: 1.849018931388855, Final Batch Loss: 0.4857468605041504\n",
      "Epoch 643, Loss: 2.004265695810318, Final Batch Loss: 0.4910964071750641\n",
      "Epoch 644, Loss: 1.939359039068222, Final Batch Loss: 0.5101441740989685\n",
      "Epoch 645, Loss: 1.945171982049942, Final Batch Loss: 0.5057761669158936\n",
      "Epoch 646, Loss: 1.9252164661884308, Final Batch Loss: 0.42737603187561035\n",
      "Epoch 647, Loss: 1.831505447626114, Final Batch Loss: 0.38984811305999756\n",
      "Epoch 648, Loss: 1.882701814174652, Final Batch Loss: 0.3977435231208801\n",
      "Epoch 649, Loss: 1.8919348418712616, Final Batch Loss: 0.4837186932563782\n",
      "Epoch 650, Loss: 1.9098107516765594, Final Batch Loss: 0.46620357036590576\n",
      "Epoch 651, Loss: 1.878109097480774, Final Batch Loss: 0.41768914461135864\n",
      "Epoch 652, Loss: 1.8448071777820587, Final Batch Loss: 0.3995867371559143\n",
      "Epoch 653, Loss: 1.817411482334137, Final Batch Loss: 0.5100556015968323\n",
      "Epoch 654, Loss: 2.0434131622314453, Final Batch Loss: 0.5947331786155701\n",
      "Epoch 655, Loss: 2.0360925495624542, Final Batch Loss: 0.549271821975708\n",
      "Epoch 656, Loss: 1.8829668462276459, Final Batch Loss: 0.46796715259552\n",
      "Epoch 657, Loss: 1.9313940405845642, Final Batch Loss: 0.4871584177017212\n",
      "Epoch 658, Loss: 1.862245500087738, Final Batch Loss: 0.4394128918647766\n",
      "Epoch 659, Loss: 1.9083750545978546, Final Batch Loss: 0.47870543599128723\n",
      "Epoch 660, Loss: 1.9091720283031464, Final Batch Loss: 0.4432438015937805\n",
      "Epoch 661, Loss: 1.8376007080078125, Final Batch Loss: 0.4453010857105255\n",
      "Epoch 662, Loss: 1.891989529132843, Final Batch Loss: 0.4635051190853119\n",
      "Epoch 663, Loss: 1.855896681547165, Final Batch Loss: 0.48409098386764526\n",
      "Epoch 664, Loss: 1.9381726682186127, Final Batch Loss: 0.4911729693412781\n",
      "Epoch 665, Loss: 1.9069130718708038, Final Batch Loss: 0.47404006123542786\n",
      "Epoch 666, Loss: 1.8408259749412537, Final Batch Loss: 0.417812317609787\n",
      "Epoch 667, Loss: 1.942774474620819, Final Batch Loss: 0.5001686215400696\n",
      "Epoch 668, Loss: 2.0101384818553925, Final Batch Loss: 0.6144412755966187\n",
      "Epoch 669, Loss: 1.8067791759967804, Final Batch Loss: 0.3839321434497833\n",
      "Epoch 670, Loss: 1.757902204990387, Final Batch Loss: 0.4781395494937897\n",
      "Epoch 671, Loss: 1.8242367208003998, Final Batch Loss: 0.4781506359577179\n",
      "Epoch 672, Loss: 2.019077092409134, Final Batch Loss: 0.4465307593345642\n",
      "Epoch 673, Loss: 1.948157548904419, Final Batch Loss: 0.5416502356529236\n",
      "Epoch 674, Loss: 1.924612134695053, Final Batch Loss: 0.4685591459274292\n",
      "Epoch 675, Loss: 1.8084069788455963, Final Batch Loss: 0.42570436000823975\n",
      "Epoch 676, Loss: 1.886982023715973, Final Batch Loss: 0.4866679608821869\n",
      "Epoch 677, Loss: 1.8827728629112244, Final Batch Loss: 0.44161537289619446\n",
      "Epoch 678, Loss: 1.7814072370529175, Final Batch Loss: 0.376443475484848\n",
      "Epoch 679, Loss: 1.7083091735839844, Final Batch Loss: 0.483855664730072\n",
      "Epoch 680, Loss: 1.776223599910736, Final Batch Loss: 0.4157748818397522\n",
      "Epoch 681, Loss: 1.881332516670227, Final Batch Loss: 0.4320135712623596\n",
      "Epoch 682, Loss: 1.8615688383579254, Final Batch Loss: 0.46001169085502625\n",
      "Epoch 683, Loss: 1.8996574580669403, Final Batch Loss: 0.5835446119308472\n",
      "Epoch 684, Loss: 1.7944045066833496, Final Batch Loss: 0.40903863310813904\n",
      "Epoch 685, Loss: 1.9546043276786804, Final Batch Loss: 0.48735249042510986\n",
      "Epoch 686, Loss: 1.808831512928009, Final Batch Loss: 0.4419421851634979\n",
      "Epoch 687, Loss: 1.8483320772647858, Final Batch Loss: 0.41533103585243225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 688, Loss: 1.8294244706630707, Final Batch Loss: 0.4046889841556549\n",
      "Epoch 689, Loss: 1.890891194343567, Final Batch Loss: 0.49465274810791016\n",
      "Epoch 690, Loss: 1.8640623986721039, Final Batch Loss: 0.4641936123371124\n",
      "Epoch 691, Loss: 1.8661259710788727, Final Batch Loss: 0.47509005665779114\n",
      "Epoch 692, Loss: 1.8733049631118774, Final Batch Loss: 0.42448604106903076\n",
      "Epoch 693, Loss: 2.074081063270569, Final Batch Loss: 0.4972860813140869\n",
      "Epoch 694, Loss: 1.85788494348526, Final Batch Loss: 0.46350520849227905\n",
      "Epoch 695, Loss: 1.8955897986888885, Final Batch Loss: 0.49193641543388367\n",
      "Epoch 696, Loss: 1.7891098260879517, Final Batch Loss: 0.48929286003112793\n",
      "Epoch 697, Loss: 1.960775226354599, Final Batch Loss: 0.5689515471458435\n",
      "Epoch 698, Loss: 1.9459209442138672, Final Batch Loss: 0.5535749793052673\n",
      "Epoch 699, Loss: 1.8315180838108063, Final Batch Loss: 0.41258999705314636\n",
      "Epoch 700, Loss: 1.9244992434978485, Final Batch Loss: 0.43897342681884766\n",
      "Epoch 701, Loss: 1.957584023475647, Final Batch Loss: 0.5203343629837036\n",
      "Epoch 702, Loss: 1.9025372564792633, Final Batch Loss: 0.47253188490867615\n",
      "Epoch 703, Loss: 1.9861874878406525, Final Batch Loss: 0.5028077960014343\n",
      "Epoch 704, Loss: 1.8795410096645355, Final Batch Loss: 0.4487660527229309\n",
      "Epoch 705, Loss: 1.9443630278110504, Final Batch Loss: 0.5120111703872681\n",
      "Epoch 706, Loss: 1.8576232194900513, Final Batch Loss: 0.4663894474506378\n",
      "Epoch 707, Loss: 1.7690431773662567, Final Batch Loss: 0.4698922336101532\n",
      "Epoch 708, Loss: 1.9193366765975952, Final Batch Loss: 0.5075164437294006\n",
      "Epoch 709, Loss: 1.8112212717533112, Final Batch Loss: 0.48937711119651794\n",
      "Epoch 710, Loss: 1.9446170628070831, Final Batch Loss: 0.5058566927909851\n",
      "Epoch 711, Loss: 1.9087361693382263, Final Batch Loss: 0.4454270601272583\n",
      "Epoch 712, Loss: 1.8605446219444275, Final Batch Loss: 0.4787116050720215\n",
      "Epoch 713, Loss: 1.88920658826828, Final Batch Loss: 0.4254283607006073\n",
      "Epoch 714, Loss: 1.8912041187286377, Final Batch Loss: 0.40217071771621704\n",
      "Epoch 715, Loss: 1.8583666682243347, Final Batch Loss: 0.47290751338005066\n",
      "Epoch 716, Loss: 1.8127457201480865, Final Batch Loss: 0.4005616009235382\n",
      "Epoch 717, Loss: 1.7953167259693146, Final Batch Loss: 0.40267518162727356\n",
      "Epoch 718, Loss: 1.951640009880066, Final Batch Loss: 0.4833374321460724\n",
      "Epoch 719, Loss: 1.8721519112586975, Final Batch Loss: 0.5445078611373901\n",
      "Epoch 720, Loss: 1.949641466140747, Final Batch Loss: 0.4833773970603943\n",
      "Epoch 721, Loss: 1.8900292217731476, Final Batch Loss: 0.4332982897758484\n",
      "Epoch 722, Loss: 1.956783652305603, Final Batch Loss: 0.46123114228248596\n",
      "Epoch 723, Loss: 1.9380446374416351, Final Batch Loss: 0.4777407944202423\n",
      "Epoch 724, Loss: 1.8208004534244537, Final Batch Loss: 0.48282912373542786\n",
      "Epoch 725, Loss: 1.7586491405963898, Final Batch Loss: 0.42915815114974976\n",
      "Epoch 726, Loss: 1.8332305252552032, Final Batch Loss: 0.4738891124725342\n",
      "Epoch 727, Loss: 1.7990560829639435, Final Batch Loss: 0.40917831659317017\n",
      "Epoch 728, Loss: 1.8941212892532349, Final Batch Loss: 0.44642123579978943\n",
      "Epoch 729, Loss: 1.9443420469760895, Final Batch Loss: 0.5128055810928345\n",
      "Epoch 730, Loss: 1.762730449438095, Final Batch Loss: 0.44764620065689087\n",
      "Epoch 731, Loss: 1.8152412176132202, Final Batch Loss: 0.5037743449211121\n",
      "Epoch 732, Loss: 1.9633323848247528, Final Batch Loss: 0.5588369965553284\n",
      "Epoch 733, Loss: 1.7530434727668762, Final Batch Loss: 0.4637543857097626\n",
      "Epoch 734, Loss: 1.8515622019767761, Final Batch Loss: 0.5050939321517944\n",
      "Epoch 735, Loss: 1.8961547315120697, Final Batch Loss: 0.5455227494239807\n",
      "Epoch 736, Loss: 2.0237089693546295, Final Batch Loss: 0.4651890993118286\n",
      "Epoch 737, Loss: 1.7977970838546753, Final Batch Loss: 0.4558942914009094\n",
      "Epoch 738, Loss: 1.824270635843277, Final Batch Loss: 0.43899914622306824\n",
      "Epoch 739, Loss: 1.8284951150417328, Final Batch Loss: 0.43150249123573303\n",
      "Epoch 740, Loss: 1.7993691861629486, Final Batch Loss: 0.5077568292617798\n",
      "Epoch 741, Loss: 1.8053550720214844, Final Batch Loss: 0.48160821199417114\n",
      "Epoch 742, Loss: 1.76846843957901, Final Batch Loss: 0.4673330783843994\n",
      "Epoch 743, Loss: 1.709599882364273, Final Batch Loss: 0.3606472313404083\n",
      "Epoch 744, Loss: 1.8201197981834412, Final Batch Loss: 0.3904028534889221\n",
      "Epoch 745, Loss: 1.876985490322113, Final Batch Loss: 0.5202699303627014\n",
      "Epoch 746, Loss: 1.7800894975662231, Final Batch Loss: 0.46463096141815186\n",
      "Epoch 747, Loss: 1.8523383736610413, Final Batch Loss: 0.4809015393257141\n",
      "Epoch 748, Loss: 1.812397301197052, Final Batch Loss: 0.3663688898086548\n",
      "Epoch 749, Loss: 1.8113319277763367, Final Batch Loss: 0.5130215287208557\n",
      "Epoch 750, Loss: 1.9391910433769226, Final Batch Loss: 0.5047610998153687\n",
      "Epoch 751, Loss: 1.861639678478241, Final Batch Loss: 0.4578418433666229\n",
      "Epoch 752, Loss: 1.7745502591133118, Final Batch Loss: 0.4912087619304657\n",
      "Epoch 753, Loss: 1.851325660943985, Final Batch Loss: 0.4208289682865143\n",
      "Epoch 754, Loss: 1.7658125758171082, Final Batch Loss: 0.41437995433807373\n",
      "Epoch 755, Loss: 1.867067277431488, Final Batch Loss: 0.38897621631622314\n",
      "Epoch 756, Loss: 1.7038471698760986, Final Batch Loss: 0.3407061696052551\n",
      "Epoch 757, Loss: 1.8242627680301666, Final Batch Loss: 0.48821133375167847\n",
      "Epoch 758, Loss: 1.8359168767929077, Final Batch Loss: 0.4104810953140259\n",
      "Epoch 759, Loss: 1.94497749209404, Final Batch Loss: 0.47581687569618225\n",
      "Epoch 760, Loss: 1.7917941808700562, Final Batch Loss: 0.321770578622818\n",
      "Epoch 761, Loss: 1.7350323796272278, Final Batch Loss: 0.40998342633247375\n",
      "Epoch 762, Loss: 1.8834961354732513, Final Batch Loss: 0.4609270691871643\n",
      "Epoch 763, Loss: 1.7981561422348022, Final Batch Loss: 0.3516603112220764\n",
      "Epoch 764, Loss: 1.815176010131836, Final Batch Loss: 0.4198707044124603\n",
      "Epoch 765, Loss: 1.8330805003643036, Final Batch Loss: 0.5232731699943542\n",
      "Epoch 766, Loss: 1.7963370978832245, Final Batch Loss: 0.4836540222167969\n",
      "Epoch 767, Loss: 1.7560548186302185, Final Batch Loss: 0.41431495547294617\n",
      "Epoch 768, Loss: 1.8096288740634918, Final Batch Loss: 0.45470955967903137\n",
      "Epoch 769, Loss: 1.834742158651352, Final Batch Loss: 0.5154004096984863\n",
      "Epoch 770, Loss: 1.8318894505500793, Final Batch Loss: 0.4707486927509308\n",
      "Epoch 771, Loss: 1.843799352645874, Final Batch Loss: 0.4221701920032501\n",
      "Epoch 772, Loss: 1.9279994070529938, Final Batch Loss: 0.5858781337738037\n",
      "Epoch 773, Loss: 1.7044646739959717, Final Batch Loss: 0.42615437507629395\n",
      "Epoch 774, Loss: 1.8721454739570618, Final Batch Loss: 0.48822104930877686\n",
      "Epoch 775, Loss: 1.8561426401138306, Final Batch Loss: 0.5023242235183716\n",
      "Epoch 776, Loss: 1.8270321190357208, Final Batch Loss: 0.4536316394805908\n",
      "Epoch 777, Loss: 1.8462632298469543, Final Batch Loss: 0.4407709538936615\n",
      "Epoch 778, Loss: 1.824655294418335, Final Batch Loss: 0.5805640816688538\n",
      "Epoch 779, Loss: 1.8933209776878357, Final Batch Loss: 0.4683019518852234\n",
      "Epoch 780, Loss: 1.8354138731956482, Final Batch Loss: 0.481404185295105\n",
      "Epoch 781, Loss: 1.8508028388023376, Final Batch Loss: 0.4412837624549866\n",
      "Epoch 782, Loss: 1.839099407196045, Final Batch Loss: 0.47336575388908386\n",
      "Epoch 783, Loss: 1.9135561883449554, Final Batch Loss: 0.5482428669929504\n",
      "Epoch 784, Loss: 1.8480375111103058, Final Batch Loss: 0.43345722556114197\n",
      "Epoch 785, Loss: 1.8245187997817993, Final Batch Loss: 0.4628821015357971\n",
      "Epoch 786, Loss: 1.8157658576965332, Final Batch Loss: 0.48645341396331787\n",
      "Epoch 787, Loss: 1.7124495804309845, Final Batch Loss: 0.31465867161750793\n",
      "Epoch 788, Loss: 1.7296831011772156, Final Batch Loss: 0.41858160495758057\n",
      "Epoch 789, Loss: 1.8300385177135468, Final Batch Loss: 0.5017264485359192\n",
      "Epoch 790, Loss: 1.800623893737793, Final Batch Loss: 0.5311995148658752\n",
      "Epoch 791, Loss: 1.7414034605026245, Final Batch Loss: 0.4493227005004883\n",
      "Epoch 792, Loss: 1.7627134919166565, Final Batch Loss: 0.43175622820854187\n",
      "Epoch 793, Loss: 1.8399298787117004, Final Batch Loss: 0.42375048995018005\n",
      "Epoch 794, Loss: 1.9930529296398163, Final Batch Loss: 0.535795271396637\n",
      "Epoch 795, Loss: 1.7234908044338226, Final Batch Loss: 0.36818763613700867\n",
      "Epoch 796, Loss: 1.7525063157081604, Final Batch Loss: 0.3598467707633972\n",
      "Epoch 797, Loss: 1.7248404026031494, Final Batch Loss: 0.4372352361679077\n",
      "Epoch 798, Loss: 1.8085018694400787, Final Batch Loss: 0.4327256381511688\n",
      "Epoch 799, Loss: 1.7991321682929993, Final Batch Loss: 0.4244401156902313\n",
      "Epoch 800, Loss: 1.7422500848770142, Final Batch Loss: 0.37454286217689514\n",
      "Epoch 801, Loss: 1.7930286526679993, Final Batch Loss: 0.4307374954223633\n",
      "Epoch 802, Loss: 1.840043306350708, Final Batch Loss: 0.5138424634933472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 803, Loss: 1.8197730779647827, Final Batch Loss: 0.4164818227291107\n",
      "Epoch 804, Loss: 1.8738275468349457, Final Batch Loss: 0.5114067196846008\n",
      "Epoch 805, Loss: 1.8303504884243011, Final Batch Loss: 0.5039497017860413\n",
      "Epoch 806, Loss: 1.7120963335037231, Final Batch Loss: 0.40818798542022705\n",
      "Epoch 807, Loss: 1.7711267471313477, Final Batch Loss: 0.4404308497905731\n",
      "Epoch 808, Loss: 1.8531213402748108, Final Batch Loss: 0.4540582597255707\n",
      "Epoch 809, Loss: 1.860916256904602, Final Batch Loss: 0.44578349590301514\n",
      "Epoch 810, Loss: 1.7392502129077911, Final Batch Loss: 0.4559650123119354\n",
      "Epoch 811, Loss: 1.863925725221634, Final Batch Loss: 0.5613695979118347\n",
      "Epoch 812, Loss: 1.7484096586704254, Final Batch Loss: 0.39344269037246704\n",
      "Epoch 813, Loss: 1.782689392566681, Final Batch Loss: 0.541283130645752\n",
      "Epoch 814, Loss: 1.743246853351593, Final Batch Loss: 0.4424295127391815\n",
      "Epoch 815, Loss: 1.745193362236023, Final Batch Loss: 0.38603323698043823\n",
      "Epoch 816, Loss: 1.7311199009418488, Final Batch Loss: 0.44011369347572327\n",
      "Epoch 817, Loss: 2.0078468322753906, Final Batch Loss: 0.5080503225326538\n",
      "Epoch 818, Loss: 1.8212862610816956, Final Batch Loss: 0.41223806142807007\n",
      "Epoch 819, Loss: 1.7667738795280457, Final Batch Loss: 0.39679986238479614\n",
      "Epoch 820, Loss: 1.9767352044582367, Final Batch Loss: 0.5133541226387024\n",
      "Epoch 821, Loss: 1.8412772715091705, Final Batch Loss: 0.43594786524772644\n",
      "Epoch 822, Loss: 1.7128979861736298, Final Batch Loss: 0.42295652627944946\n",
      "Epoch 823, Loss: 1.8269113302230835, Final Batch Loss: 0.4029000699520111\n",
      "Epoch 824, Loss: 1.8139114081859589, Final Batch Loss: 0.4123910367488861\n",
      "Epoch 825, Loss: 1.6968081891536713, Final Batch Loss: 0.45908334851264954\n",
      "Epoch 826, Loss: 1.7948755323886871, Final Batch Loss: 0.4281165301799774\n",
      "Epoch 827, Loss: 1.7670295238494873, Final Batch Loss: 0.4183356463909149\n",
      "Epoch 828, Loss: 1.8360566198825836, Final Batch Loss: 0.45380640029907227\n",
      "Epoch 829, Loss: 1.7360436916351318, Final Batch Loss: 0.3849763870239258\n",
      "Epoch 830, Loss: 1.8765502870082855, Final Batch Loss: 0.47299906611442566\n",
      "Epoch 831, Loss: 1.8164282441139221, Final Batch Loss: 0.47993120551109314\n",
      "Epoch 832, Loss: 1.828914850950241, Final Batch Loss: 0.3938750922679901\n",
      "Epoch 833, Loss: 1.6940489411354065, Final Batch Loss: 0.42938265204429626\n",
      "Epoch 834, Loss: 1.7961994409561157, Final Batch Loss: 0.4715536832809448\n",
      "Epoch 835, Loss: 1.7712381780147552, Final Batch Loss: 0.37710461020469666\n",
      "Epoch 836, Loss: 1.9081606566905975, Final Batch Loss: 0.5520741939544678\n",
      "Epoch 837, Loss: 1.9120001196861267, Final Batch Loss: 0.5235096216201782\n",
      "Epoch 838, Loss: 1.843664824962616, Final Batch Loss: 0.42148908972740173\n",
      "Epoch 839, Loss: 1.7874795794487, Final Batch Loss: 0.4302164316177368\n",
      "Epoch 840, Loss: 1.7745469510555267, Final Batch Loss: 0.45523130893707275\n",
      "Epoch 841, Loss: 1.8352940082550049, Final Batch Loss: 0.4628281593322754\n",
      "Epoch 842, Loss: 1.8153404891490936, Final Batch Loss: 0.4887198209762573\n",
      "Epoch 843, Loss: 1.7580465078353882, Final Batch Loss: 0.529734194278717\n",
      "Epoch 844, Loss: 1.8454018235206604, Final Batch Loss: 0.4368649125099182\n",
      "Epoch 845, Loss: 1.786704272031784, Final Batch Loss: 0.4892839789390564\n",
      "Epoch 846, Loss: 1.7706395089626312, Final Batch Loss: 0.3878241181373596\n",
      "Epoch 847, Loss: 1.7616808414459229, Final Batch Loss: 0.4247325360774994\n",
      "Epoch 848, Loss: 1.6967856287956238, Final Batch Loss: 0.37420889735221863\n",
      "Epoch 849, Loss: 1.7051650285720825, Final Batch Loss: 0.3971785604953766\n",
      "Epoch 850, Loss: 1.8683075308799744, Final Batch Loss: 0.39339837431907654\n",
      "Epoch 851, Loss: 1.7842777967453003, Final Batch Loss: 0.5111663937568665\n",
      "Epoch 852, Loss: 1.6932861506938934, Final Batch Loss: 0.37816503643989563\n",
      "Epoch 853, Loss: 1.7680848836898804, Final Batch Loss: 0.49725598096847534\n",
      "Epoch 854, Loss: 1.6695644855499268, Final Batch Loss: 0.4022555947303772\n",
      "Epoch 855, Loss: 1.722719520330429, Final Batch Loss: 0.37507450580596924\n",
      "Epoch 856, Loss: 1.7922815680503845, Final Batch Loss: 0.43777012825012207\n",
      "Epoch 857, Loss: 1.6809280812740326, Final Batch Loss: 0.5050536394119263\n",
      "Epoch 858, Loss: 1.7337981760501862, Final Batch Loss: 0.49914655089378357\n",
      "Epoch 859, Loss: 1.6972470879554749, Final Batch Loss: 0.43126556277275085\n",
      "Epoch 860, Loss: 1.8124714493751526, Final Batch Loss: 0.44022122025489807\n",
      "Epoch 861, Loss: 1.670926719903946, Final Batch Loss: 0.4355696439743042\n",
      "Epoch 862, Loss: 1.7144568860530853, Final Batch Loss: 0.4067692160606384\n",
      "Epoch 863, Loss: 1.8073837161064148, Final Batch Loss: 0.41348421573638916\n",
      "Epoch 864, Loss: 1.7297279238700867, Final Batch Loss: 0.4850226640701294\n",
      "Epoch 865, Loss: 1.7253819704055786, Final Batch Loss: 0.409870982170105\n",
      "Epoch 866, Loss: 1.8344595432281494, Final Batch Loss: 0.4693719744682312\n",
      "Epoch 867, Loss: 1.8076274693012238, Final Batch Loss: 0.4166773557662964\n",
      "Epoch 868, Loss: 1.6327121257781982, Final Batch Loss: 0.3691037893295288\n",
      "Epoch 869, Loss: 1.6772955060005188, Final Batch Loss: 0.43410399556159973\n",
      "Epoch 870, Loss: 1.7252065241336823, Final Batch Loss: 0.41626396775245667\n",
      "Epoch 871, Loss: 1.7388830184936523, Final Batch Loss: 0.4647983908653259\n",
      "Epoch 872, Loss: 1.8275687098503113, Final Batch Loss: 0.3925080895423889\n",
      "Epoch 873, Loss: 1.726028859615326, Final Batch Loss: 0.41191935539245605\n",
      "Epoch 874, Loss: 1.8549309074878693, Final Batch Loss: 0.4336041808128357\n",
      "Epoch 875, Loss: 1.7539929747581482, Final Batch Loss: 0.4730440378189087\n",
      "Epoch 876, Loss: 1.7600228786468506, Final Batch Loss: 0.4636508822441101\n",
      "Epoch 877, Loss: 1.8523766100406647, Final Batch Loss: 0.49665647745132446\n",
      "Epoch 878, Loss: 1.7614985406398773, Final Batch Loss: 0.42946118116378784\n",
      "Epoch 879, Loss: 1.8628776967525482, Final Batch Loss: 0.4189266860485077\n",
      "Epoch 880, Loss: 1.7419280111789703, Final Batch Loss: 0.4480307996273041\n",
      "Epoch 881, Loss: 1.785735547542572, Final Batch Loss: 0.40142181515693665\n",
      "Epoch 882, Loss: 1.7224290072917938, Final Batch Loss: 0.31887930631637573\n",
      "Epoch 883, Loss: 1.828807145357132, Final Batch Loss: 0.5400575995445251\n",
      "Epoch 884, Loss: 1.6998164057731628, Final Batch Loss: 0.36709457635879517\n",
      "Epoch 885, Loss: 1.7452109456062317, Final Batch Loss: 0.4144739806652069\n",
      "Epoch 886, Loss: 1.7142436504364014, Final Batch Loss: 0.4542604684829712\n",
      "Epoch 887, Loss: 1.671699345111847, Final Batch Loss: 0.41159385442733765\n",
      "Epoch 888, Loss: 1.71506068110466, Final Batch Loss: 0.39208725094795227\n",
      "Epoch 889, Loss: 1.6925061345100403, Final Batch Loss: 0.41823557019233704\n",
      "Epoch 890, Loss: 1.7351352870464325, Final Batch Loss: 0.3969825208187103\n",
      "Epoch 891, Loss: 1.6879579722881317, Final Batch Loss: 0.4604487121105194\n",
      "Epoch 892, Loss: 1.683457762002945, Final Batch Loss: 0.5063072443008423\n",
      "Epoch 893, Loss: 1.749767154455185, Final Batch Loss: 0.45750677585601807\n",
      "Epoch 894, Loss: 1.688103437423706, Final Batch Loss: 0.40970444679260254\n",
      "Epoch 895, Loss: 1.7766429781913757, Final Batch Loss: 0.412393718957901\n",
      "Epoch 896, Loss: 1.7880648076534271, Final Batch Loss: 0.4457061290740967\n",
      "Epoch 897, Loss: 1.6934309303760529, Final Batch Loss: 0.49700048565864563\n",
      "Epoch 898, Loss: 1.5805926024913788, Final Batch Loss: 0.3088042140007019\n",
      "Epoch 899, Loss: 1.93692946434021, Final Batch Loss: 0.6014721393585205\n",
      "Epoch 900, Loss: 1.7128128707408905, Final Batch Loss: 0.366737961769104\n",
      "Epoch 901, Loss: 1.7122159004211426, Final Batch Loss: 0.3686998784542084\n",
      "Epoch 902, Loss: 1.7004957795143127, Final Batch Loss: 0.5150209069252014\n",
      "Epoch 903, Loss: 1.7690690457820892, Final Batch Loss: 0.4999520778656006\n",
      "Epoch 904, Loss: 1.743886649608612, Final Batch Loss: 0.4164217710494995\n",
      "Epoch 905, Loss: 1.6525371670722961, Final Batch Loss: 0.4291379749774933\n",
      "Epoch 906, Loss: 1.648140400648117, Final Batch Loss: 0.4174918234348297\n",
      "Epoch 907, Loss: 1.5668480098247528, Final Batch Loss: 0.3472766876220703\n",
      "Epoch 908, Loss: 1.72060826420784, Final Batch Loss: 0.38899683952331543\n",
      "Epoch 909, Loss: 1.7858244180679321, Final Batch Loss: 0.46777042746543884\n",
      "Epoch 910, Loss: 1.765539437532425, Final Batch Loss: 0.411925733089447\n",
      "Epoch 911, Loss: 1.7020324766635895, Final Batch Loss: 0.4790484309196472\n",
      "Epoch 912, Loss: 1.6565368175506592, Final Batch Loss: 0.4202064573764801\n",
      "Epoch 913, Loss: 1.7250597774982452, Final Batch Loss: 0.4671381413936615\n",
      "Epoch 914, Loss: 1.6450340747833252, Final Batch Loss: 0.36598554253578186\n",
      "Epoch 915, Loss: 1.6482240557670593, Final Batch Loss: 0.4749845862388611\n",
      "Epoch 916, Loss: 1.7056398391723633, Final Batch Loss: 0.3898921608924866\n",
      "Epoch 917, Loss: 1.6697327196598053, Final Batch Loss: 0.4316537380218506\n",
      "Epoch 918, Loss: 1.7322598695755005, Final Batch Loss: 0.42381805181503296\n",
      "Epoch 919, Loss: 1.6988434791564941, Final Batch Loss: 0.43459299206733704\n",
      "Epoch 920, Loss: 1.7483454942703247, Final Batch Loss: 0.43341949582099915\n",
      "Epoch 921, Loss: 1.7174273133277893, Final Batch Loss: 0.44634807109832764\n",
      "Epoch 922, Loss: 1.793010652065277, Final Batch Loss: 0.5171056389808655\n",
      "Epoch 923, Loss: 1.83687424659729, Final Batch Loss: 0.5150660276412964\n",
      "Epoch 924, Loss: 1.6992876529693604, Final Batch Loss: 0.38874107599258423\n",
      "Epoch 925, Loss: 1.6433220505714417, Final Batch Loss: 0.42284056544303894\n",
      "Epoch 926, Loss: 1.719716191291809, Final Batch Loss: 0.5134507417678833\n",
      "Epoch 927, Loss: 1.7443777918815613, Final Batch Loss: 0.4075062572956085\n",
      "Epoch 928, Loss: 1.6970468759536743, Final Batch Loss: 0.4893788695335388\n",
      "Epoch 929, Loss: 1.7242785096168518, Final Batch Loss: 0.4347202479839325\n",
      "Epoch 930, Loss: 1.7536430358886719, Final Batch Loss: 0.40847355127334595\n",
      "Epoch 931, Loss: 1.7859793901443481, Final Batch Loss: 0.47089868783950806\n",
      "Epoch 932, Loss: 1.7650381028652191, Final Batch Loss: 0.3506366014480591\n",
      "Epoch 933, Loss: 1.7233279943466187, Final Batch Loss: 0.42342305183410645\n",
      "Epoch 934, Loss: 1.6630599200725555, Final Batch Loss: 0.42071595788002014\n",
      "Epoch 935, Loss: 1.8321703970432281, Final Batch Loss: 0.514694094657898\n",
      "Epoch 936, Loss: 1.697471410036087, Final Batch Loss: 0.39679405093193054\n",
      "Epoch 937, Loss: 1.723354011774063, Final Batch Loss: 0.4274126887321472\n",
      "Epoch 938, Loss: 1.6889108717441559, Final Batch Loss: 0.39845943450927734\n",
      "Epoch 939, Loss: 1.6449499130249023, Final Batch Loss: 0.3888212740421295\n",
      "Epoch 940, Loss: 1.7032109498977661, Final Batch Loss: 0.3702181577682495\n",
      "Epoch 941, Loss: 1.7874485552310944, Final Batch Loss: 0.46341660618782043\n",
      "Epoch 942, Loss: 1.7552965879440308, Final Batch Loss: 0.4074413478374481\n",
      "Epoch 943, Loss: 1.709935873746872, Final Batch Loss: 0.420789897441864\n",
      "Epoch 944, Loss: 1.5983508825302124, Final Batch Loss: 0.4294799268245697\n",
      "Epoch 945, Loss: 1.7268597185611725, Final Batch Loss: 0.4309246838092804\n",
      "Epoch 946, Loss: 1.6377313435077667, Final Batch Loss: 0.3304060697555542\n",
      "Epoch 947, Loss: 1.7244220972061157, Final Batch Loss: 0.44000181555747986\n",
      "Epoch 948, Loss: 1.7240400910377502, Final Batch Loss: 0.41093116998672485\n",
      "Epoch 949, Loss: 1.6229946613311768, Final Batch Loss: 0.3957798182964325\n",
      "Epoch 950, Loss: 1.6767221987247467, Final Batch Loss: 0.4088462293148041\n",
      "Epoch 951, Loss: 1.6337643563747406, Final Batch Loss: 0.3723880350589752\n",
      "Epoch 952, Loss: 1.7094289064407349, Final Batch Loss: 0.5169939994812012\n",
      "Epoch 953, Loss: 1.6147741675376892, Final Batch Loss: 0.31941473484039307\n",
      "Epoch 954, Loss: 1.6531320810317993, Final Batch Loss: 0.44420868158340454\n",
      "Epoch 955, Loss: 1.67095947265625, Final Batch Loss: 0.3989885449409485\n",
      "Epoch 956, Loss: 1.6760008931159973, Final Batch Loss: 0.46877384185791016\n",
      "Epoch 957, Loss: 1.620257943868637, Final Batch Loss: 0.43434327840805054\n",
      "Epoch 958, Loss: 1.702254444360733, Final Batch Loss: 0.3893587589263916\n",
      "Epoch 959, Loss: 1.5759159922599792, Final Batch Loss: 0.37180060148239136\n",
      "Epoch 960, Loss: 1.6751988232135773, Final Batch Loss: 0.4153653383255005\n",
      "Epoch 961, Loss: 1.7084312736988068, Final Batch Loss: 0.40448784828186035\n",
      "Epoch 962, Loss: 1.67902272939682, Final Batch Loss: 0.4374496638774872\n",
      "Epoch 963, Loss: 1.6054426431655884, Final Batch Loss: 0.37704911828041077\n",
      "Epoch 964, Loss: 1.572282612323761, Final Batch Loss: 0.36739999055862427\n",
      "Epoch 965, Loss: 1.689648300409317, Final Batch Loss: 0.40947431325912476\n",
      "Epoch 966, Loss: 1.6409927904605865, Final Batch Loss: 0.44894659519195557\n",
      "Epoch 967, Loss: 1.501304805278778, Final Batch Loss: 0.3286115229129791\n",
      "Epoch 968, Loss: 1.6741221249103546, Final Batch Loss: 0.45349442958831787\n",
      "Epoch 969, Loss: 1.8061904609203339, Final Batch Loss: 0.41065141558647156\n",
      "Epoch 970, Loss: 1.6775745749473572, Final Batch Loss: 0.4290617108345032\n",
      "Epoch 971, Loss: 1.7267597615718842, Final Batch Loss: 0.42857909202575684\n",
      "Epoch 972, Loss: 1.6374867260456085, Final Batch Loss: 0.38014665246009827\n",
      "Epoch 973, Loss: 1.6531179249286652, Final Batch Loss: 0.3898094892501831\n",
      "Epoch 974, Loss: 1.6110450327396393, Final Batch Loss: 0.4470912516117096\n",
      "Epoch 975, Loss: 1.6769206821918488, Final Batch Loss: 0.47570502758026123\n",
      "Epoch 976, Loss: 1.6394124031066895, Final Batch Loss: 0.439331591129303\n",
      "Epoch 977, Loss: 1.565872311592102, Final Batch Loss: 0.4246785640716553\n",
      "Epoch 978, Loss: 1.5390170812606812, Final Batch Loss: 0.3969208300113678\n",
      "Epoch 979, Loss: 1.7200812101364136, Final Batch Loss: 0.36677056550979614\n",
      "Epoch 980, Loss: 1.6346106231212616, Final Batch Loss: 0.3992897570133209\n",
      "Epoch 981, Loss: 1.6499086022377014, Final Batch Loss: 0.39455753564834595\n",
      "Epoch 982, Loss: 1.5840123295783997, Final Batch Loss: 0.3145015835762024\n",
      "Epoch 983, Loss: 1.6551453471183777, Final Batch Loss: 0.43834999203681946\n",
      "Epoch 984, Loss: 1.634996771812439, Final Batch Loss: 0.3233012855052948\n",
      "Epoch 985, Loss: 1.5743608474731445, Final Batch Loss: 0.397238165140152\n",
      "Epoch 986, Loss: 1.6479195356369019, Final Batch Loss: 0.4321589767932892\n",
      "Epoch 987, Loss: 1.589340090751648, Final Batch Loss: 0.35210487246513367\n",
      "Epoch 988, Loss: 1.5812561213970184, Final Batch Loss: 0.36992132663726807\n",
      "Epoch 989, Loss: 1.6889359951019287, Final Batch Loss: 0.3970704972743988\n",
      "Epoch 990, Loss: 1.6870020627975464, Final Batch Loss: 0.42581599950790405\n",
      "Epoch 991, Loss: 1.5899417400360107, Final Batch Loss: 0.37350234389305115\n",
      "Epoch 992, Loss: 1.6685070395469666, Final Batch Loss: 0.4989320635795593\n",
      "Epoch 993, Loss: 1.6681196093559265, Final Batch Loss: 0.40147748589515686\n",
      "Epoch 994, Loss: 1.6154699623584747, Final Batch Loss: 0.3598240911960602\n",
      "Epoch 995, Loss: 1.6104589998722076, Final Batch Loss: 0.4531373977661133\n",
      "Epoch 996, Loss: 1.6842381954193115, Final Batch Loss: 0.4283396601676941\n",
      "Epoch 997, Loss: 1.6918601095676422, Final Batch Loss: 0.4606032371520996\n",
      "Epoch 998, Loss: 1.599403738975525, Final Batch Loss: 0.41709762811660767\n",
      "Epoch 999, Loss: 1.6624841094017029, Final Batch Loss: 0.412639856338501\n",
      "Epoch 1000, Loss: 1.6719099283218384, Final Batch Loss: 0.4452323019504547\n",
      "Epoch 1001, Loss: 1.7499979138374329, Final Batch Loss: 0.449707955121994\n",
      "Epoch 1002, Loss: 1.611234188079834, Final Batch Loss: 0.37472912669181824\n",
      "Epoch 1003, Loss: 1.6189943253993988, Final Batch Loss: 0.3239438235759735\n",
      "Epoch 1004, Loss: 1.681145042181015, Final Batch Loss: 0.36825570464134216\n",
      "Epoch 1005, Loss: 1.6210557222366333, Final Batch Loss: 0.30608728528022766\n",
      "Epoch 1006, Loss: 1.7210673987865448, Final Batch Loss: 0.3971514403820038\n",
      "Epoch 1007, Loss: 1.6382622718811035, Final Batch Loss: 0.4247461259365082\n",
      "Epoch 1008, Loss: 1.6772640645503998, Final Batch Loss: 0.35681048035621643\n",
      "Epoch 1009, Loss: 1.617745816707611, Final Batch Loss: 0.4397658407688141\n",
      "Epoch 1010, Loss: 1.5708135962486267, Final Batch Loss: 0.40963712334632874\n",
      "Epoch 1011, Loss: 1.5487266778945923, Final Batch Loss: 0.3756253123283386\n",
      "Epoch 1012, Loss: 1.703861117362976, Final Batch Loss: 0.3824203312397003\n",
      "Epoch 1013, Loss: 1.6452263593673706, Final Batch Loss: 0.38922590017318726\n",
      "Epoch 1014, Loss: 1.6127565801143646, Final Batch Loss: 0.33882173895835876\n",
      "Epoch 1015, Loss: 1.5792303085327148, Final Batch Loss: 0.4294528663158417\n",
      "Epoch 1016, Loss: 1.6182828545570374, Final Batch Loss: 0.3746810853481293\n",
      "Epoch 1017, Loss: 1.7791060209274292, Final Batch Loss: 0.4755723178386688\n",
      "Epoch 1018, Loss: 1.6156945824623108, Final Batch Loss: 0.3606066107749939\n",
      "Epoch 1019, Loss: 1.697959989309311, Final Batch Loss: 0.43445414304733276\n",
      "Epoch 1020, Loss: 1.5977903306484222, Final Batch Loss: 0.481426864862442\n",
      "Epoch 1021, Loss: 1.5701890587806702, Final Batch Loss: 0.32208484411239624\n",
      "Epoch 1022, Loss: 1.6337423026561737, Final Batch Loss: 0.43134650588035583\n",
      "Epoch 1023, Loss: 1.6368552148342133, Final Batch Loss: 0.3281121551990509\n",
      "Epoch 1024, Loss: 1.6223470866680145, Final Batch Loss: 0.4171384871006012\n",
      "Epoch 1025, Loss: 1.6947280764579773, Final Batch Loss: 0.48951810598373413\n",
      "Epoch 1026, Loss: 1.5738226473331451, Final Batch Loss: 0.37496218085289\n",
      "Epoch 1027, Loss: 1.5820547938346863, Final Batch Loss: 0.3685847222805023\n",
      "Epoch 1028, Loss: 1.5460178554058075, Final Batch Loss: 0.39980271458625793\n",
      "Epoch 1029, Loss: 1.699966311454773, Final Batch Loss: 0.4239899814128876\n",
      "Epoch 1030, Loss: 1.7312875092029572, Final Batch Loss: 0.4636736810207367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1031, Loss: 1.6235961616039276, Final Batch Loss: 0.41436856985092163\n",
      "Epoch 1032, Loss: 1.5695287883281708, Final Batch Loss: 0.4225764870643616\n",
      "Epoch 1033, Loss: 1.610067754983902, Final Batch Loss: 0.3783878982067108\n",
      "Epoch 1034, Loss: 1.541200339794159, Final Batch Loss: 0.42561075091362\n",
      "Epoch 1035, Loss: 1.6300159096717834, Final Batch Loss: 0.4415126442909241\n",
      "Epoch 1036, Loss: 1.5361888408660889, Final Batch Loss: 0.33916351199150085\n",
      "Epoch 1037, Loss: 1.6246723532676697, Final Batch Loss: 0.3477991819381714\n",
      "Epoch 1038, Loss: 1.7240956127643585, Final Batch Loss: 0.3431769907474518\n",
      "Epoch 1039, Loss: 1.6344923079013824, Final Batch Loss: 0.3284491002559662\n",
      "Epoch 1040, Loss: 1.723438173532486, Final Batch Loss: 0.4631150960922241\n",
      "Epoch 1041, Loss: 1.6687174439430237, Final Batch Loss: 0.43491074442863464\n",
      "Epoch 1042, Loss: 1.655553787946701, Final Batch Loss: 0.39232540130615234\n",
      "Epoch 1043, Loss: 1.4894709587097168, Final Batch Loss: 0.35893428325653076\n",
      "Epoch 1044, Loss: 1.588268131017685, Final Batch Loss: 0.427409827709198\n",
      "Epoch 1045, Loss: 1.5780558288097382, Final Batch Loss: 0.3530685007572174\n",
      "Epoch 1046, Loss: 1.5903766751289368, Final Batch Loss: 0.3859493136405945\n",
      "Epoch 1047, Loss: 1.4980139136314392, Final Batch Loss: 0.3497326970100403\n",
      "Epoch 1048, Loss: 1.6641529500484467, Final Batch Loss: 0.43771612644195557\n",
      "Epoch 1049, Loss: 1.5470410287380219, Final Batch Loss: 0.37839433550834656\n",
      "Epoch 1050, Loss: 1.5897865295410156, Final Batch Loss: 0.37016385793685913\n",
      "Epoch 1051, Loss: 1.6495831608772278, Final Batch Loss: 0.37227290868759155\n",
      "Epoch 1052, Loss: 1.5808476209640503, Final Batch Loss: 0.43650171160697937\n",
      "Epoch 1053, Loss: 1.5995667576789856, Final Batch Loss: 0.43524491786956787\n",
      "Epoch 1054, Loss: 1.527045339345932, Final Batch Loss: 0.4198610484600067\n",
      "Epoch 1055, Loss: 1.5409105718135834, Final Batch Loss: 0.36417633295059204\n",
      "Epoch 1056, Loss: 1.6059115827083588, Final Batch Loss: 0.4296174645423889\n",
      "Epoch 1057, Loss: 1.5048610866069794, Final Batch Loss: 0.3799079954624176\n",
      "Epoch 1058, Loss: 1.5640612840652466, Final Batch Loss: 0.3940492868423462\n",
      "Epoch 1059, Loss: 1.4364961981773376, Final Batch Loss: 0.29065901041030884\n",
      "Epoch 1060, Loss: 1.6472707688808441, Final Batch Loss: 0.48161524534225464\n",
      "Epoch 1061, Loss: 1.637483835220337, Final Batch Loss: 0.36856070160865784\n",
      "Epoch 1062, Loss: 1.5824299454689026, Final Batch Loss: 0.345954567193985\n",
      "Epoch 1063, Loss: 1.6197202801704407, Final Batch Loss: 0.40158119797706604\n",
      "Epoch 1064, Loss: 1.586564540863037, Final Batch Loss: 0.4471004605293274\n",
      "Epoch 1065, Loss: 1.507656306028366, Final Batch Loss: 0.33428993821144104\n",
      "Epoch 1066, Loss: 1.627867430448532, Final Batch Loss: 0.3830955922603607\n",
      "Epoch 1067, Loss: 1.5018793046474457, Final Batch Loss: 0.32767587900161743\n",
      "Epoch 1068, Loss: 1.8254060447216034, Final Batch Loss: 0.4819338917732239\n",
      "Epoch 1069, Loss: 1.6163528561592102, Final Batch Loss: 0.4640237092971802\n",
      "Epoch 1070, Loss: 1.6327648162841797, Final Batch Loss: 0.4538593590259552\n",
      "Epoch 1071, Loss: 1.584498643875122, Final Batch Loss: 0.3282744586467743\n",
      "Epoch 1072, Loss: 1.6500882804393768, Final Batch Loss: 0.4406464397907257\n",
      "Epoch 1073, Loss: 1.5811434388160706, Final Batch Loss: 0.41136476397514343\n",
      "Epoch 1074, Loss: 1.5785278677940369, Final Batch Loss: 0.3915916979312897\n",
      "Epoch 1075, Loss: 1.478158324956894, Final Batch Loss: 0.2958178222179413\n",
      "Epoch 1076, Loss: 1.5764140784740448, Final Batch Loss: 0.37400880455970764\n",
      "Epoch 1077, Loss: 1.6209484934806824, Final Batch Loss: 0.4666568636894226\n",
      "Epoch 1078, Loss: 1.629934161901474, Final Batch Loss: 0.4222109019756317\n",
      "Epoch 1079, Loss: 1.6113092005252838, Final Batch Loss: 0.3398195505142212\n",
      "Epoch 1080, Loss: 1.6243114471435547, Final Batch Loss: 0.3560830056667328\n",
      "Epoch 1081, Loss: 1.5615154206752777, Final Batch Loss: 0.44478002190589905\n",
      "Epoch 1082, Loss: 1.6099733710289001, Final Batch Loss: 0.39999520778656006\n",
      "Epoch 1083, Loss: 1.6165962517261505, Final Batch Loss: 0.4214307367801666\n",
      "Epoch 1084, Loss: 1.6472479104995728, Final Batch Loss: 0.4442827105522156\n",
      "Epoch 1085, Loss: 1.633856177330017, Final Batch Loss: 0.3933485746383667\n",
      "Epoch 1086, Loss: 1.684378981590271, Final Batch Loss: 0.45834529399871826\n",
      "Epoch 1087, Loss: 1.589897483587265, Final Batch Loss: 0.43621087074279785\n",
      "Epoch 1088, Loss: 1.511710911989212, Final Batch Loss: 0.36362820863723755\n",
      "Epoch 1089, Loss: 1.5990979075431824, Final Batch Loss: 0.39227744936943054\n",
      "Epoch 1090, Loss: 1.5191995203495026, Final Batch Loss: 0.34016406536102295\n",
      "Epoch 1091, Loss: 1.6041115522384644, Final Batch Loss: 0.3818283975124359\n",
      "Epoch 1092, Loss: 1.5710375308990479, Final Batch Loss: 0.40759822726249695\n",
      "Epoch 1093, Loss: 1.5351540744304657, Final Batch Loss: 0.4332917034626007\n",
      "Epoch 1094, Loss: 1.553423434495926, Final Batch Loss: 0.3798390030860901\n",
      "Epoch 1095, Loss: 1.6850227117538452, Final Batch Loss: 0.3671307861804962\n",
      "Epoch 1096, Loss: 1.5176807343959808, Final Batch Loss: 0.3627692759037018\n",
      "Epoch 1097, Loss: 1.6455005705356598, Final Batch Loss: 0.3870962858200073\n",
      "Epoch 1098, Loss: 1.6755647957324982, Final Batch Loss: 0.3711303174495697\n",
      "Epoch 1099, Loss: 1.6771521866321564, Final Batch Loss: 0.3846801221370697\n",
      "Epoch 1100, Loss: 1.6433463096618652, Final Batch Loss: 0.34131380915641785\n",
      "Epoch 1101, Loss: 1.6078134775161743, Final Batch Loss: 0.37809044122695923\n",
      "Epoch 1102, Loss: 1.5225993692874908, Final Batch Loss: 0.3345182538032532\n",
      "Epoch 1103, Loss: 1.6282705962657928, Final Batch Loss: 0.45275041460990906\n",
      "Epoch 1104, Loss: 1.5893415808677673, Final Batch Loss: 0.45189881324768066\n",
      "Epoch 1105, Loss: 1.645327866077423, Final Batch Loss: 0.37522122263908386\n",
      "Epoch 1106, Loss: 1.5547181963920593, Final Batch Loss: 0.35644471645355225\n",
      "Epoch 1107, Loss: 1.5625957548618317, Final Batch Loss: 0.37601983547210693\n",
      "Epoch 1108, Loss: 1.61758753657341, Final Batch Loss: 0.44005388021469116\n",
      "Epoch 1109, Loss: 1.535868614912033, Final Batch Loss: 0.43977439403533936\n",
      "Epoch 1110, Loss: 1.7930070161819458, Final Batch Loss: 0.46695080399513245\n",
      "Epoch 1111, Loss: 1.5307562053203583, Final Batch Loss: 0.3818313181400299\n",
      "Epoch 1112, Loss: 1.5588206052780151, Final Batch Loss: 0.4456935524940491\n",
      "Epoch 1113, Loss: 1.590559333562851, Final Batch Loss: 0.39110586047172546\n",
      "Epoch 1114, Loss: 1.6027488112449646, Final Batch Loss: 0.37854212522506714\n",
      "Epoch 1115, Loss: 1.6200362741947174, Final Batch Loss: 0.37769824266433716\n",
      "Epoch 1116, Loss: 1.5546898245811462, Final Batch Loss: 0.3708480894565582\n",
      "Epoch 1117, Loss: 1.5754196047782898, Final Batch Loss: 0.3016423285007477\n",
      "Epoch 1118, Loss: 1.5688968300819397, Final Batch Loss: 0.40793630480766296\n",
      "Epoch 1119, Loss: 1.5184627771377563, Final Batch Loss: 0.312227338552475\n",
      "Epoch 1120, Loss: 1.6016332507133484, Final Batch Loss: 0.4219863712787628\n",
      "Epoch 1121, Loss: 1.582954466342926, Final Batch Loss: 0.42124277353286743\n",
      "Epoch 1122, Loss: 1.4840295612812042, Final Batch Loss: 0.33163028955459595\n",
      "Epoch 1123, Loss: 1.5370982885360718, Final Batch Loss: 0.3314441740512848\n",
      "Epoch 1124, Loss: 1.6401213109493256, Final Batch Loss: 0.3750931918621063\n",
      "Epoch 1125, Loss: 1.5095051229000092, Final Batch Loss: 0.4116936922073364\n",
      "Epoch 1126, Loss: 1.6185247302055359, Final Batch Loss: 0.45012399554252625\n",
      "Epoch 1127, Loss: 1.436786025762558, Final Batch Loss: 0.3004206418991089\n",
      "Epoch 1128, Loss: 1.5197438895702362, Final Batch Loss: 0.3405734598636627\n",
      "Epoch 1129, Loss: 1.54279625415802, Final Batch Loss: 0.4340863525867462\n",
      "Epoch 1130, Loss: 1.5585521161556244, Final Batch Loss: 0.39116424322128296\n",
      "Epoch 1131, Loss: 1.545953243970871, Final Batch Loss: 0.3039715588092804\n",
      "Epoch 1132, Loss: 1.6611144542694092, Final Batch Loss: 0.40547704696655273\n",
      "Epoch 1133, Loss: 1.5278688073158264, Final Batch Loss: 0.33806878328323364\n",
      "Epoch 1134, Loss: 1.597038358449936, Final Batch Loss: 0.4502604603767395\n",
      "Epoch 1135, Loss: 1.6730605959892273, Final Batch Loss: 0.440306693315506\n",
      "Epoch 1136, Loss: 1.5217857360839844, Final Batch Loss: 0.40421417355537415\n",
      "Epoch 1137, Loss: 1.6223084926605225, Final Batch Loss: 0.3398578464984894\n",
      "Epoch 1138, Loss: 1.5108487904071808, Final Batch Loss: 0.3754320740699768\n",
      "Epoch 1139, Loss: 1.5750183463096619, Final Batch Loss: 0.366891086101532\n",
      "Epoch 1140, Loss: 1.5972866714000702, Final Batch Loss: 0.4456788897514343\n",
      "Epoch 1141, Loss: 1.5757068991661072, Final Batch Loss: 0.33031922578811646\n",
      "Epoch 1142, Loss: 1.5092683732509613, Final Batch Loss: 0.38366639614105225\n",
      "Epoch 1143, Loss: 1.6064983010292053, Final Batch Loss: 0.4366992115974426\n",
      "Epoch 1144, Loss: 1.5334865748882294, Final Batch Loss: 0.39071333408355713\n",
      "Epoch 1145, Loss: 1.5277576744556427, Final Batch Loss: 0.42115268111228943\n",
      "Epoch 1146, Loss: 1.668342113494873, Final Batch Loss: 0.534651517868042\n",
      "Epoch 1147, Loss: 1.5033365190029144, Final Batch Loss: 0.35485315322875977\n",
      "Epoch 1148, Loss: 1.6428474187850952, Final Batch Loss: 0.4268184006214142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1149, Loss: 1.5800486207008362, Final Batch Loss: 0.3948977589607239\n",
      "Epoch 1150, Loss: 1.5969224870204926, Final Batch Loss: 0.4131096601486206\n",
      "Epoch 1151, Loss: 1.5203274190425873, Final Batch Loss: 0.3730413019657135\n",
      "Epoch 1152, Loss: 1.6170614957809448, Final Batch Loss: 0.45891156792640686\n",
      "Epoch 1153, Loss: 1.5509237945079803, Final Batch Loss: 0.41103559732437134\n",
      "Epoch 1154, Loss: 1.5111336708068848, Final Batch Loss: 0.4322631359100342\n",
      "Epoch 1155, Loss: 1.705862432718277, Final Batch Loss: 0.40200990438461304\n",
      "Epoch 1156, Loss: 1.635071575641632, Final Batch Loss: 0.424874871969223\n",
      "Epoch 1157, Loss: 1.519762396812439, Final Batch Loss: 0.34339988231658936\n",
      "Epoch 1158, Loss: 1.4984348118305206, Final Batch Loss: 0.3583317995071411\n",
      "Epoch 1159, Loss: 1.5123268365859985, Final Batch Loss: 0.4121139943599701\n",
      "Epoch 1160, Loss: 1.4943967163562775, Final Batch Loss: 0.41847050189971924\n",
      "Epoch 1161, Loss: 1.5643137395381927, Final Batch Loss: 0.3744939863681793\n",
      "Epoch 1162, Loss: 1.641415685415268, Final Batch Loss: 0.47332361340522766\n",
      "Epoch 1163, Loss: 1.559960126876831, Final Batch Loss: 0.4462874233722687\n",
      "Epoch 1164, Loss: 1.6583237648010254, Final Batch Loss: 0.46300947666168213\n",
      "Epoch 1165, Loss: 1.6035399436950684, Final Batch Loss: 0.375911682844162\n",
      "Epoch 1166, Loss: 1.567793369293213, Final Batch Loss: 0.4119860827922821\n",
      "Epoch 1167, Loss: 1.69417604804039, Final Batch Loss: 0.4230138063430786\n",
      "Epoch 1168, Loss: 1.6387375593185425, Final Batch Loss: 0.4186115860939026\n",
      "Epoch 1169, Loss: 1.5618333220481873, Final Batch Loss: 0.4735344648361206\n",
      "Epoch 1170, Loss: 1.5437761843204498, Final Batch Loss: 0.34172675013542175\n",
      "Epoch 1171, Loss: 1.5922695398330688, Final Batch Loss: 0.4261077344417572\n",
      "Epoch 1172, Loss: 1.5243523716926575, Final Batch Loss: 0.41059133410453796\n",
      "Epoch 1173, Loss: 1.4128535687923431, Final Batch Loss: 0.35671669244766235\n",
      "Epoch 1174, Loss: 1.5799480080604553, Final Batch Loss: 0.4545973837375641\n",
      "Epoch 1175, Loss: 1.4164210259914398, Final Batch Loss: 0.358071506023407\n",
      "Epoch 1176, Loss: 1.545655757188797, Final Batch Loss: 0.4602537751197815\n",
      "Epoch 1177, Loss: 1.5003576874732971, Final Batch Loss: 0.37062767148017883\n",
      "Epoch 1178, Loss: 1.538520485162735, Final Batch Loss: 0.41118064522743225\n",
      "Epoch 1179, Loss: 1.5332995653152466, Final Batch Loss: 0.32266944646835327\n",
      "Epoch 1180, Loss: 1.4713407158851624, Final Batch Loss: 0.3857673406600952\n",
      "Epoch 1181, Loss: 1.4988632202148438, Final Batch Loss: 0.35368943214416504\n",
      "Epoch 1182, Loss: 1.4700786471366882, Final Batch Loss: 0.3687487542629242\n",
      "Epoch 1183, Loss: 1.4562363624572754, Final Batch Loss: 0.3646470904350281\n",
      "Epoch 1184, Loss: 1.420482873916626, Final Batch Loss: 0.3462131917476654\n",
      "Epoch 1185, Loss: 1.5532081127166748, Final Batch Loss: 0.36205312609672546\n",
      "Epoch 1186, Loss: 1.5655914545059204, Final Batch Loss: 0.42615532875061035\n",
      "Epoch 1187, Loss: 1.6118066906929016, Final Batch Loss: 0.42088159918785095\n",
      "Epoch 1188, Loss: 1.4772400259971619, Final Batch Loss: 0.33672964572906494\n",
      "Epoch 1189, Loss: 1.5529645383358002, Final Batch Loss: 0.408765971660614\n",
      "Epoch 1190, Loss: 1.5893719792366028, Final Batch Loss: 0.39419129490852356\n",
      "Epoch 1191, Loss: 1.7209274470806122, Final Batch Loss: 0.47792041301727295\n",
      "Epoch 1192, Loss: 1.4762531518936157, Final Batch Loss: 0.3317375183105469\n",
      "Epoch 1193, Loss: 1.5504348576068878, Final Batch Loss: 0.4010915756225586\n",
      "Epoch 1194, Loss: 1.5990831851959229, Final Batch Loss: 0.4454307556152344\n",
      "Epoch 1195, Loss: 1.5688749849796295, Final Batch Loss: 0.35719794034957886\n",
      "Epoch 1196, Loss: 1.4398444592952728, Final Batch Loss: 0.3370363712310791\n",
      "Epoch 1197, Loss: 1.562252163887024, Final Batch Loss: 0.42158252000808716\n",
      "Epoch 1198, Loss: 1.5045530498027802, Final Batch Loss: 0.31161949038505554\n",
      "Epoch 1199, Loss: 1.521606206893921, Final Batch Loss: 0.3790794610977173\n",
      "Epoch 1200, Loss: 1.442958950996399, Final Batch Loss: 0.3084997236728668\n",
      "Epoch 1201, Loss: 1.6255630552768707, Final Batch Loss: 0.3902125656604767\n",
      "Epoch 1202, Loss: 1.5286816358566284, Final Batch Loss: 0.4399550259113312\n",
      "Epoch 1203, Loss: 1.5479666888713837, Final Batch Loss: 0.4223794639110565\n",
      "Epoch 1204, Loss: 1.4955696165561676, Final Batch Loss: 0.4292238652706146\n",
      "Epoch 1205, Loss: 1.6298481225967407, Final Batch Loss: 0.39365115761756897\n",
      "Epoch 1206, Loss: 1.5273988246917725, Final Batch Loss: 0.49128568172454834\n",
      "Epoch 1207, Loss: 1.57502880692482, Final Batch Loss: 0.3965444564819336\n",
      "Epoch 1208, Loss: 1.553689420223236, Final Batch Loss: 0.442679226398468\n",
      "Epoch 1209, Loss: 1.5929845869541168, Final Batch Loss: 0.44325605034828186\n",
      "Epoch 1210, Loss: 1.568465530872345, Final Batch Loss: 0.4517461061477661\n",
      "Epoch 1211, Loss: 1.565563440322876, Final Batch Loss: 0.37186646461486816\n",
      "Epoch 1212, Loss: 1.5108564496040344, Final Batch Loss: 0.3886306583881378\n",
      "Epoch 1213, Loss: 1.4873396158218384, Final Batch Loss: 0.3528836965560913\n",
      "Epoch 1214, Loss: 1.6052838861942291, Final Batch Loss: 0.44045957922935486\n",
      "Epoch 1215, Loss: 1.492213249206543, Final Batch Loss: 0.38612157106399536\n",
      "Epoch 1216, Loss: 1.5687729716300964, Final Batch Loss: 0.4320215880870819\n",
      "Epoch 1217, Loss: 1.5457910597324371, Final Batch Loss: 0.39638397097587585\n",
      "Epoch 1218, Loss: 1.5416633188724518, Final Batch Loss: 0.5070784687995911\n",
      "Epoch 1219, Loss: 1.750506430864334, Final Batch Loss: 0.576023280620575\n",
      "Epoch 1220, Loss: 1.4794110357761383, Final Batch Loss: 0.39344507455825806\n",
      "Epoch 1221, Loss: 1.553709477186203, Final Batch Loss: 0.3600526452064514\n",
      "Epoch 1222, Loss: 1.4626659750938416, Final Batch Loss: 0.38004326820373535\n",
      "Epoch 1223, Loss: 1.5280544757843018, Final Batch Loss: 0.333791047334671\n",
      "Epoch 1224, Loss: 1.522832378745079, Final Batch Loss: 0.4325983226299286\n",
      "Epoch 1225, Loss: 1.5059766173362732, Final Batch Loss: 0.3937109708786011\n",
      "Epoch 1226, Loss: 1.4439118206501007, Final Batch Loss: 0.30514049530029297\n",
      "Epoch 1227, Loss: 1.5662131607532501, Final Batch Loss: 0.5355217456817627\n",
      "Epoch 1228, Loss: 1.5844060182571411, Final Batch Loss: 0.4512992799282074\n",
      "Epoch 1229, Loss: 1.5257847607135773, Final Batch Loss: 0.37837526202201843\n",
      "Epoch 1230, Loss: 1.5550595819950104, Final Batch Loss: 0.35214585065841675\n",
      "Epoch 1231, Loss: 1.5182614028453827, Final Batch Loss: 0.3544726073741913\n",
      "Epoch 1232, Loss: 1.501144528388977, Final Batch Loss: 0.40880945324897766\n",
      "Epoch 1233, Loss: 1.506288230419159, Final Batch Loss: 0.35241878032684326\n",
      "Epoch 1234, Loss: 1.4605465531349182, Final Batch Loss: 0.36805447936058044\n",
      "Epoch 1235, Loss: 1.4538806080818176, Final Batch Loss: 0.32850712537765503\n",
      "Epoch 1236, Loss: 1.6862827241420746, Final Batch Loss: 0.36458712816238403\n",
      "Epoch 1237, Loss: 1.4171216189861298, Final Batch Loss: 0.3114696443080902\n",
      "Epoch 1238, Loss: 1.5369438230991364, Final Batch Loss: 0.3720272481441498\n",
      "Epoch 1239, Loss: 1.5178225934505463, Final Batch Loss: 0.3867294490337372\n",
      "Epoch 1240, Loss: 1.3894457817077637, Final Batch Loss: 0.3705320358276367\n",
      "Epoch 1241, Loss: 1.4126847982406616, Final Batch Loss: 0.3777414560317993\n",
      "Epoch 1242, Loss: 1.5358982682228088, Final Batch Loss: 0.3439636528491974\n",
      "Epoch 1243, Loss: 1.4629620611667633, Final Batch Loss: 0.34914249181747437\n",
      "Epoch 1244, Loss: 1.39821395277977, Final Batch Loss: 0.3710820972919464\n",
      "Epoch 1245, Loss: 1.406412124633789, Final Batch Loss: 0.30557969212532043\n",
      "Epoch 1246, Loss: 1.4885468184947968, Final Batch Loss: 0.445736825466156\n",
      "Epoch 1247, Loss: 1.4649847447872162, Final Batch Loss: 0.354830265045166\n",
      "Epoch 1248, Loss: 1.4980782866477966, Final Batch Loss: 0.30688488483428955\n",
      "Epoch 1249, Loss: 1.4795082211494446, Final Batch Loss: 0.3645833730697632\n",
      "Epoch 1250, Loss: 1.5416255593299866, Final Batch Loss: 0.4713825583457947\n",
      "Epoch 1251, Loss: 1.4581148326396942, Final Batch Loss: 0.339453786611557\n",
      "Epoch 1252, Loss: 1.5535900294780731, Final Batch Loss: 0.37851402163505554\n",
      "Epoch 1253, Loss: 1.4466478526592255, Final Batch Loss: 0.38547632098197937\n",
      "Epoch 1254, Loss: 1.3568524420261383, Final Batch Loss: 0.327880322933197\n",
      "Epoch 1255, Loss: 1.4466700851917267, Final Batch Loss: 0.36022648215293884\n",
      "Epoch 1256, Loss: 1.401560366153717, Final Batch Loss: 0.3804199993610382\n",
      "Epoch 1257, Loss: 1.4283427000045776, Final Batch Loss: 0.2505412697792053\n",
      "Epoch 1258, Loss: 1.3916119039058685, Final Batch Loss: 0.3622519075870514\n",
      "Epoch 1259, Loss: 1.4231884479522705, Final Batch Loss: 0.45004305243492126\n",
      "Epoch 1260, Loss: 1.4404626488685608, Final Batch Loss: 0.36359578371047974\n",
      "Epoch 1261, Loss: 1.5135164558887482, Final Batch Loss: 0.47020408511161804\n",
      "Epoch 1262, Loss: 1.4933646023273468, Final Batch Loss: 0.33767762780189514\n",
      "Epoch 1263, Loss: 1.5094412565231323, Final Batch Loss: 0.39289307594299316\n",
      "Epoch 1264, Loss: 1.4463877379894257, Final Batch Loss: 0.37150657176971436\n",
      "Epoch 1265, Loss: 1.4871337711811066, Final Batch Loss: 0.3292182683944702\n",
      "Epoch 1266, Loss: 1.5057047307491302, Final Batch Loss: 0.31303921341896057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1267, Loss: 1.3825255930423737, Final Batch Loss: 0.3669819235801697\n",
      "Epoch 1268, Loss: 1.4801914393901825, Final Batch Loss: 0.375427782535553\n",
      "Epoch 1269, Loss: 1.4470391869544983, Final Batch Loss: 0.35394132137298584\n",
      "Epoch 1270, Loss: 1.5247918963432312, Final Batch Loss: 0.3834368884563446\n",
      "Epoch 1271, Loss: 1.5868587493896484, Final Batch Loss: 0.4138895869255066\n",
      "Epoch 1272, Loss: 1.5223169922828674, Final Batch Loss: 0.46446216106414795\n",
      "Epoch 1273, Loss: 1.54975026845932, Final Batch Loss: 0.42046064138412476\n",
      "Epoch 1274, Loss: 1.504530280828476, Final Batch Loss: 0.36241602897644043\n",
      "Epoch 1275, Loss: 1.4712520837783813, Final Batch Loss: 0.3403945863246918\n",
      "Epoch 1276, Loss: 1.413294792175293, Final Batch Loss: 0.30222973227500916\n",
      "Epoch 1277, Loss: 1.4901103079319, Final Batch Loss: 0.3767094016075134\n",
      "Epoch 1278, Loss: 1.4411342144012451, Final Batch Loss: 0.3265284299850464\n",
      "Epoch 1279, Loss: 1.5046296417713165, Final Batch Loss: 0.377358078956604\n",
      "Epoch 1280, Loss: 1.6058959662914276, Final Batch Loss: 0.48890334367752075\n",
      "Epoch 1281, Loss: 1.41472989320755, Final Batch Loss: 0.3710308372974396\n",
      "Epoch 1282, Loss: 1.5485949516296387, Final Batch Loss: 0.4651443660259247\n",
      "Epoch 1283, Loss: 1.4711893200874329, Final Batch Loss: 0.39821499586105347\n",
      "Epoch 1284, Loss: 1.6092548966407776, Final Batch Loss: 0.42586421966552734\n",
      "Epoch 1285, Loss: 1.3930946290493011, Final Batch Loss: 0.36283016204833984\n",
      "Epoch 1286, Loss: 1.4279151856899261, Final Batch Loss: 0.4341685175895691\n",
      "Epoch 1287, Loss: 1.4275595843791962, Final Batch Loss: 0.32791200280189514\n",
      "Epoch 1288, Loss: 1.4902127087116241, Final Batch Loss: 0.4307282865047455\n",
      "Epoch 1289, Loss: 1.5608694851398468, Final Batch Loss: 0.42216360569000244\n",
      "Epoch 1290, Loss: 1.44855597615242, Final Batch Loss: 0.37748658657073975\n",
      "Epoch 1291, Loss: 1.4656241834163666, Final Batch Loss: 0.4108327031135559\n",
      "Epoch 1292, Loss: 1.4478349089622498, Final Batch Loss: 0.295386403799057\n",
      "Epoch 1293, Loss: 1.5580668151378632, Final Batch Loss: 0.3741082549095154\n",
      "Epoch 1294, Loss: 1.4191592037677765, Final Batch Loss: 0.26519763469696045\n",
      "Epoch 1295, Loss: 1.4043331742286682, Final Batch Loss: 0.36901575326919556\n",
      "Epoch 1296, Loss: 1.5388073921203613, Final Batch Loss: 0.45482337474823\n",
      "Epoch 1297, Loss: 1.5361741185188293, Final Batch Loss: 0.35481342673301697\n",
      "Epoch 1298, Loss: 1.5439142882823944, Final Batch Loss: 0.4997811019420624\n",
      "Epoch 1299, Loss: 1.5211668014526367, Final Batch Loss: 0.45431435108184814\n",
      "Epoch 1300, Loss: 1.605194330215454, Final Batch Loss: 0.4594326317310333\n",
      "Epoch 1301, Loss: 1.430893450975418, Final Batch Loss: 0.3413478434085846\n",
      "Epoch 1302, Loss: 1.377943754196167, Final Batch Loss: 0.30981677770614624\n",
      "Epoch 1303, Loss: 1.4923415780067444, Final Batch Loss: 0.4259544610977173\n",
      "Epoch 1304, Loss: 1.407172977924347, Final Batch Loss: 0.3767641484737396\n",
      "Epoch 1305, Loss: 1.4872563183307648, Final Batch Loss: 0.3777848482131958\n",
      "Epoch 1306, Loss: 1.5169003307819366, Final Batch Loss: 0.45499366521835327\n",
      "Epoch 1307, Loss: 1.379303514957428, Final Batch Loss: 0.33326223492622375\n",
      "Epoch 1308, Loss: 1.4581551253795624, Final Batch Loss: 0.3640454411506653\n",
      "Epoch 1309, Loss: 1.5137653052806854, Final Batch Loss: 0.3764510154724121\n",
      "Epoch 1310, Loss: 1.4523963034152985, Final Batch Loss: 0.3378492593765259\n",
      "Epoch 1311, Loss: 1.4512152075767517, Final Batch Loss: 0.33170685172080994\n",
      "Epoch 1312, Loss: 1.530331313610077, Final Batch Loss: 0.3775327801704407\n",
      "Epoch 1313, Loss: 1.389723926782608, Final Batch Loss: 0.3516278862953186\n",
      "Epoch 1314, Loss: 1.5362679660320282, Final Batch Loss: 0.45825380086898804\n",
      "Epoch 1315, Loss: 1.4886928796768188, Final Batch Loss: 0.3747616112232208\n",
      "Epoch 1316, Loss: 1.4204348623752594, Final Batch Loss: 0.349656343460083\n",
      "Epoch 1317, Loss: 1.4345753490924835, Final Batch Loss: 0.3511289358139038\n",
      "Epoch 1318, Loss: 1.4952851235866547, Final Batch Loss: 0.36956095695495605\n",
      "Epoch 1319, Loss: 1.4588578343391418, Final Batch Loss: 0.3314276933670044\n",
      "Epoch 1320, Loss: 1.5248786807060242, Final Batch Loss: 0.4223235845565796\n",
      "Epoch 1321, Loss: 1.5081754624843597, Final Batch Loss: 0.36692872643470764\n",
      "Epoch 1322, Loss: 1.5238912999629974, Final Batch Loss: 0.3587796688079834\n",
      "Epoch 1323, Loss: 1.397704005241394, Final Batch Loss: 0.3432161509990692\n",
      "Epoch 1324, Loss: 1.4206255376338959, Final Batch Loss: 0.3089694678783417\n",
      "Epoch 1325, Loss: 1.3718055486679077, Final Batch Loss: 0.35658055543899536\n",
      "Epoch 1326, Loss: 1.4277141392230988, Final Batch Loss: 0.30219322443008423\n",
      "Epoch 1327, Loss: 1.5751286745071411, Final Batch Loss: 0.3825967311859131\n",
      "Epoch 1328, Loss: 1.4939712882041931, Final Batch Loss: 0.3706176280975342\n",
      "Epoch 1329, Loss: 1.4216420650482178, Final Batch Loss: 0.43025824427604675\n",
      "Epoch 1330, Loss: 1.5455615818500519, Final Batch Loss: 0.47385838627815247\n",
      "Epoch 1331, Loss: 1.458765059709549, Final Batch Loss: 0.3528590500354767\n",
      "Epoch 1332, Loss: 1.5730653703212738, Final Batch Loss: 0.37439069151878357\n",
      "Epoch 1333, Loss: 1.5270318388938904, Final Batch Loss: 0.4293081760406494\n",
      "Epoch 1334, Loss: 1.5428159832954407, Final Batch Loss: 0.41920769214630127\n",
      "Epoch 1335, Loss: 1.548842579126358, Final Batch Loss: 0.3733053207397461\n",
      "Epoch 1336, Loss: 1.6887809932231903, Final Batch Loss: 0.43664127588272095\n",
      "Epoch 1337, Loss: 1.3832790851593018, Final Batch Loss: 0.34072446823120117\n",
      "Epoch 1338, Loss: 1.454215407371521, Final Batch Loss: 0.29922544956207275\n",
      "Epoch 1339, Loss: 1.488640695810318, Final Batch Loss: 0.39267459511756897\n",
      "Epoch 1340, Loss: 1.395112305879593, Final Batch Loss: 0.3306516110897064\n",
      "Epoch 1341, Loss: 1.5198746025562286, Final Batch Loss: 0.38334736227989197\n",
      "Epoch 1342, Loss: 1.4292739629745483, Final Batch Loss: 0.37648704648017883\n",
      "Epoch 1343, Loss: 1.3973489105701447, Final Batch Loss: 0.27277669310569763\n",
      "Epoch 1344, Loss: 1.4332329332828522, Final Batch Loss: 0.35796797275543213\n",
      "Epoch 1345, Loss: 1.4863374531269073, Final Batch Loss: 0.3345799744129181\n",
      "Epoch 1346, Loss: 1.546133428812027, Final Batch Loss: 0.37870532274246216\n",
      "Epoch 1347, Loss: 1.4663614928722382, Final Batch Loss: 0.40418416261672974\n",
      "Epoch 1348, Loss: 1.4844202995300293, Final Batch Loss: 0.4063494801521301\n",
      "Epoch 1349, Loss: 1.4130451679229736, Final Batch Loss: 0.3395395278930664\n",
      "Epoch 1350, Loss: 1.3980086147785187, Final Batch Loss: 0.3503662049770355\n",
      "Epoch 1351, Loss: 1.4762161672115326, Final Batch Loss: 0.3762032687664032\n",
      "Epoch 1352, Loss: 1.5850169956684113, Final Batch Loss: 0.4186142683029175\n",
      "Epoch 1353, Loss: 1.4437529742717743, Final Batch Loss: 0.3959362804889679\n",
      "Epoch 1354, Loss: 1.4190848469734192, Final Batch Loss: 0.41325607895851135\n",
      "Epoch 1355, Loss: 1.4038096368312836, Final Batch Loss: 0.38984784483909607\n",
      "Epoch 1356, Loss: 1.390523076057434, Final Batch Loss: 0.29457801580429077\n",
      "Epoch 1357, Loss: 1.4943887293338776, Final Batch Loss: 0.40784597396850586\n",
      "Epoch 1358, Loss: 1.3750548660755157, Final Batch Loss: 0.35550644993782043\n",
      "Epoch 1359, Loss: 1.4921132028102875, Final Batch Loss: 0.3187567889690399\n",
      "Epoch 1360, Loss: 1.4314150214195251, Final Batch Loss: 0.38419798016548157\n",
      "Epoch 1361, Loss: 1.4148987829685211, Final Batch Loss: 0.4261840283870697\n",
      "Epoch 1362, Loss: 1.348033756017685, Final Batch Loss: 0.36867237091064453\n",
      "Epoch 1363, Loss: 1.334978610277176, Final Batch Loss: 0.2711980938911438\n",
      "Epoch 1364, Loss: 1.3881470561027527, Final Batch Loss: 0.36883968114852905\n",
      "Epoch 1365, Loss: 1.4310216307640076, Final Batch Loss: 0.35679036378860474\n",
      "Epoch 1366, Loss: 1.3203963041305542, Final Batch Loss: 0.3678770065307617\n",
      "Epoch 1367, Loss: 1.3533307015895844, Final Batch Loss: 0.36057528853416443\n",
      "Epoch 1368, Loss: 1.5456162691116333, Final Batch Loss: 0.4383963346481323\n",
      "Epoch 1369, Loss: 1.5235390961170197, Final Batch Loss: 0.4434888958930969\n",
      "Epoch 1370, Loss: 1.432865470647812, Final Batch Loss: 0.3486730456352234\n",
      "Epoch 1371, Loss: 1.4569239616394043, Final Batch Loss: 0.32972797751426697\n",
      "Epoch 1372, Loss: 1.506500005722046, Final Batch Loss: 0.5416436791419983\n",
      "Epoch 1373, Loss: 1.3427439033985138, Final Batch Loss: 0.24143117666244507\n",
      "Epoch 1374, Loss: 1.4533789157867432, Final Batch Loss: 0.40712931752204895\n",
      "Epoch 1375, Loss: 1.3392361104488373, Final Batch Loss: 0.2568035423755646\n",
      "Epoch 1376, Loss: 1.5017104744911194, Final Batch Loss: 0.43687912821769714\n",
      "Epoch 1377, Loss: 1.4115342795848846, Final Batch Loss: 0.2675687372684479\n",
      "Epoch 1378, Loss: 1.4412730038166046, Final Batch Loss: 0.42811405658721924\n",
      "Epoch 1379, Loss: 1.6392268240451813, Final Batch Loss: 0.4899227023124695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1380, Loss: 1.4869755804538727, Final Batch Loss: 0.3610379099845886\n",
      "Epoch 1381, Loss: 1.4720970392227173, Final Batch Loss: 0.42012321949005127\n",
      "Epoch 1382, Loss: 1.6199813187122345, Final Batch Loss: 0.4877130687236786\n",
      "Epoch 1383, Loss: 1.3839098811149597, Final Batch Loss: 0.30457064509391785\n",
      "Epoch 1384, Loss: 1.4362630248069763, Final Batch Loss: 0.4242497682571411\n",
      "Epoch 1385, Loss: 1.4549006819725037, Final Batch Loss: 0.364764928817749\n",
      "Epoch 1386, Loss: 1.4777469038963318, Final Batch Loss: 0.3156788647174835\n",
      "Epoch 1387, Loss: 1.420612394809723, Final Batch Loss: 0.35499584674835205\n",
      "Epoch 1388, Loss: 1.50397390127182, Final Batch Loss: 0.4686400592327118\n",
      "Epoch 1389, Loss: 1.402826726436615, Final Batch Loss: 0.30694591999053955\n",
      "Epoch 1390, Loss: 1.4677404761314392, Final Batch Loss: 0.35127687454223633\n",
      "Epoch 1391, Loss: 1.4235175549983978, Final Batch Loss: 0.3377760946750641\n",
      "Epoch 1392, Loss: 1.3902121782302856, Final Batch Loss: 0.317335844039917\n",
      "Epoch 1393, Loss: 1.4109255373477936, Final Batch Loss: 0.3051048219203949\n",
      "Epoch 1394, Loss: 1.429454505443573, Final Batch Loss: 0.3157578706741333\n",
      "Epoch 1395, Loss: 1.2711434662342072, Final Batch Loss: 0.3290008008480072\n",
      "Epoch 1396, Loss: 1.3805935084819794, Final Batch Loss: 0.38508254289627075\n",
      "Epoch 1397, Loss: 1.4188528060913086, Final Batch Loss: 0.4147970974445343\n",
      "Epoch 1398, Loss: 1.4005941450595856, Final Batch Loss: 0.36620503664016724\n",
      "Epoch 1399, Loss: 1.5314764082431793, Final Batch Loss: 0.3629249334335327\n",
      "Epoch 1400, Loss: 1.4650354087352753, Final Batch Loss: 0.39997556805610657\n",
      "Epoch 1401, Loss: 1.4701983332633972, Final Batch Loss: 0.3877280056476593\n",
      "Epoch 1402, Loss: 1.4218756258487701, Final Batch Loss: 0.284914493560791\n",
      "Epoch 1403, Loss: 1.3820414543151855, Final Batch Loss: 0.3543998897075653\n",
      "Epoch 1404, Loss: 1.4482673704624176, Final Batch Loss: 0.3770916759967804\n",
      "Epoch 1405, Loss: 1.504818320274353, Final Batch Loss: 0.35294029116630554\n",
      "Epoch 1406, Loss: 1.4732873737812042, Final Batch Loss: 0.4027974605560303\n",
      "Epoch 1407, Loss: 1.509502112865448, Final Batch Loss: 0.44643834233283997\n",
      "Epoch 1408, Loss: 1.4674111008644104, Final Batch Loss: 0.2964636981487274\n",
      "Epoch 1409, Loss: 1.4953021705150604, Final Batch Loss: 0.3988974392414093\n",
      "Epoch 1410, Loss: 1.4575093388557434, Final Batch Loss: 0.37712496519088745\n",
      "Epoch 1411, Loss: 1.4563055634498596, Final Batch Loss: 0.32084986567497253\n",
      "Epoch 1412, Loss: 1.4362697005271912, Final Batch Loss: 0.41408413648605347\n",
      "Epoch 1413, Loss: 1.4512751400470734, Final Batch Loss: 0.4087684452533722\n",
      "Epoch 1414, Loss: 1.3320362865924835, Final Batch Loss: 0.3431406617164612\n",
      "Epoch 1415, Loss: 1.4031944572925568, Final Batch Loss: 0.3819805681705475\n",
      "Epoch 1416, Loss: 1.5567589402198792, Final Batch Loss: 0.4094873368740082\n",
      "Epoch 1417, Loss: 1.5796664655208588, Final Batch Loss: 0.4389828145503998\n",
      "Epoch 1418, Loss: 1.5517711341381073, Final Batch Loss: 0.4346681237220764\n",
      "Epoch 1419, Loss: 1.4214937686920166, Final Batch Loss: 0.39665400981903076\n",
      "Epoch 1420, Loss: 1.421783685684204, Final Batch Loss: 0.3266283869743347\n",
      "Epoch 1421, Loss: 1.5031736195087433, Final Batch Loss: 0.3945141136646271\n",
      "Epoch 1422, Loss: 1.491095870733261, Final Batch Loss: 0.2982054650783539\n",
      "Epoch 1423, Loss: 1.396142065525055, Final Batch Loss: 0.3255486488342285\n",
      "Epoch 1424, Loss: 1.5495542287826538, Final Batch Loss: 0.39482301473617554\n",
      "Epoch 1425, Loss: 1.3200883269309998, Final Batch Loss: 0.26938596367836\n",
      "Epoch 1426, Loss: 1.3407948315143585, Final Batch Loss: 0.3346865773200989\n",
      "Epoch 1427, Loss: 1.515033096075058, Final Batch Loss: 0.40115267038345337\n",
      "Epoch 1428, Loss: 1.4189295768737793, Final Batch Loss: 0.4135501980781555\n",
      "Epoch 1429, Loss: 1.4335640668869019, Final Batch Loss: 0.39289817214012146\n",
      "Epoch 1430, Loss: 1.4307764172554016, Final Batch Loss: 0.36321112513542175\n",
      "Epoch 1431, Loss: 1.3799761533737183, Final Batch Loss: 0.36624982953071594\n",
      "Epoch 1432, Loss: 1.489551991224289, Final Batch Loss: 0.47663331031799316\n",
      "Epoch 1433, Loss: 1.372189849615097, Final Batch Loss: 0.30964645743370056\n",
      "Epoch 1434, Loss: 1.499659776687622, Final Batch Loss: 0.4206794202327728\n",
      "Epoch 1435, Loss: 1.4519879221916199, Final Batch Loss: 0.29395848512649536\n",
      "Epoch 1436, Loss: 1.3451520502567291, Final Batch Loss: 0.3279212713241577\n",
      "Epoch 1437, Loss: 1.4601960182189941, Final Batch Loss: 0.39954379200935364\n",
      "Epoch 1438, Loss: 1.4011766612529755, Final Batch Loss: 0.3382561504840851\n",
      "Epoch 1439, Loss: 1.386993169784546, Final Batch Loss: 0.3484392762184143\n",
      "Epoch 1440, Loss: 1.4208670258522034, Final Batch Loss: 0.4224737882614136\n",
      "Epoch 1441, Loss: 1.4695751368999481, Final Batch Loss: 0.3767787516117096\n",
      "Epoch 1442, Loss: 1.4132541418075562, Final Batch Loss: 0.3146061599254608\n",
      "Epoch 1443, Loss: 1.3607659935951233, Final Batch Loss: 0.29983383417129517\n",
      "Epoch 1444, Loss: 1.3819708228111267, Final Batch Loss: 0.3464897871017456\n",
      "Epoch 1445, Loss: 1.4953433871269226, Final Batch Loss: 0.41143521666526794\n",
      "Epoch 1446, Loss: 1.474639356136322, Final Batch Loss: 0.37956157326698303\n",
      "Epoch 1447, Loss: 1.4414103627204895, Final Batch Loss: 0.27058231830596924\n",
      "Epoch 1448, Loss: 1.4046596884727478, Final Batch Loss: 0.45699694752693176\n",
      "Epoch 1449, Loss: 1.3125440180301666, Final Batch Loss: 0.23278391361236572\n",
      "Epoch 1450, Loss: 1.4307168424129486, Final Batch Loss: 0.3525316119194031\n",
      "Epoch 1451, Loss: 1.4108955562114716, Final Batch Loss: 0.3591099977493286\n",
      "Epoch 1452, Loss: 1.4907499253749847, Final Batch Loss: 0.4270906448364258\n",
      "Epoch 1453, Loss: 1.3577145040035248, Final Batch Loss: 0.30088356137275696\n",
      "Epoch 1454, Loss: 1.406280905008316, Final Batch Loss: 0.32945120334625244\n",
      "Epoch 1455, Loss: 1.4886066317558289, Final Batch Loss: 0.39812231063842773\n",
      "Epoch 1456, Loss: 1.466604858636856, Final Batch Loss: 0.46705397963523865\n",
      "Epoch 1457, Loss: 1.3025636970996857, Final Batch Loss: 0.28218159079551697\n",
      "Epoch 1458, Loss: 1.4069689214229584, Final Batch Loss: 0.4064325988292694\n",
      "Epoch 1459, Loss: 1.3942577838897705, Final Batch Loss: 0.3556897044181824\n",
      "Epoch 1460, Loss: 1.3821842074394226, Final Batch Loss: 0.3279525935649872\n",
      "Epoch 1461, Loss: 1.3276712894439697, Final Batch Loss: 0.35327622294425964\n",
      "Epoch 1462, Loss: 1.3825865387916565, Final Batch Loss: 0.35217079520225525\n",
      "Epoch 1463, Loss: 1.4710724353790283, Final Batch Loss: 0.4094211757183075\n",
      "Epoch 1464, Loss: 1.4749903678894043, Final Batch Loss: 0.3670232594013214\n",
      "Epoch 1465, Loss: 1.4562253952026367, Final Batch Loss: 0.3841152489185333\n",
      "Epoch 1466, Loss: 1.4508658349514008, Final Batch Loss: 0.300802618265152\n",
      "Epoch 1467, Loss: 1.560297280550003, Final Batch Loss: 0.5268464684486389\n",
      "Epoch 1468, Loss: 1.3295752108097076, Final Batch Loss: 0.36092087626457214\n",
      "Epoch 1469, Loss: 1.4120267927646637, Final Batch Loss: 0.3669053912162781\n",
      "Epoch 1470, Loss: 1.5005893111228943, Final Batch Loss: 0.3502242863178253\n",
      "Epoch 1471, Loss: 1.3970918655395508, Final Batch Loss: 0.3542264997959137\n",
      "Epoch 1472, Loss: 1.4206927418708801, Final Batch Loss: 0.388662189245224\n",
      "Epoch 1473, Loss: 1.4183920919895172, Final Batch Loss: 0.3084975481033325\n",
      "Epoch 1474, Loss: 1.3034728467464447, Final Batch Loss: 0.29764822125434875\n",
      "Epoch 1475, Loss: 1.4258042573928833, Final Batch Loss: 0.4015319347381592\n",
      "Epoch 1476, Loss: 1.5095122754573822, Final Batch Loss: 0.47844141721725464\n",
      "Epoch 1477, Loss: 1.4431370794773102, Final Batch Loss: 0.31258007884025574\n",
      "Epoch 1478, Loss: 1.3696852922439575, Final Batch Loss: 0.35743385553359985\n",
      "Epoch 1479, Loss: 1.348149299621582, Final Batch Loss: 0.2892206907272339\n",
      "Epoch 1480, Loss: 1.4958663582801819, Final Batch Loss: 0.3984423577785492\n",
      "Epoch 1481, Loss: 1.4464972913265228, Final Batch Loss: 0.3988700807094574\n",
      "Epoch 1482, Loss: 1.3798445165157318, Final Batch Loss: 0.30700528621673584\n",
      "Epoch 1483, Loss: 1.5100762248039246, Final Batch Loss: 0.46475282311439514\n",
      "Epoch 1484, Loss: 1.3323344588279724, Final Batch Loss: 0.3662082254886627\n",
      "Epoch 1485, Loss: 1.4695794582366943, Final Batch Loss: 0.35085731744766235\n",
      "Epoch 1486, Loss: 1.4102680087089539, Final Batch Loss: 0.3076956868171692\n",
      "Epoch 1487, Loss: 1.3774526417255402, Final Batch Loss: 0.2898104190826416\n",
      "Epoch 1488, Loss: 1.2603097558021545, Final Batch Loss: 0.3188239634037018\n",
      "Epoch 1489, Loss: 1.3548189103603363, Final Batch Loss: 0.3308953642845154\n",
      "Epoch 1490, Loss: 1.4604186415672302, Final Batch Loss: 0.4607250392436981\n",
      "Epoch 1491, Loss: 1.422640472650528, Final Batch Loss: 0.3212969899177551\n",
      "Epoch 1492, Loss: 1.4574413001537323, Final Batch Loss: 0.3995077908039093\n",
      "Epoch 1493, Loss: 1.500667929649353, Final Batch Loss: 0.3518572449684143\n",
      "Epoch 1494, Loss: 1.3958704471588135, Final Batch Loss: 0.37360402941703796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1495, Loss: 1.4149500131607056, Final Batch Loss: 0.3930709660053253\n",
      "Epoch 1496, Loss: 1.3484288454055786, Final Batch Loss: 0.34103071689605713\n",
      "Epoch 1497, Loss: 1.3833461105823517, Final Batch Loss: 0.3078247010707855\n",
      "Epoch 1498, Loss: 1.4550102353096008, Final Batch Loss: 0.2980751693248749\n",
      "Epoch 1499, Loss: 1.4252934753894806, Final Batch Loss: 0.38355913758277893\n",
      "Epoch 1500, Loss: 1.4362802505493164, Final Batch Loss: 0.3921293318271637\n",
      "Epoch 1501, Loss: 1.3824766874313354, Final Batch Loss: 0.3011554777622223\n",
      "Epoch 1502, Loss: 1.3273656070232391, Final Batch Loss: 0.31944403052330017\n",
      "Epoch 1503, Loss: 1.3767483830451965, Final Batch Loss: 0.37068256735801697\n",
      "Epoch 1504, Loss: 1.3733067512512207, Final Batch Loss: 0.2902994751930237\n",
      "Epoch 1505, Loss: 1.5228300988674164, Final Batch Loss: 0.40194422006607056\n",
      "Epoch 1506, Loss: 1.4598277807235718, Final Batch Loss: 0.46133899688720703\n",
      "Epoch 1507, Loss: 1.3863186836242676, Final Batch Loss: 0.3155849874019623\n",
      "Epoch 1508, Loss: 1.416757881641388, Final Batch Loss: 0.31718501448631287\n",
      "Epoch 1509, Loss: 1.409822940826416, Final Batch Loss: 0.32839977741241455\n",
      "Epoch 1510, Loss: 1.3601810038089752, Final Batch Loss: 0.3576202094554901\n",
      "Epoch 1511, Loss: 1.5182690620422363, Final Batch Loss: 0.3699275851249695\n",
      "Epoch 1512, Loss: 1.4484380781650543, Final Batch Loss: 0.3693520724773407\n",
      "Epoch 1513, Loss: 1.4207369685173035, Final Batch Loss: 0.3405565917491913\n",
      "Epoch 1514, Loss: 1.4520558714866638, Final Batch Loss: 0.3007992208003998\n",
      "Epoch 1515, Loss: 1.3841146230697632, Final Batch Loss: 0.39350253343582153\n",
      "Epoch 1516, Loss: 1.3678274154663086, Final Batch Loss: 0.36835765838623047\n",
      "Epoch 1517, Loss: 1.432934284210205, Final Batch Loss: 0.40813344717025757\n",
      "Epoch 1518, Loss: 1.4046860039234161, Final Batch Loss: 0.40043291449546814\n",
      "Epoch 1519, Loss: 1.444606989622116, Final Batch Loss: 0.33629459142684937\n",
      "Epoch 1520, Loss: 1.4066118896007538, Final Batch Loss: 0.36286279559135437\n",
      "Epoch 1521, Loss: 1.58142951130867, Final Batch Loss: 0.28413376212120056\n",
      "Epoch 1522, Loss: 1.4966163635253906, Final Batch Loss: 0.42933395504951477\n",
      "Epoch 1523, Loss: 1.4157432317733765, Final Batch Loss: 0.37348297238349915\n",
      "Epoch 1524, Loss: 1.383700579404831, Final Batch Loss: 0.4564829468727112\n",
      "Epoch 1525, Loss: 1.5445546805858612, Final Batch Loss: 0.3741968274116516\n",
      "Epoch 1526, Loss: 1.4324527382850647, Final Batch Loss: 0.37918898463249207\n",
      "Epoch 1527, Loss: 1.235752284526825, Final Batch Loss: 0.2844184339046478\n",
      "Epoch 1528, Loss: 1.495471328496933, Final Batch Loss: 0.391975998878479\n",
      "Epoch 1529, Loss: 1.4230679273605347, Final Batch Loss: 0.3358236849308014\n",
      "Epoch 1530, Loss: 1.4345977008342743, Final Batch Loss: 0.34300416707992554\n",
      "Epoch 1531, Loss: 1.3659178018569946, Final Batch Loss: 0.35626131296157837\n",
      "Epoch 1532, Loss: 1.5360048413276672, Final Batch Loss: 0.42367953062057495\n",
      "Epoch 1533, Loss: 1.376003086566925, Final Batch Loss: 0.3279629945755005\n",
      "Epoch 1534, Loss: 1.323728859424591, Final Batch Loss: 0.3735305368900299\n",
      "Epoch 1535, Loss: 1.4002752304077148, Final Batch Loss: 0.3727984130382538\n",
      "Epoch 1536, Loss: 1.3416233360767365, Final Batch Loss: 0.33211082220077515\n",
      "Epoch 1537, Loss: 1.5028521716594696, Final Batch Loss: 0.3047061562538147\n",
      "Epoch 1538, Loss: 1.360624372959137, Final Batch Loss: 0.3671635091304779\n",
      "Epoch 1539, Loss: 1.2346566319465637, Final Batch Loss: 0.29909223318099976\n",
      "Epoch 1540, Loss: 1.4234079718589783, Final Batch Loss: 0.45228493213653564\n",
      "Epoch 1541, Loss: 1.2944809198379517, Final Batch Loss: 0.32238635420799255\n",
      "Epoch 1542, Loss: 1.4485701024532318, Final Batch Loss: 0.4089296758174896\n",
      "Epoch 1543, Loss: 1.4381110966205597, Final Batch Loss: 0.3730636537075043\n",
      "Epoch 1544, Loss: 1.379871517419815, Final Batch Loss: 0.3585285544395447\n",
      "Epoch 1545, Loss: 1.378865659236908, Final Batch Loss: 0.32561907172203064\n",
      "Epoch 1546, Loss: 1.4221838414669037, Final Batch Loss: 0.35835036635398865\n",
      "Epoch 1547, Loss: 1.3112322390079498, Final Batch Loss: 0.3693222999572754\n",
      "Epoch 1548, Loss: 1.3910093307495117, Final Batch Loss: 0.3727540969848633\n",
      "Epoch 1549, Loss: 1.4831158816814423, Final Batch Loss: 0.400400847196579\n",
      "Epoch 1550, Loss: 1.4183537364006042, Final Batch Loss: 0.3194032907485962\n",
      "Epoch 1551, Loss: 1.4031678438186646, Final Batch Loss: 0.34418511390686035\n",
      "Epoch 1552, Loss: 1.4354380369186401, Final Batch Loss: 0.3297601044178009\n",
      "Epoch 1553, Loss: 1.3476983606815338, Final Batch Loss: 0.2763967514038086\n",
      "Epoch 1554, Loss: 1.4449347257614136, Final Batch Loss: 0.35416296124458313\n",
      "Epoch 1555, Loss: 1.4029113948345184, Final Batch Loss: 0.32003235816955566\n",
      "Epoch 1556, Loss: 1.3759374022483826, Final Batch Loss: 0.3913112282752991\n",
      "Epoch 1557, Loss: 1.3213765025138855, Final Batch Loss: 0.3889288008213043\n",
      "Epoch 1558, Loss: 1.4784534275531769, Final Batch Loss: 0.3677412271499634\n",
      "Epoch 1559, Loss: 1.423378586769104, Final Batch Loss: 0.4317799210548401\n",
      "Epoch 1560, Loss: 1.4132890403270721, Final Batch Loss: 0.37405672669410706\n",
      "Epoch 1561, Loss: 1.3693043291568756, Final Batch Loss: 0.36707860231399536\n",
      "Epoch 1562, Loss: 1.3947212398052216, Final Batch Loss: 0.28428736329078674\n",
      "Epoch 1563, Loss: 1.424680918455124, Final Batch Loss: 0.28716984391212463\n",
      "Epoch 1564, Loss: 1.2911151349544525, Final Batch Loss: 0.2781750559806824\n",
      "Epoch 1565, Loss: 1.4956617951393127, Final Batch Loss: 0.36524125933647156\n",
      "Epoch 1566, Loss: 1.3782145082950592, Final Batch Loss: 0.3442532420158386\n",
      "Epoch 1567, Loss: 1.4463779628276825, Final Batch Loss: 0.34891289472579956\n",
      "Epoch 1568, Loss: 1.2627369165420532, Final Batch Loss: 0.3374815285205841\n",
      "Epoch 1569, Loss: 1.4477003812789917, Final Batch Loss: 0.3746882975101471\n",
      "Epoch 1570, Loss: 1.3376870453357697, Final Batch Loss: 0.2144339382648468\n",
      "Epoch 1571, Loss: 1.3161364495754242, Final Batch Loss: 0.33382686972618103\n",
      "Epoch 1572, Loss: 1.4490284323692322, Final Batch Loss: 0.3265683650970459\n",
      "Epoch 1573, Loss: 1.4676115214824677, Final Batch Loss: 0.39033016562461853\n",
      "Epoch 1574, Loss: 1.2636321187019348, Final Batch Loss: 0.22941499948501587\n",
      "Epoch 1575, Loss: 1.3887971341609955, Final Batch Loss: 0.2464146614074707\n",
      "Epoch 1576, Loss: 1.401483952999115, Final Batch Loss: 0.2948524057865143\n",
      "Epoch 1577, Loss: 1.3015615344047546, Final Batch Loss: 0.3436982333660126\n",
      "Epoch 1578, Loss: 1.3244589567184448, Final Batch Loss: 0.3064696490764618\n",
      "Epoch 1579, Loss: 1.3606326282024384, Final Batch Loss: 0.324311763048172\n",
      "Epoch 1580, Loss: 1.2701996862888336, Final Batch Loss: 0.3081570863723755\n",
      "Epoch 1581, Loss: 1.3302279710769653, Final Batch Loss: 0.3746483623981476\n",
      "Epoch 1582, Loss: 1.362729161977768, Final Batch Loss: 0.32484227418899536\n",
      "Epoch 1583, Loss: 1.3009214997291565, Final Batch Loss: 0.3165454566478729\n",
      "Epoch 1584, Loss: 1.2334851324558258, Final Batch Loss: 0.26954343914985657\n",
      "Epoch 1585, Loss: 1.3873351216316223, Final Batch Loss: 0.3262484073638916\n",
      "Epoch 1586, Loss: 1.3934352397918701, Final Batch Loss: 0.38010746240615845\n",
      "Epoch 1587, Loss: 1.4312495291233063, Final Batch Loss: 0.32502326369285583\n",
      "Epoch 1588, Loss: 1.3484551906585693, Final Batch Loss: 0.31999048590660095\n",
      "Epoch 1589, Loss: 1.4700686037540436, Final Batch Loss: 0.3535919189453125\n",
      "Epoch 1590, Loss: 1.2920814454555511, Final Batch Loss: 0.31169968843460083\n",
      "Epoch 1591, Loss: 1.3279236257076263, Final Batch Loss: 0.42095401883125305\n",
      "Epoch 1592, Loss: 1.3454666435718536, Final Batch Loss: 0.3288308084011078\n",
      "Epoch 1593, Loss: 1.2944908142089844, Final Batch Loss: 0.3727092146873474\n",
      "Epoch 1594, Loss: 1.4389377236366272, Final Batch Loss: 0.4209846258163452\n",
      "Epoch 1595, Loss: 1.3349344730377197, Final Batch Loss: 0.32320958375930786\n",
      "Epoch 1596, Loss: 1.3463814854621887, Final Batch Loss: 0.4000775218009949\n",
      "Epoch 1597, Loss: 1.4408705532550812, Final Batch Loss: 0.3567950427532196\n",
      "Epoch 1598, Loss: 1.4059421122074127, Final Batch Loss: 0.38852301239967346\n",
      "Epoch 1599, Loss: 1.398479700088501, Final Batch Loss: 0.3405262529850006\n",
      "Epoch 1600, Loss: 1.361880898475647, Final Batch Loss: 0.38900694251060486\n",
      "Epoch 1601, Loss: 1.3880990743637085, Final Batch Loss: 0.36202290654182434\n",
      "Epoch 1602, Loss: 1.4220204651355743, Final Batch Loss: 0.36252614855766296\n",
      "Epoch 1603, Loss: 1.3316894173622131, Final Batch Loss: 0.3380240499973297\n",
      "Epoch 1604, Loss: 1.3835155069828033, Final Batch Loss: 0.3203037977218628\n",
      "Epoch 1605, Loss: 1.3847729861736298, Final Batch Loss: 0.371430903673172\n",
      "Epoch 1606, Loss: 1.424502283334732, Final Batch Loss: 0.38064244389533997\n",
      "Epoch 1607, Loss: 1.317405790090561, Final Batch Loss: 0.3563701808452606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1608, Loss: 1.3624632060527802, Final Batch Loss: 0.4201909601688385\n",
      "Epoch 1609, Loss: 1.3068905174732208, Final Batch Loss: 0.3748111128807068\n",
      "Epoch 1610, Loss: 1.3424259424209595, Final Batch Loss: 0.31683191657066345\n",
      "Epoch 1611, Loss: 1.3776549994945526, Final Batch Loss: 0.363464891910553\n",
      "Epoch 1612, Loss: 1.3887988030910492, Final Batch Loss: 0.3535040020942688\n",
      "Epoch 1613, Loss: 1.2980997860431671, Final Batch Loss: 0.3724161982536316\n",
      "Epoch 1614, Loss: 1.346887618303299, Final Batch Loss: 0.3233586549758911\n",
      "Epoch 1615, Loss: 1.4567046761512756, Final Batch Loss: 0.34972456097602844\n",
      "Epoch 1616, Loss: 1.4184223115444183, Final Batch Loss: 0.32259809970855713\n",
      "Epoch 1617, Loss: 1.4225655496120453, Final Batch Loss: 0.3849111795425415\n",
      "Epoch 1618, Loss: 1.469861477613449, Final Batch Loss: 0.3917727768421173\n",
      "Epoch 1619, Loss: 1.4315807819366455, Final Batch Loss: 0.33550840616226196\n",
      "Epoch 1620, Loss: 1.36235311627388, Final Batch Loss: 0.35161229968070984\n",
      "Epoch 1621, Loss: 1.3835492730140686, Final Batch Loss: 0.3516656160354614\n",
      "Epoch 1622, Loss: 1.3501278758049011, Final Batch Loss: 0.36311519145965576\n",
      "Epoch 1623, Loss: 1.3906515836715698, Final Batch Loss: 0.35019582509994507\n",
      "Epoch 1624, Loss: 1.3506123423576355, Final Batch Loss: 0.2840682566165924\n",
      "Epoch 1625, Loss: 1.472988784313202, Final Batch Loss: 0.3957578241825104\n",
      "Epoch 1626, Loss: 1.3001913726329803, Final Batch Loss: 0.3222517669200897\n",
      "Epoch 1627, Loss: 1.4912776947021484, Final Batch Loss: 0.5151060223579407\n",
      "Epoch 1628, Loss: 1.324880212545395, Final Batch Loss: 0.337983101606369\n",
      "Epoch 1629, Loss: 1.3255876004695892, Final Batch Loss: 0.31881532073020935\n",
      "Epoch 1630, Loss: 1.3297745883464813, Final Batch Loss: 0.37568584084510803\n",
      "Epoch 1631, Loss: 1.3453795909881592, Final Batch Loss: 0.3845879137516022\n",
      "Epoch 1632, Loss: 1.3407199680805206, Final Batch Loss: 0.33207783102989197\n",
      "Epoch 1633, Loss: 1.2253828346729279, Final Batch Loss: 0.33037465810775757\n",
      "Epoch 1634, Loss: 1.2759545147418976, Final Batch Loss: 0.34446948766708374\n",
      "Epoch 1635, Loss: 1.2230494916439056, Final Batch Loss: 0.3354898691177368\n",
      "Epoch 1636, Loss: 1.3854953944683075, Final Batch Loss: 0.3767978847026825\n",
      "Epoch 1637, Loss: 1.3124649822711945, Final Batch Loss: 0.3944757878780365\n",
      "Epoch 1638, Loss: 1.4192937016487122, Final Batch Loss: 0.3306677043437958\n",
      "Epoch 1639, Loss: 1.340319663286209, Final Batch Loss: 0.36151379346847534\n",
      "Epoch 1640, Loss: 1.2570304870605469, Final Batch Loss: 0.2612759470939636\n",
      "Epoch 1641, Loss: 1.3388281464576721, Final Batch Loss: 0.2761125862598419\n",
      "Epoch 1642, Loss: 1.2578039169311523, Final Batch Loss: 0.27416324615478516\n",
      "Epoch 1643, Loss: 1.4579056799411774, Final Batch Loss: 0.37479907274246216\n",
      "Epoch 1644, Loss: 1.3957782983779907, Final Batch Loss: 0.35666438937187195\n",
      "Epoch 1645, Loss: 1.3726085722446442, Final Batch Loss: 0.36605754494667053\n",
      "Epoch 1646, Loss: 1.237640142440796, Final Batch Loss: 0.30581462383270264\n",
      "Epoch 1647, Loss: 1.3756184577941895, Final Batch Loss: 0.3640905022621155\n",
      "Epoch 1648, Loss: 1.2692604660987854, Final Batch Loss: 0.26221051812171936\n",
      "Epoch 1649, Loss: 1.444901704788208, Final Batch Loss: 0.37074539065361023\n",
      "Epoch 1650, Loss: 1.3993444740772247, Final Batch Loss: 0.37389907240867615\n",
      "Epoch 1651, Loss: 1.4298955202102661, Final Batch Loss: 0.3613925576210022\n",
      "Epoch 1652, Loss: 1.5133469700813293, Final Batch Loss: 0.4055207669734955\n",
      "Epoch 1653, Loss: 1.3735631108283997, Final Batch Loss: 0.30046772956848145\n",
      "Epoch 1654, Loss: 1.3778351843357086, Final Batch Loss: 0.39884352684020996\n",
      "Epoch 1655, Loss: 1.4068532288074493, Final Batch Loss: 0.322939932346344\n",
      "Epoch 1656, Loss: 1.3734293580055237, Final Batch Loss: 0.32123711705207825\n",
      "Epoch 1657, Loss: 1.3416707217693329, Final Batch Loss: 0.37503939867019653\n",
      "Epoch 1658, Loss: 1.3360007405281067, Final Batch Loss: 0.3691140413284302\n",
      "Epoch 1659, Loss: 1.3383274972438812, Final Batch Loss: 0.3122102618217468\n",
      "Epoch 1660, Loss: 1.21181820333004, Final Batch Loss: 0.2426265925168991\n",
      "Epoch 1661, Loss: 1.210442453622818, Final Batch Loss: 0.2886984348297119\n",
      "Epoch 1662, Loss: 1.366562396287918, Final Batch Loss: 0.2998744547367096\n",
      "Epoch 1663, Loss: 1.4329930245876312, Final Batch Loss: 0.3483703136444092\n",
      "Epoch 1664, Loss: 1.3140197694301605, Final Batch Loss: 0.28939196467399597\n",
      "Epoch 1665, Loss: 1.290292739868164, Final Batch Loss: 0.3655190169811249\n",
      "Epoch 1666, Loss: 1.4354504346847534, Final Batch Loss: 0.2713176906108856\n",
      "Epoch 1667, Loss: 1.3932041823863983, Final Batch Loss: 0.40834665298461914\n",
      "Epoch 1668, Loss: 1.3519622385501862, Final Batch Loss: 0.30626311898231506\n",
      "Epoch 1669, Loss: 1.3471368551254272, Final Batch Loss: 0.3049992620944977\n",
      "Epoch 1670, Loss: 1.4324531853199005, Final Batch Loss: 0.4951168894767761\n",
      "Epoch 1671, Loss: 1.3692705035209656, Final Batch Loss: 0.3023644685745239\n",
      "Epoch 1672, Loss: 1.4132844805717468, Final Batch Loss: 0.37936875224113464\n",
      "Epoch 1673, Loss: 1.3745782971382141, Final Batch Loss: 0.2480454444885254\n",
      "Epoch 1674, Loss: 1.4581075012683868, Final Batch Loss: 0.46163374185562134\n",
      "Epoch 1675, Loss: 1.3110501319169998, Final Batch Loss: 0.33035266399383545\n",
      "Epoch 1676, Loss: 1.3873481154441833, Final Batch Loss: 0.32673951983451843\n",
      "Epoch 1677, Loss: 1.3012275397777557, Final Batch Loss: 0.31791871786117554\n",
      "Epoch 1678, Loss: 1.2639722526073456, Final Batch Loss: 0.24594822525978088\n",
      "Epoch 1679, Loss: 1.3631081581115723, Final Batch Loss: 0.31680959463119507\n",
      "Epoch 1680, Loss: 1.2568045854568481, Final Batch Loss: 0.3155022859573364\n",
      "Epoch 1681, Loss: 1.3684411346912384, Final Batch Loss: 0.4704821705818176\n",
      "Epoch 1682, Loss: 1.3111319839954376, Final Batch Loss: 0.3314446806907654\n",
      "Epoch 1683, Loss: 1.3187047839164734, Final Batch Loss: 0.3573157489299774\n",
      "Epoch 1684, Loss: 1.205806016921997, Final Batch Loss: 0.34936004877090454\n",
      "Epoch 1685, Loss: 1.3743057250976562, Final Batch Loss: 0.3473314940929413\n",
      "Epoch 1686, Loss: 1.2976936101913452, Final Batch Loss: 0.37987253069877625\n",
      "Epoch 1687, Loss: 1.4087142944335938, Final Batch Loss: 0.3825257122516632\n",
      "Epoch 1688, Loss: 1.3911097347736359, Final Batch Loss: 0.32066941261291504\n",
      "Epoch 1689, Loss: 1.4044665694236755, Final Batch Loss: 0.30289730429649353\n",
      "Epoch 1690, Loss: 1.2347357273101807, Final Batch Loss: 0.2910320460796356\n",
      "Epoch 1691, Loss: 1.3679687976837158, Final Batch Loss: 0.35042330622673035\n",
      "Epoch 1692, Loss: 1.2834970951080322, Final Batch Loss: 0.28559058904647827\n",
      "Epoch 1693, Loss: 1.385259062051773, Final Batch Loss: 0.4442617893218994\n",
      "Epoch 1694, Loss: 1.2571468949317932, Final Batch Loss: 0.27481427788734436\n",
      "Epoch 1695, Loss: 1.3541498184204102, Final Batch Loss: 0.4191880226135254\n",
      "Epoch 1696, Loss: 1.3165170848369598, Final Batch Loss: 0.3257606625556946\n",
      "Epoch 1697, Loss: 1.270887941122055, Final Batch Loss: 0.3784547448158264\n",
      "Epoch 1698, Loss: 1.3258338570594788, Final Batch Loss: 0.269754022359848\n",
      "Epoch 1699, Loss: 1.3513202667236328, Final Batch Loss: 0.30671364068984985\n",
      "Epoch 1700, Loss: 1.3546084761619568, Final Batch Loss: 0.3956078886985779\n",
      "Epoch 1701, Loss: 1.292212575674057, Final Batch Loss: 0.3043029010295868\n",
      "Epoch 1702, Loss: 1.2193706333637238, Final Batch Loss: 0.2980007231235504\n",
      "Epoch 1703, Loss: 1.3271999955177307, Final Batch Loss: 0.326690137386322\n",
      "Epoch 1704, Loss: 1.3356059193611145, Final Batch Loss: 0.3402726650238037\n",
      "Epoch 1705, Loss: 1.2093274891376495, Final Batch Loss: 0.27567505836486816\n",
      "Epoch 1706, Loss: 1.3547809720039368, Final Batch Loss: 0.33726170659065247\n",
      "Epoch 1707, Loss: 1.314286395907402, Final Batch Loss: 0.243555948138237\n",
      "Epoch 1708, Loss: 1.3640718162059784, Final Batch Loss: 0.33957821130752563\n",
      "Epoch 1709, Loss: 1.4001514613628387, Final Batch Loss: 0.41310256719589233\n",
      "Epoch 1710, Loss: 1.4844237864017487, Final Batch Loss: 0.42781582474708557\n",
      "Epoch 1711, Loss: 1.4518572986125946, Final Batch Loss: 0.42328259348869324\n",
      "Epoch 1712, Loss: 1.3336346745491028, Final Batch Loss: 0.35246655344963074\n",
      "Epoch 1713, Loss: 1.3508639931678772, Final Batch Loss: 0.3349514603614807\n",
      "Epoch 1714, Loss: 1.2914533615112305, Final Batch Loss: 0.3552522361278534\n",
      "Epoch 1715, Loss: 1.2773450314998627, Final Batch Loss: 0.26179271936416626\n",
      "Epoch 1716, Loss: 1.2343150526285172, Final Batch Loss: 0.24655254185199738\n",
      "Epoch 1717, Loss: 1.2572293877601624, Final Batch Loss: 0.392821341753006\n",
      "Epoch 1718, Loss: 1.4153764247894287, Final Batch Loss: 0.32555583119392395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1719, Loss: 1.223193645477295, Final Batch Loss: 0.32964763045310974\n",
      "Epoch 1720, Loss: 1.3165241181850433, Final Batch Loss: 0.3606446087360382\n",
      "Epoch 1721, Loss: 1.4231809377670288, Final Batch Loss: 0.3951379954814911\n",
      "Epoch 1722, Loss: 1.3139704465866089, Final Batch Loss: 0.2633909583091736\n",
      "Epoch 1723, Loss: 1.4514451920986176, Final Batch Loss: 0.4959161877632141\n",
      "Epoch 1724, Loss: 1.3477748036384583, Final Batch Loss: 0.3969273865222931\n",
      "Epoch 1725, Loss: 1.2905097901821136, Final Batch Loss: 0.3677816689014435\n",
      "Epoch 1726, Loss: 1.4267680644989014, Final Batch Loss: 0.3339548408985138\n",
      "Epoch 1727, Loss: 1.308256059885025, Final Batch Loss: 0.29792001843452454\n",
      "Epoch 1728, Loss: 1.3551805913448334, Final Batch Loss: 0.34125494956970215\n",
      "Epoch 1729, Loss: 1.3254083096981049, Final Batch Loss: 0.29313844442367554\n",
      "Epoch 1730, Loss: 1.3406594693660736, Final Batch Loss: 0.38573411107063293\n",
      "Epoch 1731, Loss: 1.2648175954818726, Final Batch Loss: 0.3076052665710449\n",
      "Epoch 1732, Loss: 1.3875169157981873, Final Batch Loss: 0.3523177206516266\n",
      "Epoch 1733, Loss: 1.278667688369751, Final Batch Loss: 0.34430357813835144\n",
      "Epoch 1734, Loss: 1.3667549192905426, Final Batch Loss: 0.3595670461654663\n",
      "Epoch 1735, Loss: 1.3651922643184662, Final Batch Loss: 0.35835000872612\n",
      "Epoch 1736, Loss: 1.2686971724033356, Final Batch Loss: 0.29495322704315186\n",
      "Epoch 1737, Loss: 1.3353231847286224, Final Batch Loss: 0.3484703302383423\n",
      "Epoch 1738, Loss: 1.3033047914505005, Final Batch Loss: 0.27558180689811707\n",
      "Epoch 1739, Loss: 1.3879487812519073, Final Batch Loss: 0.37834692001342773\n",
      "Epoch 1740, Loss: 1.1953195929527283, Final Batch Loss: 0.2766060531139374\n",
      "Epoch 1741, Loss: 1.4728961884975433, Final Batch Loss: 0.35734114050865173\n",
      "Epoch 1742, Loss: 1.300302803516388, Final Batch Loss: 0.3905707895755768\n",
      "Epoch 1743, Loss: 1.2953681349754333, Final Batch Loss: 0.3290272057056427\n",
      "Epoch 1744, Loss: 1.3625448942184448, Final Batch Loss: 0.2582295536994934\n",
      "Epoch 1745, Loss: 1.3624115884304047, Final Batch Loss: 0.39512014389038086\n",
      "Epoch 1746, Loss: 1.3923459351062775, Final Batch Loss: 0.29396605491638184\n",
      "Epoch 1747, Loss: 1.185096025466919, Final Batch Loss: 0.2849554717540741\n",
      "Epoch 1748, Loss: 1.2206911146640778, Final Batch Loss: 0.3501685559749603\n",
      "Epoch 1749, Loss: 1.404051661491394, Final Batch Loss: 0.29884397983551025\n",
      "Epoch 1750, Loss: 1.26907616853714, Final Batch Loss: 0.3248053789138794\n",
      "Epoch 1751, Loss: 1.3041334748268127, Final Batch Loss: 0.3209204375743866\n",
      "Epoch 1752, Loss: 1.3734261989593506, Final Batch Loss: 0.39553481340408325\n",
      "Epoch 1753, Loss: 1.1705973446369171, Final Batch Loss: 0.33374202251434326\n",
      "Epoch 1754, Loss: 1.3384651243686676, Final Batch Loss: 0.3010061979293823\n",
      "Epoch 1755, Loss: 1.3461487591266632, Final Batch Loss: 0.41032296419143677\n",
      "Epoch 1756, Loss: 1.2534516751766205, Final Batch Loss: 0.30022138357162476\n",
      "Epoch 1757, Loss: 1.3579184114933014, Final Batch Loss: 0.3241758942604065\n",
      "Epoch 1758, Loss: 1.2764916718006134, Final Batch Loss: 0.31909796595573425\n",
      "Epoch 1759, Loss: 1.3089390695095062, Final Batch Loss: 0.31387415528297424\n",
      "Epoch 1760, Loss: 1.196231871843338, Final Batch Loss: 0.28917330503463745\n",
      "Epoch 1761, Loss: 1.3377562761306763, Final Batch Loss: 0.2929391860961914\n",
      "Epoch 1762, Loss: 1.2955693900585175, Final Batch Loss: 0.29378893971443176\n",
      "Epoch 1763, Loss: 1.3165119290351868, Final Batch Loss: 0.3188961446285248\n",
      "Epoch 1764, Loss: 1.2210581004619598, Final Batch Loss: 0.2819457948207855\n",
      "Epoch 1765, Loss: 1.2209379076957703, Final Batch Loss: 0.2923717796802521\n",
      "Epoch 1766, Loss: 1.3070241957902908, Final Batch Loss: 0.3974149823188782\n",
      "Epoch 1767, Loss: 1.2217091619968414, Final Batch Loss: 0.28551632165908813\n",
      "Epoch 1768, Loss: 1.317003071308136, Final Batch Loss: 0.33666902780532837\n",
      "Epoch 1769, Loss: 1.3033750355243683, Final Batch Loss: 0.27967920899391174\n",
      "Epoch 1770, Loss: 1.3637791872024536, Final Batch Loss: 0.4161987006664276\n",
      "Epoch 1771, Loss: 1.2168801724910736, Final Batch Loss: 0.2988819181919098\n",
      "Epoch 1772, Loss: 1.372222751379013, Final Batch Loss: 0.29172056913375854\n",
      "Epoch 1773, Loss: 1.4111336171627045, Final Batch Loss: 0.3838884234428406\n",
      "Epoch 1774, Loss: 1.3038139641284943, Final Batch Loss: 0.3600885272026062\n",
      "Epoch 1775, Loss: 1.3126945197582245, Final Batch Loss: 0.3167021572589874\n",
      "Epoch 1776, Loss: 1.2599208056926727, Final Batch Loss: 0.32439467310905457\n",
      "Epoch 1777, Loss: 1.3163675367832184, Final Batch Loss: 0.34673935174942017\n",
      "Epoch 1778, Loss: 1.262694001197815, Final Batch Loss: 0.2486075758934021\n",
      "Epoch 1779, Loss: 1.4091448187828064, Final Batch Loss: 0.30110323429107666\n",
      "Epoch 1780, Loss: 1.2902446389198303, Final Batch Loss: 0.3273961544036865\n",
      "Epoch 1781, Loss: 1.3055157661437988, Final Batch Loss: 0.32141897082328796\n",
      "Epoch 1782, Loss: 1.306889683008194, Final Batch Loss: 0.3533594608306885\n",
      "Epoch 1783, Loss: 1.325972080230713, Final Batch Loss: 0.31234079599380493\n",
      "Epoch 1784, Loss: 1.2991480231285095, Final Batch Loss: 0.3889397084712982\n",
      "Epoch 1785, Loss: 1.371539980173111, Final Batch Loss: 0.39260536432266235\n",
      "Epoch 1786, Loss: 1.3723454475402832, Final Batch Loss: 0.35379233956336975\n",
      "Epoch 1787, Loss: 1.3954264223575592, Final Batch Loss: 0.35097721219062805\n",
      "Epoch 1788, Loss: 1.2778939008712769, Final Batch Loss: 0.32723310589790344\n",
      "Epoch 1789, Loss: 1.3609380722045898, Final Batch Loss: 0.4175899922847748\n",
      "Epoch 1790, Loss: 1.4681305885314941, Final Batch Loss: 0.30125781893730164\n",
      "Epoch 1791, Loss: 1.365349441766739, Final Batch Loss: 0.3117397725582123\n",
      "Epoch 1792, Loss: 1.3848663866519928, Final Batch Loss: 0.45156168937683105\n",
      "Epoch 1793, Loss: 1.3520047068595886, Final Batch Loss: 0.42557939887046814\n",
      "Epoch 1794, Loss: 1.361025184392929, Final Batch Loss: 0.32760488986968994\n",
      "Epoch 1795, Loss: 1.2860533595085144, Final Batch Loss: 0.27677080035209656\n",
      "Epoch 1796, Loss: 1.2763315737247467, Final Batch Loss: 0.30764275789260864\n",
      "Epoch 1797, Loss: 1.294129103422165, Final Batch Loss: 0.28659725189208984\n",
      "Epoch 1798, Loss: 1.28994682431221, Final Batch Loss: 0.3058071732521057\n",
      "Epoch 1799, Loss: 1.4150396287441254, Final Batch Loss: 0.2846093773841858\n",
      "Epoch 1800, Loss: 1.3291042149066925, Final Batch Loss: 0.34825533628463745\n",
      "Epoch 1801, Loss: 1.3395034670829773, Final Batch Loss: 0.37234675884246826\n",
      "Epoch 1802, Loss: 1.3055160343647003, Final Batch Loss: 0.2686862051486969\n",
      "Epoch 1803, Loss: 1.3247919976711273, Final Batch Loss: 0.34069544076919556\n",
      "Epoch 1804, Loss: 1.5283185243606567, Final Batch Loss: 0.3719462752342224\n",
      "Epoch 1805, Loss: 1.3185728192329407, Final Batch Loss: 0.34432491660118103\n",
      "Epoch 1806, Loss: 1.2844168841838837, Final Batch Loss: 0.3521351218223572\n",
      "Epoch 1807, Loss: 1.3878816962242126, Final Batch Loss: 0.3878768980503082\n",
      "Epoch 1808, Loss: 1.2089450061321259, Final Batch Loss: 0.26162227988243103\n",
      "Epoch 1809, Loss: 1.3176825046539307, Final Batch Loss: 0.29126229882240295\n",
      "Epoch 1810, Loss: 1.2328697443008423, Final Batch Loss: 0.3472551703453064\n",
      "Epoch 1811, Loss: 1.3295550346374512, Final Batch Loss: 0.34346750378608704\n",
      "Epoch 1812, Loss: 1.316517412662506, Final Batch Loss: 0.3765735924243927\n",
      "Epoch 1813, Loss: 1.373178392648697, Final Batch Loss: 0.34473973512649536\n",
      "Epoch 1814, Loss: 1.2682454288005829, Final Batch Loss: 0.30327320098876953\n",
      "Epoch 1815, Loss: 1.335223764181137, Final Batch Loss: 0.340023010969162\n",
      "Epoch 1816, Loss: 1.3274610936641693, Final Batch Loss: 0.3183654546737671\n",
      "Epoch 1817, Loss: 1.2350516021251678, Final Batch Loss: 0.344121515750885\n",
      "Epoch 1818, Loss: 1.3472242653369904, Final Batch Loss: 0.3554822504520416\n",
      "Epoch 1819, Loss: 1.2826074361801147, Final Batch Loss: 0.26044705510139465\n",
      "Epoch 1820, Loss: 1.3486407697200775, Final Batch Loss: 0.32577452063560486\n",
      "Epoch 1821, Loss: 1.4086731374263763, Final Batch Loss: 0.4253349304199219\n",
      "Epoch 1822, Loss: 1.4531555771827698, Final Batch Loss: 0.39006567001342773\n",
      "Epoch 1823, Loss: 1.3004681169986725, Final Batch Loss: 0.3114006817340851\n",
      "Epoch 1824, Loss: 1.2775590121746063, Final Batch Loss: 0.29548943042755127\n",
      "Epoch 1825, Loss: 1.3475326597690582, Final Batch Loss: 0.30736926198005676\n",
      "Epoch 1826, Loss: 1.2023605406284332, Final Batch Loss: 0.2526193857192993\n",
      "Epoch 1827, Loss: 1.3154543042182922, Final Batch Loss: 0.3051033318042755\n",
      "Epoch 1828, Loss: 1.3496504724025726, Final Batch Loss: 0.36293885111808777\n",
      "Epoch 1829, Loss: 1.3238564431667328, Final Batch Loss: 0.348502516746521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1830, Loss: 1.353170484304428, Final Batch Loss: 0.38453149795532227\n",
      "Epoch 1831, Loss: 1.2606517672538757, Final Batch Loss: 0.27523109316825867\n",
      "Epoch 1832, Loss: 1.2892524302005768, Final Batch Loss: 0.3206437826156616\n",
      "Epoch 1833, Loss: 1.286099374294281, Final Batch Loss: 0.3230586051940918\n",
      "Epoch 1834, Loss: 1.3408450186252594, Final Batch Loss: 0.288667768239975\n",
      "Epoch 1835, Loss: 1.2693910896778107, Final Batch Loss: 0.2725195586681366\n",
      "Epoch 1836, Loss: 1.3426334857940674, Final Batch Loss: 0.3911760747432709\n",
      "Epoch 1837, Loss: 1.1665526330471039, Final Batch Loss: 0.2395167052745819\n",
      "Epoch 1838, Loss: 1.401704102754593, Final Batch Loss: 0.4307954013347626\n",
      "Epoch 1839, Loss: 1.2034438252449036, Final Batch Loss: 0.3229933977127075\n",
      "Epoch 1840, Loss: 1.2722587585449219, Final Batch Loss: 0.34317314624786377\n",
      "Epoch 1841, Loss: 1.1615813970565796, Final Batch Loss: 0.29610592126846313\n",
      "Epoch 1842, Loss: 1.2803459465503693, Final Batch Loss: 0.3474109470844269\n",
      "Epoch 1843, Loss: 1.2673023641109467, Final Batch Loss: 0.3089853823184967\n",
      "Epoch 1844, Loss: 1.2933390140533447, Final Batch Loss: 0.33293020725250244\n",
      "Epoch 1845, Loss: 1.3347024619579315, Final Batch Loss: 0.29399776458740234\n",
      "Epoch 1846, Loss: 1.3492265939712524, Final Batch Loss: 0.40838778018951416\n",
      "Epoch 1847, Loss: 1.2058505415916443, Final Batch Loss: 0.26524871587753296\n",
      "Epoch 1848, Loss: 1.3016639649868011, Final Batch Loss: 0.2811201512813568\n",
      "Epoch 1849, Loss: 1.2468378841876984, Final Batch Loss: 0.3615383803844452\n",
      "Epoch 1850, Loss: 1.2883363366127014, Final Batch Loss: 0.3672027885913849\n",
      "Epoch 1851, Loss: 1.3217694163322449, Final Batch Loss: 0.36032021045684814\n",
      "Epoch 1852, Loss: 1.2585066854953766, Final Batch Loss: 0.3081653416156769\n",
      "Epoch 1853, Loss: 1.264253318309784, Final Batch Loss: 0.2606923580169678\n",
      "Epoch 1854, Loss: 1.3257527947425842, Final Batch Loss: 0.29332271218299866\n",
      "Epoch 1855, Loss: 1.2557546496391296, Final Batch Loss: 0.2989295423030853\n",
      "Epoch 1856, Loss: 1.3524895906448364, Final Batch Loss: 0.28925707936286926\n",
      "Epoch 1857, Loss: 1.2303263545036316, Final Batch Loss: 0.2507750689983368\n",
      "Epoch 1858, Loss: 1.1848630905151367, Final Batch Loss: 0.2535010278224945\n",
      "Epoch 1859, Loss: 1.159129336476326, Final Batch Loss: 0.3352651000022888\n",
      "Epoch 1860, Loss: 1.3326722383499146, Final Batch Loss: 0.3811001479625702\n",
      "Epoch 1861, Loss: 1.2977247536182404, Final Batch Loss: 0.2766094505786896\n",
      "Epoch 1862, Loss: 1.2566636800765991, Final Batch Loss: 0.3313119411468506\n",
      "Epoch 1863, Loss: 1.230231136083603, Final Batch Loss: 0.28279662132263184\n",
      "Epoch 1864, Loss: 1.3893680572509766, Final Batch Loss: 0.41975733637809753\n",
      "Epoch 1865, Loss: 1.3461628556251526, Final Batch Loss: 0.39634743332862854\n",
      "Epoch 1866, Loss: 1.354645013809204, Final Batch Loss: 0.3557441532611847\n",
      "Epoch 1867, Loss: 1.2838442027568817, Final Batch Loss: 0.276178240776062\n",
      "Epoch 1868, Loss: 1.2193922102451324, Final Batch Loss: 0.2576562464237213\n",
      "Epoch 1869, Loss: 1.3109621703624725, Final Batch Loss: 0.3211839497089386\n",
      "Epoch 1870, Loss: 1.3050139248371124, Final Batch Loss: 0.32778605818748474\n",
      "Epoch 1871, Loss: 1.3391090333461761, Final Batch Loss: 0.3106946349143982\n",
      "Epoch 1872, Loss: 1.4167389869689941, Final Batch Loss: 0.45136797428131104\n",
      "Epoch 1873, Loss: 1.1079228222370148, Final Batch Loss: 0.28208446502685547\n",
      "Epoch 1874, Loss: 1.196609914302826, Final Batch Loss: 0.2796200215816498\n",
      "Epoch 1875, Loss: 1.1979331374168396, Final Batch Loss: 0.31790998578071594\n",
      "Epoch 1876, Loss: 1.238726556301117, Final Batch Loss: 0.3387053608894348\n",
      "Epoch 1877, Loss: 1.2207832634449005, Final Batch Loss: 0.32520776987075806\n",
      "Epoch 1878, Loss: 1.2486322820186615, Final Batch Loss: 0.346346378326416\n",
      "Epoch 1879, Loss: 1.219037026166916, Final Batch Loss: 0.3104557693004608\n",
      "Epoch 1880, Loss: 1.2284481823444366, Final Batch Loss: 0.29302799701690674\n",
      "Epoch 1881, Loss: 1.3225657045841217, Final Batch Loss: 0.3541715443134308\n",
      "Epoch 1882, Loss: 1.2376712262630463, Final Batch Loss: 0.3583061993122101\n",
      "Epoch 1883, Loss: 1.2554037272930145, Final Batch Loss: 0.31271642446517944\n",
      "Epoch 1884, Loss: 1.3148172199726105, Final Batch Loss: 0.27629807591438293\n",
      "Epoch 1885, Loss: 1.154632642865181, Final Batch Loss: 0.35463157296180725\n",
      "Epoch 1886, Loss: 1.2230490446090698, Final Batch Loss: 0.30472785234451294\n",
      "Epoch 1887, Loss: 1.1988306939601898, Final Batch Loss: 0.2850385308265686\n",
      "Epoch 1888, Loss: 1.1958597898483276, Final Batch Loss: 0.2350465953350067\n",
      "Epoch 1889, Loss: 1.2582625150680542, Final Batch Loss: 0.32331615686416626\n",
      "Epoch 1890, Loss: 1.2170521318912506, Final Batch Loss: 0.3139817714691162\n",
      "Epoch 1891, Loss: 1.2645373046398163, Final Batch Loss: 0.2644578516483307\n",
      "Epoch 1892, Loss: 1.1883715689182281, Final Batch Loss: 0.27213650941848755\n",
      "Epoch 1893, Loss: 1.209224060177803, Final Batch Loss: 0.4394930303096771\n",
      "Epoch 1894, Loss: 1.2467076182365417, Final Batch Loss: 0.23573479056358337\n",
      "Epoch 1895, Loss: 1.2614683210849762, Final Batch Loss: 0.2943829894065857\n",
      "Epoch 1896, Loss: 1.2478805184364319, Final Batch Loss: 0.3536231517791748\n",
      "Epoch 1897, Loss: 1.3438613712787628, Final Batch Loss: 0.33897390961647034\n",
      "Epoch 1898, Loss: 1.1862556040287018, Final Batch Loss: 0.25817641615867615\n",
      "Epoch 1899, Loss: 1.251990720629692, Final Batch Loss: 0.36762258410453796\n",
      "Epoch 1900, Loss: 1.2725706696510315, Final Batch Loss: 0.3199903964996338\n",
      "Epoch 1901, Loss: 1.218646377325058, Final Batch Loss: 0.22869780659675598\n",
      "Epoch 1902, Loss: 1.320269137620926, Final Batch Loss: 0.3137321472167969\n",
      "Epoch 1903, Loss: 1.2320317029953003, Final Batch Loss: 0.2641143202781677\n",
      "Epoch 1904, Loss: 1.370950922369957, Final Batch Loss: 0.4688624143600464\n",
      "Epoch 1905, Loss: 1.2315617501735687, Final Batch Loss: 0.32967308163642883\n",
      "Epoch 1906, Loss: 1.2045390009880066, Final Batch Loss: 0.2711135447025299\n",
      "Epoch 1907, Loss: 1.2233229279518127, Final Batch Loss: 0.28392985463142395\n",
      "Epoch 1908, Loss: 1.208788424730301, Final Batch Loss: 0.27839401364326477\n",
      "Epoch 1909, Loss: 1.2738877534866333, Final Batch Loss: 0.3514567017555237\n",
      "Epoch 1910, Loss: 1.2949157357215881, Final Batch Loss: 0.3216763734817505\n",
      "Epoch 1911, Loss: 1.1095645874738693, Final Batch Loss: 0.24986720085144043\n",
      "Epoch 1912, Loss: 1.2523755580186844, Final Batch Loss: 0.32624995708465576\n",
      "Epoch 1913, Loss: 1.2082035839557648, Final Batch Loss: 0.29875704646110535\n",
      "Epoch 1914, Loss: 1.372988224029541, Final Batch Loss: 0.29878655076026917\n",
      "Epoch 1915, Loss: 1.2148073613643646, Final Batch Loss: 0.2596960663795471\n",
      "Epoch 1916, Loss: 1.1574923396110535, Final Batch Loss: 0.26020827889442444\n",
      "Epoch 1917, Loss: 1.2942970097064972, Final Batch Loss: 0.37289655208587646\n",
      "Epoch 1918, Loss: 1.3106397986412048, Final Batch Loss: 0.29048553109169006\n",
      "Epoch 1919, Loss: 1.176433026790619, Final Batch Loss: 0.3019484281539917\n",
      "Epoch 1920, Loss: 1.2423952519893646, Final Batch Loss: 0.32259610295295715\n",
      "Epoch 1921, Loss: 1.2075592577457428, Final Batch Loss: 0.2566494047641754\n",
      "Epoch 1922, Loss: 1.2305367290973663, Final Batch Loss: 0.3316722512245178\n",
      "Epoch 1923, Loss: 1.3427865505218506, Final Batch Loss: 0.2798653841018677\n",
      "Epoch 1924, Loss: 1.2213724553585052, Final Batch Loss: 0.2668091356754303\n",
      "Epoch 1925, Loss: 1.2493368089199066, Final Batch Loss: 0.2902821898460388\n",
      "Epoch 1926, Loss: 1.20359605550766, Final Batch Loss: 0.3049205243587494\n",
      "Epoch 1927, Loss: 1.2079626023769379, Final Batch Loss: 0.3669435679912567\n",
      "Epoch 1928, Loss: 1.2306139022111893, Final Batch Loss: 0.24617652595043182\n",
      "Epoch 1929, Loss: 1.2400992810726166, Final Batch Loss: 0.2940356433391571\n",
      "Epoch 1930, Loss: 1.2324864268302917, Final Batch Loss: 0.305265873670578\n",
      "Epoch 1931, Loss: 1.2124469578266144, Final Batch Loss: 0.27284353971481323\n",
      "Epoch 1932, Loss: 1.2063244879245758, Final Batch Loss: 0.360463410615921\n",
      "Epoch 1933, Loss: 1.2678488492965698, Final Batch Loss: 0.36045610904693604\n",
      "Epoch 1934, Loss: 1.2502341866493225, Final Batch Loss: 0.2785435914993286\n",
      "Epoch 1935, Loss: 1.3081104010343552, Final Batch Loss: 0.2766841650009155\n",
      "Epoch 1936, Loss: 1.1558531820774078, Final Batch Loss: 0.2880999743938446\n",
      "Epoch 1937, Loss: 1.299012690782547, Final Batch Loss: 0.28500857949256897\n",
      "Epoch 1938, Loss: 1.1768721640110016, Final Batch Loss: 0.30185332894325256\n",
      "Epoch 1939, Loss: 1.258829653263092, Final Batch Loss: 0.2973659336566925\n",
      "Epoch 1940, Loss: 1.3078472912311554, Final Batch Loss: 0.37258654832839966\n",
      "Epoch 1941, Loss: 1.1299200654029846, Final Batch Loss: 0.32585904002189636\n",
      "Epoch 1942, Loss: 1.260795682668686, Final Batch Loss: 0.25493213534355164\n",
      "Epoch 1943, Loss: 1.2398480772972107, Final Batch Loss: 0.2873864471912384\n",
      "Epoch 1944, Loss: 1.3509975671768188, Final Batch Loss: 0.3386685252189636\n",
      "Epoch 1945, Loss: 1.3117480874061584, Final Batch Loss: 0.31349751353263855\n",
      "Epoch 1946, Loss: 1.3000116646289825, Final Batch Loss: 0.4076922833919525\n",
      "Epoch 1947, Loss: 1.3638457357883453, Final Batch Loss: 0.33635756373405457\n",
      "Epoch 1948, Loss: 1.3123851418495178, Final Batch Loss: 0.3357589840888977\n",
      "Epoch 1949, Loss: 1.238284781575203, Final Batch Loss: 0.2377208024263382\n",
      "Epoch 1950, Loss: 1.2346328347921371, Final Batch Loss: 0.230020210146904\n",
      "Epoch 1951, Loss: 1.185242474079132, Final Batch Loss: 0.287060022354126\n",
      "Epoch 1952, Loss: 1.1390644013881683, Final Batch Loss: 0.2749777138233185\n",
      "Epoch 1953, Loss: 1.1350327283143997, Final Batch Loss: 0.22265063226222992\n",
      "Epoch 1954, Loss: 1.2273233532905579, Final Batch Loss: 0.24868077039718628\n",
      "Epoch 1955, Loss: 1.2876259684562683, Final Batch Loss: 0.34007930755615234\n",
      "Epoch 1956, Loss: 1.2208647429943085, Final Batch Loss: 0.3043714165687561\n",
      "Epoch 1957, Loss: 1.2195806056261063, Final Batch Loss: 0.31010520458221436\n",
      "Epoch 1958, Loss: 1.3269034326076508, Final Batch Loss: 0.36401477456092834\n",
      "Epoch 1959, Loss: 1.3377509117126465, Final Batch Loss: 0.3667076826095581\n",
      "Epoch 1960, Loss: 1.205029845237732, Final Batch Loss: 0.2951337695121765\n",
      "Epoch 1961, Loss: 1.1023413836956024, Final Batch Loss: 0.3237612545490265\n",
      "Epoch 1962, Loss: 1.3266113698482513, Final Batch Loss: 0.49297067523002625\n",
      "Epoch 1963, Loss: 1.1646372377872467, Final Batch Loss: 0.28045356273651123\n",
      "Epoch 1964, Loss: 1.318044662475586, Final Batch Loss: 0.316570907831192\n",
      "Epoch 1965, Loss: 1.295375019311905, Final Batch Loss: 0.36664047837257385\n",
      "Epoch 1966, Loss: 1.183167040348053, Final Batch Loss: 0.3464033305644989\n",
      "Epoch 1967, Loss: 1.321489840745926, Final Batch Loss: 0.35242027044296265\n",
      "Epoch 1968, Loss: 1.2807486355304718, Final Batch Loss: 0.3105122447013855\n",
      "Epoch 1969, Loss: 1.2466456294059753, Final Batch Loss: 0.30393388867378235\n",
      "Epoch 1970, Loss: 1.351561576128006, Final Batch Loss: 0.38966071605682373\n",
      "Epoch 1971, Loss: 1.3896007537841797, Final Batch Loss: 0.32800331711769104\n",
      "Epoch 1972, Loss: 1.2176996022462845, Final Batch Loss: 0.3084857165813446\n",
      "Epoch 1973, Loss: 1.2162687182426453, Final Batch Loss: 0.24432232975959778\n",
      "Epoch 1974, Loss: 1.2582443058490753, Final Batch Loss: 0.27167972922325134\n",
      "Epoch 1975, Loss: 1.248527005314827, Final Batch Loss: 0.3205808401107788\n",
      "Epoch 1976, Loss: 1.3993357419967651, Final Batch Loss: 0.38397642970085144\n",
      "Epoch 1977, Loss: 1.2246594429016113, Final Batch Loss: 0.32841163873672485\n",
      "Epoch 1978, Loss: 1.3897264897823334, Final Batch Loss: 0.4329099655151367\n",
      "Epoch 1979, Loss: 1.3267686367034912, Final Batch Loss: 0.3598376214504242\n",
      "Epoch 1980, Loss: 1.2664775252342224, Final Batch Loss: 0.34416815638542175\n",
      "Epoch 1981, Loss: 1.3030660450458527, Final Batch Loss: 0.3000308573246002\n",
      "Epoch 1982, Loss: 1.2671911418437958, Final Batch Loss: 0.34211212396621704\n",
      "Epoch 1983, Loss: 1.2855469286441803, Final Batch Loss: 0.28191888332366943\n",
      "Epoch 1984, Loss: 1.226444885134697, Final Batch Loss: 0.3224099278450012\n",
      "Epoch 1985, Loss: 1.2692778706550598, Final Batch Loss: 0.29033488035202026\n",
      "Epoch 1986, Loss: 1.2148054540157318, Final Batch Loss: 0.317973256111145\n",
      "Epoch 1987, Loss: 1.1788150072097778, Final Batch Loss: 0.29391682147979736\n",
      "Epoch 1988, Loss: 1.3153594434261322, Final Batch Loss: 0.38312602043151855\n",
      "Epoch 1989, Loss: 1.1569596081972122, Final Batch Loss: 0.35135746002197266\n",
      "Epoch 1990, Loss: 1.273288756608963, Final Batch Loss: 0.3574797213077545\n",
      "Epoch 1991, Loss: 1.2195366322994232, Final Batch Loss: 0.2781190872192383\n",
      "Epoch 1992, Loss: 1.1813695132732391, Final Batch Loss: 0.2706429064273834\n",
      "Epoch 1993, Loss: 1.2819146364927292, Final Batch Loss: 0.38615280389785767\n",
      "Epoch 1994, Loss: 1.3169738948345184, Final Batch Loss: 0.39659354090690613\n",
      "Epoch 1995, Loss: 1.1752490997314453, Final Batch Loss: 0.31245723366737366\n",
      "Epoch 1996, Loss: 1.1127143949270248, Final Batch Loss: 0.2477302998304367\n",
      "Epoch 1997, Loss: 1.2435099184513092, Final Batch Loss: 0.33435675501823425\n",
      "Epoch 1998, Loss: 1.2142615914344788, Final Batch Loss: 0.30345794558525085\n",
      "Epoch 1999, Loss: 1.2215568721294403, Final Batch Loss: 0.35407671332359314\n",
      "Epoch 2000, Loss: 1.2465008795261383, Final Batch Loss: 0.2536907196044922\n",
      "Epoch 2001, Loss: 1.2291700839996338, Final Batch Loss: 0.3314456641674042\n",
      "Epoch 2002, Loss: 1.279856652021408, Final Batch Loss: 0.368613600730896\n",
      "Epoch 2003, Loss: 1.164259523153305, Final Batch Loss: 0.29725024104118347\n",
      "Epoch 2004, Loss: 1.1940861940383911, Final Batch Loss: 0.3035377562046051\n",
      "Epoch 2005, Loss: 1.2614244520664215, Final Batch Loss: 0.3131334185600281\n",
      "Epoch 2006, Loss: 1.3079786002635956, Final Batch Loss: 0.3486412465572357\n",
      "Epoch 2007, Loss: 1.2610597908496857, Final Batch Loss: 0.3851849138736725\n",
      "Epoch 2008, Loss: 1.1787875890731812, Final Batch Loss: 0.31664153933525085\n",
      "Epoch 2009, Loss: 1.1806118786334991, Final Batch Loss: 0.27706268429756165\n",
      "Epoch 2010, Loss: 1.2842978835105896, Final Batch Loss: 0.4410088360309601\n",
      "Epoch 2011, Loss: 1.1521515250205994, Final Batch Loss: 0.2986929416656494\n",
      "Epoch 2012, Loss: 1.1614360064268112, Final Batch Loss: 0.31482481956481934\n",
      "Epoch 2013, Loss: 1.3883365988731384, Final Batch Loss: 0.3486894369125366\n",
      "Epoch 2014, Loss: 1.2852716743946075, Final Batch Loss: 0.3278503119945526\n",
      "Epoch 2015, Loss: 1.2448556274175644, Final Batch Loss: 0.3278924524784088\n",
      "Epoch 2016, Loss: 1.225789487361908, Final Batch Loss: 0.2912122309207916\n",
      "Epoch 2017, Loss: 1.3268782496452332, Final Batch Loss: 0.37042126059532166\n",
      "Epoch 2018, Loss: 1.3101404309272766, Final Batch Loss: 0.35110074281692505\n",
      "Epoch 2019, Loss: 1.1847259998321533, Final Batch Loss: 0.2759127914905548\n",
      "Epoch 2020, Loss: 1.2100758254528046, Final Batch Loss: 0.3145674467086792\n",
      "Epoch 2021, Loss: 1.3108079731464386, Final Batch Loss: 0.26003021001815796\n",
      "Epoch 2022, Loss: 1.1854588240385056, Final Batch Loss: 0.24069146811962128\n",
      "Epoch 2023, Loss: 1.35900217294693, Final Batch Loss: 0.30454233288764954\n",
      "Epoch 2024, Loss: 1.291903167963028, Final Batch Loss: 0.3229295611381531\n",
      "Epoch 2025, Loss: 1.2730741798877716, Final Batch Loss: 0.358112633228302\n",
      "Epoch 2026, Loss: 1.1793966591358185, Final Batch Loss: 0.2819356322288513\n",
      "Epoch 2027, Loss: 1.3059957176446915, Final Batch Loss: 0.4325351417064667\n",
      "Epoch 2028, Loss: 1.2659346610307693, Final Batch Loss: 0.24245469272136688\n",
      "Epoch 2029, Loss: 1.287502110004425, Final Batch Loss: 0.29507097601890564\n",
      "Epoch 2030, Loss: 1.188778206706047, Final Batch Loss: 0.2903542220592499\n",
      "Epoch 2031, Loss: 1.2849164307117462, Final Batch Loss: 0.3800593912601471\n",
      "Epoch 2032, Loss: 1.2666156589984894, Final Batch Loss: 0.3691628873348236\n",
      "Epoch 2033, Loss: 1.2176524102687836, Final Batch Loss: 0.33684685826301575\n",
      "Epoch 2034, Loss: 1.226972982287407, Final Batch Loss: 0.391965389251709\n",
      "Epoch 2035, Loss: 1.1542481482028961, Final Batch Loss: 0.2789011597633362\n",
      "Epoch 2036, Loss: 1.1433019638061523, Final Batch Loss: 0.3026825785636902\n",
      "Epoch 2037, Loss: 1.2779106497764587, Final Batch Loss: 0.350333034992218\n",
      "Epoch 2038, Loss: 1.217034250497818, Final Batch Loss: 0.3021294176578522\n",
      "Epoch 2039, Loss: 1.1672379672527313, Final Batch Loss: 0.256553053855896\n",
      "Epoch 2040, Loss: 1.1716754138469696, Final Batch Loss: 0.22364367544651031\n",
      "Epoch 2041, Loss: 1.1092078685760498, Final Batch Loss: 0.2747381031513214\n",
      "Epoch 2042, Loss: 1.1743378341197968, Final Batch Loss: 0.2247694730758667\n",
      "Epoch 2043, Loss: 1.173487812280655, Final Batch Loss: 0.31968164443969727\n",
      "Epoch 2044, Loss: 1.2440169155597687, Final Batch Loss: 0.3057100772857666\n",
      "Epoch 2045, Loss: 1.161782443523407, Final Batch Loss: 0.26045021414756775\n",
      "Epoch 2046, Loss: 1.1797458678483963, Final Batch Loss: 0.23711608350276947\n",
      "Epoch 2047, Loss: 1.3650912940502167, Final Batch Loss: 0.3881611227989197\n",
      "Epoch 2048, Loss: 1.1517926156520844, Final Batch Loss: 0.226742684841156\n",
      "Epoch 2049, Loss: 1.3319378197193146, Final Batch Loss: 0.4235231280326843\n",
      "Epoch 2050, Loss: 1.144122526049614, Final Batch Loss: 0.27595028281211853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2051, Loss: 1.2195425927639008, Final Batch Loss: 0.27109283208847046\n",
      "Epoch 2052, Loss: 1.1917412877082825, Final Batch Loss: 0.2562752068042755\n",
      "Epoch 2053, Loss: 1.251481294631958, Final Batch Loss: 0.3767431080341339\n",
      "Epoch 2054, Loss: 1.2415883839130402, Final Batch Loss: 0.29550573229789734\n",
      "Epoch 2055, Loss: 1.2880737334489822, Final Batch Loss: 0.4188828468322754\n",
      "Epoch 2056, Loss: 1.2664261162281036, Final Batch Loss: 0.24895823001861572\n",
      "Epoch 2057, Loss: 1.261912852525711, Final Batch Loss: 0.3485517203807831\n",
      "Epoch 2058, Loss: 1.2346454560756683, Final Batch Loss: 0.3769873380661011\n",
      "Epoch 2059, Loss: 1.2200586795806885, Final Batch Loss: 0.28017181158065796\n",
      "Epoch 2060, Loss: 1.1559543311595917, Final Batch Loss: 0.23620730638504028\n",
      "Epoch 2061, Loss: 1.1866508722305298, Final Batch Loss: 0.307121604681015\n",
      "Epoch 2062, Loss: 1.1548746824264526, Final Batch Loss: 0.3001585006713867\n",
      "Epoch 2063, Loss: 1.2839954793453217, Final Batch Loss: 0.3742562234401703\n",
      "Epoch 2064, Loss: 1.265519380569458, Final Batch Loss: 0.28761762380599976\n",
      "Epoch 2065, Loss: 1.2306852042675018, Final Batch Loss: 0.27832067012786865\n",
      "Epoch 2066, Loss: 1.3044432699680328, Final Batch Loss: 0.380073219537735\n",
      "Epoch 2067, Loss: 1.1984500885009766, Final Batch Loss: 0.3009122312068939\n",
      "Epoch 2068, Loss: 1.2845875024795532, Final Batch Loss: 0.36308157444000244\n",
      "Epoch 2069, Loss: 1.234455168247223, Final Batch Loss: 0.37879282236099243\n",
      "Epoch 2070, Loss: 1.2091483175754547, Final Batch Loss: 0.3457004725933075\n",
      "Epoch 2071, Loss: 1.1736584603786469, Final Batch Loss: 0.26053887605667114\n",
      "Epoch 2072, Loss: 1.264535829424858, Final Batch Loss: 0.3245038390159607\n",
      "Epoch 2073, Loss: 1.2732681035995483, Final Batch Loss: 0.34626656770706177\n",
      "Epoch 2074, Loss: 1.1664355993270874, Final Batch Loss: 0.2850720286369324\n",
      "Epoch 2075, Loss: 1.214745283126831, Final Batch Loss: 0.2813360095024109\n",
      "Epoch 2076, Loss: 1.198328047990799, Final Batch Loss: 0.17940562963485718\n",
      "Epoch 2077, Loss: 1.2644712924957275, Final Batch Loss: 0.30371057987213135\n",
      "Epoch 2078, Loss: 1.1390069127082825, Final Batch Loss: 0.21844559907913208\n",
      "Epoch 2079, Loss: 1.129420906305313, Final Batch Loss: 0.2760390043258667\n",
      "Epoch 2080, Loss: 1.2022470831871033, Final Batch Loss: 0.25901544094085693\n",
      "Epoch 2081, Loss: 1.1220979988574982, Final Batch Loss: 0.261309951543808\n",
      "Epoch 2082, Loss: 1.35900217294693, Final Batch Loss: 0.35909828543663025\n",
      "Epoch 2083, Loss: 1.2038266956806183, Final Batch Loss: 0.2642372250556946\n",
      "Epoch 2084, Loss: 1.2439095377922058, Final Batch Loss: 0.33384135365486145\n",
      "Epoch 2085, Loss: 1.1875879168510437, Final Batch Loss: 0.22980844974517822\n",
      "Epoch 2086, Loss: 1.1382974684238434, Final Batch Loss: 0.2836875021457672\n",
      "Epoch 2087, Loss: 1.2472842335700989, Final Batch Loss: 0.3344079554080963\n",
      "Epoch 2088, Loss: 1.3278106153011322, Final Batch Loss: 0.34796690940856934\n",
      "Epoch 2089, Loss: 1.2128205299377441, Final Batch Loss: 0.3344971835613251\n",
      "Epoch 2090, Loss: 1.269667774438858, Final Batch Loss: 0.28972211480140686\n",
      "Epoch 2091, Loss: 1.227720007300377, Final Batch Loss: 0.21469156444072723\n",
      "Epoch 2092, Loss: 1.2697533667087555, Final Batch Loss: 0.3006463944911957\n",
      "Epoch 2093, Loss: 1.2431057840585709, Final Batch Loss: 0.2242707461118698\n",
      "Epoch 2094, Loss: 1.331924855709076, Final Batch Loss: 0.3857375383377075\n",
      "Epoch 2095, Loss: 1.252871036529541, Final Batch Loss: 0.3019070625305176\n",
      "Epoch 2096, Loss: 1.3237996399402618, Final Batch Loss: 0.3912092447280884\n",
      "Epoch 2097, Loss: 1.2751456201076508, Final Batch Loss: 0.25932517647743225\n",
      "Epoch 2098, Loss: 1.242671638727188, Final Batch Loss: 0.32948315143585205\n",
      "Epoch 2099, Loss: 1.279833346605301, Final Batch Loss: 0.3199922740459442\n",
      "Epoch 2100, Loss: 1.164268434047699, Final Batch Loss: 0.3012503385543823\n",
      "Epoch 2101, Loss: 1.2011486887931824, Final Batch Loss: 0.3047732710838318\n",
      "Epoch 2102, Loss: 1.2450430989265442, Final Batch Loss: 0.2864307761192322\n",
      "Epoch 2103, Loss: 1.2870914041996002, Final Batch Loss: 0.38307300209999084\n",
      "Epoch 2104, Loss: 1.2268076241016388, Final Batch Loss: 0.2858746349811554\n",
      "Epoch 2105, Loss: 1.1655407547950745, Final Batch Loss: 0.24343499541282654\n",
      "Epoch 2106, Loss: 1.1294651925563812, Final Batch Loss: 0.2648372948169708\n",
      "Epoch 2107, Loss: 1.2778409719467163, Final Batch Loss: 0.35117554664611816\n",
      "Epoch 2108, Loss: 1.2039491534233093, Final Batch Loss: 0.3050258457660675\n",
      "Epoch 2109, Loss: 1.3941530287265778, Final Batch Loss: 0.5025823712348938\n",
      "Epoch 2110, Loss: 1.1058603674173355, Final Batch Loss: 0.26322850584983826\n",
      "Epoch 2111, Loss: 1.1930593699216843, Final Batch Loss: 0.24419613182544708\n",
      "Epoch 2112, Loss: 1.143789380788803, Final Batch Loss: 0.296993225812912\n",
      "Epoch 2113, Loss: 1.273735523223877, Final Batch Loss: 0.3532490134239197\n",
      "Epoch 2114, Loss: 1.223303660750389, Final Batch Loss: 0.23740990459918976\n",
      "Epoch 2115, Loss: 1.3257759511470795, Final Batch Loss: 0.37651577591896057\n",
      "Epoch 2116, Loss: 1.199733167886734, Final Batch Loss: 0.33384963870048523\n",
      "Epoch 2117, Loss: 1.1929762661457062, Final Batch Loss: 0.2542760968208313\n",
      "Epoch 2118, Loss: 1.2382193803787231, Final Batch Loss: 0.3537016212940216\n",
      "Epoch 2119, Loss: 1.2017664015293121, Final Batch Loss: 0.29915398359298706\n",
      "Epoch 2120, Loss: 1.254092514514923, Final Batch Loss: 0.27969467639923096\n",
      "Epoch 2121, Loss: 1.2031554132699966, Final Batch Loss: 0.24733369052410126\n",
      "Epoch 2122, Loss: 1.2403065860271454, Final Batch Loss: 0.293660044670105\n",
      "Epoch 2123, Loss: 1.3201059699058533, Final Batch Loss: 0.24991026520729065\n",
      "Epoch 2124, Loss: 1.2816260010004044, Final Batch Loss: 0.48832011222839355\n",
      "Epoch 2125, Loss: 1.2518155872821808, Final Batch Loss: 0.32287973165512085\n",
      "Epoch 2126, Loss: 1.1368474066257477, Final Batch Loss: 0.22219973802566528\n",
      "Epoch 2127, Loss: 1.3187076151371002, Final Batch Loss: 0.328348308801651\n",
      "Epoch 2128, Loss: 1.4092262089252472, Final Batch Loss: 0.37467464804649353\n",
      "Epoch 2129, Loss: 1.110402688384056, Final Batch Loss: 0.2502281069755554\n",
      "Epoch 2130, Loss: 1.256877452135086, Final Batch Loss: 0.33294716477394104\n",
      "Epoch 2131, Loss: 1.264930933713913, Final Batch Loss: 0.3248886466026306\n",
      "Epoch 2132, Loss: 1.1506300568580627, Final Batch Loss: 0.2966635227203369\n",
      "Epoch 2133, Loss: 1.2419258952140808, Final Batch Loss: 0.32488006353378296\n",
      "Epoch 2134, Loss: 1.2022906839847565, Final Batch Loss: 0.3648471236228943\n",
      "Epoch 2135, Loss: 1.125936582684517, Final Batch Loss: 0.31152769923210144\n",
      "Epoch 2136, Loss: 1.2206786274909973, Final Batch Loss: 0.31437259912490845\n",
      "Epoch 2137, Loss: 1.2010907530784607, Final Batch Loss: 0.30884820222854614\n",
      "Epoch 2138, Loss: 1.2802831828594208, Final Batch Loss: 0.346144437789917\n",
      "Epoch 2139, Loss: 1.1688449680805206, Final Batch Loss: 0.31621798872947693\n",
      "Epoch 2140, Loss: 1.215376377105713, Final Batch Loss: 0.31412771344184875\n",
      "Epoch 2141, Loss: 1.1804229021072388, Final Batch Loss: 0.29244914650917053\n",
      "Epoch 2142, Loss: 1.2902003228664398, Final Batch Loss: 0.35287177562713623\n",
      "Epoch 2143, Loss: 1.1967023611068726, Final Batch Loss: 0.26390203833580017\n",
      "Epoch 2144, Loss: 1.2619528025388718, Final Batch Loss: 0.3443198502063751\n",
      "Epoch 2145, Loss: 1.165527731180191, Final Batch Loss: 0.24747821688652039\n",
      "Epoch 2146, Loss: 1.1282436847686768, Final Batch Loss: 0.3138872981071472\n",
      "Epoch 2147, Loss: 1.1649754047393799, Final Batch Loss: 0.29562556743621826\n",
      "Epoch 2148, Loss: 1.2274597883224487, Final Batch Loss: 0.36428582668304443\n",
      "Epoch 2149, Loss: 1.1846734285354614, Final Batch Loss: 0.2887079417705536\n",
      "Epoch 2150, Loss: 1.304153934121132, Final Batch Loss: 0.3698139488697052\n",
      "Epoch 2151, Loss: 1.2311657965183258, Final Batch Loss: 0.22857186198234558\n",
      "Epoch 2152, Loss: 1.2484902143478394, Final Batch Loss: 0.2766565680503845\n",
      "Epoch 2153, Loss: 1.1622498333454132, Final Batch Loss: 0.3496842086315155\n",
      "Epoch 2154, Loss: 1.1347987949848175, Final Batch Loss: 0.2540687620639801\n",
      "Epoch 2155, Loss: 1.2663575112819672, Final Batch Loss: 0.2802961766719818\n",
      "Epoch 2156, Loss: 1.1263324916362762, Final Batch Loss: 0.27084439992904663\n",
      "Epoch 2157, Loss: 1.078576683998108, Final Batch Loss: 0.26168423891067505\n",
      "Epoch 2158, Loss: 1.302714079618454, Final Batch Loss: 0.3417538106441498\n",
      "Epoch 2159, Loss: 1.1901764869689941, Final Batch Loss: 0.2624727785587311\n",
      "Epoch 2160, Loss: 1.1936945915222168, Final Batch Loss: 0.2727676033973694\n",
      "Epoch 2161, Loss: 1.2463978826999664, Final Batch Loss: 0.29762470722198486\n",
      "Epoch 2162, Loss: 1.189087226986885, Final Batch Loss: 0.3105084002017975\n",
      "Epoch 2163, Loss: 1.0533560663461685, Final Batch Loss: 0.20251129567623138\n",
      "Epoch 2164, Loss: 1.1698398292064667, Final Batch Loss: 0.3040376603603363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2165, Loss: 1.1801410913467407, Final Batch Loss: 0.3010961413383484\n",
      "Epoch 2166, Loss: 1.1049489825963974, Final Batch Loss: 0.31783780455589294\n",
      "Epoch 2167, Loss: 1.203751653432846, Final Batch Loss: 0.3420259654521942\n",
      "Epoch 2168, Loss: 1.224145621061325, Final Batch Loss: 0.28958314657211304\n",
      "Epoch 2169, Loss: 1.2520994544029236, Final Batch Loss: 0.28832927346229553\n",
      "Epoch 2170, Loss: 1.286867618560791, Final Batch Loss: 0.30390939116477966\n",
      "Epoch 2171, Loss: 1.2270934581756592, Final Batch Loss: 0.31363242864608765\n",
      "Epoch 2172, Loss: 1.1535247564315796, Final Batch Loss: 0.29355302453041077\n",
      "Epoch 2173, Loss: 1.1713656783103943, Final Batch Loss: 0.30923914909362793\n",
      "Epoch 2174, Loss: 1.0926409512758255, Final Batch Loss: 0.22777268290519714\n",
      "Epoch 2175, Loss: 1.1766467988491058, Final Batch Loss: 0.28294360637664795\n",
      "Epoch 2176, Loss: 1.1335827708244324, Final Batch Loss: 0.22578883171081543\n",
      "Epoch 2177, Loss: 1.0861503332853317, Final Batch Loss: 0.23300465941429138\n",
      "Epoch 2178, Loss: 1.2054400146007538, Final Batch Loss: 0.27978745102882385\n",
      "Epoch 2179, Loss: 1.1527411937713623, Final Batch Loss: 0.308260977268219\n",
      "Epoch 2180, Loss: 1.1369205564260483, Final Batch Loss: 0.2878433167934418\n",
      "Epoch 2181, Loss: 1.1345428824424744, Final Batch Loss: 0.2611536383628845\n",
      "Epoch 2182, Loss: 1.2253831326961517, Final Batch Loss: 0.3290782868862152\n",
      "Epoch 2183, Loss: 1.151119515299797, Final Batch Loss: 0.27116918563842773\n",
      "Epoch 2184, Loss: 1.1799547374248505, Final Batch Loss: 0.3075149357318878\n",
      "Epoch 2185, Loss: 1.2646704018115997, Final Batch Loss: 0.27054816484451294\n",
      "Epoch 2186, Loss: 1.173398107290268, Final Batch Loss: 0.26994916796684265\n",
      "Epoch 2187, Loss: 1.2224875688552856, Final Batch Loss: 0.3398856222629547\n",
      "Epoch 2188, Loss: 1.264254480600357, Final Batch Loss: 0.29833513498306274\n",
      "Epoch 2189, Loss: 1.3159412443637848, Final Batch Loss: 0.3037734925746918\n",
      "Epoch 2190, Loss: 1.1863593608140945, Final Batch Loss: 0.24694772064685822\n",
      "Epoch 2191, Loss: 1.1882583051919937, Final Batch Loss: 0.24367497861385345\n",
      "Epoch 2192, Loss: 1.238202065229416, Final Batch Loss: 0.3233570158481598\n",
      "Epoch 2193, Loss: 1.1327592730522156, Final Batch Loss: 0.2684209942817688\n",
      "Epoch 2194, Loss: 1.3035228848457336, Final Batch Loss: 0.3714906573295593\n",
      "Epoch 2195, Loss: 1.2376407980918884, Final Batch Loss: 0.299846351146698\n",
      "Epoch 2196, Loss: 1.0779835134744644, Final Batch Loss: 0.22012369334697723\n",
      "Epoch 2197, Loss: 1.0636848360300064, Final Batch Loss: 0.2759152054786682\n",
      "Epoch 2198, Loss: 1.1191577315330505, Final Batch Loss: 0.2600712478160858\n",
      "Epoch 2199, Loss: 1.1069581359624863, Final Batch Loss: 0.30070918798446655\n",
      "Epoch 2200, Loss: 1.1803803741931915, Final Batch Loss: 0.29126155376434326\n",
      "Epoch 2201, Loss: 1.322488933801651, Final Batch Loss: 0.29026395082473755\n",
      "Epoch 2202, Loss: 1.1298445165157318, Final Batch Loss: 0.23850631713867188\n",
      "Epoch 2203, Loss: 1.184106320142746, Final Batch Loss: 0.2887246906757355\n",
      "Epoch 2204, Loss: 1.2552754580974579, Final Batch Loss: 0.3500487506389618\n",
      "Epoch 2205, Loss: 1.3010717034339905, Final Batch Loss: 0.3537665903568268\n",
      "Epoch 2206, Loss: 1.16275292634964, Final Batch Loss: 0.3317159414291382\n",
      "Epoch 2207, Loss: 1.1981518268585205, Final Batch Loss: 0.3430941104888916\n",
      "Epoch 2208, Loss: 1.0938676595687866, Final Batch Loss: 0.274460107088089\n",
      "Epoch 2209, Loss: 1.153457522392273, Final Batch Loss: 0.2936866879463196\n",
      "Epoch 2210, Loss: 1.1133308112621307, Final Batch Loss: 0.2848586440086365\n",
      "Epoch 2211, Loss: 1.1439207196235657, Final Batch Loss: 0.2527581453323364\n",
      "Epoch 2212, Loss: 1.1242596805095673, Final Batch Loss: 0.30104923248291016\n",
      "Epoch 2213, Loss: 1.2194448113441467, Final Batch Loss: 0.33690333366394043\n",
      "Epoch 2214, Loss: 1.2582006752490997, Final Batch Loss: 0.3549797236919403\n",
      "Epoch 2215, Loss: 1.1509969234466553, Final Batch Loss: 0.2825850546360016\n",
      "Epoch 2216, Loss: 1.263503760099411, Final Batch Loss: 0.27324095368385315\n",
      "Epoch 2217, Loss: 1.2862773537635803, Final Batch Loss: 0.26191771030426025\n",
      "Epoch 2218, Loss: 1.3122927844524384, Final Batch Loss: 0.3048402965068817\n",
      "Epoch 2219, Loss: 1.119924545288086, Final Batch Loss: 0.36506643891334534\n",
      "Epoch 2220, Loss: 1.2178983390331268, Final Batch Loss: 0.3094606101512909\n",
      "Epoch 2221, Loss: 1.1921930462121964, Final Batch Loss: 0.23889748752117157\n",
      "Epoch 2222, Loss: 1.1335264295339584, Final Batch Loss: 0.3168649673461914\n",
      "Epoch 2223, Loss: 1.208398312330246, Final Batch Loss: 0.29538214206695557\n",
      "Epoch 2224, Loss: 1.2715296298265457, Final Batch Loss: 0.3968046009540558\n",
      "Epoch 2225, Loss: 1.1556383222341537, Final Batch Loss: 0.3439340889453888\n",
      "Epoch 2226, Loss: 1.2335765063762665, Final Batch Loss: 0.3383382558822632\n",
      "Epoch 2227, Loss: 1.199596643447876, Final Batch Loss: 0.3427601456642151\n",
      "Epoch 2228, Loss: 1.1659241914749146, Final Batch Loss: 0.3216157853603363\n",
      "Epoch 2229, Loss: 1.2262237071990967, Final Batch Loss: 0.2558007836341858\n",
      "Epoch 2230, Loss: 1.1019204258918762, Final Batch Loss: 0.27596643567085266\n",
      "Epoch 2231, Loss: 1.1804874241352081, Final Batch Loss: 0.33168867230415344\n",
      "Epoch 2232, Loss: 1.2827719748020172, Final Batch Loss: 0.3407577574253082\n",
      "Epoch 2233, Loss: 1.1468786895275116, Final Batch Loss: 0.2669563889503479\n",
      "Epoch 2234, Loss: 1.240517944097519, Final Batch Loss: 0.2592371702194214\n",
      "Epoch 2235, Loss: 1.0780621767044067, Final Batch Loss: 0.24349796772003174\n",
      "Epoch 2236, Loss: 1.1104107797145844, Final Batch Loss: 0.2563365399837494\n",
      "Epoch 2237, Loss: 1.0401162505149841, Final Batch Loss: 0.2500807046890259\n",
      "Epoch 2238, Loss: 1.1647620797157288, Final Batch Loss: 0.2817929983139038\n",
      "Epoch 2239, Loss: 1.2150019407272339, Final Batch Loss: 0.27604013681411743\n",
      "Epoch 2240, Loss: 1.1623562425374985, Final Batch Loss: 0.43425917625427246\n",
      "Epoch 2241, Loss: 1.155882716178894, Final Batch Loss: 0.29889988899230957\n",
      "Epoch 2242, Loss: 1.2937673330307007, Final Batch Loss: 0.2125435471534729\n",
      "Epoch 2243, Loss: 1.1853011697530746, Final Batch Loss: 0.24938398599624634\n",
      "Epoch 2244, Loss: 1.2738292664289474, Final Batch Loss: 0.22493629157543182\n",
      "Epoch 2245, Loss: 1.2977086901664734, Final Batch Loss: 0.3695802092552185\n",
      "Epoch 2246, Loss: 1.1023728102445602, Final Batch Loss: 0.33637499809265137\n",
      "Epoch 2247, Loss: 1.165787696838379, Final Batch Loss: 0.2629113793373108\n",
      "Epoch 2248, Loss: 1.2421469986438751, Final Batch Loss: 0.40900614857673645\n",
      "Epoch 2249, Loss: 1.1283357441425323, Final Batch Loss: 0.25342971086502075\n",
      "Epoch 2250, Loss: 1.2116735577583313, Final Batch Loss: 0.2616555392742157\n",
      "Epoch 2251, Loss: 1.299750566482544, Final Batch Loss: 0.3026007413864136\n",
      "Epoch 2252, Loss: 1.11780846118927, Final Batch Loss: 0.2594040632247925\n",
      "Epoch 2253, Loss: 1.2299654483795166, Final Batch Loss: 0.4187859296798706\n",
      "Epoch 2254, Loss: 1.1152940690517426, Final Batch Loss: 0.2744913697242737\n",
      "Epoch 2255, Loss: 1.1578381061553955, Final Batch Loss: 0.27752405405044556\n",
      "Epoch 2256, Loss: 1.154353380203247, Final Batch Loss: 0.28674885630607605\n",
      "Epoch 2257, Loss: 1.1898384988307953, Final Batch Loss: 0.3287277817726135\n",
      "Epoch 2258, Loss: 1.203633889555931, Final Batch Loss: 0.3067936599254608\n",
      "Epoch 2259, Loss: 1.3174883723258972, Final Batch Loss: 0.40257272124290466\n",
      "Epoch 2260, Loss: 1.217803955078125, Final Batch Loss: 0.29363375902175903\n",
      "Epoch 2261, Loss: 1.1130684316158295, Final Batch Loss: 0.21777406334877014\n",
      "Epoch 2262, Loss: 1.227921724319458, Final Batch Loss: 0.25990599393844604\n",
      "Epoch 2263, Loss: 1.1395826935768127, Final Batch Loss: 0.30924829840660095\n",
      "Epoch 2264, Loss: 1.1947316825389862, Final Batch Loss: 0.2606956958770752\n",
      "Epoch 2265, Loss: 1.262143313884735, Final Batch Loss: 0.2577466368675232\n",
      "Epoch 2266, Loss: 1.197242110967636, Final Batch Loss: 0.31313830614089966\n",
      "Epoch 2267, Loss: 1.190493643283844, Final Batch Loss: 0.31980153918266296\n",
      "Epoch 2268, Loss: 1.1445949226617813, Final Batch Loss: 0.34934747219085693\n",
      "Epoch 2269, Loss: 1.1919949650764465, Final Batch Loss: 0.2730714976787567\n",
      "Epoch 2270, Loss: 1.2358873188495636, Final Batch Loss: 0.29266947507858276\n",
      "Epoch 2271, Loss: 1.3321915566921234, Final Batch Loss: 0.3924804627895355\n",
      "Epoch 2272, Loss: 1.1388950049877167, Final Batch Loss: 0.2658194303512573\n",
      "Epoch 2273, Loss: 1.2510057389736176, Final Batch Loss: 0.3063860833644867\n",
      "Epoch 2274, Loss: 1.3075934052467346, Final Batch Loss: 0.35780665278434753\n",
      "Epoch 2275, Loss: 1.0596276223659515, Final Batch Loss: 0.20253528654575348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2276, Loss: 1.1981820166110992, Final Batch Loss: 0.34250888228416443\n",
      "Epoch 2277, Loss: 1.2146853506565094, Final Batch Loss: 0.30402350425720215\n",
      "Epoch 2278, Loss: 1.2693351805210114, Final Batch Loss: 0.29910188913345337\n",
      "Epoch 2279, Loss: 1.3075405955314636, Final Batch Loss: 0.27413210272789\n",
      "Epoch 2280, Loss: 1.1926169693470001, Final Batch Loss: 0.27239519357681274\n",
      "Epoch 2281, Loss: 1.2032762467861176, Final Batch Loss: 0.28009262681007385\n",
      "Epoch 2282, Loss: 1.1433340907096863, Final Batch Loss: 0.20317238569259644\n",
      "Epoch 2283, Loss: 1.242640107870102, Final Batch Loss: 0.34657347202301025\n",
      "Epoch 2284, Loss: 1.2668525874614716, Final Batch Loss: 0.25708356499671936\n",
      "Epoch 2285, Loss: 1.1565136164426804, Final Batch Loss: 0.24798522889614105\n",
      "Epoch 2286, Loss: 1.1371957212686539, Final Batch Loss: 0.23880772292613983\n",
      "Epoch 2287, Loss: 1.0968220829963684, Final Batch Loss: 0.24758297204971313\n",
      "Epoch 2288, Loss: 1.15666663646698, Final Batch Loss: 0.3352554142475128\n",
      "Epoch 2289, Loss: 1.0759700387716293, Final Batch Loss: 0.27446505427360535\n",
      "Epoch 2290, Loss: 1.0876759141683578, Final Batch Loss: 0.2328103929758072\n",
      "Epoch 2291, Loss: 1.1141459494829178, Final Batch Loss: 0.2765372395515442\n",
      "Epoch 2292, Loss: 1.3302212357521057, Final Batch Loss: 0.33295968174934387\n",
      "Epoch 2293, Loss: 1.1683013141155243, Final Batch Loss: 0.2235141396522522\n",
      "Epoch 2294, Loss: 1.1635621786117554, Final Batch Loss: 0.3648620545864105\n",
      "Epoch 2295, Loss: 1.1000481843948364, Final Batch Loss: 0.31688353419303894\n",
      "Epoch 2296, Loss: 1.1588057726621628, Final Batch Loss: 0.31456291675567627\n",
      "Epoch 2297, Loss: 1.1929157078266144, Final Batch Loss: 0.3403291404247284\n",
      "Epoch 2298, Loss: 1.1358028650283813, Final Batch Loss: 0.28103357553482056\n",
      "Epoch 2299, Loss: 1.190874606370926, Final Batch Loss: 0.3198799788951874\n",
      "Epoch 2300, Loss: 1.1966681480407715, Final Batch Loss: 0.2528739273548126\n",
      "Epoch 2301, Loss: 1.1688898503780365, Final Batch Loss: 0.2585313320159912\n",
      "Epoch 2302, Loss: 1.2415766417980194, Final Batch Loss: 0.3299420177936554\n",
      "Epoch 2303, Loss: 1.113268330693245, Final Batch Loss: 0.31614625453948975\n",
      "Epoch 2304, Loss: 1.1964589208364487, Final Batch Loss: 0.3676377236843109\n",
      "Epoch 2305, Loss: 1.2126842737197876, Final Batch Loss: 0.34149977564811707\n",
      "Epoch 2306, Loss: 1.1959643363952637, Final Batch Loss: 0.2531772553920746\n",
      "Epoch 2307, Loss: 1.0726496577262878, Final Batch Loss: 0.31829553842544556\n",
      "Epoch 2308, Loss: 1.1391287595033646, Final Batch Loss: 0.2533339262008667\n",
      "Epoch 2309, Loss: 1.2084526717662811, Final Batch Loss: 0.28250205516815186\n",
      "Epoch 2310, Loss: 1.0980307161808014, Final Batch Loss: 0.25541946291923523\n",
      "Epoch 2311, Loss: 1.1907039284706116, Final Batch Loss: 0.3085649013519287\n",
      "Epoch 2312, Loss: 1.1643089950084686, Final Batch Loss: 0.2388390600681305\n",
      "Epoch 2313, Loss: 1.1696884632110596, Final Batch Loss: 0.2855830490589142\n",
      "Epoch 2314, Loss: 1.1720023304224014, Final Batch Loss: 0.3426664471626282\n",
      "Epoch 2315, Loss: 1.0711201876401901, Final Batch Loss: 0.27439484000205994\n",
      "Epoch 2316, Loss: 1.2930250465869904, Final Batch Loss: 0.3576727509498596\n",
      "Epoch 2317, Loss: 1.1264986097812653, Final Batch Loss: 0.23763909935951233\n",
      "Epoch 2318, Loss: 1.1212677955627441, Final Batch Loss: 0.28751140832901\n",
      "Epoch 2319, Loss: 1.1417458653450012, Final Batch Loss: 0.28569239377975464\n",
      "Epoch 2320, Loss: 1.3347904980182648, Final Batch Loss: 0.4448696970939636\n",
      "Epoch 2321, Loss: 1.1543704569339752, Final Batch Loss: 0.3114422857761383\n",
      "Epoch 2322, Loss: 1.156269133090973, Final Batch Loss: 0.2609386146068573\n",
      "Epoch 2323, Loss: 1.205260992050171, Final Batch Loss: 0.30381399393081665\n",
      "Epoch 2324, Loss: 1.156363844871521, Final Batch Loss: 0.2609855532646179\n",
      "Epoch 2325, Loss: 1.0991330742835999, Final Batch Loss: 0.178225576877594\n",
      "Epoch 2326, Loss: 1.1527439057826996, Final Batch Loss: 0.29384955763816833\n",
      "Epoch 2327, Loss: 1.2651086747646332, Final Batch Loss: 0.3178976774215698\n",
      "Epoch 2328, Loss: 1.217959612607956, Final Batch Loss: 0.3891175091266632\n",
      "Epoch 2329, Loss: 1.125280886888504, Final Batch Loss: 0.1865139901638031\n",
      "Epoch 2330, Loss: 1.2470905035734177, Final Batch Loss: 0.23533405363559723\n",
      "Epoch 2331, Loss: 1.0232117176055908, Final Batch Loss: 0.2125391960144043\n",
      "Epoch 2332, Loss: 1.0139144510030746, Final Batch Loss: 0.16454534232616425\n",
      "Epoch 2333, Loss: 1.1085528880357742, Final Batch Loss: 0.23135706782341003\n",
      "Epoch 2334, Loss: 1.1064478158950806, Final Batch Loss: 0.302629679441452\n",
      "Epoch 2335, Loss: 1.1555905938148499, Final Batch Loss: 0.20653924345970154\n",
      "Epoch 2336, Loss: 1.185681402683258, Final Batch Loss: 0.2560979425907135\n",
      "Epoch 2337, Loss: 1.145059272646904, Final Batch Loss: 0.24078159034252167\n",
      "Epoch 2338, Loss: 1.2370730340480804, Final Batch Loss: 0.30435049533843994\n",
      "Epoch 2339, Loss: 1.234113872051239, Final Batch Loss: 0.3174710273742676\n",
      "Epoch 2340, Loss: 1.2009299993515015, Final Batch Loss: 0.26562806963920593\n",
      "Epoch 2341, Loss: 1.1929630637168884, Final Batch Loss: 0.2799109220504761\n",
      "Epoch 2342, Loss: 1.1923376321792603, Final Batch Loss: 0.32860711216926575\n",
      "Epoch 2343, Loss: 1.1445804834365845, Final Batch Loss: 0.22329573333263397\n",
      "Epoch 2344, Loss: 1.1456810534000397, Final Batch Loss: 0.32644444704055786\n",
      "Epoch 2345, Loss: 1.2815374433994293, Final Batch Loss: 0.335218220949173\n",
      "Epoch 2346, Loss: 1.2398078739643097, Final Batch Loss: 0.33629539608955383\n",
      "Epoch 2347, Loss: 1.3055136799812317, Final Batch Loss: 0.3190704882144928\n",
      "Epoch 2348, Loss: 1.126880168914795, Final Batch Loss: 0.2381928265094757\n",
      "Epoch 2349, Loss: 1.1727029532194138, Final Batch Loss: 0.276858389377594\n",
      "Epoch 2350, Loss: 1.1355810314416885, Final Batch Loss: 0.2368779480457306\n",
      "Epoch 2351, Loss: 1.0816386938095093, Final Batch Loss: 0.21852916479110718\n",
      "Epoch 2352, Loss: 1.3324329555034637, Final Batch Loss: 0.32496190071105957\n",
      "Epoch 2353, Loss: 1.2067990452051163, Final Batch Loss: 0.32538267970085144\n",
      "Epoch 2354, Loss: 1.0103520601987839, Final Batch Loss: 0.2841550409793854\n",
      "Epoch 2355, Loss: 1.2849299013614655, Final Batch Loss: 0.3339046835899353\n",
      "Epoch 2356, Loss: 1.1722801178693771, Final Batch Loss: 0.32229915261268616\n",
      "Epoch 2357, Loss: 1.0898425728082657, Final Batch Loss: 0.2575388550758362\n",
      "Epoch 2358, Loss: 1.1103265434503555, Final Batch Loss: 0.20451654493808746\n",
      "Epoch 2359, Loss: 1.1269645988941193, Final Batch Loss: 0.2820678651332855\n",
      "Epoch 2360, Loss: 1.2201247364282608, Final Batch Loss: 0.2447805255651474\n",
      "Epoch 2361, Loss: 1.1856005787849426, Final Batch Loss: 0.3033417761325836\n",
      "Epoch 2362, Loss: 1.3385455310344696, Final Batch Loss: 0.35169464349746704\n",
      "Epoch 2363, Loss: 1.2141827791929245, Final Batch Loss: 0.2329872101545334\n",
      "Epoch 2364, Loss: 1.172209620475769, Final Batch Loss: 0.277020663022995\n",
      "Epoch 2365, Loss: 1.1473928391933441, Final Batch Loss: 0.3385522961616516\n",
      "Epoch 2366, Loss: 1.093548208475113, Final Batch Loss: 0.19568556547164917\n",
      "Epoch 2367, Loss: 1.0979111045598984, Final Batch Loss: 0.21737520396709442\n",
      "Epoch 2368, Loss: 1.0150029063224792, Final Batch Loss: 0.2509271502494812\n",
      "Epoch 2369, Loss: 1.2082203030586243, Final Batch Loss: 0.33019691705703735\n",
      "Epoch 2370, Loss: 1.2207539975643158, Final Batch Loss: 0.20973876118659973\n",
      "Epoch 2371, Loss: 1.1928488910198212, Final Batch Loss: 0.30465322732925415\n",
      "Epoch 2372, Loss: 1.231416016817093, Final Batch Loss: 0.2785131633281708\n",
      "Epoch 2373, Loss: 1.1484741866588593, Final Batch Loss: 0.250214159488678\n",
      "Epoch 2374, Loss: 1.1708801984786987, Final Batch Loss: 0.2880958616733551\n",
      "Epoch 2375, Loss: 1.1652841717004776, Final Batch Loss: 0.3686140179634094\n",
      "Epoch 2376, Loss: 1.054801657795906, Final Batch Loss: 0.2618647515773773\n",
      "Epoch 2377, Loss: 1.1998324394226074, Final Batch Loss: 0.287916898727417\n",
      "Epoch 2378, Loss: 1.150248259305954, Final Batch Loss: 0.2645083963871002\n",
      "Epoch 2379, Loss: 1.0242339074611664, Final Batch Loss: 0.2368207424879074\n",
      "Epoch 2380, Loss: 1.0385144799947739, Final Batch Loss: 0.24736617505550385\n",
      "Epoch 2381, Loss: 1.2953144311904907, Final Batch Loss: 0.3111840486526489\n",
      "Epoch 2382, Loss: 1.155263140797615, Final Batch Loss: 0.33522599935531616\n",
      "Epoch 2383, Loss: 1.0217450112104416, Final Batch Loss: 0.18505503237247467\n",
      "Epoch 2384, Loss: 1.2243402898311615, Final Batch Loss: 0.34399640560150146\n",
      "Epoch 2385, Loss: 1.164302483201027, Final Batch Loss: 0.37899884581565857\n",
      "Epoch 2386, Loss: 1.2749462127685547, Final Batch Loss: 0.29756104946136475\n",
      "Epoch 2387, Loss: 1.1613472998142242, Final Batch Loss: 0.26411816477775574\n",
      "Epoch 2388, Loss: 1.1982252597808838, Final Batch Loss: 0.4434906244277954\n",
      "Epoch 2389, Loss: 1.2299865782260895, Final Batch Loss: 0.3364082872867584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2390, Loss: 1.1743052303791046, Final Batch Loss: 0.3630964756011963\n",
      "Epoch 2391, Loss: 1.1273775547742844, Final Batch Loss: 0.24871326982975006\n",
      "Epoch 2392, Loss: 1.093401700258255, Final Batch Loss: 0.2373887300491333\n",
      "Epoch 2393, Loss: 1.2022979855537415, Final Batch Loss: 0.26409414410591125\n",
      "Epoch 2394, Loss: 1.1724332869052887, Final Batch Loss: 0.2840052843093872\n",
      "Epoch 2395, Loss: 1.0480501055717468, Final Batch Loss: 0.2696770131587982\n",
      "Epoch 2396, Loss: 1.156305193901062, Final Batch Loss: 0.28165483474731445\n",
      "Epoch 2397, Loss: 1.1197270452976227, Final Batch Loss: 0.2280207872390747\n",
      "Epoch 2398, Loss: 1.1120727509260178, Final Batch Loss: 0.2734590470790863\n",
      "Epoch 2399, Loss: 1.1777901351451874, Final Batch Loss: 0.2645929455757141\n",
      "Epoch 2400, Loss: 1.256475567817688, Final Batch Loss: 0.3112122714519501\n",
      "Epoch 2401, Loss: 1.1418412923812866, Final Batch Loss: 0.2940199375152588\n",
      "Epoch 2402, Loss: 1.2819207608699799, Final Batch Loss: 0.35234740376472473\n",
      "Epoch 2403, Loss: 1.3146646916866302, Final Batch Loss: 0.2715722918510437\n",
      "Epoch 2404, Loss: 1.2416293621063232, Final Batch Loss: 0.352805495262146\n",
      "Epoch 2405, Loss: 1.2316930890083313, Final Batch Loss: 0.39878517389297485\n",
      "Epoch 2406, Loss: 1.1608726978302002, Final Batch Loss: 0.25738275051116943\n",
      "Epoch 2407, Loss: 1.196398377418518, Final Batch Loss: 0.29677969217300415\n",
      "Epoch 2408, Loss: 1.1583613455295563, Final Batch Loss: 0.30400264263153076\n",
      "Epoch 2409, Loss: 1.1746214926242828, Final Batch Loss: 0.2972782254219055\n",
      "Epoch 2410, Loss: 1.045090675354004, Final Batch Loss: 0.2763507664203644\n",
      "Epoch 2411, Loss: 1.2118798792362213, Final Batch Loss: 0.2942572832107544\n",
      "Epoch 2412, Loss: 1.1771044731140137, Final Batch Loss: 0.3769800066947937\n",
      "Epoch 2413, Loss: 1.267388105392456, Final Batch Loss: 0.3144172728061676\n",
      "Epoch 2414, Loss: 0.9859036654233932, Final Batch Loss: 0.1841469258069992\n",
      "Epoch 2415, Loss: 1.0872564017772675, Final Batch Loss: 0.2942422926425934\n",
      "Epoch 2416, Loss: 1.113424375653267, Final Batch Loss: 0.21788673102855682\n",
      "Epoch 2417, Loss: 1.0441581904888153, Final Batch Loss: 0.2752125561237335\n",
      "Epoch 2418, Loss: 1.1469376683235168, Final Batch Loss: 0.23837435245513916\n",
      "Epoch 2419, Loss: 1.1605679541826248, Final Batch Loss: 0.29035431146621704\n",
      "Epoch 2420, Loss: 1.2881380319595337, Final Batch Loss: 0.3708314597606659\n",
      "Epoch 2421, Loss: 1.0800746977329254, Final Batch Loss: 0.2788889706134796\n",
      "Epoch 2422, Loss: 1.165932685136795, Final Batch Loss: 0.32193708419799805\n",
      "Epoch 2423, Loss: 1.1840577870607376, Final Batch Loss: 0.2259119302034378\n",
      "Epoch 2424, Loss: 1.1479766815900803, Final Batch Loss: 0.26303359866142273\n",
      "Epoch 2425, Loss: 1.1650698482990265, Final Batch Loss: 0.2930541932582855\n",
      "Epoch 2426, Loss: 1.0928100496530533, Final Batch Loss: 0.23329944908618927\n",
      "Epoch 2427, Loss: 1.1991613507270813, Final Batch Loss: 0.3559955060482025\n",
      "Epoch 2428, Loss: 1.146840512752533, Final Batch Loss: 0.25787419080734253\n",
      "Epoch 2429, Loss: 1.1408970654010773, Final Batch Loss: 0.30096396803855896\n",
      "Epoch 2430, Loss: 1.2868396937847137, Final Batch Loss: 0.25130337476730347\n",
      "Epoch 2431, Loss: 1.1364159882068634, Final Batch Loss: 0.25555551052093506\n",
      "Epoch 2432, Loss: 1.2018332779407501, Final Batch Loss: 0.34511834383010864\n",
      "Epoch 2433, Loss: 1.1910122483968735, Final Batch Loss: 0.3314906060695648\n",
      "Epoch 2434, Loss: 1.1651955246925354, Final Batch Loss: 0.2805560827255249\n",
      "Epoch 2435, Loss: 1.1719521582126617, Final Batch Loss: 0.27932649850845337\n",
      "Epoch 2436, Loss: 1.1852702051401138, Final Batch Loss: 0.2641006410121918\n",
      "Epoch 2437, Loss: 1.1875041127204895, Final Batch Loss: 0.24083051085472107\n",
      "Epoch 2438, Loss: 1.208084687590599, Final Batch Loss: 0.236617311835289\n",
      "Epoch 2439, Loss: 1.1691496968269348, Final Batch Loss: 0.3517856299877167\n",
      "Epoch 2440, Loss: 1.1053911596536636, Final Batch Loss: 0.21103285253047943\n",
      "Epoch 2441, Loss: 1.123028188943863, Final Batch Loss: 0.27303841710090637\n",
      "Epoch 2442, Loss: 1.153595119714737, Final Batch Loss: 0.3143296539783478\n",
      "Epoch 2443, Loss: 1.1026826798915863, Final Batch Loss: 0.26509273052215576\n",
      "Epoch 2444, Loss: 1.1559903770685196, Final Batch Loss: 0.24608488380908966\n",
      "Epoch 2445, Loss: 1.1631839871406555, Final Batch Loss: 0.32747408747673035\n",
      "Epoch 2446, Loss: 1.2193774580955505, Final Batch Loss: 0.2735626697540283\n",
      "Epoch 2447, Loss: 1.0634576380252838, Final Batch Loss: 0.2673700153827667\n",
      "Epoch 2448, Loss: 1.1676375269889832, Final Batch Loss: 0.21954315900802612\n",
      "Epoch 2449, Loss: 1.2571529150009155, Final Batch Loss: 0.3663894832134247\n",
      "Epoch 2450, Loss: 1.2039258182048798, Final Batch Loss: 0.3688753545284271\n",
      "Epoch 2451, Loss: 1.234319418668747, Final Batch Loss: 0.3404827415943146\n",
      "Epoch 2452, Loss: 1.122697800397873, Final Batch Loss: 0.2510824203491211\n",
      "Epoch 2453, Loss: 1.0856851041316986, Final Batch Loss: 0.2591642141342163\n",
      "Epoch 2454, Loss: 1.1874560117721558, Final Batch Loss: 0.33622971177101135\n",
      "Epoch 2455, Loss: 1.1315082609653473, Final Batch Loss: 0.3169228136539459\n",
      "Epoch 2456, Loss: 1.2578027546405792, Final Batch Loss: 0.36654186248779297\n",
      "Epoch 2457, Loss: 1.2582954168319702, Final Batch Loss: 0.2784785330295563\n",
      "Epoch 2458, Loss: 1.0905937254428864, Final Batch Loss: 0.26519379019737244\n",
      "Epoch 2459, Loss: 1.3027706146240234, Final Batch Loss: 0.25683388113975525\n",
      "Epoch 2460, Loss: 1.23475781083107, Final Batch Loss: 0.26203763484954834\n",
      "Epoch 2461, Loss: 1.015256479382515, Final Batch Loss: 0.17449437081813812\n",
      "Epoch 2462, Loss: 1.0873438864946365, Final Batch Loss: 0.30249133706092834\n",
      "Epoch 2463, Loss: 1.131824642419815, Final Batch Loss: 0.26505735516548157\n",
      "Epoch 2464, Loss: 1.0907700061798096, Final Batch Loss: 0.31544873118400574\n",
      "Epoch 2465, Loss: 1.0486120581626892, Final Batch Loss: 0.23513676226139069\n",
      "Epoch 2466, Loss: 1.0923622101545334, Final Batch Loss: 0.2045614868402481\n",
      "Epoch 2467, Loss: 1.0720451772212982, Final Batch Loss: 0.31777146458625793\n",
      "Epoch 2468, Loss: 1.1347511410713196, Final Batch Loss: 0.31330108642578125\n",
      "Epoch 2469, Loss: 1.0145753026008606, Final Batch Loss: 0.2391730695962906\n",
      "Epoch 2470, Loss: 1.1514571011066437, Final Batch Loss: 0.33596158027648926\n",
      "Epoch 2471, Loss: 1.119458258152008, Final Batch Loss: 0.2486853301525116\n",
      "Epoch 2472, Loss: 1.1206402778625488, Final Batch Loss: 0.25456932187080383\n",
      "Epoch 2473, Loss: 1.1038089245557785, Final Batch Loss: 0.2769257426261902\n",
      "Epoch 2474, Loss: 1.1318139880895615, Final Batch Loss: 0.3302474915981293\n",
      "Epoch 2475, Loss: 1.1873686611652374, Final Batch Loss: 0.22695010900497437\n",
      "Epoch 2476, Loss: 1.12513667345047, Final Batch Loss: 0.2799259126186371\n",
      "Epoch 2477, Loss: 1.3938391506671906, Final Batch Loss: 0.38987573981285095\n",
      "Epoch 2478, Loss: 1.1817850768566132, Final Batch Loss: 0.2751554250717163\n",
      "Epoch 2479, Loss: 1.227989912033081, Final Batch Loss: 0.30061230063438416\n",
      "Epoch 2480, Loss: 1.1459825038909912, Final Batch Loss: 0.29307666420936584\n",
      "Epoch 2481, Loss: 1.0546639412641525, Final Batch Loss: 0.28870078921318054\n",
      "Epoch 2482, Loss: 1.1372972279787064, Final Batch Loss: 0.24227507412433624\n",
      "Epoch 2483, Loss: 1.1063130348920822, Final Batch Loss: 0.3192724585533142\n",
      "Epoch 2484, Loss: 1.1053723394870758, Final Batch Loss: 0.3462282419204712\n",
      "Epoch 2485, Loss: 1.1050177812576294, Final Batch Loss: 0.27733275294303894\n",
      "Epoch 2486, Loss: 1.0099047422409058, Final Batch Loss: 0.23468805849552155\n",
      "Epoch 2487, Loss: 1.0787054598331451, Final Batch Loss: 0.3358680009841919\n",
      "Epoch 2488, Loss: 1.1475484520196915, Final Batch Loss: 0.33192920684814453\n",
      "Epoch 2489, Loss: 1.147136464715004, Final Batch Loss: 0.21657659113407135\n",
      "Epoch 2490, Loss: 1.0616802871227264, Final Batch Loss: 0.2643314599990845\n",
      "Epoch 2491, Loss: 1.1145292818546295, Final Batch Loss: 0.30055952072143555\n",
      "Epoch 2492, Loss: 1.2075206637382507, Final Batch Loss: 0.2592470049858093\n",
      "Epoch 2493, Loss: 1.058104857802391, Final Batch Loss: 0.2634481191635132\n",
      "Epoch 2494, Loss: 1.0423806458711624, Final Batch Loss: 0.310804545879364\n",
      "Epoch 2495, Loss: 1.140165537595749, Final Batch Loss: 0.2914973795413971\n",
      "Epoch 2496, Loss: 1.080494225025177, Final Batch Loss: 0.2896181344985962\n",
      "Epoch 2497, Loss: 1.19373619556427, Final Batch Loss: 0.32057085633277893\n",
      "Epoch 2498, Loss: 1.0751422196626663, Final Batch Loss: 0.2845481336116791\n",
      "Epoch 2499, Loss: 1.1468793749809265, Final Batch Loss: 0.27941855788230896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2500, Loss: 1.019699603319168, Final Batch Loss: 0.2437163144350052\n",
      "Epoch 2501, Loss: 1.1205974221229553, Final Batch Loss: 0.25838029384613037\n",
      "Epoch 2502, Loss: 1.1273442804813385, Final Batch Loss: 0.3027845323085785\n",
      "Epoch 2503, Loss: 1.1065259873867035, Final Batch Loss: 0.30487170815467834\n",
      "Epoch 2504, Loss: 1.0624775886535645, Final Batch Loss: 0.22501307725906372\n",
      "Epoch 2505, Loss: 1.0631278306245804, Final Batch Loss: 0.24848271906375885\n",
      "Epoch 2506, Loss: 1.2879764139652252, Final Batch Loss: 0.21955573558807373\n",
      "Epoch 2507, Loss: 1.0989049077033997, Final Batch Loss: 0.23567286133766174\n",
      "Epoch 2508, Loss: 1.0918647199869156, Final Batch Loss: 0.23959742486476898\n",
      "Epoch 2509, Loss: 1.1138472259044647, Final Batch Loss: 0.2628135681152344\n",
      "Epoch 2510, Loss: 1.1131894886493683, Final Batch Loss: 0.22056668996810913\n",
      "Epoch 2511, Loss: 1.2004938572645187, Final Batch Loss: 0.2405470758676529\n",
      "Epoch 2512, Loss: 1.1518876254558563, Final Batch Loss: 0.2642996311187744\n",
      "Epoch 2513, Loss: 1.108018845319748, Final Batch Loss: 0.25542882084846497\n",
      "Epoch 2514, Loss: 1.265345573425293, Final Batch Loss: 0.3649694323539734\n",
      "Epoch 2515, Loss: 1.1112342029809952, Final Batch Loss: 0.24764446914196014\n",
      "Epoch 2516, Loss: 1.1352462619543076, Final Batch Loss: 0.31452861428260803\n",
      "Epoch 2517, Loss: 1.2493822276592255, Final Batch Loss: 0.38410571217536926\n",
      "Epoch 2518, Loss: 1.1185355633497238, Final Batch Loss: 0.18977417051792145\n",
      "Epoch 2519, Loss: 1.2313222289085388, Final Batch Loss: 0.2782588005065918\n",
      "Epoch 2520, Loss: 1.045145571231842, Final Batch Loss: 0.2629998028278351\n",
      "Epoch 2521, Loss: 1.1272217333316803, Final Batch Loss: 0.23758667707443237\n",
      "Epoch 2522, Loss: 1.1444013118743896, Final Batch Loss: 0.2847072184085846\n",
      "Epoch 2523, Loss: 1.042698249220848, Final Batch Loss: 0.18718667328357697\n",
      "Epoch 2524, Loss: 1.1180949807167053, Final Batch Loss: 0.305270254611969\n",
      "Epoch 2525, Loss: 1.1195243895053864, Final Batch Loss: 0.285890132188797\n",
      "Epoch 2526, Loss: 1.1978024691343307, Final Batch Loss: 0.3108213543891907\n",
      "Epoch 2527, Loss: 1.186893880367279, Final Batch Loss: 0.32709500193595886\n",
      "Epoch 2528, Loss: 1.0550156384706497, Final Batch Loss: 0.22160334885120392\n",
      "Epoch 2529, Loss: 1.0268652886152267, Final Batch Loss: 0.25967684388160706\n",
      "Epoch 2530, Loss: 1.2218348979949951, Final Batch Loss: 0.39861544966697693\n",
      "Epoch 2531, Loss: 1.1306093633174896, Final Batch Loss: 0.2611686587333679\n",
      "Epoch 2532, Loss: 1.12413989007473, Final Batch Loss: 0.27657923102378845\n",
      "Epoch 2533, Loss: 1.3831105530261993, Final Batch Loss: 0.3285881280899048\n",
      "Epoch 2534, Loss: 1.0753392577171326, Final Batch Loss: 0.25180584192276\n",
      "Epoch 2535, Loss: 1.1631336212158203, Final Batch Loss: 0.273447185754776\n",
      "Epoch 2536, Loss: 1.1483825147151947, Final Batch Loss: 0.27450233697891235\n",
      "Epoch 2537, Loss: 1.0714893192052841, Final Batch Loss: 0.25008895993232727\n",
      "Epoch 2538, Loss: 1.1983902007341385, Final Batch Loss: 0.3306540250778198\n",
      "Epoch 2539, Loss: 1.1575837135314941, Final Batch Loss: 0.2923734188079834\n",
      "Epoch 2540, Loss: 1.130872219800949, Final Batch Loss: 0.24476608633995056\n",
      "Epoch 2541, Loss: 1.1555605679750443, Final Batch Loss: 0.2890391945838928\n",
      "Epoch 2542, Loss: 1.0836555659770966, Final Batch Loss: 0.3125329911708832\n",
      "Epoch 2543, Loss: 1.066309779882431, Final Batch Loss: 0.2661525309085846\n",
      "Epoch 2544, Loss: 1.0441333651542664, Final Batch Loss: 0.27593129873275757\n",
      "Epoch 2545, Loss: 1.2856378555297852, Final Batch Loss: 0.44726452231407166\n",
      "Epoch 2546, Loss: 1.0591772496700287, Final Batch Loss: 0.26467135548591614\n",
      "Epoch 2547, Loss: 1.1818923652172089, Final Batch Loss: 0.2785746157169342\n",
      "Epoch 2548, Loss: 1.111620768904686, Final Batch Loss: 0.30778345465660095\n",
      "Epoch 2549, Loss: 1.0515343248844147, Final Batch Loss: 0.2004350870847702\n",
      "Epoch 2550, Loss: 1.0647484362125397, Final Batch Loss: 0.3128478229045868\n",
      "Epoch 2551, Loss: 0.9452469050884247, Final Batch Loss: 0.23675839602947235\n",
      "Epoch 2552, Loss: 1.0974511802196503, Final Batch Loss: 0.31244105100631714\n",
      "Epoch 2553, Loss: 1.087463453412056, Final Batch Loss: 0.30597877502441406\n",
      "Epoch 2554, Loss: 0.9925814121961594, Final Batch Loss: 0.24304689466953278\n",
      "Epoch 2555, Loss: 1.1073524802923203, Final Batch Loss: 0.2755213677883148\n",
      "Epoch 2556, Loss: 1.0587602108716965, Final Batch Loss: 0.3292921483516693\n",
      "Epoch 2557, Loss: 1.1006525158882141, Final Batch Loss: 0.2312823235988617\n",
      "Epoch 2558, Loss: 1.1365372389554977, Final Batch Loss: 0.3691788911819458\n",
      "Epoch 2559, Loss: 1.247178554534912, Final Batch Loss: 0.3778398633003235\n",
      "Epoch 2560, Loss: 1.1598498970270157, Final Batch Loss: 0.24115557968616486\n",
      "Epoch 2561, Loss: 1.139715388417244, Final Batch Loss: 0.2777232825756073\n",
      "Epoch 2562, Loss: 0.996443897485733, Final Batch Loss: 0.2507951259613037\n",
      "Epoch 2563, Loss: 0.989477813243866, Final Batch Loss: 0.19889041781425476\n",
      "Epoch 2564, Loss: 0.9298485666513443, Final Batch Loss: 0.20000828802585602\n",
      "Epoch 2565, Loss: 1.1194469332695007, Final Batch Loss: 0.2722420394420624\n",
      "Epoch 2566, Loss: 1.2381618171930313, Final Batch Loss: 0.3387981355190277\n",
      "Epoch 2567, Loss: 1.1501644849777222, Final Batch Loss: 0.33905112743377686\n",
      "Epoch 2568, Loss: 1.1839412152767181, Final Batch Loss: 0.2958660125732422\n",
      "Epoch 2569, Loss: 1.0726319253444672, Final Batch Loss: 0.28823813796043396\n",
      "Epoch 2570, Loss: 1.1670283079147339, Final Batch Loss: 0.3497292995452881\n",
      "Epoch 2571, Loss: 1.0282978415489197, Final Batch Loss: 0.256671667098999\n",
      "Epoch 2572, Loss: 1.1297576129436493, Final Batch Loss: 0.2780076563358307\n",
      "Epoch 2573, Loss: 1.1395464092493057, Final Batch Loss: 0.3212902247905731\n",
      "Epoch 2574, Loss: 1.0659809857606888, Final Batch Loss: 0.2650783956050873\n",
      "Epoch 2575, Loss: 1.147990271449089, Final Batch Loss: 0.24568025767803192\n",
      "Epoch 2576, Loss: 1.074182152748108, Final Batch Loss: 0.28736257553100586\n",
      "Epoch 2577, Loss: 1.1853218078613281, Final Batch Loss: 0.2656697630882263\n",
      "Epoch 2578, Loss: 1.1886503100395203, Final Batch Loss: 0.29143524169921875\n",
      "Epoch 2579, Loss: 1.2389066815376282, Final Batch Loss: 0.276474267244339\n",
      "Epoch 2580, Loss: 1.0645782351493835, Final Batch Loss: 0.25306642055511475\n",
      "Epoch 2581, Loss: 1.1073990762233734, Final Batch Loss: 0.2652328312397003\n",
      "Epoch 2582, Loss: 1.1415297240018845, Final Batch Loss: 0.29414087533950806\n",
      "Epoch 2583, Loss: 1.0301576405763626, Final Batch Loss: 0.2505768835544586\n",
      "Epoch 2584, Loss: 1.142021581530571, Final Batch Loss: 0.31084388494491577\n",
      "Epoch 2585, Loss: 1.0787160247564316, Final Batch Loss: 0.26365238428115845\n",
      "Epoch 2586, Loss: 1.151087999343872, Final Batch Loss: 0.3228536546230316\n",
      "Epoch 2587, Loss: 1.0215748250484467, Final Batch Loss: 0.26717737317085266\n",
      "Epoch 2588, Loss: 1.1348153352737427, Final Batch Loss: 0.2819488048553467\n",
      "Epoch 2589, Loss: 1.1149656027555466, Final Batch Loss: 0.24974152445793152\n",
      "Epoch 2590, Loss: 1.0253841578960419, Final Batch Loss: 0.27314019203186035\n",
      "Epoch 2591, Loss: 1.117367535829544, Final Batch Loss: 0.2659662067890167\n",
      "Epoch 2592, Loss: 1.137699156999588, Final Batch Loss: 0.3092690706253052\n",
      "Epoch 2593, Loss: 1.1435551643371582, Final Batch Loss: 0.34311193227767944\n",
      "Epoch 2594, Loss: 1.0323594212532043, Final Batch Loss: 0.24139875173568726\n",
      "Epoch 2595, Loss: 1.0921313166618347, Final Batch Loss: 0.22266274690628052\n",
      "Epoch 2596, Loss: 1.08370803296566, Final Batch Loss: 0.27039942145347595\n",
      "Epoch 2597, Loss: 1.149191290140152, Final Batch Loss: 0.3897010087966919\n",
      "Epoch 2598, Loss: 1.1900158822536469, Final Batch Loss: 0.3207662105560303\n",
      "Epoch 2599, Loss: 1.0637315213680267, Final Batch Loss: 0.2497398853302002\n",
      "Epoch 2600, Loss: 1.101820021867752, Final Batch Loss: 0.2070106863975525\n",
      "Epoch 2601, Loss: 1.1781332939863205, Final Batch Loss: 0.37218111753463745\n",
      "Epoch 2602, Loss: 1.1455693691968918, Final Batch Loss: 0.256827712059021\n",
      "Epoch 2603, Loss: 1.1472161263227463, Final Batch Loss: 0.3244088292121887\n",
      "Epoch 2604, Loss: 1.089080736041069, Final Batch Loss: 0.21964232623577118\n",
      "Epoch 2605, Loss: 1.1556779146194458, Final Batch Loss: 0.3053656220436096\n",
      "Epoch 2606, Loss: 1.1721572875976562, Final Batch Loss: 0.3286133110523224\n",
      "Epoch 2607, Loss: 1.1865860968828201, Final Batch Loss: 0.19473546743392944\n",
      "Epoch 2608, Loss: 1.1567703932523727, Final Batch Loss: 0.41053101420402527\n",
      "Epoch 2609, Loss: 1.154090628027916, Final Batch Loss: 0.248666450381279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2610, Loss: 1.1198244094848633, Final Batch Loss: 0.2345007210969925\n",
      "Epoch 2611, Loss: 1.0323213040828705, Final Batch Loss: 0.3201863169670105\n",
      "Epoch 2612, Loss: 1.1480428576469421, Final Batch Loss: 0.30661270022392273\n",
      "Epoch 2613, Loss: 1.037071481347084, Final Batch Loss: 0.3029414713382721\n",
      "Epoch 2614, Loss: 1.2039062082767487, Final Batch Loss: 0.333755761384964\n",
      "Epoch 2615, Loss: 1.0967591851949692, Final Batch Loss: 0.24657882750034332\n",
      "Epoch 2616, Loss: 1.0636736899614334, Final Batch Loss: 0.3277539014816284\n",
      "Epoch 2617, Loss: 1.190359205007553, Final Batch Loss: 0.31297770142555237\n",
      "Epoch 2618, Loss: 1.085606649518013, Final Batch Loss: 0.28437864780426025\n",
      "Epoch 2619, Loss: 1.114246353507042, Final Batch Loss: 0.29386377334594727\n",
      "Epoch 2620, Loss: 1.0986294597387314, Final Batch Loss: 0.2562512159347534\n",
      "Epoch 2621, Loss: 1.1249446272850037, Final Batch Loss: 0.25285249948501587\n",
      "Epoch 2622, Loss: 1.0145397931337357, Final Batch Loss: 0.25301775336265564\n",
      "Epoch 2623, Loss: 1.0190868973731995, Final Batch Loss: 0.282169371843338\n",
      "Epoch 2624, Loss: 1.1733446419239044, Final Batch Loss: 0.280823290348053\n",
      "Epoch 2625, Loss: 1.0663567930459976, Final Batch Loss: 0.2964910864830017\n",
      "Epoch 2626, Loss: 1.07501782476902, Final Batch Loss: 0.21413035690784454\n",
      "Epoch 2627, Loss: 1.216548502445221, Final Batch Loss: 0.3553467094898224\n",
      "Epoch 2628, Loss: 1.0376012176275253, Final Batch Loss: 0.2290918529033661\n",
      "Epoch 2629, Loss: 1.137031927704811, Final Batch Loss: 0.27611708641052246\n",
      "Epoch 2630, Loss: 1.2227738052606583, Final Batch Loss: 0.34446626901626587\n",
      "Epoch 2631, Loss: 1.0102828592061996, Final Batch Loss: 0.21463434398174286\n",
      "Epoch 2632, Loss: 1.1573183983564377, Final Batch Loss: 0.23690208792686462\n",
      "Epoch 2633, Loss: 1.0541101694107056, Final Batch Loss: 0.2475247085094452\n",
      "Epoch 2634, Loss: 1.1578488945960999, Final Batch Loss: 0.3081333041191101\n",
      "Epoch 2635, Loss: 1.0869100987911224, Final Batch Loss: 0.27916470170021057\n",
      "Epoch 2636, Loss: 1.2333670556545258, Final Batch Loss: 0.3435412645339966\n",
      "Epoch 2637, Loss: 1.1591316610574722, Final Batch Loss: 0.2893683612346649\n",
      "Epoch 2638, Loss: 0.9760625064373016, Final Batch Loss: 0.3335132300853729\n",
      "Epoch 2639, Loss: 1.1163841634988785, Final Batch Loss: 0.23783865571022034\n",
      "Epoch 2640, Loss: 1.1876279413700104, Final Batch Loss: 0.3336280286312103\n",
      "Epoch 2641, Loss: 1.193917766213417, Final Batch Loss: 0.3281526267528534\n",
      "Epoch 2642, Loss: 1.1420730948448181, Final Batch Loss: 0.3884468078613281\n",
      "Epoch 2643, Loss: 1.0324851423501968, Final Batch Loss: 0.32402539253234863\n",
      "Epoch 2644, Loss: 1.024454802274704, Final Batch Loss: 0.263528436422348\n",
      "Epoch 2645, Loss: 1.048829823732376, Final Batch Loss: 0.19306662678718567\n",
      "Epoch 2646, Loss: 1.0528351217508316, Final Batch Loss: 0.23596829175949097\n",
      "Epoch 2647, Loss: 1.1103782653808594, Final Batch Loss: 0.280284583568573\n",
      "Epoch 2648, Loss: 1.0001021176576614, Final Batch Loss: 0.21985016763210297\n",
      "Epoch 2649, Loss: 1.004268154501915, Final Batch Loss: 0.22881348431110382\n",
      "Epoch 2650, Loss: 1.0899535417556763, Final Batch Loss: 0.23431524634361267\n",
      "Epoch 2651, Loss: 1.0549097061157227, Final Batch Loss: 0.20752085745334625\n",
      "Epoch 2652, Loss: 1.199714556336403, Final Batch Loss: 0.19548843801021576\n",
      "Epoch 2653, Loss: 1.181274265050888, Final Batch Loss: 0.279898077249527\n",
      "Epoch 2654, Loss: 1.1153612285852432, Final Batch Loss: 0.30489256978034973\n",
      "Epoch 2655, Loss: 1.0536721497774124, Final Batch Loss: 0.23421354591846466\n",
      "Epoch 2656, Loss: 1.1107401251792908, Final Batch Loss: 0.29600414633750916\n",
      "Epoch 2657, Loss: 1.1344896256923676, Final Batch Loss: 0.20652416348457336\n",
      "Epoch 2658, Loss: 1.1678739190101624, Final Batch Loss: 0.29189252853393555\n",
      "Epoch 2659, Loss: 1.0592772364616394, Final Batch Loss: 0.290647029876709\n",
      "Epoch 2660, Loss: 1.0630951672792435, Final Batch Loss: 0.24573914706707\n",
      "Epoch 2661, Loss: 0.9981940239667892, Final Batch Loss: 0.2528240978717804\n",
      "Epoch 2662, Loss: 1.1009586155414581, Final Batch Loss: 0.25368213653564453\n",
      "Epoch 2663, Loss: 1.0510734915733337, Final Batch Loss: 0.355085551738739\n",
      "Epoch 2664, Loss: 1.2240299582481384, Final Batch Loss: 0.3039063811302185\n",
      "Epoch 2665, Loss: 1.1124361753463745, Final Batch Loss: 0.25819993019104004\n",
      "Epoch 2666, Loss: 1.0795440971851349, Final Batch Loss: 0.2556013762950897\n",
      "Epoch 2667, Loss: 1.1480507850646973, Final Batch Loss: 0.2837265431880951\n",
      "Epoch 2668, Loss: 1.191627562046051, Final Batch Loss: 0.3158699572086334\n",
      "Epoch 2669, Loss: 0.9895768761634827, Final Batch Loss: 0.17426389455795288\n",
      "Epoch 2670, Loss: 1.2211989760398865, Final Batch Loss: 0.323402464389801\n",
      "Epoch 2671, Loss: 1.1054006218910217, Final Batch Loss: 0.2879810333251953\n",
      "Epoch 2672, Loss: 1.2254279553890228, Final Batch Loss: 0.30376267433166504\n",
      "Epoch 2673, Loss: 1.056291088461876, Final Batch Loss: 0.3252580463886261\n",
      "Epoch 2674, Loss: 1.0343275368213654, Final Batch Loss: 0.2666809856891632\n",
      "Epoch 2675, Loss: 1.0222904831171036, Final Batch Loss: 0.2125280350446701\n",
      "Epoch 2676, Loss: 1.0559654235839844, Final Batch Loss: 0.20487192273139954\n",
      "Epoch 2677, Loss: 1.0584044009447098, Final Batch Loss: 0.2341291755437851\n",
      "Epoch 2678, Loss: 1.0710991472005844, Final Batch Loss: 0.22803612053394318\n",
      "Epoch 2679, Loss: 1.1349619776010513, Final Batch Loss: 0.32272911071777344\n",
      "Epoch 2680, Loss: 1.1165802329778671, Final Batch Loss: 0.24377284944057465\n",
      "Epoch 2681, Loss: 1.1753734350204468, Final Batch Loss: 0.42303383350372314\n",
      "Epoch 2682, Loss: 1.1573234498500824, Final Batch Loss: 0.2992170453071594\n",
      "Epoch 2683, Loss: 1.0410126745700836, Final Batch Loss: 0.24393601715564728\n",
      "Epoch 2684, Loss: 1.040442556142807, Final Batch Loss: 0.22068889439105988\n",
      "Epoch 2685, Loss: 1.2229463458061218, Final Batch Loss: 0.28595229983329773\n",
      "Epoch 2686, Loss: 1.032734289765358, Final Batch Loss: 0.25125443935394287\n",
      "Epoch 2687, Loss: 1.0681401640176773, Final Batch Loss: 0.24926051497459412\n",
      "Epoch 2688, Loss: 1.2309258878231049, Final Batch Loss: 0.30867770314216614\n",
      "Epoch 2689, Loss: 1.0527662634849548, Final Batch Loss: 0.27741894125938416\n",
      "Epoch 2690, Loss: 1.102205067873001, Final Batch Loss: 0.2514866888523102\n",
      "Epoch 2691, Loss: 1.020218551158905, Final Batch Loss: 0.19971178472042084\n",
      "Epoch 2692, Loss: 1.0833564549684525, Final Batch Loss: 0.3257134258747101\n",
      "Epoch 2693, Loss: 1.0406306982040405, Final Batch Loss: 0.25732794404029846\n",
      "Epoch 2694, Loss: 1.1910186558961868, Final Batch Loss: 0.3343031704425812\n",
      "Epoch 2695, Loss: 1.0823367834091187, Final Batch Loss: 0.27323803305625916\n",
      "Epoch 2696, Loss: 1.1105027198791504, Final Batch Loss: 0.3109504282474518\n",
      "Epoch 2697, Loss: 1.1219833195209503, Final Batch Loss: 0.32163214683532715\n",
      "Epoch 2698, Loss: 1.2591145634651184, Final Batch Loss: 0.3682529032230377\n",
      "Epoch 2699, Loss: 1.0093231052160263, Final Batch Loss: 0.2310541421175003\n",
      "Epoch 2700, Loss: 1.0928159952163696, Final Batch Loss: 0.2624797224998474\n",
      "Epoch 2701, Loss: 0.985031858086586, Final Batch Loss: 0.24128207564353943\n",
      "Epoch 2702, Loss: 1.2483308762311935, Final Batch Loss: 0.31049445271492004\n",
      "Epoch 2703, Loss: 1.0759406089782715, Final Batch Loss: 0.2007559835910797\n",
      "Epoch 2704, Loss: 1.0221508741378784, Final Batch Loss: 0.2299828976392746\n",
      "Epoch 2705, Loss: 1.0420745760202408, Final Batch Loss: 0.23507219552993774\n",
      "Epoch 2706, Loss: 1.0771951377391815, Final Batch Loss: 0.31014788150787354\n",
      "Epoch 2707, Loss: 0.9918799102306366, Final Batch Loss: 0.24461787939071655\n",
      "Epoch 2708, Loss: 0.9673557579517365, Final Batch Loss: 0.2676807940006256\n",
      "Epoch 2709, Loss: 1.0787602812051773, Final Batch Loss: 0.2979826033115387\n",
      "Epoch 2710, Loss: 1.1303951740264893, Final Batch Loss: 0.255487322807312\n",
      "Epoch 2711, Loss: 1.0800868570804596, Final Batch Loss: 0.2859552502632141\n",
      "Epoch 2712, Loss: 0.9902419894933701, Final Batch Loss: 0.2086409330368042\n",
      "Epoch 2713, Loss: 1.1561321318149567, Final Batch Loss: 0.3120653033256531\n",
      "Epoch 2714, Loss: 1.1728479266166687, Final Batch Loss: 0.33346661925315857\n",
      "Epoch 2715, Loss: 1.081093356013298, Final Batch Loss: 0.3349268138408661\n",
      "Epoch 2716, Loss: 1.1337431371212006, Final Batch Loss: 0.2621728181838989\n",
      "Epoch 2717, Loss: 1.1096501350402832, Final Batch Loss: 0.27045169472694397\n",
      "Epoch 2718, Loss: 1.0328302532434464, Final Batch Loss: 0.22695185244083405\n",
      "Epoch 2719, Loss: 1.0920506417751312, Final Batch Loss: 0.29363319277763367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2720, Loss: 1.0862256437540054, Final Batch Loss: 0.33198535442352295\n",
      "Epoch 2721, Loss: 1.0378934293985367, Final Batch Loss: 0.1946859508752823\n",
      "Epoch 2722, Loss: 1.0930920839309692, Final Batch Loss: 0.31222257018089294\n",
      "Epoch 2723, Loss: 1.088935598731041, Final Batch Loss: 0.3265211582183838\n",
      "Epoch 2724, Loss: 1.1374260187149048, Final Batch Loss: 0.32684609293937683\n",
      "Epoch 2725, Loss: 1.1727606058120728, Final Batch Loss: 0.32009100914001465\n",
      "Epoch 2726, Loss: 1.0673041492700577, Final Batch Loss: 0.22220848500728607\n",
      "Epoch 2727, Loss: 1.1004751473665237, Final Batch Loss: 0.22460098564624786\n",
      "Epoch 2728, Loss: 1.0783172696828842, Final Batch Loss: 0.2229158580303192\n",
      "Epoch 2729, Loss: 0.9977794736623764, Final Batch Loss: 0.19827698171138763\n",
      "Epoch 2730, Loss: 1.068456992506981, Final Batch Loss: 0.3451511859893799\n",
      "Epoch 2731, Loss: 1.1021191328763962, Final Batch Loss: 0.25588423013687134\n",
      "Epoch 2732, Loss: 1.1556817293167114, Final Batch Loss: 0.220915287733078\n",
      "Epoch 2733, Loss: 1.1216787993907928, Final Batch Loss: 0.2834421396255493\n",
      "Epoch 2734, Loss: 1.0757483392953873, Final Batch Loss: 0.2685815095901489\n",
      "Epoch 2735, Loss: 1.1439911425113678, Final Batch Loss: 0.2892003655433655\n",
      "Epoch 2736, Loss: 1.0144348591566086, Final Batch Loss: 0.22378259897232056\n",
      "Epoch 2737, Loss: 1.095022663474083, Final Batch Loss: 0.23118956387043\n",
      "Epoch 2738, Loss: 0.9534049183130264, Final Batch Loss: 0.226846843957901\n",
      "Epoch 2739, Loss: 1.1591183841228485, Final Batch Loss: 0.34186387062072754\n",
      "Epoch 2740, Loss: 1.1086816936731339, Final Batch Loss: 0.3124258518218994\n",
      "Epoch 2741, Loss: 1.0641670525074005, Final Batch Loss: 0.2248682677745819\n",
      "Epoch 2742, Loss: 1.049155592918396, Final Batch Loss: 0.2704343795776367\n",
      "Epoch 2743, Loss: 1.1509155631065369, Final Batch Loss: 0.2639874815940857\n",
      "Epoch 2744, Loss: 1.1877068281173706, Final Batch Loss: 0.31440848112106323\n",
      "Epoch 2745, Loss: 1.083874523639679, Final Batch Loss: 0.2913857400417328\n",
      "Epoch 2746, Loss: 0.9686329662799835, Final Batch Loss: 0.23199288547039032\n",
      "Epoch 2747, Loss: 1.2453683614730835, Final Batch Loss: 0.35252276062965393\n",
      "Epoch 2748, Loss: 1.2049691379070282, Final Batch Loss: 0.2995080053806305\n",
      "Epoch 2749, Loss: 1.0729458928108215, Final Batch Loss: 0.19215089082717896\n",
      "Epoch 2750, Loss: 1.1077910661697388, Final Batch Loss: 0.2257784903049469\n",
      "Epoch 2751, Loss: 1.0682256370782852, Final Batch Loss: 0.26146912574768066\n",
      "Epoch 2752, Loss: 1.053782194852829, Final Batch Loss: 0.30712074041366577\n",
      "Epoch 2753, Loss: 1.1377333402633667, Final Batch Loss: 0.2616475820541382\n",
      "Epoch 2754, Loss: 1.066318690776825, Final Batch Loss: 0.24652093648910522\n",
      "Epoch 2755, Loss: 1.056379109621048, Final Batch Loss: 0.2572615146636963\n",
      "Epoch 2756, Loss: 1.0581188201904297, Final Batch Loss: 0.30761587619781494\n",
      "Epoch 2757, Loss: 1.061729073524475, Final Batch Loss: 0.24783898890018463\n",
      "Epoch 2758, Loss: 1.2532964795827866, Final Batch Loss: 0.23152200877666473\n",
      "Epoch 2759, Loss: 1.0760875791311264, Final Batch Loss: 0.24887631833553314\n",
      "Epoch 2760, Loss: 1.1421185731887817, Final Batch Loss: 0.2468014359474182\n",
      "Epoch 2761, Loss: 1.076244130730629, Final Batch Loss: 0.25053876638412476\n",
      "Epoch 2762, Loss: 1.1132881194353104, Final Batch Loss: 0.2744600176811218\n",
      "Epoch 2763, Loss: 1.15647953748703, Final Batch Loss: 0.2311544120311737\n",
      "Epoch 2764, Loss: 1.0934545248746872, Final Batch Loss: 0.24077962338924408\n",
      "Epoch 2765, Loss: 1.0227127522230148, Final Batch Loss: 0.22729699313640594\n",
      "Epoch 2766, Loss: 1.09984989464283, Final Batch Loss: 0.2888851463794708\n",
      "Epoch 2767, Loss: 0.9026879072189331, Final Batch Loss: 0.2173713594675064\n",
      "Epoch 2768, Loss: 1.1242725253105164, Final Batch Loss: 0.33402666449546814\n",
      "Epoch 2769, Loss: 1.0949086546897888, Final Batch Loss: 0.33395707607269287\n",
      "Epoch 2770, Loss: 1.0779779702425003, Final Batch Loss: 0.20310480892658234\n",
      "Epoch 2771, Loss: 1.150961846113205, Final Batch Loss: 0.2455427348613739\n",
      "Epoch 2772, Loss: 1.2527363300323486, Final Batch Loss: 0.27654406428337097\n",
      "Epoch 2773, Loss: 1.2053525149822235, Final Batch Loss: 0.2163497805595398\n",
      "Epoch 2774, Loss: 1.0170884728431702, Final Batch Loss: 0.20805229246616364\n",
      "Epoch 2775, Loss: 1.0579632371664047, Final Batch Loss: 0.2239442616701126\n",
      "Epoch 2776, Loss: 1.0207935720682144, Final Batch Loss: 0.19489608705043793\n",
      "Epoch 2777, Loss: 1.123543992638588, Final Batch Loss: 0.23291365802288055\n",
      "Epoch 2778, Loss: 0.9982856810092926, Final Batch Loss: 0.27375781536102295\n",
      "Epoch 2779, Loss: 1.0710114538669586, Final Batch Loss: 0.27857711911201477\n",
      "Epoch 2780, Loss: 1.1158468127250671, Final Batch Loss: 0.32156312465667725\n",
      "Epoch 2781, Loss: 1.1540842652320862, Final Batch Loss: 0.3183648884296417\n",
      "Epoch 2782, Loss: 1.130504846572876, Final Batch Loss: 0.22203785181045532\n",
      "Epoch 2783, Loss: 1.152564823627472, Final Batch Loss: 0.2813253700733185\n",
      "Epoch 2784, Loss: 1.1082327365875244, Final Batch Loss: 0.21205168962478638\n",
      "Epoch 2785, Loss: 1.0644304156303406, Final Batch Loss: 0.2657497227191925\n",
      "Epoch 2786, Loss: 0.9994706809520721, Final Batch Loss: 0.24876560270786285\n",
      "Epoch 2787, Loss: 1.1087633222341537, Final Batch Loss: 0.27942031621932983\n",
      "Epoch 2788, Loss: 0.9857983887195587, Final Batch Loss: 0.2672712802886963\n",
      "Epoch 2789, Loss: 1.0792152136564255, Final Batch Loss: 0.2850770950317383\n",
      "Epoch 2790, Loss: 1.1186621338129044, Final Batch Loss: 0.3346576392650604\n",
      "Epoch 2791, Loss: 1.118655502796173, Final Batch Loss: 0.3681023418903351\n",
      "Epoch 2792, Loss: 0.9836780726909637, Final Batch Loss: 0.24360240995883942\n",
      "Epoch 2793, Loss: 1.1020229756832123, Final Batch Loss: 0.29327821731567383\n",
      "Epoch 2794, Loss: 1.1385880261659622, Final Batch Loss: 0.39220064878463745\n",
      "Epoch 2795, Loss: 1.073394626379013, Final Batch Loss: 0.2687338590621948\n",
      "Epoch 2796, Loss: 0.9721063673496246, Final Batch Loss: 0.24767915904521942\n",
      "Epoch 2797, Loss: 1.0125574171543121, Final Batch Loss: 0.2546132802963257\n",
      "Epoch 2798, Loss: 1.1856197118759155, Final Batch Loss: 0.3232100307941437\n",
      "Epoch 2799, Loss: 1.025357499718666, Final Batch Loss: 0.2682526111602783\n",
      "Epoch 2800, Loss: 1.0878161787986755, Final Batch Loss: 0.34053367376327515\n",
      "Epoch 2801, Loss: 0.9251460134983063, Final Batch Loss: 0.22079366445541382\n",
      "Epoch 2802, Loss: 1.0579052716493607, Final Batch Loss: 0.2975265085697174\n",
      "Epoch 2803, Loss: 1.0366098135709763, Final Batch Loss: 0.2983216643333435\n",
      "Epoch 2804, Loss: 1.0381147861480713, Final Batch Loss: 0.2722145915031433\n",
      "Epoch 2805, Loss: 1.0390373766422272, Final Batch Loss: 0.29047954082489014\n",
      "Epoch 2806, Loss: 1.0295373350381851, Final Batch Loss: 0.29736870527267456\n",
      "Epoch 2807, Loss: 1.112938329577446, Final Batch Loss: 0.21497248113155365\n",
      "Epoch 2808, Loss: 1.186960518360138, Final Batch Loss: 0.29859432578086853\n",
      "Epoch 2809, Loss: 0.8666810393333435, Final Batch Loss: 0.19898571074008942\n",
      "Epoch 2810, Loss: 1.1621738374233246, Final Batch Loss: 0.3054884076118469\n",
      "Epoch 2811, Loss: 1.0334857404232025, Final Batch Loss: 0.2243216335773468\n",
      "Epoch 2812, Loss: 1.0661426037549973, Final Batch Loss: 0.27749454975128174\n",
      "Epoch 2813, Loss: 1.0454799383878708, Final Batch Loss: 0.3053346574306488\n",
      "Epoch 2814, Loss: 1.0066928416490555, Final Batch Loss: 0.25059834122657776\n",
      "Epoch 2815, Loss: 1.16183602809906, Final Batch Loss: 0.30809617042541504\n",
      "Epoch 2816, Loss: 1.0879967361688614, Final Batch Loss: 0.25882667303085327\n",
      "Epoch 2817, Loss: 1.136548012495041, Final Batch Loss: 0.32684263586997986\n",
      "Epoch 2818, Loss: 0.9399087876081467, Final Batch Loss: 0.2195315659046173\n",
      "Epoch 2819, Loss: 1.0889820456504822, Final Batch Loss: 0.29494139552116394\n",
      "Epoch 2820, Loss: 1.0765048116445541, Final Batch Loss: 0.20075103640556335\n",
      "Epoch 2821, Loss: 1.0609188228845596, Final Batch Loss: 0.32708320021629333\n",
      "Epoch 2822, Loss: 1.0913803577423096, Final Batch Loss: 0.3734140992164612\n",
      "Epoch 2823, Loss: 1.129975974559784, Final Batch Loss: 0.2914299964904785\n",
      "Epoch 2824, Loss: 1.0926032662391663, Final Batch Loss: 0.28324294090270996\n",
      "Epoch 2825, Loss: 1.0863058269023895, Final Batch Loss: 0.29616808891296387\n",
      "Epoch 2826, Loss: 1.1442558616399765, Final Batch Loss: 0.3316303789615631\n",
      "Epoch 2827, Loss: 1.1815220713615417, Final Batch Loss: 0.2930900454521179\n",
      "Epoch 2828, Loss: 1.1531801670789719, Final Batch Loss: 0.3006434440612793\n",
      "Epoch 2829, Loss: 1.105518400669098, Final Batch Loss: 0.23984232544898987\n",
      "Epoch 2830, Loss: 1.0945501029491425, Final Batch Loss: 0.2570692300796509\n",
      "Epoch 2831, Loss: 1.045283392071724, Final Batch Loss: 0.29252129793167114\n",
      "Epoch 2832, Loss: 1.1000007838010788, Final Batch Loss: 0.3487682640552521\n",
      "Epoch 2833, Loss: 1.10118767619133, Final Batch Loss: 0.2578698396682739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2834, Loss: 1.1104016304016113, Final Batch Loss: 0.22841593623161316\n",
      "Epoch 2835, Loss: 1.0398509353399277, Final Batch Loss: 0.2020830661058426\n",
      "Epoch 2836, Loss: 1.0204043090343475, Final Batch Loss: 0.23683907091617584\n",
      "Epoch 2837, Loss: 1.1883525103330612, Final Batch Loss: 0.32108208537101746\n",
      "Epoch 2838, Loss: 1.048728123307228, Final Batch Loss: 0.24990972876548767\n",
      "Epoch 2839, Loss: 1.1294454336166382, Final Batch Loss: 0.30077263712882996\n",
      "Epoch 2840, Loss: 1.042557343840599, Final Batch Loss: 0.2345765233039856\n",
      "Epoch 2841, Loss: 1.1065084487199783, Final Batch Loss: 0.3433876633644104\n",
      "Epoch 2842, Loss: 0.9870615750551224, Final Batch Loss: 0.23243503272533417\n",
      "Epoch 2843, Loss: 1.0767133682966232, Final Batch Loss: 0.29089847207069397\n",
      "Epoch 2844, Loss: 1.1470687538385391, Final Batch Loss: 0.3473820388317108\n",
      "Epoch 2845, Loss: 1.0212861746549606, Final Batch Loss: 0.2279675304889679\n",
      "Epoch 2846, Loss: 1.1724920570850372, Final Batch Loss: 0.28116172552108765\n",
      "Epoch 2847, Loss: 1.0134608149528503, Final Batch Loss: 0.2991281747817993\n",
      "Epoch 2848, Loss: 1.0354530066251755, Final Batch Loss: 0.2895708680152893\n",
      "Epoch 2849, Loss: 1.0741415172815323, Final Batch Loss: 0.2533775269985199\n",
      "Epoch 2850, Loss: 1.0335720032453537, Final Batch Loss: 0.19795730710029602\n",
      "Epoch 2851, Loss: 1.1275024712085724, Final Batch Loss: 0.239732027053833\n",
      "Epoch 2852, Loss: 1.1176232248544693, Final Batch Loss: 0.25459009408950806\n",
      "Epoch 2853, Loss: 0.9154030829668045, Final Batch Loss: 0.20374689996242523\n",
      "Epoch 2854, Loss: 0.9960139095783234, Final Batch Loss: 0.32469049096107483\n",
      "Epoch 2855, Loss: 1.0121714174747467, Final Batch Loss: 0.29848286509513855\n",
      "Epoch 2856, Loss: 1.0847661942243576, Final Batch Loss: 0.264655739068985\n",
      "Epoch 2857, Loss: 1.3038048446178436, Final Batch Loss: 0.3548092544078827\n",
      "Epoch 2858, Loss: 1.1278822720050812, Final Batch Loss: 0.23609784245491028\n",
      "Epoch 2859, Loss: 1.0633472949266434, Final Batch Loss: 0.21177203953266144\n",
      "Epoch 2860, Loss: 1.0397540777921677, Final Batch Loss: 0.23763403296470642\n",
      "Epoch 2861, Loss: 1.0509391278028488, Final Batch Loss: 0.25227609276771545\n",
      "Epoch 2862, Loss: 1.0310972929000854, Final Batch Loss: 0.2397666573524475\n",
      "Epoch 2863, Loss: 1.1286397874355316, Final Batch Loss: 0.28024888038635254\n",
      "Epoch 2864, Loss: 1.079807922244072, Final Batch Loss: 0.26143407821655273\n",
      "Epoch 2865, Loss: 1.0437305271625519, Final Batch Loss: 0.2668967545032501\n",
      "Epoch 2866, Loss: 1.0078959912061691, Final Batch Loss: 0.23095248639583588\n",
      "Epoch 2867, Loss: 1.1687441170215607, Final Batch Loss: 0.29387935996055603\n",
      "Epoch 2868, Loss: 1.034298986196518, Final Batch Loss: 0.30200669169425964\n",
      "Epoch 2869, Loss: 1.0580562204122543, Final Batch Loss: 0.2767626643180847\n",
      "Epoch 2870, Loss: 1.0425208508968353, Final Batch Loss: 0.2616307735443115\n",
      "Epoch 2871, Loss: 1.041164070367813, Final Batch Loss: 0.19413292407989502\n",
      "Epoch 2872, Loss: 1.067324385046959, Final Batch Loss: 0.30252984166145325\n",
      "Epoch 2873, Loss: 1.1502745896577835, Final Batch Loss: 0.2911175787448883\n",
      "Epoch 2874, Loss: 1.1302690356969833, Final Batch Loss: 0.3746131360530853\n",
      "Epoch 2875, Loss: 1.1472265869379044, Final Batch Loss: 0.23472830653190613\n",
      "Epoch 2876, Loss: 0.886507585644722, Final Batch Loss: 0.1910935342311859\n",
      "Epoch 2877, Loss: 1.1166668385267258, Final Batch Loss: 0.34236177802085876\n",
      "Epoch 2878, Loss: 0.948261097073555, Final Batch Loss: 0.18598179519176483\n",
      "Epoch 2879, Loss: 1.0725085884332657, Final Batch Loss: 0.25504329800605774\n",
      "Epoch 2880, Loss: 0.9968327134847641, Final Batch Loss: 0.18373282253742218\n",
      "Epoch 2881, Loss: 1.1455472111701965, Final Batch Loss: 0.36612075567245483\n",
      "Epoch 2882, Loss: 1.0795428603887558, Final Batch Loss: 0.2605358362197876\n",
      "Epoch 2883, Loss: 1.1083244532346725, Final Batch Loss: 0.24532003700733185\n",
      "Epoch 2884, Loss: 1.1107519567012787, Final Batch Loss: 0.2623404562473297\n",
      "Epoch 2885, Loss: 1.0727635622024536, Final Batch Loss: 0.2121780961751938\n",
      "Epoch 2886, Loss: 1.0468022525310516, Final Batch Loss: 0.26396289467811584\n",
      "Epoch 2887, Loss: 1.1301539987325668, Final Batch Loss: 0.24535778164863586\n",
      "Epoch 2888, Loss: 1.0799065083265305, Final Batch Loss: 0.28929901123046875\n",
      "Epoch 2889, Loss: 1.0076769888401031, Final Batch Loss: 0.25460517406463623\n",
      "Epoch 2890, Loss: 0.9549759328365326, Final Batch Loss: 0.22275547683238983\n",
      "Epoch 2891, Loss: 1.2184350788593292, Final Batch Loss: 0.3254479467868805\n",
      "Epoch 2892, Loss: 1.119532734155655, Final Batch Loss: 0.24418175220489502\n",
      "Epoch 2893, Loss: 1.0477007627487183, Final Batch Loss: 0.28333669900894165\n",
      "Epoch 2894, Loss: 1.0366858690977097, Final Batch Loss: 0.268023818731308\n",
      "Epoch 2895, Loss: 1.146245315670967, Final Batch Loss: 0.34493303298950195\n",
      "Epoch 2896, Loss: 1.0701811611652374, Final Batch Loss: 0.26204735040664673\n",
      "Epoch 2897, Loss: 0.9795085340738297, Final Batch Loss: 0.23109427094459534\n",
      "Epoch 2898, Loss: 1.0627268850803375, Final Batch Loss: 0.30915117263793945\n",
      "Epoch 2899, Loss: 1.0569379329681396, Final Batch Loss: 0.25835973024368286\n",
      "Epoch 2900, Loss: 1.167228639125824, Final Batch Loss: 0.27477020025253296\n",
      "Epoch 2901, Loss: 1.0048131942749023, Final Batch Loss: 0.2657245099544525\n",
      "Epoch 2902, Loss: 1.0929124802350998, Final Batch Loss: 0.2008260190486908\n",
      "Epoch 2903, Loss: 1.1703501045703888, Final Batch Loss: 0.29657843708992004\n",
      "Epoch 2904, Loss: 1.0099167376756668, Final Batch Loss: 0.17795516550540924\n",
      "Epoch 2905, Loss: 1.0667439997196198, Final Batch Loss: 0.23910121619701385\n",
      "Epoch 2906, Loss: 1.0496953129768372, Final Batch Loss: 0.2466125637292862\n",
      "Epoch 2907, Loss: 1.082052007317543, Final Batch Loss: 0.24811775982379913\n",
      "Epoch 2908, Loss: 1.0384922623634338, Final Batch Loss: 0.24544721841812134\n",
      "Epoch 2909, Loss: 1.0293091088533401, Final Batch Loss: 0.29347309470176697\n",
      "Epoch 2910, Loss: 1.0203555673360825, Final Batch Loss: 0.27373063564300537\n",
      "Epoch 2911, Loss: 1.0771478414535522, Final Batch Loss: 0.25087666511535645\n",
      "Epoch 2912, Loss: 1.1424610018730164, Final Batch Loss: 0.319644957780838\n",
      "Epoch 2913, Loss: 0.9978278428316116, Final Batch Loss: 0.26071757078170776\n",
      "Epoch 2914, Loss: 1.0805828869342804, Final Batch Loss: 0.19446364045143127\n",
      "Epoch 2915, Loss: 1.0646645724773407, Final Batch Loss: 0.24497424066066742\n",
      "Epoch 2916, Loss: 1.0966721177101135, Final Batch Loss: 0.2264348715543747\n",
      "Epoch 2917, Loss: 1.0342402905225754, Final Batch Loss: 0.26587000489234924\n",
      "Epoch 2918, Loss: 1.1137923002243042, Final Batch Loss: 0.2546720802783966\n",
      "Epoch 2919, Loss: 1.1391301602125168, Final Batch Loss: 0.39896777272224426\n",
      "Epoch 2920, Loss: 1.0615432411432266, Final Batch Loss: 0.1731225699186325\n",
      "Epoch 2921, Loss: 1.0317659676074982, Final Batch Loss: 0.2657415270805359\n",
      "Epoch 2922, Loss: 1.0015979707241058, Final Batch Loss: 0.23593904078006744\n",
      "Epoch 2923, Loss: 1.1425992250442505, Final Batch Loss: 0.278548002243042\n",
      "Epoch 2924, Loss: 1.0492932051420212, Final Batch Loss: 0.22585101425647736\n",
      "Epoch 2925, Loss: 1.1104118824005127, Final Batch Loss: 0.2440887689590454\n",
      "Epoch 2926, Loss: 1.0418157428503036, Final Batch Loss: 0.22593489289283752\n",
      "Epoch 2927, Loss: 1.1008062958717346, Final Batch Loss: 0.2289334535598755\n",
      "Epoch 2928, Loss: 1.038072869181633, Final Batch Loss: 0.2448706179857254\n",
      "Epoch 2929, Loss: 1.1125451922416687, Final Batch Loss: 0.267344206571579\n",
      "Epoch 2930, Loss: 1.177937537431717, Final Batch Loss: 0.30754899978637695\n",
      "Epoch 2931, Loss: 1.1349626928567886, Final Batch Loss: 0.3627539873123169\n",
      "Epoch 2932, Loss: 1.0828571617603302, Final Batch Loss: 0.3128763735294342\n",
      "Epoch 2933, Loss: 1.0630961954593658, Final Batch Loss: 0.2733454406261444\n",
      "Epoch 2934, Loss: 1.1122235804796219, Final Batch Loss: 0.20999754965305328\n",
      "Epoch 2935, Loss: 1.0307406932115555, Final Batch Loss: 0.2227475792169571\n",
      "Epoch 2936, Loss: 1.2140346318483353, Final Batch Loss: 0.3466150164604187\n",
      "Epoch 2937, Loss: 1.1712476909160614, Final Batch Loss: 0.28946375846862793\n",
      "Epoch 2938, Loss: 1.1101660877466202, Final Batch Loss: 0.3476211130619049\n",
      "Epoch 2939, Loss: 1.1509953439235687, Final Batch Loss: 0.30000564455986023\n",
      "Epoch 2940, Loss: 1.0876532793045044, Final Batch Loss: 0.2546726167201996\n",
      "Epoch 2941, Loss: 1.010964810848236, Final Batch Loss: 0.2522983253002167\n",
      "Epoch 2942, Loss: 0.9783514738082886, Final Batch Loss: 0.2110740691423416\n",
      "Epoch 2943, Loss: 1.0484601557254791, Final Batch Loss: 0.2434210479259491\n",
      "Epoch 2944, Loss: 1.095094308257103, Final Batch Loss: 0.2682775855064392\n",
      "Epoch 2945, Loss: 1.0743380635976791, Final Batch Loss: 0.33233383297920227\n",
      "Epoch 2946, Loss: 1.025409147143364, Final Batch Loss: 0.17592038214206696\n",
      "Epoch 2947, Loss: 1.0411871373653412, Final Batch Loss: 0.2551247775554657\n",
      "Epoch 2948, Loss: 1.096797376871109, Final Batch Loss: 0.2744007706642151\n",
      "Epoch 2949, Loss: 1.0338916331529617, Final Batch Loss: 0.17897966504096985\n",
      "Epoch 2950, Loss: 1.1498744785785675, Final Batch Loss: 0.2895777225494385\n",
      "Epoch 2951, Loss: 1.0285350680351257, Final Batch Loss: 0.2528136372566223\n",
      "Epoch 2952, Loss: 1.153325840830803, Final Batch Loss: 0.3200893998146057\n",
      "Epoch 2953, Loss: 1.020206093788147, Final Batch Loss: 0.2801361680030823\n",
      "Epoch 2954, Loss: 0.9907106012105942, Final Batch Loss: 0.21985842287540436\n",
      "Epoch 2955, Loss: 0.9671428799629211, Final Batch Loss: 0.2590668201446533\n",
      "Epoch 2956, Loss: 1.1261014491319656, Final Batch Loss: 0.22229959070682526\n",
      "Epoch 2957, Loss: 1.0065998882055283, Final Batch Loss: 0.2614508867263794\n",
      "Epoch 2958, Loss: 1.0744698345661163, Final Batch Loss: 0.2315259426832199\n",
      "Epoch 2959, Loss: 0.9713714271783829, Final Batch Loss: 0.26143699884414673\n",
      "Epoch 2960, Loss: 1.0151570439338684, Final Batch Loss: 0.2755424380302429\n",
      "Epoch 2961, Loss: 0.9229903966188431, Final Batch Loss: 0.20558057725429535\n",
      "Epoch 2962, Loss: 1.0267965644598007, Final Batch Loss: 0.308386892080307\n",
      "Epoch 2963, Loss: 1.007486715912819, Final Batch Loss: 0.17240463197231293\n",
      "Epoch 2964, Loss: 0.9488192051649094, Final Batch Loss: 0.250332772731781\n",
      "Epoch 2965, Loss: 1.001550778746605, Final Batch Loss: 0.2520151138305664\n",
      "Epoch 2966, Loss: 0.9553131312131882, Final Batch Loss: 0.2496870458126068\n",
      "Epoch 2967, Loss: 1.0361666530370712, Final Batch Loss: 0.27934327721595764\n",
      "Epoch 2968, Loss: 0.9708458632230759, Final Batch Loss: 0.20560692250728607\n",
      "Epoch 2969, Loss: 1.0749127119779587, Final Batch Loss: 0.3082521855831146\n",
      "Epoch 2970, Loss: 0.9441173374652863, Final Batch Loss: 0.2502167522907257\n",
      "Epoch 2971, Loss: 0.9728054851293564, Final Batch Loss: 0.25291359424591064\n",
      "Epoch 2972, Loss: 1.011772483587265, Final Batch Loss: 0.26526567339897156\n",
      "Epoch 2973, Loss: 0.9536249488592148, Final Batch Loss: 0.22970350086688995\n",
      "Epoch 2974, Loss: 1.0353318452835083, Final Batch Loss: 0.2792980670928955\n",
      "Epoch 2975, Loss: 1.0280324220657349, Final Batch Loss: 0.23716546595096588\n",
      "Epoch 2976, Loss: 1.1115412265062332, Final Batch Loss: 0.2972443997859955\n",
      "Epoch 2977, Loss: 1.1026534736156464, Final Batch Loss: 0.2925336956977844\n",
      "Epoch 2978, Loss: 1.0969621390104294, Final Batch Loss: 0.22619350254535675\n",
      "Epoch 2979, Loss: 1.0461596697568893, Final Batch Loss: 0.30037596821784973\n",
      "Epoch 2980, Loss: 1.0537744164466858, Final Batch Loss: 0.2647588849067688\n",
      "Epoch 2981, Loss: 1.0164455473423004, Final Batch Loss: 0.23469558358192444\n",
      "Epoch 2982, Loss: 0.9280453473329544, Final Batch Loss: 0.18283291161060333\n",
      "Epoch 2983, Loss: 0.8584848195314407, Final Batch Loss: 0.173664852976799\n",
      "Epoch 2984, Loss: 1.0010223388671875, Final Batch Loss: 0.27497613430023193\n",
      "Epoch 2985, Loss: 0.8905802667140961, Final Batch Loss: 0.20516391098499298\n",
      "Epoch 2986, Loss: 1.0981137156486511, Final Batch Loss: 0.3033292591571808\n",
      "Epoch 2987, Loss: 0.9969162493944168, Final Batch Loss: 0.24581168591976166\n",
      "Epoch 2988, Loss: 1.0236116647720337, Final Batch Loss: 0.288430392742157\n",
      "Epoch 2989, Loss: 0.9523097723722458, Final Batch Loss: 0.25400015711784363\n",
      "Epoch 2990, Loss: 1.011492908000946, Final Batch Loss: 0.13766968250274658\n",
      "Epoch 2991, Loss: 0.9816292524337769, Final Batch Loss: 0.226796954870224\n",
      "Epoch 2992, Loss: 0.9680581837892532, Final Batch Loss: 0.2048134207725525\n",
      "Epoch 2993, Loss: 1.1159689128398895, Final Batch Loss: 0.20778296887874603\n",
      "Epoch 2994, Loss: 1.057775154709816, Final Batch Loss: 0.33397307991981506\n",
      "Epoch 2995, Loss: 0.9283434897661209, Final Batch Loss: 0.21013908088207245\n",
      "Epoch 2996, Loss: 1.1244942843914032, Final Batch Loss: 0.2401905655860901\n",
      "Epoch 2997, Loss: 1.0852992981672287, Final Batch Loss: 0.3118908107280731\n",
      "Epoch 2998, Loss: 1.1896028220653534, Final Batch Loss: 0.3325137197971344\n",
      "Epoch 2999, Loss: 1.1553112864494324, Final Batch Loss: 0.28807532787323\n",
      "Epoch 3000, Loss: 1.074586659669876, Final Batch Loss: 0.2683926820755005\n",
      "Epoch 3001, Loss: 0.9930000901222229, Final Batch Loss: 0.1992982029914856\n",
      "Epoch 3002, Loss: 1.0354973375797272, Final Batch Loss: 0.2577098309993744\n",
      "Epoch 3003, Loss: 0.9982555210590363, Final Batch Loss: 0.18572896718978882\n",
      "Epoch 3004, Loss: 1.0416487157344818, Final Batch Loss: 0.27596017718315125\n",
      "Epoch 3005, Loss: 1.0668466240167618, Final Batch Loss: 0.2784583270549774\n",
      "Epoch 3006, Loss: 1.104162409901619, Final Batch Loss: 0.2921566665172577\n",
      "Epoch 3007, Loss: 0.936689555644989, Final Batch Loss: 0.16938330233097076\n",
      "Epoch 3008, Loss: 0.8929395824670792, Final Batch Loss: 0.19331111013889313\n",
      "Epoch 3009, Loss: 1.2097698748111725, Final Batch Loss: 0.30458739399909973\n",
      "Epoch 3010, Loss: 0.9892662167549133, Final Batch Loss: 0.23648566007614136\n",
      "Epoch 3011, Loss: 0.9585574716329575, Final Batch Loss: 0.2386978268623352\n",
      "Epoch 3012, Loss: 1.0578497648239136, Final Batch Loss: 0.2849399447441101\n",
      "Epoch 3013, Loss: 0.9488689303398132, Final Batch Loss: 0.2335158735513687\n",
      "Epoch 3014, Loss: 0.9853411465883255, Final Batch Loss: 0.27199143171310425\n",
      "Epoch 3015, Loss: 1.0961569249629974, Final Batch Loss: 0.2264041304588318\n",
      "Epoch 3016, Loss: 1.071657970547676, Final Batch Loss: 0.27880239486694336\n",
      "Epoch 3017, Loss: 0.9744858592748642, Final Batch Loss: 0.2095249891281128\n",
      "Epoch 3018, Loss: 1.0272558629512787, Final Batch Loss: 0.26458436250686646\n",
      "Epoch 3019, Loss: 1.0212095528841019, Final Batch Loss: 0.29395022988319397\n",
      "Epoch 3020, Loss: 0.9609096795320511, Final Batch Loss: 0.21699504554271698\n",
      "Epoch 3021, Loss: 0.9729452431201935, Final Batch Loss: 0.23757915198802948\n",
      "Epoch 3022, Loss: 0.9656453132629395, Final Batch Loss: 0.2557605803012848\n",
      "Epoch 3023, Loss: 1.1831544041633606, Final Batch Loss: 0.3230762779712677\n",
      "Epoch 3024, Loss: 1.148526817560196, Final Batch Loss: 0.2906459867954254\n",
      "Epoch 3025, Loss: 1.0272443294525146, Final Batch Loss: 0.2507666349411011\n",
      "Epoch 3026, Loss: 0.9023200124502182, Final Batch Loss: 0.20949269831180573\n",
      "Epoch 3027, Loss: 1.0000344663858414, Final Batch Loss: 0.17332306504249573\n",
      "Epoch 3028, Loss: 0.8978167623281479, Final Batch Loss: 0.2612811028957367\n",
      "Epoch 3029, Loss: 1.0693342983722687, Final Batch Loss: 0.2581169009208679\n",
      "Epoch 3030, Loss: 0.9628586173057556, Final Batch Loss: 0.24567660689353943\n",
      "Epoch 3031, Loss: 1.0929266661405563, Final Batch Loss: 0.28067606687545776\n",
      "Epoch 3032, Loss: 0.9188992232084274, Final Batch Loss: 0.16876204311847687\n",
      "Epoch 3033, Loss: 1.0286131352186203, Final Batch Loss: 0.29810789227485657\n",
      "Epoch 3034, Loss: 1.0665677040815353, Final Batch Loss: 0.24570469558238983\n",
      "Epoch 3035, Loss: 1.1629558205604553, Final Batch Loss: 0.27652010321617126\n",
      "Epoch 3036, Loss: 0.9763510227203369, Final Batch Loss: 0.2332085371017456\n",
      "Epoch 3037, Loss: 1.1168479323387146, Final Batch Loss: 0.27335959672927856\n",
      "Epoch 3038, Loss: 1.0091777443885803, Final Batch Loss: 0.21189410984516144\n",
      "Epoch 3039, Loss: 1.102844163775444, Final Batch Loss: 0.2791692018508911\n",
      "Epoch 3040, Loss: 1.0831967741250992, Final Batch Loss: 0.26567044854164124\n",
      "Epoch 3041, Loss: 1.0004432052373886, Final Batch Loss: 0.2339116781949997\n",
      "Epoch 3042, Loss: 1.0084052085876465, Final Batch Loss: 0.22752469778060913\n",
      "Epoch 3043, Loss: 1.0905609875917435, Final Batch Loss: 0.2285545915365219\n",
      "Epoch 3044, Loss: 1.0922531336545944, Final Batch Loss: 0.29561474919319153\n",
      "Epoch 3045, Loss: 0.9783808141946793, Final Batch Loss: 0.22371986508369446\n",
      "Epoch 3046, Loss: 1.0546434074640274, Final Batch Loss: 0.23266814649105072\n",
      "Epoch 3047, Loss: 0.955496221780777, Final Batch Loss: 0.28826603293418884\n",
      "Epoch 3048, Loss: 1.074699804186821, Final Batch Loss: 0.32311126589775085\n",
      "Epoch 3049, Loss: 0.9534264206886292, Final Batch Loss: 0.2728976905345917\n",
      "Epoch 3050, Loss: 1.01810784637928, Final Batch Loss: 0.2920665144920349\n",
      "Epoch 3051, Loss: 1.0312367528676987, Final Batch Loss: 0.21196483075618744\n",
      "Epoch 3052, Loss: 1.1120411604642868, Final Batch Loss: 0.2856976091861725\n",
      "Epoch 3053, Loss: 1.0425529927015305, Final Batch Loss: 0.28849586844444275\n",
      "Epoch 3054, Loss: 1.0995756387710571, Final Batch Loss: 0.22047683596611023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3055, Loss: 1.0907442569732666, Final Batch Loss: 0.21201381087303162\n",
      "Epoch 3056, Loss: 1.0176148414611816, Final Batch Loss: 0.2618151307106018\n",
      "Epoch 3057, Loss: 1.100799709558487, Final Batch Loss: 0.25779423117637634\n",
      "Epoch 3058, Loss: 0.9378928542137146, Final Batch Loss: 0.26062023639678955\n",
      "Epoch 3059, Loss: 0.9476613402366638, Final Batch Loss: 0.2563544809818268\n",
      "Epoch 3060, Loss: 0.9908568561077118, Final Batch Loss: 0.23539802432060242\n",
      "Epoch 3061, Loss: 1.0481524467468262, Final Batch Loss: 0.2741217017173767\n",
      "Epoch 3062, Loss: 1.0400038957595825, Final Batch Loss: 0.22099122405052185\n",
      "Epoch 3063, Loss: 1.030955120921135, Final Batch Loss: 0.2794606685638428\n",
      "Epoch 3064, Loss: 0.9587889760732651, Final Batch Loss: 0.2874326705932617\n",
      "Epoch 3065, Loss: 1.1133186668157578, Final Batch Loss: 0.3505805432796478\n",
      "Epoch 3066, Loss: 1.0772000402212143, Final Batch Loss: 0.27275896072387695\n",
      "Epoch 3067, Loss: 1.0366743355989456, Final Batch Loss: 0.26102110743522644\n",
      "Epoch 3068, Loss: 0.8599571734666824, Final Batch Loss: 0.19771353900432587\n",
      "Epoch 3069, Loss: 1.0750324428081512, Final Batch Loss: 0.30189138650894165\n",
      "Epoch 3070, Loss: 1.153066486120224, Final Batch Loss: 0.25188592076301575\n",
      "Epoch 3071, Loss: 0.9965263903141022, Final Batch Loss: 0.2311328649520874\n",
      "Epoch 3072, Loss: 1.0596792101860046, Final Batch Loss: 0.2701944410800934\n",
      "Epoch 3073, Loss: 1.1567695587873459, Final Batch Loss: 0.2242843359708786\n",
      "Epoch 3074, Loss: 0.9569341242313385, Final Batch Loss: 0.17839504778385162\n",
      "Epoch 3075, Loss: 0.9781472384929657, Final Batch Loss: 0.28724581003189087\n",
      "Epoch 3076, Loss: 1.0792586505413055, Final Batch Loss: 0.28026553988456726\n",
      "Epoch 3077, Loss: 1.018898531794548, Final Batch Loss: 0.23762118816375732\n",
      "Epoch 3078, Loss: 1.143228143453598, Final Batch Loss: 0.36210134625434875\n",
      "Epoch 3079, Loss: 0.9875543117523193, Final Batch Loss: 0.2321900725364685\n",
      "Epoch 3080, Loss: 1.1878528594970703, Final Batch Loss: 0.2970798909664154\n",
      "Epoch 3081, Loss: 1.0539674162864685, Final Batch Loss: 0.2752268314361572\n",
      "Epoch 3082, Loss: 0.9852631390094757, Final Batch Loss: 0.14735518395900726\n",
      "Epoch 3083, Loss: 1.0473901480436325, Final Batch Loss: 0.2440970093011856\n",
      "Epoch 3084, Loss: 1.005692034959793, Final Batch Loss: 0.2681537866592407\n",
      "Epoch 3085, Loss: 1.0220158398151398, Final Batch Loss: 0.295816034078598\n",
      "Epoch 3086, Loss: 1.122167319059372, Final Batch Loss: 0.2769273519515991\n",
      "Epoch 3087, Loss: 0.7948246151208878, Final Batch Loss: 0.15106530487537384\n",
      "Epoch 3088, Loss: 0.9243185818195343, Final Batch Loss: 0.2420511394739151\n",
      "Epoch 3089, Loss: 0.9816093593835831, Final Batch Loss: 0.1988152265548706\n",
      "Epoch 3090, Loss: 1.038771852850914, Final Batch Loss: 0.2115563452243805\n",
      "Epoch 3091, Loss: 1.0786097198724747, Final Batch Loss: 0.21981415152549744\n",
      "Epoch 3092, Loss: 1.0300839841365814, Final Batch Loss: 0.25427040457725525\n",
      "Epoch 3093, Loss: 1.068706914782524, Final Batch Loss: 0.38915830850601196\n",
      "Epoch 3094, Loss: 1.0929855853319168, Final Batch Loss: 0.21169070899486542\n",
      "Epoch 3095, Loss: 0.9437967389822006, Final Batch Loss: 0.23190917074680328\n",
      "Epoch 3096, Loss: 1.017964631319046, Final Batch Loss: 0.18421921133995056\n",
      "Epoch 3097, Loss: 0.9707256555557251, Final Batch Loss: 0.23799897730350494\n",
      "Epoch 3098, Loss: 0.9327190518379211, Final Batch Loss: 0.1907290518283844\n",
      "Epoch 3099, Loss: 1.0114234685897827, Final Batch Loss: 0.25448426604270935\n",
      "Epoch 3100, Loss: 0.9804782867431641, Final Batch Loss: 0.23584915697574615\n",
      "Epoch 3101, Loss: 1.0828703343868256, Final Batch Loss: 0.288112610578537\n",
      "Epoch 3102, Loss: 1.0399381518363953, Final Batch Loss: 0.2194061279296875\n",
      "Epoch 3103, Loss: 0.9866012036800385, Final Batch Loss: 0.27270758152008057\n",
      "Epoch 3104, Loss: 0.8786100745201111, Final Batch Loss: 0.22415614128112793\n",
      "Epoch 3105, Loss: 1.0336752086877823, Final Batch Loss: 0.20068950951099396\n",
      "Epoch 3106, Loss: 1.0617018789052963, Final Batch Loss: 0.22347261011600494\n",
      "Epoch 3107, Loss: 1.0876428484916687, Final Batch Loss: 0.3026689291000366\n",
      "Epoch 3108, Loss: 1.0009164363145828, Final Batch Loss: 0.229922354221344\n",
      "Epoch 3109, Loss: 1.1225408613681793, Final Batch Loss: 0.25365379452705383\n",
      "Epoch 3110, Loss: 1.0713063478469849, Final Batch Loss: 0.2635001838207245\n",
      "Epoch 3111, Loss: 1.060049131512642, Final Batch Loss: 0.2731243371963501\n",
      "Epoch 3112, Loss: 1.1546804308891296, Final Batch Loss: 0.31564149260520935\n",
      "Epoch 3113, Loss: 0.975806787610054, Final Batch Loss: 0.22534745931625366\n",
      "Epoch 3114, Loss: 1.0506787151098251, Final Batch Loss: 0.2445669025182724\n",
      "Epoch 3115, Loss: 0.9004474878311157, Final Batch Loss: 0.19603243470191956\n",
      "Epoch 3116, Loss: 0.9875557571649551, Final Batch Loss: 0.251046746969223\n",
      "Epoch 3117, Loss: 1.0738000571727753, Final Batch Loss: 0.2884170413017273\n",
      "Epoch 3118, Loss: 1.0569203048944473, Final Batch Loss: 0.2753239870071411\n",
      "Epoch 3119, Loss: 1.39779394865036, Final Batch Loss: 0.3995018005371094\n",
      "Epoch 3120, Loss: 0.9948046207427979, Final Batch Loss: 0.23389124870300293\n",
      "Epoch 3121, Loss: 1.0754588693380356, Final Batch Loss: 0.38606759905815125\n",
      "Epoch 3122, Loss: 1.0446816384792328, Final Batch Loss: 0.2653213143348694\n",
      "Epoch 3123, Loss: 1.00905342400074, Final Batch Loss: 0.32632508873939514\n",
      "Epoch 3124, Loss: 1.0239308774471283, Final Batch Loss: 0.2717178165912628\n",
      "Epoch 3125, Loss: 0.959422305226326, Final Batch Loss: 0.23036177456378937\n",
      "Epoch 3126, Loss: 1.0615895241498947, Final Batch Loss: 0.27883243560791016\n",
      "Epoch 3127, Loss: 1.011939823627472, Final Batch Loss: 0.23680052161216736\n",
      "Epoch 3128, Loss: 0.9596947878599167, Final Batch Loss: 0.2155868262052536\n",
      "Epoch 3129, Loss: 1.1397272497415543, Final Batch Loss: 0.2443244606256485\n",
      "Epoch 3130, Loss: 1.12456813454628, Final Batch Loss: 0.27027714252471924\n",
      "Epoch 3131, Loss: 1.0026462227106094, Final Batch Loss: 0.23893582820892334\n",
      "Epoch 3132, Loss: 1.0920634865760803, Final Batch Loss: 0.2185346633195877\n",
      "Epoch 3133, Loss: 1.0088410526514053, Final Batch Loss: 0.32130882143974304\n",
      "Epoch 3134, Loss: 0.977417379617691, Final Batch Loss: 0.23416665196418762\n",
      "Epoch 3135, Loss: 1.029989317059517, Final Batch Loss: 0.2858670949935913\n",
      "Epoch 3136, Loss: 1.0904784947633743, Final Batch Loss: 0.24804171919822693\n",
      "Epoch 3137, Loss: 0.9520316869020462, Final Batch Loss: 0.22079354524612427\n",
      "Epoch 3138, Loss: 1.1203553825616837, Final Batch Loss: 0.324789434671402\n",
      "Epoch 3139, Loss: 0.9813901782035828, Final Batch Loss: 0.11815153062343597\n",
      "Epoch 3140, Loss: 0.9178741872310638, Final Batch Loss: 0.1686377078294754\n",
      "Epoch 3141, Loss: 0.8481582850217819, Final Batch Loss: 0.19668753445148468\n",
      "Epoch 3142, Loss: 1.0082141309976578, Final Batch Loss: 0.29424601793289185\n",
      "Epoch 3143, Loss: 0.8593131750822067, Final Batch Loss: 0.16405944526195526\n",
      "Epoch 3144, Loss: 0.9850688725709915, Final Batch Loss: 0.253600537776947\n",
      "Epoch 3145, Loss: 1.0970042943954468, Final Batch Loss: 0.3123548626899719\n",
      "Epoch 3146, Loss: 1.0666233599185944, Final Batch Loss: 0.22940103709697723\n",
      "Epoch 3147, Loss: 1.0252116024494171, Final Batch Loss: 0.2737133204936981\n",
      "Epoch 3148, Loss: 0.9812598079442978, Final Batch Loss: 0.23345054686069489\n",
      "Epoch 3149, Loss: 1.1579117476940155, Final Batch Loss: 0.2899130582809448\n",
      "Epoch 3150, Loss: 1.1165659427642822, Final Batch Loss: 0.22736120223999023\n",
      "Epoch 3151, Loss: 1.0646548420190811, Final Batch Loss: 0.23241500556468964\n",
      "Epoch 3152, Loss: 1.0307069271802902, Final Batch Loss: 0.22047418355941772\n",
      "Epoch 3153, Loss: 0.9061634987592697, Final Batch Loss: 0.196941539645195\n",
      "Epoch 3154, Loss: 0.9605760425329208, Final Batch Loss: 0.26426517963409424\n",
      "Epoch 3155, Loss: 1.1807710379362106, Final Batch Loss: 0.3700719177722931\n",
      "Epoch 3156, Loss: 0.982614129781723, Final Batch Loss: 0.226835697889328\n",
      "Epoch 3157, Loss: 1.04001784324646, Final Batch Loss: 0.22605416178703308\n",
      "Epoch 3158, Loss: 0.8825797587633133, Final Batch Loss: 0.23872406780719757\n",
      "Epoch 3159, Loss: 1.0497544407844543, Final Batch Loss: 0.21641376614570618\n",
      "Epoch 3160, Loss: 0.9968653172254562, Final Batch Loss: 0.14907655119895935\n",
      "Epoch 3161, Loss: 1.0322530418634415, Final Batch Loss: 0.2797507643699646\n",
      "Epoch 3162, Loss: 0.9867569953203201, Final Batch Loss: 0.2236780822277069\n",
      "Epoch 3163, Loss: 0.9918215721845627, Final Batch Loss: 0.30265164375305176\n",
      "Epoch 3164, Loss: 0.9715302586555481, Final Batch Loss: 0.208767831325531\n",
      "Epoch 3165, Loss: 0.8993903994560242, Final Batch Loss: 0.21449846029281616\n",
      "Epoch 3166, Loss: 1.0513703972101212, Final Batch Loss: 0.2619626820087433\n",
      "Epoch 3167, Loss: 0.9905981719493866, Final Batch Loss: 0.2284976691007614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3168, Loss: 1.0315414071083069, Final Batch Loss: 0.22945430874824524\n",
      "Epoch 3169, Loss: 0.9823942631483078, Final Batch Loss: 0.25562989711761475\n",
      "Epoch 3170, Loss: 1.0021916776895523, Final Batch Loss: 0.238656684756279\n",
      "Epoch 3171, Loss: 0.9649192094802856, Final Batch Loss: 0.21539919078350067\n",
      "Epoch 3172, Loss: 1.0132589042186737, Final Batch Loss: 0.26132091879844666\n",
      "Epoch 3173, Loss: 1.1733905225992203, Final Batch Loss: 0.3404412865638733\n",
      "Epoch 3174, Loss: 1.0543310940265656, Final Batch Loss: 0.2689642012119293\n",
      "Epoch 3175, Loss: 1.0328252166509628, Final Batch Loss: 0.29702723026275635\n",
      "Epoch 3176, Loss: 1.0812854766845703, Final Batch Loss: 0.253601610660553\n",
      "Epoch 3177, Loss: 1.0402715057134628, Final Batch Loss: 0.2718493938446045\n",
      "Epoch 3178, Loss: 0.949122965335846, Final Batch Loss: 0.18639753758907318\n",
      "Epoch 3179, Loss: 0.9043922573328018, Final Batch Loss: 0.21784040331840515\n",
      "Epoch 3180, Loss: 0.9652882218360901, Final Batch Loss: 0.24280093610286713\n",
      "Epoch 3181, Loss: 0.9715631604194641, Final Batch Loss: 0.22135399281978607\n",
      "Epoch 3182, Loss: 0.9446974545717239, Final Batch Loss: 0.24045872688293457\n",
      "Epoch 3183, Loss: 1.004829227924347, Final Batch Loss: 0.21728089451789856\n",
      "Epoch 3184, Loss: 1.068664327263832, Final Batch Loss: 0.3129047453403473\n",
      "Epoch 3185, Loss: 1.050881490111351, Final Batch Loss: 0.24795882403850555\n",
      "Epoch 3186, Loss: 1.1168965101242065, Final Batch Loss: 0.25837215781211853\n",
      "Epoch 3187, Loss: 0.9494361132383347, Final Batch Loss: 0.2108912169933319\n",
      "Epoch 3188, Loss: 1.013132631778717, Final Batch Loss: 0.3229186236858368\n",
      "Epoch 3189, Loss: 0.9774330705404282, Final Batch Loss: 0.22934022545814514\n",
      "Epoch 3190, Loss: 0.9480942189693451, Final Batch Loss: 0.24608878791332245\n",
      "Epoch 3191, Loss: 0.9666708409786224, Final Batch Loss: 0.23216407001018524\n",
      "Epoch 3192, Loss: 0.9848897904157639, Final Batch Loss: 0.3167133033275604\n",
      "Epoch 3193, Loss: 0.9958582520484924, Final Batch Loss: 0.25570690631866455\n",
      "Epoch 3194, Loss: 1.207867681980133, Final Batch Loss: 0.3199491500854492\n",
      "Epoch 3195, Loss: 1.1852924078702927, Final Batch Loss: 0.2246665507555008\n",
      "Epoch 3196, Loss: 0.9995152354240417, Final Batch Loss: 0.25332191586494446\n",
      "Epoch 3197, Loss: 1.0427130907773972, Final Batch Loss: 0.2881021499633789\n",
      "Epoch 3198, Loss: 1.084893450140953, Final Batch Loss: 0.31045031547546387\n",
      "Epoch 3199, Loss: 1.0239155292510986, Final Batch Loss: 0.27532273530960083\n",
      "Epoch 3200, Loss: 0.9650832265615463, Final Batch Loss: 0.1870793104171753\n",
      "Epoch 3201, Loss: 0.9779139757156372, Final Batch Loss: 0.28655287623405457\n",
      "Epoch 3202, Loss: 1.01798677444458, Final Batch Loss: 0.2944261133670807\n",
      "Epoch 3203, Loss: 1.0832242518663406, Final Batch Loss: 0.24192474782466888\n",
      "Epoch 3204, Loss: 1.0689556896686554, Final Batch Loss: 0.2438220977783203\n",
      "Epoch 3205, Loss: 1.1101419031620026, Final Batch Loss: 0.3391830623149872\n",
      "Epoch 3206, Loss: 1.0191471874713898, Final Batch Loss: 0.2635693848133087\n",
      "Epoch 3207, Loss: 0.9514999985694885, Final Batch Loss: 0.2375093400478363\n",
      "Epoch 3208, Loss: 1.001933366060257, Final Batch Loss: 0.2759038805961609\n",
      "Epoch 3209, Loss: 0.9748113304376602, Final Batch Loss: 0.24666854739189148\n",
      "Epoch 3210, Loss: 1.038952574133873, Final Batch Loss: 0.27784958481788635\n",
      "Epoch 3211, Loss: 0.9713999480009079, Final Batch Loss: 0.21426743268966675\n",
      "Epoch 3212, Loss: 0.9615224599838257, Final Batch Loss: 0.16165950894355774\n",
      "Epoch 3213, Loss: 1.002041831612587, Final Batch Loss: 0.24688060581684113\n",
      "Epoch 3214, Loss: 1.1199980080127716, Final Batch Loss: 0.2932226061820984\n",
      "Epoch 3215, Loss: 0.9284772872924805, Final Batch Loss: 0.16442523896694183\n",
      "Epoch 3216, Loss: 0.9301501363515854, Final Batch Loss: 0.26346123218536377\n",
      "Epoch 3217, Loss: 0.9696406871080399, Final Batch Loss: 0.2695850431919098\n",
      "Epoch 3218, Loss: 0.8659888952970505, Final Batch Loss: 0.13986274600028992\n",
      "Epoch 3219, Loss: 0.9426113367080688, Final Batch Loss: 0.2513563632965088\n",
      "Epoch 3220, Loss: 0.996351569890976, Final Batch Loss: 0.2881433963775635\n",
      "Epoch 3221, Loss: 0.8507338762283325, Final Batch Loss: 0.17520706355571747\n",
      "Epoch 3222, Loss: 1.055271103978157, Final Batch Loss: 0.23994693160057068\n",
      "Epoch 3223, Loss: 0.9714182764291763, Final Batch Loss: 0.23150712251663208\n",
      "Epoch 3224, Loss: 1.029977560043335, Final Batch Loss: 0.2850733995437622\n",
      "Epoch 3225, Loss: 0.9175819903612137, Final Batch Loss: 0.22427909076213837\n",
      "Epoch 3226, Loss: 0.9241282194852829, Final Batch Loss: 0.16914449632167816\n",
      "Epoch 3227, Loss: 0.9508616328239441, Final Batch Loss: 0.25668570399284363\n",
      "Epoch 3228, Loss: 0.9578315019607544, Final Batch Loss: 0.20131094753742218\n",
      "Epoch 3229, Loss: 1.0020816028118134, Final Batch Loss: 0.21766318380832672\n",
      "Epoch 3230, Loss: 0.9763354659080505, Final Batch Loss: 0.2789432108402252\n",
      "Epoch 3231, Loss: 0.9112213104963303, Final Batch Loss: 0.23796474933624268\n",
      "Epoch 3232, Loss: 0.9937192499637604, Final Batch Loss: 0.3206740915775299\n",
      "Epoch 3233, Loss: 1.0091656595468521, Final Batch Loss: 0.27714774012565613\n",
      "Epoch 3234, Loss: 1.0535326600074768, Final Batch Loss: 0.24108220636844635\n",
      "Epoch 3235, Loss: 0.9975617229938507, Final Batch Loss: 0.34093326330184937\n",
      "Epoch 3236, Loss: 1.0165486931800842, Final Batch Loss: 0.27328017354011536\n",
      "Epoch 3237, Loss: 0.9317561388015747, Final Batch Loss: 0.21325364708900452\n",
      "Epoch 3238, Loss: 0.982860803604126, Final Batch Loss: 0.25184905529022217\n",
      "Epoch 3239, Loss: 1.103227585554123, Final Batch Loss: 0.36268895864486694\n",
      "Epoch 3240, Loss: 1.0462493002414703, Final Batch Loss: 0.22257892787456512\n",
      "Epoch 3241, Loss: 0.8964250534772873, Final Batch Loss: 0.23452499508857727\n",
      "Epoch 3242, Loss: 1.0913261026144028, Final Batch Loss: 0.2537747025489807\n",
      "Epoch 3243, Loss: 1.0036138743162155, Final Batch Loss: 0.30099576711654663\n",
      "Epoch 3244, Loss: 1.0122290551662445, Final Batch Loss: 0.2616422772407532\n",
      "Epoch 3245, Loss: 0.9941093325614929, Final Batch Loss: 0.2326558232307434\n",
      "Epoch 3246, Loss: 1.0699119865894318, Final Batch Loss: 0.295303076505661\n",
      "Epoch 3247, Loss: 0.9829835295677185, Final Batch Loss: 0.2199895828962326\n",
      "Epoch 3248, Loss: 1.0555249005556107, Final Batch Loss: 0.35922932624816895\n",
      "Epoch 3249, Loss: 0.9277894049882889, Final Batch Loss: 0.2244599461555481\n",
      "Epoch 3250, Loss: 1.0881870537996292, Final Batch Loss: 0.3588888943195343\n",
      "Epoch 3251, Loss: 1.0100377798080444, Final Batch Loss: 0.20050089061260223\n",
      "Epoch 3252, Loss: 1.0256421267986298, Final Batch Loss: 0.20313161611557007\n",
      "Epoch 3253, Loss: 1.1160959303379059, Final Batch Loss: 0.29534974694252014\n",
      "Epoch 3254, Loss: 1.0321685820817947, Final Batch Loss: 0.262687623500824\n",
      "Epoch 3255, Loss: 1.013154998421669, Final Batch Loss: 0.2790190875530243\n",
      "Epoch 3256, Loss: 1.0003835558891296, Final Batch Loss: 0.17454248666763306\n",
      "Epoch 3257, Loss: 0.9717600345611572, Final Batch Loss: 0.23331767320632935\n",
      "Epoch 3258, Loss: 1.1132734268903732, Final Batch Loss: 0.1613352745771408\n",
      "Epoch 3259, Loss: 1.0226643830537796, Final Batch Loss: 0.27374574542045593\n",
      "Epoch 3260, Loss: 0.9852480888366699, Final Batch Loss: 0.18175485730171204\n",
      "Epoch 3261, Loss: 0.9736560732126236, Final Batch Loss: 0.24664270877838135\n",
      "Epoch 3262, Loss: 0.937140941619873, Final Batch Loss: 0.212713360786438\n",
      "Epoch 3263, Loss: 0.993996188044548, Final Batch Loss: 0.23419110476970673\n",
      "Epoch 3264, Loss: 1.0545323640108109, Final Batch Loss: 0.20124666392803192\n",
      "Epoch 3265, Loss: 1.050852209329605, Final Batch Loss: 0.2588481605052948\n",
      "Epoch 3266, Loss: 1.0600444972515106, Final Batch Loss: 0.25742581486701965\n",
      "Epoch 3267, Loss: 0.9421758502721786, Final Batch Loss: 0.206981360912323\n",
      "Epoch 3268, Loss: 0.9193822741508484, Final Batch Loss: 0.20827381312847137\n",
      "Epoch 3269, Loss: 0.8966555595397949, Final Batch Loss: 0.2160213589668274\n",
      "Epoch 3270, Loss: 1.084055781364441, Final Batch Loss: 0.3035935163497925\n",
      "Epoch 3271, Loss: 0.9965127408504486, Final Batch Loss: 0.18429963290691376\n",
      "Epoch 3272, Loss: 1.0960737764835358, Final Batch Loss: 0.26740536093711853\n",
      "Epoch 3273, Loss: 0.9483471065759659, Final Batch Loss: 0.2005489617586136\n",
      "Epoch 3274, Loss: 1.0428904742002487, Final Batch Loss: 0.30432459712028503\n",
      "Epoch 3275, Loss: 1.0276342183351517, Final Batch Loss: 0.2722434103488922\n",
      "Epoch 3276, Loss: 0.9821256697177887, Final Batch Loss: 0.23924776911735535\n",
      "Epoch 3277, Loss: 0.9798012375831604, Final Batch Loss: 0.19444802403450012\n",
      "Epoch 3278, Loss: 1.0528113692998886, Final Batch Loss: 0.23956815898418427\n",
      "Epoch 3279, Loss: 0.897322416305542, Final Batch Loss: 0.15279430150985718\n",
      "Epoch 3280, Loss: 0.9710993617773056, Final Batch Loss: 0.2738986909389496\n",
      "Epoch 3281, Loss: 0.9383929967880249, Final Batch Loss: 0.24557846784591675\n",
      "Epoch 3282, Loss: 1.000161424279213, Final Batch Loss: 0.21857719123363495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3283, Loss: 0.90836501121521, Final Batch Loss: 0.21142682433128357\n",
      "Epoch 3284, Loss: 0.9393295049667358, Final Batch Loss: 0.2382732629776001\n",
      "Epoch 3285, Loss: 0.9564956575632095, Final Batch Loss: 0.22025412321090698\n",
      "Epoch 3286, Loss: 0.9631639271974564, Final Batch Loss: 0.3058559000492096\n",
      "Epoch 3287, Loss: 1.0483795553445816, Final Batch Loss: 0.2923189699649811\n",
      "Epoch 3288, Loss: 0.9212067127227783, Final Batch Loss: 0.2799014449119568\n",
      "Epoch 3289, Loss: 0.8808884769678116, Final Batch Loss: 0.24297772347927094\n",
      "Epoch 3290, Loss: 1.006228819489479, Final Batch Loss: 0.2591603100299835\n",
      "Epoch 3291, Loss: 0.903226763010025, Final Batch Loss: 0.20450493693351746\n",
      "Epoch 3292, Loss: 1.0401794761419296, Final Batch Loss: 0.3328629732131958\n",
      "Epoch 3293, Loss: 0.9410024434328079, Final Batch Loss: 0.22199812531471252\n",
      "Epoch 3294, Loss: 0.9328318685293198, Final Batch Loss: 0.25779736042022705\n",
      "Epoch 3295, Loss: 0.8657292872667313, Final Batch Loss: 0.2727997601032257\n",
      "Epoch 3296, Loss: 0.8436739891767502, Final Batch Loss: 0.2176826000213623\n",
      "Epoch 3297, Loss: 0.990393340587616, Final Batch Loss: 0.25039684772491455\n",
      "Epoch 3298, Loss: 0.942263588309288, Final Batch Loss: 0.2698843479156494\n",
      "Epoch 3299, Loss: 1.0043847858905792, Final Batch Loss: 0.23350732028484344\n",
      "Epoch 3300, Loss: 1.0296314060688019, Final Batch Loss: 0.2751677930355072\n",
      "Epoch 3301, Loss: 0.9510237574577332, Final Batch Loss: 0.21830224990844727\n",
      "Epoch 3302, Loss: 0.886817529797554, Final Batch Loss: 0.2197401374578476\n",
      "Epoch 3303, Loss: 0.9672355502843857, Final Batch Loss: 0.2653147876262665\n",
      "Epoch 3304, Loss: 0.9997448474168777, Final Batch Loss: 0.20856407284736633\n",
      "Epoch 3305, Loss: 1.0085581690073013, Final Batch Loss: 0.22270084917545319\n",
      "Epoch 3306, Loss: 1.0418827533721924, Final Batch Loss: 0.2557601034641266\n",
      "Epoch 3307, Loss: 1.0137567073106766, Final Batch Loss: 0.20304881036281586\n",
      "Epoch 3308, Loss: 1.1195144206285477, Final Batch Loss: 0.3205287754535675\n",
      "Epoch 3309, Loss: 1.0371946394443512, Final Batch Loss: 0.2215069830417633\n",
      "Epoch 3310, Loss: 1.1481164395809174, Final Batch Loss: 0.29123690724372864\n",
      "Epoch 3311, Loss: 1.0136078745126724, Final Batch Loss: 0.3538322448730469\n",
      "Epoch 3312, Loss: 1.050574630498886, Final Batch Loss: 0.2607119083404541\n",
      "Epoch 3313, Loss: 0.9925828874111176, Final Batch Loss: 0.2163713276386261\n",
      "Epoch 3314, Loss: 1.1615760028362274, Final Batch Loss: 0.34860050678253174\n",
      "Epoch 3315, Loss: 1.0893661677837372, Final Batch Loss: 0.37173718214035034\n",
      "Epoch 3316, Loss: 0.9672228097915649, Final Batch Loss: 0.22626933455467224\n",
      "Epoch 3317, Loss: 0.9188474714756012, Final Batch Loss: 0.24172671139240265\n",
      "Epoch 3318, Loss: 0.9488013684749603, Final Batch Loss: 0.2207227200269699\n",
      "Epoch 3319, Loss: 1.038742572069168, Final Batch Loss: 0.3154427707195282\n",
      "Epoch 3320, Loss: 0.9973966181278229, Final Batch Loss: 0.2479928433895111\n",
      "Epoch 3321, Loss: 0.9071501791477203, Final Batch Loss: 0.21382305026054382\n",
      "Epoch 3322, Loss: 0.9027299284934998, Final Batch Loss: 0.2583872675895691\n",
      "Epoch 3323, Loss: 0.9207867085933685, Final Batch Loss: 0.24220062792301178\n",
      "Epoch 3324, Loss: 0.9442281723022461, Final Batch Loss: 0.20150794088840485\n",
      "Epoch 3325, Loss: 1.1039039194583893, Final Batch Loss: 0.23710697889328003\n",
      "Epoch 3326, Loss: 1.0012190639972687, Final Batch Loss: 0.22355403006076813\n",
      "Epoch 3327, Loss: 1.0517016500234604, Final Batch Loss: 0.27096235752105713\n",
      "Epoch 3328, Loss: 0.9279632121324539, Final Batch Loss: 0.23027776181697845\n",
      "Epoch 3329, Loss: 0.986336886882782, Final Batch Loss: 0.2099061906337738\n",
      "Epoch 3330, Loss: 0.9606893509626389, Final Batch Loss: 0.2359035611152649\n",
      "Epoch 3331, Loss: 0.9064375907182693, Final Batch Loss: 0.2647450268268585\n",
      "Epoch 3332, Loss: 0.9395633935928345, Final Batch Loss: 0.21110044419765472\n",
      "Epoch 3333, Loss: 0.962310329079628, Final Batch Loss: 0.21517017483711243\n",
      "Epoch 3334, Loss: 0.9107493758201599, Final Batch Loss: 0.2172580361366272\n",
      "Epoch 3335, Loss: 1.001126989722252, Final Batch Loss: 0.28114748001098633\n",
      "Epoch 3336, Loss: 0.8907720893621445, Final Batch Loss: 0.24556086957454681\n",
      "Epoch 3337, Loss: 1.0352858155965805, Final Batch Loss: 0.3593985140323639\n",
      "Epoch 3338, Loss: 1.0398698151111603, Final Batch Loss: 0.2601698637008667\n",
      "Epoch 3339, Loss: 0.9456975311040878, Final Batch Loss: 0.28139936923980713\n",
      "Epoch 3340, Loss: 1.0030762255191803, Final Batch Loss: 0.2179228663444519\n",
      "Epoch 3341, Loss: 1.0763940811157227, Final Batch Loss: 0.25507935881614685\n",
      "Epoch 3342, Loss: 1.0023811012506485, Final Batch Loss: 0.27572861313819885\n",
      "Epoch 3343, Loss: 1.077336236834526, Final Batch Loss: 0.16669918596744537\n",
      "Epoch 3344, Loss: 1.0294243097305298, Final Batch Loss: 0.20876702666282654\n",
      "Epoch 3345, Loss: 1.0727399587631226, Final Batch Loss: 0.27495262026786804\n",
      "Epoch 3346, Loss: 1.1117537021636963, Final Batch Loss: 0.2817842960357666\n",
      "Epoch 3347, Loss: 0.98813696205616, Final Batch Loss: 0.2641609311103821\n",
      "Epoch 3348, Loss: 1.0564244389533997, Final Batch Loss: 0.2789626121520996\n",
      "Epoch 3349, Loss: 0.9642066806554794, Final Batch Loss: 0.22509023547172546\n",
      "Epoch 3350, Loss: 0.9910604655742645, Final Batch Loss: 0.24013948440551758\n",
      "Epoch 3351, Loss: 0.974974662065506, Final Batch Loss: 0.3263683021068573\n",
      "Epoch 3352, Loss: 1.0153439939022064, Final Batch Loss: 0.23243935406208038\n",
      "Epoch 3353, Loss: 1.0029218941926956, Final Batch Loss: 0.23275117576122284\n",
      "Epoch 3354, Loss: 0.9192545264959335, Final Batch Loss: 0.16671837866306305\n",
      "Epoch 3355, Loss: 1.0503243505954742, Final Batch Loss: 0.23411931097507477\n",
      "Epoch 3356, Loss: 1.0829595774412155, Final Batch Loss: 0.2786555886268616\n",
      "Epoch 3357, Loss: 0.9020869582891464, Final Batch Loss: 0.2122359424829483\n",
      "Epoch 3358, Loss: 1.0395010560750961, Final Batch Loss: 0.22154437005519867\n",
      "Epoch 3359, Loss: 0.9717855006456375, Final Batch Loss: 0.19399362802505493\n",
      "Epoch 3360, Loss: 0.9719680398702621, Final Batch Loss: 0.2924176752567291\n",
      "Epoch 3361, Loss: 0.8582340180873871, Final Batch Loss: 0.22527432441711426\n",
      "Epoch 3362, Loss: 0.8528483062982559, Final Batch Loss: 0.24011504650115967\n",
      "Epoch 3363, Loss: 1.0710394978523254, Final Batch Loss: 0.31321007013320923\n",
      "Epoch 3364, Loss: 1.1209869980812073, Final Batch Loss: 0.2887475788593292\n",
      "Epoch 3365, Loss: 0.8179384768009186, Final Batch Loss: 0.17642958462238312\n",
      "Epoch 3366, Loss: 0.9735970497131348, Final Batch Loss: 0.23659369349479675\n",
      "Epoch 3367, Loss: 0.990279883146286, Final Batch Loss: 0.2625570595264435\n",
      "Epoch 3368, Loss: 0.877264991402626, Final Batch Loss: 0.27780911326408386\n",
      "Epoch 3369, Loss: 0.8296605348587036, Final Batch Loss: 0.2285989224910736\n",
      "Epoch 3370, Loss: 0.880744218826294, Final Batch Loss: 0.19994930922985077\n",
      "Epoch 3371, Loss: 0.9086386561393738, Final Batch Loss: 0.19657713174819946\n",
      "Epoch 3372, Loss: 0.8248844146728516, Final Batch Loss: 0.11859063804149628\n",
      "Epoch 3373, Loss: 1.0961599349975586, Final Batch Loss: 0.304518461227417\n",
      "Epoch 3374, Loss: 0.8879266083240509, Final Batch Loss: 0.20399537682533264\n",
      "Epoch 3375, Loss: 0.873269110918045, Final Batch Loss: 0.21381647884845734\n",
      "Epoch 3376, Loss: 0.8917095512151718, Final Batch Loss: 0.19935014843940735\n",
      "Epoch 3377, Loss: 0.982418030500412, Final Batch Loss: 0.268426775932312\n",
      "Epoch 3378, Loss: 0.9006632268428802, Final Batch Loss: 0.19862768054008484\n",
      "Epoch 3379, Loss: 0.9779647588729858, Final Batch Loss: 0.2485623061656952\n",
      "Epoch 3380, Loss: 0.9607757776975632, Final Batch Loss: 0.2492254078388214\n",
      "Epoch 3381, Loss: 0.9303152710199356, Final Batch Loss: 0.23993276059627533\n",
      "Epoch 3382, Loss: 0.8960731625556946, Final Batch Loss: 0.2724994719028473\n",
      "Epoch 3383, Loss: 0.9489617943763733, Final Batch Loss: 0.23884856700897217\n",
      "Epoch 3384, Loss: 0.9683492183685303, Final Batch Loss: 0.2137790471315384\n",
      "Epoch 3385, Loss: 0.9022395759820938, Final Batch Loss: 0.24120014905929565\n",
      "Epoch 3386, Loss: 0.9133282154798508, Final Batch Loss: 0.31466373801231384\n",
      "Epoch 3387, Loss: 1.0365834683179855, Final Batch Loss: 0.2976756989955902\n",
      "Epoch 3388, Loss: 0.9398354142904282, Final Batch Loss: 0.22937875986099243\n",
      "Epoch 3389, Loss: 1.0457114279270172, Final Batch Loss: 0.27558302879333496\n",
      "Epoch 3390, Loss: 0.984544113278389, Final Batch Loss: 0.2659947872161865\n",
      "Epoch 3391, Loss: 0.892919734120369, Final Batch Loss: 0.21625709533691406\n",
      "Epoch 3392, Loss: 0.8657208383083344, Final Batch Loss: 0.18081684410572052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3393, Loss: 0.8576874732971191, Final Batch Loss: 0.20207250118255615\n",
      "Epoch 3394, Loss: 1.1298817843198776, Final Batch Loss: 0.3500199019908905\n",
      "Epoch 3395, Loss: 0.9730127900838852, Final Batch Loss: 0.23927657306194305\n",
      "Epoch 3396, Loss: 0.9720070064067841, Final Batch Loss: 0.23807093501091003\n",
      "Epoch 3397, Loss: 0.9613057523965836, Final Batch Loss: 0.2411738783121109\n",
      "Epoch 3398, Loss: 1.1541435718536377, Final Batch Loss: 0.31952378153800964\n",
      "Epoch 3399, Loss: 1.0025187581777573, Final Batch Loss: 0.2185368835926056\n",
      "Epoch 3400, Loss: 1.0485431551933289, Final Batch Loss: 0.3368784487247467\n",
      "Epoch 3401, Loss: 0.889740064740181, Final Batch Loss: 0.21949124336242676\n",
      "Epoch 3402, Loss: 0.9300160706043243, Final Batch Loss: 0.2361868917942047\n",
      "Epoch 3403, Loss: 0.9992684870958328, Final Batch Loss: 0.20135986804962158\n",
      "Epoch 3404, Loss: 0.8925540894269943, Final Batch Loss: 0.22403819859027863\n",
      "Epoch 3405, Loss: 0.9201865494251251, Final Batch Loss: 0.1834298074245453\n",
      "Epoch 3406, Loss: 1.044507622718811, Final Batch Loss: 0.3034713864326477\n",
      "Epoch 3407, Loss: 0.9324358254671097, Final Batch Loss: 0.20426486432552338\n",
      "Epoch 3408, Loss: 0.9590780586004257, Final Batch Loss: 0.2873193025588989\n",
      "Epoch 3409, Loss: 0.9823586642742157, Final Batch Loss: 0.2832294702529907\n",
      "Epoch 3410, Loss: 1.0042156130075455, Final Batch Loss: 0.3210451602935791\n",
      "Epoch 3411, Loss: 0.866674616932869, Final Batch Loss: 0.24935433268547058\n",
      "Epoch 3412, Loss: 1.1245616376399994, Final Batch Loss: 0.18936550617218018\n",
      "Epoch 3413, Loss: 1.0900813937187195, Final Batch Loss: 0.25542014837265015\n",
      "Epoch 3414, Loss: 0.9799845814704895, Final Batch Loss: 0.19774968922138214\n",
      "Epoch 3415, Loss: 0.8503100425004959, Final Batch Loss: 0.208806112408638\n",
      "Epoch 3416, Loss: 1.037462905049324, Final Batch Loss: 0.26862186193466187\n",
      "Epoch 3417, Loss: 0.9910039752721786, Final Batch Loss: 0.18001268804073334\n",
      "Epoch 3418, Loss: 0.9936941862106323, Final Batch Loss: 0.29249653220176697\n",
      "Epoch 3419, Loss: 0.9986713379621506, Final Batch Loss: 0.21236810088157654\n",
      "Epoch 3420, Loss: 0.9686219841241837, Final Batch Loss: 0.2930849492549896\n",
      "Epoch 3421, Loss: 0.9271695762872696, Final Batch Loss: 0.2847560942173004\n",
      "Epoch 3422, Loss: 1.0043643712997437, Final Batch Loss: 0.3026476800441742\n",
      "Epoch 3423, Loss: 1.0323094576597214, Final Batch Loss: 0.3086102306842804\n",
      "Epoch 3424, Loss: 0.8662846237421036, Final Batch Loss: 0.1812126487493515\n",
      "Epoch 3425, Loss: 1.0229328125715256, Final Batch Loss: 0.26091083884239197\n",
      "Epoch 3426, Loss: 0.8794990330934525, Final Batch Loss: 0.14745821058750153\n",
      "Epoch 3427, Loss: 1.0327792316675186, Final Batch Loss: 0.24755200743675232\n",
      "Epoch 3428, Loss: 0.9736870229244232, Final Batch Loss: 0.30853211879730225\n",
      "Epoch 3429, Loss: 1.0961737036705017, Final Batch Loss: 0.30296632647514343\n",
      "Epoch 3430, Loss: 0.9090135097503662, Final Batch Loss: 0.2300228476524353\n",
      "Epoch 3431, Loss: 0.9057090729475021, Final Batch Loss: 0.21328312158584595\n",
      "Epoch 3432, Loss: 1.048414632678032, Final Batch Loss: 0.23110218346118927\n",
      "Epoch 3433, Loss: 0.967051163315773, Final Batch Loss: 0.22161880135536194\n",
      "Epoch 3434, Loss: 1.0616952329874039, Final Batch Loss: 0.3026382029056549\n",
      "Epoch 3435, Loss: 1.0264981240034103, Final Batch Loss: 0.29488009214401245\n",
      "Epoch 3436, Loss: 1.0211132317781448, Final Batch Loss: 0.2866499423980713\n",
      "Epoch 3437, Loss: 0.8846359550952911, Final Batch Loss: 0.2429301142692566\n",
      "Epoch 3438, Loss: 1.0037542581558228, Final Batch Loss: 0.24563553929328918\n",
      "Epoch 3439, Loss: 1.0172407776117325, Final Batch Loss: 0.2813354432582855\n",
      "Epoch 3440, Loss: 1.0057860165834427, Final Batch Loss: 0.22352565824985504\n",
      "Epoch 3441, Loss: 0.959027037024498, Final Batch Loss: 0.22823107242584229\n",
      "Epoch 3442, Loss: 1.0063220262527466, Final Batch Loss: 0.22281818091869354\n",
      "Epoch 3443, Loss: 0.9778553247451782, Final Batch Loss: 0.22716708481311798\n",
      "Epoch 3444, Loss: 0.9691252112388611, Final Batch Loss: 0.1926526129245758\n",
      "Epoch 3445, Loss: 1.0107147842645645, Final Batch Loss: 0.23267044126987457\n",
      "Epoch 3446, Loss: 0.9670554846525192, Final Batch Loss: 0.25844091176986694\n",
      "Epoch 3447, Loss: 0.9679406434297562, Final Batch Loss: 0.26905056834220886\n",
      "Epoch 3448, Loss: 1.0277585536241531, Final Batch Loss: 0.2570796012878418\n",
      "Epoch 3449, Loss: 1.0271231085062027, Final Batch Loss: 0.24036350846290588\n",
      "Epoch 3450, Loss: 0.9627019166946411, Final Batch Loss: 0.19301193952560425\n",
      "Epoch 3451, Loss: 0.8563301414251328, Final Batch Loss: 0.22286297380924225\n",
      "Epoch 3452, Loss: 0.9391560554504395, Final Batch Loss: 0.16228151321411133\n",
      "Epoch 3453, Loss: 1.005417138338089, Final Batch Loss: 0.207163468003273\n",
      "Epoch 3454, Loss: 0.8894564211368561, Final Batch Loss: 0.30170825123786926\n",
      "Epoch 3455, Loss: 0.929784432053566, Final Batch Loss: 0.22087696194648743\n",
      "Epoch 3456, Loss: 1.041071504354477, Final Batch Loss: 0.33584871888160706\n",
      "Epoch 3457, Loss: 1.0476329624652863, Final Batch Loss: 0.24613112211227417\n",
      "Epoch 3458, Loss: 0.9455840587615967, Final Batch Loss: 0.22969657182693481\n",
      "Epoch 3459, Loss: 0.9826663583517075, Final Batch Loss: 0.26903587579727173\n",
      "Epoch 3460, Loss: 0.880205050110817, Final Batch Loss: 0.22100365161895752\n",
      "Epoch 3461, Loss: 0.8697775304317474, Final Batch Loss: 0.23367449641227722\n",
      "Epoch 3462, Loss: 0.9547750055789948, Final Batch Loss: 0.20330114662647247\n",
      "Epoch 3463, Loss: 0.9263492524623871, Final Batch Loss: 0.26284366846084595\n",
      "Epoch 3464, Loss: 1.0124482661485672, Final Batch Loss: 0.2571077346801758\n",
      "Epoch 3465, Loss: 0.8956978023052216, Final Batch Loss: 0.21395502984523773\n",
      "Epoch 3466, Loss: 1.1059460043907166, Final Batch Loss: 0.2966572344303131\n",
      "Epoch 3467, Loss: 0.9223038256168365, Final Batch Loss: 0.19498148560523987\n",
      "Epoch 3468, Loss: 0.8543407022953033, Final Batch Loss: 0.2551893889904022\n",
      "Epoch 3469, Loss: 0.8987553417682648, Final Batch Loss: 0.24445544183254242\n",
      "Epoch 3470, Loss: 0.8938648849725723, Final Batch Loss: 0.2314683049917221\n",
      "Epoch 3471, Loss: 0.9727567583322525, Final Batch Loss: 0.22508297860622406\n",
      "Epoch 3472, Loss: 1.00954107940197, Final Batch Loss: 0.23092572391033173\n",
      "Epoch 3473, Loss: 0.8603201061487198, Final Batch Loss: 0.19612571597099304\n",
      "Epoch 3474, Loss: 1.0029377788305283, Final Batch Loss: 0.22144383192062378\n",
      "Epoch 3475, Loss: 0.925870418548584, Final Batch Loss: 0.25558730959892273\n",
      "Epoch 3476, Loss: 1.0374542772769928, Final Batch Loss: 0.2618444859981537\n",
      "Epoch 3477, Loss: 0.9413735568523407, Final Batch Loss: 0.19232800602912903\n",
      "Epoch 3478, Loss: 0.9561387300491333, Final Batch Loss: 0.2506392002105713\n",
      "Epoch 3479, Loss: 1.0545726865530014, Final Batch Loss: 0.20860369503498077\n",
      "Epoch 3480, Loss: 0.9729167073965073, Final Batch Loss: 0.2681024670600891\n",
      "Epoch 3481, Loss: 0.95576111972332, Final Batch Loss: 0.2775800824165344\n",
      "Epoch 3482, Loss: 0.9914965480566025, Final Batch Loss: 0.28075945377349854\n",
      "Epoch 3483, Loss: 0.8768706917762756, Final Batch Loss: 0.20047003030776978\n",
      "Epoch 3484, Loss: 0.9361220747232437, Final Batch Loss: 0.26943111419677734\n",
      "Epoch 3485, Loss: 1.0275840163230896, Final Batch Loss: 0.2288883477449417\n",
      "Epoch 3486, Loss: 1.1429623663425446, Final Batch Loss: 0.2663542926311493\n",
      "Epoch 3487, Loss: 0.8810422867536545, Final Batch Loss: 0.20297814905643463\n",
      "Epoch 3488, Loss: 0.9787032753229141, Final Batch Loss: 0.2617419362068176\n",
      "Epoch 3489, Loss: 0.9444950371980667, Final Batch Loss: 0.2265227884054184\n",
      "Epoch 3490, Loss: 0.9282269924879074, Final Batch Loss: 0.2861666977405548\n",
      "Epoch 3491, Loss: 0.8487464934587479, Final Batch Loss: 0.2334931194782257\n",
      "Epoch 3492, Loss: 1.0663943588733673, Final Batch Loss: 0.33588671684265137\n",
      "Epoch 3493, Loss: 1.0864292085170746, Final Batch Loss: 0.3574415445327759\n",
      "Epoch 3494, Loss: 0.9014228582382202, Final Batch Loss: 0.21822740137577057\n",
      "Epoch 3495, Loss: 0.8891844749450684, Final Batch Loss: 0.22863826155662537\n",
      "Epoch 3496, Loss: 0.8616722226142883, Final Batch Loss: 0.20803076028823853\n",
      "Epoch 3497, Loss: 1.0347616970539093, Final Batch Loss: 0.2579902112483978\n",
      "Epoch 3498, Loss: 0.848448634147644, Final Batch Loss: 0.20084519684314728\n",
      "Epoch 3499, Loss: 0.980056956410408, Final Batch Loss: 0.278067409992218\n",
      "Epoch 3500, Loss: 0.943339928984642, Final Batch Loss: 0.23279865086078644\n",
      "Epoch 3501, Loss: 0.9418147951364517, Final Batch Loss: 0.15477879345417023\n",
      "Epoch 3502, Loss: 0.8308493345975876, Final Batch Loss: 0.22110649943351746\n",
      "Epoch 3503, Loss: 0.8799132108688354, Final Batch Loss: 0.1823640614748001\n",
      "Epoch 3504, Loss: 1.0668528974056244, Final Batch Loss: 0.2603929936885834\n",
      "Epoch 3505, Loss: 1.077796846628189, Final Batch Loss: 0.3072569966316223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3506, Loss: 0.9390560984611511, Final Batch Loss: 0.2370654046535492\n",
      "Epoch 3507, Loss: 1.1426027119159698, Final Batch Loss: 0.2858175039291382\n",
      "Epoch 3508, Loss: 1.0248095244169235, Final Batch Loss: 0.25586041808128357\n",
      "Epoch 3509, Loss: 0.9220519959926605, Final Batch Loss: 0.22242335975170135\n",
      "Epoch 3510, Loss: 0.995297908782959, Final Batch Loss: 0.21664708852767944\n",
      "Epoch 3511, Loss: 0.9244801998138428, Final Batch Loss: 0.2468966841697693\n",
      "Epoch 3512, Loss: 0.9692555367946625, Final Batch Loss: 0.2423320859670639\n",
      "Epoch 3513, Loss: 0.9228190034627914, Final Batch Loss: 0.25352784991264343\n",
      "Epoch 3514, Loss: 0.9426545947790146, Final Batch Loss: 0.23038530349731445\n",
      "Epoch 3515, Loss: 0.9505131393671036, Final Batch Loss: 0.2607528269290924\n",
      "Epoch 3516, Loss: 1.0019104480743408, Final Batch Loss: 0.20858462154865265\n",
      "Epoch 3517, Loss: 1.120506837964058, Final Batch Loss: 0.22575949132442474\n",
      "Epoch 3518, Loss: 1.1062251180410385, Final Batch Loss: 0.27651819586753845\n",
      "Epoch 3519, Loss: 1.0114658772945404, Final Batch Loss: 0.3421013653278351\n",
      "Epoch 3520, Loss: 0.9799097180366516, Final Batch Loss: 0.27889004349708557\n",
      "Epoch 3521, Loss: 0.9602550119161606, Final Batch Loss: 0.27169328927993774\n",
      "Epoch 3522, Loss: 0.968071848154068, Final Batch Loss: 0.25736066699028015\n",
      "Epoch 3523, Loss: 0.9061921089887619, Final Batch Loss: 0.2248908281326294\n",
      "Epoch 3524, Loss: 1.0099722892045975, Final Batch Loss: 0.18844686448574066\n",
      "Epoch 3525, Loss: 0.9438671767711639, Final Batch Loss: 0.24655397236347198\n",
      "Epoch 3526, Loss: 1.0023518353700638, Final Batch Loss: 0.2454291582107544\n",
      "Epoch 3527, Loss: 0.9815110713243484, Final Batch Loss: 0.24002857506275177\n",
      "Epoch 3528, Loss: 0.9977709501981735, Final Batch Loss: 0.2811793088912964\n",
      "Epoch 3529, Loss: 0.889535516500473, Final Batch Loss: 0.23547151684761047\n",
      "Epoch 3530, Loss: 1.094530627131462, Final Batch Loss: 0.22246067225933075\n",
      "Epoch 3531, Loss: 0.9706800729036331, Final Batch Loss: 0.2858780324459076\n",
      "Epoch 3532, Loss: 0.9763334542512894, Final Batch Loss: 0.2541952431201935\n",
      "Epoch 3533, Loss: 0.9740703701972961, Final Batch Loss: 0.2837303578853607\n",
      "Epoch 3534, Loss: 0.9064466655254364, Final Batch Loss: 0.2642083168029785\n",
      "Epoch 3535, Loss: 1.0031097680330276, Final Batch Loss: 0.22078996896743774\n",
      "Epoch 3536, Loss: 0.83775794506073, Final Batch Loss: 0.2431795299053192\n",
      "Epoch 3537, Loss: 0.922216922044754, Final Batch Loss: 0.1840391904115677\n",
      "Epoch 3538, Loss: 0.9455240666866302, Final Batch Loss: 0.1876818686723709\n",
      "Epoch 3539, Loss: 1.0103569328784943, Final Batch Loss: 0.2768038511276245\n",
      "Epoch 3540, Loss: 1.0406021773815155, Final Batch Loss: 0.22606982290744781\n",
      "Epoch 3541, Loss: 1.0080437362194061, Final Batch Loss: 0.34501466155052185\n",
      "Epoch 3542, Loss: 0.9569668620824814, Final Batch Loss: 0.25678750872612\n",
      "Epoch 3543, Loss: 0.9641371518373489, Final Batch Loss: 0.2787807285785675\n",
      "Epoch 3544, Loss: 1.004622682929039, Final Batch Loss: 0.25701242685317993\n",
      "Epoch 3545, Loss: 1.0807993710041046, Final Batch Loss: 0.23307645320892334\n",
      "Epoch 3546, Loss: 0.9211084544658661, Final Batch Loss: 0.20601606369018555\n",
      "Epoch 3547, Loss: 0.9467881172895432, Final Batch Loss: 0.25602951645851135\n",
      "Epoch 3548, Loss: 0.9482585340738297, Final Batch Loss: 0.2371244728565216\n",
      "Epoch 3549, Loss: 1.0167350620031357, Final Batch Loss: 0.24661274254322052\n",
      "Epoch 3550, Loss: 0.888738751411438, Final Batch Loss: 0.19762183725833893\n",
      "Epoch 3551, Loss: 0.8774569481611252, Final Batch Loss: 0.21449701488018036\n",
      "Epoch 3552, Loss: 0.8070225268602371, Final Batch Loss: 0.22321398556232452\n",
      "Epoch 3553, Loss: 0.9004009962081909, Final Batch Loss: 0.2647179365158081\n",
      "Epoch 3554, Loss: 0.9700532257556915, Final Batch Loss: 0.25510716438293457\n",
      "Epoch 3555, Loss: 1.0337167829275131, Final Batch Loss: 0.293190062046051\n",
      "Epoch 3556, Loss: 0.9383764863014221, Final Batch Loss: 0.1785682886838913\n",
      "Epoch 3557, Loss: 1.034713089466095, Final Batch Loss: 0.3048626780509949\n",
      "Epoch 3558, Loss: 0.9511904716491699, Final Batch Loss: 0.2114875763654709\n",
      "Epoch 3559, Loss: 1.001124769449234, Final Batch Loss: 0.31713205575942993\n",
      "Epoch 3560, Loss: 0.998807817697525, Final Batch Loss: 0.22198538482189178\n",
      "Epoch 3561, Loss: 1.0262365192174911, Final Batch Loss: 0.3052646219730377\n",
      "Epoch 3562, Loss: 0.962604284286499, Final Batch Loss: 0.3087868392467499\n",
      "Epoch 3563, Loss: 0.9771392196416855, Final Batch Loss: 0.21747109293937683\n",
      "Epoch 3564, Loss: 1.0497689992189407, Final Batch Loss: 0.3310968279838562\n",
      "Epoch 3565, Loss: 0.8599768131971359, Final Batch Loss: 0.2310383915901184\n",
      "Epoch 3566, Loss: 0.9414863288402557, Final Batch Loss: 0.292958527803421\n",
      "Epoch 3567, Loss: 0.9046957343816757, Final Batch Loss: 0.20270586013793945\n",
      "Epoch 3568, Loss: 1.1007803827524185, Final Batch Loss: 0.228835329413414\n",
      "Epoch 3569, Loss: 0.9251793622970581, Final Batch Loss: 0.24233156442642212\n",
      "Epoch 3570, Loss: 0.9637331962585449, Final Batch Loss: 0.25903621315956116\n",
      "Epoch 3571, Loss: 1.0469002574682236, Final Batch Loss: 0.1972993016242981\n",
      "Epoch 3572, Loss: 0.8709841370582581, Final Batch Loss: 0.21111804246902466\n",
      "Epoch 3573, Loss: 1.0012872964143753, Final Batch Loss: 0.32772189378738403\n",
      "Epoch 3574, Loss: 0.8863931894302368, Final Batch Loss: 0.2770812511444092\n",
      "Epoch 3575, Loss: 0.8712446540594101, Final Batch Loss: 0.19937364757061005\n",
      "Epoch 3576, Loss: 0.9517425000667572, Final Batch Loss: 0.2094404548406601\n",
      "Epoch 3577, Loss: 0.8626560866832733, Final Batch Loss: 0.1784074753522873\n",
      "Epoch 3578, Loss: 1.0065351873636246, Final Batch Loss: 0.2287861555814743\n",
      "Epoch 3579, Loss: 0.9962143450975418, Final Batch Loss: 0.26291075348854065\n",
      "Epoch 3580, Loss: 1.0396541059017181, Final Batch Loss: 0.24336759746074677\n",
      "Epoch 3581, Loss: 1.004938006401062, Final Batch Loss: 0.2196231633424759\n",
      "Epoch 3582, Loss: 1.0351285189390182, Final Batch Loss: 0.3171423077583313\n",
      "Epoch 3583, Loss: 1.105327382683754, Final Batch Loss: 0.243550106883049\n",
      "Epoch 3584, Loss: 0.9417849332094193, Final Batch Loss: 0.2379698008298874\n",
      "Epoch 3585, Loss: 0.9691873639822006, Final Batch Loss: 0.1510767638683319\n",
      "Epoch 3586, Loss: 1.101454347372055, Final Batch Loss: 0.3162015676498413\n",
      "Epoch 3587, Loss: 1.0132521986961365, Final Batch Loss: 0.32223033905029297\n",
      "Epoch 3588, Loss: 0.9055467545986176, Final Batch Loss: 0.17196568846702576\n",
      "Epoch 3589, Loss: 0.9244956970214844, Final Batch Loss: 0.20132924616336823\n",
      "Epoch 3590, Loss: 0.8307386636734009, Final Batch Loss: 0.22884590923786163\n",
      "Epoch 3591, Loss: 0.9291480034589767, Final Batch Loss: 0.17994503676891327\n",
      "Epoch 3592, Loss: 0.8691480755805969, Final Batch Loss: 0.15252374112606049\n",
      "Epoch 3593, Loss: 1.000573605298996, Final Batch Loss: 0.37188297510147095\n",
      "Epoch 3594, Loss: 0.85933817923069, Final Batch Loss: 0.17985431849956512\n",
      "Epoch 3595, Loss: 0.8520796746015549, Final Batch Loss: 0.23052576184272766\n",
      "Epoch 3596, Loss: 0.8935378342866898, Final Batch Loss: 0.25147098302841187\n",
      "Epoch 3597, Loss: 0.9796450138092041, Final Batch Loss: 0.2728666663169861\n",
      "Epoch 3598, Loss: 0.8547706454992294, Final Batch Loss: 0.2748241126537323\n",
      "Epoch 3599, Loss: 0.9548022001981735, Final Batch Loss: 0.17553295195102692\n",
      "Epoch 3600, Loss: 0.878102108836174, Final Batch Loss: 0.20195810496807098\n",
      "Epoch 3601, Loss: 0.8655642867088318, Final Batch Loss: 0.1921723335981369\n",
      "Epoch 3602, Loss: 0.9851047098636627, Final Batch Loss: 0.18071632087230682\n",
      "Epoch 3603, Loss: 0.8880577236413956, Final Batch Loss: 0.25360915064811707\n",
      "Epoch 3604, Loss: 1.040001019835472, Final Batch Loss: 0.2886202335357666\n",
      "Epoch 3605, Loss: 1.0132991969585419, Final Batch Loss: 0.19449970126152039\n",
      "Epoch 3606, Loss: 0.9904071688652039, Final Batch Loss: 0.3084511458873749\n",
      "Epoch 3607, Loss: 0.9485121965408325, Final Batch Loss: 0.25408267974853516\n",
      "Epoch 3608, Loss: 1.1435935944318771, Final Batch Loss: 0.3170790672302246\n",
      "Epoch 3609, Loss: 1.0420291125774384, Final Batch Loss: 0.2574597895145416\n",
      "Epoch 3610, Loss: 0.8959705978631973, Final Batch Loss: 0.24972210824489594\n",
      "Epoch 3611, Loss: 0.9320275038480759, Final Batch Loss: 0.3472558259963989\n",
      "Epoch 3612, Loss: 1.028028279542923, Final Batch Loss: 0.2814566195011139\n",
      "Epoch 3613, Loss: 0.9093842208385468, Final Batch Loss: 0.25253307819366455\n",
      "Epoch 3614, Loss: 0.9655500799417496, Final Batch Loss: 0.20162849128246307\n",
      "Epoch 3615, Loss: 0.9714382737874985, Final Batch Loss: 0.24665048718452454\n",
      "Epoch 3616, Loss: 0.9652650058269501, Final Batch Loss: 0.26914042234420776\n",
      "Epoch 3617, Loss: 0.9424668103456497, Final Batch Loss: 0.24475690722465515\n",
      "Epoch 3618, Loss: 0.94183249771595, Final Batch Loss: 0.21601417660713196\n",
      "Epoch 3619, Loss: 0.894495502114296, Final Batch Loss: 0.22639057040214539\n",
      "Epoch 3620, Loss: 0.9327701479196548, Final Batch Loss: 0.21641428768634796\n",
      "Epoch 3621, Loss: 0.8664141297340393, Final Batch Loss: 0.17221081256866455\n",
      "Epoch 3622, Loss: 0.899615615606308, Final Batch Loss: 0.24702566862106323\n",
      "Epoch 3623, Loss: 0.9299085140228271, Final Batch Loss: 0.2128540724515915\n",
      "Epoch 3624, Loss: 1.009379267692566, Final Batch Loss: 0.279390811920166\n",
      "Epoch 3625, Loss: 0.9980549365282059, Final Batch Loss: 0.24452055990695953\n",
      "Epoch 3626, Loss: 0.9395797848701477, Final Batch Loss: 0.2256237119436264\n",
      "Epoch 3627, Loss: 0.9592300951480865, Final Batch Loss: 0.24721087515354156\n",
      "Epoch 3628, Loss: 1.0227495282888412, Final Batch Loss: 0.3140416741371155\n",
      "Epoch 3629, Loss: 0.9801408499479294, Final Batch Loss: 0.27464935183525085\n",
      "Epoch 3630, Loss: 0.8697062879800797, Final Batch Loss: 0.19609862565994263\n",
      "Epoch 3631, Loss: 0.9254273921251297, Final Batch Loss: 0.2680239975452423\n",
      "Epoch 3632, Loss: 0.8930434435606003, Final Batch Loss: 0.23081552982330322\n",
      "Epoch 3633, Loss: 0.8539167642593384, Final Batch Loss: 0.23029853403568268\n",
      "Epoch 3634, Loss: 0.9408382624387741, Final Batch Loss: 0.24189592897891998\n",
      "Epoch 3635, Loss: 0.927402600646019, Final Batch Loss: 0.2483266443014145\n",
      "Epoch 3636, Loss: 0.8275879621505737, Final Batch Loss: 0.21490982174873352\n",
      "Epoch 3637, Loss: 0.9328154176473618, Final Batch Loss: 0.27500516176223755\n",
      "Epoch 3638, Loss: 0.8755266219377518, Final Batch Loss: 0.17982321977615356\n",
      "Epoch 3639, Loss: 0.9912827461957932, Final Batch Loss: 0.23783142864704132\n",
      "Epoch 3640, Loss: 0.9986691623926163, Final Batch Loss: 0.314256876707077\n",
      "Epoch 3641, Loss: 0.9389797151088715, Final Batch Loss: 0.24546483159065247\n",
      "Epoch 3642, Loss: 0.9283471256494522, Final Batch Loss: 0.24760210514068604\n",
      "Epoch 3643, Loss: 0.8876593858003616, Final Batch Loss: 0.2055264562368393\n",
      "Epoch 3644, Loss: 1.0202760845422745, Final Batch Loss: 0.2844674289226532\n",
      "Epoch 3645, Loss: 0.9377588927745819, Final Batch Loss: 0.16430030763149261\n",
      "Epoch 3646, Loss: 1.0272819548845291, Final Batch Loss: 0.22597911953926086\n",
      "Epoch 3647, Loss: 0.9619839638471603, Final Batch Loss: 0.26484569907188416\n",
      "Epoch 3648, Loss: 0.8133175820112228, Final Batch Loss: 0.2294217199087143\n",
      "Epoch 3649, Loss: 0.9820461124181747, Final Batch Loss: 0.2326854020357132\n",
      "Epoch 3650, Loss: 0.8899889439344406, Final Batch Loss: 0.2197704315185547\n",
      "Epoch 3651, Loss: 0.9432788491249084, Final Batch Loss: 0.27888306975364685\n",
      "Epoch 3652, Loss: 0.8476237505674362, Final Batch Loss: 0.1805552989244461\n",
      "Epoch 3653, Loss: 0.9843023270368576, Final Batch Loss: 0.1891360580921173\n",
      "Epoch 3654, Loss: 0.9393748044967651, Final Batch Loss: 0.23567111790180206\n",
      "Epoch 3655, Loss: 0.9848299920558929, Final Batch Loss: 0.25886839628219604\n",
      "Epoch 3656, Loss: 1.020956039428711, Final Batch Loss: 0.25938716530799866\n",
      "Epoch 3657, Loss: 1.0142541527748108, Final Batch Loss: 0.23191525042057037\n",
      "Epoch 3658, Loss: 0.8892959654331207, Final Batch Loss: 0.17391672730445862\n",
      "Epoch 3659, Loss: 0.9305371940135956, Final Batch Loss: 0.18481694161891937\n",
      "Epoch 3660, Loss: 1.01327782869339, Final Batch Loss: 0.26694610714912415\n",
      "Epoch 3661, Loss: 0.9391306787729263, Final Batch Loss: 0.2393152415752411\n",
      "Epoch 3662, Loss: 0.8684621751308441, Final Batch Loss: 0.19801656901836395\n",
      "Epoch 3663, Loss: 1.1244088113307953, Final Batch Loss: 0.2931652367115021\n",
      "Epoch 3664, Loss: 0.9434214532375336, Final Batch Loss: 0.23745238780975342\n",
      "Epoch 3665, Loss: 0.944035217165947, Final Batch Loss: 0.2952949106693268\n",
      "Epoch 3666, Loss: 0.9594121426343918, Final Batch Loss: 0.19262392818927765\n",
      "Epoch 3667, Loss: 0.8974255323410034, Final Batch Loss: 0.25295355916023254\n",
      "Epoch 3668, Loss: 0.9130837619304657, Final Batch Loss: 0.22776155173778534\n",
      "Epoch 3669, Loss: 1.0244962722063065, Final Batch Loss: 0.2688091993331909\n",
      "Epoch 3670, Loss: 1.0792645961046219, Final Batch Loss: 0.248393252491951\n",
      "Epoch 3671, Loss: 0.9277008324861526, Final Batch Loss: 0.25097039341926575\n",
      "Epoch 3672, Loss: 0.8389679044485092, Final Batch Loss: 0.25202640891075134\n",
      "Epoch 3673, Loss: 0.9146925061941147, Final Batch Loss: 0.19416452944278717\n",
      "Epoch 3674, Loss: 0.8815584480762482, Final Batch Loss: 0.26285210251808167\n",
      "Epoch 3675, Loss: 0.8960966020822525, Final Batch Loss: 0.20649883151054382\n",
      "Epoch 3676, Loss: 0.9417047649621964, Final Batch Loss: 0.3116035759449005\n",
      "Epoch 3677, Loss: 0.932793378829956, Final Batch Loss: 0.28893911838531494\n",
      "Epoch 3678, Loss: 1.0743109583854675, Final Batch Loss: 0.2644670307636261\n",
      "Epoch 3679, Loss: 1.0233981311321259, Final Batch Loss: 0.27083975076675415\n",
      "Epoch 3680, Loss: 0.997582346200943, Final Batch Loss: 0.33433443307876587\n",
      "Epoch 3681, Loss: 0.9206859767436981, Final Batch Loss: 0.21115919947624207\n",
      "Epoch 3682, Loss: 1.0361529737710953, Final Batch Loss: 0.27843987941741943\n",
      "Epoch 3683, Loss: 0.9902683645486832, Final Batch Loss: 0.27300068736076355\n",
      "Epoch 3684, Loss: 0.882451742887497, Final Batch Loss: 0.22636646032333374\n",
      "Epoch 3685, Loss: 0.9494094848632812, Final Batch Loss: 0.21245719492435455\n",
      "Epoch 3686, Loss: 0.8541218191385269, Final Batch Loss: 0.25137650966644287\n",
      "Epoch 3687, Loss: 0.9788751006126404, Final Batch Loss: 0.21461163461208344\n",
      "Epoch 3688, Loss: 1.0254597812891006, Final Batch Loss: 0.2653922736644745\n",
      "Epoch 3689, Loss: 0.9272249937057495, Final Batch Loss: 0.22354058921337128\n",
      "Epoch 3690, Loss: 1.0586403757333755, Final Batch Loss: 0.2806585431098938\n",
      "Epoch 3691, Loss: 0.8974768221378326, Final Batch Loss: 0.2615920603275299\n",
      "Epoch 3692, Loss: 0.8595954775810242, Final Batch Loss: 0.24940167367458344\n",
      "Epoch 3693, Loss: 0.9027622491121292, Final Batch Loss: 0.21873386204242706\n",
      "Epoch 3694, Loss: 1.0150076150894165, Final Batch Loss: 0.2423795610666275\n",
      "Epoch 3695, Loss: 0.9783479422330856, Final Batch Loss: 0.23204952478408813\n",
      "Epoch 3696, Loss: 0.8750180751085281, Final Batch Loss: 0.2551124393939972\n",
      "Epoch 3697, Loss: 0.9654316306114197, Final Batch Loss: 0.3136138617992401\n",
      "Epoch 3698, Loss: 0.9624570161104202, Final Batch Loss: 0.25999975204467773\n",
      "Epoch 3699, Loss: 1.0842193216085434, Final Batch Loss: 0.32615911960601807\n",
      "Epoch 3700, Loss: 0.9312077611684799, Final Batch Loss: 0.35925763845443726\n",
      "Epoch 3701, Loss: 0.9066383093595505, Final Batch Loss: 0.250301331281662\n",
      "Epoch 3702, Loss: 0.9950990527868271, Final Batch Loss: 0.22457262873649597\n",
      "Epoch 3703, Loss: 1.0184312909841537, Final Batch Loss: 0.28202128410339355\n",
      "Epoch 3704, Loss: 1.050964504480362, Final Batch Loss: 0.24405114352703094\n",
      "Epoch 3705, Loss: 0.9375655055046082, Final Batch Loss: 0.2619664669036865\n",
      "Epoch 3706, Loss: 0.8092761635780334, Final Batch Loss: 0.21325677633285522\n",
      "Epoch 3707, Loss: 0.9451031386852264, Final Batch Loss: 0.22397851943969727\n",
      "Epoch 3708, Loss: 0.8620555549860001, Final Batch Loss: 0.25356146693229675\n",
      "Epoch 3709, Loss: 0.9424145668745041, Final Batch Loss: 0.2170688956975937\n",
      "Epoch 3710, Loss: 0.7998631298542023, Final Batch Loss: 0.22784171998500824\n",
      "Epoch 3711, Loss: 0.8160028606653214, Final Batch Loss: 0.2283342033624649\n",
      "Epoch 3712, Loss: 0.8597510308027267, Final Batch Loss: 0.15280742943286896\n",
      "Epoch 3713, Loss: 0.9488949179649353, Final Batch Loss: 0.24243128299713135\n",
      "Epoch 3714, Loss: 0.9355943948030472, Final Batch Loss: 0.2083425372838974\n",
      "Epoch 3715, Loss: 0.9083765149116516, Final Batch Loss: 0.1897711306810379\n",
      "Epoch 3716, Loss: 0.8849719762802124, Final Batch Loss: 0.20661669969558716\n",
      "Epoch 3717, Loss: 0.9710397124290466, Final Batch Loss: 0.2479620724916458\n",
      "Epoch 3718, Loss: 0.8897307813167572, Final Batch Loss: 0.2241043746471405\n",
      "Epoch 3719, Loss: 0.9008785337209702, Final Batch Loss: 0.22674979269504547\n",
      "Epoch 3720, Loss: 1.07001231610775, Final Batch Loss: 0.2749808728694916\n",
      "Epoch 3721, Loss: 0.9945971220731735, Final Batch Loss: 0.22413231432437897\n",
      "Epoch 3722, Loss: 0.9296279549598694, Final Batch Loss: 0.2509204149246216\n",
      "Epoch 3723, Loss: 0.8602257519960403, Final Batch Loss: 0.22373570501804352\n",
      "Epoch 3724, Loss: 1.0057440847158432, Final Batch Loss: 0.23886267840862274\n",
      "Epoch 3725, Loss: 0.9092441946268082, Final Batch Loss: 0.19335681200027466\n",
      "Epoch 3726, Loss: 0.9035866856575012, Final Batch Loss: 0.24992334842681885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3727, Loss: 1.0106701701879501, Final Batch Loss: 0.2605767250061035\n",
      "Epoch 3728, Loss: 0.8880303502082825, Final Batch Loss: 0.18917711079120636\n",
      "Epoch 3729, Loss: 1.1060091257095337, Final Batch Loss: 0.2916841506958008\n",
      "Epoch 3730, Loss: 1.0372851639986038, Final Batch Loss: 0.3291027247905731\n",
      "Epoch 3731, Loss: 0.8801105618476868, Final Batch Loss: 0.22723805904388428\n",
      "Epoch 3732, Loss: 0.9782207012176514, Final Batch Loss: 0.2710692286491394\n",
      "Epoch 3733, Loss: 0.8548172265291214, Final Batch Loss: 0.24535472691059113\n",
      "Epoch 3734, Loss: 0.9352899938821793, Final Batch Loss: 0.17473158240318298\n",
      "Epoch 3735, Loss: 0.8647712171077728, Final Batch Loss: 0.24430181086063385\n",
      "Epoch 3736, Loss: 0.9348851144313812, Final Batch Loss: 0.21509695053100586\n",
      "Epoch 3737, Loss: 0.8722140341997147, Final Batch Loss: 0.26646482944488525\n",
      "Epoch 3738, Loss: 0.8808210045099258, Final Batch Loss: 0.23000206053256989\n",
      "Epoch 3739, Loss: 0.8911599665880203, Final Batch Loss: 0.20465855300426483\n",
      "Epoch 3740, Loss: 0.8882652372121811, Final Batch Loss: 0.23488055169582367\n",
      "Epoch 3741, Loss: 0.8975889086723328, Final Batch Loss: 0.24335433542728424\n",
      "Epoch 3742, Loss: 0.8793278187513351, Final Batch Loss: 0.2469247430562973\n",
      "Epoch 3743, Loss: 0.9046218544244766, Final Batch Loss: 0.283256858587265\n",
      "Epoch 3744, Loss: 0.9812819361686707, Final Batch Loss: 0.24401526153087616\n",
      "Epoch 3745, Loss: 0.9327444732189178, Final Batch Loss: 0.16570334136486053\n",
      "Epoch 3746, Loss: 0.9663408994674683, Final Batch Loss: 0.30071932077407837\n",
      "Epoch 3747, Loss: 0.8566966950893402, Final Batch Loss: 0.20500801503658295\n",
      "Epoch 3748, Loss: 0.9867507666349411, Final Batch Loss: 0.2860768735408783\n",
      "Epoch 3749, Loss: 0.7901534587144852, Final Batch Loss: 0.16397550702095032\n",
      "Epoch 3750, Loss: 0.9366366267204285, Final Batch Loss: 0.26232650876045227\n",
      "Epoch 3751, Loss: 1.0275646150112152, Final Batch Loss: 0.3289174735546112\n",
      "Epoch 3752, Loss: 0.9063323438167572, Final Batch Loss: 0.14765450358390808\n",
      "Epoch 3753, Loss: 0.8419940322637558, Final Batch Loss: 0.19604244828224182\n",
      "Epoch 3754, Loss: 1.050886020064354, Final Batch Loss: 0.2331937998533249\n",
      "Epoch 3755, Loss: 1.1144338250160217, Final Batch Loss: 0.27254778146743774\n",
      "Epoch 3756, Loss: 0.8955615758895874, Final Batch Loss: 0.2413543313741684\n",
      "Epoch 3757, Loss: 0.9890783280134201, Final Batch Loss: 0.19226424396038055\n",
      "Epoch 3758, Loss: 0.8698723167181015, Final Batch Loss: 0.19458486139774323\n",
      "Epoch 3759, Loss: 1.0302324891090393, Final Batch Loss: 0.278281569480896\n",
      "Epoch 3760, Loss: 0.9393036365509033, Final Batch Loss: 0.23572230339050293\n",
      "Epoch 3761, Loss: 0.9859714806079865, Final Batch Loss: 0.2140427678823471\n",
      "Epoch 3762, Loss: 1.009168654680252, Final Batch Loss: 0.18137729167938232\n",
      "Epoch 3763, Loss: 0.8334872126579285, Final Batch Loss: 0.1859113723039627\n",
      "Epoch 3764, Loss: 0.9071190357208252, Final Batch Loss: 0.18913660943508148\n",
      "Epoch 3765, Loss: 0.9688809663057327, Final Batch Loss: 0.16172926127910614\n",
      "Epoch 3766, Loss: 0.9360412210226059, Final Batch Loss: 0.14768411219120026\n",
      "Epoch 3767, Loss: 0.9260867834091187, Final Batch Loss: 0.16287626326084137\n",
      "Epoch 3768, Loss: 1.0034828633069992, Final Batch Loss: 0.17279604077339172\n",
      "Epoch 3769, Loss: 0.8963486850261688, Final Batch Loss: 0.21982522308826447\n",
      "Epoch 3770, Loss: 0.9405714869499207, Final Batch Loss: 0.17940089106559753\n",
      "Epoch 3771, Loss: 0.9081901460886002, Final Batch Loss: 0.18328626453876495\n",
      "Epoch 3772, Loss: 0.9575006812810898, Final Batch Loss: 0.2800304591655731\n",
      "Epoch 3773, Loss: 0.9730519652366638, Final Batch Loss: 0.2529677748680115\n",
      "Epoch 3774, Loss: 0.9210334420204163, Final Batch Loss: 0.2114727795124054\n",
      "Epoch 3775, Loss: 0.8823321461677551, Final Batch Loss: 0.1836152821779251\n",
      "Epoch 3776, Loss: 0.9749531596899033, Final Batch Loss: 0.2718312442302704\n",
      "Epoch 3777, Loss: 0.8414636701345444, Final Batch Loss: 0.21749357879161835\n",
      "Epoch 3778, Loss: 1.0599330812692642, Final Batch Loss: 0.21566693484783173\n",
      "Epoch 3779, Loss: 0.8407543301582336, Final Batch Loss: 0.20247207581996918\n",
      "Epoch 3780, Loss: 0.9927549064159393, Final Batch Loss: 0.23058933019638062\n",
      "Epoch 3781, Loss: 0.9770816415548325, Final Batch Loss: 0.27663296461105347\n",
      "Epoch 3782, Loss: 0.9720098078250885, Final Batch Loss: 0.21470755338668823\n",
      "Epoch 3783, Loss: 0.8460382223129272, Final Batch Loss: 0.22834952175617218\n",
      "Epoch 3784, Loss: 0.931470513343811, Final Batch Loss: 0.22660380601882935\n",
      "Epoch 3785, Loss: 0.8388594835996628, Final Batch Loss: 0.2616328001022339\n",
      "Epoch 3786, Loss: 0.8872335255146027, Final Batch Loss: 0.21942473948001862\n",
      "Epoch 3787, Loss: 0.9201126098632812, Final Batch Loss: 0.25213634967803955\n",
      "Epoch 3788, Loss: 0.9001938253641129, Final Batch Loss: 0.22922182083129883\n",
      "Epoch 3789, Loss: 0.9817474335432053, Final Batch Loss: 0.3122549057006836\n",
      "Epoch 3790, Loss: 0.8281071782112122, Final Batch Loss: 0.2365865856409073\n",
      "Epoch 3791, Loss: 0.952983483672142, Final Batch Loss: 0.2955631613731384\n",
      "Epoch 3792, Loss: 0.9561070650815964, Final Batch Loss: 0.2369830310344696\n",
      "Epoch 3793, Loss: 0.9384397566318512, Final Batch Loss: 0.23178812861442566\n",
      "Epoch 3794, Loss: 0.8884480148553848, Final Batch Loss: 0.2486821711063385\n",
      "Epoch 3795, Loss: 0.8717273324728012, Final Batch Loss: 0.20221242308616638\n",
      "Epoch 3796, Loss: 1.038894146680832, Final Batch Loss: 0.20281869173049927\n",
      "Epoch 3797, Loss: 0.9051308929920197, Final Batch Loss: 0.2375505417585373\n",
      "Epoch 3798, Loss: 0.8642090708017349, Final Batch Loss: 0.21959145367145538\n",
      "Epoch 3799, Loss: 0.9860895127058029, Final Batch Loss: 0.2171313762664795\n",
      "Epoch 3800, Loss: 0.9174573421478271, Final Batch Loss: 0.206030011177063\n",
      "Epoch 3801, Loss: 0.9523738920688629, Final Batch Loss: 0.254736065864563\n",
      "Epoch 3802, Loss: 0.9846561402082443, Final Batch Loss: 0.2362775057554245\n",
      "Epoch 3803, Loss: 0.965536817908287, Final Batch Loss: 0.1925867795944214\n",
      "Epoch 3804, Loss: 0.9769553691148758, Final Batch Loss: 0.2531828284263611\n",
      "Epoch 3805, Loss: 0.9132703691720963, Final Batch Loss: 0.25550442934036255\n",
      "Epoch 3806, Loss: 0.910631000995636, Final Batch Loss: 0.22051438689231873\n",
      "Epoch 3807, Loss: 0.8956136256456375, Final Batch Loss: 0.17044265568256378\n",
      "Epoch 3808, Loss: 0.8847738653421402, Final Batch Loss: 0.14992530643939972\n",
      "Epoch 3809, Loss: 1.0036081075668335, Final Batch Loss: 0.2493177354335785\n",
      "Epoch 3810, Loss: 0.939584955573082, Final Batch Loss: 0.2034178525209427\n",
      "Epoch 3811, Loss: 0.8481750190258026, Final Batch Loss: 0.20135538280010223\n",
      "Epoch 3812, Loss: 0.9855659753084183, Final Batch Loss: 0.24675409495830536\n",
      "Epoch 3813, Loss: 0.932116374373436, Final Batch Loss: 0.19132129848003387\n",
      "Epoch 3814, Loss: 0.9648481160402298, Final Batch Loss: 0.23492252826690674\n",
      "Epoch 3815, Loss: 0.9684443771839142, Final Batch Loss: 0.21727019548416138\n",
      "Epoch 3816, Loss: 1.0095050185918808, Final Batch Loss: 0.24989518523216248\n",
      "Epoch 3817, Loss: 0.900686725974083, Final Batch Loss: 0.25002142786979675\n",
      "Epoch 3818, Loss: 0.9693206697702408, Final Batch Loss: 0.21427021920681\n",
      "Epoch 3819, Loss: 0.9683414399623871, Final Batch Loss: 0.27344226837158203\n",
      "Epoch 3820, Loss: 0.9483456611633301, Final Batch Loss: 0.23651015758514404\n",
      "Epoch 3821, Loss: 0.9818132519721985, Final Batch Loss: 0.2071089744567871\n",
      "Epoch 3822, Loss: 0.9128850400447845, Final Batch Loss: 0.20595724880695343\n",
      "Epoch 3823, Loss: 0.826648399233818, Final Batch Loss: 0.24941857159137726\n",
      "Epoch 3824, Loss: 0.943960964679718, Final Batch Loss: 0.22434867918491364\n",
      "Epoch 3825, Loss: 0.9941769987344742, Final Batch Loss: 0.22760345041751862\n",
      "Epoch 3826, Loss: 0.7785835415124893, Final Batch Loss: 0.20304813981056213\n",
      "Epoch 3827, Loss: 1.0398707687854767, Final Batch Loss: 0.23177404701709747\n",
      "Epoch 3828, Loss: 0.9044329077005386, Final Batch Loss: 0.22536076605319977\n",
      "Epoch 3829, Loss: 0.9761144816875458, Final Batch Loss: 0.24279481172561646\n",
      "Epoch 3830, Loss: 0.9393458217382431, Final Batch Loss: 0.3129589259624481\n",
      "Epoch 3831, Loss: 0.994705006480217, Final Batch Loss: 0.1946079134941101\n",
      "Epoch 3832, Loss: 0.9219760745763779, Final Batch Loss: 0.25173819065093994\n",
      "Epoch 3833, Loss: 0.8906260132789612, Final Batch Loss: 0.2005515843629837\n",
      "Epoch 3834, Loss: 0.9297351688146591, Final Batch Loss: 0.30275651812553406\n",
      "Epoch 3835, Loss: 0.9263494610786438, Final Batch Loss: 0.2692347764968872\n",
      "Epoch 3836, Loss: 0.9866003543138504, Final Batch Loss: 0.31224754452705383\n",
      "Epoch 3837, Loss: 0.9153240025043488, Final Batch Loss: 0.2418159395456314\n",
      "Epoch 3838, Loss: 0.9253955483436584, Final Batch Loss: 0.20703181624412537\n",
      "Epoch 3839, Loss: 0.9868976771831512, Final Batch Loss: 0.32972991466522217\n",
      "Epoch 3840, Loss: 1.0694418102502823, Final Batch Loss: 0.3309797942638397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3841, Loss: 0.9001382887363434, Final Batch Loss: 0.2736051082611084\n",
      "Epoch 3842, Loss: 0.9610363841056824, Final Batch Loss: 0.21831835806369781\n",
      "Epoch 3843, Loss: 0.9795819967985153, Final Batch Loss: 0.1749785989522934\n",
      "Epoch 3844, Loss: 0.9139861017465591, Final Batch Loss: 0.20953720808029175\n",
      "Epoch 3845, Loss: 0.891092836856842, Final Batch Loss: 0.17848962545394897\n",
      "Epoch 3846, Loss: 0.8062241226434708, Final Batch Loss: 0.18284401297569275\n",
      "Epoch 3847, Loss: 0.9187790602445602, Final Batch Loss: 0.23397518694400787\n",
      "Epoch 3848, Loss: 0.8601159304380417, Final Batch Loss: 0.26218199729919434\n",
      "Epoch 3849, Loss: 0.9244054406881332, Final Batch Loss: 0.26239484548568726\n",
      "Epoch 3850, Loss: 0.8655291795730591, Final Batch Loss: 0.2213106006383896\n",
      "Epoch 3851, Loss: 0.8716924637556076, Final Batch Loss: 0.1614425778388977\n",
      "Epoch 3852, Loss: 0.9216591715812683, Final Batch Loss: 0.2856542766094208\n",
      "Epoch 3853, Loss: 0.9139545410871506, Final Batch Loss: 0.2835905849933624\n",
      "Epoch 3854, Loss: 0.8629147559404373, Final Batch Loss: 0.19358396530151367\n",
      "Epoch 3855, Loss: 0.9099411517381668, Final Batch Loss: 0.22475329041481018\n",
      "Epoch 3856, Loss: 0.9525950998067856, Final Batch Loss: 0.19113169610500336\n",
      "Epoch 3857, Loss: 0.8491966128349304, Final Batch Loss: 0.21505481004714966\n",
      "Epoch 3858, Loss: 0.9180132001638412, Final Batch Loss: 0.26649484038352966\n",
      "Epoch 3859, Loss: 0.8778047561645508, Final Batch Loss: 0.2730430066585541\n",
      "Epoch 3860, Loss: 0.9754292368888855, Final Batch Loss: 0.23552630841732025\n",
      "Epoch 3861, Loss: 0.9228124469518661, Final Batch Loss: 0.22862425446510315\n",
      "Epoch 3862, Loss: 0.8672934770584106, Final Batch Loss: 0.17732946574687958\n",
      "Epoch 3863, Loss: 0.8773250877857208, Final Batch Loss: 0.1509784609079361\n",
      "Epoch 3864, Loss: 0.9488140642642975, Final Batch Loss: 0.2512147128582001\n",
      "Epoch 3865, Loss: 0.9003294706344604, Final Batch Loss: 0.18880508840084076\n",
      "Epoch 3866, Loss: 0.8661417961120605, Final Batch Loss: 0.1841922551393509\n",
      "Epoch 3867, Loss: 0.8575016856193542, Final Batch Loss: 0.2090531289577484\n",
      "Epoch 3868, Loss: 0.9063652008771896, Final Batch Loss: 0.20315495133399963\n",
      "Epoch 3869, Loss: 0.8279985636472702, Final Batch Loss: 0.15627175569534302\n",
      "Epoch 3870, Loss: 0.880934089422226, Final Batch Loss: 0.20369203388690948\n",
      "Epoch 3871, Loss: 0.9476363658905029, Final Batch Loss: 0.23689238727092743\n",
      "Epoch 3872, Loss: 0.8489558398723602, Final Batch Loss: 0.25783810019493103\n",
      "Epoch 3873, Loss: 0.9580207318067551, Final Batch Loss: 0.2872020900249481\n",
      "Epoch 3874, Loss: 0.8432746231555939, Final Batch Loss: 0.2019110918045044\n",
      "Epoch 3875, Loss: 1.0271200686693192, Final Batch Loss: 0.28033891320228577\n",
      "Epoch 3876, Loss: 0.8805351555347443, Final Batch Loss: 0.1831127256155014\n",
      "Epoch 3877, Loss: 0.9561511874198914, Final Batch Loss: 0.1690896898508072\n",
      "Epoch 3878, Loss: 0.8469892293214798, Final Batch Loss: 0.14803849160671234\n",
      "Epoch 3879, Loss: 1.03169783949852, Final Batch Loss: 0.17535483837127686\n",
      "Epoch 3880, Loss: 0.8796607106924057, Final Batch Loss: 0.20942027866840363\n",
      "Epoch 3881, Loss: 0.9377597272396088, Final Batch Loss: 0.2100323736667633\n",
      "Epoch 3882, Loss: 0.9162838608026505, Final Batch Loss: 0.25542059540748596\n",
      "Epoch 3883, Loss: 0.8855970203876495, Final Batch Loss: 0.2639203667640686\n",
      "Epoch 3884, Loss: 0.9598924219608307, Final Batch Loss: 0.27619796991348267\n",
      "Epoch 3885, Loss: 0.9322905391454697, Final Batch Loss: 0.22594767808914185\n",
      "Epoch 3886, Loss: 0.9112264811992645, Final Batch Loss: 0.214253231883049\n",
      "Epoch 3887, Loss: 0.9562839716672897, Final Batch Loss: 0.22574542462825775\n",
      "Epoch 3888, Loss: 0.7670056968927383, Final Batch Loss: 0.15107472240924835\n",
      "Epoch 3889, Loss: 0.8865670561790466, Final Batch Loss: 0.20119495689868927\n",
      "Epoch 3890, Loss: 0.7490170747041702, Final Batch Loss: 0.17677824199199677\n",
      "Epoch 3891, Loss: 0.9126016497612, Final Batch Loss: 0.22986957430839539\n",
      "Epoch 3892, Loss: 1.0559396743774414, Final Batch Loss: 0.4417503774166107\n",
      "Epoch 3893, Loss: 0.9106049835681915, Final Batch Loss: 0.20754504203796387\n",
      "Epoch 3894, Loss: 0.9756143391132355, Final Batch Loss: 0.23781225085258484\n",
      "Epoch 3895, Loss: 0.8463770747184753, Final Batch Loss: 0.22150249779224396\n",
      "Epoch 3896, Loss: 0.8474868685007095, Final Batch Loss: 0.2294161170721054\n",
      "Epoch 3897, Loss: 0.9077288955450058, Final Batch Loss: 0.20055921375751495\n",
      "Epoch 3898, Loss: 0.9359414428472519, Final Batch Loss: 0.1566154509782791\n",
      "Epoch 3899, Loss: 0.7877050340175629, Final Batch Loss: 0.2148003876209259\n",
      "Epoch 3900, Loss: 0.9205691069364548, Final Batch Loss: 0.23971796035766602\n",
      "Epoch 3901, Loss: 0.9044362753629684, Final Batch Loss: 0.22392798960208893\n",
      "Epoch 3902, Loss: 0.7684128880500793, Final Batch Loss: 0.19673702120780945\n",
      "Epoch 3903, Loss: 0.9534275680780411, Final Batch Loss: 0.2576734125614166\n",
      "Epoch 3904, Loss: 0.850702553987503, Final Batch Loss: 0.21751028299331665\n",
      "Epoch 3905, Loss: 0.8177899420261383, Final Batch Loss: 0.22940266132354736\n",
      "Epoch 3906, Loss: 0.909395158290863, Final Batch Loss: 0.24983365833759308\n",
      "Epoch 3907, Loss: 0.8465111553668976, Final Batch Loss: 0.1710909903049469\n",
      "Epoch 3908, Loss: 0.8835828304290771, Final Batch Loss: 0.1937362104654312\n",
      "Epoch 3909, Loss: 0.8662153482437134, Final Batch Loss: 0.19613592326641083\n",
      "Epoch 3910, Loss: 0.9452589154243469, Final Batch Loss: 0.23346899449825287\n",
      "Epoch 3911, Loss: 0.8042086362838745, Final Batch Loss: 0.20982712507247925\n",
      "Epoch 3912, Loss: 0.8172656148672104, Final Batch Loss: 0.21357928216457367\n",
      "Epoch 3913, Loss: 1.0296446084976196, Final Batch Loss: 0.1911689043045044\n",
      "Epoch 3914, Loss: 0.9490913897752762, Final Batch Loss: 0.2066778689622879\n",
      "Epoch 3915, Loss: 0.8694983422756195, Final Batch Loss: 0.2363577038049698\n",
      "Epoch 3916, Loss: 0.8802097141742706, Final Batch Loss: 0.2327231913805008\n",
      "Epoch 3917, Loss: 0.8797171264886856, Final Batch Loss: 0.2180713415145874\n",
      "Epoch 3918, Loss: 0.8279095739126205, Final Batch Loss: 0.14673523604869843\n",
      "Epoch 3919, Loss: 0.9618635922670364, Final Batch Loss: 0.2612783908843994\n",
      "Epoch 3920, Loss: 1.0402074605226517, Final Batch Loss: 0.2330007702112198\n",
      "Epoch 3921, Loss: 0.8965909779071808, Final Batch Loss: 0.16140516102313995\n",
      "Epoch 3922, Loss: 0.8830587863922119, Final Batch Loss: 0.21485894918441772\n",
      "Epoch 3923, Loss: 1.0441272556781769, Final Batch Loss: 0.26126277446746826\n",
      "Epoch 3924, Loss: 0.9216216653585434, Final Batch Loss: 0.20543120801448822\n",
      "Epoch 3925, Loss: 0.9435457736253738, Final Batch Loss: 0.23416317999362946\n",
      "Epoch 3926, Loss: 1.0110242366790771, Final Batch Loss: 0.26110103726387024\n",
      "Epoch 3927, Loss: 0.9322696328163147, Final Batch Loss: 0.2552480101585388\n",
      "Epoch 3928, Loss: 0.9172837436199188, Final Batch Loss: 0.25004905462265015\n",
      "Epoch 3929, Loss: 0.829078733921051, Final Batch Loss: 0.22794073820114136\n",
      "Epoch 3930, Loss: 0.8857898414134979, Final Batch Loss: 0.27273398637771606\n",
      "Epoch 3931, Loss: 0.9082203954458237, Final Batch Loss: 0.28229373693466187\n",
      "Epoch 3932, Loss: 0.9570446759462357, Final Batch Loss: 0.33485373854637146\n",
      "Epoch 3933, Loss: 1.0329177528619766, Final Batch Loss: 0.3136606216430664\n",
      "Epoch 3934, Loss: 0.9536343663930893, Final Batch Loss: 0.23137067258358002\n",
      "Epoch 3935, Loss: 1.0018382221460342, Final Batch Loss: 0.2847382426261902\n",
      "Epoch 3936, Loss: 0.9758480042219162, Final Batch Loss: 0.2873010039329529\n",
      "Epoch 3937, Loss: 0.9637566655874252, Final Batch Loss: 0.24886387586593628\n",
      "Epoch 3938, Loss: 1.026065617799759, Final Batch Loss: 0.18717704713344574\n",
      "Epoch 3939, Loss: 1.0031649023294449, Final Batch Loss: 0.207927867770195\n",
      "Epoch 3940, Loss: 0.8747444599866867, Final Batch Loss: 0.26426559686660767\n",
      "Epoch 3941, Loss: 0.9636924266815186, Final Batch Loss: 0.23122891783714294\n",
      "Epoch 3942, Loss: 0.9696366786956787, Final Batch Loss: 0.21272128820419312\n",
      "Epoch 3943, Loss: 0.9938976019620895, Final Batch Loss: 0.2430455982685089\n",
      "Epoch 3944, Loss: 0.9514541178941727, Final Batch Loss: 0.24260959029197693\n",
      "Epoch 3945, Loss: 0.8967299312353134, Final Batch Loss: 0.2243630737066269\n",
      "Epoch 3946, Loss: 0.9092021882534027, Final Batch Loss: 0.22107510268688202\n",
      "Epoch 3947, Loss: 0.8218330591917038, Final Batch Loss: 0.2693692743778229\n",
      "Epoch 3948, Loss: 0.8719823211431503, Final Batch Loss: 0.20681458711624146\n",
      "Epoch 3949, Loss: 0.8371308594942093, Final Batch Loss: 0.21487540006637573\n",
      "Epoch 3950, Loss: 0.9035295695066452, Final Batch Loss: 0.286891907453537\n",
      "Epoch 3951, Loss: 0.9898227006196976, Final Batch Loss: 0.2470320463180542\n",
      "Epoch 3952, Loss: 0.8946312963962555, Final Batch Loss: 0.18017686903476715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3953, Loss: 0.9050235897302628, Final Batch Loss: 0.27305009961128235\n",
      "Epoch 3954, Loss: 0.9313783496618271, Final Batch Loss: 0.21307268738746643\n",
      "Epoch 3955, Loss: 0.9325721114873886, Final Batch Loss: 0.1841796636581421\n",
      "Epoch 3956, Loss: 0.9238169342279434, Final Batch Loss: 0.2839382290840149\n",
      "Epoch 3957, Loss: 0.880779966711998, Final Batch Loss: 0.19043439626693726\n",
      "Epoch 3958, Loss: 0.8799440860748291, Final Batch Loss: 0.23793861269950867\n",
      "Epoch 3959, Loss: 1.0399371981620789, Final Batch Loss: 0.23674847185611725\n",
      "Epoch 3960, Loss: 0.9100428521633148, Final Batch Loss: 0.18188859522342682\n",
      "Epoch 3961, Loss: 0.9211499392986298, Final Batch Loss: 0.29234981536865234\n",
      "Epoch 3962, Loss: 0.8643136918544769, Final Batch Loss: 0.18223418295383453\n",
      "Epoch 3963, Loss: 0.8602783381938934, Final Batch Loss: 0.1707088202238083\n",
      "Epoch 3964, Loss: 0.8839791864156723, Final Batch Loss: 0.23776642978191376\n",
      "Epoch 3965, Loss: 0.9606162756681442, Final Batch Loss: 0.25350573658943176\n",
      "Epoch 3966, Loss: 0.8590368777513504, Final Batch Loss: 0.21038644015789032\n",
      "Epoch 3967, Loss: 0.8634686022996902, Final Batch Loss: 0.19549106061458588\n",
      "Epoch 3968, Loss: 0.9390440881252289, Final Batch Loss: 0.18051746487617493\n",
      "Epoch 3969, Loss: 0.9120750278234482, Final Batch Loss: 0.3006657660007477\n",
      "Epoch 3970, Loss: 0.7702966034412384, Final Batch Loss: 0.19489671289920807\n",
      "Epoch 3971, Loss: 0.8855625540018082, Final Batch Loss: 0.18507267534732819\n",
      "Epoch 3972, Loss: 0.8197681158781052, Final Batch Loss: 0.19027189910411835\n",
      "Epoch 3973, Loss: 0.9084930419921875, Final Batch Loss: 0.24172313511371613\n",
      "Epoch 3974, Loss: 0.9917462468147278, Final Batch Loss: 0.1968502700328827\n",
      "Epoch 3975, Loss: 0.9443287402391434, Final Batch Loss: 0.2038527876138687\n",
      "Epoch 3976, Loss: 0.8411980271339417, Final Batch Loss: 0.16173233091831207\n",
      "Epoch 3977, Loss: 0.8591002970933914, Final Batch Loss: 0.23754698038101196\n",
      "Epoch 3978, Loss: 0.9296331256628036, Final Batch Loss: 0.19217948615550995\n",
      "Epoch 3979, Loss: 0.9448670297861099, Final Batch Loss: 0.20147545635700226\n",
      "Epoch 3980, Loss: 1.0334099382162094, Final Batch Loss: 0.2466362714767456\n",
      "Epoch 3981, Loss: 0.9405963867902756, Final Batch Loss: 0.19704648852348328\n",
      "Epoch 3982, Loss: 0.7569613307714462, Final Batch Loss: 0.2124936729669571\n",
      "Epoch 3983, Loss: 0.98758365213871, Final Batch Loss: 0.3350943326950073\n",
      "Epoch 3984, Loss: 0.9031175225973129, Final Batch Loss: 0.25673627853393555\n",
      "Epoch 3985, Loss: 0.8159603476524353, Final Batch Loss: 0.18072956800460815\n",
      "Epoch 3986, Loss: 0.9020358771085739, Final Batch Loss: 0.19954414665699005\n",
      "Epoch 3987, Loss: 0.9846296608448029, Final Batch Loss: 0.2783561050891876\n",
      "Epoch 3988, Loss: 1.0665746182203293, Final Batch Loss: 0.27805620431900024\n",
      "Epoch 3989, Loss: 0.9278392642736435, Final Batch Loss: 0.2073771059513092\n",
      "Epoch 3990, Loss: 0.8946555405855179, Final Batch Loss: 0.25326046347618103\n",
      "Epoch 3991, Loss: 1.003643974661827, Final Batch Loss: 0.2048855721950531\n",
      "Epoch 3992, Loss: 0.917766198515892, Final Batch Loss: 0.28617602586746216\n",
      "Epoch 3993, Loss: 0.9171510934829712, Final Batch Loss: 0.20248186588287354\n",
      "Epoch 3994, Loss: 1.0276072174310684, Final Batch Loss: 0.2805546820163727\n",
      "Epoch 3995, Loss: 0.9831048846244812, Final Batch Loss: 0.2896377742290497\n",
      "Epoch 3996, Loss: 1.0088091492652893, Final Batch Loss: 0.2843613922595978\n",
      "Epoch 3997, Loss: 0.8877673596143723, Final Batch Loss: 0.22527077794075012\n",
      "Epoch 3998, Loss: 0.8595993369817734, Final Batch Loss: 0.23286020755767822\n",
      "Epoch 3999, Loss: 0.9046900421380997, Final Batch Loss: 0.26459887623786926\n",
      "Epoch 4000, Loss: 0.9621568322181702, Final Batch Loss: 0.22411762177944183\n",
      "Epoch 4001, Loss: 0.8629373386502266, Final Batch Loss: 0.2784281373023987\n",
      "Epoch 4002, Loss: 0.9628114402294159, Final Batch Loss: 0.2270347625017166\n",
      "Epoch 4003, Loss: 1.0376543253660202, Final Batch Loss: 0.2652881145477295\n",
      "Epoch 4004, Loss: 0.951956644654274, Final Batch Loss: 0.19473953545093536\n",
      "Epoch 4005, Loss: 0.8820189088582993, Final Batch Loss: 0.14851313829421997\n",
      "Epoch 4006, Loss: 1.0665148943662643, Final Batch Loss: 0.23040921986103058\n",
      "Epoch 4007, Loss: 0.8752073496580124, Final Batch Loss: 0.22484929859638214\n",
      "Epoch 4008, Loss: 1.0697752386331558, Final Batch Loss: 0.2699448764324188\n",
      "Epoch 4009, Loss: 0.8937245309352875, Final Batch Loss: 0.20593947172164917\n",
      "Epoch 4010, Loss: 0.9464382082223892, Final Batch Loss: 0.19524647295475006\n",
      "Epoch 4011, Loss: 0.9580918401479721, Final Batch Loss: 0.24036367237567902\n",
      "Epoch 4012, Loss: 1.0138468146324158, Final Batch Loss: 0.302023708820343\n",
      "Epoch 4013, Loss: 0.8876568526029587, Final Batch Loss: 0.21076373755931854\n",
      "Epoch 4014, Loss: 1.0601195245981216, Final Batch Loss: 0.24851034581661224\n",
      "Epoch 4015, Loss: 0.9288211315870285, Final Batch Loss: 0.2958439290523529\n",
      "Epoch 4016, Loss: 0.9011916369199753, Final Batch Loss: 0.29712575674057007\n",
      "Epoch 4017, Loss: 0.9550394415855408, Final Batch Loss: 0.2952750027179718\n",
      "Epoch 4018, Loss: 0.9147417694330215, Final Batch Loss: 0.24097789824008942\n",
      "Epoch 4019, Loss: 0.9744344502687454, Final Batch Loss: 0.25908565521240234\n",
      "Epoch 4020, Loss: 0.9330850094556808, Final Batch Loss: 0.2110002189874649\n",
      "Epoch 4021, Loss: 1.0157200992107391, Final Batch Loss: 0.3181697130203247\n",
      "Epoch 4022, Loss: 1.0818049758672714, Final Batch Loss: 0.3829383850097656\n",
      "Epoch 4023, Loss: 0.871961697936058, Final Batch Loss: 0.2598705589771271\n",
      "Epoch 4024, Loss: 0.9513486176729202, Final Batch Loss: 0.2236345261335373\n",
      "Epoch 4025, Loss: 0.7919983118772507, Final Batch Loss: 0.23106667399406433\n",
      "Epoch 4026, Loss: 1.0388683378696442, Final Batch Loss: 0.3281885087490082\n",
      "Epoch 4027, Loss: 0.8992737978696823, Final Batch Loss: 0.20946215093135834\n",
      "Epoch 4028, Loss: 0.9509505033493042, Final Batch Loss: 0.21674661338329315\n",
      "Epoch 4029, Loss: 0.9222395122051239, Final Batch Loss: 0.19920997321605682\n",
      "Epoch 4030, Loss: 0.9558018893003464, Final Batch Loss: 0.2815782427787781\n",
      "Epoch 4031, Loss: 0.814312532544136, Final Batch Loss: 0.2006653994321823\n",
      "Epoch 4032, Loss: 0.7500331401824951, Final Batch Loss: 0.21285851299762726\n",
      "Epoch 4033, Loss: 1.0406415611505508, Final Batch Loss: 0.2465587705373764\n",
      "Epoch 4034, Loss: 0.9167194068431854, Final Batch Loss: 0.25973930954933167\n",
      "Epoch 4035, Loss: 0.7995914071798325, Final Batch Loss: 0.1717907339334488\n",
      "Epoch 4036, Loss: 0.8974931091070175, Final Batch Loss: 0.2549821734428406\n",
      "Epoch 4037, Loss: 0.8492137938737869, Final Batch Loss: 0.15111882984638214\n",
      "Epoch 4038, Loss: 0.8751681447029114, Final Batch Loss: 0.23110657930374146\n",
      "Epoch 4039, Loss: 0.8276413679122925, Final Batch Loss: 0.17865222692489624\n",
      "Epoch 4040, Loss: 0.9649072885513306, Final Batch Loss: 0.28666019439697266\n",
      "Epoch 4041, Loss: 0.8679362535476685, Final Batch Loss: 0.27547499537467957\n",
      "Epoch 4042, Loss: 0.9019852727651596, Final Batch Loss: 0.27750638127326965\n",
      "Epoch 4043, Loss: 0.9889101833105087, Final Batch Loss: 0.25851237773895264\n",
      "Epoch 4044, Loss: 0.8070355206727982, Final Batch Loss: 0.2087298333644867\n",
      "Epoch 4045, Loss: 0.7419541627168655, Final Batch Loss: 0.16684891283512115\n",
      "Epoch 4046, Loss: 0.9319692254066467, Final Batch Loss: 0.22252127528190613\n",
      "Epoch 4047, Loss: 0.9764591753482819, Final Batch Loss: 0.26828479766845703\n",
      "Epoch 4048, Loss: 0.8867459744215012, Final Batch Loss: 0.22065764665603638\n",
      "Epoch 4049, Loss: 0.8002000749111176, Final Batch Loss: 0.1805930882692337\n",
      "Epoch 4050, Loss: 0.9393336623907089, Final Batch Loss: 0.1744033545255661\n",
      "Epoch 4051, Loss: 0.898881807923317, Final Batch Loss: 0.27386149764060974\n",
      "Epoch 4052, Loss: 0.8818360418081284, Final Batch Loss: 0.2624109983444214\n",
      "Epoch 4053, Loss: 0.8058688342571259, Final Batch Loss: 0.17561104893684387\n",
      "Epoch 4054, Loss: 0.9138658940792084, Final Batch Loss: 0.20872344076633453\n",
      "Epoch 4055, Loss: 0.8894154280424118, Final Batch Loss: 0.23843136429786682\n",
      "Epoch 4056, Loss: 0.8736132830381393, Final Batch Loss: 0.20493116974830627\n",
      "Epoch 4057, Loss: 0.8357438445091248, Final Batch Loss: 0.2042943239212036\n",
      "Epoch 4058, Loss: 0.8855627775192261, Final Batch Loss: 0.2584651708602905\n",
      "Epoch 4059, Loss: 0.8183109015226364, Final Batch Loss: 0.21504800021648407\n",
      "Epoch 4060, Loss: 0.9904753267765045, Final Batch Loss: 0.2681492865085602\n",
      "Epoch 4061, Loss: 0.968827024102211, Final Batch Loss: 0.33232033252716064\n",
      "Epoch 4062, Loss: 0.8943209946155548, Final Batch Loss: 0.25735363364219666\n",
      "Epoch 4063, Loss: 0.967937633395195, Final Batch Loss: 0.2168790102005005\n",
      "Epoch 4064, Loss: 0.9156346619129181, Final Batch Loss: 0.24074451625347137\n",
      "Epoch 4065, Loss: 0.8918778002262115, Final Batch Loss: 0.24934135377407074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4066, Loss: 0.8615420758724213, Final Batch Loss: 0.17767688632011414\n",
      "Epoch 4067, Loss: 0.8613298684358597, Final Batch Loss: 0.1777803748846054\n",
      "Epoch 4068, Loss: 0.9464389830827713, Final Batch Loss: 0.25644028186798096\n",
      "Epoch 4069, Loss: 0.8561059534549713, Final Batch Loss: 0.21404509246349335\n",
      "Epoch 4070, Loss: 0.7893818020820618, Final Batch Loss: 0.16678321361541748\n",
      "Epoch 4071, Loss: 0.9637462198734283, Final Batch Loss: 0.22400808334350586\n",
      "Epoch 4072, Loss: 0.9906295835971832, Final Batch Loss: 0.26515090465545654\n",
      "Epoch 4073, Loss: 0.8159913122653961, Final Batch Loss: 0.20241287350654602\n",
      "Epoch 4074, Loss: 0.8931172043085098, Final Batch Loss: 0.22776691615581512\n",
      "Epoch 4075, Loss: 0.8740726709365845, Final Batch Loss: 0.2788398563861847\n",
      "Epoch 4076, Loss: 0.8650388121604919, Final Batch Loss: 0.18528974056243896\n",
      "Epoch 4077, Loss: 0.888849601149559, Final Batch Loss: 0.17530585825443268\n",
      "Epoch 4078, Loss: 0.8630327135324478, Final Batch Loss: 0.1729031503200531\n",
      "Epoch 4079, Loss: 0.9269866198301315, Final Batch Loss: 0.16603043675422668\n",
      "Epoch 4080, Loss: 0.7437181919813156, Final Batch Loss: 0.1983078271150589\n",
      "Epoch 4081, Loss: 0.9669808596372604, Final Batch Loss: 0.19146829843521118\n",
      "Epoch 4082, Loss: 0.9803141504526138, Final Batch Loss: 0.2349727600812912\n",
      "Epoch 4083, Loss: 0.8603902906179428, Final Batch Loss: 0.18959201872348785\n",
      "Epoch 4084, Loss: 0.8075918704271317, Final Batch Loss: 0.24287600815296173\n",
      "Epoch 4085, Loss: 0.9087342768907547, Final Batch Loss: 0.32156428694725037\n",
      "Epoch 4086, Loss: 0.883509173989296, Final Batch Loss: 0.2368335723876953\n",
      "Epoch 4087, Loss: 0.8294563516974449, Final Batch Loss: 0.12336333841085434\n",
      "Epoch 4088, Loss: 0.8017248958349228, Final Batch Loss: 0.2250967174768448\n",
      "Epoch 4089, Loss: 0.9442646652460098, Final Batch Loss: 0.22892962396144867\n",
      "Epoch 4090, Loss: 0.876515582203865, Final Batch Loss: 0.2606859803199768\n",
      "Epoch 4091, Loss: 1.0862137973308563, Final Batch Loss: 0.311622679233551\n",
      "Epoch 4092, Loss: 0.9040884673595428, Final Batch Loss: 0.23489107191562653\n",
      "Epoch 4093, Loss: 0.8602095544338226, Final Batch Loss: 0.23522445559501648\n",
      "Epoch 4094, Loss: 0.8569346070289612, Final Batch Loss: 0.25652292370796204\n",
      "Epoch 4095, Loss: 0.8569813221693039, Final Batch Loss: 0.27568554878234863\n",
      "Epoch 4096, Loss: 0.9249982386827469, Final Batch Loss: 0.23204639554023743\n",
      "Epoch 4097, Loss: 0.7891126573085785, Final Batch Loss: 0.19808371365070343\n",
      "Epoch 4098, Loss: 0.8516923636198044, Final Batch Loss: 0.2802572250366211\n",
      "Epoch 4099, Loss: 1.1034921556711197, Final Batch Loss: 0.2265690118074417\n",
      "Epoch 4100, Loss: 0.9084676653146744, Final Batch Loss: 0.20302556455135345\n",
      "Epoch 4101, Loss: 0.9812036007642746, Final Batch Loss: 0.2742888629436493\n",
      "Epoch 4102, Loss: 0.9955072700977325, Final Batch Loss: 0.19571983814239502\n",
      "Epoch 4103, Loss: 0.876687541604042, Final Batch Loss: 0.2591729164123535\n",
      "Epoch 4104, Loss: 0.8201052248477936, Final Batch Loss: 0.20954380929470062\n",
      "Epoch 4105, Loss: 0.8604137301445007, Final Batch Loss: 0.21562102437019348\n",
      "Epoch 4106, Loss: 0.7861870527267456, Final Batch Loss: 0.210485577583313\n",
      "Epoch 4107, Loss: 0.9577160775661469, Final Batch Loss: 0.2614552974700928\n",
      "Epoch 4108, Loss: 0.7830571681261063, Final Batch Loss: 0.1497146040201187\n",
      "Epoch 4109, Loss: 0.8358968049287796, Final Batch Loss: 0.2914010286331177\n",
      "Epoch 4110, Loss: 0.8955710679292679, Final Batch Loss: 0.2955223321914673\n",
      "Epoch 4111, Loss: 0.9002519100904465, Final Batch Loss: 0.21277649700641632\n",
      "Epoch 4112, Loss: 0.821713998913765, Final Batch Loss: 0.18544496595859528\n",
      "Epoch 4113, Loss: 0.9190390557050705, Final Batch Loss: 0.19904755055904388\n",
      "Epoch 4114, Loss: 0.9836418777704239, Final Batch Loss: 0.2636331617832184\n",
      "Epoch 4115, Loss: 0.8448892682790756, Final Batch Loss: 0.2140716314315796\n",
      "Epoch 4116, Loss: 0.9527449309825897, Final Batch Loss: 0.24020451307296753\n",
      "Epoch 4117, Loss: 0.9334300458431244, Final Batch Loss: 0.20228318870067596\n",
      "Epoch 4118, Loss: 0.8786004483699799, Final Batch Loss: 0.22747467458248138\n",
      "Epoch 4119, Loss: 0.8965146690607071, Final Batch Loss: 0.2533334493637085\n",
      "Epoch 4120, Loss: 0.870179608464241, Final Batch Loss: 0.3101066052913666\n",
      "Epoch 4121, Loss: 0.8484512269496918, Final Batch Loss: 0.19623199105262756\n",
      "Epoch 4122, Loss: 0.8550277799367905, Final Batch Loss: 0.23519021272659302\n",
      "Epoch 4123, Loss: 1.0446152985095978, Final Batch Loss: 0.27701112627983093\n",
      "Epoch 4124, Loss: 0.927527442574501, Final Batch Loss: 0.18108652532100677\n",
      "Epoch 4125, Loss: 0.9858188778162003, Final Batch Loss: 0.31601178646087646\n",
      "Epoch 4126, Loss: 0.8332754969596863, Final Batch Loss: 0.1831968128681183\n",
      "Epoch 4127, Loss: 0.7886612862348557, Final Batch Loss: 0.1680382788181305\n",
      "Epoch 4128, Loss: 0.9611982256174088, Final Batch Loss: 0.2682691812515259\n",
      "Epoch 4129, Loss: 0.8373563438653946, Final Batch Loss: 0.21426279842853546\n",
      "Epoch 4130, Loss: 0.8514780253171921, Final Batch Loss: 0.17636360228061676\n",
      "Epoch 4131, Loss: 0.8074053674936295, Final Batch Loss: 0.19650425016880035\n",
      "Epoch 4132, Loss: 0.7696547359228134, Final Batch Loss: 0.242039293050766\n",
      "Epoch 4133, Loss: 0.8559157699346542, Final Batch Loss: 0.17910915613174438\n",
      "Epoch 4134, Loss: 0.7895772606134415, Final Batch Loss: 0.2336231917142868\n",
      "Epoch 4135, Loss: 0.8669465184211731, Final Batch Loss: 0.3054736852645874\n",
      "Epoch 4136, Loss: 0.9088063538074493, Final Batch Loss: 0.28237438201904297\n",
      "Epoch 4137, Loss: 0.8960661441087723, Final Batch Loss: 0.24808718264102936\n",
      "Epoch 4138, Loss: 0.9543348401784897, Final Batch Loss: 0.22263380885124207\n",
      "Epoch 4139, Loss: 0.7508226931095123, Final Batch Loss: 0.17872938513755798\n",
      "Epoch 4140, Loss: 0.9593053013086319, Final Batch Loss: 0.17659451067447662\n",
      "Epoch 4141, Loss: 0.8116564005613327, Final Batch Loss: 0.2233673334121704\n",
      "Epoch 4142, Loss: 0.9521302133798599, Final Batch Loss: 0.24982868134975433\n",
      "Epoch 4143, Loss: 1.0061711370944977, Final Batch Loss: 0.13830500841140747\n",
      "Epoch 4144, Loss: 1.0105552226305008, Final Batch Loss: 0.24681709706783295\n",
      "Epoch 4145, Loss: 0.8661813586950302, Final Batch Loss: 0.2338399589061737\n",
      "Epoch 4146, Loss: 1.075339898467064, Final Batch Loss: 0.2282075136899948\n",
      "Epoch 4147, Loss: 0.9080452919006348, Final Batch Loss: 0.3228859007358551\n",
      "Epoch 4148, Loss: 0.8508370518684387, Final Batch Loss: 0.22225457429885864\n",
      "Epoch 4149, Loss: 0.8354232162237167, Final Batch Loss: 0.22522832453250885\n",
      "Epoch 4150, Loss: 0.8590674102306366, Final Batch Loss: 0.22548790276050568\n",
      "Epoch 4151, Loss: 0.8754910230636597, Final Batch Loss: 0.16251468658447266\n",
      "Epoch 4152, Loss: 0.8176420331001282, Final Batch Loss: 0.23031483590602875\n",
      "Epoch 4153, Loss: 0.8752899318933487, Final Batch Loss: 0.24044561386108398\n",
      "Epoch 4154, Loss: 0.9777969717979431, Final Batch Loss: 0.19978126883506775\n",
      "Epoch 4155, Loss: 0.8657659441232681, Final Batch Loss: 0.1966315507888794\n",
      "Epoch 4156, Loss: 0.7982773631811142, Final Batch Loss: 0.12682649493217468\n",
      "Epoch 4157, Loss: 0.9407570511102676, Final Batch Loss: 0.24697721004486084\n",
      "Epoch 4158, Loss: 0.9734113961458206, Final Batch Loss: 0.23402081429958344\n",
      "Epoch 4159, Loss: 0.8507186621427536, Final Batch Loss: 0.22145631909370422\n",
      "Epoch 4160, Loss: 0.8997404128313065, Final Batch Loss: 0.3098525106906891\n",
      "Epoch 4161, Loss: 0.8606730252504349, Final Batch Loss: 0.21801236271858215\n",
      "Epoch 4162, Loss: 0.9345823973417282, Final Batch Loss: 0.17912524938583374\n",
      "Epoch 4163, Loss: 0.9775207489728928, Final Batch Loss: 0.2674733102321625\n",
      "Epoch 4164, Loss: 0.8239038735628128, Final Batch Loss: 0.21386095881462097\n",
      "Epoch 4165, Loss: 0.8664072751998901, Final Batch Loss: 0.2173052579164505\n",
      "Epoch 4166, Loss: 0.9056575447320938, Final Batch Loss: 0.23091496527194977\n",
      "Epoch 4167, Loss: 0.8114528506994247, Final Batch Loss: 0.18119646608829498\n",
      "Epoch 4168, Loss: 0.9315057247877121, Final Batch Loss: 0.26717907190322876\n",
      "Epoch 4169, Loss: 0.9973528832197189, Final Batch Loss: 0.3795607388019562\n",
      "Epoch 4170, Loss: 0.8014293611049652, Final Batch Loss: 0.19699524343013763\n",
      "Epoch 4171, Loss: 0.7744835019111633, Final Batch Loss: 0.19812214374542236\n",
      "Epoch 4172, Loss: 0.8636369407176971, Final Batch Loss: 0.25359979271888733\n",
      "Epoch 4173, Loss: 0.7941873669624329, Final Batch Loss: 0.20178532600402832\n",
      "Epoch 4174, Loss: 0.8372100591659546, Final Batch Loss: 0.1862713098526001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4175, Loss: 0.7738014906644821, Final Batch Loss: 0.24434268474578857\n",
      "Epoch 4176, Loss: 0.901390478014946, Final Batch Loss: 0.2585466504096985\n",
      "Epoch 4177, Loss: 0.8018853217363358, Final Batch Loss: 0.1849469542503357\n",
      "Epoch 4178, Loss: 1.0248962938785553, Final Batch Loss: 0.282884806394577\n",
      "Epoch 4179, Loss: 0.8318972587585449, Final Batch Loss: 0.231297567486763\n",
      "Epoch 4180, Loss: 0.9214435815811157, Final Batch Loss: 0.25688835978507996\n",
      "Epoch 4181, Loss: 0.8399254381656647, Final Batch Loss: 0.23877626657485962\n",
      "Epoch 4182, Loss: 0.890654131770134, Final Batch Loss: 0.22697943449020386\n",
      "Epoch 4183, Loss: 0.8664431571960449, Final Batch Loss: 0.15984465181827545\n",
      "Epoch 4184, Loss: 0.9440086483955383, Final Batch Loss: 0.22019062936306\n",
      "Epoch 4185, Loss: 0.943388819694519, Final Batch Loss: 0.20937271416187286\n",
      "Epoch 4186, Loss: 0.9953010380268097, Final Batch Loss: 0.24118252098560333\n",
      "Epoch 4187, Loss: 0.7925091534852982, Final Batch Loss: 0.18065595626831055\n",
      "Epoch 4188, Loss: 1.068046197295189, Final Batch Loss: 0.22050534188747406\n",
      "Epoch 4189, Loss: 0.8101727962493896, Final Batch Loss: 0.2350466549396515\n",
      "Epoch 4190, Loss: 1.0422284752130508, Final Batch Loss: 0.2767674922943115\n",
      "Epoch 4191, Loss: 0.912264496088028, Final Batch Loss: 0.18309789896011353\n",
      "Epoch 4192, Loss: 0.9405570179224014, Final Batch Loss: 0.2546805143356323\n",
      "Epoch 4193, Loss: 1.014355555176735, Final Batch Loss: 0.2536676824092865\n",
      "Epoch 4194, Loss: 1.029871866106987, Final Batch Loss: 0.3410049080848694\n",
      "Epoch 4195, Loss: 0.932622417807579, Final Batch Loss: 0.31871792674064636\n",
      "Epoch 4196, Loss: 0.9823851585388184, Final Batch Loss: 0.22020073235034943\n",
      "Epoch 4197, Loss: 0.9687737077474594, Final Batch Loss: 0.26607897877693176\n",
      "Epoch 4198, Loss: 0.8918076902627945, Final Batch Loss: 0.24312260746955872\n",
      "Epoch 4199, Loss: 0.7846434563398361, Final Batch Loss: 0.21778999269008636\n",
      "Epoch 4200, Loss: 1.1339566260576248, Final Batch Loss: 0.41811421513557434\n",
      "Epoch 4201, Loss: 0.9316523224115372, Final Batch Loss: 0.20922134816646576\n",
      "Epoch 4202, Loss: 0.9415886849164963, Final Batch Loss: 0.24902327358722687\n",
      "Epoch 4203, Loss: 0.817401260137558, Final Batch Loss: 0.2372685968875885\n",
      "Epoch 4204, Loss: 0.8490200042724609, Final Batch Loss: 0.1929253786802292\n",
      "Epoch 4205, Loss: 0.8774657547473907, Final Batch Loss: 0.17795774340629578\n",
      "Epoch 4206, Loss: 0.8335559666156769, Final Batch Loss: 0.260648250579834\n",
      "Epoch 4207, Loss: 0.9613542854785919, Final Batch Loss: 0.2837812900543213\n",
      "Epoch 4208, Loss: 0.789732351899147, Final Batch Loss: 0.1829574704170227\n",
      "Epoch 4209, Loss: 0.8041948080062866, Final Batch Loss: 0.18770509958267212\n",
      "Epoch 4210, Loss: 0.8387470543384552, Final Batch Loss: 0.17521123588085175\n",
      "Epoch 4211, Loss: 0.8750197738409042, Final Batch Loss: 0.2210199534893036\n",
      "Epoch 4212, Loss: 0.9435972273349762, Final Batch Loss: 0.24064642190933228\n",
      "Epoch 4213, Loss: 0.7625325173139572, Final Batch Loss: 0.1648634970188141\n",
      "Epoch 4214, Loss: 0.9436128735542297, Final Batch Loss: 0.20278064906597137\n",
      "Epoch 4215, Loss: 0.7886271327733994, Final Batch Loss: 0.259417325258255\n",
      "Epoch 4216, Loss: 0.8596452474594116, Final Batch Loss: 0.23269103467464447\n",
      "Epoch 4217, Loss: 0.8565064668655396, Final Batch Loss: 0.26190701127052307\n",
      "Epoch 4218, Loss: 0.8730299472808838, Final Batch Loss: 0.15796880424022675\n",
      "Epoch 4219, Loss: 0.906214565038681, Final Batch Loss: 0.26147887110710144\n",
      "Epoch 4220, Loss: 0.7876297980546951, Final Batch Loss: 0.12112821638584137\n",
      "Epoch 4221, Loss: 0.9113777130842209, Final Batch Loss: 0.2502491772174835\n",
      "Epoch 4222, Loss: 0.8435536772012711, Final Batch Loss: 0.2042953222990036\n",
      "Epoch 4223, Loss: 0.8556195348501205, Final Batch Loss: 0.19047021865844727\n",
      "Epoch 4224, Loss: 0.9366237670183182, Final Batch Loss: 0.193678617477417\n",
      "Epoch 4225, Loss: 0.8966517895460129, Final Batch Loss: 0.2663290798664093\n",
      "Epoch 4226, Loss: 0.9893179386854172, Final Batch Loss: 0.20317555963993073\n",
      "Epoch 4227, Loss: 0.9510439783334732, Final Batch Loss: 0.20472300052642822\n",
      "Epoch 4228, Loss: 0.9828535169363022, Final Batch Loss: 0.2552063763141632\n",
      "Epoch 4229, Loss: 0.8192666172981262, Final Batch Loss: 0.2544263005256653\n",
      "Epoch 4230, Loss: 0.9049657136201859, Final Batch Loss: 0.23905791342258453\n",
      "Epoch 4231, Loss: 0.8395005166530609, Final Batch Loss: 0.24859943985939026\n",
      "Epoch 4232, Loss: 0.9876005202531815, Final Batch Loss: 0.29814690351486206\n",
      "Epoch 4233, Loss: 0.9027078002691269, Final Batch Loss: 0.23205488920211792\n",
      "Epoch 4234, Loss: 0.8982500284910202, Final Batch Loss: 0.2248409390449524\n",
      "Epoch 4235, Loss: 0.9117904752492905, Final Batch Loss: 0.22022278606891632\n",
      "Epoch 4236, Loss: 0.8321032077074051, Final Batch Loss: 0.21111182868480682\n",
      "Epoch 4237, Loss: 0.9871094226837158, Final Batch Loss: 0.2971343696117401\n",
      "Epoch 4238, Loss: 0.9458509385585785, Final Batch Loss: 0.21853911876678467\n",
      "Epoch 4239, Loss: 0.9073348194360733, Final Batch Loss: 0.34015128016471863\n",
      "Epoch 4240, Loss: 0.8272254168987274, Final Batch Loss: 0.19267380237579346\n",
      "Epoch 4241, Loss: 0.8457677215337753, Final Batch Loss: 0.1448899507522583\n",
      "Epoch 4242, Loss: 0.9076357036828995, Final Batch Loss: 0.23506762087345123\n",
      "Epoch 4243, Loss: 0.9573194086551666, Final Batch Loss: 0.25030016899108887\n",
      "Epoch 4244, Loss: 0.972657322883606, Final Batch Loss: 0.21554075181484222\n",
      "Epoch 4245, Loss: 0.8969020694494247, Final Batch Loss: 0.24504661560058594\n",
      "Epoch 4246, Loss: 0.9208319932222366, Final Batch Loss: 0.23776230216026306\n",
      "Epoch 4247, Loss: 0.8949115723371506, Final Batch Loss: 0.20695200562477112\n",
      "Epoch 4248, Loss: 1.025217890739441, Final Batch Loss: 0.2646186947822571\n",
      "Epoch 4249, Loss: 0.9237097650766373, Final Batch Loss: 0.21981872618198395\n",
      "Epoch 4250, Loss: 0.7437331080436707, Final Batch Loss: 0.11983086168766022\n",
      "Epoch 4251, Loss: 0.8699692040681839, Final Batch Loss: 0.20616532862186432\n",
      "Epoch 4252, Loss: 0.9949891120195389, Final Batch Loss: 0.23056717216968536\n",
      "Epoch 4253, Loss: 0.9369053095579147, Final Batch Loss: 0.2426394373178482\n",
      "Epoch 4254, Loss: 0.9730954170227051, Final Batch Loss: 0.21930772066116333\n",
      "Epoch 4255, Loss: 0.9479977041482925, Final Batch Loss: 0.2406197190284729\n",
      "Epoch 4256, Loss: 0.8790943324565887, Final Batch Loss: 0.15830080211162567\n",
      "Epoch 4257, Loss: 0.9007449597120285, Final Batch Loss: 0.2777021527290344\n",
      "Epoch 4258, Loss: 0.8087300807237625, Final Batch Loss: 0.17134742438793182\n",
      "Epoch 4259, Loss: 0.8843794763088226, Final Batch Loss: 0.2574663758277893\n",
      "Epoch 4260, Loss: 0.8877715468406677, Final Batch Loss: 0.24415701627731323\n",
      "Epoch 4261, Loss: 0.8901091068983078, Final Batch Loss: 0.24259275197982788\n",
      "Epoch 4262, Loss: 0.8671187460422516, Final Batch Loss: 0.2115645557641983\n",
      "Epoch 4263, Loss: 0.9576453864574432, Final Batch Loss: 0.2966122627258301\n",
      "Epoch 4264, Loss: 0.8245280534029007, Final Batch Loss: 0.1618337780237198\n",
      "Epoch 4265, Loss: 0.9661020040512085, Final Batch Loss: 0.18440933525562286\n",
      "Epoch 4266, Loss: 0.9434858709573746, Final Batch Loss: 0.31283000111579895\n",
      "Epoch 4267, Loss: 0.8924876898527145, Final Batch Loss: 0.23370468616485596\n",
      "Epoch 4268, Loss: 0.8521465510129929, Final Batch Loss: 0.19407010078430176\n",
      "Epoch 4269, Loss: 0.8337695971131325, Final Batch Loss: 0.22986485064029694\n",
      "Epoch 4270, Loss: 0.8092510551214218, Final Batch Loss: 0.16497503221035004\n",
      "Epoch 4271, Loss: 0.9906810373067856, Final Batch Loss: 0.23920771479606628\n",
      "Epoch 4272, Loss: 0.9258664399385452, Final Batch Loss: 0.27311643958091736\n",
      "Epoch 4273, Loss: 0.94803786277771, Final Batch Loss: 0.20220616459846497\n",
      "Epoch 4274, Loss: 0.8767062276601791, Final Batch Loss: 0.22042831778526306\n",
      "Epoch 4275, Loss: 0.8699038773775101, Final Batch Loss: 0.14417313039302826\n",
      "Epoch 4276, Loss: 0.9890819638967514, Final Batch Loss: 0.26955288648605347\n",
      "Epoch 4277, Loss: 0.8863756358623505, Final Batch Loss: 0.17657825350761414\n",
      "Epoch 4278, Loss: 0.9141432791948318, Final Batch Loss: 0.21874989569187164\n",
      "Epoch 4279, Loss: 0.8418575376272202, Final Batch Loss: 0.21129921078681946\n",
      "Epoch 4280, Loss: 0.8601858168840408, Final Batch Loss: 0.19093477725982666\n",
      "Epoch 4281, Loss: 0.7846391797065735, Final Batch Loss: 0.15500131249427795\n",
      "Epoch 4282, Loss: 0.7801822423934937, Final Batch Loss: 0.19101384282112122\n",
      "Epoch 4283, Loss: 0.8964361548423767, Final Batch Loss: 0.21205078065395355\n",
      "Epoch 4284, Loss: 0.8922446966171265, Final Batch Loss: 0.20180951058864594\n",
      "Epoch 4285, Loss: 0.8733353912830353, Final Batch Loss: 0.23462307453155518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4286, Loss: 0.9225262254476547, Final Batch Loss: 0.2661797106266022\n",
      "Epoch 4287, Loss: 0.9718368798494339, Final Batch Loss: 0.2795420289039612\n",
      "Epoch 4288, Loss: 0.9323185831308365, Final Batch Loss: 0.31286102533340454\n",
      "Epoch 4289, Loss: 0.7993168830871582, Final Batch Loss: 0.20166514813899994\n",
      "Epoch 4290, Loss: 0.7342243492603302, Final Batch Loss: 0.1692495197057724\n",
      "Epoch 4291, Loss: 0.9221690446138382, Final Batch Loss: 0.2925875186920166\n",
      "Epoch 4292, Loss: 0.8474144339561462, Final Batch Loss: 0.21891136467456818\n",
      "Epoch 4293, Loss: 0.7852161377668381, Final Batch Loss: 0.21332430839538574\n",
      "Epoch 4294, Loss: 0.8279009461402893, Final Batch Loss: 0.20479190349578857\n",
      "Epoch 4295, Loss: 0.8946339339017868, Final Batch Loss: 0.1858888417482376\n",
      "Epoch 4296, Loss: 0.8170445710420609, Final Batch Loss: 0.20716749131679535\n",
      "Epoch 4297, Loss: 0.9884451925754547, Final Batch Loss: 0.3274044692516327\n",
      "Epoch 4298, Loss: 0.8307385295629501, Final Batch Loss: 0.18644186854362488\n",
      "Epoch 4299, Loss: 0.8716354072093964, Final Batch Loss: 0.17439007759094238\n",
      "Epoch 4300, Loss: 0.9394505172967911, Final Batch Loss: 0.2660352885723114\n",
      "Epoch 4301, Loss: 0.7854149341583252, Final Batch Loss: 0.20228783786296844\n",
      "Epoch 4302, Loss: 0.8776118606328964, Final Batch Loss: 0.20028424263000488\n",
      "Epoch 4303, Loss: 0.8671917766332626, Final Batch Loss: 0.18098367750644684\n",
      "Epoch 4304, Loss: 0.9107440114021301, Final Batch Loss: 0.2221357673406601\n",
      "Epoch 4305, Loss: 0.8354730308055878, Final Batch Loss: 0.2089291512966156\n",
      "Epoch 4306, Loss: 0.8276268392801285, Final Batch Loss: 0.18776379525661469\n",
      "Epoch 4307, Loss: 0.7973437309265137, Final Batch Loss: 0.18700052797794342\n",
      "Epoch 4308, Loss: 0.8755708932876587, Final Batch Loss: 0.2418220490217209\n",
      "Epoch 4309, Loss: 0.8330040276050568, Final Batch Loss: 0.25458019971847534\n",
      "Epoch 4310, Loss: 0.9009771347045898, Final Batch Loss: 0.15342989563941956\n",
      "Epoch 4311, Loss: 0.9393639117479324, Final Batch Loss: 0.2108774483203888\n",
      "Epoch 4312, Loss: 0.8994555771350861, Final Batch Loss: 0.18367524445056915\n",
      "Epoch 4313, Loss: 0.8612882643938065, Final Batch Loss: 0.24136173725128174\n",
      "Epoch 4314, Loss: 0.93085777759552, Final Batch Loss: 0.18642544746398926\n",
      "Epoch 4315, Loss: 0.8915432095527649, Final Batch Loss: 0.19688516855239868\n",
      "Epoch 4316, Loss: 0.936563640832901, Final Batch Loss: 0.2506377696990967\n",
      "Epoch 4317, Loss: 0.9721065610647202, Final Batch Loss: 0.2332950234413147\n",
      "Epoch 4318, Loss: 0.8198548406362534, Final Batch Loss: 0.202658012509346\n",
      "Epoch 4319, Loss: 0.8670105934143066, Final Batch Loss: 0.21943320333957672\n",
      "Epoch 4320, Loss: 0.8993981629610062, Final Batch Loss: 0.20591425895690918\n",
      "Epoch 4321, Loss: 0.7525330483913422, Final Batch Loss: 0.2063225507736206\n",
      "Epoch 4322, Loss: 0.8041128814220428, Final Batch Loss: 0.1355937123298645\n",
      "Epoch 4323, Loss: 0.7996602207422256, Final Batch Loss: 0.20216870307922363\n",
      "Epoch 4324, Loss: 0.9573427140712738, Final Batch Loss: 0.3305423855781555\n",
      "Epoch 4325, Loss: 0.9638739973306656, Final Batch Loss: 0.29749009013175964\n",
      "Epoch 4326, Loss: 0.8630053997039795, Final Batch Loss: 0.23429109156131744\n",
      "Epoch 4327, Loss: 0.8267170786857605, Final Batch Loss: 0.17511716485023499\n",
      "Epoch 4328, Loss: 0.9407978504896164, Final Batch Loss: 0.27003762125968933\n",
      "Epoch 4329, Loss: 0.9238953590393066, Final Batch Loss: 0.2199980616569519\n",
      "Epoch 4330, Loss: 0.8115796744823456, Final Batch Loss: 0.19113220274448395\n",
      "Epoch 4331, Loss: 0.8445478528738022, Final Batch Loss: 0.17617879807949066\n",
      "Epoch 4332, Loss: 0.9961463361978531, Final Batch Loss: 0.29813799262046814\n",
      "Epoch 4333, Loss: 0.8767532706260681, Final Batch Loss: 0.17543798685073853\n",
      "Epoch 4334, Loss: 0.8681239038705826, Final Batch Loss: 0.22609323263168335\n",
      "Epoch 4335, Loss: 0.9777742326259613, Final Batch Loss: 0.2335943877696991\n",
      "Epoch 4336, Loss: 0.9719079732894897, Final Batch Loss: 0.2510070502758026\n",
      "Epoch 4337, Loss: 0.8346148282289505, Final Batch Loss: 0.22550897300243378\n",
      "Epoch 4338, Loss: 0.9048412591218948, Final Batch Loss: 0.268911212682724\n",
      "Epoch 4339, Loss: 0.8570898026227951, Final Batch Loss: 0.1488909125328064\n",
      "Epoch 4340, Loss: 0.8567297160625458, Final Batch Loss: 0.19434300065040588\n",
      "Epoch 4341, Loss: 0.9335601180791855, Final Batch Loss: 0.18385754525661469\n",
      "Epoch 4342, Loss: 0.7929882109165192, Final Batch Loss: 0.18449726700782776\n",
      "Epoch 4343, Loss: 0.8818327933549881, Final Batch Loss: 0.21542616188526154\n",
      "Epoch 4344, Loss: 0.9059345126152039, Final Batch Loss: 0.204290971159935\n",
      "Epoch 4345, Loss: 1.0236676037311554, Final Batch Loss: 0.227789968252182\n",
      "Epoch 4346, Loss: 0.8635378479957581, Final Batch Loss: 0.18210884928703308\n",
      "Epoch 4347, Loss: 0.9938069581985474, Final Batch Loss: 0.23480576276779175\n",
      "Epoch 4348, Loss: 0.805801659822464, Final Batch Loss: 0.21886679530143738\n",
      "Epoch 4349, Loss: 0.9787251502275467, Final Batch Loss: 0.2717277705669403\n",
      "Epoch 4350, Loss: 0.8627627789974213, Final Batch Loss: 0.20012103021144867\n",
      "Epoch 4351, Loss: 0.9128392040729523, Final Batch Loss: 0.2130906730890274\n",
      "Epoch 4352, Loss: 0.9054900258779526, Final Batch Loss: 0.18189145624637604\n",
      "Epoch 4353, Loss: 0.8212194740772247, Final Batch Loss: 0.20254914462566376\n",
      "Epoch 4354, Loss: 0.812106728553772, Final Batch Loss: 0.17306584119796753\n",
      "Epoch 4355, Loss: 1.0108535140752792, Final Batch Loss: 0.2778182327747345\n",
      "Epoch 4356, Loss: 0.8201038092374802, Final Batch Loss: 0.17990772426128387\n",
      "Epoch 4357, Loss: 0.7694094628095627, Final Batch Loss: 0.19703219830989838\n",
      "Epoch 4358, Loss: 0.9224776923656464, Final Batch Loss: 0.29521334171295166\n",
      "Epoch 4359, Loss: 1.0583838075399399, Final Batch Loss: 0.3633512556552887\n",
      "Epoch 4360, Loss: 0.8927372246980667, Final Batch Loss: 0.26607945561408997\n",
      "Epoch 4361, Loss: 1.0138922929763794, Final Batch Loss: 0.21141234040260315\n",
      "Epoch 4362, Loss: 1.0600511878728867, Final Batch Loss: 0.28624075651168823\n",
      "Epoch 4363, Loss: 1.0157702416181564, Final Batch Loss: 0.2370651215314865\n",
      "Epoch 4364, Loss: 0.8843287527561188, Final Batch Loss: 0.21054323017597198\n",
      "Epoch 4365, Loss: 0.8033385425806046, Final Batch Loss: 0.2119593620300293\n",
      "Epoch 4366, Loss: 0.8016373962163925, Final Batch Loss: 0.191664919257164\n",
      "Epoch 4367, Loss: 1.0339681804180145, Final Batch Loss: 0.3355116844177246\n",
      "Epoch 4368, Loss: 0.8678800612688065, Final Batch Loss: 0.25863754749298096\n",
      "Epoch 4369, Loss: 0.9151983559131622, Final Batch Loss: 0.1943821758031845\n",
      "Epoch 4370, Loss: 0.945141464471817, Final Batch Loss: 0.24774514138698578\n",
      "Epoch 4371, Loss: 0.8374547958374023, Final Batch Loss: 0.20926417410373688\n",
      "Epoch 4372, Loss: 0.9180088937282562, Final Batch Loss: 0.23999720811843872\n",
      "Epoch 4373, Loss: 0.8276082277297974, Final Batch Loss: 0.1708696037530899\n",
      "Epoch 4374, Loss: 0.8976196497678757, Final Batch Loss: 0.21773473918437958\n",
      "Epoch 4375, Loss: 0.7995602637529373, Final Batch Loss: 0.15896910429000854\n",
      "Epoch 4376, Loss: 0.7754835337400436, Final Batch Loss: 0.20132224261760712\n",
      "Epoch 4377, Loss: 0.9156946986913681, Final Batch Loss: 0.28212013840675354\n",
      "Epoch 4378, Loss: 0.8809311091899872, Final Batch Loss: 0.25895074009895325\n",
      "Epoch 4379, Loss: 0.9894839823246002, Final Batch Loss: 0.21904653310775757\n",
      "Epoch 4380, Loss: 0.8542393445968628, Final Batch Loss: 0.18569564819335938\n",
      "Epoch 4381, Loss: 1.0096203684806824, Final Batch Loss: 0.2742133140563965\n",
      "Epoch 4382, Loss: 0.8223091661930084, Final Batch Loss: 0.1471768319606781\n",
      "Epoch 4383, Loss: 0.7401627600193024, Final Batch Loss: 0.14576351642608643\n",
      "Epoch 4384, Loss: 0.9065554440021515, Final Batch Loss: 0.2365386039018631\n",
      "Epoch 4385, Loss: 0.839224174618721, Final Batch Loss: 0.17371119558811188\n",
      "Epoch 4386, Loss: 0.8845346122980118, Final Batch Loss: 0.18218190968036652\n",
      "Epoch 4387, Loss: 0.7790557146072388, Final Batch Loss: 0.1955236941576004\n",
      "Epoch 4388, Loss: 0.9096450358629227, Final Batch Loss: 0.20437867939472198\n",
      "Epoch 4389, Loss: 0.873994454741478, Final Batch Loss: 0.260044664144516\n",
      "Epoch 4390, Loss: 0.8231089860200882, Final Batch Loss: 0.21813397109508514\n",
      "Epoch 4391, Loss: 0.8366577476263046, Final Batch Loss: 0.25549277663230896\n",
      "Epoch 4392, Loss: 0.9114247560501099, Final Batch Loss: 0.2250640094280243\n",
      "Epoch 4393, Loss: 0.8173632174730301, Final Batch Loss: 0.23100078105926514\n",
      "Epoch 4394, Loss: 1.0141707360744476, Final Batch Loss: 0.22635793685913086\n",
      "Epoch 4395, Loss: 0.9292593598365784, Final Batch Loss: 0.22262702882289886\n",
      "Epoch 4396, Loss: 0.8394297361373901, Final Batch Loss: 0.324544221162796\n",
      "Epoch 4397, Loss: 0.8771245181560516, Final Batch Loss: 0.19381709396839142\n",
      "Epoch 4398, Loss: 0.8707547336816788, Final Batch Loss: 0.15623115003108978\n",
      "Epoch 4399, Loss: 0.7842198312282562, Final Batch Loss: 0.20410089194774628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4400, Loss: 0.8693878203630447, Final Batch Loss: 0.20682679116725922\n",
      "Epoch 4401, Loss: 0.8182074874639511, Final Batch Loss: 0.27683699131011963\n",
      "Epoch 4402, Loss: 0.8323457762598991, Final Batch Loss: 0.23164351284503937\n",
      "Epoch 4403, Loss: 0.8456464856863022, Final Batch Loss: 0.21255981922149658\n",
      "Epoch 4404, Loss: 0.9254170060157776, Final Batch Loss: 0.22448517382144928\n",
      "Epoch 4405, Loss: 0.9408668577671051, Final Batch Loss: 0.2387075275182724\n",
      "Epoch 4406, Loss: 0.8051963895559311, Final Batch Loss: 0.25423553586006165\n",
      "Epoch 4407, Loss: 0.864280566573143, Final Batch Loss: 0.21285249292850494\n",
      "Epoch 4408, Loss: 0.8377714306116104, Final Batch Loss: 0.2193119376897812\n",
      "Epoch 4409, Loss: 0.7832414954900742, Final Batch Loss: 0.2164105772972107\n",
      "Epoch 4410, Loss: 0.8054079413414001, Final Batch Loss: 0.1538534015417099\n",
      "Epoch 4411, Loss: 0.9390694946050644, Final Batch Loss: 0.18966518342494965\n",
      "Epoch 4412, Loss: 0.9297301024198532, Final Batch Loss: 0.2106875330209732\n",
      "Epoch 4413, Loss: 1.0318887680768967, Final Batch Loss: 0.24430812895298004\n",
      "Epoch 4414, Loss: 0.8416067063808441, Final Batch Loss: 0.27119335532188416\n",
      "Epoch 4415, Loss: 0.8702642172574997, Final Batch Loss: 0.21706417202949524\n",
      "Epoch 4416, Loss: 0.9681489914655685, Final Batch Loss: 0.16601835191249847\n",
      "Epoch 4417, Loss: 1.0175216346979141, Final Batch Loss: 0.23721840977668762\n",
      "Epoch 4418, Loss: 0.8997371196746826, Final Batch Loss: 0.2693323791027069\n",
      "Epoch 4419, Loss: 0.7888801842927933, Final Batch Loss: 0.19654087722301483\n",
      "Epoch 4420, Loss: 0.9635525047779083, Final Batch Loss: 0.21850602328777313\n",
      "Epoch 4421, Loss: 0.8286567628383636, Final Batch Loss: 0.17746612429618835\n",
      "Epoch 4422, Loss: 0.8585154414176941, Final Batch Loss: 0.21312309801578522\n",
      "Epoch 4423, Loss: 0.9250171184539795, Final Batch Loss: 0.22053416073322296\n",
      "Epoch 4424, Loss: 0.9215544164180756, Final Batch Loss: 0.2410791665315628\n",
      "Epoch 4425, Loss: 0.8758698999881744, Final Batch Loss: 0.18747173249721527\n",
      "Epoch 4426, Loss: 0.9708526730537415, Final Batch Loss: 0.29360881447792053\n",
      "Epoch 4427, Loss: 0.9482802450656891, Final Batch Loss: 0.21441660821437836\n",
      "Epoch 4428, Loss: 0.7546139359474182, Final Batch Loss: 0.16862833499908447\n",
      "Epoch 4429, Loss: 0.8336406797170639, Final Batch Loss: 0.18005572259426117\n",
      "Epoch 4430, Loss: 0.9884038269519806, Final Batch Loss: 0.23170846700668335\n",
      "Epoch 4431, Loss: 0.8190026730298996, Final Batch Loss: 0.2240309715270996\n",
      "Epoch 4432, Loss: 0.8767213821411133, Final Batch Loss: 0.28246551752090454\n",
      "Epoch 4433, Loss: 0.895631268620491, Final Batch Loss: 0.2176254391670227\n",
      "Epoch 4434, Loss: 0.9675501585006714, Final Batch Loss: 0.24629326164722443\n",
      "Epoch 4435, Loss: 1.0807658582925797, Final Batch Loss: 0.29893553256988525\n",
      "Epoch 4436, Loss: 0.8608867228031158, Final Batch Loss: 0.2304767519235611\n",
      "Epoch 4437, Loss: 0.9334067553281784, Final Batch Loss: 0.259707510471344\n",
      "Epoch 4438, Loss: 0.8482252061367035, Final Batch Loss: 0.19266656041145325\n",
      "Epoch 4439, Loss: 0.9616523087024689, Final Batch Loss: 0.28507867455482483\n",
      "Epoch 4440, Loss: 0.8450938910245895, Final Batch Loss: 0.23546580970287323\n",
      "Epoch 4441, Loss: 0.7899236679077148, Final Batch Loss: 0.18256904184818268\n",
      "Epoch 4442, Loss: 0.8837460428476334, Final Batch Loss: 0.21295949816703796\n",
      "Epoch 4443, Loss: 0.8183520883321762, Final Batch Loss: 0.15875212848186493\n",
      "Epoch 4444, Loss: 0.9775488823652267, Final Batch Loss: 0.26100993156433105\n",
      "Epoch 4445, Loss: 0.8847554624080658, Final Batch Loss: 0.22024692595005035\n",
      "Epoch 4446, Loss: 0.8840587735176086, Final Batch Loss: 0.18401959538459778\n",
      "Epoch 4447, Loss: 0.9266279190778732, Final Batch Loss: 0.29492878913879395\n",
      "Epoch 4448, Loss: 0.9307614266872406, Final Batch Loss: 0.20433060824871063\n",
      "Epoch 4449, Loss: 0.8457221537828445, Final Batch Loss: 0.1699090600013733\n",
      "Epoch 4450, Loss: 0.9451718181371689, Final Batch Loss: 0.2552824318408966\n",
      "Epoch 4451, Loss: 1.1120752394199371, Final Batch Loss: 0.2724951505661011\n",
      "Epoch 4452, Loss: 0.8728812485933304, Final Batch Loss: 0.21057114005088806\n",
      "Epoch 4453, Loss: 0.7823743522167206, Final Batch Loss: 0.18125523626804352\n",
      "Epoch 4454, Loss: 0.7451007068157196, Final Batch Loss: 0.2379579395055771\n",
      "Epoch 4455, Loss: 0.7592027336359024, Final Batch Loss: 0.16545262932777405\n",
      "Epoch 4456, Loss: 0.8701977878808975, Final Batch Loss: 0.16808345913887024\n",
      "Epoch 4457, Loss: 0.8518438190221786, Final Batch Loss: 0.22303622961044312\n",
      "Epoch 4458, Loss: 0.8839962929487228, Final Batch Loss: 0.26276057958602905\n",
      "Epoch 4459, Loss: 0.8514350801706314, Final Batch Loss: 0.14461418986320496\n",
      "Epoch 4460, Loss: 0.7703482955694199, Final Batch Loss: 0.16520948708057404\n",
      "Epoch 4461, Loss: 0.8573716580867767, Final Batch Loss: 0.25832346081733704\n",
      "Epoch 4462, Loss: 0.8746650069952011, Final Batch Loss: 0.2467118203639984\n",
      "Epoch 4463, Loss: 0.9945840388536453, Final Batch Loss: 0.33107447624206543\n",
      "Epoch 4464, Loss: 0.7512926459312439, Final Batch Loss: 0.20323573052883148\n",
      "Epoch 4465, Loss: 0.888042226433754, Final Batch Loss: 0.2204677313566208\n",
      "Epoch 4466, Loss: 0.8357040882110596, Final Batch Loss: 0.24724969267845154\n",
      "Epoch 4467, Loss: 0.8913972675800323, Final Batch Loss: 0.27435511350631714\n",
      "Epoch 4468, Loss: 0.8644108176231384, Final Batch Loss: 0.27757328748703003\n",
      "Epoch 4469, Loss: 1.0328809767961502, Final Batch Loss: 0.3625878095626831\n",
      "Epoch 4470, Loss: 0.8346739858388901, Final Batch Loss: 0.22147724032402039\n",
      "Epoch 4471, Loss: 0.7872432172298431, Final Batch Loss: 0.22966821491718292\n",
      "Epoch 4472, Loss: 0.815183237195015, Final Batch Loss: 0.23270690441131592\n",
      "Epoch 4473, Loss: 0.8339648395776749, Final Batch Loss: 0.22729827463626862\n",
      "Epoch 4474, Loss: 0.9258440732955933, Final Batch Loss: 0.19366136193275452\n",
      "Epoch 4475, Loss: 0.988796666264534, Final Batch Loss: 0.32186681032180786\n",
      "Epoch 4476, Loss: 0.8639417737722397, Final Batch Loss: 0.15797069668769836\n",
      "Epoch 4477, Loss: 0.8389710187911987, Final Batch Loss: 0.16006004810333252\n",
      "Epoch 4478, Loss: 0.8215698748826981, Final Batch Loss: 0.15456221997737885\n",
      "Epoch 4479, Loss: 0.9297241270542145, Final Batch Loss: 0.2109030783176422\n",
      "Epoch 4480, Loss: 0.9094171822071075, Final Batch Loss: 0.2669019103050232\n",
      "Epoch 4481, Loss: 0.8029285371303558, Final Batch Loss: 0.18652555346488953\n",
      "Epoch 4482, Loss: 0.6943491101264954, Final Batch Loss: 0.17674681544303894\n",
      "Epoch 4483, Loss: 0.9305058270692825, Final Batch Loss: 0.20761297643184662\n",
      "Epoch 4484, Loss: 0.8251802623271942, Final Batch Loss: 0.1878400593996048\n",
      "Epoch 4485, Loss: 0.8834268748760223, Final Batch Loss: 0.1658640205860138\n",
      "Epoch 4486, Loss: 0.918807789683342, Final Batch Loss: 0.27932852506637573\n",
      "Epoch 4487, Loss: 0.8115439116954803, Final Batch Loss: 0.19528795778751373\n",
      "Epoch 4488, Loss: 0.7697432786226273, Final Batch Loss: 0.1177094429731369\n",
      "Epoch 4489, Loss: 0.8048549443483353, Final Batch Loss: 0.2349446415901184\n",
      "Epoch 4490, Loss: 0.8985271751880646, Final Batch Loss: 0.2670908570289612\n",
      "Epoch 4491, Loss: 0.8595734089612961, Final Batch Loss: 0.19859535992145538\n",
      "Epoch 4492, Loss: 0.7839260548353195, Final Batch Loss: 0.18185590207576752\n",
      "Epoch 4493, Loss: 0.8358992487192154, Final Batch Loss: 0.19012965261936188\n",
      "Epoch 4494, Loss: 0.7915191054344177, Final Batch Loss: 0.1927615851163864\n",
      "Epoch 4495, Loss: 0.8327580541372299, Final Batch Loss: 0.21319611370563507\n",
      "Epoch 4496, Loss: 0.8467441946268082, Final Batch Loss: 0.1815596967935562\n",
      "Epoch 4497, Loss: 0.8546029925346375, Final Batch Loss: 0.24007275700569153\n",
      "Epoch 4498, Loss: 0.8908348977565765, Final Batch Loss: 0.22237688302993774\n",
      "Epoch 4499, Loss: 0.7893984168767929, Final Batch Loss: 0.24216847121715546\n",
      "Epoch 4500, Loss: 0.8139052242040634, Final Batch Loss: 0.12885510921478271\n",
      "Epoch 4501, Loss: 0.9042986929416656, Final Batch Loss: 0.23455755412578583\n",
      "Epoch 4502, Loss: 0.8335285782814026, Final Batch Loss: 0.1960001140832901\n",
      "Epoch 4503, Loss: 0.858939453959465, Final Batch Loss: 0.22697199881076813\n",
      "Epoch 4504, Loss: 0.9026618450880051, Final Batch Loss: 0.17281107604503632\n",
      "Epoch 4505, Loss: 0.8357162028551102, Final Batch Loss: 0.20976626873016357\n",
      "Epoch 4506, Loss: 0.9123169034719467, Final Batch Loss: 0.19818194210529327\n",
      "Epoch 4507, Loss: 0.8046180456876755, Final Batch Loss: 0.18122781813144684\n",
      "Epoch 4508, Loss: 0.9148011207580566, Final Batch Loss: 0.1755366027355194\n",
      "Epoch 4509, Loss: 0.8097106069326401, Final Batch Loss: 0.16120535135269165\n",
      "Epoch 4510, Loss: 0.8241363018751144, Final Batch Loss: 0.23039643466472626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4511, Loss: 0.8523283153772354, Final Batch Loss: 0.1501285880804062\n",
      "Epoch 4512, Loss: 0.9509855359792709, Final Batch Loss: 0.19793009757995605\n",
      "Epoch 4513, Loss: 0.9333134144544601, Final Batch Loss: 0.3582351505756378\n",
      "Epoch 4514, Loss: 0.8846729099750519, Final Batch Loss: 0.1960209459066391\n",
      "Epoch 4515, Loss: 0.868172213435173, Final Batch Loss: 0.24133510887622833\n",
      "Epoch 4516, Loss: 0.865419402718544, Final Batch Loss: 0.19766409695148468\n",
      "Epoch 4517, Loss: 0.8283663094043732, Final Batch Loss: 0.2157447338104248\n",
      "Epoch 4518, Loss: 0.917269691824913, Final Batch Loss: 0.2398497611284256\n",
      "Epoch 4519, Loss: 0.960186704993248, Final Batch Loss: 0.23715761303901672\n",
      "Epoch 4520, Loss: 0.8091642260551453, Final Batch Loss: 0.17904554307460785\n",
      "Epoch 4521, Loss: 0.8592815697193146, Final Batch Loss: 0.18471300601959229\n",
      "Epoch 4522, Loss: 0.7561538964509964, Final Batch Loss: 0.17845912277698517\n",
      "Epoch 4523, Loss: 0.8848912268877029, Final Batch Loss: 0.26110178232192993\n",
      "Epoch 4524, Loss: 0.8554660230875015, Final Batch Loss: 0.19695192575454712\n",
      "Epoch 4525, Loss: 0.9099376201629639, Final Batch Loss: 0.31162744760513306\n",
      "Epoch 4526, Loss: 0.7958627939224243, Final Batch Loss: 0.21544130146503448\n",
      "Epoch 4527, Loss: 0.9062185287475586, Final Batch Loss: 0.2808241546154022\n",
      "Epoch 4528, Loss: 0.9324347525835037, Final Batch Loss: 0.20259989798069\n",
      "Epoch 4529, Loss: 0.7767630815505981, Final Batch Loss: 0.18781818449497223\n",
      "Epoch 4530, Loss: 0.8762364387512207, Final Batch Loss: 0.13998672366142273\n",
      "Epoch 4531, Loss: 0.8915618509054184, Final Batch Loss: 0.2328181266784668\n",
      "Epoch 4532, Loss: 0.7392160594463348, Final Batch Loss: 0.15254686772823334\n",
      "Epoch 4533, Loss: 0.8735023587942123, Final Batch Loss: 0.22102367877960205\n",
      "Epoch 4534, Loss: 0.9047004133462906, Final Batch Loss: 0.3043117821216583\n",
      "Epoch 4535, Loss: 0.9235980957746506, Final Batch Loss: 0.27483969926834106\n",
      "Epoch 4536, Loss: 0.8521901071071625, Final Batch Loss: 0.23888283967971802\n",
      "Epoch 4537, Loss: 0.8301360905170441, Final Batch Loss: 0.17499922215938568\n",
      "Epoch 4538, Loss: 0.8464908301830292, Final Batch Loss: 0.21813108026981354\n",
      "Epoch 4539, Loss: 0.7456054985523224, Final Batch Loss: 0.20906341075897217\n",
      "Epoch 4540, Loss: 0.8940904438495636, Final Batch Loss: 0.20094071328639984\n",
      "Epoch 4541, Loss: 0.8247730880975723, Final Batch Loss: 0.2308109849691391\n",
      "Epoch 4542, Loss: 1.0805982053279877, Final Batch Loss: 0.2832685112953186\n",
      "Epoch 4543, Loss: 0.7409946322441101, Final Batch Loss: 0.19298431277275085\n",
      "Epoch 4544, Loss: 0.7993997037410736, Final Batch Loss: 0.17131146788597107\n",
      "Epoch 4545, Loss: 0.7762811928987503, Final Batch Loss: 0.18608000874519348\n",
      "Epoch 4546, Loss: 0.8886187672615051, Final Batch Loss: 0.23897558450698853\n",
      "Epoch 4547, Loss: 0.7786958068609238, Final Batch Loss: 0.21641044318675995\n",
      "Epoch 4548, Loss: 0.7198267728090286, Final Batch Loss: 0.23341628909111023\n",
      "Epoch 4549, Loss: 0.930554136633873, Final Batch Loss: 0.14972084760665894\n",
      "Epoch 4550, Loss: 0.8640033602714539, Final Batch Loss: 0.1826658844947815\n",
      "Epoch 4551, Loss: 0.8251636475324631, Final Batch Loss: 0.18876910209655762\n",
      "Epoch 4552, Loss: 0.9220837652683258, Final Batch Loss: 0.24460084736347198\n",
      "Epoch 4553, Loss: 0.7648828774690628, Final Batch Loss: 0.17948824167251587\n",
      "Epoch 4554, Loss: 0.8282764554023743, Final Batch Loss: 0.2048858404159546\n",
      "Epoch 4555, Loss: 0.8674139529466629, Final Batch Loss: 0.20416037738323212\n",
      "Epoch 4556, Loss: 0.962239608168602, Final Batch Loss: 0.3275579810142517\n",
      "Epoch 4557, Loss: 0.8776437044143677, Final Batch Loss: 0.18676668405532837\n",
      "Epoch 4558, Loss: 0.8038212805986404, Final Batch Loss: 0.1933841109275818\n",
      "Epoch 4559, Loss: 0.9272611886262894, Final Batch Loss: 0.1542215496301651\n",
      "Epoch 4560, Loss: 0.9189561754465103, Final Batch Loss: 0.1919073611497879\n",
      "Epoch 4561, Loss: 0.9308920949697495, Final Batch Loss: 0.19052700698375702\n",
      "Epoch 4562, Loss: 0.8823086768388748, Final Batch Loss: 0.2636897563934326\n",
      "Epoch 4563, Loss: 0.9267487823963165, Final Batch Loss: 0.19851961731910706\n",
      "Epoch 4564, Loss: 0.92218217253685, Final Batch Loss: 0.22660936415195465\n",
      "Epoch 4565, Loss: 0.8978571742773056, Final Batch Loss: 0.25239214301109314\n",
      "Epoch 4566, Loss: 0.8484368324279785, Final Batch Loss: 0.17513473331928253\n",
      "Epoch 4567, Loss: 0.7658477872610092, Final Batch Loss: 0.1794353574514389\n",
      "Epoch 4568, Loss: 0.8931661993265152, Final Batch Loss: 0.29950323700904846\n",
      "Epoch 4569, Loss: 0.8794898837804794, Final Batch Loss: 0.28456151485443115\n",
      "Epoch 4570, Loss: 0.8736716508865356, Final Batch Loss: 0.27062129974365234\n",
      "Epoch 4571, Loss: 0.8946044147014618, Final Batch Loss: 0.20925207436084747\n",
      "Epoch 4572, Loss: 0.6964113116264343, Final Batch Loss: 0.14514011144638062\n",
      "Epoch 4573, Loss: 0.8107275068759918, Final Batch Loss: 0.18100719153881073\n",
      "Epoch 4574, Loss: 0.8057336807250977, Final Batch Loss: 0.2465127557516098\n",
      "Epoch 4575, Loss: 1.0024891048669815, Final Batch Loss: 0.21956293284893036\n",
      "Epoch 4576, Loss: 0.9160978496074677, Final Batch Loss: 0.28305956721305847\n",
      "Epoch 4577, Loss: 0.8740184307098389, Final Batch Loss: 0.16121597588062286\n",
      "Epoch 4578, Loss: 0.8288913518190384, Final Batch Loss: 0.21787461638450623\n",
      "Epoch 4579, Loss: 0.933127835392952, Final Batch Loss: 0.28360188007354736\n",
      "Epoch 4580, Loss: 0.9820436239242554, Final Batch Loss: 0.2389104813337326\n",
      "Epoch 4581, Loss: 0.9097683280706406, Final Batch Loss: 0.17629152536392212\n",
      "Epoch 4582, Loss: 0.8817915618419647, Final Batch Loss: 0.23455463349819183\n",
      "Epoch 4583, Loss: 0.9295478463172913, Final Batch Loss: 0.2918574810028076\n",
      "Epoch 4584, Loss: 0.8051125258207321, Final Batch Loss: 0.2581239342689514\n",
      "Epoch 4585, Loss: 1.0189580619335175, Final Batch Loss: 0.23439890146255493\n",
      "Epoch 4586, Loss: 0.9213718920946121, Final Batch Loss: 0.1853000521659851\n",
      "Epoch 4587, Loss: 0.8590161800384521, Final Batch Loss: 0.24546468257904053\n",
      "Epoch 4588, Loss: 0.9575160443782806, Final Batch Loss: 0.2039155811071396\n",
      "Epoch 4589, Loss: 0.8728397637605667, Final Batch Loss: 0.21017569303512573\n",
      "Epoch 4590, Loss: 0.8604627549648285, Final Batch Loss: 0.16957907378673553\n",
      "Epoch 4591, Loss: 0.7793967425823212, Final Batch Loss: 0.17392626404762268\n",
      "Epoch 4592, Loss: 0.7832454442977905, Final Batch Loss: 0.1857832670211792\n",
      "Epoch 4593, Loss: 0.8361945748329163, Final Batch Loss: 0.18348270654678345\n",
      "Epoch 4594, Loss: 0.7694925963878632, Final Batch Loss: 0.17845246195793152\n",
      "Epoch 4595, Loss: 0.8229236602783203, Final Batch Loss: 0.24825194478034973\n",
      "Epoch 4596, Loss: 0.7269976064562798, Final Batch Loss: 0.20450188219547272\n",
      "Epoch 4597, Loss: 0.8683037906885147, Final Batch Loss: 0.284332811832428\n",
      "Epoch 4598, Loss: 0.9420522004365921, Final Batch Loss: 0.2171250581741333\n",
      "Epoch 4599, Loss: 0.8720061331987381, Final Batch Loss: 0.26430076360702515\n",
      "Epoch 4600, Loss: 0.796430915594101, Final Batch Loss: 0.22232145071029663\n",
      "Epoch 4601, Loss: 0.8411805182695389, Final Batch Loss: 0.1814577728509903\n",
      "Epoch 4602, Loss: 0.8374729603528976, Final Batch Loss: 0.18759547173976898\n",
      "Epoch 4603, Loss: 0.8162490874528885, Final Batch Loss: 0.1699315458536148\n",
      "Epoch 4604, Loss: 0.8156330734491348, Final Batch Loss: 0.17662832140922546\n",
      "Epoch 4605, Loss: 0.9543508589267731, Final Batch Loss: 0.3181284964084625\n",
      "Epoch 4606, Loss: 0.8145743757486343, Final Batch Loss: 0.2183074951171875\n",
      "Epoch 4607, Loss: 0.885655403137207, Final Batch Loss: 0.26704141497612\n",
      "Epoch 4608, Loss: 0.8842651844024658, Final Batch Loss: 0.2159378081560135\n",
      "Epoch 4609, Loss: 0.9050991386175156, Final Batch Loss: 0.19894208014011383\n",
      "Epoch 4610, Loss: 0.7648953124880791, Final Batch Loss: 0.23757237195968628\n",
      "Epoch 4611, Loss: 0.8688185811042786, Final Batch Loss: 0.22751912474632263\n",
      "Epoch 4612, Loss: 0.8324275612831116, Final Batch Loss: 0.19046299159526825\n",
      "Epoch 4613, Loss: 0.847623810172081, Final Batch Loss: 0.2342829704284668\n",
      "Epoch 4614, Loss: 0.8838648647069931, Final Batch Loss: 0.1521201878786087\n",
      "Epoch 4615, Loss: 0.767455667257309, Final Batch Loss: 0.21385616064071655\n",
      "Epoch 4616, Loss: 0.9611969441175461, Final Batch Loss: 0.2806006073951721\n",
      "Epoch 4617, Loss: 0.8770726174116135, Final Batch Loss: 0.1971968412399292\n",
      "Epoch 4618, Loss: 0.9409257918596268, Final Batch Loss: 0.21623754501342773\n",
      "Epoch 4619, Loss: 0.9196418821811676, Final Batch Loss: 0.26685482263565063\n",
      "Epoch 4620, Loss: 0.9908207505941391, Final Batch Loss: 0.1619500368833542\n",
      "Epoch 4621, Loss: 0.8633480817079544, Final Batch Loss: 0.14565709233283997\n",
      "Epoch 4622, Loss: 0.8770414292812347, Final Batch Loss: 0.11835472285747528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4623, Loss: 0.8332380950450897, Final Batch Loss: 0.199644535779953\n",
      "Epoch 4624, Loss: 0.8666656464338303, Final Batch Loss: 0.1224917322397232\n",
      "Epoch 4625, Loss: 1.0169214606285095, Final Batch Loss: 0.2512088119983673\n",
      "Epoch 4626, Loss: 0.9278150945901871, Final Batch Loss: 0.261511892080307\n",
      "Epoch 4627, Loss: 0.9706252962350845, Final Batch Loss: 0.23392078280448914\n",
      "Epoch 4628, Loss: 0.957011416554451, Final Batch Loss: 0.3213624954223633\n",
      "Epoch 4629, Loss: 0.9343805760145187, Final Batch Loss: 0.19935542345046997\n",
      "Epoch 4630, Loss: 0.903928741812706, Final Batch Loss: 0.26513931155204773\n",
      "Epoch 4631, Loss: 0.9417438358068466, Final Batch Loss: 0.33979710936546326\n",
      "Epoch 4632, Loss: 0.8775253444910049, Final Batch Loss: 0.27008819580078125\n",
      "Epoch 4633, Loss: 0.8302771151065826, Final Batch Loss: 0.2301267832517624\n",
      "Epoch 4634, Loss: 0.8641787320375443, Final Batch Loss: 0.23789910972118378\n",
      "Epoch 4635, Loss: 0.8959742486476898, Final Batch Loss: 0.19661492109298706\n",
      "Epoch 4636, Loss: 0.8210524171590805, Final Batch Loss: 0.24610425531864166\n",
      "Epoch 4637, Loss: 0.9799005538225174, Final Batch Loss: 0.24424123764038086\n",
      "Epoch 4638, Loss: 0.871045857667923, Final Batch Loss: 0.15938694775104523\n",
      "Epoch 4639, Loss: 0.9018794298171997, Final Batch Loss: 0.18394455313682556\n",
      "Epoch 4640, Loss: 0.9418723732233047, Final Batch Loss: 0.2943762540817261\n",
      "Epoch 4641, Loss: 0.8959458321332932, Final Batch Loss: 0.2549382448196411\n",
      "Epoch 4642, Loss: 0.9487216472625732, Final Batch Loss: 0.30652719736099243\n",
      "Epoch 4643, Loss: 0.8092751950025558, Final Batch Loss: 0.1707693189382553\n",
      "Epoch 4644, Loss: 0.8733782172203064, Final Batch Loss: 0.23574817180633545\n",
      "Epoch 4645, Loss: 0.8809106796979904, Final Batch Loss: 0.2346716672182083\n",
      "Epoch 4646, Loss: 0.7822138220071793, Final Batch Loss: 0.13236963748931885\n",
      "Epoch 4647, Loss: 0.8092785477638245, Final Batch Loss: 0.192946657538414\n",
      "Epoch 4648, Loss: 0.8896304816007614, Final Batch Loss: 0.17953631281852722\n",
      "Epoch 4649, Loss: 0.8866996914148331, Final Batch Loss: 0.2748352289199829\n",
      "Epoch 4650, Loss: 0.8905587196350098, Final Batch Loss: 0.2352609932422638\n",
      "Epoch 4651, Loss: 0.9264883100986481, Final Batch Loss: 0.3437211811542511\n",
      "Epoch 4652, Loss: 0.8332665264606476, Final Batch Loss: 0.22831887006759644\n",
      "Epoch 4653, Loss: 0.8595041185617447, Final Batch Loss: 0.19403457641601562\n",
      "Epoch 4654, Loss: 0.7961104065179825, Final Batch Loss: 0.23736372590065002\n",
      "Epoch 4655, Loss: 0.8957112580537796, Final Batch Loss: 0.20724812150001526\n",
      "Epoch 4656, Loss: 0.7560360729694366, Final Batch Loss: 0.1705668419599533\n",
      "Epoch 4657, Loss: 0.9089568704366684, Final Batch Loss: 0.19016726315021515\n",
      "Epoch 4658, Loss: 0.8635673075914383, Final Batch Loss: 0.15912692248821259\n",
      "Epoch 4659, Loss: 0.8378640562295914, Final Batch Loss: 0.227868914604187\n",
      "Epoch 4660, Loss: 0.911195918917656, Final Batch Loss: 0.16232602298259735\n",
      "Epoch 4661, Loss: 0.8560685515403748, Final Batch Loss: 0.18482255935668945\n",
      "Epoch 4662, Loss: 0.8249089419841766, Final Batch Loss: 0.2079889327287674\n",
      "Epoch 4663, Loss: 0.7973363697528839, Final Batch Loss: 0.19941109418869019\n",
      "Epoch 4664, Loss: 0.807415634393692, Final Batch Loss: 0.17752167582511902\n",
      "Epoch 4665, Loss: 0.7693803161382675, Final Batch Loss: 0.25657960772514343\n",
      "Epoch 4666, Loss: 0.841120645403862, Final Batch Loss: 0.18447019159793854\n",
      "Epoch 4667, Loss: 0.9254798293113708, Final Batch Loss: 0.3323167860507965\n",
      "Epoch 4668, Loss: 0.7877693474292755, Final Batch Loss: 0.2065431773662567\n",
      "Epoch 4669, Loss: 0.9254466146230698, Final Batch Loss: 0.2904530465602875\n",
      "Epoch 4670, Loss: 0.876048356294632, Final Batch Loss: 0.25108054280281067\n",
      "Epoch 4671, Loss: 0.8582283109426498, Final Batch Loss: 0.217705637216568\n",
      "Epoch 4672, Loss: 0.8294630646705627, Final Batch Loss: 0.12147550284862518\n",
      "Epoch 4673, Loss: 0.7996891736984253, Final Batch Loss: 0.2320825308561325\n",
      "Epoch 4674, Loss: 0.8125494718551636, Final Batch Loss: 0.2092488706111908\n",
      "Epoch 4675, Loss: 0.8103219717741013, Final Batch Loss: 0.22612817585468292\n",
      "Epoch 4676, Loss: 0.8216834217309952, Final Batch Loss: 0.2648867070674896\n",
      "Epoch 4677, Loss: 0.8993916511535645, Final Batch Loss: 0.18670307099819183\n",
      "Epoch 4678, Loss: 0.7697317153215408, Final Batch Loss: 0.17872413992881775\n",
      "Epoch 4679, Loss: 0.9508386254310608, Final Batch Loss: 0.20506100356578827\n",
      "Epoch 4680, Loss: 0.8738904744386673, Final Batch Loss: 0.25927841663360596\n",
      "Epoch 4681, Loss: 0.916650801897049, Final Batch Loss: 0.29087725281715393\n",
      "Epoch 4682, Loss: 0.81756491959095, Final Batch Loss: 0.18281586468219757\n",
      "Epoch 4683, Loss: 0.8643600046634674, Final Batch Loss: 0.23104852437973022\n",
      "Epoch 4684, Loss: 0.9781413525342941, Final Batch Loss: 0.2750019431114197\n",
      "Epoch 4685, Loss: 0.8452162146568298, Final Batch Loss: 0.2066573202610016\n",
      "Epoch 4686, Loss: 0.7848424464464188, Final Batch Loss: 0.16499149799346924\n",
      "Epoch 4687, Loss: 0.780766099691391, Final Batch Loss: 0.2156490534543991\n",
      "Epoch 4688, Loss: 0.8200330138206482, Final Batch Loss: 0.24009226262569427\n",
      "Epoch 4689, Loss: 0.9673590213060379, Final Batch Loss: 0.2648121416568756\n",
      "Epoch 4690, Loss: 0.8037085235118866, Final Batch Loss: 0.20815131068229675\n",
      "Epoch 4691, Loss: 0.8340137898921967, Final Batch Loss: 0.17124463617801666\n",
      "Epoch 4692, Loss: 0.8648807555437088, Final Batch Loss: 0.1639062911272049\n",
      "Epoch 4693, Loss: 0.9250708222389221, Final Batch Loss: 0.17901210486888885\n",
      "Epoch 4694, Loss: 0.8955873996019363, Final Batch Loss: 0.21929767727851868\n",
      "Epoch 4695, Loss: 0.8808961063623428, Final Batch Loss: 0.32525596022605896\n",
      "Epoch 4696, Loss: 0.8704993575811386, Final Batch Loss: 0.21226553618907928\n",
      "Epoch 4697, Loss: 0.8847286254167557, Final Batch Loss: 0.12792974710464478\n",
      "Epoch 4698, Loss: 0.693303257226944, Final Batch Loss: 0.20492245256900787\n",
      "Epoch 4699, Loss: 0.7863433361053467, Final Batch Loss: 0.16280487179756165\n",
      "Epoch 4700, Loss: 0.7990138232707977, Final Batch Loss: 0.23273494839668274\n",
      "Epoch 4701, Loss: 0.7676304280757904, Final Batch Loss: 0.16561388969421387\n",
      "Epoch 4702, Loss: 0.8995465785264969, Final Batch Loss: 0.1778438240289688\n",
      "Epoch 4703, Loss: 0.7954598218202591, Final Batch Loss: 0.1983998864889145\n",
      "Epoch 4704, Loss: 0.8618717342615128, Final Batch Loss: 0.2358010709285736\n",
      "Epoch 4705, Loss: 0.7451401352882385, Final Batch Loss: 0.18178309500217438\n",
      "Epoch 4706, Loss: 0.7724052965641022, Final Batch Loss: 0.15898534655570984\n",
      "Epoch 4707, Loss: 0.8324232846498489, Final Batch Loss: 0.22368189692497253\n",
      "Epoch 4708, Loss: 0.8203820586204529, Final Batch Loss: 0.23465074598789215\n",
      "Epoch 4709, Loss: 0.8667200207710266, Final Batch Loss: 0.1599493771791458\n",
      "Epoch 4710, Loss: 0.8809589296579361, Final Batch Loss: 0.2912527918815613\n",
      "Epoch 4711, Loss: 0.8112884759902954, Final Batch Loss: 0.1663719266653061\n",
      "Epoch 4712, Loss: 0.8027030825614929, Final Batch Loss: 0.20364436507225037\n",
      "Epoch 4713, Loss: 0.8472405076026917, Final Batch Loss: 0.20424745976924896\n",
      "Epoch 4714, Loss: 0.8084222078323364, Final Batch Loss: 0.2077142298221588\n",
      "Epoch 4715, Loss: 0.7990122437477112, Final Batch Loss: 0.2350948303937912\n",
      "Epoch 4716, Loss: 0.8775862753391266, Final Batch Loss: 0.1955980807542801\n",
      "Epoch 4717, Loss: 0.8169508725404739, Final Batch Loss: 0.29017147421836853\n",
      "Epoch 4718, Loss: 0.8162304759025574, Final Batch Loss: 0.17601488530635834\n",
      "Epoch 4719, Loss: 0.9039656668901443, Final Batch Loss: 0.1781969964504242\n",
      "Epoch 4720, Loss: 0.8094918876886368, Final Batch Loss: 0.17566552758216858\n",
      "Epoch 4721, Loss: 0.8049565404653549, Final Batch Loss: 0.18156881630420685\n",
      "Epoch 4722, Loss: 0.759041503071785, Final Batch Loss: 0.18284796178340912\n",
      "Epoch 4723, Loss: 0.8845048993825912, Final Batch Loss: 0.17593538761138916\n",
      "Epoch 4724, Loss: 0.8423667550086975, Final Batch Loss: 0.25930237770080566\n",
      "Epoch 4725, Loss: 0.8467061519622803, Final Batch Loss: 0.2668516933917999\n",
      "Epoch 4726, Loss: 0.8269854635000229, Final Batch Loss: 0.16920752823352814\n",
      "Epoch 4727, Loss: 0.8498543202877045, Final Batch Loss: 0.24945944547653198\n",
      "Epoch 4728, Loss: 0.8102553188800812, Final Batch Loss: 0.17926962673664093\n",
      "Epoch 4729, Loss: 0.8344421684741974, Final Batch Loss: 0.23786818981170654\n",
      "Epoch 4730, Loss: 0.7429451793432236, Final Batch Loss: 0.17354145646095276\n",
      "Epoch 4731, Loss: 0.8858502209186554, Final Batch Loss: 0.24738137423992157\n",
      "Epoch 4732, Loss: 0.7845171093940735, Final Batch Loss: 0.16992920637130737\n",
      "Epoch 4733, Loss: 0.8684972524642944, Final Batch Loss: 0.2338789701461792\n",
      "Epoch 4734, Loss: 0.8007116764783859, Final Batch Loss: 0.2351442277431488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4735, Loss: 0.9015823751688004, Final Batch Loss: 0.23263834416866302\n",
      "Epoch 4736, Loss: 0.8408369868993759, Final Batch Loss: 0.2702752947807312\n",
      "Epoch 4737, Loss: 0.9916572272777557, Final Batch Loss: 0.2231534868478775\n",
      "Epoch 4738, Loss: 0.7599506825208664, Final Batch Loss: 0.2099740207195282\n",
      "Epoch 4739, Loss: 0.8574369251728058, Final Batch Loss: 0.2800635099411011\n",
      "Epoch 4740, Loss: 0.9526068866252899, Final Batch Loss: 0.21014802157878876\n",
      "Epoch 4741, Loss: 0.7323848605155945, Final Batch Loss: 0.1547473520040512\n",
      "Epoch 4742, Loss: 0.8346754312515259, Final Batch Loss: 0.26051244139671326\n",
      "Epoch 4743, Loss: 0.8690130561590195, Final Batch Loss: 0.24579264223575592\n",
      "Epoch 4744, Loss: 0.766782596707344, Final Batch Loss: 0.13168759644031525\n",
      "Epoch 4745, Loss: 0.808363288640976, Final Batch Loss: 0.20445182919502258\n",
      "Epoch 4746, Loss: 0.8802973926067352, Final Batch Loss: 0.2919756770133972\n",
      "Epoch 4747, Loss: 0.7910744696855545, Final Batch Loss: 0.1528475433588028\n",
      "Epoch 4748, Loss: 0.77356818318367, Final Batch Loss: 0.15280793607234955\n",
      "Epoch 4749, Loss: 0.8887924700975418, Final Batch Loss: 0.20465008914470673\n",
      "Epoch 4750, Loss: 0.7639740854501724, Final Batch Loss: 0.16678260266780853\n",
      "Epoch 4751, Loss: 0.8620187640190125, Final Batch Loss: 0.21929514408111572\n",
      "Epoch 4752, Loss: 0.7877921015024185, Final Batch Loss: 0.21167351305484772\n",
      "Epoch 4753, Loss: 0.7802293598651886, Final Batch Loss: 0.18801389634609222\n",
      "Epoch 4754, Loss: 0.9600736945867538, Final Batch Loss: 0.2145826667547226\n",
      "Epoch 4755, Loss: 0.8894291967153549, Final Batch Loss: 0.22722139954566956\n",
      "Epoch 4756, Loss: 0.7983072996139526, Final Batch Loss: 0.15136921405792236\n",
      "Epoch 4757, Loss: 0.8801174014806747, Final Batch Loss: 0.21253302693367004\n",
      "Epoch 4758, Loss: 0.7498575150966644, Final Batch Loss: 0.1620904505252838\n",
      "Epoch 4759, Loss: 0.8712009638547897, Final Batch Loss: 0.20324772596359253\n",
      "Epoch 4760, Loss: 0.9096174538135529, Final Batch Loss: 0.18686099350452423\n",
      "Epoch 4761, Loss: 0.8758319765329361, Final Batch Loss: 0.21490536630153656\n",
      "Epoch 4762, Loss: 0.8154724985361099, Final Batch Loss: 0.28632140159606934\n",
      "Epoch 4763, Loss: 0.9022537618875504, Final Batch Loss: 0.22536619007587433\n",
      "Epoch 4764, Loss: 0.7999647557735443, Final Batch Loss: 0.17184460163116455\n",
      "Epoch 4765, Loss: 0.8930922448635101, Final Batch Loss: 0.26123490929603577\n",
      "Epoch 4766, Loss: 0.9465252310037613, Final Batch Loss: 0.29080304503440857\n",
      "Epoch 4767, Loss: 0.8953961431980133, Final Batch Loss: 0.23743751645088196\n",
      "Epoch 4768, Loss: 0.7920822203159332, Final Batch Loss: 0.25760379433631897\n",
      "Epoch 4769, Loss: 0.9234249889850616, Final Batch Loss: 0.2698521614074707\n",
      "Epoch 4770, Loss: 0.8733894973993301, Final Batch Loss: 0.2257639765739441\n",
      "Epoch 4771, Loss: 0.7713717222213745, Final Batch Loss: 0.2183207869529724\n",
      "Epoch 4772, Loss: 0.9157234579324722, Final Batch Loss: 0.23918521404266357\n",
      "Epoch 4773, Loss: 0.9423960596323013, Final Batch Loss: 0.2771652638912201\n",
      "Epoch 4774, Loss: 0.8201087564229965, Final Batch Loss: 0.22817043960094452\n",
      "Epoch 4775, Loss: 0.783414900302887, Final Batch Loss: 0.21297834813594818\n",
      "Epoch 4776, Loss: 0.7814543843269348, Final Batch Loss: 0.17926694452762604\n",
      "Epoch 4777, Loss: 0.7849607616662979, Final Batch Loss: 0.20149628818035126\n",
      "Epoch 4778, Loss: 0.9021899849176407, Final Batch Loss: 0.18251802027225494\n",
      "Epoch 4779, Loss: 0.7935431897640228, Final Batch Loss: 0.1843353509902954\n",
      "Epoch 4780, Loss: 0.8542931526899338, Final Batch Loss: 0.31856197118759155\n",
      "Epoch 4781, Loss: 0.8580790609121323, Final Batch Loss: 0.2720384895801544\n",
      "Epoch 4782, Loss: 0.7157607674598694, Final Batch Loss: 0.13256137073040009\n",
      "Epoch 4783, Loss: 0.72536501288414, Final Batch Loss: 0.17088061571121216\n",
      "Epoch 4784, Loss: 0.7700094133615494, Final Batch Loss: 0.1915227174758911\n",
      "Epoch 4785, Loss: 0.7435653656721115, Final Batch Loss: 0.2012120932340622\n",
      "Epoch 4786, Loss: 0.8690827339887619, Final Batch Loss: 0.2414020448923111\n",
      "Epoch 4787, Loss: 0.8300066888332367, Final Batch Loss: 0.2214726209640503\n",
      "Epoch 4788, Loss: 0.9553554207086563, Final Batch Loss: 0.2645406424999237\n",
      "Epoch 4789, Loss: 1.077329769730568, Final Batch Loss: 0.3536466360092163\n",
      "Epoch 4790, Loss: 0.8375307619571686, Final Batch Loss: 0.2560395300388336\n",
      "Epoch 4791, Loss: 0.7658121734857559, Final Batch Loss: 0.2003551721572876\n",
      "Epoch 4792, Loss: 0.996629923582077, Final Batch Loss: 0.3490464687347412\n",
      "Epoch 4793, Loss: 0.9129258543252945, Final Batch Loss: 0.21235817670822144\n",
      "Epoch 4794, Loss: 0.9524295032024384, Final Batch Loss: 0.26632431149482727\n",
      "Epoch 4795, Loss: 0.8234288692474365, Final Batch Loss: 0.20089386403560638\n",
      "Epoch 4796, Loss: 0.8074996769428253, Final Batch Loss: 0.20164474844932556\n",
      "Epoch 4797, Loss: 0.836436465382576, Final Batch Loss: 0.18913164734840393\n",
      "Epoch 4798, Loss: 0.8949397802352905, Final Batch Loss: 0.21527701616287231\n",
      "Epoch 4799, Loss: 0.9213744699954987, Final Batch Loss: 0.2395712435245514\n",
      "Epoch 4800, Loss: 0.8930435180664062, Final Batch Loss: 0.22691939771175385\n",
      "Epoch 4801, Loss: 0.9057084321975708, Final Batch Loss: 0.20431344211101532\n",
      "Epoch 4802, Loss: 0.8883022218942642, Final Batch Loss: 0.2632855474948883\n",
      "Epoch 4803, Loss: 0.7753555476665497, Final Batch Loss: 0.19385848939418793\n",
      "Epoch 4804, Loss: 0.8184658885002136, Final Batch Loss: 0.25268134474754333\n",
      "Epoch 4805, Loss: 0.910878136754036, Final Batch Loss: 0.28946903347969055\n",
      "Epoch 4806, Loss: 0.80963434278965, Final Batch Loss: 0.25132057070732117\n",
      "Epoch 4807, Loss: 0.8760468512773514, Final Batch Loss: 0.17355670034885406\n",
      "Epoch 4808, Loss: 0.8859816193580627, Final Batch Loss: 0.2619870901107788\n",
      "Epoch 4809, Loss: 0.8654507994651794, Final Batch Loss: 0.17414915561676025\n",
      "Epoch 4810, Loss: 1.0485300868749619, Final Batch Loss: 0.28212183713912964\n",
      "Epoch 4811, Loss: 0.9568433910608292, Final Batch Loss: 0.2770915925502777\n",
      "Epoch 4812, Loss: 0.9444894641637802, Final Batch Loss: 0.31357380747795105\n",
      "Epoch 4813, Loss: 0.7999478578567505, Final Batch Loss: 0.24380899965763092\n",
      "Epoch 4814, Loss: 1.0240272879600525, Final Batch Loss: 0.24821260571479797\n",
      "Epoch 4815, Loss: 0.7994996011257172, Final Batch Loss: 0.17980261147022247\n",
      "Epoch 4816, Loss: 0.9828814715147018, Final Batch Loss: 0.18959350883960724\n",
      "Epoch 4817, Loss: 0.8848129957914352, Final Batch Loss: 0.1754845231771469\n",
      "Epoch 4818, Loss: 0.8979144245386124, Final Batch Loss: 0.20376387238502502\n",
      "Epoch 4819, Loss: 0.8410128355026245, Final Batch Loss: 0.19576947391033173\n",
      "Epoch 4820, Loss: 0.7318457961082458, Final Batch Loss: 0.18404914438724518\n",
      "Epoch 4821, Loss: 0.7886226177215576, Final Batch Loss: 0.17671184241771698\n",
      "Epoch 4822, Loss: 0.8111301511526108, Final Batch Loss: 0.20740056037902832\n",
      "Epoch 4823, Loss: 0.967351645231247, Final Batch Loss: 0.2825465500354767\n",
      "Epoch 4824, Loss: 0.8192811906337738, Final Batch Loss: 0.19858087599277496\n",
      "Epoch 4825, Loss: 0.954957127571106, Final Batch Loss: 0.2866570055484772\n",
      "Epoch 4826, Loss: 0.7371194362640381, Final Batch Loss: 0.1519644558429718\n",
      "Epoch 4827, Loss: 0.796654224395752, Final Batch Loss: 0.17300066351890564\n",
      "Epoch 4828, Loss: 0.7759471386671066, Final Batch Loss: 0.16304157674312592\n",
      "Epoch 4829, Loss: 0.851394772529602, Final Batch Loss: 0.2268436998128891\n",
      "Epoch 4830, Loss: 0.8738860934972763, Final Batch Loss: 0.20225803554058075\n",
      "Epoch 4831, Loss: 0.7669819742441177, Final Batch Loss: 0.15207406878471375\n",
      "Epoch 4832, Loss: 0.8471480458974838, Final Batch Loss: 0.22749626636505127\n",
      "Epoch 4833, Loss: 0.8275279402732849, Final Batch Loss: 0.13303740322589874\n",
      "Epoch 4834, Loss: 0.8323730677366257, Final Batch Loss: 0.20421524345874786\n",
      "Epoch 4835, Loss: 0.7798032164573669, Final Batch Loss: 0.13223399221897125\n",
      "Epoch 4836, Loss: 0.8884509205818176, Final Batch Loss: 0.20626945793628693\n",
      "Epoch 4837, Loss: 0.776768147945404, Final Batch Loss: 0.16372258961200714\n",
      "Epoch 4838, Loss: 0.8350598067045212, Final Batch Loss: 0.2340063601732254\n",
      "Epoch 4839, Loss: 0.6897393241524696, Final Batch Loss: 0.12199438363313675\n",
      "Epoch 4840, Loss: 0.854522168636322, Final Batch Loss: 0.22083641588687897\n",
      "Epoch 4841, Loss: 0.8687408864498138, Final Batch Loss: 0.18343225121498108\n",
      "Epoch 4842, Loss: 0.9675279706716537, Final Batch Loss: 0.2673261761665344\n",
      "Epoch 4843, Loss: 0.8171569108963013, Final Batch Loss: 0.26888057589530945\n",
      "Epoch 4844, Loss: 0.8064944595098495, Final Batch Loss: 0.15669067203998566\n",
      "Epoch 4845, Loss: 1.0086782723665237, Final Batch Loss: 0.3117361068725586\n",
      "Epoch 4846, Loss: 0.7808802723884583, Final Batch Loss: 0.16504190862178802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4847, Loss: 0.7779618501663208, Final Batch Loss: 0.17674511671066284\n",
      "Epoch 4848, Loss: 0.9199516475200653, Final Batch Loss: 0.24861587584018707\n",
      "Epoch 4849, Loss: 0.9417563527822495, Final Batch Loss: 0.28391149640083313\n",
      "Epoch 4850, Loss: 0.863903596997261, Final Batch Loss: 0.21441254019737244\n",
      "Epoch 4851, Loss: 1.0240589827299118, Final Batch Loss: 0.3017544746398926\n",
      "Epoch 4852, Loss: 1.0021569281816483, Final Batch Loss: 0.21185488998889923\n",
      "Epoch 4853, Loss: 0.9691563248634338, Final Batch Loss: 0.2438657134771347\n",
      "Epoch 4854, Loss: 0.8072469979524612, Final Batch Loss: 0.20569449663162231\n",
      "Epoch 4855, Loss: 0.8341134190559387, Final Batch Loss: 0.19130252301692963\n",
      "Epoch 4856, Loss: 0.8441468775272369, Final Batch Loss: 0.2022807002067566\n",
      "Epoch 4857, Loss: 0.8934753984212875, Final Batch Loss: 0.17346996068954468\n",
      "Epoch 4858, Loss: 0.8025309294462204, Final Batch Loss: 0.16427883505821228\n",
      "Epoch 4859, Loss: 0.8414255678653717, Final Batch Loss: 0.18297655880451202\n",
      "Epoch 4860, Loss: 0.9395202696323395, Final Batch Loss: 0.2884710431098938\n",
      "Epoch 4861, Loss: 0.8236412405967712, Final Batch Loss: 0.2164434939622879\n",
      "Epoch 4862, Loss: 0.9301268011331558, Final Batch Loss: 0.30881553888320923\n",
      "Epoch 4863, Loss: 0.8559743911027908, Final Batch Loss: 0.27156051993370056\n",
      "Epoch 4864, Loss: 0.9068974703550339, Final Batch Loss: 0.3748435974121094\n",
      "Epoch 4865, Loss: 0.827954038977623, Final Batch Loss: 0.18369926512241364\n",
      "Epoch 4866, Loss: 0.8986938893795013, Final Batch Loss: 0.22028081119060516\n",
      "Epoch 4867, Loss: 0.8090090304613113, Final Batch Loss: 0.2129887193441391\n",
      "Epoch 4868, Loss: 0.8570470958948135, Final Batch Loss: 0.20219190418720245\n",
      "Epoch 4869, Loss: 0.8540069609880447, Final Batch Loss: 0.19732584059238434\n",
      "Epoch 4870, Loss: 0.7502201646566391, Final Batch Loss: 0.18674083054065704\n",
      "Epoch 4871, Loss: 0.7746125161647797, Final Batch Loss: 0.26345106959342957\n",
      "Epoch 4872, Loss: 0.8213188797235489, Final Batch Loss: 0.17315997183322906\n",
      "Epoch 4873, Loss: 0.8818837255239487, Final Batch Loss: 0.22813497483730316\n",
      "Epoch 4874, Loss: 0.8599957078695297, Final Batch Loss: 0.2468840479850769\n",
      "Epoch 4875, Loss: 0.9315879940986633, Final Batch Loss: 0.272539883852005\n",
      "Epoch 4876, Loss: 0.8447701036930084, Final Batch Loss: 0.20404615998268127\n",
      "Epoch 4877, Loss: 0.8999635428190231, Final Batch Loss: 0.21919700503349304\n",
      "Epoch 4878, Loss: 0.8393873274326324, Final Batch Loss: 0.25407466292381287\n",
      "Epoch 4879, Loss: 0.8887692391872406, Final Batch Loss: 0.17819610238075256\n",
      "Epoch 4880, Loss: 0.8350093960762024, Final Batch Loss: 0.25205764174461365\n",
      "Epoch 4881, Loss: 0.7960085421800613, Final Batch Loss: 0.2076476663351059\n",
      "Epoch 4882, Loss: 0.7971387803554535, Final Batch Loss: 0.1604548543691635\n",
      "Epoch 4883, Loss: 0.8977517932653427, Final Batch Loss: 0.37184908986091614\n",
      "Epoch 4884, Loss: 0.8581466674804688, Final Batch Loss: 0.16620750725269318\n",
      "Epoch 4885, Loss: 0.8436540961265564, Final Batch Loss: 0.24885740876197815\n",
      "Epoch 4886, Loss: 0.7866758853197098, Final Batch Loss: 0.17591848969459534\n",
      "Epoch 4887, Loss: 0.8467522114515305, Final Batch Loss: 0.1899535357952118\n",
      "Epoch 4888, Loss: 0.7961734980344772, Final Batch Loss: 0.1664634346961975\n",
      "Epoch 4889, Loss: 0.76177778840065, Final Batch Loss: 0.15747572481632233\n",
      "Epoch 4890, Loss: 0.8440386205911636, Final Batch Loss: 0.15229125320911407\n",
      "Epoch 4891, Loss: 0.7198839113116264, Final Batch Loss: 0.10904719680547714\n",
      "Epoch 4892, Loss: 0.875357985496521, Final Batch Loss: 0.26684635877609253\n",
      "Epoch 4893, Loss: 0.8586391508579254, Final Batch Loss: 0.24378560483455658\n",
      "Epoch 4894, Loss: 0.8565858900547028, Final Batch Loss: 0.1648414582014084\n",
      "Epoch 4895, Loss: 0.8408517092466354, Final Batch Loss: 0.14686256647109985\n",
      "Epoch 4896, Loss: 0.9263132959604263, Final Batch Loss: 0.24806064367294312\n",
      "Epoch 4897, Loss: 0.8698109090328217, Final Batch Loss: 0.2642506957054138\n",
      "Epoch 4898, Loss: 0.7186941802501678, Final Batch Loss: 0.13446027040481567\n",
      "Epoch 4899, Loss: 0.864825889468193, Final Batch Loss: 0.25598928332328796\n",
      "Epoch 4900, Loss: 0.8712659031152725, Final Batch Loss: 0.19114531576633453\n",
      "Epoch 4901, Loss: 0.8401722013950348, Final Batch Loss: 0.23694859445095062\n",
      "Epoch 4902, Loss: 0.9230868816375732, Final Batch Loss: 0.24644644558429718\n",
      "Epoch 4903, Loss: 0.8221879601478577, Final Batch Loss: 0.263558566570282\n",
      "Epoch 4904, Loss: 0.9453971683979034, Final Batch Loss: 0.22350619733333588\n",
      "Epoch 4905, Loss: 0.7934808731079102, Final Batch Loss: 0.22432784736156464\n",
      "Epoch 4906, Loss: 0.8572588711977005, Final Batch Loss: 0.24748973548412323\n",
      "Epoch 4907, Loss: 0.7983306050300598, Final Batch Loss: 0.19651615619659424\n",
      "Epoch 4908, Loss: 0.7163034528493881, Final Batch Loss: 0.19087085127830505\n",
      "Epoch 4909, Loss: 0.8226941972970963, Final Batch Loss: 0.1888510286808014\n",
      "Epoch 4910, Loss: 0.8106940537691116, Final Batch Loss: 0.20282970368862152\n",
      "Epoch 4911, Loss: 1.0381308645009995, Final Batch Loss: 0.3024252951145172\n",
      "Epoch 4912, Loss: 0.9591624289751053, Final Batch Loss: 0.31510499119758606\n",
      "Epoch 4913, Loss: 1.0056419670581818, Final Batch Loss: 0.20543383061885834\n",
      "Epoch 4914, Loss: 1.0188231766223907, Final Batch Loss: 0.3593060374259949\n",
      "Epoch 4915, Loss: 0.8370032012462616, Final Batch Loss: 0.19949957728385925\n",
      "Epoch 4916, Loss: 0.9523544609546661, Final Batch Loss: 0.22944074869155884\n",
      "Epoch 4917, Loss: 1.1037328839302063, Final Batch Loss: 0.2895960509777069\n",
      "Epoch 4918, Loss: 0.9307429939508438, Final Batch Loss: 0.2266325205564499\n",
      "Epoch 4919, Loss: 0.774981364607811, Final Batch Loss: 0.2388484627008438\n",
      "Epoch 4920, Loss: 0.9557221382856369, Final Batch Loss: 0.25597652792930603\n",
      "Epoch 4921, Loss: 0.8620933890342712, Final Batch Loss: 0.1885807365179062\n",
      "Epoch 4922, Loss: 1.000403493642807, Final Batch Loss: 0.20825423300266266\n",
      "Epoch 4923, Loss: 0.8755788654088974, Final Batch Loss: 0.2536594569683075\n",
      "Epoch 4924, Loss: 0.8394828587770462, Final Batch Loss: 0.23395490646362305\n",
      "Epoch 4925, Loss: 0.8139051795005798, Final Batch Loss: 0.17976368963718414\n",
      "Epoch 4926, Loss: 0.8322458118200302, Final Batch Loss: 0.1691727489233017\n",
      "Epoch 4927, Loss: 0.8948320597410202, Final Batch Loss: 0.29387757182121277\n",
      "Epoch 4928, Loss: 0.8589954525232315, Final Batch Loss: 0.2583730220794678\n",
      "Epoch 4929, Loss: 0.8899418711662292, Final Batch Loss: 0.20418931543827057\n",
      "Epoch 4930, Loss: 0.8051868230104446, Final Batch Loss: 0.19395461678504944\n",
      "Epoch 4931, Loss: 0.7936543226242065, Final Batch Loss: 0.17142269015312195\n",
      "Epoch 4932, Loss: 0.8741567134857178, Final Batch Loss: 0.2821499705314636\n",
      "Epoch 4933, Loss: 1.0433620065450668, Final Batch Loss: 0.26561233401298523\n",
      "Epoch 4934, Loss: 0.9122614860534668, Final Batch Loss: 0.17782697081565857\n",
      "Epoch 4935, Loss: 0.8670727908611298, Final Batch Loss: 0.2219042032957077\n",
      "Epoch 4936, Loss: 0.8933074623346329, Final Batch Loss: 0.20102354884147644\n",
      "Epoch 4937, Loss: 0.857380747795105, Final Batch Loss: 0.2510620355606079\n",
      "Epoch 4938, Loss: 0.8527195900678635, Final Batch Loss: 0.20093457400798798\n",
      "Epoch 4939, Loss: 0.8548227995634079, Final Batch Loss: 0.18775051832199097\n",
      "Epoch 4940, Loss: 0.7833642512559891, Final Batch Loss: 0.2507726550102234\n",
      "Epoch 4941, Loss: 0.9122490435838699, Final Batch Loss: 0.26000937819480896\n",
      "Epoch 4942, Loss: 0.822908416390419, Final Batch Loss: 0.24920521676540375\n",
      "Epoch 4943, Loss: 0.7670224756002426, Final Batch Loss: 0.1799936592578888\n",
      "Epoch 4944, Loss: 0.820177435874939, Final Batch Loss: 0.18795455992221832\n",
      "Epoch 4945, Loss: 0.7453096508979797, Final Batch Loss: 0.15956692397594452\n",
      "Epoch 4946, Loss: 0.872279942035675, Final Batch Loss: 0.18816885352134705\n",
      "Epoch 4947, Loss: 0.8331930786371231, Final Batch Loss: 0.2011990249156952\n",
      "Epoch 4948, Loss: 0.8967118412256241, Final Batch Loss: 0.2205575555562973\n",
      "Epoch 4949, Loss: 0.9280300438404083, Final Batch Loss: 0.24264203011989594\n",
      "Epoch 4950, Loss: 0.8248691111803055, Final Batch Loss: 0.19261112809181213\n",
      "Epoch 4951, Loss: 0.7303888499736786, Final Batch Loss: 0.20326001942157745\n",
      "Epoch 4952, Loss: 0.7482990920543671, Final Batch Loss: 0.18318752944469452\n",
      "Epoch 4953, Loss: 0.8671234846115112, Final Batch Loss: 0.21129287779331207\n",
      "Epoch 4954, Loss: 0.819505475461483, Final Batch Loss: 0.11974029988050461\n",
      "Epoch 4955, Loss: 0.9536706209182739, Final Batch Loss: 0.25524070858955383\n",
      "Epoch 4956, Loss: 0.8312569409608841, Final Batch Loss: 0.20894339680671692\n",
      "Epoch 4957, Loss: 0.7795027047395706, Final Batch Loss: 0.18962261080741882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4958, Loss: 0.8268384486436844, Final Batch Loss: 0.17981883883476257\n",
      "Epoch 4959, Loss: 0.8608715236186981, Final Batch Loss: 0.24671043455600739\n",
      "Epoch 4960, Loss: 0.9382119625806808, Final Batch Loss: 0.24323511123657227\n",
      "Epoch 4961, Loss: 0.9405727535486221, Final Batch Loss: 0.2376844435930252\n",
      "Epoch 4962, Loss: 0.8153672367334366, Final Batch Loss: 0.25583088397979736\n",
      "Epoch 4963, Loss: 0.848686158657074, Final Batch Loss: 0.2379235029220581\n",
      "Epoch 4964, Loss: 0.8903863877058029, Final Batch Loss: 0.2194003164768219\n",
      "Epoch 4965, Loss: 0.8568451702594757, Final Batch Loss: 0.20377588272094727\n",
      "Epoch 4966, Loss: 0.906204491853714, Final Batch Loss: 0.193836510181427\n",
      "Epoch 4967, Loss: 0.8794622421264648, Final Batch Loss: 0.2210080921649933\n",
      "Epoch 4968, Loss: 0.8651906549930573, Final Batch Loss: 0.1731639802455902\n",
      "Epoch 4969, Loss: 0.835851326584816, Final Batch Loss: 0.2140902578830719\n",
      "Epoch 4970, Loss: 0.8253772705793381, Final Batch Loss: 0.22111985087394714\n",
      "Epoch 4971, Loss: 0.7811839729547501, Final Batch Loss: 0.23047278821468353\n",
      "Epoch 4972, Loss: 0.8479544818401337, Final Batch Loss: 0.2026972472667694\n",
      "Epoch 4973, Loss: 0.7865244820713997, Final Batch Loss: 0.11466997116804123\n",
      "Epoch 4974, Loss: 0.9277939647436142, Final Batch Loss: 0.33844879269599915\n",
      "Epoch 4975, Loss: 0.8723638355731964, Final Batch Loss: 0.14848341047763824\n",
      "Epoch 4976, Loss: 0.7270476818084717, Final Batch Loss: 0.1409667283296585\n",
      "Epoch 4977, Loss: 0.8450264930725098, Final Batch Loss: 0.2782702147960663\n",
      "Epoch 4978, Loss: 0.893965095281601, Final Batch Loss: 0.26078560948371887\n",
      "Epoch 4979, Loss: 0.7640535533428192, Final Batch Loss: 0.1800992786884308\n",
      "Epoch 4980, Loss: 0.7434837371110916, Final Batch Loss: 0.1753448247909546\n",
      "Epoch 4981, Loss: 0.8727937340736389, Final Batch Loss: 0.24268701672554016\n",
      "Epoch 4982, Loss: 0.8779016584157944, Final Batch Loss: 0.14680878818035126\n",
      "Epoch 4983, Loss: 0.7692725658416748, Final Batch Loss: 0.19518055021762848\n",
      "Epoch 4984, Loss: 0.884127214550972, Final Batch Loss: 0.236379936337471\n",
      "Epoch 4985, Loss: 0.8224721848964691, Final Batch Loss: 0.24373184144496918\n",
      "Epoch 4986, Loss: 0.8595975637435913, Final Batch Loss: 0.24447569251060486\n",
      "Epoch 4987, Loss: 0.750547856092453, Final Batch Loss: 0.18911485373973846\n",
      "Epoch 4988, Loss: 0.8468683958053589, Final Batch Loss: 0.20243722200393677\n",
      "Epoch 4989, Loss: 0.9750117212533951, Final Batch Loss: 0.29254525899887085\n",
      "Epoch 4990, Loss: 0.8689993023872375, Final Batch Loss: 0.2022048383951187\n",
      "Epoch 4991, Loss: 0.8479353934526443, Final Batch Loss: 0.25591418147087097\n",
      "Epoch 4992, Loss: 0.8547792881727219, Final Batch Loss: 0.24576541781425476\n",
      "Epoch 4993, Loss: 0.7711413204669952, Final Batch Loss: 0.2018052637577057\n",
      "Epoch 4994, Loss: 0.7846265584230423, Final Batch Loss: 0.24451734125614166\n",
      "Epoch 4995, Loss: 0.8643185496330261, Final Batch Loss: 0.23554126918315887\n",
      "Epoch 4996, Loss: 0.7382353991270065, Final Batch Loss: 0.20292019844055176\n",
      "Epoch 4997, Loss: 0.9435649514198303, Final Batch Loss: 0.3109740614891052\n",
      "Epoch 4998, Loss: 0.7731971442699432, Final Batch Loss: 0.1588601917028427\n",
      "Epoch 4999, Loss: 0.7905980199575424, Final Batch Loss: 0.1906944066286087\n",
      "Epoch 5000, Loss: 0.829350084066391, Final Batch Loss: 0.27694231271743774\n",
      "Epoch 5001, Loss: 0.9037082642316818, Final Batch Loss: 0.198379784822464\n",
      "Epoch 5002, Loss: 0.7726713120937347, Final Batch Loss: 0.15236350893974304\n",
      "Epoch 5003, Loss: 0.812997356057167, Final Batch Loss: 0.26329970359802246\n",
      "Epoch 5004, Loss: 0.8651316910982132, Final Batch Loss: 0.30248430371284485\n",
      "Epoch 5005, Loss: 0.9325532168149948, Final Batch Loss: 0.26367515325546265\n",
      "Epoch 5006, Loss: 0.737537682056427, Final Batch Loss: 0.17430241405963898\n",
      "Epoch 5007, Loss: 0.9031700044870377, Final Batch Loss: 0.22846128046512604\n",
      "Epoch 5008, Loss: 1.1007170677185059, Final Batch Loss: 0.21746212244033813\n",
      "Epoch 5009, Loss: 0.9301874041557312, Final Batch Loss: 0.3026522397994995\n",
      "Epoch 5010, Loss: 0.9590545892715454, Final Batch Loss: 0.21827559173107147\n",
      "Epoch 5011, Loss: 0.9533938467502594, Final Batch Loss: 0.2876756191253662\n",
      "Epoch 5012, Loss: 0.932540774345398, Final Batch Loss: 0.2620323598384857\n",
      "Epoch 5013, Loss: 0.7746667265892029, Final Batch Loss: 0.2100130021572113\n",
      "Epoch 5014, Loss: 0.8279213607311249, Final Batch Loss: 0.17836950719356537\n",
      "Epoch 5015, Loss: 0.8954097330570221, Final Batch Loss: 0.2106291949748993\n",
      "Epoch 5016, Loss: 0.8673891425132751, Final Batch Loss: 0.15067201852798462\n",
      "Epoch 5017, Loss: 0.8141966909170151, Final Batch Loss: 0.1586398184299469\n",
      "Epoch 5018, Loss: 0.7168525755405426, Final Batch Loss: 0.14338573813438416\n",
      "Epoch 5019, Loss: 0.8457482159137726, Final Batch Loss: 0.24787862598896027\n",
      "Epoch 5020, Loss: 0.7986300885677338, Final Batch Loss: 0.164866104722023\n",
      "Epoch 5021, Loss: 0.8100148141384125, Final Batch Loss: 0.24860447645187378\n",
      "Epoch 5022, Loss: 0.7976288050413132, Final Batch Loss: 0.2155994027853012\n",
      "Epoch 5023, Loss: 0.8471436500549316, Final Batch Loss: 0.22710996866226196\n",
      "Epoch 5024, Loss: 0.8761758804321289, Final Batch Loss: 0.21061161160469055\n",
      "Epoch 5025, Loss: 0.699159100651741, Final Batch Loss: 0.1539640575647354\n",
      "Epoch 5026, Loss: 0.8598715662956238, Final Batch Loss: 0.15963588654994965\n",
      "Epoch 5027, Loss: 0.8095275908708572, Final Batch Loss: 0.16775383055210114\n",
      "Epoch 5028, Loss: 0.9812321364879608, Final Batch Loss: 0.21892288327217102\n",
      "Epoch 5029, Loss: 0.9182342439889908, Final Batch Loss: 0.17044726014137268\n",
      "Epoch 5030, Loss: 0.7687532752752304, Final Batch Loss: 0.14276428520679474\n",
      "Epoch 5031, Loss: 0.8233789205551147, Final Batch Loss: 0.2389790415763855\n",
      "Epoch 5032, Loss: 0.8187088072299957, Final Batch Loss: 0.14784716069698334\n",
      "Epoch 5033, Loss: 0.8176681101322174, Final Batch Loss: 0.22107228636741638\n",
      "Epoch 5034, Loss: 0.8551841378211975, Final Batch Loss: 0.24453981220722198\n",
      "Epoch 5035, Loss: 0.7929294556379318, Final Batch Loss: 0.2137461006641388\n",
      "Epoch 5036, Loss: 0.7970259934663773, Final Batch Loss: 0.2331457883119583\n",
      "Epoch 5037, Loss: 0.8274806439876556, Final Batch Loss: 0.18378499150276184\n",
      "Epoch 5038, Loss: 0.8281743973493576, Final Batch Loss: 0.23826274275779724\n",
      "Epoch 5039, Loss: 0.795441061258316, Final Batch Loss: 0.16788379848003387\n",
      "Epoch 5040, Loss: 0.8034352362155914, Final Batch Loss: 0.220368891954422\n",
      "Epoch 5041, Loss: 0.8085009306669235, Final Batch Loss: 0.17318063974380493\n",
      "Epoch 5042, Loss: 0.7910497933626175, Final Batch Loss: 0.1982601433992386\n",
      "Epoch 5043, Loss: 0.834132730960846, Final Batch Loss: 0.23981565237045288\n",
      "Epoch 5044, Loss: 0.7998256087303162, Final Batch Loss: 0.19540935754776\n",
      "Epoch 5045, Loss: 0.7502371221780777, Final Batch Loss: 0.19791452586650848\n",
      "Epoch 5046, Loss: 0.7494711130857468, Final Batch Loss: 0.20505909621715546\n",
      "Epoch 5047, Loss: 0.7691588252782822, Final Batch Loss: 0.1428537517786026\n",
      "Epoch 5048, Loss: 0.8580208867788315, Final Batch Loss: 0.23259446024894714\n",
      "Epoch 5049, Loss: 0.8958398550748825, Final Batch Loss: 0.2584523856639862\n",
      "Epoch 5050, Loss: 0.8135362863540649, Final Batch Loss: 0.1754993051290512\n",
      "Epoch 5051, Loss: 0.7957903295755386, Final Batch Loss: 0.20889617502689362\n",
      "Epoch 5052, Loss: 0.7879926562309265, Final Batch Loss: 0.1957768201828003\n",
      "Epoch 5053, Loss: 0.9076161682605743, Final Batch Loss: 0.2645169496536255\n",
      "Epoch 5054, Loss: 0.9338179230690002, Final Batch Loss: 0.19525033235549927\n",
      "Epoch 5055, Loss: 0.8738576918840408, Final Batch Loss: 0.19156292080879211\n",
      "Epoch 5056, Loss: 0.8828660696744919, Final Batch Loss: 0.2164640724658966\n",
      "Epoch 5057, Loss: 0.9165112227201462, Final Batch Loss: 0.2633378505706787\n",
      "Epoch 5058, Loss: 0.7894440293312073, Final Batch Loss: 0.23489120602607727\n",
      "Epoch 5059, Loss: 0.900991901755333, Final Batch Loss: 0.24996688961982727\n",
      "Epoch 5060, Loss: 0.7328140735626221, Final Batch Loss: 0.117510125041008\n",
      "Epoch 5061, Loss: 0.7249184101819992, Final Batch Loss: 0.12994703650474548\n",
      "Epoch 5062, Loss: 0.7362060397863388, Final Batch Loss: 0.1690157651901245\n",
      "Epoch 5063, Loss: 0.6979826837778091, Final Batch Loss: 0.13036055862903595\n",
      "Epoch 5064, Loss: 0.7849849760532379, Final Batch Loss: 0.23304159939289093\n",
      "Epoch 5065, Loss: 0.9864749908447266, Final Batch Loss: 0.21845859289169312\n",
      "Epoch 5066, Loss: 0.8808530122041702, Final Batch Loss: 0.2320788949728012\n",
      "Epoch 5067, Loss: 0.7186446487903595, Final Batch Loss: 0.20842769742012024\n",
      "Epoch 5068, Loss: 0.7420456856489182, Final Batch Loss: 0.16181671619415283\n",
      "Epoch 5069, Loss: 0.804060161113739, Final Batch Loss: 0.18363019824028015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5070, Loss: 0.7996217161417007, Final Batch Loss: 0.2136680632829666\n",
      "Epoch 5071, Loss: 0.8318006098270416, Final Batch Loss: 0.22485409677028656\n",
      "Epoch 5072, Loss: 0.910696879029274, Final Batch Loss: 0.22477194666862488\n",
      "Epoch 5073, Loss: 0.7444138675928116, Final Batch Loss: 0.13013722002506256\n",
      "Epoch 5074, Loss: 0.8157916963100433, Final Batch Loss: 0.1829800009727478\n",
      "Epoch 5075, Loss: 0.7095617949962616, Final Batch Loss: 0.1454639583826065\n",
      "Epoch 5076, Loss: 0.8179192095994949, Final Batch Loss: 0.11248457431793213\n",
      "Epoch 5077, Loss: 0.9309690743684769, Final Batch Loss: 0.2638011574745178\n",
      "Epoch 5078, Loss: 0.9482909739017487, Final Batch Loss: 0.21869374811649323\n",
      "Epoch 5079, Loss: 0.7489886730909348, Final Batch Loss: 0.18740452826023102\n",
      "Epoch 5080, Loss: 0.8291506320238113, Final Batch Loss: 0.23756033182144165\n",
      "Epoch 5081, Loss: 0.8765829056501389, Final Batch Loss: 0.22525420784950256\n",
      "Epoch 5082, Loss: 0.6835259199142456, Final Batch Loss: 0.1742931455373764\n",
      "Epoch 5083, Loss: 0.8339710533618927, Final Batch Loss: 0.17979194223880768\n",
      "Epoch 5084, Loss: 0.915230855345726, Final Batch Loss: 0.2372812181711197\n",
      "Epoch 5085, Loss: 0.9126818031072617, Final Batch Loss: 0.22248385846614838\n",
      "Epoch 5086, Loss: 0.7542128562927246, Final Batch Loss: 0.23558375239372253\n",
      "Epoch 5087, Loss: 0.9082053899765015, Final Batch Loss: 0.2305927723646164\n",
      "Epoch 5088, Loss: 0.8662832826375961, Final Batch Loss: 0.21727155148983002\n",
      "Epoch 5089, Loss: 0.6908616572618484, Final Batch Loss: 0.1882447600364685\n",
      "Epoch 5090, Loss: 0.8526869416236877, Final Batch Loss: 0.21427437663078308\n",
      "Epoch 5091, Loss: 0.8368754833936691, Final Batch Loss: 0.15471704304218292\n",
      "Epoch 5092, Loss: 0.8430443406105042, Final Batch Loss: 0.24201321601867676\n",
      "Epoch 5093, Loss: 0.9012776762247086, Final Batch Loss: 0.25303587317466736\n",
      "Epoch 5094, Loss: 0.7813908606767654, Final Batch Loss: 0.18822583556175232\n",
      "Epoch 5095, Loss: 0.7646305561065674, Final Batch Loss: 0.20655997097492218\n",
      "Epoch 5096, Loss: 0.8963728547096252, Final Batch Loss: 0.300292432308197\n",
      "Epoch 5097, Loss: 0.9344081282615662, Final Batch Loss: 0.20221014320850372\n",
      "Epoch 5098, Loss: 0.8738654106855392, Final Batch Loss: 0.23149949312210083\n",
      "Epoch 5099, Loss: 0.8033529072999954, Final Batch Loss: 0.2248683124780655\n",
      "Epoch 5100, Loss: 0.8177794814109802, Final Batch Loss: 0.15338481962680817\n",
      "Epoch 5101, Loss: 0.8180453181266785, Final Batch Loss: 0.1721058040857315\n",
      "Epoch 5102, Loss: 0.8109869360923767, Final Batch Loss: 0.22373045980930328\n",
      "Epoch 5103, Loss: 0.7721340954303741, Final Batch Loss: 0.16510142385959625\n",
      "Epoch 5104, Loss: 0.7614384144544601, Final Batch Loss: 0.18866991996765137\n",
      "Epoch 5105, Loss: 0.8487705141305923, Final Batch Loss: 0.17347434163093567\n",
      "Epoch 5106, Loss: 0.7809765189886093, Final Batch Loss: 0.13422441482543945\n",
      "Epoch 5107, Loss: 0.9211504906415939, Final Batch Loss: 0.186982661485672\n",
      "Epoch 5108, Loss: 0.8939650058746338, Final Batch Loss: 0.29181137681007385\n",
      "Epoch 5109, Loss: 0.7146048843860626, Final Batch Loss: 0.1683230698108673\n",
      "Epoch 5110, Loss: 0.8179094046354294, Final Batch Loss: 0.2347608357667923\n",
      "Epoch 5111, Loss: 0.7631570398807526, Final Batch Loss: 0.16492852568626404\n",
      "Epoch 5112, Loss: 0.8590197414159775, Final Batch Loss: 0.2856806516647339\n",
      "Epoch 5113, Loss: 0.8093391358852386, Final Batch Loss: 0.1748560518026352\n",
      "Epoch 5114, Loss: 0.7224253714084625, Final Batch Loss: 0.10376870632171631\n",
      "Epoch 5115, Loss: 0.7631435096263885, Final Batch Loss: 0.20678970217704773\n",
      "Epoch 5116, Loss: 0.9317831546068192, Final Batch Loss: 0.31803324818611145\n",
      "Epoch 5117, Loss: 0.7872031480073929, Final Batch Loss: 0.19709214568138123\n",
      "Epoch 5118, Loss: 0.8357593268156052, Final Batch Loss: 0.24032488465309143\n",
      "Epoch 5119, Loss: 0.7852827310562134, Final Batch Loss: 0.20616360008716583\n",
      "Epoch 5120, Loss: 0.8407779484987259, Final Batch Loss: 0.18703557550907135\n",
      "Epoch 5121, Loss: 0.9140042066574097, Final Batch Loss: 0.27810052037239075\n",
      "Epoch 5122, Loss: 0.7408418506383896, Final Batch Loss: 0.15435782074928284\n",
      "Epoch 5123, Loss: 0.8207047134637833, Final Batch Loss: 0.16844475269317627\n",
      "Epoch 5124, Loss: 0.8460423350334167, Final Batch Loss: 0.14898712933063507\n",
      "Epoch 5125, Loss: 0.7729500979185104, Final Batch Loss: 0.15826836228370667\n",
      "Epoch 5126, Loss: 0.9458423405885696, Final Batch Loss: 0.21639668941497803\n",
      "Epoch 5127, Loss: 0.9911547005176544, Final Batch Loss: 0.24295461177825928\n",
      "Epoch 5128, Loss: 0.7657939493656158, Final Batch Loss: 0.1967688947916031\n",
      "Epoch 5129, Loss: 0.8055106997489929, Final Batch Loss: 0.18005742132663727\n",
      "Epoch 5130, Loss: 0.8748358339071274, Final Batch Loss: 0.2182072401046753\n",
      "Epoch 5131, Loss: 1.0327497571706772, Final Batch Loss: 0.3026939630508423\n",
      "Epoch 5132, Loss: 0.9388439208269119, Final Batch Loss: 0.2707066535949707\n",
      "Epoch 5133, Loss: 0.9814681708812714, Final Batch Loss: 0.22783422470092773\n",
      "Epoch 5134, Loss: 1.1212655156850815, Final Batch Loss: 0.22641173005104065\n",
      "Epoch 5135, Loss: 0.7833161801099777, Final Batch Loss: 0.1524742841720581\n",
      "Epoch 5136, Loss: 1.025783747434616, Final Batch Loss: 0.2735138237476349\n",
      "Epoch 5137, Loss: 0.8424198627471924, Final Batch Loss: 0.2266252189874649\n",
      "Epoch 5138, Loss: 1.0222843140363693, Final Batch Loss: 0.2347022294998169\n",
      "Epoch 5139, Loss: 0.9666179418563843, Final Batch Loss: 0.18320150673389435\n",
      "Epoch 5140, Loss: 0.9340708702802658, Final Batch Loss: 0.19751828908920288\n",
      "Epoch 5141, Loss: 0.7828559130430222, Final Batch Loss: 0.20394331216812134\n",
      "Epoch 5142, Loss: 0.8208318799734116, Final Batch Loss: 0.2555451989173889\n",
      "Epoch 5143, Loss: 0.8354615271091461, Final Batch Loss: 0.2014688104391098\n",
      "Epoch 5144, Loss: 0.8388711810112, Final Batch Loss: 0.20000173151493073\n",
      "Epoch 5145, Loss: 0.8104983568191528, Final Batch Loss: 0.1371482014656067\n",
      "Epoch 5146, Loss: 0.8373850286006927, Final Batch Loss: 0.22191743552684784\n",
      "Epoch 5147, Loss: 0.7556417733430862, Final Batch Loss: 0.16182675957679749\n",
      "Epoch 5148, Loss: 0.9783210605382919, Final Batch Loss: 0.3052175045013428\n",
      "Epoch 5149, Loss: 0.8127552270889282, Final Batch Loss: 0.22560159862041473\n",
      "Epoch 5150, Loss: 0.8234993293881416, Final Batch Loss: 0.12357442826032639\n",
      "Epoch 5151, Loss: 0.8176495581865311, Final Batch Loss: 0.24378283321857452\n",
      "Epoch 5152, Loss: 0.865254357457161, Final Batch Loss: 0.22603845596313477\n",
      "Epoch 5153, Loss: 0.8107633292675018, Final Batch Loss: 0.16704942286014557\n",
      "Epoch 5154, Loss: 0.7814164310693741, Final Batch Loss: 0.17525257170200348\n",
      "Epoch 5155, Loss: 0.7947918474674225, Final Batch Loss: 0.1611899435520172\n",
      "Epoch 5156, Loss: 0.8652510046958923, Final Batch Loss: 0.19008634984493256\n",
      "Epoch 5157, Loss: 0.7810218632221222, Final Batch Loss: 0.23077422380447388\n",
      "Epoch 5158, Loss: 0.9409080892801285, Final Batch Loss: 0.2590253949165344\n",
      "Epoch 5159, Loss: 0.7897857576608658, Final Batch Loss: 0.1255021095275879\n",
      "Epoch 5160, Loss: 0.6890861243009567, Final Batch Loss: 0.1500457376241684\n",
      "Epoch 5161, Loss: 0.8229381442070007, Final Batch Loss: 0.2680463194847107\n",
      "Epoch 5162, Loss: 0.7904069125652313, Final Batch Loss: 0.24833981692790985\n",
      "Epoch 5163, Loss: 0.8619384616613388, Final Batch Loss: 0.25258880853652954\n",
      "Epoch 5164, Loss: 0.8893068134784698, Final Batch Loss: 0.26382407546043396\n",
      "Epoch 5165, Loss: 0.7638992071151733, Final Batch Loss: 0.22238193452358246\n",
      "Epoch 5166, Loss: 0.8310499638319016, Final Batch Loss: 0.18975454568862915\n",
      "Epoch 5167, Loss: 0.90849669277668, Final Batch Loss: 0.2550150156021118\n",
      "Epoch 5168, Loss: 0.9670346528291702, Final Batch Loss: 0.2738727629184723\n",
      "Epoch 5169, Loss: 0.8599154353141785, Final Batch Loss: 0.20639044046401978\n",
      "Epoch 5170, Loss: 0.8448716849088669, Final Batch Loss: 0.15290266275405884\n",
      "Epoch 5171, Loss: 0.7487529218196869, Final Batch Loss: 0.18486645817756653\n",
      "Epoch 5172, Loss: 0.8148538768291473, Final Batch Loss: 0.22551609575748444\n",
      "Epoch 5173, Loss: 0.7655653357505798, Final Batch Loss: 0.17750409245491028\n",
      "Epoch 5174, Loss: 0.8042714446783066, Final Batch Loss: 0.25144490599632263\n",
      "Epoch 5175, Loss: 0.7698976323008537, Final Batch Loss: 0.24409887194633484\n",
      "Epoch 5176, Loss: 0.8325153738260269, Final Batch Loss: 0.2536327540874481\n",
      "Epoch 5177, Loss: 0.7825452983379364, Final Batch Loss: 0.1869937628507614\n",
      "Epoch 5178, Loss: 0.7498149871826172, Final Batch Loss: 0.206303209066391\n",
      "Epoch 5179, Loss: 0.7588962018489838, Final Batch Loss: 0.21902579069137573\n",
      "Epoch 5180, Loss: 0.7832167446613312, Final Batch Loss: 0.16683799028396606\n",
      "Epoch 5181, Loss: 0.8832254409790039, Final Batch Loss: 0.21719196438789368\n",
      "Epoch 5182, Loss: 0.8952169418334961, Final Batch Loss: 0.2135978490114212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5183, Loss: 0.7749793380498886, Final Batch Loss: 0.1762784868478775\n",
      "Epoch 5184, Loss: 0.8639539629220963, Final Batch Loss: 0.1918138563632965\n",
      "Epoch 5185, Loss: 0.7676751762628555, Final Batch Loss: 0.1603713184595108\n",
      "Epoch 5186, Loss: 0.7883935421705246, Final Batch Loss: 0.2449112981557846\n",
      "Epoch 5187, Loss: 0.8388193994760513, Final Batch Loss: 0.21942736208438873\n",
      "Epoch 5188, Loss: 0.657247930765152, Final Batch Loss: 0.1712358444929123\n",
      "Epoch 5189, Loss: 0.9989075809717178, Final Batch Loss: 0.17771205306053162\n",
      "Epoch 5190, Loss: 0.7881015241146088, Final Batch Loss: 0.17595894634723663\n",
      "Epoch 5191, Loss: 0.8220755457878113, Final Batch Loss: 0.13492777943611145\n",
      "Epoch 5192, Loss: 0.8483020663261414, Final Batch Loss: 0.2360551804304123\n",
      "Epoch 5193, Loss: 0.7354573607444763, Final Batch Loss: 0.14993098378181458\n",
      "Epoch 5194, Loss: 0.8230285793542862, Final Batch Loss: 0.19586031138896942\n",
      "Epoch 5195, Loss: 0.790142372250557, Final Batch Loss: 0.22028782963752747\n",
      "Epoch 5196, Loss: 0.810384675860405, Final Batch Loss: 0.19843736290931702\n",
      "Epoch 5197, Loss: 0.7754568606615067, Final Batch Loss: 0.17862679064273834\n",
      "Epoch 5198, Loss: 0.818110391497612, Final Batch Loss: 0.17941120266914368\n",
      "Epoch 5199, Loss: 0.7726231068372726, Final Batch Loss: 0.1555727869272232\n",
      "Epoch 5200, Loss: 0.8871830105781555, Final Batch Loss: 0.22245927155017853\n",
      "Epoch 5201, Loss: 0.7999802231788635, Final Batch Loss: 0.17787612974643707\n",
      "Epoch 5202, Loss: 0.7090703248977661, Final Batch Loss: 0.14683692157268524\n",
      "Epoch 5203, Loss: 0.8008760660886765, Final Batch Loss: 0.22733667492866516\n",
      "Epoch 5204, Loss: 0.9020677655935287, Final Batch Loss: 0.18639777600765228\n",
      "Epoch 5205, Loss: 0.7895796746015549, Final Batch Loss: 0.19960713386535645\n",
      "Epoch 5206, Loss: 0.7776375412940979, Final Batch Loss: 0.16600294411182404\n",
      "Epoch 5207, Loss: 0.7689617723226547, Final Batch Loss: 0.15247440338134766\n",
      "Epoch 5208, Loss: 0.7038895487785339, Final Batch Loss: 0.14492124319076538\n",
      "Epoch 5209, Loss: 0.9132009148597717, Final Batch Loss: 0.21578629314899445\n",
      "Epoch 5210, Loss: 0.8951584994792938, Final Batch Loss: 0.1927524209022522\n",
      "Epoch 5211, Loss: 0.711046539247036, Final Batch Loss: 0.17108429968357086\n",
      "Epoch 5212, Loss: 0.7425488531589508, Final Batch Loss: 0.18390846252441406\n",
      "Epoch 5213, Loss: 0.7325350344181061, Final Batch Loss: 0.19699586927890778\n",
      "Epoch 5214, Loss: 0.8264537900686264, Final Batch Loss: 0.24938924610614777\n",
      "Epoch 5215, Loss: 0.7851668000221252, Final Batch Loss: 0.22221921384334564\n",
      "Epoch 5216, Loss: 0.8704569190740585, Final Batch Loss: 0.29476484656333923\n",
      "Epoch 5217, Loss: 0.7712623775005341, Final Batch Loss: 0.1672397255897522\n",
      "Epoch 5218, Loss: 0.9051494300365448, Final Batch Loss: 0.2224562019109726\n",
      "Epoch 5219, Loss: 0.7712569683790207, Final Batch Loss: 0.21900378167629242\n",
      "Epoch 5220, Loss: 0.8063449263572693, Final Batch Loss: 0.2119784653186798\n",
      "Epoch 5221, Loss: 0.8619296103715897, Final Batch Loss: 0.1973010003566742\n",
      "Epoch 5222, Loss: 0.7974767833948135, Final Batch Loss: 0.18693144619464874\n",
      "Epoch 5223, Loss: 0.8823905885219574, Final Batch Loss: 0.2332664579153061\n",
      "Epoch 5224, Loss: 0.8728641122579575, Final Batch Loss: 0.30027270317077637\n",
      "Epoch 5225, Loss: 0.7523951828479767, Final Batch Loss: 0.17856518924236298\n",
      "Epoch 5226, Loss: 0.8231335282325745, Final Batch Loss: 0.21958808600902557\n",
      "Epoch 5227, Loss: 0.8964691311120987, Final Batch Loss: 0.23114795982837677\n",
      "Epoch 5228, Loss: 0.7816174477338791, Final Batch Loss: 0.24183329939842224\n",
      "Epoch 5229, Loss: 0.9111850708723068, Final Batch Loss: 0.32425934076309204\n",
      "Epoch 5230, Loss: 0.7877131998538971, Final Batch Loss: 0.12271828949451447\n",
      "Epoch 5231, Loss: 0.8593899458646774, Final Batch Loss: 0.24064260721206665\n",
      "Epoch 5232, Loss: 0.6379786878824234, Final Batch Loss: 0.1570448875427246\n",
      "Epoch 5233, Loss: 0.6710647195577621, Final Batch Loss: 0.14950425922870636\n",
      "Epoch 5234, Loss: 0.788257047533989, Final Batch Loss: 0.26153436303138733\n",
      "Epoch 5235, Loss: 0.771700993180275, Final Batch Loss: 0.2802799642086029\n",
      "Epoch 5236, Loss: 0.7771319448947906, Final Batch Loss: 0.19893433153629303\n",
      "Epoch 5237, Loss: 0.7551102489233017, Final Batch Loss: 0.1852281242609024\n",
      "Epoch 5238, Loss: 0.7582238614559174, Final Batch Loss: 0.2386026680469513\n",
      "Epoch 5239, Loss: 0.8064068257808685, Final Batch Loss: 0.208730086684227\n",
      "Epoch 5240, Loss: 0.6985576301813126, Final Batch Loss: 0.12561720609664917\n",
      "Epoch 5241, Loss: 0.9403436183929443, Final Batch Loss: 0.27541929483413696\n",
      "Epoch 5242, Loss: 0.8625794798135757, Final Batch Loss: 0.23312826454639435\n",
      "Epoch 5243, Loss: 0.8445094525814056, Final Batch Loss: 0.34756961464881897\n",
      "Epoch 5244, Loss: 0.8613817095756531, Final Batch Loss: 0.21736979484558105\n",
      "Epoch 5245, Loss: 0.8663600385189056, Final Batch Loss: 0.2134709358215332\n",
      "Epoch 5246, Loss: 0.7421810925006866, Final Batch Loss: 0.175583615899086\n",
      "Epoch 5247, Loss: 0.8968159556388855, Final Batch Loss: 0.24968859553337097\n",
      "Epoch 5248, Loss: 0.8621394634246826, Final Batch Loss: 0.21911872923374176\n",
      "Epoch 5249, Loss: 0.7395763844251633, Final Batch Loss: 0.20177453756332397\n",
      "Epoch 5250, Loss: 0.8163235783576965, Final Batch Loss: 0.2246895283460617\n",
      "Epoch 5251, Loss: 0.7527107894420624, Final Batch Loss: 0.20178624987602234\n",
      "Epoch 5252, Loss: 0.7735141664743423, Final Batch Loss: 0.15797162055969238\n",
      "Epoch 5253, Loss: 0.8259695619344711, Final Batch Loss: 0.19619911909103394\n",
      "Epoch 5254, Loss: 0.8118072599172592, Final Batch Loss: 0.2764088213443756\n",
      "Epoch 5255, Loss: 0.8012763410806656, Final Batch Loss: 0.17257779836654663\n",
      "Epoch 5256, Loss: 0.8820414990186691, Final Batch Loss: 0.17788170278072357\n",
      "Epoch 5257, Loss: 0.7689753025770187, Final Batch Loss: 0.19413717091083527\n",
      "Epoch 5258, Loss: 0.8275667279958725, Final Batch Loss: 0.20834222435951233\n",
      "Epoch 5259, Loss: 0.7823171764612198, Final Batch Loss: 0.18315868079662323\n",
      "Epoch 5260, Loss: 0.8297202438116074, Final Batch Loss: 0.24826647341251373\n",
      "Epoch 5261, Loss: 0.7458966374397278, Final Batch Loss: 0.22366367280483246\n",
      "Epoch 5262, Loss: 0.7555898278951645, Final Batch Loss: 0.1883699744939804\n",
      "Epoch 5263, Loss: 0.7401153594255447, Final Batch Loss: 0.15436066687107086\n",
      "Epoch 5264, Loss: 0.7588256746530533, Final Batch Loss: 0.2333459109067917\n",
      "Epoch 5265, Loss: 0.8313258290290833, Final Batch Loss: 0.1905219554901123\n",
      "Epoch 5266, Loss: 0.7317674607038498, Final Batch Loss: 0.15716035664081573\n",
      "Epoch 5267, Loss: 0.8226816803216934, Final Batch Loss: 0.19150464236736298\n",
      "Epoch 5268, Loss: 0.7085316926240921, Final Batch Loss: 0.1685810536146164\n",
      "Epoch 5269, Loss: 0.7696589380502701, Final Batch Loss: 0.1710532009601593\n",
      "Epoch 5270, Loss: 0.7247495502233505, Final Batch Loss: 0.13421903550624847\n",
      "Epoch 5271, Loss: 0.853848397731781, Final Batch Loss: 0.18482573330402374\n",
      "Epoch 5272, Loss: 0.7875127494335175, Final Batch Loss: 0.23960934579372406\n",
      "Epoch 5273, Loss: 0.7668261080980301, Final Batch Loss: 0.13656865060329437\n",
      "Epoch 5274, Loss: 0.8759024292230606, Final Batch Loss: 0.24414171278476715\n",
      "Epoch 5275, Loss: 0.9038769602775574, Final Batch Loss: 0.26994094252586365\n",
      "Epoch 5276, Loss: 0.7629137486219406, Final Batch Loss: 0.13589920103549957\n",
      "Epoch 5277, Loss: 0.8332600444555283, Final Batch Loss: 0.24328438937664032\n",
      "Epoch 5278, Loss: 0.8432017266750336, Final Batch Loss: 0.23407791554927826\n",
      "Epoch 5279, Loss: 0.9647349566221237, Final Batch Loss: 0.2544098496437073\n",
      "Epoch 5280, Loss: 0.7127508372068405, Final Batch Loss: 0.1828128844499588\n",
      "Epoch 5281, Loss: 0.8431304842233658, Final Batch Loss: 0.23631970584392548\n",
      "Epoch 5282, Loss: 0.7299450114369392, Final Batch Loss: 0.20364953577518463\n",
      "Epoch 5283, Loss: 0.7739729881286621, Final Batch Loss: 0.211094468832016\n",
      "Epoch 5284, Loss: 0.9198433756828308, Final Batch Loss: 0.25950929522514343\n",
      "Epoch 5285, Loss: 0.9396482110023499, Final Batch Loss: 0.24805869162082672\n",
      "Epoch 5286, Loss: 0.7584604024887085, Final Batch Loss: 0.20704331994056702\n",
      "Epoch 5287, Loss: 0.8837702423334122, Final Batch Loss: 0.2242216318845749\n",
      "Epoch 5288, Loss: 0.722217932343483, Final Batch Loss: 0.23066793382167816\n",
      "Epoch 5289, Loss: 0.9198850840330124, Final Batch Loss: 0.2373342216014862\n",
      "Epoch 5290, Loss: 0.6702946126461029, Final Batch Loss: 0.14032669365406036\n",
      "Epoch 5291, Loss: 0.7604701966047287, Final Batch Loss: 0.20790106058120728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5292, Loss: 0.7317007333040237, Final Batch Loss: 0.2059580683708191\n",
      "Epoch 5293, Loss: 0.7231395542621613, Final Batch Loss: 0.1679196059703827\n",
      "Epoch 5294, Loss: 0.853626012802124, Final Batch Loss: 0.23124219477176666\n",
      "Epoch 5295, Loss: 0.8612904846668243, Final Batch Loss: 0.22169923782348633\n",
      "Epoch 5296, Loss: 0.7490331381559372, Final Batch Loss: 0.19792070984840393\n",
      "Epoch 5297, Loss: 0.8619091659784317, Final Batch Loss: 0.23279425501823425\n",
      "Epoch 5298, Loss: 1.0093377232551575, Final Batch Loss: 0.2746494710445404\n",
      "Epoch 5299, Loss: 0.9404179602861404, Final Batch Loss: 0.18796256184577942\n",
      "Epoch 5300, Loss: 0.8660625666379929, Final Batch Loss: 0.23796729743480682\n",
      "Epoch 5301, Loss: 0.8949323892593384, Final Batch Loss: 0.2614405155181885\n",
      "Epoch 5302, Loss: 0.8345208913087845, Final Batch Loss: 0.15692076086997986\n",
      "Epoch 5303, Loss: 0.7572796791791916, Final Batch Loss: 0.20729143917560577\n",
      "Epoch 5304, Loss: 0.7696889787912369, Final Batch Loss: 0.20307520031929016\n",
      "Epoch 5305, Loss: 0.6516135632991791, Final Batch Loss: 0.15898896753787994\n",
      "Epoch 5306, Loss: 0.7230695933103561, Final Batch Loss: 0.2589915096759796\n",
      "Epoch 5307, Loss: 0.7310570776462555, Final Batch Loss: 0.17136256396770477\n",
      "Epoch 5308, Loss: 0.7839629501104355, Final Batch Loss: 0.21819062530994415\n",
      "Epoch 5309, Loss: 0.7454170137643814, Final Batch Loss: 0.17434900999069214\n",
      "Epoch 5310, Loss: 0.8117694854736328, Final Batch Loss: 0.1981363445520401\n",
      "Epoch 5311, Loss: 0.7101643830537796, Final Batch Loss: 0.19065862894058228\n",
      "Epoch 5312, Loss: 0.7937246859073639, Final Batch Loss: 0.2620050311088562\n",
      "Epoch 5313, Loss: 0.72887022793293, Final Batch Loss: 0.216319277882576\n",
      "Epoch 5314, Loss: 0.9453181624412537, Final Batch Loss: 0.17978313565254211\n",
      "Epoch 5315, Loss: 0.9003683626651764, Final Batch Loss: 0.19853276014328003\n",
      "Epoch 5316, Loss: 0.7082972526550293, Final Batch Loss: 0.16815325617790222\n",
      "Epoch 5317, Loss: 0.8587338328361511, Final Batch Loss: 0.20726874470710754\n",
      "Epoch 5318, Loss: 0.8395414352416992, Final Batch Loss: 0.21324534714221954\n",
      "Epoch 5319, Loss: 0.8178158104419708, Final Batch Loss: 0.22119753062725067\n",
      "Epoch 5320, Loss: 0.8367122709751129, Final Batch Loss: 0.1613258570432663\n",
      "Epoch 5321, Loss: 0.7428393661975861, Final Batch Loss: 0.12990571558475494\n",
      "Epoch 5322, Loss: 0.7126027047634125, Final Batch Loss: 0.16037128865718842\n",
      "Epoch 5323, Loss: 0.787107303738594, Final Batch Loss: 0.16153506934642792\n",
      "Epoch 5324, Loss: 0.8048936724662781, Final Batch Loss: 0.22006529569625854\n",
      "Epoch 5325, Loss: 0.775559201836586, Final Batch Loss: 0.21289552748203278\n",
      "Epoch 5326, Loss: 0.8232500553131104, Final Batch Loss: 0.23276369273662567\n",
      "Epoch 5327, Loss: 0.7255416959524155, Final Batch Loss: 0.15438547730445862\n",
      "Epoch 5328, Loss: 0.8234909325838089, Final Batch Loss: 0.1912160962820053\n",
      "Epoch 5329, Loss: 0.9095394909381866, Final Batch Loss: 0.21847090125083923\n",
      "Epoch 5330, Loss: 0.8077032417058945, Final Batch Loss: 0.19752952456474304\n",
      "Epoch 5331, Loss: 0.8502621650695801, Final Batch Loss: 0.12594440579414368\n",
      "Epoch 5332, Loss: 0.7820921987295151, Final Batch Loss: 0.1614174097776413\n",
      "Epoch 5333, Loss: 0.766437292098999, Final Batch Loss: 0.17164088785648346\n",
      "Epoch 5334, Loss: 0.7878309935331345, Final Batch Loss: 0.24164092540740967\n",
      "Epoch 5335, Loss: 0.723631963133812, Final Batch Loss: 0.15223772823810577\n",
      "Epoch 5336, Loss: 0.8488055169582367, Final Batch Loss: 0.17149263620376587\n",
      "Epoch 5337, Loss: 0.7785881608724594, Final Batch Loss: 0.21496418118476868\n",
      "Epoch 5338, Loss: 0.7790619432926178, Final Batch Loss: 0.13118267059326172\n",
      "Epoch 5339, Loss: 0.8244450688362122, Final Batch Loss: 0.237545907497406\n",
      "Epoch 5340, Loss: 0.8364632725715637, Final Batch Loss: 0.1672084778547287\n",
      "Epoch 5341, Loss: 0.7813547104597092, Final Batch Loss: 0.12635236978530884\n",
      "Epoch 5342, Loss: 0.8239725232124329, Final Batch Loss: 0.2062353640794754\n",
      "Epoch 5343, Loss: 0.9082826673984528, Final Batch Loss: 0.17698462307453156\n",
      "Epoch 5344, Loss: 0.8139570355415344, Final Batch Loss: 0.1883842796087265\n",
      "Epoch 5345, Loss: 0.7010004222393036, Final Batch Loss: 0.22327060997486115\n",
      "Epoch 5346, Loss: 0.8479612171649933, Final Batch Loss: 0.23816154897212982\n",
      "Epoch 5347, Loss: 0.8448906391859055, Final Batch Loss: 0.22241808474063873\n",
      "Epoch 5348, Loss: 0.8155405223369598, Final Batch Loss: 0.20206891000270844\n",
      "Epoch 5349, Loss: 0.9420996606349945, Final Batch Loss: 0.33245232701301575\n",
      "Epoch 5350, Loss: 0.9018640518188477, Final Batch Loss: 0.27198052406311035\n",
      "Epoch 5351, Loss: 0.7517658919095993, Final Batch Loss: 0.2688364088535309\n",
      "Epoch 5352, Loss: 0.8863745629787445, Final Batch Loss: 0.1472116857767105\n",
      "Epoch 5353, Loss: 0.8058674186468124, Final Batch Loss: 0.22358766198158264\n",
      "Epoch 5354, Loss: 0.8553739488124847, Final Batch Loss: 0.2382332682609558\n",
      "Epoch 5355, Loss: 0.7285643368959427, Final Batch Loss: 0.13445638120174408\n",
      "Epoch 5356, Loss: 0.7879360765218735, Final Batch Loss: 0.1750270575284958\n",
      "Epoch 5357, Loss: 0.6957484483718872, Final Batch Loss: 0.17604507505893707\n",
      "Epoch 5358, Loss: 0.8617196083068848, Final Batch Loss: 0.21802067756652832\n",
      "Epoch 5359, Loss: 0.7987394034862518, Final Batch Loss: 0.22591938078403473\n",
      "Epoch 5360, Loss: 0.8181075006723404, Final Batch Loss: 0.19385632872581482\n",
      "Epoch 5361, Loss: 0.9755029231309891, Final Batch Loss: 0.282111793756485\n",
      "Epoch 5362, Loss: 0.8284133225679398, Final Batch Loss: 0.19004490971565247\n",
      "Epoch 5363, Loss: 0.7743334621191025, Final Batch Loss: 0.21432742476463318\n",
      "Epoch 5364, Loss: 0.8082322478294373, Final Batch Loss: 0.22766056656837463\n",
      "Epoch 5365, Loss: 0.7281908541917801, Final Batch Loss: 0.16582296788692474\n",
      "Epoch 5366, Loss: 0.8508149832487106, Final Batch Loss: 0.20546592772006989\n",
      "Epoch 5367, Loss: 0.8286338895559311, Final Batch Loss: 0.19763684272766113\n",
      "Epoch 5368, Loss: 0.8096133917570114, Final Batch Loss: 0.1991642266511917\n",
      "Epoch 5369, Loss: 0.857207402586937, Final Batch Loss: 0.16515228152275085\n",
      "Epoch 5370, Loss: 0.7573710083961487, Final Batch Loss: 0.13170775771141052\n",
      "Epoch 5371, Loss: 0.9160454124212265, Final Batch Loss: 0.2589607536792755\n",
      "Epoch 5372, Loss: 0.8188696205615997, Final Batch Loss: 0.18962503969669342\n",
      "Epoch 5373, Loss: 0.7689576297998428, Final Batch Loss: 0.19419269263744354\n",
      "Epoch 5374, Loss: 0.8126037418842316, Final Batch Loss: 0.20552775263786316\n",
      "Epoch 5375, Loss: 0.6977740228176117, Final Batch Loss: 0.12122063338756561\n",
      "Epoch 5376, Loss: 0.8901108652353287, Final Batch Loss: 0.18707913160324097\n",
      "Epoch 5377, Loss: 0.7907721996307373, Final Batch Loss: 0.18397969007492065\n",
      "Epoch 5378, Loss: 0.8871501386165619, Final Batch Loss: 0.1975373923778534\n",
      "Epoch 5379, Loss: 0.8298798948526382, Final Batch Loss: 0.23026862740516663\n",
      "Epoch 5380, Loss: 0.7022173851728439, Final Batch Loss: 0.18800555169582367\n",
      "Epoch 5381, Loss: 0.7644423544406891, Final Batch Loss: 0.2228170484304428\n",
      "Epoch 5382, Loss: 0.8134688585996628, Final Batch Loss: 0.20194336771965027\n",
      "Epoch 5383, Loss: 0.7367605417966843, Final Batch Loss: 0.19057540595531464\n",
      "Epoch 5384, Loss: 0.7654536813497543, Final Batch Loss: 0.17530441284179688\n",
      "Epoch 5385, Loss: 0.895154133439064, Final Batch Loss: 0.13496343791484833\n",
      "Epoch 5386, Loss: 0.8091095983982086, Final Batch Loss: 0.28521203994750977\n",
      "Epoch 5387, Loss: 0.7860322147607803, Final Batch Loss: 0.1995147317647934\n",
      "Epoch 5388, Loss: 0.7114721089601517, Final Batch Loss: 0.17471374571323395\n",
      "Epoch 5389, Loss: 0.8897718340158463, Final Batch Loss: 0.21002814173698425\n",
      "Epoch 5390, Loss: 0.7826772630214691, Final Batch Loss: 0.21192674338817596\n",
      "Epoch 5391, Loss: 0.7757240384817123, Final Batch Loss: 0.14609737694263458\n",
      "Epoch 5392, Loss: 0.8675758987665176, Final Batch Loss: 0.1680544912815094\n",
      "Epoch 5393, Loss: 0.9503555148839951, Final Batch Loss: 0.24804043769836426\n",
      "Epoch 5394, Loss: 0.8187932372093201, Final Batch Loss: 0.21456307172775269\n",
      "Epoch 5395, Loss: 0.9014099538326263, Final Batch Loss: 0.2518123984336853\n",
      "Epoch 5396, Loss: 0.8430281430482864, Final Batch Loss: 0.18885710835456848\n",
      "Epoch 5397, Loss: 0.8006368428468704, Final Batch Loss: 0.19749748706817627\n",
      "Epoch 5398, Loss: 0.8610809743404388, Final Batch Loss: 0.19571970403194427\n",
      "Epoch 5399, Loss: 0.9390422403812408, Final Batch Loss: 0.36837130784988403\n",
      "Epoch 5400, Loss: 0.756517231464386, Final Batch Loss: 0.20308567583560944\n",
      "Epoch 5401, Loss: 0.8677362650632858, Final Batch Loss: 0.20868632197380066\n",
      "Epoch 5402, Loss: 0.8374742865562439, Final Batch Loss: 0.16971802711486816\n",
      "Epoch 5403, Loss: 0.8333512395620346, Final Batch Loss: 0.19257767498493195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5404, Loss: 0.8530462682247162, Final Batch Loss: 0.26263177394866943\n",
      "Epoch 5405, Loss: 0.7167079597711563, Final Batch Loss: 0.18217575550079346\n",
      "Epoch 5406, Loss: 0.8108631819486618, Final Batch Loss: 0.23998208343982697\n",
      "Epoch 5407, Loss: 0.7364965677261353, Final Batch Loss: 0.1535022109746933\n",
      "Epoch 5408, Loss: 0.7816133201122284, Final Batch Loss: 0.16950199007987976\n",
      "Epoch 5409, Loss: 0.8800368010997772, Final Batch Loss: 0.22584204375743866\n",
      "Epoch 5410, Loss: 0.8690058290958405, Final Batch Loss: 0.3276577889919281\n",
      "Epoch 5411, Loss: 0.8005881309509277, Final Batch Loss: 0.20894668996334076\n",
      "Epoch 5412, Loss: 0.759582981467247, Final Batch Loss: 0.2340788096189499\n",
      "Epoch 5413, Loss: 0.8614351898431778, Final Batch Loss: 0.24308378994464874\n",
      "Epoch 5414, Loss: 0.7217150777578354, Final Batch Loss: 0.1457211971282959\n",
      "Epoch 5415, Loss: 0.7502105534076691, Final Batch Loss: 0.11259263753890991\n",
      "Epoch 5416, Loss: 0.7553906738758087, Final Batch Loss: 0.21862103044986725\n",
      "Epoch 5417, Loss: 0.6656289100646973, Final Batch Loss: 0.12775547802448273\n",
      "Epoch 5418, Loss: 0.8666665256023407, Final Batch Loss: 0.22058188915252686\n",
      "Epoch 5419, Loss: 0.8743882775306702, Final Batch Loss: 0.19998447597026825\n",
      "Epoch 5420, Loss: 0.8364408910274506, Final Batch Loss: 0.23533236980438232\n",
      "Epoch 5421, Loss: 0.7887243628501892, Final Batch Loss: 0.17897440493106842\n",
      "Epoch 5422, Loss: 0.992707148194313, Final Batch Loss: 0.2786620259284973\n",
      "Epoch 5423, Loss: 0.9251687973737717, Final Batch Loss: 0.2979796826839447\n",
      "Epoch 5424, Loss: 0.8974109292030334, Final Batch Loss: 0.22324952483177185\n",
      "Epoch 5425, Loss: 0.8290814459323883, Final Batch Loss: 0.1852358728647232\n",
      "Epoch 5426, Loss: 0.8448689579963684, Final Batch Loss: 0.18683995306491852\n",
      "Epoch 5427, Loss: 0.8403530418872833, Final Batch Loss: 0.23070527613162994\n",
      "Epoch 5428, Loss: 0.7646028101444244, Final Batch Loss: 0.14978350698947906\n",
      "Epoch 5429, Loss: 0.7049332857131958, Final Batch Loss: 0.13547484576702118\n",
      "Epoch 5430, Loss: 0.8095511049032211, Final Batch Loss: 0.20831400156021118\n",
      "Epoch 5431, Loss: 0.8922756910324097, Final Batch Loss: 0.21163925528526306\n",
      "Epoch 5432, Loss: 0.8107097893953323, Final Batch Loss: 0.18087491393089294\n",
      "Epoch 5433, Loss: 0.8285944014787674, Final Batch Loss: 0.1982315629720688\n",
      "Epoch 5434, Loss: 0.7572244852781296, Final Batch Loss: 0.21936921775341034\n",
      "Epoch 5435, Loss: 0.7405920028686523, Final Batch Loss: 0.18168842792510986\n",
      "Epoch 5436, Loss: 0.6138119399547577, Final Batch Loss: 0.1200554370880127\n",
      "Epoch 5437, Loss: 0.7294903248548508, Final Batch Loss: 0.13604095578193665\n",
      "Epoch 5438, Loss: 0.7710734009742737, Final Batch Loss: 0.19800521433353424\n",
      "Epoch 5439, Loss: 0.6735757440328598, Final Batch Loss: 0.17155607044696808\n",
      "Epoch 5440, Loss: 0.755767285823822, Final Batch Loss: 0.20969678461551666\n",
      "Epoch 5441, Loss: 0.8077190741896629, Final Batch Loss: 0.265202134847641\n",
      "Epoch 5442, Loss: 0.8303238153457642, Final Batch Loss: 0.17819136381149292\n",
      "Epoch 5443, Loss: 0.951612114906311, Final Batch Loss: 0.22251097857952118\n",
      "Epoch 5444, Loss: 0.825421079993248, Final Batch Loss: 0.23140613734722137\n",
      "Epoch 5445, Loss: 0.8827247619628906, Final Batch Loss: 0.3101421594619751\n",
      "Epoch 5446, Loss: 0.9145304262638092, Final Batch Loss: 0.27436310052871704\n",
      "Epoch 5447, Loss: 0.8967460095882416, Final Batch Loss: 0.21794521808624268\n",
      "Epoch 5448, Loss: 0.8819464147090912, Final Batch Loss: 0.23398178815841675\n",
      "Epoch 5449, Loss: 1.0123366117477417, Final Batch Loss: 0.3583029508590698\n",
      "Epoch 5450, Loss: 0.8941188156604767, Final Batch Loss: 0.18204143643379211\n",
      "Epoch 5451, Loss: 0.8446652144193649, Final Batch Loss: 0.26586267352104187\n",
      "Epoch 5452, Loss: 0.7455879896879196, Final Batch Loss: 0.19097837805747986\n",
      "Epoch 5453, Loss: 0.8448788821697235, Final Batch Loss: 0.21927978098392487\n",
      "Epoch 5454, Loss: 0.8117246329784393, Final Batch Loss: 0.14061707258224487\n",
      "Epoch 5455, Loss: 0.7657976448535919, Final Batch Loss: 0.2165907770395279\n",
      "Epoch 5456, Loss: 0.7863297909498215, Final Batch Loss: 0.2027265578508377\n",
      "Epoch 5457, Loss: 0.7445855438709259, Final Batch Loss: 0.22409416735172272\n",
      "Epoch 5458, Loss: 0.7948039323091507, Final Batch Loss: 0.18724560737609863\n",
      "Epoch 5459, Loss: 0.7671102434396744, Final Batch Loss: 0.24104253947734833\n",
      "Epoch 5460, Loss: 0.8110058903694153, Final Batch Loss: 0.18810047209262848\n",
      "Epoch 5461, Loss: 0.7458231449127197, Final Batch Loss: 0.18260419368743896\n",
      "Epoch 5462, Loss: 0.7771366238594055, Final Batch Loss: 0.19643272459506989\n",
      "Epoch 5463, Loss: 0.8206613659858704, Final Batch Loss: 0.18087077140808105\n",
      "Epoch 5464, Loss: 0.7771853357553482, Final Batch Loss: 0.22191481292247772\n",
      "Epoch 5465, Loss: 0.7811529189348221, Final Batch Loss: 0.24069446325302124\n",
      "Epoch 5466, Loss: 0.7793481051921844, Final Batch Loss: 0.15460775792598724\n",
      "Epoch 5467, Loss: 0.8369895368814468, Final Batch Loss: 0.3124135732650757\n",
      "Epoch 5468, Loss: 0.7172660976648331, Final Batch Loss: 0.2086244374513626\n",
      "Epoch 5469, Loss: 0.7617964148521423, Final Batch Loss: 0.19236069917678833\n",
      "Epoch 5470, Loss: 0.8578009903430939, Final Batch Loss: 0.210901141166687\n",
      "Epoch 5471, Loss: 0.7459533214569092, Final Batch Loss: 0.17986203730106354\n",
      "Epoch 5472, Loss: 0.8189267069101334, Final Batch Loss: 0.2436312437057495\n",
      "Epoch 5473, Loss: 0.8284754008054733, Final Batch Loss: 0.23773205280303955\n",
      "Epoch 5474, Loss: 0.7458939552307129, Final Batch Loss: 0.18466316163539886\n",
      "Epoch 5475, Loss: 0.7399362623691559, Final Batch Loss: 0.13748884201049805\n",
      "Epoch 5476, Loss: 0.8924695253372192, Final Batch Loss: 0.23835913836956024\n",
      "Epoch 5477, Loss: 0.8604618310928345, Final Batch Loss: 0.17443284392356873\n",
      "Epoch 5478, Loss: 0.8962076604366302, Final Batch Loss: 0.21575747430324554\n",
      "Epoch 5479, Loss: 0.7238898277282715, Final Batch Loss: 0.14963781833648682\n",
      "Epoch 5480, Loss: 0.821652963757515, Final Batch Loss: 0.15120971202850342\n",
      "Epoch 5481, Loss: 0.805657148361206, Final Batch Loss: 0.17286577820777893\n",
      "Epoch 5482, Loss: 0.8414959758520126, Final Batch Loss: 0.2014337033033371\n",
      "Epoch 5483, Loss: 0.7323032170534134, Final Batch Loss: 0.1427912712097168\n",
      "Epoch 5484, Loss: 0.820742204785347, Final Batch Loss: 0.17143015563488007\n",
      "Epoch 5485, Loss: 0.8266348540782928, Final Batch Loss: 0.2408149391412735\n",
      "Epoch 5486, Loss: 0.8071433454751968, Final Batch Loss: 0.22065022587776184\n",
      "Epoch 5487, Loss: 0.8433095514774323, Final Batch Loss: 0.23097580671310425\n",
      "Epoch 5488, Loss: 0.9136555939912796, Final Batch Loss: 0.2777690589427948\n",
      "Epoch 5489, Loss: 0.7523556500673294, Final Batch Loss: 0.1794968843460083\n",
      "Epoch 5490, Loss: 0.7076801061630249, Final Batch Loss: 0.18006916344165802\n",
      "Epoch 5491, Loss: 0.9799561202526093, Final Batch Loss: 0.24489273130893707\n",
      "Epoch 5492, Loss: 0.855838418006897, Final Batch Loss: 0.22539843618869781\n",
      "Epoch 5493, Loss: 0.7332194745540619, Final Batch Loss: 0.20051820576190948\n",
      "Epoch 5494, Loss: 0.7796498686075211, Final Batch Loss: 0.1777006983757019\n",
      "Epoch 5495, Loss: 0.8849924653768539, Final Batch Loss: 0.24184241890907288\n",
      "Epoch 5496, Loss: 0.7253096401691437, Final Batch Loss: 0.17459729313850403\n",
      "Epoch 5497, Loss: 0.8821138739585876, Final Batch Loss: 0.19198180735111237\n",
      "Epoch 5498, Loss: 0.7178133577108383, Final Batch Loss: 0.18948334455490112\n",
      "Epoch 5499, Loss: 0.7668009996414185, Final Batch Loss: 0.279638409614563\n",
      "Epoch 5500, Loss: 0.7522585690021515, Final Batch Loss: 0.12534315884113312\n",
      "Epoch 5501, Loss: 0.7508706599473953, Final Batch Loss: 0.19577902555465698\n",
      "Epoch 5502, Loss: 0.791320875287056, Final Batch Loss: 0.1978987753391266\n",
      "Epoch 5503, Loss: 0.765778198838234, Final Batch Loss: 0.2033073455095291\n",
      "Epoch 5504, Loss: 0.7660783231258392, Final Batch Loss: 0.19951453804969788\n",
      "Epoch 5505, Loss: 0.8142277151346207, Final Batch Loss: 0.14624956250190735\n",
      "Epoch 5506, Loss: 0.7125002145767212, Final Batch Loss: 0.13587242364883423\n",
      "Epoch 5507, Loss: 0.7736251503229141, Final Batch Loss: 0.19942376017570496\n",
      "Epoch 5508, Loss: 0.8323948681354523, Final Batch Loss: 0.256183385848999\n",
      "Epoch 5509, Loss: 0.8197660446166992, Final Batch Loss: 0.15955202281475067\n",
      "Epoch 5510, Loss: 0.8462198078632355, Final Batch Loss: 0.16684509813785553\n",
      "Epoch 5511, Loss: 0.9101685136556625, Final Batch Loss: 0.21760451793670654\n",
      "Epoch 5512, Loss: 0.7341622412204742, Final Batch Loss: 0.24600864946842194\n",
      "Epoch 5513, Loss: 0.9475881457328796, Final Batch Loss: 0.25899702310562134\n",
      "Epoch 5514, Loss: 0.8899186700582504, Final Batch Loss: 0.26790621876716614\n",
      "Epoch 5515, Loss: 0.8068938702344894, Final Batch Loss: 0.17847123742103577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5516, Loss: 0.8514508306980133, Final Batch Loss: 0.24243927001953125\n",
      "Epoch 5517, Loss: 0.7852789461612701, Final Batch Loss: 0.2024417668581009\n",
      "Epoch 5518, Loss: 0.8733412474393845, Final Batch Loss: 0.2082357555627823\n",
      "Epoch 5519, Loss: 0.8049609363079071, Final Batch Loss: 0.1863952875137329\n",
      "Epoch 5520, Loss: 0.7405677139759064, Final Batch Loss: 0.1773836314678192\n",
      "Epoch 5521, Loss: 0.9863751530647278, Final Batch Loss: 0.22902052104473114\n",
      "Epoch 5522, Loss: 0.765966460108757, Final Batch Loss: 0.1249733418226242\n",
      "Epoch 5523, Loss: 0.8072374612092972, Final Batch Loss: 0.2591096758842468\n",
      "Epoch 5524, Loss: 0.8088139295578003, Final Batch Loss: 0.17287231981754303\n",
      "Epoch 5525, Loss: 0.8200636059045792, Final Batch Loss: 0.21362221240997314\n",
      "Epoch 5526, Loss: 0.8602785170078278, Final Batch Loss: 0.23134784400463104\n",
      "Epoch 5527, Loss: 0.7774783968925476, Final Batch Loss: 0.17598125338554382\n",
      "Epoch 5528, Loss: 0.9736315608024597, Final Batch Loss: 0.2831561267375946\n",
      "Epoch 5529, Loss: 0.7975620031356812, Final Batch Loss: 0.20999227464199066\n",
      "Epoch 5530, Loss: 0.7508151829242706, Final Batch Loss: 0.18220140039920807\n",
      "Epoch 5531, Loss: 0.7427905350923538, Final Batch Loss: 0.213791161775589\n",
      "Epoch 5532, Loss: 0.8418260514736176, Final Batch Loss: 0.20631206035614014\n",
      "Epoch 5533, Loss: 0.7974640727043152, Final Batch Loss: 0.184835284948349\n",
      "Epoch 5534, Loss: 0.7949273586273193, Final Batch Loss: 0.2028319388628006\n",
      "Epoch 5535, Loss: 0.8887236416339874, Final Batch Loss: 0.22481238842010498\n",
      "Epoch 5536, Loss: 1.002351999282837, Final Batch Loss: 0.20241563022136688\n",
      "Epoch 5537, Loss: 0.830214262008667, Final Batch Loss: 0.19339513778686523\n",
      "Epoch 5538, Loss: 0.8283339589834213, Final Batch Loss: 0.15845851600170135\n",
      "Epoch 5539, Loss: 0.6897444278001785, Final Batch Loss: 0.22431880235671997\n",
      "Epoch 5540, Loss: 0.6698343008756638, Final Batch Loss: 0.22067317366600037\n",
      "Epoch 5541, Loss: 0.8518031686544418, Final Batch Loss: 0.23225867748260498\n",
      "Epoch 5542, Loss: 0.6893048733472824, Final Batch Loss: 0.2012379765510559\n",
      "Epoch 5543, Loss: 0.7678767740726471, Final Batch Loss: 0.2360793799161911\n",
      "Epoch 5544, Loss: 0.7989127933979034, Final Batch Loss: 0.1729356050491333\n",
      "Epoch 5545, Loss: 0.8212565928697586, Final Batch Loss: 0.28204429149627686\n",
      "Epoch 5546, Loss: 0.7665387690067291, Final Batch Loss: 0.21090318262577057\n",
      "Epoch 5547, Loss: 0.9377681761980057, Final Batch Loss: 0.23689059913158417\n",
      "Epoch 5548, Loss: 0.8758850395679474, Final Batch Loss: 0.22772887349128723\n",
      "Epoch 5549, Loss: 0.7122627794742584, Final Batch Loss: 0.14856940507888794\n",
      "Epoch 5550, Loss: 0.7801081091165543, Final Batch Loss: 0.16937346756458282\n",
      "Epoch 5551, Loss: 0.8471942394971848, Final Batch Loss: 0.2025832235813141\n",
      "Epoch 5552, Loss: 0.8597215861082077, Final Batch Loss: 0.17132239043712616\n",
      "Epoch 5553, Loss: 0.7286651879549026, Final Batch Loss: 0.12322878837585449\n",
      "Epoch 5554, Loss: 0.8105145394802094, Final Batch Loss: 0.15260034799575806\n",
      "Epoch 5555, Loss: 0.7951803058385849, Final Batch Loss: 0.1807658076286316\n",
      "Epoch 5556, Loss: 0.6947647333145142, Final Batch Loss: 0.14819149672985077\n",
      "Epoch 5557, Loss: 0.8334250301122665, Final Batch Loss: 0.19675663113594055\n",
      "Epoch 5558, Loss: 0.7552712112665176, Final Batch Loss: 0.1873612403869629\n",
      "Epoch 5559, Loss: 0.7887594401836395, Final Batch Loss: 0.2293168157339096\n",
      "Epoch 5560, Loss: 0.7683757394552231, Final Batch Loss: 0.2176934778690338\n",
      "Epoch 5561, Loss: 0.8482457399368286, Final Batch Loss: 0.23039855062961578\n",
      "Epoch 5562, Loss: 0.8560101240873337, Final Batch Loss: 0.268070250749588\n",
      "Epoch 5563, Loss: 0.9270949214696884, Final Batch Loss: 0.21856999397277832\n",
      "Epoch 5564, Loss: 0.7302863150835037, Final Batch Loss: 0.14566953480243683\n",
      "Epoch 5565, Loss: 0.7626697272062302, Final Batch Loss: 0.18026398122310638\n",
      "Epoch 5566, Loss: 0.7394289374351501, Final Batch Loss: 0.2153228372335434\n",
      "Epoch 5567, Loss: 0.7575360834598541, Final Batch Loss: 0.1535886824131012\n",
      "Epoch 5568, Loss: 0.9444591552019119, Final Batch Loss: 0.26473772525787354\n",
      "Epoch 5569, Loss: 0.7932947799563408, Final Batch Loss: 0.2341436892747879\n",
      "Epoch 5570, Loss: 0.7899810671806335, Final Batch Loss: 0.22567057609558105\n",
      "Epoch 5571, Loss: 0.7703172862529755, Final Batch Loss: 0.16167546808719635\n",
      "Epoch 5572, Loss: 0.7964176833629608, Final Batch Loss: 0.2393554449081421\n",
      "Epoch 5573, Loss: 0.7633317410945892, Final Batch Loss: 0.1872224062681198\n",
      "Epoch 5574, Loss: 0.7127111256122589, Final Batch Loss: 0.23722097277641296\n",
      "Epoch 5575, Loss: 0.8778571337461472, Final Batch Loss: 0.2758384346961975\n",
      "Epoch 5576, Loss: 0.7134995311498642, Final Batch Loss: 0.18899737298488617\n",
      "Epoch 5577, Loss: 0.7135763019323349, Final Batch Loss: 0.1819770336151123\n",
      "Epoch 5578, Loss: 0.8591931313276291, Final Batch Loss: 0.1704738885164261\n",
      "Epoch 5579, Loss: 0.8108731210231781, Final Batch Loss: 0.1935528814792633\n",
      "Epoch 5580, Loss: 0.6952682286500931, Final Batch Loss: 0.17452453076839447\n",
      "Epoch 5581, Loss: 0.8245915472507477, Final Batch Loss: 0.20582681894302368\n",
      "Epoch 5582, Loss: 0.8257980942726135, Final Batch Loss: 0.17027676105499268\n",
      "Epoch 5583, Loss: 0.8285253196954727, Final Batch Loss: 0.27609726786613464\n",
      "Epoch 5584, Loss: 0.9257410168647766, Final Batch Loss: 0.19682838022708893\n",
      "Epoch 5585, Loss: 0.6841425150632858, Final Batch Loss: 0.22103720903396606\n",
      "Epoch 5586, Loss: 0.8355044722557068, Final Batch Loss: 0.22055013477802277\n",
      "Epoch 5587, Loss: 0.7401875257492065, Final Batch Loss: 0.21740831434726715\n",
      "Epoch 5588, Loss: 0.7123395651578903, Final Batch Loss: 0.22183862328529358\n",
      "Epoch 5589, Loss: 0.7779051512479782, Final Batch Loss: 0.16502344608306885\n",
      "Epoch 5590, Loss: 0.7451040595769882, Final Batch Loss: 0.16649629175662994\n",
      "Epoch 5591, Loss: 0.8821434080600739, Final Batch Loss: 0.1849156618118286\n",
      "Epoch 5592, Loss: 0.7317787930369377, Final Batch Loss: 0.18418818712234497\n",
      "Epoch 5593, Loss: 0.7775194644927979, Final Batch Loss: 0.17246711254119873\n",
      "Epoch 5594, Loss: 0.7895784378051758, Final Batch Loss: 0.2354087382555008\n",
      "Epoch 5595, Loss: 0.862790122628212, Final Batch Loss: 0.23855815827846527\n",
      "Epoch 5596, Loss: 0.719590313732624, Final Batch Loss: 0.17046219110488892\n",
      "Epoch 5597, Loss: 0.8424496054649353, Final Batch Loss: 0.2411588579416275\n",
      "Epoch 5598, Loss: 0.8168105781078339, Final Batch Loss: 0.1935400664806366\n",
      "Epoch 5599, Loss: 0.7741992175579071, Final Batch Loss: 0.1929008513689041\n",
      "Epoch 5600, Loss: 0.8110094219446182, Final Batch Loss: 0.19790133833885193\n",
      "Epoch 5601, Loss: 0.7675287872552872, Final Batch Loss: 0.24143926799297333\n",
      "Epoch 5602, Loss: 0.7145512998104095, Final Batch Loss: 0.15435157716274261\n",
      "Epoch 5603, Loss: 0.7457866072654724, Final Batch Loss: 0.18549774587154388\n",
      "Epoch 5604, Loss: 0.8968778848648071, Final Batch Loss: 0.22980324923992157\n",
      "Epoch 5605, Loss: 0.7436351329088211, Final Batch Loss: 0.17664702236652374\n",
      "Epoch 5606, Loss: 0.8467922806739807, Final Batch Loss: 0.17824703454971313\n",
      "Epoch 5607, Loss: 0.8476449251174927, Final Batch Loss: 0.14526620507240295\n",
      "Epoch 5608, Loss: 0.8529940247535706, Final Batch Loss: 0.291579931974411\n",
      "Epoch 5609, Loss: 0.7896035313606262, Final Batch Loss: 0.2065378725528717\n",
      "Epoch 5610, Loss: 0.9065444469451904, Final Batch Loss: 0.18472909927368164\n",
      "Epoch 5611, Loss: 0.7954041585326195, Final Batch Loss: 0.25484350323677063\n",
      "Epoch 5612, Loss: 0.7290407717227936, Final Batch Loss: 0.17663811147212982\n",
      "Epoch 5613, Loss: 0.7869149297475815, Final Batch Loss: 0.15077976882457733\n",
      "Epoch 5614, Loss: 0.7338393181562424, Final Batch Loss: 0.19667796790599823\n",
      "Epoch 5615, Loss: 0.6565075516700745, Final Batch Loss: 0.1839054673910141\n",
      "Epoch 5616, Loss: 0.8255914002656937, Final Batch Loss: 0.18268591165542603\n",
      "Epoch 5617, Loss: 0.68597012758255, Final Batch Loss: 0.1705973595380783\n",
      "Epoch 5618, Loss: 0.7914883196353912, Final Batch Loss: 0.20373007655143738\n",
      "Epoch 5619, Loss: 0.83423912525177, Final Batch Loss: 0.20934996008872986\n",
      "Epoch 5620, Loss: 0.7960619926452637, Final Batch Loss: 0.1571388840675354\n",
      "Epoch 5621, Loss: 0.7561340183019638, Final Batch Loss: 0.2833811938762665\n",
      "Epoch 5622, Loss: 0.910304993391037, Final Batch Loss: 0.24339114129543304\n",
      "Epoch 5623, Loss: 0.7133875042200089, Final Batch Loss: 0.17163239419460297\n",
      "Epoch 5624, Loss: 0.9264909476041794, Final Batch Loss: 0.28623253107070923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5625, Loss: 0.82311150431633, Final Batch Loss: 0.24791449308395386\n",
      "Epoch 5626, Loss: 0.9462472796440125, Final Batch Loss: 0.20026393234729767\n",
      "Epoch 5627, Loss: 0.7353154271841049, Final Batch Loss: 0.1996953934431076\n",
      "Epoch 5628, Loss: 0.8575240075588226, Final Batch Loss: 0.13484789431095123\n",
      "Epoch 5629, Loss: 0.7124253958463669, Final Batch Loss: 0.17420876026153564\n",
      "Epoch 5630, Loss: 0.7185527235269547, Final Batch Loss: 0.1501283198595047\n",
      "Epoch 5631, Loss: 0.7998879104852676, Final Batch Loss: 0.24553579092025757\n",
      "Epoch 5632, Loss: 0.9143798351287842, Final Batch Loss: 0.20439493656158447\n",
      "Epoch 5633, Loss: 0.7654983401298523, Final Batch Loss: 0.16290728747844696\n",
      "Epoch 5634, Loss: 0.756891518831253, Final Batch Loss: 0.17540091276168823\n",
      "Epoch 5635, Loss: 0.8582514971494675, Final Batch Loss: 0.25803136825561523\n",
      "Epoch 5636, Loss: 0.7519557774066925, Final Batch Loss: 0.15732648968696594\n",
      "Epoch 5637, Loss: 0.7043597400188446, Final Batch Loss: 0.18397113680839539\n",
      "Epoch 5638, Loss: 0.7632675170898438, Final Batch Loss: 0.16511014103889465\n",
      "Epoch 5639, Loss: 0.8563321083784103, Final Batch Loss: 0.2646922171115875\n",
      "Epoch 5640, Loss: 0.7931753098964691, Final Batch Loss: 0.25859805941581726\n",
      "Epoch 5641, Loss: 0.6467086002230644, Final Batch Loss: 0.17017780244350433\n",
      "Epoch 5642, Loss: 0.9166355580091476, Final Batch Loss: 0.22754597663879395\n",
      "Epoch 5643, Loss: 0.8083678781986237, Final Batch Loss: 0.16836804151535034\n",
      "Epoch 5644, Loss: 0.8164170533418655, Final Batch Loss: 0.22904010117053986\n",
      "Epoch 5645, Loss: 0.7569193840026855, Final Batch Loss: 0.1616646647453308\n",
      "Epoch 5646, Loss: 0.8054316490888596, Final Batch Loss: 0.13332851231098175\n",
      "Epoch 5647, Loss: 0.7682466208934784, Final Batch Loss: 0.2125912606716156\n",
      "Epoch 5648, Loss: 0.7566518634557724, Final Batch Loss: 0.1925717294216156\n",
      "Epoch 5649, Loss: 0.8028968870639801, Final Batch Loss: 0.14280201494693756\n",
      "Epoch 5650, Loss: 0.8068110942840576, Final Batch Loss: 0.28115609288215637\n",
      "Epoch 5651, Loss: 0.8659441024065018, Final Batch Loss: 0.254616379737854\n",
      "Epoch 5652, Loss: 0.7235783487558365, Final Batch Loss: 0.22576327621936798\n",
      "Epoch 5653, Loss: 0.6788692474365234, Final Batch Loss: 0.16049984097480774\n",
      "Epoch 5654, Loss: 0.8882080614566803, Final Batch Loss: 0.28640294075012207\n",
      "Epoch 5655, Loss: 0.7413576692342758, Final Batch Loss: 0.19021664559841156\n",
      "Epoch 5656, Loss: 0.691742554306984, Final Batch Loss: 0.1471593677997589\n",
      "Epoch 5657, Loss: 0.684392973780632, Final Batch Loss: 0.12705600261688232\n",
      "Epoch 5658, Loss: 0.7641144096851349, Final Batch Loss: 0.19337932765483856\n",
      "Epoch 5659, Loss: 0.75797039270401, Final Batch Loss: 0.15214534103870392\n",
      "Epoch 5660, Loss: 0.8483789712190628, Final Batch Loss: 0.1390874683856964\n",
      "Epoch 5661, Loss: 0.8152908831834793, Final Batch Loss: 0.2213708609342575\n",
      "Epoch 5662, Loss: 0.7336925566196442, Final Batch Loss: 0.1963339000940323\n",
      "Epoch 5663, Loss: 0.8391970545053482, Final Batch Loss: 0.2670981287956238\n",
      "Epoch 5664, Loss: 0.9520469307899475, Final Batch Loss: 0.2493356615304947\n",
      "Epoch 5665, Loss: 0.7110544592142105, Final Batch Loss: 0.19618648290634155\n",
      "Epoch 5666, Loss: 0.7686698287725449, Final Batch Loss: 0.2157025784254074\n",
      "Epoch 5667, Loss: 0.6956872045993805, Final Batch Loss: 0.1164754331111908\n",
      "Epoch 5668, Loss: 0.7787439078092575, Final Batch Loss: 0.19445568323135376\n",
      "Epoch 5669, Loss: 0.793257087469101, Final Batch Loss: 0.22789743542671204\n",
      "Epoch 5670, Loss: 0.7392576336860657, Final Batch Loss: 0.17386187613010406\n",
      "Epoch 5671, Loss: 0.907012864947319, Final Batch Loss: 0.2608281970024109\n",
      "Epoch 5672, Loss: 0.9086986184120178, Final Batch Loss: 0.20068389177322388\n",
      "Epoch 5673, Loss: 1.008766070008278, Final Batch Loss: 0.20886941254138947\n",
      "Epoch 5674, Loss: 0.8232864886522293, Final Batch Loss: 0.18337462842464447\n",
      "Epoch 5675, Loss: 0.7240179628133774, Final Batch Loss: 0.13927620649337769\n",
      "Epoch 5676, Loss: 0.8762317597866058, Final Batch Loss: 0.1988953799009323\n",
      "Epoch 5677, Loss: 0.808369368314743, Final Batch Loss: 0.2545449733734131\n",
      "Epoch 5678, Loss: 0.8699416667222977, Final Batch Loss: 0.15348967909812927\n",
      "Epoch 5679, Loss: 0.8027567714452744, Final Batch Loss: 0.24903622269630432\n",
      "Epoch 5680, Loss: 0.916462630033493, Final Batch Loss: 0.24018394947052002\n",
      "Epoch 5681, Loss: 0.8992495685815811, Final Batch Loss: 0.25865647196769714\n",
      "Epoch 5682, Loss: 0.8277855813503265, Final Batch Loss: 0.21838714182376862\n",
      "Epoch 5683, Loss: 0.7491929084062576, Final Batch Loss: 0.17130692303180695\n",
      "Epoch 5684, Loss: 0.8507059216499329, Final Batch Loss: 0.16997064650058746\n",
      "Epoch 5685, Loss: 0.8511882126331329, Final Batch Loss: 0.20187608897686005\n",
      "Epoch 5686, Loss: 0.7419582307338715, Final Batch Loss: 0.1813216209411621\n",
      "Epoch 5687, Loss: 0.8296262919902802, Final Batch Loss: 0.22780939936637878\n",
      "Epoch 5688, Loss: 0.8771574348211288, Final Batch Loss: 0.1994422823190689\n",
      "Epoch 5689, Loss: 0.7625733762979507, Final Batch Loss: 0.1524232029914856\n",
      "Epoch 5690, Loss: 0.92283795773983, Final Batch Loss: 0.24364812672138214\n",
      "Epoch 5691, Loss: 0.7633570730686188, Final Batch Loss: 0.18379932641983032\n",
      "Epoch 5692, Loss: 0.8235146999359131, Final Batch Loss: 0.2293129563331604\n",
      "Epoch 5693, Loss: 0.7866737991571426, Final Batch Loss: 0.22743889689445496\n",
      "Epoch 5694, Loss: 0.7637514173984528, Final Batch Loss: 0.22853417694568634\n",
      "Epoch 5695, Loss: 0.7189157903194427, Final Batch Loss: 0.15272247791290283\n",
      "Epoch 5696, Loss: 0.6973619610071182, Final Batch Loss: 0.17684945464134216\n",
      "Epoch 5697, Loss: 0.7794425934553146, Final Batch Loss: 0.20060285925865173\n",
      "Epoch 5698, Loss: 0.9659392535686493, Final Batch Loss: 0.18664711713790894\n",
      "Epoch 5699, Loss: 0.7444798052310944, Final Batch Loss: 0.2164820432662964\n",
      "Epoch 5700, Loss: 0.7418207228183746, Final Batch Loss: 0.16335393488407135\n",
      "Epoch 5701, Loss: 0.7713172286748886, Final Batch Loss: 0.14969849586486816\n",
      "Epoch 5702, Loss: 0.8454531133174896, Final Batch Loss: 0.25400131940841675\n",
      "Epoch 5703, Loss: 0.8271691054105759, Final Batch Loss: 0.25115668773651123\n",
      "Epoch 5704, Loss: 0.8663220703601837, Final Batch Loss: 0.16065713763237\n",
      "Epoch 5705, Loss: 0.8331567049026489, Final Batch Loss: 0.2300904244184494\n",
      "Epoch 5706, Loss: 0.8784089535474777, Final Batch Loss: 0.15750272572040558\n",
      "Epoch 5707, Loss: 0.8900936096906662, Final Batch Loss: 0.27994608879089355\n",
      "Epoch 5708, Loss: 0.775454580783844, Final Batch Loss: 0.1533922702074051\n",
      "Epoch 5709, Loss: 0.8219475597143173, Final Batch Loss: 0.15429797768592834\n",
      "Epoch 5710, Loss: 0.8419378399848938, Final Batch Loss: 0.27989450097084045\n",
      "Epoch 5711, Loss: 0.6519929468631744, Final Batch Loss: 0.14555411040782928\n",
      "Epoch 5712, Loss: 0.752898558974266, Final Batch Loss: 0.19873717427253723\n",
      "Epoch 5713, Loss: 1.0781191438436508, Final Batch Loss: 0.23573824763298035\n",
      "Epoch 5714, Loss: 0.6914889812469482, Final Batch Loss: 0.17238415777683258\n",
      "Epoch 5715, Loss: 0.8288372606039047, Final Batch Loss: 0.23985709249973297\n",
      "Epoch 5716, Loss: 0.727199375629425, Final Batch Loss: 0.1799091249704361\n",
      "Epoch 5717, Loss: 0.7731040418148041, Final Batch Loss: 0.21830567717552185\n",
      "Epoch 5718, Loss: 0.7687957733869553, Final Batch Loss: 0.14033439755439758\n",
      "Epoch 5719, Loss: 0.8034672737121582, Final Batch Loss: 0.17875345051288605\n",
      "Epoch 5720, Loss: 0.9625363945960999, Final Batch Loss: 0.37713295221328735\n",
      "Epoch 5721, Loss: 0.8841313272714615, Final Batch Loss: 0.20915335416793823\n",
      "Epoch 5722, Loss: 0.7571994662284851, Final Batch Loss: 0.1268993467092514\n",
      "Epoch 5723, Loss: 0.7291532605886459, Final Batch Loss: 0.14312095940113068\n",
      "Epoch 5724, Loss: 0.7913522869348526, Final Batch Loss: 0.215818390250206\n",
      "Epoch 5725, Loss: 0.804298460483551, Final Batch Loss: 0.164803609251976\n",
      "Epoch 5726, Loss: 0.6946613863110542, Final Batch Loss: 0.2125636339187622\n",
      "Epoch 5727, Loss: 0.7493255436420441, Final Batch Loss: 0.21881969273090363\n",
      "Epoch 5728, Loss: 0.8733159452676773, Final Batch Loss: 0.22078390419483185\n",
      "Epoch 5729, Loss: 0.6288971528410912, Final Batch Loss: 0.19614487886428833\n",
      "Epoch 5730, Loss: 0.7909456491470337, Final Batch Loss: 0.1992943286895752\n",
      "Epoch 5731, Loss: 0.8566187620162964, Final Batch Loss: 0.18100345134735107\n",
      "Epoch 5732, Loss: 0.8679210841655731, Final Batch Loss: 0.22481830418109894\n",
      "Epoch 5733, Loss: 0.8889166861772537, Final Batch Loss: 0.31345459818840027\n",
      "Epoch 5734, Loss: 0.8156567513942719, Final Batch Loss: 0.1864723414182663\n",
      "Epoch 5735, Loss: 0.6872255504131317, Final Batch Loss: 0.14412543177604675\n",
      "Epoch 5736, Loss: 0.8412657231092453, Final Batch Loss: 0.21233832836151123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5737, Loss: 0.8835771381855011, Final Batch Loss: 0.21013078093528748\n",
      "Epoch 5738, Loss: 0.8324141651391983, Final Batch Loss: 0.18852105736732483\n",
      "Epoch 5739, Loss: 0.7971430271863937, Final Batch Loss: 0.179060697555542\n",
      "Epoch 5740, Loss: 0.6906462982296944, Final Batch Loss: 0.13989153504371643\n",
      "Epoch 5741, Loss: 0.8420916050672531, Final Batch Loss: 0.2499726265668869\n",
      "Epoch 5742, Loss: 0.8831836432218552, Final Batch Loss: 0.2372925877571106\n",
      "Epoch 5743, Loss: 0.8597098737955093, Final Batch Loss: 0.38427814841270447\n",
      "Epoch 5744, Loss: 0.8872105330228806, Final Batch Loss: 0.20513412356376648\n",
      "Epoch 5745, Loss: 0.7812165468931198, Final Batch Loss: 0.21715664863586426\n",
      "Epoch 5746, Loss: 0.7718890905380249, Final Batch Loss: 0.22353756427764893\n",
      "Epoch 5747, Loss: 0.7969168424606323, Final Batch Loss: 0.22892916202545166\n",
      "Epoch 5748, Loss: 0.8741713762283325, Final Batch Loss: 0.2222491055727005\n",
      "Epoch 5749, Loss: 0.8414981365203857, Final Batch Loss: 0.25551649928092957\n",
      "Epoch 5750, Loss: 0.8344040662050247, Final Batch Loss: 0.232633575797081\n",
      "Epoch 5751, Loss: 0.8651768416166306, Final Batch Loss: 0.22506694495677948\n",
      "Epoch 5752, Loss: 0.8570355772972107, Final Batch Loss: 0.23168940842151642\n",
      "Epoch 5753, Loss: 0.7541011571884155, Final Batch Loss: 0.13935811817646027\n",
      "Epoch 5754, Loss: 0.7279872745275497, Final Batch Loss: 0.1632215529680252\n",
      "Epoch 5755, Loss: 0.897188276052475, Final Batch Loss: 0.226054847240448\n",
      "Epoch 5756, Loss: 0.8175980597734451, Final Batch Loss: 0.24216274917125702\n",
      "Epoch 5757, Loss: 0.7697209119796753, Final Batch Loss: 0.18893346190452576\n",
      "Epoch 5758, Loss: 0.8037030100822449, Final Batch Loss: 0.1855466216802597\n",
      "Epoch 5759, Loss: 0.7757578790187836, Final Batch Loss: 0.14410726726055145\n",
      "Epoch 5760, Loss: 0.8014801442623138, Final Batch Loss: 0.20429518818855286\n",
      "Epoch 5761, Loss: 0.8135702610015869, Final Batch Loss: 0.22243289649486542\n",
      "Epoch 5762, Loss: 0.8389776647090912, Final Batch Loss: 0.13301147520542145\n",
      "Epoch 5763, Loss: 0.8840587884187698, Final Batch Loss: 0.18511709570884705\n",
      "Epoch 5764, Loss: 0.7603421807289124, Final Batch Loss: 0.18840382993221283\n",
      "Epoch 5765, Loss: 0.7810607552528381, Final Batch Loss: 0.22333495318889618\n",
      "Epoch 5766, Loss: 0.6638811230659485, Final Batch Loss: 0.14201635122299194\n",
      "Epoch 5767, Loss: 0.7173022776842117, Final Batch Loss: 0.19717633724212646\n",
      "Epoch 5768, Loss: 0.8224959075450897, Final Batch Loss: 0.2152956873178482\n",
      "Epoch 5769, Loss: 0.8622100353240967, Final Batch Loss: 0.14343440532684326\n",
      "Epoch 5770, Loss: 0.7576622813940048, Final Batch Loss: 0.2557368278503418\n",
      "Epoch 5771, Loss: 0.7568455934524536, Final Batch Loss: 0.21846388280391693\n",
      "Epoch 5772, Loss: 0.6936933249235153, Final Batch Loss: 0.20263129472732544\n",
      "Epoch 5773, Loss: 0.6991452723741531, Final Batch Loss: 0.18156114220619202\n",
      "Epoch 5774, Loss: 0.8055657595396042, Final Batch Loss: 0.21383464336395264\n",
      "Epoch 5775, Loss: 0.8308569714426994, Final Batch Loss: 0.1613120585680008\n",
      "Epoch 5776, Loss: 0.8344811797142029, Final Batch Loss: 0.21824195981025696\n",
      "Epoch 5777, Loss: 0.7724439799785614, Final Batch Loss: 0.22877158224582672\n",
      "Epoch 5778, Loss: 0.7247944623231888, Final Batch Loss: 0.14468184113502502\n",
      "Epoch 5779, Loss: 0.7983470857143402, Final Batch Loss: 0.22834596037864685\n",
      "Epoch 5780, Loss: 0.8128046840429306, Final Batch Loss: 0.22633342444896698\n",
      "Epoch 5781, Loss: 0.8694043457508087, Final Batch Loss: 0.20014140009880066\n",
      "Epoch 5782, Loss: 0.7337789684534073, Final Batch Loss: 0.16179195046424866\n",
      "Epoch 5783, Loss: 0.8389798998832703, Final Batch Loss: 0.1756170392036438\n",
      "Epoch 5784, Loss: 0.7052669078111649, Final Batch Loss: 0.2032025307416916\n",
      "Epoch 5785, Loss: 0.729713574051857, Final Batch Loss: 0.26484182476997375\n",
      "Epoch 5786, Loss: 0.7272225320339203, Final Batch Loss: 0.22314505279064178\n",
      "Epoch 5787, Loss: 0.8040377795696259, Final Batch Loss: 0.16804924607276917\n",
      "Epoch 5788, Loss: 0.754790797829628, Final Batch Loss: 0.16519400477409363\n",
      "Epoch 5789, Loss: 0.7687212824821472, Final Batch Loss: 0.24246294796466827\n",
      "Epoch 5790, Loss: 0.9126842021942139, Final Batch Loss: 0.28500601649284363\n",
      "Epoch 5791, Loss: 0.7500588893890381, Final Batch Loss: 0.20243830978870392\n",
      "Epoch 5792, Loss: 0.8044983297586441, Final Batch Loss: 0.2263234704732895\n",
      "Epoch 5793, Loss: 0.9018776267766953, Final Batch Loss: 0.2866385877132416\n",
      "Epoch 5794, Loss: 0.8178422898054123, Final Batch Loss: 0.2017175257205963\n",
      "Epoch 5795, Loss: 0.7492868900299072, Final Batch Loss: 0.20948584377765656\n",
      "Epoch 5796, Loss: 0.7700199037790298, Final Batch Loss: 0.1585337072610855\n",
      "Epoch 5797, Loss: 0.754480853676796, Final Batch Loss: 0.10583357512950897\n",
      "Epoch 5798, Loss: 0.8205137848854065, Final Batch Loss: 0.1916152834892273\n",
      "Epoch 5799, Loss: 0.8935306519269943, Final Batch Loss: 0.21868985891342163\n",
      "Epoch 5800, Loss: 0.674768328666687, Final Batch Loss: 0.17969952523708344\n",
      "Epoch 5801, Loss: 0.7956337928771973, Final Batch Loss: 0.23988687992095947\n",
      "Epoch 5802, Loss: 0.8039809316396713, Final Batch Loss: 0.17844617366790771\n",
      "Epoch 5803, Loss: 0.8684832602739334, Final Batch Loss: 0.17488063871860504\n",
      "Epoch 5804, Loss: 0.771346926689148, Final Batch Loss: 0.16507863998413086\n",
      "Epoch 5805, Loss: 0.7314450740814209, Final Batch Loss: 0.21166925132274628\n",
      "Epoch 5806, Loss: 0.6653314977884293, Final Batch Loss: 0.14182431995868683\n",
      "Epoch 5807, Loss: 0.7013071030378342, Final Batch Loss: 0.15474729239940643\n",
      "Epoch 5808, Loss: 0.8176703751087189, Final Batch Loss: 0.21969836950302124\n",
      "Epoch 5809, Loss: 0.8596038669347763, Final Batch Loss: 0.20025907456874847\n",
      "Epoch 5810, Loss: 0.8869637995958328, Final Batch Loss: 0.2326166331768036\n",
      "Epoch 5811, Loss: 0.7983800172805786, Final Batch Loss: 0.19544948637485504\n",
      "Epoch 5812, Loss: 0.8206498622894287, Final Batch Loss: 0.15913616120815277\n",
      "Epoch 5813, Loss: 0.7548356801271439, Final Batch Loss: 0.22722603380680084\n",
      "Epoch 5814, Loss: 0.7416538745164871, Final Batch Loss: 0.1676570177078247\n",
      "Epoch 5815, Loss: 0.7881431728601456, Final Batch Loss: 0.21556787192821503\n",
      "Epoch 5816, Loss: 0.7672391682863235, Final Batch Loss: 0.24524468183517456\n",
      "Epoch 5817, Loss: 0.7292448133230209, Final Batch Loss: 0.21697360277175903\n",
      "Epoch 5818, Loss: 0.7880423069000244, Final Batch Loss: 0.16267703473567963\n",
      "Epoch 5819, Loss: 0.8551837056875229, Final Batch Loss: 0.24858734011650085\n",
      "Epoch 5820, Loss: 0.7820941209793091, Final Batch Loss: 0.16959209740161896\n",
      "Epoch 5821, Loss: 0.7513054460287094, Final Batch Loss: 0.23968461155891418\n",
      "Epoch 5822, Loss: 0.7736045718193054, Final Batch Loss: 0.1886075884103775\n",
      "Epoch 5823, Loss: 0.7067923247814178, Final Batch Loss: 0.17067168653011322\n",
      "Epoch 5824, Loss: 0.9379656910896301, Final Batch Loss: 0.2920370399951935\n",
      "Epoch 5825, Loss: 0.7733573913574219, Final Batch Loss: 0.2044544219970703\n",
      "Epoch 5826, Loss: 0.8292103558778763, Final Batch Loss: 0.15557357668876648\n",
      "Epoch 5827, Loss: 0.721414104104042, Final Batch Loss: 0.14143863320350647\n",
      "Epoch 5828, Loss: 0.7421901673078537, Final Batch Loss: 0.1431524008512497\n",
      "Epoch 5829, Loss: 0.7499611526727676, Final Batch Loss: 0.14833396673202515\n",
      "Epoch 5830, Loss: 0.8749480992555618, Final Batch Loss: 0.1981046050786972\n",
      "Epoch 5831, Loss: 0.7435152530670166, Final Batch Loss: 0.1957104504108429\n",
      "Epoch 5832, Loss: 0.7675748318433762, Final Batch Loss: 0.16404826939105988\n",
      "Epoch 5833, Loss: 0.7513018846511841, Final Batch Loss: 0.22215434908866882\n",
      "Epoch 5834, Loss: 0.7429402470588684, Final Batch Loss: 0.20720316469669342\n",
      "Epoch 5835, Loss: 0.789961114525795, Final Batch Loss: 0.2040984034538269\n",
      "Epoch 5836, Loss: 0.7004265636205673, Final Batch Loss: 0.15099190175533295\n",
      "Epoch 5837, Loss: 0.7990546524524689, Final Batch Loss: 0.17480316758155823\n",
      "Epoch 5838, Loss: 0.7481780052185059, Final Batch Loss: 0.16906367242336273\n",
      "Epoch 5839, Loss: 0.701415553689003, Final Batch Loss: 0.13129384815692902\n",
      "Epoch 5840, Loss: 0.9153289645910263, Final Batch Loss: 0.28114649653434753\n",
      "Epoch 5841, Loss: 0.6706389263272285, Final Batch Loss: 0.15188278257846832\n",
      "Epoch 5842, Loss: 0.7815217524766922, Final Batch Loss: 0.18049032986164093\n",
      "Epoch 5843, Loss: 0.8345334976911545, Final Batch Loss: 0.22677427530288696\n",
      "Epoch 5844, Loss: 0.7519350498914719, Final Batch Loss: 0.17845317721366882\n",
      "Epoch 5845, Loss: 0.7254514098167419, Final Batch Loss: 0.18202471733093262\n",
      "Epoch 5846, Loss: 0.8386064022779465, Final Batch Loss: 0.2639671862125397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5847, Loss: 0.7359699606895447, Final Batch Loss: 0.14188680052757263\n",
      "Epoch 5848, Loss: 0.784737303853035, Final Batch Loss: 0.1882057785987854\n",
      "Epoch 5849, Loss: 0.7679391801357269, Final Batch Loss: 0.15316033363342285\n",
      "Epoch 5850, Loss: 0.919746607542038, Final Batch Loss: 0.23071898519992828\n",
      "Epoch 5851, Loss: 0.7336117327213287, Final Batch Loss: 0.1894630640745163\n",
      "Epoch 5852, Loss: 0.8975627273321152, Final Batch Loss: 0.24356219172477722\n",
      "Epoch 5853, Loss: 0.8372720181941986, Final Batch Loss: 0.15412667393684387\n",
      "Epoch 5854, Loss: 0.7367458194494247, Final Batch Loss: 0.2109636813402176\n",
      "Epoch 5855, Loss: 0.7592188715934753, Final Batch Loss: 0.167583629488945\n",
      "Epoch 5856, Loss: 0.8086776286363602, Final Batch Loss: 0.24057266116142273\n",
      "Epoch 5857, Loss: 0.8207934647798538, Final Batch Loss: 0.20381344854831696\n",
      "Epoch 5858, Loss: 0.8379723578691483, Final Batch Loss: 0.20534668862819672\n",
      "Epoch 5859, Loss: 0.7707716226577759, Final Batch Loss: 0.1864539384841919\n",
      "Epoch 5860, Loss: 0.8059251606464386, Final Batch Loss: 0.2607439458370209\n",
      "Epoch 5861, Loss: 0.8504259139299393, Final Batch Loss: 0.2873264253139496\n",
      "Epoch 5862, Loss: 0.6246561855077744, Final Batch Loss: 0.1642565280199051\n",
      "Epoch 5863, Loss: 0.9323203563690186, Final Batch Loss: 0.21607841551303864\n",
      "Epoch 5864, Loss: 0.8618040978908539, Final Batch Loss: 0.18707558512687683\n",
      "Epoch 5865, Loss: 0.8071926236152649, Final Batch Loss: 0.20196202397346497\n",
      "Epoch 5866, Loss: 0.8576926440000534, Final Batch Loss: 0.28485071659088135\n",
      "Epoch 5867, Loss: 0.8206602782011032, Final Batch Loss: 0.18932189047336578\n",
      "Epoch 5868, Loss: 0.7863878607749939, Final Batch Loss: 0.2631848156452179\n",
      "Epoch 5869, Loss: 0.6854294538497925, Final Batch Loss: 0.21044592559337616\n",
      "Epoch 5870, Loss: 0.7750055938959122, Final Batch Loss: 0.12433786690235138\n",
      "Epoch 5871, Loss: 0.8503745943307877, Final Batch Loss: 0.2510609030723572\n",
      "Epoch 5872, Loss: 0.8157368153333664, Final Batch Loss: 0.21675322949886322\n",
      "Epoch 5873, Loss: 0.8375068008899689, Final Batch Loss: 0.24159091711044312\n",
      "Epoch 5874, Loss: 0.6945711672306061, Final Batch Loss: 0.18991218507289886\n",
      "Epoch 5875, Loss: 0.8028087168931961, Final Batch Loss: 0.18164969980716705\n",
      "Epoch 5876, Loss: 0.7696375101804733, Final Batch Loss: 0.22239065170288086\n",
      "Epoch 5877, Loss: 0.7629978358745575, Final Batch Loss: 0.22630427777767181\n",
      "Epoch 5878, Loss: 0.7836230397224426, Final Batch Loss: 0.20513834059238434\n",
      "Epoch 5879, Loss: 0.694936990737915, Final Batch Loss: 0.1620766520500183\n",
      "Epoch 5880, Loss: 0.8682145029306412, Final Batch Loss: 0.20205160975456238\n",
      "Epoch 5881, Loss: 0.8449247628450394, Final Batch Loss: 0.18159426748752594\n",
      "Epoch 5882, Loss: 0.763640433549881, Final Batch Loss: 0.1716838926076889\n",
      "Epoch 5883, Loss: 0.694852352142334, Final Batch Loss: 0.21821917593479156\n",
      "Epoch 5884, Loss: 0.7251933515071869, Final Batch Loss: 0.25057047605514526\n",
      "Epoch 5885, Loss: 0.7058943659067154, Final Batch Loss: 0.12962396442890167\n",
      "Epoch 5886, Loss: 0.7845790833234787, Final Batch Loss: 0.18171511590480804\n",
      "Epoch 5887, Loss: 0.7531338930130005, Final Batch Loss: 0.23025476932525635\n",
      "Epoch 5888, Loss: 0.816410206258297, Final Batch Loss: 0.2664584815502167\n",
      "Epoch 5889, Loss: 0.7970408648252487, Final Batch Loss: 0.20528580248355865\n",
      "Epoch 5890, Loss: 0.6687492951750755, Final Batch Loss: 0.22303560376167297\n",
      "Epoch 5891, Loss: 0.8479523360729218, Final Batch Loss: 0.2108706384897232\n",
      "Epoch 5892, Loss: 0.7093794941902161, Final Batch Loss: 0.1675025224685669\n",
      "Epoch 5893, Loss: 0.7238407284021378, Final Batch Loss: 0.18103347718715668\n",
      "Epoch 5894, Loss: 0.8084910959005356, Final Batch Loss: 0.20359821617603302\n",
      "Epoch 5895, Loss: 0.6972305178642273, Final Batch Loss: 0.19179119169712067\n",
      "Epoch 5896, Loss: 0.8088879138231277, Final Batch Loss: 0.26584556698799133\n",
      "Epoch 5897, Loss: 0.8077177405357361, Final Batch Loss: 0.20004743337631226\n",
      "Epoch 5898, Loss: 0.8610824346542358, Final Batch Loss: 0.15889035165309906\n",
      "Epoch 5899, Loss: 0.8384762853384018, Final Batch Loss: 0.23117895424365997\n",
      "Epoch 5900, Loss: 0.8285431861877441, Final Batch Loss: 0.2183154672384262\n",
      "Epoch 5901, Loss: 0.7714666128158569, Final Batch Loss: 0.1857994794845581\n",
      "Epoch 5902, Loss: 0.7285155653953552, Final Batch Loss: 0.22419925034046173\n",
      "Epoch 5903, Loss: 0.7164885550737381, Final Batch Loss: 0.15020491182804108\n",
      "Epoch 5904, Loss: 0.6861431449651718, Final Batch Loss: 0.20297333598136902\n",
      "Epoch 5905, Loss: 0.8854171335697174, Final Batch Loss: 0.2561781108379364\n",
      "Epoch 5906, Loss: 0.6922134459018707, Final Batch Loss: 0.15330632030963898\n",
      "Epoch 5907, Loss: 0.7230192720890045, Final Batch Loss: 0.15289823710918427\n",
      "Epoch 5908, Loss: 0.819640040397644, Final Batch Loss: 0.15811575949192047\n",
      "Epoch 5909, Loss: 0.7403841614723206, Final Batch Loss: 0.13577380776405334\n",
      "Epoch 5910, Loss: 0.7040156424045563, Final Batch Loss: 0.13284766674041748\n",
      "Epoch 5911, Loss: 0.7460073828697205, Final Batch Loss: 0.1975637525320053\n",
      "Epoch 5912, Loss: 0.805700421333313, Final Batch Loss: 0.2656285762786865\n",
      "Epoch 5913, Loss: 0.6811716705560684, Final Batch Loss: 0.18482749164104462\n",
      "Epoch 5914, Loss: 0.778534010052681, Final Batch Loss: 0.19830924272537231\n",
      "Epoch 5915, Loss: 0.8825471326708794, Final Batch Loss: 0.38445964455604553\n",
      "Epoch 5916, Loss: 0.8351986557245255, Final Batch Loss: 0.17998258769512177\n",
      "Epoch 5917, Loss: 0.9569516032934189, Final Batch Loss: 0.29724952578544617\n",
      "Epoch 5918, Loss: 0.9520484358072281, Final Batch Loss: 0.28899022936820984\n",
      "Epoch 5919, Loss: 0.7337040156126022, Final Batch Loss: 0.15059642493724823\n",
      "Epoch 5920, Loss: 0.8093842267990112, Final Batch Loss: 0.17428408563137054\n",
      "Epoch 5921, Loss: 0.8594791442155838, Final Batch Loss: 0.20979799330234528\n",
      "Epoch 5922, Loss: 0.9561778753995895, Final Batch Loss: 0.3396487534046173\n",
      "Epoch 5923, Loss: 0.8919583410024643, Final Batch Loss: 0.20074979960918427\n",
      "Epoch 5924, Loss: 0.768135666847229, Final Batch Loss: 0.18214726448059082\n",
      "Epoch 5925, Loss: 0.7628384828567505, Final Batch Loss: 0.17716600000858307\n",
      "Epoch 5926, Loss: 0.8086298257112503, Final Batch Loss: 0.2512388527393341\n",
      "Epoch 5927, Loss: 0.9590862691402435, Final Batch Loss: 0.2379726767539978\n",
      "Epoch 5928, Loss: 0.7660875469446182, Final Batch Loss: 0.15961579978466034\n",
      "Epoch 5929, Loss: 0.883845642209053, Final Batch Loss: 0.28996017575263977\n",
      "Epoch 5930, Loss: 0.7864595204591751, Final Batch Loss: 0.184251070022583\n",
      "Epoch 5931, Loss: 0.8876362591981888, Final Batch Loss: 0.22925668954849243\n",
      "Epoch 5932, Loss: 0.7879678010940552, Final Batch Loss: 0.2118409126996994\n",
      "Epoch 5933, Loss: 0.9796624481678009, Final Batch Loss: 0.4217141568660736\n",
      "Epoch 5934, Loss: 0.8957683891057968, Final Batch Loss: 0.17339159548282623\n",
      "Epoch 5935, Loss: 0.8998228907585144, Final Batch Loss: 0.2012675404548645\n",
      "Epoch 5936, Loss: 0.8610481470823288, Final Batch Loss: 0.24841828644275665\n",
      "Epoch 5937, Loss: 0.7923964858055115, Final Batch Loss: 0.2557063400745392\n",
      "Epoch 5938, Loss: 0.8658626824617386, Final Batch Loss: 0.28624629974365234\n",
      "Epoch 5939, Loss: 0.7934998869895935, Final Batch Loss: 0.21812357008457184\n",
      "Epoch 5940, Loss: 0.742473840713501, Final Batch Loss: 0.24177557229995728\n",
      "Epoch 5941, Loss: 0.7560419589281082, Final Batch Loss: 0.1969614177942276\n",
      "Epoch 5942, Loss: 0.8549001812934875, Final Batch Loss: 0.23427915573120117\n",
      "Epoch 5943, Loss: 0.6717073619365692, Final Batch Loss: 0.15637987852096558\n",
      "Epoch 5944, Loss: 0.8023702204227448, Final Batch Loss: 0.20136767625808716\n",
      "Epoch 5945, Loss: 0.6946184486150742, Final Batch Loss: 0.20568343997001648\n",
      "Epoch 5946, Loss: 0.7999360263347626, Final Batch Loss: 0.21974100172519684\n",
      "Epoch 5947, Loss: 0.7156189531087875, Final Batch Loss: 0.24120263755321503\n",
      "Epoch 5948, Loss: 0.7594432383775711, Final Batch Loss: 0.1452457755804062\n",
      "Epoch 5949, Loss: 0.7616383582353592, Final Batch Loss: 0.15195202827453613\n",
      "Epoch 5950, Loss: 0.6735775023698807, Final Batch Loss: 0.19393834471702576\n",
      "Epoch 5951, Loss: 0.9692692309617996, Final Batch Loss: 0.23143108189105988\n",
      "Epoch 5952, Loss: 0.6873131394386292, Final Batch Loss: 0.14473047852516174\n",
      "Epoch 5953, Loss: 0.7395198494195938, Final Batch Loss: 0.1316981315612793\n",
      "Epoch 5954, Loss: 0.7868003100156784, Final Batch Loss: 0.22017671167850494\n",
      "Epoch 5955, Loss: 0.7402856349945068, Final Batch Loss: 0.16471125185489655\n",
      "Epoch 5956, Loss: 0.7428904324769974, Final Batch Loss: 0.1440383642911911\n",
      "Epoch 5957, Loss: 0.8103389963507652, Final Batch Loss: 0.12440835684537888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5958, Loss: 0.8854097276926041, Final Batch Loss: 0.19338488578796387\n",
      "Epoch 5959, Loss: 0.8340919017791748, Final Batch Loss: 0.19850946962833405\n",
      "Epoch 5960, Loss: 0.7795351892709732, Final Batch Loss: 0.207443505525589\n",
      "Epoch 5961, Loss: 0.7665981948375702, Final Batch Loss: 0.18715883791446686\n",
      "Epoch 5962, Loss: 0.7653178721666336, Final Batch Loss: 0.167038694024086\n",
      "Epoch 5963, Loss: 0.9327096939086914, Final Batch Loss: 0.293751984834671\n",
      "Epoch 5964, Loss: 0.7415505349636078, Final Batch Loss: 0.2549678385257721\n",
      "Epoch 5965, Loss: 0.8211895078420639, Final Batch Loss: 0.17331421375274658\n",
      "Epoch 5966, Loss: 0.7565661370754242, Final Batch Loss: 0.16814398765563965\n",
      "Epoch 5967, Loss: 0.846917599439621, Final Batch Loss: 0.1869821697473526\n",
      "Epoch 5968, Loss: 0.8377093970775604, Final Batch Loss: 0.1778145432472229\n",
      "Epoch 5969, Loss: 0.8322587609291077, Final Batch Loss: 0.21826614439487457\n",
      "Epoch 5970, Loss: 0.9000327289104462, Final Batch Loss: 0.2350386679172516\n",
      "Epoch 5971, Loss: 0.8802654445171356, Final Batch Loss: 0.29966405034065247\n",
      "Epoch 5972, Loss: 0.9295710474252701, Final Batch Loss: 0.240009605884552\n",
      "Epoch 5973, Loss: 0.9378128200769424, Final Batch Loss: 0.2177465707063675\n",
      "Epoch 5974, Loss: 0.6877314746379852, Final Batch Loss: 0.2046012282371521\n",
      "Epoch 5975, Loss: 1.1049698144197464, Final Batch Loss: 0.2883235216140747\n",
      "Epoch 5976, Loss: 0.7643131762742996, Final Batch Loss: 0.17512518167495728\n",
      "Epoch 5977, Loss: 1.0411702245473862, Final Batch Loss: 0.1854441910982132\n",
      "Epoch 5978, Loss: 0.9452884942293167, Final Batch Loss: 0.3053944706916809\n",
      "Epoch 5979, Loss: 0.8226941376924515, Final Batch Loss: 0.17938944697380066\n",
      "Epoch 5980, Loss: 0.8780723065137863, Final Batch Loss: 0.1522248089313507\n",
      "Epoch 5981, Loss: 0.8492178618907928, Final Batch Loss: 0.205454021692276\n",
      "Epoch 5982, Loss: 0.675633043050766, Final Batch Loss: 0.1687852442264557\n",
      "Epoch 5983, Loss: 0.8687351047992706, Final Batch Loss: 0.11383120715618134\n",
      "Epoch 5984, Loss: 0.8590487241744995, Final Batch Loss: 0.21520687639713287\n",
      "Epoch 5985, Loss: 0.9623750299215317, Final Batch Loss: 0.21410061419010162\n",
      "Epoch 5986, Loss: 0.800726056098938, Final Batch Loss: 0.2542233467102051\n",
      "Epoch 5987, Loss: 0.7917021363973618, Final Batch Loss: 0.16466344892978668\n",
      "Epoch 5988, Loss: 0.7450512498617172, Final Batch Loss: 0.1844496876001358\n",
      "Epoch 5989, Loss: 0.7854801714420319, Final Batch Loss: 0.22414150834083557\n",
      "Epoch 5990, Loss: 0.7809980511665344, Final Batch Loss: 0.20261219143867493\n",
      "Epoch 5991, Loss: 0.7050087749958038, Final Batch Loss: 0.19383616745471954\n",
      "Epoch 5992, Loss: 0.7667368948459625, Final Batch Loss: 0.20378078520298004\n",
      "Epoch 5993, Loss: 0.6440001726150513, Final Batch Loss: 0.1358524113893509\n",
      "Epoch 5994, Loss: 0.7727459371089935, Final Batch Loss: 0.19651000201702118\n",
      "Epoch 5995, Loss: 0.6825157552957535, Final Batch Loss: 0.19752204418182373\n",
      "Epoch 5996, Loss: 0.7615656107664108, Final Batch Loss: 0.23198379576206207\n",
      "Epoch 5997, Loss: 0.7206351011991501, Final Batch Loss: 0.17748595774173737\n",
      "Epoch 5998, Loss: 0.7331781536340714, Final Batch Loss: 0.18458984792232513\n",
      "Epoch 5999, Loss: 0.789879783987999, Final Batch Loss: 0.17949773371219635\n",
      "Epoch 6000, Loss: 0.6828902959823608, Final Batch Loss: 0.15668736398220062\n",
      "Epoch 6001, Loss: 0.7461038380861282, Final Batch Loss: 0.20404744148254395\n",
      "Epoch 6002, Loss: 0.9119439125061035, Final Batch Loss: 0.16783803701400757\n",
      "Epoch 6003, Loss: 0.6777042597532272, Final Batch Loss: 0.15387479960918427\n",
      "Epoch 6004, Loss: 0.7618226185441017, Final Batch Loss: 0.20384210348129272\n",
      "Epoch 6005, Loss: 0.8726481795310974, Final Batch Loss: 0.2221439927816391\n",
      "Epoch 6006, Loss: 0.8268959373235703, Final Batch Loss: 0.17713913321495056\n",
      "Epoch 6007, Loss: 0.8047435581684113, Final Batch Loss: 0.2001059353351593\n",
      "Epoch 6008, Loss: 0.8062181919813156, Final Batch Loss: 0.19106267392635345\n",
      "Epoch 6009, Loss: 0.7590189278125763, Final Batch Loss: 0.16734226047992706\n",
      "Epoch 6010, Loss: 0.7724542319774628, Final Batch Loss: 0.1814502328634262\n",
      "Epoch 6011, Loss: 0.7547696828842163, Final Batch Loss: 0.1863202303647995\n",
      "Epoch 6012, Loss: 0.7914456129074097, Final Batch Loss: 0.24504348635673523\n",
      "Epoch 6013, Loss: 0.6945137083530426, Final Batch Loss: 0.09389300644397736\n",
      "Epoch 6014, Loss: 0.6768221855163574, Final Batch Loss: 0.18104621767997742\n",
      "Epoch 6015, Loss: 0.8443178087472916, Final Batch Loss: 0.2916724979877472\n",
      "Epoch 6016, Loss: 0.7648840695619583, Final Batch Loss: 0.14283078908920288\n",
      "Epoch 6017, Loss: 0.6999755352735519, Final Batch Loss: 0.17734766006469727\n",
      "Epoch 6018, Loss: 0.7563162818551064, Final Batch Loss: 0.1864781677722931\n",
      "Epoch 6019, Loss: 0.8192562609910965, Final Batch Loss: 0.24331486225128174\n",
      "Epoch 6020, Loss: 0.8801350891590118, Final Batch Loss: 0.24915693700313568\n",
      "Epoch 6021, Loss: 0.7102953940629959, Final Batch Loss: 0.146922767162323\n",
      "Epoch 6022, Loss: 0.7739795446395874, Final Batch Loss: 0.21740318834781647\n",
      "Epoch 6023, Loss: 0.7434076815843582, Final Batch Loss: 0.19309628009796143\n",
      "Epoch 6024, Loss: 0.7171985954046249, Final Batch Loss: 0.1519884616136551\n",
      "Epoch 6025, Loss: 0.7351678758859634, Final Batch Loss: 0.15285666286945343\n",
      "Epoch 6026, Loss: 0.6815148741006851, Final Batch Loss: 0.15771889686584473\n",
      "Epoch 6027, Loss: 0.6536111906170845, Final Batch Loss: 0.11735094338655472\n",
      "Epoch 6028, Loss: 0.8418159782886505, Final Batch Loss: 0.20325836539268494\n",
      "Epoch 6029, Loss: 0.7423696368932724, Final Batch Loss: 0.18113023042678833\n",
      "Epoch 6030, Loss: 0.9212790578603745, Final Batch Loss: 0.27961465716362\n",
      "Epoch 6031, Loss: 0.7318965494632721, Final Batch Loss: 0.18491637706756592\n",
      "Epoch 6032, Loss: 0.6718710958957672, Final Batch Loss: 0.1780572384595871\n",
      "Epoch 6033, Loss: 0.6192007213830948, Final Batch Loss: 0.1369072049856186\n",
      "Epoch 6034, Loss: 0.9016655087471008, Final Batch Loss: 0.1643676459789276\n",
      "Epoch 6035, Loss: 0.729850247502327, Final Batch Loss: 0.23355454206466675\n",
      "Epoch 6036, Loss: 0.7393334358930588, Final Batch Loss: 0.15906666219234467\n",
      "Epoch 6037, Loss: 0.7813660949468613, Final Batch Loss: 0.12261559069156647\n",
      "Epoch 6038, Loss: 0.8943099826574326, Final Batch Loss: 0.16329534351825714\n",
      "Epoch 6039, Loss: 0.6789171099662781, Final Batch Loss: 0.17596609890460968\n",
      "Epoch 6040, Loss: 0.9092735648155212, Final Batch Loss: 0.3558695912361145\n",
      "Epoch 6041, Loss: 0.9027151018381119, Final Batch Loss: 0.29065409302711487\n",
      "Epoch 6042, Loss: 0.7613244950771332, Final Batch Loss: 0.18885958194732666\n",
      "Epoch 6043, Loss: 0.7888358235359192, Final Batch Loss: 0.194595605134964\n",
      "Epoch 6044, Loss: 0.6677651852369308, Final Batch Loss: 0.13844771683216095\n",
      "Epoch 6045, Loss: 0.747221514582634, Final Batch Loss: 0.20305632054805756\n",
      "Epoch 6046, Loss: 0.6455044001340866, Final Batch Loss: 0.13529813289642334\n",
      "Epoch 6047, Loss: 0.685586154460907, Final Batch Loss: 0.13030856847763062\n",
      "Epoch 6048, Loss: 0.7599478214979172, Final Batch Loss: 0.1990949660539627\n",
      "Epoch 6049, Loss: 0.8513745963573456, Final Batch Loss: 0.23932762444019318\n",
      "Epoch 6050, Loss: 0.8098209947347641, Final Batch Loss: 0.18401187658309937\n",
      "Epoch 6051, Loss: 0.8022984117269516, Final Batch Loss: 0.1754927933216095\n",
      "Epoch 6052, Loss: 0.7885689288377762, Final Batch Loss: 0.1715351641178131\n",
      "Epoch 6053, Loss: 0.7118555009365082, Final Batch Loss: 0.12665487825870514\n",
      "Epoch 6054, Loss: 0.690511628985405, Final Batch Loss: 0.22752515971660614\n",
      "Epoch 6055, Loss: 0.7705478817224503, Final Batch Loss: 0.17018863558769226\n",
      "Epoch 6056, Loss: 0.7680042088031769, Final Batch Loss: 0.21121013164520264\n",
      "Epoch 6057, Loss: 0.8534294962882996, Final Batch Loss: 0.21979397535324097\n",
      "Epoch 6058, Loss: 0.7120258063077927, Final Batch Loss: 0.25831255316734314\n",
      "Epoch 6059, Loss: 0.8472779244184494, Final Batch Loss: 0.2756139934062958\n",
      "Epoch 6060, Loss: 0.8257119208574295, Final Batch Loss: 0.19455388188362122\n",
      "Epoch 6061, Loss: 0.859471008181572, Final Batch Loss: 0.23063360154628754\n",
      "Epoch 6062, Loss: 0.9483548104763031, Final Batch Loss: 0.2282155454158783\n",
      "Epoch 6063, Loss: 0.8532209992408752, Final Batch Loss: 0.17475245893001556\n",
      "Epoch 6064, Loss: 0.9267670810222626, Final Batch Loss: 0.24674870073795319\n",
      "Epoch 6065, Loss: 0.8596640080213547, Final Batch Loss: 0.20867116749286652\n",
      "Epoch 6066, Loss: 0.7822757959365845, Final Batch Loss: 0.1808786243200302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6067, Loss: 0.7667862325906754, Final Batch Loss: 0.18980485200881958\n",
      "Epoch 6068, Loss: 0.7445026338100433, Final Batch Loss: 0.15934976935386658\n",
      "Epoch 6069, Loss: 0.7476423978805542, Final Batch Loss: 0.1712689846754074\n",
      "Epoch 6070, Loss: 0.7069366723299026, Final Batch Loss: 0.17753632366657257\n",
      "Epoch 6071, Loss: 0.7627983838319778, Final Batch Loss: 0.20129607617855072\n",
      "Epoch 6072, Loss: 0.7779199928045273, Final Batch Loss: 0.19353610277175903\n",
      "Epoch 6073, Loss: 0.783593624830246, Final Batch Loss: 0.1794586330652237\n",
      "Epoch 6074, Loss: 0.6639704555273056, Final Batch Loss: 0.181594580411911\n",
      "Epoch 6075, Loss: 0.7447777986526489, Final Batch Loss: 0.12269558012485504\n",
      "Epoch 6076, Loss: 0.7619013786315918, Final Batch Loss: 0.14876598119735718\n",
      "Epoch 6077, Loss: 0.7597556859254837, Final Batch Loss: 0.16553081572055817\n",
      "Epoch 6078, Loss: 0.8635281920433044, Final Batch Loss: 0.16998746991157532\n",
      "Epoch 6079, Loss: 0.6551471501588821, Final Batch Loss: 0.17750704288482666\n",
      "Epoch 6080, Loss: 0.6966658383607864, Final Batch Loss: 0.18512871861457825\n",
      "Epoch 6081, Loss: 0.749218538403511, Final Batch Loss: 0.137748584151268\n",
      "Epoch 6082, Loss: 0.7041706144809723, Final Batch Loss: 0.11596627533435822\n",
      "Epoch 6083, Loss: 0.6263746544718742, Final Batch Loss: 0.15382975339889526\n",
      "Epoch 6084, Loss: 0.7796930819749832, Final Batch Loss: 0.22206316888332367\n",
      "Epoch 6085, Loss: 0.8186559826135635, Final Batch Loss: 0.16114109754562378\n",
      "Epoch 6086, Loss: 0.9500003159046173, Final Batch Loss: 0.30153757333755493\n",
      "Epoch 6087, Loss: 0.7864158302545547, Final Batch Loss: 0.18634620308876038\n",
      "Epoch 6088, Loss: 0.7773493230342865, Final Batch Loss: 0.17692013084888458\n",
      "Epoch 6089, Loss: 0.7442252337932587, Final Batch Loss: 0.19160796701908112\n",
      "Epoch 6090, Loss: 0.9936972409486771, Final Batch Loss: 0.25095266103744507\n",
      "Epoch 6091, Loss: 0.6962258666753769, Final Batch Loss: 0.12171123921871185\n",
      "Epoch 6092, Loss: 0.7241534739732742, Final Batch Loss: 0.1523360162973404\n",
      "Epoch 6093, Loss: 0.6553220450878143, Final Batch Loss: 0.1576465368270874\n",
      "Epoch 6094, Loss: 0.7826351970434189, Final Batch Loss: 0.21102052927017212\n",
      "Epoch 6095, Loss: 0.7057871520519257, Final Batch Loss: 0.1621554046869278\n",
      "Epoch 6096, Loss: 0.777775913476944, Final Batch Loss: 0.22189800441265106\n",
      "Epoch 6097, Loss: 0.7669729888439178, Final Batch Loss: 0.2209719568490982\n",
      "Epoch 6098, Loss: 0.7161512821912766, Final Batch Loss: 0.1830049604177475\n",
      "Epoch 6099, Loss: 0.7798175513744354, Final Batch Loss: 0.2526680529117584\n",
      "Epoch 6100, Loss: 0.8390138000249863, Final Batch Loss: 0.25777798891067505\n",
      "Epoch 6101, Loss: 0.6535229235887527, Final Batch Loss: 0.1377778798341751\n",
      "Epoch 6102, Loss: 0.8063800781965256, Final Batch Loss: 0.19281882047653198\n",
      "Epoch 6103, Loss: 0.8669749051332474, Final Batch Loss: 0.2669181227684021\n",
      "Epoch 6104, Loss: 0.6873703896999359, Final Batch Loss: 0.18711672723293304\n",
      "Epoch 6105, Loss: 0.6340644061565399, Final Batch Loss: 0.14886713027954102\n",
      "Epoch 6106, Loss: 0.696191281080246, Final Batch Loss: 0.14401981234550476\n",
      "Epoch 6107, Loss: 0.7516081035137177, Final Batch Loss: 0.18435870110988617\n",
      "Epoch 6108, Loss: 0.7277702838182449, Final Batch Loss: 0.1473521590232849\n",
      "Epoch 6109, Loss: 0.6640901267528534, Final Batch Loss: 0.1267683207988739\n",
      "Epoch 6110, Loss: 0.6692330688238144, Final Batch Loss: 0.16060470044612885\n",
      "Epoch 6111, Loss: 0.7245678007602692, Final Batch Loss: 0.16949284076690674\n",
      "Epoch 6112, Loss: 0.7251407653093338, Final Batch Loss: 0.14799240231513977\n",
      "Epoch 6113, Loss: 0.7479119598865509, Final Batch Loss: 0.1418294459581375\n",
      "Epoch 6114, Loss: 0.6643558442592621, Final Batch Loss: 0.13985911011695862\n",
      "Epoch 6115, Loss: 0.8482671529054642, Final Batch Loss: 0.1814289689064026\n",
      "Epoch 6116, Loss: 0.7863220721483231, Final Batch Loss: 0.20592902600765228\n",
      "Epoch 6117, Loss: 0.8855434656143188, Final Batch Loss: 0.2432672530412674\n",
      "Epoch 6118, Loss: 0.8377050161361694, Final Batch Loss: 0.25582003593444824\n",
      "Epoch 6119, Loss: 0.7161826193332672, Final Batch Loss: 0.18495476245880127\n",
      "Epoch 6120, Loss: 0.7587061524391174, Final Batch Loss: 0.16070514917373657\n",
      "Epoch 6121, Loss: 0.7877482771873474, Final Batch Loss: 0.20343466103076935\n",
      "Epoch 6122, Loss: 0.7936541736125946, Final Batch Loss: 0.19757893681526184\n",
      "Epoch 6123, Loss: 0.9124350547790527, Final Batch Loss: 0.24705824255943298\n",
      "Epoch 6124, Loss: 0.8047318011522293, Final Batch Loss: 0.2003597915172577\n",
      "Epoch 6125, Loss: 0.841300368309021, Final Batch Loss: 0.2598845064640045\n",
      "Epoch 6126, Loss: 0.731353148818016, Final Batch Loss: 0.15137498080730438\n",
      "Epoch 6127, Loss: 0.8100769519805908, Final Batch Loss: 0.21188998222351074\n",
      "Epoch 6128, Loss: 0.7875489145517349, Final Batch Loss: 0.2072487771511078\n",
      "Epoch 6129, Loss: 0.7874782830476761, Final Batch Loss: 0.21564407646656036\n",
      "Epoch 6130, Loss: 0.830428957939148, Final Batch Loss: 0.22684872150421143\n",
      "Epoch 6131, Loss: 0.8536648079752922, Final Batch Loss: 0.1145855113863945\n",
      "Epoch 6132, Loss: 0.8884758204221725, Final Batch Loss: 0.24783149361610413\n",
      "Epoch 6133, Loss: 0.6878916025161743, Final Batch Loss: 0.1740204244852066\n",
      "Epoch 6134, Loss: 0.793464794754982, Final Batch Loss: 0.1633281707763672\n",
      "Epoch 6135, Loss: 0.8361914157867432, Final Batch Loss: 0.1943395882844925\n",
      "Epoch 6136, Loss: 0.7278364300727844, Final Batch Loss: 0.16637387871742249\n",
      "Epoch 6137, Loss: 0.7761227488517761, Final Batch Loss: 0.16680195927619934\n",
      "Epoch 6138, Loss: 0.7612909227609634, Final Batch Loss: 0.17641690373420715\n",
      "Epoch 6139, Loss: 0.7985309809446335, Final Batch Loss: 0.22389250993728638\n",
      "Epoch 6140, Loss: 0.8374119997024536, Final Batch Loss: 0.24514393508434296\n",
      "Epoch 6141, Loss: 0.8973260521888733, Final Batch Loss: 0.2431430220603943\n",
      "Epoch 6142, Loss: 0.8063674718141556, Final Batch Loss: 0.2739303708076477\n",
      "Epoch 6143, Loss: 0.9341039806604385, Final Batch Loss: 0.3386077582836151\n",
      "Epoch 6144, Loss: 0.7586411088705063, Final Batch Loss: 0.12287010252475739\n",
      "Epoch 6145, Loss: 0.8105226457118988, Final Batch Loss: 0.21635808050632477\n",
      "Epoch 6146, Loss: 0.8817507177591324, Final Batch Loss: 0.199763685464859\n",
      "Epoch 6147, Loss: 0.6941618770360947, Final Batch Loss: 0.13445879518985748\n",
      "Epoch 6148, Loss: 0.9400360584259033, Final Batch Loss: 0.23747463524341583\n",
      "Epoch 6149, Loss: 0.731792539358139, Final Batch Loss: 0.1647924929857254\n",
      "Epoch 6150, Loss: 0.7882125228643417, Final Batch Loss: 0.26644548773765564\n",
      "Epoch 6151, Loss: 0.8835647106170654, Final Batch Loss: 0.22464901208877563\n",
      "Epoch 6152, Loss: 0.6883257031440735, Final Batch Loss: 0.1233513206243515\n",
      "Epoch 6153, Loss: 0.7329339534044266, Final Batch Loss: 0.2002669721841812\n",
      "Epoch 6154, Loss: 0.8934153467416763, Final Batch Loss: 0.2522136867046356\n",
      "Epoch 6155, Loss: 0.7033070027828217, Final Batch Loss: 0.22187763452529907\n",
      "Epoch 6156, Loss: 0.8215276151895523, Final Batch Loss: 0.2223183810710907\n",
      "Epoch 6157, Loss: 0.7387479692697525, Final Batch Loss: 0.2155313342809677\n",
      "Epoch 6158, Loss: 0.9134974479675293, Final Batch Loss: 0.26976436376571655\n",
      "Epoch 6159, Loss: 0.747487485408783, Final Batch Loss: 0.1897018700838089\n",
      "Epoch 6160, Loss: 0.6827551573514938, Final Batch Loss: 0.2070407271385193\n",
      "Epoch 6161, Loss: 0.7100744247436523, Final Batch Loss: 0.1790381669998169\n",
      "Epoch 6162, Loss: 0.8238504081964493, Final Batch Loss: 0.2295253872871399\n",
      "Epoch 6163, Loss: 0.7361330091953278, Final Batch Loss: 0.20167309045791626\n",
      "Epoch 6164, Loss: 0.8547971099615097, Final Batch Loss: 0.20588816702365875\n",
      "Epoch 6165, Loss: 0.641652874648571, Final Batch Loss: 0.08985096961259842\n",
      "Epoch 6166, Loss: 0.7642676532268524, Final Batch Loss: 0.2020680159330368\n",
      "Epoch 6167, Loss: 0.6807356923818588, Final Batch Loss: 0.19411632418632507\n",
      "Epoch 6168, Loss: 0.5886588171124458, Final Batch Loss: 0.11761688441038132\n",
      "Epoch 6169, Loss: 0.8490383327007294, Final Batch Loss: 0.2700708210468292\n",
      "Epoch 6170, Loss: 0.7624609172344208, Final Batch Loss: 0.23193076252937317\n",
      "Epoch 6171, Loss: 0.7852631658315659, Final Batch Loss: 0.19635124504566193\n",
      "Epoch 6172, Loss: 0.807151272892952, Final Batch Loss: 0.26743924617767334\n",
      "Epoch 6173, Loss: 0.7073767259716988, Final Batch Loss: 0.11008525639772415\n",
      "Epoch 6174, Loss: 0.743847668170929, Final Batch Loss: 0.21945470571517944\n",
      "Epoch 6175, Loss: 0.6709982231259346, Final Batch Loss: 0.11093490570783615\n",
      "Epoch 6176, Loss: 0.8006664514541626, Final Batch Loss: 0.24077145755290985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6177, Loss: 0.8467466533184052, Final Batch Loss: 0.14416943490505219\n",
      "Epoch 6178, Loss: 0.8075269013643265, Final Batch Loss: 0.1662314236164093\n",
      "Epoch 6179, Loss: 0.8741419762372971, Final Batch Loss: 0.21344715356826782\n",
      "Epoch 6180, Loss: 0.7636512517929077, Final Batch Loss: 0.18234515190124512\n",
      "Epoch 6181, Loss: 0.7291973158717155, Final Batch Loss: 0.11328821629285812\n",
      "Epoch 6182, Loss: 0.6726807951927185, Final Batch Loss: 0.1388998031616211\n",
      "Epoch 6183, Loss: 0.8179906755685806, Final Batch Loss: 0.2010679841041565\n",
      "Epoch 6184, Loss: 0.8052264451980591, Final Batch Loss: 0.22768782079219818\n",
      "Epoch 6185, Loss: 0.7915882766246796, Final Batch Loss: 0.25199851393699646\n",
      "Epoch 6186, Loss: 0.7719555646181107, Final Batch Loss: 0.15776671469211578\n",
      "Epoch 6187, Loss: 0.7729245275259018, Final Batch Loss: 0.14575083553791046\n",
      "Epoch 6188, Loss: 0.7900748550891876, Final Batch Loss: 0.17501920461654663\n",
      "Epoch 6189, Loss: 0.7453624159097672, Final Batch Loss: 0.1396344006061554\n",
      "Epoch 6190, Loss: 0.784497857093811, Final Batch Loss: 0.23570935428142548\n",
      "Epoch 6191, Loss: 0.8488307148218155, Final Batch Loss: 0.18509332835674286\n",
      "Epoch 6192, Loss: 0.6917631477117538, Final Batch Loss: 0.16768045723438263\n",
      "Epoch 6193, Loss: 0.8963415771722794, Final Batch Loss: 0.23687517642974854\n",
      "Epoch 6194, Loss: 0.8771945387125015, Final Batch Loss: 0.31854867935180664\n",
      "Epoch 6195, Loss: 0.8496842235326767, Final Batch Loss: 0.18940627574920654\n",
      "Epoch 6196, Loss: 0.9038174450397491, Final Batch Loss: 0.20751559734344482\n",
      "Epoch 6197, Loss: 0.8121932148933411, Final Batch Loss: 0.184693843126297\n",
      "Epoch 6198, Loss: 0.7853255718946457, Final Batch Loss: 0.1370319277048111\n",
      "Epoch 6199, Loss: 0.8608071357011795, Final Batch Loss: 0.25853779911994934\n",
      "Epoch 6200, Loss: 0.7792478501796722, Final Batch Loss: 0.25574004650115967\n",
      "Epoch 6201, Loss: 0.8153024762868881, Final Batch Loss: 0.21652927994728088\n",
      "Epoch 6202, Loss: 0.9493344873189926, Final Batch Loss: 0.3253198564052582\n",
      "Epoch 6203, Loss: 0.6691640317440033, Final Batch Loss: 0.12748925387859344\n",
      "Epoch 6204, Loss: 0.742040365934372, Final Batch Loss: 0.1478281319141388\n",
      "Epoch 6205, Loss: 0.7229898199439049, Final Batch Loss: 0.1433015912771225\n",
      "Epoch 6206, Loss: 0.641307570040226, Final Batch Loss: 0.11791475862264633\n",
      "Epoch 6207, Loss: 0.8093221485614777, Final Batch Loss: 0.17710818350315094\n",
      "Epoch 6208, Loss: 0.7182804495096207, Final Batch Loss: 0.2010582536458969\n",
      "Epoch 6209, Loss: 0.8109764009714127, Final Batch Loss: 0.1874949038028717\n",
      "Epoch 6210, Loss: 0.7165221571922302, Final Batch Loss: 0.19374273717403412\n",
      "Epoch 6211, Loss: 0.8207525014877319, Final Batch Loss: 0.2732188105583191\n",
      "Epoch 6212, Loss: 0.7570220082998276, Final Batch Loss: 0.22942966222763062\n",
      "Epoch 6213, Loss: 0.7289921939373016, Final Batch Loss: 0.15913724899291992\n",
      "Epoch 6214, Loss: 0.7624210864305496, Final Batch Loss: 0.17358647286891937\n",
      "Epoch 6215, Loss: 0.7646710425615311, Final Batch Loss: 0.18367205560207367\n",
      "Epoch 6216, Loss: 0.6478448808193207, Final Batch Loss: 0.11737717688083649\n",
      "Epoch 6217, Loss: 0.7085250914096832, Final Batch Loss: 0.15633073449134827\n",
      "Epoch 6218, Loss: 0.6252261623740196, Final Batch Loss: 0.13295984268188477\n",
      "Epoch 6219, Loss: 0.722732201218605, Final Batch Loss: 0.19463559985160828\n",
      "Epoch 6220, Loss: 0.6809288263320923, Final Batch Loss: 0.1263185292482376\n",
      "Epoch 6221, Loss: 0.7903324514627457, Final Batch Loss: 0.25641217827796936\n",
      "Epoch 6222, Loss: 0.7329342514276505, Final Batch Loss: 0.2061702460050583\n",
      "Epoch 6223, Loss: 0.8095745593309402, Final Batch Loss: 0.23110903799533844\n",
      "Epoch 6224, Loss: 0.875339612364769, Final Batch Loss: 0.19355079531669617\n",
      "Epoch 6225, Loss: 0.9683662950992584, Final Batch Loss: 0.24183736741542816\n",
      "Epoch 6226, Loss: 0.8392922282218933, Final Batch Loss: 0.17350929975509644\n",
      "Epoch 6227, Loss: 0.8167081922292709, Final Batch Loss: 0.2663305699825287\n",
      "Epoch 6228, Loss: 0.7145012021064758, Final Batch Loss: 0.12342667579650879\n",
      "Epoch 6229, Loss: 0.6838468015193939, Final Batch Loss: 0.20469720661640167\n",
      "Epoch 6230, Loss: 0.7272309064865112, Final Batch Loss: 0.1813516765832901\n",
      "Epoch 6231, Loss: 0.7047676146030426, Final Batch Loss: 0.17186377942562103\n",
      "Epoch 6232, Loss: 0.6983911097049713, Final Batch Loss: 0.11465489864349365\n",
      "Epoch 6233, Loss: 0.7235304713249207, Final Batch Loss: 0.2271132469177246\n",
      "Epoch 6234, Loss: 0.7115055322647095, Final Batch Loss: 0.2125486433506012\n",
      "Epoch 6235, Loss: 0.8051500171422958, Final Batch Loss: 0.2523564398288727\n",
      "Epoch 6236, Loss: 0.7956771850585938, Final Batch Loss: 0.19660089910030365\n",
      "Epoch 6237, Loss: 0.9531899392604828, Final Batch Loss: 0.3807717263698578\n",
      "Epoch 6238, Loss: 0.7395693361759186, Final Batch Loss: 0.20825691521167755\n",
      "Epoch 6239, Loss: 0.8368019014596939, Final Batch Loss: 0.22337615489959717\n",
      "Epoch 6240, Loss: 0.8812296688556671, Final Batch Loss: 0.25293514132499695\n",
      "Epoch 6241, Loss: 0.7598334401845932, Final Batch Loss: 0.188658207654953\n",
      "Epoch 6242, Loss: 0.8401472866535187, Final Batch Loss: 0.21532580256462097\n",
      "Epoch 6243, Loss: 0.7528170347213745, Final Batch Loss: 0.17081691324710846\n",
      "Epoch 6244, Loss: 0.7132532149553299, Final Batch Loss: 0.15456002950668335\n",
      "Epoch 6245, Loss: 0.7806593775749207, Final Batch Loss: 0.17784039676189423\n",
      "Epoch 6246, Loss: 0.8818664401769638, Final Batch Loss: 0.22927474975585938\n",
      "Epoch 6247, Loss: 0.8046397566795349, Final Batch Loss: 0.21124571561813354\n",
      "Epoch 6248, Loss: 0.7524858564138412, Final Batch Loss: 0.16765153408050537\n",
      "Epoch 6249, Loss: 0.748962938785553, Final Batch Loss: 0.12417040765285492\n",
      "Epoch 6250, Loss: 0.7366294413805008, Final Batch Loss: 0.17381657660007477\n",
      "Epoch 6251, Loss: 0.7458913326263428, Final Batch Loss: 0.16523583233356476\n",
      "Epoch 6252, Loss: 0.7344673424959183, Final Batch Loss: 0.19335272908210754\n",
      "Epoch 6253, Loss: 0.675847053527832, Final Batch Loss: 0.21054042875766754\n",
      "Epoch 6254, Loss: 0.7180614471435547, Final Batch Loss: 0.18026506900787354\n",
      "Epoch 6255, Loss: 0.9268270581960678, Final Batch Loss: 0.2754492163658142\n",
      "Epoch 6256, Loss: 0.7637500762939453, Final Batch Loss: 0.16269062459468842\n",
      "Epoch 6257, Loss: 0.763676181435585, Final Batch Loss: 0.21976131200790405\n",
      "Epoch 6258, Loss: 0.7235063761472702, Final Batch Loss: 0.1816157102584839\n",
      "Epoch 6259, Loss: 0.8477757424116135, Final Batch Loss: 0.23905150592327118\n",
      "Epoch 6260, Loss: 0.857233926653862, Final Batch Loss: 0.16968058049678802\n",
      "Epoch 6261, Loss: 0.837799534201622, Final Batch Loss: 0.2268686145544052\n",
      "Epoch 6262, Loss: 0.7470841705799103, Final Batch Loss: 0.15515419840812683\n",
      "Epoch 6263, Loss: 0.6310017108917236, Final Batch Loss: 0.12069563567638397\n",
      "Epoch 6264, Loss: 0.7597710639238358, Final Batch Loss: 0.24307991564273834\n",
      "Epoch 6265, Loss: 0.8579531013965607, Final Batch Loss: 0.2275976687669754\n",
      "Epoch 6266, Loss: 0.8318187892436981, Final Batch Loss: 0.16371044516563416\n",
      "Epoch 6267, Loss: 0.7963597774505615, Final Batch Loss: 0.26195982098579407\n",
      "Epoch 6268, Loss: 0.7298476099967957, Final Batch Loss: 0.2406003624200821\n",
      "Epoch 6269, Loss: 0.7525798007845879, Final Batch Loss: 0.11932943016290665\n",
      "Epoch 6270, Loss: 0.8465268760919571, Final Batch Loss: 0.21106787025928497\n",
      "Epoch 6271, Loss: 0.665383368730545, Final Batch Loss: 0.1659644991159439\n",
      "Epoch 6272, Loss: 0.8054353594779968, Final Batch Loss: 0.1908232718706131\n",
      "Epoch 6273, Loss: 0.8258251398801804, Final Batch Loss: 0.20544356107711792\n",
      "Epoch 6274, Loss: 0.7633024007081985, Final Batch Loss: 0.2018764466047287\n",
      "Epoch 6275, Loss: 0.7826312929391861, Final Batch Loss: 0.23918132483959198\n",
      "Epoch 6276, Loss: 0.8917432874441147, Final Batch Loss: 0.2231256663799286\n",
      "Epoch 6277, Loss: 0.7536244839429855, Final Batch Loss: 0.16019585728645325\n",
      "Epoch 6278, Loss: 0.6753734350204468, Final Batch Loss: 0.16732241213321686\n",
      "Epoch 6279, Loss: 0.7940504848957062, Final Batch Loss: 0.23753567039966583\n",
      "Epoch 6280, Loss: 0.6591537743806839, Final Batch Loss: 0.16894516348838806\n",
      "Epoch 6281, Loss: 0.7138025164604187, Final Batch Loss: 0.1364773064851761\n",
      "Epoch 6282, Loss: 0.7984363585710526, Final Batch Loss: 0.18265940248966217\n",
      "Epoch 6283, Loss: 0.724611908197403, Final Batch Loss: 0.14752481877803802\n",
      "Epoch 6284, Loss: 0.7562620490789413, Final Batch Loss: 0.18411219120025635\n",
      "Epoch 6285, Loss: 0.8854617923498154, Final Batch Loss: 0.20779520273208618\n",
      "Epoch 6286, Loss: 0.797711506485939, Final Batch Loss: 0.23332127928733826\n",
      "Epoch 6287, Loss: 0.8112189024686813, Final Batch Loss: 0.21665988862514496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6288, Loss: 0.8593791127204895, Final Batch Loss: 0.25652992725372314\n",
      "Epoch 6289, Loss: 0.8615303039550781, Final Batch Loss: 0.15515100955963135\n",
      "Epoch 6290, Loss: 0.7359760254621506, Final Batch Loss: 0.19834865629673004\n",
      "Epoch 6291, Loss: 0.7767957001924515, Final Batch Loss: 0.16493171453475952\n",
      "Epoch 6292, Loss: 0.7964418232440948, Final Batch Loss: 0.0965292900800705\n",
      "Epoch 6293, Loss: 0.7494575157761574, Final Batch Loss: 0.19999021291732788\n",
      "Epoch 6294, Loss: 0.6728098392486572, Final Batch Loss: 0.1816202998161316\n",
      "Epoch 6295, Loss: 0.7615518718957901, Final Batch Loss: 0.2015131413936615\n",
      "Epoch 6296, Loss: 0.890863448381424, Final Batch Loss: 0.2874452471733093\n",
      "Epoch 6297, Loss: 0.77310511469841, Final Batch Loss: 0.1291133314371109\n",
      "Epoch 6298, Loss: 0.8767469376325607, Final Batch Loss: 0.19714531302452087\n",
      "Epoch 6299, Loss: 0.7643058001995087, Final Batch Loss: 0.18574339151382446\n",
      "Epoch 6300, Loss: 0.7140376716852188, Final Batch Loss: 0.19395367801189423\n",
      "Epoch 6301, Loss: 0.9785351455211639, Final Batch Loss: 0.26800698041915894\n",
      "Epoch 6302, Loss: 0.8245150297880173, Final Batch Loss: 0.13986517488956451\n",
      "Epoch 6303, Loss: 0.8242170065641403, Final Batch Loss: 0.14383257925510406\n",
      "Epoch 6304, Loss: 0.8293198645114899, Final Batch Loss: 0.2591395378112793\n",
      "Epoch 6305, Loss: 0.7315885126590729, Final Batch Loss: 0.16511350870132446\n",
      "Epoch 6306, Loss: 0.7725507169961929, Final Batch Loss: 0.1408412605524063\n",
      "Epoch 6307, Loss: 0.7864715158939362, Final Batch Loss: 0.18257474899291992\n",
      "Epoch 6308, Loss: 0.8113124668598175, Final Batch Loss: 0.2749429941177368\n",
      "Epoch 6309, Loss: 0.7670379281044006, Final Batch Loss: 0.209971621632576\n",
      "Epoch 6310, Loss: 0.6846904307603836, Final Batch Loss: 0.1260155737400055\n",
      "Epoch 6311, Loss: 0.7603222280740738, Final Batch Loss: 0.19010086357593536\n",
      "Epoch 6312, Loss: 0.7967850118875504, Final Batch Loss: 0.22694313526153564\n",
      "Epoch 6313, Loss: 0.7840137928724289, Final Batch Loss: 0.2275916337966919\n",
      "Epoch 6314, Loss: 0.7249405980110168, Final Batch Loss: 0.1801479309797287\n",
      "Epoch 6315, Loss: 0.7973089516162872, Final Batch Loss: 0.2600274980068207\n",
      "Epoch 6316, Loss: 0.76096610724926, Final Batch Loss: 0.22478227317333221\n",
      "Epoch 6317, Loss: 0.7727471143007278, Final Batch Loss: 0.21887196600437164\n",
      "Epoch 6318, Loss: 0.7949989438056946, Final Batch Loss: 0.18436847627162933\n",
      "Epoch 6319, Loss: 0.7642898559570312, Final Batch Loss: 0.17595668137073517\n",
      "Epoch 6320, Loss: 0.7995713651180267, Final Batch Loss: 0.19516894221305847\n",
      "Epoch 6321, Loss: 0.7596620619297028, Final Batch Loss: 0.13704340159893036\n",
      "Epoch 6322, Loss: 0.8633285313844681, Final Batch Loss: 0.22286540269851685\n",
      "Epoch 6323, Loss: 0.6742733865976334, Final Batch Loss: 0.14567412436008453\n",
      "Epoch 6324, Loss: 0.7684374451637268, Final Batch Loss: 0.20633475482463837\n",
      "Epoch 6325, Loss: 0.8728600144386292, Final Batch Loss: 0.23148922622203827\n",
      "Epoch 6326, Loss: 0.776260182261467, Final Batch Loss: 0.1501101702451706\n",
      "Epoch 6327, Loss: 0.7979819625616074, Final Batch Loss: 0.14932380616664886\n",
      "Epoch 6328, Loss: 0.7719108313322067, Final Batch Loss: 0.15070253610610962\n",
      "Epoch 6329, Loss: 0.7583087533712387, Final Batch Loss: 0.15070274472236633\n",
      "Epoch 6330, Loss: 0.7411804497241974, Final Batch Loss: 0.21048015356063843\n",
      "Epoch 6331, Loss: 0.9246759116649628, Final Batch Loss: 0.23701083660125732\n",
      "Epoch 6332, Loss: 0.8635749816894531, Final Batch Loss: 0.3196759521961212\n",
      "Epoch 6333, Loss: 0.788780152797699, Final Batch Loss: 0.20408505201339722\n",
      "Epoch 6334, Loss: 0.6505254805088043, Final Batch Loss: 0.17501242458820343\n",
      "Epoch 6335, Loss: 0.7989077568054199, Final Batch Loss: 0.2318858504295349\n",
      "Epoch 6336, Loss: 0.9153970628976822, Final Batch Loss: 0.1915644407272339\n",
      "Epoch 6337, Loss: 0.9507941901683807, Final Batch Loss: 0.3418715000152588\n",
      "Epoch 6338, Loss: 0.7283042520284653, Final Batch Loss: 0.12708021700382233\n",
      "Epoch 6339, Loss: 0.7801913768053055, Final Batch Loss: 0.18836423754692078\n",
      "Epoch 6340, Loss: 0.8324251174926758, Final Batch Loss: 0.2517332434654236\n",
      "Epoch 6341, Loss: 0.7191827744245529, Final Batch Loss: 0.19385212659835815\n",
      "Epoch 6342, Loss: 0.7394619733095169, Final Batch Loss: 0.26502111554145813\n",
      "Epoch 6343, Loss: 0.7308417558670044, Final Batch Loss: 0.17188827693462372\n",
      "Epoch 6344, Loss: 0.7893863916397095, Final Batch Loss: 0.208251953125\n",
      "Epoch 6345, Loss: 0.74422687292099, Final Batch Loss: 0.16660071909427643\n",
      "Epoch 6346, Loss: 0.6561274379491806, Final Batch Loss: 0.24991628527641296\n",
      "Epoch 6347, Loss: 0.786665678024292, Final Batch Loss: 0.2347661703824997\n",
      "Epoch 6348, Loss: 0.8256728053092957, Final Batch Loss: 0.20814719796180725\n",
      "Epoch 6349, Loss: 0.7222494035959244, Final Batch Loss: 0.17889176309108734\n",
      "Epoch 6350, Loss: 0.751028910279274, Final Batch Loss: 0.21071089804172516\n",
      "Epoch 6351, Loss: 0.6744068264961243, Final Batch Loss: 0.1602853685617447\n",
      "Epoch 6352, Loss: 0.8470291197299957, Final Batch Loss: 0.23064947128295898\n",
      "Epoch 6353, Loss: 0.746843159198761, Final Batch Loss: 0.19832950830459595\n",
      "Epoch 6354, Loss: 0.7506380826234818, Final Batch Loss: 0.15644443035125732\n",
      "Epoch 6355, Loss: 0.7473392635583878, Final Batch Loss: 0.20601150393486023\n",
      "Epoch 6356, Loss: 0.7707239985466003, Final Batch Loss: 0.199429452419281\n",
      "Epoch 6357, Loss: 0.7877179831266403, Final Batch Loss: 0.2046167403459549\n",
      "Epoch 6358, Loss: 0.8081260770559311, Final Batch Loss: 0.23376211524009705\n",
      "Epoch 6359, Loss: 0.655086025595665, Final Batch Loss: 0.13751088082790375\n",
      "Epoch 6360, Loss: 0.7285412847995758, Final Batch Loss: 0.19334430992603302\n",
      "Epoch 6361, Loss: 0.896156907081604, Final Batch Loss: 0.20634186267852783\n",
      "Epoch 6362, Loss: 0.6670170277357101, Final Batch Loss: 0.180425226688385\n",
      "Epoch 6363, Loss: 0.6698829829692841, Final Batch Loss: 0.17076624929904938\n",
      "Epoch 6364, Loss: 0.7796524167060852, Final Batch Loss: 0.23045045137405396\n",
      "Epoch 6365, Loss: 0.7595133036375046, Final Batch Loss: 0.2056085616350174\n",
      "Epoch 6366, Loss: 0.7429224029183388, Final Batch Loss: 0.09669951349496841\n",
      "Epoch 6367, Loss: 0.90318962931633, Final Batch Loss: 0.23708517849445343\n",
      "Epoch 6368, Loss: 0.7363656461238861, Final Batch Loss: 0.17783965170383453\n",
      "Epoch 6369, Loss: 0.7998378276824951, Final Batch Loss: 0.18382762372493744\n",
      "Epoch 6370, Loss: 0.8499480187892914, Final Batch Loss: 0.20273636281490326\n",
      "Epoch 6371, Loss: 0.7666032463312149, Final Batch Loss: 0.25603538751602173\n",
      "Epoch 6372, Loss: 0.7395347505807877, Final Batch Loss: 0.18094249069690704\n",
      "Epoch 6373, Loss: 0.7757271975278854, Final Batch Loss: 0.17405879497528076\n",
      "Epoch 6374, Loss: 0.9187823534011841, Final Batch Loss: 0.27445125579833984\n",
      "Epoch 6375, Loss: 0.7838116139173508, Final Batch Loss: 0.15176279842853546\n",
      "Epoch 6376, Loss: 0.6792481392621994, Final Batch Loss: 0.16945551335811615\n",
      "Epoch 6377, Loss: 0.6851728707551956, Final Batch Loss: 0.18571308255195618\n",
      "Epoch 6378, Loss: 0.744753435254097, Final Batch Loss: 0.15201984345912933\n",
      "Epoch 6379, Loss: 0.6436248123645782, Final Batch Loss: 0.12594856321811676\n",
      "Epoch 6380, Loss: 0.8463662713766098, Final Batch Loss: 0.2271990031003952\n",
      "Epoch 6381, Loss: 0.7371722608804703, Final Batch Loss: 0.19314493238925934\n",
      "Epoch 6382, Loss: 0.6649481803178787, Final Batch Loss: 0.12038831412792206\n",
      "Epoch 6383, Loss: 0.9155467748641968, Final Batch Loss: 0.18883375823497772\n",
      "Epoch 6384, Loss: 0.7040515094995499, Final Batch Loss: 0.15475474298000336\n",
      "Epoch 6385, Loss: 0.9554621875286102, Final Batch Loss: 0.3724916875362396\n",
      "Epoch 6386, Loss: 0.7958360463380814, Final Batch Loss: 0.24899780750274658\n",
      "Epoch 6387, Loss: 0.8120887875556946, Final Batch Loss: 0.20590977370738983\n",
      "Epoch 6388, Loss: 0.6828669458627701, Final Batch Loss: 0.08608697354793549\n",
      "Epoch 6389, Loss: 0.8502336293458939, Final Batch Loss: 0.2567223608493805\n",
      "Epoch 6390, Loss: 0.7956552803516388, Final Batch Loss: 0.20954850316047668\n",
      "Epoch 6391, Loss: 0.8547251373529434, Final Batch Loss: 0.24557457864284515\n",
      "Epoch 6392, Loss: 0.7473223954439163, Final Batch Loss: 0.20762163400650024\n",
      "Epoch 6393, Loss: 0.7467477917671204, Final Batch Loss: 0.2001442313194275\n",
      "Epoch 6394, Loss: 0.6953778117895126, Final Batch Loss: 0.2021617889404297\n",
      "Epoch 6395, Loss: 0.8914748132228851, Final Batch Loss: 0.1845945566892624\n",
      "Epoch 6396, Loss: 0.7881080657243729, Final Batch Loss: 0.23802734911441803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6397, Loss: 0.6784054338932037, Final Batch Loss: 0.1577376127243042\n",
      "Epoch 6398, Loss: 0.713779479265213, Final Batch Loss: 0.18970537185668945\n",
      "Epoch 6399, Loss: 0.6567641943693161, Final Batch Loss: 0.13988810777664185\n",
      "Epoch 6400, Loss: 0.7909910827875137, Final Batch Loss: 0.1558857560157776\n",
      "Epoch 6401, Loss: 0.7269318401813507, Final Batch Loss: 0.2183203399181366\n",
      "Epoch 6402, Loss: 0.6675922870635986, Final Batch Loss: 0.13883072137832642\n",
      "Epoch 6403, Loss: 0.871096059679985, Final Batch Loss: 0.2975158393383026\n",
      "Epoch 6404, Loss: 0.7147276997566223, Final Batch Loss: 0.1970304399728775\n",
      "Epoch 6405, Loss: 0.6354175209999084, Final Batch Loss: 0.21147121489048004\n",
      "Epoch 6406, Loss: 0.8124736696481705, Final Batch Loss: 0.20448684692382812\n",
      "Epoch 6407, Loss: 0.8574051707983017, Final Batch Loss: 0.2413206249475479\n",
      "Epoch 6408, Loss: 0.7672176510095596, Final Batch Loss: 0.16064494848251343\n",
      "Epoch 6409, Loss: 0.6610277444124222, Final Batch Loss: 0.14545752108097076\n",
      "Epoch 6410, Loss: 0.7723387777805328, Final Batch Loss: 0.13319966197013855\n",
      "Epoch 6411, Loss: 0.7200170457363129, Final Batch Loss: 0.17113853991031647\n",
      "Epoch 6412, Loss: 0.7883594632148743, Final Batch Loss: 0.19585095345973969\n",
      "Epoch 6413, Loss: 0.6846951544284821, Final Batch Loss: 0.17540568113327026\n",
      "Epoch 6414, Loss: 0.7164233177900314, Final Batch Loss: 0.15718787908554077\n",
      "Epoch 6415, Loss: 0.7134382501244545, Final Batch Loss: 0.12099725753068924\n",
      "Epoch 6416, Loss: 0.6397966668009758, Final Batch Loss: 0.11234978586435318\n",
      "Epoch 6417, Loss: 0.6588564366102219, Final Batch Loss: 0.17436879873275757\n",
      "Epoch 6418, Loss: 0.8782831579446793, Final Batch Loss: 0.3818244934082031\n",
      "Epoch 6419, Loss: 0.8001610934734344, Final Batch Loss: 0.21633879840373993\n",
      "Epoch 6420, Loss: 0.7913491725921631, Final Batch Loss: 0.23881496489048004\n",
      "Epoch 6421, Loss: 0.7456312924623489, Final Batch Loss: 0.17572563886642456\n",
      "Epoch 6422, Loss: 0.7109581232070923, Final Batch Loss: 0.19780312478542328\n",
      "Epoch 6423, Loss: 0.7522015124559402, Final Batch Loss: 0.1898261457681656\n",
      "Epoch 6424, Loss: 0.833266943693161, Final Batch Loss: 0.25885042548179626\n",
      "Epoch 6425, Loss: 0.8139490187168121, Final Batch Loss: 0.20365232229232788\n",
      "Epoch 6426, Loss: 0.7792057171463966, Final Batch Loss: 0.12486761063337326\n",
      "Epoch 6427, Loss: 0.7394975423812866, Final Batch Loss: 0.20853686332702637\n",
      "Epoch 6428, Loss: 0.7609479576349258, Final Batch Loss: 0.14809495210647583\n",
      "Epoch 6429, Loss: 0.774457573890686, Final Batch Loss: 0.3595220446586609\n",
      "Epoch 6430, Loss: 0.7199386358261108, Final Batch Loss: 0.15498633682727814\n",
      "Epoch 6431, Loss: 0.7272067666053772, Final Batch Loss: 0.15250049531459808\n",
      "Epoch 6432, Loss: 0.873916745185852, Final Batch Loss: 0.19886016845703125\n",
      "Epoch 6433, Loss: 0.7649047821760178, Final Batch Loss: 0.19506335258483887\n",
      "Epoch 6434, Loss: 0.8020382523536682, Final Batch Loss: 0.33993223309516907\n",
      "Epoch 6435, Loss: 0.6131096258759499, Final Batch Loss: 0.1226380243897438\n",
      "Epoch 6436, Loss: 0.7033287137746811, Final Batch Loss: 0.14367204904556274\n",
      "Epoch 6437, Loss: 0.7144704461097717, Final Batch Loss: 0.21109451353549957\n",
      "Epoch 6438, Loss: 0.8831538707017899, Final Batch Loss: 0.20783007144927979\n",
      "Epoch 6439, Loss: 0.9792924672365189, Final Batch Loss: 0.21764925122261047\n",
      "Epoch 6440, Loss: 0.6419698745012283, Final Batch Loss: 0.15408220887184143\n",
      "Epoch 6441, Loss: 0.7540214359760284, Final Batch Loss: 0.21147269010543823\n",
      "Epoch 6442, Loss: 0.910814493894577, Final Batch Loss: 0.19128091633319855\n",
      "Epoch 6443, Loss: 0.7882107794284821, Final Batch Loss: 0.1302759200334549\n",
      "Epoch 6444, Loss: 0.7955923676490784, Final Batch Loss: 0.19864578545093536\n",
      "Epoch 6445, Loss: 0.6586920619010925, Final Batch Loss: 0.2029343843460083\n",
      "Epoch 6446, Loss: 0.8236012160778046, Final Batch Loss: 0.2109663188457489\n",
      "Epoch 6447, Loss: 0.6899803578853607, Final Batch Loss: 0.19761882722377777\n",
      "Epoch 6448, Loss: 0.6670051962137222, Final Batch Loss: 0.15720432996749878\n",
      "Epoch 6449, Loss: 0.7599127441644669, Final Batch Loss: 0.19822761416435242\n",
      "Epoch 6450, Loss: 0.6681726276874542, Final Batch Loss: 0.19775791466236115\n",
      "Epoch 6451, Loss: 0.7434909790754318, Final Batch Loss: 0.20185789465904236\n",
      "Epoch 6452, Loss: 0.6598072350025177, Final Batch Loss: 0.14035719633102417\n",
      "Epoch 6453, Loss: 0.7050811871886253, Final Batch Loss: 0.12425961345434189\n",
      "Epoch 6454, Loss: 0.7993070930242538, Final Batch Loss: 0.20808683335781097\n",
      "Epoch 6455, Loss: 0.8210713565349579, Final Batch Loss: 0.2527516186237335\n",
      "Epoch 6456, Loss: 0.8174205720424652, Final Batch Loss: 0.2096184343099594\n",
      "Epoch 6457, Loss: 0.7675666511058807, Final Batch Loss: 0.2030603587627411\n",
      "Epoch 6458, Loss: 0.7910296320915222, Final Batch Loss: 0.20370891690254211\n",
      "Epoch 6459, Loss: 0.7301248759031296, Final Batch Loss: 0.1939784735441208\n",
      "Epoch 6460, Loss: 0.6896992772817612, Final Batch Loss: 0.1388549953699112\n",
      "Epoch 6461, Loss: 0.7650115340948105, Final Batch Loss: 0.1312737762928009\n",
      "Epoch 6462, Loss: 0.7124068140983582, Final Batch Loss: 0.16790278255939484\n",
      "Epoch 6463, Loss: 0.7750549167394638, Final Batch Loss: 0.2183428406715393\n",
      "Epoch 6464, Loss: 0.617361806333065, Final Batch Loss: 0.1837988793849945\n",
      "Epoch 6465, Loss: 0.7663417309522629, Final Batch Loss: 0.20965154469013214\n",
      "Epoch 6466, Loss: 0.7642010003328323, Final Batch Loss: 0.21242228150367737\n",
      "Epoch 6467, Loss: 0.7011621743440628, Final Batch Loss: 0.2136346846818924\n",
      "Epoch 6468, Loss: 0.7131757587194443, Final Batch Loss: 0.1333996057510376\n",
      "Epoch 6469, Loss: 0.6585558503866196, Final Batch Loss: 0.13434341549873352\n",
      "Epoch 6470, Loss: 0.830794483423233, Final Batch Loss: 0.18730224668979645\n",
      "Epoch 6471, Loss: 0.8540214896202087, Final Batch Loss: 0.23019950091838837\n",
      "Epoch 6472, Loss: 0.8066718131303787, Final Batch Loss: 0.14050447940826416\n",
      "Epoch 6473, Loss: 0.7461621463298798, Final Batch Loss: 0.13450320065021515\n",
      "Epoch 6474, Loss: 0.7863134145736694, Final Batch Loss: 0.2478533536195755\n",
      "Epoch 6475, Loss: 0.7567626088857651, Final Batch Loss: 0.20640580356121063\n",
      "Epoch 6476, Loss: 0.7652553170919418, Final Batch Loss: 0.19891361892223358\n",
      "Epoch 6477, Loss: 0.7792947739362717, Final Batch Loss: 0.2665439248085022\n",
      "Epoch 6478, Loss: 0.7585970759391785, Final Batch Loss: 0.21424821019172668\n",
      "Epoch 6479, Loss: 0.7940795123577118, Final Batch Loss: 0.16337324678897858\n",
      "Epoch 6480, Loss: 0.6921535581350327, Final Batch Loss: 0.11265227198600769\n",
      "Epoch 6481, Loss: 0.7212522029876709, Final Batch Loss: 0.267083078622818\n",
      "Epoch 6482, Loss: 0.742232009768486, Final Batch Loss: 0.24248851835727692\n",
      "Epoch 6483, Loss: 0.7530954629182816, Final Batch Loss: 0.20925432443618774\n",
      "Epoch 6484, Loss: 0.7483445554971695, Final Batch Loss: 0.21604005992412567\n",
      "Epoch 6485, Loss: 0.615480400621891, Final Batch Loss: 0.21563075482845306\n",
      "Epoch 6486, Loss: 0.7661702409386635, Final Batch Loss: 0.11394859105348587\n",
      "Epoch 6487, Loss: 0.7647193819284439, Final Batch Loss: 0.16611038148403168\n",
      "Epoch 6488, Loss: 0.7732420116662979, Final Batch Loss: 0.20110394060611725\n",
      "Epoch 6489, Loss: 0.6547452509403229, Final Batch Loss: 0.18031221628189087\n",
      "Epoch 6490, Loss: 0.8560049533843994, Final Batch Loss: 0.28582829236984253\n",
      "Epoch 6491, Loss: 0.7839063704013824, Final Batch Loss: 0.16204051673412323\n",
      "Epoch 6492, Loss: 0.6883293688297272, Final Batch Loss: 0.14238616824150085\n",
      "Epoch 6493, Loss: 0.7735451757907867, Final Batch Loss: 0.22573253512382507\n",
      "Epoch 6494, Loss: 0.8225458413362503, Final Batch Loss: 0.18526098132133484\n",
      "Epoch 6495, Loss: 0.6465905755758286, Final Batch Loss: 0.13436520099639893\n",
      "Epoch 6496, Loss: 0.6564946621656418, Final Batch Loss: 0.1407802402973175\n",
      "Epoch 6497, Loss: 0.69325290620327, Final Batch Loss: 0.1924477219581604\n",
      "Epoch 6498, Loss: 0.9584720730781555, Final Batch Loss: 0.32447436451911926\n",
      "Epoch 6499, Loss: 0.6925956457853317, Final Batch Loss: 0.18415631353855133\n",
      "Epoch 6500, Loss: 0.8675417900085449, Final Batch Loss: 0.2144753336906433\n",
      "Epoch 6501, Loss: 0.8088469803333282, Final Batch Loss: 0.16831259429454803\n",
      "Epoch 6502, Loss: 0.8529977798461914, Final Batch Loss: 0.22982680797576904\n",
      "Epoch 6503, Loss: 0.775998517870903, Final Batch Loss: 0.22718113660812378\n",
      "Epoch 6504, Loss: 0.9254664182662964, Final Batch Loss: 0.2515895664691925\n",
      "Epoch 6505, Loss: 0.917010635137558, Final Batch Loss: 0.23179805278778076\n",
      "Epoch 6506, Loss: 0.6435196250677109, Final Batch Loss: 0.15376748144626617\n",
      "Epoch 6507, Loss: 0.8028857409954071, Final Batch Loss: 0.24890632927417755\n",
      "Epoch 6508, Loss: 0.9079538285732269, Final Batch Loss: 0.3170284032821655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6509, Loss: 0.7349295169115067, Final Batch Loss: 0.18665246665477753\n",
      "Epoch 6510, Loss: 0.8027981668710709, Final Batch Loss: 0.1421719789505005\n",
      "Epoch 6511, Loss: 0.7929201722145081, Final Batch Loss: 0.1817849576473236\n",
      "Epoch 6512, Loss: 0.8496350049972534, Final Batch Loss: 0.22987009584903717\n",
      "Epoch 6513, Loss: 0.819029226899147, Final Batch Loss: 0.28883978724479675\n",
      "Epoch 6514, Loss: 0.736895352602005, Final Batch Loss: 0.1768893599510193\n",
      "Epoch 6515, Loss: 0.6513821557164192, Final Batch Loss: 0.10206408053636551\n",
      "Epoch 6516, Loss: 0.677296593785286, Final Batch Loss: 0.15866531431674957\n",
      "Epoch 6517, Loss: 0.7238472402095795, Final Batch Loss: 0.1482163965702057\n",
      "Epoch 6518, Loss: 0.768963560461998, Final Batch Loss: 0.2539401650428772\n",
      "Epoch 6519, Loss: 0.7760075926780701, Final Batch Loss: 0.14364410936832428\n",
      "Epoch 6520, Loss: 0.7296218574047089, Final Batch Loss: 0.2347755879163742\n",
      "Epoch 6521, Loss: 0.7458667457103729, Final Batch Loss: 0.12052404880523682\n",
      "Epoch 6522, Loss: 0.8081584721803665, Final Batch Loss: 0.21369192004203796\n",
      "Epoch 6523, Loss: 0.6945433914661407, Final Batch Loss: 0.19194243848323822\n",
      "Epoch 6524, Loss: 0.750667616724968, Final Batch Loss: 0.1892254799604416\n",
      "Epoch 6525, Loss: 0.6902260929346085, Final Batch Loss: 0.13386256992816925\n",
      "Epoch 6526, Loss: 0.7136844918131828, Final Batch Loss: 0.23397673666477203\n",
      "Epoch 6527, Loss: 0.6697277650237083, Final Batch Loss: 0.17747358977794647\n",
      "Epoch 6528, Loss: 0.6600870937108994, Final Batch Loss: 0.1590128093957901\n",
      "Epoch 6529, Loss: 0.7564451545476913, Final Batch Loss: 0.234834685921669\n",
      "Epoch 6530, Loss: 0.7436066418886185, Final Batch Loss: 0.20189480483531952\n",
      "Epoch 6531, Loss: 0.8926176279783249, Final Batch Loss: 0.2384006381034851\n",
      "Epoch 6532, Loss: 0.715599000453949, Final Batch Loss: 0.14234265685081482\n",
      "Epoch 6533, Loss: 0.7533173710107803, Final Batch Loss: 0.15553723275661469\n",
      "Epoch 6534, Loss: 0.8368685096502304, Final Batch Loss: 0.2453249841928482\n",
      "Epoch 6535, Loss: 0.7676177024841309, Final Batch Loss: 0.15424251556396484\n",
      "Epoch 6536, Loss: 0.8160735368728638, Final Batch Loss: 0.14022523164749146\n",
      "Epoch 6537, Loss: 0.6696013063192368, Final Batch Loss: 0.18099889159202576\n",
      "Epoch 6538, Loss: 0.6879006773233414, Final Batch Loss: 0.1861296445131302\n",
      "Epoch 6539, Loss: 0.7696501165628433, Final Batch Loss: 0.13927079737186432\n",
      "Epoch 6540, Loss: 0.867611438035965, Final Batch Loss: 0.25871357321739197\n",
      "Epoch 6541, Loss: 0.7615097910165787, Final Batch Loss: 0.1677173525094986\n",
      "Epoch 6542, Loss: 0.672719344496727, Final Batch Loss: 0.15785126388072968\n",
      "Epoch 6543, Loss: 0.7157634794712067, Final Batch Loss: 0.14880044758319855\n",
      "Epoch 6544, Loss: 0.7634296044707298, Final Batch Loss: 0.12283863872289658\n",
      "Epoch 6545, Loss: 0.738486647605896, Final Batch Loss: 0.23622599244117737\n",
      "Epoch 6546, Loss: 0.6797934174537659, Final Batch Loss: 0.16482748091220856\n",
      "Epoch 6547, Loss: 0.6180180460214615, Final Batch Loss: 0.1767759472131729\n",
      "Epoch 6548, Loss: 0.7370581179857254, Final Batch Loss: 0.192361518740654\n",
      "Epoch 6549, Loss: 0.8174163997173309, Final Batch Loss: 0.20642127096652985\n",
      "Epoch 6550, Loss: 0.9442452639341354, Final Batch Loss: 0.31207185983657837\n",
      "Epoch 6551, Loss: 0.6825077682733536, Final Batch Loss: 0.135994091629982\n",
      "Epoch 6552, Loss: 0.7858572751283646, Final Batch Loss: 0.19950507581233978\n",
      "Epoch 6553, Loss: 0.8370256870985031, Final Batch Loss: 0.15288807451725006\n",
      "Epoch 6554, Loss: 0.8158873170614243, Final Batch Loss: 0.1679910272359848\n",
      "Epoch 6555, Loss: 0.7518597990274429, Final Batch Loss: 0.24859696626663208\n",
      "Epoch 6556, Loss: 0.7956257164478302, Final Batch Loss: 0.1550361067056656\n",
      "Epoch 6557, Loss: 0.8033875823020935, Final Batch Loss: 0.17498992383480072\n",
      "Epoch 6558, Loss: 0.7557187378406525, Final Batch Loss: 0.1481868475675583\n",
      "Epoch 6559, Loss: 0.733808845281601, Final Batch Loss: 0.1919756829738617\n",
      "Epoch 6560, Loss: 0.702120840549469, Final Batch Loss: 0.16330060362815857\n",
      "Epoch 6561, Loss: 0.7547650039196014, Final Batch Loss: 0.18745297193527222\n",
      "Epoch 6562, Loss: 0.711039587855339, Final Batch Loss: 0.16670472919940948\n",
      "Epoch 6563, Loss: 0.759789377450943, Final Batch Loss: 0.22365255653858185\n",
      "Epoch 6564, Loss: 0.6979328840970993, Final Batch Loss: 0.21995317935943604\n",
      "Epoch 6565, Loss: 0.705719530582428, Final Batch Loss: 0.19458676874637604\n",
      "Epoch 6566, Loss: 0.7153757363557816, Final Batch Loss: 0.1851409524679184\n",
      "Epoch 6567, Loss: 0.6976497620344162, Final Batch Loss: 0.20377278327941895\n",
      "Epoch 6568, Loss: 0.6942173987627029, Final Batch Loss: 0.167610302567482\n",
      "Epoch 6569, Loss: 0.8083181083202362, Final Batch Loss: 0.20947198569774628\n",
      "Epoch 6570, Loss: 0.6867963522672653, Final Batch Loss: 0.17324087023735046\n",
      "Epoch 6571, Loss: 0.733828067779541, Final Batch Loss: 0.22767913341522217\n",
      "Epoch 6572, Loss: 0.6820463538169861, Final Batch Loss: 0.12805362045764923\n",
      "Epoch 6573, Loss: 0.7139547169208527, Final Batch Loss: 0.15334507822990417\n",
      "Epoch 6574, Loss: 0.6526474133133888, Final Batch Loss: 0.1359766572713852\n",
      "Epoch 6575, Loss: 0.7069072127342224, Final Batch Loss: 0.19993901252746582\n",
      "Epoch 6576, Loss: 0.7616325318813324, Final Batch Loss: 0.12837010622024536\n",
      "Epoch 6577, Loss: 0.715082049369812, Final Batch Loss: 0.2707013785839081\n",
      "Epoch 6578, Loss: 0.7808453738689423, Final Batch Loss: 0.20434291660785675\n",
      "Epoch 6579, Loss: 0.7116694152355194, Final Batch Loss: 0.13161230087280273\n",
      "Epoch 6580, Loss: 0.57523113489151, Final Batch Loss: 0.14319650828838348\n",
      "Epoch 6581, Loss: 0.7531695663928986, Final Batch Loss: 0.16968263685703278\n",
      "Epoch 6582, Loss: 0.8465259373188019, Final Batch Loss: 0.2735975384712219\n",
      "Epoch 6583, Loss: 0.8045905083417892, Final Batch Loss: 0.26624390482902527\n",
      "Epoch 6584, Loss: 0.8053139895200729, Final Batch Loss: 0.2183735966682434\n",
      "Epoch 6585, Loss: 0.8111027628183365, Final Batch Loss: 0.22609524428844452\n",
      "Epoch 6586, Loss: 0.7949380725622177, Final Batch Loss: 0.1686353087425232\n",
      "Epoch 6587, Loss: 0.7715449780225754, Final Batch Loss: 0.17124803364276886\n",
      "Epoch 6588, Loss: 0.8207849413156509, Final Batch Loss: 0.23468895256519318\n",
      "Epoch 6589, Loss: 0.9320888668298721, Final Batch Loss: 0.24280723929405212\n",
      "Epoch 6590, Loss: 0.7188971638679504, Final Batch Loss: 0.14416581392288208\n",
      "Epoch 6591, Loss: 0.7189495861530304, Final Batch Loss: 0.1919737160205841\n",
      "Epoch 6592, Loss: 0.731319397687912, Final Batch Loss: 0.14060287177562714\n",
      "Epoch 6593, Loss: 0.7399934828281403, Final Batch Loss: 0.20287998020648956\n",
      "Epoch 6594, Loss: 0.9551139324903488, Final Batch Loss: 0.3986338973045349\n",
      "Epoch 6595, Loss: 0.9933241978287697, Final Batch Loss: 0.38236507773399353\n",
      "Epoch 6596, Loss: 0.8339644074440002, Final Batch Loss: 0.1973360776901245\n",
      "Epoch 6597, Loss: 0.7628037482500076, Final Batch Loss: 0.18824759125709534\n",
      "Epoch 6598, Loss: 0.7373106330633163, Final Batch Loss: 0.18200264871120453\n",
      "Epoch 6599, Loss: 0.7551204562187195, Final Batch Loss: 0.20721200108528137\n",
      "Epoch 6600, Loss: 0.781595379114151, Final Batch Loss: 0.19087745249271393\n",
      "Epoch 6601, Loss: 0.8124419152736664, Final Batch Loss: 0.20022718608379364\n",
      "Epoch 6602, Loss: 0.7581003904342651, Final Batch Loss: 0.2089073210954666\n",
      "Epoch 6603, Loss: 0.8426730036735535, Final Batch Loss: 0.2393997460603714\n",
      "Epoch 6604, Loss: 0.7441980242729187, Final Batch Loss: 0.28340834379196167\n",
      "Epoch 6605, Loss: 0.8205280750989914, Final Batch Loss: 0.14616617560386658\n",
      "Epoch 6606, Loss: 0.7785802334547043, Final Batch Loss: 0.28559666872024536\n",
      "Epoch 6607, Loss: 0.8154965043067932, Final Batch Loss: 0.17443670332431793\n",
      "Epoch 6608, Loss: 0.749459370970726, Final Batch Loss: 0.1972806304693222\n",
      "Epoch 6609, Loss: 0.6493990123271942, Final Batch Loss: 0.13695354759693146\n",
      "Epoch 6610, Loss: 0.7432780116796494, Final Batch Loss: 0.14882124960422516\n",
      "Epoch 6611, Loss: 0.8561424612998962, Final Batch Loss: 0.22785693407058716\n",
      "Epoch 6612, Loss: 0.7647661417722702, Final Batch Loss: 0.15464648604393005\n",
      "Epoch 6613, Loss: 0.7405456900596619, Final Batch Loss: 0.19946062564849854\n",
      "Epoch 6614, Loss: 0.7467762231826782, Final Batch Loss: 0.1124168336391449\n",
      "Epoch 6615, Loss: 0.8071280866861343, Final Batch Loss: 0.20326703786849976\n",
      "Epoch 6616, Loss: 0.7145562022924423, Final Batch Loss: 0.19086940586566925\n",
      "Epoch 6617, Loss: 0.7470914423465729, Final Batch Loss: 0.24399417638778687\n",
      "Epoch 6618, Loss: 0.6706327795982361, Final Batch Loss: 0.15741942822933197\n",
      "Epoch 6619, Loss: 0.715784952044487, Final Batch Loss: 0.1708448827266693\n",
      "Epoch 6620, Loss: 0.7644151747226715, Final Batch Loss: 0.1817195862531662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6621, Loss: 0.781628206372261, Final Batch Loss: 0.2338714748620987\n",
      "Epoch 6622, Loss: 0.7179530113935471, Final Batch Loss: 0.1706380397081375\n",
      "Epoch 6623, Loss: 0.7344203442335129, Final Batch Loss: 0.2564241886138916\n",
      "Epoch 6624, Loss: 0.7358625084161758, Final Batch Loss: 0.16913610696792603\n",
      "Epoch 6625, Loss: 0.7209799587726593, Final Batch Loss: 0.13565249741077423\n",
      "Epoch 6626, Loss: 0.7504430264234543, Final Batch Loss: 0.15387551486492157\n",
      "Epoch 6627, Loss: 0.6617420613765717, Final Batch Loss: 0.17648454010486603\n",
      "Epoch 6628, Loss: 0.6392239406704903, Final Batch Loss: 0.10907823592424393\n",
      "Epoch 6629, Loss: 0.588720940053463, Final Batch Loss: 0.1484937071800232\n",
      "Epoch 6630, Loss: 0.7909598350524902, Final Batch Loss: 0.25380536913871765\n",
      "Epoch 6631, Loss: 0.8155106157064438, Final Batch Loss: 0.17577889561653137\n",
      "Epoch 6632, Loss: 0.6766783595085144, Final Batch Loss: 0.19818003475666046\n",
      "Epoch 6633, Loss: 0.7341757863759995, Final Batch Loss: 0.23962371051311493\n",
      "Epoch 6634, Loss: 0.6642762720584869, Final Batch Loss: 0.16825798153877258\n",
      "Epoch 6635, Loss: 0.705244317650795, Final Batch Loss: 0.16838887333869934\n",
      "Epoch 6636, Loss: 0.6925093084573746, Final Batch Loss: 0.1845337599515915\n",
      "Epoch 6637, Loss: 0.6336431801319122, Final Batch Loss: 0.14473485946655273\n",
      "Epoch 6638, Loss: 0.8369573503732681, Final Batch Loss: 0.19931717216968536\n",
      "Epoch 6639, Loss: 0.8424621522426605, Final Batch Loss: 0.20252056419849396\n",
      "Epoch 6640, Loss: 0.8632104247808456, Final Batch Loss: 0.1726721078157425\n",
      "Epoch 6641, Loss: 0.7619757950305939, Final Batch Loss: 0.1830320656299591\n",
      "Epoch 6642, Loss: 0.7162446975708008, Final Batch Loss: 0.178809255361557\n",
      "Epoch 6643, Loss: 0.7120627164840698, Final Batch Loss: 0.18027162551879883\n",
      "Epoch 6644, Loss: 0.7597980052232742, Final Batch Loss: 0.2199135422706604\n",
      "Epoch 6645, Loss: 0.6658524721860886, Final Batch Loss: 0.17256441712379456\n",
      "Epoch 6646, Loss: 0.6480491533875465, Final Batch Loss: 0.16719724237918854\n",
      "Epoch 6647, Loss: 0.6974370628595352, Final Batch Loss: 0.14532089233398438\n",
      "Epoch 6648, Loss: 0.7954451739788055, Final Batch Loss: 0.15258079767227173\n",
      "Epoch 6649, Loss: 0.760531559586525, Final Batch Loss: 0.16783507168293\n",
      "Epoch 6650, Loss: 0.7864845097064972, Final Batch Loss: 0.2026808112859726\n",
      "Epoch 6651, Loss: 0.8285893797874451, Final Batch Loss: 0.16548632085323334\n",
      "Epoch 6652, Loss: 0.8197585344314575, Final Batch Loss: 0.23099301755428314\n",
      "Epoch 6653, Loss: 0.7352824658155441, Final Batch Loss: 0.1533333659172058\n",
      "Epoch 6654, Loss: 0.7519805133342743, Final Batch Loss: 0.14050722122192383\n",
      "Epoch 6655, Loss: 0.6560617834329605, Final Batch Loss: 0.1569242924451828\n",
      "Epoch 6656, Loss: 0.6503504514694214, Final Batch Loss: 0.18540193140506744\n",
      "Epoch 6657, Loss: 0.760610356926918, Final Batch Loss: 0.16477926075458527\n",
      "Epoch 6658, Loss: 0.8112239688634872, Final Batch Loss: 0.1721767634153366\n",
      "Epoch 6659, Loss: 0.7876713275909424, Final Batch Loss: 0.2366059124469757\n",
      "Epoch 6660, Loss: 0.751144140958786, Final Batch Loss: 0.17477528750896454\n",
      "Epoch 6661, Loss: 0.6601964980363846, Final Batch Loss: 0.16123035550117493\n",
      "Epoch 6662, Loss: 0.8072881400585175, Final Batch Loss: 0.26409244537353516\n",
      "Epoch 6663, Loss: 0.6818380355834961, Final Batch Loss: 0.141615629196167\n",
      "Epoch 6664, Loss: 0.7039986401796341, Final Batch Loss: 0.15996351838111877\n",
      "Epoch 6665, Loss: 0.7125036269426346, Final Batch Loss: 0.16747340559959412\n",
      "Epoch 6666, Loss: 0.8088769018650055, Final Batch Loss: 0.24757899343967438\n",
      "Epoch 6667, Loss: 0.753100261092186, Final Batch Loss: 0.15477503836154938\n",
      "Epoch 6668, Loss: 0.7889842987060547, Final Batch Loss: 0.16052143275737762\n",
      "Epoch 6669, Loss: 0.8099900484085083, Final Batch Loss: 0.19426806271076202\n",
      "Epoch 6670, Loss: 0.7404309809207916, Final Batch Loss: 0.24346059560775757\n",
      "Epoch 6671, Loss: 0.7470998913049698, Final Batch Loss: 0.2661314010620117\n",
      "Epoch 6672, Loss: 0.6970949769020081, Final Batch Loss: 0.15806590020656586\n",
      "Epoch 6673, Loss: 0.720422625541687, Final Batch Loss: 0.19364279508590698\n",
      "Epoch 6674, Loss: 0.8124684393405914, Final Batch Loss: 0.1816779524087906\n",
      "Epoch 6675, Loss: 0.6970504820346832, Final Batch Loss: 0.20410315692424774\n",
      "Epoch 6676, Loss: 0.8337592631578445, Final Batch Loss: 0.14815247058868408\n",
      "Epoch 6677, Loss: 0.7149300500750542, Final Batch Loss: 0.19010697305202484\n",
      "Epoch 6678, Loss: 0.676233634352684, Final Batch Loss: 0.18894411623477936\n",
      "Epoch 6679, Loss: 0.7482307404279709, Final Batch Loss: 0.11136171221733093\n",
      "Epoch 6680, Loss: 0.7713517099618912, Final Batch Loss: 0.14316044747829437\n",
      "Epoch 6681, Loss: 0.6881918758153915, Final Batch Loss: 0.14634226262569427\n",
      "Epoch 6682, Loss: 0.7949036210775375, Final Batch Loss: 0.200962632894516\n",
      "Epoch 6683, Loss: 0.697647213935852, Final Batch Loss: 0.18224921822547913\n",
      "Epoch 6684, Loss: 0.614791989326477, Final Batch Loss: 0.16680046916007996\n",
      "Epoch 6685, Loss: 0.7689449489116669, Final Batch Loss: 0.21862109005451202\n",
      "Epoch 6686, Loss: 0.7146442458033562, Final Batch Loss: 0.11854854971170425\n",
      "Epoch 6687, Loss: 0.7940838038921356, Final Batch Loss: 0.17375144362449646\n",
      "Epoch 6688, Loss: 0.7569620013237, Final Batch Loss: 0.1394907683134079\n",
      "Epoch 6689, Loss: 0.927958607673645, Final Batch Loss: 0.21825048327445984\n",
      "Epoch 6690, Loss: 0.7920405119657516, Final Batch Loss: 0.16697005927562714\n",
      "Epoch 6691, Loss: 0.6967044919729233, Final Batch Loss: 0.18499720096588135\n",
      "Epoch 6692, Loss: 0.6614611744880676, Final Batch Loss: 0.20879647135734558\n",
      "Epoch 6693, Loss: 0.770528569817543, Final Batch Loss: 0.16803504526615143\n",
      "Epoch 6694, Loss: 0.8659424930810928, Final Batch Loss: 0.14294634759426117\n",
      "Epoch 6695, Loss: 0.7700864672660828, Final Batch Loss: 0.15291354060173035\n",
      "Epoch 6696, Loss: 0.76078000664711, Final Batch Loss: 0.15480943024158478\n",
      "Epoch 6697, Loss: 0.738934725522995, Final Batch Loss: 0.11455875635147095\n",
      "Epoch 6698, Loss: 0.6986823976039886, Final Batch Loss: 0.1627630889415741\n",
      "Epoch 6699, Loss: 0.6910935640335083, Final Batch Loss: 0.21189051866531372\n",
      "Epoch 6700, Loss: 0.7311102002859116, Final Batch Loss: 0.21420694887638092\n",
      "Epoch 6701, Loss: 0.682535246014595, Final Batch Loss: 0.16791120171546936\n",
      "Epoch 6702, Loss: 0.7690265029668808, Final Batch Loss: 0.15481865406036377\n",
      "Epoch 6703, Loss: 0.842451736330986, Final Batch Loss: 0.1855035126209259\n",
      "Epoch 6704, Loss: 0.7503256350755692, Final Batch Loss: 0.23657949268817902\n",
      "Epoch 6705, Loss: 0.6470548659563065, Final Batch Loss: 0.21637172996997833\n",
      "Epoch 6706, Loss: 0.8939082324504852, Final Batch Loss: 0.20954184234142303\n",
      "Epoch 6707, Loss: 0.7885685935616493, Final Batch Loss: 0.17994627356529236\n",
      "Epoch 6708, Loss: 0.7362867444753647, Final Batch Loss: 0.19546791911125183\n",
      "Epoch 6709, Loss: 0.7181870490312576, Final Batch Loss: 0.17143714427947998\n",
      "Epoch 6710, Loss: 0.7398644834756851, Final Batch Loss: 0.2082802653312683\n",
      "Epoch 6711, Loss: 0.9039899259805679, Final Batch Loss: 0.282686322927475\n",
      "Epoch 6712, Loss: 0.8092867583036423, Final Batch Loss: 0.20554789900779724\n",
      "Epoch 6713, Loss: 0.8002768754959106, Final Batch Loss: 0.17520344257354736\n",
      "Epoch 6714, Loss: 0.6950653493404388, Final Batch Loss: 0.11783914268016815\n",
      "Epoch 6715, Loss: 0.9136820584535599, Final Batch Loss: 0.21660272777080536\n",
      "Epoch 6716, Loss: 0.7983076274394989, Final Batch Loss: 0.1775190383195877\n",
      "Epoch 6717, Loss: 0.7606336921453476, Final Batch Loss: 0.22615467011928558\n",
      "Epoch 6718, Loss: 0.6484152302145958, Final Batch Loss: 0.1325504332780838\n",
      "Epoch 6719, Loss: 0.7635011076927185, Final Batch Loss: 0.15947869420051575\n",
      "Epoch 6720, Loss: 0.7388597279787064, Final Batch Loss: 0.16827765107154846\n",
      "Epoch 6721, Loss: 0.8445038944482803, Final Batch Loss: 0.2817586064338684\n",
      "Epoch 6722, Loss: 0.7958519905805588, Final Batch Loss: 0.18865728378295898\n",
      "Epoch 6723, Loss: 0.6764297485351562, Final Batch Loss: 0.09420804679393768\n",
      "Epoch 6724, Loss: 0.7308773249387741, Final Batch Loss: 0.13243912160396576\n",
      "Epoch 6725, Loss: 0.7514734119176865, Final Batch Loss: 0.21224987506866455\n",
      "Epoch 6726, Loss: 0.6804883927106857, Final Batch Loss: 0.1424802541732788\n",
      "Epoch 6727, Loss: 0.6835548132658005, Final Batch Loss: 0.14713054895401\n",
      "Epoch 6728, Loss: 0.780813917517662, Final Batch Loss: 0.16571015119552612\n",
      "Epoch 6729, Loss: 0.692464679479599, Final Batch Loss: 0.18842832744121552\n",
      "Epoch 6730, Loss: 0.7887247204780579, Final Batch Loss: 0.15863920748233795\n",
      "Epoch 6731, Loss: 0.7819599211215973, Final Batch Loss: 0.18627367913722992\n",
      "Epoch 6732, Loss: 0.6444894224405289, Final Batch Loss: 0.13486135005950928\n",
      "Epoch 6733, Loss: 0.6438529938459396, Final Batch Loss: 0.13980521261692047\n",
      "Epoch 6734, Loss: 0.6547536700963974, Final Batch Loss: 0.1598799079656601\n",
      "Epoch 6735, Loss: 0.7361743003129959, Final Batch Loss: 0.17111711204051971\n",
      "Epoch 6736, Loss: 0.7451611161231995, Final Batch Loss: 0.22028468549251556\n",
      "Epoch 6737, Loss: 0.6572066396474838, Final Batch Loss: 0.15322346985340118\n",
      "Epoch 6738, Loss: 0.7097940295934677, Final Batch Loss: 0.22098205983638763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6739, Loss: 0.7966529279947281, Final Batch Loss: 0.166798397898674\n",
      "Epoch 6740, Loss: 0.6679857969284058, Final Batch Loss: 0.23782631754875183\n",
      "Epoch 6741, Loss: 0.678946316242218, Final Batch Loss: 0.20099866390228271\n",
      "Epoch 6742, Loss: 0.6421711966395378, Final Batch Loss: 0.17331647872924805\n",
      "Epoch 6743, Loss: 0.660998210310936, Final Batch Loss: 0.1657390296459198\n",
      "Epoch 6744, Loss: 0.6747636646032333, Final Batch Loss: 0.15749850869178772\n",
      "Epoch 6745, Loss: 0.6160396635532379, Final Batch Loss: 0.12975065410137177\n",
      "Epoch 6746, Loss: 0.7634470462799072, Final Batch Loss: 0.1606220304965973\n",
      "Epoch 6747, Loss: 0.8136883527040482, Final Batch Loss: 0.17341414093971252\n",
      "Epoch 6748, Loss: 0.70548115670681, Final Batch Loss: 0.21008580923080444\n",
      "Epoch 6749, Loss: 0.8421014547348022, Final Batch Loss: 0.21819211542606354\n",
      "Epoch 6750, Loss: 0.6788303703069687, Final Batch Loss: 0.17272239923477173\n",
      "Epoch 6751, Loss: 0.7971309423446655, Final Batch Loss: 0.20534846186637878\n",
      "Epoch 6752, Loss: 0.7629185616970062, Final Batch Loss: 0.16890119016170502\n",
      "Epoch 6753, Loss: 0.7311854958534241, Final Batch Loss: 0.22224023938179016\n",
      "Epoch 6754, Loss: 0.6273548007011414, Final Batch Loss: 0.09607569873332977\n",
      "Epoch 6755, Loss: 0.698286235332489, Final Batch Loss: 0.1677645444869995\n",
      "Epoch 6756, Loss: 0.7493148744106293, Final Batch Loss: 0.19573381543159485\n",
      "Epoch 6757, Loss: 0.7498945891857147, Final Batch Loss: 0.17780211567878723\n",
      "Epoch 6758, Loss: 0.7141354531049728, Final Batch Loss: 0.18289487063884735\n",
      "Epoch 6759, Loss: 0.7876425683498383, Final Batch Loss: 0.13638652861118317\n",
      "Epoch 6760, Loss: 0.8427289724349976, Final Batch Loss: 0.21301032602787018\n",
      "Epoch 6761, Loss: 0.7011348754167557, Final Batch Loss: 0.121848464012146\n",
      "Epoch 6762, Loss: 0.6168759018182755, Final Batch Loss: 0.1236337274312973\n",
      "Epoch 6763, Loss: 0.7648228853940964, Final Batch Loss: 0.16169825196266174\n",
      "Epoch 6764, Loss: 0.7290418595075607, Final Batch Loss: 0.1583358496427536\n",
      "Epoch 6765, Loss: 0.6859729588031769, Final Batch Loss: 0.1867889165878296\n",
      "Epoch 6766, Loss: 0.6927972733974457, Final Batch Loss: 0.1413632035255432\n",
      "Epoch 6767, Loss: 0.7159806042909622, Final Batch Loss: 0.1721014678478241\n",
      "Epoch 6768, Loss: 0.7808891087770462, Final Batch Loss: 0.14668796956539154\n",
      "Epoch 6769, Loss: 0.7095460593700409, Final Batch Loss: 0.13704964518547058\n",
      "Epoch 6770, Loss: 0.7799765020608902, Final Batch Loss: 0.26280367374420166\n",
      "Epoch 6771, Loss: 0.8172406554222107, Final Batch Loss: 0.21984413266181946\n",
      "Epoch 6772, Loss: 0.6880480796098709, Final Batch Loss: 0.25067099928855896\n",
      "Epoch 6773, Loss: 0.7014654576778412, Final Batch Loss: 0.20735864341259003\n",
      "Epoch 6774, Loss: 0.898923322558403, Final Batch Loss: 0.33669427037239075\n",
      "Epoch 6775, Loss: 0.6906793713569641, Final Batch Loss: 0.106470987200737\n",
      "Epoch 6776, Loss: 0.6773582994937897, Final Batch Loss: 0.18732507526874542\n",
      "Epoch 6777, Loss: 0.7956673800945282, Final Batch Loss: 0.24224719405174255\n",
      "Epoch 6778, Loss: 0.8495132923126221, Final Batch Loss: 0.28340810537338257\n",
      "Epoch 6779, Loss: 0.6349790990352631, Final Batch Loss: 0.15256157517433167\n",
      "Epoch 6780, Loss: 0.7305742651224136, Final Batch Loss: 0.18212850391864777\n",
      "Epoch 6781, Loss: 0.7138572782278061, Final Batch Loss: 0.19216707348823547\n",
      "Epoch 6782, Loss: 0.8900465667247772, Final Batch Loss: 0.24506023526191711\n",
      "Epoch 6783, Loss: 0.8300487846136093, Final Batch Loss: 0.23132680356502533\n",
      "Epoch 6784, Loss: 0.8040171265602112, Final Batch Loss: 0.19277708232402802\n",
      "Epoch 6785, Loss: 0.6748729199171066, Final Batch Loss: 0.1791900247335434\n",
      "Epoch 6786, Loss: 0.7809569984674454, Final Batch Loss: 0.21732959151268005\n",
      "Epoch 6787, Loss: 0.6923156231641769, Final Batch Loss: 0.1697714626789093\n",
      "Epoch 6788, Loss: 0.7007073163986206, Final Batch Loss: 0.1455918252468109\n",
      "Epoch 6789, Loss: 0.89576455950737, Final Batch Loss: 0.2343767136335373\n",
      "Epoch 6790, Loss: 0.668313056230545, Final Batch Loss: 0.19402848184108734\n",
      "Epoch 6791, Loss: 0.6769270300865173, Final Batch Loss: 0.1495988517999649\n",
      "Epoch 6792, Loss: 0.6651000082492828, Final Batch Loss: 0.14367476105690002\n",
      "Epoch 6793, Loss: 0.7336478978395462, Final Batch Loss: 0.10242705047130585\n",
      "Epoch 6794, Loss: 0.7270870357751846, Final Batch Loss: 0.18801312148571014\n",
      "Epoch 6795, Loss: 0.8879348784685135, Final Batch Loss: 0.23428267240524292\n",
      "Epoch 6796, Loss: 0.8582366406917572, Final Batch Loss: 0.19608885049819946\n",
      "Epoch 6797, Loss: 0.8393184691667557, Final Batch Loss: 0.22412903606891632\n",
      "Epoch 6798, Loss: 0.649392381310463, Final Batch Loss: 0.172883540391922\n",
      "Epoch 6799, Loss: 0.7435059100389481, Final Batch Loss: 0.20420973002910614\n",
      "Epoch 6800, Loss: 0.8209110796451569, Final Batch Loss: 0.14904014766216278\n",
      "Epoch 6801, Loss: 0.690119281411171, Final Batch Loss: 0.21578335762023926\n",
      "Epoch 6802, Loss: 0.6763975620269775, Final Batch Loss: 0.19944407045841217\n",
      "Epoch 6803, Loss: 0.6779453903436661, Final Batch Loss: 0.14726313948631287\n",
      "Epoch 6804, Loss: 0.7044184058904648, Final Batch Loss: 0.14323070645332336\n",
      "Epoch 6805, Loss: 0.7479998618364334, Final Batch Loss: 0.2103317826986313\n",
      "Epoch 6806, Loss: 0.7456685602664948, Final Batch Loss: 0.12193752825260162\n",
      "Epoch 6807, Loss: 0.6316009610891342, Final Batch Loss: 0.1610700935125351\n",
      "Epoch 6808, Loss: 0.6783060431480408, Final Batch Loss: 0.16609486937522888\n",
      "Epoch 6809, Loss: 0.8351279646158218, Final Batch Loss: 0.20269905030727386\n",
      "Epoch 6810, Loss: 0.643664762377739, Final Batch Loss: 0.1425081193447113\n",
      "Epoch 6811, Loss: 0.7216001003980637, Final Batch Loss: 0.2319168746471405\n",
      "Epoch 6812, Loss: 0.7018980383872986, Final Batch Loss: 0.1704729199409485\n",
      "Epoch 6813, Loss: 0.7289967834949493, Final Batch Loss: 0.1904914826154709\n",
      "Epoch 6814, Loss: 0.8193956464529037, Final Batch Loss: 0.17257137596607208\n",
      "Epoch 6815, Loss: 0.7787617295980453, Final Batch Loss: 0.19665059447288513\n",
      "Epoch 6816, Loss: 0.7509213984012604, Final Batch Loss: 0.18923775851726532\n",
      "Epoch 6817, Loss: 0.6063154339790344, Final Batch Loss: 0.15428218245506287\n",
      "Epoch 6818, Loss: 0.7303074598312378, Final Batch Loss: 0.24688152968883514\n",
      "Epoch 6819, Loss: 0.7694944888353348, Final Batch Loss: 0.19043083488941193\n",
      "Epoch 6820, Loss: 0.7717551589012146, Final Batch Loss: 0.17121270298957825\n",
      "Epoch 6821, Loss: 0.8008963316679001, Final Batch Loss: 0.19303220510482788\n",
      "Epoch 6822, Loss: 0.7802439033985138, Final Batch Loss: 0.2669674754142761\n",
      "Epoch 6823, Loss: 0.7465760707855225, Final Batch Loss: 0.2142529934644699\n",
      "Epoch 6824, Loss: 0.7196451127529144, Final Batch Loss: 0.19540801644325256\n",
      "Epoch 6825, Loss: 0.7562044858932495, Final Batch Loss: 0.1828213632106781\n",
      "Epoch 6826, Loss: 0.7357614785432816, Final Batch Loss: 0.18849389255046844\n",
      "Epoch 6827, Loss: 0.6906214356422424, Final Batch Loss: 0.1318987011909485\n",
      "Epoch 6828, Loss: 0.8239023983478546, Final Batch Loss: 0.21250832080841064\n",
      "Epoch 6829, Loss: 0.6281808018684387, Final Batch Loss: 0.1007121354341507\n",
      "Epoch 6830, Loss: 0.6782010868191719, Final Batch Loss: 0.15165989100933075\n",
      "Epoch 6831, Loss: 0.7155858278274536, Final Batch Loss: 0.16205641627311707\n",
      "Epoch 6832, Loss: 0.6928574740886688, Final Batch Loss: 0.14835119247436523\n",
      "Epoch 6833, Loss: 0.8526918739080429, Final Batch Loss: 0.1970193088054657\n",
      "Epoch 6834, Loss: 0.8262424916028976, Final Batch Loss: 0.2931442856788635\n",
      "Epoch 6835, Loss: 0.7266971915960312, Final Batch Loss: 0.17491650581359863\n",
      "Epoch 6836, Loss: 0.7192279994487762, Final Batch Loss: 0.1564980000257492\n",
      "Epoch 6837, Loss: 0.819626972079277, Final Batch Loss: 0.21279102563858032\n",
      "Epoch 6838, Loss: 0.8018022775650024, Final Batch Loss: 0.2330615520477295\n",
      "Epoch 6839, Loss: 0.8094231188297272, Final Batch Loss: 0.14235547184944153\n",
      "Epoch 6840, Loss: 0.6699363887310028, Final Batch Loss: 0.15524576604366302\n",
      "Epoch 6841, Loss: 0.7677137553691864, Final Batch Loss: 0.2373964488506317\n",
      "Epoch 6842, Loss: 0.8073481023311615, Final Batch Loss: 0.1928037703037262\n",
      "Epoch 6843, Loss: 0.8344752788543701, Final Batch Loss: 0.258470743894577\n",
      "Epoch 6844, Loss: 0.767343744635582, Final Batch Loss: 0.23017434775829315\n",
      "Epoch 6845, Loss: 0.8241492658853531, Final Batch Loss: 0.2607986032962799\n",
      "Epoch 6846, Loss: 0.7319978028535843, Final Batch Loss: 0.257346510887146\n",
      "Epoch 6847, Loss: 0.8298727124929428, Final Batch Loss: 0.1764083206653595\n",
      "Epoch 6848, Loss: 0.6983538120985031, Final Batch Loss: 0.19814924895763397\n",
      "Epoch 6849, Loss: 0.6195060238242149, Final Batch Loss: 0.12440340965986252\n",
      "Epoch 6850, Loss: 0.8408204317092896, Final Batch Loss: 0.295319139957428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6851, Loss: 0.621072418987751, Final Batch Loss: 0.11500865966081619\n",
      "Epoch 6852, Loss: 0.7976837903261185, Final Batch Loss: 0.14866337180137634\n",
      "Epoch 6853, Loss: 0.8720840662717819, Final Batch Loss: 0.19690796732902527\n",
      "Epoch 6854, Loss: 0.7757856100797653, Final Batch Loss: 0.298054575920105\n",
      "Epoch 6855, Loss: 0.8035594373941422, Final Batch Loss: 0.20879694819450378\n",
      "Epoch 6856, Loss: 0.8312268257141113, Final Batch Loss: 0.2074306160211563\n",
      "Epoch 6857, Loss: 0.8363947048783302, Final Batch Loss: 0.2980248034000397\n",
      "Epoch 6858, Loss: 0.7963646948337555, Final Batch Loss: 0.13946372270584106\n",
      "Epoch 6859, Loss: 0.8081562966108322, Final Batch Loss: 0.15766234695911407\n",
      "Epoch 6860, Loss: 0.7932648211717606, Final Batch Loss: 0.17177845537662506\n",
      "Epoch 6861, Loss: 0.7212304249405861, Final Batch Loss: 0.12490168958902359\n",
      "Epoch 6862, Loss: 0.6836227029561996, Final Batch Loss: 0.09912268817424774\n",
      "Epoch 6863, Loss: 0.8986100405454636, Final Batch Loss: 0.27507996559143066\n",
      "Epoch 6864, Loss: 0.6553447023034096, Final Batch Loss: 0.16884145140647888\n",
      "Epoch 6865, Loss: 0.8981345742940903, Final Batch Loss: 0.33351555466651917\n",
      "Epoch 6866, Loss: 0.7544875741004944, Final Batch Loss: 0.1981000304222107\n",
      "Epoch 6867, Loss: 0.82622030377388, Final Batch Loss: 0.1786724328994751\n",
      "Epoch 6868, Loss: 0.6163834482431412, Final Batch Loss: 0.12165701389312744\n",
      "Epoch 6869, Loss: 0.7121172547340393, Final Batch Loss: 0.10843591392040253\n",
      "Epoch 6870, Loss: 0.7345733642578125, Final Batch Loss: 0.1663367748260498\n",
      "Epoch 6871, Loss: 0.749850794672966, Final Batch Loss: 0.21695885062217712\n",
      "Epoch 6872, Loss: 0.6564855426549911, Final Batch Loss: 0.21980325877666473\n",
      "Epoch 6873, Loss: 0.6967410370707512, Final Batch Loss: 0.11937620490789413\n",
      "Epoch 6874, Loss: 0.7400082498788834, Final Batch Loss: 0.21565964818000793\n",
      "Epoch 6875, Loss: 0.8160483539104462, Final Batch Loss: 0.2428632229566574\n",
      "Epoch 6876, Loss: 0.84097820520401, Final Batch Loss: 0.23465706408023834\n",
      "Epoch 6877, Loss: 0.7561772763729095, Final Batch Loss: 0.13969025015830994\n",
      "Epoch 6878, Loss: 0.7991038262844086, Final Batch Loss: 0.2682031989097595\n",
      "Epoch 6879, Loss: 0.844884380698204, Final Batch Loss: 0.21393266320228577\n",
      "Epoch 6880, Loss: 0.7584568709135056, Final Batch Loss: 0.19846704602241516\n",
      "Epoch 6881, Loss: 0.8156312704086304, Final Batch Loss: 0.2550702691078186\n",
      "Epoch 6882, Loss: 0.8219704180955887, Final Batch Loss: 0.224208265542984\n",
      "Epoch 6883, Loss: 0.798561617732048, Final Batch Loss: 0.16583669185638428\n",
      "Epoch 6884, Loss: 0.7065107524394989, Final Batch Loss: 0.15768292546272278\n",
      "Epoch 6885, Loss: 0.8123148083686829, Final Batch Loss: 0.1918826848268509\n",
      "Epoch 6886, Loss: 0.7640653252601624, Final Batch Loss: 0.1999262571334839\n",
      "Epoch 6887, Loss: 0.7686500549316406, Final Batch Loss: 0.2249370664358139\n",
      "Epoch 6888, Loss: 0.914665937423706, Final Batch Loss: 0.24182692170143127\n",
      "Epoch 6889, Loss: 0.8500469923019409, Final Batch Loss: 0.16681969165802002\n",
      "Epoch 6890, Loss: 0.9176300764083862, Final Batch Loss: 0.2599506080150604\n",
      "Epoch 6891, Loss: 0.8622561991214752, Final Batch Loss: 0.18761898577213287\n",
      "Epoch 6892, Loss: 0.7454765290021896, Final Batch Loss: 0.19251683354377747\n",
      "Epoch 6893, Loss: 0.8170207291841507, Final Batch Loss: 0.22605019807815552\n",
      "Epoch 6894, Loss: 0.7044660747051239, Final Batch Loss: 0.14277401566505432\n",
      "Epoch 6895, Loss: 0.8433728963136673, Final Batch Loss: 0.2767636775970459\n",
      "Epoch 6896, Loss: 0.7117569297552109, Final Batch Loss: 0.17290236055850983\n",
      "Epoch 6897, Loss: 0.8280371576547623, Final Batch Loss: 0.2914658188819885\n",
      "Epoch 6898, Loss: 0.8486691936850548, Final Batch Loss: 0.36203181743621826\n",
      "Epoch 6899, Loss: 0.7260817736387253, Final Batch Loss: 0.22608314454555511\n",
      "Epoch 6900, Loss: 0.777697890996933, Final Batch Loss: 0.19793689250946045\n",
      "Epoch 6901, Loss: 0.6728562563657761, Final Batch Loss: 0.1657310277223587\n",
      "Epoch 6902, Loss: 0.9200569689273834, Final Batch Loss: 0.2299327403306961\n",
      "Epoch 6903, Loss: 0.7191879898309708, Final Batch Loss: 0.1837633103132248\n",
      "Epoch 6904, Loss: 0.7493905425071716, Final Batch Loss: 0.2558935582637787\n",
      "Epoch 6905, Loss: 0.7295597344636917, Final Batch Loss: 0.1686096042394638\n",
      "Epoch 6906, Loss: 0.6703315377235413, Final Batch Loss: 0.16285574436187744\n",
      "Epoch 6907, Loss: 0.6878371834754944, Final Batch Loss: 0.157810240983963\n",
      "Epoch 6908, Loss: 0.6878863498568535, Final Batch Loss: 0.21429181098937988\n",
      "Epoch 6909, Loss: 0.9045875668525696, Final Batch Loss: 0.24753354489803314\n",
      "Epoch 6910, Loss: 0.75931116938591, Final Batch Loss: 0.21100091934204102\n",
      "Epoch 6911, Loss: 0.7169570624828339, Final Batch Loss: 0.21803948283195496\n",
      "Epoch 6912, Loss: 0.763509064912796, Final Batch Loss: 0.18199554085731506\n",
      "Epoch 6913, Loss: 0.8705228269100189, Final Batch Loss: 0.22407092154026031\n",
      "Epoch 6914, Loss: 0.6816258504986763, Final Batch Loss: 0.2331780642271042\n",
      "Epoch 6915, Loss: 0.8817096799612045, Final Batch Loss: 0.13963687419891357\n",
      "Epoch 6916, Loss: 0.7014796882867813, Final Batch Loss: 0.1890205293893814\n",
      "Epoch 6917, Loss: 0.8454900830984116, Final Batch Loss: 0.2003338634967804\n",
      "Epoch 6918, Loss: 0.8195339888334274, Final Batch Loss: 0.1832607239484787\n",
      "Epoch 6919, Loss: 0.8619221746921539, Final Batch Loss: 0.21195872128009796\n",
      "Epoch 6920, Loss: 0.7292536050081253, Final Batch Loss: 0.21215327084064484\n",
      "Epoch 6921, Loss: 0.7462843358516693, Final Batch Loss: 0.21852022409439087\n",
      "Epoch 6922, Loss: 0.715998038649559, Final Batch Loss: 0.1808207780122757\n",
      "Epoch 6923, Loss: 0.697523444890976, Final Batch Loss: 0.14897394180297852\n",
      "Epoch 6924, Loss: 0.7642185986042023, Final Batch Loss: 0.16896389424800873\n",
      "Epoch 6925, Loss: 0.7398836016654968, Final Batch Loss: 0.16843394935131073\n",
      "Epoch 6926, Loss: 0.8147259056568146, Final Batch Loss: 0.1632637232542038\n",
      "Epoch 6927, Loss: 0.7062486484646797, Final Batch Loss: 0.1425764113664627\n",
      "Epoch 6928, Loss: 0.9284928739070892, Final Batch Loss: 0.3198717534542084\n",
      "Epoch 6929, Loss: 0.6824341639876366, Final Batch Loss: 0.16403742134571075\n",
      "Epoch 6930, Loss: 0.6681489199399948, Final Batch Loss: 0.18335270881652832\n",
      "Epoch 6931, Loss: 0.8813249915838242, Final Batch Loss: 0.20658329129219055\n",
      "Epoch 6932, Loss: 0.7376420646905899, Final Batch Loss: 0.19723403453826904\n",
      "Epoch 6933, Loss: 0.6731893122196198, Final Batch Loss: 0.19044160842895508\n",
      "Epoch 6934, Loss: 0.7642425447702408, Final Batch Loss: 0.15075470507144928\n",
      "Epoch 6935, Loss: 0.7913459539413452, Final Batch Loss: 0.14105919003486633\n",
      "Epoch 6936, Loss: 0.594284251332283, Final Batch Loss: 0.16199374198913574\n",
      "Epoch 6937, Loss: 0.7241606265306473, Final Batch Loss: 0.18085581064224243\n",
      "Epoch 6938, Loss: 0.7653290182352066, Final Batch Loss: 0.3393312692642212\n",
      "Epoch 6939, Loss: 0.6345694661140442, Final Batch Loss: 0.19136573374271393\n",
      "Epoch 6940, Loss: 0.7613227665424347, Final Batch Loss: 0.22148843109607697\n",
      "Epoch 6941, Loss: 0.7150304168462753, Final Batch Loss: 0.24807465076446533\n",
      "Epoch 6942, Loss: 0.5643550902605057, Final Batch Loss: 0.09226660430431366\n",
      "Epoch 6943, Loss: 0.7890163213014603, Final Batch Loss: 0.2866177260875702\n",
      "Epoch 6944, Loss: 0.6351233348250389, Final Batch Loss: 0.12288634479045868\n",
      "Epoch 6945, Loss: 0.7162741422653198, Final Batch Loss: 0.17600563168525696\n",
      "Epoch 6946, Loss: 0.7761262059211731, Final Batch Loss: 0.20257830619812012\n",
      "Epoch 6947, Loss: 0.7766203433275223, Final Batch Loss: 0.20140236616134644\n",
      "Epoch 6948, Loss: 0.5917510837316513, Final Batch Loss: 0.14143012464046478\n",
      "Epoch 6949, Loss: 0.7169743627309799, Final Batch Loss: 0.19114473462104797\n",
      "Epoch 6950, Loss: 0.6443944573402405, Final Batch Loss: 0.19337618350982666\n",
      "Epoch 6951, Loss: 0.6989488378167152, Final Batch Loss: 0.12424778193235397\n",
      "Epoch 6952, Loss: 0.7029304355382919, Final Batch Loss: 0.16896305978298187\n",
      "Epoch 6953, Loss: 0.7746939659118652, Final Batch Loss: 0.13358436524868011\n",
      "Epoch 6954, Loss: 0.6350273489952087, Final Batch Loss: 0.17207935452461243\n",
      "Epoch 6955, Loss: 0.7007693126797676, Final Batch Loss: 0.10973060876131058\n",
      "Epoch 6956, Loss: 0.7110323011875153, Final Batch Loss: 0.20348769426345825\n",
      "Epoch 6957, Loss: 0.6800331771373749, Final Batch Loss: 0.12701423466205597\n",
      "Epoch 6958, Loss: 0.682841032743454, Final Batch Loss: 0.15325923264026642\n",
      "Epoch 6959, Loss: 0.6719596832990646, Final Batch Loss: 0.1833374947309494\n",
      "Epoch 6960, Loss: 0.7620930671691895, Final Batch Loss: 0.16511432826519012\n",
      "Epoch 6961, Loss: 0.5622548088431358, Final Batch Loss: 0.14293663203716278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6962, Loss: 0.665391594171524, Final Batch Loss: 0.1547689586877823\n",
      "Epoch 6963, Loss: 0.8214003294706345, Final Batch Loss: 0.19795788824558258\n",
      "Epoch 6964, Loss: 0.6664813607931137, Final Batch Loss: 0.18751659989356995\n",
      "Epoch 6965, Loss: 0.7065252661705017, Final Batch Loss: 0.21623027324676514\n",
      "Epoch 6966, Loss: 0.6692633926868439, Final Batch Loss: 0.18776589632034302\n",
      "Epoch 6967, Loss: 0.7668143063783646, Final Batch Loss: 0.25433096289634705\n",
      "Epoch 6968, Loss: 0.8263190984725952, Final Batch Loss: 0.2099957913160324\n",
      "Epoch 6969, Loss: 0.7396218031644821, Final Batch Loss: 0.2017151564359665\n",
      "Epoch 6970, Loss: 0.8221395760774612, Final Batch Loss: 0.1457880139350891\n",
      "Epoch 6971, Loss: 0.7794769406318665, Final Batch Loss: 0.19643577933311462\n",
      "Epoch 6972, Loss: 0.7772790789604187, Final Batch Loss: 0.1663244366645813\n",
      "Epoch 6973, Loss: 0.8148336410522461, Final Batch Loss: 0.22914639115333557\n",
      "Epoch 6974, Loss: 0.7451577037572861, Final Batch Loss: 0.20770280063152313\n",
      "Epoch 6975, Loss: 0.6909286379814148, Final Batch Loss: 0.20054563879966736\n",
      "Epoch 6976, Loss: 0.6569910496473312, Final Batch Loss: 0.131022647023201\n",
      "Epoch 6977, Loss: 0.6617335230112076, Final Batch Loss: 0.1944526582956314\n",
      "Epoch 6978, Loss: 0.6619115620851517, Final Batch Loss: 0.1256786286830902\n",
      "Epoch 6979, Loss: 0.742596760392189, Final Batch Loss: 0.2400418519973755\n",
      "Epoch 6980, Loss: 0.695659339427948, Final Batch Loss: 0.13925063610076904\n",
      "Epoch 6981, Loss: 0.7881802320480347, Final Batch Loss: 0.2098514884710312\n",
      "Epoch 6982, Loss: 0.7832639217376709, Final Batch Loss: 0.18471840023994446\n",
      "Epoch 6983, Loss: 0.6791733056306839, Final Batch Loss: 0.2247725874185562\n",
      "Epoch 6984, Loss: 0.7873999029397964, Final Batch Loss: 0.1621583253145218\n",
      "Epoch 6985, Loss: 0.7082705795764923, Final Batch Loss: 0.2518099844455719\n",
      "Epoch 6986, Loss: 0.6500100344419479, Final Batch Loss: 0.1423862725496292\n",
      "Epoch 6987, Loss: 0.7125275805592537, Final Batch Loss: 0.17615677416324615\n",
      "Epoch 6988, Loss: 0.8204637765884399, Final Batch Loss: 0.2054816633462906\n",
      "Epoch 6989, Loss: 0.6623122245073318, Final Batch Loss: 0.12789912521839142\n",
      "Epoch 6990, Loss: 0.6125052645802498, Final Batch Loss: 0.19879566133022308\n",
      "Epoch 6991, Loss: 0.7373896390199661, Final Batch Loss: 0.20124714076519012\n",
      "Epoch 6992, Loss: 0.6876802742481232, Final Batch Loss: 0.17351940274238586\n",
      "Epoch 6993, Loss: 0.6922709196805954, Final Batch Loss: 0.19636714458465576\n",
      "Epoch 6994, Loss: 0.8141546919941902, Final Batch Loss: 0.10766617208719254\n",
      "Epoch 6995, Loss: 0.7932291179895401, Final Batch Loss: 0.21685323119163513\n",
      "Epoch 6996, Loss: 0.686132550239563, Final Batch Loss: 0.1513717621564865\n",
      "Epoch 6997, Loss: 0.6731145083904266, Final Batch Loss: 0.1646021604537964\n",
      "Epoch 6998, Loss: 0.6858848184347153, Final Batch Loss: 0.14874549210071564\n",
      "Epoch 6999, Loss: 0.822527289390564, Final Batch Loss: 0.1703527718782425\n",
      "Epoch 7000, Loss: 0.6130844801664352, Final Batch Loss: 0.1648946851491928\n",
      "Epoch 7001, Loss: 0.5946943163871765, Final Batch Loss: 0.13836562633514404\n",
      "Epoch 7002, Loss: 0.7055634707212448, Final Batch Loss: 0.16250884532928467\n",
      "Epoch 7003, Loss: 0.6433927863836288, Final Batch Loss: 0.1589038074016571\n",
      "Epoch 7004, Loss: 0.7149070650339127, Final Batch Loss: 0.16596423089504242\n",
      "Epoch 7005, Loss: 0.6957675069570541, Final Batch Loss: 0.17329536378383636\n",
      "Epoch 7006, Loss: 0.7146017998456955, Final Batch Loss: 0.18085743486881256\n",
      "Epoch 7007, Loss: 0.8467271625995636, Final Batch Loss: 0.2696194648742676\n",
      "Epoch 7008, Loss: 0.7054948210716248, Final Batch Loss: 0.15978331863880157\n",
      "Epoch 7009, Loss: 0.6914355754852295, Final Batch Loss: 0.2350580245256424\n",
      "Epoch 7010, Loss: 0.7232410162687302, Final Batch Loss: 0.17803548276424408\n",
      "Epoch 7011, Loss: 0.6640125513076782, Final Batch Loss: 0.13452185690402985\n",
      "Epoch 7012, Loss: 0.7782625705003738, Final Batch Loss: 0.17870202660560608\n",
      "Epoch 7013, Loss: 0.7198375165462494, Final Batch Loss: 0.18462355434894562\n",
      "Epoch 7014, Loss: 0.8862407207489014, Final Batch Loss: 0.22352033853530884\n",
      "Epoch 7015, Loss: 0.800698459148407, Final Batch Loss: 0.16936549544334412\n",
      "Epoch 7016, Loss: 0.7029430866241455, Final Batch Loss: 0.1379694640636444\n",
      "Epoch 7017, Loss: 0.7163799703121185, Final Batch Loss: 0.13311856985092163\n",
      "Epoch 7018, Loss: 0.7286846339702606, Final Batch Loss: 0.1302630603313446\n",
      "Epoch 7019, Loss: 0.8396746218204498, Final Batch Loss: 0.12837903201580048\n",
      "Epoch 7020, Loss: 0.8474692553281784, Final Batch Loss: 0.1902395784854889\n",
      "Epoch 7021, Loss: 0.8243665397167206, Final Batch Loss: 0.19636711478233337\n",
      "Epoch 7022, Loss: 0.7300697267055511, Final Batch Loss: 0.18083257973194122\n",
      "Epoch 7023, Loss: 0.8289505690336227, Final Batch Loss: 0.22482994198799133\n",
      "Epoch 7024, Loss: 0.7115950286388397, Final Batch Loss: 0.14964020252227783\n",
      "Epoch 7025, Loss: 0.8137274384498596, Final Batch Loss: 0.25575822591781616\n",
      "Epoch 7026, Loss: 0.6934853941202164, Final Batch Loss: 0.21652603149414062\n",
      "Epoch 7027, Loss: 0.7547152787446976, Final Batch Loss: 0.1644073873758316\n",
      "Epoch 7028, Loss: 0.7137473970651627, Final Batch Loss: 0.17126360535621643\n",
      "Epoch 7029, Loss: 0.8577865660190582, Final Batch Loss: 0.13427342474460602\n",
      "Epoch 7030, Loss: 0.8569779694080353, Final Batch Loss: 0.2381080538034439\n",
      "Epoch 7031, Loss: 0.7481530904769897, Final Batch Loss: 0.15782850980758667\n",
      "Epoch 7032, Loss: 0.6797733455896378, Final Batch Loss: 0.15241314470767975\n",
      "Epoch 7033, Loss: 0.9248655736446381, Final Batch Loss: 0.3269970715045929\n",
      "Epoch 7034, Loss: 0.8235792815685272, Final Batch Loss: 0.26884886622428894\n",
      "Epoch 7035, Loss: 0.7718980014324188, Final Batch Loss: 0.17717112600803375\n",
      "Epoch 7036, Loss: 0.8182223439216614, Final Batch Loss: 0.19838519394397736\n",
      "Epoch 7037, Loss: 0.6904204487800598, Final Batch Loss: 0.15801258385181427\n",
      "Epoch 7038, Loss: 0.7243253588676453, Final Batch Loss: 0.20050694048404694\n",
      "Epoch 7039, Loss: 0.7140990346670151, Final Batch Loss: 0.20876727998256683\n",
      "Epoch 7040, Loss: 0.7638770937919617, Final Batch Loss: 0.14274320006370544\n",
      "Epoch 7041, Loss: 0.7465229481458664, Final Batch Loss: 0.17206883430480957\n",
      "Epoch 7042, Loss: 0.7279316782951355, Final Batch Loss: 0.14712348580360413\n",
      "Epoch 7043, Loss: 0.7264523953199387, Final Batch Loss: 0.17910203337669373\n",
      "Epoch 7044, Loss: 0.660815104842186, Final Batch Loss: 0.15348178148269653\n",
      "Epoch 7045, Loss: 0.8172063678503036, Final Batch Loss: 0.17550130188465118\n",
      "Epoch 7046, Loss: 0.6990169882774353, Final Batch Loss: 0.12930114567279816\n",
      "Epoch 7047, Loss: 0.8656646013259888, Final Batch Loss: 0.19617623090744019\n",
      "Epoch 7048, Loss: 0.6523246169090271, Final Batch Loss: 0.18267957866191864\n",
      "Epoch 7049, Loss: 0.7545143663883209, Final Batch Loss: 0.18855296075344086\n",
      "Epoch 7050, Loss: 0.8737951368093491, Final Batch Loss: 0.3093239665031433\n",
      "Epoch 7051, Loss: 0.7812436670064926, Final Batch Loss: 0.14844302833080292\n",
      "Epoch 7052, Loss: 0.6728320866823196, Final Batch Loss: 0.1534907966852188\n",
      "Epoch 7053, Loss: 0.8036776036024094, Final Batch Loss: 0.18604563176631927\n",
      "Epoch 7054, Loss: 0.5952455997467041, Final Batch Loss: 0.14519798755645752\n",
      "Epoch 7055, Loss: 0.6420699581503868, Final Batch Loss: 0.13950493931770325\n",
      "Epoch 7056, Loss: 0.6470909714698792, Final Batch Loss: 0.13915354013442993\n",
      "Epoch 7057, Loss: 0.7349273413419724, Final Batch Loss: 0.2045418620109558\n",
      "Epoch 7058, Loss: 0.6163960546255112, Final Batch Loss: 0.16282981634140015\n",
      "Epoch 7059, Loss: 0.6585285663604736, Final Batch Loss: 0.204739511013031\n",
      "Epoch 7060, Loss: 0.7251365780830383, Final Batch Loss: 0.1580333411693573\n",
      "Epoch 7061, Loss: 0.7202388346195221, Final Batch Loss: 0.14571191370487213\n",
      "Epoch 7062, Loss: 0.6604259163141251, Final Batch Loss: 0.16798759996891022\n",
      "Epoch 7063, Loss: 0.7025069296360016, Final Batch Loss: 0.12883546948432922\n",
      "Epoch 7064, Loss: 0.6561627984046936, Final Batch Loss: 0.16633842885494232\n",
      "Epoch 7065, Loss: 0.7226347327232361, Final Batch Loss: 0.20802342891693115\n",
      "Epoch 7066, Loss: 0.7774525582790375, Final Batch Loss: 0.15185777842998505\n",
      "Epoch 7067, Loss: 0.8140796720981598, Final Batch Loss: 0.14722611010074615\n",
      "Epoch 7068, Loss: 0.7783149629831314, Final Batch Loss: 0.1409260630607605\n",
      "Epoch 7069, Loss: 0.7809919714927673, Final Batch Loss: 0.14599503576755524\n",
      "Epoch 7070, Loss: 0.8215003609657288, Final Batch Loss: 0.1937991976737976\n",
      "Epoch 7071, Loss: 0.8602327555418015, Final Batch Loss: 0.20987413823604584\n",
      "Epoch 7072, Loss: 0.8126944601535797, Final Batch Loss: 0.1835460513830185\n",
      "Epoch 7073, Loss: 0.7447851225733757, Final Batch Loss: 0.24940775334835052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7074, Loss: 0.8849150091409683, Final Batch Loss: 0.23961274325847626\n",
      "Epoch 7075, Loss: 0.705074742436409, Final Batch Loss: 0.1714545488357544\n",
      "Epoch 7076, Loss: 0.7034273743629456, Final Batch Loss: 0.20534372329711914\n",
      "Epoch 7077, Loss: 0.7566360682249069, Final Batch Loss: 0.17339083552360535\n",
      "Epoch 7078, Loss: 0.7245407998561859, Final Batch Loss: 0.17044982314109802\n",
      "Epoch 7079, Loss: 0.8365837931632996, Final Batch Loss: 0.20720204710960388\n",
      "Epoch 7080, Loss: 0.8007610440254211, Final Batch Loss: 0.2810905873775482\n",
      "Epoch 7081, Loss: 0.8198940306901932, Final Batch Loss: 0.19187209010124207\n",
      "Epoch 7082, Loss: 0.9142758697271347, Final Batch Loss: 0.2539585530757904\n",
      "Epoch 7083, Loss: 0.7167848348617554, Final Batch Loss: 0.2315647453069687\n",
      "Epoch 7084, Loss: 0.7634226828813553, Final Batch Loss: 0.1914048194885254\n",
      "Epoch 7085, Loss: 0.8346535414457321, Final Batch Loss: 0.3511519730091095\n",
      "Epoch 7086, Loss: 0.7283372282981873, Final Batch Loss: 0.21038560569286346\n",
      "Epoch 7087, Loss: 0.6681890338659286, Final Batch Loss: 0.1804279237985611\n",
      "Epoch 7088, Loss: 0.763077437877655, Final Batch Loss: 0.14267194271087646\n",
      "Epoch 7089, Loss: 0.6883113980293274, Final Batch Loss: 0.14260856807231903\n",
      "Epoch 7090, Loss: 0.6856671869754791, Final Batch Loss: 0.14483202993869781\n",
      "Epoch 7091, Loss: 0.5695218741893768, Final Batch Loss: 0.14042195677757263\n",
      "Epoch 7092, Loss: 0.6545732766389847, Final Batch Loss: 0.17419002950191498\n",
      "Epoch 7093, Loss: 0.8651798367500305, Final Batch Loss: 0.2840985655784607\n",
      "Epoch 7094, Loss: 0.7179848253726959, Final Batch Loss: 0.19492097198963165\n",
      "Epoch 7095, Loss: 0.8191308081150055, Final Batch Loss: 0.22605495154857635\n",
      "Epoch 7096, Loss: 0.7878046780824661, Final Batch Loss: 0.18166576325893402\n",
      "Epoch 7097, Loss: 0.7389032691717148, Final Batch Loss: 0.17097556591033936\n",
      "Epoch 7098, Loss: 0.8395802527666092, Final Batch Loss: 0.20758959650993347\n",
      "Epoch 7099, Loss: 0.7722982317209244, Final Batch Loss: 0.23862509429454803\n",
      "Epoch 7100, Loss: 0.781089574098587, Final Batch Loss: 0.30729153752326965\n",
      "Epoch 7101, Loss: 0.5990520119667053, Final Batch Loss: 0.08521567285060883\n",
      "Epoch 7102, Loss: 0.6449298709630966, Final Batch Loss: 0.170948788523674\n",
      "Epoch 7103, Loss: 0.6912724524736404, Final Batch Loss: 0.1573835164308548\n",
      "Epoch 7104, Loss: 0.7592346221208572, Final Batch Loss: 0.16455204784870148\n",
      "Epoch 7105, Loss: 0.6329837739467621, Final Batch Loss: 0.1696023792028427\n",
      "Epoch 7106, Loss: 0.7298590689897537, Final Batch Loss: 0.20173844695091248\n",
      "Epoch 7107, Loss: 0.5982097908854485, Final Batch Loss: 0.0891948863863945\n",
      "Epoch 7108, Loss: 0.6591597348451614, Final Batch Loss: 0.19010160863399506\n",
      "Epoch 7109, Loss: 0.6139178276062012, Final Batch Loss: 0.14590589702129364\n",
      "Epoch 7110, Loss: 0.7033607661724091, Final Batch Loss: 0.12846755981445312\n",
      "Epoch 7111, Loss: 0.7167540490627289, Final Batch Loss: 0.1919374316930771\n",
      "Epoch 7112, Loss: 0.6160467565059662, Final Batch Loss: 0.13599185645580292\n",
      "Epoch 7113, Loss: 0.6940550953149796, Final Batch Loss: 0.15675804018974304\n",
      "Epoch 7114, Loss: 0.7414715141057968, Final Batch Loss: 0.15059712529182434\n",
      "Epoch 7115, Loss: 0.7738283723592758, Final Batch Loss: 0.16442391276359558\n",
      "Epoch 7116, Loss: 0.6310829520225525, Final Batch Loss: 0.16888564825057983\n",
      "Epoch 7117, Loss: 0.676578015089035, Final Batch Loss: 0.17199711501598358\n",
      "Epoch 7118, Loss: 0.697174996137619, Final Batch Loss: 0.13457676768302917\n",
      "Epoch 7119, Loss: 0.6475512683391571, Final Batch Loss: 0.15833880007266998\n",
      "Epoch 7120, Loss: 0.6832669079303741, Final Batch Loss: 0.14805065095424652\n",
      "Epoch 7121, Loss: 0.62895667552948, Final Batch Loss: 0.14965756237506866\n",
      "Epoch 7122, Loss: 0.7534510046243668, Final Batch Loss: 0.17864803969860077\n",
      "Epoch 7123, Loss: 0.7056382894515991, Final Batch Loss: 0.1376364529132843\n",
      "Epoch 7124, Loss: 0.6633689999580383, Final Batch Loss: 0.1399117410182953\n",
      "Epoch 7125, Loss: 0.6866592168807983, Final Batch Loss: 0.13421276211738586\n",
      "Epoch 7126, Loss: 0.7202194929122925, Final Batch Loss: 0.1909119337797165\n",
      "Epoch 7127, Loss: 0.8241844177246094, Final Batch Loss: 0.1613791137933731\n",
      "Epoch 7128, Loss: 0.6818815767765045, Final Batch Loss: 0.1756608635187149\n",
      "Epoch 7129, Loss: 0.754601776599884, Final Batch Loss: 0.12133464217185974\n",
      "Epoch 7130, Loss: 0.8448458164930344, Final Batch Loss: 0.1707918643951416\n",
      "Epoch 7131, Loss: 0.7012272328138351, Final Batch Loss: 0.1476939469575882\n",
      "Epoch 7132, Loss: 0.7287658452987671, Final Batch Loss: 0.19178883731365204\n",
      "Epoch 7133, Loss: 0.72355717420578, Final Batch Loss: 0.1891341358423233\n",
      "Epoch 7134, Loss: 0.7070092260837555, Final Batch Loss: 0.16001828014850616\n",
      "Epoch 7135, Loss: 0.7431601136922836, Final Batch Loss: 0.2309471219778061\n",
      "Epoch 7136, Loss: 0.6325340270996094, Final Batch Loss: 0.15224304795265198\n",
      "Epoch 7137, Loss: 0.7036260813474655, Final Batch Loss: 0.20714356005191803\n",
      "Epoch 7138, Loss: 0.755560114979744, Final Batch Loss: 0.11675961315631866\n",
      "Epoch 7139, Loss: 0.9135290533304214, Final Batch Loss: 0.21673856675624847\n",
      "Epoch 7140, Loss: 0.7223642021417618, Final Batch Loss: 0.22541075944900513\n",
      "Epoch 7141, Loss: 0.722952201962471, Final Batch Loss: 0.18417038023471832\n",
      "Epoch 7142, Loss: 0.7487724125385284, Final Batch Loss: 0.16124257445335388\n",
      "Epoch 7143, Loss: 0.6814815551042557, Final Batch Loss: 0.20446157455444336\n",
      "Epoch 7144, Loss: 0.728473111987114, Final Batch Loss: 0.20046119391918182\n",
      "Epoch 7145, Loss: 0.8026270121335983, Final Batch Loss: 0.20173370838165283\n",
      "Epoch 7146, Loss: 0.7584086060523987, Final Batch Loss: 0.14276379346847534\n",
      "Epoch 7147, Loss: 0.8010096549987793, Final Batch Loss: 0.11567924916744232\n",
      "Epoch 7148, Loss: 0.7551383525133133, Final Batch Loss: 0.2095891535282135\n",
      "Epoch 7149, Loss: 0.7241997569799423, Final Batch Loss: 0.16385020315647125\n",
      "Epoch 7150, Loss: 0.6577442437410355, Final Batch Loss: 0.1465967893600464\n",
      "Epoch 7151, Loss: 0.7323553711175919, Final Batch Loss: 0.2303946316242218\n",
      "Epoch 7152, Loss: 0.6998332664370537, Final Batch Loss: 0.21350876986980438\n",
      "Epoch 7153, Loss: 0.6354871541261673, Final Batch Loss: 0.14186260104179382\n",
      "Epoch 7154, Loss: 0.6314424723386765, Final Batch Loss: 0.1845005303621292\n",
      "Epoch 7155, Loss: 0.7158524841070175, Final Batch Loss: 0.20310968160629272\n",
      "Epoch 7156, Loss: 0.6260485500097275, Final Batch Loss: 0.22440508008003235\n",
      "Epoch 7157, Loss: 0.6668186634778976, Final Batch Loss: 0.1948937028646469\n",
      "Epoch 7158, Loss: 0.6706510782241821, Final Batch Loss: 0.21336941421031952\n",
      "Epoch 7159, Loss: 0.7881748229265213, Final Batch Loss: 0.1511336863040924\n",
      "Epoch 7160, Loss: 0.651587963104248, Final Batch Loss: 0.15714700520038605\n",
      "Epoch 7161, Loss: 0.6981117427349091, Final Batch Loss: 0.19808609783649445\n",
      "Epoch 7162, Loss: 0.6056072935461998, Final Batch Loss: 0.1703050136566162\n",
      "Epoch 7163, Loss: 0.7470749020576477, Final Batch Loss: 0.1275515854358673\n",
      "Epoch 7164, Loss: 0.7085381597280502, Final Batch Loss: 0.19516687095165253\n",
      "Epoch 7165, Loss: 0.7424541562795639, Final Batch Loss: 0.22316452860832214\n",
      "Epoch 7166, Loss: 0.7381411492824554, Final Batch Loss: 0.19889360666275024\n",
      "Epoch 7167, Loss: 0.704955667257309, Final Batch Loss: 0.15421874821186066\n",
      "Epoch 7168, Loss: 0.6723519116640091, Final Batch Loss: 0.16750267148017883\n",
      "Epoch 7169, Loss: 0.6914324015378952, Final Batch Loss: 0.1646464616060257\n",
      "Epoch 7170, Loss: 0.6975709646940231, Final Batch Loss: 0.18941497802734375\n",
      "Epoch 7171, Loss: 0.6983709037303925, Final Batch Loss: 0.19894728064537048\n",
      "Epoch 7172, Loss: 0.878494918346405, Final Batch Loss: 0.307470440864563\n",
      "Epoch 7173, Loss: 0.5819502919912338, Final Batch Loss: 0.14447259902954102\n",
      "Epoch 7174, Loss: 0.8142362833023071, Final Batch Loss: 0.1981167048215866\n",
      "Epoch 7175, Loss: 0.7607254683971405, Final Batch Loss: 0.19296853244304657\n",
      "Epoch 7176, Loss: 0.7461013346910477, Final Batch Loss: 0.18297748267650604\n",
      "Epoch 7177, Loss: 0.8437900841236115, Final Batch Loss: 0.20953896641731262\n",
      "Epoch 7178, Loss: 0.735548309981823, Final Batch Loss: 0.1200442984700203\n",
      "Epoch 7179, Loss: 0.7963823974132538, Final Batch Loss: 0.19866566359996796\n",
      "Epoch 7180, Loss: 0.6359087079763412, Final Batch Loss: 0.16944818198680878\n",
      "Epoch 7181, Loss: 0.7440886497497559, Final Batch Loss: 0.13866670429706573\n",
      "Epoch 7182, Loss: 0.7289784699678421, Final Batch Loss: 0.1517060101032257\n",
      "Epoch 7183, Loss: 0.7560682743787766, Final Batch Loss: 0.22342973947525024\n",
      "Epoch 7184, Loss: 0.6435816735029221, Final Batch Loss: 0.12639863789081573\n",
      "Epoch 7185, Loss: 0.7941372990608215, Final Batch Loss: 0.2294643521308899\n",
      "Epoch 7186, Loss: 0.88667893409729, Final Batch Loss: 0.1511818766593933\n",
      "Epoch 7187, Loss: 0.7056018859148026, Final Batch Loss: 0.1762843281030655\n",
      "Epoch 7188, Loss: 0.7871733754873276, Final Batch Loss: 0.16860638558864594\n",
      "Epoch 7189, Loss: 0.9162217676639557, Final Batch Loss: 0.24230298399925232\n",
      "Epoch 7190, Loss: 0.8160417377948761, Final Batch Loss: 0.2076413929462433\n",
      "Epoch 7191, Loss: 0.8274481743574142, Final Batch Loss: 0.2391231805086136\n",
      "Epoch 7192, Loss: 0.8098491430282593, Final Batch Loss: 0.2213912159204483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7193, Loss: 0.7438157945871353, Final Batch Loss: 0.21428494155406952\n",
      "Epoch 7194, Loss: 0.6631605327129364, Final Batch Loss: 0.117449089884758\n",
      "Epoch 7195, Loss: 0.7570580244064331, Final Batch Loss: 0.18913249671459198\n",
      "Epoch 7196, Loss: 0.7108574211597443, Final Batch Loss: 0.15895569324493408\n",
      "Epoch 7197, Loss: 0.6846817880868912, Final Batch Loss: 0.17568476498126984\n",
      "Epoch 7198, Loss: 0.7278042435646057, Final Batch Loss: 0.2525536119937897\n",
      "Epoch 7199, Loss: 0.7438817620277405, Final Batch Loss: 0.21643975377082825\n",
      "Epoch 7200, Loss: 0.6988282948732376, Final Batch Loss: 0.19487638771533966\n",
      "Epoch 7201, Loss: 0.7622648924589157, Final Batch Loss: 0.19245277345180511\n",
      "Epoch 7202, Loss: 0.8850878775119781, Final Batch Loss: 0.16416265070438385\n",
      "Epoch 7203, Loss: 0.8228202015161514, Final Batch Loss: 0.20070572197437286\n",
      "Epoch 7204, Loss: 0.8044625818729401, Final Batch Loss: 0.18312112987041473\n",
      "Epoch 7205, Loss: 0.7382620573043823, Final Batch Loss: 0.20293951034545898\n",
      "Epoch 7206, Loss: 0.7201962471008301, Final Batch Loss: 0.15693587064743042\n",
      "Epoch 7207, Loss: 0.6476209610700607, Final Batch Loss: 0.14148329198360443\n",
      "Epoch 7208, Loss: 0.7368028461933136, Final Batch Loss: 0.20718573033809662\n",
      "Epoch 7209, Loss: 0.6753929704427719, Final Batch Loss: 0.1363821029663086\n",
      "Epoch 7210, Loss: 0.8911299705505371, Final Batch Loss: 0.14883393049240112\n",
      "Epoch 7211, Loss: 0.7744843363761902, Final Batch Loss: 0.14705798029899597\n",
      "Epoch 7212, Loss: 0.7382092028856277, Final Batch Loss: 0.19809561967849731\n",
      "Epoch 7213, Loss: 0.7519547343254089, Final Batch Loss: 0.1600949466228485\n",
      "Epoch 7214, Loss: 0.7751010209321976, Final Batch Loss: 0.22997575998306274\n",
      "Epoch 7215, Loss: 0.8357619792222977, Final Batch Loss: 0.16520407795906067\n",
      "Epoch 7216, Loss: 0.6342181861400604, Final Batch Loss: 0.20446674525737762\n",
      "Epoch 7217, Loss: 0.7802329361438751, Final Batch Loss: 0.15530788898468018\n",
      "Epoch 7218, Loss: 0.6636126041412354, Final Batch Loss: 0.15093567967414856\n",
      "Epoch 7219, Loss: 0.7077002972364426, Final Batch Loss: 0.1744610220193863\n",
      "Epoch 7220, Loss: 0.7770236134529114, Final Batch Loss: 0.21611377596855164\n",
      "Epoch 7221, Loss: 0.7312076836824417, Final Batch Loss: 0.1276242434978485\n",
      "Epoch 7222, Loss: 0.7285250723361969, Final Batch Loss: 0.1847805231809616\n",
      "Epoch 7223, Loss: 0.7591550201177597, Final Batch Loss: 0.21061979234218597\n",
      "Epoch 7224, Loss: 0.7495931088924408, Final Batch Loss: 0.18812772631645203\n",
      "Epoch 7225, Loss: 0.7193896472454071, Final Batch Loss: 0.1981383115053177\n",
      "Epoch 7226, Loss: 0.6682899296283722, Final Batch Loss: 0.211186021566391\n",
      "Epoch 7227, Loss: 0.756420373916626, Final Batch Loss: 0.11288866400718689\n",
      "Epoch 7228, Loss: 0.6405109390616417, Final Batch Loss: 0.11686969548463821\n",
      "Epoch 7229, Loss: 0.6590408608317375, Final Batch Loss: 0.2069559395313263\n",
      "Epoch 7230, Loss: 0.7572181820869446, Final Batch Loss: 0.19492673873901367\n",
      "Epoch 7231, Loss: 0.7023690342903137, Final Batch Loss: 0.15362590551376343\n",
      "Epoch 7232, Loss: 0.6726266592741013, Final Batch Loss: 0.12933200597763062\n",
      "Epoch 7233, Loss: 0.7123836576938629, Final Batch Loss: 0.1447993516921997\n",
      "Epoch 7234, Loss: 0.8217684477567673, Final Batch Loss: 0.28486284613609314\n",
      "Epoch 7235, Loss: 0.7211852967739105, Final Batch Loss: 0.2017708271741867\n",
      "Epoch 7236, Loss: 0.6771418899297714, Final Batch Loss: 0.15401984751224518\n",
      "Epoch 7237, Loss: 0.6932662278413773, Final Batch Loss: 0.18484358489513397\n",
      "Epoch 7238, Loss: 0.7502620071172714, Final Batch Loss: 0.18250146508216858\n",
      "Epoch 7239, Loss: 0.6931962221860886, Final Batch Loss: 0.19103175401687622\n",
      "Epoch 7240, Loss: 0.690315842628479, Final Batch Loss: 0.2081298977136612\n",
      "Epoch 7241, Loss: 0.6631097048521042, Final Batch Loss: 0.14262519776821136\n",
      "Epoch 7242, Loss: 0.7340977042913437, Final Batch Loss: 0.26684606075286865\n",
      "Epoch 7243, Loss: 0.7081761658191681, Final Batch Loss: 0.20422258973121643\n",
      "Epoch 7244, Loss: 0.7295043617486954, Final Batch Loss: 0.20996145904064178\n",
      "Epoch 7245, Loss: 0.65443554520607, Final Batch Loss: 0.18008001148700714\n",
      "Epoch 7246, Loss: 0.6821792349219322, Final Batch Loss: 0.1767560988664627\n",
      "Epoch 7247, Loss: 0.6433917209506035, Final Batch Loss: 0.12094410508871078\n",
      "Epoch 7248, Loss: 0.7270345687866211, Final Batch Loss: 0.13230925798416138\n",
      "Epoch 7249, Loss: 0.667491227388382, Final Batch Loss: 0.14071732759475708\n",
      "Epoch 7250, Loss: 0.758194625377655, Final Batch Loss: 0.2194889634847641\n",
      "Epoch 7251, Loss: 0.7471618801355362, Final Batch Loss: 0.2039872109889984\n",
      "Epoch 7252, Loss: 0.7481303960084915, Final Batch Loss: 0.20057687163352966\n",
      "Epoch 7253, Loss: 0.6300798207521439, Final Batch Loss: 0.1102013885974884\n",
      "Epoch 7254, Loss: 0.601261630654335, Final Batch Loss: 0.11493474245071411\n",
      "Epoch 7255, Loss: 0.6999596804380417, Final Batch Loss: 0.1451577991247177\n",
      "Epoch 7256, Loss: 0.724428802728653, Final Batch Loss: 0.15105287730693817\n",
      "Epoch 7257, Loss: 0.7776729837059975, Final Batch Loss: 0.15876111388206482\n",
      "Epoch 7258, Loss: 0.8247848451137543, Final Batch Loss: 0.2449653595685959\n",
      "Epoch 7259, Loss: 0.7858160138130188, Final Batch Loss: 0.2368827909231186\n",
      "Epoch 7260, Loss: 0.6267691999673843, Final Batch Loss: 0.12745703756809235\n",
      "Epoch 7261, Loss: 0.7927601784467697, Final Batch Loss: 0.2094443142414093\n",
      "Epoch 7262, Loss: 0.6452781856060028, Final Batch Loss: 0.14921386539936066\n",
      "Epoch 7263, Loss: 0.7273035645484924, Final Batch Loss: 0.22832109034061432\n",
      "Epoch 7264, Loss: 0.8087766021490097, Final Batch Loss: 0.18887363374233246\n",
      "Epoch 7265, Loss: 0.763030119240284, Final Batch Loss: 0.1579940915107727\n",
      "Epoch 7266, Loss: 0.7223996818065643, Final Batch Loss: 0.18124055862426758\n",
      "Epoch 7267, Loss: 0.7626118212938309, Final Batch Loss: 0.1417379379272461\n",
      "Epoch 7268, Loss: 0.7536206543445587, Final Batch Loss: 0.18202301859855652\n",
      "Epoch 7269, Loss: 0.7427989691495895, Final Batch Loss: 0.15085284411907196\n",
      "Epoch 7270, Loss: 0.8599654212594032, Final Batch Loss: 0.1997181624174118\n",
      "Epoch 7271, Loss: 0.7907612174749374, Final Batch Loss: 0.21170960366725922\n",
      "Epoch 7272, Loss: 0.8924913853406906, Final Batch Loss: 0.2604420781135559\n",
      "Epoch 7273, Loss: 0.6855566650629044, Final Batch Loss: 0.18458421528339386\n",
      "Epoch 7274, Loss: 0.8925021290779114, Final Batch Loss: 0.22528421878814697\n",
      "Epoch 7275, Loss: 0.7641152590513229, Final Batch Loss: 0.2164335995912552\n",
      "Epoch 7276, Loss: 0.7111111581325531, Final Batch Loss: 0.18844537436962128\n",
      "Epoch 7277, Loss: 0.9550122171640396, Final Batch Loss: 0.2700275182723999\n",
      "Epoch 7278, Loss: 0.7716470807790756, Final Batch Loss: 0.1669967770576477\n",
      "Epoch 7279, Loss: 0.85638727247715, Final Batch Loss: 0.1771710216999054\n",
      "Epoch 7280, Loss: 0.7763171494007111, Final Batch Loss: 0.16861368715763092\n",
      "Epoch 7281, Loss: 1.0538149774074554, Final Batch Loss: 0.21631085872650146\n",
      "Epoch 7282, Loss: 0.7887245565652847, Final Batch Loss: 0.22460639476776123\n",
      "Epoch 7283, Loss: 0.8150451630353928, Final Batch Loss: 0.26702749729156494\n",
      "Epoch 7284, Loss: 0.6680094748735428, Final Batch Loss: 0.15220388770103455\n",
      "Epoch 7285, Loss: 0.745877131819725, Final Batch Loss: 0.1546459197998047\n",
      "Epoch 7286, Loss: 0.7729636281728745, Final Batch Loss: 0.21856236457824707\n",
      "Epoch 7287, Loss: 0.7159718573093414, Final Batch Loss: 0.19993683695793152\n",
      "Epoch 7288, Loss: 0.678320437669754, Final Batch Loss: 0.14075618982315063\n",
      "Epoch 7289, Loss: 0.8351823389530182, Final Batch Loss: 0.2068946361541748\n",
      "Epoch 7290, Loss: 0.8165195286273956, Final Batch Loss: 0.18467429280281067\n",
      "Epoch 7291, Loss: 0.715266764163971, Final Batch Loss: 0.2048213928937912\n",
      "Epoch 7292, Loss: 0.707276001572609, Final Batch Loss: 0.16939586400985718\n",
      "Epoch 7293, Loss: 0.7432135194540024, Final Batch Loss: 0.16772663593292236\n",
      "Epoch 7294, Loss: 0.6606089621782303, Final Batch Loss: 0.14121247828006744\n",
      "Epoch 7295, Loss: 0.7265789210796356, Final Batch Loss: 0.18646329641342163\n",
      "Epoch 7296, Loss: 0.7486983686685562, Final Batch Loss: 0.16434748470783234\n",
      "Epoch 7297, Loss: 0.7748340964317322, Final Batch Loss: 0.15703116357326508\n",
      "Epoch 7298, Loss: 0.5923194140195847, Final Batch Loss: 0.12565302848815918\n",
      "Epoch 7299, Loss: 0.6390949636697769, Final Batch Loss: 0.16040906310081482\n",
      "Epoch 7300, Loss: 0.5892907604575157, Final Batch Loss: 0.09994297474622726\n",
      "Epoch 7301, Loss: 0.7930136024951935, Final Batch Loss: 0.2651559114456177\n",
      "Epoch 7302, Loss: 0.6434216052293777, Final Batch Loss: 0.12948046624660492\n",
      "Epoch 7303, Loss: 0.726240485906601, Final Batch Loss: 0.17597229778766632\n",
      "Epoch 7304, Loss: 0.7505969628691673, Final Batch Loss: 0.12436496466398239\n",
      "Epoch 7305, Loss: 0.7796871662139893, Final Batch Loss: 0.2003859430551529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7306, Loss: 0.7437122464179993, Final Batch Loss: 0.1918870210647583\n",
      "Epoch 7307, Loss: 0.6073914021253586, Final Batch Loss: 0.17928142845630646\n",
      "Epoch 7308, Loss: 0.7230002209544182, Final Batch Loss: 0.2319558709859848\n",
      "Epoch 7309, Loss: 0.7018192410469055, Final Batch Loss: 0.14833343029022217\n",
      "Epoch 7310, Loss: 0.7327262684702873, Final Batch Loss: 0.15128310024738312\n",
      "Epoch 7311, Loss: 0.7907265424728394, Final Batch Loss: 0.16431163251399994\n",
      "Epoch 7312, Loss: 0.6856254488229752, Final Batch Loss: 0.17924299836158752\n",
      "Epoch 7313, Loss: 0.7843794375658035, Final Batch Loss: 0.2286391407251358\n",
      "Epoch 7314, Loss: 0.7153209000825882, Final Batch Loss: 0.19091002643108368\n",
      "Epoch 7315, Loss: 0.6927280128002167, Final Batch Loss: 0.15633629262447357\n",
      "Epoch 7316, Loss: 0.7885891646146774, Final Batch Loss: 0.16305884718894958\n",
      "Epoch 7317, Loss: 0.8860556185245514, Final Batch Loss: 0.2090790569782257\n",
      "Epoch 7318, Loss: 0.8159442096948624, Final Batch Loss: 0.2562890946865082\n",
      "Epoch 7319, Loss: 0.8094218224287033, Final Batch Loss: 0.17438757419586182\n",
      "Epoch 7320, Loss: 1.05890454351902, Final Batch Loss: 0.2853491008281708\n",
      "Epoch 7321, Loss: 0.7955798953771591, Final Batch Loss: 0.20778590440750122\n",
      "Epoch 7322, Loss: 0.9442660212516785, Final Batch Loss: 0.2714076340198517\n",
      "Epoch 7323, Loss: 0.7428871095180511, Final Batch Loss: 0.15368221700191498\n",
      "Epoch 7324, Loss: 0.883514940738678, Final Batch Loss: 0.177058145403862\n",
      "Epoch 7325, Loss: 0.8773856163024902, Final Batch Loss: 0.27626731991767883\n",
      "Epoch 7326, Loss: 0.7525695562362671, Final Batch Loss: 0.21067462861537933\n",
      "Epoch 7327, Loss: 0.7247884124517441, Final Batch Loss: 0.14269299805164337\n",
      "Epoch 7328, Loss: 0.9043653607368469, Final Batch Loss: 0.15944720804691315\n",
      "Epoch 7329, Loss: 0.839403823018074, Final Batch Loss: 0.19941309094429016\n",
      "Epoch 7330, Loss: 0.7617624849081039, Final Batch Loss: 0.2116817682981491\n",
      "Epoch 7331, Loss: 0.6481525748968124, Final Batch Loss: 0.1392325758934021\n",
      "Epoch 7332, Loss: 0.8130632787942886, Final Batch Loss: 0.1672252118587494\n",
      "Epoch 7333, Loss: 0.7614613324403763, Final Batch Loss: 0.20696383714675903\n",
      "Epoch 7334, Loss: 0.6188724339008331, Final Batch Loss: 0.13454926013946533\n",
      "Epoch 7335, Loss: 0.9091162979602814, Final Batch Loss: 0.25050580501556396\n",
      "Epoch 7336, Loss: 0.6310390681028366, Final Batch Loss: 0.11547647416591644\n",
      "Epoch 7337, Loss: 0.737889289855957, Final Batch Loss: 0.20167690515518188\n",
      "Epoch 7338, Loss: 0.7912503033876419, Final Batch Loss: 0.21665889024734497\n",
      "Epoch 7339, Loss: 0.7295630723237991, Final Batch Loss: 0.19190789759159088\n",
      "Epoch 7340, Loss: 0.6821869611740112, Final Batch Loss: 0.14939476549625397\n",
      "Epoch 7341, Loss: 0.7027100920677185, Final Batch Loss: 0.2032691240310669\n",
      "Epoch 7342, Loss: 0.6883882731199265, Final Batch Loss: 0.14204582571983337\n",
      "Epoch 7343, Loss: 0.7518978118896484, Final Batch Loss: 0.26848194003105164\n",
      "Epoch 7344, Loss: 0.7281434237957001, Final Batch Loss: 0.1718408465385437\n",
      "Epoch 7345, Loss: 0.7795145511627197, Final Batch Loss: 0.1968740075826645\n",
      "Epoch 7346, Loss: 0.7028983533382416, Final Batch Loss: 0.17664748430252075\n",
      "Epoch 7347, Loss: 0.6019231826066971, Final Batch Loss: 0.16609062254428864\n",
      "Epoch 7348, Loss: 0.6985473036766052, Final Batch Loss: 0.1677895188331604\n",
      "Epoch 7349, Loss: 0.7706843763589859, Final Batch Loss: 0.2309841811656952\n",
      "Epoch 7350, Loss: 0.7238043248653412, Final Batch Loss: 0.21106615662574768\n",
      "Epoch 7351, Loss: 0.7239020392298698, Final Batch Loss: 0.15864984691143036\n",
      "Epoch 7352, Loss: 0.8009135872125626, Final Batch Loss: 0.19872017204761505\n",
      "Epoch 7353, Loss: 0.737098217010498, Final Batch Loss: 0.1348118633031845\n",
      "Epoch 7354, Loss: 0.7457492277026176, Final Batch Loss: 0.19784118235111237\n",
      "Epoch 7355, Loss: 0.8271857425570488, Final Batch Loss: 0.1156826838850975\n",
      "Epoch 7356, Loss: 0.6798406392335892, Final Batch Loss: 0.17644356191158295\n",
      "Epoch 7357, Loss: 0.671888530254364, Final Batch Loss: 0.18704316020011902\n",
      "Epoch 7358, Loss: 0.6734260469675064, Final Batch Loss: 0.12589015066623688\n",
      "Epoch 7359, Loss: 0.788416400551796, Final Batch Loss: 0.18443498015403748\n",
      "Epoch 7360, Loss: 0.7661499977111816, Final Batch Loss: 0.14343693852424622\n",
      "Epoch 7361, Loss: 0.8486392349004745, Final Batch Loss: 0.2300492823123932\n",
      "Epoch 7362, Loss: 0.6167254894971848, Final Batch Loss: 0.15896159410476685\n",
      "Epoch 7363, Loss: 0.8012347370386124, Final Batch Loss: 0.24794961512088776\n",
      "Epoch 7364, Loss: 0.8446467369794846, Final Batch Loss: 0.30696937441825867\n",
      "Epoch 7365, Loss: 0.7493801712989807, Final Batch Loss: 0.11929014325141907\n",
      "Epoch 7366, Loss: 0.7219903767108917, Final Batch Loss: 0.12382298707962036\n",
      "Epoch 7367, Loss: 0.7652581632137299, Final Batch Loss: 0.2137415111064911\n",
      "Epoch 7368, Loss: 0.6012187674641609, Final Batch Loss: 0.10147202759981155\n",
      "Epoch 7369, Loss: 0.6310530751943588, Final Batch Loss: 0.19659648835659027\n",
      "Epoch 7370, Loss: 0.6054029911756516, Final Batch Loss: 0.1322105973958969\n",
      "Epoch 7371, Loss: 0.7702577710151672, Final Batch Loss: 0.181676983833313\n",
      "Epoch 7372, Loss: 0.6279589533805847, Final Batch Loss: 0.10711033642292023\n",
      "Epoch 7373, Loss: 0.7413931787014008, Final Batch Loss: 0.18449653685092926\n",
      "Epoch 7374, Loss: 0.6563132405281067, Final Batch Loss: 0.18131501972675323\n",
      "Epoch 7375, Loss: 0.6654734164476395, Final Batch Loss: 0.1317732185125351\n",
      "Epoch 7376, Loss: 0.7305038422346115, Final Batch Loss: 0.16269519925117493\n",
      "Epoch 7377, Loss: 0.7929192185401917, Final Batch Loss: 0.15216736495494843\n",
      "Epoch 7378, Loss: 0.6498028934001923, Final Batch Loss: 0.16632908582687378\n",
      "Epoch 7379, Loss: 0.7711460441350937, Final Batch Loss: 0.1492285132408142\n",
      "Epoch 7380, Loss: 0.6335733830928802, Final Batch Loss: 0.14601927995681763\n",
      "Epoch 7381, Loss: 0.6896398812532425, Final Batch Loss: 0.2198079228401184\n",
      "Epoch 7382, Loss: 0.7014601677656174, Final Batch Loss: 0.22825153172016144\n",
      "Epoch 7383, Loss: 0.8342252373695374, Final Batch Loss: 0.23824085295200348\n",
      "Epoch 7384, Loss: 0.7178703397512436, Final Batch Loss: 0.24981015920639038\n",
      "Epoch 7385, Loss: 0.7351187616586685, Final Batch Loss: 0.1612638682126999\n",
      "Epoch 7386, Loss: 0.7702493518590927, Final Batch Loss: 0.23035451769828796\n",
      "Epoch 7387, Loss: 0.7770137339830399, Final Batch Loss: 0.23401254415512085\n",
      "Epoch 7388, Loss: 0.7050479054450989, Final Batch Loss: 0.16080689430236816\n",
      "Epoch 7389, Loss: 0.7301688492298126, Final Batch Loss: 0.167935311794281\n",
      "Epoch 7390, Loss: 0.7402488514780998, Final Batch Loss: 0.10967246443033218\n",
      "Epoch 7391, Loss: 0.7404734492301941, Final Batch Loss: 0.23222380876541138\n",
      "Epoch 7392, Loss: 0.7544003278017044, Final Batch Loss: 0.20894044637680054\n",
      "Epoch 7393, Loss: 0.6233416646718979, Final Batch Loss: 0.1416759490966797\n",
      "Epoch 7394, Loss: 0.7842582613229752, Final Batch Loss: 0.22666215896606445\n",
      "Epoch 7395, Loss: 0.7625100761651993, Final Batch Loss: 0.14989247918128967\n",
      "Epoch 7396, Loss: 0.7812801450490952, Final Batch Loss: 0.13560883700847626\n",
      "Epoch 7397, Loss: 0.6767621338367462, Final Batch Loss: 0.15313570201396942\n",
      "Epoch 7398, Loss: 0.7937520891427994, Final Batch Loss: 0.1986057311296463\n",
      "Epoch 7399, Loss: 0.8676611483097076, Final Batch Loss: 0.3098551034927368\n",
      "Epoch 7400, Loss: 0.7114752382040024, Final Batch Loss: 0.14948521554470062\n",
      "Epoch 7401, Loss: 0.7702828794717789, Final Batch Loss: 0.14291274547576904\n",
      "Epoch 7402, Loss: 0.8896382674574852, Final Batch Loss: 0.3714139759540558\n",
      "Epoch 7403, Loss: 0.7687074244022369, Final Batch Loss: 0.20346462726593018\n",
      "Epoch 7404, Loss: 0.7812986969947815, Final Batch Loss: 0.2539568543434143\n",
      "Epoch 7405, Loss: 0.7139706164598465, Final Batch Loss: 0.15512514114379883\n",
      "Epoch 7406, Loss: 0.8703657984733582, Final Batch Loss: 0.3127332329750061\n",
      "Epoch 7407, Loss: 0.666235938668251, Final Batch Loss: 0.16924887895584106\n",
      "Epoch 7408, Loss: 0.8433445394039154, Final Batch Loss: 0.2562982439994812\n",
      "Epoch 7409, Loss: 0.7451174259185791, Final Batch Loss: 0.19799542427062988\n",
      "Epoch 7410, Loss: 0.7734116911888123, Final Batch Loss: 0.2002439796924591\n",
      "Epoch 7411, Loss: 0.6976692825555801, Final Batch Loss: 0.19348593056201935\n",
      "Epoch 7412, Loss: 0.6658460944890976, Final Batch Loss: 0.20451487600803375\n",
      "Epoch 7413, Loss: 0.7112701386213303, Final Batch Loss: 0.12003999948501587\n",
      "Epoch 7414, Loss: 0.9161434024572372, Final Batch Loss: 0.22644728422164917\n",
      "Epoch 7415, Loss: 0.7716306746006012, Final Batch Loss: 0.2698783874511719\n",
      "Epoch 7416, Loss: 0.7225660383701324, Final Batch Loss: 0.10937260091304779\n",
      "Epoch 7417, Loss: 0.7080901861190796, Final Batch Loss: 0.23706205189228058\n",
      "Epoch 7418, Loss: 0.7827523499727249, Final Batch Loss: 0.15724094212055206\n",
      "Epoch 7419, Loss: 0.7897890210151672, Final Batch Loss: 0.2069721668958664\n",
      "Epoch 7420, Loss: 0.672847181558609, Final Batch Loss: 0.1828199326992035\n",
      "Epoch 7421, Loss: 0.8769686073064804, Final Batch Loss: 0.18423569202423096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7422, Loss: 0.6954921782016754, Final Batch Loss: 0.13447315990924835\n",
      "Epoch 7423, Loss: 0.7908840775489807, Final Batch Loss: 0.20600339770317078\n",
      "Epoch 7424, Loss: 0.7046161890029907, Final Batch Loss: 0.15418830513954163\n",
      "Epoch 7425, Loss: 0.7877000868320465, Final Batch Loss: 0.1627270132303238\n",
      "Epoch 7426, Loss: 0.6966813504695892, Final Batch Loss: 0.15881918370723724\n",
      "Epoch 7427, Loss: 0.8445151001214981, Final Batch Loss: 0.14849719405174255\n",
      "Epoch 7428, Loss: 0.7619578987360001, Final Batch Loss: 0.1799268126487732\n",
      "Epoch 7429, Loss: 0.6852237582206726, Final Batch Loss: 0.1378197968006134\n",
      "Epoch 7430, Loss: 1.0722867250442505, Final Batch Loss: 0.24748961627483368\n",
      "Epoch 7431, Loss: 0.8798613250255585, Final Batch Loss: 0.19042257964611053\n",
      "Epoch 7432, Loss: 0.8293455690145493, Final Batch Loss: 0.20238366723060608\n",
      "Epoch 7433, Loss: 0.7473358809947968, Final Batch Loss: 0.23013003170490265\n",
      "Epoch 7434, Loss: 0.7716814130544662, Final Batch Loss: 0.23790079355239868\n",
      "Epoch 7435, Loss: 0.7146833837032318, Final Batch Loss: 0.1557691991329193\n",
      "Epoch 7436, Loss: 0.747229278087616, Final Batch Loss: 0.18983624875545502\n",
      "Epoch 7437, Loss: 0.6653159707784653, Final Batch Loss: 0.14528372883796692\n",
      "Epoch 7438, Loss: 0.7587073296308517, Final Batch Loss: 0.23261003196239471\n",
      "Epoch 7439, Loss: 0.8191787078976631, Final Batch Loss: 0.2408469319343567\n",
      "Epoch 7440, Loss: 0.7805261015892029, Final Batch Loss: 0.2185419499874115\n",
      "Epoch 7441, Loss: 0.6220776587724686, Final Batch Loss: 0.14092910289764404\n",
      "Epoch 7442, Loss: 0.6663556471467018, Final Batch Loss: 0.16858598589897156\n",
      "Epoch 7443, Loss: 0.6897020488977432, Final Batch Loss: 0.203855499625206\n",
      "Epoch 7444, Loss: 0.8489819765090942, Final Batch Loss: 0.29209741950035095\n",
      "Epoch 7445, Loss: 0.697634294629097, Final Batch Loss: 0.183945432305336\n",
      "Epoch 7446, Loss: 0.7371717095375061, Final Batch Loss: 0.19919902086257935\n",
      "Epoch 7447, Loss: 0.7329951822757721, Final Batch Loss: 0.20065747201442719\n",
      "Epoch 7448, Loss: 0.7618797570466995, Final Batch Loss: 0.1723630577325821\n",
      "Epoch 7449, Loss: 0.6940397769212723, Final Batch Loss: 0.19797031581401825\n",
      "Epoch 7450, Loss: 0.674825593829155, Final Batch Loss: 0.1775789111852646\n",
      "Epoch 7451, Loss: 0.7559518814086914, Final Batch Loss: 0.2268775850534439\n",
      "Epoch 7452, Loss: 0.8108890205621719, Final Batch Loss: 0.2789779305458069\n",
      "Epoch 7453, Loss: 0.8275683969259262, Final Batch Loss: 0.1968579739332199\n",
      "Epoch 7454, Loss: 0.7628354877233505, Final Batch Loss: 0.14972969889640808\n",
      "Epoch 7455, Loss: 0.6288893818855286, Final Batch Loss: 0.15011680126190186\n",
      "Epoch 7456, Loss: 0.6795837432146072, Final Batch Loss: 0.16445285081863403\n",
      "Epoch 7457, Loss: 0.7799278050661087, Final Batch Loss: 0.2222207635641098\n",
      "Epoch 7458, Loss: 0.6973174959421158, Final Batch Loss: 0.14293137192726135\n",
      "Epoch 7459, Loss: 0.7227845638990402, Final Batch Loss: 0.18957559764385223\n",
      "Epoch 7460, Loss: 0.6413318067789078, Final Batch Loss: 0.19506806135177612\n",
      "Epoch 7461, Loss: 0.7467858642339706, Final Batch Loss: 0.24773722887039185\n",
      "Epoch 7462, Loss: 0.7851066142320633, Final Batch Loss: 0.20147335529327393\n",
      "Epoch 7463, Loss: 0.6719111800193787, Final Batch Loss: 0.13718445599079132\n",
      "Epoch 7464, Loss: 0.642966128885746, Final Batch Loss: 0.1779395490884781\n",
      "Epoch 7465, Loss: 0.7482609301805496, Final Batch Loss: 0.15348194539546967\n",
      "Epoch 7466, Loss: 0.53834118694067, Final Batch Loss: 0.07582380622625351\n",
      "Epoch 7467, Loss: 0.6065238267183304, Final Batch Loss: 0.08691079914569855\n",
      "Epoch 7468, Loss: 0.6096322983503342, Final Batch Loss: 0.15844686329364777\n",
      "Epoch 7469, Loss: 0.7662602663040161, Final Batch Loss: 0.1538325995206833\n",
      "Epoch 7470, Loss: 0.6792822778224945, Final Batch Loss: 0.1523532271385193\n",
      "Epoch 7471, Loss: 0.8004771173000336, Final Batch Loss: 0.1836954951286316\n",
      "Epoch 7472, Loss: 0.8203465193510056, Final Batch Loss: 0.19502507150173187\n",
      "Epoch 7473, Loss: 0.6427255719900131, Final Batch Loss: 0.11978358030319214\n",
      "Epoch 7474, Loss: 0.6992440521717072, Final Batch Loss: 0.13273531198501587\n",
      "Epoch 7475, Loss: 0.774209052324295, Final Batch Loss: 0.24819152057170868\n",
      "Epoch 7476, Loss: 0.7982516586780548, Final Batch Loss: 0.17609049379825592\n",
      "Epoch 7477, Loss: 0.7381860017776489, Final Batch Loss: 0.2657766044139862\n",
      "Epoch 7478, Loss: 0.8852306753396988, Final Batch Loss: 0.3525349497795105\n",
      "Epoch 7479, Loss: 0.774349108338356, Final Batch Loss: 0.16253243386745453\n",
      "Epoch 7480, Loss: 0.7461363226175308, Final Batch Loss: 0.14252646267414093\n",
      "Epoch 7481, Loss: 0.7901020646095276, Final Batch Loss: 0.20465566217899323\n",
      "Epoch 7482, Loss: 0.6764084547758102, Final Batch Loss: 0.13436076045036316\n",
      "Epoch 7483, Loss: 0.7519081085920334, Final Batch Loss: 0.217976376414299\n",
      "Epoch 7484, Loss: 0.6792858839035034, Final Batch Loss: 0.17244721949100494\n",
      "Epoch 7485, Loss: 0.6817835718393326, Final Batch Loss: 0.12970179319381714\n",
      "Epoch 7486, Loss: 0.7666478306055069, Final Batch Loss: 0.23305608332157135\n",
      "Epoch 7487, Loss: 0.7950657606124878, Final Batch Loss: 0.2604268193244934\n",
      "Epoch 7488, Loss: 0.7954529523849487, Final Batch Loss: 0.1618693470954895\n",
      "Epoch 7489, Loss: 0.9249133616685867, Final Batch Loss: 0.39639532566070557\n",
      "Epoch 7490, Loss: 0.7534269243478775, Final Batch Loss: 0.19566114246845245\n",
      "Epoch 7491, Loss: 0.6686775535345078, Final Batch Loss: 0.12725067138671875\n",
      "Epoch 7492, Loss: 0.7967706471681595, Final Batch Loss: 0.19906775653362274\n",
      "Epoch 7493, Loss: 0.696746751666069, Final Batch Loss: 0.1702278107404709\n",
      "Epoch 7494, Loss: 1.042851522564888, Final Batch Loss: 0.34362804889678955\n",
      "Epoch 7495, Loss: 0.6988517642021179, Final Batch Loss: 0.1822923868894577\n",
      "Epoch 7496, Loss: 0.6627118289470673, Final Batch Loss: 0.144558846950531\n",
      "Epoch 7497, Loss: 0.6900967508554459, Final Batch Loss: 0.20266956090927124\n",
      "Epoch 7498, Loss: 0.8312641233205795, Final Batch Loss: 0.17367351055145264\n",
      "Epoch 7499, Loss: 0.707492969930172, Final Batch Loss: 0.2493240386247635\n",
      "Epoch 7500, Loss: 0.7350434213876724, Final Batch Loss: 0.0854109525680542\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30  0  0  0  1  0  0]\n",
      " [ 0 17  0  2  0  0  1]\n",
      " [ 0  1 22  2  1  0  0]\n",
      " [ 0  0  0 17  0  0  1]\n",
      " [ 0  2  0  0 20  0  1]\n",
      " [ 0  1  0  0  0 26  0]\n",
      " [ 0  0  0  0  0  0 20]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.968     0.984        31\n",
      "           1      0.810     0.850     0.829        20\n",
      "           2      1.000     0.846     0.917        26\n",
      "           3      0.810     0.944     0.872        18\n",
      "           4      0.909     0.870     0.889        23\n",
      "           5      1.000     0.963     0.981        27\n",
      "           6      0.870     1.000     0.930        20\n",
      "\n",
      "    accuracy                          0.921       165\n",
      "   macro avg      0.914     0.920     0.915       165\n",
      "weighted avg      0.928     0.921     0.922       165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'../../../saved_models/UCI 7 User Classifier Ablation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
