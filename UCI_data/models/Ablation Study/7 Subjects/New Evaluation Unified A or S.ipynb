{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '90 tBodyAccJerk-max()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Activity_Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Activity_Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "class Subject_Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Subject_Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 7)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def get_act_matrix(batch_size, a_dim):\n",
    "    indexes = np.random.randint(a_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((len(indexes), indexes.max()+1))\n",
    "    one_hot[np.arange(len(indexes)),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "    \n",
    "def get_usr_matrix(batch_size, u_dim):\n",
    "    indexes = np.random.randint(u_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((indexes.size, indexes.max()+1))\n",
    "    one_hot[np.arange(indexes.size),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "    \n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_train = GAN_data.iloc[:,:-2].values\n",
    "    y_train = GAN_data[[y_label]].values\n",
    "    \n",
    "    return X_train, y_train.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5, 7, 8, 11, 14]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y)):\n",
    "    if y[k] == 1:\n",
    "        y[k] = 0\n",
    "    elif y[k] == 3:\n",
    "        y[k] = 1\n",
    "    else:\n",
    "        y[k] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Activity_Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.407615065574646, Final Batch Loss: 1.1043334007263184\n",
      "Epoch 2, Loss: 4.398714065551758, Final Batch Loss: 1.108474612236023\n",
      "Epoch 3, Loss: 4.385259985923767, Final Batch Loss: 1.1003540754318237\n",
      "Epoch 4, Loss: 4.363013863563538, Final Batch Loss: 1.0798442363739014\n",
      "Epoch 5, Loss: 4.355208516120911, Final Batch Loss: 1.091536045074463\n",
      "Epoch 6, Loss: 4.3286179304122925, Final Batch Loss: 1.079339623451233\n",
      "Epoch 7, Loss: 4.304621458053589, Final Batch Loss: 1.0723742246627808\n",
      "Epoch 8, Loss: 4.264464855194092, Final Batch Loss: 1.0609840154647827\n",
      "Epoch 9, Loss: 4.220078706741333, Final Batch Loss: 1.0450940132141113\n",
      "Epoch 10, Loss: 4.162905812263489, Final Batch Loss: 1.0374436378479004\n",
      "Epoch 11, Loss: 4.086629152297974, Final Batch Loss: 1.0132759809494019\n",
      "Epoch 12, Loss: 3.9892364144325256, Final Batch Loss: 0.9896432757377625\n",
      "Epoch 13, Loss: 3.852113425731659, Final Batch Loss: 0.9519662857055664\n",
      "Epoch 14, Loss: 3.6816746592521667, Final Batch Loss: 0.8905184864997864\n",
      "Epoch 15, Loss: 3.4845386743545532, Final Batch Loss: 0.8549730181694031\n",
      "Epoch 16, Loss: 3.279770076274872, Final Batch Loss: 0.8115411400794983\n",
      "Epoch 17, Loss: 3.047953426837921, Final Batch Loss: 0.7413334846496582\n",
      "Epoch 18, Loss: 2.831060469150543, Final Batch Loss: 0.6982284188270569\n",
      "Epoch 19, Loss: 2.643092632293701, Final Batch Loss: 0.6834182143211365\n",
      "Epoch 20, Loss: 2.324079990386963, Final Batch Loss: 0.5020159482955933\n",
      "Epoch 21, Loss: 2.1955575942993164, Final Batch Loss: 0.5197644829750061\n",
      "Epoch 22, Loss: 2.04207044839859, Final Batch Loss: 0.5203410387039185\n",
      "Epoch 23, Loss: 1.8308003842830658, Final Batch Loss: 0.405929297208786\n",
      "Epoch 24, Loss: 1.6889106631278992, Final Batch Loss: 0.4500617980957031\n",
      "Epoch 25, Loss: 1.485111266374588, Final Batch Loss: 0.3443152606487274\n",
      "Epoch 26, Loss: 1.3651018142700195, Final Batch Loss: 0.31831449270248413\n",
      "Epoch 27, Loss: 1.338177502155304, Final Batch Loss: 0.4018065333366394\n",
      "Epoch 28, Loss: 1.1535049080848694, Final Batch Loss: 0.27777940034866333\n",
      "Epoch 29, Loss: 1.044413611292839, Final Batch Loss: 0.26916930079460144\n",
      "Epoch 30, Loss: 0.9974624812602997, Final Batch Loss: 0.2299698293209076\n",
      "Epoch 31, Loss: 0.8666756898164749, Final Batch Loss: 0.20670337975025177\n",
      "Epoch 32, Loss: 0.8468064069747925, Final Batch Loss: 0.22011609375476837\n",
      "Epoch 33, Loss: 0.875182718038559, Final Batch Loss: 0.15247155725955963\n",
      "Epoch 34, Loss: 0.9045436978340149, Final Batch Loss: 0.21202591061592102\n",
      "Epoch 35, Loss: 0.8019742369651794, Final Batch Loss: 0.13547973334789276\n",
      "Epoch 36, Loss: 0.6784244924783707, Final Batch Loss: 0.17829769849777222\n",
      "Epoch 37, Loss: 0.7518375217914581, Final Batch Loss: 0.25515908002853394\n",
      "Epoch 38, Loss: 0.6747945547103882, Final Batch Loss: 0.1693086475133896\n",
      "Epoch 39, Loss: 0.5731141418218613, Final Batch Loss: 0.12334556877613068\n",
      "Epoch 40, Loss: 0.6766768246889114, Final Batch Loss: 0.14882083237171173\n",
      "Epoch 41, Loss: 0.768135592341423, Final Batch Loss: 0.17528384923934937\n",
      "Epoch 42, Loss: 0.5758114159107208, Final Batch Loss: 0.09615251421928406\n",
      "Epoch 43, Loss: 0.6602281481027603, Final Batch Loss: 0.1883487105369568\n",
      "Epoch 44, Loss: 0.6186149418354034, Final Batch Loss: 0.17913730442523956\n",
      "Epoch 45, Loss: 0.5745761841535568, Final Batch Loss: 0.11814506351947784\n",
      "Epoch 46, Loss: 0.6653222441673279, Final Batch Loss: 0.14533765614032745\n",
      "Epoch 47, Loss: 0.6746320873498917, Final Batch Loss: 0.20402462780475616\n",
      "Epoch 48, Loss: 0.5492596104741096, Final Batch Loss: 0.14175406098365784\n",
      "Epoch 49, Loss: 0.5868869498372078, Final Batch Loss: 0.17333798110485077\n",
      "Epoch 50, Loss: 0.5519736707210541, Final Batch Loss: 0.14611107110977173\n",
      "Epoch 51, Loss: 0.5476671531796455, Final Batch Loss: 0.11180279403924942\n",
      "Epoch 52, Loss: 0.5336311683058739, Final Batch Loss: 0.11806298047304153\n",
      "Epoch 53, Loss: 0.45097440481185913, Final Batch Loss: 0.0877692922949791\n",
      "Epoch 54, Loss: 0.46205686032772064, Final Batch Loss: 0.08123421669006348\n",
      "Epoch 55, Loss: 0.5606899261474609, Final Batch Loss: 0.13044287264347076\n",
      "Epoch 56, Loss: 0.5123968124389648, Final Batch Loss: 0.1407109498977661\n",
      "Epoch 57, Loss: 0.5855585858225822, Final Batch Loss: 0.18755891919136047\n",
      "Epoch 58, Loss: 0.46232009679079056, Final Batch Loss: 0.11340824514627457\n",
      "Epoch 59, Loss: 0.46977587789297104, Final Batch Loss: 0.09714767336845398\n",
      "Epoch 60, Loss: 0.5093003213405609, Final Batch Loss: 0.09331758320331573\n",
      "Epoch 61, Loss: 0.39341893419623375, Final Batch Loss: 0.11069352179765701\n",
      "Epoch 62, Loss: 0.4693833887577057, Final Batch Loss: 0.11453404277563095\n",
      "Epoch 63, Loss: 0.44817736744880676, Final Batch Loss: 0.1405363231897354\n",
      "Epoch 64, Loss: 0.37432029843330383, Final Batch Loss: 0.08343233168125153\n",
      "Epoch 65, Loss: 0.4703618660569191, Final Batch Loss: 0.13767842948436737\n",
      "Epoch 66, Loss: 0.4033718481659889, Final Batch Loss: 0.06548969447612762\n",
      "Epoch 67, Loss: 0.40310244262218475, Final Batch Loss: 0.13895563781261444\n",
      "Epoch 68, Loss: 0.46681997179985046, Final Batch Loss: 0.12774014472961426\n",
      "Epoch 69, Loss: 0.4648422300815582, Final Batch Loss: 0.13135385513305664\n",
      "Epoch 70, Loss: 0.40387073904275894, Final Batch Loss: 0.08655902743339539\n",
      "Epoch 71, Loss: 0.39236167818307877, Final Batch Loss: 0.10498620569705963\n",
      "Epoch 72, Loss: 0.42691222578287125, Final Batch Loss: 0.10598960518836975\n",
      "Epoch 73, Loss: 0.43028590828180313, Final Batch Loss: 0.1248934417963028\n",
      "Epoch 74, Loss: 0.4388667158782482, Final Batch Loss: 0.11219893395900726\n",
      "Epoch 75, Loss: 0.3552866503596306, Final Batch Loss: 0.07659997045993805\n",
      "Epoch 76, Loss: 0.4050835147500038, Final Batch Loss: 0.09873279929161072\n",
      "Epoch 77, Loss: 0.3921234905719757, Final Batch Loss: 0.13389283418655396\n",
      "Epoch 78, Loss: 0.3734758421778679, Final Batch Loss: 0.06525955349206924\n",
      "Epoch 79, Loss: 0.4066544324159622, Final Batch Loss: 0.1146903857588768\n",
      "Epoch 80, Loss: 0.36744479835033417, Final Batch Loss: 0.1311546117067337\n",
      "Epoch 81, Loss: 0.35760289803147316, Final Batch Loss: 0.060498107224702835\n",
      "Epoch 82, Loss: 0.37565598264336586, Final Batch Loss: 0.10620336979627609\n",
      "Epoch 83, Loss: 0.3579115942120552, Final Batch Loss: 0.05058354139328003\n",
      "Epoch 84, Loss: 0.34442581981420517, Final Batch Loss: 0.0765291079878807\n",
      "Epoch 85, Loss: 0.3716641664505005, Final Batch Loss: 0.081362783908844\n",
      "Epoch 86, Loss: 0.4517566077411175, Final Batch Loss: 0.20916084945201874\n",
      "Epoch 87, Loss: 0.38695891574025154, Final Batch Loss: 0.13262325525283813\n",
      "Epoch 88, Loss: 0.25182951614260674, Final Batch Loss: 0.04056895524263382\n",
      "Epoch 89, Loss: 0.38427963107824326, Final Batch Loss: 0.15624168515205383\n",
      "Epoch 90, Loss: 0.28953785821795464, Final Batch Loss: 0.08065123856067657\n",
      "Epoch 91, Loss: 0.2659064382314682, Final Batch Loss: 0.03150555491447449\n",
      "Epoch 92, Loss: 0.35412629321217537, Final Batch Loss: 0.13593803346157074\n",
      "Epoch 93, Loss: 0.2915562018752098, Final Batch Loss: 0.07696763426065445\n",
      "Epoch 94, Loss: 0.3728032112121582, Final Batch Loss: 0.12222819775342941\n",
      "Epoch 95, Loss: 0.30820271000266075, Final Batch Loss: 0.05160442367196083\n",
      "Epoch 96, Loss: 0.2682303860783577, Final Batch Loss: 0.06864553689956665\n",
      "Epoch 97, Loss: 0.28677821531891823, Final Batch Loss: 0.08830475062131882\n",
      "Epoch 98, Loss: 0.34684450179338455, Final Batch Loss: 0.06872089952230453\n",
      "Epoch 99, Loss: 0.2798451855778694, Final Batch Loss: 0.11436845362186432\n",
      "Epoch 100, Loss: 0.27922607585787773, Final Batch Loss: 0.10419513285160065\n",
      "Epoch 101, Loss: 0.39481422305107117, Final Batch Loss: 0.14464500546455383\n",
      "Epoch 102, Loss: 0.2996239960193634, Final Batch Loss: 0.10932419449090958\n",
      "Epoch 103, Loss: 0.26834601163864136, Final Batch Loss: 0.048049040138721466\n",
      "Epoch 104, Loss: 0.2969590537250042, Final Batch Loss: 0.07948493957519531\n",
      "Epoch 105, Loss: 0.2755552567541599, Final Batch Loss: 0.10297290235757828\n",
      "Epoch 106, Loss: 0.32871341705322266, Final Batch Loss: 0.08285664021968842\n",
      "Epoch 107, Loss: 0.3601984903216362, Final Batch Loss: 0.13926443457603455\n",
      "Epoch 108, Loss: 0.26688327267766, Final Batch Loss: 0.02077237144112587\n",
      "Epoch 109, Loss: 0.3032388240098953, Final Batch Loss: 0.09132588654756546\n",
      "Epoch 110, Loss: 0.29916196689009666, Final Batch Loss: 0.10338994860649109\n",
      "Epoch 111, Loss: 0.2369655929505825, Final Batch Loss: 0.03305613249540329\n",
      "Epoch 112, Loss: 0.3421134725213051, Final Batch Loss: 0.1230601966381073\n",
      "Epoch 113, Loss: 0.227631576359272, Final Batch Loss: 0.03668897971510887\n",
      "Epoch 114, Loss: 0.29958176985383034, Final Batch Loss: 0.0685349851846695\n",
      "Epoch 115, Loss: 0.32135896384716034, Final Batch Loss: 0.060406170785427094\n",
      "Epoch 116, Loss: 0.26616767421364784, Final Batch Loss: 0.08175671100616455\n",
      "Epoch 117, Loss: 0.29194318503141403, Final Batch Loss: 0.04217585176229477\n",
      "Epoch 118, Loss: 0.31949653662741184, Final Batch Loss: 0.08636652678251266\n",
      "Epoch 119, Loss: 0.26885510981082916, Final Batch Loss: 0.03296304866671562\n",
      "Epoch 120, Loss: 0.2670532092452049, Final Batch Loss: 0.07787040621042252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121, Loss: 0.20679648220539093, Final Batch Loss: 0.050010159611701965\n",
      "Epoch 122, Loss: 0.26600281894207, Final Batch Loss: 0.08677597343921661\n",
      "Epoch 123, Loss: 0.2739914432168007, Final Batch Loss: 0.05807482823729515\n",
      "Epoch 124, Loss: 0.2560640834271908, Final Batch Loss: 0.08005407452583313\n",
      "Epoch 125, Loss: 0.307220209389925, Final Batch Loss: 0.139631986618042\n",
      "Epoch 126, Loss: 0.2643237076699734, Final Batch Loss: 0.06467802822589874\n",
      "Epoch 127, Loss: 0.24748511612415314, Final Batch Loss: 0.06330756843090057\n",
      "Epoch 128, Loss: 0.22050448320806026, Final Batch Loss: 0.022290444001555443\n",
      "Epoch 129, Loss: 0.26227834820747375, Final Batch Loss: 0.054410867393016815\n",
      "Epoch 130, Loss: 0.2891252972185612, Final Batch Loss: 0.10203540325164795\n",
      "Epoch 131, Loss: 0.21887899935245514, Final Batch Loss: 0.04141683503985405\n",
      "Epoch 132, Loss: 0.2642732486128807, Final Batch Loss: 0.04446975514292717\n",
      "Epoch 133, Loss: 0.28763920441269875, Final Batch Loss: 0.1107063814997673\n",
      "Epoch 134, Loss: 0.24648195505142212, Final Batch Loss: 0.07075586169958115\n",
      "Epoch 135, Loss: 0.2475293017923832, Final Batch Loss: 0.11764253675937653\n",
      "Epoch 136, Loss: 0.21862404979765415, Final Batch Loss: 0.0545673705637455\n",
      "Epoch 137, Loss: 0.23581447079777718, Final Batch Loss: 0.048114072531461716\n",
      "Epoch 138, Loss: 0.30027782171964645, Final Batch Loss: 0.09883231669664383\n",
      "Epoch 139, Loss: 0.18845610320568085, Final Batch Loss: 0.03743262216448784\n",
      "Epoch 140, Loss: 0.27009929344058037, Final Batch Loss: 0.12157133221626282\n",
      "Epoch 141, Loss: 0.23329972848296165, Final Batch Loss: 0.06560448557138443\n",
      "Epoch 142, Loss: 0.2405322641134262, Final Batch Loss: 0.03525451570749283\n",
      "Epoch 143, Loss: 0.2658371664583683, Final Batch Loss: 0.09560845047235489\n",
      "Epoch 144, Loss: 0.24764036759734154, Final Batch Loss: 0.042822957038879395\n",
      "Epoch 145, Loss: 0.18620898574590683, Final Batch Loss: 0.03734428435564041\n",
      "Epoch 146, Loss: 0.21797829493880272, Final Batch Loss: 0.06591233611106873\n",
      "Epoch 147, Loss: 0.2675104960799217, Final Batch Loss: 0.10400016605854034\n",
      "Epoch 148, Loss: 0.25214146077632904, Final Batch Loss: 0.06298629939556122\n",
      "Epoch 149, Loss: 0.25352561473846436, Final Batch Loss: 0.05493849515914917\n",
      "Epoch 150, Loss: 0.20191575959324837, Final Batch Loss: 0.043947555124759674\n",
      "Epoch 151, Loss: 0.16825137846171856, Final Batch Loss: 0.026880009099841118\n",
      "Epoch 152, Loss: 0.2259812392294407, Final Batch Loss: 0.06458847969770432\n",
      "Epoch 153, Loss: 0.1666982276365161, Final Batch Loss: 0.00728484895080328\n",
      "Epoch 154, Loss: 0.21373451873660088, Final Batch Loss: 0.0326167494058609\n",
      "Epoch 155, Loss: 0.2253140527755022, Final Batch Loss: 0.025095874443650246\n",
      "Epoch 156, Loss: 0.2104069646447897, Final Batch Loss: 0.05525268241763115\n",
      "Epoch 157, Loss: 0.21491282805800438, Final Batch Loss: 0.07215078175067902\n",
      "Epoch 158, Loss: 0.221567552536726, Final Batch Loss: 0.06530595570802689\n",
      "Epoch 159, Loss: 0.23024782538414001, Final Batch Loss: 0.0384272038936615\n",
      "Epoch 160, Loss: 0.18673585541546345, Final Batch Loss: 0.02414165623486042\n",
      "Epoch 161, Loss: 0.24084129929542542, Final Batch Loss: 0.06070366129279137\n",
      "Epoch 162, Loss: 0.2103956714272499, Final Batch Loss: 0.07838220149278641\n",
      "Epoch 163, Loss: 0.19346961937844753, Final Batch Loss: 0.03061111457645893\n",
      "Epoch 164, Loss: 0.20267154835164547, Final Batch Loss: 0.07772407680749893\n",
      "Epoch 165, Loss: 0.2096244115382433, Final Batch Loss: 0.02898227982223034\n",
      "Epoch 166, Loss: 0.24987560883164406, Final Batch Loss: 0.08497589081525803\n",
      "Epoch 167, Loss: 0.2236422374844551, Final Batch Loss: 0.043682731688022614\n",
      "Epoch 168, Loss: 0.20389925315976143, Final Batch Loss: 0.0445341020822525\n",
      "Epoch 169, Loss: 0.18147825077176094, Final Batch Loss: 0.02888905629515648\n",
      "Epoch 170, Loss: 0.1982906609773636, Final Batch Loss: 0.04385269060730934\n",
      "Epoch 171, Loss: 0.17633874621242285, Final Batch Loss: 0.012721302919089794\n",
      "Epoch 172, Loss: 0.2316687274724245, Final Batch Loss: 0.055951885879039764\n",
      "Epoch 173, Loss: 0.2127143107354641, Final Batch Loss: 0.05876292288303375\n",
      "Epoch 174, Loss: 0.21308788284659386, Final Batch Loss: 0.03256258741021156\n",
      "Epoch 175, Loss: 0.228494206443429, Final Batch Loss: 0.09607148915529251\n",
      "Epoch 176, Loss: 0.21753376349806786, Final Batch Loss: 0.08207578957080841\n",
      "Epoch 177, Loss: 0.1953934971243143, Final Batch Loss: 0.06118010729551315\n",
      "Epoch 178, Loss: 0.17518560215830803, Final Batch Loss: 0.034869253635406494\n",
      "Epoch 179, Loss: 0.2554003931581974, Final Batch Loss: 0.0848187729716301\n",
      "Epoch 180, Loss: 0.1726028248667717, Final Batch Loss: 0.03420869633555412\n",
      "Epoch 181, Loss: 0.2003904990851879, Final Batch Loss: 0.03739092871546745\n",
      "Epoch 182, Loss: 0.1603315081447363, Final Batch Loss: 0.019545624032616615\n",
      "Epoch 183, Loss: 0.24184785410761833, Final Batch Loss: 0.0681866928935051\n",
      "Epoch 184, Loss: 0.17556816898286343, Final Batch Loss: 0.03513738140463829\n",
      "Epoch 185, Loss: 0.17657681740820408, Final Batch Loss: 0.042567119002342224\n",
      "Epoch 186, Loss: 0.16663062758743763, Final Batch Loss: 0.023611662909388542\n",
      "Epoch 187, Loss: 0.22370561212301254, Final Batch Loss: 0.04994170740246773\n",
      "Epoch 188, Loss: 0.18748555332422256, Final Batch Loss: 0.016201142221689224\n",
      "Epoch 189, Loss: 0.22177524864673615, Final Batch Loss: 0.05104546248912811\n",
      "Epoch 190, Loss: 0.18352094292640686, Final Batch Loss: 0.0550822913646698\n",
      "Epoch 191, Loss: 0.18709980696439743, Final Batch Loss: 0.037489213049411774\n",
      "Epoch 192, Loss: 0.18053551763296127, Final Batch Loss: 0.03943601995706558\n",
      "Epoch 193, Loss: 0.22463082894682884, Final Batch Loss: 0.07231418788433075\n",
      "Epoch 194, Loss: 0.18304304406046867, Final Batch Loss: 0.0666220560669899\n",
      "Epoch 195, Loss: 0.16634795628488064, Final Batch Loss: 0.053215451538562775\n",
      "Epoch 196, Loss: 0.1848972737789154, Final Batch Loss: 0.07697805017232895\n",
      "Epoch 197, Loss: 0.17204311676323414, Final Batch Loss: 0.02639644220471382\n",
      "Epoch 198, Loss: 0.179665669798851, Final Batch Loss: 0.04411725327372551\n",
      "Epoch 199, Loss: 0.1878506699576974, Final Batch Loss: 0.014439777471125126\n",
      "Epoch 200, Loss: 0.14126777276396751, Final Batch Loss: 0.020185288041830063\n",
      "Epoch 201, Loss: 0.17094771936535835, Final Batch Loss: 0.029337719082832336\n",
      "Epoch 202, Loss: 0.16422663815319538, Final Batch Loss: 0.03888877108693123\n",
      "Epoch 203, Loss: 0.1427270844578743, Final Batch Loss: 0.01837204024195671\n",
      "Epoch 204, Loss: 0.19862547144293785, Final Batch Loss: 0.037209562957286835\n",
      "Epoch 205, Loss: 0.17654616944491863, Final Batch Loss: 0.015999654307961464\n",
      "Epoch 206, Loss: 0.1954001635313034, Final Batch Loss: 0.0717790424823761\n",
      "Epoch 207, Loss: 0.18674423918128014, Final Batch Loss: 0.0339030958712101\n",
      "Epoch 208, Loss: 0.19356459379196167, Final Batch Loss: 0.05752070993185043\n",
      "Epoch 209, Loss: 0.21128295920789242, Final Batch Loss: 0.08404645323753357\n",
      "Epoch 210, Loss: 0.16859907191246748, Final Batch Loss: 0.0518014170229435\n",
      "Epoch 211, Loss: 0.18615030497312546, Final Batch Loss: 0.036964673548936844\n",
      "Epoch 212, Loss: 0.11807587370276451, Final Batch Loss: 0.03172096237540245\n",
      "Epoch 213, Loss: 0.2226041480898857, Final Batch Loss: 0.0735263004899025\n",
      "Epoch 214, Loss: 0.13882696069777012, Final Batch Loss: 0.034429628401994705\n",
      "Epoch 215, Loss: 0.1964244544506073, Final Batch Loss: 0.05288773030042648\n",
      "Epoch 216, Loss: 0.18387408554553986, Final Batch Loss: 0.08259187638759613\n",
      "Epoch 217, Loss: 0.18474997393786907, Final Batch Loss: 0.026148738339543343\n",
      "Epoch 218, Loss: 0.14889321476221085, Final Batch Loss: 0.020601317286491394\n",
      "Epoch 219, Loss: 0.15748150134459138, Final Batch Loss: 0.007322160992771387\n",
      "Epoch 220, Loss: 0.17541740648448467, Final Batch Loss: 0.058114320039749146\n",
      "Epoch 221, Loss: 0.13477865792810917, Final Batch Loss: 0.012642823159694672\n",
      "Epoch 222, Loss: 0.1683395253494382, Final Batch Loss: 0.011001874692738056\n",
      "Epoch 223, Loss: 0.16003047116100788, Final Batch Loss: 0.063361257314682\n",
      "Epoch 224, Loss: 0.20124530047178268, Final Batch Loss: 0.04961126297712326\n",
      "Epoch 225, Loss: 0.17576590739190578, Final Batch Loss: 0.042635440826416016\n",
      "Epoch 226, Loss: 0.16472863964736462, Final Batch Loss: 0.03341776132583618\n",
      "Epoch 227, Loss: 0.1489276885986328, Final Batch Loss: 0.030085360631346703\n",
      "Epoch 228, Loss: 0.17096661403775215, Final Batch Loss: 0.05175231024622917\n",
      "Epoch 229, Loss: 0.1830279678106308, Final Batch Loss: 0.062253132462501526\n",
      "Epoch 230, Loss: 0.2136593461036682, Final Batch Loss: 0.06359123438596725\n",
      "Epoch 231, Loss: 0.15222149714827538, Final Batch Loss: 0.042681872844696045\n",
      "Epoch 232, Loss: 0.16090377047657967, Final Batch Loss: 0.05019097402691841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233, Loss: 0.2255111914128065, Final Batch Loss: 0.0878181979060173\n",
      "Epoch 234, Loss: 0.18177083134651184, Final Batch Loss: 0.07615618407726288\n",
      "Epoch 235, Loss: 0.1527277585119009, Final Batch Loss: 0.028770051896572113\n",
      "Epoch 236, Loss: 0.17594588734209538, Final Batch Loss: 0.05166855826973915\n",
      "Epoch 237, Loss: 0.12012515496462584, Final Batch Loss: 0.010812350548803806\n",
      "Epoch 238, Loss: 0.14767546765506268, Final Batch Loss: 0.059348151087760925\n",
      "Epoch 239, Loss: 0.16020315513014793, Final Batch Loss: 0.042653556913137436\n",
      "Epoch 240, Loss: 0.14563021808862686, Final Batch Loss: 0.022155694663524628\n",
      "Epoch 241, Loss: 0.16684851795434952, Final Batch Loss: 0.02118539810180664\n",
      "Epoch 242, Loss: 0.14582477509975433, Final Batch Loss: 0.03969934955239296\n",
      "Epoch 243, Loss: 0.13185051549226046, Final Batch Loss: 0.03152872994542122\n",
      "Epoch 244, Loss: 0.18772580288350582, Final Batch Loss: 0.05964372679591179\n",
      "Epoch 245, Loss: 0.155909551307559, Final Batch Loss: 0.020080359652638435\n",
      "Epoch 246, Loss: 0.13136121723800898, Final Batch Loss: 0.011351917870342731\n",
      "Epoch 247, Loss: 0.11511043272912502, Final Batch Loss: 0.022136937826871872\n",
      "Epoch 248, Loss: 0.15127052180469036, Final Batch Loss: 0.03769141808152199\n",
      "Epoch 249, Loss: 0.16816934198141098, Final Batch Loss: 0.02036110684275627\n",
      "Epoch 250, Loss: 0.17947210371494293, Final Batch Loss: 0.08642803132534027\n",
      "Epoch 251, Loss: 0.1685604089871049, Final Batch Loss: 0.010843432508409023\n",
      "Epoch 252, Loss: 0.1368289291858673, Final Batch Loss: 0.040753111243247986\n",
      "Epoch 253, Loss: 0.11989559698849916, Final Batch Loss: 0.05366745591163635\n",
      "Epoch 254, Loss: 0.15803301893174648, Final Batch Loss: 0.04714014008641243\n",
      "Epoch 255, Loss: 0.13126234989613295, Final Batch Loss: 0.008611525408923626\n",
      "Epoch 256, Loss: 0.16334393434226513, Final Batch Loss: 0.06183646619319916\n",
      "Epoch 257, Loss: 0.11967294849455357, Final Batch Loss: 0.009600946679711342\n",
      "Epoch 258, Loss: 0.15380137786269188, Final Batch Loss: 0.04287533462047577\n",
      "Epoch 259, Loss: 0.16015720553696156, Final Batch Loss: 0.03907002881169319\n",
      "Epoch 260, Loss: 0.13728845305740833, Final Batch Loss: 0.024833280593156815\n",
      "Epoch 261, Loss: 0.15570882614701986, Final Batch Loss: 0.061981718987226486\n",
      "Epoch 262, Loss: 0.10713113658130169, Final Batch Loss: 0.009816901758313179\n",
      "Epoch 263, Loss: 0.13302327692508698, Final Batch Loss: 0.04508171230554581\n",
      "Epoch 264, Loss: 0.16948725655674934, Final Batch Loss: 0.030845487490296364\n",
      "Epoch 265, Loss: 0.15905173309147358, Final Batch Loss: 0.059649690985679626\n",
      "Epoch 266, Loss: 0.1342710144817829, Final Batch Loss: 0.016256114467978477\n",
      "Epoch 267, Loss: 0.13296953216195107, Final Batch Loss: 0.024037731811404228\n",
      "Epoch 268, Loss: 0.11829721741378307, Final Batch Loss: 0.013690339401364326\n",
      "Epoch 269, Loss: 0.18639265932142735, Final Batch Loss: 0.031114602461457253\n",
      "Epoch 270, Loss: 0.14240671880543232, Final Batch Loss: 0.035264912992715836\n",
      "Epoch 271, Loss: 0.16490065958350897, Final Batch Loss: 0.04906207323074341\n",
      "Epoch 272, Loss: 0.12238254677504301, Final Batch Loss: 0.03971220925450325\n",
      "Epoch 273, Loss: 0.12526243831962347, Final Batch Loss: 0.053619831800460815\n",
      "Epoch 274, Loss: 0.10240162396803498, Final Batch Loss: 0.02136852964758873\n",
      "Epoch 275, Loss: 0.16242706961929798, Final Batch Loss: 0.05998045951128006\n",
      "Epoch 276, Loss: 0.11256327293813229, Final Batch Loss: 0.013025002554059029\n",
      "Epoch 277, Loss: 0.13845948316156864, Final Batch Loss: 0.032043565064668655\n",
      "Epoch 278, Loss: 0.10699401423335075, Final Batch Loss: 0.016452686861157417\n",
      "Epoch 279, Loss: 0.15254002809524536, Final Batch Loss: 0.043699320405721664\n",
      "Epoch 280, Loss: 0.13078997284173965, Final Batch Loss: 0.014761121943593025\n",
      "Epoch 281, Loss: 0.13766570622101426, Final Batch Loss: 0.007594710681587458\n",
      "Epoch 282, Loss: 0.1147945262491703, Final Batch Loss: 0.017205912619829178\n",
      "Epoch 283, Loss: 0.159793047234416, Final Batch Loss: 0.06881332397460938\n",
      "Epoch 284, Loss: 0.11742662079632282, Final Batch Loss: 0.018693724647164345\n",
      "Epoch 285, Loss: 0.11706706695258617, Final Batch Loss: 0.012061910703778267\n",
      "Epoch 286, Loss: 0.1786442743614316, Final Batch Loss: 0.09952085465192795\n",
      "Epoch 287, Loss: 0.1354681784287095, Final Batch Loss: 0.04467116668820381\n",
      "Epoch 288, Loss: 0.14578774571418762, Final Batch Loss: 0.045683663338422775\n",
      "Epoch 289, Loss: 0.15047076903283596, Final Batch Loss: 0.041356008499860764\n",
      "Epoch 290, Loss: 0.1754486784338951, Final Batch Loss: 0.05048399418592453\n",
      "Epoch 291, Loss: 0.08787811733782291, Final Batch Loss: 0.015013204887509346\n",
      "Epoch 292, Loss: 0.12840929068624973, Final Batch Loss: 0.012171905487775803\n",
      "Epoch 293, Loss: 0.12025082111358643, Final Batch Loss: 0.020626846700906754\n",
      "Epoch 294, Loss: 0.14485855214297771, Final Batch Loss: 0.0527646467089653\n",
      "Epoch 295, Loss: 0.10785983316600323, Final Batch Loss: 0.03076484240591526\n",
      "Epoch 296, Loss: 0.15093624219298363, Final Batch Loss: 0.034672368317842484\n",
      "Epoch 297, Loss: 0.1548067294061184, Final Batch Loss: 0.06403673440217972\n",
      "Epoch 298, Loss: 0.15209981426596642, Final Batch Loss: 0.0571136437356472\n",
      "Epoch 299, Loss: 0.12722776271402836, Final Batch Loss: 0.024004332721233368\n",
      "Epoch 300, Loss: 0.10956058464944363, Final Batch Loss: 0.030332881957292557\n",
      "Epoch 301, Loss: 0.11430350504815578, Final Batch Loss: 0.008904222398996353\n",
      "Epoch 302, Loss: 0.14566147234290838, Final Batch Loss: 0.01640091836452484\n",
      "Epoch 303, Loss: 0.11597169004380703, Final Batch Loss: 0.029794687405228615\n",
      "Epoch 304, Loss: 0.1070078806951642, Final Batch Loss: 0.013669461943209171\n",
      "Epoch 305, Loss: 0.11094575747847557, Final Batch Loss: 0.018728729337453842\n",
      "Epoch 306, Loss: 0.11425535194575787, Final Batch Loss: 0.01571117527782917\n",
      "Epoch 307, Loss: 0.09853189066052437, Final Batch Loss: 0.026209045201539993\n",
      "Epoch 308, Loss: 0.10491298045963049, Final Batch Loss: 0.014535530470311642\n",
      "Epoch 309, Loss: 0.10032023955136538, Final Batch Loss: 0.018874170258641243\n",
      "Epoch 310, Loss: 0.11422652471810579, Final Batch Loss: 0.020806260406970978\n",
      "Epoch 311, Loss: 0.1132949935272336, Final Batch Loss: 0.039072342216968536\n",
      "Epoch 312, Loss: 0.16320472583174706, Final Batch Loss: 0.08486592024564743\n",
      "Epoch 313, Loss: 0.11339324526488781, Final Batch Loss: 0.02229265309870243\n",
      "Epoch 314, Loss: 0.12445702310651541, Final Batch Loss: 0.01760769821703434\n",
      "Epoch 315, Loss: 0.1258164420723915, Final Batch Loss: 0.017896298319101334\n",
      "Epoch 316, Loss: 0.15512476209551096, Final Batch Loss: 0.061070121824741364\n",
      "Epoch 317, Loss: 0.1209464743733406, Final Batch Loss: 0.04597726836800575\n",
      "Epoch 318, Loss: 0.12837362848222256, Final Batch Loss: 0.03234326094388962\n",
      "Epoch 319, Loss: 0.12128922622650862, Final Batch Loss: 0.007161655463278294\n",
      "Epoch 320, Loss: 0.1289807758294046, Final Batch Loss: 0.03448288142681122\n",
      "Epoch 321, Loss: 0.14297344908118248, Final Batch Loss: 0.017199907451868057\n",
      "Epoch 322, Loss: 0.1147568840533495, Final Batch Loss: 0.010966543108224869\n",
      "Epoch 323, Loss: 0.16527835465967655, Final Batch Loss: 0.07789653539657593\n",
      "Epoch 324, Loss: 0.1739848107099533, Final Batch Loss: 0.06525903940200806\n",
      "Epoch 325, Loss: 0.11973468400537968, Final Batch Loss: 0.04732917249202728\n",
      "Epoch 326, Loss: 0.10324610024690628, Final Batch Loss: 0.027690185233950615\n",
      "Epoch 327, Loss: 0.11561542609706521, Final Batch Loss: 0.0049801054410636425\n",
      "Epoch 328, Loss: 0.10880947858095169, Final Batch Loss: 0.010335076600313187\n",
      "Epoch 329, Loss: 0.1320462916046381, Final Batch Loss: 0.05835418403148651\n",
      "Epoch 330, Loss: 0.1094639040529728, Final Batch Loss: 0.04173542186617851\n",
      "Epoch 331, Loss: 0.09898290131241083, Final Batch Loss: 0.011407800018787384\n",
      "Epoch 332, Loss: 0.1520662810653448, Final Batch Loss: 0.05650555342435837\n",
      "Epoch 333, Loss: 0.0850474126636982, Final Batch Loss: 0.025233013555407524\n",
      "Epoch 334, Loss: 0.09795018006116152, Final Batch Loss: 0.01227854285389185\n",
      "Epoch 335, Loss: 0.14294829219579697, Final Batch Loss: 0.029942438006401062\n",
      "Epoch 336, Loss: 0.1326911486685276, Final Batch Loss: 0.08112204074859619\n",
      "Epoch 337, Loss: 0.11975482106208801, Final Batch Loss: 0.04498843476176262\n",
      "Epoch 338, Loss: 0.0998589564114809, Final Batch Loss: 0.014851409010589123\n",
      "Epoch 339, Loss: 0.12556140357628465, Final Batch Loss: 0.06289587914943695\n",
      "Epoch 340, Loss: 0.08698528073728085, Final Batch Loss: 0.026791445910930634\n",
      "Epoch 341, Loss: 0.11675610905513167, Final Batch Loss: 0.04949681833386421\n",
      "Epoch 342, Loss: 0.13300433894619346, Final Batch Loss: 0.0636325553059578\n",
      "Epoch 343, Loss: 0.09170150849968195, Final Batch Loss: 0.009295178577303886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 344, Loss: 0.1309898979961872, Final Batch Loss: 0.050582513213157654\n",
      "Epoch 345, Loss: 0.09255671221762896, Final Batch Loss: 0.008179026655852795\n",
      "Epoch 346, Loss: 0.13067663926631212, Final Batch Loss: 0.061694297939538956\n",
      "Epoch 347, Loss: 0.11888368893414736, Final Batch Loss: 0.010941524989902973\n",
      "Epoch 348, Loss: 0.11565045919269323, Final Batch Loss: 0.020946471020579338\n",
      "Epoch 349, Loss: 0.13248484022915363, Final Batch Loss: 0.03512432053685188\n",
      "Epoch 350, Loss: 0.12304296530783176, Final Batch Loss: 0.03482505679130554\n",
      "Epoch 351, Loss: 0.16752678249031305, Final Batch Loss: 0.09657741338014603\n",
      "Epoch 352, Loss: 0.1185189695097506, Final Batch Loss: 0.040772695094347\n",
      "Epoch 353, Loss: 0.09133201744407415, Final Batch Loss: 0.017920581623911858\n",
      "Epoch 354, Loss: 0.11671847570687532, Final Batch Loss: 0.005875426344573498\n",
      "Epoch 355, Loss: 0.10551252821460366, Final Batch Loss: 0.012390757910907269\n",
      "Epoch 356, Loss: 0.08732648100703955, Final Batch Loss: 0.007749189622700214\n",
      "Epoch 357, Loss: 0.09384435601532459, Final Batch Loss: 0.04750651866197586\n",
      "Epoch 358, Loss: 0.11245196871459484, Final Batch Loss: 0.014845544472336769\n",
      "Epoch 359, Loss: 0.10385862737894058, Final Batch Loss: 0.0278545543551445\n",
      "Epoch 360, Loss: 0.11319935321807861, Final Batch Loss: 0.01606188528239727\n",
      "Epoch 361, Loss: 0.1359481019899249, Final Batch Loss: 0.059298817068338394\n",
      "Epoch 362, Loss: 0.15022306703031063, Final Batch Loss: 0.028904734179377556\n",
      "Epoch 363, Loss: 0.15658274292945862, Final Batch Loss: 0.08209088444709778\n",
      "Epoch 364, Loss: 0.07657165778800845, Final Batch Loss: 0.01184889581054449\n",
      "Epoch 365, Loss: 0.11416131258010864, Final Batch Loss: 0.04890728369355202\n",
      "Epoch 366, Loss: 0.11705363262444735, Final Batch Loss: 0.01765468716621399\n",
      "Epoch 367, Loss: 0.12408812344074249, Final Batch Loss: 0.06691920012235641\n",
      "Epoch 368, Loss: 0.11552665568888187, Final Batch Loss: 0.021608991548419\n",
      "Epoch 369, Loss: 0.09450879879295826, Final Batch Loss: 0.018286598846316338\n",
      "Epoch 370, Loss: 0.11878291144967079, Final Batch Loss: 0.05106799677014351\n",
      "Epoch 371, Loss: 0.12146927416324615, Final Batch Loss: 0.029530992731451988\n",
      "Epoch 372, Loss: 0.09646099526435137, Final Batch Loss: 0.010428780689835548\n",
      "Epoch 373, Loss: 0.12396059278398752, Final Batch Loss: 0.00810809526592493\n",
      "Epoch 374, Loss: 0.1308162771165371, Final Batch Loss: 0.022306663915514946\n",
      "Epoch 375, Loss: 0.10648715682327747, Final Batch Loss: 0.01295807957649231\n",
      "Epoch 376, Loss: 0.11790323071181774, Final Batch Loss: 0.016138335689902306\n",
      "Epoch 377, Loss: 0.10081318113952875, Final Batch Loss: 0.009296395815908909\n",
      "Epoch 378, Loss: 0.14423523843288422, Final Batch Loss: 0.053413085639476776\n",
      "Epoch 379, Loss: 0.10388473235070705, Final Batch Loss: 0.009476110339164734\n",
      "Epoch 380, Loss: 0.13410709984600544, Final Batch Loss: 0.04540127515792847\n",
      "Epoch 381, Loss: 0.08940226305276155, Final Batch Loss: 0.01178268063813448\n",
      "Epoch 382, Loss: 0.11814374476671219, Final Batch Loss: 0.024473996832966805\n",
      "Epoch 383, Loss: 0.11068044044077396, Final Batch Loss: 0.01878698356449604\n",
      "Epoch 384, Loss: 0.08027975051663816, Final Batch Loss: 0.0033944237511605024\n",
      "Epoch 385, Loss: 0.12096620351076126, Final Batch Loss: 0.06484661251306534\n",
      "Epoch 386, Loss: 0.1332678683102131, Final Batch Loss: 0.05138011649250984\n",
      "Epoch 387, Loss: 0.08908750023692846, Final Batch Loss: 0.0345272570848465\n",
      "Epoch 388, Loss: 0.09530248120427132, Final Batch Loss: 0.04526948183774948\n",
      "Epoch 389, Loss: 0.11619847919791937, Final Batch Loss: 0.06715944409370422\n",
      "Epoch 390, Loss: 0.1350607629865408, Final Batch Loss: 0.05161845311522484\n",
      "Epoch 391, Loss: 0.09185206890106201, Final Batch Loss: 0.01320534199476242\n",
      "Epoch 392, Loss: 0.12190359039232135, Final Batch Loss: 0.06053070351481438\n",
      "Epoch 393, Loss: 0.10738958790898323, Final Batch Loss: 0.008198140189051628\n",
      "Epoch 394, Loss: 0.08904599770903587, Final Batch Loss: 0.012086600065231323\n",
      "Epoch 395, Loss: 0.11944498121738434, Final Batch Loss: 0.0285731740295887\n",
      "Epoch 396, Loss: 0.14505314081907272, Final Batch Loss: 0.07905865460634232\n",
      "Epoch 397, Loss: 0.07580990996211767, Final Batch Loss: 0.00865270011126995\n",
      "Epoch 398, Loss: 0.12677265238016844, Final Batch Loss: 0.09378219395875931\n",
      "Epoch 399, Loss: 0.08162029692903161, Final Batch Loss: 0.005342536140233278\n",
      "Epoch 400, Loss: 0.08245745254680514, Final Batch Loss: 0.04036905616521835\n",
      "Epoch 401, Loss: 0.10360124055296183, Final Batch Loss: 0.03486860916018486\n",
      "Epoch 402, Loss: 0.08204366825520992, Final Batch Loss: 0.025229640305042267\n",
      "Epoch 403, Loss: 0.08475449681282043, Final Batch Loss: 0.008368417620658875\n",
      "Epoch 404, Loss: 0.08007655944675207, Final Batch Loss: 0.006844227202236652\n",
      "Epoch 405, Loss: 0.08494462724775076, Final Batch Loss: 0.007876639254391193\n",
      "Epoch 406, Loss: 0.07168165873736143, Final Batch Loss: 0.004022969864308834\n",
      "Epoch 407, Loss: 0.07597538083791733, Final Batch Loss: 0.014179915189743042\n",
      "Epoch 408, Loss: 0.06646132655441761, Final Batch Loss: 0.00899513065814972\n",
      "Epoch 409, Loss: 0.08908231696113944, Final Batch Loss: 0.0050620283000171185\n",
      "Epoch 410, Loss: 0.07782093761488795, Final Batch Loss: 0.004430302884429693\n",
      "Epoch 411, Loss: 0.07744073774665594, Final Batch Loss: 0.008731626905500889\n",
      "Epoch 412, Loss: 0.08178952662274241, Final Batch Loss: 0.00440391106531024\n",
      "Epoch 413, Loss: 0.07820986141450703, Final Batch Loss: 0.0029452911112457514\n",
      "Epoch 414, Loss: 0.09348506107926369, Final Batch Loss: 0.030050408095121384\n",
      "Epoch 415, Loss: 0.07863432494923472, Final Batch Loss: 0.0178969856351614\n",
      "Epoch 416, Loss: 0.0898222029209137, Final Batch Loss: 0.012351338751614094\n",
      "Epoch 417, Loss: 0.12868281081318855, Final Batch Loss: 0.02315325289964676\n",
      "Epoch 418, Loss: 0.06855271197855473, Final Batch Loss: 0.005977606400847435\n",
      "Epoch 419, Loss: 0.12173656560480595, Final Batch Loss: 0.044625721871852875\n",
      "Epoch 420, Loss: 0.09458418190479279, Final Batch Loss: 0.03198641166090965\n",
      "Epoch 421, Loss: 0.0826226188801229, Final Batch Loss: 0.003439714666455984\n",
      "Epoch 422, Loss: 0.09651411604136229, Final Batch Loss: 0.008296404965221882\n",
      "Epoch 423, Loss: 0.10319417994469404, Final Batch Loss: 0.01067541353404522\n",
      "Epoch 424, Loss: 0.09259691275656223, Final Batch Loss: 0.008769422769546509\n",
      "Epoch 425, Loss: 0.10679618176072836, Final Batch Loss: 0.02981998212635517\n",
      "Epoch 426, Loss: 0.07758883899077773, Final Batch Loss: 0.017705826088786125\n",
      "Epoch 427, Loss: 0.06820577848702669, Final Batch Loss: 0.011379512958228588\n",
      "Epoch 428, Loss: 0.0857051620259881, Final Batch Loss: 0.008763731457293034\n",
      "Epoch 429, Loss: 0.09288211818784475, Final Batch Loss: 0.027123071253299713\n",
      "Epoch 430, Loss: 0.07379655307158828, Final Batch Loss: 0.004525542259216309\n",
      "Epoch 431, Loss: 0.09371752478182316, Final Batch Loss: 0.01756235398352146\n",
      "Epoch 432, Loss: 0.06239132536575198, Final Batch Loss: 0.0071475631557404995\n",
      "Epoch 433, Loss: 0.06328761205077171, Final Batch Loss: 0.005900484509766102\n",
      "Epoch 434, Loss: 0.07528976770117879, Final Batch Loss: 0.004067575093358755\n",
      "Epoch 435, Loss: 0.12699072621762753, Final Batch Loss: 0.07042988389730453\n",
      "Epoch 436, Loss: 0.050852617248892784, Final Batch Loss: 0.012757973745465279\n",
      "Epoch 437, Loss: 0.08799301739782095, Final Batch Loss: 0.02425207383930683\n",
      "Epoch 438, Loss: 0.07849755603820086, Final Batch Loss: 0.017908690497279167\n",
      "Epoch 439, Loss: 0.07466410519555211, Final Batch Loss: 0.018896782770752907\n",
      "Epoch 440, Loss: 0.05880812765099108, Final Batch Loss: 0.011941679753363132\n",
      "Epoch 441, Loss: 0.07414676621556282, Final Batch Loss: 0.009095752611756325\n",
      "Epoch 442, Loss: 0.07879409985616803, Final Batch Loss: 0.02729070745408535\n",
      "Epoch 443, Loss: 0.0999063104391098, Final Batch Loss: 0.01299738883972168\n",
      "Epoch 444, Loss: 0.09961571311578155, Final Batch Loss: 0.004098196979612112\n",
      "Epoch 445, Loss: 0.10336635075509548, Final Batch Loss: 0.008923493325710297\n",
      "Epoch 446, Loss: 0.08815747406333685, Final Batch Loss: 0.043504390865564346\n",
      "Epoch 447, Loss: 0.10729712247848511, Final Batch Loss: 0.06320752948522568\n",
      "Epoch 448, Loss: 0.1149865728802979, Final Batch Loss: 0.006790383253246546\n",
      "Epoch 449, Loss: 0.09250880079343915, Final Batch Loss: 0.0517754852771759\n",
      "Epoch 450, Loss: 0.08165871119126678, Final Batch Loss: 0.03935334458947182\n",
      "Epoch 451, Loss: 0.07106656022369862, Final Batch Loss: 0.0028919335454702377\n",
      "Epoch 452, Loss: 0.07862206641584635, Final Batch Loss: 0.01117032952606678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 453, Loss: 0.10503459768369794, Final Batch Loss: 0.045077044516801834\n",
      "Epoch 454, Loss: 0.09114749170839787, Final Batch Loss: 0.010731926187872887\n",
      "Epoch 455, Loss: 0.08390268729999661, Final Batch Loss: 0.026446759700775146\n",
      "Epoch 456, Loss: 0.07074400968849659, Final Batch Loss: 0.017750512808561325\n",
      "Epoch 457, Loss: 0.08453594893217087, Final Batch Loss: 0.013301641680300236\n",
      "Epoch 458, Loss: 0.08966620080173016, Final Batch Loss: 0.013005937449634075\n",
      "Epoch 459, Loss: 0.0591371594928205, Final Batch Loss: 0.00670918682590127\n",
      "Epoch 460, Loss: 0.12448962335474789, Final Batch Loss: 0.08094365149736404\n",
      "Epoch 461, Loss: 0.04598035989329219, Final Batch Loss: 0.003422951325774193\n",
      "Epoch 462, Loss: 0.09073618613183498, Final Batch Loss: 0.025178158655762672\n",
      "Epoch 463, Loss: 0.06473920261487365, Final Batch Loss: 0.017936725169420242\n",
      "Epoch 464, Loss: 0.05245127156376839, Final Batch Loss: 0.011354340240359306\n",
      "Epoch 465, Loss: 0.06865036394447088, Final Batch Loss: 0.004421261139214039\n",
      "Epoch 466, Loss: 0.07906858064234257, Final Batch Loss: 0.023614617064595222\n",
      "Epoch 467, Loss: 0.05470101302489638, Final Batch Loss: 0.006351017858833075\n",
      "Epoch 468, Loss: 0.08974221348762512, Final Batch Loss: 0.03686707466840744\n",
      "Epoch 469, Loss: 0.09782676491886377, Final Batch Loss: 0.010848854668438435\n",
      "Epoch 470, Loss: 0.07613884378224611, Final Batch Loss: 0.03822857886552811\n",
      "Epoch 471, Loss: 0.08863476663827896, Final Batch Loss: 0.024654459208250046\n",
      "Epoch 472, Loss: 0.06335289869457483, Final Batch Loss: 0.025368675589561462\n",
      "Epoch 473, Loss: 0.05708153557498008, Final Batch Loss: 0.0017642887542024255\n",
      "Epoch 474, Loss: 0.05688342242501676, Final Batch Loss: 0.003677596105262637\n",
      "Epoch 475, Loss: 0.10722326207906008, Final Batch Loss: 0.04172351583838463\n",
      "Epoch 476, Loss: 0.07384476158767939, Final Batch Loss: 0.0036055007949471474\n",
      "Epoch 477, Loss: 0.053680647164583206, Final Batch Loss: 0.005046003498136997\n",
      "Epoch 478, Loss: 0.06791939865797758, Final Batch Loss: 0.020928867161273956\n",
      "Epoch 479, Loss: 0.05945828324183822, Final Batch Loss: 0.00683547230437398\n",
      "Epoch 480, Loss: 0.056541614001616836, Final Batch Loss: 0.002423984697088599\n",
      "Epoch 481, Loss: 0.05514239985495806, Final Batch Loss: 0.009857752360403538\n",
      "Epoch 482, Loss: 0.08482317253947258, Final Batch Loss: 0.00751563161611557\n",
      "Epoch 483, Loss: 0.06473989319056273, Final Batch Loss: 0.0021907491609454155\n",
      "Epoch 484, Loss: 0.0787934334948659, Final Batch Loss: 0.01910638064146042\n",
      "Epoch 485, Loss: 0.09347287146374583, Final Batch Loss: 0.028629673644900322\n",
      "Epoch 486, Loss: 0.08733583986759186, Final Batch Loss: 0.03934407979249954\n",
      "Epoch 487, Loss: 0.0712999333627522, Final Batch Loss: 0.009418521076440811\n",
      "Epoch 488, Loss: 0.05372109357267618, Final Batch Loss: 0.004551821853965521\n",
      "Epoch 489, Loss: 0.09129815618507564, Final Batch Loss: 0.0027401007246226072\n",
      "Epoch 490, Loss: 0.08066511899232864, Final Batch Loss: 0.010517114773392677\n",
      "Epoch 491, Loss: 0.11574834771454334, Final Batch Loss: 0.04026101902127266\n",
      "Epoch 492, Loss: 0.0552633348852396, Final Batch Loss: 0.005847634747624397\n",
      "Epoch 493, Loss: 0.05501560471020639, Final Batch Loss: 0.02216983214020729\n",
      "Epoch 494, Loss: 0.061014314531348646, Final Batch Loss: 0.0018352697370573878\n",
      "Epoch 495, Loss: 0.08401172049343586, Final Batch Loss: 0.003947027958929539\n",
      "Epoch 496, Loss: 0.09116044687107205, Final Batch Loss: 0.007162683177739382\n",
      "Epoch 497, Loss: 0.08769802236929536, Final Batch Loss: 0.03648894280195236\n",
      "Epoch 498, Loss: 0.050957093480974436, Final Batch Loss: 0.00586799206212163\n",
      "Epoch 499, Loss: 0.04778618784621358, Final Batch Loss: 0.008365589193999767\n",
      "Epoch 500, Loss: 0.0772830278147012, Final Batch Loss: 0.017680024728178978\n",
      "Epoch 501, Loss: 0.07571384496986866, Final Batch Loss: 0.006730290129780769\n",
      "Epoch 502, Loss: 0.06323190033435822, Final Batch Loss: 0.01578250713646412\n",
      "Epoch 503, Loss: 0.08703887136653066, Final Batch Loss: 0.05258198827505112\n",
      "Epoch 504, Loss: 0.05868824431672692, Final Batch Loss: 0.013224054127931595\n",
      "Epoch 505, Loss: 0.04196855612099171, Final Batch Loss: 0.0053391167894005775\n",
      "Epoch 506, Loss: 0.06184192234650254, Final Batch Loss: 0.005357962567359209\n",
      "Epoch 507, Loss: 0.06442982191219926, Final Batch Loss: 0.014099699445068836\n",
      "Epoch 508, Loss: 0.05611206730827689, Final Batch Loss: 0.012233548797667027\n",
      "Epoch 509, Loss: 0.1000449201092124, Final Batch Loss: 0.05240599438548088\n",
      "Epoch 510, Loss: 0.06658708117902279, Final Batch Loss: 0.009908987209200859\n",
      "Epoch 511, Loss: 0.05058902385644615, Final Batch Loss: 0.0017385066021233797\n",
      "Epoch 512, Loss: 0.08042946457862854, Final Batch Loss: 0.03532138839364052\n",
      "Epoch 513, Loss: 0.07169685792177916, Final Batch Loss: 0.03057982586324215\n",
      "Epoch 514, Loss: 0.05445933062583208, Final Batch Loss: 0.013891438953578472\n",
      "Epoch 515, Loss: 0.07934955134987831, Final Batch Loss: 0.03407789394259453\n",
      "Epoch 516, Loss: 0.08009715704247355, Final Batch Loss: 0.04683757200837135\n",
      "Epoch 517, Loss: 0.05275423126295209, Final Batch Loss: 0.010871920734643936\n",
      "Epoch 518, Loss: 0.09303396381437778, Final Batch Loss: 0.029962845146656036\n",
      "Epoch 519, Loss: 0.041575856041163206, Final Batch Loss: 0.004772351589053869\n",
      "Epoch 520, Loss: 0.06840052222833037, Final Batch Loss: 0.004819273948669434\n",
      "Epoch 521, Loss: 0.06675190385431051, Final Batch Loss: 0.007868940941989422\n",
      "Epoch 522, Loss: 0.0791543391533196, Final Batch Loss: 0.009481646120548248\n",
      "Epoch 523, Loss: 0.09181923232972622, Final Batch Loss: 0.025599056854844093\n",
      "Epoch 524, Loss: 0.05119712185114622, Final Batch Loss: 0.01712261326611042\n",
      "Epoch 525, Loss: 0.06827391125261784, Final Batch Loss: 0.0028062937781214714\n",
      "Epoch 526, Loss: 0.08546047750860453, Final Batch Loss: 0.015333459712564945\n",
      "Epoch 527, Loss: 0.05953385215252638, Final Batch Loss: 0.021062567830085754\n",
      "Epoch 528, Loss: 0.1062368880957365, Final Batch Loss: 0.06933347880840302\n",
      "Epoch 529, Loss: 0.0707511929795146, Final Batch Loss: 0.04322567209601402\n",
      "Epoch 530, Loss: 0.059794641681946814, Final Batch Loss: 0.005381835624575615\n",
      "Epoch 531, Loss: 0.0587393210735172, Final Batch Loss: 0.004229032434523106\n",
      "Epoch 532, Loss: 0.0730426236987114, Final Batch Loss: 0.031108716502785683\n",
      "Epoch 533, Loss: 0.05610644677653909, Final Batch Loss: 0.005381778348237276\n",
      "Epoch 534, Loss: 0.08539186883717775, Final Batch Loss: 0.03970026597380638\n",
      "Epoch 535, Loss: 0.04820610303431749, Final Batch Loss: 0.008550634607672691\n",
      "Epoch 536, Loss: 0.08500686008483171, Final Batch Loss: 0.04002033919095993\n",
      "Epoch 537, Loss: 0.04435651283711195, Final Batch Loss: 0.004271863028407097\n",
      "Epoch 538, Loss: 0.06072322651743889, Final Batch Loss: 0.007257592864334583\n",
      "Epoch 539, Loss: 0.043959970003925264, Final Batch Loss: 0.0017933951457962394\n",
      "Epoch 540, Loss: 0.04567774559836835, Final Batch Loss: 0.0016782720340415835\n",
      "Epoch 541, Loss: 0.05783007200807333, Final Batch Loss: 0.0026976047083735466\n",
      "Epoch 542, Loss: 0.08440940687432885, Final Batch Loss: 0.00448758527636528\n",
      "Epoch 543, Loss: 0.03626356599852443, Final Batch Loss: 0.005778809543699026\n",
      "Epoch 544, Loss: 0.09065974550321698, Final Batch Loss: 0.007297762203961611\n",
      "Epoch 545, Loss: 0.07037956360727549, Final Batch Loss: 0.012115197256207466\n",
      "Epoch 546, Loss: 0.06875424878671765, Final Batch Loss: 0.01475902833044529\n",
      "Epoch 547, Loss: 0.058958022156730294, Final Batch Loss: 0.003647738369181752\n",
      "Epoch 548, Loss: 0.04788734391331673, Final Batch Loss: 0.004691675771027803\n",
      "Epoch 549, Loss: 0.07477655168622732, Final Batch Loss: 0.020201927050948143\n",
      "Epoch 550, Loss: 0.04195916885510087, Final Batch Loss: 0.002944021951407194\n",
      "Epoch 551, Loss: 0.04847670439630747, Final Batch Loss: 0.0030703339725732803\n",
      "Epoch 552, Loss: 0.045821528299711645, Final Batch Loss: 0.0022871203254908323\n",
      "Epoch 553, Loss: 0.052370450575836, Final Batch Loss: 0.0019501339411363006\n",
      "Epoch 554, Loss: 0.08404975617304444, Final Batch Loss: 0.031834911555051804\n",
      "Epoch 555, Loss: 0.05533601786009967, Final Batch Loss: 0.012808161787688732\n",
      "Epoch 556, Loss: 0.049133751541376114, Final Batch Loss: 0.023022307083010674\n",
      "Epoch 557, Loss: 0.0810459041967988, Final Batch Loss: 0.04701537266373634\n",
      "Epoch 558, Loss: 0.09318979736417532, Final Batch Loss: 0.049522463232278824\n",
      "Epoch 559, Loss: 0.0403070580214262, Final Batch Loss: 0.007778485305607319\n",
      "Epoch 560, Loss: 0.08576220786198974, Final Batch Loss: 0.03441867604851723\n",
      "Epoch 561, Loss: 0.0791013662237674, Final Batch Loss: 0.003198222490027547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 562, Loss: 0.05109594017267227, Final Batch Loss: 0.006460857577621937\n",
      "Epoch 563, Loss: 0.0768140833824873, Final Batch Loss: 0.009514734148979187\n",
      "Epoch 564, Loss: 0.06875641201622784, Final Batch Loss: 0.004229172598570585\n",
      "Epoch 565, Loss: 0.04590741591528058, Final Batch Loss: 0.004643277730792761\n",
      "Epoch 566, Loss: 0.055568981217220426, Final Batch Loss: 0.02765006385743618\n",
      "Epoch 567, Loss: 0.04967592004686594, Final Batch Loss: 0.008483445271849632\n",
      "Epoch 568, Loss: 0.05110693583264947, Final Batch Loss: 0.0045229410752654076\n",
      "Epoch 569, Loss: 0.056978848995640874, Final Batch Loss: 0.02428487315773964\n",
      "Epoch 570, Loss: 0.1429002892691642, Final Batch Loss: 0.1034698635339737\n",
      "Epoch 571, Loss: 0.04990356229245663, Final Batch Loss: 0.006262021604925394\n",
      "Epoch 572, Loss: 0.06634575594216585, Final Batch Loss: 0.03484586626291275\n",
      "Epoch 573, Loss: 0.06625351076945662, Final Batch Loss: 0.014602797105908394\n",
      "Epoch 574, Loss: 0.13142968015745282, Final Batch Loss: 0.028481528162956238\n",
      "Epoch 575, Loss: 0.060747097712010145, Final Batch Loss: 0.006012267898768187\n",
      "Epoch 576, Loss: 0.04656702931970358, Final Batch Loss: 0.0256083644926548\n",
      "Epoch 577, Loss: 0.07487950101494789, Final Batch Loss: 0.03189084306359291\n",
      "Epoch 578, Loss: 0.060573738999664783, Final Batch Loss: 0.025079667568206787\n",
      "Epoch 579, Loss: 0.0641791713424027, Final Batch Loss: 0.017897645011544228\n",
      "Epoch 580, Loss: 0.02550425846129656, Final Batch Loss: 0.002586890012025833\n",
      "Epoch 581, Loss: 0.03972310712561011, Final Batch Loss: 0.010494052432477474\n",
      "Epoch 582, Loss: 0.040076567558571696, Final Batch Loss: 0.0011692491825670004\n",
      "Epoch 583, Loss: 0.05461615649983287, Final Batch Loss: 0.005477209109812975\n",
      "Epoch 584, Loss: 0.08329974068328738, Final Batch Loss: 0.0349048487842083\n",
      "Epoch 585, Loss: 0.06514689419418573, Final Batch Loss: 0.04071822762489319\n",
      "Epoch 586, Loss: 0.030122163181658834, Final Batch Loss: 0.0007903887308202684\n",
      "Epoch 587, Loss: 0.037452886113896966, Final Batch Loss: 0.018942521885037422\n",
      "Epoch 588, Loss: 0.044693816220387816, Final Batch Loss: 0.0033510092180222273\n",
      "Epoch 589, Loss: 0.04133539367467165, Final Batch Loss: 0.005552026443183422\n",
      "Epoch 590, Loss: 0.051594124641269445, Final Batch Loss: 0.014746027067303658\n",
      "Epoch 591, Loss: 0.0239129476249218, Final Batch Loss: 0.0012732383329421282\n",
      "Epoch 592, Loss: 0.060280005214735866, Final Batch Loss: 0.0035891488660126925\n",
      "Epoch 593, Loss: 0.07263208599761128, Final Batch Loss: 0.03019746020436287\n",
      "Epoch 594, Loss: 0.03981308452785015, Final Batch Loss: 0.004840318579226732\n",
      "Epoch 595, Loss: 0.050876179710030556, Final Batch Loss: 0.007311955094337463\n",
      "Epoch 596, Loss: 0.03610042855143547, Final Batch Loss: 0.0035792605485767126\n",
      "Epoch 597, Loss: 0.05622578598558903, Final Batch Loss: 0.017426785081624985\n",
      "Epoch 598, Loss: 0.042203298304229975, Final Batch Loss: 0.008850852027535439\n",
      "Epoch 599, Loss: 0.05835683108307421, Final Batch Loss: 0.003327977145090699\n",
      "Epoch 600, Loss: 0.06930068274959922, Final Batch Loss: 0.006359729450196028\n",
      "Epoch 601, Loss: 0.05715846107341349, Final Batch Loss: 0.023124130442738533\n",
      "Epoch 602, Loss: 0.0231088288128376, Final Batch Loss: 0.004054353106766939\n",
      "Epoch 603, Loss: 0.06549871759489179, Final Batch Loss: 0.02198638580739498\n",
      "Epoch 604, Loss: 0.03965420066379011, Final Batch Loss: 0.0026468231808394194\n",
      "Epoch 605, Loss: 0.034174432046711445, Final Batch Loss: 0.012247588485479355\n",
      "Epoch 606, Loss: 0.04194365255534649, Final Batch Loss: 0.009033606387674809\n",
      "Epoch 607, Loss: 0.017645433079451323, Final Batch Loss: 0.0026369246188551188\n",
      "Epoch 608, Loss: 0.061199415009468794, Final Batch Loss: 0.009006578475236893\n",
      "Epoch 609, Loss: 0.05106407916173339, Final Batch Loss: 0.018197882920503616\n",
      "Epoch 610, Loss: 0.08169672545045614, Final Batch Loss: 0.0032248692587018013\n",
      "Epoch 611, Loss: 0.046979060978628695, Final Batch Loss: 0.006852542050182819\n",
      "Epoch 612, Loss: 0.03728692699223757, Final Batch Loss: 0.016604457050561905\n",
      "Epoch 613, Loss: 0.03163443016819656, Final Batch Loss: 0.0023250544909387827\n",
      "Epoch 614, Loss: 0.035523996222764254, Final Batch Loss: 0.01449921727180481\n",
      "Epoch 615, Loss: 0.04092920816037804, Final Batch Loss: 0.0011275593424215913\n",
      "Epoch 616, Loss: 0.017002988373860717, Final Batch Loss: 0.000522043788805604\n",
      "Epoch 617, Loss: 0.02873541903682053, Final Batch Loss: 0.004354426171630621\n",
      "Epoch 618, Loss: 0.04045739118009806, Final Batch Loss: 0.0029438778292387724\n",
      "Epoch 619, Loss: 0.05071237310767174, Final Batch Loss: 0.0048767272382974625\n",
      "Epoch 620, Loss: 0.06716403132304549, Final Batch Loss: 0.041329264640808105\n",
      "Epoch 621, Loss: 0.0341743528842926, Final Batch Loss: 0.01124257780611515\n",
      "Epoch 622, Loss: 0.026675874658394605, Final Batch Loss: 0.0007426010561175644\n",
      "Epoch 623, Loss: 0.04638666822575033, Final Batch Loss: 0.0027727114502340555\n",
      "Epoch 624, Loss: 0.02413038269151002, Final Batch Loss: 0.015320342034101486\n",
      "Epoch 625, Loss: 0.020184887456707656, Final Batch Loss: 0.003661464201286435\n",
      "Epoch 626, Loss: 0.04438146506436169, Final Batch Loss: 0.011162074282765388\n",
      "Epoch 627, Loss: 0.02786275849211961, Final Batch Loss: 0.0069633168168365955\n",
      "Epoch 628, Loss: 0.021440131647977978, Final Batch Loss: 0.0008119661943055689\n",
      "Epoch 629, Loss: 0.048091606702655554, Final Batch Loss: 0.012352563440799713\n",
      "Epoch 630, Loss: 0.02377043734304607, Final Batch Loss: 0.002857787301763892\n",
      "Epoch 631, Loss: 0.04087313078343868, Final Batch Loss: 0.0117432726547122\n",
      "Epoch 632, Loss: 0.03790619340725243, Final Batch Loss: 0.011819718405604362\n",
      "Epoch 633, Loss: 0.05803004209883511, Final Batch Loss: 0.0022519726771861315\n",
      "Epoch 634, Loss: 0.0462594858254306, Final Batch Loss: 0.005332066677510738\n",
      "Epoch 635, Loss: 0.05480373394675553, Final Batch Loss: 0.010871060192584991\n",
      "Epoch 636, Loss: 0.02167535643093288, Final Batch Loss: 0.005122130736708641\n",
      "Epoch 637, Loss: 0.03251382242888212, Final Batch Loss: 0.001960468478500843\n",
      "Epoch 638, Loss: 0.05884791910648346, Final Batch Loss: 0.008383233100175858\n",
      "Epoch 639, Loss: 0.03636129549704492, Final Batch Loss: 0.02925228327512741\n",
      "Epoch 640, Loss: 0.06373517663450912, Final Batch Loss: 0.0007693439838476479\n",
      "Epoch 641, Loss: 0.05735258071217686, Final Batch Loss: 0.02989216335117817\n",
      "Epoch 642, Loss: 0.024283880833536386, Final Batch Loss: 0.005055143963545561\n",
      "Epoch 643, Loss: 0.06051246775314212, Final Batch Loss: 0.002583345863968134\n",
      "Epoch 644, Loss: 0.02646558964625001, Final Batch Loss: 0.0036637946031987667\n",
      "Epoch 645, Loss: 0.04869255539961159, Final Batch Loss: 0.023193197324872017\n",
      "Epoch 646, Loss: 0.031837082700803876, Final Batch Loss: 0.008569587953388691\n",
      "Epoch 647, Loss: 0.04650185629725456, Final Batch Loss: 0.02867553010582924\n",
      "Epoch 648, Loss: 0.0329243732849136, Final Batch Loss: 0.0014382899971678853\n",
      "Epoch 649, Loss: 0.03209664858877659, Final Batch Loss: 0.014426443725824356\n",
      "Epoch 650, Loss: 0.04296682006679475, Final Batch Loss: 0.024819687008857727\n",
      "Epoch 651, Loss: 0.03949178499169648, Final Batch Loss: 0.003038478782400489\n",
      "Epoch 652, Loss: 0.026288039749488235, Final Batch Loss: 0.002242170972749591\n",
      "Epoch 653, Loss: 0.0413753860630095, Final Batch Loss: 0.0204094760119915\n",
      "Epoch 654, Loss: 0.03564763069152832, Final Batch Loss: 0.01446100976318121\n",
      "Epoch 655, Loss: 0.02576968423090875, Final Batch Loss: 0.006444047205150127\n",
      "Epoch 656, Loss: 0.028915592469274998, Final Batch Loss: 0.00658859359100461\n",
      "Epoch 657, Loss: 0.034853841876611114, Final Batch Loss: 0.0036937477998435497\n",
      "Epoch 658, Loss: 0.024519011611118913, Final Batch Loss: 0.0033484019804745913\n",
      "Epoch 659, Loss: 0.02888000081293285, Final Batch Loss: 0.01468612253665924\n",
      "Epoch 660, Loss: 0.029527137230616063, Final Batch Loss: 0.002503806259483099\n",
      "Epoch 661, Loss: 0.08817081071902066, Final Batch Loss: 0.08334288001060486\n",
      "Epoch 662, Loss: 0.025998488417826593, Final Batch Loss: 0.001179537153802812\n",
      "Epoch 663, Loss: 0.023100726073607802, Final Batch Loss: 0.001341134775429964\n",
      "Epoch 664, Loss: 0.02989653404802084, Final Batch Loss: 0.003684638999402523\n",
      "Epoch 665, Loss: 0.020531212678179145, Final Batch Loss: 0.006001781672239304\n",
      "Epoch 666, Loss: 0.027362459804862738, Final Batch Loss: 0.001174181466922164\n",
      "Epoch 667, Loss: 0.04115952784195542, Final Batch Loss: 0.020618000999093056\n",
      "Epoch 668, Loss: 0.02489690249785781, Final Batch Loss: 0.0033437705133110285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 669, Loss: 0.06893536914139986, Final Batch Loss: 0.024170946329832077\n",
      "Epoch 670, Loss: 0.05595794168766588, Final Batch Loss: 0.03660929203033447\n",
      "Epoch 671, Loss: 0.041965733049437404, Final Batch Loss: 0.007845278829336166\n",
      "Epoch 672, Loss: 0.025790232000872493, Final Batch Loss: 0.004281484987586737\n",
      "Epoch 673, Loss: 0.0665710351895541, Final Batch Loss: 0.0012148162350058556\n",
      "Epoch 674, Loss: 0.021190000232309103, Final Batch Loss: 0.0006307625444605947\n",
      "Epoch 675, Loss: 0.017027138033881783, Final Batch Loss: 0.002117387019097805\n",
      "Epoch 676, Loss: 0.0367177251027897, Final Batch Loss: 0.006157777737826109\n",
      "Epoch 677, Loss: 0.012146325781941414, Final Batch Loss: 0.0018297319766134024\n",
      "Epoch 678, Loss: 0.018642090260982513, Final Batch Loss: 0.0013604398118332028\n",
      "Epoch 679, Loss: 0.06732328096404672, Final Batch Loss: 0.0006831614300608635\n",
      "Epoch 680, Loss: 0.014872584026306868, Final Batch Loss: 0.005618630442768335\n",
      "Epoch 681, Loss: 0.029619670240208507, Final Batch Loss: 0.002980234567075968\n",
      "Epoch 682, Loss: 0.011704553733579814, Final Batch Loss: 0.0036189863458275795\n",
      "Epoch 683, Loss: 0.05083848047070205, Final Batch Loss: 0.019051726907491684\n",
      "Epoch 684, Loss: 0.024481100670527667, Final Batch Loss: 0.0009024617611430585\n",
      "Epoch 685, Loss: 0.025646043592132628, Final Batch Loss: 0.00700696837157011\n",
      "Epoch 686, Loss: 0.03974833362735808, Final Batch Loss: 0.0047316886484622955\n",
      "Epoch 687, Loss: 0.03028173593338579, Final Batch Loss: 0.0008073884528130293\n",
      "Epoch 688, Loss: 0.04359898832626641, Final Batch Loss: 0.0017481555696576834\n",
      "Epoch 689, Loss: 0.0723072555847466, Final Batch Loss: 0.04775770381093025\n",
      "Epoch 690, Loss: 0.029452516260789707, Final Batch Loss: 0.0003138396714348346\n",
      "Epoch 691, Loss: 0.05449231783859432, Final Batch Loss: 0.013223374262452126\n",
      "Epoch 692, Loss: 0.031409508199431, Final Batch Loss: 0.021878855302929878\n",
      "Epoch 693, Loss: 0.045198380132205784, Final Batch Loss: 0.0031940455082803965\n",
      "Epoch 694, Loss: 0.07043398986570537, Final Batch Loss: 0.03894014284014702\n",
      "Epoch 695, Loss: 0.018507642089389265, Final Batch Loss: 0.002585113048553467\n",
      "Epoch 696, Loss: 0.028524352703243494, Final Batch Loss: 0.012457717210054398\n",
      "Epoch 697, Loss: 0.03270272724330425, Final Batch Loss: 0.0077013056725263596\n",
      "Epoch 698, Loss: 0.010829105041921139, Final Batch Loss: 0.0007413015700876713\n",
      "Epoch 699, Loss: 0.03712966409511864, Final Batch Loss: 0.010843907482922077\n",
      "Epoch 700, Loss: 0.012905301875434816, Final Batch Loss: 0.005188191309571266\n",
      "Epoch 701, Loss: 0.018588364124298096, Final Batch Loss: 0.0023221296723932028\n",
      "Epoch 702, Loss: 0.022355955094099045, Final Batch Loss: 0.0024576408322900534\n",
      "Epoch 703, Loss: 0.06448711594566703, Final Batch Loss: 0.04193076863884926\n",
      "Epoch 704, Loss: 0.06412289384752512, Final Batch Loss: 0.012292895466089249\n",
      "Epoch 705, Loss: 0.020837197196669877, Final Batch Loss: 0.0008468305459246039\n",
      "Epoch 706, Loss: 0.029261436022352427, Final Batch Loss: 0.0009546738001517951\n",
      "Epoch 707, Loss: 0.023947443929500878, Final Batch Loss: 0.001699932967312634\n",
      "Epoch 708, Loss: 0.01212465763092041, Final Batch Loss: 0.0028370823711156845\n",
      "Epoch 709, Loss: 0.019715686328709126, Final Batch Loss: 0.0028257921803742647\n",
      "Epoch 710, Loss: 0.01539965020492673, Final Batch Loss: 0.007731523830443621\n",
      "Epoch 711, Loss: 0.042621790897101164, Final Batch Loss: 0.002301893662661314\n",
      "Epoch 712, Loss: 0.02668471320066601, Final Batch Loss: 0.0012866841861978173\n",
      "Epoch 713, Loss: 0.02521686372347176, Final Batch Loss: 0.012306113727390766\n",
      "Epoch 714, Loss: 0.015891764080151916, Final Batch Loss: 0.006006009876728058\n",
      "Epoch 715, Loss: 0.03430137236136943, Final Batch Loss: 0.00033811316825449467\n",
      "Epoch 716, Loss: 0.02987881621811539, Final Batch Loss: 0.0071980576030910015\n",
      "Epoch 717, Loss: 0.016827266925247386, Final Batch Loss: 0.0004351781972218305\n",
      "Epoch 718, Loss: 0.03104527690447867, Final Batch Loss: 0.01332246232777834\n",
      "Epoch 719, Loss: 0.015729735139757395, Final Batch Loss: 0.004645442124456167\n",
      "Epoch 720, Loss: 0.028669923543930054, Final Batch Loss: 0.0006168847903609276\n",
      "Epoch 721, Loss: 0.04946898913476616, Final Batch Loss: 0.030863819643855095\n",
      "Epoch 722, Loss: 0.042026356561109424, Final Batch Loss: 0.007696917280554771\n",
      "Epoch 723, Loss: 0.03984013863373548, Final Batch Loss: 0.016802843660116196\n",
      "Epoch 724, Loss: 0.04444332281127572, Final Batch Loss: 0.0024956511333584785\n",
      "Epoch 725, Loss: 0.018007640028372407, Final Batch Loss: 0.004015710670500994\n",
      "Epoch 726, Loss: 0.047483268775977194, Final Batch Loss: 0.015827296301722527\n",
      "Epoch 727, Loss: 0.021752732805907726, Final Batch Loss: 0.0023311255499720573\n",
      "Epoch 728, Loss: 0.02059286250732839, Final Batch Loss: 0.006288784556090832\n",
      "Epoch 729, Loss: 0.040800781454890966, Final Batch Loss: 0.003454809309914708\n",
      "Epoch 730, Loss: 0.03419100935570896, Final Batch Loss: 0.023234773427248\n",
      "Epoch 731, Loss: 0.050986094400286674, Final Batch Loss: 0.0028193898033350706\n",
      "Epoch 732, Loss: 0.04631982394494116, Final Batch Loss: 0.0021579202730208635\n",
      "Epoch 733, Loss: 0.015562052140012383, Final Batch Loss: 0.003345270873978734\n",
      "Epoch 734, Loss: 0.03408731543458998, Final Batch Loss: 0.004092483781278133\n",
      "Epoch 735, Loss: 0.04379233333747834, Final Batch Loss: 0.038669317960739136\n",
      "Epoch 736, Loss: 0.026950851606670767, Final Batch Loss: 0.0008774034795351326\n",
      "Epoch 737, Loss: 0.02027016261126846, Final Batch Loss: 0.005515955854207277\n",
      "Epoch 738, Loss: 0.008786044083535671, Final Batch Loss: 0.002574086654931307\n",
      "Epoch 739, Loss: 0.027962628286331892, Final Batch Loss: 0.0018221752252429724\n",
      "Epoch 740, Loss: 0.05158996116369963, Final Batch Loss: 0.014324777759611607\n",
      "Epoch 741, Loss: 0.01153329445514828, Final Batch Loss: 0.005353327374905348\n",
      "Epoch 742, Loss: 0.04139733128249645, Final Batch Loss: 0.009820736944675446\n",
      "Epoch 743, Loss: 0.015255605307174847, Final Batch Loss: 0.0004584105627145618\n",
      "Epoch 744, Loss: 0.014723743428476155, Final Batch Loss: 0.001393394428305328\n",
      "Epoch 745, Loss: 0.01797945809084922, Final Batch Loss: 0.0018229818670079112\n",
      "Epoch 746, Loss: 0.054302028147503734, Final Batch Loss: 0.002604966750368476\n",
      "Epoch 747, Loss: 0.01766898389905691, Final Batch Loss: 0.0038343581836670637\n",
      "Epoch 748, Loss: 0.02351434581214562, Final Batch Loss: 0.00045710691483691335\n",
      "Epoch 749, Loss: 0.019635607080999762, Final Batch Loss: 0.0009000725694932044\n",
      "Epoch 750, Loss: 0.04151021596044302, Final Batch Loss: 0.012115832418203354\n",
      "Epoch 751, Loss: 0.011391669046133757, Final Batch Loss: 0.0022337189875543118\n",
      "Epoch 752, Loss: 0.03324059396982193, Final Batch Loss: 0.011299015954136848\n",
      "Epoch 753, Loss: 0.043681571434717625, Final Batch Loss: 0.0009535292047075927\n",
      "Epoch 754, Loss: 0.019718711148016155, Final Batch Loss: 0.0015525793423876166\n",
      "Epoch 755, Loss: 0.070562768378295, Final Batch Loss: 0.0573960579931736\n",
      "Epoch 756, Loss: 0.035641110967844725, Final Batch Loss: 0.023173097521066666\n",
      "Epoch 757, Loss: 0.04028685891535133, Final Batch Loss: 0.03204123675823212\n",
      "Epoch 758, Loss: 0.04526987997815013, Final Batch Loss: 0.02201608195900917\n",
      "Epoch 759, Loss: 0.025142489233985543, Final Batch Loss: 0.008498533628880978\n",
      "Epoch 760, Loss: 0.048974076053127646, Final Batch Loss: 0.0009514179546386003\n",
      "Epoch 761, Loss: 0.04040365107357502, Final Batch Loss: 0.005466945935040712\n",
      "Epoch 762, Loss: 0.049709237180650234, Final Batch Loss: 0.005484746769070625\n",
      "Epoch 763, Loss: 0.024903884856030345, Final Batch Loss: 0.0023624065797775984\n",
      "Epoch 764, Loss: 0.015137225156649947, Final Batch Loss: 0.0017684181220829487\n",
      "Epoch 765, Loss: 0.018779969395836815, Final Batch Loss: 0.00048438701196573675\n",
      "Epoch 766, Loss: 0.020770526258274913, Final Batch Loss: 0.006755308713763952\n",
      "Epoch 767, Loss: 0.014980387408286333, Final Batch Loss: 0.006846982054412365\n",
      "Epoch 768, Loss: 0.014397734892554581, Final Batch Loss: 0.000783487455919385\n",
      "Epoch 769, Loss: 0.019153974950313568, Final Batch Loss: 0.009451166726648808\n",
      "Epoch 770, Loss: 0.01316306262742728, Final Batch Loss: 0.00124655372928828\n",
      "Epoch 771, Loss: 0.011940835160203278, Final Batch Loss: 0.001426846836693585\n",
      "Epoch 772, Loss: 0.05242843832820654, Final Batch Loss: 0.008068581111729145\n",
      "Epoch 773, Loss: 0.0476064532995224, Final Batch Loss: 0.03176267817616463\n",
      "Epoch 774, Loss: 0.04073663137387484, Final Batch Loss: 0.033709920942783356\n",
      "Epoch 775, Loss: 0.01571729755960405, Final Batch Loss: 0.005425569135695696\n",
      "Epoch 776, Loss: 0.03650989895686507, Final Batch Loss: 0.001856704824604094\n",
      "Epoch 777, Loss: 0.009552768780849874, Final Batch Loss: 0.0015287447022274137\n",
      "Epoch 778, Loss: 0.03541255777236074, Final Batch Loss: 0.0009984549833461642\n",
      "Epoch 779, Loss: 0.030595493968576193, Final Batch Loss: 0.008435484021902084\n",
      "Epoch 780, Loss: 0.01990504248533398, Final Batch Loss: 0.00438228202983737\n",
      "Epoch 781, Loss: 0.03790284623391926, Final Batch Loss: 0.0017499940004199743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 782, Loss: 0.01993993460200727, Final Batch Loss: 0.001648128847591579\n",
      "Epoch 783, Loss: 0.014517985400743783, Final Batch Loss: 0.001031093648634851\n",
      "Epoch 784, Loss: 0.012448186753317714, Final Batch Loss: 0.0022523892112076283\n",
      "Epoch 785, Loss: 0.04379046754911542, Final Batch Loss: 0.016993746161460876\n",
      "Epoch 786, Loss: 0.04000696417642757, Final Batch Loss: 0.031192079186439514\n",
      "Epoch 787, Loss: 0.03749703228822909, Final Batch Loss: 0.0004293516685720533\n",
      "Epoch 788, Loss: 0.04486904019722715, Final Batch Loss: 0.02542916312813759\n",
      "Epoch 789, Loss: 0.01753576227929443, Final Batch Loss: 0.007003076374530792\n",
      "Epoch 790, Loss: 0.016257623676210642, Final Batch Loss: 0.0020300389733165503\n",
      "Epoch 791, Loss: 0.025509499246254563, Final Batch Loss: 0.002537862630560994\n",
      "Epoch 792, Loss: 0.02170636097434908, Final Batch Loss: 0.0006612945580855012\n",
      "Epoch 793, Loss: 0.035245677921921015, Final Batch Loss: 0.027278389781713486\n",
      "Epoch 794, Loss: 0.01576984114944935, Final Batch Loss: 0.0015798818785697222\n",
      "Epoch 795, Loss: 0.017025989829562604, Final Batch Loss: 0.009173575788736343\n",
      "Epoch 796, Loss: 0.01739204378100112, Final Batch Loss: 0.000743580108974129\n",
      "Epoch 797, Loss: 0.02748135320143774, Final Batch Loss: 0.012428589165210724\n",
      "Epoch 798, Loss: 0.04054753645323217, Final Batch Loss: 0.010176049545407295\n",
      "Epoch 799, Loss: 0.016929338220506907, Final Batch Loss: 0.0011877510696649551\n",
      "Epoch 800, Loss: 0.03524569235742092, Final Batch Loss: 0.011188879609107971\n",
      "Epoch 801, Loss: 0.04060349561041221, Final Batch Loss: 0.025886496528983116\n",
      "Epoch 802, Loss: 0.015107361599802971, Final Batch Loss: 0.0010716241085901856\n",
      "Epoch 803, Loss: 0.030869742506183684, Final Batch Loss: 0.002912979805842042\n",
      "Epoch 804, Loss: 0.03112879954278469, Final Batch Loss: 0.0010915426537394524\n",
      "Epoch 805, Loss: 0.024899955838918686, Final Batch Loss: 0.007335292175412178\n",
      "Epoch 806, Loss: 0.026841634564334527, Final Batch Loss: 0.0004153811896685511\n",
      "Epoch 807, Loss: 0.03883918118663132, Final Batch Loss: 0.0026788220275193453\n",
      "Epoch 808, Loss: 0.00909825274720788, Final Batch Loss: 0.0008156151743605733\n",
      "Epoch 809, Loss: 0.0060470434837043285, Final Batch Loss: 0.0014552278444170952\n",
      "Epoch 810, Loss: 0.05205620999913663, Final Batch Loss: 0.01825585588812828\n",
      "Epoch 811, Loss: 0.04248700093012303, Final Batch Loss: 0.0009125616634264588\n",
      "Epoch 812, Loss: 0.007976454216986895, Final Batch Loss: 0.002525630407035351\n",
      "Epoch 813, Loss: 0.01260203222045675, Final Batch Loss: 0.0045567466877400875\n",
      "Epoch 814, Loss: 0.014381414745002985, Final Batch Loss: 0.00605446333065629\n",
      "Epoch 815, Loss: 0.012495368486270308, Final Batch Loss: 0.00016331742517650127\n",
      "Epoch 816, Loss: 0.015465255244635046, Final Batch Loss: 0.0037337266840040684\n",
      "Epoch 817, Loss: 0.011595403775572777, Final Batch Loss: 0.002064906060695648\n",
      "Epoch 818, Loss: 0.03228424827102572, Final Batch Loss: 0.003449015785008669\n",
      "Epoch 819, Loss: 0.009014148032292724, Final Batch Loss: 0.0053799510933458805\n",
      "Epoch 820, Loss: 0.010222378594335169, Final Batch Loss: 0.0022295634262263775\n",
      "Epoch 821, Loss: 0.0065285462769679725, Final Batch Loss: 0.0005424738046713173\n",
      "Epoch 822, Loss: 0.03441126976395026, Final Batch Loss: 0.0009534084820188582\n",
      "Epoch 823, Loss: 0.010509486892260611, Final Batch Loss: 0.0006501033203676343\n",
      "Epoch 824, Loss: 0.020811664639040828, Final Batch Loss: 0.01034165732562542\n",
      "Epoch 825, Loss: 0.0063902229885570705, Final Batch Loss: 0.0006320500979200006\n",
      "Epoch 826, Loss: 0.008291043224744499, Final Batch Loss: 0.003759128972887993\n",
      "Epoch 827, Loss: 0.009458815562538803, Final Batch Loss: 0.0013319485587999225\n",
      "Epoch 828, Loss: 0.016223387443460524, Final Batch Loss: 0.004205797798931599\n",
      "Epoch 829, Loss: 0.017769802652765065, Final Batch Loss: 0.0007398661109618843\n",
      "Epoch 830, Loss: 0.012822048855014145, Final Batch Loss: 0.001872947788797319\n",
      "Epoch 831, Loss: 0.007159238564781845, Final Batch Loss: 0.0022014142014086246\n",
      "Epoch 832, Loss: 0.010524562560021877, Final Batch Loss: 0.0022029075771570206\n",
      "Epoch 833, Loss: 0.014348442695336416, Final Batch Loss: 0.0018925039330497384\n",
      "Epoch 834, Loss: 0.040522127237636596, Final Batch Loss: 0.005295669659972191\n",
      "Epoch 835, Loss: 0.016359903616830707, Final Batch Loss: 0.005416371859610081\n",
      "Epoch 836, Loss: 0.01054982579080388, Final Batch Loss: 0.0006381598650477827\n",
      "Epoch 837, Loss: 0.012231117754708976, Final Batch Loss: 0.0045279222540557384\n",
      "Epoch 838, Loss: 0.007012310961727053, Final Batch Loss: 0.002351301023736596\n",
      "Epoch 839, Loss: 0.024607232306152582, Final Batch Loss: 0.0018620637711137533\n",
      "Epoch 840, Loss: 0.007869947294238955, Final Batch Loss: 0.0027923735324293375\n",
      "Epoch 841, Loss: 0.015730533632449806, Final Batch Loss: 0.0014453405747190118\n",
      "Epoch 842, Loss: 0.004133693262701854, Final Batch Loss: 0.00040448023355565965\n",
      "Epoch 843, Loss: 0.019408833235502243, Final Batch Loss: 0.003398008644580841\n",
      "Epoch 844, Loss: 0.008370741386897862, Final Batch Loss: 0.0015843407018110156\n",
      "Epoch 845, Loss: 0.014894925901899114, Final Batch Loss: 0.0002903974673245102\n",
      "Epoch 846, Loss: 0.006357167381793261, Final Batch Loss: 0.00022866844665259123\n",
      "Epoch 847, Loss: 0.011118945491034538, Final Batch Loss: 0.00739177456125617\n",
      "Epoch 848, Loss: 0.011427841440308839, Final Batch Loss: 0.0007608262239955366\n",
      "Epoch 849, Loss: 0.01166666962672025, Final Batch Loss: 0.008012217469513416\n",
      "Epoch 850, Loss: 0.01175341330235824, Final Batch Loss: 0.006828391458839178\n",
      "Epoch 851, Loss: 0.06399063015123829, Final Batch Loss: 0.0006461617886088789\n",
      "Epoch 852, Loss: 0.0715244266029913, Final Batch Loss: 0.05467422306537628\n",
      "Epoch 853, Loss: 0.023894518380984664, Final Batch Loss: 0.0013422875199466944\n",
      "Epoch 854, Loss: 0.00890874641481787, Final Batch Loss: 0.002324419328942895\n",
      "Epoch 855, Loss: 0.009657557646278292, Final Batch Loss: 0.0006354886572808027\n",
      "Epoch 856, Loss: 0.01248557202052325, Final Batch Loss: 0.002409616019576788\n",
      "Epoch 857, Loss: 0.0106762399955187, Final Batch Loss: 0.0014829238643869758\n",
      "Epoch 858, Loss: 0.027626167226117104, Final Batch Loss: 0.0008208979270420969\n",
      "Epoch 859, Loss: 0.031671526114223525, Final Batch Loss: 0.018171092495322227\n",
      "Epoch 860, Loss: 0.04560162464622408, Final Batch Loss: 0.037032775580883026\n",
      "Epoch 861, Loss: 0.027766008861362934, Final Batch Loss: 0.015019837766885757\n",
      "Epoch 862, Loss: 0.034952192450873554, Final Batch Loss: 0.00860839057713747\n",
      "Epoch 863, Loss: 0.04260458017233759, Final Batch Loss: 0.0018460700521245599\n",
      "Epoch 864, Loss: 0.03762259113136679, Final Batch Loss: 0.033539336174726486\n",
      "Epoch 865, Loss: 0.00951432395959273, Final Batch Loss: 0.0009401631541550159\n",
      "Epoch 866, Loss: 0.018409858574159443, Final Batch Loss: 0.0014628689968958497\n",
      "Epoch 867, Loss: 0.008208041312173009, Final Batch Loss: 0.0036875412333756685\n",
      "Epoch 868, Loss: 0.00834416173165664, Final Batch Loss: 0.0016388216754421592\n",
      "Epoch 869, Loss: 0.004998840391635895, Final Batch Loss: 0.0007432022830471396\n",
      "Epoch 870, Loss: 0.007648893108125776, Final Batch Loss: 0.0008760203490965068\n",
      "Epoch 871, Loss: 0.022026319231372327, Final Batch Loss: 0.0007145482231862843\n",
      "Epoch 872, Loss: 0.007482631714083254, Final Batch Loss: 0.0005241228500381112\n",
      "Epoch 873, Loss: 0.04368945164605975, Final Batch Loss: 0.040896717458963394\n",
      "Epoch 874, Loss: 0.005492295778822154, Final Batch Loss: 0.0009256494813598692\n",
      "Epoch 875, Loss: 0.011803705827333033, Final Batch Loss: 0.0008428192231804132\n",
      "Epoch 876, Loss: 0.020942685310728848, Final Batch Loss: 0.0043151951394975185\n",
      "Epoch 877, Loss: 0.059533974272198975, Final Batch Loss: 0.0012416411191225052\n",
      "Epoch 878, Loss: 0.020946655713487417, Final Batch Loss: 0.0004232398350723088\n",
      "Epoch 879, Loss: 0.008597838575951755, Final Batch Loss: 0.0007529458962380886\n",
      "Epoch 880, Loss: 0.013494835060555488, Final Batch Loss: 0.0004138782969675958\n",
      "Epoch 881, Loss: 0.017394405673258007, Final Batch Loss: 0.0011343181831762195\n",
      "Epoch 882, Loss: 0.006762492703273892, Final Batch Loss: 0.00038966909050941467\n",
      "Epoch 883, Loss: 0.036914495402015746, Final Batch Loss: 0.030116360634565353\n",
      "Epoch 884, Loss: 0.03295998345129192, Final Batch Loss: 0.004581010900437832\n",
      "Epoch 885, Loss: 0.0070798853994347155, Final Batch Loss: 0.000743876735214144\n",
      "Epoch 886, Loss: 0.03401300456607714, Final Batch Loss: 0.022717034444212914\n",
      "Epoch 887, Loss: 0.0040648787980899215, Final Batch Loss: 0.0017098255921155214\n",
      "Epoch 888, Loss: 0.04103765857871622, Final Batch Loss: 0.013115376234054565\n",
      "Epoch 889, Loss: 0.039262699719984084, Final Batch Loss: 0.033824797719717026\n",
      "Epoch 890, Loss: 0.007623904501087964, Final Batch Loss: 0.0006242548115551472\n",
      "Epoch 891, Loss: 0.025414195435587317, Final Batch Loss: 0.0012353591155260801\n",
      "Epoch 892, Loss: 0.006147992331534624, Final Batch Loss: 0.0016047985991463065\n",
      "Epoch 893, Loss: 0.017783124785637483, Final Batch Loss: 0.0003115069994237274\n",
      "Epoch 894, Loss: 0.015332461800426245, Final Batch Loss: 0.0028048530220985413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 895, Loss: 0.015405404788907617, Final Batch Loss: 0.0005604449543170631\n",
      "Epoch 896, Loss: 0.012012673309072852, Final Batch Loss: 0.0007238530088216066\n",
      "Epoch 897, Loss: 0.004855323291849345, Final Batch Loss: 0.0005855076597072184\n",
      "Epoch 898, Loss: 0.0190646939445287, Final Batch Loss: 0.0022217321675270796\n",
      "Epoch 899, Loss: 0.012312795559410006, Final Batch Loss: 0.0008317265310324728\n",
      "Epoch 900, Loss: 0.010991449235007167, Final Batch Loss: 0.0018876635003834963\n",
      "Epoch 901, Loss: 0.012944506481289864, Final Batch Loss: 0.002186530502513051\n",
      "Epoch 902, Loss: 0.030013504321686924, Final Batch Loss: 0.0010801462922245264\n",
      "Epoch 903, Loss: 0.04893208760768175, Final Batch Loss: 0.0005000114906579256\n",
      "Epoch 904, Loss: 0.00517523312009871, Final Batch Loss: 0.0004615118377842009\n",
      "Epoch 905, Loss: 0.03996271651703864, Final Batch Loss: 0.001648997189477086\n",
      "Epoch 906, Loss: 0.012980831554159522, Final Batch Loss: 0.0025113432202488184\n",
      "Epoch 907, Loss: 0.01088561725919135, Final Batch Loss: 0.0014124190201982856\n",
      "Epoch 908, Loss: 0.04118673061020672, Final Batch Loss: 0.0015983980847522616\n",
      "Epoch 909, Loss: 0.025002957496326417, Final Batch Loss: 0.0010870915139093995\n",
      "Epoch 910, Loss: 0.005433035141322762, Final Batch Loss: 0.0007860703044570982\n",
      "Epoch 911, Loss: 0.014194358140230179, Final Batch Loss: 0.005332904402166605\n",
      "Epoch 912, Loss: 0.008360883453860879, Final Batch Loss: 0.00039599183946847916\n",
      "Epoch 913, Loss: 0.0032873782329261303, Final Batch Loss: 0.00048440444516018033\n",
      "Epoch 914, Loss: 0.0088677320163697, Final Batch Loss: 0.0002920629922300577\n",
      "Epoch 915, Loss: 0.009000184247270226, Final Batch Loss: 0.0011820649961009622\n",
      "Epoch 916, Loss: 0.01111859705997631, Final Batch Loss: 0.0013982178643345833\n",
      "Epoch 917, Loss: 0.009424901916645467, Final Batch Loss: 0.0016324837924912572\n",
      "Epoch 918, Loss: 0.006943139829672873, Final Batch Loss: 0.0014342940412461758\n",
      "Epoch 919, Loss: 0.018164798035286367, Final Batch Loss: 0.003469631541520357\n",
      "Epoch 920, Loss: 0.0069111016928218305, Final Batch Loss: 0.00034519494511187077\n",
      "Epoch 921, Loss: 0.027434797077148687, Final Batch Loss: 0.0001131491080741398\n",
      "Epoch 922, Loss: 0.011116050911368802, Final Batch Loss: 0.0006175646558403969\n",
      "Epoch 923, Loss: 0.01122585276607424, Final Batch Loss: 0.0018143034540116787\n",
      "Epoch 924, Loss: 0.007429149933159351, Final Batch Loss: 0.0016125846887007356\n",
      "Epoch 925, Loss: 0.005103082570713013, Final Batch Loss: 0.0025856599677354097\n",
      "Epoch 926, Loss: 0.029167823726311326, Final Batch Loss: 0.0037096126470714808\n",
      "Epoch 927, Loss: 0.004554362618364394, Final Batch Loss: 0.001199567224830389\n",
      "Epoch 928, Loss: 0.007426131924148649, Final Batch Loss: 0.0005568439955823123\n",
      "Epoch 929, Loss: 0.005016264243749902, Final Batch Loss: 0.0002630318340379745\n",
      "Epoch 930, Loss: 0.0038500974769704044, Final Batch Loss: 0.0006334729841910303\n",
      "Epoch 931, Loss: 0.0067226176033727825, Final Batch Loss: 0.0009332805057056248\n",
      "Epoch 932, Loss: 0.01877434051129967, Final Batch Loss: 0.0020509411115199327\n",
      "Epoch 933, Loss: 0.003072826482821256, Final Batch Loss: 0.00028234958881512284\n",
      "Epoch 934, Loss: 0.006702443235553801, Final Batch Loss: 0.0005443035042844713\n",
      "Epoch 935, Loss: 0.057629793824162334, Final Batch Loss: 0.05352884531021118\n",
      "Epoch 936, Loss: 0.00956528281676583, Final Batch Loss: 0.0005106804892420769\n",
      "Epoch 937, Loss: 0.020505762775428593, Final Batch Loss: 0.013232710771262646\n",
      "Epoch 938, Loss: 0.017060318030416965, Final Batch Loss: 0.004930128809064627\n",
      "Epoch 939, Loss: 0.017954594921320677, Final Batch Loss: 0.008517575450241566\n",
      "Epoch 940, Loss: 0.03175644762814045, Final Batch Loss: 7.311833905987442e-05\n",
      "Epoch 941, Loss: 0.0026874253526329994, Final Batch Loss: 0.0005581983714364469\n",
      "Epoch 942, Loss: 0.010378523904364556, Final Batch Loss: 0.007361893076449633\n",
      "Epoch 943, Loss: 0.0030468184559140354, Final Batch Loss: 0.00026619413984008133\n",
      "Epoch 944, Loss: 0.0324107704218477, Final Batch Loss: 0.0004652275238186121\n",
      "Epoch 945, Loss: 0.02108449680963531, Final Batch Loss: 0.003679126501083374\n",
      "Epoch 946, Loss: 0.010684868902899325, Final Batch Loss: 0.0004119717050343752\n",
      "Epoch 947, Loss: 0.006398793571861461, Final Batch Loss: 0.0005052495980635285\n",
      "Epoch 948, Loss: 0.015078501193784177, Final Batch Loss: 0.002721623284742236\n",
      "Epoch 949, Loss: 0.016957993677351624, Final Batch Loss: 0.0005293586873449385\n",
      "Epoch 950, Loss: 0.0037869007792323828, Final Batch Loss: 0.0011824429966509342\n",
      "Epoch 951, Loss: 0.003935106229619123, Final Batch Loss: 0.0006351700867526233\n",
      "Epoch 952, Loss: 0.010138875964912586, Final Batch Loss: 0.0009499697480350733\n",
      "Epoch 953, Loss: 0.0071331418585032225, Final Batch Loss: 0.0018547127256169915\n",
      "Epoch 954, Loss: 0.010356715865782462, Final Batch Loss: 0.0006155919400043786\n",
      "Epoch 955, Loss: 0.00246469140984118, Final Batch Loss: 0.0006585897062905133\n",
      "Epoch 956, Loss: 0.05691844824468717, Final Batch Loss: 0.00046659907093271613\n",
      "Epoch 957, Loss: 0.014994983037468046, Final Batch Loss: 0.0008868312579579651\n",
      "Epoch 958, Loss: 0.008317753323353827, Final Batch Loss: 0.0013599194353446364\n",
      "Epoch 959, Loss: 0.0057350973947905, Final Batch Loss: 0.0006543085328303277\n",
      "Epoch 960, Loss: 0.01789537977310829, Final Batch Loss: 0.00021174081484787166\n",
      "Epoch 961, Loss: 0.006290653138421476, Final Batch Loss: 0.00032949517481029034\n",
      "Epoch 962, Loss: 0.004116233059903607, Final Batch Loss: 0.001473182812333107\n",
      "Epoch 963, Loss: 0.034487656463170424, Final Batch Loss: 0.000260963017353788\n",
      "Epoch 964, Loss: 0.04464316455414519, Final Batch Loss: 0.003538416465744376\n",
      "Epoch 965, Loss: 0.009239399922080338, Final Batch Loss: 0.0012459896970540285\n",
      "Epoch 966, Loss: 0.006320614134892821, Final Batch Loss: 0.0006366468733176589\n",
      "Epoch 967, Loss: 0.07006111467489973, Final Batch Loss: 0.06733504682779312\n",
      "Epoch 968, Loss: 0.006226469820830971, Final Batch Loss: 0.00319321034476161\n",
      "Epoch 969, Loss: 0.011242579203099012, Final Batch Loss: 0.006053242366760969\n",
      "Epoch 970, Loss: 0.0020024168770760298, Final Batch Loss: 0.00028875580755993724\n",
      "Epoch 971, Loss: 0.008151955902576447, Final Batch Loss: 0.0037284388672560453\n",
      "Epoch 972, Loss: 0.009725172101752833, Final Batch Loss: 0.0003210809954907745\n",
      "Epoch 973, Loss: 0.01485761019284837, Final Batch Loss: 0.0010545395780354738\n",
      "Epoch 974, Loss: 0.004981287202099338, Final Batch Loss: 0.00209474447183311\n",
      "Epoch 975, Loss: 0.0356197573710233, Final Batch Loss: 0.029294053092598915\n",
      "Epoch 976, Loss: 0.005991780926706269, Final Batch Loss: 0.004374724812805653\n",
      "Epoch 977, Loss: 0.0152899184031412, Final Batch Loss: 0.005209696013480425\n",
      "Epoch 978, Loss: 0.003713255515322089, Final Batch Loss: 0.0005734729347750545\n",
      "Epoch 979, Loss: 0.012498961412347853, Final Batch Loss: 0.0037266500294208527\n",
      "Epoch 980, Loss: 0.028222194116096944, Final Batch Loss: 0.004500364884734154\n",
      "Epoch 981, Loss: 0.007983338524354622, Final Batch Loss: 0.0025759791024029255\n",
      "Epoch 982, Loss: 0.005189051618799567, Final Batch Loss: 0.0004023368237540126\n",
      "Epoch 983, Loss: 0.002859383530449122, Final Batch Loss: 0.0008310413104481995\n",
      "Epoch 984, Loss: 0.003860237920889631, Final Batch Loss: 0.002003265544772148\n",
      "Epoch 985, Loss: 0.014711205789353698, Final Batch Loss: 0.0009462008019909263\n",
      "Epoch 986, Loss: 0.002145295584341511, Final Batch Loss: 0.000267831317614764\n",
      "Epoch 987, Loss: 0.00954509130679071, Final Batch Loss: 0.001810141489841044\n",
      "Epoch 988, Loss: 0.015513882361119613, Final Batch Loss: 0.00028110918356105685\n",
      "Epoch 989, Loss: 0.03046180898672901, Final Batch Loss: 0.00027310848236083984\n",
      "Epoch 990, Loss: 0.00836991734104231, Final Batch Loss: 0.00036771217128261924\n",
      "Epoch 991, Loss: 0.0141692825127393, Final Batch Loss: 0.0015635439194738865\n",
      "Epoch 992, Loss: 0.021560835884884, Final Batch Loss: 0.0014348512049764395\n",
      "Epoch 993, Loss: 0.007475464459275827, Final Batch Loss: 0.0009358215611428022\n",
      "Epoch 994, Loss: 0.01585686657926999, Final Batch Loss: 0.0008296779706142843\n",
      "Epoch 995, Loss: 0.009498367609921843, Final Batch Loss: 0.004701448604464531\n",
      "Epoch 996, Loss: 0.05651802965439856, Final Batch Loss: 0.05230697989463806\n",
      "Epoch 997, Loss: 0.006305140603217296, Final Batch Loss: 0.000206865937798284\n",
      "Epoch 998, Loss: 0.018044969627226237, Final Batch Loss: 0.0001032593208947219\n",
      "Epoch 999, Loss: 0.011263306310866028, Final Batch Loss: 0.0014581619761884212\n",
      "Epoch 1000, Loss: 0.0032408172264695168, Final Batch Loss: 0.0003328100428916514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1001, Loss: 0.00857741286745295, Final Batch Loss: 0.000578062201384455\n",
      "Epoch 1002, Loss: 0.0047927255218382925, Final Batch Loss: 0.001189662260003388\n",
      "Epoch 1003, Loss: 0.00587622266903054, Final Batch Loss: 0.0032683336175978184\n",
      "Epoch 1004, Loss: 0.022704675619024783, Final Batch Loss: 0.0006019425927661359\n",
      "Epoch 1005, Loss: 0.003046631201868877, Final Batch Loss: 0.00023383134976029396\n",
      "Epoch 1006, Loss: 0.0022939199116081, Final Batch Loss: 0.0009084416087716818\n",
      "Epoch 1007, Loss: 0.02255455998238176, Final Batch Loss: 0.0011530715273693204\n",
      "Epoch 1008, Loss: 0.009573462055413984, Final Batch Loss: 0.005710634868592024\n",
      "Epoch 1009, Loss: 0.0022579310461878777, Final Batch Loss: 0.0002526656026020646\n",
      "Epoch 1010, Loss: 0.0030582806502934545, Final Batch Loss: 0.0017952423077076674\n",
      "Epoch 1011, Loss: 0.0385015542851761, Final Batch Loss: 0.03296668455004692\n",
      "Epoch 1012, Loss: 0.03448396714520641, Final Batch Loss: 0.00039538322016596794\n",
      "Epoch 1013, Loss: 0.005191742529859766, Final Batch Loss: 0.0003285339626017958\n",
      "Epoch 1014, Loss: 0.005048829043516889, Final Batch Loss: 0.00015033266390673816\n",
      "Epoch 1015, Loss: 0.005473945173434913, Final Batch Loss: 0.000788553268648684\n",
      "Epoch 1016, Loss: 0.0056030017149169, Final Batch Loss: 0.00031927975942380726\n",
      "Epoch 1017, Loss: 0.00632351532112807, Final Batch Loss: 0.00158922397531569\n",
      "Epoch 1018, Loss: 0.010824301396496594, Final Batch Loss: 0.0006922786124050617\n",
      "Epoch 1019, Loss: 0.013927483407314867, Final Batch Loss: 0.0007938483031466603\n",
      "Epoch 1020, Loss: 0.003913040942279622, Final Batch Loss: 0.00043875208939425647\n",
      "Epoch 1021, Loss: 0.010597995424177498, Final Batch Loss: 0.001721013686619699\n",
      "Epoch 1022, Loss: 0.013447970733977854, Final Batch Loss: 0.00048621196765452623\n",
      "Epoch 1023, Loss: 0.008250745333498344, Final Batch Loss: 0.0003896003763657063\n",
      "Epoch 1024, Loss: 0.008440535893896595, Final Batch Loss: 0.0011599953286349773\n",
      "Epoch 1025, Loss: 0.02197543124202639, Final Batch Loss: 0.01881800964474678\n",
      "Epoch 1026, Loss: 0.006689369271043688, Final Batch Loss: 0.0002771938743535429\n",
      "Epoch 1027, Loss: 0.04339111258741468, Final Batch Loss: 0.04228787124156952\n",
      "Epoch 1028, Loss: 0.032745838980190456, Final Batch Loss: 0.000418788637034595\n",
      "Epoch 1029, Loss: 0.030723495001439005, Final Batch Loss: 0.0021355508361011744\n",
      "Epoch 1030, Loss: 0.06308333834749646, Final Batch Loss: 0.025824764743447304\n",
      "Epoch 1031, Loss: 0.09691964223748073, Final Batch Loss: 0.0009544502827338874\n",
      "Epoch 1032, Loss: 0.01193430449347943, Final Batch Loss: 0.004388270899653435\n",
      "Epoch 1033, Loss: 0.01988281143712811, Final Batch Loss: 0.0003960272006224841\n",
      "Epoch 1034, Loss: 0.006111611903179437, Final Batch Loss: 0.003324090037494898\n",
      "Epoch 1035, Loss: 0.018216497788671404, Final Batch Loss: 0.00975274108350277\n",
      "Epoch 1036, Loss: 0.030626674881204963, Final Batch Loss: 0.026642387732863426\n",
      "Epoch 1037, Loss: 0.0508886706084013, Final Batch Loss: 0.0008859081426635385\n",
      "Epoch 1038, Loss: 0.005870576744200662, Final Batch Loss: 0.00041979647357948124\n",
      "Epoch 1039, Loss: 0.016672455822117627, Final Batch Loss: 0.0014393528690561652\n",
      "Epoch 1040, Loss: 0.008173456066288054, Final Batch Loss: 0.00021906878100708127\n",
      "Epoch 1041, Loss: 0.0019612143514677882, Final Batch Loss: 0.0004290816723369062\n",
      "Epoch 1042, Loss: 0.006642847904004157, Final Batch Loss: 0.0014768402324989438\n",
      "Epoch 1043, Loss: 0.030666250735521317, Final Batch Loss: 0.010997770354151726\n",
      "Epoch 1044, Loss: 0.006756325718015432, Final Batch Loss: 0.0012034039245918393\n",
      "Epoch 1045, Loss: 0.03888530208496377, Final Batch Loss: 0.006374299060553312\n",
      "Epoch 1046, Loss: 0.006246692792046815, Final Batch Loss: 0.0002984156017191708\n",
      "Epoch 1047, Loss: 0.0026151254714932293, Final Batch Loss: 0.0005813749739900231\n",
      "Epoch 1048, Loss: 0.013343826460186392, Final Batch Loss: 0.000565890280995518\n",
      "Epoch 1049, Loss: 0.011377137911040336, Final Batch Loss: 0.0038478723727166653\n",
      "Epoch 1050, Loss: 0.004702782724052668, Final Batch Loss: 0.0005774524179287255\n",
      "Epoch 1051, Loss: 0.0033208015156560577, Final Batch Loss: 0.0001099274741136469\n",
      "Epoch 1052, Loss: 0.0033060064306482673, Final Batch Loss: 0.000622701772954315\n",
      "Epoch 1053, Loss: 0.016639197026961483, Final Batch Loss: 0.000593341130297631\n",
      "Epoch 1054, Loss: 0.009017042146297172, Final Batch Loss: 0.000320047460263595\n",
      "Epoch 1055, Loss: 0.013722209550905973, Final Batch Loss: 0.0046313852071762085\n",
      "Epoch 1056, Loss: 0.04595397113007493, Final Batch Loss: 0.0007651138002984226\n",
      "Epoch 1057, Loss: 0.01327409065561369, Final Batch Loss: 0.000594179262407124\n",
      "Epoch 1058, Loss: 0.04509041068376973, Final Batch Loss: 0.003127735573798418\n",
      "Epoch 1059, Loss: 0.007098468471667729, Final Batch Loss: 0.00014586762699764222\n",
      "Epoch 1060, Loss: 0.01913119514938444, Final Batch Loss: 0.001323789358139038\n",
      "Epoch 1061, Loss: 0.011214152560569346, Final Batch Loss: 0.0010535565670579672\n",
      "Epoch 1062, Loss: 0.011309028952382505, Final Batch Loss: 0.0018247562693431973\n",
      "Epoch 1063, Loss: 0.00610423187026754, Final Batch Loss: 0.0019137694034725428\n",
      "Epoch 1064, Loss: 0.003534728253725916, Final Batch Loss: 0.0004989567678421736\n",
      "Epoch 1065, Loss: 0.005838697499711998, Final Batch Loss: 0.00043315350194461644\n",
      "Epoch 1066, Loss: 0.01708498949301429, Final Batch Loss: 0.0003125578223261982\n",
      "Epoch 1067, Loss: 0.01084848094615154, Final Batch Loss: 0.006491302978247404\n",
      "Epoch 1068, Loss: 0.006048588315024972, Final Batch Loss: 0.0005648047663271427\n",
      "Epoch 1069, Loss: 0.05956071201944724, Final Batch Loss: 0.03495379909873009\n",
      "Epoch 1070, Loss: 0.07007436326239258, Final Batch Loss: 0.02934824675321579\n",
      "Epoch 1071, Loss: 0.015077901654876769, Final Batch Loss: 0.00035325263161212206\n",
      "Epoch 1072, Loss: 0.0027892609359696507, Final Batch Loss: 0.00031800405122339725\n",
      "Epoch 1073, Loss: 0.005403933464549482, Final Batch Loss: 0.0008056517690420151\n",
      "Epoch 1074, Loss: 0.020347931189462543, Final Batch Loss: 0.011971826665103436\n",
      "Epoch 1075, Loss: 0.010070104734040797, Final Batch Loss: 0.001709036179818213\n",
      "Epoch 1076, Loss: 0.023115260642953217, Final Batch Loss: 0.0026029155123978853\n",
      "Epoch 1077, Loss: 0.008582414069678634, Final Batch Loss: 0.0020670294761657715\n",
      "Epoch 1078, Loss: 0.029859037429559976, Final Batch Loss: 0.0011649008374661207\n",
      "Epoch 1079, Loss: 0.007819346734322608, Final Batch Loss: 0.0021613880526274443\n",
      "Epoch 1080, Loss: 0.010224538506008685, Final Batch Loss: 0.0020804035011678934\n",
      "Epoch 1081, Loss: 0.00773885368835181, Final Batch Loss: 0.0008988440968096256\n",
      "Epoch 1082, Loss: 0.023154617461841553, Final Batch Loss: 0.0005442430847324431\n",
      "Epoch 1083, Loss: 0.022533010167535394, Final Batch Loss: 0.01647014543414116\n",
      "Epoch 1084, Loss: 0.0077419926528818905, Final Batch Loss: 0.0003819750854745507\n",
      "Epoch 1085, Loss: 0.0248566985828802, Final Batch Loss: 0.019688628613948822\n",
      "Epoch 1086, Loss: 0.006867173477075994, Final Batch Loss: 0.0019964815583080053\n",
      "Epoch 1087, Loss: 0.0022582942619919777, Final Batch Loss: 0.0003728670417331159\n",
      "Epoch 1088, Loss: 0.01130508747883141, Final Batch Loss: 0.0010436425218358636\n",
      "Epoch 1089, Loss: 0.04924725607270375, Final Batch Loss: 0.014013797044754028\n",
      "Epoch 1090, Loss: 0.0071046564262360334, Final Batch Loss: 0.0005848402506671846\n",
      "Epoch 1091, Loss: 0.011067823565099388, Final Batch Loss: 0.003657611785456538\n",
      "Epoch 1092, Loss: 0.003416196588659659, Final Batch Loss: 0.0013619690435007215\n",
      "Epoch 1093, Loss: 0.006690268564852886, Final Batch Loss: 0.00015791640907991678\n",
      "Epoch 1094, Loss: 0.008305407653097063, Final Batch Loss: 0.005695488303899765\n",
      "Epoch 1095, Loss: 0.015074744951562025, Final Batch Loss: 0.00020110061450395733\n",
      "Epoch 1096, Loss: 0.007958007423439994, Final Batch Loss: 0.0013727827463299036\n",
      "Epoch 1097, Loss: 0.014274489774834365, Final Batch Loss: 0.000895176490303129\n",
      "Epoch 1098, Loss: 0.01680652005597949, Final Batch Loss: 0.007707042619585991\n",
      "Epoch 1099, Loss: 0.004844446200877428, Final Batch Loss: 0.0002965967287309468\n",
      "Epoch 1100, Loss: 0.007374281849479303, Final Batch Loss: 0.0010307072661817074\n",
      "Epoch 1101, Loss: 0.03761449741432443, Final Batch Loss: 0.0017008221475407481\n",
      "Epoch 1102, Loss: 0.00857353868195787, Final Batch Loss: 0.0013376757269725204\n",
      "Epoch 1103, Loss: 0.023514284257544205, Final Batch Loss: 0.0004878816835116595\n",
      "Epoch 1104, Loss: 0.010297300294041634, Final Batch Loss: 0.0011502736015245318\n",
      "Epoch 1105, Loss: 0.02189488848671317, Final Batch Loss: 0.015875088050961494\n",
      "Epoch 1106, Loss: 0.008357773709576577, Final Batch Loss: 0.002219701884314418\n",
      "Epoch 1107, Loss: 0.027675845340127125, Final Batch Loss: 0.0009572459384799004\n",
      "Epoch 1108, Loss: 0.003841945144813508, Final Batch Loss: 0.0006344102439470589\n",
      "Epoch 1109, Loss: 0.005660735216224566, Final Batch Loss: 0.00023662330931983888\n",
      "Epoch 1110, Loss: 0.025054272613488138, Final Batch Loss: 0.0015493551036342978\n",
      "Epoch 1111, Loss: 0.008442529710009694, Final Batch Loss: 0.006026388145983219\n",
      "Epoch 1112, Loss: 0.04457489401102066, Final Batch Loss: 0.003476580372080207\n",
      "Epoch 1113, Loss: 0.003171537915477529, Final Batch Loss: 0.0008458710508421063\n",
      "Epoch 1114, Loss: 0.0037337038083933294, Final Batch Loss: 0.0005278174648992717\n",
      "Epoch 1115, Loss: 0.0026654558023437858, Final Batch Loss: 0.000289152842015028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1116, Loss: 0.039608697341463994, Final Batch Loss: 0.00011960212577832863\n",
      "Epoch 1117, Loss: 0.005930112558417022, Final Batch Loss: 0.0005018007941544056\n",
      "Epoch 1118, Loss: 0.008285096409963444, Final Batch Loss: 0.0032498680520802736\n",
      "Epoch 1119, Loss: 0.00659211102174595, Final Batch Loss: 0.00010815239511430264\n",
      "Epoch 1120, Loss: 0.012740835605654866, Final Batch Loss: 0.0003158751642331481\n",
      "Epoch 1121, Loss: 0.008299909852212295, Final Batch Loss: 0.0002930251357611269\n",
      "Epoch 1122, Loss: 0.01156026718672365, Final Batch Loss: 0.0010247183963656425\n",
      "Epoch 1123, Loss: 0.006888950360007584, Final Batch Loss: 0.002961672842502594\n",
      "Epoch 1124, Loss: 0.01740600122138858, Final Batch Loss: 0.0003554366994649172\n",
      "Epoch 1125, Loss: 0.010228443308733404, Final Batch Loss: 0.004914118442684412\n",
      "Epoch 1126, Loss: 0.0035369338584132493, Final Batch Loss: 0.0014251258689910173\n",
      "Epoch 1127, Loss: 0.0017541481938678771, Final Batch Loss: 0.0003446057380642742\n",
      "Epoch 1128, Loss: 0.013686939317267388, Final Batch Loss: 0.009615152142941952\n",
      "Epoch 1129, Loss: 0.004181077121756971, Final Batch Loss: 0.0005189235671423376\n",
      "Epoch 1130, Loss: 0.0045494899386540055, Final Batch Loss: 0.0010114547330886126\n",
      "Epoch 1131, Loss: 0.004277461463061627, Final Batch Loss: 8.837182394927368e-05\n",
      "Epoch 1132, Loss: 0.002215863365563564, Final Batch Loss: 0.0004665490996558219\n",
      "Epoch 1133, Loss: 0.020407913310918957, Final Batch Loss: 0.018260497599840164\n",
      "Epoch 1134, Loss: 0.005038077855715528, Final Batch Loss: 0.00027108428184874356\n",
      "Epoch 1135, Loss: 0.01871356347692199, Final Batch Loss: 0.013704887591302395\n",
      "Epoch 1136, Loss: 0.013429780403384939, Final Batch Loss: 0.001152040553279221\n",
      "Epoch 1137, Loss: 0.015708063612692058, Final Batch Loss: 0.00037180641083978117\n",
      "Epoch 1138, Loss: 0.01287067262455821, Final Batch Loss: 0.007084038574248552\n",
      "Epoch 1139, Loss: 0.053246008668793365, Final Batch Loss: 0.0003832818183582276\n",
      "Epoch 1140, Loss: 0.001949938858160749, Final Batch Loss: 0.0004597483202815056\n",
      "Epoch 1141, Loss: 0.0044578491360880435, Final Batch Loss: 0.00034888574737124145\n",
      "Epoch 1142, Loss: 0.021200095186941326, Final Batch Loss: 0.0008847828721627593\n",
      "Epoch 1143, Loss: 0.005751321325078607, Final Batch Loss: 0.001072276383638382\n",
      "Epoch 1144, Loss: 0.011583462008275092, Final Batch Loss: 0.0059729572385549545\n",
      "Epoch 1145, Loss: 0.028378936345689, Final Batch Loss: 0.0015854163793846965\n",
      "Epoch 1146, Loss: 0.01809247277560644, Final Batch Loss: 0.00024791809846647084\n",
      "Epoch 1147, Loss: 0.004367881309008226, Final Batch Loss: 0.0003173666191287339\n",
      "Epoch 1148, Loss: 0.0052389707416296005, Final Batch Loss: 0.0018630479462444782\n",
      "Epoch 1149, Loss: 0.00515432603424415, Final Batch Loss: 0.0006361615960486233\n",
      "Epoch 1150, Loss: 0.02955950886826031, Final Batch Loss: 0.00023513552150689065\n",
      "Epoch 1151, Loss: 0.008186905935872346, Final Batch Loss: 0.0038071474991738796\n",
      "Epoch 1152, Loss: 0.006870808429084718, Final Batch Loss: 0.0014201863668859005\n",
      "Epoch 1153, Loss: 0.0019440819305600598, Final Batch Loss: 0.00023701386817265302\n",
      "Epoch 1154, Loss: 0.004503941658185795, Final Batch Loss: 0.00023926477297209203\n",
      "Epoch 1155, Loss: 0.00544951792107895, Final Batch Loss: 0.0005309254047460854\n",
      "Epoch 1156, Loss: 0.0065619931629044, Final Batch Loss: 9.324982966063544e-05\n",
      "Epoch 1157, Loss: 0.032897860699449666, Final Batch Loss: 0.0001757590944180265\n",
      "Epoch 1158, Loss: 0.0036274355952627957, Final Batch Loss: 0.000640045793261379\n",
      "Epoch 1159, Loss: 0.0032378679898101836, Final Batch Loss: 0.0005016374052502215\n",
      "Epoch 1160, Loss: 0.0034044792118947953, Final Batch Loss: 0.0014583150623366237\n",
      "Epoch 1161, Loss: 0.007673690706724301, Final Batch Loss: 0.006066218484193087\n",
      "Epoch 1162, Loss: 0.0037396280677057803, Final Batch Loss: 0.0016222998965531588\n",
      "Epoch 1163, Loss: 0.0035698762512765825, Final Batch Loss: 0.0018410946941003203\n",
      "Epoch 1164, Loss: 0.045559659076388925, Final Batch Loss: 8.907399023883045e-05\n",
      "Epoch 1165, Loss: 0.03579944631928811, Final Batch Loss: 0.013054304756224155\n",
      "Epoch 1166, Loss: 0.004898814848274924, Final Batch Loss: 0.0001871840941021219\n",
      "Epoch 1167, Loss: 0.0019868169620167464, Final Batch Loss: 0.0008644877234473825\n",
      "Epoch 1168, Loss: 0.0026029759901575744, Final Batch Loss: 0.001389549346640706\n",
      "Epoch 1169, Loss: 0.0026388103142380714, Final Batch Loss: 0.0005294567672535777\n",
      "Epoch 1170, Loss: 0.005913884000619873, Final Batch Loss: 0.0007916944450698793\n",
      "Epoch 1171, Loss: 0.0026524597815296147, Final Batch Loss: 0.00037641587550751865\n",
      "Epoch 1172, Loss: 0.002247810254630167, Final Batch Loss: 6.416279211407527e-05\n",
      "Epoch 1173, Loss: 0.027042723435442895, Final Batch Loss: 0.0008426264976151288\n",
      "Epoch 1174, Loss: 0.002529014105675742, Final Batch Loss: 0.0009227782138623297\n",
      "Epoch 1175, Loss: 0.0036064492451259866, Final Batch Loss: 0.0009577485616318882\n",
      "Epoch 1176, Loss: 0.0026494889316381887, Final Batch Loss: 0.00019510816491674632\n",
      "Epoch 1177, Loss: 0.06393625392229296, Final Batch Loss: 0.06253255903720856\n",
      "Epoch 1178, Loss: 0.03606884024338797, Final Batch Loss: 0.00030548765789717436\n",
      "Epoch 1179, Loss: 0.005122858216054738, Final Batch Loss: 0.001030490268021822\n",
      "Epoch 1180, Loss: 0.031151602161116898, Final Batch Loss: 0.017634015530347824\n",
      "Epoch 1181, Loss: 0.009922750294208527, Final Batch Loss: 0.0001288358762394637\n",
      "Epoch 1182, Loss: 0.009062385535798967, Final Batch Loss: 0.006198256276547909\n",
      "Epoch 1183, Loss: 0.0321192191040609, Final Batch Loss: 0.00028036770527251065\n",
      "Epoch 1184, Loss: 0.014281602459959686, Final Batch Loss: 0.00027329643489792943\n",
      "Epoch 1185, Loss: 0.00222386242239736, Final Batch Loss: 0.000387020583730191\n",
      "Epoch 1186, Loss: 0.004141661513131112, Final Batch Loss: 0.002869419986382127\n",
      "Epoch 1187, Loss: 0.011893293645698577, Final Batch Loss: 0.0002742184733506292\n",
      "Epoch 1188, Loss: 0.07534384744940326, Final Batch Loss: 0.06885433197021484\n",
      "Epoch 1189, Loss: 0.0041061943338718265, Final Batch Loss: 0.0006648945272900164\n",
      "Epoch 1190, Loss: 0.009889873734209687, Final Batch Loss: 0.0010960263898596168\n",
      "Epoch 1191, Loss: 0.00674055889248848, Final Batch Loss: 0.0003734611673280597\n",
      "Epoch 1192, Loss: 0.03281503976904787, Final Batch Loss: 0.0007721506990492344\n",
      "Epoch 1193, Loss: 0.0024768543080426753, Final Batch Loss: 0.0004251891514286399\n",
      "Epoch 1194, Loss: 0.006978282181080431, Final Batch Loss: 0.0005858198856003582\n",
      "Epoch 1195, Loss: 0.0033327674318570644, Final Batch Loss: 0.0010456722229719162\n",
      "Epoch 1196, Loss: 0.04113281086029019, Final Batch Loss: 0.00013668804604094476\n",
      "Epoch 1197, Loss: 0.029458494973368943, Final Batch Loss: 0.0010347990319132805\n",
      "Epoch 1198, Loss: 0.006305765055003576, Final Batch Loss: 0.0001296007976634428\n",
      "Epoch 1199, Loss: 0.00803859590087086, Final Batch Loss: 0.004253671504557133\n",
      "Epoch 1200, Loss: 0.013494881233782507, Final Batch Loss: 0.01039414294064045\n",
      "Epoch 1201, Loss: 0.010262719879392534, Final Batch Loss: 0.0016065395902842283\n",
      "Epoch 1202, Loss: 0.0037858416617382318, Final Batch Loss: 0.0012782284757122397\n",
      "Epoch 1203, Loss: 0.015503134782193229, Final Batch Loss: 0.00030726040131412446\n",
      "Epoch 1204, Loss: 0.01348686043638736, Final Batch Loss: 0.0007672776700928807\n",
      "Epoch 1205, Loss: 0.007467844232451171, Final Batch Loss: 0.0013650343753397465\n",
      "Epoch 1206, Loss: 0.0065775914408732206, Final Batch Loss: 0.0003202278458047658\n",
      "Epoch 1207, Loss: 0.003293020825367421, Final Batch Loss: 0.000887485162820667\n",
      "Epoch 1208, Loss: 0.012093432655092329, Final Batch Loss: 0.0019253670470789075\n",
      "Epoch 1209, Loss: 0.013503002701327205, Final Batch Loss: 0.0025026369839906693\n",
      "Epoch 1210, Loss: 0.003999703680165112, Final Batch Loss: 0.0015912101371213794\n",
      "Epoch 1211, Loss: 0.01136270684946794, Final Batch Loss: 0.00021723088866565377\n",
      "Epoch 1212, Loss: 0.006685346277663484, Final Batch Loss: 0.0015922350576147437\n",
      "Epoch 1213, Loss: 0.002083571016555652, Final Batch Loss: 0.0008642278844490647\n",
      "Epoch 1214, Loss: 0.0032452172017656267, Final Batch Loss: 0.0006282787071540952\n",
      "Epoch 1215, Loss: 0.05497632673359476, Final Batch Loss: 0.04508543387055397\n",
      "Epoch 1216, Loss: 0.02480574743822217, Final Batch Loss: 0.010069352574646473\n",
      "Epoch 1217, Loss: 0.007673642772715539, Final Batch Loss: 0.001765390974469483\n",
      "Epoch 1218, Loss: 0.04530143359443173, Final Batch Loss: 0.000529698736499995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1219, Loss: 0.0037650984595529735, Final Batch Loss: 0.0025704323779791594\n",
      "Epoch 1220, Loss: 0.008914904203265905, Final Batch Loss: 0.004104332998394966\n",
      "Epoch 1221, Loss: 0.022193884651642293, Final Batch Loss: 0.005099937319755554\n",
      "Epoch 1222, Loss: 0.012186722524347715, Final Batch Loss: 0.00017880408267956227\n",
      "Epoch 1223, Loss: 0.028024800994899124, Final Batch Loss: 0.0006114101852290332\n",
      "Epoch 1224, Loss: 0.005227328685577959, Final Batch Loss: 0.0013353143585845828\n",
      "Epoch 1225, Loss: 0.06846407701959834, Final Batch Loss: 0.06498891115188599\n",
      "Epoch 1226, Loss: 0.007178660947829485, Final Batch Loss: 0.0015589860267937183\n",
      "Epoch 1227, Loss: 0.009749530639965087, Final Batch Loss: 0.0005270185065455735\n",
      "Epoch 1228, Loss: 0.006750216547516175, Final Batch Loss: 0.005395554006099701\n",
      "Epoch 1229, Loss: 0.0021436320967040956, Final Batch Loss: 0.0002324605011381209\n",
      "Epoch 1230, Loss: 0.008335199061548337, Final Batch Loss: 0.00036254120641388\n",
      "Epoch 1231, Loss: 0.007057186507154256, Final Batch Loss: 0.0008969237678684294\n",
      "Epoch 1232, Loss: 0.004526655175141059, Final Batch Loss: 0.00014603153977077454\n",
      "Epoch 1233, Loss: 0.0022106125543359667, Final Batch Loss: 0.0001700051943771541\n",
      "Epoch 1234, Loss: 0.009059647389221936, Final Batch Loss: 0.00021669379202648997\n",
      "Epoch 1235, Loss: 0.007359153940342367, Final Batch Loss: 0.0011766990646719933\n",
      "Epoch 1236, Loss: 0.006427145621273667, Final Batch Loss: 0.0018210746347904205\n",
      "Epoch 1237, Loss: 0.003944909447454847, Final Batch Loss: 0.0006919384468346834\n",
      "Epoch 1238, Loss: 0.04283611092250794, Final Batch Loss: 0.04011324793100357\n",
      "Epoch 1239, Loss: 0.001372996048303321, Final Batch Loss: 0.00010078107879962772\n",
      "Epoch 1240, Loss: 0.016010437975637615, Final Batch Loss: 0.0015737026697024703\n",
      "Epoch 1241, Loss: 0.002388021777733229, Final Batch Loss: 0.0001264556631213054\n",
      "Epoch 1242, Loss: 0.004666288790758699, Final Batch Loss: 0.001611705170944333\n",
      "Epoch 1243, Loss: 0.007546006556367502, Final Batch Loss: 0.00010224684956483543\n",
      "Epoch 1244, Loss: 0.003766508598346263, Final Batch Loss: 0.00044610496843233705\n",
      "Epoch 1245, Loss: 0.02521178312599659, Final Batch Loss: 0.008674281649291515\n",
      "Epoch 1246, Loss: 0.007953854103107005, Final Batch Loss: 0.003948532976210117\n",
      "Epoch 1247, Loss: 0.005501821404322982, Final Batch Loss: 0.002020643325522542\n",
      "Epoch 1248, Loss: 0.008150655281497166, Final Batch Loss: 0.00025923404609784484\n",
      "Epoch 1249, Loss: 0.0027436933014541864, Final Batch Loss: 0.000633586139883846\n",
      "Epoch 1250, Loss: 0.0018741656822385266, Final Batch Loss: 0.00017193476378452033\n",
      "Epoch 1251, Loss: 0.009659130009822547, Final Batch Loss: 0.00026467430870980024\n",
      "Epoch 1252, Loss: 0.0035303038894198835, Final Batch Loss: 0.00033950357465073466\n",
      "Epoch 1253, Loss: 0.004114367256988771, Final Batch Loss: 0.0002962595608551055\n",
      "Epoch 1254, Loss: 0.0044072046293877065, Final Batch Loss: 0.0010868479730561376\n",
      "Epoch 1255, Loss: 0.0019666979205794632, Final Batch Loss: 0.0002994579554069787\n",
      "Epoch 1256, Loss: 0.006701797101413831, Final Batch Loss: 0.00018654597806744277\n",
      "Epoch 1257, Loss: 0.002620534098241478, Final Batch Loss: 0.00018622766947373748\n",
      "Epoch 1258, Loss: 0.004491478146519512, Final Batch Loss: 0.0008369650458917022\n",
      "Epoch 1259, Loss: 0.003476939207757823, Final Batch Loss: 0.0023485892452299595\n",
      "Epoch 1260, Loss: 0.0020415858161868528, Final Batch Loss: 5.070996121503413e-05\n",
      "Epoch 1261, Loss: 0.013795663980999961, Final Batch Loss: 0.005289389286190271\n",
      "Epoch 1262, Loss: 0.0032173067884286866, Final Batch Loss: 5.11051039211452e-05\n",
      "Epoch 1263, Loss: 0.016228100372245535, Final Batch Loss: 0.00036667686072178185\n",
      "Epoch 1264, Loss: 0.011089556152001023, Final Batch Loss: 0.00128181092441082\n",
      "Epoch 1265, Loss: 0.007244074386107968, Final Batch Loss: 4.460924901650287e-05\n",
      "Epoch 1266, Loss: 0.014070414938032627, Final Batch Loss: 0.005288923159241676\n",
      "Epoch 1267, Loss: 0.0037880562304053456, Final Batch Loss: 0.0004952619201503694\n",
      "Epoch 1268, Loss: 0.007349539693677798, Final Batch Loss: 0.0028051864355802536\n",
      "Epoch 1269, Loss: 0.004173119319602847, Final Batch Loss: 0.0004172170301899314\n",
      "Epoch 1270, Loss: 0.004176921560429037, Final Batch Loss: 0.00019388057989999652\n",
      "Epoch 1271, Loss: 0.0037571226712316275, Final Batch Loss: 0.0003306650323793292\n",
      "Epoch 1272, Loss: 0.002398112410446629, Final Batch Loss: 0.0005337714683264494\n",
      "Epoch 1273, Loss: 0.002554877777583897, Final Batch Loss: 0.00031566398683935404\n",
      "Epoch 1274, Loss: 0.00612826398719335, Final Batch Loss: 0.0005205283523537219\n",
      "Epoch 1275, Loss: 0.02847381989704445, Final Batch Loss: 0.0014372295700013638\n",
      "Epoch 1276, Loss: 0.018981819826876745, Final Batch Loss: 0.0005419192020781338\n",
      "Epoch 1277, Loss: 0.003939782414818183, Final Batch Loss: 0.0004146888095419854\n",
      "Epoch 1278, Loss: 0.07642793553532101, Final Batch Loss: 0.07049743831157684\n",
      "Epoch 1279, Loss: 0.011040221550501883, Final Batch Loss: 0.0017155447276309133\n",
      "Epoch 1280, Loss: 0.0029250988009152934, Final Batch Loss: 0.0004429799737408757\n",
      "Epoch 1281, Loss: 0.008594783517764881, Final Batch Loss: 0.0011395763140171766\n",
      "Epoch 1282, Loss: 0.007641605625394732, Final Batch Loss: 0.0006330927135422826\n",
      "Epoch 1283, Loss: 0.006094415366533212, Final Batch Loss: 0.003537817858159542\n",
      "Epoch 1284, Loss: 0.02421402336040046, Final Batch Loss: 0.0004520686634350568\n",
      "Epoch 1285, Loss: 0.003432783531025052, Final Batch Loss: 0.00021760555682703853\n",
      "Epoch 1286, Loss: 0.005253412702586502, Final Batch Loss: 0.0013373363763093948\n",
      "Epoch 1287, Loss: 0.003109555007540621, Final Batch Loss: 0.0011256819125264883\n",
      "Epoch 1288, Loss: 0.001444116947823204, Final Batch Loss: 0.0003508341033011675\n",
      "Epoch 1289, Loss: 0.005640508083160967, Final Batch Loss: 0.0014356712345033884\n",
      "Epoch 1290, Loss: 0.01991870123310946, Final Batch Loss: 0.00045875567593611777\n",
      "Epoch 1291, Loss: 0.052245553059037775, Final Batch Loss: 0.01489726360887289\n",
      "Epoch 1292, Loss: 0.0027360935346223414, Final Batch Loss: 0.0006608578260056674\n",
      "Epoch 1293, Loss: 0.005183622415643185, Final Batch Loss: 0.0017490664031356573\n",
      "Epoch 1294, Loss: 0.003413930142414756, Final Batch Loss: 0.0005041583208367229\n",
      "Epoch 1295, Loss: 0.020271772053092718, Final Batch Loss: 0.00634037284180522\n",
      "Epoch 1296, Loss: 0.008185308892279863, Final Batch Loss: 0.0013335515977814794\n",
      "Epoch 1297, Loss: 0.021870575816137716, Final Batch Loss: 0.0010496798204258084\n",
      "Epoch 1298, Loss: 0.009008772773086093, Final Batch Loss: 0.007084811571985483\n",
      "Epoch 1299, Loss: 0.006936206511454657, Final Batch Loss: 0.0007360592717304826\n",
      "Epoch 1300, Loss: 0.007893631147453561, Final Batch Loss: 0.0002475979272276163\n",
      "Epoch 1301, Loss: 0.006956837809411809, Final Batch Loss: 0.0004742925229948014\n",
      "Epoch 1302, Loss: 0.0028659400122705847, Final Batch Loss: 0.0002887100854422897\n",
      "Epoch 1303, Loss: 0.05656606811680831, Final Batch Loss: 0.0004125590203329921\n",
      "Epoch 1304, Loss: 0.006406437372788787, Final Batch Loss: 0.0006620039348490536\n",
      "Epoch 1305, Loss: 0.002897547237807885, Final Batch Loss: 0.0018166593508794904\n",
      "Epoch 1306, Loss: 0.012731337803415954, Final Batch Loss: 0.0009580555488355458\n",
      "Epoch 1307, Loss: 0.014942975045414641, Final Batch Loss: 0.0002319149934919551\n",
      "Epoch 1308, Loss: 0.005651001702062786, Final Batch Loss: 0.003207596717402339\n",
      "Epoch 1309, Loss: 0.0026153427606914192, Final Batch Loss: 0.001082496833987534\n",
      "Epoch 1310, Loss: 0.015483646770007908, Final Batch Loss: 0.001885257544927299\n",
      "Epoch 1311, Loss: 0.009356517519336194, Final Batch Loss: 0.0005908468156121671\n",
      "Epoch 1312, Loss: 0.002959267483674921, Final Batch Loss: 0.0005030825850553811\n",
      "Epoch 1313, Loss: 0.008942653483245522, Final Batch Loss: 0.0004894404555670917\n",
      "Epoch 1314, Loss: 0.0018449948111083359, Final Batch Loss: 0.0005984438466839492\n",
      "Epoch 1315, Loss: 0.008510003550327383, Final Batch Loss: 0.00018665367679204792\n",
      "Epoch 1316, Loss: 0.003576837290893309, Final Batch Loss: 0.00018058727437164634\n",
      "Epoch 1317, Loss: 0.007603651327372063, Final Batch Loss: 0.00010330341319786385\n",
      "Epoch 1318, Loss: 0.009195237536914647, Final Batch Loss: 0.00031627167481929064\n",
      "Epoch 1319, Loss: 0.041693405495607294, Final Batch Loss: 0.0003189191920682788\n",
      "Epoch 1320, Loss: 0.028364567464450374, Final Batch Loss: 0.004997085779905319\n",
      "Epoch 1321, Loss: 0.02860103323473595, Final Batch Loss: 0.001578946365043521\n",
      "Epoch 1322, Loss: 0.005439365049824119, Final Batch Loss: 0.0012133038835600019\n",
      "Epoch 1323, Loss: 0.006911313743330538, Final Batch Loss: 0.0006626661634072661\n",
      "Epoch 1324, Loss: 0.0015308664733311161, Final Batch Loss: 0.00027473486261442304\n",
      "Epoch 1325, Loss: 0.0038335560238920152, Final Batch Loss: 0.00037990795681253076\n",
      "Epoch 1326, Loss: 0.0072709156083874404, Final Batch Loss: 0.0002904149005189538\n",
      "Epoch 1327, Loss: 0.003463736764388159, Final Batch Loss: 0.00020389619749039412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1328, Loss: 0.002640595455886796, Final Batch Loss: 0.0011363475350663066\n",
      "Epoch 1329, Loss: 0.003862778627080843, Final Batch Loss: 0.0003175104211550206\n",
      "Epoch 1330, Loss: 0.002048374415608123, Final Batch Loss: 0.00042932911310344934\n",
      "Epoch 1331, Loss: 0.01687067555030808, Final Batch Loss: 0.00036289935815148056\n",
      "Epoch 1332, Loss: 0.004766733974975068, Final Batch Loss: 5.709450488211587e-05\n",
      "Epoch 1333, Loss: 0.0032614058145554736, Final Batch Loss: 0.00013095491158310324\n",
      "Epoch 1334, Loss: 0.002128119522240013, Final Batch Loss: 0.00044092239113524556\n",
      "Epoch 1335, Loss: 0.012008937745122239, Final Batch Loss: 0.0009804132860153913\n",
      "Epoch 1336, Loss: 0.014633720275014639, Final Batch Loss: 0.000872324628289789\n",
      "Epoch 1337, Loss: 0.05701320880325511, Final Batch Loss: 0.0007676673703826964\n",
      "Epoch 1338, Loss: 0.0022620022064074874, Final Batch Loss: 0.00028885481879115105\n",
      "Epoch 1339, Loss: 0.020610183040844277, Final Batch Loss: 0.00037398203858174384\n",
      "Epoch 1340, Loss: 0.0052098444575676695, Final Batch Loss: 0.00019456424342934042\n",
      "Epoch 1341, Loss: 0.019138867035508156, Final Batch Loss: 0.003920601215213537\n",
      "Epoch 1342, Loss: 0.004385888751130551, Final Batch Loss: 0.00040767158498056233\n",
      "Epoch 1343, Loss: 0.010072043805848807, Final Batch Loss: 0.006152438465505838\n",
      "Epoch 1344, Loss: 0.0076655949669657275, Final Batch Loss: 9.622277866583318e-05\n",
      "Epoch 1345, Loss: 0.03671962639782578, Final Batch Loss: 0.003408791497349739\n",
      "Epoch 1346, Loss: 0.0026747835072455928, Final Batch Loss: 0.0004519857175182551\n",
      "Epoch 1347, Loss: 0.02037434119847603, Final Batch Loss: 0.00280436547473073\n",
      "Epoch 1348, Loss: 0.0025211835600202903, Final Batch Loss: 0.0009338789968751371\n",
      "Epoch 1349, Loss: 0.0032322581973858178, Final Batch Loss: 9.89180407486856e-05\n",
      "Epoch 1350, Loss: 0.010240863310173154, Final Batch Loss: 0.0010731248185038567\n",
      "Epoch 1351, Loss: 0.004210257102386095, Final Batch Loss: 0.00017897720681503415\n",
      "Epoch 1352, Loss: 0.014119477535132319, Final Batch Loss: 0.0007617276860401034\n",
      "Epoch 1353, Loss: 0.04563213756773621, Final Batch Loss: 0.011758661828935146\n",
      "Epoch 1354, Loss: 0.00586004089564085, Final Batch Loss: 0.0027364385314285755\n",
      "Epoch 1355, Loss: 0.004599096224410459, Final Batch Loss: 0.0009074858971871436\n",
      "Epoch 1356, Loss: 0.011424061260186136, Final Batch Loss: 0.0043408810161054134\n",
      "Epoch 1357, Loss: 0.009661440562922508, Final Batch Loss: 0.0026908060535788536\n",
      "Epoch 1358, Loss: 0.0063155750976875424, Final Batch Loss: 0.0033111460506916046\n",
      "Epoch 1359, Loss: 0.05715157491795253, Final Batch Loss: 0.055501531809568405\n",
      "Epoch 1360, Loss: 0.037090857862494886, Final Batch Loss: 0.0007585195126011968\n",
      "Epoch 1361, Loss: 0.010825603530975059, Final Batch Loss: 0.0038846638053655624\n",
      "Epoch 1362, Loss: 0.0024109149089781567, Final Batch Loss: 0.00010281274444423616\n",
      "Epoch 1363, Loss: 0.004656309087295085, Final Batch Loss: 0.00035848276456817985\n",
      "Epoch 1364, Loss: 0.0028085927187930793, Final Batch Loss: 0.0015145549550652504\n",
      "Epoch 1365, Loss: 0.006370229268213734, Final Batch Loss: 0.002281502354890108\n",
      "Epoch 1366, Loss: 0.004768807557411492, Final Batch Loss: 0.0006275432533584535\n",
      "Epoch 1367, Loss: 0.011379805335309356, Final Batch Loss: 0.0006061726599000394\n",
      "Epoch 1368, Loss: 0.005234045092947781, Final Batch Loss: 0.0011146063916385174\n",
      "Epoch 1369, Loss: 0.009611402841983363, Final Batch Loss: 0.004866390954703093\n",
      "Epoch 1370, Loss: 0.008579061570344493, Final Batch Loss: 0.00033882123534567654\n",
      "Epoch 1371, Loss: 0.0014061582769500092, Final Batch Loss: 0.00032210315112024546\n",
      "Epoch 1372, Loss: 0.0027007749158656225, Final Batch Loss: 0.0007327879429794848\n",
      "Epoch 1373, Loss: 0.0055780604016035795, Final Batch Loss: 0.0007552139577455819\n",
      "Epoch 1374, Loss: 0.028853365747636417, Final Batch Loss: 0.0005810857401229441\n",
      "Epoch 1375, Loss: 0.0035879535134881735, Final Batch Loss: 0.0007119438960216939\n",
      "Epoch 1376, Loss: 0.009525657049380243, Final Batch Loss: 0.0005783939268440008\n",
      "Epoch 1377, Loss: 0.004440426011569798, Final Batch Loss: 0.0002485877776052803\n",
      "Epoch 1378, Loss: 0.00573364133015275, Final Batch Loss: 0.0030605061911046505\n",
      "Epoch 1379, Loss: 0.026148038916289806, Final Batch Loss: 0.02085117995738983\n",
      "Epoch 1380, Loss: 0.01807046087924391, Final Batch Loss: 0.0122548658400774\n",
      "Epoch 1381, Loss: 0.01348251328454353, Final Batch Loss: 0.005923467222601175\n",
      "Epoch 1382, Loss: 0.004071129660587758, Final Batch Loss: 0.0027031272184103727\n",
      "Epoch 1383, Loss: 0.005593621637672186, Final Batch Loss: 0.0010112819727510214\n",
      "Epoch 1384, Loss: 0.002600815234472975, Final Batch Loss: 0.000581003783736378\n",
      "Epoch 1385, Loss: 0.005754251978942193, Final Batch Loss: 0.00041093063191510737\n",
      "Epoch 1386, Loss: 0.0028693235071841627, Final Batch Loss: 0.0005852822214365005\n",
      "Epoch 1387, Loss: 0.004001101944595575, Final Batch Loss: 0.0003655388136394322\n",
      "Epoch 1388, Loss: 0.004840754583710805, Final Batch Loss: 0.0008255258435383439\n",
      "Epoch 1389, Loss: 0.007252244744449854, Final Batch Loss: 0.0029937357176095247\n",
      "Epoch 1390, Loss: 0.005312091467203572, Final Batch Loss: 0.0016128316055983305\n",
      "Epoch 1391, Loss: 0.008547860896214843, Final Batch Loss: 0.00029108161106705666\n",
      "Epoch 1392, Loss: 0.0033363732509315014, Final Batch Loss: 0.0011397722410038114\n",
      "Epoch 1393, Loss: 0.01597372003016062, Final Batch Loss: 0.00029486141283996403\n",
      "Epoch 1394, Loss: 0.003847065265290439, Final Batch Loss: 0.0018490073271095753\n",
      "Epoch 1395, Loss: 0.002833120081049856, Final Batch Loss: 0.0007024327060207725\n",
      "Epoch 1396, Loss: 0.003145442169625312, Final Batch Loss: 0.00030154953128658235\n",
      "Epoch 1397, Loss: 0.0038898787315702066, Final Batch Loss: 0.0002940530830528587\n",
      "Epoch 1398, Loss: 0.014290616163634695, Final Batch Loss: 0.0002372006856603548\n",
      "Epoch 1399, Loss: 0.10092983391950838, Final Batch Loss: 0.09490350633859634\n",
      "Epoch 1400, Loss: 0.047659438190748915, Final Batch Loss: 0.0013059944612905383\n",
      "Epoch 1401, Loss: 0.026180299242696492, Final Batch Loss: 5.384515316109173e-05\n",
      "Epoch 1402, Loss: 0.013151380117051303, Final Batch Loss: 0.005539126694202423\n",
      "Epoch 1403, Loss: 0.020969145669369027, Final Batch Loss: 0.019200220704078674\n",
      "Epoch 1404, Loss: 0.01164031878579408, Final Batch Loss: 0.0070812636986374855\n",
      "Epoch 1405, Loss: 0.02899959613569081, Final Batch Loss: 0.002605077810585499\n",
      "Epoch 1406, Loss: 0.008302146336063743, Final Batch Loss: 0.0008130613714456558\n",
      "Epoch 1407, Loss: 0.004426194878760725, Final Batch Loss: 0.0020337700843811035\n",
      "Epoch 1408, Loss: 0.05528796277940273, Final Batch Loss: 0.01360273640602827\n",
      "Epoch 1409, Loss: 0.012286178302019835, Final Batch Loss: 0.00128755543846637\n",
      "Epoch 1410, Loss: 0.009067490173038095, Final Batch Loss: 0.006127767264842987\n",
      "Epoch 1411, Loss: 0.01603698666440323, Final Batch Loss: 0.0016382420435547829\n",
      "Epoch 1412, Loss: 0.007832278293790296, Final Batch Loss: 0.0006411743815988302\n",
      "Epoch 1413, Loss: 0.003550737368641421, Final Batch Loss: 0.00026363335200585425\n",
      "Epoch 1414, Loss: 0.025098320533288643, Final Batch Loss: 9.159257751889527e-05\n",
      "Epoch 1415, Loss: 0.03167033870704472, Final Batch Loss: 0.02901182882487774\n",
      "Epoch 1416, Loss: 0.004165448888670653, Final Batch Loss: 0.0010284928139299154\n",
      "Epoch 1417, Loss: 0.010983151965774596, Final Batch Loss: 0.0005922564305365086\n",
      "Epoch 1418, Loss: 0.006643741347943433, Final Batch Loss: 0.001319938455708325\n",
      "Epoch 1419, Loss: 0.0075206063920632005, Final Batch Loss: 0.0006323675042949617\n",
      "Epoch 1420, Loss: 0.005051848012953997, Final Batch Loss: 0.0004953459720127285\n",
      "Epoch 1421, Loss: 0.0035768192828982137, Final Batch Loss: 7.745177572360262e-05\n",
      "Epoch 1422, Loss: 0.006718464312143624, Final Batch Loss: 0.0004920861683785915\n",
      "Epoch 1423, Loss: 0.008745807921513915, Final Batch Loss: 0.0004269422497600317\n",
      "Epoch 1424, Loss: 0.0034196304477518424, Final Batch Loss: 0.0001523169339634478\n",
      "Epoch 1425, Loss: 0.005106627082568593, Final Batch Loss: 0.00013637024676427245\n",
      "Epoch 1426, Loss: 0.006102627143263817, Final Batch Loss: 0.001186121255159378\n",
      "Epoch 1427, Loss: 0.028750536483130418, Final Batch Loss: 0.0006853527738712728\n",
      "Epoch 1428, Loss: 0.003302950703073293, Final Batch Loss: 0.0014208025531843305\n",
      "Epoch 1429, Loss: 0.0027218230679864064, Final Batch Loss: 0.0016464428044855595\n",
      "Epoch 1430, Loss: 0.0011906701402040198, Final Batch Loss: 0.00013994566688779742\n",
      "Epoch 1431, Loss: 0.00518351328355493, Final Batch Loss: 0.0007035488961264491\n",
      "Epoch 1432, Loss: 0.005739942716900259, Final Batch Loss: 0.002463751705363393\n",
      "Epoch 1433, Loss: 0.03429629014863167, Final Batch Loss: 0.0003818055847659707\n",
      "Epoch 1434, Loss: 0.03047720983158797, Final Batch Loss: 0.0002172890235669911\n",
      "Epoch 1435, Loss: 0.0017524842696730047, Final Batch Loss: 0.0003209324786439538\n",
      "Epoch 1436, Loss: 0.020991626544855535, Final Batch Loss: 0.002983245998620987\n",
      "Epoch 1437, Loss: 0.0057077036472037435, Final Batch Loss: 0.004115836229175329\n",
      "Epoch 1438, Loss: 0.04641914811509196, Final Batch Loss: 0.045776087790727615\n",
      "Epoch 1439, Loss: 0.00397798532503657, Final Batch Loss: 0.002831093268468976\n",
      "Epoch 1440, Loss: 0.004491985106142238, Final Batch Loss: 8.649760275147855e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1441, Loss: 0.002831769787007943, Final Batch Loss: 0.00045074507943354547\n",
      "Epoch 1442, Loss: 0.012001791095826775, Final Batch Loss: 0.0005131830112077296\n",
      "Epoch 1443, Loss: 0.011076507798861712, Final Batch Loss: 0.0007439617183990777\n",
      "Epoch 1444, Loss: 0.0030222351124393754, Final Batch Loss: 5.496615631273016e-05\n",
      "Epoch 1445, Loss: 0.01624875520064961, Final Batch Loss: 0.0003522175538819283\n",
      "Epoch 1446, Loss: 0.003919041701010428, Final Batch Loss: 0.0023791794665157795\n",
      "Epoch 1447, Loss: 0.0039661406190134585, Final Batch Loss: 0.002133516827598214\n",
      "Epoch 1448, Loss: 0.0023011003504507244, Final Batch Loss: 0.0004777454014401883\n",
      "Epoch 1449, Loss: 0.01259318066877313, Final Batch Loss: 0.0014310573460534215\n",
      "Epoch 1450, Loss: 0.007588606418721611, Final Batch Loss: 0.005508209113031626\n",
      "Epoch 1451, Loss: 0.00741080884472467, Final Batch Loss: 0.00035476163611747324\n",
      "Epoch 1452, Loss: 0.028212096585775726, Final Batch Loss: 0.008184653706848621\n",
      "Epoch 1453, Loss: 0.005262547027086839, Final Batch Loss: 0.0024139932356774807\n",
      "Epoch 1454, Loss: 0.032157531939446926, Final Batch Loss: 0.01555123645812273\n",
      "Epoch 1455, Loss: 0.004615866258973256, Final Batch Loss: 0.0009948532097041607\n",
      "Epoch 1456, Loss: 0.00575544703315245, Final Batch Loss: 0.0001036239045788534\n",
      "Epoch 1457, Loss: 0.0036537619889713824, Final Batch Loss: 0.0012027344200760126\n",
      "Epoch 1458, Loss: 0.006905970600200817, Final Batch Loss: 0.0006984372739680111\n",
      "Epoch 1459, Loss: 0.004656868506572209, Final Batch Loss: 0.00021727111015934497\n",
      "Epoch 1460, Loss: 0.02018310339190066, Final Batch Loss: 0.0015835814410820603\n",
      "Epoch 1461, Loss: 0.006883315116283484, Final Batch Loss: 0.00029627015464939177\n",
      "Epoch 1462, Loss: 0.017695600632578135, Final Batch Loss: 0.0008730622939765453\n",
      "Epoch 1463, Loss: 0.0030817465885775164, Final Batch Loss: 0.00020064837008249015\n",
      "Epoch 1464, Loss: 0.014711888477904722, Final Batch Loss: 0.0003570737026166171\n",
      "Epoch 1465, Loss: 0.010825792327523232, Final Batch Loss: 0.0008515953668393195\n",
      "Epoch 1466, Loss: 0.007881949946749955, Final Batch Loss: 0.005446691066026688\n",
      "Epoch 1467, Loss: 0.004622927488526329, Final Batch Loss: 0.0001415145816281438\n",
      "Epoch 1468, Loss: 0.011947791586862877, Final Batch Loss: 0.000591139541938901\n",
      "Epoch 1469, Loss: 0.00415311218239367, Final Batch Loss: 0.002474459121003747\n",
      "Epoch 1470, Loss: 0.0036249386321287602, Final Batch Loss: 0.001171686570160091\n",
      "Epoch 1471, Loss: 0.008160124532878399, Final Batch Loss: 0.006631129886955023\n",
      "Epoch 1472, Loss: 0.09079006360843778, Final Batch Loss: 0.0703871101140976\n",
      "Epoch 1473, Loss: 0.0011400055227568373, Final Batch Loss: 0.00026980997063219547\n",
      "Epoch 1474, Loss: 0.0015109533269423991, Final Batch Loss: 0.0001235504896612838\n",
      "Epoch 1475, Loss: 0.002807823591865599, Final Batch Loss: 0.0006277056527324021\n",
      "Epoch 1476, Loss: 0.0035867799597326666, Final Batch Loss: 0.00027163876802660525\n",
      "Epoch 1477, Loss: 0.006238609734282363, Final Batch Loss: 0.003591239918023348\n",
      "Epoch 1478, Loss: 0.0030687939433846623, Final Batch Loss: 0.0005260438192635775\n",
      "Epoch 1479, Loss: 0.01862361401435919, Final Batch Loss: 0.000251197227044031\n",
      "Epoch 1480, Loss: 0.0063320288609247655, Final Batch Loss: 0.0008963727159425616\n",
      "Epoch 1481, Loss: 0.0024193023855332285, Final Batch Loss: 0.0005028773448430002\n",
      "Epoch 1482, Loss: 0.02193517261184752, Final Batch Loss: 0.020345941185951233\n",
      "Epoch 1483, Loss: 0.004192297303234227, Final Batch Loss: 0.0005751106073148549\n",
      "Epoch 1484, Loss: 0.002204569165769499, Final Batch Loss: 0.00010010913683800027\n",
      "Epoch 1485, Loss: 0.014057881380722392, Final Batch Loss: 0.00010321377340005711\n",
      "Epoch 1486, Loss: 0.013479548244504258, Final Batch Loss: 0.0034483650233596563\n",
      "Epoch 1487, Loss: 0.010590467660222203, Final Batch Loss: 0.0006030314834788442\n",
      "Epoch 1488, Loss: 0.0031589352001901716, Final Batch Loss: 0.00014686078066006303\n",
      "Epoch 1489, Loss: 0.002034806879237294, Final Batch Loss: 0.0005938602844253182\n",
      "Epoch 1490, Loss: 0.007231999930809252, Final Batch Loss: 0.000101399069535546\n",
      "Epoch 1491, Loss: 0.04840618162415922, Final Batch Loss: 0.0006241663359105587\n",
      "Epoch 1492, Loss: 0.02309500437695533, Final Batch Loss: 0.0007773570250719786\n",
      "Epoch 1493, Loss: 0.06910191653878428, Final Batch Loss: 0.06749450415372849\n",
      "Epoch 1494, Loss: 0.002362656290642917, Final Batch Loss: 0.00018452652147971094\n",
      "Epoch 1495, Loss: 0.009460061584832147, Final Batch Loss: 0.0010213973000645638\n",
      "Epoch 1496, Loss: 0.06591336047858931, Final Batch Loss: 0.0005204621120356023\n",
      "Epoch 1497, Loss: 0.007459618558641523, Final Batch Loss: 0.0006224007811397314\n",
      "Epoch 1498, Loss: 0.0037587799597531557, Final Batch Loss: 0.000716589274816215\n",
      "Epoch 1499, Loss: 0.011275319091510028, Final Batch Loss: 0.0031112406868487597\n",
      "Epoch 1500, Loss: 0.004826960677746683, Final Batch Loss: 0.0007921255892142653\n",
      "Epoch 1501, Loss: 0.004974517403752543, Final Batch Loss: 0.001523975981399417\n",
      "Epoch 1502, Loss: 0.02983414984191768, Final Batch Loss: 0.0003484082408249378\n",
      "Epoch 1503, Loss: 0.002800053436658345, Final Batch Loss: 0.00017621250299271196\n",
      "Epoch 1504, Loss: 0.024515617405995727, Final Batch Loss: 0.0015233566518872976\n",
      "Epoch 1505, Loss: 0.002670494024641812, Final Batch Loss: 0.0005701415939256549\n",
      "Epoch 1506, Loss: 0.004765390185639262, Final Batch Loss: 0.0005672802217304707\n",
      "Epoch 1507, Loss: 0.0085185745410854, Final Batch Loss: 0.00021491754159796983\n",
      "Epoch 1508, Loss: 0.00247495761141181, Final Batch Loss: 8.314460865221918e-05\n",
      "Epoch 1509, Loss: 0.012876152482931502, Final Batch Loss: 0.004208387341350317\n",
      "Epoch 1510, Loss: 0.004878454812569544, Final Batch Loss: 0.00035398415639065206\n",
      "Epoch 1511, Loss: 0.010673283526557498, Final Batch Loss: 0.009463980793952942\n",
      "Epoch 1512, Loss: 0.004467443359317258, Final Batch Loss: 0.0022868127562105656\n",
      "Epoch 1513, Loss: 0.0172769445925951, Final Batch Loss: 0.0014341924106702209\n",
      "Epoch 1514, Loss: 0.008148326407535933, Final Batch Loss: 0.00018533870752435178\n",
      "Epoch 1515, Loss: 0.004342152256867848, Final Batch Loss: 0.0016106670955196023\n",
      "Epoch 1516, Loss: 0.047581823862856254, Final Batch Loss: 0.0005713284481316805\n",
      "Epoch 1517, Loss: 0.004529819008894265, Final Batch Loss: 0.0020832018926739693\n",
      "Epoch 1518, Loss: 0.028341251745587215, Final Batch Loss: 0.0002729166008066386\n",
      "Epoch 1519, Loss: 0.021163635072298348, Final Batch Loss: 0.0005439952947199345\n",
      "Epoch 1520, Loss: 0.00384883105289191, Final Batch Loss: 0.0002660687023308128\n",
      "Epoch 1521, Loss: 0.005683015217073262, Final Batch Loss: 0.0023348985705524683\n",
      "Epoch 1522, Loss: 0.0037103057838976383, Final Batch Loss: 0.0015254761092364788\n",
      "Epoch 1523, Loss: 0.00798018486239016, Final Batch Loss: 0.005692372564226389\n",
      "Epoch 1524, Loss: 0.016213975759455934, Final Batch Loss: 0.00158892129547894\n",
      "Epoch 1525, Loss: 0.0036532952508423477, Final Batch Loss: 0.0007765311165712774\n",
      "Epoch 1526, Loss: 0.006988768836890813, Final Batch Loss: 5.612908717012033e-05\n",
      "Epoch 1527, Loss: 0.02421861818584148, Final Batch Loss: 0.0005539641715586185\n",
      "Epoch 1528, Loss: 0.006097744102589786, Final Batch Loss: 0.00038146149017848074\n",
      "Epoch 1529, Loss: 0.003365734279213939, Final Batch Loss: 5.5291988246608526e-05\n",
      "Epoch 1530, Loss: 0.0044010066776536405, Final Batch Loss: 0.000259467342402786\n",
      "Epoch 1531, Loss: 0.004015917278593406, Final Batch Loss: 0.001222339109517634\n",
      "Epoch 1532, Loss: 0.003624369914177805, Final Batch Loss: 0.0018990777898579836\n",
      "Epoch 1533, Loss: 0.002370609770878218, Final Batch Loss: 0.0002092688955599442\n",
      "Epoch 1534, Loss: 0.01796821276366245, Final Batch Loss: 0.00013808997755404562\n",
      "Epoch 1535, Loss: 0.006292152145761065, Final Batch Loss: 0.00022451514087151736\n",
      "Epoch 1536, Loss: 0.009829883318161592, Final Batch Loss: 0.0003577713796403259\n",
      "Epoch 1537, Loss: 0.003958503584726714, Final Batch Loss: 0.00021241109061520547\n",
      "Epoch 1538, Loss: 0.0032614773663226515, Final Batch Loss: 0.0006959486054256558\n",
      "Epoch 1539, Loss: 0.00412022543605417, Final Batch Loss: 0.0001530384470243007\n",
      "Epoch 1540, Loss: 0.003307973878690973, Final Batch Loss: 0.0002479831164237112\n",
      "Epoch 1541, Loss: 0.0186942741565872, Final Batch Loss: 0.00046920758904889226\n",
      "Epoch 1542, Loss: 0.0034802476584445685, Final Batch Loss: 0.0017382786609232426\n",
      "Epoch 1543, Loss: 0.0037359459383878857, Final Batch Loss: 0.0011519665131345391\n",
      "Epoch 1544, Loss: 0.02258948644157499, Final Batch Loss: 0.001057209330610931\n",
      "Epoch 1545, Loss: 0.08195818564854562, Final Batch Loss: 0.0026078245136886835\n",
      "Epoch 1546, Loss: 0.010903639427851886, Final Batch Loss: 0.0004275062237866223\n",
      "Epoch 1547, Loss: 0.007808639435097575, Final Batch Loss: 0.0009660747018642724\n",
      "Epoch 1548, Loss: 0.005327441904228181, Final Batch Loss: 0.00019543274538591504\n",
      "Epoch 1549, Loss: 0.017011851463394123, Final Batch Loss: 2.8527034373837523e-05\n",
      "Epoch 1550, Loss: 0.006026837014360353, Final Batch Loss: 0.0021910977084189653\n",
      "Epoch 1551, Loss: 0.004333368269726634, Final Batch Loss: 0.0014813970774412155\n",
      "Epoch 1552, Loss: 0.002053788470220752, Final Batch Loss: 0.00013139731890987605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1553, Loss: 0.005490468887728639, Final Batch Loss: 0.0011080943513661623\n",
      "Epoch 1554, Loss: 0.00361071054066997, Final Batch Loss: 0.0025354446843266487\n",
      "Epoch 1555, Loss: 0.005068555299658328, Final Batch Loss: 0.0005842780810780823\n",
      "Epoch 1556, Loss: 0.005370146973291412, Final Batch Loss: 0.003004312515258789\n",
      "Epoch 1557, Loss: 0.003472741474979557, Final Batch Loss: 0.0001812504488043487\n",
      "Epoch 1558, Loss: 0.004333668635808863, Final Batch Loss: 0.0005804976099170744\n",
      "Epoch 1559, Loss: 0.0010410443428554572, Final Batch Loss: 0.00010511358414078131\n",
      "Epoch 1560, Loss: 0.031018810666864738, Final Batch Loss: 0.003649696009233594\n",
      "Epoch 1561, Loss: 0.005517338890058454, Final Batch Loss: 0.0003118204476777464\n",
      "Epoch 1562, Loss: 0.009381022187881172, Final Batch Loss: 0.0010544387623667717\n",
      "Epoch 1563, Loss: 0.02056833363894839, Final Batch Loss: 0.0004070134600624442\n",
      "Epoch 1564, Loss: 0.003580970165785402, Final Batch Loss: 0.0026781775522977114\n",
      "Epoch 1565, Loss: 0.029646304552443326, Final Batch Loss: 0.0011460823006927967\n",
      "Epoch 1566, Loss: 0.002726520091528073, Final Batch Loss: 0.002062371699139476\n",
      "Epoch 1567, Loss: 0.0012417435355018824, Final Batch Loss: 0.00019282982975710183\n",
      "Epoch 1568, Loss: 0.018475899589248, Final Batch Loss: 0.013269448652863503\n",
      "Epoch 1569, Loss: 0.010084249413921498, Final Batch Loss: 0.0032619121484458447\n",
      "Epoch 1570, Loss: 0.019942798462579958, Final Batch Loss: 0.00016438002057839185\n",
      "Epoch 1571, Loss: 0.004325147223426029, Final Batch Loss: 0.00048387653077952564\n",
      "Epoch 1572, Loss: 0.00951074949989561, Final Batch Loss: 0.00019225712458137423\n",
      "Epoch 1573, Loss: 0.002902356383856386, Final Batch Loss: 0.0002011800097534433\n",
      "Epoch 1574, Loss: 0.012752282491419464, Final Batch Loss: 0.00021931505762040615\n",
      "Epoch 1575, Loss: 0.005296105009620078, Final Batch Loss: 0.004498742055147886\n",
      "Epoch 1576, Loss: 0.017133222077973187, Final Batch Loss: 0.00906116608530283\n",
      "Epoch 1577, Loss: 0.0194250745116733, Final Batch Loss: 0.00019793299725279212\n",
      "Epoch 1578, Loss: 0.003083458825130947, Final Batch Loss: 0.00023975950898602605\n",
      "Epoch 1579, Loss: 0.004932201190968044, Final Batch Loss: 0.0034993074368685484\n",
      "Epoch 1580, Loss: 0.0021823581482749432, Final Batch Loss: 0.00013729228521697223\n",
      "Epoch 1581, Loss: 0.01703247523983009, Final Batch Loss: 0.0002261160552734509\n",
      "Epoch 1582, Loss: 0.0055265393675654195, Final Batch Loss: 0.0001241932768607512\n",
      "Epoch 1583, Loss: 0.0027730113361030817, Final Batch Loss: 0.0004982775426469743\n",
      "Epoch 1584, Loss: 0.005315029760822654, Final Batch Loss: 0.00017566501628607512\n",
      "Epoch 1585, Loss: 0.003542945923982188, Final Batch Loss: 0.0014195729745551944\n",
      "Epoch 1586, Loss: 0.0012347413139650598, Final Batch Loss: 5.908324965275824e-05\n",
      "Epoch 1587, Loss: 0.018383987684501335, Final Batch Loss: 0.00019659503595903516\n",
      "Epoch 1588, Loss: 0.0016165950000868179, Final Batch Loss: 0.0005346916150301695\n",
      "Epoch 1589, Loss: 0.021439803938847035, Final Batch Loss: 0.0003622285439632833\n",
      "Epoch 1590, Loss: 0.06052157810336212, Final Batch Loss: 0.0581781268119812\n",
      "Epoch 1591, Loss: 0.006554707157192752, Final Batch Loss: 0.00591128459200263\n",
      "Epoch 1592, Loss: 0.003227732391678728, Final Batch Loss: 0.0003795590018853545\n",
      "Epoch 1593, Loss: 0.004800415117642842, Final Batch Loss: 0.00020047994621563703\n",
      "Epoch 1594, Loss: 0.005266037871479057, Final Batch Loss: 0.0002570690121501684\n",
      "Epoch 1595, Loss: 0.002555558065068908, Final Batch Loss: 0.0008595889084972441\n",
      "Epoch 1596, Loss: 0.0020878439645457547, Final Batch Loss: 4.995114795747213e-05\n",
      "Epoch 1597, Loss: 0.002592326869489625, Final Batch Loss: 0.0002810521109495312\n",
      "Epoch 1598, Loss: 0.03223921067547053, Final Batch Loss: 0.0004151304892729968\n",
      "Epoch 1599, Loss: 0.041661057257442735, Final Batch Loss: 0.01847182959318161\n",
      "Epoch 1600, Loss: 0.004765950987348333, Final Batch Loss: 0.00041225060704164207\n",
      "Epoch 1601, Loss: 0.05095506180077791, Final Batch Loss: 0.029073582962155342\n",
      "Epoch 1602, Loss: 0.0066147836041636765, Final Batch Loss: 0.0002877964871004224\n",
      "Epoch 1603, Loss: 0.02061433948983904, Final Batch Loss: 0.01949966885149479\n",
      "Epoch 1604, Loss: 0.0024698615598026663, Final Batch Loss: 0.0002791269216686487\n",
      "Epoch 1605, Loss: 0.016146435227710754, Final Batch Loss: 0.0003923812764696777\n",
      "Epoch 1606, Loss: 0.009881653677439317, Final Batch Loss: 0.00037966607487760484\n",
      "Epoch 1607, Loss: 0.004082488740095869, Final Batch Loss: 0.0009180210763588548\n",
      "Epoch 1608, Loss: 0.0025748753978405148, Final Batch Loss: 0.00045897287782281637\n",
      "Epoch 1609, Loss: 0.002632356568938121, Final Batch Loss: 0.0005836716736666858\n",
      "Epoch 1610, Loss: 0.0028905085200676695, Final Batch Loss: 0.00020347944519016892\n",
      "Epoch 1611, Loss: 0.0027041079592891037, Final Batch Loss: 0.00037532899295911193\n",
      "Epoch 1612, Loss: 0.003189456299878657, Final Batch Loss: 0.0015960432356223464\n",
      "Epoch 1613, Loss: 0.047752768827194814, Final Batch Loss: 8.087512833299115e-05\n",
      "Epoch 1614, Loss: 0.0026942857075482607, Final Batch Loss: 0.0005752007127739489\n",
      "Epoch 1615, Loss: 0.024273916147649288, Final Batch Loss: 0.0007527066045440733\n",
      "Epoch 1616, Loss: 0.0021090908121550456, Final Batch Loss: 0.0006220057839527726\n",
      "Epoch 1617, Loss: 0.002747858816292137, Final Batch Loss: 0.0005778464255854487\n",
      "Epoch 1618, Loss: 0.0030253236764110625, Final Batch Loss: 0.00036868450115434825\n",
      "Epoch 1619, Loss: 0.0032375410228269175, Final Batch Loss: 0.00021866113820578903\n",
      "Epoch 1620, Loss: 0.0034485662035876885, Final Batch Loss: 0.0011777206091210246\n",
      "Epoch 1621, Loss: 0.0022861265897518024, Final Batch Loss: 0.00153580610640347\n",
      "Epoch 1622, Loss: 0.0021692640148103237, Final Batch Loss: 0.0006130911642685533\n",
      "Epoch 1623, Loss: 0.004189918399788439, Final Batch Loss: 0.0001454849843867123\n",
      "Epoch 1624, Loss: 0.012885280651971698, Final Batch Loss: 0.0006827986799180508\n",
      "Epoch 1625, Loss: 0.003037723829038441, Final Batch Loss: 0.0008411353337578475\n",
      "Epoch 1626, Loss: 0.007331328553846106, Final Batch Loss: 0.000772502098698169\n",
      "Epoch 1627, Loss: 0.06895249159424566, Final Batch Loss: 0.0007703909650444984\n",
      "Epoch 1628, Loss: 0.0023718410811852664, Final Batch Loss: 0.0009134948486462235\n",
      "Epoch 1629, Loss: 0.011987040430540219, Final Batch Loss: 0.0034546498209238052\n",
      "Epoch 1630, Loss: 0.010630777993355878, Final Batch Loss: 0.009025760926306248\n",
      "Epoch 1631, Loss: 0.003680540045024827, Final Batch Loss: 0.001702447421848774\n",
      "Epoch 1632, Loss: 0.003753364013391547, Final Batch Loss: 0.00023210453218780458\n",
      "Epoch 1633, Loss: 0.006860553985461593, Final Batch Loss: 0.0026841165963560343\n",
      "Epoch 1634, Loss: 0.02344844161416404, Final Batch Loss: 0.001006942824460566\n",
      "Epoch 1635, Loss: 0.0019762843003263697, Final Batch Loss: 0.00043348356848582625\n",
      "Epoch 1636, Loss: 0.01563043482019566, Final Batch Loss: 0.0017550820484757423\n",
      "Epoch 1637, Loss: 0.00933650671504438, Final Batch Loss: 0.0017508298624306917\n",
      "Epoch 1638, Loss: 0.012966930720722303, Final Batch Loss: 0.0038787510711699724\n",
      "Epoch 1639, Loss: 0.029151497990824282, Final Batch Loss: 0.02645588479936123\n",
      "Epoch 1640, Loss: 0.0026084794953931123, Final Batch Loss: 0.0003344500146340579\n",
      "Epoch 1641, Loss: 0.002414551578112878, Final Batch Loss: 0.00021112417744006962\n",
      "Epoch 1642, Loss: 0.008927577990107238, Final Batch Loss: 0.0003867841442115605\n",
      "Epoch 1643, Loss: 0.03651921625714749, Final Batch Loss: 0.003620036179199815\n",
      "Epoch 1644, Loss: 0.007721512360149063, Final Batch Loss: 0.0002866222057491541\n",
      "Epoch 1645, Loss: 0.00555290502961725, Final Batch Loss: 0.0032245602924376726\n",
      "Epoch 1646, Loss: 0.0037342769210226834, Final Batch Loss: 0.0008407981949858367\n",
      "Epoch 1647, Loss: 0.0020270367967896163, Final Batch Loss: 0.0007980014197528362\n",
      "Epoch 1648, Loss: 0.00221909078391036, Final Batch Loss: 0.00010865640797419474\n",
      "Epoch 1649, Loss: 0.004897920982330106, Final Batch Loss: 0.0015796368243172765\n",
      "Epoch 1650, Loss: 0.0021963062172289938, Final Batch Loss: 0.000934696348849684\n",
      "Epoch 1651, Loss: 0.0033801481768023223, Final Batch Loss: 0.0011273986892774701\n",
      "Epoch 1652, Loss: 0.016552212473470718, Final Batch Loss: 0.00013615780335385352\n",
      "Epoch 1653, Loss: 0.015929332235828042, Final Batch Loss: 0.0008344529196619987\n",
      "Epoch 1654, Loss: 0.0293735186860431, Final Batch Loss: 0.02776351571083069\n",
      "Epoch 1655, Loss: 0.0047967477003112435, Final Batch Loss: 0.004095296375453472\n",
      "Epoch 1656, Loss: 0.0040626098634675145, Final Batch Loss: 0.0003648740239441395\n",
      "Epoch 1657, Loss: 0.012494270340539515, Final Batch Loss: 0.0006163408397696912\n",
      "Epoch 1658, Loss: 0.0051814990874845535, Final Batch Loss: 0.002983907936140895\n",
      "Epoch 1659, Loss: 0.003997857362264767, Final Batch Loss: 0.0003000465512741357\n",
      "Epoch 1660, Loss: 0.0013010971888434142, Final Batch Loss: 0.0004056468897033483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1661, Loss: 0.0025114958989433944, Final Batch Loss: 0.0004122201935388148\n",
      "Epoch 1662, Loss: 0.0013399743256741203, Final Batch Loss: 5.048678576713428e-05\n",
      "Epoch 1663, Loss: 0.0034924082865472883, Final Batch Loss: 0.00028567761182785034\n",
      "Epoch 1664, Loss: 0.007588969761854969, Final Batch Loss: 0.0013643800048157573\n",
      "Epoch 1665, Loss: 0.006496713089291006, Final Batch Loss: 0.0008161457371897995\n",
      "Epoch 1666, Loss: 0.016875838562555145, Final Batch Loss: 0.0005695646395906806\n",
      "Epoch 1667, Loss: 0.004506223995122127, Final Batch Loss: 0.0027537124697118998\n",
      "Epoch 1668, Loss: 0.002408485423075035, Final Batch Loss: 0.00022438468295149505\n",
      "Epoch 1669, Loss: 0.0013667156599694863, Final Batch Loss: 0.0005904019344598055\n",
      "Epoch 1670, Loss: 0.007650173327419907, Final Batch Loss: 0.004715087823569775\n",
      "Epoch 1671, Loss: 0.003606861486332491, Final Batch Loss: 0.000273685873253271\n",
      "Epoch 1672, Loss: 0.03076547008822672, Final Batch Loss: 0.0009716688073240221\n",
      "Epoch 1673, Loss: 0.039994560793275014, Final Batch Loss: 0.00038052565651014447\n",
      "Epoch 1674, Loss: 0.03816944846767001, Final Batch Loss: 0.0011658458970487118\n",
      "Epoch 1675, Loss: 0.0038634312732028775, Final Batch Loss: 7.678628753637895e-05\n",
      "Epoch 1676, Loss: 0.013243074441561475, Final Batch Loss: 0.0002569180214777589\n",
      "Epoch 1677, Loss: 0.005434101854916662, Final Batch Loss: 0.00015988954692147672\n",
      "Epoch 1678, Loss: 0.005454767364426516, Final Batch Loss: 9.342505654785782e-05\n",
      "Epoch 1679, Loss: 0.006212095380760729, Final Batch Loss: 0.001716603059321642\n",
      "Epoch 1680, Loss: 0.00672588380984962, Final Batch Loss: 0.003231724724173546\n",
      "Epoch 1681, Loss: 0.005505329827428795, Final Batch Loss: 0.004503158852458\n",
      "Epoch 1682, Loss: 0.005303453763190191, Final Batch Loss: 9.93265348370187e-05\n",
      "Epoch 1683, Loss: 0.0025156201736535877, Final Batch Loss: 0.00022098900808487087\n",
      "Epoch 1684, Loss: 0.028337840543827042, Final Batch Loss: 0.0007327616913244128\n",
      "Epoch 1685, Loss: 0.019569803233025596, Final Batch Loss: 0.00023686091299168766\n",
      "Epoch 1686, Loss: 0.008711610338650644, Final Batch Loss: 0.002177357906475663\n",
      "Epoch 1687, Loss: 0.004536138323601335, Final Batch Loss: 0.0002710302360355854\n",
      "Epoch 1688, Loss: 0.008439678487775382, Final Batch Loss: 0.0003793349605984986\n",
      "Epoch 1689, Loss: 0.0020432629389688373, Final Batch Loss: 0.0008759626070968807\n",
      "Epoch 1690, Loss: 0.0021889085473958403, Final Batch Loss: 0.0012030736543238163\n",
      "Epoch 1691, Loss: 0.0011134925516671501, Final Batch Loss: 4.929901479044929e-05\n",
      "Epoch 1692, Loss: 0.0248947991640307, Final Batch Loss: 0.020519990473985672\n",
      "Epoch 1693, Loss: 0.00430081834201701, Final Batch Loss: 0.003206918016076088\n",
      "Epoch 1694, Loss: 0.00603750059963204, Final Batch Loss: 0.0002153909590560943\n",
      "Epoch 1695, Loss: 0.01268801286641974, Final Batch Loss: 0.010282513685524464\n",
      "Epoch 1696, Loss: 0.03148111562768463, Final Batch Loss: 0.004393746145069599\n",
      "Epoch 1697, Loss: 0.0024346991413040087, Final Batch Loss: 0.00019733839144464582\n",
      "Epoch 1698, Loss: 0.0011001458915416151, Final Batch Loss: 0.0003810778434854001\n",
      "Epoch 1699, Loss: 0.001663896517129615, Final Batch Loss: 0.00014528479368891567\n",
      "Epoch 1700, Loss: 0.007998452740139328, Final Batch Loss: 8.528945909347385e-05\n",
      "Epoch 1701, Loss: 0.001560445671202615, Final Batch Loss: 0.0002625436754897237\n",
      "Epoch 1702, Loss: 0.015556903061224148, Final Batch Loss: 0.013695266097784042\n",
      "Epoch 1703, Loss: 0.07742782293644268, Final Batch Loss: 0.0011860777158290148\n",
      "Epoch 1704, Loss: 0.0038643273583147675, Final Batch Loss: 0.0005109791527502239\n",
      "Epoch 1705, Loss: 0.006163023645058274, Final Batch Loss: 0.0007697031833231449\n",
      "Epoch 1706, Loss: 0.003661958224256523, Final Batch Loss: 0.00015583734784740955\n",
      "Epoch 1707, Loss: 0.0011522127460921183, Final Batch Loss: 0.00031556267640553415\n",
      "Epoch 1708, Loss: 0.0019386743151699193, Final Batch Loss: 2.2676926164422184e-05\n",
      "Epoch 1709, Loss: 0.0010904582377406769, Final Batch Loss: 7.039232150418684e-05\n",
      "Epoch 1710, Loss: 0.004095438460353762, Final Batch Loss: 0.0008832910098135471\n",
      "Epoch 1711, Loss: 0.004513556035817601, Final Batch Loss: 0.0001881595962913707\n",
      "Epoch 1712, Loss: 0.001735904414090328, Final Batch Loss: 0.00019384051847737283\n",
      "Epoch 1713, Loss: 0.002178423834266141, Final Batch Loss: 0.0005382696981541812\n",
      "Epoch 1714, Loss: 0.003143818204989657, Final Batch Loss: 0.0002716572489589453\n",
      "Epoch 1715, Loss: 0.0065846819343278185, Final Batch Loss: 0.00017453705368097872\n",
      "Epoch 1716, Loss: 0.0023038570361677557, Final Batch Loss: 0.0003017920535057783\n",
      "Epoch 1717, Loss: 0.001992771402001381, Final Batch Loss: 0.00014385045506060123\n",
      "Epoch 1718, Loss: 0.005065070319687948, Final Batch Loss: 0.0001324908371316269\n",
      "Epoch 1719, Loss: 0.08618203857622575, Final Batch Loss: 0.05731188505887985\n",
      "Epoch 1720, Loss: 0.00749532799818553, Final Batch Loss: 0.0011834038887172937\n",
      "Epoch 1721, Loss: 0.005994658829877153, Final Batch Loss: 0.00033641254412941635\n",
      "Epoch 1722, Loss: 0.006758732793969102, Final Batch Loss: 0.004327097442001104\n",
      "Epoch 1723, Loss: 0.002933996256615501, Final Batch Loss: 0.0010495709720999002\n",
      "Epoch 1724, Loss: 0.03483222301292699, Final Batch Loss: 0.00016539471107535064\n",
      "Epoch 1725, Loss: 0.0022806160268373787, Final Batch Loss: 0.0007212464115582407\n",
      "Epoch 1726, Loss: 0.0016428386006737128, Final Batch Loss: 0.00016948401753325015\n",
      "Epoch 1727, Loss: 0.01231413020286709, Final Batch Loss: 0.00044158933451399207\n",
      "Epoch 1728, Loss: 0.002653331437613815, Final Batch Loss: 0.00039972891681827605\n",
      "Epoch 1729, Loss: 0.0060387590929167345, Final Batch Loss: 0.0003142168279737234\n",
      "Epoch 1730, Loss: 0.0013263639484648593, Final Batch Loss: 9.434857201995328e-05\n",
      "Epoch 1731, Loss: 0.011191790777957067, Final Batch Loss: 0.0006893465761095285\n",
      "Epoch 1732, Loss: 0.005918384937103838, Final Batch Loss: 0.0005522004212252796\n",
      "Epoch 1733, Loss: 0.0033079185377573594, Final Batch Loss: 0.00024292177113238722\n",
      "Epoch 1734, Loss: 0.0009857626864686608, Final Batch Loss: 0.0002389921573922038\n",
      "Epoch 1735, Loss: 0.005285205003019655, Final Batch Loss: 3.7470515962922946e-05\n",
      "Epoch 1736, Loss: 0.002687548767426051, Final Batch Loss: 0.0007002048660069704\n",
      "Epoch 1737, Loss: 0.0009582414568285458, Final Batch Loss: 0.0005372885498218238\n",
      "Epoch 1738, Loss: 0.0018489150679670274, Final Batch Loss: 0.0002802872331812978\n",
      "Epoch 1739, Loss: 0.00252035420271568, Final Batch Loss: 0.0010779876029118896\n",
      "Epoch 1740, Loss: 0.021276852465234697, Final Batch Loss: 0.0008810245781205595\n",
      "Epoch 1741, Loss: 0.004299261112464592, Final Batch Loss: 0.0004553093749564141\n",
      "Epoch 1742, Loss: 0.0047527783463010564, Final Batch Loss: 0.00022712086501996964\n",
      "Epoch 1743, Loss: 0.0038644086744170636, Final Batch Loss: 0.00019465411605779082\n",
      "Epoch 1744, Loss: 0.0029133607749827206, Final Batch Loss: 6.735249189659953e-05\n",
      "Epoch 1745, Loss: 0.003522543134749867, Final Batch Loss: 6.308514275588095e-05\n",
      "Epoch 1746, Loss: 0.0032602209830656648, Final Batch Loss: 0.000920590478926897\n",
      "Epoch 1747, Loss: 0.02939471499848878, Final Batch Loss: 0.027904625982046127\n",
      "Epoch 1748, Loss: 0.0010739342833403498, Final Batch Loss: 9.745392162585631e-05\n",
      "Epoch 1749, Loss: 0.0028624503247556277, Final Batch Loss: 0.0008633255492895842\n",
      "Epoch 1750, Loss: 0.00285707633884158, Final Batch Loss: 0.000944248226005584\n",
      "Epoch 1751, Loss: 0.002333930169697851, Final Batch Loss: 8.690886897966266e-05\n",
      "Epoch 1752, Loss: 0.03537450605654158, Final Batch Loss: 0.00022428357624448836\n",
      "Epoch 1753, Loss: 0.002638314945215825, Final Batch Loss: 6.918182043591514e-05\n",
      "Epoch 1754, Loss: 0.0013461583002936095, Final Batch Loss: 0.00028136593755334616\n",
      "Epoch 1755, Loss: 0.006155806069727987, Final Batch Loss: 0.0015422770520672202\n",
      "Epoch 1756, Loss: 0.0021132708789082244, Final Batch Loss: 0.0001396251900587231\n",
      "Epoch 1757, Loss: 0.004513870575465262, Final Batch Loss: 0.0016636870568618178\n",
      "Epoch 1758, Loss: 0.01632708022953011, Final Batch Loss: 0.0002600082952994853\n",
      "Epoch 1759, Loss: 0.003236497432226315, Final Batch Loss: 0.000721755379345268\n",
      "Epoch 1760, Loss: 0.0017512024787720293, Final Batch Loss: 0.0006692563183605671\n",
      "Epoch 1761, Loss: 0.0015803803107701242, Final Batch Loss: 0.000208677927730605\n",
      "Epoch 1762, Loss: 0.0026210183277726173, Final Batch Loss: 0.0008490751497447491\n",
      "Epoch 1763, Loss: 0.027546485187485814, Final Batch Loss: 0.0024855725932866335\n",
      "Epoch 1764, Loss: 0.00368824353790842, Final Batch Loss: 8.322109351865947e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1765, Loss: 0.002428820400382392, Final Batch Loss: 0.0006172538851387799\n",
      "Epoch 1766, Loss: 0.0021561179382842965, Final Batch Loss: 8.388941932935268e-05\n",
      "Epoch 1767, Loss: 0.0028754910235875286, Final Batch Loss: 0.00033284604432992637\n",
      "Epoch 1768, Loss: 0.0032482985698152333, Final Batch Loss: 0.0006987650995142758\n",
      "Epoch 1769, Loss: 0.00478671752352966, Final Batch Loss: 0.0002931830531451851\n",
      "Epoch 1770, Loss: 0.04558218845340889, Final Batch Loss: 0.00016451433475594968\n",
      "Epoch 1771, Loss: 0.0070257405313896015, Final Batch Loss: 0.00031908650998957455\n",
      "Epoch 1772, Loss: 0.0034651176392799243, Final Batch Loss: 0.00020198445417918265\n",
      "Epoch 1773, Loss: 0.006397987250238657, Final Batch Loss: 0.00037606354453600943\n",
      "Epoch 1774, Loss: 0.00183578283758834, Final Batch Loss: 0.0005379693466238678\n",
      "Epoch 1775, Loss: 0.07600016335345572, Final Batch Loss: 0.00011926612205570564\n",
      "Epoch 1776, Loss: 0.004946279936120845, Final Batch Loss: 0.0007911273860372603\n",
      "Epoch 1777, Loss: 0.03136234049452469, Final Batch Loss: 0.00034621322993189096\n",
      "Epoch 1778, Loss: 0.007427485543303192, Final Batch Loss: 0.0005715313600376248\n",
      "Epoch 1779, Loss: 0.004302699904656038, Final Batch Loss: 0.000255079212365672\n",
      "Epoch 1780, Loss: 0.001789195550372824, Final Batch Loss: 0.00017863517859950662\n",
      "Epoch 1781, Loss: 0.0028463150665629655, Final Batch Loss: 0.0010054371086880565\n",
      "Epoch 1782, Loss: 0.0038609753537457436, Final Batch Loss: 0.0016225004801526666\n",
      "Epoch 1783, Loss: 0.0039870421824161895, Final Batch Loss: 0.0005223649204708636\n",
      "Epoch 1784, Loss: 0.0035988433519378304, Final Batch Loss: 0.0021895940881222486\n",
      "Epoch 1785, Loss: 0.00239357294049114, Final Batch Loss: 0.0006263448158279061\n",
      "Epoch 1786, Loss: 0.029181170335505158, Final Batch Loss: 0.0019117342308163643\n",
      "Epoch 1787, Loss: 0.007015696697635576, Final Batch Loss: 0.004412002395838499\n",
      "Epoch 1788, Loss: 0.008512687170878053, Final Batch Loss: 0.0001664024603087455\n",
      "Epoch 1789, Loss: 0.0022194817574927583, Final Batch Loss: 0.00018918993009719998\n",
      "Epoch 1790, Loss: 0.02352599930964061, Final Batch Loss: 5.366673940443434e-05\n",
      "Epoch 1791, Loss: 0.03834730671951547, Final Batch Loss: 0.032139889895915985\n",
      "Epoch 1792, Loss: 0.0093850435805507, Final Batch Loss: 0.0010747775668278337\n",
      "Epoch 1793, Loss: 0.010211863205768168, Final Batch Loss: 0.000671610061544925\n",
      "Epoch 1794, Loss: 0.0034581459913169965, Final Batch Loss: 0.0012792120687663555\n",
      "Epoch 1795, Loss: 0.0029205750615801662, Final Batch Loss: 0.0003037545538973063\n",
      "Epoch 1796, Loss: 0.008415683929342777, Final Batch Loss: 0.0004976484342478216\n",
      "Epoch 1797, Loss: 0.0428748810372781, Final Batch Loss: 5.4172633099369705e-05\n",
      "Epoch 1798, Loss: 0.0053104161997907795, Final Batch Loss: 0.0006230340804904699\n",
      "Epoch 1799, Loss: 0.046271540923044086, Final Batch Loss: 0.04261988773941994\n",
      "Epoch 1800, Loss: 0.07226030749734491, Final Batch Loss: 0.000977065647020936\n",
      "Epoch 1801, Loss: 0.11266607884317636, Final Batch Loss: 0.05883190408349037\n",
      "Epoch 1802, Loss: 0.07997237821109593, Final Batch Loss: 0.0021039501298218966\n",
      "Epoch 1803, Loss: 0.06643481086939573, Final Batch Loss: 0.0017363547813147306\n",
      "Epoch 1804, Loss: 0.08859667740762234, Final Batch Loss: 0.024686364457011223\n",
      "Epoch 1805, Loss: 0.03506034635938704, Final Batch Loss: 0.002554394770413637\n",
      "Epoch 1806, Loss: 0.03458803758258, Final Batch Loss: 0.0005390260485000908\n",
      "Epoch 1807, Loss: 0.02874535450246185, Final Batch Loss: 0.0012174774892628193\n",
      "Epoch 1808, Loss: 0.008751341607421637, Final Batch Loss: 0.0006846749456599355\n",
      "Epoch 1809, Loss: 0.008315337894600816, Final Batch Loss: 0.00241904822178185\n",
      "Epoch 1810, Loss: 0.020579673757310957, Final Batch Loss: 0.0002880011161323637\n",
      "Epoch 1811, Loss: 0.004053115495480597, Final Batch Loss: 0.0010073042940348387\n",
      "Epoch 1812, Loss: 0.00344587882864289, Final Batch Loss: 0.0010919161140918732\n",
      "Epoch 1813, Loss: 0.005426305462606251, Final Batch Loss: 0.00047570717288181186\n",
      "Epoch 1814, Loss: 0.013198000349802896, Final Batch Loss: 0.0009582471684552729\n",
      "Epoch 1815, Loss: 0.0022364075703080744, Final Batch Loss: 0.00020627598860301077\n",
      "Epoch 1816, Loss: 0.006659106758888811, Final Batch Loss: 0.0004571534227579832\n",
      "Epoch 1817, Loss: 0.007324763049837202, Final Batch Loss: 0.0015514361439272761\n",
      "Epoch 1818, Loss: 0.03912448730261531, Final Batch Loss: 0.00012997050362173468\n",
      "Epoch 1819, Loss: 0.023100965132471174, Final Batch Loss: 0.000725736899767071\n",
      "Epoch 1820, Loss: 0.005664002819685265, Final Batch Loss: 0.0003479904553387314\n",
      "Epoch 1821, Loss: 0.03290496114641428, Final Batch Loss: 0.0020101796835660934\n",
      "Epoch 1822, Loss: 0.040062385553028435, Final Batch Loss: 0.032915472984313965\n",
      "Epoch 1823, Loss: 0.024134566803695634, Final Batch Loss: 0.00044938703649677336\n",
      "Epoch 1824, Loss: 0.006321255641523749, Final Batch Loss: 0.003708174917846918\n",
      "Epoch 1825, Loss: 0.005032795001170598, Final Batch Loss: 0.00012527759827207774\n",
      "Epoch 1826, Loss: 0.007754834266961552, Final Batch Loss: 0.00011397061462048441\n",
      "Epoch 1827, Loss: 0.025637388898758218, Final Batch Loss: 0.0004987860447727144\n",
      "Epoch 1828, Loss: 0.04473035060800612, Final Batch Loss: 0.04209360480308533\n",
      "Epoch 1829, Loss: 0.005965785414446145, Final Batch Loss: 0.002817834261804819\n",
      "Epoch 1830, Loss: 0.010620815082802437, Final Batch Loss: 0.0045392694883048534\n",
      "Epoch 1831, Loss: 0.010937444982118905, Final Batch Loss: 0.004003383219242096\n",
      "Epoch 1832, Loss: 0.008274794032331556, Final Batch Loss: 0.005652482621371746\n",
      "Epoch 1833, Loss: 0.002634022763231769, Final Batch Loss: 0.00027012682403437793\n",
      "Epoch 1834, Loss: 0.003168382652802393, Final Batch Loss: 0.0004615966754499823\n",
      "Epoch 1835, Loss: 0.008029255433939397, Final Batch Loss: 0.0016827232902869582\n",
      "Epoch 1836, Loss: 0.0068001512554474175, Final Batch Loss: 0.003874133573845029\n",
      "Epoch 1837, Loss: 0.015355129289673641, Final Batch Loss: 0.001955439103767276\n",
      "Epoch 1838, Loss: 0.004363665851997212, Final Batch Loss: 0.0010230302577838302\n",
      "Epoch 1839, Loss: 0.004378303230623715, Final Batch Loss: 0.00016223917191382498\n",
      "Epoch 1840, Loss: 0.004106004285858944, Final Batch Loss: 0.0002909602189902216\n",
      "Epoch 1841, Loss: 0.010171325062401593, Final Batch Loss: 0.0009011280490085483\n",
      "Epoch 1842, Loss: 0.004839317814912647, Final Batch Loss: 0.00046974653378129005\n",
      "Epoch 1843, Loss: 0.015574743854813278, Final Batch Loss: 0.00026140856789425015\n",
      "Epoch 1844, Loss: 0.01940437545272289, Final Batch Loss: 0.00010998822835972533\n",
      "Epoch 1845, Loss: 0.002763750933809206, Final Batch Loss: 0.0010862378403544426\n",
      "Epoch 1846, Loss: 0.0120467770320829, Final Batch Loss: 0.010581452399492264\n",
      "Epoch 1847, Loss: 0.004770509694935754, Final Batch Loss: 0.002914257114753127\n",
      "Epoch 1848, Loss: 0.0015536942810285836, Final Batch Loss: 0.00015862946747802198\n",
      "Epoch 1849, Loss: 0.006972218456212431, Final Batch Loss: 0.005495675839483738\n",
      "Epoch 1850, Loss: 0.00472998307668604, Final Batch Loss: 0.002852571429684758\n",
      "Epoch 1851, Loss: 0.0017899622034747154, Final Batch Loss: 0.0002498938993085176\n",
      "Epoch 1852, Loss: 0.002340924576856196, Final Batch Loss: 0.0010469347471371293\n",
      "Epoch 1853, Loss: 0.0012924740003654733, Final Batch Loss: 0.0003543737984728068\n",
      "Epoch 1854, Loss: 0.035770947630226146, Final Batch Loss: 4.4717649871017784e-05\n",
      "Epoch 1855, Loss: 0.004315112339099869, Final Batch Loss: 0.0007075410103425384\n",
      "Epoch 1856, Loss: 0.0024429784098174423, Final Batch Loss: 0.000508698693010956\n",
      "Epoch 1857, Loss: 0.008522324176738039, Final Batch Loss: 0.00039451263728551567\n",
      "Epoch 1858, Loss: 0.027799368006526493, Final Batch Loss: 0.0002441304677631706\n",
      "Epoch 1859, Loss: 0.01963285320380237, Final Batch Loss: 0.00013250803749542683\n",
      "Epoch 1860, Loss: 0.002363275911193341, Final Batch Loss: 0.0005638219299726188\n",
      "Epoch 1861, Loss: 0.004510591199505143, Final Batch Loss: 0.002090917667374015\n",
      "Epoch 1862, Loss: 0.0033716601610649377, Final Batch Loss: 0.0014773571165278554\n",
      "Epoch 1863, Loss: 0.0012242006923770532, Final Batch Loss: 0.0002088914334308356\n",
      "Epoch 1864, Loss: 0.004642431333195418, Final Batch Loss: 0.0013513751327991486\n",
      "Epoch 1865, Loss: 0.00499525511986576, Final Batch Loss: 0.00022647205332759768\n",
      "Epoch 1866, Loss: 0.004436945644556545, Final Batch Loss: 0.0015394685324281454\n",
      "Epoch 1867, Loss: 0.004699557728599757, Final Batch Loss: 0.0017762703355401754\n",
      "Epoch 1868, Loss: 0.008743260535993613, Final Batch Loss: 0.00014255910355132073\n",
      "Epoch 1869, Loss: 0.005116308864671737, Final Batch Loss: 0.0002525891177356243\n",
      "Epoch 1870, Loss: 0.003035449073649943, Final Batch Loss: 0.0003627669648267329\n",
      "Epoch 1871, Loss: 0.004043403474497609, Final Batch Loss: 0.0002284368238179013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1872, Loss: 0.021197830355959013, Final Batch Loss: 0.00028953063883818686\n",
      "Epoch 1873, Loss: 0.002806222881190479, Final Batch Loss: 0.0006027515046298504\n",
      "Epoch 1874, Loss: 0.0034918180608656257, Final Batch Loss: 0.0006291375611908734\n",
      "Epoch 1875, Loss: 0.002163705154089257, Final Batch Loss: 8.557378896512091e-05\n",
      "Epoch 1876, Loss: 0.0075301346951164305, Final Batch Loss: 0.005363886710256338\n",
      "Epoch 1877, Loss: 0.035543090576538816, Final Batch Loss: 5.6643824791535735e-05\n",
      "Epoch 1878, Loss: 0.006089858332416043, Final Batch Loss: 0.00040217142668552697\n",
      "Epoch 1879, Loss: 0.029988681941176765, Final Batch Loss: 5.319771298673004e-05\n",
      "Epoch 1880, Loss: 0.003867490158881992, Final Batch Loss: 0.0008065320434980094\n",
      "Epoch 1881, Loss: 0.00532637577271089, Final Batch Loss: 0.00023803793010301888\n",
      "Epoch 1882, Loss: 0.0044734568655258045, Final Batch Loss: 0.0001836497540352866\n",
      "Epoch 1883, Loss: 0.006476058013504371, Final Batch Loss: 0.0005023959092795849\n",
      "Epoch 1884, Loss: 0.008692146104294807, Final Batch Loss: 0.0009888919303193688\n",
      "Epoch 1885, Loss: 0.00823909044265747, Final Batch Loss: 0.0012099846499040723\n",
      "Epoch 1886, Loss: 0.01822652335249586, Final Batch Loss: 7.845256914151832e-05\n",
      "Epoch 1887, Loss: 0.008231932180933654, Final Batch Loss: 0.0030640566255897284\n",
      "Epoch 1888, Loss: 0.011941309858229943, Final Batch Loss: 0.00014160842692945153\n",
      "Epoch 1889, Loss: 0.013830598589265719, Final Batch Loss: 0.0001981919922400266\n",
      "Epoch 1890, Loss: 0.003484327608020976, Final Batch Loss: 0.0002075950033031404\n",
      "Epoch 1891, Loss: 0.04676751113584032, Final Batch Loss: 0.0006772459018975496\n",
      "Epoch 1892, Loss: 0.006073122131056152, Final Batch Loss: 0.00019535755563993007\n",
      "Epoch 1893, Loss: 0.009179700136883184, Final Batch Loss: 0.0055004144087433815\n",
      "Epoch 1894, Loss: 0.0011064707105106208, Final Batch Loss: 0.00018707140407059342\n",
      "Epoch 1895, Loss: 0.003272419358836487, Final Batch Loss: 0.00022426634677685797\n",
      "Epoch 1896, Loss: 0.0060437068896135315, Final Batch Loss: 0.003485331777483225\n",
      "Epoch 1897, Loss: 0.005692411621566862, Final Batch Loss: 0.0031344774179160595\n",
      "Epoch 1898, Loss: 0.0277002425500541, Final Batch Loss: 6.118750752648339e-05\n",
      "Epoch 1899, Loss: 0.01083147844474297, Final Batch Loss: 0.00967529509216547\n",
      "Epoch 1900, Loss: 0.00386843693559058, Final Batch Loss: 0.0013947142288088799\n",
      "Epoch 1901, Loss: 0.002111642374075018, Final Batch Loss: 0.0003240734222345054\n",
      "Epoch 1902, Loss: 0.006574243365321308, Final Batch Loss: 0.004167733248323202\n",
      "Epoch 1903, Loss: 0.001517894328571856, Final Batch Loss: 0.0003723700938280672\n",
      "Epoch 1904, Loss: 0.0011245710047660396, Final Batch Loss: 0.000117298070108518\n",
      "Epoch 1905, Loss: 0.01331286939966958, Final Batch Loss: 0.00015167232777457684\n",
      "Epoch 1906, Loss: 0.006055199890397489, Final Batch Loss: 0.0012574342545121908\n",
      "Epoch 1907, Loss: 0.0015604460058966652, Final Batch Loss: 0.00019239221001043916\n",
      "Epoch 1908, Loss: 0.002515284242690541, Final Batch Loss: 0.0008968740003183484\n",
      "Epoch 1909, Loss: 0.006242907838895917, Final Batch Loss: 0.004753496497869492\n",
      "Epoch 1910, Loss: 0.0010982536077790428, Final Batch Loss: 1.762241663527675e-05\n",
      "Epoch 1911, Loss: 0.0018499988364055753, Final Batch Loss: 0.0002167600905522704\n",
      "Epoch 1912, Loss: 0.02311907991679618, Final Batch Loss: 0.0015965003985911608\n",
      "Epoch 1913, Loss: 0.003900451905792579, Final Batch Loss: 0.00020399187633302063\n",
      "Epoch 1914, Loss: 0.0011946294835070148, Final Batch Loss: 0.00043496349826455116\n",
      "Epoch 1915, Loss: 0.003434601836488582, Final Batch Loss: 0.0004222900315653533\n",
      "Epoch 1916, Loss: 0.003007225939654745, Final Batch Loss: 0.0008536055684089661\n",
      "Epoch 1917, Loss: 0.029332778736716136, Final Batch Loss: 0.0072789485566318035\n",
      "Epoch 1918, Loss: 0.002983978352858685, Final Batch Loss: 0.0013022568309679627\n",
      "Epoch 1919, Loss: 0.002937681805633474, Final Batch Loss: 9.268243593396619e-05\n",
      "Epoch 1920, Loss: 0.0038648502668365836, Final Batch Loss: 5.0332717364653945e-05\n",
      "Epoch 1921, Loss: 0.002151234933990054, Final Batch Loss: 0.001212515402585268\n",
      "Epoch 1922, Loss: 0.019519368310284335, Final Batch Loss: 2.7235837478656322e-05\n",
      "Epoch 1923, Loss: 0.0014684312991448678, Final Batch Loss: 0.00011900050594704226\n",
      "Epoch 1924, Loss: 0.004110522801056504, Final Batch Loss: 0.0004367646761238575\n",
      "Epoch 1925, Loss: 0.004523212890489958, Final Batch Loss: 0.00024119876616168767\n",
      "Epoch 1926, Loss: 0.0014558387847500853, Final Batch Loss: 0.0002011564647546038\n",
      "Epoch 1927, Loss: 0.0016247196472249925, Final Batch Loss: 0.00020402675727382302\n",
      "Epoch 1928, Loss: 0.0040811649450915866, Final Batch Loss: 6.868775381008163e-05\n",
      "Epoch 1929, Loss: 0.002377600940235425, Final Batch Loss: 8.096203237073496e-05\n",
      "Epoch 1930, Loss: 0.0031553095759591088, Final Batch Loss: 0.0001833839196478948\n",
      "Epoch 1931, Loss: 0.0024191834963858128, Final Batch Loss: 0.0001809131063055247\n",
      "Epoch 1932, Loss: 0.030948731757234782, Final Batch Loss: 0.00021659801132045686\n",
      "Epoch 1933, Loss: 0.002629785143653862, Final Batch Loss: 0.00024348990700673312\n",
      "Epoch 1934, Loss: 0.0007946324330987409, Final Batch Loss: 0.00017121426935773343\n",
      "Epoch 1935, Loss: 0.0013467154931277037, Final Batch Loss: 0.00010000741167459637\n",
      "Epoch 1936, Loss: 0.007339981297263876, Final Batch Loss: 0.005967072211205959\n",
      "Epoch 1937, Loss: 0.019139923046168406, Final Batch Loss: 0.0002716057642828673\n",
      "Epoch 1938, Loss: 0.004539026223937981, Final Batch Loss: 0.0040114824660122395\n",
      "Epoch 1939, Loss: 0.04205105217988603, Final Batch Loss: 0.00031098219915293157\n",
      "Epoch 1940, Loss: 0.004700484438217245, Final Batch Loss: 0.0025517712347209454\n",
      "Epoch 1941, Loss: 0.041656442219391465, Final Batch Loss: 0.0011859878432005644\n",
      "Epoch 1942, Loss: 0.0012194512964924797, Final Batch Loss: 0.0002010918833548203\n",
      "Epoch 1943, Loss: 0.004595951410010457, Final Batch Loss: 0.0018624396761879325\n",
      "Epoch 1944, Loss: 0.003172991840983741, Final Batch Loss: 0.00015934398106765002\n",
      "Epoch 1945, Loss: 0.0025818994035944343, Final Batch Loss: 0.0002477106754668057\n",
      "Epoch 1946, Loss: 0.039763205277267843, Final Batch Loss: 0.0009666681871749461\n",
      "Epoch 1947, Loss: 0.0015061942176544107, Final Batch Loss: 7.560224184999242e-05\n",
      "Epoch 1948, Loss: 0.009637597890105098, Final Batch Loss: 0.005326687823981047\n",
      "Epoch 1949, Loss: 0.002645262618898414, Final Batch Loss: 0.0004329912771936506\n",
      "Epoch 1950, Loss: 0.002967520384117961, Final Batch Loss: 0.00021575659047812223\n",
      "Epoch 1951, Loss: 0.0070201824710238725, Final Batch Loss: 0.00019470270490273833\n",
      "Epoch 1952, Loss: 0.0027901441790163517, Final Batch Loss: 7.747136987745762e-05\n",
      "Epoch 1953, Loss: 0.009126361735980026, Final Batch Loss: 0.000232559468713589\n",
      "Epoch 1954, Loss: 0.0006784618162782863, Final Batch Loss: 0.00018864676530938596\n",
      "Epoch 1955, Loss: 0.006465389345976291, Final Batch Loss: 0.0002973913215100765\n",
      "Epoch 1956, Loss: 0.0005616699054371566, Final Batch Loss: 0.00011140113201690838\n",
      "Epoch 1957, Loss: 0.0017725704092299566, Final Batch Loss: 0.0011714290594682097\n",
      "Epoch 1958, Loss: 0.0017599032435100526, Final Batch Loss: 0.0001262076839338988\n",
      "Epoch 1959, Loss: 0.01723303340259008, Final Batch Loss: 0.00019917212193831801\n",
      "Epoch 1960, Loss: 0.012884790892712772, Final Batch Loss: 0.0009188587428070605\n",
      "Epoch 1961, Loss: 0.0016869914543349296, Final Batch Loss: 0.0005926768644712865\n",
      "Epoch 1962, Loss: 0.014360659959493205, Final Batch Loss: 8.745308150537312e-05\n",
      "Epoch 1963, Loss: 0.017326459259493276, Final Batch Loss: 0.00025788176571950316\n",
      "Epoch 1964, Loss: 0.004497669855481945, Final Batch Loss: 0.0011262818006798625\n",
      "Epoch 1965, Loss: 0.018361496626312146, Final Batch Loss: 2.984615639434196e-05\n",
      "Epoch 1966, Loss: 0.004716510884463787, Final Batch Loss: 0.0007531476439908147\n",
      "Epoch 1967, Loss: 0.0009506962378509343, Final Batch Loss: 0.0001894247834570706\n",
      "Epoch 1968, Loss: 0.022002406294632237, Final Batch Loss: 0.00013591747847385705\n",
      "Epoch 1969, Loss: 0.0016242153651546687, Final Batch Loss: 0.00016788922948762774\n",
      "Epoch 1970, Loss: 0.019036264828173444, Final Batch Loss: 0.0003170588461216539\n",
      "Epoch 1971, Loss: 0.0029624867747770622, Final Batch Loss: 5.59848704142496e-05\n",
      "Epoch 1972, Loss: 0.026594942006340716, Final Batch Loss: 0.0001812724076444283\n",
      "Epoch 1973, Loss: 0.03211569911218248, Final Batch Loss: 0.00032454560277983546\n",
      "Epoch 1974, Loss: 0.0013553139797295444, Final Batch Loss: 0.00010830701648956165\n",
      "Epoch 1975, Loss: 0.02363043410878163, Final Batch Loss: 0.00136465800460428\n",
      "Epoch 1976, Loss: 0.006419840865419246, Final Batch Loss: 5.8049001381732523e-05\n",
      "Epoch 1977, Loss: 0.004774241562699899, Final Batch Loss: 6.783279241062701e-05\n",
      "Epoch 1978, Loss: 0.006033046083757654, Final Batch Loss: 0.0019806823693215847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1979, Loss: 0.016306952922604978, Final Batch Loss: 0.0004807638470083475\n",
      "Epoch 1980, Loss: 0.0225835022283718, Final Batch Loss: 0.000610632705502212\n",
      "Epoch 1981, Loss: 0.0014573304360965267, Final Batch Loss: 0.0004428664979059249\n",
      "Epoch 1982, Loss: 0.043204720801441, Final Batch Loss: 0.008211808279156685\n",
      "Epoch 1983, Loss: 0.017275686281209346, Final Batch Loss: 0.0009351689368486404\n",
      "Epoch 1984, Loss: 0.011954966466873884, Final Batch Loss: 0.0014376420294865966\n",
      "Epoch 1985, Loss: 0.01911659944380517, Final Batch Loss: 0.0005385439144447446\n",
      "Epoch 1986, Loss: 0.016496990276209544, Final Batch Loss: 0.008230467326939106\n",
      "Epoch 1987, Loss: 0.010941215616185218, Final Batch Loss: 0.0009387154714204371\n",
      "Epoch 1988, Loss: 0.003957800043281168, Final Batch Loss: 0.0003764433495234698\n",
      "Epoch 1989, Loss: 0.006957309160497971, Final Batch Loss: 0.0002073420473607257\n",
      "Epoch 1990, Loss: 0.00443590541544836, Final Batch Loss: 0.00038175610825419426\n",
      "Epoch 1991, Loss: 0.011315636482322589, Final Batch Loss: 0.0003765975998248905\n",
      "Epoch 1992, Loss: 0.016820606746478006, Final Batch Loss: 0.01383267156779766\n",
      "Epoch 1993, Loss: 0.0031473072813241743, Final Batch Loss: 0.0003692682948894799\n",
      "Epoch 1994, Loss: 0.03677057311870158, Final Batch Loss: 0.0001932929444592446\n",
      "Epoch 1995, Loss: 0.0007751438606646843, Final Batch Loss: 0.00010217463568551466\n",
      "Epoch 1996, Loss: 0.015422418509842828, Final Batch Loss: 0.009832500480115414\n",
      "Epoch 1997, Loss: 0.008947202935814857, Final Batch Loss: 0.0003426634066272527\n",
      "Epoch 1998, Loss: 0.015124731115065515, Final Batch Loss: 0.0002889438474085182\n",
      "Epoch 1999, Loss: 0.013344322826014832, Final Batch Loss: 0.00018070204532705247\n",
      "Epoch 2000, Loss: 0.038094589981483296, Final Batch Loss: 0.00775269977748394\n",
      "Epoch 2001, Loss: 0.0371631961024832, Final Batch Loss: 0.0006321800756268203\n",
      "Epoch 2002, Loss: 0.0026210564537905157, Final Batch Loss: 0.0004872665158472955\n",
      "Epoch 2003, Loss: 0.002392134949332103, Final Batch Loss: 0.0003106713411398232\n",
      "Epoch 2004, Loss: 0.011706370365573093, Final Batch Loss: 0.00021702185040339828\n",
      "Epoch 2005, Loss: 0.0029275250853970647, Final Batch Loss: 0.0010905640665441751\n",
      "Epoch 2006, Loss: 0.009190047145239078, Final Batch Loss: 0.00011661807366181165\n",
      "Epoch 2007, Loss: 0.049629251880105585, Final Batch Loss: 0.0002594428660813719\n",
      "Epoch 2008, Loss: 0.03217444085748866, Final Batch Loss: 0.000154276960529387\n",
      "Epoch 2009, Loss: 0.0027423386636655778, Final Batch Loss: 0.001139446278102696\n",
      "Epoch 2010, Loss: 0.02007711368787568, Final Batch Loss: 0.00040416428237222135\n",
      "Epoch 2011, Loss: 0.00283233993104659, Final Batch Loss: 0.0007173290941864252\n",
      "Epoch 2012, Loss: 0.02072924218373373, Final Batch Loss: 0.0006736214854754508\n",
      "Epoch 2013, Loss: 0.0025517537142150104, Final Batch Loss: 0.0001745070912875235\n",
      "Epoch 2014, Loss: 0.006913654120580759, Final Batch Loss: 0.0012551401741802692\n",
      "Epoch 2015, Loss: 0.004972808237653226, Final Batch Loss: 0.0030733917374163866\n",
      "Epoch 2016, Loss: 0.006652107404079288, Final Batch Loss: 0.0009278655634261668\n",
      "Epoch 2017, Loss: 0.002591679891338572, Final Batch Loss: 0.00035649744677357376\n",
      "Epoch 2018, Loss: 0.023116915952414274, Final Batch Loss: 0.0006305962451733649\n",
      "Epoch 2019, Loss: 0.0032293776312144473, Final Batch Loss: 0.00017526272858958691\n",
      "Epoch 2020, Loss: 0.010538940434344113, Final Batch Loss: 0.0003063271869905293\n",
      "Epoch 2021, Loss: 0.007228841714095324, Final Batch Loss: 0.00038368406239897013\n",
      "Epoch 2022, Loss: 0.013833986158715561, Final Batch Loss: 0.0006218542694114149\n",
      "Epoch 2023, Loss: 0.005061252668383531, Final Batch Loss: 0.0025057976599782705\n",
      "Epoch 2024, Loss: 0.0032860054925549775, Final Batch Loss: 0.0002866495051421225\n",
      "Epoch 2025, Loss: 0.010774776557809673, Final Batch Loss: 0.00016102270456030965\n",
      "Epoch 2026, Loss: 0.021700586308725178, Final Batch Loss: 0.0017843869281932712\n",
      "Epoch 2027, Loss: 0.0023986313317436725, Final Batch Loss: 0.00020313751883804798\n",
      "Epoch 2028, Loss: 0.007151413039537147, Final Batch Loss: 0.0003405088500585407\n",
      "Epoch 2029, Loss: 0.0016451291739940643, Final Batch Loss: 0.00040107741369865835\n",
      "Epoch 2030, Loss: 0.0028504349756985903, Final Batch Loss: 0.0009295947384089231\n",
      "Epoch 2031, Loss: 0.017524101567687467, Final Batch Loss: 0.010374688543379307\n",
      "Epoch 2032, Loss: 0.0011860423619509675, Final Batch Loss: 0.00038502970710396767\n",
      "Epoch 2033, Loss: 0.022384898678865284, Final Batch Loss: 0.0002557509869802743\n",
      "Epoch 2034, Loss: 0.0009464388167543802, Final Batch Loss: 1.8534443370299414e-05\n",
      "Epoch 2035, Loss: 0.002280375876580365, Final Batch Loss: 0.0015966377686709166\n",
      "Epoch 2036, Loss: 0.004140532386372797, Final Batch Loss: 0.0033492918591946363\n",
      "Epoch 2037, Loss: 0.0031540700656478293, Final Batch Loss: 0.00018307482241652906\n",
      "Epoch 2038, Loss: 0.04180149908643216, Final Batch Loss: 0.03405225649476051\n",
      "Epoch 2039, Loss: 0.0040680825331946835, Final Batch Loss: 0.0011223044712096453\n",
      "Epoch 2040, Loss: 0.01068486919393763, Final Batch Loss: 0.001196154858916998\n",
      "Epoch 2041, Loss: 0.007820501283276826, Final Batch Loss: 0.0013209076132625341\n",
      "Epoch 2042, Loss: 0.0009372179920319468, Final Batch Loss: 0.00032163469586521387\n",
      "Epoch 2043, Loss: 0.0019560613145586103, Final Batch Loss: 0.0007413623970933259\n",
      "Epoch 2044, Loss: 0.01083309730165638, Final Batch Loss: 0.0003011274093296379\n",
      "Epoch 2045, Loss: 0.005140639608725905, Final Batch Loss: 0.0004966877750121057\n",
      "Epoch 2046, Loss: 0.0026289330853614956, Final Batch Loss: 0.000981079530902207\n",
      "Epoch 2047, Loss: 0.0060711072001140565, Final Batch Loss: 8.313314174301922e-05\n",
      "Epoch 2048, Loss: 0.01591590221505612, Final Batch Loss: 0.0022888192906975746\n",
      "Epoch 2049, Loss: 0.008363925880985335, Final Batch Loss: 0.0011211598757654428\n",
      "Epoch 2050, Loss: 0.0019403446713113226, Final Batch Loss: 9.372420754516497e-05\n",
      "Epoch 2051, Loss: 0.00213790315319784, Final Batch Loss: 0.0008757138275541365\n",
      "Epoch 2052, Loss: 0.002346797933569178, Final Batch Loss: 0.0011108779581263661\n",
      "Epoch 2053, Loss: 0.004929911810904741, Final Batch Loss: 0.00018101709429174662\n",
      "Epoch 2054, Loss: 0.007140643952880055, Final Batch Loss: 0.004257757682353258\n",
      "Epoch 2055, Loss: 0.0014430452865781263, Final Batch Loss: 0.00013507019320968539\n",
      "Epoch 2056, Loss: 0.0038777468143962324, Final Batch Loss: 0.0002210855600424111\n",
      "Epoch 2057, Loss: 0.0009320782555732876, Final Batch Loss: 0.00023959242389537394\n",
      "Epoch 2058, Loss: 0.00656187288404908, Final Batch Loss: 0.00026280764723196626\n",
      "Epoch 2059, Loss: 0.02663321320869727, Final Batch Loss: 0.026169784367084503\n",
      "Epoch 2060, Loss: 0.0015383417921839282, Final Batch Loss: 0.0002125999890267849\n",
      "Epoch 2061, Loss: 0.01764647991512902, Final Batch Loss: 0.0037612933665513992\n",
      "Epoch 2062, Loss: 0.0065667043381836265, Final Batch Loss: 0.0002787488920148462\n",
      "Epoch 2063, Loss: 0.005443756119348109, Final Batch Loss: 0.0010837097652256489\n",
      "Epoch 2064, Loss: 0.004759070157888345, Final Batch Loss: 0.00013913655129726976\n",
      "Epoch 2065, Loss: 0.0008086962770903483, Final Batch Loss: 0.0001033093431033194\n",
      "Epoch 2066, Loss: 0.006475741465692408, Final Batch Loss: 0.00015474343672394753\n",
      "Epoch 2067, Loss: 0.002039784172666259, Final Batch Loss: 0.00018749518494587392\n",
      "Epoch 2068, Loss: 0.005585861894360278, Final Batch Loss: 0.004686874803155661\n",
      "Epoch 2069, Loss: 0.0021935440454399213, Final Batch Loss: 0.0010701809078454971\n",
      "Epoch 2070, Loss: 0.0007842000923119485, Final Batch Loss: 9.215684258379042e-06\n",
      "Epoch 2071, Loss: 0.0032651787041686475, Final Batch Loss: 0.002059941878542304\n",
      "Epoch 2072, Loss: 0.0012154188661952503, Final Batch Loss: 0.0004796851717401296\n",
      "Epoch 2073, Loss: 0.010587997596303467, Final Batch Loss: 6.34870448266156e-05\n",
      "Epoch 2074, Loss: 0.0019279579792055301, Final Batch Loss: 3.600958007154986e-05\n",
      "Epoch 2075, Loss: 0.0017582940363354282, Final Batch Loss: 2.047293855866883e-05\n",
      "Epoch 2076, Loss: 0.0011170656944159418, Final Batch Loss: 0.0006838752306066453\n",
      "Epoch 2077, Loss: 0.002600937837996753, Final Batch Loss: 4.872905628872104e-05\n",
      "Epoch 2078, Loss: 0.0016600492836005287, Final Batch Loss: 1.1481040928629227e-05\n",
      "Epoch 2079, Loss: 0.0007403057898045518, Final Batch Loss: 5.725745722884312e-05\n",
      "Epoch 2080, Loss: 0.002454545086948201, Final Batch Loss: 0.00025334928068332374\n",
      "Epoch 2081, Loss: 0.0016871295374585316, Final Batch Loss: 0.000479915615869686\n",
      "Epoch 2082, Loss: 0.00042470020707696676, Final Batch Loss: 2.6706111384555697e-05\n",
      "Epoch 2083, Loss: 0.0015110430540516973, Final Batch Loss: 6.386729364749044e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2084, Loss: 0.0014325202364489087, Final Batch Loss: 8.751420864427928e-06\n",
      "Epoch 2085, Loss: 0.0029941291068098508, Final Batch Loss: 3.901370655512437e-05\n",
      "Epoch 2086, Loss: 0.001133966819907073, Final Batch Loss: 8.328105468535796e-05\n",
      "Epoch 2087, Loss: 0.0008288379685836844, Final Batch Loss: 9.573654824635014e-05\n",
      "Epoch 2088, Loss: 0.0014654772603535093, Final Batch Loss: 0.0011355156311765313\n",
      "Epoch 2089, Loss: 0.0034182417148258537, Final Batch Loss: 0.00255708210170269\n",
      "Epoch 2090, Loss: 0.001356563028821256, Final Batch Loss: 0.0006285634590312839\n",
      "Epoch 2091, Loss: 0.0011865996439155424, Final Batch Loss: 2.1721609300584532e-05\n",
      "Epoch 2092, Loss: 0.001728075061691925, Final Batch Loss: 9.65884028119035e-05\n",
      "Epoch 2093, Loss: 0.005862151501787594, Final Batch Loss: 3.640760769485496e-05\n",
      "Epoch 2094, Loss: 0.001988220908515359, Final Batch Loss: 4.729684860649286e-06\n",
      "Epoch 2095, Loss: 0.007597651179821696, Final Batch Loss: 0.007049625273793936\n",
      "Epoch 2096, Loss: 0.001932925251821871, Final Batch Loss: 7.095410546753556e-05\n",
      "Epoch 2097, Loss: 0.014267877355450764, Final Batch Loss: 0.0006341176922433078\n",
      "Epoch 2098, Loss: 0.020484188193222508, Final Batch Loss: 0.0003405023308005184\n",
      "Epoch 2099, Loss: 0.0024310990102094365, Final Batch Loss: 2.706439954636153e-05\n",
      "Epoch 2100, Loss: 0.003107533004367724, Final Batch Loss: 0.00017861451487988234\n",
      "Epoch 2101, Loss: 0.004819488880457357, Final Batch Loss: 0.0030506823677569628\n",
      "Epoch 2102, Loss: 0.006522048970509786, Final Batch Loss: 0.0005662838811986148\n",
      "Epoch 2103, Loss: 0.0009046593550010584, Final Batch Loss: 0.00024378274974878877\n",
      "Epoch 2104, Loss: 0.02570643761282554, Final Batch Loss: 0.00026953170890919864\n",
      "Epoch 2105, Loss: 0.0009046971390489489, Final Batch Loss: 0.0003463370958343148\n",
      "Epoch 2106, Loss: 0.007484289948479272, Final Batch Loss: 0.0001771029201336205\n",
      "Epoch 2107, Loss: 0.0025874889179249294, Final Batch Loss: 6.74268594593741e-05\n",
      "Epoch 2108, Loss: 0.0010838794114533812, Final Batch Loss: 0.00022885415819473565\n",
      "Epoch 2109, Loss: 0.06271861864661332, Final Batch Loss: 0.00014906794240232557\n",
      "Epoch 2110, Loss: 0.0005426934367278591, Final Batch Loss: 0.00016800891899038106\n",
      "Epoch 2111, Loss: 0.002510895224986598, Final Batch Loss: 0.00048259616596624255\n",
      "Epoch 2112, Loss: 0.001919085465488024, Final Batch Loss: 0.0008399109356105328\n",
      "Epoch 2113, Loss: 0.019408531094086356, Final Batch Loss: 0.0002820261870510876\n",
      "Epoch 2114, Loss: 0.001275411297683604, Final Batch Loss: 0.0002402491372777149\n",
      "Epoch 2115, Loss: 0.004432751058629947, Final Batch Loss: 0.002891638083383441\n",
      "Epoch 2116, Loss: 0.004186654528893996, Final Batch Loss: 0.00011271067342022434\n",
      "Epoch 2117, Loss: 0.004629184099030681, Final Batch Loss: 0.0002326605172129348\n",
      "Epoch 2118, Loss: 0.008312444573675748, Final Batch Loss: 0.00011180410365341231\n",
      "Epoch 2119, Loss: 0.0006942096006241627, Final Batch Loss: 0.00018637542962096632\n",
      "Epoch 2120, Loss: 0.004142358011449687, Final Batch Loss: 0.0003525142092257738\n",
      "Epoch 2121, Loss: 0.0020878002396784723, Final Batch Loss: 9.402968862559646e-05\n",
      "Epoch 2122, Loss: 0.0030149459635140374, Final Batch Loss: 0.0010540506336838007\n",
      "Epoch 2123, Loss: 0.02411074845076655, Final Batch Loss: 0.0006071898387745023\n",
      "Epoch 2124, Loss: 0.002141037693945691, Final Batch Loss: 0.0002962068538181484\n",
      "Epoch 2125, Loss: 0.001057494017004501, Final Batch Loss: 0.00012152398267062381\n",
      "Epoch 2126, Loss: 0.00066052854526788, Final Batch Loss: 0.00014403303794097155\n",
      "Epoch 2127, Loss: 0.0028844939661212265, Final Batch Loss: 0.0006157390889711678\n",
      "Epoch 2128, Loss: 0.002409245731541887, Final Batch Loss: 0.001501129474490881\n",
      "Epoch 2129, Loss: 0.001647194687393494, Final Batch Loss: 0.00014120928244665265\n",
      "Epoch 2130, Loss: 0.0011990483981207944, Final Batch Loss: 0.00011426264973124489\n",
      "Epoch 2131, Loss: 0.0010351032397011295, Final Batch Loss: 8.330126001965255e-05\n",
      "Epoch 2132, Loss: 0.0007555858755949885, Final Batch Loss: 0.00017860175285022706\n",
      "Epoch 2133, Loss: 0.004000140739663038, Final Batch Loss: 0.00011583923333091661\n",
      "Epoch 2134, Loss: 0.0009180794004350901, Final Batch Loss: 0.00021200421906542033\n",
      "Epoch 2135, Loss: 0.00886463165807072, Final Batch Loss: 0.001984434900805354\n",
      "Epoch 2136, Loss: 0.0010396272846264765, Final Batch Loss: 0.0005083868163637817\n",
      "Epoch 2137, Loss: 0.0006752620392944664, Final Batch Loss: 6.413771188817918e-05\n",
      "Epoch 2138, Loss: 0.002282881312567042, Final Batch Loss: 0.0019556626211851835\n",
      "Epoch 2139, Loss: 0.003066985507757636, Final Batch Loss: 4.2208052036585286e-05\n",
      "Epoch 2140, Loss: 0.00046304796342155896, Final Batch Loss: 3.574813672457822e-05\n",
      "Epoch 2141, Loss: 0.00954157436353853, Final Batch Loss: 0.0005551657523028553\n",
      "Epoch 2142, Loss: 0.01890544355046586, Final Batch Loss: 5.039672032580711e-05\n",
      "Epoch 2143, Loss: 0.0024154947241186164, Final Batch Loss: 8.45494432724081e-05\n",
      "Epoch 2144, Loss: 0.0018157255326514132, Final Batch Loss: 0.0007145805866457522\n",
      "Epoch 2145, Loss: 0.0010391294417786412, Final Batch Loss: 0.00041528677684254944\n",
      "Epoch 2146, Loss: 0.026153386876103468, Final Batch Loss: 5.387241253629327e-05\n",
      "Epoch 2147, Loss: 0.0008111099159577861, Final Batch Loss: 0.0002626679779496044\n",
      "Epoch 2148, Loss: 0.001038120113662444, Final Batch Loss: 7.128569995984435e-05\n",
      "Epoch 2149, Loss: 0.006804125863709487, Final Batch Loss: 0.00038760609459131956\n",
      "Epoch 2150, Loss: 0.01569414856930962, Final Batch Loss: 0.00011148166231578216\n",
      "Epoch 2151, Loss: 0.002136486193194287, Final Batch Loss: 4.921344589092769e-05\n",
      "Epoch 2152, Loss: 0.0032463541538163554, Final Batch Loss: 1.9445458747213706e-05\n",
      "Epoch 2153, Loss: 0.003284068217908498, Final Batch Loss: 7.260777783812955e-05\n",
      "Epoch 2154, Loss: 0.004849970307077456, Final Batch Loss: 1.1960301890212577e-05\n",
      "Epoch 2155, Loss: 0.0024845151347108185, Final Batch Loss: 0.00024261866929009557\n",
      "Epoch 2156, Loss: 0.0005067262209195178, Final Batch Loss: 9.99027761281468e-05\n",
      "Epoch 2157, Loss: 0.04753078230714891, Final Batch Loss: 0.018286200240254402\n",
      "Epoch 2158, Loss: 0.0010355943522881716, Final Batch Loss: 7.675182132516056e-05\n",
      "Epoch 2159, Loss: 0.0018257138290209696, Final Batch Loss: 0.0005378652131184936\n",
      "Epoch 2160, Loss: 0.0020019996882183477, Final Batch Loss: 0.0010130451992154121\n",
      "Epoch 2161, Loss: 0.001061494134773966, Final Batch Loss: 7.313128298847005e-05\n",
      "Epoch 2162, Loss: 0.005948484431428369, Final Batch Loss: 7.420477777486667e-05\n",
      "Epoch 2163, Loss: 0.0007028344625723548, Final Batch Loss: 7.771029777359217e-05\n",
      "Epoch 2164, Loss: 0.0019716686656465754, Final Batch Loss: 0.00021698350610677153\n",
      "Epoch 2165, Loss: 0.0012276078487047926, Final Batch Loss: 0.0004302822344470769\n",
      "Epoch 2166, Loss: 0.0004684279556386173, Final Batch Loss: 0.0002823714748956263\n",
      "Epoch 2167, Loss: 0.04294939553074073, Final Batch Loss: 8.942955901147798e-05\n",
      "Epoch 2168, Loss: 0.001611272728041513, Final Batch Loss: 4.737589551950805e-05\n",
      "Epoch 2169, Loss: 0.0036621469480451196, Final Batch Loss: 0.001984615344554186\n",
      "Epoch 2170, Loss: 0.001385357609251514, Final Batch Loss: 0.000550133059732616\n",
      "Epoch 2171, Loss: 0.0034651023197511677, Final Batch Loss: 3.838719931081869e-05\n",
      "Epoch 2172, Loss: 0.0010325197745260084, Final Batch Loss: 2.7497664632392116e-05\n",
      "Epoch 2173, Loss: 0.0010169809102080762, Final Batch Loss: 3.527051012497395e-05\n",
      "Epoch 2174, Loss: 0.04823625121207442, Final Batch Loss: 0.0001440926716895774\n",
      "Epoch 2175, Loss: 0.0032716806745156646, Final Batch Loss: 0.00025889629614539444\n",
      "Epoch 2176, Loss: 0.007940201525343582, Final Batch Loss: 0.0004642711137421429\n",
      "Epoch 2177, Loss: 0.016815565817523748, Final Batch Loss: 0.00023847445845603943\n",
      "Epoch 2178, Loss: 0.0013284081796882674, Final Batch Loss: 4.839304892811924e-05\n",
      "Epoch 2179, Loss: 0.0028283624560572207, Final Batch Loss: 0.00035390688572078943\n",
      "Epoch 2180, Loss: 0.007248159556183964, Final Batch Loss: 0.004385396838188171\n",
      "Epoch 2181, Loss: 0.002460989257087931, Final Batch Loss: 0.00048239881289191544\n",
      "Epoch 2182, Loss: 0.005998826345603447, Final Batch Loss: 8.575359970564023e-05\n",
      "Epoch 2183, Loss: 0.006864084114567959, Final Batch Loss: 1.620103466848377e-05\n",
      "Epoch 2184, Loss: 0.002226104117653449, Final Batch Loss: 2.125362698279787e-05\n",
      "Epoch 2185, Loss: 0.004479890216316562, Final Batch Loss: 9.245452383765951e-05\n",
      "Epoch 2186, Loss: 0.0009946495774784125, Final Batch Loss: 6.079241575207561e-05\n",
      "Epoch 2187, Loss: 0.0008450308450846933, Final Batch Loss: 0.00027598912129178643\n",
      "Epoch 2188, Loss: 0.0016589396545896307, Final Batch Loss: 0.00014496869698632509\n",
      "Epoch 2189, Loss: 0.0033108063507825136, Final Batch Loss: 0.0004306983028072864\n",
      "Epoch 2190, Loss: 0.01725063860794762, Final Batch Loss: 0.013995731249451637\n",
      "Epoch 2191, Loss: 0.0008979082267615013, Final Batch Loss: 0.00011292655108263716\n",
      "Epoch 2192, Loss: 0.00166183998589986, Final Batch Loss: 3.114798528258689e-05\n",
      "Epoch 2193, Loss: 0.0009947607250069268, Final Batch Loss: 8.863914990797639e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2194, Loss: 0.05775506429199595, Final Batch Loss: 0.004009588155895472\n",
      "Epoch 2195, Loss: 0.032722761447075754, Final Batch Loss: 0.00017257273430004716\n",
      "Epoch 2196, Loss: 0.016351651822333224, Final Batch Loss: 6.126263178884983e-05\n",
      "Epoch 2197, Loss: 0.00563289737328887, Final Batch Loss: 0.0009272086899727583\n",
      "Epoch 2198, Loss: 0.010943719767965376, Final Batch Loss: 0.0017053670017048717\n",
      "Epoch 2199, Loss: 0.009374088840559125, Final Batch Loss: 0.0013822366017848253\n",
      "Epoch 2200, Loss: 0.0038393939030356705, Final Batch Loss: 0.00027913032681681216\n",
      "Epoch 2201, Loss: 0.014483869541436434, Final Batch Loss: 0.002515591448172927\n",
      "Epoch 2202, Loss: 0.01235543200164102, Final Batch Loss: 0.011008140631020069\n",
      "Epoch 2203, Loss: 0.0030292574883787893, Final Batch Loss: 0.00011926699517061934\n",
      "Epoch 2204, Loss: 0.002194864151533693, Final Batch Loss: 0.000556331011466682\n",
      "Epoch 2205, Loss: 0.004716478055343032, Final Batch Loss: 0.002401011763140559\n",
      "Epoch 2206, Loss: 0.0028473341517383233, Final Batch Loss: 0.0022130650468170643\n",
      "Epoch 2207, Loss: 0.0012582829222083092, Final Batch Loss: 0.00035926749114878476\n",
      "Epoch 2208, Loss: 0.0030888304027030244, Final Batch Loss: 0.0002080642298096791\n",
      "Epoch 2209, Loss: 0.010497272574866656, Final Batch Loss: 8.06485113571398e-05\n",
      "Epoch 2210, Loss: 0.007459124793967931, Final Batch Loss: 0.0003181684296578169\n",
      "Epoch 2211, Loss: 0.016917481887503527, Final Batch Loss: 0.0072409966960549355\n",
      "Epoch 2212, Loss: 0.009631184471800225, Final Batch Loss: 7.011181151028723e-05\n",
      "Epoch 2213, Loss: 0.009743176880874671, Final Batch Loss: 0.001580013893544674\n",
      "Epoch 2214, Loss: 0.011620626151852775, Final Batch Loss: 2.9685274057555944e-05\n",
      "Epoch 2215, Loss: 0.005008329273550771, Final Batch Loss: 6.504071643576026e-05\n",
      "Epoch 2216, Loss: 0.0008434414176008431, Final Batch Loss: 2.2376012566382997e-05\n",
      "Epoch 2217, Loss: 0.004268377699190751, Final Batch Loss: 0.002304021269083023\n",
      "Epoch 2218, Loss: 0.03328608488664031, Final Batch Loss: 0.0002511593047529459\n",
      "Epoch 2219, Loss: 0.004210867737128865, Final Batch Loss: 0.00072735600406304\n",
      "Epoch 2220, Loss: 0.004424018472491298, Final Batch Loss: 5.636321293422952e-05\n",
      "Epoch 2221, Loss: 0.0010450843692524359, Final Batch Loss: 0.00027096987469121814\n",
      "Epoch 2222, Loss: 0.0014190271467668936, Final Batch Loss: 0.00038313487311825156\n",
      "Epoch 2223, Loss: 0.003811320260865614, Final Batch Loss: 0.0001911643339553848\n",
      "Epoch 2224, Loss: 0.009825984488998074, Final Batch Loss: 0.0001146856156992726\n",
      "Epoch 2225, Loss: 0.00314858288038522, Final Batch Loss: 0.00033904705196619034\n",
      "Epoch 2226, Loss: 0.013927419000538066, Final Batch Loss: 0.0003853492089547217\n",
      "Epoch 2227, Loss: 0.03715232641843613, Final Batch Loss: 7.557235949207097e-05\n",
      "Epoch 2228, Loss: 0.005407044794992544, Final Batch Loss: 0.0001261586876353249\n",
      "Epoch 2229, Loss: 0.00041867220716085285, Final Batch Loss: 9.726057760417461e-05\n",
      "Epoch 2230, Loss: 0.07307099976605969, Final Batch Loss: 0.07222015410661697\n",
      "Epoch 2231, Loss: 0.0024551064852857962, Final Batch Loss: 0.001239609089680016\n",
      "Epoch 2232, Loss: 0.001073873103450751, Final Batch Loss: 4.603800698532723e-05\n",
      "Epoch 2233, Loss: 0.0016746753826737404, Final Batch Loss: 0.00032106763683259487\n",
      "Epoch 2234, Loss: 0.0019452214910415933, Final Batch Loss: 0.0012071061646565795\n",
      "Epoch 2235, Loss: 0.0030701284122187644, Final Batch Loss: 0.00020548750762827694\n",
      "Epoch 2236, Loss: 0.004199514136416838, Final Batch Loss: 0.000562700442969799\n",
      "Epoch 2237, Loss: 0.0010304259922122583, Final Batch Loss: 0.00038712567766197026\n",
      "Epoch 2238, Loss: 0.007435790641466156, Final Batch Loss: 0.0005427628057077527\n",
      "Epoch 2239, Loss: 0.0020271521498216316, Final Batch Loss: 0.0008488411549478769\n",
      "Epoch 2240, Loss: 0.0074548087432049215, Final Batch Loss: 0.0007808868540450931\n",
      "Epoch 2241, Loss: 0.0069840273063164204, Final Batch Loss: 0.0003990390105172992\n",
      "Epoch 2242, Loss: 0.004572820747853257, Final Batch Loss: 0.0005137316766194999\n",
      "Epoch 2243, Loss: 0.002717952709645033, Final Batch Loss: 0.00025731141795404255\n",
      "Epoch 2244, Loss: 0.056836809279047884, Final Batch Loss: 0.05454394221305847\n",
      "Epoch 2245, Loss: 0.03982443761196919, Final Batch Loss: 0.0006924741319380701\n",
      "Epoch 2246, Loss: 0.01718744565732777, Final Batch Loss: 0.0005113134975545108\n",
      "Epoch 2247, Loss: 0.0014105210866546258, Final Batch Loss: 0.0003214767493773252\n",
      "Epoch 2248, Loss: 0.0037254302005749196, Final Batch Loss: 0.0019125171238556504\n",
      "Epoch 2249, Loss: 0.0035190290509490296, Final Batch Loss: 0.00022653744963463396\n",
      "Epoch 2250, Loss: 0.0060550805646926165, Final Batch Loss: 0.0005452796467579901\n",
      "Epoch 2251, Loss: 0.008715420117368922, Final Batch Loss: 0.0011248734081164002\n",
      "Epoch 2252, Loss: 0.03769513953011483, Final Batch Loss: 0.00013255227531772107\n",
      "Epoch 2253, Loss: 0.010374274745117873, Final Batch Loss: 0.0003528072847984731\n",
      "Epoch 2254, Loss: 0.020466629706788808, Final Batch Loss: 0.0012765489518642426\n",
      "Epoch 2255, Loss: 0.00580873410217464, Final Batch Loss: 0.00029427220579236746\n",
      "Epoch 2256, Loss: 0.0050776920397765934, Final Batch Loss: 0.0027102462481707335\n",
      "Epoch 2257, Loss: 0.004481658746954054, Final Batch Loss: 0.001828424516133964\n",
      "Epoch 2258, Loss: 0.002047215330094332, Final Batch Loss: 3.4892760595539585e-05\n",
      "Epoch 2259, Loss: 0.0018428982511977665, Final Batch Loss: 0.0009261532104574144\n",
      "Epoch 2260, Loss: 0.0050278031412744895, Final Batch Loss: 0.0001271018263651058\n",
      "Epoch 2261, Loss: 0.0023077418154571205, Final Batch Loss: 0.0003893010434694588\n",
      "Epoch 2262, Loss: 0.001887442369479686, Final Batch Loss: 0.00046089047100394964\n",
      "Epoch 2263, Loss: 0.012169165434897877, Final Batch Loss: 0.00022719906701240689\n",
      "Epoch 2264, Loss: 0.004175053974904586, Final Batch Loss: 0.000528431439306587\n",
      "Epoch 2265, Loss: 0.002480157883837819, Final Batch Loss: 0.0006835731910541654\n",
      "Epoch 2266, Loss: 0.016470530710648745, Final Batch Loss: 0.00017664139159023762\n",
      "Epoch 2267, Loss: 0.001230769368703477, Final Batch Loss: 6.113440031185746e-05\n",
      "Epoch 2268, Loss: 0.003480598228634335, Final Batch Loss: 0.00022580644872505218\n",
      "Epoch 2269, Loss: 0.0020037003196193837, Final Batch Loss: 0.00016801120364107192\n",
      "Epoch 2270, Loss: 0.005305434198817238, Final Batch Loss: 0.0003749383904505521\n",
      "Epoch 2271, Loss: 0.0028002951003145427, Final Batch Loss: 0.0002180845767725259\n",
      "Epoch 2272, Loss: 0.0017240052402485162, Final Batch Loss: 3.166345413774252e-05\n",
      "Epoch 2273, Loss: 0.0011325752275297418, Final Batch Loss: 0.0003656770568341017\n",
      "Epoch 2274, Loss: 0.004191000014543533, Final Batch Loss: 0.002478309441357851\n",
      "Epoch 2275, Loss: 0.0010029831028077751, Final Batch Loss: 0.0001059792484738864\n",
      "Epoch 2276, Loss: 0.008505069490638562, Final Batch Loss: 5.287969543132931e-05\n",
      "Epoch 2277, Loss: 0.0074803066163440235, Final Batch Loss: 0.006924869958311319\n",
      "Epoch 2278, Loss: 0.0025887478259392083, Final Batch Loss: 0.0012112342519685626\n",
      "Epoch 2279, Loss: 0.0025289821714977734, Final Batch Loss: 0.0008634169353172183\n",
      "Epoch 2280, Loss: 0.0015255908438120969, Final Batch Loss: 0.00010534259490668774\n",
      "Epoch 2281, Loss: 0.018810687794029946, Final Batch Loss: 0.003902261145412922\n",
      "Epoch 2282, Loss: 0.004842907630518312, Final Batch Loss: 5.505716035258956e-05\n",
      "Epoch 2283, Loss: 0.0077788358175894246, Final Batch Loss: 0.006571597419679165\n",
      "Epoch 2284, Loss: 0.001161389041953953, Final Batch Loss: 0.00037539948243647814\n",
      "Epoch 2285, Loss: 0.0014990096024121158, Final Batch Loss: 0.0005657170549966395\n",
      "Epoch 2286, Loss: 0.0007356982241617516, Final Batch Loss: 0.00010633112106006593\n",
      "Epoch 2287, Loss: 0.0011500746004458051, Final Batch Loss: 1.633105057408102e-05\n",
      "Epoch 2288, Loss: 0.0018911228398792446, Final Batch Loss: 0.0015263211680576205\n",
      "Epoch 2289, Loss: 0.0015031147268018685, Final Batch Loss: 0.0005200920859351754\n",
      "Epoch 2290, Loss: 0.03615691245067865, Final Batch Loss: 0.02183893322944641\n",
      "Epoch 2291, Loss: 0.0014457595971180126, Final Batch Loss: 0.00019627671281341463\n",
      "Epoch 2292, Loss: 0.003015627444256097, Final Batch Loss: 0.0007891744025982916\n",
      "Epoch 2293, Loss: 0.002860775675799232, Final Batch Loss: 0.0010326685151085258\n",
      "Epoch 2294, Loss: 0.027358345163520426, Final Batch Loss: 0.0011063744314014912\n",
      "Epoch 2295, Loss: 0.009359914794913493, Final Batch Loss: 0.00014814849419053644\n",
      "Epoch 2296, Loss: 0.0026319994540244807, Final Batch Loss: 5.427816722658463e-05\n",
      "Epoch 2297, Loss: 0.003416762439883314, Final Batch Loss: 4.548005381366238e-05\n",
      "Epoch 2298, Loss: 0.0010914193771895953, Final Batch Loss: 6.90084052621387e-05\n",
      "Epoch 2299, Loss: 0.0007455377308360767, Final Batch Loss: 3.393817678443156e-05\n",
      "Epoch 2300, Loss: 0.0019069555964961182, Final Batch Loss: 0.0001626333105377853\n",
      "Epoch 2301, Loss: 0.0014242510515032336, Final Batch Loss: 0.00037930510006845\n",
      "Epoch 2302, Loss: 0.0007600569078931585, Final Batch Loss: 0.00014307297533378005\n",
      "Epoch 2303, Loss: 0.001062277122400701, Final Batch Loss: 0.00012850614439230412\n",
      "Epoch 2304, Loss: 0.0027836933222715743, Final Batch Loss: 0.0009946140926331282\n",
      "Epoch 2305, Loss: 0.0006506479394374765, Final Batch Loss: 4.992135473003145e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2306, Loss: 0.010712315212003887, Final Batch Loss: 0.009022644720971584\n",
      "Epoch 2307, Loss: 0.00038887849950697273, Final Batch Loss: 0.0001628436439204961\n",
      "Epoch 2308, Loss: 0.0036258247564546764, Final Batch Loss: 0.00033265960519202054\n",
      "Epoch 2309, Loss: 0.011173840845003724, Final Batch Loss: 0.009484486654400826\n",
      "Epoch 2310, Loss: 0.001268697167688515, Final Batch Loss: 0.0004986505955457687\n",
      "Epoch 2311, Loss: 0.0016714908233552705, Final Batch Loss: 0.0014185447944328189\n",
      "Epoch 2312, Loss: 0.023930209659738466, Final Batch Loss: 0.001197962905280292\n",
      "Epoch 2313, Loss: 0.0003515322459861636, Final Batch Loss: 6.531443068524823e-05\n",
      "Epoch 2314, Loss: 0.0008277672022813931, Final Batch Loss: 4.009850090369582e-05\n",
      "Epoch 2315, Loss: 0.0006536085238622036, Final Batch Loss: 0.00018797356460709125\n",
      "Epoch 2316, Loss: 0.0005603372119367123, Final Batch Loss: 8.690423419466242e-05\n",
      "Epoch 2317, Loss: 0.0012126470282964874, Final Batch Loss: 0.0005904539139010012\n",
      "Epoch 2318, Loss: 0.0016470830596517771, Final Batch Loss: 9.63154379860498e-05\n",
      "Epoch 2319, Loss: 0.0028108356564189307, Final Batch Loss: 0.0009736247593536973\n",
      "Epoch 2320, Loss: 0.0010840843096957542, Final Batch Loss: 4.265040479367599e-05\n",
      "Epoch 2321, Loss: 0.0019150712905684486, Final Batch Loss: 0.000517110398504883\n",
      "Epoch 2322, Loss: 0.001030838040605886, Final Batch Loss: 3.355248554726131e-05\n",
      "Epoch 2323, Loss: 0.023513239400926977, Final Batch Loss: 0.0009491717210039496\n",
      "Epoch 2324, Loss: 0.0015130953306652373, Final Batch Loss: 1.4698240192956291e-05\n",
      "Epoch 2325, Loss: 0.000549567695998121, Final Batch Loss: 7.087063568178564e-05\n",
      "Epoch 2326, Loss: 0.001037490710587008, Final Batch Loss: 0.0003954375279136002\n",
      "Epoch 2327, Loss: 0.0012007361874566413, Final Batch Loss: 0.0003765750443562865\n",
      "Epoch 2328, Loss: 0.011656900634989142, Final Batch Loss: 7.731711957603693e-05\n",
      "Epoch 2329, Loss: 0.0045973985543241724, Final Batch Loss: 0.0004030745185445994\n",
      "Epoch 2330, Loss: 0.0026555961230769753, Final Batch Loss: 0.0004682985891122371\n",
      "Epoch 2331, Loss: 0.001399818473146297, Final Batch Loss: 6.140916957519948e-05\n",
      "Epoch 2332, Loss: 0.0008549976882932242, Final Batch Loss: 3.802917854045518e-05\n",
      "Epoch 2333, Loss: 0.0034994728484889492, Final Batch Loss: 0.0003654629399534315\n",
      "Epoch 2334, Loss: 0.0066765823867172, Final Batch Loss: 5.892990157008171e-05\n",
      "Epoch 2335, Loss: 0.002238992448837962, Final Batch Loss: 0.0001136435748776421\n",
      "Epoch 2336, Loss: 0.001765467706718482, Final Batch Loss: 0.00039433117490261793\n",
      "Epoch 2337, Loss: 0.0027004415351257194, Final Batch Loss: 8.667816291563213e-05\n",
      "Epoch 2338, Loss: 0.00045811970994691364, Final Batch Loss: 6.018621570547111e-05\n",
      "Epoch 2339, Loss: 0.0002363738494750578, Final Batch Loss: 4.6778182877460495e-05\n",
      "Epoch 2340, Loss: 0.0005639948431053199, Final Batch Loss: 0.00010183162521570921\n",
      "Epoch 2341, Loss: 0.011392859436455183, Final Batch Loss: 0.0106821209192276\n",
      "Epoch 2342, Loss: 0.0027799552990472876, Final Batch Loss: 5.2420196880120784e-05\n",
      "Epoch 2343, Loss: 0.0011601876321947202, Final Batch Loss: 0.00013824713823851198\n",
      "Epoch 2344, Loss: 0.0005044711670052493, Final Batch Loss: 7.4710460467031226e-06\n",
      "Epoch 2345, Loss: 0.000714742905984167, Final Batch Loss: 7.902480138000101e-05\n",
      "Epoch 2346, Loss: 0.0006471556553151459, Final Batch Loss: 5.68109389860183e-05\n",
      "Epoch 2347, Loss: 0.0004951164082740434, Final Batch Loss: 0.00027985050110146403\n",
      "Epoch 2348, Loss: 0.026006914420577232, Final Batch Loss: 4.492166044656187e-05\n",
      "Epoch 2349, Loss: 0.0031293213469325565, Final Batch Loss: 0.00016185504500754178\n",
      "Epoch 2350, Loss: 0.005543213919736445, Final Batch Loss: 0.0003368611796759069\n",
      "Epoch 2351, Loss: 0.001085841104213614, Final Batch Loss: 0.00014961970737203956\n",
      "Epoch 2352, Loss: 0.00048497422176296823, Final Batch Loss: 2.411122841294855e-05\n",
      "Epoch 2353, Loss: 0.0009924975820467807, Final Batch Loss: 9.736582433106378e-05\n",
      "Epoch 2354, Loss: 0.0005776992529717973, Final Batch Loss: 4.545303818304092e-05\n",
      "Epoch 2355, Loss: 0.00044508736027637497, Final Batch Loss: 6.180191849125549e-05\n",
      "Epoch 2356, Loss: 0.0004967945715179667, Final Batch Loss: 0.00010275767999701202\n",
      "Epoch 2357, Loss: 0.0012634263157451642, Final Batch Loss: 1.40166348501225e-05\n",
      "Epoch 2358, Loss: 0.0009833922122197691, Final Batch Loss: 5.3042931540403515e-05\n",
      "Epoch 2359, Loss: 0.001356641252641566, Final Batch Loss: 0.00010455554001964629\n",
      "Epoch 2360, Loss: 0.005113357393383922, Final Batch Loss: 8.744216756895185e-05\n",
      "Epoch 2361, Loss: 0.0007253480762301479, Final Batch Loss: 6.931031384738162e-05\n",
      "Epoch 2362, Loss: 0.0037497712000913452, Final Batch Loss: 4.266896939952858e-05\n",
      "Epoch 2363, Loss: 0.0008164335595211014, Final Batch Loss: 0.00018832151545211673\n",
      "Epoch 2364, Loss: 0.0015022903389763087, Final Batch Loss: 0.00033730693394318223\n",
      "Epoch 2365, Loss: 0.0024572869515395723, Final Batch Loss: 0.002061072736978531\n",
      "Epoch 2366, Loss: 0.0011803138550021686, Final Batch Loss: 0.0008327850955538452\n",
      "Epoch 2367, Loss: 0.023073068081430392, Final Batch Loss: 0.021508624777197838\n",
      "Epoch 2368, Loss: 0.0012348323416517815, Final Batch Loss: 0.0006282796966843307\n",
      "Epoch 2369, Loss: 0.0008878839907993097, Final Batch Loss: 4.438934047357179e-05\n",
      "Epoch 2370, Loss: 0.016600108894635923, Final Batch Loss: 0.0005515663069672883\n",
      "Epoch 2371, Loss: 0.0012318787339609116, Final Batch Loss: 1.9591199816204607e-05\n",
      "Epoch 2372, Loss: 0.002922773546742974, Final Batch Loss: 3.4157743357354775e-05\n",
      "Epoch 2373, Loss: 0.014976000351452967, Final Batch Loss: 0.008362540043890476\n",
      "Epoch 2374, Loss: 0.0004170257361693075, Final Batch Loss: 0.0002019503735937178\n",
      "Epoch 2375, Loss: 0.0006668933347100392, Final Batch Loss: 0.0005073871579952538\n",
      "Epoch 2376, Loss: 0.0027181162804481573, Final Batch Loss: 0.0002376536576775834\n",
      "Epoch 2377, Loss: 0.0012420082902053764, Final Batch Loss: 1.3065845223536598e-06\n",
      "Epoch 2378, Loss: 0.002860958567907801, Final Batch Loss: 5.3487838158616796e-05\n",
      "Epoch 2379, Loss: 0.0009561520128045231, Final Batch Loss: 0.0004938669153489172\n",
      "Epoch 2380, Loss: 0.000903838915291999, Final Batch Loss: 7.167644980654586e-06\n",
      "Epoch 2381, Loss: 0.00034401459197397344, Final Batch Loss: 2.8676129659288563e-05\n",
      "Epoch 2382, Loss: 0.0017449286242481321, Final Batch Loss: 0.0014325986849144101\n",
      "Epoch 2383, Loss: 0.005352253720047884, Final Batch Loss: 3.9983075112104416e-05\n",
      "Epoch 2384, Loss: 0.003338709720992483, Final Batch Loss: 0.0001244388840859756\n",
      "Epoch 2385, Loss: 0.021748043174738996, Final Batch Loss: 6.622049841098487e-05\n",
      "Epoch 2386, Loss: 0.0007291529673238983, Final Batch Loss: 1.4340917914523743e-05\n",
      "Epoch 2387, Loss: 0.004048001661431044, Final Batch Loss: 0.00040445628110319376\n",
      "Epoch 2388, Loss: 0.0021179143514018506, Final Batch Loss: 9.337178198620677e-05\n",
      "Epoch 2389, Loss: 0.0024585679320807685, Final Batch Loss: 1.0558266694715712e-05\n",
      "Epoch 2390, Loss: 0.0004421370103955269, Final Batch Loss: 0.00011321740021230653\n",
      "Epoch 2391, Loss: 0.0014146048633847386, Final Batch Loss: 0.00021213843137957156\n",
      "Epoch 2392, Loss: 0.003325619666611601, Final Batch Loss: 9.425905773241539e-06\n",
      "Epoch 2393, Loss: 0.0007662938733119518, Final Batch Loss: 0.0002587441704235971\n",
      "Epoch 2394, Loss: 0.001468570037104655, Final Batch Loss: 7.23938865121454e-06\n",
      "Epoch 2395, Loss: 0.002997404353664024, Final Batch Loss: 0.0005492384661920369\n",
      "Epoch 2396, Loss: 0.001815908559365198, Final Batch Loss: 9.349021274829283e-05\n",
      "Epoch 2397, Loss: 0.026837737677851692, Final Batch Loss: 7.196699152700603e-05\n",
      "Epoch 2398, Loss: 0.0005671657963830512, Final Batch Loss: 3.577940879040398e-05\n",
      "Epoch 2399, Loss: 0.0013432228624878917, Final Batch Loss: 0.0008900598622858524\n",
      "Epoch 2400, Loss: 0.0006130775582278147, Final Batch Loss: 2.28266253543552e-05\n",
      "Epoch 2401, Loss: 0.004343524426076328, Final Batch Loss: 0.004123519640415907\n",
      "Epoch 2402, Loss: 0.0007112328094081022, Final Batch Loss: 5.991268699290231e-05\n",
      "Epoch 2403, Loss: 0.0004799849557457492, Final Batch Loss: 1.412839992553927e-05\n",
      "Epoch 2404, Loss: 0.0012553516589832725, Final Batch Loss: 9.15324744710233e-06\n",
      "Epoch 2405, Loss: 0.0014005398406879976, Final Batch Loss: 0.0003910128434654325\n",
      "Epoch 2406, Loss: 0.00212621565151494, Final Batch Loss: 0.000517899461556226\n",
      "Epoch 2407, Loss: 0.0007849293142498937, Final Batch Loss: 0.00012079066800652072\n",
      "Epoch 2408, Loss: 0.04046553642547224, Final Batch Loss: 0.03808603435754776\n",
      "Epoch 2409, Loss: 0.0004872067020187387, Final Batch Loss: 0.00011519430699991062\n",
      "Epoch 2410, Loss: 0.0033577335489098914, Final Batch Loss: 0.00010526827099965885\n",
      "Epoch 2411, Loss: 0.010048118987469934, Final Batch Loss: 0.00021006447786930948\n",
      "Epoch 2412, Loss: 0.012663308101764414, Final Batch Loss: 2.053962816717103e-05\n",
      "Epoch 2413, Loss: 0.001600993171450682, Final Batch Loss: 0.00025211667525582016\n",
      "Epoch 2414, Loss: 0.0013052159338258207, Final Batch Loss: 3.208518319297582e-05\n",
      "Epoch 2415, Loss: 0.001774296470102854, Final Batch Loss: 0.00015659375640098006\n",
      "Epoch 2416, Loss: 0.0034626934175321367, Final Batch Loss: 7.858272874727845e-05\n",
      "Epoch 2417, Loss: 0.026745765762825613, Final Batch Loss: 2.843752190528903e-05\n",
      "Epoch 2418, Loss: 0.0016488783512613736, Final Batch Loss: 0.00037618190981447697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2419, Loss: 0.0009714078114484437, Final Batch Loss: 0.00030900631099939346\n",
      "Epoch 2420, Loss: 0.024369904946070164, Final Batch Loss: 0.02365768514573574\n",
      "Epoch 2421, Loss: 0.005766410940850619, Final Batch Loss: 0.004799823276698589\n",
      "Epoch 2422, Loss: 0.003537866403348744, Final Batch Loss: 0.00044283567694947124\n",
      "Epoch 2423, Loss: 0.008070218493230641, Final Batch Loss: 7.990325684659183e-05\n",
      "Epoch 2424, Loss: 0.0018360794201726094, Final Batch Loss: 0.00013227602175902575\n",
      "Epoch 2425, Loss: 0.001240705867530778, Final Batch Loss: 0.0002364116080570966\n",
      "Epoch 2426, Loss: 0.002954939700430259, Final Batch Loss: 0.0001374771527480334\n",
      "Epoch 2427, Loss: 0.0008090720475593116, Final Batch Loss: 0.0002801590017043054\n",
      "Epoch 2428, Loss: 0.013642577629070729, Final Batch Loss: 0.00031052660779096186\n",
      "Epoch 2429, Loss: 0.010993915304425173, Final Batch Loss: 0.010351271368563175\n",
      "Epoch 2430, Loss: 0.009058200634171953, Final Batch Loss: 2.324051092728041e-05\n",
      "Epoch 2431, Loss: 0.0008103546351776458, Final Batch Loss: 0.0004927602712996304\n",
      "Epoch 2432, Loss: 0.000843538980006997, Final Batch Loss: 1.3586705790658016e-05\n",
      "Epoch 2433, Loss: 0.0011520430271048099, Final Batch Loss: 0.0003019364085048437\n",
      "Epoch 2434, Loss: 0.0016277179529424757, Final Batch Loss: 0.00020316427981015295\n",
      "Epoch 2435, Loss: 0.0016502619691891596, Final Batch Loss: 0.00029921095119789243\n",
      "Epoch 2436, Loss: 0.0016237603267654777, Final Batch Loss: 0.0008602978778071702\n",
      "Epoch 2437, Loss: 0.029912531201262027, Final Batch Loss: 7.234868826344609e-05\n",
      "Epoch 2438, Loss: 0.001297016759053804, Final Batch Loss: 0.00029841982177458704\n",
      "Epoch 2439, Loss: 0.0016981301159830764, Final Batch Loss: 0.0011156206019222736\n",
      "Epoch 2440, Loss: 0.0017242903195437975, Final Batch Loss: 8.246467768913135e-05\n",
      "Epoch 2441, Loss: 0.003791308045038022, Final Batch Loss: 0.00015961345343384892\n",
      "Epoch 2442, Loss: 0.0003989807919424493, Final Batch Loss: 6.985993240959942e-05\n",
      "Epoch 2443, Loss: 0.002307790200575255, Final Batch Loss: 0.0007747266790829599\n",
      "Epoch 2444, Loss: 0.0007305556500796229, Final Batch Loss: 0.00033555820118635893\n",
      "Epoch 2445, Loss: 0.002921065330156125, Final Batch Loss: 0.0023668850772082806\n",
      "Epoch 2446, Loss: 0.000890437382622622, Final Batch Loss: 0.0001812897971831262\n",
      "Epoch 2447, Loss: 0.0066261394385946915, Final Batch Loss: 0.0005309019470587373\n",
      "Epoch 2448, Loss: 0.009727793109050253, Final Batch Loss: 2.3822711227694526e-05\n",
      "Epoch 2449, Loss: 0.025324281348730437, Final Batch Loss: 0.02366342768073082\n",
      "Epoch 2450, Loss: 0.0016346112950031966, Final Batch Loss: 2.2029687443136936e-06\n",
      "Epoch 2451, Loss: 0.007076002882968169, Final Batch Loss: 5.4990072385407984e-05\n",
      "Epoch 2452, Loss: 0.030267781814472983, Final Batch Loss: 0.029670264571905136\n",
      "Epoch 2453, Loss: 0.000555254657228943, Final Batch Loss: 9.837630932452157e-05\n",
      "Epoch 2454, Loss: 0.004520287322520744, Final Batch Loss: 9.135168511420488e-05\n",
      "Epoch 2455, Loss: 0.006923163135070354, Final Batch Loss: 0.00011042263213312253\n",
      "Epoch 2456, Loss: 0.003016830283741001, Final Batch Loss: 9.780776599654928e-05\n",
      "Epoch 2457, Loss: 0.013684931167517789, Final Batch Loss: 0.00019565159163903445\n",
      "Epoch 2458, Loss: 0.0019846729301207233, Final Batch Loss: 1.9675138901220635e-05\n",
      "Epoch 2459, Loss: 0.0010943043162114918, Final Batch Loss: 0.00014485121937468648\n",
      "Epoch 2460, Loss: 0.018664270057342947, Final Batch Loss: 0.00013785416376776993\n",
      "Epoch 2461, Loss: 0.003124022186966613, Final Batch Loss: 0.00024230271810665727\n",
      "Epoch 2462, Loss: 0.0024222124775405973, Final Batch Loss: 0.000158819486387074\n",
      "Epoch 2463, Loss: 0.003061949413677212, Final Batch Loss: 3.846352774417028e-05\n",
      "Epoch 2464, Loss: 0.004055379049532348, Final Batch Loss: 4.147454819758423e-05\n",
      "Epoch 2465, Loss: 0.0035073776452918537, Final Batch Loss: 8.430096931988373e-05\n",
      "Epoch 2466, Loss: 0.0008746231251279823, Final Batch Loss: 0.00010462258796906099\n",
      "Epoch 2467, Loss: 0.0018040041031781584, Final Batch Loss: 0.00038133759517222643\n",
      "Epoch 2468, Loss: 0.0009586555970599875, Final Batch Loss: 2.1391038899309933e-05\n",
      "Epoch 2469, Loss: 0.0017417103554180358, Final Batch Loss: 0.00044236009125597775\n",
      "Epoch 2470, Loss: 0.0019353534044057596, Final Batch Loss: 0.0002930976916104555\n",
      "Epoch 2471, Loss: 0.01217560726126976, Final Batch Loss: 1.1562805411813315e-05\n",
      "Epoch 2472, Loss: 0.000936081314648618, Final Batch Loss: 2.5474635549471714e-05\n",
      "Epoch 2473, Loss: 0.0026603129954310134, Final Batch Loss: 0.00018755895143840462\n",
      "Epoch 2474, Loss: 0.003965136129409075, Final Batch Loss: 0.0029539461247622967\n",
      "Epoch 2475, Loss: 0.0007522453561250586, Final Batch Loss: 0.00011030251334886998\n",
      "Epoch 2476, Loss: 0.0018204717780463398, Final Batch Loss: 0.00012436951510608196\n",
      "Epoch 2477, Loss: 0.002303375793417217, Final Batch Loss: 0.00045873181079514325\n",
      "Epoch 2478, Loss: 0.0008501848788000643, Final Batch Loss: 0.0002531929640099406\n",
      "Epoch 2479, Loss: 0.0032787941709102597, Final Batch Loss: 0.0029063744004815817\n",
      "Epoch 2480, Loss: 0.0009722506365505978, Final Batch Loss: 0.00015847269969526678\n",
      "Epoch 2481, Loss: 0.019040990970097482, Final Batch Loss: 0.009825092740356922\n",
      "Epoch 2482, Loss: 0.008462930636596866, Final Batch Loss: 0.00024301049415953457\n",
      "Epoch 2483, Loss: 0.02307520651811501, Final Batch Loss: 7.021330384304747e-05\n",
      "Epoch 2484, Loss: 0.0005347309852368198, Final Batch Loss: 4.537653148872778e-05\n",
      "Epoch 2485, Loss: 0.011448468270828016, Final Batch Loss: 0.00016714261437300593\n",
      "Epoch 2486, Loss: 0.0022938761539990082, Final Batch Loss: 0.00021169561659917235\n",
      "Epoch 2487, Loss: 0.0025460775941610336, Final Batch Loss: 0.0012279264628887177\n",
      "Epoch 2488, Loss: 0.015978772738890257, Final Batch Loss: 5.4599826398771256e-05\n",
      "Epoch 2489, Loss: 0.0013236125960247591, Final Batch Loss: 0.00014686185750178993\n",
      "Epoch 2490, Loss: 0.002351712384552229, Final Batch Loss: 8.34292295621708e-05\n",
      "Epoch 2491, Loss: 0.008646729787869845, Final Batch Loss: 0.007346461061388254\n",
      "Epoch 2492, Loss: 0.0026533763430052204, Final Batch Loss: 2.933624455181416e-05\n",
      "Epoch 2493, Loss: 0.02101008183672093, Final Batch Loss: 0.0002754689776338637\n",
      "Epoch 2494, Loss: 0.01782734162406996, Final Batch Loss: 0.00027769262669607997\n",
      "Epoch 2495, Loss: 0.0015771280959597789, Final Batch Loss: 0.0007263689185492694\n",
      "Epoch 2496, Loss: 0.0012650705175474286, Final Batch Loss: 0.00033631108817644417\n",
      "Epoch 2497, Loss: 0.00197820903849788, Final Batch Loss: 0.00024129150551743805\n",
      "Epoch 2498, Loss: 0.017923301289556548, Final Batch Loss: 0.0006314741913229227\n",
      "Epoch 2499, Loss: 0.0022746651484339964, Final Batch Loss: 0.0007563672261312604\n",
      "Epoch 2500, Loss: 0.012971760406799149, Final Batch Loss: 0.0005190673400647938\n",
      "Epoch 2501, Loss: 0.009939450945239514, Final Batch Loss: 0.00014474867202807218\n",
      "Epoch 2502, Loss: 0.005036176211433485, Final Batch Loss: 6.180565105751157e-05\n",
      "Epoch 2503, Loss: 0.002725961443502456, Final Batch Loss: 0.0021083964966237545\n",
      "Epoch 2504, Loss: 0.012389004397846293, Final Batch Loss: 0.00010697847028495744\n",
      "Epoch 2505, Loss: 0.01855970402539242, Final Batch Loss: 0.00023932437761686742\n",
      "Epoch 2506, Loss: 0.0020687443466158584, Final Batch Loss: 0.00011201518645975739\n",
      "Epoch 2507, Loss: 0.004681168618844822, Final Batch Loss: 0.001485405140556395\n",
      "Epoch 2508, Loss: 0.006871406163554639, Final Batch Loss: 0.002442005556076765\n",
      "Epoch 2509, Loss: 0.004627140719094314, Final Batch Loss: 0.0002545804891269654\n",
      "Epoch 2510, Loss: 0.007580273995699827, Final Batch Loss: 0.00044315337436273694\n",
      "Epoch 2511, Loss: 0.0010355122867622413, Final Batch Loss: 8.741697092773393e-05\n",
      "Epoch 2512, Loss: 0.024314138583577005, Final Batch Loss: 0.0046873376704752445\n",
      "Epoch 2513, Loss: 0.000808023163699545, Final Batch Loss: 0.00023512016923632473\n",
      "Epoch 2514, Loss: 0.003039655470274738, Final Batch Loss: 2.662848237378057e-05\n",
      "Epoch 2515, Loss: 0.0012810978660127148, Final Batch Loss: 7.336735143326223e-05\n",
      "Epoch 2516, Loss: 0.0018898207563324831, Final Batch Loss: 0.0003647575795184821\n",
      "Epoch 2517, Loss: 0.0040177409537136555, Final Batch Loss: 0.00021452902001328766\n",
      "Epoch 2518, Loss: 0.0020922918192809448, Final Batch Loss: 0.00022103122319094837\n",
      "Epoch 2519, Loss: 0.0019685763654706534, Final Batch Loss: 6.504192424472421e-05\n",
      "Epoch 2520, Loss: 0.0012861112918471918, Final Batch Loss: 0.00013533960736822337\n",
      "Epoch 2521, Loss: 0.00046831812869641, Final Batch Loss: 0.00020499005040619522\n",
      "Epoch 2522, Loss: 0.02645424693764653, Final Batch Loss: 0.00013386936916504055\n",
      "Epoch 2523, Loss: 0.0030518713101628236, Final Batch Loss: 0.00021997465228196234\n",
      "Epoch 2524, Loss: 0.0013372989851632155, Final Batch Loss: 0.0009713874896988273\n",
      "Epoch 2525, Loss: 0.0026585737359710038, Final Batch Loss: 0.0002595073892734945\n",
      "Epoch 2526, Loss: 0.00713615774657228, Final Batch Loss: 7.812771946191788e-05\n",
      "Epoch 2527, Loss: 0.01712007721653208, Final Batch Loss: 0.0003454540856182575\n",
      "Epoch 2528, Loss: 0.0009540682658553123, Final Batch Loss: 0.00023502555268350989\n",
      "Epoch 2529, Loss: 0.0007702598086325452, Final Batch Loss: 0.00024740202934481204\n",
      "Epoch 2530, Loss: 0.0015330001042457297, Final Batch Loss: 0.00025298629770986736\n",
      "Epoch 2531, Loss: 0.0031933652935549617, Final Batch Loss: 0.00014880990784149617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2532, Loss: 0.016574462468270212, Final Batch Loss: 0.0004910214338451624\n",
      "Epoch 2533, Loss: 0.0018804266874212772, Final Batch Loss: 3.656736225821078e-05\n",
      "Epoch 2534, Loss: 0.0009578205972502474, Final Batch Loss: 3.7745190638815984e-05\n",
      "Epoch 2535, Loss: 0.0012741247337544337, Final Batch Loss: 9.135284199146554e-05\n",
      "Epoch 2536, Loss: 0.001463121941924328, Final Batch Loss: 0.00011055233335355297\n",
      "Epoch 2537, Loss: 0.011257032922003418, Final Batch Loss: 0.00019815696578007191\n",
      "Epoch 2538, Loss: 0.0011482334411994088, Final Batch Loss: 3.736718281288631e-05\n",
      "Epoch 2539, Loss: 0.007460198212356772, Final Batch Loss: 0.0022473225835710764\n",
      "Epoch 2540, Loss: 0.0008083666980382986, Final Batch Loss: 0.00015262619126588106\n",
      "Epoch 2541, Loss: 0.0017055821663234383, Final Batch Loss: 7.571578316856176e-05\n",
      "Epoch 2542, Loss: 0.001304608389546047, Final Batch Loss: 1.692125624686014e-05\n",
      "Epoch 2543, Loss: 0.000611587149251136, Final Batch Loss: 7.571982132503763e-05\n",
      "Epoch 2544, Loss: 0.009246426096069627, Final Batch Loss: 0.00012699257058557123\n",
      "Epoch 2545, Loss: 0.0027454629744170234, Final Batch Loss: 0.0019794839899986982\n",
      "Epoch 2546, Loss: 0.0009450995130464435, Final Batch Loss: 0.00038449603016488254\n",
      "Epoch 2547, Loss: 0.001869731369879446, Final Batch Loss: 1.9927980247302912e-05\n",
      "Epoch 2548, Loss: 0.002331533018150367, Final Batch Loss: 0.0002859995875041932\n",
      "Epoch 2549, Loss: 0.0008310529328809935, Final Batch Loss: 1.023985805659322e-05\n",
      "Epoch 2550, Loss: 0.002347556570384768, Final Batch Loss: 0.00025757792172953486\n",
      "Epoch 2551, Loss: 0.0009777138620847836, Final Batch Loss: 0.00027608161326497793\n",
      "Epoch 2552, Loss: 0.0034112901412299834, Final Batch Loss: 4.604959758580662e-05\n",
      "Epoch 2553, Loss: 0.0009658878261689097, Final Batch Loss: 0.0006574233993887901\n",
      "Epoch 2554, Loss: 0.0008368544131371891, Final Batch Loss: 0.00019210267055314034\n",
      "Epoch 2555, Loss: 0.0013603102415800095, Final Batch Loss: 4.176524089416489e-05\n",
      "Epoch 2556, Loss: 0.0004333255437813932, Final Batch Loss: 1.754655568220187e-05\n",
      "Epoch 2557, Loss: 0.0029487092433555517, Final Batch Loss: 4.796943176188506e-05\n",
      "Epoch 2558, Loss: 0.0009374658220622223, Final Batch Loss: 5.1643361075548455e-05\n",
      "Epoch 2559, Loss: 0.0005818040299345739, Final Batch Loss: 0.00012863229494541883\n",
      "Epoch 2560, Loss: 0.001197120031065424, Final Batch Loss: 9.682347808848135e-06\n",
      "Epoch 2561, Loss: 0.0031901010570436483, Final Batch Loss: 0.0019687614403665066\n",
      "Epoch 2562, Loss: 0.01125119075004477, Final Batch Loss: 0.00011679825547616929\n",
      "Epoch 2563, Loss: 0.0015269135642483889, Final Batch Loss: 5.195014182390878e-06\n",
      "Epoch 2564, Loss: 0.001632999672438018, Final Batch Loss: 8.715287549421191e-05\n",
      "Epoch 2565, Loss: 0.013909327331930399, Final Batch Loss: 0.0006619376945309341\n",
      "Epoch 2566, Loss: 0.025097400379308965, Final Batch Loss: 0.012005725875496864\n",
      "Epoch 2567, Loss: 0.03684214832901489, Final Batch Loss: 0.00011611184163484722\n",
      "Epoch 2568, Loss: 0.008030017863347894, Final Batch Loss: 6.073424810892902e-05\n",
      "Epoch 2569, Loss: 0.04704204671725165, Final Batch Loss: 0.00024319432850461453\n",
      "Epoch 2570, Loss: 0.042429009365150705, Final Batch Loss: 0.040550462901592255\n",
      "Epoch 2571, Loss: 0.003004848214914091, Final Batch Loss: 0.0008935233927331865\n",
      "Epoch 2572, Loss: 0.007309114080271684, Final Batch Loss: 0.0007589786546304822\n",
      "Epoch 2573, Loss: 0.0033898416440933943, Final Batch Loss: 0.0020562142599374056\n",
      "Epoch 2574, Loss: 0.0033631747064646333, Final Batch Loss: 0.0013043215731158853\n",
      "Epoch 2575, Loss: 0.0019001466716872528, Final Batch Loss: 0.00012949614028912038\n",
      "Epoch 2576, Loss: 0.00338587470469065, Final Batch Loss: 0.0013933618320152164\n",
      "Epoch 2577, Loss: 0.0007370594212261494, Final Batch Loss: 0.0001727553753880784\n",
      "Epoch 2578, Loss: 0.0011235433048568666, Final Batch Loss: 0.0005116910906508565\n",
      "Epoch 2579, Loss: 0.005866020110261161, Final Batch Loss: 6.825937452958897e-05\n",
      "Epoch 2580, Loss: 0.0016537343908566982, Final Batch Loss: 0.00047225429443642497\n",
      "Epoch 2581, Loss: 0.002079620651784353, Final Batch Loss: 0.0003061274765059352\n",
      "Epoch 2582, Loss: 0.00039361516246572137, Final Batch Loss: 9.83060963335447e-05\n",
      "Epoch 2583, Loss: 0.005515233329788316, Final Batch Loss: 0.004491980187594891\n",
      "Epoch 2584, Loss: 0.00659548749536043, Final Batch Loss: 7.22045951988548e-05\n",
      "Epoch 2585, Loss: 0.01714740560782957, Final Batch Loss: 0.00011178725253557786\n",
      "Epoch 2586, Loss: 0.0007013017584540648, Final Batch Loss: 0.00043248277506791055\n",
      "Epoch 2587, Loss: 0.0020375079111545347, Final Batch Loss: 7.226582238217816e-05\n",
      "Epoch 2588, Loss: 0.000502122362377122, Final Batch Loss: 4.942570376442745e-05\n",
      "Epoch 2589, Loss: 0.0011421231101849116, Final Batch Loss: 7.619441021233797e-05\n",
      "Epoch 2590, Loss: 0.00491246936871903, Final Batch Loss: 9.580936603015289e-05\n",
      "Epoch 2591, Loss: 0.002336401135835331, Final Batch Loss: 6.29658970865421e-05\n",
      "Epoch 2592, Loss: 0.008048246672842652, Final Batch Loss: 0.00016346880875062197\n",
      "Epoch 2593, Loss: 0.001798154556126974, Final Batch Loss: 0.0001479252678109333\n",
      "Epoch 2594, Loss: 0.001159144056146033, Final Batch Loss: 0.00019954073650296777\n",
      "Epoch 2595, Loss: 0.0011809879506472498, Final Batch Loss: 7.88550969446078e-05\n",
      "Epoch 2596, Loss: 0.0009206918402924202, Final Batch Loss: 0.00014833567547611892\n",
      "Epoch 2597, Loss: 0.0007430780588038033, Final Batch Loss: 3.494744669296779e-05\n",
      "Epoch 2598, Loss: 0.0027446948934084503, Final Batch Loss: 2.079501973639708e-05\n",
      "Epoch 2599, Loss: 0.0024211909803852905, Final Batch Loss: 0.0017016396159306169\n",
      "Epoch 2600, Loss: 0.0033009501494234428, Final Batch Loss: 0.0006953190895728767\n",
      "Epoch 2601, Loss: 0.00021975120034767315, Final Batch Loss: 5.470619726111181e-05\n",
      "Epoch 2602, Loss: 0.0014947872696211562, Final Batch Loss: 3.4048265661112964e-05\n",
      "Epoch 2603, Loss: 0.003403136661290773, Final Batch Loss: 2.4074275643215515e-05\n",
      "Epoch 2604, Loss: 0.00033841751792351715, Final Batch Loss: 1.9451126718195155e-05\n",
      "Epoch 2605, Loss: 0.0008329505362780765, Final Batch Loss: 7.351347449002787e-05\n",
      "Epoch 2606, Loss: 0.006296389839917538, Final Batch Loss: 7.98670735093765e-05\n",
      "Epoch 2607, Loss: 0.0033866948760987725, Final Batch Loss: 1.709388379822485e-05\n",
      "Epoch 2608, Loss: 0.0022858926276967395, Final Batch Loss: 4.109750807401724e-05\n",
      "Epoch 2609, Loss: 0.0021407906897366047, Final Batch Loss: 9.576534648658708e-05\n",
      "Epoch 2610, Loss: 0.0007718193519394845, Final Batch Loss: 7.212947093648836e-05\n",
      "Epoch 2611, Loss: 0.02227740516536869, Final Batch Loss: 0.00021309370640665293\n",
      "Epoch 2612, Loss: 0.001324057040619664, Final Batch Loss: 0.00013743838644586504\n",
      "Epoch 2613, Loss: 0.0018465433313394897, Final Batch Loss: 0.001417841063812375\n",
      "Epoch 2614, Loss: 0.000667411986796651, Final Batch Loss: 4.5382665120996535e-05\n",
      "Epoch 2615, Loss: 0.00029872408049413934, Final Batch Loss: 3.420169377932325e-05\n",
      "Epoch 2616, Loss: 0.0020935125940013677, Final Batch Loss: 0.0010971458395943046\n",
      "Epoch 2617, Loss: 0.0009097188376472332, Final Batch Loss: 9.27104483707808e-05\n",
      "Epoch 2618, Loss: 0.002453359054925386, Final Batch Loss: 1.4368517440743744e-05\n",
      "Epoch 2619, Loss: 0.00020119500550208613, Final Batch Loss: 3.516035940265283e-05\n",
      "Epoch 2620, Loss: 0.035334074640559265, Final Batch Loss: 0.00017392019799444824\n",
      "Epoch 2621, Loss: 0.05050600967661012, Final Batch Loss: 0.012623350135982037\n",
      "Epoch 2622, Loss: 0.000832371755677741, Final Batch Loss: 0.00012635445455089211\n",
      "Epoch 2623, Loss: 0.003919546361430548, Final Batch Loss: 0.0009981801267713308\n",
      "Epoch 2624, Loss: 0.0018634027219377458, Final Batch Loss: 0.0003770299081224948\n",
      "Epoch 2625, Loss: 0.0008869258563208859, Final Batch Loss: 0.0001578961091581732\n",
      "Epoch 2626, Loss: 0.020750969648361206, Final Batch Loss: 0.0070215919986367226\n",
      "Epoch 2627, Loss: 0.01648654720702325, Final Batch Loss: 0.0006848523626103997\n",
      "Epoch 2628, Loss: 0.003406909225304844, Final Batch Loss: 4.7528334107482806e-05\n",
      "Epoch 2629, Loss: 0.018618072193930857, Final Batch Loss: 0.00039790745358914137\n",
      "Epoch 2630, Loss: 0.0015425314986714511, Final Batch Loss: 1.0982285857608076e-05\n",
      "Epoch 2631, Loss: 0.05984638708468992, Final Batch Loss: 0.0028657494112849236\n",
      "Epoch 2632, Loss: 0.03977274158387445, Final Batch Loss: 0.03363228216767311\n",
      "Epoch 2633, Loss: 0.0008716537486179732, Final Batch Loss: 6.60894438624382e-05\n",
      "Epoch 2634, Loss: 0.0017766491218935698, Final Batch Loss: 0.0008443612605333328\n",
      "Epoch 2635, Loss: 0.018308291269931942, Final Batch Loss: 0.007970434613525867\n",
      "Epoch 2636, Loss: 0.003261850080889417, Final Batch Loss: 3.824635859928094e-05\n",
      "Epoch 2637, Loss: 0.0013025801345065702, Final Batch Loss: 0.0010477266041561961\n",
      "Epoch 2638, Loss: 0.002293167206516955, Final Batch Loss: 6.795165973016992e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2639, Loss: 0.04282848727598321, Final Batch Loss: 0.04191512614488602\n",
      "Epoch 2640, Loss: 0.004757354588946328, Final Batch Loss: 0.00018562996410764754\n",
      "Epoch 2641, Loss: 0.0013435405271593481, Final Batch Loss: 0.0006022762972861528\n",
      "Epoch 2642, Loss: 0.003294744004961103, Final Batch Loss: 0.0007712281076237559\n",
      "Epoch 2643, Loss: 0.000977636402240023, Final Batch Loss: 4.102957609575242e-05\n",
      "Epoch 2644, Loss: 0.003353105843416415, Final Batch Loss: 0.0022376126144081354\n",
      "Epoch 2645, Loss: 0.021386116088251583, Final Batch Loss: 0.02000192552804947\n",
      "Epoch 2646, Loss: 0.0036024925211677328, Final Batch Loss: 0.002109907567501068\n",
      "Epoch 2647, Loss: 0.0050988381044589914, Final Batch Loss: 6.351000774884596e-05\n",
      "Epoch 2648, Loss: 0.004075986216776073, Final Batch Loss: 0.000372537033399567\n",
      "Epoch 2649, Loss: 0.0005634395783999935, Final Batch Loss: 8.916415390558541e-05\n",
      "Epoch 2650, Loss: 0.0018170776093029417, Final Batch Loss: 0.00019547164265532047\n",
      "Epoch 2651, Loss: 0.0013154001426300965, Final Batch Loss: 3.1186114938464016e-05\n",
      "Epoch 2652, Loss: 0.01861386970267631, Final Batch Loss: 0.00034390133805572987\n",
      "Epoch 2653, Loss: 0.006482703462097561, Final Batch Loss: 2.184934783144854e-05\n",
      "Epoch 2654, Loss: 0.0037555191811406985, Final Batch Loss: 0.0011214815312996507\n",
      "Epoch 2655, Loss: 0.0035465658947941847, Final Batch Loss: 8.614717080490664e-05\n",
      "Epoch 2656, Loss: 0.007532128365710378, Final Batch Loss: 0.0011696558212861419\n",
      "Epoch 2657, Loss: 0.004450473046745174, Final Batch Loss: 0.003937591332942247\n",
      "Epoch 2658, Loss: 0.009101644143811427, Final Batch Loss: 0.00011658873700071126\n",
      "Epoch 2659, Loss: 0.0020853680289292242, Final Batch Loss: 4.535748666967265e-05\n",
      "Epoch 2660, Loss: 0.0032387901046604384, Final Batch Loss: 4.022517896373756e-05\n",
      "Epoch 2661, Loss: 0.026260911545250565, Final Batch Loss: 0.005219253245741129\n",
      "Epoch 2662, Loss: 0.0010796907590702176, Final Batch Loss: 8.375146717298776e-05\n",
      "Epoch 2663, Loss: 0.0021439850534079596, Final Batch Loss: 0.00025618402287364006\n",
      "Epoch 2664, Loss: 0.0013786867202725261, Final Batch Loss: 0.0003203570086043328\n",
      "Epoch 2665, Loss: 0.005487867470947094, Final Batch Loss: 0.00017428529099561274\n",
      "Epoch 2666, Loss: 0.0011738241446437314, Final Batch Loss: 0.00043564900988712907\n",
      "Epoch 2667, Loss: 0.0015320117890951224, Final Batch Loss: 0.00011145293683512136\n",
      "Epoch 2668, Loss: 0.0007274318049894646, Final Batch Loss: 0.0001326656638411805\n",
      "Epoch 2669, Loss: 0.006194547844643239, Final Batch Loss: 0.00021520980226341635\n",
      "Epoch 2670, Loss: 0.01660473281663144, Final Batch Loss: 0.009052970446646214\n",
      "Epoch 2671, Loss: 0.0005412873506429605, Final Batch Loss: 0.0003264638071414083\n",
      "Epoch 2672, Loss: 0.0012627975083887577, Final Batch Loss: 0.0004234222578816116\n",
      "Epoch 2673, Loss: 0.0009211083015543409, Final Batch Loss: 0.00022525327221956104\n",
      "Epoch 2674, Loss: 0.004245406642439775, Final Batch Loss: 5.988536577206105e-05\n",
      "Epoch 2675, Loss: 0.0016409919844591059, Final Batch Loss: 0.00012145940127084032\n",
      "Epoch 2676, Loss: 0.00825503446685616, Final Batch Loss: 0.0002705841907300055\n",
      "Epoch 2677, Loss: 0.002487730816937983, Final Batch Loss: 0.0013581077801063657\n",
      "Epoch 2678, Loss: 0.0030550753726856783, Final Batch Loss: 0.001343448180705309\n",
      "Epoch 2679, Loss: 0.0010815870937221916, Final Batch Loss: 0.0001677010441198945\n",
      "Epoch 2680, Loss: 0.002191094506997615, Final Batch Loss: 0.0015829632757231593\n",
      "Epoch 2681, Loss: 0.004987463413272053, Final Batch Loss: 0.00026006222469732165\n",
      "Epoch 2682, Loss: 0.0008720968471607193, Final Batch Loss: 0.0001630600163480267\n",
      "Epoch 2683, Loss: 0.000921659309824463, Final Batch Loss: 3.402724541956559e-05\n",
      "Epoch 2684, Loss: 0.0005625229205179494, Final Batch Loss: 0.0002015893260249868\n",
      "Epoch 2685, Loss: 0.020942869363352656, Final Batch Loss: 0.00038075284101068974\n",
      "Epoch 2686, Loss: 0.000905257387785241, Final Batch Loss: 6.240695802262053e-05\n",
      "Epoch 2687, Loss: 0.0009287130233133212, Final Batch Loss: 3.732461846084334e-05\n",
      "Epoch 2688, Loss: 0.0016106517086882377, Final Batch Loss: 0.00017695011047180742\n",
      "Epoch 2689, Loss: 0.0014386674793058774, Final Batch Loss: 2.9517241273424588e-05\n",
      "Epoch 2690, Loss: 0.0011544868311830214, Final Batch Loss: 1.476697525504278e-05\n",
      "Epoch 2691, Loss: 0.03223815810633823, Final Batch Loss: 0.0006397066754288971\n",
      "Epoch 2692, Loss: 0.015096002913196571, Final Batch Loss: 0.0003765845322050154\n",
      "Epoch 2693, Loss: 0.002101699967170134, Final Batch Loss: 0.0001924788230098784\n",
      "Epoch 2694, Loss: 0.0007933462729852181, Final Batch Loss: 0.00027293176390230656\n",
      "Epoch 2695, Loss: 0.003238714394683484, Final Batch Loss: 0.00030879347468726337\n",
      "Epoch 2696, Loss: 0.004689687048085034, Final Batch Loss: 0.00015144492499530315\n",
      "Epoch 2697, Loss: 0.041056161218875786, Final Batch Loss: 3.106466829194687e-05\n",
      "Epoch 2698, Loss: 0.0025088925867748912, Final Batch Loss: 7.60918774176389e-05\n",
      "Epoch 2699, Loss: 0.0021615531004499644, Final Batch Loss: 0.0009247070993296802\n",
      "Epoch 2700, Loss: 0.010779649921460077, Final Batch Loss: 6.414714152924716e-05\n",
      "Epoch 2701, Loss: 0.014507706997392233, Final Batch Loss: 1.93925152416341e-05\n",
      "Epoch 2702, Loss: 0.00152663894550642, Final Batch Loss: 0.00016934236919041723\n",
      "Epoch 2703, Loss: 0.002915101169492118, Final Batch Loss: 0.001173255848698318\n",
      "Epoch 2704, Loss: 0.004398975892399903, Final Batch Loss: 0.00010014656436396763\n",
      "Epoch 2705, Loss: 0.00211549797677435, Final Batch Loss: 0.00043412428931333125\n",
      "Epoch 2706, Loss: 0.0012868825506302528, Final Batch Loss: 0.0008423750405199826\n",
      "Epoch 2707, Loss: 0.0017903541011037305, Final Batch Loss: 0.0001439979678252712\n",
      "Epoch 2708, Loss: 0.01789775677025318, Final Batch Loss: 0.0053255255334079266\n",
      "Epoch 2709, Loss: 0.01157595957920421, Final Batch Loss: 4.757221176987514e-05\n",
      "Epoch 2710, Loss: 0.0015453229862032458, Final Batch Loss: 0.00015810331387910992\n",
      "Epoch 2711, Loss: 0.009821443039982114, Final Batch Loss: 3.082121111219749e-05\n",
      "Epoch 2712, Loss: 0.0003669187099148985, Final Batch Loss: 1.0801999451359734e-05\n",
      "Epoch 2713, Loss: 0.004440352713572793, Final Batch Loss: 0.00010119993385160342\n",
      "Epoch 2714, Loss: 0.006664761604042724, Final Batch Loss: 0.00012112083641113713\n",
      "Epoch 2715, Loss: 0.0022416803694795817, Final Batch Loss: 0.00029261954477988183\n",
      "Epoch 2716, Loss: 0.0015022232619230635, Final Batch Loss: 0.00012033274833811447\n",
      "Epoch 2717, Loss: 0.001029841398121789, Final Batch Loss: 0.00026355840964242816\n",
      "Epoch 2718, Loss: 0.021445724538352806, Final Batch Loss: 9.759107342688367e-05\n",
      "Epoch 2719, Loss: 0.01609004123019986, Final Batch Loss: 0.013682288117706776\n",
      "Epoch 2720, Loss: 0.0008738667584111681, Final Batch Loss: 4.633714706869796e-05\n",
      "Epoch 2721, Loss: 0.0015853301556489896, Final Batch Loss: 3.9896814996609464e-05\n",
      "Epoch 2722, Loss: 0.004918385922792368, Final Batch Loss: 0.00021325342822819948\n",
      "Epoch 2723, Loss: 0.006379988983098883, Final Batch Loss: 0.000629389367531985\n",
      "Epoch 2724, Loss: 0.005432880134321749, Final Batch Loss: 0.0012654268648475409\n",
      "Epoch 2725, Loss: 0.022817731984105194, Final Batch Loss: 0.00496232183650136\n",
      "Epoch 2726, Loss: 0.003659312344097998, Final Batch Loss: 0.0032690700609236956\n",
      "Epoch 2727, Loss: 0.024453711972455494, Final Batch Loss: 0.00023522770788986236\n",
      "Epoch 2728, Loss: 0.004204901502816938, Final Batch Loss: 0.0001756902929628268\n",
      "Epoch 2729, Loss: 0.0009371821506647393, Final Batch Loss: 5.820805381517857e-05\n",
      "Epoch 2730, Loss: 0.0006146625673864037, Final Batch Loss: 0.00028413752443157136\n",
      "Epoch 2731, Loss: 0.013768364835414104, Final Batch Loss: 0.00016645502182655036\n",
      "Epoch 2732, Loss: 0.001891014544526115, Final Batch Loss: 0.0005885619320906699\n",
      "Epoch 2733, Loss: 0.0010261317147524096, Final Batch Loss: 0.00010996808850904927\n",
      "Epoch 2734, Loss: 0.0014223882371879881, Final Batch Loss: 2.0220735677867197e-05\n",
      "Epoch 2735, Loss: 0.0018132079421775416, Final Batch Loss: 6.210232095327228e-05\n",
      "Epoch 2736, Loss: 0.0009814779987209477, Final Batch Loss: 0.0004087271518073976\n",
      "Epoch 2737, Loss: 0.04243557441077428, Final Batch Loss: 0.04197977855801582\n",
      "Epoch 2738, Loss: 0.0006836304091848433, Final Batch Loss: 5.7654739066492766e-05\n",
      "Epoch 2739, Loss: 0.0032229198259301484, Final Batch Loss: 0.0002941523853223771\n",
      "Epoch 2740, Loss: 0.0018308593007532181, Final Batch Loss: 1.4383304005605169e-05\n",
      "Epoch 2741, Loss: 0.0015686981496401131, Final Batch Loss: 0.0005765517125837505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2742, Loss: 0.0024962507700365677, Final Batch Loss: 4.4095572775404435e-06\n",
      "Epoch 2743, Loss: 0.004128296246562968, Final Batch Loss: 0.0022899415343999863\n",
      "Epoch 2744, Loss: 0.0012649865675484762, Final Batch Loss: 0.00033449422335252166\n",
      "Epoch 2745, Loss: 0.0018532467220211402, Final Batch Loss: 0.0006749252206645906\n",
      "Epoch 2746, Loss: 0.005013679339754162, Final Batch Loss: 0.0004541264788713306\n",
      "Epoch 2747, Loss: 0.0009093918015423696, Final Batch Loss: 4.541295857052319e-05\n",
      "Epoch 2748, Loss: 0.0005617898423224688, Final Batch Loss: 8.106859604595229e-05\n",
      "Epoch 2749, Loss: 0.0012406959940562956, Final Batch Loss: 0.00041424832306802273\n",
      "Epoch 2750, Loss: 0.0008821630181046203, Final Batch Loss: 6.227631820365787e-05\n",
      "Epoch 2751, Loss: 0.009966599493054673, Final Batch Loss: 8.762983634369448e-05\n",
      "Epoch 2752, Loss: 0.000959694072662387, Final Batch Loss: 0.0001934080064529553\n",
      "Epoch 2753, Loss: 0.0012700352890533395, Final Batch Loss: 0.0004310462099965662\n",
      "Epoch 2754, Loss: 0.0009914741458487697, Final Batch Loss: 0.00035543524427339435\n",
      "Epoch 2755, Loss: 0.0127485022530891, Final Batch Loss: 0.01088478323072195\n",
      "Epoch 2756, Loss: 0.0007629110004927497, Final Batch Loss: 3.3331223676213995e-05\n",
      "Epoch 2757, Loss: 0.0008516208899891353, Final Batch Loss: 1.0509588719287422e-05\n",
      "Epoch 2758, Loss: 0.0006394448137143627, Final Batch Loss: 0.0001514501782367006\n",
      "Epoch 2759, Loss: 0.000584623100621684, Final Batch Loss: 1.4521731827699114e-05\n",
      "Epoch 2760, Loss: 0.001322098818491213, Final Batch Loss: 0.0003765189612749964\n",
      "Epoch 2761, Loss: 0.0012381621891108807, Final Batch Loss: 0.0001469740818720311\n",
      "Epoch 2762, Loss: 0.001372628861645353, Final Batch Loss: 0.00016278315160889179\n",
      "Epoch 2763, Loss: 0.0004718350919574732, Final Batch Loss: 2.1784066120744683e-05\n",
      "Epoch 2764, Loss: 0.015261828069924377, Final Batch Loss: 6.851406214991584e-05\n",
      "Epoch 2765, Loss: 0.008248902908235323, Final Batch Loss: 6.985759682720527e-05\n",
      "Epoch 2766, Loss: 0.008400081642321311, Final Batch Loss: 6.231915904209018e-05\n",
      "Epoch 2767, Loss: 0.00967277211725559, Final Batch Loss: 2.6018749395007035e-06\n",
      "Epoch 2768, Loss: 0.0062341467855731025, Final Batch Loss: 0.00012757153308484703\n",
      "Epoch 2769, Loss: 0.0029649155330844223, Final Batch Loss: 0.0011351134162396193\n",
      "Epoch 2770, Loss: 0.005334759283869062, Final Batch Loss: 4.015351441921666e-05\n",
      "Epoch 2771, Loss: 0.017969377397093922, Final Batch Loss: 8.28256452223286e-05\n",
      "Epoch 2772, Loss: 0.0019958126358687878, Final Batch Loss: 0.00022940798953641206\n",
      "Epoch 2773, Loss: 0.0003542390368238557, Final Batch Loss: 5.113158840686083e-05\n",
      "Epoch 2774, Loss: 0.0004618771263267263, Final Batch Loss: 7.139641638786998e-06\n",
      "Epoch 2775, Loss: 0.0007233915348479059, Final Batch Loss: 0.0003850547655019909\n",
      "Epoch 2776, Loss: 0.001514000992756337, Final Batch Loss: 0.00010488735279068351\n",
      "Epoch 2777, Loss: 0.0060685827920679, Final Batch Loss: 0.00015968862862791866\n",
      "Epoch 2778, Loss: 0.013832640561304288, Final Batch Loss: 0.00012156625598436221\n",
      "Epoch 2779, Loss: 0.003058330767089501, Final Batch Loss: 0.0020731776021420956\n",
      "Epoch 2780, Loss: 0.01049115534988232, Final Batch Loss: 0.0019284303998574615\n",
      "Epoch 2781, Loss: 0.0010222469354630448, Final Batch Loss: 0.00014049674791749567\n",
      "Epoch 2782, Loss: 0.0006786557314626407, Final Batch Loss: 0.0003166177193634212\n",
      "Epoch 2783, Loss: 0.00136508175637573, Final Batch Loss: 0.0008563666488043964\n",
      "Epoch 2784, Loss: 0.00022907766106072813, Final Batch Loss: 8.402320963796228e-05\n",
      "Epoch 2785, Loss: 0.0016516587857040577, Final Batch Loss: 0.000666585227008909\n",
      "Epoch 2786, Loss: 0.008457848227408249, Final Batch Loss: 8.141405851347372e-05\n",
      "Epoch 2787, Loss: 0.0007707754775765352, Final Batch Loss: 4.04963648179546e-05\n",
      "Epoch 2788, Loss: 0.01342604177625617, Final Batch Loss: 0.00015215961320791394\n",
      "Epoch 2789, Loss: 0.000695957409334369, Final Batch Loss: 3.774816286750138e-05\n",
      "Epoch 2790, Loss: 0.0005461729706439655, Final Batch Loss: 0.00022556667681783438\n",
      "Epoch 2791, Loss: 0.002152807792299427, Final Batch Loss: 0.0001469470007577911\n",
      "Epoch 2792, Loss: 0.013227648818428861, Final Batch Loss: 0.00019094956223852932\n",
      "Epoch 2793, Loss: 0.000886639611053397, Final Batch Loss: 5.275146759231575e-05\n",
      "Epoch 2794, Loss: 0.0011719032045220956, Final Batch Loss: 0.00010327393829356879\n",
      "Epoch 2795, Loss: 0.002334887692995835, Final Batch Loss: 0.00021984276827424765\n",
      "Epoch 2796, Loss: 0.0008007539381651441, Final Batch Loss: 2.2135713152238168e-05\n",
      "Epoch 2797, Loss: 0.0009383780034113443, Final Batch Loss: 1.1131049177492969e-05\n",
      "Epoch 2798, Loss: 0.0011355245223967358, Final Batch Loss: 0.00023364600201603025\n",
      "Epoch 2799, Loss: 0.0047886129905236885, Final Batch Loss: 5.410816811490804e-05\n",
      "Epoch 2800, Loss: 0.0025772490989766084, Final Batch Loss: 6.284892879193649e-05\n",
      "Epoch 2801, Loss: 0.00025918393839674536, Final Batch Loss: 8.613055251771584e-05\n",
      "Epoch 2802, Loss: 0.0012518753574113362, Final Batch Loss: 8.973329386208206e-05\n",
      "Epoch 2803, Loss: 0.0010307179763913155, Final Batch Loss: 0.0001551389432279393\n",
      "Epoch 2804, Loss: 0.0027880981360794976, Final Batch Loss: 0.0022140981163829565\n",
      "Epoch 2805, Loss: 0.02460864641761873, Final Batch Loss: 0.00030903317383490503\n",
      "Epoch 2806, Loss: 0.0016721886549930787, Final Batch Loss: 4.187821105006151e-05\n",
      "Epoch 2807, Loss: 0.001205109030706808, Final Batch Loss: 0.00014960175030864775\n",
      "Epoch 2808, Loss: 0.0006311843499133829, Final Batch Loss: 4.898470069747418e-05\n",
      "Epoch 2809, Loss: 0.0023248260313266655, Final Batch Loss: 1.877792783488985e-05\n",
      "Epoch 2810, Loss: 0.002295120677445084, Final Batch Loss: 0.00031570179271511734\n",
      "Epoch 2811, Loss: 0.00023003936985332984, Final Batch Loss: 1.8546577848610468e-05\n",
      "Epoch 2812, Loss: 0.0004817150293092709, Final Batch Loss: 4.174132118350826e-05\n",
      "Epoch 2813, Loss: 0.00925583389471285, Final Batch Loss: 0.00023929672897793353\n",
      "Epoch 2814, Loss: 0.0005867343479621923, Final Batch Loss: 1.629855796636548e-05\n",
      "Epoch 2815, Loss: 0.0007727232004981488, Final Batch Loss: 3.638455746113323e-05\n",
      "Epoch 2816, Loss: 0.0007008989705354907, Final Batch Loss: 0.00016514325398020446\n",
      "Epoch 2817, Loss: 0.0005194114273763262, Final Batch Loss: 0.00011472245387267321\n",
      "Epoch 2818, Loss: 0.00038866264094394865, Final Batch Loss: 1.2804636753571685e-05\n",
      "Epoch 2819, Loss: 0.0002919740836659912, Final Batch Loss: 1.9670136680360883e-05\n",
      "Epoch 2820, Loss: 0.0004956706061420846, Final Batch Loss: 7.954032298584934e-06\n",
      "Epoch 2821, Loss: 0.010447441898577381, Final Batch Loss: 0.010079571977257729\n",
      "Epoch 2822, Loss: 0.00047430884296773, Final Batch Loss: 0.00016250372573267668\n",
      "Epoch 2823, Loss: 0.0029617834152304567, Final Batch Loss: 0.0003572036512196064\n",
      "Epoch 2824, Loss: 0.0026231681840727106, Final Batch Loss: 0.00018638606707099825\n",
      "Epoch 2825, Loss: 0.00340036695888557, Final Batch Loss: 2.8291020498727448e-05\n",
      "Epoch 2826, Loss: 0.008879516750312177, Final Batch Loss: 0.0005180549924261868\n",
      "Epoch 2827, Loss: 0.03158174353666254, Final Batch Loss: 0.0019520726054906845\n",
      "Epoch 2828, Loss: 0.0034476970467949286, Final Batch Loss: 0.00030398336821235716\n",
      "Epoch 2829, Loss: 0.002449581057589967, Final Batch Loss: 7.81461494625546e-05\n",
      "Epoch 2830, Loss: 0.006621244091547851, Final Batch Loss: 1.518000954092713e-05\n",
      "Epoch 2831, Loss: 0.0033435130299039884, Final Batch Loss: 0.00034879709710367024\n",
      "Epoch 2832, Loss: 0.002552781199938181, Final Batch Loss: 5.202721240493702e-06\n",
      "Epoch 2833, Loss: 0.0005911472326260991, Final Batch Loss: 0.00023763193166814744\n",
      "Epoch 2834, Loss: 0.01869015935517382, Final Batch Loss: 0.0008158340351656079\n",
      "Epoch 2835, Loss: 0.0008018629159778357, Final Batch Loss: 0.00038082414539530873\n",
      "Epoch 2836, Loss: 0.0015146427758736536, Final Batch Loss: 0.0001737337588565424\n",
      "Epoch 2837, Loss: 0.0015158881651586853, Final Batch Loss: 0.0007928941049613059\n",
      "Epoch 2838, Loss: 0.0021621134255838115, Final Batch Loss: 3.889808795065619e-05\n",
      "Epoch 2839, Loss: 0.0018526262247178238, Final Batch Loss: 2.6654543034965172e-05\n",
      "Epoch 2840, Loss: 0.003418939988478087, Final Batch Loss: 0.002215531887486577\n",
      "Epoch 2841, Loss: 0.0017766753589967266, Final Batch Loss: 7.484229718102142e-05\n",
      "Epoch 2842, Loss: 0.0004902160053461557, Final Batch Loss: 0.0003162365173920989\n",
      "Epoch 2843, Loss: 0.000579723171540536, Final Batch Loss: 3.923531039617956e-05\n",
      "Epoch 2844, Loss: 0.0007334093916142592, Final Batch Loss: 2.0500536265899427e-05\n",
      "Epoch 2845, Loss: 0.05405227093888243, Final Batch Loss: 1.017882186715724e-05\n",
      "Epoch 2846, Loss: 0.0023608844185218913, Final Batch Loss: 2.0866651539108716e-05\n",
      "Epoch 2847, Loss: 0.004103991719603073, Final Batch Loss: 0.00014081207336857915\n",
      "Epoch 2848, Loss: 0.0010106235858984292, Final Batch Loss: 0.0001685228053247556\n",
      "Epoch 2849, Loss: 0.0008466725830658106, Final Batch Loss: 2.3033137040329166e-05\n",
      "Epoch 2850, Loss: 0.007814391916326713, Final Batch Loss: 0.00011561164137674496\n",
      "Epoch 2851, Loss: 0.004728079846245237, Final Batch Loss: 9.272513852920383e-05\n",
      "Epoch 2852, Loss: 0.0015752183899166994, Final Batch Loss: 0.0012069018557667732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2853, Loss: 0.0013043884182479815, Final Batch Loss: 0.0002129290805896744\n",
      "Epoch 2854, Loss: 0.011522625234647421, Final Batch Loss: 5.786878682556562e-05\n",
      "Epoch 2855, Loss: 0.005643287731800228, Final Batch Loss: 0.004581796005368233\n",
      "Epoch 2856, Loss: 0.0017474291307735257, Final Batch Loss: 0.0006298258667811751\n",
      "Epoch 2857, Loss: 0.001708887535642134, Final Batch Loss: 0.00014473896590061486\n",
      "Epoch 2858, Loss: 0.003429086627875222, Final Batch Loss: 0.0010506543330848217\n",
      "Epoch 2859, Loss: 0.0009422211223863997, Final Batch Loss: 0.0006881494773551822\n",
      "Epoch 2860, Loss: 0.003380640404429869, Final Batch Loss: 2.116955874953419e-05\n",
      "Epoch 2861, Loss: 0.0004968649591319263, Final Batch Loss: 3.708811345859431e-05\n",
      "Epoch 2862, Loss: 0.0017568884413776686, Final Batch Loss: 1.6006879377528094e-05\n",
      "Epoch 2863, Loss: 0.001318482703936752, Final Batch Loss: 5.4786411055829376e-05\n",
      "Epoch 2864, Loss: 0.001420950497049489, Final Batch Loss: 2.5273709979956038e-05\n",
      "Epoch 2865, Loss: 0.004985858911823016, Final Batch Loss: 0.0032319833990186453\n",
      "Epoch 2866, Loss: 0.006441443183575757, Final Batch Loss: 0.005900843068957329\n",
      "Epoch 2867, Loss: 0.004671732162023545, Final Batch Loss: 8.714570867596194e-05\n",
      "Epoch 2868, Loss: 0.0007598448464705143, Final Batch Loss: 0.0003057756111957133\n",
      "Epoch 2869, Loss: 0.0016788785214885138, Final Batch Loss: 3.679635119624436e-05\n",
      "Epoch 2870, Loss: 0.00032867166737560183, Final Batch Loss: 8.434278424829245e-05\n",
      "Epoch 2871, Loss: 0.00420898342053988, Final Batch Loss: 0.00010653572098817676\n",
      "Epoch 2872, Loss: 0.0007091842999216169, Final Batch Loss: 0.00023579566914122552\n",
      "Epoch 2873, Loss: 0.0011707946978276595, Final Batch Loss: 0.0001542063691886142\n",
      "Epoch 2874, Loss: 0.00041502525891701225, Final Batch Loss: 2.1391258997027762e-05\n",
      "Epoch 2875, Loss: 0.0010271463142998982, Final Batch Loss: 0.000270023854682222\n",
      "Epoch 2876, Loss: 0.0006453788882936351, Final Batch Loss: 8.710166002856568e-05\n",
      "Epoch 2877, Loss: 0.0008874705326888943, Final Batch Loss: 2.5589994038455188e-05\n",
      "Epoch 2878, Loss: 0.000683369147736812, Final Batch Loss: 0.00011732280108844861\n",
      "Epoch 2879, Loss: 0.00023303914531425107, Final Batch Loss: 0.0001269081694772467\n",
      "Epoch 2880, Loss: 0.003917278299923055, Final Batch Loss: 0.003365184413269162\n",
      "Epoch 2881, Loss: 0.0022710092143825023, Final Batch Loss: 1.881050229712855e-05\n",
      "Epoch 2882, Loss: 0.0002592631753941532, Final Batch Loss: 0.0001268171181436628\n",
      "Epoch 2883, Loss: 0.0007185115828178823, Final Batch Loss: 0.00010310309153283015\n",
      "Epoch 2884, Loss: 0.001568485691677779, Final Batch Loss: 2.1992673282511532e-05\n",
      "Epoch 2885, Loss: 0.00039065649252734147, Final Batch Loss: 8.034464553929865e-05\n",
      "Epoch 2886, Loss: 0.000609238868491957, Final Batch Loss: 0.0003479410370346159\n",
      "Epoch 2887, Loss: 0.00036922841536579654, Final Batch Loss: 2.2451227778219618e-05\n",
      "Epoch 2888, Loss: 0.0003061171737499535, Final Batch Loss: 8.876610809238628e-05\n",
      "Epoch 2889, Loss: 0.0012230781139805913, Final Batch Loss: 0.0004417354357428849\n",
      "Epoch 2890, Loss: 0.0005822012381031527, Final Batch Loss: 1.0124021173396613e-05\n",
      "Epoch 2891, Loss: 0.0009381760028190911, Final Batch Loss: 0.0005803766543976963\n",
      "Epoch 2892, Loss: 0.0027585786920099054, Final Batch Loss: 0.00019957730546593666\n",
      "Epoch 2893, Loss: 0.00043903299956582487, Final Batch Loss: 0.00023765281366650015\n",
      "Epoch 2894, Loss: 0.019871197811880847, Final Batch Loss: 7.313981041079387e-05\n",
      "Epoch 2895, Loss: 0.0012484925100579858, Final Batch Loss: 0.000879010884091258\n",
      "Epoch 2896, Loss: 0.005913053490075981, Final Batch Loss: 1.5390189219033346e-05\n",
      "Epoch 2897, Loss: 0.0010509612293390092, Final Batch Loss: 0.00023166823666542768\n",
      "Epoch 2898, Loss: 0.0008210259838961065, Final Batch Loss: 0.0003961588954553008\n",
      "Epoch 2899, Loss: 0.00017294514509558212, Final Batch Loss: 1.1555182936717756e-05\n",
      "Epoch 2900, Loss: 0.00057102390928776, Final Batch Loss: 3.583408397389576e-05\n",
      "Epoch 2901, Loss: 0.00035887945887225214, Final Batch Loss: 1.850903754530009e-05\n",
      "Epoch 2902, Loss: 0.00012365541351755382, Final Batch Loss: 4.890952550340444e-05\n",
      "Epoch 2903, Loss: 0.010507530110771768, Final Batch Loss: 0.009995197877287865\n",
      "Epoch 2904, Loss: 0.0005029496460338123, Final Batch Loss: 0.00025506297242827713\n",
      "Epoch 2905, Loss: 0.0035360721813049167, Final Batch Loss: 7.523334352299571e-05\n",
      "Epoch 2906, Loss: 0.000913445015612524, Final Batch Loss: 0.0001777627330739051\n",
      "Epoch 2907, Loss: 0.006927657334017567, Final Batch Loss: 9.291309106629342e-05\n",
      "Epoch 2908, Loss: 0.001013031112961471, Final Batch Loss: 0.00018181961786467582\n",
      "Epoch 2909, Loss: 0.04196331153616484, Final Batch Loss: 0.04078152775764465\n",
      "Epoch 2910, Loss: 0.0004913879565719981, Final Batch Loss: 0.00017925037536770105\n",
      "Epoch 2911, Loss: 0.001853325571573805, Final Batch Loss: 5.610676817013882e-05\n",
      "Epoch 2912, Loss: 0.0005827749009768013, Final Batch Loss: 2.7813370252260938e-05\n",
      "Epoch 2913, Loss: 0.0004360236198408529, Final Batch Loss: 0.0001428003452019766\n",
      "Epoch 2914, Loss: 0.0020046799545525573, Final Batch Loss: 0.0005073096253909171\n",
      "Epoch 2915, Loss: 0.014886436631059041, Final Batch Loss: 0.01417079009115696\n",
      "Epoch 2916, Loss: 0.0016800150660856161, Final Batch Loss: 2.3673190298723057e-05\n",
      "Epoch 2917, Loss: 0.05375774229742092, Final Batch Loss: 1.0569364349066745e-05\n",
      "Epoch 2918, Loss: 0.010713743231463013, Final Batch Loss: 0.00011894052295247093\n",
      "Epoch 2919, Loss: 0.034431218809913844, Final Batch Loss: 0.02567007765173912\n",
      "Epoch 2920, Loss: 0.0016024989236029796, Final Batch Loss: 0.00011357566836522892\n",
      "Epoch 2921, Loss: 0.005081733463157434, Final Batch Loss: 7.882444333517924e-05\n",
      "Epoch 2922, Loss: 0.08547510820790194, Final Batch Loss: 0.00020232200040481985\n",
      "Epoch 2923, Loss: 0.004953604438924231, Final Batch Loss: 0.00012803020945284516\n",
      "Epoch 2924, Loss: 0.0028957059330423363, Final Batch Loss: 0.0017691433895379305\n",
      "Epoch 2925, Loss: 0.07489593600621447, Final Batch Loss: 0.0676998421549797\n",
      "Epoch 2926, Loss: 0.008231380255892873, Final Batch Loss: 0.007256958167999983\n",
      "Epoch 2927, Loss: 0.012823093653423712, Final Batch Loss: 0.0010110652074217796\n",
      "Epoch 2928, Loss: 0.026934797351714224, Final Batch Loss: 0.0005892493645660579\n",
      "Epoch 2929, Loss: 0.0023905794878373854, Final Batch Loss: 0.00012540307943709195\n",
      "Epoch 2930, Loss: 0.0007876907111494802, Final Batch Loss: 9.409835183760151e-05\n",
      "Epoch 2931, Loss: 0.0008356676771654747, Final Batch Loss: 6.848591874586418e-05\n",
      "Epoch 2932, Loss: 0.001391705467540305, Final Batch Loss: 0.00011307694512652233\n",
      "Epoch 2933, Loss: 0.020776510995347053, Final Batch Loss: 0.01903137005865574\n",
      "Epoch 2934, Loss: 0.0006239334397832863, Final Batch Loss: 6.55929688946344e-05\n",
      "Epoch 2935, Loss: 0.004604112755259848, Final Batch Loss: 2.305732596141752e-05\n",
      "Epoch 2936, Loss: 0.0025172334571834654, Final Batch Loss: 0.0005692720878869295\n",
      "Epoch 2937, Loss: 0.0012747205328196287, Final Batch Loss: 0.0002560818684287369\n",
      "Epoch 2938, Loss: 0.017634720337809995, Final Batch Loss: 0.01707266829907894\n",
      "Epoch 2939, Loss: 0.004421966506924946, Final Batch Loss: 0.00028777337865903974\n",
      "Epoch 2940, Loss: 0.0028128794801887125, Final Batch Loss: 0.00029209128115326166\n",
      "Epoch 2941, Loss: 0.004942762621794827, Final Batch Loss: 0.00032973464112728834\n",
      "Epoch 2942, Loss: 0.0026959574897773564, Final Batch Loss: 0.0003683440154418349\n",
      "Epoch 2943, Loss: 0.0046187047264538705, Final Batch Loss: 0.0015659540658816695\n",
      "Epoch 2944, Loss: 0.012839292678108905, Final Batch Loss: 2.4544111511204392e-05\n",
      "Epoch 2945, Loss: 0.005987125801766524, Final Batch Loss: 0.0018467705231159925\n",
      "Epoch 2946, Loss: 0.0023101191254681908, Final Batch Loss: 0.00032079717493616045\n",
      "Epoch 2947, Loss: 0.0029829429258825257, Final Batch Loss: 0.0011997923720628023\n",
      "Epoch 2948, Loss: 0.0033122233289759606, Final Batch Loss: 0.0025639550294727087\n",
      "Epoch 2949, Loss: 0.001043257536366582, Final Batch Loss: 9.411892096977681e-05\n",
      "Epoch 2950, Loss: 0.021523376140976325, Final Batch Loss: 0.0002641427854541689\n",
      "Epoch 2951, Loss: 0.00274478062056005, Final Batch Loss: 0.0011210422962903976\n",
      "Epoch 2952, Loss: 0.006193016786710359, Final Batch Loss: 0.00016306609904859215\n",
      "Epoch 2953, Loss: 0.002501151531760115, Final Batch Loss: 0.00013099375064484775\n",
      "Epoch 2954, Loss: 0.001494272253694362, Final Batch Loss: 0.0005897727678529918\n",
      "Epoch 2955, Loss: 0.0013249041585368104, Final Batch Loss: 0.0005330705898813903\n",
      "Epoch 2956, Loss: 0.0035027140693273395, Final Batch Loss: 0.0016953678568825126\n",
      "Epoch 2957, Loss: 0.002245815710921306, Final Batch Loss: 2.6165565941482782e-05\n",
      "Epoch 2958, Loss: 0.004650901952118147, Final Batch Loss: 0.003906463738530874\n",
      "Epoch 2959, Loss: 0.0016806474013719708, Final Batch Loss: 0.000293013610644266\n",
      "Epoch 2960, Loss: 0.0026000696743722074, Final Batch Loss: 5.294534639688209e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2961, Loss: 0.0009101691757678054, Final Batch Loss: 0.00013921847858000547\n",
      "Epoch 2962, Loss: 0.0003233741572330473, Final Batch Loss: 5.296236486174166e-05\n",
      "Epoch 2963, Loss: 0.0003050922096008435, Final Batch Loss: 4.0275535866385326e-05\n",
      "Epoch 2964, Loss: 0.0005681311813532375, Final Batch Loss: 3.803181607509032e-05\n",
      "Epoch 2965, Loss: 0.00038238641354837455, Final Batch Loss: 1.6770589354564436e-05\n",
      "Epoch 2966, Loss: 0.045414194730255986, Final Batch Loss: 0.04295756667852402\n",
      "Epoch 2967, Loss: 0.0004746786999021424, Final Batch Loss: 1.2227937986608595e-05\n",
      "Epoch 2968, Loss: 0.0010922376241069287, Final Batch Loss: 0.0003039283910766244\n",
      "Epoch 2969, Loss: 0.0018622605239215773, Final Batch Loss: 3.307406950625591e-05\n",
      "Epoch 2970, Loss: 0.0004655320881283842, Final Batch Loss: 3.399958222871646e-05\n",
      "Epoch 2971, Loss: 0.0010006194352172315, Final Batch Loss: 8.395951590500772e-05\n",
      "Epoch 2972, Loss: 0.012693446908087935, Final Batch Loss: 7.883983926149085e-05\n",
      "Epoch 2973, Loss: 0.0028704787691822276, Final Batch Loss: 0.00035806302912533283\n",
      "Epoch 2974, Loss: 0.013946444654720835, Final Batch Loss: 0.010243578813970089\n",
      "Epoch 2975, Loss: 0.0015424797165906057, Final Batch Loss: 0.00022085326781962067\n",
      "Epoch 2976, Loss: 0.0015989061248546932, Final Batch Loss: 0.0008277958841063082\n",
      "Epoch 2977, Loss: 0.000753392498154426, Final Batch Loss: 0.00015223778609652072\n",
      "Epoch 2978, Loss: 0.0007787504728185013, Final Batch Loss: 4.99387679155916e-05\n",
      "Epoch 2979, Loss: 0.0011722302151611075, Final Batch Loss: 0.00031063705682754517\n",
      "Epoch 2980, Loss: 0.0009610678389435634, Final Batch Loss: 8.145911124302074e-05\n",
      "Epoch 2981, Loss: 0.0004200928160571493, Final Batch Loss: 3.776503581320867e-05\n",
      "Epoch 2982, Loss: 0.0010217185772489756, Final Batch Loss: 5.441605026135221e-05\n",
      "Epoch 2983, Loss: 0.0010073651101265568, Final Batch Loss: 3.3933105441974476e-05\n",
      "Epoch 2984, Loss: 0.0008587479023844935, Final Batch Loss: 0.000587532646022737\n",
      "Epoch 2985, Loss: 0.00047241510037565604, Final Batch Loss: 1.7889713490149006e-05\n",
      "Epoch 2986, Loss: 0.0024061305448412895, Final Batch Loss: 0.0020385743118822575\n",
      "Epoch 2987, Loss: 0.00045092727668816224, Final Batch Loss: 3.605775418691337e-05\n",
      "Epoch 2988, Loss: 0.00046139088954078034, Final Batch Loss: 9.048769425135106e-05\n",
      "Epoch 2989, Loss: 0.00021333404083634377, Final Batch Loss: 3.95237884731614e-06\n",
      "Epoch 2990, Loss: 0.017962841789994854, Final Batch Loss: 0.00037134371814318\n",
      "Epoch 2991, Loss: 0.00023589602642459795, Final Batch Loss: 6.724143167957664e-05\n",
      "Epoch 2992, Loss: 0.001735902222662844, Final Batch Loss: 4.569918473862344e-06\n",
      "Epoch 2993, Loss: 0.0012690137264144141, Final Batch Loss: 0.0005648635560646653\n",
      "Epoch 2994, Loss: 0.002442914163111709, Final Batch Loss: 3.0205999792087823e-05\n",
      "Epoch 2995, Loss: 0.0004387485223560361, Final Batch Loss: 2.1511781596927904e-05\n",
      "Epoch 2996, Loss: 0.001086361771740485, Final Batch Loss: 0.000658211822155863\n",
      "Epoch 2997, Loss: 0.03198853804497048, Final Batch Loss: 1.4635592378908768e-05\n",
      "Epoch 2998, Loss: 0.002329744656890398, Final Batch Loss: 6.918524013599381e-05\n",
      "Epoch 2999, Loss: 0.0030712480038346257, Final Batch Loss: 0.0015035381074994802\n",
      "Epoch 3000, Loss: 0.0017310203038505279, Final Batch Loss: 6.486845086328685e-05\n",
      "Epoch 3001, Loss: 0.004335197379987221, Final Batch Loss: 3.4839373256545514e-05\n",
      "Epoch 3002, Loss: 0.0012537940820038784, Final Batch Loss: 4.528188947006129e-05\n",
      "Epoch 3003, Loss: 0.000433043074735906, Final Batch Loss: 8.406932465732098e-05\n",
      "Epoch 3004, Loss: 0.002779896487481892, Final Batch Loss: 0.0023974422365427017\n",
      "Epoch 3005, Loss: 0.0008396224729949608, Final Batch Loss: 0.00011336749594192952\n",
      "Epoch 3006, Loss: 0.0005905222824367229, Final Batch Loss: 0.00044351769611239433\n",
      "Epoch 3007, Loss: 0.0003590129235817585, Final Batch Loss: 4.4847143726656213e-05\n",
      "Epoch 3008, Loss: 0.00031038367342262063, Final Batch Loss: 0.0001177232334157452\n",
      "Epoch 3009, Loss: 0.001264673010155093, Final Batch Loss: 0.0003280826786067337\n",
      "Epoch 3010, Loss: 0.00040035421261563897, Final Batch Loss: 4.476345202419907e-05\n",
      "Epoch 3011, Loss: 0.011043572223570663, Final Batch Loss: 4.492764128372073e-05\n",
      "Epoch 3012, Loss: 0.006116354634286836, Final Batch Loss: 0.0016597529174759984\n",
      "Epoch 3013, Loss: 0.011741107955458574, Final Batch Loss: 0.00223316322080791\n",
      "Epoch 3014, Loss: 0.0003020040639967192, Final Batch Loss: 3.442673187237233e-05\n",
      "Epoch 3015, Loss: 0.0014694137862534262, Final Batch Loss: 0.00010270946222590283\n",
      "Epoch 3016, Loss: 0.0010475712915649638, Final Batch Loss: 5.9543977840803564e-05\n",
      "Epoch 3017, Loss: 0.0006518218178825919, Final Batch Loss: 0.00022298310068435967\n",
      "Epoch 3018, Loss: 0.0006866178300697356, Final Batch Loss: 0.0003279544471297413\n",
      "Epoch 3019, Loss: 0.000537487079782295, Final Batch Loss: 2.4267088519991376e-05\n",
      "Epoch 3020, Loss: 0.0011104805380455218, Final Batch Loss: 0.0002078334364341572\n",
      "Epoch 3021, Loss: 0.000833253980090376, Final Batch Loss: 0.00017328323156107217\n",
      "Epoch 3022, Loss: 0.003726843286131043, Final Batch Loss: 3.051792737096548e-05\n",
      "Epoch 3023, Loss: 0.004094337902643019, Final Batch Loss: 0.000157237533130683\n",
      "Epoch 3024, Loss: 0.0007047483013593592, Final Batch Loss: 7.96913736849092e-05\n",
      "Epoch 3025, Loss: 0.0005319355150277261, Final Batch Loss: 5.906238584429957e-05\n",
      "Epoch 3026, Loss: 0.00029519923555199057, Final Batch Loss: 2.1335974452085793e-05\n",
      "Epoch 3027, Loss: 0.012727148503472563, Final Batch Loss: 5.983516530250199e-05\n",
      "Epoch 3028, Loss: 0.0007925845420686528, Final Batch Loss: 4.429861292010173e-05\n",
      "Epoch 3029, Loss: 0.00194442611609702, Final Batch Loss: 0.0009964666096493602\n",
      "Epoch 3030, Loss: 0.0004740725489682518, Final Batch Loss: 5.522739957086742e-05\n",
      "Epoch 3031, Loss: 0.0011902169935638085, Final Batch Loss: 0.0007802884792909026\n",
      "Epoch 3032, Loss: 0.007605526039696997, Final Batch Loss: 6.551505794050172e-05\n",
      "Epoch 3033, Loss: 0.0002705053921090439, Final Batch Loss: 4.808482844964601e-05\n",
      "Epoch 3034, Loss: 0.0006996205192990601, Final Batch Loss: 3.060746530536562e-05\n",
      "Epoch 3035, Loss: 0.0004533000028459355, Final Batch Loss: 1.4139543054625392e-05\n",
      "Epoch 3036, Loss: 0.024534864249289967, Final Batch Loss: 0.00020650441001635045\n",
      "Epoch 3037, Loss: 0.002768697802821407, Final Batch Loss: 4.628078386303969e-05\n",
      "Epoch 3038, Loss: 0.0008653680160932709, Final Batch Loss: 2.307769682374783e-05\n",
      "Epoch 3039, Loss: 0.0002837107385857962, Final Batch Loss: 5.124993549543433e-05\n",
      "Epoch 3040, Loss: 0.0012505624035838991, Final Batch Loss: 9.22027975320816e-05\n",
      "Epoch 3041, Loss: 0.0001730651952129847, Final Batch Loss: 3.925337296095677e-05\n",
      "Epoch 3042, Loss: 0.0002692880252652685, Final Batch Loss: 1.3982283235236537e-05\n",
      "Epoch 3043, Loss: 0.0014344256196636707, Final Batch Loss: 0.00040545451338402927\n",
      "Epoch 3044, Loss: 0.00030868065732647665, Final Batch Loss: 2.4095581466099247e-05\n",
      "Epoch 3045, Loss: 0.007016845513135195, Final Batch Loss: 0.005874914117157459\n",
      "Epoch 3046, Loss: 0.008873091111126996, Final Batch Loss: 9.62305875873426e-06\n",
      "Epoch 3047, Loss: 0.035445305322355125, Final Batch Loss: 0.0004031018470413983\n",
      "Epoch 3048, Loss: 0.004776377463713288, Final Batch Loss: 0.00018353009363636374\n",
      "Epoch 3049, Loss: 0.0009525085042696446, Final Batch Loss: 0.00030217363382689655\n",
      "Epoch 3050, Loss: 0.0011891226031366386, Final Batch Loss: 1.3212834346632008e-05\n",
      "Epoch 3051, Loss: 0.0016629046149319038, Final Batch Loss: 0.00022385794727597386\n",
      "Epoch 3052, Loss: 0.0024257440491055604, Final Batch Loss: 2.1950039808871225e-05\n",
      "Epoch 3053, Loss: 0.005666456832841504, Final Batch Loss: 4.100190562894568e-05\n",
      "Epoch 3054, Loss: 0.0007360366944340058, Final Batch Loss: 7.225012086564675e-05\n",
      "Epoch 3055, Loss: 0.0004164178553764941, Final Batch Loss: 8.69854266056791e-05\n",
      "Epoch 3056, Loss: 0.0005121400354255456, Final Batch Loss: 7.747651397949085e-05\n",
      "Epoch 3057, Loss: 0.03446390839144442, Final Batch Loss: 4.229411388223525e-06\n",
      "Epoch 3058, Loss: 0.000567963103094371, Final Batch Loss: 0.00032810462289489806\n",
      "Epoch 3059, Loss: 0.0008073148128460161, Final Batch Loss: 5.41090194019489e-05\n",
      "Epoch 3060, Loss: 0.0013598603909485973, Final Batch Loss: 0.00020362083159852773\n",
      "Epoch 3061, Loss: 0.0005790592258563265, Final Batch Loss: 8.294646249851212e-05\n",
      "Epoch 3062, Loss: 0.00025541360082570463, Final Batch Loss: 5.910687104915269e-05\n",
      "Epoch 3063, Loss: 0.0005125697316543665, Final Batch Loss: 5.389447687775828e-05\n",
      "Epoch 3064, Loss: 0.0008305655319418292, Final Batch Loss: 0.00025146169355139136\n",
      "Epoch 3065, Loss: 0.001058617388480343, Final Batch Loss: 0.00017736863810569048\n",
      "Epoch 3066, Loss: 0.0006670654947811272, Final Batch Loss: 0.0004381207108963281\n",
      "Epoch 3067, Loss: 0.0006170089909574017, Final Batch Loss: 0.00030280015198513865\n",
      "Epoch 3068, Loss: 0.03704280657984782, Final Batch Loss: 0.00017087186279240996\n",
      "Epoch 3069, Loss: 0.0011561238206923008, Final Batch Loss: 0.00011167793127242476\n",
      "Epoch 3070, Loss: 0.005567701307882089, Final Batch Loss: 0.0051086051389575005\n",
      "Epoch 3071, Loss: 0.000895252735062968, Final Batch Loss: 7.962606468936428e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3072, Loss: 0.0003751681633730186, Final Batch Loss: 0.0001596145739313215\n",
      "Epoch 3073, Loss: 0.00029966259535285644, Final Batch Loss: 3.10576033371035e-05\n",
      "Epoch 3074, Loss: 0.052244465419789776, Final Batch Loss: 9.582202619640157e-05\n",
      "Epoch 3075, Loss: 0.0008456810319330543, Final Batch Loss: 7.266366446856409e-05\n",
      "Epoch 3076, Loss: 0.0007075967005221173, Final Batch Loss: 3.447951530688442e-05\n",
      "Epoch 3077, Loss: 0.0004695697898569051, Final Batch Loss: 2.6972553314408287e-05\n",
      "Epoch 3078, Loss: 0.0008953077795013087, Final Batch Loss: 2.318444057891611e-05\n",
      "Epoch 3079, Loss: 0.0005301438413880533, Final Batch Loss: 3.576099334168248e-05\n",
      "Epoch 3080, Loss: 0.0003396730899112299, Final Batch Loss: 8.718680328456685e-05\n",
      "Epoch 3081, Loss: 0.0006872361700516194, Final Batch Loss: 4.6993474825285375e-05\n",
      "Epoch 3082, Loss: 0.0004250784040777944, Final Batch Loss: 3.544400169630535e-05\n",
      "Epoch 3083, Loss: 0.00041630388477642555, Final Batch Loss: 0.0001944374671438709\n",
      "Epoch 3084, Loss: 0.00027925846916332375, Final Batch Loss: 0.0001517794735264033\n",
      "Epoch 3085, Loss: 0.009218906596288434, Final Batch Loss: 4.816229557036422e-05\n",
      "Epoch 3086, Loss: 0.000974124894128181, Final Batch Loss: 0.00036316507612355053\n",
      "Epoch 3087, Loss: 0.028344699952867813, Final Batch Loss: 0.026974286884069443\n",
      "Epoch 3088, Loss: 0.0042067497270181775, Final Batch Loss: 0.0010879314504563808\n",
      "Epoch 3089, Loss: 0.0007104835021891631, Final Batch Loss: 0.00019845610950142145\n",
      "Epoch 3090, Loss: 0.0010633373331074836, Final Batch Loss: 0.00021501118317246437\n",
      "Epoch 3091, Loss: 0.002577291314537433, Final Batch Loss: 6.354305241984548e-06\n",
      "Epoch 3092, Loss: 0.006736837891367031, Final Batch Loss: 5.288944157655351e-05\n",
      "Epoch 3093, Loss: 0.003293374764325563, Final Batch Loss: 9.556789154885337e-05\n",
      "Epoch 3094, Loss: 0.03384416464905371, Final Batch Loss: 5.260890247882344e-05\n",
      "Epoch 3095, Loss: 0.0014861864401609637, Final Batch Loss: 0.0006629036506637931\n",
      "Epoch 3096, Loss: 0.002095983312756289, Final Batch Loss: 0.00038636429235339165\n",
      "Epoch 3097, Loss: 0.0006569196593773086, Final Batch Loss: 5.5114960559876636e-05\n",
      "Epoch 3098, Loss: 0.0005122252550791018, Final Batch Loss: 0.00012100859748898074\n",
      "Epoch 3099, Loss: 0.0005961723873042502, Final Batch Loss: 0.00017640451551415026\n",
      "Epoch 3100, Loss: 0.0018922776507679373, Final Batch Loss: 5.2236238843761384e-05\n",
      "Epoch 3101, Loss: 0.000981192642939277, Final Batch Loss: 0.000709215528331697\n",
      "Epoch 3102, Loss: 0.0002512229111744091, Final Batch Loss: 7.559520599897951e-05\n",
      "Epoch 3103, Loss: 0.0003806520762736909, Final Batch Loss: 9.87622479442507e-05\n",
      "Epoch 3104, Loss: 0.0008020604291232303, Final Batch Loss: 8.932216587709263e-05\n",
      "Epoch 3105, Loss: 0.003326968100736849, Final Batch Loss: 6.821700662840158e-05\n",
      "Epoch 3106, Loss: 0.0006087493638915475, Final Batch Loss: 5.7591645600041375e-05\n",
      "Epoch 3107, Loss: 0.001051099388860166, Final Batch Loss: 0.00016550456348340958\n",
      "Epoch 3108, Loss: 0.001506174403402838, Final Batch Loss: 1.4091934644966386e-05\n",
      "Epoch 3109, Loss: 0.0012201292556710541, Final Batch Loss: 0.00012973547563888133\n",
      "Epoch 3110, Loss: 0.0026485641865292564, Final Batch Loss: 0.0002487370220478624\n",
      "Epoch 3111, Loss: 0.0009020320430863649, Final Batch Loss: 0.00013451359700411558\n",
      "Epoch 3112, Loss: 0.0006805156081099994, Final Batch Loss: 0.00026395326131023467\n",
      "Epoch 3113, Loss: 0.001443410939828027, Final Batch Loss: 0.001064001931808889\n",
      "Epoch 3114, Loss: 0.01523905323119834, Final Batch Loss: 0.014957571402192116\n",
      "Epoch 3115, Loss: 0.0007389383536064997, Final Batch Loss: 0.00012748608423862606\n",
      "Epoch 3116, Loss: 0.00038766370198572986, Final Batch Loss: 0.00016828416846692562\n",
      "Epoch 3117, Loss: 0.0005697596279787831, Final Batch Loss: 5.484725988935679e-05\n",
      "Epoch 3118, Loss: 0.0002801400751195615, Final Batch Loss: 3.279554584878497e-05\n",
      "Epoch 3119, Loss: 0.005166133589227684, Final Batch Loss: 0.00020206155022606254\n",
      "Epoch 3120, Loss: 0.0018782907500280999, Final Batch Loss: 0.00060011021560058\n",
      "Epoch 3121, Loss: 0.0010557763889664784, Final Batch Loss: 7.04772537574172e-05\n",
      "Epoch 3122, Loss: 0.00047574769268976524, Final Batch Loss: 8.52086886879988e-05\n",
      "Epoch 3123, Loss: 0.0006299769884208217, Final Batch Loss: 0.00017158577975351363\n",
      "Epoch 3124, Loss: 0.050433794502168894, Final Batch Loss: 0.027226798236370087\n",
      "Epoch 3125, Loss: 0.0043792302094516344, Final Batch Loss: 0.0009068555082194507\n",
      "Epoch 3126, Loss: 0.0017548871765029617, Final Batch Loss: 8.529281330993399e-05\n",
      "Epoch 3127, Loss: 0.023284514143597335, Final Batch Loss: 0.0004541878297459334\n",
      "Epoch 3128, Loss: 0.00414080687914975, Final Batch Loss: 0.00018666189862415195\n",
      "Epoch 3129, Loss: 0.0011942284836550243, Final Batch Loss: 0.00041611044434830546\n",
      "Epoch 3130, Loss: 0.006340276879200246, Final Batch Loss: 0.00592085812240839\n",
      "Epoch 3131, Loss: 0.016965343806077726, Final Batch Loss: 0.0005823758547194302\n",
      "Epoch 3132, Loss: 0.006099478130636271, Final Batch Loss: 0.00031554719316773117\n",
      "Epoch 3133, Loss: 0.003994438386143884, Final Batch Loss: 1.564322883496061e-05\n",
      "Epoch 3134, Loss: 0.0012517182767624035, Final Batch Loss: 0.00022110385179985315\n",
      "Epoch 3135, Loss: 0.0026112031482625753, Final Batch Loss: 0.0019844663329422474\n",
      "Epoch 3136, Loss: 0.003681988651806023, Final Batch Loss: 2.5193912733811885e-05\n",
      "Epoch 3137, Loss: 0.0011506233422551304, Final Batch Loss: 0.0006023680325597525\n",
      "Epoch 3138, Loss: 0.0015877749337960267, Final Batch Loss: 0.000255247054155916\n",
      "Epoch 3139, Loss: 0.0003635781067714561, Final Batch Loss: 7.105409895302728e-05\n",
      "Epoch 3140, Loss: 0.0016823281985125504, Final Batch Loss: 9.141945338342339e-05\n",
      "Epoch 3141, Loss: 0.00894633642747067, Final Batch Loss: 0.0005350546562112868\n",
      "Epoch 3142, Loss: 0.01813546740595484, Final Batch Loss: 3.46445849572774e-05\n",
      "Epoch 3143, Loss: 0.007596676434332039, Final Batch Loss: 4.7135072236415e-05\n",
      "Epoch 3144, Loss: 0.0019579537874960806, Final Batch Loss: 4.98474582855124e-05\n",
      "Epoch 3145, Loss: 0.005189959738345351, Final Batch Loss: 6.085497443564236e-05\n",
      "Epoch 3146, Loss: 0.0005727778552682139, Final Batch Loss: 5.338404298527166e-05\n",
      "Epoch 3147, Loss: 0.0004928922789986245, Final Batch Loss: 0.00010972632298944518\n",
      "Epoch 3148, Loss: 0.0010878544562729076, Final Batch Loss: 0.00033206053194589913\n",
      "Epoch 3149, Loss: 0.001468943344661966, Final Batch Loss: 0.0003835285606328398\n",
      "Epoch 3150, Loss: 0.00032628402368573006, Final Batch Loss: 2.3719030650681816e-05\n",
      "Epoch 3151, Loss: 0.0013503479731298285, Final Batch Loss: 1.3685266821994446e-05\n",
      "Epoch 3152, Loss: 0.0011865967680932954, Final Batch Loss: 0.00032695321715436876\n",
      "Epoch 3153, Loss: 0.0015275246987584978, Final Batch Loss: 0.0002460147952660918\n",
      "Epoch 3154, Loss: 0.003683764000015799, Final Batch Loss: 9.975735156331211e-05\n",
      "Epoch 3155, Loss: 0.0042444877417437965, Final Batch Loss: 0.00013121489610057324\n",
      "Epoch 3156, Loss: 0.0009919001204252709, Final Batch Loss: 0.00016588615835644305\n",
      "Epoch 3157, Loss: 0.0009010573994601145, Final Batch Loss: 0.0002124168531736359\n",
      "Epoch 3158, Loss: 0.0002976999112433987, Final Batch Loss: 3.0492034056806006e-05\n",
      "Epoch 3159, Loss: 0.00026966535733663477, Final Batch Loss: 1.035435707308352e-05\n",
      "Epoch 3160, Loss: 0.00047317294047388714, Final Batch Loss: 3.0125394914648496e-05\n",
      "Epoch 3161, Loss: 0.0005888902560400311, Final Batch Loss: 4.4360906031215563e-05\n",
      "Epoch 3162, Loss: 0.009017730597406626, Final Batch Loss: 0.004894791636615992\n",
      "Epoch 3163, Loss: 0.00782663674772266, Final Batch Loss: 7.101871688064421e-06\n",
      "Epoch 3164, Loss: 0.0029648577510670293, Final Batch Loss: 0.0002916821977123618\n",
      "Epoch 3165, Loss: 0.0021324551016732585, Final Batch Loss: 3.641941657406278e-05\n",
      "Epoch 3166, Loss: 0.003266744184657, Final Batch Loss: 0.0023801878560334444\n",
      "Epoch 3167, Loss: 0.0028158019049442373, Final Batch Loss: 0.00010470514098415151\n",
      "Epoch 3168, Loss: 0.0023666999622946605, Final Batch Loss: 0.0018148007802665234\n",
      "Epoch 3169, Loss: 0.0004952349263476208, Final Batch Loss: 6.0706577642122284e-05\n",
      "Epoch 3170, Loss: 0.0008864236297085881, Final Batch Loss: 0.00014468826702795923\n",
      "Epoch 3171, Loss: 0.0009576977422511845, Final Batch Loss: 4.0660847844264936e-06\n",
      "Epoch 3172, Loss: 0.00027692126741385437, Final Batch Loss: 6.644555014645448e-06\n",
      "Epoch 3173, Loss: 0.0005570845678448677, Final Batch Loss: 4.227584940963425e-05\n",
      "Epoch 3174, Loss: 0.0003035504223589669, Final Batch Loss: 9.235244760930073e-06\n",
      "Epoch 3175, Loss: 0.0011740168411051854, Final Batch Loss: 5.05060852447059e-05\n",
      "Epoch 3176, Loss: 0.00026533785603533033, Final Batch Loss: 0.00017846566333901137\n",
      "Epoch 3177, Loss: 0.0004152604860792053, Final Batch Loss: 0.0001512938179075718\n",
      "Epoch 3178, Loss: 0.00039392226972267963, Final Batch Loss: 8.730980334803462e-05\n",
      "Epoch 3179, Loss: 0.0007566248386865482, Final Batch Loss: 0.0002488879254087806\n",
      "Epoch 3180, Loss: 0.0007713492523180321, Final Batch Loss: 5.461560431285761e-05\n",
      "Epoch 3181, Loss: 0.0008065313049883116, Final Batch Loss: 0.0004607615992426872\n",
      "Epoch 3182, Loss: 0.015275400583959708, Final Batch Loss: 1.0153010407520924e-05\n",
      "Epoch 3183, Loss: 0.0013455627647545043, Final Batch Loss: 2.5934366476576542e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3184, Loss: 0.00042797681271622423, Final Batch Loss: 4.621372136170976e-05\n",
      "Epoch 3185, Loss: 0.0006914131154189818, Final Batch Loss: 0.00037764388252981007\n",
      "Epoch 3186, Loss: 0.00013258892386147636, Final Batch Loss: 3.669974103104323e-05\n",
      "Epoch 3187, Loss: 0.00026532591618888546, Final Batch Loss: 0.0001579674135427922\n",
      "Epoch 3188, Loss: 0.00037351932041929103, Final Batch Loss: 3.244728213758208e-05\n",
      "Epoch 3189, Loss: 0.0004834562150790589, Final Batch Loss: 2.507781573513057e-05\n",
      "Epoch 3190, Loss: 0.00028221410957485205, Final Batch Loss: 3.6139870644547045e-05\n",
      "Epoch 3191, Loss: 0.0003803157633228693, Final Batch Loss: 2.4413151550106704e-05\n",
      "Epoch 3192, Loss: 0.00036977476702304557, Final Batch Loss: 4.3468597141327336e-05\n",
      "Epoch 3193, Loss: 0.001143298239185242, Final Batch Loss: 3.0779985536355525e-05\n",
      "Epoch 3194, Loss: 0.000518328437465243, Final Batch Loss: 0.00021229464618954808\n",
      "Epoch 3195, Loss: 0.007113496023521293, Final Batch Loss: 0.0005769164999946952\n",
      "Epoch 3196, Loss: 0.00545475558101316, Final Batch Loss: 3.410832505323924e-05\n",
      "Epoch 3197, Loss: 0.00038031352960388176, Final Batch Loss: 7.785786146996543e-05\n",
      "Epoch 3198, Loss: 0.003142086990919779, Final Batch Loss: 0.00233702571131289\n",
      "Epoch 3199, Loss: 0.0008272176419268362, Final Batch Loss: 0.00010747331543825567\n",
      "Epoch 3200, Loss: 0.008317516290844651, Final Batch Loss: 2.350662907701917e-05\n",
      "Epoch 3201, Loss: 0.008195187117962632, Final Batch Loss: 0.00010226989252259955\n",
      "Epoch 3202, Loss: 0.0008628420400782488, Final Batch Loss: 2.7388909074943513e-05\n",
      "Epoch 3203, Loss: 0.00886731604259694, Final Batch Loss: 3.3821277611423284e-05\n",
      "Epoch 3204, Loss: 0.0012146693770773709, Final Batch Loss: 0.0006208614795468748\n",
      "Epoch 3205, Loss: 0.00028446653777791653, Final Batch Loss: 3.0523424356942996e-05\n",
      "Epoch 3206, Loss: 0.0002272050278406823, Final Batch Loss: 1.3952538211015053e-05\n",
      "Epoch 3207, Loss: 0.00228346542280633, Final Batch Loss: 0.0021191276609897614\n",
      "Epoch 3208, Loss: 0.0002544621947890846, Final Batch Loss: 7.257684046635404e-05\n",
      "Epoch 3209, Loss: 0.00032233681486104615, Final Batch Loss: 3.8922487874515355e-05\n",
      "Epoch 3210, Loss: 0.0002924289574366412, Final Batch Loss: 2.635016971908044e-05\n",
      "Epoch 3211, Loss: 0.00043429357901914045, Final Batch Loss: 1.5719917428214103e-05\n",
      "Epoch 3212, Loss: 0.0278922448751473, Final Batch Loss: 2.2498530597658828e-05\n",
      "Epoch 3213, Loss: 0.009836685352638597, Final Batch Loss: 3.406623727642e-05\n",
      "Epoch 3214, Loss: 0.0010302739101462066, Final Batch Loss: 1.0199000826105475e-05\n",
      "Epoch 3215, Loss: 0.0006600542001251597, Final Batch Loss: 0.00024696352193132043\n",
      "Epoch 3216, Loss: 0.0017129211883002426, Final Batch Loss: 4.291646837373264e-05\n",
      "Epoch 3217, Loss: 0.0007361738753388636, Final Batch Loss: 6.247704732231796e-05\n",
      "Epoch 3218, Loss: 0.0038546209270862164, Final Batch Loss: 1.1232295946683735e-05\n",
      "Epoch 3219, Loss: 0.0009289529552916065, Final Batch Loss: 3.070786260650493e-05\n",
      "Epoch 3220, Loss: 0.0008879481028998271, Final Batch Loss: 0.0002257175074191764\n",
      "Epoch 3221, Loss: 0.002056451987300534, Final Batch Loss: 0.0015506437048316002\n",
      "Epoch 3222, Loss: 0.00014088452462601708, Final Batch Loss: 7.056495087454095e-05\n",
      "Epoch 3223, Loss: 0.00102455893647857, Final Batch Loss: 8.254290150944144e-05\n",
      "Epoch 3224, Loss: 0.0013105094512866344, Final Batch Loss: 4.599796739057638e-05\n",
      "Epoch 3225, Loss: 0.0007986276286828797, Final Batch Loss: 4.390612230054103e-05\n",
      "Epoch 3226, Loss: 0.011997161869658157, Final Batch Loss: 0.0034275664947927\n",
      "Epoch 3227, Loss: 0.00044878164771944284, Final Batch Loss: 1.575288115418516e-05\n",
      "Epoch 3228, Loss: 0.02021377247365308, Final Batch Loss: 4.225654629408382e-05\n",
      "Epoch 3229, Loss: 0.0008144673270180647, Final Batch Loss: 2.058412519545527e-06\n",
      "Epoch 3230, Loss: 0.003185125017807877, Final Batch Loss: 0.0007358047878369689\n",
      "Epoch 3231, Loss: 0.0020063960109837353, Final Batch Loss: 0.00014716068108100444\n",
      "Epoch 3232, Loss: 0.00594485519832233, Final Batch Loss: 6.388777546817437e-05\n",
      "Epoch 3233, Loss: 0.0014952644596633036, Final Batch Loss: 1.7945058061741292e-05\n",
      "Epoch 3234, Loss: 0.0020441475389816333, Final Batch Loss: 0.0012069629738107324\n",
      "Epoch 3235, Loss: 0.0012951866665389389, Final Batch Loss: 0.0009454856044612825\n",
      "Epoch 3236, Loss: 0.0009817809586820658, Final Batch Loss: 0.00010188726446358487\n",
      "Epoch 3237, Loss: 0.0005697109099855879, Final Batch Loss: 3.759277751669288e-05\n",
      "Epoch 3238, Loss: 0.0009878551500150934, Final Batch Loss: 9.140987822320312e-05\n",
      "Epoch 3239, Loss: 0.010937912702502217, Final Batch Loss: 0.00019457831513136625\n",
      "Epoch 3240, Loss: 0.001407238480169326, Final Batch Loss: 0.0012899363646283746\n",
      "Epoch 3241, Loss: 0.001426559825631557, Final Batch Loss: 1.5428446204168722e-05\n",
      "Epoch 3242, Loss: 0.003425313656407525, Final Batch Loss: 2.0607587430276908e-05\n",
      "Epoch 3243, Loss: 0.00016320901340804994, Final Batch Loss: 5.440762106445618e-05\n",
      "Epoch 3244, Loss: 0.0002480374059814494, Final Batch Loss: 0.0001334803382633254\n",
      "Epoch 3245, Loss: 0.00027052310815633973, Final Batch Loss: 1.4258690498536453e-05\n",
      "Epoch 3246, Loss: 0.002230745682027191, Final Batch Loss: 3.513366391416639e-05\n",
      "Epoch 3247, Loss: 0.0011880897609444219, Final Batch Loss: 1.3553580174630042e-05\n",
      "Epoch 3248, Loss: 0.00022740762324247044, Final Batch Loss: 1.388553027936723e-05\n",
      "Epoch 3249, Loss: 0.0004960982332704589, Final Batch Loss: 5.6709734053583816e-05\n",
      "Epoch 3250, Loss: 0.00035355713589524385, Final Batch Loss: 4.95356616738718e-05\n",
      "Epoch 3251, Loss: 0.0001206737479151343, Final Batch Loss: 1.3734780623053666e-05\n",
      "Epoch 3252, Loss: 0.0005126260075485334, Final Batch Loss: 3.118887252639979e-05\n",
      "Epoch 3253, Loss: 0.0007552909264632035, Final Batch Loss: 2.416626739432104e-05\n",
      "Epoch 3254, Loss: 0.001161450736617553, Final Batch Loss: 2.4676204702700488e-05\n",
      "Epoch 3255, Loss: 0.00013653534188051708, Final Batch Loss: 2.841055356839206e-05\n",
      "Epoch 3256, Loss: 0.0002867686798708746, Final Batch Loss: 5.028912710258737e-06\n",
      "Epoch 3257, Loss: 0.0005590047367149964, Final Batch Loss: 0.00016705533198546618\n",
      "Epoch 3258, Loss: 0.0039991305384319276, Final Batch Loss: 0.00033017422538250685\n",
      "Epoch 3259, Loss: 0.0009454022292629816, Final Batch Loss: 0.000255577324423939\n",
      "Epoch 3260, Loss: 0.001081564796550083, Final Batch Loss: 0.00011282923514954746\n",
      "Epoch 3261, Loss: 0.00040413678652839735, Final Batch Loss: 0.0002656342985574156\n",
      "Epoch 3262, Loss: 0.000996462236798834, Final Batch Loss: 7.078525959514081e-05\n",
      "Epoch 3263, Loss: 0.00022889362571731908, Final Batch Loss: 4.4797267037210986e-05\n",
      "Epoch 3264, Loss: 0.0010203354868281167, Final Batch Loss: 0.0006812694482505322\n",
      "Epoch 3265, Loss: 0.047284234409744386, Final Batch Loss: 0.047052349895238876\n",
      "Epoch 3266, Loss: 0.0008620385196991265, Final Batch Loss: 0.00019003680790774524\n",
      "Epoch 3267, Loss: 0.0019086381544184405, Final Batch Loss: 0.0005629296647384763\n",
      "Epoch 3268, Loss: 0.00807339457605849, Final Batch Loss: 0.0001404333015671\n",
      "Epoch 3269, Loss: 0.00030256079662649427, Final Batch Loss: 5.418265573098324e-06\n",
      "Epoch 3270, Loss: 0.0003208799407730112, Final Batch Loss: 2.1710606233682483e-05\n",
      "Epoch 3271, Loss: 0.004577345785946818, Final Batch Loss: 6.336825754260644e-05\n",
      "Epoch 3272, Loss: 0.016973309073364362, Final Batch Loss: 0.0018030955689027905\n",
      "Epoch 3273, Loss: 0.01036738746552146, Final Batch Loss: 0.010119648650288582\n",
      "Epoch 3274, Loss: 0.0005303543259742582, Final Batch Loss: 2.8262331852602074e-06\n",
      "Epoch 3275, Loss: 0.023224553689942695, Final Batch Loss: 0.022210340946912766\n",
      "Epoch 3276, Loss: 0.0012136590157751925, Final Batch Loss: 0.0001141112734330818\n",
      "Epoch 3277, Loss: 0.0017054774762073066, Final Batch Loss: 0.00011547793837962672\n",
      "Epoch 3278, Loss: 0.00018660162277228665, Final Batch Loss: 1.6227511878241785e-05\n",
      "Epoch 3279, Loss: 0.000611304902122356, Final Batch Loss: 0.00036814913619309664\n",
      "Epoch 3280, Loss: 0.003263720005634241, Final Batch Loss: 0.0005178864230401814\n",
      "Epoch 3281, Loss: 0.0010281171216774965, Final Batch Loss: 4.7302921302616596e-05\n",
      "Epoch 3282, Loss: 0.0003884006291627884, Final Batch Loss: 7.594077760586515e-05\n",
      "Epoch 3283, Loss: 0.008131081493047532, Final Batch Loss: 6.717488577123731e-05\n",
      "Epoch 3284, Loss: 0.0012830660825784435, Final Batch Loss: 1.448995044484036e-05\n",
      "Epoch 3285, Loss: 0.00022395429186872207, Final Batch Loss: 2.284798028995283e-05\n",
      "Epoch 3286, Loss: 0.005587363935774192, Final Batch Loss: 0.00048791978042572737\n",
      "Epoch 3287, Loss: 0.00036970375731470995, Final Batch Loss: 0.00017209519864991307\n",
      "Epoch 3288, Loss: 0.0009735570201883093, Final Batch Loss: 0.0001319023285759613\n",
      "Epoch 3289, Loss: 0.0008641459189675516, Final Batch Loss: 1.638586945773568e-05\n",
      "Epoch 3290, Loss: 0.0003300793905509636, Final Batch Loss: 2.4406683223787695e-05\n",
      "Epoch 3291, Loss: 0.0008299712608277332, Final Batch Loss: 0.000302145432215184\n",
      "Epoch 3292, Loss: 0.00038788392885180656, Final Batch Loss: 1.9861588953062892e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3293, Loss: 0.0006059570077923127, Final Batch Loss: 3.974645369453356e-05\n",
      "Epoch 3294, Loss: 0.0003398166809347458, Final Batch Loss: 2.172209497075528e-05\n",
      "Epoch 3295, Loss: 0.00574697327465401, Final Batch Loss: 0.00012153585703345016\n",
      "Epoch 3296, Loss: 0.0006999876532063354, Final Batch Loss: 0.0003133522695861757\n",
      "Epoch 3297, Loss: 0.00038957626748015173, Final Batch Loss: 1.576072463649325e-05\n",
      "Epoch 3298, Loss: 0.0005449164746096358, Final Batch Loss: 7.753564568702132e-05\n",
      "Epoch 3299, Loss: 0.006909464573254809, Final Batch Loss: 0.0002581229491624981\n",
      "Epoch 3300, Loss: 0.000250524371949723, Final Batch Loss: 3.733178891707212e-05\n",
      "Epoch 3301, Loss: 0.00048589207654003985, Final Batch Loss: 1.326182609773241e-05\n",
      "Epoch 3302, Loss: 0.0003354422906340915, Final Batch Loss: 6.572632992174476e-05\n",
      "Epoch 3303, Loss: 0.014303043513791636, Final Batch Loss: 6.663669410045259e-06\n",
      "Epoch 3304, Loss: 0.000584953941142885, Final Batch Loss: 1.5379322576336563e-05\n",
      "Epoch 3305, Loss: 0.02705905499169603, Final Batch Loss: 0.00032805808586999774\n",
      "Epoch 3306, Loss: 0.0005214454868109897, Final Batch Loss: 5.180565858609043e-05\n",
      "Epoch 3307, Loss: 0.00038516379754582886, Final Batch Loss: 0.0001036892062984407\n",
      "Epoch 3308, Loss: 0.004359661948910798, Final Batch Loss: 2.115082497766707e-05\n",
      "Epoch 3309, Loss: 0.0002088549563268316, Final Batch Loss: 1.4002042917127255e-05\n",
      "Epoch 3310, Loss: 0.0013555419973272365, Final Batch Loss: 0.0006869699573144317\n",
      "Epoch 3311, Loss: 0.0018874532524932874, Final Batch Loss: 7.753091267659329e-06\n",
      "Epoch 3312, Loss: 0.00935730181299732, Final Batch Loss: 0.0003078438458032906\n",
      "Epoch 3313, Loss: 0.00043837349039677065, Final Batch Loss: 4.1661023715278134e-05\n",
      "Epoch 3314, Loss: 0.0005594181639025919, Final Batch Loss: 0.00017405749531462789\n",
      "Epoch 3315, Loss: 0.0007885040276960353, Final Batch Loss: 0.00023370679991785437\n",
      "Epoch 3316, Loss: 0.001192544325022027, Final Batch Loss: 0.0001109690492739901\n",
      "Epoch 3317, Loss: 0.0009655491230660118, Final Batch Loss: 7.611084583913907e-05\n",
      "Epoch 3318, Loss: 0.0036399211685420596, Final Batch Loss: 1.5043552593851928e-05\n",
      "Epoch 3319, Loss: 0.0002857317240341217, Final Batch Loss: 1.2698049431492109e-05\n",
      "Epoch 3320, Loss: 0.000523354781762464, Final Batch Loss: 4.792413892573677e-05\n",
      "Epoch 3321, Loss: 0.002821436205522332, Final Batch Loss: 0.002450129482895136\n",
      "Epoch 3322, Loss: 0.0014193586885085097, Final Batch Loss: 1.3122122254571877e-05\n",
      "Epoch 3323, Loss: 0.006547562108607963, Final Batch Loss: 4.316207196097821e-05\n",
      "Epoch 3324, Loss: 0.00046568558991566533, Final Batch Loss: 4.589974560076371e-05\n",
      "Epoch 3325, Loss: 0.00013507126459444407, Final Batch Loss: 1.8083321265294217e-05\n",
      "Epoch 3326, Loss: 0.0008769914347794838, Final Batch Loss: 3.9482252759626135e-05\n",
      "Epoch 3327, Loss: 0.0005516902092495002, Final Batch Loss: 9.296419739257544e-05\n",
      "Epoch 3328, Loss: 0.0002730279629759025, Final Batch Loss: 5.217360012466088e-05\n",
      "Epoch 3329, Loss: 0.0002606105281302007, Final Batch Loss: 2.8433127226890065e-05\n",
      "Epoch 3330, Loss: 0.00012961892934981734, Final Batch Loss: 2.142858284059912e-05\n",
      "Epoch 3331, Loss: 0.0012430603219399927, Final Batch Loss: 0.00013078573101665825\n",
      "Epoch 3332, Loss: 0.0011908700471394695, Final Batch Loss: 5.6540899095125496e-05\n",
      "Epoch 3333, Loss: 0.0010103114291268867, Final Batch Loss: 3.161890708724968e-05\n",
      "Epoch 3334, Loss: 0.004106272972421721, Final Batch Loss: 2.0072198822163045e-05\n",
      "Epoch 3335, Loss: 0.0011479370223241858, Final Batch Loss: 0.0003241875092498958\n",
      "Epoch 3336, Loss: 0.0019004694877367, Final Batch Loss: 0.0003115953295491636\n",
      "Epoch 3337, Loss: 0.004434163623955101, Final Batch Loss: 0.00021188936079852283\n",
      "Epoch 3338, Loss: 0.0003907405989593826, Final Batch Loss: 0.00012670224532485008\n",
      "Epoch 3339, Loss: 0.0001718869434625958, Final Batch Loss: 5.129840792506002e-05\n",
      "Epoch 3340, Loss: 0.0008428665832980187, Final Batch Loss: 1.0378867045801599e-05\n",
      "Epoch 3341, Loss: 0.0005778003760497086, Final Batch Loss: 6.324501009657979e-05\n",
      "Epoch 3342, Loss: 0.0019071561200689757, Final Batch Loss: 4.704851016867906e-05\n",
      "Epoch 3343, Loss: 0.0008466435483569512, Final Batch Loss: 1.2822909411625005e-05\n",
      "Epoch 3344, Loss: 0.00020789327300008154, Final Batch Loss: 9.797252459975425e-06\n",
      "Epoch 3345, Loss: 0.0010292529259459116, Final Batch Loss: 0.00018374979845248163\n",
      "Epoch 3346, Loss: 0.0001771024803929322, Final Batch Loss: 3.1311469683714677e-06\n",
      "Epoch 3347, Loss: 0.007134498789127974, Final Batch Loss: 0.006710600107908249\n",
      "Epoch 3348, Loss: 0.001198157163344149, Final Batch Loss: 0.0001428230170859024\n",
      "Epoch 3349, Loss: 0.0007288011784112314, Final Batch Loss: 0.00061327766161412\n",
      "Epoch 3350, Loss: 0.0012945483194926055, Final Batch Loss: 0.00048085246817208827\n",
      "Epoch 3351, Loss: 0.00043536203520488925, Final Batch Loss: 1.093089122150559e-05\n",
      "Epoch 3352, Loss: 0.0009121391844928439, Final Batch Loss: 6.105984084570082e-06\n",
      "Epoch 3353, Loss: 0.007103735798409616, Final Batch Loss: 3.990938239439856e-06\n",
      "Epoch 3354, Loss: 0.00012059624805260682, Final Batch Loss: 8.00627312855795e-05\n",
      "Epoch 3355, Loss: 0.0011579864176383126, Final Batch Loss: 1.7782364011509344e-05\n",
      "Epoch 3356, Loss: 0.012608018218088546, Final Batch Loss: 2.967502769024577e-05\n",
      "Epoch 3357, Loss: 0.00051542201799748, Final Batch Loss: 8.55370490171481e-06\n",
      "Epoch 3358, Loss: 0.007670972669075127, Final Batch Loss: 0.007108911871910095\n",
      "Epoch 3359, Loss: 0.003152527497150004, Final Batch Loss: 0.00145256076939404\n",
      "Epoch 3360, Loss: 0.00028627339543163544, Final Batch Loss: 5.797991980216466e-05\n",
      "Epoch 3361, Loss: 0.0015871176692598965, Final Batch Loss: 2.270877121191006e-05\n",
      "Epoch 3362, Loss: 0.00022447111950896215, Final Batch Loss: 2.898756974900607e-05\n",
      "Epoch 3363, Loss: 0.03405104701050732, Final Batch Loss: 0.03324589505791664\n",
      "Epoch 3364, Loss: 0.006141359630419174, Final Batch Loss: 7.22689219401218e-06\n",
      "Epoch 3365, Loss: 0.14293182940309634, Final Batch Loss: 0.04034078121185303\n",
      "Epoch 3366, Loss: 0.0037104100756550906, Final Batch Loss: 0.00010085066605824977\n",
      "Epoch 3367, Loss: 0.024296360730659217, Final Batch Loss: 0.00017781642964109778\n",
      "Epoch 3368, Loss: 0.07043492968659848, Final Batch Loss: 0.001759671838954091\n",
      "Epoch 3369, Loss: 0.0009582504571881145, Final Batch Loss: 0.00019124947721138597\n",
      "Epoch 3370, Loss: 0.04850691023602849, Final Batch Loss: 0.04087065905332565\n",
      "Epoch 3371, Loss: 0.001724822839605622, Final Batch Loss: 0.0001941833907039836\n",
      "Epoch 3372, Loss: 0.004065571789396927, Final Batch Loss: 0.002526648109778762\n",
      "Epoch 3373, Loss: 0.0011909661843674257, Final Batch Loss: 0.0004514337342698127\n",
      "Epoch 3374, Loss: 0.009337148017948493, Final Batch Loss: 0.0004438714240677655\n",
      "Epoch 3375, Loss: 0.0017617032463022042, Final Batch Loss: 0.0006362244603224099\n",
      "Epoch 3376, Loss: 0.02604662640078459, Final Batch Loss: 0.00015909808280412108\n",
      "Epoch 3377, Loss: 0.01776090598286828, Final Batch Loss: 8.542645809939131e-05\n",
      "Epoch 3378, Loss: 0.0022799501166446134, Final Batch Loss: 0.00023516024521086365\n",
      "Epoch 3379, Loss: 0.03698209402500652, Final Batch Loss: 0.034327566623687744\n",
      "Epoch 3380, Loss: 0.0024195471341954544, Final Batch Loss: 0.00014987059694249183\n",
      "Epoch 3381, Loss: 0.00749487905704882, Final Batch Loss: 0.0006793770007789135\n",
      "Epoch 3382, Loss: 0.007721512520220131, Final Batch Loss: 0.004229940939694643\n",
      "Epoch 3383, Loss: 0.00765248484822223, Final Batch Loss: 0.0005350600695237517\n",
      "Epoch 3384, Loss: 0.017363474420562852, Final Batch Loss: 7.146030111471191e-05\n",
      "Epoch 3385, Loss: 0.038064329128246754, Final Batch Loss: 0.0008056553197093308\n",
      "Epoch 3386, Loss: 0.004714675975264981, Final Batch Loss: 0.001267199986614287\n",
      "Epoch 3387, Loss: 0.004773298125655856, Final Batch Loss: 0.0003218738711439073\n",
      "Epoch 3388, Loss: 0.010643186164088547, Final Batch Loss: 2.0324805518612266e-05\n",
      "Epoch 3389, Loss: 0.02157379817799665, Final Batch Loss: 8.640598389320076e-05\n",
      "Epoch 3390, Loss: 0.007237322017317638, Final Batch Loss: 0.00011296034062979743\n",
      "Epoch 3391, Loss: 0.002733904213528149, Final Batch Loss: 0.0016786782070994377\n",
      "Epoch 3392, Loss: 0.002315249541425146, Final Batch Loss: 0.00017797738837543875\n",
      "Epoch 3393, Loss: 0.0018828032916644588, Final Batch Loss: 8.778773189987987e-05\n",
      "Epoch 3394, Loss: 0.0022251814771152567, Final Batch Loss: 5.303838042891584e-05\n",
      "Epoch 3395, Loss: 0.003122367837931961, Final Batch Loss: 0.000140608346555382\n",
      "Epoch 3396, Loss: 0.0014342992362799123, Final Batch Loss: 0.00028751068748533726\n",
      "Epoch 3397, Loss: 0.0017633272836974356, Final Batch Loss: 4.967951463186182e-05\n",
      "Epoch 3398, Loss: 0.0019481362724036444, Final Batch Loss: 5.2300008974270895e-05\n",
      "Epoch 3399, Loss: 0.024953678039310034, Final Batch Loss: 0.00010937960905721411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3400, Loss: 0.0012399368206388317, Final Batch Loss: 0.00013792210665997118\n",
      "Epoch 3401, Loss: 0.0015287354835891165, Final Batch Loss: 9.459568536840379e-05\n",
      "Epoch 3402, Loss: 0.0015504070033784956, Final Batch Loss: 0.0001252552610822022\n",
      "Epoch 3403, Loss: 0.0035463331296341494, Final Batch Loss: 0.00022809745860286057\n",
      "Epoch 3404, Loss: 0.002786283934256062, Final Batch Loss: 0.0006372308125719428\n",
      "Epoch 3405, Loss: 0.0011731639606296085, Final Batch Loss: 0.0007114937761798501\n",
      "Epoch 3406, Loss: 0.003589679574361071, Final Batch Loss: 0.00014390869182534516\n",
      "Epoch 3407, Loss: 0.0007204679932328872, Final Batch Loss: 0.00018753999029286206\n",
      "Epoch 3408, Loss: 0.0030904788727639243, Final Batch Loss: 0.0002123938756994903\n",
      "Epoch 3409, Loss: 0.0004965245716448408, Final Batch Loss: 3.06318215734791e-05\n",
      "Epoch 3410, Loss: 0.0014655796003353316, Final Batch Loss: 0.00024666415993124247\n",
      "Epoch 3411, Loss: 0.0009105902463488746, Final Batch Loss: 4.1312083340017125e-05\n",
      "Epoch 3412, Loss: 0.0026306656800443307, Final Batch Loss: 3.82281286874786e-05\n",
      "Epoch 3413, Loss: 0.0013884880318073556, Final Batch Loss: 0.0004761703312397003\n",
      "Epoch 3414, Loss: 0.002194522381614661, Final Batch Loss: 4.49860563094262e-05\n",
      "Epoch 3415, Loss: 0.007067771788570099, Final Batch Loss: 0.00010732931696111336\n",
      "Epoch 3416, Loss: 0.0013366171333473176, Final Batch Loss: 5.257397424429655e-05\n",
      "Epoch 3417, Loss: 0.0027924376699957065, Final Batch Loss: 0.00012026957847410813\n",
      "Epoch 3418, Loss: 0.0015582231389998924, Final Batch Loss: 0.00015799295215401798\n",
      "Epoch 3419, Loss: 0.003468818285909947, Final Batch Loss: 0.0009778401581570506\n",
      "Epoch 3420, Loss: 0.0015025294342194684, Final Batch Loss: 6.332610064418986e-05\n",
      "Epoch 3421, Loss: 0.0011866829809150659, Final Batch Loss: 0.000801403890363872\n",
      "Epoch 3422, Loss: 0.001967009920917917, Final Batch Loss: 0.0007521898951381445\n",
      "Epoch 3423, Loss: 0.0019215194188291207, Final Batch Loss: 0.00015503719623666257\n",
      "Epoch 3424, Loss: 0.0005470925698318752, Final Batch Loss: 4.3309908505761996e-05\n",
      "Epoch 3425, Loss: 0.0008617219082225347, Final Batch Loss: 2.6383915610495023e-05\n",
      "Epoch 3426, Loss: 0.0029054816495772684, Final Batch Loss: 2.6594463633955456e-05\n",
      "Epoch 3427, Loss: 0.0005134347320563393, Final Batch Loss: 5.336080357665196e-05\n",
      "Epoch 3428, Loss: 0.0035947399301221594, Final Batch Loss: 0.00012486432387959212\n",
      "Epoch 3429, Loss: 0.015294373188226018, Final Batch Loss: 3.259001096012071e-05\n",
      "Epoch 3430, Loss: 0.001187071957247099, Final Batch Loss: 1.9669347238959745e-05\n",
      "Epoch 3431, Loss: 0.002862770442334295, Final Batch Loss: 1.1326542335154954e-05\n",
      "Epoch 3432, Loss: 0.010890076009673066, Final Batch Loss: 0.000985813676379621\n",
      "Epoch 3433, Loss: 0.0006022557918186067, Final Batch Loss: 1.3930943168816157e-05\n",
      "Epoch 3434, Loss: 0.0008921404732973315, Final Batch Loss: 0.00010416241275379434\n",
      "Epoch 3435, Loss: 0.0022257604759943206, Final Batch Loss: 2.6235637051286176e-05\n",
      "Epoch 3436, Loss: 0.0035462363121041562, Final Batch Loss: 0.0029235132969915867\n",
      "Epoch 3437, Loss: 0.0016722424261388369, Final Batch Loss: 0.0012189126573503017\n",
      "Epoch 3438, Loss: 0.0004832684608118143, Final Batch Loss: 9.59570606937632e-05\n",
      "Epoch 3439, Loss: 0.003541861442499794, Final Batch Loss: 0.00036761938827112317\n",
      "Epoch 3440, Loss: 0.0003148036375932861, Final Batch Loss: 3.7019115552539006e-05\n",
      "Epoch 3441, Loss: 0.0004246367661835393, Final Batch Loss: 0.00021108404325786978\n",
      "Epoch 3442, Loss: 0.01701128138893182, Final Batch Loss: 6.69719383949996e-06\n",
      "Epoch 3443, Loss: 0.0007896861498011276, Final Batch Loss: 0.0001185036962851882\n",
      "Epoch 3444, Loss: 0.00047438263936783187, Final Batch Loss: 0.00034590953146107495\n",
      "Epoch 3445, Loss: 0.000607504065555986, Final Batch Loss: 0.00020326559024397284\n",
      "Epoch 3446, Loss: 0.0008135426032822579, Final Batch Loss: 0.00013158640649635345\n",
      "Epoch 3447, Loss: 0.0010219637624686584, Final Batch Loss: 0.00040223528048954904\n",
      "Epoch 3448, Loss: 0.002123831720382441, Final Batch Loss: 0.000353100651409477\n",
      "Epoch 3449, Loss: 0.0004041718111693626, Final Batch Loss: 9.539206803310663e-05\n",
      "Epoch 3450, Loss: 0.0021328817820176482, Final Batch Loss: 5.321991920936853e-05\n",
      "Epoch 3451, Loss: 0.006778488925192505, Final Batch Loss: 0.003584363264963031\n",
      "Epoch 3452, Loss: 0.0006879186730657239, Final Batch Loss: 4.764808181789704e-05\n",
      "Epoch 3453, Loss: 0.0021623060238198377, Final Batch Loss: 0.000546760915312916\n",
      "Epoch 3454, Loss: 0.0006284943810896948, Final Batch Loss: 0.00011046472354792058\n",
      "Epoch 3455, Loss: 0.002249207012937404, Final Batch Loss: 5.750509080826305e-05\n",
      "Epoch 3456, Loss: 0.000641797945718281, Final Batch Loss: 0.00032818494946695864\n",
      "Epoch 3457, Loss: 0.017473391599196475, Final Batch Loss: 3.173373261233792e-05\n",
      "Epoch 3458, Loss: 0.0009129507525358349, Final Batch Loss: 0.00030456995591521263\n",
      "Epoch 3459, Loss: 0.0006025893599144183, Final Batch Loss: 0.00023695541312918067\n",
      "Epoch 3460, Loss: 0.000856456215842627, Final Batch Loss: 0.00011312877177260816\n",
      "Epoch 3461, Loss: 0.0033953500460484065, Final Batch Loss: 0.001782476669177413\n",
      "Epoch 3462, Loss: 0.0034672861074795946, Final Batch Loss: 0.001073926337994635\n",
      "Epoch 3463, Loss: 0.0004907771772195701, Final Batch Loss: 0.00014355195162352175\n",
      "Epoch 3464, Loss: 0.0015998746930563357, Final Batch Loss: 0.0013990746811032295\n",
      "Epoch 3465, Loss: 0.00027066887696491904, Final Batch Loss: 7.598454885737738e-06\n",
      "Epoch 3466, Loss: 0.009717519214973436, Final Batch Loss: 0.0020816922187805176\n",
      "Epoch 3467, Loss: 0.00017372730235365452, Final Batch Loss: 1.4314114196167793e-05\n",
      "Epoch 3468, Loss: 0.0003518855792208342, Final Batch Loss: 6.454344838857651e-05\n",
      "Epoch 3469, Loss: 0.0013786284434900153, Final Batch Loss: 1.230216003023088e-05\n",
      "Epoch 3470, Loss: 0.0005458880150399636, Final Batch Loss: 7.801707397447899e-05\n",
      "Epoch 3471, Loss: 0.0043023914186051115, Final Batch Loss: 5.643662007059902e-06\n",
      "Epoch 3472, Loss: 0.00049208312339033, Final Batch Loss: 1.530246481706854e-05\n",
      "Epoch 3473, Loss: 0.0018453848388162442, Final Batch Loss: 0.000688620435539633\n",
      "Epoch 3474, Loss: 0.0016458195023005828, Final Batch Loss: 0.000735714565962553\n",
      "Epoch 3475, Loss: 0.0006580131866940064, Final Batch Loss: 1.24955167848384e-05\n",
      "Epoch 3476, Loss: 0.0009457534688408487, Final Batch Loss: 0.0003009496140293777\n",
      "Epoch 3477, Loss: 0.0005910112304263748, Final Batch Loss: 0.00016707985196262598\n",
      "Epoch 3478, Loss: 0.0004583024710882455, Final Batch Loss: 0.00010886328527703881\n",
      "Epoch 3479, Loss: 0.0008852163882693276, Final Batch Loss: 9.819128172239289e-05\n",
      "Epoch 3480, Loss: 0.0016379871449316852, Final Batch Loss: 0.001341458410024643\n",
      "Epoch 3481, Loss: 0.0021387444176070858, Final Batch Loss: 0.0019014133140444756\n",
      "Epoch 3482, Loss: 0.0002476398894941667, Final Batch Loss: 8.311303099617362e-05\n",
      "Epoch 3483, Loss: 0.001360321574338741, Final Batch Loss: 3.15738761855755e-05\n",
      "Epoch 3484, Loss: 0.00010222462697129231, Final Batch Loss: 1.0519994248170406e-05\n",
      "Epoch 3485, Loss: 0.00020055884124303702, Final Batch Loss: 2.4892886358429678e-05\n",
      "Epoch 3486, Loss: 0.0002915233308158349, Final Batch Loss: 3.376324457349256e-05\n",
      "Epoch 3487, Loss: 0.0003264773968112422, Final Batch Loss: 4.153680492890999e-05\n",
      "Epoch 3488, Loss: 0.00036300389183452353, Final Batch Loss: 4.7306901251431555e-05\n",
      "Epoch 3489, Loss: 0.0006160439497762127, Final Batch Loss: 6.544963252963498e-05\n",
      "Epoch 3490, Loss: 0.0003922459145542234, Final Batch Loss: 0.0002924257714767009\n",
      "Epoch 3491, Loss: 0.001106428340790444, Final Batch Loss: 2.485696313669905e-05\n",
      "Epoch 3492, Loss: 0.004448071253136732, Final Batch Loss: 6.308164302026853e-05\n",
      "Epoch 3493, Loss: 0.0002851641438610386, Final Batch Loss: 8.310606062877923e-05\n",
      "Epoch 3494, Loss: 0.0004895052443316672, Final Batch Loss: 8.463217091048136e-05\n",
      "Epoch 3495, Loss: 0.0002176524285459891, Final Batch Loss: 7.436478335876018e-05\n",
      "Epoch 3496, Loss: 0.00020254221544746542, Final Batch Loss: 8.754921145737171e-05\n",
      "Epoch 3497, Loss: 0.0002102722000927315, Final Batch Loss: 0.00017007302085403353\n",
      "Epoch 3498, Loss: 0.0004681129885284463, Final Batch Loss: 8.563248229620513e-06\n",
      "Epoch 3499, Loss: 0.0005423464072009665, Final Batch Loss: 1.3849225979356561e-05\n",
      "Epoch 3500, Loss: 0.00017859373656392563, Final Batch Loss: 7.806362555129454e-05\n",
      "Epoch 3501, Loss: 0.0008867584929248551, Final Batch Loss: 8.35072023619432e-06\n",
      "Epoch 3502, Loss: 0.0002449247376716812, Final Batch Loss: 1.0606354408082552e-06\n",
      "Epoch 3503, Loss: 0.0003600226991693489, Final Batch Loss: 0.00026800684281624854\n",
      "Epoch 3504, Loss: 0.017800354897190118, Final Batch Loss: 0.0002732182911131531\n",
      "Epoch 3505, Loss: 0.0002226456690550549, Final Batch Loss: 3.28072055708617e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3506, Loss: 0.00010335676188333309, Final Batch Loss: 7.1572535489394795e-06\n",
      "Epoch 3507, Loss: 0.0012239343254805135, Final Batch Loss: 3.694417046062881e-06\n",
      "Epoch 3508, Loss: 0.00013629662407765863, Final Batch Loss: 5.8943616750184447e-05\n",
      "Epoch 3509, Loss: 0.023377249322948046, Final Batch Loss: 0.000133088935399428\n",
      "Epoch 3510, Loss: 0.0002677023730939254, Final Batch Loss: 1.5692658053012565e-05\n",
      "Epoch 3511, Loss: 0.0006966657147131627, Final Batch Loss: 2.117285293934401e-05\n",
      "Epoch 3512, Loss: 0.004701966374341282, Final Batch Loss: 2.9894732506363653e-05\n",
      "Epoch 3513, Loss: 0.002692865606150008, Final Batch Loss: 9.100879105972126e-05\n",
      "Epoch 3514, Loss: 0.00041214234204289824, Final Batch Loss: 9.714980251374072e-07\n",
      "Epoch 3515, Loss: 0.00029894549516029656, Final Batch Loss: 5.461122054839507e-05\n",
      "Epoch 3516, Loss: 0.00039573672802362125, Final Batch Loss: 1.4215815099305473e-05\n",
      "Epoch 3517, Loss: 0.00030156128741509747, Final Batch Loss: 0.00017531239427626133\n",
      "Epoch 3518, Loss: 0.00031286977537092753, Final Batch Loss: 0.0001199781327159144\n",
      "Epoch 3519, Loss: 0.00024126950120262336, Final Batch Loss: 2.7251066057942808e-05\n",
      "Epoch 3520, Loss: 0.0013745679216299322, Final Batch Loss: 8.634287223685533e-05\n",
      "Epoch 3521, Loss: 0.0009060108668563771, Final Batch Loss: 1.388504733768059e-05\n",
      "Epoch 3522, Loss: 0.00028928861229360336, Final Batch Loss: 8.812613486952614e-06\n",
      "Epoch 3523, Loss: 0.00029277479188749567, Final Batch Loss: 3.932135223294608e-05\n",
      "Epoch 3524, Loss: 0.00010660178122634534, Final Batch Loss: 3.398914122954011e-05\n",
      "Epoch 3525, Loss: 0.00043848150426128996, Final Batch Loss: 4.507732683123322e-06\n",
      "Epoch 3526, Loss: 0.0003036492362298304, Final Batch Loss: 1.8536249626777135e-05\n",
      "Epoch 3527, Loss: 0.00018744811313808896, Final Batch Loss: 5.7358025514986366e-05\n",
      "Epoch 3528, Loss: 0.0002496268480172148, Final Batch Loss: 7.422938506351784e-05\n",
      "Epoch 3529, Loss: 0.0010214151297986973, Final Batch Loss: 4.863696449319832e-05\n",
      "Epoch 3530, Loss: 0.0003319042043585796, Final Batch Loss: 0.00011846452252939343\n",
      "Epoch 3531, Loss: 0.00025619521238695597, Final Batch Loss: 1.4311453924165107e-05\n",
      "Epoch 3532, Loss: 7.540794103988446e-05, Final Batch Loss: 8.601600711699575e-06\n",
      "Epoch 3533, Loss: 0.00033354482184222434, Final Batch Loss: 0.00014841128722764552\n",
      "Epoch 3534, Loss: 0.0007127562894311268, Final Batch Loss: 0.00034516953746788204\n",
      "Epoch 3535, Loss: 0.0002073336827379535, Final Batch Loss: 1.0864524483622517e-05\n",
      "Epoch 3536, Loss: 0.0008362863081856631, Final Batch Loss: 5.996139952912927e-05\n",
      "Epoch 3537, Loss: 0.0009957150505215395, Final Batch Loss: 0.0001088264980353415\n",
      "Epoch 3538, Loss: 0.0032238502959671678, Final Batch Loss: 4.383700434118509e-05\n",
      "Epoch 3539, Loss: 0.0002970514542539604, Final Batch Loss: 9.675328328739852e-06\n",
      "Epoch 3540, Loss: 0.009885307341392036, Final Batch Loss: 8.478984454995953e-06\n",
      "Epoch 3541, Loss: 0.0005526690556507674, Final Batch Loss: 4.746475497086067e-06\n",
      "Epoch 3542, Loss: 0.005648823233059375, Final Batch Loss: 5.9991009038640186e-05\n",
      "Epoch 3543, Loss: 0.0004885132921117474, Final Batch Loss: 4.946957233187277e-06\n",
      "Epoch 3544, Loss: 0.0004224956101097632, Final Batch Loss: 8.238359441747889e-05\n",
      "Epoch 3545, Loss: 0.0009094465749512892, Final Batch Loss: 0.0002310033596586436\n",
      "Epoch 3546, Loss: 0.0005842018654220738, Final Batch Loss: 0.00010560706869000569\n",
      "Epoch 3547, Loss: 0.007655893098672095, Final Batch Loss: 0.007398860063403845\n",
      "Epoch 3548, Loss: 0.0009307896907557733, Final Batch Loss: 6.84200567775406e-05\n",
      "Epoch 3549, Loss: 0.0005178857099963352, Final Batch Loss: 1.933759995154105e-05\n",
      "Epoch 3550, Loss: 0.00027532707463251427, Final Batch Loss: 5.021206379751675e-05\n",
      "Epoch 3551, Loss: 0.0008075218684098218, Final Batch Loss: 0.0006584316724911332\n",
      "Epoch 3552, Loss: 0.0006865357154310914, Final Batch Loss: 0.0005044386489316821\n",
      "Epoch 3553, Loss: 0.0003483695945760701, Final Batch Loss: 0.00014061071851756424\n",
      "Epoch 3554, Loss: 0.0002854785052477382, Final Batch Loss: 1.9818508008029312e-05\n",
      "Epoch 3555, Loss: 0.018599185132188722, Final Batch Loss: 0.00037205551052466035\n",
      "Epoch 3556, Loss: 0.0006313086978479987, Final Batch Loss: 5.18939305038657e-06\n",
      "Epoch 3557, Loss: 0.0009187987079712912, Final Batch Loss: 0.0006839937996119261\n",
      "Epoch 3558, Loss: 0.0005005884740967304, Final Batch Loss: 0.00032726526842452586\n",
      "Epoch 3559, Loss: 0.0002730847081693355, Final Batch Loss: 0.0001841107732616365\n",
      "Epoch 3560, Loss: 0.009951581553650612, Final Batch Loss: 0.00010870051482925192\n",
      "Epoch 3561, Loss: 0.0011799493622675072, Final Batch Loss: 1.6839574527693912e-05\n",
      "Epoch 3562, Loss: 0.006944200735233608, Final Batch Loss: 1.9196360881323926e-05\n",
      "Epoch 3563, Loss: 0.00056888883409556, Final Batch Loss: 0.00022926917881704867\n",
      "Epoch 3564, Loss: 0.0004794045744347386, Final Batch Loss: 0.00024224072694778442\n",
      "Epoch 3565, Loss: 0.00025893427118717227, Final Batch Loss: 1.8956296116812155e-05\n",
      "Epoch 3566, Loss: 0.00034238897205796093, Final Batch Loss: 2.165617115679197e-05\n",
      "Epoch 3567, Loss: 0.00048741365299065365, Final Batch Loss: 1.5084179722180124e-05\n",
      "Epoch 3568, Loss: 0.001241771755303489, Final Batch Loss: 0.0005087416502647102\n",
      "Epoch 3569, Loss: 0.0021257380376482615, Final Batch Loss: 0.001996571198105812\n",
      "Epoch 3570, Loss: 0.00022247522065299563, Final Batch Loss: 3.375677624717355e-05\n",
      "Epoch 3571, Loss: 0.0011437921684773755, Final Batch Loss: 6.145787665445823e-06\n",
      "Epoch 3572, Loss: 0.00012263451480976073, Final Batch Loss: 3.08895978378132e-05\n",
      "Epoch 3573, Loss: 0.000324512948282063, Final Batch Loss: 1.642738607188221e-05\n",
      "Epoch 3574, Loss: 0.02938459383949521, Final Batch Loss: 0.00015138259914238006\n",
      "Epoch 3575, Loss: 0.0013983719691168517, Final Batch Loss: 8.96796555025503e-05\n",
      "Epoch 3576, Loss: 0.00042130244264626526, Final Batch Loss: 6.4675900830479804e-06\n",
      "Epoch 3577, Loss: 0.008668011227200623, Final Batch Loss: 8.960128980106674e-06\n",
      "Epoch 3578, Loss: 0.0006246771954465657, Final Batch Loss: 3.1887419027043507e-05\n",
      "Epoch 3579, Loss: 0.015107570057807607, Final Batch Loss: 8.977849938673899e-05\n",
      "Epoch 3580, Loss: 0.0038644583473796956, Final Batch Loss: 1.4072393241804093e-05\n",
      "Epoch 3581, Loss: 0.0004817524340978707, Final Batch Loss: 5.818895260745194e-06\n",
      "Epoch 3582, Loss: 0.0015434925007866696, Final Batch Loss: 7.924041710793972e-05\n",
      "Epoch 3583, Loss: 0.0011062428420700599, Final Batch Loss: 3.43590036209207e-05\n",
      "Epoch 3584, Loss: 0.0007015074115770403, Final Batch Loss: 0.0004383069172035903\n",
      "Epoch 3585, Loss: 0.0005527751200133935, Final Batch Loss: 4.60964220110327e-05\n",
      "Epoch 3586, Loss: 0.00685137420805404, Final Batch Loss: 0.0044957613572478294\n",
      "Epoch 3587, Loss: 0.007010074740719574, Final Batch Loss: 1.3063036931271199e-05\n",
      "Epoch 3588, Loss: 0.002317438986210618, Final Batch Loss: 0.0006019470165483654\n",
      "Epoch 3589, Loss: 0.005290263655297167, Final Batch Loss: 3.139440195809584e-06\n",
      "Epoch 3590, Loss: 0.0026665130717447028, Final Batch Loss: 0.00020582873548846692\n",
      "Epoch 3591, Loss: 0.0003825631520157913, Final Batch Loss: 5.4664273193338886e-05\n",
      "Epoch 3592, Loss: 0.00047113601613091305, Final Batch Loss: 1.9735265595954843e-05\n",
      "Epoch 3593, Loss: 0.0011168389064550865, Final Batch Loss: 0.00015116579015739262\n",
      "Epoch 3594, Loss: 0.0003250139889132697, Final Batch Loss: 7.981412636581808e-05\n",
      "Epoch 3595, Loss: 0.0004257282116668648, Final Batch Loss: 0.00014964310685172677\n",
      "Epoch 3596, Loss: 0.00020618404050765093, Final Batch Loss: 1.3540042345994152e-05\n",
      "Epoch 3597, Loss: 0.0008956841193139553, Final Batch Loss: 0.00011235793499508873\n",
      "Epoch 3598, Loss: 0.0003685635638248641, Final Batch Loss: 7.406078657368198e-05\n",
      "Epoch 3599, Loss: 0.00022838093354948796, Final Batch Loss: 9.656334441388026e-05\n",
      "Epoch 3600, Loss: 0.0003190503266523592, Final Batch Loss: 3.178200131515041e-05\n",
      "Epoch 3601, Loss: 0.010966135283524636, Final Batch Loss: 6.531355029437691e-05\n",
      "Epoch 3602, Loss: 0.0006164345541037619, Final Batch Loss: 8.023378177313134e-05\n",
      "Epoch 3603, Loss: 0.0004599404419423081, Final Batch Loss: 2.1565450879279524e-05\n",
      "Epoch 3604, Loss: 0.03596413950435817, Final Batch Loss: 7.288245251402259e-05\n",
      "Epoch 3605, Loss: 0.04851348666124977, Final Batch Loss: 3.0012946808710694e-05\n",
      "Epoch 3606, Loss: 0.00034138589398935437, Final Batch Loss: 0.0001056897672242485\n",
      "Epoch 3607, Loss: 0.001488573121605441, Final Batch Loss: 1.3983313692733645e-05\n",
      "Epoch 3608, Loss: 0.004412336384120863, Final Batch Loss: 0.0026185563765466213\n",
      "Epoch 3609, Loss: 0.000787738210419775, Final Batch Loss: 0.0005142602021805942\n",
      "Epoch 3610, Loss: 0.017155524150439305, Final Batch Loss: 0.0016268067993223667\n",
      "Epoch 3611, Loss: 0.02222258786787279, Final Batch Loss: 0.012774807401001453\n",
      "Epoch 3612, Loss: 0.0024369668171857484, Final Batch Loss: 9.984256030293182e-05\n",
      "Epoch 3613, Loss: 0.0009505872731097043, Final Batch Loss: 0.00014714416465722024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3614, Loss: 0.009382649324834347, Final Batch Loss: 0.00023340387269854546\n",
      "Epoch 3615, Loss: 0.0010015666630351916, Final Batch Loss: 0.00013608837616629899\n",
      "Epoch 3616, Loss: 0.002680217643501237, Final Batch Loss: 0.0010301590664312243\n",
      "Epoch 3617, Loss: 0.013115810361341573, Final Batch Loss: 0.0002693661954253912\n",
      "Epoch 3618, Loss: 0.026656378982806928, Final Batch Loss: 0.00011004140105796978\n",
      "Epoch 3619, Loss: 0.001048160185746383, Final Batch Loss: 0.00017437913629692048\n",
      "Epoch 3620, Loss: 0.0034988004626939073, Final Batch Loss: 0.00022746386821381748\n",
      "Epoch 3621, Loss: 0.0028643182122323196, Final Batch Loss: 5.881249308004044e-05\n",
      "Epoch 3622, Loss: 0.0075449310534168035, Final Batch Loss: 0.005982454866170883\n",
      "Epoch 3623, Loss: 0.005214735880144872, Final Batch Loss: 0.00042367944843135774\n",
      "Epoch 3624, Loss: 0.0004583843510772567, Final Batch Loss: 3.499786907923408e-05\n",
      "Epoch 3625, Loss: 0.02099924586218549, Final Batch Loss: 0.0179003719240427\n",
      "Epoch 3626, Loss: 0.0008426805361523293, Final Batch Loss: 0.00010052423021988943\n",
      "Epoch 3627, Loss: 0.0022440687534981407, Final Batch Loss: 0.000214260071516037\n",
      "Epoch 3628, Loss: 0.03601816683658399, Final Batch Loss: 0.021817415952682495\n",
      "Epoch 3629, Loss: 0.0006299923188635148, Final Batch Loss: 0.00021178662427701056\n",
      "Epoch 3630, Loss: 0.009964713164663408, Final Batch Loss: 0.009433796629309654\n",
      "Epoch 3631, Loss: 0.029485790837497916, Final Batch Loss: 0.015798164531588554\n",
      "Epoch 3632, Loss: 0.004894851561402902, Final Batch Loss: 0.0024351428728550673\n",
      "Epoch 3633, Loss: 0.0367506796028465, Final Batch Loss: 0.0008496933151036501\n",
      "Epoch 3634, Loss: 0.017479348956840113, Final Batch Loss: 0.000363741914043203\n",
      "Epoch 3635, Loss: 0.015211606747470796, Final Batch Loss: 0.0005120577989146113\n",
      "Epoch 3636, Loss: 0.031816718736081384, Final Batch Loss: 0.00018167139205615968\n",
      "Epoch 3637, Loss: 0.007439386157784611, Final Batch Loss: 0.005134374834597111\n",
      "Epoch 3638, Loss: 0.006314913567621261, Final Batch Loss: 0.0023840509820729494\n",
      "Epoch 3639, Loss: 0.0032826444949023426, Final Batch Loss: 0.00042041903361678123\n",
      "Epoch 3640, Loss: 0.005929766688495874, Final Batch Loss: 0.0010812092805281281\n",
      "Epoch 3641, Loss: 0.01343311472737696, Final Batch Loss: 0.00020558272080961615\n",
      "Epoch 3642, Loss: 0.021611070391372778, Final Batch Loss: 0.019969288259744644\n",
      "Epoch 3643, Loss: 0.0028360033757053316, Final Batch Loss: 0.00038642087019979954\n",
      "Epoch 3644, Loss: 0.013076289091259241, Final Batch Loss: 0.009080154821276665\n",
      "Epoch 3645, Loss: 0.010884211078519002, Final Batch Loss: 0.0019379102159291506\n",
      "Epoch 3646, Loss: 0.001580418917001225, Final Batch Loss: 0.0003419116837903857\n",
      "Epoch 3647, Loss: 0.0019269080366939306, Final Batch Loss: 0.00015770632307976484\n",
      "Epoch 3648, Loss: 0.009736213949508965, Final Batch Loss: 0.00027560937451198697\n",
      "Epoch 3649, Loss: 0.010483892270713113, Final Batch Loss: 0.009745189920067787\n",
      "Epoch 3650, Loss: 0.0007966431658132933, Final Batch Loss: 0.00012009940110147\n",
      "Epoch 3651, Loss: 0.0020033618056913838, Final Batch Loss: 6.9963643909432e-05\n",
      "Epoch 3652, Loss: 0.0016108682684716769, Final Batch Loss: 8.23192167445086e-05\n",
      "Epoch 3653, Loss: 0.00325152586447075, Final Batch Loss: 0.0004391722904983908\n",
      "Epoch 3654, Loss: 0.005509891612746287, Final Batch Loss: 7.53212152631022e-05\n",
      "Epoch 3655, Loss: 0.0023440727818524465, Final Batch Loss: 0.0012073115212842822\n",
      "Epoch 3656, Loss: 0.0006207621008798014, Final Batch Loss: 0.00014408249990083277\n",
      "Epoch 3657, Loss: 0.000914859316253569, Final Batch Loss: 0.00018568907398730516\n",
      "Epoch 3658, Loss: 0.0009973812666430604, Final Batch Loss: 0.00014559918781742454\n",
      "Epoch 3659, Loss: 0.0006070611707400531, Final Batch Loss: 7.616303628310561e-05\n",
      "Epoch 3660, Loss: 0.0029687407659366727, Final Batch Loss: 0.0014544454170390964\n",
      "Epoch 3661, Loss: 0.0009621322096791118, Final Batch Loss: 0.00032639969140291214\n",
      "Epoch 3662, Loss: 0.003636340392404236, Final Batch Loss: 0.0023617802653461695\n",
      "Epoch 3663, Loss: 0.00047687366168247536, Final Batch Loss: 7.965354598127306e-05\n",
      "Epoch 3664, Loss: 0.0008680525934323668, Final Batch Loss: 9.457500709686428e-05\n",
      "Epoch 3665, Loss: 0.0002758617429208243, Final Batch Loss: 2.1451951397466473e-05\n",
      "Epoch 3666, Loss: 0.0005051012412877753, Final Batch Loss: 0.0001062951487256214\n",
      "Epoch 3667, Loss: 0.004519841611909214, Final Batch Loss: 0.004241528455168009\n",
      "Epoch 3668, Loss: 0.0005035446374677122, Final Batch Loss: 8.062581764534116e-05\n",
      "Epoch 3669, Loss: 0.0004905883961328072, Final Batch Loss: 0.0001434838050045073\n",
      "Epoch 3670, Loss: 0.0007778161434544018, Final Batch Loss: 0.00031522774952463806\n",
      "Epoch 3671, Loss: 0.0012734389147226466, Final Batch Loss: 0.0007619303069077432\n",
      "Epoch 3672, Loss: 0.00039254377770703286, Final Batch Loss: 4.360636376077309e-05\n",
      "Epoch 3673, Loss: 0.001457587113691261, Final Batch Loss: 5.5125758080976084e-05\n",
      "Epoch 3674, Loss: 0.00327957658601008, Final Batch Loss: 5.868007519893581e-06\n",
      "Epoch 3675, Loss: 0.0005477619515659171, Final Batch Loss: 1.0802340511872899e-05\n",
      "Epoch 3676, Loss: 0.007075678615365177, Final Batch Loss: 6.540579488500953e-05\n",
      "Epoch 3677, Loss: 0.0009985903998313006, Final Batch Loss: 6.313253834377974e-05\n",
      "Epoch 3678, Loss: 0.00044169099237478804, Final Batch Loss: 2.7620679247775115e-05\n",
      "Epoch 3679, Loss: 0.0008717248456378002, Final Batch Loss: 6.529772508656606e-05\n",
      "Epoch 3680, Loss: 0.0015050755355332512, Final Batch Loss: 0.0012247574049979448\n",
      "Epoch 3681, Loss: 0.009799330189707689, Final Batch Loss: 2.7524794859346002e-05\n",
      "Epoch 3682, Loss: 0.0016675775768817402, Final Batch Loss: 0.0011602594750002027\n",
      "Epoch 3683, Loss: 0.0009554974640195724, Final Batch Loss: 0.00030010714544914663\n",
      "Epoch 3684, Loss: 0.00027237459289608523, Final Batch Loss: 2.643408879521303e-05\n",
      "Epoch 3685, Loss: 0.0015200060151983052, Final Batch Loss: 0.00030233984580263495\n",
      "Epoch 3686, Loss: 0.0065967330447165295, Final Batch Loss: 2.2835840354673564e-05\n",
      "Epoch 3687, Loss: 0.0006949656135475379, Final Batch Loss: 1.1519700819917489e-05\n",
      "Epoch 3688, Loss: 0.00508091085794149, Final Batch Loss: 3.844356979243457e-05\n",
      "Epoch 3689, Loss: 0.0004057767873746343, Final Batch Loss: 7.890931010479107e-05\n",
      "Epoch 3690, Loss: 0.000191527744391351, Final Batch Loss: 0.00010574841144261882\n",
      "Epoch 3691, Loss: 0.00033283137236139737, Final Batch Loss: 3.48016619682312e-05\n",
      "Epoch 3692, Loss: 0.024690847552847117, Final Batch Loss: 0.0003896706039085984\n",
      "Epoch 3693, Loss: 0.0007574251867481507, Final Batch Loss: 9.878149285214022e-06\n",
      "Epoch 3694, Loss: 0.0030128068647172768, Final Batch Loss: 0.0003302842960692942\n",
      "Epoch 3695, Loss: 0.0010162512826354941, Final Batch Loss: 0.0005186241469345987\n",
      "Epoch 3696, Loss: 0.006855437459307723, Final Batch Loss: 0.005645945202559233\n",
      "Epoch 3697, Loss: 0.004005612077889964, Final Batch Loss: 0.002318646991625428\n",
      "Epoch 3698, Loss: 0.0010115498880622908, Final Batch Loss: 0.00011830878065666184\n",
      "Epoch 3699, Loss: 0.0018760108250717167, Final Batch Loss: 0.00021771241154056042\n",
      "Epoch 3700, Loss: 0.023102498191292398, Final Batch Loss: 0.0009221386280842125\n",
      "Epoch 3701, Loss: 0.029715126695919025, Final Batch Loss: 4.576601895678323e-06\n",
      "Epoch 3702, Loss: 0.003534664281687583, Final Batch Loss: 1.2005430107819848e-05\n",
      "Epoch 3703, Loss: 0.00026962050469592214, Final Batch Loss: 1.7251266399398446e-05\n",
      "Epoch 3704, Loss: 0.019428373557275336, Final Batch Loss: 8.77164893609006e-06\n",
      "Epoch 3705, Loss: 0.0006020072141836863, Final Batch Loss: 4.834769424633123e-05\n",
      "Epoch 3706, Loss: 0.00608758813541499, Final Batch Loss: 0.004040806088596582\n",
      "Epoch 3707, Loss: 0.0019793955943896435, Final Batch Loss: 0.00020088312157895416\n",
      "Epoch 3708, Loss: 0.0006250724982237443, Final Batch Loss: 9.40989120863378e-05\n",
      "Epoch 3709, Loss: 0.01899124987903633, Final Batch Loss: 4.656894088839181e-05\n",
      "Epoch 3710, Loss: 0.0006442977901315317, Final Batch Loss: 6.59459619782865e-05\n",
      "Epoch 3711, Loss: 0.0034519004184403457, Final Batch Loss: 0.00019510503625497222\n",
      "Epoch 3712, Loss: 0.0013066048595646862, Final Batch Loss: 7.207699673017487e-05\n",
      "Epoch 3713, Loss: 0.005160803870239761, Final Batch Loss: 0.00014773881412111223\n",
      "Epoch 3714, Loss: 0.0038048817586968653, Final Batch Loss: 0.00040458148578181863\n",
      "Epoch 3715, Loss: 0.0017742964082572144, Final Batch Loss: 0.0003941893228329718\n",
      "Epoch 3716, Loss: 0.001909791593789123, Final Batch Loss: 0.0011606402695178986\n",
      "Epoch 3717, Loss: 0.0012552942862384953, Final Batch Loss: 0.00024328124709427357\n",
      "Epoch 3718, Loss: 0.01342332024796633, Final Batch Loss: 7.043562800390646e-05\n",
      "Epoch 3719, Loss: 0.0017912896037159953, Final Batch Loss: 0.0006512359832413495\n",
      "Epoch 3720, Loss: 0.0022932973370188847, Final Batch Loss: 0.0018042578594759107\n",
      "Epoch 3721, Loss: 0.000948873352172086, Final Batch Loss: 4.91962164232973e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3722, Loss: 0.0007132273931347299, Final Batch Loss: 8.099847036646679e-06\n",
      "Epoch 3723, Loss: 0.008767383675149176, Final Batch Loss: 5.42869747732766e-05\n",
      "Epoch 3724, Loss: 0.02854439744260162, Final Batch Loss: 0.0001309254002990201\n",
      "Epoch 3725, Loss: 0.00164469974788517, Final Batch Loss: 0.0010645025176927447\n",
      "Epoch 3726, Loss: 0.0014104326401138678, Final Batch Loss: 0.00016556616174057126\n",
      "Epoch 3727, Loss: 0.005312798770319205, Final Batch Loss: 8.849466394167393e-05\n",
      "Epoch 3728, Loss: 0.004314417481509736, Final Batch Loss: 4.022486609756015e-05\n",
      "Epoch 3729, Loss: 0.0005208802358538378, Final Batch Loss: 0.0002383362443652004\n",
      "Epoch 3730, Loss: 0.0006300803397607524, Final Batch Loss: 3.281224417150952e-05\n",
      "Epoch 3731, Loss: 0.000987248400633689, Final Batch Loss: 0.00024192039563786238\n",
      "Epoch 3732, Loss: 0.001954629780811956, Final Batch Loss: 0.0011309748515486717\n",
      "Epoch 3733, Loss: 0.00046309053141158074, Final Batch Loss: 0.00011712751438608393\n",
      "Epoch 3734, Loss: 0.0011284386164334137, Final Batch Loss: 5.1060455007245764e-05\n",
      "Epoch 3735, Loss: 0.0033001004958350677, Final Batch Loss: 2.3169659471022896e-05\n",
      "Epoch 3736, Loss: 0.0060965765587752685, Final Batch Loss: 0.00032077348441816866\n",
      "Epoch 3737, Loss: 0.0024623902718303725, Final Batch Loss: 0.0021809441968798637\n",
      "Epoch 3738, Loss: 0.003765988214581739, Final Batch Loss: 7.28143859305419e-05\n",
      "Epoch 3739, Loss: 0.021458443494339008, Final Batch Loss: 0.00011032389738829806\n",
      "Epoch 3740, Loss: 0.00038270327786449343, Final Batch Loss: 0.00010329503857064992\n",
      "Epoch 3741, Loss: 0.0007960454568092246, Final Batch Loss: 6.940650928299874e-05\n",
      "Epoch 3742, Loss: 0.000325682040056563, Final Batch Loss: 0.00013979533105157316\n",
      "Epoch 3743, Loss: 0.0034107278188457713, Final Batch Loss: 0.00017231986566912383\n",
      "Epoch 3744, Loss: 0.0009645417485444341, Final Batch Loss: 2.9008293495280668e-05\n",
      "Epoch 3745, Loss: 0.0010993186551786494, Final Batch Loss: 0.00012655365571845323\n",
      "Epoch 3746, Loss: 0.0002588165079941973, Final Batch Loss: 9.409715858055279e-05\n",
      "Epoch 3747, Loss: 0.0003263131893618265, Final Batch Loss: 0.00012277263158466667\n",
      "Epoch 3748, Loss: 0.0007300762408704031, Final Batch Loss: 1.6778340068412945e-05\n",
      "Epoch 3749, Loss: 0.000732572687411448, Final Batch Loss: 3.225969703635201e-05\n",
      "Epoch 3750, Loss: 0.0005223603420745349, Final Batch Loss: 0.00018202800129074603\n",
      "Epoch 3751, Loss: 0.00032384598125645425, Final Batch Loss: 5.07597578689456e-05\n",
      "Epoch 3752, Loss: 0.0006948671143618412, Final Batch Loss: 0.0002708341635297984\n",
      "Epoch 3753, Loss: 0.00025833047311607515, Final Batch Loss: 7.702378206886351e-05\n",
      "Epoch 3754, Loss: 0.005298475225572474, Final Batch Loss: 0.0007900941418483853\n",
      "Epoch 3755, Loss: 0.0006644245877396315, Final Batch Loss: 0.00011876057396875694\n",
      "Epoch 3756, Loss: 0.000368760600395035, Final Batch Loss: 5.4480653489008546e-05\n",
      "Epoch 3757, Loss: 0.00020607715487130918, Final Batch Loss: 3.1723389838589355e-05\n",
      "Epoch 3758, Loss: 0.0003650706748885568, Final Batch Loss: 0.0001436312886653468\n",
      "Epoch 3759, Loss: 0.0018086201162077487, Final Batch Loss: 0.0014895300846546888\n",
      "Epoch 3760, Loss: 0.0005749096835643286, Final Batch Loss: 0.0003992110723629594\n",
      "Epoch 3761, Loss: 0.0017580148687557084, Final Batch Loss: 0.0010395044228062034\n",
      "Epoch 3762, Loss: 0.002931763951892208, Final Batch Loss: 2.4562423277529888e-06\n",
      "Epoch 3763, Loss: 0.0009136443732131738, Final Batch Loss: 8.569344208808616e-05\n",
      "Epoch 3764, Loss: 0.011486700364912394, Final Batch Loss: 0.00252732215449214\n",
      "Epoch 3765, Loss: 0.005187170874705771, Final Batch Loss: 0.0004181035910733044\n",
      "Epoch 3766, Loss: 0.00669112669856986, Final Batch Loss: 5.2519892051350325e-05\n",
      "Epoch 3767, Loss: 0.002634888718603179, Final Batch Loss: 0.0022518623154610395\n",
      "Epoch 3768, Loss: 0.00883513648295775, Final Batch Loss: 0.0002119517303071916\n",
      "Epoch 3769, Loss: 0.0005854056435055099, Final Batch Loss: 0.00027445817249827087\n",
      "Epoch 3770, Loss: 0.026239719103614334, Final Batch Loss: 0.00028093086439184844\n",
      "Epoch 3771, Loss: 0.0016473676450914354, Final Batch Loss: 0.00010702414147090167\n",
      "Epoch 3772, Loss: 0.0018439303094055504, Final Batch Loss: 0.0015433968510478735\n",
      "Epoch 3773, Loss: 0.01571501368744066, Final Batch Loss: 4.372581315692514e-06\n",
      "Epoch 3774, Loss: 0.0002520652578823501, Final Batch Loss: 2.0082899936824106e-05\n",
      "Epoch 3775, Loss: 0.00077190925458126, Final Batch Loss: 0.00010646771261235699\n",
      "Epoch 3776, Loss: 0.008458029395114863, Final Batch Loss: 3.615727837313898e-05\n",
      "Epoch 3777, Loss: 0.0005640337021759478, Final Batch Loss: 0.0001990134478546679\n",
      "Epoch 3778, Loss: 0.0023519154638051987, Final Batch Loss: 0.0006968618836253881\n",
      "Epoch 3779, Loss: 0.0170477007941372, Final Batch Loss: 0.01511501893401146\n",
      "Epoch 3780, Loss: 0.0002768450867733918, Final Batch Loss: 7.087755511747673e-05\n",
      "Epoch 3781, Loss: 0.0002387654149060836, Final Batch Loss: 2.7516731279320084e-05\n",
      "Epoch 3782, Loss: 0.0003953423656639643, Final Batch Loss: 7.503399683628231e-05\n",
      "Epoch 3783, Loss: 0.0011022535181837156, Final Batch Loss: 3.28405003529042e-05\n",
      "Epoch 3784, Loss: 0.0003973133279941976, Final Batch Loss: 7.754263788228855e-05\n",
      "Epoch 3785, Loss: 0.0036227351392881246, Final Batch Loss: 0.00010256963287247345\n",
      "Epoch 3786, Loss: 0.0003577550523914397, Final Batch Loss: 9.092008986044675e-05\n",
      "Epoch 3787, Loss: 0.0005628177550534019, Final Batch Loss: 0.0002968910848721862\n",
      "Epoch 3788, Loss: 0.00045732728904113173, Final Batch Loss: 0.00024014593509491533\n",
      "Epoch 3789, Loss: 0.0004343271648394875, Final Batch Loss: 4.5345688704401255e-05\n",
      "Epoch 3790, Loss: 0.0012279074435355142, Final Batch Loss: 1.3758100976701826e-05\n",
      "Epoch 3791, Loss: 0.00020589984160324093, Final Batch Loss: 1.0094366189150605e-05\n",
      "Epoch 3792, Loss: 0.0007892280227679294, Final Batch Loss: 0.00011478003580123186\n",
      "Epoch 3793, Loss: 0.00023805290129530476, Final Batch Loss: 8.067283488344401e-05\n",
      "Epoch 3794, Loss: 0.0005174976922717178, Final Batch Loss: 0.0003958146262448281\n",
      "Epoch 3795, Loss: 0.0003129562601316138, Final Batch Loss: 0.00016079867782536894\n",
      "Epoch 3796, Loss: 0.00972324662416213, Final Batch Loss: 1.4283362361311447e-05\n",
      "Epoch 3797, Loss: 0.0019802114729827736, Final Batch Loss: 0.000742307398468256\n",
      "Epoch 3798, Loss: 0.00039035366717143916, Final Batch Loss: 4.68143516627606e-05\n",
      "Epoch 3799, Loss: 0.0012898897803097498, Final Batch Loss: 1.0891778401855845e-05\n",
      "Epoch 3800, Loss: 0.0006088793097660528, Final Batch Loss: 8.116157005133573e-06\n",
      "Epoch 3801, Loss: 0.0003873013170050399, Final Batch Loss: 5.7292613746540155e-06\n",
      "Epoch 3802, Loss: 0.00033797751893871464, Final Batch Loss: 6.781720730941743e-05\n",
      "Epoch 3803, Loss: 0.0002069356514766696, Final Batch Loss: 0.00010876901069423184\n",
      "Epoch 3804, Loss: 0.00016702175798855023, Final Batch Loss: 6.983095772739034e-06\n",
      "Epoch 3805, Loss: 0.0047244607922038995, Final Batch Loss: 0.0027550950180739164\n",
      "Epoch 3806, Loss: 0.00016323235286108684, Final Batch Loss: 2.6969593818648718e-05\n",
      "Epoch 3807, Loss: 0.0003585271697374992, Final Batch Loss: 0.000199030022486113\n",
      "Epoch 3808, Loss: 0.060488920091302134, Final Batch Loss: 3.978815584559925e-05\n",
      "Epoch 3809, Loss: 0.027249231679888908, Final Batch Loss: 0.00010247856698697433\n",
      "Epoch 3810, Loss: 0.0003736103462870233, Final Batch Loss: 6.675013719359413e-05\n",
      "Epoch 3811, Loss: 0.0006209216726347222, Final Batch Loss: 1.3222429515735712e-05\n",
      "Epoch 3812, Loss: 0.0005202351712796371, Final Batch Loss: 0.0003243040991947055\n",
      "Epoch 3813, Loss: 0.037615019391523674, Final Batch Loss: 0.02342112921178341\n",
      "Epoch 3814, Loss: 0.0016742602201702539, Final Batch Loss: 5.460186002892442e-05\n",
      "Epoch 3815, Loss: 0.0022672424456686713, Final Batch Loss: 0.0014690899988636374\n",
      "Epoch 3816, Loss: 0.00580952857126249, Final Batch Loss: 7.501342770410702e-05\n",
      "Epoch 3817, Loss: 0.02268851498956792, Final Batch Loss: 0.000388271757401526\n",
      "Epoch 3818, Loss: 0.04807434309987002, Final Batch Loss: 0.00013607204891741276\n",
      "Epoch 3819, Loss: 0.0007861011818022234, Final Batch Loss: 2.5023806301760487e-05\n",
      "Epoch 3820, Loss: 0.006206436441061669, Final Batch Loss: 1.916899964271579e-05\n",
      "Epoch 3821, Loss: 0.0017553644356667064, Final Batch Loss: 0.00034148443955928087\n",
      "Epoch 3822, Loss: 0.002782427520287456, Final Batch Loss: 8.218154107453302e-06\n",
      "Epoch 3823, Loss: 0.03986103311763145, Final Batch Loss: 0.00016147849964909256\n",
      "Epoch 3824, Loss: 0.0018150723190046847, Final Batch Loss: 0.000548704934772104\n",
      "Epoch 3825, Loss: 0.021765091703855433, Final Batch Loss: 0.0021980232559144497\n",
      "Epoch 3826, Loss: 0.0032247218041447923, Final Batch Loss: 0.00015863208682276309\n",
      "Epoch 3827, Loss: 0.005031456610595342, Final Batch Loss: 0.0005225215572863817\n",
      "Epoch 3828, Loss: 0.0016828515290399082, Final Batch Loss: 9.51108886511065e-05\n",
      "Epoch 3829, Loss: 0.0110355838041869, Final Batch Loss: 5.338600749382749e-05\n",
      "Epoch 3830, Loss: 0.0009068098734132946, Final Batch Loss: 0.00020967381715308875\n",
      "Epoch 3831, Loss: 0.004741286218632013, Final Batch Loss: 6.841606227681041e-05\n",
      "Epoch 3832, Loss: 0.0006986320513533428, Final Batch Loss: 0.00016181044338736683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3833, Loss: 0.00365600914665265, Final Batch Loss: 0.003017086535692215\n",
      "Epoch 3834, Loss: 0.0012312478793319315, Final Batch Loss: 0.0009515287238173187\n",
      "Epoch 3835, Loss: 0.000990985383396037, Final Batch Loss: 0.0005701485788449645\n",
      "Epoch 3836, Loss: 0.00102144994889386, Final Batch Loss: 3.999094769824296e-05\n",
      "Epoch 3837, Loss: 0.0034347790497122332, Final Batch Loss: 0.0007252782233990729\n",
      "Epoch 3838, Loss: 0.004040504260046873, Final Batch Loss: 0.00024087849305942655\n",
      "Epoch 3839, Loss: 0.0005237169189058477, Final Batch Loss: 9.698969370219857e-05\n",
      "Epoch 3840, Loss: 0.001468972048314754, Final Batch Loss: 0.00031803379533812404\n",
      "Epoch 3841, Loss: 0.0003469727762421826, Final Batch Loss: 6.200826464919373e-05\n",
      "Epoch 3842, Loss: 0.0004524855630734237, Final Batch Loss: 2.1630878109135665e-05\n",
      "Epoch 3843, Loss: 0.00038062689873186173, Final Batch Loss: 0.00023186342150438577\n",
      "Epoch 3844, Loss: 0.0006598981417482719, Final Batch Loss: 0.0001373525446979329\n",
      "Epoch 3845, Loss: 0.0003476818928902503, Final Batch Loss: 2.3972699636942707e-05\n",
      "Epoch 3846, Loss: 0.0066244401277799625, Final Batch Loss: 3.8936246710363775e-05\n",
      "Epoch 3847, Loss: 0.0015374285994766979, Final Batch Loss: 0.0007804131018929183\n",
      "Epoch 3848, Loss: 0.0031848426879150793, Final Batch Loss: 0.0014968024333938956\n",
      "Epoch 3849, Loss: 0.0006623020944971358, Final Batch Loss: 2.5943436412489973e-05\n",
      "Epoch 3850, Loss: 0.0014071097757550888, Final Batch Loss: 0.0007870637928135693\n",
      "Epoch 3851, Loss: 0.0008833266401779838, Final Batch Loss: 0.0005620841402560472\n",
      "Epoch 3852, Loss: 0.00192514554146328, Final Batch Loss: 3.71373062080238e-05\n",
      "Epoch 3853, Loss: 0.0021376698787207715, Final Batch Loss: 6.438933633035049e-05\n",
      "Epoch 3854, Loss: 0.0010251718667859677, Final Batch Loss: 0.00013159123773220927\n",
      "Epoch 3855, Loss: 0.000491097685880959, Final Batch Loss: 7.971657760208473e-05\n",
      "Epoch 3856, Loss: 0.0018737892896751873, Final Batch Loss: 8.473110938211903e-05\n",
      "Epoch 3857, Loss: 0.0017104549001487612, Final Batch Loss: 4.433957201399608e-06\n",
      "Epoch 3858, Loss: 0.000545294074981939, Final Batch Loss: 1.1832125892397016e-05\n",
      "Epoch 3859, Loss: 0.0009439059722353704, Final Batch Loss: 0.0006654997123405337\n",
      "Epoch 3860, Loss: 0.0008225269120885059, Final Batch Loss: 6.67261629132554e-05\n",
      "Epoch 3861, Loss: 0.0013248032119008712, Final Batch Loss: 2.41065354202874e-05\n",
      "Epoch 3862, Loss: 0.0007356935329880798, Final Batch Loss: 3.0264522138168104e-05\n",
      "Epoch 3863, Loss: 0.0007214481338451151, Final Batch Loss: 3.949024903704412e-05\n",
      "Epoch 3864, Loss: 0.0013146572255209321, Final Batch Loss: 0.00014125945745036006\n",
      "Epoch 3865, Loss: 0.00041564465209376067, Final Batch Loss: 6.661023508058861e-05\n",
      "Epoch 3866, Loss: 0.00042456237133592367, Final Batch Loss: 2.0776267774635926e-05\n",
      "Epoch 3867, Loss: 0.000786095955845667, Final Batch Loss: 9.342694829683751e-05\n",
      "Epoch 3868, Loss: 0.0014853296852379572, Final Batch Loss: 0.0005716364248655736\n",
      "Epoch 3869, Loss: 0.00232073988445336, Final Batch Loss: 8.84963883436285e-05\n",
      "Epoch 3870, Loss: 0.03099326277151704, Final Batch Loss: 4.3660249502863735e-05\n",
      "Epoch 3871, Loss: 0.00449105290681473, Final Batch Loss: 0.004126853309571743\n",
      "Epoch 3872, Loss: 0.004306705637645791, Final Batch Loss: 1.0660818588803522e-05\n",
      "Epoch 3873, Loss: 0.0005845906198373996, Final Batch Loss: 5.653220432577655e-05\n",
      "Epoch 3874, Loss: 0.002917450292443391, Final Batch Loss: 2.0164188754279166e-05\n",
      "Epoch 3875, Loss: 0.030672315107949544, Final Batch Loss: 1.3291361028677784e-05\n",
      "Epoch 3876, Loss: 0.0004286303592380136, Final Batch Loss: 0.00011084484867751598\n",
      "Epoch 3877, Loss: 0.0004013170037069358, Final Batch Loss: 9.505284833721817e-05\n",
      "Epoch 3878, Loss: 0.00048396738566225395, Final Batch Loss: 2.011262040468864e-05\n",
      "Epoch 3879, Loss: 0.05942145979497582, Final Batch Loss: 0.059184394776821136\n",
      "Epoch 3880, Loss: 0.0005196242673264351, Final Batch Loss: 0.0001991787285078317\n",
      "Epoch 3881, Loss: 0.00068178832589183, Final Batch Loss: 0.00020823163504246622\n",
      "Epoch 3882, Loss: 0.02619230110576609, Final Batch Loss: 0.007552224211394787\n",
      "Epoch 3883, Loss: 0.00046274159103631973, Final Batch Loss: 8.943059219745919e-05\n",
      "Epoch 3884, Loss: 0.002089263329253299, Final Batch Loss: 0.00027057091938331723\n",
      "Epoch 3885, Loss: 0.0008116405369946733, Final Batch Loss: 0.00033238710602745414\n",
      "Epoch 3886, Loss: 0.0005450175158330239, Final Batch Loss: 0.00012090086966054514\n",
      "Epoch 3887, Loss: 0.01132191575015895, Final Batch Loss: 9.983186464523897e-05\n",
      "Epoch 3888, Loss: 0.02154404914472252, Final Batch Loss: 0.00013794790720567107\n",
      "Epoch 3889, Loss: 0.026687914971262217, Final Batch Loss: 0.026273690164089203\n",
      "Epoch 3890, Loss: 0.0069530786568066105, Final Batch Loss: 6.427163316402584e-05\n",
      "Epoch 3891, Loss: 0.03160882485099137, Final Batch Loss: 0.02152041345834732\n",
      "Epoch 3892, Loss: 0.01910919847432524, Final Batch Loss: 0.001568570383824408\n",
      "Epoch 3893, Loss: 0.025864546740194783, Final Batch Loss: 0.00018694656318984926\n",
      "Epoch 3894, Loss: 0.06273551846970804, Final Batch Loss: 0.0006173779256641865\n",
      "Epoch 3895, Loss: 0.023946436471305788, Final Batch Loss: 0.0005747464019805193\n",
      "Epoch 3896, Loss: 0.027579491725191474, Final Batch Loss: 0.00395485945045948\n",
      "Epoch 3897, Loss: 0.008553254199796356, Final Batch Loss: 0.0002258661697851494\n",
      "Epoch 3898, Loss: 0.010109842958627269, Final Batch Loss: 0.008299978449940681\n",
      "Epoch 3899, Loss: 0.0032948940352071077, Final Batch Loss: 0.0006132101989351213\n",
      "Epoch 3900, Loss: 0.005928850150667131, Final Batch Loss: 0.0017195434775203466\n",
      "Epoch 3901, Loss: 0.0076591190008912235, Final Batch Loss: 0.00036133386311121285\n",
      "Epoch 3902, Loss: 0.0061993282797629945, Final Batch Loss: 9.208264964399859e-05\n",
      "Epoch 3903, Loss: 0.002067517110845074, Final Batch Loss: 0.00022636103676632047\n",
      "Epoch 3904, Loss: 0.003376181826752145, Final Batch Loss: 9.564318315824494e-05\n",
      "Epoch 3905, Loss: 0.009467830474022776, Final Batch Loss: 0.00028114771703258157\n",
      "Epoch 3906, Loss: 0.004072555180755444, Final Batch Loss: 0.002757280133664608\n",
      "Epoch 3907, Loss: 0.0008611712546553463, Final Batch Loss: 0.000237364336499013\n",
      "Epoch 3908, Loss: 0.001275911694392562, Final Batch Loss: 0.00022180781525094062\n",
      "Epoch 3909, Loss: 0.0011739118126570247, Final Batch Loss: 0.00010958055645460263\n",
      "Epoch 3910, Loss: 0.0011709760001394898, Final Batch Loss: 0.00017030598246492445\n",
      "Epoch 3911, Loss: 0.004863350282903411, Final Batch Loss: 2.1545763956964947e-05\n",
      "Epoch 3912, Loss: 0.0017969860127777793, Final Batch Loss: 7.07358485669829e-05\n",
      "Epoch 3913, Loss: 0.01584110813564621, Final Batch Loss: 0.012630943208932877\n",
      "Epoch 3914, Loss: 0.006054781493730843, Final Batch Loss: 0.00017896729696076363\n",
      "Epoch 3915, Loss: 0.006061222724383697, Final Batch Loss: 0.004708643537014723\n",
      "Epoch 3916, Loss: 0.004493749176617712, Final Batch Loss: 0.00033575971610844135\n",
      "Epoch 3917, Loss: 0.0021493103122338653, Final Batch Loss: 0.00033673777943477035\n",
      "Epoch 3918, Loss: 0.0014847089260001667, Final Batch Loss: 0.0007295795367099345\n",
      "Epoch 3919, Loss: 0.001956679960130714, Final Batch Loss: 2.7282931114314124e-05\n",
      "Epoch 3920, Loss: 0.0012892460563307395, Final Batch Loss: 1.9935219825129025e-05\n",
      "Epoch 3921, Loss: 0.000739430048270151, Final Batch Loss: 0.0002726409293245524\n",
      "Epoch 3922, Loss: 0.0016976812912616879, Final Batch Loss: 0.0006624809466302395\n",
      "Epoch 3923, Loss: 0.020593230434315046, Final Batch Loss: 0.0006409268244169652\n",
      "Epoch 3924, Loss: 0.001378059372655116, Final Batch Loss: 0.0001395248546032235\n",
      "Epoch 3925, Loss: 0.0008172370435204357, Final Batch Loss: 0.0002824425755534321\n",
      "Epoch 3926, Loss: 0.001248656422831118, Final Batch Loss: 3.7729914765805006e-05\n",
      "Epoch 3927, Loss: 0.0007223879283628776, Final Batch Loss: 7.287852895387914e-06\n",
      "Epoch 3928, Loss: 0.0015492081583943218, Final Batch Loss: 0.00038428063271567225\n",
      "Epoch 3929, Loss: 0.003969927660364192, Final Batch Loss: 3.9666636439505965e-05\n",
      "Epoch 3930, Loss: 0.01291516187848174, Final Batch Loss: 0.00011745833762688562\n",
      "Epoch 3931, Loss: 0.004950073162035551, Final Batch Loss: 0.0011999642010778189\n",
      "Epoch 3932, Loss: 0.010952962475130334, Final Batch Loss: 0.0006250218721106648\n",
      "Epoch 3933, Loss: 0.011117506721348036, Final Batch Loss: 5.44838621863164e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3934, Loss: 0.0014537596143782139, Final Batch Loss: 0.0004610125906765461\n",
      "Epoch 3935, Loss: 0.0006955466087674722, Final Batch Loss: 0.0001553849142510444\n",
      "Epoch 3936, Loss: 0.01720884270616807, Final Batch Loss: 0.00012832377979066223\n",
      "Epoch 3937, Loss: 0.0010839867100003175, Final Batch Loss: 7.027166429907084e-05\n",
      "Epoch 3938, Loss: 0.00938724578008987, Final Batch Loss: 0.00032463661045767367\n",
      "Epoch 3939, Loss: 0.0007081298463162966, Final Batch Loss: 0.000118790972919669\n",
      "Epoch 3940, Loss: 0.0017729440423863707, Final Batch Loss: 5.662795956595801e-05\n",
      "Epoch 3941, Loss: 0.004636683865101077, Final Batch Loss: 0.00012488728680182248\n",
      "Epoch 3942, Loss: 0.005345174911781214, Final Batch Loss: 0.004204423166811466\n",
      "Epoch 3943, Loss: 0.0014624561899836408, Final Batch Loss: 0.0004937162157148123\n",
      "Epoch 3944, Loss: 0.006408384262613254, Final Batch Loss: 0.00027864743606187403\n",
      "Epoch 3945, Loss: 0.001412796787917614, Final Batch Loss: 0.00012127580703236163\n",
      "Epoch 3946, Loss: 0.0013767958007520065, Final Batch Loss: 0.000344918662449345\n",
      "Epoch 3947, Loss: 0.026050180873426143, Final Batch Loss: 0.00013451234553940594\n",
      "Epoch 3948, Loss: 0.008511882304446772, Final Batch Loss: 2.800753281917423e-05\n",
      "Epoch 3949, Loss: 0.0009663588916737353, Final Batch Loss: 2.492695602995809e-05\n",
      "Epoch 3950, Loss: 0.0015495508305320982, Final Batch Loss: 0.00020542180573102087\n",
      "Epoch 3951, Loss: 0.0011614352024480468, Final Batch Loss: 0.0001036229805322364\n",
      "Epoch 3952, Loss: 0.0013104656245559454, Final Batch Loss: 0.00016821257304400206\n",
      "Epoch 3953, Loss: 0.0051379467163314985, Final Batch Loss: 7.1838126132206526e-06\n",
      "Epoch 3954, Loss: 0.001133290377765661, Final Batch Loss: 0.0002169543586205691\n",
      "Epoch 3955, Loss: 0.0015741011520731263, Final Batch Loss: 0.00041106672142632306\n",
      "Epoch 3956, Loss: 0.0005010164913983317, Final Batch Loss: 7.516309415223077e-05\n",
      "Epoch 3957, Loss: 0.0005735463455494028, Final Batch Loss: 3.4357839467702433e-05\n",
      "Epoch 3958, Loss: 0.0005747286959376652, Final Batch Loss: 6.865070463391021e-05\n",
      "Epoch 3959, Loss: 0.0043335212612873875, Final Batch Loss: 4.16264811065048e-05\n",
      "Epoch 3960, Loss: 0.0003399155211809557, Final Batch Loss: 4.8497582611162215e-05\n",
      "Epoch 3961, Loss: 0.0004118940832995577, Final Batch Loss: 1.3783950635115616e-05\n",
      "Epoch 3962, Loss: 0.03348565927808522, Final Batch Loss: 5.451518518384546e-05\n",
      "Epoch 3963, Loss: 0.0010775049486255739, Final Batch Loss: 0.00077606993727386\n",
      "Epoch 3964, Loss: 0.00047128949336183723, Final Batch Loss: 7.197606464615092e-05\n",
      "Epoch 3965, Loss: 0.004570202352624619, Final Batch Loss: 0.0009878085693344474\n",
      "Epoch 3966, Loss: 0.0006977669399930164, Final Batch Loss: 0.0001994874473894015\n",
      "Epoch 3967, Loss: 0.0006906903072376736, Final Batch Loss: 2.708914689719677e-05\n",
      "Epoch 3968, Loss: 0.02576598919404205, Final Batch Loss: 8.699590398464352e-05\n",
      "Epoch 3969, Loss: 0.006773930153940455, Final Batch Loss: 2.4222043066401966e-05\n",
      "Epoch 3970, Loss: 0.0005247700937616173, Final Batch Loss: 5.6998269428731874e-05\n",
      "Epoch 3971, Loss: 0.01013323117103937, Final Batch Loss: 3.9318506424024235e-06\n",
      "Epoch 3972, Loss: 0.0011942523124162108, Final Batch Loss: 0.0006656904006376863\n",
      "Epoch 3973, Loss: 0.0006265873344091233, Final Batch Loss: 3.4476910514058545e-05\n",
      "Epoch 3974, Loss: 0.0005292188143357635, Final Batch Loss: 0.00019103664089925587\n",
      "Epoch 3975, Loss: 0.010287748198607005, Final Batch Loss: 0.0002558891719672829\n",
      "Epoch 3976, Loss: 0.0012219029558764305, Final Batch Loss: 8.416616765316576e-06\n",
      "Epoch 3977, Loss: 0.0032749078309279867, Final Batch Loss: 0.00013115118781570345\n",
      "Epoch 3978, Loss: 0.001299833063967526, Final Batch Loss: 7.799657032592222e-05\n",
      "Epoch 3979, Loss: 0.0015296111050702166, Final Batch Loss: 0.0002838999207597226\n",
      "Epoch 3980, Loss: 0.002030786949035246, Final Batch Loss: 0.00018216473108623177\n",
      "Epoch 3981, Loss: 0.009839863188972231, Final Batch Loss: 9.120531467488036e-05\n",
      "Epoch 3982, Loss: 0.0030627170053776354, Final Batch Loss: 0.0005171681987121701\n",
      "Epoch 3983, Loss: 0.0014382659792318009, Final Batch Loss: 0.0008541761199012399\n",
      "Epoch 3984, Loss: 0.0024923794371716212, Final Batch Loss: 4.2944720917148516e-05\n",
      "Epoch 3985, Loss: 0.003198934871761594, Final Batch Loss: 0.0005698893801309168\n",
      "Epoch 3986, Loss: 0.0005636874811898451, Final Batch Loss: 5.0459970225347206e-05\n",
      "Epoch 3987, Loss: 0.001408740587066859, Final Batch Loss: 0.0009374037617817521\n",
      "Epoch 3988, Loss: 0.008936003659982816, Final Batch Loss: 4.2497813410591334e-05\n",
      "Epoch 3989, Loss: 0.0048473839124199, Final Batch Loss: 0.000744438380934298\n",
      "Epoch 3990, Loss: 0.0027127780485898256, Final Batch Loss: 0.0014420421794056892\n",
      "Epoch 3991, Loss: 0.0028310079760558438, Final Batch Loss: 5.179147046874277e-05\n",
      "Epoch 3992, Loss: 0.009259321610443294, Final Batch Loss: 0.0018710449803620577\n",
      "Epoch 3993, Loss: 0.0007943664240883663, Final Batch Loss: 0.00015500090376008302\n",
      "Epoch 3994, Loss: 0.0002966624633700121, Final Batch Loss: 4.188630191492848e-05\n",
      "Epoch 3995, Loss: 0.004175416921498254, Final Batch Loss: 0.002605554647743702\n",
      "Epoch 3996, Loss: 0.0004154661037318874, Final Batch Loss: 5.043792771175504e-05\n",
      "Epoch 3997, Loss: 0.0010356531529396307, Final Batch Loss: 9.915511327562854e-05\n",
      "Epoch 3998, Loss: 0.0006269150690059178, Final Batch Loss: 7.143402035580948e-05\n",
      "Epoch 3999, Loss: 0.000753764710680116, Final Batch Loss: 5.809313370264135e-05\n",
      "Epoch 4000, Loss: 0.0004813708037545439, Final Batch Loss: 8.667738438816741e-05\n",
      "Epoch 4001, Loss: 0.0005078810063423589, Final Batch Loss: 0.00020551757188513875\n",
      "Epoch 4002, Loss: 0.0004423926511663012, Final Batch Loss: 6.452259549405426e-05\n",
      "Epoch 4003, Loss: 0.000384897643016302, Final Batch Loss: 1.4116327292867936e-05\n",
      "Epoch 4004, Loss: 0.00036367462780617643, Final Batch Loss: 8.541262650396675e-05\n",
      "Epoch 4005, Loss: 0.0007007527146924986, Final Batch Loss: 1.4635477782576345e-05\n",
      "Epoch 4006, Loss: 0.000616294772044057, Final Batch Loss: 0.0001518775534350425\n",
      "Epoch 4007, Loss: 0.0003980672795478313, Final Batch Loss: 5.974894975224743e-06\n",
      "Epoch 4008, Loss: 0.0006463866629928816, Final Batch Loss: 0.00047925676335580647\n",
      "Epoch 4009, Loss: 0.0004112917958991602, Final Batch Loss: 0.00010547534475335851\n",
      "Epoch 4010, Loss: 0.00014082363395573338, Final Batch Loss: 1.9989069187431596e-05\n",
      "Epoch 4011, Loss: 0.00035134507470502285, Final Batch Loss: 7.708155862928834e-06\n",
      "Epoch 4012, Loss: 0.00017549406311445637, Final Batch Loss: 1.4925149116606917e-05\n",
      "Epoch 4013, Loss: 0.004409710982145043, Final Batch Loss: 6.492577085737139e-05\n",
      "Epoch 4014, Loss: 0.0009235380466634524, Final Batch Loss: 7.472441939171404e-05\n",
      "Epoch 4015, Loss: 0.0006954898262847564, Final Batch Loss: 1.4868913240206894e-05\n",
      "Epoch 4016, Loss: 0.0003442868219281081, Final Batch Loss: 4.4998840166954324e-05\n",
      "Epoch 4017, Loss: 0.000246191675614682, Final Batch Loss: 1.942727976711467e-05\n",
      "Epoch 4018, Loss: 0.001005962818453554, Final Batch Loss: 6.266382843023166e-05\n",
      "Epoch 4019, Loss: 0.00028957126778550446, Final Batch Loss: 1.1971169442404062e-05\n",
      "Epoch 4020, Loss: 0.0027836293520522304, Final Batch Loss: 3.376718086656183e-05\n",
      "Epoch 4021, Loss: 0.00023965487707755528, Final Batch Loss: 5.977472756057978e-05\n",
      "Epoch 4022, Loss: 0.0012895537074655294, Final Batch Loss: 3.869280044455081e-05\n",
      "Epoch 4023, Loss: 0.00015985159552656114, Final Batch Loss: 1.900561255752109e-05\n",
      "Epoch 4024, Loss: 0.019468818942186772, Final Batch Loss: 2.784392017929349e-05\n",
      "Epoch 4025, Loss: 0.00040496686415281147, Final Batch Loss: 0.00014233851106837392\n",
      "Epoch 4026, Loss: 0.0003209966218946647, Final Batch Loss: 2.033053760897019e-06\n",
      "Epoch 4027, Loss: 0.03230622498085722, Final Batch Loss: 0.00018246912804897875\n",
      "Epoch 4028, Loss: 0.0005568276828853413, Final Batch Loss: 0.00014690880198031664\n",
      "Epoch 4029, Loss: 0.0006005917457514443, Final Batch Loss: 0.00019713788060471416\n",
      "Epoch 4030, Loss: 0.0023218762144097127, Final Batch Loss: 0.0001453569857403636\n",
      "Epoch 4031, Loss: 0.007935329205793096, Final Batch Loss: 4.576596984406933e-05\n",
      "Epoch 4032, Loss: 0.03233711155917263, Final Batch Loss: 0.001201271777972579\n",
      "Epoch 4033, Loss: 0.0004319024649248604, Final Batch Loss: 2.059369080598117e-06\n",
      "Epoch 4034, Loss: 0.000613757292740047, Final Batch Loss: 0.0004609796451404691\n",
      "Epoch 4035, Loss: 0.00029433147074087174, Final Batch Loss: 5.513224550668383e-06\n",
      "Epoch 4036, Loss: 0.0006215862035787723, Final Batch Loss: 4.830896159546683e-06\n",
      "Epoch 4037, Loss: 0.019625982698926236, Final Batch Loss: 2.250329271191731e-05\n",
      "Epoch 4038, Loss: 0.00047295282092818525, Final Batch Loss: 1.266278741240967e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4039, Loss: 0.00017009950170177035, Final Batch Loss: 5.584625614574179e-05\n",
      "Epoch 4040, Loss: 0.014654952985438285, Final Batch Loss: 0.0013530261348932981\n",
      "Epoch 4041, Loss: 0.0027126886725454824, Final Batch Loss: 0.0021731737069785595\n",
      "Epoch 4042, Loss: 0.000895769553608261, Final Batch Loss: 1.9954150047851726e-05\n",
      "Epoch 4043, Loss: 0.0012626060415641405, Final Batch Loss: 0.0003823276492767036\n",
      "Epoch 4044, Loss: 0.00035465000109979883, Final Batch Loss: 9.4209099188447e-06\n",
      "Epoch 4045, Loss: 0.0012324248764343793, Final Batch Loss: 3.132129859295674e-05\n",
      "Epoch 4046, Loss: 0.0034567094007798005, Final Batch Loss: 4.138124859309755e-05\n",
      "Epoch 4047, Loss: 0.04063584381765395, Final Batch Loss: 0.03823341801762581\n",
      "Epoch 4048, Loss: 0.0003980632245657034, Final Batch Loss: 3.807526809396222e-05\n",
      "Epoch 4049, Loss: 0.001658919602050446, Final Batch Loss: 0.0001144224515883252\n",
      "Epoch 4050, Loss: 0.011316018382785842, Final Batch Loss: 0.0006411939393728971\n",
      "Epoch 4051, Loss: 0.0010849497484741732, Final Batch Loss: 0.0003017308481503278\n",
      "Epoch 4052, Loss: 0.018663902414118638, Final Batch Loss: 3.165899761370383e-05\n",
      "Epoch 4053, Loss: 0.00041105645141215064, Final Batch Loss: 9.059109288500622e-05\n",
      "Epoch 4054, Loss: 0.0004928347989334725, Final Batch Loss: 1.8625560187501833e-05\n",
      "Epoch 4055, Loss: 0.016670217224600492, Final Batch Loss: 5.885661448701285e-05\n",
      "Epoch 4056, Loss: 0.0013968604180263355, Final Batch Loss: 0.0011997581459581852\n",
      "Epoch 4057, Loss: 0.020979245789931156, Final Batch Loss: 0.0001774850970832631\n",
      "Epoch 4058, Loss: 0.00136865243439388, Final Batch Loss: 0.00015129071834962815\n",
      "Epoch 4059, Loss: 0.00031134784148889594, Final Batch Loss: 3.511982868076302e-05\n",
      "Epoch 4060, Loss: 0.005868415631994139, Final Batch Loss: 8.79047074704431e-05\n",
      "Epoch 4061, Loss: 0.0003395612147869542, Final Batch Loss: 1.2623539078049362e-05\n",
      "Epoch 4062, Loss: 0.0014251344691729173, Final Batch Loss: 4.2725027014967054e-05\n",
      "Epoch 4063, Loss: 0.0007213756325654685, Final Batch Loss: 0.00020044736447744071\n",
      "Epoch 4064, Loss: 0.0014670675445813686, Final Batch Loss: 0.0013362314784899354\n",
      "Epoch 4065, Loss: 0.0011322574464429636, Final Batch Loss: 7.377278234343976e-05\n",
      "Epoch 4066, Loss: 0.0013544353205361404, Final Batch Loss: 4.295180406188592e-05\n",
      "Epoch 4067, Loss: 0.02925181926912046, Final Batch Loss: 0.0012532881228253245\n",
      "Epoch 4068, Loss: 0.00043610915054159705, Final Batch Loss: 1.7531261619296856e-05\n",
      "Epoch 4069, Loss: 0.0007544386535300873, Final Batch Loss: 0.0005056762602180243\n",
      "Epoch 4070, Loss: 0.0007775686171953566, Final Batch Loss: 7.492650183849037e-05\n",
      "Epoch 4071, Loss: 0.014207187792635523, Final Batch Loss: 0.00012280992814339697\n",
      "Epoch 4072, Loss: 0.0003175756864948198, Final Batch Loss: 2.1909494535066187e-05\n",
      "Epoch 4073, Loss: 0.0009704386211524252, Final Batch Loss: 0.000723650271538645\n",
      "Epoch 4074, Loss: 0.002780808870738838, Final Batch Loss: 4.467600956559181e-05\n",
      "Epoch 4075, Loss: 0.0006034503821865655, Final Batch Loss: 0.0002669063687790185\n",
      "Epoch 4076, Loss: 0.0012988803537155036, Final Batch Loss: 0.0010504934471100569\n",
      "Epoch 4077, Loss: 0.000977695279289037, Final Batch Loss: 0.00031853633117862046\n",
      "Epoch 4078, Loss: 0.0014737412202521227, Final Batch Loss: 0.00033149131922982633\n",
      "Epoch 4079, Loss: 0.0019954327872255817, Final Batch Loss: 0.0003734801721293479\n",
      "Epoch 4080, Loss: 0.001670266750807059, Final Batch Loss: 0.0011551842326298356\n",
      "Epoch 4081, Loss: 0.0019132193265249953, Final Batch Loss: 0.0008374734898097813\n",
      "Epoch 4082, Loss: 0.0002451814907544758, Final Batch Loss: 8.512572094332427e-05\n",
      "Epoch 4083, Loss: 0.0009556724908179604, Final Batch Loss: 0.0007590753375552595\n",
      "Epoch 4084, Loss: 0.0005446963841677643, Final Batch Loss: 4.081113002030179e-05\n",
      "Epoch 4085, Loss: 0.001346577271760907, Final Batch Loss: 0.00025579758221283555\n",
      "Epoch 4086, Loss: 0.0005998525084578432, Final Batch Loss: 0.00022857960721012205\n",
      "Epoch 4087, Loss: 0.001008021139568882, Final Batch Loss: 0.0005315617308951914\n",
      "Epoch 4088, Loss: 0.0002658114754012786, Final Batch Loss: 8.04705050541088e-05\n",
      "Epoch 4089, Loss: 0.011589620497943542, Final Batch Loss: 0.00020268181106075644\n",
      "Epoch 4090, Loss: 0.00039444902358809486, Final Batch Loss: 0.00020828322158195078\n",
      "Epoch 4091, Loss: 0.0005084545300633181, Final Batch Loss: 9.183489601127803e-05\n",
      "Epoch 4092, Loss: 0.009728505108796526, Final Batch Loss: 0.00018633891886565834\n",
      "Epoch 4093, Loss: 0.0002679245817489573, Final Batch Loss: 1.5079604963830207e-05\n",
      "Epoch 4094, Loss: 0.0007985758420545608, Final Batch Loss: 1.0986317647621036e-05\n",
      "Epoch 4095, Loss: 0.12374903299087237, Final Batch Loss: 0.1234510987997055\n",
      "Epoch 4096, Loss: 0.00085452994244406, Final Batch Loss: 3.058179936488159e-05\n",
      "Epoch 4097, Loss: 0.004525299922534032, Final Batch Loss: 5.001924364478327e-05\n",
      "Epoch 4098, Loss: 0.000736515839889762, Final Batch Loss: 1.8372867998550646e-05\n",
      "Epoch 4099, Loss: 0.0009090826206374913, Final Batch Loss: 4.7320485464297235e-05\n",
      "Epoch 4100, Loss: 0.0009634575926611433, Final Batch Loss: 2.0105782823520713e-05\n",
      "Epoch 4101, Loss: 0.003009132411534665, Final Batch Loss: 5.403528848546557e-05\n",
      "Epoch 4102, Loss: 0.0014530540211126208, Final Batch Loss: 0.00012319794041104615\n",
      "Epoch 4103, Loss: 0.0011844532182294643, Final Batch Loss: 2.9361772249103524e-05\n",
      "Epoch 4104, Loss: 0.0008451858684566105, Final Batch Loss: 0.00012398313265293837\n",
      "Epoch 4105, Loss: 0.0008524140357621945, Final Batch Loss: 3.220432699890807e-05\n",
      "Epoch 4106, Loss: 0.0005362996089388616, Final Batch Loss: 6.570465484401211e-05\n",
      "Epoch 4107, Loss: 0.0008814164539217018, Final Batch Loss: 0.00012435710232239217\n",
      "Epoch 4108, Loss: 0.0025810148817981826, Final Batch Loss: 0.0001958022330654785\n",
      "Epoch 4109, Loss: 0.0011234352896281052, Final Batch Loss: 0.00013279542326927185\n",
      "Epoch 4110, Loss: 0.0011084261313953903, Final Batch Loss: 3.367980752955191e-05\n",
      "Epoch 4111, Loss: 0.005649182599881897, Final Batch Loss: 0.0002554877719376236\n",
      "Epoch 4112, Loss: 0.0017390188004355878, Final Batch Loss: 0.00010612933692755178\n",
      "Epoch 4113, Loss: 0.0004566388906823704, Final Batch Loss: 2.7738262360799126e-05\n",
      "Epoch 4114, Loss: 0.0003690918965730816, Final Batch Loss: 9.80348777375184e-05\n",
      "Epoch 4115, Loss: 0.005154400905666989, Final Batch Loss: 5.765368769061752e-05\n",
      "Epoch 4116, Loss: 0.0007494087185477838, Final Batch Loss: 0.00013893545838072896\n",
      "Epoch 4117, Loss: 0.0004412206726556178, Final Batch Loss: 0.0001212363931699656\n",
      "Epoch 4118, Loss: 0.0006758795752830338, Final Batch Loss: 4.657723548007198e-05\n",
      "Epoch 4119, Loss: 0.0010930840726359747, Final Batch Loss: 0.0004995739436708391\n",
      "Epoch 4120, Loss: 0.000663073200485087, Final Batch Loss: 0.00027404469437897205\n",
      "Epoch 4121, Loss: 0.0007198648017947562, Final Batch Loss: 0.00026556829106993973\n",
      "Epoch 4122, Loss: 0.00257175193837611, Final Batch Loss: 7.437445310642943e-05\n",
      "Epoch 4123, Loss: 0.0019995859838672914, Final Batch Loss: 0.00023950685863383114\n",
      "Epoch 4124, Loss: 0.000260686607362004, Final Batch Loss: 3.4616510674823076e-05\n",
      "Epoch 4125, Loss: 0.00021123178612469928, Final Batch Loss: 1.2944216905452777e-05\n",
      "Epoch 4126, Loss: 0.00027281177062832285, Final Batch Loss: 2.35466577578336e-05\n",
      "Epoch 4127, Loss: 0.0013232896735644317, Final Batch Loss: 0.0005657747969962656\n",
      "Epoch 4128, Loss: 0.0002761912155619939, Final Batch Loss: 8.793327288003638e-05\n",
      "Epoch 4129, Loss: 0.001652140619853526, Final Batch Loss: 2.304301460753777e-06\n",
      "Epoch 4130, Loss: 0.0014088887364778202, Final Batch Loss: 3.2070151064544916e-05\n",
      "Epoch 4131, Loss: 0.00033540892763994634, Final Batch Loss: 6.889324868097901e-05\n",
      "Epoch 4132, Loss: 0.00018409222138870973, Final Batch Loss: 2.636095268826466e-05\n",
      "Epoch 4133, Loss: 0.002623438481350604, Final Batch Loss: 8.391296432819217e-05\n",
      "Epoch 4134, Loss: 0.0010039755716206855, Final Batch Loss: 0.0007309108623303473\n",
      "Epoch 4135, Loss: 0.001163500272014062, Final Batch Loss: 1.9606157366069965e-05\n",
      "Epoch 4136, Loss: 0.0006727424770360813, Final Batch Loss: 6.563953502336517e-05\n",
      "Epoch 4137, Loss: 0.022161092623719014, Final Batch Loss: 0.0003287559957243502\n",
      "Epoch 4138, Loss: 0.00015710750358266523, Final Batch Loss: 3.3636961234151386e-06\n",
      "Epoch 4139, Loss: 0.0056884812056523515, Final Batch Loss: 9.922781828208826e-06\n",
      "Epoch 4140, Loss: 0.0012384395486151334, Final Batch Loss: 0.00019363767933100462\n",
      "Epoch 4141, Loss: 0.0019416096638451563, Final Batch Loss: 6.388531619450077e-05\n",
      "Epoch 4142, Loss: 0.0008910187752917409, Final Batch Loss: 0.0001640485570533201\n",
      "Epoch 4143, Loss: 0.0009199204578180797, Final Batch Loss: 0.00019653269555419683\n",
      "Epoch 4144, Loss: 0.0009502530188001401, Final Batch Loss: 6.779325758543564e-06\n",
      "Epoch 4145, Loss: 0.0015936841109578381, Final Batch Loss: 0.0001479153143009171\n",
      "Epoch 4146, Loss: 0.0021077436358609702, Final Batch Loss: 1.6514164599357173e-05\n",
      "Epoch 4147, Loss: 0.001141480835940456, Final Batch Loss: 3.847391781164333e-06\n",
      "Epoch 4148, Loss: 0.008669178129821375, Final Batch Loss: 1.4639407709182706e-05\n",
      "Epoch 4149, Loss: 0.0002846089882950764, Final Batch Loss: 7.494668534491211e-05\n",
      "Epoch 4150, Loss: 0.0022305159855022794, Final Batch Loss: 0.00023066926223691553\n",
      "Epoch 4151, Loss: 0.0005012287565477891, Final Batch Loss: 1.7128419131040573e-05\n",
      "Epoch 4152, Loss: 0.0006561578520631883, Final Batch Loss: 2.7596706786425784e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4153, Loss: 0.0003821876125584822, Final Batch Loss: 6.861196015961468e-05\n",
      "Epoch 4154, Loss: 0.0006210622832441004, Final Batch Loss: 0.00022885976068209857\n",
      "Epoch 4155, Loss: 0.0020582602237482206, Final Batch Loss: 8.685295142640825e-06\n",
      "Epoch 4156, Loss: 0.0001675300054557738, Final Batch Loss: 8.204028745240066e-06\n",
      "Epoch 4157, Loss: 0.0001804221728889388, Final Batch Loss: 4.685016756411642e-05\n",
      "Epoch 4158, Loss: 0.00028161496129541774, Final Batch Loss: 7.547003860963741e-06\n",
      "Epoch 4159, Loss: 0.0002594169654912548, Final Batch Loss: 3.8815458538010716e-05\n",
      "Epoch 4160, Loss: 0.000646795381726406, Final Batch Loss: 7.74378077039728e-06\n",
      "Epoch 4161, Loss: 0.00030609954046667553, Final Batch Loss: 1.593148772371933e-05\n",
      "Epoch 4162, Loss: 0.00023645421606488526, Final Batch Loss: 3.646738696261309e-05\n",
      "Epoch 4163, Loss: 0.010163541461224668, Final Batch Loss: 5.6741471780696884e-05\n",
      "Epoch 4164, Loss: 0.00021277423184073996, Final Batch Loss: 3.3545766200404614e-06\n",
      "Epoch 4165, Loss: 0.00022731358421879122, Final Batch Loss: 9.56202802626649e-06\n",
      "Epoch 4166, Loss: 0.0009396542300237343, Final Batch Loss: 0.00016258431423921138\n",
      "Epoch 4167, Loss: 0.0005972573562758043, Final Batch Loss: 0.00021238898625597358\n",
      "Epoch 4168, Loss: 0.0007553713230663561, Final Batch Loss: 0.0006679994403384626\n",
      "Epoch 4169, Loss: 0.000882809872564394, Final Batch Loss: 4.476379035622813e-05\n",
      "Epoch 4170, Loss: 0.00301854362260201, Final Batch Loss: 2.0028410290251486e-05\n",
      "Epoch 4171, Loss: 0.0003295544665888883, Final Batch Loss: 0.00010450005356688052\n",
      "Epoch 4172, Loss: 0.00010311342521163169, Final Batch Loss: 2.6339010219089687e-05\n",
      "Epoch 4173, Loss: 0.00041932200110750273, Final Batch Loss: 0.0002407854190096259\n",
      "Epoch 4174, Loss: 0.006549604448082391, Final Batch Loss: 0.0007221830310299993\n",
      "Epoch 4175, Loss: 0.00016217171105381567, Final Batch Loss: 4.407666710903868e-05\n",
      "Epoch 4176, Loss: 0.00034792579208442476, Final Batch Loss: 2.7211242922930978e-05\n",
      "Epoch 4177, Loss: 0.00021311797354428563, Final Batch Loss: 8.326293027494103e-05\n",
      "Epoch 4178, Loss: 0.00044434053052100353, Final Batch Loss: 0.0001474703021813184\n",
      "Epoch 4179, Loss: 0.00024636564921820536, Final Batch Loss: 5.061778210802004e-06\n",
      "Epoch 4180, Loss: 0.014044804105651565, Final Batch Loss: 0.011924196034669876\n",
      "Epoch 4181, Loss: 0.001780302896804642, Final Batch Loss: 0.00030087254708632827\n",
      "Epoch 4182, Loss: 0.0019318089325679466, Final Batch Loss: 0.0001129586307797581\n",
      "Epoch 4183, Loss: 0.0006734170056006406, Final Batch Loss: 9.093798871617764e-05\n",
      "Epoch 4184, Loss: 0.0033240371813008096, Final Batch Loss: 0.0006788268219679594\n",
      "Epoch 4185, Loss: 0.0006835086969658732, Final Batch Loss: 0.00031218939693644643\n",
      "Epoch 4186, Loss: 0.000939623834710801, Final Batch Loss: 3.0421595511143096e-05\n",
      "Epoch 4187, Loss: 0.00025149215434794314, Final Batch Loss: 8.290148980449885e-05\n",
      "Epoch 4188, Loss: 0.03330949260180205, Final Batch Loss: 0.00038172912900336087\n",
      "Epoch 4189, Loss: 0.0003833059636235703, Final Batch Loss: 0.00011269725655438378\n",
      "Epoch 4190, Loss: 0.00029378158592408, Final Batch Loss: 2.526777961975313e-06\n",
      "Epoch 4191, Loss: 0.0002255572517242399, Final Batch Loss: 8.853559120325372e-05\n",
      "Epoch 4192, Loss: 0.00022854194776300574, Final Batch Loss: 3.034958353964612e-05\n",
      "Epoch 4193, Loss: 0.0002866407412511762, Final Batch Loss: 0.00011167360935360193\n",
      "Epoch 4194, Loss: 0.001146758091636002, Final Batch Loss: 1.722059096209705e-05\n",
      "Epoch 4195, Loss: 0.0001388471919199219, Final Batch Loss: 2.372786184423603e-06\n",
      "Epoch 4196, Loss: 0.029259709219331853, Final Batch Loss: 0.0004173448833171278\n",
      "Epoch 4197, Loss: 0.00040729710599407554, Final Batch Loss: 7.838109013391659e-05\n",
      "Epoch 4198, Loss: 0.011412526553613134, Final Batch Loss: 3.3462169085396454e-05\n",
      "Epoch 4199, Loss: 0.00047769695629540365, Final Batch Loss: 8.156792318914086e-05\n",
      "Epoch 4200, Loss: 0.00038016264988982584, Final Batch Loss: 1.0718335033743642e-05\n",
      "Epoch 4201, Loss: 0.0004656182190956315, Final Batch Loss: 0.00026588793843984604\n",
      "Epoch 4202, Loss: 0.0010448057364556007, Final Batch Loss: 0.00017672631656751037\n",
      "Epoch 4203, Loss: 0.00043242085484962445, Final Batch Loss: 0.00015524739865213633\n",
      "Epoch 4204, Loss: 0.0014842553646303713, Final Batch Loss: 0.0002584577305242419\n",
      "Epoch 4205, Loss: 0.0025731808973432635, Final Batch Loss: 0.0019153517205268145\n",
      "Epoch 4206, Loss: 0.0005744131667597685, Final Batch Loss: 3.408817065064795e-05\n",
      "Epoch 4207, Loss: 0.00013837954429618549, Final Batch Loss: 1.659336885495577e-05\n",
      "Epoch 4208, Loss: 0.00022685509156872286, Final Batch Loss: 8.100454579107463e-05\n",
      "Epoch 4209, Loss: 0.0011032984402845614, Final Batch Loss: 6.913897232152522e-05\n",
      "Epoch 4210, Loss: 0.0003379013687663246, Final Batch Loss: 5.529428017325699e-05\n",
      "Epoch 4211, Loss: 0.0038179847379069543, Final Batch Loss: 1.538728793093469e-05\n",
      "Epoch 4212, Loss: 0.00032359379474655725, Final Batch Loss: 5.670631435350515e-05\n",
      "Epoch 4213, Loss: 0.0008317249958054163, Final Batch Loss: 4.65140474261716e-05\n",
      "Epoch 4214, Loss: 0.00022488073318527313, Final Batch Loss: 2.276079794683028e-05\n",
      "Epoch 4215, Loss: 0.0001625558525120141, Final Batch Loss: 1.1407852980482858e-05\n",
      "Epoch 4216, Loss: 0.0006606951665162342, Final Batch Loss: 1.421333035978023e-05\n",
      "Epoch 4217, Loss: 0.0002783563068078365, Final Batch Loss: 7.684605952817947e-05\n",
      "Epoch 4218, Loss: 0.0015796564121046686, Final Batch Loss: 0.00039480923442170024\n",
      "Epoch 4219, Loss: 0.00044992921175435185, Final Batch Loss: 4.3364361772546545e-05\n",
      "Epoch 4220, Loss: 0.0004298875737731578, Final Batch Loss: 0.00018509446817915887\n",
      "Epoch 4221, Loss: 0.0012738655113935238, Final Batch Loss: 5.980256901239045e-05\n",
      "Epoch 4222, Loss: 0.00012870681598542433, Final Batch Loss: 2.9702681786147878e-05\n",
      "Epoch 4223, Loss: 0.00031445805143448524, Final Batch Loss: 1.4642435417044908e-05\n",
      "Epoch 4224, Loss: 0.00015149197315622587, Final Batch Loss: 4.9792026402428746e-05\n",
      "Epoch 4225, Loss: 0.001196597289890633, Final Batch Loss: 4.287563206162304e-05\n",
      "Epoch 4226, Loss: 0.0016398360053244687, Final Batch Loss: 6.578141892532585e-06\n",
      "Epoch 4227, Loss: 0.0004921758636555751, Final Batch Loss: 2.5210523745045066e-05\n",
      "Epoch 4228, Loss: 0.033039686129541224, Final Batch Loss: 2.8273514544707723e-05\n",
      "Epoch 4229, Loss: 0.00044981152950640535, Final Batch Loss: 8.32948262541322e-06\n",
      "Epoch 4230, Loss: 0.00041492494938211166, Final Batch Loss: 0.00019710180640686303\n",
      "Epoch 4231, Loss: 0.00010045708131656284, Final Batch Loss: 4.33606965088984e-06\n",
      "Epoch 4232, Loss: 0.00020276939540053718, Final Batch Loss: 6.580235640285537e-05\n",
      "Epoch 4233, Loss: 0.0010824037608472281, Final Batch Loss: 4.9859201681101695e-05\n",
      "Epoch 4234, Loss: 0.0017046059874701314, Final Batch Loss: 4.253074439475313e-05\n",
      "Epoch 4235, Loss: 0.0017368347980664112, Final Batch Loss: 0.000719265139196068\n",
      "Epoch 4236, Loss: 0.00044940119551029056, Final Batch Loss: 0.00011989980703219771\n",
      "Epoch 4237, Loss: 0.001912518226163229, Final Batch Loss: 0.00018300530791748315\n",
      "Epoch 4238, Loss: 0.00017230170851689763, Final Batch Loss: 1.2463266102713533e-05\n",
      "Epoch 4239, Loss: 0.00037912148400209844, Final Batch Loss: 0.0002455479698255658\n",
      "Epoch 4240, Loss: 0.0007346519869315671, Final Batch Loss: 0.0003224509418942034\n",
      "Epoch 4241, Loss: 0.00014021779315953609, Final Batch Loss: 1.8898210328188725e-05\n",
      "Epoch 4242, Loss: 0.016378570300730644, Final Batch Loss: 3.472860043984838e-05\n",
      "Epoch 4243, Loss: 0.00025592188649170566, Final Batch Loss: 4.255260500940494e-05\n",
      "Epoch 4244, Loss: 0.0008347462571691722, Final Batch Loss: 4.873923899140209e-05\n",
      "Epoch 4245, Loss: 0.0013348763750400394, Final Batch Loss: 8.576846994401421e-06\n",
      "Epoch 4246, Loss: 0.00025030469623743556, Final Batch Loss: 3.2408384868176654e-05\n",
      "Epoch 4247, Loss: 0.0002060849083136418, Final Batch Loss: 0.00012609572149813175\n",
      "Epoch 4248, Loss: 0.0018206939421361312, Final Batch Loss: 0.001021724659949541\n",
      "Epoch 4249, Loss: 0.0006135470393928699, Final Batch Loss: 0.0004064071399625391\n",
      "Epoch 4250, Loss: 0.00024542838627894525, Final Batch Loss: 3.806132008321583e-05\n",
      "Epoch 4251, Loss: 0.011758968261347036, Final Batch Loss: 2.2422098481911235e-05\n",
      "Epoch 4252, Loss: 0.0005074052837699128, Final Batch Loss: 8.164876635419205e-05\n",
      "Epoch 4253, Loss: 0.00031414732120538247, Final Batch Loss: 4.9523810048413e-06\n",
      "Epoch 4254, Loss: 0.00017404195978087955, Final Batch Loss: 4.492025254876353e-05\n",
      "Epoch 4255, Loss: 0.0003202364393928292, Final Batch Loss: 7.702515176788438e-06\n",
      "Epoch 4256, Loss: 0.0003213045511074597, Final Batch Loss: 0.000258959160419181\n",
      "Epoch 4257, Loss: 0.05001653497629377, Final Batch Loss: 0.04998994618654251\n",
      "Epoch 4258, Loss: 0.002971407566747075, Final Batch Loss: 7.298022410395788e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4259, Loss: 0.0009673440017650137, Final Batch Loss: 4.504378011915833e-05\n",
      "Epoch 4260, Loss: 0.03856168447055097, Final Batch Loss: 1.3178783774492331e-05\n",
      "Epoch 4261, Loss: 0.001090425910660997, Final Batch Loss: 0.000273455458227545\n",
      "Epoch 4262, Loss: 0.00018465283574187197, Final Batch Loss: 1.5580511899315752e-05\n",
      "Epoch 4263, Loss: 0.0007486142144443875, Final Batch Loss: 7.556022865173873e-07\n",
      "Epoch 4264, Loss: 0.0004632756972569041, Final Batch Loss: 4.4903870730195194e-05\n",
      "Epoch 4265, Loss: 0.0010939703606709372, Final Batch Loss: 0.0002131616638507694\n",
      "Epoch 4266, Loss: 0.001264184485989972, Final Batch Loss: 1.2227645129314624e-05\n",
      "Epoch 4267, Loss: 0.000476261386211263, Final Batch Loss: 4.172524131718092e-05\n",
      "Epoch 4268, Loss: 0.0007184824635260156, Final Batch Loss: 6.497313734143972e-05\n",
      "Epoch 4269, Loss: 0.0007103640691639157, Final Batch Loss: 1.7283493434661068e-05\n",
      "Epoch 4270, Loss: 0.010373471923230682, Final Batch Loss: 3.8113423215691e-05\n",
      "Epoch 4271, Loss: 0.0024944990291260183, Final Batch Loss: 0.00179823930375278\n",
      "Epoch 4272, Loss: 0.0004246208118274808, Final Batch Loss: 9.545563079882413e-05\n",
      "Epoch 4273, Loss: 0.049492473653117486, Final Batch Loss: 3.68695255019702e-05\n",
      "Epoch 4274, Loss: 0.0003786163706536172, Final Batch Loss: 2.1354586351662874e-05\n",
      "Epoch 4275, Loss: 0.0002000801541726105, Final Batch Loss: 2.6526728106546216e-05\n",
      "Epoch 4276, Loss: 0.1279735593489022, Final Batch Loss: 8.252001862274483e-05\n",
      "Epoch 4277, Loss: 0.0006598819381906651, Final Batch Loss: 0.00014564432785846293\n",
      "Epoch 4278, Loss: 0.004012226028862642, Final Batch Loss: 0.0002711526467464864\n",
      "Epoch 4279, Loss: 0.0009696263587102294, Final Batch Loss: 0.0002564155438449234\n",
      "Epoch 4280, Loss: 0.0013407215592451394, Final Batch Loss: 0.00021686014952138066\n",
      "Epoch 4281, Loss: 0.002244435774628073, Final Batch Loss: 0.0016615852946415544\n",
      "Epoch 4282, Loss: 0.004035447393107461, Final Batch Loss: 4.399544923217036e-05\n",
      "Epoch 4283, Loss: 0.00908227362560865, Final Batch Loss: 2.6191484721493907e-05\n",
      "Epoch 4284, Loss: 0.0008444094710284844, Final Batch Loss: 7.142368122003973e-05\n",
      "Epoch 4285, Loss: 0.0014476149444817565, Final Batch Loss: 7.544493564637378e-05\n",
      "Epoch 4286, Loss: 0.0009479057171120076, Final Batch Loss: 2.6269777663401328e-05\n",
      "Epoch 4287, Loss: 0.00048739267731434666, Final Batch Loss: 3.659598951344378e-05\n",
      "Epoch 4288, Loss: 0.002754486253252253, Final Batch Loss: 0.0020306226797401905\n",
      "Epoch 4289, Loss: 0.0017148108872788725, Final Batch Loss: 2.8019594537909143e-05\n",
      "Epoch 4290, Loss: 0.024791400752292247, Final Batch Loss: 3.836999167106114e-05\n",
      "Epoch 4291, Loss: 0.0010226167087239446, Final Batch Loss: 0.0004211746563669294\n",
      "Epoch 4292, Loss: 0.028212196821186808, Final Batch Loss: 2.5467425075476058e-05\n",
      "Epoch 4293, Loss: 0.004607125811162405, Final Batch Loss: 0.00018356840882916003\n",
      "Epoch 4294, Loss: 0.0024940655275713652, Final Batch Loss: 0.0001412894343957305\n",
      "Epoch 4295, Loss: 0.0009886214174912311, Final Batch Loss: 0.0001513646129751578\n",
      "Epoch 4296, Loss: 0.0011982166615780443, Final Batch Loss: 0.00012564353528432548\n",
      "Epoch 4297, Loss: 0.000434710513218306, Final Batch Loss: 9.616071474738419e-05\n",
      "Epoch 4298, Loss: 0.000776794822741067, Final Batch Loss: 5.518549369298853e-05\n",
      "Epoch 4299, Loss: 0.0009371828709845431, Final Batch Loss: 8.630697993794456e-05\n",
      "Epoch 4300, Loss: 0.0019800092559307814, Final Batch Loss: 0.0013884271029382944\n",
      "Epoch 4301, Loss: 0.0018324537959415466, Final Batch Loss: 9.792719356482849e-05\n",
      "Epoch 4302, Loss: 0.0020662651222664863, Final Batch Loss: 0.0006739851669408381\n",
      "Epoch 4303, Loss: 0.0024751144519541413, Final Batch Loss: 0.0012577419402077794\n",
      "Epoch 4304, Loss: 0.0002573475539975334, Final Batch Loss: 3.772768104681745e-05\n",
      "Epoch 4305, Loss: 0.0009282978862756863, Final Batch Loss: 0.000411853106925264\n",
      "Epoch 4306, Loss: 0.0005064708166173659, Final Batch Loss: 0.00027475605020299554\n",
      "Epoch 4307, Loss: 0.00046878750072210096, Final Batch Loss: 1.592792250448838e-05\n",
      "Epoch 4308, Loss: 0.0003921063771485933, Final Batch Loss: 1.225241521751741e-05\n",
      "Epoch 4309, Loss: 0.0002082462906400906, Final Batch Loss: 8.134175004670396e-06\n",
      "Epoch 4310, Loss: 0.00020574083100655116, Final Batch Loss: 3.6934517993358895e-05\n",
      "Epoch 4311, Loss: 0.001886208265204914, Final Batch Loss: 0.00019076808530371636\n",
      "Epoch 4312, Loss: 0.0009582484854036011, Final Batch Loss: 5.523477011593059e-05\n",
      "Epoch 4313, Loss: 0.0006196517642820254, Final Batch Loss: 0.00011226681817788631\n",
      "Epoch 4314, Loss: 0.0003977729211328551, Final Batch Loss: 3.0110681109363213e-05\n",
      "Epoch 4315, Loss: 0.00213017500936985, Final Batch Loss: 0.00010467803076608106\n",
      "Epoch 4316, Loss: 0.00037147280454519205, Final Batch Loss: 0.00020276680879760534\n",
      "Epoch 4317, Loss: 0.000631682232778985, Final Batch Loss: 0.0005279111792333424\n",
      "Epoch 4318, Loss: 0.001053982436133083, Final Batch Loss: 9.583350765751675e-05\n",
      "Epoch 4319, Loss: 0.004185262758255703, Final Batch Loss: 5.259270983515307e-05\n",
      "Epoch 4320, Loss: 0.0015960176751832478, Final Batch Loss: 3.4877761208917946e-05\n",
      "Epoch 4321, Loss: 0.0006846577125543263, Final Batch Loss: 5.827857967233285e-05\n",
      "Epoch 4322, Loss: 0.0006120574071246665, Final Batch Loss: 0.0002632480172906071\n",
      "Epoch 4323, Loss: 0.00057254645798821, Final Batch Loss: 0.00012800897820852697\n",
      "Epoch 4324, Loss: 0.0005229660855547991, Final Batch Loss: 1.2780832548742183e-05\n",
      "Epoch 4325, Loss: 0.00042728126936708577, Final Batch Loss: 0.0002551715006120503\n",
      "Epoch 4326, Loss: 0.000291046999336686, Final Batch Loss: 4.46112135250587e-05\n",
      "Epoch 4327, Loss: 0.00018401768375042593, Final Batch Loss: 5.331186002877075e-06\n",
      "Epoch 4328, Loss: 0.0003003167812494212, Final Batch Loss: 7.4539793786243536e-06\n",
      "Epoch 4329, Loss: 0.0020555576193146408, Final Batch Loss: 3.912465399480425e-05\n",
      "Epoch 4330, Loss: 0.003073503048653947, Final Batch Loss: 0.001456873258575797\n",
      "Epoch 4331, Loss: 0.0018046858604066074, Final Batch Loss: 9.360535477753729e-05\n",
      "Epoch 4332, Loss: 0.00036341967643238604, Final Batch Loss: 8.24841990834102e-05\n",
      "Epoch 4333, Loss: 0.00037016083842900116, Final Batch Loss: 2.3599948690389283e-05\n",
      "Epoch 4334, Loss: 0.00044585980958800064, Final Batch Loss: 6.1847320466768e-05\n",
      "Epoch 4335, Loss: 0.0008585691612097435, Final Batch Loss: 4.0860719309421256e-05\n",
      "Epoch 4336, Loss: 0.00021686573018087074, Final Batch Loss: 3.355692751938477e-05\n",
      "Epoch 4337, Loss: 0.001564990297993063, Final Batch Loss: 1.5382405763375573e-05\n",
      "Epoch 4338, Loss: 0.0005235074222582625, Final Batch Loss: 9.506637070444413e-06\n",
      "Epoch 4339, Loss: 0.0009171402125502937, Final Batch Loss: 0.0002151747903553769\n",
      "Epoch 4340, Loss: 0.00164144320115156, Final Batch Loss: 2.2183849068824202e-05\n",
      "Epoch 4341, Loss: 0.0008223728145821951, Final Batch Loss: 1.7713669876684435e-05\n",
      "Epoch 4342, Loss: 0.004567021522234427, Final Batch Loss: 7.505889516323805e-05\n",
      "Epoch 4343, Loss: 0.001587196224136278, Final Batch Loss: 0.0008379643550142646\n",
      "Epoch 4344, Loss: 0.0004141227418585913, Final Batch Loss: 3.192182703060098e-06\n",
      "Epoch 4345, Loss: 0.0015401442879010574, Final Batch Loss: 5.163282366993371e-06\n",
      "Epoch 4346, Loss: 0.0006552583072334528, Final Batch Loss: 0.00010572995233815163\n",
      "Epoch 4347, Loss: 0.0025780685828067362, Final Batch Loss: 0.0001982604735530913\n",
      "Epoch 4348, Loss: 0.0019527740541889216, Final Batch Loss: 1.3975985893921461e-05\n",
      "Epoch 4349, Loss: 0.0009390036675540614, Final Batch Loss: 3.823053339147009e-05\n",
      "Epoch 4350, Loss: 0.0005312632001732709, Final Batch Loss: 1.730647090880666e-05\n",
      "Epoch 4351, Loss: 0.00025481239572400227, Final Batch Loss: 2.7669717383105308e-05\n",
      "Epoch 4352, Loss: 0.0004758017967105843, Final Batch Loss: 0.00010562397801550105\n",
      "Epoch 4353, Loss: 0.0016888624886632897, Final Batch Loss: 0.00010115419718204066\n",
      "Epoch 4354, Loss: 0.0008303536915263976, Final Batch Loss: 5.8367722886032425e-06\n",
      "Epoch 4355, Loss: 0.0004831671903957613, Final Batch Loss: 0.0001757380086928606\n",
      "Epoch 4356, Loss: 0.0009729905832500663, Final Batch Loss: 8.387494744965807e-05\n",
      "Epoch 4357, Loss: 0.0011120997660327703, Final Batch Loss: 2.5919362087734044e-05\n",
      "Epoch 4358, Loss: 0.00019880139370798133, Final Batch Loss: 2.0764684450114146e-05\n",
      "Epoch 4359, Loss: 0.0009546880664856872, Final Batch Loss: 8.967204485088587e-05\n",
      "Epoch 4360, Loss: 0.0031049573299242184, Final Batch Loss: 7.131091842893511e-05\n",
      "Epoch 4361, Loss: 0.0006773302666260861, Final Batch Loss: 0.00016428431263193488\n",
      "Epoch 4362, Loss: 0.00019031320016438258, Final Batch Loss: 2.601330925244838e-05\n",
      "Epoch 4363, Loss: 0.0005154725167813012, Final Batch Loss: 4.216802699374966e-05\n",
      "Epoch 4364, Loss: 0.0012345552295300877, Final Batch Loss: 0.0011138851987197995\n",
      "Epoch 4365, Loss: 0.0007565825762867462, Final Batch Loss: 0.000302196218399331\n",
      "Epoch 4366, Loss: 0.0007007442463873303, Final Batch Loss: 1.2503954167186748e-05\n",
      "Epoch 4367, Loss: 0.00011138834634039085, Final Batch Loss: 2.009651143453084e-05\n",
      "Epoch 4368, Loss: 0.00014275070680014323, Final Batch Loss: 2.461873555148486e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4369, Loss: 0.00012468530007936351, Final Batch Loss: 3.789978336499189e-06\n",
      "Epoch 4370, Loss: 0.004743042547488585, Final Batch Loss: 0.0036539826542139053\n",
      "Epoch 4371, Loss: 0.0007657905039195612, Final Batch Loss: 5.81349149797461e-06\n",
      "Epoch 4372, Loss: 0.00011564866235858062, Final Batch Loss: 1.911193066916894e-05\n",
      "Epoch 4373, Loss: 0.00011403889379835164, Final Batch Loss: 3.0655462524009636e-06\n",
      "Epoch 4374, Loss: 0.00022696097039442975, Final Batch Loss: 3.55352894985117e-05\n",
      "Epoch 4375, Loss: 0.00021859839671378722, Final Batch Loss: 1.2023951967421453e-05\n",
      "Epoch 4376, Loss: 0.0013439402137009893, Final Batch Loss: 1.2891684491478372e-05\n",
      "Epoch 4377, Loss: 0.00039557163654535543, Final Batch Loss: 4.530029764282517e-05\n",
      "Epoch 4378, Loss: 0.0022717298415955156, Final Batch Loss: 1.786481516319327e-05\n",
      "Epoch 4379, Loss: 0.0002563169964560075, Final Batch Loss: 6.145231600385159e-05\n",
      "Epoch 4380, Loss: 0.0034946455707540736, Final Batch Loss: 0.00011264022759860381\n",
      "Epoch 4381, Loss: 0.000402190209570108, Final Batch Loss: 4.745762635138817e-05\n",
      "Epoch 4382, Loss: 0.0009068056015166803, Final Batch Loss: 0.0003321193507872522\n",
      "Epoch 4383, Loss: 0.00016774489449744578, Final Batch Loss: 4.593057747115381e-05\n",
      "Epoch 4384, Loss: 0.00018480415747035295, Final Batch Loss: 5.7182125601684675e-05\n",
      "Epoch 4385, Loss: 6.247430519579211e-05, Final Batch Loss: 8.345434252987616e-06\n",
      "Epoch 4386, Loss: 0.00044975853961659595, Final Batch Loss: 2.132489316863939e-05\n",
      "Epoch 4387, Loss: 0.000253715188591741, Final Batch Loss: 4.201820775051601e-05\n",
      "Epoch 4388, Loss: 0.03859880992240505, Final Batch Loss: 3.223145904485136e-05\n",
      "Epoch 4389, Loss: 0.0005385892072808929, Final Batch Loss: 0.00014134783123154193\n",
      "Epoch 4390, Loss: 0.00020110473815293517, Final Batch Loss: 5.5291162425419316e-05\n",
      "Epoch 4391, Loss: 0.00041114202031167224, Final Batch Loss: 0.0002463215496391058\n",
      "Epoch 4392, Loss: 0.00018085374267684529, Final Batch Loss: 3.9286187529796734e-06\n",
      "Epoch 4393, Loss: 0.0005954525295237545, Final Batch Loss: 0.00010993792966473848\n",
      "Epoch 4394, Loss: 0.00040133781294571236, Final Batch Loss: 3.908377402694896e-05\n",
      "Epoch 4395, Loss: 0.0002804014311550418, Final Batch Loss: 5.501699706655927e-05\n",
      "Epoch 4396, Loss: 0.0002087173161271494, Final Batch Loss: 2.253455932077486e-05\n",
      "Epoch 4397, Loss: 0.0006133715924079297, Final Batch Loss: 2.193223735957872e-05\n",
      "Epoch 4398, Loss: 0.0002771779008980957, Final Batch Loss: 8.978154255601112e-06\n",
      "Epoch 4399, Loss: 0.022309360199869843, Final Batch Loss: 4.745113619719632e-05\n",
      "Epoch 4400, Loss: 0.0004391383972688345, Final Batch Loss: 3.3378946682205424e-05\n",
      "Epoch 4401, Loss: 0.0005916769905525143, Final Batch Loss: 5.300359589455184e-06\n",
      "Epoch 4402, Loss: 0.0009594831499271095, Final Batch Loss: 0.0008345007663592696\n",
      "Epoch 4403, Loss: 0.00027802217300632037, Final Batch Loss: 4.2162006138823926e-05\n",
      "Epoch 4404, Loss: 0.000911620659280743, Final Batch Loss: 1.1317682037770282e-05\n",
      "Epoch 4405, Loss: 0.0009344629993393028, Final Batch Loss: 7.536321845691418e-06\n",
      "Epoch 4406, Loss: 0.0010471126333868597, Final Batch Loss: 0.0009315707720816135\n",
      "Epoch 4407, Loss: 0.00036075760681342217, Final Batch Loss: 3.909293809556402e-06\n",
      "Epoch 4408, Loss: 0.00034078370072165853, Final Batch Loss: 5.576051535172155e-06\n",
      "Epoch 4409, Loss: 0.01553364692881587, Final Batch Loss: 0.014189613983035088\n",
      "Epoch 4410, Loss: 0.0015015643957667635, Final Batch Loss: 9.766564289748203e-06\n",
      "Epoch 4411, Loss: 0.0004890302971034544, Final Batch Loss: 0.00030507793417200446\n",
      "Epoch 4412, Loss: 0.02326362137500837, Final Batch Loss: 0.021878723055124283\n",
      "Epoch 4413, Loss: 0.00048387241031377926, Final Batch Loss: 7.2343441388511565e-06\n",
      "Epoch 4414, Loss: 0.0002340299424758996, Final Batch Loss: 7.881316378188785e-06\n",
      "Epoch 4415, Loss: 0.0002459229099258664, Final Batch Loss: 0.0001091233134502545\n",
      "Epoch 4416, Loss: 0.0006422909427783452, Final Batch Loss: 3.233554889447987e-05\n",
      "Epoch 4417, Loss: 0.0006799543043598533, Final Batch Loss: 3.077283327002078e-05\n",
      "Epoch 4418, Loss: 0.00030226038870750926, Final Batch Loss: 1.5712270396761596e-05\n",
      "Epoch 4419, Loss: 0.0011207564239157364, Final Batch Loss: 4.660651757149026e-05\n",
      "Epoch 4420, Loss: 0.002635442611790495, Final Batch Loss: 4.41854317614343e-05\n",
      "Epoch 4421, Loss: 0.0010532596206758171, Final Batch Loss: 0.00011718605674104765\n",
      "Epoch 4422, Loss: 0.002149288120563142, Final Batch Loss: 0.001433268771506846\n",
      "Epoch 4423, Loss: 0.000493789240863407, Final Batch Loss: 6.780451803933829e-05\n",
      "Epoch 4424, Loss: 0.00038131123619677965, Final Batch Loss: 1.739804065437056e-05\n",
      "Epoch 4425, Loss: 0.0005293541689752601, Final Batch Loss: 3.526602449710481e-05\n",
      "Epoch 4426, Loss: 0.01990599332111742, Final Batch Loss: 4.327550777816214e-05\n",
      "Epoch 4427, Loss: 0.004678878940467257, Final Batch Loss: 6.49074136163108e-05\n",
      "Epoch 4428, Loss: 0.0027793918691259023, Final Batch Loss: 3.032701215488487e-06\n",
      "Epoch 4429, Loss: 0.03630789973431092, Final Batch Loss: 0.0005580034921877086\n",
      "Epoch 4430, Loss: 0.0005793758464278653, Final Batch Loss: 1.6945939933066256e-05\n",
      "Epoch 4431, Loss: 0.0012189718654553872, Final Batch Loss: 0.0006011916557326913\n",
      "Epoch 4432, Loss: 0.000337579996084969, Final Batch Loss: 1.5802341295056976e-05\n",
      "Epoch 4433, Loss: 0.011072049803260597, Final Batch Loss: 2.183655851695221e-05\n",
      "Epoch 4434, Loss: 0.0003145195441902615, Final Batch Loss: 5.828731082146987e-05\n",
      "Epoch 4435, Loss: 0.003964754772823653, Final Batch Loss: 3.331801781314425e-05\n",
      "Epoch 4436, Loss: 0.00044914320551470155, Final Batch Loss: 5.4690230172127485e-05\n",
      "Epoch 4437, Loss: 0.0020431298653420527, Final Batch Loss: 3.346379162394442e-05\n",
      "Epoch 4438, Loss: 0.0002659831297933124, Final Batch Loss: 1.2370870535960421e-05\n",
      "Epoch 4439, Loss: 0.002036958758253604, Final Batch Loss: 0.0002768820559140295\n",
      "Epoch 4440, Loss: 0.0003803858926403336, Final Batch Loss: 4.426001396495849e-05\n",
      "Epoch 4441, Loss: 0.004187638314760989, Final Batch Loss: 0.003850961569696665\n",
      "Epoch 4442, Loss: 0.001556535692543548, Final Batch Loss: 7.768975774524733e-05\n",
      "Epoch 4443, Loss: 0.00023520912873209454, Final Batch Loss: 5.4030067985877395e-05\n",
      "Epoch 4444, Loss: 0.00020445096470211865, Final Batch Loss: 4.251332575222477e-05\n",
      "Epoch 4445, Loss: 0.0009093693588511087, Final Batch Loss: 0.00015494217223022133\n",
      "Epoch 4446, Loss: 0.0005007482395740226, Final Batch Loss: 8.729752153158188e-05\n",
      "Epoch 4447, Loss: 0.0006307614880824985, Final Batch Loss: 6.17874547970132e-06\n",
      "Epoch 4448, Loss: 0.0004358998885436449, Final Batch Loss: 4.8196903662756085e-05\n",
      "Epoch 4449, Loss: 0.001602482567250263, Final Batch Loss: 1.5993475244613364e-05\n",
      "Epoch 4450, Loss: 0.017999221120589937, Final Batch Loss: 6.280767593125347e-06\n",
      "Epoch 4451, Loss: 0.0003059398331970442, Final Batch Loss: 4.8704168875701725e-06\n",
      "Epoch 4452, Loss: 0.0001749979210217134, Final Batch Loss: 5.0831364205805585e-05\n",
      "Epoch 4453, Loss: 0.0018787592853186652, Final Batch Loss: 0.0002511739730834961\n",
      "Epoch 4454, Loss: 0.0003946283977711573, Final Batch Loss: 2.261217377963476e-05\n",
      "Epoch 4455, Loss: 0.0002354255939280847, Final Batch Loss: 2.468317143211607e-05\n",
      "Epoch 4456, Loss: 0.0006881470544612966, Final Batch Loss: 2.679511089809239e-05\n",
      "Epoch 4457, Loss: 5.911256766921724e-05, Final Batch Loss: 2.5998992896347772e-06\n",
      "Epoch 4458, Loss: 0.0016189720854526968, Final Batch Loss: 1.1231361895625014e-05\n",
      "Epoch 4459, Loss: 0.0007887814790592529, Final Batch Loss: 0.00018579175230115652\n",
      "Epoch 4460, Loss: 0.0011064309946959838, Final Batch Loss: 4.436092058313079e-05\n",
      "Epoch 4461, Loss: 0.0003059717723772337, Final Batch Loss: 1.565927595947869e-05\n",
      "Epoch 4462, Loss: 0.002009074538364075, Final Batch Loss: 4.748762876261026e-05\n",
      "Epoch 4463, Loss: 0.0004815109050468891, Final Batch Loss: 1.242844791704556e-05\n",
      "Epoch 4464, Loss: 0.006554884919751203, Final Batch Loss: 0.0031245199497789145\n",
      "Epoch 4465, Loss: 0.0002102481385009014, Final Batch Loss: 7.140021807572339e-06\n",
      "Epoch 4466, Loss: 0.0005101545611978509, Final Batch Loss: 1.8677215848583728e-05\n",
      "Epoch 4467, Loss: 0.0005080271430415451, Final Batch Loss: 0.0004297704144846648\n",
      "Epoch 4468, Loss: 0.0003694782153615961, Final Batch Loss: 9.097909787669778e-05\n",
      "Epoch 4469, Loss: 0.00015927126923998003, Final Batch Loss: 2.2042777345632203e-05\n",
      "Epoch 4470, Loss: 0.00023593577111569175, Final Batch Loss: 3.790042455875664e-06\n",
      "Epoch 4471, Loss: 0.0003770196381083224, Final Batch Loss: 0.00017222491442225873\n",
      "Epoch 4472, Loss: 0.00034092300847987644, Final Batch Loss: 1.9120770957670175e-05\n",
      "Epoch 4473, Loss: 0.010411399814984179, Final Batch Loss: 0.010314266197383404\n",
      "Epoch 4474, Loss: 0.0005377414781833068, Final Batch Loss: 0.0001043818992911838\n",
      "Epoch 4475, Loss: 0.00021992695747030666, Final Batch Loss: 5.325170059222728e-05\n",
      "Epoch 4476, Loss: 0.001849046905135765, Final Batch Loss: 0.0013180261012166739\n",
      "Epoch 4477, Loss: 0.0007581826107525558, Final Batch Loss: 4.603095021593617e-06\n",
      "Epoch 4478, Loss: 0.0010071907272504177, Final Batch Loss: 0.0006700846133753657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4479, Loss: 0.0009408114667621703, Final Batch Loss: 7.84928761277115e-06\n",
      "Epoch 4480, Loss: 0.0010953871478704968, Final Batch Loss: 3.515420394251123e-05\n",
      "Epoch 4481, Loss: 0.00026857318312067946, Final Batch Loss: 6.17107143625617e-05\n",
      "Epoch 4482, Loss: 0.00013370869237405714, Final Batch Loss: 1.550593697174918e-05\n",
      "Epoch 4483, Loss: 0.0004524546839093091, Final Batch Loss: 5.484801658894867e-05\n",
      "Epoch 4484, Loss: 9.677182379164151e-05, Final Batch Loss: 1.812089431041386e-05\n",
      "Epoch 4485, Loss: 0.0006363990105455741, Final Batch Loss: 0.00010440043843118474\n",
      "Epoch 4486, Loss: 0.0007625403441124945, Final Batch Loss: 2.5689578251331113e-06\n",
      "Epoch 4487, Loss: 0.00018416788225295022, Final Batch Loss: 4.755991540150717e-05\n",
      "Epoch 4488, Loss: 0.0006265769043238834, Final Batch Loss: 0.0001661132409935817\n",
      "Epoch 4489, Loss: 0.00019717955001397058, Final Batch Loss: 7.247113535413519e-05\n",
      "Epoch 4490, Loss: 0.0002284891090766905, Final Batch Loss: 0.00014781726349610835\n",
      "Epoch 4491, Loss: 7.35297508072108e-05, Final Batch Loss: 6.3491688706562854e-06\n",
      "Epoch 4492, Loss: 0.0002165161206448829, Final Batch Loss: 6.095508979342412e-06\n",
      "Epoch 4493, Loss: 0.036093599435844226, Final Batch Loss: 0.00011333110160194337\n",
      "Epoch 4494, Loss: 0.0004962907032677322, Final Batch Loss: 0.0004334913974162191\n",
      "Epoch 4495, Loss: 0.0034887444139712898, Final Batch Loss: 5.3442140597326215e-06\n",
      "Epoch 4496, Loss: 0.00028521551212179475, Final Batch Loss: 4.2987714550690725e-05\n",
      "Epoch 4497, Loss: 0.0003850828306894982, Final Batch Loss: 1.4918226952431723e-05\n",
      "Epoch 4498, Loss: 0.005159221042049467, Final Batch Loss: 0.00014277287118602544\n",
      "Epoch 4499, Loss: 0.00027121227958559757, Final Batch Loss: 0.00012210660497657955\n",
      "Epoch 4500, Loss: 0.0007122222705220338, Final Batch Loss: 0.00028398362337611616\n",
      "Epoch 4501, Loss: 0.0006953105657885317, Final Batch Loss: 0.00047357328003272414\n",
      "Epoch 4502, Loss: 0.0004972346177964937, Final Batch Loss: 1.207497189170681e-05\n",
      "Epoch 4503, Loss: 0.03489206156154978, Final Batch Loss: 0.012275722809135914\n",
      "Epoch 4504, Loss: 0.0006598405743716285, Final Batch Loss: 8.326062379637733e-05\n",
      "Epoch 4505, Loss: 0.004140279860166629, Final Batch Loss: 5.654881988448324e-06\n",
      "Epoch 4506, Loss: 0.011742206808776245, Final Batch Loss: 2.2400194211513735e-05\n",
      "Epoch 4507, Loss: 0.009717029046441894, Final Batch Loss: 9.367350867250934e-05\n",
      "Epoch 4508, Loss: 0.03269616307807155, Final Batch Loss: 0.0004797198926098645\n",
      "Epoch 4509, Loss: 0.009625345381209627, Final Batch Loss: 0.0001387314696330577\n",
      "Epoch 4510, Loss: 0.01408408664065064, Final Batch Loss: 6.065410343580879e-05\n",
      "Epoch 4511, Loss: 0.005163647292647511, Final Batch Loss: 0.0008855581399984658\n",
      "Epoch 4512, Loss: 0.007933167216833681, Final Batch Loss: 0.000665504892822355\n",
      "Epoch 4513, Loss: 0.0030008768881089054, Final Batch Loss: 0.0006432096124626696\n",
      "Epoch 4514, Loss: 0.01734756618316169, Final Batch Loss: 5.4870859457878396e-05\n",
      "Epoch 4515, Loss: 0.0014240775744838174, Final Batch Loss: 3.9934089727466926e-05\n",
      "Epoch 4516, Loss: 0.014208870066795498, Final Batch Loss: 0.012675521895289421\n",
      "Epoch 4517, Loss: 0.0017503745111753233, Final Batch Loss: 6.523816409753636e-05\n",
      "Epoch 4518, Loss: 0.0014594990498153493, Final Batch Loss: 9.949284140020609e-05\n",
      "Epoch 4519, Loss: 0.001941765042829502, Final Batch Loss: 1.501099450251786e-05\n",
      "Epoch 4520, Loss: 0.007172022214945173, Final Batch Loss: 0.0001940894580911845\n",
      "Epoch 4521, Loss: 0.00314880080986768, Final Batch Loss: 0.00012944292393513024\n",
      "Epoch 4522, Loss: 0.0015082701138453558, Final Batch Loss: 0.00010016828309744596\n",
      "Epoch 4523, Loss: 0.004198808524051856, Final Batch Loss: 1.3299994861881714e-05\n",
      "Epoch 4524, Loss: 0.0013639820826938376, Final Batch Loss: 0.0005375099717639387\n",
      "Epoch 4525, Loss: 0.0011506654700497165, Final Batch Loss: 0.0008639228180982172\n",
      "Epoch 4526, Loss: 0.002629852904647123, Final Batch Loss: 9.213545854436234e-05\n",
      "Epoch 4527, Loss: 0.002873814242775552, Final Batch Loss: 0.00040775115485303104\n",
      "Epoch 4528, Loss: 0.001438373186829267, Final Batch Loss: 5.940087794442661e-05\n",
      "Epoch 4529, Loss: 0.0002848890162567841, Final Batch Loss: 2.1544537958106957e-05\n",
      "Epoch 4530, Loss: 0.0003707576834131032, Final Batch Loss: 0.00021921470761299133\n",
      "Epoch 4531, Loss: 0.00221881247125566, Final Batch Loss: 0.00011772388825193048\n",
      "Epoch 4532, Loss: 0.0005474221989061334, Final Batch Loss: 6.454439426306635e-05\n",
      "Epoch 4533, Loss: 0.015870579296461074, Final Batch Loss: 0.002056638477370143\n",
      "Epoch 4534, Loss: 0.0003239990783185931, Final Batch Loss: 1.3031423804932274e-05\n",
      "Epoch 4535, Loss: 0.0008195093614631332, Final Batch Loss: 3.2138188544195145e-05\n",
      "Epoch 4536, Loss: 0.04663135470946145, Final Batch Loss: 1.5279505532816984e-05\n",
      "Epoch 4537, Loss: 0.0003951602429879131, Final Batch Loss: 3.682431997731328e-05\n",
      "Epoch 4538, Loss: 0.00040371464274358004, Final Batch Loss: 5.0819544412661344e-05\n",
      "Epoch 4539, Loss: 0.0012860302231274545, Final Batch Loss: 8.237429574364796e-05\n",
      "Epoch 4540, Loss: 0.0003673305345728295, Final Batch Loss: 7.826756700524129e-06\n",
      "Epoch 4541, Loss: 0.0022623418390139705, Final Batch Loss: 1.0779474905575626e-05\n",
      "Epoch 4542, Loss: 0.00021694875249522738, Final Batch Loss: 3.069400918320753e-05\n",
      "Epoch 4543, Loss: 0.0007192611537902849, Final Batch Loss: 2.4430382836726494e-05\n",
      "Epoch 4544, Loss: 0.0003371835537109291, Final Batch Loss: 0.00012659169442486018\n",
      "Epoch 4545, Loss: 0.0002228173707408132, Final Batch Loss: 4.6781620767433196e-05\n",
      "Epoch 4546, Loss: 0.0009451393561903387, Final Batch Loss: 8.277864253614098e-05\n",
      "Epoch 4547, Loss: 0.0038537906693818513, Final Batch Loss: 4.66446035716217e-05\n",
      "Epoch 4548, Loss: 0.0006355435652949382, Final Batch Loss: 8.37829866213724e-05\n",
      "Epoch 4549, Loss: 0.00022331955187837593, Final Batch Loss: 0.00010444189683767036\n",
      "Epoch 4550, Loss: 0.0002808382560033351, Final Batch Loss: 0.00016915073501877487\n",
      "Epoch 4551, Loss: 0.001746799287502654, Final Batch Loss: 0.000520100467838347\n",
      "Epoch 4552, Loss: 0.000533055348569178, Final Batch Loss: 0.0003161961503792554\n",
      "Epoch 4553, Loss: 0.0003391890127204533, Final Batch Loss: 7.34894774723216e-06\n",
      "Epoch 4554, Loss: 0.011293283889244776, Final Batch Loss: 0.00041585409780964255\n",
      "Epoch 4555, Loss: 0.0004399265908432426, Final Batch Loss: 8.253891792264767e-06\n",
      "Epoch 4556, Loss: 0.0018192072038800688, Final Batch Loss: 1.7794440282159485e-05\n",
      "Epoch 4557, Loss: 0.0020032249303767458, Final Batch Loss: 1.397422602167353e-05\n",
      "Epoch 4558, Loss: 0.001797584798623575, Final Batch Loss: 0.00020134307851549238\n",
      "Epoch 4559, Loss: 0.00020079245496162912, Final Batch Loss: 6.154638685984537e-05\n",
      "Epoch 4560, Loss: 0.0020210257644066587, Final Batch Loss: 0.00024149469390977174\n",
      "Epoch 4561, Loss: 0.00048319195320800645, Final Batch Loss: 1.3279054655868094e-05\n",
      "Epoch 4562, Loss: 0.00035955461225967156, Final Batch Loss: 1.415206952515291e-05\n",
      "Epoch 4563, Loss: 0.0002435616625007242, Final Batch Loss: 1.6219812096096575e-05\n",
      "Epoch 4564, Loss: 0.0009226623415088397, Final Batch Loss: 0.0002716928138397634\n",
      "Epoch 4565, Loss: 0.00017021437952280394, Final Batch Loss: 4.9409904022468254e-05\n",
      "Epoch 4566, Loss: 0.006787794733099872, Final Batch Loss: 0.006573523860424757\n",
      "Epoch 4567, Loss: 0.00030937314568291185, Final Batch Loss: 7.358447874139529e-06\n",
      "Epoch 4568, Loss: 0.0007667293612030335, Final Batch Loss: 0.00011423919204389676\n",
      "Epoch 4569, Loss: 0.040675924174138345, Final Batch Loss: 0.000247676158323884\n",
      "Epoch 4570, Loss: 0.0005489044742716942, Final Batch Loss: 0.00012612160935532302\n",
      "Epoch 4571, Loss: 0.0002968135377159342, Final Batch Loss: 0.00010853305138880387\n",
      "Epoch 4572, Loss: 0.0002243490544060478, Final Batch Loss: 5.2423562010517344e-05\n",
      "Epoch 4573, Loss: 0.006146218458980002, Final Batch Loss: 0.004978964105248451\n",
      "Epoch 4574, Loss: 0.000533852157786896, Final Batch Loss: 8.753911970416084e-05\n",
      "Epoch 4575, Loss: 0.0002550723011154332, Final Batch Loss: 1.2365746442810632e-05\n",
      "Epoch 4576, Loss: 0.00036365052619657945, Final Batch Loss: 0.00024291999579872936\n",
      "Epoch 4577, Loss: 0.0002945072828879347, Final Batch Loss: 1.1603899110923521e-05\n",
      "Epoch 4578, Loss: 0.00042917034079437144, Final Batch Loss: 3.715617276611738e-05\n",
      "Epoch 4579, Loss: 0.0006393213734554593, Final Batch Loss: 3.56342970917467e-05\n",
      "Epoch 4580, Loss: 0.0021578616433544084, Final Batch Loss: 3.3565229387022555e-05\n",
      "Epoch 4581, Loss: 0.0010973538028338226, Final Batch Loss: 2.0082728951820172e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4582, Loss: 0.00013276029494591057, Final Batch Loss: 3.852722147712484e-05\n",
      "Epoch 4583, Loss: 0.00010856278640858363, Final Batch Loss: 3.235591429984197e-05\n",
      "Epoch 4584, Loss: 8.67807439135504e-05, Final Batch Loss: 3.009668944287114e-05\n",
      "Epoch 4585, Loss: 0.00045444603892974555, Final Batch Loss: 8.220593008445576e-05\n",
      "Epoch 4586, Loss: 0.0013919630055170273, Final Batch Loss: 0.0002459383977111429\n",
      "Epoch 4587, Loss: 0.006787686892494094, Final Batch Loss: 0.0003123761562164873\n",
      "Epoch 4588, Loss: 0.0008064976918831235, Final Batch Loss: 0.00040420351433567703\n",
      "Epoch 4589, Loss: 0.0007198869170679245, Final Batch Loss: 0.0003411920624785125\n",
      "Epoch 4590, Loss: 0.02469338798618992, Final Batch Loss: 0.024386128410696983\n",
      "Epoch 4591, Loss: 0.006951260002097115, Final Batch Loss: 0.002803178271278739\n",
      "Epoch 4592, Loss: 0.0015460667345905676, Final Batch Loss: 0.0011794929159805179\n",
      "Epoch 4593, Loss: 0.008624526693893131, Final Batch Loss: 0.00829935260117054\n",
      "Epoch 4594, Loss: 0.0004591327870002715, Final Batch Loss: 6.817391840741038e-05\n",
      "Epoch 4595, Loss: 0.0007525303353759227, Final Batch Loss: 4.972641181666404e-05\n",
      "Epoch 4596, Loss: 0.0044804134322475875, Final Batch Loss: 0.00031910071265883744\n",
      "Epoch 4597, Loss: 0.012922214627906214, Final Batch Loss: 0.003562403377145529\n",
      "Epoch 4598, Loss: 0.030923243359211483, Final Batch Loss: 2.5984112653532065e-05\n",
      "Epoch 4599, Loss: 0.0016818541789689334, Final Batch Loss: 1.5169820471783169e-05\n",
      "Epoch 4600, Loss: 0.009453603095607832, Final Batch Loss: 0.0004650541231967509\n",
      "Epoch 4601, Loss: 0.015027703391751857, Final Batch Loss: 5.257965312921442e-06\n",
      "Epoch 4602, Loss: 0.002738035014772322, Final Batch Loss: 0.00010284402378601953\n",
      "Epoch 4603, Loss: 0.0004933968966724933, Final Batch Loss: 0.00011378346971469\n",
      "Epoch 4604, Loss: 0.0005586647348536644, Final Batch Loss: 1.5633777366019785e-05\n",
      "Epoch 4605, Loss: 0.0020742301530844998, Final Batch Loss: 0.00011528863979037851\n",
      "Epoch 4606, Loss: 0.00025447108055232093, Final Batch Loss: 4.059158163727261e-05\n",
      "Epoch 4607, Loss: 0.00019056194832955953, Final Batch Loss: 5.114442319609225e-05\n",
      "Epoch 4608, Loss: 0.049753592029446736, Final Batch Loss: 0.04918495565652847\n",
      "Epoch 4609, Loss: 0.0445976625487674, Final Batch Loss: 5.190828233025968e-05\n",
      "Epoch 4610, Loss: 0.0017207526361744385, Final Batch Loss: 4.3811884097522125e-05\n",
      "Epoch 4611, Loss: 0.0009049700001924066, Final Batch Loss: 2.127562947862316e-05\n",
      "Epoch 4612, Loss: 0.0011270946270087734, Final Batch Loss: 6.539214518852532e-05\n",
      "Epoch 4613, Loss: 0.000960889925408992, Final Batch Loss: 2.6884625185630284e-05\n",
      "Epoch 4614, Loss: 0.0009157881795545109, Final Batch Loss: 0.0002557485713623464\n",
      "Epoch 4615, Loss: 0.0009181671048281714, Final Batch Loss: 0.0001322327443631366\n",
      "Epoch 4616, Loss: 0.002696639718124061, Final Batch Loss: 2.0220837541273795e-05\n",
      "Epoch 4617, Loss: 0.000848731491714716, Final Batch Loss: 8.276918379124254e-05\n",
      "Epoch 4618, Loss: 0.0018660849600564688, Final Batch Loss: 0.00044371781405061483\n",
      "Epoch 4619, Loss: 0.002532846323447302, Final Batch Loss: 0.0003787609457504004\n",
      "Epoch 4620, Loss: 0.0012510695178207243, Final Batch Loss: 0.0002399755030637607\n",
      "Epoch 4621, Loss: 0.0010976242629112676, Final Batch Loss: 0.0001586080325068906\n",
      "Epoch 4622, Loss: 0.0006462272722274065, Final Batch Loss: 0.000430621876148507\n",
      "Epoch 4623, Loss: 0.001520144620371866, Final Batch Loss: 9.042685633176006e-06\n",
      "Epoch 4624, Loss: 0.0006192186483531259, Final Batch Loss: 0.00013481815403793007\n",
      "Epoch 4625, Loss: 0.0022293563815765083, Final Batch Loss: 6.257136556087062e-05\n",
      "Epoch 4626, Loss: 0.0007503017823182745, Final Batch Loss: 2.93701214104658e-05\n",
      "Epoch 4627, Loss: 0.0009230362484231591, Final Batch Loss: 8.240062015829608e-05\n",
      "Epoch 4628, Loss: 0.0002952534132418805, Final Batch Loss: 1.3138233953213785e-05\n",
      "Epoch 4629, Loss: 0.0015473998919333098, Final Batch Loss: 0.0005624984623864293\n",
      "Epoch 4630, Loss: 0.000628449983196333, Final Batch Loss: 5.714906728826463e-05\n",
      "Epoch 4631, Loss: 0.00023101179613149725, Final Batch Loss: 1.5820647604414262e-05\n",
      "Epoch 4632, Loss: 0.00020630784183595097, Final Batch Loss: 1.1435467058618087e-05\n",
      "Epoch 4633, Loss: 0.0005095256128697656, Final Batch Loss: 2.22429953282699e-05\n",
      "Epoch 4634, Loss: 0.000781235938120517, Final Batch Loss: 0.0004048406262882054\n",
      "Epoch 4635, Loss: 0.000350301876096637, Final Batch Loss: 1.2157872333773412e-05\n",
      "Epoch 4636, Loss: 0.0003135055048915092, Final Batch Loss: 0.00011057913798140362\n",
      "Epoch 4637, Loss: 0.03065243580203969, Final Batch Loss: 0.00014504244609270245\n",
      "Epoch 4638, Loss: 0.003520789759932086, Final Batch Loss: 0.0030170732643455267\n",
      "Epoch 4639, Loss: 0.0011646938601188594, Final Batch Loss: 1.981677814910654e-05\n",
      "Epoch 4640, Loss: 0.007318459734960925, Final Batch Loss: 0.00010913050937233493\n",
      "Epoch 4641, Loss: 0.0009593341164872982, Final Batch Loss: 0.00046652459423057735\n",
      "Epoch 4642, Loss: 0.00036135401387582533, Final Batch Loss: 0.0002423711121082306\n",
      "Epoch 4643, Loss: 0.0007562925311503932, Final Batch Loss: 0.00014787704276386648\n",
      "Epoch 4644, Loss: 0.007863516828365391, Final Batch Loss: 2.1038711565779522e-05\n",
      "Epoch 4645, Loss: 0.0008826582197798416, Final Batch Loss: 0.00010812008986249566\n",
      "Epoch 4646, Loss: 0.0007484649504476693, Final Batch Loss: 0.00022443196212407202\n",
      "Epoch 4647, Loss: 0.0005438480584416538, Final Batch Loss: 7.534278847742826e-05\n",
      "Epoch 4648, Loss: 0.043114309537486406, Final Batch Loss: 0.04266824573278427\n",
      "Epoch 4649, Loss: 0.0006001940382702742, Final Batch Loss: 0.00040894976700656116\n",
      "Epoch 4650, Loss: 0.002618245591293089, Final Batch Loss: 0.0006063437322154641\n",
      "Epoch 4651, Loss: 0.03853245226491708, Final Batch Loss: 0.011729415506124496\n",
      "Epoch 4652, Loss: 0.0031960712840373162, Final Batch Loss: 5.027048246120103e-05\n",
      "Epoch 4653, Loss: 0.008532163952622795, Final Batch Loss: 1.5353423805208877e-05\n",
      "Epoch 4654, Loss: 0.0021882102046220098, Final Batch Loss: 3.1980638595996425e-05\n",
      "Epoch 4655, Loss: 0.0011641421656349848, Final Batch Loss: 3.1638842301617842e-06\n",
      "Epoch 4656, Loss: 0.002121909932611743, Final Batch Loss: 0.00040071524563245475\n",
      "Epoch 4657, Loss: 0.0012554212880786508, Final Batch Loss: 0.00022116387845017016\n",
      "Epoch 4658, Loss: 0.007777782366247266, Final Batch Loss: 1.4890791135258041e-05\n",
      "Epoch 4659, Loss: 0.0003919867922377307, Final Batch Loss: 3.6367633583722636e-05\n",
      "Epoch 4660, Loss: 0.0007068053382681683, Final Batch Loss: 1.8728329450823367e-05\n",
      "Epoch 4661, Loss: 0.0011308474604447838, Final Batch Loss: 2.936897726613097e-05\n",
      "Epoch 4662, Loss: 0.013392243316047825, Final Batch Loss: 0.0011274723801761866\n",
      "Epoch 4663, Loss: 0.0003096343684774183, Final Batch Loss: 0.00011321142665110528\n",
      "Epoch 4664, Loss: 0.000730183672203566, Final Batch Loss: 2.48125124926446e-05\n",
      "Epoch 4665, Loss: 0.000647664193820674, Final Batch Loss: 0.0002584723988547921\n",
      "Epoch 4666, Loss: 0.001688682865278679, Final Batch Loss: 0.0007090136641636491\n",
      "Epoch 4667, Loss: 0.0004136089664825704, Final Batch Loss: 6.878692511236295e-05\n",
      "Epoch 4668, Loss: 0.0009866697546385694, Final Batch Loss: 5.2459399739746004e-05\n",
      "Epoch 4669, Loss: 0.005588943475231645, Final Batch Loss: 0.004491208586841822\n",
      "Epoch 4670, Loss: 0.0017338018242298858, Final Batch Loss: 0.0015194846782833338\n",
      "Epoch 4671, Loss: 0.0001874646022770321, Final Batch Loss: 2.8531247153296135e-05\n",
      "Epoch 4672, Loss: 0.0008853236577124335, Final Batch Loss: 5.261213664198294e-05\n",
      "Epoch 4673, Loss: 0.00366481483069947, Final Batch Loss: 0.00012040783622069284\n",
      "Epoch 4674, Loss: 0.0004142687903367914, Final Batch Loss: 2.4822154955472797e-05\n",
      "Epoch 4675, Loss: 0.0002643987672854564, Final Batch Loss: 1.0677314094209578e-05\n",
      "Epoch 4676, Loss: 0.002519865596696036, Final Batch Loss: 8.562498987885192e-05\n",
      "Epoch 4677, Loss: 0.001504836582171265, Final Batch Loss: 0.0013201149413362145\n",
      "Epoch 4678, Loss: 0.0015391577835544012, Final Batch Loss: 6.378872058121487e-05\n",
      "Epoch 4679, Loss: 0.00015345804149546893, Final Batch Loss: 1.20978465929511e-05\n",
      "Epoch 4680, Loss: 0.0005963550593151012, Final Batch Loss: 0.0004118726646993309\n",
      "Epoch 4681, Loss: 0.0003255125502619194, Final Batch Loss: 2.4953233150881715e-05\n",
      "Epoch 4682, Loss: 0.0030723973122803727, Final Batch Loss: 2.351879265916068e-05\n",
      "Epoch 4683, Loss: 0.001660085360526864, Final Batch Loss: 0.0002336768084205687\n",
      "Epoch 4684, Loss: 0.10316648451407673, Final Batch Loss: 0.10227783024311066\n",
      "Epoch 4685, Loss: 0.00021216105596977286, Final Batch Loss: 6.704205588903278e-05\n",
      "Epoch 4686, Loss: 0.00022946768331166822, Final Batch Loss: 6.561288319062442e-05\n",
      "Epoch 4687, Loss: 0.00043589640881691594, Final Batch Loss: 2.0057115762028843e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4688, Loss: 0.0005263700513751246, Final Batch Loss: 6.812715582782403e-05\n",
      "Epoch 4689, Loss: 0.0005370488433982246, Final Batch Loss: 5.048359889769927e-05\n",
      "Epoch 4690, Loss: 0.0017848576435426367, Final Batch Loss: 1.1647648534562904e-05\n",
      "Epoch 4691, Loss: 0.004626162954082247, Final Batch Loss: 0.004266564268618822\n",
      "Epoch 4692, Loss: 0.0009523183125565993, Final Batch Loss: 0.0006845167954452336\n",
      "Epoch 4693, Loss: 0.0020203663625579793, Final Batch Loss: 0.000509000092279166\n",
      "Epoch 4694, Loss: 0.0011002588653354906, Final Batch Loss: 1.8664206436369568e-05\n",
      "Epoch 4695, Loss: 0.0012638818479899783, Final Batch Loss: 2.4668832338647917e-05\n",
      "Epoch 4696, Loss: 0.0009008020788314752, Final Batch Loss: 0.00020030232553835958\n",
      "Epoch 4697, Loss: 0.0009968052254407667, Final Batch Loss: 0.00046333958744071424\n",
      "Epoch 4698, Loss: 0.0028369005231070332, Final Batch Loss: 0.0025537912733852863\n",
      "Epoch 4699, Loss: 0.0006909946059749927, Final Batch Loss: 1.6605827113380656e-05\n",
      "Epoch 4700, Loss: 0.0008495209185639396, Final Batch Loss: 0.0003150472475681454\n",
      "Epoch 4701, Loss: 0.001176830493250236, Final Batch Loss: 1.9946048723795684e-06\n",
      "Epoch 4702, Loss: 0.0005855554809386376, Final Batch Loss: 9.747708099894226e-05\n",
      "Epoch 4703, Loss: 0.0015657993189961417, Final Batch Loss: 0.0002655447169672698\n",
      "Epoch 4704, Loss: 0.0006020670598445577, Final Batch Loss: 3.556237061275169e-05\n",
      "Epoch 4705, Loss: 0.0003157195842504734, Final Batch Loss: 1.3688966646441258e-05\n",
      "Epoch 4706, Loss: 0.0002719560361583717, Final Batch Loss: 4.560356683214195e-05\n",
      "Epoch 4707, Loss: 0.0010210126711172052, Final Batch Loss: 4.551108941086568e-05\n",
      "Epoch 4708, Loss: 0.0017938203000085196, Final Batch Loss: 0.0015159922186285257\n",
      "Epoch 4709, Loss: 0.0001904199962154962, Final Batch Loss: 1.7324198779533617e-05\n",
      "Epoch 4710, Loss: 0.0008440204505859583, Final Batch Loss: 6.729660071869148e-06\n",
      "Epoch 4711, Loss: 0.001612307554751169, Final Batch Loss: 3.261950769228861e-05\n",
      "Epoch 4712, Loss: 0.00024645910889375955, Final Batch Loss: 0.00011088024621130899\n",
      "Epoch 4713, Loss: 0.0005611873493762687, Final Batch Loss: 1.5346631698776037e-05\n",
      "Epoch 4714, Loss: 0.00015937195348669775, Final Batch Loss: 1.19691376312403e-05\n",
      "Epoch 4715, Loss: 0.00034978073608726845, Final Batch Loss: 3.903702690877253e-06\n",
      "Epoch 4716, Loss: 0.0003768782989936881, Final Batch Loss: 4.1782983316807076e-05\n",
      "Epoch 4717, Loss: 0.0014194981638411264, Final Batch Loss: 2.687277401491883e-06\n",
      "Epoch 4718, Loss: 0.00045409053200273775, Final Batch Loss: 9.757521183928475e-05\n",
      "Epoch 4719, Loss: 0.0043227870810369495, Final Batch Loss: 1.8756876670522615e-05\n",
      "Epoch 4720, Loss: 0.015988947906407702, Final Batch Loss: 3.017655217263382e-06\n",
      "Epoch 4721, Loss: 0.0035849366904585622, Final Batch Loss: 0.0012588680256158113\n",
      "Epoch 4722, Loss: 0.00044464366874308325, Final Batch Loss: 1.699380300124176e-05\n",
      "Epoch 4723, Loss: 0.0001503035937275854, Final Batch Loss: 2.471224252076354e-05\n",
      "Epoch 4724, Loss: 0.0002132628214894794, Final Batch Loss: 9.621759090805426e-06\n",
      "Epoch 4725, Loss: 0.0002450130068609724, Final Batch Loss: 1.153308494394878e-05\n",
      "Epoch 4726, Loss: 0.0024141454996424727, Final Batch Loss: 0.002216009423136711\n",
      "Epoch 4727, Loss: 0.001064261083229212, Final Batch Loss: 0.0006263120449148118\n",
      "Epoch 4728, Loss: 0.00045138842142478097, Final Batch Loss: 5.3458643378689885e-05\n",
      "Epoch 4729, Loss: 0.00013299839520186651, Final Batch Loss: 2.1886255126446486e-05\n",
      "Epoch 4730, Loss: 0.00013250724350655219, Final Batch Loss: 1.5136142792471219e-05\n",
      "Epoch 4731, Loss: 0.0004361247870292573, Final Batch Loss: 3.505659606162226e-06\n",
      "Epoch 4732, Loss: 0.0006806499786762288, Final Batch Loss: 1.8029988495982252e-05\n",
      "Epoch 4733, Loss: 0.0010364155969000421, Final Batch Loss: 2.5517721951473504e-05\n",
      "Epoch 4734, Loss: 0.0006422920569093549, Final Batch Loss: 3.3462518331361935e-05\n",
      "Epoch 4735, Loss: 0.00012264304996278952, Final Batch Loss: 1.6876579138624948e-06\n",
      "Epoch 4736, Loss: 0.000365899761163746, Final Batch Loss: 7.31340260244906e-05\n",
      "Epoch 4737, Loss: 0.00016929036701185396, Final Batch Loss: 3.453022145549767e-05\n",
      "Epoch 4738, Loss: 0.0004046515041409293, Final Batch Loss: 0.0003149061813019216\n",
      "Epoch 4739, Loss: 0.0015460451395483688, Final Batch Loss: 2.293038778589107e-05\n",
      "Epoch 4740, Loss: 0.0007811787336322595, Final Batch Loss: 1.638668072700966e-05\n",
      "Epoch 4741, Loss: 0.00028120705792389344, Final Batch Loss: 9.661884178058244e-06\n",
      "Epoch 4742, Loss: 0.0004897174248981173, Final Batch Loss: 0.00011607418127823621\n",
      "Epoch 4743, Loss: 0.012137417903431924, Final Batch Loss: 0.00018816681404132396\n",
      "Epoch 4744, Loss: 0.00023955923825269565, Final Batch Loss: 1.8218775949208066e-05\n",
      "Epoch 4745, Loss: 0.00015697030858063954, Final Batch Loss: 3.0908372536941897e-06\n",
      "Epoch 4746, Loss: 0.0027030935652874177, Final Batch Loss: 0.00016727892216295004\n",
      "Epoch 4747, Loss: 0.0010556971683399752, Final Batch Loss: 0.00021050460054539144\n",
      "Epoch 4748, Loss: 0.003506997862132266, Final Batch Loss: 6.611231219721958e-05\n",
      "Epoch 4749, Loss: 0.00042900860898953397, Final Batch Loss: 0.000230783800361678\n",
      "Epoch 4750, Loss: 0.007726576584900613, Final Batch Loss: 0.007193453144282103\n",
      "Epoch 4751, Loss: 0.0027025697364706502, Final Batch Loss: 1.8897024347097613e-05\n",
      "Epoch 4752, Loss: 0.0005540645142900757, Final Batch Loss: 8.948958566179499e-06\n",
      "Epoch 4753, Loss: 0.0004670395546781947, Final Batch Loss: 0.0003746509610209614\n",
      "Epoch 4754, Loss: 0.040536163385695545, Final Batch Loss: 4.909396739094518e-05\n",
      "Epoch 4755, Loss: 0.00017850992844614666, Final Batch Loss: 5.838937795488164e-05\n",
      "Epoch 4756, Loss: 0.019532599952071905, Final Batch Loss: 3.797499812208116e-05\n",
      "Epoch 4757, Loss: 0.0065975291709037265, Final Batch Loss: 0.006422434002161026\n",
      "Epoch 4758, Loss: 0.00033559084840817377, Final Batch Loss: 0.0002445750869810581\n",
      "Epoch 4759, Loss: 0.04044977631565416, Final Batch Loss: 8.034893107833341e-05\n",
      "Epoch 4760, Loss: 0.001060507660440635, Final Batch Loss: 0.00011921862460440025\n",
      "Epoch 4761, Loss: 0.000967551033681957, Final Batch Loss: 0.00014492429909296334\n",
      "Epoch 4762, Loss: 0.004923126311041415, Final Batch Loss: 7.378868758678436e-05\n",
      "Epoch 4763, Loss: 0.0008045267386478372, Final Batch Loss: 0.00027209840482100844\n",
      "Epoch 4764, Loss: 0.018513982322474476, Final Batch Loss: 0.000111529188870918\n",
      "Epoch 4765, Loss: 0.000774241521867225, Final Batch Loss: 0.00010728098277468234\n",
      "Epoch 4766, Loss: 0.0005724394713979564, Final Batch Loss: 8.488455205224454e-05\n",
      "Epoch 4767, Loss: 0.0002656565993675031, Final Batch Loss: 4.086939225089736e-05\n",
      "Epoch 4768, Loss: 0.0007448483702319209, Final Batch Loss: 0.00030856451485306025\n",
      "Epoch 4769, Loss: 0.0016119985157274641, Final Batch Loss: 0.0007115394109860063\n",
      "Epoch 4770, Loss: 0.001306825046412996, Final Batch Loss: 0.00016906336531974375\n",
      "Epoch 4771, Loss: 0.00026294230156054255, Final Batch Loss: 3.5580305848270655e-05\n",
      "Epoch 4772, Loss: 0.0023873285126683186, Final Batch Loss: 1.0027112693933304e-05\n",
      "Epoch 4773, Loss: 0.00914130217552156, Final Batch Loss: 9.515420060779434e-06\n",
      "Epoch 4774, Loss: 0.0007988784127519466, Final Batch Loss: 5.50934491911903e-05\n",
      "Epoch 4775, Loss: 0.0019805771589744836, Final Batch Loss: 0.00017715306603349745\n",
      "Epoch 4776, Loss: 0.002077238677884452, Final Batch Loss: 0.00020136886450927705\n",
      "Epoch 4777, Loss: 0.0005702198159269756, Final Batch Loss: 0.00029387802351266146\n",
      "Epoch 4778, Loss: 0.010513686229387531, Final Batch Loss: 6.620357453357428e-05\n",
      "Epoch 4779, Loss: 0.000755704561015591, Final Batch Loss: 0.0004421827325131744\n",
      "Epoch 4780, Loss: 0.0005485827769007301, Final Batch Loss: 0.0002134800306521356\n",
      "Epoch 4781, Loss: 0.01729384480495355, Final Batch Loss: 3.7424997572088614e-05\n",
      "Epoch 4782, Loss: 0.00019481874460325344, Final Batch Loss: 9.68978838500334e-06\n",
      "Epoch 4783, Loss: 0.007503418275518925, Final Batch Loss: 2.9919285225332715e-05\n",
      "Epoch 4784, Loss: 0.001185113491374068, Final Batch Loss: 0.0002204506890848279\n",
      "Epoch 4785, Loss: 0.001245507304702187, Final Batch Loss: 0.0009559368481859565\n",
      "Epoch 4786, Loss: 0.0004494560998864472, Final Batch Loss: 9.256700286641717e-05\n",
      "Epoch 4787, Loss: 0.0017202598646690603, Final Batch Loss: 5.611692540696822e-05\n",
      "Epoch 4788, Loss: 0.0009115957109315787, Final Batch Loss: 0.0002383806713623926\n",
      "Epoch 4789, Loss: 0.0013401505784713663, Final Batch Loss: 0.00014389936404768378\n",
      "Epoch 4790, Loss: 0.0003675726075016428, Final Batch Loss: 8.581611473346129e-05\n",
      "Epoch 4791, Loss: 0.021937582501777797, Final Batch Loss: 1.5263582099578343e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4792, Loss: 0.0017293420469286502, Final Batch Loss: 0.001107138697989285\n",
      "Epoch 4793, Loss: 0.00117914225847926, Final Batch Loss: 3.21576080750674e-05\n",
      "Epoch 4794, Loss: 0.0022575750153919216, Final Batch Loss: 0.00011007965076714754\n",
      "Epoch 4795, Loss: 0.06856362827238627, Final Batch Loss: 0.00032161184935830534\n",
      "Epoch 4796, Loss: 0.0005107751348987222, Final Batch Loss: 0.00035245102480985224\n",
      "Epoch 4797, Loss: 0.0026482506800675765, Final Batch Loss: 0.001205120817758143\n",
      "Epoch 4798, Loss: 0.0035235286777606234, Final Batch Loss: 0.0033248511608690023\n",
      "Epoch 4799, Loss: 0.03738733053614851, Final Batch Loss: 0.00011648815416265279\n",
      "Epoch 4800, Loss: 0.000601532799009874, Final Batch Loss: 7.5900961746810935e-06\n",
      "Epoch 4801, Loss: 0.0010224377474514768, Final Batch Loss: 0.0006429177010431886\n",
      "Epoch 4802, Loss: 0.0012634130998776527, Final Batch Loss: 0.0007450981647707522\n",
      "Epoch 4803, Loss: 0.0007911282427812694, Final Batch Loss: 9.324894563178532e-06\n",
      "Epoch 4804, Loss: 0.012986296120288898, Final Batch Loss: 4.302623347030021e-05\n",
      "Epoch 4805, Loss: 0.015305170687497593, Final Batch Loss: 0.00010105618275702\n",
      "Epoch 4806, Loss: 0.0006838783629063983, Final Batch Loss: 0.00042370319715701044\n",
      "Epoch 4807, Loss: 0.0014781566496822052, Final Batch Loss: 7.150755118345842e-05\n",
      "Epoch 4808, Loss: 0.0013719450507778674, Final Batch Loss: 0.0003374661901034415\n",
      "Epoch 4809, Loss: 0.013531129647162743, Final Batch Loss: 0.00011528255708981305\n",
      "Epoch 4810, Loss: 0.001701428773230873, Final Batch Loss: 0.0001378649176331237\n",
      "Epoch 4811, Loss: 0.0014267410806496628, Final Batch Loss: 6.755487265763804e-05\n",
      "Epoch 4812, Loss: 0.001192276849906193, Final Batch Loss: 6.053336619515903e-05\n",
      "Epoch 4813, Loss: 0.0012254654575372115, Final Batch Loss: 0.00016809070075396448\n",
      "Epoch 4814, Loss: 0.0005768554037786089, Final Batch Loss: 0.00016715037054382265\n",
      "Epoch 4815, Loss: 0.0010333822065149434, Final Batch Loss: 0.000273812998784706\n",
      "Epoch 4816, Loss: 0.0005184515575820114, Final Batch Loss: 7.584783452330157e-05\n",
      "Epoch 4817, Loss: 0.0014285326324170455, Final Batch Loss: 8.836256165523082e-05\n",
      "Epoch 4818, Loss: 0.009700385468022432, Final Batch Loss: 0.00019520238856785\n",
      "Epoch 4819, Loss: 0.0007999407607712783, Final Batch Loss: 0.00037902971962466836\n",
      "Epoch 4820, Loss: 0.003937279876481625, Final Batch Loss: 0.0007767481729388237\n",
      "Epoch 4821, Loss: 0.0013319593599590007, Final Batch Loss: 2.0751140255015343e-05\n",
      "Epoch 4822, Loss: 0.0002593097283352108, Final Batch Loss: 6.888306870678207e-06\n",
      "Epoch 4823, Loss: 0.003117967947218858, Final Batch Loss: 4.3975225707981735e-05\n",
      "Epoch 4824, Loss: 0.00034748235793813365, Final Batch Loss: 8.39906442706706e-06\n",
      "Epoch 4825, Loss: 0.00046678928720211843, Final Batch Loss: 2.925351327576209e-05\n",
      "Epoch 4826, Loss: 0.003314064808364492, Final Batch Loss: 0.0021949021611362696\n",
      "Epoch 4827, Loss: 0.022344566521496745, Final Batch Loss: 0.00033740000799298286\n",
      "Epoch 4828, Loss: 0.0004725570252048783, Final Batch Loss: 7.149003795348108e-05\n",
      "Epoch 4829, Loss: 0.0018455843346600886, Final Batch Loss: 0.0006566434749402106\n",
      "Epoch 4830, Loss: 0.004649011846595386, Final Batch Loss: 8.986272405309137e-06\n",
      "Epoch 4831, Loss: 0.0005260737825665274, Final Batch Loss: 0.0002446404250804335\n",
      "Epoch 4832, Loss: 0.0013920518904342316, Final Batch Loss: 0.0004517490742728114\n",
      "Epoch 4833, Loss: 0.0007319625437958166, Final Batch Loss: 0.0002663689956534654\n",
      "Epoch 4834, Loss: 0.0005134049461048562, Final Batch Loss: 4.437619645614177e-05\n",
      "Epoch 4835, Loss: 0.054899600756471045, Final Batch Loss: 6.403862789738923e-05\n",
      "Epoch 4836, Loss: 0.0029573637220892124, Final Batch Loss: 0.0026700261514633894\n",
      "Epoch 4837, Loss: 0.00668675520864781, Final Batch Loss: 0.0001440380437998101\n",
      "Epoch 4838, Loss: 0.0047565026907250285, Final Batch Loss: 0.003119309199973941\n",
      "Epoch 4839, Loss: 0.0018106394709320739, Final Batch Loss: 0.0001953825558302924\n",
      "Epoch 4840, Loss: 0.0034813197271432728, Final Batch Loss: 0.0022763963788747787\n",
      "Epoch 4841, Loss: 0.006377338620950468, Final Batch Loss: 0.0002175499830627814\n",
      "Epoch 4842, Loss: 0.0239006139254343, Final Batch Loss: 0.004357173573225737\n",
      "Epoch 4843, Loss: 0.0014386929397005588, Final Batch Loss: 0.0005735958111472428\n",
      "Epoch 4844, Loss: 0.0008420020312769338, Final Batch Loss: 7.57154484745115e-05\n",
      "Epoch 4845, Loss: 0.003099153400398791, Final Batch Loss: 0.00036739057395607233\n",
      "Epoch 4846, Loss: 0.00047679229646746535, Final Batch Loss: 4.056090256199241e-05\n",
      "Epoch 4847, Loss: 0.00077297354437178, Final Batch Loss: 0.0005856936913914979\n",
      "Epoch 4848, Loss: 0.0013311929815245094, Final Batch Loss: 0.0008762564975768328\n",
      "Epoch 4849, Loss: 0.027093604869151022, Final Batch Loss: 0.00028494567959569395\n",
      "Epoch 4850, Loss: 0.0005660584829456639, Final Batch Loss: 0.00030026357853785157\n",
      "Epoch 4851, Loss: 0.00045113057240087073, Final Batch Loss: 2.5915427613654174e-05\n",
      "Epoch 4852, Loss: 0.0019334780954523012, Final Batch Loss: 0.00024002119607757777\n",
      "Epoch 4853, Loss: 0.0019125729304505512, Final Batch Loss: 4.19993739342317e-05\n",
      "Epoch 4854, Loss: 0.0004879628941125702, Final Batch Loss: 5.8531633840175346e-05\n",
      "Epoch 4855, Loss: 0.0003735466962098144, Final Batch Loss: 4.115541378268972e-05\n",
      "Epoch 4856, Loss: 0.00043616778566502035, Final Batch Loss: 8.119395351968706e-05\n",
      "Epoch 4857, Loss: 0.0004625717820090358, Final Batch Loss: 8.592429367126897e-05\n",
      "Epoch 4858, Loss: 0.00029690419250982814, Final Batch Loss: 0.00010601802205201238\n",
      "Epoch 4859, Loss: 0.0009326410272478824, Final Batch Loss: 0.00035317911533638835\n",
      "Epoch 4860, Loss: 0.0010407062400190625, Final Batch Loss: 5.923170465393923e-05\n",
      "Epoch 4861, Loss: 0.00021077432120364392, Final Batch Loss: 1.1737139175238553e-05\n",
      "Epoch 4862, Loss: 0.0003761820262297988, Final Batch Loss: 6.628039409406483e-05\n",
      "Epoch 4863, Loss: 0.0005682698538294062, Final Batch Loss: 5.0485654355725273e-05\n",
      "Epoch 4864, Loss: 0.0002524089904909488, Final Batch Loss: 0.00010548780846875161\n",
      "Epoch 4865, Loss: 0.0005821622071380261, Final Batch Loss: 1.507765773567371e-05\n",
      "Epoch 4866, Loss: 0.0007179881467891391, Final Batch Loss: 0.0001486011315137148\n",
      "Epoch 4867, Loss: 0.00010836375713552115, Final Batch Loss: 2.330700408492703e-05\n",
      "Epoch 4868, Loss: 0.00021211353487160522, Final Batch Loss: 3.540689795045182e-05\n",
      "Epoch 4869, Loss: 0.0002652982257131953, Final Batch Loss: 3.1074287107912824e-05\n",
      "Epoch 4870, Loss: 0.0012030073048663326, Final Batch Loss: 0.00010139474761672318\n",
      "Epoch 4871, Loss: 0.0010772572604764719, Final Batch Loss: 0.000182377640157938\n",
      "Epoch 4872, Loss: 0.00047834081124165095, Final Batch Loss: 3.17995909426827e-05\n",
      "Epoch 4873, Loss: 0.0008198493087547831, Final Batch Loss: 8.918093953980133e-05\n",
      "Epoch 4874, Loss: 0.002171660009480547, Final Batch Loss: 0.00011483381240395829\n",
      "Epoch 4875, Loss: 0.000154122468757123, Final Batch Loss: 9.210444113705307e-05\n",
      "Epoch 4876, Loss: 0.0002099609628203325, Final Batch Loss: 1.4183373423293233e-05\n",
      "Epoch 4877, Loss: 0.0005200383966439404, Final Batch Loss: 0.0001857770694186911\n",
      "Epoch 4878, Loss: 0.00035164521614206024, Final Batch Loss: 3.539737008395605e-05\n",
      "Epoch 4879, Loss: 0.00039237069358932786, Final Batch Loss: 6.636924808844924e-05\n",
      "Epoch 4880, Loss: 0.0003612687942222692, Final Batch Loss: 2.50322773354128e-05\n",
      "Epoch 4881, Loss: 0.005326241815055255, Final Batch Loss: 7.604239362990484e-05\n",
      "Epoch 4882, Loss: 0.013974816964946513, Final Batch Loss: 1.4151290997688193e-05\n",
      "Epoch 4883, Loss: 0.0002026550573646091, Final Batch Loss: 1.571665779920295e-05\n",
      "Epoch 4884, Loss: 0.008807421087112743, Final Batch Loss: 1.0695912351366132e-05\n",
      "Epoch 4885, Loss: 0.0022117194682778063, Final Batch Loss: 1.8613143311085878e-06\n",
      "Epoch 4886, Loss: 0.00073361466274946, Final Batch Loss: 0.00019681263074744493\n",
      "Epoch 4887, Loss: 0.004095254858839326, Final Batch Loss: 0.00018063554307445884\n",
      "Epoch 4888, Loss: 0.008135074604069814, Final Batch Loss: 0.006822923664003611\n",
      "Epoch 4889, Loss: 0.0006170085653138813, Final Batch Loss: 6.631863652728498e-05\n",
      "Epoch 4890, Loss: 0.0036841076143900864, Final Batch Loss: 0.0001244021550519392\n",
      "Epoch 4891, Loss: 0.0007848257255318458, Final Batch Loss: 6.516163011838216e-06\n",
      "Epoch 4892, Loss: 0.022542734961461974, Final Batch Loss: 4.604042260325514e-05\n",
      "Epoch 4893, Loss: 0.0007737548075965606, Final Batch Loss: 0.0005523863947018981\n",
      "Epoch 4894, Loss: 0.01399001944810152, Final Batch Loss: 0.0003026436024811119\n",
      "Epoch 4895, Loss: 0.0030900379315426107, Final Batch Loss: 0.0010258472757413983\n",
      "Epoch 4896, Loss: 0.005948634097876493, Final Batch Loss: 0.0007598318625241518\n",
      "Epoch 4897, Loss: 0.005531133276235778, Final Batch Loss: 7.43016935302876e-05\n",
      "Epoch 4898, Loss: 0.0007511496951337904, Final Batch Loss: 0.00019855944265145808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4899, Loss: 0.0008503049102728255, Final Batch Loss: 0.0006767807644791901\n",
      "Epoch 4900, Loss: 0.0005553324444917962, Final Batch Loss: 6.05620771239046e-05\n",
      "Epoch 4901, Loss: 0.0003117797641607467, Final Batch Loss: 5.456334838527255e-05\n",
      "Epoch 4902, Loss: 0.0009984648859244771, Final Batch Loss: 0.0002771381114143878\n",
      "Epoch 4903, Loss: 0.0004777208323503146, Final Batch Loss: 5.1428607548587024e-05\n",
      "Epoch 4904, Loss: 0.00043354337503842544, Final Batch Loss: 0.00025355888647027314\n",
      "Epoch 4905, Loss: 0.020752832657308318, Final Batch Loss: 0.0002709259570110589\n",
      "Epoch 4906, Loss: 0.0007149205557652749, Final Batch Loss: 7.621783879585564e-07\n",
      "Epoch 4907, Loss: 0.0008880015457179979, Final Batch Loss: 7.412615559587721e-06\n",
      "Epoch 4908, Loss: 0.01599886869371403, Final Batch Loss: 0.000589317933190614\n",
      "Epoch 4909, Loss: 0.00013110589316056576, Final Batch Loss: 7.208199531305581e-05\n",
      "Epoch 4910, Loss: 0.0026458304237166885, Final Batch Loss: 1.7121141354436986e-05\n",
      "Epoch 4911, Loss: 0.0003550573410393554, Final Batch Loss: 1.0230593943560962e-05\n",
      "Epoch 4912, Loss: 0.0009545169232296757, Final Batch Loss: 0.0007327818893827498\n",
      "Epoch 4913, Loss: 0.004427472054885584, Final Batch Loss: 4.925502435071394e-05\n",
      "Epoch 4914, Loss: 0.0006826064054621384, Final Batch Loss: 0.0002341728686587885\n",
      "Epoch 4915, Loss: 0.0003729982636286877, Final Batch Loss: 8.316495222970843e-05\n",
      "Epoch 4916, Loss: 0.0002644525943651388, Final Batch Loss: 1.5637583601346705e-06\n",
      "Epoch 4917, Loss: 0.0008284583218483021, Final Batch Loss: 0.00031186756677925587\n",
      "Epoch 4918, Loss: 0.0026646217265806627, Final Batch Loss: 4.1970295569626614e-05\n",
      "Epoch 4919, Loss: 0.0002626270024848054, Final Batch Loss: 4.0664268453838304e-05\n",
      "Epoch 4920, Loss: 0.0013477151660481468, Final Batch Loss: 0.00010307780030416325\n",
      "Epoch 4921, Loss: 0.05172891234360577, Final Batch Loss: 0.050845954567193985\n",
      "Epoch 4922, Loss: 0.03209150180373399, Final Batch Loss: 3.974888022639789e-05\n",
      "Epoch 4923, Loss: 0.0007840937323635444, Final Batch Loss: 0.00015523099864367396\n",
      "Epoch 4924, Loss: 0.0005850017078046221, Final Batch Loss: 7.104617543518543e-05\n",
      "Epoch 4925, Loss: 0.0011219384905416518, Final Batch Loss: 0.00033367652213200927\n",
      "Epoch 4926, Loss: 0.00022745512342225993, Final Batch Loss: 6.388316251104698e-05\n",
      "Epoch 4927, Loss: 0.04359116070463642, Final Batch Loss: 1.5235214050335344e-05\n",
      "Epoch 4928, Loss: 0.001310869951339555, Final Batch Loss: 1.5744752090540715e-05\n",
      "Epoch 4929, Loss: 0.02907309468719177, Final Batch Loss: 0.0008426591521129012\n",
      "Epoch 4930, Loss: 0.0061897212735857465, Final Batch Loss: 0.001601472613401711\n",
      "Epoch 4931, Loss: 0.004969686679942242, Final Batch Loss: 1.4984437257226091e-05\n",
      "Epoch 4932, Loss: 0.0020635066903196275, Final Batch Loss: 0.00039763806853443384\n",
      "Epoch 4933, Loss: 0.0008533890686521772, Final Batch Loss: 0.0003755204379558563\n",
      "Epoch 4934, Loss: 0.013998213673858118, Final Batch Loss: 6.5080516833404545e-06\n",
      "Epoch 4935, Loss: 0.0027109552320325747, Final Batch Loss: 0.0004994675982743502\n",
      "Epoch 4936, Loss: 0.0006865324794489425, Final Batch Loss: 0.00027334666810929775\n",
      "Epoch 4937, Loss: 0.0011315428346279077, Final Batch Loss: 0.0004007382085546851\n",
      "Epoch 4938, Loss: 0.011351462904713117, Final Batch Loss: 0.00011157704284414649\n",
      "Epoch 4939, Loss: 0.00041412826567466254, Final Batch Loss: 1.9004886780749075e-05\n",
      "Epoch 4940, Loss: 0.0004023810543003492, Final Batch Loss: 7.029070548014715e-05\n",
      "Epoch 4941, Loss: 0.0009134466672549024, Final Batch Loss: 0.00032054397161118686\n",
      "Epoch 4942, Loss: 0.0004847890704695601, Final Batch Loss: 1.6481299098813906e-05\n",
      "Epoch 4943, Loss: 0.000569566102058161, Final Batch Loss: 3.7290898035280406e-05\n",
      "Epoch 4944, Loss: 0.012246342826983891, Final Batch Loss: 0.00042102779843844473\n",
      "Epoch 4945, Loss: 0.0005454213205666747, Final Batch Loss: 0.00014309200923889875\n",
      "Epoch 4946, Loss: 0.02006118546705693, Final Batch Loss: 0.00031997860060073435\n",
      "Epoch 4947, Loss: 0.00035273409139335854, Final Batch Loss: 1.3211602890805807e-05\n",
      "Epoch 4948, Loss: 0.003022375181899406, Final Batch Loss: 0.0015703880926594138\n",
      "Epoch 4949, Loss: 0.0007602636542287655, Final Batch Loss: 3.7231169699225575e-05\n",
      "Epoch 4950, Loss: 0.0022360877046594396, Final Batch Loss: 5.7152516092173755e-05\n",
      "Epoch 4951, Loss: 0.0011162090326024554, Final Batch Loss: 1.8537779169491841e-06\n",
      "Epoch 4952, Loss: 0.000713111099685193, Final Batch Loss: 4.097063356311992e-05\n",
      "Epoch 4953, Loss: 0.0008392408099098247, Final Batch Loss: 7.260773600137327e-06\n",
      "Epoch 4954, Loss: 0.00036989212094340473, Final Batch Loss: 1.9286568203824572e-05\n",
      "Epoch 4955, Loss: 0.0002894028330047149, Final Batch Loss: 8.314320439239964e-05\n",
      "Epoch 4956, Loss: 0.0007540611377407913, Final Batch Loss: 8.338877705682535e-06\n",
      "Epoch 4957, Loss: 0.000909501042769989, Final Batch Loss: 0.0003868854546453804\n",
      "Epoch 4958, Loss: 0.0006235606269910932, Final Batch Loss: 1.708604395389557e-05\n",
      "Epoch 4959, Loss: 0.0002270767308800714, Final Batch Loss: 2.4585966457379982e-05\n",
      "Epoch 4960, Loss: 0.0020013275861856528, Final Batch Loss: 0.0003518955782055855\n",
      "Epoch 4961, Loss: 0.0005093203053547768, Final Batch Loss: 1.7755102817318402e-05\n",
      "Epoch 4962, Loss: 0.00047740266199980397, Final Batch Loss: 1.7068183296942152e-05\n",
      "Epoch 4963, Loss: 0.003986122999776853, Final Batch Loss: 7.971390732564032e-05\n",
      "Epoch 4964, Loss: 0.00026504334891797043, Final Batch Loss: 6.765445141354576e-05\n",
      "Epoch 4965, Loss: 0.01288032477532397, Final Batch Loss: 2.2258080207393505e-05\n",
      "Epoch 4966, Loss: 0.00026048595827887766, Final Batch Loss: 7.515517063438892e-05\n",
      "Epoch 4967, Loss: 0.00027953880999120884, Final Batch Loss: 0.00010146787099074572\n",
      "Epoch 4968, Loss: 0.0002395240662735887, Final Batch Loss: 5.916415466344915e-05\n",
      "Epoch 4969, Loss: 0.0004057675541844219, Final Batch Loss: 3.9370097510982305e-05\n",
      "Epoch 4970, Loss: 0.0005248834240774158, Final Batch Loss: 0.00014656125858891755\n",
      "Epoch 4971, Loss: 0.0004892257056781091, Final Batch Loss: 7.590623135911301e-05\n",
      "Epoch 4972, Loss: 0.0015477867309527937, Final Batch Loss: 9.328110900241882e-05\n",
      "Epoch 4973, Loss: 0.0006116600020504848, Final Batch Loss: 3.965469659306109e-06\n",
      "Epoch 4974, Loss: 0.00044213761611899827, Final Batch Loss: 0.00018039761926047504\n",
      "Epoch 4975, Loss: 0.0005135247938596876, Final Batch Loss: 0.0003412988153286278\n",
      "Epoch 4976, Loss: 0.0008472115514450707, Final Batch Loss: 0.0004919900093227625\n",
      "Epoch 4977, Loss: 0.0004468143724807305, Final Batch Loss: 8.246591460192576e-05\n",
      "Epoch 4978, Loss: 0.0024896036193240434, Final Batch Loss: 0.0002504619478713721\n",
      "Epoch 4979, Loss: 0.07936032770339807, Final Batch Loss: 0.07128387689590454\n",
      "Epoch 4980, Loss: 0.0021908228864049306, Final Batch Loss: 0.0019434135174378753\n",
      "Epoch 4981, Loss: 0.0033142692409455776, Final Batch Loss: 0.0016012346604838967\n",
      "Epoch 4982, Loss: 0.0013601640594060882, Final Batch Loss: 0.00018433290824759752\n",
      "Epoch 4983, Loss: 0.004657495468563866, Final Batch Loss: 0.0029192871879786253\n",
      "Epoch 4984, Loss: 0.004026135939056985, Final Batch Loss: 7.964731776155531e-05\n",
      "Epoch 4985, Loss: 0.0006065623001632048, Final Batch Loss: 0.0002838651998899877\n",
      "Epoch 4986, Loss: 0.0007567367792944424, Final Batch Loss: 0.00010373597615398467\n",
      "Epoch 4987, Loss: 0.0010713978249441425, Final Batch Loss: 5.817825240228558e-06\n",
      "Epoch 4988, Loss: 0.0004063241808580642, Final Batch Loss: 4.767106929648435e-06\n",
      "Epoch 4989, Loss: 0.0002627299509185832, Final Batch Loss: 1.8780454411171377e-05\n",
      "Epoch 4990, Loss: 0.0001601877097527904, Final Batch Loss: 8.612484634795692e-06\n",
      "Epoch 4991, Loss: 0.0005299872736941325, Final Batch Loss: 1.2628655895241536e-05\n",
      "Epoch 4992, Loss: 0.00034134345332859084, Final Batch Loss: 7.482717774109915e-05\n",
      "Epoch 4993, Loss: 0.0006742670375388116, Final Batch Loss: 5.5651209549978375e-05\n",
      "Epoch 4994, Loss: 0.02120798485520936, Final Batch Loss: 2.508942998247221e-05\n",
      "Epoch 4995, Loss: 0.007269464826094918, Final Batch Loss: 0.00011179863940924406\n",
      "Epoch 4996, Loss: 0.0011757049178413581, Final Batch Loss: 6.655802280874923e-05\n",
      "Epoch 4997, Loss: 0.008757463565416401, Final Batch Loss: 1.5841524145798758e-05\n",
      "Epoch 4998, Loss: 0.0004302102424844634, Final Batch Loss: 5.695534127880819e-05\n",
      "Epoch 4999, Loss: 0.006382666790159419, Final Batch Loss: 0.0001640569680603221\n",
      "Epoch 5000, Loss: 0.0003474894183455035, Final Batch Loss: 4.22858283855021e-05\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[81  1  0]\n",
      " [ 0 65  0]\n",
      " [ 0  0 73]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.98780   0.99387        82\n",
      "           1    0.98485   1.00000   0.99237        65\n",
      "           2    1.00000   1.00000   1.00000        73\n",
      "\n",
      "    accuracy                        0.99545       220\n",
      "   macro avg    0.99495   0.99593   0.99541       220\n",
      "weighted avg    0.99552   0.99545   0.99546       220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (gen): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=110, out_features=80, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=80, out_features=60, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=60, out_features=50, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Linear(in_features=50, out_features=33, bias=True)\n",
       "    (4): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator(z_dim = 110)\n",
    "load_model(gen, \"3 Label 7 Subject GAN Ablation_gen.param\")\n",
    "gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(X_test)\n",
    "latent_vectors = get_noise(size, 100)\n",
    "act_vectors = get_act_matrix(size, 3)\n",
    "usr_vectors = get_usr_matrix(size, 7)\n",
    "\n",
    "to_gen = torch.cat((latent_vectors, act_vectors[1], usr_vectors[1]), 1)\n",
    "fake_features = gen(to_gen).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[72  0  0]\n",
      " [ 0 72  0]\n",
      " [ 0  0 76]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        72\n",
      "           1    1.00000   1.00000   1.00000        72\n",
      "           2    1.00000   1.00000   1.00000        76\n",
      "\n",
      "    accuracy                        1.00000       220\n",
      "   macro avg    1.00000   1.00000   1.00000       220\n",
      "weighted avg    1.00000   1.00000   1.00000       220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix(act_vectors[0], preds.cpu()))\n",
    "print(metrics.classification_report(act_vectors[0], preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5, 7, 8, 11, 14]\n",
    "\n",
    "X, y = start_data(activities, users, \"Subject\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y)):\n",
    "    if y[k] == 1:\n",
    "        y[k] = 0\n",
    "    elif y[k] == 3:\n",
    "        y[k] = 1\n",
    "    elif y[k] == 5:\n",
    "        y[k] = 2\n",
    "    elif y[k] == 7:\n",
    "        y[k] = 3\n",
    "    elif y[k] == 8:\n",
    "        y[k] = 4\n",
    "    elif y[k] == 11:\n",
    "        y[k] = 5\n",
    "    else:\n",
    "        y[k] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model_subject = Subject_Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_subject.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.894679546356201, Final Batch Loss: 1.9603559970855713\n",
      "Epoch 2, Loss: 7.884167313575745, Final Batch Loss: 1.9705694913864136\n",
      "Epoch 3, Loss: 7.868780970573425, Final Batch Loss: 1.9658170938491821\n",
      "Epoch 4, Loss: 7.8607670068740845, Final Batch Loss: 1.9567447900772095\n",
      "Epoch 5, Loss: 7.8448837995529175, Final Batch Loss: 1.9596424102783203\n",
      "Epoch 6, Loss: 7.835121512413025, Final Batch Loss: 1.969307541847229\n",
      "Epoch 7, Loss: 7.817938685417175, Final Batch Loss: 1.9432238340377808\n",
      "Epoch 8, Loss: 7.81113600730896, Final Batch Loss: 1.9700939655303955\n",
      "Epoch 9, Loss: 7.793146133422852, Final Batch Loss: 1.9553602933883667\n",
      "Epoch 10, Loss: 7.751376390457153, Final Batch Loss: 1.922414779663086\n",
      "Epoch 11, Loss: 7.7407472133636475, Final Batch Loss: 1.9402016401290894\n",
      "Epoch 12, Loss: 7.721186876296997, Final Batch Loss: 1.9380511045455933\n",
      "Epoch 13, Loss: 7.692253947257996, Final Batch Loss: 1.9323482513427734\n",
      "Epoch 14, Loss: 7.645402908325195, Final Batch Loss: 1.9056884050369263\n",
      "Epoch 15, Loss: 7.588041067123413, Final Batch Loss: 1.8890811204910278\n",
      "Epoch 16, Loss: 7.539942741394043, Final Batch Loss: 1.8918330669403076\n",
      "Epoch 17, Loss: 7.456735134124756, Final Batch Loss: 1.8659319877624512\n",
      "Epoch 18, Loss: 7.352056503295898, Final Batch Loss: 1.8162169456481934\n",
      "Epoch 19, Loss: 7.3193570375442505, Final Batch Loss: 1.8306670188903809\n",
      "Epoch 20, Loss: 7.270859479904175, Final Batch Loss: 1.822882890701294\n",
      "Epoch 21, Loss: 7.1557440757751465, Final Batch Loss: 1.797035813331604\n",
      "Epoch 22, Loss: 7.100500464439392, Final Batch Loss: 1.7986747026443481\n",
      "Epoch 23, Loss: 7.024598240852356, Final Batch Loss: 1.7297824621200562\n",
      "Epoch 24, Loss: 6.957009792327881, Final Batch Loss: 1.6826084852218628\n",
      "Epoch 25, Loss: 6.968424677848816, Final Batch Loss: 1.733258605003357\n",
      "Epoch 26, Loss: 6.874374032020569, Final Batch Loss: 1.7567663192749023\n",
      "Epoch 27, Loss: 6.885452508926392, Final Batch Loss: 1.714463233947754\n",
      "Epoch 28, Loss: 6.825135231018066, Final Batch Loss: 1.722397804260254\n",
      "Epoch 29, Loss: 6.819312572479248, Final Batch Loss: 1.725641131401062\n",
      "Epoch 30, Loss: 6.716647028923035, Final Batch Loss: 1.696097493171692\n",
      "Epoch 31, Loss: 6.713918447494507, Final Batch Loss: 1.7357170581817627\n",
      "Epoch 32, Loss: 6.686769723892212, Final Batch Loss: 1.7229359149932861\n",
      "Epoch 33, Loss: 6.639315128326416, Final Batch Loss: 1.6899243593215942\n",
      "Epoch 34, Loss: 6.5974273681640625, Final Batch Loss: 1.655576229095459\n",
      "Epoch 35, Loss: 6.477247595787048, Final Batch Loss: 1.634348750114441\n",
      "Epoch 36, Loss: 6.552146792411804, Final Batch Loss: 1.6839605569839478\n",
      "Epoch 37, Loss: 6.464792370796204, Final Batch Loss: 1.6080503463745117\n",
      "Epoch 38, Loss: 6.47626256942749, Final Batch Loss: 1.6886574029922485\n",
      "Epoch 39, Loss: 6.392661690711975, Final Batch Loss: 1.5954502820968628\n",
      "Epoch 40, Loss: 6.344275236129761, Final Batch Loss: 1.5783958435058594\n",
      "Epoch 41, Loss: 6.337989449501038, Final Batch Loss: 1.6373063325881958\n",
      "Epoch 42, Loss: 6.287286639213562, Final Batch Loss: 1.5848596096038818\n",
      "Epoch 43, Loss: 6.213736891746521, Final Batch Loss: 1.5192856788635254\n",
      "Epoch 44, Loss: 6.16145384311676, Final Batch Loss: 1.6183258295059204\n",
      "Epoch 45, Loss: 6.093175768852234, Final Batch Loss: 1.536245346069336\n",
      "Epoch 46, Loss: 6.025691270828247, Final Batch Loss: 1.383620023727417\n",
      "Epoch 47, Loss: 6.084304332733154, Final Batch Loss: 1.4680848121643066\n",
      "Epoch 48, Loss: 5.926761507987976, Final Batch Loss: 1.3983522653579712\n",
      "Epoch 49, Loss: 6.01271665096283, Final Batch Loss: 1.5826902389526367\n",
      "Epoch 50, Loss: 5.926238059997559, Final Batch Loss: 1.4857714176177979\n",
      "Epoch 51, Loss: 5.898090839385986, Final Batch Loss: 1.5124603509902954\n",
      "Epoch 52, Loss: 5.880892515182495, Final Batch Loss: 1.434708595275879\n",
      "Epoch 53, Loss: 5.871749043464661, Final Batch Loss: 1.4474049806594849\n",
      "Epoch 54, Loss: 5.797890067100525, Final Batch Loss: 1.4095630645751953\n",
      "Epoch 55, Loss: 5.862033128738403, Final Batch Loss: 1.4636822938919067\n",
      "Epoch 56, Loss: 5.797375202178955, Final Batch Loss: 1.4093049764633179\n",
      "Epoch 57, Loss: 5.697861075401306, Final Batch Loss: 1.413783073425293\n",
      "Epoch 58, Loss: 5.675166845321655, Final Batch Loss: 1.42842698097229\n",
      "Epoch 59, Loss: 5.702800750732422, Final Batch Loss: 1.4331169128417969\n",
      "Epoch 60, Loss: 5.66662323474884, Final Batch Loss: 1.4115405082702637\n",
      "Epoch 61, Loss: 5.686829090118408, Final Batch Loss: 1.4774656295776367\n",
      "Epoch 62, Loss: 5.583694577217102, Final Batch Loss: 1.403748631477356\n",
      "Epoch 63, Loss: 5.668387532234192, Final Batch Loss: 1.4819273948669434\n",
      "Epoch 64, Loss: 5.566720008850098, Final Batch Loss: 1.3484879732131958\n",
      "Epoch 65, Loss: 5.609771490097046, Final Batch Loss: 1.515030026435852\n",
      "Epoch 66, Loss: 5.49026620388031, Final Batch Loss: 1.2638791799545288\n",
      "Epoch 67, Loss: 5.564826488494873, Final Batch Loss: 1.4509670734405518\n",
      "Epoch 68, Loss: 5.423906564712524, Final Batch Loss: 1.3492707014083862\n",
      "Epoch 69, Loss: 5.363653302192688, Final Batch Loss: 1.2797625064849854\n",
      "Epoch 70, Loss: 5.421552538871765, Final Batch Loss: 1.3387209177017212\n",
      "Epoch 71, Loss: 5.469651103019714, Final Batch Loss: 1.4338005781173706\n",
      "Epoch 72, Loss: 5.491657495498657, Final Batch Loss: 1.386263132095337\n",
      "Epoch 73, Loss: 5.31596577167511, Final Batch Loss: 1.2817870378494263\n",
      "Epoch 74, Loss: 5.389864206314087, Final Batch Loss: 1.3857240676879883\n",
      "Epoch 75, Loss: 5.385134339332581, Final Batch Loss: 1.2591434717178345\n",
      "Epoch 76, Loss: 5.420283913612366, Final Batch Loss: 1.3824189901351929\n",
      "Epoch 77, Loss: 5.357577919960022, Final Batch Loss: 1.4038010835647583\n",
      "Epoch 78, Loss: 5.315563440322876, Final Batch Loss: 1.3144323825836182\n",
      "Epoch 79, Loss: 5.298540472984314, Final Batch Loss: 1.2828515768051147\n",
      "Epoch 80, Loss: 5.266428351402283, Final Batch Loss: 1.4546420574188232\n",
      "Epoch 81, Loss: 5.200003981590271, Final Batch Loss: 1.2640037536621094\n",
      "Epoch 82, Loss: 5.232617259025574, Final Batch Loss: 1.2896064519882202\n",
      "Epoch 83, Loss: 5.203936815261841, Final Batch Loss: 1.2154834270477295\n",
      "Epoch 84, Loss: 5.041858196258545, Final Batch Loss: 1.1520084142684937\n",
      "Epoch 85, Loss: 5.189762711524963, Final Batch Loss: 1.3119653463363647\n",
      "Epoch 86, Loss: 5.275240540504456, Final Batch Loss: 1.302703619003296\n",
      "Epoch 87, Loss: 5.313007116317749, Final Batch Loss: 1.3796848058700562\n",
      "Epoch 88, Loss: 5.1439714431762695, Final Batch Loss: 1.2401134967803955\n",
      "Epoch 89, Loss: 5.02826988697052, Final Batch Loss: 1.2410624027252197\n",
      "Epoch 90, Loss: 5.156559228897095, Final Batch Loss: 1.3095358610153198\n",
      "Epoch 91, Loss: 5.110534191131592, Final Batch Loss: 1.320555329322815\n",
      "Epoch 92, Loss: 5.2006995677948, Final Batch Loss: 1.337396264076233\n",
      "Epoch 93, Loss: 5.0895771980285645, Final Batch Loss: 1.1894280910491943\n",
      "Epoch 94, Loss: 5.171312928199768, Final Batch Loss: 1.271958827972412\n",
      "Epoch 95, Loss: 5.12650203704834, Final Batch Loss: 1.303101658821106\n",
      "Epoch 96, Loss: 5.10696280002594, Final Batch Loss: 1.3367701768875122\n",
      "Epoch 97, Loss: 4.898768424987793, Final Batch Loss: 1.157436490058899\n",
      "Epoch 98, Loss: 5.001195430755615, Final Batch Loss: 1.3221328258514404\n",
      "Epoch 99, Loss: 4.919724822044373, Final Batch Loss: 1.1390177011489868\n",
      "Epoch 100, Loss: 5.038379669189453, Final Batch Loss: 1.3532695770263672\n",
      "Epoch 101, Loss: 4.991093277931213, Final Batch Loss: 1.2311450242996216\n",
      "Epoch 102, Loss: 5.074386954307556, Final Batch Loss: 1.2009576559066772\n",
      "Epoch 103, Loss: 5.049092173576355, Final Batch Loss: 1.2773844003677368\n",
      "Epoch 104, Loss: 4.980477690696716, Final Batch Loss: 1.2921737432479858\n",
      "Epoch 105, Loss: 4.9502716064453125, Final Batch Loss: 1.2213398218154907\n",
      "Epoch 106, Loss: 4.914356708526611, Final Batch Loss: 1.1993741989135742\n",
      "Epoch 107, Loss: 4.903043031692505, Final Batch Loss: 1.2675411701202393\n",
      "Epoch 108, Loss: 4.890225052833557, Final Batch Loss: 1.2001858949661255\n",
      "Epoch 109, Loss: 4.965985178947449, Final Batch Loss: 1.2566279172897339\n",
      "Epoch 110, Loss: 4.899533271789551, Final Batch Loss: 1.1808058023452759\n",
      "Epoch 111, Loss: 4.896528840065002, Final Batch Loss: 1.2804925441741943\n",
      "Epoch 112, Loss: 4.841992259025574, Final Batch Loss: 1.347595453262329\n",
      "Epoch 113, Loss: 4.868012189865112, Final Batch Loss: 1.182060718536377\n",
      "Epoch 114, Loss: 4.965311884880066, Final Batch Loss: 1.3849846124649048\n",
      "Epoch 115, Loss: 4.778429388999939, Final Batch Loss: 1.0566414594650269\n",
      "Epoch 116, Loss: 4.799901843070984, Final Batch Loss: 1.2744102478027344\n",
      "Epoch 117, Loss: 4.757840871810913, Final Batch Loss: 1.109403133392334\n",
      "Epoch 118, Loss: 4.802877426147461, Final Batch Loss: 1.2287596464157104\n",
      "Epoch 119, Loss: 4.8657649755477905, Final Batch Loss: 1.2834317684173584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120, Loss: 4.761829853057861, Final Batch Loss: 1.1650975942611694\n",
      "Epoch 121, Loss: 4.720450282096863, Final Batch Loss: 1.2311360836029053\n",
      "Epoch 122, Loss: 4.768608212471008, Final Batch Loss: 1.2076743841171265\n",
      "Epoch 123, Loss: 4.627653360366821, Final Batch Loss: 1.0995984077453613\n",
      "Epoch 124, Loss: 4.835886001586914, Final Batch Loss: 1.2386032342910767\n",
      "Epoch 125, Loss: 4.746479868888855, Final Batch Loss: 1.1711076498031616\n",
      "Epoch 126, Loss: 4.692871809005737, Final Batch Loss: 1.3063807487487793\n",
      "Epoch 127, Loss: 4.638744950294495, Final Batch Loss: 1.1661179065704346\n",
      "Epoch 128, Loss: 4.617733359336853, Final Batch Loss: 1.1089468002319336\n",
      "Epoch 129, Loss: 4.543329238891602, Final Batch Loss: 1.0282597541809082\n",
      "Epoch 130, Loss: 4.66612446308136, Final Batch Loss: 1.1977118253707886\n",
      "Epoch 131, Loss: 4.675183296203613, Final Batch Loss: 1.153680682182312\n",
      "Epoch 132, Loss: 4.587562441825867, Final Batch Loss: 1.0570013523101807\n",
      "Epoch 133, Loss: 4.793037056922913, Final Batch Loss: 1.1854568719863892\n",
      "Epoch 134, Loss: 4.671757698059082, Final Batch Loss: 1.1505635976791382\n",
      "Epoch 135, Loss: 4.60238790512085, Final Batch Loss: 1.168433666229248\n",
      "Epoch 136, Loss: 4.464672803878784, Final Batch Loss: 1.0716999769210815\n",
      "Epoch 137, Loss: 4.597687602043152, Final Batch Loss: 1.2237848043441772\n",
      "Epoch 138, Loss: 4.591812610626221, Final Batch Loss: 1.150809407234192\n",
      "Epoch 139, Loss: 4.539531707763672, Final Batch Loss: 1.1141290664672852\n",
      "Epoch 140, Loss: 4.435368895530701, Final Batch Loss: 1.0895878076553345\n",
      "Epoch 141, Loss: 4.613324522972107, Final Batch Loss: 1.1238322257995605\n",
      "Epoch 142, Loss: 4.373703241348267, Final Batch Loss: 1.0086641311645508\n",
      "Epoch 143, Loss: 4.318689465522766, Final Batch Loss: 1.0014595985412598\n",
      "Epoch 144, Loss: 4.4623788595199585, Final Batch Loss: 1.1422662734985352\n",
      "Epoch 145, Loss: 4.469333529472351, Final Batch Loss: 1.117567777633667\n",
      "Epoch 146, Loss: 4.45977783203125, Final Batch Loss: 1.0882022380828857\n",
      "Epoch 147, Loss: 4.407695293426514, Final Batch Loss: 1.1265599727630615\n",
      "Epoch 148, Loss: 4.478268623352051, Final Batch Loss: 1.1472278833389282\n",
      "Epoch 149, Loss: 4.377083957195282, Final Batch Loss: 0.9385268092155457\n",
      "Epoch 150, Loss: 4.349238753318787, Final Batch Loss: 1.0436896085739136\n",
      "Epoch 151, Loss: 4.551131963729858, Final Batch Loss: 1.217851161956787\n",
      "Epoch 152, Loss: 4.445720672607422, Final Batch Loss: 1.0623693466186523\n",
      "Epoch 153, Loss: 4.376585602760315, Final Batch Loss: 1.105391502380371\n",
      "Epoch 154, Loss: 4.394816517829895, Final Batch Loss: 1.1363428831100464\n",
      "Epoch 155, Loss: 4.339978575706482, Final Batch Loss: 1.095777153968811\n",
      "Epoch 156, Loss: 4.399508714675903, Final Batch Loss: 1.1560003757476807\n",
      "Epoch 157, Loss: 4.251451373100281, Final Batch Loss: 1.0262113809585571\n",
      "Epoch 158, Loss: 4.3438339829444885, Final Batch Loss: 1.152936339378357\n",
      "Epoch 159, Loss: 4.273679852485657, Final Batch Loss: 1.0999786853790283\n",
      "Epoch 160, Loss: 4.165223002433777, Final Batch Loss: 0.9889260530471802\n",
      "Epoch 161, Loss: 4.153489589691162, Final Batch Loss: 0.9856182336807251\n",
      "Epoch 162, Loss: 4.350024342536926, Final Batch Loss: 1.2097376585006714\n",
      "Epoch 163, Loss: 4.107430815696716, Final Batch Loss: 0.9499905109405518\n",
      "Epoch 164, Loss: 4.179765045642853, Final Batch Loss: 1.0557770729064941\n",
      "Epoch 165, Loss: 4.221005916595459, Final Batch Loss: 1.0636677742004395\n",
      "Epoch 166, Loss: 3.9534398317337036, Final Batch Loss: 0.8759119510650635\n",
      "Epoch 167, Loss: 4.124962627887726, Final Batch Loss: 0.9062705636024475\n",
      "Epoch 168, Loss: 4.095871567726135, Final Batch Loss: 0.9461398124694824\n",
      "Epoch 169, Loss: 4.109268665313721, Final Batch Loss: 0.9920860528945923\n",
      "Epoch 170, Loss: 4.0730114579200745, Final Batch Loss: 0.978101909160614\n",
      "Epoch 171, Loss: 4.091591894626617, Final Batch Loss: 1.0704662799835205\n",
      "Epoch 172, Loss: 4.0386359095573425, Final Batch Loss: 1.043534278869629\n",
      "Epoch 173, Loss: 4.049520552158356, Final Batch Loss: 1.0150192975997925\n",
      "Epoch 174, Loss: 4.066900432109833, Final Batch Loss: 1.0043575763702393\n",
      "Epoch 175, Loss: 4.002665042877197, Final Batch Loss: 0.990520715713501\n",
      "Epoch 176, Loss: 4.052466511726379, Final Batch Loss: 1.027206301689148\n",
      "Epoch 177, Loss: 3.9371728897094727, Final Batch Loss: 0.9800860285758972\n",
      "Epoch 178, Loss: 3.920808434486389, Final Batch Loss: 1.022596001625061\n",
      "Epoch 179, Loss: 4.162635028362274, Final Batch Loss: 1.1411625146865845\n",
      "Epoch 180, Loss: 3.9029149413108826, Final Batch Loss: 1.0483132600784302\n",
      "Epoch 181, Loss: 3.9197956323623657, Final Batch Loss: 1.0811535120010376\n",
      "Epoch 182, Loss: 3.959219753742218, Final Batch Loss: 0.968269944190979\n",
      "Epoch 183, Loss: 3.850843846797943, Final Batch Loss: 0.9120957851409912\n",
      "Epoch 184, Loss: 3.95994770526886, Final Batch Loss: 1.037355899810791\n",
      "Epoch 185, Loss: 3.8516452312469482, Final Batch Loss: 0.8873751163482666\n",
      "Epoch 186, Loss: 3.827227294445038, Final Batch Loss: 1.050187110900879\n",
      "Epoch 187, Loss: 3.764917016029358, Final Batch Loss: 0.9226208925247192\n",
      "Epoch 188, Loss: 3.9187191128730774, Final Batch Loss: 0.9667767882347107\n",
      "Epoch 189, Loss: 3.819474935531616, Final Batch Loss: 0.9594318270683289\n",
      "Epoch 190, Loss: 3.8761751651763916, Final Batch Loss: 0.8554714918136597\n",
      "Epoch 191, Loss: 3.768222987651825, Final Batch Loss: 0.8439005017280579\n",
      "Epoch 192, Loss: 3.809350907802582, Final Batch Loss: 0.9722469449043274\n",
      "Epoch 193, Loss: 3.876486599445343, Final Batch Loss: 0.9810188412666321\n",
      "Epoch 194, Loss: 3.6516435742378235, Final Batch Loss: 0.8634980916976929\n",
      "Epoch 195, Loss: 3.7263774275779724, Final Batch Loss: 0.8852365612983704\n",
      "Epoch 196, Loss: 3.710880219936371, Final Batch Loss: 0.9187013506889343\n",
      "Epoch 197, Loss: 3.7654471397399902, Final Batch Loss: 0.8883872032165527\n",
      "Epoch 198, Loss: 3.8055384159088135, Final Batch Loss: 0.8980669975280762\n",
      "Epoch 199, Loss: 3.6874446272850037, Final Batch Loss: 0.969403088092804\n",
      "Epoch 200, Loss: 3.7344574332237244, Final Batch Loss: 0.9192520976066589\n",
      "Epoch 201, Loss: 3.827212393283844, Final Batch Loss: 0.8807038068771362\n",
      "Epoch 202, Loss: 3.6564122438430786, Final Batch Loss: 0.9085209369659424\n",
      "Epoch 203, Loss: 3.6165695786476135, Final Batch Loss: 0.994805097579956\n",
      "Epoch 204, Loss: 3.5148943066596985, Final Batch Loss: 0.919422447681427\n",
      "Epoch 205, Loss: 3.7177754044532776, Final Batch Loss: 0.9873716831207275\n",
      "Epoch 206, Loss: 3.5956700444221497, Final Batch Loss: 0.8753396272659302\n",
      "Epoch 207, Loss: 3.6206621527671814, Final Batch Loss: 0.9830670952796936\n",
      "Epoch 208, Loss: 3.605059862136841, Final Batch Loss: 0.8959699273109436\n",
      "Epoch 209, Loss: 3.754424273967743, Final Batch Loss: 0.953276515007019\n",
      "Epoch 210, Loss: 3.5539883971214294, Final Batch Loss: 0.9909088015556335\n",
      "Epoch 211, Loss: 3.543211877346039, Final Batch Loss: 0.9910208582878113\n",
      "Epoch 212, Loss: 3.6413029432296753, Final Batch Loss: 1.0209498405456543\n",
      "Epoch 213, Loss: 3.4488083124160767, Final Batch Loss: 0.7502107620239258\n",
      "Epoch 214, Loss: 3.6018224954605103, Final Batch Loss: 0.8789597749710083\n",
      "Epoch 215, Loss: 3.52225661277771, Final Batch Loss: 0.8230835795402527\n",
      "Epoch 216, Loss: 3.5957518815994263, Final Batch Loss: 0.9078819751739502\n",
      "Epoch 217, Loss: 3.523849308490753, Final Batch Loss: 0.857748806476593\n",
      "Epoch 218, Loss: 3.380430281162262, Final Batch Loss: 0.7203001379966736\n",
      "Epoch 219, Loss: 3.5365723967552185, Final Batch Loss: 0.9619234204292297\n",
      "Epoch 220, Loss: 3.5672507882118225, Final Batch Loss: 0.9620002508163452\n",
      "Epoch 221, Loss: 3.596743643283844, Final Batch Loss: 0.9502554535865784\n",
      "Epoch 222, Loss: 3.4073426127433777, Final Batch Loss: 0.7861560583114624\n",
      "Epoch 223, Loss: 3.4459672570228577, Final Batch Loss: 0.8438938856124878\n",
      "Epoch 224, Loss: 3.3792460560798645, Final Batch Loss: 0.8010573983192444\n",
      "Epoch 225, Loss: 3.3847976326942444, Final Batch Loss: 0.8077638745307922\n",
      "Epoch 226, Loss: 3.512749433517456, Final Batch Loss: 0.963715136051178\n",
      "Epoch 227, Loss: 3.307638704776764, Final Batch Loss: 0.8980538845062256\n",
      "Epoch 228, Loss: 3.250400424003601, Final Batch Loss: 0.767207145690918\n",
      "Epoch 229, Loss: 3.6046250462532043, Final Batch Loss: 0.9465199112892151\n",
      "Epoch 230, Loss: 3.4764103293418884, Final Batch Loss: 0.865357518196106\n",
      "Epoch 231, Loss: 3.400487542152405, Final Batch Loss: 0.973736584186554\n",
      "Epoch 232, Loss: 3.383753180503845, Final Batch Loss: 0.809057354927063\n",
      "Epoch 233, Loss: 3.4018629789352417, Final Batch Loss: 0.8648688793182373\n",
      "Epoch 234, Loss: 3.268181324005127, Final Batch Loss: 0.8666662573814392\n",
      "Epoch 235, Loss: 3.3477590084075928, Final Batch Loss: 0.874280571937561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236, Loss: 3.382176458835602, Final Batch Loss: 0.8651947975158691\n",
      "Epoch 237, Loss: 3.4292789697647095, Final Batch Loss: 0.7854171991348267\n",
      "Epoch 238, Loss: 3.4484748244285583, Final Batch Loss: 0.8241082429885864\n",
      "Epoch 239, Loss: 3.346791386604309, Final Batch Loss: 0.7900749444961548\n",
      "Epoch 240, Loss: 3.25363427400589, Final Batch Loss: 0.8025116324424744\n",
      "Epoch 241, Loss: 3.249528169631958, Final Batch Loss: 0.7394524216651917\n",
      "Epoch 242, Loss: 3.4468418955802917, Final Batch Loss: 0.8999666571617126\n",
      "Epoch 243, Loss: 3.334760069847107, Final Batch Loss: 0.7846537828445435\n",
      "Epoch 244, Loss: 3.2972081303596497, Final Batch Loss: 0.8107482194900513\n",
      "Epoch 245, Loss: 3.4304621815681458, Final Batch Loss: 0.853219211101532\n",
      "Epoch 246, Loss: 3.1606516242027283, Final Batch Loss: 0.7256135940551758\n",
      "Epoch 247, Loss: 3.3860356211662292, Final Batch Loss: 0.8874524831771851\n",
      "Epoch 248, Loss: 3.3434910774230957, Final Batch Loss: 0.8090512752532959\n",
      "Epoch 249, Loss: 3.313083291053772, Final Batch Loss: 0.8561477065086365\n",
      "Epoch 250, Loss: 3.312574088573456, Final Batch Loss: 0.7134220004081726\n",
      "Epoch 251, Loss: 3.231777310371399, Final Batch Loss: 0.82194983959198\n",
      "Epoch 252, Loss: 3.311988413333893, Final Batch Loss: 0.8675017952919006\n",
      "Epoch 253, Loss: 3.268880784511566, Final Batch Loss: 0.8935657739639282\n",
      "Epoch 254, Loss: 3.255515217781067, Final Batch Loss: 0.749327540397644\n",
      "Epoch 255, Loss: 3.232604444026947, Final Batch Loss: 0.7642825841903687\n",
      "Epoch 256, Loss: 3.3688125610351562, Final Batch Loss: 0.8248782753944397\n",
      "Epoch 257, Loss: 3.2876766324043274, Final Batch Loss: 0.8288780450820923\n",
      "Epoch 258, Loss: 3.2136480808258057, Final Batch Loss: 0.7932561635971069\n",
      "Epoch 259, Loss: 3.2973186373710632, Final Batch Loss: 0.7800117135047913\n",
      "Epoch 260, Loss: 3.284966468811035, Final Batch Loss: 0.8215832710266113\n",
      "Epoch 261, Loss: 3.1794689297676086, Final Batch Loss: 0.7297370433807373\n",
      "Epoch 262, Loss: 3.0453598499298096, Final Batch Loss: 0.7479799389839172\n",
      "Epoch 263, Loss: 3.2113410234451294, Final Batch Loss: 0.7359945774078369\n",
      "Epoch 264, Loss: 3.1479057669639587, Final Batch Loss: 0.8018706440925598\n",
      "Epoch 265, Loss: 3.2139626145362854, Final Batch Loss: 0.7652962803840637\n",
      "Epoch 266, Loss: 3.2900518774986267, Final Batch Loss: 0.8230366706848145\n",
      "Epoch 267, Loss: 3.1322948336601257, Final Batch Loss: 0.7387593388557434\n",
      "Epoch 268, Loss: 3.274195373058319, Final Batch Loss: 0.9718007445335388\n",
      "Epoch 269, Loss: 3.1807231307029724, Final Batch Loss: 0.7493906617164612\n",
      "Epoch 270, Loss: 3.2489115595817566, Final Batch Loss: 0.8690171837806702\n",
      "Epoch 271, Loss: 3.1950840950012207, Final Batch Loss: 0.7393916845321655\n",
      "Epoch 272, Loss: 3.113259434700012, Final Batch Loss: 0.7960029244422913\n",
      "Epoch 273, Loss: 3.1516682505607605, Final Batch Loss: 0.7897021174430847\n",
      "Epoch 274, Loss: 3.1612132787704468, Final Batch Loss: 0.7961061000823975\n",
      "Epoch 275, Loss: 3.2586430311203003, Final Batch Loss: 0.8845738172531128\n",
      "Epoch 276, Loss: 3.215549647808075, Final Batch Loss: 0.7928942441940308\n",
      "Epoch 277, Loss: 3.167409598827362, Final Batch Loss: 0.8785610795021057\n",
      "Epoch 278, Loss: 3.2448906898498535, Final Batch Loss: 0.8750838041305542\n",
      "Epoch 279, Loss: 2.99784654378891, Final Batch Loss: 0.7199229001998901\n",
      "Epoch 280, Loss: 3.288707911968231, Final Batch Loss: 0.7806631922721863\n",
      "Epoch 281, Loss: 3.107769191265106, Final Batch Loss: 0.7136057615280151\n",
      "Epoch 282, Loss: 3.0666879415512085, Final Batch Loss: 0.7819509506225586\n",
      "Epoch 283, Loss: 3.0411680936813354, Final Batch Loss: 0.7391154170036316\n",
      "Epoch 284, Loss: 3.1205139756202698, Final Batch Loss: 0.7807906866073608\n",
      "Epoch 285, Loss: 3.1624338030815125, Final Batch Loss: 0.7587116956710815\n",
      "Epoch 286, Loss: 3.0563783049583435, Final Batch Loss: 0.7512629628181458\n",
      "Epoch 287, Loss: 3.1731719970703125, Final Batch Loss: 0.8344030976295471\n",
      "Epoch 288, Loss: 3.1327609419822693, Final Batch Loss: 0.9134673476219177\n",
      "Epoch 289, Loss: 3.1620288491249084, Final Batch Loss: 0.797511875629425\n",
      "Epoch 290, Loss: 3.028619408607483, Final Batch Loss: 0.7521426677703857\n",
      "Epoch 291, Loss: 3.001059889793396, Final Batch Loss: 0.7605201601982117\n",
      "Epoch 292, Loss: 3.102491021156311, Final Batch Loss: 0.679693877696991\n",
      "Epoch 293, Loss: 3.1548807621002197, Final Batch Loss: 0.7739660739898682\n",
      "Epoch 294, Loss: 3.0165130496025085, Final Batch Loss: 0.6885120272636414\n",
      "Epoch 295, Loss: 3.0806090235710144, Final Batch Loss: 0.8143172264099121\n",
      "Epoch 296, Loss: 3.2288445234298706, Final Batch Loss: 0.8125628232955933\n",
      "Epoch 297, Loss: 3.107504189014435, Final Batch Loss: 0.8701839447021484\n",
      "Epoch 298, Loss: 3.0382611751556396, Final Batch Loss: 0.7622236609458923\n",
      "Epoch 299, Loss: 3.0132656693458557, Final Batch Loss: 0.6834109425544739\n",
      "Epoch 300, Loss: 3.1225393414497375, Final Batch Loss: 0.6621356010437012\n",
      "Epoch 301, Loss: 3.1310359239578247, Final Batch Loss: 0.7621389627456665\n",
      "Epoch 302, Loss: 3.201276659965515, Final Batch Loss: 0.8611459732055664\n",
      "Epoch 303, Loss: 3.055116057395935, Final Batch Loss: 0.8084146976470947\n",
      "Epoch 304, Loss: 3.0388986468315125, Final Batch Loss: 0.7962962985038757\n",
      "Epoch 305, Loss: 2.9961528182029724, Final Batch Loss: 0.7489590048789978\n",
      "Epoch 306, Loss: 2.9455583095550537, Final Batch Loss: 0.7340090274810791\n",
      "Epoch 307, Loss: 2.9508419036865234, Final Batch Loss: 0.7113458514213562\n",
      "Epoch 308, Loss: 3.1604238748550415, Final Batch Loss: 0.8141443133354187\n",
      "Epoch 309, Loss: 3.0833309292793274, Final Batch Loss: 0.9197795987129211\n",
      "Epoch 310, Loss: 3.102512300014496, Final Batch Loss: 0.8282667398452759\n",
      "Epoch 311, Loss: 3.04485946893692, Final Batch Loss: 0.749039351940155\n",
      "Epoch 312, Loss: 3.0067219734191895, Final Batch Loss: 0.7146347165107727\n",
      "Epoch 313, Loss: 3.018444240093231, Final Batch Loss: 0.8335185050964355\n",
      "Epoch 314, Loss: 2.975128650665283, Final Batch Loss: 0.7555474042892456\n",
      "Epoch 315, Loss: 3.0012013912200928, Final Batch Loss: 0.7918121218681335\n",
      "Epoch 316, Loss: 3.0659574270248413, Final Batch Loss: 0.7678182125091553\n",
      "Epoch 317, Loss: 3.133365035057068, Final Batch Loss: 1.031104564666748\n",
      "Epoch 318, Loss: 2.9061580300331116, Final Batch Loss: 0.6444058418273926\n",
      "Epoch 319, Loss: 2.9752246141433716, Final Batch Loss: 0.7166100144386292\n",
      "Epoch 320, Loss: 2.931912362575531, Final Batch Loss: 0.7935799360275269\n",
      "Epoch 321, Loss: 3.0034106373786926, Final Batch Loss: 0.6727156043052673\n",
      "Epoch 322, Loss: 2.929699182510376, Final Batch Loss: 0.7480933666229248\n",
      "Epoch 323, Loss: 3.0764241814613342, Final Batch Loss: 0.7750251889228821\n",
      "Epoch 324, Loss: 3.0603727102279663, Final Batch Loss: 0.7316807508468628\n",
      "Epoch 325, Loss: 2.9157538414001465, Final Batch Loss: 0.7368914484977722\n",
      "Epoch 326, Loss: 3.141808271408081, Final Batch Loss: 0.9390085339546204\n",
      "Epoch 327, Loss: 2.9160494804382324, Final Batch Loss: 0.6735177636146545\n",
      "Epoch 328, Loss: 3.0059797167778015, Final Batch Loss: 0.7652714252471924\n",
      "Epoch 329, Loss: 2.9248828291893005, Final Batch Loss: 0.6031427383422852\n",
      "Epoch 330, Loss: 2.9071052074432373, Final Batch Loss: 0.6113462448120117\n",
      "Epoch 331, Loss: 2.9909929037094116, Final Batch Loss: 0.6875709295272827\n",
      "Epoch 332, Loss: 2.9779775142669678, Final Batch Loss: 0.7069376707077026\n",
      "Epoch 333, Loss: 3.0208417773246765, Final Batch Loss: 0.7410165667533875\n",
      "Epoch 334, Loss: 2.9833913445472717, Final Batch Loss: 0.7815588116645813\n",
      "Epoch 335, Loss: 2.987355887889862, Final Batch Loss: 0.7719957828521729\n",
      "Epoch 336, Loss: 2.8181349635124207, Final Batch Loss: 0.6004889011383057\n",
      "Epoch 337, Loss: 3.012033522129059, Final Batch Loss: 0.761957585811615\n",
      "Epoch 338, Loss: 2.9099701046943665, Final Batch Loss: 0.7633454203605652\n",
      "Epoch 339, Loss: 2.804989516735077, Final Batch Loss: 0.7101931571960449\n",
      "Epoch 340, Loss: 2.910623252391815, Final Batch Loss: 0.6986984014511108\n",
      "Epoch 341, Loss: 2.8490949869155884, Final Batch Loss: 0.7322410941123962\n",
      "Epoch 342, Loss: 2.914257764816284, Final Batch Loss: 0.7977809309959412\n",
      "Epoch 343, Loss: 2.7108116149902344, Final Batch Loss: 0.6006016135215759\n",
      "Epoch 344, Loss: 2.89292049407959, Final Batch Loss: 0.7095770835876465\n",
      "Epoch 345, Loss: 2.8082786798477173, Final Batch Loss: 0.7179971933364868\n",
      "Epoch 346, Loss: 2.883530378341675, Final Batch Loss: 0.778361439704895\n",
      "Epoch 347, Loss: 2.9070810675621033, Final Batch Loss: 0.8106875419616699\n",
      "Epoch 348, Loss: 2.928570568561554, Final Batch Loss: 0.6752051711082458\n",
      "Epoch 349, Loss: 2.8503034114837646, Final Batch Loss: 0.7305417060852051\n",
      "Epoch 350, Loss: 2.9579554200172424, Final Batch Loss: 0.8465089201927185\n",
      "Epoch 351, Loss: 2.915173590183258, Final Batch Loss: 0.7565017938613892\n",
      "Epoch 352, Loss: 2.9957950711250305, Final Batch Loss: 0.8775126338005066\n",
      "Epoch 353, Loss: 2.8308823704719543, Final Batch Loss: 0.7019310593605042\n",
      "Epoch 354, Loss: 2.864807426929474, Final Batch Loss: 0.7817649245262146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355, Loss: 2.8371059894561768, Final Batch Loss: 0.6314424276351929\n",
      "Epoch 356, Loss: 2.9289382696151733, Final Batch Loss: 0.7464593052864075\n",
      "Epoch 357, Loss: 2.8546247482299805, Final Batch Loss: 0.752282977104187\n",
      "Epoch 358, Loss: 2.830771028995514, Final Batch Loss: 0.6865600943565369\n",
      "Epoch 359, Loss: 2.9037293195724487, Final Batch Loss: 0.740623414516449\n",
      "Epoch 360, Loss: 2.892388880252838, Final Batch Loss: 0.7007708549499512\n",
      "Epoch 361, Loss: 2.850001037120819, Final Batch Loss: 0.7431154847145081\n",
      "Epoch 362, Loss: 2.9414098858833313, Final Batch Loss: 0.7018176317214966\n",
      "Epoch 363, Loss: 2.7979738116264343, Final Batch Loss: 0.696622371673584\n",
      "Epoch 364, Loss: 2.9103307127952576, Final Batch Loss: 0.7063626050949097\n",
      "Epoch 365, Loss: 2.764580011367798, Final Batch Loss: 0.6654723286628723\n",
      "Epoch 366, Loss: 2.918960452079773, Final Batch Loss: 0.7269576787948608\n",
      "Epoch 367, Loss: 2.746744394302368, Final Batch Loss: 0.6782156229019165\n",
      "Epoch 368, Loss: 2.9935149550437927, Final Batch Loss: 0.8021602034568787\n",
      "Epoch 369, Loss: 2.8516315817832947, Final Batch Loss: 0.6269330978393555\n",
      "Epoch 370, Loss: 2.8911643624305725, Final Batch Loss: 0.7730096578598022\n",
      "Epoch 371, Loss: 2.850311577320099, Final Batch Loss: 0.7335147261619568\n",
      "Epoch 372, Loss: 2.873885452747345, Final Batch Loss: 0.7253032326698303\n",
      "Epoch 373, Loss: 2.958240270614624, Final Batch Loss: 0.8582580089569092\n",
      "Epoch 374, Loss: 2.886112928390503, Final Batch Loss: 0.7657977938652039\n",
      "Epoch 375, Loss: 2.7773008346557617, Final Batch Loss: 0.5651241540908813\n",
      "Epoch 376, Loss: 2.962630271911621, Final Batch Loss: 0.6776825189590454\n",
      "Epoch 377, Loss: 2.6118216514587402, Final Batch Loss: 0.640737771987915\n",
      "Epoch 378, Loss: 2.8958922028541565, Final Batch Loss: 0.656052827835083\n",
      "Epoch 379, Loss: 2.851852059364319, Final Batch Loss: 0.7629866003990173\n",
      "Epoch 380, Loss: 2.87821501493454, Final Batch Loss: 0.817573606967926\n",
      "Epoch 381, Loss: 2.700132668018341, Final Batch Loss: 0.6118233799934387\n",
      "Epoch 382, Loss: 2.7942018508911133, Final Batch Loss: 0.6258745789527893\n",
      "Epoch 383, Loss: 2.8645381331443787, Final Batch Loss: 0.8776115775108337\n",
      "Epoch 384, Loss: 2.8721418380737305, Final Batch Loss: 0.7292785048484802\n",
      "Epoch 385, Loss: 2.727411448955536, Final Batch Loss: 0.620919942855835\n",
      "Epoch 386, Loss: 2.8124213814735413, Final Batch Loss: 0.707085371017456\n",
      "Epoch 387, Loss: 2.7661091089248657, Final Batch Loss: 0.6844449043273926\n",
      "Epoch 388, Loss: 2.8767365217208862, Final Batch Loss: 0.7855589389801025\n",
      "Epoch 389, Loss: 2.7007585763931274, Final Batch Loss: 0.6281309723854065\n",
      "Epoch 390, Loss: 2.71519672870636, Final Batch Loss: 0.6276100873947144\n",
      "Epoch 391, Loss: 2.651715040206909, Final Batch Loss: 0.6249348521232605\n",
      "Epoch 392, Loss: 2.8958658576011658, Final Batch Loss: 0.7407614588737488\n",
      "Epoch 393, Loss: 2.799065351486206, Final Batch Loss: 0.7302190065383911\n",
      "Epoch 394, Loss: 2.831115484237671, Final Batch Loss: 0.7300825119018555\n",
      "Epoch 395, Loss: 2.725650668144226, Final Batch Loss: 0.728679895401001\n",
      "Epoch 396, Loss: 2.8876941204071045, Final Batch Loss: 0.7667509913444519\n",
      "Epoch 397, Loss: 2.846232533454895, Final Batch Loss: 0.6610066294670105\n",
      "Epoch 398, Loss: 2.7861526012420654, Final Batch Loss: 0.7131558656692505\n",
      "Epoch 399, Loss: 2.8938105702400208, Final Batch Loss: 0.7052361965179443\n",
      "Epoch 400, Loss: 2.816489100456238, Final Batch Loss: 0.7515395283699036\n",
      "Epoch 401, Loss: 2.7153600454330444, Final Batch Loss: 0.6543368697166443\n",
      "Epoch 402, Loss: 2.852541208267212, Final Batch Loss: 0.6996282935142517\n",
      "Epoch 403, Loss: 2.6788522005081177, Final Batch Loss: 0.6474440693855286\n",
      "Epoch 404, Loss: 2.674095332622528, Final Batch Loss: 0.6066452264785767\n",
      "Epoch 405, Loss: 2.857166051864624, Final Batch Loss: 0.691527247428894\n",
      "Epoch 406, Loss: 2.895445466041565, Final Batch Loss: 0.8234970569610596\n",
      "Epoch 407, Loss: 2.945809304714203, Final Batch Loss: 0.8032105565071106\n",
      "Epoch 408, Loss: 2.806730270385742, Final Batch Loss: 0.7390780448913574\n",
      "Epoch 409, Loss: 2.916287899017334, Final Batch Loss: 0.742893636226654\n",
      "Epoch 410, Loss: 2.819844126701355, Final Batch Loss: 0.8014069199562073\n",
      "Epoch 411, Loss: 2.8552780747413635, Final Batch Loss: 0.7396359443664551\n",
      "Epoch 412, Loss: 2.727247953414917, Final Batch Loss: 0.6539185047149658\n",
      "Epoch 413, Loss: 2.7669806480407715, Final Batch Loss: 0.7749982476234436\n",
      "Epoch 414, Loss: 2.823537230491638, Final Batch Loss: 0.7850772738456726\n",
      "Epoch 415, Loss: 2.742291569709778, Final Batch Loss: 0.7536700963973999\n",
      "Epoch 416, Loss: 2.861716151237488, Final Batch Loss: 0.7510536313056946\n",
      "Epoch 417, Loss: 2.7216912508010864, Final Batch Loss: 0.658549427986145\n",
      "Epoch 418, Loss: 2.76727694272995, Final Batch Loss: 0.6209424734115601\n",
      "Epoch 419, Loss: 2.6857444643974304, Final Batch Loss: 0.7008712291717529\n",
      "Epoch 420, Loss: 2.7062317728996277, Final Batch Loss: 0.6914508938789368\n",
      "Epoch 421, Loss: 2.725107431411743, Final Batch Loss: 0.6753906011581421\n",
      "Epoch 422, Loss: 2.792058289051056, Final Batch Loss: 0.7172141671180725\n",
      "Epoch 423, Loss: 2.68092542886734, Final Batch Loss: 0.7484971880912781\n",
      "Epoch 424, Loss: 2.644567131996155, Final Batch Loss: 0.598471462726593\n",
      "Epoch 425, Loss: 2.6647214889526367, Final Batch Loss: 0.644442081451416\n",
      "Epoch 426, Loss: 2.7258764505386353, Final Batch Loss: 0.695121169090271\n",
      "Epoch 427, Loss: 2.693736433982849, Final Batch Loss: 0.6986528635025024\n",
      "Epoch 428, Loss: 2.7467021346092224, Final Batch Loss: 0.7943623661994934\n",
      "Epoch 429, Loss: 2.817841589450836, Final Batch Loss: 0.7651808261871338\n",
      "Epoch 430, Loss: 2.713078737258911, Final Batch Loss: 0.588119387626648\n",
      "Epoch 431, Loss: 2.747755765914917, Final Batch Loss: 0.6473178267478943\n",
      "Epoch 432, Loss: 2.6607536673545837, Final Batch Loss: 0.5633047819137573\n",
      "Epoch 433, Loss: 2.670222759246826, Final Batch Loss: 0.6933948397636414\n",
      "Epoch 434, Loss: 2.673758029937744, Final Batch Loss: 0.7058706879615784\n",
      "Epoch 435, Loss: 2.8253474831581116, Final Batch Loss: 0.6790060997009277\n",
      "Epoch 436, Loss: 2.7028788328170776, Final Batch Loss: 0.6390629410743713\n",
      "Epoch 437, Loss: 2.6970234513282776, Final Batch Loss: 0.6875857710838318\n",
      "Epoch 438, Loss: 2.832188129425049, Final Batch Loss: 0.7473954558372498\n",
      "Epoch 439, Loss: 2.6647430062294006, Final Batch Loss: 0.5806326866149902\n",
      "Epoch 440, Loss: 2.8890920877456665, Final Batch Loss: 0.7878414988517761\n",
      "Epoch 441, Loss: 2.8432191014289856, Final Batch Loss: 0.7415547370910645\n",
      "Epoch 442, Loss: 2.6993337869644165, Final Batch Loss: 0.6349282264709473\n",
      "Epoch 443, Loss: 2.6828304529190063, Final Batch Loss: 0.6607967019081116\n",
      "Epoch 444, Loss: 2.689332902431488, Final Batch Loss: 0.6197128891944885\n",
      "Epoch 445, Loss: 2.75872403383255, Final Batch Loss: 0.7178230881690979\n",
      "Epoch 446, Loss: 2.747143268585205, Final Batch Loss: 0.7528702616691589\n",
      "Epoch 447, Loss: 2.6596311926841736, Final Batch Loss: 0.6468298435211182\n",
      "Epoch 448, Loss: 2.8226991295814514, Final Batch Loss: 0.7794079780578613\n",
      "Epoch 449, Loss: 2.759720265865326, Final Batch Loss: 0.6781172156333923\n",
      "Epoch 450, Loss: 2.5988659858703613, Final Batch Loss: 0.5813650488853455\n",
      "Epoch 451, Loss: 2.8033921122550964, Final Batch Loss: 0.7675086259841919\n",
      "Epoch 452, Loss: 2.6592981219291687, Final Batch Loss: 0.5992095470428467\n",
      "Epoch 453, Loss: 2.661628544330597, Final Batch Loss: 0.7106013894081116\n",
      "Epoch 454, Loss: 2.6403934359550476, Final Batch Loss: 0.6760247945785522\n",
      "Epoch 455, Loss: 2.660019874572754, Final Batch Loss: 0.727629542350769\n",
      "Epoch 456, Loss: 2.7024462819099426, Final Batch Loss: 0.7091153860092163\n",
      "Epoch 457, Loss: 2.590303599834442, Final Batch Loss: 0.6348233222961426\n",
      "Epoch 458, Loss: 2.610326111316681, Final Batch Loss: 0.646410346031189\n",
      "Epoch 459, Loss: 2.7464107871055603, Final Batch Loss: 0.7324724197387695\n",
      "Epoch 460, Loss: 2.78611820936203, Final Batch Loss: 0.7862458229064941\n",
      "Epoch 461, Loss: 2.6682084798812866, Final Batch Loss: 0.6783289313316345\n",
      "Epoch 462, Loss: 2.7564940452575684, Final Batch Loss: 0.8042799830436707\n",
      "Epoch 463, Loss: 2.7582111954689026, Final Batch Loss: 0.7364406585693359\n",
      "Epoch 464, Loss: 2.6892770528793335, Final Batch Loss: 0.7648323178291321\n",
      "Epoch 465, Loss: 2.4234719574451447, Final Batch Loss: 0.4923243820667267\n",
      "Epoch 466, Loss: 2.6352978348731995, Final Batch Loss: 0.6238961815834045\n",
      "Epoch 467, Loss: 2.5457054376602173, Final Batch Loss: 0.5614003539085388\n",
      "Epoch 468, Loss: 2.786432147026062, Final Batch Loss: 0.7177053689956665\n",
      "Epoch 469, Loss: 2.5930432081222534, Final Batch Loss: 0.6577509641647339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470, Loss: 2.7283325791358948, Final Batch Loss: 0.7144932150840759\n",
      "Epoch 471, Loss: 2.695101797580719, Final Batch Loss: 0.7419723868370056\n",
      "Epoch 472, Loss: 2.528804838657379, Final Batch Loss: 0.5191831588745117\n",
      "Epoch 473, Loss: 2.5064348578453064, Final Batch Loss: 0.6198750138282776\n",
      "Epoch 474, Loss: 2.578514277935028, Final Batch Loss: 0.6518962979316711\n",
      "Epoch 475, Loss: 2.6700241565704346, Final Batch Loss: 0.6974052786827087\n",
      "Epoch 476, Loss: 2.677920639514923, Final Batch Loss: 0.6978408694267273\n",
      "Epoch 477, Loss: 2.6498395204544067, Final Batch Loss: 0.6538849472999573\n",
      "Epoch 478, Loss: 2.5224552750587463, Final Batch Loss: 0.574826717376709\n",
      "Epoch 479, Loss: 2.639980673789978, Final Batch Loss: 0.6509215235710144\n",
      "Epoch 480, Loss: 2.6358866095542908, Final Batch Loss: 0.6539267897605896\n",
      "Epoch 481, Loss: 2.6178298592567444, Final Batch Loss: 0.5634018778800964\n",
      "Epoch 482, Loss: 2.61919629573822, Final Batch Loss: 0.6101124882698059\n",
      "Epoch 483, Loss: 2.485285997390747, Final Batch Loss: 0.5559175610542297\n",
      "Epoch 484, Loss: 2.6294533610343933, Final Batch Loss: 0.6537967324256897\n",
      "Epoch 485, Loss: 2.6207353472709656, Final Batch Loss: 0.5993554592132568\n",
      "Epoch 486, Loss: 2.6245461106300354, Final Batch Loss: 0.6943150162696838\n",
      "Epoch 487, Loss: 2.5681552290916443, Final Batch Loss: 0.5517652630805969\n",
      "Epoch 488, Loss: 2.6685553789138794, Final Batch Loss: 0.7237619161605835\n",
      "Epoch 489, Loss: 2.5794113278388977, Final Batch Loss: 0.6760908365249634\n",
      "Epoch 490, Loss: 2.7520220279693604, Final Batch Loss: 0.8176647424697876\n",
      "Epoch 491, Loss: 2.548201262950897, Final Batch Loss: 0.6696280241012573\n",
      "Epoch 492, Loss: 2.7072778940200806, Final Batch Loss: 0.8043474555015564\n",
      "Epoch 493, Loss: 2.521920919418335, Final Batch Loss: 0.7068946361541748\n",
      "Epoch 494, Loss: 2.6089303493499756, Final Batch Loss: 0.5584515333175659\n",
      "Epoch 495, Loss: 2.5948039889335632, Final Batch Loss: 0.6204111576080322\n",
      "Epoch 496, Loss: 2.517833948135376, Final Batch Loss: 0.632861852645874\n",
      "Epoch 497, Loss: 2.6425353288650513, Final Batch Loss: 0.6426745057106018\n",
      "Epoch 498, Loss: 2.527831733226776, Final Batch Loss: 0.7845454216003418\n",
      "Epoch 499, Loss: 2.6696594953536987, Final Batch Loss: 0.7173193097114563\n",
      "Epoch 500, Loss: 2.6305856704711914, Final Batch Loss: 0.7601507902145386\n",
      "Epoch 501, Loss: 2.647039532661438, Final Batch Loss: 0.6764662265777588\n",
      "Epoch 502, Loss: 2.5457913279533386, Final Batch Loss: 0.5376989841461182\n",
      "Epoch 503, Loss: 2.72194504737854, Final Batch Loss: 0.7017759084701538\n",
      "Epoch 504, Loss: 2.551904857158661, Final Batch Loss: 0.6793133020401001\n",
      "Epoch 505, Loss: 2.5256048440933228, Final Batch Loss: 0.5725969076156616\n",
      "Epoch 506, Loss: 2.5871517062187195, Final Batch Loss: 0.7144135236740112\n",
      "Epoch 507, Loss: 2.4928303360939026, Final Batch Loss: 0.6644107103347778\n",
      "Epoch 508, Loss: 2.526215374469757, Final Batch Loss: 0.6577582955360413\n",
      "Epoch 509, Loss: 2.544152796268463, Final Batch Loss: 0.6546648740768433\n",
      "Epoch 510, Loss: 2.5441761016845703, Final Batch Loss: 0.7163076400756836\n",
      "Epoch 511, Loss: 2.6143494844436646, Final Batch Loss: 0.6286102533340454\n",
      "Epoch 512, Loss: 2.5444380044937134, Final Batch Loss: 0.6779972314834595\n",
      "Epoch 513, Loss: 2.5610655546188354, Final Batch Loss: 0.6238948106765747\n",
      "Epoch 514, Loss: 2.5934616327285767, Final Batch Loss: 0.6471649408340454\n",
      "Epoch 515, Loss: 2.5644238591194153, Final Batch Loss: 0.6931300163269043\n",
      "Epoch 516, Loss: 2.5426214933395386, Final Batch Loss: 0.6686373353004456\n",
      "Epoch 517, Loss: 2.5046542286872864, Final Batch Loss: 0.6531810164451599\n",
      "Epoch 518, Loss: 2.5771045684814453, Final Batch Loss: 0.5935697555541992\n",
      "Epoch 519, Loss: 2.741806745529175, Final Batch Loss: 0.6720916032791138\n",
      "Epoch 520, Loss: 2.553834557533264, Final Batch Loss: 0.6243515014648438\n",
      "Epoch 521, Loss: 2.586775541305542, Final Batch Loss: 0.6267584562301636\n",
      "Epoch 522, Loss: 2.6440518498420715, Final Batch Loss: 0.7502844333648682\n",
      "Epoch 523, Loss: 2.5549548268318176, Final Batch Loss: 0.6247884035110474\n",
      "Epoch 524, Loss: 2.4871426224708557, Final Batch Loss: 0.6262058019638062\n",
      "Epoch 525, Loss: 2.515743613243103, Final Batch Loss: 0.5990266799926758\n",
      "Epoch 526, Loss: 2.734299600124359, Final Batch Loss: 0.7550007104873657\n",
      "Epoch 527, Loss: 2.4463210105895996, Final Batch Loss: 0.528371274471283\n",
      "Epoch 528, Loss: 2.581711232662201, Final Batch Loss: 0.6532487273216248\n",
      "Epoch 529, Loss: 2.536133110523224, Final Batch Loss: 0.5967823266983032\n",
      "Epoch 530, Loss: 2.5978798270225525, Final Batch Loss: 0.6568644642829895\n",
      "Epoch 531, Loss: 2.482767939567566, Final Batch Loss: 0.5937689542770386\n",
      "Epoch 532, Loss: 2.521764874458313, Final Batch Loss: 0.67596834897995\n",
      "Epoch 533, Loss: 2.4664701223373413, Final Batch Loss: 0.5279671549797058\n",
      "Epoch 534, Loss: 2.5517044067382812, Final Batch Loss: 0.6421840190887451\n",
      "Epoch 535, Loss: 2.5795235633850098, Final Batch Loss: 0.5619075894355774\n",
      "Epoch 536, Loss: 2.552809774875641, Final Batch Loss: 0.5825566053390503\n",
      "Epoch 537, Loss: 2.6385133862495422, Final Batch Loss: 0.6604943871498108\n",
      "Epoch 538, Loss: 2.5377495288848877, Final Batch Loss: 0.626119077205658\n",
      "Epoch 539, Loss: 2.7658225297927856, Final Batch Loss: 0.7099603414535522\n",
      "Epoch 540, Loss: 2.5436806082725525, Final Batch Loss: 0.5692941546440125\n",
      "Epoch 541, Loss: 2.424692988395691, Final Batch Loss: 0.640478789806366\n",
      "Epoch 542, Loss: 2.6631978154182434, Final Batch Loss: 0.8132725954055786\n",
      "Epoch 543, Loss: 2.5648792386054993, Final Batch Loss: 0.7323228716850281\n",
      "Epoch 544, Loss: 2.547746956348419, Final Batch Loss: 0.7156750559806824\n",
      "Epoch 545, Loss: 2.504889130592346, Final Batch Loss: 0.6119591593742371\n",
      "Epoch 546, Loss: 2.616334855556488, Final Batch Loss: 0.7733789682388306\n",
      "Epoch 547, Loss: 2.48528653383255, Final Batch Loss: 0.5753580331802368\n",
      "Epoch 548, Loss: 2.462855815887451, Final Batch Loss: 0.5946553349494934\n",
      "Epoch 549, Loss: 2.525673747062683, Final Batch Loss: 0.654984712600708\n",
      "Epoch 550, Loss: 2.5917938947677612, Final Batch Loss: 0.6209315657615662\n",
      "Epoch 551, Loss: 2.4683915972709656, Final Batch Loss: 0.6844835877418518\n",
      "Epoch 552, Loss: 2.632887601852417, Final Batch Loss: 0.615028440952301\n",
      "Epoch 553, Loss: 2.586886942386627, Final Batch Loss: 0.7168306112289429\n",
      "Epoch 554, Loss: 2.5809980034828186, Final Batch Loss: 0.6961599588394165\n",
      "Epoch 555, Loss: 2.6336814761161804, Final Batch Loss: 0.7553920149803162\n",
      "Epoch 556, Loss: 2.569746971130371, Final Batch Loss: 0.6008500456809998\n",
      "Epoch 557, Loss: 2.5785099267959595, Final Batch Loss: 0.6813690662384033\n",
      "Epoch 558, Loss: 2.5718594193458557, Final Batch Loss: 0.5772507786750793\n",
      "Epoch 559, Loss: 2.361728459596634, Final Batch Loss: 0.4876919090747833\n",
      "Epoch 560, Loss: 2.525141954421997, Final Batch Loss: 0.6757340431213379\n",
      "Epoch 561, Loss: 2.589582681655884, Final Batch Loss: 0.6361491680145264\n",
      "Epoch 562, Loss: 2.6437450647354126, Final Batch Loss: 0.7147090435028076\n",
      "Epoch 563, Loss: 2.541940927505493, Final Batch Loss: 0.6510255336761475\n",
      "Epoch 564, Loss: 2.5268872380256653, Final Batch Loss: 0.7039588689804077\n",
      "Epoch 565, Loss: 2.5781049728393555, Final Batch Loss: 0.7558698058128357\n",
      "Epoch 566, Loss: 2.5173404216766357, Final Batch Loss: 0.5826565027236938\n",
      "Epoch 567, Loss: 2.5110042691230774, Final Batch Loss: 0.5540655851364136\n",
      "Epoch 568, Loss: 2.62642365694046, Final Batch Loss: 0.7909014225006104\n",
      "Epoch 569, Loss: 2.453552722930908, Final Batch Loss: 0.5971449017524719\n",
      "Epoch 570, Loss: 2.575758159160614, Final Batch Loss: 0.7028763890266418\n",
      "Epoch 571, Loss: 2.529930293560028, Final Batch Loss: 0.5660504698753357\n",
      "Epoch 572, Loss: 2.4388046264648438, Final Batch Loss: 0.6091569066047668\n",
      "Epoch 573, Loss: 2.38759982585907, Final Batch Loss: 0.5796334743499756\n",
      "Epoch 574, Loss: 2.4651151299476624, Final Batch Loss: 0.6075502038002014\n",
      "Epoch 575, Loss: 2.499114215373993, Final Batch Loss: 0.636124849319458\n",
      "Epoch 576, Loss: 2.5479440689086914, Final Batch Loss: 0.6525409817695618\n",
      "Epoch 577, Loss: 2.4541637897491455, Final Batch Loss: 0.6462501287460327\n",
      "Epoch 578, Loss: 2.561842441558838, Final Batch Loss: 0.6832451224327087\n",
      "Epoch 579, Loss: 2.5239714980125427, Final Batch Loss: 0.6267609000205994\n",
      "Epoch 580, Loss: 2.3556262850761414, Final Batch Loss: 0.5437821745872498\n",
      "Epoch 581, Loss: 2.408181369304657, Final Batch Loss: 0.6267955303192139\n",
      "Epoch 582, Loss: 2.430468499660492, Final Batch Loss: 0.5882508158683777\n",
      "Epoch 583, Loss: 2.4833987951278687, Final Batch Loss: 0.6226227879524231\n",
      "Epoch 584, Loss: 2.3725468516349792, Final Batch Loss: 0.5203545093536377\n",
      "Epoch 585, Loss: 2.504982829093933, Final Batch Loss: 0.603057324886322\n",
      "Epoch 586, Loss: 2.3694400787353516, Final Batch Loss: 0.5237031579017639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 587, Loss: 2.5015315413475037, Final Batch Loss: 0.5998781323432922\n",
      "Epoch 588, Loss: 2.542340576648712, Final Batch Loss: 0.6092004179954529\n",
      "Epoch 589, Loss: 2.4734283685684204, Final Batch Loss: 0.5349409580230713\n",
      "Epoch 590, Loss: 2.468689739704132, Final Batch Loss: 0.6221821904182434\n",
      "Epoch 591, Loss: 2.505377233028412, Final Batch Loss: 0.5892009735107422\n",
      "Epoch 592, Loss: 2.414467453956604, Final Batch Loss: 0.5402904152870178\n",
      "Epoch 593, Loss: 2.369523584842682, Final Batch Loss: 0.6035006642341614\n",
      "Epoch 594, Loss: 2.535512149333954, Final Batch Loss: 0.6570724844932556\n",
      "Epoch 595, Loss: 2.4323174357414246, Final Batch Loss: 0.6105274558067322\n",
      "Epoch 596, Loss: 2.4723753929138184, Final Batch Loss: 0.5922595262527466\n",
      "Epoch 597, Loss: 2.492409586906433, Final Batch Loss: 0.6257339119911194\n",
      "Epoch 598, Loss: 2.415751039981842, Final Batch Loss: 0.7040355205535889\n",
      "Epoch 599, Loss: 2.388087570667267, Final Batch Loss: 0.5970650911331177\n",
      "Epoch 600, Loss: 2.4194632172584534, Final Batch Loss: 0.6051000356674194\n",
      "Epoch 601, Loss: 2.5011390447616577, Final Batch Loss: 0.6980631947517395\n",
      "Epoch 602, Loss: 2.4352110624313354, Final Batch Loss: 0.5393933653831482\n",
      "Epoch 603, Loss: 2.508740186691284, Final Batch Loss: 0.6865146160125732\n",
      "Epoch 604, Loss: 2.4361231923103333, Final Batch Loss: 0.6868462562561035\n",
      "Epoch 605, Loss: 2.3960790634155273, Final Batch Loss: 0.600889265537262\n",
      "Epoch 606, Loss: 2.449553906917572, Final Batch Loss: 0.5091622471809387\n",
      "Epoch 607, Loss: 2.4407200813293457, Final Batch Loss: 0.6421970129013062\n",
      "Epoch 608, Loss: 2.512652575969696, Final Batch Loss: 0.5812946557998657\n",
      "Epoch 609, Loss: 2.4439534544944763, Final Batch Loss: 0.6226011514663696\n",
      "Epoch 610, Loss: 2.319081664085388, Final Batch Loss: 0.6005256772041321\n",
      "Epoch 611, Loss: 2.3552387952804565, Final Batch Loss: 0.4928451180458069\n",
      "Epoch 612, Loss: 2.4525283575057983, Final Batch Loss: 0.6192103624343872\n",
      "Epoch 613, Loss: 2.3920350670814514, Final Batch Loss: 0.6699866056442261\n",
      "Epoch 614, Loss: 2.2772486805915833, Final Batch Loss: 0.5465060472488403\n",
      "Epoch 615, Loss: 2.4037004113197327, Final Batch Loss: 0.6116992831230164\n",
      "Epoch 616, Loss: 2.4873868823051453, Final Batch Loss: 0.6425473093986511\n",
      "Epoch 617, Loss: 2.4415473341941833, Final Batch Loss: 0.6169397830963135\n",
      "Epoch 618, Loss: 2.4234630465507507, Final Batch Loss: 0.55389803647995\n",
      "Epoch 619, Loss: 2.4299967288970947, Final Batch Loss: 0.6381727457046509\n",
      "Epoch 620, Loss: 2.478216052055359, Final Batch Loss: 0.6368958353996277\n",
      "Epoch 621, Loss: 2.5742815136909485, Final Batch Loss: 0.6687020659446716\n",
      "Epoch 622, Loss: 2.395322948694229, Final Batch Loss: 0.49817463755607605\n",
      "Epoch 623, Loss: 2.4246466159820557, Final Batch Loss: 0.6042106747627258\n",
      "Epoch 624, Loss: 2.3769524097442627, Final Batch Loss: 0.5738918781280518\n",
      "Epoch 625, Loss: 2.379071831703186, Final Batch Loss: 0.632978618144989\n",
      "Epoch 626, Loss: 2.4457234144210815, Final Batch Loss: 0.6225893497467041\n",
      "Epoch 627, Loss: 2.3521057963371277, Final Batch Loss: 0.6405568718910217\n",
      "Epoch 628, Loss: 2.3664037585258484, Final Batch Loss: 0.5525656342506409\n",
      "Epoch 629, Loss: 2.438658654689789, Final Batch Loss: 0.7114185690879822\n",
      "Epoch 630, Loss: 2.535273551940918, Final Batch Loss: 0.7315333485603333\n",
      "Epoch 631, Loss: 2.384502351284027, Final Batch Loss: 0.5529263615608215\n",
      "Epoch 632, Loss: 2.437550663948059, Final Batch Loss: 0.6571229100227356\n",
      "Epoch 633, Loss: 2.379854917526245, Final Batch Loss: 0.5818992853164673\n",
      "Epoch 634, Loss: 2.4027256965637207, Final Batch Loss: 0.5305904746055603\n",
      "Epoch 635, Loss: 2.3453946113586426, Final Batch Loss: 0.5680376291275024\n",
      "Epoch 636, Loss: 2.3337735533714294, Final Batch Loss: 0.5298763513565063\n",
      "Epoch 637, Loss: 2.4921700954437256, Final Batch Loss: 0.6808343529701233\n",
      "Epoch 638, Loss: 2.4445149898529053, Final Batch Loss: 0.6180956959724426\n",
      "Epoch 639, Loss: 2.377505123615265, Final Batch Loss: 0.5907863974571228\n",
      "Epoch 640, Loss: 2.3653740882873535, Final Batch Loss: 0.5961242318153381\n",
      "Epoch 641, Loss: 2.3673176169395447, Final Batch Loss: 0.561726987361908\n",
      "Epoch 642, Loss: 2.451148569583893, Final Batch Loss: 0.5954131484031677\n",
      "Epoch 643, Loss: 2.404440939426422, Final Batch Loss: 0.5668497085571289\n",
      "Epoch 644, Loss: 2.4449689984321594, Final Batch Loss: 0.6321151852607727\n",
      "Epoch 645, Loss: 2.4127681851387024, Final Batch Loss: 0.6476535797119141\n",
      "Epoch 646, Loss: 2.2843873500823975, Final Batch Loss: 0.5911555290222168\n",
      "Epoch 647, Loss: 2.4377747774124146, Final Batch Loss: 0.7167580723762512\n",
      "Epoch 648, Loss: 2.395306408405304, Final Batch Loss: 0.5587211847305298\n",
      "Epoch 649, Loss: 2.4705599546432495, Final Batch Loss: 0.6332607865333557\n",
      "Epoch 650, Loss: 2.394886076450348, Final Batch Loss: 0.6226150989532471\n",
      "Epoch 651, Loss: 2.3764816522598267, Final Batch Loss: 0.5258686542510986\n",
      "Epoch 652, Loss: 2.4174404740333557, Final Batch Loss: 0.5494809746742249\n",
      "Epoch 653, Loss: 2.395133852958679, Final Batch Loss: 0.64252769947052\n",
      "Epoch 654, Loss: 2.480575442314148, Final Batch Loss: 0.5952943563461304\n",
      "Epoch 655, Loss: 2.3042203187942505, Final Batch Loss: 0.521652102470398\n",
      "Epoch 656, Loss: 2.384855091571808, Final Batch Loss: 0.579123318195343\n",
      "Epoch 657, Loss: 2.204664707183838, Final Batch Loss: 0.4951586127281189\n",
      "Epoch 658, Loss: 2.276547610759735, Final Batch Loss: 0.5210243463516235\n",
      "Epoch 659, Loss: 2.380313754081726, Final Batch Loss: 0.6208626627922058\n",
      "Epoch 660, Loss: 2.336078703403473, Final Batch Loss: 0.6072115302085876\n",
      "Epoch 661, Loss: 2.405112385749817, Final Batch Loss: 0.6215944290161133\n",
      "Epoch 662, Loss: 2.2132234275341034, Final Batch Loss: 0.47544726729393005\n",
      "Epoch 663, Loss: 2.351740539073944, Final Batch Loss: 0.6406396627426147\n",
      "Epoch 664, Loss: 2.395923614501953, Final Batch Loss: 0.6478242874145508\n",
      "Epoch 665, Loss: 2.2254170179367065, Final Batch Loss: 0.5236853957176208\n",
      "Epoch 666, Loss: 2.5624422430992126, Final Batch Loss: 0.6303306221961975\n",
      "Epoch 667, Loss: 2.375100255012512, Final Batch Loss: 0.5920765995979309\n",
      "Epoch 668, Loss: 2.3894914984703064, Final Batch Loss: 0.6942485570907593\n",
      "Epoch 669, Loss: 2.3628960251808167, Final Batch Loss: 0.6586077213287354\n",
      "Epoch 670, Loss: 2.326842784881592, Final Batch Loss: 0.654381513595581\n",
      "Epoch 671, Loss: 2.301430881023407, Final Batch Loss: 0.545684278011322\n",
      "Epoch 672, Loss: 2.310954451560974, Final Batch Loss: 0.5493223071098328\n",
      "Epoch 673, Loss: 2.347356140613556, Final Batch Loss: 0.6211332082748413\n",
      "Epoch 674, Loss: 2.3607033491134644, Final Batch Loss: 0.6784799695014954\n",
      "Epoch 675, Loss: 2.4058175086975098, Final Batch Loss: 0.5971070528030396\n",
      "Epoch 676, Loss: 2.384040951728821, Final Batch Loss: 0.6775805354118347\n",
      "Epoch 677, Loss: 2.1834107041358948, Final Batch Loss: 0.4670560956001282\n",
      "Epoch 678, Loss: 2.2975428700447083, Final Batch Loss: 0.5628501772880554\n",
      "Epoch 679, Loss: 2.26291024684906, Final Batch Loss: 0.4914705753326416\n",
      "Epoch 680, Loss: 2.3828057050704956, Final Batch Loss: 0.6072589159011841\n",
      "Epoch 681, Loss: 2.475755989551544, Final Batch Loss: 0.7562574744224548\n",
      "Epoch 682, Loss: 2.428792953491211, Final Batch Loss: 0.5288608074188232\n",
      "Epoch 683, Loss: 2.3718482851982117, Final Batch Loss: 0.5765069723129272\n",
      "Epoch 684, Loss: 2.3735597729682922, Final Batch Loss: 0.5737773776054382\n",
      "Epoch 685, Loss: 2.3348348736763, Final Batch Loss: 0.6041640639305115\n",
      "Epoch 686, Loss: 2.3265095353126526, Final Batch Loss: 0.528893232345581\n",
      "Epoch 687, Loss: 2.2544147968292236, Final Batch Loss: 0.523496150970459\n",
      "Epoch 688, Loss: 2.4366226196289062, Final Batch Loss: 0.5794200897216797\n",
      "Epoch 689, Loss: 2.2688782811164856, Final Batch Loss: 0.5826492309570312\n",
      "Epoch 690, Loss: 2.371130883693695, Final Batch Loss: 0.616980791091919\n",
      "Epoch 691, Loss: 2.212546855211258, Final Batch Loss: 0.5818920731544495\n",
      "Epoch 692, Loss: 2.2724467515945435, Final Batch Loss: 0.5550172924995422\n",
      "Epoch 693, Loss: 2.3291077613830566, Final Batch Loss: 0.636179506778717\n",
      "Epoch 694, Loss: 2.243652045726776, Final Batch Loss: 0.663489043712616\n",
      "Epoch 695, Loss: 2.3019105195999146, Final Batch Loss: 0.560867965221405\n",
      "Epoch 696, Loss: 2.251336634159088, Final Batch Loss: 0.6230059862136841\n",
      "Epoch 697, Loss: 2.256301164627075, Final Batch Loss: 0.5820426344871521\n",
      "Epoch 698, Loss: 2.342339873313904, Final Batch Loss: 0.5215866565704346\n",
      "Epoch 699, Loss: 2.3826125264167786, Final Batch Loss: 0.6700929999351501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 700, Loss: 2.2966501712799072, Final Batch Loss: 0.5722119808197021\n",
      "Epoch 701, Loss: 2.319939434528351, Final Batch Loss: 0.7021238803863525\n",
      "Epoch 702, Loss: 2.1969940662384033, Final Batch Loss: 0.5172163248062134\n",
      "Epoch 703, Loss: 2.4351046681404114, Final Batch Loss: 0.6176559925079346\n",
      "Epoch 704, Loss: 2.4959433674812317, Final Batch Loss: 0.6939377188682556\n",
      "Epoch 705, Loss: 2.352541923522949, Final Batch Loss: 0.5682262182235718\n",
      "Epoch 706, Loss: 2.401185154914856, Final Batch Loss: 0.5945806503295898\n",
      "Epoch 707, Loss: 2.3479339480400085, Final Batch Loss: 0.561692476272583\n",
      "Epoch 708, Loss: 2.2996217012405396, Final Batch Loss: 0.6148223280906677\n",
      "Epoch 709, Loss: 2.2451725602149963, Final Batch Loss: 0.5409141778945923\n",
      "Epoch 710, Loss: 2.30658096075058, Final Batch Loss: 0.590529203414917\n",
      "Epoch 711, Loss: 2.275185525417328, Final Batch Loss: 0.5333618521690369\n",
      "Epoch 712, Loss: 2.2902507185935974, Final Batch Loss: 0.6155623197555542\n",
      "Epoch 713, Loss: 2.224667191505432, Final Batch Loss: 0.5018044114112854\n",
      "Epoch 714, Loss: 2.1298747658729553, Final Batch Loss: 0.4228074550628662\n",
      "Epoch 715, Loss: 2.3202242851257324, Final Batch Loss: 0.577578067779541\n",
      "Epoch 716, Loss: 2.2420929670333862, Final Batch Loss: 0.5987023115158081\n",
      "Epoch 717, Loss: 2.4052132666110992, Final Batch Loss: 0.6967290639877319\n",
      "Epoch 718, Loss: 2.3055832982063293, Final Batch Loss: 0.5588226318359375\n",
      "Epoch 719, Loss: 2.2452472448349, Final Batch Loss: 0.5785759091377258\n",
      "Epoch 720, Loss: 2.212213397026062, Final Batch Loss: 0.514834463596344\n",
      "Epoch 721, Loss: 2.3052423000335693, Final Batch Loss: 0.5301634669303894\n",
      "Epoch 722, Loss: 2.143722325563431, Final Batch Loss: 0.47428038716316223\n",
      "Epoch 723, Loss: 2.153138518333435, Final Batch Loss: 0.5283877849578857\n",
      "Epoch 724, Loss: 2.3083029985427856, Final Batch Loss: 0.6156579256057739\n",
      "Epoch 725, Loss: 2.3761987686157227, Final Batch Loss: 0.6575737595558167\n",
      "Epoch 726, Loss: 2.294704496860504, Final Batch Loss: 0.5945875644683838\n",
      "Epoch 727, Loss: 2.1794081926345825, Final Batch Loss: 0.5278164148330688\n",
      "Epoch 728, Loss: 2.1570538580417633, Final Batch Loss: 0.4561270773410797\n",
      "Epoch 729, Loss: 2.338986814022064, Final Batch Loss: 0.5659592747688293\n",
      "Epoch 730, Loss: 2.2608465254306793, Final Batch Loss: 0.4928983151912689\n",
      "Epoch 731, Loss: 2.242923855781555, Final Batch Loss: 0.5810352563858032\n",
      "Epoch 732, Loss: 2.246792882680893, Final Batch Loss: 0.4684339463710785\n",
      "Epoch 733, Loss: 2.2509787678718567, Final Batch Loss: 0.5883638858795166\n",
      "Epoch 734, Loss: 2.2020175755023956, Final Batch Loss: 0.5244778394699097\n",
      "Epoch 735, Loss: 2.168312132358551, Final Batch Loss: 0.5045737028121948\n",
      "Epoch 736, Loss: 2.178782194852829, Final Batch Loss: 0.4623424708843231\n",
      "Epoch 737, Loss: 2.3353092670440674, Final Batch Loss: 0.5699113607406616\n",
      "Epoch 738, Loss: 2.3156601190567017, Final Batch Loss: 0.5962639451026917\n",
      "Epoch 739, Loss: 2.206337869167328, Final Batch Loss: 0.5646072626113892\n",
      "Epoch 740, Loss: 2.322354793548584, Final Batch Loss: 0.65254807472229\n",
      "Epoch 741, Loss: 2.358485758304596, Final Batch Loss: 0.6777507066726685\n",
      "Epoch 742, Loss: 2.2338832020759583, Final Batch Loss: 0.5687239766120911\n",
      "Epoch 743, Loss: 2.3132596015930176, Final Batch Loss: 0.5258889198303223\n",
      "Epoch 744, Loss: 2.2843043208122253, Final Batch Loss: 0.5662484765052795\n",
      "Epoch 745, Loss: 2.2887677550315857, Final Batch Loss: 0.6151250600814819\n",
      "Epoch 746, Loss: 2.2438632249832153, Final Batch Loss: 0.6062193512916565\n",
      "Epoch 747, Loss: 2.159759670495987, Final Batch Loss: 0.5553296208381653\n",
      "Epoch 748, Loss: 2.3622519969940186, Final Batch Loss: 0.6755846738815308\n",
      "Epoch 749, Loss: 2.1928492188453674, Final Batch Loss: 0.5031104683876038\n",
      "Epoch 750, Loss: 2.274808645248413, Final Batch Loss: 0.5677176117897034\n",
      "Epoch 751, Loss: 2.3139452934265137, Final Batch Loss: 0.6834002137184143\n",
      "Epoch 752, Loss: 2.2776796519756317, Final Batch Loss: 0.6388571858406067\n",
      "Epoch 753, Loss: 2.259942054748535, Final Batch Loss: 0.5827755331993103\n",
      "Epoch 754, Loss: 2.2194623947143555, Final Batch Loss: 0.5805599093437195\n",
      "Epoch 755, Loss: 2.2455220222473145, Final Batch Loss: 0.567534327507019\n",
      "Epoch 756, Loss: 2.252717614173889, Final Batch Loss: 0.6003252863883972\n",
      "Epoch 757, Loss: 2.163057893514633, Final Batch Loss: 0.5690112709999084\n",
      "Epoch 758, Loss: 2.2098716497421265, Final Batch Loss: 0.5252208709716797\n",
      "Epoch 759, Loss: 2.225464701652527, Final Batch Loss: 0.5832129716873169\n",
      "Epoch 760, Loss: 2.3093243539333344, Final Batch Loss: 0.6979594826698303\n",
      "Epoch 761, Loss: 2.1699527502059937, Final Batch Loss: 0.5254371166229248\n",
      "Epoch 762, Loss: 2.3152312636375427, Final Batch Loss: 0.6132631301879883\n",
      "Epoch 763, Loss: 2.246111035346985, Final Batch Loss: 0.5692052841186523\n",
      "Epoch 764, Loss: 2.247713625431061, Final Batch Loss: 0.5198461413383484\n",
      "Epoch 765, Loss: 2.18828222155571, Final Batch Loss: 0.4991886019706726\n",
      "Epoch 766, Loss: 2.3587692975997925, Final Batch Loss: 0.7026599049568176\n",
      "Epoch 767, Loss: 2.3125720024108887, Final Batch Loss: 0.6194136142730713\n",
      "Epoch 768, Loss: 2.0692920088768005, Final Batch Loss: 0.5176286697387695\n",
      "Epoch 769, Loss: 2.313796639442444, Final Batch Loss: 0.6334840655326843\n",
      "Epoch 770, Loss: 2.3556950092315674, Final Batch Loss: 0.6349446177482605\n",
      "Epoch 771, Loss: 2.1966755986213684, Final Batch Loss: 0.5199211239814758\n",
      "Epoch 772, Loss: 2.2380650639533997, Final Batch Loss: 0.5445997714996338\n",
      "Epoch 773, Loss: 2.1800724864006042, Final Batch Loss: 0.5395399332046509\n",
      "Epoch 774, Loss: 2.323352634906769, Final Batch Loss: 0.628841757774353\n",
      "Epoch 775, Loss: 2.221387505531311, Final Batch Loss: 0.5924015641212463\n",
      "Epoch 776, Loss: 2.186231404542923, Final Batch Loss: 0.5471406579017639\n",
      "Epoch 777, Loss: 2.2693145275115967, Final Batch Loss: 0.5791569948196411\n",
      "Epoch 778, Loss: 2.296545147895813, Final Batch Loss: 0.6279980540275574\n",
      "Epoch 779, Loss: 2.297327995300293, Final Batch Loss: 0.5607573986053467\n",
      "Epoch 780, Loss: 2.121538519859314, Final Batch Loss: 0.46311289072036743\n",
      "Epoch 781, Loss: 2.2488712668418884, Final Batch Loss: 0.5165047645568848\n",
      "Epoch 782, Loss: 2.2169647812843323, Final Batch Loss: 0.5512526035308838\n",
      "Epoch 783, Loss: 2.246365010738373, Final Batch Loss: 0.5171166062355042\n",
      "Epoch 784, Loss: 2.172395706176758, Final Batch Loss: 0.511490523815155\n",
      "Epoch 785, Loss: 2.2595751583576202, Final Batch Loss: 0.597425103187561\n",
      "Epoch 786, Loss: 2.211156904697418, Final Batch Loss: 0.5037590861320496\n",
      "Epoch 787, Loss: 2.3065452575683594, Final Batch Loss: 0.6444539427757263\n",
      "Epoch 788, Loss: 2.173274517059326, Final Batch Loss: 0.5822085738182068\n",
      "Epoch 789, Loss: 2.1826940774917603, Final Batch Loss: 0.6413941979408264\n",
      "Epoch 790, Loss: 2.309808611869812, Final Batch Loss: 0.5192364454269409\n",
      "Epoch 791, Loss: 2.1787591576576233, Final Batch Loss: 0.6010357737541199\n",
      "Epoch 792, Loss: 2.199308156967163, Final Batch Loss: 0.5014626383781433\n",
      "Epoch 793, Loss: 2.235857367515564, Final Batch Loss: 0.5891949534416199\n",
      "Epoch 794, Loss: 2.2694305777549744, Final Batch Loss: 0.5010142922401428\n",
      "Epoch 795, Loss: 2.131228983402252, Final Batch Loss: 0.5094615817070007\n",
      "Epoch 796, Loss: 2.261984705924988, Final Batch Loss: 0.6085580587387085\n",
      "Epoch 797, Loss: 2.096504718065262, Final Batch Loss: 0.4157037138938904\n",
      "Epoch 798, Loss: 2.1947659850120544, Final Batch Loss: 0.5184288620948792\n",
      "Epoch 799, Loss: 2.1444337368011475, Final Batch Loss: 0.506188690662384\n",
      "Epoch 800, Loss: 2.2173687517642975, Final Batch Loss: 0.47396668791770935\n",
      "Epoch 801, Loss: 2.1494743525981903, Final Batch Loss: 0.5476635694503784\n",
      "Epoch 802, Loss: 2.2059863209724426, Final Batch Loss: 0.5858471989631653\n",
      "Epoch 803, Loss: 2.13190758228302, Final Batch Loss: 0.5790373086929321\n",
      "Epoch 804, Loss: 2.19618684053421, Final Batch Loss: 0.6210569143295288\n",
      "Epoch 805, Loss: 2.2314276695251465, Final Batch Loss: 0.586287796497345\n",
      "Epoch 806, Loss: 2.103550463914871, Final Batch Loss: 0.5152227878570557\n",
      "Epoch 807, Loss: 2.1794974207878113, Final Batch Loss: 0.5867484211921692\n",
      "Epoch 808, Loss: 2.1344668567180634, Final Batch Loss: 0.4271424114704132\n",
      "Epoch 809, Loss: 2.070718616247177, Final Batch Loss: 0.47166427969932556\n",
      "Epoch 810, Loss: 2.172324627637863, Final Batch Loss: 0.6060431003570557\n",
      "Epoch 811, Loss: 2.2338674068450928, Final Batch Loss: 0.5778958797454834\n",
      "Epoch 812, Loss: 2.0981449484825134, Final Batch Loss: 0.6017069816589355\n",
      "Epoch 813, Loss: 2.23069304227829, Final Batch Loss: 0.4703332781791687\n",
      "Epoch 814, Loss: 2.2010742127895355, Final Batch Loss: 0.4976836144924164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 815, Loss: 2.2068545818328857, Final Batch Loss: 0.6203441023826599\n",
      "Epoch 816, Loss: 2.1998519897460938, Final Batch Loss: 0.557340681552887\n",
      "Epoch 817, Loss: 2.1847690641880035, Final Batch Loss: 0.5822880864143372\n",
      "Epoch 818, Loss: 2.237479090690613, Final Batch Loss: 0.6115595698356628\n",
      "Epoch 819, Loss: 2.2129440009593964, Final Batch Loss: 0.5991101264953613\n",
      "Epoch 820, Loss: 2.164480894804001, Final Batch Loss: 0.482069194316864\n",
      "Epoch 821, Loss: 2.1481456756591797, Final Batch Loss: 0.5370047688484192\n",
      "Epoch 822, Loss: 2.3593074083328247, Final Batch Loss: 0.7597153782844543\n",
      "Epoch 823, Loss: 2.279321253299713, Final Batch Loss: 0.5448057055473328\n",
      "Epoch 824, Loss: 2.168639302253723, Final Batch Loss: 0.5301684737205505\n",
      "Epoch 825, Loss: 2.2043575644493103, Final Batch Loss: 0.4875061511993408\n",
      "Epoch 826, Loss: 2.1128053665161133, Final Batch Loss: 0.5290595293045044\n",
      "Epoch 827, Loss: 2.276174545288086, Final Batch Loss: 0.6352924704551697\n",
      "Epoch 828, Loss: 2.1002750992774963, Final Batch Loss: 0.570469856262207\n",
      "Epoch 829, Loss: 2.201510012149811, Final Batch Loss: 0.5856522917747498\n",
      "Epoch 830, Loss: 2.1135804653167725, Final Batch Loss: 0.4549594819545746\n",
      "Epoch 831, Loss: 2.345238447189331, Final Batch Loss: 0.6805506348609924\n",
      "Epoch 832, Loss: 2.1993943452835083, Final Batch Loss: 0.5622556805610657\n",
      "Epoch 833, Loss: 2.2392618060112, Final Batch Loss: 0.5717996954917908\n",
      "Epoch 834, Loss: 2.1702519357204437, Final Batch Loss: 0.44493117928504944\n",
      "Epoch 835, Loss: 2.2095183730125427, Final Batch Loss: 0.5780611038208008\n",
      "Epoch 836, Loss: 2.151952385902405, Final Batch Loss: 0.5482407212257385\n",
      "Epoch 837, Loss: 2.127521276473999, Final Batch Loss: 0.5579783320426941\n",
      "Epoch 838, Loss: 2.167867958545685, Final Batch Loss: 0.5370624661445618\n",
      "Epoch 839, Loss: 2.145797163248062, Final Batch Loss: 0.4096735417842865\n",
      "Epoch 840, Loss: 2.2455346286296844, Final Batch Loss: 0.591708242893219\n",
      "Epoch 841, Loss: 2.1442839205265045, Final Batch Loss: 0.5502530336380005\n",
      "Epoch 842, Loss: 2.261998414993286, Final Batch Loss: 0.5242578387260437\n",
      "Epoch 843, Loss: 2.1441600024700165, Final Batch Loss: 0.5571907162666321\n",
      "Epoch 844, Loss: 2.073067992925644, Final Batch Loss: 0.44329020380973816\n",
      "Epoch 845, Loss: 2.0995976626873016, Final Batch Loss: 0.5381948351860046\n",
      "Epoch 846, Loss: 2.110716372728348, Final Batch Loss: 0.5949674844741821\n",
      "Epoch 847, Loss: 2.1162863969802856, Final Batch Loss: 0.560311496257782\n",
      "Epoch 848, Loss: 2.1550731658935547, Final Batch Loss: 0.5947853922843933\n",
      "Epoch 849, Loss: 2.203568547964096, Final Batch Loss: 0.6596713066101074\n",
      "Epoch 850, Loss: 2.243840306997299, Final Batch Loss: 0.5962194800376892\n",
      "Epoch 851, Loss: 2.217787653207779, Final Batch Loss: 0.5190596580505371\n",
      "Epoch 852, Loss: 2.040093868970871, Final Batch Loss: 0.5388543605804443\n",
      "Epoch 853, Loss: 2.261539340019226, Final Batch Loss: 0.6236571073532104\n",
      "Epoch 854, Loss: 2.246396631002426, Final Batch Loss: 0.5918874144554138\n",
      "Epoch 855, Loss: 2.1161674857139587, Final Batch Loss: 0.45443618297576904\n",
      "Epoch 856, Loss: 2.092874437570572, Final Batch Loss: 0.47590601444244385\n",
      "Epoch 857, Loss: 2.2746047377586365, Final Batch Loss: 0.6430009007453918\n",
      "Epoch 858, Loss: 2.114865303039551, Final Batch Loss: 0.560256838798523\n",
      "Epoch 859, Loss: 2.2801584601402283, Final Batch Loss: 0.5934625267982483\n",
      "Epoch 860, Loss: 2.1660870909690857, Final Batch Loss: 0.5102052092552185\n",
      "Epoch 861, Loss: 2.0966328382492065, Final Batch Loss: 0.4778427183628082\n",
      "Epoch 862, Loss: 2.1836414337158203, Final Batch Loss: 0.5710015892982483\n",
      "Epoch 863, Loss: 2.1535964012145996, Final Batch Loss: 0.4625275731086731\n",
      "Epoch 864, Loss: 2.181650757789612, Final Batch Loss: 0.6589258313179016\n",
      "Epoch 865, Loss: 2.1665239334106445, Final Batch Loss: 0.5388373136520386\n",
      "Epoch 866, Loss: 2.130288064479828, Final Batch Loss: 0.5359864234924316\n",
      "Epoch 867, Loss: 2.2174594402313232, Final Batch Loss: 0.5719192028045654\n",
      "Epoch 868, Loss: 1.967235803604126, Final Batch Loss: 0.4608728885650635\n",
      "Epoch 869, Loss: 2.0710866451263428, Final Batch Loss: 0.4844752848148346\n",
      "Epoch 870, Loss: 2.178294748067856, Final Batch Loss: 0.6293232440948486\n",
      "Epoch 871, Loss: 2.1043301820755005, Final Batch Loss: 0.5289084315299988\n",
      "Epoch 872, Loss: 2.0980706810951233, Final Batch Loss: 0.5133574604988098\n",
      "Epoch 873, Loss: 2.091499924659729, Final Batch Loss: 0.4079936742782593\n",
      "Epoch 874, Loss: 2.1545788943767548, Final Batch Loss: 0.6188873052597046\n",
      "Epoch 875, Loss: 2.1472183763980865, Final Batch Loss: 0.48705247044563293\n",
      "Epoch 876, Loss: 2.2025294303894043, Final Batch Loss: 0.5785693526268005\n",
      "Epoch 877, Loss: 2.2170220017433167, Final Batch Loss: 0.5907366871833801\n",
      "Epoch 878, Loss: 2.07657054066658, Final Batch Loss: 0.5588272213935852\n",
      "Epoch 879, Loss: 2.1984480023384094, Final Batch Loss: 0.4981839060783386\n",
      "Epoch 880, Loss: 2.1209325790405273, Final Batch Loss: 0.5501565933227539\n",
      "Epoch 881, Loss: 2.003190517425537, Final Batch Loss: 0.47852030396461487\n",
      "Epoch 882, Loss: 2.0995456278324127, Final Batch Loss: 0.5387930870056152\n",
      "Epoch 883, Loss: 2.2784485518932343, Final Batch Loss: 0.5553925633430481\n",
      "Epoch 884, Loss: 2.222694993019104, Final Batch Loss: 0.6399393081665039\n",
      "Epoch 885, Loss: 2.1665905714035034, Final Batch Loss: 0.46721315383911133\n",
      "Epoch 886, Loss: 2.037132829427719, Final Batch Loss: 0.4793013036251068\n",
      "Epoch 887, Loss: 2.1249765157699585, Final Batch Loss: 0.5118038654327393\n",
      "Epoch 888, Loss: 2.072677195072174, Final Batch Loss: 0.43558841943740845\n",
      "Epoch 889, Loss: 2.173227846622467, Final Batch Loss: 0.6177889704704285\n",
      "Epoch 890, Loss: 2.1151098012924194, Final Batch Loss: 0.4839959740638733\n",
      "Epoch 891, Loss: 2.0297394692897797, Final Batch Loss: 0.46956703066825867\n",
      "Epoch 892, Loss: 2.114784836769104, Final Batch Loss: 0.506369411945343\n",
      "Epoch 893, Loss: 2.181349515914917, Final Batch Loss: 0.5692994594573975\n",
      "Epoch 894, Loss: 2.021595925092697, Final Batch Loss: 0.45260733366012573\n",
      "Epoch 895, Loss: 2.132041275501251, Final Batch Loss: 0.5331003665924072\n",
      "Epoch 896, Loss: 2.051926463842392, Final Batch Loss: 0.4663286507129669\n",
      "Epoch 897, Loss: 2.1202667355537415, Final Batch Loss: 0.535561740398407\n",
      "Epoch 898, Loss: 2.2012177407741547, Final Batch Loss: 0.7363434433937073\n",
      "Epoch 899, Loss: 2.0674433410167694, Final Batch Loss: 0.5683857798576355\n",
      "Epoch 900, Loss: 2.2026796638965607, Final Batch Loss: 0.6651684045791626\n",
      "Epoch 901, Loss: 2.129571706056595, Final Batch Loss: 0.5522798299789429\n",
      "Epoch 902, Loss: 2.2419388592243195, Final Batch Loss: 0.6192597150802612\n",
      "Epoch 903, Loss: 2.1480879485607147, Final Batch Loss: 0.5229297876358032\n",
      "Epoch 904, Loss: 2.0961694419384003, Final Batch Loss: 0.5727546215057373\n",
      "Epoch 905, Loss: 2.1459757685661316, Final Batch Loss: 0.5322161912918091\n",
      "Epoch 906, Loss: 2.0232345163822174, Final Batch Loss: 0.4213118255138397\n",
      "Epoch 907, Loss: 2.1113873720169067, Final Batch Loss: 0.6969233751296997\n",
      "Epoch 908, Loss: 2.07007560133934, Final Batch Loss: 0.4943487048149109\n",
      "Epoch 909, Loss: 2.082092523574829, Final Batch Loss: 0.5018055438995361\n",
      "Epoch 910, Loss: 2.057041674852371, Final Batch Loss: 0.44517067074775696\n",
      "Epoch 911, Loss: 2.081895262002945, Final Batch Loss: 0.5234707593917847\n",
      "Epoch 912, Loss: 2.1395602226257324, Final Batch Loss: 0.574964702129364\n",
      "Epoch 913, Loss: 2.1168259978294373, Final Batch Loss: 0.543204665184021\n",
      "Epoch 914, Loss: 2.1326116919517517, Final Batch Loss: 0.5077948570251465\n",
      "Epoch 915, Loss: 2.0701196491718292, Final Batch Loss: 0.5153420567512512\n",
      "Epoch 916, Loss: 1.9718995988368988, Final Batch Loss: 0.4055510461330414\n",
      "Epoch 917, Loss: 2.044675439596176, Final Batch Loss: 0.5084749460220337\n",
      "Epoch 918, Loss: 2.2372469902038574, Final Batch Loss: 0.6062096357345581\n",
      "Epoch 919, Loss: 2.0586659908294678, Final Batch Loss: 0.42941075563430786\n",
      "Epoch 920, Loss: 2.152247130870819, Final Batch Loss: 0.5822882652282715\n",
      "Epoch 921, Loss: 2.2172878980636597, Final Batch Loss: 0.5190801620483398\n",
      "Epoch 922, Loss: 2.2193164229393005, Final Batch Loss: 0.5185453295707703\n",
      "Epoch 923, Loss: 2.06456395983696, Final Batch Loss: 0.5737584233283997\n",
      "Epoch 924, Loss: 2.199297994375229, Final Batch Loss: 0.6683031916618347\n",
      "Epoch 925, Loss: 2.0971764624118805, Final Batch Loss: 0.5856641530990601\n",
      "Epoch 926, Loss: 2.066078305244446, Final Batch Loss: 0.5612825155258179\n",
      "Epoch 927, Loss: 2.1104524731636047, Final Batch Loss: 0.5053980946540833\n",
      "Epoch 928, Loss: 2.0914353728294373, Final Batch Loss: 0.5508965849876404\n",
      "Epoch 929, Loss: 2.089762330055237, Final Batch Loss: 0.5704642534255981\n",
      "Epoch 930, Loss: 2.123995780944824, Final Batch Loss: 0.5755833983421326\n",
      "Epoch 931, Loss: 2.0852410197257996, Final Batch Loss: 0.5728992223739624\n",
      "Epoch 932, Loss: 2.1152967512607574, Final Batch Loss: 0.527545690536499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 933, Loss: 2.2285079658031464, Final Batch Loss: 0.6588587164878845\n",
      "Epoch 934, Loss: 2.056713044643402, Final Batch Loss: 0.5418328046798706\n",
      "Epoch 935, Loss: 2.091959536075592, Final Batch Loss: 0.587066113948822\n",
      "Epoch 936, Loss: 2.14545214176178, Final Batch Loss: 0.5685575008392334\n",
      "Epoch 937, Loss: 1.9109016954898834, Final Batch Loss: 0.41396650671958923\n",
      "Epoch 938, Loss: 1.9987834990024567, Final Batch Loss: 0.42191022634506226\n",
      "Epoch 939, Loss: 2.0208490788936615, Final Batch Loss: 0.5450426340103149\n",
      "Epoch 940, Loss: 2.0268382728099823, Final Batch Loss: 0.4804452955722809\n",
      "Epoch 941, Loss: 2.003705322742462, Final Batch Loss: 0.5468428730964661\n",
      "Epoch 942, Loss: 2.0732312202453613, Final Batch Loss: 0.48395586013793945\n",
      "Epoch 943, Loss: 2.104697048664093, Final Batch Loss: 0.6137632727622986\n",
      "Epoch 944, Loss: 2.115479052066803, Final Batch Loss: 0.5388909578323364\n",
      "Epoch 945, Loss: 2.0888916850090027, Final Batch Loss: 0.4723321795463562\n",
      "Epoch 946, Loss: 2.07474884390831, Final Batch Loss: 0.4627331793308258\n",
      "Epoch 947, Loss: 2.058260887861252, Final Batch Loss: 0.480835884809494\n",
      "Epoch 948, Loss: 2.211534321308136, Final Batch Loss: 0.5987469553947449\n",
      "Epoch 949, Loss: 2.0960598587989807, Final Batch Loss: 0.5000545382499695\n",
      "Epoch 950, Loss: 2.1486659944057465, Final Batch Loss: 0.5710822343826294\n",
      "Epoch 951, Loss: 2.0362625122070312, Final Batch Loss: 0.4988885521888733\n",
      "Epoch 952, Loss: 2.102166712284088, Final Batch Loss: 0.5258429050445557\n",
      "Epoch 953, Loss: 2.0325250327587128, Final Batch Loss: 0.441182941198349\n",
      "Epoch 954, Loss: 2.2156751453876495, Final Batch Loss: 0.5927704572677612\n",
      "Epoch 955, Loss: 2.013538658618927, Final Batch Loss: 0.5205367207527161\n",
      "Epoch 956, Loss: 2.102548122406006, Final Batch Loss: 0.5948941707611084\n",
      "Epoch 957, Loss: 2.112494647502899, Final Batch Loss: 0.5337866544723511\n",
      "Epoch 958, Loss: 2.0065965354442596, Final Batch Loss: 0.41334912180900574\n",
      "Epoch 959, Loss: 2.122131824493408, Final Batch Loss: 0.5938613414764404\n",
      "Epoch 960, Loss: 2.0954687893390656, Final Batch Loss: 0.525763750076294\n",
      "Epoch 961, Loss: 2.001378834247589, Final Batch Loss: 0.5023682713508606\n",
      "Epoch 962, Loss: 1.9828360974788666, Final Batch Loss: 0.48560696840286255\n",
      "Epoch 963, Loss: 2.120011955499649, Final Batch Loss: 0.635981559753418\n",
      "Epoch 964, Loss: 2.049891471862793, Final Batch Loss: 0.4973812699317932\n",
      "Epoch 965, Loss: 2.0463628470897675, Final Batch Loss: 0.49283215403556824\n",
      "Epoch 966, Loss: 2.115914076566696, Final Batch Loss: 0.6333233118057251\n",
      "Epoch 967, Loss: 1.9194113910198212, Final Batch Loss: 0.4452377259731293\n",
      "Epoch 968, Loss: 1.8936297595500946, Final Batch Loss: 0.40629300475120544\n",
      "Epoch 969, Loss: 2.0031817853450775, Final Batch Loss: 0.467721551656723\n",
      "Epoch 970, Loss: 2.1550737619400024, Final Batch Loss: 0.6281250715255737\n",
      "Epoch 971, Loss: 1.911757230758667, Final Batch Loss: 0.47547388076782227\n",
      "Epoch 972, Loss: 2.216915935277939, Final Batch Loss: 0.6657507419586182\n",
      "Epoch 973, Loss: 2.039068341255188, Final Batch Loss: 0.5359449982643127\n",
      "Epoch 974, Loss: 2.099383234977722, Final Batch Loss: 0.6314130425453186\n",
      "Epoch 975, Loss: 1.9799695909023285, Final Batch Loss: 0.44659051299095154\n",
      "Epoch 976, Loss: 2.0194517076015472, Final Batch Loss: 0.4758491814136505\n",
      "Epoch 977, Loss: 2.159638285636902, Final Batch Loss: 0.6220701336860657\n",
      "Epoch 978, Loss: 2.0492222011089325, Final Batch Loss: 0.5527821183204651\n",
      "Epoch 979, Loss: 2.1081675589084625, Final Batch Loss: 0.47572657465934753\n",
      "Epoch 980, Loss: 2.058507263660431, Final Batch Loss: 0.5360398888587952\n",
      "Epoch 981, Loss: 1.9589543044567108, Final Batch Loss: 0.3922070264816284\n",
      "Epoch 982, Loss: 2.1689690947532654, Final Batch Loss: 0.5239055752754211\n",
      "Epoch 983, Loss: 2.074733167886734, Final Batch Loss: 0.5394139289855957\n",
      "Epoch 984, Loss: 2.0367191433906555, Final Batch Loss: 0.5555090308189392\n",
      "Epoch 985, Loss: 2.191368281841278, Final Batch Loss: 0.5998131036758423\n",
      "Epoch 986, Loss: 2.1174601018428802, Final Batch Loss: 0.5746551156044006\n",
      "Epoch 987, Loss: 2.0883772671222687, Final Batch Loss: 0.4666949212551117\n",
      "Epoch 988, Loss: 2.0601062774658203, Final Batch Loss: 0.5131779909133911\n",
      "Epoch 989, Loss: 2.119097739458084, Final Batch Loss: 0.582097053527832\n",
      "Epoch 990, Loss: 2.037435621023178, Final Batch Loss: 0.5463802814483643\n",
      "Epoch 991, Loss: 1.9638178944587708, Final Batch Loss: 0.5203514695167542\n",
      "Epoch 992, Loss: 2.004776269197464, Final Batch Loss: 0.4690339267253876\n",
      "Epoch 993, Loss: 2.064985603094101, Final Batch Loss: 0.49336522817611694\n",
      "Epoch 994, Loss: 2.0095914602279663, Final Batch Loss: 0.4944033622741699\n",
      "Epoch 995, Loss: 1.9549654126167297, Final Batch Loss: 0.4180004596710205\n",
      "Epoch 996, Loss: 2.0192206501960754, Final Batch Loss: 0.4838029146194458\n",
      "Epoch 997, Loss: 1.9482006430625916, Final Batch Loss: 0.5099884867668152\n",
      "Epoch 998, Loss: 1.9530664384365082, Final Batch Loss: 0.49574607610702515\n",
      "Epoch 999, Loss: 2.023915708065033, Final Batch Loss: 0.6132883429527283\n",
      "Epoch 1000, Loss: 2.12185138463974, Final Batch Loss: 0.6005175709724426\n",
      "Epoch 1001, Loss: 2.0211151242256165, Final Batch Loss: 0.5714090466499329\n",
      "Epoch 1002, Loss: 1.9937105476856232, Final Batch Loss: 0.45614129304885864\n",
      "Epoch 1003, Loss: 1.911475956439972, Final Batch Loss: 0.4646170139312744\n",
      "Epoch 1004, Loss: 2.005785644054413, Final Batch Loss: 0.4187397360801697\n",
      "Epoch 1005, Loss: 2.050112307071686, Final Batch Loss: 0.4674965441226959\n",
      "Epoch 1006, Loss: 2.0468981862068176, Final Batch Loss: 0.446521520614624\n",
      "Epoch 1007, Loss: 1.96405029296875, Final Batch Loss: 0.4222275912761688\n",
      "Epoch 1008, Loss: 1.8846527934074402, Final Batch Loss: 0.4158134162425995\n",
      "Epoch 1009, Loss: 2.0435088872909546, Final Batch Loss: 0.587071418762207\n",
      "Epoch 1010, Loss: 2.066293239593506, Final Batch Loss: 0.523655891418457\n",
      "Epoch 1011, Loss: 1.967683106660843, Final Batch Loss: 0.5089443325996399\n",
      "Epoch 1012, Loss: 2.0152716636657715, Final Batch Loss: 0.5184733271598816\n",
      "Epoch 1013, Loss: 2.081429809331894, Final Batch Loss: 0.6414289474487305\n",
      "Epoch 1014, Loss: 2.1101832687854767, Final Batch Loss: 0.48228296637535095\n",
      "Epoch 1015, Loss: 2.074367433786392, Final Batch Loss: 0.45052918791770935\n",
      "Epoch 1016, Loss: 2.0216138660907745, Final Batch Loss: 0.5102936029434204\n",
      "Epoch 1017, Loss: 1.9902759790420532, Final Batch Loss: 0.5195159912109375\n",
      "Epoch 1018, Loss: 1.921444445848465, Final Batch Loss: 0.3646831214427948\n",
      "Epoch 1019, Loss: 1.9526838958263397, Final Batch Loss: 0.3795343339443207\n",
      "Epoch 1020, Loss: 2.0177232921123505, Final Batch Loss: 0.5684332251548767\n",
      "Epoch 1021, Loss: 2.140180826187134, Final Batch Loss: 0.5531009435653687\n",
      "Epoch 1022, Loss: 1.9272829592227936, Final Batch Loss: 0.48150408267974854\n",
      "Epoch 1023, Loss: 2.117461770772934, Final Batch Loss: 0.6282304525375366\n",
      "Epoch 1024, Loss: 2.0169463753700256, Final Batch Loss: 0.5196629166603088\n",
      "Epoch 1025, Loss: 2.0082297325134277, Final Batch Loss: 0.5768678784370422\n",
      "Epoch 1026, Loss: 1.8997999429702759, Final Batch Loss: 0.4582182466983795\n",
      "Epoch 1027, Loss: 2.130026489496231, Final Batch Loss: 0.6103744506835938\n",
      "Epoch 1028, Loss: 1.9657819867134094, Final Batch Loss: 0.4937871992588043\n",
      "Epoch 1029, Loss: 2.0221078395843506, Final Batch Loss: 0.49875351786613464\n",
      "Epoch 1030, Loss: 1.8521988987922668, Final Batch Loss: 0.3774799406528473\n",
      "Epoch 1031, Loss: 2.019350051879883, Final Batch Loss: 0.5454800128936768\n",
      "Epoch 1032, Loss: 2.0122976303100586, Final Batch Loss: 0.45349380373954773\n",
      "Epoch 1033, Loss: 1.900573343038559, Final Batch Loss: 0.47193408012390137\n",
      "Epoch 1034, Loss: 2.104466289281845, Final Batch Loss: 0.5601651072502136\n",
      "Epoch 1035, Loss: 2.0560401678085327, Final Batch Loss: 0.6266327500343323\n",
      "Epoch 1036, Loss: 2.0774319767951965, Final Batch Loss: 0.4807684123516083\n",
      "Epoch 1037, Loss: 1.9880283176898956, Final Batch Loss: 0.43651410937309265\n",
      "Epoch 1038, Loss: 2.039926826953888, Final Batch Loss: 0.5051969289779663\n",
      "Epoch 1039, Loss: 1.987912118434906, Final Batch Loss: 0.5166373252868652\n",
      "Epoch 1040, Loss: 2.093880534172058, Final Batch Loss: 0.516484797000885\n",
      "Epoch 1041, Loss: 2.0838580429553986, Final Batch Loss: 0.5663069486618042\n",
      "Epoch 1042, Loss: 1.9465788304805756, Final Batch Loss: 0.48327791690826416\n",
      "Epoch 1043, Loss: 1.9430424869060516, Final Batch Loss: 0.49446550011634827\n",
      "Epoch 1044, Loss: 1.9412079751491547, Final Batch Loss: 0.44739001989364624\n",
      "Epoch 1045, Loss: 2.03848934173584, Final Batch Loss: 0.5010522603988647\n",
      "Epoch 1046, Loss: 1.990452229976654, Final Batch Loss: 0.5437806844711304\n",
      "Epoch 1047, Loss: 1.947846680879593, Final Batch Loss: 0.4923926293849945\n",
      "Epoch 1048, Loss: 2.0664255917072296, Final Batch Loss: 0.5874037742614746\n",
      "Epoch 1049, Loss: 1.8960554003715515, Final Batch Loss: 0.38469839096069336\n",
      "Epoch 1050, Loss: 2.086773306131363, Final Batch Loss: 0.6435416340827942\n",
      "Epoch 1051, Loss: 2.055581569671631, Final Batch Loss: 0.5832982063293457\n",
      "Epoch 1052, Loss: 2.00815150141716, Final Batch Loss: 0.4209479093551636\n",
      "Epoch 1053, Loss: 1.9756177067756653, Final Batch Loss: 0.43744733929634094\n",
      "Epoch 1054, Loss: 2.0057199597358704, Final Batch Loss: 0.43007802963256836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1055, Loss: 1.957447588443756, Final Batch Loss: 0.5946530103683472\n",
      "Epoch 1056, Loss: 2.016486316919327, Final Batch Loss: 0.6052829623222351\n",
      "Epoch 1057, Loss: 1.8995638489723206, Final Batch Loss: 0.38062402606010437\n",
      "Epoch 1058, Loss: 2.0544992685317993, Final Batch Loss: 0.4698059558868408\n",
      "Epoch 1059, Loss: 1.9975579977035522, Final Batch Loss: 0.576847493648529\n",
      "Epoch 1060, Loss: 2.011071741580963, Final Batch Loss: 0.4772973358631134\n",
      "Epoch 1061, Loss: 2.018679678440094, Final Batch Loss: 0.4751833379268646\n",
      "Epoch 1062, Loss: 2.0332140624523163, Final Batch Loss: 0.5526449084281921\n",
      "Epoch 1063, Loss: 1.9846810698509216, Final Batch Loss: 0.49380919337272644\n",
      "Epoch 1064, Loss: 2.0062763392925262, Final Batch Loss: 0.44955113530158997\n",
      "Epoch 1065, Loss: 1.9505350291728973, Final Batch Loss: 0.44769638776779175\n",
      "Epoch 1066, Loss: 2.0263226330280304, Final Batch Loss: 0.603948712348938\n",
      "Epoch 1067, Loss: 2.012321263551712, Final Batch Loss: 0.6230263710021973\n",
      "Epoch 1068, Loss: 1.9122653305530548, Final Batch Loss: 0.4601578116416931\n",
      "Epoch 1069, Loss: 2.0668735206127167, Final Batch Loss: 0.5679795145988464\n",
      "Epoch 1070, Loss: 1.9717828631401062, Final Batch Loss: 0.5388454794883728\n",
      "Epoch 1071, Loss: 1.865164339542389, Final Batch Loss: 0.502819299697876\n",
      "Epoch 1072, Loss: 1.9922731220722198, Final Batch Loss: 0.5102316737174988\n",
      "Epoch 1073, Loss: 2.129548043012619, Final Batch Loss: 0.642941951751709\n",
      "Epoch 1074, Loss: 1.9715741276741028, Final Batch Loss: 0.5480307340621948\n",
      "Epoch 1075, Loss: 2.058550238609314, Final Batch Loss: 0.515619158744812\n",
      "Epoch 1076, Loss: 1.9645116329193115, Final Batch Loss: 0.49245935678482056\n",
      "Epoch 1077, Loss: 1.9982372224330902, Final Batch Loss: 0.5244360566139221\n",
      "Epoch 1078, Loss: 2.1151713728904724, Final Batch Loss: 0.5112287998199463\n",
      "Epoch 1079, Loss: 2.0224797427654266, Final Batch Loss: 0.5007082223892212\n",
      "Epoch 1080, Loss: 2.0693713426589966, Final Batch Loss: 0.5924910306930542\n",
      "Epoch 1081, Loss: 2.1429988145828247, Final Batch Loss: 0.6168084144592285\n",
      "Epoch 1082, Loss: 1.9884607791900635, Final Batch Loss: 0.47379204630851746\n",
      "Epoch 1083, Loss: 2.064084142446518, Final Batch Loss: 0.48281076550483704\n",
      "Epoch 1084, Loss: 1.9663130342960358, Final Batch Loss: 0.4668329656124115\n",
      "Epoch 1085, Loss: 1.9621528685092926, Final Batch Loss: 0.5223886370658875\n",
      "Epoch 1086, Loss: 1.8958693444728851, Final Batch Loss: 0.5230069756507874\n",
      "Epoch 1087, Loss: 1.945862591266632, Final Batch Loss: 0.4261816740036011\n",
      "Epoch 1088, Loss: 1.9503043293952942, Final Batch Loss: 0.4816502332687378\n",
      "Epoch 1089, Loss: 2.0334337055683136, Final Batch Loss: 0.5040866732597351\n",
      "Epoch 1090, Loss: 1.9378927946090698, Final Batch Loss: 0.5365613102912903\n",
      "Epoch 1091, Loss: 1.9910501837730408, Final Batch Loss: 0.5101534724235535\n",
      "Epoch 1092, Loss: 1.9798420667648315, Final Batch Loss: 0.5032497644424438\n",
      "Epoch 1093, Loss: 1.9594852328300476, Final Batch Loss: 0.4956223666667938\n",
      "Epoch 1094, Loss: 1.951128363609314, Final Batch Loss: 0.4022473692893982\n",
      "Epoch 1095, Loss: 2.027109831571579, Final Batch Loss: 0.4934113919734955\n",
      "Epoch 1096, Loss: 1.9503079950809479, Final Batch Loss: 0.46289992332458496\n",
      "Epoch 1097, Loss: 2.043840318918228, Final Batch Loss: 0.5677856206893921\n",
      "Epoch 1098, Loss: 1.9303598403930664, Final Batch Loss: 0.45329007506370544\n",
      "Epoch 1099, Loss: 2.019766628742218, Final Batch Loss: 0.5104066729545593\n",
      "Epoch 1100, Loss: 1.9238254129886627, Final Batch Loss: 0.43701523542404175\n",
      "Epoch 1101, Loss: 2.010345906019211, Final Batch Loss: 0.492646723985672\n",
      "Epoch 1102, Loss: 1.9731230735778809, Final Batch Loss: 0.509909987449646\n",
      "Epoch 1103, Loss: 2.006685793399811, Final Batch Loss: 0.52107834815979\n",
      "Epoch 1104, Loss: 1.9912392795085907, Final Batch Loss: 0.49393677711486816\n",
      "Epoch 1105, Loss: 1.9278112947940826, Final Batch Loss: 0.471014142036438\n",
      "Epoch 1106, Loss: 1.9745133817195892, Final Batch Loss: 0.442292183637619\n",
      "Epoch 1107, Loss: 1.9296109974384308, Final Batch Loss: 0.4545537233352661\n",
      "Epoch 1108, Loss: 1.9406259655952454, Final Batch Loss: 0.4781019389629364\n",
      "Epoch 1109, Loss: 1.9126747250556946, Final Batch Loss: 0.5370326042175293\n",
      "Epoch 1110, Loss: 1.9336294531822205, Final Batch Loss: 0.4486289620399475\n",
      "Epoch 1111, Loss: 1.967784821987152, Final Batch Loss: 0.528975248336792\n",
      "Epoch 1112, Loss: 2.01213800907135, Final Batch Loss: 0.5037395358085632\n",
      "Epoch 1113, Loss: 1.876791387796402, Final Batch Loss: 0.49889621138572693\n",
      "Epoch 1114, Loss: 1.9047121107578278, Final Batch Loss: 0.4264685809612274\n",
      "Epoch 1115, Loss: 1.9604174196720123, Final Batch Loss: 0.32840076088905334\n",
      "Epoch 1116, Loss: 2.038542538881302, Final Batch Loss: 0.6142467260360718\n",
      "Epoch 1117, Loss: 1.9926265478134155, Final Batch Loss: 0.4475937485694885\n",
      "Epoch 1118, Loss: 1.9147220849990845, Final Batch Loss: 0.5101284980773926\n",
      "Epoch 1119, Loss: 2.0152520835399628, Final Batch Loss: 0.5297684669494629\n",
      "Epoch 1120, Loss: 1.8769581615924835, Final Batch Loss: 0.5034793019294739\n",
      "Epoch 1121, Loss: 1.9156593084335327, Final Batch Loss: 0.3676675260066986\n",
      "Epoch 1122, Loss: 2.1455068588256836, Final Batch Loss: 0.5997424125671387\n",
      "Epoch 1123, Loss: 2.094950318336487, Final Batch Loss: 0.6020634770393372\n",
      "Epoch 1124, Loss: 1.940411776304245, Final Batch Loss: 0.45598772168159485\n",
      "Epoch 1125, Loss: 1.9574668407440186, Final Batch Loss: 0.5468085408210754\n",
      "Epoch 1126, Loss: 2.0070293843746185, Final Batch Loss: 0.5318332314491272\n",
      "Epoch 1127, Loss: 1.8632645010948181, Final Batch Loss: 0.4498741030693054\n",
      "Epoch 1128, Loss: 1.9838427603244781, Final Batch Loss: 0.5521226525306702\n",
      "Epoch 1129, Loss: 1.869209736585617, Final Batch Loss: 0.40921032428741455\n",
      "Epoch 1130, Loss: 1.913312405347824, Final Batch Loss: 0.5443640351295471\n",
      "Epoch 1131, Loss: 1.8492974936962128, Final Batch Loss: 0.46396589279174805\n",
      "Epoch 1132, Loss: 1.9219293296337128, Final Batch Loss: 0.5198255777359009\n",
      "Epoch 1133, Loss: 1.9609918296337128, Final Batch Loss: 0.5256125330924988\n",
      "Epoch 1134, Loss: 1.870736837387085, Final Batch Loss: 0.39794763922691345\n",
      "Epoch 1135, Loss: 1.9615816473960876, Final Batch Loss: 0.46736863255500793\n",
      "Epoch 1136, Loss: 1.9281468093395233, Final Batch Loss: 0.5643787980079651\n",
      "Epoch 1137, Loss: 1.875174641609192, Final Batch Loss: 0.4841720163822174\n",
      "Epoch 1138, Loss: 1.9378994703292847, Final Batch Loss: 0.5345761179924011\n",
      "Epoch 1139, Loss: 1.9515151679515839, Final Batch Loss: 0.4113517999649048\n",
      "Epoch 1140, Loss: 2.046136438846588, Final Batch Loss: 0.5459179878234863\n",
      "Epoch 1141, Loss: 1.938364714384079, Final Batch Loss: 0.47666123509407043\n",
      "Epoch 1142, Loss: 2.1501751244068146, Final Batch Loss: 0.634550929069519\n",
      "Epoch 1143, Loss: 1.8392570316791534, Final Batch Loss: 0.39957326650619507\n",
      "Epoch 1144, Loss: 1.9808124899864197, Final Batch Loss: 0.5359827280044556\n",
      "Epoch 1145, Loss: 1.8561758399009705, Final Batch Loss: 0.3971148729324341\n",
      "Epoch 1146, Loss: 1.884721964597702, Final Batch Loss: 0.4864140450954437\n",
      "Epoch 1147, Loss: 1.8728637993335724, Final Batch Loss: 0.41378867626190186\n",
      "Epoch 1148, Loss: 1.9881476759910583, Final Batch Loss: 0.5390928983688354\n",
      "Epoch 1149, Loss: 1.9046804308891296, Final Batch Loss: 0.5460689663887024\n",
      "Epoch 1150, Loss: 1.8951844274997711, Final Batch Loss: 0.46472251415252686\n",
      "Epoch 1151, Loss: 1.9403593242168427, Final Batch Loss: 0.4534527063369751\n",
      "Epoch 1152, Loss: 1.922603189945221, Final Batch Loss: 0.4162076413631439\n",
      "Epoch 1153, Loss: 1.9854287207126617, Final Batch Loss: 0.5352683067321777\n",
      "Epoch 1154, Loss: 1.957906812429428, Final Batch Loss: 0.4848167598247528\n",
      "Epoch 1155, Loss: 1.8884426057338715, Final Batch Loss: 0.3645005226135254\n",
      "Epoch 1156, Loss: 1.93185693025589, Final Batch Loss: 0.44923704862594604\n",
      "Epoch 1157, Loss: 1.8527597784996033, Final Batch Loss: 0.3664460778236389\n",
      "Epoch 1158, Loss: 2.0147693753242493, Final Batch Loss: 0.5967968702316284\n",
      "Epoch 1159, Loss: 1.8817984461784363, Final Batch Loss: 0.4180833399295807\n",
      "Epoch 1160, Loss: 1.87314772605896, Final Batch Loss: 0.4228995740413666\n",
      "Epoch 1161, Loss: 2.039619743824005, Final Batch Loss: 0.5036495327949524\n",
      "Epoch 1162, Loss: 1.8821912705898285, Final Batch Loss: 0.45244506001472473\n",
      "Epoch 1163, Loss: 1.96637761592865, Final Batch Loss: 0.5124652981758118\n",
      "Epoch 1164, Loss: 1.9534251987934113, Final Batch Loss: 0.5536358952522278\n",
      "Epoch 1165, Loss: 2.0408811569213867, Final Batch Loss: 0.4994591474533081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1166, Loss: 1.9040405452251434, Final Batch Loss: 0.4435005784034729\n",
      "Epoch 1167, Loss: 2.005863845348358, Final Batch Loss: 0.5407161712646484\n",
      "Epoch 1168, Loss: 1.9650906324386597, Final Batch Loss: 0.5480897426605225\n",
      "Epoch 1169, Loss: 1.8715543448925018, Final Batch Loss: 0.41021615266799927\n",
      "Epoch 1170, Loss: 1.8775953948497772, Final Batch Loss: 0.43212828040122986\n",
      "Epoch 1171, Loss: 1.8396298289299011, Final Batch Loss: 0.42985790967941284\n",
      "Epoch 1172, Loss: 1.9127021133899689, Final Batch Loss: 0.5287186503410339\n",
      "Epoch 1173, Loss: 1.9859135150909424, Final Batch Loss: 0.47588297724723816\n",
      "Epoch 1174, Loss: 1.9313431680202484, Final Batch Loss: 0.4120861291885376\n",
      "Epoch 1175, Loss: 1.9583291411399841, Final Batch Loss: 0.5406709313392639\n",
      "Epoch 1176, Loss: 1.9382374584674835, Final Batch Loss: 0.48573195934295654\n",
      "Epoch 1177, Loss: 1.9897958934307098, Final Batch Loss: 0.5105865597724915\n",
      "Epoch 1178, Loss: 1.8277203440666199, Final Batch Loss: 0.4626002907752991\n",
      "Epoch 1179, Loss: 1.9546380043029785, Final Batch Loss: 0.5386883020401001\n",
      "Epoch 1180, Loss: 1.9527490735054016, Final Batch Loss: 0.47637611627578735\n",
      "Epoch 1181, Loss: 1.962299644947052, Final Batch Loss: 0.532418429851532\n",
      "Epoch 1182, Loss: 1.8622049987316132, Final Batch Loss: 0.45459696650505066\n",
      "Epoch 1183, Loss: 2.040037363767624, Final Batch Loss: 0.5974597930908203\n",
      "Epoch 1184, Loss: 1.9111151695251465, Final Batch Loss: 0.4747074544429779\n",
      "Epoch 1185, Loss: 2.013341099023819, Final Batch Loss: 0.4961487352848053\n",
      "Epoch 1186, Loss: 2.0225934386253357, Final Batch Loss: 0.5710031390190125\n",
      "Epoch 1187, Loss: 1.9136420786380768, Final Batch Loss: 0.4036923050880432\n",
      "Epoch 1188, Loss: 1.8696182370185852, Final Batch Loss: 0.4403274655342102\n",
      "Epoch 1189, Loss: 1.8469320833683014, Final Batch Loss: 0.42500436305999756\n",
      "Epoch 1190, Loss: 2.006348639726639, Final Batch Loss: 0.5368382334709167\n",
      "Epoch 1191, Loss: 1.9783053398132324, Final Batch Loss: 0.5256991386413574\n",
      "Epoch 1192, Loss: 1.954993486404419, Final Batch Loss: 0.5547293424606323\n",
      "Epoch 1193, Loss: 1.9183396697044373, Final Batch Loss: 0.39496728777885437\n",
      "Epoch 1194, Loss: 1.9714581668376923, Final Batch Loss: 0.5534826517105103\n",
      "Epoch 1195, Loss: 1.8170642256736755, Final Batch Loss: 0.47714975476264954\n",
      "Epoch 1196, Loss: 1.8561508655548096, Final Batch Loss: 0.3898784816265106\n",
      "Epoch 1197, Loss: 1.8813692927360535, Final Batch Loss: 0.4397636353969574\n",
      "Epoch 1198, Loss: 1.9986330270767212, Final Batch Loss: 0.5737481117248535\n",
      "Epoch 1199, Loss: 1.932140737771988, Final Batch Loss: 0.4648023545742035\n",
      "Epoch 1200, Loss: 2.0009010434150696, Final Batch Loss: 0.5857349634170532\n",
      "Epoch 1201, Loss: 1.9337929785251617, Final Batch Loss: 0.42158883810043335\n",
      "Epoch 1202, Loss: 1.8481530845165253, Final Batch Loss: 0.4785054922103882\n",
      "Epoch 1203, Loss: 1.922329992055893, Final Batch Loss: 0.556194007396698\n",
      "Epoch 1204, Loss: 1.7807520031929016, Final Batch Loss: 0.37162673473358154\n",
      "Epoch 1205, Loss: 1.8398610353469849, Final Batch Loss: 0.41092538833618164\n",
      "Epoch 1206, Loss: 2.115429639816284, Final Batch Loss: 0.7132794857025146\n",
      "Epoch 1207, Loss: 2.0168166756629944, Final Batch Loss: 0.5630687475204468\n",
      "Epoch 1208, Loss: 2.019604355096817, Final Batch Loss: 0.5281047224998474\n",
      "Epoch 1209, Loss: 1.9153146147727966, Final Batch Loss: 0.48730286955833435\n",
      "Epoch 1210, Loss: 2.0223485231399536, Final Batch Loss: 0.4698035418987274\n",
      "Epoch 1211, Loss: 2.011511445045471, Final Batch Loss: 0.6344233155250549\n",
      "Epoch 1212, Loss: 2.003675192594528, Final Batch Loss: 0.5336573719978333\n",
      "Epoch 1213, Loss: 1.9411626756191254, Final Batch Loss: 0.4497718811035156\n",
      "Epoch 1214, Loss: 1.8321498036384583, Final Batch Loss: 0.46923041343688965\n",
      "Epoch 1215, Loss: 1.9302568435668945, Final Batch Loss: 0.4865119159221649\n",
      "Epoch 1216, Loss: 1.8950942158699036, Final Batch Loss: 0.4834214448928833\n",
      "Epoch 1217, Loss: 1.8203775584697723, Final Batch Loss: 0.42812293767929077\n",
      "Epoch 1218, Loss: 1.8498960137367249, Final Batch Loss: 0.48273059725761414\n",
      "Epoch 1219, Loss: 1.9045306742191315, Final Batch Loss: 0.5435585975646973\n",
      "Epoch 1220, Loss: 1.7714018821716309, Final Batch Loss: 0.4071882963180542\n",
      "Epoch 1221, Loss: 1.9673536121845245, Final Batch Loss: 0.5340378284454346\n",
      "Epoch 1222, Loss: 1.8122891187667847, Final Batch Loss: 0.48461562395095825\n",
      "Epoch 1223, Loss: 1.9079627394676208, Final Batch Loss: 0.47327056527137756\n",
      "Epoch 1224, Loss: 2.0306858122348785, Final Batch Loss: 0.42121240496635437\n",
      "Epoch 1225, Loss: 2.1140004098415375, Final Batch Loss: 0.6683524250984192\n",
      "Epoch 1226, Loss: 1.848129153251648, Final Batch Loss: 0.37685802578926086\n",
      "Epoch 1227, Loss: 2.052204668521881, Final Batch Loss: 0.6124431490898132\n",
      "Epoch 1228, Loss: 1.8479220271110535, Final Batch Loss: 0.507419764995575\n",
      "Epoch 1229, Loss: 2.0361935794353485, Final Batch Loss: 0.5727102756500244\n",
      "Epoch 1230, Loss: 1.862871378660202, Final Batch Loss: 0.4036925435066223\n",
      "Epoch 1231, Loss: 1.862744301557541, Final Batch Loss: 0.45514583587646484\n",
      "Epoch 1232, Loss: 1.8819052278995514, Final Batch Loss: 0.4460337162017822\n",
      "Epoch 1233, Loss: 1.83669775724411, Final Batch Loss: 0.4481358230113983\n",
      "Epoch 1234, Loss: 1.9642548263072968, Final Batch Loss: 0.45230650901794434\n",
      "Epoch 1235, Loss: 1.8273133635520935, Final Batch Loss: 0.5212439894676208\n",
      "Epoch 1236, Loss: 1.8267196416854858, Final Batch Loss: 0.4639780819416046\n",
      "Epoch 1237, Loss: 1.9598734080791473, Final Batch Loss: 0.5757303237915039\n",
      "Epoch 1238, Loss: 2.04954993724823, Final Batch Loss: 0.5188018679618835\n",
      "Epoch 1239, Loss: 1.8157300055027008, Final Batch Loss: 0.3775232136249542\n",
      "Epoch 1240, Loss: 1.8491714000701904, Final Batch Loss: 0.4042310118675232\n",
      "Epoch 1241, Loss: 1.8802606761455536, Final Batch Loss: 0.4372604489326477\n",
      "Epoch 1242, Loss: 1.8129660785198212, Final Batch Loss: 0.4643554091453552\n",
      "Epoch 1243, Loss: 1.9697414338588715, Final Batch Loss: 0.43633338809013367\n",
      "Epoch 1244, Loss: 1.761161983013153, Final Batch Loss: 0.3708842396736145\n",
      "Epoch 1245, Loss: 2.0135352313518524, Final Batch Loss: 0.5806517004966736\n",
      "Epoch 1246, Loss: 1.8339711427688599, Final Batch Loss: 0.4301685690879822\n",
      "Epoch 1247, Loss: 1.8896471858024597, Final Batch Loss: 0.5408053398132324\n",
      "Epoch 1248, Loss: 1.9097455739974976, Final Batch Loss: 0.39387989044189453\n",
      "Epoch 1249, Loss: 1.812987595796585, Final Batch Loss: 0.38152116537094116\n",
      "Epoch 1250, Loss: 1.7634702622890472, Final Batch Loss: 0.35923412442207336\n",
      "Epoch 1251, Loss: 1.9372231364250183, Final Batch Loss: 0.5155773758888245\n",
      "Epoch 1252, Loss: 1.9025647640228271, Final Batch Loss: 0.48288384079933167\n",
      "Epoch 1253, Loss: 1.8203302025794983, Final Batch Loss: 0.4474300444126129\n",
      "Epoch 1254, Loss: 1.9522180557250977, Final Batch Loss: 0.44619154930114746\n",
      "Epoch 1255, Loss: 1.846811205148697, Final Batch Loss: 0.4070190489292145\n",
      "Epoch 1256, Loss: 1.8736678659915924, Final Batch Loss: 0.41817811131477356\n",
      "Epoch 1257, Loss: 1.9032697677612305, Final Batch Loss: 0.4761902689933777\n",
      "Epoch 1258, Loss: 1.8299294114112854, Final Batch Loss: 0.5051806569099426\n",
      "Epoch 1259, Loss: 1.9195521473884583, Final Batch Loss: 0.42691686749458313\n",
      "Epoch 1260, Loss: 1.9617900252342224, Final Batch Loss: 0.48284924030303955\n",
      "Epoch 1261, Loss: 1.7817166149616241, Final Batch Loss: 0.31717416644096375\n",
      "Epoch 1262, Loss: 1.9003411829471588, Final Batch Loss: 0.4817624092102051\n",
      "Epoch 1263, Loss: 1.8915261626243591, Final Batch Loss: 0.4524887800216675\n",
      "Epoch 1264, Loss: 1.8602421581745148, Final Batch Loss: 0.5241649746894836\n",
      "Epoch 1265, Loss: 1.9838090240955353, Final Batch Loss: 0.4873575270175934\n",
      "Epoch 1266, Loss: 1.939229965209961, Final Batch Loss: 0.4336383044719696\n",
      "Epoch 1267, Loss: 1.9798372685909271, Final Batch Loss: 0.5104993581771851\n",
      "Epoch 1268, Loss: 1.8458295166492462, Final Batch Loss: 0.38278713822364807\n",
      "Epoch 1269, Loss: 1.756187617778778, Final Batch Loss: 0.33894696831703186\n",
      "Epoch 1270, Loss: 1.7847041189670563, Final Batch Loss: 0.4376720190048218\n",
      "Epoch 1271, Loss: 1.931517869234085, Final Batch Loss: 0.4754194915294647\n",
      "Epoch 1272, Loss: 1.9538434743881226, Final Batch Loss: 0.41066229343414307\n",
      "Epoch 1273, Loss: 2.0438744127750397, Final Batch Loss: 0.5469518899917603\n",
      "Epoch 1274, Loss: 1.8348223865032196, Final Batch Loss: 0.42863571643829346\n",
      "Epoch 1275, Loss: 1.8441885113716125, Final Batch Loss: 0.4200950264930725\n",
      "Epoch 1276, Loss: 1.919912189245224, Final Batch Loss: 0.5809159874916077\n",
      "Epoch 1277, Loss: 1.901095688343048, Final Batch Loss: 0.46400225162506104\n",
      "Epoch 1278, Loss: 1.8603443801403046, Final Batch Loss: 0.47832030057907104\n",
      "Epoch 1279, Loss: 1.9023494720458984, Final Batch Loss: 0.49856382608413696\n",
      "Epoch 1280, Loss: 1.8893480598926544, Final Batch Loss: 0.49935996532440186\n",
      "Epoch 1281, Loss: 1.8942140936851501, Final Batch Loss: 0.4843449294567108\n",
      "Epoch 1282, Loss: 1.7969520688056946, Final Batch Loss: 0.4629218280315399\n",
      "Epoch 1283, Loss: 1.8603916466236115, Final Batch Loss: 0.5390868186950684\n",
      "Epoch 1284, Loss: 1.8987316489219666, Final Batch Loss: 0.43121427297592163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1285, Loss: 1.862545907497406, Final Batch Loss: 0.494285523891449\n",
      "Epoch 1286, Loss: 1.7174806594848633, Final Batch Loss: 0.3989490568637848\n",
      "Epoch 1287, Loss: 1.8067561984062195, Final Batch Loss: 0.4295852780342102\n",
      "Epoch 1288, Loss: 1.7656505405902863, Final Batch Loss: 0.3666125535964966\n",
      "Epoch 1289, Loss: 1.7645403742790222, Final Batch Loss: 0.4357304871082306\n",
      "Epoch 1290, Loss: 1.8319518268108368, Final Batch Loss: 0.4832398593425751\n",
      "Epoch 1291, Loss: 1.8868376016616821, Final Batch Loss: 0.4811747670173645\n",
      "Epoch 1292, Loss: 1.8462828397750854, Final Batch Loss: 0.4299705922603607\n",
      "Epoch 1293, Loss: 1.9936363995075226, Final Batch Loss: 0.4140288829803467\n",
      "Epoch 1294, Loss: 1.79298996925354, Final Batch Loss: 0.3923693895339966\n",
      "Epoch 1295, Loss: 1.8617830872535706, Final Batch Loss: 0.4434145390987396\n",
      "Epoch 1296, Loss: 1.9475230276584625, Final Batch Loss: 0.4869077205657959\n",
      "Epoch 1297, Loss: 1.8363088369369507, Final Batch Loss: 0.4696185886859894\n",
      "Epoch 1298, Loss: 1.8974573612213135, Final Batch Loss: 0.44953209161758423\n",
      "Epoch 1299, Loss: 1.9015972018241882, Final Batch Loss: 0.4431949555873871\n",
      "Epoch 1300, Loss: 1.8759296834468842, Final Batch Loss: 0.4503580927848816\n",
      "Epoch 1301, Loss: 1.8629887700080872, Final Batch Loss: 0.43605390191078186\n",
      "Epoch 1302, Loss: 1.8583830296993256, Final Batch Loss: 0.5128734111785889\n",
      "Epoch 1303, Loss: 1.921355664730072, Final Batch Loss: 0.5047712922096252\n",
      "Epoch 1304, Loss: 1.9803420305252075, Final Batch Loss: 0.5083202719688416\n",
      "Epoch 1305, Loss: 1.8180832862854004, Final Batch Loss: 0.44395333528518677\n",
      "Epoch 1306, Loss: 1.9229618906974792, Final Batch Loss: 0.4592709541320801\n",
      "Epoch 1307, Loss: 1.8504973649978638, Final Batch Loss: 0.546938955783844\n",
      "Epoch 1308, Loss: 1.867468684911728, Final Batch Loss: 0.5190543532371521\n",
      "Epoch 1309, Loss: 1.728731781244278, Final Batch Loss: 0.41389626264572144\n",
      "Epoch 1310, Loss: 1.768416166305542, Final Batch Loss: 0.38394463062286377\n",
      "Epoch 1311, Loss: 1.9139790832996368, Final Batch Loss: 0.6417470574378967\n",
      "Epoch 1312, Loss: 1.6806202828884125, Final Batch Loss: 0.4235902726650238\n",
      "Epoch 1313, Loss: 1.9289424419403076, Final Batch Loss: 0.4435843527317047\n",
      "Epoch 1314, Loss: 1.8402283787727356, Final Batch Loss: 0.4563833475112915\n",
      "Epoch 1315, Loss: 1.8468929529190063, Final Batch Loss: 0.4154013991355896\n",
      "Epoch 1316, Loss: 1.8309692740440369, Final Batch Loss: 0.40163639187812805\n",
      "Epoch 1317, Loss: 1.7448331415653229, Final Batch Loss: 0.4783520996570587\n",
      "Epoch 1318, Loss: 1.8268100023269653, Final Batch Loss: 0.46493595838546753\n",
      "Epoch 1319, Loss: 1.8348254263401031, Final Batch Loss: 0.41561949253082275\n",
      "Epoch 1320, Loss: 1.868723452091217, Final Batch Loss: 0.3918988108634949\n",
      "Epoch 1321, Loss: 1.738968014717102, Final Batch Loss: 0.4251694977283478\n",
      "Epoch 1322, Loss: 1.8288640975952148, Final Batch Loss: 0.5042111873626709\n",
      "Epoch 1323, Loss: 1.8949675559997559, Final Batch Loss: 0.49266478419303894\n",
      "Epoch 1324, Loss: 1.8820367753505707, Final Batch Loss: 0.43093112111091614\n",
      "Epoch 1325, Loss: 2.0126408338546753, Final Batch Loss: 0.5919143557548523\n",
      "Epoch 1326, Loss: 1.7945827841758728, Final Batch Loss: 0.5178455114364624\n",
      "Epoch 1327, Loss: 1.7979933619499207, Final Batch Loss: 0.4405144155025482\n",
      "Epoch 1328, Loss: 1.9583168029785156, Final Batch Loss: 0.5951493382453918\n",
      "Epoch 1329, Loss: 1.9675101935863495, Final Batch Loss: 0.47266629338264465\n",
      "Epoch 1330, Loss: 1.797088623046875, Final Batch Loss: 0.40940675139427185\n",
      "Epoch 1331, Loss: 1.8716892302036285, Final Batch Loss: 0.4326912462711334\n",
      "Epoch 1332, Loss: 1.9055736362934113, Final Batch Loss: 0.4771665930747986\n",
      "Epoch 1333, Loss: 1.7826395332813263, Final Batch Loss: 0.42182424664497375\n",
      "Epoch 1334, Loss: 1.7951346337795258, Final Batch Loss: 0.4522172808647156\n",
      "Epoch 1335, Loss: 1.8268515765666962, Final Batch Loss: 0.39370250701904297\n",
      "Epoch 1336, Loss: 1.8325621783733368, Final Batch Loss: 0.433444619178772\n",
      "Epoch 1337, Loss: 1.781300276517868, Final Batch Loss: 0.3776357173919678\n",
      "Epoch 1338, Loss: 1.7363071739673615, Final Batch Loss: 0.3614986538887024\n",
      "Epoch 1339, Loss: 1.8441844284534454, Final Batch Loss: 0.5154822468757629\n",
      "Epoch 1340, Loss: 1.947205513715744, Final Batch Loss: 0.4488217830657959\n",
      "Epoch 1341, Loss: 1.9740928709506989, Final Batch Loss: 0.42406028509140015\n",
      "Epoch 1342, Loss: 1.8036703765392303, Final Batch Loss: 0.44220635294914246\n",
      "Epoch 1343, Loss: 1.8570261895656586, Final Batch Loss: 0.4618418514728546\n",
      "Epoch 1344, Loss: 1.892317771911621, Final Batch Loss: 0.4975121021270752\n",
      "Epoch 1345, Loss: 1.9551813006401062, Final Batch Loss: 0.5013746619224548\n",
      "Epoch 1346, Loss: 1.7834970653057098, Final Batch Loss: 0.3338657319545746\n",
      "Epoch 1347, Loss: 1.8300938606262207, Final Batch Loss: 0.4141940176486969\n",
      "Epoch 1348, Loss: 1.8655707836151123, Final Batch Loss: 0.4561711549758911\n",
      "Epoch 1349, Loss: 1.7796423733234406, Final Batch Loss: 0.4110773205757141\n",
      "Epoch 1350, Loss: 1.8532058596611023, Final Batch Loss: 0.4814121425151825\n",
      "Epoch 1351, Loss: 1.8047417998313904, Final Batch Loss: 0.446933776140213\n",
      "Epoch 1352, Loss: 1.7344518899917603, Final Batch Loss: 0.3900294303894043\n",
      "Epoch 1353, Loss: 2.060571312904358, Final Batch Loss: 0.5643315315246582\n",
      "Epoch 1354, Loss: 1.9023977220058441, Final Batch Loss: 0.38085439801216125\n",
      "Epoch 1355, Loss: 1.7053927183151245, Final Batch Loss: 0.39704394340515137\n",
      "Epoch 1356, Loss: 1.7989705502986908, Final Batch Loss: 0.4537492096424103\n",
      "Epoch 1357, Loss: 1.8925076127052307, Final Batch Loss: 0.5159199833869934\n",
      "Epoch 1358, Loss: 1.8559158146381378, Final Batch Loss: 0.5548983812332153\n",
      "Epoch 1359, Loss: 1.9013318419456482, Final Batch Loss: 0.49115800857543945\n",
      "Epoch 1360, Loss: 1.747721642255783, Final Batch Loss: 0.37590649724006653\n",
      "Epoch 1361, Loss: 1.8418163359165192, Final Batch Loss: 0.4700239896774292\n",
      "Epoch 1362, Loss: 1.867165982723236, Final Batch Loss: 0.5273871421813965\n",
      "Epoch 1363, Loss: 1.9180267453193665, Final Batch Loss: 0.5442532300949097\n",
      "Epoch 1364, Loss: 1.8511678278446198, Final Batch Loss: 0.46695420145988464\n",
      "Epoch 1365, Loss: 1.7347451746463776, Final Batch Loss: 0.37883931398391724\n",
      "Epoch 1366, Loss: 1.7815704643726349, Final Batch Loss: 0.4980570077896118\n",
      "Epoch 1367, Loss: 1.8133085668087006, Final Batch Loss: 0.41525959968566895\n",
      "Epoch 1368, Loss: 1.7008966505527496, Final Batch Loss: 0.36054694652557373\n",
      "Epoch 1369, Loss: 1.7660804688930511, Final Batch Loss: 0.3853949010372162\n",
      "Epoch 1370, Loss: 1.9537749886512756, Final Batch Loss: 0.5337204933166504\n",
      "Epoch 1371, Loss: 1.7912784814834595, Final Batch Loss: 0.5133997201919556\n",
      "Epoch 1372, Loss: 1.7989787459373474, Final Batch Loss: 0.46749430894851685\n",
      "Epoch 1373, Loss: 1.7776234149932861, Final Batch Loss: 0.48354750871658325\n",
      "Epoch 1374, Loss: 1.8661933243274689, Final Batch Loss: 0.5084426403045654\n",
      "Epoch 1375, Loss: 1.8957801461219788, Final Batch Loss: 0.4550835192203522\n",
      "Epoch 1376, Loss: 1.888540804386139, Final Batch Loss: 0.45400547981262207\n",
      "Epoch 1377, Loss: 1.8595849573612213, Final Batch Loss: 0.4082464575767517\n",
      "Epoch 1378, Loss: 1.9555404484272003, Final Batch Loss: 0.6261316537857056\n",
      "Epoch 1379, Loss: 1.799765706062317, Final Batch Loss: 0.4039119482040405\n",
      "Epoch 1380, Loss: 1.7331910133361816, Final Batch Loss: 0.3723360300064087\n",
      "Epoch 1381, Loss: 1.928103268146515, Final Batch Loss: 0.532878041267395\n",
      "Epoch 1382, Loss: 1.8046380877494812, Final Batch Loss: 0.3916134834289551\n",
      "Epoch 1383, Loss: 1.8274703323841095, Final Batch Loss: 0.4897024929523468\n",
      "Epoch 1384, Loss: 1.916836827993393, Final Batch Loss: 0.5199491381645203\n",
      "Epoch 1385, Loss: 1.7378243505954742, Final Batch Loss: 0.3304661512374878\n",
      "Epoch 1386, Loss: 1.9240772426128387, Final Batch Loss: 0.5465455055236816\n",
      "Epoch 1387, Loss: 1.8718202710151672, Final Batch Loss: 0.4609127342700958\n",
      "Epoch 1388, Loss: 1.8266659379005432, Final Batch Loss: 0.3644798994064331\n",
      "Epoch 1389, Loss: 1.791311264038086, Final Batch Loss: 0.3768236041069031\n",
      "Epoch 1390, Loss: 1.8701871931552887, Final Batch Loss: 0.5162797570228577\n",
      "Epoch 1391, Loss: 1.916674017906189, Final Batch Loss: 0.5818686485290527\n",
      "Epoch 1392, Loss: 1.8208774030208588, Final Batch Loss: 0.4648970365524292\n",
      "Epoch 1393, Loss: 1.9389117658138275, Final Batch Loss: 0.45265790820121765\n",
      "Epoch 1394, Loss: 1.9437236189842224, Final Batch Loss: 0.5827735662460327\n",
      "Epoch 1395, Loss: 1.8177407383918762, Final Batch Loss: 0.41560354828834534\n",
      "Epoch 1396, Loss: 1.9380794167518616, Final Batch Loss: 0.5449839234352112\n",
      "Epoch 1397, Loss: 1.8403950333595276, Final Batch Loss: 0.3653128743171692\n",
      "Epoch 1398, Loss: 1.749213308095932, Final Batch Loss: 0.33359524607658386\n",
      "Epoch 1399, Loss: 1.7515707910060883, Final Batch Loss: 0.35300013422966003\n",
      "Epoch 1400, Loss: 1.7467336654663086, Final Batch Loss: 0.4170631170272827\n",
      "Epoch 1401, Loss: 1.802026778459549, Final Batch Loss: 0.4439832866191864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1402, Loss: 1.8455019891262054, Final Batch Loss: 0.5049931406974792\n",
      "Epoch 1403, Loss: 1.7546306550502777, Final Batch Loss: 0.5270859003067017\n",
      "Epoch 1404, Loss: 1.7908675372600555, Final Batch Loss: 0.4346320927143097\n",
      "Epoch 1405, Loss: 1.84441077709198, Final Batch Loss: 0.41135266423225403\n",
      "Epoch 1406, Loss: 1.729886770248413, Final Batch Loss: 0.5664440989494324\n",
      "Epoch 1407, Loss: 1.7488067746162415, Final Batch Loss: 0.34756070375442505\n",
      "Epoch 1408, Loss: 1.734142541885376, Final Batch Loss: 0.404825896024704\n",
      "Epoch 1409, Loss: 1.8238113224506378, Final Batch Loss: 0.407977432012558\n",
      "Epoch 1410, Loss: 1.8844925463199615, Final Batch Loss: 0.43125271797180176\n",
      "Epoch 1411, Loss: 1.7895041704177856, Final Batch Loss: 0.505156934261322\n",
      "Epoch 1412, Loss: 1.784204363822937, Final Batch Loss: 0.4174387753009796\n",
      "Epoch 1413, Loss: 1.8491833209991455, Final Batch Loss: 0.5300917029380798\n",
      "Epoch 1414, Loss: 1.9168717563152313, Final Batch Loss: 0.5194451808929443\n",
      "Epoch 1415, Loss: 1.8419868052005768, Final Batch Loss: 0.46570178866386414\n",
      "Epoch 1416, Loss: 1.8964237570762634, Final Batch Loss: 0.5353183746337891\n",
      "Epoch 1417, Loss: 1.8140711784362793, Final Batch Loss: 0.4157205820083618\n",
      "Epoch 1418, Loss: 1.7582014501094818, Final Batch Loss: 0.4192807972431183\n",
      "Epoch 1419, Loss: 1.8836676180362701, Final Batch Loss: 0.6072904467582703\n",
      "Epoch 1420, Loss: 1.7962185144424438, Final Batch Loss: 0.4664948582649231\n",
      "Epoch 1421, Loss: 1.8492063581943512, Final Batch Loss: 0.4909381568431854\n",
      "Epoch 1422, Loss: 1.679821491241455, Final Batch Loss: 0.3891831934452057\n",
      "Epoch 1423, Loss: 1.8129289150238037, Final Batch Loss: 0.4787777364253998\n",
      "Epoch 1424, Loss: 1.9052379131317139, Final Batch Loss: 0.5986489057540894\n",
      "Epoch 1425, Loss: 1.7741666436195374, Final Batch Loss: 0.48616498708724976\n",
      "Epoch 1426, Loss: 1.9525566101074219, Final Batch Loss: 0.5338313579559326\n",
      "Epoch 1427, Loss: 1.8343254029750824, Final Batch Loss: 0.46059566736221313\n",
      "Epoch 1428, Loss: 1.8701611757278442, Final Batch Loss: 0.36651092767715454\n",
      "Epoch 1429, Loss: 1.9032986164093018, Final Batch Loss: 0.5410177111625671\n",
      "Epoch 1430, Loss: 1.835707575082779, Final Batch Loss: 0.4285397529602051\n",
      "Epoch 1431, Loss: 1.7459432184696198, Final Batch Loss: 0.4312628209590912\n",
      "Epoch 1432, Loss: 1.9071641862392426, Final Batch Loss: 0.5446822047233582\n",
      "Epoch 1433, Loss: 1.8147630989551544, Final Batch Loss: 0.49715545773506165\n",
      "Epoch 1434, Loss: 1.8074751496315002, Final Batch Loss: 0.42636966705322266\n",
      "Epoch 1435, Loss: 1.8197324573993683, Final Batch Loss: 0.46833932399749756\n",
      "Epoch 1436, Loss: 1.794217199087143, Final Batch Loss: 0.4464975893497467\n",
      "Epoch 1437, Loss: 1.8362817466259003, Final Batch Loss: 0.44624727964401245\n",
      "Epoch 1438, Loss: 1.7000497877597809, Final Batch Loss: 0.37638354301452637\n",
      "Epoch 1439, Loss: 1.752445787191391, Final Batch Loss: 0.4070015251636505\n",
      "Epoch 1440, Loss: 1.826228678226471, Final Batch Loss: 0.43194863200187683\n",
      "Epoch 1441, Loss: 1.8654572367668152, Final Batch Loss: 0.5030879974365234\n",
      "Epoch 1442, Loss: 1.8849384784698486, Final Batch Loss: 0.5612461566925049\n",
      "Epoch 1443, Loss: 1.792883276939392, Final Batch Loss: 0.43365180492401123\n",
      "Epoch 1444, Loss: 1.831141084432602, Final Batch Loss: 0.468504399061203\n",
      "Epoch 1445, Loss: 1.8443818986415863, Final Batch Loss: 0.5082577466964722\n",
      "Epoch 1446, Loss: 1.830001711845398, Final Batch Loss: 0.4695919156074524\n",
      "Epoch 1447, Loss: 1.9725592732429504, Final Batch Loss: 0.5981082916259766\n",
      "Epoch 1448, Loss: 1.7065893411636353, Final Batch Loss: 0.3793177008628845\n",
      "Epoch 1449, Loss: 1.804975152015686, Final Batch Loss: 0.48490726947784424\n",
      "Epoch 1450, Loss: 1.8088549375534058, Final Batch Loss: 0.5438041090965271\n",
      "Epoch 1451, Loss: 1.6962156295776367, Final Batch Loss: 0.4249401390552521\n",
      "Epoch 1452, Loss: 1.6686850786209106, Final Batch Loss: 0.45963597297668457\n",
      "Epoch 1453, Loss: 1.7813657224178314, Final Batch Loss: 0.45277640223503113\n",
      "Epoch 1454, Loss: 1.721182107925415, Final Batch Loss: 0.39019593596458435\n",
      "Epoch 1455, Loss: 1.733592450618744, Final Batch Loss: 0.42790454626083374\n",
      "Epoch 1456, Loss: 1.8203960359096527, Final Batch Loss: 0.49717244505882263\n",
      "Epoch 1457, Loss: 1.8419867753982544, Final Batch Loss: 0.5122013092041016\n",
      "Epoch 1458, Loss: 1.6904789805412292, Final Batch Loss: 0.3941345512866974\n",
      "Epoch 1459, Loss: 1.9438849091529846, Final Batch Loss: 0.4632878601551056\n",
      "Epoch 1460, Loss: 1.6993243992328644, Final Batch Loss: 0.4401046633720398\n",
      "Epoch 1461, Loss: 1.8771207630634308, Final Batch Loss: 0.4837896227836609\n",
      "Epoch 1462, Loss: 1.650153398513794, Final Batch Loss: 0.3397761285305023\n",
      "Epoch 1463, Loss: 1.7937395572662354, Final Batch Loss: 0.45396894216537476\n",
      "Epoch 1464, Loss: 1.6986512243747711, Final Batch Loss: 0.3618878424167633\n",
      "Epoch 1465, Loss: 1.6929473876953125, Final Batch Loss: 0.377244234085083\n",
      "Epoch 1466, Loss: 1.7764643728733063, Final Batch Loss: 0.4238051474094391\n",
      "Epoch 1467, Loss: 1.693305492401123, Final Batch Loss: 0.3671853542327881\n",
      "Epoch 1468, Loss: 1.7646702229976654, Final Batch Loss: 0.40992024540901184\n",
      "Epoch 1469, Loss: 1.8451925814151764, Final Batch Loss: 0.48964449763298035\n",
      "Epoch 1470, Loss: 1.7011023163795471, Final Batch Loss: 0.3544359803199768\n",
      "Epoch 1471, Loss: 1.8169476687908173, Final Batch Loss: 0.4453642666339874\n",
      "Epoch 1472, Loss: 1.8231471478939056, Final Batch Loss: 0.43686753511428833\n",
      "Epoch 1473, Loss: 1.8084802031517029, Final Batch Loss: 0.5139098167419434\n",
      "Epoch 1474, Loss: 1.6986947655677795, Final Batch Loss: 0.4122854769229889\n",
      "Epoch 1475, Loss: 1.806926429271698, Final Batch Loss: 0.5155243873596191\n",
      "Epoch 1476, Loss: 1.7558308839797974, Final Batch Loss: 0.4504677355289459\n",
      "Epoch 1477, Loss: 1.6589430570602417, Final Batch Loss: 0.42919614911079407\n",
      "Epoch 1478, Loss: 1.9743144512176514, Final Batch Loss: 0.4505091607570648\n",
      "Epoch 1479, Loss: 1.7586280405521393, Final Batch Loss: 0.3588455617427826\n",
      "Epoch 1480, Loss: 1.7507971823215485, Final Batch Loss: 0.42456188797950745\n",
      "Epoch 1481, Loss: 1.8140568733215332, Final Batch Loss: 0.5064273476600647\n",
      "Epoch 1482, Loss: 1.7460204362869263, Final Batch Loss: 0.4164898097515106\n",
      "Epoch 1483, Loss: 1.8270207941532135, Final Batch Loss: 0.44588983058929443\n",
      "Epoch 1484, Loss: 1.8504475355148315, Final Batch Loss: 0.41276028752326965\n",
      "Epoch 1485, Loss: 1.7671934068202972, Final Batch Loss: 0.4248403012752533\n",
      "Epoch 1486, Loss: 1.671453207731247, Final Batch Loss: 0.3692184090614319\n",
      "Epoch 1487, Loss: 1.7882708609104156, Final Batch Loss: 0.4294716417789459\n",
      "Epoch 1488, Loss: 1.7260889410972595, Final Batch Loss: 0.3764828145503998\n",
      "Epoch 1489, Loss: 1.695682168006897, Final Batch Loss: 0.3972402513027191\n",
      "Epoch 1490, Loss: 1.7742183208465576, Final Batch Loss: 0.3960860073566437\n",
      "Epoch 1491, Loss: 1.8278296291828156, Final Batch Loss: 0.40285760164260864\n",
      "Epoch 1492, Loss: 1.782397985458374, Final Batch Loss: 0.48667699098587036\n",
      "Epoch 1493, Loss: 1.7229437232017517, Final Batch Loss: 0.4735278785228729\n",
      "Epoch 1494, Loss: 1.802051067352295, Final Batch Loss: 0.45863890647888184\n",
      "Epoch 1495, Loss: 1.8502054512500763, Final Batch Loss: 0.47693413496017456\n",
      "Epoch 1496, Loss: 1.6750126481056213, Final Batch Loss: 0.34671151638031006\n",
      "Epoch 1497, Loss: 1.763217955827713, Final Batch Loss: 0.49609628319740295\n",
      "Epoch 1498, Loss: 1.7713288366794586, Final Batch Loss: 0.4190419018268585\n",
      "Epoch 1499, Loss: 1.9200687110424042, Final Batch Loss: 0.39870116114616394\n",
      "Epoch 1500, Loss: 1.8678650856018066, Final Batch Loss: 0.4279453456401825\n",
      "Epoch 1501, Loss: 1.8876128792762756, Final Batch Loss: 0.5585370659828186\n",
      "Epoch 1502, Loss: 1.844466894865036, Final Batch Loss: 0.4615667760372162\n",
      "Epoch 1503, Loss: 1.8416946232318878, Final Batch Loss: 0.5008990168571472\n",
      "Epoch 1504, Loss: 1.7462655007839203, Final Batch Loss: 0.3370678424835205\n",
      "Epoch 1505, Loss: 1.7370043396949768, Final Batch Loss: 0.4751371145248413\n",
      "Epoch 1506, Loss: 1.779787689447403, Final Batch Loss: 0.42212623357772827\n",
      "Epoch 1507, Loss: 1.9031093418598175, Final Batch Loss: 0.5235047936439514\n",
      "Epoch 1508, Loss: 1.6806879341602325, Final Batch Loss: 0.38088005781173706\n",
      "Epoch 1509, Loss: 1.8317729830741882, Final Batch Loss: 0.393157958984375\n",
      "Epoch 1510, Loss: 1.772143840789795, Final Batch Loss: 0.5045667290687561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1511, Loss: 1.7636238634586334, Final Batch Loss: 0.45272064208984375\n",
      "Epoch 1512, Loss: 1.8328256011009216, Final Batch Loss: 0.5268781781196594\n",
      "Epoch 1513, Loss: 1.7643032371997833, Final Batch Loss: 0.41842272877693176\n",
      "Epoch 1514, Loss: 1.8710162341594696, Final Batch Loss: 0.5255337953567505\n",
      "Epoch 1515, Loss: 1.7715863287448883, Final Batch Loss: 0.4133973717689514\n",
      "Epoch 1516, Loss: 1.7636176943778992, Final Batch Loss: 0.41779088973999023\n",
      "Epoch 1517, Loss: 1.7118078470230103, Final Batch Loss: 0.37142232060432434\n",
      "Epoch 1518, Loss: 1.6749398410320282, Final Batch Loss: 0.35890960693359375\n",
      "Epoch 1519, Loss: 1.734272986650467, Final Batch Loss: 0.36507648229599\n",
      "Epoch 1520, Loss: 1.7895117402076721, Final Batch Loss: 0.3875536024570465\n",
      "Epoch 1521, Loss: 1.7282978296279907, Final Batch Loss: 0.4350650906562805\n",
      "Epoch 1522, Loss: 1.8681477308273315, Final Batch Loss: 0.570939302444458\n",
      "Epoch 1523, Loss: 1.7674308121204376, Final Batch Loss: 0.4293670952320099\n",
      "Epoch 1524, Loss: 1.8391223847866058, Final Batch Loss: 0.43313243985176086\n",
      "Epoch 1525, Loss: 1.9427123069763184, Final Batch Loss: 0.544580340385437\n",
      "Epoch 1526, Loss: 1.6753592789173126, Final Batch Loss: 0.3964258134365082\n",
      "Epoch 1527, Loss: 1.7926966547966003, Final Batch Loss: 0.437044620513916\n",
      "Epoch 1528, Loss: 1.8187407851219177, Final Batch Loss: 0.49889492988586426\n",
      "Epoch 1529, Loss: 1.7559155821800232, Final Batch Loss: 0.41198864579200745\n",
      "Epoch 1530, Loss: 1.7402801215648651, Final Batch Loss: 0.4109930396080017\n",
      "Epoch 1531, Loss: 1.8166540563106537, Final Batch Loss: 0.5507568120956421\n",
      "Epoch 1532, Loss: 1.7418068051338196, Final Batch Loss: 0.43543165922164917\n",
      "Epoch 1533, Loss: 1.723524570465088, Final Batch Loss: 0.41553714871406555\n",
      "Epoch 1534, Loss: 1.7862940430641174, Final Batch Loss: 0.4312538802623749\n",
      "Epoch 1535, Loss: 1.725336730480194, Final Batch Loss: 0.47971078753471375\n",
      "Epoch 1536, Loss: 1.7047737836837769, Final Batch Loss: 0.4138091504573822\n",
      "Epoch 1537, Loss: 1.7341858744621277, Final Batch Loss: 0.3736962378025055\n",
      "Epoch 1538, Loss: 1.646081030368805, Final Batch Loss: 0.33059951663017273\n",
      "Epoch 1539, Loss: 1.7675184309482574, Final Batch Loss: 0.3945561349391937\n",
      "Epoch 1540, Loss: 1.7835603058338165, Final Batch Loss: 0.5752004981040955\n",
      "Epoch 1541, Loss: 1.828303962945938, Final Batch Loss: 0.3692915439605713\n",
      "Epoch 1542, Loss: 1.645458698272705, Final Batch Loss: 0.36149147152900696\n",
      "Epoch 1543, Loss: 1.7042916417121887, Final Batch Loss: 0.36947113275527954\n",
      "Epoch 1544, Loss: 1.7416945695877075, Final Batch Loss: 0.4350166618824005\n",
      "Epoch 1545, Loss: 1.8019936084747314, Final Batch Loss: 0.5127193331718445\n",
      "Epoch 1546, Loss: 1.7133334875106812, Final Batch Loss: 0.3586490750312805\n",
      "Epoch 1547, Loss: 1.728384792804718, Final Batch Loss: 0.5307924151420593\n",
      "Epoch 1548, Loss: 1.6899094581604004, Final Batch Loss: 0.36387473344802856\n",
      "Epoch 1549, Loss: 1.9152998328208923, Final Batch Loss: 0.6183491349220276\n",
      "Epoch 1550, Loss: 1.7983454167842865, Final Batch Loss: 0.5300628542900085\n",
      "Epoch 1551, Loss: 1.6773678958415985, Final Batch Loss: 0.40593376755714417\n",
      "Epoch 1552, Loss: 1.7872949540615082, Final Batch Loss: 0.47705093026161194\n",
      "Epoch 1553, Loss: 1.800111085176468, Final Batch Loss: 0.4333861470222473\n",
      "Epoch 1554, Loss: 1.754336178302765, Final Batch Loss: 0.5026506185531616\n",
      "Epoch 1555, Loss: 1.6658777594566345, Final Batch Loss: 0.37524887919425964\n",
      "Epoch 1556, Loss: 1.638426423072815, Final Batch Loss: 0.3988797068595886\n",
      "Epoch 1557, Loss: 1.7304252088069916, Final Batch Loss: 0.4034700095653534\n",
      "Epoch 1558, Loss: 1.7435813546180725, Final Batch Loss: 0.4216020107269287\n",
      "Epoch 1559, Loss: 1.8446734547615051, Final Batch Loss: 0.4415489137172699\n",
      "Epoch 1560, Loss: 1.7247630655765533, Final Batch Loss: 0.45316997170448303\n",
      "Epoch 1561, Loss: 1.7961662411689758, Final Batch Loss: 0.4843512177467346\n",
      "Epoch 1562, Loss: 1.776755839586258, Final Batch Loss: 0.5300456285476685\n",
      "Epoch 1563, Loss: 1.7508727312088013, Final Batch Loss: 0.29989689588546753\n",
      "Epoch 1564, Loss: 1.755740761756897, Final Batch Loss: 0.40281713008880615\n",
      "Epoch 1565, Loss: 1.759400725364685, Final Batch Loss: 0.4290713965892792\n",
      "Epoch 1566, Loss: 1.783182293176651, Final Batch Loss: 0.41048938035964966\n",
      "Epoch 1567, Loss: 1.9461518824100494, Final Batch Loss: 0.5754019021987915\n",
      "Epoch 1568, Loss: 1.7356323897838593, Final Batch Loss: 0.45626211166381836\n",
      "Epoch 1569, Loss: 1.7989339232444763, Final Batch Loss: 0.5338968634605408\n",
      "Epoch 1570, Loss: 1.848396360874176, Final Batch Loss: 0.453685998916626\n",
      "Epoch 1571, Loss: 1.7529408931732178, Final Batch Loss: 0.39459189772605896\n",
      "Epoch 1572, Loss: 1.6560816764831543, Final Batch Loss: 0.44237977266311646\n",
      "Epoch 1573, Loss: 1.663334608078003, Final Batch Loss: 0.42794114351272583\n",
      "Epoch 1574, Loss: 1.7438296675682068, Final Batch Loss: 0.40324071049690247\n",
      "Epoch 1575, Loss: 1.7382631301879883, Final Batch Loss: 0.5250034332275391\n",
      "Epoch 1576, Loss: 1.886262059211731, Final Batch Loss: 0.5810139179229736\n",
      "Epoch 1577, Loss: 1.8219696581363678, Final Batch Loss: 0.38959401845932007\n",
      "Epoch 1578, Loss: 1.7629568576812744, Final Batch Loss: 0.4449453353881836\n",
      "Epoch 1579, Loss: 1.7905649840831757, Final Batch Loss: 0.4863947629928589\n",
      "Epoch 1580, Loss: 1.7443098723888397, Final Batch Loss: 0.44252920150756836\n",
      "Epoch 1581, Loss: 1.75096195936203, Final Batch Loss: 0.36535993218421936\n",
      "Epoch 1582, Loss: 1.77153480052948, Final Batch Loss: 0.44583234190940857\n",
      "Epoch 1583, Loss: 1.763814628124237, Final Batch Loss: 0.4167308211326599\n",
      "Epoch 1584, Loss: 1.9097571671009064, Final Batch Loss: 0.6079922914505005\n",
      "Epoch 1585, Loss: 1.8123926520347595, Final Batch Loss: 0.5484236478805542\n",
      "Epoch 1586, Loss: 1.7323311865329742, Final Batch Loss: 0.47320589423179626\n",
      "Epoch 1587, Loss: 1.6467275023460388, Final Batch Loss: 0.3850022852420807\n",
      "Epoch 1588, Loss: 1.7653739154338837, Final Batch Loss: 0.4407580494880676\n",
      "Epoch 1589, Loss: 1.8164953887462616, Final Batch Loss: 0.4810660183429718\n",
      "Epoch 1590, Loss: 1.798875093460083, Final Batch Loss: 0.3958839476108551\n",
      "Epoch 1591, Loss: 1.7729756832122803, Final Batch Loss: 0.41935214400291443\n",
      "Epoch 1592, Loss: 1.6427787244319916, Final Batch Loss: 0.4289860725402832\n",
      "Epoch 1593, Loss: 1.7077657878398895, Final Batch Loss: 0.37179121375083923\n",
      "Epoch 1594, Loss: 1.8218410313129425, Final Batch Loss: 0.5036410093307495\n",
      "Epoch 1595, Loss: 1.760005235671997, Final Batch Loss: 0.4319893419742584\n",
      "Epoch 1596, Loss: 1.8026142418384552, Final Batch Loss: 0.3094791769981384\n",
      "Epoch 1597, Loss: 1.7670831978321075, Final Batch Loss: 0.4729214906692505\n",
      "Epoch 1598, Loss: 1.7346375584602356, Final Batch Loss: 0.36313119530677795\n",
      "Epoch 1599, Loss: 1.7726329565048218, Final Batch Loss: 0.4056304395198822\n",
      "Epoch 1600, Loss: 1.6013272106647491, Final Batch Loss: 0.25901609659194946\n",
      "Epoch 1601, Loss: 1.642490804195404, Final Batch Loss: 0.3734337091445923\n",
      "Epoch 1602, Loss: 1.7176455855369568, Final Batch Loss: 0.34362760186195374\n",
      "Epoch 1603, Loss: 1.7167706787586212, Final Batch Loss: 0.4399976432323456\n",
      "Epoch 1604, Loss: 1.7159244418144226, Final Batch Loss: 0.46086278557777405\n",
      "Epoch 1605, Loss: 1.6598051488399506, Final Batch Loss: 0.4321557879447937\n",
      "Epoch 1606, Loss: 1.7098240852355957, Final Batch Loss: 0.3995212912559509\n",
      "Epoch 1607, Loss: 1.7212844491004944, Final Batch Loss: 0.4246464669704437\n",
      "Epoch 1608, Loss: 1.6378010213375092, Final Batch Loss: 0.3188171982765198\n",
      "Epoch 1609, Loss: 1.9055332243442535, Final Batch Loss: 0.589460551738739\n",
      "Epoch 1610, Loss: 1.791657954454422, Final Batch Loss: 0.4822767674922943\n",
      "Epoch 1611, Loss: 1.7879202961921692, Final Batch Loss: 0.4539254903793335\n",
      "Epoch 1612, Loss: 1.7823674976825714, Final Batch Loss: 0.3264791965484619\n",
      "Epoch 1613, Loss: 1.862759292125702, Final Batch Loss: 0.4203380048274994\n",
      "Epoch 1614, Loss: 1.7785362005233765, Final Batch Loss: 0.5736702084541321\n",
      "Epoch 1615, Loss: 1.7210160791873932, Final Batch Loss: 0.4452548027038574\n",
      "Epoch 1616, Loss: 1.7946247458457947, Final Batch Loss: 0.43594565987586975\n",
      "Epoch 1617, Loss: 1.6984882950782776, Final Batch Loss: 0.39597463607788086\n",
      "Epoch 1618, Loss: 1.705511599779129, Final Batch Loss: 0.42714518308639526\n",
      "Epoch 1619, Loss: 1.7684795260429382, Final Batch Loss: 0.5285621881484985\n",
      "Epoch 1620, Loss: 1.7181717455387115, Final Batch Loss: 0.4208683669567108\n",
      "Epoch 1621, Loss: 1.8420979678630829, Final Batch Loss: 0.4476480185985565\n",
      "Epoch 1622, Loss: 1.7268424928188324, Final Batch Loss: 0.4233759939670563\n",
      "Epoch 1623, Loss: 1.7449085712432861, Final Batch Loss: 0.41103798151016235\n",
      "Epoch 1624, Loss: 1.7156670987606049, Final Batch Loss: 0.4623086452484131\n",
      "Epoch 1625, Loss: 1.6792054772377014, Final Batch Loss: 0.42505010962486267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1626, Loss: 1.7869544923305511, Final Batch Loss: 0.4252254068851471\n",
      "Epoch 1627, Loss: 1.757333129644394, Final Batch Loss: 0.4272787570953369\n",
      "Epoch 1628, Loss: 1.7407453656196594, Final Batch Loss: 0.5051470398902893\n",
      "Epoch 1629, Loss: 1.739307701587677, Final Batch Loss: 0.4147215783596039\n",
      "Epoch 1630, Loss: 1.7192217111587524, Final Batch Loss: 0.3974978029727936\n",
      "Epoch 1631, Loss: 1.697090983390808, Final Batch Loss: 0.4209577739238739\n",
      "Epoch 1632, Loss: 1.7204559445381165, Final Batch Loss: 0.43938642740249634\n",
      "Epoch 1633, Loss: 1.8088386952877045, Final Batch Loss: 0.5316192507743835\n",
      "Epoch 1634, Loss: 1.7767020165920258, Final Batch Loss: 0.4373907744884491\n",
      "Epoch 1635, Loss: 1.661336213350296, Final Batch Loss: 0.38996055722236633\n",
      "Epoch 1636, Loss: 1.736211746931076, Final Batch Loss: 0.38086169958114624\n",
      "Epoch 1637, Loss: 1.8512109518051147, Final Batch Loss: 0.4869069457054138\n",
      "Epoch 1638, Loss: 1.7684390544891357, Final Batch Loss: 0.4600040316581726\n",
      "Epoch 1639, Loss: 1.617161899805069, Final Batch Loss: 0.3782760202884674\n",
      "Epoch 1640, Loss: 1.7407445311546326, Final Batch Loss: 0.4106166362762451\n",
      "Epoch 1641, Loss: 1.7818570137023926, Final Batch Loss: 0.38014382123947144\n",
      "Epoch 1642, Loss: 1.7270134389400482, Final Batch Loss: 0.4252099096775055\n",
      "Epoch 1643, Loss: 1.6603755354881287, Final Batch Loss: 0.3317028880119324\n",
      "Epoch 1644, Loss: 1.6907801926136017, Final Batch Loss: 0.40197888016700745\n",
      "Epoch 1645, Loss: 1.7221408784389496, Final Batch Loss: 0.3503117859363556\n",
      "Epoch 1646, Loss: 1.767707645893097, Final Batch Loss: 0.4615819454193115\n",
      "Epoch 1647, Loss: 1.7692411541938782, Final Batch Loss: 0.37716129422187805\n",
      "Epoch 1648, Loss: 1.6800253093242645, Final Batch Loss: 0.4059918224811554\n",
      "Epoch 1649, Loss: 1.6279682517051697, Final Batch Loss: 0.4467330574989319\n",
      "Epoch 1650, Loss: 1.7770503163337708, Final Batch Loss: 0.44947803020477295\n",
      "Epoch 1651, Loss: 1.7338217496871948, Final Batch Loss: 0.4485803246498108\n",
      "Epoch 1652, Loss: 1.7117825746536255, Final Batch Loss: 0.3729667365550995\n",
      "Epoch 1653, Loss: 1.6918657422065735, Final Batch Loss: 0.44666948914527893\n",
      "Epoch 1654, Loss: 1.6343379616737366, Final Batch Loss: 0.44264987111091614\n",
      "Epoch 1655, Loss: 1.7617545425891876, Final Batch Loss: 0.35763993859291077\n",
      "Epoch 1656, Loss: 1.6660644710063934, Final Batch Loss: 0.36794716119766235\n",
      "Epoch 1657, Loss: 1.6253346502780914, Final Batch Loss: 0.42143958806991577\n",
      "Epoch 1658, Loss: 1.774792104959488, Final Batch Loss: 0.4163298010826111\n",
      "Epoch 1659, Loss: 1.8068915605545044, Final Batch Loss: 0.47935062646865845\n",
      "Epoch 1660, Loss: 1.7730461955070496, Final Batch Loss: 0.4123944640159607\n",
      "Epoch 1661, Loss: 1.771054983139038, Final Batch Loss: 0.5193294286727905\n",
      "Epoch 1662, Loss: 1.6238852143287659, Final Batch Loss: 0.3013586401939392\n",
      "Epoch 1663, Loss: 1.6674983501434326, Final Batch Loss: 0.45658308267593384\n",
      "Epoch 1664, Loss: 1.7325924038887024, Final Batch Loss: 0.5148322582244873\n",
      "Epoch 1665, Loss: 1.7426191866397858, Final Batch Loss: 0.4629248380661011\n",
      "Epoch 1666, Loss: 1.682738333940506, Final Batch Loss: 0.4412134885787964\n",
      "Epoch 1667, Loss: 1.7630735039710999, Final Batch Loss: 0.43161657452583313\n",
      "Epoch 1668, Loss: 1.7756072878837585, Final Batch Loss: 0.4530559778213501\n",
      "Epoch 1669, Loss: 1.6578726470470428, Final Batch Loss: 0.3410789966583252\n",
      "Epoch 1670, Loss: 1.6490617990493774, Final Batch Loss: 0.38097426295280457\n",
      "Epoch 1671, Loss: 1.6369050741195679, Final Batch Loss: 0.37354370951652527\n",
      "Epoch 1672, Loss: 1.7846045196056366, Final Batch Loss: 0.44343680143356323\n",
      "Epoch 1673, Loss: 1.812951773405075, Final Batch Loss: 0.5103306770324707\n",
      "Epoch 1674, Loss: 1.8701664209365845, Final Batch Loss: 0.5784196853637695\n",
      "Epoch 1675, Loss: 1.7800942063331604, Final Batch Loss: 0.5445178151130676\n",
      "Epoch 1676, Loss: 1.6529005765914917, Final Batch Loss: 0.4151388108730316\n",
      "Epoch 1677, Loss: 1.7612026631832123, Final Batch Loss: 0.4801042377948761\n",
      "Epoch 1678, Loss: 1.7895247638225555, Final Batch Loss: 0.4482947587966919\n",
      "Epoch 1679, Loss: 1.7934381663799286, Final Batch Loss: 0.43316879868507385\n",
      "Epoch 1680, Loss: 1.8761935830116272, Final Batch Loss: 0.5930209755897522\n",
      "Epoch 1681, Loss: 1.8125196695327759, Final Batch Loss: 0.5139830708503723\n",
      "Epoch 1682, Loss: 1.7674607038497925, Final Batch Loss: 0.5215234160423279\n",
      "Epoch 1683, Loss: 1.7452883422374725, Final Batch Loss: 0.40587130188941956\n",
      "Epoch 1684, Loss: 1.786841094493866, Final Batch Loss: 0.4552652835845947\n",
      "Epoch 1685, Loss: 1.8977494835853577, Final Batch Loss: 0.46993234753608704\n",
      "Epoch 1686, Loss: 1.8225443065166473, Final Batch Loss: 0.5284168720245361\n",
      "Epoch 1687, Loss: 1.7451810240745544, Final Batch Loss: 0.42223218083381653\n",
      "Epoch 1688, Loss: 1.7570706903934479, Final Batch Loss: 0.4280846416950226\n",
      "Epoch 1689, Loss: 1.6347803473472595, Final Batch Loss: 0.44185736775398254\n",
      "Epoch 1690, Loss: 1.7423612773418427, Final Batch Loss: 0.4051780104637146\n",
      "Epoch 1691, Loss: 1.8154001832008362, Final Batch Loss: 0.5106277465820312\n",
      "Epoch 1692, Loss: 1.7605113089084625, Final Batch Loss: 0.44229334592819214\n",
      "Epoch 1693, Loss: 1.8292571306228638, Final Batch Loss: 0.4292731285095215\n",
      "Epoch 1694, Loss: 1.7173820734024048, Final Batch Loss: 0.36897504329681396\n",
      "Epoch 1695, Loss: 1.6374945938587189, Final Batch Loss: 0.40624892711639404\n",
      "Epoch 1696, Loss: 1.746086061000824, Final Batch Loss: 0.38981038331985474\n",
      "Epoch 1697, Loss: 1.7577252686023712, Final Batch Loss: 0.361113965511322\n",
      "Epoch 1698, Loss: 1.7825354933738708, Final Batch Loss: 0.43304869532585144\n",
      "Epoch 1699, Loss: 1.7886016368865967, Final Batch Loss: 0.44281890988349915\n",
      "Epoch 1700, Loss: 1.6751481294631958, Final Batch Loss: 0.389488160610199\n",
      "Epoch 1701, Loss: 1.7579647898674011, Final Batch Loss: 0.42719319462776184\n",
      "Epoch 1702, Loss: 1.6480375528335571, Final Batch Loss: 0.4250374734401703\n",
      "Epoch 1703, Loss: 1.6922904551029205, Final Batch Loss: 0.4537649154663086\n",
      "Epoch 1704, Loss: 1.7307828962802887, Final Batch Loss: 0.4935281574726105\n",
      "Epoch 1705, Loss: 1.684865653514862, Final Batch Loss: 0.42520344257354736\n",
      "Epoch 1706, Loss: 1.7808932662010193, Final Batch Loss: 0.5145382285118103\n",
      "Epoch 1707, Loss: 1.6441704034805298, Final Batch Loss: 0.36956608295440674\n",
      "Epoch 1708, Loss: 1.75479656457901, Final Batch Loss: 0.4253242611885071\n",
      "Epoch 1709, Loss: 1.593931257724762, Final Batch Loss: 0.39718177914619446\n",
      "Epoch 1710, Loss: 1.7577318251132965, Final Batch Loss: 0.4914894104003906\n",
      "Epoch 1711, Loss: 1.7633476853370667, Final Batch Loss: 0.4894764721393585\n",
      "Epoch 1712, Loss: 1.6484302878379822, Final Batch Loss: 0.4307488203048706\n",
      "Epoch 1713, Loss: 1.8081766963005066, Final Batch Loss: 0.4121793210506439\n",
      "Epoch 1714, Loss: 1.5610443949699402, Final Batch Loss: 0.29852494597435\n",
      "Epoch 1715, Loss: 1.61992946267128, Final Batch Loss: 0.34264853596687317\n",
      "Epoch 1716, Loss: 1.6794354617595673, Final Batch Loss: 0.36591240763664246\n",
      "Epoch 1717, Loss: 1.6871744096279144, Final Batch Loss: 0.46318915486335754\n",
      "Epoch 1718, Loss: 1.6646989583969116, Final Batch Loss: 0.41323021054267883\n",
      "Epoch 1719, Loss: 1.7039454281330109, Final Batch Loss: 0.3939321041107178\n",
      "Epoch 1720, Loss: 1.6813978850841522, Final Batch Loss: 0.39952903985977173\n",
      "Epoch 1721, Loss: 1.7371435165405273, Final Batch Loss: 0.4659859836101532\n",
      "Epoch 1722, Loss: 1.700658142566681, Final Batch Loss: 0.4220515787601471\n",
      "Epoch 1723, Loss: 1.5931894481182098, Final Batch Loss: 0.3562310039997101\n",
      "Epoch 1724, Loss: 1.6808651387691498, Final Batch Loss: 0.3826398551464081\n",
      "Epoch 1725, Loss: 1.710972636938095, Final Batch Loss: 0.3635607659816742\n",
      "Epoch 1726, Loss: 1.757412999868393, Final Batch Loss: 0.4221664071083069\n",
      "Epoch 1727, Loss: 1.6169837713241577, Final Batch Loss: 0.3836103677749634\n",
      "Epoch 1728, Loss: 1.7461647987365723, Final Batch Loss: 0.4155997037887573\n",
      "Epoch 1729, Loss: 1.6251814365386963, Final Batch Loss: 0.3695116341114044\n",
      "Epoch 1730, Loss: 1.6411614120006561, Final Batch Loss: 0.3708518445491791\n",
      "Epoch 1731, Loss: 1.7437295317649841, Final Batch Loss: 0.46691882610321045\n",
      "Epoch 1732, Loss: 1.6607138514518738, Final Batch Loss: 0.379235178232193\n",
      "Epoch 1733, Loss: 1.5810002982616425, Final Batch Loss: 0.39308416843414307\n",
      "Epoch 1734, Loss: 1.6527342796325684, Final Batch Loss: 0.3854646682739258\n",
      "Epoch 1735, Loss: 1.6811492443084717, Final Batch Loss: 0.35635438561439514\n",
      "Epoch 1736, Loss: 1.74570232629776, Final Batch Loss: 0.3912109434604645\n",
      "Epoch 1737, Loss: 1.9896275699138641, Final Batch Loss: 0.5089484453201294\n",
      "Epoch 1738, Loss: 1.7514943182468414, Final Batch Loss: 0.45168909430503845\n",
      "Epoch 1739, Loss: 1.6943585872650146, Final Batch Loss: 0.4456329941749573\n",
      "Epoch 1740, Loss: 1.6339620053768158, Final Batch Loss: 0.37906500697135925\n",
      "Epoch 1741, Loss: 1.6888539791107178, Final Batch Loss: 0.4864042401313782\n",
      "Epoch 1742, Loss: 1.8472244441509247, Final Batch Loss: 0.5174327492713928\n",
      "Epoch 1743, Loss: 1.714450478553772, Final Batch Loss: 0.41101667284965515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1744, Loss: 1.7010199129581451, Final Batch Loss: 0.4387483596801758\n",
      "Epoch 1745, Loss: 1.6098664104938507, Final Batch Loss: 0.39049118757247925\n",
      "Epoch 1746, Loss: 1.6759857535362244, Final Batch Loss: 0.3414548337459564\n",
      "Epoch 1747, Loss: 1.8788666427135468, Final Batch Loss: 0.5100138187408447\n",
      "Epoch 1748, Loss: 1.656616598367691, Final Batch Loss: 0.3837774395942688\n",
      "Epoch 1749, Loss: 1.7655797898769379, Final Batch Loss: 0.42224010825157166\n",
      "Epoch 1750, Loss: 1.6570378839969635, Final Batch Loss: 0.40109866857528687\n",
      "Epoch 1751, Loss: 1.6275098621845245, Final Batch Loss: 0.3736213445663452\n",
      "Epoch 1752, Loss: 1.657432496547699, Final Batch Loss: 0.35513418912887573\n",
      "Epoch 1753, Loss: 1.696018397808075, Final Batch Loss: 0.4099861979484558\n",
      "Epoch 1754, Loss: 1.8397967517375946, Final Batch Loss: 0.49695438146591187\n",
      "Epoch 1755, Loss: 1.7248869836330414, Final Batch Loss: 0.4210807681083679\n",
      "Epoch 1756, Loss: 1.6876031756401062, Final Batch Loss: 0.37867283821105957\n",
      "Epoch 1757, Loss: 1.5621234774589539, Final Batch Loss: 0.2877798080444336\n",
      "Epoch 1758, Loss: 1.6903604865074158, Final Batch Loss: 0.440534770488739\n",
      "Epoch 1759, Loss: 1.8134831190109253, Final Batch Loss: 0.5551385283470154\n",
      "Epoch 1760, Loss: 1.8039957582950592, Final Batch Loss: 0.5272504687309265\n",
      "Epoch 1761, Loss: 1.866307646036148, Final Batch Loss: 0.5732184052467346\n",
      "Epoch 1762, Loss: 1.7642060220241547, Final Batch Loss: 0.39830532670021057\n",
      "Epoch 1763, Loss: 1.688087373971939, Final Batch Loss: 0.4339841604232788\n",
      "Epoch 1764, Loss: 1.7306428253650665, Final Batch Loss: 0.43015092611312866\n",
      "Epoch 1765, Loss: 1.7008253037929535, Final Batch Loss: 0.4508562386035919\n",
      "Epoch 1766, Loss: 1.601393610239029, Final Batch Loss: 0.37931448221206665\n",
      "Epoch 1767, Loss: 1.661097139120102, Final Batch Loss: 0.38131868839263916\n",
      "Epoch 1768, Loss: 1.8216702342033386, Final Batch Loss: 0.5466814637184143\n",
      "Epoch 1769, Loss: 1.6851272583007812, Final Batch Loss: 0.44209858775138855\n",
      "Epoch 1770, Loss: 1.5504698157310486, Final Batch Loss: 0.3899870812892914\n",
      "Epoch 1771, Loss: 1.716287225484848, Final Batch Loss: 0.40144404768943787\n",
      "Epoch 1772, Loss: 1.6054587364196777, Final Batch Loss: 0.29939085245132446\n",
      "Epoch 1773, Loss: 1.6527117490768433, Final Batch Loss: 0.41934025287628174\n",
      "Epoch 1774, Loss: 1.7429023087024689, Final Batch Loss: 0.4746786952018738\n",
      "Epoch 1775, Loss: 1.666696161031723, Final Batch Loss: 0.3976410925388336\n",
      "Epoch 1776, Loss: 1.7790029346942902, Final Batch Loss: 0.5052313208580017\n",
      "Epoch 1777, Loss: 1.6372767388820648, Final Batch Loss: 0.36978697776794434\n",
      "Epoch 1778, Loss: 1.8223932981491089, Final Batch Loss: 0.5473359227180481\n",
      "Epoch 1779, Loss: 1.739864557981491, Final Batch Loss: 0.4393758475780487\n",
      "Epoch 1780, Loss: 1.7196039855480194, Final Batch Loss: 0.4218735694885254\n",
      "Epoch 1781, Loss: 1.711143970489502, Final Batch Loss: 0.49208104610443115\n",
      "Epoch 1782, Loss: 1.798502653837204, Final Batch Loss: 0.5016816854476929\n",
      "Epoch 1783, Loss: 1.915034830570221, Final Batch Loss: 0.6265955567359924\n",
      "Epoch 1784, Loss: 1.6569431126117706, Final Batch Loss: 0.31093016266822815\n",
      "Epoch 1785, Loss: 1.6947410702705383, Final Batch Loss: 0.4714106023311615\n",
      "Epoch 1786, Loss: 1.7454874217510223, Final Batch Loss: 0.4352903962135315\n",
      "Epoch 1787, Loss: 1.8201183080673218, Final Batch Loss: 0.46028217673301697\n",
      "Epoch 1788, Loss: 1.7705498933792114, Final Batch Loss: 0.5117324590682983\n",
      "Epoch 1789, Loss: 1.792623519897461, Final Batch Loss: 0.4807203412055969\n",
      "Epoch 1790, Loss: 1.6474953293800354, Final Batch Loss: 0.3587441146373749\n",
      "Epoch 1791, Loss: 1.6766386330127716, Final Batch Loss: 0.4233381152153015\n",
      "Epoch 1792, Loss: 1.7041045129299164, Final Batch Loss: 0.4227685034275055\n",
      "Epoch 1793, Loss: 1.6848458051681519, Final Batch Loss: 0.40635472536087036\n",
      "Epoch 1794, Loss: 1.777136355638504, Final Batch Loss: 0.5495572090148926\n",
      "Epoch 1795, Loss: 1.6784485876560211, Final Batch Loss: 0.3889223635196686\n",
      "Epoch 1796, Loss: 1.5608211159706116, Final Batch Loss: 0.31153377890586853\n",
      "Epoch 1797, Loss: 1.578378140926361, Final Batch Loss: 0.3938022255897522\n",
      "Epoch 1798, Loss: 1.6397994458675385, Final Batch Loss: 0.38165417313575745\n",
      "Epoch 1799, Loss: 1.611435204744339, Final Batch Loss: 0.31830793619155884\n",
      "Epoch 1800, Loss: 1.6408254206180573, Final Batch Loss: 0.388720840215683\n",
      "Epoch 1801, Loss: 1.6475608944892883, Final Batch Loss: 0.43286964297294617\n",
      "Epoch 1802, Loss: 1.6767998039722443, Final Batch Loss: 0.37734755873680115\n",
      "Epoch 1803, Loss: 1.7240292131900787, Final Batch Loss: 0.5194665193557739\n",
      "Epoch 1804, Loss: 1.6647363007068634, Final Batch Loss: 0.4334375560283661\n",
      "Epoch 1805, Loss: 1.7293371856212616, Final Batch Loss: 0.45187145471572876\n",
      "Epoch 1806, Loss: 1.7272021174430847, Final Batch Loss: 0.3389451801776886\n",
      "Epoch 1807, Loss: 1.65506711602211, Final Batch Loss: 0.3858407735824585\n",
      "Epoch 1808, Loss: 1.74708291888237, Final Batch Loss: 0.3721033036708832\n",
      "Epoch 1809, Loss: 1.6602716147899628, Final Batch Loss: 0.4254828989505768\n",
      "Epoch 1810, Loss: 1.647720992565155, Final Batch Loss: 0.40597403049468994\n",
      "Epoch 1811, Loss: 1.8818843066692352, Final Batch Loss: 0.6114471554756165\n",
      "Epoch 1812, Loss: 1.9023068845272064, Final Batch Loss: 0.540835440158844\n",
      "Epoch 1813, Loss: 1.758031815290451, Final Batch Loss: 0.38445979356765747\n",
      "Epoch 1814, Loss: 1.7076329290866852, Final Batch Loss: 0.46881917119026184\n",
      "Epoch 1815, Loss: 1.8397305011749268, Final Batch Loss: 0.5220818519592285\n",
      "Epoch 1816, Loss: 1.6617903113365173, Final Batch Loss: 0.4189927875995636\n",
      "Epoch 1817, Loss: 1.7369000315666199, Final Batch Loss: 0.38614049553871155\n",
      "Epoch 1818, Loss: 1.773792952299118, Final Batch Loss: 0.4049082398414612\n",
      "Epoch 1819, Loss: 1.6732361912727356, Final Batch Loss: 0.43126729130744934\n",
      "Epoch 1820, Loss: 1.6048408448696136, Final Batch Loss: 0.3946261703968048\n",
      "Epoch 1821, Loss: 1.7168966233730316, Final Batch Loss: 0.4429143965244293\n",
      "Epoch 1822, Loss: 1.6964865624904633, Final Batch Loss: 0.4756130278110504\n",
      "Epoch 1823, Loss: 1.7990835309028625, Final Batch Loss: 0.453995943069458\n",
      "Epoch 1824, Loss: 1.664209634065628, Final Batch Loss: 0.4107236862182617\n",
      "Epoch 1825, Loss: 1.71227166056633, Final Batch Loss: 0.3639175593852997\n",
      "Epoch 1826, Loss: 1.7572146654129028, Final Batch Loss: 0.5800851583480835\n",
      "Epoch 1827, Loss: 1.6570171117782593, Final Batch Loss: 0.3071102201938629\n",
      "Epoch 1828, Loss: 1.6242501437664032, Final Batch Loss: 0.3377278447151184\n",
      "Epoch 1829, Loss: 1.706935852766037, Final Batch Loss: 0.47187793254852295\n",
      "Epoch 1830, Loss: 1.6859427094459534, Final Batch Loss: 0.38061919808387756\n",
      "Epoch 1831, Loss: 1.7126465141773224, Final Batch Loss: 0.42329034209251404\n",
      "Epoch 1832, Loss: 1.8686344623565674, Final Batch Loss: 0.5674804449081421\n",
      "Epoch 1833, Loss: 1.5979775488376617, Final Batch Loss: 0.4401804804801941\n",
      "Epoch 1834, Loss: 1.63515242934227, Final Batch Loss: 0.35145869851112366\n",
      "Epoch 1835, Loss: 1.8999702632427216, Final Batch Loss: 0.48205938935279846\n",
      "Epoch 1836, Loss: 1.7347191870212555, Final Batch Loss: 0.4623366594314575\n",
      "Epoch 1837, Loss: 1.6825430393218994, Final Batch Loss: 0.3647356927394867\n",
      "Epoch 1838, Loss: 1.721973031759262, Final Batch Loss: 0.4189251661300659\n",
      "Epoch 1839, Loss: 1.6916674077510834, Final Batch Loss: 0.5009642243385315\n",
      "Epoch 1840, Loss: 1.6258188486099243, Final Batch Loss: 0.3462836742401123\n",
      "Epoch 1841, Loss: 1.666075587272644, Final Batch Loss: 0.3841463029384613\n",
      "Epoch 1842, Loss: 1.6390713453292847, Final Batch Loss: 0.45571961998939514\n",
      "Epoch 1843, Loss: 1.555875539779663, Final Batch Loss: 0.3302991986274719\n",
      "Epoch 1844, Loss: 1.7514578104019165, Final Batch Loss: 0.48778799176216125\n",
      "Epoch 1845, Loss: 1.6705933809280396, Final Batch Loss: 0.45559823513031006\n",
      "Epoch 1846, Loss: 1.6491683721542358, Final Batch Loss: 0.3592457175254822\n",
      "Epoch 1847, Loss: 1.6709130108356476, Final Batch Loss: 0.34168004989624023\n",
      "Epoch 1848, Loss: 1.6923604011535645, Final Batch Loss: 0.4146890342235565\n",
      "Epoch 1849, Loss: 1.7981614470481873, Final Batch Loss: 0.4281485080718994\n",
      "Epoch 1850, Loss: 1.7115496695041656, Final Batch Loss: 0.48086869716644287\n",
      "Epoch 1851, Loss: 1.717590570449829, Final Batch Loss: 0.41957446932792664\n",
      "Epoch 1852, Loss: 1.6519724428653717, Final Batch Loss: 0.4625401794910431\n",
      "Epoch 1853, Loss: 1.6961185932159424, Final Batch Loss: 0.4840857982635498\n",
      "Epoch 1854, Loss: 1.7169567942619324, Final Batch Loss: 0.4478595554828644\n",
      "Epoch 1855, Loss: 1.6399559378623962, Final Batch Loss: 0.39157193899154663\n",
      "Epoch 1856, Loss: 1.9495482742786407, Final Batch Loss: 0.6086923480033875\n",
      "Epoch 1857, Loss: 1.650403916835785, Final Batch Loss: 0.35946378111839294\n",
      "Epoch 1858, Loss: 1.7695949971675873, Final Batch Loss: 0.378116637468338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1859, Loss: 1.5889330506324768, Final Batch Loss: 0.2868136167526245\n",
      "Epoch 1860, Loss: 1.7208297550678253, Final Batch Loss: 0.4953589141368866\n",
      "Epoch 1861, Loss: 1.816394567489624, Final Batch Loss: 0.42914775013923645\n",
      "Epoch 1862, Loss: 1.6316098272800446, Final Batch Loss: 0.40729695558547974\n",
      "Epoch 1863, Loss: 1.6562450528144836, Final Batch Loss: 0.45894065499305725\n",
      "Epoch 1864, Loss: 1.7591902613639832, Final Batch Loss: 0.5029206275939941\n",
      "Epoch 1865, Loss: 1.7121001183986664, Final Batch Loss: 0.36993491649627686\n",
      "Epoch 1866, Loss: 1.6973040699958801, Final Batch Loss: 0.3950280249118805\n",
      "Epoch 1867, Loss: 1.6463463604450226, Final Batch Loss: 0.39490020275115967\n",
      "Epoch 1868, Loss: 1.6861388683319092, Final Batch Loss: 0.3249272108078003\n",
      "Epoch 1869, Loss: 1.7563092410564423, Final Batch Loss: 0.4419189691543579\n",
      "Epoch 1870, Loss: 1.7356017529964447, Final Batch Loss: 0.48788711428642273\n",
      "Epoch 1871, Loss: 1.7786599397659302, Final Batch Loss: 0.5454858541488647\n",
      "Epoch 1872, Loss: 1.6329744756221771, Final Batch Loss: 0.4224863648414612\n",
      "Epoch 1873, Loss: 1.6479913592338562, Final Batch Loss: 0.3856464922428131\n",
      "Epoch 1874, Loss: 1.5469149053096771, Final Batch Loss: 0.40395864844322205\n",
      "Epoch 1875, Loss: 1.654011994600296, Final Batch Loss: 0.44046393036842346\n",
      "Epoch 1876, Loss: 1.62853142619133, Final Batch Loss: 0.3298867344856262\n",
      "Epoch 1877, Loss: 1.6741178333759308, Final Batch Loss: 0.47941625118255615\n",
      "Epoch 1878, Loss: 1.6406615376472473, Final Batch Loss: 0.4287031292915344\n",
      "Epoch 1879, Loss: 1.7498060762882233, Final Batch Loss: 0.41830766201019287\n",
      "Epoch 1880, Loss: 1.6693022549152374, Final Batch Loss: 0.4651118516921997\n",
      "Epoch 1881, Loss: 1.6157658696174622, Final Batch Loss: 0.3584362864494324\n",
      "Epoch 1882, Loss: 1.6349784135818481, Final Batch Loss: 0.4093795418739319\n",
      "Epoch 1883, Loss: 1.566960096359253, Final Batch Loss: 0.2549934685230255\n",
      "Epoch 1884, Loss: 1.7386176586151123, Final Batch Loss: 0.5896342396736145\n",
      "Epoch 1885, Loss: 1.6231586039066315, Final Batch Loss: 0.32725080847740173\n",
      "Epoch 1886, Loss: 1.5814854502677917, Final Batch Loss: 0.35300683975219727\n",
      "Epoch 1887, Loss: 1.5978189706802368, Final Batch Loss: 0.35856372117996216\n",
      "Epoch 1888, Loss: 1.67522394657135, Final Batch Loss: 0.37154659628868103\n",
      "Epoch 1889, Loss: 1.6428289413452148, Final Batch Loss: 0.4374636113643646\n",
      "Epoch 1890, Loss: 1.66670161485672, Final Batch Loss: 0.4382553696632385\n",
      "Epoch 1891, Loss: 1.7206392586231232, Final Batch Loss: 0.4623502194881439\n",
      "Epoch 1892, Loss: 1.6754815578460693, Final Batch Loss: 0.4572216272354126\n",
      "Epoch 1893, Loss: 1.5802740156650543, Final Batch Loss: 0.39254230260849\n",
      "Epoch 1894, Loss: 1.6639290750026703, Final Batch Loss: 0.5076360106468201\n",
      "Epoch 1895, Loss: 1.6475631892681122, Final Batch Loss: 0.4951191544532776\n",
      "Epoch 1896, Loss: 1.6498574018478394, Final Batch Loss: 0.45827850699424744\n",
      "Epoch 1897, Loss: 1.71392622590065, Final Batch Loss: 0.4579174518585205\n",
      "Epoch 1898, Loss: 1.828658550977707, Final Batch Loss: 0.5806869268417358\n",
      "Epoch 1899, Loss: 1.86265429854393, Final Batch Loss: 0.536872148513794\n",
      "Epoch 1900, Loss: 1.7107746303081512, Final Batch Loss: 0.4015161991119385\n",
      "Epoch 1901, Loss: 1.784768968820572, Final Batch Loss: 0.5225180983543396\n",
      "Epoch 1902, Loss: 1.7473879158496857, Final Batch Loss: 0.3914579749107361\n",
      "Epoch 1903, Loss: 1.6218344271183014, Final Batch Loss: 0.3665098547935486\n",
      "Epoch 1904, Loss: 1.7527611553668976, Final Batch Loss: 0.4205096662044525\n",
      "Epoch 1905, Loss: 1.6307353079319, Final Batch Loss: 0.33435365557670593\n",
      "Epoch 1906, Loss: 1.6655406653881073, Final Batch Loss: 0.48145756125450134\n",
      "Epoch 1907, Loss: 1.67612624168396, Final Batch Loss: 0.4221273958683014\n",
      "Epoch 1908, Loss: 1.7616615891456604, Final Batch Loss: 0.44623568654060364\n",
      "Epoch 1909, Loss: 1.7221082150936127, Final Batch Loss: 0.3945522904396057\n",
      "Epoch 1910, Loss: 1.7345378398895264, Final Batch Loss: 0.5225275158882141\n",
      "Epoch 1911, Loss: 1.7475880682468414, Final Batch Loss: 0.4477168321609497\n",
      "Epoch 1912, Loss: 1.6993844509124756, Final Batch Loss: 0.4596242606639862\n",
      "Epoch 1913, Loss: 1.6360911130905151, Final Batch Loss: 0.4458293616771698\n",
      "Epoch 1914, Loss: 1.7090741395950317, Final Batch Loss: 0.4878230392932892\n",
      "Epoch 1915, Loss: 1.6538428366184235, Final Batch Loss: 0.4423977732658386\n",
      "Epoch 1916, Loss: 1.696073830127716, Final Batch Loss: 0.5202843546867371\n",
      "Epoch 1917, Loss: 1.5047226548194885, Final Batch Loss: 0.35444918274879456\n",
      "Epoch 1918, Loss: 1.6129602193832397, Final Batch Loss: 0.42702582478523254\n",
      "Epoch 1919, Loss: 1.647836297750473, Final Batch Loss: 0.4447097182273865\n",
      "Epoch 1920, Loss: 1.6192361414432526, Final Batch Loss: 0.3694937527179718\n",
      "Epoch 1921, Loss: 1.6786771416664124, Final Batch Loss: 0.4377174973487854\n",
      "Epoch 1922, Loss: 1.7129957675933838, Final Batch Loss: 0.419080525636673\n",
      "Epoch 1923, Loss: 1.6701021194458008, Final Batch Loss: 0.47833451628685\n",
      "Epoch 1924, Loss: 1.7188583314418793, Final Batch Loss: 0.3836868703365326\n",
      "Epoch 1925, Loss: 1.6934446096420288, Final Batch Loss: 0.411431223154068\n",
      "Epoch 1926, Loss: 1.6747196316719055, Final Batch Loss: 0.43561697006225586\n",
      "Epoch 1927, Loss: 1.6849776208400726, Final Batch Loss: 0.4193277657032013\n",
      "Epoch 1928, Loss: 1.6351903080940247, Final Batch Loss: 0.30264556407928467\n",
      "Epoch 1929, Loss: 1.6627964973449707, Final Batch Loss: 0.3515969216823578\n",
      "Epoch 1930, Loss: 1.7125464081764221, Final Batch Loss: 0.45333409309387207\n",
      "Epoch 1931, Loss: 1.6846639811992645, Final Batch Loss: 0.38770848512649536\n",
      "Epoch 1932, Loss: 1.5589866042137146, Final Batch Loss: 0.41170793771743774\n",
      "Epoch 1933, Loss: 1.7226565182209015, Final Batch Loss: 0.4301205277442932\n",
      "Epoch 1934, Loss: 1.7341294288635254, Final Batch Loss: 0.4808226227760315\n",
      "Epoch 1935, Loss: 1.533068597316742, Final Batch Loss: 0.35887816548347473\n",
      "Epoch 1936, Loss: 1.7563286423683167, Final Batch Loss: 0.41893160343170166\n",
      "Epoch 1937, Loss: 1.6436381340026855, Final Batch Loss: 0.375095009803772\n",
      "Epoch 1938, Loss: 1.5897204875946045, Final Batch Loss: 0.4093930721282959\n",
      "Epoch 1939, Loss: 1.5728091597557068, Final Batch Loss: 0.4021805226802826\n",
      "Epoch 1940, Loss: 1.6483226418495178, Final Batch Loss: 0.35942012071609497\n",
      "Epoch 1941, Loss: 1.6337870359420776, Final Batch Loss: 0.4572446346282959\n",
      "Epoch 1942, Loss: 1.5969651639461517, Final Batch Loss: 0.39389798045158386\n",
      "Epoch 1943, Loss: 1.7186290919780731, Final Batch Loss: 0.4457389712333679\n",
      "Epoch 1944, Loss: 1.6205333471298218, Final Batch Loss: 0.39543646574020386\n",
      "Epoch 1945, Loss: 1.718964695930481, Final Batch Loss: 0.3927918076515198\n",
      "Epoch 1946, Loss: 1.568286120891571, Final Batch Loss: 0.3557013273239136\n",
      "Epoch 1947, Loss: 1.7723809480667114, Final Batch Loss: 0.4815615117549896\n",
      "Epoch 1948, Loss: 1.8232429921627045, Final Batch Loss: 0.5530663132667542\n",
      "Epoch 1949, Loss: 1.5946866273880005, Final Batch Loss: 0.3526860177516937\n",
      "Epoch 1950, Loss: 1.7989640831947327, Final Batch Loss: 0.44782188534736633\n",
      "Epoch 1951, Loss: 1.6964537799358368, Final Batch Loss: 0.43807536363601685\n",
      "Epoch 1952, Loss: 1.7991478741168976, Final Batch Loss: 0.44564783573150635\n",
      "Epoch 1953, Loss: 1.623275637626648, Final Batch Loss: 0.3784199655056\n",
      "Epoch 1954, Loss: 1.686089277267456, Final Batch Loss: 0.39495986700057983\n",
      "Epoch 1955, Loss: 1.6207769811153412, Final Batch Loss: 0.3207707107067108\n",
      "Epoch 1956, Loss: 1.666431039571762, Final Batch Loss: 0.49677878618240356\n",
      "Epoch 1957, Loss: 1.7178031206130981, Final Batch Loss: 0.5026679635047913\n",
      "Epoch 1958, Loss: 1.5797748267650604, Final Batch Loss: 0.3503955900669098\n",
      "Epoch 1959, Loss: 1.601657748222351, Final Batch Loss: 0.44329342246055603\n",
      "Epoch 1960, Loss: 1.754262089729309, Final Batch Loss: 0.5041384696960449\n",
      "Epoch 1961, Loss: 1.7376719117164612, Final Batch Loss: 0.4420901834964752\n",
      "Epoch 1962, Loss: 1.7133454084396362, Final Batch Loss: 0.51658034324646\n",
      "Epoch 1963, Loss: 1.7787845730781555, Final Batch Loss: 0.34719082713127136\n",
      "Epoch 1964, Loss: 1.6903030276298523, Final Batch Loss: 0.5003920197486877\n",
      "Epoch 1965, Loss: 1.8418116569519043, Final Batch Loss: 0.5047839879989624\n",
      "Epoch 1966, Loss: 1.7732957899570465, Final Batch Loss: 0.494995653629303\n",
      "Epoch 1967, Loss: 1.6787213385105133, Final Batch Loss: 0.3823627531528473\n",
      "Epoch 1968, Loss: 1.5248105227947235, Final Batch Loss: 0.291556179523468\n",
      "Epoch 1969, Loss: 1.7180707454681396, Final Batch Loss: 0.4665054678916931\n",
      "Epoch 1970, Loss: 1.7534969747066498, Final Batch Loss: 0.3756347596645355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1971, Loss: 1.609543353319168, Final Batch Loss: 0.34977060556411743\n",
      "Epoch 1972, Loss: 1.6825173199176788, Final Batch Loss: 0.48930615186691284\n",
      "Epoch 1973, Loss: 1.6453405022621155, Final Batch Loss: 0.44369637966156006\n",
      "Epoch 1974, Loss: 1.5970024764537811, Final Batch Loss: 0.3294663727283478\n",
      "Epoch 1975, Loss: 1.5962104797363281, Final Batch Loss: 0.3694842457771301\n",
      "Epoch 1976, Loss: 1.5221956372261047, Final Batch Loss: 0.32795006036758423\n",
      "Epoch 1977, Loss: 1.7081586122512817, Final Batch Loss: 0.46837785840034485\n",
      "Epoch 1978, Loss: 1.7889010310173035, Final Batch Loss: 0.4318492114543915\n",
      "Epoch 1979, Loss: 1.5605252087116241, Final Batch Loss: 0.36528974771499634\n",
      "Epoch 1980, Loss: 1.5814565122127533, Final Batch Loss: 0.3506329357624054\n",
      "Epoch 1981, Loss: 1.8131698369979858, Final Batch Loss: 0.5361899733543396\n",
      "Epoch 1982, Loss: 1.614079087972641, Final Batch Loss: 0.33437904715538025\n",
      "Epoch 1983, Loss: 1.6706582009792328, Final Batch Loss: 0.3727770447731018\n",
      "Epoch 1984, Loss: 1.6923329532146454, Final Batch Loss: 0.3300098180770874\n",
      "Epoch 1985, Loss: 1.5821758210659027, Final Batch Loss: 0.3354526162147522\n",
      "Epoch 1986, Loss: 1.8478361368179321, Final Batch Loss: 0.5005283355712891\n",
      "Epoch 1987, Loss: 1.5809862315654755, Final Batch Loss: 0.34530937671661377\n",
      "Epoch 1988, Loss: 1.6223104298114777, Final Batch Loss: 0.36260929703712463\n",
      "Epoch 1989, Loss: 1.5925136506557465, Final Batch Loss: 0.4113110303878784\n",
      "Epoch 1990, Loss: 1.7181123793125153, Final Batch Loss: 0.47573330998420715\n",
      "Epoch 1991, Loss: 1.80874103307724, Final Batch Loss: 0.42887309193611145\n",
      "Epoch 1992, Loss: 1.620844155550003, Final Batch Loss: 0.4138561487197876\n",
      "Epoch 1993, Loss: 1.7348245680332184, Final Batch Loss: 0.45443734526634216\n",
      "Epoch 1994, Loss: 1.597624272108078, Final Batch Loss: 0.37270328402519226\n",
      "Epoch 1995, Loss: 1.6196984648704529, Final Batch Loss: 0.3254457712173462\n",
      "Epoch 1996, Loss: 1.7314328253269196, Final Batch Loss: 0.40786805748939514\n",
      "Epoch 1997, Loss: 1.7172161042690277, Final Batch Loss: 0.4806804060935974\n",
      "Epoch 1998, Loss: 1.7892555892467499, Final Batch Loss: 0.4437359869480133\n",
      "Epoch 1999, Loss: 1.6799401938915253, Final Batch Loss: 0.4092986583709717\n",
      "Epoch 2000, Loss: 1.6891202330589294, Final Batch Loss: 0.5175575613975525\n",
      "Epoch 2001, Loss: 1.5449865758419037, Final Batch Loss: 0.26085618138313293\n",
      "Epoch 2002, Loss: 1.6020017564296722, Final Batch Loss: 0.30959752202033997\n",
      "Epoch 2003, Loss: 1.6322225630283356, Final Batch Loss: 0.4193713068962097\n",
      "Epoch 2004, Loss: 1.6597330868244171, Final Batch Loss: 0.4715397357940674\n",
      "Epoch 2005, Loss: 1.6730432212352753, Final Batch Loss: 0.4918252229690552\n",
      "Epoch 2006, Loss: 1.6362218856811523, Final Batch Loss: 0.44356948137283325\n",
      "Epoch 2007, Loss: 1.610742449760437, Final Batch Loss: 0.3957730531692505\n",
      "Epoch 2008, Loss: 1.8910878002643585, Final Batch Loss: 0.4648640751838684\n",
      "Epoch 2009, Loss: 1.7069704830646515, Final Batch Loss: 0.3654756546020508\n",
      "Epoch 2010, Loss: 1.5440826416015625, Final Batch Loss: 0.35316064953804016\n",
      "Epoch 2011, Loss: 1.688769370317459, Final Batch Loss: 0.40676385164260864\n",
      "Epoch 2012, Loss: 1.6130450367927551, Final Batch Loss: 0.40315788984298706\n",
      "Epoch 2013, Loss: 1.6531360149383545, Final Batch Loss: 0.4965295195579529\n",
      "Epoch 2014, Loss: 1.7275643348693848, Final Batch Loss: 0.39510002732276917\n",
      "Epoch 2015, Loss: 1.6566440165042877, Final Batch Loss: 0.3960077166557312\n",
      "Epoch 2016, Loss: 1.7531406581401825, Final Batch Loss: 0.45243650674819946\n",
      "Epoch 2017, Loss: 1.678349882364273, Final Batch Loss: 0.41853851079940796\n",
      "Epoch 2018, Loss: 1.6529105305671692, Final Batch Loss: 0.37618497014045715\n",
      "Epoch 2019, Loss: 1.6213061809539795, Final Batch Loss: 0.4108060896396637\n",
      "Epoch 2020, Loss: 1.6765044927597046, Final Batch Loss: 0.4259127974510193\n",
      "Epoch 2021, Loss: 1.4955106973648071, Final Batch Loss: 0.3661884069442749\n",
      "Epoch 2022, Loss: 1.691646248102188, Final Batch Loss: 0.445941299200058\n",
      "Epoch 2023, Loss: 1.7128023505210876, Final Batch Loss: 0.418331116437912\n",
      "Epoch 2024, Loss: 1.5070675313472748, Final Batch Loss: 0.3642761707305908\n",
      "Epoch 2025, Loss: 1.6233193278312683, Final Batch Loss: 0.43842414021492004\n",
      "Epoch 2026, Loss: 1.6551623344421387, Final Batch Loss: 0.46000558137893677\n",
      "Epoch 2027, Loss: 1.8095589876174927, Final Batch Loss: 0.47389447689056396\n",
      "Epoch 2028, Loss: 1.596719890832901, Final Batch Loss: 0.3680219352245331\n",
      "Epoch 2029, Loss: 1.6110514104366302, Final Batch Loss: 0.4618314504623413\n",
      "Epoch 2030, Loss: 1.7152432799339294, Final Batch Loss: 0.4924018383026123\n",
      "Epoch 2031, Loss: 1.7535401582717896, Final Batch Loss: 0.5040900707244873\n",
      "Epoch 2032, Loss: 1.6637925803661346, Final Batch Loss: 0.39241987466812134\n",
      "Epoch 2033, Loss: 1.5898626446723938, Final Batch Loss: 0.4289017915725708\n",
      "Epoch 2034, Loss: 1.5459966659545898, Final Batch Loss: 0.4185342788696289\n",
      "Epoch 2035, Loss: 1.6584455966949463, Final Batch Loss: 0.39039111137390137\n",
      "Epoch 2036, Loss: 1.5669556260108948, Final Batch Loss: 0.41762977838516235\n",
      "Epoch 2037, Loss: 1.616187334060669, Final Batch Loss: 0.43630746006965637\n",
      "Epoch 2038, Loss: 1.6369958519935608, Final Batch Loss: 0.45752376317977905\n",
      "Epoch 2039, Loss: 1.5660882592201233, Final Batch Loss: 0.37149500846862793\n",
      "Epoch 2040, Loss: 1.5446785986423492, Final Batch Loss: 0.3758106529712677\n",
      "Epoch 2041, Loss: 1.5848424434661865, Final Batch Loss: 0.3689136207103729\n",
      "Epoch 2042, Loss: 1.663943588733673, Final Batch Loss: 0.48924627900123596\n",
      "Epoch 2043, Loss: 1.935767263174057, Final Batch Loss: 0.6290857195854187\n",
      "Epoch 2044, Loss: 1.7618990242481232, Final Batch Loss: 0.48568347096443176\n",
      "Epoch 2045, Loss: 1.6119688153266907, Final Batch Loss: 0.42135727405548096\n",
      "Epoch 2046, Loss: 1.5466641783714294, Final Batch Loss: 0.40131181478500366\n",
      "Epoch 2047, Loss: 1.6073565781116486, Final Batch Loss: 0.44447317719459534\n",
      "Epoch 2048, Loss: 1.7442260682582855, Final Batch Loss: 0.5090615153312683\n",
      "Epoch 2049, Loss: 1.6892139613628387, Final Batch Loss: 0.3859553039073944\n",
      "Epoch 2050, Loss: 1.6726441383361816, Final Batch Loss: 0.4892871677875519\n",
      "Epoch 2051, Loss: 1.7121794521808624, Final Batch Loss: 0.42435911297798157\n",
      "Epoch 2052, Loss: 1.6196980476379395, Final Batch Loss: 0.2980344295501709\n",
      "Epoch 2053, Loss: 1.6141115725040436, Final Batch Loss: 0.44929632544517517\n",
      "Epoch 2054, Loss: 1.6746541857719421, Final Batch Loss: 0.44536373019218445\n",
      "Epoch 2055, Loss: 1.5405820608139038, Final Batch Loss: 0.36231324076652527\n",
      "Epoch 2056, Loss: 1.677969217300415, Final Batch Loss: 0.4534628689289093\n",
      "Epoch 2057, Loss: 1.7420976161956787, Final Batch Loss: 0.4889460802078247\n",
      "Epoch 2058, Loss: 1.6258822679519653, Final Batch Loss: 0.3664929270744324\n",
      "Epoch 2059, Loss: 1.7139353454113007, Final Batch Loss: 0.40865764021873474\n",
      "Epoch 2060, Loss: 1.5899337232112885, Final Batch Loss: 0.37119749188423157\n",
      "Epoch 2061, Loss: 1.6596368849277496, Final Batch Loss: 0.487490713596344\n",
      "Epoch 2062, Loss: 1.6045026779174805, Final Batch Loss: 0.40099889039993286\n",
      "Epoch 2063, Loss: 1.7367390394210815, Final Batch Loss: 0.42325451970100403\n",
      "Epoch 2064, Loss: 1.5927370488643646, Final Batch Loss: 0.37393927574157715\n",
      "Epoch 2065, Loss: 1.5712215602397919, Final Batch Loss: 0.37545689940452576\n",
      "Epoch 2066, Loss: 1.6029746234416962, Final Batch Loss: 0.41978710889816284\n",
      "Epoch 2067, Loss: 1.7109948098659515, Final Batch Loss: 0.4306190311908722\n",
      "Epoch 2068, Loss: 1.6187661290168762, Final Batch Loss: 0.42110151052474976\n",
      "Epoch 2069, Loss: 1.6641936600208282, Final Batch Loss: 0.42008981108665466\n",
      "Epoch 2070, Loss: 1.6103108525276184, Final Batch Loss: 0.4046309292316437\n",
      "Epoch 2071, Loss: 1.691716879606247, Final Batch Loss: 0.4424085021018982\n",
      "Epoch 2072, Loss: 1.5880757868289948, Final Batch Loss: 0.41475069522857666\n",
      "Epoch 2073, Loss: 1.694944441318512, Final Batch Loss: 0.4638778567314148\n",
      "Epoch 2074, Loss: 1.629639059305191, Final Batch Loss: 0.4343739449977875\n",
      "Epoch 2075, Loss: 1.6507787704467773, Final Batch Loss: 0.4149024486541748\n",
      "Epoch 2076, Loss: 1.711199164390564, Final Batch Loss: 0.4783124327659607\n",
      "Epoch 2077, Loss: 1.7787815928459167, Final Batch Loss: 0.5981377363204956\n",
      "Epoch 2078, Loss: 1.6699531972408295, Final Batch Loss: 0.42753130197525024\n",
      "Epoch 2079, Loss: 1.7379289269447327, Final Batch Loss: 0.3721749484539032\n",
      "Epoch 2080, Loss: 1.6154474020004272, Final Batch Loss: 0.3562769293785095\n",
      "Epoch 2081, Loss: 1.6457530558109283, Final Batch Loss: 0.43255892395973206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2082, Loss: 1.5585571229457855, Final Batch Loss: 0.4263242781162262\n",
      "Epoch 2083, Loss: 1.5338205695152283, Final Batch Loss: 0.4195192754268646\n",
      "Epoch 2084, Loss: 1.5035696923732758, Final Batch Loss: 0.3308766782283783\n",
      "Epoch 2085, Loss: 1.5606087148189545, Final Batch Loss: 0.3527491092681885\n",
      "Epoch 2086, Loss: 1.6004807353019714, Final Batch Loss: 0.44169187545776367\n",
      "Epoch 2087, Loss: 1.5419064164161682, Final Batch Loss: 0.3632732927799225\n",
      "Epoch 2088, Loss: 1.612103432416916, Final Batch Loss: 0.4221511483192444\n",
      "Epoch 2089, Loss: 1.5999845564365387, Final Batch Loss: 0.346458375453949\n",
      "Epoch 2090, Loss: 1.7634524405002594, Final Batch Loss: 0.4519963264465332\n",
      "Epoch 2091, Loss: 1.5910082757472992, Final Batch Loss: 0.4278036653995514\n",
      "Epoch 2092, Loss: 1.6731095910072327, Final Batch Loss: 0.44098079204559326\n",
      "Epoch 2093, Loss: 1.5948202311992645, Final Batch Loss: 0.3938165307044983\n",
      "Epoch 2094, Loss: 1.5342462956905365, Final Batch Loss: 0.37201741337776184\n",
      "Epoch 2095, Loss: 1.5650381743907928, Final Batch Loss: 0.3779985010623932\n",
      "Epoch 2096, Loss: 1.678250104188919, Final Batch Loss: 0.3867151439189911\n",
      "Epoch 2097, Loss: 1.6903760135173798, Final Batch Loss: 0.5223707556724548\n",
      "Epoch 2098, Loss: 1.4912010729312897, Final Batch Loss: 0.30132314562797546\n",
      "Epoch 2099, Loss: 1.6515899002552032, Final Batch Loss: 0.4581909775733948\n",
      "Epoch 2100, Loss: 1.6332275867462158, Final Batch Loss: 0.4267602264881134\n",
      "Epoch 2101, Loss: 1.5947619378566742, Final Batch Loss: 0.35616710782051086\n",
      "Epoch 2102, Loss: 1.4364701509475708, Final Batch Loss: 0.35419517755508423\n",
      "Epoch 2103, Loss: 1.6436713635921478, Final Batch Loss: 0.3212120234966278\n",
      "Epoch 2104, Loss: 1.59092777967453, Final Batch Loss: 0.3925701677799225\n",
      "Epoch 2105, Loss: 1.6445665657520294, Final Batch Loss: 0.4097009301185608\n",
      "Epoch 2106, Loss: 1.5920507907867432, Final Batch Loss: 0.3763105571269989\n",
      "Epoch 2107, Loss: 1.6528655886650085, Final Batch Loss: 0.469870388507843\n",
      "Epoch 2108, Loss: 1.4980011284351349, Final Batch Loss: 0.46807849407196045\n",
      "Epoch 2109, Loss: 1.4989855289459229, Final Batch Loss: 0.3209814429283142\n",
      "Epoch 2110, Loss: 1.6582738161087036, Final Batch Loss: 0.47443604469299316\n",
      "Epoch 2111, Loss: 1.604536235332489, Final Batch Loss: 0.4382118284702301\n",
      "Epoch 2112, Loss: 1.615784913301468, Final Batch Loss: 0.3302096724510193\n",
      "Epoch 2113, Loss: 1.843235969543457, Final Batch Loss: 0.5649821162223816\n",
      "Epoch 2114, Loss: 1.5402432978153229, Final Batch Loss: 0.39531034231185913\n",
      "Epoch 2115, Loss: 1.6340471506118774, Final Batch Loss: 0.45958372950553894\n",
      "Epoch 2116, Loss: 1.6271271407604218, Final Batch Loss: 0.42181673645973206\n",
      "Epoch 2117, Loss: 1.4819605946540833, Final Batch Loss: 0.39820805191993713\n",
      "Epoch 2118, Loss: 1.6633550822734833, Final Batch Loss: 0.4567374289035797\n",
      "Epoch 2119, Loss: 1.509123831987381, Final Batch Loss: 0.340897798538208\n",
      "Epoch 2120, Loss: 1.6488607227802277, Final Batch Loss: 0.371347576379776\n",
      "Epoch 2121, Loss: 1.7048395574092865, Final Batch Loss: 0.4528397023677826\n",
      "Epoch 2122, Loss: 1.5543376207351685, Final Batch Loss: 0.46477583050727844\n",
      "Epoch 2123, Loss: 1.6768610179424286, Final Batch Loss: 0.5195631980895996\n",
      "Epoch 2124, Loss: 1.6546622216701508, Final Batch Loss: 0.3795011341571808\n",
      "Epoch 2125, Loss: 1.8424696624279022, Final Batch Loss: 0.5471646785736084\n",
      "Epoch 2126, Loss: 1.6038323938846588, Final Batch Loss: 0.37544146180152893\n",
      "Epoch 2127, Loss: 1.5732106566429138, Final Batch Loss: 0.45003125071525574\n",
      "Epoch 2128, Loss: 1.5551601350307465, Final Batch Loss: 0.37427571415901184\n",
      "Epoch 2129, Loss: 1.551629215478897, Final Batch Loss: 0.35747218132019043\n",
      "Epoch 2130, Loss: 1.558776617050171, Final Batch Loss: 0.37583303451538086\n",
      "Epoch 2131, Loss: 1.5982389450073242, Final Batch Loss: 0.3877483606338501\n",
      "Epoch 2132, Loss: 1.5557714104652405, Final Batch Loss: 0.43174952268600464\n",
      "Epoch 2133, Loss: 1.7680512368679047, Final Batch Loss: 0.4362521469593048\n",
      "Epoch 2134, Loss: 1.5464161336421967, Final Batch Loss: 0.33073779940605164\n",
      "Epoch 2135, Loss: 1.4845203161239624, Final Batch Loss: 0.37078148126602173\n",
      "Epoch 2136, Loss: 1.5299725532531738, Final Batch Loss: 0.3906456530094147\n",
      "Epoch 2137, Loss: 1.6281724870204926, Final Batch Loss: 0.46904444694519043\n",
      "Epoch 2138, Loss: 1.5627918541431427, Final Batch Loss: 0.4353831112384796\n",
      "Epoch 2139, Loss: 1.4689341187477112, Final Batch Loss: 0.34962543845176697\n",
      "Epoch 2140, Loss: 1.5939049124717712, Final Batch Loss: 0.35342174768447876\n",
      "Epoch 2141, Loss: 1.602250337600708, Final Batch Loss: 0.399672269821167\n",
      "Epoch 2142, Loss: 1.648326575756073, Final Batch Loss: 0.4510032534599304\n",
      "Epoch 2143, Loss: 1.47664475440979, Final Batch Loss: 0.35386744141578674\n",
      "Epoch 2144, Loss: 1.5363826751708984, Final Batch Loss: 0.3408462703227997\n",
      "Epoch 2145, Loss: 1.6709808111190796, Final Batch Loss: 0.47337427735328674\n",
      "Epoch 2146, Loss: 1.4980975985527039, Final Batch Loss: 0.36940425634384155\n",
      "Epoch 2147, Loss: 1.4841493964195251, Final Batch Loss: 0.3665980398654938\n",
      "Epoch 2148, Loss: 1.6497111916542053, Final Batch Loss: 0.36584749817848206\n",
      "Epoch 2149, Loss: 1.6993725001811981, Final Batch Loss: 0.3362821936607361\n",
      "Epoch 2150, Loss: 1.6795158386230469, Final Batch Loss: 0.46428510546684265\n",
      "Epoch 2151, Loss: 1.64547997713089, Final Batch Loss: 0.42638200521469116\n",
      "Epoch 2152, Loss: 1.5435114204883575, Final Batch Loss: 0.3773469924926758\n",
      "Epoch 2153, Loss: 1.5866631269454956, Final Batch Loss: 0.37750348448753357\n",
      "Epoch 2154, Loss: 1.6399493515491486, Final Batch Loss: 0.430590957403183\n",
      "Epoch 2155, Loss: 1.5703062415122986, Final Batch Loss: 0.29785579442977905\n",
      "Epoch 2156, Loss: 1.5434156954288483, Final Batch Loss: 0.3562839925289154\n",
      "Epoch 2157, Loss: 1.6111498773097992, Final Batch Loss: 0.41591596603393555\n",
      "Epoch 2158, Loss: 1.6162206530570984, Final Batch Loss: 0.4822786748409271\n",
      "Epoch 2159, Loss: 1.5940365195274353, Final Batch Loss: 0.41040506958961487\n",
      "Epoch 2160, Loss: 1.5216067731380463, Final Batch Loss: 0.33598387241363525\n",
      "Epoch 2161, Loss: 1.6480287611484528, Final Batch Loss: 0.43623530864715576\n",
      "Epoch 2162, Loss: 1.6830646395683289, Final Batch Loss: 0.515531063079834\n",
      "Epoch 2163, Loss: 1.6169191896915436, Final Batch Loss: 0.4552864134311676\n",
      "Epoch 2164, Loss: 1.48125621676445, Final Batch Loss: 0.31487584114074707\n",
      "Epoch 2165, Loss: 1.501031756401062, Final Batch Loss: 0.3259100317955017\n",
      "Epoch 2166, Loss: 1.602219432592392, Final Batch Loss: 0.4097379148006439\n",
      "Epoch 2167, Loss: 1.6836880147457123, Final Batch Loss: 0.4471859633922577\n",
      "Epoch 2168, Loss: 1.506966233253479, Final Batch Loss: 0.2882457375526428\n",
      "Epoch 2169, Loss: 1.5450413525104523, Final Batch Loss: 0.35745367407798767\n",
      "Epoch 2170, Loss: 1.6480861902236938, Final Batch Loss: 0.4462665915489197\n",
      "Epoch 2171, Loss: 1.634362131357193, Final Batch Loss: 0.47434064745903015\n",
      "Epoch 2172, Loss: 1.621662974357605, Final Batch Loss: 0.46565112471580505\n",
      "Epoch 2173, Loss: 1.5913061499595642, Final Batch Loss: 0.35755035281181335\n",
      "Epoch 2174, Loss: 1.5184691548347473, Final Batch Loss: 0.36472588777542114\n",
      "Epoch 2175, Loss: 1.5602876245975494, Final Batch Loss: 0.4799415171146393\n",
      "Epoch 2176, Loss: 1.6256014108657837, Final Batch Loss: 0.4039549231529236\n",
      "Epoch 2177, Loss: 1.5815418064594269, Final Batch Loss: 0.3532277047634125\n",
      "Epoch 2178, Loss: 1.5999222695827484, Final Batch Loss: 0.3392195999622345\n",
      "Epoch 2179, Loss: 1.629942923784256, Final Batch Loss: 0.4340328574180603\n",
      "Epoch 2180, Loss: 1.627602905035019, Final Batch Loss: 0.4179598093032837\n",
      "Epoch 2181, Loss: 1.6180756390094757, Final Batch Loss: 0.3440147936344147\n",
      "Epoch 2182, Loss: 1.752388060092926, Final Batch Loss: 0.4373014271259308\n",
      "Epoch 2183, Loss: 1.4459232687950134, Final Batch Loss: 0.19259408116340637\n",
      "Epoch 2184, Loss: 1.719120442867279, Final Batch Loss: 0.5648089051246643\n",
      "Epoch 2185, Loss: 1.5745626389980316, Final Batch Loss: 0.4162038564682007\n",
      "Epoch 2186, Loss: 1.5079443752765656, Final Batch Loss: 0.38811957836151123\n",
      "Epoch 2187, Loss: 1.6311051845550537, Final Batch Loss: 0.4706932604312897\n",
      "Epoch 2188, Loss: 1.61976557970047, Final Batch Loss: 0.46753790974617004\n",
      "Epoch 2189, Loss: 1.609958678483963, Final Batch Loss: 0.36560842394828796\n",
      "Epoch 2190, Loss: 1.6142495572566986, Final Batch Loss: 0.4197930097579956\n",
      "Epoch 2191, Loss: 1.618338406085968, Final Batch Loss: 0.40733402967453003\n",
      "Epoch 2192, Loss: 1.6313482522964478, Final Batch Loss: 0.36363863945007324\n",
      "Epoch 2193, Loss: 1.6665609776973724, Final Batch Loss: 0.4624738395214081\n",
      "Epoch 2194, Loss: 1.5479210317134857, Final Batch Loss: 0.394896537065506\n",
      "Epoch 2195, Loss: 1.5519645810127258, Final Batch Loss: 0.38643476366996765\n",
      "Epoch 2196, Loss: 1.5453338921070099, Final Batch Loss: 0.3606830835342407\n",
      "Epoch 2197, Loss: 1.5114754140377045, Final Batch Loss: 0.45600613951683044\n",
      "Epoch 2198, Loss: 1.5261422395706177, Final Batch Loss: 0.41715896129608154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2199, Loss: 1.5549106001853943, Final Batch Loss: 0.37798011302948\n",
      "Epoch 2200, Loss: 1.5877557694911957, Final Batch Loss: 0.4305962920188904\n",
      "Epoch 2201, Loss: 1.632498413324356, Final Batch Loss: 0.5320305824279785\n",
      "Epoch 2202, Loss: 1.668089598417282, Final Batch Loss: 0.4627382159233093\n",
      "Epoch 2203, Loss: 1.5439602732658386, Final Batch Loss: 0.3135845959186554\n",
      "Epoch 2204, Loss: 1.5485926568508148, Final Batch Loss: 0.34410953521728516\n",
      "Epoch 2205, Loss: 1.5315791368484497, Final Batch Loss: 0.35659298300743103\n",
      "Epoch 2206, Loss: 1.541581779718399, Final Batch Loss: 0.3666853904724121\n",
      "Epoch 2207, Loss: 1.574442833662033, Final Batch Loss: 0.3589656949043274\n",
      "Epoch 2208, Loss: 1.574611872434616, Final Batch Loss: 0.34451010823249817\n",
      "Epoch 2209, Loss: 1.5104976296424866, Final Batch Loss: 0.3285164535045624\n",
      "Epoch 2210, Loss: 1.6235844194889069, Final Batch Loss: 0.35139578580856323\n",
      "Epoch 2211, Loss: 1.5789955258369446, Final Batch Loss: 0.4607185125350952\n",
      "Epoch 2212, Loss: 1.6775536835193634, Final Batch Loss: 0.4273144602775574\n",
      "Epoch 2213, Loss: 1.5446227490901947, Final Batch Loss: 0.3513520061969757\n",
      "Epoch 2214, Loss: 1.5555410385131836, Final Batch Loss: 0.3945482671260834\n",
      "Epoch 2215, Loss: 1.7282631695270538, Final Batch Loss: 0.3992835283279419\n",
      "Epoch 2216, Loss: 1.49909245967865, Final Batch Loss: 0.33688923716545105\n",
      "Epoch 2217, Loss: 1.5059803128242493, Final Batch Loss: 0.3321264088153839\n",
      "Epoch 2218, Loss: 1.6251358389854431, Final Batch Loss: 0.4518113136291504\n",
      "Epoch 2219, Loss: 1.576958805322647, Final Batch Loss: 0.3936385214328766\n",
      "Epoch 2220, Loss: 1.623541384935379, Final Batch Loss: 0.3515640199184418\n",
      "Epoch 2221, Loss: 1.6285932958126068, Final Batch Loss: 0.44699281454086304\n",
      "Epoch 2222, Loss: 1.5093875229358673, Final Batch Loss: 0.4003547132015228\n",
      "Epoch 2223, Loss: 1.5174113512039185, Final Batch Loss: 0.40269744396209717\n",
      "Epoch 2224, Loss: 1.6580796539783478, Final Batch Loss: 0.443808376789093\n",
      "Epoch 2225, Loss: 1.6759262084960938, Final Batch Loss: 0.4067576825618744\n",
      "Epoch 2226, Loss: 1.669348269701004, Final Batch Loss: 0.4187058210372925\n",
      "Epoch 2227, Loss: 1.5790727138519287, Final Batch Loss: 0.3068232536315918\n",
      "Epoch 2228, Loss: 1.5364205837249756, Final Batch Loss: 0.364439994096756\n",
      "Epoch 2229, Loss: 1.47164386510849, Final Batch Loss: 0.2961282432079315\n",
      "Epoch 2230, Loss: 1.579109936952591, Final Batch Loss: 0.3399416208267212\n",
      "Epoch 2231, Loss: 1.4714994728565216, Final Batch Loss: 0.3165580928325653\n",
      "Epoch 2232, Loss: 1.556601107120514, Final Batch Loss: 0.33060404658317566\n",
      "Epoch 2233, Loss: 1.67799973487854, Final Batch Loss: 0.43120077252388\n",
      "Epoch 2234, Loss: 1.4749622344970703, Final Batch Loss: 0.3093574047088623\n",
      "Epoch 2235, Loss: 1.513285517692566, Final Batch Loss: 0.35020044445991516\n",
      "Epoch 2236, Loss: 1.5451030433177948, Final Batch Loss: 0.34822383522987366\n",
      "Epoch 2237, Loss: 1.6513215601444244, Final Batch Loss: 0.4780213534832001\n",
      "Epoch 2238, Loss: 1.6847796440124512, Final Batch Loss: 0.454399436712265\n",
      "Epoch 2239, Loss: 1.6560385823249817, Final Batch Loss: 0.41859564185142517\n",
      "Epoch 2240, Loss: 1.557007223367691, Final Batch Loss: 0.44616827368736267\n",
      "Epoch 2241, Loss: 1.5038702487945557, Final Batch Loss: 0.3826858103275299\n",
      "Epoch 2242, Loss: 1.4888184666633606, Final Batch Loss: 0.31995725631713867\n",
      "Epoch 2243, Loss: 1.4743803441524506, Final Batch Loss: 0.33690905570983887\n",
      "Epoch 2244, Loss: 1.499279409646988, Final Batch Loss: 0.3984765410423279\n",
      "Epoch 2245, Loss: 1.3524873852729797, Final Batch Loss: 0.2948395609855652\n",
      "Epoch 2246, Loss: 1.5876307487487793, Final Batch Loss: 0.4035950005054474\n",
      "Epoch 2247, Loss: 1.6186708807945251, Final Batch Loss: 0.4253188669681549\n",
      "Epoch 2248, Loss: 1.515756070613861, Final Batch Loss: 0.3501216471195221\n",
      "Epoch 2249, Loss: 1.6380296349525452, Final Batch Loss: 0.34724774956703186\n",
      "Epoch 2250, Loss: 1.6117351353168488, Final Batch Loss: 0.3979977071285248\n",
      "Epoch 2251, Loss: 1.640924483537674, Final Batch Loss: 0.4852333068847656\n",
      "Epoch 2252, Loss: 1.5365735590457916, Final Batch Loss: 0.472713828086853\n",
      "Epoch 2253, Loss: 1.5054353773593903, Final Batch Loss: 0.2981354296207428\n",
      "Epoch 2254, Loss: 1.609021931886673, Final Batch Loss: 0.49060189723968506\n",
      "Epoch 2255, Loss: 1.6265468001365662, Final Batch Loss: 0.4341510832309723\n",
      "Epoch 2256, Loss: 1.600026786327362, Final Batch Loss: 0.4895952641963959\n",
      "Epoch 2257, Loss: 1.7212989926338196, Final Batch Loss: 0.4174134433269501\n",
      "Epoch 2258, Loss: 1.5818572044372559, Final Batch Loss: 0.324252724647522\n",
      "Epoch 2259, Loss: 1.5848762094974518, Final Batch Loss: 0.42013677954673767\n",
      "Epoch 2260, Loss: 1.5374713242053986, Final Batch Loss: 0.3223330080509186\n",
      "Epoch 2261, Loss: 1.508879542350769, Final Batch Loss: 0.3431495130062103\n",
      "Epoch 2262, Loss: 1.6016557812690735, Final Batch Loss: 0.3473002314567566\n",
      "Epoch 2263, Loss: 1.5027221739292145, Final Batch Loss: 0.39116039872169495\n",
      "Epoch 2264, Loss: 1.4592107236385345, Final Batch Loss: 0.3766777217388153\n",
      "Epoch 2265, Loss: 1.615515798330307, Final Batch Loss: 0.42284783720970154\n",
      "Epoch 2266, Loss: 1.539972335100174, Final Batch Loss: 0.3180612027645111\n",
      "Epoch 2267, Loss: 1.59601292014122, Final Batch Loss: 0.4512975811958313\n",
      "Epoch 2268, Loss: 1.6023409962654114, Final Batch Loss: 0.45588424801826477\n",
      "Epoch 2269, Loss: 1.467835247516632, Final Batch Loss: 0.3389875590801239\n",
      "Epoch 2270, Loss: 1.6965041160583496, Final Batch Loss: 0.4346815049648285\n",
      "Epoch 2271, Loss: 1.5282494723796844, Final Batch Loss: 0.3692895472049713\n",
      "Epoch 2272, Loss: 1.5406405627727509, Final Batch Loss: 0.39137494564056396\n",
      "Epoch 2273, Loss: 1.54984650015831, Final Batch Loss: 0.40748557448387146\n",
      "Epoch 2274, Loss: 1.4764049053192139, Final Batch Loss: 0.324084609746933\n",
      "Epoch 2275, Loss: 1.7089183628559113, Final Batch Loss: 0.47860273718833923\n",
      "Epoch 2276, Loss: 1.617544561624527, Final Batch Loss: 0.36661669611930847\n",
      "Epoch 2277, Loss: 1.6277155578136444, Final Batch Loss: 0.43973493576049805\n",
      "Epoch 2278, Loss: 1.5380377173423767, Final Batch Loss: 0.34932881593704224\n",
      "Epoch 2279, Loss: 1.538791298866272, Final Batch Loss: 0.3895004093647003\n",
      "Epoch 2280, Loss: 1.6018395721912384, Final Batch Loss: 0.47666144371032715\n",
      "Epoch 2281, Loss: 1.6467573046684265, Final Batch Loss: 0.34723982214927673\n",
      "Epoch 2282, Loss: 1.5162065029144287, Final Batch Loss: 0.2988315224647522\n",
      "Epoch 2283, Loss: 1.6105700731277466, Final Batch Loss: 0.4436159133911133\n",
      "Epoch 2284, Loss: 1.6469766199588776, Final Batch Loss: 0.4450915455818176\n",
      "Epoch 2285, Loss: 1.5090720355510712, Final Batch Loss: 0.43239378929138184\n",
      "Epoch 2286, Loss: 1.5359819829463959, Final Batch Loss: 0.42130541801452637\n",
      "Epoch 2287, Loss: 1.5278574228286743, Final Batch Loss: 0.41502830386161804\n",
      "Epoch 2288, Loss: 1.7473450601100922, Final Batch Loss: 0.5103156566619873\n",
      "Epoch 2289, Loss: 1.584648847579956, Final Batch Loss: 0.4089209735393524\n",
      "Epoch 2290, Loss: 1.6018934547901154, Final Batch Loss: 0.3654184937477112\n",
      "Epoch 2291, Loss: 1.629377007484436, Final Batch Loss: 0.43152472376823425\n",
      "Epoch 2292, Loss: 1.5199476182460785, Final Batch Loss: 0.3500893712043762\n",
      "Epoch 2293, Loss: 1.5572404265403748, Final Batch Loss: 0.3115982711315155\n",
      "Epoch 2294, Loss: 1.5424241125583649, Final Batch Loss: 0.4267124831676483\n",
      "Epoch 2295, Loss: 1.6606330275535583, Final Batch Loss: 0.38353288173675537\n",
      "Epoch 2296, Loss: 1.487965703010559, Final Batch Loss: 0.4154249429702759\n",
      "Epoch 2297, Loss: 1.5401609241962433, Final Batch Loss: 0.2889957129955292\n",
      "Epoch 2298, Loss: 1.483286291360855, Final Batch Loss: 0.27419739961624146\n",
      "Epoch 2299, Loss: 1.5015304684638977, Final Batch Loss: 0.35337427258491516\n",
      "Epoch 2300, Loss: 1.4836337566375732, Final Batch Loss: 0.35493770241737366\n",
      "Epoch 2301, Loss: 1.476485937833786, Final Batch Loss: 0.3167494833469391\n",
      "Epoch 2302, Loss: 1.6248588263988495, Final Batch Loss: 0.5018725395202637\n",
      "Epoch 2303, Loss: 1.6795574426651, Final Batch Loss: 0.4699045419692993\n",
      "Epoch 2304, Loss: 1.6837690770626068, Final Batch Loss: 0.4276055097579956\n",
      "Epoch 2305, Loss: 1.6028882563114166, Final Batch Loss: 0.3454902470111847\n",
      "Epoch 2306, Loss: 1.4303596913814545, Final Batch Loss: 0.31337055563926697\n",
      "Epoch 2307, Loss: 1.4558349549770355, Final Batch Loss: 0.3262840509414673\n",
      "Epoch 2308, Loss: 1.5900833308696747, Final Batch Loss: 0.4121203124523163\n",
      "Epoch 2309, Loss: 1.7923089563846588, Final Batch Loss: 0.3902064859867096\n",
      "Epoch 2310, Loss: 1.643282264471054, Final Batch Loss: 0.5335654616355896\n",
      "Epoch 2311, Loss: 1.558577448129654, Final Batch Loss: 0.38116613030433655\n",
      "Epoch 2312, Loss: 1.4124363958835602, Final Batch Loss: 0.2780662775039673\n",
      "Epoch 2313, Loss: 1.6782841980457306, Final Batch Loss: 0.48908671736717224\n",
      "Epoch 2314, Loss: 1.5779498517513275, Final Batch Loss: 0.4339475929737091\n",
      "Epoch 2315, Loss: 1.6674038767814636, Final Batch Loss: 0.4243493378162384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2316, Loss: 1.5926689207553864, Final Batch Loss: 0.3239206373691559\n",
      "Epoch 2317, Loss: 1.5622794032096863, Final Batch Loss: 0.4149641692638397\n",
      "Epoch 2318, Loss: 1.590652972459793, Final Batch Loss: 0.43203824758529663\n",
      "Epoch 2319, Loss: 1.4738613665103912, Final Batch Loss: 0.4127987027168274\n",
      "Epoch 2320, Loss: 1.601885586977005, Final Batch Loss: 0.42137888073921204\n",
      "Epoch 2321, Loss: 1.5774978697299957, Final Batch Loss: 0.4221242070198059\n",
      "Epoch 2322, Loss: 1.7529795169830322, Final Batch Loss: 0.42676740884780884\n",
      "Epoch 2323, Loss: 1.639075219631195, Final Batch Loss: 0.3679458796977997\n",
      "Epoch 2324, Loss: 1.6994819939136505, Final Batch Loss: 0.39424383640289307\n",
      "Epoch 2325, Loss: 1.5453692376613617, Final Batch Loss: 0.35997259616851807\n",
      "Epoch 2326, Loss: 1.441388189792633, Final Batch Loss: 0.31212642788887024\n",
      "Epoch 2327, Loss: 1.6934604942798615, Final Batch Loss: 0.42532363533973694\n",
      "Epoch 2328, Loss: 1.614458978176117, Final Batch Loss: 0.4563193917274475\n",
      "Epoch 2329, Loss: 1.7016263604164124, Final Batch Loss: 0.4310903549194336\n",
      "Epoch 2330, Loss: 1.5786773562431335, Final Batch Loss: 0.45534753799438477\n",
      "Epoch 2331, Loss: 1.462603211402893, Final Batch Loss: 0.3021048903465271\n",
      "Epoch 2332, Loss: 1.5779480040073395, Final Batch Loss: 0.43043020367622375\n",
      "Epoch 2333, Loss: 1.561549961566925, Final Batch Loss: 0.42715802788734436\n",
      "Epoch 2334, Loss: 1.4686559438705444, Final Batch Loss: 0.33435559272766113\n",
      "Epoch 2335, Loss: 1.6401863098144531, Final Batch Loss: 0.4197174906730652\n",
      "Epoch 2336, Loss: 1.5897572040557861, Final Batch Loss: 0.29788559675216675\n",
      "Epoch 2337, Loss: 1.573180764913559, Final Batch Loss: 0.3085480332374573\n",
      "Epoch 2338, Loss: 1.5131697058677673, Final Batch Loss: 0.3350555896759033\n",
      "Epoch 2339, Loss: 1.6027153432369232, Final Batch Loss: 0.4394819140434265\n",
      "Epoch 2340, Loss: 1.6158249378204346, Final Batch Loss: 0.43963825702667236\n",
      "Epoch 2341, Loss: 1.5955352187156677, Final Batch Loss: 0.3381253182888031\n",
      "Epoch 2342, Loss: 1.5879677832126617, Final Batch Loss: 0.42359042167663574\n",
      "Epoch 2343, Loss: 1.6439906656742096, Final Batch Loss: 0.47306689620018005\n",
      "Epoch 2344, Loss: 1.4182514548301697, Final Batch Loss: 0.2794497013092041\n",
      "Epoch 2345, Loss: 1.6298279464244843, Final Batch Loss: 0.4974605441093445\n",
      "Epoch 2346, Loss: 1.5508144199848175, Final Batch Loss: 0.4615560472011566\n",
      "Epoch 2347, Loss: 1.6266813278198242, Final Batch Loss: 0.40100741386413574\n",
      "Epoch 2348, Loss: 1.464178889989853, Final Batch Loss: 0.38102757930755615\n",
      "Epoch 2349, Loss: 1.5849723517894745, Final Batch Loss: 0.3708277940750122\n",
      "Epoch 2350, Loss: 1.5902794897556305, Final Batch Loss: 0.3372027575969696\n",
      "Epoch 2351, Loss: 1.5240775346755981, Final Batch Loss: 0.33315789699554443\n",
      "Epoch 2352, Loss: 1.5047895014286041, Final Batch Loss: 0.36367616057395935\n",
      "Epoch 2353, Loss: 1.5578187704086304, Final Batch Loss: 0.3955400586128235\n",
      "Epoch 2354, Loss: 1.4699570536613464, Final Batch Loss: 0.3990745544433594\n",
      "Epoch 2355, Loss: 1.5587376654148102, Final Batch Loss: 0.3890531659126282\n",
      "Epoch 2356, Loss: 1.668506681919098, Final Batch Loss: 0.4273400902748108\n",
      "Epoch 2357, Loss: 1.542375773191452, Final Batch Loss: 0.4988023340702057\n",
      "Epoch 2358, Loss: 1.4328420460224152, Final Batch Loss: 0.3624402582645416\n",
      "Epoch 2359, Loss: 1.5741385519504547, Final Batch Loss: 0.36640405654907227\n",
      "Epoch 2360, Loss: 1.4583554863929749, Final Batch Loss: 0.384475439786911\n",
      "Epoch 2361, Loss: 1.5233341753482819, Final Batch Loss: 0.4449988305568695\n",
      "Epoch 2362, Loss: 1.546351432800293, Final Batch Loss: 0.3870604634284973\n",
      "Epoch 2363, Loss: 1.5842546224594116, Final Batch Loss: 0.35193049907684326\n",
      "Epoch 2364, Loss: 1.4662768244743347, Final Batch Loss: 0.352909654378891\n",
      "Epoch 2365, Loss: 1.5193410515785217, Final Batch Loss: 0.33000051975250244\n",
      "Epoch 2366, Loss: 1.6523646712303162, Final Batch Loss: 0.47359153628349304\n",
      "Epoch 2367, Loss: 1.5792729556560516, Final Batch Loss: 0.3571535348892212\n",
      "Epoch 2368, Loss: 1.5765597522258759, Final Batch Loss: 0.36564016342163086\n",
      "Epoch 2369, Loss: 1.5686635971069336, Final Batch Loss: 0.5547207593917847\n",
      "Epoch 2370, Loss: 1.6430861055850983, Final Batch Loss: 0.38075128197669983\n",
      "Epoch 2371, Loss: 1.6026282012462616, Final Batch Loss: 0.43139153718948364\n",
      "Epoch 2372, Loss: 1.4631710946559906, Final Batch Loss: 0.33807283639907837\n",
      "Epoch 2373, Loss: 1.518809199333191, Final Batch Loss: 0.39808136224746704\n",
      "Epoch 2374, Loss: 1.5231073200702667, Final Batch Loss: 0.29363328218460083\n",
      "Epoch 2375, Loss: 1.4889111518859863, Final Batch Loss: 0.35382089018821716\n",
      "Epoch 2376, Loss: 1.6309632658958435, Final Batch Loss: 0.45879223942756653\n",
      "Epoch 2377, Loss: 1.5115233361721039, Final Batch Loss: 0.3821908235549927\n",
      "Epoch 2378, Loss: 1.5442849695682526, Final Batch Loss: 0.31958311796188354\n",
      "Epoch 2379, Loss: 1.5504758656024933, Final Batch Loss: 0.35356244444847107\n",
      "Epoch 2380, Loss: 1.586802363395691, Final Batch Loss: 0.43389666080474854\n",
      "Epoch 2381, Loss: 1.4771257638931274, Final Batch Loss: 0.3576841354370117\n",
      "Epoch 2382, Loss: 1.4266718029975891, Final Batch Loss: 0.397605299949646\n",
      "Epoch 2383, Loss: 1.689630538225174, Final Batch Loss: 0.4281262457370758\n",
      "Epoch 2384, Loss: 1.522736370563507, Final Batch Loss: 0.443099707365036\n",
      "Epoch 2385, Loss: 1.455983728170395, Final Batch Loss: 0.3252634108066559\n",
      "Epoch 2386, Loss: 1.5823068916797638, Final Batch Loss: 0.37150949239730835\n",
      "Epoch 2387, Loss: 1.4329902529716492, Final Batch Loss: 0.3421265184879303\n",
      "Epoch 2388, Loss: 1.4083426296710968, Final Batch Loss: 0.32158949971199036\n",
      "Epoch 2389, Loss: 1.5419620871543884, Final Batch Loss: 0.29543444514274597\n",
      "Epoch 2390, Loss: 1.5235668122768402, Final Batch Loss: 0.42071613669395447\n",
      "Epoch 2391, Loss: 1.6628101170063019, Final Batch Loss: 0.48493918776512146\n",
      "Epoch 2392, Loss: 1.6399872601032257, Final Batch Loss: 0.33549678325653076\n",
      "Epoch 2393, Loss: 1.490671992301941, Final Batch Loss: 0.3414332866668701\n",
      "Epoch 2394, Loss: 1.556891918182373, Final Batch Loss: 0.4013650417327881\n",
      "Epoch 2395, Loss: 1.480339765548706, Final Batch Loss: 0.2946174740791321\n",
      "Epoch 2396, Loss: 1.423615425825119, Final Batch Loss: 0.30627018213272095\n",
      "Epoch 2397, Loss: 1.5037421882152557, Final Batch Loss: 0.37864822149276733\n",
      "Epoch 2398, Loss: 1.4651680290699005, Final Batch Loss: 0.3691541850566864\n",
      "Epoch 2399, Loss: 1.4324102401733398, Final Batch Loss: 0.2802410423755646\n",
      "Epoch 2400, Loss: 1.4925944209098816, Final Batch Loss: 0.37049272656440735\n",
      "Epoch 2401, Loss: 1.4731420576572418, Final Batch Loss: 0.37509557604789734\n",
      "Epoch 2402, Loss: 1.6628222167491913, Final Batch Loss: 0.42873236536979675\n",
      "Epoch 2403, Loss: 1.5409352779388428, Final Batch Loss: 0.4124900698661804\n",
      "Epoch 2404, Loss: 1.4748909771442413, Final Batch Loss: 0.3851207494735718\n",
      "Epoch 2405, Loss: 1.5997170507907867, Final Batch Loss: 0.5155603289604187\n",
      "Epoch 2406, Loss: 1.4489220678806305, Final Batch Loss: 0.3044140636920929\n",
      "Epoch 2407, Loss: 1.385987251996994, Final Batch Loss: 0.35786911845207214\n",
      "Epoch 2408, Loss: 1.545666664838791, Final Batch Loss: 0.3611661493778229\n",
      "Epoch 2409, Loss: 1.5950318276882172, Final Batch Loss: 0.3189496099948883\n",
      "Epoch 2410, Loss: 1.6567558348178864, Final Batch Loss: 0.49487748742103577\n",
      "Epoch 2411, Loss: 1.4320479929447174, Final Batch Loss: 0.39167165756225586\n",
      "Epoch 2412, Loss: 1.4795236587524414, Final Batch Loss: 0.3909388780593872\n",
      "Epoch 2413, Loss: 1.4903377294540405, Final Batch Loss: 0.31282857060432434\n",
      "Epoch 2414, Loss: 1.4933031797409058, Final Batch Loss: 0.37548667192459106\n",
      "Epoch 2415, Loss: 1.6055003702640533, Final Batch Loss: 0.4135713279247284\n",
      "Epoch 2416, Loss: 1.4796715080738068, Final Batch Loss: 0.3239254653453827\n",
      "Epoch 2417, Loss: 1.6954125463962555, Final Batch Loss: 0.43679505586624146\n",
      "Epoch 2418, Loss: 1.4328237175941467, Final Batch Loss: 0.32967302203178406\n",
      "Epoch 2419, Loss: 1.6267297565937042, Final Batch Loss: 0.44329214096069336\n",
      "Epoch 2420, Loss: 1.4342980086803436, Final Batch Loss: 0.317789763212204\n",
      "Epoch 2421, Loss: 1.5078020989894867, Final Batch Loss: 0.4185248613357544\n",
      "Epoch 2422, Loss: 1.5793440341949463, Final Batch Loss: 0.4272141456604004\n",
      "Epoch 2423, Loss: 1.6396771371364594, Final Batch Loss: 0.40323585271835327\n",
      "Epoch 2424, Loss: 1.5850527584552765, Final Batch Loss: 0.40849292278289795\n",
      "Epoch 2425, Loss: 1.4358145892620087, Final Batch Loss: 0.3366137146949768\n",
      "Epoch 2426, Loss: 1.5245251953601837, Final Batch Loss: 0.29952898621559143\n",
      "Epoch 2427, Loss: 1.5976179242134094, Final Batch Loss: 0.39911508560180664\n",
      "Epoch 2428, Loss: 1.3908321261405945, Final Batch Loss: 0.337900847196579\n",
      "Epoch 2429, Loss: 1.5321597754955292, Final Batch Loss: 0.39200127124786377\n",
      "Epoch 2430, Loss: 1.5068362355232239, Final Batch Loss: 0.3745094835758209\n",
      "Epoch 2431, Loss: 1.6845958828926086, Final Batch Loss: 0.4309293329715729\n",
      "Epoch 2432, Loss: 1.5243550837039948, Final Batch Loss: 0.44575414061546326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2433, Loss: 1.5798360109329224, Final Batch Loss: 0.317976176738739\n",
      "Epoch 2434, Loss: 1.553522914648056, Final Batch Loss: 0.32218706607818604\n",
      "Epoch 2435, Loss: 1.5392404198646545, Final Batch Loss: 0.38724884390830994\n",
      "Epoch 2436, Loss: 1.4944708943367004, Final Batch Loss: 0.44493362307548523\n",
      "Epoch 2437, Loss: 1.3703701943159103, Final Batch Loss: 0.24602417647838593\n",
      "Epoch 2438, Loss: 1.4963224232196808, Final Batch Loss: 0.44284605979919434\n",
      "Epoch 2439, Loss: 1.5388667285442352, Final Batch Loss: 0.33989211916923523\n",
      "Epoch 2440, Loss: 1.592350721359253, Final Batch Loss: 0.3840811550617218\n",
      "Epoch 2441, Loss: 1.6486239731311798, Final Batch Loss: 0.47156795859336853\n",
      "Epoch 2442, Loss: 1.548336535692215, Final Batch Loss: 0.3509451448917389\n",
      "Epoch 2443, Loss: 1.5287292003631592, Final Batch Loss: 0.31470924615859985\n",
      "Epoch 2444, Loss: 1.6552429497241974, Final Batch Loss: 0.5164414644241333\n",
      "Epoch 2445, Loss: 1.568130075931549, Final Batch Loss: 0.3523053228855133\n",
      "Epoch 2446, Loss: 1.607374757528305, Final Batch Loss: 0.35191866755485535\n",
      "Epoch 2447, Loss: 1.5999045670032501, Final Batch Loss: 0.470518559217453\n",
      "Epoch 2448, Loss: 1.4748925864696503, Final Batch Loss: 0.3348939120769501\n",
      "Epoch 2449, Loss: 1.4703591167926788, Final Batch Loss: 0.34580883383750916\n",
      "Epoch 2450, Loss: 1.489520788192749, Final Batch Loss: 0.41314685344696045\n",
      "Epoch 2451, Loss: 1.6835148632526398, Final Batch Loss: 0.4535597562789917\n",
      "Epoch 2452, Loss: 1.5378426015377045, Final Batch Loss: 0.3585650324821472\n",
      "Epoch 2453, Loss: 1.455372303724289, Final Batch Loss: 0.38890954852104187\n",
      "Epoch 2454, Loss: 1.5652204751968384, Final Batch Loss: 0.4022079110145569\n",
      "Epoch 2455, Loss: 1.47550567984581, Final Batch Loss: 0.2827690839767456\n",
      "Epoch 2456, Loss: 1.5731711685657501, Final Batch Loss: 0.4050845503807068\n",
      "Epoch 2457, Loss: 1.4944708049297333, Final Batch Loss: 0.32503288984298706\n",
      "Epoch 2458, Loss: 1.4454612135887146, Final Batch Loss: 0.33079051971435547\n",
      "Epoch 2459, Loss: 1.54898139834404, Final Batch Loss: 0.35145342350006104\n",
      "Epoch 2460, Loss: 1.4710946381092072, Final Batch Loss: 0.386042058467865\n",
      "Epoch 2461, Loss: 1.7394651174545288, Final Batch Loss: 0.5507152676582336\n",
      "Epoch 2462, Loss: 1.495601326227188, Final Batch Loss: 0.266859769821167\n",
      "Epoch 2463, Loss: 1.4315359592437744, Final Batch Loss: 0.29070666432380676\n",
      "Epoch 2464, Loss: 1.702529102563858, Final Batch Loss: 0.3980329632759094\n",
      "Epoch 2465, Loss: 1.5241906940937042, Final Batch Loss: 0.39556291699409485\n",
      "Epoch 2466, Loss: 1.5569189488887787, Final Batch Loss: 0.3542967140674591\n",
      "Epoch 2467, Loss: 1.5213104784488678, Final Batch Loss: 0.37741631269454956\n",
      "Epoch 2468, Loss: 1.5486234426498413, Final Batch Loss: 0.37114137411117554\n",
      "Epoch 2469, Loss: 1.6360255777835846, Final Batch Loss: 0.41660574078559875\n",
      "Epoch 2470, Loss: 1.4531841576099396, Final Batch Loss: 0.29348671436309814\n",
      "Epoch 2471, Loss: 1.605322778224945, Final Batch Loss: 0.44233864545822144\n",
      "Epoch 2472, Loss: 1.5194354057312012, Final Batch Loss: 0.37151357531547546\n",
      "Epoch 2473, Loss: 1.4661305248737335, Final Batch Loss: 0.37269362807273865\n",
      "Epoch 2474, Loss: 1.4397522807121277, Final Batch Loss: 0.36019110679626465\n",
      "Epoch 2475, Loss: 1.56072798371315, Final Batch Loss: 0.4094315469264984\n",
      "Epoch 2476, Loss: 1.4659566283226013, Final Batch Loss: 0.4177737236022949\n",
      "Epoch 2477, Loss: 1.6252823770046234, Final Batch Loss: 0.4640073776245117\n",
      "Epoch 2478, Loss: 1.5473571717739105, Final Batch Loss: 0.3375079333782196\n",
      "Epoch 2479, Loss: 1.6364506483078003, Final Batch Loss: 0.4568599462509155\n",
      "Epoch 2480, Loss: 1.4064428210258484, Final Batch Loss: 0.2696194648742676\n",
      "Epoch 2481, Loss: 1.538185715675354, Final Batch Loss: 0.4413544237613678\n",
      "Epoch 2482, Loss: 1.5440861284732819, Final Batch Loss: 0.3992682695388794\n",
      "Epoch 2483, Loss: 1.52975794672966, Final Batch Loss: 0.4252162575721741\n",
      "Epoch 2484, Loss: 1.5454506874084473, Final Batch Loss: 0.4354160726070404\n",
      "Epoch 2485, Loss: 1.5325540900230408, Final Batch Loss: 0.43941935896873474\n",
      "Epoch 2486, Loss: 1.6148993968963623, Final Batch Loss: 0.4119923710823059\n",
      "Epoch 2487, Loss: 1.6592980325222015, Final Batch Loss: 0.36248162388801575\n",
      "Epoch 2488, Loss: 1.5683508217334747, Final Batch Loss: 0.3966737985610962\n",
      "Epoch 2489, Loss: 1.5968968570232391, Final Batch Loss: 0.4834524095058441\n",
      "Epoch 2490, Loss: 1.5387439727783203, Final Batch Loss: 0.40896520018577576\n",
      "Epoch 2491, Loss: 1.5697314143180847, Final Batch Loss: 0.4177984297275543\n",
      "Epoch 2492, Loss: 1.5938454866409302, Final Batch Loss: 0.4236544370651245\n",
      "Epoch 2493, Loss: 1.5957030653953552, Final Batch Loss: 0.4094068109989166\n",
      "Epoch 2494, Loss: 1.6243235766887665, Final Batch Loss: 0.4034571051597595\n",
      "Epoch 2495, Loss: 1.4263124465942383, Final Batch Loss: 0.33513638377189636\n",
      "Epoch 2496, Loss: 1.5495551228523254, Final Batch Loss: 0.4618367552757263\n",
      "Epoch 2497, Loss: 1.5420318245887756, Final Batch Loss: 0.3169389069080353\n",
      "Epoch 2498, Loss: 1.5476234257221222, Final Batch Loss: 0.38496196269989014\n",
      "Epoch 2499, Loss: 1.5705845355987549, Final Batch Loss: 0.41649556159973145\n",
      "Epoch 2500, Loss: 1.4180972278118134, Final Batch Loss: 0.3649827539920807\n",
      "Epoch 2501, Loss: 1.4804486483335495, Final Batch Loss: 0.24754150211811066\n",
      "Epoch 2502, Loss: 1.4899625182151794, Final Batch Loss: 0.3639034926891327\n",
      "Epoch 2503, Loss: 1.4947240948677063, Final Batch Loss: 0.3668314218521118\n",
      "Epoch 2504, Loss: 1.5737616121768951, Final Batch Loss: 0.35828733444213867\n",
      "Epoch 2505, Loss: 1.6427424550056458, Final Batch Loss: 0.541243314743042\n",
      "Epoch 2506, Loss: 1.4532729387283325, Final Batch Loss: 0.3641214370727539\n",
      "Epoch 2507, Loss: 1.5698660910129547, Final Batch Loss: 0.3638618290424347\n",
      "Epoch 2508, Loss: 1.5167518258094788, Final Batch Loss: 0.3648335337638855\n",
      "Epoch 2509, Loss: 1.5117113888263702, Final Batch Loss: 0.3619757294654846\n",
      "Epoch 2510, Loss: 1.6041601598262787, Final Batch Loss: 0.3750092089176178\n",
      "Epoch 2511, Loss: 1.5219289064407349, Final Batch Loss: 0.42774125933647156\n",
      "Epoch 2512, Loss: 1.456211805343628, Final Batch Loss: 0.34217020869255066\n",
      "Epoch 2513, Loss: 1.5460604429244995, Final Batch Loss: 0.40263012051582336\n",
      "Epoch 2514, Loss: 1.6862044632434845, Final Batch Loss: 0.4933939576148987\n",
      "Epoch 2515, Loss: 1.4972416162490845, Final Batch Loss: 0.42313018441200256\n",
      "Epoch 2516, Loss: 1.4186589419841766, Final Batch Loss: 0.31202787160873413\n",
      "Epoch 2517, Loss: 1.4813383221626282, Final Batch Loss: 0.33619603514671326\n",
      "Epoch 2518, Loss: 1.4828082025051117, Final Batch Loss: 0.32748568058013916\n",
      "Epoch 2519, Loss: 1.5001917481422424, Final Batch Loss: 0.3609996736049652\n",
      "Epoch 2520, Loss: 1.5992830693721771, Final Batch Loss: 0.44119197130203247\n",
      "Epoch 2521, Loss: 1.753788411617279, Final Batch Loss: 0.4583040177822113\n",
      "Epoch 2522, Loss: 1.6119739413261414, Final Batch Loss: 0.371542364358902\n",
      "Epoch 2523, Loss: 1.4493810534477234, Final Batch Loss: 0.3521222472190857\n",
      "Epoch 2524, Loss: 1.489128440618515, Final Batch Loss: 0.3346139192581177\n",
      "Epoch 2525, Loss: 1.492752581834793, Final Batch Loss: 0.3663998246192932\n",
      "Epoch 2526, Loss: 1.6243330836296082, Final Batch Loss: 0.41637566685676575\n",
      "Epoch 2527, Loss: 1.5836255252361298, Final Batch Loss: 0.36965975165367126\n",
      "Epoch 2528, Loss: 1.5195981562137604, Final Batch Loss: 0.3419094383716583\n",
      "Epoch 2529, Loss: 1.360121101140976, Final Batch Loss: 0.29104167222976685\n",
      "Epoch 2530, Loss: 1.481481522321701, Final Batch Loss: 0.345853716135025\n",
      "Epoch 2531, Loss: 1.6183432340621948, Final Batch Loss: 0.3692869246006012\n",
      "Epoch 2532, Loss: 1.518530696630478, Final Batch Loss: 0.3661099970340729\n",
      "Epoch 2533, Loss: 1.5192744135856628, Final Batch Loss: 0.3705475628376007\n",
      "Epoch 2534, Loss: 1.577385812997818, Final Batch Loss: 0.4372361898422241\n",
      "Epoch 2535, Loss: 1.5311289727687836, Final Batch Loss: 0.4407348036766052\n",
      "Epoch 2536, Loss: 1.4467737674713135, Final Batch Loss: 0.2902395725250244\n",
      "Epoch 2537, Loss: 1.5265050828456879, Final Batch Loss: 0.34305137395858765\n",
      "Epoch 2538, Loss: 1.5864497423171997, Final Batch Loss: 0.46480172872543335\n",
      "Epoch 2539, Loss: 1.4915826320648193, Final Batch Loss: 0.4009426236152649\n",
      "Epoch 2540, Loss: 1.5864288806915283, Final Batch Loss: 0.4383758008480072\n",
      "Epoch 2541, Loss: 1.4642502069473267, Final Batch Loss: 0.3280988335609436\n",
      "Epoch 2542, Loss: 1.5759020745754242, Final Batch Loss: 0.47818464040756226\n",
      "Epoch 2543, Loss: 1.4416880011558533, Final Batch Loss: 0.30305448174476624\n",
      "Epoch 2544, Loss: 1.488037794828415, Final Batch Loss: 0.3982976973056793\n",
      "Epoch 2545, Loss: 1.5372913479804993, Final Batch Loss: 0.4194163382053375\n",
      "Epoch 2546, Loss: 1.5323482751846313, Final Batch Loss: 0.345090389251709\n",
      "Epoch 2547, Loss: 1.5778390169143677, Final Batch Loss: 0.3739525377750397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2548, Loss: 1.645367681980133, Final Batch Loss: 0.37543293833732605\n",
      "Epoch 2549, Loss: 1.5790391862392426, Final Batch Loss: 0.48124048113822937\n",
      "Epoch 2550, Loss: 1.5714478194713593, Final Batch Loss: 0.38596537709236145\n",
      "Epoch 2551, Loss: 1.5720078945159912, Final Batch Loss: 0.369467169046402\n",
      "Epoch 2552, Loss: 1.4534240663051605, Final Batch Loss: 0.44592756032943726\n",
      "Epoch 2553, Loss: 1.4264476299285889, Final Batch Loss: 0.35021016001701355\n",
      "Epoch 2554, Loss: 1.5614542663097382, Final Batch Loss: 0.3932555019855499\n",
      "Epoch 2555, Loss: 1.487323373556137, Final Batch Loss: 0.33426764607429504\n",
      "Epoch 2556, Loss: 1.4049687683582306, Final Batch Loss: 0.2539474070072174\n",
      "Epoch 2557, Loss: 1.5154449045658112, Final Batch Loss: 0.3674221932888031\n",
      "Epoch 2558, Loss: 1.6171301305294037, Final Batch Loss: 0.6267197132110596\n",
      "Epoch 2559, Loss: 1.426685780286789, Final Batch Loss: 0.33177393674850464\n",
      "Epoch 2560, Loss: 1.578910380601883, Final Batch Loss: 0.3949011564254761\n",
      "Epoch 2561, Loss: 1.4164403080940247, Final Batch Loss: 0.31527581810951233\n",
      "Epoch 2562, Loss: 1.498069852590561, Final Batch Loss: 0.3610992431640625\n",
      "Epoch 2563, Loss: 1.5430227220058441, Final Batch Loss: 0.4154855012893677\n",
      "Epoch 2564, Loss: 1.4741605818271637, Final Batch Loss: 0.2861739993095398\n",
      "Epoch 2565, Loss: 1.6942781507968903, Final Batch Loss: 0.35059982538223267\n",
      "Epoch 2566, Loss: 1.497869610786438, Final Batch Loss: 0.3857518434524536\n",
      "Epoch 2567, Loss: 1.589071661233902, Final Batch Loss: 0.4428216516971588\n",
      "Epoch 2568, Loss: 1.737181395292282, Final Batch Loss: 0.5707100033760071\n",
      "Epoch 2569, Loss: 1.52567857503891, Final Batch Loss: 0.33025577664375305\n",
      "Epoch 2570, Loss: 1.531262069940567, Final Batch Loss: 0.3635382354259491\n",
      "Epoch 2571, Loss: 1.5480102896690369, Final Batch Loss: 0.44443491101264954\n",
      "Epoch 2572, Loss: 1.4916515052318573, Final Batch Loss: 0.2864275276660919\n",
      "Epoch 2573, Loss: 1.455389142036438, Final Batch Loss: 0.3019537925720215\n",
      "Epoch 2574, Loss: 1.5044061839580536, Final Batch Loss: 0.3314349949359894\n",
      "Epoch 2575, Loss: 1.545235812664032, Final Batch Loss: 0.45133131742477417\n",
      "Epoch 2576, Loss: 1.586038738489151, Final Batch Loss: 0.44975587725639343\n",
      "Epoch 2577, Loss: 1.3788219094276428, Final Batch Loss: 0.3751261234283447\n",
      "Epoch 2578, Loss: 1.4118365943431854, Final Batch Loss: 0.29554152488708496\n",
      "Epoch 2579, Loss: 1.4700740575790405, Final Batch Loss: 0.3504813015460968\n",
      "Epoch 2580, Loss: 1.5354548692703247, Final Batch Loss: 0.3136664628982544\n",
      "Epoch 2581, Loss: 1.4767425060272217, Final Batch Loss: 0.41063764691352844\n",
      "Epoch 2582, Loss: 1.4486961364746094, Final Batch Loss: 0.33877214789390564\n",
      "Epoch 2583, Loss: 1.5432174503803253, Final Batch Loss: 0.38564443588256836\n",
      "Epoch 2584, Loss: 1.4131757616996765, Final Batch Loss: 0.3143255114555359\n",
      "Epoch 2585, Loss: 1.5647756457328796, Final Batch Loss: 0.4942949414253235\n",
      "Epoch 2586, Loss: 1.553823471069336, Final Batch Loss: 0.4504697620868683\n",
      "Epoch 2587, Loss: 1.6579065024852753, Final Batch Loss: 0.42615920305252075\n",
      "Epoch 2588, Loss: 1.640670359134674, Final Batch Loss: 0.5240573287010193\n",
      "Epoch 2589, Loss: 1.4459038078784943, Final Batch Loss: 0.2808872163295746\n",
      "Epoch 2590, Loss: 1.3827466666698456, Final Batch Loss: 0.26193293929100037\n",
      "Epoch 2591, Loss: 1.4646528363227844, Final Batch Loss: 0.36784079670906067\n",
      "Epoch 2592, Loss: 1.4574666023254395, Final Batch Loss: 0.3959828317165375\n",
      "Epoch 2593, Loss: 1.503966897726059, Final Batch Loss: 0.4458311200141907\n",
      "Epoch 2594, Loss: 1.4785219430923462, Final Batch Loss: 0.3404172956943512\n",
      "Epoch 2595, Loss: 1.4140446484088898, Final Batch Loss: 0.3421257734298706\n",
      "Epoch 2596, Loss: 1.612370640039444, Final Batch Loss: 0.4194532036781311\n",
      "Epoch 2597, Loss: 1.5554781258106232, Final Batch Loss: 0.42693498730659485\n",
      "Epoch 2598, Loss: 1.4835470914840698, Final Batch Loss: 0.3998713791370392\n",
      "Epoch 2599, Loss: 1.3079407215118408, Final Batch Loss: 0.21897709369659424\n",
      "Epoch 2600, Loss: 1.4933333694934845, Final Batch Loss: 0.2597323954105377\n",
      "Epoch 2601, Loss: 1.4689617455005646, Final Batch Loss: 0.34323441982269287\n",
      "Epoch 2602, Loss: 1.4488459527492523, Final Batch Loss: 0.3446362018585205\n",
      "Epoch 2603, Loss: 1.446653813123703, Final Batch Loss: 0.42331644892692566\n",
      "Epoch 2604, Loss: 1.4382952451705933, Final Batch Loss: 0.2528584599494934\n",
      "Epoch 2605, Loss: 1.4202569127082825, Final Batch Loss: 0.37832385301589966\n",
      "Epoch 2606, Loss: 1.4132722616195679, Final Batch Loss: 0.3163992166519165\n",
      "Epoch 2607, Loss: 1.495249181985855, Final Batch Loss: 0.3575294017791748\n",
      "Epoch 2608, Loss: 1.4151794016361237, Final Batch Loss: 0.35835379362106323\n",
      "Epoch 2609, Loss: 1.4231134951114655, Final Batch Loss: 0.29916882514953613\n",
      "Epoch 2610, Loss: 1.5472901463508606, Final Batch Loss: 0.3829597532749176\n",
      "Epoch 2611, Loss: 1.5895180404186249, Final Batch Loss: 0.4654490053653717\n",
      "Epoch 2612, Loss: 1.4882563352584839, Final Batch Loss: 0.4219913184642792\n",
      "Epoch 2613, Loss: 1.3710319697856903, Final Batch Loss: 0.3238774836063385\n",
      "Epoch 2614, Loss: 1.3829123377799988, Final Batch Loss: 0.3889882266521454\n",
      "Epoch 2615, Loss: 1.5014324188232422, Final Batch Loss: 0.42210280895233154\n",
      "Epoch 2616, Loss: 1.5343568623065948, Final Batch Loss: 0.39866554737091064\n",
      "Epoch 2617, Loss: 1.5045151114463806, Final Batch Loss: 0.4179927110671997\n",
      "Epoch 2618, Loss: 1.58854940533638, Final Batch Loss: 0.4214571416378021\n",
      "Epoch 2619, Loss: 1.5211879014968872, Final Batch Loss: 0.33410319685935974\n",
      "Epoch 2620, Loss: 1.5907958447933197, Final Batch Loss: 0.4929662048816681\n",
      "Epoch 2621, Loss: 1.5242908596992493, Final Batch Loss: 0.4646962285041809\n",
      "Epoch 2622, Loss: 1.431921362876892, Final Batch Loss: 0.3441236615180969\n",
      "Epoch 2623, Loss: 1.4208946228027344, Final Batch Loss: 0.37886422872543335\n",
      "Epoch 2624, Loss: 1.4855786263942719, Final Batch Loss: 0.4283362329006195\n",
      "Epoch 2625, Loss: 1.540252447128296, Final Batch Loss: 0.3823813796043396\n",
      "Epoch 2626, Loss: 1.3860755264759064, Final Batch Loss: 0.30302542448043823\n",
      "Epoch 2627, Loss: 1.4484613537788391, Final Batch Loss: 0.33614182472229004\n",
      "Epoch 2628, Loss: 1.3924274444580078, Final Batch Loss: 0.3368825614452362\n",
      "Epoch 2629, Loss: 1.384933054447174, Final Batch Loss: 0.32018524408340454\n",
      "Epoch 2630, Loss: 1.4572787880897522, Final Batch Loss: 0.3604257106781006\n",
      "Epoch 2631, Loss: 1.5578923225402832, Final Batch Loss: 0.41275396943092346\n",
      "Epoch 2632, Loss: 1.5537348091602325, Final Batch Loss: 0.4379991590976715\n",
      "Epoch 2633, Loss: 1.6986899971961975, Final Batch Loss: 0.5591791868209839\n",
      "Epoch 2634, Loss: 1.4787741601467133, Final Batch Loss: 0.30348774790763855\n",
      "Epoch 2635, Loss: 1.505958467721939, Final Batch Loss: 0.34168627858161926\n",
      "Epoch 2636, Loss: 1.4369030892848969, Final Batch Loss: 0.37526383996009827\n",
      "Epoch 2637, Loss: 1.688933789730072, Final Batch Loss: 0.4376286566257477\n",
      "Epoch 2638, Loss: 1.508790910243988, Final Batch Loss: 0.3673057556152344\n",
      "Epoch 2639, Loss: 1.5538505911827087, Final Batch Loss: 0.3199610710144043\n",
      "Epoch 2640, Loss: 1.5191218256950378, Final Batch Loss: 0.38499316573143005\n",
      "Epoch 2641, Loss: 1.4648449420928955, Final Batch Loss: 0.31295379996299744\n",
      "Epoch 2642, Loss: 1.4770221710205078, Final Batch Loss: 0.277761846780777\n",
      "Epoch 2643, Loss: 1.465458869934082, Final Batch Loss: 0.37638214230537415\n",
      "Epoch 2644, Loss: 1.5111610293388367, Final Batch Loss: 0.3190189003944397\n",
      "Epoch 2645, Loss: 1.379385381937027, Final Batch Loss: 0.3673301339149475\n",
      "Epoch 2646, Loss: 1.5916004478931427, Final Batch Loss: 0.4682660698890686\n",
      "Epoch 2647, Loss: 1.4579358398914337, Final Batch Loss: 0.3795281648635864\n",
      "Epoch 2648, Loss: 1.4467527568340302, Final Batch Loss: 0.4279363751411438\n",
      "Epoch 2649, Loss: 1.4401540160179138, Final Batch Loss: 0.3589082956314087\n",
      "Epoch 2650, Loss: 1.4098045825958252, Final Batch Loss: 0.29858291149139404\n",
      "Epoch 2651, Loss: 1.4388729631900787, Final Batch Loss: 0.3680585026741028\n",
      "Epoch 2652, Loss: 1.5723768472671509, Final Batch Loss: 0.447322815656662\n",
      "Epoch 2653, Loss: 1.424194097518921, Final Batch Loss: 0.24798986315727234\n",
      "Epoch 2654, Loss: 1.5320772975683212, Final Batch Loss: 0.22912831604480743\n",
      "Epoch 2655, Loss: 1.397611528635025, Final Batch Loss: 0.2861659824848175\n",
      "Epoch 2656, Loss: 1.5327740609645844, Final Batch Loss: 0.4311394691467285\n",
      "Epoch 2657, Loss: 1.4323199689388275, Final Batch Loss: 0.33989882469177246\n",
      "Epoch 2658, Loss: 1.5470322370529175, Final Batch Loss: 0.395770400762558\n",
      "Epoch 2659, Loss: 1.575607717037201, Final Batch Loss: 0.42310911417007446\n",
      "Epoch 2660, Loss: 1.430214524269104, Final Batch Loss: 0.34760570526123047\n",
      "Epoch 2661, Loss: 1.4616421163082123, Final Batch Loss: 0.31737929582595825\n",
      "Epoch 2662, Loss: 1.5138777494430542, Final Batch Loss: 0.42107516527175903\n",
      "Epoch 2663, Loss: 1.3550310730934143, Final Batch Loss: 0.3000243902206421\n",
      "Epoch 2664, Loss: 1.3970922231674194, Final Batch Loss: 0.3001639246940613\n",
      "Epoch 2665, Loss: 1.5328353345394135, Final Batch Loss: 0.3163125216960907\n",
      "Epoch 2666, Loss: 1.360052227973938, Final Batch Loss: 0.298641562461853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2667, Loss: 1.4359764158725739, Final Batch Loss: 0.4082569479942322\n",
      "Epoch 2668, Loss: 1.5035783648490906, Final Batch Loss: 0.3810705244541168\n",
      "Epoch 2669, Loss: 1.3381866812705994, Final Batch Loss: 0.3208487629890442\n",
      "Epoch 2670, Loss: 1.4205325543880463, Final Batch Loss: 0.42917460203170776\n",
      "Epoch 2671, Loss: 1.5343648493289948, Final Batch Loss: 0.38844984769821167\n",
      "Epoch 2672, Loss: 1.5237363874912262, Final Batch Loss: 0.36097946763038635\n",
      "Epoch 2673, Loss: 1.4677427113056183, Final Batch Loss: 0.3498327136039734\n",
      "Epoch 2674, Loss: 1.4771414697170258, Final Batch Loss: 0.37861424684524536\n",
      "Epoch 2675, Loss: 1.4575211107730865, Final Batch Loss: 0.3755349814891815\n",
      "Epoch 2676, Loss: 1.56659334897995, Final Batch Loss: 0.4020555317401886\n",
      "Epoch 2677, Loss: 1.5850553512573242, Final Batch Loss: 0.3981344997882843\n",
      "Epoch 2678, Loss: 1.5103592574596405, Final Batch Loss: 0.37454289197921753\n",
      "Epoch 2679, Loss: 1.479495882987976, Final Batch Loss: 0.29384058713912964\n",
      "Epoch 2680, Loss: 1.4237057268619537, Final Batch Loss: 0.346113383769989\n",
      "Epoch 2681, Loss: 1.5355713665485382, Final Batch Loss: 0.40198996663093567\n",
      "Epoch 2682, Loss: 1.351652830839157, Final Batch Loss: 0.3044518530368805\n",
      "Epoch 2683, Loss: 1.4644930064678192, Final Batch Loss: 0.26995399594306946\n",
      "Epoch 2684, Loss: 1.5998350381851196, Final Batch Loss: 0.4093746840953827\n",
      "Epoch 2685, Loss: 1.4771159887313843, Final Batch Loss: 0.40919971466064453\n",
      "Epoch 2686, Loss: 1.5156208276748657, Final Batch Loss: 0.38941264152526855\n",
      "Epoch 2687, Loss: 1.6659785211086273, Final Batch Loss: 0.4710807502269745\n",
      "Epoch 2688, Loss: 1.5219743251800537, Final Batch Loss: 0.4000515043735504\n",
      "Epoch 2689, Loss: 1.5068239867687225, Final Batch Loss: 0.4015192687511444\n",
      "Epoch 2690, Loss: 1.4307996928691864, Final Batch Loss: 0.3979945480823517\n",
      "Epoch 2691, Loss: 1.4986484348773956, Final Batch Loss: 0.3446546196937561\n",
      "Epoch 2692, Loss: 1.524659901857376, Final Batch Loss: 0.36615556478500366\n",
      "Epoch 2693, Loss: 1.5692118108272552, Final Batch Loss: 0.4507593512535095\n",
      "Epoch 2694, Loss: 1.539717197418213, Final Batch Loss: 0.43556228280067444\n",
      "Epoch 2695, Loss: 1.4504877924919128, Final Batch Loss: 0.3020108640193939\n",
      "Epoch 2696, Loss: 1.5506068766117096, Final Batch Loss: 0.3659362196922302\n",
      "Epoch 2697, Loss: 1.4429570734500885, Final Batch Loss: 0.2647806406021118\n",
      "Epoch 2698, Loss: 1.5738384425640106, Final Batch Loss: 0.46917784214019775\n",
      "Epoch 2699, Loss: 1.5030736029148102, Final Batch Loss: 0.4420335292816162\n",
      "Epoch 2700, Loss: 1.520886093378067, Final Batch Loss: 0.4083840548992157\n",
      "Epoch 2701, Loss: 1.496410459280014, Final Batch Loss: 0.35423338413238525\n",
      "Epoch 2702, Loss: 1.6126783192157745, Final Batch Loss: 0.4374443590641022\n",
      "Epoch 2703, Loss: 1.5085205137729645, Final Batch Loss: 0.5345132946968079\n",
      "Epoch 2704, Loss: 1.4128116965293884, Final Batch Loss: 0.28002533316612244\n",
      "Epoch 2705, Loss: 1.5934634804725647, Final Batch Loss: 0.43115535378456116\n",
      "Epoch 2706, Loss: 1.4927419424057007, Final Batch Loss: 0.39921343326568604\n",
      "Epoch 2707, Loss: 1.466655433177948, Final Batch Loss: 0.3062521517276764\n",
      "Epoch 2708, Loss: 1.530243456363678, Final Batch Loss: 0.38989758491516113\n",
      "Epoch 2709, Loss: 1.5188196897506714, Final Batch Loss: 0.3753940463066101\n",
      "Epoch 2710, Loss: 1.457762211561203, Final Batch Loss: 0.34801995754241943\n",
      "Epoch 2711, Loss: 1.4933306872844696, Final Batch Loss: 0.43832075595855713\n",
      "Epoch 2712, Loss: 1.5889522731304169, Final Batch Loss: 0.38885781168937683\n",
      "Epoch 2713, Loss: 1.7052765786647797, Final Batch Loss: 0.4438793957233429\n",
      "Epoch 2714, Loss: 1.3991455733776093, Final Batch Loss: 0.2759121060371399\n",
      "Epoch 2715, Loss: 1.4497692584991455, Final Batch Loss: 0.36323317885398865\n",
      "Epoch 2716, Loss: 1.4516481459140778, Final Batch Loss: 0.32452431321144104\n",
      "Epoch 2717, Loss: 1.5286725163459778, Final Batch Loss: 0.2813195288181305\n",
      "Epoch 2718, Loss: 1.5261308252811432, Final Batch Loss: 0.3503430187702179\n",
      "Epoch 2719, Loss: 1.4434845745563507, Final Batch Loss: 0.36029937863349915\n",
      "Epoch 2720, Loss: 1.457997739315033, Final Batch Loss: 0.30623725056648254\n",
      "Epoch 2721, Loss: 1.3727412223815918, Final Batch Loss: 0.28637340664863586\n",
      "Epoch 2722, Loss: 1.561397224664688, Final Batch Loss: 0.4310365915298462\n",
      "Epoch 2723, Loss: 1.525898814201355, Final Batch Loss: 0.41473671793937683\n",
      "Epoch 2724, Loss: 1.5518377125263214, Final Batch Loss: 0.3021099865436554\n",
      "Epoch 2725, Loss: 1.504808932542801, Final Batch Loss: 0.4458797574043274\n",
      "Epoch 2726, Loss: 1.3581264913082123, Final Batch Loss: 0.25021103024482727\n",
      "Epoch 2727, Loss: 1.437632828950882, Final Batch Loss: 0.3239687383174896\n",
      "Epoch 2728, Loss: 1.590410202741623, Final Batch Loss: 0.40477046370506287\n",
      "Epoch 2729, Loss: 1.514075130224228, Final Batch Loss: 0.35453590750694275\n",
      "Epoch 2730, Loss: 1.5064149796962738, Final Batch Loss: 0.362403005361557\n",
      "Epoch 2731, Loss: 1.4327414333820343, Final Batch Loss: 0.307177871465683\n",
      "Epoch 2732, Loss: 1.4745765030384064, Final Batch Loss: 0.2849609851837158\n",
      "Epoch 2733, Loss: 1.5810109078884125, Final Batch Loss: 0.4436742067337036\n",
      "Epoch 2734, Loss: 1.5418292582035065, Final Batch Loss: 0.3496952950954437\n",
      "Epoch 2735, Loss: 1.397229254245758, Final Batch Loss: 0.322450190782547\n",
      "Epoch 2736, Loss: 1.5863021612167358, Final Batch Loss: 0.3964504599571228\n",
      "Epoch 2737, Loss: 1.4584620296955109, Final Batch Loss: 0.33511513471603394\n",
      "Epoch 2738, Loss: 1.4925167560577393, Final Batch Loss: 0.2957806885242462\n",
      "Epoch 2739, Loss: 1.4134730696678162, Final Batch Loss: 0.329283207654953\n",
      "Epoch 2740, Loss: 1.4192082583904266, Final Batch Loss: 0.2886471152305603\n",
      "Epoch 2741, Loss: 1.520799845457077, Final Batch Loss: 0.3987170159816742\n",
      "Epoch 2742, Loss: 1.4968861043453217, Final Batch Loss: 0.38802042603492737\n",
      "Epoch 2743, Loss: 1.3827034831047058, Final Batch Loss: 0.34281477332115173\n",
      "Epoch 2744, Loss: 1.3830888271331787, Final Batch Loss: 0.3253307640552521\n",
      "Epoch 2745, Loss: 1.4471652209758759, Final Batch Loss: 0.3812709152698517\n",
      "Epoch 2746, Loss: 1.3555138111114502, Final Batch Loss: 0.35980263352394104\n",
      "Epoch 2747, Loss: 1.498460054397583, Final Batch Loss: 0.3675282597541809\n",
      "Epoch 2748, Loss: 1.552637755870819, Final Batch Loss: 0.30260321497917175\n",
      "Epoch 2749, Loss: 1.5580862164497375, Final Batch Loss: 0.34920063614845276\n",
      "Epoch 2750, Loss: 1.439525067806244, Final Batch Loss: 0.32460659742355347\n",
      "Epoch 2751, Loss: 1.4812645316123962, Final Batch Loss: 0.4233827590942383\n",
      "Epoch 2752, Loss: 1.5686995685100555, Final Batch Loss: 0.39455655217170715\n",
      "Epoch 2753, Loss: 1.5915547311306, Final Batch Loss: 0.4088447391986847\n",
      "Epoch 2754, Loss: 1.5181110799312592, Final Batch Loss: 0.3669562339782715\n",
      "Epoch 2755, Loss: 1.5864265859127045, Final Batch Loss: 0.4127182960510254\n",
      "Epoch 2756, Loss: 1.4797734320163727, Final Batch Loss: 0.3762480914592743\n",
      "Epoch 2757, Loss: 1.5242068469524384, Final Batch Loss: 0.33787232637405396\n",
      "Epoch 2758, Loss: 1.5283497273921967, Final Batch Loss: 0.42436927556991577\n",
      "Epoch 2759, Loss: 1.5284441411495209, Final Batch Loss: 0.48488396406173706\n",
      "Epoch 2760, Loss: 1.5181068181991577, Final Batch Loss: 0.36486557126045227\n",
      "Epoch 2761, Loss: 1.4454744756221771, Final Batch Loss: 0.317034512758255\n",
      "Epoch 2762, Loss: 1.545853316783905, Final Batch Loss: 0.3089855909347534\n",
      "Epoch 2763, Loss: 1.4226883053779602, Final Batch Loss: 0.3880545198917389\n",
      "Epoch 2764, Loss: 1.5306584537029266, Final Batch Loss: 0.3005933463573456\n",
      "Epoch 2765, Loss: 1.4161347150802612, Final Batch Loss: 0.35471782088279724\n",
      "Epoch 2766, Loss: 1.3581596612930298, Final Batch Loss: 0.3442727029323578\n",
      "Epoch 2767, Loss: 1.393263339996338, Final Batch Loss: 0.3932853639125824\n",
      "Epoch 2768, Loss: 1.5670937895774841, Final Batch Loss: 0.3768136203289032\n",
      "Epoch 2769, Loss: 1.4170458614826202, Final Batch Loss: 0.3528251349925995\n",
      "Epoch 2770, Loss: 1.4266151785850525, Final Batch Loss: 0.34650900959968567\n",
      "Epoch 2771, Loss: 1.490785926580429, Final Batch Loss: 0.44801461696624756\n",
      "Epoch 2772, Loss: 1.5509366989135742, Final Batch Loss: 0.45625606179237366\n",
      "Epoch 2773, Loss: 1.4071984887123108, Final Batch Loss: 0.29216089844703674\n",
      "Epoch 2774, Loss: 1.462248980998993, Final Batch Loss: 0.31058117747306824\n",
      "Epoch 2775, Loss: 1.4157924056053162, Final Batch Loss: 0.31481972336769104\n",
      "Epoch 2776, Loss: 1.5431014597415924, Final Batch Loss: 0.41078752279281616\n",
      "Epoch 2777, Loss: 1.5376165211200714, Final Batch Loss: 0.3334212899208069\n",
      "Epoch 2778, Loss: 1.3988428711891174, Final Batch Loss: 0.32051464915275574\n",
      "Epoch 2779, Loss: 1.4397899806499481, Final Batch Loss: 0.3280644118785858\n",
      "Epoch 2780, Loss: 1.4836211502552032, Final Batch Loss: 0.38718166947364807\n",
      "Epoch 2781, Loss: 1.4136110544204712, Final Batch Loss: 0.3066486716270447\n",
      "Epoch 2782, Loss: 1.5021232962608337, Final Batch Loss: 0.40621092915534973\n",
      "Epoch 2783, Loss: 1.398486316204071, Final Batch Loss: 0.34738683700561523\n",
      "Epoch 2784, Loss: 1.3795287609100342, Final Batch Loss: 0.26781657338142395\n",
      "Epoch 2785, Loss: 1.6328247785568237, Final Batch Loss: 0.41690847277641296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2786, Loss: 1.5237198173999786, Final Batch Loss: 0.3919338881969452\n",
      "Epoch 2787, Loss: 1.5539168119430542, Final Batch Loss: 0.35960468649864197\n",
      "Epoch 2788, Loss: 1.577130377292633, Final Batch Loss: 0.4074137806892395\n",
      "Epoch 2789, Loss: 1.585454136133194, Final Batch Loss: 0.49440091848373413\n",
      "Epoch 2790, Loss: 1.590556800365448, Final Batch Loss: 0.34658002853393555\n",
      "Epoch 2791, Loss: 1.618145376443863, Final Batch Loss: 0.4466509521007538\n",
      "Epoch 2792, Loss: 1.4303288757801056, Final Batch Loss: 0.35459277033805847\n",
      "Epoch 2793, Loss: 1.4304795265197754, Final Batch Loss: 0.3378683030605316\n",
      "Epoch 2794, Loss: 1.5792312622070312, Final Batch Loss: 0.3402769863605499\n",
      "Epoch 2795, Loss: 1.4207791090011597, Final Batch Loss: 0.35210922360420227\n",
      "Epoch 2796, Loss: 1.6951059699058533, Final Batch Loss: 0.39808937907218933\n",
      "Epoch 2797, Loss: 1.550366848707199, Final Batch Loss: 0.49892255663871765\n",
      "Epoch 2798, Loss: 1.542481243610382, Final Batch Loss: 0.3978026211261749\n",
      "Epoch 2799, Loss: 1.4404410421848297, Final Batch Loss: 0.41782766580581665\n",
      "Epoch 2800, Loss: 1.4517260193824768, Final Batch Loss: 0.43434157967567444\n",
      "Epoch 2801, Loss: 1.4132482409477234, Final Batch Loss: 0.3507850766181946\n",
      "Epoch 2802, Loss: 1.5447017848491669, Final Batch Loss: 0.3723083436489105\n",
      "Epoch 2803, Loss: 1.5346215665340424, Final Batch Loss: 0.48476675152778625\n",
      "Epoch 2804, Loss: 1.4883231967687607, Final Batch Loss: 0.23240824043750763\n",
      "Epoch 2805, Loss: 1.548499435186386, Final Batch Loss: 0.42997488379478455\n",
      "Epoch 2806, Loss: 1.494454026222229, Final Batch Loss: 0.3934595286846161\n",
      "Epoch 2807, Loss: 1.5711756646633148, Final Batch Loss: 0.38408663868904114\n",
      "Epoch 2808, Loss: 1.4081637561321259, Final Batch Loss: 0.34270477294921875\n",
      "Epoch 2809, Loss: 1.425445795059204, Final Batch Loss: 0.3210020065307617\n",
      "Epoch 2810, Loss: 1.4600790143013, Final Batch Loss: 0.47372540831565857\n",
      "Epoch 2811, Loss: 1.5286620557308197, Final Batch Loss: 0.41882795095443726\n",
      "Epoch 2812, Loss: 1.4339789748191833, Final Batch Loss: 0.3641595244407654\n",
      "Epoch 2813, Loss: 1.5066576302051544, Final Batch Loss: 0.39921289682388306\n",
      "Epoch 2814, Loss: 1.4060310274362564, Final Batch Loss: 0.2426479309797287\n",
      "Epoch 2815, Loss: 1.3331229984760284, Final Batch Loss: 0.3575492799282074\n",
      "Epoch 2816, Loss: 1.3390626311302185, Final Batch Loss: 0.29041844606399536\n",
      "Epoch 2817, Loss: 1.3660743832588196, Final Batch Loss: 0.32744503021240234\n",
      "Epoch 2818, Loss: 1.4297133088111877, Final Batch Loss: 0.31925061345100403\n",
      "Epoch 2819, Loss: 1.4760626554489136, Final Batch Loss: 0.4460494816303253\n",
      "Epoch 2820, Loss: 1.401295393705368, Final Batch Loss: 0.38141369819641113\n",
      "Epoch 2821, Loss: 1.5089439153671265, Final Batch Loss: 0.33377015590667725\n",
      "Epoch 2822, Loss: 1.4386971294879913, Final Batch Loss: 0.3354957103729248\n",
      "Epoch 2823, Loss: 1.4893485307693481, Final Batch Loss: 0.3856482207775116\n",
      "Epoch 2824, Loss: 1.3961330652236938, Final Batch Loss: 0.3093234896659851\n",
      "Epoch 2825, Loss: 1.5132719278335571, Final Batch Loss: 0.3605819344520569\n",
      "Epoch 2826, Loss: 1.4197703301906586, Final Batch Loss: 0.3775447905063629\n",
      "Epoch 2827, Loss: 1.3561894297599792, Final Batch Loss: 0.3604987859725952\n",
      "Epoch 2828, Loss: 1.634645402431488, Final Batch Loss: 0.37000197172164917\n",
      "Epoch 2829, Loss: 1.6443550288677216, Final Batch Loss: 0.3802281618118286\n",
      "Epoch 2830, Loss: 1.3970624208450317, Final Batch Loss: 0.3132486939430237\n",
      "Epoch 2831, Loss: 1.4202297627925873, Final Batch Loss: 0.3299737870693207\n",
      "Epoch 2832, Loss: 1.457847386598587, Final Batch Loss: 0.33472368121147156\n",
      "Epoch 2833, Loss: 1.551307737827301, Final Batch Loss: 0.4406627416610718\n",
      "Epoch 2834, Loss: 1.5088083148002625, Final Batch Loss: 0.3101235032081604\n",
      "Epoch 2835, Loss: 1.3976105153560638, Final Batch Loss: 0.302597314119339\n",
      "Epoch 2836, Loss: 1.57880499958992, Final Batch Loss: 0.2989894449710846\n",
      "Epoch 2837, Loss: 1.5224554538726807, Final Batch Loss: 0.4051160216331482\n",
      "Epoch 2838, Loss: 1.4606088399887085, Final Batch Loss: 0.3644508123397827\n",
      "Epoch 2839, Loss: 1.6409037709236145, Final Batch Loss: 0.4030941426753998\n",
      "Epoch 2840, Loss: 1.3507220149040222, Final Batch Loss: 0.33129340410232544\n",
      "Epoch 2841, Loss: 1.532005175948143, Final Batch Loss: 0.3882596790790558\n",
      "Epoch 2842, Loss: 1.4199538230895996, Final Batch Loss: 0.31825774908065796\n",
      "Epoch 2843, Loss: 1.485999971628189, Final Batch Loss: 0.3486914336681366\n",
      "Epoch 2844, Loss: 1.597514122724533, Final Batch Loss: 0.4083898365497589\n",
      "Epoch 2845, Loss: 1.5410816073417664, Final Batch Loss: 0.43284279108047485\n",
      "Epoch 2846, Loss: 1.5168870091438293, Final Batch Loss: 0.45686963200569153\n",
      "Epoch 2847, Loss: 1.3177366852760315, Final Batch Loss: 0.2889343500137329\n",
      "Epoch 2848, Loss: 1.3278188705444336, Final Batch Loss: 0.2931222915649414\n",
      "Epoch 2849, Loss: 1.4017442166805267, Final Batch Loss: 0.37443190813064575\n",
      "Epoch 2850, Loss: 1.2485812604427338, Final Batch Loss: 0.2228175699710846\n",
      "Epoch 2851, Loss: 1.2740367352962494, Final Batch Loss: 0.29117587208747864\n",
      "Epoch 2852, Loss: 1.5239960551261902, Final Batch Loss: 0.3341047167778015\n",
      "Epoch 2853, Loss: 1.5928135812282562, Final Batch Loss: 0.4736487567424774\n",
      "Epoch 2854, Loss: 1.4663449823856354, Final Batch Loss: 0.3752251863479614\n",
      "Epoch 2855, Loss: 1.6328037679195404, Final Batch Loss: 0.45275434851646423\n",
      "Epoch 2856, Loss: 1.501427799463272, Final Batch Loss: 0.3590551018714905\n",
      "Epoch 2857, Loss: 1.5544120371341705, Final Batch Loss: 0.42494311928749084\n",
      "Epoch 2858, Loss: 1.7378464341163635, Final Batch Loss: 0.41524797677993774\n",
      "Epoch 2859, Loss: 1.5973230600357056, Final Batch Loss: 0.38250747323036194\n",
      "Epoch 2860, Loss: 1.4054151773452759, Final Batch Loss: 0.3181541860103607\n",
      "Epoch 2861, Loss: 1.5308298766613007, Final Batch Loss: 0.34451279044151306\n",
      "Epoch 2862, Loss: 1.5466428399085999, Final Batch Loss: 0.38350623846054077\n",
      "Epoch 2863, Loss: 1.4380301535129547, Final Batch Loss: 0.3547073304653168\n",
      "Epoch 2864, Loss: 1.3341920375823975, Final Batch Loss: 0.3164537250995636\n",
      "Epoch 2865, Loss: 1.399395614862442, Final Batch Loss: 0.37320542335510254\n",
      "Epoch 2866, Loss: 1.4022444784641266, Final Batch Loss: 0.4153800308704376\n",
      "Epoch 2867, Loss: 1.471167892217636, Final Batch Loss: 0.393312931060791\n",
      "Epoch 2868, Loss: 1.4799304604530334, Final Batch Loss: 0.3235214054584503\n",
      "Epoch 2869, Loss: 1.3956563174724579, Final Batch Loss: 0.4162789285182953\n",
      "Epoch 2870, Loss: 1.4873389899730682, Final Batch Loss: 0.3396465480327606\n",
      "Epoch 2871, Loss: 1.4097199738025665, Final Batch Loss: 0.31250298023223877\n",
      "Epoch 2872, Loss: 1.4951370060443878, Final Batch Loss: 0.25769683718681335\n",
      "Epoch 2873, Loss: 1.3970468044281006, Final Batch Loss: 0.31232118606567383\n",
      "Epoch 2874, Loss: 1.464125543832779, Final Batch Loss: 0.33231064677238464\n",
      "Epoch 2875, Loss: 1.7084261178970337, Final Batch Loss: 0.4477526843547821\n",
      "Epoch 2876, Loss: 1.4250780493021011, Final Batch Loss: 0.1930166631937027\n",
      "Epoch 2877, Loss: 1.437338262796402, Final Batch Loss: 0.2868376076221466\n",
      "Epoch 2878, Loss: 1.4607488811016083, Final Batch Loss: 0.32770511507987976\n",
      "Epoch 2879, Loss: 1.5480267107486725, Final Batch Loss: 0.379948228597641\n",
      "Epoch 2880, Loss: 1.6270056366920471, Final Batch Loss: 0.4624786078929901\n",
      "Epoch 2881, Loss: 1.4230550229549408, Final Batch Loss: 0.43498483300209045\n",
      "Epoch 2882, Loss: 1.5297675132751465, Final Batch Loss: 0.30762502551078796\n",
      "Epoch 2883, Loss: 1.6039592027664185, Final Batch Loss: 0.41752898693084717\n",
      "Epoch 2884, Loss: 1.786797046661377, Final Batch Loss: 0.5384750962257385\n",
      "Epoch 2885, Loss: 1.4012423753738403, Final Batch Loss: 0.2948698401451111\n",
      "Epoch 2886, Loss: 1.3878793716430664, Final Batch Loss: 0.37916669249534607\n",
      "Epoch 2887, Loss: 1.4406188130378723, Final Batch Loss: 0.38612470030784607\n",
      "Epoch 2888, Loss: 1.4726547598838806, Final Batch Loss: 0.39177951216697693\n",
      "Epoch 2889, Loss: 1.5512690544128418, Final Batch Loss: 0.41787490248680115\n",
      "Epoch 2890, Loss: 1.4802310466766357, Final Batch Loss: 0.41041380167007446\n",
      "Epoch 2891, Loss: 1.4636406898498535, Final Batch Loss: 0.31926026940345764\n",
      "Epoch 2892, Loss: 1.4867567121982574, Final Batch Loss: 0.41177016496658325\n",
      "Epoch 2893, Loss: 1.3908818662166595, Final Batch Loss: 0.3975926339626312\n",
      "Epoch 2894, Loss: 1.4667020440101624, Final Batch Loss: 0.30756962299346924\n",
      "Epoch 2895, Loss: 1.297860473394394, Final Batch Loss: 0.2588076889514923\n",
      "Epoch 2896, Loss: 1.484958529472351, Final Batch Loss: 0.3847165107727051\n",
      "Epoch 2897, Loss: 1.3592058420181274, Final Batch Loss: 0.2903376817703247\n",
      "Epoch 2898, Loss: 1.5413499176502228, Final Batch Loss: 0.47910505533218384\n",
      "Epoch 2899, Loss: 1.5275534093379974, Final Batch Loss: 0.47712624073028564\n",
      "Epoch 2900, Loss: 1.388219803571701, Final Batch Loss: 0.3580295443534851\n",
      "Epoch 2901, Loss: 1.4015055000782013, Final Batch Loss: 0.2819772958755493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2902, Loss: 1.3953793942928314, Final Batch Loss: 0.3457298278808594\n",
      "Epoch 2903, Loss: 1.5055510997772217, Final Batch Loss: 0.33797964453697205\n",
      "Epoch 2904, Loss: 1.537916123867035, Final Batch Loss: 0.525791347026825\n",
      "Epoch 2905, Loss: 1.56515371799469, Final Batch Loss: 0.43653780221939087\n",
      "Epoch 2906, Loss: 1.4653645753860474, Final Batch Loss: 0.27242693305015564\n",
      "Epoch 2907, Loss: 1.4966048300266266, Final Batch Loss: 0.4765608012676239\n",
      "Epoch 2908, Loss: 1.3444755375385284, Final Batch Loss: 0.28885963559150696\n",
      "Epoch 2909, Loss: 1.4339068830013275, Final Batch Loss: 0.38041970133781433\n",
      "Epoch 2910, Loss: 1.4314780235290527, Final Batch Loss: 0.3900519013404846\n",
      "Epoch 2911, Loss: 1.3032374680042267, Final Batch Loss: 0.32178300619125366\n",
      "Epoch 2912, Loss: 1.4453339874744415, Final Batch Loss: 0.3297954797744751\n",
      "Epoch 2913, Loss: 1.3726260960102081, Final Batch Loss: 0.2875785529613495\n",
      "Epoch 2914, Loss: 1.4364502727985382, Final Batch Loss: 0.3601769506931305\n",
      "Epoch 2915, Loss: 1.5216041505336761, Final Batch Loss: 0.5105862021446228\n",
      "Epoch 2916, Loss: 1.3801291584968567, Final Batch Loss: 0.3091285526752472\n",
      "Epoch 2917, Loss: 1.4596255421638489, Final Batch Loss: 0.41689276695251465\n",
      "Epoch 2918, Loss: 1.500055879354477, Final Batch Loss: 0.3846670091152191\n",
      "Epoch 2919, Loss: 1.5524614751338959, Final Batch Loss: 0.401230126619339\n",
      "Epoch 2920, Loss: 1.4562962651252747, Final Batch Loss: 0.37853801250457764\n",
      "Epoch 2921, Loss: 1.410366415977478, Final Batch Loss: 0.3157840371131897\n",
      "Epoch 2922, Loss: 1.379077523946762, Final Batch Loss: 0.40806740522384644\n",
      "Epoch 2923, Loss: 1.3385615348815918, Final Batch Loss: 0.2902579605579376\n",
      "Epoch 2924, Loss: 1.27573624253273, Final Batch Loss: 0.3100345730781555\n",
      "Epoch 2925, Loss: 1.4682835042476654, Final Batch Loss: 0.34382766485214233\n",
      "Epoch 2926, Loss: 1.3379061222076416, Final Batch Loss: 0.2860645651817322\n",
      "Epoch 2927, Loss: 1.4561756551265717, Final Batch Loss: 0.40189483761787415\n",
      "Epoch 2928, Loss: 1.455096960067749, Final Batch Loss: 0.4650193750858307\n",
      "Epoch 2929, Loss: 1.4152642786502838, Final Batch Loss: 0.35084211826324463\n",
      "Epoch 2930, Loss: 1.3143725097179413, Final Batch Loss: 0.371628075838089\n",
      "Epoch 2931, Loss: 1.5253919064998627, Final Batch Loss: 0.42627471685409546\n",
      "Epoch 2932, Loss: 1.4475966393947601, Final Batch Loss: 0.31869930028915405\n",
      "Epoch 2933, Loss: 1.4723557233810425, Final Batch Loss: 0.35329097509384155\n",
      "Epoch 2934, Loss: 1.4688927829265594, Final Batch Loss: 0.4666588306427002\n",
      "Epoch 2935, Loss: 1.4397083222866058, Final Batch Loss: 0.309770405292511\n",
      "Epoch 2936, Loss: 1.550666093826294, Final Batch Loss: 0.3825535178184509\n",
      "Epoch 2937, Loss: 1.4691067039966583, Final Batch Loss: 0.2629489302635193\n",
      "Epoch 2938, Loss: 1.680866301059723, Final Batch Loss: 0.47420066595077515\n",
      "Epoch 2939, Loss: 1.4460520148277283, Final Batch Loss: 0.2867561876773834\n",
      "Epoch 2940, Loss: 1.3715125918388367, Final Batch Loss: 0.30796122550964355\n",
      "Epoch 2941, Loss: 1.4680372476577759, Final Batch Loss: 0.4439261853694916\n",
      "Epoch 2942, Loss: 1.4632482826709747, Final Batch Loss: 0.3682100772857666\n",
      "Epoch 2943, Loss: 1.3866530656814575, Final Batch Loss: 0.36043599247932434\n",
      "Epoch 2944, Loss: 1.3587193191051483, Final Batch Loss: 0.3451331853866577\n",
      "Epoch 2945, Loss: 1.290028065443039, Final Batch Loss: 0.3190295100212097\n",
      "Epoch 2946, Loss: 1.3669779598712921, Final Batch Loss: 0.2686949670314789\n",
      "Epoch 2947, Loss: 1.372918039560318, Final Batch Loss: 0.29255014657974243\n",
      "Epoch 2948, Loss: 1.3738625049591064, Final Batch Loss: 0.374035507440567\n",
      "Epoch 2949, Loss: 1.6711301803588867, Final Batch Loss: 0.5979514122009277\n",
      "Epoch 2950, Loss: 1.425300508737564, Final Batch Loss: 0.27941206097602844\n",
      "Epoch 2951, Loss: 1.4587920308113098, Final Batch Loss: 0.3458983600139618\n",
      "Epoch 2952, Loss: 1.460590660572052, Final Batch Loss: 0.333556205034256\n",
      "Epoch 2953, Loss: 1.3920586705207825, Final Batch Loss: 0.3850923478603363\n",
      "Epoch 2954, Loss: 1.3367064595222473, Final Batch Loss: 0.2673053443431854\n",
      "Epoch 2955, Loss: 1.5004556477069855, Final Batch Loss: 0.39988988637924194\n",
      "Epoch 2956, Loss: 1.6070810854434967, Final Batch Loss: 0.42854973673820496\n",
      "Epoch 2957, Loss: 1.398253619670868, Final Batch Loss: 0.39435654878616333\n",
      "Epoch 2958, Loss: 1.4554270207881927, Final Batch Loss: 0.3771832287311554\n",
      "Epoch 2959, Loss: 1.4060378074645996, Final Batch Loss: 0.30985283851623535\n",
      "Epoch 2960, Loss: 1.6081502437591553, Final Batch Loss: 0.3918780982494354\n",
      "Epoch 2961, Loss: 1.3419630229473114, Final Batch Loss: 0.3216155469417572\n",
      "Epoch 2962, Loss: 1.4512990415096283, Final Batch Loss: 0.4033261835575104\n",
      "Epoch 2963, Loss: 1.465683937072754, Final Batch Loss: 0.41512423753738403\n",
      "Epoch 2964, Loss: 1.5185586214065552, Final Batch Loss: 0.4391596019268036\n",
      "Epoch 2965, Loss: 1.3840495496988297, Final Batch Loss: 0.24472351372241974\n",
      "Epoch 2966, Loss: 1.5302508473396301, Final Batch Loss: 0.4009985327720642\n",
      "Epoch 2967, Loss: 1.3772722482681274, Final Batch Loss: 0.34494730830192566\n",
      "Epoch 2968, Loss: 1.5435396134853363, Final Batch Loss: 0.41908296942710876\n",
      "Epoch 2969, Loss: 1.3465909659862518, Final Batch Loss: 0.2977801263332367\n",
      "Epoch 2970, Loss: 1.3919323980808258, Final Batch Loss: 0.397183895111084\n",
      "Epoch 2971, Loss: 1.439250499010086, Final Batch Loss: 0.2986770570278168\n",
      "Epoch 2972, Loss: 1.4870545864105225, Final Batch Loss: 0.32409512996673584\n",
      "Epoch 2973, Loss: 1.3650413751602173, Final Batch Loss: 0.3092454969882965\n",
      "Epoch 2974, Loss: 1.5115076005458832, Final Batch Loss: 0.3248549699783325\n",
      "Epoch 2975, Loss: 1.4435082972049713, Final Batch Loss: 0.4155552089214325\n",
      "Epoch 2976, Loss: 1.4478202760219574, Final Batch Loss: 0.3878960609436035\n",
      "Epoch 2977, Loss: 1.4591596722602844, Final Batch Loss: 0.368764191865921\n",
      "Epoch 2978, Loss: 1.3608767688274384, Final Batch Loss: 0.3604234755039215\n",
      "Epoch 2979, Loss: 1.4184062778949738, Final Batch Loss: 0.38324835896492004\n",
      "Epoch 2980, Loss: 1.5094904005527496, Final Batch Loss: 0.31363457441329956\n",
      "Epoch 2981, Loss: 1.3758866488933563, Final Batch Loss: 0.29899221658706665\n",
      "Epoch 2982, Loss: 1.370008260011673, Final Batch Loss: 0.33235424757003784\n",
      "Epoch 2983, Loss: 1.330595314502716, Final Batch Loss: 0.34115880727767944\n",
      "Epoch 2984, Loss: 1.3675863146781921, Final Batch Loss: 0.38457903265953064\n",
      "Epoch 2985, Loss: 1.3566054105758667, Final Batch Loss: 0.29567891359329224\n",
      "Epoch 2986, Loss: 1.4153867065906525, Final Batch Loss: 0.3268726170063019\n",
      "Epoch 2987, Loss: 1.4989939332008362, Final Batch Loss: 0.36520853638648987\n",
      "Epoch 2988, Loss: 1.4196307063102722, Final Batch Loss: 0.30938801169395447\n",
      "Epoch 2989, Loss: 1.4934819042682648, Final Batch Loss: 0.40317270159721375\n",
      "Epoch 2990, Loss: 1.4464532434940338, Final Batch Loss: 0.3663637340068817\n",
      "Epoch 2991, Loss: 1.516722947359085, Final Batch Loss: 0.4817529320716858\n",
      "Epoch 2992, Loss: 1.3291799128055573, Final Batch Loss: 0.24918869137763977\n",
      "Epoch 2993, Loss: 1.3776123821735382, Final Batch Loss: 0.33455535769462585\n",
      "Epoch 2994, Loss: 1.6245608627796173, Final Batch Loss: 0.3724076747894287\n",
      "Epoch 2995, Loss: 1.4965562522411346, Final Batch Loss: 0.3513863682746887\n",
      "Epoch 2996, Loss: 1.4495360851287842, Final Batch Loss: 0.4638708829879761\n",
      "Epoch 2997, Loss: 1.3422134518623352, Final Batch Loss: 0.3544951379299164\n",
      "Epoch 2998, Loss: 1.5593169629573822, Final Batch Loss: 0.44675177335739136\n",
      "Epoch 2999, Loss: 1.4586035311222076, Final Batch Loss: 0.3551148474216461\n",
      "Epoch 3000, Loss: 1.4728177785873413, Final Batch Loss: 0.39109697937965393\n",
      "Epoch 3001, Loss: 1.4558337330818176, Final Batch Loss: 0.4032604992389679\n",
      "Epoch 3002, Loss: 1.4991289973258972, Final Batch Loss: 0.3721051812171936\n",
      "Epoch 3003, Loss: 1.3933106064796448, Final Batch Loss: 0.27258169651031494\n",
      "Epoch 3004, Loss: 1.4457400143146515, Final Batch Loss: 0.46884071826934814\n",
      "Epoch 3005, Loss: 1.4105309844017029, Final Batch Loss: 0.3183405101299286\n",
      "Epoch 3006, Loss: 1.3417659997940063, Final Batch Loss: 0.31727588176727295\n",
      "Epoch 3007, Loss: 1.5738073289394379, Final Batch Loss: 0.42717376351356506\n",
      "Epoch 3008, Loss: 1.5225339829921722, Final Batch Loss: 0.47775620222091675\n",
      "Epoch 3009, Loss: 1.2566323578357697, Final Batch Loss: 0.3211832046508789\n",
      "Epoch 3010, Loss: 1.3653054535388947, Final Batch Loss: 0.3619738221168518\n",
      "Epoch 3011, Loss: 1.369835615158081, Final Batch Loss: 0.3837864398956299\n",
      "Epoch 3012, Loss: 1.4607557952404022, Final Batch Loss: 0.372009813785553\n",
      "Epoch 3013, Loss: 1.347158282995224, Final Batch Loss: 0.3574545383453369\n",
      "Epoch 3014, Loss: 1.3647671937942505, Final Batch Loss: 0.3935409486293793\n",
      "Epoch 3015, Loss: 1.28037528693676, Final Batch Loss: 0.24859817326068878\n",
      "Epoch 3016, Loss: 1.3202790319919586, Final Batch Loss: 0.3209611475467682\n",
      "Epoch 3017, Loss: 1.464062362909317, Final Batch Loss: 0.36788174510002136\n",
      "Epoch 3018, Loss: 1.4937751293182373, Final Batch Loss: 0.4826814532279968\n",
      "Epoch 3019, Loss: 1.3521491289138794, Final Batch Loss: 0.41778916120529175\n",
      "Epoch 3020, Loss: 1.569919914007187, Final Batch Loss: 0.49825072288513184\n",
      "Epoch 3021, Loss: 1.441155582666397, Final Batch Loss: 0.4037478268146515\n",
      "Epoch 3022, Loss: 1.7121555507183075, Final Batch Loss: 0.6055493354797363\n",
      "Epoch 3023, Loss: 1.4027175903320312, Final Batch Loss: 0.28181007504463196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3024, Loss: 1.4896861910820007, Final Batch Loss: 0.37961557507514954\n",
      "Epoch 3025, Loss: 1.3874360024929047, Final Batch Loss: 0.3335703909397125\n",
      "Epoch 3026, Loss: 1.4901507198810577, Final Batch Loss: 0.3879852592945099\n",
      "Epoch 3027, Loss: 1.3621605932712555, Final Batch Loss: 0.39373883605003357\n",
      "Epoch 3028, Loss: 1.4312732219696045, Final Batch Loss: 0.3490179479122162\n",
      "Epoch 3029, Loss: 1.583649069070816, Final Batch Loss: 0.551129937171936\n",
      "Epoch 3030, Loss: 1.4375546872615814, Final Batch Loss: 0.3163176476955414\n",
      "Epoch 3031, Loss: 1.5514845848083496, Final Batch Loss: 0.3157722055912018\n",
      "Epoch 3032, Loss: 1.4309532642364502, Final Batch Loss: 0.3879258930683136\n",
      "Epoch 3033, Loss: 1.3540292084217072, Final Batch Loss: 0.30851954221725464\n",
      "Epoch 3034, Loss: 1.5547215342521667, Final Batch Loss: 0.39898574352264404\n",
      "Epoch 3035, Loss: 1.3867263495922089, Final Batch Loss: 0.32461559772491455\n",
      "Epoch 3036, Loss: 1.5443482398986816, Final Batch Loss: 0.3509133458137512\n",
      "Epoch 3037, Loss: 1.455845296382904, Final Batch Loss: 0.3186704218387604\n",
      "Epoch 3038, Loss: 1.4668962061405182, Final Batch Loss: 0.3792426586151123\n",
      "Epoch 3039, Loss: 1.4662111401557922, Final Batch Loss: 0.46394121646881104\n",
      "Epoch 3040, Loss: 1.5195753276348114, Final Batch Loss: 0.3867075443267822\n",
      "Epoch 3041, Loss: 1.5943090319633484, Final Batch Loss: 0.4377332329750061\n",
      "Epoch 3042, Loss: 1.4418575167655945, Final Batch Loss: 0.41264739632606506\n",
      "Epoch 3043, Loss: 1.4068249464035034, Final Batch Loss: 0.3418601453304291\n",
      "Epoch 3044, Loss: 1.3934265673160553, Final Batch Loss: 0.3587861955165863\n",
      "Epoch 3045, Loss: 1.421916425228119, Final Batch Loss: 0.3488498628139496\n",
      "Epoch 3046, Loss: 1.4772283434867859, Final Batch Loss: 0.4246861934661865\n",
      "Epoch 3047, Loss: 1.4264629185199738, Final Batch Loss: 0.41629210114479065\n",
      "Epoch 3048, Loss: 1.466977208852768, Final Batch Loss: 0.5152378082275391\n",
      "Epoch 3049, Loss: 1.3897072672843933, Final Batch Loss: 0.3373172879219055\n",
      "Epoch 3050, Loss: 1.4239302575588226, Final Batch Loss: 0.3965015709400177\n",
      "Epoch 3051, Loss: 1.4798177480697632, Final Batch Loss: 0.40605390071868896\n",
      "Epoch 3052, Loss: 1.4669943153858185, Final Batch Loss: 0.3773078918457031\n",
      "Epoch 3053, Loss: 1.3747316598892212, Final Batch Loss: 0.3680911958217621\n",
      "Epoch 3054, Loss: 1.5304213166236877, Final Batch Loss: 0.3765701651573181\n",
      "Epoch 3055, Loss: 1.4260623157024384, Final Batch Loss: 0.3723699152469635\n",
      "Epoch 3056, Loss: 1.2753173410892487, Final Batch Loss: 0.33178451657295227\n",
      "Epoch 3057, Loss: 1.356277659535408, Final Batch Loss: 0.4628910720348358\n",
      "Epoch 3058, Loss: 1.8102573454380035, Final Batch Loss: 0.7152182459831238\n",
      "Epoch 3059, Loss: 1.3833186328411102, Final Batch Loss: 0.3003477156162262\n",
      "Epoch 3060, Loss: 1.4311484694480896, Final Batch Loss: 0.3242183327674866\n",
      "Epoch 3061, Loss: 1.5177894532680511, Final Batch Loss: 0.35471707582473755\n",
      "Epoch 3062, Loss: 1.30105522274971, Final Batch Loss: 0.2824365496635437\n",
      "Epoch 3063, Loss: 1.3659468591213226, Final Batch Loss: 0.34145626425743103\n",
      "Epoch 3064, Loss: 1.3614994883537292, Final Batch Loss: 0.26337841153144836\n",
      "Epoch 3065, Loss: 1.4532342851161957, Final Batch Loss: 0.36817607283592224\n",
      "Epoch 3066, Loss: 1.360211431980133, Final Batch Loss: 0.31717002391815186\n",
      "Epoch 3067, Loss: 1.4198638796806335, Final Batch Loss: 0.34474897384643555\n",
      "Epoch 3068, Loss: 1.411637544631958, Final Batch Loss: 0.29466331005096436\n",
      "Epoch 3069, Loss: 1.3211511969566345, Final Batch Loss: 0.27508485317230225\n",
      "Epoch 3070, Loss: 1.2972560822963715, Final Batch Loss: 0.28170639276504517\n",
      "Epoch 3071, Loss: 1.3909247517585754, Final Batch Loss: 0.33363640308380127\n",
      "Epoch 3072, Loss: 1.3659709990024567, Final Batch Loss: 0.2992960810661316\n",
      "Epoch 3073, Loss: 1.3706919252872467, Final Batch Loss: 0.3141196668148041\n",
      "Epoch 3074, Loss: 1.4798248708248138, Final Batch Loss: 0.43301501870155334\n",
      "Epoch 3075, Loss: 1.446064978837967, Final Batch Loss: 0.3396458029747009\n",
      "Epoch 3076, Loss: 1.5217640697956085, Final Batch Loss: 0.377387672662735\n",
      "Epoch 3077, Loss: 1.4696409106254578, Final Batch Loss: 0.3229873478412628\n",
      "Epoch 3078, Loss: 1.392877995967865, Final Batch Loss: 0.3837866187095642\n",
      "Epoch 3079, Loss: 1.432386428117752, Final Batch Loss: 0.3218536376953125\n",
      "Epoch 3080, Loss: 1.595899224281311, Final Batch Loss: 0.3965352177619934\n",
      "Epoch 3081, Loss: 1.589919239282608, Final Batch Loss: 0.3632490038871765\n",
      "Epoch 3082, Loss: 1.379633516073227, Final Batch Loss: 0.2325913906097412\n",
      "Epoch 3083, Loss: 1.584793359041214, Final Batch Loss: 0.35394689440727234\n",
      "Epoch 3084, Loss: 1.458108901977539, Final Batch Loss: 0.37610721588134766\n",
      "Epoch 3085, Loss: 1.4462062418460846, Final Batch Loss: 0.3625325560569763\n",
      "Epoch 3086, Loss: 1.518505483865738, Final Batch Loss: 0.3822779357433319\n",
      "Epoch 3087, Loss: 1.384332299232483, Final Batch Loss: 0.3497132360935211\n",
      "Epoch 3088, Loss: 1.484417885541916, Final Batch Loss: 0.38233014941215515\n",
      "Epoch 3089, Loss: 1.4634622633457184, Final Batch Loss: 0.47408586740493774\n",
      "Epoch 3090, Loss: 1.462571233510971, Final Batch Loss: 0.33284714818000793\n",
      "Epoch 3091, Loss: 1.4850180447101593, Final Batch Loss: 0.3305034339427948\n",
      "Epoch 3092, Loss: 1.3047821521759033, Final Batch Loss: 0.2670547366142273\n",
      "Epoch 3093, Loss: 1.412667840719223, Final Batch Loss: 0.25319844484329224\n",
      "Epoch 3094, Loss: 1.5164656043052673, Final Batch Loss: 0.47455984354019165\n",
      "Epoch 3095, Loss: 1.5066724717617035, Final Batch Loss: 0.40056219696998596\n",
      "Epoch 3096, Loss: 1.3910678029060364, Final Batch Loss: 0.3550673723220825\n",
      "Epoch 3097, Loss: 1.4843312799930573, Final Batch Loss: 0.4069146513938904\n",
      "Epoch 3098, Loss: 1.3326258063316345, Final Batch Loss: 0.34509584307670593\n",
      "Epoch 3099, Loss: 1.4042987525463104, Final Batch Loss: 0.3418958783149719\n",
      "Epoch 3100, Loss: 1.4661478400230408, Final Batch Loss: 0.3395754098892212\n",
      "Epoch 3101, Loss: 1.4630517363548279, Final Batch Loss: 0.3915054202079773\n",
      "Epoch 3102, Loss: 1.291886180639267, Final Batch Loss: 0.24539333581924438\n",
      "Epoch 3103, Loss: 1.4032730609178543, Final Batch Loss: 0.23287393152713776\n",
      "Epoch 3104, Loss: 1.3439431190490723, Final Batch Loss: 0.279407262802124\n",
      "Epoch 3105, Loss: 1.4588654935359955, Final Batch Loss: 0.3261503279209137\n",
      "Epoch 3106, Loss: 1.480520635843277, Final Batch Loss: 0.4586990773677826\n",
      "Epoch 3107, Loss: 1.452114224433899, Final Batch Loss: 0.46372345089912415\n",
      "Epoch 3108, Loss: 1.435107797384262, Final Batch Loss: 0.3844905197620392\n",
      "Epoch 3109, Loss: 1.498589187860489, Final Batch Loss: 0.3727411925792694\n",
      "Epoch 3110, Loss: 1.537928968667984, Final Batch Loss: 0.383147269487381\n",
      "Epoch 3111, Loss: 1.5115426778793335, Final Batch Loss: 0.31687596440315247\n",
      "Epoch 3112, Loss: 1.3893161416053772, Final Batch Loss: 0.3163781762123108\n",
      "Epoch 3113, Loss: 1.3458011150360107, Final Batch Loss: 0.2946909964084625\n",
      "Epoch 3114, Loss: 1.3998607993125916, Final Batch Loss: 0.3921104967594147\n",
      "Epoch 3115, Loss: 1.3868306577205658, Final Batch Loss: 0.3485676646232605\n",
      "Epoch 3116, Loss: 1.5144880414009094, Final Batch Loss: 0.4010821282863617\n",
      "Epoch 3117, Loss: 1.518259733915329, Final Batch Loss: 0.43010857701301575\n",
      "Epoch 3118, Loss: 1.3017135709524155, Final Batch Loss: 0.23642338812351227\n",
      "Epoch 3119, Loss: 1.4329828023910522, Final Batch Loss: 0.3682354986667633\n",
      "Epoch 3120, Loss: 1.4044758677482605, Final Batch Loss: 0.37315666675567627\n",
      "Epoch 3121, Loss: 1.4366892576217651, Final Batch Loss: 0.32865145802497864\n",
      "Epoch 3122, Loss: 1.2772647440433502, Final Batch Loss: 0.2324596345424652\n",
      "Epoch 3123, Loss: 1.3981422781944275, Final Batch Loss: 0.30552881956100464\n",
      "Epoch 3124, Loss: 1.4411056339740753, Final Batch Loss: 0.33852919936180115\n",
      "Epoch 3125, Loss: 1.6499158442020416, Final Batch Loss: 0.4650484025478363\n",
      "Epoch 3126, Loss: 1.4300192296504974, Final Batch Loss: 0.4192594885826111\n",
      "Epoch 3127, Loss: 1.5008096992969513, Final Batch Loss: 0.3663497567176819\n",
      "Epoch 3128, Loss: 1.3978548645973206, Final Batch Loss: 0.2828896939754486\n",
      "Epoch 3129, Loss: 1.3168074488639832, Final Batch Loss: 0.22484567761421204\n",
      "Epoch 3130, Loss: 1.335850477218628, Final Batch Loss: 0.3651503920555115\n",
      "Epoch 3131, Loss: 1.4128631949424744, Final Batch Loss: 0.358524352312088\n",
      "Epoch 3132, Loss: 1.369408279657364, Final Batch Loss: 0.3186757266521454\n",
      "Epoch 3133, Loss: 1.4632707238197327, Final Batch Loss: 0.3264429569244385\n",
      "Epoch 3134, Loss: 1.4011346697807312, Final Batch Loss: 0.3789849877357483\n",
      "Epoch 3135, Loss: 1.416018396615982, Final Batch Loss: 0.35638943314552307\n",
      "Epoch 3136, Loss: 1.5259811878204346, Final Batch Loss: 0.39916735887527466\n",
      "Epoch 3137, Loss: 1.3793798089027405, Final Batch Loss: 0.33827346563339233\n",
      "Epoch 3138, Loss: 1.3910586833953857, Final Batch Loss: 0.4119938313961029\n",
      "Epoch 3139, Loss: 1.4536753594875336, Final Batch Loss: 0.38292014598846436\n",
      "Epoch 3140, Loss: 1.4374765455722809, Final Batch Loss: 0.3899950087070465\n",
      "Epoch 3141, Loss: 1.3383142352104187, Final Batch Loss: 0.274411141872406\n",
      "Epoch 3142, Loss: 1.3252763599157333, Final Batch Loss: 0.23938767611980438\n",
      "Epoch 3143, Loss: 1.4235399663448334, Final Batch Loss: 0.351024329662323\n",
      "Epoch 3144, Loss: 1.3806964457035065, Final Batch Loss: 0.2999127209186554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3145, Loss: 1.2995890974998474, Final Batch Loss: 0.350056916475296\n",
      "Epoch 3146, Loss: 1.5751953423023224, Final Batch Loss: 0.47553032636642456\n",
      "Epoch 3147, Loss: 1.4148034751415253, Final Batch Loss: 0.332904577255249\n",
      "Epoch 3148, Loss: 1.5158711671829224, Final Batch Loss: 0.2776087522506714\n",
      "Epoch 3149, Loss: 1.4522078931331635, Final Batch Loss: 0.32754284143447876\n",
      "Epoch 3150, Loss: 1.3873840272426605, Final Batch Loss: 0.3291867673397064\n",
      "Epoch 3151, Loss: 1.4673058092594147, Final Batch Loss: 0.3660280704498291\n",
      "Epoch 3152, Loss: 1.309884488582611, Final Batch Loss: 0.3349943459033966\n",
      "Epoch 3153, Loss: 1.3732104301452637, Final Batch Loss: 0.4031050205230713\n",
      "Epoch 3154, Loss: 1.4366587400436401, Final Batch Loss: 0.38597336411476135\n",
      "Epoch 3155, Loss: 1.325185239315033, Final Batch Loss: 0.22109737992286682\n",
      "Epoch 3156, Loss: 1.3188936412334442, Final Batch Loss: 0.3179139494895935\n",
      "Epoch 3157, Loss: 1.6374724507331848, Final Batch Loss: 0.4934970736503601\n",
      "Epoch 3158, Loss: 1.4575552940368652, Final Batch Loss: 0.38542622327804565\n",
      "Epoch 3159, Loss: 1.4238024652004242, Final Batch Loss: 0.3991653025150299\n",
      "Epoch 3160, Loss: 1.3818525075912476, Final Batch Loss: 0.3032326400279999\n",
      "Epoch 3161, Loss: 1.6449893414974213, Final Batch Loss: 0.34705910086631775\n",
      "Epoch 3162, Loss: 1.3648749887943268, Final Batch Loss: 0.3551679253578186\n",
      "Epoch 3163, Loss: 1.3932460248470306, Final Batch Loss: 0.41471800208091736\n",
      "Epoch 3164, Loss: 1.4094869792461395, Final Batch Loss: 0.3629036545753479\n",
      "Epoch 3165, Loss: 1.351832926273346, Final Batch Loss: 0.332907110452652\n",
      "Epoch 3166, Loss: 1.1916940659284592, Final Batch Loss: 0.2120412439107895\n",
      "Epoch 3167, Loss: 1.287218987941742, Final Batch Loss: 0.2175723910331726\n",
      "Epoch 3168, Loss: 1.3709108233451843, Final Batch Loss: 0.2608351707458496\n",
      "Epoch 3169, Loss: 1.3487365543842316, Final Batch Loss: 0.36584189534187317\n",
      "Epoch 3170, Loss: 1.3100201487541199, Final Batch Loss: 0.2937990128993988\n",
      "Epoch 3171, Loss: 1.4883652031421661, Final Batch Loss: 0.46153587102890015\n",
      "Epoch 3172, Loss: 1.4098672270774841, Final Batch Loss: 0.34596163034439087\n",
      "Epoch 3173, Loss: 1.4820412397384644, Final Batch Loss: 0.4086490273475647\n",
      "Epoch 3174, Loss: 1.4550513327121735, Final Batch Loss: 0.35675469040870667\n",
      "Epoch 3175, Loss: 1.353326678276062, Final Batch Loss: 0.36712363362312317\n",
      "Epoch 3176, Loss: 1.3702810406684875, Final Batch Loss: 0.2879125475883484\n",
      "Epoch 3177, Loss: 1.4231456220149994, Final Batch Loss: 0.29848071932792664\n",
      "Epoch 3178, Loss: 1.4681207239627838, Final Batch Loss: 0.3443830609321594\n",
      "Epoch 3179, Loss: 1.42346853017807, Final Batch Loss: 0.3848910331726074\n",
      "Epoch 3180, Loss: 1.438506543636322, Final Batch Loss: 0.41823747754096985\n",
      "Epoch 3181, Loss: 1.414229393005371, Final Batch Loss: 0.40996959805488586\n",
      "Epoch 3182, Loss: 1.4971015453338623, Final Batch Loss: 0.36359336972236633\n",
      "Epoch 3183, Loss: 1.4795461595058441, Final Batch Loss: 0.4006590247154236\n",
      "Epoch 3184, Loss: 1.4113945662975311, Final Batch Loss: 0.319813996553421\n",
      "Epoch 3185, Loss: 1.412274420261383, Final Batch Loss: 0.4067636728286743\n",
      "Epoch 3186, Loss: 1.3016978800296783, Final Batch Loss: 0.26391881704330444\n",
      "Epoch 3187, Loss: 1.6082804799079895, Final Batch Loss: 0.5182939767837524\n",
      "Epoch 3188, Loss: 1.3015161454677582, Final Batch Loss: 0.3479749262332916\n",
      "Epoch 3189, Loss: 1.315907895565033, Final Batch Loss: 0.3519229590892792\n",
      "Epoch 3190, Loss: 1.4420832395553589, Final Batch Loss: 0.3589063286781311\n",
      "Epoch 3191, Loss: 1.3458753526210785, Final Batch Loss: 0.260518878698349\n",
      "Epoch 3192, Loss: 1.4075979888439178, Final Batch Loss: 0.3888961374759674\n",
      "Epoch 3193, Loss: 1.4403165876865387, Final Batch Loss: 0.36322158575057983\n",
      "Epoch 3194, Loss: 1.3035326898097992, Final Batch Loss: 0.2278897762298584\n",
      "Epoch 3195, Loss: 1.4426762759685516, Final Batch Loss: 0.3077440857887268\n",
      "Epoch 3196, Loss: 1.4713388085365295, Final Batch Loss: 0.40373775362968445\n",
      "Epoch 3197, Loss: 1.5836361944675446, Final Batch Loss: 0.4156389534473419\n",
      "Epoch 3198, Loss: 1.3995929956436157, Final Batch Loss: 0.3619036376476288\n",
      "Epoch 3199, Loss: 1.4934854209423065, Final Batch Loss: 0.32709190249443054\n",
      "Epoch 3200, Loss: 1.3728532791137695, Final Batch Loss: 0.30750709772109985\n",
      "Epoch 3201, Loss: 1.4350723922252655, Final Batch Loss: 0.3755793273448944\n",
      "Epoch 3202, Loss: 1.5117811858654022, Final Batch Loss: 0.3361559808254242\n",
      "Epoch 3203, Loss: 1.2877203524112701, Final Batch Loss: 0.2547193169593811\n",
      "Epoch 3204, Loss: 1.529113918542862, Final Batch Loss: 0.31860584020614624\n",
      "Epoch 3205, Loss: 1.335155576467514, Final Batch Loss: 0.3737139105796814\n",
      "Epoch 3206, Loss: 1.3598758280277252, Final Batch Loss: 0.26562270522117615\n",
      "Epoch 3207, Loss: 1.2620068788528442, Final Batch Loss: 0.253656804561615\n",
      "Epoch 3208, Loss: 1.386731594800949, Final Batch Loss: 0.3307307958602905\n",
      "Epoch 3209, Loss: 1.4498138725757599, Final Batch Loss: 0.3922930657863617\n",
      "Epoch 3210, Loss: 1.367522418498993, Final Batch Loss: 0.27069440484046936\n",
      "Epoch 3211, Loss: 1.5264602303504944, Final Batch Loss: 0.33249029517173767\n",
      "Epoch 3212, Loss: 1.3981305658817291, Final Batch Loss: 0.40808799862861633\n",
      "Epoch 3213, Loss: 1.5111084878444672, Final Batch Loss: 0.2979556620121002\n",
      "Epoch 3214, Loss: 1.3519154638051987, Final Batch Loss: 0.22519128024578094\n",
      "Epoch 3215, Loss: 1.411233901977539, Final Batch Loss: 0.366727739572525\n",
      "Epoch 3216, Loss: 1.3605507016181946, Final Batch Loss: 0.2798692584037781\n",
      "Epoch 3217, Loss: 1.3306048512458801, Final Batch Loss: 0.3755200505256653\n",
      "Epoch 3218, Loss: 1.4930141866207123, Final Batch Loss: 0.36220401525497437\n",
      "Epoch 3219, Loss: 1.3746884167194366, Final Batch Loss: 0.3223949372768402\n",
      "Epoch 3220, Loss: 1.409135788679123, Final Batch Loss: 0.39868441224098206\n",
      "Epoch 3221, Loss: 1.4784348011016846, Final Batch Loss: 0.4324663281440735\n",
      "Epoch 3222, Loss: 1.3388375341892242, Final Batch Loss: 0.40273943543434143\n",
      "Epoch 3223, Loss: 1.3282686173915863, Final Batch Loss: 0.2848804295063019\n",
      "Epoch 3224, Loss: 1.3194094002246857, Final Batch Loss: 0.2599579393863678\n",
      "Epoch 3225, Loss: 1.3033345639705658, Final Batch Loss: 0.2778915464878082\n",
      "Epoch 3226, Loss: 1.3925605118274689, Final Batch Loss: 0.29614537954330444\n",
      "Epoch 3227, Loss: 1.558411866426468, Final Batch Loss: 0.46475082635879517\n",
      "Epoch 3228, Loss: 1.4081285893917084, Final Batch Loss: 0.27918538451194763\n",
      "Epoch 3229, Loss: 1.3160586655139923, Final Batch Loss: 0.28429266810417175\n",
      "Epoch 3230, Loss: 1.3139147758483887, Final Batch Loss: 0.3032330572605133\n",
      "Epoch 3231, Loss: 1.4017857313156128, Final Batch Loss: 0.278831422328949\n",
      "Epoch 3232, Loss: 1.3969306945800781, Final Batch Loss: 0.3748471438884735\n",
      "Epoch 3233, Loss: 1.4340744614601135, Final Batch Loss: 0.3347397744655609\n",
      "Epoch 3234, Loss: 1.372257798910141, Final Batch Loss: 0.33359307050704956\n",
      "Epoch 3235, Loss: 1.4684515297412872, Final Batch Loss: 0.34987983107566833\n",
      "Epoch 3236, Loss: 1.622246503829956, Final Batch Loss: 0.33797433972358704\n",
      "Epoch 3237, Loss: 1.299945592880249, Final Batch Loss: 0.2834599018096924\n",
      "Epoch 3238, Loss: 1.4451507925987244, Final Batch Loss: 0.3733057975769043\n",
      "Epoch 3239, Loss: 1.3662370145320892, Final Batch Loss: 0.31925168633461\n",
      "Epoch 3240, Loss: 1.2689795792102814, Final Batch Loss: 0.24765443801879883\n",
      "Epoch 3241, Loss: 1.2966735363006592, Final Batch Loss: 0.30774980783462524\n",
      "Epoch 3242, Loss: 1.3352159261703491, Final Batch Loss: 0.29488348960876465\n",
      "Epoch 3243, Loss: 1.511944442987442, Final Batch Loss: 0.39226651191711426\n",
      "Epoch 3244, Loss: 1.3734910488128662, Final Batch Loss: 0.3649026155471802\n",
      "Epoch 3245, Loss: 1.4446468353271484, Final Batch Loss: 0.3209232985973358\n",
      "Epoch 3246, Loss: 1.318158745765686, Final Batch Loss: 0.3570127487182617\n",
      "Epoch 3247, Loss: 1.3077309429645538, Final Batch Loss: 0.32528138160705566\n",
      "Epoch 3248, Loss: 1.3442128002643585, Final Batch Loss: 0.34272417426109314\n",
      "Epoch 3249, Loss: 1.3244905471801758, Final Batch Loss: 0.33829450607299805\n",
      "Epoch 3250, Loss: 1.2892693877220154, Final Batch Loss: 0.3032659888267517\n",
      "Epoch 3251, Loss: 1.4483283460140228, Final Batch Loss: 0.40175339579582214\n",
      "Epoch 3252, Loss: 1.4282553493976593, Final Batch Loss: 0.2739413380622864\n",
      "Epoch 3253, Loss: 1.3215323686599731, Final Batch Loss: 0.34670472145080566\n",
      "Epoch 3254, Loss: 1.2754718959331512, Final Batch Loss: 0.29499173164367676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3255, Loss: 1.3758225739002228, Final Batch Loss: 0.32154446840286255\n",
      "Epoch 3256, Loss: 1.386867195367813, Final Batch Loss: 0.2609480321407318\n",
      "Epoch 3257, Loss: 1.3504551947116852, Final Batch Loss: 0.29404377937316895\n",
      "Epoch 3258, Loss: 1.3895895183086395, Final Batch Loss: 0.3206947445869446\n",
      "Epoch 3259, Loss: 1.5281636416912079, Final Batch Loss: 0.4691271185874939\n",
      "Epoch 3260, Loss: 1.3332925736904144, Final Batch Loss: 0.27227872610092163\n",
      "Epoch 3261, Loss: 1.5084787905216217, Final Batch Loss: 0.4695271849632263\n",
      "Epoch 3262, Loss: 1.3444747626781464, Final Batch Loss: 0.44530990719795227\n",
      "Epoch 3263, Loss: 1.3785147070884705, Final Batch Loss: 0.3952481150627136\n",
      "Epoch 3264, Loss: 1.2793579399585724, Final Batch Loss: 0.269917368888855\n",
      "Epoch 3265, Loss: 1.4485738575458527, Final Batch Loss: 0.31141817569732666\n",
      "Epoch 3266, Loss: 1.4180380702018738, Final Batch Loss: 0.37978166341781616\n",
      "Epoch 3267, Loss: 1.2858200073242188, Final Batch Loss: 0.30186691880226135\n",
      "Epoch 3268, Loss: 1.3406383693218231, Final Batch Loss: 0.2950608432292938\n",
      "Epoch 3269, Loss: 1.4390495717525482, Final Batch Loss: 0.3367776870727539\n",
      "Epoch 3270, Loss: 1.327940508723259, Final Batch Loss: 0.21525238454341888\n",
      "Epoch 3271, Loss: 1.3548900187015533, Final Batch Loss: 0.29091688990592957\n",
      "Epoch 3272, Loss: 1.2644448578357697, Final Batch Loss: 0.282436728477478\n",
      "Epoch 3273, Loss: 1.3809410333633423, Final Batch Loss: 0.40927281975746155\n",
      "Epoch 3274, Loss: 1.3354175686836243, Final Batch Loss: 0.328915536403656\n",
      "Epoch 3275, Loss: 1.3480632305145264, Final Batch Loss: 0.32814913988113403\n",
      "Epoch 3276, Loss: 1.4966644048690796, Final Batch Loss: 0.3338218629360199\n",
      "Epoch 3277, Loss: 1.421980232000351, Final Batch Loss: 0.23073765635490417\n",
      "Epoch 3278, Loss: 1.411641389131546, Final Batch Loss: 0.332110196352005\n",
      "Epoch 3279, Loss: 1.4226731359958649, Final Batch Loss: 0.2980653941631317\n",
      "Epoch 3280, Loss: 1.5639747083187103, Final Batch Loss: 0.47374987602233887\n",
      "Epoch 3281, Loss: 1.4227094054222107, Final Batch Loss: 0.2938782870769501\n",
      "Epoch 3282, Loss: 1.3780188858509064, Final Batch Loss: 0.32940468192100525\n",
      "Epoch 3283, Loss: 1.4269148409366608, Final Batch Loss: 0.2804928421974182\n",
      "Epoch 3284, Loss: 1.2151728719472885, Final Batch Loss: 0.30942264199256897\n",
      "Epoch 3285, Loss: 1.4284147024154663, Final Batch Loss: 0.4899860620498657\n",
      "Epoch 3286, Loss: 1.430194914340973, Final Batch Loss: 0.41510194540023804\n",
      "Epoch 3287, Loss: 1.3488364517688751, Final Batch Loss: 0.36397242546081543\n",
      "Epoch 3288, Loss: 1.2320613861083984, Final Batch Loss: 0.33580049872398376\n",
      "Epoch 3289, Loss: 1.4881166815757751, Final Batch Loss: 0.3952091336250305\n",
      "Epoch 3290, Loss: 1.4444168210029602, Final Batch Loss: 0.3627348840236664\n",
      "Epoch 3291, Loss: 1.5435634553432465, Final Batch Loss: 0.3389933407306671\n",
      "Epoch 3292, Loss: 1.6035622358322144, Final Batch Loss: 0.4919367730617523\n",
      "Epoch 3293, Loss: 1.4319012761116028, Final Batch Loss: 0.40674179792404175\n",
      "Epoch 3294, Loss: 1.4468171894550323, Final Batch Loss: 0.3861582577228546\n",
      "Epoch 3295, Loss: 1.4273841381072998, Final Batch Loss: 0.3499859869480133\n",
      "Epoch 3296, Loss: 1.3968406915664673, Final Batch Loss: 0.26874804496765137\n",
      "Epoch 3297, Loss: 1.5462594032287598, Final Batch Loss: 0.36421963572502136\n",
      "Epoch 3298, Loss: 1.4970373213291168, Final Batch Loss: 0.4831215739250183\n",
      "Epoch 3299, Loss: 1.433445930480957, Final Batch Loss: 0.3230898380279541\n",
      "Epoch 3300, Loss: 1.4704230725765228, Final Batch Loss: 0.40831834077835083\n",
      "Epoch 3301, Loss: 1.4918790459632874, Final Batch Loss: 0.3754408061504364\n",
      "Epoch 3302, Loss: 1.4471643269062042, Final Batch Loss: 0.30076947808265686\n",
      "Epoch 3303, Loss: 1.521581381559372, Final Batch Loss: 0.40832653641700745\n",
      "Epoch 3304, Loss: 1.46357861161232, Final Batch Loss: 0.35865482687950134\n",
      "Epoch 3305, Loss: 1.3964536488056183, Final Batch Loss: 0.37056154012680054\n",
      "Epoch 3306, Loss: 1.3316175639629364, Final Batch Loss: 0.29589492082595825\n",
      "Epoch 3307, Loss: 1.3773492276668549, Final Batch Loss: 0.37944844365119934\n",
      "Epoch 3308, Loss: 1.3918452262878418, Final Batch Loss: 0.3869539499282837\n",
      "Epoch 3309, Loss: 1.4235248863697052, Final Batch Loss: 0.3814276456832886\n",
      "Epoch 3310, Loss: 1.4424506425857544, Final Batch Loss: 0.4316915273666382\n",
      "Epoch 3311, Loss: 1.3961193561553955, Final Batch Loss: 0.3463491201400757\n",
      "Epoch 3312, Loss: 1.4289415776729584, Final Batch Loss: 0.30919066071510315\n",
      "Epoch 3313, Loss: 1.4380066394805908, Final Batch Loss: 0.3240896165370941\n",
      "Epoch 3314, Loss: 1.41129732131958, Final Batch Loss: 0.3241397738456726\n",
      "Epoch 3315, Loss: 1.4323040544986725, Final Batch Loss: 0.30684539675712585\n",
      "Epoch 3316, Loss: 1.34440478682518, Final Batch Loss: 0.31553563475608826\n",
      "Epoch 3317, Loss: 1.3777609169483185, Final Batch Loss: 0.305631548166275\n",
      "Epoch 3318, Loss: 1.3487894833087921, Final Batch Loss: 0.30835360288619995\n",
      "Epoch 3319, Loss: 1.4887060821056366, Final Batch Loss: 0.3541213274002075\n",
      "Epoch 3320, Loss: 1.3397657871246338, Final Batch Loss: 0.3025289475917816\n",
      "Epoch 3321, Loss: 1.4924761950969696, Final Batch Loss: 0.3537137806415558\n",
      "Epoch 3322, Loss: 1.4419717192649841, Final Batch Loss: 0.3466527462005615\n",
      "Epoch 3323, Loss: 1.3223599195480347, Final Batch Loss: 0.3320729434490204\n",
      "Epoch 3324, Loss: 1.3190620243549347, Final Batch Loss: 0.2843034565448761\n",
      "Epoch 3325, Loss: 1.3111908733844757, Final Batch Loss: 0.37937960028648376\n",
      "Epoch 3326, Loss: 1.2324290573596954, Final Batch Loss: 0.26940757036209106\n",
      "Epoch 3327, Loss: 1.3027729541063309, Final Batch Loss: 0.2487870305776596\n",
      "Epoch 3328, Loss: 1.4354348480701447, Final Batch Loss: 0.3513723909854889\n",
      "Epoch 3329, Loss: 1.3774507343769073, Final Batch Loss: 0.33955830335617065\n",
      "Epoch 3330, Loss: 1.3628706634044647, Final Batch Loss: 0.3411998450756073\n",
      "Epoch 3331, Loss: 1.3756065666675568, Final Batch Loss: 0.34244540333747864\n",
      "Epoch 3332, Loss: 1.3703214824199677, Final Batch Loss: 0.30199724435806274\n",
      "Epoch 3333, Loss: 1.3299552202224731, Final Batch Loss: 0.34045636653900146\n",
      "Epoch 3334, Loss: 1.3516955375671387, Final Batch Loss: 0.37188735604286194\n",
      "Epoch 3335, Loss: 1.4070045053958893, Final Batch Loss: 0.355280339717865\n",
      "Epoch 3336, Loss: 1.4020897150039673, Final Batch Loss: 0.26699379086494446\n",
      "Epoch 3337, Loss: 1.3723345696926117, Final Batch Loss: 0.2880796194076538\n",
      "Epoch 3338, Loss: 1.8053121864795685, Final Batch Loss: 0.7146921753883362\n",
      "Epoch 3339, Loss: 1.4677493572235107, Final Batch Loss: 0.4417937099933624\n",
      "Epoch 3340, Loss: 1.315478801727295, Final Batch Loss: 0.28603222966194153\n",
      "Epoch 3341, Loss: 1.445471167564392, Final Batch Loss: 0.39783525466918945\n",
      "Epoch 3342, Loss: 1.3734601736068726, Final Batch Loss: 0.3756994307041168\n",
      "Epoch 3343, Loss: 1.552028477191925, Final Batch Loss: 0.45189717411994934\n",
      "Epoch 3344, Loss: 1.2420954704284668, Final Batch Loss: 0.25146734714508057\n",
      "Epoch 3345, Loss: 1.4757040143013, Final Batch Loss: 0.30986303091049194\n",
      "Epoch 3346, Loss: 1.3138017058372498, Final Batch Loss: 0.2716031074523926\n",
      "Epoch 3347, Loss: 1.278831034898758, Final Batch Loss: 0.30651482939720154\n",
      "Epoch 3348, Loss: 1.2260662913322449, Final Batch Loss: 0.2564725875854492\n",
      "Epoch 3349, Loss: 1.433855026960373, Final Batch Loss: 0.2805045247077942\n",
      "Epoch 3350, Loss: 1.4396055936813354, Final Batch Loss: 0.33194488286972046\n",
      "Epoch 3351, Loss: 1.3705093264579773, Final Batch Loss: 0.3346219062805176\n",
      "Epoch 3352, Loss: 1.2907074689865112, Final Batch Loss: 0.32271653413772583\n",
      "Epoch 3353, Loss: 1.4378641247749329, Final Batch Loss: 0.3506961762905121\n",
      "Epoch 3354, Loss: 1.1814843863248825, Final Batch Loss: 0.18212686479091644\n",
      "Epoch 3355, Loss: 1.2154140919446945, Final Batch Loss: 0.22087647020816803\n",
      "Epoch 3356, Loss: 1.45499187707901, Final Batch Loss: 0.389538437128067\n",
      "Epoch 3357, Loss: 1.3945966064929962, Final Batch Loss: 0.37993112206459045\n",
      "Epoch 3358, Loss: 1.4573425948619843, Final Batch Loss: 0.43829450011253357\n",
      "Epoch 3359, Loss: 1.3322734534740448, Final Batch Loss: 0.2676350474357605\n",
      "Epoch 3360, Loss: 1.4227127730846405, Final Batch Loss: 0.3974657952785492\n",
      "Epoch 3361, Loss: 1.4410748779773712, Final Batch Loss: 0.4172312319278717\n",
      "Epoch 3362, Loss: 1.3082910776138306, Final Batch Loss: 0.26256388425827026\n",
      "Epoch 3363, Loss: 1.36135733127594, Final Batch Loss: 0.3035253882408142\n",
      "Epoch 3364, Loss: 1.4144983291625977, Final Batch Loss: 0.41208699345588684\n",
      "Epoch 3365, Loss: 1.3262176215648651, Final Batch Loss: 0.31259897351264954\n",
      "Epoch 3366, Loss: 1.5113558769226074, Final Batch Loss: 0.3746426999568939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3367, Loss: 1.5126251578330994, Final Batch Loss: 0.4506804049015045\n",
      "Epoch 3368, Loss: 1.3623138070106506, Final Batch Loss: 0.2752945125102997\n",
      "Epoch 3369, Loss: 1.3754575848579407, Final Batch Loss: 0.3575868606567383\n",
      "Epoch 3370, Loss: 1.3051987290382385, Final Batch Loss: 0.30729055404663086\n",
      "Epoch 3371, Loss: 1.3747050762176514, Final Batch Loss: 0.3370322287082672\n",
      "Epoch 3372, Loss: 1.334947109222412, Final Batch Loss: 0.2976871132850647\n",
      "Epoch 3373, Loss: 1.2291532754898071, Final Batch Loss: 0.2547450065612793\n",
      "Epoch 3374, Loss: 1.4553883969783783, Final Batch Loss: 0.4028822183609009\n",
      "Epoch 3375, Loss: 1.3371059596538544, Final Batch Loss: 0.3357090353965759\n",
      "Epoch 3376, Loss: 1.3296834528446198, Final Batch Loss: 0.2917197644710541\n",
      "Epoch 3377, Loss: 1.320403277873993, Final Batch Loss: 0.32964450120925903\n",
      "Epoch 3378, Loss: 1.2913739681243896, Final Batch Loss: 0.2843620777130127\n",
      "Epoch 3379, Loss: 1.2252897322177887, Final Batch Loss: 0.274795264005661\n",
      "Epoch 3380, Loss: 1.5366535484790802, Final Batch Loss: 0.2490091621875763\n",
      "Epoch 3381, Loss: 1.3463151156902313, Final Batch Loss: 0.3711412847042084\n",
      "Epoch 3382, Loss: 1.4137840270996094, Final Batch Loss: 0.36487048864364624\n",
      "Epoch 3383, Loss: 1.342489331960678, Final Batch Loss: 0.2853586971759796\n",
      "Epoch 3384, Loss: 1.3362444788217545, Final Batch Loss: 0.17717866599559784\n",
      "Epoch 3385, Loss: 1.239418312907219, Final Batch Loss: 0.3263360261917114\n",
      "Epoch 3386, Loss: 1.3322932422161102, Final Batch Loss: 0.33442771434783936\n",
      "Epoch 3387, Loss: 1.282402515411377, Final Batch Loss: 0.248559832572937\n",
      "Epoch 3388, Loss: 1.4549116790294647, Final Batch Loss: 0.37219923734664917\n",
      "Epoch 3389, Loss: 1.2705119848251343, Final Batch Loss: 0.27534255385398865\n",
      "Epoch 3390, Loss: 1.52949720621109, Final Batch Loss: 0.40092888474464417\n",
      "Epoch 3391, Loss: 1.5170120596885681, Final Batch Loss: 0.3131820857524872\n",
      "Epoch 3392, Loss: 1.386508584022522, Final Batch Loss: 0.2224043905735016\n",
      "Epoch 3393, Loss: 1.4021967351436615, Final Batch Loss: 0.43483662605285645\n",
      "Epoch 3394, Loss: 1.3323669731616974, Final Batch Loss: 0.30390721559524536\n",
      "Epoch 3395, Loss: 1.3853347599506378, Final Batch Loss: 0.3115236163139343\n",
      "Epoch 3396, Loss: 1.3347943127155304, Final Batch Loss: 0.3318256139755249\n",
      "Epoch 3397, Loss: 1.4726790189743042, Final Batch Loss: 0.39651334285736084\n",
      "Epoch 3398, Loss: 1.4062250554561615, Final Batch Loss: 0.4412324130535126\n",
      "Epoch 3399, Loss: 1.350549578666687, Final Batch Loss: 0.2998497486114502\n",
      "Epoch 3400, Loss: 1.4047612845897675, Final Batch Loss: 0.39525938034057617\n",
      "Epoch 3401, Loss: 1.5324410200119019, Final Batch Loss: 0.3145916163921356\n",
      "Epoch 3402, Loss: 1.3930922746658325, Final Batch Loss: 0.37880197167396545\n",
      "Epoch 3403, Loss: 1.3911505341529846, Final Batch Loss: 0.3952427804470062\n",
      "Epoch 3404, Loss: 1.2055242359638214, Final Batch Loss: 0.21282675862312317\n",
      "Epoch 3405, Loss: 1.4315981566905975, Final Batch Loss: 0.2944343388080597\n",
      "Epoch 3406, Loss: 1.2335103154182434, Final Batch Loss: 0.299146443605423\n",
      "Epoch 3407, Loss: 1.4567146599292755, Final Batch Loss: 0.3281112611293793\n",
      "Epoch 3408, Loss: 1.2725875079631805, Final Batch Loss: 0.23913395404815674\n",
      "Epoch 3409, Loss: 1.510333925485611, Final Batch Loss: 0.4059489369392395\n",
      "Epoch 3410, Loss: 1.4532268941402435, Final Batch Loss: 0.46674442291259766\n",
      "Epoch 3411, Loss: 1.3787254095077515, Final Batch Loss: 0.359238862991333\n",
      "Epoch 3412, Loss: 1.3771687746047974, Final Batch Loss: 0.3623543679714203\n",
      "Epoch 3413, Loss: 1.3957653641700745, Final Batch Loss: 0.35184812545776367\n",
      "Epoch 3414, Loss: 1.3789196610450745, Final Batch Loss: 0.33866626024246216\n",
      "Epoch 3415, Loss: 1.357035517692566, Final Batch Loss: 0.3953258991241455\n",
      "Epoch 3416, Loss: 1.357515424489975, Final Batch Loss: 0.3102409243583679\n",
      "Epoch 3417, Loss: 1.2822840809822083, Final Batch Loss: 0.28597041964530945\n",
      "Epoch 3418, Loss: 1.2563742995262146, Final Batch Loss: 0.2533241808414459\n",
      "Epoch 3419, Loss: 1.3559504747390747, Final Batch Loss: 0.26437103748321533\n",
      "Epoch 3420, Loss: 1.422483652830124, Final Batch Loss: 0.35315650701522827\n",
      "Epoch 3421, Loss: 1.508026123046875, Final Batch Loss: 0.46383219957351685\n",
      "Epoch 3422, Loss: 1.4870272278785706, Final Batch Loss: 0.4093727767467499\n",
      "Epoch 3423, Loss: 1.406657874584198, Final Batch Loss: 0.34038087725639343\n",
      "Epoch 3424, Loss: 1.397748440504074, Final Batch Loss: 0.3603937029838562\n",
      "Epoch 3425, Loss: 1.3643963634967804, Final Batch Loss: 0.4067418575286865\n",
      "Epoch 3426, Loss: 1.467853844165802, Final Batch Loss: 0.4176000952720642\n",
      "Epoch 3427, Loss: 1.2963041961193085, Final Batch Loss: 0.3393844664096832\n",
      "Epoch 3428, Loss: 1.2156533598899841, Final Batch Loss: 0.27592161297798157\n",
      "Epoch 3429, Loss: 1.1995544284582138, Final Batch Loss: 0.19552917778491974\n",
      "Epoch 3430, Loss: 1.398051917552948, Final Batch Loss: 0.30233636498451233\n",
      "Epoch 3431, Loss: 1.4074220657348633, Final Batch Loss: 0.43223291635513306\n",
      "Epoch 3432, Loss: 1.4444854855537415, Final Batch Loss: 0.36774417757987976\n",
      "Epoch 3433, Loss: 1.2764845341444016, Final Batch Loss: 0.238014355301857\n",
      "Epoch 3434, Loss: 1.285708099603653, Final Batch Loss: 0.28686264157295227\n",
      "Epoch 3435, Loss: 1.317727118730545, Final Batch Loss: 0.2742973864078522\n",
      "Epoch 3436, Loss: 1.3930691182613373, Final Batch Loss: 0.4550168812274933\n",
      "Epoch 3437, Loss: 1.4257702231407166, Final Batch Loss: 0.22521734237670898\n",
      "Epoch 3438, Loss: 1.3523012399673462, Final Batch Loss: 0.3545534908771515\n",
      "Epoch 3439, Loss: 1.3003214001655579, Final Batch Loss: 0.3387443721294403\n",
      "Epoch 3440, Loss: 1.4017081558704376, Final Batch Loss: 0.3738463222980499\n",
      "Epoch 3441, Loss: 1.3963775038719177, Final Batch Loss: 0.3900337517261505\n",
      "Epoch 3442, Loss: 1.3525619804859161, Final Batch Loss: 0.3299940228462219\n",
      "Epoch 3443, Loss: 1.396143615245819, Final Batch Loss: 0.4768003523349762\n",
      "Epoch 3444, Loss: 1.3826434314250946, Final Batch Loss: 0.4537656307220459\n",
      "Epoch 3445, Loss: 1.2897184789180756, Final Batch Loss: 0.2480892837047577\n",
      "Epoch 3446, Loss: 1.4503348171710968, Final Batch Loss: 0.3604240417480469\n",
      "Epoch 3447, Loss: 1.370925486087799, Final Batch Loss: 0.3259912133216858\n",
      "Epoch 3448, Loss: 1.309773474931717, Final Batch Loss: 0.3507944345474243\n",
      "Epoch 3449, Loss: 1.31082084774971, Final Batch Loss: 0.376698762178421\n",
      "Epoch 3450, Loss: 1.3712961971759796, Final Batch Loss: 0.33322873711586\n",
      "Epoch 3451, Loss: 1.4356810450553894, Final Batch Loss: 0.4134347438812256\n",
      "Epoch 3452, Loss: 1.3388954102993011, Final Batch Loss: 0.3630143105983734\n",
      "Epoch 3453, Loss: 1.371452659368515, Final Batch Loss: 0.35001012682914734\n",
      "Epoch 3454, Loss: 1.389893889427185, Final Batch Loss: 0.3606443703174591\n",
      "Epoch 3455, Loss: 1.308561384677887, Final Batch Loss: 0.3799128830432892\n",
      "Epoch 3456, Loss: 1.3239552974700928, Final Batch Loss: 0.37068411707878113\n",
      "Epoch 3457, Loss: 1.2641307562589645, Final Batch Loss: 0.34716904163360596\n",
      "Epoch 3458, Loss: 1.346070557832718, Final Batch Loss: 0.3296574652194977\n",
      "Epoch 3459, Loss: 1.269215315580368, Final Batch Loss: 0.2907348573207855\n",
      "Epoch 3460, Loss: 1.4098413586616516, Final Batch Loss: 0.40274161100387573\n",
      "Epoch 3461, Loss: 1.3689802885055542, Final Batch Loss: 0.5268760919570923\n",
      "Epoch 3462, Loss: 1.3261440694332123, Final Batch Loss: 0.33342745900154114\n",
      "Epoch 3463, Loss: 1.4848870635032654, Final Batch Loss: 0.40745535492897034\n",
      "Epoch 3464, Loss: 1.2870975136756897, Final Batch Loss: 0.2836552858352661\n",
      "Epoch 3465, Loss: 1.3451932072639465, Final Batch Loss: 0.30032777786254883\n",
      "Epoch 3466, Loss: 1.3151025772094727, Final Batch Loss: 0.29796984791755676\n",
      "Epoch 3467, Loss: 1.2817757725715637, Final Batch Loss: 0.32126253843307495\n",
      "Epoch 3468, Loss: 1.3141896724700928, Final Batch Loss: 0.23983484506607056\n",
      "Epoch 3469, Loss: 1.289864793419838, Final Batch Loss: 0.26353082060813904\n",
      "Epoch 3470, Loss: 1.2092176973819733, Final Batch Loss: 0.2623746395111084\n",
      "Epoch 3471, Loss: 1.3095383942127228, Final Batch Loss: 0.31185293197631836\n",
      "Epoch 3472, Loss: 1.3537560999393463, Final Batch Loss: 0.2221207618713379\n",
      "Epoch 3473, Loss: 1.3821088373661041, Final Batch Loss: 0.3571007549762726\n",
      "Epoch 3474, Loss: 1.3336617052555084, Final Batch Loss: 0.3074505031108856\n",
      "Epoch 3475, Loss: 1.3040368258953094, Final Batch Loss: 0.3053451478481293\n",
      "Epoch 3476, Loss: 1.3603229522705078, Final Batch Loss: 0.3105822801589966\n",
      "Epoch 3477, Loss: 1.5403034090995789, Final Batch Loss: 0.40910235047340393\n",
      "Epoch 3478, Loss: 1.5063159763813019, Final Batch Loss: 0.3832900822162628\n",
      "Epoch 3479, Loss: 1.3623729944229126, Final Batch Loss: 0.3279328942298889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3480, Loss: 1.3456582427024841, Final Batch Loss: 0.38279980421066284\n",
      "Epoch 3481, Loss: 1.5095003545284271, Final Batch Loss: 0.4622907042503357\n",
      "Epoch 3482, Loss: 1.3028380274772644, Final Batch Loss: 0.3628603219985962\n",
      "Epoch 3483, Loss: 1.5200446248054504, Final Batch Loss: 0.35449713468551636\n",
      "Epoch 3484, Loss: 1.421639859676361, Final Batch Loss: 0.313879132270813\n",
      "Epoch 3485, Loss: 1.214870423078537, Final Batch Loss: 0.24095222353935242\n",
      "Epoch 3486, Loss: 1.4749993681907654, Final Batch Loss: 0.4234899580478668\n",
      "Epoch 3487, Loss: 1.3194774389266968, Final Batch Loss: 0.2618583142757416\n",
      "Epoch 3488, Loss: 1.421566218137741, Final Batch Loss: 0.37536880373954773\n",
      "Epoch 3489, Loss: 1.2873115539550781, Final Batch Loss: 0.2860138714313507\n",
      "Epoch 3490, Loss: 1.3646140694618225, Final Batch Loss: 0.33718639612197876\n",
      "Epoch 3491, Loss: 1.366822987794876, Final Batch Loss: 0.3371433615684509\n",
      "Epoch 3492, Loss: 1.3302561938762665, Final Batch Loss: 0.35885700583457947\n",
      "Epoch 3493, Loss: 1.3788387179374695, Final Batch Loss: 0.3141818046569824\n",
      "Epoch 3494, Loss: 1.3021325469017029, Final Batch Loss: 0.3179515302181244\n",
      "Epoch 3495, Loss: 1.466823250055313, Final Batch Loss: 0.39156970381736755\n",
      "Epoch 3496, Loss: 1.583347886800766, Final Batch Loss: 0.3477553129196167\n",
      "Epoch 3497, Loss: 1.6061833798885345, Final Batch Loss: 0.5009808540344238\n",
      "Epoch 3498, Loss: 1.2832386791706085, Final Batch Loss: 0.29940345883369446\n",
      "Epoch 3499, Loss: 1.3578480780124664, Final Batch Loss: 0.39100635051727295\n",
      "Epoch 3500, Loss: 1.5191672444343567, Final Batch Loss: 0.40174123644828796\n",
      "Epoch 3501, Loss: 1.3363544940948486, Final Batch Loss: 0.3722256124019623\n",
      "Epoch 3502, Loss: 1.3287811279296875, Final Batch Loss: 0.3345268666744232\n",
      "Epoch 3503, Loss: 1.300103634595871, Final Batch Loss: 0.29054000973701477\n",
      "Epoch 3504, Loss: 1.3807694911956787, Final Batch Loss: 0.32792630791664124\n",
      "Epoch 3505, Loss: 1.3385822772979736, Final Batch Loss: 0.32175058126449585\n",
      "Epoch 3506, Loss: 1.3663875758647919, Final Batch Loss: 0.36638668179512024\n",
      "Epoch 3507, Loss: 1.284999504685402, Final Batch Loss: 0.2433367818593979\n",
      "Epoch 3508, Loss: 1.2759164869785309, Final Batch Loss: 0.3217540979385376\n",
      "Epoch 3509, Loss: 1.348799154162407, Final Batch Loss: 0.3624318838119507\n",
      "Epoch 3510, Loss: 1.3138893246650696, Final Batch Loss: 0.36268654465675354\n",
      "Epoch 3511, Loss: 1.210616186261177, Final Batch Loss: 0.2753245234489441\n",
      "Epoch 3512, Loss: 1.4271398484706879, Final Batch Loss: 0.28100043535232544\n",
      "Epoch 3513, Loss: 1.3160628974437714, Final Batch Loss: 0.39366814494132996\n",
      "Epoch 3514, Loss: 1.5117235481739044, Final Batch Loss: 0.38837793469429016\n",
      "Epoch 3515, Loss: 1.2981124520301819, Final Batch Loss: 0.37573888897895813\n",
      "Epoch 3516, Loss: 1.3222152590751648, Final Batch Loss: 0.25434333086013794\n",
      "Epoch 3517, Loss: 1.412971705198288, Final Batch Loss: 0.3492146134376526\n",
      "Epoch 3518, Loss: 1.3164406418800354, Final Batch Loss: 0.2873711585998535\n",
      "Epoch 3519, Loss: 1.2815894484519958, Final Batch Loss: 0.3488132059574127\n",
      "Epoch 3520, Loss: 1.29217329621315, Final Batch Loss: 0.28364503383636475\n",
      "Epoch 3521, Loss: 1.2737583816051483, Final Batch Loss: 0.26517587900161743\n",
      "Epoch 3522, Loss: 1.3244852125644684, Final Batch Loss: 0.3067775070667267\n",
      "Epoch 3523, Loss: 1.412462830543518, Final Batch Loss: 0.2807002663612366\n",
      "Epoch 3524, Loss: 1.333146095275879, Final Batch Loss: 0.32743924856185913\n",
      "Epoch 3525, Loss: 1.2774778008460999, Final Batch Loss: 0.26485610008239746\n",
      "Epoch 3526, Loss: 1.4741818308830261, Final Batch Loss: 0.43629416823387146\n",
      "Epoch 3527, Loss: 1.4655342400074005, Final Batch Loss: 0.3919329047203064\n",
      "Epoch 3528, Loss: 1.436807096004486, Final Batch Loss: 0.31695041060447693\n",
      "Epoch 3529, Loss: 1.2183695137500763, Final Batch Loss: 0.2736993134021759\n",
      "Epoch 3530, Loss: 1.277739092707634, Final Batch Loss: 0.2297023981809616\n",
      "Epoch 3531, Loss: 1.4313142597675323, Final Batch Loss: 0.37368059158325195\n",
      "Epoch 3532, Loss: 1.295678436756134, Final Batch Loss: 0.3103213608264923\n",
      "Epoch 3533, Loss: 1.325315684080124, Final Batch Loss: 0.3724115788936615\n",
      "Epoch 3534, Loss: 1.274868667125702, Final Batch Loss: 0.3650057911872864\n",
      "Epoch 3535, Loss: 1.4740052819252014, Final Batch Loss: 0.3333488404750824\n",
      "Epoch 3536, Loss: 1.2557737231254578, Final Batch Loss: 0.27036502957344055\n",
      "Epoch 3537, Loss: 1.284903645515442, Final Batch Loss: 0.33803313970565796\n",
      "Epoch 3538, Loss: 1.2560352087020874, Final Batch Loss: 0.3507472276687622\n",
      "Epoch 3539, Loss: 1.2968345284461975, Final Batch Loss: 0.2698515057563782\n",
      "Epoch 3540, Loss: 1.3507709503173828, Final Batch Loss: 0.3605578541755676\n",
      "Epoch 3541, Loss: 1.2655799984931946, Final Batch Loss: 0.3393838703632355\n",
      "Epoch 3542, Loss: 1.3699904382228851, Final Batch Loss: 0.4280593693256378\n",
      "Epoch 3543, Loss: 1.3319151997566223, Final Batch Loss: 0.31626543402671814\n",
      "Epoch 3544, Loss: 1.4849556982517242, Final Batch Loss: 0.3116272985935211\n",
      "Epoch 3545, Loss: 1.3282052874565125, Final Batch Loss: 0.26565641164779663\n",
      "Epoch 3546, Loss: 1.2910393178462982, Final Batch Loss: 0.2563336491584778\n",
      "Epoch 3547, Loss: 1.4347181618213654, Final Batch Loss: 0.36515358090400696\n",
      "Epoch 3548, Loss: 1.3334712982177734, Final Batch Loss: 0.29013726115226746\n",
      "Epoch 3549, Loss: 1.3502558469772339, Final Batch Loss: 0.33368980884552\n",
      "Epoch 3550, Loss: 1.3114205300807953, Final Batch Loss: 0.38253188133239746\n",
      "Epoch 3551, Loss: 1.3756915628910065, Final Batch Loss: 0.3381219506263733\n",
      "Epoch 3552, Loss: 1.2622894644737244, Final Batch Loss: 0.257026731967926\n",
      "Epoch 3553, Loss: 1.3998175859451294, Final Batch Loss: 0.47362059354782104\n",
      "Epoch 3554, Loss: 1.319860190153122, Final Batch Loss: 0.465359091758728\n",
      "Epoch 3555, Loss: 1.4830073118209839, Final Batch Loss: 0.33988311886787415\n",
      "Epoch 3556, Loss: 1.3282887637615204, Final Batch Loss: 0.36316314339637756\n",
      "Epoch 3557, Loss: 1.2460748851299286, Final Batch Loss: 0.31374093890190125\n",
      "Epoch 3558, Loss: 1.385070413351059, Final Batch Loss: 0.4070000946521759\n",
      "Epoch 3559, Loss: 1.2817327380180359, Final Batch Loss: 0.26240506768226624\n",
      "Epoch 3560, Loss: 1.431084781885147, Final Batch Loss: 0.3245113790035248\n",
      "Epoch 3561, Loss: 1.3154497593641281, Final Batch Loss: 0.2459750920534134\n",
      "Epoch 3562, Loss: 1.4265524744987488, Final Batch Loss: 0.40933626890182495\n",
      "Epoch 3563, Loss: 1.3614293336868286, Final Batch Loss: 0.3892233073711395\n",
      "Epoch 3564, Loss: 1.3611962497234344, Final Batch Loss: 0.3879760205745697\n",
      "Epoch 3565, Loss: 1.3309840559959412, Final Batch Loss: 0.34625348448753357\n",
      "Epoch 3566, Loss: 1.3082592189311981, Final Batch Loss: 0.347592830657959\n",
      "Epoch 3567, Loss: 1.3198595643043518, Final Batch Loss: 0.34532201290130615\n",
      "Epoch 3568, Loss: 1.406693935394287, Final Batch Loss: 0.3617318272590637\n",
      "Epoch 3569, Loss: 1.3793677389621735, Final Batch Loss: 0.31123438477516174\n",
      "Epoch 3570, Loss: 1.4096385836601257, Final Batch Loss: 0.34046128392219543\n",
      "Epoch 3571, Loss: 1.3629900813102722, Final Batch Loss: 0.3664464056491852\n",
      "Epoch 3572, Loss: 1.3460617065429688, Final Batch Loss: 0.3581087291240692\n",
      "Epoch 3573, Loss: 1.3345780968666077, Final Batch Loss: 0.3103596866130829\n",
      "Epoch 3574, Loss: 1.2660262882709503, Final Batch Loss: 0.3617570400238037\n",
      "Epoch 3575, Loss: 1.466273307800293, Final Batch Loss: 0.3003235459327698\n",
      "Epoch 3576, Loss: 1.3206963539123535, Final Batch Loss: 0.32725268602371216\n",
      "Epoch 3577, Loss: 1.3042689263820648, Final Batch Loss: 0.3715755343437195\n",
      "Epoch 3578, Loss: 1.3274865448474884, Final Batch Loss: 0.3619913160800934\n",
      "Epoch 3579, Loss: 1.3165635168552399, Final Batch Loss: 0.2758813202381134\n",
      "Epoch 3580, Loss: 1.3005083203315735, Final Batch Loss: 0.2972484529018402\n",
      "Epoch 3581, Loss: 1.3561642169952393, Final Batch Loss: 0.3102111518383026\n",
      "Epoch 3582, Loss: 1.3419271409511566, Final Batch Loss: 0.3493693768978119\n",
      "Epoch 3583, Loss: 1.3584754467010498, Final Batch Loss: 0.3046545088291168\n",
      "Epoch 3584, Loss: 1.2217383086681366, Final Batch Loss: 0.33193740248680115\n",
      "Epoch 3585, Loss: 1.2565913498401642, Final Batch Loss: 0.34415221214294434\n",
      "Epoch 3586, Loss: 1.3384199738502502, Final Batch Loss: 0.26331859827041626\n",
      "Epoch 3587, Loss: 1.2205979526042938, Final Batch Loss: 0.2436160445213318\n",
      "Epoch 3588, Loss: 1.257407933473587, Final Batch Loss: 0.26574385166168213\n",
      "Epoch 3589, Loss: 1.3153327107429504, Final Batch Loss: 0.30318957567214966\n",
      "Epoch 3590, Loss: 1.4423995614051819, Final Batch Loss: 0.31694820523262024\n",
      "Epoch 3591, Loss: 1.2296684384346008, Final Batch Loss: 0.3465171754360199\n",
      "Epoch 3592, Loss: 1.2935144007205963, Final Batch Loss: 0.2899544835090637\n",
      "Epoch 3593, Loss: 1.3803240060806274, Final Batch Loss: 0.4136163592338562\n",
      "Epoch 3594, Loss: 1.4117631912231445, Final Batch Loss: 0.34106549620628357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3595, Loss: 1.3118915855884552, Final Batch Loss: 0.40040990710258484\n",
      "Epoch 3596, Loss: 1.4609850645065308, Final Batch Loss: 0.3617700934410095\n",
      "Epoch 3597, Loss: 1.4793719053268433, Final Batch Loss: 0.4762478768825531\n",
      "Epoch 3598, Loss: 1.2574367225170135, Final Batch Loss: 0.28661778569221497\n",
      "Epoch 3599, Loss: 1.3492071032524109, Final Batch Loss: 0.3806445002555847\n",
      "Epoch 3600, Loss: 1.1933497488498688, Final Batch Loss: 0.2869320809841156\n",
      "Epoch 3601, Loss: 1.3217097371816635, Final Batch Loss: 0.3983731269836426\n",
      "Epoch 3602, Loss: 1.3980500996112823, Final Batch Loss: 0.47062283754348755\n",
      "Epoch 3603, Loss: 1.3916629552841187, Final Batch Loss: 0.33324864506721497\n",
      "Epoch 3604, Loss: 1.377918154001236, Final Batch Loss: 0.3455585241317749\n",
      "Epoch 3605, Loss: 1.3110748827457428, Final Batch Loss: 0.31306707859039307\n",
      "Epoch 3606, Loss: 1.3962882161140442, Final Batch Loss: 0.333357036113739\n",
      "Epoch 3607, Loss: 1.4780460596084595, Final Batch Loss: 0.44584885239601135\n",
      "Epoch 3608, Loss: 1.2471201717853546, Final Batch Loss: 0.3000740706920624\n",
      "Epoch 3609, Loss: 1.159241110086441, Final Batch Loss: 0.3006493151187897\n",
      "Epoch 3610, Loss: 1.3768536150455475, Final Batch Loss: 0.39395034313201904\n",
      "Epoch 3611, Loss: 1.389901041984558, Final Batch Loss: 0.2852570116519928\n",
      "Epoch 3612, Loss: 1.36284139752388, Final Batch Loss: 0.3006002604961395\n",
      "Epoch 3613, Loss: 1.418583482503891, Final Batch Loss: 0.3607636094093323\n",
      "Epoch 3614, Loss: 1.4600113034248352, Final Batch Loss: 0.4452012777328491\n",
      "Epoch 3615, Loss: 1.310752958059311, Final Batch Loss: 0.2855323255062103\n",
      "Epoch 3616, Loss: 1.286749705672264, Final Batch Loss: 0.23102878034114838\n",
      "Epoch 3617, Loss: 1.3828149437904358, Final Batch Loss: 0.32101109623908997\n",
      "Epoch 3618, Loss: 1.3469149321317673, Final Batch Loss: 0.3439600169658661\n",
      "Epoch 3619, Loss: 1.2381688058376312, Final Batch Loss: 0.3696584105491638\n",
      "Epoch 3620, Loss: 1.5196222364902496, Final Batch Loss: 0.5430610179901123\n",
      "Epoch 3621, Loss: 1.1949425488710403, Final Batch Loss: 0.22827474772930145\n",
      "Epoch 3622, Loss: 1.3782519102096558, Final Batch Loss: 0.4055054783821106\n",
      "Epoch 3623, Loss: 1.2763161063194275, Final Batch Loss: 0.3362623155117035\n",
      "Epoch 3624, Loss: 1.3104188442230225, Final Batch Loss: 0.3384801745414734\n",
      "Epoch 3625, Loss: 1.2815270721912384, Final Batch Loss: 0.2852494418621063\n",
      "Epoch 3626, Loss: 1.3940255641937256, Final Batch Loss: 0.3096553385257721\n",
      "Epoch 3627, Loss: 1.4680978953838348, Final Batch Loss: 0.5220131874084473\n",
      "Epoch 3628, Loss: 1.269896924495697, Final Batch Loss: 0.322113573551178\n",
      "Epoch 3629, Loss: 1.4582911729812622, Final Batch Loss: 0.4016346037387848\n",
      "Epoch 3630, Loss: 1.4683851301670074, Final Batch Loss: 0.28668147325515747\n",
      "Epoch 3631, Loss: 1.1878465116024017, Final Batch Loss: 0.30240747332572937\n",
      "Epoch 3632, Loss: 1.2488153874874115, Final Batch Loss: 0.25092098116874695\n",
      "Epoch 3633, Loss: 1.4011261761188507, Final Batch Loss: 0.38219401240348816\n",
      "Epoch 3634, Loss: 1.2239882797002792, Final Batch Loss: 0.24519352614879608\n",
      "Epoch 3635, Loss: 1.3193292617797852, Final Batch Loss: 0.3681158721446991\n",
      "Epoch 3636, Loss: 1.3473553657531738, Final Batch Loss: 0.3942106366157532\n",
      "Epoch 3637, Loss: 1.405825138092041, Final Batch Loss: 0.398798406124115\n",
      "Epoch 3638, Loss: 1.341846078634262, Final Batch Loss: 0.3044856786727905\n",
      "Epoch 3639, Loss: 1.2758301794528961, Final Batch Loss: 0.33765214681625366\n",
      "Epoch 3640, Loss: 1.30050528049469, Final Batch Loss: 0.32680439949035645\n",
      "Epoch 3641, Loss: 1.3858077228069305, Final Batch Loss: 0.28047844767570496\n",
      "Epoch 3642, Loss: 1.5355237126350403, Final Batch Loss: 0.5147386193275452\n",
      "Epoch 3643, Loss: 1.2936933636665344, Final Batch Loss: 0.31528589129447937\n",
      "Epoch 3644, Loss: 1.32205930352211, Final Batch Loss: 0.28427186608314514\n",
      "Epoch 3645, Loss: 1.3921990394592285, Final Batch Loss: 0.2835559546947479\n",
      "Epoch 3646, Loss: 1.3438953161239624, Final Batch Loss: 0.32866865396499634\n",
      "Epoch 3647, Loss: 1.428824633359909, Final Batch Loss: 0.31453248858451843\n",
      "Epoch 3648, Loss: 1.4476642906665802, Final Batch Loss: 0.2883823812007904\n",
      "Epoch 3649, Loss: 1.259212851524353, Final Batch Loss: 0.31312665343284607\n",
      "Epoch 3650, Loss: 1.3756428360939026, Final Batch Loss: 0.3518819212913513\n",
      "Epoch 3651, Loss: 1.2942011654376984, Final Batch Loss: 0.3516387641429901\n",
      "Epoch 3652, Loss: 1.3466041386127472, Final Batch Loss: 0.2808755934238434\n",
      "Epoch 3653, Loss: 1.416483759880066, Final Batch Loss: 0.41648030281066895\n",
      "Epoch 3654, Loss: 1.2861602306365967, Final Batch Loss: 0.3107849657535553\n",
      "Epoch 3655, Loss: 1.2560749650001526, Final Batch Loss: 0.37842318415641785\n",
      "Epoch 3656, Loss: 1.4891659617424011, Final Batch Loss: 0.3629436194896698\n",
      "Epoch 3657, Loss: 1.3100869059562683, Final Batch Loss: 0.3493320643901825\n",
      "Epoch 3658, Loss: 1.337173193693161, Final Batch Loss: 0.34076279401779175\n",
      "Epoch 3659, Loss: 1.308713048696518, Final Batch Loss: 0.3181615471839905\n",
      "Epoch 3660, Loss: 1.3248314559459686, Final Batch Loss: 0.41621914505958557\n",
      "Epoch 3661, Loss: 1.29477921128273, Final Batch Loss: 0.3204513192176819\n",
      "Epoch 3662, Loss: 1.2410282641649246, Final Batch Loss: 0.2359563559293747\n",
      "Epoch 3663, Loss: 1.3252061605453491, Final Batch Loss: 0.35908862948417664\n",
      "Epoch 3664, Loss: 1.4085109233856201, Final Batch Loss: 0.3157142996788025\n",
      "Epoch 3665, Loss: 1.3230127394199371, Final Batch Loss: 0.33812373876571655\n",
      "Epoch 3666, Loss: 1.4424735605716705, Final Batch Loss: 0.46267184615135193\n",
      "Epoch 3667, Loss: 1.44004625082016, Final Batch Loss: 0.3996196687221527\n",
      "Epoch 3668, Loss: 1.3579592108726501, Final Batch Loss: 0.3129977583885193\n",
      "Epoch 3669, Loss: 1.2256998121738434, Final Batch Loss: 0.23603206872940063\n",
      "Epoch 3670, Loss: 1.2928795516490936, Final Batch Loss: 0.3953433930873871\n",
      "Epoch 3671, Loss: 1.3899663090705872, Final Batch Loss: 0.3692103326320648\n",
      "Epoch 3672, Loss: 1.2025521993637085, Final Batch Loss: 0.3316028416156769\n",
      "Epoch 3673, Loss: 1.4444580674171448, Final Batch Loss: 0.423056036233902\n",
      "Epoch 3674, Loss: 1.3437791466712952, Final Batch Loss: 0.31166550517082214\n",
      "Epoch 3675, Loss: 1.3187050819396973, Final Batch Loss: 0.3597451150417328\n",
      "Epoch 3676, Loss: 1.3340620696544647, Final Batch Loss: 0.3227121829986572\n",
      "Epoch 3677, Loss: 1.2466053366661072, Final Batch Loss: 0.28577399253845215\n",
      "Epoch 3678, Loss: 1.2871476709842682, Final Batch Loss: 0.3453621566295624\n",
      "Epoch 3679, Loss: 1.4386629164218903, Final Batch Loss: 0.48020437359809875\n",
      "Epoch 3680, Loss: 1.3237596452236176, Final Batch Loss: 0.27358001470565796\n",
      "Epoch 3681, Loss: 1.3091722130775452, Final Batch Loss: 0.28812289237976074\n",
      "Epoch 3682, Loss: 1.3687414824962616, Final Batch Loss: 0.3079148530960083\n",
      "Epoch 3683, Loss: 1.1774283647537231, Final Batch Loss: 0.27206358313560486\n",
      "Epoch 3684, Loss: 1.2403993606567383, Final Batch Loss: 0.28569817543029785\n",
      "Epoch 3685, Loss: 1.3694394528865814, Final Batch Loss: 0.4413261413574219\n",
      "Epoch 3686, Loss: 1.2614901214838028, Final Batch Loss: 0.1919659823179245\n",
      "Epoch 3687, Loss: 1.4339869618415833, Final Batch Loss: 0.3763633072376251\n",
      "Epoch 3688, Loss: 1.502663642168045, Final Batch Loss: 0.44681039452552795\n",
      "Epoch 3689, Loss: 1.3121173977851868, Final Batch Loss: 0.3600379526615143\n",
      "Epoch 3690, Loss: 1.32510644197464, Final Batch Loss: 0.30627694725990295\n",
      "Epoch 3691, Loss: 1.4317187368869781, Final Batch Loss: 0.45111143589019775\n",
      "Epoch 3692, Loss: 1.2276746332645416, Final Batch Loss: 0.25118106603622437\n",
      "Epoch 3693, Loss: 1.3854932487010956, Final Batch Loss: 0.28453198075294495\n",
      "Epoch 3694, Loss: 1.401346892118454, Final Batch Loss: 0.41628792881965637\n",
      "Epoch 3695, Loss: 1.3438740074634552, Final Batch Loss: 0.3293432593345642\n",
      "Epoch 3696, Loss: 1.562780499458313, Final Batch Loss: 0.33902162313461304\n",
      "Epoch 3697, Loss: 1.358678549528122, Final Batch Loss: 0.3874535858631134\n",
      "Epoch 3698, Loss: 1.3676028549671173, Final Batch Loss: 0.33957305550575256\n",
      "Epoch 3699, Loss: 1.2189951837062836, Final Batch Loss: 0.22596153616905212\n",
      "Epoch 3700, Loss: 1.325181245803833, Final Batch Loss: 0.34520193934440613\n",
      "Epoch 3701, Loss: 1.3346514403820038, Final Batch Loss: 0.3179757595062256\n",
      "Epoch 3702, Loss: 1.4169440269470215, Final Batch Loss: 0.3079456388950348\n",
      "Epoch 3703, Loss: 1.3179142773151398, Final Batch Loss: 0.33538737893104553\n",
      "Epoch 3704, Loss: 1.427039235830307, Final Batch Loss: 0.4001104235649109\n",
      "Epoch 3705, Loss: 1.388385146856308, Final Batch Loss: 0.4335137605667114\n",
      "Epoch 3706, Loss: 1.3293731212615967, Final Batch Loss: 0.29153886437416077\n",
      "Epoch 3707, Loss: 1.1828750371932983, Final Batch Loss: 0.23207661509513855\n",
      "Epoch 3708, Loss: 1.3807466328144073, Final Batch Loss: 0.36108389496803284\n",
      "Epoch 3709, Loss: 1.2903024554252625, Final Batch Loss: 0.3232104182243347\n",
      "Epoch 3710, Loss: 1.3130159080028534, Final Batch Loss: 0.3885984718799591\n",
      "Epoch 3711, Loss: 1.2585110664367676, Final Batch Loss: 0.3109687268733978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3712, Loss: 1.3245574235916138, Final Batch Loss: 0.3778180181980133\n",
      "Epoch 3713, Loss: 1.32547989487648, Final Batch Loss: 0.28545039892196655\n",
      "Epoch 3714, Loss: 1.2864657640457153, Final Batch Loss: 0.3403218388557434\n",
      "Epoch 3715, Loss: 1.3522422909736633, Final Batch Loss: 0.35563135147094727\n",
      "Epoch 3716, Loss: 1.3888252675533295, Final Batch Loss: 0.37711235880851746\n",
      "Epoch 3717, Loss: 1.342999279499054, Final Batch Loss: 0.34836968779563904\n",
      "Epoch 3718, Loss: 1.3534020483493805, Final Batch Loss: 0.29426875710487366\n",
      "Epoch 3719, Loss: 1.2550576627254486, Final Batch Loss: 0.2417275607585907\n",
      "Epoch 3720, Loss: 1.169012412428856, Final Batch Loss: 0.22750870883464813\n",
      "Epoch 3721, Loss: 1.1769573092460632, Final Batch Loss: 0.23118510842323303\n",
      "Epoch 3722, Loss: 1.3995678126811981, Final Batch Loss: 0.3299865424633026\n",
      "Epoch 3723, Loss: 1.3188467621803284, Final Batch Loss: 0.31805217266082764\n",
      "Epoch 3724, Loss: 1.354285717010498, Final Batch Loss: 0.37691816687583923\n",
      "Epoch 3725, Loss: 1.347549170255661, Final Batch Loss: 0.3058079481124878\n",
      "Epoch 3726, Loss: 1.3506074249744415, Final Batch Loss: 0.24847427010536194\n",
      "Epoch 3727, Loss: 1.3540073931217194, Final Batch Loss: 0.3257242441177368\n",
      "Epoch 3728, Loss: 1.3226975500583649, Final Batch Loss: 0.29300886392593384\n",
      "Epoch 3729, Loss: 1.3272881507873535, Final Batch Loss: 0.39119401574134827\n",
      "Epoch 3730, Loss: 1.3305890262126923, Final Batch Loss: 0.32063156366348267\n",
      "Epoch 3731, Loss: 1.4328840374946594, Final Batch Loss: 0.3798642158508301\n",
      "Epoch 3732, Loss: 1.3079575300216675, Final Batch Loss: 0.3257138431072235\n",
      "Epoch 3733, Loss: 1.3962003886699677, Final Batch Loss: 0.32141733169555664\n",
      "Epoch 3734, Loss: 1.3118743896484375, Final Batch Loss: 0.296044260263443\n",
      "Epoch 3735, Loss: 1.3919908106327057, Final Batch Loss: 0.2925506830215454\n",
      "Epoch 3736, Loss: 1.427477091550827, Final Batch Loss: 0.38703444600105286\n",
      "Epoch 3737, Loss: 1.2560216784477234, Final Batch Loss: 0.3203960061073303\n",
      "Epoch 3738, Loss: 1.2405531108379364, Final Batch Loss: 0.32000061869621277\n",
      "Epoch 3739, Loss: 1.2710113823413849, Final Batch Loss: 0.2781656086444855\n",
      "Epoch 3740, Loss: 1.3120779991149902, Final Batch Loss: 0.3905198872089386\n",
      "Epoch 3741, Loss: 1.3044414222240448, Final Batch Loss: 0.36275917291641235\n",
      "Epoch 3742, Loss: 1.261180430650711, Final Batch Loss: 0.33316323161125183\n",
      "Epoch 3743, Loss: 1.2631097733974457, Final Batch Loss: 0.3215442895889282\n",
      "Epoch 3744, Loss: 1.4799170196056366, Final Batch Loss: 0.38484063744544983\n",
      "Epoch 3745, Loss: 1.3930233716964722, Final Batch Loss: 0.507826566696167\n",
      "Epoch 3746, Loss: 1.3441015183925629, Final Batch Loss: 0.3975801169872284\n",
      "Epoch 3747, Loss: 1.3798218071460724, Final Batch Loss: 0.37652480602264404\n",
      "Epoch 3748, Loss: 1.4392773509025574, Final Batch Loss: 0.42719611525535583\n",
      "Epoch 3749, Loss: 1.1718093305826187, Final Batch Loss: 0.24012957513332367\n",
      "Epoch 3750, Loss: 1.3136395514011383, Final Batch Loss: 0.28363853693008423\n",
      "Epoch 3751, Loss: 1.3448266088962555, Final Batch Loss: 0.393999844789505\n",
      "Epoch 3752, Loss: 1.3627575933933258, Final Batch Loss: 0.34235021471977234\n",
      "Epoch 3753, Loss: 1.346516102552414, Final Batch Loss: 0.36306267976760864\n",
      "Epoch 3754, Loss: 1.4434335231781006, Final Batch Loss: 0.3981499671936035\n",
      "Epoch 3755, Loss: 1.3965092599391937, Final Batch Loss: 0.4057995676994324\n",
      "Epoch 3756, Loss: 1.362934410572052, Final Batch Loss: 0.2953762710094452\n",
      "Epoch 3757, Loss: 1.179919183254242, Final Batch Loss: 0.25746211409568787\n",
      "Epoch 3758, Loss: 1.4217242896556854, Final Batch Loss: 0.35271504521369934\n",
      "Epoch 3759, Loss: 1.3898832201957703, Final Batch Loss: 0.3338017463684082\n",
      "Epoch 3760, Loss: 1.2467472851276398, Final Batch Loss: 0.2821720540523529\n",
      "Epoch 3761, Loss: 1.3173890709877014, Final Batch Loss: 0.2930227518081665\n",
      "Epoch 3762, Loss: 1.2539601027965546, Final Batch Loss: 0.25131428241729736\n",
      "Epoch 3763, Loss: 1.180805653333664, Final Batch Loss: 0.23342344164848328\n",
      "Epoch 3764, Loss: 1.3116488456726074, Final Batch Loss: 0.4204252064228058\n",
      "Epoch 3765, Loss: 1.252276360988617, Final Batch Loss: 0.27116087079048157\n",
      "Epoch 3766, Loss: 1.2314340323209763, Final Batch Loss: 0.23850063979625702\n",
      "Epoch 3767, Loss: 1.434964269399643, Final Batch Loss: 0.34237948060035706\n",
      "Epoch 3768, Loss: 1.1694454550743103, Final Batch Loss: 0.2782262861728668\n",
      "Epoch 3769, Loss: 1.3821073472499847, Final Batch Loss: 0.3282565772533417\n",
      "Epoch 3770, Loss: 1.2646513432264328, Final Batch Loss: 0.2464580088853836\n",
      "Epoch 3771, Loss: 1.3633835911750793, Final Batch Loss: 0.413280725479126\n",
      "Epoch 3772, Loss: 1.2678366601467133, Final Batch Loss: 0.26826775074005127\n",
      "Epoch 3773, Loss: 1.1674213409423828, Final Batch Loss: 0.27599284052848816\n",
      "Epoch 3774, Loss: 1.6948683857917786, Final Batch Loss: 0.7862616181373596\n",
      "Epoch 3775, Loss: 1.293432891368866, Final Batch Loss: 0.3524971008300781\n",
      "Epoch 3776, Loss: 1.2740594148635864, Final Batch Loss: 0.3700348734855652\n",
      "Epoch 3777, Loss: 1.2087847590446472, Final Batch Loss: 0.2654322683811188\n",
      "Epoch 3778, Loss: 1.3810506165027618, Final Batch Loss: 0.4312402606010437\n",
      "Epoch 3779, Loss: 1.347250521183014, Final Batch Loss: 0.3706425428390503\n",
      "Epoch 3780, Loss: 1.462116539478302, Final Batch Loss: 0.3731517493724823\n",
      "Epoch 3781, Loss: 1.3973968923091888, Final Batch Loss: 0.3532756268978119\n",
      "Epoch 3782, Loss: 1.2642763257026672, Final Batch Loss: 0.29851022362709045\n",
      "Epoch 3783, Loss: 1.4075978994369507, Final Batch Loss: 0.3425774872303009\n",
      "Epoch 3784, Loss: 1.2789346277713776, Final Batch Loss: 0.31067582964897156\n",
      "Epoch 3785, Loss: 1.2968293130397797, Final Batch Loss: 0.4201001822948456\n",
      "Epoch 3786, Loss: 1.3770250529050827, Final Batch Loss: 0.2376934140920639\n",
      "Epoch 3787, Loss: 1.3814346492290497, Final Batch Loss: 0.332716703414917\n",
      "Epoch 3788, Loss: 1.234580785036087, Final Batch Loss: 0.24648264050483704\n",
      "Epoch 3789, Loss: 1.3238971829414368, Final Batch Loss: 0.4327636659145355\n",
      "Epoch 3790, Loss: 1.2751070261001587, Final Batch Loss: 0.3059438169002533\n",
      "Epoch 3791, Loss: 1.265991896390915, Final Batch Loss: 0.37269705533981323\n",
      "Epoch 3792, Loss: 1.3457269966602325, Final Batch Loss: 0.3505218029022217\n",
      "Epoch 3793, Loss: 1.2306668758392334, Final Batch Loss: 0.25057855248451233\n",
      "Epoch 3794, Loss: 1.347765475511551, Final Batch Loss: 0.4497353136539459\n",
      "Epoch 3795, Loss: 1.273522138595581, Final Batch Loss: 0.33818480372428894\n",
      "Epoch 3796, Loss: 1.2858602404594421, Final Batch Loss: 0.317609578371048\n",
      "Epoch 3797, Loss: 1.417639583349228, Final Batch Loss: 0.2771482467651367\n",
      "Epoch 3798, Loss: 1.332707554101944, Final Batch Loss: 0.33820122480392456\n",
      "Epoch 3799, Loss: 1.1088464707136154, Final Batch Loss: 0.21000392735004425\n",
      "Epoch 3800, Loss: 1.1986059248447418, Final Batch Loss: 0.3170982599258423\n",
      "Epoch 3801, Loss: 1.3094202280044556, Final Batch Loss: 0.405520498752594\n",
      "Epoch 3802, Loss: 1.4301732182502747, Final Batch Loss: 0.359937459230423\n",
      "Epoch 3803, Loss: 1.3635389804840088, Final Batch Loss: 0.30326905846595764\n",
      "Epoch 3804, Loss: 1.2211710512638092, Final Batch Loss: 0.2965744435787201\n",
      "Epoch 3805, Loss: 1.3284737467765808, Final Batch Loss: 0.35333117842674255\n",
      "Epoch 3806, Loss: 1.3395259082317352, Final Batch Loss: 0.35692381858825684\n",
      "Epoch 3807, Loss: 1.3845147490501404, Final Batch Loss: 0.3431430160999298\n",
      "Epoch 3808, Loss: 1.2092394828796387, Final Batch Loss: 0.2896111309528351\n",
      "Epoch 3809, Loss: 1.2989915311336517, Final Batch Loss: 0.3651529848575592\n",
      "Epoch 3810, Loss: 1.3459253013134003, Final Batch Loss: 0.2560988962650299\n",
      "Epoch 3811, Loss: 1.214982807636261, Final Batch Loss: 0.29741066694259644\n",
      "Epoch 3812, Loss: 1.4634875059127808, Final Batch Loss: 0.40451329946517944\n",
      "Epoch 3813, Loss: 1.335915058851242, Final Batch Loss: 0.3286350965499878\n",
      "Epoch 3814, Loss: 1.3200048208236694, Final Batch Loss: 0.28001412749290466\n",
      "Epoch 3815, Loss: 1.3197209239006042, Final Batch Loss: 0.28199851512908936\n",
      "Epoch 3816, Loss: 1.3784706592559814, Final Batch Loss: 0.4973103702068329\n",
      "Epoch 3817, Loss: 1.2431620657444, Final Batch Loss: 0.29873189330101013\n",
      "Epoch 3818, Loss: 1.3293038606643677, Final Batch Loss: 0.28697556257247925\n",
      "Epoch 3819, Loss: 1.2899258434772491, Final Batch Loss: 0.3611971437931061\n",
      "Epoch 3820, Loss: 1.2193310260772705, Final Batch Loss: 0.2889164388179779\n",
      "Epoch 3821, Loss: 1.3762231171131134, Final Batch Loss: 0.3607725501060486\n",
      "Epoch 3822, Loss: 1.3570094406604767, Final Batch Loss: 0.29407379031181335\n",
      "Epoch 3823, Loss: 1.4506204724311829, Final Batch Loss: 0.4180237650871277\n",
      "Epoch 3824, Loss: 1.3185089826583862, Final Batch Loss: 0.3410836458206177\n",
      "Epoch 3825, Loss: 1.2749861478805542, Final Batch Loss: 0.34776392579078674\n",
      "Epoch 3826, Loss: 1.382273256778717, Final Batch Loss: 0.3588970899581909\n",
      "Epoch 3827, Loss: 1.3186480700969696, Final Batch Loss: 0.3678308129310608\n",
      "Epoch 3828, Loss: 1.3429321646690369, Final Batch Loss: 0.3505406975746155\n",
      "Epoch 3829, Loss: 1.2946833074092865, Final Batch Loss: 0.31847262382507324\n",
      "Epoch 3830, Loss: 1.462718814611435, Final Batch Loss: 0.3991420865058899\n",
      "Epoch 3831, Loss: 1.254640519618988, Final Batch Loss: 0.32719770073890686\n",
      "Epoch 3832, Loss: 1.4350119829177856, Final Batch Loss: 0.39557453989982605\n",
      "Epoch 3833, Loss: 1.2574875950813293, Final Batch Loss: 0.24245280027389526\n",
      "Epoch 3834, Loss: 1.2910423576831818, Final Batch Loss: 0.4438868761062622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3835, Loss: 1.215769201517105, Final Batch Loss: 0.2892295718193054\n",
      "Epoch 3836, Loss: 1.1822825074195862, Final Batch Loss: 0.2284955382347107\n",
      "Epoch 3837, Loss: 1.3724107444286346, Final Batch Loss: 0.281017005443573\n",
      "Epoch 3838, Loss: 1.385273426771164, Final Batch Loss: 0.29812100529670715\n",
      "Epoch 3839, Loss: 1.1452455818653107, Final Batch Loss: 0.17569121718406677\n",
      "Epoch 3840, Loss: 1.264488160610199, Final Batch Loss: 0.30640459060668945\n",
      "Epoch 3841, Loss: 1.3027075231075287, Final Batch Loss: 0.3150281310081482\n",
      "Epoch 3842, Loss: 1.288008213043213, Final Batch Loss: 0.35866931080818176\n",
      "Epoch 3843, Loss: 1.3281101286411285, Final Batch Loss: 0.34572911262512207\n",
      "Epoch 3844, Loss: 1.254681646823883, Final Batch Loss: 0.3330961763858795\n",
      "Epoch 3845, Loss: 1.4165790677070618, Final Batch Loss: 0.31891176104545593\n",
      "Epoch 3846, Loss: 1.3503279983997345, Final Batch Loss: 0.2984134554862976\n",
      "Epoch 3847, Loss: 1.209892064332962, Final Batch Loss: 0.26760134100914\n",
      "Epoch 3848, Loss: 1.2751315236091614, Final Batch Loss: 0.4403725862503052\n",
      "Epoch 3849, Loss: 1.196791261434555, Final Batch Loss: 0.23581010103225708\n",
      "Epoch 3850, Loss: 1.1309105902910233, Final Batch Loss: 0.334215372800827\n",
      "Epoch 3851, Loss: 1.292487621307373, Final Batch Loss: 0.3067946434020996\n",
      "Epoch 3852, Loss: 1.2234895527362823, Final Batch Loss: 0.2978065013885498\n",
      "Epoch 3853, Loss: 1.1928565502166748, Final Batch Loss: 0.26324135065078735\n",
      "Epoch 3854, Loss: 1.2577560245990753, Final Batch Loss: 0.2961519956588745\n",
      "Epoch 3855, Loss: 1.284460335969925, Final Batch Loss: 0.3034384846687317\n",
      "Epoch 3856, Loss: 1.2715794444084167, Final Batch Loss: 0.32220083475112915\n",
      "Epoch 3857, Loss: 1.1932235062122345, Final Batch Loss: 0.29549357295036316\n",
      "Epoch 3858, Loss: 1.3622335195541382, Final Batch Loss: 0.4089978039264679\n",
      "Epoch 3859, Loss: 1.2787747979164124, Final Batch Loss: 0.34982216358184814\n",
      "Epoch 3860, Loss: 1.2829813659191132, Final Batch Loss: 0.41339144110679626\n",
      "Epoch 3861, Loss: 1.2086175680160522, Final Batch Loss: 0.3565492331981659\n",
      "Epoch 3862, Loss: 1.2653634548187256, Final Batch Loss: 0.3126927614212036\n",
      "Epoch 3863, Loss: 1.2252426743507385, Final Batch Loss: 0.3222574293613434\n",
      "Epoch 3864, Loss: 1.4502962231636047, Final Batch Loss: 0.43287932872772217\n",
      "Epoch 3865, Loss: 1.3427925109863281, Final Batch Loss: 0.2832590341567993\n",
      "Epoch 3866, Loss: 1.447171300649643, Final Batch Loss: 0.46481114625930786\n",
      "Epoch 3867, Loss: 1.3060841262340546, Final Batch Loss: 0.3611730933189392\n",
      "Epoch 3868, Loss: 1.3005077838897705, Final Batch Loss: 0.44500643014907837\n",
      "Epoch 3869, Loss: 1.4299664795398712, Final Batch Loss: 0.3366445302963257\n",
      "Epoch 3870, Loss: 1.3161974847316742, Final Batch Loss: 0.4146101772785187\n",
      "Epoch 3871, Loss: 1.3438449501991272, Final Batch Loss: 0.3075096309185028\n",
      "Epoch 3872, Loss: 1.2363582849502563, Final Batch Loss: 0.31828466057777405\n",
      "Epoch 3873, Loss: 1.3662244975566864, Final Batch Loss: 0.2676316499710083\n",
      "Epoch 3874, Loss: 1.384433627128601, Final Batch Loss: 0.30120211839675903\n",
      "Epoch 3875, Loss: 1.312529057264328, Final Batch Loss: 0.352618932723999\n",
      "Epoch 3876, Loss: 1.2253995537757874, Final Batch Loss: 0.28927385807037354\n",
      "Epoch 3877, Loss: 1.3215816020965576, Final Batch Loss: 0.2517681121826172\n",
      "Epoch 3878, Loss: 1.2984381318092346, Final Batch Loss: 0.30787163972854614\n",
      "Epoch 3879, Loss: 1.3521508276462555, Final Batch Loss: 0.3348190486431122\n",
      "Epoch 3880, Loss: 1.1982619911432266, Final Batch Loss: 0.294439435005188\n",
      "Epoch 3881, Loss: 1.4132618606090546, Final Batch Loss: 0.3831009864807129\n",
      "Epoch 3882, Loss: 1.1767632812261581, Final Batch Loss: 0.33916354179382324\n",
      "Epoch 3883, Loss: 1.0765902251005173, Final Batch Loss: 0.3125443458557129\n",
      "Epoch 3884, Loss: 1.3458536118268967, Final Batch Loss: 0.2395961433649063\n",
      "Epoch 3885, Loss: 1.2379416972398758, Final Batch Loss: 0.2198750525712967\n",
      "Epoch 3886, Loss: 1.2215911149978638, Final Batch Loss: 0.3086894452571869\n",
      "Epoch 3887, Loss: 1.2505893409252167, Final Batch Loss: 0.34308990836143494\n",
      "Epoch 3888, Loss: 1.2677220404148102, Final Batch Loss: 0.3161299526691437\n",
      "Epoch 3889, Loss: 1.3093262314796448, Final Batch Loss: 0.3290102779865265\n",
      "Epoch 3890, Loss: 1.2492704093456268, Final Batch Loss: 0.3044172525405884\n",
      "Epoch 3891, Loss: 1.2068555057048798, Final Batch Loss: 0.28157752752304077\n",
      "Epoch 3892, Loss: 1.3919024467468262, Final Batch Loss: 0.29717880487442017\n",
      "Epoch 3893, Loss: 1.3002438843250275, Final Batch Loss: 0.3167635500431061\n",
      "Epoch 3894, Loss: 1.2724355459213257, Final Batch Loss: 0.31365734338760376\n",
      "Epoch 3895, Loss: 1.3437184989452362, Final Batch Loss: 0.27951478958129883\n",
      "Epoch 3896, Loss: 1.1840559244155884, Final Batch Loss: 0.26751503348350525\n",
      "Epoch 3897, Loss: 1.3511623740196228, Final Batch Loss: 0.37784287333488464\n",
      "Epoch 3898, Loss: 1.340134859085083, Final Batch Loss: 0.37149715423583984\n",
      "Epoch 3899, Loss: 1.185477375984192, Final Batch Loss: 0.18237030506134033\n",
      "Epoch 3900, Loss: 1.2536555081605911, Final Batch Loss: 0.3109099864959717\n",
      "Epoch 3901, Loss: 1.3349593877792358, Final Batch Loss: 0.28561073541641235\n",
      "Epoch 3902, Loss: 1.4084007740020752, Final Batch Loss: 0.4407303035259247\n",
      "Epoch 3903, Loss: 1.3707081079483032, Final Batch Loss: 0.4932897388935089\n",
      "Epoch 3904, Loss: 1.2499911785125732, Final Batch Loss: 0.26835185289382935\n",
      "Epoch 3905, Loss: 1.2487393021583557, Final Batch Loss: 0.31288906931877136\n",
      "Epoch 3906, Loss: 1.3532336354255676, Final Batch Loss: 0.34418123960494995\n",
      "Epoch 3907, Loss: 1.244900792837143, Final Batch Loss: 0.28204774856567383\n",
      "Epoch 3908, Loss: 1.3159036934375763, Final Batch Loss: 0.4301615059375763\n",
      "Epoch 3909, Loss: 1.3467314541339874, Final Batch Loss: 0.3271331191062927\n",
      "Epoch 3910, Loss: 1.3204503059387207, Final Batch Loss: 0.39602139592170715\n",
      "Epoch 3911, Loss: 1.3388254642486572, Final Batch Loss: 0.2547757625579834\n",
      "Epoch 3912, Loss: 1.173630565404892, Final Batch Loss: 0.3131949007511139\n",
      "Epoch 3913, Loss: 1.3312317728996277, Final Batch Loss: 0.32170382142066956\n",
      "Epoch 3914, Loss: 1.3812225759029388, Final Batch Loss: 0.4129430949687958\n",
      "Epoch 3915, Loss: 1.3769562393426895, Final Batch Loss: 0.4514995515346527\n",
      "Epoch 3916, Loss: 1.2327109575271606, Final Batch Loss: 0.3300781846046448\n",
      "Epoch 3917, Loss: 1.2340510487556458, Final Batch Loss: 0.27413123846054077\n",
      "Epoch 3918, Loss: 1.2876534461975098, Final Batch Loss: 0.33880138397216797\n",
      "Epoch 3919, Loss: 1.3874905109405518, Final Batch Loss: 0.2712997496128082\n",
      "Epoch 3920, Loss: 1.2968689501285553, Final Batch Loss: 0.2677072584629059\n",
      "Epoch 3921, Loss: 1.3657138645648956, Final Batch Loss: 0.30267030000686646\n",
      "Epoch 3922, Loss: 1.2343376278877258, Final Batch Loss: 0.37218940258026123\n",
      "Epoch 3923, Loss: 1.3085999190807343, Final Batch Loss: 0.34974831342697144\n",
      "Epoch 3924, Loss: 1.3402371406555176, Final Batch Loss: 0.3807092308998108\n",
      "Epoch 3925, Loss: 1.2458704710006714, Final Batch Loss: 0.29809731245040894\n",
      "Epoch 3926, Loss: 1.4236121475696564, Final Batch Loss: 0.3566892147064209\n",
      "Epoch 3927, Loss: 1.3687856197357178, Final Batch Loss: 0.3079935610294342\n",
      "Epoch 3928, Loss: 1.3323305547237396, Final Batch Loss: 0.29251036047935486\n",
      "Epoch 3929, Loss: 1.2509422600269318, Final Batch Loss: 0.3139232397079468\n",
      "Epoch 3930, Loss: 1.3654820322990417, Final Batch Loss: 0.39447927474975586\n",
      "Epoch 3931, Loss: 1.2877106368541718, Final Batch Loss: 0.37021294236183167\n",
      "Epoch 3932, Loss: 1.2153012156486511, Final Batch Loss: 0.2818108797073364\n",
      "Epoch 3933, Loss: 1.3256519436836243, Final Batch Loss: 0.3913297653198242\n",
      "Epoch 3934, Loss: 1.2425276935100555, Final Batch Loss: 0.35121476650238037\n",
      "Epoch 3935, Loss: 1.3659531474113464, Final Batch Loss: 0.3861526548862457\n",
      "Epoch 3936, Loss: 1.221964567899704, Final Batch Loss: 0.3464345335960388\n",
      "Epoch 3937, Loss: 1.2383478879928589, Final Batch Loss: 0.34544774889945984\n",
      "Epoch 3938, Loss: 1.3091912269592285, Final Batch Loss: 0.32077556848526\n",
      "Epoch 3939, Loss: 1.4184857606887817, Final Batch Loss: 0.4326726794242859\n",
      "Epoch 3940, Loss: 1.4311310946941376, Final Batch Loss: 0.4205155372619629\n",
      "Epoch 3941, Loss: 1.367041915655136, Final Batch Loss: 0.3767373263835907\n",
      "Epoch 3942, Loss: 1.3526093065738678, Final Batch Loss: 0.3290230929851532\n",
      "Epoch 3943, Loss: 1.2672791182994843, Final Batch Loss: 0.36804038286209106\n",
      "Epoch 3944, Loss: 1.4155573546886444, Final Batch Loss: 0.36910590529441833\n",
      "Epoch 3945, Loss: 1.3347946405410767, Final Batch Loss: 0.30482423305511475\n",
      "Epoch 3946, Loss: 1.159773349761963, Final Batch Loss: 0.26484254002571106\n",
      "Epoch 3947, Loss: 1.284154623746872, Final Batch Loss: 0.3909592032432556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3948, Loss: 1.332421213388443, Final Batch Loss: 0.28711017966270447\n",
      "Epoch 3949, Loss: 1.2926314622163773, Final Batch Loss: 0.3978458046913147\n",
      "Epoch 3950, Loss: 1.1426938772201538, Final Batch Loss: 0.26477718353271484\n",
      "Epoch 3951, Loss: 1.4091814160346985, Final Batch Loss: 0.4693935513496399\n",
      "Epoch 3952, Loss: 1.1666936874389648, Final Batch Loss: 0.39341655373573303\n",
      "Epoch 3953, Loss: 1.294551968574524, Final Batch Loss: 0.3147944211959839\n",
      "Epoch 3954, Loss: 1.2227869778871536, Final Batch Loss: 0.25387415289878845\n",
      "Epoch 3955, Loss: 1.2087573111057281, Final Batch Loss: 0.2934039235115051\n",
      "Epoch 3956, Loss: 1.177628993988037, Final Batch Loss: 0.2694751024246216\n",
      "Epoch 3957, Loss: 1.436530441045761, Final Batch Loss: 0.2514745891094208\n",
      "Epoch 3958, Loss: 1.4565812647342682, Final Batch Loss: 0.32968032360076904\n",
      "Epoch 3959, Loss: 1.3442407846450806, Final Batch Loss: 0.3500649333000183\n",
      "Epoch 3960, Loss: 1.1525550484657288, Final Batch Loss: 0.26923471689224243\n",
      "Epoch 3961, Loss: 1.2174735963344574, Final Batch Loss: 0.3784889280796051\n",
      "Epoch 3962, Loss: 1.2185001373291016, Final Batch Loss: 0.23429125547409058\n",
      "Epoch 3963, Loss: 1.2626238763332367, Final Batch Loss: 0.37455493211746216\n",
      "Epoch 3964, Loss: 1.2388648390769958, Final Batch Loss: 0.23643627762794495\n",
      "Epoch 3965, Loss: 1.3312324285507202, Final Batch Loss: 0.27999240159988403\n",
      "Epoch 3966, Loss: 1.298857033252716, Final Batch Loss: 0.3231966197490692\n",
      "Epoch 3967, Loss: 1.1779206991195679, Final Batch Loss: 0.29856666922569275\n",
      "Epoch 3968, Loss: 1.3548021912574768, Final Batch Loss: 0.4593895971775055\n",
      "Epoch 3969, Loss: 1.4518036246299744, Final Batch Loss: 0.3356859087944031\n",
      "Epoch 3970, Loss: 1.5212038159370422, Final Batch Loss: 0.4442468285560608\n",
      "Epoch 3971, Loss: 1.3306282758712769, Final Batch Loss: 0.3259284794330597\n",
      "Epoch 3972, Loss: 1.4117382764816284, Final Batch Loss: 0.29412955045700073\n",
      "Epoch 3973, Loss: 1.2392479330301285, Final Batch Loss: 0.285354882478714\n",
      "Epoch 3974, Loss: 1.3000704646110535, Final Batch Loss: 0.251395046710968\n",
      "Epoch 3975, Loss: 1.3171047866344452, Final Batch Loss: 0.3214053511619568\n",
      "Epoch 3976, Loss: 1.2600039541721344, Final Batch Loss: 0.3121176064014435\n",
      "Epoch 3977, Loss: 1.22394260764122, Final Batch Loss: 0.23400753736495972\n",
      "Epoch 3978, Loss: 1.3560127019882202, Final Batch Loss: 0.27083808183670044\n",
      "Epoch 3979, Loss: 1.3881122469902039, Final Batch Loss: 0.33498650789260864\n",
      "Epoch 3980, Loss: 1.4114725589752197, Final Batch Loss: 0.3838113248348236\n",
      "Epoch 3981, Loss: 1.5245717465877533, Final Batch Loss: 0.46570634841918945\n",
      "Epoch 3982, Loss: 1.4000378251075745, Final Batch Loss: 0.3207855820655823\n",
      "Epoch 3983, Loss: 1.4935304522514343, Final Batch Loss: 0.5133704543113708\n",
      "Epoch 3984, Loss: 1.328751564025879, Final Batch Loss: 0.2777751684188843\n",
      "Epoch 3985, Loss: 1.4003250896930695, Final Batch Loss: 0.3911498188972473\n",
      "Epoch 3986, Loss: 1.2771691381931305, Final Batch Loss: 0.3594026565551758\n",
      "Epoch 3987, Loss: 1.392459750175476, Final Batch Loss: 0.3885573446750641\n",
      "Epoch 3988, Loss: 1.3734363317489624, Final Batch Loss: 0.3534311056137085\n",
      "Epoch 3989, Loss: 1.3145999908447266, Final Batch Loss: 0.39228561520576477\n",
      "Epoch 3990, Loss: 1.2147584855556488, Final Batch Loss: 0.27997347712516785\n",
      "Epoch 3991, Loss: 1.327420949935913, Final Batch Loss: 0.3459048867225647\n",
      "Epoch 3992, Loss: 1.3635459542274475, Final Batch Loss: 0.2956601083278656\n",
      "Epoch 3993, Loss: 1.26963609457016, Final Batch Loss: 0.2967991232872009\n",
      "Epoch 3994, Loss: 1.4252313077449799, Final Batch Loss: 0.34827861189842224\n",
      "Epoch 3995, Loss: 1.3779345750808716, Final Batch Loss: 0.3635646402835846\n",
      "Epoch 3996, Loss: 1.2645785808563232, Final Batch Loss: 0.3130064904689789\n",
      "Epoch 3997, Loss: 1.3770028948783875, Final Batch Loss: 0.36234936118125916\n",
      "Epoch 3998, Loss: 1.3340591490268707, Final Batch Loss: 0.2597346305847168\n",
      "Epoch 3999, Loss: 1.285673588514328, Final Batch Loss: 0.32763931155204773\n",
      "Epoch 4000, Loss: 1.359890729188919, Final Batch Loss: 0.3051317632198334\n",
      "Epoch 4001, Loss: 1.3430896699428558, Final Batch Loss: 0.308199405670166\n",
      "Epoch 4002, Loss: 1.2623842507600784, Final Batch Loss: 0.24017734825611115\n",
      "Epoch 4003, Loss: 1.3496530055999756, Final Batch Loss: 0.4057949483394623\n",
      "Epoch 4004, Loss: 1.2964144051074982, Final Batch Loss: 0.3429107964038849\n",
      "Epoch 4005, Loss: 1.2595669031143188, Final Batch Loss: 0.3658056855201721\n",
      "Epoch 4006, Loss: 1.226707398891449, Final Batch Loss: 0.27148133516311646\n",
      "Epoch 4007, Loss: 1.3047977685928345, Final Batch Loss: 0.3443622887134552\n",
      "Epoch 4008, Loss: 1.2538261711597443, Final Batch Loss: 0.30761879682540894\n",
      "Epoch 4009, Loss: 1.3085632026195526, Final Batch Loss: 0.36065447330474854\n",
      "Epoch 4010, Loss: 1.1892926096916199, Final Batch Loss: 0.28827542066574097\n",
      "Epoch 4011, Loss: 1.2419588267803192, Final Batch Loss: 0.28910723328590393\n",
      "Epoch 4012, Loss: 1.190081149339676, Final Batch Loss: 0.30887168645858765\n",
      "Epoch 4013, Loss: 1.187540888786316, Final Batch Loss: 0.26319655776023865\n",
      "Epoch 4014, Loss: 1.4334205090999603, Final Batch Loss: 0.42022988200187683\n",
      "Epoch 4015, Loss: 1.2982152849435806, Final Batch Loss: 0.38446903228759766\n",
      "Epoch 4016, Loss: 1.3655615448951721, Final Batch Loss: 0.3482525050640106\n",
      "Epoch 4017, Loss: 1.3137879967689514, Final Batch Loss: 0.3338374197483063\n",
      "Epoch 4018, Loss: 1.3195155262947083, Final Batch Loss: 0.263484925031662\n",
      "Epoch 4019, Loss: 1.3169797658920288, Final Batch Loss: 0.4110037684440613\n",
      "Epoch 4020, Loss: 1.248332679271698, Final Batch Loss: 0.301113486289978\n",
      "Epoch 4021, Loss: 1.3986772000789642, Final Batch Loss: 0.40932583808898926\n",
      "Epoch 4022, Loss: 1.4131406545639038, Final Batch Loss: 0.41268011927604675\n",
      "Epoch 4023, Loss: 1.2579082250595093, Final Batch Loss: 0.3385623097419739\n",
      "Epoch 4024, Loss: 1.2880312502384186, Final Batch Loss: 0.30543646216392517\n",
      "Epoch 4025, Loss: 1.25983327627182, Final Batch Loss: 0.30518102645874023\n",
      "Epoch 4026, Loss: 1.3779642879962921, Final Batch Loss: 0.3553301990032196\n",
      "Epoch 4027, Loss: 1.2551466226577759, Final Batch Loss: 0.2392112910747528\n",
      "Epoch 4028, Loss: 1.6146568655967712, Final Batch Loss: 0.6237419843673706\n",
      "Epoch 4029, Loss: 1.3722564578056335, Final Batch Loss: 0.3143390715122223\n",
      "Epoch 4030, Loss: 1.4692304134368896, Final Batch Loss: 0.35385680198669434\n",
      "Epoch 4031, Loss: 1.331769347190857, Final Batch Loss: 0.31219735741615295\n",
      "Epoch 4032, Loss: 1.3078247010707855, Final Batch Loss: 0.30616599321365356\n",
      "Epoch 4033, Loss: 1.2453097701072693, Final Batch Loss: 0.3071068823337555\n",
      "Epoch 4034, Loss: 1.3073922395706177, Final Batch Loss: 0.3840136229991913\n",
      "Epoch 4035, Loss: 1.3819618225097656, Final Batch Loss: 0.2921169102191925\n",
      "Epoch 4036, Loss: 1.476077824831009, Final Batch Loss: 0.43206048011779785\n",
      "Epoch 4037, Loss: 1.2791440188884735, Final Batch Loss: 0.40695127844810486\n",
      "Epoch 4038, Loss: 1.215240329504013, Final Batch Loss: 0.3192887604236603\n",
      "Epoch 4039, Loss: 1.2158915102481842, Final Batch Loss: 0.2677708864212036\n",
      "Epoch 4040, Loss: 1.1880611777305603, Final Batch Loss: 0.21550479531288147\n",
      "Epoch 4041, Loss: 1.2674146592617035, Final Batch Loss: 0.25062790513038635\n",
      "Epoch 4042, Loss: 1.3153001070022583, Final Batch Loss: 0.26317277550697327\n",
      "Epoch 4043, Loss: 1.2548606991767883, Final Batch Loss: 0.44437292218208313\n",
      "Epoch 4044, Loss: 1.2120577096939087, Final Batch Loss: 0.26221397519111633\n",
      "Epoch 4045, Loss: 1.3536260426044464, Final Batch Loss: 0.36842843890190125\n",
      "Epoch 4046, Loss: 1.3170678615570068, Final Batch Loss: 0.35754281282424927\n",
      "Epoch 4047, Loss: 1.2474616169929504, Final Batch Loss: 0.30499035120010376\n",
      "Epoch 4048, Loss: 1.3296842277050018, Final Batch Loss: 0.2897588908672333\n",
      "Epoch 4049, Loss: 1.2796257436275482, Final Batch Loss: 0.3209225535392761\n",
      "Epoch 4050, Loss: 1.3092127740383148, Final Batch Loss: 0.3364732563495636\n",
      "Epoch 4051, Loss: 1.285276859998703, Final Batch Loss: 0.36664101481437683\n",
      "Epoch 4052, Loss: 1.2543450593948364, Final Batch Loss: 0.28987202048301697\n",
      "Epoch 4053, Loss: 1.351576328277588, Final Batch Loss: 0.41179347038269043\n",
      "Epoch 4054, Loss: 1.3117810189723969, Final Batch Loss: 0.3187754452228546\n",
      "Epoch 4055, Loss: 1.423518180847168, Final Batch Loss: 0.3639635443687439\n",
      "Epoch 4056, Loss: 1.46495121717453, Final Batch Loss: 0.5034127235412598\n",
      "Epoch 4057, Loss: 1.23913212120533, Final Batch Loss: 0.4310704171657562\n",
      "Epoch 4058, Loss: 1.3144164979457855, Final Batch Loss: 0.3063011169433594\n",
      "Epoch 4059, Loss: 1.1918945908546448, Final Batch Loss: 0.30217790603637695\n",
      "Epoch 4060, Loss: 1.323631227016449, Final Batch Loss: 0.32883501052856445\n",
      "Epoch 4061, Loss: 1.3202676475048065, Final Batch Loss: 0.4310203790664673\n",
      "Epoch 4062, Loss: 1.4276075065135956, Final Batch Loss: 0.3292734622955322\n",
      "Epoch 4063, Loss: 1.340521216392517, Final Batch Loss: 0.39470693469047546\n",
      "Epoch 4064, Loss: 1.371662825345993, Final Batch Loss: 0.2885167896747589\n",
      "Epoch 4065, Loss: 1.3028132021427155, Final Batch Loss: 0.30883342027664185\n",
      "Epoch 4066, Loss: 1.3437682390213013, Final Batch Loss: 0.33773648738861084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4067, Loss: 1.4614970088005066, Final Batch Loss: 0.3971010148525238\n",
      "Epoch 4068, Loss: 1.2700314819812775, Final Batch Loss: 0.28207385540008545\n",
      "Epoch 4069, Loss: 1.2453081160783768, Final Batch Loss: 0.23129026591777802\n",
      "Epoch 4070, Loss: 1.2818050682544708, Final Batch Loss: 0.29227209091186523\n",
      "Epoch 4071, Loss: 1.3233333826065063, Final Batch Loss: 0.3381742238998413\n",
      "Epoch 4072, Loss: 1.291281521320343, Final Batch Loss: 0.29892498254776\n",
      "Epoch 4073, Loss: 1.2482717335224152, Final Batch Loss: 0.3179762065410614\n",
      "Epoch 4074, Loss: 1.3354369401931763, Final Batch Loss: 0.28344035148620605\n",
      "Epoch 4075, Loss: 1.370951384305954, Final Batch Loss: 0.32709527015686035\n",
      "Epoch 4076, Loss: 1.182686686515808, Final Batch Loss: 0.2519383430480957\n",
      "Epoch 4077, Loss: 1.3620786666870117, Final Batch Loss: 0.33893510699272156\n",
      "Epoch 4078, Loss: 1.271419882774353, Final Batch Loss: 0.3024764955043793\n",
      "Epoch 4079, Loss: 1.22964546084404, Final Batch Loss: 0.2997046113014221\n",
      "Epoch 4080, Loss: 1.211864709854126, Final Batch Loss: 0.27923256158828735\n",
      "Epoch 4081, Loss: 1.2581977248191833, Final Batch Loss: 0.27676016092300415\n",
      "Epoch 4082, Loss: 1.1666020452976227, Final Batch Loss: 0.30980053544044495\n",
      "Epoch 4083, Loss: 1.2532680034637451, Final Batch Loss: 0.2962568402290344\n",
      "Epoch 4084, Loss: 1.2644778192043304, Final Batch Loss: 0.3281029164791107\n",
      "Epoch 4085, Loss: 1.2248227894306183, Final Batch Loss: 0.2794817090034485\n",
      "Epoch 4086, Loss: 1.4088915884494781, Final Batch Loss: 0.33613321185112\n",
      "Epoch 4087, Loss: 1.316196471452713, Final Batch Loss: 0.4060100317001343\n",
      "Epoch 4088, Loss: 1.3298849165439606, Final Batch Loss: 0.4183249771595001\n",
      "Epoch 4089, Loss: 1.140665590763092, Final Batch Loss: 0.27937933802604675\n",
      "Epoch 4090, Loss: 1.2414762377738953, Final Batch Loss: 0.31870853900909424\n",
      "Epoch 4091, Loss: 1.3105692863464355, Final Batch Loss: 0.2568597197532654\n",
      "Epoch 4092, Loss: 1.2532772421836853, Final Batch Loss: 0.300107479095459\n",
      "Epoch 4093, Loss: 1.1988507807254791, Final Batch Loss: 0.2843722701072693\n",
      "Epoch 4094, Loss: 1.2915124297142029, Final Batch Loss: 0.3558836877346039\n",
      "Epoch 4095, Loss: 1.2328110337257385, Final Batch Loss: 0.2442886233329773\n",
      "Epoch 4096, Loss: 1.3558753430843353, Final Batch Loss: 0.3651459813117981\n",
      "Epoch 4097, Loss: 1.2464625835418701, Final Batch Loss: 0.3983350396156311\n",
      "Epoch 4098, Loss: 1.2333065122365952, Final Batch Loss: 0.21213294565677643\n",
      "Epoch 4099, Loss: 1.3290647268295288, Final Batch Loss: 0.3157784938812256\n",
      "Epoch 4100, Loss: 1.1473195105791092, Final Batch Loss: 0.23550017178058624\n",
      "Epoch 4101, Loss: 1.2964155673980713, Final Batch Loss: 0.34726083278656006\n",
      "Epoch 4102, Loss: 1.257422000169754, Final Batch Loss: 0.35460904240608215\n",
      "Epoch 4103, Loss: 1.2718932330608368, Final Batch Loss: 0.33931872248649597\n",
      "Epoch 4104, Loss: 1.305842936038971, Final Batch Loss: 0.330883264541626\n",
      "Epoch 4105, Loss: 1.2137580513954163, Final Batch Loss: 0.2556861639022827\n",
      "Epoch 4106, Loss: 1.2566119730472565, Final Batch Loss: 0.2629512548446655\n",
      "Epoch 4107, Loss: 1.219042807817459, Final Batch Loss: 0.29770272970199585\n",
      "Epoch 4108, Loss: 1.2118507325649261, Final Batch Loss: 0.33435776829719543\n",
      "Epoch 4109, Loss: 1.4685812890529633, Final Batch Loss: 0.4106920063495636\n",
      "Epoch 4110, Loss: 1.067254438996315, Final Batch Loss: 0.20410969853401184\n",
      "Epoch 4111, Loss: 1.2620256841182709, Final Batch Loss: 0.35977235436439514\n",
      "Epoch 4112, Loss: 1.430979609489441, Final Batch Loss: 0.345882385969162\n",
      "Epoch 4113, Loss: 1.229610115289688, Final Batch Loss: 0.31403079628944397\n",
      "Epoch 4114, Loss: 1.2761428952217102, Final Batch Loss: 0.41798585653305054\n",
      "Epoch 4115, Loss: 1.295585721731186, Final Batch Loss: 0.3083936870098114\n",
      "Epoch 4116, Loss: 1.1974182426929474, Final Batch Loss: 0.2467471957206726\n",
      "Epoch 4117, Loss: 1.2190257906913757, Final Batch Loss: 0.25098735094070435\n",
      "Epoch 4118, Loss: 1.2065869271755219, Final Batch Loss: 0.24607491493225098\n",
      "Epoch 4119, Loss: 1.276691734790802, Final Batch Loss: 0.28364723920822144\n",
      "Epoch 4120, Loss: 1.355116754770279, Final Batch Loss: 0.32054048776626587\n",
      "Epoch 4121, Loss: 1.2476096153259277, Final Batch Loss: 0.3082159757614136\n",
      "Epoch 4122, Loss: 1.4286128878593445, Final Batch Loss: 0.44433122873306274\n",
      "Epoch 4123, Loss: 1.3334077447652817, Final Batch Loss: 0.24158255755901337\n",
      "Epoch 4124, Loss: 1.2094584852457047, Final Batch Loss: 0.19839490950107574\n",
      "Epoch 4125, Loss: 1.1346960812807083, Final Batch Loss: 0.22442467510700226\n",
      "Epoch 4126, Loss: 1.2667411267757416, Final Batch Loss: 0.354850709438324\n",
      "Epoch 4127, Loss: 1.250598058104515, Final Batch Loss: 0.30536437034606934\n",
      "Epoch 4128, Loss: 1.2777357697486877, Final Batch Loss: 0.2715449631214142\n",
      "Epoch 4129, Loss: 1.3032333552837372, Final Batch Loss: 0.3963422179222107\n",
      "Epoch 4130, Loss: 1.212411493062973, Final Batch Loss: 0.31348103284835815\n",
      "Epoch 4131, Loss: 1.2161429822444916, Final Batch Loss: 0.31396737694740295\n",
      "Epoch 4132, Loss: 1.322196364402771, Final Batch Loss: 0.3135474622249603\n",
      "Epoch 4133, Loss: 1.3345938920974731, Final Batch Loss: 0.3682698905467987\n",
      "Epoch 4134, Loss: 1.2209799885749817, Final Batch Loss: 0.2507535219192505\n",
      "Epoch 4135, Loss: 1.1390806287527084, Final Batch Loss: 0.23952631652355194\n",
      "Epoch 4136, Loss: 1.3019853830337524, Final Batch Loss: 0.38607949018478394\n",
      "Epoch 4137, Loss: 1.2665072977542877, Final Batch Loss: 0.3013567328453064\n",
      "Epoch 4138, Loss: 1.1825483441352844, Final Batch Loss: 0.23887869715690613\n",
      "Epoch 4139, Loss: 1.3114875257015228, Final Batch Loss: 0.3518171012401581\n",
      "Epoch 4140, Loss: 1.3317763358354568, Final Batch Loss: 0.3973875939846039\n",
      "Epoch 4141, Loss: 1.267968237400055, Final Batch Loss: 0.22898805141448975\n",
      "Epoch 4142, Loss: 1.230487883090973, Final Batch Loss: 0.30969613790512085\n",
      "Epoch 4143, Loss: 1.3015381395816803, Final Batch Loss: 0.37589478492736816\n",
      "Epoch 4144, Loss: 1.2061247825622559, Final Batch Loss: 0.2623721957206726\n",
      "Epoch 4145, Loss: 1.2524036169052124, Final Batch Loss: 0.2932059168815613\n",
      "Epoch 4146, Loss: 1.2575611174106598, Final Batch Loss: 0.2776127755641937\n",
      "Epoch 4147, Loss: 1.1352118104696274, Final Batch Loss: 0.22113220393657684\n",
      "Epoch 4148, Loss: 1.3118044137954712, Final Batch Loss: 0.36825963854789734\n",
      "Epoch 4149, Loss: 1.2788001149892807, Final Batch Loss: 0.2419167011976242\n",
      "Epoch 4150, Loss: 1.3434522151947021, Final Batch Loss: 0.40745824575424194\n",
      "Epoch 4151, Loss: 1.2017499506473541, Final Batch Loss: 0.27611371874809265\n",
      "Epoch 4152, Loss: 1.2217954695224762, Final Batch Loss: 0.2602374255657196\n",
      "Epoch 4153, Loss: 1.2601872086524963, Final Batch Loss: 0.30385449528694153\n",
      "Epoch 4154, Loss: 1.4587152302265167, Final Batch Loss: 0.28727471828460693\n",
      "Epoch 4155, Loss: 1.2683294415473938, Final Batch Loss: 0.37671709060668945\n",
      "Epoch 4156, Loss: 1.5044060051441193, Final Batch Loss: 0.36731550097465515\n",
      "Epoch 4157, Loss: 1.1687283962965012, Final Batch Loss: 0.23238711059093475\n",
      "Epoch 4158, Loss: 1.4142414331436157, Final Batch Loss: 0.4121793210506439\n",
      "Epoch 4159, Loss: 1.2908715307712555, Final Batch Loss: 0.26562899351119995\n",
      "Epoch 4160, Loss: 1.271555334329605, Final Batch Loss: 0.29514050483703613\n",
      "Epoch 4161, Loss: 1.3729678094387054, Final Batch Loss: 0.34466198086738586\n",
      "Epoch 4162, Loss: 1.3908423483371735, Final Batch Loss: 0.3957517743110657\n",
      "Epoch 4163, Loss: 1.3328351378440857, Final Batch Loss: 0.35028529167175293\n",
      "Epoch 4164, Loss: 1.4784119427204132, Final Batch Loss: 0.44503384828567505\n",
      "Epoch 4165, Loss: 1.311209499835968, Final Batch Loss: 0.3743942379951477\n",
      "Epoch 4166, Loss: 1.3402119278907776, Final Batch Loss: 0.344272643327713\n",
      "Epoch 4167, Loss: 1.344912052154541, Final Batch Loss: 0.34954583644866943\n",
      "Epoch 4168, Loss: 1.3661643862724304, Final Batch Loss: 0.3508146107196808\n",
      "Epoch 4169, Loss: 1.4192303568124771, Final Batch Loss: 0.30602914094924927\n",
      "Epoch 4170, Loss: 1.4109821319580078, Final Batch Loss: 0.3490974009037018\n",
      "Epoch 4171, Loss: 1.5265585482120514, Final Batch Loss: 0.32936543226242065\n",
      "Epoch 4172, Loss: 1.3553660809993744, Final Batch Loss: 0.37221959233283997\n",
      "Epoch 4173, Loss: 1.26466304063797, Final Batch Loss: 0.2813234031200409\n",
      "Epoch 4174, Loss: 1.2861586809158325, Final Batch Loss: 0.31192532181739807\n",
      "Epoch 4175, Loss: 1.2775855958461761, Final Batch Loss: 0.31302598118782043\n",
      "Epoch 4176, Loss: 1.3253876864910126, Final Batch Loss: 0.3134476840496063\n",
      "Epoch 4177, Loss: 1.3137640058994293, Final Batch Loss: 0.3044499158859253\n",
      "Epoch 4178, Loss: 1.3365957736968994, Final Batch Loss: 0.2533971965312958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4179, Loss: 1.2419466227293015, Final Batch Loss: 0.23822955787181854\n",
      "Epoch 4180, Loss: 1.385739952325821, Final Batch Loss: 0.3162679672241211\n",
      "Epoch 4181, Loss: 1.357869029045105, Final Batch Loss: 0.34928229451179504\n",
      "Epoch 4182, Loss: 1.2992594838142395, Final Batch Loss: 0.3404688537120819\n",
      "Epoch 4183, Loss: 1.2023699879646301, Final Batch Loss: 0.27771931886672974\n",
      "Epoch 4184, Loss: 1.2426919043064117, Final Batch Loss: 0.2698216438293457\n",
      "Epoch 4185, Loss: 1.4260929226875305, Final Batch Loss: 0.4423568844795227\n",
      "Epoch 4186, Loss: 1.299073040485382, Final Batch Loss: 0.3821427524089813\n",
      "Epoch 4187, Loss: 1.2191438525915146, Final Batch Loss: 0.21140272915363312\n",
      "Epoch 4188, Loss: 1.172509640455246, Final Batch Loss: 0.2198854386806488\n",
      "Epoch 4189, Loss: 1.2409012019634247, Final Batch Loss: 0.3493024408817291\n",
      "Epoch 4190, Loss: 1.2230702340602875, Final Batch Loss: 0.26993414759635925\n",
      "Epoch 4191, Loss: 1.252049446105957, Final Batch Loss: 0.29743367433547974\n",
      "Epoch 4192, Loss: 1.319066971540451, Final Batch Loss: 0.3334813714027405\n",
      "Epoch 4193, Loss: 1.1759549379348755, Final Batch Loss: 0.2652847170829773\n",
      "Epoch 4194, Loss: 1.296533077955246, Final Batch Loss: 0.3626647889614105\n",
      "Epoch 4195, Loss: 1.324265033006668, Final Batch Loss: 0.38023409247398376\n",
      "Epoch 4196, Loss: 1.2092640101909637, Final Batch Loss: 0.32996395230293274\n",
      "Epoch 4197, Loss: 1.4627671241760254, Final Batch Loss: 0.37822988629341125\n",
      "Epoch 4198, Loss: 1.2248742878437042, Final Batch Loss: 0.30684933066368103\n",
      "Epoch 4199, Loss: 1.3219237625598907, Final Batch Loss: 0.38698863983154297\n",
      "Epoch 4200, Loss: 1.2641758918762207, Final Batch Loss: 0.2817494571208954\n",
      "Epoch 4201, Loss: 1.3056255877017975, Final Batch Loss: 0.35839807987213135\n",
      "Epoch 4202, Loss: 1.2484355866909027, Final Batch Loss: 0.27035507559776306\n",
      "Epoch 4203, Loss: 1.2512384355068207, Final Batch Loss: 0.3542957007884979\n",
      "Epoch 4204, Loss: 1.2313241958618164, Final Batch Loss: 0.30083414912223816\n",
      "Epoch 4205, Loss: 1.3527973294258118, Final Batch Loss: 0.35225149989128113\n",
      "Epoch 4206, Loss: 1.3074324429035187, Final Batch Loss: 0.35195913910865784\n",
      "Epoch 4207, Loss: 1.1389951556921005, Final Batch Loss: 0.23986001312732697\n",
      "Epoch 4208, Loss: 1.3333052098751068, Final Batch Loss: 0.3517128527164459\n",
      "Epoch 4209, Loss: 1.2738532423973083, Final Batch Loss: 0.3020451068878174\n",
      "Epoch 4210, Loss: 1.1957578212022781, Final Batch Loss: 0.20223934948444366\n",
      "Epoch 4211, Loss: 1.3776275515556335, Final Batch Loss: 0.27691006660461426\n",
      "Epoch 4212, Loss: 1.3386255204677582, Final Batch Loss: 0.3539378345012665\n",
      "Epoch 4213, Loss: 1.3923338949680328, Final Batch Loss: 0.3923424780368805\n",
      "Epoch 4214, Loss: 1.3863206207752228, Final Batch Loss: 0.46544480323791504\n",
      "Epoch 4215, Loss: 1.279846042394638, Final Batch Loss: 0.24257540702819824\n",
      "Epoch 4216, Loss: 1.3324789702892303, Final Batch Loss: 0.347685843706131\n",
      "Epoch 4217, Loss: 1.2766231298446655, Final Batch Loss: 0.3397676348686218\n",
      "Epoch 4218, Loss: 1.3694920241832733, Final Batch Loss: 0.3333994150161743\n",
      "Epoch 4219, Loss: 1.3614194095134735, Final Batch Loss: 0.35608479380607605\n",
      "Epoch 4220, Loss: 1.1903244107961655, Final Batch Loss: 0.2477331906557083\n",
      "Epoch 4221, Loss: 1.3313631117343903, Final Batch Loss: 0.27740469574928284\n",
      "Epoch 4222, Loss: 1.2012542486190796, Final Batch Loss: 0.30914339423179626\n",
      "Epoch 4223, Loss: 1.2802378237247467, Final Batch Loss: 0.34428536891937256\n",
      "Epoch 4224, Loss: 1.2121484279632568, Final Batch Loss: 0.3160367012023926\n",
      "Epoch 4225, Loss: 1.3574844598770142, Final Batch Loss: 0.3449392020702362\n",
      "Epoch 4226, Loss: 1.2983582615852356, Final Batch Loss: 0.33308616280555725\n",
      "Epoch 4227, Loss: 1.260735958814621, Final Batch Loss: 0.3134150803089142\n",
      "Epoch 4228, Loss: 1.2211446464061737, Final Batch Loss: 0.21080973744392395\n",
      "Epoch 4229, Loss: 1.2597242891788483, Final Batch Loss: 0.3214796781539917\n",
      "Epoch 4230, Loss: 1.3212864398956299, Final Batch Loss: 0.265825092792511\n",
      "Epoch 4231, Loss: 1.2828088104724884, Final Batch Loss: 0.30043506622314453\n",
      "Epoch 4232, Loss: 1.394145429134369, Final Batch Loss: 0.4560418725013733\n",
      "Epoch 4233, Loss: 1.3022440373897552, Final Batch Loss: 0.3417793810367584\n",
      "Epoch 4234, Loss: 1.3143847733736038, Final Batch Loss: 0.2025422304868698\n",
      "Epoch 4235, Loss: 1.2608511447906494, Final Batch Loss: 0.2680090069770813\n",
      "Epoch 4236, Loss: 1.3478236198425293, Final Batch Loss: 0.41211429238319397\n",
      "Epoch 4237, Loss: 1.2818773984909058, Final Batch Loss: 0.2555507719516754\n",
      "Epoch 4238, Loss: 1.4861113727092743, Final Batch Loss: 0.28078410029411316\n",
      "Epoch 4239, Loss: 1.1849330365657806, Final Batch Loss: 0.3094242811203003\n",
      "Epoch 4240, Loss: 1.2059509754180908, Final Batch Loss: 0.2746027410030365\n",
      "Epoch 4241, Loss: 1.4995075464248657, Final Batch Loss: 0.4507053792476654\n",
      "Epoch 4242, Loss: 1.3025467693805695, Final Batch Loss: 0.3837704062461853\n",
      "Epoch 4243, Loss: 1.4009809494018555, Final Batch Loss: 0.3282080590724945\n",
      "Epoch 4244, Loss: 1.1812547445297241, Final Batch Loss: 0.22499200701713562\n",
      "Epoch 4245, Loss: 1.3548159301280975, Final Batch Loss: 0.3938792645931244\n",
      "Epoch 4246, Loss: 1.2490791380405426, Final Batch Loss: 0.24894952774047852\n",
      "Epoch 4247, Loss: 1.3097661137580872, Final Batch Loss: 0.3426746129989624\n",
      "Epoch 4248, Loss: 1.3466964960098267, Final Batch Loss: 0.2881617546081543\n",
      "Epoch 4249, Loss: 1.2608953714370728, Final Batch Loss: 0.3096764385700226\n",
      "Epoch 4250, Loss: 1.2364114820957184, Final Batch Loss: 0.31405603885650635\n",
      "Epoch 4251, Loss: 1.2705576419830322, Final Batch Loss: 0.29706093668937683\n",
      "Epoch 4252, Loss: 1.336523562669754, Final Batch Loss: 0.38221320509910583\n",
      "Epoch 4253, Loss: 1.2430404275655746, Final Batch Loss: 0.34394171833992004\n",
      "Epoch 4254, Loss: 1.297075629234314, Final Batch Loss: 0.35493388772010803\n",
      "Epoch 4255, Loss: 1.2745164930820465, Final Batch Loss: 0.31112387776374817\n",
      "Epoch 4256, Loss: 1.2396336495876312, Final Batch Loss: 0.32271608710289\n",
      "Epoch 4257, Loss: 1.2149339020252228, Final Batch Loss: 0.2869274616241455\n",
      "Epoch 4258, Loss: 1.2303993999958038, Final Batch Loss: 0.31336769461631775\n",
      "Epoch 4259, Loss: 1.2871533930301666, Final Batch Loss: 0.2771659195423126\n",
      "Epoch 4260, Loss: 1.1280286461114883, Final Batch Loss: 0.252323716878891\n",
      "Epoch 4261, Loss: 1.3107585310935974, Final Batch Loss: 0.39758285880088806\n",
      "Epoch 4262, Loss: 1.260612815618515, Final Batch Loss: 0.2740017771720886\n",
      "Epoch 4263, Loss: 1.3207804262638092, Final Batch Loss: 0.36183539032936096\n",
      "Epoch 4264, Loss: 1.2458209693431854, Final Batch Loss: 0.2736574113368988\n",
      "Epoch 4265, Loss: 1.4358662068843842, Final Batch Loss: 0.4047713279724121\n",
      "Epoch 4266, Loss: 1.2265486419200897, Final Batch Loss: 0.34834370017051697\n",
      "Epoch 4267, Loss: 1.088041603565216, Final Batch Loss: 0.2836776077747345\n",
      "Epoch 4268, Loss: 1.2641509771347046, Final Batch Loss: 0.3791133463382721\n",
      "Epoch 4269, Loss: 1.181385800242424, Final Batch Loss: 0.22501970827579498\n",
      "Epoch 4270, Loss: 1.16511769592762, Final Batch Loss: 0.20046670734882355\n",
      "Epoch 4271, Loss: 1.3168806433677673, Final Batch Loss: 0.27264153957366943\n",
      "Epoch 4272, Loss: 1.26717147231102, Final Batch Loss: 0.3045632541179657\n",
      "Epoch 4273, Loss: 1.2012787163257599, Final Batch Loss: 0.3052591383457184\n",
      "Epoch 4274, Loss: 1.1822361201047897, Final Batch Loss: 0.3982936441898346\n",
      "Epoch 4275, Loss: 1.259928435087204, Final Batch Loss: 0.28276559710502625\n",
      "Epoch 4276, Loss: 1.1024834215641022, Final Batch Loss: 0.2489214837551117\n",
      "Epoch 4277, Loss: 1.2819776833057404, Final Batch Loss: 0.3765523433685303\n",
      "Epoch 4278, Loss: 1.3709003329277039, Final Batch Loss: 0.3844549059867859\n",
      "Epoch 4279, Loss: 1.4076912701129913, Final Batch Loss: 0.39677906036376953\n",
      "Epoch 4280, Loss: 1.1229894906282425, Final Batch Loss: 0.26147210597991943\n",
      "Epoch 4281, Loss: 1.3202283084392548, Final Batch Loss: 0.3316437602043152\n",
      "Epoch 4282, Loss: 1.3597991466522217, Final Batch Loss: 0.39196935296058655\n",
      "Epoch 4283, Loss: 1.301750898361206, Final Batch Loss: 0.3740208148956299\n",
      "Epoch 4284, Loss: 1.1504781544208527, Final Batch Loss: 0.2513843774795532\n",
      "Epoch 4285, Loss: 1.2721145153045654, Final Batch Loss: 0.26715555787086487\n",
      "Epoch 4286, Loss: 1.2897293865680695, Final Batch Loss: 0.3790411055088043\n",
      "Epoch 4287, Loss: 1.2149941623210907, Final Batch Loss: 0.25607162714004517\n",
      "Epoch 4288, Loss: 1.3364562094211578, Final Batch Loss: 0.42794835567474365\n",
      "Epoch 4289, Loss: 1.2564421594142914, Final Batch Loss: 0.27942249178886414\n",
      "Epoch 4290, Loss: 1.232620656490326, Final Batch Loss: 0.3370949625968933\n",
      "Epoch 4291, Loss: 1.441158264875412, Final Batch Loss: 0.595778226852417\n",
      "Epoch 4292, Loss: 1.4045346677303314, Final Batch Loss: 0.5138120055198669\n",
      "Epoch 4293, Loss: 1.3068756461143494, Final Batch Loss: 0.33887389302253723\n",
      "Epoch 4294, Loss: 1.2964360415935516, Final Batch Loss: 0.2812170386314392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4295, Loss: 1.269445687532425, Final Batch Loss: 0.3478128910064697\n",
      "Epoch 4296, Loss: 1.1546395421028137, Final Batch Loss: 0.26027920842170715\n",
      "Epoch 4297, Loss: 1.3312131464481354, Final Batch Loss: 0.39553043246269226\n",
      "Epoch 4298, Loss: 1.3166496455669403, Final Batch Loss: 0.30246859788894653\n",
      "Epoch 4299, Loss: 1.4236735105514526, Final Batch Loss: 0.43092042207717896\n",
      "Epoch 4300, Loss: 1.1979896128177643, Final Batch Loss: 0.27639976143836975\n",
      "Epoch 4301, Loss: 1.1695479899644852, Final Batch Loss: 0.22755445539951324\n",
      "Epoch 4302, Loss: 1.310453623533249, Final Batch Loss: 0.4046157896518707\n",
      "Epoch 4303, Loss: 1.3527348041534424, Final Batch Loss: 0.3731335699558258\n",
      "Epoch 4304, Loss: 1.2534809410572052, Final Batch Loss: 0.2631203234195709\n",
      "Epoch 4305, Loss: 1.356332778930664, Final Batch Loss: 0.30961719155311584\n",
      "Epoch 4306, Loss: 1.3441551625728607, Final Batch Loss: 0.3975842297077179\n",
      "Epoch 4307, Loss: 1.3491004407405853, Final Batch Loss: 0.4106845557689667\n",
      "Epoch 4308, Loss: 1.3550135493278503, Final Batch Loss: 0.33987942337989807\n",
      "Epoch 4309, Loss: 1.234855830669403, Final Batch Loss: 0.27256107330322266\n",
      "Epoch 4310, Loss: 1.245407372713089, Final Batch Loss: 0.2710736393928528\n",
      "Epoch 4311, Loss: 1.1788835972547531, Final Batch Loss: 0.24939236044883728\n",
      "Epoch 4312, Loss: 1.3288950026035309, Final Batch Loss: 0.34233880043029785\n",
      "Epoch 4313, Loss: 1.2717439234256744, Final Batch Loss: 0.31618157029151917\n",
      "Epoch 4314, Loss: 1.2013685405254364, Final Batch Loss: 0.31434768438339233\n",
      "Epoch 4315, Loss: 1.266636073589325, Final Batch Loss: 0.29994574189186096\n",
      "Epoch 4316, Loss: 1.298995852470398, Final Batch Loss: 0.33966073393821716\n",
      "Epoch 4317, Loss: 1.2572208642959595, Final Batch Loss: 0.34407326579093933\n",
      "Epoch 4318, Loss: 1.4191609621047974, Final Batch Loss: 0.4527904987335205\n",
      "Epoch 4319, Loss: 1.3720609247684479, Final Batch Loss: 0.37228915095329285\n",
      "Epoch 4320, Loss: 1.2721194177865982, Final Batch Loss: 0.20560212433338165\n",
      "Epoch 4321, Loss: 1.2268949151039124, Final Batch Loss: 0.35407790541648865\n",
      "Epoch 4322, Loss: 1.186584085226059, Final Batch Loss: 0.2602440416812897\n",
      "Epoch 4323, Loss: 1.1593759953975677, Final Batch Loss: 0.2591988742351532\n",
      "Epoch 4324, Loss: 1.2446902096271515, Final Batch Loss: 0.2517048418521881\n",
      "Epoch 4325, Loss: 1.3765731751918793, Final Batch Loss: 0.35860350728034973\n",
      "Epoch 4326, Loss: 1.2616985440254211, Final Batch Loss: 0.29596948623657227\n",
      "Epoch 4327, Loss: 1.2330895364284515, Final Batch Loss: 0.398281067609787\n",
      "Epoch 4328, Loss: 1.320646196603775, Final Batch Loss: 0.2984580397605896\n",
      "Epoch 4329, Loss: 1.229891687631607, Final Batch Loss: 0.24989932775497437\n",
      "Epoch 4330, Loss: 1.3228493928909302, Final Batch Loss: 0.3323369026184082\n",
      "Epoch 4331, Loss: 1.1834571361541748, Final Batch Loss: 0.2845473289489746\n",
      "Epoch 4332, Loss: 1.1502689719200134, Final Batch Loss: 0.24498805403709412\n",
      "Epoch 4333, Loss: 1.2399666011333466, Final Batch Loss: 0.33770936727523804\n",
      "Epoch 4334, Loss: 1.312893956899643, Final Batch Loss: 0.37515759468078613\n",
      "Epoch 4335, Loss: 1.248254656791687, Final Batch Loss: 0.31429144740104675\n",
      "Epoch 4336, Loss: 1.1882566064596176, Final Batch Loss: 0.24942763149738312\n",
      "Epoch 4337, Loss: 1.2199634313583374, Final Batch Loss: 0.28152695298194885\n",
      "Epoch 4338, Loss: 1.2185963690280914, Final Batch Loss: 0.3139723241329193\n",
      "Epoch 4339, Loss: 1.297087401151657, Final Batch Loss: 0.2797386348247528\n",
      "Epoch 4340, Loss: 1.3373689651489258, Final Batch Loss: 0.41166219115257263\n",
      "Epoch 4341, Loss: 1.3442004919052124, Final Batch Loss: 0.37349990010261536\n",
      "Epoch 4342, Loss: 1.2746228873729706, Final Batch Loss: 0.29678791761398315\n",
      "Epoch 4343, Loss: 1.3613509833812714, Final Batch Loss: 0.3217408061027527\n",
      "Epoch 4344, Loss: 1.395273119211197, Final Batch Loss: 0.3555653691291809\n",
      "Epoch 4345, Loss: 1.2035205662250519, Final Batch Loss: 0.29547616839408875\n",
      "Epoch 4346, Loss: 1.2787731885910034, Final Batch Loss: 0.3959885537624359\n",
      "Epoch 4347, Loss: 1.2505993247032166, Final Batch Loss: 0.312442421913147\n",
      "Epoch 4348, Loss: 1.4735524952411652, Final Batch Loss: 0.467928022146225\n",
      "Epoch 4349, Loss: 1.2706044614315033, Final Batch Loss: 0.3516090214252472\n",
      "Epoch 4350, Loss: 1.1636778712272644, Final Batch Loss: 0.30577707290649414\n",
      "Epoch 4351, Loss: 1.3385218381881714, Final Batch Loss: 0.2887208163738251\n",
      "Epoch 4352, Loss: 1.2057850360870361, Final Batch Loss: 0.28442904353141785\n",
      "Epoch 4353, Loss: 1.1272924095392227, Final Batch Loss: 0.2857775092124939\n",
      "Epoch 4354, Loss: 1.384602963924408, Final Batch Loss: 0.4219590127468109\n",
      "Epoch 4355, Loss: 1.2443460524082184, Final Batch Loss: 0.3188503086566925\n",
      "Epoch 4356, Loss: 1.2695693373680115, Final Batch Loss: 0.33212828636169434\n",
      "Epoch 4357, Loss: 1.1024956703186035, Final Batch Loss: 0.24520966410636902\n",
      "Epoch 4358, Loss: 1.2963007390499115, Final Batch Loss: 0.3759160041809082\n",
      "Epoch 4359, Loss: 1.2402931898832321, Final Batch Loss: 0.36060458421707153\n",
      "Epoch 4360, Loss: 1.3723856806755066, Final Batch Loss: 0.40642908215522766\n",
      "Epoch 4361, Loss: 1.134621798992157, Final Batch Loss: 0.29910239577293396\n",
      "Epoch 4362, Loss: 1.1933356672525406, Final Batch Loss: 0.24646376073360443\n",
      "Epoch 4363, Loss: 1.2616419792175293, Final Batch Loss: 0.3214450180530548\n",
      "Epoch 4364, Loss: 1.2190603017807007, Final Batch Loss: 0.27555742859840393\n",
      "Epoch 4365, Loss: 1.2164470851421356, Final Batch Loss: 0.3096696734428406\n",
      "Epoch 4366, Loss: 1.1686209738254547, Final Batch Loss: 0.24506741762161255\n",
      "Epoch 4367, Loss: 1.2689128816127777, Final Batch Loss: 0.34180328249931335\n",
      "Epoch 4368, Loss: 1.2649451792240143, Final Batch Loss: 0.33418571949005127\n",
      "Epoch 4369, Loss: 1.3182855248451233, Final Batch Loss: 0.37498071789741516\n",
      "Epoch 4370, Loss: 1.166930913925171, Final Batch Loss: 0.2931874692440033\n",
      "Epoch 4371, Loss: 1.3031351268291473, Final Batch Loss: 0.45957013964653015\n",
      "Epoch 4372, Loss: 1.157971277832985, Final Batch Loss: 0.30641835927963257\n",
      "Epoch 4373, Loss: 1.376363605260849, Final Batch Loss: 0.40532320737838745\n",
      "Epoch 4374, Loss: 1.396760731935501, Final Batch Loss: 0.3866073489189148\n",
      "Epoch 4375, Loss: 1.190713793039322, Final Batch Loss: 0.3294847309589386\n",
      "Epoch 4376, Loss: 1.1486624479293823, Final Batch Loss: 0.28138381242752075\n",
      "Epoch 4377, Loss: 1.395562320947647, Final Batch Loss: 0.3726509213447571\n",
      "Epoch 4378, Loss: 1.3144672811031342, Final Batch Loss: 0.3857327401638031\n",
      "Epoch 4379, Loss: 1.2612872421741486, Final Batch Loss: 0.2708696722984314\n",
      "Epoch 4380, Loss: 1.2568937540054321, Final Batch Loss: 0.29805776476860046\n",
      "Epoch 4381, Loss: 1.249104842543602, Final Batch Loss: 0.2080865353345871\n",
      "Epoch 4382, Loss: 1.1701425909996033, Final Batch Loss: 0.28253334760665894\n",
      "Epoch 4383, Loss: 1.1203010380268097, Final Batch Loss: 0.2759670615196228\n",
      "Epoch 4384, Loss: 1.417720377445221, Final Batch Loss: 0.4377998113632202\n",
      "Epoch 4385, Loss: 1.113898828625679, Final Batch Loss: 0.2762574553489685\n",
      "Epoch 4386, Loss: 1.1714710593223572, Final Batch Loss: 0.28857937455177307\n",
      "Epoch 4387, Loss: 1.2327324748039246, Final Batch Loss: 0.29234519600868225\n",
      "Epoch 4388, Loss: 1.195423036813736, Final Batch Loss: 0.24375712871551514\n",
      "Epoch 4389, Loss: 1.3426723182201385, Final Batch Loss: 0.3828079104423523\n",
      "Epoch 4390, Loss: 1.3970542550086975, Final Batch Loss: 0.34988754987716675\n",
      "Epoch 4391, Loss: 1.318388819694519, Final Batch Loss: 0.32348158955574036\n",
      "Epoch 4392, Loss: 1.1893151998519897, Final Batch Loss: 0.296248197555542\n",
      "Epoch 4393, Loss: 1.2964029610157013, Final Batch Loss: 0.3945440948009491\n",
      "Epoch 4394, Loss: 1.236940085887909, Final Batch Loss: 0.2561950087547302\n",
      "Epoch 4395, Loss: 1.3486825823783875, Final Batch Loss: 0.3573973774909973\n",
      "Epoch 4396, Loss: 1.3262871503829956, Final Batch Loss: 0.28701862692832947\n",
      "Epoch 4397, Loss: 1.3650359511375427, Final Batch Loss: 0.2600611448287964\n",
      "Epoch 4398, Loss: 1.2107245326042175, Final Batch Loss: 0.2687167525291443\n",
      "Epoch 4399, Loss: 1.2181895226240158, Final Batch Loss: 0.34777718782424927\n",
      "Epoch 4400, Loss: 1.1702232956886292, Final Batch Loss: 0.23024249076843262\n",
      "Epoch 4401, Loss: 1.2555642127990723, Final Batch Loss: 0.3673030734062195\n",
      "Epoch 4402, Loss: 1.2969033420085907, Final Batch Loss: 0.2970953583717346\n",
      "Epoch 4403, Loss: 1.1353110671043396, Final Batch Loss: 0.33960095047950745\n",
      "Epoch 4404, Loss: 1.154892772436142, Final Batch Loss: 0.25713980197906494\n",
      "Epoch 4405, Loss: 1.2212358117103577, Final Batch Loss: 0.31976190209388733\n",
      "Epoch 4406, Loss: 1.2024055123329163, Final Batch Loss: 0.30302128195762634\n",
      "Epoch 4407, Loss: 1.2690784335136414, Final Batch Loss: 0.34351646900177\n",
      "Epoch 4408, Loss: 1.196005403995514, Final Batch Loss: 0.29030168056488037\n",
      "Epoch 4409, Loss: 1.2452664822340012, Final Batch Loss: 0.4180367887020111\n",
      "Epoch 4410, Loss: 1.143622487783432, Final Batch Loss: 0.2708520293235779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4411, Loss: 1.282733142375946, Final Batch Loss: 0.3153727650642395\n",
      "Epoch 4412, Loss: 1.2765856981277466, Final Batch Loss: 0.24297678470611572\n",
      "Epoch 4413, Loss: 1.3931743949651718, Final Batch Loss: 0.3710443675518036\n",
      "Epoch 4414, Loss: 1.1861488223075867, Final Batch Loss: 0.29236507415771484\n",
      "Epoch 4415, Loss: 1.2967348098754883, Final Batch Loss: 0.35000109672546387\n",
      "Epoch 4416, Loss: 1.281710833311081, Final Batch Loss: 0.3838943541049957\n",
      "Epoch 4417, Loss: 1.189279019832611, Final Batch Loss: 0.338443785905838\n",
      "Epoch 4418, Loss: 1.2991571724414825, Final Batch Loss: 0.28773048520088196\n",
      "Epoch 4419, Loss: 1.2535299062728882, Final Batch Loss: 0.2890126407146454\n",
      "Epoch 4420, Loss: 1.288462072610855, Final Batch Loss: 0.267238974571228\n",
      "Epoch 4421, Loss: 1.399406611919403, Final Batch Loss: 0.3110077381134033\n",
      "Epoch 4422, Loss: 1.2053655683994293, Final Batch Loss: 0.370860755443573\n",
      "Epoch 4423, Loss: 1.420196145772934, Final Batch Loss: 0.42457979917526245\n",
      "Epoch 4424, Loss: 1.4986612200737, Final Batch Loss: 0.514771044254303\n",
      "Epoch 4425, Loss: 1.3707818388938904, Final Batch Loss: 0.2749302089214325\n",
      "Epoch 4426, Loss: 1.1506260931491852, Final Batch Loss: 0.24319544434547424\n",
      "Epoch 4427, Loss: 1.1626139879226685, Final Batch Loss: 0.2772220969200134\n",
      "Epoch 4428, Loss: 1.222817301750183, Final Batch Loss: 0.24634981155395508\n",
      "Epoch 4429, Loss: 1.316819965839386, Final Batch Loss: 0.4123634994029999\n",
      "Epoch 4430, Loss: 1.2049683332443237, Final Batch Loss: 0.32804203033447266\n",
      "Epoch 4431, Loss: 1.4780203700065613, Final Batch Loss: 0.4689001142978668\n",
      "Epoch 4432, Loss: 1.1502335667610168, Final Batch Loss: 0.3610985577106476\n",
      "Epoch 4433, Loss: 1.2048567831516266, Final Batch Loss: 0.2395174205303192\n",
      "Epoch 4434, Loss: 1.3328685760498047, Final Batch Loss: 0.36494359374046326\n",
      "Epoch 4435, Loss: 1.3492003679275513, Final Batch Loss: 0.3255137503147125\n",
      "Epoch 4436, Loss: 1.1602208018302917, Final Batch Loss: 0.3259792923927307\n",
      "Epoch 4437, Loss: 1.2553462088108063, Final Batch Loss: 0.29400476813316345\n",
      "Epoch 4438, Loss: 1.4536741077899933, Final Batch Loss: 0.3678566813468933\n",
      "Epoch 4439, Loss: 1.2194712609052658, Final Batch Loss: 0.24800823628902435\n",
      "Epoch 4440, Loss: 1.2062024176120758, Final Batch Loss: 0.29174962639808655\n",
      "Epoch 4441, Loss: 1.3419179618358612, Final Batch Loss: 0.337334007024765\n",
      "Epoch 4442, Loss: 1.2556851506233215, Final Batch Loss: 0.27926358580589294\n",
      "Epoch 4443, Loss: 1.3891312777996063, Final Batch Loss: 0.41147661209106445\n",
      "Epoch 4444, Loss: 1.1943844258785248, Final Batch Loss: 0.2715027928352356\n",
      "Epoch 4445, Loss: 1.2647393643856049, Final Batch Loss: 0.33255723118782043\n",
      "Epoch 4446, Loss: 1.3535560071468353, Final Batch Loss: 0.33781933784484863\n",
      "Epoch 4447, Loss: 1.2414697259664536, Final Batch Loss: 0.29026275873184204\n",
      "Epoch 4448, Loss: 1.2393592298030853, Final Batch Loss: 0.3701585829257965\n",
      "Epoch 4449, Loss: 1.2203319668769836, Final Batch Loss: 0.2595338821411133\n",
      "Epoch 4450, Loss: 1.2023568749427795, Final Batch Loss: 0.29251766204833984\n",
      "Epoch 4451, Loss: 1.340473085641861, Final Batch Loss: 0.2926868796348572\n",
      "Epoch 4452, Loss: 1.1857775151729584, Final Batch Loss: 0.24626204371452332\n",
      "Epoch 4453, Loss: 1.2650671899318695, Final Batch Loss: 0.2947182059288025\n",
      "Epoch 4454, Loss: 1.212059497833252, Final Batch Loss: 0.25255027413368225\n",
      "Epoch 4455, Loss: 1.151374727487564, Final Batch Loss: 0.3282758295536041\n",
      "Epoch 4456, Loss: 1.2513937056064606, Final Batch Loss: 0.32240185141563416\n",
      "Epoch 4457, Loss: 1.2498748302459717, Final Batch Loss: 0.2906409204006195\n",
      "Epoch 4458, Loss: 1.325347751379013, Final Batch Loss: 0.3104651868343353\n",
      "Epoch 4459, Loss: 1.30817312002182, Final Batch Loss: 0.3644861876964569\n",
      "Epoch 4460, Loss: 1.3069118857383728, Final Batch Loss: 0.36249932646751404\n",
      "Epoch 4461, Loss: 1.2290770709514618, Final Batch Loss: 0.3407760262489319\n",
      "Epoch 4462, Loss: 1.1709053665399551, Final Batch Loss: 0.3013087213039398\n",
      "Epoch 4463, Loss: 1.089642733335495, Final Batch Loss: 0.2336753010749817\n",
      "Epoch 4464, Loss: 1.103937804698944, Final Batch Loss: 0.2382902204990387\n",
      "Epoch 4465, Loss: 1.285595878958702, Final Batch Loss: 0.3006461560726166\n",
      "Epoch 4466, Loss: 1.300405204296112, Final Batch Loss: 0.3990950286388397\n",
      "Epoch 4467, Loss: 1.1330923736095428, Final Batch Loss: 0.27420857548713684\n",
      "Epoch 4468, Loss: 1.3261528015136719, Final Batch Loss: 0.3354451656341553\n",
      "Epoch 4469, Loss: 1.2537090182304382, Final Batch Loss: 0.29559749364852905\n",
      "Epoch 4470, Loss: 1.3783854246139526, Final Batch Loss: 0.36041033267974854\n",
      "Epoch 4471, Loss: 1.300735056400299, Final Batch Loss: 0.3244989514350891\n",
      "Epoch 4472, Loss: 1.2890419661998749, Final Batch Loss: 0.3760553300380707\n",
      "Epoch 4473, Loss: 1.2174570262432098, Final Batch Loss: 0.3235071003437042\n",
      "Epoch 4474, Loss: 1.2463752925395966, Final Batch Loss: 0.2476469874382019\n",
      "Epoch 4475, Loss: 1.3380786776542664, Final Batch Loss: 0.40979015827178955\n",
      "Epoch 4476, Loss: 1.2710818648338318, Final Batch Loss: 0.35255494713783264\n",
      "Epoch 4477, Loss: 1.2256986647844315, Final Batch Loss: 0.2494479864835739\n",
      "Epoch 4478, Loss: 1.2643840610980988, Final Batch Loss: 0.29445943236351013\n",
      "Epoch 4479, Loss: 1.3469267189502716, Final Batch Loss: 0.4094444811344147\n",
      "Epoch 4480, Loss: 1.2216155230998993, Final Batch Loss: 0.28897953033447266\n",
      "Epoch 4481, Loss: 1.1170670241117477, Final Batch Loss: 0.24292801320552826\n",
      "Epoch 4482, Loss: 1.2621567249298096, Final Batch Loss: 0.3857683539390564\n",
      "Epoch 4483, Loss: 1.1518122851848602, Final Batch Loss: 0.28879454731941223\n",
      "Epoch 4484, Loss: 1.2006630599498749, Final Batch Loss: 0.2956274449825287\n",
      "Epoch 4485, Loss: 1.2589077353477478, Final Batch Loss: 0.3426116704940796\n",
      "Epoch 4486, Loss: 1.2326073795557022, Final Batch Loss: 0.2355344146490097\n",
      "Epoch 4487, Loss: 1.342744767665863, Final Batch Loss: 0.3267875015735626\n",
      "Epoch 4488, Loss: 1.4103463888168335, Final Batch Loss: 0.45227670669555664\n",
      "Epoch 4489, Loss: 1.4227020144462585, Final Batch Loss: 0.4256836473941803\n",
      "Epoch 4490, Loss: 1.1715440154075623, Final Batch Loss: 0.34307846426963806\n",
      "Epoch 4491, Loss: 1.1940051019191742, Final Batch Loss: 0.2722441256046295\n",
      "Epoch 4492, Loss: 1.202122688293457, Final Batch Loss: 0.2797897458076477\n",
      "Epoch 4493, Loss: 1.200767457485199, Final Batch Loss: 0.2574835419654846\n",
      "Epoch 4494, Loss: 1.2019772231578827, Final Batch Loss: 0.3460337817668915\n",
      "Epoch 4495, Loss: 1.2288574874401093, Final Batch Loss: 0.3562864661216736\n",
      "Epoch 4496, Loss: 1.426269143819809, Final Batch Loss: 0.3648971915245056\n",
      "Epoch 4497, Loss: 1.396845817565918, Final Batch Loss: 0.3208254277706146\n",
      "Epoch 4498, Loss: 1.3607520759105682, Final Batch Loss: 0.35678648948669434\n",
      "Epoch 4499, Loss: 1.338640958070755, Final Batch Loss: 0.3536081910133362\n",
      "Epoch 4500, Loss: 1.220777153968811, Final Batch Loss: 0.2998088598251343\n",
      "Epoch 4501, Loss: 1.1762208938598633, Final Batch Loss: 0.2144065499305725\n",
      "Epoch 4502, Loss: 1.2877701818943024, Final Batch Loss: 0.3549383580684662\n",
      "Epoch 4503, Loss: 1.2157215774059296, Final Batch Loss: 0.25418218970298767\n",
      "Epoch 4504, Loss: 1.2770805954933167, Final Batch Loss: 0.2765841484069824\n",
      "Epoch 4505, Loss: 1.1999960988759995, Final Batch Loss: 0.38042789697647095\n",
      "Epoch 4506, Loss: 1.119667887687683, Final Batch Loss: 0.26889321208000183\n",
      "Epoch 4507, Loss: 1.3385733664035797, Final Batch Loss: 0.45196035504341125\n",
      "Epoch 4508, Loss: 1.1658320724964142, Final Batch Loss: 0.2591283321380615\n",
      "Epoch 4509, Loss: 1.2187192738056183, Final Batch Loss: 0.3386840224266052\n",
      "Epoch 4510, Loss: 1.2013000845909119, Final Batch Loss: 0.3331901431083679\n",
      "Epoch 4511, Loss: 1.2437733709812164, Final Batch Loss: 0.2848864793777466\n",
      "Epoch 4512, Loss: 1.1745915412902832, Final Batch Loss: 0.24888688325881958\n",
      "Epoch 4513, Loss: 1.279768466949463, Final Batch Loss: 0.31004565954208374\n",
      "Epoch 4514, Loss: 1.2464786171913147, Final Batch Loss: 0.26711827516555786\n",
      "Epoch 4515, Loss: 1.328696757555008, Final Batch Loss: 0.28178924322128296\n",
      "Epoch 4516, Loss: 1.1501240730285645, Final Batch Loss: 0.3088393211364746\n",
      "Epoch 4517, Loss: 1.5030130743980408, Final Batch Loss: 0.3791477084159851\n",
      "Epoch 4518, Loss: 1.2437243163585663, Final Batch Loss: 0.30190911889076233\n",
      "Epoch 4519, Loss: 1.2097544074058533, Final Batch Loss: 0.2815275192260742\n",
      "Epoch 4520, Loss: 1.2496197819709778, Final Batch Loss: 0.37340569496154785\n",
      "Epoch 4521, Loss: 1.2930893898010254, Final Batch Loss: 0.32033854722976685\n",
      "Epoch 4522, Loss: 1.2913229763507843, Final Batch Loss: 0.2963812053203583\n",
      "Epoch 4523, Loss: 1.2716796100139618, Final Batch Loss: 0.33636823296546936\n",
      "Epoch 4524, Loss: 1.2692951261997223, Final Batch Loss: 0.4075329303741455\n",
      "Epoch 4525, Loss: 1.327526867389679, Final Batch Loss: 0.4508715867996216\n",
      "Epoch 4526, Loss: 1.2725014686584473, Final Batch Loss: 0.3115391433238983\n",
      "Epoch 4527, Loss: 1.2752924859523773, Final Batch Loss: 0.36978864669799805\n",
      "Epoch 4528, Loss: 1.2189461588859558, Final Batch Loss: 0.34561461210250854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4529, Loss: 1.2277870327234268, Final Batch Loss: 0.29693618416786194\n",
      "Epoch 4530, Loss: 1.2606996297836304, Final Batch Loss: 0.2418765127658844\n",
      "Epoch 4531, Loss: 1.1462725698947906, Final Batch Loss: 0.247208833694458\n",
      "Epoch 4532, Loss: 1.2078561186790466, Final Batch Loss: 0.29528650641441345\n",
      "Epoch 4533, Loss: 1.239321231842041, Final Batch Loss: 0.3424593508243561\n",
      "Epoch 4534, Loss: 1.1863926351070404, Final Batch Loss: 0.2781734764575958\n",
      "Epoch 4535, Loss: 1.204152524471283, Final Batch Loss: 0.2806399464607239\n",
      "Epoch 4536, Loss: 1.4624488651752472, Final Batch Loss: 0.5392325520515442\n",
      "Epoch 4537, Loss: 1.3211407363414764, Final Batch Loss: 0.376618355512619\n",
      "Epoch 4538, Loss: 1.1551049053668976, Final Batch Loss: 0.2975119948387146\n",
      "Epoch 4539, Loss: 1.2563871145248413, Final Batch Loss: 0.3120400011539459\n",
      "Epoch 4540, Loss: 1.1595192849636078, Final Batch Loss: 0.2876076102256775\n",
      "Epoch 4541, Loss: 1.2729679942131042, Final Batch Loss: 0.32834377884864807\n",
      "Epoch 4542, Loss: 1.2623302340507507, Final Batch Loss: 0.3351748585700989\n",
      "Epoch 4543, Loss: 1.2671226561069489, Final Batch Loss: 0.3051959276199341\n",
      "Epoch 4544, Loss: 1.3215815126895905, Final Batch Loss: 0.32528939843177795\n",
      "Epoch 4545, Loss: 1.2575122117996216, Final Batch Loss: 0.3185064494609833\n",
      "Epoch 4546, Loss: 1.135354220867157, Final Batch Loss: 0.2331710159778595\n",
      "Epoch 4547, Loss: 1.1480141431093216, Final Batch Loss: 0.24924542009830475\n",
      "Epoch 4548, Loss: 1.3058181703090668, Final Batch Loss: 0.2921053171157837\n",
      "Epoch 4549, Loss: 1.1974704265594482, Final Batch Loss: 0.27177029848098755\n",
      "Epoch 4550, Loss: 1.1982499063014984, Final Batch Loss: 0.2584987282752991\n",
      "Epoch 4551, Loss: 1.2018668353557587, Final Batch Loss: 0.36720046401023865\n",
      "Epoch 4552, Loss: 1.2904557287693024, Final Batch Loss: 0.33372223377227783\n",
      "Epoch 4553, Loss: 1.1455295979976654, Final Batch Loss: 0.29304349422454834\n",
      "Epoch 4554, Loss: 1.1963546574115753, Final Batch Loss: 0.32724279165267944\n",
      "Epoch 4555, Loss: 1.3447539806365967, Final Batch Loss: 0.347322016954422\n",
      "Epoch 4556, Loss: 1.1608329117298126, Final Batch Loss: 0.29171085357666016\n",
      "Epoch 4557, Loss: 1.1329596787691116, Final Batch Loss: 0.25380632281303406\n",
      "Epoch 4558, Loss: 1.242724895477295, Final Batch Loss: 0.30968883633613586\n",
      "Epoch 4559, Loss: 1.0926980823278427, Final Batch Loss: 0.27055394649505615\n",
      "Epoch 4560, Loss: 1.275363266468048, Final Batch Loss: 0.24315500259399414\n",
      "Epoch 4561, Loss: 1.266161471605301, Final Batch Loss: 0.33457621932029724\n",
      "Epoch 4562, Loss: 1.3305380940437317, Final Batch Loss: 0.3548751175403595\n",
      "Epoch 4563, Loss: 1.0758087635040283, Final Batch Loss: 0.23358868062496185\n",
      "Epoch 4564, Loss: 1.2411046922206879, Final Batch Loss: 0.2935549020767212\n",
      "Epoch 4565, Loss: 1.2315932661294937, Final Batch Loss: 0.33830884099006653\n",
      "Epoch 4566, Loss: 1.1955598592758179, Final Batch Loss: 0.26656606793403625\n",
      "Epoch 4567, Loss: 1.1624803841114044, Final Batch Loss: 0.33372682332992554\n",
      "Epoch 4568, Loss: 1.1398158371448517, Final Batch Loss: 0.32135143876075745\n",
      "Epoch 4569, Loss: 1.2224837243556976, Final Batch Loss: 0.30693161487579346\n",
      "Epoch 4570, Loss: 1.2911660075187683, Final Batch Loss: 0.307101845741272\n",
      "Epoch 4571, Loss: 1.1943316906690598, Final Batch Loss: 0.31379956007003784\n",
      "Epoch 4572, Loss: 1.5640136897563934, Final Batch Loss: 0.607682466506958\n",
      "Epoch 4573, Loss: 1.2377687394618988, Final Batch Loss: 0.3255581557750702\n",
      "Epoch 4574, Loss: 1.2250535488128662, Final Batch Loss: 0.23095844686031342\n",
      "Epoch 4575, Loss: 1.247047796845436, Final Batch Loss: 0.20848868787288666\n",
      "Epoch 4576, Loss: 1.2868880331516266, Final Batch Loss: 0.31185677647590637\n",
      "Epoch 4577, Loss: 1.1531014740467072, Final Batch Loss: 0.2440885305404663\n",
      "Epoch 4578, Loss: 1.4648690223693848, Final Batch Loss: 0.4394887089729309\n",
      "Epoch 4579, Loss: 1.2514788508415222, Final Batch Loss: 0.29095226526260376\n",
      "Epoch 4580, Loss: 1.4007125198841095, Final Batch Loss: 0.3621498942375183\n",
      "Epoch 4581, Loss: 1.3185953795909882, Final Batch Loss: 0.2689143717288971\n",
      "Epoch 4582, Loss: 1.4897038340568542, Final Batch Loss: 0.513181746006012\n",
      "Epoch 4583, Loss: 1.2803076207637787, Final Batch Loss: 0.3120969533920288\n",
      "Epoch 4584, Loss: 1.266911119222641, Final Batch Loss: 0.30486318469047546\n",
      "Epoch 4585, Loss: 1.2764174789190292, Final Batch Loss: 0.22764824330806732\n",
      "Epoch 4586, Loss: 1.409427970647812, Final Batch Loss: 0.3296981155872345\n",
      "Epoch 4587, Loss: 1.3088265359401703, Final Batch Loss: 0.297829270362854\n",
      "Epoch 4588, Loss: 1.4220493733882904, Final Batch Loss: 0.5636706352233887\n",
      "Epoch 4589, Loss: 1.4172277450561523, Final Batch Loss: 0.38272625207901\n",
      "Epoch 4590, Loss: 1.201411247253418, Final Batch Loss: 0.39200374484062195\n",
      "Epoch 4591, Loss: 1.0672872066497803, Final Batch Loss: 0.28899645805358887\n",
      "Epoch 4592, Loss: 1.2568388879299164, Final Batch Loss: 0.2576117515563965\n",
      "Epoch 4593, Loss: 1.3359557688236237, Final Batch Loss: 0.31041067838668823\n",
      "Epoch 4594, Loss: 1.1562636196613312, Final Batch Loss: 0.27629703283309937\n",
      "Epoch 4595, Loss: 1.2630914449691772, Final Batch Loss: 0.30840301513671875\n",
      "Epoch 4596, Loss: 1.2233326733112335, Final Batch Loss: 0.2687451243400574\n",
      "Epoch 4597, Loss: 1.2314585447311401, Final Batch Loss: 0.3077353239059448\n",
      "Epoch 4598, Loss: 1.281078279018402, Final Batch Loss: 0.29821348190307617\n",
      "Epoch 4599, Loss: 1.2697252184152603, Final Batch Loss: 0.24843932688236237\n",
      "Epoch 4600, Loss: 1.1957366466522217, Final Batch Loss: 0.3463086783885956\n",
      "Epoch 4601, Loss: 1.207713171839714, Final Batch Loss: 0.39791417121887207\n",
      "Epoch 4602, Loss: 1.1507409363985062, Final Batch Loss: 0.22649334371089935\n",
      "Epoch 4603, Loss: 1.2297070622444153, Final Batch Loss: 0.2665840983390808\n",
      "Epoch 4604, Loss: 1.1380965113639832, Final Batch Loss: 0.29669222235679626\n",
      "Epoch 4605, Loss: 1.3205911219120026, Final Batch Loss: 0.26339977979660034\n",
      "Epoch 4606, Loss: 1.1949228346347809, Final Batch Loss: 0.23432686924934387\n",
      "Epoch 4607, Loss: 1.318364679813385, Final Batch Loss: 0.32514557242393494\n",
      "Epoch 4608, Loss: 1.2770868688821793, Final Batch Loss: 0.24864421784877777\n",
      "Epoch 4609, Loss: 1.3607444167137146, Final Batch Loss: 0.3412931561470032\n",
      "Epoch 4610, Loss: 1.2934775650501251, Final Batch Loss: 0.27041998505592346\n",
      "Epoch 4611, Loss: 1.3523206114768982, Final Batch Loss: 0.43116968870162964\n",
      "Epoch 4612, Loss: 1.305254489183426, Final Batch Loss: 0.3319137990474701\n",
      "Epoch 4613, Loss: 1.2429568469524384, Final Batch Loss: 0.2924226224422455\n",
      "Epoch 4614, Loss: 1.3926151096820831, Final Batch Loss: 0.3282744288444519\n",
      "Epoch 4615, Loss: 1.3741028606891632, Final Batch Loss: 0.34153756499290466\n",
      "Epoch 4616, Loss: 1.1378691643476486, Final Batch Loss: 0.2045346051454544\n",
      "Epoch 4617, Loss: 1.124078169465065, Final Batch Loss: 0.26124104857444763\n",
      "Epoch 4618, Loss: 1.2907585799694061, Final Batch Loss: 0.37633952498435974\n",
      "Epoch 4619, Loss: 1.254822701215744, Final Batch Loss: 0.2297687828540802\n",
      "Epoch 4620, Loss: 1.3557140827178955, Final Batch Loss: 0.3609839975833893\n",
      "Epoch 4621, Loss: 1.182105392217636, Final Batch Loss: 0.25395211577415466\n",
      "Epoch 4622, Loss: 1.2366265654563904, Final Batch Loss: 0.2554796040058136\n",
      "Epoch 4623, Loss: 1.3766715228557587, Final Batch Loss: 0.3064999282360077\n",
      "Epoch 4624, Loss: 1.3444050550460815, Final Batch Loss: 0.3513852655887604\n",
      "Epoch 4625, Loss: 1.1182581782341003, Final Batch Loss: 0.22639381885528564\n",
      "Epoch 4626, Loss: 1.2877472937107086, Final Batch Loss: 0.3560100793838501\n",
      "Epoch 4627, Loss: 1.136909395456314, Final Batch Loss: 0.2686122953891754\n",
      "Epoch 4628, Loss: 1.3137344717979431, Final Batch Loss: 0.32573917508125305\n",
      "Epoch 4629, Loss: 1.4534188210964203, Final Batch Loss: 0.41309598088264465\n",
      "Epoch 4630, Loss: 1.283388614654541, Final Batch Loss: 0.344122976064682\n",
      "Epoch 4631, Loss: 1.2910624146461487, Final Batch Loss: 0.2865757644176483\n",
      "Epoch 4632, Loss: 1.1493339836597443, Final Batch Loss: 0.2674860954284668\n",
      "Epoch 4633, Loss: 1.1716878116130829, Final Batch Loss: 0.2696751356124878\n",
      "Epoch 4634, Loss: 1.255711853504181, Final Batch Loss: 0.2534123361110687\n",
      "Epoch 4635, Loss: 1.214664250612259, Final Batch Loss: 0.3067123591899872\n",
      "Epoch 4636, Loss: 1.2505941689014435, Final Batch Loss: 0.2575754225254059\n",
      "Epoch 4637, Loss: 1.1191506385803223, Final Batch Loss: 0.30864274501800537\n",
      "Epoch 4638, Loss: 1.3190949261188507, Final Batch Loss: 0.3020656704902649\n",
      "Epoch 4639, Loss: 1.2055246829986572, Final Batch Loss: 0.30017322301864624\n",
      "Epoch 4640, Loss: 1.2540186941623688, Final Batch Loss: 0.2781500220298767\n",
      "Epoch 4641, Loss: 1.1325517296791077, Final Batch Loss: 0.3056875765323639\n",
      "Epoch 4642, Loss: 1.161928802728653, Final Batch Loss: 0.35536089539527893\n",
      "Epoch 4643, Loss: 1.3715047538280487, Final Batch Loss: 0.3219775855541229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4644, Loss: 1.266038566827774, Final Batch Loss: 0.28568992018699646\n",
      "Epoch 4645, Loss: 1.2464888095855713, Final Batch Loss: 0.29927581548690796\n",
      "Epoch 4646, Loss: 1.2427315413951874, Final Batch Loss: 0.29653990268707275\n",
      "Epoch 4647, Loss: 1.1103334575891495, Final Batch Loss: 0.2240780144929886\n",
      "Epoch 4648, Loss: 1.3522989153862, Final Batch Loss: 0.3002619743347168\n",
      "Epoch 4649, Loss: 1.2502687275409698, Final Batch Loss: 0.3150210678577423\n",
      "Epoch 4650, Loss: 1.1966864168643951, Final Batch Loss: 0.2830294966697693\n",
      "Epoch 4651, Loss: 1.238889530301094, Final Batch Loss: 0.4088543951511383\n",
      "Epoch 4652, Loss: 1.2310752868652344, Final Batch Loss: 0.30336347222328186\n",
      "Epoch 4653, Loss: 1.3588300049304962, Final Batch Loss: 0.37964147329330444\n",
      "Epoch 4654, Loss: 1.1597389876842499, Final Batch Loss: 0.25501763820648193\n",
      "Epoch 4655, Loss: 1.0935272872447968, Final Batch Loss: 0.2457655370235443\n",
      "Epoch 4656, Loss: 1.2057030647993088, Final Batch Loss: 0.2805196940898895\n",
      "Epoch 4657, Loss: 1.185953438282013, Final Batch Loss: 0.3014450669288635\n",
      "Epoch 4658, Loss: 1.246551126241684, Final Batch Loss: 0.30884072184562683\n",
      "Epoch 4659, Loss: 1.2545146644115448, Final Batch Loss: 0.2900904417037964\n",
      "Epoch 4660, Loss: 1.324130654335022, Final Batch Loss: 0.37575817108154297\n",
      "Epoch 4661, Loss: 1.1556767523288727, Final Batch Loss: 0.29344573616981506\n",
      "Epoch 4662, Loss: 1.141536831855774, Final Batch Loss: 0.29275837540626526\n",
      "Epoch 4663, Loss: 1.222786784172058, Final Batch Loss: 0.38412055373191833\n",
      "Epoch 4664, Loss: 1.1906172931194305, Final Batch Loss: 0.30494818091392517\n",
      "Epoch 4665, Loss: 1.15036940574646, Final Batch Loss: 0.2969280481338501\n",
      "Epoch 4666, Loss: 1.238692730665207, Final Batch Loss: 0.27452489733695984\n",
      "Epoch 4667, Loss: 1.1201159954071045, Final Batch Loss: 0.2145225703716278\n",
      "Epoch 4668, Loss: 1.3238957822322845, Final Batch Loss: 0.37837710976600647\n",
      "Epoch 4669, Loss: 1.2340992093086243, Final Batch Loss: 0.2948973774909973\n",
      "Epoch 4670, Loss: 1.233790099620819, Final Batch Loss: 0.28663575649261475\n",
      "Epoch 4671, Loss: 1.3076447248458862, Final Batch Loss: 0.37585723400115967\n",
      "Epoch 4672, Loss: 1.1816191524267197, Final Batch Loss: 0.18609140813350677\n",
      "Epoch 4673, Loss: 1.2127010226249695, Final Batch Loss: 0.2943758964538574\n",
      "Epoch 4674, Loss: 1.1784727871418, Final Batch Loss: 0.27979183197021484\n",
      "Epoch 4675, Loss: 1.1463752090930939, Final Batch Loss: 0.27444037795066833\n",
      "Epoch 4676, Loss: 1.239827275276184, Final Batch Loss: 0.2949528098106384\n",
      "Epoch 4677, Loss: 1.2859933972358704, Final Batch Loss: 0.31057822704315186\n",
      "Epoch 4678, Loss: 1.0754756480455399, Final Batch Loss: 0.21117006242275238\n",
      "Epoch 4679, Loss: 1.212266981601715, Final Batch Loss: 0.2805362641811371\n",
      "Epoch 4680, Loss: 1.2073141932487488, Final Batch Loss: 0.2889394760131836\n",
      "Epoch 4681, Loss: 1.2236273288726807, Final Batch Loss: 0.32307374477386475\n",
      "Epoch 4682, Loss: 1.2132416665554047, Final Batch Loss: 0.30053946375846863\n",
      "Epoch 4683, Loss: 1.1769965887069702, Final Batch Loss: 0.3497595191001892\n",
      "Epoch 4684, Loss: 1.264385998249054, Final Batch Loss: 0.352009117603302\n",
      "Epoch 4685, Loss: 1.22817662358284, Final Batch Loss: 0.34701237082481384\n",
      "Epoch 4686, Loss: 1.2540450394153595, Final Batch Loss: 0.32886889576911926\n",
      "Epoch 4687, Loss: 1.3404614627361298, Final Batch Loss: 0.452620267868042\n",
      "Epoch 4688, Loss: 1.2332225441932678, Final Batch Loss: 0.3426395058631897\n",
      "Epoch 4689, Loss: 1.2086974084377289, Final Batch Loss: 0.3001907765865326\n",
      "Epoch 4690, Loss: 1.3481179028749466, Final Batch Loss: 0.373134970664978\n",
      "Epoch 4691, Loss: 1.1862130761146545, Final Batch Loss: 0.3453640937805176\n",
      "Epoch 4692, Loss: 1.0974289178848267, Final Batch Loss: 0.2561436593532562\n",
      "Epoch 4693, Loss: 1.292060226202011, Final Batch Loss: 0.30327099561691284\n",
      "Epoch 4694, Loss: 1.222065955400467, Final Batch Loss: 0.26470911502838135\n",
      "Epoch 4695, Loss: 1.1710819900035858, Final Batch Loss: 0.26375001668930054\n",
      "Epoch 4696, Loss: 1.3244878053665161, Final Batch Loss: 0.3603276312351227\n",
      "Epoch 4697, Loss: 1.2489122152328491, Final Batch Loss: 0.31531816720962524\n",
      "Epoch 4698, Loss: 1.2604953348636627, Final Batch Loss: 0.353630930185318\n",
      "Epoch 4699, Loss: 1.1201162934303284, Final Batch Loss: 0.27229636907577515\n",
      "Epoch 4700, Loss: 1.1805014312267303, Final Batch Loss: 0.27400660514831543\n",
      "Epoch 4701, Loss: 1.3341672718524933, Final Batch Loss: 0.3538662791252136\n",
      "Epoch 4702, Loss: 1.1748427748680115, Final Batch Loss: 0.310079962015152\n",
      "Epoch 4703, Loss: 1.2306528389453888, Final Batch Loss: 0.342044860124588\n",
      "Epoch 4704, Loss: 1.2677400708198547, Final Batch Loss: 0.2980799973011017\n",
      "Epoch 4705, Loss: 1.0763196498155594, Final Batch Loss: 0.21111276745796204\n",
      "Epoch 4706, Loss: 1.2952390015125275, Final Batch Loss: 0.42704927921295166\n",
      "Epoch 4707, Loss: 1.2797044515609741, Final Batch Loss: 0.29748180508613586\n",
      "Epoch 4708, Loss: 1.2178860902786255, Final Batch Loss: 0.2811602056026459\n",
      "Epoch 4709, Loss: 1.1092731654644012, Final Batch Loss: 0.26515793800354004\n",
      "Epoch 4710, Loss: 1.105995699763298, Final Batch Loss: 0.24895219504833221\n",
      "Epoch 4711, Loss: 1.1878162026405334, Final Batch Loss: 0.36621737480163574\n",
      "Epoch 4712, Loss: 1.261835128068924, Final Batch Loss: 0.31755366921424866\n",
      "Epoch 4713, Loss: 1.0757308900356293, Final Batch Loss: 0.2642553448677063\n",
      "Epoch 4714, Loss: 1.1279770731925964, Final Batch Loss: 0.2741625905036926\n",
      "Epoch 4715, Loss: 1.2075295448303223, Final Batch Loss: 0.27505508065223694\n",
      "Epoch 4716, Loss: 1.3235436081886292, Final Batch Loss: 0.35558265447616577\n",
      "Epoch 4717, Loss: 1.1496208310127258, Final Batch Loss: 0.28489142656326294\n",
      "Epoch 4718, Loss: 1.1831336915493011, Final Batch Loss: 0.34944218397140503\n",
      "Epoch 4719, Loss: 1.1060695052146912, Final Batch Loss: 0.22331038117408752\n",
      "Epoch 4720, Loss: 1.2033555954694748, Final Batch Loss: 0.25175634026527405\n",
      "Epoch 4721, Loss: 1.172897219657898, Final Batch Loss: 0.3238576352596283\n",
      "Epoch 4722, Loss: 1.2882322072982788, Final Batch Loss: 0.330532431602478\n",
      "Epoch 4723, Loss: 1.2914295941591263, Final Batch Loss: 0.24780313670635223\n",
      "Epoch 4724, Loss: 1.1914577782154083, Final Batch Loss: 0.31582510471343994\n",
      "Epoch 4725, Loss: 1.4404373466968536, Final Batch Loss: 0.4025575816631317\n",
      "Epoch 4726, Loss: 1.2537480592727661, Final Batch Loss: 0.3344559073448181\n",
      "Epoch 4727, Loss: 1.1927319318056107, Final Batch Loss: 0.3554333448410034\n",
      "Epoch 4728, Loss: 1.1144317090511322, Final Batch Loss: 0.292766809463501\n",
      "Epoch 4729, Loss: 1.3874202370643616, Final Batch Loss: 0.3396546542644501\n",
      "Epoch 4730, Loss: 1.4189855456352234, Final Batch Loss: 0.4364684224128723\n",
      "Epoch 4731, Loss: 1.2758500277996063, Final Batch Loss: 0.2827445864677429\n",
      "Epoch 4732, Loss: 1.2235360741615295, Final Batch Loss: 0.2905355989933014\n",
      "Epoch 4733, Loss: 1.1753530204296112, Final Batch Loss: 0.30496275424957275\n",
      "Epoch 4734, Loss: 1.1901287734508514, Final Batch Loss: 0.2789076268672943\n",
      "Epoch 4735, Loss: 1.3556101620197296, Final Batch Loss: 0.38024622201919556\n",
      "Epoch 4736, Loss: 1.3286609053611755, Final Batch Loss: 0.3086746633052826\n",
      "Epoch 4737, Loss: 1.3366338610649109, Final Batch Loss: 0.422818660736084\n",
      "Epoch 4738, Loss: 1.3154213726520538, Final Batch Loss: 0.35024493932724\n",
      "Epoch 4739, Loss: 1.231077879667282, Final Batch Loss: 0.26551371812820435\n",
      "Epoch 4740, Loss: 1.2740070521831512, Final Batch Loss: 0.2945595979690552\n",
      "Epoch 4741, Loss: 1.3447307646274567, Final Batch Loss: 0.4473003149032593\n",
      "Epoch 4742, Loss: 1.1641774773597717, Final Batch Loss: 0.3520844578742981\n",
      "Epoch 4743, Loss: 1.3295812010765076, Final Batch Loss: 0.3525654375553131\n",
      "Epoch 4744, Loss: 1.2549515068531036, Final Batch Loss: 0.3388729989528656\n",
      "Epoch 4745, Loss: 1.262725293636322, Final Batch Loss: 0.32584327459335327\n",
      "Epoch 4746, Loss: 1.100709706544876, Final Batch Loss: 0.20238706469535828\n",
      "Epoch 4747, Loss: 1.1915536522865295, Final Batch Loss: 0.3494202196598053\n",
      "Epoch 4748, Loss: 1.2706871330738068, Final Batch Loss: 0.41139376163482666\n",
      "Epoch 4749, Loss: 1.2963168621063232, Final Batch Loss: 0.34946224093437195\n",
      "Epoch 4750, Loss: 1.1134120672941208, Final Batch Loss: 0.2459590584039688\n",
      "Epoch 4751, Loss: 1.115330621600151, Final Batch Loss: 0.23051081597805023\n",
      "Epoch 4752, Loss: 1.243222177028656, Final Batch Loss: 0.279231458902359\n",
      "Epoch 4753, Loss: 1.2777517437934875, Final Batch Loss: 0.323586642742157\n",
      "Epoch 4754, Loss: 1.2063883543014526, Final Batch Loss: 0.25407564640045166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4755, Loss: 1.29717355966568, Final Batch Loss: 0.35458213090896606\n",
      "Epoch 4756, Loss: 1.0327169746160507, Final Batch Loss: 0.279802143573761\n",
      "Epoch 4757, Loss: 1.2520713359117508, Final Batch Loss: 0.36272716522216797\n",
      "Epoch 4758, Loss: 1.2827832996845245, Final Batch Loss: 0.315185010433197\n",
      "Epoch 4759, Loss: 1.172174945473671, Final Batch Loss: 0.24855704605579376\n",
      "Epoch 4760, Loss: 1.2639807611703873, Final Batch Loss: 0.2442997246980667\n",
      "Epoch 4761, Loss: 1.2475461959838867, Final Batch Loss: 0.36794817447662354\n",
      "Epoch 4762, Loss: 1.1879960596561432, Final Batch Loss: 0.27480649948120117\n",
      "Epoch 4763, Loss: 1.12965689599514, Final Batch Loss: 0.2888336479663849\n",
      "Epoch 4764, Loss: 1.314573496580124, Final Batch Loss: 0.4956381916999817\n",
      "Epoch 4765, Loss: 1.0655092597007751, Final Batch Loss: 0.26162466406822205\n",
      "Epoch 4766, Loss: 1.1204862594604492, Final Batch Loss: 0.2595764994621277\n",
      "Epoch 4767, Loss: 1.301710605621338, Final Batch Loss: 0.29747575521469116\n",
      "Epoch 4768, Loss: 1.475906491279602, Final Batch Loss: 0.5080426335334778\n",
      "Epoch 4769, Loss: 1.25667405128479, Final Batch Loss: 0.2602670192718506\n",
      "Epoch 4770, Loss: 1.167248710989952, Final Batch Loss: 0.354971319437027\n",
      "Epoch 4771, Loss: 1.1513705551624298, Final Batch Loss: 0.3066202998161316\n",
      "Epoch 4772, Loss: 1.3085704445838928, Final Batch Loss: 0.4121565520763397\n",
      "Epoch 4773, Loss: 1.1214418411254883, Final Batch Loss: 0.2555314600467682\n",
      "Epoch 4774, Loss: 1.113499477505684, Final Batch Loss: 0.2428334504365921\n",
      "Epoch 4775, Loss: 1.2904073894023895, Final Batch Loss: 0.27746522426605225\n",
      "Epoch 4776, Loss: 1.1506404727697372, Final Batch Loss: 0.34018927812576294\n",
      "Epoch 4777, Loss: 1.190295621752739, Final Batch Loss: 0.23705990612506866\n",
      "Epoch 4778, Loss: 1.325939029455185, Final Batch Loss: 0.3482075035572052\n",
      "Epoch 4779, Loss: 1.194108486175537, Final Batch Loss: 0.33879977464675903\n",
      "Epoch 4780, Loss: 1.363719403743744, Final Batch Loss: 0.29791221022605896\n",
      "Epoch 4781, Loss: 1.3280726671218872, Final Batch Loss: 0.2809276282787323\n",
      "Epoch 4782, Loss: 1.3780739903450012, Final Batch Loss: 0.32284802198410034\n",
      "Epoch 4783, Loss: 1.313146561384201, Final Batch Loss: 0.351071298122406\n",
      "Epoch 4784, Loss: 1.2362582683563232, Final Batch Loss: 0.26886650919914246\n",
      "Epoch 4785, Loss: 1.3255141973495483, Final Batch Loss: 0.3144749104976654\n",
      "Epoch 4786, Loss: 1.3676929771900177, Final Batch Loss: 0.31343215703964233\n",
      "Epoch 4787, Loss: 1.2252332419157028, Final Batch Loss: 0.2432730346918106\n",
      "Epoch 4788, Loss: 1.2553468942642212, Final Batch Loss: 0.2696292996406555\n",
      "Epoch 4789, Loss: 1.3115619719028473, Final Batch Loss: 0.38305261731147766\n",
      "Epoch 4790, Loss: 1.0518036037683487, Final Batch Loss: 0.2512182593345642\n",
      "Epoch 4791, Loss: 1.22349414229393, Final Batch Loss: 0.36332887411117554\n",
      "Epoch 4792, Loss: 1.2853887230157852, Final Batch Loss: 0.37374573945999146\n",
      "Epoch 4793, Loss: 1.3523333072662354, Final Batch Loss: 0.3626338839530945\n",
      "Epoch 4794, Loss: 1.2035794258117676, Final Batch Loss: 0.33232736587524414\n",
      "Epoch 4795, Loss: 1.2934077382087708, Final Batch Loss: 0.361803263425827\n",
      "Epoch 4796, Loss: 1.4011754393577576, Final Batch Loss: 0.4425288438796997\n",
      "Epoch 4797, Loss: 1.2305123805999756, Final Batch Loss: 0.30649664998054504\n",
      "Epoch 4798, Loss: 1.070144608616829, Final Batch Loss: 0.23116903007030487\n",
      "Epoch 4799, Loss: 1.1460921317338943, Final Batch Loss: 0.24232037365436554\n",
      "Epoch 4800, Loss: 1.1610176861286163, Final Batch Loss: 0.21560972929000854\n",
      "Epoch 4801, Loss: 1.184172511100769, Final Batch Loss: 0.2851718068122864\n",
      "Epoch 4802, Loss: 1.1081799864768982, Final Batch Loss: 0.29303285479545593\n",
      "Epoch 4803, Loss: 1.2558960765600204, Final Batch Loss: 0.3043918013572693\n",
      "Epoch 4804, Loss: 1.3871963024139404, Final Batch Loss: 0.2665027379989624\n",
      "Epoch 4805, Loss: 1.1672957241535187, Final Batch Loss: 0.31214639544487\n",
      "Epoch 4806, Loss: 1.1912855505943298, Final Batch Loss: 0.3814581632614136\n",
      "Epoch 4807, Loss: 1.3807876408100128, Final Batch Loss: 0.34974148869514465\n",
      "Epoch 4808, Loss: 1.261259213089943, Final Batch Loss: 0.31503748893737793\n",
      "Epoch 4809, Loss: 1.2594760060310364, Final Batch Loss: 0.34503716230392456\n",
      "Epoch 4810, Loss: 1.2320328056812286, Final Batch Loss: 0.3703572154045105\n",
      "Epoch 4811, Loss: 1.2596482932567596, Final Batch Loss: 0.2154361605644226\n",
      "Epoch 4812, Loss: 1.325998604297638, Final Batch Loss: 0.33635321259498596\n",
      "Epoch 4813, Loss: 1.2964096367359161, Final Batch Loss: 0.4080103933811188\n",
      "Epoch 4814, Loss: 1.4132458567619324, Final Batch Loss: 0.35794857144355774\n",
      "Epoch 4815, Loss: 1.3383950889110565, Final Batch Loss: 0.47213736176490784\n",
      "Epoch 4816, Loss: 1.2193019688129425, Final Batch Loss: 0.25733479857444763\n",
      "Epoch 4817, Loss: 1.2483907639980316, Final Batch Loss: 0.32003509998321533\n",
      "Epoch 4818, Loss: 1.4388971030712128, Final Batch Loss: 0.40988871455192566\n",
      "Epoch 4819, Loss: 1.3472923040390015, Final Batch Loss: 0.3859257400035858\n",
      "Epoch 4820, Loss: 1.2252853512763977, Final Batch Loss: 0.20724651217460632\n",
      "Epoch 4821, Loss: 1.3254002034664154, Final Batch Loss: 0.41838714480400085\n",
      "Epoch 4822, Loss: 1.3247292637825012, Final Batch Loss: 0.41441503167152405\n",
      "Epoch 4823, Loss: 1.2635304033756256, Final Batch Loss: 0.32564523816108704\n",
      "Epoch 4824, Loss: 1.2878452837467194, Final Batch Loss: 0.23427313566207886\n",
      "Epoch 4825, Loss: 1.0775391310453415, Final Batch Loss: 0.24162925779819489\n",
      "Epoch 4826, Loss: 1.2134305238723755, Final Batch Loss: 0.26405444741249084\n",
      "Epoch 4827, Loss: 1.1485880017280579, Final Batch Loss: 0.26717641949653625\n",
      "Epoch 4828, Loss: 1.158881038427353, Final Batch Loss: 0.28984400629997253\n",
      "Epoch 4829, Loss: 1.1571561694145203, Final Batch Loss: 0.2451251745223999\n",
      "Epoch 4830, Loss: 1.1811927258968353, Final Batch Loss: 0.280461847782135\n",
      "Epoch 4831, Loss: 1.1755008101463318, Final Batch Loss: 0.3002915382385254\n",
      "Epoch 4832, Loss: 1.2203599959611893, Final Batch Loss: 0.35075700283050537\n",
      "Epoch 4833, Loss: 1.1694766581058502, Final Batch Loss: 0.28725603222846985\n",
      "Epoch 4834, Loss: 1.1762906163930893, Final Batch Loss: 0.2692312002182007\n",
      "Epoch 4835, Loss: 1.20286226272583, Final Batch Loss: 0.2568221092224121\n",
      "Epoch 4836, Loss: 1.17135488986969, Final Batch Loss: 0.284191370010376\n",
      "Epoch 4837, Loss: 1.2263882607221603, Final Batch Loss: 0.29008233547210693\n",
      "Epoch 4838, Loss: 1.208740085363388, Final Batch Loss: 0.32211923599243164\n",
      "Epoch 4839, Loss: 1.1399908810853958, Final Batch Loss: 0.22763364017009735\n",
      "Epoch 4840, Loss: 1.2486567795276642, Final Batch Loss: 0.30590587854385376\n",
      "Epoch 4841, Loss: 1.1912629008293152, Final Batch Loss: 0.3845014274120331\n",
      "Epoch 4842, Loss: 1.2486615478992462, Final Batch Loss: 0.33335384726524353\n",
      "Epoch 4843, Loss: 1.2579192519187927, Final Batch Loss: 0.29791221022605896\n",
      "Epoch 4844, Loss: 1.2652270793914795, Final Batch Loss: 0.35950496792793274\n",
      "Epoch 4845, Loss: 1.222750037908554, Final Batch Loss: 0.2760998606681824\n",
      "Epoch 4846, Loss: 1.0845547765493393, Final Batch Loss: 0.2390412539243698\n",
      "Epoch 4847, Loss: 1.1527512073516846, Final Batch Loss: 0.2616450786590576\n",
      "Epoch 4848, Loss: 1.2436420619487762, Final Batch Loss: 0.30796122550964355\n",
      "Epoch 4849, Loss: 1.260480672121048, Final Batch Loss: 0.3019927442073822\n",
      "Epoch 4850, Loss: 1.2397333234548569, Final Batch Loss: 0.27867648005485535\n",
      "Epoch 4851, Loss: 1.2248830497264862, Final Batch Loss: 0.27447158098220825\n",
      "Epoch 4852, Loss: 1.3231031894683838, Final Batch Loss: 0.46719175577163696\n",
      "Epoch 4853, Loss: 1.127852737903595, Final Batch Loss: 0.2845258116722107\n",
      "Epoch 4854, Loss: 1.2439343333244324, Final Batch Loss: 0.3228341341018677\n",
      "Epoch 4855, Loss: 1.1955943703651428, Final Batch Loss: 0.38178908824920654\n",
      "Epoch 4856, Loss: 1.277675986289978, Final Batch Loss: 0.33552998304367065\n",
      "Epoch 4857, Loss: 1.3108734488487244, Final Batch Loss: 0.3078545928001404\n",
      "Epoch 4858, Loss: 1.265376627445221, Final Batch Loss: 0.3675363063812256\n",
      "Epoch 4859, Loss: 1.2431855201721191, Final Batch Loss: 0.24809131026268005\n",
      "Epoch 4860, Loss: 1.2663670480251312, Final Batch Loss: 0.31359240412712097\n",
      "Epoch 4861, Loss: 1.4288093745708466, Final Batch Loss: 0.4136519432067871\n",
      "Epoch 4862, Loss: 1.2186979353427887, Final Batch Loss: 0.28252577781677246\n",
      "Epoch 4863, Loss: 1.1630694568157196, Final Batch Loss: 0.3326250910758972\n",
      "Epoch 4864, Loss: 1.160118579864502, Final Batch Loss: 0.24210581183433533\n",
      "Epoch 4865, Loss: 1.19231216609478, Final Batch Loss: 0.3624390959739685\n",
      "Epoch 4866, Loss: 1.3174476772546768, Final Batch Loss: 0.38642755150794983\n",
      "Epoch 4867, Loss: 1.1149658113718033, Final Batch Loss: 0.24727876484394073\n",
      "Epoch 4868, Loss: 1.069169282913208, Final Batch Loss: 0.2847890853881836\n",
      "Epoch 4869, Loss: 1.2130390107631683, Final Batch Loss: 0.21949175000190735\n",
      "Epoch 4870, Loss: 1.1697214841842651, Final Batch Loss: 0.3632209599018097\n",
      "Epoch 4871, Loss: 1.1071186810731888, Final Batch Loss: 0.3206908702850342\n",
      "Epoch 4872, Loss: 1.1449335515499115, Final Batch Loss: 0.3135391175746918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4873, Loss: 1.216128408908844, Final Batch Loss: 0.28981903195381165\n",
      "Epoch 4874, Loss: 1.4278627038002014, Final Batch Loss: 0.3800791800022125\n",
      "Epoch 4875, Loss: 1.5507558286190033, Final Batch Loss: 0.5120417475700378\n",
      "Epoch 4876, Loss: 1.2342262268066406, Final Batch Loss: 0.3228306174278259\n",
      "Epoch 4877, Loss: 1.2323959171772003, Final Batch Loss: 0.299152135848999\n",
      "Epoch 4878, Loss: 1.3741221725940704, Final Batch Loss: 0.3378228545188904\n",
      "Epoch 4879, Loss: 1.3215179145336151, Final Batch Loss: 0.35339289903640747\n",
      "Epoch 4880, Loss: 1.2378810942173004, Final Batch Loss: 0.30015912652015686\n",
      "Epoch 4881, Loss: 1.3328085839748383, Final Batch Loss: 0.29239514470100403\n",
      "Epoch 4882, Loss: 1.2782068848609924, Final Batch Loss: 0.326535165309906\n",
      "Epoch 4883, Loss: 1.2263135612010956, Final Batch Loss: 0.3144080638885498\n",
      "Epoch 4884, Loss: 1.1864383220672607, Final Batch Loss: 0.29833173751831055\n",
      "Epoch 4885, Loss: 1.0721582174301147, Final Batch Loss: 0.23937097191810608\n",
      "Epoch 4886, Loss: 1.2609874159097672, Final Batch Loss: 0.32455185055732727\n",
      "Epoch 4887, Loss: 1.0954809188842773, Final Batch Loss: 0.28672435879707336\n",
      "Epoch 4888, Loss: 1.3172034919261932, Final Batch Loss: 0.40044328570365906\n",
      "Epoch 4889, Loss: 1.2309714555740356, Final Batch Loss: 0.30041271448135376\n",
      "Epoch 4890, Loss: 1.2151789367198944, Final Batch Loss: 0.2652498483657837\n",
      "Epoch 4891, Loss: 1.1804025024175644, Final Batch Loss: 0.34458127617836\n",
      "Epoch 4892, Loss: 1.3121539652347565, Final Batch Loss: 0.23374658823013306\n",
      "Epoch 4893, Loss: 1.2562129199504852, Final Batch Loss: 0.27043429017066956\n",
      "Epoch 4894, Loss: 1.106775388121605, Final Batch Loss: 0.2415795773267746\n",
      "Epoch 4895, Loss: 1.2001900970935822, Final Batch Loss: 0.20732790231704712\n",
      "Epoch 4896, Loss: 1.1947941780090332, Final Batch Loss: 0.27034157514572144\n",
      "Epoch 4897, Loss: 1.3333641290664673, Final Batch Loss: 0.37880975008010864\n",
      "Epoch 4898, Loss: 1.3137581050395966, Final Batch Loss: 0.35406550765037537\n",
      "Epoch 4899, Loss: 1.313173994421959, Final Batch Loss: 0.5062135457992554\n",
      "Epoch 4900, Loss: 1.2305790781974792, Final Batch Loss: 0.2897578179836273\n",
      "Epoch 4901, Loss: 1.1604523062705994, Final Batch Loss: 0.2956383526325226\n",
      "Epoch 4902, Loss: 1.233739361166954, Final Batch Loss: 0.23358403146266937\n",
      "Epoch 4903, Loss: 1.2519421428442001, Final Batch Loss: 0.2182302623987198\n",
      "Epoch 4904, Loss: 1.285201519727707, Final Batch Loss: 0.28481313586235046\n",
      "Epoch 4905, Loss: 1.23316690325737, Final Batch Loss: 0.32950425148010254\n",
      "Epoch 4906, Loss: 1.2290509641170502, Final Batch Loss: 0.42433103919029236\n",
      "Epoch 4907, Loss: 1.2836531102657318, Final Batch Loss: 0.2630450427532196\n",
      "Epoch 4908, Loss: 1.167518675327301, Final Batch Loss: 0.2900795340538025\n",
      "Epoch 4909, Loss: 1.2779780328273773, Final Batch Loss: 0.3622366487979889\n",
      "Epoch 4910, Loss: 1.2002398371696472, Final Batch Loss: 0.33021265268325806\n",
      "Epoch 4911, Loss: 1.1779614686965942, Final Batch Loss: 0.2656441926956177\n",
      "Epoch 4912, Loss: 1.2183917313814163, Final Batch Loss: 0.3440869152545929\n",
      "Epoch 4913, Loss: 1.1617951095104218, Final Batch Loss: 0.30903273820877075\n",
      "Epoch 4914, Loss: 1.2517200112342834, Final Batch Loss: 0.3318644165992737\n",
      "Epoch 4915, Loss: 1.2216403186321259, Final Batch Loss: 0.3738342523574829\n",
      "Epoch 4916, Loss: 1.3162580132484436, Final Batch Loss: 0.35901686549186707\n",
      "Epoch 4917, Loss: 1.2675116658210754, Final Batch Loss: 0.33475634455680847\n",
      "Epoch 4918, Loss: 1.248153418302536, Final Batch Loss: 0.2630528211593628\n",
      "Epoch 4919, Loss: 1.2075430750846863, Final Batch Loss: 0.33592090010643005\n",
      "Epoch 4920, Loss: 1.1305465400218964, Final Batch Loss: 0.2693115472793579\n",
      "Epoch 4921, Loss: 1.134891614317894, Final Batch Loss: 0.21695105731487274\n",
      "Epoch 4922, Loss: 1.275184839963913, Final Batch Loss: 0.2523200511932373\n",
      "Epoch 4923, Loss: 1.1587718427181244, Final Batch Loss: 0.26147228479385376\n",
      "Epoch 4924, Loss: 1.17894446849823, Final Batch Loss: 0.303231805562973\n",
      "Epoch 4925, Loss: 1.1355247348546982, Final Batch Loss: 0.25100529193878174\n",
      "Epoch 4926, Loss: 1.3303925842046738, Final Batch Loss: 0.4482719302177429\n",
      "Epoch 4927, Loss: 1.2193896770477295, Final Batch Loss: 0.314330130815506\n",
      "Epoch 4928, Loss: 1.225674331188202, Final Batch Loss: 0.33315297961235046\n",
      "Epoch 4929, Loss: 1.1092843115329742, Final Batch Loss: 0.29313114285469055\n",
      "Epoch 4930, Loss: 1.1323491036891937, Final Batch Loss: 0.22267192602157593\n",
      "Epoch 4931, Loss: 1.174587070941925, Final Batch Loss: 0.2933202087879181\n",
      "Epoch 4932, Loss: 1.2889231443405151, Final Batch Loss: 0.3090394139289856\n",
      "Epoch 4933, Loss: 1.2187792658805847, Final Batch Loss: 0.31421977281570435\n",
      "Epoch 4934, Loss: 1.2947949171066284, Final Batch Loss: 0.306950181722641\n",
      "Epoch 4935, Loss: 1.2215812504291534, Final Batch Loss: 0.2513923943042755\n",
      "Epoch 4936, Loss: 1.1289546489715576, Final Batch Loss: 0.2683279514312744\n",
      "Epoch 4937, Loss: 1.1796223521232605, Final Batch Loss: 0.28324851393699646\n",
      "Epoch 4938, Loss: 1.1924648582935333, Final Batch Loss: 0.26798659563064575\n",
      "Epoch 4939, Loss: 1.2588230073451996, Final Batch Loss: 0.3209385871887207\n",
      "Epoch 4940, Loss: 1.19505375623703, Final Batch Loss: 0.28392505645751953\n",
      "Epoch 4941, Loss: 1.2653329819440842, Final Batch Loss: 0.3593516945838928\n",
      "Epoch 4942, Loss: 1.3424021005630493, Final Batch Loss: 0.4685094654560089\n",
      "Epoch 4943, Loss: 1.260433167219162, Final Batch Loss: 0.29408636689186096\n",
      "Epoch 4944, Loss: 1.1772340536117554, Final Batch Loss: 0.29965129494667053\n",
      "Epoch 4945, Loss: 1.3160447031259537, Final Batch Loss: 0.2372569739818573\n",
      "Epoch 4946, Loss: 1.1103747338056564, Final Batch Loss: 0.23001451790332794\n",
      "Epoch 4947, Loss: 1.117551326751709, Final Batch Loss: 0.24372947216033936\n",
      "Epoch 4948, Loss: 1.1985137462615967, Final Batch Loss: 0.3975171446800232\n",
      "Epoch 4949, Loss: 1.1350619792938232, Final Batch Loss: 0.2069060504436493\n",
      "Epoch 4950, Loss: 1.2347918152809143, Final Batch Loss: 0.28569093346595764\n",
      "Epoch 4951, Loss: 1.2160145044326782, Final Batch Loss: 0.29404109716415405\n",
      "Epoch 4952, Loss: 1.1942457854747772, Final Batch Loss: 0.34679967164993286\n",
      "Epoch 4953, Loss: 1.1062403619289398, Final Batch Loss: 0.21125182509422302\n",
      "Epoch 4954, Loss: 1.1906468272209167, Final Batch Loss: 0.3101966679096222\n",
      "Epoch 4955, Loss: 1.1993806958198547, Final Batch Loss: 0.2721375823020935\n",
      "Epoch 4956, Loss: 1.1878140270709991, Final Batch Loss: 0.3147455155849457\n",
      "Epoch 4957, Loss: 1.2661328613758087, Final Batch Loss: 0.32285675406455994\n",
      "Epoch 4958, Loss: 1.181691974401474, Final Batch Loss: 0.3801209032535553\n",
      "Epoch 4959, Loss: 1.1084711104631424, Final Batch Loss: 0.2676922380924225\n",
      "Epoch 4960, Loss: 1.1378756761550903, Final Batch Loss: 0.2774284780025482\n",
      "Epoch 4961, Loss: 1.2175967991352081, Final Batch Loss: 0.3058047890663147\n",
      "Epoch 4962, Loss: 1.1301908940076828, Final Batch Loss: 0.16246472299098969\n",
      "Epoch 4963, Loss: 1.2483151853084564, Final Batch Loss: 0.3452354073524475\n",
      "Epoch 4964, Loss: 1.1399498879909515, Final Batch Loss: 0.3016156852245331\n",
      "Epoch 4965, Loss: 1.197828859090805, Final Batch Loss: 0.296423077583313\n",
      "Epoch 4966, Loss: 1.2272838056087494, Final Batch Loss: 0.2978038191795349\n",
      "Epoch 4967, Loss: 1.0614463239908218, Final Batch Loss: 0.24010159075260162\n",
      "Epoch 4968, Loss: 1.2100197672843933, Final Batch Loss: 0.37771812081336975\n",
      "Epoch 4969, Loss: 1.2662189602851868, Final Batch Loss: 0.31931358575820923\n",
      "Epoch 4970, Loss: 1.2102602422237396, Final Batch Loss: 0.26835373044013977\n",
      "Epoch 4971, Loss: 1.302510291337967, Final Batch Loss: 0.349716454744339\n",
      "Epoch 4972, Loss: 1.2154491245746613, Final Batch Loss: 0.27063727378845215\n",
      "Epoch 4973, Loss: 1.2166536152362823, Final Batch Loss: 0.2747345268726349\n",
      "Epoch 4974, Loss: 1.199239194393158, Final Batch Loss: 0.3405402898788452\n",
      "Epoch 4975, Loss: 1.2909201979637146, Final Batch Loss: 0.434469997882843\n",
      "Epoch 4976, Loss: 1.2148783504962921, Final Batch Loss: 0.387751966714859\n",
      "Epoch 4977, Loss: 1.187509149312973, Final Batch Loss: 0.2883453965187073\n",
      "Epoch 4978, Loss: 1.3412690460681915, Final Batch Loss: 0.3427584767341614\n",
      "Epoch 4979, Loss: 1.2611551135778427, Final Batch Loss: 0.38570958375930786\n",
      "Epoch 4980, Loss: 1.2206739783287048, Final Batch Loss: 0.33118098974227905\n",
      "Epoch 4981, Loss: 1.239480435848236, Final Batch Loss: 0.32964879274368286\n",
      "Epoch 4982, Loss: 1.3377527296543121, Final Batch Loss: 0.28716081380844116\n",
      "Epoch 4983, Loss: 1.3397222459316254, Final Batch Loss: 0.3456205129623413\n",
      "Epoch 4984, Loss: 1.1433578729629517, Final Batch Loss: 0.24281010031700134\n",
      "Epoch 4985, Loss: 1.2767326533794403, Final Batch Loss: 0.26112616062164307\n",
      "Epoch 4986, Loss: 1.2097895443439484, Final Batch Loss: 0.3611556589603424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4987, Loss: 1.197618007659912, Final Batch Loss: 0.29686328768730164\n",
      "Epoch 4988, Loss: 1.2044507265090942, Final Batch Loss: 0.3033429980278015\n",
      "Epoch 4989, Loss: 1.1407773047685623, Final Batch Loss: 0.24895624816417694\n",
      "Epoch 4990, Loss: 1.2543337941169739, Final Batch Loss: 0.2688353955745697\n",
      "Epoch 4991, Loss: 1.3983927369117737, Final Batch Loss: 0.5414756536483765\n",
      "Epoch 4992, Loss: 1.1965278685092926, Final Batch Loss: 0.25854167342185974\n",
      "Epoch 4993, Loss: 1.3044307678937912, Final Batch Loss: 0.34919828176498413\n",
      "Epoch 4994, Loss: 1.2885797023773193, Final Batch Loss: 0.3830014765262604\n",
      "Epoch 4995, Loss: 1.278082251548767, Final Batch Loss: 0.29426494240760803\n",
      "Epoch 4996, Loss: 1.206554338335991, Final Batch Loss: 0.32202768325805664\n",
      "Epoch 4997, Loss: 1.1780408322811127, Final Batch Loss: 0.2429296374320984\n",
      "Epoch 4998, Loss: 1.2745114862918854, Final Batch Loss: 0.3301940858364105\n",
      "Epoch 4999, Loss: 1.1091800928115845, Final Batch Loss: 0.25905612111091614\n",
      "Epoch 5000, Loss: 1.2331387549638748, Final Batch Loss: 0.3301750421524048\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model_subject(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[49  0  0  0  6  0  0]\n",
      " [ 0 25  0  0  0  1  0]\n",
      " [ 0  1 23  0  0  0  0]\n",
      " [ 0  1  1 29  0  0  0]\n",
      " [ 2  2  0  0 21  0  1]\n",
      " [ 1  0  0  0  0 25  0]\n",
      " [ 0  5  0  2  0  0 25]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.94231   0.89091   0.91589        55\n",
      "           1    0.73529   0.96154   0.83333        26\n",
      "           2    0.95833   0.95833   0.95833        24\n",
      "           3    0.93548   0.93548   0.93548        31\n",
      "           4    0.77778   0.80769   0.79245        26\n",
      "           5    0.96154   0.96154   0.96154        26\n",
      "           6    0.96154   0.78125   0.86207        32\n",
      "\n",
      "    accuracy                        0.89545       220\n",
      "   macro avg    0.89604   0.89954   0.89416       220\n",
      "weighted avg    0.90425   0.89545   0.89650       220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model_subject.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model_subject(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  0  0  0  0  0  0]\n",
      " [ 0 17  9  0  0  0  0]\n",
      " [ 0  0  0 16 14  0  0]\n",
      " [ 0  0  1 44  0  0  0]\n",
      " [ 0  2  0  0 33  0  0]\n",
      " [ 0  0  0  0  0 33  0]\n",
      " [ 0  0  0  0  0  0 32]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        19\n",
      "           1    0.89474   0.65385   0.75556        26\n",
      "           2    0.00000   0.00000   0.00000        30\n",
      "           3    0.73333   0.97778   0.83810        45\n",
      "           4    0.70213   0.94286   0.80488        35\n",
      "           5    1.00000   1.00000   1.00000        33\n",
      "           6    1.00000   1.00000   1.00000        32\n",
      "\n",
      "    accuracy                        0.80909       220\n",
      "   macro avg    0.76146   0.79635   0.77122       220\n",
      "weighted avg    0.74926   0.80909   0.77059       220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model_subject(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix(usr_vectors[0], preds.cpu()))\n",
    "print(metrics.classification_report(usr_vectors[0], preds.cpu(), digits = 5, zero_division = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
