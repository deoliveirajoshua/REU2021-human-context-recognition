{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '390 fBodyAccJerk-bandsEnergy()-1,16',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 40),\n",
    "            classifier_block(40, 30),\n",
    "            classifier_block(30, 25),\n",
    "            classifier_block(25, 25),\n",
    "            nn.Linear(25, 24)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def get_act_matrix(batch_size, a_dim):\n",
    "    indexes = np.random.randint(a_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((len(indexes), indexes.max()+1))\n",
    "    one_hot[np.arange(len(indexes)),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "    \n",
    "def get_usr_matrix(batch_size, u_dim):\n",
    "    indexes = np.random.randint(u_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((indexes.size, indexes.max()+1))\n",
    "    one_hot[np.arange(indexes.size),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Real Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "    \n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_10 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_11 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_12 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_13 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_14 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_15 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_16 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_17 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_18 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_19 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_20 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_21 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_22 = GAN_data[(GAN_data['Subject'] == 17) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_23 = GAN_data[(GAN_data['Subject'] == 17) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_24 = GAN_data[(GAN_data['Subject'] == 17) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9, X_10, X_11, X_12, X_13, X_14, X_15, X_16, X_17, X_18, X_19, X_20, X_21, X_22, X_23, X_24))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9) + [9] * len(X_10) + [10] * len(X_11) + [11] * len(X_12) + [12] * len(X_13) + [13] * len(X_14) + [14] * len(X_15) + [15] * len(X_16) + [16] * len(X_17) + [17] * len(X_18) + [18] * len(X_19) + [19] * len(X_20) + [20] * len(X_21) + [21] * len(X_22) + [22] * len(X_23) + [23] * len(X_24)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5, 7, 8, 11, 14, 17]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 15.937380075454712, Final Batch Loss: 3.235614776611328\n",
      "Epoch 2, Loss: 15.838646650314331, Final Batch Loss: 3.1604206562042236\n",
      "Epoch 3, Loss: 15.839359521865845, Final Batch Loss: 3.1766059398651123\n",
      "Epoch 4, Loss: 15.766544103622437, Final Batch Loss: 3.1313016414642334\n",
      "Epoch 5, Loss: 15.759380578994751, Final Batch Loss: 3.1677663326263428\n",
      "Epoch 6, Loss: 15.655831336975098, Final Batch Loss: 3.109492778778076\n",
      "Epoch 7, Loss: 15.57589340209961, Final Batch Loss: 3.1090586185455322\n",
      "Epoch 8, Loss: 15.50944471359253, Final Batch Loss: 3.147876262664795\n",
      "Epoch 9, Loss: 15.2925283908844, Final Batch Loss: 3.0778558254241943\n",
      "Epoch 10, Loss: 15.008805274963379, Final Batch Loss: 2.959566831588745\n",
      "Epoch 11, Loss: 14.978736877441406, Final Batch Loss: 3.174306869506836\n",
      "Epoch 12, Loss: 14.5722975730896, Final Batch Loss: 2.9933362007141113\n",
      "Epoch 13, Loss: 14.149859428405762, Final Batch Loss: 2.804372787475586\n",
      "Epoch 14, Loss: 13.734985828399658, Final Batch Loss: 2.6676602363586426\n",
      "Epoch 15, Loss: 13.196328401565552, Final Batch Loss: 2.3652725219726562\n",
      "Epoch 16, Loss: 13.0240318775177, Final Batch Loss: 2.448176622390747\n",
      "Epoch 17, Loss: 12.856644630432129, Final Batch Loss: 2.4746439456939697\n",
      "Epoch 18, Loss: 12.47647213935852, Final Batch Loss: 2.292447328567505\n",
      "Epoch 19, Loss: 12.441925764083862, Final Batch Loss: 2.379256010055542\n",
      "Epoch 20, Loss: 12.010960102081299, Final Batch Loss: 2.2647910118103027\n",
      "Epoch 21, Loss: 11.863916635513306, Final Batch Loss: 2.2332515716552734\n",
      "Epoch 22, Loss: 11.858577013015747, Final Batch Loss: 2.4367623329162598\n",
      "Epoch 23, Loss: 11.807393312454224, Final Batch Loss: 2.4080469608306885\n",
      "Epoch 24, Loss: 11.491910219192505, Final Batch Loss: 2.3014843463897705\n",
      "Epoch 25, Loss: 11.37950348854065, Final Batch Loss: 2.361121416091919\n",
      "Epoch 26, Loss: 11.160094738006592, Final Batch Loss: 2.256788492202759\n",
      "Epoch 27, Loss: 11.288532018661499, Final Batch Loss: 2.3710033893585205\n",
      "Epoch 28, Loss: 11.01924729347229, Final Batch Loss: 2.192547082901001\n",
      "Epoch 29, Loss: 10.89196515083313, Final Batch Loss: 2.166590690612793\n",
      "Epoch 30, Loss: 11.099654197692871, Final Batch Loss: 2.423433542251587\n",
      "Epoch 31, Loss: 10.864456415176392, Final Batch Loss: 2.202561855316162\n",
      "Epoch 32, Loss: 10.92610502243042, Final Batch Loss: 2.2973265647888184\n",
      "Epoch 33, Loss: 10.779812812805176, Final Batch Loss: 2.2586991786956787\n",
      "Epoch 34, Loss: 10.221930146217346, Final Batch Loss: 1.7526980638504028\n",
      "Epoch 35, Loss: 10.594324111938477, Final Batch Loss: 2.203733205795288\n",
      "Epoch 36, Loss: 10.927590608596802, Final Batch Loss: 2.5168659687042236\n",
      "Epoch 37, Loss: 10.37000823020935, Final Batch Loss: 2.052067279815674\n",
      "Epoch 38, Loss: 10.409626007080078, Final Batch Loss: 2.152257204055786\n",
      "Epoch 39, Loss: 10.122303128242493, Final Batch Loss: 1.9308747053146362\n",
      "Epoch 40, Loss: 10.360336542129517, Final Batch Loss: 2.0928492546081543\n",
      "Epoch 41, Loss: 10.361319780349731, Final Batch Loss: 2.2331833839416504\n",
      "Epoch 42, Loss: 10.337531566619873, Final Batch Loss: 2.2046568393707275\n",
      "Epoch 43, Loss: 10.03456962108612, Final Batch Loss: 1.9421013593673706\n",
      "Epoch 44, Loss: 10.015107274055481, Final Batch Loss: 1.9966381788253784\n",
      "Epoch 45, Loss: 10.107332348823547, Final Batch Loss: 2.0807764530181885\n",
      "Epoch 46, Loss: 9.866567492485046, Final Batch Loss: 1.9792736768722534\n",
      "Epoch 47, Loss: 9.928815007209778, Final Batch Loss: 1.857383370399475\n",
      "Epoch 48, Loss: 10.17861521244049, Final Batch Loss: 2.053135633468628\n",
      "Epoch 49, Loss: 10.07244348526001, Final Batch Loss: 2.163700580596924\n",
      "Epoch 50, Loss: 9.745290637016296, Final Batch Loss: 1.849056363105774\n",
      "Epoch 51, Loss: 9.957013726234436, Final Batch Loss: 2.0572667121887207\n",
      "Epoch 52, Loss: 9.612706184387207, Final Batch Loss: 1.7893860340118408\n",
      "Epoch 53, Loss: 9.661477327346802, Final Batch Loss: 1.8747975826263428\n",
      "Epoch 54, Loss: 9.966314315795898, Final Batch Loss: 2.1391167640686035\n",
      "Epoch 55, Loss: 9.613593697547913, Final Batch Loss: 1.9072946310043335\n",
      "Epoch 56, Loss: 9.68577790260315, Final Batch Loss: 1.904428243637085\n",
      "Epoch 57, Loss: 9.43234395980835, Final Batch Loss: 1.8685894012451172\n",
      "Epoch 58, Loss: 9.585842490196228, Final Batch Loss: 1.9210237264633179\n",
      "Epoch 59, Loss: 9.72751784324646, Final Batch Loss: 2.0796563625335693\n",
      "Epoch 60, Loss: 9.767053127288818, Final Batch Loss: 2.037658452987671\n",
      "Epoch 61, Loss: 9.530822992324829, Final Batch Loss: 1.8607137203216553\n",
      "Epoch 62, Loss: 9.298035383224487, Final Batch Loss: 1.7525873184204102\n",
      "Epoch 63, Loss: 9.440773725509644, Final Batch Loss: 1.811623215675354\n",
      "Epoch 64, Loss: 9.342984557151794, Final Batch Loss: 1.849927306175232\n",
      "Epoch 65, Loss: 9.063011646270752, Final Batch Loss: 1.6060081720352173\n",
      "Epoch 66, Loss: 9.308918476104736, Final Batch Loss: 1.8783671855926514\n",
      "Epoch 67, Loss: 9.088674664497375, Final Batch Loss: 1.7423375844955444\n",
      "Epoch 68, Loss: 9.189280271530151, Final Batch Loss: 1.8571099042892456\n",
      "Epoch 69, Loss: 9.112938404083252, Final Batch Loss: 1.7408896684646606\n",
      "Epoch 70, Loss: 9.240785837173462, Final Batch Loss: 1.8201643228530884\n",
      "Epoch 71, Loss: 9.222612380981445, Final Batch Loss: 1.8521339893341064\n",
      "Epoch 72, Loss: 8.849051833152771, Final Batch Loss: 1.6953976154327393\n",
      "Epoch 73, Loss: 9.243060231208801, Final Batch Loss: 2.020092010498047\n",
      "Epoch 74, Loss: 9.178860425949097, Final Batch Loss: 1.850673794746399\n",
      "Epoch 75, Loss: 9.293095111846924, Final Batch Loss: 1.9308809041976929\n",
      "Epoch 76, Loss: 9.170978784561157, Final Batch Loss: 2.009260892868042\n",
      "Epoch 77, Loss: 9.06686019897461, Final Batch Loss: 1.8643537759780884\n",
      "Epoch 78, Loss: 8.812675714492798, Final Batch Loss: 1.685516595840454\n",
      "Epoch 79, Loss: 8.922713875770569, Final Batch Loss: 1.8939911127090454\n",
      "Epoch 80, Loss: 8.59609854221344, Final Batch Loss: 1.6013269424438477\n",
      "Epoch 81, Loss: 8.60413670539856, Final Batch Loss: 1.6211698055267334\n",
      "Epoch 82, Loss: 8.879597663879395, Final Batch Loss: 1.9198194742202759\n",
      "Epoch 83, Loss: 8.586458921432495, Final Batch Loss: 1.704360008239746\n",
      "Epoch 84, Loss: 8.61854350566864, Final Batch Loss: 1.7191187143325806\n",
      "Epoch 85, Loss: 8.69933533668518, Final Batch Loss: 1.7193710803985596\n",
      "Epoch 86, Loss: 8.911723971366882, Final Batch Loss: 2.0669186115264893\n",
      "Epoch 87, Loss: 8.387218713760376, Final Batch Loss: 1.6311290264129639\n",
      "Epoch 88, Loss: 8.183010458946228, Final Batch Loss: 1.3983917236328125\n",
      "Epoch 89, Loss: 8.817014932632446, Final Batch Loss: 1.9560902118682861\n",
      "Epoch 90, Loss: 7.801379919052124, Final Batch Loss: 1.0629583597183228\n",
      "Epoch 91, Loss: 8.208515882492065, Final Batch Loss: 1.4178532361984253\n",
      "Epoch 92, Loss: 8.67297112941742, Final Batch Loss: 1.9369698762893677\n",
      "Epoch 93, Loss: 8.155292272567749, Final Batch Loss: 1.423399567604065\n",
      "Epoch 94, Loss: 8.600502610206604, Final Batch Loss: 1.7868902683258057\n",
      "Epoch 95, Loss: 8.36091661453247, Final Batch Loss: 1.782847285270691\n",
      "Epoch 96, Loss: 8.16233777999878, Final Batch Loss: 1.5555471181869507\n",
      "Epoch 97, Loss: 8.24053418636322, Final Batch Loss: 1.6340173482894897\n",
      "Epoch 98, Loss: 8.377182841300964, Final Batch Loss: 1.8475780487060547\n",
      "Epoch 99, Loss: 8.036990523338318, Final Batch Loss: 1.5842870473861694\n",
      "Epoch 100, Loss: 8.26756465435028, Final Batch Loss: 1.7667733430862427\n",
      "Epoch 101, Loss: 8.501930117607117, Final Batch Loss: 2.001546859741211\n",
      "Epoch 102, Loss: 8.184638500213623, Final Batch Loss: 1.73210871219635\n",
      "Epoch 103, Loss: 7.981305241584778, Final Batch Loss: 1.634800672531128\n",
      "Epoch 104, Loss: 8.11961317062378, Final Batch Loss: 1.7156312465667725\n",
      "Epoch 105, Loss: 7.825023055076599, Final Batch Loss: 1.3952200412750244\n",
      "Epoch 106, Loss: 8.360209941864014, Final Batch Loss: 1.8484294414520264\n",
      "Epoch 107, Loss: 7.839176654815674, Final Batch Loss: 1.455898642539978\n",
      "Epoch 108, Loss: 7.946849942207336, Final Batch Loss: 1.7902965545654297\n",
      "Epoch 109, Loss: 8.26635491847992, Final Batch Loss: 1.9168416261672974\n",
      "Epoch 110, Loss: 7.82339882850647, Final Batch Loss: 1.6100366115570068\n",
      "Epoch 111, Loss: 7.7359408140182495, Final Batch Loss: 1.3769029378890991\n",
      "Epoch 112, Loss: 8.236009120941162, Final Batch Loss: 1.9336005449295044\n",
      "Epoch 113, Loss: 7.590913534164429, Final Batch Loss: 1.436504602432251\n",
      "Epoch 114, Loss: 7.921166896820068, Final Batch Loss: 1.6163026094436646\n",
      "Epoch 115, Loss: 8.001950860023499, Final Batch Loss: 1.7071187496185303\n",
      "Epoch 116, Loss: 7.71169900894165, Final Batch Loss: 1.4141638278961182\n",
      "Epoch 117, Loss: 8.108338952064514, Final Batch Loss: 1.9196960926055908\n",
      "Epoch 118, Loss: 7.666371822357178, Final Batch Loss: 1.4375473260879517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119, Loss: 7.556719541549683, Final Batch Loss: 1.3809492588043213\n",
      "Epoch 120, Loss: 7.407825708389282, Final Batch Loss: 1.3216485977172852\n",
      "Epoch 121, Loss: 7.701578497886658, Final Batch Loss: 1.5736013650894165\n",
      "Epoch 122, Loss: 7.665084362030029, Final Batch Loss: 1.6203268766403198\n",
      "Epoch 123, Loss: 7.688673377037048, Final Batch Loss: 1.5337859392166138\n",
      "Epoch 124, Loss: 7.564011335372925, Final Batch Loss: 1.4941507577896118\n",
      "Epoch 125, Loss: 7.771791100502014, Final Batch Loss: 1.5619646310806274\n",
      "Epoch 126, Loss: 7.4590864181518555, Final Batch Loss: 1.4571233987808228\n",
      "Epoch 127, Loss: 7.671635866165161, Final Batch Loss: 1.6422141790390015\n",
      "Epoch 128, Loss: 7.920945405960083, Final Batch Loss: 1.9721649885177612\n",
      "Epoch 129, Loss: 7.309614896774292, Final Batch Loss: 1.370534062385559\n",
      "Epoch 130, Loss: 7.8140387535095215, Final Batch Loss: 1.799122929573059\n",
      "Epoch 131, Loss: 7.541677474975586, Final Batch Loss: 1.6175528764724731\n",
      "Epoch 132, Loss: 7.63666558265686, Final Batch Loss: 1.603276014328003\n",
      "Epoch 133, Loss: 7.385066509246826, Final Batch Loss: 1.5350186824798584\n",
      "Epoch 134, Loss: 7.54786229133606, Final Batch Loss: 1.6377432346343994\n",
      "Epoch 135, Loss: 7.18900990486145, Final Batch Loss: 1.1654226779937744\n",
      "Epoch 136, Loss: 7.322274565696716, Final Batch Loss: 1.3871155977249146\n",
      "Epoch 137, Loss: 7.217014193534851, Final Batch Loss: 1.3298375606536865\n",
      "Epoch 138, Loss: 7.160665273666382, Final Batch Loss: 1.199879765510559\n",
      "Epoch 139, Loss: 7.50296676158905, Final Batch Loss: 1.6384011507034302\n",
      "Epoch 140, Loss: 7.201176166534424, Final Batch Loss: 1.453986406326294\n",
      "Epoch 141, Loss: 7.472315669059753, Final Batch Loss: 1.7167599201202393\n",
      "Epoch 142, Loss: 7.335265874862671, Final Batch Loss: 1.4408998489379883\n",
      "Epoch 143, Loss: 7.265623211860657, Final Batch Loss: 1.4152987003326416\n",
      "Epoch 144, Loss: 7.328713417053223, Final Batch Loss: 1.6339398622512817\n",
      "Epoch 145, Loss: 7.134238362312317, Final Batch Loss: 1.429997444152832\n",
      "Epoch 146, Loss: 7.241141676902771, Final Batch Loss: 1.521823763847351\n",
      "Epoch 147, Loss: 7.112811326980591, Final Batch Loss: 1.3629896640777588\n",
      "Epoch 148, Loss: 7.124252915382385, Final Batch Loss: 1.3993042707443237\n",
      "Epoch 149, Loss: 7.162968993186951, Final Batch Loss: 1.4042909145355225\n",
      "Epoch 150, Loss: 7.188661694526672, Final Batch Loss: 1.5702265501022339\n",
      "Epoch 151, Loss: 6.902499079704285, Final Batch Loss: 1.3121329545974731\n",
      "Epoch 152, Loss: 7.160114765167236, Final Batch Loss: 1.3801430463790894\n",
      "Epoch 153, Loss: 7.002187490463257, Final Batch Loss: 1.359592080116272\n",
      "Epoch 154, Loss: 6.80902886390686, Final Batch Loss: 1.2546930313110352\n",
      "Epoch 155, Loss: 6.759687304496765, Final Batch Loss: 1.21025550365448\n",
      "Epoch 156, Loss: 7.059700965881348, Final Batch Loss: 1.529910683631897\n",
      "Epoch 157, Loss: 6.835291624069214, Final Batch Loss: 1.322576880455017\n",
      "Epoch 158, Loss: 7.016241192817688, Final Batch Loss: 1.2930335998535156\n",
      "Epoch 159, Loss: 7.171030521392822, Final Batch Loss: 1.6963138580322266\n",
      "Epoch 160, Loss: 7.115455269813538, Final Batch Loss: 1.555872917175293\n",
      "Epoch 161, Loss: 6.858150243759155, Final Batch Loss: 1.2699998617172241\n",
      "Epoch 162, Loss: 7.164933919906616, Final Batch Loss: 1.4661742448806763\n",
      "Epoch 163, Loss: 6.709012746810913, Final Batch Loss: 1.1990469694137573\n",
      "Epoch 164, Loss: 6.99117636680603, Final Batch Loss: 1.3503509759902954\n",
      "Epoch 165, Loss: 7.2108025550842285, Final Batch Loss: 1.6271982192993164\n",
      "Epoch 166, Loss: 6.940816521644592, Final Batch Loss: 1.4717960357666016\n",
      "Epoch 167, Loss: 7.349998593330383, Final Batch Loss: 1.8700370788574219\n",
      "Epoch 168, Loss: 6.8368330001831055, Final Batch Loss: 1.4070228338241577\n",
      "Epoch 169, Loss: 7.075671195983887, Final Batch Loss: 1.6004921197891235\n",
      "Epoch 170, Loss: 6.789635896682739, Final Batch Loss: 1.1575937271118164\n",
      "Epoch 171, Loss: 6.741104245185852, Final Batch Loss: 1.2808727025985718\n",
      "Epoch 172, Loss: 7.141046643257141, Final Batch Loss: 1.660768747329712\n",
      "Epoch 173, Loss: 6.8455036878585815, Final Batch Loss: 1.3245060443878174\n",
      "Epoch 174, Loss: 6.791541337966919, Final Batch Loss: 1.4355804920196533\n",
      "Epoch 175, Loss: 6.4911569356918335, Final Batch Loss: 1.1016286611557007\n",
      "Epoch 176, Loss: 6.71654736995697, Final Batch Loss: 1.294886589050293\n",
      "Epoch 177, Loss: 6.770992755889893, Final Batch Loss: 1.321975588798523\n",
      "Epoch 178, Loss: 7.200701117515564, Final Batch Loss: 1.8502200841903687\n",
      "Epoch 179, Loss: 6.819252967834473, Final Batch Loss: 1.4752024412155151\n",
      "Epoch 180, Loss: 6.989976406097412, Final Batch Loss: 1.548697590827942\n",
      "Epoch 181, Loss: 6.5038206577301025, Final Batch Loss: 1.1165697574615479\n",
      "Epoch 182, Loss: 6.558812737464905, Final Batch Loss: 1.0522531270980835\n",
      "Epoch 183, Loss: 6.825192332267761, Final Batch Loss: 1.3052551746368408\n",
      "Epoch 184, Loss: 6.670720338821411, Final Batch Loss: 1.4110212326049805\n",
      "Epoch 185, Loss: 6.353580176830292, Final Batch Loss: 0.9318726658821106\n",
      "Epoch 186, Loss: 6.477246105670929, Final Batch Loss: 0.9738498330116272\n",
      "Epoch 187, Loss: 6.477030992507935, Final Batch Loss: 1.0702918767929077\n",
      "Epoch 188, Loss: 6.639159917831421, Final Batch Loss: 1.2688344717025757\n",
      "Epoch 189, Loss: 6.548792123794556, Final Batch Loss: 1.2530230283737183\n",
      "Epoch 190, Loss: 6.759641170501709, Final Batch Loss: 1.439737319946289\n",
      "Epoch 191, Loss: 6.488288402557373, Final Batch Loss: 1.1389487981796265\n",
      "Epoch 192, Loss: 6.502047061920166, Final Batch Loss: 1.3029152154922485\n",
      "Epoch 193, Loss: 6.814934015274048, Final Batch Loss: 1.4541304111480713\n",
      "Epoch 194, Loss: 6.7034687995910645, Final Batch Loss: 1.4754562377929688\n",
      "Epoch 195, Loss: 6.5407960414886475, Final Batch Loss: 1.3317279815673828\n",
      "Epoch 196, Loss: 6.312016725540161, Final Batch Loss: 1.082634687423706\n",
      "Epoch 197, Loss: 6.696657299995422, Final Batch Loss: 1.4094535112380981\n",
      "Epoch 198, Loss: 6.254777550697327, Final Batch Loss: 0.9944649934768677\n",
      "Epoch 199, Loss: 6.161625385284424, Final Batch Loss: 1.0262467861175537\n",
      "Epoch 200, Loss: 6.863987326622009, Final Batch Loss: 1.7055720090866089\n",
      "Epoch 201, Loss: 6.437253355979919, Final Batch Loss: 1.4163875579833984\n",
      "Epoch 202, Loss: 6.421951174736023, Final Batch Loss: 1.1829917430877686\n",
      "Epoch 203, Loss: 6.322073459625244, Final Batch Loss: 1.113186001777649\n",
      "Epoch 204, Loss: 6.2645474672317505, Final Batch Loss: 1.104807734489441\n",
      "Epoch 205, Loss: 6.343166470527649, Final Batch Loss: 1.2594040632247925\n",
      "Epoch 206, Loss: 5.976714313030243, Final Batch Loss: 0.8388252854347229\n",
      "Epoch 207, Loss: 5.92506617307663, Final Batch Loss: 0.7293774485588074\n",
      "Epoch 208, Loss: 6.465454339981079, Final Batch Loss: 1.2840205430984497\n",
      "Epoch 209, Loss: 6.770120859146118, Final Batch Loss: 1.526122808456421\n",
      "Epoch 210, Loss: 6.572710156440735, Final Batch Loss: 1.299534559249878\n",
      "Epoch 211, Loss: 5.864534616470337, Final Batch Loss: 0.7548675537109375\n",
      "Epoch 212, Loss: 6.646833181381226, Final Batch Loss: 1.4948707818984985\n",
      "Epoch 213, Loss: 6.386060357093811, Final Batch Loss: 1.2500807046890259\n",
      "Epoch 214, Loss: 6.674800395965576, Final Batch Loss: 1.761810064315796\n",
      "Epoch 215, Loss: 6.566210627555847, Final Batch Loss: 1.2630608081817627\n",
      "Epoch 216, Loss: 6.289243698120117, Final Batch Loss: 1.260466456413269\n",
      "Epoch 217, Loss: 6.292330861091614, Final Batch Loss: 1.1462129354476929\n",
      "Epoch 218, Loss: 6.469845294952393, Final Batch Loss: 1.4304475784301758\n",
      "Epoch 219, Loss: 6.303347706794739, Final Batch Loss: 1.2170484066009521\n",
      "Epoch 220, Loss: 6.101763844490051, Final Batch Loss: 1.0035902261734009\n",
      "Epoch 221, Loss: 6.373944044113159, Final Batch Loss: 1.3492534160614014\n",
      "Epoch 222, Loss: 6.378665924072266, Final Batch Loss: 1.444815993309021\n",
      "Epoch 223, Loss: 6.548879146575928, Final Batch Loss: 1.5120121240615845\n",
      "Epoch 224, Loss: 6.285970687866211, Final Batch Loss: 1.2528350353240967\n",
      "Epoch 225, Loss: 6.1681050062179565, Final Batch Loss: 1.1961990594863892\n",
      "Epoch 226, Loss: 6.312858462333679, Final Batch Loss: 1.3440793752670288\n",
      "Epoch 227, Loss: 6.4065306186676025, Final Batch Loss: 1.379531979560852\n",
      "Epoch 228, Loss: 6.453221559524536, Final Batch Loss: 1.3387784957885742\n",
      "Epoch 229, Loss: 5.944967150688171, Final Batch Loss: 0.9787896871566772\n",
      "Epoch 230, Loss: 6.107807636260986, Final Batch Loss: 1.0269602537155151\n",
      "Epoch 231, Loss: 5.787603259086609, Final Batch Loss: 0.7565826177597046\n",
      "Epoch 232, Loss: 5.951348423957825, Final Batch Loss: 0.9891467094421387\n",
      "Epoch 233, Loss: 6.380548596382141, Final Batch Loss: 1.3162678480148315\n",
      "Epoch 234, Loss: 6.587557077407837, Final Batch Loss: 1.4887205362319946\n",
      "Epoch 235, Loss: 6.423983812332153, Final Batch Loss: 1.4908479452133179\n",
      "Epoch 236, Loss: 6.177507519721985, Final Batch Loss: 1.1076492071151733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237, Loss: 6.265029072761536, Final Batch Loss: 1.2797572612762451\n",
      "Epoch 238, Loss: 6.449880123138428, Final Batch Loss: 1.5597515106201172\n",
      "Epoch 239, Loss: 6.1014145612716675, Final Batch Loss: 1.2609460353851318\n",
      "Epoch 240, Loss: 5.682827174663544, Final Batch Loss: 0.8262578845024109\n",
      "Epoch 241, Loss: 5.871882617473602, Final Batch Loss: 0.9991458058357239\n",
      "Epoch 242, Loss: 6.264104843139648, Final Batch Loss: 1.395630955696106\n",
      "Epoch 243, Loss: 5.889220356941223, Final Batch Loss: 1.0054367780685425\n",
      "Epoch 244, Loss: 6.039827227592468, Final Batch Loss: 1.2137516736984253\n",
      "Epoch 245, Loss: 5.9927942752838135, Final Batch Loss: 1.2187166213989258\n",
      "Epoch 246, Loss: 6.385956048965454, Final Batch Loss: 1.4761635065078735\n",
      "Epoch 247, Loss: 6.361606597900391, Final Batch Loss: 1.5796210765838623\n",
      "Epoch 248, Loss: 5.888304591178894, Final Batch Loss: 1.0273126363754272\n",
      "Epoch 249, Loss: 6.287814617156982, Final Batch Loss: 1.500476598739624\n",
      "Epoch 250, Loss: 6.40536105632782, Final Batch Loss: 1.6319071054458618\n",
      "Epoch 251, Loss: 5.865899443626404, Final Batch Loss: 1.0003589391708374\n",
      "Epoch 252, Loss: 5.88452410697937, Final Batch Loss: 1.0343213081359863\n",
      "Epoch 253, Loss: 5.887933969497681, Final Batch Loss: 1.0334267616271973\n",
      "Epoch 254, Loss: 5.91673469543457, Final Batch Loss: 1.0713205337524414\n",
      "Epoch 255, Loss: 5.98222804069519, Final Batch Loss: 1.1895220279693604\n",
      "Epoch 256, Loss: 5.708914399147034, Final Batch Loss: 0.9530601501464844\n",
      "Epoch 257, Loss: 5.762219309806824, Final Batch Loss: 1.0621286630630493\n",
      "Epoch 258, Loss: 6.010481119155884, Final Batch Loss: 1.1664230823516846\n",
      "Epoch 259, Loss: 5.746607601642609, Final Batch Loss: 0.9968804717063904\n",
      "Epoch 260, Loss: 6.224506735801697, Final Batch Loss: 1.4347385168075562\n",
      "Epoch 261, Loss: 6.357576847076416, Final Batch Loss: 1.5903453826904297\n",
      "Epoch 262, Loss: 6.084520220756531, Final Batch Loss: 1.377687692642212\n",
      "Epoch 263, Loss: 6.231594443321228, Final Batch Loss: 1.3822298049926758\n",
      "Epoch 264, Loss: 5.98644745349884, Final Batch Loss: 1.1624990701675415\n",
      "Epoch 265, Loss: 5.831532716751099, Final Batch Loss: 1.1355069875717163\n",
      "Epoch 266, Loss: 6.1311739683151245, Final Batch Loss: 1.3795076608657837\n",
      "Epoch 267, Loss: 6.1129066944122314, Final Batch Loss: 1.45894455909729\n",
      "Epoch 268, Loss: 6.007796049118042, Final Batch Loss: 1.2761436700820923\n",
      "Epoch 269, Loss: 5.899132490158081, Final Batch Loss: 1.1624253988265991\n",
      "Epoch 270, Loss: 6.024986386299133, Final Batch Loss: 1.142590045928955\n",
      "Epoch 271, Loss: 5.739587187767029, Final Batch Loss: 1.0622336864471436\n",
      "Epoch 272, Loss: 5.6580963134765625, Final Batch Loss: 1.021891474723816\n",
      "Epoch 273, Loss: 6.1091132164001465, Final Batch Loss: 1.4214860200881958\n",
      "Epoch 274, Loss: 5.456767439842224, Final Batch Loss: 0.8868228197097778\n",
      "Epoch 275, Loss: 6.0456812381744385, Final Batch Loss: 1.4356621503829956\n",
      "Epoch 276, Loss: 5.630336165428162, Final Batch Loss: 0.9346189498901367\n",
      "Epoch 277, Loss: 5.806031942367554, Final Batch Loss: 1.167744755744934\n",
      "Epoch 278, Loss: 5.755887866020203, Final Batch Loss: 1.162965178489685\n",
      "Epoch 279, Loss: 5.757631301879883, Final Batch Loss: 1.1949068307876587\n",
      "Epoch 280, Loss: 5.891464829444885, Final Batch Loss: 1.3924249410629272\n",
      "Epoch 281, Loss: 5.342082679271698, Final Batch Loss: 0.846533477306366\n",
      "Epoch 282, Loss: 5.785479664802551, Final Batch Loss: 1.2436493635177612\n",
      "Epoch 283, Loss: 5.689599633216858, Final Batch Loss: 1.203385591506958\n",
      "Epoch 284, Loss: 5.771671175956726, Final Batch Loss: 1.2138683795928955\n",
      "Epoch 285, Loss: 5.80919086933136, Final Batch Loss: 1.1474195718765259\n",
      "Epoch 286, Loss: 5.859126925468445, Final Batch Loss: 1.3734016418457031\n",
      "Epoch 287, Loss: 5.882707595825195, Final Batch Loss: 1.2445038557052612\n",
      "Epoch 288, Loss: 5.977158069610596, Final Batch Loss: 1.3424559831619263\n",
      "Epoch 289, Loss: 5.794190764427185, Final Batch Loss: 1.2610328197479248\n",
      "Epoch 290, Loss: 5.512963056564331, Final Batch Loss: 0.9604473114013672\n",
      "Epoch 291, Loss: 5.612799406051636, Final Batch Loss: 1.1026676893234253\n",
      "Epoch 292, Loss: 5.539112567901611, Final Batch Loss: 1.0235931873321533\n",
      "Epoch 293, Loss: 5.783523917198181, Final Batch Loss: 1.2320826053619385\n",
      "Epoch 294, Loss: 5.768979787826538, Final Batch Loss: 1.1482552289962769\n",
      "Epoch 295, Loss: 5.6581186056137085, Final Batch Loss: 1.1059714555740356\n",
      "Epoch 296, Loss: 5.6700509786605835, Final Batch Loss: 1.1428173780441284\n",
      "Epoch 297, Loss: 5.546378254890442, Final Batch Loss: 1.0063735246658325\n",
      "Epoch 298, Loss: 5.838068127632141, Final Batch Loss: 1.2308685779571533\n",
      "Epoch 299, Loss: 5.65512490272522, Final Batch Loss: 1.0858526229858398\n",
      "Epoch 300, Loss: 5.499687910079956, Final Batch Loss: 1.032650113105774\n",
      "Epoch 301, Loss: 5.73162567615509, Final Batch Loss: 1.0967961549758911\n",
      "Epoch 302, Loss: 5.481551289558411, Final Batch Loss: 1.0317275524139404\n",
      "Epoch 303, Loss: 5.4264960289001465, Final Batch Loss: 1.0274226665496826\n",
      "Epoch 304, Loss: 5.923667550086975, Final Batch Loss: 1.4102261066436768\n",
      "Epoch 305, Loss: 5.352712988853455, Final Batch Loss: 0.9116615056991577\n",
      "Epoch 306, Loss: 5.478602409362793, Final Batch Loss: 1.0120621919631958\n",
      "Epoch 307, Loss: 5.7037307024002075, Final Batch Loss: 1.2879865169525146\n",
      "Epoch 308, Loss: 5.644816279411316, Final Batch Loss: 1.2868329286575317\n",
      "Epoch 309, Loss: 5.504320740699768, Final Batch Loss: 1.093580961227417\n",
      "Epoch 310, Loss: 5.434607207775116, Final Batch Loss: 0.9939542412757874\n",
      "Epoch 311, Loss: 5.330517411231995, Final Batch Loss: 0.9747990369796753\n",
      "Epoch 312, Loss: 5.523151278495789, Final Batch Loss: 1.1471309661865234\n",
      "Epoch 313, Loss: 5.663311243057251, Final Batch Loss: 1.2597789764404297\n",
      "Epoch 314, Loss: 5.430193543434143, Final Batch Loss: 1.06396484375\n",
      "Epoch 315, Loss: 5.077240705490112, Final Batch Loss: 0.7242087125778198\n",
      "Epoch 316, Loss: 5.9950162172317505, Final Batch Loss: 1.5452336072921753\n",
      "Epoch 317, Loss: 5.336942434310913, Final Batch Loss: 1.1687757968902588\n",
      "Epoch 318, Loss: 5.530307769775391, Final Batch Loss: 1.158089518547058\n",
      "Epoch 319, Loss: 5.795637726783752, Final Batch Loss: 1.4937463998794556\n",
      "Epoch 320, Loss: 5.630887985229492, Final Batch Loss: 1.2332738637924194\n",
      "Epoch 321, Loss: 5.6929309368133545, Final Batch Loss: 1.3132888078689575\n",
      "Epoch 322, Loss: 5.514288663864136, Final Batch Loss: 1.160955786705017\n",
      "Epoch 323, Loss: 5.205203115940094, Final Batch Loss: 0.8667455315589905\n",
      "Epoch 324, Loss: 5.621870756149292, Final Batch Loss: 1.4114305973052979\n",
      "Epoch 325, Loss: 5.093961656093597, Final Batch Loss: 0.8177732825279236\n",
      "Epoch 326, Loss: 5.204358696937561, Final Batch Loss: 0.8974670767784119\n",
      "Epoch 327, Loss: 5.104723393917084, Final Batch Loss: 0.8773788809776306\n",
      "Epoch 328, Loss: 5.301212668418884, Final Batch Loss: 1.089645266532898\n",
      "Epoch 329, Loss: 5.651126742362976, Final Batch Loss: 1.3280293941497803\n",
      "Epoch 330, Loss: 5.560893535614014, Final Batch Loss: 1.2074419260025024\n",
      "Epoch 331, Loss: 5.217327654361725, Final Batch Loss: 0.8686667680740356\n",
      "Epoch 332, Loss: 5.54237699508667, Final Batch Loss: 1.2237757444381714\n",
      "Epoch 333, Loss: 5.029450178146362, Final Batch Loss: 0.7720977067947388\n",
      "Epoch 334, Loss: 5.615242004394531, Final Batch Loss: 1.354785680770874\n",
      "Epoch 335, Loss: 5.549559831619263, Final Batch Loss: 1.2491531372070312\n",
      "Epoch 336, Loss: 5.511862397193909, Final Batch Loss: 1.092657208442688\n",
      "Epoch 337, Loss: 5.358280181884766, Final Batch Loss: 1.169530987739563\n",
      "Epoch 338, Loss: 5.112127602100372, Final Batch Loss: 0.8044900894165039\n",
      "Epoch 339, Loss: 5.342617630958557, Final Batch Loss: 1.0634498596191406\n",
      "Epoch 340, Loss: 5.474031925201416, Final Batch Loss: 1.232897400856018\n",
      "Epoch 341, Loss: 5.581863045692444, Final Batch Loss: 1.4631305932998657\n",
      "Epoch 342, Loss: 5.1760475635528564, Final Batch Loss: 0.9643024206161499\n",
      "Epoch 343, Loss: 5.2638484835624695, Final Batch Loss: 1.0156128406524658\n",
      "Epoch 344, Loss: 5.5449138879776, Final Batch Loss: 1.2263184785842896\n",
      "Epoch 345, Loss: 5.376084268093109, Final Batch Loss: 1.203791856765747\n",
      "Epoch 346, Loss: 5.316788911819458, Final Batch Loss: 1.0718435049057007\n",
      "Epoch 347, Loss: 5.716874539852142, Final Batch Loss: 1.4832004308700562\n",
      "Epoch 348, Loss: 5.128060698509216, Final Batch Loss: 0.843152642250061\n",
      "Epoch 349, Loss: 5.092727303504944, Final Batch Loss: 0.9154549837112427\n",
      "Epoch 350, Loss: 4.876618027687073, Final Batch Loss: 0.7434577345848083\n",
      "Epoch 351, Loss: 5.335388958454132, Final Batch Loss: 1.142075538635254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 352, Loss: 5.0750232338905334, Final Batch Loss: 0.8376269936561584\n",
      "Epoch 353, Loss: 5.36143171787262, Final Batch Loss: 1.2447483539581299\n",
      "Epoch 354, Loss: 5.382011711597443, Final Batch Loss: 1.2122623920440674\n",
      "Epoch 355, Loss: 5.125693380832672, Final Batch Loss: 1.0697342157363892\n",
      "Epoch 356, Loss: 4.886724591255188, Final Batch Loss: 0.7198971509933472\n",
      "Epoch 357, Loss: 5.417776048183441, Final Batch Loss: 1.1685751676559448\n",
      "Epoch 358, Loss: 5.22874128818512, Final Batch Loss: 1.0814683437347412\n",
      "Epoch 359, Loss: 5.234933018684387, Final Batch Loss: 1.074804425239563\n",
      "Epoch 360, Loss: 5.542828559875488, Final Batch Loss: 1.373955488204956\n",
      "Epoch 361, Loss: 5.212827444076538, Final Batch Loss: 1.1800851821899414\n",
      "Epoch 362, Loss: 5.180410325527191, Final Batch Loss: 0.9596810936927795\n",
      "Epoch 363, Loss: 5.2441911697387695, Final Batch Loss: 1.0934747457504272\n",
      "Epoch 364, Loss: 5.070677876472473, Final Batch Loss: 1.0006989240646362\n",
      "Epoch 365, Loss: 5.154768228530884, Final Batch Loss: 1.1237356662750244\n",
      "Epoch 366, Loss: 5.178578495979309, Final Batch Loss: 1.007266879081726\n",
      "Epoch 367, Loss: 5.236947178840637, Final Batch Loss: 1.1659246683120728\n",
      "Epoch 368, Loss: 5.395580410957336, Final Batch Loss: 1.3322194814682007\n",
      "Epoch 369, Loss: 5.229599952697754, Final Batch Loss: 1.0964621305465698\n",
      "Epoch 370, Loss: 5.21041864156723, Final Batch Loss: 1.0540624856948853\n",
      "Epoch 371, Loss: 4.632237374782562, Final Batch Loss: 0.6207813620567322\n",
      "Epoch 372, Loss: 5.106992483139038, Final Batch Loss: 1.0385338068008423\n",
      "Epoch 373, Loss: 5.075514435768127, Final Batch Loss: 0.9780294299125671\n",
      "Epoch 374, Loss: 5.152093708515167, Final Batch Loss: 1.052067756652832\n",
      "Epoch 375, Loss: 5.686392724514008, Final Batch Loss: 1.6492435932159424\n",
      "Epoch 376, Loss: 5.224695324897766, Final Batch Loss: 1.0898826122283936\n",
      "Epoch 377, Loss: 5.143412292003632, Final Batch Loss: 0.9074966311454773\n",
      "Epoch 378, Loss: 5.175762414932251, Final Batch Loss: 0.987862765789032\n",
      "Epoch 379, Loss: 5.134622871875763, Final Batch Loss: 0.9490097761154175\n",
      "Epoch 380, Loss: 5.212998867034912, Final Batch Loss: 1.042275071144104\n",
      "Epoch 381, Loss: 5.255972802639008, Final Batch Loss: 1.2719379663467407\n",
      "Epoch 382, Loss: 4.733222186565399, Final Batch Loss: 0.6961504817008972\n",
      "Epoch 383, Loss: 4.989045083522797, Final Batch Loss: 0.876859724521637\n",
      "Epoch 384, Loss: 5.410459399223328, Final Batch Loss: 1.3107987642288208\n",
      "Epoch 385, Loss: 4.893836617469788, Final Batch Loss: 0.8753008246421814\n",
      "Epoch 386, Loss: 5.000756859779358, Final Batch Loss: 1.0048768520355225\n",
      "Epoch 387, Loss: 4.909146547317505, Final Batch Loss: 0.9223468899726868\n",
      "Epoch 388, Loss: 4.904286861419678, Final Batch Loss: 0.8966812491416931\n",
      "Epoch 389, Loss: 4.819023847579956, Final Batch Loss: 0.7676073312759399\n",
      "Epoch 390, Loss: 5.42364102602005, Final Batch Loss: 1.3201179504394531\n",
      "Epoch 391, Loss: 5.173239350318909, Final Batch Loss: 1.1294002532958984\n",
      "Epoch 392, Loss: 5.044005215167999, Final Batch Loss: 1.0515782833099365\n",
      "Epoch 393, Loss: 4.805255174636841, Final Batch Loss: 0.9038011431694031\n",
      "Epoch 394, Loss: 5.6195677518844604, Final Batch Loss: 1.397783875465393\n",
      "Epoch 395, Loss: 5.256079435348511, Final Batch Loss: 1.1954375505447388\n",
      "Epoch 396, Loss: 4.630631268024445, Final Batch Loss: 0.8337486386299133\n",
      "Epoch 397, Loss: 5.084597647190094, Final Batch Loss: 1.1070095300674438\n",
      "Epoch 398, Loss: 5.2520323395729065, Final Batch Loss: 1.3666340112686157\n",
      "Epoch 399, Loss: 4.680848062038422, Final Batch Loss: 0.7345816493034363\n",
      "Epoch 400, Loss: 4.792359292507172, Final Batch Loss: 0.7931613326072693\n",
      "Epoch 401, Loss: 4.879502713680267, Final Batch Loss: 0.9739410281181335\n",
      "Epoch 402, Loss: 4.759254634380341, Final Batch Loss: 0.804925799369812\n",
      "Epoch 403, Loss: 4.851830542087555, Final Batch Loss: 0.7543257474899292\n",
      "Epoch 404, Loss: 4.651472389698029, Final Batch Loss: 0.6780344247817993\n",
      "Epoch 405, Loss: 4.916066110134125, Final Batch Loss: 1.0493005514144897\n",
      "Epoch 406, Loss: 4.955780625343323, Final Batch Loss: 1.0031269788742065\n",
      "Epoch 407, Loss: 5.124342918395996, Final Batch Loss: 1.111833930015564\n",
      "Epoch 408, Loss: 5.01877635717392, Final Batch Loss: 1.0759557485580444\n",
      "Epoch 409, Loss: 5.058979272842407, Final Batch Loss: 1.12557852268219\n",
      "Epoch 410, Loss: 4.973887920379639, Final Batch Loss: 1.113185167312622\n",
      "Epoch 411, Loss: 5.034974575042725, Final Batch Loss: 1.0979026556015015\n",
      "Epoch 412, Loss: 4.619251370429993, Final Batch Loss: 0.7171767950057983\n",
      "Epoch 413, Loss: 4.95119571685791, Final Batch Loss: 0.9462073445320129\n",
      "Epoch 414, Loss: 4.77462112903595, Final Batch Loss: 1.0392400026321411\n",
      "Epoch 415, Loss: 4.723489284515381, Final Batch Loss: 0.8915138840675354\n",
      "Epoch 416, Loss: 4.858143448829651, Final Batch Loss: 0.8956726789474487\n",
      "Epoch 417, Loss: 5.4607585072517395, Final Batch Loss: 1.5739314556121826\n",
      "Epoch 418, Loss: 5.044696152210236, Final Batch Loss: 1.1381944417953491\n",
      "Epoch 419, Loss: 4.916510105133057, Final Batch Loss: 0.8376078605651855\n",
      "Epoch 420, Loss: 4.806262910366058, Final Batch Loss: 0.8712244629859924\n",
      "Epoch 421, Loss: 5.038725912570953, Final Batch Loss: 1.1522713899612427\n",
      "Epoch 422, Loss: 4.47938746213913, Final Batch Loss: 0.6302671432495117\n",
      "Epoch 423, Loss: 5.196076333522797, Final Batch Loss: 1.339656114578247\n",
      "Epoch 424, Loss: 4.619757950305939, Final Batch Loss: 0.7628936171531677\n",
      "Epoch 425, Loss: 4.578414559364319, Final Batch Loss: 0.7640174031257629\n",
      "Epoch 426, Loss: 4.271903455257416, Final Batch Loss: 0.5215513110160828\n",
      "Epoch 427, Loss: 4.497630476951599, Final Batch Loss: 0.613195538520813\n",
      "Epoch 428, Loss: 4.563551068305969, Final Batch Loss: 0.7354629635810852\n",
      "Epoch 429, Loss: 4.798488795757294, Final Batch Loss: 0.9929385185241699\n",
      "Epoch 430, Loss: 5.004518151283264, Final Batch Loss: 1.0507426261901855\n",
      "Epoch 431, Loss: 5.18719357252121, Final Batch Loss: 1.412137746810913\n",
      "Epoch 432, Loss: 4.821546673774719, Final Batch Loss: 0.9328588247299194\n",
      "Epoch 433, Loss: 4.52565199136734, Final Batch Loss: 0.6910582780838013\n",
      "Epoch 434, Loss: 4.721092760562897, Final Batch Loss: 0.7761349081993103\n",
      "Epoch 435, Loss: 4.879144728183746, Final Batch Loss: 1.1092222929000854\n",
      "Epoch 436, Loss: 4.923290014266968, Final Batch Loss: 1.072174310684204\n",
      "Epoch 437, Loss: 4.626715362071991, Final Batch Loss: 0.7976357340812683\n",
      "Epoch 438, Loss: 4.844776034355164, Final Batch Loss: 1.0259109735488892\n",
      "Epoch 439, Loss: 4.6736990213394165, Final Batch Loss: 0.8851063847541809\n",
      "Epoch 440, Loss: 5.386029422283173, Final Batch Loss: 1.6611099243164062\n",
      "Epoch 441, Loss: 4.825605273246765, Final Batch Loss: 0.9541549682617188\n",
      "Epoch 442, Loss: 4.655634045600891, Final Batch Loss: 0.8533076047897339\n",
      "Epoch 443, Loss: 5.0892027616500854, Final Batch Loss: 1.2403711080551147\n",
      "Epoch 444, Loss: 4.736123025417328, Final Batch Loss: 0.829218327999115\n",
      "Epoch 445, Loss: 4.491901278495789, Final Batch Loss: 0.7234571576118469\n",
      "Epoch 446, Loss: 4.52563613653183, Final Batch Loss: 0.688949465751648\n",
      "Epoch 447, Loss: 4.838543772697449, Final Batch Loss: 0.9422246217727661\n",
      "Epoch 448, Loss: 4.629657745361328, Final Batch Loss: 0.772293210029602\n",
      "Epoch 449, Loss: 4.940149366855621, Final Batch Loss: 1.015810489654541\n",
      "Epoch 450, Loss: 4.535853803157806, Final Batch Loss: 0.7023687958717346\n",
      "Epoch 451, Loss: 4.4885993003845215, Final Batch Loss: 0.7872623801231384\n",
      "Epoch 452, Loss: 4.88594263792038, Final Batch Loss: 1.0091406106948853\n",
      "Epoch 453, Loss: 4.9606465101242065, Final Batch Loss: 1.229034185409546\n",
      "Epoch 454, Loss: 4.73514586687088, Final Batch Loss: 1.102019190788269\n",
      "Epoch 455, Loss: 4.977294504642487, Final Batch Loss: 1.187496542930603\n",
      "Epoch 456, Loss: 4.419447302818298, Final Batch Loss: 0.7080487012863159\n",
      "Epoch 457, Loss: 4.745063304901123, Final Batch Loss: 0.9288474917411804\n",
      "Epoch 458, Loss: 4.860560655593872, Final Batch Loss: 1.0775240659713745\n",
      "Epoch 459, Loss: 5.048663854598999, Final Batch Loss: 1.2467224597930908\n",
      "Epoch 460, Loss: 4.622381031513214, Final Batch Loss: 0.9848718047142029\n",
      "Epoch 461, Loss: 4.802152693271637, Final Batch Loss: 0.9948538541793823\n",
      "Epoch 462, Loss: 4.504432141780853, Final Batch Loss: 0.6802705526351929\n",
      "Epoch 463, Loss: 4.524448871612549, Final Batch Loss: 0.7710328698158264\n",
      "Epoch 464, Loss: 4.658695697784424, Final Batch Loss: 0.9747690558433533\n",
      "Epoch 465, Loss: 4.643043577671051, Final Batch Loss: 0.8746663331985474\n",
      "Epoch 466, Loss: 4.532092809677124, Final Batch Loss: 0.7474743723869324\n",
      "Epoch 467, Loss: 4.479732632637024, Final Batch Loss: 0.7598212957382202\n",
      "Epoch 468, Loss: 4.9406057596206665, Final Batch Loss: 1.2004969120025635\n",
      "Epoch 469, Loss: 4.770830035209656, Final Batch Loss: 1.1707172393798828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470, Loss: 4.680335760116577, Final Batch Loss: 1.0522072315216064\n",
      "Epoch 471, Loss: 4.3410614132881165, Final Batch Loss: 0.6707324981689453\n",
      "Epoch 472, Loss: 4.678810894489288, Final Batch Loss: 1.031516432762146\n",
      "Epoch 473, Loss: 4.9865840673446655, Final Batch Loss: 1.3513213396072388\n",
      "Epoch 474, Loss: 4.5654279589653015, Final Batch Loss: 0.7094364166259766\n",
      "Epoch 475, Loss: 4.711355745792389, Final Batch Loss: 1.0690332651138306\n",
      "Epoch 476, Loss: 4.373522758483887, Final Batch Loss: 0.7072591185569763\n",
      "Epoch 477, Loss: 5.0230395793914795, Final Batch Loss: 1.3021043539047241\n",
      "Epoch 478, Loss: 4.403699815273285, Final Batch Loss: 0.7119150757789612\n",
      "Epoch 479, Loss: 4.779449045658112, Final Batch Loss: 1.1492596864700317\n",
      "Epoch 480, Loss: 4.615052402019501, Final Batch Loss: 0.9159550666809082\n",
      "Epoch 481, Loss: 4.82598477602005, Final Batch Loss: 1.074839472770691\n",
      "Epoch 482, Loss: 5.056930422782898, Final Batch Loss: 1.375945806503296\n",
      "Epoch 483, Loss: 4.767443418502808, Final Batch Loss: 1.051206111907959\n",
      "Epoch 484, Loss: 4.582968592643738, Final Batch Loss: 1.0340936183929443\n",
      "Epoch 485, Loss: 4.454585134983063, Final Batch Loss: 0.8390602469444275\n",
      "Epoch 486, Loss: 4.2920331954956055, Final Batch Loss: 0.7556434273719788\n",
      "Epoch 487, Loss: 4.626225113868713, Final Batch Loss: 0.8381378054618835\n",
      "Epoch 488, Loss: 4.868479549884796, Final Batch Loss: 1.1746962070465088\n",
      "Epoch 489, Loss: 4.516964793205261, Final Batch Loss: 0.8755890727043152\n",
      "Epoch 490, Loss: 4.850023508071899, Final Batch Loss: 1.3182345628738403\n",
      "Epoch 491, Loss: 4.045516222715378, Final Batch Loss: 0.44692787528038025\n",
      "Epoch 492, Loss: 4.529365122318268, Final Batch Loss: 0.8375465273857117\n",
      "Epoch 493, Loss: 4.659626603126526, Final Batch Loss: 1.0086729526519775\n",
      "Epoch 494, Loss: 4.711787462234497, Final Batch Loss: 1.1535743474960327\n",
      "Epoch 495, Loss: 4.556502997875214, Final Batch Loss: 0.9362354874610901\n",
      "Epoch 496, Loss: 4.587817966938019, Final Batch Loss: 0.9737910032272339\n",
      "Epoch 497, Loss: 4.477887749671936, Final Batch Loss: 0.8971371054649353\n",
      "Epoch 498, Loss: 4.360063374042511, Final Batch Loss: 0.6856274604797363\n",
      "Epoch 499, Loss: 5.052263498306274, Final Batch Loss: 1.4668867588043213\n",
      "Epoch 500, Loss: 4.186656892299652, Final Batch Loss: 0.6229255795478821\n",
      "Epoch 501, Loss: 4.865896821022034, Final Batch Loss: 1.2459080219268799\n",
      "Epoch 502, Loss: 4.633240044116974, Final Batch Loss: 1.01746666431427\n",
      "Epoch 503, Loss: 4.316828608512878, Final Batch Loss: 0.7445797324180603\n",
      "Epoch 504, Loss: 4.220789015293121, Final Batch Loss: 0.6550076603889465\n",
      "Epoch 505, Loss: 4.770285725593567, Final Batch Loss: 1.072973608970642\n",
      "Epoch 506, Loss: 4.498850345611572, Final Batch Loss: 0.8000125288963318\n",
      "Epoch 507, Loss: 4.534166991710663, Final Batch Loss: 0.8755567669868469\n",
      "Epoch 508, Loss: 3.970046252012253, Final Batch Loss: 0.47346553206443787\n",
      "Epoch 509, Loss: 4.299775183200836, Final Batch Loss: 0.707420825958252\n",
      "Epoch 510, Loss: 4.575582027435303, Final Batch Loss: 1.0861742496490479\n",
      "Epoch 511, Loss: 4.777551531791687, Final Batch Loss: 1.1890960931777954\n",
      "Epoch 512, Loss: 4.7311732172966, Final Batch Loss: 1.0531624555587769\n",
      "Epoch 513, Loss: 4.252808332443237, Final Batch Loss: 0.6471922993659973\n",
      "Epoch 514, Loss: 4.434928357601166, Final Batch Loss: 0.8607898354530334\n",
      "Epoch 515, Loss: 4.801132380962372, Final Batch Loss: 1.1875524520874023\n",
      "Epoch 516, Loss: 4.381397485733032, Final Batch Loss: 0.7932412028312683\n",
      "Epoch 517, Loss: 4.637161016464233, Final Batch Loss: 1.0415771007537842\n",
      "Epoch 518, Loss: 4.16279000043869, Final Batch Loss: 0.6057335734367371\n",
      "Epoch 519, Loss: 4.294312238693237, Final Batch Loss: 0.8102474212646484\n",
      "Epoch 520, Loss: 4.950390994548798, Final Batch Loss: 1.4843318462371826\n",
      "Epoch 521, Loss: 4.195630431175232, Final Batch Loss: 0.7152546644210815\n",
      "Epoch 522, Loss: 4.389158248901367, Final Batch Loss: 0.8613184690475464\n",
      "Epoch 523, Loss: 4.589757263660431, Final Batch Loss: 0.8902329206466675\n",
      "Epoch 524, Loss: 4.533738553524017, Final Batch Loss: 1.0472352504730225\n",
      "Epoch 525, Loss: 4.34139358997345, Final Batch Loss: 0.8325117826461792\n",
      "Epoch 526, Loss: 4.707526862621307, Final Batch Loss: 1.1154073476791382\n",
      "Epoch 527, Loss: 4.483039081096649, Final Batch Loss: 0.902432382106781\n",
      "Epoch 528, Loss: 4.3750651478767395, Final Batch Loss: 0.7480425834655762\n",
      "Epoch 529, Loss: 4.165663540363312, Final Batch Loss: 0.665361762046814\n",
      "Epoch 530, Loss: 4.22612464427948, Final Batch Loss: 0.7495183944702148\n",
      "Epoch 531, Loss: 4.3889040350914, Final Batch Loss: 0.8432674407958984\n",
      "Epoch 532, Loss: 4.534354507923126, Final Batch Loss: 1.1274335384368896\n",
      "Epoch 533, Loss: 4.5145423412323, Final Batch Loss: 0.9880298376083374\n",
      "Epoch 534, Loss: 4.014888107776642, Final Batch Loss: 0.5662392377853394\n",
      "Epoch 535, Loss: 4.21157705783844, Final Batch Loss: 0.7268823385238647\n",
      "Epoch 536, Loss: 4.416189193725586, Final Batch Loss: 0.9445033073425293\n",
      "Epoch 537, Loss: 4.054908812046051, Final Batch Loss: 0.6458287835121155\n",
      "Epoch 538, Loss: 4.256740510463715, Final Batch Loss: 0.766901969909668\n",
      "Epoch 539, Loss: 3.945252299308777, Final Batch Loss: 0.52181476354599\n",
      "Epoch 540, Loss: 4.594381034374237, Final Batch Loss: 0.9530097246170044\n",
      "Epoch 541, Loss: 4.193701088428497, Final Batch Loss: 0.7851211428642273\n",
      "Epoch 542, Loss: 4.24029415845871, Final Batch Loss: 0.7471757531166077\n",
      "Epoch 543, Loss: 4.279566764831543, Final Batch Loss: 0.7571769952774048\n",
      "Epoch 544, Loss: 4.347338795661926, Final Batch Loss: 0.8580357432365417\n",
      "Epoch 545, Loss: 4.535418152809143, Final Batch Loss: 0.9993407130241394\n",
      "Epoch 546, Loss: 4.366096138954163, Final Batch Loss: 0.8613402247428894\n",
      "Epoch 547, Loss: 4.292478024959564, Final Batch Loss: 0.8901328444480896\n",
      "Epoch 548, Loss: 4.100518107414246, Final Batch Loss: 0.6729139089584351\n",
      "Epoch 549, Loss: 4.430887162685394, Final Batch Loss: 0.856438934803009\n",
      "Epoch 550, Loss: 4.127799868583679, Final Batch Loss: 0.6418474316596985\n",
      "Epoch 551, Loss: 4.255539298057556, Final Batch Loss: 0.8636766076087952\n",
      "Epoch 552, Loss: 4.3860262632369995, Final Batch Loss: 0.8307315707206726\n",
      "Epoch 553, Loss: 4.320499658584595, Final Batch Loss: 0.7999730110168457\n",
      "Epoch 554, Loss: 4.445492088794708, Final Batch Loss: 0.9652303457260132\n",
      "Epoch 555, Loss: 4.251459896564484, Final Batch Loss: 0.9289814829826355\n",
      "Epoch 556, Loss: 4.359357118606567, Final Batch Loss: 0.954391360282898\n",
      "Epoch 557, Loss: 4.289452016353607, Final Batch Loss: 0.8596410751342773\n",
      "Epoch 558, Loss: 4.120523571968079, Final Batch Loss: 0.7511013150215149\n",
      "Epoch 559, Loss: 4.166010081768036, Final Batch Loss: 0.827903151512146\n",
      "Epoch 560, Loss: 4.342629909515381, Final Batch Loss: 0.9108230471611023\n",
      "Epoch 561, Loss: 4.411300420761108, Final Batch Loss: 1.0062364339828491\n",
      "Epoch 562, Loss: 4.204320728778839, Final Batch Loss: 0.9391743540763855\n",
      "Epoch 563, Loss: 4.440713167190552, Final Batch Loss: 1.0122076272964478\n",
      "Epoch 564, Loss: 4.216324031352997, Final Batch Loss: 0.8571887016296387\n",
      "Epoch 565, Loss: 4.183374881744385, Final Batch Loss: 0.8588674664497375\n",
      "Epoch 566, Loss: 4.811077833175659, Final Batch Loss: 1.5335290431976318\n",
      "Epoch 567, Loss: 3.9819889664649963, Final Batch Loss: 0.6573424339294434\n",
      "Epoch 568, Loss: 4.262863993644714, Final Batch Loss: 0.9027022123336792\n",
      "Epoch 569, Loss: 3.9870793223381042, Final Batch Loss: 0.5352907180786133\n",
      "Epoch 570, Loss: 4.232074975967407, Final Batch Loss: 0.7486991882324219\n",
      "Epoch 571, Loss: 4.241857826709747, Final Batch Loss: 0.867331326007843\n",
      "Epoch 572, Loss: 4.248234510421753, Final Batch Loss: 0.7826464772224426\n",
      "Epoch 573, Loss: 4.176400423049927, Final Batch Loss: 0.7270431518554688\n",
      "Epoch 574, Loss: 4.433821082115173, Final Batch Loss: 1.1293593645095825\n",
      "Epoch 575, Loss: 3.980170786380768, Final Batch Loss: 0.5869361162185669\n",
      "Epoch 576, Loss: 4.164639890193939, Final Batch Loss: 0.9271202683448792\n",
      "Epoch 577, Loss: 3.9454265832901, Final Batch Loss: 0.6169191598892212\n",
      "Epoch 578, Loss: 4.350561618804932, Final Batch Loss: 1.0384321212768555\n",
      "Epoch 579, Loss: 4.3615723848342896, Final Batch Loss: 0.9679409265518188\n",
      "Epoch 580, Loss: 3.7326668798923492, Final Batch Loss: 0.441229909658432\n",
      "Epoch 581, Loss: 4.122125685214996, Final Batch Loss: 0.8594174981117249\n",
      "Epoch 582, Loss: 4.575288534164429, Final Batch Loss: 1.3627536296844482\n",
      "Epoch 583, Loss: 4.797072768211365, Final Batch Loss: 1.4500290155410767\n",
      "Epoch 584, Loss: 4.805380940437317, Final Batch Loss: 1.3362371921539307\n",
      "Epoch 585, Loss: 4.122739315032959, Final Batch Loss: 0.6119881868362427\n",
      "Epoch 586, Loss: 4.164629399776459, Final Batch Loss: 0.7878822088241577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 587, Loss: 4.451578140258789, Final Batch Loss: 1.1782398223876953\n",
      "Epoch 588, Loss: 4.197038114070892, Final Batch Loss: 0.9503714442253113\n",
      "Epoch 589, Loss: 3.895269989967346, Final Batch Loss: 0.622497022151947\n",
      "Epoch 590, Loss: 4.006572067737579, Final Batch Loss: 0.7450345158576965\n",
      "Epoch 591, Loss: 4.150162756443024, Final Batch Loss: 0.7785460352897644\n",
      "Epoch 592, Loss: 4.47522896528244, Final Batch Loss: 1.1474103927612305\n",
      "Epoch 593, Loss: 4.283687353134155, Final Batch Loss: 0.8921141028404236\n",
      "Epoch 594, Loss: 3.827569395303726, Final Batch Loss: 0.4256241023540497\n",
      "Epoch 595, Loss: 3.97044438123703, Final Batch Loss: 0.6147072911262512\n",
      "Epoch 596, Loss: 3.788129985332489, Final Batch Loss: 0.5150066018104553\n",
      "Epoch 597, Loss: 4.040853977203369, Final Batch Loss: 0.8871555328369141\n",
      "Epoch 598, Loss: 3.979031562805176, Final Batch Loss: 0.6913030743598938\n",
      "Epoch 599, Loss: 4.259728848934174, Final Batch Loss: 0.9257954955101013\n",
      "Epoch 600, Loss: 4.19517970085144, Final Batch Loss: 0.8433147072792053\n",
      "Epoch 601, Loss: 4.062832474708557, Final Batch Loss: 0.7837365865707397\n",
      "Epoch 602, Loss: 4.731727361679077, Final Batch Loss: 1.2552292346954346\n",
      "Epoch 603, Loss: 3.9911007285118103, Final Batch Loss: 0.6231223940849304\n",
      "Epoch 604, Loss: 4.004501223564148, Final Batch Loss: 0.8124404549598694\n",
      "Epoch 605, Loss: 3.906431496143341, Final Batch Loss: 0.702629029750824\n",
      "Epoch 606, Loss: 4.153911292552948, Final Batch Loss: 0.8805156350135803\n",
      "Epoch 607, Loss: 4.500281870365143, Final Batch Loss: 1.229981780052185\n",
      "Epoch 608, Loss: 3.800118178129196, Final Batch Loss: 0.476664274930954\n",
      "Epoch 609, Loss: 4.0478373765945435, Final Batch Loss: 0.8512828946113586\n",
      "Epoch 610, Loss: 4.112912178039551, Final Batch Loss: 0.8464336395263672\n",
      "Epoch 611, Loss: 3.896822690963745, Final Batch Loss: 0.7245607972145081\n",
      "Epoch 612, Loss: 4.17145437002182, Final Batch Loss: 0.8752855062484741\n",
      "Epoch 613, Loss: 4.2120054960250854, Final Batch Loss: 0.8098685145378113\n",
      "Epoch 614, Loss: 3.8656203746795654, Final Batch Loss: 0.7297356724739075\n",
      "Epoch 615, Loss: 3.769587755203247, Final Batch Loss: 0.5596490502357483\n",
      "Epoch 616, Loss: 4.111445426940918, Final Batch Loss: 0.8837785720825195\n",
      "Epoch 617, Loss: 4.259564578533173, Final Batch Loss: 0.8709967732429504\n",
      "Epoch 618, Loss: 3.942070722579956, Final Batch Loss: 0.6014768481254578\n",
      "Epoch 619, Loss: 4.041983127593994, Final Batch Loss: 0.8681347966194153\n",
      "Epoch 620, Loss: 3.9308469891548157, Final Batch Loss: 0.6545141339302063\n",
      "Epoch 621, Loss: 4.2625680565834045, Final Batch Loss: 0.9940586686134338\n",
      "Epoch 622, Loss: 4.144332051277161, Final Batch Loss: 0.8661275506019592\n",
      "Epoch 623, Loss: 3.603642165660858, Final Batch Loss: 0.35311102867126465\n",
      "Epoch 624, Loss: 4.561261534690857, Final Batch Loss: 1.3653970956802368\n",
      "Epoch 625, Loss: 3.7521302103996277, Final Batch Loss: 0.44431138038635254\n",
      "Epoch 626, Loss: 3.592934012413025, Final Batch Loss: 0.3417680263519287\n",
      "Epoch 627, Loss: 3.837296724319458, Final Batch Loss: 0.6553324460983276\n",
      "Epoch 628, Loss: 4.400768756866455, Final Batch Loss: 1.1326121091842651\n",
      "Epoch 629, Loss: 4.392469584941864, Final Batch Loss: 1.1204965114593506\n",
      "Epoch 630, Loss: 4.483545362949371, Final Batch Loss: 1.2601659297943115\n",
      "Epoch 631, Loss: 3.8037686347961426, Final Batch Loss: 0.6409738659858704\n",
      "Epoch 632, Loss: 3.938511908054352, Final Batch Loss: 0.7992740273475647\n",
      "Epoch 633, Loss: 3.819217026233673, Final Batch Loss: 0.6009535789489746\n",
      "Epoch 634, Loss: 3.9771687984466553, Final Batch Loss: 0.781499981880188\n",
      "Epoch 635, Loss: 3.869475543498993, Final Batch Loss: 0.6770461797714233\n",
      "Epoch 636, Loss: 4.1809887290000916, Final Batch Loss: 0.9689863324165344\n",
      "Epoch 637, Loss: 4.199616849422455, Final Batch Loss: 1.069476842880249\n",
      "Epoch 638, Loss: 3.6926246881484985, Final Batch Loss: 0.5147484540939331\n",
      "Epoch 639, Loss: 4.041749179363251, Final Batch Loss: 0.8296669721603394\n",
      "Epoch 640, Loss: 3.873366355895996, Final Batch Loss: 0.6936307549476624\n",
      "Epoch 641, Loss: 4.108886361122131, Final Batch Loss: 0.8671211004257202\n",
      "Epoch 642, Loss: 3.9113645553588867, Final Batch Loss: 0.7343680262565613\n",
      "Epoch 643, Loss: 4.064639449119568, Final Batch Loss: 0.8501170873641968\n",
      "Epoch 644, Loss: 3.804315149784088, Final Batch Loss: 0.7343348860740662\n",
      "Epoch 645, Loss: 3.7865267992019653, Final Batch Loss: 0.6728034615516663\n",
      "Epoch 646, Loss: 3.7810810804367065, Final Batch Loss: 0.6198872327804565\n",
      "Epoch 647, Loss: 3.7609728574752808, Final Batch Loss: 0.5157887935638428\n",
      "Epoch 648, Loss: 3.8658652305603027, Final Batch Loss: 0.6626260876655579\n",
      "Epoch 649, Loss: 3.9269480109214783, Final Batch Loss: 0.7314857840538025\n",
      "Epoch 650, Loss: 3.841070055961609, Final Batch Loss: 0.7486085891723633\n",
      "Epoch 651, Loss: 3.866958498954773, Final Batch Loss: 0.6953358054161072\n",
      "Epoch 652, Loss: 3.89437073469162, Final Batch Loss: 0.8189821243286133\n",
      "Epoch 653, Loss: 3.972718060016632, Final Batch Loss: 0.8375500440597534\n",
      "Epoch 654, Loss: 4.435948550701141, Final Batch Loss: 1.3867204189300537\n",
      "Epoch 655, Loss: 3.6953724026679993, Final Batch Loss: 0.6610621213912964\n",
      "Epoch 656, Loss: 3.7749890089035034, Final Batch Loss: 0.7279644012451172\n",
      "Epoch 657, Loss: 3.8002452850341797, Final Batch Loss: 0.6252185702323914\n",
      "Epoch 658, Loss: 3.8776283860206604, Final Batch Loss: 0.6564077734947205\n",
      "Epoch 659, Loss: 4.180179715156555, Final Batch Loss: 0.9912847280502319\n",
      "Epoch 660, Loss: 3.6176269352436066, Final Batch Loss: 0.44141295552253723\n",
      "Epoch 661, Loss: 4.17607057094574, Final Batch Loss: 1.0396589040756226\n",
      "Epoch 662, Loss: 3.9661614298820496, Final Batch Loss: 0.8064388036727905\n",
      "Epoch 663, Loss: 3.9344632029533386, Final Batch Loss: 0.7722802758216858\n",
      "Epoch 664, Loss: 3.821464240550995, Final Batch Loss: 0.6877701878547668\n",
      "Epoch 665, Loss: 3.547123521566391, Final Batch Loss: 0.3750236928462982\n",
      "Epoch 666, Loss: 3.8341566920280457, Final Batch Loss: 0.7477673888206482\n",
      "Epoch 667, Loss: 4.153066694736481, Final Batch Loss: 0.9782280921936035\n",
      "Epoch 668, Loss: 3.952745020389557, Final Batch Loss: 0.7223502397537231\n",
      "Epoch 669, Loss: 4.206664681434631, Final Batch Loss: 1.0169262886047363\n",
      "Epoch 670, Loss: 3.8954306840896606, Final Batch Loss: 0.7583225965499878\n",
      "Epoch 671, Loss: 4.059003412723541, Final Batch Loss: 0.7861868739128113\n",
      "Epoch 672, Loss: 3.7048674821853638, Final Batch Loss: 0.515318751335144\n",
      "Epoch 673, Loss: 4.109561383724213, Final Batch Loss: 0.944200873374939\n",
      "Epoch 674, Loss: 3.8680195212364197, Final Batch Loss: 0.7523956298828125\n",
      "Epoch 675, Loss: 3.776701509952545, Final Batch Loss: 0.6071434617042542\n",
      "Epoch 676, Loss: 3.4867283403873444, Final Batch Loss: 0.45188894867897034\n",
      "Epoch 677, Loss: 3.999195635318756, Final Batch Loss: 0.9757242798805237\n",
      "Epoch 678, Loss: 4.168552875518799, Final Batch Loss: 1.1223379373550415\n",
      "Epoch 679, Loss: 3.5261026322841644, Final Batch Loss: 0.4414388835430145\n",
      "Epoch 680, Loss: 4.078933358192444, Final Batch Loss: 1.0727733373641968\n",
      "Epoch 681, Loss: 3.922725737094879, Final Batch Loss: 0.8163070678710938\n",
      "Epoch 682, Loss: 3.7239129543304443, Final Batch Loss: 0.5910366177558899\n",
      "Epoch 683, Loss: 3.6952481269836426, Final Batch Loss: 0.7101067900657654\n",
      "Epoch 684, Loss: 3.8768235445022583, Final Batch Loss: 0.734428882598877\n",
      "Epoch 685, Loss: 3.593026280403137, Final Batch Loss: 0.5337611436843872\n",
      "Epoch 686, Loss: 4.015073835849762, Final Batch Loss: 0.8839778900146484\n",
      "Epoch 687, Loss: 3.9702141284942627, Final Batch Loss: 0.7792590260505676\n",
      "Epoch 688, Loss: 3.5407449901103973, Final Batch Loss: 0.49104419350624084\n",
      "Epoch 689, Loss: 3.4196521937847137, Final Batch Loss: 0.4108246862888336\n",
      "Epoch 690, Loss: 3.5225932002067566, Final Batch Loss: 0.4693285822868347\n",
      "Epoch 691, Loss: 3.526666671037674, Final Batch Loss: 0.45500466227531433\n",
      "Epoch 692, Loss: 4.255656480789185, Final Batch Loss: 1.1563137769699097\n",
      "Epoch 693, Loss: 3.7459221482276917, Final Batch Loss: 0.7148820161819458\n",
      "Epoch 694, Loss: 3.806962728500366, Final Batch Loss: 0.6689238548278809\n",
      "Epoch 695, Loss: 4.014724493026733, Final Batch Loss: 0.7592004537582397\n",
      "Epoch 696, Loss: 3.8732762336730957, Final Batch Loss: 0.8195487260818481\n",
      "Epoch 697, Loss: 3.8351171016693115, Final Batch Loss: 0.7315848469734192\n",
      "Epoch 698, Loss: 3.900586783885956, Final Batch Loss: 0.7498621940612793\n",
      "Epoch 699, Loss: 3.8953371047973633, Final Batch Loss: 0.8235527276992798\n",
      "Epoch 700, Loss: 3.9185903072357178, Final Batch Loss: 0.7229465842247009\n",
      "Epoch 701, Loss: 3.6677167415618896, Final Batch Loss: 0.6001039147377014\n",
      "Epoch 702, Loss: 3.421374559402466, Final Batch Loss: 0.39000123739242554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 703, Loss: 4.042892754077911, Final Batch Loss: 1.0500284433364868\n",
      "Epoch 704, Loss: 4.17257297039032, Final Batch Loss: 1.0716512203216553\n",
      "Epoch 705, Loss: 3.661650240421295, Final Batch Loss: 0.5297379493713379\n",
      "Epoch 706, Loss: 3.8591538667678833, Final Batch Loss: 0.7884578704833984\n",
      "Epoch 707, Loss: 4.137423396110535, Final Batch Loss: 1.141570806503296\n",
      "Epoch 708, Loss: 3.993346333503723, Final Batch Loss: 0.8647961616516113\n",
      "Epoch 709, Loss: 3.5855600237846375, Final Batch Loss: 0.6313950419425964\n",
      "Epoch 710, Loss: 3.7141119241714478, Final Batch Loss: 0.6737118363380432\n",
      "Epoch 711, Loss: 3.533948063850403, Final Batch Loss: 0.5212711691856384\n",
      "Epoch 712, Loss: 3.5669317841529846, Final Batch Loss: 0.4812120795249939\n",
      "Epoch 713, Loss: 3.854772686958313, Final Batch Loss: 0.8228899240493774\n",
      "Epoch 714, Loss: 3.745350658893585, Final Batch Loss: 0.7727206349372864\n",
      "Epoch 715, Loss: 4.224898993968964, Final Batch Loss: 1.1564759016036987\n",
      "Epoch 716, Loss: 3.5949347019195557, Final Batch Loss: 0.6251925826072693\n",
      "Epoch 717, Loss: 3.623088538646698, Final Batch Loss: 0.6278284788131714\n",
      "Epoch 718, Loss: 3.5785143971443176, Final Batch Loss: 0.5982934236526489\n",
      "Epoch 719, Loss: 3.3776938021183014, Final Batch Loss: 0.47810885310173035\n",
      "Epoch 720, Loss: 3.713707387447357, Final Batch Loss: 0.7132934927940369\n",
      "Epoch 721, Loss: 4.225980460643768, Final Batch Loss: 1.3206082582473755\n",
      "Epoch 722, Loss: 3.7712321877479553, Final Batch Loss: 0.7513976097106934\n",
      "Epoch 723, Loss: 3.6307852268218994, Final Batch Loss: 0.6541795134544373\n",
      "Epoch 724, Loss: 3.4027328193187714, Final Batch Loss: 0.3154050409793854\n",
      "Epoch 725, Loss: 4.205207169055939, Final Batch Loss: 1.1603950262069702\n",
      "Epoch 726, Loss: 3.966237425804138, Final Batch Loss: 0.9209136962890625\n",
      "Epoch 727, Loss: 3.7282163500785828, Final Batch Loss: 0.7402303814888\n",
      "Epoch 728, Loss: 3.9077191948890686, Final Batch Loss: 0.9337989687919617\n",
      "Epoch 729, Loss: 3.5054858922958374, Final Batch Loss: 0.6377200484275818\n",
      "Epoch 730, Loss: 3.6411535441875458, Final Batch Loss: 0.4956684410572052\n",
      "Epoch 731, Loss: 3.8714492321014404, Final Batch Loss: 0.9356853365898132\n",
      "Epoch 732, Loss: 3.647800922393799, Final Batch Loss: 0.5893508195877075\n",
      "Epoch 733, Loss: 3.353270024061203, Final Batch Loss: 0.446389764547348\n",
      "Epoch 734, Loss: 3.875717282295227, Final Batch Loss: 0.9058716893196106\n",
      "Epoch 735, Loss: 3.3790318965911865, Final Batch Loss: 0.5193401575088501\n",
      "Epoch 736, Loss: 3.268544226884842, Final Batch Loss: 0.29482725262641907\n",
      "Epoch 737, Loss: 3.8409016132354736, Final Batch Loss: 0.8232517838478088\n",
      "Epoch 738, Loss: 3.739747107028961, Final Batch Loss: 0.7248343825340271\n",
      "Epoch 739, Loss: 3.8364243507385254, Final Batch Loss: 0.9153844118118286\n",
      "Epoch 740, Loss: 3.9777228236198425, Final Batch Loss: 1.039140224456787\n",
      "Epoch 741, Loss: 4.096088945865631, Final Batch Loss: 1.1512833833694458\n",
      "Epoch 742, Loss: 3.6124736666679382, Final Batch Loss: 0.742761492729187\n",
      "Epoch 743, Loss: 3.9599549174308777, Final Batch Loss: 0.9442494511604309\n",
      "Epoch 744, Loss: 3.4981056451797485, Final Batch Loss: 0.5184831619262695\n",
      "Epoch 745, Loss: 3.7229158878326416, Final Batch Loss: 0.8413747549057007\n",
      "Epoch 746, Loss: 4.291541159152985, Final Batch Loss: 1.40439772605896\n",
      "Epoch 747, Loss: 3.3571370244026184, Final Batch Loss: 0.47052001953125\n",
      "Epoch 748, Loss: 3.793517291545868, Final Batch Loss: 0.8058773875236511\n",
      "Epoch 749, Loss: 3.5140867233276367, Final Batch Loss: 0.5310201048851013\n",
      "Epoch 750, Loss: 3.958851993083954, Final Batch Loss: 0.9787837266921997\n",
      "Epoch 751, Loss: 3.6710628271102905, Final Batch Loss: 0.73537677526474\n",
      "Epoch 752, Loss: 3.2937936186790466, Final Batch Loss: 0.3758743405342102\n",
      "Epoch 753, Loss: 3.4834185242652893, Final Batch Loss: 0.6467288136482239\n",
      "Epoch 754, Loss: 3.534380614757538, Final Batch Loss: 0.5929268002510071\n",
      "Epoch 755, Loss: 3.559643864631653, Final Batch Loss: 0.6240355372428894\n",
      "Epoch 756, Loss: 3.5640804171562195, Final Batch Loss: 0.7262957692146301\n",
      "Epoch 757, Loss: 3.982864558696747, Final Batch Loss: 0.9703445434570312\n",
      "Epoch 758, Loss: 3.6205970644950867, Final Batch Loss: 0.7611978650093079\n",
      "Epoch 759, Loss: 3.5515950322151184, Final Batch Loss: 0.7010641098022461\n",
      "Epoch 760, Loss: 3.3674174547195435, Final Batch Loss: 0.47503358125686646\n",
      "Epoch 761, Loss: 3.3722539842128754, Final Batch Loss: 0.37854471802711487\n",
      "Epoch 762, Loss: 3.684258282184601, Final Batch Loss: 0.8993965983390808\n",
      "Epoch 763, Loss: 3.2872257232666016, Final Batch Loss: 0.4210342764854431\n",
      "Epoch 764, Loss: 3.5712336897850037, Final Batch Loss: 0.5863469839096069\n",
      "Epoch 765, Loss: 3.5190776586532593, Final Batch Loss: 0.4963342547416687\n",
      "Epoch 766, Loss: 3.8712411522865295, Final Batch Loss: 0.8656724095344543\n",
      "Epoch 767, Loss: 3.192951649427414, Final Batch Loss: 0.2674005329608917\n",
      "Epoch 768, Loss: 3.6742610931396484, Final Batch Loss: 0.7787476778030396\n",
      "Epoch 769, Loss: 3.280065953731537, Final Batch Loss: 0.36012572050094604\n",
      "Epoch 770, Loss: 3.426289975643158, Final Batch Loss: 0.5245808362960815\n",
      "Epoch 771, Loss: 3.6808456778526306, Final Batch Loss: 0.7324588894844055\n",
      "Epoch 772, Loss: 3.609755277633667, Final Batch Loss: 0.720926821231842\n",
      "Epoch 773, Loss: 3.5733067393302917, Final Batch Loss: 0.6600098609924316\n",
      "Epoch 774, Loss: 3.7010831236839294, Final Batch Loss: 0.8208591341972351\n",
      "Epoch 775, Loss: 3.4400036931037903, Final Batch Loss: 0.6686609387397766\n",
      "Epoch 776, Loss: 3.553137242794037, Final Batch Loss: 0.6481121778488159\n",
      "Epoch 777, Loss: 3.611571431159973, Final Batch Loss: 0.636628270149231\n",
      "Epoch 778, Loss: 3.6693339347839355, Final Batch Loss: 0.5519300103187561\n",
      "Epoch 779, Loss: 3.4181384444236755, Final Batch Loss: 0.45755481719970703\n",
      "Epoch 780, Loss: 3.25091752409935, Final Batch Loss: 0.3618142306804657\n",
      "Epoch 781, Loss: 3.799850046634674, Final Batch Loss: 0.9997075796127319\n",
      "Epoch 782, Loss: 4.000316858291626, Final Batch Loss: 1.0578807592391968\n",
      "Epoch 783, Loss: 3.677473306655884, Final Batch Loss: 0.8874578475952148\n",
      "Epoch 784, Loss: 3.525599479675293, Final Batch Loss: 0.590657651424408\n",
      "Epoch 785, Loss: 3.383428156375885, Final Batch Loss: 0.5921660661697388\n",
      "Epoch 786, Loss: 3.5428274869918823, Final Batch Loss: 0.6149746775627136\n",
      "Epoch 787, Loss: 3.5845159888267517, Final Batch Loss: 0.6653856039047241\n",
      "Epoch 788, Loss: 3.5990973711013794, Final Batch Loss: 0.7597288489341736\n",
      "Epoch 789, Loss: 3.4486826062202454, Final Batch Loss: 0.5211056470870972\n",
      "Epoch 790, Loss: 3.353290855884552, Final Batch Loss: 0.5221195816993713\n",
      "Epoch 791, Loss: 3.124562919139862, Final Batch Loss: 0.28925615549087524\n",
      "Epoch 792, Loss: 3.426610231399536, Final Batch Loss: 0.6053145527839661\n",
      "Epoch 793, Loss: 3.689770519733429, Final Batch Loss: 0.931581974029541\n",
      "Epoch 794, Loss: 3.7693507075309753, Final Batch Loss: 0.8556246161460876\n",
      "Epoch 795, Loss: 3.2233418822288513, Final Batch Loss: 0.4174177646636963\n",
      "Epoch 796, Loss: 3.350501239299774, Final Batch Loss: 0.5206127762794495\n",
      "Epoch 797, Loss: 3.5042887330055237, Final Batch Loss: 0.7511210441589355\n",
      "Epoch 798, Loss: 3.6011061668395996, Final Batch Loss: 0.7776305079460144\n",
      "Epoch 799, Loss: 3.757885992527008, Final Batch Loss: 0.855209469795227\n",
      "Epoch 800, Loss: 3.564064860343933, Final Batch Loss: 0.6011853814125061\n",
      "Epoch 801, Loss: 3.966816008090973, Final Batch Loss: 0.9686166644096375\n",
      "Epoch 802, Loss: 3.1938054263591766, Final Batch Loss: 0.3840581476688385\n",
      "Epoch 803, Loss: 3.378658264875412, Final Batch Loss: 0.42422714829444885\n",
      "Epoch 804, Loss: 3.275940090417862, Final Batch Loss: 0.43413886427879333\n",
      "Epoch 805, Loss: 3.679745376110077, Final Batch Loss: 0.9376295804977417\n",
      "Epoch 806, Loss: 3.496018350124359, Final Batch Loss: 0.7166807055473328\n",
      "Epoch 807, Loss: 3.504950225353241, Final Batch Loss: 0.6203106641769409\n",
      "Epoch 808, Loss: 3.3637319207191467, Final Batch Loss: 0.5549663305282593\n",
      "Epoch 809, Loss: 3.637157440185547, Final Batch Loss: 0.9017928242683411\n",
      "Epoch 810, Loss: 3.578435003757477, Final Batch Loss: 0.7659016847610474\n",
      "Epoch 811, Loss: 3.631274998188019, Final Batch Loss: 0.6638031601905823\n",
      "Epoch 812, Loss: 2.961456313729286, Final Batch Loss: 0.22560282051563263\n",
      "Epoch 813, Loss: 3.9405863881111145, Final Batch Loss: 0.9947556853294373\n",
      "Epoch 814, Loss: 3.7234854102134705, Final Batch Loss: 0.8909082412719727\n",
      "Epoch 815, Loss: 3.15752711892128, Final Batch Loss: 0.347408264875412\n",
      "Epoch 816, Loss: 3.680198013782501, Final Batch Loss: 0.8751932382583618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 817, Loss: 3.5654029846191406, Final Batch Loss: 0.6184566617012024\n",
      "Epoch 818, Loss: 3.521871507167816, Final Batch Loss: 0.7351773381233215\n",
      "Epoch 819, Loss: 3.3846647143363953, Final Batch Loss: 0.5141862630844116\n",
      "Epoch 820, Loss: 3.410908877849579, Final Batch Loss: 0.5941787362098694\n",
      "Epoch 821, Loss: 3.4910477995872498, Final Batch Loss: 0.6553317308425903\n",
      "Epoch 822, Loss: 3.465358853340149, Final Batch Loss: 0.6788745522499084\n",
      "Epoch 823, Loss: 3.7994651794433594, Final Batch Loss: 0.9958643317222595\n",
      "Epoch 824, Loss: 3.797205328941345, Final Batch Loss: 1.0474690198898315\n",
      "Epoch 825, Loss: 3.3429012298583984, Final Batch Loss: 0.5299071669578552\n",
      "Epoch 826, Loss: 3.2725214064121246, Final Batch Loss: 0.48235949873924255\n",
      "Epoch 827, Loss: 3.0918417274951935, Final Batch Loss: 0.33172520995140076\n",
      "Epoch 828, Loss: 3.214739501476288, Final Batch Loss: 0.46065306663513184\n",
      "Epoch 829, Loss: 3.5171753764152527, Final Batch Loss: 0.7245392799377441\n",
      "Epoch 830, Loss: 3.5282431840896606, Final Batch Loss: 0.748650074005127\n",
      "Epoch 831, Loss: 3.490766704082489, Final Batch Loss: 0.749323844909668\n",
      "Epoch 832, Loss: 3.5437803268432617, Final Batch Loss: 0.887403666973114\n",
      "Epoch 833, Loss: 3.2622873783111572, Final Batch Loss: 0.4822239279747009\n",
      "Epoch 834, Loss: 3.6377132534980774, Final Batch Loss: 0.8389970064163208\n",
      "Epoch 835, Loss: 3.464050054550171, Final Batch Loss: 0.6754098534584045\n",
      "Epoch 836, Loss: 3.1947087347507477, Final Batch Loss: 0.30177995562553406\n",
      "Epoch 837, Loss: 3.5037349462509155, Final Batch Loss: 0.6274219155311584\n",
      "Epoch 838, Loss: 3.8100345730781555, Final Batch Loss: 0.9619103670120239\n",
      "Epoch 839, Loss: 3.243399679660797, Final Batch Loss: 0.5135135054588318\n",
      "Epoch 840, Loss: 3.6924152970314026, Final Batch Loss: 0.9503612518310547\n",
      "Epoch 841, Loss: 3.1777689158916473, Final Batch Loss: 0.3467561900615692\n",
      "Epoch 842, Loss: 3.169231802225113, Final Batch Loss: 0.4924602806568146\n",
      "Epoch 843, Loss: 3.8614158630371094, Final Batch Loss: 0.9544839262962341\n",
      "Epoch 844, Loss: 3.528830885887146, Final Batch Loss: 0.7895885705947876\n",
      "Epoch 845, Loss: 3.5221054553985596, Final Batch Loss: 0.8449949026107788\n",
      "Epoch 846, Loss: 3.5137566328048706, Final Batch Loss: 0.6910465955734253\n",
      "Epoch 847, Loss: 3.6545551419258118, Final Batch Loss: 0.818489670753479\n",
      "Epoch 848, Loss: 3.768537223339081, Final Batch Loss: 1.0065124034881592\n",
      "Epoch 849, Loss: 3.467357575893402, Final Batch Loss: 0.7354000806808472\n",
      "Epoch 850, Loss: 3.770895302295685, Final Batch Loss: 0.9577816724777222\n",
      "Epoch 851, Loss: 3.6144224405288696, Final Batch Loss: 0.93917316198349\n",
      "Epoch 852, Loss: 3.445370316505432, Final Batch Loss: 0.5846544504165649\n",
      "Epoch 853, Loss: 3.306083917617798, Final Batch Loss: 0.5871955752372742\n",
      "Epoch 854, Loss: 3.5114294290542603, Final Batch Loss: 0.7694435119628906\n",
      "Epoch 855, Loss: 3.1614422500133514, Final Batch Loss: 0.40305694937705994\n",
      "Epoch 856, Loss: 3.541863441467285, Final Batch Loss: 0.7878807187080383\n",
      "Epoch 857, Loss: 3.604094445705414, Final Batch Loss: 0.892175018787384\n",
      "Epoch 858, Loss: 3.4326655864715576, Final Batch Loss: 0.6772872805595398\n",
      "Epoch 859, Loss: 3.5840953588485718, Final Batch Loss: 0.9312278628349304\n",
      "Epoch 860, Loss: 3.33603698015213, Final Batch Loss: 0.5812366604804993\n",
      "Epoch 861, Loss: 3.0995393991470337, Final Batch Loss: 0.43971937894821167\n",
      "Epoch 862, Loss: 3.2311935424804688, Final Batch Loss: 0.5116567611694336\n",
      "Epoch 863, Loss: 3.414466083049774, Final Batch Loss: 0.6105315089225769\n",
      "Epoch 864, Loss: 3.3756510615348816, Final Batch Loss: 0.5769336819648743\n",
      "Epoch 865, Loss: 3.503243148326874, Final Batch Loss: 0.729694664478302\n",
      "Epoch 866, Loss: 3.777334451675415, Final Batch Loss: 1.0033866167068481\n",
      "Epoch 867, Loss: 3.1824846863746643, Final Batch Loss: 0.5127795338630676\n",
      "Epoch 868, Loss: 3.313399612903595, Final Batch Loss: 0.6280719041824341\n",
      "Epoch 869, Loss: 3.4660897254943848, Final Batch Loss: 0.7489452958106995\n",
      "Epoch 870, Loss: 3.1585067212581635, Final Batch Loss: 0.3152429759502411\n",
      "Epoch 871, Loss: 2.898174077272415, Final Batch Loss: 0.25491926074028015\n",
      "Epoch 872, Loss: 3.268754541873932, Final Batch Loss: 0.5539281368255615\n",
      "Epoch 873, Loss: 3.1996028423309326, Final Batch Loss: 0.6397724151611328\n",
      "Epoch 874, Loss: 2.97212216258049, Final Batch Loss: 0.2858223021030426\n",
      "Epoch 875, Loss: 3.3344298005104065, Final Batch Loss: 0.5914617776870728\n",
      "Epoch 876, Loss: 3.1411103308200836, Final Batch Loss: 0.47281786799430847\n",
      "Epoch 877, Loss: 3.633147656917572, Final Batch Loss: 0.8192934989929199\n",
      "Epoch 878, Loss: 3.420868754386902, Final Batch Loss: 0.6562680006027222\n",
      "Epoch 879, Loss: 3.229161858558655, Final Batch Loss: 0.6210256218910217\n",
      "Epoch 880, Loss: 3.8844217658042908, Final Batch Loss: 1.255722999572754\n",
      "Epoch 881, Loss: 3.414987027645111, Final Batch Loss: 0.6361992955207825\n",
      "Epoch 882, Loss: 3.5261190533638, Final Batch Loss: 0.67621910572052\n",
      "Epoch 883, Loss: 3.243699163198471, Final Batch Loss: 0.42900845408439636\n",
      "Epoch 884, Loss: 3.153676688671112, Final Batch Loss: 0.36585164070129395\n",
      "Epoch 885, Loss: 3.3051355481147766, Final Batch Loss: 0.5771733522415161\n",
      "Epoch 886, Loss: 3.547502100467682, Final Batch Loss: 0.8176302909851074\n",
      "Epoch 887, Loss: 3.138095051050186, Final Batch Loss: 0.4720599949359894\n",
      "Epoch 888, Loss: 3.445960819721222, Final Batch Loss: 0.6914858222007751\n",
      "Epoch 889, Loss: 3.422975718975067, Final Batch Loss: 0.6518911719322205\n",
      "Epoch 890, Loss: 3.27352374792099, Final Batch Loss: 0.6224273443222046\n",
      "Epoch 891, Loss: 3.5013676285743713, Final Batch Loss: 0.8695778846740723\n",
      "Epoch 892, Loss: 2.7110526636242867, Final Batch Loss: 0.11580503731966019\n",
      "Epoch 893, Loss: 3.2135887145996094, Final Batch Loss: 0.6200537085533142\n",
      "Epoch 894, Loss: 3.6376850605010986, Final Batch Loss: 0.9234601855278015\n",
      "Epoch 895, Loss: 3.22744619846344, Final Batch Loss: 0.620103657245636\n",
      "Epoch 896, Loss: 3.755423128604889, Final Batch Loss: 0.989747166633606\n",
      "Epoch 897, Loss: 3.0948894023895264, Final Batch Loss: 0.4763907790184021\n",
      "Epoch 898, Loss: 3.415439009666443, Final Batch Loss: 0.6910732984542847\n",
      "Epoch 899, Loss: 3.537413716316223, Final Batch Loss: 0.777775228023529\n",
      "Epoch 900, Loss: 3.2738821506500244, Final Batch Loss: 0.5367543697357178\n",
      "Epoch 901, Loss: 2.94562990963459, Final Batch Loss: 0.24919135868549347\n",
      "Epoch 902, Loss: 3.0507791936397552, Final Batch Loss: 0.3606274425983429\n",
      "Epoch 903, Loss: 2.8034766614437103, Final Batch Loss: 0.25102540850639343\n",
      "Epoch 904, Loss: 3.32853901386261, Final Batch Loss: 0.5256168246269226\n",
      "Epoch 905, Loss: 3.5325571298599243, Final Batch Loss: 0.8399071097373962\n",
      "Epoch 906, Loss: 3.502767503261566, Final Batch Loss: 0.8577872514724731\n",
      "Epoch 907, Loss: 2.710516795516014, Final Batch Loss: 0.1260722130537033\n",
      "Epoch 908, Loss: 3.386728346347809, Final Batch Loss: 0.6459892392158508\n",
      "Epoch 909, Loss: 3.2787124514579773, Final Batch Loss: 0.6125591397285461\n",
      "Epoch 910, Loss: 3.3547977209091187, Final Batch Loss: 0.6739545464515686\n",
      "Epoch 911, Loss: 2.956324130296707, Final Batch Loss: 0.4873235523700714\n",
      "Epoch 912, Loss: 3.350519061088562, Final Batch Loss: 0.699874997138977\n",
      "Epoch 913, Loss: 3.2323646545410156, Final Batch Loss: 0.5044440627098083\n",
      "Epoch 914, Loss: 3.230275571346283, Final Batch Loss: 0.7067537307739258\n",
      "Epoch 915, Loss: 3.1795735359191895, Final Batch Loss: 0.5202397108078003\n",
      "Epoch 916, Loss: 2.852023094892502, Final Batch Loss: 0.31629452109336853\n",
      "Epoch 917, Loss: 3.0001558661460876, Final Batch Loss: 0.4185476303100586\n",
      "Epoch 918, Loss: 2.8349795937538147, Final Batch Loss: 0.2627723813056946\n",
      "Epoch 919, Loss: 3.0148325860500336, Final Batch Loss: 0.38710591197013855\n",
      "Epoch 920, Loss: 3.390235126018524, Final Batch Loss: 0.746911346912384\n",
      "Epoch 921, Loss: 3.6194050908088684, Final Batch Loss: 0.9037104249000549\n",
      "Epoch 922, Loss: 3.079158514738083, Final Batch Loss: 0.38697853684425354\n",
      "Epoch 923, Loss: 3.476196765899658, Final Batch Loss: 0.8879156708717346\n",
      "Epoch 924, Loss: 3.2090144753456116, Final Batch Loss: 0.4980795979499817\n",
      "Epoch 925, Loss: 3.310613751411438, Final Batch Loss: 0.7426517605781555\n",
      "Epoch 926, Loss: 3.121462821960449, Final Batch Loss: 0.6042639017105103\n",
      "Epoch 927, Loss: 3.198155701160431, Final Batch Loss: 0.5356962084770203\n",
      "Epoch 928, Loss: 3.2702768445014954, Final Batch Loss: 0.6697052717208862\n",
      "Epoch 929, Loss: 3.236726939678192, Final Batch Loss: 0.7284436225891113\n",
      "Epoch 930, Loss: 3.3895114064216614, Final Batch Loss: 0.9097556471824646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 931, Loss: 3.1690532565116882, Final Batch Loss: 0.541054368019104\n",
      "Epoch 932, Loss: 3.1980621218681335, Final Batch Loss: 0.5041126608848572\n",
      "Epoch 933, Loss: 3.189954459667206, Final Batch Loss: 0.6557515859603882\n",
      "Epoch 934, Loss: 3.286547541618347, Final Batch Loss: 0.7407892942428589\n",
      "Epoch 935, Loss: 3.5541642904281616, Final Batch Loss: 1.0014063119888306\n",
      "Epoch 936, Loss: 3.1979544162750244, Final Batch Loss: 0.46842002868652344\n",
      "Epoch 937, Loss: 3.161178410053253, Final Batch Loss: 0.5897001028060913\n",
      "Epoch 938, Loss: 3.725052773952484, Final Batch Loss: 1.0954887866973877\n",
      "Epoch 939, Loss: 3.486091911792755, Final Batch Loss: 0.8749174475669861\n",
      "Epoch 940, Loss: 3.218143403530121, Final Batch Loss: 0.5356869101524353\n",
      "Epoch 941, Loss: 3.3981735706329346, Final Batch Loss: 0.8129541277885437\n",
      "Epoch 942, Loss: 3.484389007091522, Final Batch Loss: 1.0066343545913696\n",
      "Epoch 943, Loss: 3.333767354488373, Final Batch Loss: 0.7268195152282715\n",
      "Epoch 944, Loss: 2.7587678283452988, Final Batch Loss: 0.1851099580526352\n",
      "Epoch 945, Loss: 2.7722471952438354, Final Batch Loss: 0.14882493019104004\n",
      "Epoch 946, Loss: 3.0690456330776215, Final Batch Loss: 0.47892120480537415\n",
      "Epoch 947, Loss: 2.9073140621185303, Final Batch Loss: 0.26077908277511597\n",
      "Epoch 948, Loss: 3.6795173287391663, Final Batch Loss: 1.0552045106887817\n",
      "Epoch 949, Loss: 3.118017315864563, Final Batch Loss: 0.6085444688796997\n",
      "Epoch 950, Loss: 3.1636601090431213, Final Batch Loss: 0.6643099188804626\n",
      "Epoch 951, Loss: 3.5619438886642456, Final Batch Loss: 0.9416745901107788\n",
      "Epoch 952, Loss: 3.068485051393509, Final Batch Loss: 0.4121480882167816\n",
      "Epoch 953, Loss: 3.6269420385360718, Final Batch Loss: 0.9412528276443481\n",
      "Epoch 954, Loss: 3.189772665500641, Final Batch Loss: 0.5596253871917725\n",
      "Epoch 955, Loss: 2.985053926706314, Final Batch Loss: 0.41935065388679504\n",
      "Epoch 956, Loss: 3.166543483734131, Final Batch Loss: 0.5166133046150208\n",
      "Epoch 957, Loss: 2.8479743599891663, Final Batch Loss: 0.38430142402648926\n",
      "Epoch 958, Loss: 3.122088313102722, Final Batch Loss: 0.5219866037368774\n",
      "Epoch 959, Loss: 2.9899975955486298, Final Batch Loss: 0.42352232336997986\n",
      "Epoch 960, Loss: 3.575556755065918, Final Batch Loss: 0.9739958643913269\n",
      "Epoch 961, Loss: 3.274475634098053, Final Batch Loss: 0.7300009727478027\n",
      "Epoch 962, Loss: 2.9332486391067505, Final Batch Loss: 0.3350335955619812\n",
      "Epoch 963, Loss: 3.1559093594551086, Final Batch Loss: 0.5616382360458374\n",
      "Epoch 964, Loss: 3.176447570323944, Final Batch Loss: 0.6219748258590698\n",
      "Epoch 965, Loss: 3.344512462615967, Final Batch Loss: 0.8910785913467407\n",
      "Epoch 966, Loss: 3.0951922237873077, Final Batch Loss: 0.4995857775211334\n",
      "Epoch 967, Loss: 3.2117358446121216, Final Batch Loss: 0.6081151366233826\n",
      "Epoch 968, Loss: 3.1142856776714325, Final Batch Loss: 0.48450103402137756\n",
      "Epoch 969, Loss: 3.4839932918548584, Final Batch Loss: 0.8615173697471619\n",
      "Epoch 970, Loss: 3.1824023127555847, Final Batch Loss: 0.5429428815841675\n",
      "Epoch 971, Loss: 3.1564033031463623, Final Batch Loss: 0.5664122104644775\n",
      "Epoch 972, Loss: 3.1520930528640747, Final Batch Loss: 0.6163825392723083\n",
      "Epoch 973, Loss: 2.9284283816814423, Final Batch Loss: 0.35219213366508484\n",
      "Epoch 974, Loss: 2.947566866874695, Final Batch Loss: 0.4046304225921631\n",
      "Epoch 975, Loss: 3.2248098254203796, Final Batch Loss: 0.6711550951004028\n",
      "Epoch 976, Loss: 3.117794930934906, Final Batch Loss: 0.6471096277236938\n",
      "Epoch 977, Loss: 2.999096930027008, Final Batch Loss: 0.3332837224006653\n",
      "Epoch 978, Loss: 3.5371700525283813, Final Batch Loss: 1.0563298463821411\n",
      "Epoch 979, Loss: 3.2961586713790894, Final Batch Loss: 0.6801546812057495\n",
      "Epoch 980, Loss: 3.204739987850189, Final Batch Loss: 0.735944390296936\n",
      "Epoch 981, Loss: 3.2328301668167114, Final Batch Loss: 0.7112687826156616\n",
      "Epoch 982, Loss: 3.302762448787689, Final Batch Loss: 0.7070888876914978\n",
      "Epoch 983, Loss: 3.0710627734661102, Final Batch Loss: 0.4823363721370697\n",
      "Epoch 984, Loss: 3.0947603285312653, Final Batch Loss: 0.4301845133304596\n",
      "Epoch 985, Loss: 3.0513084828853607, Final Batch Loss: 0.446029394865036\n",
      "Epoch 986, Loss: 3.1347551345825195, Final Batch Loss: 0.4982319474220276\n",
      "Epoch 987, Loss: 3.0664151310920715, Final Batch Loss: 0.6732515692710876\n",
      "Epoch 988, Loss: 2.7647311687469482, Final Batch Loss: 0.2510119676589966\n",
      "Epoch 989, Loss: 3.09336519241333, Final Batch Loss: 0.603699266910553\n",
      "Epoch 990, Loss: 3.2105056047439575, Final Batch Loss: 0.6534468531608582\n",
      "Epoch 991, Loss: 3.1757219433784485, Final Batch Loss: 0.65976482629776\n",
      "Epoch 992, Loss: 3.3412578105926514, Final Batch Loss: 0.8001106977462769\n",
      "Epoch 993, Loss: 3.2569798827171326, Final Batch Loss: 0.6498316526412964\n",
      "Epoch 994, Loss: 3.215111017227173, Final Batch Loss: 0.598628580570221\n",
      "Epoch 995, Loss: 3.4228480458259583, Final Batch Loss: 0.8193383812904358\n",
      "Epoch 996, Loss: 3.21860671043396, Final Batch Loss: 0.6713311076164246\n",
      "Epoch 997, Loss: 3.3485915660858154, Final Batch Loss: 0.6251692771911621\n",
      "Epoch 998, Loss: 3.200486719608307, Final Batch Loss: 0.6296244859695435\n",
      "Epoch 999, Loss: 2.895735055208206, Final Batch Loss: 0.39515140652656555\n",
      "Epoch 1000, Loss: 3.352707862854004, Final Batch Loss: 0.7892686128616333\n",
      "Epoch 1001, Loss: 2.8874413073062897, Final Batch Loss: 0.3621232807636261\n",
      "Epoch 1002, Loss: 2.9361692368984222, Final Batch Loss: 0.3728828728199005\n",
      "Epoch 1003, Loss: 2.8877697587013245, Final Batch Loss: 0.41150879859924316\n",
      "Epoch 1004, Loss: 3.2431713342666626, Final Batch Loss: 0.8153459429740906\n",
      "Epoch 1005, Loss: 2.985744535923004, Final Batch Loss: 0.6323927640914917\n",
      "Epoch 1006, Loss: 2.7847535610198975, Final Batch Loss: 0.47174400091171265\n",
      "Epoch 1007, Loss: 2.7450999915599823, Final Batch Loss: 0.2607908546924591\n",
      "Epoch 1008, Loss: 3.224636495113373, Final Batch Loss: 0.5746812224388123\n",
      "Epoch 1009, Loss: 3.1578195691108704, Final Batch Loss: 0.742223858833313\n",
      "Epoch 1010, Loss: 3.1436962485313416, Final Batch Loss: 0.658053994178772\n",
      "Epoch 1011, Loss: 3.0717424750328064, Final Batch Loss: 0.6179419159889221\n",
      "Epoch 1012, Loss: 3.464207351207733, Final Batch Loss: 0.9858881831169128\n",
      "Epoch 1013, Loss: 3.03851181268692, Final Batch Loss: 0.5190597772598267\n",
      "Epoch 1014, Loss: 3.177018105983734, Final Batch Loss: 0.6937693357467651\n",
      "Epoch 1015, Loss: 3.335188627243042, Final Batch Loss: 0.9467889666557312\n",
      "Epoch 1016, Loss: 2.9357155561447144, Final Batch Loss: 0.32237815856933594\n",
      "Epoch 1017, Loss: 3.1603147983551025, Final Batch Loss: 0.7003533244132996\n",
      "Epoch 1018, Loss: 3.08799147605896, Final Batch Loss: 0.5834059715270996\n",
      "Epoch 1019, Loss: 3.060541868209839, Final Batch Loss: 0.5981950163841248\n",
      "Epoch 1020, Loss: 3.112812876701355, Final Batch Loss: 0.6089741587638855\n",
      "Epoch 1021, Loss: 3.3914738297462463, Final Batch Loss: 0.7740594148635864\n",
      "Epoch 1022, Loss: 3.578910768032074, Final Batch Loss: 0.9640818238258362\n",
      "Epoch 1023, Loss: 2.935368150472641, Final Batch Loss: 0.49476900696754456\n",
      "Epoch 1024, Loss: 2.8795718252658844, Final Batch Loss: 0.25340911746025085\n",
      "Epoch 1025, Loss: 2.987675905227661, Final Batch Loss: 0.5509224534034729\n",
      "Epoch 1026, Loss: 3.175622820854187, Final Batch Loss: 0.5000971555709839\n",
      "Epoch 1027, Loss: 2.795010596513748, Final Batch Loss: 0.38142678141593933\n",
      "Epoch 1028, Loss: 4.007052719593048, Final Batch Loss: 1.4938910007476807\n",
      "Epoch 1029, Loss: 3.057650148868561, Final Batch Loss: 0.5737469792366028\n",
      "Epoch 1030, Loss: 3.328127086162567, Final Batch Loss: 0.8319523930549622\n",
      "Epoch 1031, Loss: 3.267199695110321, Final Batch Loss: 0.7113803625106812\n",
      "Epoch 1032, Loss: 2.8944021463394165, Final Batch Loss: 0.4360043406486511\n",
      "Epoch 1033, Loss: 3.11879825592041, Final Batch Loss: 0.6727948188781738\n",
      "Epoch 1034, Loss: 2.975267469882965, Final Batch Loss: 0.6439911127090454\n",
      "Epoch 1035, Loss: 3.2940508127212524, Final Batch Loss: 0.8809935450553894\n",
      "Epoch 1036, Loss: 3.082999348640442, Final Batch Loss: 0.600601851940155\n",
      "Epoch 1037, Loss: 3.160934865474701, Final Batch Loss: 0.778586745262146\n",
      "Epoch 1038, Loss: 2.72919961810112, Final Batch Loss: 0.2793918550014496\n",
      "Epoch 1039, Loss: 3.4788357615470886, Final Batch Loss: 1.0056365728378296\n",
      "Epoch 1040, Loss: 3.029346227645874, Final Batch Loss: 0.5256868004798889\n",
      "Epoch 1041, Loss: 3.4847710728645325, Final Batch Loss: 0.9776934385299683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1042, Loss: 2.6620081961154938, Final Batch Loss: 0.21297290921211243\n",
      "Epoch 1043, Loss: 2.8256645500659943, Final Batch Loss: 0.3869730532169342\n",
      "Epoch 1044, Loss: 3.2135862708091736, Final Batch Loss: 0.7692801356315613\n",
      "Epoch 1045, Loss: 3.0323650240898132, Final Batch Loss: 0.5064999461174011\n",
      "Epoch 1046, Loss: 3.0773606300354004, Final Batch Loss: 0.6174759268760681\n",
      "Epoch 1047, Loss: 3.0842984318733215, Final Batch Loss: 0.5666951537132263\n",
      "Epoch 1048, Loss: 3.532983660697937, Final Batch Loss: 0.9225796461105347\n",
      "Epoch 1049, Loss: 3.0904229283332825, Final Batch Loss: 0.6249585747718811\n",
      "Epoch 1050, Loss: 2.777259588241577, Final Batch Loss: 0.3786713480949402\n",
      "Epoch 1051, Loss: 3.0583258867263794, Final Batch Loss: 0.6389416456222534\n",
      "Epoch 1052, Loss: 3.0618852376937866, Final Batch Loss: 0.5906369090080261\n",
      "Epoch 1053, Loss: 3.348967432975769, Final Batch Loss: 1.0671826601028442\n",
      "Epoch 1054, Loss: 2.7820172905921936, Final Batch Loss: 0.40673238039016724\n",
      "Epoch 1055, Loss: 3.3311245441436768, Final Batch Loss: 0.9035480618476868\n",
      "Epoch 1056, Loss: 3.300783336162567, Final Batch Loss: 0.6911629438400269\n",
      "Epoch 1057, Loss: 3.2831096053123474, Final Batch Loss: 0.8374330401420593\n",
      "Epoch 1058, Loss: 2.9814966022968292, Final Batch Loss: 0.637178897857666\n",
      "Epoch 1059, Loss: 2.9926385283470154, Final Batch Loss: 0.5859830975532532\n",
      "Epoch 1060, Loss: 2.920288860797882, Final Batch Loss: 0.5218436121940613\n",
      "Epoch 1061, Loss: 3.2753748297691345, Final Batch Loss: 0.756767213344574\n",
      "Epoch 1062, Loss: 2.9170802235603333, Final Batch Loss: 0.5111926198005676\n",
      "Epoch 1063, Loss: 3.101325809955597, Final Batch Loss: 0.7196372151374817\n",
      "Epoch 1064, Loss: 3.007876753807068, Final Batch Loss: 0.5825307965278625\n",
      "Epoch 1065, Loss: 2.837120682001114, Final Batch Loss: 0.3966129720211029\n",
      "Epoch 1066, Loss: 3.202140510082245, Final Batch Loss: 0.8223319053649902\n",
      "Epoch 1067, Loss: 2.7488591074943542, Final Batch Loss: 0.4860076904296875\n",
      "Epoch 1068, Loss: 2.6823607683181763, Final Batch Loss: 0.302176296710968\n",
      "Epoch 1069, Loss: 2.94672355055809, Final Batch Loss: 0.3934766352176666\n",
      "Epoch 1070, Loss: 3.1073803305625916, Final Batch Loss: 0.7809696793556213\n",
      "Epoch 1071, Loss: 3.1457343101501465, Final Batch Loss: 0.7620585560798645\n",
      "Epoch 1072, Loss: 2.769613206386566, Final Batch Loss: 0.405447781085968\n",
      "Epoch 1073, Loss: 2.864399492740631, Final Batch Loss: 0.41554808616638184\n",
      "Epoch 1074, Loss: 2.7636525332927704, Final Batch Loss: 0.3327384293079376\n",
      "Epoch 1075, Loss: 3.2968931198120117, Final Batch Loss: 0.9470915198326111\n",
      "Epoch 1076, Loss: 2.7523128390312195, Final Batch Loss: 0.3504447937011719\n",
      "Epoch 1077, Loss: 3.065243721008301, Final Batch Loss: 0.5936907529830933\n",
      "Epoch 1078, Loss: 3.3607820868492126, Final Batch Loss: 1.0063492059707642\n",
      "Epoch 1079, Loss: 3.224493622779846, Final Batch Loss: 0.7496610283851624\n",
      "Epoch 1080, Loss: 2.8642208576202393, Final Batch Loss: 0.5642897486686707\n",
      "Epoch 1081, Loss: 2.9484276175498962, Final Batch Loss: 0.6420268416404724\n",
      "Epoch 1082, Loss: 2.7940222918987274, Final Batch Loss: 0.3559764325618744\n",
      "Epoch 1083, Loss: 3.5414063930511475, Final Batch Loss: 1.1474345922470093\n",
      "Epoch 1084, Loss: 2.8967040479183197, Final Batch Loss: 0.47894713282585144\n",
      "Epoch 1085, Loss: 3.0920370221138, Final Batch Loss: 0.702801525592804\n",
      "Epoch 1086, Loss: 3.348233699798584, Final Batch Loss: 0.795319139957428\n",
      "Epoch 1087, Loss: 3.0812647342681885, Final Batch Loss: 0.5186168551445007\n",
      "Epoch 1088, Loss: 3.0456767082214355, Final Batch Loss: 0.538593590259552\n",
      "Epoch 1089, Loss: 3.0157402753829956, Final Batch Loss: 0.6231399774551392\n",
      "Epoch 1090, Loss: 2.877837121486664, Final Batch Loss: 0.5562238097190857\n",
      "Epoch 1091, Loss: 3.0125385522842407, Final Batch Loss: 0.6420949101448059\n",
      "Epoch 1092, Loss: 3.032924175262451, Final Batch Loss: 0.6152471303939819\n",
      "Epoch 1093, Loss: 3.522527813911438, Final Batch Loss: 1.0704048871994019\n",
      "Epoch 1094, Loss: 3.1628846526145935, Final Batch Loss: 0.845887303352356\n",
      "Epoch 1095, Loss: 2.872467279434204, Final Batch Loss: 0.529978334903717\n",
      "Epoch 1096, Loss: 3.3173393607139587, Final Batch Loss: 0.8801358938217163\n",
      "Epoch 1097, Loss: 2.9395831823349, Final Batch Loss: 0.5919983983039856\n",
      "Epoch 1098, Loss: 3.1836283802986145, Final Batch Loss: 0.8326899409294128\n",
      "Epoch 1099, Loss: 3.372168481349945, Final Batch Loss: 1.001671314239502\n",
      "Epoch 1100, Loss: 2.7677584290504456, Final Batch Loss: 0.4316937029361725\n",
      "Epoch 1101, Loss: 3.443467080593109, Final Batch Loss: 0.8963901400566101\n",
      "Epoch 1102, Loss: 2.869137406349182, Final Batch Loss: 0.44125932455062866\n",
      "Epoch 1103, Loss: 2.9818082749843597, Final Batch Loss: 0.4981529414653778\n",
      "Epoch 1104, Loss: 3.1133630573749542, Final Batch Loss: 0.8273375630378723\n",
      "Epoch 1105, Loss: 2.6748071014881134, Final Batch Loss: 0.3106885850429535\n",
      "Epoch 1106, Loss: 3.4270830154418945, Final Batch Loss: 1.078818440437317\n",
      "Epoch 1107, Loss: 2.9857805967330933, Final Batch Loss: 0.5285184979438782\n",
      "Epoch 1108, Loss: 3.040451169013977, Final Batch Loss: 0.6303879022598267\n",
      "Epoch 1109, Loss: 3.12641841173172, Final Batch Loss: 0.7962908148765564\n",
      "Epoch 1110, Loss: 2.9506773352622986, Final Batch Loss: 0.6275906562805176\n",
      "Epoch 1111, Loss: 3.0863633155822754, Final Batch Loss: 0.49494075775146484\n",
      "Epoch 1112, Loss: 2.6081285774707794, Final Batch Loss: 0.2208421528339386\n",
      "Epoch 1113, Loss: 3.022768259048462, Final Batch Loss: 0.5132156014442444\n",
      "Epoch 1114, Loss: 2.9483257830142975, Final Batch Loss: 0.660099446773529\n",
      "Epoch 1115, Loss: 3.1322394013404846, Final Batch Loss: 0.6176068186759949\n",
      "Epoch 1116, Loss: 3.001430332660675, Final Batch Loss: 0.6158283352851868\n",
      "Epoch 1117, Loss: 3.2535423636436462, Final Batch Loss: 0.961419403553009\n",
      "Epoch 1118, Loss: 2.7114957571029663, Final Batch Loss: 0.29190707206726074\n",
      "Epoch 1119, Loss: 3.1652772426605225, Final Batch Loss: 0.7754589319229126\n",
      "Epoch 1120, Loss: 2.7977451384067535, Final Batch Loss: 0.378513902425766\n",
      "Epoch 1121, Loss: 2.683523654937744, Final Batch Loss: 0.3483174443244934\n",
      "Epoch 1122, Loss: 3.093379259109497, Final Batch Loss: 0.7232456803321838\n",
      "Epoch 1123, Loss: 2.6598518788814545, Final Batch Loss: 0.2667764723300934\n",
      "Epoch 1124, Loss: 3.737505167722702, Final Batch Loss: 1.5134668350219727\n",
      "Epoch 1125, Loss: 3.170009732246399, Final Batch Loss: 0.8065716028213501\n",
      "Epoch 1126, Loss: 3.2668763995170593, Final Batch Loss: 0.9154618382453918\n",
      "Epoch 1127, Loss: 3.0171207785606384, Final Batch Loss: 0.605635941028595\n",
      "Epoch 1128, Loss: 2.74254509806633, Final Batch Loss: 0.2511410415172577\n",
      "Epoch 1129, Loss: 2.8851792216300964, Final Batch Loss: 0.5273082852363586\n",
      "Epoch 1130, Loss: 2.8365090787410736, Final Batch Loss: 0.46166130900382996\n",
      "Epoch 1131, Loss: 3.1508907079696655, Final Batch Loss: 0.7419670820236206\n",
      "Epoch 1132, Loss: 3.1221638917922974, Final Batch Loss: 0.7327983975410461\n",
      "Epoch 1133, Loss: 3.2353387475013733, Final Batch Loss: 0.7326418161392212\n",
      "Epoch 1134, Loss: 2.579089432954788, Final Batch Loss: 0.3136748969554901\n",
      "Epoch 1135, Loss: 3.1244728565216064, Final Batch Loss: 0.7777525782585144\n",
      "Epoch 1136, Loss: 3.095621645450592, Final Batch Loss: 0.7726512551307678\n",
      "Epoch 1137, Loss: 2.6279647052288055, Final Batch Loss: 0.33461132645606995\n",
      "Epoch 1138, Loss: 3.031258463859558, Final Batch Loss: 0.7015885710716248\n",
      "Epoch 1139, Loss: 2.778235375881195, Final Batch Loss: 0.5412672162055969\n",
      "Epoch 1140, Loss: 3.195521354675293, Final Batch Loss: 0.7210292816162109\n",
      "Epoch 1141, Loss: 3.403727173805237, Final Batch Loss: 0.9926994442939758\n",
      "Epoch 1142, Loss: 2.9682379364967346, Final Batch Loss: 0.5603808164596558\n",
      "Epoch 1143, Loss: 2.8293261229991913, Final Batch Loss: 0.3826250433921814\n",
      "Epoch 1144, Loss: 3.307664930820465, Final Batch Loss: 1.0210477113723755\n",
      "Epoch 1145, Loss: 2.889518439769745, Final Batch Loss: 0.5356429815292358\n",
      "Epoch 1146, Loss: 2.8413748145103455, Final Batch Loss: 0.444228857755661\n",
      "Epoch 1147, Loss: 3.7145720720291138, Final Batch Loss: 1.4428508281707764\n",
      "Epoch 1148, Loss: 2.904770076274872, Final Batch Loss: 0.6171494126319885\n",
      "Epoch 1149, Loss: 2.5909657180309296, Final Batch Loss: 0.1903780996799469\n",
      "Epoch 1150, Loss: 2.836803138256073, Final Batch Loss: 0.5125331878662109\n",
      "Epoch 1151, Loss: 3.087418019771576, Final Batch Loss: 0.8029815554618835\n",
      "Epoch 1152, Loss: 3.1134530305862427, Final Batch Loss: 0.6954333186149597\n",
      "Epoch 1153, Loss: 2.89095538854599, Final Batch Loss: 0.5131502747535706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1154, Loss: 2.9855674505233765, Final Batch Loss: 0.5608118176460266\n",
      "Epoch 1155, Loss: 3.2300496697425842, Final Batch Loss: 0.9023270010948181\n",
      "Epoch 1156, Loss: 2.8696924448013306, Final Batch Loss: 0.5814128518104553\n",
      "Epoch 1157, Loss: 2.8375181555747986, Final Batch Loss: 0.6299611926078796\n",
      "Epoch 1158, Loss: 2.7415904700756073, Final Batch Loss: 0.3753298223018646\n",
      "Epoch 1159, Loss: 3.3190845251083374, Final Batch Loss: 0.9646862149238586\n",
      "Epoch 1160, Loss: 3.3784740567207336, Final Batch Loss: 1.0570989847183228\n",
      "Epoch 1161, Loss: 2.9208691120147705, Final Batch Loss: 0.37902265787124634\n",
      "Epoch 1162, Loss: 2.5589442253112793, Final Batch Loss: 0.26530396938323975\n",
      "Epoch 1163, Loss: 3.2297877073287964, Final Batch Loss: 0.8218773603439331\n",
      "Epoch 1164, Loss: 2.9110403656959534, Final Batch Loss: 0.5889650583267212\n",
      "Epoch 1165, Loss: 2.943133234977722, Final Batch Loss: 0.5712196826934814\n",
      "Epoch 1166, Loss: 2.747560977935791, Final Batch Loss: 0.5155674815177917\n",
      "Epoch 1167, Loss: 2.6016702950000763, Final Batch Loss: 0.2532716691493988\n",
      "Epoch 1168, Loss: 2.770882695913315, Final Batch Loss: 0.33152636885643005\n",
      "Epoch 1169, Loss: 2.9650376439094543, Final Batch Loss: 0.7099606394767761\n",
      "Epoch 1170, Loss: 2.633185923099518, Final Batch Loss: 0.44376325607299805\n",
      "Epoch 1171, Loss: 2.840924918651581, Final Batch Loss: 0.5349419713020325\n",
      "Epoch 1172, Loss: 2.828098952770233, Final Batch Loss: 0.5055884718894958\n",
      "Epoch 1173, Loss: 2.6986199617385864, Final Batch Loss: 0.3301844000816345\n",
      "Epoch 1174, Loss: 2.684478908777237, Final Batch Loss: 0.417376846075058\n",
      "Epoch 1175, Loss: 2.8386447429656982, Final Batch Loss: 0.6714553833007812\n",
      "Epoch 1176, Loss: 2.568452924489975, Final Batch Loss: 0.29051586985588074\n",
      "Epoch 1177, Loss: 2.8450346291065216, Final Batch Loss: 0.5933877229690552\n",
      "Epoch 1178, Loss: 2.77875155210495, Final Batch Loss: 0.5768895745277405\n",
      "Epoch 1179, Loss: 3.4483311772346497, Final Batch Loss: 1.1891180276870728\n",
      "Epoch 1180, Loss: 3.1383138298988342, Final Batch Loss: 0.8445113897323608\n",
      "Epoch 1181, Loss: 3.0830593705177307, Final Batch Loss: 0.8072947263717651\n",
      "Epoch 1182, Loss: 2.91013902425766, Final Batch Loss: 0.5332592725753784\n",
      "Epoch 1183, Loss: 2.794022262096405, Final Batch Loss: 0.45554250478744507\n",
      "Epoch 1184, Loss: 2.8509462773799896, Final Batch Loss: 0.6008750200271606\n",
      "Epoch 1185, Loss: 3.086580455303192, Final Batch Loss: 0.7440885901451111\n",
      "Epoch 1186, Loss: 3.1791500449180603, Final Batch Loss: 0.8686025738716125\n",
      "Epoch 1187, Loss: 2.97988498210907, Final Batch Loss: 0.6797005534172058\n",
      "Epoch 1188, Loss: 2.7918154895305634, Final Batch Loss: 0.3193919360637665\n",
      "Epoch 1189, Loss: 3.135445386171341, Final Batch Loss: 0.8282220959663391\n",
      "Epoch 1190, Loss: 2.6780038475990295, Final Batch Loss: 0.49454575777053833\n",
      "Epoch 1191, Loss: 2.598819375038147, Final Batch Loss: 0.3090199828147888\n",
      "Epoch 1192, Loss: 2.7054536640644073, Final Batch Loss: 0.45863935351371765\n",
      "Epoch 1193, Loss: 2.9734466671943665, Final Batch Loss: 0.6346296072006226\n",
      "Epoch 1194, Loss: 3.238361895084381, Final Batch Loss: 0.9586941003799438\n",
      "Epoch 1195, Loss: 2.861852318048477, Final Batch Loss: 0.47219720482826233\n",
      "Epoch 1196, Loss: 2.766832172870636, Final Batch Loss: 0.6084215044975281\n",
      "Epoch 1197, Loss: 2.639051139354706, Final Batch Loss: 0.44404786825180054\n",
      "Epoch 1198, Loss: 3.0856873989105225, Final Batch Loss: 0.7978087663650513\n",
      "Epoch 1199, Loss: 2.4119465202093124, Final Batch Loss: 0.2186381071805954\n",
      "Epoch 1200, Loss: 2.8736685514450073, Final Batch Loss: 0.6494352221488953\n",
      "Epoch 1201, Loss: 3.029848337173462, Final Batch Loss: 0.8912016153335571\n",
      "Epoch 1202, Loss: 3.109037458896637, Final Batch Loss: 0.6880685687065125\n",
      "Epoch 1203, Loss: 2.9394351840019226, Final Batch Loss: 0.6410232782363892\n",
      "Epoch 1204, Loss: 3.1259565353393555, Final Batch Loss: 0.790727436542511\n",
      "Epoch 1205, Loss: 2.869358718395233, Final Batch Loss: 0.49321532249450684\n",
      "Epoch 1206, Loss: 3.1203104853630066, Final Batch Loss: 0.795040488243103\n",
      "Epoch 1207, Loss: 2.721966117620468, Final Batch Loss: 0.40304264426231384\n",
      "Epoch 1208, Loss: 2.970845341682434, Final Batch Loss: 0.5452788472175598\n",
      "Epoch 1209, Loss: 2.837149918079376, Final Batch Loss: 0.5800206065177917\n",
      "Epoch 1210, Loss: 2.660444498062134, Final Batch Loss: 0.37262412905693054\n",
      "Epoch 1211, Loss: 2.4866054505109787, Final Batch Loss: 0.21836434304714203\n",
      "Epoch 1212, Loss: 2.71610626578331, Final Batch Loss: 0.32660773396492004\n",
      "Epoch 1213, Loss: 2.8760029673576355, Final Batch Loss: 0.6937306523323059\n",
      "Epoch 1214, Loss: 2.814063310623169, Final Batch Loss: 0.4021678566932678\n",
      "Epoch 1215, Loss: 2.522408038377762, Final Batch Loss: 0.16519960761070251\n",
      "Epoch 1216, Loss: 2.563072055578232, Final Batch Loss: 0.31325098872184753\n",
      "Epoch 1217, Loss: 2.6847020983695984, Final Batch Loss: 0.40810680389404297\n",
      "Epoch 1218, Loss: 2.6914797723293304, Final Batch Loss: 0.6036761403083801\n",
      "Epoch 1219, Loss: 3.115989089012146, Final Batch Loss: 0.9062901735305786\n",
      "Epoch 1220, Loss: 3.2873586118221283, Final Batch Loss: 1.0645968914031982\n",
      "Epoch 1221, Loss: 3.330061435699463, Final Batch Loss: 0.9935015439987183\n",
      "Epoch 1222, Loss: 3.366656720638275, Final Batch Loss: 0.962036669254303\n",
      "Epoch 1223, Loss: 2.973103642463684, Final Batch Loss: 0.6305780410766602\n",
      "Epoch 1224, Loss: 2.5057359635829926, Final Batch Loss: 0.2755748927593231\n",
      "Epoch 1225, Loss: 2.8324660658836365, Final Batch Loss: 0.5732355713844299\n",
      "Epoch 1226, Loss: 2.93340927362442, Final Batch Loss: 0.7245402932167053\n",
      "Epoch 1227, Loss: 2.6859597265720367, Final Batch Loss: 0.4487104117870331\n",
      "Epoch 1228, Loss: 3.1763522624969482, Final Batch Loss: 0.8388473391532898\n",
      "Epoch 1229, Loss: 3.06722629070282, Final Batch Loss: 0.6832770109176636\n",
      "Epoch 1230, Loss: 2.686492681503296, Final Batch Loss: 0.46902886033058167\n",
      "Epoch 1231, Loss: 3.123449385166168, Final Batch Loss: 0.8850991129875183\n",
      "Epoch 1232, Loss: 2.9587348103523254, Final Batch Loss: 0.5913235545158386\n",
      "Epoch 1233, Loss: 3.0752174258232117, Final Batch Loss: 0.8171637654304504\n",
      "Epoch 1234, Loss: 2.6630744636058807, Final Batch Loss: 0.44548967480659485\n",
      "Epoch 1235, Loss: 3.2211645245552063, Final Batch Loss: 0.9612550735473633\n",
      "Epoch 1236, Loss: 2.677662491798401, Final Batch Loss: 0.4564945101737976\n",
      "Epoch 1237, Loss: 2.5633536875247955, Final Batch Loss: 0.333341509103775\n",
      "Epoch 1238, Loss: 3.2673211097717285, Final Batch Loss: 1.0844591856002808\n",
      "Epoch 1239, Loss: 2.712157726287842, Final Batch Loss: 0.5519765019416809\n",
      "Epoch 1240, Loss: 2.7996506690979004, Final Batch Loss: 0.6091955304145813\n",
      "Epoch 1241, Loss: 2.747788190841675, Final Batch Loss: 0.5314720273017883\n",
      "Epoch 1242, Loss: 2.9586256742477417, Final Batch Loss: 0.7127947211265564\n",
      "Epoch 1243, Loss: 2.62183478474617, Final Batch Loss: 0.37403348088264465\n",
      "Epoch 1244, Loss: 3.0085766315460205, Final Batch Loss: 0.8290249705314636\n",
      "Epoch 1245, Loss: 2.824232518672943, Final Batch Loss: 0.6283078789710999\n",
      "Epoch 1246, Loss: 2.5479484498500824, Final Batch Loss: 0.3130009174346924\n",
      "Epoch 1247, Loss: 2.448434740304947, Final Batch Loss: 0.279874712228775\n",
      "Epoch 1248, Loss: 2.781162351369858, Final Batch Loss: 0.42305788397789\n",
      "Epoch 1249, Loss: 2.6429337859153748, Final Batch Loss: 0.3529370427131653\n",
      "Epoch 1250, Loss: 2.5548788607120514, Final Batch Loss: 0.44150951504707336\n",
      "Epoch 1251, Loss: 2.7891445755958557, Final Batch Loss: 0.4232521057128906\n",
      "Epoch 1252, Loss: 2.6293524503707886, Final Batch Loss: 0.378692626953125\n",
      "Epoch 1253, Loss: 2.9514376521110535, Final Batch Loss: 0.7213364839553833\n",
      "Epoch 1254, Loss: 2.602476477622986, Final Batch Loss: 0.34996265172958374\n",
      "Epoch 1255, Loss: 2.84494411945343, Final Batch Loss: 0.6588335633277893\n",
      "Epoch 1256, Loss: 2.7027373611927032, Final Batch Loss: 0.42309895157814026\n",
      "Epoch 1257, Loss: 2.6143331825733185, Final Batch Loss: 0.36068421602249146\n",
      "Epoch 1258, Loss: 3.1347447335720062, Final Batch Loss: 0.903071939945221\n",
      "Epoch 1259, Loss: 2.576200008392334, Final Batch Loss: 0.4137353301048279\n",
      "Epoch 1260, Loss: 2.7055346965789795, Final Batch Loss: 0.47585529088974\n",
      "Epoch 1261, Loss: 2.4471383094787598, Final Batch Loss: 0.2397371232509613\n",
      "Epoch 1262, Loss: 3.1284640431404114, Final Batch Loss: 0.9591664671897888\n",
      "Epoch 1263, Loss: 2.593859374523163, Final Batch Loss: 0.5146227478981018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1264, Loss: 2.5040415227413177, Final Batch Loss: 0.37774038314819336\n",
      "Epoch 1265, Loss: 2.5681983828544617, Final Batch Loss: 0.3662770390510559\n",
      "Epoch 1266, Loss: 3.1570438146591187, Final Batch Loss: 1.0443581342697144\n",
      "Epoch 1267, Loss: 2.776915490627289, Final Batch Loss: 0.44861191511154175\n",
      "Epoch 1268, Loss: 2.7581529021263123, Final Batch Loss: 0.5088708996772766\n",
      "Epoch 1269, Loss: 2.8369770646095276, Final Batch Loss: 0.5474634170532227\n",
      "Epoch 1270, Loss: 2.4229461699724197, Final Batch Loss: 0.21419115364551544\n",
      "Epoch 1271, Loss: 2.99090376496315, Final Batch Loss: 0.7843261361122131\n",
      "Epoch 1272, Loss: 2.647695094347, Final Batch Loss: 0.43645504117012024\n",
      "Epoch 1273, Loss: 2.85003525018692, Final Batch Loss: 0.4724969267845154\n",
      "Epoch 1274, Loss: 2.6955322325229645, Final Batch Loss: 0.4320984184741974\n",
      "Epoch 1275, Loss: 3.1258986592292786, Final Batch Loss: 0.9041799306869507\n",
      "Epoch 1276, Loss: 2.681748330593109, Final Batch Loss: 0.469473659992218\n",
      "Epoch 1277, Loss: 3.2086786031723022, Final Batch Loss: 0.9112294316291809\n",
      "Epoch 1278, Loss: 2.5386672019958496, Final Batch Loss: 0.3019940257072449\n",
      "Epoch 1279, Loss: 2.8839403986930847, Final Batch Loss: 0.7017854452133179\n",
      "Epoch 1280, Loss: 2.6386578977108, Final Batch Loss: 0.5089199542999268\n",
      "Epoch 1281, Loss: 2.7418392300605774, Final Batch Loss: 0.3902166485786438\n",
      "Epoch 1282, Loss: 2.476421445608139, Final Batch Loss: 0.3437379002571106\n",
      "Epoch 1283, Loss: 2.74143847823143, Final Batch Loss: 0.5737709999084473\n",
      "Epoch 1284, Loss: 2.520433694124222, Final Batch Loss: 0.35937103629112244\n",
      "Epoch 1285, Loss: 2.678161770105362, Final Batch Loss: 0.4565194547176361\n",
      "Epoch 1286, Loss: 2.852233111858368, Final Batch Loss: 0.719600260257721\n",
      "Epoch 1287, Loss: 2.857170283794403, Final Batch Loss: 0.6927757859230042\n",
      "Epoch 1288, Loss: 2.484634816646576, Final Batch Loss: 0.33561161160469055\n",
      "Epoch 1289, Loss: 2.6858910620212555, Final Batch Loss: 0.6485538482666016\n",
      "Epoch 1290, Loss: 2.319544091820717, Final Batch Loss: 0.22045646607875824\n",
      "Epoch 1291, Loss: 3.552750825881958, Final Batch Loss: 1.3063161373138428\n",
      "Epoch 1292, Loss: 2.8064510226249695, Final Batch Loss: 0.6523554921150208\n",
      "Epoch 1293, Loss: 2.7416365146636963, Final Batch Loss: 0.4137518107891083\n",
      "Epoch 1294, Loss: 2.6185878217220306, Final Batch Loss: 0.47101035714149475\n",
      "Epoch 1295, Loss: 3.1305628418922424, Final Batch Loss: 1.0652223825454712\n",
      "Epoch 1296, Loss: 2.6215710639953613, Final Batch Loss: 0.3832727074623108\n",
      "Epoch 1297, Loss: 2.709605485200882, Final Batch Loss: 0.5642658472061157\n",
      "Epoch 1298, Loss: 2.7614359259605408, Final Batch Loss: 0.5320371389389038\n",
      "Epoch 1299, Loss: 2.7779287099838257, Final Batch Loss: 0.5686748623847961\n",
      "Epoch 1300, Loss: 2.601009875535965, Final Batch Loss: 0.4420657455921173\n",
      "Epoch 1301, Loss: 2.454628348350525, Final Batch Loss: 0.33590149879455566\n",
      "Epoch 1302, Loss: 2.8516833782196045, Final Batch Loss: 0.8109502792358398\n",
      "Epoch 1303, Loss: 2.5162546932697296, Final Batch Loss: 0.33562591671943665\n",
      "Epoch 1304, Loss: 2.667548179626465, Final Batch Loss: 0.4480612277984619\n",
      "Epoch 1305, Loss: 2.625663787126541, Final Batch Loss: 0.3673919141292572\n",
      "Epoch 1306, Loss: 2.297587603330612, Final Batch Loss: 0.13490083813667297\n",
      "Epoch 1307, Loss: 2.5976259410381317, Final Batch Loss: 0.3802565932273865\n",
      "Epoch 1308, Loss: 2.660757750272751, Final Batch Loss: 0.599449634552002\n",
      "Epoch 1309, Loss: 2.4646816849708557, Final Batch Loss: 0.2970784306526184\n",
      "Epoch 1310, Loss: 2.7942294478416443, Final Batch Loss: 0.5896315574645996\n",
      "Epoch 1311, Loss: 2.517293632030487, Final Batch Loss: 0.2515625059604645\n",
      "Epoch 1312, Loss: 2.500675916671753, Final Batch Loss: 0.30908507108688354\n",
      "Epoch 1313, Loss: 2.8259454667568207, Final Batch Loss: 0.5973070859909058\n",
      "Epoch 1314, Loss: 2.366805136203766, Final Batch Loss: 0.297024667263031\n",
      "Epoch 1315, Loss: 2.4454892575740814, Final Batch Loss: 0.38088861107826233\n",
      "Epoch 1316, Loss: 3.1927667558193207, Final Batch Loss: 1.0086126327514648\n",
      "Epoch 1317, Loss: 2.7785429656505585, Final Batch Loss: 0.6448138356208801\n",
      "Epoch 1318, Loss: 2.694251000881195, Final Batch Loss: 0.501754641532898\n",
      "Epoch 1319, Loss: 2.813883602619171, Final Batch Loss: 0.7430442571640015\n",
      "Epoch 1320, Loss: 3.144286811351776, Final Batch Loss: 0.9281967878341675\n",
      "Epoch 1321, Loss: 2.546349048614502, Final Batch Loss: 0.3629886209964752\n",
      "Epoch 1322, Loss: 2.557374984025955, Final Batch Loss: 0.3502442538738251\n",
      "Epoch 1323, Loss: 3.214739739894867, Final Batch Loss: 1.0287975072860718\n",
      "Epoch 1324, Loss: 2.7387003898620605, Final Batch Loss: 0.5311798453330994\n",
      "Epoch 1325, Loss: 2.352534234523773, Final Batch Loss: 0.21194696426391602\n",
      "Epoch 1326, Loss: 2.3025138527154922, Final Batch Loss: 0.08449284732341766\n",
      "Epoch 1327, Loss: 2.5924257338047028, Final Batch Loss: 0.4130493104457855\n",
      "Epoch 1328, Loss: 2.7354235351085663, Final Batch Loss: 0.6699981093406677\n",
      "Epoch 1329, Loss: 2.4652699530124664, Final Batch Loss: 0.37848153710365295\n",
      "Epoch 1330, Loss: 2.4085992872714996, Final Batch Loss: 0.26679790019989014\n",
      "Epoch 1331, Loss: 2.5780216455459595, Final Batch Loss: 0.40082260966300964\n",
      "Epoch 1332, Loss: 2.6405862867832184, Final Batch Loss: 0.48640456795692444\n",
      "Epoch 1333, Loss: 2.703850358724594, Final Batch Loss: 0.5903546214103699\n",
      "Epoch 1334, Loss: 2.697964668273926, Final Batch Loss: 0.5053969621658325\n",
      "Epoch 1335, Loss: 2.699720412492752, Final Batch Loss: 0.5720333456993103\n",
      "Epoch 1336, Loss: 2.455037087202072, Final Batch Loss: 0.31979185342788696\n",
      "Epoch 1337, Loss: 2.7507849037647247, Final Batch Loss: 0.6201137900352478\n",
      "Epoch 1338, Loss: 2.560010403394699, Final Batch Loss: 0.37690529227256775\n",
      "Epoch 1339, Loss: 3.3664371073246, Final Batch Loss: 1.2977266311645508\n",
      "Epoch 1340, Loss: 2.7409392595291138, Final Batch Loss: 0.5515986084938049\n",
      "Epoch 1341, Loss: 2.3718056082725525, Final Batch Loss: 0.23165860772132874\n",
      "Epoch 1342, Loss: 2.3809354305267334, Final Batch Loss: 0.3306659758090973\n",
      "Epoch 1343, Loss: 2.7106655836105347, Final Batch Loss: 0.6118215918540955\n",
      "Epoch 1344, Loss: 3.083744704723358, Final Batch Loss: 0.8031011819839478\n",
      "Epoch 1345, Loss: 2.9893140494823456, Final Batch Loss: 0.8648630976676941\n",
      "Epoch 1346, Loss: 2.2923165559768677, Final Batch Loss: 0.15732824802398682\n",
      "Epoch 1347, Loss: 2.707820236682892, Final Batch Loss: 0.5772671103477478\n",
      "Epoch 1348, Loss: 2.543682277202606, Final Batch Loss: 0.3594181537628174\n",
      "Epoch 1349, Loss: 2.54338002204895, Final Batch Loss: 0.5288995504379272\n",
      "Epoch 1350, Loss: 2.5494337677955627, Final Batch Loss: 0.3851906657218933\n",
      "Epoch 1351, Loss: 2.726271092891693, Final Batch Loss: 0.5523458123207092\n",
      "Epoch 1352, Loss: 2.443011999130249, Final Batch Loss: 0.27293962240219116\n",
      "Epoch 1353, Loss: 2.7178930938243866, Final Batch Loss: 0.7708457112312317\n",
      "Epoch 1354, Loss: 2.757479816675186, Final Batch Loss: 0.6457557678222656\n",
      "Epoch 1355, Loss: 2.4977678656578064, Final Batch Loss: 0.35904330015182495\n",
      "Epoch 1356, Loss: 2.765944182872772, Final Batch Loss: 0.6093297004699707\n",
      "Epoch 1357, Loss: 2.790308654308319, Final Batch Loss: 0.5646700859069824\n",
      "Epoch 1358, Loss: 3.1627140045166016, Final Batch Loss: 0.9904475212097168\n",
      "Epoch 1359, Loss: 2.879868358373642, Final Batch Loss: 0.6797869801521301\n",
      "Epoch 1360, Loss: 2.4238805174827576, Final Batch Loss: 0.3036324977874756\n",
      "Epoch 1361, Loss: 2.753491073846817, Final Batch Loss: 0.7079070210456848\n",
      "Epoch 1362, Loss: 2.2833488285541534, Final Batch Loss: 0.25731655955314636\n",
      "Epoch 1363, Loss: 2.615969240665436, Final Batch Loss: 0.45859840512275696\n",
      "Epoch 1364, Loss: 2.537069708108902, Final Batch Loss: 0.457105427980423\n",
      "Epoch 1365, Loss: 2.671301454305649, Final Batch Loss: 0.525479793548584\n",
      "Epoch 1366, Loss: 2.216539613902569, Final Batch Loss: 0.08229260891675949\n",
      "Epoch 1367, Loss: 2.7590815126895905, Final Batch Loss: 0.630994975566864\n",
      "Epoch 1368, Loss: 2.4859562516212463, Final Batch Loss: 0.279457151889801\n",
      "Epoch 1369, Loss: 2.495286703109741, Final Batch Loss: 0.3482322096824646\n",
      "Epoch 1370, Loss: 2.5920729637145996, Final Batch Loss: 0.4923311769962311\n",
      "Epoch 1371, Loss: 2.456507056951523, Final Batch Loss: 0.2605244219303131\n",
      "Epoch 1372, Loss: 2.6751464903354645, Final Batch Loss: 0.4771418869495392\n",
      "Epoch 1373, Loss: 2.755414992570877, Final Batch Loss: 0.7148157954216003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1374, Loss: 2.676491230726242, Final Batch Loss: 0.5950098037719727\n",
      "Epoch 1375, Loss: 2.544954478740692, Final Batch Loss: 0.4606887400150299\n",
      "Epoch 1376, Loss: 2.7102443277835846, Final Batch Loss: 0.6226475834846497\n",
      "Epoch 1377, Loss: 3.050024092197418, Final Batch Loss: 0.9869090914726257\n",
      "Epoch 1378, Loss: 2.4845909476280212, Final Batch Loss: 0.45452743768692017\n",
      "Epoch 1379, Loss: 2.7564807534217834, Final Batch Loss: 0.5971169471740723\n",
      "Epoch 1380, Loss: 2.8556411266326904, Final Batch Loss: 0.8487973809242249\n",
      "Epoch 1381, Loss: 3.7045509815216064, Final Batch Loss: 1.3964669704437256\n",
      "Epoch 1382, Loss: 2.345635309815407, Final Batch Loss: 0.15174327790737152\n",
      "Epoch 1383, Loss: 2.810625195503235, Final Batch Loss: 0.6611502766609192\n",
      "Epoch 1384, Loss: 2.458479329943657, Final Batch Loss: 0.1951904147863388\n",
      "Epoch 1385, Loss: 2.602659434080124, Final Batch Loss: 0.4130711555480957\n",
      "Epoch 1386, Loss: 2.41238334774971, Final Batch Loss: 0.30176353454589844\n",
      "Epoch 1387, Loss: 2.515062630176544, Final Batch Loss: 0.4320147633552551\n",
      "Epoch 1388, Loss: 2.676591634750366, Final Batch Loss: 0.5674520134925842\n",
      "Epoch 1389, Loss: 2.8838749825954437, Final Batch Loss: 0.886421263217926\n",
      "Epoch 1390, Loss: 2.4008705765008926, Final Batch Loss: 0.24956150352954865\n",
      "Epoch 1391, Loss: 2.6027889251708984, Final Batch Loss: 0.44224458932876587\n",
      "Epoch 1392, Loss: 2.802154839038849, Final Batch Loss: 0.7778080701828003\n",
      "Epoch 1393, Loss: 2.761881113052368, Final Batch Loss: 0.6410627961158752\n",
      "Epoch 1394, Loss: 2.491679310798645, Final Batch Loss: 0.4685755670070648\n",
      "Epoch 1395, Loss: 2.856611520051956, Final Batch Loss: 0.6872925162315369\n",
      "Epoch 1396, Loss: 2.8404842615127563, Final Batch Loss: 0.6736793518066406\n",
      "Epoch 1397, Loss: 2.7120732367038727, Final Batch Loss: 0.4076671898365021\n",
      "Epoch 1398, Loss: 2.6936436891555786, Final Batch Loss: 0.5255564451217651\n",
      "Epoch 1399, Loss: 2.5433493852615356, Final Batch Loss: 0.44910115003585815\n",
      "Epoch 1400, Loss: 2.7007408440113068, Final Batch Loss: 0.7308082580566406\n",
      "Epoch 1401, Loss: 2.287358731031418, Final Batch Loss: 0.17389866709709167\n",
      "Epoch 1402, Loss: 3.097490131855011, Final Batch Loss: 0.9778180718421936\n",
      "Epoch 1403, Loss: 2.505238175392151, Final Batch Loss: 0.4439336359500885\n",
      "Epoch 1404, Loss: 2.524400532245636, Final Batch Loss: 0.43919363617897034\n",
      "Epoch 1405, Loss: 2.5677568912506104, Final Batch Loss: 0.44144225120544434\n",
      "Epoch 1406, Loss: 2.7443209886550903, Final Batch Loss: 0.648091197013855\n",
      "Epoch 1407, Loss: 2.5251773595809937, Final Batch Loss: 0.42142558097839355\n",
      "Epoch 1408, Loss: 2.443723976612091, Final Batch Loss: 0.3199039399623871\n",
      "Epoch 1409, Loss: 2.3892918527126312, Final Batch Loss: 0.38230180740356445\n",
      "Epoch 1410, Loss: 2.6014215648174286, Final Batch Loss: 0.543485164642334\n",
      "Epoch 1411, Loss: 2.9535903930664062, Final Batch Loss: 0.953239917755127\n",
      "Epoch 1412, Loss: 2.3247097730636597, Final Batch Loss: 0.3099437654018402\n",
      "Epoch 1413, Loss: 2.991354525089264, Final Batch Loss: 0.7836641073226929\n",
      "Epoch 1414, Loss: 2.538791298866272, Final Batch Loss: 0.45490917563438416\n",
      "Epoch 1415, Loss: 2.941264033317566, Final Batch Loss: 0.7497793436050415\n",
      "Epoch 1416, Loss: 2.740590214729309, Final Batch Loss: 0.702241063117981\n",
      "Epoch 1417, Loss: 2.603994071483612, Final Batch Loss: 0.4357757568359375\n",
      "Epoch 1418, Loss: 2.609924852848053, Final Batch Loss: 0.48416584730148315\n",
      "Epoch 1419, Loss: 3.2599213123321533, Final Batch Loss: 1.062178611755371\n",
      "Epoch 1420, Loss: 2.68206587433815, Final Batch Loss: 0.42375198006629944\n",
      "Epoch 1421, Loss: 2.8543333411216736, Final Batch Loss: 0.5949140787124634\n",
      "Epoch 1422, Loss: 2.905960440635681, Final Batch Loss: 0.7984467148780823\n",
      "Epoch 1423, Loss: 2.5468137562274933, Final Batch Loss: 0.4510003626346588\n",
      "Epoch 1424, Loss: 2.5981397330760956, Final Batch Loss: 0.5512054562568665\n",
      "Epoch 1425, Loss: 2.699797809123993, Final Batch Loss: 0.6464229822158813\n",
      "Epoch 1426, Loss: 2.3470929712057114, Final Batch Loss: 0.24943740665912628\n",
      "Epoch 1427, Loss: 2.399636387825012, Final Batch Loss: 0.39917483925819397\n",
      "Epoch 1428, Loss: 2.426852762699127, Final Batch Loss: 0.44013136625289917\n",
      "Epoch 1429, Loss: 2.365200102329254, Final Batch Loss: 0.3468606472015381\n",
      "Epoch 1430, Loss: 2.8277319371700287, Final Batch Loss: 0.7533577680587769\n",
      "Epoch 1431, Loss: 2.477644920349121, Final Batch Loss: 0.4449109733104706\n",
      "Epoch 1432, Loss: 2.52350652217865, Final Batch Loss: 0.4761691987514496\n",
      "Epoch 1433, Loss: 2.490147113800049, Final Batch Loss: 0.38557153940200806\n",
      "Epoch 1434, Loss: 2.471356362104416, Final Batch Loss: 0.36154404282569885\n",
      "Epoch 1435, Loss: 2.669109046459198, Final Batch Loss: 0.5475202798843384\n",
      "Epoch 1436, Loss: 2.448779970407486, Final Batch Loss: 0.5068252682685852\n",
      "Epoch 1437, Loss: 2.3520072996616364, Final Batch Loss: 0.38513872027397156\n",
      "Epoch 1438, Loss: 2.995943009853363, Final Batch Loss: 0.6910295486450195\n",
      "Epoch 1439, Loss: 2.4424860179424286, Final Batch Loss: 0.33719953894615173\n",
      "Epoch 1440, Loss: 3.0205856561660767, Final Batch Loss: 1.0779461860656738\n",
      "Epoch 1441, Loss: 2.622152030467987, Final Batch Loss: 0.4690372347831726\n",
      "Epoch 1442, Loss: 2.6631666123867035, Final Batch Loss: 0.4328707754611969\n",
      "Epoch 1443, Loss: 2.768731325864792, Final Batch Loss: 0.5278105735778809\n",
      "Epoch 1444, Loss: 2.6602530479431152, Final Batch Loss: 0.506908655166626\n",
      "Epoch 1445, Loss: 2.6418028473854065, Final Batch Loss: 0.4927811622619629\n",
      "Epoch 1446, Loss: 2.94392067193985, Final Batch Loss: 0.6949537396430969\n",
      "Epoch 1447, Loss: 2.418991655111313, Final Batch Loss: 0.3035706579685211\n",
      "Epoch 1448, Loss: 2.6278321146965027, Final Batch Loss: 0.5067862868309021\n",
      "Epoch 1449, Loss: 2.580562472343445, Final Batch Loss: 0.528370201587677\n",
      "Epoch 1450, Loss: 2.3188915848731995, Final Batch Loss: 0.18128812313079834\n",
      "Epoch 1451, Loss: 2.9675468504428864, Final Batch Loss: 0.91443932056427\n",
      "Epoch 1452, Loss: 2.458535224199295, Final Batch Loss: 0.4160478413105011\n",
      "Epoch 1453, Loss: 2.7703036665916443, Final Batch Loss: 0.6016256213188171\n",
      "Epoch 1454, Loss: 2.3646527230739594, Final Batch Loss: 0.4754710793495178\n",
      "Epoch 1455, Loss: 2.863165706396103, Final Batch Loss: 0.837427020072937\n",
      "Epoch 1456, Loss: 2.4074332416057587, Final Batch Loss: 0.3895643353462219\n",
      "Epoch 1457, Loss: 2.801538050174713, Final Batch Loss: 0.6310097575187683\n",
      "Epoch 1458, Loss: 2.40883269906044, Final Batch Loss: 0.4246354401111603\n",
      "Epoch 1459, Loss: 2.4768856167793274, Final Batch Loss: 0.5619863867759705\n",
      "Epoch 1460, Loss: 2.6041422188282013, Final Batch Loss: 0.49263516068458557\n",
      "Epoch 1461, Loss: 2.1821901202201843, Final Batch Loss: 0.16516873240470886\n",
      "Epoch 1462, Loss: 2.2770223915576935, Final Batch Loss: 0.24624863266944885\n",
      "Epoch 1463, Loss: 2.5274930596351624, Final Batch Loss: 0.42200690507888794\n",
      "Epoch 1464, Loss: 2.540951520204544, Final Batch Loss: 0.46290332078933716\n",
      "Epoch 1465, Loss: 2.651694178581238, Final Batch Loss: 0.5183503031730652\n",
      "Epoch 1466, Loss: 2.16281221807003, Final Batch Loss: 0.16310925781726837\n",
      "Epoch 1467, Loss: 2.59384948015213, Final Batch Loss: 0.5431801080703735\n",
      "Epoch 1468, Loss: 2.5998984575271606, Final Batch Loss: 0.529434859752655\n",
      "Epoch 1469, Loss: 2.427466332912445, Final Batch Loss: 0.358458936214447\n",
      "Epoch 1470, Loss: 2.396859645843506, Final Batch Loss: 0.43980997800827026\n",
      "Epoch 1471, Loss: 2.5437839031219482, Final Batch Loss: 0.46443885564804077\n",
      "Epoch 1472, Loss: 2.7067232728004456, Final Batch Loss: 0.6683121919631958\n",
      "Epoch 1473, Loss: 2.3124476075172424, Final Batch Loss: 0.36831119656562805\n",
      "Epoch 1474, Loss: 2.526541143655777, Final Batch Loss: 0.4278602600097656\n",
      "Epoch 1475, Loss: 2.210221529006958, Final Batch Loss: 0.19458520412445068\n",
      "Epoch 1476, Loss: 2.5642494559288025, Final Batch Loss: 0.5430269241333008\n",
      "Epoch 1477, Loss: 2.3049791157245636, Final Batch Loss: 0.29421982169151306\n",
      "Epoch 1478, Loss: 2.4379830062389374, Final Batch Loss: 0.4044000208377838\n",
      "Epoch 1479, Loss: 2.4159847795963287, Final Batch Loss: 0.3788747787475586\n",
      "Epoch 1480, Loss: 2.3417906165122986, Final Batch Loss: 0.30552372336387634\n",
      "Epoch 1481, Loss: 3.1687761545181274, Final Batch Loss: 1.1346464157104492\n",
      "Epoch 1482, Loss: 2.3131096959114075, Final Batch Loss: 0.3169730603694916\n",
      "Epoch 1483, Loss: 2.2427932769060135, Final Batch Loss: 0.2021745890378952\n",
      "Epoch 1484, Loss: 2.553386777639389, Final Batch Loss: 0.482759028673172\n",
      "Epoch 1485, Loss: 2.524227350950241, Final Batch Loss: 0.5848148465156555\n",
      "Epoch 1486, Loss: 2.673821121454239, Final Batch Loss: 0.5714265704154968\n",
      "Epoch 1487, Loss: 2.3218841552734375, Final Batch Loss: 0.26182645559310913\n",
      "Epoch 1488, Loss: 2.2043336778879166, Final Batch Loss: 0.14483647048473358\n",
      "Epoch 1489, Loss: 2.522229939699173, Final Batch Loss: 0.4261125922203064\n",
      "Epoch 1490, Loss: 2.4273992776870728, Final Batch Loss: 0.3858586847782135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1491, Loss: 2.6702761352062225, Final Batch Loss: 0.691491425037384\n",
      "Epoch 1492, Loss: 2.5934222638607025, Final Batch Loss: 0.4877692759037018\n",
      "Epoch 1493, Loss: 2.6362257599830627, Final Batch Loss: 0.6422598958015442\n",
      "Epoch 1494, Loss: 2.7207189202308655, Final Batch Loss: 0.7145991921424866\n",
      "Epoch 1495, Loss: 2.8882191479206085, Final Batch Loss: 0.9323150515556335\n",
      "Epoch 1496, Loss: 2.792224556207657, Final Batch Loss: 0.8230805397033691\n",
      "Epoch 1497, Loss: 2.8203614950180054, Final Batch Loss: 0.6584148406982422\n",
      "Epoch 1498, Loss: 2.470856696367264, Final Batch Loss: 0.29127925634384155\n",
      "Epoch 1499, Loss: 2.337303549051285, Final Batch Loss: 0.3231295943260193\n",
      "Epoch 1500, Loss: 2.725342273712158, Final Batch Loss: 0.7686367034912109\n",
      "Epoch 1501, Loss: 2.336157649755478, Final Batch Loss: 0.2799047529697418\n",
      "Epoch 1502, Loss: 2.5958683490753174, Final Batch Loss: 0.5887660384178162\n",
      "Epoch 1503, Loss: 2.65233650803566, Final Batch Loss: 0.611995279788971\n",
      "Epoch 1504, Loss: 2.5206578075885773, Final Batch Loss: 0.5564616918563843\n",
      "Epoch 1505, Loss: 2.712795317173004, Final Batch Loss: 0.5735264420509338\n",
      "Epoch 1506, Loss: 2.5616234838962555, Final Batch Loss: 0.5145832300186157\n",
      "Epoch 1507, Loss: 2.411411315202713, Final Batch Loss: 0.39997607469558716\n",
      "Epoch 1508, Loss: 2.4633016288280487, Final Batch Loss: 0.4376382529735565\n",
      "Epoch 1509, Loss: 2.736769288778305, Final Batch Loss: 0.804176926612854\n",
      "Epoch 1510, Loss: 2.3233856558799744, Final Batch Loss: 0.34577712416648865\n",
      "Epoch 1511, Loss: 2.3790377974510193, Final Batch Loss: 0.25617966055870056\n",
      "Epoch 1512, Loss: 2.4418948888778687, Final Batch Loss: 0.3786827623844147\n",
      "Epoch 1513, Loss: 3.260433793067932, Final Batch Loss: 1.1956465244293213\n",
      "Epoch 1514, Loss: 2.461157888174057, Final Batch Loss: 0.46961861848831177\n",
      "Epoch 1515, Loss: 2.7240676283836365, Final Batch Loss: 0.6199925541877747\n",
      "Epoch 1516, Loss: 2.522048741579056, Final Batch Loss: 0.5029113292694092\n",
      "Epoch 1517, Loss: 2.687635123729706, Final Batch Loss: 0.593610942363739\n",
      "Epoch 1518, Loss: 2.4636828899383545, Final Batch Loss: 0.4180469810962677\n",
      "Epoch 1519, Loss: 2.5022612512111664, Final Batch Loss: 0.5906714200973511\n",
      "Epoch 1520, Loss: 2.594185471534729, Final Batch Loss: 0.522584080696106\n",
      "Epoch 1521, Loss: 2.7882470190525055, Final Batch Loss: 0.8104478120803833\n",
      "Epoch 1522, Loss: 2.29008886218071, Final Batch Loss: 0.1823733150959015\n",
      "Epoch 1523, Loss: 2.4788580238819122, Final Batch Loss: 0.38787341117858887\n",
      "Epoch 1524, Loss: 2.5542737245559692, Final Batch Loss: 0.5417103171348572\n",
      "Epoch 1525, Loss: 2.4343106746673584, Final Batch Loss: 0.5098236799240112\n",
      "Epoch 1526, Loss: 2.562507152557373, Final Batch Loss: 0.5534312129020691\n",
      "Epoch 1527, Loss: 2.32429638504982, Final Batch Loss: 0.4422227740287781\n",
      "Epoch 1528, Loss: 2.311014175415039, Final Batch Loss: 0.42838430404663086\n",
      "Epoch 1529, Loss: 2.0996114760637283, Final Batch Loss: 0.15980203449726105\n",
      "Epoch 1530, Loss: 2.3370721638202667, Final Batch Loss: 0.26515504717826843\n",
      "Epoch 1531, Loss: 2.411803424358368, Final Batch Loss: 0.4473241865634918\n",
      "Epoch 1532, Loss: 2.3499675989151, Final Batch Loss: 0.28250783681869507\n",
      "Epoch 1533, Loss: 2.455344945192337, Final Batch Loss: 0.5804585218429565\n",
      "Epoch 1534, Loss: 2.3072772324085236, Final Batch Loss: 0.26742854714393616\n",
      "Epoch 1535, Loss: 2.267335206270218, Final Batch Loss: 0.22541630268096924\n",
      "Epoch 1536, Loss: 2.5496238470077515, Final Batch Loss: 0.5793663859367371\n",
      "Epoch 1537, Loss: 2.4218342900276184, Final Batch Loss: 0.5879223942756653\n",
      "Epoch 1538, Loss: 2.5073873698711395, Final Batch Loss: 0.5821664929389954\n",
      "Epoch 1539, Loss: 2.2358847707509995, Final Batch Loss: 0.1552029699087143\n",
      "Epoch 1540, Loss: 2.631367027759552, Final Batch Loss: 0.6308550834655762\n",
      "Epoch 1541, Loss: 2.195903033018112, Final Batch Loss: 0.15674540400505066\n",
      "Epoch 1542, Loss: 2.4846515357494354, Final Batch Loss: 0.5769796371459961\n",
      "Epoch 1543, Loss: 2.6687703132629395, Final Batch Loss: 0.6888177990913391\n",
      "Epoch 1544, Loss: 2.628897786140442, Final Batch Loss: 0.5912546515464783\n",
      "Epoch 1545, Loss: 2.2091160267591476, Final Batch Loss: 0.20808257162570953\n",
      "Epoch 1546, Loss: 2.32574862241745, Final Batch Loss: 0.35921764373779297\n",
      "Epoch 1547, Loss: 2.9951731860637665, Final Batch Loss: 1.0277734994888306\n",
      "Epoch 1548, Loss: 2.4416322708129883, Final Batch Loss: 0.552182137966156\n",
      "Epoch 1549, Loss: 2.7134354412555695, Final Batch Loss: 0.6811414957046509\n",
      "Epoch 1550, Loss: 2.340646058320999, Final Batch Loss: 0.37269312143325806\n",
      "Epoch 1551, Loss: 2.487827718257904, Final Batch Loss: 0.469645231962204\n",
      "Epoch 1552, Loss: 2.1319962963461876, Final Batch Loss: 0.11940493434667587\n",
      "Epoch 1553, Loss: 2.2184469997882843, Final Batch Loss: 0.2815304100513458\n",
      "Epoch 1554, Loss: 2.283556640148163, Final Batch Loss: 0.3152044415473938\n",
      "Epoch 1555, Loss: 2.6482549607753754, Final Batch Loss: 0.7590108513832092\n",
      "Epoch 1556, Loss: 2.5617860853672028, Final Batch Loss: 0.5860214233398438\n",
      "Epoch 1557, Loss: 2.4931373596191406, Final Batch Loss: 0.48887357115745544\n",
      "Epoch 1558, Loss: 2.5710970759391785, Final Batch Loss: 0.4871840476989746\n",
      "Epoch 1559, Loss: 2.5936810672283173, Final Batch Loss: 0.4301806390285492\n",
      "Epoch 1560, Loss: 2.2612310647964478, Final Batch Loss: 0.24219545722007751\n",
      "Epoch 1561, Loss: 2.5494685769081116, Final Batch Loss: 0.4842766225337982\n",
      "Epoch 1562, Loss: 2.3575049936771393, Final Batch Loss: 0.3830123543739319\n",
      "Epoch 1563, Loss: 2.290875166654587, Final Batch Loss: 0.19012686610221863\n",
      "Epoch 1564, Loss: 2.5124417543411255, Final Batch Loss: 0.42653733491897583\n",
      "Epoch 1565, Loss: 2.5729607939720154, Final Batch Loss: 0.659540593624115\n",
      "Epoch 1566, Loss: 2.4602168798446655, Final Batch Loss: 0.5012387037277222\n",
      "Epoch 1567, Loss: 2.256038933992386, Final Batch Loss: 0.3611825406551361\n",
      "Epoch 1568, Loss: 2.311236172914505, Final Batch Loss: 0.36817386746406555\n",
      "Epoch 1569, Loss: 2.302392303943634, Final Batch Loss: 0.26246052980422974\n",
      "Epoch 1570, Loss: 2.066633090376854, Final Batch Loss: 0.13122959434986115\n",
      "Epoch 1571, Loss: 2.4111572802066803, Final Batch Loss: 0.42017433047294617\n",
      "Epoch 1572, Loss: 2.2864755392074585, Final Batch Loss: 0.2709786891937256\n",
      "Epoch 1573, Loss: 2.4332335889339447, Final Batch Loss: 0.5205605626106262\n",
      "Epoch 1574, Loss: 2.46965628862381, Final Batch Loss: 0.4493412673473358\n",
      "Epoch 1575, Loss: 2.549330413341522, Final Batch Loss: 0.6505872011184692\n",
      "Epoch 1576, Loss: 2.353344142436981, Final Batch Loss: 0.42398253083229065\n",
      "Epoch 1577, Loss: 2.295643299818039, Final Batch Loss: 0.3112986087799072\n",
      "Epoch 1578, Loss: 2.4549192786216736, Final Batch Loss: 0.46953874826431274\n",
      "Epoch 1579, Loss: 2.311775267124176, Final Batch Loss: 0.29364943504333496\n",
      "Epoch 1580, Loss: 2.4588148295879364, Final Batch Loss: 0.4474688172340393\n",
      "Epoch 1581, Loss: 2.0945640429854393, Final Batch Loss: 0.11538853496313095\n",
      "Epoch 1582, Loss: 2.5134847164154053, Final Batch Loss: 0.40121859312057495\n",
      "Epoch 1583, Loss: 2.466725528240204, Final Batch Loss: 0.4959871768951416\n",
      "Epoch 1584, Loss: 2.350837141275406, Final Batch Loss: 0.4718243181705475\n",
      "Epoch 1585, Loss: 2.1297895312309265, Final Batch Loss: 0.27921807765960693\n",
      "Epoch 1586, Loss: 2.478524327278137, Final Batch Loss: 0.4275915324687958\n",
      "Epoch 1587, Loss: 2.6427466571331024, Final Batch Loss: 0.6102746725082397\n",
      "Epoch 1588, Loss: 2.3176289498806, Final Batch Loss: 0.3745318353176117\n",
      "Epoch 1589, Loss: 2.3227696418762207, Final Batch Loss: 0.3139512240886688\n",
      "Epoch 1590, Loss: 2.547370493412018, Final Batch Loss: 0.6047215461730957\n",
      "Epoch 1591, Loss: 2.5076824426651, Final Batch Loss: 0.512466311454773\n",
      "Epoch 1592, Loss: 2.3705500662326813, Final Batch Loss: 0.43090176582336426\n",
      "Epoch 1593, Loss: 2.4916833639144897, Final Batch Loss: 0.5257563591003418\n",
      "Epoch 1594, Loss: 2.6558324098587036, Final Batch Loss: 0.46713152527809143\n",
      "Epoch 1595, Loss: 2.3185082972049713, Final Batch Loss: 0.27372220158576965\n",
      "Epoch 1596, Loss: 2.3976809084415436, Final Batch Loss: 0.45118099451065063\n",
      "Epoch 1597, Loss: 2.455500990152359, Final Batch Loss: 0.5334427356719971\n",
      "Epoch 1598, Loss: 2.7392234206199646, Final Batch Loss: 0.6616724729537964\n",
      "Epoch 1599, Loss: 1.989828921854496, Final Batch Loss: 0.09084267169237137\n",
      "Epoch 1600, Loss: 2.475358545780182, Final Batch Loss: 0.5140135884284973\n",
      "Epoch 1601, Loss: 2.354752480983734, Final Batch Loss: 0.5161250829696655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1602, Loss: 2.4034528136253357, Final Batch Loss: 0.4584888815879822\n",
      "Epoch 1603, Loss: 2.5050907731056213, Final Batch Loss: 0.6442188620567322\n",
      "Epoch 1604, Loss: 2.8405532836914062, Final Batch Loss: 0.8595673441886902\n",
      "Epoch 1605, Loss: 2.4156374335289, Final Batch Loss: 0.4327041506767273\n",
      "Epoch 1606, Loss: 2.3903927505016327, Final Batch Loss: 0.5089300870895386\n",
      "Epoch 1607, Loss: 2.31122624874115, Final Batch Loss: 0.38601064682006836\n",
      "Epoch 1608, Loss: 2.273770183324814, Final Batch Loss: 0.29054728150367737\n",
      "Epoch 1609, Loss: 2.012451335787773, Final Batch Loss: 0.07758991420269012\n",
      "Epoch 1610, Loss: 2.3749831318855286, Final Batch Loss: 0.42287200689315796\n",
      "Epoch 1611, Loss: 2.4566935896873474, Final Batch Loss: 0.5440369248390198\n",
      "Epoch 1612, Loss: 2.6402996480464935, Final Batch Loss: 0.7004519104957581\n",
      "Epoch 1613, Loss: 2.3898304104804993, Final Batch Loss: 0.40677446126937866\n",
      "Epoch 1614, Loss: 2.6598819494247437, Final Batch Loss: 0.7050215601921082\n",
      "Epoch 1615, Loss: 2.906091094017029, Final Batch Loss: 0.8372663855552673\n",
      "Epoch 1616, Loss: 2.753887176513672, Final Batch Loss: 0.7035993337631226\n",
      "Epoch 1617, Loss: 2.689452350139618, Final Batch Loss: 0.5966241955757141\n",
      "Epoch 1618, Loss: 2.357111483812332, Final Batch Loss: 0.40574216842651367\n",
      "Epoch 1619, Loss: 2.8526939749717712, Final Batch Loss: 0.6615745425224304\n",
      "Epoch 1620, Loss: 2.4071439504623413, Final Batch Loss: 0.5187600255012512\n",
      "Epoch 1621, Loss: 2.461602061986923, Final Batch Loss: 0.5130115747451782\n",
      "Epoch 1622, Loss: 2.3275869488716125, Final Batch Loss: 0.42401525378227234\n",
      "Epoch 1623, Loss: 2.6556241512298584, Final Batch Loss: 0.7630694508552551\n",
      "Epoch 1624, Loss: 2.48820760846138, Final Batch Loss: 0.5424689650535583\n",
      "Epoch 1625, Loss: 2.3484403789043427, Final Batch Loss: 0.4138386845588684\n",
      "Epoch 1626, Loss: 2.3222907185554504, Final Batch Loss: 0.44005969166755676\n",
      "Epoch 1627, Loss: 2.4738244712352753, Final Batch Loss: 0.5984147191047668\n",
      "Epoch 1628, Loss: 2.376449167728424, Final Batch Loss: 0.47618362307548523\n",
      "Epoch 1629, Loss: 2.444376587867737, Final Batch Loss: 0.3900730311870575\n",
      "Epoch 1630, Loss: 2.7154841125011444, Final Batch Loss: 0.7609097361564636\n",
      "Epoch 1631, Loss: 2.6162863075733185, Final Batch Loss: 0.6561838388442993\n",
      "Epoch 1632, Loss: 2.557138830423355, Final Batch Loss: 0.5954226851463318\n",
      "Epoch 1633, Loss: 2.3140573501586914, Final Batch Loss: 0.3038542866706848\n",
      "Epoch 1634, Loss: 2.2644364833831787, Final Batch Loss: 0.45782962441444397\n",
      "Epoch 1635, Loss: 2.4519025683403015, Final Batch Loss: 0.4740867018699646\n",
      "Epoch 1636, Loss: 2.701192557811737, Final Batch Loss: 0.6537520289421082\n",
      "Epoch 1637, Loss: 2.7165293097496033, Final Batch Loss: 0.7333155870437622\n",
      "Epoch 1638, Loss: 2.26264026761055, Final Batch Loss: 0.33830758929252625\n",
      "Epoch 1639, Loss: 2.6627275347709656, Final Batch Loss: 0.765251636505127\n",
      "Epoch 1640, Loss: 2.512473702430725, Final Batch Loss: 0.7017472386360168\n",
      "Epoch 1641, Loss: 2.3132641911506653, Final Batch Loss: 0.38403287529945374\n",
      "Epoch 1642, Loss: 2.3392809629440308, Final Batch Loss: 0.34625187516212463\n",
      "Epoch 1643, Loss: 2.2716365456581116, Final Batch Loss: 0.3959028422832489\n",
      "Epoch 1644, Loss: 2.3786700963974, Final Batch Loss: 0.4218992292881012\n",
      "Epoch 1645, Loss: 2.7474555671215057, Final Batch Loss: 0.9046909213066101\n",
      "Epoch 1646, Loss: 2.2945208847522736, Final Batch Loss: 0.4977755546569824\n",
      "Epoch 1647, Loss: 2.5456517040729523, Final Batch Loss: 0.4747641682624817\n",
      "Epoch 1648, Loss: 2.3052766919136047, Final Batch Loss: 0.4048306345939636\n",
      "Epoch 1649, Loss: 2.661925435066223, Final Batch Loss: 0.7461022138595581\n",
      "Epoch 1650, Loss: 2.866367995738983, Final Batch Loss: 0.7518560290336609\n",
      "Epoch 1651, Loss: 2.8496690690517426, Final Batch Loss: 0.6778931021690369\n",
      "Epoch 1652, Loss: 2.8391128182411194, Final Batch Loss: 0.7494999766349792\n",
      "Epoch 1653, Loss: 2.39983993768692, Final Batch Loss: 0.3994147777557373\n",
      "Epoch 1654, Loss: 2.4554628431797028, Final Batch Loss: 0.4957396388053894\n",
      "Epoch 1655, Loss: 2.739707797765732, Final Batch Loss: 0.7484635710716248\n",
      "Epoch 1656, Loss: 2.220440596342087, Final Batch Loss: 0.27518612146377563\n",
      "Epoch 1657, Loss: 2.472440242767334, Final Batch Loss: 0.5317317247390747\n",
      "Epoch 1658, Loss: 2.8266972601413727, Final Batch Loss: 0.9879352450370789\n",
      "Epoch 1659, Loss: 2.5454478561878204, Final Batch Loss: 0.5875580906867981\n",
      "Epoch 1660, Loss: 2.6080963015556335, Final Batch Loss: 0.6249180436134338\n",
      "Epoch 1661, Loss: 2.6132797598838806, Final Batch Loss: 0.6323975324630737\n",
      "Epoch 1662, Loss: 2.580317348241806, Final Batch Loss: 0.5597010254859924\n",
      "Epoch 1663, Loss: 2.3081102073192596, Final Batch Loss: 0.35787394642829895\n",
      "Epoch 1664, Loss: 2.2012560069561005, Final Batch Loss: 0.3082885444164276\n",
      "Epoch 1665, Loss: 2.883461683988571, Final Batch Loss: 0.7700991034507751\n",
      "Epoch 1666, Loss: 2.491428017616272, Final Batch Loss: 0.6513064503669739\n",
      "Epoch 1667, Loss: 2.369918614625931, Final Batch Loss: 0.3958098888397217\n",
      "Epoch 1668, Loss: 2.14568755030632, Final Batch Loss: 0.25509998202323914\n",
      "Epoch 1669, Loss: 2.6687643229961395, Final Batch Loss: 0.6787673234939575\n",
      "Epoch 1670, Loss: 2.499525785446167, Final Batch Loss: 0.4730806052684784\n",
      "Epoch 1671, Loss: 2.357998937368393, Final Batch Loss: 0.43939924240112305\n",
      "Epoch 1672, Loss: 2.2689294815063477, Final Batch Loss: 0.30565598607063293\n",
      "Epoch 1673, Loss: 2.4424996376037598, Final Batch Loss: 0.45822909474372864\n",
      "Epoch 1674, Loss: 2.3879739940166473, Final Batch Loss: 0.4439335763454437\n",
      "Epoch 1675, Loss: 2.542458266019821, Final Batch Loss: 0.5551255345344543\n",
      "Epoch 1676, Loss: 2.3362798988819122, Final Batch Loss: 0.3935934603214264\n",
      "Epoch 1677, Loss: 2.245089054107666, Final Batch Loss: 0.436237633228302\n",
      "Epoch 1678, Loss: 2.4842631220817566, Final Batch Loss: 0.5476137399673462\n",
      "Epoch 1679, Loss: 2.133297249674797, Final Batch Loss: 0.2071303278207779\n",
      "Epoch 1680, Loss: 2.2300668954849243, Final Batch Loss: 0.3141372501850128\n",
      "Epoch 1681, Loss: 2.4140850007534027, Final Batch Loss: 0.5556820631027222\n",
      "Epoch 1682, Loss: 2.4037588834762573, Final Batch Loss: 0.5247917771339417\n",
      "Epoch 1683, Loss: 2.211952954530716, Final Batch Loss: 0.2943871319293976\n",
      "Epoch 1684, Loss: 2.167806997895241, Final Batch Loss: 0.2311113327741623\n",
      "Epoch 1685, Loss: 2.382323831319809, Final Batch Loss: 0.4566691517829895\n",
      "Epoch 1686, Loss: 2.3924387097358704, Final Batch Loss: 0.4194563031196594\n",
      "Epoch 1687, Loss: 2.4374714493751526, Final Batch Loss: 0.6851905584335327\n",
      "Epoch 1688, Loss: 2.226409912109375, Final Batch Loss: 0.333519846200943\n",
      "Epoch 1689, Loss: 2.191039651632309, Final Batch Loss: 0.45553308725357056\n",
      "Epoch 1690, Loss: 2.1788091361522675, Final Batch Loss: 0.3554079830646515\n",
      "Epoch 1691, Loss: 2.4153482019901276, Final Batch Loss: 0.5841699242591858\n",
      "Epoch 1692, Loss: 2.258111983537674, Final Batch Loss: 0.40639734268188477\n",
      "Epoch 1693, Loss: 2.2619340419769287, Final Batch Loss: 0.3660944402217865\n",
      "Epoch 1694, Loss: 2.1340916752815247, Final Batch Loss: 0.2598339021205902\n",
      "Epoch 1695, Loss: 2.1853092312812805, Final Batch Loss: 0.2991493046283722\n",
      "Epoch 1696, Loss: 2.389553815126419, Final Batch Loss: 0.4192987382411957\n",
      "Epoch 1697, Loss: 2.6871648728847504, Final Batch Loss: 0.6429861783981323\n",
      "Epoch 1698, Loss: 2.6994489133358, Final Batch Loss: 0.7448987364768982\n",
      "Epoch 1699, Loss: 2.4343391358852386, Final Batch Loss: 0.47449204325675964\n",
      "Epoch 1700, Loss: 2.172391504049301, Final Batch Loss: 0.2934304177761078\n",
      "Epoch 1701, Loss: 2.627001941204071, Final Batch Loss: 0.6636604070663452\n",
      "Epoch 1702, Loss: 2.64360710978508, Final Batch Loss: 0.7128394246101379\n",
      "Epoch 1703, Loss: 2.098142370581627, Final Batch Loss: 0.19210003316402435\n",
      "Epoch 1704, Loss: 2.4024688005447388, Final Batch Loss: 0.4975067973136902\n",
      "Epoch 1705, Loss: 2.278439998626709, Final Batch Loss: 0.19735416769981384\n",
      "Epoch 1706, Loss: 2.4214082956314087, Final Batch Loss: 0.5286591053009033\n",
      "Epoch 1707, Loss: 2.320948153734207, Final Batch Loss: 0.4130534827709198\n",
      "Epoch 1708, Loss: 2.33787938952446, Final Batch Loss: 0.37296780943870544\n",
      "Epoch 1709, Loss: 2.206100821495056, Final Batch Loss: 0.3744142949581146\n",
      "Epoch 1710, Loss: 2.1855470836162567, Final Batch Loss: 0.21209731698036194\n",
      "Epoch 1711, Loss: 2.5913560688495636, Final Batch Loss: 0.6662955284118652\n",
      "Epoch 1712, Loss: 2.329054594039917, Final Batch Loss: 0.4938885271549225\n",
      "Epoch 1713, Loss: 2.482611268758774, Final Batch Loss: 0.6131988167762756\n",
      "Epoch 1714, Loss: 2.874963164329529, Final Batch Loss: 1.0303181409835815\n",
      "Epoch 1715, Loss: 2.1226810291409492, Final Batch Loss: 0.12322816997766495\n",
      "Epoch 1716, Loss: 2.604142665863037, Final Batch Loss: 0.5966336131095886\n",
      "Epoch 1717, Loss: 2.335486739873886, Final Batch Loss: 0.47085028886795044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1718, Loss: 2.2578163146972656, Final Batch Loss: 0.3435191512107849\n",
      "Epoch 1719, Loss: 2.2233801782131195, Final Batch Loss: 0.3050099015235901\n",
      "Epoch 1720, Loss: 2.115330845117569, Final Batch Loss: 0.3449226915836334\n",
      "Epoch 1721, Loss: 2.293949633836746, Final Batch Loss: 0.44223424792289734\n",
      "Epoch 1722, Loss: 1.9749472439289093, Final Batch Loss: 0.21052077412605286\n",
      "Epoch 1723, Loss: 2.7035659551620483, Final Batch Loss: 0.7542632818222046\n",
      "Epoch 1724, Loss: 2.55084890127182, Final Batch Loss: 0.7656283974647522\n",
      "Epoch 1725, Loss: 2.484149694442749, Final Batch Loss: 0.6257988214492798\n",
      "Epoch 1726, Loss: 2.5184419453144073, Final Batch Loss: 0.6211133003234863\n",
      "Epoch 1727, Loss: 2.012462481856346, Final Batch Loss: 0.2115679532289505\n",
      "Epoch 1728, Loss: 2.2963375449180603, Final Batch Loss: 0.40187567472457886\n",
      "Epoch 1729, Loss: 2.114516243338585, Final Batch Loss: 0.22536064684391022\n",
      "Epoch 1730, Loss: 2.9355777502059937, Final Batch Loss: 1.0744378566741943\n",
      "Epoch 1731, Loss: 2.418986737728119, Final Batch Loss: 0.4472563564777374\n",
      "Epoch 1732, Loss: 2.268142431974411, Final Batch Loss: 0.38967207074165344\n",
      "Epoch 1733, Loss: 2.107964336872101, Final Batch Loss: 0.2627002000808716\n",
      "Epoch 1734, Loss: 2.5819468200206757, Final Batch Loss: 0.4601477086544037\n",
      "Epoch 1735, Loss: 2.518974930047989, Final Batch Loss: 0.5994071960449219\n",
      "Epoch 1736, Loss: 2.4324109256267548, Final Batch Loss: 0.5390638113021851\n",
      "Epoch 1737, Loss: 2.682862877845764, Final Batch Loss: 0.8165654540061951\n",
      "Epoch 1738, Loss: 2.297222524881363, Final Batch Loss: 0.32456016540527344\n",
      "Epoch 1739, Loss: 2.4642860293388367, Final Batch Loss: 0.5646266341209412\n",
      "Epoch 1740, Loss: 2.4993076026439667, Final Batch Loss: 0.557658851146698\n",
      "Epoch 1741, Loss: 2.4814987182617188, Final Batch Loss: 0.49373218417167664\n",
      "Epoch 1742, Loss: 2.763897955417633, Final Batch Loss: 0.8604416251182556\n",
      "Epoch 1743, Loss: 2.4518543481826782, Final Batch Loss: 0.5287098288536072\n",
      "Epoch 1744, Loss: 2.3748573660850525, Final Batch Loss: 0.275448739528656\n",
      "Epoch 1745, Loss: 2.1176734268665314, Final Batch Loss: 0.29113492369651794\n",
      "Epoch 1746, Loss: 2.362604856491089, Final Batch Loss: 0.44473177194595337\n",
      "Epoch 1747, Loss: 2.2250341176986694, Final Batch Loss: 0.271877646446228\n",
      "Epoch 1748, Loss: 2.234954595565796, Final Batch Loss: 0.31418201327323914\n",
      "Epoch 1749, Loss: 2.2773360908031464, Final Batch Loss: 0.4383563995361328\n",
      "Epoch 1750, Loss: 2.2525475323200226, Final Batch Loss: 0.3972194492816925\n",
      "Epoch 1751, Loss: 2.158421963453293, Final Batch Loss: 0.3716379702091217\n",
      "Epoch 1752, Loss: 2.6108241975307465, Final Batch Loss: 0.8722097277641296\n",
      "Epoch 1753, Loss: 2.3416444063186646, Final Batch Loss: 0.4975276291370392\n",
      "Epoch 1754, Loss: 2.131217360496521, Final Batch Loss: 0.25599929690361023\n",
      "Epoch 1755, Loss: 2.237107664346695, Final Batch Loss: 0.4584020972251892\n",
      "Epoch 1756, Loss: 2.359100967645645, Final Batch Loss: 0.5208978056907654\n",
      "Epoch 1757, Loss: 2.2847147285938263, Final Batch Loss: 0.38913995027542114\n",
      "Epoch 1758, Loss: 2.431084632873535, Final Batch Loss: 0.5876268148422241\n",
      "Epoch 1759, Loss: 2.592159688472748, Final Batch Loss: 0.7642334699630737\n",
      "Epoch 1760, Loss: 2.236703187227249, Final Batch Loss: 0.31734561920166016\n",
      "Epoch 1761, Loss: 2.3633393943309784, Final Batch Loss: 0.4504394233226776\n",
      "Epoch 1762, Loss: 2.08419406414032, Final Batch Loss: 0.20429649949073792\n",
      "Epoch 1763, Loss: 2.4045810401439667, Final Batch Loss: 0.5656250715255737\n",
      "Epoch 1764, Loss: 2.508595257997513, Final Batch Loss: 0.6670562624931335\n",
      "Epoch 1765, Loss: 1.9760271161794662, Final Batch Loss: 0.17369802296161652\n",
      "Epoch 1766, Loss: 2.139022320508957, Final Batch Loss: 0.300769180059433\n",
      "Epoch 1767, Loss: 2.2785170674324036, Final Batch Loss: 0.458096981048584\n",
      "Epoch 1768, Loss: 2.185357838869095, Final Batch Loss: 0.31867384910583496\n",
      "Epoch 1769, Loss: 2.0845388621091843, Final Batch Loss: 0.2028736025094986\n",
      "Epoch 1770, Loss: 2.236228585243225, Final Batch Loss: 0.33904173970222473\n",
      "Epoch 1771, Loss: 2.2986398339271545, Final Batch Loss: 0.4883882403373718\n",
      "Epoch 1772, Loss: 2.5084598660469055, Final Batch Loss: 0.671815812587738\n",
      "Epoch 1773, Loss: 2.1270231008529663, Final Batch Loss: 0.3741165101528168\n",
      "Epoch 1774, Loss: 2.306360572576523, Final Batch Loss: 0.4921775758266449\n",
      "Epoch 1775, Loss: 2.261717438697815, Final Batch Loss: 0.32310816645622253\n",
      "Epoch 1776, Loss: 2.455996334552765, Final Batch Loss: 0.5611886382102966\n",
      "Epoch 1777, Loss: 2.279173970222473, Final Batch Loss: 0.40310606360435486\n",
      "Epoch 1778, Loss: 2.0214932411909103, Final Batch Loss: 0.16147302091121674\n",
      "Epoch 1779, Loss: 2.271958976984024, Final Batch Loss: 0.35172441601753235\n",
      "Epoch 1780, Loss: 2.0817302465438843, Final Batch Loss: 0.2085343599319458\n",
      "Epoch 1781, Loss: 2.584974318742752, Final Batch Loss: 0.6219330430030823\n",
      "Epoch 1782, Loss: 2.9159408807754517, Final Batch Loss: 1.0686261653900146\n",
      "Epoch 1783, Loss: 2.425731211900711, Final Batch Loss: 0.35813072323799133\n",
      "Epoch 1784, Loss: 2.473963290452957, Final Batch Loss: 0.5805105566978455\n",
      "Epoch 1785, Loss: 2.288453996181488, Final Batch Loss: 0.5762456655502319\n",
      "Epoch 1786, Loss: 2.7285819351673126, Final Batch Loss: 0.7059558629989624\n",
      "Epoch 1787, Loss: 2.241909474134445, Final Batch Loss: 0.31641268730163574\n",
      "Epoch 1788, Loss: 2.2736513018608093, Final Batch Loss: 0.2864244878292084\n",
      "Epoch 1789, Loss: 2.5428335070610046, Final Batch Loss: 0.5583329796791077\n",
      "Epoch 1790, Loss: 2.514700263738632, Final Batch Loss: 0.6827853322029114\n",
      "Epoch 1791, Loss: 2.3308008313179016, Final Batch Loss: 0.5596466064453125\n",
      "Epoch 1792, Loss: 2.614431619644165, Final Batch Loss: 0.7490993738174438\n",
      "Epoch 1793, Loss: 2.5259885787963867, Final Batch Loss: 0.32833099365234375\n",
      "Epoch 1794, Loss: 2.261019855737686, Final Batch Loss: 0.32485318183898926\n",
      "Epoch 1795, Loss: 2.3033980429172516, Final Batch Loss: 0.3309133052825928\n",
      "Epoch 1796, Loss: 2.3752781748771667, Final Batch Loss: 0.6095425486564636\n",
      "Epoch 1797, Loss: 2.327086716890335, Final Batch Loss: 0.4089408814907074\n",
      "Epoch 1798, Loss: 2.258725941181183, Final Batch Loss: 0.3399696350097656\n",
      "Epoch 1799, Loss: 2.123494729399681, Final Batch Loss: 0.2317407876253128\n",
      "Epoch 1800, Loss: 2.334322512149811, Final Batch Loss: 0.3584572672843933\n",
      "Epoch 1801, Loss: 2.497471123933792, Final Batch Loss: 0.6435193419456482\n",
      "Epoch 1802, Loss: 2.6346639692783356, Final Batch Loss: 0.8052312135696411\n",
      "Epoch 1803, Loss: 1.9614174216985703, Final Batch Loss: 0.21361632645130157\n",
      "Epoch 1804, Loss: 2.6631887555122375, Final Batch Loss: 0.8530722856521606\n",
      "Epoch 1805, Loss: 2.2981040477752686, Final Batch Loss: 0.5053418874740601\n",
      "Epoch 1806, Loss: 2.0745419561862946, Final Batch Loss: 0.3227910101413727\n",
      "Epoch 1807, Loss: 2.28447762131691, Final Batch Loss: 0.43403932452201843\n",
      "Epoch 1808, Loss: 2.214237928390503, Final Batch Loss: 0.4068021774291992\n",
      "Epoch 1809, Loss: 2.611748844385147, Final Batch Loss: 0.6907321810722351\n",
      "Epoch 1810, Loss: 2.6258146464824677, Final Batch Loss: 0.7336392402648926\n",
      "Epoch 1811, Loss: 2.015436515212059, Final Batch Loss: 0.16350619494915009\n",
      "Epoch 1812, Loss: 2.3508715331554413, Final Batch Loss: 0.5772565007209778\n",
      "Epoch 1813, Loss: 2.3508115708827972, Final Batch Loss: 0.4859873652458191\n",
      "Epoch 1814, Loss: 2.4261322021484375, Final Batch Loss: 0.5810308456420898\n",
      "Epoch 1815, Loss: 2.040284648537636, Final Batch Loss: 0.2271718531847\n",
      "Epoch 1816, Loss: 2.1029073894023895, Final Batch Loss: 0.315987765789032\n",
      "Epoch 1817, Loss: 2.131421834230423, Final Batch Loss: 0.3599323332309723\n",
      "Epoch 1818, Loss: 2.108066827058792, Final Batch Loss: 0.25347012281417847\n",
      "Epoch 1819, Loss: 2.3061057329177856, Final Batch Loss: 0.4489932358264923\n",
      "Epoch 1820, Loss: 2.7915497422218323, Final Batch Loss: 1.1062469482421875\n",
      "Epoch 1821, Loss: 2.348771333694458, Final Batch Loss: 0.27143803238868713\n",
      "Epoch 1822, Loss: 2.467016100883484, Final Batch Loss: 0.33824509382247925\n",
      "Epoch 1823, Loss: 2.532345473766327, Final Batch Loss: 0.36783719062805176\n",
      "Epoch 1824, Loss: 2.827159196138382, Final Batch Loss: 0.730220377445221\n",
      "Epoch 1825, Loss: 2.475369095802307, Final Batch Loss: 0.5340437889099121\n",
      "Epoch 1826, Loss: 2.5124618113040924, Final Batch Loss: 0.4675526022911072\n",
      "Epoch 1827, Loss: 2.2434419840574265, Final Batch Loss: 0.17736481130123138\n",
      "Epoch 1828, Loss: 2.1079005897045135, Final Batch Loss: 0.22486141324043274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1829, Loss: 2.139389470219612, Final Batch Loss: 0.23501335084438324\n",
      "Epoch 1830, Loss: 2.2213033735752106, Final Batch Loss: 0.44849446415901184\n",
      "Epoch 1831, Loss: 2.4282300770282745, Final Batch Loss: 0.5059633255004883\n",
      "Epoch 1832, Loss: 2.782124698162079, Final Batch Loss: 0.9797729849815369\n",
      "Epoch 1833, Loss: 2.3520622551441193, Final Batch Loss: 0.520107090473175\n",
      "Epoch 1834, Loss: 2.3784931302070618, Final Batch Loss: 0.42696648836135864\n",
      "Epoch 1835, Loss: 2.192035436630249, Final Batch Loss: 0.3844609260559082\n",
      "Epoch 1836, Loss: 2.328195869922638, Final Batch Loss: 0.5455626845359802\n",
      "Epoch 1837, Loss: 2.1353572010993958, Final Batch Loss: 0.33418846130371094\n",
      "Epoch 1838, Loss: 2.3264014422893524, Final Batch Loss: 0.5531275868415833\n",
      "Epoch 1839, Loss: 2.240060865879059, Final Batch Loss: 0.26446643471717834\n",
      "Epoch 1840, Loss: 2.696379214525223, Final Batch Loss: 0.7086352109909058\n",
      "Epoch 1841, Loss: 2.314227432012558, Final Batch Loss: 0.4784274399280548\n",
      "Epoch 1842, Loss: 2.2854130566120148, Final Batch Loss: 0.4638659656047821\n",
      "Epoch 1843, Loss: 2.3641429245471954, Final Batch Loss: 0.42392030358314514\n",
      "Epoch 1844, Loss: 2.200235813856125, Final Batch Loss: 0.36373165249824524\n",
      "Epoch 1845, Loss: 2.5925616025924683, Final Batch Loss: 0.6285813450813293\n",
      "Epoch 1846, Loss: 2.3749959468841553, Final Batch Loss: 0.5257700085639954\n",
      "Epoch 1847, Loss: 2.662738114595413, Final Batch Loss: 0.6743979454040527\n",
      "Epoch 1848, Loss: 2.52255779504776, Final Batch Loss: 0.6707738041877747\n",
      "Epoch 1849, Loss: 2.7877619564533234, Final Batch Loss: 0.9316638708114624\n",
      "Epoch 1850, Loss: 2.3610119223594666, Final Batch Loss: 0.4421444535255432\n",
      "Epoch 1851, Loss: 3.2545919716358185, Final Batch Loss: 1.383112907409668\n",
      "Epoch 1852, Loss: 2.2873550951480865, Final Batch Loss: 0.3697208762168884\n",
      "Epoch 1853, Loss: 2.4529307186603546, Final Batch Loss: 0.5215243697166443\n",
      "Epoch 1854, Loss: 2.37408185005188, Final Batch Loss: 0.39746323227882385\n",
      "Epoch 1855, Loss: 2.3693644404411316, Final Batch Loss: 0.42506954073905945\n",
      "Epoch 1856, Loss: 2.4537752866744995, Final Batch Loss: 0.5347630381584167\n",
      "Epoch 1857, Loss: 2.298597425222397, Final Batch Loss: 0.35036927461624146\n",
      "Epoch 1858, Loss: 2.2130954265594482, Final Batch Loss: 0.41050663590431213\n",
      "Epoch 1859, Loss: 2.1238611042499542, Final Batch Loss: 0.364793598651886\n",
      "Epoch 1860, Loss: 2.300999879837036, Final Batch Loss: 0.49442529678344727\n",
      "Epoch 1861, Loss: 2.3538321554660797, Final Batch Loss: 0.5481204390525818\n",
      "Epoch 1862, Loss: 2.1939195096492767, Final Batch Loss: 0.4257871210575104\n",
      "Epoch 1863, Loss: 2.3030890226364136, Final Batch Loss: 0.6008469462394714\n",
      "Epoch 1864, Loss: 2.3439562618732452, Final Batch Loss: 0.5375155210494995\n",
      "Epoch 1865, Loss: 2.338254749774933, Final Batch Loss: 0.6320740580558777\n",
      "Epoch 1866, Loss: 2.8016245663166046, Final Batch Loss: 0.9330770373344421\n",
      "Epoch 1867, Loss: 2.1072627156972885, Final Batch Loss: 0.2431952804327011\n",
      "Epoch 1868, Loss: 2.267839550971985, Final Batch Loss: 0.40008777379989624\n",
      "Epoch 1869, Loss: 2.050533175468445, Final Batch Loss: 0.35019633173942566\n",
      "Epoch 1870, Loss: 2.2901503443717957, Final Batch Loss: 0.4699622094631195\n",
      "Epoch 1871, Loss: 2.1873410642147064, Final Batch Loss: 0.40435221791267395\n",
      "Epoch 1872, Loss: 2.3652474880218506, Final Batch Loss: 0.5948278307914734\n",
      "Epoch 1873, Loss: 2.1071873903274536, Final Batch Loss: 0.325641006231308\n",
      "Epoch 1874, Loss: 2.026643380522728, Final Batch Loss: 0.20348750054836273\n",
      "Epoch 1875, Loss: 2.160830497741699, Final Batch Loss: 0.34484508633613586\n",
      "Epoch 1876, Loss: 2.3835593461990356, Final Batch Loss: 0.5854324698448181\n",
      "Epoch 1877, Loss: 2.192744880914688, Final Batch Loss: 0.4105646312236786\n",
      "Epoch 1878, Loss: 2.2876554429531097, Final Batch Loss: 0.42644351720809937\n",
      "Epoch 1879, Loss: 2.0977075695991516, Final Batch Loss: 0.2551918625831604\n",
      "Epoch 1880, Loss: 1.9834293574094772, Final Batch Loss: 0.16708652675151825\n",
      "Epoch 1881, Loss: 2.1370648741722107, Final Batch Loss: 0.2694272994995117\n",
      "Epoch 1882, Loss: 2.109516143798828, Final Batch Loss: 0.41372889280319214\n",
      "Epoch 1883, Loss: 2.581207364797592, Final Batch Loss: 0.8094764351844788\n",
      "Epoch 1884, Loss: 2.6686073541641235, Final Batch Loss: 0.7752662301063538\n",
      "Epoch 1885, Loss: 2.56407567858696, Final Batch Loss: 0.791149914264679\n",
      "Epoch 1886, Loss: 2.578869044780731, Final Batch Loss: 0.6674085259437561\n",
      "Epoch 1887, Loss: 2.398773729801178, Final Batch Loss: 0.3701261878013611\n",
      "Epoch 1888, Loss: 2.3503924310207367, Final Batch Loss: 0.5326888561248779\n",
      "Epoch 1889, Loss: 2.126287341117859, Final Batch Loss: 0.33043986558914185\n",
      "Epoch 1890, Loss: 2.3999911844730377, Final Batch Loss: 0.5918785929679871\n",
      "Epoch 1891, Loss: 2.300205320119858, Final Batch Loss: 0.5002698302268982\n",
      "Epoch 1892, Loss: 2.5170211493968964, Final Batch Loss: 0.70025634765625\n",
      "Epoch 1893, Loss: 2.404447853565216, Final Batch Loss: 0.5393432378768921\n",
      "Epoch 1894, Loss: 2.7241457998752594, Final Batch Loss: 0.7276253700256348\n",
      "Epoch 1895, Loss: 2.162921518087387, Final Batch Loss: 0.3177375793457031\n",
      "Epoch 1896, Loss: 2.155030697584152, Final Batch Loss: 0.18392089009284973\n",
      "Epoch 1897, Loss: 2.1918312907218933, Final Batch Loss: 0.24571844935417175\n",
      "Epoch 1898, Loss: 1.9643906354904175, Final Batch Loss: 0.3291945159435272\n",
      "Epoch 1899, Loss: 2.290509283542633, Final Batch Loss: 0.4475587010383606\n",
      "Epoch 1900, Loss: 2.4414890110492706, Final Batch Loss: 0.6500004529953003\n",
      "Epoch 1901, Loss: 2.476617395877838, Final Batch Loss: 0.6805146336555481\n",
      "Epoch 1902, Loss: 2.1639800667762756, Final Batch Loss: 0.25734999775886536\n",
      "Epoch 1903, Loss: 2.0996967554092407, Final Batch Loss: 0.37050795555114746\n",
      "Epoch 1904, Loss: 2.247347116470337, Final Batch Loss: 0.5161272287368774\n",
      "Epoch 1905, Loss: 2.2967617213726044, Final Batch Loss: 0.6011902689933777\n",
      "Epoch 1906, Loss: 2.473464846611023, Final Batch Loss: 0.7347086071968079\n",
      "Epoch 1907, Loss: 2.452003449201584, Final Batch Loss: 0.6252071261405945\n",
      "Epoch 1908, Loss: 2.5424942076206207, Final Batch Loss: 0.7836329340934753\n",
      "Epoch 1909, Loss: 2.0183899253606796, Final Batch Loss: 0.22758923470973969\n",
      "Epoch 1910, Loss: 2.4929870665073395, Final Batch Loss: 0.7871503233909607\n",
      "Epoch 1911, Loss: 2.1974930465221405, Final Batch Loss: 0.3272847831249237\n",
      "Epoch 1912, Loss: 2.221357226371765, Final Batch Loss: 0.3473832607269287\n",
      "Epoch 1913, Loss: 1.9003534018993378, Final Batch Loss: 0.17911961674690247\n",
      "Epoch 1914, Loss: 2.1362965404987335, Final Batch Loss: 0.42326244711875916\n",
      "Epoch 1915, Loss: 2.1340509355068207, Final Batch Loss: 0.43763166666030884\n",
      "Epoch 1916, Loss: 2.2784892916679382, Final Batch Loss: 0.41667303442955017\n",
      "Epoch 1917, Loss: 2.0679512321949005, Final Batch Loss: 0.3471105694770813\n",
      "Epoch 1918, Loss: 2.2861913442611694, Final Batch Loss: 0.36328747868537903\n",
      "Epoch 1919, Loss: 2.1318883299827576, Final Batch Loss: 0.36474156379699707\n",
      "Epoch 1920, Loss: 2.199922353029251, Final Batch Loss: 0.45835766196250916\n",
      "Epoch 1921, Loss: 1.9726233184337616, Final Batch Loss: 0.23308515548706055\n",
      "Epoch 1922, Loss: 2.1716554760932922, Final Batch Loss: 0.44842246174812317\n",
      "Epoch 1923, Loss: 2.299640417098999, Final Batch Loss: 0.5328001379966736\n",
      "Epoch 1924, Loss: 2.0444313287734985, Final Batch Loss: 0.31396907567977905\n",
      "Epoch 1925, Loss: 2.3099377751350403, Final Batch Loss: 0.46619489789009094\n",
      "Epoch 1926, Loss: 2.1581663489341736, Final Batch Loss: 0.47910746932029724\n",
      "Epoch 1927, Loss: 2.353473871946335, Final Batch Loss: 0.5840034484863281\n",
      "Epoch 1928, Loss: 1.9381913840770721, Final Batch Loss: 0.17256879806518555\n",
      "Epoch 1929, Loss: 2.2850704193115234, Final Batch Loss: 0.5700251460075378\n",
      "Epoch 1930, Loss: 2.049807161092758, Final Batch Loss: 0.25323787331581116\n",
      "Epoch 1931, Loss: 2.076845198869705, Final Batch Loss: 0.3707906901836395\n",
      "Epoch 1932, Loss: 2.325011968612671, Final Batch Loss: 0.42195194959640503\n",
      "Epoch 1933, Loss: 2.3169429898262024, Final Batch Loss: 0.6339014172554016\n",
      "Epoch 1934, Loss: 2.028582513332367, Final Batch Loss: 0.23047351837158203\n",
      "Epoch 1935, Loss: 2.571065902709961, Final Batch Loss: 0.8789981603622437\n",
      "Epoch 1936, Loss: 2.2522341907024384, Final Batch Loss: 0.5711320638656616\n",
      "Epoch 1937, Loss: 2.0492371916770935, Final Batch Loss: 0.2620961368083954\n",
      "Epoch 1938, Loss: 2.1579767167568207, Final Batch Loss: 0.34776225686073303\n",
      "Epoch 1939, Loss: 2.2680563032627106, Final Batch Loss: 0.5355040431022644\n",
      "Epoch 1940, Loss: 1.9426861107349396, Final Batch Loss: 0.32344910502433777\n",
      "Epoch 1941, Loss: 1.8543512374162674, Final Batch Loss: 0.17318813502788544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1942, Loss: 2.2602561116218567, Final Batch Loss: 0.49592095613479614\n",
      "Epoch 1943, Loss: 2.058957040309906, Final Batch Loss: 0.24314674735069275\n",
      "Epoch 1944, Loss: 2.0616932213306427, Final Batch Loss: 0.2547931373119354\n",
      "Epoch 1945, Loss: 2.0710346400737762, Final Batch Loss: 0.30421438813209534\n",
      "Epoch 1946, Loss: 2.3655996918678284, Final Batch Loss: 0.6065478920936584\n",
      "Epoch 1947, Loss: 2.108894407749176, Final Batch Loss: 0.34366700053215027\n",
      "Epoch 1948, Loss: 2.28672131896019, Final Batch Loss: 0.4819726347923279\n",
      "Epoch 1949, Loss: 2.273907631635666, Final Batch Loss: 0.4490041136741638\n",
      "Epoch 1950, Loss: 1.986885815858841, Final Batch Loss: 0.1989668309688568\n",
      "Epoch 1951, Loss: 2.1750669479370117, Final Batch Loss: 0.39500492811203003\n",
      "Epoch 1952, Loss: 2.4734452664852142, Final Batch Loss: 0.6597999334335327\n",
      "Epoch 1953, Loss: 2.391150325536728, Final Batch Loss: 0.578861653804779\n",
      "Epoch 1954, Loss: 1.9506221115589142, Final Batch Loss: 0.24430349469184875\n",
      "Epoch 1955, Loss: 2.153048187494278, Final Batch Loss: 0.38569170236587524\n",
      "Epoch 1956, Loss: 2.347660630941391, Final Batch Loss: 0.5769744515419006\n",
      "Epoch 1957, Loss: 2.2207230627536774, Final Batch Loss: 0.4139325022697449\n",
      "Epoch 1958, Loss: 2.152554601430893, Final Batch Loss: 0.43198877573013306\n",
      "Epoch 1959, Loss: 2.047550767660141, Final Batch Loss: 0.35094743967056274\n",
      "Epoch 1960, Loss: 2.186998039484024, Final Batch Loss: 0.4163260757923126\n",
      "Epoch 1961, Loss: 2.268228054046631, Final Batch Loss: 0.5634261965751648\n",
      "Epoch 1962, Loss: 2.285046398639679, Final Batch Loss: 0.6435680389404297\n",
      "Epoch 1963, Loss: 2.029122769832611, Final Batch Loss: 0.24357330799102783\n",
      "Epoch 1964, Loss: 2.4835964739322662, Final Batch Loss: 0.698843777179718\n",
      "Epoch 1965, Loss: 2.0223382711410522, Final Batch Loss: 0.26870962977409363\n",
      "Epoch 1966, Loss: 2.1980817914009094, Final Batch Loss: 0.39244917035102844\n",
      "Epoch 1967, Loss: 2.456338495016098, Final Batch Loss: 0.827879786491394\n",
      "Epoch 1968, Loss: 2.0121673941612244, Final Batch Loss: 0.3172987103462219\n",
      "Epoch 1969, Loss: 2.463676780462265, Final Batch Loss: 0.588554859161377\n",
      "Epoch 1970, Loss: 2.1210087537765503, Final Batch Loss: 0.3531043827533722\n",
      "Epoch 1971, Loss: 2.3289226293563843, Final Batch Loss: 0.5257116556167603\n",
      "Epoch 1972, Loss: 2.1059829592704773, Final Batch Loss: 0.33774319291114807\n",
      "Epoch 1973, Loss: 2.55479297041893, Final Batch Loss: 0.8077529072761536\n",
      "Epoch 1974, Loss: 2.278393864631653, Final Batch Loss: 0.593214213848114\n",
      "Epoch 1975, Loss: 2.0640141367912292, Final Batch Loss: 0.32696929574012756\n",
      "Epoch 1976, Loss: 2.518339514732361, Final Batch Loss: 0.6818361878395081\n",
      "Epoch 1977, Loss: 2.1483707427978516, Final Batch Loss: 0.34131190180778503\n",
      "Epoch 1978, Loss: 2.2178558707237244, Final Batch Loss: 0.39878639578819275\n",
      "Epoch 1979, Loss: 2.178503453731537, Final Batch Loss: 0.36590123176574707\n",
      "Epoch 1980, Loss: 2.368994027376175, Final Batch Loss: 0.6044355034828186\n",
      "Epoch 1981, Loss: 2.3256190419197083, Final Batch Loss: 0.5300989747047424\n",
      "Epoch 1982, Loss: 2.379152327775955, Final Batch Loss: 0.7070289850234985\n",
      "Epoch 1983, Loss: 1.935930758714676, Final Batch Loss: 0.18750658631324768\n",
      "Epoch 1984, Loss: 2.4352973103523254, Final Batch Loss: 0.610702395439148\n",
      "Epoch 1985, Loss: 2.119028091430664, Final Batch Loss: 0.3100753426551819\n",
      "Epoch 1986, Loss: 1.7912388741970062, Final Batch Loss: 0.21324384212493896\n",
      "Epoch 1987, Loss: 2.2512410283088684, Final Batch Loss: 0.3733501434326172\n",
      "Epoch 1988, Loss: 2.390479475259781, Final Batch Loss: 0.6634379625320435\n",
      "Epoch 1989, Loss: 2.244484096765518, Final Batch Loss: 0.531125009059906\n",
      "Epoch 1990, Loss: 2.108024299144745, Final Batch Loss: 0.442352831363678\n",
      "Epoch 1991, Loss: 1.9624202251434326, Final Batch Loss: 0.2793465256690979\n",
      "Epoch 1992, Loss: 2.534353792667389, Final Batch Loss: 0.7735095024108887\n",
      "Epoch 1993, Loss: 2.2690526247024536, Final Batch Loss: 0.43089476227760315\n",
      "Epoch 1994, Loss: 2.1451615393161774, Final Batch Loss: 0.37706834077835083\n",
      "Epoch 1995, Loss: 2.1752014756202698, Final Batch Loss: 0.3648630976676941\n",
      "Epoch 1996, Loss: 2.0501241385936737, Final Batch Loss: 0.23544085025787354\n",
      "Epoch 1997, Loss: 2.21218341588974, Final Batch Loss: 0.4288444221019745\n",
      "Epoch 1998, Loss: 2.2610017359256744, Final Batch Loss: 0.3839245140552521\n",
      "Epoch 1999, Loss: 2.5477373600006104, Final Batch Loss: 0.7901455163955688\n",
      "Epoch 2000, Loss: 2.4618013203144073, Final Batch Loss: 0.34921401739120483\n",
      "Epoch 2001, Loss: 2.850912392139435, Final Batch Loss: 0.3853015899658203\n",
      "Epoch 2002, Loss: 2.3890088498592377, Final Batch Loss: 0.4938998520374298\n",
      "Epoch 2003, Loss: 2.335799366235733, Final Batch Loss: 0.484952837228775\n",
      "Epoch 2004, Loss: 2.253897547721863, Final Batch Loss: 0.29389771819114685\n",
      "Epoch 2005, Loss: 2.409194201231003, Final Batch Loss: 0.5407055020332336\n",
      "Epoch 2006, Loss: 2.261555105447769, Final Batch Loss: 0.3493504226207733\n",
      "Epoch 2007, Loss: 2.5463518500328064, Final Batch Loss: 0.6629387736320496\n",
      "Epoch 2008, Loss: 2.0158423632383347, Final Batch Loss: 0.22635392844676971\n",
      "Epoch 2009, Loss: 2.4780094027519226, Final Batch Loss: 0.7034590840339661\n",
      "Epoch 2010, Loss: 2.5939294695854187, Final Batch Loss: 0.8660706877708435\n",
      "Epoch 2011, Loss: 2.1790688037872314, Final Batch Loss: 0.20569226145744324\n",
      "Epoch 2012, Loss: 2.3211142122745514, Final Batch Loss: 0.4350833594799042\n",
      "Epoch 2013, Loss: 2.2207773625850677, Final Batch Loss: 0.32822105288505554\n",
      "Epoch 2014, Loss: 2.219161570072174, Final Batch Loss: 0.43123215436935425\n",
      "Epoch 2015, Loss: 2.339178591966629, Final Batch Loss: 0.47459444403648376\n",
      "Epoch 2016, Loss: 2.329552501440048, Final Batch Loss: 0.42834681272506714\n",
      "Epoch 2017, Loss: 2.0660905241966248, Final Batch Loss: 0.14515498280525208\n",
      "Epoch 2018, Loss: 2.412130743265152, Final Batch Loss: 0.5948213338851929\n",
      "Epoch 2019, Loss: 2.441338360309601, Final Batch Loss: 0.6505890488624573\n",
      "Epoch 2020, Loss: 2.1452066898345947, Final Batch Loss: 0.3875786364078522\n",
      "Epoch 2021, Loss: 2.2806951701641083, Final Batch Loss: 0.42768147587776184\n",
      "Epoch 2022, Loss: 2.529341757297516, Final Batch Loss: 0.5940935015678406\n",
      "Epoch 2023, Loss: 2.282903492450714, Final Batch Loss: 0.48472708463668823\n",
      "Epoch 2024, Loss: 2.121410697698593, Final Batch Loss: 0.3673947751522064\n",
      "Epoch 2025, Loss: 1.9011750519275665, Final Batch Loss: 0.2904874384403229\n",
      "Epoch 2026, Loss: 2.2622747719287872, Final Batch Loss: 0.5943590998649597\n",
      "Epoch 2027, Loss: 2.211982846260071, Final Batch Loss: 0.4201410412788391\n",
      "Epoch 2028, Loss: 2.170461893081665, Final Batch Loss: 0.390089750289917\n",
      "Epoch 2029, Loss: 2.1522733867168427, Final Batch Loss: 0.46876969933509827\n",
      "Epoch 2030, Loss: 2.0498806536197662, Final Batch Loss: 0.26617565751075745\n",
      "Epoch 2031, Loss: 2.332468956708908, Final Batch Loss: 0.5503095984458923\n",
      "Epoch 2032, Loss: 2.139963060617447, Final Batch Loss: 0.34144607186317444\n",
      "Epoch 2033, Loss: 2.0753211081027985, Final Batch Loss: 0.27596092224121094\n",
      "Epoch 2034, Loss: 1.851718693971634, Final Batch Loss: 0.1492331326007843\n",
      "Epoch 2035, Loss: 2.0378618240356445, Final Batch Loss: 0.262870192527771\n",
      "Epoch 2036, Loss: 2.164071351289749, Final Batch Loss: 0.3917001783847809\n",
      "Epoch 2037, Loss: 1.979447364807129, Final Batch Loss: 0.27046969532966614\n",
      "Epoch 2038, Loss: 1.800708718597889, Final Batch Loss: 0.11929444223642349\n",
      "Epoch 2039, Loss: 2.0894975662231445, Final Batch Loss: 0.45062634348869324\n",
      "Epoch 2040, Loss: 2.2704676389694214, Final Batch Loss: 0.581039309501648\n",
      "Epoch 2041, Loss: 2.159749060869217, Final Batch Loss: 0.303013414144516\n",
      "Epoch 2042, Loss: 1.9160591661930084, Final Batch Loss: 0.309995174407959\n",
      "Epoch 2043, Loss: 2.1849551498889923, Final Batch Loss: 0.47518855333328247\n",
      "Epoch 2044, Loss: 2.082480490207672, Final Batch Loss: 0.3590441048145294\n",
      "Epoch 2045, Loss: 1.9938915967941284, Final Batch Loss: 0.2675580680370331\n",
      "Epoch 2046, Loss: 2.3550525903701782, Final Batch Loss: 0.48856547474861145\n",
      "Epoch 2047, Loss: 2.10928276181221, Final Batch Loss: 0.40400439500808716\n",
      "Epoch 2048, Loss: 2.0066612362861633, Final Batch Loss: 0.20078203082084656\n",
      "Epoch 2049, Loss: 2.424714595079422, Final Batch Loss: 0.6367012858390808\n",
      "Epoch 2050, Loss: 2.37818506360054, Final Batch Loss: 0.6420342326164246\n",
      "Epoch 2051, Loss: 1.8813426792621613, Final Batch Loss: 0.27126580476760864\n",
      "Epoch 2052, Loss: 2.121779352426529, Final Batch Loss: 0.42947813868522644\n",
      "Epoch 2053, Loss: 2.0274141877889633, Final Batch Loss: 0.2063903957605362\n",
      "Epoch 2054, Loss: 2.2645102441310883, Final Batch Loss: 0.6109399199485779\n",
      "Epoch 2055, Loss: 1.9904242753982544, Final Batch Loss: 0.3126138746738434\n",
      "Epoch 2056, Loss: 1.9592370092868805, Final Batch Loss: 0.3563631474971771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2057, Loss: 1.9086463749408722, Final Batch Loss: 0.2587532699108124\n",
      "Epoch 2058, Loss: 2.2753641605377197, Final Batch Loss: 0.633583128452301\n",
      "Epoch 2059, Loss: 2.015865668654442, Final Batch Loss: 0.24629037082195282\n",
      "Epoch 2060, Loss: 2.1001496613025665, Final Batch Loss: 0.3272917568683624\n",
      "Epoch 2061, Loss: 2.3899534046649933, Final Batch Loss: 0.5606130361557007\n",
      "Epoch 2062, Loss: 2.237119495868683, Final Batch Loss: 0.4910913407802582\n",
      "Epoch 2063, Loss: 2.121095597743988, Final Batch Loss: 0.4101352095603943\n",
      "Epoch 2064, Loss: 2.0422704219818115, Final Batch Loss: 0.41347378492355347\n",
      "Epoch 2065, Loss: 2.4722579419612885, Final Batch Loss: 0.7069603204727173\n",
      "Epoch 2066, Loss: 2.1565282940864563, Final Batch Loss: 0.5446034669876099\n",
      "Epoch 2067, Loss: 1.829342320561409, Final Batch Loss: 0.13303013145923615\n",
      "Epoch 2068, Loss: 2.113856256008148, Final Batch Loss: 0.4681573212146759\n",
      "Epoch 2069, Loss: 2.323078542947769, Final Batch Loss: 0.6471737027168274\n",
      "Epoch 2070, Loss: 1.8670872896909714, Final Batch Loss: 0.1576910763978958\n",
      "Epoch 2071, Loss: 2.659566342830658, Final Batch Loss: 0.9095277786254883\n",
      "Epoch 2072, Loss: 2.0777860581874847, Final Batch Loss: 0.31220242381095886\n",
      "Epoch 2073, Loss: 2.189729019999504, Final Batch Loss: 0.23128025233745575\n",
      "Epoch 2074, Loss: 2.1720562279224396, Final Batch Loss: 0.3024267256259918\n",
      "Epoch 2075, Loss: 2.2015214562416077, Final Batch Loss: 0.31146812438964844\n",
      "Epoch 2076, Loss: 2.378395825624466, Final Batch Loss: 0.41566988825798035\n",
      "Epoch 2077, Loss: 2.7559487521648407, Final Batch Loss: 1.0594704151153564\n",
      "Epoch 2078, Loss: 1.9734029918909073, Final Batch Loss: 0.19996501505374908\n",
      "Epoch 2079, Loss: 2.43743759393692, Final Batch Loss: 0.5095952749252319\n",
      "Epoch 2080, Loss: 2.3323726654052734, Final Batch Loss: 0.5207189321517944\n",
      "Epoch 2081, Loss: 2.3383651077747345, Final Batch Loss: 0.27068954706192017\n",
      "Epoch 2082, Loss: 2.2512784004211426, Final Batch Loss: 0.3605455458164215\n",
      "Epoch 2083, Loss: 1.8506967462599277, Final Batch Loss: 0.054396096616983414\n",
      "Epoch 2084, Loss: 2.6804506480693817, Final Batch Loss: 0.8183225393295288\n",
      "Epoch 2085, Loss: 2.4502092897892, Final Batch Loss: 0.6232159733772278\n",
      "Epoch 2086, Loss: 2.2407205998897552, Final Batch Loss: 0.3222486674785614\n",
      "Epoch 2087, Loss: 2.0856984555721283, Final Batch Loss: 0.3383825123310089\n",
      "Epoch 2088, Loss: 2.295476049184799, Final Batch Loss: 0.607570469379425\n",
      "Epoch 2089, Loss: 2.3030134737491608, Final Batch Loss: 0.49852851033210754\n",
      "Epoch 2090, Loss: 1.9486754536628723, Final Batch Loss: 0.2584056556224823\n",
      "Epoch 2091, Loss: 2.047662079334259, Final Batch Loss: 0.2818008065223694\n",
      "Epoch 2092, Loss: 2.2521254420280457, Final Batch Loss: 0.49245187640190125\n",
      "Epoch 2093, Loss: 1.958791047334671, Final Batch Loss: 0.21803167462348938\n",
      "Epoch 2094, Loss: 2.297013998031616, Final Batch Loss: 0.6068423986434937\n",
      "Epoch 2095, Loss: 2.4526637196540833, Final Batch Loss: 0.7503958940505981\n",
      "Epoch 2096, Loss: 2.3746050894260406, Final Batch Loss: 0.6990846991539001\n",
      "Epoch 2097, Loss: 1.8883329629898071, Final Batch Loss: 0.21419712901115417\n",
      "Epoch 2098, Loss: 2.170170307159424, Final Batch Loss: 0.4291647970676422\n",
      "Epoch 2099, Loss: 2.004783719778061, Final Batch Loss: 0.2261730134487152\n",
      "Epoch 2100, Loss: 1.9652439057826996, Final Batch Loss: 0.2943916916847229\n",
      "Epoch 2101, Loss: 2.1888065338134766, Final Batch Loss: 0.44986629486083984\n",
      "Epoch 2102, Loss: 2.2538126409053802, Final Batch Loss: 0.49317580461502075\n",
      "Epoch 2103, Loss: 2.42939755320549, Final Batch Loss: 0.7702413201332092\n",
      "Epoch 2104, Loss: 2.1273739635944366, Final Batch Loss: 0.33302921056747437\n",
      "Epoch 2105, Loss: 2.3404646515846252, Final Batch Loss: 0.6343790888786316\n",
      "Epoch 2106, Loss: 2.6087586581707, Final Batch Loss: 0.7617800831794739\n",
      "Epoch 2107, Loss: 2.0321963131427765, Final Batch Loss: 0.4011387228965759\n",
      "Epoch 2108, Loss: 2.37601438164711, Final Batch Loss: 0.5654652714729309\n",
      "Epoch 2109, Loss: 2.1048765778541565, Final Batch Loss: 0.4869506061077118\n",
      "Epoch 2110, Loss: 1.9830853044986725, Final Batch Loss: 0.28946805000305176\n",
      "Epoch 2111, Loss: 2.1792783737182617, Final Batch Loss: 0.4867646396160126\n",
      "Epoch 2112, Loss: 2.2725818157196045, Final Batch Loss: 0.4573840796947479\n",
      "Epoch 2113, Loss: 2.023003026843071, Final Batch Loss: 0.2488398402929306\n",
      "Epoch 2114, Loss: 1.9272520989179611, Final Batch Loss: 0.23678897321224213\n",
      "Epoch 2115, Loss: 1.9836576282978058, Final Batch Loss: 0.3077526390552521\n",
      "Epoch 2116, Loss: 2.111679255962372, Final Batch Loss: 0.43964698910713196\n",
      "Epoch 2117, Loss: 2.165610045194626, Final Batch Loss: 0.3774261474609375\n",
      "Epoch 2118, Loss: 2.026006758213043, Final Batch Loss: 0.4262049198150635\n",
      "Epoch 2119, Loss: 1.9710551500320435, Final Batch Loss: 0.35478538274765015\n",
      "Epoch 2120, Loss: 2.4302588999271393, Final Batch Loss: 0.7653796076774597\n",
      "Epoch 2121, Loss: 2.3115410208702087, Final Batch Loss: 0.6188092231750488\n",
      "Epoch 2122, Loss: 2.0902208983898163, Final Batch Loss: 0.32690849900245667\n",
      "Epoch 2123, Loss: 2.2912182807922363, Final Batch Loss: 0.5352562069892883\n",
      "Epoch 2124, Loss: 2.1623052656650543, Final Batch Loss: 0.4848936200141907\n",
      "Epoch 2125, Loss: 2.111183285713196, Final Batch Loss: 0.4551556706428528\n",
      "Epoch 2126, Loss: 2.304724246263504, Final Batch Loss: 0.46715617179870605\n",
      "Epoch 2127, Loss: 2.1621964871883392, Final Batch Loss: 0.3160462975502014\n",
      "Epoch 2128, Loss: 2.6979070603847504, Final Batch Loss: 0.9873242974281311\n",
      "Epoch 2129, Loss: 1.911663606762886, Final Batch Loss: 0.1996852308511734\n",
      "Epoch 2130, Loss: 2.0518695414066315, Final Batch Loss: 0.29679855704307556\n",
      "Epoch 2131, Loss: 2.089455723762512, Final Batch Loss: 0.3631076216697693\n",
      "Epoch 2132, Loss: 2.1890622675418854, Final Batch Loss: 0.5897791981697083\n",
      "Epoch 2133, Loss: 1.9466390311717987, Final Batch Loss: 0.2855021357536316\n",
      "Epoch 2134, Loss: 2.596214175224304, Final Batch Loss: 0.7616335153579712\n",
      "Epoch 2135, Loss: 2.0651727318763733, Final Batch Loss: 0.39750489592552185\n",
      "Epoch 2136, Loss: 2.0381902903318405, Final Batch Loss: 0.22500644624233246\n",
      "Epoch 2137, Loss: 2.2368322610855103, Final Batch Loss: 0.6368363499641418\n",
      "Epoch 2138, Loss: 2.180028736591339, Final Batch Loss: 0.5692153573036194\n",
      "Epoch 2139, Loss: 2.292999178171158, Final Batch Loss: 0.5489071607589722\n",
      "Epoch 2140, Loss: 1.9806007593870163, Final Batch Loss: 0.21089769899845123\n",
      "Epoch 2141, Loss: 2.415449470281601, Final Batch Loss: 0.7093129754066467\n",
      "Epoch 2142, Loss: 1.9281262457370758, Final Batch Loss: 0.26815065741539\n",
      "Epoch 2143, Loss: 2.002879172563553, Final Batch Loss: 0.31690317392349243\n",
      "Epoch 2144, Loss: 2.026309698820114, Final Batch Loss: 0.3524501621723175\n",
      "Epoch 2145, Loss: 1.9368840605020523, Final Batch Loss: 0.156758651137352\n",
      "Epoch 2146, Loss: 2.035238653421402, Final Batch Loss: 0.47366732358932495\n",
      "Epoch 2147, Loss: 2.0850412249565125, Final Batch Loss: 0.3109597861766815\n",
      "Epoch 2148, Loss: 2.085610806941986, Final Batch Loss: 0.4496608376502991\n",
      "Epoch 2149, Loss: 2.022108733654022, Final Batch Loss: 0.3667251169681549\n",
      "Epoch 2150, Loss: 2.106607675552368, Final Batch Loss: 0.37512722611427307\n",
      "Epoch 2151, Loss: 1.9596092700958252, Final Batch Loss: 0.19792214035987854\n",
      "Epoch 2152, Loss: 2.0078036785125732, Final Batch Loss: 0.28728392720222473\n",
      "Epoch 2153, Loss: 2.4881140291690826, Final Batch Loss: 0.7692999839782715\n",
      "Epoch 2154, Loss: 2.162102371454239, Final Batch Loss: 0.4925305247306824\n",
      "Epoch 2155, Loss: 2.2750873267650604, Final Batch Loss: 0.6298016309738159\n",
      "Epoch 2156, Loss: 2.3113609850406647, Final Batch Loss: 0.5059741139411926\n",
      "Epoch 2157, Loss: 2.183678090572357, Final Batch Loss: 0.3323952555656433\n",
      "Epoch 2158, Loss: 2.2037719190120697, Final Batch Loss: 0.1890726089477539\n",
      "Epoch 2159, Loss: 2.0233243107795715, Final Batch Loss: 0.3253573775291443\n",
      "Epoch 2160, Loss: 2.154941439628601, Final Batch Loss: 0.5156737565994263\n",
      "Epoch 2161, Loss: 2.0731155574321747, Final Batch Loss: 0.40239235758781433\n",
      "Epoch 2162, Loss: 1.8171881586313248, Final Batch Loss: 0.22169221937656403\n",
      "Epoch 2163, Loss: 1.9865945726633072, Final Batch Loss: 0.24632154405117035\n",
      "Epoch 2164, Loss: 2.0299724638462067, Final Batch Loss: 0.34200313687324524\n",
      "Epoch 2165, Loss: 2.0504939556121826, Final Batch Loss: 0.38071998953819275\n",
      "Epoch 2166, Loss: 2.210046410560608, Final Batch Loss: 0.5481273531913757\n",
      "Epoch 2167, Loss: 2.167323589324951, Final Batch Loss: 0.5125283598899841\n",
      "Epoch 2168, Loss: 2.3798485696315765, Final Batch Loss: 0.5675392746925354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2169, Loss: 2.395291656255722, Final Batch Loss: 0.5909348130226135\n",
      "Epoch 2170, Loss: 2.0509800016880035, Final Batch Loss: 0.2534719407558441\n",
      "Epoch 2171, Loss: 1.9892387092113495, Final Batch Loss: 0.26371827721595764\n",
      "Epoch 2172, Loss: 2.2395528852939606, Final Batch Loss: 0.4894144833087921\n",
      "Epoch 2173, Loss: 2.122625768184662, Final Batch Loss: 0.4672413766384125\n",
      "Epoch 2174, Loss: 2.15603169798851, Final Batch Loss: 0.4100801944732666\n",
      "Epoch 2175, Loss: 1.778222531080246, Final Batch Loss: 0.21772277355194092\n",
      "Epoch 2176, Loss: 2.056923121213913, Final Batch Loss: 0.32231757044792175\n",
      "Epoch 2177, Loss: 2.316058248281479, Final Batch Loss: 0.6375243067741394\n",
      "Epoch 2178, Loss: 2.8586871922016144, Final Batch Loss: 1.1919564008712769\n",
      "Epoch 2179, Loss: 2.2887759506702423, Final Batch Loss: 0.574081301689148\n",
      "Epoch 2180, Loss: 2.7431493401527405, Final Batch Loss: 0.7452926635742188\n",
      "Epoch 2181, Loss: 2.304284989833832, Final Batch Loss: 0.5166283845901489\n",
      "Epoch 2182, Loss: 2.7648523151874542, Final Batch Loss: 1.0453568696975708\n",
      "Epoch 2183, Loss: 2.4486308693885803, Final Batch Loss: 0.6865944862365723\n",
      "Epoch 2184, Loss: 2.352929323911667, Final Batch Loss: 0.5934286117553711\n",
      "Epoch 2185, Loss: 2.102087825536728, Final Batch Loss: 0.3617437779903412\n",
      "Epoch 2186, Loss: 2.331524759531021, Final Batch Loss: 0.5351699590682983\n",
      "Epoch 2187, Loss: 2.053349882364273, Final Batch Loss: 0.39906957745552063\n",
      "Epoch 2188, Loss: 2.583390384912491, Final Batch Loss: 0.890322744846344\n",
      "Epoch 2189, Loss: 2.263135015964508, Final Batch Loss: 0.46310514211654663\n",
      "Epoch 2190, Loss: 1.9664270728826523, Final Batch Loss: 0.1417885273694992\n",
      "Epoch 2191, Loss: 2.0561622977256775, Final Batch Loss: 0.3276638388633728\n",
      "Epoch 2192, Loss: 2.1087356507778168, Final Batch Loss: 0.2599990963935852\n",
      "Epoch 2193, Loss: 2.1178396940231323, Final Batch Loss: 0.4514118731021881\n",
      "Epoch 2194, Loss: 1.8727773874998093, Final Batch Loss: 0.0704742819070816\n",
      "Epoch 2195, Loss: 2.1314399242401123, Final Batch Loss: 0.3327848017215729\n",
      "Epoch 2196, Loss: 1.9852047562599182, Final Batch Loss: 0.3573918342590332\n",
      "Epoch 2197, Loss: 2.2578018605709076, Final Batch Loss: 0.5422458052635193\n",
      "Epoch 2198, Loss: 2.4824648797512054, Final Batch Loss: 0.802854597568512\n",
      "Epoch 2199, Loss: 2.4040819108486176, Final Batch Loss: 0.762469470500946\n",
      "Epoch 2200, Loss: 2.041655421257019, Final Batch Loss: 0.4815485179424286\n",
      "Epoch 2201, Loss: 1.8968288451433182, Final Batch Loss: 0.22793792188167572\n",
      "Epoch 2202, Loss: 2.2260923385620117, Final Batch Loss: 0.543336808681488\n",
      "Epoch 2203, Loss: 2.033224582672119, Final Batch Loss: 0.28450730443000793\n",
      "Epoch 2204, Loss: 2.4273844361305237, Final Batch Loss: 0.6271949410438538\n",
      "Epoch 2205, Loss: 2.2174929678440094, Final Batch Loss: 0.4384358823299408\n",
      "Epoch 2206, Loss: 2.1639974117279053, Final Batch Loss: 0.4686404764652252\n",
      "Epoch 2207, Loss: 1.7548040002584457, Final Batch Loss: 0.11030806601047516\n",
      "Epoch 2208, Loss: 2.6240386068820953, Final Batch Loss: 0.9245460629463196\n",
      "Epoch 2209, Loss: 2.2692231237888336, Final Batch Loss: 0.5966605544090271\n",
      "Epoch 2210, Loss: 2.1019808650016785, Final Batch Loss: 0.46017223596572876\n",
      "Epoch 2211, Loss: 2.336900442838669, Final Batch Loss: 0.6474882364273071\n",
      "Epoch 2212, Loss: 2.9045160710811615, Final Batch Loss: 1.1878862380981445\n",
      "Epoch 2213, Loss: 2.4712568819522858, Final Batch Loss: 0.5211700201034546\n",
      "Epoch 2214, Loss: 2.4301622807979584, Final Batch Loss: 0.6925448775291443\n",
      "Epoch 2215, Loss: 1.95322984457016, Final Batch Loss: 0.29859623312950134\n",
      "Epoch 2216, Loss: 2.3483164608478546, Final Batch Loss: 0.4842762351036072\n",
      "Epoch 2217, Loss: 2.1222459077835083, Final Batch Loss: 0.27985796332359314\n",
      "Epoch 2218, Loss: 2.139639288187027, Final Batch Loss: 0.48021742701530457\n",
      "Epoch 2219, Loss: 1.8094285801053047, Final Batch Loss: 0.11975014954805374\n",
      "Epoch 2220, Loss: 1.992327556014061, Final Batch Loss: 0.2092370241880417\n",
      "Epoch 2221, Loss: 1.8075414299964905, Final Batch Loss: 0.2735673487186432\n",
      "Epoch 2222, Loss: 2.8004069328308105, Final Batch Loss: 1.0589425563812256\n",
      "Epoch 2223, Loss: 1.9739716947078705, Final Batch Loss: 0.3021712601184845\n",
      "Epoch 2224, Loss: 1.880410373210907, Final Batch Loss: 0.34970125555992126\n",
      "Epoch 2225, Loss: 2.2874617874622345, Final Batch Loss: 0.555305540561676\n",
      "Epoch 2226, Loss: 2.013029843568802, Final Batch Loss: 0.3153690993785858\n",
      "Epoch 2227, Loss: 2.33519971370697, Final Batch Loss: 0.6805391907691956\n",
      "Epoch 2228, Loss: 2.678187221288681, Final Batch Loss: 0.8920941948890686\n",
      "Epoch 2229, Loss: 2.1924451887607574, Final Batch Loss: 0.3624318242073059\n",
      "Epoch 2230, Loss: 1.9835551083087921, Final Batch Loss: 0.19540658593177795\n",
      "Epoch 2231, Loss: 2.1758269667625427, Final Batch Loss: 0.2953946888446808\n",
      "Epoch 2232, Loss: 2.109067589044571, Final Batch Loss: 0.3193833529949188\n",
      "Epoch 2233, Loss: 2.3341764509677887, Final Batch Loss: 0.5992977023124695\n",
      "Epoch 2234, Loss: 2.020891308784485, Final Batch Loss: 0.3280878961086273\n",
      "Epoch 2235, Loss: 1.8687683194875717, Final Batch Loss: 0.2323901504278183\n",
      "Epoch 2236, Loss: 2.024665951728821, Final Batch Loss: 0.4281597137451172\n",
      "Epoch 2237, Loss: 1.7746428325772285, Final Batch Loss: 0.11677027493715286\n",
      "Epoch 2238, Loss: 2.1828459203243256, Final Batch Loss: 0.5078076720237732\n",
      "Epoch 2239, Loss: 1.9622879326343536, Final Batch Loss: 0.3317258059978485\n",
      "Epoch 2240, Loss: 2.213529020547867, Final Batch Loss: 0.4571594297885895\n",
      "Epoch 2241, Loss: 2.1236454844474792, Final Batch Loss: 0.38475924730300903\n",
      "Epoch 2242, Loss: 2.5293859243392944, Final Batch Loss: 0.7686986327171326\n",
      "Epoch 2243, Loss: 1.8730609863996506, Final Batch Loss: 0.13889659941196442\n",
      "Epoch 2244, Loss: 2.825327515602112, Final Batch Loss: 1.0012251138687134\n",
      "Epoch 2245, Loss: 2.13410285115242, Final Batch Loss: 0.42718055844306946\n",
      "Epoch 2246, Loss: 2.313200056552887, Final Batch Loss: 0.5982251167297363\n",
      "Epoch 2247, Loss: 2.326301872730255, Final Batch Loss: 0.5617555975914001\n",
      "Epoch 2248, Loss: 2.048794537782669, Final Batch Loss: 0.4035351872444153\n",
      "Epoch 2249, Loss: 2.154904395341873, Final Batch Loss: 0.413193017244339\n",
      "Epoch 2250, Loss: 2.2188926339149475, Final Batch Loss: 0.3244636058807373\n",
      "Epoch 2251, Loss: 2.104373514652252, Final Batch Loss: 0.4187762439250946\n",
      "Epoch 2252, Loss: 2.1963663399219513, Final Batch Loss: 0.587492823600769\n",
      "Epoch 2253, Loss: 2.166471093893051, Final Batch Loss: 0.5060211420059204\n",
      "Epoch 2254, Loss: 1.863140493631363, Final Batch Loss: 0.20629462599754333\n",
      "Epoch 2255, Loss: 2.100210428237915, Final Batch Loss: 0.41141465306282043\n",
      "Epoch 2256, Loss: 2.0264082551002502, Final Batch Loss: 0.2508719861507416\n",
      "Epoch 2257, Loss: 2.153583526611328, Final Batch Loss: 0.5080935955047607\n",
      "Epoch 2258, Loss: 2.7291565537452698, Final Batch Loss: 1.1367734670639038\n",
      "Epoch 2259, Loss: 2.255008190870285, Final Batch Loss: 0.5203431844711304\n",
      "Epoch 2260, Loss: 2.1743863821029663, Final Batch Loss: 0.3400515615940094\n",
      "Epoch 2261, Loss: 2.262980192899704, Final Batch Loss: 0.5278465747833252\n",
      "Epoch 2262, Loss: 2.298613041639328, Final Batch Loss: 0.5424586534500122\n",
      "Epoch 2263, Loss: 2.088413178920746, Final Batch Loss: 0.3712269961833954\n",
      "Epoch 2264, Loss: 2.114157259464264, Final Batch Loss: 0.42469289898872375\n",
      "Epoch 2265, Loss: 1.962797850370407, Final Batch Loss: 0.3025185167789459\n",
      "Epoch 2266, Loss: 1.7470708042383194, Final Batch Loss: 0.12896208465099335\n",
      "Epoch 2267, Loss: 2.088993161916733, Final Batch Loss: 0.470685750246048\n",
      "Epoch 2268, Loss: 2.10934641957283, Final Batch Loss: 0.5138047337532043\n",
      "Epoch 2269, Loss: 2.0661721527576447, Final Batch Loss: 0.46310994029045105\n",
      "Epoch 2270, Loss: 1.9949072897434235, Final Batch Loss: 0.39232560992240906\n",
      "Epoch 2271, Loss: 2.397989273071289, Final Batch Loss: 0.737052321434021\n",
      "Epoch 2272, Loss: 1.917546659708023, Final Batch Loss: 0.17583522200584412\n",
      "Epoch 2273, Loss: 2.114568293094635, Final Batch Loss: 0.5788675546646118\n",
      "Epoch 2274, Loss: 2.0279446840286255, Final Batch Loss: 0.38120928406715393\n",
      "Epoch 2275, Loss: 2.0347267389297485, Final Batch Loss: 0.2943921983242035\n",
      "Epoch 2276, Loss: 2.2897238731384277, Final Batch Loss: 0.5736221671104431\n",
      "Epoch 2277, Loss: 2.121243953704834, Final Batch Loss: 0.40607526898384094\n",
      "Epoch 2278, Loss: 2.248315155506134, Final Batch Loss: 0.4413742125034332\n",
      "Epoch 2279, Loss: 1.7938269674777985, Final Batch Loss: 0.3071063160896301\n",
      "Epoch 2280, Loss: 1.9428682029247284, Final Batch Loss: 0.2838072180747986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2281, Loss: 2.5729140043258667, Final Batch Loss: 0.9569322466850281\n",
      "Epoch 2282, Loss: 2.1531553268432617, Final Batch Loss: 0.45836013555526733\n",
      "Epoch 2283, Loss: 1.9672739207744598, Final Batch Loss: 0.30647802352905273\n",
      "Epoch 2284, Loss: 2.257246971130371, Final Batch Loss: 0.6302801966667175\n",
      "Epoch 2285, Loss: 2.2102458775043488, Final Batch Loss: 0.5136035680770874\n",
      "Epoch 2286, Loss: 2.168722778558731, Final Batch Loss: 0.47392040491104126\n",
      "Epoch 2287, Loss: 2.054363250732422, Final Batch Loss: 0.3896106779575348\n",
      "Epoch 2288, Loss: 1.9951739013195038, Final Batch Loss: 0.28501778841018677\n",
      "Epoch 2289, Loss: 2.2746669352054596, Final Batch Loss: 0.6002090573310852\n",
      "Epoch 2290, Loss: 2.0624599754810333, Final Batch Loss: 0.37833255529403687\n",
      "Epoch 2291, Loss: 1.9252579510211945, Final Batch Loss: 0.33916693925857544\n",
      "Epoch 2292, Loss: 1.8381383121013641, Final Batch Loss: 0.2236439287662506\n",
      "Epoch 2293, Loss: 2.424625873565674, Final Batch Loss: 0.7784889340400696\n",
      "Epoch 2294, Loss: 1.9597567170858383, Final Batch Loss: 0.22704242169857025\n",
      "Epoch 2295, Loss: 2.0173038244247437, Final Batch Loss: 0.3370026648044586\n",
      "Epoch 2296, Loss: 2.1415983140468597, Final Batch Loss: 0.3889412581920624\n",
      "Epoch 2297, Loss: 2.3016664683818817, Final Batch Loss: 0.6301137804985046\n",
      "Epoch 2298, Loss: 1.660158235579729, Final Batch Loss: 0.047207463532686234\n",
      "Epoch 2299, Loss: 2.4848876297473907, Final Batch Loss: 0.8645084500312805\n",
      "Epoch 2300, Loss: 2.092312306165695, Final Batch Loss: 0.33791783452033997\n",
      "Epoch 2301, Loss: 2.132452756166458, Final Batch Loss: 0.39761972427368164\n",
      "Epoch 2302, Loss: 1.9507833123207092, Final Batch Loss: 0.3021368384361267\n",
      "Epoch 2303, Loss: 1.9272612929344177, Final Batch Loss: 0.34136563539505005\n",
      "Epoch 2304, Loss: 1.8722156286239624, Final Batch Loss: 0.20318475365638733\n",
      "Epoch 2305, Loss: 2.179623395204544, Final Batch Loss: 0.5406660437583923\n",
      "Epoch 2306, Loss: 2.1542652547359467, Final Batch Loss: 0.41428327560424805\n",
      "Epoch 2307, Loss: 1.9420596957206726, Final Batch Loss: 0.36769384145736694\n",
      "Epoch 2308, Loss: 1.907732903957367, Final Batch Loss: 0.3122490346431732\n",
      "Epoch 2309, Loss: 2.086766391992569, Final Batch Loss: 0.47532591223716736\n",
      "Epoch 2310, Loss: 2.409790277481079, Final Batch Loss: 0.7173462510108948\n",
      "Epoch 2311, Loss: 2.202228367328644, Final Batch Loss: 0.5956408381462097\n",
      "Epoch 2312, Loss: 2.251267671585083, Final Batch Loss: 0.5064056515693665\n",
      "Epoch 2313, Loss: 2.2293477654457092, Final Batch Loss: 0.46713918447494507\n",
      "Epoch 2314, Loss: 2.3849879801273346, Final Batch Loss: 0.5740563273429871\n",
      "Epoch 2315, Loss: 2.074432998895645, Final Batch Loss: 0.2834915518760681\n",
      "Epoch 2316, Loss: 2.0664288103580475, Final Batch Loss: 0.4009513556957245\n",
      "Epoch 2317, Loss: 2.452425390481949, Final Batch Loss: 0.6240097880363464\n",
      "Epoch 2318, Loss: 2.185666650533676, Final Batch Loss: 0.47440773248672485\n",
      "Epoch 2319, Loss: 2.244455099105835, Final Batch Loss: 0.43955305218696594\n",
      "Epoch 2320, Loss: 2.2144806385040283, Final Batch Loss: 0.4538033604621887\n",
      "Epoch 2321, Loss: 2.2431540191173553, Final Batch Loss: 0.46723857522010803\n",
      "Epoch 2322, Loss: 2.1232895851135254, Final Batch Loss: 0.46886610984802246\n",
      "Epoch 2323, Loss: 1.8272562623023987, Final Batch Loss: 0.15681925415992737\n",
      "Epoch 2324, Loss: 1.848764330148697, Final Batch Loss: 0.16926920413970947\n",
      "Epoch 2325, Loss: 2.0549871027469635, Final Batch Loss: 0.4674072265625\n",
      "Epoch 2326, Loss: 1.9995976388454437, Final Batch Loss: 0.3520383834838867\n",
      "Epoch 2327, Loss: 1.7948464155197144, Final Batch Loss: 0.2469177544116974\n",
      "Epoch 2328, Loss: 1.876942217350006, Final Batch Loss: 0.2593551278114319\n",
      "Epoch 2329, Loss: 1.9649007320404053, Final Batch Loss: 0.26029208302497864\n",
      "Epoch 2330, Loss: 1.989956647157669, Final Batch Loss: 0.28876549005508423\n",
      "Epoch 2331, Loss: 2.2947844862937927, Final Batch Loss: 0.4527726471424103\n",
      "Epoch 2332, Loss: 2.4702743589878082, Final Batch Loss: 0.6904641389846802\n",
      "Epoch 2333, Loss: 1.8633053749799728, Final Batch Loss: 0.08363176882266998\n",
      "Epoch 2334, Loss: 2.331969976425171, Final Batch Loss: 0.6044958233833313\n",
      "Epoch 2335, Loss: 1.8082928508520126, Final Batch Loss: 0.16175033152103424\n",
      "Epoch 2336, Loss: 2.0324003994464874, Final Batch Loss: 0.40383628010749817\n",
      "Epoch 2337, Loss: 1.941871553659439, Final Batch Loss: 0.37000590562820435\n",
      "Epoch 2338, Loss: 2.3918831050395966, Final Batch Loss: 0.6533981561660767\n",
      "Epoch 2339, Loss: 1.9327907264232635, Final Batch Loss: 0.3252648711204529\n",
      "Epoch 2340, Loss: 2.372653156518936, Final Batch Loss: 0.6456366181373596\n",
      "Epoch 2341, Loss: 2.0857465863227844, Final Batch Loss: 0.4834297001361847\n",
      "Epoch 2342, Loss: 1.8819665908813477, Final Batch Loss: 0.2723718583583832\n",
      "Epoch 2343, Loss: 2.007069945335388, Final Batch Loss: 0.4383511245250702\n",
      "Epoch 2344, Loss: 2.04953870177269, Final Batch Loss: 0.47007274627685547\n",
      "Epoch 2345, Loss: 2.0049813091754913, Final Batch Loss: 0.4060918390750885\n",
      "Epoch 2346, Loss: 2.1894545555114746, Final Batch Loss: 0.5204448699951172\n",
      "Epoch 2347, Loss: 2.1960955560207367, Final Batch Loss: 0.4700252115726471\n",
      "Epoch 2348, Loss: 1.9437764883041382, Final Batch Loss: 0.2573280334472656\n",
      "Epoch 2349, Loss: 2.647191673517227, Final Batch Loss: 1.0230203866958618\n",
      "Epoch 2350, Loss: 2.2234140932559967, Final Batch Loss: 0.5736191868782043\n",
      "Epoch 2351, Loss: 2.5388140976428986, Final Batch Loss: 0.8365097641944885\n",
      "Epoch 2352, Loss: 2.249351888895035, Final Batch Loss: 0.5555201768875122\n",
      "Epoch 2353, Loss: 1.9548797011375427, Final Batch Loss: 0.15940284729003906\n",
      "Epoch 2354, Loss: 1.944451242685318, Final Batch Loss: 0.3907773196697235\n",
      "Epoch 2355, Loss: 2.0208060145378113, Final Batch Loss: 0.30776742100715637\n",
      "Epoch 2356, Loss: 1.8248627334833145, Final Batch Loss: 0.11881642043590546\n",
      "Epoch 2357, Loss: 2.08004954457283, Final Batch Loss: 0.2564288377761841\n",
      "Epoch 2358, Loss: 2.094711810350418, Final Batch Loss: 0.41146501898765564\n",
      "Epoch 2359, Loss: 2.064157336950302, Final Batch Loss: 0.42668861150741577\n",
      "Epoch 2360, Loss: 2.1804187893867493, Final Batch Loss: 0.5238024592399597\n",
      "Epoch 2361, Loss: 2.115431398153305, Final Batch Loss: 0.4135853946208954\n",
      "Epoch 2362, Loss: 1.7151221334934235, Final Batch Loss: 0.22233310341835022\n",
      "Epoch 2363, Loss: 1.8564797043800354, Final Batch Loss: 0.3030500113964081\n",
      "Epoch 2364, Loss: 1.922095149755478, Final Batch Loss: 0.29495054483413696\n",
      "Epoch 2365, Loss: 2.0255337953567505, Final Batch Loss: 0.5428773760795593\n",
      "Epoch 2366, Loss: 2.0087077617645264, Final Batch Loss: 0.39048901200294495\n",
      "Epoch 2367, Loss: 2.249552935361862, Final Batch Loss: 0.6114019155502319\n",
      "Epoch 2368, Loss: 1.7710294723510742, Final Batch Loss: 0.1367253065109253\n",
      "Epoch 2369, Loss: 2.2126801311969757, Final Batch Loss: 0.5160970091819763\n",
      "Epoch 2370, Loss: 2.0595327019691467, Final Batch Loss: 0.4512162208557129\n",
      "Epoch 2371, Loss: 1.9709398746490479, Final Batch Loss: 0.35094577074050903\n",
      "Epoch 2372, Loss: 2.2109686136245728, Final Batch Loss: 0.6180065870285034\n",
      "Epoch 2373, Loss: 1.8680860102176666, Final Batch Loss: 0.2706628441810608\n",
      "Epoch 2374, Loss: 1.7738615423440933, Final Batch Loss: 0.2019685059785843\n",
      "Epoch 2375, Loss: 1.7966532558202744, Final Batch Loss: 0.1281307488679886\n",
      "Epoch 2376, Loss: 2.278731405735016, Final Batch Loss: 0.5775794386863708\n",
      "Epoch 2377, Loss: 2.162387192249298, Final Batch Loss: 0.6237100958824158\n",
      "Epoch 2378, Loss: 1.9369931817054749, Final Batch Loss: 0.29982948303222656\n",
      "Epoch 2379, Loss: 1.9275726228952408, Final Batch Loss: 0.20382343232631683\n",
      "Epoch 2380, Loss: 1.7871776670217514, Final Batch Loss: 0.18055640161037445\n",
      "Epoch 2381, Loss: 1.8158681690692902, Final Batch Loss: 0.24392881989479065\n",
      "Epoch 2382, Loss: 1.9149423986673355, Final Batch Loss: 0.24648787081241608\n",
      "Epoch 2383, Loss: 2.0515318512916565, Final Batch Loss: 0.30199506878852844\n",
      "Epoch 2384, Loss: 2.9134354889392853, Final Batch Loss: 1.1478006839752197\n",
      "Epoch 2385, Loss: 1.929362803697586, Final Batch Loss: 0.3495195209980011\n",
      "Epoch 2386, Loss: 1.9202545210719109, Final Batch Loss: 0.08268933743238449\n",
      "Epoch 2387, Loss: 1.8378674983978271, Final Batch Loss: 0.17098411917686462\n",
      "Epoch 2388, Loss: 2.216702491044998, Final Batch Loss: 0.5629803538322449\n",
      "Epoch 2389, Loss: 1.7681902348995209, Final Batch Loss: 0.13278129696846008\n",
      "Epoch 2390, Loss: 1.9820063412189484, Final Batch Loss: 0.32998794317245483\n",
      "Epoch 2391, Loss: 2.3797324001789093, Final Batch Loss: 0.6908769607543945\n",
      "Epoch 2392, Loss: 1.960388332605362, Final Batch Loss: 0.33470940589904785\n",
      "Epoch 2393, Loss: 2.094110459089279, Final Batch Loss: 0.4630689322948456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2394, Loss: 2.6150615513324738, Final Batch Loss: 0.9839878082275391\n",
      "Epoch 2395, Loss: 2.097316265106201, Final Batch Loss: 0.36060211062431335\n",
      "Epoch 2396, Loss: 2.221217304468155, Final Batch Loss: 0.5301672220230103\n",
      "Epoch 2397, Loss: 2.0750701129436493, Final Batch Loss: 0.24041175842285156\n",
      "Epoch 2398, Loss: 1.9610023647546768, Final Batch Loss: 0.23156292736530304\n",
      "Epoch 2399, Loss: 1.9589523524045944, Final Batch Loss: 0.1803116351366043\n",
      "Epoch 2400, Loss: 2.0465650856494904, Final Batch Loss: 0.370246559381485\n",
      "Epoch 2401, Loss: 2.020585298538208, Final Batch Loss: 0.38758185505867004\n",
      "Epoch 2402, Loss: 1.9427394568920135, Final Batch Loss: 0.3280128836631775\n",
      "Epoch 2403, Loss: 1.8529632687568665, Final Batch Loss: 0.2821286618709564\n",
      "Epoch 2404, Loss: 1.829059511423111, Final Batch Loss: 0.23603889346122742\n",
      "Epoch 2405, Loss: 1.8787650614976883, Final Batch Loss: 0.22500617802143097\n",
      "Epoch 2406, Loss: 2.084884852170944, Final Batch Loss: 0.3970238268375397\n",
      "Epoch 2407, Loss: 2.0740866363048553, Final Batch Loss: 0.48662251234054565\n",
      "Epoch 2408, Loss: 1.796085998415947, Final Batch Loss: 0.15970103442668915\n",
      "Epoch 2409, Loss: 2.0022829473018646, Final Batch Loss: 0.4028645157814026\n",
      "Epoch 2410, Loss: 2.140188068151474, Final Batch Loss: 0.521725594997406\n",
      "Epoch 2411, Loss: 2.032648652791977, Final Batch Loss: 0.469636470079422\n",
      "Epoch 2412, Loss: 1.907349854707718, Final Batch Loss: 0.22471371293067932\n",
      "Epoch 2413, Loss: 2.3123346865177155, Final Batch Loss: 0.7346311211585999\n",
      "Epoch 2414, Loss: 1.9949665665626526, Final Batch Loss: 0.43993982672691345\n",
      "Epoch 2415, Loss: 1.960215002298355, Final Batch Loss: 0.2502806782722473\n",
      "Epoch 2416, Loss: 2.165666788816452, Final Batch Loss: 0.3887335956096649\n",
      "Epoch 2417, Loss: 1.9348623305559158, Final Batch Loss: 0.19352097809314728\n",
      "Epoch 2418, Loss: 1.620047777891159, Final Batch Loss: 0.10583335161209106\n",
      "Epoch 2419, Loss: 1.9284208416938782, Final Batch Loss: 0.35980167984962463\n",
      "Epoch 2420, Loss: 2.001460015773773, Final Batch Loss: 0.35500195622444153\n",
      "Epoch 2421, Loss: 2.472065806388855, Final Batch Loss: 0.7002584338188171\n",
      "Epoch 2422, Loss: 1.7650974541902542, Final Batch Loss: 0.22302691638469696\n",
      "Epoch 2423, Loss: 2.1039934158325195, Final Batch Loss: 0.5802497863769531\n",
      "Epoch 2424, Loss: 1.8856047987937927, Final Batch Loss: 0.2966530919075012\n",
      "Epoch 2425, Loss: 1.999624103307724, Final Batch Loss: 0.3483269512653351\n",
      "Epoch 2426, Loss: 1.7722718715667725, Final Batch Loss: 0.1338072419166565\n",
      "Epoch 2427, Loss: 1.9323361217975616, Final Batch Loss: 0.24245736002922058\n",
      "Epoch 2428, Loss: 2.122802585363388, Final Batch Loss: 0.520267903804779\n",
      "Epoch 2429, Loss: 1.8527054190635681, Final Batch Loss: 0.20714405179023743\n",
      "Epoch 2430, Loss: 1.9658743739128113, Final Batch Loss: 0.36149653792381287\n",
      "Epoch 2431, Loss: 1.974219173192978, Final Batch Loss: 0.3812837600708008\n",
      "Epoch 2432, Loss: 2.6948863565921783, Final Batch Loss: 1.1230837106704712\n",
      "Epoch 2433, Loss: 2.052096903324127, Final Batch Loss: 0.4277925491333008\n",
      "Epoch 2434, Loss: 2.1518858075141907, Final Batch Loss: 0.4111853539943695\n",
      "Epoch 2435, Loss: 2.532140225172043, Final Batch Loss: 0.8470370173454285\n",
      "Epoch 2436, Loss: 2.070377856492996, Final Batch Loss: 0.46517303586006165\n",
      "Epoch 2437, Loss: 2.6049995124340057, Final Batch Loss: 0.9321654438972473\n",
      "Epoch 2438, Loss: 2.048647403717041, Final Batch Loss: 0.21170899271965027\n",
      "Epoch 2439, Loss: 2.2880174219608307, Final Batch Loss: 0.5905911326408386\n",
      "Epoch 2440, Loss: 2.3117392659187317, Final Batch Loss: 0.5718012452125549\n",
      "Epoch 2441, Loss: 2.062446415424347, Final Batch Loss: 0.3944106101989746\n",
      "Epoch 2442, Loss: 2.4354920089244843, Final Batch Loss: 0.5956627130508423\n",
      "Epoch 2443, Loss: 2.695050358772278, Final Batch Loss: 0.969344973564148\n",
      "Epoch 2444, Loss: 1.8189067393541336, Final Batch Loss: 0.12314797937870026\n",
      "Epoch 2445, Loss: 1.9927353262901306, Final Batch Loss: 0.30086269974708557\n",
      "Epoch 2446, Loss: 1.9031035900115967, Final Batch Loss: 0.34500738978385925\n",
      "Epoch 2447, Loss: 1.8462739139795303, Final Batch Loss: 0.24780048429965973\n",
      "Epoch 2448, Loss: 1.9094441086053848, Final Batch Loss: 0.243827685713768\n",
      "Epoch 2449, Loss: 1.999252438545227, Final Batch Loss: 0.39145389199256897\n",
      "Epoch 2450, Loss: 2.176930248737335, Final Batch Loss: 0.5516706109046936\n",
      "Epoch 2451, Loss: 2.105769693851471, Final Batch Loss: 0.412783682346344\n",
      "Epoch 2452, Loss: 2.2036163806915283, Final Batch Loss: 0.4783327877521515\n",
      "Epoch 2453, Loss: 2.060081720352173, Final Batch Loss: 0.4490860104560852\n",
      "Epoch 2454, Loss: 1.8844496607780457, Final Batch Loss: 0.27083003520965576\n",
      "Epoch 2455, Loss: 1.6317856907844543, Final Batch Loss: 0.13669845461845398\n",
      "Epoch 2456, Loss: 2.360122948884964, Final Batch Loss: 0.5928777456283569\n",
      "Epoch 2457, Loss: 2.1009577214717865, Final Batch Loss: 0.44819512963294983\n",
      "Epoch 2458, Loss: 2.2465502619743347, Final Batch Loss: 0.5478088855743408\n",
      "Epoch 2459, Loss: 2.20027819275856, Final Batch Loss: 0.5083959698677063\n",
      "Epoch 2460, Loss: 1.9541709423065186, Final Batch Loss: 0.32445669174194336\n",
      "Epoch 2461, Loss: 2.089428037405014, Final Batch Loss: 0.4871293008327484\n",
      "Epoch 2462, Loss: 2.1222915649414062, Final Batch Loss: 0.49283578991889954\n",
      "Epoch 2463, Loss: 2.305268347263336, Final Batch Loss: 0.5092867612838745\n",
      "Epoch 2464, Loss: 2.2592507004737854, Final Batch Loss: 0.5960750579833984\n",
      "Epoch 2465, Loss: 1.8886182606220245, Final Batch Loss: 0.2886340916156769\n",
      "Epoch 2466, Loss: 2.2066226303577423, Final Batch Loss: 0.6202677488327026\n",
      "Epoch 2467, Loss: 2.1057505309581757, Final Batch Loss: 0.4156201481819153\n",
      "Epoch 2468, Loss: 1.8072843253612518, Final Batch Loss: 0.14228662848472595\n",
      "Epoch 2469, Loss: 2.2761531472206116, Final Batch Loss: 0.584587574005127\n",
      "Epoch 2470, Loss: 2.0549939572811127, Final Batch Loss: 0.32571879029273987\n",
      "Epoch 2471, Loss: 2.0799393355846405, Final Batch Loss: 0.5700349807739258\n",
      "Epoch 2472, Loss: 2.530313938856125, Final Batch Loss: 0.8687869310379028\n",
      "Epoch 2473, Loss: 2.682725965976715, Final Batch Loss: 1.076685905456543\n",
      "Epoch 2474, Loss: 1.9057410508394241, Final Batch Loss: 0.22576938569545746\n",
      "Epoch 2475, Loss: 2.406516581773758, Final Batch Loss: 0.4389847218990326\n",
      "Epoch 2476, Loss: 2.199760675430298, Final Batch Loss: 0.5841067433357239\n",
      "Epoch 2477, Loss: 2.0562198758125305, Final Batch Loss: 0.3692210018634796\n",
      "Epoch 2478, Loss: 2.1238843500614166, Final Batch Loss: 0.43473440408706665\n",
      "Epoch 2479, Loss: 2.2433336973190308, Final Batch Loss: 0.6448361277580261\n",
      "Epoch 2480, Loss: 2.2147654592990875, Final Batch Loss: 0.378868967294693\n",
      "Epoch 2481, Loss: 2.327684074640274, Final Batch Loss: 0.43689361214637756\n",
      "Epoch 2482, Loss: 1.886405885219574, Final Batch Loss: 0.2539612650871277\n",
      "Epoch 2483, Loss: 2.2253579795360565, Final Batch Loss: 0.5453731417655945\n",
      "Epoch 2484, Loss: 2.1991496086120605, Final Batch Loss: 0.5732845067977905\n",
      "Epoch 2485, Loss: 1.9945678561925888, Final Batch Loss: 0.19619758427143097\n",
      "Epoch 2486, Loss: 2.0648208558559418, Final Batch Loss: 0.3327954411506653\n",
      "Epoch 2487, Loss: 2.0138875544071198, Final Batch Loss: 0.30051904916763306\n",
      "Epoch 2488, Loss: 2.051533192396164, Final Batch Loss: 0.40637969970703125\n",
      "Epoch 2489, Loss: 2.050823003053665, Final Batch Loss: 0.5148417949676514\n",
      "Epoch 2490, Loss: 1.8227862268686295, Final Batch Loss: 0.22202090919017792\n",
      "Epoch 2491, Loss: 2.3690044283866882, Final Batch Loss: 0.6618320345878601\n",
      "Epoch 2492, Loss: 1.9905093908309937, Final Batch Loss: 0.42252638936042786\n",
      "Epoch 2493, Loss: 2.1548112630844116, Final Batch Loss: 0.4858546555042267\n",
      "Epoch 2494, Loss: 2.3711216151714325, Final Batch Loss: 0.8344488143920898\n",
      "Epoch 2495, Loss: 1.9614944458007812, Final Batch Loss: 0.2750907242298126\n",
      "Epoch 2496, Loss: 2.008682757616043, Final Batch Loss: 0.3449273109436035\n",
      "Epoch 2497, Loss: 1.8596590757369995, Final Batch Loss: 0.2709774076938629\n",
      "Epoch 2498, Loss: 1.921138346195221, Final Batch Loss: 0.3191297650337219\n",
      "Epoch 2499, Loss: 2.189672827720642, Final Batch Loss: 0.43299204111099243\n",
      "Epoch 2500, Loss: 2.4291091561317444, Final Batch Loss: 0.7985503077507019\n",
      "Epoch 2501, Loss: 2.0783328115940094, Final Batch Loss: 0.4561360776424408\n",
      "Epoch 2502, Loss: 1.9047967195510864, Final Batch Loss: 0.3374497592449188\n",
      "Epoch 2503, Loss: 2.021561622619629, Final Batch Loss: 0.39013436436653137\n",
      "Epoch 2504, Loss: 2.010233998298645, Final Batch Loss: 0.30900976061820984\n",
      "Epoch 2505, Loss: 2.1397828459739685, Final Batch Loss: 0.4798232614994049\n",
      "Epoch 2506, Loss: 1.9358918964862823, Final Batch Loss: 0.35286521911621094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2507, Loss: 1.9062752425670624, Final Batch Loss: 0.34923386573791504\n",
      "Epoch 2508, Loss: 2.1335874497890472, Final Batch Loss: 0.44544264674186707\n",
      "Epoch 2509, Loss: 1.9564216136932373, Final Batch Loss: 0.32691410183906555\n",
      "Epoch 2510, Loss: 1.949002891778946, Final Batch Loss: 0.26919445395469666\n",
      "Epoch 2511, Loss: 1.9898816347122192, Final Batch Loss: 0.41724762320518494\n",
      "Epoch 2512, Loss: 1.6938487514853477, Final Batch Loss: 0.08276604861021042\n",
      "Epoch 2513, Loss: 2.167422890663147, Final Batch Loss: 0.5223144888877869\n",
      "Epoch 2514, Loss: 2.082979679107666, Final Batch Loss: 0.4340454638004303\n",
      "Epoch 2515, Loss: 2.332226812839508, Final Batch Loss: 0.6638995409011841\n",
      "Epoch 2516, Loss: 1.8497570306062698, Final Batch Loss: 0.19942839443683624\n",
      "Epoch 2517, Loss: 2.0752953588962555, Final Batch Loss: 0.4433809816837311\n",
      "Epoch 2518, Loss: 1.736672393977642, Final Batch Loss: 0.11307819932699203\n",
      "Epoch 2519, Loss: 1.9312059581279755, Final Batch Loss: 0.2543948292732239\n",
      "Epoch 2520, Loss: 1.8537627160549164, Final Batch Loss: 0.2744958996772766\n",
      "Epoch 2521, Loss: 2.1029804944992065, Final Batch Loss: 0.42797574400901794\n",
      "Epoch 2522, Loss: 2.1905388236045837, Final Batch Loss: 0.5686490535736084\n",
      "Epoch 2523, Loss: 2.4384784400463104, Final Batch Loss: 0.7720755338668823\n",
      "Epoch 2524, Loss: 2.00106480717659, Final Batch Loss: 0.3486763536930084\n",
      "Epoch 2525, Loss: 2.5016574561595917, Final Batch Loss: 0.7784175276756287\n",
      "Epoch 2526, Loss: 1.9650029689073563, Final Batch Loss: 0.17414383590221405\n",
      "Epoch 2527, Loss: 2.375887840986252, Final Batch Loss: 0.7146569490432739\n",
      "Epoch 2528, Loss: 2.080798953771591, Final Batch Loss: 0.4559535086154938\n",
      "Epoch 2529, Loss: 2.2598371505737305, Final Batch Loss: 0.5938282608985901\n",
      "Epoch 2530, Loss: 1.9827637076377869, Final Batch Loss: 0.2750553488731384\n",
      "Epoch 2531, Loss: 2.035685747861862, Final Batch Loss: 0.44905465841293335\n",
      "Epoch 2532, Loss: 2.044198602437973, Final Batch Loss: 0.3547708988189697\n",
      "Epoch 2533, Loss: 1.9642791450023651, Final Batch Loss: 0.4117662310600281\n",
      "Epoch 2534, Loss: 1.9799765348434448, Final Batch Loss: 0.4071756899356842\n",
      "Epoch 2535, Loss: 1.8054079711437225, Final Batch Loss: 0.18430480360984802\n",
      "Epoch 2536, Loss: 1.8887252509593964, Final Batch Loss: 0.2523123323917389\n",
      "Epoch 2537, Loss: 3.0058396458625793, Final Batch Loss: 1.4498261213302612\n",
      "Epoch 2538, Loss: 1.910647988319397, Final Batch Loss: 0.361163467168808\n",
      "Epoch 2539, Loss: 2.416246682405472, Final Batch Loss: 0.8237360715866089\n",
      "Epoch 2540, Loss: 1.7821796536445618, Final Batch Loss: 0.16804370284080505\n",
      "Epoch 2541, Loss: 1.934204250574112, Final Batch Loss: 0.2779686748981476\n",
      "Epoch 2542, Loss: 2.204421252012253, Final Batch Loss: 0.6715826988220215\n",
      "Epoch 2543, Loss: 1.977154165506363, Final Batch Loss: 0.35527482628822327\n",
      "Epoch 2544, Loss: 1.928398221731186, Final Batch Loss: 0.41365739703178406\n",
      "Epoch 2545, Loss: 1.863013356924057, Final Batch Loss: 0.26172277331352234\n",
      "Epoch 2546, Loss: 2.1737139523029327, Final Batch Loss: 0.6228870749473572\n",
      "Epoch 2547, Loss: 1.8068092912435532, Final Batch Loss: 0.21183426678180695\n",
      "Epoch 2548, Loss: 2.083663046360016, Final Batch Loss: 0.41655465960502625\n",
      "Epoch 2549, Loss: 2.1986733078956604, Final Batch Loss: 0.672269880771637\n",
      "Epoch 2550, Loss: 1.9680156409740448, Final Batch Loss: 0.4395815432071686\n",
      "Epoch 2551, Loss: 1.8576627671718597, Final Batch Loss: 0.2236340343952179\n",
      "Epoch 2552, Loss: 2.0827819406986237, Final Batch Loss: 0.5462306141853333\n",
      "Epoch 2553, Loss: 2.0103947818279266, Final Batch Loss: 0.4060843586921692\n",
      "Epoch 2554, Loss: 2.0567329227924347, Final Batch Loss: 0.5070244669914246\n",
      "Epoch 2555, Loss: 2.042591780424118, Final Batch Loss: 0.33808207511901855\n",
      "Epoch 2556, Loss: 2.009512782096863, Final Batch Loss: 0.4106464684009552\n",
      "Epoch 2557, Loss: 2.5769284069538116, Final Batch Loss: 1.0597305297851562\n",
      "Epoch 2558, Loss: 2.161429286003113, Final Batch Loss: 0.6622456312179565\n",
      "Epoch 2559, Loss: 2.2488285303115845, Final Batch Loss: 0.7221318483352661\n",
      "Epoch 2560, Loss: 2.1391889452934265, Final Batch Loss: 0.5375512838363647\n",
      "Epoch 2561, Loss: 1.9684979021549225, Final Batch Loss: 0.35384005308151245\n",
      "Epoch 2562, Loss: 2.0065809190273285, Final Batch Loss: 0.41046372056007385\n",
      "Epoch 2563, Loss: 1.7968982756137848, Final Batch Loss: 0.25099989771842957\n",
      "Epoch 2564, Loss: 1.9850139617919922, Final Batch Loss: 0.42438802123069763\n",
      "Epoch 2565, Loss: 1.818035900592804, Final Batch Loss: 0.32305973768234253\n",
      "Epoch 2566, Loss: 2.007395416498184, Final Batch Loss: 0.43439218401908875\n",
      "Epoch 2567, Loss: 2.37491911649704, Final Batch Loss: 0.7038300633430481\n",
      "Epoch 2568, Loss: 1.6794848144054413, Final Batch Loss: 0.18395602703094482\n",
      "Epoch 2569, Loss: 1.7723924219608307, Final Batch Loss: 0.25320500135421753\n",
      "Epoch 2570, Loss: 1.975025475025177, Final Batch Loss: 0.3577289879322052\n",
      "Epoch 2571, Loss: 1.9709333777427673, Final Batch Loss: 0.41205736994743347\n",
      "Epoch 2572, Loss: 1.9236276149749756, Final Batch Loss: 0.2692120373249054\n",
      "Epoch 2573, Loss: 2.3753783106803894, Final Batch Loss: 0.8473444581031799\n",
      "Epoch 2574, Loss: 1.9718310236930847, Final Batch Loss: 0.44525009393692017\n",
      "Epoch 2575, Loss: 1.9781518578529358, Final Batch Loss: 0.30887433886528015\n",
      "Epoch 2576, Loss: 2.14626607298851, Final Batch Loss: 0.5202541947364807\n",
      "Epoch 2577, Loss: 1.740586295723915, Final Batch Loss: 0.08010970056056976\n",
      "Epoch 2578, Loss: 1.954811304807663, Final Batch Loss: 0.3805674612522125\n",
      "Epoch 2579, Loss: 2.311335861682892, Final Batch Loss: 0.7757997512817383\n",
      "Epoch 2580, Loss: 1.8918797075748444, Final Batch Loss: 0.2793895900249481\n",
      "Epoch 2581, Loss: 1.7958826571702957, Final Batch Loss: 0.1638440042734146\n",
      "Epoch 2582, Loss: 1.8927437365055084, Final Batch Loss: 0.2633328437805176\n",
      "Epoch 2583, Loss: 1.834657371044159, Final Batch Loss: 0.2857911288738251\n",
      "Epoch 2584, Loss: 2.1202293038368225, Final Batch Loss: 0.4658544063568115\n",
      "Epoch 2585, Loss: 1.9048153162002563, Final Batch Loss: 0.44983819127082825\n",
      "Epoch 2586, Loss: 2.0711967051029205, Final Batch Loss: 0.4784070551395416\n",
      "Epoch 2587, Loss: 1.7558173090219498, Final Batch Loss: 0.2032901495695114\n",
      "Epoch 2588, Loss: 1.8146087229251862, Final Batch Loss: 0.33289432525634766\n",
      "Epoch 2589, Loss: 1.9858592450618744, Final Batch Loss: 0.4500558078289032\n",
      "Epoch 2590, Loss: 2.0049646198749542, Final Batch Loss: 0.3755323588848114\n",
      "Epoch 2591, Loss: 2.0571093261241913, Final Batch Loss: 0.5377371907234192\n",
      "Epoch 2592, Loss: 1.8153538703918457, Final Batch Loss: 0.3171464800834656\n",
      "Epoch 2593, Loss: 1.9938051402568817, Final Batch Loss: 0.3294403553009033\n",
      "Epoch 2594, Loss: 2.05174258351326, Final Batch Loss: 0.40913626551628113\n",
      "Epoch 2595, Loss: 1.6202563904225826, Final Batch Loss: 0.03033098205924034\n",
      "Epoch 2596, Loss: 1.9822945296764374, Final Batch Loss: 0.32513779401779175\n",
      "Epoch 2597, Loss: 1.939854621887207, Final Batch Loss: 0.39941301941871643\n",
      "Epoch 2598, Loss: 1.9493896663188934, Final Batch Loss: 0.42802897095680237\n",
      "Epoch 2599, Loss: 2.0175771713256836, Final Batch Loss: 0.407012015581131\n",
      "Epoch 2600, Loss: 1.7910840213298798, Final Batch Loss: 0.23813775181770325\n",
      "Epoch 2601, Loss: 1.9626505076885223, Final Batch Loss: 0.3351910412311554\n",
      "Epoch 2602, Loss: 2.1587206721305847, Final Batch Loss: 0.5171330571174622\n",
      "Epoch 2603, Loss: 2.2147983014583588, Final Batch Loss: 0.5886784195899963\n",
      "Epoch 2604, Loss: 2.2770255506038666, Final Batch Loss: 0.44766807556152344\n",
      "Epoch 2605, Loss: 2.0522173941135406, Final Batch Loss: 0.49263057112693787\n",
      "Epoch 2606, Loss: 1.8251663148403168, Final Batch Loss: 0.20070108771324158\n",
      "Epoch 2607, Loss: 1.9154126346111298, Final Batch Loss: 0.20938816666603088\n",
      "Epoch 2608, Loss: 2.088693231344223, Final Batch Loss: 0.43992820382118225\n",
      "Epoch 2609, Loss: 2.0366160571575165, Final Batch Loss: 0.4778389632701874\n",
      "Epoch 2610, Loss: 1.8617124557495117, Final Batch Loss: 0.31026577949523926\n",
      "Epoch 2611, Loss: 1.9484914243221283, Final Batch Loss: 0.3675616681575775\n",
      "Epoch 2612, Loss: 1.93084055185318, Final Batch Loss: 0.4279458522796631\n",
      "Epoch 2613, Loss: 1.9025033116340637, Final Batch Loss: 0.27757760882377625\n",
      "Epoch 2614, Loss: 2.0422519147396088, Final Batch Loss: 0.45874837040901184\n",
      "Epoch 2615, Loss: 2.306837648153305, Final Batch Loss: 0.6969744563102722\n",
      "Epoch 2616, Loss: 1.9610845446586609, Final Batch Loss: 0.3513152301311493\n",
      "Epoch 2617, Loss: 2.080086976289749, Final Batch Loss: 0.47973671555519104\n",
      "Epoch 2618, Loss: 1.8918024599552155, Final Batch Loss: 0.16287466883659363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2619, Loss: 1.6940953060984612, Final Batch Loss: 0.09826614707708359\n",
      "Epoch 2620, Loss: 1.9449054598808289, Final Batch Loss: 0.44621047377586365\n",
      "Epoch 2621, Loss: 1.8850726187229156, Final Batch Loss: 0.45321378111839294\n",
      "Epoch 2622, Loss: 1.905201107263565, Final Batch Loss: 0.2764430046081543\n",
      "Epoch 2623, Loss: 1.991724044084549, Final Batch Loss: 0.46803969144821167\n",
      "Epoch 2624, Loss: 2.044481724500656, Final Batch Loss: 0.4769309461116791\n",
      "Epoch 2625, Loss: 1.7836255729198456, Final Batch Loss: 0.3265838921070099\n",
      "Epoch 2626, Loss: 1.6813192516565323, Final Batch Loss: 0.17332400381565094\n",
      "Epoch 2627, Loss: 1.9665555953979492, Final Batch Loss: 0.34994062781333923\n",
      "Epoch 2628, Loss: 2.0952526330947876, Final Batch Loss: 0.5285928845405579\n",
      "Epoch 2629, Loss: 2.1735468804836273, Final Batch Loss: 0.6174405217170715\n",
      "Epoch 2630, Loss: 1.6623397469520569, Final Batch Loss: 0.1352774202823639\n",
      "Epoch 2631, Loss: 1.779154870659113, Final Batch Loss: 0.05127403512597084\n",
      "Epoch 2632, Loss: 2.058969557285309, Final Batch Loss: 0.4773317277431488\n",
      "Epoch 2633, Loss: 1.8844265341758728, Final Batch Loss: 0.2714904248714447\n",
      "Epoch 2634, Loss: 1.9288634955883026, Final Batch Loss: 0.2694862186908722\n",
      "Epoch 2635, Loss: 1.8159688115119934, Final Batch Loss: 0.27929019927978516\n",
      "Epoch 2636, Loss: 2.0929302275180817, Final Batch Loss: 0.5585489869117737\n",
      "Epoch 2637, Loss: 1.9349151849746704, Final Batch Loss: 0.3523409962654114\n",
      "Epoch 2638, Loss: 1.9152288734912872, Final Batch Loss: 0.29743289947509766\n",
      "Epoch 2639, Loss: 2.280826300382614, Final Batch Loss: 0.7072427868843079\n",
      "Epoch 2640, Loss: 1.730140745639801, Final Batch Loss: 0.22779250144958496\n",
      "Epoch 2641, Loss: 2.2749364376068115, Final Batch Loss: 0.704288125038147\n",
      "Epoch 2642, Loss: 2.1017898321151733, Final Batch Loss: 0.576571524143219\n",
      "Epoch 2643, Loss: 1.8353550732135773, Final Batch Loss: 0.37943461537361145\n",
      "Epoch 2644, Loss: 1.977220356464386, Final Batch Loss: 0.4375285506248474\n",
      "Epoch 2645, Loss: 2.132088840007782, Final Batch Loss: 0.4831147789955139\n",
      "Epoch 2646, Loss: 2.0640138685703278, Final Batch Loss: 0.5213195085525513\n",
      "Epoch 2647, Loss: 1.8452949821949005, Final Batch Loss: 0.40349721908569336\n",
      "Epoch 2648, Loss: 1.9421878457069397, Final Batch Loss: 0.4231331944465637\n",
      "Epoch 2649, Loss: 2.186806708574295, Final Batch Loss: 0.6274107694625854\n",
      "Epoch 2650, Loss: 1.8893147557973862, Final Batch Loss: 0.18541301786899567\n",
      "Epoch 2651, Loss: 2.2032156884670258, Final Batch Loss: 0.6546044945716858\n",
      "Epoch 2652, Loss: 1.9699353575706482, Final Batch Loss: 0.4880995750427246\n",
      "Epoch 2653, Loss: 1.7976851910352707, Final Batch Loss: 0.23100866377353668\n",
      "Epoch 2654, Loss: 1.7985852360725403, Final Batch Loss: 0.3093310296535492\n",
      "Epoch 2655, Loss: 2.048254668712616, Final Batch Loss: 0.6273724436759949\n",
      "Epoch 2656, Loss: 1.943653017282486, Final Batch Loss: 0.39185744524002075\n",
      "Epoch 2657, Loss: 2.0379169434309006, Final Batch Loss: 0.2441759556531906\n",
      "Epoch 2658, Loss: 2.115795522928238, Final Batch Loss: 0.427011102437973\n",
      "Epoch 2659, Loss: 1.9051089584827423, Final Batch Loss: 0.36215969920158386\n",
      "Epoch 2660, Loss: 2.08050474524498, Final Batch Loss: 0.5328769683837891\n",
      "Epoch 2661, Loss: 1.729852318763733, Final Batch Loss: 0.14493930339813232\n",
      "Epoch 2662, Loss: 1.9899784922599792, Final Batch Loss: 0.3326632082462311\n",
      "Epoch 2663, Loss: 2.2619901299476624, Final Batch Loss: 0.7773104310035706\n",
      "Epoch 2664, Loss: 1.9091433584690094, Final Batch Loss: 0.3322809040546417\n",
      "Epoch 2665, Loss: 2.208438813686371, Final Batch Loss: 0.6319199800491333\n",
      "Epoch 2666, Loss: 1.8389775604009628, Final Batch Loss: 0.14672191441059113\n",
      "Epoch 2667, Loss: 2.0764964520931244, Final Batch Loss: 0.35827258229255676\n",
      "Epoch 2668, Loss: 1.8119111359119415, Final Batch Loss: 0.13891908526420593\n",
      "Epoch 2669, Loss: 1.8473983407020569, Final Batch Loss: 0.2273179590702057\n",
      "Epoch 2670, Loss: 1.9404742121696472, Final Batch Loss: 0.31256309151649475\n",
      "Epoch 2671, Loss: 1.847300499677658, Final Batch Loss: 0.23825857043266296\n",
      "Epoch 2672, Loss: 2.3761046826839447, Final Batch Loss: 0.7780941724777222\n",
      "Epoch 2673, Loss: 1.8033922016620636, Final Batch Loss: 0.1419380009174347\n",
      "Epoch 2674, Loss: 1.733613908290863, Final Batch Loss: 0.1535618007183075\n",
      "Epoch 2675, Loss: 2.112331122159958, Final Batch Loss: 0.5403159856796265\n",
      "Epoch 2676, Loss: 1.9599408507347107, Final Batch Loss: 0.3029189109802246\n",
      "Epoch 2677, Loss: 1.7178300321102142, Final Batch Loss: 0.14095479249954224\n",
      "Epoch 2678, Loss: 1.8713570684194565, Final Batch Loss: 0.18910731375217438\n",
      "Epoch 2679, Loss: 2.1608192026615143, Final Batch Loss: 0.5060645937919617\n",
      "Epoch 2680, Loss: 1.7205598503351212, Final Batch Loss: 0.17642872035503387\n",
      "Epoch 2681, Loss: 1.9477238357067108, Final Batch Loss: 0.43290621042251587\n",
      "Epoch 2682, Loss: 2.084894210100174, Final Batch Loss: 0.5903263092041016\n",
      "Epoch 2683, Loss: 1.8214311003684998, Final Batch Loss: 0.3695283830165863\n",
      "Epoch 2684, Loss: 2.059463232755661, Final Batch Loss: 0.46618741750717163\n",
      "Epoch 2685, Loss: 2.307154595851898, Final Batch Loss: 0.6783828139305115\n",
      "Epoch 2686, Loss: 1.9176945984363556, Final Batch Loss: 0.37462902069091797\n",
      "Epoch 2687, Loss: 1.9139436483383179, Final Batch Loss: 0.2741832137107849\n",
      "Epoch 2688, Loss: 1.762527659535408, Final Batch Loss: 0.2497761994600296\n",
      "Epoch 2689, Loss: 1.7366123795509338, Final Batch Loss: 0.19770073890686035\n",
      "Epoch 2690, Loss: 2.0287186205387115, Final Batch Loss: 0.4457160532474518\n",
      "Epoch 2691, Loss: 2.6059957146644592, Final Batch Loss: 0.9083994626998901\n",
      "Epoch 2692, Loss: 1.9450511038303375, Final Batch Loss: 0.3966749310493469\n",
      "Epoch 2693, Loss: 1.9394002854824066, Final Batch Loss: 0.25640958547592163\n",
      "Epoch 2694, Loss: 2.1670447885990143, Final Batch Loss: 0.35786885023117065\n",
      "Epoch 2695, Loss: 2.4583079516887665, Final Batch Loss: 0.7410021424293518\n",
      "Epoch 2696, Loss: 1.861628070473671, Final Batch Loss: 0.2480372041463852\n",
      "Epoch 2697, Loss: 2.287335127592087, Final Batch Loss: 0.6593447327613831\n",
      "Epoch 2698, Loss: 1.8877893090248108, Final Batch Loss: 0.35706469416618347\n",
      "Epoch 2699, Loss: 1.915069729089737, Final Batch Loss: 0.20026186108589172\n",
      "Epoch 2700, Loss: 2.044671416282654, Final Batch Loss: 0.4022265076637268\n",
      "Epoch 2701, Loss: 2.1802925765514374, Final Batch Loss: 0.47532764077186584\n",
      "Epoch 2702, Loss: 2.000092476606369, Final Batch Loss: 0.30413201451301575\n",
      "Epoch 2703, Loss: 1.7928406894207, Final Batch Loss: 0.35049471259117126\n",
      "Epoch 2704, Loss: 2.1055576503276825, Final Batch Loss: 0.45806804299354553\n",
      "Epoch 2705, Loss: 1.816454291343689, Final Batch Loss: 0.21203234791755676\n",
      "Epoch 2706, Loss: 2.280865043401718, Final Batch Loss: 0.6510001420974731\n",
      "Epoch 2707, Loss: 1.9810201823711395, Final Batch Loss: 0.3934961259365082\n",
      "Epoch 2708, Loss: 1.720039963722229, Final Batch Loss: 0.21121475100517273\n",
      "Epoch 2709, Loss: 2.2392505407333374, Final Batch Loss: 0.6926542520523071\n",
      "Epoch 2710, Loss: 2.218960404396057, Final Batch Loss: 0.5593375563621521\n",
      "Epoch 2711, Loss: 2.2689693570137024, Final Batch Loss: 0.566351592540741\n",
      "Epoch 2712, Loss: 1.7961944788694382, Final Batch Loss: 0.14055152237415314\n",
      "Epoch 2713, Loss: 1.8801184296607971, Final Batch Loss: 0.34205397963523865\n",
      "Epoch 2714, Loss: 2.3296333849430084, Final Batch Loss: 0.7956839203834534\n",
      "Epoch 2715, Loss: 1.9116067439317703, Final Batch Loss: 0.23454730212688446\n",
      "Epoch 2716, Loss: 2.0243819653987885, Final Batch Loss: 0.3783470690250397\n",
      "Epoch 2717, Loss: 2.139729142189026, Final Batch Loss: 0.5867037177085876\n",
      "Epoch 2718, Loss: 1.9324585497379303, Final Batch Loss: 0.419432133436203\n",
      "Epoch 2719, Loss: 2.181594669818878, Final Batch Loss: 0.5416606068611145\n",
      "Epoch 2720, Loss: 1.6816268563270569, Final Batch Loss: 0.16525015234947205\n",
      "Epoch 2721, Loss: 1.7839038670063019, Final Batch Loss: 0.18291687965393066\n",
      "Epoch 2722, Loss: 2.0192131400108337, Final Batch Loss: 0.4391605257987976\n",
      "Epoch 2723, Loss: 1.9524759352207184, Final Batch Loss: 0.38594961166381836\n",
      "Epoch 2724, Loss: 1.842029720544815, Final Batch Loss: 0.29673293232917786\n",
      "Epoch 2725, Loss: 1.794293999671936, Final Batch Loss: 0.2987082004547119\n",
      "Epoch 2726, Loss: 1.887736976146698, Final Batch Loss: 0.30799970030784607\n",
      "Epoch 2727, Loss: 1.9066487550735474, Final Batch Loss: 0.2520613968372345\n",
      "Epoch 2728, Loss: 1.769791916012764, Final Batch Loss: 0.23691041767597198\n",
      "Epoch 2729, Loss: 2.1201059222221375, Final Batch Loss: 0.5362603664398193\n",
      "Epoch 2730, Loss: 2.239007383584976, Final Batch Loss: 0.6139653325080872\n",
      "Epoch 2731, Loss: 1.994786262512207, Final Batch Loss: 0.4525579810142517\n",
      "Epoch 2732, Loss: 1.970522791147232, Final Batch Loss: 0.2863568663597107\n",
      "Epoch 2733, Loss: 2.013953596353531, Final Batch Loss: 0.4407264292240143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2734, Loss: 1.9676395952701569, Final Batch Loss: 0.3745596408843994\n",
      "Epoch 2735, Loss: 2.0788058638572693, Final Batch Loss: 0.44391968846321106\n",
      "Epoch 2736, Loss: 2.1881821751594543, Final Batch Loss: 0.6615462899208069\n",
      "Epoch 2737, Loss: 1.897524744272232, Final Batch Loss: 0.3911585211753845\n",
      "Epoch 2738, Loss: 1.8550027310848236, Final Batch Loss: 0.3414137363433838\n",
      "Epoch 2739, Loss: 1.7555465027689934, Final Batch Loss: 0.10846801847219467\n",
      "Epoch 2740, Loss: 1.89923757314682, Final Batch Loss: 0.39155882596969604\n",
      "Epoch 2741, Loss: 1.871328592300415, Final Batch Loss: 0.34956058859825134\n",
      "Epoch 2742, Loss: 1.6118585243821144, Final Batch Loss: 0.06543765217065811\n",
      "Epoch 2743, Loss: 1.8559861183166504, Final Batch Loss: 0.3280427157878876\n",
      "Epoch 2744, Loss: 1.7136452570557594, Final Batch Loss: 0.06140131503343582\n",
      "Epoch 2745, Loss: 1.870427668094635, Final Batch Loss: 0.327888160943985\n",
      "Epoch 2746, Loss: 2.029688835144043, Final Batch Loss: 0.4533940255641937\n",
      "Epoch 2747, Loss: 1.9752879738807678, Final Batch Loss: 0.46654561161994934\n",
      "Epoch 2748, Loss: 1.6237697750329971, Final Batch Loss: 0.14048729836940765\n",
      "Epoch 2749, Loss: 1.5595920085906982, Final Batch Loss: 0.16281819343566895\n",
      "Epoch 2750, Loss: 1.8966792523860931, Final Batch Loss: 0.46990150213241577\n",
      "Epoch 2751, Loss: 1.787955716252327, Final Batch Loss: 0.2198888510465622\n",
      "Epoch 2752, Loss: 2.0717385709285736, Final Batch Loss: 0.49514564871788025\n",
      "Epoch 2753, Loss: 1.8812121450901031, Final Batch Loss: 0.3952164947986603\n",
      "Epoch 2754, Loss: 1.7380366623401642, Final Batch Loss: 0.13124585151672363\n",
      "Epoch 2755, Loss: 2.205820471048355, Final Batch Loss: 0.6831108331680298\n",
      "Epoch 2756, Loss: 2.1245976388454437, Final Batch Loss: 0.6886724233627319\n",
      "Epoch 2757, Loss: 1.787267953157425, Final Batch Loss: 0.3491368889808655\n",
      "Epoch 2758, Loss: 2.2017521262168884, Final Batch Loss: 0.6054192781448364\n",
      "Epoch 2759, Loss: 1.8061398565769196, Final Batch Loss: 0.16647085547447205\n",
      "Epoch 2760, Loss: 1.790875494480133, Final Batch Loss: 0.24217119812965393\n",
      "Epoch 2761, Loss: 2.1230233907699585, Final Batch Loss: 0.3585260510444641\n",
      "Epoch 2762, Loss: 1.7002167850732803, Final Batch Loss: 0.15393321216106415\n",
      "Epoch 2763, Loss: 2.030373603105545, Final Batch Loss: 0.4996655285358429\n",
      "Epoch 2764, Loss: 1.7207550406455994, Final Batch Loss: 0.26457974314689636\n",
      "Epoch 2765, Loss: 1.732812613248825, Final Batch Loss: 0.2992129623889923\n",
      "Epoch 2766, Loss: 2.504029333591461, Final Batch Loss: 1.0496306419372559\n",
      "Epoch 2767, Loss: 1.7962347567081451, Final Batch Loss: 0.28584012389183044\n",
      "Epoch 2768, Loss: 1.965141773223877, Final Batch Loss: 0.3802582323551178\n",
      "Epoch 2769, Loss: 1.797979086637497, Final Batch Loss: 0.17260178923606873\n",
      "Epoch 2770, Loss: 2.0359404385089874, Final Batch Loss: 0.46563807129859924\n",
      "Epoch 2771, Loss: 2.5244712233543396, Final Batch Loss: 0.9224440455436707\n",
      "Epoch 2772, Loss: 1.9397169053554535, Final Batch Loss: 0.38060852885246277\n",
      "Epoch 2773, Loss: 1.9574087858200073, Final Batch Loss: 0.4329472482204437\n",
      "Epoch 2774, Loss: 1.7557317912578583, Final Batch Loss: 0.27481934428215027\n",
      "Epoch 2775, Loss: 1.7777526080608368, Final Batch Loss: 0.3087778389453888\n",
      "Epoch 2776, Loss: 1.9064966142177582, Final Batch Loss: 0.3872470259666443\n",
      "Epoch 2777, Loss: 2.0421494245529175, Final Batch Loss: 0.4539748728275299\n",
      "Epoch 2778, Loss: 2.188867300748825, Final Batch Loss: 0.5585330128669739\n",
      "Epoch 2779, Loss: 1.755741611123085, Final Batch Loss: 0.24865268170833588\n",
      "Epoch 2780, Loss: 1.9864696860313416, Final Batch Loss: 0.3694637715816498\n",
      "Epoch 2781, Loss: 1.841227799654007, Final Batch Loss: 0.19174158573150635\n",
      "Epoch 2782, Loss: 1.8439176082611084, Final Batch Loss: 0.34534257650375366\n",
      "Epoch 2783, Loss: 2.09654238820076, Final Batch Loss: 0.6409526467323303\n",
      "Epoch 2784, Loss: 1.789983868598938, Final Batch Loss: 0.18713662028312683\n",
      "Epoch 2785, Loss: 2.0391329526901245, Final Batch Loss: 0.47651180624961853\n",
      "Epoch 2786, Loss: 1.7753408253192902, Final Batch Loss: 0.2809343636035919\n",
      "Epoch 2787, Loss: 1.7832230776548386, Final Batch Loss: 0.21726860105991364\n",
      "Epoch 2788, Loss: 1.8447303175926208, Final Batch Loss: 0.312865287065506\n",
      "Epoch 2789, Loss: 1.9405099153518677, Final Batch Loss: 0.4415261149406433\n",
      "Epoch 2790, Loss: 1.8069431781768799, Final Batch Loss: 0.31539639830589294\n",
      "Epoch 2791, Loss: 2.0946070849895477, Final Batch Loss: 0.5459381937980652\n",
      "Epoch 2792, Loss: 1.6709883604198694, Final Batch Loss: 0.027334490790963173\n",
      "Epoch 2793, Loss: 1.7146532237529755, Final Batch Loss: 0.26072001457214355\n",
      "Epoch 2794, Loss: 1.7611977756023407, Final Batch Loss: 0.20932114124298096\n",
      "Epoch 2795, Loss: 2.35087251663208, Final Batch Loss: 0.7827040553092957\n",
      "Epoch 2796, Loss: 1.9463618695735931, Final Batch Loss: 0.44424858689308167\n",
      "Epoch 2797, Loss: 2.136479437351227, Final Batch Loss: 0.5626049637794495\n",
      "Epoch 2798, Loss: 1.9542255997657776, Final Batch Loss: 0.32654428482055664\n",
      "Epoch 2799, Loss: 2.0467684864997864, Final Batch Loss: 0.44726046919822693\n",
      "Epoch 2800, Loss: 1.924546867609024, Final Batch Loss: 0.35452699661254883\n",
      "Epoch 2801, Loss: 1.829728126525879, Final Batch Loss: 0.3465392589569092\n",
      "Epoch 2802, Loss: 2.038702130317688, Final Batch Loss: 0.4716780483722687\n",
      "Epoch 2803, Loss: 2.0801687836647034, Final Batch Loss: 0.5851139426231384\n",
      "Epoch 2804, Loss: 2.2169438302516937, Final Batch Loss: 0.5272741913795471\n",
      "Epoch 2805, Loss: 1.8300048857927322, Final Batch Loss: 0.20609842240810394\n",
      "Epoch 2806, Loss: 1.875196397304535, Final Batch Loss: 0.3874132037162781\n",
      "Epoch 2807, Loss: 1.8757282197475433, Final Batch Loss: 0.28510117530822754\n",
      "Epoch 2808, Loss: 1.857187420129776, Final Batch Loss: 0.4119192659854889\n",
      "Epoch 2809, Loss: 1.8109031915664673, Final Batch Loss: 0.30030956864356995\n",
      "Epoch 2810, Loss: 1.9004429876804352, Final Batch Loss: 0.35106760263442993\n",
      "Epoch 2811, Loss: 2.0139374434947968, Final Batch Loss: 0.39147740602493286\n",
      "Epoch 2812, Loss: 1.8679662346839905, Final Batch Loss: 0.42561689019203186\n",
      "Epoch 2813, Loss: 1.9020417928695679, Final Batch Loss: 0.47293928265571594\n",
      "Epoch 2814, Loss: 1.6365250200033188, Final Batch Loss: 0.13430075347423553\n",
      "Epoch 2815, Loss: 1.9726965725421906, Final Batch Loss: 0.36242756247520447\n",
      "Epoch 2816, Loss: 2.0411984026432037, Final Batch Loss: 0.47592493891716003\n",
      "Epoch 2817, Loss: 1.753726288676262, Final Batch Loss: 0.2126164585351944\n",
      "Epoch 2818, Loss: 2.1524132192134857, Final Batch Loss: 0.7102643847465515\n",
      "Epoch 2819, Loss: 1.8543357849121094, Final Batch Loss: 0.3061533272266388\n",
      "Epoch 2820, Loss: 2.1966820657253265, Final Batch Loss: 0.5374336838722229\n",
      "Epoch 2821, Loss: 2.014957368373871, Final Batch Loss: 0.4052797257900238\n",
      "Epoch 2822, Loss: 1.8903693854808807, Final Batch Loss: 0.42125871777534485\n",
      "Epoch 2823, Loss: 2.023048222064972, Final Batch Loss: 0.4833275377750397\n",
      "Epoch 2824, Loss: 2.3468244075775146, Final Batch Loss: 0.8039541244506836\n",
      "Epoch 2825, Loss: 1.8558330535888672, Final Batch Loss: 0.3519843518733978\n",
      "Epoch 2826, Loss: 1.6901502460241318, Final Batch Loss: 0.09858907759189606\n",
      "Epoch 2827, Loss: 1.8014993518590927, Final Batch Loss: 0.24138231575489044\n",
      "Epoch 2828, Loss: 2.4042761027812958, Final Batch Loss: 0.8361126184463501\n",
      "Epoch 2829, Loss: 1.8331477046012878, Final Batch Loss: 0.2378757894039154\n",
      "Epoch 2830, Loss: 1.5889310836791992, Final Batch Loss: 0.045229554176330566\n",
      "Epoch 2831, Loss: 1.7665642201900482, Final Batch Loss: 0.17643198370933533\n",
      "Epoch 2832, Loss: 1.9177214205265045, Final Batch Loss: 0.3906613290309906\n",
      "Epoch 2833, Loss: 1.6919201537966728, Final Batch Loss: 0.09041839092969894\n",
      "Epoch 2834, Loss: 2.2305723130702972, Final Batch Loss: 0.7380179166793823\n",
      "Epoch 2835, Loss: 1.7367423921823502, Final Batch Loss: 0.22971583902835846\n",
      "Epoch 2836, Loss: 2.1008322536945343, Final Batch Loss: 0.5528523325920105\n",
      "Epoch 2837, Loss: 1.9036981165409088, Final Batch Loss: 0.36331695318222046\n",
      "Epoch 2838, Loss: 1.6896141096949577, Final Batch Loss: 0.05966576188802719\n",
      "Epoch 2839, Loss: 1.754421591758728, Final Batch Loss: 0.33009690046310425\n",
      "Epoch 2840, Loss: 1.8976550102233887, Final Batch Loss: 0.3784288763999939\n",
      "Epoch 2841, Loss: 1.736008197069168, Final Batch Loss: 0.25790828466415405\n",
      "Epoch 2842, Loss: 1.7389780580997467, Final Batch Loss: 0.25363072752952576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2843, Loss: 1.5970125421881676, Final Batch Loss: 0.09884055703878403\n",
      "Epoch 2844, Loss: 1.8460439443588257, Final Batch Loss: 0.360103040933609\n",
      "Epoch 2845, Loss: 1.5911220610141754, Final Batch Loss: 0.2043595016002655\n",
      "Epoch 2846, Loss: 1.7542199790477753, Final Batch Loss: 0.27177149057388306\n",
      "Epoch 2847, Loss: 1.75023752450943, Final Batch Loss: 0.3143220841884613\n",
      "Epoch 2848, Loss: 1.8285980522632599, Final Batch Loss: 0.3421168923377991\n",
      "Epoch 2849, Loss: 1.9013644754886627, Final Batch Loss: 0.32705339789390564\n",
      "Epoch 2850, Loss: 2.0987802743911743, Final Batch Loss: 0.5302807688713074\n",
      "Epoch 2851, Loss: 1.7153591215610504, Final Batch Loss: 0.21597817540168762\n",
      "Epoch 2852, Loss: 1.7678442299365997, Final Batch Loss: 0.24972951412200928\n",
      "Epoch 2853, Loss: 2.1921893656253815, Final Batch Loss: 0.6607053875923157\n",
      "Epoch 2854, Loss: 1.692915752530098, Final Batch Loss: 0.1694924384355545\n",
      "Epoch 2855, Loss: 2.000639945268631, Final Batch Loss: 0.49123474955558777\n",
      "Epoch 2856, Loss: 2.157633602619171, Final Batch Loss: 0.6442900896072388\n",
      "Epoch 2857, Loss: 1.6640605255961418, Final Batch Loss: 0.10831474512815475\n",
      "Epoch 2858, Loss: 1.8936181366443634, Final Batch Loss: 0.3517818748950958\n",
      "Epoch 2859, Loss: 1.9510182738304138, Final Batch Loss: 0.5516477823257446\n",
      "Epoch 2860, Loss: 1.887047439813614, Final Batch Loss: 0.3730752468109131\n",
      "Epoch 2861, Loss: 2.1625083684921265, Final Batch Loss: 0.6391386389732361\n",
      "Epoch 2862, Loss: 1.8881953060626984, Final Batch Loss: 0.43085965514183044\n",
      "Epoch 2863, Loss: 1.9455820322036743, Final Batch Loss: 0.4084185063838959\n",
      "Epoch 2864, Loss: 2.5026992857456207, Final Batch Loss: 0.8944780230522156\n",
      "Epoch 2865, Loss: 2.298463135957718, Final Batch Loss: 0.7237815260887146\n",
      "Epoch 2866, Loss: 2.1288801729679108, Final Batch Loss: 0.42623069882392883\n",
      "Epoch 2867, Loss: 1.8361781686544418, Final Batch Loss: 0.16775299608707428\n",
      "Epoch 2868, Loss: 1.656168207526207, Final Batch Loss: 0.161023810505867\n",
      "Epoch 2869, Loss: 2.217571973800659, Final Batch Loss: 0.5348325371742249\n",
      "Epoch 2870, Loss: 1.9139423668384552, Final Batch Loss: 0.3893219828605652\n",
      "Epoch 2871, Loss: 2.0113112032413483, Final Batch Loss: 0.3647361099720001\n",
      "Epoch 2872, Loss: 1.8765758574008942, Final Batch Loss: 0.36449140310287476\n",
      "Epoch 2873, Loss: 2.1320672631263733, Final Batch Loss: 0.5884131193161011\n",
      "Epoch 2874, Loss: 2.0338805615901947, Final Batch Loss: 0.5053436756134033\n",
      "Epoch 2875, Loss: 1.8179149329662323, Final Batch Loss: 0.284530371427536\n",
      "Epoch 2876, Loss: 1.9894829243421555, Final Batch Loss: 0.22891633212566376\n",
      "Epoch 2877, Loss: 2.07974112033844, Final Batch Loss: 0.510745644569397\n",
      "Epoch 2878, Loss: 2.1711076200008392, Final Batch Loss: 0.677049458026886\n",
      "Epoch 2879, Loss: 2.022298127412796, Final Batch Loss: 0.34252747893333435\n",
      "Epoch 2880, Loss: 1.8160012662410736, Final Batch Loss: 0.22711887955665588\n",
      "Epoch 2881, Loss: 2.002731114625931, Final Batch Loss: 0.4738965630531311\n",
      "Epoch 2882, Loss: 1.6908159703016281, Final Batch Loss: 0.16192971169948578\n",
      "Epoch 2883, Loss: 1.8659235537052155, Final Batch Loss: 0.3393833637237549\n",
      "Epoch 2884, Loss: 1.960512399673462, Final Batch Loss: 0.5213438272476196\n",
      "Epoch 2885, Loss: 2.2520254254341125, Final Batch Loss: 0.6988755464553833\n",
      "Epoch 2886, Loss: 1.9226735532283783, Final Batch Loss: 0.3485690951347351\n",
      "Epoch 2887, Loss: 2.0285422801971436, Final Batch Loss: 0.615456223487854\n",
      "Epoch 2888, Loss: 1.891244500875473, Final Batch Loss: 0.4379783272743225\n",
      "Epoch 2889, Loss: 1.8016633242368698, Final Batch Loss: 0.24830757081508636\n",
      "Epoch 2890, Loss: 2.2101387083530426, Final Batch Loss: 0.5940319895744324\n",
      "Epoch 2891, Loss: 1.7686153650283813, Final Batch Loss: 0.31124284863471985\n",
      "Epoch 2892, Loss: 2.159302979707718, Final Batch Loss: 0.5932751297950745\n",
      "Epoch 2893, Loss: 1.8977098315954208, Final Batch Loss: 0.23868073523044586\n",
      "Epoch 2894, Loss: 1.8265094757080078, Final Batch Loss: 0.24141088128089905\n",
      "Epoch 2895, Loss: 1.7951256334781647, Final Batch Loss: 0.28968504071235657\n",
      "Epoch 2896, Loss: 1.863164484500885, Final Batch Loss: 0.4066712558269501\n",
      "Epoch 2897, Loss: 1.8102373778820038, Final Batch Loss: 0.3110685646533966\n",
      "Epoch 2898, Loss: 1.7066614478826523, Final Batch Loss: 0.2316349297761917\n",
      "Epoch 2899, Loss: 1.7281692773103714, Final Batch Loss: 0.17974050343036652\n",
      "Epoch 2900, Loss: 1.8236969858407974, Final Batch Loss: 0.18948517739772797\n",
      "Epoch 2901, Loss: 1.8994574546813965, Final Batch Loss: 0.3330870568752289\n",
      "Epoch 2902, Loss: 1.6619882434606552, Final Batch Loss: 0.20761097967624664\n",
      "Epoch 2903, Loss: 1.7078750431537628, Final Batch Loss: 0.26196399331092834\n",
      "Epoch 2904, Loss: 2.0438336730003357, Final Batch Loss: 0.563694179058075\n",
      "Epoch 2905, Loss: 2.033665269613266, Final Batch Loss: 0.5627397894859314\n",
      "Epoch 2906, Loss: 1.5995854437351227, Final Batch Loss: 0.16584885120391846\n",
      "Epoch 2907, Loss: 1.6704973429441452, Final Batch Loss: 0.16975824534893036\n",
      "Epoch 2908, Loss: 1.8967463076114655, Final Batch Loss: 0.5160819292068481\n",
      "Epoch 2909, Loss: 2.418759286403656, Final Batch Loss: 0.8753923773765564\n",
      "Epoch 2910, Loss: 2.046126127243042, Final Batch Loss: 0.5476529002189636\n",
      "Epoch 2911, Loss: 1.936227023601532, Final Batch Loss: 0.27956119179725647\n",
      "Epoch 2912, Loss: 2.268959492444992, Final Batch Loss: 0.6491021513938904\n",
      "Epoch 2913, Loss: 2.055755138397217, Final Batch Loss: 0.5187111496925354\n",
      "Epoch 2914, Loss: 1.813859298825264, Final Batch Loss: 0.23544718325138092\n",
      "Epoch 2915, Loss: 2.1138489842414856, Final Batch Loss: 0.5286186337471008\n",
      "Epoch 2916, Loss: 1.9627313315868378, Final Batch Loss: 0.5044728517532349\n",
      "Epoch 2917, Loss: 1.8986642062664032, Final Batch Loss: 0.4488844871520996\n",
      "Epoch 2918, Loss: 2.3503832519054413, Final Batch Loss: 0.8476243019104004\n",
      "Epoch 2919, Loss: 1.9067046642303467, Final Batch Loss: 0.3512416183948517\n",
      "Epoch 2920, Loss: 2.17521670460701, Final Batch Loss: 0.5724379420280457\n",
      "Epoch 2921, Loss: 2.024193912744522, Final Batch Loss: 0.5100876092910767\n",
      "Epoch 2922, Loss: 1.8487791419029236, Final Batch Loss: 0.2975459396839142\n",
      "Epoch 2923, Loss: 2.044115275144577, Final Batch Loss: 0.4704737067222595\n",
      "Epoch 2924, Loss: 1.8914198279380798, Final Batch Loss: 0.25360894203186035\n",
      "Epoch 2925, Loss: 2.0490534901618958, Final Batch Loss: 0.4706057608127594\n",
      "Epoch 2926, Loss: 1.8542162775993347, Final Batch Loss: 0.5151552557945251\n",
      "Epoch 2927, Loss: 1.7913145422935486, Final Batch Loss: 0.3090793490409851\n",
      "Epoch 2928, Loss: 1.8405340760946274, Final Batch Loss: 0.23506464064121246\n",
      "Epoch 2929, Loss: 1.9647990763187408, Final Batch Loss: 0.41281965374946594\n",
      "Epoch 2930, Loss: 2.102151572704315, Final Batch Loss: 0.6187206506729126\n",
      "Epoch 2931, Loss: 1.8471134901046753, Final Batch Loss: 0.2970929741859436\n",
      "Epoch 2932, Loss: 1.8415097892284393, Final Batch Loss: 0.2629120945930481\n",
      "Epoch 2933, Loss: 1.8681251406669617, Final Batch Loss: 0.3470360338687897\n",
      "Epoch 2934, Loss: 2.433966487646103, Final Batch Loss: 0.955973207950592\n",
      "Epoch 2935, Loss: 1.8022042512893677, Final Batch Loss: 0.34002751111984253\n",
      "Epoch 2936, Loss: 1.7074199616909027, Final Batch Loss: 0.16777929663658142\n",
      "Epoch 2937, Loss: 1.8918446004390717, Final Batch Loss: 0.4215802848339081\n",
      "Epoch 2938, Loss: 2.0610247254371643, Final Batch Loss: 0.5662060379981995\n",
      "Epoch 2939, Loss: 1.91500586271286, Final Batch Loss: 0.3554784655570984\n",
      "Epoch 2940, Loss: 2.4625295102596283, Final Batch Loss: 1.0442242622375488\n",
      "Epoch 2941, Loss: 2.375127911567688, Final Batch Loss: 0.9249552488327026\n",
      "Epoch 2942, Loss: 2.0597748160362244, Final Batch Loss: 0.381279855966568\n",
      "Epoch 2943, Loss: 2.1260710060596466, Final Batch Loss: 0.6435276865959167\n",
      "Epoch 2944, Loss: 2.118052691221237, Final Batch Loss: 0.6072222590446472\n",
      "Epoch 2945, Loss: 1.8840791434049606, Final Batch Loss: 0.21382887661457062\n",
      "Epoch 2946, Loss: 2.0490500926971436, Final Batch Loss: 0.47061800956726074\n",
      "Epoch 2947, Loss: 1.9248414933681488, Final Batch Loss: 0.3091588020324707\n",
      "Epoch 2948, Loss: 1.959604650735855, Final Batch Loss: 0.3484225273132324\n",
      "Epoch 2949, Loss: 2.1483699679374695, Final Batch Loss: 0.7229813933372498\n",
      "Epoch 2950, Loss: 1.9721340239048004, Final Batch Loss: 0.31488919258117676\n",
      "Epoch 2951, Loss: 2.250459671020508, Final Batch Loss: 0.2580816149711609\n",
      "Epoch 2952, Loss: 2.458943337202072, Final Batch Loss: 0.7223297357559204\n",
      "Epoch 2953, Loss: 1.9800180494785309, Final Batch Loss: 0.3121550381183624\n",
      "Epoch 2954, Loss: 2.0150183737277985, Final Batch Loss: 0.32677727937698364\n",
      "Epoch 2955, Loss: 2.3565089404582977, Final Batch Loss: 0.7640500664710999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2956, Loss: 1.8276166021823883, Final Batch Loss: 0.27760374546051025\n",
      "Epoch 2957, Loss: 2.0121428072452545, Final Batch Loss: 0.3800511360168457\n",
      "Epoch 2958, Loss: 1.7571665942668915, Final Batch Loss: 0.2567586600780487\n",
      "Epoch 2959, Loss: 1.6924197971820831, Final Batch Loss: 0.23538827896118164\n",
      "Epoch 2960, Loss: 2.3071461617946625, Final Batch Loss: 0.7068365812301636\n",
      "Epoch 2961, Loss: 1.6871298998594284, Final Batch Loss: 0.1936293989419937\n",
      "Epoch 2962, Loss: 2.063665807247162, Final Batch Loss: 0.507523238658905\n",
      "Epoch 2963, Loss: 1.7460215836763382, Final Batch Loss: 0.12541471421718597\n",
      "Epoch 2964, Loss: 1.9672633707523346, Final Batch Loss: 0.2996692359447479\n",
      "Epoch 2965, Loss: 2.3094770908355713, Final Batch Loss: 0.5622653961181641\n",
      "Epoch 2966, Loss: 2.592919886112213, Final Batch Loss: 0.9798049926757812\n",
      "Epoch 2967, Loss: 1.9269823729991913, Final Batch Loss: 0.22109660506248474\n",
      "Epoch 2968, Loss: 2.1972790360450745, Final Batch Loss: 0.4788103699684143\n",
      "Epoch 2969, Loss: 2.114132970571518, Final Batch Loss: 0.47722938656806946\n",
      "Epoch 2970, Loss: 2.17362841963768, Final Batch Loss: 0.5868972539901733\n",
      "Epoch 2971, Loss: 1.8047298789024353, Final Batch Loss: 0.2274036407470703\n",
      "Epoch 2972, Loss: 1.8132803440093994, Final Batch Loss: 0.2840215265750885\n",
      "Epoch 2973, Loss: 1.6521816067397594, Final Batch Loss: 0.03260348364710808\n",
      "Epoch 2974, Loss: 1.91865935921669, Final Batch Loss: 0.39523035287857056\n",
      "Epoch 2975, Loss: 2.1387640833854675, Final Batch Loss: 0.6011813879013062\n",
      "Epoch 2976, Loss: 1.71500925719738, Final Batch Loss: 0.22275041043758392\n",
      "Epoch 2977, Loss: 2.09152615070343, Final Batch Loss: 0.5543013215065002\n",
      "Epoch 2978, Loss: 2.4971436262130737, Final Batch Loss: 0.8576420545578003\n",
      "Epoch 2979, Loss: 1.7121875584125519, Final Batch Loss: 0.2644776403903961\n",
      "Epoch 2980, Loss: 1.930653989315033, Final Batch Loss: 0.4020601809024811\n",
      "Epoch 2981, Loss: 1.7344695925712585, Final Batch Loss: 0.23892930150032043\n",
      "Epoch 2982, Loss: 1.9046279788017273, Final Batch Loss: 0.3585522770881653\n",
      "Epoch 2983, Loss: 1.9977481663227081, Final Batch Loss: 0.5435625910758972\n",
      "Epoch 2984, Loss: 1.9237516820430756, Final Batch Loss: 0.42003923654556274\n",
      "Epoch 2985, Loss: 2.3503124713897705, Final Batch Loss: 0.8008330464363098\n",
      "Epoch 2986, Loss: 1.6613620072603226, Final Batch Loss: 0.12641958892345428\n",
      "Epoch 2987, Loss: 1.913234382867813, Final Batch Loss: 0.33287906646728516\n",
      "Epoch 2988, Loss: 1.7741063833236694, Final Batch Loss: 0.3028722107410431\n",
      "Epoch 2989, Loss: 1.9110255539417267, Final Batch Loss: 0.42315202951431274\n",
      "Epoch 2990, Loss: 1.6515576988458633, Final Batch Loss: 0.1862940639257431\n",
      "Epoch 2991, Loss: 2.0110942125320435, Final Batch Loss: 0.456712931394577\n",
      "Epoch 2992, Loss: 1.8050568401813507, Final Batch Loss: 0.28216251730918884\n",
      "Epoch 2993, Loss: 1.6713972389698029, Final Batch Loss: 0.1226353645324707\n",
      "Epoch 2994, Loss: 1.8864420652389526, Final Batch Loss: 0.376676082611084\n",
      "Epoch 2995, Loss: 1.7642322927713394, Final Batch Loss: 0.21275459229946136\n",
      "Epoch 2996, Loss: 1.7912128567695618, Final Batch Loss: 0.3202260136604309\n",
      "Epoch 2997, Loss: 1.6928460896015167, Final Batch Loss: 0.2538920044898987\n",
      "Epoch 2998, Loss: 2.0213067829608917, Final Batch Loss: 0.49299392104148865\n",
      "Epoch 2999, Loss: 1.7191510498523712, Final Batch Loss: 0.26026594638824463\n",
      "Epoch 3000, Loss: 1.9353174567222595, Final Batch Loss: 0.49136704206466675\n",
      "Epoch 3001, Loss: 3.0358458757400513, Final Batch Loss: 1.5187503099441528\n",
      "Epoch 3002, Loss: 1.7735407948493958, Final Batch Loss: 0.2490120232105255\n",
      "Epoch 3003, Loss: 1.8202262222766876, Final Batch Loss: 0.369874507188797\n",
      "Epoch 3004, Loss: 1.7933300733566284, Final Batch Loss: 0.33759114146232605\n",
      "Epoch 3005, Loss: 1.8426571190357208, Final Batch Loss: 0.38193613290786743\n",
      "Epoch 3006, Loss: 2.125404864549637, Final Batch Loss: 0.7045900225639343\n",
      "Epoch 3007, Loss: 1.509651517495513, Final Batch Loss: 0.020808016881346703\n",
      "Epoch 3008, Loss: 1.5454960614442825, Final Batch Loss: 0.18168790638446808\n",
      "Epoch 3009, Loss: 1.6304690837860107, Final Batch Loss: 0.2062285840511322\n",
      "Epoch 3010, Loss: 1.6318419501185417, Final Batch Loss: 0.08888896554708481\n",
      "Epoch 3011, Loss: 2.066418617963791, Final Batch Loss: 0.3780442178249359\n",
      "Epoch 3012, Loss: 1.9982591569423676, Final Batch Loss: 0.4017958343029022\n",
      "Epoch 3013, Loss: 2.0262117981910706, Final Batch Loss: 0.4125064015388489\n",
      "Epoch 3014, Loss: 1.8543678522109985, Final Batch Loss: 0.34676551818847656\n",
      "Epoch 3015, Loss: 1.8923730552196503, Final Batch Loss: 0.38816913962364197\n",
      "Epoch 3016, Loss: 1.502010464668274, Final Batch Loss: 0.04588449001312256\n",
      "Epoch 3017, Loss: 1.7943672239780426, Final Batch Loss: 0.28754740953445435\n",
      "Epoch 3018, Loss: 1.6361179053783417, Final Batch Loss: 0.16488194465637207\n",
      "Epoch 3019, Loss: 1.7202933132648468, Final Batch Loss: 0.2789701521396637\n",
      "Epoch 3020, Loss: 1.7739669680595398, Final Batch Loss: 0.3227570950984955\n",
      "Epoch 3021, Loss: 1.6372903883457184, Final Batch Loss: 0.13182580471038818\n",
      "Epoch 3022, Loss: 1.7899545431137085, Final Batch Loss: 0.28534945845603943\n",
      "Epoch 3023, Loss: 1.8325929045677185, Final Batch Loss: 0.2692456543445587\n",
      "Epoch 3024, Loss: 1.9178053438663483, Final Batch Loss: 0.3820338845252991\n",
      "Epoch 3025, Loss: 1.6553871184587479, Final Batch Loss: 0.15757925808429718\n",
      "Epoch 3026, Loss: 2.002323180437088, Final Batch Loss: 0.44886431097984314\n",
      "Epoch 3027, Loss: 1.6507214158773422, Final Batch Loss: 0.1625002771615982\n",
      "Epoch 3028, Loss: 1.6112474650144577, Final Batch Loss: 0.13478903472423553\n",
      "Epoch 3029, Loss: 1.7222913652658463, Final Batch Loss: 0.24988605082035065\n",
      "Epoch 3030, Loss: 1.6937824487686157, Final Batch Loss: 0.14493966102600098\n",
      "Epoch 3031, Loss: 1.6837750673294067, Final Batch Loss: 0.20475494861602783\n",
      "Epoch 3032, Loss: 2.4193463027477264, Final Batch Loss: 0.8581764101982117\n",
      "Epoch 3033, Loss: 2.128951370716095, Final Batch Loss: 0.4981568455696106\n",
      "Epoch 3034, Loss: 2.1562365293502808, Final Batch Loss: 0.5794046521186829\n",
      "Epoch 3035, Loss: 2.1458524763584137, Final Batch Loss: 0.524526059627533\n",
      "Epoch 3036, Loss: 2.39351025223732, Final Batch Loss: 0.8776959180831909\n",
      "Epoch 3037, Loss: 1.79639932513237, Final Batch Loss: 0.1713590919971466\n",
      "Epoch 3038, Loss: 2.1358785033226013, Final Batch Loss: 0.445410817861557\n",
      "Epoch 3039, Loss: 1.9747594594955444, Final Batch Loss: 0.4005758464336395\n",
      "Epoch 3040, Loss: 1.8051795363426208, Final Batch Loss: 0.3298908770084381\n",
      "Epoch 3041, Loss: 1.7136220484972, Final Batch Loss: 0.16379891335964203\n",
      "Epoch 3042, Loss: 2.467158079147339, Final Batch Loss: 0.8232558965682983\n",
      "Epoch 3043, Loss: 1.9749907851219177, Final Batch Loss: 0.5310302972793579\n",
      "Epoch 3044, Loss: 2.00151589512825, Final Batch Loss: 0.3430154025554657\n",
      "Epoch 3045, Loss: 1.8565990328788757, Final Batch Loss: 0.3427343964576721\n",
      "Epoch 3046, Loss: 1.9404222071170807, Final Batch Loss: 0.4298451542854309\n",
      "Epoch 3047, Loss: 1.8767410814762115, Final Batch Loss: 0.33115217089653015\n",
      "Epoch 3048, Loss: 1.614523109048605, Final Batch Loss: 0.05059173330664635\n",
      "Epoch 3049, Loss: 1.8654015362262726, Final Batch Loss: 0.3900287449359894\n",
      "Epoch 3050, Loss: 2.0134073197841644, Final Batch Loss: 0.5666950941085815\n",
      "Epoch 3051, Loss: 2.0914359092712402, Final Batch Loss: 0.6214335560798645\n",
      "Epoch 3052, Loss: 2.0071813762187958, Final Batch Loss: 0.5583458542823792\n",
      "Epoch 3053, Loss: 1.8181371092796326, Final Batch Loss: 0.13144326210021973\n",
      "Epoch 3054, Loss: 1.75819830596447, Final Batch Loss: 0.21061484515666962\n",
      "Epoch 3055, Loss: 1.815051943063736, Final Batch Loss: 0.1894756257534027\n",
      "Epoch 3056, Loss: 1.8790325224399567, Final Batch Loss: 0.33937856554985046\n",
      "Epoch 3057, Loss: 1.721317008137703, Final Batch Loss: 0.22646136581897736\n",
      "Epoch 3058, Loss: 2.1706714034080505, Final Batch Loss: 0.6421403884887695\n",
      "Epoch 3059, Loss: 1.7516047358512878, Final Batch Loss: 0.2877044081687927\n",
      "Epoch 3060, Loss: 1.801426649093628, Final Batch Loss: 0.26538845896720886\n",
      "Epoch 3061, Loss: 1.899804711341858, Final Batch Loss: 0.4431597888469696\n",
      "Epoch 3062, Loss: 2.120271176099777, Final Batch Loss: 0.7226064801216125\n",
      "Epoch 3063, Loss: 1.8141272366046906, Final Batch Loss: 0.4209666848182678\n",
      "Epoch 3064, Loss: 1.8714042901992798, Final Batch Loss: 0.372408390045166\n",
      "Epoch 3065, Loss: 1.9262857139110565, Final Batch Loss: 0.4887867569923401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3066, Loss: 1.913848638534546, Final Batch Loss: 0.4008290469646454\n",
      "Epoch 3067, Loss: 2.017664074897766, Final Batch Loss: 0.5104352235794067\n",
      "Epoch 3068, Loss: 1.699044182896614, Final Batch Loss: 0.23165486752986908\n",
      "Epoch 3069, Loss: 2.0398579835891724, Final Batch Loss: 0.5809992551803589\n",
      "Epoch 3070, Loss: 1.955028623342514, Final Batch Loss: 0.3633013367652893\n",
      "Epoch 3071, Loss: 1.8341506719589233, Final Batch Loss: 0.48550543189048767\n",
      "Epoch 3072, Loss: 2.068960815668106, Final Batch Loss: 0.5087495446205139\n",
      "Epoch 3073, Loss: 1.9698545038700104, Final Batch Loss: 0.37966641783714294\n",
      "Epoch 3074, Loss: 1.6954728066921234, Final Batch Loss: 0.20183420181274414\n",
      "Epoch 3075, Loss: 1.8723827600479126, Final Batch Loss: 0.3878819942474365\n",
      "Epoch 3076, Loss: 2.3462518751621246, Final Batch Loss: 0.8097214102745056\n",
      "Epoch 3077, Loss: 1.5974483639001846, Final Batch Loss: 0.0640900582075119\n",
      "Epoch 3078, Loss: 1.8349459171295166, Final Batch Loss: 0.4032290279865265\n",
      "Epoch 3079, Loss: 1.9272596538066864, Final Batch Loss: 0.5522201657295227\n",
      "Epoch 3080, Loss: 1.8691674768924713, Final Batch Loss: 0.27312520146369934\n",
      "Epoch 3081, Loss: 1.8439580500125885, Final Batch Loss: 0.42988112568855286\n",
      "Epoch 3082, Loss: 1.9456546902656555, Final Batch Loss: 0.4524679183959961\n",
      "Epoch 3083, Loss: 1.8322625160217285, Final Batch Loss: 0.2600277066230774\n",
      "Epoch 3084, Loss: 1.8956869095563889, Final Batch Loss: 0.19586358964443207\n",
      "Epoch 3085, Loss: 2.285706877708435, Final Batch Loss: 0.8127281069755554\n",
      "Epoch 3086, Loss: 2.06267848610878, Final Batch Loss: 0.5498859286308289\n",
      "Epoch 3087, Loss: 2.017623722553253, Final Batch Loss: 0.5786315202713013\n",
      "Epoch 3088, Loss: 1.7613389790058136, Final Batch Loss: 0.16231581568717957\n",
      "Epoch 3089, Loss: 1.7281841486692429, Final Batch Loss: 0.17071019113063812\n",
      "Epoch 3090, Loss: 1.7890730202198029, Final Batch Loss: 0.2920056879520416\n",
      "Epoch 3091, Loss: 2.107055425643921, Final Batch Loss: 0.5570653676986694\n",
      "Epoch 3092, Loss: 2.0800937116146088, Final Batch Loss: 0.4250795841217041\n",
      "Epoch 3093, Loss: 1.7641474902629852, Final Batch Loss: 0.30376699566841125\n",
      "Epoch 3094, Loss: 1.8154101818799973, Final Batch Loss: 0.18292711675167084\n",
      "Epoch 3095, Loss: 1.7621721178293228, Final Batch Loss: 0.2178090661764145\n",
      "Epoch 3096, Loss: 1.8847369253635406, Final Batch Loss: 0.2775280773639679\n",
      "Epoch 3097, Loss: 1.9342367351055145, Final Batch Loss: 0.42550569772720337\n",
      "Epoch 3098, Loss: 1.8614376485347748, Final Batch Loss: 0.501015305519104\n",
      "Epoch 3099, Loss: 1.8991702795028687, Final Batch Loss: 0.448617160320282\n",
      "Epoch 3100, Loss: 1.8993432223796844, Final Batch Loss: 0.4713185429573059\n",
      "Epoch 3101, Loss: 2.2432643473148346, Final Batch Loss: 0.881409764289856\n",
      "Epoch 3102, Loss: 1.5412391796708107, Final Batch Loss: 0.08494219928979874\n",
      "Epoch 3103, Loss: 1.7963320910930634, Final Batch Loss: 0.34363123774528503\n",
      "Epoch 3104, Loss: 1.8670482337474823, Final Batch Loss: 0.4602312743663788\n",
      "Epoch 3105, Loss: 1.9012556076049805, Final Batch Loss: 0.2987457811832428\n",
      "Epoch 3106, Loss: 1.7804444432258606, Final Batch Loss: 0.36268118023872375\n",
      "Epoch 3107, Loss: 1.5857171416282654, Final Batch Loss: 0.1906515657901764\n",
      "Epoch 3108, Loss: 1.577270232141018, Final Batch Loss: 0.06855540722608566\n",
      "Epoch 3109, Loss: 2.3302018642425537, Final Batch Loss: 0.8322309255599976\n",
      "Epoch 3110, Loss: 1.7995084375143051, Final Batch Loss: 0.1694817990064621\n",
      "Epoch 3111, Loss: 1.6984415799379349, Final Batch Loss: 0.18840409815311432\n",
      "Epoch 3112, Loss: 1.7723973393440247, Final Batch Loss: 0.27625444531440735\n",
      "Epoch 3113, Loss: 1.666311427950859, Final Batch Loss: 0.1477026790380478\n",
      "Epoch 3114, Loss: 1.8298248648643494, Final Batch Loss: 0.45356056094169617\n",
      "Epoch 3115, Loss: 1.8136864602565765, Final Batch Loss: 0.3604561686515808\n",
      "Epoch 3116, Loss: 1.567846640944481, Final Batch Loss: 0.18211092054843903\n",
      "Epoch 3117, Loss: 1.8658190667629242, Final Batch Loss: 0.4024002254009247\n",
      "Epoch 3118, Loss: 2.065398156642914, Final Batch Loss: 0.5378038883209229\n",
      "Epoch 3119, Loss: 1.8630326092243195, Final Batch Loss: 0.3232592046260834\n",
      "Epoch 3120, Loss: 1.688280001282692, Final Batch Loss: 0.18985728919506073\n",
      "Epoch 3121, Loss: 1.8455124497413635, Final Batch Loss: 0.39122191071510315\n",
      "Epoch 3122, Loss: 1.7977375388145447, Final Batch Loss: 0.2550348937511444\n",
      "Epoch 3123, Loss: 1.7237577438354492, Final Batch Loss: 0.3886853754520416\n",
      "Epoch 3124, Loss: 2.049979090690613, Final Batch Loss: 0.5789567828178406\n",
      "Epoch 3125, Loss: 1.743175745010376, Final Batch Loss: 0.2808869779109955\n",
      "Epoch 3126, Loss: 2.064394772052765, Final Batch Loss: 0.6653364896774292\n",
      "Epoch 3127, Loss: 1.6887012720108032, Final Batch Loss: 0.2672373652458191\n",
      "Epoch 3128, Loss: 1.8720007240772247, Final Batch Loss: 0.42655691504478455\n",
      "Epoch 3129, Loss: 1.6252232640981674, Final Batch Loss: 0.22210083901882172\n",
      "Epoch 3130, Loss: 1.8172571510076523, Final Batch Loss: 0.2462366670370102\n",
      "Epoch 3131, Loss: 1.7865402102470398, Final Batch Loss: 0.28613004088401794\n",
      "Epoch 3132, Loss: 1.6924806386232376, Final Batch Loss: 0.23429180681705475\n",
      "Epoch 3133, Loss: 2.0089648962020874, Final Batch Loss: 0.6237671971321106\n",
      "Epoch 3134, Loss: 1.9337947070598602, Final Batch Loss: 0.5101775527000427\n",
      "Epoch 3135, Loss: 1.700770527124405, Final Batch Loss: 0.3520779013633728\n",
      "Epoch 3136, Loss: 1.9787480533123016, Final Batch Loss: 0.4433901309967041\n",
      "Epoch 3137, Loss: 1.9520829021930695, Final Batch Loss: 0.5629398226737976\n",
      "Epoch 3138, Loss: 1.6564476191997528, Final Batch Loss: 0.2687150537967682\n",
      "Epoch 3139, Loss: 1.7267554104328156, Final Batch Loss: 0.23475107550621033\n",
      "Epoch 3140, Loss: 1.8562082350254059, Final Batch Loss: 0.46131235361099243\n",
      "Epoch 3141, Loss: 1.819037526845932, Final Batch Loss: 0.3959563672542572\n",
      "Epoch 3142, Loss: 1.7816629111766815, Final Batch Loss: 0.35427770018577576\n",
      "Epoch 3143, Loss: 2.0941202044487, Final Batch Loss: 0.6588471531867981\n",
      "Epoch 3144, Loss: 1.763791412115097, Final Batch Loss: 0.27507588267326355\n",
      "Epoch 3145, Loss: 1.5866922587156296, Final Batch Loss: 0.12766112387180328\n",
      "Epoch 3146, Loss: 1.9214965105056763, Final Batch Loss: 0.48250383138656616\n",
      "Epoch 3147, Loss: 1.6787565499544144, Final Batch Loss: 0.19287510216236115\n",
      "Epoch 3148, Loss: 1.69670769572258, Final Batch Loss: 0.3144543468952179\n",
      "Epoch 3149, Loss: 1.9409525394439697, Final Batch Loss: 0.5949323773384094\n",
      "Epoch 3150, Loss: 1.8073098957538605, Final Batch Loss: 0.3617390990257263\n",
      "Epoch 3151, Loss: 1.8005697280168533, Final Batch Loss: 0.2336128205060959\n",
      "Epoch 3152, Loss: 1.865676149725914, Final Batch Loss: 0.23339323699474335\n",
      "Epoch 3153, Loss: 1.7567576169967651, Final Batch Loss: 0.3002539277076721\n",
      "Epoch 3154, Loss: 1.9511079788208008, Final Batch Loss: 0.5770969390869141\n",
      "Epoch 3155, Loss: 1.7777555882930756, Final Batch Loss: 0.18942692875862122\n",
      "Epoch 3156, Loss: 1.457792967557907, Final Batch Loss: 0.02234196662902832\n",
      "Epoch 3157, Loss: 2.198314607143402, Final Batch Loss: 0.8310291171073914\n",
      "Epoch 3158, Loss: 1.7648145854473114, Final Batch Loss: 0.3229432702064514\n",
      "Epoch 3159, Loss: 1.5928074643015862, Final Batch Loss: 0.09878119081258774\n",
      "Epoch 3160, Loss: 2.0978015065193176, Final Batch Loss: 0.8093833923339844\n",
      "Epoch 3161, Loss: 1.690169408917427, Final Batch Loss: 0.1496206372976303\n",
      "Epoch 3162, Loss: 1.9842587113380432, Final Batch Loss: 0.4604271948337555\n",
      "Epoch 3163, Loss: 1.8584296703338623, Final Batch Loss: 0.4124544560909271\n",
      "Epoch 3164, Loss: 1.6404208093881607, Final Batch Loss: 0.15931619703769684\n",
      "Epoch 3165, Loss: 1.8215872049331665, Final Batch Loss: 0.38349300622940063\n",
      "Epoch 3166, Loss: 1.8661217093467712, Final Batch Loss: 0.5235780477523804\n",
      "Epoch 3167, Loss: 1.9824676811695099, Final Batch Loss: 0.40152767300605774\n",
      "Epoch 3168, Loss: 2.0496516823768616, Final Batch Loss: 0.43608593940734863\n",
      "Epoch 3169, Loss: 2.0405561327934265, Final Batch Loss: 0.502258837223053\n",
      "Epoch 3170, Loss: 1.8389844000339508, Final Batch Loss: 0.358036071062088\n",
      "Epoch 3171, Loss: 1.997578650712967, Final Batch Loss: 0.4213174283504486\n",
      "Epoch 3172, Loss: 2.3046239614486694, Final Batch Loss: 0.761064350605011\n",
      "Epoch 3173, Loss: 1.5515733305364847, Final Batch Loss: 0.01633726991713047\n",
      "Epoch 3174, Loss: 1.6672046035528183, Final Batch Loss: 0.19276539981365204\n",
      "Epoch 3175, Loss: 1.717809647321701, Final Batch Loss: 0.30930599570274353\n",
      "Epoch 3176, Loss: 1.6541236191987991, Final Batch Loss: 0.1847953349351883\n",
      "Epoch 3177, Loss: 2.1139150857925415, Final Batch Loss: 0.624611496925354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3178, Loss: 1.8043113052845001, Final Batch Loss: 0.3485831320285797\n",
      "Epoch 3179, Loss: 1.6348433792591095, Final Batch Loss: 0.17881730198860168\n",
      "Epoch 3180, Loss: 1.6593983173370361, Final Batch Loss: 0.1954542100429535\n",
      "Epoch 3181, Loss: 2.232485979795456, Final Batch Loss: 0.7112273573875427\n",
      "Epoch 3182, Loss: 2.2709136307239532, Final Batch Loss: 0.8472275733947754\n",
      "Epoch 3183, Loss: 1.7816099971532822, Final Batch Loss: 0.20964930951595306\n",
      "Epoch 3184, Loss: 1.9379447400569916, Final Batch Loss: 0.40355372428894043\n",
      "Epoch 3185, Loss: 2.279056668281555, Final Batch Loss: 0.7352291345596313\n",
      "Epoch 3186, Loss: 1.6075390949845314, Final Batch Loss: 0.09710530191659927\n",
      "Epoch 3187, Loss: 1.7990627735853195, Final Batch Loss: 0.19314272701740265\n",
      "Epoch 3188, Loss: 2.087369292974472, Final Batch Loss: 0.48683005571365356\n",
      "Epoch 3189, Loss: 2.0341930389404297, Final Batch Loss: 0.48554590344429016\n",
      "Epoch 3190, Loss: 2.0936070382595062, Final Batch Loss: 0.5798554420471191\n",
      "Epoch 3191, Loss: 1.6139944270253181, Final Batch Loss: 0.10382077842950821\n",
      "Epoch 3192, Loss: 1.6652366071939468, Final Batch Loss: 0.20694221556186676\n",
      "Epoch 3193, Loss: 1.5110177844762802, Final Batch Loss: 0.14362697303295135\n",
      "Epoch 3194, Loss: 1.8793647587299347, Final Batch Loss: 0.5265485048294067\n",
      "Epoch 3195, Loss: 1.9567741751670837, Final Batch Loss: 0.4639278054237366\n",
      "Epoch 3196, Loss: 2.0332466661930084, Final Batch Loss: 0.5761361122131348\n",
      "Epoch 3197, Loss: 2.275145173072815, Final Batch Loss: 0.6322215795516968\n",
      "Epoch 3198, Loss: 1.9071184992790222, Final Batch Loss: 0.39785510301589966\n",
      "Epoch 3199, Loss: 1.7982563078403473, Final Batch Loss: 0.16574648022651672\n",
      "Epoch 3200, Loss: 1.8825892210006714, Final Batch Loss: 0.25806182622909546\n",
      "Epoch 3201, Loss: 1.9787688553333282, Final Batch Loss: 0.4045748710632324\n",
      "Epoch 3202, Loss: 1.5942250043153763, Final Batch Loss: 0.16563479602336884\n",
      "Epoch 3203, Loss: 1.9643285274505615, Final Batch Loss: 0.3384024202823639\n",
      "Epoch 3204, Loss: 1.7957025468349457, Final Batch Loss: 0.22213789820671082\n",
      "Epoch 3205, Loss: 1.66323621571064, Final Batch Loss: 0.2370811253786087\n",
      "Epoch 3206, Loss: 1.8508907556533813, Final Batch Loss: 0.28724774718284607\n",
      "Epoch 3207, Loss: 1.7444533705711365, Final Batch Loss: 0.33712539076805115\n",
      "Epoch 3208, Loss: 1.7404873073101044, Final Batch Loss: 0.36324355006217957\n",
      "Epoch 3209, Loss: 2.0242339968681335, Final Batch Loss: 0.5130252242088318\n",
      "Epoch 3210, Loss: 1.85481595993042, Final Batch Loss: 0.31959348917007446\n",
      "Epoch 3211, Loss: 1.7580546438694, Final Batch Loss: 0.1835760772228241\n",
      "Epoch 3212, Loss: 1.6301568448543549, Final Batch Loss: 0.18166077136993408\n",
      "Epoch 3213, Loss: 1.9599736630916595, Final Batch Loss: 0.5338778495788574\n",
      "Epoch 3214, Loss: 1.9510460495948792, Final Batch Loss: 0.49442315101623535\n",
      "Epoch 3215, Loss: 1.929662138223648, Final Batch Loss: 0.42207464575767517\n",
      "Epoch 3216, Loss: 1.804243266582489, Final Batch Loss: 0.30306872725486755\n",
      "Epoch 3217, Loss: 1.8699079304933548, Final Batch Loss: 0.2201799899339676\n",
      "Epoch 3218, Loss: 1.960019737482071, Final Batch Loss: 0.4860469400882721\n",
      "Epoch 3219, Loss: 1.5940380841493607, Final Batch Loss: 0.1572667807340622\n",
      "Epoch 3220, Loss: 1.5785990580916405, Final Batch Loss: 0.11280303448438644\n",
      "Epoch 3221, Loss: 1.6662842780351639, Final Batch Loss: 0.22244812548160553\n",
      "Epoch 3222, Loss: 1.5230024084448814, Final Batch Loss: 0.08536946028470993\n",
      "Epoch 3223, Loss: 1.6135778427124023, Final Batch Loss: 0.2637020945549011\n",
      "Epoch 3224, Loss: 1.4951852224767208, Final Batch Loss: 0.041008707135915756\n",
      "Epoch 3225, Loss: 1.7450317442417145, Final Batch Loss: 0.23654243350028992\n",
      "Epoch 3226, Loss: 2.139491617679596, Final Batch Loss: 0.628545343875885\n",
      "Epoch 3227, Loss: 1.6423227190971375, Final Batch Loss: 0.24149325489997864\n",
      "Epoch 3228, Loss: 2.0490227341651917, Final Batch Loss: 0.4666975438594818\n",
      "Epoch 3229, Loss: 1.6657944023609161, Final Batch Loss: 0.2548653483390808\n",
      "Epoch 3230, Loss: 1.5982304811477661, Final Batch Loss: 0.14471113681793213\n",
      "Epoch 3231, Loss: 1.9848776459693909, Final Batch Loss: 0.5082741379737854\n",
      "Epoch 3232, Loss: 1.8930118680000305, Final Batch Loss: 0.4227527976036072\n",
      "Epoch 3233, Loss: 1.6183053851127625, Final Batch Loss: 0.2915569841861725\n",
      "Epoch 3234, Loss: 1.8198419511318207, Final Batch Loss: 0.41207313537597656\n",
      "Epoch 3235, Loss: 1.7888940274715424, Final Batch Loss: 0.30703479051589966\n",
      "Epoch 3236, Loss: 2.0697785317897797, Final Batch Loss: 0.7256450057029724\n",
      "Epoch 3237, Loss: 1.3948481529951096, Final Batch Loss: 0.07826162874698639\n",
      "Epoch 3238, Loss: 1.965900868177414, Final Batch Loss: 0.4068807065486908\n",
      "Epoch 3239, Loss: 1.4264434464275837, Final Batch Loss: 0.049888331443071365\n",
      "Epoch 3240, Loss: 1.8369418382644653, Final Batch Loss: 0.37318092584609985\n",
      "Epoch 3241, Loss: 2.1266893446445465, Final Batch Loss: 0.7073783278465271\n",
      "Epoch 3242, Loss: 1.7050334811210632, Final Batch Loss: 0.24056926369667053\n",
      "Epoch 3243, Loss: 1.6512230038642883, Final Batch Loss: 0.10334736108779907\n",
      "Epoch 3244, Loss: 1.5488279089331627, Final Batch Loss: 0.09392478317022324\n",
      "Epoch 3245, Loss: 1.5286914110183716, Final Batch Loss: 0.2092287540435791\n",
      "Epoch 3246, Loss: 1.6615461111068726, Final Batch Loss: 0.27263766527175903\n",
      "Epoch 3247, Loss: 1.56463373452425, Final Batch Loss: 0.1162426546216011\n",
      "Epoch 3248, Loss: 1.7866233587265015, Final Batch Loss: 0.39880043268203735\n",
      "Epoch 3249, Loss: 1.478510320186615, Final Batch Loss: 0.05806562304496765\n",
      "Epoch 3250, Loss: 1.7664876878261566, Final Batch Loss: 0.3448486328125\n",
      "Epoch 3251, Loss: 1.8395769000053406, Final Batch Loss: 0.44628384709358215\n",
      "Epoch 3252, Loss: 1.6690946221351624, Final Batch Loss: 0.21184372901916504\n",
      "Epoch 3253, Loss: 1.783680945634842, Final Batch Loss: 0.4288732409477234\n",
      "Epoch 3254, Loss: 2.0418268144130707, Final Batch Loss: 0.5944412350654602\n",
      "Epoch 3255, Loss: 1.8665768504142761, Final Batch Loss: 0.39202192425727844\n",
      "Epoch 3256, Loss: 1.7354668229818344, Final Batch Loss: 0.16187359392642975\n",
      "Epoch 3257, Loss: 2.089028477668762, Final Batch Loss: 0.45757296681404114\n",
      "Epoch 3258, Loss: 1.7668601274490356, Final Batch Loss: 0.333567351102829\n",
      "Epoch 3259, Loss: 2.1473861634731293, Final Batch Loss: 0.5267134308815002\n",
      "Epoch 3260, Loss: 1.9747532308101654, Final Batch Loss: 0.5083229541778564\n",
      "Epoch 3261, Loss: 1.8382898271083832, Final Batch Loss: 0.27409863471984863\n",
      "Epoch 3262, Loss: 2.123426705598831, Final Batch Loss: 0.5605930089950562\n",
      "Epoch 3263, Loss: 2.1857546269893646, Final Batch Loss: 0.47193318605422974\n",
      "Epoch 3264, Loss: 1.8678967952728271, Final Batch Loss: 0.18898847699165344\n",
      "Epoch 3265, Loss: 1.749382197856903, Final Batch Loss: 0.17721149325370789\n",
      "Epoch 3266, Loss: 1.722318023443222, Final Batch Loss: 0.26587408781051636\n",
      "Epoch 3267, Loss: 2.431756019592285, Final Batch Loss: 0.7890258431434631\n",
      "Epoch 3268, Loss: 1.740993082523346, Final Batch Loss: 0.2769063711166382\n",
      "Epoch 3269, Loss: 1.8549413233995438, Final Batch Loss: 0.19637520611286163\n",
      "Epoch 3270, Loss: 1.757287323474884, Final Batch Loss: 0.3371100425720215\n",
      "Epoch 3271, Loss: 1.881539225578308, Final Batch Loss: 0.45591041445732117\n",
      "Epoch 3272, Loss: 1.5924886167049408, Final Batch Loss: 0.15245112776756287\n",
      "Epoch 3273, Loss: 2.473034143447876, Final Batch Loss: 0.9704455137252808\n",
      "Epoch 3274, Loss: 2.5971957743167877, Final Batch Loss: 1.2845475673675537\n",
      "Epoch 3275, Loss: 1.9370337128639221, Final Batch Loss: 0.3289789855480194\n",
      "Epoch 3276, Loss: 2.205480009317398, Final Batch Loss: 0.4612825810909271\n",
      "Epoch 3277, Loss: 2.1929098069667816, Final Batch Loss: 0.5891116857528687\n",
      "Epoch 3278, Loss: 1.9919477701187134, Final Batch Loss: 0.2679211497306824\n",
      "Epoch 3279, Loss: 2.1287251114845276, Final Batch Loss: 0.5419661402702332\n",
      "Epoch 3280, Loss: 1.7678310871124268, Final Batch Loss: 0.28389543294906616\n",
      "Epoch 3281, Loss: 1.7096723318099976, Final Batch Loss: 0.18936371803283691\n",
      "Epoch 3282, Loss: 1.6948329657316208, Final Batch Loss: 0.0990092009305954\n",
      "Epoch 3283, Loss: 1.6630233824253082, Final Batch Loss: 0.16952499747276306\n",
      "Epoch 3284, Loss: 1.940776288509369, Final Batch Loss: 0.40341153740882874\n",
      "Epoch 3285, Loss: 1.9030336141586304, Final Batch Loss: 0.3840636909008026\n",
      "Epoch 3286, Loss: 1.9665838479995728, Final Batch Loss: 0.461636483669281\n",
      "Epoch 3287, Loss: 1.9004310369491577, Final Batch Loss: 0.40180522203445435\n",
      "Epoch 3288, Loss: 2.005228728055954, Final Batch Loss: 0.513148844242096\n",
      "Epoch 3289, Loss: 2.0029586255550385, Final Batch Loss: 0.5428118109703064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3290, Loss: 1.5994803309440613, Final Batch Loss: 0.286302387714386\n",
      "Epoch 3291, Loss: 1.809081882238388, Final Batch Loss: 0.37922120094299316\n",
      "Epoch 3292, Loss: 1.8869928121566772, Final Batch Loss: 0.40496304631233215\n",
      "Epoch 3293, Loss: 1.8202517926692963, Final Batch Loss: 0.46998950839042664\n",
      "Epoch 3294, Loss: 1.5241800248622894, Final Batch Loss: 0.18669018149375916\n",
      "Epoch 3295, Loss: 1.7997027039527893, Final Batch Loss: 0.36932533979415894\n",
      "Epoch 3296, Loss: 1.5891956388950348, Final Batch Loss: 0.17227014899253845\n",
      "Epoch 3297, Loss: 1.6852195709943771, Final Batch Loss: 0.23001264035701752\n",
      "Epoch 3298, Loss: 1.7759045958518982, Final Batch Loss: 0.33616238832473755\n",
      "Epoch 3299, Loss: 1.7597108483314514, Final Batch Loss: 0.36964747309684753\n",
      "Epoch 3300, Loss: 2.215262711048126, Final Batch Loss: 0.7207756638526917\n",
      "Epoch 3301, Loss: 1.7070141732692719, Final Batch Loss: 0.3108905255794525\n",
      "Epoch 3302, Loss: 1.7091746628284454, Final Batch Loss: 0.3240102231502533\n",
      "Epoch 3303, Loss: 1.6850089728832245, Final Batch Loss: 0.17707112431526184\n",
      "Epoch 3304, Loss: 1.7752453684806824, Final Batch Loss: 0.33450812101364136\n",
      "Epoch 3305, Loss: 1.6945910453796387, Final Batch Loss: 0.31412553787231445\n",
      "Epoch 3306, Loss: 2.0261839032173157, Final Batch Loss: 0.5985276103019714\n",
      "Epoch 3307, Loss: 2.1023455560207367, Final Batch Loss: 0.7340806126594543\n",
      "Epoch 3308, Loss: 1.94320410490036, Final Batch Loss: 0.48099207878112793\n",
      "Epoch 3309, Loss: 1.7197888493537903, Final Batch Loss: 0.39675384759902954\n",
      "Epoch 3310, Loss: 1.8375272005796432, Final Batch Loss: 0.45994654297828674\n",
      "Epoch 3311, Loss: 2.157711625099182, Final Batch Loss: 0.632526695728302\n",
      "Epoch 3312, Loss: 1.6896132975816727, Final Batch Loss: 0.2368466705083847\n",
      "Epoch 3313, Loss: 1.9673507809638977, Final Batch Loss: 0.44251587986946106\n",
      "Epoch 3314, Loss: 1.6543805599212646, Final Batch Loss: 0.11569947004318237\n",
      "Epoch 3315, Loss: 1.8525269627571106, Final Batch Loss: 0.3439781963825226\n",
      "Epoch 3316, Loss: 1.6716740131378174, Final Batch Loss: 0.221195250749588\n",
      "Epoch 3317, Loss: 1.9165683686733246, Final Batch Loss: 0.31683582067489624\n",
      "Epoch 3318, Loss: 1.94962677359581, Final Batch Loss: 0.5242030024528503\n",
      "Epoch 3319, Loss: 1.7828516513109207, Final Batch Loss: 0.2105037122964859\n",
      "Epoch 3320, Loss: 1.8551748394966125, Final Batch Loss: 0.2802759110927582\n",
      "Epoch 3321, Loss: 1.982113629579544, Final Batch Loss: 0.5633992552757263\n",
      "Epoch 3322, Loss: 1.8851380050182343, Final Batch Loss: 0.3641962707042694\n",
      "Epoch 3323, Loss: 1.8675333559513092, Final Batch Loss: 0.41547682881355286\n",
      "Epoch 3324, Loss: 1.5405988544225693, Final Batch Loss: 0.13431783020496368\n",
      "Epoch 3325, Loss: 1.9936640858650208, Final Batch Loss: 0.5871208906173706\n",
      "Epoch 3326, Loss: 2.0340465903282166, Final Batch Loss: 0.5776878595352173\n",
      "Epoch 3327, Loss: 1.61207015812397, Final Batch Loss: 0.19589225947856903\n",
      "Epoch 3328, Loss: 1.4187980182468891, Final Batch Loss: 0.025523800402879715\n",
      "Epoch 3329, Loss: 1.837734431028366, Final Batch Loss: 0.5145107507705688\n",
      "Epoch 3330, Loss: 1.758098304271698, Final Batch Loss: 0.33317893743515015\n",
      "Epoch 3331, Loss: 1.8237144649028778, Final Batch Loss: 0.29498809576034546\n",
      "Epoch 3332, Loss: 1.60344747453928, Final Batch Loss: 0.09947574883699417\n",
      "Epoch 3333, Loss: 1.991950362920761, Final Batch Loss: 0.5035041570663452\n",
      "Epoch 3334, Loss: 1.611881673336029, Final Batch Loss: 0.17854535579681396\n",
      "Epoch 3335, Loss: 2.0862030386924744, Final Batch Loss: 0.5642231106758118\n",
      "Epoch 3336, Loss: 1.6754324436187744, Final Batch Loss: 0.3071799874305725\n",
      "Epoch 3337, Loss: 1.5320304036140442, Final Batch Loss: 0.23778000473976135\n",
      "Epoch 3338, Loss: 1.7829916179180145, Final Batch Loss: 0.3210320770740509\n",
      "Epoch 3339, Loss: 1.724320650100708, Final Batch Loss: 0.30359503626823425\n",
      "Epoch 3340, Loss: 1.7965681850910187, Final Batch Loss: 0.4081293046474457\n",
      "Epoch 3341, Loss: 2.0892566442489624, Final Batch Loss: 0.6169002652168274\n",
      "Epoch 3342, Loss: 2.043168932199478, Final Batch Loss: 0.5456961393356323\n",
      "Epoch 3343, Loss: 1.9753184914588928, Final Batch Loss: 0.560170590877533\n",
      "Epoch 3344, Loss: 1.5911868661642075, Final Batch Loss: 0.22234170138835907\n",
      "Epoch 3345, Loss: 1.8085430562496185, Final Batch Loss: 0.3016812205314636\n",
      "Epoch 3346, Loss: 1.8443891406059265, Final Batch Loss: 0.30448904633522034\n",
      "Epoch 3347, Loss: 1.5311749018728733, Final Batch Loss: 0.05105021968483925\n",
      "Epoch 3348, Loss: 1.791049063205719, Final Batch Loss: 0.35064461827278137\n",
      "Epoch 3349, Loss: 2.077347904443741, Final Batch Loss: 0.679813802242279\n",
      "Epoch 3350, Loss: 1.8048818409442902, Final Batch Loss: 0.34799739718437195\n",
      "Epoch 3351, Loss: 1.9836094677448273, Final Batch Loss: 0.6019526124000549\n",
      "Epoch 3352, Loss: 1.6136905252933502, Final Batch Loss: 0.25927671790122986\n",
      "Epoch 3353, Loss: 1.6457707583904266, Final Batch Loss: 0.23293936252593994\n",
      "Epoch 3354, Loss: 1.7149548530578613, Final Batch Loss: 0.31882479786872864\n",
      "Epoch 3355, Loss: 1.6770116090774536, Final Batch Loss: 0.21027973294258118\n",
      "Epoch 3356, Loss: 1.547522097826004, Final Batch Loss: 0.18663707375526428\n",
      "Epoch 3357, Loss: 1.4249940887093544, Final Batch Loss: 0.11505932360887527\n",
      "Epoch 3358, Loss: 1.8014167547225952, Final Batch Loss: 0.3923056423664093\n",
      "Epoch 3359, Loss: 1.6058113425970078, Final Batch Loss: 0.2281961292028427\n",
      "Epoch 3360, Loss: 2.6555283963680267, Final Batch Loss: 1.2223902940750122\n",
      "Epoch 3361, Loss: 1.5360631495714188, Final Batch Loss: 0.17524145543575287\n",
      "Epoch 3362, Loss: 1.9881880283355713, Final Batch Loss: 0.5254318118095398\n",
      "Epoch 3363, Loss: 2.451303780078888, Final Batch Loss: 0.8643975853919983\n",
      "Epoch 3364, Loss: 1.7750267684459686, Final Batch Loss: 0.29136922955513\n",
      "Epoch 3365, Loss: 2.036205768585205, Final Batch Loss: 0.40637966990470886\n",
      "Epoch 3366, Loss: 2.0678136944770813, Final Batch Loss: 0.42601361870765686\n",
      "Epoch 3367, Loss: 2.0317181944847107, Final Batch Loss: 0.5497312545776367\n",
      "Epoch 3368, Loss: 1.530229777097702, Final Batch Loss: 0.17359545826911926\n",
      "Epoch 3369, Loss: 1.9620032608509064, Final Batch Loss: 0.3460957407951355\n",
      "Epoch 3370, Loss: 1.9119816422462463, Final Batch Loss: 0.34852102398872375\n",
      "Epoch 3371, Loss: 1.682449921965599, Final Batch Loss: 0.19277633726596832\n",
      "Epoch 3372, Loss: 2.004133939743042, Final Batch Loss: 0.38887569308280945\n",
      "Epoch 3373, Loss: 2.0209752023220062, Final Batch Loss: 0.5162160992622375\n",
      "Epoch 3374, Loss: 1.7847378551959991, Final Batch Loss: 0.3460882306098938\n",
      "Epoch 3375, Loss: 1.5544672012329102, Final Batch Loss: 0.20535799860954285\n",
      "Epoch 3376, Loss: 1.8168402314186096, Final Batch Loss: 0.3941330313682556\n",
      "Epoch 3377, Loss: 1.987067312002182, Final Batch Loss: 0.6007055640220642\n",
      "Epoch 3378, Loss: 1.6458998918533325, Final Batch Loss: 0.1370728313922882\n",
      "Epoch 3379, Loss: 1.7918310761451721, Final Batch Loss: 0.4065296947956085\n",
      "Epoch 3380, Loss: 1.7919553816318512, Final Batch Loss: 0.37752142548561096\n",
      "Epoch 3381, Loss: 2.2442627549171448, Final Batch Loss: 0.869566798210144\n",
      "Epoch 3382, Loss: 1.8258312344551086, Final Batch Loss: 0.3526809513568878\n",
      "Epoch 3383, Loss: 1.8276866376399994, Final Batch Loss: 0.3852744996547699\n",
      "Epoch 3384, Loss: 1.9062860608100891, Final Batch Loss: 0.4580453336238861\n",
      "Epoch 3385, Loss: 1.520976297557354, Final Batch Loss: 0.06349091976881027\n",
      "Epoch 3386, Loss: 1.7310254573822021, Final Batch Loss: 0.2965005338191986\n",
      "Epoch 3387, Loss: 1.6845778971910477, Final Batch Loss: 0.18060536682605743\n",
      "Epoch 3388, Loss: 1.9946974515914917, Final Batch Loss: 0.5673267245292664\n",
      "Epoch 3389, Loss: 1.5354230552911758, Final Batch Loss: 0.12910492718219757\n",
      "Epoch 3390, Loss: 1.8249849677085876, Final Batch Loss: 0.3842149078845978\n",
      "Epoch 3391, Loss: 1.7313195765018463, Final Batch Loss: 0.40235263109207153\n",
      "Epoch 3392, Loss: 1.7684752941131592, Final Batch Loss: 0.33382412791252136\n",
      "Epoch 3393, Loss: 1.5097002685070038, Final Batch Loss: 0.1764465570449829\n",
      "Epoch 3394, Loss: 1.6692376285791397, Final Batch Loss: 0.20608894526958466\n",
      "Epoch 3395, Loss: 1.6389997601509094, Final Batch Loss: 0.30185598134994507\n",
      "Epoch 3396, Loss: 1.7556923031806946, Final Batch Loss: 0.3549250662326813\n",
      "Epoch 3397, Loss: 1.958065152168274, Final Batch Loss: 0.44730186462402344\n",
      "Epoch 3398, Loss: 1.7590930759906769, Final Batch Loss: 0.2622303068637848\n",
      "Epoch 3399, Loss: 1.90738445520401, Final Batch Loss: 0.5150508284568787\n",
      "Epoch 3400, Loss: 1.7367469370365143, Final Batch Loss: 0.24176868796348572\n",
      "Epoch 3401, Loss: 1.5510194897651672, Final Batch Loss: 0.27345010638237\n",
      "Epoch 3402, Loss: 1.8060766160488129, Final Batch Loss: 0.4102332592010498\n",
      "Epoch 3403, Loss: 1.7086674273014069, Final Batch Loss: 0.33497047424316406\n",
      "Epoch 3404, Loss: 1.5207083225250244, Final Batch Loss: 0.13175231218338013\n",
      "Epoch 3405, Loss: 2.128854900598526, Final Batch Loss: 0.7250032424926758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3406, Loss: 1.5529136210680008, Final Batch Loss: 0.11108706891536713\n",
      "Epoch 3407, Loss: 1.6708002388477325, Final Batch Loss: 0.30452674627304077\n",
      "Epoch 3408, Loss: 1.504167065024376, Final Batch Loss: 0.11384634673595428\n",
      "Epoch 3409, Loss: 1.5232724100351334, Final Batch Loss: 0.19484789669513702\n",
      "Epoch 3410, Loss: 1.468019299209118, Final Batch Loss: 0.11488258093595505\n",
      "Epoch 3411, Loss: 1.8623719960451126, Final Batch Loss: 0.505659282207489\n",
      "Epoch 3412, Loss: 2.047182321548462, Final Batch Loss: 0.7257800102233887\n",
      "Epoch 3413, Loss: 2.139722168445587, Final Batch Loss: 0.5198814868927002\n",
      "Epoch 3414, Loss: 2.412121683359146, Final Batch Loss: 0.7148996591567993\n",
      "Epoch 3415, Loss: 1.950114667415619, Final Batch Loss: 0.3401780426502228\n",
      "Epoch 3416, Loss: 1.7845974825322628, Final Batch Loss: 0.06162377819418907\n",
      "Epoch 3417, Loss: 1.9794232547283173, Final Batch Loss: 0.29853034019470215\n",
      "Epoch 3418, Loss: 1.8386962413787842, Final Batch Loss: 0.3725271224975586\n",
      "Epoch 3419, Loss: 2.041069984436035, Final Batch Loss: 0.5172930359840393\n",
      "Epoch 3420, Loss: 1.6104141548275948, Final Batch Loss: 0.11995450407266617\n",
      "Epoch 3421, Loss: 1.7150650024414062, Final Batch Loss: 0.21662747859954834\n",
      "Epoch 3422, Loss: 1.65752774477005, Final Batch Loss: 0.23603865504264832\n",
      "Epoch 3423, Loss: 1.9163734912872314, Final Batch Loss: 0.36940619349479675\n",
      "Epoch 3424, Loss: 1.6102901548147202, Final Batch Loss: 0.1952304095029831\n",
      "Epoch 3425, Loss: 2.1225868463516235, Final Batch Loss: 0.46821528673171997\n",
      "Epoch 3426, Loss: 2.0285403430461884, Final Batch Loss: 0.39727330207824707\n",
      "Epoch 3427, Loss: 1.5712764412164688, Final Batch Loss: 0.14438487589359283\n",
      "Epoch 3428, Loss: 2.1038343012332916, Final Batch Loss: 0.6675520539283752\n",
      "Epoch 3429, Loss: 1.8099004924297333, Final Batch Loss: 0.2849191725254059\n",
      "Epoch 3430, Loss: 1.820763885974884, Final Batch Loss: 0.3323286175727844\n",
      "Epoch 3431, Loss: 1.7798629701137543, Final Batch Loss: 0.3001136779785156\n",
      "Epoch 3432, Loss: 2.598790258169174, Final Batch Loss: 1.1283814907073975\n",
      "Epoch 3433, Loss: 1.8053203523159027, Final Batch Loss: 0.39008936285972595\n",
      "Epoch 3434, Loss: 2.039103090763092, Final Batch Loss: 0.5366089940071106\n",
      "Epoch 3435, Loss: 1.7803975939750671, Final Batch Loss: 0.2528681457042694\n",
      "Epoch 3436, Loss: 1.7837359011173248, Final Batch Loss: 0.2901594340801239\n",
      "Epoch 3437, Loss: 1.908768355846405, Final Batch Loss: 0.3363519608974457\n",
      "Epoch 3438, Loss: 1.8023996651172638, Final Batch Loss: 0.22890815138816833\n",
      "Epoch 3439, Loss: 1.511979378759861, Final Batch Loss: 0.07637760788202286\n",
      "Epoch 3440, Loss: 1.7972447574138641, Final Batch Loss: 0.23685181140899658\n",
      "Epoch 3441, Loss: 1.6441572606563568, Final Batch Loss: 0.2720482349395752\n",
      "Epoch 3442, Loss: 1.6618681252002716, Final Batch Loss: 0.2810605764389038\n",
      "Epoch 3443, Loss: 2.057106465101242, Final Batch Loss: 0.640168309211731\n",
      "Epoch 3444, Loss: 1.6082722395658493, Final Batch Loss: 0.1711701601743698\n",
      "Epoch 3445, Loss: 1.870443344116211, Final Batch Loss: 0.4240760803222656\n",
      "Epoch 3446, Loss: 1.7094492316246033, Final Batch Loss: 0.38638681173324585\n",
      "Epoch 3447, Loss: 1.614311933517456, Final Batch Loss: 0.24655857682228088\n",
      "Epoch 3448, Loss: 1.7141853719949722, Final Batch Loss: 0.18794424831867218\n",
      "Epoch 3449, Loss: 1.696631759405136, Final Batch Loss: 0.27064359188079834\n",
      "Epoch 3450, Loss: 2.0383190512657166, Final Batch Loss: 0.49796009063720703\n",
      "Epoch 3451, Loss: 1.6053949892520905, Final Batch Loss: 0.2652951776981354\n",
      "Epoch 3452, Loss: 1.5883911699056625, Final Batch Loss: 0.14062105119228363\n",
      "Epoch 3453, Loss: 1.77352374792099, Final Batch Loss: 0.23813176155090332\n",
      "Epoch 3454, Loss: 1.9909241795539856, Final Batch Loss: 0.5794311761856079\n",
      "Epoch 3455, Loss: 1.8701163828372955, Final Batch Loss: 0.4414582848548889\n",
      "Epoch 3456, Loss: 2.012518361210823, Final Batch Loss: 0.635208785533905\n",
      "Epoch 3457, Loss: 2.006259322166443, Final Batch Loss: 0.4585123658180237\n",
      "Epoch 3458, Loss: 1.9244961142539978, Final Batch Loss: 0.5038012266159058\n",
      "Epoch 3459, Loss: 2.1704632341861725, Final Batch Loss: 0.6391549706459045\n",
      "Epoch 3460, Loss: 1.693487361073494, Final Batch Loss: 0.21483276784420013\n",
      "Epoch 3461, Loss: 1.9041218757629395, Final Batch Loss: 0.35266488790512085\n",
      "Epoch 3462, Loss: 1.7894732058048248, Final Batch Loss: 0.3735975921154022\n",
      "Epoch 3463, Loss: 2.009075850248337, Final Batch Loss: 0.5505049228668213\n",
      "Epoch 3464, Loss: 1.6778913140296936, Final Batch Loss: 0.30094626545906067\n",
      "Epoch 3465, Loss: 1.6282761916518211, Final Batch Loss: 0.11594858020544052\n",
      "Epoch 3466, Loss: 1.653467059135437, Final Batch Loss: 0.22498169541358948\n",
      "Epoch 3467, Loss: 1.816411405801773, Final Batch Loss: 0.42058929800987244\n",
      "Epoch 3468, Loss: 1.7604903876781464, Final Batch Loss: 0.3672584891319275\n",
      "Epoch 3469, Loss: 1.534329742193222, Final Batch Loss: 0.2187117040157318\n",
      "Epoch 3470, Loss: 2.00022354722023, Final Batch Loss: 0.5494635701179504\n",
      "Epoch 3471, Loss: 1.7017896473407745, Final Batch Loss: 0.2378523349761963\n",
      "Epoch 3472, Loss: 1.878583163022995, Final Batch Loss: 0.5324009656906128\n",
      "Epoch 3473, Loss: 2.1162136793136597, Final Batch Loss: 0.7836052775382996\n",
      "Epoch 3474, Loss: 1.763381466269493, Final Batch Loss: 0.2022121399641037\n",
      "Epoch 3475, Loss: 1.773400604724884, Final Batch Loss: 0.393391877412796\n",
      "Epoch 3476, Loss: 1.8147014379501343, Final Batch Loss: 0.46971192955970764\n",
      "Epoch 3477, Loss: 2.1245439648628235, Final Batch Loss: 0.5946535468101501\n",
      "Epoch 3478, Loss: 1.6876174211502075, Final Batch Loss: 0.29627442359924316\n",
      "Epoch 3479, Loss: 1.7456076443195343, Final Batch Loss: 0.25341567397117615\n",
      "Epoch 3480, Loss: 1.9165212512016296, Final Batch Loss: 0.505460798740387\n",
      "Epoch 3481, Loss: 1.7225696593523026, Final Batch Loss: 0.18693126738071442\n",
      "Epoch 3482, Loss: 2.0094815492630005, Final Batch Loss: 0.6230164170265198\n",
      "Epoch 3483, Loss: 1.8431170284748077, Final Batch Loss: 0.2825787365436554\n",
      "Epoch 3484, Loss: 2.094157427549362, Final Batch Loss: 0.5981359481811523\n",
      "Epoch 3485, Loss: 2.1937568187713623, Final Batch Loss: 0.8001644015312195\n",
      "Epoch 3486, Loss: 1.744921088218689, Final Batch Loss: 0.37061557173728943\n",
      "Epoch 3487, Loss: 1.8162842094898224, Final Batch Loss: 0.31155356764793396\n",
      "Epoch 3488, Loss: 1.7246381044387817, Final Batch Loss: 0.3427960276603699\n",
      "Epoch 3489, Loss: 1.63740573823452, Final Batch Loss: 0.17791278660297394\n",
      "Epoch 3490, Loss: 1.6766971200704575, Final Batch Loss: 0.13997377455234528\n",
      "Epoch 3491, Loss: 1.6161015331745148, Final Batch Loss: 0.2854641377925873\n",
      "Epoch 3492, Loss: 2.331909328699112, Final Batch Loss: 0.9728798270225525\n",
      "Epoch 3493, Loss: 2.321447402238846, Final Batch Loss: 0.8554297089576721\n",
      "Epoch 3494, Loss: 1.886178433895111, Final Batch Loss: 0.44880539178848267\n",
      "Epoch 3495, Loss: 1.6795311123132706, Final Batch Loss: 0.20413677394390106\n",
      "Epoch 3496, Loss: 1.5369603335857391, Final Batch Loss: 0.14323973655700684\n",
      "Epoch 3497, Loss: 1.8525803089141846, Final Batch Loss: 0.4962916672229767\n",
      "Epoch 3498, Loss: 1.7872932851314545, Final Batch Loss: 0.3670315146446228\n",
      "Epoch 3499, Loss: 1.9189229607582092, Final Batch Loss: 0.47170358896255493\n",
      "Epoch 3500, Loss: 1.7624800205230713, Final Batch Loss: 0.2615687847137451\n",
      "Epoch 3501, Loss: 1.9666414856910706, Final Batch Loss: 0.598759651184082\n",
      "Epoch 3502, Loss: 1.8477979898452759, Final Batch Loss: 0.29342713952064514\n",
      "Epoch 3503, Loss: 2.188496083021164, Final Batch Loss: 0.620378851890564\n",
      "Epoch 3504, Loss: 1.760781079530716, Final Batch Loss: 0.2783581614494324\n",
      "Epoch 3505, Loss: 1.7052939385175705, Final Batch Loss: 0.18237726390361786\n",
      "Epoch 3506, Loss: 1.5414761379361153, Final Batch Loss: 0.06357497721910477\n",
      "Epoch 3507, Loss: 1.547859475016594, Final Batch Loss: 0.14408712089061737\n",
      "Epoch 3508, Loss: 1.475935012102127, Final Batch Loss: 0.10101109743118286\n",
      "Epoch 3509, Loss: 1.7734187841415405, Final Batch Loss: 0.47494131326675415\n",
      "Epoch 3510, Loss: 1.686950534582138, Final Batch Loss: 0.37945303320884705\n",
      "Epoch 3511, Loss: 1.8797048926353455, Final Batch Loss: 0.3200928270816803\n",
      "Epoch 3512, Loss: 1.565361987799406, Final Batch Loss: 0.04992286488413811\n",
      "Epoch 3513, Loss: 2.0369425415992737, Final Batch Loss: 0.5865293741226196\n",
      "Epoch 3514, Loss: 1.6845709681510925, Final Batch Loss: 0.27388331294059753\n",
      "Epoch 3515, Loss: 1.7580718994140625, Final Batch Loss: 0.46760421991348267\n",
      "Epoch 3516, Loss: 1.789488047361374, Final Batch Loss: 0.4121060371398926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3517, Loss: 1.671934962272644, Final Batch Loss: 0.17351174354553223\n",
      "Epoch 3518, Loss: 1.6569068431854248, Final Batch Loss: 0.3665630519390106\n",
      "Epoch 3519, Loss: 1.725468248128891, Final Batch Loss: 0.28214675188064575\n",
      "Epoch 3520, Loss: 1.7329452335834503, Final Batch Loss: 0.44520166516304016\n",
      "Epoch 3521, Loss: 1.9331556558609009, Final Batch Loss: 0.5483071804046631\n",
      "Epoch 3522, Loss: 1.689322680234909, Final Batch Loss: 0.3036908209323883\n",
      "Epoch 3523, Loss: 1.6770178973674774, Final Batch Loss: 0.3162488639354706\n",
      "Epoch 3524, Loss: 2.010677456855774, Final Batch Loss: 0.644112229347229\n",
      "Epoch 3525, Loss: 1.6994785219430923, Final Batch Loss: 0.18409554660320282\n",
      "Epoch 3526, Loss: 1.8495438992977142, Final Batch Loss: 0.37809106707572937\n",
      "Epoch 3527, Loss: 2.155876874923706, Final Batch Loss: 0.7304315567016602\n",
      "Epoch 3528, Loss: 1.988464117050171, Final Batch Loss: 0.5361183881759644\n",
      "Epoch 3529, Loss: 1.5970681607723236, Final Batch Loss: 0.12461787462234497\n",
      "Epoch 3530, Loss: 1.4638219438493252, Final Batch Loss: 0.0424480102956295\n",
      "Epoch 3531, Loss: 2.0108028650283813, Final Batch Loss: 0.6241120100021362\n",
      "Epoch 3532, Loss: 1.7794017493724823, Final Batch Loss: 0.2640727162361145\n",
      "Epoch 3533, Loss: 1.7220886051654816, Final Batch Loss: 0.3166074752807617\n",
      "Epoch 3534, Loss: 1.681339830160141, Final Batch Loss: 0.3386523723602295\n",
      "Epoch 3535, Loss: 1.8386599123477936, Final Batch Loss: 0.37883082032203674\n",
      "Epoch 3536, Loss: 1.8038491010665894, Final Batch Loss: 0.441962867975235\n",
      "Epoch 3537, Loss: 2.0765488147735596, Final Batch Loss: 0.47909241914749146\n",
      "Epoch 3538, Loss: 1.6817400753498077, Final Batch Loss: 0.39325329661369324\n",
      "Epoch 3539, Loss: 1.8212377429008484, Final Batch Loss: 0.34649279713630676\n",
      "Epoch 3540, Loss: 2.011403650045395, Final Batch Loss: 0.6280314326286316\n",
      "Epoch 3541, Loss: 2.0948160886764526, Final Batch Loss: 0.5228223204612732\n",
      "Epoch 3542, Loss: 2.268313705921173, Final Batch Loss: 0.7669788002967834\n",
      "Epoch 3543, Loss: 2.011686474084854, Final Batch Loss: 0.3595179617404938\n",
      "Epoch 3544, Loss: 1.869212955236435, Final Batch Loss: 0.41916513442993164\n",
      "Epoch 3545, Loss: 1.8791704773902893, Final Batch Loss: 0.38644179701805115\n",
      "Epoch 3546, Loss: 1.919253408908844, Final Batch Loss: 0.49661022424697876\n",
      "Epoch 3547, Loss: 1.805773764848709, Final Batch Loss: 0.38509392738342285\n",
      "Epoch 3548, Loss: 2.2069858610630035, Final Batch Loss: 0.7108176946640015\n",
      "Epoch 3549, Loss: 1.755202978849411, Final Batch Loss: 0.3182654082775116\n",
      "Epoch 3550, Loss: 1.5098875351250172, Final Batch Loss: 0.03204454854130745\n",
      "Epoch 3551, Loss: 1.585117094218731, Final Batch Loss: 0.1229468509554863\n",
      "Epoch 3552, Loss: 2.0119252800941467, Final Batch Loss: 0.5663820505142212\n",
      "Epoch 3553, Loss: 1.8131277859210968, Final Batch Loss: 0.4358993172645569\n",
      "Epoch 3554, Loss: 1.9303767681121826, Final Batch Loss: 0.43120279908180237\n",
      "Epoch 3555, Loss: 1.849932461977005, Final Batch Loss: 0.5341747403144836\n",
      "Epoch 3556, Loss: 1.6170636415481567, Final Batch Loss: 0.22773563861846924\n",
      "Epoch 3557, Loss: 1.4970925226807594, Final Batch Loss: 0.07566113024950027\n",
      "Epoch 3558, Loss: 2.2401019036769867, Final Batch Loss: 0.7470425963401794\n",
      "Epoch 3559, Loss: 1.719621866941452, Final Batch Loss: 0.2681967616081238\n",
      "Epoch 3560, Loss: 1.6971388757228851, Final Batch Loss: 0.3709200918674469\n",
      "Epoch 3561, Loss: 1.8486264050006866, Final Batch Loss: 0.3373493254184723\n",
      "Epoch 3562, Loss: 1.5282617285847664, Final Batch Loss: 0.08303684741258621\n",
      "Epoch 3563, Loss: 1.7005159556865692, Final Batch Loss: 0.2793195843696594\n",
      "Epoch 3564, Loss: 1.8497208058834076, Final Batch Loss: 0.4258005619049072\n",
      "Epoch 3565, Loss: 1.8216372430324554, Final Batch Loss: 0.36832523345947266\n",
      "Epoch 3566, Loss: 1.6175410449504852, Final Batch Loss: 0.30854132771492004\n",
      "Epoch 3567, Loss: 1.445341281592846, Final Batch Loss: 0.06569310277700424\n",
      "Epoch 3568, Loss: 1.90275239944458, Final Batch Loss: 0.5647692680358887\n",
      "Epoch 3569, Loss: 1.8589064180850983, Final Batch Loss: 0.4781361520290375\n",
      "Epoch 3570, Loss: 1.9254453778266907, Final Batch Loss: 0.44055643677711487\n",
      "Epoch 3571, Loss: 1.9827036559581757, Final Batch Loss: 0.49196919798851013\n",
      "Epoch 3572, Loss: 2.0282448828220367, Final Batch Loss: 0.49852868914604187\n",
      "Epoch 3573, Loss: 1.8427161574363708, Final Batch Loss: 0.4625513553619385\n",
      "Epoch 3574, Loss: 1.9666013717651367, Final Batch Loss: 0.4731912314891815\n",
      "Epoch 3575, Loss: 1.9818547368049622, Final Batch Loss: 0.5757721066474915\n",
      "Epoch 3576, Loss: 1.6260147094726562, Final Batch Loss: 0.2509228587150574\n",
      "Epoch 3577, Loss: 1.6246950030326843, Final Batch Loss: 0.298288494348526\n",
      "Epoch 3578, Loss: 1.7725103497505188, Final Batch Loss: 0.24926796555519104\n",
      "Epoch 3579, Loss: 1.7167274355888367, Final Batch Loss: 0.3376755118370056\n",
      "Epoch 3580, Loss: 1.6485942900180817, Final Batch Loss: 0.26442667841911316\n",
      "Epoch 3581, Loss: 2.0238634049892426, Final Batch Loss: 0.8140696883201599\n",
      "Epoch 3582, Loss: 1.5143383145332336, Final Batch Loss: 0.28288596868515015\n",
      "Epoch 3583, Loss: 1.6457716226577759, Final Batch Loss: 0.2764500677585602\n",
      "Epoch 3584, Loss: 1.967724621295929, Final Batch Loss: 0.47682449221611023\n",
      "Epoch 3585, Loss: 1.525911569595337, Final Batch Loss: 0.2423192262649536\n",
      "Epoch 3586, Loss: 1.6795936226844788, Final Batch Loss: 0.29743966460227966\n",
      "Epoch 3587, Loss: 1.7476156651973724, Final Batch Loss: 0.40614786744117737\n",
      "Epoch 3588, Loss: 1.720759391784668, Final Batch Loss: 0.3717480003833771\n",
      "Epoch 3589, Loss: 2.340740382671356, Final Batch Loss: 0.9379399418830872\n",
      "Epoch 3590, Loss: 1.7419121265411377, Final Batch Loss: 0.3218415379524231\n",
      "Epoch 3591, Loss: 2.1298062205314636, Final Batch Loss: 0.5624341368675232\n",
      "Epoch 3592, Loss: 1.9854984283447266, Final Batch Loss: 0.453704297542572\n",
      "Epoch 3593, Loss: 1.5337309464812279, Final Batch Loss: 0.08867239207029343\n",
      "Epoch 3594, Loss: 1.7564699947834015, Final Batch Loss: 0.36473530530929565\n",
      "Epoch 3595, Loss: 2.0368970930576324, Final Batch Loss: 0.5975446105003357\n",
      "Epoch 3596, Loss: 1.8268083930015564, Final Batch Loss: 0.3803129196166992\n",
      "Epoch 3597, Loss: 1.5168818980455399, Final Batch Loss: 0.15364955365657806\n",
      "Epoch 3598, Loss: 1.9931849241256714, Final Batch Loss: 0.5467863082885742\n",
      "Epoch 3599, Loss: 1.4317188933491707, Final Batch Loss: 0.12451077252626419\n",
      "Epoch 3600, Loss: 1.7574769854545593, Final Batch Loss: 0.40410831570625305\n",
      "Epoch 3601, Loss: 1.992602825164795, Final Batch Loss: 0.49512365460395813\n",
      "Epoch 3602, Loss: 1.625167891383171, Final Batch Loss: 0.23881648480892181\n",
      "Epoch 3603, Loss: 1.7777707874774933, Final Batch Loss: 0.36549505591392517\n",
      "Epoch 3604, Loss: 1.4976987317204475, Final Batch Loss: 0.10695794969797134\n",
      "Epoch 3605, Loss: 1.7254712879657745, Final Batch Loss: 0.325064480304718\n",
      "Epoch 3606, Loss: 2.2761645913124084, Final Batch Loss: 0.9392408132553101\n",
      "Epoch 3607, Loss: 1.7597314715385437, Final Batch Loss: 0.3912934362888336\n",
      "Epoch 3608, Loss: 1.6826216280460358, Final Batch Loss: 0.26063641905784607\n",
      "Epoch 3609, Loss: 1.618247702717781, Final Batch Loss: 0.20850799977779388\n",
      "Epoch 3610, Loss: 1.9858012795448303, Final Batch Loss: 0.544904887676239\n",
      "Epoch 3611, Loss: 1.6545432657003403, Final Batch Loss: 0.2457771748304367\n",
      "Epoch 3612, Loss: 1.6026698648929596, Final Batch Loss: 0.1862945258617401\n",
      "Epoch 3613, Loss: 1.7643111050128937, Final Batch Loss: 0.3300408720970154\n",
      "Epoch 3614, Loss: 1.6629758477210999, Final Batch Loss: 0.2560194134712219\n",
      "Epoch 3615, Loss: 1.6893037557601929, Final Batch Loss: 0.30287712812423706\n",
      "Epoch 3616, Loss: 1.4656258299946785, Final Batch Loss: 0.09272093325853348\n",
      "Epoch 3617, Loss: 1.593570426106453, Final Batch Loss: 0.2285102754831314\n",
      "Epoch 3618, Loss: 1.5454561561346054, Final Batch Loss: 0.1760474294424057\n",
      "Epoch 3619, Loss: 1.3692049346864223, Final Batch Loss: 0.04539170488715172\n",
      "Epoch 3620, Loss: 1.4757359027862549, Final Batch Loss: 0.13778111338615417\n",
      "Epoch 3621, Loss: 1.7872162759304047, Final Batch Loss: 0.3270394504070282\n",
      "Epoch 3622, Loss: 2.006709098815918, Final Batch Loss: 0.642825722694397\n",
      "Epoch 3623, Loss: 1.6457105875015259, Final Batch Loss: 0.24765047430992126\n",
      "Epoch 3624, Loss: 1.8314580023288727, Final Batch Loss: 0.2753210663795471\n",
      "Epoch 3625, Loss: 1.521561786532402, Final Batch Loss: 0.17049209773540497\n",
      "Epoch 3626, Loss: 1.8011002242565155, Final Batch Loss: 0.4464651644229889\n",
      "Epoch 3627, Loss: 1.7589757144451141, Final Batch Loss: 0.4797072410583496\n",
      "Epoch 3628, Loss: 1.8754965662956238, Final Batch Loss: 0.6058182716369629\n",
      "Epoch 3629, Loss: 1.6566018164157867, Final Batch Loss: 0.300775945186615\n",
      "Epoch 3630, Loss: 1.659530371427536, Final Batch Loss: 0.3827270567417145\n",
      "Epoch 3631, Loss: 1.6375183463096619, Final Batch Loss: 0.24989303946495056\n",
      "Epoch 3632, Loss: 2.06435164809227, Final Batch Loss: 0.7502408027648926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3633, Loss: 1.7527889013290405, Final Batch Loss: 0.4050881266593933\n",
      "Epoch 3634, Loss: 1.9148178696632385, Final Batch Loss: 0.37129661440849304\n",
      "Epoch 3635, Loss: 2.3201206028461456, Final Batch Loss: 0.948304295539856\n",
      "Epoch 3636, Loss: 1.6397092044353485, Final Batch Loss: 0.3524138629436493\n",
      "Epoch 3637, Loss: 1.6369935013353825, Final Batch Loss: 0.040164265781641006\n",
      "Epoch 3638, Loss: 1.7414373010396957, Final Batch Loss: 0.1881394237279892\n",
      "Epoch 3639, Loss: 2.19612455368042, Final Batch Loss: 0.6193372011184692\n",
      "Epoch 3640, Loss: 1.5106603801250458, Final Batch Loss: 0.0860406756401062\n",
      "Epoch 3641, Loss: 1.7739792466163635, Final Batch Loss: 0.2586461901664734\n",
      "Epoch 3642, Loss: 1.7410172820091248, Final Batch Loss: 0.3410632312297821\n",
      "Epoch 3643, Loss: 1.7845811545848846, Final Batch Loss: 0.28082576394081116\n",
      "Epoch 3644, Loss: 1.4455076456069946, Final Batch Loss: 0.08509629964828491\n",
      "Epoch 3645, Loss: 1.7213121950626373, Final Batch Loss: 0.2603656053543091\n",
      "Epoch 3646, Loss: 1.483607366681099, Final Batch Loss: 0.16350437700748444\n",
      "Epoch 3647, Loss: 1.6173932999372482, Final Batch Loss: 0.24175818264484406\n",
      "Epoch 3648, Loss: 2.4272001683712006, Final Batch Loss: 0.9055624008178711\n",
      "Epoch 3649, Loss: 1.8926162421703339, Final Batch Loss: 0.5384350419044495\n",
      "Epoch 3650, Loss: 1.5448578000068665, Final Batch Loss: 0.25332000851631165\n",
      "Epoch 3651, Loss: 1.8124927282333374, Final Batch Loss: 0.31023433804512024\n",
      "Epoch 3652, Loss: 1.8436146974563599, Final Batch Loss: 0.38917991518974304\n",
      "Epoch 3653, Loss: 1.6278708279132843, Final Batch Loss: 0.2863031029701233\n",
      "Epoch 3654, Loss: 1.7289681881666183, Final Batch Loss: 0.18107198178768158\n",
      "Epoch 3655, Loss: 1.4961803890764713, Final Batch Loss: 0.05570203438401222\n",
      "Epoch 3656, Loss: 1.767418533563614, Final Batch Loss: 0.304993599653244\n",
      "Epoch 3657, Loss: 1.5194196552038193, Final Batch Loss: 0.07464136183261871\n",
      "Epoch 3658, Loss: 1.8030015230178833, Final Batch Loss: 0.4907618463039398\n",
      "Epoch 3659, Loss: 1.801396518945694, Final Batch Loss: 0.3438580334186554\n",
      "Epoch 3660, Loss: 1.5085868835449219, Final Batch Loss: 0.16286492347717285\n",
      "Epoch 3661, Loss: 1.6633133590221405, Final Batch Loss: 0.3517006039619446\n",
      "Epoch 3662, Loss: 1.5127086862921715, Final Batch Loss: 0.1144457682967186\n",
      "Epoch 3663, Loss: 1.6192200481891632, Final Batch Loss: 0.35222581028938293\n",
      "Epoch 3664, Loss: 2.158531665802002, Final Batch Loss: 0.7675698399543762\n",
      "Epoch 3665, Loss: 2.357880562543869, Final Batch Loss: 0.9268743395805359\n",
      "Epoch 3666, Loss: 1.5323548465967178, Final Batch Loss: 0.16876761615276337\n",
      "Epoch 3667, Loss: 1.7855865359306335, Final Batch Loss: 0.3375730812549591\n",
      "Epoch 3668, Loss: 1.5266151130199432, Final Batch Loss: 0.21371421217918396\n",
      "Epoch 3669, Loss: 1.817751169204712, Final Batch Loss: 0.48143887519836426\n",
      "Epoch 3670, Loss: 2.1596747040748596, Final Batch Loss: 0.7918965220451355\n",
      "Epoch 3671, Loss: 1.4726513400673866, Final Batch Loss: 0.07366032153367996\n",
      "Epoch 3672, Loss: 1.8985322415828705, Final Batch Loss: 0.4668408930301666\n",
      "Epoch 3673, Loss: 1.6850131750106812, Final Batch Loss: 0.2984750270843506\n",
      "Epoch 3674, Loss: 1.6838679611682892, Final Batch Loss: 0.28336426615715027\n",
      "Epoch 3675, Loss: 1.3594271093606949, Final Batch Loss: 0.10878695547580719\n",
      "Epoch 3676, Loss: 1.7393151223659515, Final Batch Loss: 0.43335413932800293\n",
      "Epoch 3677, Loss: 1.9352006614208221, Final Batch Loss: 0.5919751524925232\n",
      "Epoch 3678, Loss: 1.6800461262464523, Final Batch Loss: 0.3311595022678375\n",
      "Epoch 3679, Loss: 1.6396985352039337, Final Batch Loss: 0.23375356197357178\n",
      "Epoch 3680, Loss: 2.207794636487961, Final Batch Loss: 0.7864143252372742\n",
      "Epoch 3681, Loss: 1.8165881633758545, Final Batch Loss: 0.3092844784259796\n",
      "Epoch 3682, Loss: 1.6509467363357544, Final Batch Loss: 0.2651965916156769\n",
      "Epoch 3683, Loss: 1.7323848903179169, Final Batch Loss: 0.31570547819137573\n",
      "Epoch 3684, Loss: 1.728293776512146, Final Batch Loss: 0.3073264956474304\n",
      "Epoch 3685, Loss: 1.6309296190738678, Final Batch Loss: 0.20034921169281006\n",
      "Epoch 3686, Loss: 1.7691557109355927, Final Batch Loss: 0.399921178817749\n",
      "Epoch 3687, Loss: 1.4548259973526, Final Batch Loss: 0.13801753520965576\n",
      "Epoch 3688, Loss: 1.860400229692459, Final Batch Loss: 0.39973750710487366\n",
      "Epoch 3689, Loss: 1.5621671229600906, Final Batch Loss: 0.22755326330661774\n",
      "Epoch 3690, Loss: 1.7508988976478577, Final Batch Loss: 0.4166000485420227\n",
      "Epoch 3691, Loss: 1.5621581748127937, Final Batch Loss: 0.11889690905809402\n",
      "Epoch 3692, Loss: 1.8194874823093414, Final Batch Loss: 0.39858296513557434\n",
      "Epoch 3693, Loss: 1.4784768670797348, Final Batch Loss: 0.15203391015529633\n",
      "Epoch 3694, Loss: 1.6914629638195038, Final Batch Loss: 0.25747209787368774\n",
      "Epoch 3695, Loss: 2.0939139127731323, Final Batch Loss: 0.7183533310890198\n",
      "Epoch 3696, Loss: 1.6364902257919312, Final Batch Loss: 0.3141941428184509\n",
      "Epoch 3697, Loss: 1.5425405651330948, Final Batch Loss: 0.18045230209827423\n",
      "Epoch 3698, Loss: 1.526477724313736, Final Batch Loss: 0.2015637457370758\n",
      "Epoch 3699, Loss: 1.621468722820282, Final Batch Loss: 0.28664085268974304\n",
      "Epoch 3700, Loss: 1.618704915046692, Final Batch Loss: 0.31032678484916687\n",
      "Epoch 3701, Loss: 1.8653667271137238, Final Batch Loss: 0.47830793261528015\n",
      "Epoch 3702, Loss: 1.5262748897075653, Final Batch Loss: 0.1911395788192749\n",
      "Epoch 3703, Loss: 1.7029240429401398, Final Batch Loss: 0.28492361307144165\n",
      "Epoch 3704, Loss: 1.4462466835975647, Final Batch Loss: 0.19658517837524414\n",
      "Epoch 3705, Loss: 2.7069594264030457, Final Batch Loss: 1.413691520690918\n",
      "Epoch 3706, Loss: 1.500009909272194, Final Batch Loss: 0.1280222088098526\n",
      "Epoch 3707, Loss: 1.5708515867590904, Final Batch Loss: 0.10025732964277267\n",
      "Epoch 3708, Loss: 1.6822434961795807, Final Batch Loss: 0.26327353715896606\n",
      "Epoch 3709, Loss: 1.69778111577034, Final Batch Loss: 0.2720634937286377\n",
      "Epoch 3710, Loss: 1.9107163846492767, Final Batch Loss: 0.4433397352695465\n",
      "Epoch 3711, Loss: 1.8297014832496643, Final Batch Loss: 0.3824719488620758\n",
      "Epoch 3712, Loss: 1.6516116559505463, Final Batch Loss: 0.21024248003959656\n",
      "Epoch 3713, Loss: 2.459576517343521, Final Batch Loss: 0.6520940065383911\n",
      "Epoch 3714, Loss: 1.7481865882873535, Final Batch Loss: 0.3369084298610687\n",
      "Epoch 3715, Loss: 1.9127997756004333, Final Batch Loss: 0.5248544216156006\n",
      "Epoch 3716, Loss: 1.6168680936098099, Final Batch Loss: 0.13224519789218903\n",
      "Epoch 3717, Loss: 1.3658259250223637, Final Batch Loss: 0.02181128039956093\n",
      "Epoch 3718, Loss: 1.8649044036865234, Final Batch Loss: 0.4309788644313812\n",
      "Epoch 3719, Loss: 1.6900360584259033, Final Batch Loss: 0.2694948613643646\n",
      "Epoch 3720, Loss: 1.6103188395500183, Final Batch Loss: 0.28801342844963074\n",
      "Epoch 3721, Loss: 1.551059067249298, Final Batch Loss: 0.25237879157066345\n",
      "Epoch 3722, Loss: 1.657357394695282, Final Batch Loss: 0.2548575699329376\n",
      "Epoch 3723, Loss: 1.6201741099357605, Final Batch Loss: 0.2805418074131012\n",
      "Epoch 3724, Loss: 1.6975477039813995, Final Batch Loss: 0.3511391580104828\n",
      "Epoch 3725, Loss: 1.8646129965782166, Final Batch Loss: 0.5247650146484375\n",
      "Epoch 3726, Loss: 2.0788964331150055, Final Batch Loss: 0.7366365790367126\n",
      "Epoch 3727, Loss: 1.4314855188131332, Final Batch Loss: 0.12605823576450348\n",
      "Epoch 3728, Loss: 1.780140370130539, Final Batch Loss: 0.4001971185207367\n",
      "Epoch 3729, Loss: 1.8155148029327393, Final Batch Loss: 0.4331779479980469\n",
      "Epoch 3730, Loss: 1.8857629299163818, Final Batch Loss: 0.37772873044013977\n",
      "Epoch 3731, Loss: 1.7364159524440765, Final Batch Loss: 0.28979015350341797\n",
      "Epoch 3732, Loss: 1.5197257697582245, Final Batch Loss: 0.14516210556030273\n",
      "Epoch 3733, Loss: 1.5482463389635086, Final Batch Loss: 0.16625766456127167\n",
      "Epoch 3734, Loss: 1.7002143263816833, Final Batch Loss: 0.271705687046051\n",
      "Epoch 3735, Loss: 1.384141694754362, Final Batch Loss: 0.0368807427585125\n",
      "Epoch 3736, Loss: 1.5367434322834015, Final Batch Loss: 0.24173638224601746\n",
      "Epoch 3737, Loss: 2.0049622654914856, Final Batch Loss: 0.7283465266227722\n",
      "Epoch 3738, Loss: 1.50831700861454, Final Batch Loss: 0.23065008223056793\n",
      "Epoch 3739, Loss: 1.765896588563919, Final Batch Loss: 0.35151034593582153\n",
      "Epoch 3740, Loss: 1.5073170885443687, Final Batch Loss: 0.11233054846525192\n",
      "Epoch 3741, Loss: 1.7296628654003143, Final Batch Loss: 0.3835635781288147\n",
      "Epoch 3742, Loss: 1.8818693161010742, Final Batch Loss: 0.5575190782546997\n",
      "Epoch 3743, Loss: 2.094096302986145, Final Batch Loss: 0.5681114196777344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3744, Loss: 1.4363694936037064, Final Batch Loss: 0.21154595911502838\n",
      "Epoch 3745, Loss: 1.717183381319046, Final Batch Loss: 0.38539496064186096\n",
      "Epoch 3746, Loss: 1.6821835339069366, Final Batch Loss: 0.38004782795906067\n",
      "Epoch 3747, Loss: 1.960587352514267, Final Batch Loss: 0.6236698031425476\n",
      "Epoch 3748, Loss: 1.8179587721824646, Final Batch Loss: 0.38226228952407837\n",
      "Epoch 3749, Loss: 1.528958447277546, Final Batch Loss: 0.12397242337465286\n",
      "Epoch 3750, Loss: 1.874734103679657, Final Batch Loss: 0.4364781081676483\n",
      "Epoch 3751, Loss: 1.666137009859085, Final Batch Loss: 0.2782686948776245\n",
      "Epoch 3752, Loss: 1.7061849534511566, Final Batch Loss: 0.27047714591026306\n",
      "Epoch 3753, Loss: 1.565783068537712, Final Batch Loss: 0.23176248371601105\n",
      "Epoch 3754, Loss: 1.6606099605560303, Final Batch Loss: 0.4071520268917084\n",
      "Epoch 3755, Loss: 1.5917856395244598, Final Batch Loss: 0.14830851554870605\n",
      "Epoch 3756, Loss: 1.6463274359703064, Final Batch Loss: 0.3251475691795349\n",
      "Epoch 3757, Loss: 1.4846084266901016, Final Batch Loss: 0.1789664477109909\n",
      "Epoch 3758, Loss: 1.6998552083969116, Final Batch Loss: 0.31250882148742676\n",
      "Epoch 3759, Loss: 1.5649537444114685, Final Batch Loss: 0.3118918836116791\n",
      "Epoch 3760, Loss: 1.6452976167201996, Final Batch Loss: 0.313090056180954\n",
      "Epoch 3761, Loss: 1.614465057849884, Final Batch Loss: 0.36914944648742676\n",
      "Epoch 3762, Loss: 1.8722215592861176, Final Batch Loss: 0.44122621417045593\n",
      "Epoch 3763, Loss: 1.6343286335468292, Final Batch Loss: 0.3210087716579437\n",
      "Epoch 3764, Loss: 1.7589406669139862, Final Batch Loss: 0.3833273947238922\n",
      "Epoch 3765, Loss: 1.7657647728919983, Final Batch Loss: 0.40001869201660156\n",
      "Epoch 3766, Loss: 1.5322548598051071, Final Batch Loss: 0.2060387283563614\n",
      "Epoch 3767, Loss: 1.9081452190876007, Final Batch Loss: 0.5467877984046936\n",
      "Epoch 3768, Loss: 1.6229711174964905, Final Batch Loss: 0.25867778062820435\n",
      "Epoch 3769, Loss: 1.7862136662006378, Final Batch Loss: 0.396530419588089\n",
      "Epoch 3770, Loss: 1.7338672578334808, Final Batch Loss: 0.3244636356830597\n",
      "Epoch 3771, Loss: 1.6197577714920044, Final Batch Loss: 0.32066649198532104\n",
      "Epoch 3772, Loss: 1.6409143805503845, Final Batch Loss: 0.25336533784866333\n",
      "Epoch 3773, Loss: 1.9127808213233948, Final Batch Loss: 0.495222806930542\n",
      "Epoch 3774, Loss: 1.7339229583740234, Final Batch Loss: 0.3573911190032959\n",
      "Epoch 3775, Loss: 2.1343943774700165, Final Batch Loss: 0.7458305358886719\n",
      "Epoch 3776, Loss: 1.5112930089235306, Final Batch Loss: 0.19057224690914154\n",
      "Epoch 3777, Loss: 1.9803338646888733, Final Batch Loss: 0.5465693473815918\n",
      "Epoch 3778, Loss: 1.6227617114782333, Final Batch Loss: 0.22315017879009247\n",
      "Epoch 3779, Loss: 1.508802369236946, Final Batch Loss: 0.14180053770542145\n",
      "Epoch 3780, Loss: 1.8168182075023651, Final Batch Loss: 0.37309813499450684\n",
      "Epoch 3781, Loss: 1.5396515727043152, Final Batch Loss: 0.22391626238822937\n",
      "Epoch 3782, Loss: 1.5537298917770386, Final Batch Loss: 0.16214123368263245\n",
      "Epoch 3783, Loss: 1.3799468949437141, Final Batch Loss: 0.0713961198925972\n",
      "Epoch 3784, Loss: 1.67957703769207, Final Batch Loss: 0.21028123795986176\n",
      "Epoch 3785, Loss: 1.8881110548973083, Final Batch Loss: 0.4630832374095917\n",
      "Epoch 3786, Loss: 1.6391765773296356, Final Batch Loss: 0.3066013753414154\n",
      "Epoch 3787, Loss: 1.7296897172927856, Final Batch Loss: 0.30590352416038513\n",
      "Epoch 3788, Loss: 1.5153331458568573, Final Batch Loss: 0.05790606141090393\n",
      "Epoch 3789, Loss: 1.6520468592643738, Final Batch Loss: 0.2029304802417755\n",
      "Epoch 3790, Loss: 1.8192228376865387, Final Batch Loss: 0.2914981544017792\n",
      "Epoch 3791, Loss: 1.5527736246585846, Final Batch Loss: 0.1604628562927246\n",
      "Epoch 3792, Loss: 1.715549260377884, Final Batch Loss: 0.39074039459228516\n",
      "Epoch 3793, Loss: 1.9590719044208527, Final Batch Loss: 0.5716103911399841\n",
      "Epoch 3794, Loss: 1.737782895565033, Final Batch Loss: 0.28899380564689636\n",
      "Epoch 3795, Loss: 1.545656606554985, Final Batch Loss: 0.17760293185710907\n",
      "Epoch 3796, Loss: 1.819937914609909, Final Batch Loss: 0.4391782581806183\n",
      "Epoch 3797, Loss: 1.7680082023143768, Final Batch Loss: 0.4464241862297058\n",
      "Epoch 3798, Loss: 1.5119805335998535, Final Batch Loss: 0.20850345492362976\n",
      "Epoch 3799, Loss: 1.5586589723825455, Final Batch Loss: 0.15254546701908112\n",
      "Epoch 3800, Loss: 1.649944856762886, Final Batch Loss: 0.3377287685871124\n",
      "Epoch 3801, Loss: 2.031986951828003, Final Batch Loss: 0.5669079422950745\n",
      "Epoch 3802, Loss: 1.5591332763433456, Final Batch Loss: 0.2436378449201584\n",
      "Epoch 3803, Loss: 1.5752856954932213, Final Batch Loss: 0.10298340767621994\n",
      "Epoch 3804, Loss: 1.5708146691322327, Final Batch Loss: 0.24893534183502197\n",
      "Epoch 3805, Loss: 1.555763304233551, Final Batch Loss: 0.24358618259429932\n",
      "Epoch 3806, Loss: 1.5403333902359009, Final Batch Loss: 0.3166733384132385\n",
      "Epoch 3807, Loss: 1.6088314801454544, Final Batch Loss: 0.2283799797296524\n",
      "Epoch 3808, Loss: 1.8339498341083527, Final Batch Loss: 0.559337317943573\n",
      "Epoch 3809, Loss: 1.5615509748458862, Final Batch Loss: 0.2801246643066406\n",
      "Epoch 3810, Loss: 1.4459071615710855, Final Batch Loss: 0.014246837235987186\n",
      "Epoch 3811, Loss: 1.6517701148986816, Final Batch Loss: 0.36787500977516174\n",
      "Epoch 3812, Loss: 1.7396142780780792, Final Batch Loss: 0.2736711800098419\n",
      "Epoch 3813, Loss: 1.7381073236465454, Final Batch Loss: 0.5090646147727966\n",
      "Epoch 3814, Loss: 1.5929515361785889, Final Batch Loss: 0.2050156593322754\n",
      "Epoch 3815, Loss: 2.0736181139945984, Final Batch Loss: 0.7919314503669739\n",
      "Epoch 3816, Loss: 1.337126113474369, Final Batch Loss: 0.09909460693597794\n",
      "Epoch 3817, Loss: 1.7686451971530914, Final Batch Loss: 0.43746641278266907\n",
      "Epoch 3818, Loss: 1.4262267611920834, Final Batch Loss: 0.027667518705129623\n",
      "Epoch 3819, Loss: 1.7028437703847885, Final Batch Loss: 0.13543011248111725\n",
      "Epoch 3820, Loss: 1.7828373610973358, Final Batch Loss: 0.26754361391067505\n",
      "Epoch 3821, Loss: 1.572981208562851, Final Batch Loss: 0.28251320123672485\n",
      "Epoch 3822, Loss: 1.7381929755210876, Final Batch Loss: 0.49306073784828186\n",
      "Epoch 3823, Loss: 1.4827775210142136, Final Batch Loss: 0.213261678814888\n",
      "Epoch 3824, Loss: 1.6765608489513397, Final Batch Loss: 0.342986136674881\n",
      "Epoch 3825, Loss: 1.76503986120224, Final Batch Loss: 0.44309738278388977\n",
      "Epoch 3826, Loss: 1.7264090478420258, Final Batch Loss: 0.3532859981060028\n",
      "Epoch 3827, Loss: 1.5425844937562943, Final Batch Loss: 0.18487225472927094\n",
      "Epoch 3828, Loss: 2.0059207677841187, Final Batch Loss: 0.623263418674469\n",
      "Epoch 3829, Loss: 2.1335399448871613, Final Batch Loss: 0.7397088408470154\n",
      "Epoch 3830, Loss: 1.6602833569049835, Final Batch Loss: 0.27536579966545105\n",
      "Epoch 3831, Loss: 1.7150010615587234, Final Batch Loss: 0.19212548434734344\n",
      "Epoch 3832, Loss: 2.031836986541748, Final Batch Loss: 0.5527820587158203\n",
      "Epoch 3833, Loss: 1.7380293309688568, Final Batch Loss: 0.3244365155696869\n",
      "Epoch 3834, Loss: 1.7140691131353378, Final Batch Loss: 0.16921885311603546\n",
      "Epoch 3835, Loss: 2.0073262155056, Final Batch Loss: 0.4637877643108368\n",
      "Epoch 3836, Loss: 1.8118973672389984, Final Batch Loss: 0.2501530051231384\n",
      "Epoch 3837, Loss: 1.6527342200279236, Final Batch Loss: 0.1658376157283783\n",
      "Epoch 3838, Loss: 1.773981899023056, Final Batch Loss: 0.30314892530441284\n",
      "Epoch 3839, Loss: 1.8970028758049011, Final Batch Loss: 0.5655571222305298\n",
      "Epoch 3840, Loss: 1.974433720111847, Final Batch Loss: 0.4793829321861267\n",
      "Epoch 3841, Loss: 1.676968440413475, Final Batch Loss: 0.191365048289299\n",
      "Epoch 3842, Loss: 1.4069849867373705, Final Batch Loss: 0.02421829290688038\n",
      "Epoch 3843, Loss: 1.687110185623169, Final Batch Loss: 0.40129905939102173\n",
      "Epoch 3844, Loss: 1.552748665213585, Final Batch Loss: 0.1250583380460739\n",
      "Epoch 3845, Loss: 1.575134515762329, Final Batch Loss: 0.28854045271873474\n",
      "Epoch 3846, Loss: 1.6612926423549652, Final Batch Loss: 0.4200880527496338\n",
      "Epoch 3847, Loss: 1.7528344094753265, Final Batch Loss: 0.4383116364479065\n",
      "Epoch 3848, Loss: 1.5455841273069382, Final Batch Loss: 0.1650514453649521\n",
      "Epoch 3849, Loss: 1.6145193427801132, Final Batch Loss: 0.20095346868038177\n",
      "Epoch 3850, Loss: 1.8613952994346619, Final Batch Loss: 0.45705562829971313\n",
      "Epoch 3851, Loss: 1.4911661893129349, Final Batch Loss: 0.17883865535259247\n",
      "Epoch 3852, Loss: 2.09813791513443, Final Batch Loss: 0.6477807760238647\n",
      "Epoch 3853, Loss: 1.3895058371126652, Final Batch Loss: 0.04565686360001564\n",
      "Epoch 3854, Loss: 1.4908242374658585, Final Batch Loss: 0.11594708263874054\n",
      "Epoch 3855, Loss: 1.7276934683322906, Final Batch Loss: 0.3589341938495636\n",
      "Epoch 3856, Loss: 1.6984555572271347, Final Batch Loss: 0.16785825788974762\n",
      "Epoch 3857, Loss: 1.6257330104708672, Final Batch Loss: 0.10055705159902573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3858, Loss: 1.5015320628881454, Final Batch Loss: 0.22277326881885529\n",
      "Epoch 3859, Loss: 1.4930284172296524, Final Batch Loss: 0.21899618208408356\n",
      "Epoch 3860, Loss: 1.5767284333705902, Final Batch Loss: 0.20726454257965088\n",
      "Epoch 3861, Loss: 1.8905069828033447, Final Batch Loss: 0.6252564787864685\n",
      "Epoch 3862, Loss: 1.5859998762607574, Final Batch Loss: 0.3324475884437561\n",
      "Epoch 3863, Loss: 1.5251133441925049, Final Batch Loss: 0.21928462386131287\n",
      "Epoch 3864, Loss: 1.7591766715049744, Final Batch Loss: 0.42466649413108826\n",
      "Epoch 3865, Loss: 1.5298977345228195, Final Batch Loss: 0.20096419751644135\n",
      "Epoch 3866, Loss: 2.060969591140747, Final Batch Loss: 0.7155415415763855\n",
      "Epoch 3867, Loss: 1.5220509767532349, Final Batch Loss: 0.1749386489391327\n",
      "Epoch 3868, Loss: 1.8312151432037354, Final Batch Loss: 0.34873777627944946\n",
      "Epoch 3869, Loss: 1.785616785287857, Final Batch Loss: 0.4034174084663391\n",
      "Epoch 3870, Loss: 1.5612440556287766, Final Batch Loss: 0.22070665657520294\n",
      "Epoch 3871, Loss: 1.7385629713535309, Final Batch Loss: 0.4395887553691864\n",
      "Epoch 3872, Loss: 1.5813322961330414, Final Batch Loss: 0.2938399910926819\n",
      "Epoch 3873, Loss: 1.8175381422042847, Final Batch Loss: 0.5061031579971313\n",
      "Epoch 3874, Loss: 1.8116744458675385, Final Batch Loss: 0.46886977553367615\n",
      "Epoch 3875, Loss: 1.5083998143672943, Final Batch Loss: 0.13277161121368408\n",
      "Epoch 3876, Loss: 1.7194835096597672, Final Batch Loss: 0.22659872472286224\n",
      "Epoch 3877, Loss: 1.811911016702652, Final Batch Loss: 0.36995914578437805\n",
      "Epoch 3878, Loss: 1.747693121433258, Final Batch Loss: 0.3018685281276703\n",
      "Epoch 3879, Loss: 1.5022055357694626, Final Batch Loss: 0.12278436124324799\n",
      "Epoch 3880, Loss: 1.840055912733078, Final Batch Loss: 0.39226245880126953\n",
      "Epoch 3881, Loss: 1.9229816794395447, Final Batch Loss: 0.4715093672275543\n",
      "Epoch 3882, Loss: 1.5675952434539795, Final Batch Loss: 0.26215142011642456\n",
      "Epoch 3883, Loss: 1.826930671930313, Final Batch Loss: 0.4267761707305908\n",
      "Epoch 3884, Loss: 2.0538529455661774, Final Batch Loss: 0.6270996332168579\n",
      "Epoch 3885, Loss: 1.4932509511709213, Final Batch Loss: 0.15076936781406403\n",
      "Epoch 3886, Loss: 1.5689803808927536, Final Batch Loss: 0.19349320232868195\n",
      "Epoch 3887, Loss: 1.8250710368156433, Final Batch Loss: 0.5354390740394592\n",
      "Epoch 3888, Loss: 1.9103883802890778, Final Batch Loss: 0.36627838015556335\n",
      "Epoch 3889, Loss: 1.5784739255905151, Final Batch Loss: 0.21995556354522705\n",
      "Epoch 3890, Loss: 1.6084339767694473, Final Batch Loss: 0.16071699559688568\n",
      "Epoch 3891, Loss: 1.8213990032672882, Final Batch Loss: 0.3090990483760834\n",
      "Epoch 3892, Loss: 1.7748174965381622, Final Batch Loss: 0.4961448013782501\n",
      "Epoch 3893, Loss: 1.9297116994857788, Final Batch Loss: 0.4450697898864746\n",
      "Epoch 3894, Loss: 1.8729072511196136, Final Batch Loss: 0.3959943950176239\n",
      "Epoch 3895, Loss: 2.0931682884693146, Final Batch Loss: 0.7700563669204712\n",
      "Epoch 3896, Loss: 1.83965003490448, Final Batch Loss: 0.4589051306247711\n",
      "Epoch 3897, Loss: 1.731133133172989, Final Batch Loss: 0.31770139932632446\n",
      "Epoch 3898, Loss: 1.9494155645370483, Final Batch Loss: 0.44846153259277344\n",
      "Epoch 3899, Loss: 1.9363785088062286, Final Batch Loss: 0.549716055393219\n",
      "Epoch 3900, Loss: 1.8546833992004395, Final Batch Loss: 0.28943583369255066\n",
      "Epoch 3901, Loss: 2.1105403900146484, Final Batch Loss: 0.3659890592098236\n",
      "Epoch 3902, Loss: 1.5548973232507706, Final Batch Loss: 0.16966037452220917\n",
      "Epoch 3903, Loss: 1.4202437549829483, Final Batch Loss: 0.11703388392925262\n",
      "Epoch 3904, Loss: 2.1674346029758453, Final Batch Loss: 0.7541027665138245\n",
      "Epoch 3905, Loss: 1.7844607532024384, Final Batch Loss: 0.4092453122138977\n",
      "Epoch 3906, Loss: 1.844510167837143, Final Batch Loss: 0.3586311340332031\n",
      "Epoch 3907, Loss: 1.6149472445249557, Final Batch Loss: 0.24822939932346344\n",
      "Epoch 3908, Loss: 2.0313474535942078, Final Batch Loss: 0.4967653751373291\n",
      "Epoch 3909, Loss: 1.6407128125429153, Final Batch Loss: 0.16030000150203705\n",
      "Epoch 3910, Loss: 2.166734755039215, Final Batch Loss: 0.7390977144241333\n",
      "Epoch 3911, Loss: 1.768429934978485, Final Batch Loss: 0.3243418335914612\n",
      "Epoch 3912, Loss: 1.9192110002040863, Final Batch Loss: 0.519015908241272\n",
      "Epoch 3913, Loss: 1.6609040200710297, Final Batch Loss: 0.3762446343898773\n",
      "Epoch 3914, Loss: 2.403726279735565, Final Batch Loss: 0.8815003633499146\n",
      "Epoch 3915, Loss: 1.7957504391670227, Final Batch Loss: 0.2773417830467224\n",
      "Epoch 3916, Loss: 1.5649167746305466, Final Batch Loss: 0.19945360720157623\n",
      "Epoch 3917, Loss: 1.7887604534626007, Final Batch Loss: 0.4185781478881836\n",
      "Epoch 3918, Loss: 1.5370789617300034, Final Batch Loss: 0.20753006637096405\n",
      "Epoch 3919, Loss: 2.354631632566452, Final Batch Loss: 0.9392207860946655\n",
      "Epoch 3920, Loss: 1.5285860300064087, Final Batch Loss: 0.19537308812141418\n",
      "Epoch 3921, Loss: 2.038868397474289, Final Batch Loss: 0.565004289150238\n",
      "Epoch 3922, Loss: 1.9030137956142426, Final Batch Loss: 0.4560561776161194\n",
      "Epoch 3923, Loss: 2.0528593957424164, Final Batch Loss: 0.5607748627662659\n",
      "Epoch 3924, Loss: 1.8904171288013458, Final Batch Loss: 0.37125682830810547\n",
      "Epoch 3925, Loss: 1.8324397504329681, Final Batch Loss: 0.19501793384552002\n",
      "Epoch 3926, Loss: 1.674555093050003, Final Batch Loss: 0.2727702558040619\n",
      "Epoch 3927, Loss: 1.752170592546463, Final Batch Loss: 0.3491949439048767\n",
      "Epoch 3928, Loss: 1.686795711517334, Final Batch Loss: 0.37746334075927734\n",
      "Epoch 3929, Loss: 1.7657221257686615, Final Batch Loss: 0.31576162576675415\n",
      "Epoch 3930, Loss: 1.4051701202988625, Final Batch Loss: 0.07830650359392166\n",
      "Epoch 3931, Loss: 1.6403712332248688, Final Batch Loss: 0.2841601073741913\n",
      "Epoch 3932, Loss: 1.7460957467556, Final Batch Loss: 0.2778957784175873\n",
      "Epoch 3933, Loss: 1.7525137960910797, Final Batch Loss: 0.45948949456214905\n",
      "Epoch 3934, Loss: 1.771536648273468, Final Batch Loss: 0.41887399554252625\n",
      "Epoch 3935, Loss: 2.0939970314502716, Final Batch Loss: 0.7076244950294495\n",
      "Epoch 3936, Loss: 1.8040663301944733, Final Batch Loss: 0.39496323466300964\n",
      "Epoch 3937, Loss: 1.8183718621730804, Final Batch Loss: 0.30991777777671814\n",
      "Epoch 3938, Loss: 1.5303164571523666, Final Batch Loss: 0.1638088971376419\n",
      "Epoch 3939, Loss: 1.7900749742984772, Final Batch Loss: 0.446725994348526\n",
      "Epoch 3940, Loss: 1.6819994151592255, Final Batch Loss: 0.2992342412471771\n",
      "Epoch 3941, Loss: 1.7740136682987213, Final Batch Loss: 0.4154479503631592\n",
      "Epoch 3942, Loss: 1.540903925895691, Final Batch Loss: 0.3000166714191437\n",
      "Epoch 3943, Loss: 1.731910228729248, Final Batch Loss: 0.1792553961277008\n",
      "Epoch 3944, Loss: 1.5958338975906372, Final Batch Loss: 0.1630328893661499\n",
      "Epoch 3945, Loss: 1.6393756866455078, Final Batch Loss: 0.296158105134964\n",
      "Epoch 3946, Loss: 1.8001325130462646, Final Batch Loss: 0.5633135437965393\n",
      "Epoch 3947, Loss: 1.7283684611320496, Final Batch Loss: 0.31995782256126404\n",
      "Epoch 3948, Loss: 1.6159660816192627, Final Batch Loss: 0.27195245027542114\n",
      "Epoch 3949, Loss: 1.8491139113903046, Final Batch Loss: 0.4646945893764496\n",
      "Epoch 3950, Loss: 1.5804643034934998, Final Batch Loss: 0.3043482005596161\n",
      "Epoch 3951, Loss: 1.7882456481456757, Final Batch Loss: 0.3551127016544342\n",
      "Epoch 3952, Loss: 1.7379763424396515, Final Batch Loss: 0.33673295378685\n",
      "Epoch 3953, Loss: 1.8096119463443756, Final Batch Loss: 0.337696373462677\n",
      "Epoch 3954, Loss: 1.7133960276842117, Final Batch Loss: 0.24990145862102509\n",
      "Epoch 3955, Loss: 1.4660881832242012, Final Batch Loss: 0.12242240458726883\n",
      "Epoch 3956, Loss: 1.68446284532547, Final Batch Loss: 0.3240816295146942\n",
      "Epoch 3957, Loss: 1.7778036892414093, Final Batch Loss: 0.3129694163799286\n",
      "Epoch 3958, Loss: 1.7762930691242218, Final Batch Loss: 0.4322998523712158\n",
      "Epoch 3959, Loss: 1.5895941704511642, Final Batch Loss: 0.2360755056142807\n",
      "Epoch 3960, Loss: 1.3104980415664613, Final Batch Loss: 0.004917812999337912\n",
      "Epoch 3961, Loss: 1.5918148756027222, Final Batch Loss: 0.27625927329063416\n",
      "Epoch 3962, Loss: 1.8264142870903015, Final Batch Loss: 0.516072154045105\n",
      "Epoch 3963, Loss: 1.529646947979927, Final Batch Loss: 0.23910294473171234\n",
      "Epoch 3964, Loss: 1.5283153057098389, Final Batch Loss: 0.26812678575515747\n",
      "Epoch 3965, Loss: 1.5214623659849167, Final Batch Loss: 0.21372126042842865\n",
      "Epoch 3966, Loss: 1.6444967985153198, Final Batch Loss: 0.33985739946365356\n",
      "Epoch 3967, Loss: 1.779836505651474, Final Batch Loss: 0.339659720659256\n",
      "Epoch 3968, Loss: 1.8976597487926483, Final Batch Loss: 0.5346325635910034\n",
      "Epoch 3969, Loss: 1.9979770183563232, Final Batch Loss: 0.6933749318122864\n",
      "Epoch 3970, Loss: 1.6040587425231934, Final Batch Loss: 0.2838728129863739\n",
      "Epoch 3971, Loss: 1.732180267572403, Final Batch Loss: 0.3704221248626709\n",
      "Epoch 3972, Loss: 1.5599177479743958, Final Batch Loss: 0.2980198860168457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3973, Loss: 1.6574046313762665, Final Batch Loss: 0.33096012473106384\n",
      "Epoch 3974, Loss: 1.5846691131591797, Final Batch Loss: 0.28346604108810425\n",
      "Epoch 3975, Loss: 1.514073833823204, Final Batch Loss: 0.15289051830768585\n",
      "Epoch 3976, Loss: 1.609542340040207, Final Batch Loss: 0.350526362657547\n",
      "Epoch 3977, Loss: 1.953877717256546, Final Batch Loss: 0.6250749826431274\n",
      "Epoch 3978, Loss: 2.225178509950638, Final Batch Loss: 0.8146029114723206\n",
      "Epoch 3979, Loss: 1.636960282921791, Final Batch Loss: 0.23862190544605255\n",
      "Epoch 3980, Loss: 1.7679249793291092, Final Batch Loss: 0.23112885653972626\n",
      "Epoch 3981, Loss: 1.9601770043373108, Final Batch Loss: 0.48511403799057007\n",
      "Epoch 3982, Loss: 1.7260498702526093, Final Batch Loss: 0.4421728253364563\n",
      "Epoch 3983, Loss: 1.5501283258199692, Final Batch Loss: 0.16529031097888947\n",
      "Epoch 3984, Loss: 1.6832843124866486, Final Batch Loss: 0.4126870930194855\n",
      "Epoch 3985, Loss: 1.9652334451675415, Final Batch Loss: 0.5490394830703735\n",
      "Epoch 3986, Loss: 1.6527861058712006, Final Batch Loss: 0.30095288157463074\n",
      "Epoch 3987, Loss: 1.722431093454361, Final Batch Loss: 0.12789225578308105\n",
      "Epoch 3988, Loss: 1.7189525067806244, Final Batch Loss: 0.28212663531303406\n",
      "Epoch 3989, Loss: 1.727531760931015, Final Batch Loss: 0.2496943473815918\n",
      "Epoch 3990, Loss: 2.0299975872039795, Final Batch Loss: 0.6734499335289001\n",
      "Epoch 3991, Loss: 1.7143840342760086, Final Batch Loss: 0.222841277718544\n",
      "Epoch 3992, Loss: 1.8835354447364807, Final Batch Loss: 0.33315593004226685\n",
      "Epoch 3993, Loss: 2.0136837661266327, Final Batch Loss: 0.6555215716362\n",
      "Epoch 3994, Loss: 1.8244830667972565, Final Batch Loss: 0.29021820425987244\n",
      "Epoch 3995, Loss: 1.5004155058413744, Final Batch Loss: 0.016076983883976936\n",
      "Epoch 3996, Loss: 1.6671117693185806, Final Batch Loss: 0.18511997163295746\n",
      "Epoch 3997, Loss: 1.6491034775972366, Final Batch Loss: 0.16077636182308197\n",
      "Epoch 3998, Loss: 1.9179846942424774, Final Batch Loss: 0.558604896068573\n",
      "Epoch 3999, Loss: 1.8045167922973633, Final Batch Loss: 0.35386696457862854\n",
      "Epoch 4000, Loss: 2.1900971233844757, Final Batch Loss: 0.5934414863586426\n",
      "Epoch 4001, Loss: 1.73661670088768, Final Batch Loss: 0.3139728605747223\n",
      "Epoch 4002, Loss: 1.793927788734436, Final Batch Loss: 0.27841708064079285\n",
      "Epoch 4003, Loss: 2.126282215118408, Final Batch Loss: 0.636997401714325\n",
      "Epoch 4004, Loss: 2.0755799412727356, Final Batch Loss: 0.5877967476844788\n",
      "Epoch 4005, Loss: 1.4103496745228767, Final Batch Loss: 0.10978392511606216\n",
      "Epoch 4006, Loss: 1.5190751403570175, Final Batch Loss: 0.23773039877414703\n",
      "Epoch 4007, Loss: 1.8600225448608398, Final Batch Loss: 0.5808972120285034\n",
      "Epoch 4008, Loss: 1.639905869960785, Final Batch Loss: 0.2738536298274994\n",
      "Epoch 4009, Loss: 1.7364096343517303, Final Batch Loss: 0.37687692046165466\n",
      "Epoch 4010, Loss: 2.078562915325165, Final Batch Loss: 0.8288608193397522\n",
      "Epoch 4011, Loss: 2.033513456583023, Final Batch Loss: 0.5314275026321411\n",
      "Epoch 4012, Loss: 1.835174173116684, Final Batch Loss: 0.4486989378929138\n",
      "Epoch 4013, Loss: 1.8801518976688385, Final Batch Loss: 0.2580195963382721\n",
      "Epoch 4014, Loss: 1.4486645385622978, Final Batch Loss: 0.048109881579875946\n",
      "Epoch 4015, Loss: 2.028055638074875, Final Batch Loss: 0.6560587286949158\n",
      "Epoch 4016, Loss: 1.3857850097119808, Final Batch Loss: 0.03450920805335045\n",
      "Epoch 4017, Loss: 1.5852736234664917, Final Batch Loss: 0.27606940269470215\n",
      "Epoch 4018, Loss: 1.8854247331619263, Final Batch Loss: 0.49450257420539856\n",
      "Epoch 4019, Loss: 1.6135867834091187, Final Batch Loss: 0.38655146956443787\n",
      "Epoch 4020, Loss: 1.695170283317566, Final Batch Loss: 0.32055845856666565\n",
      "Epoch 4021, Loss: 1.5031611025333405, Final Batch Loss: 0.2332824468612671\n",
      "Epoch 4022, Loss: 1.705299198627472, Final Batch Loss: 0.37885910272598267\n",
      "Epoch 4023, Loss: 1.542654387652874, Final Batch Loss: 0.1005183681845665\n",
      "Epoch 4024, Loss: 2.1727405786514282, Final Batch Loss: 0.5114070177078247\n",
      "Epoch 4025, Loss: 1.7563146948814392, Final Batch Loss: 0.2932378947734833\n",
      "Epoch 4026, Loss: 1.7191464453935623, Final Batch Loss: 0.24109946191310883\n",
      "Epoch 4027, Loss: 1.8948764204978943, Final Batch Loss: 0.5230360627174377\n",
      "Epoch 4028, Loss: 1.7065217196941376, Final Batch Loss: 0.3562938868999481\n",
      "Epoch 4029, Loss: 1.6829809695482254, Final Batch Loss: 0.24410350620746613\n",
      "Epoch 4030, Loss: 1.9595609605312347, Final Batch Loss: 0.5435909032821655\n",
      "Epoch 4031, Loss: 1.738014280796051, Final Batch Loss: 0.40745025873184204\n",
      "Epoch 4032, Loss: 1.6890284717082977, Final Batch Loss: 0.40255388617515564\n",
      "Epoch 4033, Loss: 1.7798829674720764, Final Batch Loss: 0.4684949219226837\n",
      "Epoch 4034, Loss: 1.4066938981413841, Final Batch Loss: 0.11254670470952988\n",
      "Epoch 4035, Loss: 1.5839274674654007, Final Batch Loss: 0.2146858423948288\n",
      "Epoch 4036, Loss: 1.7992675006389618, Final Batch Loss: 0.3570440709590912\n",
      "Epoch 4037, Loss: 1.562379240989685, Final Batch Loss: 0.24917030334472656\n",
      "Epoch 4038, Loss: 1.708561509847641, Final Batch Loss: 0.41692766547203064\n",
      "Epoch 4039, Loss: 1.347600370645523, Final Batch Loss: 0.12577897310256958\n",
      "Epoch 4040, Loss: 1.6597593426704407, Final Batch Loss: 0.3275071978569031\n",
      "Epoch 4041, Loss: 1.7159729599952698, Final Batch Loss: 0.42981311678886414\n",
      "Epoch 4042, Loss: 1.708185076713562, Final Batch Loss: 0.5717657804489136\n",
      "Epoch 4043, Loss: 1.8426019549369812, Final Batch Loss: 0.5114680528640747\n",
      "Epoch 4044, Loss: 1.6079285144805908, Final Batch Loss: 0.3883281648159027\n",
      "Epoch 4045, Loss: 1.5579763650894165, Final Batch Loss: 0.2504989802837372\n",
      "Epoch 4046, Loss: 1.5479873456060886, Final Batch Loss: 0.055814120918512344\n",
      "Epoch 4047, Loss: 1.6599304676055908, Final Batch Loss: 0.27583885192871094\n",
      "Epoch 4048, Loss: 1.8820452392101288, Final Batch Loss: 0.48284611105918884\n",
      "Epoch 4049, Loss: 1.6657173037528992, Final Batch Loss: 0.2920833230018616\n",
      "Epoch 4050, Loss: 1.402634084224701, Final Batch Loss: 0.16266930103302002\n",
      "Epoch 4051, Loss: 1.570336103439331, Final Batch Loss: 0.32998040318489075\n",
      "Epoch 4052, Loss: 1.871688187122345, Final Batch Loss: 0.519777238368988\n",
      "Epoch 4053, Loss: 1.9422414302825928, Final Batch Loss: 0.5807825326919556\n",
      "Epoch 4054, Loss: 1.7889409959316254, Final Batch Loss: 0.43889355659484863\n",
      "Epoch 4055, Loss: 1.507822424173355, Final Batch Loss: 0.14315858483314514\n",
      "Epoch 4056, Loss: 1.7612272202968597, Final Batch Loss: 0.34839460253715515\n",
      "Epoch 4057, Loss: 1.7241489589214325, Final Batch Loss: 0.29895028471946716\n",
      "Epoch 4058, Loss: 1.650488719344139, Final Batch Loss: 0.15199695527553558\n",
      "Epoch 4059, Loss: 1.8137043118476868, Final Batch Loss: 0.2875119149684906\n",
      "Epoch 4060, Loss: 1.6830693185329437, Final Batch Loss: 0.3070816397666931\n",
      "Epoch 4061, Loss: 1.6410518884658813, Final Batch Loss: 0.2556965947151184\n",
      "Epoch 4062, Loss: 1.5005712360143661, Final Batch Loss: 0.1501050740480423\n",
      "Epoch 4063, Loss: 1.7943266332149506, Final Batch Loss: 0.44123926758766174\n",
      "Epoch 4064, Loss: 1.6912749409675598, Final Batch Loss: 0.3168535530567169\n",
      "Epoch 4065, Loss: 1.5542200356721878, Final Batch Loss: 0.207449808716774\n",
      "Epoch 4066, Loss: 1.624865561723709, Final Batch Loss: 0.25175920128822327\n",
      "Epoch 4067, Loss: 1.5122464150190353, Final Batch Loss: 0.21729223430156708\n",
      "Epoch 4068, Loss: 1.3174430876970291, Final Batch Loss: 0.0886949747800827\n",
      "Epoch 4069, Loss: 1.5271433591842651, Final Batch Loss: 0.23058345913887024\n",
      "Epoch 4070, Loss: 1.5807896256446838, Final Batch Loss: 0.30817270278930664\n",
      "Epoch 4071, Loss: 1.8200177252292633, Final Batch Loss: 0.5758501291275024\n",
      "Epoch 4072, Loss: 1.526169314980507, Final Batch Loss: 0.1869369000196457\n",
      "Epoch 4073, Loss: 1.6968439817428589, Final Batch Loss: 0.25589001178741455\n",
      "Epoch 4074, Loss: 1.9702728390693665, Final Batch Loss: 0.597100555896759\n",
      "Epoch 4075, Loss: 1.7994016110897064, Final Batch Loss: 0.3937400281429291\n",
      "Epoch 4076, Loss: 1.9721920192241669, Final Batch Loss: 0.37286505103111267\n",
      "Epoch 4077, Loss: 1.8734988272190094, Final Batch Loss: 0.4648001790046692\n",
      "Epoch 4078, Loss: 1.9436256885528564, Final Batch Loss: 0.5004345774650574\n",
      "Epoch 4079, Loss: 1.9660882949829102, Final Batch Loss: 0.560536801815033\n",
      "Epoch 4080, Loss: 2.0267067551612854, Final Batch Loss: 0.5536019802093506\n",
      "Epoch 4081, Loss: 1.7201635241508484, Final Batch Loss: 0.31384187936782837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4082, Loss: 1.6543003022670746, Final Batch Loss: 0.19683316349983215\n",
      "Epoch 4083, Loss: 1.7222935557365417, Final Batch Loss: 0.41866418719291687\n",
      "Epoch 4084, Loss: 1.740930199623108, Final Batch Loss: 0.13821235299110413\n",
      "Epoch 4085, Loss: 2.084661513566971, Final Batch Loss: 0.6730309128761292\n",
      "Epoch 4086, Loss: 1.6047578006982803, Final Batch Loss: 0.17505042254924774\n",
      "Epoch 4087, Loss: 1.9504174292087555, Final Batch Loss: 0.5134692788124084\n",
      "Epoch 4088, Loss: 2.006587117910385, Final Batch Loss: 0.5871421694755554\n",
      "Epoch 4089, Loss: 2.272611141204834, Final Batch Loss: 0.7930352091789246\n",
      "Epoch 4090, Loss: 1.5971624106168747, Final Batch Loss: 0.1839151233434677\n",
      "Epoch 4091, Loss: 1.9365494847297668, Final Batch Loss: 0.3844606280326843\n",
      "Epoch 4092, Loss: 1.653414636850357, Final Batch Loss: 0.3625578284263611\n",
      "Epoch 4093, Loss: 1.7499569952487946, Final Batch Loss: 0.34891775250434875\n",
      "Epoch 4094, Loss: 2.077423334121704, Final Batch Loss: 0.7355822920799255\n",
      "Epoch 4095, Loss: 2.1196438670158386, Final Batch Loss: 0.7171673774719238\n",
      "Epoch 4096, Loss: 1.5574389100074768, Final Batch Loss: 0.17823997139930725\n",
      "Epoch 4097, Loss: 1.9510248303413391, Final Batch Loss: 0.6303783655166626\n",
      "Epoch 4098, Loss: 1.5823093801736832, Final Batch Loss: 0.22477494180202484\n",
      "Epoch 4099, Loss: 1.71681746840477, Final Batch Loss: 0.333294540643692\n",
      "Epoch 4100, Loss: 1.6021122336387634, Final Batch Loss: 0.2985539734363556\n",
      "Epoch 4101, Loss: 1.6450913846492767, Final Batch Loss: 0.31315508484840393\n",
      "Epoch 4102, Loss: 1.6288476288318634, Final Batch Loss: 0.3119218945503235\n",
      "Epoch 4103, Loss: 1.7610920369625092, Final Batch Loss: 0.39798155426979065\n",
      "Epoch 4104, Loss: 1.5563621819019318, Final Batch Loss: 0.1711963713169098\n",
      "Epoch 4105, Loss: 2.0347744822502136, Final Batch Loss: 0.6526013612747192\n",
      "Epoch 4106, Loss: 1.7762625813484192, Final Batch Loss: 0.48309436440467834\n",
      "Epoch 4107, Loss: 2.0345458388328552, Final Batch Loss: 0.5933486819267273\n",
      "Epoch 4108, Loss: 1.91903555393219, Final Batch Loss: 0.3688296675682068\n",
      "Epoch 4109, Loss: 2.6920420825481415, Final Batch Loss: 1.2938756942749023\n",
      "Epoch 4110, Loss: 1.7232969403266907, Final Batch Loss: 0.37363529205322266\n",
      "Epoch 4111, Loss: 1.7991297878324986, Final Batch Loss: 0.04840347543358803\n",
      "Epoch 4112, Loss: 1.9701208174228668, Final Batch Loss: 0.5062443614006042\n",
      "Epoch 4113, Loss: 1.702446699142456, Final Batch Loss: 0.2732512354850769\n",
      "Epoch 4114, Loss: 1.3907112255692482, Final Batch Loss: 0.06041193753480911\n",
      "Epoch 4115, Loss: 1.6117876321077347, Final Batch Loss: 0.2073630541563034\n",
      "Epoch 4116, Loss: 1.7699871957302094, Final Batch Loss: 0.4201838970184326\n",
      "Epoch 4117, Loss: 1.602407231926918, Final Batch Loss: 0.20693986117839813\n",
      "Epoch 4118, Loss: 1.5399682074785233, Final Batch Loss: 0.1536768525838852\n",
      "Epoch 4119, Loss: 1.7905749380588531, Final Batch Loss: 0.3905940353870392\n",
      "Epoch 4120, Loss: 1.542960524559021, Final Batch Loss: 0.2838367819786072\n",
      "Epoch 4121, Loss: 1.3016419857740402, Final Batch Loss: 0.06932364404201508\n",
      "Epoch 4122, Loss: 1.7964325845241547, Final Batch Loss: 0.5217835307121277\n",
      "Epoch 4123, Loss: 1.51486936211586, Final Batch Loss: 0.22339966893196106\n",
      "Epoch 4124, Loss: 1.6280635595321655, Final Batch Loss: 0.3027957081794739\n",
      "Epoch 4125, Loss: 1.615431785583496, Final Batch Loss: 0.2975952625274658\n",
      "Epoch 4126, Loss: 1.7301073670387268, Final Batch Loss: 0.3971657156944275\n",
      "Epoch 4127, Loss: 1.747390866279602, Final Batch Loss: 0.40261325240135193\n",
      "Epoch 4128, Loss: 1.637522667646408, Final Batch Loss: 0.2812611162662506\n",
      "Epoch 4129, Loss: 1.534434735774994, Final Batch Loss: 0.2801201641559601\n",
      "Epoch 4130, Loss: 1.442523181438446, Final Batch Loss: 0.14760026335716248\n",
      "Epoch 4131, Loss: 1.4419408589601517, Final Batch Loss: 0.1750836819410324\n",
      "Epoch 4132, Loss: 1.4745276123285294, Final Batch Loss: 0.24542860686779022\n",
      "Epoch 4133, Loss: 1.7421423494815826, Final Batch Loss: 0.3197498917579651\n",
      "Epoch 4134, Loss: 1.6089199781417847, Final Batch Loss: 0.26607638597488403\n",
      "Epoch 4135, Loss: 1.7994602620601654, Final Batch Loss: 0.5301901698112488\n",
      "Epoch 4136, Loss: 1.5106478333473206, Final Batch Loss: 0.24604323506355286\n",
      "Epoch 4137, Loss: 1.763660043478012, Final Batch Loss: 0.46100351214408875\n",
      "Epoch 4138, Loss: 2.301322430372238, Final Batch Loss: 1.0469008684158325\n",
      "Epoch 4139, Loss: 1.650847315788269, Final Batch Loss: 0.2805556356906891\n",
      "Epoch 4140, Loss: 1.6133374571800232, Final Batch Loss: 0.16155648231506348\n",
      "Epoch 4141, Loss: 1.9749016761779785, Final Batch Loss: 0.6451624631881714\n",
      "Epoch 4142, Loss: 2.1218903064727783, Final Batch Loss: 0.7342963814735413\n",
      "Epoch 4143, Loss: 1.554355725646019, Final Batch Loss: 0.16510818898677826\n",
      "Epoch 4144, Loss: 1.7543672919273376, Final Batch Loss: 0.3804221749305725\n",
      "Epoch 4145, Loss: 1.62139493227005, Final Batch Loss: 0.30281129479408264\n",
      "Epoch 4146, Loss: 1.5881384015083313, Final Batch Loss: 0.2868998646736145\n",
      "Epoch 4147, Loss: 1.5533105283975601, Final Batch Loss: 0.17334960401058197\n",
      "Epoch 4148, Loss: 1.6664733290672302, Final Batch Loss: 0.34257176518440247\n",
      "Epoch 4149, Loss: 1.8134726881980896, Final Batch Loss: 0.4928801953792572\n",
      "Epoch 4150, Loss: 1.6763664484024048, Final Batch Loss: 0.3290117383003235\n",
      "Epoch 4151, Loss: 1.5675312131643295, Final Batch Loss: 0.2092316895723343\n",
      "Epoch 4152, Loss: 1.7798149287700653, Final Batch Loss: 0.3774336278438568\n",
      "Epoch 4153, Loss: 1.770790696144104, Final Batch Loss: 0.46484383940696716\n",
      "Epoch 4154, Loss: 1.5324531197547913, Final Batch Loss: 0.15629982948303223\n",
      "Epoch 4155, Loss: 1.6557470858097076, Final Batch Loss: 0.384598970413208\n",
      "Epoch 4156, Loss: 1.6634268164634705, Final Batch Loss: 0.44615671038627625\n",
      "Epoch 4157, Loss: 1.4299624264240265, Final Batch Loss: 0.15996572375297546\n",
      "Epoch 4158, Loss: 1.357927031815052, Final Batch Loss: 0.07607180625200272\n",
      "Epoch 4159, Loss: 1.4650485068559647, Final Batch Loss: 0.21744997799396515\n",
      "Epoch 4160, Loss: 1.5661959946155548, Final Batch Loss: 0.2934991717338562\n",
      "Epoch 4161, Loss: 1.7133669555187225, Final Batch Loss: 0.4051620662212372\n",
      "Epoch 4162, Loss: 1.8058671057224274, Final Batch Loss: 0.5041689872741699\n",
      "Epoch 4163, Loss: 1.511264979839325, Final Batch Loss: 0.15932103991508484\n",
      "Epoch 4164, Loss: 2.0261765718460083, Final Batch Loss: 0.6588672995567322\n",
      "Epoch 4165, Loss: 1.6091359555721283, Final Batch Loss: 0.260128915309906\n",
      "Epoch 4166, Loss: 1.8066782057285309, Final Batch Loss: 0.47470980882644653\n",
      "Epoch 4167, Loss: 1.3847632929682732, Final Batch Loss: 0.08931543678045273\n",
      "Epoch 4168, Loss: 1.5855612754821777, Final Batch Loss: 0.26330819725990295\n",
      "Epoch 4169, Loss: 1.747077852487564, Final Batch Loss: 0.47807127237319946\n",
      "Epoch 4170, Loss: 1.8698680400848389, Final Batch Loss: 0.5646370649337769\n",
      "Epoch 4171, Loss: 1.638429343700409, Final Batch Loss: 0.3140501379966736\n",
      "Epoch 4172, Loss: 1.8175674676895142, Final Batch Loss: 0.4802476465702057\n",
      "Epoch 4173, Loss: 1.8323765397071838, Final Batch Loss: 0.4647200405597687\n",
      "Epoch 4174, Loss: 1.4928386509418488, Final Batch Loss: 0.2403406798839569\n",
      "Epoch 4175, Loss: 1.6352072954177856, Final Batch Loss: 0.2695102095603943\n",
      "Epoch 4176, Loss: 1.7075090110301971, Final Batch Loss: 0.25969353318214417\n",
      "Epoch 4177, Loss: 1.5669448673725128, Final Batch Loss: 0.20282292366027832\n",
      "Epoch 4178, Loss: 1.7482117712497711, Final Batch Loss: 0.2537262737751007\n",
      "Epoch 4179, Loss: 1.815151035785675, Final Batch Loss: 0.32164499163627625\n",
      "Epoch 4180, Loss: 1.7753745317459106, Final Batch Loss: 0.42353883385658264\n",
      "Epoch 4181, Loss: 1.5149365812540054, Final Batch Loss: 0.1412125676870346\n",
      "Epoch 4182, Loss: 1.4683025181293488, Final Batch Loss: 0.15508177876472473\n",
      "Epoch 4183, Loss: 1.4750291109085083, Final Batch Loss: 0.0974128246307373\n",
      "Epoch 4184, Loss: 1.7026610672473907, Final Batch Loss: 0.3460509479045868\n",
      "Epoch 4185, Loss: 1.8264310359954834, Final Batch Loss: 0.5590043663978577\n",
      "Epoch 4186, Loss: 1.5625059008598328, Final Batch Loss: 0.3192313015460968\n",
      "Epoch 4187, Loss: 1.7494811713695526, Final Batch Loss: 0.4591801166534424\n",
      "Epoch 4188, Loss: 1.4610333144664764, Final Batch Loss: 0.2625895142555237\n",
      "Epoch 4189, Loss: 1.59112086892128, Final Batch Loss: 0.22273918986320496\n",
      "Epoch 4190, Loss: 1.4390275701880455, Final Batch Loss: 0.09140821546316147\n",
      "Epoch 4191, Loss: 1.7067717909812927, Final Batch Loss: 0.4299863278865814\n",
      "Epoch 4192, Loss: 1.7229623198509216, Final Batch Loss: 0.34051749110221863\n",
      "Epoch 4193, Loss: 1.7642155438661575, Final Batch Loss: 0.5197144150733948\n",
      "Epoch 4194, Loss: 1.444289356470108, Final Batch Loss: 0.23488280177116394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4195, Loss: 1.3979288786649704, Final Batch Loss: 0.1636226922273636\n",
      "Epoch 4196, Loss: 1.5689540952444077, Final Batch Loss: 0.21888889372348785\n",
      "Epoch 4197, Loss: 1.41623155772686, Final Batch Loss: 0.18055473268032074\n",
      "Epoch 4198, Loss: 1.7864137589931488, Final Batch Loss: 0.4173758029937744\n",
      "Epoch 4199, Loss: 1.686057686805725, Final Batch Loss: 0.34601807594299316\n",
      "Epoch 4200, Loss: 1.4765211045742035, Final Batch Loss: 0.30324456095695496\n",
      "Epoch 4201, Loss: 1.5502971857786179, Final Batch Loss: 0.3581823706626892\n",
      "Epoch 4202, Loss: 1.708988606929779, Final Batch Loss: 0.45083457231521606\n",
      "Epoch 4203, Loss: 1.4056779593229294, Final Batch Loss: 0.1764795035123825\n",
      "Epoch 4204, Loss: 1.5087740421295166, Final Batch Loss: 0.16146329045295715\n",
      "Epoch 4205, Loss: 1.6190583258867264, Final Batch Loss: 0.19398237764835358\n",
      "Epoch 4206, Loss: 1.4471477419137955, Final Batch Loss: 0.1745065599679947\n",
      "Epoch 4207, Loss: 1.43008291721344, Final Batch Loss: 0.18739959597587585\n",
      "Epoch 4208, Loss: 1.6394361555576324, Final Batch Loss: 0.33911755681037903\n",
      "Epoch 4209, Loss: 2.2120389342308044, Final Batch Loss: 0.9401286244392395\n",
      "Epoch 4210, Loss: 1.450071096420288, Final Batch Loss: 0.15684938430786133\n",
      "Epoch 4211, Loss: 1.4069431871175766, Final Batch Loss: 0.14453436434268951\n",
      "Epoch 4212, Loss: 1.5737802982330322, Final Batch Loss: 0.2801797389984131\n",
      "Epoch 4213, Loss: 1.6563681662082672, Final Batch Loss: 0.3282148241996765\n",
      "Epoch 4214, Loss: 1.4090188667178154, Final Batch Loss: 0.11973568052053452\n",
      "Epoch 4215, Loss: 1.5734026283025742, Final Batch Loss: 0.2417992800474167\n",
      "Epoch 4216, Loss: 1.4930311143398285, Final Batch Loss: 0.2315199375152588\n",
      "Epoch 4217, Loss: 1.7389714121818542, Final Batch Loss: 0.31316012144088745\n",
      "Epoch 4218, Loss: 1.5843381434679031, Final Batch Loss: 0.09102462232112885\n",
      "Epoch 4219, Loss: 1.8484213650226593, Final Batch Loss: 0.4126322567462921\n",
      "Epoch 4220, Loss: 1.9959665834903717, Final Batch Loss: 0.3695757985115051\n",
      "Epoch 4221, Loss: 1.9169184565544128, Final Batch Loss: 0.45207634568214417\n",
      "Epoch 4222, Loss: 1.5464253146201372, Final Batch Loss: 0.029930802062153816\n",
      "Epoch 4223, Loss: 1.9473142325878143, Final Batch Loss: 0.47194743156433105\n",
      "Epoch 4224, Loss: 1.6014183461666107, Final Batch Loss: 0.35168394446372986\n",
      "Epoch 4225, Loss: 1.9702558517456055, Final Batch Loss: 0.6759577989578247\n",
      "Epoch 4226, Loss: 1.7119368612766266, Final Batch Loss: 0.3970811367034912\n",
      "Epoch 4227, Loss: 1.5051928609609604, Final Batch Loss: 0.20960311591625214\n",
      "Epoch 4228, Loss: 1.652882993221283, Final Batch Loss: 0.2568053603172302\n",
      "Epoch 4229, Loss: 1.4785142987966537, Final Batch Loss: 0.15144069492816925\n",
      "Epoch 4230, Loss: 1.90204656124115, Final Batch Loss: 0.6226726770401001\n",
      "Epoch 4231, Loss: 1.7896715104579926, Final Batch Loss: 0.46747368574142456\n",
      "Epoch 4232, Loss: 2.1491967141628265, Final Batch Loss: 0.8457704186439514\n",
      "Epoch 4233, Loss: 1.910799503326416, Final Batch Loss: 0.6356514096260071\n",
      "Epoch 4234, Loss: 1.5822162181138992, Final Batch Loss: 0.17444713413715363\n",
      "Epoch 4235, Loss: 1.4491615891456604, Final Batch Loss: 0.09671789407730103\n",
      "Epoch 4236, Loss: 1.9599848091602325, Final Batch Loss: 0.36642155051231384\n",
      "Epoch 4237, Loss: 1.4437445551156998, Final Batch Loss: 0.12091882526874542\n",
      "Epoch 4238, Loss: 1.3941592797636986, Final Batch Loss: 0.1199851855635643\n",
      "Epoch 4239, Loss: 1.9596334993839264, Final Batch Loss: 0.6279762983322144\n",
      "Epoch 4240, Loss: 1.8869343400001526, Final Batch Loss: 0.5315036177635193\n",
      "Epoch 4241, Loss: 1.572554737329483, Final Batch Loss: 0.25177904963493347\n",
      "Epoch 4242, Loss: 1.5070990324020386, Final Batch Loss: 0.27848535776138306\n",
      "Epoch 4243, Loss: 1.5728382766246796, Final Batch Loss: 0.24273920059204102\n",
      "Epoch 4244, Loss: 1.4813131839036942, Final Batch Loss: 0.20077185332775116\n",
      "Epoch 4245, Loss: 1.544153481721878, Final Batch Loss: 0.2564116418361664\n",
      "Epoch 4246, Loss: 1.4510457813739777, Final Batch Loss: 0.1499263048171997\n",
      "Epoch 4247, Loss: 1.6180703938007355, Final Batch Loss: 0.2817822992801666\n",
      "Epoch 4248, Loss: 1.6605657935142517, Final Batch Loss: 0.39713048934936523\n",
      "Epoch 4249, Loss: 1.6162353456020355, Final Batch Loss: 0.38959765434265137\n",
      "Epoch 4250, Loss: 1.5386191606521606, Final Batch Loss: 0.2849788963794708\n",
      "Epoch 4251, Loss: 1.5690647065639496, Final Batch Loss: 0.28829365968704224\n",
      "Epoch 4252, Loss: 1.3546119034290314, Final Batch Loss: 0.18035948276519775\n",
      "Epoch 4253, Loss: 1.634446769952774, Final Batch Loss: 0.39664310216903687\n",
      "Epoch 4254, Loss: 1.5978646278381348, Final Batch Loss: 0.2958175837993622\n",
      "Epoch 4255, Loss: 1.532198503613472, Final Batch Loss: 0.36003661155700684\n",
      "Epoch 4256, Loss: 1.3949442356824875, Final Batch Loss: 0.14212776720523834\n",
      "Epoch 4257, Loss: 1.649429589509964, Final Batch Loss: 0.33980345726013184\n",
      "Epoch 4258, Loss: 1.5499301105737686, Final Batch Loss: 0.21513207256793976\n",
      "Epoch 4259, Loss: 1.6141063570976257, Final Batch Loss: 0.3061736226081848\n",
      "Epoch 4260, Loss: 1.6041840612888336, Final Batch Loss: 0.3724551796913147\n",
      "Epoch 4261, Loss: 1.2668153643608093, Final Batch Loss: 0.05512350797653198\n",
      "Epoch 4262, Loss: 1.585623025894165, Final Batch Loss: 0.307370662689209\n",
      "Epoch 4263, Loss: 1.4794723093509674, Final Batch Loss: 0.2204258143901825\n",
      "Epoch 4264, Loss: 1.6922147274017334, Final Batch Loss: 0.37998440861701965\n",
      "Epoch 4265, Loss: 1.6626038253307343, Final Batch Loss: 0.4039851129055023\n",
      "Epoch 4266, Loss: 1.7463623881340027, Final Batch Loss: 0.41910210251808167\n",
      "Epoch 4267, Loss: 1.5779096484184265, Final Batch Loss: 0.3461780250072479\n",
      "Epoch 4268, Loss: 1.6901174783706665, Final Batch Loss: 0.3388008773326874\n",
      "Epoch 4269, Loss: 1.8033744096755981, Final Batch Loss: 0.527282178401947\n",
      "Epoch 4270, Loss: 1.351558044552803, Final Batch Loss: 0.11879633367061615\n",
      "Epoch 4271, Loss: 1.5358980298042297, Final Batch Loss: 0.21769559383392334\n",
      "Epoch 4272, Loss: 1.6000464260578156, Final Batch Loss: 0.27675753831863403\n",
      "Epoch 4273, Loss: 1.413579735904932, Final Batch Loss: 0.03899667039513588\n",
      "Epoch 4274, Loss: 1.5425328314304352, Final Batch Loss: 0.33965927362442017\n",
      "Epoch 4275, Loss: 1.3438258692622185, Final Batch Loss: 0.09276797622442245\n",
      "Epoch 4276, Loss: 1.7502979040145874, Final Batch Loss: 0.4770609736442566\n",
      "Epoch 4277, Loss: 1.6775020509958267, Final Batch Loss: 0.42598697543144226\n",
      "Epoch 4278, Loss: 1.6302904188632965, Final Batch Loss: 0.33789554238319397\n",
      "Epoch 4279, Loss: 1.7876712083816528, Final Batch Loss: 0.42860057950019836\n",
      "Epoch 4280, Loss: 1.7245299220085144, Final Batch Loss: 0.4529452919960022\n",
      "Epoch 4281, Loss: 1.6356836557388306, Final Batch Loss: 0.3431282639503479\n",
      "Epoch 4282, Loss: 1.3308018203824759, Final Batch Loss: 0.025918548926711082\n",
      "Epoch 4283, Loss: 1.7106822431087494, Final Batch Loss: 0.4670378267765045\n",
      "Epoch 4284, Loss: 1.8642637729644775, Final Batch Loss: 0.5098171830177307\n",
      "Epoch 4285, Loss: 1.8277691006660461, Final Batch Loss: 0.41086533665657043\n",
      "Epoch 4286, Loss: 1.6576655507087708, Final Batch Loss: 0.4288007915019989\n",
      "Epoch 4287, Loss: 1.8672555088996887, Final Batch Loss: 0.4933573305606842\n",
      "Epoch 4288, Loss: 1.4935187548398972, Final Batch Loss: 0.21655474603176117\n",
      "Epoch 4289, Loss: 1.587872937321663, Final Batch Loss: 0.24340112507343292\n",
      "Epoch 4290, Loss: 1.4945162013173103, Final Batch Loss: 0.06385064870119095\n",
      "Epoch 4291, Loss: 1.983056128025055, Final Batch Loss: 0.67254638671875\n",
      "Epoch 4292, Loss: 1.7583329379558563, Final Batch Loss: 0.4198262095451355\n",
      "Epoch 4293, Loss: 1.5606688559055328, Final Batch Loss: 0.23629319667816162\n",
      "Epoch 4294, Loss: 1.6940047144889832, Final Batch Loss: 0.4122084975242615\n",
      "Epoch 4295, Loss: 2.0042587220668793, Final Batch Loss: 0.6113139986991882\n",
      "Epoch 4296, Loss: 1.5780912637710571, Final Batch Loss: 0.34810346364974976\n",
      "Epoch 4297, Loss: 1.5604964196681976, Final Batch Loss: 0.26077431440353394\n",
      "Epoch 4298, Loss: 1.541753888130188, Final Batch Loss: 0.33634379506111145\n",
      "Epoch 4299, Loss: 1.7833420634269714, Final Batch Loss: 0.3870777487754822\n",
      "Epoch 4300, Loss: 1.8610919117927551, Final Batch Loss: 0.5268715023994446\n",
      "Epoch 4301, Loss: 1.906146228313446, Final Batch Loss: 0.6758900880813599\n",
      "Epoch 4302, Loss: 1.4286872074007988, Final Batch Loss: 0.10848798602819443\n",
      "Epoch 4303, Loss: 1.499181181192398, Final Batch Loss: 0.14673253893852234\n",
      "Epoch 4304, Loss: 1.6912763714790344, Final Batch Loss: 0.3185112476348877\n",
      "Epoch 4305, Loss: 1.5547239184379578, Final Batch Loss: 0.28169241547584534\n",
      "Epoch 4306, Loss: 1.517497107386589, Final Batch Loss: 0.30452898144721985\n",
      "Epoch 4307, Loss: 1.5682999938726425, Final Batch Loss: 0.24986569583415985\n",
      "Epoch 4308, Loss: 1.4962476789951324, Final Batch Loss: 0.1840651035308838\n",
      "Epoch 4309, Loss: 1.4539948850870132, Final Batch Loss: 0.06667830049991608\n",
      "Epoch 4310, Loss: 1.9455130994319916, Final Batch Loss: 0.3297976553440094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4311, Loss: 1.6572920978069305, Final Batch Loss: 0.32837316393852234\n",
      "Epoch 4312, Loss: 1.6261481940746307, Final Batch Loss: 0.33751222491264343\n",
      "Epoch 4313, Loss: 1.508090615272522, Final Batch Loss: 0.08714157342910767\n",
      "Epoch 4314, Loss: 1.459261417388916, Final Batch Loss: 0.053846269845962524\n",
      "Epoch 4315, Loss: 1.8997796773910522, Final Batch Loss: 0.4724533259868622\n",
      "Epoch 4316, Loss: 1.6049737483263016, Final Batch Loss: 0.23729531466960907\n",
      "Epoch 4317, Loss: 1.4118899255990982, Final Batch Loss: 0.06794138252735138\n",
      "Epoch 4318, Loss: 1.660523772239685, Final Batch Loss: 0.31302008032798767\n",
      "Epoch 4319, Loss: 1.80455881357193, Final Batch Loss: 0.46969208121299744\n",
      "Epoch 4320, Loss: 1.493957556784153, Final Batch Loss: 0.10937399417161942\n",
      "Epoch 4321, Loss: 1.7044222950935364, Final Batch Loss: 0.3442240357398987\n",
      "Epoch 4322, Loss: 1.805236965417862, Final Batch Loss: 0.46362432837486267\n",
      "Epoch 4323, Loss: 1.6059061586856842, Final Batch Loss: 0.33018049597740173\n",
      "Epoch 4324, Loss: 1.5376926511526108, Final Batch Loss: 0.16741646826267242\n",
      "Epoch 4325, Loss: 1.5890162885189056, Final Batch Loss: 0.25281384587287903\n",
      "Epoch 4326, Loss: 1.5723953023552895, Final Batch Loss: 0.11022991687059402\n",
      "Epoch 4327, Loss: 1.3619325309991837, Final Batch Loss: 0.08101372420787811\n",
      "Epoch 4328, Loss: 1.4494356215000153, Final Batch Loss: 0.2541436553001404\n",
      "Epoch 4329, Loss: 1.4245855882763863, Final Batch Loss: 0.10154532641172409\n",
      "Epoch 4330, Loss: 1.7474577128887177, Final Batch Loss: 0.3912680745124817\n",
      "Epoch 4331, Loss: 1.3816437646746635, Final Batch Loss: 0.12177937477827072\n",
      "Epoch 4332, Loss: 1.5663136541843414, Final Batch Loss: 0.24492383003234863\n",
      "Epoch 4333, Loss: 1.716301143169403, Final Batch Loss: 0.38261309266090393\n",
      "Epoch 4334, Loss: 1.6291214525699615, Final Batch Loss: 0.36935755610466003\n",
      "Epoch 4335, Loss: 1.4999975860118866, Final Batch Loss: 0.29966339468955994\n",
      "Epoch 4336, Loss: 1.590456634759903, Final Batch Loss: 0.2712574005126953\n",
      "Epoch 4337, Loss: 1.7802939116954803, Final Batch Loss: 0.4798445999622345\n",
      "Epoch 4338, Loss: 1.5795681178569794, Final Batch Loss: 0.33467230200767517\n",
      "Epoch 4339, Loss: 1.9057845771312714, Final Batch Loss: 0.6192708611488342\n",
      "Epoch 4340, Loss: 1.4869872480630875, Final Batch Loss: 0.1444186121225357\n",
      "Epoch 4341, Loss: 1.5933480560779572, Final Batch Loss: 0.38465458154678345\n",
      "Epoch 4342, Loss: 1.731933355331421, Final Batch Loss: 0.35853639245033264\n",
      "Epoch 4343, Loss: 1.5590042173862457, Final Batch Loss: 0.2840760052204132\n",
      "Epoch 4344, Loss: 1.575396567583084, Final Batch Loss: 0.3359449803829193\n",
      "Epoch 4345, Loss: 1.7275411188602448, Final Batch Loss: 0.31450191140174866\n",
      "Epoch 4346, Loss: 1.4182805716991425, Final Batch Loss: 0.1892615258693695\n",
      "Epoch 4347, Loss: 1.7096308469772339, Final Batch Loss: 0.3964289128780365\n",
      "Epoch 4348, Loss: 1.7602105736732483, Final Batch Loss: 0.45692795515060425\n",
      "Epoch 4349, Loss: 1.5093755573034286, Final Batch Loss: 0.1662377268075943\n",
      "Epoch 4350, Loss: 1.721642643213272, Final Batch Loss: 0.39007991552352905\n",
      "Epoch 4351, Loss: 1.7095932215452194, Final Batch Loss: 0.2434120923280716\n",
      "Epoch 4352, Loss: 1.4560551047325134, Final Batch Loss: 0.18734443187713623\n",
      "Epoch 4353, Loss: 1.6569900512695312, Final Batch Loss: 0.2738772928714752\n",
      "Epoch 4354, Loss: 1.8995538353919983, Final Batch Loss: 0.5396665930747986\n",
      "Epoch 4355, Loss: 2.209991306066513, Final Batch Loss: 0.8553001284599304\n",
      "Epoch 4356, Loss: 1.9172782003879547, Final Batch Loss: 0.3894112706184387\n",
      "Epoch 4357, Loss: 2.0050064027309418, Final Batch Loss: 0.6665025949478149\n",
      "Epoch 4358, Loss: 2.0683548152446747, Final Batch Loss: 0.7616368532180786\n",
      "Epoch 4359, Loss: 1.6151212006807327, Final Batch Loss: 0.20063786208629608\n",
      "Epoch 4360, Loss: 1.8107782006263733, Final Batch Loss: 0.4544667601585388\n",
      "Epoch 4361, Loss: 1.6630690395832062, Final Batch Loss: 0.3135471045970917\n",
      "Epoch 4362, Loss: 1.5493209213018417, Final Batch Loss: 0.22253181040287018\n",
      "Epoch 4363, Loss: 1.5454601645469666, Final Batch Loss: 0.19489240646362305\n",
      "Epoch 4364, Loss: 1.6997096538543701, Final Batch Loss: 0.3992709219455719\n",
      "Epoch 4365, Loss: 1.487367182970047, Final Batch Loss: 0.14089956879615784\n",
      "Epoch 4366, Loss: 1.5101827681064606, Final Batch Loss: 0.31349244713783264\n",
      "Epoch 4367, Loss: 1.565715592354536, Final Batch Loss: 0.05827439948916435\n",
      "Epoch 4368, Loss: 1.6108694970607758, Final Batch Loss: 0.27674657106399536\n",
      "Epoch 4369, Loss: 1.4802089780569077, Final Batch Loss: 0.19864045083522797\n",
      "Epoch 4370, Loss: 1.6772153079509735, Final Batch Loss: 0.44130340218544006\n",
      "Epoch 4371, Loss: 1.3469381481409073, Final Batch Loss: 0.08639122545719147\n",
      "Epoch 4372, Loss: 1.3830727487802505, Final Batch Loss: 0.13406451046466827\n",
      "Epoch 4373, Loss: 1.6327015161514282, Final Batch Loss: 0.40004706382751465\n",
      "Epoch 4374, Loss: 1.480661503970623, Final Batch Loss: 0.12042158097028732\n",
      "Epoch 4375, Loss: 1.4491557627916336, Final Batch Loss: 0.21362553536891937\n",
      "Epoch 4376, Loss: 1.6636129319667816, Final Batch Loss: 0.33286309242248535\n",
      "Epoch 4377, Loss: 1.8001599609851837, Final Batch Loss: 0.4889368712902069\n",
      "Epoch 4378, Loss: 1.7043536007404327, Final Batch Loss: 0.4325058162212372\n",
      "Epoch 4379, Loss: 1.9821155667304993, Final Batch Loss: 0.5459734797477722\n",
      "Epoch 4380, Loss: 1.7008537352085114, Final Batch Loss: 0.4111960530281067\n",
      "Epoch 4381, Loss: 2.3486841022968292, Final Batch Loss: 0.9635032415390015\n",
      "Epoch 4382, Loss: 1.7865441888570786, Final Batch Loss: 0.33086082339286804\n",
      "Epoch 4383, Loss: 1.6264230459928513, Final Batch Loss: 0.131086066365242\n",
      "Epoch 4384, Loss: 1.5544791966676712, Final Batch Loss: 0.2492019683122635\n",
      "Epoch 4385, Loss: 1.4548786133527756, Final Batch Loss: 0.17093898355960846\n",
      "Epoch 4386, Loss: 1.6589789390563965, Final Batch Loss: 0.33757176995277405\n",
      "Epoch 4387, Loss: 2.0283574610948563, Final Batch Loss: 0.75278240442276\n",
      "Epoch 4388, Loss: 1.6211716532707214, Final Batch Loss: 0.4014880061149597\n",
      "Epoch 4389, Loss: 1.595577210187912, Final Batch Loss: 0.374615877866745\n",
      "Epoch 4390, Loss: 1.6477811336517334, Final Batch Loss: 0.2887512147426605\n",
      "Epoch 4391, Loss: 1.7010263204574585, Final Batch Loss: 0.3903200924396515\n",
      "Epoch 4392, Loss: 1.575078308582306, Final Batch Loss: 0.2982754409313202\n",
      "Epoch 4393, Loss: 1.6361796408891678, Final Batch Loss: 0.21769045293331146\n",
      "Epoch 4394, Loss: 1.706757366657257, Final Batch Loss: 0.3587625324726105\n",
      "Epoch 4395, Loss: 1.6687833368778229, Final Batch Loss: 0.3817322552204132\n",
      "Epoch 4396, Loss: 1.8914473354816437, Final Batch Loss: 0.6039287447929382\n",
      "Epoch 4397, Loss: 1.5956559032201767, Final Batch Loss: 0.2391125112771988\n",
      "Epoch 4398, Loss: 1.8218439519405365, Final Batch Loss: 0.3183286786079407\n",
      "Epoch 4399, Loss: 1.7110826075077057, Final Batch Loss: 0.311964750289917\n",
      "Epoch 4400, Loss: 1.7732454240322113, Final Batch Loss: 0.41060978174209595\n",
      "Epoch 4401, Loss: 1.5609223991632462, Final Batch Loss: 0.22420065104961395\n",
      "Epoch 4402, Loss: 1.5326035618782043, Final Batch Loss: 0.2952691912651062\n",
      "Epoch 4403, Loss: 1.574671745300293, Final Batch Loss: 0.2150183618068695\n",
      "Epoch 4404, Loss: 1.353982850909233, Final Batch Loss: 0.13248440623283386\n",
      "Epoch 4405, Loss: 1.540835052728653, Final Batch Loss: 0.29668593406677246\n",
      "Epoch 4406, Loss: 1.8298526406288147, Final Batch Loss: 0.5115808248519897\n",
      "Epoch 4407, Loss: 1.4128891378641129, Final Batch Loss: 0.2010810822248459\n",
      "Epoch 4408, Loss: 1.664869785308838, Final Batch Loss: 0.3333304822444916\n",
      "Epoch 4409, Loss: 1.7216253578662872, Final Batch Loss: 0.4434834122657776\n",
      "Epoch 4410, Loss: 1.589629352092743, Final Batch Loss: 0.3068177402019501\n",
      "Epoch 4411, Loss: 1.6313691437244415, Final Batch Loss: 0.33024904131889343\n",
      "Epoch 4412, Loss: 1.4128351211547852, Final Batch Loss: 0.1332300305366516\n",
      "Epoch 4413, Loss: 1.330933228135109, Final Batch Loss: 0.09445686638355255\n",
      "Epoch 4414, Loss: 1.5789085924625397, Final Batch Loss: 0.27457383275032043\n",
      "Epoch 4415, Loss: 1.5430769324302673, Final Batch Loss: 0.34430649876594543\n",
      "Epoch 4416, Loss: 1.7975932955741882, Final Batch Loss: 0.5933166146278381\n",
      "Epoch 4417, Loss: 1.648803025484085, Final Batch Loss: 0.3065800368785858\n",
      "Epoch 4418, Loss: 1.655714750289917, Final Batch Loss: 0.27940309047698975\n",
      "Epoch 4419, Loss: 1.7321718633174896, Final Batch Loss: 0.36171743273735046\n",
      "Epoch 4420, Loss: 1.8832853436470032, Final Batch Loss: 0.567937970161438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4421, Loss: 1.4354041442275047, Final Batch Loss: 0.0996432825922966\n",
      "Epoch 4422, Loss: 1.7181451916694641, Final Batch Loss: 0.39370378851890564\n",
      "Epoch 4423, Loss: 1.6870754063129425, Final Batch Loss: 0.3942279517650604\n",
      "Epoch 4424, Loss: 1.6047371625900269, Final Batch Loss: 0.4109848141670227\n",
      "Epoch 4425, Loss: 1.485688254237175, Final Batch Loss: 0.1513158529996872\n",
      "Epoch 4426, Loss: 1.357923462986946, Final Batch Loss: 0.08818657696247101\n",
      "Epoch 4427, Loss: 1.4033411741256714, Final Batch Loss: 0.16484618186950684\n",
      "Epoch 4428, Loss: 1.7092696577310562, Final Batch Loss: 0.4103213846683502\n",
      "Epoch 4429, Loss: 1.6075778901576996, Final Batch Loss: 0.32805970311164856\n",
      "Epoch 4430, Loss: 1.74995955824852, Final Batch Loss: 0.2725980877876282\n",
      "Epoch 4431, Loss: 1.5225881040096283, Final Batch Loss: 0.18142366409301758\n",
      "Epoch 4432, Loss: 2.2146187722682953, Final Batch Loss: 0.901379406452179\n",
      "Epoch 4433, Loss: 1.4258845746517181, Final Batch Loss: 0.1891176402568817\n",
      "Epoch 4434, Loss: 1.5615836530923843, Final Batch Loss: 0.2280699759721756\n",
      "Epoch 4435, Loss: 1.4241386502981186, Final Batch Loss: 0.17659743130207062\n",
      "Epoch 4436, Loss: 1.5062931329011917, Final Batch Loss: 0.1777707189321518\n",
      "Epoch 4437, Loss: 1.6669240891933441, Final Batch Loss: 0.3940616548061371\n",
      "Epoch 4438, Loss: 1.913609892129898, Final Batch Loss: 0.6180061101913452\n",
      "Epoch 4439, Loss: 1.3456268198788166, Final Batch Loss: 0.05892859771847725\n",
      "Epoch 4440, Loss: 1.455295853316784, Final Batch Loss: 0.0863075777888298\n",
      "Epoch 4441, Loss: 1.9491865783929825, Final Batch Loss: 0.6602531671524048\n",
      "Epoch 4442, Loss: 1.3956985399127007, Final Batch Loss: 0.06469058245420456\n",
      "Epoch 4443, Loss: 1.6271201372146606, Final Batch Loss: 0.3720405101776123\n",
      "Epoch 4444, Loss: 1.758365273475647, Final Batch Loss: 0.43009063601493835\n",
      "Epoch 4445, Loss: 1.646695613861084, Final Batch Loss: 0.3345754146575928\n",
      "Epoch 4446, Loss: 1.769176334142685, Final Batch Loss: 0.28702354431152344\n",
      "Epoch 4447, Loss: 1.5963043868541718, Final Batch Loss: 0.25129005312919617\n",
      "Epoch 4448, Loss: 1.7101188004016876, Final Batch Loss: 0.40989547967910767\n",
      "Epoch 4449, Loss: 1.5434631407260895, Final Batch Loss: 0.25942477583885193\n",
      "Epoch 4450, Loss: 1.3343120999634266, Final Batch Loss: 0.03901704028248787\n",
      "Epoch 4451, Loss: 1.3459173366427422, Final Batch Loss: 0.10259359329938889\n",
      "Epoch 4452, Loss: 1.8293379247188568, Final Batch Loss: 0.5011025667190552\n",
      "Epoch 4453, Loss: 1.915532410144806, Final Batch Loss: 0.5058061480522156\n",
      "Epoch 4454, Loss: 1.8460130393505096, Final Batch Loss: 0.348338782787323\n",
      "Epoch 4455, Loss: 1.7392832040786743, Final Batch Loss: 0.36475446820259094\n",
      "Epoch 4456, Loss: 1.7290854454040527, Final Batch Loss: 0.5012050271034241\n",
      "Epoch 4457, Loss: 1.8853833973407745, Final Batch Loss: 0.42786863446235657\n",
      "Epoch 4458, Loss: 1.662587821483612, Final Batch Loss: 0.1506195068359375\n",
      "Epoch 4459, Loss: 1.3889154866337776, Final Batch Loss: 0.11385779827833176\n",
      "Epoch 4460, Loss: 1.7320421040058136, Final Batch Loss: 0.44950181245803833\n",
      "Epoch 4461, Loss: 1.455632284283638, Final Batch Loss: 0.11011476814746857\n",
      "Epoch 4462, Loss: 1.774480938911438, Final Batch Loss: 0.4567451477050781\n",
      "Epoch 4463, Loss: 1.5577762573957443, Final Batch Loss: 0.21610550582408905\n",
      "Epoch 4464, Loss: 1.5328694880008698, Final Batch Loss: 0.24449822306632996\n",
      "Epoch 4465, Loss: 1.3458541706204414, Final Batch Loss: 0.08968592435121536\n",
      "Epoch 4466, Loss: 1.3839649483561516, Final Batch Loss: 0.08775060623884201\n",
      "Epoch 4467, Loss: 1.5081869065761566, Final Batch Loss: 0.33534011244773865\n",
      "Epoch 4468, Loss: 1.4535071924328804, Final Batch Loss: 0.10741633921861649\n",
      "Epoch 4469, Loss: 1.58080592751503, Final Batch Loss: 0.28568997979164124\n",
      "Epoch 4470, Loss: 1.746737778186798, Final Batch Loss: 0.3271400034427643\n",
      "Epoch 4471, Loss: 1.7388094365596771, Final Batch Loss: 0.4036124646663666\n",
      "Epoch 4472, Loss: 1.4898445010185242, Final Batch Loss: 0.2980201840400696\n",
      "Epoch 4473, Loss: 1.350569911301136, Final Batch Loss: 0.11434284597635269\n",
      "Epoch 4474, Loss: 2.0359852612018585, Final Batch Loss: 0.6997685432434082\n",
      "Epoch 4475, Loss: 1.5684736371040344, Final Batch Loss: 0.4054182469844818\n",
      "Epoch 4476, Loss: 1.3170353472232819, Final Batch Loss: 0.03982418775558472\n",
      "Epoch 4477, Loss: 1.5130338370800018, Final Batch Loss: 0.3524262309074402\n",
      "Epoch 4478, Loss: 1.6888139247894287, Final Batch Loss: 0.30972346663475037\n",
      "Epoch 4479, Loss: 1.6169822812080383, Final Batch Loss: 0.2529792785644531\n",
      "Epoch 4480, Loss: 1.8845320045948029, Final Batch Loss: 0.5047129392623901\n",
      "Epoch 4481, Loss: 1.8549349904060364, Final Batch Loss: 0.5937560796737671\n",
      "Epoch 4482, Loss: 1.7380573898553848, Final Batch Loss: 0.22369088232517242\n",
      "Epoch 4483, Loss: 2.16105455160141, Final Batch Loss: 0.46901458501815796\n",
      "Epoch 4484, Loss: 1.7112853825092316, Final Batch Loss: 0.3814261853694916\n",
      "Epoch 4485, Loss: 1.6171622276306152, Final Batch Loss: 0.3589233458042145\n",
      "Epoch 4486, Loss: 2.1351391077041626, Final Batch Loss: 0.7948589324951172\n",
      "Epoch 4487, Loss: 1.694110780954361, Final Batch Loss: 0.4111374020576477\n",
      "Epoch 4488, Loss: 1.4783821254968643, Final Batch Loss: 0.17116700112819672\n",
      "Epoch 4489, Loss: 2.0691884756088257, Final Batch Loss: 0.7503101229667664\n",
      "Epoch 4490, Loss: 1.717886358499527, Final Batch Loss: 0.3797629177570343\n",
      "Epoch 4491, Loss: 1.6977610886096954, Final Batch Loss: 0.3934537470340729\n",
      "Epoch 4492, Loss: 1.5014922320842743, Final Batch Loss: 0.21441081166267395\n",
      "Epoch 4493, Loss: 1.6013475060462952, Final Batch Loss: 0.28735044598579407\n",
      "Epoch 4494, Loss: 1.4140205681324005, Final Batch Loss: 0.13870488107204437\n",
      "Epoch 4495, Loss: 1.4611264914274216, Final Batch Loss: 0.2168402522802353\n",
      "Epoch 4496, Loss: 1.6043077409267426, Final Batch Loss: 0.3263034224510193\n",
      "Epoch 4497, Loss: 1.7236664593219757, Final Batch Loss: 0.531978964805603\n",
      "Epoch 4498, Loss: 1.6334040462970734, Final Batch Loss: 0.4351871907711029\n",
      "Epoch 4499, Loss: 1.8312052488327026, Final Batch Loss: 0.48298579454421997\n",
      "Epoch 4500, Loss: 1.3105275072157383, Final Batch Loss: 0.0618194080889225\n",
      "Epoch 4501, Loss: 1.7046502828598022, Final Batch Loss: 0.3850892186164856\n",
      "Epoch 4502, Loss: 1.805048555135727, Final Batch Loss: 0.47583767771720886\n",
      "Epoch 4503, Loss: 1.3141472786664963, Final Batch Loss: 0.1262815147638321\n",
      "Epoch 4504, Loss: 1.6970190405845642, Final Batch Loss: 0.4974631369113922\n",
      "Epoch 4505, Loss: 1.6720749288797379, Final Batch Loss: 0.40185195207595825\n",
      "Epoch 4506, Loss: 1.7655200958251953, Final Batch Loss: 0.6254924535751343\n",
      "Epoch 4507, Loss: 1.7313649654388428, Final Batch Loss: 0.48426517844200134\n",
      "Epoch 4508, Loss: 1.7296877205371857, Final Batch Loss: 0.36593541502952576\n",
      "Epoch 4509, Loss: 1.5256086140871048, Final Batch Loss: 0.21504630148410797\n",
      "Epoch 4510, Loss: 1.9691225290298462, Final Batch Loss: 0.6117329597473145\n",
      "Epoch 4511, Loss: 1.4410275965929031, Final Batch Loss: 0.08981572091579437\n",
      "Epoch 4512, Loss: 1.4477069973945618, Final Batch Loss: 0.21897545456886292\n",
      "Epoch 4513, Loss: 1.4743967950344086, Final Batch Loss: 0.16983410716056824\n",
      "Epoch 4514, Loss: 1.5948058664798737, Final Batch Loss: 0.2748291790485382\n",
      "Epoch 4515, Loss: 1.902179777622223, Final Batch Loss: 0.5947675704956055\n",
      "Epoch 4516, Loss: 1.5120068788528442, Final Batch Loss: 0.2910374701023102\n",
      "Epoch 4517, Loss: 1.6920053660869598, Final Batch Loss: 0.3929912745952606\n",
      "Epoch 4518, Loss: 1.6775264739990234, Final Batch Loss: 0.39229923486709595\n",
      "Epoch 4519, Loss: 1.634704440832138, Final Batch Loss: 0.3348328173160553\n",
      "Epoch 4520, Loss: 1.3469568938016891, Final Batch Loss: 0.08084399998188019\n",
      "Epoch 4521, Loss: 1.4394812732934952, Final Batch Loss: 0.19541271030902863\n",
      "Epoch 4522, Loss: 1.7533469498157501, Final Batch Loss: 0.3395543694496155\n",
      "Epoch 4523, Loss: 2.032164454460144, Final Batch Loss: 0.4917304515838623\n",
      "Epoch 4524, Loss: 1.6804917752742767, Final Batch Loss: 0.09092825651168823\n",
      "Epoch 4525, Loss: 1.9629440903663635, Final Batch Loss: 0.4864334464073181\n",
      "Epoch 4526, Loss: 1.8103186190128326, Final Batch Loss: 0.40077510476112366\n",
      "Epoch 4527, Loss: 1.5921686887741089, Final Batch Loss: 0.2763819992542267\n",
      "Epoch 4528, Loss: 1.9664629399776459, Final Batch Loss: 0.6580485105514526\n",
      "Epoch 4529, Loss: 1.6699214279651642, Final Batch Loss: 0.3796912729740143\n",
      "Epoch 4530, Loss: 1.7579162418842316, Final Batch Loss: 0.3224925696849823\n",
      "Epoch 4531, Loss: 1.6795857846736908, Final Batch Loss: 0.30600157380104065\n",
      "Epoch 4532, Loss: 1.7029604315757751, Final Batch Loss: 0.3515067994594574\n",
      "Epoch 4533, Loss: 1.551036298274994, Final Batch Loss: 0.2571219801902771\n",
      "Epoch 4534, Loss: 1.5272043347358704, Final Batch Loss: 0.3023800849914551\n",
      "Epoch 4535, Loss: 1.4094829186797142, Final Batch Loss: 0.06819417327642441\n",
      "Epoch 4536, Loss: 1.6680341362953186, Final Batch Loss: 0.4156254231929779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4537, Loss: 1.7959008812904358, Final Batch Loss: 0.6057360768318176\n",
      "Epoch 4538, Loss: 1.9117180407047272, Final Batch Loss: 0.6859598755836487\n",
      "Epoch 4539, Loss: 1.2167630270123482, Final Batch Loss: 0.06574303656816483\n",
      "Epoch 4540, Loss: 1.4526012241840363, Final Batch Loss: 0.20804288983345032\n",
      "Epoch 4541, Loss: 1.344199150800705, Final Batch Loss: 0.11685365438461304\n",
      "Epoch 4542, Loss: 1.3385108970105648, Final Batch Loss: 0.051686253398656845\n",
      "Epoch 4543, Loss: 1.6538376212120056, Final Batch Loss: 0.39232465624809265\n",
      "Epoch 4544, Loss: 1.7836168706417084, Final Batch Loss: 0.4495803415775299\n",
      "Epoch 4545, Loss: 1.3354259729385376, Final Batch Loss: 0.13686522841453552\n",
      "Epoch 4546, Loss: 1.4401096850633621, Final Batch Loss: 0.23881883919239044\n",
      "Epoch 4547, Loss: 1.5220265686511993, Final Batch Loss: 0.24050727486610413\n",
      "Epoch 4548, Loss: 1.7252914309501648, Final Batch Loss: 0.5042794346809387\n",
      "Epoch 4549, Loss: 1.797893911600113, Final Batch Loss: 0.5371542572975159\n",
      "Epoch 4550, Loss: 1.571816861629486, Final Batch Loss: 0.261802613735199\n",
      "Epoch 4551, Loss: 1.4323192834854126, Final Batch Loss: 0.24730408191680908\n",
      "Epoch 4552, Loss: 1.4271087050437927, Final Batch Loss: 0.14317122101783752\n",
      "Epoch 4553, Loss: 1.4821668416261673, Final Batch Loss: 0.2096153050661087\n",
      "Epoch 4554, Loss: 1.3330305516719818, Final Batch Loss: 0.14219257235527039\n",
      "Epoch 4555, Loss: 1.4371088296175003, Final Batch Loss: 0.13904167711734772\n",
      "Epoch 4556, Loss: 1.678459256887436, Final Batch Loss: 0.4145665168762207\n",
      "Epoch 4557, Loss: 1.7311311960220337, Final Batch Loss: 0.42654678225517273\n",
      "Epoch 4558, Loss: 1.6285084187984467, Final Batch Loss: 0.39720773696899414\n",
      "Epoch 4559, Loss: 1.4003788605332375, Final Batch Loss: 0.09811326116323471\n",
      "Epoch 4560, Loss: 1.546998992562294, Final Batch Loss: 0.2436356097459793\n",
      "Epoch 4561, Loss: 1.682956576347351, Final Batch Loss: 0.32933253049850464\n",
      "Epoch 4562, Loss: 1.492523618042469, Final Batch Loss: 0.11347062140703201\n",
      "Epoch 4563, Loss: 1.962713360786438, Final Batch Loss: 0.532328724861145\n",
      "Epoch 4564, Loss: 1.8739512264728546, Final Batch Loss: 0.4933915138244629\n",
      "Epoch 4565, Loss: 1.4635650888085365, Final Batch Loss: 0.034268178045749664\n",
      "Epoch 4566, Loss: 1.8813587129116058, Final Batch Loss: 0.5588401556015015\n",
      "Epoch 4567, Loss: 1.253071490675211, Final Batch Loss: 0.05748899653553963\n",
      "Epoch 4568, Loss: 1.589852437376976, Final Batch Loss: 0.15753670036792755\n",
      "Epoch 4569, Loss: 1.4195976108312607, Final Batch Loss: 0.1903759092092514\n",
      "Epoch 4570, Loss: 1.5301683694124222, Final Batch Loss: 0.1557675153017044\n",
      "Epoch 4571, Loss: 1.3458151742815971, Final Batch Loss: 0.032633475959300995\n",
      "Epoch 4572, Loss: 1.8338603973388672, Final Batch Loss: 0.4845612943172455\n",
      "Epoch 4573, Loss: 1.3089230209589005, Final Batch Loss: 0.06737120449542999\n",
      "Epoch 4574, Loss: 1.4703217893838882, Final Batch Loss: 0.19343341886997223\n",
      "Epoch 4575, Loss: 1.4552279710769653, Final Batch Loss: 0.2037951648235321\n",
      "Epoch 4576, Loss: 1.6197293996810913, Final Batch Loss: 0.35513725876808167\n",
      "Epoch 4577, Loss: 1.619393914937973, Final Batch Loss: 0.2479187548160553\n",
      "Epoch 4578, Loss: 2.013983368873596, Final Batch Loss: 0.7475568652153015\n",
      "Epoch 4579, Loss: 1.3950542472302914, Final Batch Loss: 0.042218830436468124\n",
      "Epoch 4580, Loss: 1.3641291558742523, Final Batch Loss: 0.06331735849380493\n",
      "Epoch 4581, Loss: 1.2487313449382782, Final Batch Loss: 0.08101582527160645\n",
      "Epoch 4582, Loss: 1.5785133689641953, Final Batch Loss: 0.20505379140377045\n",
      "Epoch 4583, Loss: 1.7999344170093536, Final Batch Loss: 0.5465660691261292\n",
      "Epoch 4584, Loss: 1.50184765458107, Final Batch Loss: 0.2672220766544342\n",
      "Epoch 4585, Loss: 1.7699010074138641, Final Batch Loss: 0.5039702653884888\n",
      "Epoch 4586, Loss: 1.3846701383590698, Final Batch Loss: 0.11554627120494843\n",
      "Epoch 4587, Loss: 1.712452381849289, Final Batch Loss: 0.437054306268692\n",
      "Epoch 4588, Loss: 1.5319398045539856, Final Batch Loss: 0.253724604845047\n",
      "Epoch 4589, Loss: 1.6980393826961517, Final Batch Loss: 0.3730975091457367\n",
      "Epoch 4590, Loss: 1.608904391527176, Final Batch Loss: 0.2524881958961487\n",
      "Epoch 4591, Loss: 1.570906825363636, Final Batch Loss: 0.07126233726739883\n",
      "Epoch 4592, Loss: 1.4412314146757126, Final Batch Loss: 0.22526301443576813\n",
      "Epoch 4593, Loss: 1.4462944269180298, Final Batch Loss: 0.2924886643886566\n",
      "Epoch 4594, Loss: 1.724477231502533, Final Batch Loss: 0.515499472618103\n",
      "Epoch 4595, Loss: 1.5242464542388916, Final Batch Loss: 0.19091519713401794\n",
      "Epoch 4596, Loss: 1.5074117183685303, Final Batch Loss: 0.2625134587287903\n",
      "Epoch 4597, Loss: 1.701792150735855, Final Batch Loss: 0.4769844114780426\n",
      "Epoch 4598, Loss: 1.4937685132026672, Final Batch Loss: 0.2523892819881439\n",
      "Epoch 4599, Loss: 1.824769288301468, Final Batch Loss: 0.525898277759552\n",
      "Epoch 4600, Loss: 1.69515922665596, Final Batch Loss: 0.39363202452659607\n",
      "Epoch 4601, Loss: 1.6720823645591736, Final Batch Loss: 0.5064967274665833\n",
      "Epoch 4602, Loss: 1.5554661750793457, Final Batch Loss: 0.23584023118019104\n",
      "Epoch 4603, Loss: 1.7035874128341675, Final Batch Loss: 0.38641342520713806\n",
      "Epoch 4604, Loss: 1.7996077835559845, Final Batch Loss: 0.40083274245262146\n",
      "Epoch 4605, Loss: 1.4772049263119698, Final Batch Loss: 0.08027056604623795\n",
      "Epoch 4606, Loss: 1.4991866946220398, Final Batch Loss: 0.2323606014251709\n",
      "Epoch 4607, Loss: 1.4951131790876389, Final Batch Loss: 0.1121283620595932\n",
      "Epoch 4608, Loss: 1.6627902388572693, Final Batch Loss: 0.28946128487586975\n",
      "Epoch 4609, Loss: 1.6218310594558716, Final Batch Loss: 0.3229637145996094\n",
      "Epoch 4610, Loss: 1.342271238565445, Final Batch Loss: 0.1867574006319046\n",
      "Epoch 4611, Loss: 1.4422844499349594, Final Batch Loss: 0.20424021780490875\n",
      "Epoch 4612, Loss: 1.4304009675979614, Final Batch Loss: 0.1946331262588501\n",
      "Epoch 4613, Loss: 1.31459341943264, Final Batch Loss: 0.06399069726467133\n",
      "Epoch 4614, Loss: 1.394185185432434, Final Batch Loss: 0.14782026410102844\n",
      "Epoch 4615, Loss: 1.3520689457654953, Final Batch Loss: 0.14915503561496735\n",
      "Epoch 4616, Loss: 1.457520104944706, Final Batch Loss: 0.11833081394433975\n",
      "Epoch 4617, Loss: 1.7809450924396515, Final Batch Loss: 0.5663079619407654\n",
      "Epoch 4618, Loss: 1.3453139811754227, Final Batch Loss: 0.12028516829013824\n",
      "Epoch 4619, Loss: 1.5994793176651, Final Batch Loss: 0.2968244254589081\n",
      "Epoch 4620, Loss: 1.7243435680866241, Final Batch Loss: 0.4389665722846985\n",
      "Epoch 4621, Loss: 1.3345944434404373, Final Batch Loss: 0.24556933343410492\n",
      "Epoch 4622, Loss: 1.4183136075735092, Final Batch Loss: 0.19289349019527435\n",
      "Epoch 4623, Loss: 1.620434731245041, Final Batch Loss: 0.2504637837409973\n",
      "Epoch 4624, Loss: 1.6018387079238892, Final Batch Loss: 0.30444684624671936\n",
      "Epoch 4625, Loss: 1.3907819986343384, Final Batch Loss: 0.2103452980518341\n",
      "Epoch 4626, Loss: 1.3836676105856895, Final Batch Loss: 0.07886048406362534\n",
      "Epoch 4627, Loss: 1.4166752249002457, Final Batch Loss: 0.16399644315242767\n",
      "Epoch 4628, Loss: 1.485539048910141, Final Batch Loss: 0.25186052918434143\n",
      "Epoch 4629, Loss: 1.5009301453828812, Final Batch Loss: 0.31342315673828125\n",
      "Epoch 4630, Loss: 1.5370595753192902, Final Batch Loss: 0.27057257294654846\n",
      "Epoch 4631, Loss: 1.3994582742452621, Final Batch Loss: 0.1507941633462906\n",
      "Epoch 4632, Loss: 1.7408089488744736, Final Batch Loss: 0.2403566986322403\n",
      "Epoch 4633, Loss: 1.5420477986335754, Final Batch Loss: 0.27892041206359863\n",
      "Epoch 4634, Loss: 1.3467258997261524, Final Batch Loss: 0.03761493042111397\n",
      "Epoch 4635, Loss: 1.3960498124361038, Final Batch Loss: 0.16076770424842834\n",
      "Epoch 4636, Loss: 1.4087360948324203, Final Batch Loss: 0.15966205298900604\n",
      "Epoch 4637, Loss: 1.638922095298767, Final Batch Loss: 0.2726450562477112\n",
      "Epoch 4638, Loss: 1.4576400965452194, Final Batch Loss: 0.20270182192325592\n",
      "Epoch 4639, Loss: 1.371322676539421, Final Batch Loss: 0.13263900578022003\n",
      "Epoch 4640, Loss: 1.9905562698841095, Final Batch Loss: 0.6779597401618958\n",
      "Epoch 4641, Loss: 1.4863137602806091, Final Batch Loss: 0.24223282933235168\n",
      "Epoch 4642, Loss: 1.5347108244895935, Final Batch Loss: 0.2847161889076233\n",
      "Epoch 4643, Loss: 1.5939958095550537, Final Batch Loss: 0.2509074807167053\n",
      "Epoch 4644, Loss: 1.6691307723522186, Final Batch Loss: 0.48822712898254395\n",
      "Epoch 4645, Loss: 1.4863294959068298, Final Batch Loss: 0.25245946645736694\n",
      "Epoch 4646, Loss: 1.4253918826580048, Final Batch Loss: 0.11112093925476074\n",
      "Epoch 4647, Loss: 1.4750241339206696, Final Batch Loss: 0.19827091693878174\n",
      "Epoch 4648, Loss: 1.8262258768081665, Final Batch Loss: 0.5625932216644287\n",
      "Epoch 4649, Loss: 1.570504516363144, Final Batch Loss: 0.2515968978404999\n",
      "Epoch 4650, Loss: 1.4973135143518448, Final Batch Loss: 0.18312297761440277\n",
      "Epoch 4651, Loss: 1.5476457476615906, Final Batch Loss: 0.2958339750766754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4652, Loss: 1.4926857650279999, Final Batch Loss: 0.31707528233528137\n",
      "Epoch 4653, Loss: 1.8978899419307709, Final Batch Loss: 0.6415185928344727\n",
      "Epoch 4654, Loss: 1.6471637785434723, Final Batch Loss: 0.3913722038269043\n",
      "Epoch 4655, Loss: 1.5062413439154625, Final Batch Loss: 0.09036274999380112\n",
      "Epoch 4656, Loss: 1.3932485282421112, Final Batch Loss: 0.049750059843063354\n",
      "Epoch 4657, Loss: 1.6235210001468658, Final Batch Loss: 0.39148879051208496\n",
      "Epoch 4658, Loss: 1.4918900281190872, Final Batch Loss: 0.24011816084384918\n",
      "Epoch 4659, Loss: 1.70378777384758, Final Batch Loss: 0.42004913091659546\n",
      "Epoch 4660, Loss: 1.8739351034164429, Final Batch Loss: 0.4563877284526825\n",
      "Epoch 4661, Loss: 1.4624264985322952, Final Batch Loss: 0.09192900359630585\n",
      "Epoch 4662, Loss: 1.7628569304943085, Final Batch Loss: 0.38380399346351624\n",
      "Epoch 4663, Loss: 1.360618695616722, Final Batch Loss: 0.09064634144306183\n",
      "Epoch 4664, Loss: 1.5213534533977509, Final Batch Loss: 0.17719149589538574\n",
      "Epoch 4665, Loss: 1.409946396946907, Final Batch Loss: 0.13969917595386505\n",
      "Epoch 4666, Loss: 1.418618068099022, Final Batch Loss: 0.2050180286169052\n",
      "Epoch 4667, Loss: 1.7269112765789032, Final Batch Loss: 0.5453456044197083\n",
      "Epoch 4668, Loss: 1.445647954940796, Final Batch Loss: 0.21359851956367493\n",
      "Epoch 4669, Loss: 1.6318226754665375, Final Batch Loss: 0.4244391918182373\n",
      "Epoch 4670, Loss: 1.53872412443161, Final Batch Loss: 0.28139522671699524\n",
      "Epoch 4671, Loss: 1.2563982009887695, Final Batch Loss: 0.07192647457122803\n",
      "Epoch 4672, Loss: 1.9689196050167084, Final Batch Loss: 0.5464575886726379\n",
      "Epoch 4673, Loss: 1.9943231344223022, Final Batch Loss: 0.6608062386512756\n",
      "Epoch 4674, Loss: 1.6655770391225815, Final Batch Loss: 0.2206891030073166\n",
      "Epoch 4675, Loss: 1.773021161556244, Final Batch Loss: 0.2286721169948578\n",
      "Epoch 4676, Loss: 1.7219327688217163, Final Batch Loss: 0.36900898814201355\n",
      "Epoch 4677, Loss: 1.653536081314087, Final Batch Loss: 0.32693180441856384\n",
      "Epoch 4678, Loss: 1.6621035635471344, Final Batch Loss: 0.354338675737381\n",
      "Epoch 4679, Loss: 1.7744132280349731, Final Batch Loss: 0.36208051443099976\n",
      "Epoch 4680, Loss: 1.6677954196929932, Final Batch Loss: 0.39026281237602234\n",
      "Epoch 4681, Loss: 1.6006876826286316, Final Batch Loss: 0.27637979388237\n",
      "Epoch 4682, Loss: 1.3608094900846481, Final Batch Loss: 0.1819634586572647\n",
      "Epoch 4683, Loss: 1.7733489871025085, Final Batch Loss: 0.4338146150112152\n",
      "Epoch 4684, Loss: 1.7632810175418854, Final Batch Loss: 0.30360159277915955\n",
      "Epoch 4685, Loss: 1.6825911104679108, Final Batch Loss: 0.3617289364337921\n",
      "Epoch 4686, Loss: 2.1030793488025665, Final Batch Loss: 0.823788046836853\n",
      "Epoch 4687, Loss: 1.6822600364685059, Final Batch Loss: 0.30171719193458557\n",
      "Epoch 4688, Loss: 1.5587358325719833, Final Batch Loss: 0.37421998381614685\n",
      "Epoch 4689, Loss: 1.657700628042221, Final Batch Loss: 0.44685035943984985\n",
      "Epoch 4690, Loss: 1.546131208539009, Final Batch Loss: 0.19000931084156036\n",
      "Epoch 4691, Loss: 1.3873132020235062, Final Batch Loss: 0.15217702090740204\n",
      "Epoch 4692, Loss: 1.354748159646988, Final Batch Loss: 0.19158145785331726\n",
      "Epoch 4693, Loss: 1.4463003873825073, Final Batch Loss: 0.13804000616073608\n",
      "Epoch 4694, Loss: 1.223920650780201, Final Batch Loss: 0.10575804859399796\n",
      "Epoch 4695, Loss: 1.7133658230304718, Final Batch Loss: 0.5483684539794922\n",
      "Epoch 4696, Loss: 1.4623951613903046, Final Batch Loss: 0.2734110653400421\n",
      "Epoch 4697, Loss: 1.7762401700019836, Final Batch Loss: 0.5347748398780823\n",
      "Epoch 4698, Loss: 1.712072730064392, Final Batch Loss: 0.399750292301178\n",
      "Epoch 4699, Loss: 1.2626928314566612, Final Batch Loss: 0.0770469531416893\n",
      "Epoch 4700, Loss: 1.545137271285057, Final Batch Loss: 0.18322961032390594\n",
      "Epoch 4701, Loss: 1.609364628791809, Final Batch Loss: 0.4250365197658539\n",
      "Epoch 4702, Loss: 2.0858342349529266, Final Batch Loss: 0.8509741425514221\n",
      "Epoch 4703, Loss: 1.6365964114665985, Final Batch Loss: 0.3618350625038147\n",
      "Epoch 4704, Loss: 1.3610710948705673, Final Batch Loss: 0.18864567577838898\n",
      "Epoch 4705, Loss: 1.8911626189947128, Final Batch Loss: 0.7700093388557434\n",
      "Epoch 4706, Loss: 1.5646074712276459, Final Batch Loss: 0.2600889801979065\n",
      "Epoch 4707, Loss: 1.6667149364948273, Final Batch Loss: 0.3810139000415802\n",
      "Epoch 4708, Loss: 1.646676629781723, Final Batch Loss: 0.4602522552013397\n",
      "Epoch 4709, Loss: 1.3846179842948914, Final Batch Loss: 0.12809216976165771\n",
      "Epoch 4710, Loss: 1.2926799207925797, Final Batch Loss: 0.11780695617198944\n",
      "Epoch 4711, Loss: 1.2465276271104813, Final Batch Loss: 0.10479338467121124\n",
      "Epoch 4712, Loss: 1.54610076546669, Final Batch Loss: 0.30872684717178345\n",
      "Epoch 4713, Loss: 1.5748444497585297, Final Batch Loss: 0.2860974967479706\n",
      "Epoch 4714, Loss: 1.6699965596199036, Final Batch Loss: 0.5240591168403625\n",
      "Epoch 4715, Loss: 1.764682948589325, Final Batch Loss: 0.538840651512146\n",
      "Epoch 4716, Loss: 1.5677544176578522, Final Batch Loss: 0.2833864092826843\n",
      "Epoch 4717, Loss: 1.634312391281128, Final Batch Loss: 0.24535074830055237\n",
      "Epoch 4718, Loss: 1.2851066663861275, Final Batch Loss: 0.03579071909189224\n",
      "Epoch 4719, Loss: 1.4299234449863434, Final Batch Loss: 0.24511365592479706\n",
      "Epoch 4720, Loss: 1.5340109318494797, Final Batch Loss: 0.19107384979724884\n",
      "Epoch 4721, Loss: 1.4478911012411118, Final Batch Loss: 0.2007247656583786\n",
      "Epoch 4722, Loss: 1.4348954111337662, Final Batch Loss: 0.19970159232616425\n",
      "Epoch 4723, Loss: 1.679163619875908, Final Batch Loss: 0.4953540861606598\n",
      "Epoch 4724, Loss: 1.7745253443717957, Final Batch Loss: 0.4849463403224945\n",
      "Epoch 4725, Loss: 1.6947385668754578, Final Batch Loss: 0.40849703550338745\n",
      "Epoch 4726, Loss: 1.473719596862793, Final Batch Loss: 0.2131088674068451\n",
      "Epoch 4727, Loss: 1.3703095391392708, Final Batch Loss: 0.0962633565068245\n",
      "Epoch 4728, Loss: 1.4478256702423096, Final Batch Loss: 0.24097701907157898\n",
      "Epoch 4729, Loss: 1.5888266414403915, Final Batch Loss: 0.20663656294345856\n",
      "Epoch 4730, Loss: 1.6760035902261734, Final Batch Loss: 0.43665337562561035\n",
      "Epoch 4731, Loss: 1.7166512310504913, Final Batch Loss: 0.4768677055835724\n",
      "Epoch 4732, Loss: 1.4834943413734436, Final Batch Loss: 0.2582739293575287\n",
      "Epoch 4733, Loss: 1.4679167717695236, Final Batch Loss: 0.17333702743053436\n",
      "Epoch 4734, Loss: 1.8269217312335968, Final Batch Loss: 0.4998440146446228\n",
      "Epoch 4735, Loss: 1.4546330571174622, Final Batch Loss: 0.19234076142311096\n",
      "Epoch 4736, Loss: 1.5342425107955933, Final Batch Loss: 0.2672538161277771\n",
      "Epoch 4737, Loss: 1.6799256503582, Final Batch Loss: 0.4642738699913025\n",
      "Epoch 4738, Loss: 1.6844859719276428, Final Batch Loss: 0.3744044601917267\n",
      "Epoch 4739, Loss: 1.3765776827931404, Final Batch Loss: 0.10539398342370987\n",
      "Epoch 4740, Loss: 2.337678611278534, Final Batch Loss: 1.070734977722168\n",
      "Epoch 4741, Loss: 1.6109387874603271, Final Batch Loss: 0.25945231318473816\n",
      "Epoch 4742, Loss: 1.7887890487909317, Final Batch Loss: 0.6282413601875305\n",
      "Epoch 4743, Loss: 2.060807704925537, Final Batch Loss: 0.8074498176574707\n",
      "Epoch 4744, Loss: 1.7113075852394104, Final Batch Loss: 0.4136185646057129\n",
      "Epoch 4745, Loss: 1.4489048644900322, Final Batch Loss: 0.11736487597227097\n",
      "Epoch 4746, Loss: 1.823332041501999, Final Batch Loss: 0.5837112665176392\n",
      "Epoch 4747, Loss: 1.3695841953158379, Final Batch Loss: 0.03870343416929245\n",
      "Epoch 4748, Loss: 1.9322481453418732, Final Batch Loss: 0.6444083452224731\n",
      "Epoch 4749, Loss: 1.6101528406143188, Final Batch Loss: 0.3284335732460022\n",
      "Epoch 4750, Loss: 1.7805988043546677, Final Batch Loss: 0.4802778661251068\n",
      "Epoch 4751, Loss: 1.5314626693725586, Final Batch Loss: 0.2650797665119171\n",
      "Epoch 4752, Loss: 1.874034434556961, Final Batch Loss: 0.6047472357749939\n",
      "Epoch 4753, Loss: 1.460472971200943, Final Batch Loss: 0.18868684768676758\n",
      "Epoch 4754, Loss: 1.9158750772476196, Final Batch Loss: 0.6720865368843079\n",
      "Epoch 4755, Loss: 1.5805604457855225, Final Batch Loss: 0.277174711227417\n",
      "Epoch 4756, Loss: 1.5626931190490723, Final Batch Loss: 0.26066115498542786\n",
      "Epoch 4757, Loss: 1.6336346864700317, Final Batch Loss: 0.3179197907447815\n",
      "Epoch 4758, Loss: 1.919596016407013, Final Batch Loss: 0.22361814975738525\n",
      "Epoch 4759, Loss: 1.773522526025772, Final Batch Loss: 0.38956716656684875\n",
      "Epoch 4760, Loss: 1.8989483267068863, Final Batch Loss: 0.45126110315322876\n",
      "Epoch 4761, Loss: 1.72980797290802, Final Batch Loss: 0.293715238571167\n",
      "Epoch 4762, Loss: 1.7393259108066559, Final Batch Loss: 0.3009631037712097\n",
      "Epoch 4763, Loss: 1.7444437742233276, Final Batch Loss: 0.3457854688167572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4764, Loss: 1.3181604892015457, Final Batch Loss: 0.08824069797992706\n",
      "Epoch 4765, Loss: 1.6328075230121613, Final Batch Loss: 0.3384116590023041\n",
      "Epoch 4766, Loss: 1.4635802954435349, Final Batch Loss: 0.1685890406370163\n",
      "Epoch 4767, Loss: 1.6410718858242035, Final Batch Loss: 0.376504123210907\n",
      "Epoch 4768, Loss: 1.6540104150772095, Final Batch Loss: 0.3832504451274872\n",
      "Epoch 4769, Loss: 1.3623510748147964, Final Batch Loss: 0.18857689201831818\n",
      "Epoch 4770, Loss: 1.3561007231473923, Final Batch Loss: 0.11581166088581085\n",
      "Epoch 4771, Loss: 1.560865968465805, Final Batch Loss: 0.31873688101768494\n",
      "Epoch 4772, Loss: 1.549048274755478, Final Batch Loss: 0.2719821333885193\n",
      "Epoch 4773, Loss: 1.8771938979625702, Final Batch Loss: 0.5844098329544067\n",
      "Epoch 4774, Loss: 1.6636315286159515, Final Batch Loss: 0.3811592161655426\n",
      "Epoch 4775, Loss: 1.757193773984909, Final Batch Loss: 0.5177679657936096\n",
      "Epoch 4776, Loss: 1.5711308717727661, Final Batch Loss: 0.35538095235824585\n",
      "Epoch 4777, Loss: 1.629081815481186, Final Batch Loss: 0.375751256942749\n",
      "Epoch 4778, Loss: 1.6095777750015259, Final Batch Loss: 0.3989541828632355\n",
      "Epoch 4779, Loss: 1.3732183575630188, Final Batch Loss: 0.17171907424926758\n",
      "Epoch 4780, Loss: 1.666993960738182, Final Batch Loss: 0.4956173896789551\n",
      "Epoch 4781, Loss: 1.4005414694547653, Final Batch Loss: 0.21769319474697113\n",
      "Epoch 4782, Loss: 1.3748876824975014, Final Batch Loss: 0.11922828108072281\n",
      "Epoch 4783, Loss: 1.4756088256835938, Final Batch Loss: 0.2677341103553772\n",
      "Epoch 4784, Loss: 1.4567576497793198, Final Batch Loss: 0.1991988867521286\n",
      "Epoch 4785, Loss: 1.6103120148181915, Final Batch Loss: 0.46261587738990784\n",
      "Epoch 4786, Loss: 1.4489924907684326, Final Batch Loss: 0.31086447834968567\n",
      "Epoch 4787, Loss: 1.3645468428730965, Final Batch Loss: 0.06778711825609207\n",
      "Epoch 4788, Loss: 1.504301130771637, Final Batch Loss: 0.2802092134952545\n",
      "Epoch 4789, Loss: 1.325729139149189, Final Batch Loss: 0.11790690571069717\n",
      "Epoch 4790, Loss: 1.774363100528717, Final Batch Loss: 0.5209309458732605\n",
      "Epoch 4791, Loss: 1.2786563336849213, Final Batch Loss: 0.11304837465286255\n",
      "Epoch 4792, Loss: 1.625402182340622, Final Batch Loss: 0.2822103798389435\n",
      "Epoch 4793, Loss: 1.6011097431182861, Final Batch Loss: 0.30176201462745667\n",
      "Epoch 4794, Loss: 1.4417306184768677, Final Batch Loss: 0.2058420181274414\n",
      "Epoch 4795, Loss: 1.8413703739643097, Final Batch Loss: 0.6703042387962341\n",
      "Epoch 4796, Loss: 1.6180494576692581, Final Batch Loss: 0.3421784043312073\n",
      "Epoch 4797, Loss: 1.639970600605011, Final Batch Loss: 0.4091391861438751\n",
      "Epoch 4798, Loss: 1.323806717991829, Final Batch Loss: 0.16912515461444855\n",
      "Epoch 4799, Loss: 1.8116840422153473, Final Batch Loss: 0.5977548360824585\n",
      "Epoch 4800, Loss: 1.2392794862389565, Final Batch Loss: 0.06983087211847305\n",
      "Epoch 4801, Loss: 1.266536682844162, Final Batch Loss: 0.046813368797302246\n",
      "Epoch 4802, Loss: 1.3931669294834137, Final Batch Loss: 0.18377652764320374\n",
      "Epoch 4803, Loss: 1.26222600415349, Final Batch Loss: 0.03846638277173042\n",
      "Epoch 4804, Loss: 1.4479020237922668, Final Batch Loss: 0.25524696707725525\n",
      "Epoch 4805, Loss: 1.2235656194388866, Final Batch Loss: 0.04567710682749748\n",
      "Epoch 4806, Loss: 2.0193251818418503, Final Batch Loss: 0.886819064617157\n",
      "Epoch 4807, Loss: 1.6121539175510406, Final Batch Loss: 0.285184770822525\n",
      "Epoch 4808, Loss: 1.637638658285141, Final Batch Loss: 0.5026368498802185\n",
      "Epoch 4809, Loss: 1.4984425902366638, Final Batch Loss: 0.3003104627132416\n",
      "Epoch 4810, Loss: 1.5878959000110626, Final Batch Loss: 0.2556777596473694\n",
      "Epoch 4811, Loss: 1.4508084952831268, Final Batch Loss: 0.24698495864868164\n",
      "Epoch 4812, Loss: 1.4755179584026337, Final Batch Loss: 0.26185131072998047\n",
      "Epoch 4813, Loss: 1.3569414913654327, Final Batch Loss: 0.16834750771522522\n",
      "Epoch 4814, Loss: 1.2486863136291504, Final Batch Loss: 0.06238707900047302\n",
      "Epoch 4815, Loss: 1.4168306589126587, Final Batch Loss: 0.21457087993621826\n",
      "Epoch 4816, Loss: 1.2654896602034569, Final Batch Loss: 0.09940608590841293\n",
      "Epoch 4817, Loss: 1.4972880631685257, Final Batch Loss: 0.4034323990345001\n",
      "Epoch 4818, Loss: 1.5269455015659332, Final Batch Loss: 0.33075886964797974\n",
      "Epoch 4819, Loss: 1.4593029767274857, Final Batch Loss: 0.2674786150455475\n",
      "Epoch 4820, Loss: 1.6022772192955017, Final Batch Loss: 0.2896030843257904\n",
      "Epoch 4821, Loss: 1.5000970661640167, Final Batch Loss: 0.28372207283973694\n",
      "Epoch 4822, Loss: 1.6699170768260956, Final Batch Loss: 0.379487007856369\n",
      "Epoch 4823, Loss: 1.6714910864830017, Final Batch Loss: 0.2867465615272522\n",
      "Epoch 4824, Loss: 1.351279854774475, Final Batch Loss: 0.21869027614593506\n",
      "Epoch 4825, Loss: 1.6712385267019272, Final Batch Loss: 0.3227178156375885\n",
      "Epoch 4826, Loss: 1.623084396123886, Final Batch Loss: 0.3588157296180725\n",
      "Epoch 4827, Loss: 1.677289754152298, Final Batch Loss: 0.4040886461734772\n",
      "Epoch 4828, Loss: 1.4465644508600235, Final Batch Loss: 0.18447349965572357\n",
      "Epoch 4829, Loss: 1.714119702577591, Final Batch Loss: 0.3428936004638672\n",
      "Epoch 4830, Loss: 1.395663559436798, Final Batch Loss: 0.1925845444202423\n",
      "Epoch 4831, Loss: 1.6572777777910233, Final Batch Loss: 0.4530762732028961\n",
      "Epoch 4832, Loss: 1.3694879040122032, Final Batch Loss: 0.08033689111471176\n",
      "Epoch 4833, Loss: 1.8710463345050812, Final Batch Loss: 0.5349292159080505\n",
      "Epoch 4834, Loss: 1.6411537826061249, Final Batch Loss: 0.3422439396381378\n",
      "Epoch 4835, Loss: 1.4836233407258987, Final Batch Loss: 0.13284547626972198\n",
      "Epoch 4836, Loss: 1.5191737413406372, Final Batch Loss: 0.4243693947792053\n",
      "Epoch 4837, Loss: 1.7997715175151825, Final Batch Loss: 0.3195934593677521\n",
      "Epoch 4838, Loss: 1.748127043247223, Final Batch Loss: 0.3457021415233612\n",
      "Epoch 4839, Loss: 1.6851363331079483, Final Batch Loss: 0.2269650250673294\n",
      "Epoch 4840, Loss: 1.80887371301651, Final Batch Loss: 0.34129372239112854\n",
      "Epoch 4841, Loss: 1.4934986867010593, Final Batch Loss: 0.02146277204155922\n",
      "Epoch 4842, Loss: 1.6347268670797348, Final Batch Loss: 0.24414809048175812\n",
      "Epoch 4843, Loss: 1.8076997697353363, Final Batch Loss: 0.5315297842025757\n",
      "Epoch 4844, Loss: 1.661135047674179, Final Batch Loss: 0.2724706828594208\n",
      "Epoch 4845, Loss: 1.5125867500901222, Final Batch Loss: 0.08456612378358841\n",
      "Epoch 4846, Loss: 1.645288348197937, Final Batch Loss: 0.3982791304588318\n",
      "Epoch 4847, Loss: 1.5454451143741608, Final Batch Loss: 0.36713290214538574\n",
      "Epoch 4848, Loss: 1.7959214746952057, Final Batch Loss: 0.5192066431045532\n",
      "Epoch 4849, Loss: 1.4977922141551971, Final Batch Loss: 0.24395516514778137\n",
      "Epoch 4850, Loss: 1.913586676120758, Final Batch Loss: 0.5435745120048523\n",
      "Epoch 4851, Loss: 1.4515680074691772, Final Batch Loss: 0.21330654621124268\n",
      "Epoch 4852, Loss: 1.273669756948948, Final Batch Loss: 0.0834488645195961\n",
      "Epoch 4853, Loss: 1.5356521904468536, Final Batch Loss: 0.25277742743492126\n",
      "Epoch 4854, Loss: 1.7924036085605621, Final Batch Loss: 0.5079486966133118\n",
      "Epoch 4855, Loss: 1.747789353132248, Final Batch Loss: 0.48988011479377747\n",
      "Epoch 4856, Loss: 1.5909310579299927, Final Batch Loss: 0.3759450316429138\n",
      "Epoch 4857, Loss: 1.341996394097805, Final Batch Loss: 0.08439966291189194\n",
      "Epoch 4858, Loss: 1.445922464132309, Final Batch Loss: 0.26481857895851135\n",
      "Epoch 4859, Loss: 1.4251552522182465, Final Batch Loss: 0.14074939489364624\n",
      "Epoch 4860, Loss: 1.5964470207691193, Final Batch Loss: 0.43142059445381165\n",
      "Epoch 4861, Loss: 1.344763234257698, Final Batch Loss: 0.16574697196483612\n",
      "Epoch 4862, Loss: 1.6410797536373138, Final Batch Loss: 0.4158916771411896\n",
      "Epoch 4863, Loss: 1.4511260241270065, Final Batch Loss: 0.20864395797252655\n",
      "Epoch 4864, Loss: 1.6825588792562485, Final Batch Loss: 0.4932781159877777\n",
      "Epoch 4865, Loss: 1.4037625044584274, Final Batch Loss: 0.17085836827754974\n",
      "Epoch 4866, Loss: 1.5890888273715973, Final Batch Loss: 0.27953073382377625\n",
      "Epoch 4867, Loss: 1.8931941390037537, Final Batch Loss: 0.5583687424659729\n",
      "Epoch 4868, Loss: 1.4275758564472198, Final Batch Loss: 0.15736842155456543\n",
      "Epoch 4869, Loss: 1.702736109495163, Final Batch Loss: 0.3238401412963867\n",
      "Epoch 4870, Loss: 1.5538326501846313, Final Batch Loss: 0.32235240936279297\n",
      "Epoch 4871, Loss: 1.7439543455839157, Final Batch Loss: 0.5332670211791992\n",
      "Epoch 4872, Loss: 1.6606252789497375, Final Batch Loss: 0.3901117742061615\n",
      "Epoch 4873, Loss: 1.8902416229248047, Final Batch Loss: 0.33745622634887695\n",
      "Epoch 4874, Loss: 1.5791128128767014, Final Batch Loss: 0.20415471494197845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4875, Loss: 1.4723033010959625, Final Batch Loss: 0.13981953263282776\n",
      "Epoch 4876, Loss: 1.5574347078800201, Final Batch Loss: 0.27486488223075867\n",
      "Epoch 4877, Loss: 1.7846229374408722, Final Batch Loss: 0.48560264706611633\n",
      "Epoch 4878, Loss: 1.5859536230564117, Final Batch Loss: 0.3776788115501404\n",
      "Epoch 4879, Loss: 1.5126896500587463, Final Batch Loss: 0.28425973653793335\n",
      "Epoch 4880, Loss: 1.4493619501590729, Final Batch Loss: 0.20003843307495117\n",
      "Epoch 4881, Loss: 1.5925387144088745, Final Batch Loss: 0.27369818091392517\n",
      "Epoch 4882, Loss: 1.329044371843338, Final Batch Loss: 0.17108884453773499\n",
      "Epoch 4883, Loss: 1.5584344863891602, Final Batch Loss: 0.3970637917518616\n",
      "Epoch 4884, Loss: 1.4926426708698273, Final Batch Loss: 0.3132566809654236\n",
      "Epoch 4885, Loss: 1.2397691532969475, Final Batch Loss: 0.07684733718633652\n",
      "Epoch 4886, Loss: 1.526261329650879, Final Batch Loss: 0.3180946409702301\n",
      "Epoch 4887, Loss: 1.6051425337791443, Final Batch Loss: 0.29118695855140686\n",
      "Epoch 4888, Loss: 1.8812085390090942, Final Batch Loss: 0.6041166186332703\n",
      "Epoch 4889, Loss: 1.9442036151885986, Final Batch Loss: 0.728020966053009\n",
      "Epoch 4890, Loss: 1.4953471571207047, Final Batch Loss: 0.0825905054807663\n",
      "Epoch 4891, Loss: 1.5159540325403214, Final Batch Loss: 0.10856209695339203\n",
      "Epoch 4892, Loss: 2.227731615304947, Final Batch Loss: 0.956244170665741\n",
      "Epoch 4893, Loss: 1.7928160727024078, Final Batch Loss: 0.47459283471107483\n",
      "Epoch 4894, Loss: 1.4733398482203484, Final Batch Loss: 0.09757525473833084\n",
      "Epoch 4895, Loss: 1.9768208861351013, Final Batch Loss: 0.5828472375869751\n",
      "Epoch 4896, Loss: 1.9164105355739594, Final Batch Loss: 0.49202337861061096\n",
      "Epoch 4897, Loss: 1.6588707268238068, Final Batch Loss: 0.36664167046546936\n",
      "Epoch 4898, Loss: 1.9534277617931366, Final Batch Loss: 0.6039180755615234\n",
      "Epoch 4899, Loss: 1.6912884414196014, Final Batch Loss: 0.4136113226413727\n",
      "Epoch 4900, Loss: 1.7151981592178345, Final Batch Loss: 0.2973889708518982\n",
      "Epoch 4901, Loss: 1.6103771328926086, Final Batch Loss: 0.3813025653362274\n",
      "Epoch 4902, Loss: 1.6675845384597778, Final Batch Loss: 0.2950728237628937\n",
      "Epoch 4903, Loss: 1.567748337984085, Final Batch Loss: 0.2941826283931732\n",
      "Epoch 4904, Loss: 1.4659924358129501, Final Batch Loss: 0.20642484724521637\n",
      "Epoch 4905, Loss: 1.373728595674038, Final Batch Loss: 0.11455627530813217\n",
      "Epoch 4906, Loss: 1.6367529928684235, Final Batch Loss: 0.3091813623905182\n",
      "Epoch 4907, Loss: 1.6651123464107513, Final Batch Loss: 0.4612870216369629\n",
      "Epoch 4908, Loss: 1.4322968125343323, Final Batch Loss: 0.1431247442960739\n",
      "Epoch 4909, Loss: 1.8098407983779907, Final Batch Loss: 0.41214704513549805\n",
      "Epoch 4910, Loss: 1.5922677516937256, Final Batch Loss: 0.23646214604377747\n",
      "Epoch 4911, Loss: 1.4720053374767303, Final Batch Loss: 0.19019034504890442\n",
      "Epoch 4912, Loss: 1.3975671119987965, Final Batch Loss: 0.0620722658932209\n",
      "Epoch 4913, Loss: 1.9419638514518738, Final Batch Loss: 0.663720965385437\n",
      "Epoch 4914, Loss: 1.3954214751720428, Final Batch Loss: 0.1409229338169098\n",
      "Epoch 4915, Loss: 2.0056824386119843, Final Batch Loss: 0.6737938523292542\n",
      "Epoch 4916, Loss: 1.5963332056999207, Final Batch Loss: 0.26900148391723633\n",
      "Epoch 4917, Loss: 1.9209605157375336, Final Batch Loss: 0.6450718641281128\n",
      "Epoch 4918, Loss: 1.8001408278942108, Final Batch Loss: 0.5497490763664246\n",
      "Epoch 4919, Loss: 1.5802265107631683, Final Batch Loss: 0.2983851134777069\n",
      "Epoch 4920, Loss: 1.50505630671978, Final Batch Loss: 0.17214031517505646\n",
      "Epoch 4921, Loss: 1.383543774485588, Final Batch Loss: 0.0679972916841507\n",
      "Epoch 4922, Loss: 1.9307910203933716, Final Batch Loss: 0.666172981262207\n",
      "Epoch 4923, Loss: 1.5525349378585815, Final Batch Loss: 0.24293115735054016\n",
      "Epoch 4924, Loss: 1.5444725006818771, Final Batch Loss: 0.14107449352741241\n",
      "Epoch 4925, Loss: 1.5083363950252533, Final Batch Loss: 0.21110662817955017\n",
      "Epoch 4926, Loss: 1.787538766860962, Final Batch Loss: 0.6006120443344116\n",
      "Epoch 4927, Loss: 1.5261141061782837, Final Batch Loss: 0.28652840852737427\n",
      "Epoch 4928, Loss: 1.6270732879638672, Final Batch Loss: 0.4134872257709503\n",
      "Epoch 4929, Loss: 1.559522032737732, Final Batch Loss: 0.3489914536476135\n",
      "Epoch 4930, Loss: 1.7325003147125244, Final Batch Loss: 0.538669228553772\n",
      "Epoch 4931, Loss: 1.5120868980884552, Final Batch Loss: 0.23466220498085022\n",
      "Epoch 4932, Loss: 1.3012474030256271, Final Batch Loss: 0.13006849586963654\n",
      "Epoch 4933, Loss: 1.4346207678318024, Final Batch Loss: 0.22006508708000183\n",
      "Epoch 4934, Loss: 1.410780131816864, Final Batch Loss: 0.19880518317222595\n",
      "Epoch 4935, Loss: 1.4922001361846924, Final Batch Loss: 0.2729877531528473\n",
      "Epoch 4936, Loss: 1.3729649484157562, Final Batch Loss: 0.2342924028635025\n",
      "Epoch 4937, Loss: 1.3915871381759644, Final Batch Loss: 0.26746666431427\n",
      "Epoch 4938, Loss: 1.2647287230938673, Final Batch Loss: 0.02362564019858837\n",
      "Epoch 4939, Loss: 1.278094220906496, Final Batch Loss: 0.06226151064038277\n",
      "Epoch 4940, Loss: 1.810896784067154, Final Batch Loss: 0.5931639075279236\n",
      "Epoch 4941, Loss: 1.3647029995918274, Final Batch Loss: 0.14921703934669495\n",
      "Epoch 4942, Loss: 1.3491879422217607, Final Batch Loss: 0.018206866458058357\n",
      "Epoch 4943, Loss: 1.404665246605873, Final Batch Loss: 0.2054165154695511\n",
      "Epoch 4944, Loss: 1.1729443334043026, Final Batch Loss: 0.04816850647330284\n",
      "Epoch 4945, Loss: 1.2657454013824463, Final Batch Loss: 0.14042864739894867\n",
      "Epoch 4946, Loss: 1.3966254591941833, Final Batch Loss: 0.2193433791399002\n",
      "Epoch 4947, Loss: 1.6589342951774597, Final Batch Loss: 0.4353109300136566\n",
      "Epoch 4948, Loss: 1.4205841571092606, Final Batch Loss: 0.18183396756649017\n",
      "Epoch 4949, Loss: 1.5086477100849152, Final Batch Loss: 0.22645989060401917\n",
      "Epoch 4950, Loss: 1.2725499980151653, Final Batch Loss: 0.05449024960398674\n",
      "Epoch 4951, Loss: 1.5143926739692688, Final Batch Loss: 0.32598423957824707\n",
      "Epoch 4952, Loss: 1.2440549954771996, Final Batch Loss: 0.10551733523607254\n",
      "Epoch 4953, Loss: 1.485785499215126, Final Batch Loss: 0.1936810463666916\n",
      "Epoch 4954, Loss: 1.3223957866430283, Final Batch Loss: 0.19326813519001007\n",
      "Epoch 4955, Loss: 1.5813843458890915, Final Batch Loss: 0.21949516236782074\n",
      "Epoch 4956, Loss: 1.3161086738109589, Final Batch Loss: 0.1535985916852951\n",
      "Epoch 4957, Loss: 1.440661445260048, Final Batch Loss: 0.2712370455265045\n",
      "Epoch 4958, Loss: 1.3744833767414093, Final Batch Loss: 0.18137288093566895\n",
      "Epoch 4959, Loss: 1.4406033009290695, Final Batch Loss: 0.18645928800106049\n",
      "Epoch 4960, Loss: 1.4511404782533646, Final Batch Loss: 0.28494831919670105\n",
      "Epoch 4961, Loss: 1.6521832942962646, Final Batch Loss: 0.4466261565685272\n",
      "Epoch 4962, Loss: 1.3672379404306412, Final Batch Loss: 0.15153548121452332\n",
      "Epoch 4963, Loss: 1.2974150255322456, Final Batch Loss: 0.11492756754159927\n",
      "Epoch 4964, Loss: 1.711574763059616, Final Batch Loss: 0.5745011568069458\n",
      "Epoch 4965, Loss: 1.682650774717331, Final Batch Loss: 0.3422122895717621\n",
      "Epoch 4966, Loss: 1.5671797394752502, Final Batch Loss: 0.37141481041908264\n",
      "Epoch 4967, Loss: 1.487474873661995, Final Batch Loss: 0.228484109044075\n",
      "Epoch 4968, Loss: 1.8432502448558807, Final Batch Loss: 0.5294496417045593\n",
      "Epoch 4969, Loss: 1.5582077205181122, Final Batch Loss: 0.3713809847831726\n",
      "Epoch 4970, Loss: 1.613153725862503, Final Batch Loss: 0.4282231032848358\n",
      "Epoch 4971, Loss: 1.2823386043310165, Final Batch Loss: 0.09785573184490204\n",
      "Epoch 4972, Loss: 1.4188275039196014, Final Batch Loss: 0.264560729265213\n",
      "Epoch 4973, Loss: 1.5714031457901, Final Batch Loss: 0.2601458728313446\n",
      "Epoch 4974, Loss: 1.5431419610977173, Final Batch Loss: 0.39966651797294617\n",
      "Epoch 4975, Loss: 1.3260160610079765, Final Batch Loss: 0.0901399478316307\n",
      "Epoch 4976, Loss: 1.2233826518058777, Final Batch Loss: 0.13076341152191162\n",
      "Epoch 4977, Loss: 1.6219902336597443, Final Batch Loss: 0.4241422116756439\n",
      "Epoch 4978, Loss: 1.5508815944194794, Final Batch Loss: 0.26501476764678955\n",
      "Epoch 4979, Loss: 2.1516670286655426, Final Batch Loss: 0.8120719194412231\n",
      "Epoch 4980, Loss: 1.4926199615001678, Final Batch Loss: 0.20947861671447754\n",
      "Epoch 4981, Loss: 1.7143507897853851, Final Batch Loss: 0.39342090487480164\n",
      "Epoch 4982, Loss: 1.6050180792808533, Final Batch Loss: 0.2702811658382416\n",
      "Epoch 4983, Loss: 1.6889727115631104, Final Batch Loss: 0.4290776252746582\n",
      "Epoch 4984, Loss: 1.6716501414775848, Final Batch Loss: 0.3671063780784607\n",
      "Epoch 4985, Loss: 1.5597879588603973, Final Batch Loss: 0.2572271525859833\n",
      "Epoch 4986, Loss: 1.4834605008363724, Final Batch Loss: 0.2110738605260849\n",
      "Epoch 4987, Loss: 1.5780848562717438, Final Batch Loss: 0.2810722589492798\n",
      "Epoch 4988, Loss: 1.353760290890932, Final Batch Loss: 0.05406739190220833\n",
      "Epoch 4989, Loss: 1.6611745357513428, Final Batch Loss: 0.4291689991950989\n",
      "Epoch 4990, Loss: 1.5184667855501175, Final Batch Loss: 0.19841523468494415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4991, Loss: 1.6779500246047974, Final Batch Loss: 0.40050381422042847\n",
      "Epoch 4992, Loss: 1.5949321389198303, Final Batch Loss: 0.3211595118045807\n",
      "Epoch 4993, Loss: 1.3985361903905869, Final Batch Loss: 0.17456196248531342\n",
      "Epoch 4994, Loss: 1.6181000769138336, Final Batch Loss: 0.3325694501399994\n",
      "Epoch 4995, Loss: 1.4676940329372883, Final Batch Loss: 0.058357227593660355\n",
      "Epoch 4996, Loss: 1.3499031066894531, Final Batch Loss: 0.08158093690872192\n",
      "Epoch 4997, Loss: 1.4322989135980606, Final Batch Loss: 0.07290883362293243\n",
      "Epoch 4998, Loss: 1.5558934211730957, Final Batch Loss: 0.34720054268836975\n",
      "Epoch 4999, Loss: 1.3225022703409195, Final Batch Loss: 0.15284039080142975\n",
      "Epoch 5000, Loss: 1.3336227014660835, Final Batch Loss: 0.0535629466176033\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  3  0  0  1  0  0  0  0  0  2  0  0  0  0  0  1  0  0  1]\n",
      " [ 0  0  0  0  0  0 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  4  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1 12  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1 10  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  2  0  0  0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  3  0  0  3  0  0  1  0  0  1  0  0  2  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 13  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0  2  0  0  0  0  0  1  0  0  5  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 13  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  0  0]\n",
      " [ 0  0  0  0  0  2  0  0  1  0  0  0  0  0  0  0  0  0  0  0  4  0  0  2]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 14  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  0  1  0  0  0  0  0  0  0  0  1  0  0 12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.94118   1.00000   0.96970        16\n",
      "           1    0.81818   1.00000   0.90000         9\n",
      "           2    0.55556   1.00000   0.71429         5\n",
      "           3    1.00000   1.00000   1.00000        11\n",
      "           4    0.90909   1.00000   0.95238        10\n",
      "           5    0.33333   0.37500   0.35294         8\n",
      "           6    1.00000   1.00000   1.00000        12\n",
      "           7    1.00000   0.91667   0.95652        12\n",
      "           8    0.44444   0.80000   0.57143         5\n",
      "           9    0.92308   0.92308   0.92308        13\n",
      "          10    0.85714   0.92308   0.88889        13\n",
      "          11    0.76923   0.90909   0.83333        11\n",
      "          12    0.90000   0.75000   0.81818        12\n",
      "          13    1.00000   1.00000   1.00000         4\n",
      "          14    0.40000   0.18182   0.25000        11\n",
      "          15    1.00000   1.00000   1.00000        13\n",
      "          16    1.00000   1.00000   1.00000        12\n",
      "          17    1.00000   0.55556   0.71429         9\n",
      "          18    1.00000   0.92857   0.96296        14\n",
      "          19    1.00000   1.00000   1.00000         9\n",
      "          20    0.57143   0.44444   0.50000         9\n",
      "          21    1.00000   1.00000   1.00000        14\n",
      "          22    1.00000   0.85714   0.92308         7\n",
      "          23    0.75000   0.80000   0.77419        15\n",
      "\n",
      "    accuracy                        0.85827       254\n",
      "   macro avg    0.84053   0.84852   0.83355       254\n",
      "weighted avg    0.86369   0.85827   0.85293       254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.train()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Fake Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20,000 epochs DEFAULT PARAMETERS FOR THRESHOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (gen): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=111, out_features=80, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=80, out_features=60, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=60, out_features=50, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Linear(in_features=50, out_features=32, bias=True)\n",
       "    (4): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator(z_dim = 111)\n",
    "load_model(gen, \"3 Label 8 Subject GAN Ablation_gen.param\")\n",
    "gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(X_test)\n",
    "latent_vectors = get_noise(size, 100)\n",
    "act_vectors = get_act_matrix(size, 3)\n",
    "usr_vectors = get_usr_matrix(size, 8)\n",
    "\n",
    "fake_labels = []\n",
    "\n",
    "for k in range(size):\n",
    "    if act_vectors[0][k] == 0 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(0)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(1)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(2)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(3)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(4)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(5)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(6)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(7)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(8)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 3:\n",
    "        fake_labels.append(9)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 3:\n",
    "        fake_labels.append(10)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 3:\n",
    "        fake_labels.append(11)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 4:\n",
    "        fake_labels.append(12)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 4:\n",
    "        fake_labels.append(13)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 4:\n",
    "        fake_labels.append(14)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 5:\n",
    "        fake_labels.append(15)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 5:\n",
    "        fake_labels.append(16)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 5:\n",
    "        fake_labels.append(17)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 6:\n",
    "        fake_labels.append(18)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 6:\n",
    "        fake_labels.append(19)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 6:\n",
    "        fake_labels.append(20)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 7:\n",
    "        fake_labels.append(21)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 7:\n",
    "        fake_labels.append(22)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 7:\n",
    "        fake_labels.append(23)\n",
    "        \n",
    "fake_labels = np.asarray(fake_labels)\n",
    "to_gen = torch.cat((latent_vectors, act_vectors[1], usr_vectors[1]), 1)\n",
    "fake_features = gen(to_gen).detach()\n",
    "#fake_features = gen(to_gen).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 11  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  4  0  0  0  0  0  2  0  0  0  0  0  1  0  0  1]\n",
      " [ 0  0  0  0  0  0 13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  6  0  1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1 10  0  0  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5  0  0  0  0  0]\n",
      " [ 0  2  0  0  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  2]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  9  0  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  1  1 14  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 14  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  7  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0 18  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0]\n",
      " [ 0  1  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.76923   1.00000   0.86957        10\n",
      "           1    0.72727   1.00000   0.84211         8\n",
      "           2    0.84615   0.91667   0.88000        12\n",
      "           3    1.00000   1.00000   1.00000         7\n",
      "           4    0.93333   1.00000   0.96552        14\n",
      "           5    1.00000   0.11111   0.20000         9\n",
      "           6    1.00000   0.92857   0.96296        14\n",
      "           7    1.00000   1.00000   1.00000        10\n",
      "           8    0.00000   0.00000   0.00000         9\n",
      "           9    0.75000   0.85714   0.80000         7\n",
      "          10    0.90909   0.83333   0.86957        12\n",
      "          11    0.47059   1.00000   0.64000         8\n",
      "          12    0.00000   0.00000   0.00000         8\n",
      "          13    0.72727   0.80000   0.76190        10\n",
      "          14    0.00000   0.00000   0.00000         9\n",
      "          15    0.90909   1.00000   0.95238        10\n",
      "          16    0.90000   0.90000   0.90000        10\n",
      "          17    1.00000   0.77778   0.87500        18\n",
      "          18    0.73684   1.00000   0.84848        14\n",
      "          19    1.00000   0.77778   0.87500         9\n",
      "          20    0.69231   0.90000   0.78261        20\n",
      "          21    0.87500   1.00000   0.93333         7\n",
      "          22    1.00000   0.83333   0.90909        12\n",
      "          23    0.58333   1.00000   0.73684         7\n",
      "\n",
      "    accuracy                        0.79528       254\n",
      "   macro avg    0.74290   0.77649   0.73352       254\n",
      "weighted avg    0.76925   0.79528   0.75923       254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5, zero_division = 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
