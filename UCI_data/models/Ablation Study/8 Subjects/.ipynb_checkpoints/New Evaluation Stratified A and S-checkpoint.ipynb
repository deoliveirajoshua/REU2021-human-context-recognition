{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '141 tBodyGyro-iqr()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '434 fBodyGyro-max()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 35),\n",
    "            classifier_block(35, 30),\n",
    "            classifier_block(30, 25),\n",
    "            classifier_block(25, 20),\n",
    "            nn.Linear(20, 21)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "    \n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_10 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_11 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_12 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_13 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_14 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_15 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_16 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_17 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_18 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_19 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_20 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_21 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9, X_10, X_11, X_12, X_13, X_14, X_15, X_16, X_17, X_18, X_19, X_20, X_21))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9) + [9] * len(X_10) + [10] * len(X_11) + [11] * len(X_12) + [12] * len(X_13) + [13] * len(X_14) + [14] * len(X_15) + [15] * len(X_16) + [16] * len(X_17) + [17] * len(X_18) + [18] * len(X_19) + [19] * len(X_20) + [20] * len(X_21)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5, 7, 8, 11, 14]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 12.186753749847412, Final Batch Loss: 3.0528132915496826\n",
      "Epoch 2, Loss: 12.161528587341309, Final Batch Loss: 3.017024040222168\n",
      "Epoch 3, Loss: 12.165661334991455, Final Batch Loss: 3.0325965881347656\n",
      "Epoch 4, Loss: 12.15985894203186, Final Batch Loss: 3.040766716003418\n",
      "Epoch 5, Loss: 12.129819869995117, Final Batch Loss: 3.0135154724121094\n",
      "Epoch 6, Loss: 12.13258171081543, Final Batch Loss: 3.047989845275879\n",
      "Epoch 7, Loss: 12.088533878326416, Final Batch Loss: 2.9951462745666504\n",
      "Epoch 8, Loss: 12.073436260223389, Final Batch Loss: 3.0157346725463867\n",
      "Epoch 9, Loss: 12.010997772216797, Final Batch Loss: 2.9893839359283447\n",
      "Epoch 10, Loss: 11.949265956878662, Final Batch Loss: 2.96512770652771\n",
      "Epoch 11, Loss: 11.88308334350586, Final Batch Loss: 2.9608733654022217\n",
      "Epoch 12, Loss: 11.78440809249878, Final Batch Loss: 2.9408421516418457\n",
      "Epoch 13, Loss: 11.642230749130249, Final Batch Loss: 2.8970816135406494\n",
      "Epoch 14, Loss: 11.498175859451294, Final Batch Loss: 2.8717901706695557\n",
      "Epoch 15, Loss: 11.273573398590088, Final Batch Loss: 2.8117668628692627\n",
      "Epoch 16, Loss: 11.006939172744751, Final Batch Loss: 2.7116403579711914\n",
      "Epoch 17, Loss: 10.77608036994934, Final Batch Loss: 2.6914539337158203\n",
      "Epoch 18, Loss: 10.546778917312622, Final Batch Loss: 2.586177110671997\n",
      "Epoch 19, Loss: 10.284638166427612, Final Batch Loss: 2.5611164569854736\n",
      "Epoch 20, Loss: 10.080183267593384, Final Batch Loss: 2.516594409942627\n",
      "Epoch 21, Loss: 9.922209024429321, Final Batch Loss: 2.406064510345459\n",
      "Epoch 22, Loss: 9.699191808700562, Final Batch Loss: 2.3579742908477783\n",
      "Epoch 23, Loss: 9.473522663116455, Final Batch Loss: 2.3613369464874268\n",
      "Epoch 24, Loss: 9.200968027114868, Final Batch Loss: 2.2250239849090576\n",
      "Epoch 25, Loss: 9.072911500930786, Final Batch Loss: 2.255037307739258\n",
      "Epoch 26, Loss: 8.80472731590271, Final Batch Loss: 2.1023755073547363\n",
      "Epoch 27, Loss: 8.789122819900513, Final Batch Loss: 2.224341869354248\n",
      "Epoch 28, Loss: 8.506670475006104, Final Batch Loss: 2.062736988067627\n",
      "Epoch 29, Loss: 8.274364709854126, Final Batch Loss: 2.037872552871704\n",
      "Epoch 30, Loss: 8.223030805587769, Final Batch Loss: 2.0459516048431396\n",
      "Epoch 31, Loss: 8.000937461853027, Final Batch Loss: 2.006488561630249\n",
      "Epoch 32, Loss: 7.871629238128662, Final Batch Loss: 1.9468035697937012\n",
      "Epoch 33, Loss: 7.829187512397766, Final Batch Loss: 2.0158345699310303\n",
      "Epoch 34, Loss: 7.652217626571655, Final Batch Loss: 1.863168716430664\n",
      "Epoch 35, Loss: 7.422183990478516, Final Batch Loss: 1.8357911109924316\n",
      "Epoch 36, Loss: 7.301915168762207, Final Batch Loss: 1.800727367401123\n",
      "Epoch 37, Loss: 7.227693796157837, Final Batch Loss: 1.8039042949676514\n",
      "Epoch 38, Loss: 7.0587722063064575, Final Batch Loss: 1.6720613241195679\n",
      "Epoch 39, Loss: 7.169927954673767, Final Batch Loss: 1.7533698081970215\n",
      "Epoch 40, Loss: 6.942957639694214, Final Batch Loss: 1.7552450895309448\n",
      "Epoch 41, Loss: 6.743671178817749, Final Batch Loss: 1.6212726831436157\n",
      "Epoch 42, Loss: 6.575930237770081, Final Batch Loss: 1.5785231590270996\n",
      "Epoch 43, Loss: 6.597289562225342, Final Batch Loss: 1.616159200668335\n",
      "Epoch 44, Loss: 6.424506902694702, Final Batch Loss: 1.586330533027649\n",
      "Epoch 45, Loss: 6.499673128128052, Final Batch Loss: 1.780278205871582\n",
      "Epoch 46, Loss: 6.377110004425049, Final Batch Loss: 1.6229581832885742\n",
      "Epoch 47, Loss: 6.415260672569275, Final Batch Loss: 1.6182621717453003\n",
      "Epoch 48, Loss: 6.237126350402832, Final Batch Loss: 1.5027029514312744\n",
      "Epoch 49, Loss: 6.077866196632385, Final Batch Loss: 1.5167815685272217\n",
      "Epoch 50, Loss: 6.253502011299133, Final Batch Loss: 1.6269011497497559\n",
      "Epoch 51, Loss: 6.101486563682556, Final Batch Loss: 1.579111099243164\n",
      "Epoch 52, Loss: 6.007264614105225, Final Batch Loss: 1.4114638566970825\n",
      "Epoch 53, Loss: 5.824334740638733, Final Batch Loss: 1.4802067279815674\n",
      "Epoch 54, Loss: 5.898069858551025, Final Batch Loss: 1.4694207906723022\n",
      "Epoch 55, Loss: 5.851939678192139, Final Batch Loss: 1.512139081954956\n",
      "Epoch 56, Loss: 5.875349164009094, Final Batch Loss: 1.4128210544586182\n",
      "Epoch 57, Loss: 5.761575102806091, Final Batch Loss: 1.4184393882751465\n",
      "Epoch 58, Loss: 5.6798175573349, Final Batch Loss: 1.4314123392105103\n",
      "Epoch 59, Loss: 5.821550965309143, Final Batch Loss: 1.4870691299438477\n",
      "Epoch 60, Loss: 5.722474098205566, Final Batch Loss: 1.4912922382354736\n",
      "Epoch 61, Loss: 5.712352395057678, Final Batch Loss: 1.4387301206588745\n",
      "Epoch 62, Loss: 5.506861209869385, Final Batch Loss: 1.318999171257019\n",
      "Epoch 63, Loss: 5.566890239715576, Final Batch Loss: 1.3403372764587402\n",
      "Epoch 64, Loss: 5.604599833488464, Final Batch Loss: 1.451942801475525\n",
      "Epoch 65, Loss: 5.504500985145569, Final Batch Loss: 1.439326286315918\n",
      "Epoch 66, Loss: 5.619096398353577, Final Batch Loss: 1.4136146306991577\n",
      "Epoch 67, Loss: 5.521020889282227, Final Batch Loss: 1.4256328344345093\n",
      "Epoch 68, Loss: 5.6023736000061035, Final Batch Loss: 1.479824185371399\n",
      "Epoch 69, Loss: 5.432562828063965, Final Batch Loss: 1.3427581787109375\n",
      "Epoch 70, Loss: 5.501317262649536, Final Batch Loss: 1.4152672290802002\n",
      "Epoch 71, Loss: 5.332445740699768, Final Batch Loss: 1.4820321798324585\n",
      "Epoch 72, Loss: 5.438037395477295, Final Batch Loss: 1.401525616645813\n",
      "Epoch 73, Loss: 5.3057756423950195, Final Batch Loss: 1.2618975639343262\n",
      "Epoch 74, Loss: 5.377467036247253, Final Batch Loss: 1.42047119140625\n",
      "Epoch 75, Loss: 5.392327547073364, Final Batch Loss: 1.3267723321914673\n",
      "Epoch 76, Loss: 5.048457860946655, Final Batch Loss: 1.0477147102355957\n",
      "Epoch 77, Loss: 5.437475562095642, Final Batch Loss: 1.4833400249481201\n",
      "Epoch 78, Loss: 5.283485054969788, Final Batch Loss: 1.2825927734375\n",
      "Epoch 79, Loss: 5.3305171728134155, Final Batch Loss: 1.4064964056015015\n",
      "Epoch 80, Loss: 5.1927649974823, Final Batch Loss: 1.2783304452896118\n",
      "Epoch 81, Loss: 5.158098101615906, Final Batch Loss: 1.2716556787490845\n",
      "Epoch 82, Loss: 4.989309072494507, Final Batch Loss: 1.274757742881775\n",
      "Epoch 83, Loss: 5.158661842346191, Final Batch Loss: 1.3077641725540161\n",
      "Epoch 84, Loss: 4.973996996879578, Final Batch Loss: 1.235489010810852\n",
      "Epoch 85, Loss: 5.043847680091858, Final Batch Loss: 1.1995364427566528\n",
      "Epoch 86, Loss: 5.085625648498535, Final Batch Loss: 1.2848999500274658\n",
      "Epoch 87, Loss: 5.004834890365601, Final Batch Loss: 1.1799944639205933\n",
      "Epoch 88, Loss: 4.82886803150177, Final Batch Loss: 1.1651793718338013\n",
      "Epoch 89, Loss: 5.140288591384888, Final Batch Loss: 1.2660868167877197\n",
      "Epoch 90, Loss: 5.073002219200134, Final Batch Loss: 1.3131753206253052\n",
      "Epoch 91, Loss: 5.069514155387878, Final Batch Loss: 1.2783156633377075\n",
      "Epoch 92, Loss: 5.046123743057251, Final Batch Loss: 1.1646199226379395\n",
      "Epoch 93, Loss: 5.070080280303955, Final Batch Loss: 1.3661320209503174\n",
      "Epoch 94, Loss: 4.947944760322571, Final Batch Loss: 1.2440273761749268\n",
      "Epoch 95, Loss: 4.813893675804138, Final Batch Loss: 1.2081246376037598\n",
      "Epoch 96, Loss: 4.898200154304504, Final Batch Loss: 1.2555572986602783\n",
      "Epoch 97, Loss: 4.842925786972046, Final Batch Loss: 1.0758527517318726\n",
      "Epoch 98, Loss: 4.96667218208313, Final Batch Loss: 1.2591673135757446\n",
      "Epoch 99, Loss: 4.906247615814209, Final Batch Loss: 1.1885422468185425\n",
      "Epoch 100, Loss: 4.6803354024887085, Final Batch Loss: 1.0342943668365479\n",
      "Epoch 101, Loss: 4.85865318775177, Final Batch Loss: 1.2375420331954956\n",
      "Epoch 102, Loss: 4.75226354598999, Final Batch Loss: 1.2091509103775024\n",
      "Epoch 103, Loss: 4.729154586791992, Final Batch Loss: 1.1094272136688232\n",
      "Epoch 104, Loss: 4.823008298873901, Final Batch Loss: 1.197859764099121\n",
      "Epoch 105, Loss: 4.548589646816254, Final Batch Loss: 0.9375483393669128\n",
      "Epoch 106, Loss: 4.749301314353943, Final Batch Loss: 1.211029052734375\n",
      "Epoch 107, Loss: 4.834417343139648, Final Batch Loss: 1.2909473180770874\n",
      "Epoch 108, Loss: 4.7439234256744385, Final Batch Loss: 1.2102670669555664\n",
      "Epoch 109, Loss: 4.579683244228363, Final Batch Loss: 0.9936855435371399\n",
      "Epoch 110, Loss: 4.684494614601135, Final Batch Loss: 1.2944979667663574\n",
      "Epoch 111, Loss: 4.538169503211975, Final Batch Loss: 1.0961284637451172\n",
      "Epoch 112, Loss: 4.618782162666321, Final Batch Loss: 1.1727279424667358\n",
      "Epoch 113, Loss: 4.734121799468994, Final Batch Loss: 1.2192091941833496\n",
      "Epoch 114, Loss: 4.5202168226242065, Final Batch Loss: 1.0924835205078125\n",
      "Epoch 115, Loss: 4.546866774559021, Final Batch Loss: 1.1697763204574585\n",
      "Epoch 116, Loss: 4.608847737312317, Final Batch Loss: 1.153295874595642\n",
      "Epoch 117, Loss: 4.597411036491394, Final Batch Loss: 1.2111332416534424\n",
      "Epoch 118, Loss: 4.418697476387024, Final Batch Loss: 1.072444200515747\n",
      "Epoch 119, Loss: 4.574202060699463, Final Batch Loss: 1.177344560623169\n",
      "Epoch 120, Loss: 4.358392119407654, Final Batch Loss: 1.0993062257766724\n",
      "Epoch 121, Loss: 4.4460673332214355, Final Batch Loss: 1.058449387550354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122, Loss: 4.425016641616821, Final Batch Loss: 1.021633267402649\n",
      "Epoch 123, Loss: 4.521065711975098, Final Batch Loss: 1.0679272413253784\n",
      "Epoch 124, Loss: 4.35047721862793, Final Batch Loss: 1.0665427446365356\n",
      "Epoch 125, Loss: 4.4548410177230835, Final Batch Loss: 1.2075109481811523\n",
      "Epoch 126, Loss: 4.318204164505005, Final Batch Loss: 1.0428038835525513\n",
      "Epoch 127, Loss: 4.464916467666626, Final Batch Loss: 1.1281663179397583\n",
      "Epoch 128, Loss: 4.211661994457245, Final Batch Loss: 0.9508123993873596\n",
      "Epoch 129, Loss: 4.437358856201172, Final Batch Loss: 1.0165241956710815\n",
      "Epoch 130, Loss: 4.312524318695068, Final Batch Loss: 1.1484007835388184\n",
      "Epoch 131, Loss: 4.3065043687820435, Final Batch Loss: 1.0081368684768677\n",
      "Epoch 132, Loss: 4.352254688739777, Final Batch Loss: 1.2361794710159302\n",
      "Epoch 133, Loss: 4.251407861709595, Final Batch Loss: 0.991828203201294\n",
      "Epoch 134, Loss: 4.264559388160706, Final Batch Loss: 1.0668238401412964\n",
      "Epoch 135, Loss: 4.0402752161026, Final Batch Loss: 0.869335412979126\n",
      "Epoch 136, Loss: 4.264981150627136, Final Batch Loss: 0.979418158531189\n",
      "Epoch 137, Loss: 4.1210527420043945, Final Batch Loss: 1.0428404808044434\n",
      "Epoch 138, Loss: 4.148008465766907, Final Batch Loss: 0.9958357810974121\n",
      "Epoch 139, Loss: 4.064856112003326, Final Batch Loss: 0.8638041019439697\n",
      "Epoch 140, Loss: 4.25697386264801, Final Batch Loss: 1.1617686748504639\n",
      "Epoch 141, Loss: 4.324320316314697, Final Batch Loss: 1.0663694143295288\n",
      "Epoch 142, Loss: 4.057676911354065, Final Batch Loss: 0.952068567276001\n",
      "Epoch 143, Loss: 4.137166440486908, Final Batch Loss: 1.031173825263977\n",
      "Epoch 144, Loss: 4.06753808259964, Final Batch Loss: 0.9186534285545349\n",
      "Epoch 145, Loss: 4.107076346874237, Final Batch Loss: 0.9530255198478699\n",
      "Epoch 146, Loss: 4.103515803813934, Final Batch Loss: 0.9747181534767151\n",
      "Epoch 147, Loss: 4.141595542430878, Final Batch Loss: 0.9985169768333435\n",
      "Epoch 148, Loss: 4.09629213809967, Final Batch Loss: 1.012229561805725\n",
      "Epoch 149, Loss: 4.223255157470703, Final Batch Loss: 1.1499435901641846\n",
      "Epoch 150, Loss: 4.112811803817749, Final Batch Loss: 0.9073102474212646\n",
      "Epoch 151, Loss: 4.00641530752182, Final Batch Loss: 0.9724185466766357\n",
      "Epoch 152, Loss: 4.224149167537689, Final Batch Loss: 1.1125013828277588\n",
      "Epoch 153, Loss: 3.9533079862594604, Final Batch Loss: 0.8412838578224182\n",
      "Epoch 154, Loss: 4.13102251291275, Final Batch Loss: 0.9405880570411682\n",
      "Epoch 155, Loss: 4.054369270801544, Final Batch Loss: 0.9862354397773743\n",
      "Epoch 156, Loss: 4.14272677898407, Final Batch Loss: 1.1643667221069336\n",
      "Epoch 157, Loss: 4.066018104553223, Final Batch Loss: 1.098777413368225\n",
      "Epoch 158, Loss: 4.13538658618927, Final Batch Loss: 1.0933520793914795\n",
      "Epoch 159, Loss: 3.9991668462753296, Final Batch Loss: 1.0097159147262573\n",
      "Epoch 160, Loss: 3.987992286682129, Final Batch Loss: 1.044703722000122\n",
      "Epoch 161, Loss: 4.0212522149086, Final Batch Loss: 1.1198081970214844\n",
      "Epoch 162, Loss: 3.971243143081665, Final Batch Loss: 1.0006810426712036\n",
      "Epoch 163, Loss: 4.0748754143714905, Final Batch Loss: 1.1286355257034302\n",
      "Epoch 164, Loss: 4.044920325279236, Final Batch Loss: 1.0855077505111694\n",
      "Epoch 165, Loss: 4.148078620433807, Final Batch Loss: 1.0421216487884521\n",
      "Epoch 166, Loss: 4.049280107021332, Final Batch Loss: 1.125983715057373\n",
      "Epoch 167, Loss: 3.951606571674347, Final Batch Loss: 1.0539222955703735\n",
      "Epoch 168, Loss: 4.026391446590424, Final Batch Loss: 1.0604572296142578\n",
      "Epoch 169, Loss: 3.948567807674408, Final Batch Loss: 1.0087801218032837\n",
      "Epoch 170, Loss: 4.0476935505867, Final Batch Loss: 0.965349555015564\n",
      "Epoch 171, Loss: 4.123113453388214, Final Batch Loss: 1.1010829210281372\n",
      "Epoch 172, Loss: 3.942093074321747, Final Batch Loss: 1.0472393035888672\n",
      "Epoch 173, Loss: 3.8578285574913025, Final Batch Loss: 0.8848082423210144\n",
      "Epoch 174, Loss: 3.8537782430648804, Final Batch Loss: 1.0242210626602173\n",
      "Epoch 175, Loss: 3.7719780802726746, Final Batch Loss: 0.8544526100158691\n",
      "Epoch 176, Loss: 3.966963291168213, Final Batch Loss: 1.0391207933425903\n",
      "Epoch 177, Loss: 3.6371126770973206, Final Batch Loss: 0.7767922878265381\n",
      "Epoch 178, Loss: 3.740708351135254, Final Batch Loss: 0.9008116722106934\n",
      "Epoch 179, Loss: 3.8898751735687256, Final Batch Loss: 0.9551024436950684\n",
      "Epoch 180, Loss: 3.9315744638442993, Final Batch Loss: 0.8845447897911072\n",
      "Epoch 181, Loss: 3.855791211128235, Final Batch Loss: 0.944972574710846\n",
      "Epoch 182, Loss: 3.853059947490692, Final Batch Loss: 0.9215066432952881\n",
      "Epoch 183, Loss: 4.022253096103668, Final Batch Loss: 0.9575774073600769\n",
      "Epoch 184, Loss: 3.830378472805023, Final Batch Loss: 0.9813588857650757\n",
      "Epoch 185, Loss: 3.866847336292267, Final Batch Loss: 0.961648166179657\n",
      "Epoch 186, Loss: 3.8571460843086243, Final Batch Loss: 1.0315260887145996\n",
      "Epoch 187, Loss: 3.7886723279953003, Final Batch Loss: 0.9788727760314941\n",
      "Epoch 188, Loss: 3.889679968357086, Final Batch Loss: 0.9690550565719604\n",
      "Epoch 189, Loss: 3.7881980538368225, Final Batch Loss: 0.8392186760902405\n",
      "Epoch 190, Loss: 3.7774769067764282, Final Batch Loss: 0.8268885016441345\n",
      "Epoch 191, Loss: 3.913796842098236, Final Batch Loss: 0.9359995722770691\n",
      "Epoch 192, Loss: 3.7534573078155518, Final Batch Loss: 0.9631412029266357\n",
      "Epoch 193, Loss: 3.738164961338043, Final Batch Loss: 0.9668541550636292\n",
      "Epoch 194, Loss: 3.9380746483802795, Final Batch Loss: 1.0542516708374023\n",
      "Epoch 195, Loss: 3.74185049533844, Final Batch Loss: 0.7412359714508057\n",
      "Epoch 196, Loss: 3.7762086987495422, Final Batch Loss: 0.9557310342788696\n",
      "Epoch 197, Loss: 3.7072426080703735, Final Batch Loss: 0.8393857479095459\n",
      "Epoch 198, Loss: 3.816167712211609, Final Batch Loss: 0.9535750150680542\n",
      "Epoch 199, Loss: 3.9994657039642334, Final Batch Loss: 1.1005356311798096\n",
      "Epoch 200, Loss: 3.730475604534149, Final Batch Loss: 0.9417372941970825\n",
      "Epoch 201, Loss: 3.830850660800934, Final Batch Loss: 1.0388280153274536\n",
      "Epoch 202, Loss: 3.8251174688339233, Final Batch Loss: 0.9891008734703064\n",
      "Epoch 203, Loss: 3.850299060344696, Final Batch Loss: 1.0376170873641968\n",
      "Epoch 204, Loss: 3.7407308220863342, Final Batch Loss: 0.9685394167900085\n",
      "Epoch 205, Loss: 3.681069076061249, Final Batch Loss: 0.9211626648902893\n",
      "Epoch 206, Loss: 3.6921586394309998, Final Batch Loss: 0.8544030785560608\n",
      "Epoch 207, Loss: 3.8003710508346558, Final Batch Loss: 0.8494442105293274\n",
      "Epoch 208, Loss: 3.6988444328308105, Final Batch Loss: 0.8989056944847107\n",
      "Epoch 209, Loss: 3.7360661029815674, Final Batch Loss: 0.8776206374168396\n",
      "Epoch 210, Loss: 3.559214472770691, Final Batch Loss: 0.8580901622772217\n",
      "Epoch 211, Loss: 3.750738024711609, Final Batch Loss: 0.9797046184539795\n",
      "Epoch 212, Loss: 3.6898341178894043, Final Batch Loss: 1.0065741539001465\n",
      "Epoch 213, Loss: 3.635970890522003, Final Batch Loss: 0.8680026531219482\n",
      "Epoch 214, Loss: 3.5309593081474304, Final Batch Loss: 0.747460663318634\n",
      "Epoch 215, Loss: 3.6679447889328003, Final Batch Loss: 0.9205610752105713\n",
      "Epoch 216, Loss: 3.8437156081199646, Final Batch Loss: 1.019463300704956\n",
      "Epoch 217, Loss: 3.702322006225586, Final Batch Loss: 0.9605263471603394\n",
      "Epoch 218, Loss: 3.7476519346237183, Final Batch Loss: 0.9410569071769714\n",
      "Epoch 219, Loss: 3.6010705828666687, Final Batch Loss: 0.9548349976539612\n",
      "Epoch 220, Loss: 3.7570009231567383, Final Batch Loss: 0.9045168161392212\n",
      "Epoch 221, Loss: 3.6553430557250977, Final Batch Loss: 0.815759003162384\n",
      "Epoch 222, Loss: 3.622592866420746, Final Batch Loss: 0.8469118475914001\n",
      "Epoch 223, Loss: 3.5107277035713196, Final Batch Loss: 0.8580520153045654\n",
      "Epoch 224, Loss: 3.5584681034088135, Final Batch Loss: 0.8612217307090759\n",
      "Epoch 225, Loss: 3.5841139554977417, Final Batch Loss: 0.8246941566467285\n",
      "Epoch 226, Loss: 3.6700019240379333, Final Batch Loss: 0.9374350905418396\n",
      "Epoch 227, Loss: 3.5963826179504395, Final Batch Loss: 0.8968502879142761\n",
      "Epoch 228, Loss: 3.69416207075119, Final Batch Loss: 0.8887069225311279\n",
      "Epoch 229, Loss: 3.5522415041923523, Final Batch Loss: 0.86524498462677\n",
      "Epoch 230, Loss: 3.558234930038452, Final Batch Loss: 0.933332085609436\n",
      "Epoch 231, Loss: 3.5622262954711914, Final Batch Loss: 0.7868901491165161\n",
      "Epoch 232, Loss: 3.5950111746788025, Final Batch Loss: 0.9025453329086304\n",
      "Epoch 233, Loss: 3.5674437284469604, Final Batch Loss: 0.8925031423568726\n",
      "Epoch 234, Loss: 3.698190748691559, Final Batch Loss: 0.9355871677398682\n",
      "Epoch 235, Loss: 3.7323386669158936, Final Batch Loss: 1.022714614868164\n",
      "Epoch 236, Loss: 3.568752884864807, Final Batch Loss: 0.8607991933822632\n",
      "Epoch 237, Loss: 3.5611602663993835, Final Batch Loss: 0.9547286629676819\n",
      "Epoch 238, Loss: 3.593965768814087, Final Batch Loss: 0.9140015840530396\n",
      "Epoch 239, Loss: 3.6023149490356445, Final Batch Loss: 0.9587345719337463\n",
      "Epoch 240, Loss: 3.457001566886902, Final Batch Loss: 0.9141988158226013\n",
      "Epoch 241, Loss: 3.761896252632141, Final Batch Loss: 1.0103100538253784\n",
      "Epoch 242, Loss: 3.589737892150879, Final Batch Loss: 0.8634589910507202\n",
      "Epoch 243, Loss: 3.4781564474105835, Final Batch Loss: 0.8708575367927551\n",
      "Epoch 244, Loss: 3.5627979040145874, Final Batch Loss: 0.830649733543396\n",
      "Epoch 245, Loss: 3.594745635986328, Final Batch Loss: 0.8495441675186157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246, Loss: 3.529292345046997, Final Batch Loss: 0.9485269784927368\n",
      "Epoch 247, Loss: 3.6230539679527283, Final Batch Loss: 0.850766658782959\n",
      "Epoch 248, Loss: 3.6101056933403015, Final Batch Loss: 0.923913300037384\n",
      "Epoch 249, Loss: 3.5377028584480286, Final Batch Loss: 0.8838250637054443\n",
      "Epoch 250, Loss: 3.5554428696632385, Final Batch Loss: 0.8328309059143066\n",
      "Epoch 251, Loss: 3.527076244354248, Final Batch Loss: 0.9048000574111938\n",
      "Epoch 252, Loss: 3.550906538963318, Final Batch Loss: 0.9055417776107788\n",
      "Epoch 253, Loss: 3.5602278113365173, Final Batch Loss: 0.8199220299720764\n",
      "Epoch 254, Loss: 3.5499463081359863, Final Batch Loss: 0.9067045450210571\n",
      "Epoch 255, Loss: 3.440363883972168, Final Batch Loss: 0.8542454838752747\n",
      "Epoch 256, Loss: 3.5893877744674683, Final Batch Loss: 0.8296941518783569\n",
      "Epoch 257, Loss: 3.487176775932312, Final Batch Loss: 0.8258385062217712\n",
      "Epoch 258, Loss: 3.444176197052002, Final Batch Loss: 0.9115859866142273\n",
      "Epoch 259, Loss: 3.6469786763191223, Final Batch Loss: 0.9820045828819275\n",
      "Epoch 260, Loss: 3.474886953830719, Final Batch Loss: 0.8464418649673462\n",
      "Epoch 261, Loss: 3.441366493701935, Final Batch Loss: 0.9530238509178162\n",
      "Epoch 262, Loss: 3.436825752258301, Final Batch Loss: 0.8750777840614319\n",
      "Epoch 263, Loss: 3.4914355278015137, Final Batch Loss: 0.8779230117797852\n",
      "Epoch 264, Loss: 3.5439118146896362, Final Batch Loss: 0.9056389927864075\n",
      "Epoch 265, Loss: 3.4108477234840393, Final Batch Loss: 0.9021182060241699\n",
      "Epoch 266, Loss: 3.5129188299179077, Final Batch Loss: 0.9364395141601562\n",
      "Epoch 267, Loss: 3.528478741645813, Final Batch Loss: 0.9091941118240356\n",
      "Epoch 268, Loss: 3.428411066532135, Final Batch Loss: 0.8076755404472351\n",
      "Epoch 269, Loss: 3.4787803888320923, Final Batch Loss: 0.8132961392402649\n",
      "Epoch 270, Loss: 3.451338529586792, Final Batch Loss: 0.9507243633270264\n",
      "Epoch 271, Loss: 3.3534069061279297, Final Batch Loss: 0.7281012535095215\n",
      "Epoch 272, Loss: 3.3861199617385864, Final Batch Loss: 0.8420374989509583\n",
      "Epoch 273, Loss: 3.340560555458069, Final Batch Loss: 0.8395829200744629\n",
      "Epoch 274, Loss: 3.322741150856018, Final Batch Loss: 0.8002782464027405\n",
      "Epoch 275, Loss: 3.513343334197998, Final Batch Loss: 0.9698286652565002\n",
      "Epoch 276, Loss: 3.3744122982025146, Final Batch Loss: 0.7170856595039368\n",
      "Epoch 277, Loss: 3.3017032742500305, Final Batch Loss: 0.8028676509857178\n",
      "Epoch 278, Loss: 3.500265896320343, Final Batch Loss: 0.8650495409965515\n",
      "Epoch 279, Loss: 3.3536415696144104, Final Batch Loss: 0.8705235719680786\n",
      "Epoch 280, Loss: 3.4599713683128357, Final Batch Loss: 0.8757249116897583\n",
      "Epoch 281, Loss: 3.4588142037391663, Final Batch Loss: 0.9588621258735657\n",
      "Epoch 282, Loss: 3.5201860070228577, Final Batch Loss: 1.014962077140808\n",
      "Epoch 283, Loss: 3.2330484986305237, Final Batch Loss: 0.7525815963745117\n",
      "Epoch 284, Loss: 3.437847673892975, Final Batch Loss: 0.804585874080658\n",
      "Epoch 285, Loss: 3.368813991546631, Final Batch Loss: 0.9163813591003418\n",
      "Epoch 286, Loss: 3.3806172609329224, Final Batch Loss: 0.8035983443260193\n",
      "Epoch 287, Loss: 3.205298960208893, Final Batch Loss: 0.7168954014778137\n",
      "Epoch 288, Loss: 3.479039251804352, Final Batch Loss: 0.9463645219802856\n",
      "Epoch 289, Loss: 3.4688188433647156, Final Batch Loss: 0.8910290002822876\n",
      "Epoch 290, Loss: 3.4710548520088196, Final Batch Loss: 0.8873714208602905\n",
      "Epoch 291, Loss: 3.408400058746338, Final Batch Loss: 0.8297103047370911\n",
      "Epoch 292, Loss: 3.4665802717208862, Final Batch Loss: 0.9218763709068298\n",
      "Epoch 293, Loss: 3.3961893916130066, Final Batch Loss: 0.8545816540718079\n",
      "Epoch 294, Loss: 3.385860562324524, Final Batch Loss: 0.9258109331130981\n",
      "Epoch 295, Loss: 3.4297860860824585, Final Batch Loss: 0.8359146118164062\n",
      "Epoch 296, Loss: 3.3320656418800354, Final Batch Loss: 0.925727128982544\n",
      "Epoch 297, Loss: 3.4988011717796326, Final Batch Loss: 0.9856849312782288\n",
      "Epoch 298, Loss: 3.3144419193267822, Final Batch Loss: 0.7475439310073853\n",
      "Epoch 299, Loss: 3.356406331062317, Final Batch Loss: 0.8666112422943115\n",
      "Epoch 300, Loss: 3.3070203065872192, Final Batch Loss: 0.8240065574645996\n",
      "Epoch 301, Loss: 3.3398898243904114, Final Batch Loss: 0.881797194480896\n",
      "Epoch 302, Loss: 3.3694024682044983, Final Batch Loss: 0.8892031311988831\n",
      "Epoch 303, Loss: 3.433265447616577, Final Batch Loss: 1.0297157764434814\n",
      "Epoch 304, Loss: 3.3288521766662598, Final Batch Loss: 0.7965033650398254\n",
      "Epoch 305, Loss: 3.3145712018013, Final Batch Loss: 0.8506277203559875\n",
      "Epoch 306, Loss: 3.2165130972862244, Final Batch Loss: 0.7720841765403748\n",
      "Epoch 307, Loss: 3.3104584217071533, Final Batch Loss: 0.8023439049720764\n",
      "Epoch 308, Loss: 3.342263162136078, Final Batch Loss: 0.7874854207038879\n",
      "Epoch 309, Loss: 3.4179911017417908, Final Batch Loss: 0.7899765968322754\n",
      "Epoch 310, Loss: 3.379482090473175, Final Batch Loss: 0.8252283334732056\n",
      "Epoch 311, Loss: 3.195177972316742, Final Batch Loss: 0.7618457078933716\n",
      "Epoch 312, Loss: 3.2252413034439087, Final Batch Loss: 0.849898636341095\n",
      "Epoch 313, Loss: 3.172569453716278, Final Batch Loss: 0.7455106377601624\n",
      "Epoch 314, Loss: 3.3500574827194214, Final Batch Loss: 0.9001976251602173\n",
      "Epoch 315, Loss: 3.3253544569015503, Final Batch Loss: 0.7863557934761047\n",
      "Epoch 316, Loss: 3.3484540581703186, Final Batch Loss: 0.8243285417556763\n",
      "Epoch 317, Loss: 3.276198387145996, Final Batch Loss: 0.7386407852172852\n",
      "Epoch 318, Loss: 3.325364828109741, Final Batch Loss: 0.8602737188339233\n",
      "Epoch 319, Loss: 3.2791303396224976, Final Batch Loss: 0.7968038320541382\n",
      "Epoch 320, Loss: 3.335062086582184, Final Batch Loss: 0.7595508098602295\n",
      "Epoch 321, Loss: 3.2957969307899475, Final Batch Loss: 0.7549151182174683\n",
      "Epoch 322, Loss: 3.3489538431167603, Final Batch Loss: 0.9273420572280884\n",
      "Epoch 323, Loss: 3.1865161657333374, Final Batch Loss: 0.7688406705856323\n",
      "Epoch 324, Loss: 3.346266806125641, Final Batch Loss: 0.8651677370071411\n",
      "Epoch 325, Loss: 3.140443503856659, Final Batch Loss: 0.705413818359375\n",
      "Epoch 326, Loss: 3.3352519273757935, Final Batch Loss: 0.8870141506195068\n",
      "Epoch 327, Loss: 3.1866649389266968, Final Batch Loss: 0.7298890948295593\n",
      "Epoch 328, Loss: 3.270804226398468, Final Batch Loss: 0.8800078630447388\n",
      "Epoch 329, Loss: 3.292856752872467, Final Batch Loss: 0.8610858917236328\n",
      "Epoch 330, Loss: 3.0427178144454956, Final Batch Loss: 0.8314805030822754\n",
      "Epoch 331, Loss: 3.2299956679344177, Final Batch Loss: 0.7536301612854004\n",
      "Epoch 332, Loss: 3.2119007110595703, Final Batch Loss: 0.7595003843307495\n",
      "Epoch 333, Loss: 3.3218787908554077, Final Batch Loss: 0.8273854851722717\n",
      "Epoch 334, Loss: 3.1264540553092957, Final Batch Loss: 0.7480542659759521\n",
      "Epoch 335, Loss: 3.141974449157715, Final Batch Loss: 0.763914942741394\n",
      "Epoch 336, Loss: 3.2707846760749817, Final Batch Loss: 0.8254473805427551\n",
      "Epoch 337, Loss: 3.2004007697105408, Final Batch Loss: 0.7492108941078186\n",
      "Epoch 338, Loss: 3.2214009165763855, Final Batch Loss: 0.7226004600524902\n",
      "Epoch 339, Loss: 3.1226736307144165, Final Batch Loss: 0.7908248901367188\n",
      "Epoch 340, Loss: 3.3538286685943604, Final Batch Loss: 0.9794573187828064\n",
      "Epoch 341, Loss: 3.249552547931671, Final Batch Loss: 0.8169524669647217\n",
      "Epoch 342, Loss: 3.3084762692451477, Final Batch Loss: 0.8675443530082703\n",
      "Epoch 343, Loss: 3.272399425506592, Final Batch Loss: 0.8681281805038452\n",
      "Epoch 344, Loss: 3.3098828196525574, Final Batch Loss: 0.8378769755363464\n",
      "Epoch 345, Loss: 3.2434087991714478, Final Batch Loss: 0.8820445537567139\n",
      "Epoch 346, Loss: 3.2145314812660217, Final Batch Loss: 0.8149467706680298\n",
      "Epoch 347, Loss: 3.0760123133659363, Final Batch Loss: 0.7993825674057007\n",
      "Epoch 348, Loss: 3.2097513675689697, Final Batch Loss: 0.7320945262908936\n",
      "Epoch 349, Loss: 3.2031668424606323, Final Batch Loss: 0.6831145882606506\n",
      "Epoch 350, Loss: 3.170211970806122, Final Batch Loss: 0.8551119565963745\n",
      "Epoch 351, Loss: 3.237190365791321, Final Batch Loss: 0.9381961226463318\n",
      "Epoch 352, Loss: 3.288516342639923, Final Batch Loss: 0.77138751745224\n",
      "Epoch 353, Loss: 3.2508201003074646, Final Batch Loss: 0.887800395488739\n",
      "Epoch 354, Loss: 3.242728352546692, Final Batch Loss: 0.7859270572662354\n",
      "Epoch 355, Loss: 3.212390184402466, Final Batch Loss: 0.8545219302177429\n",
      "Epoch 356, Loss: 3.1996222138404846, Final Batch Loss: 0.7452747225761414\n",
      "Epoch 357, Loss: 3.253861904144287, Final Batch Loss: 0.8694786429405212\n",
      "Epoch 358, Loss: 3.110556960105896, Final Batch Loss: 0.7711517810821533\n",
      "Epoch 359, Loss: 3.1832895278930664, Final Batch Loss: 0.7592880129814148\n",
      "Epoch 360, Loss: 3.0927639603614807, Final Batch Loss: 0.7497597932815552\n",
      "Epoch 361, Loss: 3.114034354686737, Final Batch Loss: 0.743830144405365\n",
      "Epoch 362, Loss: 3.072193682193756, Final Batch Loss: 0.7590815424919128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363, Loss: 3.0693310499191284, Final Batch Loss: 0.8147198557853699\n",
      "Epoch 364, Loss: 3.1644861102104187, Final Batch Loss: 0.7435886263847351\n",
      "Epoch 365, Loss: 3.142872929573059, Final Batch Loss: 0.7614025473594666\n",
      "Epoch 366, Loss: 3.1647929549217224, Final Batch Loss: 0.7001302242279053\n",
      "Epoch 367, Loss: 3.089626669883728, Final Batch Loss: 0.6782196164131165\n",
      "Epoch 368, Loss: 3.1822999715805054, Final Batch Loss: 0.7408865690231323\n",
      "Epoch 369, Loss: 3.06778883934021, Final Batch Loss: 0.733695924282074\n",
      "Epoch 370, Loss: 3.13156521320343, Final Batch Loss: 0.8301035165786743\n",
      "Epoch 371, Loss: 3.0075035095214844, Final Batch Loss: 0.813162088394165\n",
      "Epoch 372, Loss: 3.0112330317497253, Final Batch Loss: 0.663661777973175\n",
      "Epoch 373, Loss: 3.1231667399406433, Final Batch Loss: 0.7758026719093323\n",
      "Epoch 374, Loss: 3.1016451716423035, Final Batch Loss: 0.7516869902610779\n",
      "Epoch 375, Loss: 3.1306377053260803, Final Batch Loss: 0.7967182993888855\n",
      "Epoch 376, Loss: 3.0667293667793274, Final Batch Loss: 0.7370764017105103\n",
      "Epoch 377, Loss: 3.1246241331100464, Final Batch Loss: 0.7971333265304565\n",
      "Epoch 378, Loss: 3.0483381748199463, Final Batch Loss: 0.730624794960022\n",
      "Epoch 379, Loss: 3.0817195773124695, Final Batch Loss: 0.7808924317359924\n",
      "Epoch 380, Loss: 2.942870616912842, Final Batch Loss: 0.6903491616249084\n",
      "Epoch 381, Loss: 3.104840576648712, Final Batch Loss: 0.7502167820930481\n",
      "Epoch 382, Loss: 3.180092751979828, Final Batch Loss: 0.903592586517334\n",
      "Epoch 383, Loss: 2.932157516479492, Final Batch Loss: 0.6879826188087463\n",
      "Epoch 384, Loss: 3.101947009563446, Final Batch Loss: 0.8167851567268372\n",
      "Epoch 385, Loss: 3.004616439342499, Final Batch Loss: 0.7597985863685608\n",
      "Epoch 386, Loss: 3.133481740951538, Final Batch Loss: 0.9468588829040527\n",
      "Epoch 387, Loss: 3.1252973675727844, Final Batch Loss: 0.7970501780509949\n",
      "Epoch 388, Loss: 3.0269811749458313, Final Batch Loss: 0.7195449471473694\n",
      "Epoch 389, Loss: 3.134542465209961, Final Batch Loss: 0.8882836699485779\n",
      "Epoch 390, Loss: 3.0564699172973633, Final Batch Loss: 0.7346469759941101\n",
      "Epoch 391, Loss: 3.0189351439476013, Final Batch Loss: 0.718359112739563\n",
      "Epoch 392, Loss: 3.03744900226593, Final Batch Loss: 0.7903741598129272\n",
      "Epoch 393, Loss: 2.991181194782257, Final Batch Loss: 0.7513799071311951\n",
      "Epoch 394, Loss: 3.0472300052642822, Final Batch Loss: 0.8402300477027893\n",
      "Epoch 395, Loss: 2.854503870010376, Final Batch Loss: 0.6453498005867004\n",
      "Epoch 396, Loss: 3.0179017782211304, Final Batch Loss: 0.6223533153533936\n",
      "Epoch 397, Loss: 2.922763168811798, Final Batch Loss: 0.6644971966743469\n",
      "Epoch 398, Loss: 3.0896688103675842, Final Batch Loss: 0.7535249590873718\n",
      "Epoch 399, Loss: 2.9363596439361572, Final Batch Loss: 0.6353744268417358\n",
      "Epoch 400, Loss: 2.9408716559410095, Final Batch Loss: 0.8082652688026428\n",
      "Epoch 401, Loss: 3.1511682271957397, Final Batch Loss: 0.908146321773529\n",
      "Epoch 402, Loss: 2.947014272212982, Final Batch Loss: 0.80393385887146\n",
      "Epoch 403, Loss: 3.0889458060264587, Final Batch Loss: 0.7552139759063721\n",
      "Epoch 404, Loss: 2.9807648062705994, Final Batch Loss: 0.7203826308250427\n",
      "Epoch 405, Loss: 2.9340649247169495, Final Batch Loss: 0.656461775302887\n",
      "Epoch 406, Loss: 3.0723661184310913, Final Batch Loss: 0.843245804309845\n",
      "Epoch 407, Loss: 2.9420406222343445, Final Batch Loss: 0.715732753276825\n",
      "Epoch 408, Loss: 3.0393755435943604, Final Batch Loss: 0.7965384721755981\n",
      "Epoch 409, Loss: 3.0266048908233643, Final Batch Loss: 0.7679719924926758\n",
      "Epoch 410, Loss: 2.970739483833313, Final Batch Loss: 0.7027760148048401\n",
      "Epoch 411, Loss: 3.025778651237488, Final Batch Loss: 0.775161862373352\n",
      "Epoch 412, Loss: 3.0666624903678894, Final Batch Loss: 0.8034738302230835\n",
      "Epoch 413, Loss: 2.9096686244010925, Final Batch Loss: 0.6495859622955322\n",
      "Epoch 414, Loss: 2.873752772808075, Final Batch Loss: 0.7160173654556274\n",
      "Epoch 415, Loss: 2.9219748973846436, Final Batch Loss: 0.691755473613739\n",
      "Epoch 416, Loss: 2.820055842399597, Final Batch Loss: 0.6257880926132202\n",
      "Epoch 417, Loss: 2.885479986667633, Final Batch Loss: 0.6552739143371582\n",
      "Epoch 418, Loss: 2.9621220231056213, Final Batch Loss: 0.7857438921928406\n",
      "Epoch 419, Loss: 2.928209960460663, Final Batch Loss: 0.7902184724807739\n",
      "Epoch 420, Loss: 2.963661551475525, Final Batch Loss: 0.8246092796325684\n",
      "Epoch 421, Loss: 2.9209588170051575, Final Batch Loss: 0.6109546422958374\n",
      "Epoch 422, Loss: 2.92154461145401, Final Batch Loss: 0.7234675288200378\n",
      "Epoch 423, Loss: 2.9304831624031067, Final Batch Loss: 0.7132193446159363\n",
      "Epoch 424, Loss: 2.9006950855255127, Final Batch Loss: 0.6974840760231018\n",
      "Epoch 425, Loss: 2.8726237416267395, Final Batch Loss: 0.7279654741287231\n",
      "Epoch 426, Loss: 3.063378393650055, Final Batch Loss: 0.7856941819190979\n",
      "Epoch 427, Loss: 2.8807782530784607, Final Batch Loss: 0.6957628726959229\n",
      "Epoch 428, Loss: 2.958550810813904, Final Batch Loss: 0.7732377052307129\n",
      "Epoch 429, Loss: 3.1099367141723633, Final Batch Loss: 0.8603947162628174\n",
      "Epoch 430, Loss: 2.9720138907432556, Final Batch Loss: 0.7561883330345154\n",
      "Epoch 431, Loss: 2.937197685241699, Final Batch Loss: 0.726245641708374\n",
      "Epoch 432, Loss: 2.9462183713912964, Final Batch Loss: 0.7164973020553589\n",
      "Epoch 433, Loss: 3.0021332502365112, Final Batch Loss: 0.7508816719055176\n",
      "Epoch 434, Loss: 2.9752261638641357, Final Batch Loss: 0.7947527170181274\n",
      "Epoch 435, Loss: 3.0387179851531982, Final Batch Loss: 0.8004077076911926\n",
      "Epoch 436, Loss: 2.933722734451294, Final Batch Loss: 0.7551611661911011\n",
      "Epoch 437, Loss: 2.8534592390060425, Final Batch Loss: 0.6525297164916992\n",
      "Epoch 438, Loss: 3.053657829761505, Final Batch Loss: 0.8213964700698853\n",
      "Epoch 439, Loss: 2.9156004786491394, Final Batch Loss: 0.7777450680732727\n",
      "Epoch 440, Loss: 2.9001100659370422, Final Batch Loss: 0.7363821268081665\n",
      "Epoch 441, Loss: 2.9523198008537292, Final Batch Loss: 0.7551537156105042\n",
      "Epoch 442, Loss: 2.9170600175857544, Final Batch Loss: 0.781671404838562\n",
      "Epoch 443, Loss: 2.865814208984375, Final Batch Loss: 0.6507907509803772\n",
      "Epoch 444, Loss: 2.925004243850708, Final Batch Loss: 0.6994378566741943\n",
      "Epoch 445, Loss: 2.9212490916252136, Final Batch Loss: 0.753987193107605\n",
      "Epoch 446, Loss: 3.0001607537269592, Final Batch Loss: 0.8203678727149963\n",
      "Epoch 447, Loss: 2.8923606276512146, Final Batch Loss: 0.7971689105033875\n",
      "Epoch 448, Loss: 2.7292659878730774, Final Batch Loss: 0.6834734082221985\n",
      "Epoch 449, Loss: 2.9645309448242188, Final Batch Loss: 0.7024286389350891\n",
      "Epoch 450, Loss: 2.89033180475235, Final Batch Loss: 0.603790819644928\n",
      "Epoch 451, Loss: 2.9201786518096924, Final Batch Loss: 0.7519038915634155\n",
      "Epoch 452, Loss: 3.0712791681289673, Final Batch Loss: 0.8211183547973633\n",
      "Epoch 453, Loss: 2.8044843077659607, Final Batch Loss: 0.6226486563682556\n",
      "Epoch 454, Loss: 2.7574572563171387, Final Batch Loss: 0.6313501000404358\n",
      "Epoch 455, Loss: 2.777244508266449, Final Batch Loss: 0.6024966835975647\n",
      "Epoch 456, Loss: 2.898178815841675, Final Batch Loss: 0.7378880977630615\n",
      "Epoch 457, Loss: 2.7884814143180847, Final Batch Loss: 0.7397671341896057\n",
      "Epoch 458, Loss: 2.8718090653419495, Final Batch Loss: 0.7314746975898743\n",
      "Epoch 459, Loss: 2.9217891693115234, Final Batch Loss: 0.729857325553894\n",
      "Epoch 460, Loss: 2.9244840145111084, Final Batch Loss: 0.7586182951927185\n",
      "Epoch 461, Loss: 2.824214816093445, Final Batch Loss: 0.7046498656272888\n",
      "Epoch 462, Loss: 2.8321380019187927, Final Batch Loss: 0.7752140760421753\n",
      "Epoch 463, Loss: 2.8143399357795715, Final Batch Loss: 0.6505668759346008\n",
      "Epoch 464, Loss: 2.748379647731781, Final Batch Loss: 0.575639009475708\n",
      "Epoch 465, Loss: 2.9267383217811584, Final Batch Loss: 0.8194884061813354\n",
      "Epoch 466, Loss: 2.858330011367798, Final Batch Loss: 0.6709888577461243\n",
      "Epoch 467, Loss: 2.9185080528259277, Final Batch Loss: 0.8038965463638306\n",
      "Epoch 468, Loss: 2.8136053681373596, Final Batch Loss: 0.6428066492080688\n",
      "Epoch 469, Loss: 2.7385658621788025, Final Batch Loss: 0.7026647329330444\n",
      "Epoch 470, Loss: 2.892680048942566, Final Batch Loss: 0.7395856976509094\n",
      "Epoch 471, Loss: 2.8233203887939453, Final Batch Loss: 0.634315013885498\n",
      "Epoch 472, Loss: 2.7297316193580627, Final Batch Loss: 0.5794363617897034\n",
      "Epoch 473, Loss: 2.7357946634292603, Final Batch Loss: 0.6223992109298706\n",
      "Epoch 474, Loss: 2.691679894924164, Final Batch Loss: 0.6531116962432861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 475, Loss: 2.912660300731659, Final Batch Loss: 0.6577284932136536\n",
      "Epoch 476, Loss: 2.7251283526420593, Final Batch Loss: 0.6260770559310913\n",
      "Epoch 477, Loss: 2.8124359250068665, Final Batch Loss: 0.7565857172012329\n",
      "Epoch 478, Loss: 2.7661132216453552, Final Batch Loss: 0.6249929666519165\n",
      "Epoch 479, Loss: 2.8505306243896484, Final Batch Loss: 0.7965459227561951\n",
      "Epoch 480, Loss: 2.86466383934021, Final Batch Loss: 0.73478102684021\n",
      "Epoch 481, Loss: 2.768095850944519, Final Batch Loss: 0.6990626454353333\n",
      "Epoch 482, Loss: 2.8326409459114075, Final Batch Loss: 0.6899676322937012\n",
      "Epoch 483, Loss: 2.7789852619171143, Final Batch Loss: 0.7543066143989563\n",
      "Epoch 484, Loss: 2.828208029270172, Final Batch Loss: 0.6868404746055603\n",
      "Epoch 485, Loss: 2.7431188821792603, Final Batch Loss: 0.6800028681755066\n",
      "Epoch 486, Loss: 2.8411041498184204, Final Batch Loss: 0.7321388125419617\n",
      "Epoch 487, Loss: 2.592483639717102, Final Batch Loss: 0.5999507308006287\n",
      "Epoch 488, Loss: 2.7598626613616943, Final Batch Loss: 0.6904014945030212\n",
      "Epoch 489, Loss: 2.793983519077301, Final Batch Loss: 0.6755865812301636\n",
      "Epoch 490, Loss: 2.7649617195129395, Final Batch Loss: 0.5671066045761108\n",
      "Epoch 491, Loss: 2.7976878881454468, Final Batch Loss: 0.7937177419662476\n",
      "Epoch 492, Loss: 2.7428810596466064, Final Batch Loss: 0.6804192066192627\n",
      "Epoch 493, Loss: 2.796334207057953, Final Batch Loss: 0.7097175717353821\n",
      "Epoch 494, Loss: 2.6697566509246826, Final Batch Loss: 0.6411640644073486\n",
      "Epoch 495, Loss: 2.700064182281494, Final Batch Loss: 0.6332610845565796\n",
      "Epoch 496, Loss: 2.7670419812202454, Final Batch Loss: 0.7657535672187805\n",
      "Epoch 497, Loss: 2.6231983304023743, Final Batch Loss: 0.6371455192565918\n",
      "Epoch 498, Loss: 2.7559821009635925, Final Batch Loss: 0.6875762343406677\n",
      "Epoch 499, Loss: 2.7503329515457153, Final Batch Loss: 0.561854898929596\n",
      "Epoch 500, Loss: 2.750781834125519, Final Batch Loss: 0.6472514867782593\n",
      "Epoch 501, Loss: 2.548986077308655, Final Batch Loss: 0.6340556144714355\n",
      "Epoch 502, Loss: 2.742105185985565, Final Batch Loss: 0.6616731286048889\n",
      "Epoch 503, Loss: 2.6597402691841125, Final Batch Loss: 0.6642321944236755\n",
      "Epoch 504, Loss: 2.6026805639266968, Final Batch Loss: 0.6385044455528259\n",
      "Epoch 505, Loss: 2.8544780611991882, Final Batch Loss: 0.8063148856163025\n",
      "Epoch 506, Loss: 2.7097785472869873, Final Batch Loss: 0.6811596751213074\n",
      "Epoch 507, Loss: 2.87870454788208, Final Batch Loss: 0.7273001670837402\n",
      "Epoch 508, Loss: 2.5982214212417603, Final Batch Loss: 0.6517348289489746\n",
      "Epoch 509, Loss: 2.8290852904319763, Final Batch Loss: 0.7357613444328308\n",
      "Epoch 510, Loss: 2.7604990005493164, Final Batch Loss: 0.7376295328140259\n",
      "Epoch 511, Loss: 2.6605799794197083, Final Batch Loss: 0.599412202835083\n",
      "Epoch 512, Loss: 2.713675081729889, Final Batch Loss: 0.7346150279045105\n",
      "Epoch 513, Loss: 2.7779802083969116, Final Batch Loss: 0.7129794955253601\n",
      "Epoch 514, Loss: 2.8002628087997437, Final Batch Loss: 0.6982739567756653\n",
      "Epoch 515, Loss: 2.6153793931007385, Final Batch Loss: 0.644354522228241\n",
      "Epoch 516, Loss: 2.7077810764312744, Final Batch Loss: 0.6863739490509033\n",
      "Epoch 517, Loss: 2.8113242387771606, Final Batch Loss: 0.7145325541496277\n",
      "Epoch 518, Loss: 2.639782726764679, Final Batch Loss: 0.611869215965271\n",
      "Epoch 519, Loss: 2.671814203262329, Final Batch Loss: 0.6733545660972595\n",
      "Epoch 520, Loss: 2.809257924556732, Final Batch Loss: 0.7696598768234253\n",
      "Epoch 521, Loss: 2.6378689408302307, Final Batch Loss: 0.6555333137512207\n",
      "Epoch 522, Loss: 2.637669026851654, Final Batch Loss: 0.6864280104637146\n",
      "Epoch 523, Loss: 2.6252753734588623, Final Batch Loss: 0.6236973404884338\n",
      "Epoch 524, Loss: 2.6408241987228394, Final Batch Loss: 0.6337031126022339\n",
      "Epoch 525, Loss: 2.6538363695144653, Final Batch Loss: 0.6574133634567261\n",
      "Epoch 526, Loss: 2.6358925700187683, Final Batch Loss: 0.6207649111747742\n",
      "Epoch 527, Loss: 2.676521360874176, Final Batch Loss: 0.6417954564094543\n",
      "Epoch 528, Loss: 2.6994144320487976, Final Batch Loss: 0.6560482978820801\n",
      "Epoch 529, Loss: 2.5987934470176697, Final Batch Loss: 0.6900365948677063\n",
      "Epoch 530, Loss: 2.73281466960907, Final Batch Loss: 0.7954499125480652\n",
      "Epoch 531, Loss: 2.5950761437416077, Final Batch Loss: 0.658171534538269\n",
      "Epoch 532, Loss: 2.6427473425865173, Final Batch Loss: 0.6524501442909241\n",
      "Epoch 533, Loss: 2.6398579478263855, Final Batch Loss: 0.7203506231307983\n",
      "Epoch 534, Loss: 2.661099910736084, Final Batch Loss: 0.7422342300415039\n",
      "Epoch 535, Loss: 2.6268755197525024, Final Batch Loss: 0.6617521047592163\n",
      "Epoch 536, Loss: 2.616312623023987, Final Batch Loss: 0.7004290819168091\n",
      "Epoch 537, Loss: 2.673743188381195, Final Batch Loss: 0.6984226107597351\n",
      "Epoch 538, Loss: 2.7057928442955017, Final Batch Loss: 0.7208553552627563\n",
      "Epoch 539, Loss: 2.7632022500038147, Final Batch Loss: 0.6294879913330078\n",
      "Epoch 540, Loss: 2.5808716416358948, Final Batch Loss: 0.6596764922142029\n",
      "Epoch 541, Loss: 2.597932815551758, Final Batch Loss: 0.6343223452568054\n",
      "Epoch 542, Loss: 2.780702292919159, Final Batch Loss: 0.6737802028656006\n",
      "Epoch 543, Loss: 2.5454599261283875, Final Batch Loss: 0.6072672009468079\n",
      "Epoch 544, Loss: 2.4530665278434753, Final Batch Loss: 0.553108274936676\n",
      "Epoch 545, Loss: 2.6421072483062744, Final Batch Loss: 0.6942821145057678\n",
      "Epoch 546, Loss: 2.6396912932395935, Final Batch Loss: 0.6708341836929321\n",
      "Epoch 547, Loss: 2.6009148359298706, Final Batch Loss: 0.62503582239151\n",
      "Epoch 548, Loss: 2.445331633090973, Final Batch Loss: 0.6149603128433228\n",
      "Epoch 549, Loss: 2.70336776971817, Final Batch Loss: 0.686107337474823\n",
      "Epoch 550, Loss: 2.726734220981598, Final Batch Loss: 0.6795246601104736\n",
      "Epoch 551, Loss: 2.634977102279663, Final Batch Loss: 0.7296776175498962\n",
      "Epoch 552, Loss: 2.624366343021393, Final Batch Loss: 0.6525609493255615\n",
      "Epoch 553, Loss: 2.761917233467102, Final Batch Loss: 0.7773675918579102\n",
      "Epoch 554, Loss: 2.6977005004882812, Final Batch Loss: 0.762222170829773\n",
      "Epoch 555, Loss: 2.5613983869552612, Final Batch Loss: 0.528863787651062\n",
      "Epoch 556, Loss: 2.62239146232605, Final Batch Loss: 0.6204022169113159\n",
      "Epoch 557, Loss: 2.6068902015686035, Final Batch Loss: 0.6502113938331604\n",
      "Epoch 558, Loss: 2.606899857521057, Final Batch Loss: 0.6322727203369141\n",
      "Epoch 559, Loss: 2.505169093608856, Final Batch Loss: 0.7225145697593689\n",
      "Epoch 560, Loss: 2.7373659014701843, Final Batch Loss: 0.6691096425056458\n",
      "Epoch 561, Loss: 2.5795679688453674, Final Batch Loss: 0.6395720839500427\n",
      "Epoch 562, Loss: 2.4884195923805237, Final Batch Loss: 0.626336932182312\n",
      "Epoch 563, Loss: 2.5746241211891174, Final Batch Loss: 0.603171706199646\n",
      "Epoch 564, Loss: 2.6480191349983215, Final Batch Loss: 0.6963914632797241\n",
      "Epoch 565, Loss: 2.496562659740448, Final Batch Loss: 0.6019448637962341\n",
      "Epoch 566, Loss: 2.748648524284363, Final Batch Loss: 0.8513725399971008\n",
      "Epoch 567, Loss: 2.748712122440338, Final Batch Loss: 0.7579057812690735\n",
      "Epoch 568, Loss: 2.459877133369446, Final Batch Loss: 0.5923212766647339\n",
      "Epoch 569, Loss: 2.5760528445243835, Final Batch Loss: 0.6876360177993774\n",
      "Epoch 570, Loss: 2.5427751541137695, Final Batch Loss: 0.7070927619934082\n",
      "Epoch 571, Loss: 2.7421358823776245, Final Batch Loss: 0.7887661457061768\n",
      "Epoch 572, Loss: 2.480136454105377, Final Batch Loss: 0.6611130237579346\n",
      "Epoch 573, Loss: 2.516162157058716, Final Batch Loss: 0.5442085862159729\n",
      "Epoch 574, Loss: 2.5414148569107056, Final Batch Loss: 0.6500518321990967\n",
      "Epoch 575, Loss: 2.5684019923210144, Final Batch Loss: 0.677939772605896\n",
      "Epoch 576, Loss: 2.4890456795692444, Final Batch Loss: 0.7188452482223511\n",
      "Epoch 577, Loss: 2.556235671043396, Final Batch Loss: 0.5617616772651672\n",
      "Epoch 578, Loss: 2.5847367644309998, Final Batch Loss: 0.689793586730957\n",
      "Epoch 579, Loss: 2.624864399433136, Final Batch Loss: 0.6284860968589783\n",
      "Epoch 580, Loss: 2.555085599422455, Final Batch Loss: 0.6341670751571655\n",
      "Epoch 581, Loss: 2.514473855495453, Final Batch Loss: 0.6544610857963562\n",
      "Epoch 582, Loss: 2.4071730375289917, Final Batch Loss: 0.5203293561935425\n",
      "Epoch 583, Loss: 2.5282428860664368, Final Batch Loss: 0.5769308805465698\n",
      "Epoch 584, Loss: 2.51327782869339, Final Batch Loss: 0.6178625822067261\n",
      "Epoch 585, Loss: 2.6418219804763794, Final Batch Loss: 0.6123610734939575\n",
      "Epoch 586, Loss: 2.4838743209838867, Final Batch Loss: 0.6407073140144348\n",
      "Epoch 587, Loss: 2.519478440284729, Final Batch Loss: 0.6518245339393616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 588, Loss: 2.564965546131134, Final Batch Loss: 0.7001262307167053\n",
      "Epoch 589, Loss: 2.608674466609955, Final Batch Loss: 0.5979444980621338\n",
      "Epoch 590, Loss: 2.463521361351013, Final Batch Loss: 0.5318750739097595\n",
      "Epoch 591, Loss: 2.6594342589378357, Final Batch Loss: 0.763033390045166\n",
      "Epoch 592, Loss: 2.499953866004944, Final Batch Loss: 0.590409517288208\n",
      "Epoch 593, Loss: 2.5704395174980164, Final Batch Loss: 0.6782826781272888\n",
      "Epoch 594, Loss: 2.595210373401642, Final Batch Loss: 0.759743869304657\n",
      "Epoch 595, Loss: 2.536718249320984, Final Batch Loss: 0.5894142985343933\n",
      "Epoch 596, Loss: 2.459092915058136, Final Batch Loss: 0.5661264061927795\n",
      "Epoch 597, Loss: 2.527508854866028, Final Batch Loss: 0.5680993795394897\n",
      "Epoch 598, Loss: 2.474113345146179, Final Batch Loss: 0.6908802390098572\n",
      "Epoch 599, Loss: 2.502901017665863, Final Batch Loss: 0.6175878047943115\n",
      "Epoch 600, Loss: 2.55122309923172, Final Batch Loss: 0.6435102224349976\n",
      "Epoch 601, Loss: 2.554397404193878, Final Batch Loss: 0.5638914108276367\n",
      "Epoch 602, Loss: 2.4247790575027466, Final Batch Loss: 0.5470172166824341\n",
      "Epoch 603, Loss: 2.530543863773346, Final Batch Loss: 0.6049666404724121\n",
      "Epoch 604, Loss: 2.5684547424316406, Final Batch Loss: 0.7590353488922119\n",
      "Epoch 605, Loss: 2.386099725961685, Final Batch Loss: 0.4753360450267792\n",
      "Epoch 606, Loss: 2.5627630949020386, Final Batch Loss: 0.7033019065856934\n",
      "Epoch 607, Loss: 2.383192777633667, Final Batch Loss: 0.589400589466095\n",
      "Epoch 608, Loss: 2.5299702286720276, Final Batch Loss: 0.5955832004547119\n",
      "Epoch 609, Loss: 2.437666893005371, Final Batch Loss: 0.6013606190681458\n",
      "Epoch 610, Loss: 2.4315447211265564, Final Batch Loss: 0.6014481782913208\n",
      "Epoch 611, Loss: 2.5713343024253845, Final Batch Loss: 0.6511845588684082\n",
      "Epoch 612, Loss: 2.4710835814476013, Final Batch Loss: 0.6643956899642944\n",
      "Epoch 613, Loss: 2.4744845628738403, Final Batch Loss: 0.5998714566230774\n",
      "Epoch 614, Loss: 2.4615795016288757, Final Batch Loss: 0.6962705850601196\n",
      "Epoch 615, Loss: 2.5430411100387573, Final Batch Loss: 0.7447469830513\n",
      "Epoch 616, Loss: 2.4295243620872498, Final Batch Loss: 0.6296296119689941\n",
      "Epoch 617, Loss: 2.2725937962532043, Final Batch Loss: 0.5592941045761108\n",
      "Epoch 618, Loss: 2.511363446712494, Final Batch Loss: 0.6216530799865723\n",
      "Epoch 619, Loss: 2.386481821537018, Final Batch Loss: 0.6122817397117615\n",
      "Epoch 620, Loss: 2.558200240135193, Final Batch Loss: 0.7762036323547363\n",
      "Epoch 621, Loss: 2.312864661216736, Final Batch Loss: 0.4954906702041626\n",
      "Epoch 622, Loss: 2.4897828698158264, Final Batch Loss: 0.6851859092712402\n",
      "Epoch 623, Loss: 2.3982319831848145, Final Batch Loss: 0.5489394068717957\n",
      "Epoch 624, Loss: 2.5277770161628723, Final Batch Loss: 0.6029004454612732\n",
      "Epoch 625, Loss: 2.407452940940857, Final Batch Loss: 0.5750406980514526\n",
      "Epoch 626, Loss: 2.542625367641449, Final Batch Loss: 0.7309192419052124\n",
      "Epoch 627, Loss: 2.487770915031433, Final Batch Loss: 0.675957202911377\n",
      "Epoch 628, Loss: 2.3493098616600037, Final Batch Loss: 0.6221933960914612\n",
      "Epoch 629, Loss: 2.537265419960022, Final Batch Loss: 0.6869964599609375\n",
      "Epoch 630, Loss: 2.3487133383750916, Final Batch Loss: 0.4863871932029724\n",
      "Epoch 631, Loss: 2.378495216369629, Final Batch Loss: 0.5141696929931641\n",
      "Epoch 632, Loss: 2.4068970680236816, Final Batch Loss: 0.5479791760444641\n",
      "Epoch 633, Loss: 2.5105186700820923, Final Batch Loss: 0.6574604511260986\n",
      "Epoch 634, Loss: 2.3480993509292603, Final Batch Loss: 0.5919926762580872\n",
      "Epoch 635, Loss: 2.6411694288253784, Final Batch Loss: 0.7434523105621338\n",
      "Epoch 636, Loss: 2.486063539981842, Final Batch Loss: 0.7275211811065674\n",
      "Epoch 637, Loss: 2.4227038621902466, Final Batch Loss: 0.5948917269706726\n",
      "Epoch 638, Loss: 2.4333717823028564, Final Batch Loss: 0.6549224853515625\n",
      "Epoch 639, Loss: 2.343068540096283, Final Batch Loss: 0.5105752348899841\n",
      "Epoch 640, Loss: 2.5304309725761414, Final Batch Loss: 0.7134177684783936\n",
      "Epoch 641, Loss: 2.35315477848053, Final Batch Loss: 0.591774046421051\n",
      "Epoch 642, Loss: 2.438806414604187, Final Batch Loss: 0.559198796749115\n",
      "Epoch 643, Loss: 2.44711571931839, Final Batch Loss: 0.5680239796638489\n",
      "Epoch 644, Loss: 2.4431386590003967, Final Batch Loss: 0.6456181406974792\n",
      "Epoch 645, Loss: 2.410219371318817, Final Batch Loss: 0.6415834426879883\n",
      "Epoch 646, Loss: 2.3164008259773254, Final Batch Loss: 0.5438042283058167\n",
      "Epoch 647, Loss: 2.4523470401763916, Final Batch Loss: 0.5917179584503174\n",
      "Epoch 648, Loss: 2.4544803500175476, Final Batch Loss: 0.7075331807136536\n",
      "Epoch 649, Loss: 2.6073684096336365, Final Batch Loss: 0.7548350691795349\n",
      "Epoch 650, Loss: 2.517419159412384, Final Batch Loss: 0.6258982419967651\n",
      "Epoch 651, Loss: 2.3300668001174927, Final Batch Loss: 0.6139891147613525\n",
      "Epoch 652, Loss: 2.4673426151275635, Final Batch Loss: 0.6182224154472351\n",
      "Epoch 653, Loss: 2.362307071685791, Final Batch Loss: 0.5929735898971558\n",
      "Epoch 654, Loss: 2.4183306097984314, Final Batch Loss: 0.6562480330467224\n",
      "Epoch 655, Loss: 2.3638132214546204, Final Batch Loss: 0.47439152002334595\n",
      "Epoch 656, Loss: 2.381722092628479, Final Batch Loss: 0.5050533413887024\n",
      "Epoch 657, Loss: 2.335653781890869, Final Batch Loss: 0.530792236328125\n",
      "Epoch 658, Loss: 2.3647788166999817, Final Batch Loss: 0.5199922919273376\n",
      "Epoch 659, Loss: 2.365790158510208, Final Batch Loss: 0.6064769625663757\n",
      "Epoch 660, Loss: 2.5378026962280273, Final Batch Loss: 0.7300612330436707\n",
      "Epoch 661, Loss: 2.3299986720085144, Final Batch Loss: 0.5259915590286255\n",
      "Epoch 662, Loss: 2.4288026094436646, Final Batch Loss: 0.6202077865600586\n",
      "Epoch 663, Loss: 2.4326427578926086, Final Batch Loss: 0.6116411089897156\n",
      "Epoch 664, Loss: 2.312332510948181, Final Batch Loss: 0.5584971904754639\n",
      "Epoch 665, Loss: 2.3891927003860474, Final Batch Loss: 0.6112486124038696\n",
      "Epoch 666, Loss: 2.329471528530121, Final Batch Loss: 0.5884550213813782\n",
      "Epoch 667, Loss: 2.429564416408539, Final Batch Loss: 0.5878620147705078\n",
      "Epoch 668, Loss: 2.332610249519348, Final Batch Loss: 0.5270525813102722\n",
      "Epoch 669, Loss: 2.3898940682411194, Final Batch Loss: 0.5877747535705566\n",
      "Epoch 670, Loss: 2.3510348200798035, Final Batch Loss: 0.6461948752403259\n",
      "Epoch 671, Loss: 2.3072556257247925, Final Batch Loss: 0.5401654839515686\n",
      "Epoch 672, Loss: 2.427004039287567, Final Batch Loss: 0.6243932247161865\n",
      "Epoch 673, Loss: 2.538241147994995, Final Batch Loss: 0.7510650157928467\n",
      "Epoch 674, Loss: 2.515948474407196, Final Batch Loss: 0.7344672083854675\n",
      "Epoch 675, Loss: 2.5550575256347656, Final Batch Loss: 0.7835718393325806\n",
      "Epoch 676, Loss: 2.284988909959793, Final Batch Loss: 0.4636293351650238\n",
      "Epoch 677, Loss: 2.3577550649642944, Final Batch Loss: 0.6645222306251526\n",
      "Epoch 678, Loss: 2.42425400018692, Final Batch Loss: 0.5968701839447021\n",
      "Epoch 679, Loss: 2.3415541648864746, Final Batch Loss: 0.6052228808403015\n",
      "Epoch 680, Loss: 2.397977292537689, Final Batch Loss: 0.5685389041900635\n",
      "Epoch 681, Loss: 2.317237675189972, Final Batch Loss: 0.5491324067115784\n",
      "Epoch 682, Loss: 2.224964290857315, Final Batch Loss: 0.4665689170360565\n",
      "Epoch 683, Loss: 2.3089210093021393, Final Batch Loss: 0.48704853653907776\n",
      "Epoch 684, Loss: 2.365304410457611, Final Batch Loss: 0.7179028987884521\n",
      "Epoch 685, Loss: 2.359147071838379, Final Batch Loss: 0.556891679763794\n",
      "Epoch 686, Loss: 2.3468878269195557, Final Batch Loss: 0.543578565120697\n",
      "Epoch 687, Loss: 2.322226405143738, Final Batch Loss: 0.5976247191429138\n",
      "Epoch 688, Loss: 2.3557904958724976, Final Batch Loss: 0.6855960488319397\n",
      "Epoch 689, Loss: 2.2728123664855957, Final Batch Loss: 0.5417895913124084\n",
      "Epoch 690, Loss: 2.4368733763694763, Final Batch Loss: 0.5587325692176819\n",
      "Epoch 691, Loss: 2.3911648988723755, Final Batch Loss: 0.5562328696250916\n",
      "Epoch 692, Loss: 2.4510990381240845, Final Batch Loss: 0.6043402552604675\n",
      "Epoch 693, Loss: 2.253127932548523, Final Batch Loss: 0.5208181738853455\n",
      "Epoch 694, Loss: 2.2114389538764954, Final Batch Loss: 0.5518441200256348\n",
      "Epoch 695, Loss: 2.3554455637931824, Final Batch Loss: 0.5663648843765259\n",
      "Epoch 696, Loss: 2.362005293369293, Final Batch Loss: 0.5709054470062256\n",
      "Epoch 697, Loss: 2.334081918001175, Final Batch Loss: 0.607740044593811\n",
      "Epoch 698, Loss: 2.3785677552223206, Final Batch Loss: 0.5268502235412598\n",
      "Epoch 699, Loss: 2.2711676955223083, Final Batch Loss: 0.5720306634902954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 700, Loss: 2.3207274973392487, Final Batch Loss: 0.6966763138771057\n",
      "Epoch 701, Loss: 2.2485193014144897, Final Batch Loss: 0.5491538643836975\n",
      "Epoch 702, Loss: 2.485350549221039, Final Batch Loss: 0.6806938052177429\n",
      "Epoch 703, Loss: 2.4158138632774353, Final Batch Loss: 0.7208399176597595\n",
      "Epoch 704, Loss: 2.3843421936035156, Final Batch Loss: 0.6261764764785767\n",
      "Epoch 705, Loss: 2.236811101436615, Final Batch Loss: 0.5799668431282043\n",
      "Epoch 706, Loss: 2.298166036605835, Final Batch Loss: 0.48042261600494385\n",
      "Epoch 707, Loss: 2.2161380648612976, Final Batch Loss: 0.5887420177459717\n",
      "Epoch 708, Loss: 2.2946874797344208, Final Batch Loss: 0.6626186966896057\n",
      "Epoch 709, Loss: 2.2576080560684204, Final Batch Loss: 0.5446485280990601\n",
      "Epoch 710, Loss: 2.314516097307205, Final Batch Loss: 0.43970510363578796\n",
      "Epoch 711, Loss: 2.349234938621521, Final Batch Loss: 0.6182180047035217\n",
      "Epoch 712, Loss: 2.2228477597236633, Final Batch Loss: 0.5027294754981995\n",
      "Epoch 713, Loss: 2.2793091535568237, Final Batch Loss: 0.6602807641029358\n",
      "Epoch 714, Loss: 2.2256993651390076, Final Batch Loss: 0.5549619793891907\n",
      "Epoch 715, Loss: 2.244545102119446, Final Batch Loss: 0.6208242774009705\n",
      "Epoch 716, Loss: 2.2142695784568787, Final Batch Loss: 0.42991453409194946\n",
      "Epoch 717, Loss: 2.3520881831645966, Final Batch Loss: 0.629609227180481\n",
      "Epoch 718, Loss: 2.3372894525527954, Final Batch Loss: 0.6413367390632629\n",
      "Epoch 719, Loss: 2.245140314102173, Final Batch Loss: 0.5424136519432068\n",
      "Epoch 720, Loss: 2.289982497692108, Final Batch Loss: 0.5558862090110779\n",
      "Epoch 721, Loss: 2.451147675514221, Final Batch Loss: 0.6791602969169617\n",
      "Epoch 722, Loss: 2.3350433111190796, Final Batch Loss: 0.5726151466369629\n",
      "Epoch 723, Loss: 2.3022841811180115, Final Batch Loss: 0.5996385812759399\n",
      "Epoch 724, Loss: 2.3174092173576355, Final Batch Loss: 0.6600773930549622\n",
      "Epoch 725, Loss: 2.1750411987304688, Final Batch Loss: 0.536511242389679\n",
      "Epoch 726, Loss: 2.2690373063087463, Final Batch Loss: 0.5828122496604919\n",
      "Epoch 727, Loss: 2.3523428440093994, Final Batch Loss: 0.6743752956390381\n",
      "Epoch 728, Loss: 2.23544579744339, Final Batch Loss: 0.5926839113235474\n",
      "Epoch 729, Loss: 2.226256251335144, Final Batch Loss: 0.5472388863563538\n",
      "Epoch 730, Loss: 2.2441047728061676, Final Batch Loss: 0.5490708351135254\n",
      "Epoch 731, Loss: 2.3177739679813385, Final Batch Loss: 0.4890500009059906\n",
      "Epoch 732, Loss: 2.2203245162963867, Final Batch Loss: 0.551224946975708\n",
      "Epoch 733, Loss: 2.323864221572876, Final Batch Loss: 0.6715993881225586\n",
      "Epoch 734, Loss: 2.3486722111701965, Final Batch Loss: 0.5988109111785889\n",
      "Epoch 735, Loss: 2.355086088180542, Final Batch Loss: 0.5922458171844482\n",
      "Epoch 736, Loss: 2.3170177936553955, Final Batch Loss: 0.5519551634788513\n",
      "Epoch 737, Loss: 2.1773595809936523, Final Batch Loss: 0.5123675465583801\n",
      "Epoch 738, Loss: 2.2558946013450623, Final Batch Loss: 0.5412434935569763\n",
      "Epoch 739, Loss: 2.368547558784485, Final Batch Loss: 0.6770014762878418\n",
      "Epoch 740, Loss: 2.159110963344574, Final Batch Loss: 0.4456050992012024\n",
      "Epoch 741, Loss: 2.2451165914535522, Final Batch Loss: 0.4916762113571167\n",
      "Epoch 742, Loss: 2.220205247402191, Final Batch Loss: 0.47063779830932617\n",
      "Epoch 743, Loss: 2.2093522548675537, Final Batch Loss: 0.42541635036468506\n",
      "Epoch 744, Loss: 2.257263660430908, Final Batch Loss: 0.5850685238838196\n",
      "Epoch 745, Loss: 2.1929348707199097, Final Batch Loss: 0.5804606080055237\n",
      "Epoch 746, Loss: 2.1708468794822693, Final Batch Loss: 0.5142357349395752\n",
      "Epoch 747, Loss: 2.336338520050049, Final Batch Loss: 0.5916633009910583\n",
      "Epoch 748, Loss: 2.2247780859470367, Final Batch Loss: 0.47983142733573914\n",
      "Epoch 749, Loss: 2.3386498987674713, Final Batch Loss: 0.4868547022342682\n",
      "Epoch 750, Loss: 2.2302680909633636, Final Batch Loss: 0.5770810842514038\n",
      "Epoch 751, Loss: 2.2540435194969177, Final Batch Loss: 0.6132183074951172\n",
      "Epoch 752, Loss: 2.259620428085327, Final Batch Loss: 0.5429677367210388\n",
      "Epoch 753, Loss: 2.229580581188202, Final Batch Loss: 0.5515236854553223\n",
      "Epoch 754, Loss: 2.3860681653022766, Final Batch Loss: 0.6342880725860596\n",
      "Epoch 755, Loss: 2.266867995262146, Final Batch Loss: 0.5724015831947327\n",
      "Epoch 756, Loss: 2.199053704738617, Final Batch Loss: 0.47813183069229126\n",
      "Epoch 757, Loss: 2.343834936618805, Final Batch Loss: 0.6674820184707642\n",
      "Epoch 758, Loss: 2.105587065219879, Final Batch Loss: 0.4853411912918091\n",
      "Epoch 759, Loss: 2.1975071132183075, Final Batch Loss: 0.45160266757011414\n",
      "Epoch 760, Loss: 2.225882440805435, Final Batch Loss: 0.5760355591773987\n",
      "Epoch 761, Loss: 2.1892994046211243, Final Batch Loss: 0.5634034276008606\n",
      "Epoch 762, Loss: 2.270297944545746, Final Batch Loss: 0.6133559346199036\n",
      "Epoch 763, Loss: 2.2303530871868134, Final Batch Loss: 0.46908876299858093\n",
      "Epoch 764, Loss: 2.1614840030670166, Final Batch Loss: 0.5845876336097717\n",
      "Epoch 765, Loss: 2.08988219499588, Final Batch Loss: 0.5202351212501526\n",
      "Epoch 766, Loss: 2.234902322292328, Final Batch Loss: 0.5863237380981445\n",
      "Epoch 767, Loss: 2.187346428632736, Final Batch Loss: 0.565857470035553\n",
      "Epoch 768, Loss: 2.279155135154724, Final Batch Loss: 0.5476974248886108\n",
      "Epoch 769, Loss: 2.213929980993271, Final Batch Loss: 0.6390449404716492\n",
      "Epoch 770, Loss: 2.120545268058777, Final Batch Loss: 0.5387491583824158\n",
      "Epoch 771, Loss: 2.2623054683208466, Final Batch Loss: 0.4762682020664215\n",
      "Epoch 772, Loss: 2.2445408701896667, Final Batch Loss: 0.6399267315864563\n",
      "Epoch 773, Loss: 2.31895911693573, Final Batch Loss: 0.5869156718254089\n",
      "Epoch 774, Loss: 2.2555862069129944, Final Batch Loss: 0.5810375213623047\n",
      "Epoch 775, Loss: 2.1490564942359924, Final Batch Loss: 0.4976060390472412\n",
      "Epoch 776, Loss: 2.317901164293289, Final Batch Loss: 0.5928370356559753\n",
      "Epoch 777, Loss: 2.3007323145866394, Final Batch Loss: 0.6050851345062256\n",
      "Epoch 778, Loss: 2.154915452003479, Final Batch Loss: 0.5043461322784424\n",
      "Epoch 779, Loss: 2.2397711873054504, Final Batch Loss: 0.5606908202171326\n",
      "Epoch 780, Loss: 2.1961382031440735, Final Batch Loss: 0.5421267747879028\n",
      "Epoch 781, Loss: 2.1293172538280487, Final Batch Loss: 0.5563808083534241\n",
      "Epoch 782, Loss: 2.194586992263794, Final Batch Loss: 0.5698187351226807\n",
      "Epoch 783, Loss: 2.1137150824069977, Final Batch Loss: 0.4400419294834137\n",
      "Epoch 784, Loss: 2.1951490938663483, Final Batch Loss: 0.647824764251709\n",
      "Epoch 785, Loss: 2.19865545630455, Final Batch Loss: 0.6268590092658997\n",
      "Epoch 786, Loss: 2.2170433402061462, Final Batch Loss: 0.6116530299186707\n",
      "Epoch 787, Loss: 2.271366983652115, Final Batch Loss: 0.6717579960823059\n",
      "Epoch 788, Loss: 2.1286965012550354, Final Batch Loss: 0.5252446532249451\n",
      "Epoch 789, Loss: 2.2421902418136597, Final Batch Loss: 0.5878244042396545\n",
      "Epoch 790, Loss: 2.1909452378749847, Final Batch Loss: 0.5639316439628601\n",
      "Epoch 791, Loss: 2.1964613497257233, Final Batch Loss: 0.568648099899292\n",
      "Epoch 792, Loss: 2.16115266084671, Final Batch Loss: 0.5221362709999084\n",
      "Epoch 793, Loss: 2.2055800557136536, Final Batch Loss: 0.5183306932449341\n",
      "Epoch 794, Loss: 2.237759053707123, Final Batch Loss: 0.6063376665115356\n",
      "Epoch 795, Loss: 2.1462994813919067, Final Batch Loss: 0.4808192253112793\n",
      "Epoch 796, Loss: 2.315018653869629, Final Batch Loss: 0.5936226844787598\n",
      "Epoch 797, Loss: 2.1480638086795807, Final Batch Loss: 0.5473237633705139\n",
      "Epoch 798, Loss: 2.203371971845627, Final Batch Loss: 0.5771375894546509\n",
      "Epoch 799, Loss: 2.201401174068451, Final Batch Loss: 0.451438307762146\n",
      "Epoch 800, Loss: 2.159116715192795, Final Batch Loss: 0.5787443518638611\n",
      "Epoch 801, Loss: 2.120570659637451, Final Batch Loss: 0.546242356300354\n",
      "Epoch 802, Loss: 2.1874258518218994, Final Batch Loss: 0.6270943284034729\n",
      "Epoch 803, Loss: 2.1804665327072144, Final Batch Loss: 0.4879879355430603\n",
      "Epoch 804, Loss: 2.2125017642974854, Final Batch Loss: 0.480582058429718\n",
      "Epoch 805, Loss: 2.1324129700660706, Final Batch Loss: 0.4697332978248596\n",
      "Epoch 806, Loss: 2.2043820917606354, Final Batch Loss: 0.5748348236083984\n",
      "Epoch 807, Loss: 2.223552495241165, Final Batch Loss: 0.5992637276649475\n",
      "Epoch 808, Loss: 2.1947617530822754, Final Batch Loss: 0.5931189060211182\n",
      "Epoch 809, Loss: 2.163253664970398, Final Batch Loss: 0.5004785060882568\n",
      "Epoch 810, Loss: 2.190389037132263, Final Batch Loss: 0.5271138548851013\n",
      "Epoch 811, Loss: 2.2153082489967346, Final Batch Loss: 0.5554879903793335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 812, Loss: 2.1173770129680634, Final Batch Loss: 0.39745214581489563\n",
      "Epoch 813, Loss: 2.109454780817032, Final Batch Loss: 0.5123867988586426\n",
      "Epoch 814, Loss: 2.111387014389038, Final Batch Loss: 0.5049681067466736\n",
      "Epoch 815, Loss: 2.1908752024173737, Final Batch Loss: 0.6121464371681213\n",
      "Epoch 816, Loss: 2.1847632229328156, Final Batch Loss: 0.5860569477081299\n",
      "Epoch 817, Loss: 2.186455488204956, Final Batch Loss: 0.5573546886444092\n",
      "Epoch 818, Loss: 2.125168114900589, Final Batch Loss: 0.48669126629829407\n",
      "Epoch 819, Loss: 2.149089515209198, Final Batch Loss: 0.4663330912590027\n",
      "Epoch 820, Loss: 2.0704192519187927, Final Batch Loss: 0.42629730701446533\n",
      "Epoch 821, Loss: 2.055738151073456, Final Batch Loss: 0.4990970194339752\n",
      "Epoch 822, Loss: 2.188609004020691, Final Batch Loss: 0.6065778136253357\n",
      "Epoch 823, Loss: 2.052907317876816, Final Batch Loss: 0.4509629011154175\n",
      "Epoch 824, Loss: 2.1095407009124756, Final Batch Loss: 0.49122780561447144\n",
      "Epoch 825, Loss: 2.05693718791008, Final Batch Loss: 0.4519638121128082\n",
      "Epoch 826, Loss: 2.2276394367218018, Final Batch Loss: 0.7303758859634399\n",
      "Epoch 827, Loss: 2.154403269290924, Final Batch Loss: 0.5963661670684814\n",
      "Epoch 828, Loss: 2.2278345227241516, Final Batch Loss: 0.5552843809127808\n",
      "Epoch 829, Loss: 2.0883225202560425, Final Batch Loss: 0.4856024384498596\n",
      "Epoch 830, Loss: 2.2980238795280457, Final Batch Loss: 0.5926920175552368\n",
      "Epoch 831, Loss: 2.1449106633663177, Final Batch Loss: 0.6433191895484924\n",
      "Epoch 832, Loss: 2.216574043035507, Final Batch Loss: 0.6204621195793152\n",
      "Epoch 833, Loss: 2.1001859307289124, Final Batch Loss: 0.47648319602012634\n",
      "Epoch 834, Loss: 2.068719655275345, Final Batch Loss: 0.5243523716926575\n",
      "Epoch 835, Loss: 2.128445416688919, Final Batch Loss: 0.4574033319950104\n",
      "Epoch 836, Loss: 2.1738904118537903, Final Batch Loss: 0.5513979196548462\n",
      "Epoch 837, Loss: 2.208311140537262, Final Batch Loss: 0.5787354707717896\n",
      "Epoch 838, Loss: 2.193568170070648, Final Batch Loss: 0.6372966766357422\n",
      "Epoch 839, Loss: 2.1965676844120026, Final Batch Loss: 0.6018644571304321\n",
      "Epoch 840, Loss: 2.1091129183769226, Final Batch Loss: 0.5675631165504456\n",
      "Epoch 841, Loss: 2.0935814678668976, Final Batch Loss: 0.4896710515022278\n",
      "Epoch 842, Loss: 2.1355736553668976, Final Batch Loss: 0.5751153826713562\n",
      "Epoch 843, Loss: 2.1953752040863037, Final Batch Loss: 0.6509093642234802\n",
      "Epoch 844, Loss: 2.051785796880722, Final Batch Loss: 0.48957106471061707\n",
      "Epoch 845, Loss: 2.1916623711586, Final Batch Loss: 0.4993257522583008\n",
      "Epoch 846, Loss: 2.1461567878723145, Final Batch Loss: 0.4749007225036621\n",
      "Epoch 847, Loss: 2.2264601588249207, Final Batch Loss: 0.5419251918792725\n",
      "Epoch 848, Loss: 2.190499782562256, Final Batch Loss: 0.6404080390930176\n",
      "Epoch 849, Loss: 2.147396594285965, Final Batch Loss: 0.5716384649276733\n",
      "Epoch 850, Loss: 2.3533181250095367, Final Batch Loss: 0.7247956395149231\n",
      "Epoch 851, Loss: 2.0926403403282166, Final Batch Loss: 0.4556257724761963\n",
      "Epoch 852, Loss: 2.1024830043315887, Final Batch Loss: 0.40826401114463806\n",
      "Epoch 853, Loss: 2.10336497426033, Final Batch Loss: 0.47210320830345154\n",
      "Epoch 854, Loss: 2.0975061655044556, Final Batch Loss: 0.5818660855293274\n",
      "Epoch 855, Loss: 2.0046655237674713, Final Batch Loss: 0.45100125670433044\n",
      "Epoch 856, Loss: 2.1437536478042603, Final Batch Loss: 0.4343520402908325\n",
      "Epoch 857, Loss: 2.1591288447380066, Final Batch Loss: 0.5600273609161377\n",
      "Epoch 858, Loss: 2.0473503470420837, Final Batch Loss: 0.5107820630073547\n",
      "Epoch 859, Loss: 2.2383842766284943, Final Batch Loss: 0.658115804195404\n",
      "Epoch 860, Loss: 2.0714976489543915, Final Batch Loss: 0.3771820366382599\n",
      "Epoch 861, Loss: 2.1176092624664307, Final Batch Loss: 0.5258481502532959\n",
      "Epoch 862, Loss: 2.073239654302597, Final Batch Loss: 0.5503568649291992\n",
      "Epoch 863, Loss: 2.0700104236602783, Final Batch Loss: 0.4775542616844177\n",
      "Epoch 864, Loss: 2.087555408477783, Final Batch Loss: 0.530968189239502\n",
      "Epoch 865, Loss: 2.137016236782074, Final Batch Loss: 0.520850419998169\n",
      "Epoch 866, Loss: 2.0642696022987366, Final Batch Loss: 0.5179077982902527\n",
      "Epoch 867, Loss: 2.0476487576961517, Final Batch Loss: 0.45914965867996216\n",
      "Epoch 868, Loss: 2.1075747907161713, Final Batch Loss: 0.4830939471721649\n",
      "Epoch 869, Loss: 1.9322823286056519, Final Batch Loss: 0.4196433424949646\n",
      "Epoch 870, Loss: 2.1353490352630615, Final Batch Loss: 0.4453197717666626\n",
      "Epoch 871, Loss: 2.2007207572460175, Final Batch Loss: 0.5888945460319519\n",
      "Epoch 872, Loss: 2.0261412262916565, Final Batch Loss: 0.5406087636947632\n",
      "Epoch 873, Loss: 2.0708247423171997, Final Batch Loss: 0.5299262404441833\n",
      "Epoch 874, Loss: 2.1266660690307617, Final Batch Loss: 0.5276614427566528\n",
      "Epoch 875, Loss: 1.9962033033370972, Final Batch Loss: 0.4634418785572052\n",
      "Epoch 876, Loss: 2.223974108695984, Final Batch Loss: 0.5751502513885498\n",
      "Epoch 877, Loss: 2.141196995973587, Final Batch Loss: 0.605178952217102\n",
      "Epoch 878, Loss: 2.254181683063507, Final Batch Loss: 0.6401615738868713\n",
      "Epoch 879, Loss: 2.0896977186203003, Final Batch Loss: 0.4676172733306885\n",
      "Epoch 880, Loss: 2.1108856201171875, Final Batch Loss: 0.49064549803733826\n",
      "Epoch 881, Loss: 2.1952744126319885, Final Batch Loss: 0.6103681921958923\n",
      "Epoch 882, Loss: 2.2744195461273193, Final Batch Loss: 0.5847567319869995\n",
      "Epoch 883, Loss: 2.097025603055954, Final Batch Loss: 0.4988316297531128\n",
      "Epoch 884, Loss: 2.1502602994441986, Final Batch Loss: 0.44842973351478577\n",
      "Epoch 885, Loss: 2.16630819439888, Final Batch Loss: 0.5969147682189941\n",
      "Epoch 886, Loss: 1.9459908604621887, Final Batch Loss: 0.37619614601135254\n",
      "Epoch 887, Loss: 2.1860371530056, Final Batch Loss: 0.4658946692943573\n",
      "Epoch 888, Loss: 1.9927201569080353, Final Batch Loss: 0.47588035464286804\n",
      "Epoch 889, Loss: 2.051933318376541, Final Batch Loss: 0.5196865797042847\n",
      "Epoch 890, Loss: 2.147681623697281, Final Batch Loss: 0.5831225514411926\n",
      "Epoch 891, Loss: 2.1466904878616333, Final Batch Loss: 0.5699819922447205\n",
      "Epoch 892, Loss: 2.0968649685382843, Final Batch Loss: 0.4552532136440277\n",
      "Epoch 893, Loss: 2.0799458622932434, Final Batch Loss: 0.5769297480583191\n",
      "Epoch 894, Loss: 2.018196314573288, Final Batch Loss: 0.45158883929252625\n",
      "Epoch 895, Loss: 2.00745490193367, Final Batch Loss: 0.4378269910812378\n",
      "Epoch 896, Loss: 2.1375709176063538, Final Batch Loss: 0.5426363348960876\n",
      "Epoch 897, Loss: 2.178867846727371, Final Batch Loss: 0.5630962252616882\n",
      "Epoch 898, Loss: 2.1002806425094604, Final Batch Loss: 0.5399522185325623\n",
      "Epoch 899, Loss: 2.0468011498451233, Final Batch Loss: 0.5614634156227112\n",
      "Epoch 900, Loss: 2.0712251365184784, Final Batch Loss: 0.5144042372703552\n",
      "Epoch 901, Loss: 2.072827786207199, Final Batch Loss: 0.5817524790763855\n",
      "Epoch 902, Loss: 2.013259142637253, Final Batch Loss: 0.4363086521625519\n",
      "Epoch 903, Loss: 2.024592310190201, Final Batch Loss: 0.4972541928291321\n",
      "Epoch 904, Loss: 2.2037388682365417, Final Batch Loss: 0.5115824937820435\n",
      "Epoch 905, Loss: 1.9468940794467926, Final Batch Loss: 0.49282342195510864\n",
      "Epoch 906, Loss: 2.22966268658638, Final Batch Loss: 0.6710934638977051\n",
      "Epoch 907, Loss: 2.008503705263138, Final Batch Loss: 0.4565604627132416\n",
      "Epoch 908, Loss: 2.0117363333702087, Final Batch Loss: 0.6028635501861572\n",
      "Epoch 909, Loss: 2.120718628168106, Final Batch Loss: 0.5726662874221802\n",
      "Epoch 910, Loss: 2.1149344742298126, Final Batch Loss: 0.6559672355651855\n",
      "Epoch 911, Loss: 2.1181789338588715, Final Batch Loss: 0.44749191403388977\n",
      "Epoch 912, Loss: 2.0491961240768433, Final Batch Loss: 0.46066904067993164\n",
      "Epoch 913, Loss: 2.127389281988144, Final Batch Loss: 0.6473845839500427\n",
      "Epoch 914, Loss: 2.093761622905731, Final Batch Loss: 0.5209245085716248\n",
      "Epoch 915, Loss: 2.0954249501228333, Final Batch Loss: 0.5571286082267761\n",
      "Epoch 916, Loss: 1.9892714023590088, Final Batch Loss: 0.489871084690094\n",
      "Epoch 917, Loss: 2.0068914592266083, Final Batch Loss: 0.4954319894313812\n",
      "Epoch 918, Loss: 1.9908800721168518, Final Batch Loss: 0.5080093741416931\n",
      "Epoch 919, Loss: 2.02548611164093, Final Batch Loss: 0.50847989320755\n",
      "Epoch 920, Loss: 2.2034624218940735, Final Batch Loss: 0.6316215395927429\n",
      "Epoch 921, Loss: 1.9233713448047638, Final Batch Loss: 0.3923588693141937\n",
      "Epoch 922, Loss: 2.0264451801776886, Final Batch Loss: 0.4485451877117157\n",
      "Epoch 923, Loss: 2.139380544424057, Final Batch Loss: 0.5175123810768127\n",
      "Epoch 924, Loss: 2.0350494384765625, Final Batch Loss: 0.5222981572151184\n",
      "Epoch 925, Loss: 1.9896701872348785, Final Batch Loss: 0.48456987738609314\n",
      "Epoch 926, Loss: 2.091134339570999, Final Batch Loss: 0.5984247326850891\n",
      "Epoch 927, Loss: 1.9889492392539978, Final Batch Loss: 0.5312955379486084\n",
      "Epoch 928, Loss: 1.8818284273147583, Final Batch Loss: 0.3668319880962372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 929, Loss: 2.070977032184601, Final Batch Loss: 0.5392004251480103\n",
      "Epoch 930, Loss: 1.9246518313884735, Final Batch Loss: 0.5074983835220337\n",
      "Epoch 931, Loss: 1.970315307378769, Final Batch Loss: 0.4962654113769531\n",
      "Epoch 932, Loss: 2.009764164686203, Final Batch Loss: 0.48988065123558044\n",
      "Epoch 933, Loss: 2.031801849603653, Final Batch Loss: 0.6259153485298157\n",
      "Epoch 934, Loss: 2.0880763232707977, Final Batch Loss: 0.4718043804168701\n",
      "Epoch 935, Loss: 2.0332356691360474, Final Batch Loss: 0.48453351855278015\n",
      "Epoch 936, Loss: 1.9309249818325043, Final Batch Loss: 0.4475960433483124\n",
      "Epoch 937, Loss: 2.1555413603782654, Final Batch Loss: 0.5624219179153442\n",
      "Epoch 938, Loss: 2.1217608749866486, Final Batch Loss: 0.61650151014328\n",
      "Epoch 939, Loss: 2.0328777730464935, Final Batch Loss: 0.6071756482124329\n",
      "Epoch 940, Loss: 1.9782530963420868, Final Batch Loss: 0.5005577206611633\n",
      "Epoch 941, Loss: 2.0661613047122955, Final Batch Loss: 0.5179552435874939\n",
      "Epoch 942, Loss: 1.9939778447151184, Final Batch Loss: 0.505771815776825\n",
      "Epoch 943, Loss: 2.0701733827590942, Final Batch Loss: 0.5588825345039368\n",
      "Epoch 944, Loss: 2.034004122018814, Final Batch Loss: 0.5251251459121704\n",
      "Epoch 945, Loss: 1.9847807586193085, Final Batch Loss: 0.47638216614723206\n",
      "Epoch 946, Loss: 1.9636741280555725, Final Batch Loss: 0.49948784708976746\n",
      "Epoch 947, Loss: 2.01493763923645, Final Batch Loss: 0.4402385652065277\n",
      "Epoch 948, Loss: 1.9899951219558716, Final Batch Loss: 0.5258448719978333\n",
      "Epoch 949, Loss: 1.9637513160705566, Final Batch Loss: 0.5811852812767029\n",
      "Epoch 950, Loss: 2.0689883828163147, Final Batch Loss: 0.5112358331680298\n",
      "Epoch 951, Loss: 2.068278670310974, Final Batch Loss: 0.551182746887207\n",
      "Epoch 952, Loss: 2.0923984348773956, Final Batch Loss: 0.49898552894592285\n",
      "Epoch 953, Loss: 1.9732950329780579, Final Batch Loss: 0.4369884431362152\n",
      "Epoch 954, Loss: 2.0079499185085297, Final Batch Loss: 0.48239850997924805\n",
      "Epoch 955, Loss: 2.0416560769081116, Final Batch Loss: 0.5749635100364685\n",
      "Epoch 956, Loss: 2.1597901582717896, Final Batch Loss: 0.6048580408096313\n",
      "Epoch 957, Loss: 2.140819013118744, Final Batch Loss: 0.6758979558944702\n",
      "Epoch 958, Loss: 1.9502777755260468, Final Batch Loss: 0.46067169308662415\n",
      "Epoch 959, Loss: 2.064469039440155, Final Batch Loss: 0.5063148140907288\n",
      "Epoch 960, Loss: 2.077605038881302, Final Batch Loss: 0.5464892983436584\n",
      "Epoch 961, Loss: 1.8986765444278717, Final Batch Loss: 0.47379645705223083\n",
      "Epoch 962, Loss: 2.150334894657135, Final Batch Loss: 0.5792258977890015\n",
      "Epoch 963, Loss: 2.014580637216568, Final Batch Loss: 0.4349459707736969\n",
      "Epoch 964, Loss: 1.9220308363437653, Final Batch Loss: 0.4245939552783966\n",
      "Epoch 965, Loss: 2.0488007068634033, Final Batch Loss: 0.5634975433349609\n",
      "Epoch 966, Loss: 2.0381839871406555, Final Batch Loss: 0.6027472615242004\n",
      "Epoch 967, Loss: 1.9613855183124542, Final Batch Loss: 0.515674889087677\n",
      "Epoch 968, Loss: 1.9877735078334808, Final Batch Loss: 0.4565238654613495\n",
      "Epoch 969, Loss: 2.082063764333725, Final Batch Loss: 0.5236623883247375\n",
      "Epoch 970, Loss: 1.9315005540847778, Final Batch Loss: 0.5201241970062256\n",
      "Epoch 971, Loss: 2.100383073091507, Final Batch Loss: 0.6126089096069336\n",
      "Epoch 972, Loss: 2.009458839893341, Final Batch Loss: 0.450668066740036\n",
      "Epoch 973, Loss: 2.0027762055397034, Final Batch Loss: 0.4599810838699341\n",
      "Epoch 974, Loss: 1.942947894334793, Final Batch Loss: 0.4600893259048462\n",
      "Epoch 975, Loss: 1.8653704524040222, Final Batch Loss: 0.32599350810050964\n",
      "Epoch 976, Loss: 1.9109413027763367, Final Batch Loss: 0.40163302421569824\n",
      "Epoch 977, Loss: 1.871512085199356, Final Batch Loss: 0.4982762038707733\n",
      "Epoch 978, Loss: 2.1636279821395874, Final Batch Loss: 0.5473393201828003\n",
      "Epoch 979, Loss: 2.0060209035873413, Final Batch Loss: 0.5234988331794739\n",
      "Epoch 980, Loss: 1.972511649131775, Final Batch Loss: 0.5121920704841614\n",
      "Epoch 981, Loss: 1.9321743547916412, Final Batch Loss: 0.4112277925014496\n",
      "Epoch 982, Loss: 1.9629373252391815, Final Batch Loss: 0.41590043902397156\n",
      "Epoch 983, Loss: 1.9356615841388702, Final Batch Loss: 0.516026496887207\n",
      "Epoch 984, Loss: 2.032542735338211, Final Batch Loss: 0.4931241273880005\n",
      "Epoch 985, Loss: 1.9266953468322754, Final Batch Loss: 0.4953320324420929\n",
      "Epoch 986, Loss: 1.889723300933838, Final Batch Loss: 0.4168451726436615\n",
      "Epoch 987, Loss: 2.0932035744190216, Final Batch Loss: 0.583716094493866\n",
      "Epoch 988, Loss: 1.8759796917438507, Final Batch Loss: 0.36455315351486206\n",
      "Epoch 989, Loss: 1.9649621546268463, Final Batch Loss: 0.5345218777656555\n",
      "Epoch 990, Loss: 1.9863701462745667, Final Batch Loss: 0.5186011791229248\n",
      "Epoch 991, Loss: 2.0852873623371124, Final Batch Loss: 0.534975528717041\n",
      "Epoch 992, Loss: 2.10665500164032, Final Batch Loss: 0.5272747278213501\n",
      "Epoch 993, Loss: 1.9419388473033905, Final Batch Loss: 0.516120195388794\n",
      "Epoch 994, Loss: 2.018690526485443, Final Batch Loss: 0.5708838105201721\n",
      "Epoch 995, Loss: 1.9813385009765625, Final Batch Loss: 0.5021368265151978\n",
      "Epoch 996, Loss: 1.8678713142871857, Final Batch Loss: 0.4727519750595093\n",
      "Epoch 997, Loss: 1.952297955751419, Final Batch Loss: 0.43301835656166077\n",
      "Epoch 998, Loss: 2.061621308326721, Final Batch Loss: 0.5244925618171692\n",
      "Epoch 999, Loss: 2.019941806793213, Final Batch Loss: 0.48807600140571594\n",
      "Epoch 1000, Loss: 2.0151359736919403, Final Batch Loss: 0.5567063093185425\n",
      "Epoch 1001, Loss: 1.9646108448505402, Final Batch Loss: 0.526523232460022\n",
      "Epoch 1002, Loss: 1.9720911085605621, Final Batch Loss: 0.5245652198791504\n",
      "Epoch 1003, Loss: 2.107802450656891, Final Batch Loss: 0.6059690713882446\n",
      "Epoch 1004, Loss: 2.171027958393097, Final Batch Loss: 0.5990845561027527\n",
      "Epoch 1005, Loss: 2.077525109052658, Final Batch Loss: 0.5056564211845398\n",
      "Epoch 1006, Loss: 1.9482164680957794, Final Batch Loss: 0.42567145824432373\n",
      "Epoch 1007, Loss: 2.0304223001003265, Final Batch Loss: 0.5836194157600403\n",
      "Epoch 1008, Loss: 2.0840304493904114, Final Batch Loss: 0.6698011755943298\n",
      "Epoch 1009, Loss: 1.9231364727020264, Final Batch Loss: 0.473097562789917\n",
      "Epoch 1010, Loss: 1.9429903328418732, Final Batch Loss: 0.5044735670089722\n",
      "Epoch 1011, Loss: 1.9957585036754608, Final Batch Loss: 0.5061953067779541\n",
      "Epoch 1012, Loss: 2.0001589357852936, Final Batch Loss: 0.47244760394096375\n",
      "Epoch 1013, Loss: 1.9334852397441864, Final Batch Loss: 0.4849070906639099\n",
      "Epoch 1014, Loss: 1.9499082565307617, Final Batch Loss: 0.48309338092803955\n",
      "Epoch 1015, Loss: 2.0386085510253906, Final Batch Loss: 0.541050136089325\n",
      "Epoch 1016, Loss: 2.0508095622062683, Final Batch Loss: 0.5231156349182129\n",
      "Epoch 1017, Loss: 1.9922709465026855, Final Batch Loss: 0.5124906301498413\n",
      "Epoch 1018, Loss: 2.0932815074920654, Final Batch Loss: 0.6271399855613708\n",
      "Epoch 1019, Loss: 1.83974289894104, Final Batch Loss: 0.43532633781433105\n",
      "Epoch 1020, Loss: 1.9892017245292664, Final Batch Loss: 0.5515518188476562\n",
      "Epoch 1021, Loss: 1.9755114912986755, Final Batch Loss: 0.42529627680778503\n",
      "Epoch 1022, Loss: 1.918375700712204, Final Batch Loss: 0.4834708869457245\n",
      "Epoch 1023, Loss: 2.0878853797912598, Final Batch Loss: 0.6356285214424133\n",
      "Epoch 1024, Loss: 1.9043031930923462, Final Batch Loss: 0.4237295389175415\n",
      "Epoch 1025, Loss: 2.0014944076538086, Final Batch Loss: 0.5284267663955688\n",
      "Epoch 1026, Loss: 1.848955750465393, Final Batch Loss: 0.44050291180610657\n",
      "Epoch 1027, Loss: 2.0562944412231445, Final Batch Loss: 0.500577986240387\n",
      "Epoch 1028, Loss: 2.021970212459564, Final Batch Loss: 0.48394113779067993\n",
      "Epoch 1029, Loss: 1.9080855250358582, Final Batch Loss: 0.5532257556915283\n",
      "Epoch 1030, Loss: 1.8834816813468933, Final Batch Loss: 0.39328497648239136\n",
      "Epoch 1031, Loss: 1.8211047649383545, Final Batch Loss: 0.41114869713783264\n",
      "Epoch 1032, Loss: 1.9378799200057983, Final Batch Loss: 0.49550139904022217\n",
      "Epoch 1033, Loss: 2.0377027094364166, Final Batch Loss: 0.528188169002533\n",
      "Epoch 1034, Loss: 2.0025765001773834, Final Batch Loss: 0.500534176826477\n",
      "Epoch 1035, Loss: 1.7994028329849243, Final Batch Loss: 0.3352971374988556\n",
      "Epoch 1036, Loss: 1.9336074590682983, Final Batch Loss: 0.44679906964302063\n",
      "Epoch 1037, Loss: 1.9929872155189514, Final Batch Loss: 0.5577642917633057\n",
      "Epoch 1038, Loss: 1.9318107962608337, Final Batch Loss: 0.4697881042957306\n",
      "Epoch 1039, Loss: 1.9175437688827515, Final Batch Loss: 0.5812523365020752\n",
      "Epoch 1040, Loss: 1.948271930217743, Final Batch Loss: 0.4872828423976898\n",
      "Epoch 1041, Loss: 1.9654083251953125, Final Batch Loss: 0.47826820611953735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1042, Loss: 1.9504662454128265, Final Batch Loss: 0.49691784381866455\n",
      "Epoch 1043, Loss: 1.9021387994289398, Final Batch Loss: 0.41699379682540894\n",
      "Epoch 1044, Loss: 1.967454880475998, Final Batch Loss: 0.4811722934246063\n",
      "Epoch 1045, Loss: 1.8538658618927002, Final Batch Loss: 0.4699970483779907\n",
      "Epoch 1046, Loss: 1.9970867335796356, Final Batch Loss: 0.4973130524158478\n",
      "Epoch 1047, Loss: 1.8788581490516663, Final Batch Loss: 0.4399753510951996\n",
      "Epoch 1048, Loss: 1.9366008341312408, Final Batch Loss: 0.491482138633728\n",
      "Epoch 1049, Loss: 1.91401207447052, Final Batch Loss: 0.5000459551811218\n",
      "Epoch 1050, Loss: 1.8428449928760529, Final Batch Loss: 0.41743215918540955\n",
      "Epoch 1051, Loss: 1.9192788898944855, Final Batch Loss: 0.42615455389022827\n",
      "Epoch 1052, Loss: 1.876668006181717, Final Batch Loss: 0.41891077160835266\n",
      "Epoch 1053, Loss: 1.8996155560016632, Final Batch Loss: 0.5008946657180786\n",
      "Epoch 1054, Loss: 1.8996007144451141, Final Batch Loss: 0.49381765723228455\n",
      "Epoch 1055, Loss: 1.9636703133583069, Final Batch Loss: 0.5639418363571167\n",
      "Epoch 1056, Loss: 1.878115564584732, Final Batch Loss: 0.47056785225868225\n",
      "Epoch 1057, Loss: 2.0279069244861603, Final Batch Loss: 0.4868619740009308\n",
      "Epoch 1058, Loss: 1.9771590828895569, Final Batch Loss: 0.5400323271751404\n",
      "Epoch 1059, Loss: 1.82666477560997, Final Batch Loss: 0.4695224463939667\n",
      "Epoch 1060, Loss: 1.9592759311199188, Final Batch Loss: 0.48586371541023254\n",
      "Epoch 1061, Loss: 1.9138813316822052, Final Batch Loss: 0.44640448689460754\n",
      "Epoch 1062, Loss: 1.8988831639289856, Final Batch Loss: 0.44128820300102234\n",
      "Epoch 1063, Loss: 1.8078155815601349, Final Batch Loss: 0.3575904071331024\n",
      "Epoch 1064, Loss: 1.949280321598053, Final Batch Loss: 0.5698994994163513\n",
      "Epoch 1065, Loss: 2.123461425304413, Final Batch Loss: 0.6191462874412537\n",
      "Epoch 1066, Loss: 1.7933272123336792, Final Batch Loss: 0.46694499254226685\n",
      "Epoch 1067, Loss: 1.8747650980949402, Final Batch Loss: 0.4789625108242035\n",
      "Epoch 1068, Loss: 1.9941965639591217, Final Batch Loss: 0.5243434309959412\n",
      "Epoch 1069, Loss: 1.9235219657421112, Final Batch Loss: 0.5257801413536072\n",
      "Epoch 1070, Loss: 1.7524675726890564, Final Batch Loss: 0.43241339921951294\n",
      "Epoch 1071, Loss: 1.9355744421482086, Final Batch Loss: 0.4084392786026001\n",
      "Epoch 1072, Loss: 1.8835178315639496, Final Batch Loss: 0.4121260643005371\n",
      "Epoch 1073, Loss: 2.029493749141693, Final Batch Loss: 0.5587833523750305\n",
      "Epoch 1074, Loss: 1.9281294643878937, Final Batch Loss: 0.47440242767333984\n",
      "Epoch 1075, Loss: 1.872609168291092, Final Batch Loss: 0.4093888998031616\n",
      "Epoch 1076, Loss: 1.9932645857334137, Final Batch Loss: 0.44883114099502563\n",
      "Epoch 1077, Loss: 1.876953363418579, Final Batch Loss: 0.49331164360046387\n",
      "Epoch 1078, Loss: 1.9110815823078156, Final Batch Loss: 0.5337541699409485\n",
      "Epoch 1079, Loss: 1.8974886238574982, Final Batch Loss: 0.5175555944442749\n",
      "Epoch 1080, Loss: 2.0204881131649017, Final Batch Loss: 0.5840342044830322\n",
      "Epoch 1081, Loss: 1.9574664235115051, Final Batch Loss: 0.49929890036582947\n",
      "Epoch 1082, Loss: 1.937561184167862, Final Batch Loss: 0.520164430141449\n",
      "Epoch 1083, Loss: 1.7733263075351715, Final Batch Loss: 0.43703633546829224\n",
      "Epoch 1084, Loss: 1.8544729053974152, Final Batch Loss: 0.4091488718986511\n",
      "Epoch 1085, Loss: 1.885996401309967, Final Batch Loss: 0.46932852268218994\n",
      "Epoch 1086, Loss: 1.9379664659500122, Final Batch Loss: 0.4757949709892273\n",
      "Epoch 1087, Loss: 1.8694632351398468, Final Batch Loss: 0.4939216077327728\n",
      "Epoch 1088, Loss: 1.9863802194595337, Final Batch Loss: 0.534741997718811\n",
      "Epoch 1089, Loss: 1.802986592054367, Final Batch Loss: 0.42197340726852417\n",
      "Epoch 1090, Loss: 1.924925535917282, Final Batch Loss: 0.5087074041366577\n",
      "Epoch 1091, Loss: 1.895699292421341, Final Batch Loss: 0.5223093032836914\n",
      "Epoch 1092, Loss: 1.7704437971115112, Final Batch Loss: 0.38404226303100586\n",
      "Epoch 1093, Loss: 1.9682622849941254, Final Batch Loss: 0.5016435980796814\n",
      "Epoch 1094, Loss: 1.9278430342674255, Final Batch Loss: 0.4880160689353943\n",
      "Epoch 1095, Loss: 1.883571743965149, Final Batch Loss: 0.4564950466156006\n",
      "Epoch 1096, Loss: 1.985053926706314, Final Batch Loss: 0.5229743719100952\n",
      "Epoch 1097, Loss: 1.7077540457248688, Final Batch Loss: 0.4298674166202545\n",
      "Epoch 1098, Loss: 1.964576244354248, Final Batch Loss: 0.560428261756897\n",
      "Epoch 1099, Loss: 1.9853984713554382, Final Batch Loss: 0.40042543411254883\n",
      "Epoch 1100, Loss: 1.886089712381363, Final Batch Loss: 0.4334057867527008\n",
      "Epoch 1101, Loss: 1.9020841717720032, Final Batch Loss: 0.4818268418312073\n",
      "Epoch 1102, Loss: 1.8898205757141113, Final Batch Loss: 0.4154947102069855\n",
      "Epoch 1103, Loss: 2.0118846595287323, Final Batch Loss: 0.6186420321464539\n",
      "Epoch 1104, Loss: 1.7980502545833588, Final Batch Loss: 0.3917973041534424\n",
      "Epoch 1105, Loss: 1.8636478781700134, Final Batch Loss: 0.4591987431049347\n",
      "Epoch 1106, Loss: 1.9745187759399414, Final Batch Loss: 0.40415075421333313\n",
      "Epoch 1107, Loss: 1.842767745256424, Final Batch Loss: 0.43502703309059143\n",
      "Epoch 1108, Loss: 1.9385941326618195, Final Batch Loss: 0.6314005255699158\n",
      "Epoch 1109, Loss: 1.9565125107765198, Final Batch Loss: 0.5668486952781677\n",
      "Epoch 1110, Loss: 1.7850392758846283, Final Batch Loss: 0.44013842940330505\n",
      "Epoch 1111, Loss: 1.6942519843578339, Final Batch Loss: 0.33057665824890137\n",
      "Epoch 1112, Loss: 1.945264607667923, Final Batch Loss: 0.5117860436439514\n",
      "Epoch 1113, Loss: 1.886953443288803, Final Batch Loss: 0.5219589471817017\n",
      "Epoch 1114, Loss: 1.854179322719574, Final Batch Loss: 0.48182931542396545\n",
      "Epoch 1115, Loss: 1.8871601223945618, Final Batch Loss: 0.4881729781627655\n",
      "Epoch 1116, Loss: 1.7590954303741455, Final Batch Loss: 0.3806031346321106\n",
      "Epoch 1117, Loss: 1.8442652523517609, Final Batch Loss: 0.5386083126068115\n",
      "Epoch 1118, Loss: 1.9047427475452423, Final Batch Loss: 0.4431583881378174\n",
      "Epoch 1119, Loss: 1.9145398139953613, Final Batch Loss: 0.5496811866760254\n",
      "Epoch 1120, Loss: 1.9843169748783112, Final Batch Loss: 0.6168429255485535\n",
      "Epoch 1121, Loss: 1.780998170375824, Final Batch Loss: 0.4181111752986908\n",
      "Epoch 1122, Loss: 1.7585120797157288, Final Batch Loss: 0.4177148938179016\n",
      "Epoch 1123, Loss: 1.8810789287090302, Final Batch Loss: 0.5101746916770935\n",
      "Epoch 1124, Loss: 1.7792924642562866, Final Batch Loss: 0.4165104329586029\n",
      "Epoch 1125, Loss: 1.8827436864376068, Final Batch Loss: 0.4125251770019531\n",
      "Epoch 1126, Loss: 1.882165938615799, Final Batch Loss: 0.47332698106765747\n",
      "Epoch 1127, Loss: 1.7949369549751282, Final Batch Loss: 0.41526684165000916\n",
      "Epoch 1128, Loss: 1.9854508340358734, Final Batch Loss: 0.6704313158988953\n",
      "Epoch 1129, Loss: 1.8776319324970245, Final Batch Loss: 0.5713109970092773\n",
      "Epoch 1130, Loss: 1.8772733211517334, Final Batch Loss: 0.48966458439826965\n",
      "Epoch 1131, Loss: 1.8594587445259094, Final Batch Loss: 0.5110239386558533\n",
      "Epoch 1132, Loss: 1.7878274023532867, Final Batch Loss: 0.42772233486175537\n",
      "Epoch 1133, Loss: 1.9444954097270966, Final Batch Loss: 0.5875817537307739\n",
      "Epoch 1134, Loss: 1.7439309656620026, Final Batch Loss: 0.4129869043827057\n",
      "Epoch 1135, Loss: 1.7198225557804108, Final Batch Loss: 0.44509145617485046\n",
      "Epoch 1136, Loss: 1.877560794353485, Final Batch Loss: 0.5063410401344299\n",
      "Epoch 1137, Loss: 1.814219981431961, Final Batch Loss: 0.5043714642524719\n",
      "Epoch 1138, Loss: 1.7964579164981842, Final Batch Loss: 0.3760550916194916\n",
      "Epoch 1139, Loss: 1.812173753976822, Final Batch Loss: 0.45515549182891846\n",
      "Epoch 1140, Loss: 1.8439810574054718, Final Batch Loss: 0.48459744453430176\n",
      "Epoch 1141, Loss: 1.8684354424476624, Final Batch Loss: 0.49099087715148926\n",
      "Epoch 1142, Loss: 1.8349778652191162, Final Batch Loss: 0.44030237197875977\n",
      "Epoch 1143, Loss: 1.7485459446907043, Final Batch Loss: 0.3758324384689331\n",
      "Epoch 1144, Loss: 1.8048616349697113, Final Batch Loss: 0.46727848052978516\n",
      "Epoch 1145, Loss: 1.8707104325294495, Final Batch Loss: 0.47787776589393616\n",
      "Epoch 1146, Loss: 1.7917331457138062, Final Batch Loss: 0.4443853497505188\n",
      "Epoch 1147, Loss: 1.7475100457668304, Final Batch Loss: 0.416048526763916\n",
      "Epoch 1148, Loss: 1.7514122128486633, Final Batch Loss: 0.3573303520679474\n",
      "Epoch 1149, Loss: 1.7906893491744995, Final Batch Loss: 0.49184462428092957\n",
      "Epoch 1150, Loss: 1.8390240669250488, Final Batch Loss: 0.41147929430007935\n",
      "Epoch 1151, Loss: 1.744614452123642, Final Batch Loss: 0.37798622250556946\n",
      "Epoch 1152, Loss: 1.7951841056346893, Final Batch Loss: 0.39727118611335754\n",
      "Epoch 1153, Loss: 1.9307285845279694, Final Batch Loss: 0.5418192744255066\n",
      "Epoch 1154, Loss: 1.7622174620628357, Final Batch Loss: 0.42833349108695984\n",
      "Epoch 1155, Loss: 1.8762261271476746, Final Batch Loss: 0.5619661211967468\n",
      "Epoch 1156, Loss: 1.7622060477733612, Final Batch Loss: 0.4454187750816345\n",
      "Epoch 1157, Loss: 1.7604467868804932, Final Batch Loss: 0.4202469289302826\n",
      "Epoch 1158, Loss: 1.9114418923854828, Final Batch Loss: 0.6401805877685547\n",
      "Epoch 1159, Loss: 1.731079787015915, Final Batch Loss: 0.3516484200954437\n",
      "Epoch 1160, Loss: 1.8761389255523682, Final Batch Loss: 0.44957536458969116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1161, Loss: 1.9286410212516785, Final Batch Loss: 0.5601099133491516\n",
      "Epoch 1162, Loss: 1.835055559873581, Final Batch Loss: 0.43533962965011597\n",
      "Epoch 1163, Loss: 1.8602876961231232, Final Batch Loss: 0.37227895855903625\n",
      "Epoch 1164, Loss: 1.789898306131363, Final Batch Loss: 0.4385421574115753\n",
      "Epoch 1165, Loss: 1.9574761986732483, Final Batch Loss: 0.4512614607810974\n",
      "Epoch 1166, Loss: 1.828016072511673, Final Batch Loss: 0.42242568731307983\n",
      "Epoch 1167, Loss: 1.8762714862823486, Final Batch Loss: 0.5135125517845154\n",
      "Epoch 1168, Loss: 1.7349957823753357, Final Batch Loss: 0.33810189366340637\n",
      "Epoch 1169, Loss: 1.7491282224655151, Final Batch Loss: 0.4256442189216614\n",
      "Epoch 1170, Loss: 1.777224361896515, Final Batch Loss: 0.36980998516082764\n",
      "Epoch 1171, Loss: 1.860939383506775, Final Batch Loss: 0.4998263418674469\n",
      "Epoch 1172, Loss: 1.914916217327118, Final Batch Loss: 0.49041005969047546\n",
      "Epoch 1173, Loss: 1.9091145694255829, Final Batch Loss: 0.5582461357116699\n",
      "Epoch 1174, Loss: 1.7195890247821808, Final Batch Loss: 0.43855738639831543\n",
      "Epoch 1175, Loss: 1.7844164371490479, Final Batch Loss: 0.42504966259002686\n",
      "Epoch 1176, Loss: 1.7987674474716187, Final Batch Loss: 0.3579656183719635\n",
      "Epoch 1177, Loss: 1.7683034837245941, Final Batch Loss: 0.47195127606391907\n",
      "Epoch 1178, Loss: 1.7716421484947205, Final Batch Loss: 0.37910720705986023\n",
      "Epoch 1179, Loss: 1.7308045327663422, Final Batch Loss: 0.42670851945877075\n",
      "Epoch 1180, Loss: 1.7382881045341492, Final Batch Loss: 0.35596510767936707\n",
      "Epoch 1181, Loss: 1.9057993292808533, Final Batch Loss: 0.4063577950000763\n",
      "Epoch 1182, Loss: 1.7489993572235107, Final Batch Loss: 0.3251378834247589\n",
      "Epoch 1183, Loss: 1.7255405485630035, Final Batch Loss: 0.395875483751297\n",
      "Epoch 1184, Loss: 1.7131273448467255, Final Batch Loss: 0.3669061064720154\n",
      "Epoch 1185, Loss: 1.8925627768039703, Final Batch Loss: 0.4723063111305237\n",
      "Epoch 1186, Loss: 1.8231561481952667, Final Batch Loss: 0.3295848071575165\n",
      "Epoch 1187, Loss: 1.8785489797592163, Final Batch Loss: 0.5481335520744324\n",
      "Epoch 1188, Loss: 1.909752070903778, Final Batch Loss: 0.453435480594635\n",
      "Epoch 1189, Loss: 1.8276695311069489, Final Batch Loss: 0.44058704376220703\n",
      "Epoch 1190, Loss: 1.8436115980148315, Final Batch Loss: 0.4351865351200104\n",
      "Epoch 1191, Loss: 1.871253341436386, Final Batch Loss: 0.5159995555877686\n",
      "Epoch 1192, Loss: 1.7980089485645294, Final Batch Loss: 0.38712581992149353\n",
      "Epoch 1193, Loss: 1.9086566269397736, Final Batch Loss: 0.4508214592933655\n",
      "Epoch 1194, Loss: 1.8517628014087677, Final Batch Loss: 0.5308079123497009\n",
      "Epoch 1195, Loss: 1.818558156490326, Final Batch Loss: 0.43013256788253784\n",
      "Epoch 1196, Loss: 1.9035006165504456, Final Batch Loss: 0.570163905620575\n",
      "Epoch 1197, Loss: 1.9740404784679413, Final Batch Loss: 0.5494218468666077\n",
      "Epoch 1198, Loss: 1.7130915522575378, Final Batch Loss: 0.3648395836353302\n",
      "Epoch 1199, Loss: 1.7918365597724915, Final Batch Loss: 0.5525032877922058\n",
      "Epoch 1200, Loss: 1.8658878803253174, Final Batch Loss: 0.5648726224899292\n",
      "Epoch 1201, Loss: 1.7492352426052094, Final Batch Loss: 0.4592268764972687\n",
      "Epoch 1202, Loss: 1.7526870667934418, Final Batch Loss: 0.4519820511341095\n",
      "Epoch 1203, Loss: 1.8786223530769348, Final Batch Loss: 0.5087242722511292\n",
      "Epoch 1204, Loss: 1.76905557513237, Final Batch Loss: 0.3872838020324707\n",
      "Epoch 1205, Loss: 1.8180301487445831, Final Batch Loss: 0.4478350579738617\n",
      "Epoch 1206, Loss: 1.927123874425888, Final Batch Loss: 0.5196725726127625\n",
      "Epoch 1207, Loss: 1.82228422164917, Final Batch Loss: 0.3781031370162964\n",
      "Epoch 1208, Loss: 1.799171894788742, Final Batch Loss: 0.4126461148262024\n",
      "Epoch 1209, Loss: 1.7837469577789307, Final Batch Loss: 0.5226403474807739\n",
      "Epoch 1210, Loss: 1.7138889729976654, Final Batch Loss: 0.4046182632446289\n",
      "Epoch 1211, Loss: 1.7942523956298828, Final Batch Loss: 0.5124318599700928\n",
      "Epoch 1212, Loss: 2.068922221660614, Final Batch Loss: 0.6215187311172485\n",
      "Epoch 1213, Loss: 1.8902854919433594, Final Batch Loss: 0.5106350183486938\n",
      "Epoch 1214, Loss: 1.8130092024803162, Final Batch Loss: 0.37701451778411865\n",
      "Epoch 1215, Loss: 1.8519432544708252, Final Batch Loss: 0.4345514178276062\n",
      "Epoch 1216, Loss: 1.815778136253357, Final Batch Loss: 0.48088908195495605\n",
      "Epoch 1217, Loss: 1.8072236478328705, Final Batch Loss: 0.5101337432861328\n",
      "Epoch 1218, Loss: 1.9215965270996094, Final Batch Loss: 0.5279656052589417\n",
      "Epoch 1219, Loss: 1.8491321206092834, Final Batch Loss: 0.40271925926208496\n",
      "Epoch 1220, Loss: 1.8739134669303894, Final Batch Loss: 0.46874502301216125\n",
      "Epoch 1221, Loss: 1.8046720623970032, Final Batch Loss: 0.38701093196868896\n",
      "Epoch 1222, Loss: 1.722719520330429, Final Batch Loss: 0.4325731098651886\n",
      "Epoch 1223, Loss: 1.7336194217205048, Final Batch Loss: 0.46796584129333496\n",
      "Epoch 1224, Loss: 1.7089679837226868, Final Batch Loss: 0.3694567382335663\n",
      "Epoch 1225, Loss: 1.785435527563095, Final Batch Loss: 0.47938331961631775\n",
      "Epoch 1226, Loss: 1.8065540790557861, Final Batch Loss: 0.41518735885620117\n",
      "Epoch 1227, Loss: 1.7348764836788177, Final Batch Loss: 0.37185168266296387\n",
      "Epoch 1228, Loss: 1.9587132930755615, Final Batch Loss: 0.49588218331336975\n",
      "Epoch 1229, Loss: 1.8289779424667358, Final Batch Loss: 0.40424594283103943\n",
      "Epoch 1230, Loss: 1.8267989754676819, Final Batch Loss: 0.501811683177948\n",
      "Epoch 1231, Loss: 1.909119576215744, Final Batch Loss: 0.5046072602272034\n",
      "Epoch 1232, Loss: 1.790309727191925, Final Batch Loss: 0.4647015631198883\n",
      "Epoch 1233, Loss: 1.6934226751327515, Final Batch Loss: 0.2971615195274353\n",
      "Epoch 1234, Loss: 1.9505113065242767, Final Batch Loss: 0.5489164590835571\n",
      "Epoch 1235, Loss: 1.6887229681015015, Final Batch Loss: 0.3949679434299469\n",
      "Epoch 1236, Loss: 1.783967912197113, Final Batch Loss: 0.3974413275718689\n",
      "Epoch 1237, Loss: 1.8468733131885529, Final Batch Loss: 0.4425516128540039\n",
      "Epoch 1238, Loss: 1.8224605023860931, Final Batch Loss: 0.44948017597198486\n",
      "Epoch 1239, Loss: 1.7610353529453278, Final Batch Loss: 0.4061894416809082\n",
      "Epoch 1240, Loss: 1.7730478942394257, Final Batch Loss: 0.40503132343292236\n",
      "Epoch 1241, Loss: 1.94370499253273, Final Batch Loss: 0.49349167943000793\n",
      "Epoch 1242, Loss: 1.7532603442668915, Final Batch Loss: 0.4330154061317444\n",
      "Epoch 1243, Loss: 1.7776912450790405, Final Batch Loss: 0.437800794839859\n",
      "Epoch 1244, Loss: 1.6673355996608734, Final Batch Loss: 0.3705326020717621\n",
      "Epoch 1245, Loss: 1.8367596566677094, Final Batch Loss: 0.48295748233795166\n",
      "Epoch 1246, Loss: 1.6826902329921722, Final Batch Loss: 0.3711177408695221\n",
      "Epoch 1247, Loss: 1.6278943717479706, Final Batch Loss: 0.3374077081680298\n",
      "Epoch 1248, Loss: 1.7225887775421143, Final Batch Loss: 0.45327529311180115\n",
      "Epoch 1249, Loss: 1.7282443344593048, Final Batch Loss: 0.3240637481212616\n",
      "Epoch 1250, Loss: 1.8362175524234772, Final Batch Loss: 0.5596131086349487\n",
      "Epoch 1251, Loss: 1.7958696484565735, Final Batch Loss: 0.4296938478946686\n",
      "Epoch 1252, Loss: 1.6554624438285828, Final Batch Loss: 0.36301249265670776\n",
      "Epoch 1253, Loss: 1.7621434330940247, Final Batch Loss: 0.39305126667022705\n",
      "Epoch 1254, Loss: 1.7994115948677063, Final Batch Loss: 0.5700584650039673\n",
      "Epoch 1255, Loss: 1.7821345925331116, Final Batch Loss: 0.3876052498817444\n",
      "Epoch 1256, Loss: 1.7129709720611572, Final Batch Loss: 0.3891843855381012\n",
      "Epoch 1257, Loss: 1.7369555234909058, Final Batch Loss: 0.4338301718235016\n",
      "Epoch 1258, Loss: 1.7608579695224762, Final Batch Loss: 0.34457048773765564\n",
      "Epoch 1259, Loss: 1.8320176899433136, Final Batch Loss: 0.4963531196117401\n",
      "Epoch 1260, Loss: 1.919555515050888, Final Batch Loss: 0.5680049061775208\n",
      "Epoch 1261, Loss: 1.7969302833080292, Final Batch Loss: 0.3444536626338959\n",
      "Epoch 1262, Loss: 1.8956820964813232, Final Batch Loss: 0.46471473574638367\n",
      "Epoch 1263, Loss: 1.7540122866630554, Final Batch Loss: 0.446645587682724\n",
      "Epoch 1264, Loss: 1.8342730104923248, Final Batch Loss: 0.4907754957675934\n",
      "Epoch 1265, Loss: 1.8113426864147186, Final Batch Loss: 0.5140480995178223\n",
      "Epoch 1266, Loss: 1.813078224658966, Final Batch Loss: 0.4725589156150818\n",
      "Epoch 1267, Loss: 1.95185387134552, Final Batch Loss: 0.587867259979248\n",
      "Epoch 1268, Loss: 1.809197574853897, Final Batch Loss: 0.4761812388896942\n",
      "Epoch 1269, Loss: 1.6283444166183472, Final Batch Loss: 0.36520540714263916\n",
      "Epoch 1270, Loss: 1.8305632770061493, Final Batch Loss: 0.39150941371917725\n",
      "Epoch 1271, Loss: 1.8378623723983765, Final Batch Loss: 0.45154839754104614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1272, Loss: 1.8873388767242432, Final Batch Loss: 0.5053497552871704\n",
      "Epoch 1273, Loss: 1.7054831683635712, Final Batch Loss: 0.3688426911830902\n",
      "Epoch 1274, Loss: 1.7020370960235596, Final Batch Loss: 0.4225527346134186\n",
      "Epoch 1275, Loss: 1.7305618822574615, Final Batch Loss: 0.4273111820220947\n",
      "Epoch 1276, Loss: 1.7470042705535889, Final Batch Loss: 0.4009586274623871\n",
      "Epoch 1277, Loss: 1.6861259639263153, Final Batch Loss: 0.42883047461509705\n",
      "Epoch 1278, Loss: 1.8176754415035248, Final Batch Loss: 0.36609703302383423\n",
      "Epoch 1279, Loss: 1.8389365375041962, Final Batch Loss: 0.5856107473373413\n",
      "Epoch 1280, Loss: 1.830595076084137, Final Batch Loss: 0.45805269479751587\n",
      "Epoch 1281, Loss: 1.829874187707901, Final Batch Loss: 0.5437374114990234\n",
      "Epoch 1282, Loss: 1.764166384935379, Final Batch Loss: 0.4506389796733856\n",
      "Epoch 1283, Loss: 1.8072946071624756, Final Batch Loss: 0.48829618096351624\n",
      "Epoch 1284, Loss: 1.896592229604721, Final Batch Loss: 0.43951770663261414\n",
      "Epoch 1285, Loss: 1.8256414234638214, Final Batch Loss: 0.5317742824554443\n",
      "Epoch 1286, Loss: 1.7756651043891907, Final Batch Loss: 0.49015483260154724\n",
      "Epoch 1287, Loss: 1.7393796741962433, Final Batch Loss: 0.3624742031097412\n",
      "Epoch 1288, Loss: 1.7382935881614685, Final Batch Loss: 0.42969346046447754\n",
      "Epoch 1289, Loss: 1.7144708633422852, Final Batch Loss: 0.4438961446285248\n",
      "Epoch 1290, Loss: 1.7128978371620178, Final Batch Loss: 0.4174661636352539\n",
      "Epoch 1291, Loss: 1.7456795871257782, Final Batch Loss: 0.45708030462265015\n",
      "Epoch 1292, Loss: 1.6614777147769928, Final Batch Loss: 0.45458346605300903\n",
      "Epoch 1293, Loss: 1.7558253705501556, Final Batch Loss: 0.44583356380462646\n",
      "Epoch 1294, Loss: 1.8118467330932617, Final Batch Loss: 0.44417205452919006\n",
      "Epoch 1295, Loss: 1.740722119808197, Final Batch Loss: 0.42113223671913147\n",
      "Epoch 1296, Loss: 1.6130458116531372, Final Batch Loss: 0.37003564834594727\n",
      "Epoch 1297, Loss: 1.699591487646103, Final Batch Loss: 0.35712677240371704\n",
      "Epoch 1298, Loss: 1.7010902762413025, Final Batch Loss: 0.37816792726516724\n",
      "Epoch 1299, Loss: 1.729148507118225, Final Batch Loss: 0.4658084213733673\n",
      "Epoch 1300, Loss: 1.8600014746189117, Final Batch Loss: 0.5785605907440186\n",
      "Epoch 1301, Loss: 1.7024074792861938, Final Batch Loss: 0.35943689942359924\n",
      "Epoch 1302, Loss: 1.6215208172798157, Final Batch Loss: 0.35270410776138306\n",
      "Epoch 1303, Loss: 1.7960258424282074, Final Batch Loss: 0.4824688732624054\n",
      "Epoch 1304, Loss: 1.676554799079895, Final Batch Loss: 0.3787915110588074\n",
      "Epoch 1305, Loss: 1.7094047367572784, Final Batch Loss: 0.47547346353530884\n",
      "Epoch 1306, Loss: 1.8065393567085266, Final Batch Loss: 0.4473477303981781\n",
      "Epoch 1307, Loss: 1.6500744819641113, Final Batch Loss: 0.36965155601501465\n",
      "Epoch 1308, Loss: 1.7170355916023254, Final Batch Loss: 0.43977272510528564\n",
      "Epoch 1309, Loss: 1.7571442425251007, Final Batch Loss: 0.4200201630592346\n",
      "Epoch 1310, Loss: 1.7183551788330078, Final Batch Loss: 0.33893218636512756\n",
      "Epoch 1311, Loss: 1.778502881526947, Final Batch Loss: 0.40771204233169556\n",
      "Epoch 1312, Loss: 1.7731431126594543, Final Batch Loss: 0.4242752492427826\n",
      "Epoch 1313, Loss: 1.8349902629852295, Final Batch Loss: 0.44329777359962463\n",
      "Epoch 1314, Loss: 1.7522745728492737, Final Batch Loss: 0.48185470700263977\n",
      "Epoch 1315, Loss: 1.6289745569229126, Final Batch Loss: 0.42728516459465027\n",
      "Epoch 1316, Loss: 1.7748280763626099, Final Batch Loss: 0.4519924223423004\n",
      "Epoch 1317, Loss: 1.7773822247982025, Final Batch Loss: 0.55289226770401\n",
      "Epoch 1318, Loss: 1.8476587533950806, Final Batch Loss: 0.5202851295471191\n",
      "Epoch 1319, Loss: 1.746319741010666, Final Batch Loss: 0.40279993414878845\n",
      "Epoch 1320, Loss: 1.7979313135147095, Final Batch Loss: 0.4630820155143738\n",
      "Epoch 1321, Loss: 1.7183850705623627, Final Batch Loss: 0.3463779389858246\n",
      "Epoch 1322, Loss: 1.786382645368576, Final Batch Loss: 0.4916406273841858\n",
      "Epoch 1323, Loss: 1.6217376291751862, Final Batch Loss: 0.4369686245918274\n",
      "Epoch 1324, Loss: 1.7070260047912598, Final Batch Loss: 0.3792853355407715\n",
      "Epoch 1325, Loss: 1.6912218034267426, Final Batch Loss: 0.43583086133003235\n",
      "Epoch 1326, Loss: 1.6976368129253387, Final Batch Loss: 0.4431494176387787\n",
      "Epoch 1327, Loss: 1.775122344493866, Final Batch Loss: 0.5160297751426697\n",
      "Epoch 1328, Loss: 1.8355379104614258, Final Batch Loss: 0.4728759527206421\n",
      "Epoch 1329, Loss: 1.7582845091819763, Final Batch Loss: 0.45658034086227417\n",
      "Epoch 1330, Loss: 1.7443138360977173, Final Batch Loss: 0.5197476148605347\n",
      "Epoch 1331, Loss: 1.7528751194477081, Final Batch Loss: 0.5618359446525574\n",
      "Epoch 1332, Loss: 1.5947401821613312, Final Batch Loss: 0.31146177649497986\n",
      "Epoch 1333, Loss: 1.558239996433258, Final Batch Loss: 0.34605637192726135\n",
      "Epoch 1334, Loss: 1.7164096236228943, Final Batch Loss: 0.42141440510749817\n",
      "Epoch 1335, Loss: 1.734489530324936, Final Batch Loss: 0.4790509343147278\n",
      "Epoch 1336, Loss: 1.706202745437622, Final Batch Loss: 0.45234882831573486\n",
      "Epoch 1337, Loss: 1.6102880239486694, Final Batch Loss: 0.30232760310173035\n",
      "Epoch 1338, Loss: 1.689769685268402, Final Batch Loss: 0.38432079553604126\n",
      "Epoch 1339, Loss: 1.7676207721233368, Final Batch Loss: 0.3837004005908966\n",
      "Epoch 1340, Loss: 1.5894262194633484, Final Batch Loss: 0.30871936678886414\n",
      "Epoch 1341, Loss: 1.7012980282306671, Final Batch Loss: 0.40268418192863464\n",
      "Epoch 1342, Loss: 1.7881126701831818, Final Batch Loss: 0.4121808111667633\n",
      "Epoch 1343, Loss: 1.690086156129837, Final Batch Loss: 0.3338029086589813\n",
      "Epoch 1344, Loss: 1.7169995307922363, Final Batch Loss: 0.38720232248306274\n",
      "Epoch 1345, Loss: 1.9214662909507751, Final Batch Loss: 0.4561658501625061\n",
      "Epoch 1346, Loss: 1.7755463421344757, Final Batch Loss: 0.5052857398986816\n",
      "Epoch 1347, Loss: 1.7008762955665588, Final Batch Loss: 0.4109993577003479\n",
      "Epoch 1348, Loss: 1.6591005325317383, Final Batch Loss: 0.4815114438533783\n",
      "Epoch 1349, Loss: 1.7349726259708405, Final Batch Loss: 0.4806683361530304\n",
      "Epoch 1350, Loss: 1.6843609511852264, Final Batch Loss: 0.4103338122367859\n",
      "Epoch 1351, Loss: 1.5765066146850586, Final Batch Loss: 0.3373611569404602\n",
      "Epoch 1352, Loss: 1.6579511761665344, Final Batch Loss: 0.41350555419921875\n",
      "Epoch 1353, Loss: 1.7114347219467163, Final Batch Loss: 0.5131930112838745\n",
      "Epoch 1354, Loss: 1.7755538523197174, Final Batch Loss: 0.48973631858825684\n",
      "Epoch 1355, Loss: 1.6785039901733398, Final Batch Loss: 0.3388027250766754\n",
      "Epoch 1356, Loss: 1.6418115198612213, Final Batch Loss: 0.3678802251815796\n",
      "Epoch 1357, Loss: 1.791745275259018, Final Batch Loss: 0.47994571924209595\n",
      "Epoch 1358, Loss: 1.6526735424995422, Final Batch Loss: 0.3662336468696594\n",
      "Epoch 1359, Loss: 1.7495703101158142, Final Batch Loss: 0.45882803201675415\n",
      "Epoch 1360, Loss: 1.7532382011413574, Final Batch Loss: 0.5447407364845276\n",
      "Epoch 1361, Loss: 1.612162858247757, Final Batch Loss: 0.46162447333335876\n",
      "Epoch 1362, Loss: 1.6645034551620483, Final Batch Loss: 0.3621880114078522\n",
      "Epoch 1363, Loss: 1.683474212884903, Final Batch Loss: 0.38695916533470154\n",
      "Epoch 1364, Loss: 1.6107761561870575, Final Batch Loss: 0.32052233815193176\n",
      "Epoch 1365, Loss: 1.6419891119003296, Final Batch Loss: 0.4306631088256836\n",
      "Epoch 1366, Loss: 1.6575270891189575, Final Batch Loss: 0.4128568768501282\n",
      "Epoch 1367, Loss: 1.8152053356170654, Final Batch Loss: 0.43485668301582336\n",
      "Epoch 1368, Loss: 1.704838514328003, Final Batch Loss: 0.40488266944885254\n",
      "Epoch 1369, Loss: 1.7078805565834045, Final Batch Loss: 0.36076685786247253\n",
      "Epoch 1370, Loss: 1.7113337218761444, Final Batch Loss: 0.45063337683677673\n",
      "Epoch 1371, Loss: 1.757623553276062, Final Batch Loss: 0.4434813857078552\n",
      "Epoch 1372, Loss: 1.6239495277404785, Final Batch Loss: 0.42153021693229675\n",
      "Epoch 1373, Loss: 1.6621654629707336, Final Batch Loss: 0.35889488458633423\n",
      "Epoch 1374, Loss: 1.63953697681427, Final Batch Loss: 0.3883790969848633\n",
      "Epoch 1375, Loss: 1.6582556664943695, Final Batch Loss: 0.38234198093414307\n",
      "Epoch 1376, Loss: 1.61131751537323, Final Batch Loss: 0.3089132606983185\n",
      "Epoch 1377, Loss: 1.703929454088211, Final Batch Loss: 0.34378573298454285\n",
      "Epoch 1378, Loss: 1.7493991553783417, Final Batch Loss: 0.42066943645477295\n",
      "Epoch 1379, Loss: 1.631702035665512, Final Batch Loss: 0.4370660185813904\n",
      "Epoch 1380, Loss: 1.8214176893234253, Final Batch Loss: 0.34849369525909424\n",
      "Epoch 1381, Loss: 1.7162640690803528, Final Batch Loss: 0.514679491519928\n",
      "Epoch 1382, Loss: 1.6297267079353333, Final Batch Loss: 0.47343185544013977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1383, Loss: 1.6574017107486725, Final Batch Loss: 0.4192662239074707\n",
      "Epoch 1384, Loss: 1.8240254521369934, Final Batch Loss: 0.4787091314792633\n",
      "Epoch 1385, Loss: 1.7901164293289185, Final Batch Loss: 0.45001929998397827\n",
      "Epoch 1386, Loss: 1.5826899707317352, Final Batch Loss: 0.2950426936149597\n",
      "Epoch 1387, Loss: 1.7042586207389832, Final Batch Loss: 0.4857245981693268\n",
      "Epoch 1388, Loss: 1.6001344621181488, Final Batch Loss: 0.358198344707489\n",
      "Epoch 1389, Loss: 1.6512910723686218, Final Batch Loss: 0.3511824905872345\n",
      "Epoch 1390, Loss: 1.699334979057312, Final Batch Loss: 0.4219018518924713\n",
      "Epoch 1391, Loss: 1.6403497755527496, Final Batch Loss: 0.4120352566242218\n",
      "Epoch 1392, Loss: 1.765891581773758, Final Batch Loss: 0.5152705907821655\n",
      "Epoch 1393, Loss: 1.5612046420574188, Final Batch Loss: 0.33700257539749146\n",
      "Epoch 1394, Loss: 1.6346334517002106, Final Batch Loss: 0.3990475535392761\n",
      "Epoch 1395, Loss: 1.7369543015956879, Final Batch Loss: 0.4296940565109253\n",
      "Epoch 1396, Loss: 1.7432304322719574, Final Batch Loss: 0.3981788754463196\n",
      "Epoch 1397, Loss: 1.6327449083328247, Final Batch Loss: 0.40195006132125854\n",
      "Epoch 1398, Loss: 1.6378425061702728, Final Batch Loss: 0.3741133511066437\n",
      "Epoch 1399, Loss: 1.6718610525131226, Final Batch Loss: 0.4289100468158722\n",
      "Epoch 1400, Loss: 1.6430834829807281, Final Batch Loss: 0.3749907612800598\n",
      "Epoch 1401, Loss: 1.6506390273571014, Final Batch Loss: 0.43214860558509827\n",
      "Epoch 1402, Loss: 1.7703143954277039, Final Batch Loss: 0.44715097546577454\n",
      "Epoch 1403, Loss: 1.6618502736091614, Final Batch Loss: 0.34783998131752014\n",
      "Epoch 1404, Loss: 1.6699183285236359, Final Batch Loss: 0.48733142018318176\n",
      "Epoch 1405, Loss: 1.7213421165943146, Final Batch Loss: 0.31475675106048584\n",
      "Epoch 1406, Loss: 1.7672386765480042, Final Batch Loss: 0.45323389768600464\n",
      "Epoch 1407, Loss: 1.856104463338852, Final Batch Loss: 0.5399267673492432\n",
      "Epoch 1408, Loss: 1.6156951189041138, Final Batch Loss: 0.4331519901752472\n",
      "Epoch 1409, Loss: 1.7198971211910248, Final Batch Loss: 0.43917277455329895\n",
      "Epoch 1410, Loss: 1.643476128578186, Final Batch Loss: 0.3589092195034027\n",
      "Epoch 1411, Loss: 1.6840523183345795, Final Batch Loss: 0.41296273469924927\n",
      "Epoch 1412, Loss: 1.7297772765159607, Final Batch Loss: 0.4645293951034546\n",
      "Epoch 1413, Loss: 1.6428004801273346, Final Batch Loss: 0.30724218487739563\n",
      "Epoch 1414, Loss: 1.5477820336818695, Final Batch Loss: 0.37432849407196045\n",
      "Epoch 1415, Loss: 1.6517289578914642, Final Batch Loss: 0.37070947885513306\n",
      "Epoch 1416, Loss: 1.6058310568332672, Final Batch Loss: 0.3141126036643982\n",
      "Epoch 1417, Loss: 1.7299045324325562, Final Batch Loss: 0.42084982991218567\n",
      "Epoch 1418, Loss: 1.690321296453476, Final Batch Loss: 0.42854851484298706\n",
      "Epoch 1419, Loss: 1.6142008900642395, Final Batch Loss: 0.3337554335594177\n",
      "Epoch 1420, Loss: 1.7164449393749237, Final Batch Loss: 0.421600878238678\n",
      "Epoch 1421, Loss: 1.667687177658081, Final Batch Loss: 0.44601595401763916\n",
      "Epoch 1422, Loss: 1.6646340489387512, Final Batch Loss: 0.46325016021728516\n",
      "Epoch 1423, Loss: 1.5823694169521332, Final Batch Loss: 0.3428005576133728\n",
      "Epoch 1424, Loss: 1.696131706237793, Final Batch Loss: 0.4850389063358307\n",
      "Epoch 1425, Loss: 1.7346184849739075, Final Batch Loss: 0.5020588040351868\n",
      "Epoch 1426, Loss: 1.6660696268081665, Final Batch Loss: 0.44726431369781494\n",
      "Epoch 1427, Loss: 1.6260578036308289, Final Batch Loss: 0.34752577543258667\n",
      "Epoch 1428, Loss: 1.5960269272327423, Final Batch Loss: 0.40207815170288086\n",
      "Epoch 1429, Loss: 1.6089429557323456, Final Batch Loss: 0.3139001429080963\n",
      "Epoch 1430, Loss: 1.6971946060657501, Final Batch Loss: 0.495273619890213\n",
      "Epoch 1431, Loss: 1.5406225323677063, Final Batch Loss: 0.3528553545475006\n",
      "Epoch 1432, Loss: 1.7095063030719757, Final Batch Loss: 0.43022578954696655\n",
      "Epoch 1433, Loss: 1.715288907289505, Final Batch Loss: 0.4921586513519287\n",
      "Epoch 1434, Loss: 1.8061926662921906, Final Batch Loss: 0.6099247932434082\n",
      "Epoch 1435, Loss: 1.6229701340198517, Final Batch Loss: 0.4090442955493927\n",
      "Epoch 1436, Loss: 1.6621362268924713, Final Batch Loss: 0.4483286440372467\n",
      "Epoch 1437, Loss: 1.6290941536426544, Final Batch Loss: 0.31269571185112\n",
      "Epoch 1438, Loss: 1.7146271467208862, Final Batch Loss: 0.5327046513557434\n",
      "Epoch 1439, Loss: 1.7044326066970825, Final Batch Loss: 0.4267424941062927\n",
      "Epoch 1440, Loss: 1.7807362973690033, Final Batch Loss: 0.44336578249931335\n",
      "Epoch 1441, Loss: 1.8170346021652222, Final Batch Loss: 0.47737058997154236\n",
      "Epoch 1442, Loss: 1.6442104279994965, Final Batch Loss: 0.38098224997520447\n",
      "Epoch 1443, Loss: 1.6474272906780243, Final Batch Loss: 0.4164470136165619\n",
      "Epoch 1444, Loss: 1.7798155844211578, Final Batch Loss: 0.5837376713752747\n",
      "Epoch 1445, Loss: 1.7048487961292267, Final Batch Loss: 0.390548437833786\n",
      "Epoch 1446, Loss: 1.6025838255882263, Final Batch Loss: 0.34655141830444336\n",
      "Epoch 1447, Loss: 1.7039695978164673, Final Batch Loss: 0.435224711894989\n",
      "Epoch 1448, Loss: 1.6185653805732727, Final Batch Loss: 0.3988874852657318\n",
      "Epoch 1449, Loss: 1.5421122908592224, Final Batch Loss: 0.2626786530017853\n",
      "Epoch 1450, Loss: 1.728874683380127, Final Batch Loss: 0.3684568703174591\n",
      "Epoch 1451, Loss: 1.6393525302410126, Final Batch Loss: 0.36777549982070923\n",
      "Epoch 1452, Loss: 1.5662000179290771, Final Batch Loss: 0.3584352433681488\n",
      "Epoch 1453, Loss: 1.6943123638629913, Final Batch Loss: 0.4041043519973755\n",
      "Epoch 1454, Loss: 1.5395889282226562, Final Batch Loss: 0.368988037109375\n",
      "Epoch 1455, Loss: 1.5045537650585175, Final Batch Loss: 0.3245818614959717\n",
      "Epoch 1456, Loss: 1.7194238603115082, Final Batch Loss: 0.4129001200199127\n",
      "Epoch 1457, Loss: 1.6686553955078125, Final Batch Loss: 0.4407985210418701\n",
      "Epoch 1458, Loss: 1.677370548248291, Final Batch Loss: 0.4050225615501404\n",
      "Epoch 1459, Loss: 1.8378245532512665, Final Batch Loss: 0.5284419059753418\n",
      "Epoch 1460, Loss: 1.6828704178333282, Final Batch Loss: 0.4045993685722351\n",
      "Epoch 1461, Loss: 1.7372795641422272, Final Batch Loss: 0.4784204661846161\n",
      "Epoch 1462, Loss: 1.6538757979869843, Final Batch Loss: 0.4147084653377533\n",
      "Epoch 1463, Loss: 1.507249265909195, Final Batch Loss: 0.27259209752082825\n",
      "Epoch 1464, Loss: 1.592449814081192, Final Batch Loss: 0.4001046419143677\n",
      "Epoch 1465, Loss: 1.5782727003097534, Final Batch Loss: 0.4483661353588104\n",
      "Epoch 1466, Loss: 1.6830264627933502, Final Batch Loss: 0.46749770641326904\n",
      "Epoch 1467, Loss: 1.4553292989730835, Final Batch Loss: 0.32331374287605286\n",
      "Epoch 1468, Loss: 1.57930389046669, Final Batch Loss: 0.39385730028152466\n",
      "Epoch 1469, Loss: 1.8663078546524048, Final Batch Loss: 0.5070732831954956\n",
      "Epoch 1470, Loss: 1.5604718923568726, Final Batch Loss: 0.3620745539665222\n",
      "Epoch 1471, Loss: 1.7703633606433868, Final Batch Loss: 0.46137672662734985\n",
      "Epoch 1472, Loss: 1.745014488697052, Final Batch Loss: 0.539280116558075\n",
      "Epoch 1473, Loss: 1.6365719139575958, Final Batch Loss: 0.4018632769584656\n",
      "Epoch 1474, Loss: 1.5656673610210419, Final Batch Loss: 0.4015321731567383\n",
      "Epoch 1475, Loss: 1.538417786359787, Final Batch Loss: 0.40854060649871826\n",
      "Epoch 1476, Loss: 1.634857952594757, Final Batch Loss: 0.36793771386146545\n",
      "Epoch 1477, Loss: 1.5617099106311798, Final Batch Loss: 0.4068143963813782\n",
      "Epoch 1478, Loss: 1.632328361272812, Final Batch Loss: 0.43703997135162354\n",
      "Epoch 1479, Loss: 1.7660550475120544, Final Batch Loss: 0.5732884407043457\n",
      "Epoch 1480, Loss: 1.612272471189499, Final Batch Loss: 0.35724103450775146\n",
      "Epoch 1481, Loss: 1.7041850984096527, Final Batch Loss: 0.47057121992111206\n",
      "Epoch 1482, Loss: 1.6398522555828094, Final Batch Loss: 0.4149113893508911\n",
      "Epoch 1483, Loss: 1.7797902524471283, Final Batch Loss: 0.5291123986244202\n",
      "Epoch 1484, Loss: 1.482348158955574, Final Batch Loss: 0.23494236171245575\n",
      "Epoch 1485, Loss: 1.697768658399582, Final Batch Loss: 0.4189884066581726\n",
      "Epoch 1486, Loss: 1.6562770307064056, Final Batch Loss: 0.37116262316703796\n",
      "Epoch 1487, Loss: 1.661957561969757, Final Batch Loss: 0.3951917886734009\n",
      "Epoch 1488, Loss: 1.6738891303539276, Final Batch Loss: 0.4811913073062897\n",
      "Epoch 1489, Loss: 1.5779867470264435, Final Batch Loss: 0.2870051860809326\n",
      "Epoch 1490, Loss: 1.576333612203598, Final Batch Loss: 0.3942512571811676\n",
      "Epoch 1491, Loss: 1.607146978378296, Final Batch Loss: 0.3925192952156067\n",
      "Epoch 1492, Loss: 1.6590510606765747, Final Batch Loss: 0.45502766966819763\n",
      "Epoch 1493, Loss: 1.599383533000946, Final Batch Loss: 0.3609391450881958\n",
      "Epoch 1494, Loss: 1.5261994004249573, Final Batch Loss: 0.3667699992656708\n",
      "Epoch 1495, Loss: 1.7723601758480072, Final Batch Loss: 0.5159406661987305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1496, Loss: 1.5908628106117249, Final Batch Loss: 0.42064064741134644\n",
      "Epoch 1497, Loss: 1.6283452808856964, Final Batch Loss: 0.3568743169307709\n",
      "Epoch 1498, Loss: 1.5995365679264069, Final Batch Loss: 0.4549225866794586\n",
      "Epoch 1499, Loss: 1.583299994468689, Final Batch Loss: 0.3370281159877777\n",
      "Epoch 1500, Loss: 1.6809088587760925, Final Batch Loss: 0.39815056324005127\n",
      "Epoch 1501, Loss: 1.5931320488452911, Final Batch Loss: 0.39924106001853943\n",
      "Epoch 1502, Loss: 1.7236719131469727, Final Batch Loss: 0.4177679419517517\n",
      "Epoch 1503, Loss: 1.5236654579639435, Final Batch Loss: 0.22786280512809753\n",
      "Epoch 1504, Loss: 1.5284096002578735, Final Batch Loss: 0.41212910413742065\n",
      "Epoch 1505, Loss: 1.7601941227912903, Final Batch Loss: 0.44111451506614685\n",
      "Epoch 1506, Loss: 1.6456007659435272, Final Batch Loss: 0.38203585147857666\n",
      "Epoch 1507, Loss: 1.63493013381958, Final Batch Loss: 0.3526419997215271\n",
      "Epoch 1508, Loss: 1.7930942177772522, Final Batch Loss: 0.5157918334007263\n",
      "Epoch 1509, Loss: 1.7426062226295471, Final Batch Loss: 0.4668029546737671\n",
      "Epoch 1510, Loss: 1.5986994206905365, Final Batch Loss: 0.34015676379203796\n",
      "Epoch 1511, Loss: 1.5639838576316833, Final Batch Loss: 0.32607167959213257\n",
      "Epoch 1512, Loss: 1.5340231657028198, Final Batch Loss: 0.40003538131713867\n",
      "Epoch 1513, Loss: 1.8337794840335846, Final Batch Loss: 0.49329331517219543\n",
      "Epoch 1514, Loss: 1.6069948971271515, Final Batch Loss: 0.3798355758190155\n",
      "Epoch 1515, Loss: 1.6460452675819397, Final Batch Loss: 0.44072243571281433\n",
      "Epoch 1516, Loss: 1.6737302243709564, Final Batch Loss: 0.4744626581668854\n",
      "Epoch 1517, Loss: 1.581788569688797, Final Batch Loss: 0.4588976800441742\n",
      "Epoch 1518, Loss: 1.5519105792045593, Final Batch Loss: 0.4805164933204651\n",
      "Epoch 1519, Loss: 1.5645580887794495, Final Batch Loss: 0.38624513149261475\n",
      "Epoch 1520, Loss: 1.702600210905075, Final Batch Loss: 0.4992404878139496\n",
      "Epoch 1521, Loss: 1.5933393239974976, Final Batch Loss: 0.44933924078941345\n",
      "Epoch 1522, Loss: 1.6166833937168121, Final Batch Loss: 0.3981631398200989\n",
      "Epoch 1523, Loss: 1.5021697580814362, Final Batch Loss: 0.2795981168746948\n",
      "Epoch 1524, Loss: 1.5203727781772614, Final Batch Loss: 0.29923930764198303\n",
      "Epoch 1525, Loss: 1.6094207167625427, Final Batch Loss: 0.3902893364429474\n",
      "Epoch 1526, Loss: 1.6190292239189148, Final Batch Loss: 0.38670414686203003\n",
      "Epoch 1527, Loss: 1.6726287603378296, Final Batch Loss: 0.40064188838005066\n",
      "Epoch 1528, Loss: 1.6773518323898315, Final Batch Loss: 0.47431328892707825\n",
      "Epoch 1529, Loss: 1.6492046415805817, Final Batch Loss: 0.41081881523132324\n",
      "Epoch 1530, Loss: 1.599634975194931, Final Batch Loss: 0.39253973960876465\n",
      "Epoch 1531, Loss: 1.5943878889083862, Final Batch Loss: 0.35776638984680176\n",
      "Epoch 1532, Loss: 1.7768375277519226, Final Batch Loss: 0.43884986639022827\n",
      "Epoch 1533, Loss: 1.6057885587215424, Final Batch Loss: 0.3870006203651428\n",
      "Epoch 1534, Loss: 1.660564810037613, Final Batch Loss: 0.3916893005371094\n",
      "Epoch 1535, Loss: 1.5394057929515839, Final Batch Loss: 0.4009881019592285\n",
      "Epoch 1536, Loss: 1.4538324177265167, Final Batch Loss: 0.3321981132030487\n",
      "Epoch 1537, Loss: 1.7548708319664001, Final Batch Loss: 0.46691030263900757\n",
      "Epoch 1538, Loss: 1.5810056030750275, Final Batch Loss: 0.36021628975868225\n",
      "Epoch 1539, Loss: 1.7231575548648834, Final Batch Loss: 0.39331603050231934\n",
      "Epoch 1540, Loss: 1.556498646736145, Final Batch Loss: 0.30276525020599365\n",
      "Epoch 1541, Loss: 1.682235062122345, Final Batch Loss: 0.36648690700531006\n",
      "Epoch 1542, Loss: 1.5219651758670807, Final Batch Loss: 0.3353728652000427\n",
      "Epoch 1543, Loss: 1.6056749820709229, Final Batch Loss: 0.42121991515159607\n",
      "Epoch 1544, Loss: 1.5745425820350647, Final Batch Loss: 0.3195843994617462\n",
      "Epoch 1545, Loss: 1.512350857257843, Final Batch Loss: 0.3123716115951538\n",
      "Epoch 1546, Loss: 1.5253029465675354, Final Batch Loss: 0.39010006189346313\n",
      "Epoch 1547, Loss: 1.676997184753418, Final Batch Loss: 0.39990511536598206\n",
      "Epoch 1548, Loss: 1.6100815683603287, Final Batch Loss: 0.21471841633319855\n",
      "Epoch 1549, Loss: 1.5896737277507782, Final Batch Loss: 0.36623233556747437\n",
      "Epoch 1550, Loss: 1.750778317451477, Final Batch Loss: 0.5154606699943542\n",
      "Epoch 1551, Loss: 1.5648863911628723, Final Batch Loss: 0.38590705394744873\n",
      "Epoch 1552, Loss: 1.4506456851959229, Final Batch Loss: 0.31688162684440613\n",
      "Epoch 1553, Loss: 1.5411461293697357, Final Batch Loss: 0.3813416361808777\n",
      "Epoch 1554, Loss: 1.5823416411876678, Final Batch Loss: 0.39381086826324463\n",
      "Epoch 1555, Loss: 1.52836674451828, Final Batch Loss: 0.414529949426651\n",
      "Epoch 1556, Loss: 1.4776138961315155, Final Batch Loss: 0.32844918966293335\n",
      "Epoch 1557, Loss: 1.7038586139678955, Final Batch Loss: 0.47457587718963623\n",
      "Epoch 1558, Loss: 1.588782399892807, Final Batch Loss: 0.5688055157661438\n",
      "Epoch 1559, Loss: 1.54012992978096, Final Batch Loss: 0.3303588926792145\n",
      "Epoch 1560, Loss: 1.6264248192310333, Final Batch Loss: 0.35448238253593445\n",
      "Epoch 1561, Loss: 1.6763939559459686, Final Batch Loss: 0.4815709888935089\n",
      "Epoch 1562, Loss: 1.5633910596370697, Final Batch Loss: 0.3705081343650818\n",
      "Epoch 1563, Loss: 1.4649930596351624, Final Batch Loss: 0.3227754235267639\n",
      "Epoch 1564, Loss: 1.7306156754493713, Final Batch Loss: 0.4467746913433075\n",
      "Epoch 1565, Loss: 1.5566036105155945, Final Batch Loss: 0.3556744456291199\n",
      "Epoch 1566, Loss: 1.503116101026535, Final Batch Loss: 0.26587045192718506\n",
      "Epoch 1567, Loss: 1.563843697309494, Final Batch Loss: 0.4210645854473114\n",
      "Epoch 1568, Loss: 1.6486178636550903, Final Batch Loss: 0.4326361119747162\n",
      "Epoch 1569, Loss: 1.5465578436851501, Final Batch Loss: 0.3365519344806671\n",
      "Epoch 1570, Loss: 1.4852019250392914, Final Batch Loss: 0.3998376727104187\n",
      "Epoch 1571, Loss: 1.5193237364292145, Final Batch Loss: 0.36688703298568726\n",
      "Epoch 1572, Loss: 1.6196187138557434, Final Batch Loss: 0.4216250479221344\n",
      "Epoch 1573, Loss: 1.5930603742599487, Final Batch Loss: 0.3734872341156006\n",
      "Epoch 1574, Loss: 1.535396695137024, Final Batch Loss: 0.3689574897289276\n",
      "Epoch 1575, Loss: 1.5807487666606903, Final Batch Loss: 0.33209922909736633\n",
      "Epoch 1576, Loss: 1.5843427777290344, Final Batch Loss: 0.3609201908111572\n",
      "Epoch 1577, Loss: 1.5312262177467346, Final Batch Loss: 0.3685562014579773\n",
      "Epoch 1578, Loss: 1.5691545605659485, Final Batch Loss: 0.392400860786438\n",
      "Epoch 1579, Loss: 1.548793762922287, Final Batch Loss: 0.40864676237106323\n",
      "Epoch 1580, Loss: 1.6637231409549713, Final Batch Loss: 0.4817357361316681\n",
      "Epoch 1581, Loss: 1.571569949388504, Final Batch Loss: 0.3228166997432709\n",
      "Epoch 1582, Loss: 1.4981925189495087, Final Batch Loss: 0.3497263491153717\n",
      "Epoch 1583, Loss: 1.4829800724983215, Final Batch Loss: 0.3739515244960785\n",
      "Epoch 1584, Loss: 1.4952675104141235, Final Batch Loss: 0.280798077583313\n",
      "Epoch 1585, Loss: 1.5374214053153992, Final Batch Loss: 0.37526413798332214\n",
      "Epoch 1586, Loss: 1.5298196375370026, Final Batch Loss: 0.32836514711380005\n",
      "Epoch 1587, Loss: 1.5529212355613708, Final Batch Loss: 0.4092695415019989\n",
      "Epoch 1588, Loss: 1.5281728208065033, Final Batch Loss: 0.40062999725341797\n",
      "Epoch 1589, Loss: 1.6743469834327698, Final Batch Loss: 0.36264103651046753\n",
      "Epoch 1590, Loss: 1.660560429096222, Final Batch Loss: 0.4247898757457733\n",
      "Epoch 1591, Loss: 1.7066195011138916, Final Batch Loss: 0.5542058348655701\n",
      "Epoch 1592, Loss: 1.690595030784607, Final Batch Loss: 0.4314396381378174\n",
      "Epoch 1593, Loss: 1.6366071701049805, Final Batch Loss: 0.48850157856941223\n",
      "Epoch 1594, Loss: 1.5986810624599457, Final Batch Loss: 0.28544363379478455\n",
      "Epoch 1595, Loss: 1.5291125774383545, Final Batch Loss: 0.3314880430698395\n",
      "Epoch 1596, Loss: 1.63563472032547, Final Batch Loss: 0.40252363681793213\n",
      "Epoch 1597, Loss: 1.6283440291881561, Final Batch Loss: 0.4831339120864868\n",
      "Epoch 1598, Loss: 1.6739726066589355, Final Batch Loss: 0.4543890655040741\n",
      "Epoch 1599, Loss: 1.6979939341545105, Final Batch Loss: 0.4285796880722046\n",
      "Epoch 1600, Loss: 1.6680459380149841, Final Batch Loss: 0.4217104911804199\n",
      "Epoch 1601, Loss: 1.5938159227371216, Final Batch Loss: 0.48199859261512756\n",
      "Epoch 1602, Loss: 1.6799161732196808, Final Batch Loss: 0.4370010495185852\n",
      "Epoch 1603, Loss: 1.520355373620987, Final Batch Loss: 0.42386913299560547\n",
      "Epoch 1604, Loss: 1.740985631942749, Final Batch Loss: 0.42952361702919006\n",
      "Epoch 1605, Loss: 1.5619485676288605, Final Batch Loss: 0.3616679608821869\n",
      "Epoch 1606, Loss: 1.4862640798091888, Final Batch Loss: 0.3427855670452118\n",
      "Epoch 1607, Loss: 1.5193550288677216, Final Batch Loss: 0.38272228837013245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1608, Loss: 1.545264333486557, Final Batch Loss: 0.3329511284828186\n",
      "Epoch 1609, Loss: 1.6321280300617218, Final Batch Loss: 0.4496123790740967\n",
      "Epoch 1610, Loss: 1.5433827936649323, Final Batch Loss: 0.4268243610858917\n",
      "Epoch 1611, Loss: 1.569846361875534, Final Batch Loss: 0.4056144654750824\n",
      "Epoch 1612, Loss: 1.662904053926468, Final Batch Loss: 0.49944719672203064\n",
      "Epoch 1613, Loss: 1.6569923758506775, Final Batch Loss: 0.3767792284488678\n",
      "Epoch 1614, Loss: 1.591624528169632, Final Batch Loss: 0.33618515729904175\n",
      "Epoch 1615, Loss: 1.6207087934017181, Final Batch Loss: 0.4433077275753021\n",
      "Epoch 1616, Loss: 1.5710899531841278, Final Batch Loss: 0.38026952743530273\n",
      "Epoch 1617, Loss: 1.6390285193920135, Final Batch Loss: 0.35247692465782166\n",
      "Epoch 1618, Loss: 1.593641996383667, Final Batch Loss: 0.3664206564426422\n",
      "Epoch 1619, Loss: 1.59451562166214, Final Batch Loss: 0.4526961147785187\n",
      "Epoch 1620, Loss: 1.4745345711708069, Final Batch Loss: 0.39578476548194885\n",
      "Epoch 1621, Loss: 1.598953127861023, Final Batch Loss: 0.3709787130355835\n",
      "Epoch 1622, Loss: 1.5571423768997192, Final Batch Loss: 0.4111681282520294\n",
      "Epoch 1623, Loss: 1.5590854585170746, Final Batch Loss: 0.4359270930290222\n",
      "Epoch 1624, Loss: 1.536299616098404, Final Batch Loss: 0.40456098318099976\n",
      "Epoch 1625, Loss: 1.5919146537780762, Final Batch Loss: 0.41341179609298706\n",
      "Epoch 1626, Loss: 1.5799793303012848, Final Batch Loss: 0.4260599613189697\n",
      "Epoch 1627, Loss: 1.6608602702617645, Final Batch Loss: 0.47350847721099854\n",
      "Epoch 1628, Loss: 1.3788838684558868, Final Batch Loss: 0.2703666090965271\n",
      "Epoch 1629, Loss: 1.721142828464508, Final Batch Loss: 0.5690484642982483\n",
      "Epoch 1630, Loss: 1.5090643763542175, Final Batch Loss: 0.36390334367752075\n",
      "Epoch 1631, Loss: 1.5375349819660187, Final Batch Loss: 0.4190784692764282\n",
      "Epoch 1632, Loss: 1.4607118666172028, Final Batch Loss: 0.35127824544906616\n",
      "Epoch 1633, Loss: 1.687001258134842, Final Batch Loss: 0.46446916460990906\n",
      "Epoch 1634, Loss: 1.4879641234874725, Final Batch Loss: 0.3178147077560425\n",
      "Epoch 1635, Loss: 1.6167778372764587, Final Batch Loss: 0.36451780796051025\n",
      "Epoch 1636, Loss: 1.5348482728004456, Final Batch Loss: 0.4264093041419983\n",
      "Epoch 1637, Loss: 1.6439328789710999, Final Batch Loss: 0.4352244734764099\n",
      "Epoch 1638, Loss: 1.5929681062698364, Final Batch Loss: 0.38492095470428467\n",
      "Epoch 1639, Loss: 1.5592891573905945, Final Batch Loss: 0.359432578086853\n",
      "Epoch 1640, Loss: 1.6292591989040375, Final Batch Loss: 0.3731379806995392\n",
      "Epoch 1641, Loss: 1.6004160344600677, Final Batch Loss: 0.4438014328479767\n",
      "Epoch 1642, Loss: 1.5839923322200775, Final Batch Loss: 0.4488871097564697\n",
      "Epoch 1643, Loss: 1.4419854283332825, Final Batch Loss: 0.3354702591896057\n",
      "Epoch 1644, Loss: 1.612615704536438, Final Batch Loss: 0.43474698066711426\n",
      "Epoch 1645, Loss: 1.6115661561489105, Final Batch Loss: 0.3807207942008972\n",
      "Epoch 1646, Loss: 1.464114785194397, Final Batch Loss: 0.3975374102592468\n",
      "Epoch 1647, Loss: 1.5378299057483673, Final Batch Loss: 0.3551863133907318\n",
      "Epoch 1648, Loss: 1.547829031944275, Final Batch Loss: 0.29650166630744934\n",
      "Epoch 1649, Loss: 1.5987327992916107, Final Batch Loss: 0.39000391960144043\n",
      "Epoch 1650, Loss: 1.5150264501571655, Final Batch Loss: 0.390386700630188\n",
      "Epoch 1651, Loss: 1.525212973356247, Final Batch Loss: 0.31409621238708496\n",
      "Epoch 1652, Loss: 1.5211198925971985, Final Batch Loss: 0.41885852813720703\n",
      "Epoch 1653, Loss: 1.5489063560962677, Final Batch Loss: 0.39642319083213806\n",
      "Epoch 1654, Loss: 1.6976527273654938, Final Batch Loss: 0.49439212679862976\n",
      "Epoch 1655, Loss: 1.583257108926773, Final Batch Loss: 0.39001771807670593\n",
      "Epoch 1656, Loss: 1.5811570286750793, Final Batch Loss: 0.39003682136535645\n",
      "Epoch 1657, Loss: 1.5513414442539215, Final Batch Loss: 0.3587104380130768\n",
      "Epoch 1658, Loss: 1.6667877435684204, Final Batch Loss: 0.4596858322620392\n",
      "Epoch 1659, Loss: 1.4916258752346039, Final Batch Loss: 0.4065442383289337\n",
      "Epoch 1660, Loss: 1.5073219239711761, Final Batch Loss: 0.31787925958633423\n",
      "Epoch 1661, Loss: 1.6185626089572906, Final Batch Loss: 0.375016450881958\n",
      "Epoch 1662, Loss: 1.5904778242111206, Final Batch Loss: 0.4831812083721161\n",
      "Epoch 1663, Loss: 1.5442633628845215, Final Batch Loss: 0.36877191066741943\n",
      "Epoch 1664, Loss: 1.4942777156829834, Final Batch Loss: 0.3758805990219116\n",
      "Epoch 1665, Loss: 1.6929773390293121, Final Batch Loss: 0.5889089107513428\n",
      "Epoch 1666, Loss: 1.5743839740753174, Final Batch Loss: 0.449267715215683\n",
      "Epoch 1667, Loss: 1.500579595565796, Final Batch Loss: 0.3411673307418823\n",
      "Epoch 1668, Loss: 1.5573330223560333, Final Batch Loss: 0.32915398478507996\n",
      "Epoch 1669, Loss: 1.5665364563465118, Final Batch Loss: 0.39107009768486023\n",
      "Epoch 1670, Loss: 1.4714812338352203, Final Batch Loss: 0.28762614727020264\n",
      "Epoch 1671, Loss: 1.5173353254795074, Final Batch Loss: 0.3680231273174286\n",
      "Epoch 1672, Loss: 1.5328349471092224, Final Batch Loss: 0.3021823465824127\n",
      "Epoch 1673, Loss: 1.500468909740448, Final Batch Loss: 0.37247928977012634\n",
      "Epoch 1674, Loss: 1.4459936022758484, Final Batch Loss: 0.3459244668483734\n",
      "Epoch 1675, Loss: 1.4691241085529327, Final Batch Loss: 0.3608929216861725\n",
      "Epoch 1676, Loss: 1.5663317739963531, Final Batch Loss: 0.38041383028030396\n",
      "Epoch 1677, Loss: 1.562810629606247, Final Batch Loss: 0.3464970588684082\n",
      "Epoch 1678, Loss: 1.4464073479175568, Final Batch Loss: 0.35946786403656006\n",
      "Epoch 1679, Loss: 1.5513107478618622, Final Batch Loss: 0.3729836940765381\n",
      "Epoch 1680, Loss: 1.6654010713100433, Final Batch Loss: 0.4102415442466736\n",
      "Epoch 1681, Loss: 1.498353272676468, Final Batch Loss: 0.4407888650894165\n",
      "Epoch 1682, Loss: 1.6091842353343964, Final Batch Loss: 0.3744570016860962\n",
      "Epoch 1683, Loss: 1.6053758561611176, Final Batch Loss: 0.3795410394668579\n",
      "Epoch 1684, Loss: 1.5319451987743378, Final Batch Loss: 0.4181174337863922\n",
      "Epoch 1685, Loss: 1.6020613312721252, Final Batch Loss: 0.45729318261146545\n",
      "Epoch 1686, Loss: 1.5485822558403015, Final Batch Loss: 0.3371538817882538\n",
      "Epoch 1687, Loss: 1.4412328004837036, Final Batch Loss: 0.3212648034095764\n",
      "Epoch 1688, Loss: 1.6252081990242004, Final Batch Loss: 0.4867178499698639\n",
      "Epoch 1689, Loss: 1.5520938336849213, Final Batch Loss: 0.36538779735565186\n",
      "Epoch 1690, Loss: 1.5699792802333832, Final Batch Loss: 0.46460041403770447\n",
      "Epoch 1691, Loss: 1.521423101425171, Final Batch Loss: 0.394418329000473\n",
      "Epoch 1692, Loss: 1.4586362540721893, Final Batch Loss: 0.3941023647785187\n",
      "Epoch 1693, Loss: 1.492730975151062, Final Batch Loss: 0.3314433991909027\n",
      "Epoch 1694, Loss: 1.6327316761016846, Final Batch Loss: 0.4634036123752594\n",
      "Epoch 1695, Loss: 1.5806781649589539, Final Batch Loss: 0.27706989645957947\n",
      "Epoch 1696, Loss: 1.6311714947223663, Final Batch Loss: 0.4108954668045044\n",
      "Epoch 1697, Loss: 1.5053011178970337, Final Batch Loss: 0.3726843297481537\n",
      "Epoch 1698, Loss: 1.6999970078468323, Final Batch Loss: 0.4440421164035797\n",
      "Epoch 1699, Loss: 1.6756334006786346, Final Batch Loss: 0.4423953592777252\n",
      "Epoch 1700, Loss: 1.6219252347946167, Final Batch Loss: 0.5025951266288757\n",
      "Epoch 1701, Loss: 1.5730956196784973, Final Batch Loss: 0.36664360761642456\n",
      "Epoch 1702, Loss: 1.6180581450462341, Final Batch Loss: 0.34238240122795105\n",
      "Epoch 1703, Loss: 1.5483317971229553, Final Batch Loss: 0.38481825590133667\n",
      "Epoch 1704, Loss: 1.7225748300552368, Final Batch Loss: 0.34606847167015076\n",
      "Epoch 1705, Loss: 1.4496133625507355, Final Batch Loss: 0.32963937520980835\n",
      "Epoch 1706, Loss: 1.499094933271408, Final Batch Loss: 0.3990207016468048\n",
      "Epoch 1707, Loss: 1.6291053891181946, Final Batch Loss: 0.4228106439113617\n",
      "Epoch 1708, Loss: 1.6974357068538666, Final Batch Loss: 0.5373744368553162\n",
      "Epoch 1709, Loss: 1.4085881412029266, Final Batch Loss: 0.3219589293003082\n",
      "Epoch 1710, Loss: 1.386762112379074, Final Batch Loss: 0.3263437747955322\n",
      "Epoch 1711, Loss: 1.5219061076641083, Final Batch Loss: 0.35454902052879333\n",
      "Epoch 1712, Loss: 1.404482126235962, Final Batch Loss: 0.3239274322986603\n",
      "Epoch 1713, Loss: 1.4954951703548431, Final Batch Loss: 0.3973501920700073\n",
      "Epoch 1714, Loss: 1.4668945372104645, Final Batch Loss: 0.3804697096347809\n",
      "Epoch 1715, Loss: 1.5568653643131256, Final Batch Loss: 0.45128199458122253\n",
      "Epoch 1716, Loss: 1.5300656855106354, Final Batch Loss: 0.3817824721336365\n",
      "Epoch 1717, Loss: 1.4061875641345978, Final Batch Loss: 0.2960842549800873\n",
      "Epoch 1718, Loss: 1.5436557233333588, Final Batch Loss: 0.5115009546279907\n",
      "Epoch 1719, Loss: 1.4910131692886353, Final Batch Loss: 0.256361722946167\n",
      "Epoch 1720, Loss: 1.5183203518390656, Final Batch Loss: 0.3852742910385132\n",
      "Epoch 1721, Loss: 1.5474334061145782, Final Batch Loss: 0.4292529225349426\n",
      "Epoch 1722, Loss: 1.4465278685092926, Final Batch Loss: 0.3664857745170593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1723, Loss: 1.5350731909275055, Final Batch Loss: 0.40456530451774597\n",
      "Epoch 1724, Loss: 1.4678161442279816, Final Batch Loss: 0.27815303206443787\n",
      "Epoch 1725, Loss: 1.4980583786964417, Final Batch Loss: 0.3544766306877136\n",
      "Epoch 1726, Loss: 1.5165888369083405, Final Batch Loss: 0.38046324253082275\n",
      "Epoch 1727, Loss: 1.595110923051834, Final Batch Loss: 0.4174885153770447\n",
      "Epoch 1728, Loss: 1.5150703191757202, Final Batch Loss: 0.3539944887161255\n",
      "Epoch 1729, Loss: 1.5824973285198212, Final Batch Loss: 0.4685686528682709\n",
      "Epoch 1730, Loss: 1.5002058148384094, Final Batch Loss: 0.3470608592033386\n",
      "Epoch 1731, Loss: 1.4956009685993195, Final Batch Loss: 0.39945587515830994\n",
      "Epoch 1732, Loss: 1.5841232538223267, Final Batch Loss: 0.40833941102027893\n",
      "Epoch 1733, Loss: 1.5329897105693817, Final Batch Loss: 0.38185662031173706\n",
      "Epoch 1734, Loss: 1.5140847563743591, Final Batch Loss: 0.4201447069644928\n",
      "Epoch 1735, Loss: 1.4111370146274567, Final Batch Loss: 0.3450855016708374\n",
      "Epoch 1736, Loss: 1.5648751258850098, Final Batch Loss: 0.5289644598960876\n",
      "Epoch 1737, Loss: 1.5547930598258972, Final Batch Loss: 0.3697527050971985\n",
      "Epoch 1738, Loss: 1.5090157091617584, Final Batch Loss: 0.3651224970817566\n",
      "Epoch 1739, Loss: 1.5588794648647308, Final Batch Loss: 0.3546370267868042\n",
      "Epoch 1740, Loss: 1.3670291602611542, Final Batch Loss: 0.30033382773399353\n",
      "Epoch 1741, Loss: 1.5447710752487183, Final Batch Loss: 0.39868998527526855\n",
      "Epoch 1742, Loss: 1.5108669996261597, Final Batch Loss: 0.44418036937713623\n",
      "Epoch 1743, Loss: 1.5875361561775208, Final Batch Loss: 0.4076521396636963\n",
      "Epoch 1744, Loss: 1.5717088282108307, Final Batch Loss: 0.43491578102111816\n",
      "Epoch 1745, Loss: 1.4443634152412415, Final Batch Loss: 0.3575676381587982\n",
      "Epoch 1746, Loss: 1.7257352769374847, Final Batch Loss: 0.5847904682159424\n",
      "Epoch 1747, Loss: 1.5011302828788757, Final Batch Loss: 0.35315996408462524\n",
      "Epoch 1748, Loss: 1.5245475769042969, Final Batch Loss: 0.45281267166137695\n",
      "Epoch 1749, Loss: 1.5348605811595917, Final Batch Loss: 0.46708595752716064\n",
      "Epoch 1750, Loss: 1.595283180475235, Final Batch Loss: 0.48685669898986816\n",
      "Epoch 1751, Loss: 1.4483593106269836, Final Batch Loss: 0.3176369071006775\n",
      "Epoch 1752, Loss: 1.4528807401657104, Final Batch Loss: 0.3180627226829529\n",
      "Epoch 1753, Loss: 1.5646059215068817, Final Batch Loss: 0.4363119602203369\n",
      "Epoch 1754, Loss: 1.539120763540268, Final Batch Loss: 0.41198474168777466\n",
      "Epoch 1755, Loss: 1.5241868197917938, Final Batch Loss: 0.350460946559906\n",
      "Epoch 1756, Loss: 1.4798826277256012, Final Batch Loss: 0.30012574791908264\n",
      "Epoch 1757, Loss: 1.4363835453987122, Final Batch Loss: 0.37293943762779236\n",
      "Epoch 1758, Loss: 1.3839492201805115, Final Batch Loss: 0.2720757722854614\n",
      "Epoch 1759, Loss: 1.427905559539795, Final Batch Loss: 0.3097246289253235\n",
      "Epoch 1760, Loss: 1.5324926674365997, Final Batch Loss: 0.42480871081352234\n",
      "Epoch 1761, Loss: 1.4881004989147186, Final Batch Loss: 0.43230366706848145\n",
      "Epoch 1762, Loss: 1.492932379245758, Final Batch Loss: 0.3243411183357239\n",
      "Epoch 1763, Loss: 1.5633984506130219, Final Batch Loss: 0.40183573961257935\n",
      "Epoch 1764, Loss: 1.4955799579620361, Final Batch Loss: 0.45699837803840637\n",
      "Epoch 1765, Loss: 1.5576349198818207, Final Batch Loss: 0.40809980034828186\n",
      "Epoch 1766, Loss: 1.468491941690445, Final Batch Loss: 0.3394125998020172\n",
      "Epoch 1767, Loss: 1.5262871980667114, Final Batch Loss: 0.4465446472167969\n",
      "Epoch 1768, Loss: 1.5601331889629364, Final Batch Loss: 0.3241359293460846\n",
      "Epoch 1769, Loss: 1.5228350162506104, Final Batch Loss: 0.3729034960269928\n",
      "Epoch 1770, Loss: 1.595953792333603, Final Batch Loss: 0.5061840415000916\n",
      "Epoch 1771, Loss: 1.3945674300193787, Final Batch Loss: 0.28546464443206787\n",
      "Epoch 1772, Loss: 1.551910012960434, Final Batch Loss: 0.40081968903541565\n",
      "Epoch 1773, Loss: 1.4327542781829834, Final Batch Loss: 0.33580482006073\n",
      "Epoch 1774, Loss: 1.5210029184818268, Final Batch Loss: 0.42289406061172485\n",
      "Epoch 1775, Loss: 1.384777992963791, Final Batch Loss: 0.29225701093673706\n",
      "Epoch 1776, Loss: 1.6034830212593079, Final Batch Loss: 0.4355451166629791\n",
      "Epoch 1777, Loss: 1.58220973610878, Final Batch Loss: 0.37238359451293945\n",
      "Epoch 1778, Loss: 1.423667550086975, Final Batch Loss: 0.2908938229084015\n",
      "Epoch 1779, Loss: 1.5364992916584015, Final Batch Loss: 0.46295687556266785\n",
      "Epoch 1780, Loss: 1.5581323504447937, Final Batch Loss: 0.3972889184951782\n",
      "Epoch 1781, Loss: 1.5344826579093933, Final Batch Loss: 0.492811381816864\n",
      "Epoch 1782, Loss: 1.4841222167015076, Final Batch Loss: 0.31976988911628723\n",
      "Epoch 1783, Loss: 1.447821706533432, Final Batch Loss: 0.3894250690937042\n",
      "Epoch 1784, Loss: 1.4714463353157043, Final Batch Loss: 0.3315655589103699\n",
      "Epoch 1785, Loss: 1.3859570026397705, Final Batch Loss: 0.2510143518447876\n",
      "Epoch 1786, Loss: 1.4553892016410828, Final Batch Loss: 0.400328129529953\n",
      "Epoch 1787, Loss: 1.4670989215373993, Final Batch Loss: 0.28971850872039795\n",
      "Epoch 1788, Loss: 1.438369244337082, Final Batch Loss: 0.40051567554473877\n",
      "Epoch 1789, Loss: 1.5228518843650818, Final Batch Loss: 0.5263031125068665\n",
      "Epoch 1790, Loss: 1.4605458676815033, Final Batch Loss: 0.38789060711860657\n",
      "Epoch 1791, Loss: 1.5318797528743744, Final Batch Loss: 0.3019125759601593\n",
      "Epoch 1792, Loss: 1.625287652015686, Final Batch Loss: 0.44444015622138977\n",
      "Epoch 1793, Loss: 1.4588399529457092, Final Batch Loss: 0.35075509548187256\n",
      "Epoch 1794, Loss: 1.4901618957519531, Final Batch Loss: 0.3183533251285553\n",
      "Epoch 1795, Loss: 1.5138952434062958, Final Batch Loss: 0.3862135708332062\n",
      "Epoch 1796, Loss: 1.4926767945289612, Final Batch Loss: 0.38039788603782654\n",
      "Epoch 1797, Loss: 1.4563881158828735, Final Batch Loss: 0.3634624183177948\n",
      "Epoch 1798, Loss: 1.460478812456131, Final Batch Loss: 0.33307185769081116\n",
      "Epoch 1799, Loss: 1.492825835943222, Final Batch Loss: 0.4204474687576294\n",
      "Epoch 1800, Loss: 1.5444301962852478, Final Batch Loss: 0.35305991768836975\n",
      "Epoch 1801, Loss: 1.405953973531723, Final Batch Loss: 0.25150978565216064\n",
      "Epoch 1802, Loss: 1.417166143655777, Final Batch Loss: 0.31940022110939026\n",
      "Epoch 1803, Loss: 1.4500739872455597, Final Batch Loss: 0.32917678356170654\n",
      "Epoch 1804, Loss: 1.502555787563324, Final Batch Loss: 0.3774106502532959\n",
      "Epoch 1805, Loss: 1.475906789302826, Final Batch Loss: 0.3480352759361267\n",
      "Epoch 1806, Loss: 1.418872743844986, Final Batch Loss: 0.3286360204219818\n",
      "Epoch 1807, Loss: 1.4045101404190063, Final Batch Loss: 0.39498913288116455\n",
      "Epoch 1808, Loss: 1.4363035261631012, Final Batch Loss: 0.29647475481033325\n",
      "Epoch 1809, Loss: 1.5249130427837372, Final Batch Loss: 0.4436935782432556\n",
      "Epoch 1810, Loss: 1.2700740098953247, Final Batch Loss: 0.25729191303253174\n",
      "Epoch 1811, Loss: 1.4280857145786285, Final Batch Loss: 0.3499870300292969\n",
      "Epoch 1812, Loss: 1.493581473827362, Final Batch Loss: 0.43194079399108887\n",
      "Epoch 1813, Loss: 1.5170021653175354, Final Batch Loss: 0.36677828431129456\n",
      "Epoch 1814, Loss: 1.4785647988319397, Final Batch Loss: 0.2494906485080719\n",
      "Epoch 1815, Loss: 1.5218097865581512, Final Batch Loss: 0.3909246623516083\n",
      "Epoch 1816, Loss: 1.5374042689800262, Final Batch Loss: 0.3985350728034973\n",
      "Epoch 1817, Loss: 1.4837701618671417, Final Batch Loss: 0.4013599157333374\n",
      "Epoch 1818, Loss: 1.389667272567749, Final Batch Loss: 0.368544340133667\n",
      "Epoch 1819, Loss: 1.4728597104549408, Final Batch Loss: 0.46416574716567993\n",
      "Epoch 1820, Loss: 1.592404454946518, Final Batch Loss: 0.40008315443992615\n",
      "Epoch 1821, Loss: 1.615927815437317, Final Batch Loss: 0.40207621455192566\n",
      "Epoch 1822, Loss: 1.5325227975845337, Final Batch Loss: 0.41691920161247253\n",
      "Epoch 1823, Loss: 1.5420649945735931, Final Batch Loss: 0.38735297322273254\n",
      "Epoch 1824, Loss: 1.612701952457428, Final Batch Loss: 0.37172821164131165\n",
      "Epoch 1825, Loss: 1.4920554459095001, Final Batch Loss: 0.33507585525512695\n",
      "Epoch 1826, Loss: 1.5123151540756226, Final Batch Loss: 0.326815128326416\n",
      "Epoch 1827, Loss: 1.473859816789627, Final Batch Loss: 0.40612828731536865\n",
      "Epoch 1828, Loss: 1.5554680526256561, Final Batch Loss: 0.3346498906612396\n",
      "Epoch 1829, Loss: 1.5341254770755768, Final Batch Loss: 0.404140442609787\n",
      "Epoch 1830, Loss: 1.5106374621391296, Final Batch Loss: 0.3685676157474518\n",
      "Epoch 1831, Loss: 1.5367150008678436, Final Batch Loss: 0.49768346548080444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1832, Loss: 1.4847722053527832, Final Batch Loss: 0.355671763420105\n",
      "Epoch 1833, Loss: 1.3798734545707703, Final Batch Loss: 0.35693565011024475\n",
      "Epoch 1834, Loss: 1.5256767570972443, Final Batch Loss: 0.38285017013549805\n",
      "Epoch 1835, Loss: 1.4221771657466888, Final Batch Loss: 0.32373228669166565\n",
      "Epoch 1836, Loss: 1.4214972853660583, Final Batch Loss: 0.368101567029953\n",
      "Epoch 1837, Loss: 1.4953536689281464, Final Batch Loss: 0.35029318928718567\n",
      "Epoch 1838, Loss: 1.5085987150669098, Final Batch Loss: 0.4670844078063965\n",
      "Epoch 1839, Loss: 1.435985118150711, Final Batch Loss: 0.3695507347583771\n",
      "Epoch 1840, Loss: 1.4469353258609772, Final Batch Loss: 0.3548305928707123\n",
      "Epoch 1841, Loss: 1.5869174897670746, Final Batch Loss: 0.42707791924476624\n",
      "Epoch 1842, Loss: 1.3789555430412292, Final Batch Loss: 0.27616965770721436\n",
      "Epoch 1843, Loss: 1.4370962083339691, Final Batch Loss: 0.3033044934272766\n",
      "Epoch 1844, Loss: 1.4649151861667633, Final Batch Loss: 0.34986600279808044\n",
      "Epoch 1845, Loss: 1.5026732385158539, Final Batch Loss: 0.47364550828933716\n",
      "Epoch 1846, Loss: 1.4365987479686737, Final Batch Loss: 0.3899950385093689\n",
      "Epoch 1847, Loss: 1.5479574501514435, Final Batch Loss: 0.38643181324005127\n",
      "Epoch 1848, Loss: 1.4373880624771118, Final Batch Loss: 0.30305635929107666\n",
      "Epoch 1849, Loss: 1.4905632138252258, Final Batch Loss: 0.3794425129890442\n",
      "Epoch 1850, Loss: 1.470315933227539, Final Batch Loss: 0.32621052861213684\n",
      "Epoch 1851, Loss: 1.4783669114112854, Final Batch Loss: 0.33996233344078064\n",
      "Epoch 1852, Loss: 1.6334073543548584, Final Batch Loss: 0.4530909061431885\n",
      "Epoch 1853, Loss: 1.3937153220176697, Final Batch Loss: 0.3560319244861603\n",
      "Epoch 1854, Loss: 1.3155123889446259, Final Batch Loss: 0.27074557542800903\n",
      "Epoch 1855, Loss: 1.4791108965873718, Final Batch Loss: 0.3642641305923462\n",
      "Epoch 1856, Loss: 1.5327496230602264, Final Batch Loss: 0.372286856174469\n",
      "Epoch 1857, Loss: 1.3948411345481873, Final Batch Loss: 0.34044477343559265\n",
      "Epoch 1858, Loss: 1.54152512550354, Final Batch Loss: 0.4509977698326111\n",
      "Epoch 1859, Loss: 1.3699635863304138, Final Batch Loss: 0.2926473021507263\n",
      "Epoch 1860, Loss: 1.543521910905838, Final Batch Loss: 0.49810001254081726\n",
      "Epoch 1861, Loss: 1.4556726515293121, Final Batch Loss: 0.3430339992046356\n",
      "Epoch 1862, Loss: 1.5488989353179932, Final Batch Loss: 0.4266285002231598\n",
      "Epoch 1863, Loss: 1.4095633029937744, Final Batch Loss: 0.37266290187835693\n",
      "Epoch 1864, Loss: 1.4071574807167053, Final Batch Loss: 0.3157624900341034\n",
      "Epoch 1865, Loss: 1.432872474193573, Final Batch Loss: 0.3532194197177887\n",
      "Epoch 1866, Loss: 1.4584811627864838, Final Batch Loss: 0.3908914029598236\n",
      "Epoch 1867, Loss: 1.3783473074436188, Final Batch Loss: 0.32128357887268066\n",
      "Epoch 1868, Loss: 1.5410084426403046, Final Batch Loss: 0.436838835477829\n",
      "Epoch 1869, Loss: 1.3980396091938019, Final Batch Loss: 0.34088948369026184\n",
      "Epoch 1870, Loss: 1.5352880656719208, Final Batch Loss: 0.36081892251968384\n",
      "Epoch 1871, Loss: 1.4330198168754578, Final Batch Loss: 0.41198861598968506\n",
      "Epoch 1872, Loss: 1.436416357755661, Final Batch Loss: 0.44939616322517395\n",
      "Epoch 1873, Loss: 1.3739548027515411, Final Batch Loss: 0.3305233418941498\n",
      "Epoch 1874, Loss: 1.3991877436637878, Final Batch Loss: 0.3466752767562866\n",
      "Epoch 1875, Loss: 1.4281147420406342, Final Batch Loss: 0.3045695722103119\n",
      "Epoch 1876, Loss: 1.4509309828281403, Final Batch Loss: 0.2656855285167694\n",
      "Epoch 1877, Loss: 1.5736296474933624, Final Batch Loss: 0.3688141703605652\n",
      "Epoch 1878, Loss: 1.4549529552459717, Final Batch Loss: 0.2911868691444397\n",
      "Epoch 1879, Loss: 1.5014393627643585, Final Batch Loss: 0.41275927424430847\n",
      "Epoch 1880, Loss: 1.482744574546814, Final Batch Loss: 0.3491459786891937\n",
      "Epoch 1881, Loss: 1.4236781597137451, Final Batch Loss: 0.3863125443458557\n",
      "Epoch 1882, Loss: 1.4432227611541748, Final Batch Loss: 0.37465620040893555\n",
      "Epoch 1883, Loss: 1.520442545413971, Final Batch Loss: 0.3482256531715393\n",
      "Epoch 1884, Loss: 1.4444268345832825, Final Batch Loss: 0.3597111701965332\n",
      "Epoch 1885, Loss: 1.381268471479416, Final Batch Loss: 0.3128769099712372\n",
      "Epoch 1886, Loss: 1.333361268043518, Final Batch Loss: 0.3127706050872803\n",
      "Epoch 1887, Loss: 1.3987496495246887, Final Batch Loss: 0.24888822436332703\n",
      "Epoch 1888, Loss: 1.3826242089271545, Final Batch Loss: 0.32753482460975647\n",
      "Epoch 1889, Loss: 1.4921285510063171, Final Batch Loss: 0.35889360308647156\n",
      "Epoch 1890, Loss: 1.4071739315986633, Final Batch Loss: 0.384840190410614\n",
      "Epoch 1891, Loss: 1.4622230529785156, Final Batch Loss: 0.4220203161239624\n",
      "Epoch 1892, Loss: 1.4613379538059235, Final Batch Loss: 0.37809163331985474\n",
      "Epoch 1893, Loss: 1.4806123673915863, Final Batch Loss: 0.4257686734199524\n",
      "Epoch 1894, Loss: 1.4900260269641876, Final Batch Loss: 0.37710076570510864\n",
      "Epoch 1895, Loss: 1.390374332666397, Final Batch Loss: 0.3288078308105469\n",
      "Epoch 1896, Loss: 1.3283258974552155, Final Batch Loss: 0.2678914964199066\n",
      "Epoch 1897, Loss: 1.5020229518413544, Final Batch Loss: 0.37382030487060547\n",
      "Epoch 1898, Loss: 1.4643300473690033, Final Batch Loss: 0.4123862087726593\n",
      "Epoch 1899, Loss: 1.5367027521133423, Final Batch Loss: 0.30615726113319397\n",
      "Epoch 1900, Loss: 1.337718278169632, Final Batch Loss: 0.26875534653663635\n",
      "Epoch 1901, Loss: 1.5227487087249756, Final Batch Loss: 0.42615148425102234\n",
      "Epoch 1902, Loss: 1.4838378429412842, Final Batch Loss: 0.3828319311141968\n",
      "Epoch 1903, Loss: 1.4660120904445648, Final Batch Loss: 0.4041731059551239\n",
      "Epoch 1904, Loss: 1.4596423208713531, Final Batch Loss: 0.329400897026062\n",
      "Epoch 1905, Loss: 1.474443256855011, Final Batch Loss: 0.362467885017395\n",
      "Epoch 1906, Loss: 1.3602011799812317, Final Batch Loss: 0.25689786672592163\n",
      "Epoch 1907, Loss: 1.516083687543869, Final Batch Loss: 0.31384459137916565\n",
      "Epoch 1908, Loss: 1.4650454223155975, Final Batch Loss: 0.34092381596565247\n",
      "Epoch 1909, Loss: 1.381665825843811, Final Batch Loss: 0.3222096264362335\n",
      "Epoch 1910, Loss: 1.409176081418991, Final Batch Loss: 0.3031950294971466\n",
      "Epoch 1911, Loss: 1.4980947077274323, Final Batch Loss: 0.409612774848938\n",
      "Epoch 1912, Loss: 1.5010166466236115, Final Batch Loss: 0.3953646123409271\n",
      "Epoch 1913, Loss: 1.3625898659229279, Final Batch Loss: 0.3020731806755066\n",
      "Epoch 1914, Loss: 1.4009613990783691, Final Batch Loss: 0.3720352351665497\n",
      "Epoch 1915, Loss: 1.3530331254005432, Final Batch Loss: 0.30806764960289\n",
      "Epoch 1916, Loss: 1.429131954908371, Final Batch Loss: 0.3756764233112335\n",
      "Epoch 1917, Loss: 1.414158433675766, Final Batch Loss: 0.3699789047241211\n",
      "Epoch 1918, Loss: 1.4408567249774933, Final Batch Loss: 0.39449456334114075\n",
      "Epoch 1919, Loss: 1.4043141901493073, Final Batch Loss: 0.44293150305747986\n",
      "Epoch 1920, Loss: 1.433503121137619, Final Batch Loss: 0.3640807569026947\n",
      "Epoch 1921, Loss: 1.3591743111610413, Final Batch Loss: 0.3434019684791565\n",
      "Epoch 1922, Loss: 1.4880500435829163, Final Batch Loss: 0.44118982553482056\n",
      "Epoch 1923, Loss: 1.496156632900238, Final Batch Loss: 0.3407088816165924\n",
      "Epoch 1924, Loss: 1.504628211259842, Final Batch Loss: 0.3596368730068207\n",
      "Epoch 1925, Loss: 1.3861992061138153, Final Batch Loss: 0.34959477186203003\n",
      "Epoch 1926, Loss: 1.3808391094207764, Final Batch Loss: 0.38531628251075745\n",
      "Epoch 1927, Loss: 1.4385966956615448, Final Batch Loss: 0.30767759680747986\n",
      "Epoch 1928, Loss: 1.4342678785324097, Final Batch Loss: 0.372221976518631\n",
      "Epoch 1929, Loss: 1.3789650201797485, Final Batch Loss: 0.3343296945095062\n",
      "Epoch 1930, Loss: 1.3701282292604446, Final Batch Loss: 0.24797363579273224\n",
      "Epoch 1931, Loss: 1.4424510300159454, Final Batch Loss: 0.42895233631134033\n",
      "Epoch 1932, Loss: 1.5447359681129456, Final Batch Loss: 0.4166328012943268\n",
      "Epoch 1933, Loss: 1.3108032047748566, Final Batch Loss: 0.2564961016178131\n",
      "Epoch 1934, Loss: 1.4200808107852936, Final Batch Loss: 0.3391267955303192\n",
      "Epoch 1935, Loss: 1.4311367571353912, Final Batch Loss: 0.41792017221450806\n",
      "Epoch 1936, Loss: 1.3367105722427368, Final Batch Loss: 0.29880934953689575\n",
      "Epoch 1937, Loss: 1.4711202085018158, Final Batch Loss: 0.32433053851127625\n",
      "Epoch 1938, Loss: 1.347148597240448, Final Batch Loss: 0.2846275568008423\n",
      "Epoch 1939, Loss: 1.3670605421066284, Final Batch Loss: 0.3472227156162262\n",
      "Epoch 1940, Loss: 1.4303078055381775, Final Batch Loss: 0.4216943681240082\n",
      "Epoch 1941, Loss: 1.257310077548027, Final Batch Loss: 0.24873049557209015\n",
      "Epoch 1942, Loss: 1.4616640508174896, Final Batch Loss: 0.44547051191329956\n",
      "Epoch 1943, Loss: 1.4011209309101105, Final Batch Loss: 0.32647934556007385\n",
      "Epoch 1944, Loss: 1.4662465155124664, Final Batch Loss: 0.3727911114692688\n",
      "Epoch 1945, Loss: 1.495936781167984, Final Batch Loss: 0.3890902101993561\n",
      "Epoch 1946, Loss: 1.4082265794277191, Final Batch Loss: 0.3375850021839142\n",
      "Epoch 1947, Loss: 1.4470263421535492, Final Batch Loss: 0.42004698514938354\n",
      "Epoch 1948, Loss: 1.3080509305000305, Final Batch Loss: 0.35868772864341736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1949, Loss: 1.443748116493225, Final Batch Loss: 0.41242286562919617\n",
      "Epoch 1950, Loss: 1.4158725142478943, Final Batch Loss: 0.41041892766952515\n",
      "Epoch 1951, Loss: 1.3763515055179596, Final Batch Loss: 0.2740972638130188\n",
      "Epoch 1952, Loss: 1.3713906705379486, Final Batch Loss: 0.3157503008842468\n",
      "Epoch 1953, Loss: 1.3618062138557434, Final Batch Loss: 0.26557257771492004\n",
      "Epoch 1954, Loss: 1.353417545557022, Final Batch Loss: 0.36168718338012695\n",
      "Epoch 1955, Loss: 1.3152870535850525, Final Batch Loss: 0.3495556116104126\n",
      "Epoch 1956, Loss: 1.4372031390666962, Final Batch Loss: 0.29762202501296997\n",
      "Epoch 1957, Loss: 1.4947192072868347, Final Batch Loss: 0.33744409680366516\n",
      "Epoch 1958, Loss: 1.5047155320644379, Final Batch Loss: 0.39565297961235046\n",
      "Epoch 1959, Loss: 1.3865886330604553, Final Batch Loss: 0.36239099502563477\n",
      "Epoch 1960, Loss: 1.3824160993099213, Final Batch Loss: 0.3731135427951813\n",
      "Epoch 1961, Loss: 1.443317711353302, Final Batch Loss: 0.4015748202800751\n",
      "Epoch 1962, Loss: 1.5006176233291626, Final Batch Loss: 0.3671874403953552\n",
      "Epoch 1963, Loss: 1.5453290343284607, Final Batch Loss: 0.4510035216808319\n",
      "Epoch 1964, Loss: 1.471310019493103, Final Batch Loss: 0.390383243560791\n",
      "Epoch 1965, Loss: 1.5404675900936127, Final Batch Loss: 0.44227564334869385\n",
      "Epoch 1966, Loss: 1.4425859153270721, Final Batch Loss: 0.3122100234031677\n",
      "Epoch 1967, Loss: 1.4689303040504456, Final Batch Loss: 0.36431920528411865\n",
      "Epoch 1968, Loss: 1.4441936910152435, Final Batch Loss: 0.29647085070610046\n",
      "Epoch 1969, Loss: 1.3948801159858704, Final Batch Loss: 0.2913067936897278\n",
      "Epoch 1970, Loss: 1.4341602623462677, Final Batch Loss: 0.3809306025505066\n",
      "Epoch 1971, Loss: 1.2820960730314255, Final Batch Loss: 0.23753778636455536\n",
      "Epoch 1972, Loss: 1.4044740200042725, Final Batch Loss: 0.31826770305633545\n",
      "Epoch 1973, Loss: 1.4170334935188293, Final Batch Loss: 0.44706830382347107\n",
      "Epoch 1974, Loss: 1.3035849630832672, Final Batch Loss: 0.2605403661727905\n",
      "Epoch 1975, Loss: 1.3964753448963165, Final Batch Loss: 0.33952176570892334\n",
      "Epoch 1976, Loss: 1.3992010056972504, Final Batch Loss: 0.34215694665908813\n",
      "Epoch 1977, Loss: 1.4967810809612274, Final Batch Loss: 0.4135954976081848\n",
      "Epoch 1978, Loss: 1.431190311908722, Final Batch Loss: 0.3793471157550812\n",
      "Epoch 1979, Loss: 1.3565990626811981, Final Batch Loss: 0.3048444390296936\n",
      "Epoch 1980, Loss: 1.4186748564243317, Final Batch Loss: 0.4017590284347534\n",
      "Epoch 1981, Loss: 1.5388443171977997, Final Batch Loss: 0.4863179922103882\n",
      "Epoch 1982, Loss: 1.4009223878383636, Final Batch Loss: 0.31027600169181824\n",
      "Epoch 1983, Loss: 1.5010922849178314, Final Batch Loss: 0.3818630874156952\n",
      "Epoch 1984, Loss: 1.4099413752555847, Final Batch Loss: 0.35056591033935547\n",
      "Epoch 1985, Loss: 1.38152015209198, Final Batch Loss: 0.34212008118629456\n",
      "Epoch 1986, Loss: 1.3724934309720993, Final Batch Loss: 0.22237415611743927\n",
      "Epoch 1987, Loss: 1.509911209344864, Final Batch Loss: 0.3276369273662567\n",
      "Epoch 1988, Loss: 1.4264657199382782, Final Batch Loss: 0.3921123445034027\n",
      "Epoch 1989, Loss: 1.4465230703353882, Final Batch Loss: 0.354358434677124\n",
      "Epoch 1990, Loss: 1.4120801091194153, Final Batch Loss: 0.3418789207935333\n",
      "Epoch 1991, Loss: 1.4024995863437653, Final Batch Loss: 0.314620703458786\n",
      "Epoch 1992, Loss: 1.4846517443656921, Final Batch Loss: 0.38035497069358826\n",
      "Epoch 1993, Loss: 1.4551781713962555, Final Batch Loss: 0.3359847068786621\n",
      "Epoch 1994, Loss: 1.5189969539642334, Final Batch Loss: 0.3885573148727417\n",
      "Epoch 1995, Loss: 1.3652502596378326, Final Batch Loss: 0.3368205726146698\n",
      "Epoch 1996, Loss: 1.3604670763015747, Final Batch Loss: 0.30635741353034973\n",
      "Epoch 1997, Loss: 1.3771388530731201, Final Batch Loss: 0.2909708321094513\n",
      "Epoch 1998, Loss: 1.3821467459201813, Final Batch Loss: 0.28461283445358276\n",
      "Epoch 1999, Loss: 1.3805364072322845, Final Batch Loss: 0.26155465841293335\n",
      "Epoch 2000, Loss: 1.452641636133194, Final Batch Loss: 0.384338915348053\n",
      "Epoch 2001, Loss: 1.4461725354194641, Final Batch Loss: 0.3951900005340576\n",
      "Epoch 2002, Loss: 1.4942121505737305, Final Batch Loss: 0.41872286796569824\n",
      "Epoch 2003, Loss: 1.3498231768608093, Final Batch Loss: 0.31104618310928345\n",
      "Epoch 2004, Loss: 1.4659154415130615, Final Batch Loss: 0.331000417470932\n",
      "Epoch 2005, Loss: 1.4766552150249481, Final Batch Loss: 0.2716216444969177\n",
      "Epoch 2006, Loss: 1.2886089086532593, Final Batch Loss: 0.3215213716030121\n",
      "Epoch 2007, Loss: 1.4733670055866241, Final Batch Loss: 0.37128087878227234\n",
      "Epoch 2008, Loss: 1.3757812678813934, Final Batch Loss: 0.3306483328342438\n",
      "Epoch 2009, Loss: 1.423269808292389, Final Batch Loss: 0.392923504114151\n",
      "Epoch 2010, Loss: 1.3451629281044006, Final Batch Loss: 0.32110658288002014\n",
      "Epoch 2011, Loss: 1.4731556177139282, Final Batch Loss: 0.40500786900520325\n",
      "Epoch 2012, Loss: 1.3933888673782349, Final Batch Loss: 0.32217127084732056\n",
      "Epoch 2013, Loss: 1.412183552980423, Final Batch Loss: 0.40559306740760803\n",
      "Epoch 2014, Loss: 1.4227645993232727, Final Batch Loss: 0.35531213879585266\n",
      "Epoch 2015, Loss: 1.2979449033737183, Final Batch Loss: 0.28245025873184204\n",
      "Epoch 2016, Loss: 1.4646306931972504, Final Batch Loss: 0.3201839029788971\n",
      "Epoch 2017, Loss: 1.563714474439621, Final Batch Loss: 0.4587693214416504\n",
      "Epoch 2018, Loss: 1.4751152694225311, Final Batch Loss: 0.3628877103328705\n",
      "Epoch 2019, Loss: 1.3458752036094666, Final Batch Loss: 0.2583903968334198\n",
      "Epoch 2020, Loss: 1.3819244503974915, Final Batch Loss: 0.27297383546829224\n",
      "Epoch 2021, Loss: 1.3815631568431854, Final Batch Loss: 0.34753304719924927\n",
      "Epoch 2022, Loss: 1.4991118907928467, Final Batch Loss: 0.3328377902507782\n",
      "Epoch 2023, Loss: 1.373474657535553, Final Batch Loss: 0.4205358624458313\n",
      "Epoch 2024, Loss: 1.2842651307582855, Final Batch Loss: 0.3073993921279907\n",
      "Epoch 2025, Loss: 1.3683189749717712, Final Batch Loss: 0.26074883341789246\n",
      "Epoch 2026, Loss: 1.3486799001693726, Final Batch Loss: 0.34099698066711426\n",
      "Epoch 2027, Loss: 1.517223596572876, Final Batch Loss: 0.3449656665325165\n",
      "Epoch 2028, Loss: 1.4222054183483124, Final Batch Loss: 0.29126355051994324\n",
      "Epoch 2029, Loss: 1.4310325384140015, Final Batch Loss: 0.3304283320903778\n",
      "Epoch 2030, Loss: 1.4457967281341553, Final Batch Loss: 0.34489235281944275\n",
      "Epoch 2031, Loss: 1.392115443944931, Final Batch Loss: 0.3090311288833618\n",
      "Epoch 2032, Loss: 1.390746384859085, Final Batch Loss: 0.33333882689476013\n",
      "Epoch 2033, Loss: 1.5569268465042114, Final Batch Loss: 0.45106470584869385\n",
      "Epoch 2034, Loss: 1.37229722738266, Final Batch Loss: 0.3099692761898041\n",
      "Epoch 2035, Loss: 1.447436660528183, Final Batch Loss: 0.41827496886253357\n",
      "Epoch 2036, Loss: 1.3660508394241333, Final Batch Loss: 0.369263619184494\n",
      "Epoch 2037, Loss: 1.3197790682315826, Final Batch Loss: 0.24089130759239197\n",
      "Epoch 2038, Loss: 1.5688668191432953, Final Batch Loss: 0.536588728427887\n",
      "Epoch 2039, Loss: 1.365468829870224, Final Batch Loss: 0.3553643226623535\n",
      "Epoch 2040, Loss: 1.33060023188591, Final Batch Loss: 0.3152971565723419\n",
      "Epoch 2041, Loss: 1.3720826357603073, Final Batch Loss: 0.20167098939418793\n",
      "Epoch 2042, Loss: 1.325511872768402, Final Batch Loss: 0.2438393533229828\n",
      "Epoch 2043, Loss: 1.355290174484253, Final Batch Loss: 0.3902464509010315\n",
      "Epoch 2044, Loss: 1.3311187624931335, Final Batch Loss: 0.30683067440986633\n",
      "Epoch 2045, Loss: 1.4533637762069702, Final Batch Loss: 0.4036892056465149\n",
      "Epoch 2046, Loss: 1.3555046319961548, Final Batch Loss: 0.30382493138313293\n",
      "Epoch 2047, Loss: 1.3916206359863281, Final Batch Loss: 0.3634820282459259\n",
      "Epoch 2048, Loss: 1.3247917592525482, Final Batch Loss: 0.4003865718841553\n",
      "Epoch 2049, Loss: 1.5110843777656555, Final Batch Loss: 0.4758096933364868\n",
      "Epoch 2050, Loss: 1.3183152973651886, Final Batch Loss: 0.32617247104644775\n",
      "Epoch 2051, Loss: 1.3522691428661346, Final Batch Loss: 0.39095833897590637\n",
      "Epoch 2052, Loss: 1.4592857360839844, Final Batch Loss: 0.39163240790367126\n",
      "Epoch 2053, Loss: 1.2752977311611176, Final Batch Loss: 0.3140227794647217\n",
      "Epoch 2054, Loss: 1.2822242081165314, Final Batch Loss: 0.2782798707485199\n",
      "Epoch 2055, Loss: 1.4246796369552612, Final Batch Loss: 0.4041505753993988\n",
      "Epoch 2056, Loss: 1.2740678787231445, Final Batch Loss: 0.29878848791122437\n",
      "Epoch 2057, Loss: 1.4120707213878632, Final Batch Loss: 0.46311622858047485\n",
      "Epoch 2058, Loss: 1.3917333781719208, Final Batch Loss: 0.3394477963447571\n",
      "Epoch 2059, Loss: 1.307369202375412, Final Batch Loss: 0.30938518047332764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2060, Loss: 1.5966325402259827, Final Batch Loss: 0.5967106223106384\n",
      "Epoch 2061, Loss: 1.3371174037456512, Final Batch Loss: 0.26679426431655884\n",
      "Epoch 2062, Loss: 1.263838529586792, Final Batch Loss: 0.31044745445251465\n",
      "Epoch 2063, Loss: 1.478230744600296, Final Batch Loss: 0.4415108263492584\n",
      "Epoch 2064, Loss: 1.552779734134674, Final Batch Loss: 0.39987489581108093\n",
      "Epoch 2065, Loss: 1.3901026248931885, Final Batch Loss: 0.32395991683006287\n",
      "Epoch 2066, Loss: 1.2969577610492706, Final Batch Loss: 0.2597261071205139\n",
      "Epoch 2067, Loss: 1.3277014195919037, Final Batch Loss: 0.3238745927810669\n",
      "Epoch 2068, Loss: 1.4552790820598602, Final Batch Loss: 0.40866443514823914\n",
      "Epoch 2069, Loss: 1.3776854276657104, Final Batch Loss: 0.32958149909973145\n",
      "Epoch 2070, Loss: 1.335790902376175, Final Batch Loss: 0.30298712849617004\n",
      "Epoch 2071, Loss: 1.3134389519691467, Final Batch Loss: 0.2699468731880188\n",
      "Epoch 2072, Loss: 1.3819871246814728, Final Batch Loss: 0.31995272636413574\n",
      "Epoch 2073, Loss: 1.4345886707305908, Final Batch Loss: 0.3748467266559601\n",
      "Epoch 2074, Loss: 1.3882498145103455, Final Batch Loss: 0.2859218716621399\n",
      "Epoch 2075, Loss: 1.461045354604721, Final Batch Loss: 0.38959869742393494\n",
      "Epoch 2076, Loss: 1.3523165583610535, Final Batch Loss: 0.33145031332969666\n",
      "Epoch 2077, Loss: 1.4240622520446777, Final Batch Loss: 0.3068530261516571\n",
      "Epoch 2078, Loss: 1.4533201158046722, Final Batch Loss: 0.37394988536834717\n",
      "Epoch 2079, Loss: 1.4480767250061035, Final Batch Loss: 0.4106816053390503\n",
      "Epoch 2080, Loss: 1.4916138350963593, Final Batch Loss: 0.3718340992927551\n",
      "Epoch 2081, Loss: 1.43289253115654, Final Batch Loss: 0.45085370540618896\n",
      "Epoch 2082, Loss: 1.4676428735256195, Final Batch Loss: 0.34987181425094604\n",
      "Epoch 2083, Loss: 1.3358060121536255, Final Batch Loss: 0.37149518728256226\n",
      "Epoch 2084, Loss: 1.3209185898303986, Final Batch Loss: 0.3769207298755646\n",
      "Epoch 2085, Loss: 1.411441445350647, Final Batch Loss: 0.433601975440979\n",
      "Epoch 2086, Loss: 1.512386530637741, Final Batch Loss: 0.43120628595352173\n",
      "Epoch 2087, Loss: 1.3549092411994934, Final Batch Loss: 0.2581145465373993\n",
      "Epoch 2088, Loss: 1.5478662550449371, Final Batch Loss: 0.48964977264404297\n",
      "Epoch 2089, Loss: 1.447953999042511, Final Batch Loss: 0.3618500530719757\n",
      "Epoch 2090, Loss: 1.4670891165733337, Final Batch Loss: 0.25824496150016785\n",
      "Epoch 2091, Loss: 1.4390561878681183, Final Batch Loss: 0.3625223934650421\n",
      "Epoch 2092, Loss: 1.5384613573551178, Final Batch Loss: 0.4849036633968353\n",
      "Epoch 2093, Loss: 1.403166025876999, Final Batch Loss: 0.4038463234901428\n",
      "Epoch 2094, Loss: 1.5244317054748535, Final Batch Loss: 0.3611249625682831\n",
      "Epoch 2095, Loss: 1.3451383411884308, Final Batch Loss: 0.34298965334892273\n",
      "Epoch 2096, Loss: 1.3932925760746002, Final Batch Loss: 0.37074965238571167\n",
      "Epoch 2097, Loss: 1.3269765973091125, Final Batch Loss: 0.29368355870246887\n",
      "Epoch 2098, Loss: 1.4323320388793945, Final Batch Loss: 0.29546278715133667\n",
      "Epoch 2099, Loss: 1.3621817529201508, Final Batch Loss: 0.3703959584236145\n",
      "Epoch 2100, Loss: 1.4063496589660645, Final Batch Loss: 0.3377184271812439\n",
      "Epoch 2101, Loss: 1.350858986377716, Final Batch Loss: 0.3755066394805908\n",
      "Epoch 2102, Loss: 1.396695852279663, Final Batch Loss: 0.3124261200428009\n",
      "Epoch 2103, Loss: 1.3566913902759552, Final Batch Loss: 0.32006946206092834\n",
      "Epoch 2104, Loss: 1.4841867685317993, Final Batch Loss: 0.31870973110198975\n",
      "Epoch 2105, Loss: 1.3289815485477448, Final Batch Loss: 0.3013182282447815\n",
      "Epoch 2106, Loss: 1.4404257237911224, Final Batch Loss: 0.4975009560585022\n",
      "Epoch 2107, Loss: 1.410024106502533, Final Batch Loss: 0.3410644829273224\n",
      "Epoch 2108, Loss: 1.3881610035896301, Final Batch Loss: 0.3990526795387268\n",
      "Epoch 2109, Loss: 1.3579542934894562, Final Batch Loss: 0.36011937260627747\n",
      "Epoch 2110, Loss: 1.4689587950706482, Final Batch Loss: 0.4101680517196655\n",
      "Epoch 2111, Loss: 1.4502727389335632, Final Batch Loss: 0.3855542242527008\n",
      "Epoch 2112, Loss: 1.3545364737510681, Final Batch Loss: 0.27308914065361023\n",
      "Epoch 2113, Loss: 1.3405524790287018, Final Batch Loss: 0.27252107858657837\n",
      "Epoch 2114, Loss: 1.3468826413154602, Final Batch Loss: 0.3348625898361206\n",
      "Epoch 2115, Loss: 1.4373275935649872, Final Batch Loss: 0.3581075668334961\n",
      "Epoch 2116, Loss: 1.3417142629623413, Final Batch Loss: 0.30954089760780334\n",
      "Epoch 2117, Loss: 1.2945838570594788, Final Batch Loss: 0.3384533226490021\n",
      "Epoch 2118, Loss: 1.3937109410762787, Final Batch Loss: 0.35421013832092285\n",
      "Epoch 2119, Loss: 1.3668506145477295, Final Batch Loss: 0.33579856157302856\n",
      "Epoch 2120, Loss: 1.2465478479862213, Final Batch Loss: 0.2573091685771942\n",
      "Epoch 2121, Loss: 1.378711849451065, Final Batch Loss: 0.3763582706451416\n",
      "Epoch 2122, Loss: 1.4417424499988556, Final Batch Loss: 0.3660027086734772\n",
      "Epoch 2123, Loss: 1.3678873479366302, Final Batch Loss: 0.3664988875389099\n",
      "Epoch 2124, Loss: 1.3480227291584015, Final Batch Loss: 0.33854520320892334\n",
      "Epoch 2125, Loss: 1.2955370843410492, Final Batch Loss: 0.34787437319755554\n",
      "Epoch 2126, Loss: 1.4087586104869843, Final Batch Loss: 0.4017890691757202\n",
      "Epoch 2127, Loss: 1.2442408055067062, Final Batch Loss: 0.2257348746061325\n",
      "Epoch 2128, Loss: 1.331703543663025, Final Batch Loss: 0.30647367238998413\n",
      "Epoch 2129, Loss: 1.357809454202652, Final Batch Loss: 0.3224242031574249\n",
      "Epoch 2130, Loss: 1.4513827860355377, Final Batch Loss: 0.45393577218055725\n",
      "Epoch 2131, Loss: 1.2697109878063202, Final Batch Loss: 0.31298115849494934\n",
      "Epoch 2132, Loss: 1.2566838562488556, Final Batch Loss: 0.33324700593948364\n",
      "Epoch 2133, Loss: 1.3568547070026398, Final Batch Loss: 0.3557499647140503\n",
      "Epoch 2134, Loss: 1.1956146359443665, Final Batch Loss: 0.24175351858139038\n",
      "Epoch 2135, Loss: 1.3198313117027283, Final Batch Loss: 0.3491048216819763\n",
      "Epoch 2136, Loss: 1.3119851350784302, Final Batch Loss: 0.320278525352478\n",
      "Epoch 2137, Loss: 1.3745113909244537, Final Batch Loss: 0.29943129420280457\n",
      "Epoch 2138, Loss: 1.2510782480239868, Final Batch Loss: 0.274501234292984\n",
      "Epoch 2139, Loss: 1.425453782081604, Final Batch Loss: 0.4703906774520874\n",
      "Epoch 2140, Loss: 1.2862875014543533, Final Batch Loss: 0.24348996579647064\n",
      "Epoch 2141, Loss: 1.3810404539108276, Final Batch Loss: 0.36089909076690674\n",
      "Epoch 2142, Loss: 1.3810689747333527, Final Batch Loss: 0.43321481347084045\n",
      "Epoch 2143, Loss: 1.450319528579712, Final Batch Loss: 0.4230560064315796\n",
      "Epoch 2144, Loss: 1.1341902613639832, Final Batch Loss: 0.23733481764793396\n",
      "Epoch 2145, Loss: 1.3422634899616241, Final Batch Loss: 0.3688121438026428\n",
      "Epoch 2146, Loss: 1.4327743649482727, Final Batch Loss: 0.35219281911849976\n",
      "Epoch 2147, Loss: 1.4674693048000336, Final Batch Loss: 0.4150843620300293\n",
      "Epoch 2148, Loss: 1.4032206535339355, Final Batch Loss: 0.3172960877418518\n",
      "Epoch 2149, Loss: 1.3434761464595795, Final Batch Loss: 0.2627549469470978\n",
      "Epoch 2150, Loss: 1.497435063123703, Final Batch Loss: 0.4696197211742401\n",
      "Epoch 2151, Loss: 1.3488997519016266, Final Batch Loss: 0.29405704140663147\n",
      "Epoch 2152, Loss: 1.2980148494243622, Final Batch Loss: 0.2701054513454437\n",
      "Epoch 2153, Loss: 1.36151784658432, Final Batch Loss: 0.32651931047439575\n",
      "Epoch 2154, Loss: 1.3990780413150787, Final Batch Loss: 0.25638797879219055\n",
      "Epoch 2155, Loss: 1.3429617285728455, Final Batch Loss: 0.28374525904655457\n",
      "Epoch 2156, Loss: 1.4536937773227692, Final Batch Loss: 0.40671461820602417\n",
      "Epoch 2157, Loss: 1.3282118737697601, Final Batch Loss: 0.31203123927116394\n",
      "Epoch 2158, Loss: 1.4838551878929138, Final Batch Loss: 0.5426641702651978\n",
      "Epoch 2159, Loss: 1.441569447517395, Final Batch Loss: 0.3556385934352875\n",
      "Epoch 2160, Loss: 1.3086429834365845, Final Batch Loss: 0.33455193042755127\n",
      "Epoch 2161, Loss: 1.3809175193309784, Final Batch Loss: 0.3950170576572418\n",
      "Epoch 2162, Loss: 1.4890601336956024, Final Batch Loss: 0.3588188588619232\n",
      "Epoch 2163, Loss: 1.409967452287674, Final Batch Loss: 0.45232057571411133\n",
      "Epoch 2164, Loss: 1.37428218126297, Final Batch Loss: 0.262003630399704\n",
      "Epoch 2165, Loss: 1.209676519036293, Final Batch Loss: 0.21154554188251495\n",
      "Epoch 2166, Loss: 1.3073988258838654, Final Batch Loss: 0.25565239787101746\n",
      "Epoch 2167, Loss: 1.3635541796684265, Final Batch Loss: 0.37720417976379395\n",
      "Epoch 2168, Loss: 1.3246000111103058, Final Batch Loss: 0.29214462637901306\n",
      "Epoch 2169, Loss: 1.4641073942184448, Final Batch Loss: 0.40784502029418945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2170, Loss: 1.250646948814392, Final Batch Loss: 0.31951865553855896\n",
      "Epoch 2171, Loss: 1.264149785041809, Final Batch Loss: 0.3334349989891052\n",
      "Epoch 2172, Loss: 1.3520613014698029, Final Batch Loss: 0.23322224617004395\n",
      "Epoch 2173, Loss: 1.4628072082996368, Final Batch Loss: 0.39271894097328186\n",
      "Epoch 2174, Loss: 1.3542757034301758, Final Batch Loss: 0.30299612879753113\n",
      "Epoch 2175, Loss: 1.3866166770458221, Final Batch Loss: 0.2550892233848572\n",
      "Epoch 2176, Loss: 1.4681473672389984, Final Batch Loss: 0.35529541969299316\n",
      "Epoch 2177, Loss: 1.3068963885307312, Final Batch Loss: 0.36047863960266113\n",
      "Epoch 2178, Loss: 1.348867118358612, Final Batch Loss: 0.39905402064323425\n",
      "Epoch 2179, Loss: 1.232232928276062, Final Batch Loss: 0.36291995644569397\n",
      "Epoch 2180, Loss: 1.3116137087345123, Final Batch Loss: 0.31636351346969604\n",
      "Epoch 2181, Loss: 1.317441999912262, Final Batch Loss: 0.2710174024105072\n",
      "Epoch 2182, Loss: 1.3060704469680786, Final Batch Loss: 0.35506588220596313\n",
      "Epoch 2183, Loss: 1.3758570551872253, Final Batch Loss: 0.32309025526046753\n",
      "Epoch 2184, Loss: 1.3177679479122162, Final Batch Loss: 0.30767548084259033\n",
      "Epoch 2185, Loss: 1.2893407940864563, Final Batch Loss: 0.3216613233089447\n",
      "Epoch 2186, Loss: 1.3242634683847427, Final Batch Loss: 0.22821606695652008\n",
      "Epoch 2187, Loss: 1.4484496116638184, Final Batch Loss: 0.4460272192955017\n",
      "Epoch 2188, Loss: 1.4355282187461853, Final Batch Loss: 0.4516143202781677\n",
      "Epoch 2189, Loss: 1.3907526731491089, Final Batch Loss: 0.35735201835632324\n",
      "Epoch 2190, Loss: 1.4974657595157623, Final Batch Loss: 0.4073711335659027\n",
      "Epoch 2191, Loss: 1.2233276069164276, Final Batch Loss: 0.28353768587112427\n",
      "Epoch 2192, Loss: 1.447727769613266, Final Batch Loss: 0.4522911608219147\n",
      "Epoch 2193, Loss: 1.3460588902235031, Final Batch Loss: 0.2352384477853775\n",
      "Epoch 2194, Loss: 1.4111129641532898, Final Batch Loss: 0.38867250084877014\n",
      "Epoch 2195, Loss: 1.319506585597992, Final Batch Loss: 0.3376043140888214\n",
      "Epoch 2196, Loss: 1.413017064332962, Final Batch Loss: 0.3286343514919281\n",
      "Epoch 2197, Loss: 1.362759530544281, Final Batch Loss: 0.3315298557281494\n",
      "Epoch 2198, Loss: 1.366867333650589, Final Batch Loss: 0.3614141047000885\n",
      "Epoch 2199, Loss: 1.3553010821342468, Final Batch Loss: 0.38028684258461\n",
      "Epoch 2200, Loss: 1.3272660672664642, Final Batch Loss: 0.3402150571346283\n",
      "Epoch 2201, Loss: 1.4555898010730743, Final Batch Loss: 0.38752663135528564\n",
      "Epoch 2202, Loss: 1.3303254842758179, Final Batch Loss: 0.3348228335380554\n",
      "Epoch 2203, Loss: 1.2309628427028656, Final Batch Loss: 0.2686827480792999\n",
      "Epoch 2204, Loss: 1.3671515882015228, Final Batch Loss: 0.3884999752044678\n",
      "Epoch 2205, Loss: 1.3665490448474884, Final Batch Loss: 0.3079867959022522\n",
      "Epoch 2206, Loss: 1.3921692669391632, Final Batch Loss: 0.4519818425178528\n",
      "Epoch 2207, Loss: 1.3009848594665527, Final Batch Loss: 0.3154365122318268\n",
      "Epoch 2208, Loss: 1.2431861460208893, Final Batch Loss: 0.2591203451156616\n",
      "Epoch 2209, Loss: 1.3271008133888245, Final Batch Loss: 0.3423365652561188\n",
      "Epoch 2210, Loss: 1.4245965778827667, Final Batch Loss: 0.3827787935733795\n",
      "Epoch 2211, Loss: 1.3860481679439545, Final Batch Loss: 0.38021472096443176\n",
      "Epoch 2212, Loss: 1.3472325205802917, Final Batch Loss: 0.34998276829719543\n",
      "Epoch 2213, Loss: 1.4114361107349396, Final Batch Loss: 0.4518256187438965\n",
      "Epoch 2214, Loss: 1.3127313554286957, Final Batch Loss: 0.32790863513946533\n",
      "Epoch 2215, Loss: 1.393391638994217, Final Batch Loss: 0.36300304532051086\n",
      "Epoch 2216, Loss: 1.3217073380947113, Final Batch Loss: 0.3364610970020294\n",
      "Epoch 2217, Loss: 1.2860573530197144, Final Batch Loss: 0.2577517628669739\n",
      "Epoch 2218, Loss: 1.3014681935310364, Final Batch Loss: 0.3702916204929352\n",
      "Epoch 2219, Loss: 1.3962672352790833, Final Batch Loss: 0.29935699701309204\n",
      "Epoch 2220, Loss: 1.3655413687229156, Final Batch Loss: 0.3655051290988922\n",
      "Epoch 2221, Loss: 1.3977300226688385, Final Batch Loss: 0.3197254240512848\n",
      "Epoch 2222, Loss: 1.3457515239715576, Final Batch Loss: 0.3781035244464874\n",
      "Epoch 2223, Loss: 1.2922590672969818, Final Batch Loss: 0.3302825391292572\n",
      "Epoch 2224, Loss: 1.3634840846061707, Final Batch Loss: 0.30288171768188477\n",
      "Epoch 2225, Loss: 1.3982390761375427, Final Batch Loss: 0.4149473011493683\n",
      "Epoch 2226, Loss: 1.267508089542389, Final Batch Loss: 0.29136979579925537\n",
      "Epoch 2227, Loss: 1.3003428429365158, Final Batch Loss: 0.2475985735654831\n",
      "Epoch 2228, Loss: 1.2350807189941406, Final Batch Loss: 0.2692941725254059\n",
      "Epoch 2229, Loss: 1.3475331366062164, Final Batch Loss: 0.2877993881702423\n",
      "Epoch 2230, Loss: 1.2766901552677155, Final Batch Loss: 0.2951467037200928\n",
      "Epoch 2231, Loss: 1.3042813837528229, Final Batch Loss: 0.3211609721183777\n",
      "Epoch 2232, Loss: 1.3785497844219208, Final Batch Loss: 0.3809850513935089\n",
      "Epoch 2233, Loss: 1.3712660670280457, Final Batch Loss: 0.449012815952301\n",
      "Epoch 2234, Loss: 1.3336431086063385, Final Batch Loss: 0.29302409291267395\n",
      "Epoch 2235, Loss: 1.34619602560997, Final Batch Loss: 0.3008010983467102\n",
      "Epoch 2236, Loss: 1.2826075553894043, Final Batch Loss: 0.3528074026107788\n",
      "Epoch 2237, Loss: 1.2027151584625244, Final Batch Loss: 0.28247690200805664\n",
      "Epoch 2238, Loss: 1.4140042662620544, Final Batch Loss: 0.35713547468185425\n",
      "Epoch 2239, Loss: 1.3428278267383575, Final Batch Loss: 0.3863741457462311\n",
      "Epoch 2240, Loss: 1.3237645626068115, Final Batch Loss: 0.3039568364620209\n",
      "Epoch 2241, Loss: 1.3774461448192596, Final Batch Loss: 0.32138270139694214\n",
      "Epoch 2242, Loss: 1.4221458435058594, Final Batch Loss: 0.3278071880340576\n",
      "Epoch 2243, Loss: 1.4649886190891266, Final Batch Loss: 0.3602614104747772\n",
      "Epoch 2244, Loss: 1.4896624386310577, Final Batch Loss: 0.48477548360824585\n",
      "Epoch 2245, Loss: 1.3236691653728485, Final Batch Loss: 0.46072420477867126\n",
      "Epoch 2246, Loss: 1.319307118654251, Final Batch Loss: 0.325970321893692\n",
      "Epoch 2247, Loss: 1.3473146557807922, Final Batch Loss: 0.35168910026550293\n",
      "Epoch 2248, Loss: 1.3061675131320953, Final Batch Loss: 0.33672529458999634\n",
      "Epoch 2249, Loss: 1.5211041569709778, Final Batch Loss: 0.4113135039806366\n",
      "Epoch 2250, Loss: 1.2460691183805466, Final Batch Loss: 0.24426864087581635\n",
      "Epoch 2251, Loss: 1.3971508890390396, Final Batch Loss: 0.3581778407096863\n",
      "Epoch 2252, Loss: 1.4019596874713898, Final Batch Loss: 0.3924400508403778\n",
      "Epoch 2253, Loss: 1.3688745498657227, Final Batch Loss: 0.37792521715164185\n",
      "Epoch 2254, Loss: 1.5226641595363617, Final Batch Loss: 0.45920035243034363\n",
      "Epoch 2255, Loss: 1.260740965604782, Final Batch Loss: 0.29514721035957336\n",
      "Epoch 2256, Loss: 1.3479698300361633, Final Batch Loss: 0.32630279660224915\n",
      "Epoch 2257, Loss: 1.2408940494060516, Final Batch Loss: 0.2911152243614197\n",
      "Epoch 2258, Loss: 1.2905363738536835, Final Batch Loss: 0.297091543674469\n",
      "Epoch 2259, Loss: 1.2930026948451996, Final Batch Loss: 0.27894461154937744\n",
      "Epoch 2260, Loss: 1.399916648864746, Final Batch Loss: 0.3448028862476349\n",
      "Epoch 2261, Loss: 1.3676519095897675, Final Batch Loss: 0.3432341516017914\n",
      "Epoch 2262, Loss: 1.4241264760494232, Final Batch Loss: 0.3637373149394989\n",
      "Epoch 2263, Loss: 1.3138143718242645, Final Batch Loss: 0.40461423993110657\n",
      "Epoch 2264, Loss: 1.2391590774059296, Final Batch Loss: 0.2771468758583069\n",
      "Epoch 2265, Loss: 1.3304069340229034, Final Batch Loss: 0.2817593514919281\n",
      "Epoch 2266, Loss: 1.308542251586914, Final Batch Loss: 0.4152390956878662\n",
      "Epoch 2267, Loss: 1.267574816942215, Final Batch Loss: 0.32436302304267883\n",
      "Epoch 2268, Loss: 1.3759045898914337, Final Batch Loss: 0.4199730455875397\n",
      "Epoch 2269, Loss: 1.2663460075855255, Final Batch Loss: 0.2818509042263031\n",
      "Epoch 2270, Loss: 1.3984050452709198, Final Batch Loss: 0.3034113645553589\n",
      "Epoch 2271, Loss: 1.4615391492843628, Final Batch Loss: 0.5103248953819275\n",
      "Epoch 2272, Loss: 1.2225852757692337, Final Batch Loss: 0.24395279586315155\n",
      "Epoch 2273, Loss: 1.3764703571796417, Final Batch Loss: 0.4272209107875824\n",
      "Epoch 2274, Loss: 1.2795599102973938, Final Batch Loss: 0.21436139941215515\n",
      "Epoch 2275, Loss: 1.3857455253601074, Final Batch Loss: 0.34962424635887146\n",
      "Epoch 2276, Loss: 1.4413513541221619, Final Batch Loss: 0.38101285696029663\n",
      "Epoch 2277, Loss: 1.3323229551315308, Final Batch Loss: 0.32748061418533325\n",
      "Epoch 2278, Loss: 1.3571685254573822, Final Batch Loss: 0.26129046082496643\n",
      "Epoch 2279, Loss: 1.2835873067378998, Final Batch Loss: 0.35108935832977295\n",
      "Epoch 2280, Loss: 1.3255968242883682, Final Batch Loss: 0.34967711567878723\n",
      "Epoch 2281, Loss: 1.28712797164917, Final Batch Loss: 0.27551376819610596\n",
      "Epoch 2282, Loss: 1.226290374994278, Final Batch Loss: 0.25829213857650757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2283, Loss: 1.2543964982032776, Final Batch Loss: 0.23672828078269958\n",
      "Epoch 2284, Loss: 1.2174746692180634, Final Batch Loss: 0.25687235593795776\n",
      "Epoch 2285, Loss: 1.2648305594921112, Final Batch Loss: 0.25050967931747437\n",
      "Epoch 2286, Loss: 1.4276963472366333, Final Batch Loss: 0.3648155927658081\n",
      "Epoch 2287, Loss: 1.3065083026885986, Final Batch Loss: 0.3597957193851471\n",
      "Epoch 2288, Loss: 1.27951380610466, Final Batch Loss: 0.2906639575958252\n",
      "Epoch 2289, Loss: 1.3444125056266785, Final Batch Loss: 0.3233652710914612\n",
      "Epoch 2290, Loss: 1.4346254467964172, Final Batch Loss: 0.4342938959598541\n",
      "Epoch 2291, Loss: 1.4152207970619202, Final Batch Loss: 0.3396523594856262\n",
      "Epoch 2292, Loss: 1.4208133220672607, Final Batch Loss: 0.4193728268146515\n",
      "Epoch 2293, Loss: 1.3284048736095428, Final Batch Loss: 0.3053901493549347\n",
      "Epoch 2294, Loss: 1.2858355343341827, Final Batch Loss: 0.2891157865524292\n",
      "Epoch 2295, Loss: 1.411188155412674, Final Batch Loss: 0.3366056978702545\n",
      "Epoch 2296, Loss: 1.3539327085018158, Final Batch Loss: 0.3935582637786865\n",
      "Epoch 2297, Loss: 1.3522356450557709, Final Batch Loss: 0.3597448766231537\n",
      "Epoch 2298, Loss: 1.2951589822769165, Final Batch Loss: 0.30768275260925293\n",
      "Epoch 2299, Loss: 1.369765430688858, Final Batch Loss: 0.38237813115119934\n",
      "Epoch 2300, Loss: 1.330985426902771, Final Batch Loss: 0.3483920395374298\n",
      "Epoch 2301, Loss: 1.267729938030243, Final Batch Loss: 0.31500008702278137\n",
      "Epoch 2302, Loss: 1.209951564669609, Final Batch Loss: 0.32678619027137756\n",
      "Epoch 2303, Loss: 1.3639254570007324, Final Batch Loss: 0.3870185911655426\n",
      "Epoch 2304, Loss: 1.245107501745224, Final Batch Loss: 0.29837921261787415\n",
      "Epoch 2305, Loss: 1.4513455033302307, Final Batch Loss: 0.5352017283439636\n",
      "Epoch 2306, Loss: 1.2929362952709198, Final Batch Loss: 0.3245489001274109\n",
      "Epoch 2307, Loss: 1.4582944214344025, Final Batch Loss: 0.31593915820121765\n",
      "Epoch 2308, Loss: 1.3711703717708588, Final Batch Loss: 0.3665648102760315\n",
      "Epoch 2309, Loss: 1.2929280996322632, Final Batch Loss: 0.27534669637680054\n",
      "Epoch 2310, Loss: 1.1994138956069946, Final Batch Loss: 0.2750841975212097\n",
      "Epoch 2311, Loss: 1.254558801651001, Final Batch Loss: 0.30859678983688354\n",
      "Epoch 2312, Loss: 1.350926399230957, Final Batch Loss: 0.3568400740623474\n",
      "Epoch 2313, Loss: 1.3301550149917603, Final Batch Loss: 0.2674124836921692\n",
      "Epoch 2314, Loss: 1.3148697316646576, Final Batch Loss: 0.34101349115371704\n",
      "Epoch 2315, Loss: 1.2459441125392914, Final Batch Loss: 0.2905260920524597\n",
      "Epoch 2316, Loss: 1.3672830760478973, Final Batch Loss: 0.3667698800563812\n",
      "Epoch 2317, Loss: 1.17683944106102, Final Batch Loss: 0.2784050703048706\n",
      "Epoch 2318, Loss: 1.6129816472530365, Final Batch Loss: 0.5485983490943909\n",
      "Epoch 2319, Loss: 1.381431221961975, Final Batch Loss: 0.444511353969574\n",
      "Epoch 2320, Loss: 1.3157436847686768, Final Batch Loss: 0.2817779779434204\n",
      "Epoch 2321, Loss: 1.347528874874115, Final Batch Loss: 0.354352205991745\n",
      "Epoch 2322, Loss: 1.358224630355835, Final Batch Loss: 0.38379988074302673\n",
      "Epoch 2323, Loss: 1.3025550246238708, Final Batch Loss: 0.3398914635181427\n",
      "Epoch 2324, Loss: 1.3405407667160034, Final Batch Loss: 0.3678533732891083\n",
      "Epoch 2325, Loss: 1.1189680695533752, Final Batch Loss: 0.1871979832649231\n",
      "Epoch 2326, Loss: 1.4111010134220123, Final Batch Loss: 0.3483949899673462\n",
      "Epoch 2327, Loss: 1.2793211042881012, Final Batch Loss: 0.3037225604057312\n",
      "Epoch 2328, Loss: 1.3602035343647003, Final Batch Loss: 0.34625330567359924\n",
      "Epoch 2329, Loss: 1.318589299917221, Final Batch Loss: 0.35877764225006104\n",
      "Epoch 2330, Loss: 1.3580101132392883, Final Batch Loss: 0.4006273150444031\n",
      "Epoch 2331, Loss: 1.2180711179971695, Final Batch Loss: 0.22889123857021332\n",
      "Epoch 2332, Loss: 1.3398553431034088, Final Batch Loss: 0.37131646275520325\n",
      "Epoch 2333, Loss: 1.229472130537033, Final Batch Loss: 0.2579237222671509\n",
      "Epoch 2334, Loss: 1.2439242601394653, Final Batch Loss: 0.3569014072418213\n",
      "Epoch 2335, Loss: 1.5352946817874908, Final Batch Loss: 0.4779958426952362\n",
      "Epoch 2336, Loss: 1.2998506128787994, Final Batch Loss: 0.3844468295574188\n",
      "Epoch 2337, Loss: 1.2738621830940247, Final Batch Loss: 0.27355852723121643\n",
      "Epoch 2338, Loss: 1.3670047223567963, Final Batch Loss: 0.32774361968040466\n",
      "Epoch 2339, Loss: 1.2756603956222534, Final Batch Loss: 0.29583343863487244\n",
      "Epoch 2340, Loss: 1.337678849697113, Final Batch Loss: 0.266877144575119\n",
      "Epoch 2341, Loss: 1.3141556680202484, Final Batch Loss: 0.3670084774494171\n",
      "Epoch 2342, Loss: 1.244681179523468, Final Batch Loss: 0.3223699927330017\n",
      "Epoch 2343, Loss: 1.3391841351985931, Final Batch Loss: 0.3838344216346741\n",
      "Epoch 2344, Loss: 1.2358888685703278, Final Batch Loss: 0.3377990424633026\n",
      "Epoch 2345, Loss: 1.26949080824852, Final Batch Loss: 0.35369452834129333\n",
      "Epoch 2346, Loss: 1.285218745470047, Final Batch Loss: 0.3016844391822815\n",
      "Epoch 2347, Loss: 1.283619374036789, Final Batch Loss: 0.31154388189315796\n",
      "Epoch 2348, Loss: 1.4136200249195099, Final Batch Loss: 0.3627347946166992\n",
      "Epoch 2349, Loss: 1.302403211593628, Final Batch Loss: 0.3263143002986908\n",
      "Epoch 2350, Loss: 1.350835770368576, Final Batch Loss: 0.46286606788635254\n",
      "Epoch 2351, Loss: 1.2831125557422638, Final Batch Loss: 0.27062612771987915\n",
      "Epoch 2352, Loss: 1.205024465918541, Final Batch Loss: 0.20117755234241486\n",
      "Epoch 2353, Loss: 1.2891915142536163, Final Batch Loss: 0.30705609917640686\n",
      "Epoch 2354, Loss: 1.2398806810379028, Final Batch Loss: 0.25092247128486633\n",
      "Epoch 2355, Loss: 1.2355794310569763, Final Batch Loss: 0.3364846408367157\n",
      "Epoch 2356, Loss: 1.2662642002105713, Final Batch Loss: 0.3378394544124603\n",
      "Epoch 2357, Loss: 1.368222862482071, Final Batch Loss: 0.3781481683254242\n",
      "Epoch 2358, Loss: 1.259288340806961, Final Batch Loss: 0.3822189271450043\n",
      "Epoch 2359, Loss: 1.2648183405399323, Final Batch Loss: 0.27839821577072144\n",
      "Epoch 2360, Loss: 1.3537811934947968, Final Batch Loss: 0.25350481271743774\n",
      "Epoch 2361, Loss: 1.2290559262037277, Final Batch Loss: 0.22491516172885895\n",
      "Epoch 2362, Loss: 1.2427174150943756, Final Batch Loss: 0.3304842412471771\n",
      "Epoch 2363, Loss: 1.3178820610046387, Final Batch Loss: 0.3179321885108948\n",
      "Epoch 2364, Loss: 1.4294895231723785, Final Batch Loss: 0.38402876257896423\n",
      "Epoch 2365, Loss: 1.2396458685398102, Final Batch Loss: 0.3232330083847046\n",
      "Epoch 2366, Loss: 1.2500532269477844, Final Batch Loss: 0.3792822062969208\n",
      "Epoch 2367, Loss: 1.3660927414894104, Final Batch Loss: 0.38651442527770996\n",
      "Epoch 2368, Loss: 1.3350923955440521, Final Batch Loss: 0.30942538380622864\n",
      "Epoch 2369, Loss: 1.2365818917751312, Final Batch Loss: 0.3457861542701721\n",
      "Epoch 2370, Loss: 1.251538097858429, Final Batch Loss: 0.289402574300766\n",
      "Epoch 2371, Loss: 1.36456897854805, Final Batch Loss: 0.3088199496269226\n",
      "Epoch 2372, Loss: 1.2600446939468384, Final Batch Loss: 0.3623448610305786\n",
      "Epoch 2373, Loss: 1.345473736524582, Final Batch Loss: 0.4083837568759918\n",
      "Epoch 2374, Loss: 1.4928198456764221, Final Batch Loss: 0.4005282521247864\n",
      "Epoch 2375, Loss: 1.406834363937378, Final Batch Loss: 0.34826570749282837\n",
      "Epoch 2376, Loss: 1.243015080690384, Final Batch Loss: 0.30839601159095764\n",
      "Epoch 2377, Loss: 1.4612309634685516, Final Batch Loss: 0.5337644219398499\n",
      "Epoch 2378, Loss: 1.3582552373409271, Final Batch Loss: 0.38388481736183167\n",
      "Epoch 2379, Loss: 1.2795505821704865, Final Batch Loss: 0.3241345286369324\n",
      "Epoch 2380, Loss: 1.3233383297920227, Final Batch Loss: 0.3510587811470032\n",
      "Epoch 2381, Loss: 1.4290290176868439, Final Batch Loss: 0.32792046666145325\n",
      "Epoch 2382, Loss: 1.278409093618393, Final Batch Loss: 0.2268846035003662\n",
      "Epoch 2383, Loss: 1.3545671999454498, Final Batch Loss: 0.2971459925174713\n",
      "Epoch 2384, Loss: 1.3930108547210693, Final Batch Loss: 0.3381984829902649\n",
      "Epoch 2385, Loss: 1.457719624042511, Final Batch Loss: 0.47394871711730957\n",
      "Epoch 2386, Loss: 1.3680936992168427, Final Batch Loss: 0.37855565547943115\n",
      "Epoch 2387, Loss: 1.3144476860761642, Final Batch Loss: 0.3531515598297119\n",
      "Epoch 2388, Loss: 1.3178722858428955, Final Batch Loss: 0.3858094811439514\n",
      "Epoch 2389, Loss: 1.454691618680954, Final Batch Loss: 0.4060684144496918\n",
      "Epoch 2390, Loss: 1.2563566863536835, Final Batch Loss: 0.32446715235710144\n",
      "Epoch 2391, Loss: 1.2806267440319061, Final Batch Loss: 0.358955442905426\n",
      "Epoch 2392, Loss: 1.2126446962356567, Final Batch Loss: 0.26112204790115356\n",
      "Epoch 2393, Loss: 1.3497061431407928, Final Batch Loss: 0.3504391014575958\n",
      "Epoch 2394, Loss: 1.3395594656467438, Final Batch Loss: 0.3527660667896271\n",
      "Epoch 2395, Loss: 1.3194883465766907, Final Batch Loss: 0.3997138440608978\n",
      "Epoch 2396, Loss: 1.3442705869674683, Final Batch Loss: 0.35340049862861633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2397, Loss: 1.17901211977005, Final Batch Loss: 0.23683542013168335\n",
      "Epoch 2398, Loss: 1.3769407868385315, Final Batch Loss: 0.4443781077861786\n",
      "Epoch 2399, Loss: 1.3487355411052704, Final Batch Loss: 0.33092835545539856\n",
      "Epoch 2400, Loss: 1.2617487609386444, Final Batch Loss: 0.27920472621917725\n",
      "Epoch 2401, Loss: 1.405636042356491, Final Batch Loss: 0.38645249605178833\n",
      "Epoch 2402, Loss: 1.327864095568657, Final Batch Loss: 0.24864394962787628\n",
      "Epoch 2403, Loss: 1.2977289706468582, Final Batch Loss: 0.3169378638267517\n",
      "Epoch 2404, Loss: 1.340733140707016, Final Batch Loss: 0.4206954538822174\n",
      "Epoch 2405, Loss: 1.3100089132785797, Final Batch Loss: 0.2883771061897278\n",
      "Epoch 2406, Loss: 1.2916007041931152, Final Batch Loss: 0.2914365828037262\n",
      "Epoch 2407, Loss: 1.33182093501091, Final Batch Loss: 0.39748382568359375\n",
      "Epoch 2408, Loss: 1.3391945660114288, Final Batch Loss: 0.3134140372276306\n",
      "Epoch 2409, Loss: 1.3099167048931122, Final Batch Loss: 0.35979583859443665\n",
      "Epoch 2410, Loss: 1.206189602613449, Final Batch Loss: 0.3448006212711334\n",
      "Epoch 2411, Loss: 1.293037086725235, Final Batch Loss: 0.3690493404865265\n",
      "Epoch 2412, Loss: 1.3095114827156067, Final Batch Loss: 0.2689608335494995\n",
      "Epoch 2413, Loss: 1.3567674160003662, Final Batch Loss: 0.417955219745636\n",
      "Epoch 2414, Loss: 1.3083569705486298, Final Batch Loss: 0.2514785826206207\n",
      "Epoch 2415, Loss: 1.246353656053543, Final Batch Loss: 0.28983932733535767\n",
      "Epoch 2416, Loss: 1.2291228771209717, Final Batch Loss: 0.32372674345970154\n",
      "Epoch 2417, Loss: 1.3285745084285736, Final Batch Loss: 0.33275482058525085\n",
      "Epoch 2418, Loss: 1.3640168905258179, Final Batch Loss: 0.3556232750415802\n",
      "Epoch 2419, Loss: 1.372801512479782, Final Batch Loss: 0.37529879808425903\n",
      "Epoch 2420, Loss: 1.3109129965305328, Final Batch Loss: 0.3576066493988037\n",
      "Epoch 2421, Loss: 1.3888536095619202, Final Batch Loss: 0.3741398751735687\n",
      "Epoch 2422, Loss: 1.2673845738172531, Final Batch Loss: 0.37617599964141846\n",
      "Epoch 2423, Loss: 1.2601495832204819, Final Batch Loss: 0.2426401525735855\n",
      "Epoch 2424, Loss: 1.2532354295253754, Final Batch Loss: 0.2538512647151947\n",
      "Epoch 2425, Loss: 1.331330120563507, Final Batch Loss: 0.3373437225818634\n",
      "Epoch 2426, Loss: 1.2971450090408325, Final Batch Loss: 0.3472445011138916\n",
      "Epoch 2427, Loss: 1.150895208120346, Final Batch Loss: 0.2162564992904663\n",
      "Epoch 2428, Loss: 1.3729793429374695, Final Batch Loss: 0.4032922685146332\n",
      "Epoch 2429, Loss: 1.3008778095245361, Final Batch Loss: 0.31353598833084106\n",
      "Epoch 2430, Loss: 1.2901479601860046, Final Batch Loss: 0.3354831039905548\n",
      "Epoch 2431, Loss: 1.2559154331684113, Final Batch Loss: 0.2938360869884491\n",
      "Epoch 2432, Loss: 1.254959225654602, Final Batch Loss: 0.35355377197265625\n",
      "Epoch 2433, Loss: 1.35535529255867, Final Batch Loss: 0.41462066769599915\n",
      "Epoch 2434, Loss: 1.2001242339611053, Final Batch Loss: 0.30093663930892944\n",
      "Epoch 2435, Loss: 1.3118132203817368, Final Batch Loss: 0.39256715774536133\n",
      "Epoch 2436, Loss: 1.3205605745315552, Final Batch Loss: 0.30539798736572266\n",
      "Epoch 2437, Loss: 1.3755996525287628, Final Batch Loss: 0.38753753900527954\n",
      "Epoch 2438, Loss: 1.3164012134075165, Final Batch Loss: 0.3134564459323883\n",
      "Epoch 2439, Loss: 1.2902278304100037, Final Batch Loss: 0.36438512802124023\n",
      "Epoch 2440, Loss: 1.273650199174881, Final Batch Loss: 0.3857846260070801\n",
      "Epoch 2441, Loss: 1.3579541742801666, Final Batch Loss: 0.37910786271095276\n",
      "Epoch 2442, Loss: 1.3296009004116058, Final Batch Loss: 0.3484646677970886\n",
      "Epoch 2443, Loss: 1.2247069478034973, Final Batch Loss: 0.25041887164115906\n",
      "Epoch 2444, Loss: 1.1875933706760406, Final Batch Loss: 0.22337213158607483\n",
      "Epoch 2445, Loss: 1.24119433760643, Final Batch Loss: 0.3087838888168335\n",
      "Epoch 2446, Loss: 1.2103432714939117, Final Batch Loss: 0.31695204973220825\n",
      "Epoch 2447, Loss: 1.273796707391739, Final Batch Loss: 0.29469648003578186\n",
      "Epoch 2448, Loss: 1.2476017773151398, Final Batch Loss: 0.30457648634910583\n",
      "Epoch 2449, Loss: 1.2945099472999573, Final Batch Loss: 0.28844279050827026\n",
      "Epoch 2450, Loss: 1.268168032169342, Final Batch Loss: 0.24523931741714478\n",
      "Epoch 2451, Loss: 1.2529546320438385, Final Batch Loss: 0.31106775999069214\n",
      "Epoch 2452, Loss: 1.358491063117981, Final Batch Loss: 0.4031481146812439\n",
      "Epoch 2453, Loss: 1.3666393160820007, Final Batch Loss: 0.4160918891429901\n",
      "Epoch 2454, Loss: 1.2559938430786133, Final Batch Loss: 0.33145105838775635\n",
      "Epoch 2455, Loss: 1.195106714963913, Final Batch Loss: 0.2360568642616272\n",
      "Epoch 2456, Loss: 1.2830958366394043, Final Batch Loss: 0.27693265676498413\n",
      "Epoch 2457, Loss: 1.3193431198596954, Final Batch Loss: 0.3406357169151306\n",
      "Epoch 2458, Loss: 1.3449136018753052, Final Batch Loss: 0.27390846610069275\n",
      "Epoch 2459, Loss: 1.2374681532382965, Final Batch Loss: 0.2973409593105316\n",
      "Epoch 2460, Loss: 1.2426998615264893, Final Batch Loss: 0.36120378971099854\n",
      "Epoch 2461, Loss: 1.1794541627168655, Final Batch Loss: 0.21046052873134613\n",
      "Epoch 2462, Loss: 1.218794971704483, Final Batch Loss: 0.2608017325401306\n",
      "Epoch 2463, Loss: 1.253384679555893, Final Batch Loss: 0.30141422152519226\n",
      "Epoch 2464, Loss: 1.2005500495433807, Final Batch Loss: 0.26019206643104553\n",
      "Epoch 2465, Loss: 1.3011430501937866, Final Batch Loss: 0.31707829236984253\n",
      "Epoch 2466, Loss: 1.4497508704662323, Final Batch Loss: 0.4125511646270752\n",
      "Epoch 2467, Loss: 1.285885751247406, Final Batch Loss: 0.25850048661231995\n",
      "Epoch 2468, Loss: 1.3860285580158234, Final Batch Loss: 0.45541122555732727\n",
      "Epoch 2469, Loss: 1.2149473130702972, Final Batch Loss: 0.25323331356048584\n",
      "Epoch 2470, Loss: 1.149215579032898, Final Batch Loss: 0.23158815503120422\n",
      "Epoch 2471, Loss: 1.319193571805954, Final Batch Loss: 0.27885136008262634\n",
      "Epoch 2472, Loss: 1.2348522245883942, Final Batch Loss: 0.31041595339775085\n",
      "Epoch 2473, Loss: 1.2532122731208801, Final Batch Loss: 0.2506069242954254\n",
      "Epoch 2474, Loss: 1.1807503402233124, Final Batch Loss: 0.32644936442375183\n",
      "Epoch 2475, Loss: 1.2363335490226746, Final Batch Loss: 0.2906014323234558\n",
      "Epoch 2476, Loss: 1.3726949393749237, Final Batch Loss: 0.29425397515296936\n",
      "Epoch 2477, Loss: 1.24149888753891, Final Batch Loss: 0.3684546649456024\n",
      "Epoch 2478, Loss: 1.2230137586593628, Final Batch Loss: 0.30826544761657715\n",
      "Epoch 2479, Loss: 1.181910514831543, Final Batch Loss: 0.258719265460968\n",
      "Epoch 2480, Loss: 1.2608744204044342, Final Batch Loss: 0.29772239923477173\n",
      "Epoch 2481, Loss: 1.2586387991905212, Final Batch Loss: 0.28934621810913086\n",
      "Epoch 2482, Loss: 1.244037240743637, Final Batch Loss: 0.3686301112174988\n",
      "Epoch 2483, Loss: 1.2222121357917786, Final Batch Loss: 0.3607558608055115\n",
      "Epoch 2484, Loss: 1.2431693375110626, Final Batch Loss: 0.2789965569972992\n",
      "Epoch 2485, Loss: 1.230424851179123, Final Batch Loss: 0.18232938647270203\n",
      "Epoch 2486, Loss: 1.21124766767025, Final Batch Loss: 0.36981165409088135\n",
      "Epoch 2487, Loss: 1.3670195043087006, Final Batch Loss: 0.3892039954662323\n",
      "Epoch 2488, Loss: 1.2618016600608826, Final Batch Loss: 0.3859444558620453\n",
      "Epoch 2489, Loss: 1.1842193901538849, Final Batch Loss: 0.24060237407684326\n",
      "Epoch 2490, Loss: 1.372603952884674, Final Batch Loss: 0.5127314925193787\n",
      "Epoch 2491, Loss: 1.2490111291408539, Final Batch Loss: 0.32486194372177124\n",
      "Epoch 2492, Loss: 1.2523489892482758, Final Batch Loss: 0.31542256474494934\n",
      "Epoch 2493, Loss: 1.2138670086860657, Final Batch Loss: 0.3050398826599121\n",
      "Epoch 2494, Loss: 1.3411169946193695, Final Batch Loss: 0.36239686608314514\n",
      "Epoch 2495, Loss: 1.2627538740634918, Final Batch Loss: 0.3331764042377472\n",
      "Epoch 2496, Loss: 1.2395206987857819, Final Batch Loss: 0.30680710077285767\n",
      "Epoch 2497, Loss: 1.27445849776268, Final Batch Loss: 0.3247404992580414\n",
      "Epoch 2498, Loss: 1.3288947641849518, Final Batch Loss: 0.40299364924430847\n",
      "Epoch 2499, Loss: 1.3778077960014343, Final Batch Loss: 0.30294549465179443\n",
      "Epoch 2500, Loss: 1.1562542468309402, Final Batch Loss: 0.2155732363462448\n",
      "Epoch 2501, Loss: 1.3307308852672577, Final Batch Loss: 0.3315869867801666\n",
      "Epoch 2502, Loss: 1.1691411137580872, Final Batch Loss: 0.21380296349525452\n",
      "Epoch 2503, Loss: 1.3175187408924103, Final Batch Loss: 0.2988017499446869\n",
      "Epoch 2504, Loss: 1.3918371200561523, Final Batch Loss: 0.4779404103755951\n",
      "Epoch 2505, Loss: 1.3370513021945953, Final Batch Loss: 0.3432588577270508\n",
      "Epoch 2506, Loss: 1.166770488023758, Final Batch Loss: 0.2582016587257385\n",
      "Epoch 2507, Loss: 1.2433041334152222, Final Batch Loss: 0.3118176758289337\n",
      "Epoch 2508, Loss: 1.3153717815876007, Final Batch Loss: 0.38156306743621826\n",
      "Epoch 2509, Loss: 1.2006556689739227, Final Batch Loss: 0.2857493758201599\n",
      "Epoch 2510, Loss: 1.255800262093544, Final Batch Loss: 0.23622621595859528\n",
      "Epoch 2511, Loss: 1.249642789363861, Final Batch Loss: 0.3380415737628937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2512, Loss: 1.2791133522987366, Final Batch Loss: 0.32361266016960144\n",
      "Epoch 2513, Loss: 1.200146496295929, Final Batch Loss: 0.3915039896965027\n",
      "Epoch 2514, Loss: 1.4257004261016846, Final Batch Loss: 0.4261816740036011\n",
      "Epoch 2515, Loss: 1.2406188249588013, Final Batch Loss: 0.286033570766449\n",
      "Epoch 2516, Loss: 1.3516585528850555, Final Batch Loss: 0.3680190145969391\n",
      "Epoch 2517, Loss: 1.2547050416469574, Final Batch Loss: 0.2727849781513214\n",
      "Epoch 2518, Loss: 1.2653490900993347, Final Batch Loss: 0.325806200504303\n",
      "Epoch 2519, Loss: 1.2524360716342926, Final Batch Loss: 0.28793278336524963\n",
      "Epoch 2520, Loss: 1.2535009533166885, Final Batch Loss: 0.3311856985092163\n",
      "Epoch 2521, Loss: 1.3561988770961761, Final Batch Loss: 0.47344109416007996\n",
      "Epoch 2522, Loss: 1.2776492238044739, Final Batch Loss: 0.3461191654205322\n",
      "Epoch 2523, Loss: 1.3278622925281525, Final Batch Loss: 0.33392974734306335\n",
      "Epoch 2524, Loss: 1.300748348236084, Final Batch Loss: 0.303445428609848\n",
      "Epoch 2525, Loss: 1.2212538719177246, Final Batch Loss: 0.27677199244499207\n",
      "Epoch 2526, Loss: 1.2669906318187714, Final Batch Loss: 0.34062206745147705\n",
      "Epoch 2527, Loss: 1.1564391553401947, Final Batch Loss: 0.30206799507141113\n",
      "Epoch 2528, Loss: 1.2966462969779968, Final Batch Loss: 0.35325440764427185\n",
      "Epoch 2529, Loss: 1.2244153916835785, Final Batch Loss: 0.29462817311286926\n",
      "Epoch 2530, Loss: 1.3088228404521942, Final Batch Loss: 0.3746209442615509\n",
      "Epoch 2531, Loss: 1.1659307330846786, Final Batch Loss: 0.22398187220096588\n",
      "Epoch 2532, Loss: 1.174283891916275, Final Batch Loss: 0.2502043545246124\n",
      "Epoch 2533, Loss: 1.2014570236206055, Final Batch Loss: 0.325258731842041\n",
      "Epoch 2534, Loss: 1.2202261686325073, Final Batch Loss: 0.28269997239112854\n",
      "Epoch 2535, Loss: 1.2250043749809265, Final Batch Loss: 0.3114507794380188\n",
      "Epoch 2536, Loss: 1.1880899965763092, Final Batch Loss: 0.3282979428768158\n",
      "Epoch 2537, Loss: 1.338764488697052, Final Batch Loss: 0.2520866394042969\n",
      "Epoch 2538, Loss: 1.3073031902313232, Final Batch Loss: 0.3513106107711792\n",
      "Epoch 2539, Loss: 1.3040638864040375, Final Batch Loss: 0.3907797932624817\n",
      "Epoch 2540, Loss: 1.203500211238861, Final Batch Loss: 0.3542099893093109\n",
      "Epoch 2541, Loss: 1.2426345944404602, Final Batch Loss: 0.2948409914970398\n",
      "Epoch 2542, Loss: 1.2420872300863266, Final Batch Loss: 0.3584311902523041\n",
      "Epoch 2543, Loss: 1.3301410377025604, Final Batch Loss: 0.3345649242401123\n",
      "Epoch 2544, Loss: 1.3054887056350708, Final Batch Loss: 0.29204756021499634\n",
      "Epoch 2545, Loss: 1.389854222536087, Final Batch Loss: 0.3767146170139313\n",
      "Epoch 2546, Loss: 1.2852486670017242, Final Batch Loss: 0.40313780307769775\n",
      "Epoch 2547, Loss: 1.2837540209293365, Final Batch Loss: 0.34825441241264343\n",
      "Epoch 2548, Loss: 1.1028514057397842, Final Batch Loss: 0.2827068269252777\n",
      "Epoch 2549, Loss: 1.2756635248661041, Final Batch Loss: 0.2960606813430786\n",
      "Epoch 2550, Loss: 1.295712023973465, Final Batch Loss: 0.3019971549510956\n",
      "Epoch 2551, Loss: 1.2551997601985931, Final Batch Loss: 0.3222888112068176\n",
      "Epoch 2552, Loss: 1.337408035993576, Final Batch Loss: 0.33386072516441345\n",
      "Epoch 2553, Loss: 1.3441146314144135, Final Batch Loss: 0.2880554497241974\n",
      "Epoch 2554, Loss: 1.3373639583587646, Final Batch Loss: 0.30367353558540344\n",
      "Epoch 2555, Loss: 1.2578846216201782, Final Batch Loss: 0.34649139642715454\n",
      "Epoch 2556, Loss: 1.271209180355072, Final Batch Loss: 0.2783922851085663\n",
      "Epoch 2557, Loss: 1.237562209367752, Final Batch Loss: 0.29041874408721924\n",
      "Epoch 2558, Loss: 1.2409162521362305, Final Batch Loss: 0.26329848170280457\n",
      "Epoch 2559, Loss: 1.2923379838466644, Final Batch Loss: 0.35286805033683777\n",
      "Epoch 2560, Loss: 1.3818690478801727, Final Batch Loss: 0.30788880586624146\n",
      "Epoch 2561, Loss: 1.3140794336795807, Final Batch Loss: 0.3313688635826111\n",
      "Epoch 2562, Loss: 1.3752821385860443, Final Batch Loss: 0.35779616236686707\n",
      "Epoch 2563, Loss: 1.1390322148799896, Final Batch Loss: 0.22351112961769104\n",
      "Epoch 2564, Loss: 1.1803574413061142, Final Batch Loss: 0.3296378254890442\n",
      "Epoch 2565, Loss: 1.3426261842250824, Final Batch Loss: 0.33094096183776855\n",
      "Epoch 2566, Loss: 1.1970546543598175, Final Batch Loss: 0.33587250113487244\n",
      "Epoch 2567, Loss: 1.1982535421848297, Final Batch Loss: 0.28952306509017944\n",
      "Epoch 2568, Loss: 1.1513604819774628, Final Batch Loss: 0.2952859401702881\n",
      "Epoch 2569, Loss: 1.2985046207904816, Final Batch Loss: 0.4173893928527832\n",
      "Epoch 2570, Loss: 1.2408074140548706, Final Batch Loss: 0.24810579419136047\n",
      "Epoch 2571, Loss: 1.278535932302475, Final Batch Loss: 0.29698920249938965\n",
      "Epoch 2572, Loss: 1.2427505850791931, Final Batch Loss: 0.36534959077835083\n",
      "Epoch 2573, Loss: 1.2570382505655289, Final Batch Loss: 0.330333948135376\n",
      "Epoch 2574, Loss: 1.261962741613388, Final Batch Loss: 0.3099146783351898\n",
      "Epoch 2575, Loss: 1.309652715921402, Final Batch Loss: 0.4005905091762543\n",
      "Epoch 2576, Loss: 1.2134414613246918, Final Batch Loss: 0.28903013467788696\n",
      "Epoch 2577, Loss: 1.2243602871894836, Final Batch Loss: 0.3237231969833374\n",
      "Epoch 2578, Loss: 1.1433914303779602, Final Batch Loss: 0.28067076206207275\n",
      "Epoch 2579, Loss: 1.3293277025222778, Final Batch Loss: 0.38117244839668274\n",
      "Epoch 2580, Loss: 1.287401407957077, Final Batch Loss: 0.34618765115737915\n",
      "Epoch 2581, Loss: 1.2623910158872604, Final Batch Loss: 0.30183812975883484\n",
      "Epoch 2582, Loss: 1.2748387455940247, Final Batch Loss: 0.3552466332912445\n",
      "Epoch 2583, Loss: 1.2418825328350067, Final Batch Loss: 0.335856169462204\n",
      "Epoch 2584, Loss: 1.1597032845020294, Final Batch Loss: 0.2559126913547516\n",
      "Epoch 2585, Loss: 1.2318867146968842, Final Batch Loss: 0.31726938486099243\n",
      "Epoch 2586, Loss: 1.133120357990265, Final Batch Loss: 0.24957072734832764\n",
      "Epoch 2587, Loss: 1.262556254863739, Final Batch Loss: 0.28705164790153503\n",
      "Epoch 2588, Loss: 1.3110870718955994, Final Batch Loss: 0.4347195625305176\n",
      "Epoch 2589, Loss: 1.2724387645721436, Final Batch Loss: 0.3297218084335327\n",
      "Epoch 2590, Loss: 1.2461442351341248, Final Batch Loss: 0.2704107165336609\n",
      "Epoch 2591, Loss: 1.2196548730134964, Final Batch Loss: 0.3431708812713623\n",
      "Epoch 2592, Loss: 1.280151754617691, Final Batch Loss: 0.33990806341171265\n",
      "Epoch 2593, Loss: 1.365323394536972, Final Batch Loss: 0.3940628170967102\n",
      "Epoch 2594, Loss: 1.263106346130371, Final Batch Loss: 0.24717888236045837\n",
      "Epoch 2595, Loss: 1.1880821883678436, Final Batch Loss: 0.2646177411079407\n",
      "Epoch 2596, Loss: 1.248079627752304, Final Batch Loss: 0.35108229517936707\n",
      "Epoch 2597, Loss: 1.1723416149616241, Final Batch Loss: 0.29510968923568726\n",
      "Epoch 2598, Loss: 1.2420330047607422, Final Batch Loss: 0.3764028251171112\n",
      "Epoch 2599, Loss: 1.182648167014122, Final Batch Loss: 0.23146279156208038\n",
      "Epoch 2600, Loss: 1.1899191737174988, Final Batch Loss: 0.21887239813804626\n",
      "Epoch 2601, Loss: 1.3975104689598083, Final Batch Loss: 0.2819831073284149\n",
      "Epoch 2602, Loss: 1.1877196431159973, Final Batch Loss: 0.2842358648777008\n",
      "Epoch 2603, Loss: 1.2583043575286865, Final Batch Loss: 0.38152197003364563\n",
      "Epoch 2604, Loss: 1.2381754517555237, Final Batch Loss: 0.31881216168403625\n",
      "Epoch 2605, Loss: 1.2006380558013916, Final Batch Loss: 0.2341325581073761\n",
      "Epoch 2606, Loss: 1.357898473739624, Final Batch Loss: 0.4067920744419098\n",
      "Epoch 2607, Loss: 1.1877208054065704, Final Batch Loss: 0.3138750493526459\n",
      "Epoch 2608, Loss: 1.3069918751716614, Final Batch Loss: 0.3997671604156494\n",
      "Epoch 2609, Loss: 1.2056439816951752, Final Batch Loss: 0.3163394629955292\n",
      "Epoch 2610, Loss: 1.2866417169570923, Final Batch Loss: 0.3638755679130554\n",
      "Epoch 2611, Loss: 1.188757836818695, Final Batch Loss: 0.21089580655097961\n",
      "Epoch 2612, Loss: 1.3446182608604431, Final Batch Loss: 0.38284534215927124\n",
      "Epoch 2613, Loss: 1.2781557738780975, Final Batch Loss: 0.3560478091239929\n",
      "Epoch 2614, Loss: 1.181129053235054, Final Batch Loss: 0.23921938240528107\n",
      "Epoch 2615, Loss: 1.1220708191394806, Final Batch Loss: 0.22553542256355286\n",
      "Epoch 2616, Loss: 1.205970674753189, Final Batch Loss: 0.2616115212440491\n",
      "Epoch 2617, Loss: 1.337901085615158, Final Batch Loss: 0.3890978991985321\n",
      "Epoch 2618, Loss: 1.3894642889499664, Final Batch Loss: 0.41640764474868774\n",
      "Epoch 2619, Loss: 1.3776862025260925, Final Batch Loss: 0.31754016876220703\n",
      "Epoch 2620, Loss: 1.163629725575447, Final Batch Loss: 0.27168145775794983\n",
      "Epoch 2621, Loss: 1.1997959315776825, Final Batch Loss: 0.3191104829311371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2622, Loss: 1.2500490844249725, Final Batch Loss: 0.3025277554988861\n",
      "Epoch 2623, Loss: 1.258448749780655, Final Batch Loss: 0.3291873335838318\n",
      "Epoch 2624, Loss: 1.2389340996742249, Final Batch Loss: 0.33413368463516235\n",
      "Epoch 2625, Loss: 1.2051866352558136, Final Batch Loss: 0.33676478266716003\n",
      "Epoch 2626, Loss: 1.3113904893398285, Final Batch Loss: 0.32192909717559814\n",
      "Epoch 2627, Loss: 1.399014949798584, Final Batch Loss: 0.347472608089447\n",
      "Epoch 2628, Loss: 1.2158426940441132, Final Batch Loss: 0.2979111671447754\n",
      "Epoch 2629, Loss: 1.2889890372753143, Final Batch Loss: 0.3076024055480957\n",
      "Epoch 2630, Loss: 1.3044954240322113, Final Batch Loss: 0.40994492173194885\n",
      "Epoch 2631, Loss: 1.2771448493003845, Final Batch Loss: 0.4103316068649292\n",
      "Epoch 2632, Loss: 1.1880443096160889, Final Batch Loss: 0.27504175901412964\n",
      "Epoch 2633, Loss: 1.1583261489868164, Final Batch Loss: 0.27949127554893494\n",
      "Epoch 2634, Loss: 1.2704983353614807, Final Batch Loss: 0.37906938791275024\n",
      "Epoch 2635, Loss: 1.1592638492584229, Final Batch Loss: 0.26889050006866455\n",
      "Epoch 2636, Loss: 1.1888081282377243, Final Batch Loss: 0.22657553851604462\n",
      "Epoch 2637, Loss: 1.152859315276146, Final Batch Loss: 0.3121147155761719\n",
      "Epoch 2638, Loss: 1.2153131067752838, Final Batch Loss: 0.29638051986694336\n",
      "Epoch 2639, Loss: 1.1838178038597107, Final Batch Loss: 0.29240021109580994\n",
      "Epoch 2640, Loss: 1.2541235983371735, Final Batch Loss: 0.3179027736186981\n",
      "Epoch 2641, Loss: 1.1151855289936066, Final Batch Loss: 0.21055611968040466\n",
      "Epoch 2642, Loss: 1.2001593112945557, Final Batch Loss: 0.3421204388141632\n",
      "Epoch 2643, Loss: 1.2172470688819885, Final Batch Loss: 0.2873353362083435\n",
      "Epoch 2644, Loss: 1.2688227593898773, Final Batch Loss: 0.26307693123817444\n",
      "Epoch 2645, Loss: 1.1846614331007004, Final Batch Loss: 0.2379937320947647\n",
      "Epoch 2646, Loss: 1.1261614859104156, Final Batch Loss: 0.22866633534431458\n",
      "Epoch 2647, Loss: 1.2476288080215454, Final Batch Loss: 0.2522263526916504\n",
      "Epoch 2648, Loss: 1.2379651069641113, Final Batch Loss: 0.30940961837768555\n",
      "Epoch 2649, Loss: 1.1578716337680817, Final Batch Loss: 0.2511381208896637\n",
      "Epoch 2650, Loss: 1.185095727443695, Final Batch Loss: 0.26525214314460754\n",
      "Epoch 2651, Loss: 1.2639809250831604, Final Batch Loss: 0.2697679400444031\n",
      "Epoch 2652, Loss: 1.2022331655025482, Final Batch Loss: 0.3341003358364105\n",
      "Epoch 2653, Loss: 1.2465184926986694, Final Batch Loss: 0.3089262843132019\n",
      "Epoch 2654, Loss: 1.294218897819519, Final Batch Loss: 0.3888132572174072\n",
      "Epoch 2655, Loss: 1.264569878578186, Final Batch Loss: 0.3138797879219055\n",
      "Epoch 2656, Loss: 1.1204532384872437, Final Batch Loss: 0.3141009509563446\n",
      "Epoch 2657, Loss: 1.3056706190109253, Final Batch Loss: 0.3461969792842865\n",
      "Epoch 2658, Loss: 1.322892040014267, Final Batch Loss: 0.39463162422180176\n",
      "Epoch 2659, Loss: 1.3565270006656647, Final Batch Loss: 0.327951043844223\n",
      "Epoch 2660, Loss: 1.139272779226303, Final Batch Loss: 0.20485913753509521\n",
      "Epoch 2661, Loss: 1.213826298713684, Final Batch Loss: 0.29092544317245483\n",
      "Epoch 2662, Loss: 1.1662738025188446, Final Batch Loss: 0.27501899003982544\n",
      "Epoch 2663, Loss: 1.2766664326190948, Final Batch Loss: 0.2992608845233917\n",
      "Epoch 2664, Loss: 1.3006759881973267, Final Batch Loss: 0.4027646481990814\n",
      "Epoch 2665, Loss: 1.1338696479797363, Final Batch Loss: 0.19602173566818237\n",
      "Epoch 2666, Loss: 1.270215392112732, Final Batch Loss: 0.3472172021865845\n",
      "Epoch 2667, Loss: 1.160353735089302, Final Batch Loss: 0.27117446064949036\n",
      "Epoch 2668, Loss: 1.28097802400589, Final Batch Loss: 0.336942583322525\n",
      "Epoch 2669, Loss: 1.2982042729854584, Final Batch Loss: 0.3487911522388458\n",
      "Epoch 2670, Loss: 1.2673801481723785, Final Batch Loss: 0.3435278534889221\n",
      "Epoch 2671, Loss: 1.4004132151603699, Final Batch Loss: 0.45602771639823914\n",
      "Epoch 2672, Loss: 1.2225967049598694, Final Batch Loss: 0.24459514021873474\n",
      "Epoch 2673, Loss: 1.1707054376602173, Final Batch Loss: 0.2628154456615448\n",
      "Epoch 2674, Loss: 1.2358354926109314, Final Batch Loss: 0.33059442043304443\n",
      "Epoch 2675, Loss: 1.322759062051773, Final Batch Loss: 0.36987999081611633\n",
      "Epoch 2676, Loss: 1.0800874382257462, Final Batch Loss: 0.23337723314762115\n",
      "Epoch 2677, Loss: 1.2704463005065918, Final Batch Loss: 0.28817006945610046\n",
      "Epoch 2678, Loss: 1.2345967292785645, Final Batch Loss: 0.34242597222328186\n",
      "Epoch 2679, Loss: 1.236305981874466, Final Batch Loss: 0.2872748076915741\n",
      "Epoch 2680, Loss: 1.2222005426883698, Final Batch Loss: 0.2877129912376404\n",
      "Epoch 2681, Loss: 1.2354822009801865, Final Batch Loss: 0.41577303409576416\n",
      "Epoch 2682, Loss: 1.2011501789093018, Final Batch Loss: 0.28316912055015564\n",
      "Epoch 2683, Loss: 1.2163646221160889, Final Batch Loss: 0.32821041345596313\n",
      "Epoch 2684, Loss: 1.1807402670383453, Final Batch Loss: 0.24779486656188965\n",
      "Epoch 2685, Loss: 1.0958507359027863, Final Batch Loss: 0.16832590103149414\n",
      "Epoch 2686, Loss: 1.210860252380371, Final Batch Loss: 0.2810978889465332\n",
      "Epoch 2687, Loss: 1.3748508393764496, Final Batch Loss: 0.4054877758026123\n",
      "Epoch 2688, Loss: 1.1773160845041275, Final Batch Loss: 0.22974763810634613\n",
      "Epoch 2689, Loss: 1.4074246883392334, Final Batch Loss: 0.4308493733406067\n",
      "Epoch 2690, Loss: 1.1969867199659348, Final Batch Loss: 0.36549487709999084\n",
      "Epoch 2691, Loss: 1.1877732872962952, Final Batch Loss: 0.34167441725730896\n",
      "Epoch 2692, Loss: 1.1746938079595566, Final Batch Loss: 0.354055255651474\n",
      "Epoch 2693, Loss: 1.2393529415130615, Final Batch Loss: 0.39827701449394226\n",
      "Epoch 2694, Loss: 1.30097034573555, Final Batch Loss: 0.34755030274391174\n",
      "Epoch 2695, Loss: 1.2329124510288239, Final Batch Loss: 0.2863505482673645\n",
      "Epoch 2696, Loss: 1.267758458852768, Final Batch Loss: 0.40639007091522217\n",
      "Epoch 2697, Loss: 1.2545217424631119, Final Batch Loss: 0.18252764642238617\n",
      "Epoch 2698, Loss: 1.3250789642333984, Final Batch Loss: 0.4267452359199524\n",
      "Epoch 2699, Loss: 1.149593025445938, Final Batch Loss: 0.2769567668437958\n",
      "Epoch 2700, Loss: 1.2129668593406677, Final Batch Loss: 0.25352415442466736\n",
      "Epoch 2701, Loss: 1.150054156780243, Final Batch Loss: 0.2561189830303192\n",
      "Epoch 2702, Loss: 1.1806099712848663, Final Batch Loss: 0.31799376010894775\n",
      "Epoch 2703, Loss: 1.2350491881370544, Final Batch Loss: 0.30972322821617126\n",
      "Epoch 2704, Loss: 1.2644048631191254, Final Batch Loss: 0.3257642388343811\n",
      "Epoch 2705, Loss: 1.1735958904027939, Final Batch Loss: 0.2595541775226593\n",
      "Epoch 2706, Loss: 1.176215410232544, Final Batch Loss: 0.2723749876022339\n",
      "Epoch 2707, Loss: 1.1937397867441177, Final Batch Loss: 0.31782227754592896\n",
      "Epoch 2708, Loss: 1.1554895639419556, Final Batch Loss: 0.1988508403301239\n",
      "Epoch 2709, Loss: 1.1706967651844025, Final Batch Loss: 0.2299363911151886\n",
      "Epoch 2710, Loss: 1.263148009777069, Final Batch Loss: 0.38626235723495483\n",
      "Epoch 2711, Loss: 1.2470116913318634, Final Batch Loss: 0.2844237983226776\n",
      "Epoch 2712, Loss: 1.2403477877378464, Final Batch Loss: 0.41960543394088745\n",
      "Epoch 2713, Loss: 1.2093045711517334, Final Batch Loss: 0.2992713451385498\n",
      "Epoch 2714, Loss: 1.1957665979862213, Final Batch Loss: 0.36511436104774475\n",
      "Epoch 2715, Loss: 1.2535803616046906, Final Batch Loss: 0.3147677183151245\n",
      "Epoch 2716, Loss: 1.142300546169281, Final Batch Loss: 0.1740625500679016\n",
      "Epoch 2717, Loss: 1.1542796045541763, Final Batch Loss: 0.2150225192308426\n",
      "Epoch 2718, Loss: 1.1641488671302795, Final Batch Loss: 0.2975219488143921\n",
      "Epoch 2719, Loss: 1.3011838495731354, Final Batch Loss: 0.25297361612319946\n",
      "Epoch 2720, Loss: 1.1644272208213806, Final Batch Loss: 0.21735525131225586\n",
      "Epoch 2721, Loss: 1.1448110938072205, Final Batch Loss: 0.1990344524383545\n",
      "Epoch 2722, Loss: 1.2975663840770721, Final Batch Loss: 0.36285725235939026\n",
      "Epoch 2723, Loss: 1.119963675737381, Final Batch Loss: 0.26115882396698\n",
      "Epoch 2724, Loss: 1.0826156735420227, Final Batch Loss: 0.28112930059432983\n",
      "Epoch 2725, Loss: 1.2205590307712555, Final Batch Loss: 0.3223540484905243\n",
      "Epoch 2726, Loss: 1.1715938746929169, Final Batch Loss: 0.271451473236084\n",
      "Epoch 2727, Loss: 1.219573587179184, Final Batch Loss: 0.34755828976631165\n",
      "Epoch 2728, Loss: 1.1536891609430313, Final Batch Loss: 0.2428436130285263\n",
      "Epoch 2729, Loss: 1.3300705552101135, Final Batch Loss: 0.39683544635772705\n",
      "Epoch 2730, Loss: 1.1955628991127014, Final Batch Loss: 0.2978346049785614\n",
      "Epoch 2731, Loss: 1.2132020890712738, Final Batch Loss: 0.25027263164520264\n",
      "Epoch 2732, Loss: 1.2559609115123749, Final Batch Loss: 0.3237980306148529\n",
      "Epoch 2733, Loss: 1.2592809796333313, Final Batch Loss: 0.33183252811431885\n",
      "Epoch 2734, Loss: 1.2992177307605743, Final Batch Loss: 0.3121907711029053\n",
      "Epoch 2735, Loss: 1.1364852786064148, Final Batch Loss: 0.26001864671707153\n",
      "Epoch 2736, Loss: 1.2942966222763062, Final Batch Loss: 0.3183531165122986\n",
      "Epoch 2737, Loss: 1.2082103192806244, Final Batch Loss: 0.31440573930740356\n",
      "Epoch 2738, Loss: 1.242596685886383, Final Batch Loss: 0.2911131978034973\n",
      "Epoch 2739, Loss: 1.1752767264842987, Final Batch Loss: 0.2794417142868042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2740, Loss: 1.1639827191829681, Final Batch Loss: 0.2783818244934082\n",
      "Epoch 2741, Loss: 1.225764274597168, Final Batch Loss: 0.2791491448879242\n",
      "Epoch 2742, Loss: 1.2483030557632446, Final Batch Loss: 0.28002747893333435\n",
      "Epoch 2743, Loss: 1.2370913624763489, Final Batch Loss: 0.27821552753448486\n",
      "Epoch 2744, Loss: 1.2897413671016693, Final Batch Loss: 0.3234330415725708\n",
      "Epoch 2745, Loss: 1.202121376991272, Final Batch Loss: 0.2957302927970886\n",
      "Epoch 2746, Loss: 1.1671482920646667, Final Batch Loss: 0.27001217007637024\n",
      "Epoch 2747, Loss: 1.1887769997119904, Final Batch Loss: 0.2542312443256378\n",
      "Epoch 2748, Loss: 1.204237014055252, Final Batch Loss: 0.2764916718006134\n",
      "Epoch 2749, Loss: 1.1668061017990112, Final Batch Loss: 0.28626999258995056\n",
      "Epoch 2750, Loss: 1.147008255124092, Final Batch Loss: 0.20656441152095795\n",
      "Epoch 2751, Loss: 1.1713862121105194, Final Batch Loss: 0.3043919801712036\n",
      "Epoch 2752, Loss: 1.2162707298994064, Final Batch Loss: 0.2851884067058563\n",
      "Epoch 2753, Loss: 1.2278598248958588, Final Batch Loss: 0.31275781989097595\n",
      "Epoch 2754, Loss: 1.1881348192691803, Final Batch Loss: 0.25831401348114014\n",
      "Epoch 2755, Loss: 1.1408577859401703, Final Batch Loss: 0.2758302688598633\n",
      "Epoch 2756, Loss: 1.30708447098732, Final Batch Loss: 0.3010125160217285\n",
      "Epoch 2757, Loss: 1.2123837172985077, Final Batch Loss: 0.28171613812446594\n",
      "Epoch 2758, Loss: 1.1925319135189056, Final Batch Loss: 0.3035016357898712\n",
      "Epoch 2759, Loss: 1.1475868970155716, Final Batch Loss: 0.2979361414909363\n",
      "Epoch 2760, Loss: 1.1950794160366058, Final Batch Loss: 0.28296592831611633\n",
      "Epoch 2761, Loss: 1.2831835448741913, Final Batch Loss: 0.36811500787734985\n",
      "Epoch 2762, Loss: 1.169179618358612, Final Batch Loss: 0.2726910412311554\n",
      "Epoch 2763, Loss: 1.2896964848041534, Final Batch Loss: 0.3092353641986847\n",
      "Epoch 2764, Loss: 1.2778667360544205, Final Batch Loss: 0.43829935789108276\n",
      "Epoch 2765, Loss: 1.143518552184105, Final Batch Loss: 0.23558790981769562\n",
      "Epoch 2766, Loss: 1.2247602939605713, Final Batch Loss: 0.3237486183643341\n",
      "Epoch 2767, Loss: 1.1502391993999481, Final Batch Loss: 0.2763417363166809\n",
      "Epoch 2768, Loss: 1.269590586423874, Final Batch Loss: 0.31739330291748047\n",
      "Epoch 2769, Loss: 1.1334706842899323, Final Batch Loss: 0.3193802535533905\n",
      "Epoch 2770, Loss: 1.2243543714284897, Final Batch Loss: 0.34894412755966187\n",
      "Epoch 2771, Loss: 1.2276235520839691, Final Batch Loss: 0.31156593561172485\n",
      "Epoch 2772, Loss: 1.2277680933475494, Final Batch Loss: 0.27987906336784363\n",
      "Epoch 2773, Loss: 1.1525124907493591, Final Batch Loss: 0.2255612313747406\n",
      "Epoch 2774, Loss: 1.1572359949350357, Final Batch Loss: 0.3082661032676697\n",
      "Epoch 2775, Loss: 1.1852059662342072, Final Batch Loss: 0.2620231509208679\n",
      "Epoch 2776, Loss: 1.2735390365123749, Final Batch Loss: 0.32248654961586\n",
      "Epoch 2777, Loss: 1.2684006690979004, Final Batch Loss: 0.31810083985328674\n",
      "Epoch 2778, Loss: 1.213793009519577, Final Batch Loss: 0.34415173530578613\n",
      "Epoch 2779, Loss: 1.2139213383197784, Final Batch Loss: 0.34982216358184814\n",
      "Epoch 2780, Loss: 1.3077464997768402, Final Batch Loss: 0.37870702147483826\n",
      "Epoch 2781, Loss: 1.2639166116714478, Final Batch Loss: 0.3780587911605835\n",
      "Epoch 2782, Loss: 1.2758751809597015, Final Batch Loss: 0.3831048011779785\n",
      "Epoch 2783, Loss: 1.2658185511827469, Final Batch Loss: 0.35856378078460693\n",
      "Epoch 2784, Loss: 1.1616338193416595, Final Batch Loss: 0.21302416920661926\n",
      "Epoch 2785, Loss: 1.1451211273670197, Final Batch Loss: 0.2906266152858734\n",
      "Epoch 2786, Loss: 1.1342101693153381, Final Batch Loss: 0.2209441065788269\n",
      "Epoch 2787, Loss: 1.2692258059978485, Final Batch Loss: 0.3594104051589966\n",
      "Epoch 2788, Loss: 1.049433320760727, Final Batch Loss: 0.18595215678215027\n",
      "Epoch 2789, Loss: 1.171046406030655, Final Batch Loss: 0.2811077833175659\n",
      "Epoch 2790, Loss: 1.2193914949893951, Final Batch Loss: 0.2733025848865509\n",
      "Epoch 2791, Loss: 1.1633973717689514, Final Batch Loss: 0.30067968368530273\n",
      "Epoch 2792, Loss: 1.3077006042003632, Final Batch Loss: 0.2989286482334137\n",
      "Epoch 2793, Loss: 1.2652305662631989, Final Batch Loss: 0.3473842740058899\n",
      "Epoch 2794, Loss: 1.101759910583496, Final Batch Loss: 0.32397371530532837\n",
      "Epoch 2795, Loss: 1.2578983008861542, Final Batch Loss: 0.3342388868331909\n",
      "Epoch 2796, Loss: 1.1301014125347137, Final Batch Loss: 0.2949265241622925\n",
      "Epoch 2797, Loss: 1.157947912812233, Final Batch Loss: 0.3205035924911499\n",
      "Epoch 2798, Loss: 1.232782393693924, Final Batch Loss: 0.28108319640159607\n",
      "Epoch 2799, Loss: 1.1555613279342651, Final Batch Loss: 0.31599119305610657\n",
      "Epoch 2800, Loss: 1.2853588461875916, Final Batch Loss: 0.31737828254699707\n",
      "Epoch 2801, Loss: 1.107027679681778, Final Batch Loss: 0.2151128053665161\n",
      "Epoch 2802, Loss: 1.1673919260501862, Final Batch Loss: 0.22893419861793518\n",
      "Epoch 2803, Loss: 1.1843841224908829, Final Batch Loss: 0.22953183948993683\n",
      "Epoch 2804, Loss: 1.1129252910614014, Final Batch Loss: 0.1948399841785431\n",
      "Epoch 2805, Loss: 1.180178925395012, Final Batch Loss: 0.35148128867149353\n",
      "Epoch 2806, Loss: 1.0991966724395752, Final Batch Loss: 0.30988726019859314\n",
      "Epoch 2807, Loss: 1.150129348039627, Final Batch Loss: 0.2869303524494171\n",
      "Epoch 2808, Loss: 1.2295996248722076, Final Batch Loss: 0.37201088666915894\n",
      "Epoch 2809, Loss: 1.2129449248313904, Final Batch Loss: 0.28088095784187317\n",
      "Epoch 2810, Loss: 1.1662241220474243, Final Batch Loss: 0.31169193983078003\n",
      "Epoch 2811, Loss: 1.4103285670280457, Final Batch Loss: 0.3994162976741791\n",
      "Epoch 2812, Loss: 1.232919454574585, Final Batch Loss: 0.2767401337623596\n",
      "Epoch 2813, Loss: 1.1599893420934677, Final Batch Loss: 0.30865904688835144\n",
      "Epoch 2814, Loss: 1.1781458854675293, Final Batch Loss: 0.3051849603652954\n",
      "Epoch 2815, Loss: 1.197943925857544, Final Batch Loss: 0.3590187132358551\n",
      "Epoch 2816, Loss: 1.1478128135204315, Final Batch Loss: 0.2537049949169159\n",
      "Epoch 2817, Loss: 1.2386128604412079, Final Batch Loss: 0.31983381509780884\n",
      "Epoch 2818, Loss: 1.2177650034427643, Final Batch Loss: 0.2302798628807068\n",
      "Epoch 2819, Loss: 1.1266712844371796, Final Batch Loss: 0.3503354489803314\n",
      "Epoch 2820, Loss: 1.125327229499817, Final Batch Loss: 0.20856830477714539\n",
      "Epoch 2821, Loss: 1.2087195813655853, Final Batch Loss: 0.31842494010925293\n",
      "Epoch 2822, Loss: 1.1822051256895065, Final Batch Loss: 0.36386480927467346\n",
      "Epoch 2823, Loss: 1.203093707561493, Final Batch Loss: 0.288767546415329\n",
      "Epoch 2824, Loss: 1.142966389656067, Final Batch Loss: 0.30784428119659424\n",
      "Epoch 2825, Loss: 1.0941786468029022, Final Batch Loss: 0.25623899698257446\n",
      "Epoch 2826, Loss: 1.1930757462978363, Final Batch Loss: 0.29278841614723206\n",
      "Epoch 2827, Loss: 1.14644455909729, Final Batch Loss: 0.260992169380188\n",
      "Epoch 2828, Loss: 1.2677256762981415, Final Batch Loss: 0.32056960463523865\n",
      "Epoch 2829, Loss: 1.2424043416976929, Final Batch Loss: 0.3437354266643524\n",
      "Epoch 2830, Loss: 1.2433876991271973, Final Batch Loss: 0.37760797142982483\n",
      "Epoch 2831, Loss: 1.2683561444282532, Final Batch Loss: 0.3458629846572876\n",
      "Epoch 2832, Loss: 1.1429825723171234, Final Batch Loss: 0.2528233230113983\n",
      "Epoch 2833, Loss: 1.1800779104232788, Final Batch Loss: 0.2747456729412079\n",
      "Epoch 2834, Loss: 1.2102346867322922, Final Batch Loss: 0.3573344945907593\n",
      "Epoch 2835, Loss: 1.1495780944824219, Final Batch Loss: 0.28231802582740784\n",
      "Epoch 2836, Loss: 1.1456633657217026, Final Batch Loss: 0.3124718964099884\n",
      "Epoch 2837, Loss: 1.2592442035675049, Final Batch Loss: 0.256249338388443\n",
      "Epoch 2838, Loss: 1.3552020192146301, Final Batch Loss: 0.39074814319610596\n",
      "Epoch 2839, Loss: 1.149332195520401, Final Batch Loss: 0.24583718180656433\n",
      "Epoch 2840, Loss: 1.1715154498815536, Final Batch Loss: 0.3078557252883911\n",
      "Epoch 2841, Loss: 1.295130580663681, Final Batch Loss: 0.36426201462745667\n",
      "Epoch 2842, Loss: 1.06524358689785, Final Batch Loss: 0.25196021795272827\n",
      "Epoch 2843, Loss: 1.121923491358757, Final Batch Loss: 0.20652969181537628\n",
      "Epoch 2844, Loss: 1.2182108163833618, Final Batch Loss: 0.3213854730129242\n",
      "Epoch 2845, Loss: 1.1361504644155502, Final Batch Loss: 0.3032577335834503\n",
      "Epoch 2846, Loss: 1.1668396294116974, Final Batch Loss: 0.2327931821346283\n",
      "Epoch 2847, Loss: 1.2057884633541107, Final Batch Loss: 0.2921128273010254\n",
      "Epoch 2848, Loss: 1.1214421838521957, Final Batch Loss: 0.24702180922031403\n",
      "Epoch 2849, Loss: 1.2340892255306244, Final Batch Loss: 0.318832129240036\n",
      "Epoch 2850, Loss: 1.18171027302742, Final Batch Loss: 0.2711826264858246\n",
      "Epoch 2851, Loss: 1.1439261883497238, Final Batch Loss: 0.2343861609697342\n",
      "Epoch 2852, Loss: 1.1231411844491959, Final Batch Loss: 0.3053259253501892\n",
      "Epoch 2853, Loss: 1.10657699406147, Final Batch Loss: 0.2729934751987457\n",
      "Epoch 2854, Loss: 1.1545592844486237, Final Batch Loss: 0.2990478575229645\n",
      "Epoch 2855, Loss: 1.268679291009903, Final Batch Loss: 0.34106943011283875\n",
      "Epoch 2856, Loss: 1.0741686820983887, Final Batch Loss: 0.17365163564682007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2857, Loss: 1.0314799696207047, Final Batch Loss: 0.19704554975032806\n",
      "Epoch 2858, Loss: 1.2259206473827362, Final Batch Loss: 0.2740759253501892\n",
      "Epoch 2859, Loss: 1.296272099018097, Final Batch Loss: 0.2672000229358673\n",
      "Epoch 2860, Loss: 1.2229899764060974, Final Batch Loss: 0.2959684133529663\n",
      "Epoch 2861, Loss: 1.2594872415065765, Final Batch Loss: 0.2797945439815521\n",
      "Epoch 2862, Loss: 1.2568750083446503, Final Batch Loss: 0.34586185216903687\n",
      "Epoch 2863, Loss: 1.128348097205162, Final Batch Loss: 0.23691849410533905\n",
      "Epoch 2864, Loss: 1.179741770029068, Final Batch Loss: 0.3164860010147095\n",
      "Epoch 2865, Loss: 1.0614633709192276, Final Batch Loss: 0.2903707027435303\n",
      "Epoch 2866, Loss: 1.1737131774425507, Final Batch Loss: 0.33137521147727966\n",
      "Epoch 2867, Loss: 1.1523898839950562, Final Batch Loss: 0.29579058289527893\n",
      "Epoch 2868, Loss: 1.1757473051548004, Final Batch Loss: 0.2815003991127014\n",
      "Epoch 2869, Loss: 1.1788243353366852, Final Batch Loss: 0.3345119059085846\n",
      "Epoch 2870, Loss: 1.2080697417259216, Final Batch Loss: 0.29334941506385803\n",
      "Epoch 2871, Loss: 1.270157903432846, Final Batch Loss: 0.2516840696334839\n",
      "Epoch 2872, Loss: 1.2195482552051544, Final Batch Loss: 0.3571181297302246\n",
      "Epoch 2873, Loss: 1.1900824010372162, Final Batch Loss: 0.3918592631816864\n",
      "Epoch 2874, Loss: 1.1694444566965103, Final Batch Loss: 0.34372666478157043\n",
      "Epoch 2875, Loss: 1.2488883435726166, Final Batch Loss: 0.2667768895626068\n",
      "Epoch 2876, Loss: 1.171017825603485, Final Batch Loss: 0.21696338057518005\n",
      "Epoch 2877, Loss: 1.2057558298110962, Final Batch Loss: 0.3217942416667938\n",
      "Epoch 2878, Loss: 1.2433848679065704, Final Batch Loss: 0.3544488251209259\n",
      "Epoch 2879, Loss: 1.2003273665904999, Final Batch Loss: 0.3120783865451813\n",
      "Epoch 2880, Loss: 1.0539439916610718, Final Batch Loss: 0.23344233632087708\n",
      "Epoch 2881, Loss: 1.1775838285684586, Final Batch Loss: 0.21166996657848358\n",
      "Epoch 2882, Loss: 1.2116858959197998, Final Batch Loss: 0.2931458353996277\n",
      "Epoch 2883, Loss: 1.1556000411510468, Final Batch Loss: 0.301460325717926\n",
      "Epoch 2884, Loss: 1.2901312410831451, Final Batch Loss: 0.341174453496933\n",
      "Epoch 2885, Loss: 1.1632627248764038, Final Batch Loss: 0.2983376681804657\n",
      "Epoch 2886, Loss: 1.2106884270906448, Final Batch Loss: 0.35843396186828613\n",
      "Epoch 2887, Loss: 1.2013432681560516, Final Batch Loss: 0.3234332799911499\n",
      "Epoch 2888, Loss: 1.1742749214172363, Final Batch Loss: 0.2855428457260132\n",
      "Epoch 2889, Loss: 1.1235862374305725, Final Batch Loss: 0.2729182839393616\n",
      "Epoch 2890, Loss: 1.2262675166130066, Final Batch Loss: 0.3002173602581024\n",
      "Epoch 2891, Loss: 1.1610920280218124, Final Batch Loss: 0.23125280439853668\n",
      "Epoch 2892, Loss: 1.1939806044101715, Final Batch Loss: 0.28298935294151306\n",
      "Epoch 2893, Loss: 1.1416159719228745, Final Batch Loss: 0.19450391829013824\n",
      "Epoch 2894, Loss: 1.261986404657364, Final Batch Loss: 0.33464673161506653\n",
      "Epoch 2895, Loss: 1.2947511076927185, Final Batch Loss: 0.38829943537712097\n",
      "Epoch 2896, Loss: 1.2168162763118744, Final Batch Loss: 0.28300029039382935\n",
      "Epoch 2897, Loss: 1.151269093155861, Final Batch Loss: 0.242644265294075\n",
      "Epoch 2898, Loss: 1.0570403337478638, Final Batch Loss: 0.2573869526386261\n",
      "Epoch 2899, Loss: 1.140869528055191, Final Batch Loss: 0.2523287236690521\n",
      "Epoch 2900, Loss: 1.1515946984291077, Final Batch Loss: 0.41180935502052307\n",
      "Epoch 2901, Loss: 1.1652143895626068, Final Batch Loss: 0.25187423825263977\n",
      "Epoch 2902, Loss: 1.3003890663385391, Final Batch Loss: 0.4126180112361908\n",
      "Epoch 2903, Loss: 1.2554495334625244, Final Batch Loss: 0.27552613615989685\n",
      "Epoch 2904, Loss: 1.2362907230854034, Final Batch Loss: 0.3117261826992035\n",
      "Epoch 2905, Loss: 1.1449640542268753, Final Batch Loss: 0.32334452867507935\n",
      "Epoch 2906, Loss: 1.1477286666631699, Final Batch Loss: 0.3021692633628845\n",
      "Epoch 2907, Loss: 1.0749517232179642, Final Batch Loss: 0.2337930053472519\n",
      "Epoch 2908, Loss: 1.2813845723867416, Final Batch Loss: 0.3687533438205719\n",
      "Epoch 2909, Loss: 1.1598868072032928, Final Batch Loss: 0.27712592482566833\n",
      "Epoch 2910, Loss: 1.1504363715648651, Final Batch Loss: 0.3202849328517914\n",
      "Epoch 2911, Loss: 1.1168427765369415, Final Batch Loss: 0.2700642943382263\n",
      "Epoch 2912, Loss: 1.2594732344150543, Final Batch Loss: 0.42480283975601196\n",
      "Epoch 2913, Loss: 1.156378909945488, Final Batch Loss: 0.1983528882265091\n",
      "Epoch 2914, Loss: 1.3043878376483917, Final Batch Loss: 0.2765830457210541\n",
      "Epoch 2915, Loss: 1.2711518704891205, Final Batch Loss: 0.3202797472476959\n",
      "Epoch 2916, Loss: 1.1679434478282928, Final Batch Loss: 0.3181179463863373\n",
      "Epoch 2917, Loss: 1.0314857810735703, Final Batch Loss: 0.26310697197914124\n",
      "Epoch 2918, Loss: 1.2376987934112549, Final Batch Loss: 0.37445464730262756\n",
      "Epoch 2919, Loss: 1.1684585511684418, Final Batch Loss: 0.2557569742202759\n",
      "Epoch 2920, Loss: 1.1626579761505127, Final Batch Loss: 0.2910885214805603\n",
      "Epoch 2921, Loss: 1.2751286923885345, Final Batch Loss: 0.3732500970363617\n",
      "Epoch 2922, Loss: 1.1222724467515945, Final Batch Loss: 0.21846182644367218\n",
      "Epoch 2923, Loss: 1.0205319821834564, Final Batch Loss: 0.20270460844039917\n",
      "Epoch 2924, Loss: 1.2275364398956299, Final Batch Loss: 0.2660285234451294\n",
      "Epoch 2925, Loss: 1.1805624067783356, Final Batch Loss: 0.27475693821907043\n",
      "Epoch 2926, Loss: 1.1723274886608124, Final Batch Loss: 0.34202876687049866\n",
      "Epoch 2927, Loss: 1.1273761987686157, Final Batch Loss: 0.2910304665565491\n",
      "Epoch 2928, Loss: 1.1788064539432526, Final Batch Loss: 0.27702978253364563\n",
      "Epoch 2929, Loss: 1.1132303923368454, Final Batch Loss: 0.27497977018356323\n",
      "Epoch 2930, Loss: 1.2009238004684448, Final Batch Loss: 0.292434424161911\n",
      "Epoch 2931, Loss: 1.197255164384842, Final Batch Loss: 0.4059024751186371\n",
      "Epoch 2932, Loss: 1.1225330829620361, Final Batch Loss: 0.20029468834400177\n",
      "Epoch 2933, Loss: 1.1833574324846268, Final Batch Loss: 0.3340604305267334\n",
      "Epoch 2934, Loss: 1.1572856903076172, Final Batch Loss: 0.3026675581932068\n",
      "Epoch 2935, Loss: 1.1570850908756256, Final Batch Loss: 0.30028748512268066\n",
      "Epoch 2936, Loss: 1.1475134193897247, Final Batch Loss: 0.32756415009498596\n",
      "Epoch 2937, Loss: 1.1909001469612122, Final Batch Loss: 0.26306310296058655\n",
      "Epoch 2938, Loss: 1.143623262643814, Final Batch Loss: 0.2599162757396698\n",
      "Epoch 2939, Loss: 1.2451233863830566, Final Batch Loss: 0.31565365195274353\n",
      "Epoch 2940, Loss: 1.2070353329181671, Final Batch Loss: 0.3864406943321228\n",
      "Epoch 2941, Loss: 1.226111501455307, Final Batch Loss: 0.3332824110984802\n",
      "Epoch 2942, Loss: 1.1522616744041443, Final Batch Loss: 0.28575292229652405\n",
      "Epoch 2943, Loss: 1.1648115515708923, Final Batch Loss: 0.24644720554351807\n",
      "Epoch 2944, Loss: 1.1538362801074982, Final Batch Loss: 0.19362980127334595\n",
      "Epoch 2945, Loss: 1.1386957317590714, Final Batch Loss: 0.3536626100540161\n",
      "Epoch 2946, Loss: 1.2162918448448181, Final Batch Loss: 0.3362657427787781\n",
      "Epoch 2947, Loss: 1.070859283208847, Final Batch Loss: 0.2505977153778076\n",
      "Epoch 2948, Loss: 1.3226915299892426, Final Batch Loss: 0.3498717248439789\n",
      "Epoch 2949, Loss: 1.0875801891088486, Final Batch Loss: 0.19178463518619537\n",
      "Epoch 2950, Loss: 1.078487366437912, Final Batch Loss: 0.23553088307380676\n",
      "Epoch 2951, Loss: 1.306905746459961, Final Batch Loss: 0.34031376242637634\n",
      "Epoch 2952, Loss: 1.1914866119623184, Final Batch Loss: 0.43142277002334595\n",
      "Epoch 2953, Loss: 1.318381279706955, Final Batch Loss: 0.3857625126838684\n",
      "Epoch 2954, Loss: 1.1235363185405731, Final Batch Loss: 0.27503031492233276\n",
      "Epoch 2955, Loss: 1.2017894983291626, Final Batch Loss: 0.2858717441558838\n",
      "Epoch 2956, Loss: 1.2925838828086853, Final Batch Loss: 0.3805660307407379\n",
      "Epoch 2957, Loss: 1.2262441962957382, Final Batch Loss: 0.428585946559906\n",
      "Epoch 2958, Loss: 1.2694011330604553, Final Batch Loss: 0.38752686977386475\n",
      "Epoch 2959, Loss: 1.2398031651973724, Final Batch Loss: 0.2846531569957733\n",
      "Epoch 2960, Loss: 1.2038712054491043, Final Batch Loss: 0.373151034116745\n",
      "Epoch 2961, Loss: 1.2532644867897034, Final Batch Loss: 0.43224644660949707\n",
      "Epoch 2962, Loss: 1.176870048046112, Final Batch Loss: 0.30203598737716675\n",
      "Epoch 2963, Loss: 1.0823750346899033, Final Batch Loss: 0.2046004831790924\n",
      "Epoch 2964, Loss: 1.1908986270427704, Final Batch Loss: 0.3636205494403839\n",
      "Epoch 2965, Loss: 1.043002873659134, Final Batch Loss: 0.2772251069545746\n",
      "Epoch 2966, Loss: 1.2643502205610275, Final Batch Loss: 0.42111313343048096\n",
      "Epoch 2967, Loss: 1.106034368276596, Final Batch Loss: 0.3102627098560333\n",
      "Epoch 2968, Loss: 1.125942587852478, Final Batch Loss: 0.333762526512146\n",
      "Epoch 2969, Loss: 1.047337144613266, Final Batch Loss: 0.23764419555664062\n",
      "Epoch 2970, Loss: 1.120308980345726, Final Batch Loss: 0.2916008532047272\n",
      "Epoch 2971, Loss: 1.259388655424118, Final Batch Loss: 0.2590410113334656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2972, Loss: 1.228092223405838, Final Batch Loss: 0.3037974238395691\n",
      "Epoch 2973, Loss: 1.0477149486541748, Final Batch Loss: 0.29715055227279663\n",
      "Epoch 2974, Loss: 1.1931761801242828, Final Batch Loss: 0.28928500413894653\n",
      "Epoch 2975, Loss: 1.1844260394573212, Final Batch Loss: 0.2951715588569641\n",
      "Epoch 2976, Loss: 1.1667981147766113, Final Batch Loss: 0.28480276465415955\n",
      "Epoch 2977, Loss: 1.1786540746688843, Final Batch Loss: 0.33073413372039795\n",
      "Epoch 2978, Loss: 1.1336762607097626, Final Batch Loss: 0.28542909026145935\n",
      "Epoch 2979, Loss: 1.2716718018054962, Final Batch Loss: 0.3434905707836151\n",
      "Epoch 2980, Loss: 1.4171041548252106, Final Batch Loss: 0.33801451325416565\n",
      "Epoch 2981, Loss: 1.1719480752944946, Final Batch Loss: 0.19421833753585815\n",
      "Epoch 2982, Loss: 1.2528961598873138, Final Batch Loss: 0.38127800822257996\n",
      "Epoch 2983, Loss: 1.097199872136116, Final Batch Loss: 0.17470698058605194\n",
      "Epoch 2984, Loss: 1.18936687707901, Final Batch Loss: 0.34365004301071167\n",
      "Epoch 2985, Loss: 1.2057418525218964, Final Batch Loss: 0.35940632224082947\n",
      "Epoch 2986, Loss: 1.0925121754407883, Final Batch Loss: 0.22252018749713898\n",
      "Epoch 2987, Loss: 1.1748054027557373, Final Batch Loss: 0.2506425678730011\n",
      "Epoch 2988, Loss: 1.0834030508995056, Final Batch Loss: 0.30712756514549255\n",
      "Epoch 2989, Loss: 1.1822807490825653, Final Batch Loss: 0.26531118154525757\n",
      "Epoch 2990, Loss: 1.1356417834758759, Final Batch Loss: 0.2919539511203766\n",
      "Epoch 2991, Loss: 1.1339081823825836, Final Batch Loss: 0.2199138104915619\n",
      "Epoch 2992, Loss: 1.1769782155752182, Final Batch Loss: 0.3213179111480713\n",
      "Epoch 2993, Loss: 1.161273404955864, Final Batch Loss: 0.2214997261762619\n",
      "Epoch 2994, Loss: 1.1934091448783875, Final Batch Loss: 0.2783702611923218\n",
      "Epoch 2995, Loss: 1.0671644657850266, Final Batch Loss: 0.28206145763397217\n",
      "Epoch 2996, Loss: 1.3107845783233643, Final Batch Loss: 0.42628470063209534\n",
      "Epoch 2997, Loss: 1.1810543835163116, Final Batch Loss: 0.28415781259536743\n",
      "Epoch 2998, Loss: 1.0425249636173248, Final Batch Loss: 0.20750463008880615\n",
      "Epoch 2999, Loss: 1.2188726365566254, Final Batch Loss: 0.3447805345058441\n",
      "Epoch 3000, Loss: 1.229192703962326, Final Batch Loss: 0.34624090790748596\n",
      "Epoch 3001, Loss: 1.1055689007043839, Final Batch Loss: 0.2932893633842468\n",
      "Epoch 3002, Loss: 1.1587737053632736, Final Batch Loss: 0.36568060517311096\n",
      "Epoch 3003, Loss: 1.214588850736618, Final Batch Loss: 0.3314175307750702\n",
      "Epoch 3004, Loss: 1.1188061386346817, Final Batch Loss: 0.2742019295692444\n",
      "Epoch 3005, Loss: 1.098662182688713, Final Batch Loss: 0.2312762588262558\n",
      "Epoch 3006, Loss: 1.2192097306251526, Final Batch Loss: 0.2855418622493744\n",
      "Epoch 3007, Loss: 1.1555007100105286, Final Batch Loss: 0.3618892729282379\n",
      "Epoch 3008, Loss: 1.1803401708602905, Final Batch Loss: 0.26938343048095703\n",
      "Epoch 3009, Loss: 1.2308898270130157, Final Batch Loss: 0.28806808590888977\n",
      "Epoch 3010, Loss: 1.1144488006830215, Final Batch Loss: 0.2550435960292816\n",
      "Epoch 3011, Loss: 1.2818766385316849, Final Batch Loss: 0.3739495575428009\n",
      "Epoch 3012, Loss: 1.185808926820755, Final Batch Loss: 0.3088577389717102\n",
      "Epoch 3013, Loss: 1.1171395629644394, Final Batch Loss: 0.24927839636802673\n",
      "Epoch 3014, Loss: 1.2039412260055542, Final Batch Loss: 0.3863789737224579\n",
      "Epoch 3015, Loss: 1.191523015499115, Final Batch Loss: 0.383663535118103\n",
      "Epoch 3016, Loss: 1.1812676638364792, Final Batch Loss: 0.2476608008146286\n",
      "Epoch 3017, Loss: 1.1873798072338104, Final Batch Loss: 0.25981658697128296\n",
      "Epoch 3018, Loss: 1.2743582129478455, Final Batch Loss: 0.34315091371536255\n",
      "Epoch 3019, Loss: 1.225511610507965, Final Batch Loss: 0.26736560463905334\n",
      "Epoch 3020, Loss: 1.1302866339683533, Final Batch Loss: 0.30311381816864014\n",
      "Epoch 3021, Loss: 1.2228103578090668, Final Batch Loss: 0.27704471349716187\n",
      "Epoch 3022, Loss: 1.232666939496994, Final Batch Loss: 0.337467759847641\n",
      "Epoch 3023, Loss: 1.0953264236450195, Final Batch Loss: 0.25650718808174133\n",
      "Epoch 3024, Loss: 1.1227378249168396, Final Batch Loss: 0.18977883458137512\n",
      "Epoch 3025, Loss: 1.0784170478582382, Final Batch Loss: 0.2002391517162323\n",
      "Epoch 3026, Loss: 1.1418951600790024, Final Batch Loss: 0.28732460737228394\n",
      "Epoch 3027, Loss: 1.1868585348129272, Final Batch Loss: 0.2150038778781891\n",
      "Epoch 3028, Loss: 1.2935022413730621, Final Batch Loss: 0.43535473942756653\n",
      "Epoch 3029, Loss: 1.1187515407800674, Final Batch Loss: 0.2403443604707718\n",
      "Epoch 3030, Loss: 1.1278570592403412, Final Batch Loss: 0.2840009033679962\n",
      "Epoch 3031, Loss: 1.1025865823030472, Final Batch Loss: 0.27128323912620544\n",
      "Epoch 3032, Loss: 1.2570236027240753, Final Batch Loss: 0.28797584772109985\n",
      "Epoch 3033, Loss: 1.0542476624250412, Final Batch Loss: 0.24018016457557678\n",
      "Epoch 3034, Loss: 1.2310731112957, Final Batch Loss: 0.3144863545894623\n",
      "Epoch 3035, Loss: 1.233285516500473, Final Batch Loss: 0.36150312423706055\n",
      "Epoch 3036, Loss: 1.1836868822574615, Final Batch Loss: 0.27735960483551025\n",
      "Epoch 3037, Loss: 1.151697188615799, Final Batch Loss: 0.3456873297691345\n",
      "Epoch 3038, Loss: 1.1718519628047943, Final Batch Loss: 0.3159659504890442\n",
      "Epoch 3039, Loss: 1.1964085102081299, Final Batch Loss: 0.37662166357040405\n",
      "Epoch 3040, Loss: 1.1585478335618973, Final Batch Loss: 0.23838625848293304\n",
      "Epoch 3041, Loss: 1.2303614616394043, Final Batch Loss: 0.3645547032356262\n",
      "Epoch 3042, Loss: 1.2026772201061249, Final Batch Loss: 0.28255975246429443\n",
      "Epoch 3043, Loss: 1.106088399887085, Final Batch Loss: 0.2597184479236603\n",
      "Epoch 3044, Loss: 1.2085424959659576, Final Batch Loss: 0.2926120162010193\n",
      "Epoch 3045, Loss: 1.2473905086517334, Final Batch Loss: 0.40719807147979736\n",
      "Epoch 3046, Loss: 1.1642065197229385, Final Batch Loss: 0.34754839539527893\n",
      "Epoch 3047, Loss: 1.1250813752412796, Final Batch Loss: 0.21004168689250946\n",
      "Epoch 3048, Loss: 1.1583003252744675, Final Batch Loss: 0.3063270151615143\n",
      "Epoch 3049, Loss: 1.0436285138130188, Final Batch Loss: 0.23278261721134186\n",
      "Epoch 3050, Loss: 1.157412976026535, Final Batch Loss: 0.28279176354408264\n",
      "Epoch 3051, Loss: 1.0767978727817535, Final Batch Loss: 0.2702873647212982\n",
      "Epoch 3052, Loss: 1.2381470948457718, Final Batch Loss: 0.336439311504364\n",
      "Epoch 3053, Loss: 1.1068590879440308, Final Batch Loss: 0.2559727132320404\n",
      "Epoch 3054, Loss: 1.136039286851883, Final Batch Loss: 0.2792154848575592\n",
      "Epoch 3055, Loss: 1.1225191354751587, Final Batch Loss: 0.21915093064308167\n",
      "Epoch 3056, Loss: 1.1427198946475983, Final Batch Loss: 0.25370362401008606\n",
      "Epoch 3057, Loss: 1.150375097990036, Final Batch Loss: 0.25914984941482544\n",
      "Epoch 3058, Loss: 1.065134197473526, Final Batch Loss: 0.2538672089576721\n",
      "Epoch 3059, Loss: 1.283250629901886, Final Batch Loss: 0.3280254900455475\n",
      "Epoch 3060, Loss: 1.1612458676099777, Final Batch Loss: 0.3709666132926941\n",
      "Epoch 3061, Loss: 1.1525398790836334, Final Batch Loss: 0.31049513816833496\n",
      "Epoch 3062, Loss: 1.149651661515236, Final Batch Loss: 0.2273392528295517\n",
      "Epoch 3063, Loss: 1.1714813113212585, Final Batch Loss: 0.3387886583805084\n",
      "Epoch 3064, Loss: 1.1686468720436096, Final Batch Loss: 0.301244854927063\n",
      "Epoch 3065, Loss: 1.1314038634300232, Final Batch Loss: 0.2939477264881134\n",
      "Epoch 3066, Loss: 1.0993925780057907, Final Batch Loss: 0.276607483625412\n",
      "Epoch 3067, Loss: 1.2064236402511597, Final Batch Loss: 0.3052161633968353\n",
      "Epoch 3068, Loss: 1.0840474963188171, Final Batch Loss: 0.20157763361930847\n",
      "Epoch 3069, Loss: 1.1829339563846588, Final Batch Loss: 0.3201736807823181\n",
      "Epoch 3070, Loss: 1.101177453994751, Final Batch Loss: 0.3297203481197357\n",
      "Epoch 3071, Loss: 1.16510009765625, Final Batch Loss: 0.30349090695381165\n",
      "Epoch 3072, Loss: 1.2015505731105804, Final Batch Loss: 0.34235620498657227\n",
      "Epoch 3073, Loss: 1.2787735164165497, Final Batch Loss: 0.3711385726928711\n",
      "Epoch 3074, Loss: 1.174121767282486, Final Batch Loss: 0.2810043394565582\n",
      "Epoch 3075, Loss: 1.1888464391231537, Final Batch Loss: 0.29021111130714417\n",
      "Epoch 3076, Loss: 1.1158661246299744, Final Batch Loss: 0.27018582820892334\n",
      "Epoch 3077, Loss: 1.0884895771741867, Final Batch Loss: 0.18600185215473175\n",
      "Epoch 3078, Loss: 1.3661421239376068, Final Batch Loss: 0.4394071102142334\n",
      "Epoch 3079, Loss: 1.1843521893024445, Final Batch Loss: 0.32097798585891724\n",
      "Epoch 3080, Loss: 1.2058939635753632, Final Batch Loss: 0.2898339033126831\n",
      "Epoch 3081, Loss: 1.1292918771505356, Final Batch Loss: 0.38611578941345215\n",
      "Epoch 3082, Loss: 1.1034370511770248, Final Batch Loss: 0.21385706961154938\n",
      "Epoch 3083, Loss: 1.2047272622585297, Final Batch Loss: 0.3696454167366028\n",
      "Epoch 3084, Loss: 1.1916837990283966, Final Batch Loss: 0.3583495020866394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3085, Loss: 1.111515998840332, Final Batch Loss: 0.26601263880729675\n",
      "Epoch 3086, Loss: 1.2184258103370667, Final Batch Loss: 0.22969776391983032\n",
      "Epoch 3087, Loss: 1.1592437028884888, Final Batch Loss: 0.29473116993904114\n",
      "Epoch 3088, Loss: 1.1689611971378326, Final Batch Loss: 0.3259827494621277\n",
      "Epoch 3089, Loss: 1.2341469079256058, Final Batch Loss: 0.36465540528297424\n",
      "Epoch 3090, Loss: 1.1286628544330597, Final Batch Loss: 0.33766454458236694\n",
      "Epoch 3091, Loss: 1.136143445968628, Final Batch Loss: 0.25906237959861755\n",
      "Epoch 3092, Loss: 1.1216158717870712, Final Batch Loss: 0.3189822733402252\n",
      "Epoch 3093, Loss: 1.1313975751399994, Final Batch Loss: 0.26332467794418335\n",
      "Epoch 3094, Loss: 1.0500118434429169, Final Batch Loss: 0.31639912724494934\n",
      "Epoch 3095, Loss: 1.1421624571084976, Final Batch Loss: 0.34047171473503113\n",
      "Epoch 3096, Loss: 1.227621614933014, Final Batch Loss: 0.2677868902683258\n",
      "Epoch 3097, Loss: 1.167802557349205, Final Batch Loss: 0.3556418716907501\n",
      "Epoch 3098, Loss: 1.2764770686626434, Final Batch Loss: 0.27658161520957947\n",
      "Epoch 3099, Loss: 1.0970706343650818, Final Batch Loss: 0.2757311463356018\n",
      "Epoch 3100, Loss: 1.2611373960971832, Final Batch Loss: 0.3532209098339081\n",
      "Epoch 3101, Loss: 1.1405827552080154, Final Batch Loss: 0.3033900260925293\n",
      "Epoch 3102, Loss: 1.1601670682430267, Final Batch Loss: 0.32585442066192627\n",
      "Epoch 3103, Loss: 1.0781597644090652, Final Batch Loss: 0.24387235939502716\n",
      "Epoch 3104, Loss: 1.094230517745018, Final Batch Loss: 0.17089898884296417\n",
      "Epoch 3105, Loss: 1.0508525222539902, Final Batch Loss: 0.24346640706062317\n",
      "Epoch 3106, Loss: 1.1445291340351105, Final Batch Loss: 0.2992718517780304\n",
      "Epoch 3107, Loss: 1.2081208229064941, Final Batch Loss: 0.36769208312034607\n",
      "Epoch 3108, Loss: 1.1592993438243866, Final Batch Loss: 0.2876051962375641\n",
      "Epoch 3109, Loss: 1.1426708102226257, Final Batch Loss: 0.20936301350593567\n",
      "Epoch 3110, Loss: 1.1875, Final Batch Loss: 0.32139813899993896\n",
      "Epoch 3111, Loss: 1.1108460426330566, Final Batch Loss: 0.3027692139148712\n",
      "Epoch 3112, Loss: 1.0325905233621597, Final Batch Loss: 0.24680820107460022\n",
      "Epoch 3113, Loss: 1.0445131212472916, Final Batch Loss: 0.22858752310276031\n",
      "Epoch 3114, Loss: 1.0881195366382599, Final Batch Loss: 0.32691314816474915\n",
      "Epoch 3115, Loss: 1.0748082846403122, Final Batch Loss: 0.20416021347045898\n",
      "Epoch 3116, Loss: 1.0282042771577835, Final Batch Loss: 0.17875371873378754\n",
      "Epoch 3117, Loss: 1.2024215161800385, Final Batch Loss: 0.3004511296749115\n",
      "Epoch 3118, Loss: 1.074131190776825, Final Batch Loss: 0.31208735704421997\n",
      "Epoch 3119, Loss: 1.2106600254774094, Final Batch Loss: 0.4093589186668396\n",
      "Epoch 3120, Loss: 1.0473936945199966, Final Batch Loss: 0.22872625291347504\n",
      "Epoch 3121, Loss: 1.1372544318437576, Final Batch Loss: 0.3170013129711151\n",
      "Epoch 3122, Loss: 1.083729699254036, Final Batch Loss: 0.35023555159568787\n",
      "Epoch 3123, Loss: 1.0499420315027237, Final Batch Loss: 0.24283325672149658\n",
      "Epoch 3124, Loss: 1.0454712808132172, Final Batch Loss: 0.3148024380207062\n",
      "Epoch 3125, Loss: 1.1201776266098022, Final Batch Loss: 0.25762882828712463\n",
      "Epoch 3126, Loss: 1.1293014883995056, Final Batch Loss: 0.26152583956718445\n",
      "Epoch 3127, Loss: 1.1162501573562622, Final Batch Loss: 0.2731771171092987\n",
      "Epoch 3128, Loss: 1.1317654699087143, Final Batch Loss: 0.294280081987381\n",
      "Epoch 3129, Loss: 1.2158772349357605, Final Batch Loss: 0.3574465215206146\n",
      "Epoch 3130, Loss: 1.1390442103147507, Final Batch Loss: 0.25403639674186707\n",
      "Epoch 3131, Loss: 1.1367190182209015, Final Batch Loss: 0.2675974369049072\n",
      "Epoch 3132, Loss: 1.0660142004489899, Final Batch Loss: 0.3229331076145172\n",
      "Epoch 3133, Loss: 1.1549151688814163, Final Batch Loss: 0.3753269910812378\n",
      "Epoch 3134, Loss: 1.188408076763153, Final Batch Loss: 0.2472706437110901\n",
      "Epoch 3135, Loss: 1.006958469748497, Final Batch Loss: 0.27663305401802063\n",
      "Epoch 3136, Loss: 1.1355937272310257, Final Batch Loss: 0.2294953316450119\n",
      "Epoch 3137, Loss: 1.2197970300912857, Final Batch Loss: 0.2569204866886139\n",
      "Epoch 3138, Loss: 1.0964143425226212, Final Batch Loss: 0.21800391376018524\n",
      "Epoch 3139, Loss: 1.147643730044365, Final Batch Loss: 0.23829476535320282\n",
      "Epoch 3140, Loss: 1.2325711250305176, Final Batch Loss: 0.27090349793434143\n",
      "Epoch 3141, Loss: 1.1464502960443497, Final Batch Loss: 0.2911744713783264\n",
      "Epoch 3142, Loss: 1.1714437901973724, Final Batch Loss: 0.19862136244773865\n",
      "Epoch 3143, Loss: 1.178995043039322, Final Batch Loss: 0.27764084935188293\n",
      "Epoch 3144, Loss: 1.2317663729190826, Final Batch Loss: 0.2866145074367523\n",
      "Epoch 3145, Loss: 1.222066044807434, Final Batch Loss: 0.29127824306488037\n",
      "Epoch 3146, Loss: 1.0473152995109558, Final Batch Loss: 0.26315686106681824\n",
      "Epoch 3147, Loss: 1.0710413455963135, Final Batch Loss: 0.24985960125923157\n",
      "Epoch 3148, Loss: 1.0981155633926392, Final Batch Loss: 0.19443362951278687\n",
      "Epoch 3149, Loss: 1.1331576704978943, Final Batch Loss: 0.2580215036869049\n",
      "Epoch 3150, Loss: 1.2100613713264465, Final Batch Loss: 0.29302000999450684\n",
      "Epoch 3151, Loss: 1.068048357963562, Final Batch Loss: 0.2416914403438568\n",
      "Epoch 3152, Loss: 1.0758980363607407, Final Batch Loss: 0.2852259874343872\n",
      "Epoch 3153, Loss: 1.2480558454990387, Final Batch Loss: 0.41709569096565247\n",
      "Epoch 3154, Loss: 1.072250783443451, Final Batch Loss: 0.2404821813106537\n",
      "Epoch 3155, Loss: 1.1727007031440735, Final Batch Loss: 0.260364294052124\n",
      "Epoch 3156, Loss: 1.1384489238262177, Final Batch Loss: 0.3071291446685791\n",
      "Epoch 3157, Loss: 1.1358153223991394, Final Batch Loss: 0.2678754925727844\n",
      "Epoch 3158, Loss: 1.0127804577350616, Final Batch Loss: 0.18027541041374207\n",
      "Epoch 3159, Loss: 1.0665709227323532, Final Batch Loss: 0.2470882683992386\n",
      "Epoch 3160, Loss: 1.048051878809929, Final Batch Loss: 0.2987416684627533\n",
      "Epoch 3161, Loss: 1.1664046943187714, Final Batch Loss: 0.36363962292671204\n",
      "Epoch 3162, Loss: 1.0515139997005463, Final Batch Loss: 0.20346274971961975\n",
      "Epoch 3163, Loss: 1.0530644059181213, Final Batch Loss: 0.19345983862876892\n",
      "Epoch 3164, Loss: 1.0920921117067337, Final Batch Loss: 0.2664520740509033\n",
      "Epoch 3165, Loss: 1.0267509371042252, Final Batch Loss: 0.2183157056570053\n",
      "Epoch 3166, Loss: 1.2172242105007172, Final Batch Loss: 0.38047319650650024\n",
      "Epoch 3167, Loss: 1.0908536165952682, Final Batch Loss: 0.2085433453321457\n",
      "Epoch 3168, Loss: 1.1895674765110016, Final Batch Loss: 0.31460368633270264\n",
      "Epoch 3169, Loss: 1.08221934735775, Final Batch Loss: 0.2353634387254715\n",
      "Epoch 3170, Loss: 1.0556917637586594, Final Batch Loss: 0.24945832788944244\n",
      "Epoch 3171, Loss: 1.107459545135498, Final Batch Loss: 0.2929055094718933\n",
      "Epoch 3172, Loss: 1.110380619764328, Final Batch Loss: 0.2795986533164978\n",
      "Epoch 3173, Loss: 1.112909197807312, Final Batch Loss: 0.2489965260028839\n",
      "Epoch 3174, Loss: 1.2361535727977753, Final Batch Loss: 0.35123810172080994\n",
      "Epoch 3175, Loss: 1.3127518594264984, Final Batch Loss: 0.3849887251853943\n",
      "Epoch 3176, Loss: 1.0402354896068573, Final Batch Loss: 0.21496452391147614\n",
      "Epoch 3177, Loss: 1.1509459912776947, Final Batch Loss: 0.2506099045276642\n",
      "Epoch 3178, Loss: 1.2081536650657654, Final Batch Loss: 0.27862295508384705\n",
      "Epoch 3179, Loss: 1.200825810432434, Final Batch Loss: 0.3678980767726898\n",
      "Epoch 3180, Loss: 1.0659785866737366, Final Batch Loss: 0.2675692141056061\n",
      "Epoch 3181, Loss: 1.1821117103099823, Final Batch Loss: 0.32757484912872314\n",
      "Epoch 3182, Loss: 1.2127199918031693, Final Batch Loss: 0.40380918979644775\n",
      "Epoch 3183, Loss: 1.185512825846672, Final Batch Loss: 0.4004634618759155\n",
      "Epoch 3184, Loss: 1.271693617105484, Final Batch Loss: 0.3334614038467407\n",
      "Epoch 3185, Loss: 1.1431360244750977, Final Batch Loss: 0.24910050630569458\n",
      "Epoch 3186, Loss: 1.237847626209259, Final Batch Loss: 0.30879440903663635\n",
      "Epoch 3187, Loss: 1.201674073934555, Final Batch Loss: 0.34242168068885803\n",
      "Epoch 3188, Loss: 1.049897700548172, Final Batch Loss: 0.26734334230422974\n",
      "Epoch 3189, Loss: 1.2178035080432892, Final Batch Loss: 0.33802035450935364\n",
      "Epoch 3190, Loss: 1.3169735372066498, Final Batch Loss: 0.3943638503551483\n",
      "Epoch 3191, Loss: 1.1765554249286652, Final Batch Loss: 0.33675330877304077\n",
      "Epoch 3192, Loss: 1.1040808260440826, Final Batch Loss: 0.24762707948684692\n",
      "Epoch 3193, Loss: 1.0499152094125748, Final Batch Loss: 0.2743515372276306\n",
      "Epoch 3194, Loss: 1.096598967909813, Final Batch Loss: 0.2598651945590973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3195, Loss: 1.103809654712677, Final Batch Loss: 0.25631362199783325\n",
      "Epoch 3196, Loss: 1.2267176806926727, Final Batch Loss: 0.2884930372238159\n",
      "Epoch 3197, Loss: 1.0384309142827988, Final Batch Loss: 0.20423726737499237\n",
      "Epoch 3198, Loss: 1.1670776009559631, Final Batch Loss: 0.22191476821899414\n",
      "Epoch 3199, Loss: 1.144877091050148, Final Batch Loss: 0.33732178807258606\n",
      "Epoch 3200, Loss: 1.061557799577713, Final Batch Loss: 0.2745797336101532\n",
      "Epoch 3201, Loss: 0.9886590093374252, Final Batch Loss: 0.15779827535152435\n",
      "Epoch 3202, Loss: 1.27840918302536, Final Batch Loss: 0.36617395281791687\n",
      "Epoch 3203, Loss: 1.2031667530536652, Final Batch Loss: 0.3323611915111542\n",
      "Epoch 3204, Loss: 1.1145797073841095, Final Batch Loss: 0.3587198555469513\n",
      "Epoch 3205, Loss: 1.058882087469101, Final Batch Loss: 0.2321796715259552\n",
      "Epoch 3206, Loss: 1.145590603351593, Final Batch Loss: 0.34525036811828613\n",
      "Epoch 3207, Loss: 1.1358158886432648, Final Batch Loss: 0.2145768702030182\n",
      "Epoch 3208, Loss: 1.1995432674884796, Final Batch Loss: 0.33740776777267456\n",
      "Epoch 3209, Loss: 1.1365057826042175, Final Batch Loss: 0.29144057631492615\n",
      "Epoch 3210, Loss: 1.0850540846586227, Final Batch Loss: 0.19122610986232758\n",
      "Epoch 3211, Loss: 1.1713299453258514, Final Batch Loss: 0.25918692350387573\n",
      "Epoch 3212, Loss: 1.0992317646741867, Final Batch Loss: 0.22996146976947784\n",
      "Epoch 3213, Loss: 1.2628490328788757, Final Batch Loss: 0.2853377163410187\n",
      "Epoch 3214, Loss: 1.1045664697885513, Final Batch Loss: 0.24606747925281525\n",
      "Epoch 3215, Loss: 1.1246357262134552, Final Batch Loss: 0.2626202702522278\n",
      "Epoch 3216, Loss: 1.1823001205921173, Final Batch Loss: 0.27860668301582336\n",
      "Epoch 3217, Loss: 1.0852484107017517, Final Batch Loss: 0.2585377097129822\n",
      "Epoch 3218, Loss: 1.1535313427448273, Final Batch Loss: 0.30770808458328247\n",
      "Epoch 3219, Loss: 1.2178033739328384, Final Batch Loss: 0.3646336495876312\n",
      "Epoch 3220, Loss: 1.0678448528051376, Final Batch Loss: 0.19204403460025787\n",
      "Epoch 3221, Loss: 1.148931935429573, Final Batch Loss: 0.23445110023021698\n",
      "Epoch 3222, Loss: 1.2003518044948578, Final Batch Loss: 0.38623055815696716\n",
      "Epoch 3223, Loss: 1.2084944546222687, Final Batch Loss: 0.3435821533203125\n",
      "Epoch 3224, Loss: 1.090206429362297, Final Batch Loss: 0.25006788969039917\n",
      "Epoch 3225, Loss: 1.13271963596344, Final Batch Loss: 0.2747081220149994\n",
      "Epoch 3226, Loss: 1.1266905963420868, Final Batch Loss: 0.24110332131385803\n",
      "Epoch 3227, Loss: 1.095989629626274, Final Batch Loss: 0.18989278376102448\n",
      "Epoch 3228, Loss: 1.0965482294559479, Final Batch Loss: 0.2568086087703705\n",
      "Epoch 3229, Loss: 1.3335967659950256, Final Batch Loss: 0.4207768738269806\n",
      "Epoch 3230, Loss: 1.10564823448658, Final Batch Loss: 0.30402663350105286\n",
      "Epoch 3231, Loss: 1.1848213374614716, Final Batch Loss: 0.35682108998298645\n",
      "Epoch 3232, Loss: 1.0605533421039581, Final Batch Loss: 0.26134631037712097\n",
      "Epoch 3233, Loss: 1.10476416349411, Final Batch Loss: 0.25360849499702454\n",
      "Epoch 3234, Loss: 1.0994573831558228, Final Batch Loss: 0.3436489999294281\n",
      "Epoch 3235, Loss: 1.1753832697868347, Final Batch Loss: 0.27799397706985474\n",
      "Epoch 3236, Loss: 1.0565125346183777, Final Batch Loss: 0.2927890419960022\n",
      "Epoch 3237, Loss: 1.0828442126512527, Final Batch Loss: 0.27976110577583313\n",
      "Epoch 3238, Loss: 1.0576257407665253, Final Batch Loss: 0.32615503668785095\n",
      "Epoch 3239, Loss: 0.9835634380578995, Final Batch Loss: 0.20725107192993164\n",
      "Epoch 3240, Loss: 1.2161516696214676, Final Batch Loss: 0.3604775071144104\n",
      "Epoch 3241, Loss: 1.1227083504199982, Final Batch Loss: 0.2803848683834076\n",
      "Epoch 3242, Loss: 1.179644227027893, Final Batch Loss: 0.28894513845443726\n",
      "Epoch 3243, Loss: 1.1686217188835144, Final Batch Loss: 0.30162209272384644\n",
      "Epoch 3244, Loss: 1.0593882203102112, Final Batch Loss: 0.2217610776424408\n",
      "Epoch 3245, Loss: 1.1265111416578293, Final Batch Loss: 0.3792227804660797\n",
      "Epoch 3246, Loss: 1.1257510483264923, Final Batch Loss: 0.301567405462265\n",
      "Epoch 3247, Loss: 1.1943874061107635, Final Batch Loss: 0.3013964295387268\n",
      "Epoch 3248, Loss: 1.017538607120514, Final Batch Loss: 0.27245911955833435\n",
      "Epoch 3249, Loss: 1.189500331878662, Final Batch Loss: 0.3091331124305725\n",
      "Epoch 3250, Loss: 1.1204932481050491, Final Batch Loss: 0.30513542890548706\n",
      "Epoch 3251, Loss: 1.0810133814811707, Final Batch Loss: 0.22909721732139587\n",
      "Epoch 3252, Loss: 1.0387928932905197, Final Batch Loss: 0.1711973398923874\n",
      "Epoch 3253, Loss: 1.0989250391721725, Final Batch Loss: 0.2786737382411957\n",
      "Epoch 3254, Loss: 1.06101955473423, Final Batch Loss: 0.20275448262691498\n",
      "Epoch 3255, Loss: 1.120573252439499, Final Batch Loss: 0.28005391359329224\n",
      "Epoch 3256, Loss: 1.1379064470529556, Final Batch Loss: 0.3165625035762787\n",
      "Epoch 3257, Loss: 1.0489376187324524, Final Batch Loss: 0.25858569145202637\n",
      "Epoch 3258, Loss: 1.1059624254703522, Final Batch Loss: 0.30908137559890747\n",
      "Epoch 3259, Loss: 1.108170598745346, Final Batch Loss: 0.20308053493499756\n",
      "Epoch 3260, Loss: 1.1002239435911179, Final Batch Loss: 0.20699721574783325\n",
      "Epoch 3261, Loss: 1.0999651849269867, Final Batch Loss: 0.1782628893852234\n",
      "Epoch 3262, Loss: 1.1478050649166107, Final Batch Loss: 0.27776476740837097\n",
      "Epoch 3263, Loss: 1.0207535326480865, Final Batch Loss: 0.20350763201713562\n",
      "Epoch 3264, Loss: 1.1079554110765457, Final Batch Loss: 0.2739810347557068\n",
      "Epoch 3265, Loss: 0.9607695043087006, Final Batch Loss: 0.2580907940864563\n",
      "Epoch 3266, Loss: 1.1208403706550598, Final Batch Loss: 0.319301575422287\n",
      "Epoch 3267, Loss: 1.0714380592107773, Final Batch Loss: 0.28280943632125854\n",
      "Epoch 3268, Loss: 1.1627554595470428, Final Batch Loss: 0.2612578570842743\n",
      "Epoch 3269, Loss: 1.066521480679512, Final Batch Loss: 0.24249684810638428\n",
      "Epoch 3270, Loss: 1.1760846674442291, Final Batch Loss: 0.2773723900318146\n",
      "Epoch 3271, Loss: 1.085931345820427, Final Batch Loss: 0.30862748622894287\n",
      "Epoch 3272, Loss: 1.0885783433914185, Final Batch Loss: 0.2071649432182312\n",
      "Epoch 3273, Loss: 1.0474784076213837, Final Batch Loss: 0.25365233421325684\n",
      "Epoch 3274, Loss: 1.0466218292713165, Final Batch Loss: 0.28190791606903076\n",
      "Epoch 3275, Loss: 1.155971646308899, Final Batch Loss: 0.3255298137664795\n",
      "Epoch 3276, Loss: 1.1594337821006775, Final Batch Loss: 0.2766645550727844\n",
      "Epoch 3277, Loss: 1.111706703901291, Final Batch Loss: 0.2596004009246826\n",
      "Epoch 3278, Loss: 1.1963719129562378, Final Batch Loss: 0.3399480879306793\n",
      "Epoch 3279, Loss: 1.1608789563179016, Final Batch Loss: 0.26288244128227234\n",
      "Epoch 3280, Loss: 1.0100212693214417, Final Batch Loss: 0.21241949498653412\n",
      "Epoch 3281, Loss: 1.1167244613170624, Final Batch Loss: 0.25886455178260803\n",
      "Epoch 3282, Loss: 1.190410077571869, Final Batch Loss: 0.30703386664390564\n",
      "Epoch 3283, Loss: 1.1043792068958282, Final Batch Loss: 0.2519283592700958\n",
      "Epoch 3284, Loss: 1.177456095814705, Final Batch Loss: 0.33405959606170654\n",
      "Epoch 3285, Loss: 1.1624291837215424, Final Batch Loss: 0.29667067527770996\n",
      "Epoch 3286, Loss: 1.1482555717229843, Final Batch Loss: 0.22546370327472687\n",
      "Epoch 3287, Loss: 0.9584256708621979, Final Batch Loss: 0.21202418208122253\n",
      "Epoch 3288, Loss: 1.0803810805082321, Final Batch Loss: 0.2744249701499939\n",
      "Epoch 3289, Loss: 1.094649314880371, Final Batch Loss: 0.18527710437774658\n",
      "Epoch 3290, Loss: 1.1446626633405685, Final Batch Loss: 0.3306128978729248\n",
      "Epoch 3291, Loss: 1.0654669255018234, Final Batch Loss: 0.29728633165359497\n",
      "Epoch 3292, Loss: 1.0561572313308716, Final Batch Loss: 0.2353971302509308\n",
      "Epoch 3293, Loss: 0.9865074008703232, Final Batch Loss: 0.1549084633588791\n",
      "Epoch 3294, Loss: 1.0881796777248383, Final Batch Loss: 0.2865106761455536\n",
      "Epoch 3295, Loss: 1.0895796567201614, Final Batch Loss: 0.24487878382205963\n",
      "Epoch 3296, Loss: 1.0472573339939117, Final Batch Loss: 0.27133530378341675\n",
      "Epoch 3297, Loss: 1.0781235098838806, Final Batch Loss: 0.23940137028694153\n",
      "Epoch 3298, Loss: 1.0094624906778336, Final Batch Loss: 0.24059396982192993\n",
      "Epoch 3299, Loss: 1.1429055631160736, Final Batch Loss: 0.2738627791404724\n",
      "Epoch 3300, Loss: 1.1214134693145752, Final Batch Loss: 0.35677650570869446\n",
      "Epoch 3301, Loss: 1.0898771435022354, Final Batch Loss: 0.2682037949562073\n",
      "Epoch 3302, Loss: 1.0947916507720947, Final Batch Loss: 0.26873281598091125\n",
      "Epoch 3303, Loss: 1.209976315498352, Final Batch Loss: 0.345260351896286\n",
      "Epoch 3304, Loss: 1.1934019327163696, Final Batch Loss: 0.4563477635383606\n",
      "Epoch 3305, Loss: 1.1927341371774673, Final Batch Loss: 0.3666653633117676\n",
      "Epoch 3306, Loss: 1.0459854900836945, Final Batch Loss: 0.23387441039085388\n",
      "Epoch 3307, Loss: 1.1338626742362976, Final Batch Loss: 0.26461824774742126\n",
      "Epoch 3308, Loss: 1.1611117720603943, Final Batch Loss: 0.32513317465782166\n",
      "Epoch 3309, Loss: 0.9943123012781143, Final Batch Loss: 0.25837990641593933\n",
      "Epoch 3310, Loss: 1.1551460921764374, Final Batch Loss: 0.33273646235466003\n",
      "Epoch 3311, Loss: 1.1417342722415924, Final Batch Loss: 0.3270936906337738\n",
      "Epoch 3312, Loss: 1.0231079459190369, Final Batch Loss: 0.2452341467142105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3313, Loss: 1.1323712468147278, Final Batch Loss: 0.30127325654029846\n",
      "Epoch 3314, Loss: 1.0891957730054855, Final Batch Loss: 0.2510727345943451\n",
      "Epoch 3315, Loss: 1.185016468167305, Final Batch Loss: 0.29732784628868103\n",
      "Epoch 3316, Loss: 1.0455096513032913, Final Batch Loss: 0.3572373688220978\n",
      "Epoch 3317, Loss: 1.048721268773079, Final Batch Loss: 0.27344226837158203\n",
      "Epoch 3318, Loss: 1.1434998214244843, Final Batch Loss: 0.2719959318637848\n",
      "Epoch 3319, Loss: 1.1006888449192047, Final Batch Loss: 0.2792291045188904\n",
      "Epoch 3320, Loss: 1.0748389065265656, Final Batch Loss: 0.3083421587944031\n",
      "Epoch 3321, Loss: 1.1214760839939117, Final Batch Loss: 0.25827521085739136\n",
      "Epoch 3322, Loss: 1.141371876001358, Final Batch Loss: 0.33182746171951294\n",
      "Epoch 3323, Loss: 1.1733329594135284, Final Batch Loss: 0.28337839245796204\n",
      "Epoch 3324, Loss: 1.1181927621364594, Final Batch Loss: 0.3045087456703186\n",
      "Epoch 3325, Loss: 1.1152118891477585, Final Batch Loss: 0.21502956748008728\n",
      "Epoch 3326, Loss: 1.1673366576433182, Final Batch Loss: 0.33032700419425964\n",
      "Epoch 3327, Loss: 1.1506937742233276, Final Batch Loss: 0.25375378131866455\n",
      "Epoch 3328, Loss: 1.083479642868042, Final Batch Loss: 0.28167641162872314\n",
      "Epoch 3329, Loss: 1.1347571462392807, Final Batch Loss: 0.2732856571674347\n",
      "Epoch 3330, Loss: 0.9402102530002594, Final Batch Loss: 0.17302455008029938\n",
      "Epoch 3331, Loss: 1.127610296010971, Final Batch Loss: 0.27469751238822937\n",
      "Epoch 3332, Loss: 1.1155472248792648, Final Batch Loss: 0.26819372177124023\n",
      "Epoch 3333, Loss: 1.1782282143831253, Final Batch Loss: 0.3238363265991211\n",
      "Epoch 3334, Loss: 1.047138288617134, Final Batch Loss: 0.23062346875667572\n",
      "Epoch 3335, Loss: 1.2057013511657715, Final Batch Loss: 0.3304652273654938\n",
      "Epoch 3336, Loss: 1.11875419318676, Final Batch Loss: 0.24412943422794342\n",
      "Epoch 3337, Loss: 1.1554322838783264, Final Batch Loss: 0.35600799322128296\n",
      "Epoch 3338, Loss: 1.0841723084449768, Final Batch Loss: 0.25437238812446594\n",
      "Epoch 3339, Loss: 1.1167593449354172, Final Batch Loss: 0.30594602227211\n",
      "Epoch 3340, Loss: 1.05636927485466, Final Batch Loss: 0.21357455849647522\n",
      "Epoch 3341, Loss: 1.0495594441890717, Final Batch Loss: 0.24353550374507904\n",
      "Epoch 3342, Loss: 1.0389933437108994, Final Batch Loss: 0.2742958664894104\n",
      "Epoch 3343, Loss: 1.1092102527618408, Final Batch Loss: 0.24333548545837402\n",
      "Epoch 3344, Loss: 1.2002668380737305, Final Batch Loss: 0.19614621996879578\n",
      "Epoch 3345, Loss: 1.0205043852329254, Final Batch Loss: 0.25106024742126465\n",
      "Epoch 3346, Loss: 1.0944157391786575, Final Batch Loss: 0.2463185340166092\n",
      "Epoch 3347, Loss: 1.2474993616342545, Final Batch Loss: 0.2515559196472168\n",
      "Epoch 3348, Loss: 1.1154437959194183, Final Batch Loss: 0.3085913360118866\n",
      "Epoch 3349, Loss: 1.2446978390216827, Final Batch Loss: 0.3934662640094757\n",
      "Epoch 3350, Loss: 1.0973568111658096, Final Batch Loss: 0.2097615748643875\n",
      "Epoch 3351, Loss: 1.129394680261612, Final Batch Loss: 0.3115690052509308\n",
      "Epoch 3352, Loss: 1.0581348687410355, Final Batch Loss: 0.21156464517116547\n",
      "Epoch 3353, Loss: 1.116908848285675, Final Batch Loss: 0.2846442759037018\n",
      "Epoch 3354, Loss: 1.0853580385446548, Final Batch Loss: 0.24901877343654633\n",
      "Epoch 3355, Loss: 1.0954539477825165, Final Batch Loss: 0.2937707006931305\n",
      "Epoch 3356, Loss: 0.9504774510860443, Final Batch Loss: 0.14822331070899963\n",
      "Epoch 3357, Loss: 1.1179538369178772, Final Batch Loss: 0.3066948354244232\n",
      "Epoch 3358, Loss: 1.053173005580902, Final Batch Loss: 0.1941465437412262\n",
      "Epoch 3359, Loss: 1.1332074403762817, Final Batch Loss: 0.27008411288261414\n",
      "Epoch 3360, Loss: 1.2084080576896667, Final Batch Loss: 0.3143374025821686\n",
      "Epoch 3361, Loss: 1.092437505722046, Final Batch Loss: 0.20606771111488342\n",
      "Epoch 3362, Loss: 1.0504456758499146, Final Batch Loss: 0.20906713604927063\n",
      "Epoch 3363, Loss: 1.1149074733257294, Final Batch Loss: 0.3022347092628479\n",
      "Epoch 3364, Loss: 1.1768163740634918, Final Batch Loss: 0.3061964511871338\n",
      "Epoch 3365, Loss: 1.1203544735908508, Final Batch Loss: 0.25023066997528076\n",
      "Epoch 3366, Loss: 1.0870672017335892, Final Batch Loss: 0.31777483224868774\n",
      "Epoch 3367, Loss: 1.0985119491815567, Final Batch Loss: 0.23898081481456757\n",
      "Epoch 3368, Loss: 1.219067543745041, Final Batch Loss: 0.2716448903083801\n",
      "Epoch 3369, Loss: 1.1131327152252197, Final Batch Loss: 0.23714202642440796\n",
      "Epoch 3370, Loss: 1.1723575294017792, Final Batch Loss: 0.3157762289047241\n",
      "Epoch 3371, Loss: 1.1079033017158508, Final Batch Loss: 0.36784783005714417\n",
      "Epoch 3372, Loss: 1.0191035717725754, Final Batch Loss: 0.21992412209510803\n",
      "Epoch 3373, Loss: 1.1471485197544098, Final Batch Loss: 0.29428255558013916\n",
      "Epoch 3374, Loss: 1.1193920373916626, Final Batch Loss: 0.33011138439178467\n",
      "Epoch 3375, Loss: 1.1637505292892456, Final Batch Loss: 0.2687692940235138\n",
      "Epoch 3376, Loss: 1.0175398141145706, Final Batch Loss: 0.17797507345676422\n",
      "Epoch 3377, Loss: 1.0128077864646912, Final Batch Loss: 0.2932175397872925\n",
      "Epoch 3378, Loss: 1.0957776010036469, Final Batch Loss: 0.24941986799240112\n",
      "Epoch 3379, Loss: 1.1819281727075577, Final Batch Loss: 0.3371024429798126\n",
      "Epoch 3380, Loss: 1.2164612412452698, Final Batch Loss: 0.33242347836494446\n",
      "Epoch 3381, Loss: 1.0747114717960358, Final Batch Loss: 0.23769202828407288\n",
      "Epoch 3382, Loss: 1.1069945842027664, Final Batch Loss: 0.2406490296125412\n",
      "Epoch 3383, Loss: 1.1216086000204086, Final Batch Loss: 0.35392215847969055\n",
      "Epoch 3384, Loss: 1.107466846704483, Final Batch Loss: 0.2764170169830322\n",
      "Epoch 3385, Loss: 1.2163762003183365, Final Batch Loss: 0.39882102608680725\n",
      "Epoch 3386, Loss: 1.167521744966507, Final Batch Loss: 0.2882792055606842\n",
      "Epoch 3387, Loss: 1.0678738951683044, Final Batch Loss: 0.2732338607311249\n",
      "Epoch 3388, Loss: 1.0591293275356293, Final Batch Loss: 0.265805721282959\n",
      "Epoch 3389, Loss: 1.1711929440498352, Final Batch Loss: 0.38729918003082275\n",
      "Epoch 3390, Loss: 1.1111775040626526, Final Batch Loss: 0.21464458107948303\n",
      "Epoch 3391, Loss: 1.042178213596344, Final Batch Loss: 0.21880847215652466\n",
      "Epoch 3392, Loss: 1.1228397190570831, Final Batch Loss: 0.2704119086265564\n",
      "Epoch 3393, Loss: 1.1829843819141388, Final Batch Loss: 0.2777537703514099\n",
      "Epoch 3394, Loss: 1.0349708497524261, Final Batch Loss: 0.29491665959358215\n",
      "Epoch 3395, Loss: 1.077228307723999, Final Batch Loss: 0.2320367991924286\n",
      "Epoch 3396, Loss: 0.9819149821996689, Final Batch Loss: 0.21136820316314697\n",
      "Epoch 3397, Loss: 1.1065049916505814, Final Batch Loss: 0.2901948392391205\n",
      "Epoch 3398, Loss: 1.0484864115715027, Final Batch Loss: 0.2372601330280304\n",
      "Epoch 3399, Loss: 1.0087972581386566, Final Batch Loss: 0.23905664682388306\n",
      "Epoch 3400, Loss: 1.0861044526100159, Final Batch Loss: 0.23663973808288574\n",
      "Epoch 3401, Loss: 1.0876257121562958, Final Batch Loss: 0.25142621994018555\n",
      "Epoch 3402, Loss: 1.1526867747306824, Final Batch Loss: 0.2511361539363861\n",
      "Epoch 3403, Loss: 1.013545647263527, Final Batch Loss: 0.2692059874534607\n",
      "Epoch 3404, Loss: 1.156902328133583, Final Batch Loss: 0.33081725239753723\n",
      "Epoch 3405, Loss: 1.053190141916275, Final Batch Loss: 0.24256908893585205\n",
      "Epoch 3406, Loss: 1.10855133831501, Final Batch Loss: 0.2430456131696701\n",
      "Epoch 3407, Loss: 1.0457815378904343, Final Batch Loss: 0.3483927249908447\n",
      "Epoch 3408, Loss: 1.108544871211052, Final Batch Loss: 0.3641711473464966\n",
      "Epoch 3409, Loss: 1.0691846758127213, Final Batch Loss: 0.30196547508239746\n",
      "Epoch 3410, Loss: 1.1097255498170853, Final Batch Loss: 0.3288969397544861\n",
      "Epoch 3411, Loss: 1.036871612071991, Final Batch Loss: 0.24251435697078705\n",
      "Epoch 3412, Loss: 1.0906052887439728, Final Batch Loss: 0.2231537252664566\n",
      "Epoch 3413, Loss: 1.1792875826358795, Final Batch Loss: 0.3516346216201782\n",
      "Epoch 3414, Loss: 1.1467009037733078, Final Batch Loss: 0.1913788765668869\n",
      "Epoch 3415, Loss: 1.0520398914813995, Final Batch Loss: 0.20871511101722717\n",
      "Epoch 3416, Loss: 1.0750634223222733, Final Batch Loss: 0.32356902956962585\n",
      "Epoch 3417, Loss: 1.0658995807170868, Final Batch Loss: 0.26102957129478455\n",
      "Epoch 3418, Loss: 1.0913971066474915, Final Batch Loss: 0.2650406062602997\n",
      "Epoch 3419, Loss: 1.0024740099906921, Final Batch Loss: 0.22807817161083221\n",
      "Epoch 3420, Loss: 1.1465377807617188, Final Batch Loss: 0.30422940850257874\n",
      "Epoch 3421, Loss: 0.9949566870927811, Final Batch Loss: 0.2817223072052002\n",
      "Epoch 3422, Loss: 1.2009708881378174, Final Batch Loss: 0.3089006841182709\n",
      "Epoch 3423, Loss: 1.1949813514947891, Final Batch Loss: 0.3494475781917572\n",
      "Epoch 3424, Loss: 1.149066835641861, Final Batch Loss: 0.33188387751579285\n",
      "Epoch 3425, Loss: 1.1058370769023895, Final Batch Loss: 0.19938421249389648\n",
      "Epoch 3426, Loss: 1.0841360092163086, Final Batch Loss: 0.2239469289779663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3427, Loss: 1.1016438603401184, Final Batch Loss: 0.3157426416873932\n",
      "Epoch 3428, Loss: 1.1129846274852753, Final Batch Loss: 0.3225160539150238\n",
      "Epoch 3429, Loss: 1.0606845766305923, Final Batch Loss: 0.2708621919155121\n",
      "Epoch 3430, Loss: 1.0265377163887024, Final Batch Loss: 0.26420652866363525\n",
      "Epoch 3431, Loss: 1.0898624658584595, Final Batch Loss: 0.3057025074958801\n",
      "Epoch 3432, Loss: 0.9789748340845108, Final Batch Loss: 0.14658983051776886\n",
      "Epoch 3433, Loss: 1.1635584235191345, Final Batch Loss: 0.2752545177936554\n",
      "Epoch 3434, Loss: 1.0735130608081818, Final Batch Loss: 0.24573929607868195\n",
      "Epoch 3435, Loss: 1.176806926727295, Final Batch Loss: 0.34548306465148926\n",
      "Epoch 3436, Loss: 1.1700950413942337, Final Batch Loss: 0.36326655745506287\n",
      "Epoch 3437, Loss: 1.0786096155643463, Final Batch Loss: 0.2649289667606354\n",
      "Epoch 3438, Loss: 1.1081638485193253, Final Batch Loss: 0.23241624236106873\n",
      "Epoch 3439, Loss: 1.1861573159694672, Final Batch Loss: 0.31512683629989624\n",
      "Epoch 3440, Loss: 1.0987252593040466, Final Batch Loss: 0.3403223156929016\n",
      "Epoch 3441, Loss: 1.0982184708118439, Final Batch Loss: 0.2743005156517029\n",
      "Epoch 3442, Loss: 1.040184035897255, Final Batch Loss: 0.30806800723075867\n",
      "Epoch 3443, Loss: 1.1528139561414719, Final Batch Loss: 0.28321439027786255\n",
      "Epoch 3444, Loss: 1.0925390273332596, Final Batch Loss: 0.3235974907875061\n",
      "Epoch 3445, Loss: 1.0291936993598938, Final Batch Loss: 0.25905394554138184\n",
      "Epoch 3446, Loss: 1.0449838638305664, Final Batch Loss: 0.20131465792655945\n",
      "Epoch 3447, Loss: 1.0651215314865112, Final Batch Loss: 0.2417111098766327\n",
      "Epoch 3448, Loss: 1.0204645991325378, Final Batch Loss: 0.26181092858314514\n",
      "Epoch 3449, Loss: 1.038950800895691, Final Batch Loss: 0.375792533159256\n",
      "Epoch 3450, Loss: 0.9918522983789444, Final Batch Loss: 0.23080956935882568\n",
      "Epoch 3451, Loss: 1.0298332124948502, Final Batch Loss: 0.24858364462852478\n",
      "Epoch 3452, Loss: 1.0767366737127304, Final Batch Loss: 0.34979745745658875\n",
      "Epoch 3453, Loss: 1.1536663174629211, Final Batch Loss: 0.33328908681869507\n",
      "Epoch 3454, Loss: 1.1389005184173584, Final Batch Loss: 0.3277171850204468\n",
      "Epoch 3455, Loss: 1.1652365773916245, Final Batch Loss: 0.30318477749824524\n",
      "Epoch 3456, Loss: 1.2172971069812775, Final Batch Loss: 0.436685711145401\n",
      "Epoch 3457, Loss: 1.0831160098314285, Final Batch Loss: 0.21645568311214447\n",
      "Epoch 3458, Loss: 1.0749986469745636, Final Batch Loss: 0.2827169597148895\n",
      "Epoch 3459, Loss: 1.12503382563591, Final Batch Loss: 0.2974306643009186\n",
      "Epoch 3460, Loss: 1.087120234966278, Final Batch Loss: 0.2565014660358429\n",
      "Epoch 3461, Loss: 1.0272806137800217, Final Batch Loss: 0.27975574135780334\n",
      "Epoch 3462, Loss: 1.020669236779213, Final Batch Loss: 0.1929803341627121\n",
      "Epoch 3463, Loss: 1.0215803533792496, Final Batch Loss: 0.24107201397418976\n",
      "Epoch 3464, Loss: 1.0231265127658844, Final Batch Loss: 0.2750695049762726\n",
      "Epoch 3465, Loss: 1.1385534554719925, Final Batch Loss: 0.2208908200263977\n",
      "Epoch 3466, Loss: 1.1631121635437012, Final Batch Loss: 0.3003041744232178\n",
      "Epoch 3467, Loss: 1.08368119597435, Final Batch Loss: 0.29711976647377014\n",
      "Epoch 3468, Loss: 1.2372238039970398, Final Batch Loss: 0.4001935124397278\n",
      "Epoch 3469, Loss: 1.04644475877285, Final Batch Loss: 0.3094628155231476\n",
      "Epoch 3470, Loss: 0.9274983704090118, Final Batch Loss: 0.2144888937473297\n",
      "Epoch 3471, Loss: 1.0962103605270386, Final Batch Loss: 0.23992064595222473\n",
      "Epoch 3472, Loss: 1.1967613697052002, Final Batch Loss: 0.3431894779205322\n",
      "Epoch 3473, Loss: 1.1376491338014603, Final Batch Loss: 0.3672751486301422\n",
      "Epoch 3474, Loss: 1.0872794389724731, Final Batch Loss: 0.24888980388641357\n",
      "Epoch 3475, Loss: 1.0813789516687393, Final Batch Loss: 0.28565332293510437\n",
      "Epoch 3476, Loss: 1.0049989074468613, Final Batch Loss: 0.24323469400405884\n",
      "Epoch 3477, Loss: 1.1087580025196075, Final Batch Loss: 0.3554932475090027\n",
      "Epoch 3478, Loss: 1.093550905585289, Final Batch Loss: 0.29956474900245667\n",
      "Epoch 3479, Loss: 1.0213581323623657, Final Batch Loss: 0.21946659684181213\n",
      "Epoch 3480, Loss: 1.2117430418729782, Final Batch Loss: 0.3433002233505249\n",
      "Epoch 3481, Loss: 0.9858938753604889, Final Batch Loss: 0.2572624683380127\n",
      "Epoch 3482, Loss: 1.1606923937797546, Final Batch Loss: 0.2530362904071808\n",
      "Epoch 3483, Loss: 1.1244016885757446, Final Batch Loss: 0.2563967704772949\n",
      "Epoch 3484, Loss: 1.2183503955602646, Final Batch Loss: 0.2859779894351959\n",
      "Epoch 3485, Loss: 1.0641444772481918, Final Batch Loss: 0.32120752334594727\n",
      "Epoch 3486, Loss: 1.1397792994976044, Final Batch Loss: 0.3826335370540619\n",
      "Epoch 3487, Loss: 1.1778508126735687, Final Batch Loss: 0.3187713921070099\n",
      "Epoch 3488, Loss: 1.1201883405447006, Final Batch Loss: 0.24378158152103424\n",
      "Epoch 3489, Loss: 1.1964576542377472, Final Batch Loss: 0.3197251260280609\n",
      "Epoch 3490, Loss: 0.9977440237998962, Final Batch Loss: 0.24897967278957367\n",
      "Epoch 3491, Loss: 1.0054730772972107, Final Batch Loss: 0.25613608956336975\n",
      "Epoch 3492, Loss: 1.086458370089531, Final Batch Loss: 0.20878387987613678\n",
      "Epoch 3493, Loss: 1.157200649380684, Final Batch Loss: 0.2281808704137802\n",
      "Epoch 3494, Loss: 1.0738907009363174, Final Batch Loss: 0.21309810876846313\n",
      "Epoch 3495, Loss: 1.0048279911279678, Final Batch Loss: 0.2206120491027832\n",
      "Epoch 3496, Loss: 1.0191370397806168, Final Batch Loss: 0.2131078988313675\n",
      "Epoch 3497, Loss: 1.0189866125583649, Final Batch Loss: 0.2876298427581787\n",
      "Epoch 3498, Loss: 1.0986978709697723, Final Batch Loss: 0.32795250415802\n",
      "Epoch 3499, Loss: 1.1669842004776, Final Batch Loss: 0.27431216835975647\n",
      "Epoch 3500, Loss: 1.085933119058609, Final Batch Loss: 0.2977932095527649\n",
      "Epoch 3501, Loss: 1.1080602258443832, Final Batch Loss: 0.2212960124015808\n",
      "Epoch 3502, Loss: 1.1053867936134338, Final Batch Loss: 0.3257030248641968\n",
      "Epoch 3503, Loss: 1.0832834243774414, Final Batch Loss: 0.1923224925994873\n",
      "Epoch 3504, Loss: 1.1179281175136566, Final Batch Loss: 0.3087708353996277\n",
      "Epoch 3505, Loss: 1.079982414841652, Final Batch Loss: 0.2962581217288971\n",
      "Epoch 3506, Loss: 1.1010998487472534, Final Batch Loss: 0.19832684099674225\n",
      "Epoch 3507, Loss: 1.1032467633485794, Final Batch Loss: 0.27769213914871216\n",
      "Epoch 3508, Loss: 1.0890153646469116, Final Batch Loss: 0.2979379892349243\n",
      "Epoch 3509, Loss: 1.0266706049442291, Final Batch Loss: 0.26061171293258667\n",
      "Epoch 3510, Loss: 1.118754729628563, Final Batch Loss: 0.36523908376693726\n",
      "Epoch 3511, Loss: 1.0567550510168076, Final Batch Loss: 0.2177063375711441\n",
      "Epoch 3512, Loss: 1.2163484394550323, Final Batch Loss: 0.21421730518341064\n",
      "Epoch 3513, Loss: 1.1063736975193024, Final Batch Loss: 0.26590633392333984\n",
      "Epoch 3514, Loss: 1.01919387280941, Final Batch Loss: 0.2023160457611084\n",
      "Epoch 3515, Loss: 1.2397459745407104, Final Batch Loss: 0.3301977217197418\n",
      "Epoch 3516, Loss: 1.077757716178894, Final Batch Loss: 0.27087369561195374\n",
      "Epoch 3517, Loss: 1.1023025214672089, Final Batch Loss: 0.28696590662002563\n",
      "Epoch 3518, Loss: 1.0985781997442245, Final Batch Loss: 0.23311889171600342\n",
      "Epoch 3519, Loss: 1.0928562730550766, Final Batch Loss: 0.3371836543083191\n",
      "Epoch 3520, Loss: 1.1739189624786377, Final Batch Loss: 0.34970763325691223\n",
      "Epoch 3521, Loss: 1.1431675106287003, Final Batch Loss: 0.22836057841777802\n",
      "Epoch 3522, Loss: 1.0296143293380737, Final Batch Loss: 0.280158132314682\n",
      "Epoch 3523, Loss: 1.0785890221595764, Final Batch Loss: 0.29278454184532166\n",
      "Epoch 3524, Loss: 1.1760121285915375, Final Batch Loss: 0.2754974067211151\n",
      "Epoch 3525, Loss: 1.1496209502220154, Final Batch Loss: 0.30302494764328003\n",
      "Epoch 3526, Loss: 1.1970211416482925, Final Batch Loss: 0.3608231842517853\n",
      "Epoch 3527, Loss: 1.0610154122114182, Final Batch Loss: 0.16769535839557648\n",
      "Epoch 3528, Loss: 1.0656177997589111, Final Batch Loss: 0.23569157719612122\n",
      "Epoch 3529, Loss: 1.1976154148578644, Final Batch Loss: 0.24647292494773865\n",
      "Epoch 3530, Loss: 0.9947604835033417, Final Batch Loss: 0.17618907988071442\n",
      "Epoch 3531, Loss: 1.0373947769403458, Final Batch Loss: 0.25389495491981506\n",
      "Epoch 3532, Loss: 1.0185445994138718, Final Batch Loss: 0.29685893654823303\n",
      "Epoch 3533, Loss: 1.1041284203529358, Final Batch Loss: 0.29173174500465393\n",
      "Epoch 3534, Loss: 0.9621946662664413, Final Batch Loss: 0.22979696094989777\n",
      "Epoch 3535, Loss: 0.9349508881568909, Final Batch Loss: 0.14750757813453674\n",
      "Epoch 3536, Loss: 1.1012023985385895, Final Batch Loss: 0.3899695575237274\n",
      "Epoch 3537, Loss: 1.003833845257759, Final Batch Loss: 0.23138031363487244\n",
      "Epoch 3538, Loss: 1.0577611476182938, Final Batch Loss: 0.26874804496765137\n",
      "Epoch 3539, Loss: 1.100887417793274, Final Batch Loss: 0.24258308112621307\n",
      "Epoch 3540, Loss: 0.9964904636144638, Final Batch Loss: 0.25513073801994324\n",
      "Epoch 3541, Loss: 1.0780807733535767, Final Batch Loss: 0.26530227065086365\n",
      "Epoch 3542, Loss: 1.0396234840154648, Final Batch Loss: 0.2615007162094116\n",
      "Epoch 3543, Loss: 1.061094492673874, Final Batch Loss: 0.26953375339508057\n",
      "Epoch 3544, Loss: 1.1591520607471466, Final Batch Loss: 0.19699615240097046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3545, Loss: 1.0024051368236542, Final Batch Loss: 0.32302719354629517\n",
      "Epoch 3546, Loss: 1.2395184189081192, Final Batch Loss: 0.35775628685951233\n",
      "Epoch 3547, Loss: 1.0113338232040405, Final Batch Loss: 0.18392574787139893\n",
      "Epoch 3548, Loss: 1.1709091663360596, Final Batch Loss: 0.2990760803222656\n",
      "Epoch 3549, Loss: 1.2012638747692108, Final Batch Loss: 0.3377079963684082\n",
      "Epoch 3550, Loss: 1.1166646778583527, Final Batch Loss: 0.30318471789360046\n",
      "Epoch 3551, Loss: 1.0306184887886047, Final Batch Loss: 0.23933175206184387\n",
      "Epoch 3552, Loss: 1.1358164250850677, Final Batch Loss: 0.2926192581653595\n",
      "Epoch 3553, Loss: 1.0710986852645874, Final Batch Loss: 0.21655738353729248\n",
      "Epoch 3554, Loss: 1.0487194061279297, Final Batch Loss: 0.2633325457572937\n",
      "Epoch 3555, Loss: 1.023937612771988, Final Batch Loss: 0.2389848381280899\n",
      "Epoch 3556, Loss: 1.0876623094081879, Final Batch Loss: 0.25705793499946594\n",
      "Epoch 3557, Loss: 1.0340108126401901, Final Batch Loss: 0.21759484708309174\n",
      "Epoch 3558, Loss: 1.1747067719697952, Final Batch Loss: 0.22428344190120697\n",
      "Epoch 3559, Loss: 1.092763975262642, Final Batch Loss: 0.3367465138435364\n",
      "Epoch 3560, Loss: 1.109547272324562, Final Batch Loss: 0.30931201577186584\n",
      "Epoch 3561, Loss: 1.0965321063995361, Final Batch Loss: 0.25425273180007935\n",
      "Epoch 3562, Loss: 1.060321405529976, Final Batch Loss: 0.22040022909641266\n",
      "Epoch 3563, Loss: 1.0035146325826645, Final Batch Loss: 0.21594515442848206\n",
      "Epoch 3564, Loss: 1.0027730911970139, Final Batch Loss: 0.21450649201869965\n",
      "Epoch 3565, Loss: 1.016850695014, Final Batch Loss: 0.22129833698272705\n",
      "Epoch 3566, Loss: 1.05566044151783, Final Batch Loss: 0.23438744246959686\n",
      "Epoch 3567, Loss: 1.1243501901626587, Final Batch Loss: 0.3208428621292114\n",
      "Epoch 3568, Loss: 1.0230361223220825, Final Batch Loss: 0.23681049048900604\n",
      "Epoch 3569, Loss: 1.2914939224720001, Final Batch Loss: 0.43068060278892517\n",
      "Epoch 3570, Loss: 1.0070905983448029, Final Batch Loss: 0.19409871101379395\n",
      "Epoch 3571, Loss: 0.9725450575351715, Final Batch Loss: 0.2061479091644287\n",
      "Epoch 3572, Loss: 0.9897098392248154, Final Batch Loss: 0.292222261428833\n",
      "Epoch 3573, Loss: 0.9539077430963516, Final Batch Loss: 0.22801083326339722\n",
      "Epoch 3574, Loss: 0.9364426732063293, Final Batch Loss: 0.2596299648284912\n",
      "Epoch 3575, Loss: 1.0505587458610535, Final Batch Loss: 0.2606844902038574\n",
      "Epoch 3576, Loss: 0.9621667116880417, Final Batch Loss: 0.19169291853904724\n",
      "Epoch 3577, Loss: 1.1488109230995178, Final Batch Loss: 0.3125614821910858\n",
      "Epoch 3578, Loss: 1.073703110218048, Final Batch Loss: 0.3101741373538971\n",
      "Epoch 3579, Loss: 1.0141195952892303, Final Batch Loss: 0.25204744935035706\n",
      "Epoch 3580, Loss: 1.062462955713272, Final Batch Loss: 0.2786579728126526\n",
      "Epoch 3581, Loss: 1.0795169323682785, Final Batch Loss: 0.25331535935401917\n",
      "Epoch 3582, Loss: 1.1576505303382874, Final Batch Loss: 0.4261527955532074\n",
      "Epoch 3583, Loss: 1.1448869705200195, Final Batch Loss: 0.27319538593292236\n",
      "Epoch 3584, Loss: 1.028534471988678, Final Batch Loss: 0.26971426606178284\n",
      "Epoch 3585, Loss: 1.0638288259506226, Final Batch Loss: 0.22787916660308838\n",
      "Epoch 3586, Loss: 1.2339729070663452, Final Batch Loss: 0.32587605714797974\n",
      "Epoch 3587, Loss: 1.0385936051607132, Final Batch Loss: 0.33195072412490845\n",
      "Epoch 3588, Loss: 1.1223860681056976, Final Batch Loss: 0.2690197229385376\n",
      "Epoch 3589, Loss: 1.1700020283460617, Final Batch Loss: 0.3084646761417389\n",
      "Epoch 3590, Loss: 1.0855406373739243, Final Batch Loss: 0.30777686834335327\n",
      "Epoch 3591, Loss: 1.0696325898170471, Final Batch Loss: 0.22005248069763184\n",
      "Epoch 3592, Loss: 1.0702060014009476, Final Batch Loss: 0.21334758400917053\n",
      "Epoch 3593, Loss: 1.0974970012903214, Final Batch Loss: 0.32236653566360474\n",
      "Epoch 3594, Loss: 1.0046780854463577, Final Batch Loss: 0.23911075294017792\n",
      "Epoch 3595, Loss: 1.114934355020523, Final Batch Loss: 0.2702174186706543\n",
      "Epoch 3596, Loss: 1.0044979602098465, Final Batch Loss: 0.23025865852832794\n",
      "Epoch 3597, Loss: 1.0736603140830994, Final Batch Loss: 0.2607807517051697\n",
      "Epoch 3598, Loss: 0.9903667569160461, Final Batch Loss: 0.21684172749519348\n",
      "Epoch 3599, Loss: 1.0610934495925903, Final Batch Loss: 0.24247866868972778\n",
      "Epoch 3600, Loss: 0.9647363871335983, Final Batch Loss: 0.26709672808647156\n",
      "Epoch 3601, Loss: 1.0424314588308334, Final Batch Loss: 0.18003533780574799\n",
      "Epoch 3602, Loss: 1.0870320200920105, Final Batch Loss: 0.3001433312892914\n",
      "Epoch 3603, Loss: 0.9514670968055725, Final Batch Loss: 0.2044857293367386\n",
      "Epoch 3604, Loss: 0.9684058725833893, Final Batch Loss: 0.20083265006542206\n",
      "Epoch 3605, Loss: 1.098393663764, Final Batch Loss: 0.3721674084663391\n",
      "Epoch 3606, Loss: 1.0669565051794052, Final Batch Loss: 0.2589294910430908\n",
      "Epoch 3607, Loss: 1.0490427762269974, Final Batch Loss: 0.3226926624774933\n",
      "Epoch 3608, Loss: 1.129762440919876, Final Batch Loss: 0.25536128878593445\n",
      "Epoch 3609, Loss: 1.1412697434425354, Final Batch Loss: 0.2992704212665558\n",
      "Epoch 3610, Loss: 1.1190140545368195, Final Batch Loss: 0.34560754895210266\n",
      "Epoch 3611, Loss: 1.240934118628502, Final Batch Loss: 0.35493624210357666\n",
      "Epoch 3612, Loss: 1.0842758119106293, Final Batch Loss: 0.2829640507698059\n",
      "Epoch 3613, Loss: 1.140664130449295, Final Batch Loss: 0.30433306097984314\n",
      "Epoch 3614, Loss: 1.1447462737560272, Final Batch Loss: 0.2992313802242279\n",
      "Epoch 3615, Loss: 1.0530454963445663, Final Batch Loss: 0.32044482231140137\n",
      "Epoch 3616, Loss: 1.0090074241161346, Final Batch Loss: 0.2271047681570053\n",
      "Epoch 3617, Loss: 1.0901744663715363, Final Batch Loss: 0.2160247266292572\n",
      "Epoch 3618, Loss: 1.0739321410655975, Final Batch Loss: 0.3210665285587311\n",
      "Epoch 3619, Loss: 1.0754474997520447, Final Batch Loss: 0.19091808795928955\n",
      "Epoch 3620, Loss: 1.0195524096488953, Final Batch Loss: 0.2546748220920563\n",
      "Epoch 3621, Loss: 1.050081491470337, Final Batch Loss: 0.2294369786977768\n",
      "Epoch 3622, Loss: 1.0722906440496445, Final Batch Loss: 0.24847814440727234\n",
      "Epoch 3623, Loss: 1.1078659445047379, Final Batch Loss: 0.2551835775375366\n",
      "Epoch 3624, Loss: 0.9500713646411896, Final Batch Loss: 0.2256097048521042\n",
      "Epoch 3625, Loss: 0.9910628497600555, Final Batch Loss: 0.25648000836372375\n",
      "Epoch 3626, Loss: 1.1327620148658752, Final Batch Loss: 0.25523871183395386\n",
      "Epoch 3627, Loss: 1.0694706290960312, Final Batch Loss: 0.29415568709373474\n",
      "Epoch 3628, Loss: 1.1128655225038528, Final Batch Loss: 0.26480337977409363\n",
      "Epoch 3629, Loss: 1.050088033080101, Final Batch Loss: 0.2636373043060303\n",
      "Epoch 3630, Loss: 1.030677616596222, Final Batch Loss: 0.21523143351078033\n",
      "Epoch 3631, Loss: 1.1847855150699615, Final Batch Loss: 0.3968335688114166\n",
      "Epoch 3632, Loss: 1.082993045449257, Final Batch Loss: 0.38366615772247314\n",
      "Epoch 3633, Loss: 1.2639405727386475, Final Batch Loss: 0.31814929842948914\n",
      "Epoch 3634, Loss: 0.9929232001304626, Final Batch Loss: 0.2229936271905899\n",
      "Epoch 3635, Loss: 1.0392645597457886, Final Batch Loss: 0.22631989419460297\n",
      "Epoch 3636, Loss: 1.064742162823677, Final Batch Loss: 0.3661230802536011\n",
      "Epoch 3637, Loss: 0.9936475455760956, Final Batch Loss: 0.2326948344707489\n",
      "Epoch 3638, Loss: 1.0718765556812286, Final Batch Loss: 0.28599148988723755\n",
      "Epoch 3639, Loss: 1.09769806265831, Final Batch Loss: 0.33534178137779236\n",
      "Epoch 3640, Loss: 0.9312357604503632, Final Batch Loss: 0.23474791646003723\n",
      "Epoch 3641, Loss: 1.0541272163391113, Final Batch Loss: 0.22205550968647003\n",
      "Epoch 3642, Loss: 1.229853630065918, Final Batch Loss: 0.262523353099823\n",
      "Epoch 3643, Loss: 0.9921426475048065, Final Batch Loss: 0.3271932005882263\n",
      "Epoch 3644, Loss: 1.0001121014356613, Final Batch Loss: 0.20679597556591034\n",
      "Epoch 3645, Loss: 1.1645210087299347, Final Batch Loss: 0.27299782633781433\n",
      "Epoch 3646, Loss: 1.050288364291191, Final Batch Loss: 0.1916293054819107\n",
      "Epoch 3647, Loss: 1.0643293261528015, Final Batch Loss: 0.2722448408603668\n",
      "Epoch 3648, Loss: 1.1289322078227997, Final Batch Loss: 0.31686192750930786\n",
      "Epoch 3649, Loss: 0.9864659607410431, Final Batch Loss: 0.17209258675575256\n",
      "Epoch 3650, Loss: 1.1318563371896744, Final Batch Loss: 0.3045084774494171\n",
      "Epoch 3651, Loss: 1.0460129082202911, Final Batch Loss: 0.254652202129364\n",
      "Epoch 3652, Loss: 1.0891408622264862, Final Batch Loss: 0.3379659056663513\n",
      "Epoch 3653, Loss: 1.1056633740663528, Final Batch Loss: 0.2931845188140869\n",
      "Epoch 3654, Loss: 1.0907454937696457, Final Batch Loss: 0.31871411204338074\n",
      "Epoch 3655, Loss: 1.084998294711113, Final Batch Loss: 0.2631039321422577\n",
      "Epoch 3656, Loss: 1.1367378234863281, Final Batch Loss: 0.3109743893146515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3657, Loss: 1.137623518705368, Final Batch Loss: 0.30358684062957764\n",
      "Epoch 3658, Loss: 1.1721897423267365, Final Batch Loss: 0.2673308849334717\n",
      "Epoch 3659, Loss: 1.0954648852348328, Final Batch Loss: 0.3807201087474823\n",
      "Epoch 3660, Loss: 1.0644308477640152, Final Batch Loss: 0.29807624220848083\n",
      "Epoch 3661, Loss: 1.0641869902610779, Final Batch Loss: 0.2776520252227783\n",
      "Epoch 3662, Loss: 1.0867584496736526, Final Batch Loss: 0.22407881915569305\n",
      "Epoch 3663, Loss: 1.081576332449913, Final Batch Loss: 0.23779307305812836\n",
      "Epoch 3664, Loss: 1.095547303557396, Final Batch Loss: 0.22246594727039337\n",
      "Epoch 3665, Loss: 1.0129302442073822, Final Batch Loss: 0.24642187356948853\n",
      "Epoch 3666, Loss: 1.1366668045520782, Final Batch Loss: 0.2881596088409424\n",
      "Epoch 3667, Loss: 1.0298825204372406, Final Batch Loss: 0.27627283334732056\n",
      "Epoch 3668, Loss: 1.1259610652923584, Final Batch Loss: 0.2830238342285156\n",
      "Epoch 3669, Loss: 0.9902164936065674, Final Batch Loss: 0.19179843366146088\n",
      "Epoch 3670, Loss: 1.0865557342767715, Final Batch Loss: 0.19294311106204987\n",
      "Epoch 3671, Loss: 1.2154863476753235, Final Batch Loss: 0.3334887623786926\n",
      "Epoch 3672, Loss: 1.0508503764867783, Final Batch Loss: 0.27808380126953125\n",
      "Epoch 3673, Loss: 1.1147285103797913, Final Batch Loss: 0.22249671816825867\n",
      "Epoch 3674, Loss: 1.0621682405471802, Final Batch Loss: 0.3035181164741516\n",
      "Epoch 3675, Loss: 1.0468306243419647, Final Batch Loss: 0.30729302763938904\n",
      "Epoch 3676, Loss: 0.9875913709402084, Final Batch Loss: 0.2580355107784271\n",
      "Epoch 3677, Loss: 1.0679346024990082, Final Batch Loss: 0.22712795436382294\n",
      "Epoch 3678, Loss: 1.0851501673460007, Final Batch Loss: 0.20043696463108063\n",
      "Epoch 3679, Loss: 1.102872759103775, Final Batch Loss: 0.3029339015483856\n",
      "Epoch 3680, Loss: 1.0233068019151688, Final Batch Loss: 0.18747587502002716\n",
      "Epoch 3681, Loss: 1.025043562054634, Final Batch Loss: 0.19317768514156342\n",
      "Epoch 3682, Loss: 1.0984665602445602, Final Batch Loss: 0.31759122014045715\n",
      "Epoch 3683, Loss: 1.0175929218530655, Final Batch Loss: 0.34863513708114624\n",
      "Epoch 3684, Loss: 1.1432842910289764, Final Batch Loss: 0.4047850966453552\n",
      "Epoch 3685, Loss: 1.1237985789775848, Final Batch Loss: 0.28777989745140076\n",
      "Epoch 3686, Loss: 1.0958053469657898, Final Batch Loss: 0.32313570380210876\n",
      "Epoch 3687, Loss: 0.9634395986795425, Final Batch Loss: 0.21020494401454926\n",
      "Epoch 3688, Loss: 1.054746687412262, Final Batch Loss: 0.2179972529411316\n",
      "Epoch 3689, Loss: 0.9797632098197937, Final Batch Loss: 0.24813659489154816\n",
      "Epoch 3690, Loss: 0.9511242210865021, Final Batch Loss: 0.1714847832918167\n",
      "Epoch 3691, Loss: 1.0702650845050812, Final Batch Loss: 0.2705693244934082\n",
      "Epoch 3692, Loss: 1.0963532626628876, Final Batch Loss: 0.21575847268104553\n",
      "Epoch 3693, Loss: 1.003616452217102, Final Batch Loss: 0.26788994669914246\n",
      "Epoch 3694, Loss: 1.0501573234796524, Final Batch Loss: 0.3698093891143799\n",
      "Epoch 3695, Loss: 1.0140375196933746, Final Batch Loss: 0.2808864712715149\n",
      "Epoch 3696, Loss: 1.0534131526947021, Final Batch Loss: 0.27713871002197266\n",
      "Epoch 3697, Loss: 1.04726442694664, Final Batch Loss: 0.2759253680706024\n",
      "Epoch 3698, Loss: 1.0803959369659424, Final Batch Loss: 0.23204874992370605\n",
      "Epoch 3699, Loss: 1.0918991565704346, Final Batch Loss: 0.2928759455680847\n",
      "Epoch 3700, Loss: 1.0509730726480484, Final Batch Loss: 0.22295759618282318\n",
      "Epoch 3701, Loss: 0.9797869771718979, Final Batch Loss: 0.286952406167984\n",
      "Epoch 3702, Loss: 1.076034426689148, Final Batch Loss: 0.29427316784858704\n",
      "Epoch 3703, Loss: 1.143278330564499, Final Batch Loss: 0.3000423014163971\n",
      "Epoch 3704, Loss: 1.1604975908994675, Final Batch Loss: 0.3596512973308563\n",
      "Epoch 3705, Loss: 1.0963294059038162, Final Batch Loss: 0.2924095690250397\n",
      "Epoch 3706, Loss: 1.1061853468418121, Final Batch Loss: 0.21798814833164215\n",
      "Epoch 3707, Loss: 1.1585613042116165, Final Batch Loss: 0.21521227061748505\n",
      "Epoch 3708, Loss: 1.1138646453619003, Final Batch Loss: 0.3793257772922516\n",
      "Epoch 3709, Loss: 1.1631770133972168, Final Batch Loss: 0.3498697578907013\n",
      "Epoch 3710, Loss: 1.0886635184288025, Final Batch Loss: 0.29611706733703613\n",
      "Epoch 3711, Loss: 1.0826772302389145, Final Batch Loss: 0.18865032494068146\n",
      "Epoch 3712, Loss: 0.9882614761590958, Final Batch Loss: 0.19096402823925018\n",
      "Epoch 3713, Loss: 1.0045906603336334, Final Batch Loss: 0.2230786383152008\n",
      "Epoch 3714, Loss: 1.0673689991235733, Final Batch Loss: 0.33876726031303406\n",
      "Epoch 3715, Loss: 1.0581557303667068, Final Batch Loss: 0.2373487800359726\n",
      "Epoch 3716, Loss: 1.0537475496530533, Final Batch Loss: 0.3070358633995056\n",
      "Epoch 3717, Loss: 1.000624194741249, Final Batch Loss: 0.26992738246917725\n",
      "Epoch 3718, Loss: 0.9966690391302109, Final Batch Loss: 0.21882110834121704\n",
      "Epoch 3719, Loss: 1.1843374818563461, Final Batch Loss: 0.32950669527053833\n",
      "Epoch 3720, Loss: 1.1409501433372498, Final Batch Loss: 0.23152002692222595\n",
      "Epoch 3721, Loss: 1.0710801631212234, Final Batch Loss: 0.2675614356994629\n",
      "Epoch 3722, Loss: 0.976925864815712, Final Batch Loss: 0.2990827262401581\n",
      "Epoch 3723, Loss: 1.062800094485283, Final Batch Loss: 0.24184976518154144\n",
      "Epoch 3724, Loss: 1.05843186378479, Final Batch Loss: 0.2207556962966919\n",
      "Epoch 3725, Loss: 1.1215051114559174, Final Batch Loss: 0.2563115358352661\n",
      "Epoch 3726, Loss: 1.0662051141262054, Final Batch Loss: 0.27893298864364624\n",
      "Epoch 3727, Loss: 1.1088764667510986, Final Batch Loss: 0.25870662927627563\n",
      "Epoch 3728, Loss: 1.1726837009191513, Final Batch Loss: 0.2417101114988327\n",
      "Epoch 3729, Loss: 1.0143266320228577, Final Batch Loss: 0.23022355139255524\n",
      "Epoch 3730, Loss: 1.0880523473024368, Final Batch Loss: 0.2555490732192993\n",
      "Epoch 3731, Loss: 1.0852345824241638, Final Batch Loss: 0.2664090096950531\n",
      "Epoch 3732, Loss: 1.1415015310049057, Final Batch Loss: 0.2628282606601715\n",
      "Epoch 3733, Loss: 1.2163790613412857, Final Batch Loss: 0.41010990738868713\n",
      "Epoch 3734, Loss: 1.1089603751897812, Final Batch Loss: 0.33316195011138916\n",
      "Epoch 3735, Loss: 1.0469226092100143, Final Batch Loss: 0.28792262077331543\n",
      "Epoch 3736, Loss: 1.085868015885353, Final Batch Loss: 0.30598607659339905\n",
      "Epoch 3737, Loss: 1.052507534623146, Final Batch Loss: 0.2130872905254364\n",
      "Epoch 3738, Loss: 0.998434916138649, Final Batch Loss: 0.25298041105270386\n",
      "Epoch 3739, Loss: 1.1134914606809616, Final Batch Loss: 0.3294883370399475\n",
      "Epoch 3740, Loss: 0.987856537103653, Final Batch Loss: 0.2676766812801361\n",
      "Epoch 3741, Loss: 1.1556806415319443, Final Batch Loss: 0.24085097014904022\n",
      "Epoch 3742, Loss: 0.9077529907226562, Final Batch Loss: 0.22366562485694885\n",
      "Epoch 3743, Loss: 1.0007276684045792, Final Batch Loss: 0.24289430677890778\n",
      "Epoch 3744, Loss: 1.1376197040081024, Final Batch Loss: 0.26712408661842346\n",
      "Epoch 3745, Loss: 1.0773108452558517, Final Batch Loss: 0.2779900133609772\n",
      "Epoch 3746, Loss: 1.094478279352188, Final Batch Loss: 0.3108326494693756\n",
      "Epoch 3747, Loss: 1.037124440073967, Final Batch Loss: 0.28353363275527954\n",
      "Epoch 3748, Loss: 0.8688342720270157, Final Batch Loss: 0.21337921917438507\n",
      "Epoch 3749, Loss: 1.0676447600126266, Final Batch Loss: 0.2528633177280426\n",
      "Epoch 3750, Loss: 1.1792748868465424, Final Batch Loss: 0.24815790355205536\n",
      "Epoch 3751, Loss: 1.085637629032135, Final Batch Loss: 0.23303735256195068\n",
      "Epoch 3752, Loss: 1.2099394500255585, Final Batch Loss: 0.3126088082790375\n",
      "Epoch 3753, Loss: 1.0205102562904358, Final Batch Loss: 0.23354798555374146\n",
      "Epoch 3754, Loss: 1.10223089158535, Final Batch Loss: 0.21727414429187775\n",
      "Epoch 3755, Loss: 1.127944678068161, Final Batch Loss: 0.3239768147468567\n",
      "Epoch 3756, Loss: 1.096825823187828, Final Batch Loss: 0.26963603496551514\n",
      "Epoch 3757, Loss: 1.117474988102913, Final Batch Loss: 0.3103453814983368\n",
      "Epoch 3758, Loss: 1.1704782098531723, Final Batch Loss: 0.24742840230464935\n",
      "Epoch 3759, Loss: 1.0961077809333801, Final Batch Loss: 0.28130313754081726\n",
      "Epoch 3760, Loss: 1.1126803159713745, Final Batch Loss: 0.32048267126083374\n",
      "Epoch 3761, Loss: 0.9168599545955658, Final Batch Loss: 0.1780877411365509\n",
      "Epoch 3762, Loss: 1.0223730653524399, Final Batch Loss: 0.24183203279972076\n",
      "Epoch 3763, Loss: 1.01374751329422, Final Batch Loss: 0.24480149149894714\n",
      "Epoch 3764, Loss: 0.9418353289365768, Final Batch Loss: 0.16865064203739166\n",
      "Epoch 3765, Loss: 1.0547604113817215, Final Batch Loss: 0.2671961486339569\n",
      "Epoch 3766, Loss: 1.1364123970270157, Final Batch Loss: 0.282772034406662\n",
      "Epoch 3767, Loss: 1.0353973954916, Final Batch Loss: 0.2949349582195282\n",
      "Epoch 3768, Loss: 0.992726132273674, Final Batch Loss: 0.2800152003765106\n",
      "Epoch 3769, Loss: 0.988051638007164, Final Batch Loss: 0.20823311805725098\n",
      "Epoch 3770, Loss: 0.9854110032320023, Final Batch Loss: 0.21214456856250763\n",
      "Epoch 3771, Loss: 0.9728231579065323, Final Batch Loss: 0.14818722009658813\n",
      "Epoch 3772, Loss: 1.0328475683927536, Final Batch Loss: 0.18729332089424133\n",
      "Epoch 3773, Loss: 0.9356684982776642, Final Batch Loss: 0.19753865897655487\n",
      "Epoch 3774, Loss: 1.1443331390619278, Final Batch Loss: 0.39194759726524353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3775, Loss: 0.9191219061613083, Final Batch Loss: 0.23438280820846558\n",
      "Epoch 3776, Loss: 1.1755220741033554, Final Batch Loss: 0.28949224948883057\n",
      "Epoch 3777, Loss: 1.0948124825954437, Final Batch Loss: 0.28390106558799744\n",
      "Epoch 3778, Loss: 1.0280125886201859, Final Batch Loss: 0.23677688837051392\n",
      "Epoch 3779, Loss: 1.0092922747135162, Final Batch Loss: 0.33250612020492554\n",
      "Epoch 3780, Loss: 1.0515528321266174, Final Batch Loss: 0.23642827570438385\n",
      "Epoch 3781, Loss: 1.0139630138874054, Final Batch Loss: 0.28297728300094604\n",
      "Epoch 3782, Loss: 1.2203731536865234, Final Batch Loss: 0.3111184239387512\n",
      "Epoch 3783, Loss: 1.0032195001840591, Final Batch Loss: 0.2317911684513092\n",
      "Epoch 3784, Loss: 1.0117895305156708, Final Batch Loss: 0.20202621817588806\n",
      "Epoch 3785, Loss: 1.1886841654777527, Final Batch Loss: 0.3332884907722473\n",
      "Epoch 3786, Loss: 1.1074234694242477, Final Batch Loss: 0.23320455849170685\n",
      "Epoch 3787, Loss: 1.0067142099142075, Final Batch Loss: 0.1812068670988083\n",
      "Epoch 3788, Loss: 0.9232605546712875, Final Batch Loss: 0.1602429747581482\n",
      "Epoch 3789, Loss: 0.9784878045320511, Final Batch Loss: 0.23950353264808655\n",
      "Epoch 3790, Loss: 1.0459338128566742, Final Batch Loss: 0.21655915677547455\n",
      "Epoch 3791, Loss: 1.0457006245851517, Final Batch Loss: 0.256419837474823\n",
      "Epoch 3792, Loss: 1.0884712040424347, Final Batch Loss: 0.32556480169296265\n",
      "Epoch 3793, Loss: 0.8639355599880219, Final Batch Loss: 0.19153684377670288\n",
      "Epoch 3794, Loss: 1.0501327812671661, Final Batch Loss: 0.2585580348968506\n",
      "Epoch 3795, Loss: 1.072578102350235, Final Batch Loss: 0.170762300491333\n",
      "Epoch 3796, Loss: 1.0702261626720428, Final Batch Loss: 0.22881639003753662\n",
      "Epoch 3797, Loss: 1.137790635228157, Final Batch Loss: 0.3199739158153534\n",
      "Epoch 3798, Loss: 0.9956077039241791, Final Batch Loss: 0.24385422468185425\n",
      "Epoch 3799, Loss: 1.1424212604761124, Final Batch Loss: 0.2321331948041916\n",
      "Epoch 3800, Loss: 1.0146599411964417, Final Batch Loss: 0.2243354767560959\n",
      "Epoch 3801, Loss: 1.120360180735588, Final Batch Loss: 0.28252747654914856\n",
      "Epoch 3802, Loss: 1.0405148416757584, Final Batch Loss: 0.2568054497241974\n",
      "Epoch 3803, Loss: 1.037063479423523, Final Batch Loss: 0.2737405598163605\n",
      "Epoch 3804, Loss: 0.9769323468208313, Final Batch Loss: 0.23821315169334412\n",
      "Epoch 3805, Loss: 1.0063183903694153, Final Batch Loss: 0.24365760385990143\n",
      "Epoch 3806, Loss: 0.8676199465990067, Final Batch Loss: 0.1691187173128128\n",
      "Epoch 3807, Loss: 1.0343075096607208, Final Batch Loss: 0.22754478454589844\n",
      "Epoch 3808, Loss: 1.0978110581636429, Final Batch Loss: 0.2589704692363739\n",
      "Epoch 3809, Loss: 1.091027244925499, Final Batch Loss: 0.2880759835243225\n",
      "Epoch 3810, Loss: 1.082482486963272, Final Batch Loss: 0.23271450400352478\n",
      "Epoch 3811, Loss: 0.9041145294904709, Final Batch Loss: 0.1735721230506897\n",
      "Epoch 3812, Loss: 1.0807464867830276, Final Batch Loss: 0.2678777277469635\n",
      "Epoch 3813, Loss: 0.9506966769695282, Final Batch Loss: 0.21916314959526062\n",
      "Epoch 3814, Loss: 0.9473915696144104, Final Batch Loss: 0.28101059794425964\n",
      "Epoch 3815, Loss: 1.077859714627266, Final Batch Loss: 0.3388676047325134\n",
      "Epoch 3816, Loss: 0.9650546312332153, Final Batch Loss: 0.21964815258979797\n",
      "Epoch 3817, Loss: 1.0684515237808228, Final Batch Loss: 0.3020493984222412\n",
      "Epoch 3818, Loss: 1.0331962555646896, Final Batch Loss: 0.31262844800949097\n",
      "Epoch 3819, Loss: 0.9921749532222748, Final Batch Loss: 0.2457870990037918\n",
      "Epoch 3820, Loss: 1.08505479991436, Final Batch Loss: 0.25027573108673096\n",
      "Epoch 3821, Loss: 0.9994383305311203, Final Batch Loss: 0.2535586953163147\n",
      "Epoch 3822, Loss: 1.0102009624242783, Final Batch Loss: 0.19516156613826752\n",
      "Epoch 3823, Loss: 1.1797460168600082, Final Batch Loss: 0.31456810235977173\n",
      "Epoch 3824, Loss: 1.0839843451976776, Final Batch Loss: 0.2907319664955139\n",
      "Epoch 3825, Loss: 1.0834998786449432, Final Batch Loss: 0.2617025673389435\n",
      "Epoch 3826, Loss: 1.0590583384037018, Final Batch Loss: 0.3455173373222351\n",
      "Epoch 3827, Loss: 1.132134109735489, Final Batch Loss: 0.2781319320201874\n",
      "Epoch 3828, Loss: 1.0454694628715515, Final Batch Loss: 0.27413246035575867\n",
      "Epoch 3829, Loss: 1.0412468165159225, Final Batch Loss: 0.2585374414920807\n",
      "Epoch 3830, Loss: 1.1147732138633728, Final Batch Loss: 0.25463762879371643\n",
      "Epoch 3831, Loss: 1.0753594934940338, Final Batch Loss: 0.2868255078792572\n",
      "Epoch 3832, Loss: 0.97684745490551, Final Batch Loss: 0.2847273051738739\n",
      "Epoch 3833, Loss: 0.9646652638912201, Final Batch Loss: 0.1970973014831543\n",
      "Epoch 3834, Loss: 1.1037588864564896, Final Batch Loss: 0.2606976628303528\n",
      "Epoch 3835, Loss: 1.1096654832363129, Final Batch Loss: 0.2767084240913391\n",
      "Epoch 3836, Loss: 1.045219972729683, Final Batch Loss: 0.2412729561328888\n",
      "Epoch 3837, Loss: 0.9479753971099854, Final Batch Loss: 0.14937074482440948\n",
      "Epoch 3838, Loss: 1.0099865049123764, Final Batch Loss: 0.2391018271446228\n",
      "Epoch 3839, Loss: 0.958285927772522, Final Batch Loss: 0.24267171323299408\n",
      "Epoch 3840, Loss: 0.9676886945962906, Final Batch Loss: 0.2620103359222412\n",
      "Epoch 3841, Loss: 1.023568868637085, Final Batch Loss: 0.2814251780509949\n",
      "Epoch 3842, Loss: 1.0517302453517914, Final Batch Loss: 0.21608099341392517\n",
      "Epoch 3843, Loss: 0.9822442829608917, Final Batch Loss: 0.25926458835601807\n",
      "Epoch 3844, Loss: 1.0713379234075546, Final Batch Loss: 0.2502872347831726\n",
      "Epoch 3845, Loss: 1.1123453974723816, Final Batch Loss: 0.30552729964256287\n",
      "Epoch 3846, Loss: 1.0366086810827255, Final Batch Loss: 0.2121705859899521\n",
      "Epoch 3847, Loss: 1.0097718834877014, Final Batch Loss: 0.3111426532268524\n",
      "Epoch 3848, Loss: 1.1728620678186417, Final Batch Loss: 0.4095618724822998\n",
      "Epoch 3849, Loss: 0.9939799457788467, Final Batch Loss: 0.2602275609970093\n",
      "Epoch 3850, Loss: 1.0256552398204803, Final Batch Loss: 0.22454829514026642\n",
      "Epoch 3851, Loss: 1.1031482517719269, Final Batch Loss: 0.2593901753425598\n",
      "Epoch 3852, Loss: 1.0282112807035446, Final Batch Loss: 0.20310895144939423\n",
      "Epoch 3853, Loss: 1.0859380066394806, Final Batch Loss: 0.3585239350795746\n",
      "Epoch 3854, Loss: 1.1177581995725632, Final Batch Loss: 0.3308178186416626\n",
      "Epoch 3855, Loss: 1.1219381839036942, Final Batch Loss: 0.31367480754852295\n",
      "Epoch 3856, Loss: 1.0870799273252487, Final Batch Loss: 0.29263168573379517\n",
      "Epoch 3857, Loss: 1.0339284092187881, Final Batch Loss: 0.20065608620643616\n",
      "Epoch 3858, Loss: 1.1728064119815826, Final Batch Loss: 0.307661235332489\n",
      "Epoch 3859, Loss: 1.0841868817806244, Final Batch Loss: 0.2842857837677002\n",
      "Epoch 3860, Loss: 0.935840293765068, Final Batch Loss: 0.20709970593452454\n",
      "Epoch 3861, Loss: 1.092497542500496, Final Batch Loss: 0.24873261153697968\n",
      "Epoch 3862, Loss: 1.0961281806230545, Final Batch Loss: 0.2676832675933838\n",
      "Epoch 3863, Loss: 0.9607472866773605, Final Batch Loss: 0.17954407632350922\n",
      "Epoch 3864, Loss: 1.0071194469928741, Final Batch Loss: 0.26637929677963257\n",
      "Epoch 3865, Loss: 1.0657680332660675, Final Batch Loss: 0.2551838159561157\n",
      "Epoch 3866, Loss: 0.9477634429931641, Final Batch Loss: 0.28109416365623474\n",
      "Epoch 3867, Loss: 0.9739492386579514, Final Batch Loss: 0.21289397776126862\n",
      "Epoch 3868, Loss: 1.0945186465978622, Final Batch Loss: 0.3487822413444519\n",
      "Epoch 3869, Loss: 1.0616811364889145, Final Batch Loss: 0.31306642293930054\n",
      "Epoch 3870, Loss: 1.0306693017482758, Final Batch Loss: 0.22776418924331665\n",
      "Epoch 3871, Loss: 0.9814539849758148, Final Batch Loss: 0.24443405866622925\n",
      "Epoch 3872, Loss: 1.0780698955059052, Final Batch Loss: 0.2687898874282837\n",
      "Epoch 3873, Loss: 1.017411157488823, Final Batch Loss: 0.29689735174179077\n",
      "Epoch 3874, Loss: 0.927682563662529, Final Batch Loss: 0.2296508252620697\n",
      "Epoch 3875, Loss: 1.0689569115638733, Final Batch Loss: 0.2833211421966553\n",
      "Epoch 3876, Loss: 1.098449021577835, Final Batch Loss: 0.21341772377490997\n",
      "Epoch 3877, Loss: 1.0008515864610672, Final Batch Loss: 0.20837801694869995\n",
      "Epoch 3878, Loss: 0.9212643504142761, Final Batch Loss: 0.20963385701179504\n",
      "Epoch 3879, Loss: 0.9627364128828049, Final Batch Loss: 0.197722390294075\n",
      "Epoch 3880, Loss: 1.002351850271225, Final Batch Loss: 0.2791177034378052\n",
      "Epoch 3881, Loss: 1.0711472183465958, Final Batch Loss: 0.30050814151763916\n",
      "Epoch 3882, Loss: 1.0451910495758057, Final Batch Loss: 0.23468540608882904\n",
      "Epoch 3883, Loss: 0.9675779938697815, Final Batch Loss: 0.1955001950263977\n",
      "Epoch 3884, Loss: 1.1491458266973495, Final Batch Loss: 0.3560413718223572\n",
      "Epoch 3885, Loss: 0.8806899338960648, Final Batch Loss: 0.14611738920211792\n",
      "Epoch 3886, Loss: 1.1344517469406128, Final Batch Loss: 0.2979649305343628\n",
      "Epoch 3887, Loss: 0.9892015010118484, Final Batch Loss: 0.21279220283031464\n",
      "Epoch 3888, Loss: 0.996387779712677, Final Batch Loss: 0.19722691178321838\n",
      "Epoch 3889, Loss: 1.0805874168872833, Final Batch Loss: 0.29541826248168945\n",
      "Epoch 3890, Loss: 1.1988552510738373, Final Batch Loss: 0.34214332699775696\n",
      "Epoch 3891, Loss: 1.1776253134012222, Final Batch Loss: 0.2755163013935089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3892, Loss: 0.9843190163373947, Final Batch Loss: 0.22721730172634125\n",
      "Epoch 3893, Loss: 1.0327118635177612, Final Batch Loss: 0.205101877450943\n",
      "Epoch 3894, Loss: 0.9013261646032333, Final Batch Loss: 0.1779751479625702\n",
      "Epoch 3895, Loss: 1.0860407501459122, Final Batch Loss: 0.27643465995788574\n",
      "Epoch 3896, Loss: 1.0511522889137268, Final Batch Loss: 0.21954388916492462\n",
      "Epoch 3897, Loss: 0.9785601794719696, Final Batch Loss: 0.2216750532388687\n",
      "Epoch 3898, Loss: 1.0009593218564987, Final Batch Loss: 0.1861557811498642\n",
      "Epoch 3899, Loss: 0.9669238924980164, Final Batch Loss: 0.2902064919471741\n",
      "Epoch 3900, Loss: 0.9983955472707748, Final Batch Loss: 0.22694267332553864\n",
      "Epoch 3901, Loss: 0.9588083177804947, Final Batch Loss: 0.16100168228149414\n",
      "Epoch 3902, Loss: 1.0787349045276642, Final Batch Loss: 0.3110043406486511\n",
      "Epoch 3903, Loss: 0.9356243014335632, Final Batch Loss: 0.19554820656776428\n",
      "Epoch 3904, Loss: 0.971978098154068, Final Batch Loss: 0.32426339387893677\n",
      "Epoch 3905, Loss: 0.982694998383522, Final Batch Loss: 0.2545487582683563\n",
      "Epoch 3906, Loss: 1.0423422306776047, Final Batch Loss: 0.29972949624061584\n",
      "Epoch 3907, Loss: 1.0220315009355545, Final Batch Loss: 0.3153415024280548\n",
      "Epoch 3908, Loss: 1.0083433985710144, Final Batch Loss: 0.22282324731349945\n",
      "Epoch 3909, Loss: 1.096518412232399, Final Batch Loss: 0.2405102401971817\n",
      "Epoch 3910, Loss: 1.0234352499246597, Final Batch Loss: 0.23368145525455475\n",
      "Epoch 3911, Loss: 1.0560065656900406, Final Batch Loss: 0.2863325774669647\n",
      "Epoch 3912, Loss: 1.098468005657196, Final Batch Loss: 0.2680869400501251\n",
      "Epoch 3913, Loss: 1.1858814656734467, Final Batch Loss: 0.2532999813556671\n",
      "Epoch 3914, Loss: 0.9225658774375916, Final Batch Loss: 0.14750784635543823\n",
      "Epoch 3915, Loss: 1.099794864654541, Final Batch Loss: 0.29621946811676025\n",
      "Epoch 3916, Loss: 1.1096054017543793, Final Batch Loss: 0.2973228991031647\n",
      "Epoch 3917, Loss: 0.9622644633054733, Final Batch Loss: 0.23516632616519928\n",
      "Epoch 3918, Loss: 1.0394384115934372, Final Batch Loss: 0.27137109637260437\n",
      "Epoch 3919, Loss: 1.0292372852563858, Final Batch Loss: 0.2467961460351944\n",
      "Epoch 3920, Loss: 0.993536576628685, Final Batch Loss: 0.28831160068511963\n",
      "Epoch 3921, Loss: 0.9280592650175095, Final Batch Loss: 0.21289318799972534\n",
      "Epoch 3922, Loss: 0.9774779677391052, Final Batch Loss: 0.23973044753074646\n",
      "Epoch 3923, Loss: 1.066344439983368, Final Batch Loss: 0.32635751366615295\n",
      "Epoch 3924, Loss: 1.0214465260505676, Final Batch Loss: 0.23607107996940613\n",
      "Epoch 3925, Loss: 1.0644190609455109, Final Batch Loss: 0.23552179336547852\n",
      "Epoch 3926, Loss: 1.164910912513733, Final Batch Loss: 0.3643031418323517\n",
      "Epoch 3927, Loss: 0.9709085375070572, Final Batch Loss: 0.1849149465560913\n",
      "Epoch 3928, Loss: 0.9463278651237488, Final Batch Loss: 0.2508954107761383\n",
      "Epoch 3929, Loss: 0.9359467029571533, Final Batch Loss: 0.22678348422050476\n",
      "Epoch 3930, Loss: 1.0228068679571152, Final Batch Loss: 0.16595740616321564\n",
      "Epoch 3931, Loss: 0.9812155216932297, Final Batch Loss: 0.2512826919555664\n",
      "Epoch 3932, Loss: 0.943876177072525, Final Batch Loss: 0.17092351615428925\n",
      "Epoch 3933, Loss: 1.1388582289218903, Final Batch Loss: 0.3125027120113373\n",
      "Epoch 3934, Loss: 1.0066816806793213, Final Batch Loss: 0.19658924639225006\n",
      "Epoch 3935, Loss: 0.9788436740636826, Final Batch Loss: 0.28890639543533325\n",
      "Epoch 3936, Loss: 1.082832396030426, Final Batch Loss: 0.34996649622917175\n",
      "Epoch 3937, Loss: 1.1562704294919968, Final Batch Loss: 0.3372427523136139\n",
      "Epoch 3938, Loss: 0.9729634672403336, Final Batch Loss: 0.28037163615226746\n",
      "Epoch 3939, Loss: 1.0740297138690948, Final Batch Loss: 0.2975205183029175\n",
      "Epoch 3940, Loss: 0.9078612774610519, Final Batch Loss: 0.18130004405975342\n",
      "Epoch 3941, Loss: 0.9522141516208649, Final Batch Loss: 0.1810409277677536\n",
      "Epoch 3942, Loss: 0.9291537404060364, Final Batch Loss: 0.23592373728752136\n",
      "Epoch 3943, Loss: 0.984589695930481, Final Batch Loss: 0.22378471493721008\n",
      "Epoch 3944, Loss: 1.1024810075759888, Final Batch Loss: 0.26379650831222534\n",
      "Epoch 3945, Loss: 0.984995648264885, Final Batch Loss: 0.15238057076931\n",
      "Epoch 3946, Loss: 0.9717138856649399, Final Batch Loss: 0.17493881285190582\n",
      "Epoch 3947, Loss: 1.1178288012742996, Final Batch Loss: 0.36696282029151917\n",
      "Epoch 3948, Loss: 1.0152059346437454, Final Batch Loss: 0.25949177145957947\n",
      "Epoch 3949, Loss: 1.015579268336296, Final Batch Loss: 0.24487487971782684\n",
      "Epoch 3950, Loss: 1.0401585400104523, Final Batch Loss: 0.30472037196159363\n",
      "Epoch 3951, Loss: 1.0609739124774933, Final Batch Loss: 0.2010711133480072\n",
      "Epoch 3952, Loss: 1.0587702542543411, Final Batch Loss: 0.3102072477340698\n",
      "Epoch 3953, Loss: 0.8703044801950455, Final Batch Loss: 0.1459723860025406\n",
      "Epoch 3954, Loss: 0.9488573223352432, Final Batch Loss: 0.27773749828338623\n",
      "Epoch 3955, Loss: 0.9926770180463791, Final Batch Loss: 0.27013975381851196\n",
      "Epoch 3956, Loss: 0.9634634852409363, Final Batch Loss: 0.2531147599220276\n",
      "Epoch 3957, Loss: 1.0225802809000015, Final Batch Loss: 0.23172451555728912\n",
      "Epoch 3958, Loss: 0.9093104153871536, Final Batch Loss: 0.22508765757083893\n",
      "Epoch 3959, Loss: 1.0618654787540436, Final Batch Loss: 0.1964074969291687\n",
      "Epoch 3960, Loss: 1.112688347697258, Final Batch Loss: 0.24924083054065704\n",
      "Epoch 3961, Loss: 1.126295655965805, Final Batch Loss: 0.2862580716609955\n",
      "Epoch 3962, Loss: 1.0517154932022095, Final Batch Loss: 0.24599774181842804\n",
      "Epoch 3963, Loss: 0.9971159845590591, Final Batch Loss: 0.24290454387664795\n",
      "Epoch 3964, Loss: 0.942867249250412, Final Batch Loss: 0.24946434795856476\n",
      "Epoch 3965, Loss: 1.0693867206573486, Final Batch Loss: 0.32634449005126953\n",
      "Epoch 3966, Loss: 0.9442058950662613, Final Batch Loss: 0.20334118604660034\n",
      "Epoch 3967, Loss: 1.0319207459688187, Final Batch Loss: 0.22067944705486298\n",
      "Epoch 3968, Loss: 0.9492650479078293, Final Batch Loss: 0.24976924061775208\n",
      "Epoch 3969, Loss: 1.0745780616998672, Final Batch Loss: 0.32875972986221313\n",
      "Epoch 3970, Loss: 0.9889738708734512, Final Batch Loss: 0.2778794467449188\n",
      "Epoch 3971, Loss: 1.13601616024971, Final Batch Loss: 0.4178750216960907\n",
      "Epoch 3972, Loss: 1.0627045035362244, Final Batch Loss: 0.33735767006874084\n",
      "Epoch 3973, Loss: 1.1251035928726196, Final Batch Loss: 0.21852430701255798\n",
      "Epoch 3974, Loss: 1.039788082242012, Final Batch Loss: 0.24982918798923492\n",
      "Epoch 3975, Loss: 1.0098610669374466, Final Batch Loss: 0.2662544846534729\n",
      "Epoch 3976, Loss: 0.9314451068639755, Final Batch Loss: 0.17364346981048584\n",
      "Epoch 3977, Loss: 1.0056779086589813, Final Batch Loss: 0.17285777628421783\n",
      "Epoch 3978, Loss: 1.1765735447406769, Final Batch Loss: 0.3528059422969818\n",
      "Epoch 3979, Loss: 0.9654379785060883, Final Batch Loss: 0.3104161024093628\n",
      "Epoch 3980, Loss: 0.9878966510295868, Final Batch Loss: 0.19123202562332153\n",
      "Epoch 3981, Loss: 0.9455550312995911, Final Batch Loss: 0.19601300358772278\n",
      "Epoch 3982, Loss: 1.0029759854078293, Final Batch Loss: 0.226074680685997\n",
      "Epoch 3983, Loss: 0.868613988161087, Final Batch Loss: 0.1994907408952713\n",
      "Epoch 3984, Loss: 0.9502506703138351, Final Batch Loss: 0.2334241271018982\n",
      "Epoch 3985, Loss: 0.9961148947477341, Final Batch Loss: 0.2613603472709656\n",
      "Epoch 3986, Loss: 1.0215212553739548, Final Batch Loss: 0.24125923216342926\n",
      "Epoch 3987, Loss: 0.9707728773355484, Final Batch Loss: 0.2265278548002243\n",
      "Epoch 3988, Loss: 1.0297295153141022, Final Batch Loss: 0.23299090564250946\n",
      "Epoch 3989, Loss: 0.9196903556585312, Final Batch Loss: 0.2018761932849884\n",
      "Epoch 3990, Loss: 0.9725591689348221, Final Batch Loss: 0.2013765275478363\n",
      "Epoch 3991, Loss: 0.8901364505290985, Final Batch Loss: 0.23851022124290466\n",
      "Epoch 3992, Loss: 0.9646887332201004, Final Batch Loss: 0.2047225683927536\n",
      "Epoch 3993, Loss: 0.9462520480155945, Final Batch Loss: 0.2568442225456238\n",
      "Epoch 3994, Loss: 1.064219906926155, Final Batch Loss: 0.24835799634456635\n",
      "Epoch 3995, Loss: 0.9987717717885971, Final Batch Loss: 0.21872703731060028\n",
      "Epoch 3996, Loss: 0.9676994979381561, Final Batch Loss: 0.22660650312900543\n",
      "Epoch 3997, Loss: 0.944944441318512, Final Batch Loss: 0.2942604124546051\n",
      "Epoch 3998, Loss: 0.9919490814208984, Final Batch Loss: 0.2602505683898926\n",
      "Epoch 3999, Loss: 1.1199913769960403, Final Batch Loss: 0.26922738552093506\n",
      "Epoch 4000, Loss: 0.9606533646583557, Final Batch Loss: 0.21048828959465027\n",
      "Epoch 4001, Loss: 0.9737211018800735, Final Batch Loss: 0.1958189308643341\n",
      "Epoch 4002, Loss: 0.9698679745197296, Final Batch Loss: 0.22588184475898743\n",
      "Epoch 4003, Loss: 1.0374604761600494, Final Batch Loss: 0.256696879863739\n",
      "Epoch 4004, Loss: 1.1044328957796097, Final Batch Loss: 0.32236388325691223\n",
      "Epoch 4005, Loss: 0.9940540939569473, Final Batch Loss: 0.2415706217288971\n",
      "Epoch 4006, Loss: 1.0580878853797913, Final Batch Loss: 0.3363240957260132\n",
      "Epoch 4007, Loss: 1.0090094208717346, Final Batch Loss: 0.20169241726398468\n",
      "Epoch 4008, Loss: 1.0233011841773987, Final Batch Loss: 0.288253515958786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4009, Loss: 1.118297815322876, Final Batch Loss: 0.28956568241119385\n",
      "Epoch 4010, Loss: 1.0674321055412292, Final Batch Loss: 0.2576388418674469\n",
      "Epoch 4011, Loss: 1.0762523859739304, Final Batch Loss: 0.1960446685552597\n",
      "Epoch 4012, Loss: 1.1818309277296066, Final Batch Loss: 0.36254167556762695\n",
      "Epoch 4013, Loss: 1.037438064813614, Final Batch Loss: 0.22966712713241577\n",
      "Epoch 4014, Loss: 0.9557690769433975, Final Batch Loss: 0.1979450136423111\n",
      "Epoch 4015, Loss: 0.9589105248451233, Final Batch Loss: 0.2973540127277374\n",
      "Epoch 4016, Loss: 1.005259171128273, Final Batch Loss: 0.331408828496933\n",
      "Epoch 4017, Loss: 0.9739242047071457, Final Batch Loss: 0.2703048586845398\n",
      "Epoch 4018, Loss: 1.0812853425741196, Final Batch Loss: 0.31649306416511536\n",
      "Epoch 4019, Loss: 1.1271795183420181, Final Batch Loss: 0.3037736117839813\n",
      "Epoch 4020, Loss: 1.039149522781372, Final Batch Loss: 0.23778221011161804\n",
      "Epoch 4021, Loss: 1.053736686706543, Final Batch Loss: 0.1781080663204193\n",
      "Epoch 4022, Loss: 1.068574234843254, Final Batch Loss: 0.29668375849723816\n",
      "Epoch 4023, Loss: 1.098537653684616, Final Batch Loss: 0.27802762389183044\n",
      "Epoch 4024, Loss: 1.1856118440628052, Final Batch Loss: 0.2826813757419586\n",
      "Epoch 4025, Loss: 1.274434894323349, Final Batch Loss: 0.5386188626289368\n",
      "Epoch 4026, Loss: 1.0568177253007889, Final Batch Loss: 0.23899735510349274\n",
      "Epoch 4027, Loss: 1.1277655065059662, Final Batch Loss: 0.33858975768089294\n",
      "Epoch 4028, Loss: 1.0659084618091583, Final Batch Loss: 0.27570247650146484\n",
      "Epoch 4029, Loss: 1.0897759348154068, Final Batch Loss: 0.2771228551864624\n",
      "Epoch 4030, Loss: 1.0989140421152115, Final Batch Loss: 0.16249482333660126\n",
      "Epoch 4031, Loss: 0.9690766930580139, Final Batch Loss: 0.28029975295066833\n",
      "Epoch 4032, Loss: 0.9900956004858017, Final Batch Loss: 0.25785860419273376\n",
      "Epoch 4033, Loss: 0.9847410917282104, Final Batch Loss: 0.3217692971229553\n",
      "Epoch 4034, Loss: 0.9516148418188095, Final Batch Loss: 0.24061115086078644\n",
      "Epoch 4035, Loss: 0.91600601375103, Final Batch Loss: 0.16945050656795502\n",
      "Epoch 4036, Loss: 1.0671512633562088, Final Batch Loss: 0.29492083191871643\n",
      "Epoch 4037, Loss: 0.9355498105287552, Final Batch Loss: 0.23103037476539612\n",
      "Epoch 4038, Loss: 0.9518736451864243, Final Batch Loss: 0.3044836223125458\n",
      "Epoch 4039, Loss: 1.010766476392746, Final Batch Loss: 0.18391956388950348\n",
      "Epoch 4040, Loss: 1.116055190563202, Final Batch Loss: 0.33761459589004517\n",
      "Epoch 4041, Loss: 0.9813535362482071, Final Batch Loss: 0.2052580565214157\n",
      "Epoch 4042, Loss: 0.9852848500013351, Final Batch Loss: 0.28838932514190674\n",
      "Epoch 4043, Loss: 1.0194088816642761, Final Batch Loss: 0.2741442322731018\n",
      "Epoch 4044, Loss: 1.054934561252594, Final Batch Loss: 0.26575905084609985\n",
      "Epoch 4045, Loss: 0.9593262821435928, Final Batch Loss: 0.2532970905303955\n",
      "Epoch 4046, Loss: 1.0601970255374908, Final Batch Loss: 0.3041294813156128\n",
      "Epoch 4047, Loss: 0.9860374629497528, Final Batch Loss: 0.22724956274032593\n",
      "Epoch 4048, Loss: 0.9083820730447769, Final Batch Loss: 0.22847433388233185\n",
      "Epoch 4049, Loss: 0.9344627857208252, Final Batch Loss: 0.2187870591878891\n",
      "Epoch 4050, Loss: 1.1380482017993927, Final Batch Loss: 0.3611343204975128\n",
      "Epoch 4051, Loss: 0.951348140835762, Final Batch Loss: 0.18820509314537048\n",
      "Epoch 4052, Loss: 1.1398700177669525, Final Batch Loss: 0.2912277281284332\n",
      "Epoch 4053, Loss: 1.0082639008760452, Final Batch Loss: 0.25385957956314087\n",
      "Epoch 4054, Loss: 1.0237594097852707, Final Batch Loss: 0.319104939699173\n",
      "Epoch 4055, Loss: 0.9961281269788742, Final Batch Loss: 0.2679016590118408\n",
      "Epoch 4056, Loss: 1.0176752507686615, Final Batch Loss: 0.34694114327430725\n",
      "Epoch 4057, Loss: 0.9464662075042725, Final Batch Loss: 0.21789242327213287\n",
      "Epoch 4058, Loss: 0.9941388219594955, Final Batch Loss: 0.17411987483501434\n",
      "Epoch 4059, Loss: 1.0905741453170776, Final Batch Loss: 0.2597402036190033\n",
      "Epoch 4060, Loss: 0.9704938977956772, Final Batch Loss: 0.20197425782680511\n",
      "Epoch 4061, Loss: 1.1013770401477814, Final Batch Loss: 0.21926963329315186\n",
      "Epoch 4062, Loss: 1.0442649722099304, Final Batch Loss: 0.26958486437797546\n",
      "Epoch 4063, Loss: 1.0038404762744904, Final Batch Loss: 0.3057138919830322\n",
      "Epoch 4064, Loss: 1.0022388249635696, Final Batch Loss: 0.25798478722572327\n",
      "Epoch 4065, Loss: 0.921880841255188, Final Batch Loss: 0.2095048427581787\n",
      "Epoch 4066, Loss: 0.9858913719654083, Final Batch Loss: 0.1952008754014969\n",
      "Epoch 4067, Loss: 0.9741353988647461, Final Batch Loss: 0.24479547142982483\n",
      "Epoch 4068, Loss: 1.0748531222343445, Final Batch Loss: 0.2646627426147461\n",
      "Epoch 4069, Loss: 0.9704988151788712, Final Batch Loss: 0.18317680060863495\n",
      "Epoch 4070, Loss: 0.9805960655212402, Final Batch Loss: 0.23072320222854614\n",
      "Epoch 4071, Loss: 1.0311545431613922, Final Batch Loss: 0.26968836784362793\n",
      "Epoch 4072, Loss: 0.9926108121871948, Final Batch Loss: 0.18763113021850586\n",
      "Epoch 4073, Loss: 1.0514678359031677, Final Batch Loss: 0.27918189764022827\n",
      "Epoch 4074, Loss: 0.940455973148346, Final Batch Loss: 0.17592255771160126\n",
      "Epoch 4075, Loss: 0.8894679248332977, Final Batch Loss: 0.1941237598657608\n",
      "Epoch 4076, Loss: 0.9819304794073105, Final Batch Loss: 0.27182573080062866\n",
      "Epoch 4077, Loss: 0.9168942421674728, Final Batch Loss: 0.21611447632312775\n",
      "Epoch 4078, Loss: 1.0603642612695694, Final Batch Loss: 0.21448861062526703\n",
      "Epoch 4079, Loss: 1.028734564781189, Final Batch Loss: 0.18699240684509277\n",
      "Epoch 4080, Loss: 1.1251942217350006, Final Batch Loss: 0.2290532886981964\n",
      "Epoch 4081, Loss: 0.8996858447790146, Final Batch Loss: 0.17927591502666473\n",
      "Epoch 4082, Loss: 1.0984622240066528, Final Batch Loss: 0.32396525144577026\n",
      "Epoch 4083, Loss: 0.9753743410110474, Final Batch Loss: 0.20396211743354797\n",
      "Epoch 4084, Loss: 0.9317368119955063, Final Batch Loss: 0.26091137528419495\n",
      "Epoch 4085, Loss: 0.995051771402359, Final Batch Loss: 0.23125271499156952\n",
      "Epoch 4086, Loss: 0.8420242220163345, Final Batch Loss: 0.23223383724689484\n",
      "Epoch 4087, Loss: 0.9557999521493912, Final Batch Loss: 0.263102263212204\n",
      "Epoch 4088, Loss: 0.9561675786972046, Final Batch Loss: 0.18287648260593414\n",
      "Epoch 4089, Loss: 1.0249657481908798, Final Batch Loss: 0.31376737356185913\n",
      "Epoch 4090, Loss: 1.1418072432279587, Final Batch Loss: 0.3109259605407715\n",
      "Epoch 4091, Loss: 1.06411874294281, Final Batch Loss: 0.2826404869556427\n",
      "Epoch 4092, Loss: 0.9533068686723709, Final Batch Loss: 0.24776315689086914\n",
      "Epoch 4093, Loss: 1.1842605471611023, Final Batch Loss: 0.4242549538612366\n",
      "Epoch 4094, Loss: 1.1158350259065628, Final Batch Loss: 0.32661134004592896\n",
      "Epoch 4095, Loss: 0.953068882226944, Final Batch Loss: 0.27286359667778015\n",
      "Epoch 4096, Loss: 1.0078759640455246, Final Batch Loss: 0.2325238287448883\n",
      "Epoch 4097, Loss: 1.0245124697685242, Final Batch Loss: 0.22804474830627441\n",
      "Epoch 4098, Loss: 0.9415343850851059, Final Batch Loss: 0.23958899080753326\n",
      "Epoch 4099, Loss: 0.9850446879863739, Final Batch Loss: 0.27557793259620667\n",
      "Epoch 4100, Loss: 1.037594810128212, Final Batch Loss: 0.2935088276863098\n",
      "Epoch 4101, Loss: 1.032584547996521, Final Batch Loss: 0.2525724172592163\n",
      "Epoch 4102, Loss: 0.8864424526691437, Final Batch Loss: 0.22046412527561188\n",
      "Epoch 4103, Loss: 1.1684152781963348, Final Batch Loss: 0.29750919342041016\n",
      "Epoch 4104, Loss: 0.9469514638185501, Final Batch Loss: 0.19796772301197052\n",
      "Epoch 4105, Loss: 1.0167296379804611, Final Batch Loss: 0.35512709617614746\n",
      "Epoch 4106, Loss: 1.0724894553422928, Final Batch Loss: 0.3141835927963257\n",
      "Epoch 4107, Loss: 0.9211067110300064, Final Batch Loss: 0.17284302413463593\n",
      "Epoch 4108, Loss: 0.953262522816658, Final Batch Loss: 0.21021099388599396\n",
      "Epoch 4109, Loss: 1.0634223818778992, Final Batch Loss: 0.28525662422180176\n",
      "Epoch 4110, Loss: 1.1005094647407532, Final Batch Loss: 0.2684476971626282\n",
      "Epoch 4111, Loss: 0.9728629887104034, Final Batch Loss: 0.23826871812343597\n",
      "Epoch 4112, Loss: 1.1953477412462234, Final Batch Loss: 0.32464513182640076\n",
      "Epoch 4113, Loss: 1.0842612385749817, Final Batch Loss: 0.38208216428756714\n",
      "Epoch 4114, Loss: 1.1222543716430664, Final Batch Loss: 0.1959928274154663\n",
      "Epoch 4115, Loss: 0.9584221988916397, Final Batch Loss: 0.2107151597738266\n",
      "Epoch 4116, Loss: 1.0027579367160797, Final Batch Loss: 0.24777241051197052\n",
      "Epoch 4117, Loss: 1.113633081316948, Final Batch Loss: 0.27742427587509155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4118, Loss: 0.9337728768587112, Final Batch Loss: 0.22805234789848328\n",
      "Epoch 4119, Loss: 0.9475809633731842, Final Batch Loss: 0.14824403822422028\n",
      "Epoch 4120, Loss: 0.9629088193178177, Final Batch Loss: 0.217128723859787\n",
      "Epoch 4121, Loss: 1.0352795124053955, Final Batch Loss: 0.23067593574523926\n",
      "Epoch 4122, Loss: 0.9614033699035645, Final Batch Loss: 0.16450637578964233\n",
      "Epoch 4123, Loss: 1.0278219282627106, Final Batch Loss: 0.25742799043655396\n",
      "Epoch 4124, Loss: 1.1369135677814484, Final Batch Loss: 0.34893739223480225\n",
      "Epoch 4125, Loss: 1.0242734104394913, Final Batch Loss: 0.269709974527359\n",
      "Epoch 4126, Loss: 1.0311866402626038, Final Batch Loss: 0.30437788367271423\n",
      "Epoch 4127, Loss: 1.0612913966178894, Final Batch Loss: 0.2599504590034485\n",
      "Epoch 4128, Loss: 0.9848564565181732, Final Batch Loss: 0.2409268617630005\n",
      "Epoch 4129, Loss: 0.9223843216896057, Final Batch Loss: 0.2694844901561737\n",
      "Epoch 4130, Loss: 0.9803462475538254, Final Batch Loss: 0.2803085446357727\n",
      "Epoch 4131, Loss: 1.1631907671689987, Final Batch Loss: 0.4375915825366974\n",
      "Epoch 4132, Loss: 1.0055809020996094, Final Batch Loss: 0.19461114704608917\n",
      "Epoch 4133, Loss: 1.1253540217876434, Final Batch Loss: 0.30601146817207336\n",
      "Epoch 4134, Loss: 1.0186956822872162, Final Batch Loss: 0.1717710793018341\n",
      "Epoch 4135, Loss: 1.0423368513584137, Final Batch Loss: 0.2693997323513031\n",
      "Epoch 4136, Loss: 1.1014979183673859, Final Batch Loss: 0.3276299834251404\n",
      "Epoch 4137, Loss: 1.0411342978477478, Final Batch Loss: 0.21878975629806519\n",
      "Epoch 4138, Loss: 1.0238100588321686, Final Batch Loss: 0.3076179027557373\n",
      "Epoch 4139, Loss: 0.9345852434635162, Final Batch Loss: 0.2648502290248871\n",
      "Epoch 4140, Loss: 0.9848925024271011, Final Batch Loss: 0.25116094946861267\n",
      "Epoch 4141, Loss: 0.8263141512870789, Final Batch Loss: 0.12855462729930878\n",
      "Epoch 4142, Loss: 0.9184105545282364, Final Batch Loss: 0.19753560423851013\n",
      "Epoch 4143, Loss: 1.1157972067594528, Final Batch Loss: 0.3171568214893341\n",
      "Epoch 4144, Loss: 1.0020138770341873, Final Batch Loss: 0.3224215805530548\n",
      "Epoch 4145, Loss: 1.0334848016500473, Final Batch Loss: 0.19009967148303986\n",
      "Epoch 4146, Loss: 1.0887679904699326, Final Batch Loss: 0.24917097389698029\n",
      "Epoch 4147, Loss: 1.1064243018627167, Final Batch Loss: 0.3051758408546448\n",
      "Epoch 4148, Loss: 0.9766771048307419, Final Batch Loss: 0.24754156172275543\n",
      "Epoch 4149, Loss: 1.0328703820705414, Final Batch Loss: 0.1814194917678833\n",
      "Epoch 4150, Loss: 1.0915871858596802, Final Batch Loss: 0.36841756105422974\n",
      "Epoch 4151, Loss: 0.9924345463514328, Final Batch Loss: 0.3271434009075165\n",
      "Epoch 4152, Loss: 0.9692507982254028, Final Batch Loss: 0.17079561948776245\n",
      "Epoch 4153, Loss: 1.1152548789978027, Final Batch Loss: 0.3604888617992401\n",
      "Epoch 4154, Loss: 1.0418698638677597, Final Batch Loss: 0.23011422157287598\n",
      "Epoch 4155, Loss: 0.956286832690239, Final Batch Loss: 0.22975614666938782\n",
      "Epoch 4156, Loss: 1.0105333179235458, Final Batch Loss: 0.2245568186044693\n",
      "Epoch 4157, Loss: 0.9277537316083908, Final Batch Loss: 0.2131931334733963\n",
      "Epoch 4158, Loss: 1.0293149054050446, Final Batch Loss: 0.29528364539146423\n",
      "Epoch 4159, Loss: 1.0503265708684921, Final Batch Loss: 0.19475673139095306\n",
      "Epoch 4160, Loss: 1.0470622330904007, Final Batch Loss: 0.2370394766330719\n",
      "Epoch 4161, Loss: 0.9903828948736191, Final Batch Loss: 0.2090245932340622\n",
      "Epoch 4162, Loss: 1.0120587646961212, Final Batch Loss: 0.3312905430793762\n",
      "Epoch 4163, Loss: 1.1387291848659515, Final Batch Loss: 0.31371191143989563\n",
      "Epoch 4164, Loss: 1.0655922442674637, Final Batch Loss: 0.2992280423641205\n",
      "Epoch 4165, Loss: 0.9633509516716003, Final Batch Loss: 0.1793247014284134\n",
      "Epoch 4166, Loss: 0.976401224732399, Final Batch Loss: 0.30798691511154175\n",
      "Epoch 4167, Loss: 0.9235266596078873, Final Batch Loss: 0.26905396580696106\n",
      "Epoch 4168, Loss: 1.0689801573753357, Final Batch Loss: 0.2642781138420105\n",
      "Epoch 4169, Loss: 1.0962191075086594, Final Batch Loss: 0.29450803995132446\n",
      "Epoch 4170, Loss: 0.9031951874494553, Final Batch Loss: 0.2350427359342575\n",
      "Epoch 4171, Loss: 1.0742065459489822, Final Batch Loss: 0.22479183971881866\n",
      "Epoch 4172, Loss: 1.072436273097992, Final Batch Loss: 0.24903708696365356\n",
      "Epoch 4173, Loss: 1.0266063064336777, Final Batch Loss: 0.2679644823074341\n",
      "Epoch 4174, Loss: 1.0859221667051315, Final Batch Loss: 0.3017718493938446\n",
      "Epoch 4175, Loss: 0.9970200806856155, Final Batch Loss: 0.217478409409523\n",
      "Epoch 4176, Loss: 0.950337290763855, Final Batch Loss: 0.2269551306962967\n",
      "Epoch 4177, Loss: 0.9805124551057816, Final Batch Loss: 0.2957371473312378\n",
      "Epoch 4178, Loss: 1.005544975399971, Final Batch Loss: 0.20391230285167694\n",
      "Epoch 4179, Loss: 1.008208006620407, Final Batch Loss: 0.22655488550662994\n",
      "Epoch 4180, Loss: 1.0054388642311096, Final Batch Loss: 0.2675120234489441\n",
      "Epoch 4181, Loss: 0.9829683750867844, Final Batch Loss: 0.19679290056228638\n",
      "Epoch 4182, Loss: 1.058072879910469, Final Batch Loss: 0.2287369817495346\n",
      "Epoch 4183, Loss: 0.9693827331066132, Final Batch Loss: 0.20161736011505127\n",
      "Epoch 4184, Loss: 0.9805824458599091, Final Batch Loss: 0.2398412525653839\n",
      "Epoch 4185, Loss: 0.8940156102180481, Final Batch Loss: 0.14023086428642273\n",
      "Epoch 4186, Loss: 0.9734472632408142, Final Batch Loss: 0.2684720754623413\n",
      "Epoch 4187, Loss: 0.9365612715482712, Final Batch Loss: 0.21240867674350739\n",
      "Epoch 4188, Loss: 1.141302227973938, Final Batch Loss: 0.22830888628959656\n",
      "Epoch 4189, Loss: 1.1626424044370651, Final Batch Loss: 0.3397594094276428\n",
      "Epoch 4190, Loss: 0.9172479063272476, Final Batch Loss: 0.17830833792686462\n",
      "Epoch 4191, Loss: 1.076638102531433, Final Batch Loss: 0.24805617332458496\n",
      "Epoch 4192, Loss: 1.0237789750099182, Final Batch Loss: 0.2746990919113159\n",
      "Epoch 4193, Loss: 0.9986985176801682, Final Batch Loss: 0.22087547183036804\n",
      "Epoch 4194, Loss: 1.0320998579263687, Final Batch Loss: 0.33175328373908997\n",
      "Epoch 4195, Loss: 1.001331776380539, Final Batch Loss: 0.1962040811777115\n",
      "Epoch 4196, Loss: 1.0977856665849686, Final Batch Loss: 0.30548611283302307\n",
      "Epoch 4197, Loss: 1.1128492206335068, Final Batch Loss: 0.3063041865825653\n",
      "Epoch 4198, Loss: 1.0001841932535172, Final Batch Loss: 0.24834945797920227\n",
      "Epoch 4199, Loss: 0.997153177857399, Final Batch Loss: 0.24887524545192719\n",
      "Epoch 4200, Loss: 1.0622043311595917, Final Batch Loss: 0.23654767870903015\n",
      "Epoch 4201, Loss: 0.9535368382930756, Final Batch Loss: 0.2544258236885071\n",
      "Epoch 4202, Loss: 1.0996728986501694, Final Batch Loss: 0.31587618589401245\n",
      "Epoch 4203, Loss: 0.9834709614515305, Final Batch Loss: 0.19842563569545746\n",
      "Epoch 4204, Loss: 1.0030091255903244, Final Batch Loss: 0.26906606554985046\n",
      "Epoch 4205, Loss: 1.0827472060918808, Final Batch Loss: 0.3100985884666443\n",
      "Epoch 4206, Loss: 0.9363189786672592, Final Batch Loss: 0.3158011734485626\n",
      "Epoch 4207, Loss: 0.9772743731737137, Final Batch Loss: 0.21433000266551971\n",
      "Epoch 4208, Loss: 0.9857358336448669, Final Batch Loss: 0.19519957900047302\n",
      "Epoch 4209, Loss: 0.918555960059166, Final Batch Loss: 0.2994684875011444\n",
      "Epoch 4210, Loss: 0.8234665095806122, Final Batch Loss: 0.16742168366909027\n",
      "Epoch 4211, Loss: 1.0205315500497818, Final Batch Loss: 0.3457089364528656\n",
      "Epoch 4212, Loss: 1.032443180680275, Final Batch Loss: 0.27970653772354126\n",
      "Epoch 4213, Loss: 1.0039056092500687, Final Batch Loss: 0.28185057640075684\n",
      "Epoch 4214, Loss: 0.8955108523368835, Final Batch Loss: 0.18038558959960938\n",
      "Epoch 4215, Loss: 0.9274792522192001, Final Batch Loss: 0.2103854864835739\n",
      "Epoch 4216, Loss: 0.9302163273096085, Final Batch Loss: 0.22672611474990845\n",
      "Epoch 4217, Loss: 1.0783819109201431, Final Batch Loss: 0.3350021243095398\n",
      "Epoch 4218, Loss: 0.8646277785301208, Final Batch Loss: 0.14673392474651337\n",
      "Epoch 4219, Loss: 0.9832892119884491, Final Batch Loss: 0.22309459745883942\n",
      "Epoch 4220, Loss: 1.0991113632917404, Final Batch Loss: 0.3467440903186798\n",
      "Epoch 4221, Loss: 1.115645557641983, Final Batch Loss: 0.304732084274292\n",
      "Epoch 4222, Loss: 1.0414128005504608, Final Batch Loss: 0.24308402836322784\n",
      "Epoch 4223, Loss: 1.0812785923480988, Final Batch Loss: 0.29347074031829834\n",
      "Epoch 4224, Loss: 1.009779930114746, Final Batch Loss: 0.31805068254470825\n",
      "Epoch 4225, Loss: 0.9309321641921997, Final Batch Loss: 0.17755183577537537\n",
      "Epoch 4226, Loss: 0.9214054495096207, Final Batch Loss: 0.19308483600616455\n",
      "Epoch 4227, Loss: 0.8997330367565155, Final Batch Loss: 0.125065416097641\n",
      "Epoch 4228, Loss: 1.032110020518303, Final Batch Loss: 0.3055990934371948\n",
      "Epoch 4229, Loss: 1.0859842747449875, Final Batch Loss: 0.27075430750846863\n",
      "Epoch 4230, Loss: 1.1022126078605652, Final Batch Loss: 0.322054922580719\n",
      "Epoch 4231, Loss: 0.9921168386936188, Final Batch Loss: 0.22275875508785248\n",
      "Epoch 4232, Loss: 0.9811782985925674, Final Batch Loss: 0.24282650649547577\n",
      "Epoch 4233, Loss: 1.0146469324827194, Final Batch Loss: 0.35983723402023315\n",
      "Epoch 4234, Loss: 0.9850758165121078, Final Batch Loss: 0.19090497493743896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4235, Loss: 1.1336778104305267, Final Batch Loss: 0.39072611927986145\n",
      "Epoch 4236, Loss: 1.1400908678770065, Final Batch Loss: 0.3246224522590637\n",
      "Epoch 4237, Loss: 1.0239344835281372, Final Batch Loss: 0.27004167437553406\n",
      "Epoch 4238, Loss: 0.9419591277837753, Final Batch Loss: 0.27294886112213135\n",
      "Epoch 4239, Loss: 0.9815260320901871, Final Batch Loss: 0.1638844758272171\n",
      "Epoch 4240, Loss: 1.073588490486145, Final Batch Loss: 0.22027716040611267\n",
      "Epoch 4241, Loss: 1.0859307199716568, Final Batch Loss: 0.20800337195396423\n",
      "Epoch 4242, Loss: 1.0249385833740234, Final Batch Loss: 0.2559084892272949\n",
      "Epoch 4243, Loss: 1.0235212445259094, Final Batch Loss: 0.2404526174068451\n",
      "Epoch 4244, Loss: 0.9695903062820435, Final Batch Loss: 0.226278617978096\n",
      "Epoch 4245, Loss: 0.973523274064064, Final Batch Loss: 0.20954525470733643\n",
      "Epoch 4246, Loss: 1.0199522227048874, Final Batch Loss: 0.22141240537166595\n",
      "Epoch 4247, Loss: 0.9621466547250748, Final Batch Loss: 0.19603629410266876\n",
      "Epoch 4248, Loss: 0.9459853917360306, Final Batch Loss: 0.20858663320541382\n",
      "Epoch 4249, Loss: 1.0352457165718079, Final Batch Loss: 0.20844019949436188\n",
      "Epoch 4250, Loss: 1.014978140592575, Final Batch Loss: 0.30519968271255493\n",
      "Epoch 4251, Loss: 0.9518994390964508, Final Batch Loss: 0.26531514525413513\n",
      "Epoch 4252, Loss: 0.9406166076660156, Final Batch Loss: 0.21026121079921722\n",
      "Epoch 4253, Loss: 0.9847522228956223, Final Batch Loss: 0.27341049909591675\n",
      "Epoch 4254, Loss: 0.9805230349302292, Final Batch Loss: 0.21959060430526733\n",
      "Epoch 4255, Loss: 1.0297507643699646, Final Batch Loss: 0.3598805367946625\n",
      "Epoch 4256, Loss: 0.9243263751268387, Final Batch Loss: 0.2523590922355652\n",
      "Epoch 4257, Loss: 1.1325038522481918, Final Batch Loss: 0.3866975009441376\n",
      "Epoch 4258, Loss: 0.9288480579853058, Final Batch Loss: 0.2025735080242157\n",
      "Epoch 4259, Loss: 1.1228058338165283, Final Batch Loss: 0.24736297130584717\n",
      "Epoch 4260, Loss: 0.861210972070694, Final Batch Loss: 0.24635805189609528\n",
      "Epoch 4261, Loss: 1.0382441729307175, Final Batch Loss: 0.26836174726486206\n",
      "Epoch 4262, Loss: 0.9919625818729401, Final Batch Loss: 0.21879661083221436\n",
      "Epoch 4263, Loss: 1.1202603429555893, Final Batch Loss: 0.2770809531211853\n",
      "Epoch 4264, Loss: 1.1715529561042786, Final Batch Loss: 0.3318788707256317\n",
      "Epoch 4265, Loss: 0.9412638545036316, Final Batch Loss: 0.15678927302360535\n",
      "Epoch 4266, Loss: 0.9058925211429596, Final Batch Loss: 0.18775369226932526\n",
      "Epoch 4267, Loss: 1.1090666502714157, Final Batch Loss: 0.20250047743320465\n",
      "Epoch 4268, Loss: 1.09144689142704, Final Batch Loss: 0.3258943259716034\n",
      "Epoch 4269, Loss: 0.8071160912513733, Final Batch Loss: 0.14191235601902008\n",
      "Epoch 4270, Loss: 1.0574152022600174, Final Batch Loss: 0.26602858304977417\n",
      "Epoch 4271, Loss: 1.075069859623909, Final Batch Loss: 0.28432023525238037\n",
      "Epoch 4272, Loss: 0.9419906437397003, Final Batch Loss: 0.2695475220680237\n",
      "Epoch 4273, Loss: 0.9068390280008316, Final Batch Loss: 0.25976482033729553\n",
      "Epoch 4274, Loss: 0.9297754615545273, Final Batch Loss: 0.2844911515712738\n",
      "Epoch 4275, Loss: 0.9842002540826797, Final Batch Loss: 0.21286098659038544\n",
      "Epoch 4276, Loss: 0.9972252696752548, Final Batch Loss: 0.24859821796417236\n",
      "Epoch 4277, Loss: 0.9634705781936646, Final Batch Loss: 0.25685811042785645\n",
      "Epoch 4278, Loss: 0.9678829312324524, Final Batch Loss: 0.2857338488101959\n",
      "Epoch 4279, Loss: 0.9784770309925079, Final Batch Loss: 0.250027596950531\n",
      "Epoch 4280, Loss: 0.9923002123832703, Final Batch Loss: 0.2517392039299011\n",
      "Epoch 4281, Loss: 0.9546277076005936, Final Batch Loss: 0.3359161615371704\n",
      "Epoch 4282, Loss: 0.997267872095108, Final Batch Loss: 0.17639850080013275\n",
      "Epoch 4283, Loss: 0.9299149662256241, Final Batch Loss: 0.2111666351556778\n",
      "Epoch 4284, Loss: 1.064931720495224, Final Batch Loss: 0.3822921812534332\n",
      "Epoch 4285, Loss: 1.003875806927681, Final Batch Loss: 0.29159417748451233\n",
      "Epoch 4286, Loss: 1.0948474258184433, Final Batch Loss: 0.31340882182121277\n",
      "Epoch 4287, Loss: 0.9594438374042511, Final Batch Loss: 0.26637330651283264\n",
      "Epoch 4288, Loss: 0.964776411652565, Final Batch Loss: 0.25350847840309143\n",
      "Epoch 4289, Loss: 0.9664368182420731, Final Batch Loss: 0.18062707781791687\n",
      "Epoch 4290, Loss: 1.0287912786006927, Final Batch Loss: 0.3229283392429352\n",
      "Epoch 4291, Loss: 1.070596992969513, Final Batch Loss: 0.3186056315898895\n",
      "Epoch 4292, Loss: 0.9472138732671738, Final Batch Loss: 0.2403336763381958\n",
      "Epoch 4293, Loss: 1.0127900391817093, Final Batch Loss: 0.2096029669046402\n",
      "Epoch 4294, Loss: 0.8623990565538406, Final Batch Loss: 0.13923276960849762\n",
      "Epoch 4295, Loss: 1.1672774404287338, Final Batch Loss: 0.34990668296813965\n",
      "Epoch 4296, Loss: 0.8853699639439583, Final Batch Loss: 0.12223004549741745\n",
      "Epoch 4297, Loss: 0.954134926199913, Final Batch Loss: 0.24902130663394928\n",
      "Epoch 4298, Loss: 0.9914426803588867, Final Batch Loss: 0.2430744171142578\n",
      "Epoch 4299, Loss: 1.0533103346824646, Final Batch Loss: 0.2751411497592926\n",
      "Epoch 4300, Loss: 1.0606954246759415, Final Batch Loss: 0.26041775941848755\n",
      "Epoch 4301, Loss: 1.1500126421451569, Final Batch Loss: 0.2971707582473755\n",
      "Epoch 4302, Loss: 0.939639151096344, Final Batch Loss: 0.18057382106781006\n",
      "Epoch 4303, Loss: 0.9215726256370544, Final Batch Loss: 0.25666314363479614\n",
      "Epoch 4304, Loss: 0.9193642735481262, Final Batch Loss: 0.17161276936531067\n",
      "Epoch 4305, Loss: 0.96230348944664, Final Batch Loss: 0.2760283052921295\n",
      "Epoch 4306, Loss: 1.0042172223329544, Final Batch Loss: 0.33384159207344055\n",
      "Epoch 4307, Loss: 1.0405612289905548, Final Batch Loss: 0.2609632909297943\n",
      "Epoch 4308, Loss: 0.932878702878952, Final Batch Loss: 0.1968085765838623\n",
      "Epoch 4309, Loss: 1.027056097984314, Final Batch Loss: 0.197501078248024\n",
      "Epoch 4310, Loss: 1.1231522262096405, Final Batch Loss: 0.3094577491283417\n",
      "Epoch 4311, Loss: 0.9731657654047012, Final Batch Loss: 0.2903205156326294\n",
      "Epoch 4312, Loss: 1.025747686624527, Final Batch Loss: 0.20688146352767944\n",
      "Epoch 4313, Loss: 1.0299843549728394, Final Batch Loss: 0.3286367356777191\n",
      "Epoch 4314, Loss: 0.813038557767868, Final Batch Loss: 0.16696912050247192\n",
      "Epoch 4315, Loss: 1.0170982033014297, Final Batch Loss: 0.23314231634140015\n",
      "Epoch 4316, Loss: 1.0548571944236755, Final Batch Loss: 0.263229101896286\n",
      "Epoch 4317, Loss: 1.1779836118221283, Final Batch Loss: 0.30313053727149963\n",
      "Epoch 4318, Loss: 1.0368406027555466, Final Batch Loss: 0.25613582134246826\n",
      "Epoch 4319, Loss: 0.9350389391183853, Final Batch Loss: 0.2607576251029968\n",
      "Epoch 4320, Loss: 0.9390560835599899, Final Batch Loss: 0.16645221412181854\n",
      "Epoch 4321, Loss: 0.9862959086894989, Final Batch Loss: 0.2151774913072586\n",
      "Epoch 4322, Loss: 0.9234902858734131, Final Batch Loss: 0.2033873349428177\n",
      "Epoch 4323, Loss: 0.9522330164909363, Final Batch Loss: 0.31319376826286316\n",
      "Epoch 4324, Loss: 1.0390522629022598, Final Batch Loss: 0.286756694316864\n",
      "Epoch 4325, Loss: 1.0262023359537125, Final Batch Loss: 0.30920279026031494\n",
      "Epoch 4326, Loss: 1.013109102845192, Final Batch Loss: 0.34028640389442444\n",
      "Epoch 4327, Loss: 0.9968380033969879, Final Batch Loss: 0.2172655165195465\n",
      "Epoch 4328, Loss: 1.0565936863422394, Final Batch Loss: 0.31915488839149475\n",
      "Epoch 4329, Loss: 1.0630079805850983, Final Batch Loss: 0.19694998860359192\n",
      "Epoch 4330, Loss: 0.8413756191730499, Final Batch Loss: 0.17678384482860565\n",
      "Epoch 4331, Loss: 1.0490165948867798, Final Batch Loss: 0.28203731775283813\n",
      "Epoch 4332, Loss: 1.0617020577192307, Final Batch Loss: 0.32741203904151917\n",
      "Epoch 4333, Loss: 0.9936293512582779, Final Batch Loss: 0.26828062534332275\n",
      "Epoch 4334, Loss: 0.974071130156517, Final Batch Loss: 0.2911573052406311\n",
      "Epoch 4335, Loss: 0.9783126711845398, Final Batch Loss: 0.2499920278787613\n",
      "Epoch 4336, Loss: 0.9930380582809448, Final Batch Loss: 0.26298263669013977\n",
      "Epoch 4337, Loss: 1.0350228697061539, Final Batch Loss: 0.15039433538913727\n",
      "Epoch 4338, Loss: 0.9420223385095596, Final Batch Loss: 0.272286593914032\n",
      "Epoch 4339, Loss: 1.0492995977401733, Final Batch Loss: 0.32519152760505676\n",
      "Epoch 4340, Loss: 0.8769753277301788, Final Batch Loss: 0.2207353264093399\n",
      "Epoch 4341, Loss: 0.939970389008522, Final Batch Loss: 0.18506218492984772\n",
      "Epoch 4342, Loss: 0.9374759942293167, Final Batch Loss: 0.3142499029636383\n",
      "Epoch 4343, Loss: 0.9069946706295013, Final Batch Loss: 0.19154955446720123\n",
      "Epoch 4344, Loss: 0.9522925466299057, Final Batch Loss: 0.24881210923194885\n",
      "Epoch 4345, Loss: 1.0759997516870499, Final Batch Loss: 0.3462253212928772\n",
      "Epoch 4346, Loss: 1.0619297921657562, Final Batch Loss: 0.27451223134994507\n",
      "Epoch 4347, Loss: 0.8361816704273224, Final Batch Loss: 0.191230908036232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4348, Loss: 0.9402071982622147, Final Batch Loss: 0.26379773020744324\n",
      "Epoch 4349, Loss: 0.9355310052633286, Final Batch Loss: 0.2468452751636505\n",
      "Epoch 4350, Loss: 0.9917721152305603, Final Batch Loss: 0.2613331973552704\n",
      "Epoch 4351, Loss: 0.954240545630455, Final Batch Loss: 0.21003788709640503\n",
      "Epoch 4352, Loss: 0.9721510410308838, Final Batch Loss: 0.26546943187713623\n",
      "Epoch 4353, Loss: 0.9499861598014832, Final Batch Loss: 0.21404282748699188\n",
      "Epoch 4354, Loss: 0.9747597426176071, Final Batch Loss: 0.2526894211769104\n",
      "Epoch 4355, Loss: 1.0226509049534798, Final Batch Loss: 0.12321127206087112\n",
      "Epoch 4356, Loss: 1.156408041715622, Final Batch Loss: 0.3641943335533142\n",
      "Epoch 4357, Loss: 1.064464420080185, Final Batch Loss: 0.24942949414253235\n",
      "Epoch 4358, Loss: 0.9058008641004562, Final Batch Loss: 0.24322597682476044\n",
      "Epoch 4359, Loss: 0.9895164966583252, Final Batch Loss: 0.2665809690952301\n",
      "Epoch 4360, Loss: 0.9141801446676254, Final Batch Loss: 0.21478231251239777\n",
      "Epoch 4361, Loss: 0.9578441977500916, Final Batch Loss: 0.26372745633125305\n",
      "Epoch 4362, Loss: 1.0282185524702072, Final Batch Loss: 0.2757726311683655\n",
      "Epoch 4363, Loss: 1.0104426145553589, Final Batch Loss: 0.3039132356643677\n",
      "Epoch 4364, Loss: 0.9927933514118195, Final Batch Loss: 0.19241908192634583\n",
      "Epoch 4365, Loss: 0.9641490429639816, Final Batch Loss: 0.20279809832572937\n",
      "Epoch 4366, Loss: 1.0805442780256271, Final Batch Loss: 0.29827815294265747\n",
      "Epoch 4367, Loss: 0.8902123272418976, Final Batch Loss: 0.18376240134239197\n",
      "Epoch 4368, Loss: 0.991886168718338, Final Batch Loss: 0.30938878655433655\n",
      "Epoch 4369, Loss: 0.8594500720500946, Final Batch Loss: 0.1527063548564911\n",
      "Epoch 4370, Loss: 1.011109784245491, Final Batch Loss: 0.2889064848423004\n",
      "Epoch 4371, Loss: 0.9608646333217621, Final Batch Loss: 0.24453715980052948\n",
      "Epoch 4372, Loss: 1.1805645525455475, Final Batch Loss: 0.45598816871643066\n",
      "Epoch 4373, Loss: 1.0509114414453506, Final Batch Loss: 0.3360997438430786\n",
      "Epoch 4374, Loss: 0.950652465224266, Final Batch Loss: 0.2754557132720947\n",
      "Epoch 4375, Loss: 1.0281475931406021, Final Batch Loss: 0.3041919767856598\n",
      "Epoch 4376, Loss: 0.9017884135246277, Final Batch Loss: 0.18291592597961426\n",
      "Epoch 4377, Loss: 1.0076835006475449, Final Batch Loss: 0.2640123963356018\n",
      "Epoch 4378, Loss: 0.9332928955554962, Final Batch Loss: 0.18187281489372253\n",
      "Epoch 4379, Loss: 0.9507846534252167, Final Batch Loss: 0.23257006704807281\n",
      "Epoch 4380, Loss: 0.8619239330291748, Final Batch Loss: 0.20610825717449188\n",
      "Epoch 4381, Loss: 0.9075485467910767, Final Batch Loss: 0.21400795876979828\n",
      "Epoch 4382, Loss: 0.8751836717128754, Final Batch Loss: 0.18839725852012634\n",
      "Epoch 4383, Loss: 0.9172517359256744, Final Batch Loss: 0.26490268111228943\n",
      "Epoch 4384, Loss: 1.095982939004898, Final Batch Loss: 0.24210171401500702\n",
      "Epoch 4385, Loss: 1.0784726291894913, Final Batch Loss: 0.19096557796001434\n",
      "Epoch 4386, Loss: 1.0663198381662369, Final Batch Loss: 0.32354307174682617\n",
      "Epoch 4387, Loss: 0.8826568573713303, Final Batch Loss: 0.16507235169410706\n",
      "Epoch 4388, Loss: 1.0109573304653168, Final Batch Loss: 0.27796220779418945\n",
      "Epoch 4389, Loss: 1.0491575598716736, Final Batch Loss: 0.2647673189640045\n",
      "Epoch 4390, Loss: 1.003109633922577, Final Batch Loss: 0.24555177986621857\n",
      "Epoch 4391, Loss: 1.0496129095554352, Final Batch Loss: 0.3268742561340332\n",
      "Epoch 4392, Loss: 0.9527387768030167, Final Batch Loss: 0.3680429458618164\n",
      "Epoch 4393, Loss: 0.9728173315525055, Final Batch Loss: 0.27616262435913086\n",
      "Epoch 4394, Loss: 1.0264558047056198, Final Batch Loss: 0.24823203682899475\n",
      "Epoch 4395, Loss: 0.9286395758390427, Final Batch Loss: 0.28822237253189087\n",
      "Epoch 4396, Loss: 0.9848673790693283, Final Batch Loss: 0.27576401829719543\n",
      "Epoch 4397, Loss: 0.9089673459529877, Final Batch Loss: 0.24659320712089539\n",
      "Epoch 4398, Loss: 1.0565882474184036, Final Batch Loss: 0.2883354127407074\n",
      "Epoch 4399, Loss: 0.9988452792167664, Final Batch Loss: 0.18395227193832397\n",
      "Epoch 4400, Loss: 0.9273261576890945, Final Batch Loss: 0.17860816419124603\n",
      "Epoch 4401, Loss: 1.0555942952632904, Final Batch Loss: 0.30443575978279114\n",
      "Epoch 4402, Loss: 1.0184756815433502, Final Batch Loss: 0.2545475661754608\n",
      "Epoch 4403, Loss: 0.9701600521802902, Final Batch Loss: 0.2431306093931198\n",
      "Epoch 4404, Loss: 0.9314402937889099, Final Batch Loss: 0.2973867952823639\n",
      "Epoch 4405, Loss: 0.9686916619539261, Final Batch Loss: 0.26806002855300903\n",
      "Epoch 4406, Loss: 0.952332392334938, Final Batch Loss: 0.2580755650997162\n",
      "Epoch 4407, Loss: 1.0154661536216736, Final Batch Loss: 0.18870072066783905\n",
      "Epoch 4408, Loss: 0.979735791683197, Final Batch Loss: 0.17367538809776306\n",
      "Epoch 4409, Loss: 0.969641387462616, Final Batch Loss: 0.24938003718852997\n",
      "Epoch 4410, Loss: 1.0601999908685684, Final Batch Loss: 0.22054167091846466\n",
      "Epoch 4411, Loss: 0.9422021210193634, Final Batch Loss: 0.18381915986537933\n",
      "Epoch 4412, Loss: 0.9778037518262863, Final Batch Loss: 0.22944486141204834\n",
      "Epoch 4413, Loss: 0.9707661271095276, Final Batch Loss: 0.28145310282707214\n",
      "Epoch 4414, Loss: 0.953141450881958, Final Batch Loss: 0.262686163187027\n",
      "Epoch 4415, Loss: 0.9314771890640259, Final Batch Loss: 0.18143020570278168\n",
      "Epoch 4416, Loss: 0.8917255252599716, Final Batch Loss: 0.20493310689926147\n",
      "Epoch 4417, Loss: 1.0043146759271622, Final Batch Loss: 0.21599075198173523\n",
      "Epoch 4418, Loss: 1.0932722687721252, Final Batch Loss: 0.2597824037075043\n",
      "Epoch 4419, Loss: 0.8872102499008179, Final Batch Loss: 0.18206073343753815\n",
      "Epoch 4420, Loss: 0.8659918457269669, Final Batch Loss: 0.21227183938026428\n",
      "Epoch 4421, Loss: 1.0655112862586975, Final Batch Loss: 0.3163294494152069\n",
      "Epoch 4422, Loss: 1.0205268263816833, Final Batch Loss: 0.3061462342739105\n",
      "Epoch 4423, Loss: 1.0534249991178513, Final Batch Loss: 0.290391743183136\n",
      "Epoch 4424, Loss: 0.9785793572664261, Final Batch Loss: 0.23521487414836884\n",
      "Epoch 4425, Loss: 0.9461294859647751, Final Batch Loss: 0.281058132648468\n",
      "Epoch 4426, Loss: 0.9512720555067062, Final Batch Loss: 0.2719278037548065\n",
      "Epoch 4427, Loss: 1.0426089465618134, Final Batch Loss: 0.20833958685398102\n",
      "Epoch 4428, Loss: 0.9726084619760513, Final Batch Loss: 0.26349470019340515\n",
      "Epoch 4429, Loss: 0.9587056487798691, Final Batch Loss: 0.2072301059961319\n",
      "Epoch 4430, Loss: 0.9710966348648071, Final Batch Loss: 0.3128407895565033\n",
      "Epoch 4431, Loss: 1.0601835250854492, Final Batch Loss: 0.28414806723594666\n",
      "Epoch 4432, Loss: 1.093589261174202, Final Batch Loss: 0.2560996413230896\n",
      "Epoch 4433, Loss: 0.9234980493783951, Final Batch Loss: 0.25891587138175964\n",
      "Epoch 4434, Loss: 0.8216046988964081, Final Batch Loss: 0.1409897804260254\n",
      "Epoch 4435, Loss: 1.0422593355178833, Final Batch Loss: 0.23849518597126007\n",
      "Epoch 4436, Loss: 0.9998603761196136, Final Batch Loss: 0.2695760428905487\n",
      "Epoch 4437, Loss: 1.041824832558632, Final Batch Loss: 0.2943553030490875\n",
      "Epoch 4438, Loss: 0.904580682516098, Final Batch Loss: 0.15086792409420013\n",
      "Epoch 4439, Loss: 1.0053019970655441, Final Batch Loss: 0.33815062046051025\n",
      "Epoch 4440, Loss: 0.8817751556634903, Final Batch Loss: 0.1948423534631729\n",
      "Epoch 4441, Loss: 0.9941354095935822, Final Batch Loss: 0.21178121864795685\n",
      "Epoch 4442, Loss: 0.9874420166015625, Final Batch Loss: 0.27314072847366333\n",
      "Epoch 4443, Loss: 0.9625630974769592, Final Batch Loss: 0.19663342833518982\n",
      "Epoch 4444, Loss: 1.0088073462247849, Final Batch Loss: 0.2522946000099182\n",
      "Epoch 4445, Loss: 1.0184991210699081, Final Batch Loss: 0.28072160482406616\n",
      "Epoch 4446, Loss: 1.045728862285614, Final Batch Loss: 0.33717748522758484\n",
      "Epoch 4447, Loss: 0.9782657027244568, Final Batch Loss: 0.2876158058643341\n",
      "Epoch 4448, Loss: 1.0331412106752396, Final Batch Loss: 0.2560825049877167\n",
      "Epoch 4449, Loss: 0.9483512938022614, Final Batch Loss: 0.1766720414161682\n",
      "Epoch 4450, Loss: 0.9757167398929596, Final Batch Loss: 0.21013130247592926\n",
      "Epoch 4451, Loss: 1.016315832734108, Final Batch Loss: 0.23845787346363068\n",
      "Epoch 4452, Loss: 0.905907541513443, Final Batch Loss: 0.28245019912719727\n",
      "Epoch 4453, Loss: 1.046475663781166, Final Batch Loss: 0.23876900970935822\n",
      "Epoch 4454, Loss: 1.0019821375608444, Final Batch Loss: 0.2444155216217041\n",
      "Epoch 4455, Loss: 0.833660289645195, Final Batch Loss: 0.1463300585746765\n",
      "Epoch 4456, Loss: 0.983483225107193, Final Batch Loss: 0.2190593034029007\n",
      "Epoch 4457, Loss: 1.0539093911647797, Final Batch Loss: 0.25615859031677246\n",
      "Epoch 4458, Loss: 1.1666302531957626, Final Batch Loss: 0.21877753734588623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4459, Loss: 0.9453077614307404, Final Batch Loss: 0.24262775480747223\n",
      "Epoch 4460, Loss: 0.8274197429418564, Final Batch Loss: 0.20240800082683563\n",
      "Epoch 4461, Loss: 1.035611480474472, Final Batch Loss: 0.1745840460062027\n",
      "Epoch 4462, Loss: 0.9585840851068497, Final Batch Loss: 0.2407800406217575\n",
      "Epoch 4463, Loss: 0.9360018223524094, Final Batch Loss: 0.24759022891521454\n",
      "Epoch 4464, Loss: 0.9841988831758499, Final Batch Loss: 0.21590827405452728\n",
      "Epoch 4465, Loss: 0.9707876741886139, Final Batch Loss: 0.23473595082759857\n",
      "Epoch 4466, Loss: 0.8973773717880249, Final Batch Loss: 0.1834559291601181\n",
      "Epoch 4467, Loss: 1.0438747555017471, Final Batch Loss: 0.2624290883541107\n",
      "Epoch 4468, Loss: 1.0841742753982544, Final Batch Loss: 0.2397134006023407\n",
      "Epoch 4469, Loss: 0.8909492790699005, Final Batch Loss: 0.23547305166721344\n",
      "Epoch 4470, Loss: 0.8640783578157425, Final Batch Loss: 0.18422508239746094\n",
      "Epoch 4471, Loss: 1.119960069656372, Final Batch Loss: 0.40486106276512146\n",
      "Epoch 4472, Loss: 1.011851578950882, Final Batch Loss: 0.29486772418022156\n",
      "Epoch 4473, Loss: 1.0636324286460876, Final Batch Loss: 0.24318040907382965\n",
      "Epoch 4474, Loss: 1.1122166365385056, Final Batch Loss: 0.29570215940475464\n",
      "Epoch 4475, Loss: 0.9836635887622833, Final Batch Loss: 0.21506096422672272\n",
      "Epoch 4476, Loss: 1.147577553987503, Final Batch Loss: 0.32138779759407043\n",
      "Epoch 4477, Loss: 1.051118090748787, Final Batch Loss: 0.26354625821113586\n",
      "Epoch 4478, Loss: 1.0539999157190323, Final Batch Loss: 0.2963119447231293\n",
      "Epoch 4479, Loss: 1.0421150624752045, Final Batch Loss: 0.23553624749183655\n",
      "Epoch 4480, Loss: 0.9905910789966583, Final Batch Loss: 0.25078076124191284\n",
      "Epoch 4481, Loss: 1.051914319396019, Final Batch Loss: 0.2857228219509125\n",
      "Epoch 4482, Loss: 1.0657127350568771, Final Batch Loss: 0.24418580532073975\n",
      "Epoch 4483, Loss: 1.0901750028133392, Final Batch Loss: 0.38940122723579407\n",
      "Epoch 4484, Loss: 0.9571168273687363, Final Batch Loss: 0.15107935667037964\n",
      "Epoch 4485, Loss: 0.9554119259119034, Final Batch Loss: 0.2070712447166443\n",
      "Epoch 4486, Loss: 0.9717207252979279, Final Batch Loss: 0.1920606791973114\n",
      "Epoch 4487, Loss: 0.8893932998180389, Final Batch Loss: 0.13778375089168549\n",
      "Epoch 4488, Loss: 0.9994660168886185, Final Batch Loss: 0.3184439241886139\n",
      "Epoch 4489, Loss: 0.9289027750492096, Final Batch Loss: 0.19172035157680511\n",
      "Epoch 4490, Loss: 0.8743520379066467, Final Batch Loss: 0.2513658404350281\n",
      "Epoch 4491, Loss: 0.9944176077842712, Final Batch Loss: 0.23632679879665375\n",
      "Epoch 4492, Loss: 1.0233666896820068, Final Batch Loss: 0.3088807761669159\n",
      "Epoch 4493, Loss: 1.04719278216362, Final Batch Loss: 0.29856014251708984\n",
      "Epoch 4494, Loss: 0.9417237937450409, Final Batch Loss: 0.16589152812957764\n",
      "Epoch 4495, Loss: 0.9191868752241135, Final Batch Loss: 0.23674404621124268\n",
      "Epoch 4496, Loss: 0.911573201417923, Final Batch Loss: 0.2716616690158844\n",
      "Epoch 4497, Loss: 1.0060998797416687, Final Batch Loss: 0.2658858001232147\n",
      "Epoch 4498, Loss: 1.0638457834720612, Final Batch Loss: 0.3395528197288513\n",
      "Epoch 4499, Loss: 0.9793517738580704, Final Batch Loss: 0.24441851675510406\n",
      "Epoch 4500, Loss: 1.155488297343254, Final Batch Loss: 0.3851754665374756\n",
      "Epoch 4501, Loss: 1.095774069428444, Final Batch Loss: 0.246760293841362\n",
      "Epoch 4502, Loss: 0.9009035676717758, Final Batch Loss: 0.15798285603523254\n",
      "Epoch 4503, Loss: 1.027975156903267, Final Batch Loss: 0.25629177689552307\n",
      "Epoch 4504, Loss: 0.9617770463228226, Final Batch Loss: 0.28682056069374084\n",
      "Epoch 4505, Loss: 0.9956933706998825, Final Batch Loss: 0.2865620255470276\n",
      "Epoch 4506, Loss: 0.9286472797393799, Final Batch Loss: 0.2695634067058563\n",
      "Epoch 4507, Loss: 0.9839559942483902, Final Batch Loss: 0.2428944706916809\n",
      "Epoch 4508, Loss: 1.0644032508134842, Final Batch Loss: 0.3552643954753876\n",
      "Epoch 4509, Loss: 1.0347445458173752, Final Batch Loss: 0.2962542176246643\n",
      "Epoch 4510, Loss: 0.9129290729761124, Final Batch Loss: 0.21286733448505402\n",
      "Epoch 4511, Loss: 1.0187337100505829, Final Batch Loss: 0.33336141705513\n",
      "Epoch 4512, Loss: 0.9121987670660019, Final Batch Loss: 0.1987101137638092\n",
      "Epoch 4513, Loss: 0.9431073665618896, Final Batch Loss: 0.19254742562770844\n",
      "Epoch 4514, Loss: 0.9224903434514999, Final Batch Loss: 0.27715519070625305\n",
      "Epoch 4515, Loss: 0.9846218824386597, Final Batch Loss: 0.2062039077281952\n",
      "Epoch 4516, Loss: 1.014421984553337, Final Batch Loss: 0.307209312915802\n",
      "Epoch 4517, Loss: 1.0259409099817276, Final Batch Loss: 0.3280876874923706\n",
      "Epoch 4518, Loss: 1.0568859726190567, Final Batch Loss: 0.35485824942588806\n",
      "Epoch 4519, Loss: 0.9540912955999374, Final Batch Loss: 0.18274016678333282\n",
      "Epoch 4520, Loss: 0.946735754609108, Final Batch Loss: 0.23398393392562866\n",
      "Epoch 4521, Loss: 0.9888142794370651, Final Batch Loss: 0.2852943539619446\n",
      "Epoch 4522, Loss: 0.9221826046705246, Final Batch Loss: 0.1896050125360489\n",
      "Epoch 4523, Loss: 0.8656666874885559, Final Batch Loss: 0.22683897614479065\n",
      "Epoch 4524, Loss: 0.9333585947751999, Final Batch Loss: 0.2688673138618469\n",
      "Epoch 4525, Loss: 1.0037184804677963, Final Batch Loss: 0.26428937911987305\n",
      "Epoch 4526, Loss: 1.0141240656375885, Final Batch Loss: 0.23702296614646912\n",
      "Epoch 4527, Loss: 0.9157209545373917, Final Batch Loss: 0.27034881711006165\n",
      "Epoch 4528, Loss: 1.0156221836805344, Final Batch Loss: 0.2581348121166229\n",
      "Epoch 4529, Loss: 0.89669169485569, Final Batch Loss: 0.24842053651809692\n",
      "Epoch 4530, Loss: 1.0569182336330414, Final Batch Loss: 0.21104776859283447\n",
      "Epoch 4531, Loss: 0.9842071533203125, Final Batch Loss: 0.28842926025390625\n",
      "Epoch 4532, Loss: 1.0239894688129425, Final Batch Loss: 0.38123461604118347\n",
      "Epoch 4533, Loss: 0.9512825608253479, Final Batch Loss: 0.22378580272197723\n",
      "Epoch 4534, Loss: 1.0645933598279953, Final Batch Loss: 0.3505515158176422\n",
      "Epoch 4535, Loss: 0.9865313172340393, Final Batch Loss: 0.24157105386257172\n",
      "Epoch 4536, Loss: 0.9302066564559937, Final Batch Loss: 0.2089095264673233\n",
      "Epoch 4537, Loss: 1.0304174721240997, Final Batch Loss: 0.2426304817199707\n",
      "Epoch 4538, Loss: 0.894223153591156, Final Batch Loss: 0.21085014939308167\n",
      "Epoch 4539, Loss: 0.9582711309194565, Final Batch Loss: 0.23182344436645508\n",
      "Epoch 4540, Loss: 0.9557499885559082, Final Batch Loss: 0.3055971562862396\n",
      "Epoch 4541, Loss: 1.0373197942972183, Final Batch Loss: 0.18313436210155487\n",
      "Epoch 4542, Loss: 1.047563225030899, Final Batch Loss: 0.2251621037721634\n",
      "Epoch 4543, Loss: 0.9478249102830887, Final Batch Loss: 0.23367157578468323\n",
      "Epoch 4544, Loss: 1.023595854640007, Final Batch Loss: 0.2238846868276596\n",
      "Epoch 4545, Loss: 0.9679873585700989, Final Batch Loss: 0.1694290190935135\n",
      "Epoch 4546, Loss: 0.8931835442781448, Final Batch Loss: 0.174483522772789\n",
      "Epoch 4547, Loss: 1.0399139672517776, Final Batch Loss: 0.29263001680374146\n",
      "Epoch 4548, Loss: 0.9684131294488907, Final Batch Loss: 0.14677666127681732\n",
      "Epoch 4549, Loss: 0.9771836698055267, Final Batch Loss: 0.21067577600479126\n",
      "Epoch 4550, Loss: 1.0133799761533737, Final Batch Loss: 0.27942168712615967\n",
      "Epoch 4551, Loss: 0.9831803441047668, Final Batch Loss: 0.30591365694999695\n",
      "Epoch 4552, Loss: 0.9771548509597778, Final Batch Loss: 0.26793810725212097\n",
      "Epoch 4553, Loss: 0.9241463243961334, Final Batch Loss: 0.20930622518062592\n",
      "Epoch 4554, Loss: 1.1234715729951859, Final Batch Loss: 0.39208099246025085\n",
      "Epoch 4555, Loss: 0.952949270606041, Final Batch Loss: 0.22541497647762299\n",
      "Epoch 4556, Loss: 0.8982948213815689, Final Batch Loss: 0.1451629102230072\n",
      "Epoch 4557, Loss: 1.043237715959549, Final Batch Loss: 0.25610819458961487\n",
      "Epoch 4558, Loss: 1.1497874557971954, Final Batch Loss: 0.3421800136566162\n",
      "Epoch 4559, Loss: 0.9883727580308914, Final Batch Loss: 0.2363000512123108\n",
      "Epoch 4560, Loss: 0.9634304195642471, Final Batch Loss: 0.223993718624115\n",
      "Epoch 4561, Loss: 0.97669318318367, Final Batch Loss: 0.23776990175247192\n",
      "Epoch 4562, Loss: 1.0015972703695297, Final Batch Loss: 0.22532115876674652\n",
      "Epoch 4563, Loss: 1.1090727746486664, Final Batch Loss: 0.2945440709590912\n",
      "Epoch 4564, Loss: 1.0011752545833588, Final Batch Loss: 0.25862130522727966\n",
      "Epoch 4565, Loss: 0.9582135230302811, Final Batch Loss: 0.21067316830158234\n",
      "Epoch 4566, Loss: 1.0665841102600098, Final Batch Loss: 0.19756115972995758\n",
      "Epoch 4567, Loss: 1.054386705160141, Final Batch Loss: 0.17758382856845856\n",
      "Epoch 4568, Loss: 0.864985927939415, Final Batch Loss: 0.17340761423110962\n",
      "Epoch 4569, Loss: 0.8171165585517883, Final Batch Loss: 0.16602906584739685\n",
      "Epoch 4570, Loss: 1.0357864648103714, Final Batch Loss: 0.2864611744880676\n",
      "Epoch 4571, Loss: 0.9392653405666351, Final Batch Loss: 0.21200518310070038\n",
      "Epoch 4572, Loss: 1.0366697162389755, Final Batch Loss: 0.2696154713630676\n",
      "Epoch 4573, Loss: 0.9776729196310043, Final Batch Loss: 0.2597004473209381\n",
      "Epoch 4574, Loss: 1.0268194675445557, Final Batch Loss: 0.25594010949134827\n",
      "Epoch 4575, Loss: 0.9846804291009903, Final Batch Loss: 0.264513224363327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4576, Loss: 0.9110293686389923, Final Batch Loss: 0.19173641502857208\n",
      "Epoch 4577, Loss: 0.9120796620845795, Final Batch Loss: 0.2838314473628998\n",
      "Epoch 4578, Loss: 0.8275155872106552, Final Batch Loss: 0.19811201095581055\n",
      "Epoch 4579, Loss: 1.0676774233579636, Final Batch Loss: 0.28164780139923096\n",
      "Epoch 4580, Loss: 0.9915880709886551, Final Batch Loss: 0.1771029382944107\n",
      "Epoch 4581, Loss: 0.9794144332408905, Final Batch Loss: 0.2490294724702835\n",
      "Epoch 4582, Loss: 1.0000580102205276, Final Batch Loss: 0.18161270022392273\n",
      "Epoch 4583, Loss: 0.9859844297170639, Final Batch Loss: 0.23213469982147217\n",
      "Epoch 4584, Loss: 1.0683077573776245, Final Batch Loss: 0.32380473613739014\n",
      "Epoch 4585, Loss: 0.9339355826377869, Final Batch Loss: 0.1950826197862625\n",
      "Epoch 4586, Loss: 1.0310859829187393, Final Batch Loss: 0.24700377881526947\n",
      "Epoch 4587, Loss: 1.0149421840906143, Final Batch Loss: 0.17883184552192688\n",
      "Epoch 4588, Loss: 0.9210377484560013, Final Batch Loss: 0.18367108702659607\n",
      "Epoch 4589, Loss: 0.9472705572843552, Final Batch Loss: 0.22854430973529816\n",
      "Epoch 4590, Loss: 0.937839925289154, Final Batch Loss: 0.1811075061559677\n",
      "Epoch 4591, Loss: 0.7747978568077087, Final Batch Loss: 0.1682739406824112\n",
      "Epoch 4592, Loss: 0.9324324727058411, Final Batch Loss: 0.22094209492206573\n",
      "Epoch 4593, Loss: 0.9400757700204849, Final Batch Loss: 0.18099437654018402\n",
      "Epoch 4594, Loss: 0.970840111374855, Final Batch Loss: 0.24533647298812866\n",
      "Epoch 4595, Loss: 0.9170226007699966, Final Batch Loss: 0.22195619344711304\n",
      "Epoch 4596, Loss: 0.9333185106515884, Final Batch Loss: 0.18131707608699799\n",
      "Epoch 4597, Loss: 0.9023151397705078, Final Batch Loss: 0.1757207065820694\n",
      "Epoch 4598, Loss: 0.8608189672231674, Final Batch Loss: 0.21905304491519928\n",
      "Epoch 4599, Loss: 1.1463777720928192, Final Batch Loss: 0.36923453211784363\n",
      "Epoch 4600, Loss: 1.0810919106006622, Final Batch Loss: 0.2315203845500946\n",
      "Epoch 4601, Loss: 0.9887453764677048, Final Batch Loss: 0.226423978805542\n",
      "Epoch 4602, Loss: 1.0256365686655045, Final Batch Loss: 0.2874480187892914\n",
      "Epoch 4603, Loss: 1.000160664319992, Final Batch Loss: 0.35697442293167114\n",
      "Epoch 4604, Loss: 0.9706581234931946, Final Batch Loss: 0.2794511914253235\n",
      "Epoch 4605, Loss: 0.9867506921291351, Final Batch Loss: 0.2566097676753998\n",
      "Epoch 4606, Loss: 0.9717501997947693, Final Batch Loss: 0.25695401430130005\n",
      "Epoch 4607, Loss: 0.8839868903160095, Final Batch Loss: 0.1586768627166748\n",
      "Epoch 4608, Loss: 1.0901823937892914, Final Batch Loss: 0.28848782181739807\n",
      "Epoch 4609, Loss: 0.9607359021902084, Final Batch Loss: 0.2612060308456421\n",
      "Epoch 4610, Loss: 0.9648051708936691, Final Batch Loss: 0.2155548632144928\n",
      "Epoch 4611, Loss: 0.9530504643917084, Final Batch Loss: 0.23551081120967865\n",
      "Epoch 4612, Loss: 0.9767596274614334, Final Batch Loss: 0.2552047073841095\n",
      "Epoch 4613, Loss: 1.037059098482132, Final Batch Loss: 0.2893793284893036\n",
      "Epoch 4614, Loss: 0.9190542250871658, Final Batch Loss: 0.16128520667552948\n",
      "Epoch 4615, Loss: 1.0306300669908524, Final Batch Loss: 0.3190760314464569\n",
      "Epoch 4616, Loss: 1.0097575187683105, Final Batch Loss: 0.2135193943977356\n",
      "Epoch 4617, Loss: 0.8774288594722748, Final Batch Loss: 0.1509675532579422\n",
      "Epoch 4618, Loss: 1.0167708992958069, Final Batch Loss: 0.3018021881580353\n",
      "Epoch 4619, Loss: 0.9328848421573639, Final Batch Loss: 0.25987380743026733\n",
      "Epoch 4620, Loss: 0.9309418499469757, Final Batch Loss: 0.21345865726470947\n",
      "Epoch 4621, Loss: 1.1377191841602325, Final Batch Loss: 0.2791460156440735\n",
      "Epoch 4622, Loss: 0.8894062638282776, Final Batch Loss: 0.15441513061523438\n",
      "Epoch 4623, Loss: 0.9647064954042435, Final Batch Loss: 0.34840619564056396\n",
      "Epoch 4624, Loss: 1.0599541813135147, Final Batch Loss: 0.2307785302400589\n",
      "Epoch 4625, Loss: 0.8074391633272171, Final Batch Loss: 0.17843040823936462\n",
      "Epoch 4626, Loss: 1.003327026963234, Final Batch Loss: 0.19980327785015106\n",
      "Epoch 4627, Loss: 1.0944142788648605, Final Batch Loss: 0.36710965633392334\n",
      "Epoch 4628, Loss: 0.9439966678619385, Final Batch Loss: 0.2600690424442291\n",
      "Epoch 4629, Loss: 0.9924209415912628, Final Batch Loss: 0.17607375979423523\n",
      "Epoch 4630, Loss: 1.0745124816894531, Final Batch Loss: 0.32587867975234985\n",
      "Epoch 4631, Loss: 0.9554174691438675, Final Batch Loss: 0.17974066734313965\n",
      "Epoch 4632, Loss: 0.9185146242380142, Final Batch Loss: 0.24250438809394836\n",
      "Epoch 4633, Loss: 0.9731675833463669, Final Batch Loss: 0.20986561477184296\n",
      "Epoch 4634, Loss: 0.930782213807106, Final Batch Loss: 0.17334850132465363\n",
      "Epoch 4635, Loss: 0.9180407971143723, Final Batch Loss: 0.18346822261810303\n",
      "Epoch 4636, Loss: 0.9105044156312943, Final Batch Loss: 0.3295567035675049\n",
      "Epoch 4637, Loss: 1.1574495434761047, Final Batch Loss: 0.27362924814224243\n",
      "Epoch 4638, Loss: 0.995187908411026, Final Batch Loss: 0.20768150687217712\n",
      "Epoch 4639, Loss: 0.9704846441745758, Final Batch Loss: 0.13321645557880402\n",
      "Epoch 4640, Loss: 0.9900105148553848, Final Batch Loss: 0.24315880239009857\n",
      "Epoch 4641, Loss: 0.8662935495376587, Final Batch Loss: 0.1685064435005188\n",
      "Epoch 4642, Loss: 1.0475030988454819, Final Batch Loss: 0.3096725344657898\n",
      "Epoch 4643, Loss: 0.8962963372468948, Final Batch Loss: 0.1722119003534317\n",
      "Epoch 4644, Loss: 1.1486756354570389, Final Batch Loss: 0.3682344853878021\n",
      "Epoch 4645, Loss: 0.9551379531621933, Final Batch Loss: 0.2427879422903061\n",
      "Epoch 4646, Loss: 1.004110038280487, Final Batch Loss: 0.23114162683486938\n",
      "Epoch 4647, Loss: 0.9866136014461517, Final Batch Loss: 0.2681306302547455\n",
      "Epoch 4648, Loss: 0.9824487417936325, Final Batch Loss: 0.24696065485477448\n",
      "Epoch 4649, Loss: 0.9493546932935715, Final Batch Loss: 0.20157885551452637\n",
      "Epoch 4650, Loss: 1.0176992118358612, Final Batch Loss: 0.2416045218706131\n",
      "Epoch 4651, Loss: 0.998868316411972, Final Batch Loss: 0.2607564926147461\n",
      "Epoch 4652, Loss: 0.9934353232383728, Final Batch Loss: 0.2612909972667694\n",
      "Epoch 4653, Loss: 0.953395202755928, Final Batch Loss: 0.19308406114578247\n",
      "Epoch 4654, Loss: 0.9524294435977936, Final Batch Loss: 0.2516691982746124\n",
      "Epoch 4655, Loss: 0.8834075480699539, Final Batch Loss: 0.2310429811477661\n",
      "Epoch 4656, Loss: 0.9063232839107513, Final Batch Loss: 0.20616160333156586\n",
      "Epoch 4657, Loss: 0.9417409598827362, Final Batch Loss: 0.17619265615940094\n",
      "Epoch 4658, Loss: 1.0292100608348846, Final Batch Loss: 0.3045627772808075\n",
      "Epoch 4659, Loss: 0.9789669811725616, Final Batch Loss: 0.24533063173294067\n",
      "Epoch 4660, Loss: 0.9236912131309509, Final Batch Loss: 0.18321926891803741\n",
      "Epoch 4661, Loss: 1.0515519827604294, Final Batch Loss: 0.27476850152015686\n",
      "Epoch 4662, Loss: 0.9907362759113312, Final Batch Loss: 0.2423122078180313\n",
      "Epoch 4663, Loss: 0.9181434512138367, Final Batch Loss: 0.22532644867897034\n",
      "Epoch 4664, Loss: 0.9531291127204895, Final Batch Loss: 0.25718602538108826\n",
      "Epoch 4665, Loss: 1.1133739352226257, Final Batch Loss: 0.28869226574897766\n",
      "Epoch 4666, Loss: 0.9152739644050598, Final Batch Loss: 0.23946477472782135\n",
      "Epoch 4667, Loss: 0.9532651603221893, Final Batch Loss: 0.20253342390060425\n",
      "Epoch 4668, Loss: 0.9087018966674805, Final Batch Loss: 0.2478078305721283\n",
      "Epoch 4669, Loss: 0.9485628455877304, Final Batch Loss: 0.2040369063615799\n",
      "Epoch 4670, Loss: 0.8884357064962387, Final Batch Loss: 0.19251476228237152\n",
      "Epoch 4671, Loss: 0.9305820912122726, Final Batch Loss: 0.21802964806556702\n",
      "Epoch 4672, Loss: 0.8377811163663864, Final Batch Loss: 0.183301642537117\n",
      "Epoch 4673, Loss: 1.0914685428142548, Final Batch Loss: 0.30639636516571045\n",
      "Epoch 4674, Loss: 1.0067153871059418, Final Batch Loss: 0.27265578508377075\n",
      "Epoch 4675, Loss: 1.0042602568864822, Final Batch Loss: 0.2188240885734558\n",
      "Epoch 4676, Loss: 0.9383425712585449, Final Batch Loss: 0.16243427991867065\n",
      "Epoch 4677, Loss: 0.9058481901884079, Final Batch Loss: 0.13180796802043915\n",
      "Epoch 4678, Loss: 0.9339553266763687, Final Batch Loss: 0.2678610384464264\n",
      "Epoch 4679, Loss: 0.8454468846321106, Final Batch Loss: 0.14505250751972198\n",
      "Epoch 4680, Loss: 0.9262029975652695, Final Batch Loss: 0.20434518158435822\n",
      "Epoch 4681, Loss: 1.0044886469841003, Final Batch Loss: 0.27385661005973816\n",
      "Epoch 4682, Loss: 0.9488850086927414, Final Batch Loss: 0.25086331367492676\n",
      "Epoch 4683, Loss: 0.9670972973108292, Final Batch Loss: 0.24215374886989594\n",
      "Epoch 4684, Loss: 0.9333341270685196, Final Batch Loss: 0.19929851591587067\n",
      "Epoch 4685, Loss: 1.020303636789322, Final Batch Loss: 0.25053608417510986\n",
      "Epoch 4686, Loss: 0.9273545891046524, Final Batch Loss: 0.2273387759923935\n",
      "Epoch 4687, Loss: 0.95329450070858, Final Batch Loss: 0.22823630273342133\n",
      "Epoch 4688, Loss: 0.9376345127820969, Final Batch Loss: 0.24818019568920135\n",
      "Epoch 4689, Loss: 0.9547000825405121, Final Batch Loss: 0.24098017811775208\n",
      "Epoch 4690, Loss: 0.9377598464488983, Final Batch Loss: 0.14026698470115662\n",
      "Epoch 4691, Loss: 0.9688327461481094, Final Batch Loss: 0.23206835985183716\n",
      "Epoch 4692, Loss: 0.925358772277832, Final Batch Loss: 0.1522330492734909\n",
      "Epoch 4693, Loss: 0.9635744690895081, Final Batch Loss: 0.22607168555259705\n",
      "Epoch 4694, Loss: 1.0631187707185745, Final Batch Loss: 0.25751301646232605\n",
      "Epoch 4695, Loss: 0.9618008434772491, Final Batch Loss: 0.2395382970571518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4696, Loss: 1.0644991844892502, Final Batch Loss: 0.3207704424858093\n",
      "Epoch 4697, Loss: 1.000443086028099, Final Batch Loss: 0.21438856422901154\n",
      "Epoch 4698, Loss: 0.8205740451812744, Final Batch Loss: 0.16167449951171875\n",
      "Epoch 4699, Loss: 0.9914487600326538, Final Batch Loss: 0.26340705156326294\n",
      "Epoch 4700, Loss: 1.0363260805606842, Final Batch Loss: 0.21262063086032867\n",
      "Epoch 4701, Loss: 0.9878551661968231, Final Batch Loss: 0.22764846682548523\n",
      "Epoch 4702, Loss: 0.9065843224525452, Final Batch Loss: 0.182831272482872\n",
      "Epoch 4703, Loss: 1.0129479616880417, Final Batch Loss: 0.3511291742324829\n",
      "Epoch 4704, Loss: 1.0198196321725845, Final Batch Loss: 0.2837240397930145\n",
      "Epoch 4705, Loss: 0.9205894768238068, Final Batch Loss: 0.2358156144618988\n",
      "Epoch 4706, Loss: 0.9138466268777847, Final Batch Loss: 0.19344808161258698\n",
      "Epoch 4707, Loss: 0.8642656356096268, Final Batch Loss: 0.23590721189975739\n",
      "Epoch 4708, Loss: 0.8912797421216965, Final Batch Loss: 0.21593789756298065\n",
      "Epoch 4709, Loss: 0.9611243009567261, Final Batch Loss: 0.2430177628993988\n",
      "Epoch 4710, Loss: 0.8514806479215622, Final Batch Loss: 0.13700009882450104\n",
      "Epoch 4711, Loss: 0.8015788793563843, Final Batch Loss: 0.18906208872795105\n",
      "Epoch 4712, Loss: 0.8880226314067841, Final Batch Loss: 0.24052150547504425\n",
      "Epoch 4713, Loss: 0.9160499274730682, Final Batch Loss: 0.22984199225902557\n",
      "Epoch 4714, Loss: 0.9144690036773682, Final Batch Loss: 0.2794594168663025\n",
      "Epoch 4715, Loss: 0.8792277276515961, Final Batch Loss: 0.28155526518821716\n",
      "Epoch 4716, Loss: 0.9125027060508728, Final Batch Loss: 0.19460880756378174\n",
      "Epoch 4717, Loss: 1.044607475399971, Final Batch Loss: 0.3364405632019043\n",
      "Epoch 4718, Loss: 1.0098077207803726, Final Batch Loss: 0.28997620940208435\n",
      "Epoch 4719, Loss: 0.957114115357399, Final Batch Loss: 0.2725461423397064\n",
      "Epoch 4720, Loss: 0.9645325988531113, Final Batch Loss: 0.32045918703079224\n",
      "Epoch 4721, Loss: 0.9983434230089188, Final Batch Loss: 0.19223183393478394\n",
      "Epoch 4722, Loss: 0.8934586346149445, Final Batch Loss: 0.21838442981243134\n",
      "Epoch 4723, Loss: 0.9511273801326752, Final Batch Loss: 0.2560073435306549\n",
      "Epoch 4724, Loss: 0.9097272753715515, Final Batch Loss: 0.2378343790769577\n",
      "Epoch 4725, Loss: 1.0350171625614166, Final Batch Loss: 0.2879461944103241\n",
      "Epoch 4726, Loss: 1.0001778900623322, Final Batch Loss: 0.2768699526786804\n",
      "Epoch 4727, Loss: 1.065307229757309, Final Batch Loss: 0.2729063630104065\n",
      "Epoch 4728, Loss: 0.9456934183835983, Final Batch Loss: 0.25545957684516907\n",
      "Epoch 4729, Loss: 0.891456350684166, Final Batch Loss: 0.23337256908416748\n",
      "Epoch 4730, Loss: 0.9550381600856781, Final Batch Loss: 0.22115302085876465\n",
      "Epoch 4731, Loss: 1.0036035925149918, Final Batch Loss: 0.19402068853378296\n",
      "Epoch 4732, Loss: 0.9870482534170151, Final Batch Loss: 0.24004508554935455\n",
      "Epoch 4733, Loss: 1.0445746630430222, Final Batch Loss: 0.25424712896347046\n",
      "Epoch 4734, Loss: 1.0848988145589828, Final Batch Loss: 0.30823618173599243\n",
      "Epoch 4735, Loss: 1.0026910454034805, Final Batch Loss: 0.2413572072982788\n",
      "Epoch 4736, Loss: 0.9020396173000336, Final Batch Loss: 0.22020752727985382\n",
      "Epoch 4737, Loss: 0.940566286444664, Final Batch Loss: 0.26283347606658936\n",
      "Epoch 4738, Loss: 0.9877173751592636, Final Batch Loss: 0.23338352143764496\n",
      "Epoch 4739, Loss: 0.9579795897006989, Final Batch Loss: 0.23319114744663239\n",
      "Epoch 4740, Loss: 0.9075420647859573, Final Batch Loss: 0.1693831980228424\n",
      "Epoch 4741, Loss: 0.9045339971780777, Final Batch Loss: 0.22829091548919678\n",
      "Epoch 4742, Loss: 0.9201480895280838, Final Batch Loss: 0.26309630274772644\n",
      "Epoch 4743, Loss: 0.9678348898887634, Final Batch Loss: 0.19844922423362732\n",
      "Epoch 4744, Loss: 1.0602982491254807, Final Batch Loss: 0.3319746255874634\n",
      "Epoch 4745, Loss: 0.9852249324321747, Final Batch Loss: 0.298198401927948\n",
      "Epoch 4746, Loss: 0.9632972031831741, Final Batch Loss: 0.27774253487586975\n",
      "Epoch 4747, Loss: 1.060484305024147, Final Batch Loss: 0.18704579770565033\n",
      "Epoch 4748, Loss: 0.9112604707479477, Final Batch Loss: 0.2763556241989136\n",
      "Epoch 4749, Loss: 1.0276754945516586, Final Batch Loss: 0.2790284752845764\n",
      "Epoch 4750, Loss: 1.0541530847549438, Final Batch Loss: 0.2653847932815552\n",
      "Epoch 4751, Loss: 0.9745658785104752, Final Batch Loss: 0.24489916861057281\n",
      "Epoch 4752, Loss: 1.0519766360521317, Final Batch Loss: 0.22997228801250458\n",
      "Epoch 4753, Loss: 0.8766571879386902, Final Batch Loss: 0.14029377698898315\n",
      "Epoch 4754, Loss: 0.880483090877533, Final Batch Loss: 0.2585245370864868\n",
      "Epoch 4755, Loss: 0.9312646985054016, Final Batch Loss: 0.24175016582012177\n",
      "Epoch 4756, Loss: 0.9759616404771805, Final Batch Loss: 0.26615768671035767\n",
      "Epoch 4757, Loss: 0.8708848804235458, Final Batch Loss: 0.2439962774515152\n",
      "Epoch 4758, Loss: 0.8400277495384216, Final Batch Loss: 0.11902754008769989\n",
      "Epoch 4759, Loss: 1.0028919130563736, Final Batch Loss: 0.27666646242141724\n",
      "Epoch 4760, Loss: 0.9135710150003433, Final Batch Loss: 0.17404207587242126\n",
      "Epoch 4761, Loss: 0.8946851193904877, Final Batch Loss: 0.21923723816871643\n",
      "Epoch 4762, Loss: 0.8798269778490067, Final Batch Loss: 0.1797921061515808\n",
      "Epoch 4763, Loss: 0.9849803596735001, Final Batch Loss: 0.29096537828445435\n",
      "Epoch 4764, Loss: 0.8389506489038467, Final Batch Loss: 0.20027600228786469\n",
      "Epoch 4765, Loss: 0.8841517567634583, Final Batch Loss: 0.1676105409860611\n",
      "Epoch 4766, Loss: 0.9919936805963516, Final Batch Loss: 0.30047228932380676\n",
      "Epoch 4767, Loss: 0.9324994832277298, Final Batch Loss: 0.2606865167617798\n",
      "Epoch 4768, Loss: 1.0713685154914856, Final Batch Loss: 0.26436612010002136\n",
      "Epoch 4769, Loss: 0.9153281450271606, Final Batch Loss: 0.23934762179851532\n",
      "Epoch 4770, Loss: 0.9847463220357895, Final Batch Loss: 0.20381905138492584\n",
      "Epoch 4771, Loss: 0.9800093621015549, Final Batch Loss: 0.21479558944702148\n",
      "Epoch 4772, Loss: 0.834328830242157, Final Batch Loss: 0.24560651183128357\n",
      "Epoch 4773, Loss: 1.0269549936056137, Final Batch Loss: 0.34417614340782166\n",
      "Epoch 4774, Loss: 0.916319727897644, Final Batch Loss: 0.1383679211139679\n",
      "Epoch 4775, Loss: 0.9932335466146469, Final Batch Loss: 0.2833881080150604\n",
      "Epoch 4776, Loss: 0.8751190900802612, Final Batch Loss: 0.18822816014289856\n",
      "Epoch 4777, Loss: 0.9137007594108582, Final Batch Loss: 0.20538823306560516\n",
      "Epoch 4778, Loss: 0.9246125668287277, Final Batch Loss: 0.1463860422372818\n",
      "Epoch 4779, Loss: 0.9431312531232834, Final Batch Loss: 0.25108301639556885\n",
      "Epoch 4780, Loss: 0.9729132950305939, Final Batch Loss: 0.27781370282173157\n",
      "Epoch 4781, Loss: 0.9600774347782135, Final Batch Loss: 0.2839698791503906\n",
      "Epoch 4782, Loss: 0.9266535490751266, Final Batch Loss: 0.22901180386543274\n",
      "Epoch 4783, Loss: 0.9461395293474197, Final Batch Loss: 0.1801495999097824\n",
      "Epoch 4784, Loss: 1.0526035577058792, Final Batch Loss: 0.24910229444503784\n",
      "Epoch 4785, Loss: 1.0120494961738586, Final Batch Loss: 0.2582056522369385\n",
      "Epoch 4786, Loss: 0.9420262575149536, Final Batch Loss: 0.19783160090446472\n",
      "Epoch 4787, Loss: 1.0323102474212646, Final Batch Loss: 0.28490808606147766\n",
      "Epoch 4788, Loss: 0.9719924330711365, Final Batch Loss: 0.2312111258506775\n",
      "Epoch 4789, Loss: 0.937117412686348, Final Batch Loss: 0.1792793869972229\n",
      "Epoch 4790, Loss: 0.9095204323530197, Final Batch Loss: 0.24795740842819214\n",
      "Epoch 4791, Loss: 1.0226655155420303, Final Batch Loss: 0.35188034176826477\n",
      "Epoch 4792, Loss: 1.0294934511184692, Final Batch Loss: 0.321771502494812\n",
      "Epoch 4793, Loss: 0.8765093237161636, Final Batch Loss: 0.1890605241060257\n",
      "Epoch 4794, Loss: 0.980336919426918, Final Batch Loss: 0.25265178084373474\n",
      "Epoch 4795, Loss: 0.8813202977180481, Final Batch Loss: 0.22853954136371613\n",
      "Epoch 4796, Loss: 0.9939399659633636, Final Batch Loss: 0.23296654224395752\n",
      "Epoch 4797, Loss: 1.0320464670658112, Final Batch Loss: 0.2837124764919281\n",
      "Epoch 4798, Loss: 0.963112398982048, Final Batch Loss: 0.2159852683544159\n",
      "Epoch 4799, Loss: 0.9254187941551208, Final Batch Loss: 0.194788858294487\n",
      "Epoch 4800, Loss: 0.9710680097341537, Final Batch Loss: 0.23029154539108276\n",
      "Epoch 4801, Loss: 1.041436567902565, Final Batch Loss: 0.2607259154319763\n",
      "Epoch 4802, Loss: 1.034635916352272, Final Batch Loss: 0.2614342272281647\n",
      "Epoch 4803, Loss: 0.9812971502542496, Final Batch Loss: 0.2187265306711197\n",
      "Epoch 4804, Loss: 1.0213888138532639, Final Batch Loss: 0.24690425395965576\n",
      "Epoch 4805, Loss: 0.9109561145305634, Final Batch Loss: 0.15495765209197998\n",
      "Epoch 4806, Loss: 1.0350597500801086, Final Batch Loss: 0.262882262468338\n",
      "Epoch 4807, Loss: 0.9531260579824448, Final Batch Loss: 0.23047809302806854\n",
      "Epoch 4808, Loss: 0.8477209359407425, Final Batch Loss: 0.1988028585910797\n",
      "Epoch 4809, Loss: 0.8877096176147461, Final Batch Loss: 0.16952428221702576\n",
      "Epoch 4810, Loss: 1.0075296312570572, Final Batch Loss: 0.2636090815067291\n",
      "Epoch 4811, Loss: 0.8958473652601242, Final Batch Loss: 0.2119290828704834\n",
      "Epoch 4812, Loss: 0.950896367430687, Final Batch Loss: 0.1805841028690338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4813, Loss: 1.0227623581886292, Final Batch Loss: 0.3798346519470215\n",
      "Epoch 4814, Loss: 1.0282149463891983, Final Batch Loss: 0.2850346267223358\n",
      "Epoch 4815, Loss: 0.9095686823129654, Final Batch Loss: 0.18505069613456726\n",
      "Epoch 4816, Loss: 0.9055114388465881, Final Batch Loss: 0.2294958382844925\n",
      "Epoch 4817, Loss: 0.9404466301202774, Final Batch Loss: 0.2381078451871872\n",
      "Epoch 4818, Loss: 0.9816135764122009, Final Batch Loss: 0.1830957531929016\n",
      "Epoch 4819, Loss: 1.1035866439342499, Final Batch Loss: 0.3373204469680786\n",
      "Epoch 4820, Loss: 0.9557192474603653, Final Batch Loss: 0.20804725587368011\n",
      "Epoch 4821, Loss: 0.9721901714801788, Final Batch Loss: 0.2718515992164612\n",
      "Epoch 4822, Loss: 0.9388190060853958, Final Batch Loss: 0.21827921271324158\n",
      "Epoch 4823, Loss: 0.9033713340759277, Final Batch Loss: 0.23375970125198364\n",
      "Epoch 4824, Loss: 0.9655564427375793, Final Batch Loss: 0.27653956413269043\n",
      "Epoch 4825, Loss: 0.8651275634765625, Final Batch Loss: 0.1910138726234436\n",
      "Epoch 4826, Loss: 0.8939498960971832, Final Batch Loss: 0.22052305936813354\n",
      "Epoch 4827, Loss: 0.9300645589828491, Final Batch Loss: 0.17550402879714966\n",
      "Epoch 4828, Loss: 0.9588100910186768, Final Batch Loss: 0.21808159351348877\n",
      "Epoch 4829, Loss: 1.001489743590355, Final Batch Loss: 0.2605595588684082\n",
      "Epoch 4830, Loss: 0.9449468553066254, Final Batch Loss: 0.1917746365070343\n",
      "Epoch 4831, Loss: 1.02408067882061, Final Batch Loss: 0.25546517968177795\n",
      "Epoch 4832, Loss: 0.9232467859983444, Final Batch Loss: 0.22838342189788818\n",
      "Epoch 4833, Loss: 0.9765833765268326, Final Batch Loss: 0.21518729627132416\n",
      "Epoch 4834, Loss: 1.039971113204956, Final Batch Loss: 0.29888615012168884\n",
      "Epoch 4835, Loss: 1.0134368687868118, Final Batch Loss: 0.25328460335731506\n",
      "Epoch 4836, Loss: 1.0636628568172455, Final Batch Loss: 0.22557303309440613\n",
      "Epoch 4837, Loss: 0.894573986530304, Final Batch Loss: 0.1915125846862793\n",
      "Epoch 4838, Loss: 0.9172177016735077, Final Batch Loss: 0.2619645893573761\n",
      "Epoch 4839, Loss: 1.009999766945839, Final Batch Loss: 0.24741855263710022\n",
      "Epoch 4840, Loss: 1.059938132762909, Final Batch Loss: 0.2725599408149719\n",
      "Epoch 4841, Loss: 0.8288872689008713, Final Batch Loss: 0.20640234649181366\n",
      "Epoch 4842, Loss: 0.9665352255105972, Final Batch Loss: 0.26809340715408325\n",
      "Epoch 4843, Loss: 0.8752773553133011, Final Batch Loss: 0.2199510931968689\n",
      "Epoch 4844, Loss: 0.9971450418233871, Final Batch Loss: 0.2505086064338684\n",
      "Epoch 4845, Loss: 1.0008305609226227, Final Batch Loss: 0.24955323338508606\n",
      "Epoch 4846, Loss: 0.8362545371055603, Final Batch Loss: 0.18036596477031708\n",
      "Epoch 4847, Loss: 0.9490954428911209, Final Batch Loss: 0.2619723677635193\n",
      "Epoch 4848, Loss: 0.9322128891944885, Final Batch Loss: 0.20881082117557526\n",
      "Epoch 4849, Loss: 0.8909247517585754, Final Batch Loss: 0.25129857659339905\n",
      "Epoch 4850, Loss: 0.9667409062385559, Final Batch Loss: 0.2932618260383606\n",
      "Epoch 4851, Loss: 0.9309418797492981, Final Batch Loss: 0.21738122403621674\n",
      "Epoch 4852, Loss: 0.9303054064512253, Final Batch Loss: 0.2203999161720276\n",
      "Epoch 4853, Loss: 0.9616965055465698, Final Batch Loss: 0.23677600920200348\n",
      "Epoch 4854, Loss: 0.9321285337209702, Final Batch Loss: 0.23293296992778778\n",
      "Epoch 4855, Loss: 0.9113672971725464, Final Batch Loss: 0.18986746668815613\n",
      "Epoch 4856, Loss: 1.0029521733522415, Final Batch Loss: 0.27344992756843567\n",
      "Epoch 4857, Loss: 0.9754279851913452, Final Batch Loss: 0.34928441047668457\n",
      "Epoch 4858, Loss: 0.9220975935459137, Final Batch Loss: 0.2179989069700241\n",
      "Epoch 4859, Loss: 1.0000487715005875, Final Batch Loss: 0.22491635382175446\n",
      "Epoch 4860, Loss: 0.9524527341127396, Final Batch Loss: 0.3064662516117096\n",
      "Epoch 4861, Loss: 0.9386685937643051, Final Batch Loss: 0.20447789132595062\n",
      "Epoch 4862, Loss: 0.8649848103523254, Final Batch Loss: 0.22468627989292145\n",
      "Epoch 4863, Loss: 0.9230301976203918, Final Batch Loss: 0.15292999148368835\n",
      "Epoch 4864, Loss: 0.9434746950864792, Final Batch Loss: 0.16309842467308044\n",
      "Epoch 4865, Loss: 0.9034397900104523, Final Batch Loss: 0.18507295846939087\n",
      "Epoch 4866, Loss: 1.0126540809869766, Final Batch Loss: 0.20856043696403503\n",
      "Epoch 4867, Loss: 0.8201505243778229, Final Batch Loss: 0.17739729583263397\n",
      "Epoch 4868, Loss: 0.9274933934211731, Final Batch Loss: 0.19615931808948517\n",
      "Epoch 4869, Loss: 0.9853741973638535, Final Batch Loss: 0.2602282464504242\n",
      "Epoch 4870, Loss: 0.9474076479673386, Final Batch Loss: 0.25054696202278137\n",
      "Epoch 4871, Loss: 0.9127407670021057, Final Batch Loss: 0.20148158073425293\n",
      "Epoch 4872, Loss: 0.9452868700027466, Final Batch Loss: 0.17880910634994507\n",
      "Epoch 4873, Loss: 0.9931009262800217, Final Batch Loss: 0.258192241191864\n",
      "Epoch 4874, Loss: 0.9795913994312286, Final Batch Loss: 0.30205315351486206\n",
      "Epoch 4875, Loss: 0.9821923822164536, Final Batch Loss: 0.19575457274913788\n",
      "Epoch 4876, Loss: 0.9096780568361282, Final Batch Loss: 0.24415430426597595\n",
      "Epoch 4877, Loss: 1.1080157607793808, Final Batch Loss: 0.32797005772590637\n",
      "Epoch 4878, Loss: 0.9168215692043304, Final Batch Loss: 0.30612701177597046\n",
      "Epoch 4879, Loss: 0.9750283509492874, Final Batch Loss: 0.21673154830932617\n",
      "Epoch 4880, Loss: 0.9950239956378937, Final Batch Loss: 0.2741641700267792\n",
      "Epoch 4881, Loss: 1.0139551162719727, Final Batch Loss: 0.42468515038490295\n",
      "Epoch 4882, Loss: 0.9916775226593018, Final Batch Loss: 0.19496789574623108\n",
      "Epoch 4883, Loss: 0.9825353622436523, Final Batch Loss: 0.2244347184896469\n",
      "Epoch 4884, Loss: 0.9505988657474518, Final Batch Loss: 0.27050846815109253\n",
      "Epoch 4885, Loss: 1.0556420534849167, Final Batch Loss: 0.25360584259033203\n",
      "Epoch 4886, Loss: 0.9035015851259232, Final Batch Loss: 0.18327005207538605\n",
      "Epoch 4887, Loss: 0.9866064786911011, Final Batch Loss: 0.18674854934215546\n",
      "Epoch 4888, Loss: 0.9000028371810913, Final Batch Loss: 0.21095527708530426\n",
      "Epoch 4889, Loss: 1.0511067509651184, Final Batch Loss: 0.31531187891960144\n",
      "Epoch 4890, Loss: 0.9717510640621185, Final Batch Loss: 0.26898908615112305\n",
      "Epoch 4891, Loss: 0.968891054391861, Final Batch Loss: 0.2648048400878906\n",
      "Epoch 4892, Loss: 1.0258657932281494, Final Batch Loss: 0.28914910554885864\n",
      "Epoch 4893, Loss: 0.9487976878881454, Final Batch Loss: 0.17713713645935059\n",
      "Epoch 4894, Loss: 0.9274320751428604, Final Batch Loss: 0.21334320306777954\n",
      "Epoch 4895, Loss: 0.9868907928466797, Final Batch Loss: 0.22475217282772064\n",
      "Epoch 4896, Loss: 0.9301165342330933, Final Batch Loss: 0.2128947675228119\n",
      "Epoch 4897, Loss: 0.9332340657711029, Final Batch Loss: 0.29608532786369324\n",
      "Epoch 4898, Loss: 1.010288119316101, Final Batch Loss: 0.23240946233272552\n",
      "Epoch 4899, Loss: 0.9087463915348053, Final Batch Loss: 0.27047449350357056\n",
      "Epoch 4900, Loss: 1.072087049484253, Final Batch Loss: 0.32303598523139954\n",
      "Epoch 4901, Loss: 0.9652882963418961, Final Batch Loss: 0.2536706030368805\n",
      "Epoch 4902, Loss: 0.9589527398347855, Final Batch Loss: 0.22798997163772583\n",
      "Epoch 4903, Loss: 0.9651565104722977, Final Batch Loss: 0.24336187541484833\n",
      "Epoch 4904, Loss: 0.9901543408632278, Final Batch Loss: 0.2510713040828705\n",
      "Epoch 4905, Loss: 0.8544947355985641, Final Batch Loss: 0.16080006957054138\n",
      "Epoch 4906, Loss: 0.995264932513237, Final Batch Loss: 0.23713353276252747\n",
      "Epoch 4907, Loss: 0.8847170025110245, Final Batch Loss: 0.23638896644115448\n",
      "Epoch 4908, Loss: 0.8570637255907059, Final Batch Loss: 0.2323533594608307\n",
      "Epoch 4909, Loss: 0.8754319697618484, Final Batch Loss: 0.20467866957187653\n",
      "Epoch 4910, Loss: 0.9303154051303864, Final Batch Loss: 0.27343130111694336\n",
      "Epoch 4911, Loss: 0.9345224946737289, Final Batch Loss: 0.16520468890666962\n",
      "Epoch 4912, Loss: 0.9115747958421707, Final Batch Loss: 0.20236428081989288\n",
      "Epoch 4913, Loss: 1.033218875527382, Final Batch Loss: 0.2935800850391388\n",
      "Epoch 4914, Loss: 0.9039653539657593, Final Batch Loss: 0.26955828070640564\n",
      "Epoch 4915, Loss: 0.8911532163619995, Final Batch Loss: 0.2009935975074768\n",
      "Epoch 4916, Loss: 0.9812862575054169, Final Batch Loss: 0.33746862411499023\n",
      "Epoch 4917, Loss: 1.0292723327875137, Final Batch Loss: 0.23200297355651855\n",
      "Epoch 4918, Loss: 0.9748301208019257, Final Batch Loss: 0.24810005724430084\n",
      "Epoch 4919, Loss: 0.9723260849714279, Final Batch Loss: 0.2626532316207886\n",
      "Epoch 4920, Loss: 0.9831310659646988, Final Batch Loss: 0.2737210988998413\n",
      "Epoch 4921, Loss: 1.0084479600191116, Final Batch Loss: 0.23884986340999603\n",
      "Epoch 4922, Loss: 0.9731565415859222, Final Batch Loss: 0.24313297867774963\n",
      "Epoch 4923, Loss: 0.8982150107622147, Final Batch Loss: 0.16744089126586914\n",
      "Epoch 4924, Loss: 1.0584652423858643, Final Batch Loss: 0.32091978192329407\n",
      "Epoch 4925, Loss: 0.984852135181427, Final Batch Loss: 0.2201094776391983\n",
      "Epoch 4926, Loss: 0.8891594409942627, Final Batch Loss: 0.20318573713302612\n",
      "Epoch 4927, Loss: 0.9324089288711548, Final Batch Loss: 0.22177478671073914\n",
      "Epoch 4928, Loss: 0.9384219944477081, Final Batch Loss: 0.21952010691165924\n",
      "Epoch 4929, Loss: 1.0146444737911224, Final Batch Loss: 0.26078230142593384\n",
      "Epoch 4930, Loss: 1.010079488158226, Final Batch Loss: 0.23154115676879883\n",
      "Epoch 4931, Loss: 0.8853889256715775, Final Batch Loss: 0.22150500118732452\n",
      "Epoch 4932, Loss: 0.9903415590524673, Final Batch Loss: 0.24992398917675018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4933, Loss: 0.9569321721792221, Final Batch Loss: 0.35427090525627136\n",
      "Epoch 4934, Loss: 0.9763572216033936, Final Batch Loss: 0.28869205713272095\n",
      "Epoch 4935, Loss: 1.0631952583789825, Final Batch Loss: 0.3210775852203369\n",
      "Epoch 4936, Loss: 0.8137627989053726, Final Batch Loss: 0.12086223065853119\n",
      "Epoch 4937, Loss: 1.036528766155243, Final Batch Loss: 0.2547396123409271\n",
      "Epoch 4938, Loss: 0.8986667096614838, Final Batch Loss: 0.1986042559146881\n",
      "Epoch 4939, Loss: 0.8900002837181091, Final Batch Loss: 0.19727250933647156\n",
      "Epoch 4940, Loss: 0.951569989323616, Final Batch Loss: 0.2181759476661682\n",
      "Epoch 4941, Loss: 0.8933685272932053, Final Batch Loss: 0.17293322086334229\n",
      "Epoch 4942, Loss: 0.90986368060112, Final Batch Loss: 0.2373729646205902\n",
      "Epoch 4943, Loss: 0.8912989348173141, Final Batch Loss: 0.22513650357723236\n",
      "Epoch 4944, Loss: 0.8741468787193298, Final Batch Loss: 0.10502266883850098\n",
      "Epoch 4945, Loss: 0.9248265475034714, Final Batch Loss: 0.2898330092430115\n",
      "Epoch 4946, Loss: 0.8662183880805969, Final Batch Loss: 0.17211906611919403\n",
      "Epoch 4947, Loss: 0.9233344197273254, Final Batch Loss: 0.2205701619386673\n",
      "Epoch 4948, Loss: 0.8542948216199875, Final Batch Loss: 0.207227885723114\n",
      "Epoch 4949, Loss: 0.8685588240623474, Final Batch Loss: 0.24577833712100983\n",
      "Epoch 4950, Loss: 1.0232100784778595, Final Batch Loss: 0.3126097023487091\n",
      "Epoch 4951, Loss: 0.8597349226474762, Final Batch Loss: 0.15375491976737976\n",
      "Epoch 4952, Loss: 0.8672644942998886, Final Batch Loss: 0.16799569129943848\n",
      "Epoch 4953, Loss: 0.9241727441549301, Final Batch Loss: 0.206955224275589\n",
      "Epoch 4954, Loss: 0.9834299683570862, Final Batch Loss: 0.2622557580471039\n",
      "Epoch 4955, Loss: 0.9269120544195175, Final Batch Loss: 0.23140640556812286\n",
      "Epoch 4956, Loss: 0.9751390367746353, Final Batch Loss: 0.2475956380367279\n",
      "Epoch 4957, Loss: 0.9795607179403305, Final Batch Loss: 0.22077569365501404\n",
      "Epoch 4958, Loss: 0.9549454748630524, Final Batch Loss: 0.28637924790382385\n",
      "Epoch 4959, Loss: 0.9003245085477829, Final Batch Loss: 0.20276860892772675\n",
      "Epoch 4960, Loss: 0.8758940547704697, Final Batch Loss: 0.19099771976470947\n",
      "Epoch 4961, Loss: 1.0436301231384277, Final Batch Loss: 0.2659653127193451\n",
      "Epoch 4962, Loss: 0.9255695790052414, Final Batch Loss: 0.2427021712064743\n",
      "Epoch 4963, Loss: 1.0325390845537186, Final Batch Loss: 0.24546952545642853\n",
      "Epoch 4964, Loss: 0.9129412770271301, Final Batch Loss: 0.257967472076416\n",
      "Epoch 4965, Loss: 0.9284703880548477, Final Batch Loss: 0.18332485854625702\n",
      "Epoch 4966, Loss: 0.9618625193834305, Final Batch Loss: 0.25469252467155457\n",
      "Epoch 4967, Loss: 0.8977812975645065, Final Batch Loss: 0.22711826860904694\n",
      "Epoch 4968, Loss: 0.9022359251976013, Final Batch Loss: 0.21583060920238495\n",
      "Epoch 4969, Loss: 0.995110809803009, Final Batch Loss: 0.2839953303337097\n",
      "Epoch 4970, Loss: 0.9363258630037308, Final Batch Loss: 0.21024517714977264\n",
      "Epoch 4971, Loss: 1.0191732943058014, Final Batch Loss: 0.26182758808135986\n",
      "Epoch 4972, Loss: 0.8600271493196487, Final Batch Loss: 0.21218064427375793\n",
      "Epoch 4973, Loss: 0.9778389483690262, Final Batch Loss: 0.27706095576286316\n",
      "Epoch 4974, Loss: 1.0972484797239304, Final Batch Loss: 0.3315328359603882\n",
      "Epoch 4975, Loss: 0.8258467316627502, Final Batch Loss: 0.1737818419933319\n",
      "Epoch 4976, Loss: 0.9209462553262711, Final Batch Loss: 0.25616028904914856\n",
      "Epoch 4977, Loss: 0.8923417329788208, Final Batch Loss: 0.21440215408802032\n",
      "Epoch 4978, Loss: 0.9876926690340042, Final Batch Loss: 0.2880096137523651\n",
      "Epoch 4979, Loss: 0.9784093946218491, Final Batch Loss: 0.26343491673469543\n",
      "Epoch 4980, Loss: 0.9482717961072922, Final Batch Loss: 0.26097944378852844\n",
      "Epoch 4981, Loss: 1.021143913269043, Final Batch Loss: 0.2985474169254303\n",
      "Epoch 4982, Loss: 0.887006938457489, Final Batch Loss: 0.197271928191185\n",
      "Epoch 4983, Loss: 0.8994626402854919, Final Batch Loss: 0.2293536365032196\n",
      "Epoch 4984, Loss: 0.9546787887811661, Final Batch Loss: 0.2510765492916107\n",
      "Epoch 4985, Loss: 1.0800896137952805, Final Batch Loss: 0.33482837677001953\n",
      "Epoch 4986, Loss: 1.0331611782312393, Final Batch Loss: 0.2228735238313675\n",
      "Epoch 4987, Loss: 1.012845128774643, Final Batch Loss: 0.2740800976753235\n",
      "Epoch 4988, Loss: 0.8930227160453796, Final Batch Loss: 0.2350711077451706\n",
      "Epoch 4989, Loss: 0.8673646748065948, Final Batch Loss: 0.20765160024166107\n",
      "Epoch 4990, Loss: 0.9653114229440689, Final Batch Loss: 0.21854308247566223\n",
      "Epoch 4991, Loss: 0.9432879686355591, Final Batch Loss: 0.24583955109119415\n",
      "Epoch 4992, Loss: 0.959286093711853, Final Batch Loss: 0.2101239114999771\n",
      "Epoch 4993, Loss: 0.8291978091001511, Final Batch Loss: 0.14847847819328308\n",
      "Epoch 4994, Loss: 1.0083013772964478, Final Batch Loss: 0.2892869710922241\n",
      "Epoch 4995, Loss: 0.994280606508255, Final Batch Loss: 0.2424517124891281\n",
      "Epoch 4996, Loss: 0.9355424791574478, Final Batch Loss: 0.20671489834785461\n",
      "Epoch 4997, Loss: 0.9409733563661575, Final Batch Loss: 0.25812268257141113\n",
      "Epoch 4998, Loss: 0.9063819050788879, Final Batch Loss: 0.21063101291656494\n",
      "Epoch 4999, Loss: 0.9592123329639435, Final Batch Loss: 0.3020014464855194\n",
      "Epoch 5000, Loss: 0.9086484909057617, Final Batch Loss: 0.2564908266067505\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  3  0  0  0  0  0  0  0  0  0  0  0  3  0  0  1]\n",
      " [ 0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 13  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0 10  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 13  0  0  0  0  0  0  0  0  2]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  1  0  0  5  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  4  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0  0  0  0  0  0  0  6  0  0  3  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 15  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        21\n",
      "           1    0.80000   1.00000   0.88889         8\n",
      "           2    0.80000   1.00000   0.88889         8\n",
      "           3    1.00000   1.00000   1.00000        10\n",
      "           4    0.90909   0.90909   0.90909        11\n",
      "           5    0.42857   0.37500   0.40000         8\n",
      "           6    1.00000   1.00000   1.00000         9\n",
      "           7    0.90000   1.00000   0.94737         9\n",
      "           8    1.00000   1.00000   1.00000         7\n",
      "           9    1.00000   1.00000   1.00000        13\n",
      "          10    0.90909   0.90909   0.90909        11\n",
      "          11    0.92857   0.86667   0.89655        15\n",
      "          12    1.00000   1.00000   1.00000         9\n",
      "          13    1.00000   0.71429   0.83333         7\n",
      "          14    0.57143   0.66667   0.61538        12\n",
      "          15    1.00000   1.00000   1.00000         7\n",
      "          16    1.00000   1.00000   1.00000         9\n",
      "          17    0.50000   0.30000   0.37500        10\n",
      "          18    1.00000   1.00000   1.00000        15\n",
      "          19    1.00000   1.00000   1.00000        12\n",
      "          20    0.70000   0.77778   0.73684         9\n",
      "\n",
      "    accuracy                        0.89091       220\n",
      "   macro avg    0.87842   0.88184   0.87621       220\n",
      "weighted avg    0.88825   0.89091   0.88635       220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_1 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_1 = np.zeros(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_2 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_2 = np.ones(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_3 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_3 = np.ones(n_samples) + 1\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_4 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_4 = np.ones(n_samples) + 2\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_5 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_5 = np.ones(n_samples) + 3\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_6 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_6 = np.ones(n_samples) + 4\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_7 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_7 = np.ones(n_samples) + 5\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_8 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_8 = np.ones(n_samples) + 6\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_9 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_9 = np.ones(n_samples) + 7\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U3A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_10 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_10 = np.ones(n_samples) + 8\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U3A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_11 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_11 = np.ones(n_samples) + 9\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U3A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_12 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_12 = np.ones(n_samples) + 10\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U4A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_13 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_13 = np.ones(n_samples) + 11\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U4A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_14 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_14 = np.ones(n_samples) + 12\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U4A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_15 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_15 = np.ones(n_samples) + 13\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U5A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_16 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_16 = np.ones(n_samples) + 14\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U5A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_17 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_17 = np.ones(n_samples) + 15\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U5A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_18 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_18 = np.ones(n_samples) + 16\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U6A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_19 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_19 = np.ones(n_samples) + 17\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U6A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_20 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_20 = np.ones(n_samples) + 18\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U6A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_21 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_21 = np.ones(n_samples) + 19\n",
    "\n",
    "fake_features = np.concatenate((fake_features_1, fake_features_2, fake_features_3, fake_features_4, fake_features_5, fake_features_6,\n",
    "                         fake_features_7, fake_features_8, fake_features_9, fake_features_10, fake_features_11, fake_features_12,\n",
    "                               fake_features_13, fake_features_14, fake_features_15, fake_features_16, fake_features_17, fake_features_18,\n",
    "                               fake_features_19, fake_features_20, fake_features_21))\n",
    "fake_labels = np.concatenate((y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9, y_10, y_11, y_12, y_13, y_14, y_15, y_16, y_17, y_18, y_19, y_20, y_21))\n",
    "\n",
    "fake_features = torch.Tensor(fake_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 18  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  1  0]\n",
      " [ 0  0 10  0  0  6  0  0  3  0  0  0  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  0 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 17  0  1  0  0  0  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  4  0  0  7  0  0  4  0  0  1  0  0  1  0  0  1  0  0  2]\n",
      " [ 0  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 17  0  0  3  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  5  0  0 11  0  0  3  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  2  0  0  9  8  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  2  0  0  1  0  0  0  0  0 17  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 14  0  0  3  0  0  2  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  3  4  0  6  7  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  3  0  0 17  0  0  0  0  0  0  0]\n",
      " [ 0  0 12  0  0  2  0  0  4  0  0  0  0  0  1  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 20  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  3  0  0  1  0  0  0  0  0 15  0  0  0  0]\n",
      " [ 0  0  3  0  0  5  0  0  4  0  0  0  0  0  2  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 20  0]\n",
      " [ 0  0  0  0  0  0  0  0  7  0  0  1  0  0  0  0  0  1  0  0 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    1.00000   1.00000   1.00000        20\n",
      "         1.0    0.90000   0.90000   0.90000        20\n",
      "         2.0    0.34483   0.50000   0.40816        20\n",
      "         3.0    1.00000   1.00000   1.00000        20\n",
      "         4.0    0.89474   0.85000   0.87179        20\n",
      "         5.0    0.17949   0.35000   0.23729        20\n",
      "         6.0    0.83333   1.00000   0.90909        20\n",
      "         7.0    0.85000   0.85000   0.85000        20\n",
      "         8.0    0.30556   0.55000   0.39286        20\n",
      "         9.0    0.75000   0.45000   0.56250        20\n",
      "        10.0    0.44737   0.85000   0.58621        20\n",
      "        11.0    0.28571   0.10000   0.14815        20\n",
      "        12.0    0.85714   0.30000   0.44444        20\n",
      "        13.0    0.70833   0.85000   0.77273        20\n",
      "        14.0    0.16667   0.05000   0.07692        20\n",
      "        15.0    1.00000   1.00000   1.00000        20\n",
      "        16.0    1.00000   0.75000   0.85714        20\n",
      "        17.0    0.66667   0.30000   0.41379        20\n",
      "        18.0    1.00000   1.00000   1.00000        20\n",
      "        19.0    0.95238   1.00000   0.97561        20\n",
      "        20.0    0.78571   0.55000   0.64706        20\n",
      "\n",
      "    accuracy                        0.67619       420\n",
      "   macro avg    0.71085   0.67619   0.66923       420\n",
      "weighted avg    0.71085   0.67619   0.66923       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5, zero_division = 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
