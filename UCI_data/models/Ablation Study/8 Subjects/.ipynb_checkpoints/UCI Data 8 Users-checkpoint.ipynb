{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '390 fBodyAccJerk-bandsEnergy()-1,16',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>42 tGravityAcc-mean()-Y</th>\n",
       "      <th>43 tGravityAcc-mean()-Z</th>\n",
       "      <th>51 tGravityAcc-max()-Y</th>\n",
       "      <th>52 tGravityAcc-max()-Z</th>\n",
       "      <th>54 tGravityAcc-min()-Y</th>\n",
       "      <th>55 tGravityAcc-min()-Z</th>\n",
       "      <th>56 tGravityAcc-sma()</th>\n",
       "      <th>58 tGravityAcc-energy()-Y</th>\n",
       "      <th>59 tGravityAcc-energy()-Z</th>\n",
       "      <th>475 fBodyGyro-bandsEnergy()-1,8</th>\n",
       "      <th>...</th>\n",
       "      <th>303 fBodyAcc-bandsEnergy()-1,8</th>\n",
       "      <th>311 fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <th>315 fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <th>382 fBodyAccJerk-bandsEnergy()-1,8</th>\n",
       "      <th>390 fBodyAccJerk-bandsEnergy()-1,16</th>\n",
       "      <th>504 fBodyAccMag-std()</th>\n",
       "      <th>505 fBodyAccMag-mad()</th>\n",
       "      <th>509 fBodyAccMag-energy()</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.140840</td>\n",
       "      <td>0.115375</td>\n",
       "      <td>-0.161265</td>\n",
       "      <td>0.124660</td>\n",
       "      <td>-0.123213</td>\n",
       "      <td>0.056483</td>\n",
       "      <td>-0.375426</td>\n",
       "      <td>-0.970905</td>\n",
       "      <td>-0.975510</td>\n",
       "      <td>-0.999454</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.999982</td>\n",
       "      <td>-0.956134</td>\n",
       "      <td>-0.948870</td>\n",
       "      <td>-0.998285</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.141551</td>\n",
       "      <td>0.109379</td>\n",
       "      <td>-0.161343</td>\n",
       "      <td>0.122586</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.383430</td>\n",
       "      <td>-0.970583</td>\n",
       "      <td>-0.978500</td>\n",
       "      <td>-0.999856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.999987</td>\n",
       "      <td>-0.975866</td>\n",
       "      <td>-0.975777</td>\n",
       "      <td>-0.999472</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.142010</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.094566</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.401602</td>\n",
       "      <td>-0.970368</td>\n",
       "      <td>-0.981672</td>\n",
       "      <td>-0.999954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.999972</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-0.989015</td>\n",
       "      <td>-0.985594</td>\n",
       "      <td>-0.999807</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.143976</td>\n",
       "      <td>0.099850</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.093425</td>\n",
       "      <td>-0.121336</td>\n",
       "      <td>0.095753</td>\n",
       "      <td>-0.400278</td>\n",
       "      <td>-0.969400</td>\n",
       "      <td>-0.982420</td>\n",
       "      <td>-0.999931</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-0.999978</td>\n",
       "      <td>-0.986742</td>\n",
       "      <td>-0.983524</td>\n",
       "      <td>-0.999770</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.148750</td>\n",
       "      <td>0.094486</td>\n",
       "      <td>-0.166786</td>\n",
       "      <td>0.091682</td>\n",
       "      <td>-0.121834</td>\n",
       "      <td>0.094059</td>\n",
       "      <td>-0.400477</td>\n",
       "      <td>-0.967051</td>\n",
       "      <td>-0.984363</td>\n",
       "      <td>-0.999926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-0.999988</td>\n",
       "      <td>-0.990063</td>\n",
       "      <td>-0.992324</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>-0.222004</td>\n",
       "      <td>-0.039492</td>\n",
       "      <td>-0.214233</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.071977</td>\n",
       "      <td>-0.405132</td>\n",
       "      <td>-0.918375</td>\n",
       "      <td>-0.995193</td>\n",
       "      <td>-0.053258</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.684177</td>\n",
       "      <td>-0.666429</td>\n",
       "      <td>-0.668164</td>\n",
       "      <td>-0.839256</td>\n",
       "      <td>-0.775736</td>\n",
       "      <td>-0.232600</td>\n",
       "      <td>-0.007392</td>\n",
       "      <td>-0.584282</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>-0.242054</td>\n",
       "      <td>-0.039863</td>\n",
       "      <td>-0.231477</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.358934</td>\n",
       "      <td>-0.902880</td>\n",
       "      <td>-0.995151</td>\n",
       "      <td>-0.029411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.726986</td>\n",
       "      <td>-0.704444</td>\n",
       "      <td>-0.705435</td>\n",
       "      <td>-0.854278</td>\n",
       "      <td>-0.780751</td>\n",
       "      <td>-0.275373</td>\n",
       "      <td>-0.172448</td>\n",
       "      <td>-0.632536</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>-0.236950</td>\n",
       "      <td>-0.026805</td>\n",
       "      <td>-0.249134</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.216004</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.377025</td>\n",
       "      <td>-0.907561</td>\n",
       "      <td>-0.995450</td>\n",
       "      <td>0.161404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.655263</td>\n",
       "      <td>-0.674515</td>\n",
       "      <td>-0.684729</td>\n",
       "      <td>-0.815380</td>\n",
       "      <td>-0.783616</td>\n",
       "      <td>-0.220288</td>\n",
       "      <td>-0.216074</td>\n",
       "      <td>-0.641170</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>-0.233230</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>-0.244267</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.210542</td>\n",
       "      <td>-0.040009</td>\n",
       "      <td>-0.440050</td>\n",
       "      <td>-0.910648</td>\n",
       "      <td>-0.998824</td>\n",
       "      <td>0.193585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.643425</td>\n",
       "      <td>-0.677215</td>\n",
       "      <td>-0.685088</td>\n",
       "      <td>-0.822905</td>\n",
       "      <td>-0.821137</td>\n",
       "      <td>-0.234539</td>\n",
       "      <td>-0.220443</td>\n",
       "      <td>-0.663579</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>-0.233292</td>\n",
       "      <td>-0.020954</td>\n",
       "      <td>-0.240956</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.212149</td>\n",
       "      <td>-0.047491</td>\n",
       "      <td>-0.432003</td>\n",
       "      <td>-0.910579</td>\n",
       "      <td>-0.998144</td>\n",
       "      <td>-0.129277</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.709495</td>\n",
       "      <td>-0.728519</td>\n",
       "      <td>-0.727441</td>\n",
       "      <td>-0.834215</td>\n",
       "      <td>-0.825848</td>\n",
       "      <td>-0.342670</td>\n",
       "      <td>-0.146649</td>\n",
       "      <td>-0.698087</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      42 tGravityAcc-mean()-Y  43 tGravityAcc-mean()-Z  \\\n",
       "0                   -0.140840                 0.115375   \n",
       "1                   -0.141551                 0.109379   \n",
       "2                   -0.142010                 0.101884   \n",
       "3                   -0.143976                 0.099850   \n",
       "4                   -0.148750                 0.094486   \n",
       "...                       ...                      ...   \n",
       "7347                -0.222004                -0.039492   \n",
       "7348                -0.242054                -0.039863   \n",
       "7349                -0.236950                -0.026805   \n",
       "7350                -0.233230                -0.004984   \n",
       "7351                -0.233292                -0.020954   \n",
       "\n",
       "      51 tGravityAcc-max()-Y  52 tGravityAcc-max()-Z  54 tGravityAcc-min()-Y  \\\n",
       "0                  -0.161265                0.124660               -0.123213   \n",
       "1                  -0.161343                0.122586               -0.114893   \n",
       "2                  -0.163711                0.094566               -0.114893   \n",
       "3                  -0.163711                0.093425               -0.121336   \n",
       "4                  -0.166786                0.091682               -0.121834   \n",
       "...                      ...                     ...                     ...   \n",
       "7347               -0.214233               -0.016391               -0.234998   \n",
       "7348               -0.231477               -0.016391               -0.234998   \n",
       "7349               -0.249134                0.024684               -0.216004   \n",
       "7350               -0.244267                0.024684               -0.210542   \n",
       "7351               -0.240956                0.003031               -0.212149   \n",
       "\n",
       "      55 tGravityAcc-min()-Z  56 tGravityAcc-sma()  58 tGravityAcc-energy()-Y  \\\n",
       "0                   0.056483             -0.375426                  -0.970905   \n",
       "1                   0.102764             -0.383430                  -0.970583   \n",
       "2                   0.102764             -0.401602                  -0.970368   \n",
       "3                   0.095753             -0.400278                  -0.969400   \n",
       "4                   0.094059             -0.400477                  -0.967051   \n",
       "...                      ...                   ...                        ...   \n",
       "7347               -0.071977             -0.405132                  -0.918375   \n",
       "7348               -0.068919             -0.358934                  -0.902880   \n",
       "7349               -0.068919             -0.377025                  -0.907561   \n",
       "7350               -0.040009             -0.440050                  -0.910648   \n",
       "7351               -0.047491             -0.432003                  -0.910579   \n",
       "\n",
       "      59 tGravityAcc-energy()-Z  475 fBodyGyro-bandsEnergy()-1,8  ...  \\\n",
       "0                     -0.975510                        -0.999454  ...   \n",
       "1                     -0.978500                        -0.999856  ...   \n",
       "2                     -0.981672                        -0.999954  ...   \n",
       "3                     -0.982420                        -0.999931  ...   \n",
       "4                     -0.984363                        -0.999926  ...   \n",
       "...                         ...                              ...  ...   \n",
       "7347                  -0.995193                        -0.053258  ...   \n",
       "7348                  -0.995151                        -0.029411  ...   \n",
       "7349                  -0.995450                         0.161404  ...   \n",
       "7350                  -0.998824                         0.193585  ...   \n",
       "7351                  -0.998144                        -0.129277  ...   \n",
       "\n",
       "      303 fBodyAcc-bandsEnergy()-1,8  311 fBodyAcc-bandsEnergy()-1,16  \\\n",
       "0                          -0.999963                        -0.999969   \n",
       "1                          -0.999996                        -0.999994   \n",
       "2                          -0.999989                        -0.999983   \n",
       "3                          -0.999989                        -0.999986   \n",
       "4                          -0.999994                        -0.999993   \n",
       "...                              ...                              ...   \n",
       "7347                       -0.684177                        -0.666429   \n",
       "7348                       -0.726986                        -0.704444   \n",
       "7349                       -0.655263                        -0.674515   \n",
       "7350                       -0.643425                        -0.677215   \n",
       "7351                       -0.709495                        -0.728519   \n",
       "\n",
       "      315 fBodyAcc-bandsEnergy()-1,24  382 fBodyAccJerk-bandsEnergy()-1,8  \\\n",
       "0                           -0.999971                           -0.999986   \n",
       "1                           -0.999992                           -0.999996   \n",
       "2                           -0.999972                           -0.999994   \n",
       "3                           -0.999977                           -0.999998   \n",
       "4                           -0.999991                           -0.999995   \n",
       "...                               ...                                 ...   \n",
       "7347                        -0.668164                           -0.839256   \n",
       "7348                        -0.705435                           -0.854278   \n",
       "7349                        -0.684729                           -0.815380   \n",
       "7350                        -0.685088                           -0.822905   \n",
       "7351                        -0.727441                           -0.834215   \n",
       "\n",
       "      390 fBodyAccJerk-bandsEnergy()-1,16  504 fBodyAccMag-std()  \\\n",
       "0                               -0.999982              -0.956134   \n",
       "1                               -0.999987              -0.975866   \n",
       "2                               -0.999963              -0.989015   \n",
       "3                               -0.999978              -0.986742   \n",
       "4                               -0.999988              -0.990063   \n",
       "...                                   ...                    ...   \n",
       "7347                            -0.775736              -0.232600   \n",
       "7348                            -0.780751              -0.275373   \n",
       "7349                            -0.783616              -0.220288   \n",
       "7350                            -0.821137              -0.234539   \n",
       "7351                            -0.825848              -0.342670   \n",
       "\n",
       "      505 fBodyAccMag-mad()  509 fBodyAccMag-energy()  Subject  Activity  \n",
       "0                 -0.948870                 -0.998285        1         5  \n",
       "1                 -0.975777                 -0.999472        1         5  \n",
       "2                 -0.985594                 -0.999807        1         5  \n",
       "3                 -0.983524                 -0.999770        1         5  \n",
       "4                 -0.992324                 -0.999873        1         5  \n",
       "...                     ...                       ...      ...       ...  \n",
       "7347              -0.007392                 -0.584282       30         2  \n",
       "7348              -0.172448                 -0.632536       30         2  \n",
       "7349              -0.216074                 -0.641170       30         2  \n",
       "7350              -0.220443                 -0.663579       30         2  \n",
       "7351              -0.146649                 -0.698087       30         2  \n",
       "\n",
       "[7352 rows x 34 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_names = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "train_column_names = train_names.values.tolist()\n",
    "train_column_names = [k for row in train_column_names for k in row]\n",
    "\n",
    "train_data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None)\n",
    "train_data.columns = train_column_names\n",
    "\n",
    "### Single dataframe column\n",
    "\n",
    "y_train = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "y_train.columns = ['Subject']\n",
    "\n",
    "y_train_activity = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "y_train_activity.columns = ['Activity']\n",
    "\n",
    "X_train_1 = train_data[sub_features]\n",
    "X_train_2 = train_data[act_features]\n",
    "X_train_data = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "X_train_data = pd.concat([X_train_data, y_train, y_train_activity], axis = 1)\n",
    "X_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_data[(X_train_data['Subject'].isin([1, 3, 5, 7, 8, 11, 14, 17])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,:-2].values\n",
    "y_train = X_train_data[(X_train_data['Subject'].isin([1, 3, 5, 7, 8, 11, 14, 17])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,-2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y_train)):\n",
    "    if y_train[k] == 1:\n",
    "        y_train[k] = 0\n",
    "    elif y_train[k] == 3:\n",
    "        y_train[k] = 1\n",
    "    elif y_train[k] == 5:\n",
    "        y_train[k] = 2\n",
    "    elif y_train[k] == 7:\n",
    "        y_train[k] = 3\n",
    "    elif y_train[k] == 8:\n",
    "        y_train[k] = 4\n",
    "    elif y_train[k] == 11:\n",
    "        y_train[k] = 5\n",
    "    elif y_train[k] == 14:\n",
    "        y_train[k] = 6\n",
    "    else:\n",
    "        y_train[k] = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 30),\n",
    "            classifier_block(30, 20),\n",
    "            classifier_block(20, 20),\n",
    "            classifier_block(20, 10),\n",
    "            nn.Linear(10, 8)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 7500\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 10.451120376586914, Final Batch Loss: 2.0854156017303467\n",
      "Epoch 2, Loss: 10.434607744216919, Final Batch Loss: 2.0763769149780273\n",
      "Epoch 3, Loss: 10.415525674819946, Final Batch Loss: 2.0608227252960205\n",
      "Epoch 4, Loss: 10.42675495147705, Final Batch Loss: 2.0851809978485107\n",
      "Epoch 5, Loss: 10.428563833236694, Final Batch Loss: 2.0999138355255127\n",
      "Epoch 6, Loss: 10.41072130203247, Final Batch Loss: 2.084634780883789\n",
      "Epoch 7, Loss: 10.406402826309204, Final Batch Loss: 2.087407350540161\n",
      "Epoch 8, Loss: 10.36923336982727, Final Batch Loss: 2.049072265625\n",
      "Epoch 9, Loss: 10.362582921981812, Final Batch Loss: 2.048518657684326\n",
      "Epoch 10, Loss: 10.35703420639038, Final Batch Loss: 2.064284563064575\n",
      "Epoch 11, Loss: 10.348132610321045, Final Batch Loss: 2.0672247409820557\n",
      "Epoch 12, Loss: 10.308364152908325, Final Batch Loss: 2.044891119003296\n",
      "Epoch 13, Loss: 10.27846884727478, Final Batch Loss: 2.0461950302124023\n",
      "Epoch 14, Loss: 10.248409748077393, Final Batch Loss: 2.0616307258605957\n",
      "Epoch 15, Loss: 10.214615821838379, Final Batch Loss: 2.0719568729400635\n",
      "Epoch 16, Loss: 10.12495732307434, Final Batch Loss: 2.0293519496917725\n",
      "Epoch 17, Loss: 10.056708812713623, Final Batch Loss: 2.015460729598999\n",
      "Epoch 18, Loss: 9.920344233512878, Final Batch Loss: 1.9828846454620361\n",
      "Epoch 19, Loss: 9.780134201049805, Final Batch Loss: 1.9603887796401978\n",
      "Epoch 20, Loss: 9.68088686466217, Final Batch Loss: 2.0090274810791016\n",
      "Epoch 21, Loss: 9.530007481575012, Final Batch Loss: 1.9217818975448608\n",
      "Epoch 22, Loss: 9.25576639175415, Final Batch Loss: 1.7402938604354858\n",
      "Epoch 23, Loss: 9.215209245681763, Final Batch Loss: 1.7939231395721436\n",
      "Epoch 24, Loss: 9.026159167289734, Final Batch Loss: 1.8355255126953125\n",
      "Epoch 25, Loss: 8.982706546783447, Final Batch Loss: 1.8010045289993286\n",
      "Epoch 26, Loss: 8.83008098602295, Final Batch Loss: 1.7280668020248413\n",
      "Epoch 27, Loss: 8.674000859260559, Final Batch Loss: 1.6125578880310059\n",
      "Epoch 28, Loss: 8.691878914833069, Final Batch Loss: 1.8286423683166504\n",
      "Epoch 29, Loss: 8.56630003452301, Final Batch Loss: 1.748901605606079\n",
      "Epoch 30, Loss: 8.475532531738281, Final Batch Loss: 1.67938232421875\n",
      "Epoch 31, Loss: 8.401985764503479, Final Batch Loss: 1.742025375366211\n",
      "Epoch 32, Loss: 8.305385947227478, Final Batch Loss: 1.677795171737671\n",
      "Epoch 33, Loss: 8.192672371864319, Final Batch Loss: 1.7177985906600952\n",
      "Epoch 34, Loss: 8.117220997810364, Final Batch Loss: 1.633200764656067\n",
      "Epoch 35, Loss: 7.969682335853577, Final Batch Loss: 1.5622124671936035\n",
      "Epoch 36, Loss: 7.903348326683044, Final Batch Loss: 1.6084671020507812\n",
      "Epoch 37, Loss: 7.675623178482056, Final Batch Loss: 1.3837236166000366\n",
      "Epoch 38, Loss: 7.85970401763916, Final Batch Loss: 1.6579537391662598\n",
      "Epoch 39, Loss: 7.560887336730957, Final Batch Loss: 1.4466832876205444\n",
      "Epoch 40, Loss: 7.655228495597839, Final Batch Loss: 1.5624147653579712\n",
      "Epoch 41, Loss: 7.3881179094314575, Final Batch Loss: 1.3980287313461304\n",
      "Epoch 42, Loss: 7.522976517677307, Final Batch Loss: 1.5066584348678589\n",
      "Epoch 43, Loss: 7.389745116233826, Final Batch Loss: 1.5505090951919556\n",
      "Epoch 44, Loss: 7.247429132461548, Final Batch Loss: 1.4104483127593994\n",
      "Epoch 45, Loss: 7.166602373123169, Final Batch Loss: 1.3823018074035645\n",
      "Epoch 46, Loss: 7.137588739395142, Final Batch Loss: 1.3903049230575562\n",
      "Epoch 47, Loss: 7.038336515426636, Final Batch Loss: 1.3703949451446533\n",
      "Epoch 48, Loss: 6.997668981552124, Final Batch Loss: 1.4532074928283691\n",
      "Epoch 49, Loss: 6.957592248916626, Final Batch Loss: 1.409506916999817\n",
      "Epoch 50, Loss: 6.757441401481628, Final Batch Loss: 1.161355972290039\n",
      "Epoch 51, Loss: 6.800804615020752, Final Batch Loss: 1.375302791595459\n",
      "Epoch 52, Loss: 6.831855773925781, Final Batch Loss: 1.5548826456069946\n",
      "Epoch 53, Loss: 6.796781659126282, Final Batch Loss: 1.2756565809249878\n",
      "Epoch 54, Loss: 6.747067451477051, Final Batch Loss: 1.3483409881591797\n",
      "Epoch 55, Loss: 6.619172692298889, Final Batch Loss: 1.1832475662231445\n",
      "Epoch 56, Loss: 6.552573084831238, Final Batch Loss: 1.260714054107666\n",
      "Epoch 57, Loss: 6.754627346992493, Final Batch Loss: 1.4092820882797241\n",
      "Epoch 58, Loss: 6.599161505699158, Final Batch Loss: 1.4013352394104004\n",
      "Epoch 59, Loss: 6.496773362159729, Final Batch Loss: 1.2659550905227661\n",
      "Epoch 60, Loss: 6.585198163986206, Final Batch Loss: 1.311384677886963\n",
      "Epoch 61, Loss: 6.522871732711792, Final Batch Loss: 1.3094865083694458\n",
      "Epoch 62, Loss: 6.5189448595047, Final Batch Loss: 1.4195277690887451\n",
      "Epoch 63, Loss: 6.387349843978882, Final Batch Loss: 1.3187206983566284\n",
      "Epoch 64, Loss: 6.4402687549591064, Final Batch Loss: 1.3191280364990234\n",
      "Epoch 65, Loss: 6.331701874732971, Final Batch Loss: 1.4004476070404053\n",
      "Epoch 66, Loss: 6.199075937271118, Final Batch Loss: 1.1983859539031982\n",
      "Epoch 67, Loss: 6.451133370399475, Final Batch Loss: 1.439327359199524\n",
      "Epoch 68, Loss: 6.162783980369568, Final Batch Loss: 1.2279691696166992\n",
      "Epoch 69, Loss: 6.186882257461548, Final Batch Loss: 1.1410058736801147\n",
      "Epoch 70, Loss: 6.0618250370025635, Final Batch Loss: 1.1817350387573242\n",
      "Epoch 71, Loss: 6.156485915184021, Final Batch Loss: 1.1482504606246948\n",
      "Epoch 72, Loss: 6.169628620147705, Final Batch Loss: 1.2751432657241821\n",
      "Epoch 73, Loss: 6.162653684616089, Final Batch Loss: 1.137756586074829\n",
      "Epoch 74, Loss: 5.9819194078445435, Final Batch Loss: 1.1932538747787476\n",
      "Epoch 75, Loss: 6.146628141403198, Final Batch Loss: 1.1377354860305786\n",
      "Epoch 76, Loss: 5.967707276344299, Final Batch Loss: 1.163896083831787\n",
      "Epoch 77, Loss: 6.047646164894104, Final Batch Loss: 1.1092857122421265\n",
      "Epoch 78, Loss: 6.099460244178772, Final Batch Loss: 1.3045498132705688\n",
      "Epoch 79, Loss: 6.154674768447876, Final Batch Loss: 1.3642146587371826\n",
      "Epoch 80, Loss: 5.975390195846558, Final Batch Loss: 1.182207703590393\n",
      "Epoch 81, Loss: 5.750330924987793, Final Batch Loss: 1.0636441707611084\n",
      "Epoch 82, Loss: 6.001120448112488, Final Batch Loss: 1.2868374586105347\n",
      "Epoch 83, Loss: 5.800397276878357, Final Batch Loss: 1.1340672969818115\n",
      "Epoch 84, Loss: 6.084203481674194, Final Batch Loss: 1.2777628898620605\n",
      "Epoch 85, Loss: 5.876579523086548, Final Batch Loss: 1.1255584955215454\n",
      "Epoch 86, Loss: 5.667741179466248, Final Batch Loss: 0.933952808380127\n",
      "Epoch 87, Loss: 5.7807759046554565, Final Batch Loss: 1.14830482006073\n",
      "Epoch 88, Loss: 5.720656871795654, Final Batch Loss: 1.1694046258926392\n",
      "Epoch 89, Loss: 5.79544985294342, Final Batch Loss: 1.175793170928955\n",
      "Epoch 90, Loss: 5.850673079490662, Final Batch Loss: 1.2195695638656616\n",
      "Epoch 91, Loss: 5.697211980819702, Final Batch Loss: 1.145566701889038\n",
      "Epoch 92, Loss: 5.885179877281189, Final Batch Loss: 1.2838091850280762\n",
      "Epoch 93, Loss: 5.812854528427124, Final Batch Loss: 1.1575185060501099\n",
      "Epoch 94, Loss: 5.798337936401367, Final Batch Loss: 1.2308663129806519\n",
      "Epoch 95, Loss: 5.584547400474548, Final Batch Loss: 1.006325602531433\n",
      "Epoch 96, Loss: 5.634374976158142, Final Batch Loss: 1.1687382459640503\n",
      "Epoch 97, Loss: 5.458260536193848, Final Batch Loss: 1.1018766164779663\n",
      "Epoch 98, Loss: 5.600787162780762, Final Batch Loss: 1.1860557794570923\n",
      "Epoch 99, Loss: 5.768130540847778, Final Batch Loss: 1.2541877031326294\n",
      "Epoch 100, Loss: 5.502780079841614, Final Batch Loss: 1.0359435081481934\n",
      "Epoch 101, Loss: 5.619704604148865, Final Batch Loss: 1.0425035953521729\n",
      "Epoch 102, Loss: 5.359027028083801, Final Batch Loss: 0.9594314098358154\n",
      "Epoch 103, Loss: 5.567260503768921, Final Batch Loss: 1.0968042612075806\n",
      "Epoch 104, Loss: 5.626302242279053, Final Batch Loss: 1.2168352603912354\n",
      "Epoch 105, Loss: 5.284296631813049, Final Batch Loss: 0.9033471345901489\n",
      "Epoch 106, Loss: 5.4581886529922485, Final Batch Loss: 1.139373779296875\n",
      "Epoch 107, Loss: 5.288062870502472, Final Batch Loss: 0.8561814427375793\n",
      "Epoch 108, Loss: 5.197324097156525, Final Batch Loss: 0.9375451803207397\n",
      "Epoch 109, Loss: 5.172799646854401, Final Batch Loss: 0.8023143410682678\n",
      "Epoch 110, Loss: 5.436661601066589, Final Batch Loss: 1.0386414527893066\n",
      "Epoch 111, Loss: 5.312867045402527, Final Batch Loss: 1.0539839267730713\n",
      "Epoch 112, Loss: 5.3361011147499084, Final Batch Loss: 1.0407545566558838\n",
      "Epoch 113, Loss: 5.2614951729774475, Final Batch Loss: 0.8905368447303772\n",
      "Epoch 114, Loss: 5.434748351573944, Final Batch Loss: 1.015048623085022\n",
      "Epoch 115, Loss: 5.389694809913635, Final Batch Loss: 1.0964550971984863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116, Loss: 5.271439433097839, Final Batch Loss: 1.0930789709091187\n",
      "Epoch 117, Loss: 5.246322512626648, Final Batch Loss: 0.9507216215133667\n",
      "Epoch 118, Loss: 5.305257678031921, Final Batch Loss: 1.0360406637191772\n",
      "Epoch 119, Loss: 5.272317111492157, Final Batch Loss: 0.9735918641090393\n",
      "Epoch 120, Loss: 5.291934669017792, Final Batch Loss: 1.177836537361145\n",
      "Epoch 121, Loss: 5.321067810058594, Final Batch Loss: 1.1020652055740356\n",
      "Epoch 122, Loss: 5.435362637042999, Final Batch Loss: 1.262297511100769\n",
      "Epoch 123, Loss: 5.171481072902679, Final Batch Loss: 1.044782042503357\n",
      "Epoch 124, Loss: 5.177020192146301, Final Batch Loss: 1.0284429788589478\n",
      "Epoch 125, Loss: 5.213142991065979, Final Batch Loss: 1.0414268970489502\n",
      "Epoch 126, Loss: 5.026154935359955, Final Batch Loss: 0.8485777378082275\n",
      "Epoch 127, Loss: 5.382373213768005, Final Batch Loss: 1.1993417739868164\n",
      "Epoch 128, Loss: 5.222932934761047, Final Batch Loss: 1.1710737943649292\n",
      "Epoch 129, Loss: 5.037299692630768, Final Batch Loss: 0.8380200266838074\n",
      "Epoch 130, Loss: 5.205108940601349, Final Batch Loss: 1.0295430421829224\n",
      "Epoch 131, Loss: 5.202841281890869, Final Batch Loss: 1.0299702882766724\n",
      "Epoch 132, Loss: 5.082583963871002, Final Batch Loss: 1.0626548528671265\n",
      "Epoch 133, Loss: 5.085303604602814, Final Batch Loss: 0.9565766453742981\n",
      "Epoch 134, Loss: 5.119089424610138, Final Batch Loss: 1.0522034168243408\n",
      "Epoch 135, Loss: 5.096634864807129, Final Batch Loss: 1.0549209117889404\n",
      "Epoch 136, Loss: 4.9685962200164795, Final Batch Loss: 0.9890294075012207\n",
      "Epoch 137, Loss: 5.1080352663993835, Final Batch Loss: 1.0408093929290771\n",
      "Epoch 138, Loss: 5.043833434581757, Final Batch Loss: 0.9536681771278381\n",
      "Epoch 139, Loss: 5.0398993492126465, Final Batch Loss: 1.085052490234375\n",
      "Epoch 140, Loss: 4.8161144852638245, Final Batch Loss: 0.8127834796905518\n",
      "Epoch 141, Loss: 4.865641295909882, Final Batch Loss: 0.8858510851860046\n",
      "Epoch 142, Loss: 4.957449913024902, Final Batch Loss: 1.0043209791183472\n",
      "Epoch 143, Loss: 5.041553795337677, Final Batch Loss: 1.0560506582260132\n",
      "Epoch 144, Loss: 5.1578856110572815, Final Batch Loss: 1.2295517921447754\n",
      "Epoch 145, Loss: 4.97199672460556, Final Batch Loss: 0.9422858357429504\n",
      "Epoch 146, Loss: 4.7603535652160645, Final Batch Loss: 0.9072996973991394\n",
      "Epoch 147, Loss: 4.784347116947174, Final Batch Loss: 0.9370932579040527\n",
      "Epoch 148, Loss: 4.838176667690277, Final Batch Loss: 0.9083269834518433\n",
      "Epoch 149, Loss: 5.003135919570923, Final Batch Loss: 1.097394347190857\n",
      "Epoch 150, Loss: 4.980454742908478, Final Batch Loss: 0.9799260497093201\n",
      "Epoch 151, Loss: 4.790688872337341, Final Batch Loss: 0.9150012135505676\n",
      "Epoch 152, Loss: 4.897791266441345, Final Batch Loss: 1.0237070322036743\n",
      "Epoch 153, Loss: 4.9343589544296265, Final Batch Loss: 0.8773514628410339\n",
      "Epoch 154, Loss: 4.869664132595062, Final Batch Loss: 0.9961893558502197\n",
      "Epoch 155, Loss: 4.696797966957092, Final Batch Loss: 0.9592230916023254\n",
      "Epoch 156, Loss: 4.850437760353088, Final Batch Loss: 1.044298529624939\n",
      "Epoch 157, Loss: 4.749072194099426, Final Batch Loss: 0.9984710216522217\n",
      "Epoch 158, Loss: 4.7805774211883545, Final Batch Loss: 0.9574729800224304\n",
      "Epoch 159, Loss: 4.851931035518646, Final Batch Loss: 0.9921320080757141\n",
      "Epoch 160, Loss: 4.883731305599213, Final Batch Loss: 1.0366077423095703\n",
      "Epoch 161, Loss: 4.779939770698547, Final Batch Loss: 0.8541073203086853\n",
      "Epoch 162, Loss: 4.770546913146973, Final Batch Loss: 0.8874680399894714\n",
      "Epoch 163, Loss: 5.206251919269562, Final Batch Loss: 1.195672869682312\n",
      "Epoch 164, Loss: 4.758564233779907, Final Batch Loss: 0.9220549464225769\n",
      "Epoch 165, Loss: 4.742790222167969, Final Batch Loss: 0.9204896092414856\n",
      "Epoch 166, Loss: 4.722914040088654, Final Batch Loss: 1.0386121273040771\n",
      "Epoch 167, Loss: 4.921940743923187, Final Batch Loss: 1.0774582624435425\n",
      "Epoch 168, Loss: 4.768248677253723, Final Batch Loss: 0.9721707701683044\n",
      "Epoch 169, Loss: 4.8642547726631165, Final Batch Loss: 1.070631980895996\n",
      "Epoch 170, Loss: 4.865133225917816, Final Batch Loss: 0.9957692623138428\n",
      "Epoch 171, Loss: 4.779338538646698, Final Batch Loss: 0.9607222676277161\n",
      "Epoch 172, Loss: 4.812801361083984, Final Batch Loss: 0.9322539567947388\n",
      "Epoch 173, Loss: 4.674321949481964, Final Batch Loss: 0.9470911622047424\n",
      "Epoch 174, Loss: 4.911555647850037, Final Batch Loss: 1.0942133665084839\n",
      "Epoch 175, Loss: 4.666272819042206, Final Batch Loss: 0.9293125867843628\n",
      "Epoch 176, Loss: 4.564693748950958, Final Batch Loss: 0.7718299031257629\n",
      "Epoch 177, Loss: 4.74620658159256, Final Batch Loss: 1.0932233333587646\n",
      "Epoch 178, Loss: 4.864247143268585, Final Batch Loss: 1.097625970840454\n",
      "Epoch 179, Loss: 4.680475890636444, Final Batch Loss: 0.8685987591743469\n",
      "Epoch 180, Loss: 4.651829779148102, Final Batch Loss: 1.053562045097351\n",
      "Epoch 181, Loss: 4.628120183944702, Final Batch Loss: 0.9667782187461853\n",
      "Epoch 182, Loss: 4.549646735191345, Final Batch Loss: 0.8132061958312988\n",
      "Epoch 183, Loss: 4.738885819911957, Final Batch Loss: 0.9880614876747131\n",
      "Epoch 184, Loss: 4.464366853237152, Final Batch Loss: 0.7906728386878967\n",
      "Epoch 185, Loss: 4.581774652004242, Final Batch Loss: 0.789421796798706\n",
      "Epoch 186, Loss: 4.441408336162567, Final Batch Loss: 0.8204680681228638\n",
      "Epoch 187, Loss: 4.543545246124268, Final Batch Loss: 0.787296712398529\n",
      "Epoch 188, Loss: 4.608901023864746, Final Batch Loss: 0.8325061798095703\n",
      "Epoch 189, Loss: 4.44768214225769, Final Batch Loss: 0.8275068998336792\n",
      "Epoch 190, Loss: 4.554037868976593, Final Batch Loss: 0.9181527495384216\n",
      "Epoch 191, Loss: 4.646265745162964, Final Batch Loss: 0.9898858070373535\n",
      "Epoch 192, Loss: 4.59754604101181, Final Batch Loss: 0.931059718132019\n",
      "Epoch 193, Loss: 4.574544608592987, Final Batch Loss: 0.8906012177467346\n",
      "Epoch 194, Loss: 4.419657468795776, Final Batch Loss: 0.925001859664917\n",
      "Epoch 195, Loss: 4.52820897102356, Final Batch Loss: 0.896041750907898\n",
      "Epoch 196, Loss: 4.511855065822601, Final Batch Loss: 0.8426052927970886\n",
      "Epoch 197, Loss: 4.381927907466888, Final Batch Loss: 0.8097512125968933\n",
      "Epoch 198, Loss: 4.603776335716248, Final Batch Loss: 0.9341182708740234\n",
      "Epoch 199, Loss: 4.633208513259888, Final Batch Loss: 0.9169268012046814\n",
      "Epoch 200, Loss: 4.619793176651001, Final Batch Loss: 1.0680311918258667\n",
      "Epoch 201, Loss: 4.304288864135742, Final Batch Loss: 0.7544112801551819\n",
      "Epoch 202, Loss: 4.427137196063995, Final Batch Loss: 0.8748671412467957\n",
      "Epoch 203, Loss: 4.404428839683533, Final Batch Loss: 0.8413193225860596\n",
      "Epoch 204, Loss: 4.295840859413147, Final Batch Loss: 0.8247215747833252\n",
      "Epoch 205, Loss: 4.370040059089661, Final Batch Loss: 0.7863590717315674\n",
      "Epoch 206, Loss: 4.523639798164368, Final Batch Loss: 0.9188283085823059\n",
      "Epoch 207, Loss: 4.393517255783081, Final Batch Loss: 0.8153115510940552\n",
      "Epoch 208, Loss: 4.500491797924042, Final Batch Loss: 0.8754091262817383\n",
      "Epoch 209, Loss: 4.449105501174927, Final Batch Loss: 0.9069778919219971\n",
      "Epoch 210, Loss: 4.419835031032562, Final Batch Loss: 0.8578802347183228\n",
      "Epoch 211, Loss: 4.594489753246307, Final Batch Loss: 1.2064894437789917\n",
      "Epoch 212, Loss: 4.585534632205963, Final Batch Loss: 1.0482256412506104\n",
      "Epoch 213, Loss: 4.403819918632507, Final Batch Loss: 0.8728386759757996\n",
      "Epoch 214, Loss: 4.328251600265503, Final Batch Loss: 0.8426728844642639\n",
      "Epoch 215, Loss: 4.340258896350861, Final Batch Loss: 0.8764835596084595\n",
      "Epoch 216, Loss: 4.455485224723816, Final Batch Loss: 0.9616172909736633\n",
      "Epoch 217, Loss: 4.382918119430542, Final Batch Loss: 0.8775039315223694\n",
      "Epoch 218, Loss: 4.294880211353302, Final Batch Loss: 0.8225882053375244\n",
      "Epoch 219, Loss: 4.375201940536499, Final Batch Loss: 0.9140129685401917\n",
      "Epoch 220, Loss: 4.43587589263916, Final Batch Loss: 1.018149971961975\n",
      "Epoch 221, Loss: 4.358959138393402, Final Batch Loss: 0.8556768298149109\n",
      "Epoch 222, Loss: 4.505936682224274, Final Batch Loss: 0.9356293082237244\n",
      "Epoch 223, Loss: 4.273958265781403, Final Batch Loss: 0.7794340252876282\n",
      "Epoch 224, Loss: 4.263215363025665, Final Batch Loss: 0.9396628141403198\n",
      "Epoch 225, Loss: 4.2485591173172, Final Batch Loss: 0.6259728670120239\n",
      "Epoch 226, Loss: 4.295292258262634, Final Batch Loss: 0.7647804021835327\n",
      "Epoch 227, Loss: 4.163227915763855, Final Batch Loss: 0.8456453680992126\n",
      "Epoch 228, Loss: 4.181103944778442, Final Batch Loss: 0.7838435769081116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229, Loss: 4.374252259731293, Final Batch Loss: 0.8590809106826782\n",
      "Epoch 230, Loss: 4.355055510997772, Final Batch Loss: 0.8693065643310547\n",
      "Epoch 231, Loss: 4.498254418373108, Final Batch Loss: 0.9895437955856323\n",
      "Epoch 232, Loss: 4.07526171207428, Final Batch Loss: 0.7165518403053284\n",
      "Epoch 233, Loss: 4.500253438949585, Final Batch Loss: 0.8732845783233643\n",
      "Epoch 234, Loss: 4.366224825382233, Final Batch Loss: 0.916135311126709\n",
      "Epoch 235, Loss: 4.245026171207428, Final Batch Loss: 0.7540739178657532\n",
      "Epoch 236, Loss: 4.3342713713645935, Final Batch Loss: 0.9028483629226685\n",
      "Epoch 237, Loss: 4.1990421414375305, Final Batch Loss: 0.6235476136207581\n",
      "Epoch 238, Loss: 4.249397099018097, Final Batch Loss: 0.7882069945335388\n",
      "Epoch 239, Loss: 4.234814465045929, Final Batch Loss: 0.8373515009880066\n",
      "Epoch 240, Loss: 4.190457761287689, Final Batch Loss: 0.7601671814918518\n",
      "Epoch 241, Loss: 4.387048304080963, Final Batch Loss: 0.7884288430213928\n",
      "Epoch 242, Loss: 4.064850389957428, Final Batch Loss: 0.7277273535728455\n",
      "Epoch 243, Loss: 4.228612661361694, Final Batch Loss: 0.7554945945739746\n",
      "Epoch 244, Loss: 4.221020758152008, Final Batch Loss: 0.775119423866272\n",
      "Epoch 245, Loss: 4.195095360279083, Final Batch Loss: 0.8319905996322632\n",
      "Epoch 246, Loss: 4.253228724002838, Final Batch Loss: 0.8675075173377991\n",
      "Epoch 247, Loss: 4.28470641374588, Final Batch Loss: 0.8780322670936584\n",
      "Epoch 248, Loss: 4.208198606967926, Final Batch Loss: 0.8873986601829529\n",
      "Epoch 249, Loss: 4.1900283098220825, Final Batch Loss: 0.8112357258796692\n",
      "Epoch 250, Loss: 4.325033962726593, Final Batch Loss: 0.9205929636955261\n",
      "Epoch 251, Loss: 4.262685775756836, Final Batch Loss: 0.8476589322090149\n",
      "Epoch 252, Loss: 4.331806898117065, Final Batch Loss: 0.8134797215461731\n",
      "Epoch 253, Loss: 4.2688300013542175, Final Batch Loss: 0.9140792489051819\n",
      "Epoch 254, Loss: 4.35192346572876, Final Batch Loss: 0.9163987040519714\n",
      "Epoch 255, Loss: 4.256627202033997, Final Batch Loss: 0.9187636971473694\n",
      "Epoch 256, Loss: 4.276171445846558, Final Batch Loss: 0.8542969226837158\n",
      "Epoch 257, Loss: 4.195198059082031, Final Batch Loss: 1.0062382221221924\n",
      "Epoch 258, Loss: 4.184757590293884, Final Batch Loss: 0.9226170182228088\n",
      "Epoch 259, Loss: 4.112595558166504, Final Batch Loss: 0.8785821199417114\n",
      "Epoch 260, Loss: 4.115292549133301, Final Batch Loss: 0.7473532557487488\n",
      "Epoch 261, Loss: 4.233429729938507, Final Batch Loss: 0.7725067734718323\n",
      "Epoch 262, Loss: 4.087252736091614, Final Batch Loss: 0.8557949066162109\n",
      "Epoch 263, Loss: 4.237591624259949, Final Batch Loss: 0.8847851157188416\n",
      "Epoch 264, Loss: 4.192449271678925, Final Batch Loss: 0.8702768087387085\n",
      "Epoch 265, Loss: 4.098952651023865, Final Batch Loss: 0.6965920329093933\n",
      "Epoch 266, Loss: 4.132918119430542, Final Batch Loss: 0.9377570152282715\n",
      "Epoch 267, Loss: 4.129304885864258, Final Batch Loss: 0.8399257063865662\n",
      "Epoch 268, Loss: 4.159829258918762, Final Batch Loss: 0.8430382013320923\n",
      "Epoch 269, Loss: 4.1544365882873535, Final Batch Loss: 0.8532918691635132\n",
      "Epoch 270, Loss: 3.9923510551452637, Final Batch Loss: 0.8011279702186584\n",
      "Epoch 271, Loss: 4.169307827949524, Final Batch Loss: 0.9535764455795288\n",
      "Epoch 272, Loss: 3.8653224110603333, Final Batch Loss: 0.7284483909606934\n",
      "Epoch 273, Loss: 4.204337656497955, Final Batch Loss: 0.8694471120834351\n",
      "Epoch 274, Loss: 4.068211734294891, Final Batch Loss: 0.7000845670700073\n",
      "Epoch 275, Loss: 4.16238009929657, Final Batch Loss: 0.8277643918991089\n",
      "Epoch 276, Loss: 4.1076526045799255, Final Batch Loss: 0.8196302056312561\n",
      "Epoch 277, Loss: 4.354640543460846, Final Batch Loss: 0.9928653240203857\n",
      "Epoch 278, Loss: 4.204895436763763, Final Batch Loss: 1.0000313520431519\n",
      "Epoch 279, Loss: 3.9710057973861694, Final Batch Loss: 0.789546549320221\n",
      "Epoch 280, Loss: 4.066851317882538, Final Batch Loss: 0.8247437477111816\n",
      "Epoch 281, Loss: 4.077758252620697, Final Batch Loss: 0.8009128570556641\n",
      "Epoch 282, Loss: 4.2700857520103455, Final Batch Loss: 0.923549234867096\n",
      "Epoch 283, Loss: 4.0307305455207825, Final Batch Loss: 0.8250558972358704\n",
      "Epoch 284, Loss: 3.9615712761878967, Final Batch Loss: 0.7703931927680969\n",
      "Epoch 285, Loss: 4.016386091709137, Final Batch Loss: 0.7360038161277771\n",
      "Epoch 286, Loss: 3.763190805912018, Final Batch Loss: 0.6744456887245178\n",
      "Epoch 287, Loss: 3.9408681392669678, Final Batch Loss: 0.8805546164512634\n",
      "Epoch 288, Loss: 3.9805139303207397, Final Batch Loss: 0.7118272185325623\n",
      "Epoch 289, Loss: 4.003487408161163, Final Batch Loss: 0.7785022854804993\n",
      "Epoch 290, Loss: 3.9387349486351013, Final Batch Loss: 0.791302502155304\n",
      "Epoch 291, Loss: 4.04224693775177, Final Batch Loss: 0.8347594738006592\n",
      "Epoch 292, Loss: 4.151325762271881, Final Batch Loss: 0.8631425499916077\n",
      "Epoch 293, Loss: 4.112546980381012, Final Batch Loss: 0.8022978901863098\n",
      "Epoch 294, Loss: 4.053978085517883, Final Batch Loss: 0.8004953265190125\n",
      "Epoch 295, Loss: 4.02337247133255, Final Batch Loss: 0.7592208385467529\n",
      "Epoch 296, Loss: 4.081657111644745, Final Batch Loss: 0.8990896940231323\n",
      "Epoch 297, Loss: 3.985991954803467, Final Batch Loss: 0.8756658434867859\n",
      "Epoch 298, Loss: 3.9672693014144897, Final Batch Loss: 0.8046556711196899\n",
      "Epoch 299, Loss: 3.9526928663253784, Final Batch Loss: 0.847578763961792\n",
      "Epoch 300, Loss: 4.095822691917419, Final Batch Loss: 0.95952308177948\n",
      "Epoch 301, Loss: 3.9435800909996033, Final Batch Loss: 0.6908352971076965\n",
      "Epoch 302, Loss: 4.076217710971832, Final Batch Loss: 0.7965890765190125\n",
      "Epoch 303, Loss: 3.8384923934936523, Final Batch Loss: 0.6073416471481323\n",
      "Epoch 304, Loss: 3.9103159308433533, Final Batch Loss: 0.6961366534233093\n",
      "Epoch 305, Loss: 4.066121459007263, Final Batch Loss: 0.933379054069519\n",
      "Epoch 306, Loss: 4.119533121585846, Final Batch Loss: 0.8369858860969543\n",
      "Epoch 307, Loss: 4.153110325336456, Final Batch Loss: 0.8546851277351379\n",
      "Epoch 308, Loss: 4.076121509075165, Final Batch Loss: 0.9094585180282593\n",
      "Epoch 309, Loss: 4.027573764324188, Final Batch Loss: 0.7506266832351685\n",
      "Epoch 310, Loss: 4.014778017997742, Final Batch Loss: 0.8355603814125061\n",
      "Epoch 311, Loss: 3.9012413024902344, Final Batch Loss: 0.7735686302185059\n",
      "Epoch 312, Loss: 3.863555908203125, Final Batch Loss: 0.6745769381523132\n",
      "Epoch 313, Loss: 3.982006847858429, Final Batch Loss: 0.9162722229957581\n",
      "Epoch 314, Loss: 3.684909701347351, Final Batch Loss: 0.6904952526092529\n",
      "Epoch 315, Loss: 3.883152484893799, Final Batch Loss: 0.6861249208450317\n",
      "Epoch 316, Loss: 3.9408947229385376, Final Batch Loss: 0.8696833252906799\n",
      "Epoch 317, Loss: 4.006396949291229, Final Batch Loss: 0.8890841603279114\n",
      "Epoch 318, Loss: 4.00866311788559, Final Batch Loss: 0.7394768595695496\n",
      "Epoch 319, Loss: 3.887294590473175, Final Batch Loss: 0.8077408075332642\n",
      "Epoch 320, Loss: 3.918418288230896, Final Batch Loss: 0.7876248359680176\n",
      "Epoch 321, Loss: 3.8182588815689087, Final Batch Loss: 0.6962343454360962\n",
      "Epoch 322, Loss: 3.7396575808525085, Final Batch Loss: 0.6511526703834534\n",
      "Epoch 323, Loss: 3.8938398957252502, Final Batch Loss: 0.852576732635498\n",
      "Epoch 324, Loss: 3.937467336654663, Final Batch Loss: 0.7287716269493103\n",
      "Epoch 325, Loss: 3.875591278076172, Final Batch Loss: 0.7900999784469604\n",
      "Epoch 326, Loss: 3.970994293689728, Final Batch Loss: 0.867344856262207\n",
      "Epoch 327, Loss: 3.9240397810935974, Final Batch Loss: 0.8248433470726013\n",
      "Epoch 328, Loss: 3.7732232809066772, Final Batch Loss: 0.7694140672683716\n",
      "Epoch 329, Loss: 3.865438938140869, Final Batch Loss: 0.790047824382782\n",
      "Epoch 330, Loss: 3.849793493747711, Final Batch Loss: 0.7478711605072021\n",
      "Epoch 331, Loss: 3.7238361835479736, Final Batch Loss: 0.6729435920715332\n",
      "Epoch 332, Loss: 3.8193225860595703, Final Batch Loss: 0.6519321799278259\n",
      "Epoch 333, Loss: 3.728645086288452, Final Batch Loss: 0.6898235082626343\n",
      "Epoch 334, Loss: 3.8268953561782837, Final Batch Loss: 0.7293908596038818\n",
      "Epoch 335, Loss: 3.7777839303016663, Final Batch Loss: 0.6901675462722778\n",
      "Epoch 336, Loss: 3.838394522666931, Final Batch Loss: 0.7090848088264465\n",
      "Epoch 337, Loss: 3.784375548362732, Final Batch Loss: 0.7085025906562805\n",
      "Epoch 338, Loss: 3.853309690952301, Final Batch Loss: 0.8372889161109924\n",
      "Epoch 339, Loss: 3.781302332878113, Final Batch Loss: 0.6743212938308716\n",
      "Epoch 340, Loss: 3.8928069472312927, Final Batch Loss: 0.7748919129371643\n",
      "Epoch 341, Loss: 3.8585537672042847, Final Batch Loss: 0.8892120718955994\n",
      "Epoch 342, Loss: 3.8693036437034607, Final Batch Loss: 0.8521177768707275\n",
      "Epoch 343, Loss: 3.8207393884658813, Final Batch Loss: 0.7891144752502441\n",
      "Epoch 344, Loss: 3.732546389102936, Final Batch Loss: 0.7195037007331848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 345, Loss: 3.6667439937591553, Final Batch Loss: 0.6838351488113403\n",
      "Epoch 346, Loss: 3.579752027988434, Final Batch Loss: 0.6818016767501831\n",
      "Epoch 347, Loss: 3.668098509311676, Final Batch Loss: 0.6418033242225647\n",
      "Epoch 348, Loss: 3.897902488708496, Final Batch Loss: 0.9005967974662781\n",
      "Epoch 349, Loss: 3.9286980032920837, Final Batch Loss: 0.9319719672203064\n",
      "Epoch 350, Loss: 4.086301624774933, Final Batch Loss: 0.9648889303207397\n",
      "Epoch 351, Loss: 3.9358216524124146, Final Batch Loss: 0.8595823049545288\n",
      "Epoch 352, Loss: 3.705282211303711, Final Batch Loss: 0.6936813592910767\n",
      "Epoch 353, Loss: 3.6626136898994446, Final Batch Loss: 0.5937582850456238\n",
      "Epoch 354, Loss: 3.8923644423484802, Final Batch Loss: 0.9591269493103027\n",
      "Epoch 355, Loss: 3.6943657994270325, Final Batch Loss: 0.6665225625038147\n",
      "Epoch 356, Loss: 3.6878231167793274, Final Batch Loss: 0.7582979798316956\n",
      "Epoch 357, Loss: 3.8044700026512146, Final Batch Loss: 0.689278244972229\n",
      "Epoch 358, Loss: 3.759809374809265, Final Batch Loss: 0.7110759615898132\n",
      "Epoch 359, Loss: 3.6949962377548218, Final Batch Loss: 0.6752291321754456\n",
      "Epoch 360, Loss: 3.658392548561096, Final Batch Loss: 0.7584729194641113\n",
      "Epoch 361, Loss: 3.890037953853607, Final Batch Loss: 0.8556309342384338\n",
      "Epoch 362, Loss: 3.81554913520813, Final Batch Loss: 0.8117755651473999\n",
      "Epoch 363, Loss: 3.6112822890281677, Final Batch Loss: 0.5783780813217163\n",
      "Epoch 364, Loss: 3.7243130803108215, Final Batch Loss: 0.6713176369667053\n",
      "Epoch 365, Loss: 3.6528233885765076, Final Batch Loss: 0.7058191299438477\n",
      "Epoch 366, Loss: 3.8536064624786377, Final Batch Loss: 0.8794355392456055\n",
      "Epoch 367, Loss: 3.7305216193199158, Final Batch Loss: 0.7421382069587708\n",
      "Epoch 368, Loss: 3.5892114639282227, Final Batch Loss: 0.6636059284210205\n",
      "Epoch 369, Loss: 3.84836882352829, Final Batch Loss: 0.7873659133911133\n",
      "Epoch 370, Loss: 3.4432172775268555, Final Batch Loss: 0.6221959590911865\n",
      "Epoch 371, Loss: 3.6317622661590576, Final Batch Loss: 0.7133594155311584\n",
      "Epoch 372, Loss: 3.742535412311554, Final Batch Loss: 0.8141804337501526\n",
      "Epoch 373, Loss: 3.562030613422394, Final Batch Loss: 0.5830261707305908\n",
      "Epoch 374, Loss: 3.6222780346870422, Final Batch Loss: 0.7048026919364929\n",
      "Epoch 375, Loss: 3.6520588397979736, Final Batch Loss: 0.6745299100875854\n",
      "Epoch 376, Loss: 3.841377854347229, Final Batch Loss: 0.7610411047935486\n",
      "Epoch 377, Loss: 3.7809088230133057, Final Batch Loss: 0.7617596983909607\n",
      "Epoch 378, Loss: 3.886793792247772, Final Batch Loss: 0.8911224007606506\n",
      "Epoch 379, Loss: 3.6538937091827393, Final Batch Loss: 0.8170666098594666\n",
      "Epoch 380, Loss: 3.544387698173523, Final Batch Loss: 0.628928005695343\n",
      "Epoch 381, Loss: 3.820403039455414, Final Batch Loss: 0.7218496203422546\n",
      "Epoch 382, Loss: 3.430169999599457, Final Batch Loss: 0.6135938167572021\n",
      "Epoch 383, Loss: 3.7805270552635193, Final Batch Loss: 0.845497727394104\n",
      "Epoch 384, Loss: 3.6525010466575623, Final Batch Loss: 0.7686171531677246\n",
      "Epoch 385, Loss: 3.8342859745025635, Final Batch Loss: 0.8710765242576599\n",
      "Epoch 386, Loss: 3.57541686296463, Final Batch Loss: 0.6688206791877747\n",
      "Epoch 387, Loss: 3.7841500639915466, Final Batch Loss: 0.8012198805809021\n",
      "Epoch 388, Loss: 3.761039912700653, Final Batch Loss: 0.7058884501457214\n",
      "Epoch 389, Loss: 3.6018017530441284, Final Batch Loss: 0.7644150257110596\n",
      "Epoch 390, Loss: 3.639020562171936, Final Batch Loss: 0.6967179775238037\n",
      "Epoch 391, Loss: 3.524149715900421, Final Batch Loss: 0.6130179166793823\n",
      "Epoch 392, Loss: 3.4810115098953247, Final Batch Loss: 0.67185378074646\n",
      "Epoch 393, Loss: 3.6368097066879272, Final Batch Loss: 0.7738193273544312\n",
      "Epoch 394, Loss: 3.7420777082443237, Final Batch Loss: 0.9245390295982361\n",
      "Epoch 395, Loss: 3.8446168899536133, Final Batch Loss: 0.7705965638160706\n",
      "Epoch 396, Loss: 3.675121545791626, Final Batch Loss: 0.7459844946861267\n",
      "Epoch 397, Loss: 3.4504477977752686, Final Batch Loss: 0.7637546062469482\n",
      "Epoch 398, Loss: 3.7285298109054565, Final Batch Loss: 0.8541353940963745\n",
      "Epoch 399, Loss: 3.510332405567169, Final Batch Loss: 0.6535635590553284\n",
      "Epoch 400, Loss: 3.3975080251693726, Final Batch Loss: 0.594613790512085\n",
      "Epoch 401, Loss: 3.6610069274902344, Final Batch Loss: 0.7784143090248108\n",
      "Epoch 402, Loss: 3.4116953015327454, Final Batch Loss: 0.5610927939414978\n",
      "Epoch 403, Loss: 3.652866840362549, Final Batch Loss: 0.6225669384002686\n",
      "Epoch 404, Loss: 3.709822952747345, Final Batch Loss: 0.8290941119194031\n",
      "Epoch 405, Loss: 3.461311995983124, Final Batch Loss: 0.5749233365058899\n",
      "Epoch 406, Loss: 3.647932291030884, Final Batch Loss: 0.9798881411552429\n",
      "Epoch 407, Loss: 3.50559264421463, Final Batch Loss: 0.6596249938011169\n",
      "Epoch 408, Loss: 3.5654733777046204, Final Batch Loss: 0.6881721615791321\n",
      "Epoch 409, Loss: 3.7805397510528564, Final Batch Loss: 0.8188451528549194\n",
      "Epoch 410, Loss: 3.5715792775154114, Final Batch Loss: 0.7954053282737732\n",
      "Epoch 411, Loss: 3.60115647315979, Final Batch Loss: 0.7842701077461243\n",
      "Epoch 412, Loss: 3.532534420490265, Final Batch Loss: 0.7563671469688416\n",
      "Epoch 413, Loss: 3.579712390899658, Final Batch Loss: 0.7352129220962524\n",
      "Epoch 414, Loss: 3.664910614490509, Final Batch Loss: 0.7632046341896057\n",
      "Epoch 415, Loss: 3.492410361766815, Final Batch Loss: 0.673670768737793\n",
      "Epoch 416, Loss: 3.530058979988098, Final Batch Loss: 0.6987833976745605\n",
      "Epoch 417, Loss: 3.6810370087623596, Final Batch Loss: 0.830544114112854\n",
      "Epoch 418, Loss: 3.552913248538971, Final Batch Loss: 0.7249236702919006\n",
      "Epoch 419, Loss: 3.424362897872925, Final Batch Loss: 0.6567195653915405\n",
      "Epoch 420, Loss: 3.4583383798599243, Final Batch Loss: 0.6677719950675964\n",
      "Epoch 421, Loss: 3.570439577102661, Final Batch Loss: 0.7462843656539917\n",
      "Epoch 422, Loss: 3.5514609813690186, Final Batch Loss: 0.7093234062194824\n",
      "Epoch 423, Loss: 3.591451942920685, Final Batch Loss: 0.9021196365356445\n",
      "Epoch 424, Loss: 3.4196091890335083, Final Batch Loss: 0.693379819393158\n",
      "Epoch 425, Loss: 3.599621832370758, Final Batch Loss: 0.7676079869270325\n",
      "Epoch 426, Loss: 3.3049604892730713, Final Batch Loss: 0.561353862285614\n",
      "Epoch 427, Loss: 3.5490434169769287, Final Batch Loss: 0.6338234543800354\n",
      "Epoch 428, Loss: 3.393149495124817, Final Batch Loss: 0.6345653533935547\n",
      "Epoch 429, Loss: 3.62291944026947, Final Batch Loss: 0.8665803074836731\n",
      "Epoch 430, Loss: 3.537344515323639, Final Batch Loss: 0.7245343327522278\n",
      "Epoch 431, Loss: 3.3738746643066406, Final Batch Loss: 0.651175856590271\n",
      "Epoch 432, Loss: 3.3292737007141113, Final Batch Loss: 0.7015658020973206\n",
      "Epoch 433, Loss: 3.5829206109046936, Final Batch Loss: 0.6110767722129822\n",
      "Epoch 434, Loss: 3.4027331471443176, Final Batch Loss: 0.6619736552238464\n",
      "Epoch 435, Loss: 3.40622615814209, Final Batch Loss: 0.694766640663147\n",
      "Epoch 436, Loss: 3.5878201127052307, Final Batch Loss: 0.9130691885948181\n",
      "Epoch 437, Loss: 3.3986894488334656, Final Batch Loss: 0.5677028298377991\n",
      "Epoch 438, Loss: 3.507659673690796, Final Batch Loss: 0.8326162099838257\n",
      "Epoch 439, Loss: 3.4294079542160034, Final Batch Loss: 0.7508735060691833\n",
      "Epoch 440, Loss: 3.4578503370285034, Final Batch Loss: 0.7091299891471863\n",
      "Epoch 441, Loss: 3.4160194993019104, Final Batch Loss: 0.6765852570533752\n",
      "Epoch 442, Loss: 3.365307569503784, Final Batch Loss: 0.637122631072998\n",
      "Epoch 443, Loss: 3.2800965309143066, Final Batch Loss: 0.5457655191421509\n",
      "Epoch 444, Loss: 3.4749685525894165, Final Batch Loss: 0.734527587890625\n",
      "Epoch 445, Loss: 3.4554625153541565, Final Batch Loss: 0.7481563687324524\n",
      "Epoch 446, Loss: 3.4844531416893005, Final Batch Loss: 0.6274563074111938\n",
      "Epoch 447, Loss: 3.2818946838378906, Final Batch Loss: 0.6030508279800415\n",
      "Epoch 448, Loss: 3.544234275817871, Final Batch Loss: 0.8611520528793335\n",
      "Epoch 449, Loss: 3.347599983215332, Final Batch Loss: 0.7012544870376587\n",
      "Epoch 450, Loss: 3.3446946144104004, Final Batch Loss: 0.6391571164131165\n",
      "Epoch 451, Loss: 3.456189811229706, Final Batch Loss: 0.650263786315918\n",
      "Epoch 452, Loss: 3.3612834811210632, Final Batch Loss: 0.6892287731170654\n",
      "Epoch 453, Loss: 3.3252126574516296, Final Batch Loss: 0.6625945568084717\n",
      "Epoch 454, Loss: 3.386048138141632, Final Batch Loss: 0.7504254579544067\n",
      "Epoch 455, Loss: 3.55394971370697, Final Batch Loss: 0.7778662443161011\n",
      "Epoch 456, Loss: 3.2545692920684814, Final Batch Loss: 0.5158404111862183\n",
      "Epoch 457, Loss: 3.3315953612327576, Final Batch Loss: 0.5968750715255737\n",
      "Epoch 458, Loss: 3.568110704421997, Final Batch Loss: 0.8171206116676331\n",
      "Epoch 459, Loss: 3.325253903865814, Final Batch Loss: 0.587027370929718\n",
      "Epoch 460, Loss: 3.58349072933197, Final Batch Loss: 0.749915361404419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 461, Loss: 3.4267476201057434, Final Batch Loss: 0.7469223737716675\n",
      "Epoch 462, Loss: 3.4339792728424072, Final Batch Loss: 0.6783509850502014\n",
      "Epoch 463, Loss: 3.211351990699768, Final Batch Loss: 0.550195574760437\n",
      "Epoch 464, Loss: 3.318780839443207, Final Batch Loss: 0.6199465394020081\n",
      "Epoch 465, Loss: 3.46885222196579, Final Batch Loss: 0.7101281881332397\n",
      "Epoch 466, Loss: 3.321295201778412, Final Batch Loss: 0.7096613645553589\n",
      "Epoch 467, Loss: 3.3109798431396484, Final Batch Loss: 0.5730618834495544\n",
      "Epoch 468, Loss: 3.445670962333679, Final Batch Loss: 0.6485512256622314\n",
      "Epoch 469, Loss: 3.2487218976020813, Final Batch Loss: 0.6671843528747559\n",
      "Epoch 470, Loss: 3.425486207008362, Final Batch Loss: 0.7069231867790222\n",
      "Epoch 471, Loss: 3.4828243255615234, Final Batch Loss: 0.7610230445861816\n",
      "Epoch 472, Loss: 3.3452163338661194, Final Batch Loss: 0.696283221244812\n",
      "Epoch 473, Loss: 3.4808700680732727, Final Batch Loss: 0.8247966170310974\n",
      "Epoch 474, Loss: 3.206494688987732, Final Batch Loss: 0.6547903418540955\n",
      "Epoch 475, Loss: 3.4636451601982117, Final Batch Loss: 0.7965618371963501\n",
      "Epoch 476, Loss: 3.38264924287796, Final Batch Loss: 0.8023141026496887\n",
      "Epoch 477, Loss: 3.1736902594566345, Final Batch Loss: 0.6226708889007568\n",
      "Epoch 478, Loss: 3.2249839901924133, Final Batch Loss: 0.5565261840820312\n",
      "Epoch 479, Loss: 3.0405226945877075, Final Batch Loss: 0.4644860029220581\n",
      "Epoch 480, Loss: 3.248361349105835, Final Batch Loss: 0.5139831900596619\n",
      "Epoch 481, Loss: 3.401401698589325, Final Batch Loss: 0.7431701421737671\n",
      "Epoch 482, Loss: 3.2220274806022644, Final Batch Loss: 0.6994920372962952\n",
      "Epoch 483, Loss: 3.477086067199707, Final Batch Loss: 0.8882175087928772\n",
      "Epoch 484, Loss: 3.3647851943969727, Final Batch Loss: 0.6251698732376099\n",
      "Epoch 485, Loss: 3.4094797372817993, Final Batch Loss: 0.7929980158805847\n",
      "Epoch 486, Loss: 3.3793222308158875, Final Batch Loss: 0.7916080355644226\n",
      "Epoch 487, Loss: 3.302650570869446, Final Batch Loss: 0.7613108158111572\n",
      "Epoch 488, Loss: 3.16785329580307, Final Batch Loss: 0.5866605043411255\n",
      "Epoch 489, Loss: 3.177117645740509, Final Batch Loss: 0.6648470163345337\n",
      "Epoch 490, Loss: 3.523487865924835, Final Batch Loss: 0.8139938712120056\n",
      "Epoch 491, Loss: 3.0808168947696686, Final Batch Loss: 0.46610507369041443\n",
      "Epoch 492, Loss: 3.4439173340797424, Final Batch Loss: 0.6626167893409729\n",
      "Epoch 493, Loss: 3.2219097018241882, Final Batch Loss: 0.6231022477149963\n",
      "Epoch 494, Loss: 3.2068622708320618, Final Batch Loss: 0.564063549041748\n",
      "Epoch 495, Loss: 3.1451305150985718, Final Batch Loss: 0.5702695250511169\n",
      "Epoch 496, Loss: 3.378019332885742, Final Batch Loss: 0.7761980295181274\n",
      "Epoch 497, Loss: 3.2765222787857056, Final Batch Loss: 0.6541687250137329\n",
      "Epoch 498, Loss: 3.2454166412353516, Final Batch Loss: 0.6352025866508484\n",
      "Epoch 499, Loss: 3.367750644683838, Final Batch Loss: 0.6562736630439758\n",
      "Epoch 500, Loss: 3.207911729812622, Final Batch Loss: 0.6865254640579224\n",
      "Epoch 501, Loss: 3.3527785539627075, Final Batch Loss: 0.7446467280387878\n",
      "Epoch 502, Loss: 3.0550166368484497, Final Batch Loss: 0.38129937648773193\n",
      "Epoch 503, Loss: 3.298730492591858, Final Batch Loss: 0.7347128987312317\n",
      "Epoch 504, Loss: 3.318052887916565, Final Batch Loss: 0.7153330445289612\n",
      "Epoch 505, Loss: 3.167381167411804, Final Batch Loss: 0.54030841588974\n",
      "Epoch 506, Loss: 3.1663460731506348, Final Batch Loss: 0.55927574634552\n",
      "Epoch 507, Loss: 3.2650392055511475, Final Batch Loss: 0.6151670813560486\n",
      "Epoch 508, Loss: 3.100476324558258, Final Batch Loss: 0.5967646241188049\n",
      "Epoch 509, Loss: 3.321423292160034, Final Batch Loss: 0.7295082807540894\n",
      "Epoch 510, Loss: 3.1764429807662964, Final Batch Loss: 0.6793482303619385\n",
      "Epoch 511, Loss: 3.3602867126464844, Final Batch Loss: 0.7313198447227478\n",
      "Epoch 512, Loss: 3.261902391910553, Final Batch Loss: 0.7190917134284973\n",
      "Epoch 513, Loss: 3.1645472049713135, Final Batch Loss: 0.6548004150390625\n",
      "Epoch 514, Loss: 3.010346829891205, Final Batch Loss: 0.4542192816734314\n",
      "Epoch 515, Loss: 3.2638092041015625, Final Batch Loss: 0.6935532093048096\n",
      "Epoch 516, Loss: 3.0642155408859253, Final Batch Loss: 0.41628825664520264\n",
      "Epoch 517, Loss: 2.974961996078491, Final Batch Loss: 0.5871936082839966\n",
      "Epoch 518, Loss: 3.1655397415161133, Final Batch Loss: 0.6688216328620911\n",
      "Epoch 519, Loss: 3.145024299621582, Final Batch Loss: 0.569706380367279\n",
      "Epoch 520, Loss: 3.3601471185684204, Final Batch Loss: 0.8731418251991272\n",
      "Epoch 521, Loss: 3.269089996814728, Final Batch Loss: 0.6975950598716736\n",
      "Epoch 522, Loss: 3.240038275718689, Final Batch Loss: 0.5913004875183105\n",
      "Epoch 523, Loss: 3.108464777469635, Final Batch Loss: 0.6371138095855713\n",
      "Epoch 524, Loss: 3.1790303587913513, Final Batch Loss: 0.701960563659668\n",
      "Epoch 525, Loss: 3.065148890018463, Final Batch Loss: 0.6824513673782349\n",
      "Epoch 526, Loss: 2.9655958712100983, Final Batch Loss: 0.4952680766582489\n",
      "Epoch 527, Loss: 3.046867847442627, Final Batch Loss: 0.5259523391723633\n",
      "Epoch 528, Loss: 3.187399208545685, Final Batch Loss: 0.6651591658592224\n",
      "Epoch 529, Loss: 3.0202305912971497, Final Batch Loss: 0.5122414827346802\n",
      "Epoch 530, Loss: 3.0690701007843018, Final Batch Loss: 0.5249074697494507\n",
      "Epoch 531, Loss: 3.3229647874832153, Final Batch Loss: 0.6771690249443054\n",
      "Epoch 532, Loss: 3.0450116097927094, Final Batch Loss: 0.45836976170539856\n",
      "Epoch 533, Loss: 2.9836015105247498, Final Batch Loss: 0.5150091648101807\n",
      "Epoch 534, Loss: 3.19415283203125, Final Batch Loss: 0.6640759110450745\n",
      "Epoch 535, Loss: 3.1015495657920837, Final Batch Loss: 0.5588550567626953\n",
      "Epoch 536, Loss: 2.952091872692108, Final Batch Loss: 0.4751669764518738\n",
      "Epoch 537, Loss: 3.205892860889435, Final Batch Loss: 0.6128766536712646\n",
      "Epoch 538, Loss: 3.0926290154457092, Final Batch Loss: 0.5788776874542236\n",
      "Epoch 539, Loss: 3.1584522128105164, Final Batch Loss: 0.6905084848403931\n",
      "Epoch 540, Loss: 3.0424022674560547, Final Batch Loss: 0.6142938733100891\n",
      "Epoch 541, Loss: 3.05291211605072, Final Batch Loss: 0.528270423412323\n",
      "Epoch 542, Loss: 3.1983484029769897, Final Batch Loss: 0.6214888691902161\n",
      "Epoch 543, Loss: 3.1343933939933777, Final Batch Loss: 0.5463258028030396\n",
      "Epoch 544, Loss: 3.1531498432159424, Final Batch Loss: 0.7274156808853149\n",
      "Epoch 545, Loss: 3.1772738099098206, Final Batch Loss: 0.7369104623794556\n",
      "Epoch 546, Loss: 3.0748329758644104, Final Batch Loss: 0.6687369346618652\n",
      "Epoch 547, Loss: 3.1651304364204407, Final Batch Loss: 0.6465182900428772\n",
      "Epoch 548, Loss: 2.977452039718628, Final Batch Loss: 0.5087501406669617\n",
      "Epoch 549, Loss: 2.921013116836548, Final Batch Loss: 0.5103992819786072\n",
      "Epoch 550, Loss: 3.0565922260284424, Final Batch Loss: 0.6015675663948059\n",
      "Epoch 551, Loss: 3.110198140144348, Final Batch Loss: 0.6813609004020691\n",
      "Epoch 552, Loss: 3.0028841495513916, Final Batch Loss: 0.41540712118148804\n",
      "Epoch 553, Loss: 3.1539317965507507, Final Batch Loss: 0.7194148898124695\n",
      "Epoch 554, Loss: 3.0131629705429077, Final Batch Loss: 0.6907945871353149\n",
      "Epoch 555, Loss: 3.1821274757385254, Final Batch Loss: 0.7702425718307495\n",
      "Epoch 556, Loss: 2.9222021102905273, Final Batch Loss: 0.5749557018280029\n",
      "Epoch 557, Loss: 3.3469148874282837, Final Batch Loss: 0.7584273815155029\n",
      "Epoch 558, Loss: 2.9051126539707184, Final Batch Loss: 0.44369611144065857\n",
      "Epoch 559, Loss: 3.1184418201446533, Final Batch Loss: 0.6055158972740173\n",
      "Epoch 560, Loss: 3.123078405857086, Final Batch Loss: 0.6953970193862915\n",
      "Epoch 561, Loss: 2.9487926959991455, Final Batch Loss: 0.5311456918716431\n",
      "Epoch 562, Loss: 3.0081198811531067, Final Batch Loss: 0.6777621507644653\n",
      "Epoch 563, Loss: 3.1365187764167786, Final Batch Loss: 0.7505641579627991\n",
      "Epoch 564, Loss: 3.1293572187423706, Final Batch Loss: 0.5498606562614441\n",
      "Epoch 565, Loss: 3.272425353527069, Final Batch Loss: 0.7627338767051697\n",
      "Epoch 566, Loss: 3.1175354719161987, Final Batch Loss: 0.6386663317680359\n",
      "Epoch 567, Loss: 3.2633442878723145, Final Batch Loss: 0.7350525856018066\n",
      "Epoch 568, Loss: 3.1943891644477844, Final Batch Loss: 0.5505886077880859\n",
      "Epoch 569, Loss: 2.905748814344406, Final Batch Loss: 0.4862472116947174\n",
      "Epoch 570, Loss: 2.999219834804535, Final Batch Loss: 0.47982704639434814\n",
      "Epoch 571, Loss: 2.810776114463806, Final Batch Loss: 0.346299946308136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 572, Loss: 3.1137654781341553, Final Batch Loss: 0.6400642395019531\n",
      "Epoch 573, Loss: 3.3411490321159363, Final Batch Loss: 0.7758163809776306\n",
      "Epoch 574, Loss: 3.173199713230133, Final Batch Loss: 0.7665582895278931\n",
      "Epoch 575, Loss: 3.0697420835494995, Final Batch Loss: 0.563473105430603\n",
      "Epoch 576, Loss: 3.0541279315948486, Final Batch Loss: 0.5241362452507019\n",
      "Epoch 577, Loss: 3.1989686489105225, Final Batch Loss: 0.7249885201454163\n",
      "Epoch 578, Loss: 3.0975216031074524, Final Batch Loss: 0.640318751335144\n",
      "Epoch 579, Loss: 3.234146237373352, Final Batch Loss: 0.777168333530426\n",
      "Epoch 580, Loss: 3.0858773589134216, Final Batch Loss: 0.604767382144928\n",
      "Epoch 581, Loss: 2.9376272559165955, Final Batch Loss: 0.48607099056243896\n",
      "Epoch 582, Loss: 3.186517596244812, Final Batch Loss: 0.6921555995941162\n",
      "Epoch 583, Loss: 2.8671292066574097, Final Batch Loss: 0.4892504811286926\n",
      "Epoch 584, Loss: 3.101850152015686, Final Batch Loss: 0.6614785194396973\n",
      "Epoch 585, Loss: 2.9563729763031006, Final Batch Loss: 0.5544388294219971\n",
      "Epoch 586, Loss: 3.1670706272125244, Final Batch Loss: 0.6590796709060669\n",
      "Epoch 587, Loss: 3.1118836998939514, Final Batch Loss: 0.6726982593536377\n",
      "Epoch 588, Loss: 2.9854946732521057, Final Batch Loss: 0.6282117366790771\n",
      "Epoch 589, Loss: 3.097958981990814, Final Batch Loss: 0.5716769099235535\n",
      "Epoch 590, Loss: 2.9144718647003174, Final Batch Loss: 0.5293049216270447\n",
      "Epoch 591, Loss: 3.167405605316162, Final Batch Loss: 0.6269367337226868\n",
      "Epoch 592, Loss: 3.0993192195892334, Final Batch Loss: 0.6500687003135681\n",
      "Epoch 593, Loss: 3.055464804172516, Final Batch Loss: 0.5927383899688721\n",
      "Epoch 594, Loss: 3.2575424313545227, Final Batch Loss: 0.6863483190536499\n",
      "Epoch 595, Loss: 2.9658976793289185, Final Batch Loss: 0.5323569774627686\n",
      "Epoch 596, Loss: 3.1201804876327515, Final Batch Loss: 0.6320918202400208\n",
      "Epoch 597, Loss: 3.2325137853622437, Final Batch Loss: 0.7740830779075623\n",
      "Epoch 598, Loss: 2.9095239639282227, Final Batch Loss: 0.5153144001960754\n",
      "Epoch 599, Loss: 2.9455955028533936, Final Batch Loss: 0.5256156921386719\n",
      "Epoch 600, Loss: 3.0539242029190063, Final Batch Loss: 0.5700306296348572\n",
      "Epoch 601, Loss: 3.028651773929596, Final Batch Loss: 0.5357267260551453\n",
      "Epoch 602, Loss: 3.0951988697052, Final Batch Loss: 0.7060413956642151\n",
      "Epoch 603, Loss: 2.865923762321472, Final Batch Loss: 0.5233525633811951\n",
      "Epoch 604, Loss: 3.019154906272888, Final Batch Loss: 0.6353186964988708\n",
      "Epoch 605, Loss: 2.8171653151512146, Final Batch Loss: 0.5119678378105164\n",
      "Epoch 606, Loss: 3.0711652636528015, Final Batch Loss: 0.5953723788261414\n",
      "Epoch 607, Loss: 3.014075458049774, Final Batch Loss: 0.6061816811561584\n",
      "Epoch 608, Loss: 3.0216987133026123, Final Batch Loss: 0.571453332901001\n",
      "Epoch 609, Loss: 3.013662815093994, Final Batch Loss: 0.6613211035728455\n",
      "Epoch 610, Loss: 2.9641327261924744, Final Batch Loss: 0.5608001351356506\n",
      "Epoch 611, Loss: 2.9753167629241943, Final Batch Loss: 0.5373184084892273\n",
      "Epoch 612, Loss: 2.9764912724494934, Final Batch Loss: 0.5754584074020386\n",
      "Epoch 613, Loss: 2.8678868412971497, Final Batch Loss: 0.36020976305007935\n",
      "Epoch 614, Loss: 3.190846264362335, Final Batch Loss: 0.6594740748405457\n",
      "Epoch 615, Loss: 2.879992038011551, Final Batch Loss: 0.4477284848690033\n",
      "Epoch 616, Loss: 2.9915817379951477, Final Batch Loss: 0.5552783608436584\n",
      "Epoch 617, Loss: 3.028063178062439, Final Batch Loss: 0.5495913624763489\n",
      "Epoch 618, Loss: 3.095984637737274, Final Batch Loss: 0.6302728056907654\n",
      "Epoch 619, Loss: 3.00415575504303, Final Batch Loss: 0.6506356000900269\n",
      "Epoch 620, Loss: 2.850672423839569, Final Batch Loss: 0.5834237337112427\n",
      "Epoch 621, Loss: 2.9081920385360718, Final Batch Loss: 0.46592772006988525\n",
      "Epoch 622, Loss: 3.1556143164634705, Final Batch Loss: 0.7740061283111572\n",
      "Epoch 623, Loss: 2.93632048368454, Final Batch Loss: 0.6067088842391968\n",
      "Epoch 624, Loss: 2.9921898245811462, Final Batch Loss: 0.6806290745735168\n",
      "Epoch 625, Loss: 3.081486463546753, Final Batch Loss: 0.6532630324363708\n",
      "Epoch 626, Loss: 3.0423722863197327, Final Batch Loss: 0.4997318387031555\n",
      "Epoch 627, Loss: 2.9089428782463074, Final Batch Loss: 0.4708016514778137\n",
      "Epoch 628, Loss: 3.3068425059318542, Final Batch Loss: 0.8552919626235962\n",
      "Epoch 629, Loss: 2.8030582070350647, Final Batch Loss: 0.475919246673584\n",
      "Epoch 630, Loss: 2.9302836060523987, Final Batch Loss: 0.49680131673812866\n",
      "Epoch 631, Loss: 3.206943392753601, Final Batch Loss: 0.8020924925804138\n",
      "Epoch 632, Loss: 2.993577539920807, Final Batch Loss: 0.5579676032066345\n",
      "Epoch 633, Loss: 2.8761439323425293, Final Batch Loss: 0.4833707809448242\n",
      "Epoch 634, Loss: 2.901007294654846, Final Batch Loss: 0.4538532495498657\n",
      "Epoch 635, Loss: 2.929370105266571, Final Batch Loss: 0.5392029881477356\n",
      "Epoch 636, Loss: 2.939993053674698, Final Batch Loss: 0.47877201437950134\n",
      "Epoch 637, Loss: 2.8613619804382324, Final Batch Loss: 0.5251995325088501\n",
      "Epoch 638, Loss: 2.9762625694274902, Final Batch Loss: 0.5618782639503479\n",
      "Epoch 639, Loss: 2.948628544807434, Final Batch Loss: 0.5591737031936646\n",
      "Epoch 640, Loss: 2.9279425740242004, Final Batch Loss: 0.6321811676025391\n",
      "Epoch 641, Loss: 2.946192592382431, Final Batch Loss: 0.4844945967197418\n",
      "Epoch 642, Loss: 2.9927881360054016, Final Batch Loss: 0.62190842628479\n",
      "Epoch 643, Loss: 2.9174131751060486, Final Batch Loss: 0.6442418694496155\n",
      "Epoch 644, Loss: 3.200574219226837, Final Batch Loss: 0.587327778339386\n",
      "Epoch 645, Loss: 2.967296302318573, Final Batch Loss: 0.7145074605941772\n",
      "Epoch 646, Loss: 3.0541077256202698, Final Batch Loss: 0.7490089535713196\n",
      "Epoch 647, Loss: 3.0802873373031616, Final Batch Loss: 0.6547850370407104\n",
      "Epoch 648, Loss: 2.9421913623809814, Final Batch Loss: 0.6280755996704102\n",
      "Epoch 649, Loss: 2.9332000017166138, Final Batch Loss: 0.596343994140625\n",
      "Epoch 650, Loss: 3.1013940572738647, Final Batch Loss: 0.773430585861206\n",
      "Epoch 651, Loss: 3.0968591570854187, Final Batch Loss: 0.6392454504966736\n",
      "Epoch 652, Loss: 2.7880091667175293, Final Batch Loss: 0.5851914882659912\n",
      "Epoch 653, Loss: 2.7927208840847015, Final Batch Loss: 0.5534716844558716\n",
      "Epoch 654, Loss: 2.958464205265045, Final Batch Loss: 0.6139006614685059\n",
      "Epoch 655, Loss: 2.995264232158661, Final Batch Loss: 0.6625566482543945\n",
      "Epoch 656, Loss: 2.9196566343307495, Final Batch Loss: 0.6383196711540222\n",
      "Epoch 657, Loss: 3.008042573928833, Final Batch Loss: 0.6085348129272461\n",
      "Epoch 658, Loss: 2.829287886619568, Final Batch Loss: 0.5009804368019104\n",
      "Epoch 659, Loss: 3.0800179839134216, Final Batch Loss: 0.6167746782302856\n",
      "Epoch 660, Loss: 3.0022278428077698, Final Batch Loss: 0.6698165535926819\n",
      "Epoch 661, Loss: 2.896955192089081, Final Batch Loss: 0.5896596312522888\n",
      "Epoch 662, Loss: 2.9183892607688904, Final Batch Loss: 0.6139770150184631\n",
      "Epoch 663, Loss: 2.960118532180786, Final Batch Loss: 0.623297393321991\n",
      "Epoch 664, Loss: 2.868299126625061, Final Batch Loss: 0.6879006624221802\n",
      "Epoch 665, Loss: 2.926032066345215, Final Batch Loss: 0.4797245264053345\n",
      "Epoch 666, Loss: 2.952598810195923, Final Batch Loss: 0.6316159963607788\n",
      "Epoch 667, Loss: 2.8619166016578674, Final Batch Loss: 0.5560904145240784\n",
      "Epoch 668, Loss: 2.8859339356422424, Final Batch Loss: 0.6375256180763245\n",
      "Epoch 669, Loss: 2.935851663351059, Final Batch Loss: 0.4992010295391083\n",
      "Epoch 670, Loss: 2.820552319288254, Final Batch Loss: 0.46445193886756897\n",
      "Epoch 671, Loss: 2.890735924243927, Final Batch Loss: 0.6411445140838623\n",
      "Epoch 672, Loss: 3.002354860305786, Final Batch Loss: 0.7271586656570435\n",
      "Epoch 673, Loss: 3.035016894340515, Final Batch Loss: 0.6025714874267578\n",
      "Epoch 674, Loss: 3.0051615834236145, Final Batch Loss: 0.6093167662620544\n",
      "Epoch 675, Loss: 3.01746729016304, Final Batch Loss: 0.6424014568328857\n",
      "Epoch 676, Loss: 3.0671888887882233, Final Batch Loss: 0.7335020899772644\n",
      "Epoch 677, Loss: 2.9193254709243774, Final Batch Loss: 0.6771771311759949\n",
      "Epoch 678, Loss: 2.9660148322582245, Final Batch Loss: 0.6718495488166809\n",
      "Epoch 679, Loss: 2.81196328997612, Final Batch Loss: 0.46533912420272827\n",
      "Epoch 680, Loss: 2.876135915517807, Final Batch Loss: 0.5302833318710327\n",
      "Epoch 681, Loss: 2.9020477533340454, Final Batch Loss: 0.5518500208854675\n",
      "Epoch 682, Loss: 2.8874948620796204, Final Batch Loss: 0.5087173581123352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 683, Loss: 2.75690758228302, Final Batch Loss: 0.48294126987457275\n",
      "Epoch 684, Loss: 2.860311806201935, Final Batch Loss: 0.5635536313056946\n",
      "Epoch 685, Loss: 2.850851058959961, Final Batch Loss: 0.5564723610877991\n",
      "Epoch 686, Loss: 2.9861424267292023, Final Batch Loss: 0.8207718133926392\n",
      "Epoch 687, Loss: 2.9227497577667236, Final Batch Loss: 0.3766080141067505\n",
      "Epoch 688, Loss: 2.777352213859558, Final Batch Loss: 0.6036912202835083\n",
      "Epoch 689, Loss: 3.04961895942688, Final Batch Loss: 0.732211709022522\n",
      "Epoch 690, Loss: 2.950938880443573, Final Batch Loss: 0.5302037596702576\n",
      "Epoch 691, Loss: 2.807003825902939, Final Batch Loss: 0.4525056779384613\n",
      "Epoch 692, Loss: 2.786307066679001, Final Batch Loss: 0.4698648750782013\n",
      "Epoch 693, Loss: 2.903226375579834, Final Batch Loss: 0.5862654447555542\n",
      "Epoch 694, Loss: 2.847038447856903, Final Batch Loss: 0.5447056293487549\n",
      "Epoch 695, Loss: 2.763348937034607, Final Batch Loss: 0.4438442289829254\n",
      "Epoch 696, Loss: 2.67732036113739, Final Batch Loss: 0.43808478116989136\n",
      "Epoch 697, Loss: 2.828953802585602, Final Batch Loss: 0.6039617657661438\n",
      "Epoch 698, Loss: 2.8394951224327087, Final Batch Loss: 0.5904039144515991\n",
      "Epoch 699, Loss: 2.873937666416168, Final Batch Loss: 0.6343268156051636\n",
      "Epoch 700, Loss: 2.7873371243476868, Final Batch Loss: 0.5654107332229614\n",
      "Epoch 701, Loss: 2.697515308856964, Final Batch Loss: 0.3555353879928589\n",
      "Epoch 702, Loss: 2.8204312920570374, Final Batch Loss: 0.6372361779212952\n",
      "Epoch 703, Loss: 3.0326157808303833, Final Batch Loss: 0.6599788665771484\n",
      "Epoch 704, Loss: 2.8886218070983887, Final Batch Loss: 0.5913066267967224\n",
      "Epoch 705, Loss: 3.0004910230636597, Final Batch Loss: 0.6411891579627991\n",
      "Epoch 706, Loss: 2.894144117832184, Final Batch Loss: 0.5379475355148315\n",
      "Epoch 707, Loss: 2.5874095261096954, Final Batch Loss: 0.4838280975818634\n",
      "Epoch 708, Loss: 2.8633324205875397, Final Batch Loss: 0.5197191834449768\n",
      "Epoch 709, Loss: 2.740559309720993, Final Batch Loss: 0.4015250504016876\n",
      "Epoch 710, Loss: 2.7962485551834106, Final Batch Loss: 0.42467302083969116\n",
      "Epoch 711, Loss: 2.7387541234493256, Final Batch Loss: 0.5005097985267639\n",
      "Epoch 712, Loss: 2.75355327129364, Final Batch Loss: 0.430412232875824\n",
      "Epoch 713, Loss: 2.8177444338798523, Final Batch Loss: 0.5150975584983826\n",
      "Epoch 714, Loss: 2.831163376569748, Final Batch Loss: 0.6292380094528198\n",
      "Epoch 715, Loss: 2.877145081758499, Final Batch Loss: 0.6318501830101013\n",
      "Epoch 716, Loss: 2.7861238420009613, Final Batch Loss: 0.4782263934612274\n",
      "Epoch 717, Loss: 2.7303294837474823, Final Batch Loss: 0.45582327246665955\n",
      "Epoch 718, Loss: 2.724337637424469, Final Batch Loss: 0.5322626829147339\n",
      "Epoch 719, Loss: 2.8945862650871277, Final Batch Loss: 0.5227642059326172\n",
      "Epoch 720, Loss: 2.8511038422584534, Final Batch Loss: 0.6043713092803955\n",
      "Epoch 721, Loss: 2.8056078255176544, Final Batch Loss: 0.4855360686779022\n",
      "Epoch 722, Loss: 2.6884201765060425, Final Batch Loss: 0.43392789363861084\n",
      "Epoch 723, Loss: 2.9125411212444305, Final Batch Loss: 0.6407795548439026\n",
      "Epoch 724, Loss: 2.879170686006546, Final Batch Loss: 0.49677911400794983\n",
      "Epoch 725, Loss: 2.9321314096450806, Final Batch Loss: 0.6727125644683838\n",
      "Epoch 726, Loss: 2.9228471517562866, Final Batch Loss: 0.7187045216560364\n",
      "Epoch 727, Loss: 2.8087589740753174, Final Batch Loss: 0.6058691143989563\n",
      "Epoch 728, Loss: 2.9573182463645935, Final Batch Loss: 0.5207237005233765\n",
      "Epoch 729, Loss: 2.6953466534614563, Final Batch Loss: 0.4941377639770508\n",
      "Epoch 730, Loss: 3.0479096174240112, Final Batch Loss: 0.6782850027084351\n",
      "Epoch 731, Loss: 2.798019379377365, Final Batch Loss: 0.4743993580341339\n",
      "Epoch 732, Loss: 2.7933955788612366, Final Batch Loss: 0.5673242211341858\n",
      "Epoch 733, Loss: 2.676071733236313, Final Batch Loss: 0.45587727427482605\n",
      "Epoch 734, Loss: 2.7968899309635162, Final Batch Loss: 0.6018776297569275\n",
      "Epoch 735, Loss: 2.852907717227936, Final Batch Loss: 0.6053197979927063\n",
      "Epoch 736, Loss: 2.789955735206604, Final Batch Loss: 0.5264618992805481\n",
      "Epoch 737, Loss: 2.9581755995750427, Final Batch Loss: 0.790999710559845\n",
      "Epoch 738, Loss: 2.9965322017669678, Final Batch Loss: 0.6149504780769348\n",
      "Epoch 739, Loss: 3.0641501545906067, Final Batch Loss: 0.6934463977813721\n",
      "Epoch 740, Loss: 2.8885483741760254, Final Batch Loss: 0.544880747795105\n",
      "Epoch 741, Loss: 2.70065376162529, Final Batch Loss: 0.3775022327899933\n",
      "Epoch 742, Loss: 2.838449239730835, Final Batch Loss: 0.5395823121070862\n",
      "Epoch 743, Loss: 2.9606587886810303, Final Batch Loss: 0.6372714638710022\n",
      "Epoch 744, Loss: 2.680359423160553, Final Batch Loss: 0.491205632686615\n",
      "Epoch 745, Loss: 2.777305096387863, Final Batch Loss: 0.5212801694869995\n",
      "Epoch 746, Loss: 2.74382746219635, Final Batch Loss: 0.5157836079597473\n",
      "Epoch 747, Loss: 2.803487151861191, Final Batch Loss: 0.5430645942687988\n",
      "Epoch 748, Loss: 2.849199593067169, Final Batch Loss: 0.6242665648460388\n",
      "Epoch 749, Loss: 3.0168616771698, Final Batch Loss: 0.5961165428161621\n",
      "Epoch 750, Loss: 2.830746829509735, Final Batch Loss: 0.6361844539642334\n",
      "Epoch 751, Loss: 2.928953528404236, Final Batch Loss: 0.6786729097366333\n",
      "Epoch 752, Loss: 2.730270951986313, Final Batch Loss: 0.6661251187324524\n",
      "Epoch 753, Loss: 2.9051714539527893, Final Batch Loss: 0.6478303074836731\n",
      "Epoch 754, Loss: 2.9074645042419434, Final Batch Loss: 0.5397202372550964\n",
      "Epoch 755, Loss: 2.8121133148670197, Final Batch Loss: 0.4735756814479828\n",
      "Epoch 756, Loss: 2.849456310272217, Final Batch Loss: 0.5618137717247009\n",
      "Epoch 757, Loss: 2.742328256368637, Final Batch Loss: 0.525672197341919\n",
      "Epoch 758, Loss: 2.831946521997452, Final Batch Loss: 0.47507551312446594\n",
      "Epoch 759, Loss: 2.7972692251205444, Final Batch Loss: 0.49641889333724976\n",
      "Epoch 760, Loss: 2.796461760997772, Final Batch Loss: 0.554131805896759\n",
      "Epoch 761, Loss: 2.7783443927764893, Final Batch Loss: 0.4938865303993225\n",
      "Epoch 762, Loss: 2.813241720199585, Final Batch Loss: 0.5145622491836548\n",
      "Epoch 763, Loss: 2.58115953207016, Final Batch Loss: 0.48314106464385986\n",
      "Epoch 764, Loss: 2.6549233198165894, Final Batch Loss: 0.4606906473636627\n",
      "Epoch 765, Loss: 2.6959939002990723, Final Batch Loss: 0.48519688844680786\n",
      "Epoch 766, Loss: 2.614515542984009, Final Batch Loss: 0.44965580105781555\n",
      "Epoch 767, Loss: 2.9342823028564453, Final Batch Loss: 0.6050034761428833\n",
      "Epoch 768, Loss: 2.813999503850937, Final Batch Loss: 0.681533694267273\n",
      "Epoch 769, Loss: 2.6591250896453857, Final Batch Loss: 0.5858023762702942\n",
      "Epoch 770, Loss: 2.7197732031345367, Final Batch Loss: 0.4847727119922638\n",
      "Epoch 771, Loss: 2.7265689969062805, Final Batch Loss: 0.48357638716697693\n",
      "Epoch 772, Loss: 2.9580847918987274, Final Batch Loss: 0.693361222743988\n",
      "Epoch 773, Loss: 2.8042185306549072, Final Batch Loss: 0.5250769257545471\n",
      "Epoch 774, Loss: 2.6574862003326416, Final Batch Loss: 0.49867337942123413\n",
      "Epoch 775, Loss: 2.7813058495521545, Final Batch Loss: 0.5879092216491699\n",
      "Epoch 776, Loss: 2.7539471089839935, Final Batch Loss: 0.5238900780677795\n",
      "Epoch 777, Loss: 2.920554459095001, Final Batch Loss: 0.6316527128219604\n",
      "Epoch 778, Loss: 2.721486061811447, Final Batch Loss: 0.5435733795166016\n",
      "Epoch 779, Loss: 2.9315448999404907, Final Batch Loss: 0.6310709714889526\n",
      "Epoch 780, Loss: 2.6327989995479584, Final Batch Loss: 0.4427313804626465\n",
      "Epoch 781, Loss: 2.718849927186966, Final Batch Loss: 0.4615791141986847\n",
      "Epoch 782, Loss: 2.811056077480316, Final Batch Loss: 0.4819587469100952\n",
      "Epoch 783, Loss: 2.7646461129188538, Final Batch Loss: 0.541205644607544\n",
      "Epoch 784, Loss: 2.723839282989502, Final Batch Loss: 0.587886393070221\n",
      "Epoch 785, Loss: 2.7449707090854645, Final Batch Loss: 0.529114842414856\n",
      "Epoch 786, Loss: 2.764332413673401, Final Batch Loss: 0.5334650278091431\n",
      "Epoch 787, Loss: 2.763265997171402, Final Batch Loss: 0.495047926902771\n",
      "Epoch 788, Loss: 2.8828670978546143, Final Batch Loss: 0.6087795495986938\n",
      "Epoch 789, Loss: 2.436118006706238, Final Batch Loss: 0.2619791030883789\n",
      "Epoch 790, Loss: 2.682317078113556, Final Batch Loss: 0.5945231318473816\n",
      "Epoch 791, Loss: 2.826376259326935, Final Batch Loss: 0.5471242070198059\n",
      "Epoch 792, Loss: 2.887523591518402, Final Batch Loss: 0.6951931715011597\n",
      "Epoch 793, Loss: 2.793149560689926, Final Batch Loss: 0.5968079566955566\n",
      "Epoch 794, Loss: 2.8656570315361023, Final Batch Loss: 0.6301605701446533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 795, Loss: 2.736387610435486, Final Batch Loss: 0.5195425152778625\n",
      "Epoch 796, Loss: 2.68201944231987, Final Batch Loss: 0.575725793838501\n",
      "Epoch 797, Loss: 2.8735129833221436, Final Batch Loss: 0.5858970880508423\n",
      "Epoch 798, Loss: 2.7307068705558777, Final Batch Loss: 0.5029621124267578\n",
      "Epoch 799, Loss: 2.832285165786743, Final Batch Loss: 0.6391468644142151\n",
      "Epoch 800, Loss: 2.7937327921390533, Final Batch Loss: 0.39910706877708435\n",
      "Epoch 801, Loss: 2.9944925904273987, Final Batch Loss: 0.7219603061676025\n",
      "Epoch 802, Loss: 2.8512690663337708, Final Batch Loss: 0.565394401550293\n",
      "Epoch 803, Loss: 2.6508306860923767, Final Batch Loss: 0.36092185974121094\n",
      "Epoch 804, Loss: 2.601835936307907, Final Batch Loss: 0.5967854857444763\n",
      "Epoch 805, Loss: 2.7329990565776825, Final Batch Loss: 0.49837103486061096\n",
      "Epoch 806, Loss: 2.574272572994232, Final Batch Loss: 0.3557423949241638\n",
      "Epoch 807, Loss: 2.6116380989551544, Final Batch Loss: 0.4456777572631836\n",
      "Epoch 808, Loss: 2.9898704290390015, Final Batch Loss: 0.6935122609138489\n",
      "Epoch 809, Loss: 2.748160809278488, Final Batch Loss: 0.5852740406990051\n",
      "Epoch 810, Loss: 2.6441394984722137, Final Batch Loss: 0.5717698335647583\n",
      "Epoch 811, Loss: 2.6946841180324554, Final Batch Loss: 0.5738617181777954\n",
      "Epoch 812, Loss: 2.8238068222999573, Final Batch Loss: 0.6160297989845276\n",
      "Epoch 813, Loss: 2.679393917322159, Final Batch Loss: 0.5383448600769043\n",
      "Epoch 814, Loss: 2.825536072254181, Final Batch Loss: 0.6796594858169556\n",
      "Epoch 815, Loss: 3.030725657939911, Final Batch Loss: 0.7196173071861267\n",
      "Epoch 816, Loss: 2.6755731999874115, Final Batch Loss: 0.544812798500061\n",
      "Epoch 817, Loss: 2.6993321776390076, Final Batch Loss: 0.6247457265853882\n",
      "Epoch 818, Loss: 2.5267400443553925, Final Batch Loss: 0.4040905237197876\n",
      "Epoch 819, Loss: 2.6185109615325928, Final Batch Loss: 0.4721478521823883\n",
      "Epoch 820, Loss: 2.7175624072551727, Final Batch Loss: 0.5302771329879761\n",
      "Epoch 821, Loss: 2.73708376288414, Final Batch Loss: 0.5657479166984558\n",
      "Epoch 822, Loss: 2.637754052877426, Final Batch Loss: 0.4542558789253235\n",
      "Epoch 823, Loss: 2.4664508402347565, Final Batch Loss: 0.4491574764251709\n",
      "Epoch 824, Loss: 2.676578015089035, Final Batch Loss: 0.4730144441127777\n",
      "Epoch 825, Loss: 2.9357816576957703, Final Batch Loss: 0.6755782961845398\n",
      "Epoch 826, Loss: 2.56115859746933, Final Batch Loss: 0.506081223487854\n",
      "Epoch 827, Loss: 2.474951148033142, Final Batch Loss: 0.369154691696167\n",
      "Epoch 828, Loss: 2.7858752608299255, Final Batch Loss: 0.5557035803794861\n",
      "Epoch 829, Loss: 2.5429103672504425, Final Batch Loss: 0.4049406051635742\n",
      "Epoch 830, Loss: 2.65122327208519, Final Batch Loss: 0.5243626236915588\n",
      "Epoch 831, Loss: 2.551572471857071, Final Batch Loss: 0.49407270550727844\n",
      "Epoch 832, Loss: 2.6403330862522125, Final Batch Loss: 0.5213232636451721\n",
      "Epoch 833, Loss: 2.7119460701942444, Final Batch Loss: 0.5515215396881104\n",
      "Epoch 834, Loss: 2.5977748036384583, Final Batch Loss: 0.47899478673934937\n",
      "Epoch 835, Loss: 2.696130692958832, Final Batch Loss: 0.5169159770011902\n",
      "Epoch 836, Loss: 2.6812306940555573, Final Batch Loss: 0.4890414774417877\n",
      "Epoch 837, Loss: 2.8347662687301636, Final Batch Loss: 0.6875339150428772\n",
      "Epoch 838, Loss: 2.668049693107605, Final Batch Loss: 0.6617202758789062\n",
      "Epoch 839, Loss: 2.68454909324646, Final Batch Loss: 0.515232503414154\n",
      "Epoch 840, Loss: 2.6544941663742065, Final Batch Loss: 0.44723421335220337\n",
      "Epoch 841, Loss: 2.6778631806373596, Final Batch Loss: 0.6709690093994141\n",
      "Epoch 842, Loss: 2.668361008167267, Final Batch Loss: 0.5211172699928284\n",
      "Epoch 843, Loss: 2.790714979171753, Final Batch Loss: 0.6205325126647949\n",
      "Epoch 844, Loss: 2.5839500725269318, Final Batch Loss: 0.5870585441589355\n",
      "Epoch 845, Loss: 2.8018760085105896, Final Batch Loss: 0.5009292364120483\n",
      "Epoch 846, Loss: 2.8397629261016846, Final Batch Loss: 0.7016195058822632\n",
      "Epoch 847, Loss: 2.750706911087036, Final Batch Loss: 0.5134114027023315\n",
      "Epoch 848, Loss: 2.6066905856132507, Final Batch Loss: 0.5293225049972534\n",
      "Epoch 849, Loss: 2.7074981033802032, Final Batch Loss: 0.608163058757782\n",
      "Epoch 850, Loss: 2.662236303091049, Final Batch Loss: 0.5368445515632629\n",
      "Epoch 851, Loss: 2.8180883824825287, Final Batch Loss: 0.4707435071468353\n",
      "Epoch 852, Loss: 2.7179768085479736, Final Batch Loss: 0.5764478445053101\n",
      "Epoch 853, Loss: 2.664999544620514, Final Batch Loss: 0.6041288375854492\n",
      "Epoch 854, Loss: 2.6469746232032776, Final Batch Loss: 0.38710927963256836\n",
      "Epoch 855, Loss: 2.5324589610099792, Final Batch Loss: 0.43620893359184265\n",
      "Epoch 856, Loss: 2.5724401473999023, Final Batch Loss: 0.3908759653568268\n",
      "Epoch 857, Loss: 2.7582901418209076, Final Batch Loss: 0.5853087902069092\n",
      "Epoch 858, Loss: 2.6584098041057587, Final Batch Loss: 0.4570963382720947\n",
      "Epoch 859, Loss: 2.647772789001465, Final Batch Loss: 0.6856838464736938\n",
      "Epoch 860, Loss: 2.5480899810791016, Final Batch Loss: 0.5648009777069092\n",
      "Epoch 861, Loss: 2.3929485380649567, Final Batch Loss: 0.4582369029521942\n",
      "Epoch 862, Loss: 2.908242642879486, Final Batch Loss: 0.6099399924278259\n",
      "Epoch 863, Loss: 2.8256126642227173, Final Batch Loss: 0.6868677735328674\n",
      "Epoch 864, Loss: 2.412987142801285, Final Batch Loss: 0.40356922149658203\n",
      "Epoch 865, Loss: 2.841906040906906, Final Batch Loss: 0.6217321753501892\n",
      "Epoch 866, Loss: 2.914387345314026, Final Batch Loss: 0.8394893407821655\n",
      "Epoch 867, Loss: 2.582514703273773, Final Batch Loss: 0.5062592029571533\n",
      "Epoch 868, Loss: 2.5497922599315643, Final Batch Loss: 0.3296842873096466\n",
      "Epoch 869, Loss: 2.802355408668518, Final Batch Loss: 0.6295824646949768\n",
      "Epoch 870, Loss: 2.7189913988113403, Final Batch Loss: 0.7131756544113159\n",
      "Epoch 871, Loss: 2.6469072699546814, Final Batch Loss: 0.5420179963111877\n",
      "Epoch 872, Loss: 2.6514512300491333, Final Batch Loss: 0.514589250087738\n",
      "Epoch 873, Loss: 2.747971832752228, Final Batch Loss: 0.6084240674972534\n",
      "Epoch 874, Loss: 2.465616673231125, Final Batch Loss: 0.3854544162750244\n",
      "Epoch 875, Loss: 2.417367994785309, Final Batch Loss: 0.36807435750961304\n",
      "Epoch 876, Loss: 2.5947268307209015, Final Batch Loss: 0.598970353603363\n",
      "Epoch 877, Loss: 2.6468966007232666, Final Batch Loss: 0.6340869665145874\n",
      "Epoch 878, Loss: 2.687797725200653, Final Batch Loss: 0.5265194773674011\n",
      "Epoch 879, Loss: 2.795692592859268, Final Batch Loss: 0.7271652817726135\n",
      "Epoch 880, Loss: 2.591376394033432, Final Batch Loss: 0.43194469809532166\n",
      "Epoch 881, Loss: 2.6561771631240845, Final Batch Loss: 0.5685141086578369\n",
      "Epoch 882, Loss: 2.7043561935424805, Final Batch Loss: 0.5499485731124878\n",
      "Epoch 883, Loss: 2.5432552993297577, Final Batch Loss: 0.45336592197418213\n",
      "Epoch 884, Loss: 2.616446077823639, Final Batch Loss: 0.5304400324821472\n",
      "Epoch 885, Loss: 2.604719966650009, Final Batch Loss: 0.5432130098342896\n",
      "Epoch 886, Loss: 2.7026353776454926, Final Batch Loss: 0.7002989649772644\n",
      "Epoch 887, Loss: 2.6486257016658783, Final Batch Loss: 0.5114588737487793\n",
      "Epoch 888, Loss: 2.6007769107818604, Final Batch Loss: 0.5674745440483093\n",
      "Epoch 889, Loss: 2.6810934841632843, Final Batch Loss: 0.6179871559143066\n",
      "Epoch 890, Loss: 2.779756933450699, Final Batch Loss: 0.6855831742286682\n",
      "Epoch 891, Loss: 2.6749033629894257, Final Batch Loss: 0.4668230414390564\n",
      "Epoch 892, Loss: 2.5083757638931274, Final Batch Loss: 0.48467057943344116\n",
      "Epoch 893, Loss: 2.6365796327590942, Final Batch Loss: 0.557097315788269\n",
      "Epoch 894, Loss: 2.7067639231681824, Final Batch Loss: 0.5686688423156738\n",
      "Epoch 895, Loss: 2.7958373427391052, Final Batch Loss: 0.7104015350341797\n",
      "Epoch 896, Loss: 2.5096316635608673, Final Batch Loss: 0.39046618342399597\n",
      "Epoch 897, Loss: 2.656831234693527, Final Batch Loss: 0.5468673706054688\n",
      "Epoch 898, Loss: 2.6004786491394043, Final Batch Loss: 0.5791285037994385\n",
      "Epoch 899, Loss: 2.4538584649562836, Final Batch Loss: 0.38036489486694336\n",
      "Epoch 900, Loss: 2.483471304178238, Final Batch Loss: 0.39386746287345886\n",
      "Epoch 901, Loss: 2.72417551279068, Final Batch Loss: 0.5827574133872986\n",
      "Epoch 902, Loss: 2.47847843170166, Final Batch Loss: 0.4980587661266327\n",
      "Epoch 903, Loss: 2.591326594352722, Final Batch Loss: 0.5422343015670776\n",
      "Epoch 904, Loss: 2.5532669723033905, Final Batch Loss: 0.47081074118614197\n",
      "Epoch 905, Loss: 2.397048532962799, Final Batch Loss: 0.2800297141075134\n",
      "Epoch 906, Loss: 2.512576788663864, Final Batch Loss: 0.42613843083381653\n",
      "Epoch 907, Loss: 2.770594209432602, Final Batch Loss: 0.691408097743988\n",
      "Epoch 908, Loss: 2.487620621919632, Final Batch Loss: 0.3727981150150299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 909, Loss: 2.4513693153858185, Final Batch Loss: 0.3261500597000122\n",
      "Epoch 910, Loss: 2.5506858229637146, Final Batch Loss: 0.46757835149765015\n",
      "Epoch 911, Loss: 2.6199413537979126, Final Batch Loss: 0.5231220126152039\n",
      "Epoch 912, Loss: 2.7396809458732605, Final Batch Loss: 0.5994558334350586\n",
      "Epoch 913, Loss: 2.5440875589847565, Final Batch Loss: 0.3912627100944519\n",
      "Epoch 914, Loss: 2.6028039753437042, Final Batch Loss: 0.5536304116249084\n",
      "Epoch 915, Loss: 2.559035301208496, Final Batch Loss: 0.5234939455986023\n",
      "Epoch 916, Loss: 2.6244779229164124, Final Batch Loss: 0.5696920156478882\n",
      "Epoch 917, Loss: 2.557805597782135, Final Batch Loss: 0.4692932665348053\n",
      "Epoch 918, Loss: 2.5879932045936584, Final Batch Loss: 0.5354155898094177\n",
      "Epoch 919, Loss: 2.4140418469905853, Final Batch Loss: 0.5196645855903625\n",
      "Epoch 920, Loss: 2.6598072946071625, Final Batch Loss: 0.5971921682357788\n",
      "Epoch 921, Loss: 2.723344087600708, Final Batch Loss: 0.560920238494873\n",
      "Epoch 922, Loss: 2.776252418756485, Final Batch Loss: 0.746658980846405\n",
      "Epoch 923, Loss: 2.41619336605072, Final Batch Loss: 0.4077455699443817\n",
      "Epoch 924, Loss: 2.6418614387512207, Final Batch Loss: 0.5303070545196533\n",
      "Epoch 925, Loss: 2.500534474849701, Final Batch Loss: 0.3862990736961365\n",
      "Epoch 926, Loss: 2.423117309808731, Final Batch Loss: 0.4867071807384491\n",
      "Epoch 927, Loss: 2.525904029607773, Final Batch Loss: 0.5177260637283325\n",
      "Epoch 928, Loss: 2.574038952589035, Final Batch Loss: 0.593543291091919\n",
      "Epoch 929, Loss: 2.5691197514533997, Final Batch Loss: 0.5260483026504517\n",
      "Epoch 930, Loss: 2.6218208372592926, Final Batch Loss: 0.4611538350582123\n",
      "Epoch 931, Loss: 2.606694996356964, Final Batch Loss: 0.5209760665893555\n",
      "Epoch 932, Loss: 2.5494143664836884, Final Batch Loss: 0.5226758122444153\n",
      "Epoch 933, Loss: 2.676601856946945, Final Batch Loss: 0.5641665458679199\n",
      "Epoch 934, Loss: 2.4902369678020477, Final Batch Loss: 0.4778521656990051\n",
      "Epoch 935, Loss: 2.6162052154541016, Final Batch Loss: 0.5722146034240723\n",
      "Epoch 936, Loss: 2.529128760099411, Final Batch Loss: 0.4380382299423218\n",
      "Epoch 937, Loss: 2.4985519647598267, Final Batch Loss: 0.4261152446269989\n",
      "Epoch 938, Loss: 2.528692275285721, Final Batch Loss: 0.46372199058532715\n",
      "Epoch 939, Loss: 2.5836935937404633, Final Batch Loss: 0.49814802408218384\n",
      "Epoch 940, Loss: 2.4631527960300446, Final Batch Loss: 0.4619240164756775\n",
      "Epoch 941, Loss: 2.477733790874481, Final Batch Loss: 0.48499539494514465\n",
      "Epoch 942, Loss: 2.6876109838485718, Final Batch Loss: 0.6099718809127808\n",
      "Epoch 943, Loss: 2.623592287302017, Final Batch Loss: 0.6378194093704224\n",
      "Epoch 944, Loss: 2.641174226999283, Final Batch Loss: 0.5082241296768188\n",
      "Epoch 945, Loss: 2.663380980491638, Final Batch Loss: 0.5326729416847229\n",
      "Epoch 946, Loss: 2.4513239562511444, Final Batch Loss: 0.39794930815696716\n",
      "Epoch 947, Loss: 2.5800870060920715, Final Batch Loss: 0.5455135703086853\n",
      "Epoch 948, Loss: 2.6300568282604218, Final Batch Loss: 0.639674723148346\n",
      "Epoch 949, Loss: 2.4226430356502533, Final Batch Loss: 0.3582330644130707\n",
      "Epoch 950, Loss: 2.456897646188736, Final Batch Loss: 0.417244017124176\n",
      "Epoch 951, Loss: 2.3901858031749725, Final Batch Loss: 0.5044838190078735\n",
      "Epoch 952, Loss: 2.4741279780864716, Final Batch Loss: 0.38044703006744385\n",
      "Epoch 953, Loss: 2.388425290584564, Final Batch Loss: 0.4087953269481659\n",
      "Epoch 954, Loss: 2.596241056919098, Final Batch Loss: 0.5168843865394592\n",
      "Epoch 955, Loss: 2.6790828108787537, Final Batch Loss: 0.6182384490966797\n",
      "Epoch 956, Loss: 2.5007431507110596, Final Batch Loss: 0.48748740553855896\n",
      "Epoch 957, Loss: 2.5768330097198486, Final Batch Loss: 0.5736122131347656\n",
      "Epoch 958, Loss: 2.485362231731415, Final Batch Loss: 0.4536832869052887\n",
      "Epoch 959, Loss: 2.61706206202507, Final Batch Loss: 0.4713975489139557\n",
      "Epoch 960, Loss: 2.4763284027576447, Final Batch Loss: 0.4256889522075653\n",
      "Epoch 961, Loss: 2.5465929210186005, Final Batch Loss: 0.5455300211906433\n",
      "Epoch 962, Loss: 2.412311017513275, Final Batch Loss: 0.42819520831108093\n",
      "Epoch 963, Loss: 2.525525689125061, Final Batch Loss: 0.49731341004371643\n",
      "Epoch 964, Loss: 2.452733635902405, Final Batch Loss: 0.4187625050544739\n",
      "Epoch 965, Loss: 2.5942147374153137, Final Batch Loss: 0.5040771961212158\n",
      "Epoch 966, Loss: 2.4811988174915314, Final Batch Loss: 0.3936822712421417\n",
      "Epoch 967, Loss: 2.413997620344162, Final Batch Loss: 0.4435642659664154\n",
      "Epoch 968, Loss: 2.4256860613822937, Final Batch Loss: 0.5076254606246948\n",
      "Epoch 969, Loss: 2.334138035774231, Final Batch Loss: 0.39115485548973083\n",
      "Epoch 970, Loss: 2.358649581670761, Final Batch Loss: 0.3960914611816406\n",
      "Epoch 971, Loss: 2.589530438184738, Final Batch Loss: 0.5541022419929504\n",
      "Epoch 972, Loss: 2.4610157012939453, Final Batch Loss: 0.45125719904899597\n",
      "Epoch 973, Loss: 2.5475988388061523, Final Batch Loss: 0.5290972590446472\n",
      "Epoch 974, Loss: 2.3447161316871643, Final Batch Loss: 0.3844120502471924\n",
      "Epoch 975, Loss: 2.4464279413223267, Final Batch Loss: 0.5486364960670471\n",
      "Epoch 976, Loss: 2.580683618783951, Final Batch Loss: 0.5638960599899292\n",
      "Epoch 977, Loss: 2.6032842695713043, Final Batch Loss: 0.5467108488082886\n",
      "Epoch 978, Loss: 2.389849156141281, Final Batch Loss: 0.4183046519756317\n",
      "Epoch 979, Loss: 2.5137181282043457, Final Batch Loss: 0.4901955723762512\n",
      "Epoch 980, Loss: 2.441365510225296, Final Batch Loss: 0.5198399424552917\n",
      "Epoch 981, Loss: 2.5481252670288086, Final Batch Loss: 0.5569812655448914\n",
      "Epoch 982, Loss: 2.4966089129447937, Final Batch Loss: 0.4095451235771179\n",
      "Epoch 983, Loss: 2.5910196900367737, Final Batch Loss: 0.6215246915817261\n",
      "Epoch 984, Loss: 2.4365444779396057, Final Batch Loss: 0.4118548631668091\n",
      "Epoch 985, Loss: 2.76340851187706, Final Batch Loss: 0.6837948560714722\n",
      "Epoch 986, Loss: 2.345321625471115, Final Batch Loss: 0.3965630829334259\n",
      "Epoch 987, Loss: 2.550501525402069, Final Batch Loss: 0.5518885254859924\n",
      "Epoch 988, Loss: 2.436327964067459, Final Batch Loss: 0.45288607478141785\n",
      "Epoch 989, Loss: 2.4320048093795776, Final Batch Loss: 0.47503551840782166\n",
      "Epoch 990, Loss: 2.496363341808319, Final Batch Loss: 0.4833642542362213\n",
      "Epoch 991, Loss: 2.618290990591049, Final Batch Loss: 0.4896661341190338\n",
      "Epoch 992, Loss: 2.5332437455654144, Final Batch Loss: 0.45871859788894653\n",
      "Epoch 993, Loss: 2.494746655225754, Final Batch Loss: 0.4082302153110504\n",
      "Epoch 994, Loss: 2.702207922935486, Final Batch Loss: 0.7294877767562866\n",
      "Epoch 995, Loss: 2.267087757587433, Final Batch Loss: 0.3689473271369934\n",
      "Epoch 996, Loss: 2.383576363325119, Final Batch Loss: 0.3402946889400482\n",
      "Epoch 997, Loss: 2.4253266751766205, Final Batch Loss: 0.4461933970451355\n",
      "Epoch 998, Loss: 2.6663211584091187, Final Batch Loss: 0.5793446898460388\n",
      "Epoch 999, Loss: 2.598860889673233, Final Batch Loss: 0.6219247579574585\n",
      "Epoch 1000, Loss: 2.635634630918503, Final Batch Loss: 0.421506404876709\n",
      "Epoch 1001, Loss: 2.6260606050491333, Final Batch Loss: 0.5958579182624817\n",
      "Epoch 1002, Loss: 2.5470751523971558, Final Batch Loss: 0.43627429008483887\n",
      "Epoch 1003, Loss: 2.3530605733394623, Final Batch Loss: 0.45706063508987427\n",
      "Epoch 1004, Loss: 2.4627940356731415, Final Batch Loss: 0.524955153465271\n",
      "Epoch 1005, Loss: 2.3788450062274933, Final Batch Loss: 0.5005006194114685\n",
      "Epoch 1006, Loss: 2.437816798686981, Final Batch Loss: 0.5075901746749878\n",
      "Epoch 1007, Loss: 2.383033961057663, Final Batch Loss: 0.370159387588501\n",
      "Epoch 1008, Loss: 2.68691486120224, Final Batch Loss: 0.6874241232872009\n",
      "Epoch 1009, Loss: 2.4841757714748383, Final Batch Loss: 0.3636147677898407\n",
      "Epoch 1010, Loss: 2.5562854409217834, Final Batch Loss: 0.47404083609580994\n",
      "Epoch 1011, Loss: 2.5685595273971558, Final Batch Loss: 0.499549925327301\n",
      "Epoch 1012, Loss: 2.4057290256023407, Final Batch Loss: 0.5424859523773193\n",
      "Epoch 1013, Loss: 2.4241539239883423, Final Batch Loss: 0.4315049350261688\n",
      "Epoch 1014, Loss: 2.3532387912273407, Final Batch Loss: 0.3595382273197174\n",
      "Epoch 1015, Loss: 2.501702904701233, Final Batch Loss: 0.5186212658882141\n",
      "Epoch 1016, Loss: 2.706107199192047, Final Batch Loss: 0.6540073156356812\n",
      "Epoch 1017, Loss: 2.379919946193695, Final Batch Loss: 0.45663416385650635\n",
      "Epoch 1018, Loss: 2.498180389404297, Final Batch Loss: 0.5440097451210022\n",
      "Epoch 1019, Loss: 2.7442786395549774, Final Batch Loss: 0.8035131692886353\n",
      "Epoch 1020, Loss: 2.482055127620697, Final Batch Loss: 0.5248423218727112\n",
      "Epoch 1021, Loss: 2.621208429336548, Final Batch Loss: 0.6243806481361389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1022, Loss: 2.594213515520096, Final Batch Loss: 0.6075422167778015\n",
      "Epoch 1023, Loss: 2.4986975491046906, Final Batch Loss: 0.4334322512149811\n",
      "Epoch 1024, Loss: 2.47805717587471, Final Batch Loss: 0.44097915291786194\n",
      "Epoch 1025, Loss: 2.4792516827583313, Final Batch Loss: 0.5126574635505676\n",
      "Epoch 1026, Loss: 2.4220156967639923, Final Batch Loss: 0.42499902844429016\n",
      "Epoch 1027, Loss: 2.6415231823921204, Final Batch Loss: 0.5595811605453491\n",
      "Epoch 1028, Loss: 2.2520280480384827, Final Batch Loss: 0.40734416246414185\n",
      "Epoch 1029, Loss: 2.3539303839206696, Final Batch Loss: 0.4068142771720886\n",
      "Epoch 1030, Loss: 2.4755197763442993, Final Batch Loss: 0.4727267026901245\n",
      "Epoch 1031, Loss: 2.534858852624893, Final Batch Loss: 0.5903702974319458\n",
      "Epoch 1032, Loss: 2.4135071635246277, Final Batch Loss: 0.6051039099693298\n",
      "Epoch 1033, Loss: 2.50634041428566, Final Batch Loss: 0.40365952253341675\n",
      "Epoch 1034, Loss: 2.4065379202365875, Final Batch Loss: 0.3434450328350067\n",
      "Epoch 1035, Loss: 2.439888060092926, Final Batch Loss: 0.5897717475891113\n",
      "Epoch 1036, Loss: 2.6557192504405975, Final Batch Loss: 0.707556962966919\n",
      "Epoch 1037, Loss: 2.4256635904312134, Final Batch Loss: 0.34417733550071716\n",
      "Epoch 1038, Loss: 2.4417529702186584, Final Batch Loss: 0.48239758610725403\n",
      "Epoch 1039, Loss: 2.3914029002189636, Final Batch Loss: 0.4960360825061798\n",
      "Epoch 1040, Loss: 2.5575221478939056, Final Batch Loss: 0.6066370606422424\n",
      "Epoch 1041, Loss: 2.400066912174225, Final Batch Loss: 0.5644931197166443\n",
      "Epoch 1042, Loss: 2.4203203320503235, Final Batch Loss: 0.5075401663780212\n",
      "Epoch 1043, Loss: 2.511339545249939, Final Batch Loss: 0.5811352729797363\n",
      "Epoch 1044, Loss: 2.4853775799274445, Final Batch Loss: 0.5429176092147827\n",
      "Epoch 1045, Loss: 2.652749001979828, Final Batch Loss: 0.6410760879516602\n",
      "Epoch 1046, Loss: 2.5444962978363037, Final Batch Loss: 0.4292653203010559\n",
      "Epoch 1047, Loss: 2.5108273327350616, Final Batch Loss: 0.5768451690673828\n",
      "Epoch 1048, Loss: 2.2808124125003815, Final Batch Loss: 0.3988306522369385\n",
      "Epoch 1049, Loss: 2.521740347146988, Final Batch Loss: 0.5427864193916321\n",
      "Epoch 1050, Loss: 2.601771116256714, Final Batch Loss: 0.6669696569442749\n",
      "Epoch 1051, Loss: 2.3175735473632812, Final Batch Loss: 0.3901819586753845\n",
      "Epoch 1052, Loss: 2.409261107444763, Final Batch Loss: 0.46370047330856323\n",
      "Epoch 1053, Loss: 2.301485985517502, Final Batch Loss: 0.3153032958507538\n",
      "Epoch 1054, Loss: 2.702391743659973, Final Batch Loss: 0.7310239672660828\n",
      "Epoch 1055, Loss: 2.4446211457252502, Final Batch Loss: 0.49588388204574585\n",
      "Epoch 1056, Loss: 2.3981640338897705, Final Batch Loss: 0.37925395369529724\n",
      "Epoch 1057, Loss: 2.519196391105652, Final Batch Loss: 0.5473068356513977\n",
      "Epoch 1058, Loss: 2.4762937426567078, Final Batch Loss: 0.4768821895122528\n",
      "Epoch 1059, Loss: 2.3469102680683136, Final Batch Loss: 0.3980332016944885\n",
      "Epoch 1060, Loss: 2.3634321689605713, Final Batch Loss: 0.3666543960571289\n",
      "Epoch 1061, Loss: 2.545629858970642, Final Batch Loss: 0.5121696591377258\n",
      "Epoch 1062, Loss: 2.623975843191147, Final Batch Loss: 0.7363204956054688\n",
      "Epoch 1063, Loss: 2.312251716852188, Final Batch Loss: 0.4261014461517334\n",
      "Epoch 1064, Loss: 2.2194064259529114, Final Batch Loss: 0.3610604703426361\n",
      "Epoch 1065, Loss: 2.375934600830078, Final Batch Loss: 0.4117920994758606\n",
      "Epoch 1066, Loss: 2.3270325362682343, Final Batch Loss: 0.37788164615631104\n",
      "Epoch 1067, Loss: 2.391093373298645, Final Batch Loss: 0.5132650136947632\n",
      "Epoch 1068, Loss: 2.55689936876297, Final Batch Loss: 0.5337100625038147\n",
      "Epoch 1069, Loss: 2.460943579673767, Final Batch Loss: 0.3708668053150177\n",
      "Epoch 1070, Loss: 2.3320198357105255, Final Batch Loss: 0.46902307868003845\n",
      "Epoch 1071, Loss: 2.420869141817093, Final Batch Loss: 0.4667285978794098\n",
      "Epoch 1072, Loss: 2.3232938051223755, Final Batch Loss: 0.4671918749809265\n",
      "Epoch 1073, Loss: 2.2175551652908325, Final Batch Loss: 0.3215724229812622\n",
      "Epoch 1074, Loss: 2.7195616364479065, Final Batch Loss: 0.6790473461151123\n",
      "Epoch 1075, Loss: 2.304220199584961, Final Batch Loss: 0.4161403179168701\n",
      "Epoch 1076, Loss: 2.3040146231651306, Final Batch Loss: 0.3156767189502716\n",
      "Epoch 1077, Loss: 2.621656596660614, Final Batch Loss: 0.5434682369232178\n",
      "Epoch 1078, Loss: 2.165687769651413, Final Batch Loss: 0.3122817575931549\n",
      "Epoch 1079, Loss: 2.3321340680122375, Final Batch Loss: 0.4226738214492798\n",
      "Epoch 1080, Loss: 2.4731845557689667, Final Batch Loss: 0.4592853784561157\n",
      "Epoch 1081, Loss: 2.518988251686096, Final Batch Loss: 0.654669463634491\n",
      "Epoch 1082, Loss: 2.3502621054649353, Final Batch Loss: 0.3621716797351837\n",
      "Epoch 1083, Loss: 2.4009055495262146, Final Batch Loss: 0.5231871008872986\n",
      "Epoch 1084, Loss: 2.502198964357376, Final Batch Loss: 0.5092886686325073\n",
      "Epoch 1085, Loss: 2.4444897770881653, Final Batch Loss: 0.4322250187397003\n",
      "Epoch 1086, Loss: 2.645967870950699, Final Batch Loss: 0.5494157671928406\n",
      "Epoch 1087, Loss: 2.467720866203308, Final Batch Loss: 0.5653502345085144\n",
      "Epoch 1088, Loss: 2.456264168024063, Final Batch Loss: 0.4826028048992157\n",
      "Epoch 1089, Loss: 2.4506475627422333, Final Batch Loss: 0.5406354069709778\n",
      "Epoch 1090, Loss: 2.4168999195098877, Final Batch Loss: 0.37316569685935974\n",
      "Epoch 1091, Loss: 2.6402449309825897, Final Batch Loss: 0.4741879105567932\n",
      "Epoch 1092, Loss: 2.384172797203064, Final Batch Loss: 0.4902282953262329\n",
      "Epoch 1093, Loss: 2.2679578065872192, Final Batch Loss: 0.40717557072639465\n",
      "Epoch 1094, Loss: 2.4131101965904236, Final Batch Loss: 0.48569533228874207\n",
      "Epoch 1095, Loss: 2.4168684482574463, Final Batch Loss: 0.6093234419822693\n",
      "Epoch 1096, Loss: 2.2800565361976624, Final Batch Loss: 0.3141561448574066\n",
      "Epoch 1097, Loss: 2.4962133169174194, Final Batch Loss: 0.5983877778053284\n",
      "Epoch 1098, Loss: 2.409268319606781, Final Batch Loss: 0.44653037190437317\n",
      "Epoch 1099, Loss: 2.4536343812942505, Final Batch Loss: 0.5196978449821472\n",
      "Epoch 1100, Loss: 2.5508989691734314, Final Batch Loss: 0.48963451385498047\n",
      "Epoch 1101, Loss: 2.3710696697235107, Final Batch Loss: 0.536465585231781\n",
      "Epoch 1102, Loss: 2.4136852025985718, Final Batch Loss: 0.4726557433605194\n",
      "Epoch 1103, Loss: 2.4072290658950806, Final Batch Loss: 0.5288008451461792\n",
      "Epoch 1104, Loss: 2.1893851459026337, Final Batch Loss: 0.28421276807785034\n",
      "Epoch 1105, Loss: 2.4543745815753937, Final Batch Loss: 0.5137569904327393\n",
      "Epoch 1106, Loss: 2.3488180339336395, Final Batch Loss: 0.4522227644920349\n",
      "Epoch 1107, Loss: 2.272115558385849, Final Batch Loss: 0.36524447798728943\n",
      "Epoch 1108, Loss: 2.407015562057495, Final Batch Loss: 0.4969947040081024\n",
      "Epoch 1109, Loss: 2.3308427929878235, Final Batch Loss: 0.4359901249408722\n",
      "Epoch 1110, Loss: 2.4506559669971466, Final Batch Loss: 0.3933735489845276\n",
      "Epoch 1111, Loss: 2.450452208518982, Final Batch Loss: 0.5203531980514526\n",
      "Epoch 1112, Loss: 2.449960768222809, Final Batch Loss: 0.4923458695411682\n",
      "Epoch 1113, Loss: 2.341368019580841, Final Batch Loss: 0.5007091164588928\n",
      "Epoch 1114, Loss: 2.4070987701416016, Final Batch Loss: 0.5646749138832092\n",
      "Epoch 1115, Loss: 2.382687032222748, Final Batch Loss: 0.469560444355011\n",
      "Epoch 1116, Loss: 2.2479828596115112, Final Batch Loss: 0.33422374725341797\n",
      "Epoch 1117, Loss: 2.3195561468601227, Final Batch Loss: 0.41514772176742554\n",
      "Epoch 1118, Loss: 2.464226633310318, Final Batch Loss: 0.5086029171943665\n",
      "Epoch 1119, Loss: 2.3823653757572174, Final Batch Loss: 0.43384501338005066\n",
      "Epoch 1120, Loss: 2.342567503452301, Final Batch Loss: 0.47805196046829224\n",
      "Epoch 1121, Loss: 2.4207555055618286, Final Batch Loss: 0.5703530311584473\n",
      "Epoch 1122, Loss: 2.539464682340622, Final Batch Loss: 0.6799731850624084\n",
      "Epoch 1123, Loss: 2.2805638313293457, Final Batch Loss: 0.4124319851398468\n",
      "Epoch 1124, Loss: 2.4067573249340057, Final Batch Loss: 0.3508439362049103\n",
      "Epoch 1125, Loss: 2.4751736521720886, Final Batch Loss: 0.49271899461746216\n",
      "Epoch 1126, Loss: 2.4849365055561066, Final Batch Loss: 0.6386281251907349\n",
      "Epoch 1127, Loss: 2.577594816684723, Final Batch Loss: 0.5620478987693787\n",
      "Epoch 1128, Loss: 2.6546866297721863, Final Batch Loss: 0.5741589665412903\n",
      "Epoch 1129, Loss: 2.3999796211719513, Final Batch Loss: 0.3569034934043884\n",
      "Epoch 1130, Loss: 2.5079671144485474, Final Batch Loss: 0.4756765067577362\n",
      "Epoch 1131, Loss: 2.477216809988022, Final Batch Loss: 0.5197265148162842\n",
      "Epoch 1132, Loss: 2.457181394100189, Final Batch Loss: 0.559614896774292\n",
      "Epoch 1133, Loss: 2.3965538144111633, Final Batch Loss: 0.5266222953796387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1134, Loss: 2.342779368162155, Final Batch Loss: 0.4945416748523712\n",
      "Epoch 1135, Loss: 2.4647204279899597, Final Batch Loss: 0.3848186135292053\n",
      "Epoch 1136, Loss: 2.5000104010105133, Final Batch Loss: 0.45437735319137573\n",
      "Epoch 1137, Loss: 2.6395720541477203, Final Batch Loss: 0.6826200485229492\n",
      "Epoch 1138, Loss: 2.47994601726532, Final Batch Loss: 0.49657154083251953\n",
      "Epoch 1139, Loss: 2.3735641539096832, Final Batch Loss: 0.4574183225631714\n",
      "Epoch 1140, Loss: 2.2050946056842804, Final Batch Loss: 0.36434394121170044\n",
      "Epoch 1141, Loss: 2.4096699953079224, Final Batch Loss: 0.5115324854850769\n",
      "Epoch 1142, Loss: 2.4588548243045807, Final Batch Loss: 0.488506019115448\n",
      "Epoch 1143, Loss: 2.3090251982212067, Final Batch Loss: 0.36373209953308105\n",
      "Epoch 1144, Loss: 2.4034397900104523, Final Batch Loss: 0.3799878656864166\n",
      "Epoch 1145, Loss: 2.348599374294281, Final Batch Loss: 0.4831658899784088\n",
      "Epoch 1146, Loss: 2.3703278303146362, Final Batch Loss: 0.4745522141456604\n",
      "Epoch 1147, Loss: 2.4156987369060516, Final Batch Loss: 0.5163050889968872\n",
      "Epoch 1148, Loss: 2.3752693831920624, Final Batch Loss: 0.4214962422847748\n",
      "Epoch 1149, Loss: 2.3524211943149567, Final Batch Loss: 0.5076268911361694\n",
      "Epoch 1150, Loss: 2.4350848495960236, Final Batch Loss: 0.44529372453689575\n",
      "Epoch 1151, Loss: 2.2267287373542786, Final Batch Loss: 0.4080785810947418\n",
      "Epoch 1152, Loss: 2.323324978351593, Final Batch Loss: 0.43757930397987366\n",
      "Epoch 1153, Loss: 2.4288467168807983, Final Batch Loss: 0.30769312381744385\n",
      "Epoch 1154, Loss: 2.5359147489070892, Final Batch Loss: 0.5381467938423157\n",
      "Epoch 1155, Loss: 2.3052958250045776, Final Batch Loss: 0.41194644570350647\n",
      "Epoch 1156, Loss: 2.470995992422104, Final Batch Loss: 0.4721582233905792\n",
      "Epoch 1157, Loss: 2.4650255739688873, Final Batch Loss: 0.6085936427116394\n",
      "Epoch 1158, Loss: 2.26139235496521, Final Batch Loss: 0.4639566242694855\n",
      "Epoch 1159, Loss: 2.3768686056137085, Final Batch Loss: 0.4399053752422333\n",
      "Epoch 1160, Loss: 2.3089747428894043, Final Batch Loss: 0.5466954112052917\n",
      "Epoch 1161, Loss: 2.333503007888794, Final Batch Loss: 0.4849003255367279\n",
      "Epoch 1162, Loss: 2.2714688777923584, Final Batch Loss: 0.4483400881290436\n",
      "Epoch 1163, Loss: 2.4159583747386932, Final Batch Loss: 0.5830317139625549\n",
      "Epoch 1164, Loss: 2.491396337747574, Final Batch Loss: 0.4200938940048218\n",
      "Epoch 1165, Loss: 2.6190021336078644, Final Batch Loss: 0.6351428627967834\n",
      "Epoch 1166, Loss: 2.35705628991127, Final Batch Loss: 0.5156793594360352\n",
      "Epoch 1167, Loss: 2.0946266055107117, Final Batch Loss: 0.3018726110458374\n",
      "Epoch 1168, Loss: 2.3739602863788605, Final Batch Loss: 0.4964214265346527\n",
      "Epoch 1169, Loss: 2.4075394570827484, Final Batch Loss: 0.5665001273155212\n",
      "Epoch 1170, Loss: 2.4643582105636597, Final Batch Loss: 0.4756496548652649\n",
      "Epoch 1171, Loss: 2.4366544783115387, Final Batch Loss: 0.5377962589263916\n",
      "Epoch 1172, Loss: 2.2902638912200928, Final Batch Loss: 0.5032830834388733\n",
      "Epoch 1173, Loss: 2.4960838854312897, Final Batch Loss: 0.4538159668445587\n",
      "Epoch 1174, Loss: 2.3089000582695007, Final Batch Loss: 0.4812889099121094\n",
      "Epoch 1175, Loss: 2.1925215423107147, Final Batch Loss: 0.4100954234600067\n",
      "Epoch 1176, Loss: 2.4956234097480774, Final Batch Loss: 0.6104325652122498\n",
      "Epoch 1177, Loss: 2.4915376901626587, Final Batch Loss: 0.4965563714504242\n",
      "Epoch 1178, Loss: 2.50777468085289, Final Batch Loss: 0.7470827698707581\n",
      "Epoch 1179, Loss: 2.422858864068985, Final Batch Loss: 0.6242490410804749\n",
      "Epoch 1180, Loss: 2.220962256193161, Final Batch Loss: 0.35488224029541016\n",
      "Epoch 1181, Loss: 2.357874631881714, Final Batch Loss: 0.5165819525718689\n",
      "Epoch 1182, Loss: 2.3797537088394165, Final Batch Loss: 0.4387598931789398\n",
      "Epoch 1183, Loss: 2.414936602115631, Final Batch Loss: 0.44511473178863525\n",
      "Epoch 1184, Loss: 2.4276116490364075, Final Batch Loss: 0.5505917072296143\n",
      "Epoch 1185, Loss: 2.4367921352386475, Final Batch Loss: 0.3912820518016815\n",
      "Epoch 1186, Loss: 2.277518481016159, Final Batch Loss: 0.5471466183662415\n",
      "Epoch 1187, Loss: 2.4324294924736023, Final Batch Loss: 0.4613574147224426\n",
      "Epoch 1188, Loss: 2.3617935180664062, Final Batch Loss: 0.4102994501590729\n",
      "Epoch 1189, Loss: 2.324313998222351, Final Batch Loss: 0.5214411616325378\n",
      "Epoch 1190, Loss: 2.319886028766632, Final Batch Loss: 0.390001118183136\n",
      "Epoch 1191, Loss: 2.383658140897751, Final Batch Loss: 0.4014725983142853\n",
      "Epoch 1192, Loss: 2.471960484981537, Final Batch Loss: 0.5240100622177124\n",
      "Epoch 1193, Loss: 2.2639378011226654, Final Batch Loss: 0.47005611658096313\n",
      "Epoch 1194, Loss: 2.377272754907608, Final Batch Loss: 0.4197944402694702\n",
      "Epoch 1195, Loss: 2.4153226912021637, Final Batch Loss: 0.47157955169677734\n",
      "Epoch 1196, Loss: 2.5473033487796783, Final Batch Loss: 0.5938783884048462\n",
      "Epoch 1197, Loss: 2.3602093160152435, Final Batch Loss: 0.5306264162063599\n",
      "Epoch 1198, Loss: 2.3497031331062317, Final Batch Loss: 0.4103691577911377\n",
      "Epoch 1199, Loss: 2.142943114042282, Final Batch Loss: 0.3489205837249756\n",
      "Epoch 1200, Loss: 2.5075221061706543, Final Batch Loss: 0.595222532749176\n",
      "Epoch 1201, Loss: 2.3185806274414062, Final Batch Loss: 0.45425930619239807\n",
      "Epoch 1202, Loss: 2.3233984410762787, Final Batch Loss: 0.4714881479740143\n",
      "Epoch 1203, Loss: 2.369789481163025, Final Batch Loss: 0.5011729598045349\n",
      "Epoch 1204, Loss: 2.371834546327591, Final Batch Loss: 0.4643464982509613\n",
      "Epoch 1205, Loss: 2.4876404106616974, Final Batch Loss: 0.6791974306106567\n",
      "Epoch 1206, Loss: 2.1859766244888306, Final Batch Loss: 0.3624500632286072\n",
      "Epoch 1207, Loss: 2.575159013271332, Final Batch Loss: 0.5039912462234497\n",
      "Epoch 1208, Loss: 2.3797614872455597, Final Batch Loss: 0.5368040800094604\n",
      "Epoch 1209, Loss: 2.2681961953639984, Final Batch Loss: 0.3852556347846985\n",
      "Epoch 1210, Loss: 2.2664423882961273, Final Batch Loss: 0.47448354959487915\n",
      "Epoch 1211, Loss: 2.336294412612915, Final Batch Loss: 0.5490649342536926\n",
      "Epoch 1212, Loss: 2.3728508055210114, Final Batch Loss: 0.5672314763069153\n",
      "Epoch 1213, Loss: 2.309792995452881, Final Batch Loss: 0.4012666940689087\n",
      "Epoch 1214, Loss: 2.425520896911621, Final Batch Loss: 0.4241260290145874\n",
      "Epoch 1215, Loss: 2.297234356403351, Final Batch Loss: 0.5110894441604614\n",
      "Epoch 1216, Loss: 2.4049141108989716, Final Batch Loss: 0.48280447721481323\n",
      "Epoch 1217, Loss: 2.392011821269989, Final Batch Loss: 0.5595404505729675\n",
      "Epoch 1218, Loss: 2.1169732809066772, Final Batch Loss: 0.3143407702445984\n",
      "Epoch 1219, Loss: 2.398145079612732, Final Batch Loss: 0.5307578444480896\n",
      "Epoch 1220, Loss: 2.168474316596985, Final Batch Loss: 0.30326539278030396\n",
      "Epoch 1221, Loss: 2.322543978691101, Final Batch Loss: 0.46093639731407166\n",
      "Epoch 1222, Loss: 2.268318146467209, Final Batch Loss: 0.4014563262462616\n",
      "Epoch 1223, Loss: 2.4541285037994385, Final Batch Loss: 0.5256220102310181\n",
      "Epoch 1224, Loss: 2.432409346103668, Final Batch Loss: 0.6560458540916443\n",
      "Epoch 1225, Loss: 2.3197073936462402, Final Batch Loss: 0.42608335614204407\n",
      "Epoch 1226, Loss: 2.5135840475559235, Final Batch Loss: 0.6294327974319458\n",
      "Epoch 1227, Loss: 2.302421450614929, Final Batch Loss: 0.4836134612560272\n",
      "Epoch 1228, Loss: 2.2290918827056885, Final Batch Loss: 0.39215490221977234\n",
      "Epoch 1229, Loss: 2.1905958354473114, Final Batch Loss: 0.3751063048839569\n",
      "Epoch 1230, Loss: 2.2841685116291046, Final Batch Loss: 0.43479326367378235\n",
      "Epoch 1231, Loss: 2.4774536192417145, Final Batch Loss: 0.5807538628578186\n",
      "Epoch 1232, Loss: 2.40171280503273, Final Batch Loss: 0.4617818295955658\n",
      "Epoch 1233, Loss: 2.3713001012802124, Final Batch Loss: 0.5702674388885498\n",
      "Epoch 1234, Loss: 2.158112019300461, Final Batch Loss: 0.2887352705001831\n",
      "Epoch 1235, Loss: 2.441532075405121, Final Batch Loss: 0.6436652541160583\n",
      "Epoch 1236, Loss: 2.346422404050827, Final Batch Loss: 0.4227077066898346\n",
      "Epoch 1237, Loss: 2.379608154296875, Final Batch Loss: 0.6665568947792053\n",
      "Epoch 1238, Loss: 2.1692197620868683, Final Batch Loss: 0.4088060259819031\n",
      "Epoch 1239, Loss: 2.328801214694977, Final Batch Loss: 0.4385744035243988\n",
      "Epoch 1240, Loss: 2.2278904616832733, Final Batch Loss: 0.6068874597549438\n",
      "Epoch 1241, Loss: 2.2223966121673584, Final Batch Loss: 0.42849522829055786\n",
      "Epoch 1242, Loss: 2.3058047592639923, Final Batch Loss: 0.548462450504303\n",
      "Epoch 1243, Loss: 2.436685562133789, Final Batch Loss: 0.5146228671073914\n",
      "Epoch 1244, Loss: 2.2498367726802826, Final Batch Loss: 0.3603540062904358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1245, Loss: 2.3969823122024536, Final Batch Loss: 0.5279625058174133\n",
      "Epoch 1246, Loss: 2.4180687367916107, Final Batch Loss: 0.5605255961418152\n",
      "Epoch 1247, Loss: 2.215723752975464, Final Batch Loss: 0.4400548040866852\n",
      "Epoch 1248, Loss: 2.454011559486389, Final Batch Loss: 0.6413213014602661\n",
      "Epoch 1249, Loss: 2.504887819290161, Final Batch Loss: 0.6023380756378174\n",
      "Epoch 1250, Loss: 2.365342289209366, Final Batch Loss: 0.48002690076828003\n",
      "Epoch 1251, Loss: 2.3839654624462128, Final Batch Loss: 0.5595065951347351\n",
      "Epoch 1252, Loss: 2.4491769671440125, Final Batch Loss: 0.4863623082637787\n",
      "Epoch 1253, Loss: 2.3404687643051147, Final Batch Loss: 0.4309168756008148\n",
      "Epoch 1254, Loss: 2.28472837805748, Final Batch Loss: 0.43551596999168396\n",
      "Epoch 1255, Loss: 2.270272374153137, Final Batch Loss: 0.4837350845336914\n",
      "Epoch 1256, Loss: 2.3749156296253204, Final Batch Loss: 0.4270676076412201\n",
      "Epoch 1257, Loss: 2.0839655995368958, Final Batch Loss: 0.41001397371292114\n",
      "Epoch 1258, Loss: 2.2396582067012787, Final Batch Loss: 0.42053544521331787\n",
      "Epoch 1259, Loss: 2.1755638122558594, Final Batch Loss: 0.36454713344573975\n",
      "Epoch 1260, Loss: 2.3577024936676025, Final Batch Loss: 0.5199114084243774\n",
      "Epoch 1261, Loss: 2.1937255561351776, Final Batch Loss: 0.43992990255355835\n",
      "Epoch 1262, Loss: 2.385200619697571, Final Batch Loss: 0.4699089825153351\n",
      "Epoch 1263, Loss: 2.4440404772758484, Final Batch Loss: 0.4360138475894928\n",
      "Epoch 1264, Loss: 2.2451135218143463, Final Batch Loss: 0.4459989070892334\n",
      "Epoch 1265, Loss: 2.3780535757541656, Final Batch Loss: 0.5574245452880859\n",
      "Epoch 1266, Loss: 2.2248257100582123, Final Batch Loss: 0.3794277608394623\n",
      "Epoch 1267, Loss: 2.091969519853592, Final Batch Loss: 0.31988686323165894\n",
      "Epoch 1268, Loss: 2.31622776389122, Final Batch Loss: 0.4929998815059662\n",
      "Epoch 1269, Loss: 2.2346111536026, Final Batch Loss: 0.42705821990966797\n",
      "Epoch 1270, Loss: 2.2212949991226196, Final Batch Loss: 0.49223774671554565\n",
      "Epoch 1271, Loss: 2.233396887779236, Final Batch Loss: 0.4474777579307556\n",
      "Epoch 1272, Loss: 2.4889337718486786, Final Batch Loss: 0.6335398554801941\n",
      "Epoch 1273, Loss: 2.305958181619644, Final Batch Loss: 0.5394213795661926\n",
      "Epoch 1274, Loss: 2.175472527742386, Final Batch Loss: 0.4616824686527252\n",
      "Epoch 1275, Loss: 2.2665425837039948, Final Batch Loss: 0.4709254205226898\n",
      "Epoch 1276, Loss: 2.1984933614730835, Final Batch Loss: 0.3881548345088959\n",
      "Epoch 1277, Loss: 2.3851777613162994, Final Batch Loss: 0.5288431644439697\n",
      "Epoch 1278, Loss: 2.20027893781662, Final Batch Loss: 0.3566625118255615\n",
      "Epoch 1279, Loss: 2.269702523946762, Final Batch Loss: 0.43079063296318054\n",
      "Epoch 1280, Loss: 2.2671453654766083, Final Batch Loss: 0.35579776763916016\n",
      "Epoch 1281, Loss: 2.1560879349708557, Final Batch Loss: 0.3945798873901367\n",
      "Epoch 1282, Loss: 2.2504605650901794, Final Batch Loss: 0.4779120683670044\n",
      "Epoch 1283, Loss: 2.2833109498023987, Final Batch Loss: 0.41422268748283386\n",
      "Epoch 1284, Loss: 2.1733546257019043, Final Batch Loss: 0.34930798411369324\n",
      "Epoch 1285, Loss: 2.374166816473007, Final Batch Loss: 0.5383139848709106\n",
      "Epoch 1286, Loss: 2.164202332496643, Final Batch Loss: 0.3833504021167755\n",
      "Epoch 1287, Loss: 2.1396561563014984, Final Batch Loss: 0.4661546051502228\n",
      "Epoch 1288, Loss: 2.319071412086487, Final Batch Loss: 0.46260106563568115\n",
      "Epoch 1289, Loss: 2.4173896610736847, Final Batch Loss: 0.535707950592041\n",
      "Epoch 1290, Loss: 2.049651950597763, Final Batch Loss: 0.3245929479598999\n",
      "Epoch 1291, Loss: 2.088093489408493, Final Batch Loss: 0.3467475175857544\n",
      "Epoch 1292, Loss: 2.4098605513572693, Final Batch Loss: 0.5872291326522827\n",
      "Epoch 1293, Loss: 2.2500208020210266, Final Batch Loss: 0.4104384779930115\n",
      "Epoch 1294, Loss: 2.267780065536499, Final Batch Loss: 0.4523318409919739\n",
      "Epoch 1295, Loss: 2.326159715652466, Final Batch Loss: 0.47682997584342957\n",
      "Epoch 1296, Loss: 2.306126594543457, Final Batch Loss: 0.46428659558296204\n",
      "Epoch 1297, Loss: 2.321149170398712, Final Batch Loss: 0.4342540502548218\n",
      "Epoch 1298, Loss: 2.268593817949295, Final Batch Loss: 0.4397406280040741\n",
      "Epoch 1299, Loss: 2.1770326793193817, Final Batch Loss: 0.30497896671295166\n",
      "Epoch 1300, Loss: 2.1556391417980194, Final Batch Loss: 0.28791502118110657\n",
      "Epoch 1301, Loss: 2.21741846203804, Final Batch Loss: 0.388471782207489\n",
      "Epoch 1302, Loss: 2.181487023830414, Final Batch Loss: 0.4145820736885071\n",
      "Epoch 1303, Loss: 2.2103165984153748, Final Batch Loss: 0.4174380600452423\n",
      "Epoch 1304, Loss: 2.426863372325897, Final Batch Loss: 0.4119718372821808\n",
      "Epoch 1305, Loss: 2.3119299709796906, Final Batch Loss: 0.4696856439113617\n",
      "Epoch 1306, Loss: 2.3595913648605347, Final Batch Loss: 0.46625810861587524\n",
      "Epoch 1307, Loss: 2.2512952983379364, Final Batch Loss: 0.44443973898887634\n",
      "Epoch 1308, Loss: 2.3401139080524445, Final Batch Loss: 0.5140088200569153\n",
      "Epoch 1309, Loss: 2.101513981819153, Final Batch Loss: 0.34418541193008423\n",
      "Epoch 1310, Loss: 2.3098959922790527, Final Batch Loss: 0.5286563634872437\n",
      "Epoch 1311, Loss: 2.1089543998241425, Final Batch Loss: 0.3961714804172516\n",
      "Epoch 1312, Loss: 2.414029151201248, Final Batch Loss: 0.6930491924285889\n",
      "Epoch 1313, Loss: 2.184032678604126, Final Batch Loss: 0.4876643121242523\n",
      "Epoch 1314, Loss: 2.239790201187134, Final Batch Loss: 0.4866100549697876\n",
      "Epoch 1315, Loss: 2.066341817378998, Final Batch Loss: 0.25980886816978455\n",
      "Epoch 1316, Loss: 2.258814126253128, Final Batch Loss: 0.5\n",
      "Epoch 1317, Loss: 2.270974278450012, Final Batch Loss: 0.5149793028831482\n",
      "Epoch 1318, Loss: 2.2881029844284058, Final Batch Loss: 0.4030565619468689\n",
      "Epoch 1319, Loss: 2.2359100580215454, Final Batch Loss: 0.4835284948348999\n",
      "Epoch 1320, Loss: 2.385757476091385, Final Batch Loss: 0.6370218396186829\n",
      "Epoch 1321, Loss: 2.2424581348896027, Final Batch Loss: 0.36720913648605347\n",
      "Epoch 1322, Loss: 2.238770604133606, Final Batch Loss: 0.49889716506004333\n",
      "Epoch 1323, Loss: 2.2433232069015503, Final Batch Loss: 0.45132559537887573\n",
      "Epoch 1324, Loss: 2.268932968378067, Final Batch Loss: 0.4048972725868225\n",
      "Epoch 1325, Loss: 2.1177127361297607, Final Batch Loss: 0.40215781331062317\n",
      "Epoch 1326, Loss: 2.228455603122711, Final Batch Loss: 0.5022005438804626\n",
      "Epoch 1327, Loss: 2.3416851460933685, Final Batch Loss: 0.4729454815387726\n",
      "Epoch 1328, Loss: 2.3054919838905334, Final Batch Loss: 0.38374578952789307\n",
      "Epoch 1329, Loss: 2.3505493104457855, Final Batch Loss: 0.5436655282974243\n",
      "Epoch 1330, Loss: 2.1399232149124146, Final Batch Loss: 0.32408809661865234\n",
      "Epoch 1331, Loss: 2.2352751195430756, Final Batch Loss: 0.4158780574798584\n",
      "Epoch 1332, Loss: 2.2916294038295746, Final Batch Loss: 0.5084628462791443\n",
      "Epoch 1333, Loss: 2.1321906447410583, Final Batch Loss: 0.332156777381897\n",
      "Epoch 1334, Loss: 2.17497855424881, Final Batch Loss: 0.332490473985672\n",
      "Epoch 1335, Loss: 2.3123925924301147, Final Batch Loss: 0.44169625639915466\n",
      "Epoch 1336, Loss: 2.133782386779785, Final Batch Loss: 0.4166406989097595\n",
      "Epoch 1337, Loss: 2.2442641258239746, Final Batch Loss: 0.46013784408569336\n",
      "Epoch 1338, Loss: 2.4032865464687347, Final Batch Loss: 0.5991830229759216\n",
      "Epoch 1339, Loss: 2.1561392843723297, Final Batch Loss: 0.42390555143356323\n",
      "Epoch 1340, Loss: 2.161352038383484, Final Batch Loss: 0.36324235796928406\n",
      "Epoch 1341, Loss: 2.257467418909073, Final Batch Loss: 0.4181010127067566\n",
      "Epoch 1342, Loss: 2.3323897421360016, Final Batch Loss: 0.5032975077629089\n",
      "Epoch 1343, Loss: 2.1990720629692078, Final Batch Loss: 0.28275057673454285\n",
      "Epoch 1344, Loss: 2.4410119354724884, Final Batch Loss: 0.5444059371948242\n",
      "Epoch 1345, Loss: 2.332868903875351, Final Batch Loss: 0.46547451615333557\n",
      "Epoch 1346, Loss: 2.2915346920490265, Final Batch Loss: 0.5206257104873657\n",
      "Epoch 1347, Loss: 2.362486481666565, Final Batch Loss: 0.5198206901550293\n",
      "Epoch 1348, Loss: 2.313177853822708, Final Batch Loss: 0.4716917872428894\n",
      "Epoch 1349, Loss: 2.1613191664218903, Final Batch Loss: 0.29154399037361145\n",
      "Epoch 1350, Loss: 2.3354444801807404, Final Batch Loss: 0.4045252501964569\n",
      "Epoch 1351, Loss: 2.2538166642189026, Final Batch Loss: 0.4746021032333374\n",
      "Epoch 1352, Loss: 2.4735957980155945, Final Batch Loss: 0.5903236269950867\n",
      "Epoch 1353, Loss: 2.136308193206787, Final Batch Loss: 0.30172187089920044\n",
      "Epoch 1354, Loss: 2.2970293164253235, Final Batch Loss: 0.44231951236724854\n",
      "Epoch 1355, Loss: 2.2587040066719055, Final Batch Loss: 0.3794368803501129\n",
      "Epoch 1356, Loss: 2.2355335354804993, Final Batch Loss: 0.5219591856002808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1357, Loss: 2.284268409013748, Final Batch Loss: 0.4432678818702698\n",
      "Epoch 1358, Loss: 2.2514761686325073, Final Batch Loss: 0.4924652874469757\n",
      "Epoch 1359, Loss: 2.337133139371872, Final Batch Loss: 0.4621298015117645\n",
      "Epoch 1360, Loss: 2.249486982822418, Final Batch Loss: 0.49547532200813293\n",
      "Epoch 1361, Loss: 2.142134577035904, Final Batch Loss: 0.393332302570343\n",
      "Epoch 1362, Loss: 2.096738964319229, Final Batch Loss: 0.3917889893054962\n",
      "Epoch 1363, Loss: 2.150630384683609, Final Batch Loss: 0.37954655289649963\n",
      "Epoch 1364, Loss: 2.226443886756897, Final Batch Loss: 0.4786577522754669\n",
      "Epoch 1365, Loss: 2.255482703447342, Final Batch Loss: 0.4860598146915436\n",
      "Epoch 1366, Loss: 2.3836600184440613, Final Batch Loss: 0.5173226594924927\n",
      "Epoch 1367, Loss: 2.2298634946346283, Final Batch Loss: 0.4729648530483246\n",
      "Epoch 1368, Loss: 2.202068120241165, Final Batch Loss: 0.42405498027801514\n",
      "Epoch 1369, Loss: 2.3031578361988068, Final Batch Loss: 0.5398533344268799\n",
      "Epoch 1370, Loss: 2.1531988978385925, Final Batch Loss: 0.42259740829467773\n",
      "Epoch 1371, Loss: 2.0840052366256714, Final Batch Loss: 0.2742357552051544\n",
      "Epoch 1372, Loss: 2.107000946998596, Final Batch Loss: 0.3508417010307312\n",
      "Epoch 1373, Loss: 2.21925088763237, Final Batch Loss: 0.40360647439956665\n",
      "Epoch 1374, Loss: 2.2081552147865295, Final Batch Loss: 0.4847545325756073\n",
      "Epoch 1375, Loss: 2.0768232941627502, Final Batch Loss: 0.37842097878456116\n",
      "Epoch 1376, Loss: 2.405940681695938, Final Batch Loss: 0.6336193084716797\n",
      "Epoch 1377, Loss: 2.141905039548874, Final Batch Loss: 0.4489760994911194\n",
      "Epoch 1378, Loss: 2.2510805428028107, Final Batch Loss: 0.4107820689678192\n",
      "Epoch 1379, Loss: 2.030229687690735, Final Batch Loss: 0.2905524969100952\n",
      "Epoch 1380, Loss: 2.2728488743305206, Final Batch Loss: 0.37546831369400024\n",
      "Epoch 1381, Loss: 2.339652270078659, Final Batch Loss: 0.595755398273468\n",
      "Epoch 1382, Loss: 2.259111613035202, Final Batch Loss: 0.3744979500770569\n",
      "Epoch 1383, Loss: 2.305118292570114, Final Batch Loss: 0.4971076548099518\n",
      "Epoch 1384, Loss: 2.0304808914661407, Final Batch Loss: 0.31177642941474915\n",
      "Epoch 1385, Loss: 2.143746495246887, Final Batch Loss: 0.4076968729496002\n",
      "Epoch 1386, Loss: 2.0981829464435577, Final Batch Loss: 0.4044868052005768\n",
      "Epoch 1387, Loss: 2.202094942331314, Final Batch Loss: 0.45330989360809326\n",
      "Epoch 1388, Loss: 2.14212965965271, Final Batch Loss: 0.4539230763912201\n",
      "Epoch 1389, Loss: 2.235419422388077, Final Batch Loss: 0.3531332015991211\n",
      "Epoch 1390, Loss: 2.1718844771385193, Final Batch Loss: 0.4248029589653015\n",
      "Epoch 1391, Loss: 2.1781481504440308, Final Batch Loss: 0.530816912651062\n",
      "Epoch 1392, Loss: 2.2827832996845245, Final Batch Loss: 0.5576516389846802\n",
      "Epoch 1393, Loss: 2.0503661036491394, Final Batch Loss: 0.33424121141433716\n",
      "Epoch 1394, Loss: 2.228925734758377, Final Batch Loss: 0.547828733921051\n",
      "Epoch 1395, Loss: 2.346487581729889, Final Batch Loss: 0.5245136618614197\n",
      "Epoch 1396, Loss: 2.2622352838516235, Final Batch Loss: 0.5672191977500916\n",
      "Epoch 1397, Loss: 2.094413101673126, Final Batch Loss: 0.3335830569267273\n",
      "Epoch 1398, Loss: 2.269454687833786, Final Batch Loss: 0.43315237760543823\n",
      "Epoch 1399, Loss: 2.0290339291095734, Final Batch Loss: 0.31789568066596985\n",
      "Epoch 1400, Loss: 2.2446985840797424, Final Batch Loss: 0.42473161220550537\n",
      "Epoch 1401, Loss: 2.1991165578365326, Final Batch Loss: 0.4374881982803345\n",
      "Epoch 1402, Loss: 2.1899839639663696, Final Batch Loss: 0.29974624514579773\n",
      "Epoch 1403, Loss: 2.0970464646816254, Final Batch Loss: 0.35650768876075745\n",
      "Epoch 1404, Loss: 2.090417802333832, Final Batch Loss: 0.3487042486667633\n",
      "Epoch 1405, Loss: 2.273663818836212, Final Batch Loss: 0.5727105736732483\n",
      "Epoch 1406, Loss: 2.294681489467621, Final Batch Loss: 0.5951020121574402\n",
      "Epoch 1407, Loss: 2.392659366130829, Final Batch Loss: 0.516888439655304\n",
      "Epoch 1408, Loss: 2.4321203231811523, Final Batch Loss: 0.6132642030715942\n",
      "Epoch 1409, Loss: 2.349553197622299, Final Batch Loss: 0.5534564256668091\n",
      "Epoch 1410, Loss: 2.346531003713608, Final Batch Loss: 0.6028225421905518\n",
      "Epoch 1411, Loss: 2.0878741443157196, Final Batch Loss: 0.35242602229118347\n",
      "Epoch 1412, Loss: 2.2114442884922028, Final Batch Loss: 0.3965851962566376\n",
      "Epoch 1413, Loss: 2.4086694717407227, Final Batch Loss: 0.7068673372268677\n",
      "Epoch 1414, Loss: 2.2852462828159332, Final Batch Loss: 0.5904495716094971\n",
      "Epoch 1415, Loss: 2.259041905403137, Final Batch Loss: 0.38424891233444214\n",
      "Epoch 1416, Loss: 2.230204224586487, Final Batch Loss: 0.40261316299438477\n",
      "Epoch 1417, Loss: 2.045673042535782, Final Batch Loss: 0.43403705954551697\n",
      "Epoch 1418, Loss: 2.1808736324310303, Final Batch Loss: 0.34690380096435547\n",
      "Epoch 1419, Loss: 2.1688462495803833, Final Batch Loss: 0.3656620979309082\n",
      "Epoch 1420, Loss: 2.160985976457596, Final Batch Loss: 0.47864818572998047\n",
      "Epoch 1421, Loss: 2.1332660019397736, Final Batch Loss: 0.3816341757774353\n",
      "Epoch 1422, Loss: 2.4799299836158752, Final Batch Loss: 0.7487526535987854\n",
      "Epoch 1423, Loss: 2.270837366580963, Final Batch Loss: 0.5059455633163452\n",
      "Epoch 1424, Loss: 2.305630713701248, Final Batch Loss: 0.4678662419319153\n",
      "Epoch 1425, Loss: 2.1539854407310486, Final Batch Loss: 0.4117819666862488\n",
      "Epoch 1426, Loss: 2.3008833825588226, Final Batch Loss: 0.28816771507263184\n",
      "Epoch 1427, Loss: 2.140167713165283, Final Batch Loss: 0.43752917647361755\n",
      "Epoch 1428, Loss: 2.11476269364357, Final Batch Loss: 0.4050235450267792\n",
      "Epoch 1429, Loss: 2.0102904736995697, Final Batch Loss: 0.27346324920654297\n",
      "Epoch 1430, Loss: 2.1502130925655365, Final Batch Loss: 0.3771398365497589\n",
      "Epoch 1431, Loss: 2.2633704245090485, Final Batch Loss: 0.5230726003646851\n",
      "Epoch 1432, Loss: 2.243086874485016, Final Batch Loss: 0.4915790855884552\n",
      "Epoch 1433, Loss: 2.0645988285541534, Final Batch Loss: 0.38864749670028687\n",
      "Epoch 1434, Loss: 2.204909175634384, Final Batch Loss: 0.37434300780296326\n",
      "Epoch 1435, Loss: 2.0835486948490143, Final Batch Loss: 0.46642106771469116\n",
      "Epoch 1436, Loss: 2.165637046098709, Final Batch Loss: 0.32909679412841797\n",
      "Epoch 1437, Loss: 2.177402973175049, Final Batch Loss: 0.4949193596839905\n",
      "Epoch 1438, Loss: 2.287994295358658, Final Batch Loss: 0.46869802474975586\n",
      "Epoch 1439, Loss: 2.186115801334381, Final Batch Loss: 0.38901281356811523\n",
      "Epoch 1440, Loss: 2.105680823326111, Final Batch Loss: 0.3244645595550537\n",
      "Epoch 1441, Loss: 2.211825907230377, Final Batch Loss: 0.3803481161594391\n",
      "Epoch 1442, Loss: 2.1610641181468964, Final Batch Loss: 0.568492591381073\n",
      "Epoch 1443, Loss: 2.337887018918991, Final Batch Loss: 0.5288095474243164\n",
      "Epoch 1444, Loss: 2.177323192358017, Final Batch Loss: 0.45457571744918823\n",
      "Epoch 1445, Loss: 2.22705078125, Final Batch Loss: 0.4108181297779083\n",
      "Epoch 1446, Loss: 2.1341762244701385, Final Batch Loss: 0.4153301417827606\n",
      "Epoch 1447, Loss: 2.214496225118637, Final Batch Loss: 0.3959028124809265\n",
      "Epoch 1448, Loss: 2.239700049161911, Final Batch Loss: 0.4794788062572479\n",
      "Epoch 1449, Loss: 2.075999826192856, Final Batch Loss: 0.41057920455932617\n",
      "Epoch 1450, Loss: 2.3242338597774506, Final Batch Loss: 0.44853922724723816\n",
      "Epoch 1451, Loss: 2.165554106235504, Final Batch Loss: 0.4204375445842743\n",
      "Epoch 1452, Loss: 2.292923539876938, Final Batch Loss: 0.5939513444900513\n",
      "Epoch 1453, Loss: 2.3808366656303406, Final Batch Loss: 0.4294903874397278\n",
      "Epoch 1454, Loss: 2.185247927904129, Final Batch Loss: 0.526424765586853\n",
      "Epoch 1455, Loss: 2.213724821805954, Final Batch Loss: 0.3682823181152344\n",
      "Epoch 1456, Loss: 2.1184507310390472, Final Batch Loss: 0.4178810715675354\n",
      "Epoch 1457, Loss: 2.167872279882431, Final Batch Loss: 0.45039039850234985\n",
      "Epoch 1458, Loss: 2.21821528673172, Final Batch Loss: 0.4658890664577484\n",
      "Epoch 1459, Loss: 2.0104983150959015, Final Batch Loss: 0.37868592143058777\n",
      "Epoch 1460, Loss: 2.2811178267002106, Final Batch Loss: 0.5112610459327698\n",
      "Epoch 1461, Loss: 2.3072943687438965, Final Batch Loss: 0.7037371397018433\n",
      "Epoch 1462, Loss: 2.100544512271881, Final Batch Loss: 0.4051530659198761\n",
      "Epoch 1463, Loss: 2.200789600610733, Final Batch Loss: 0.3951396346092224\n",
      "Epoch 1464, Loss: 2.2186278700828552, Final Batch Loss: 0.4397764205932617\n",
      "Epoch 1465, Loss: 2.228109657764435, Final Batch Loss: 0.4978872537612915\n",
      "Epoch 1466, Loss: 2.2013979852199554, Final Batch Loss: 0.4687803089618683\n",
      "Epoch 1467, Loss: 2.150617241859436, Final Batch Loss: 0.5142392516136169\n",
      "Epoch 1468, Loss: 2.31654754281044, Final Batch Loss: 0.5432494282722473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1469, Loss: 2.1954312324523926, Final Batch Loss: 0.4214397370815277\n",
      "Epoch 1470, Loss: 2.083948463201523, Final Batch Loss: 0.41231027245521545\n",
      "Epoch 1471, Loss: 2.241838425397873, Final Batch Loss: 0.34585899114608765\n",
      "Epoch 1472, Loss: 2.173293501138687, Final Batch Loss: 0.4131867289543152\n",
      "Epoch 1473, Loss: 2.162645787000656, Final Batch Loss: 0.46364840865135193\n",
      "Epoch 1474, Loss: 2.205258697271347, Final Batch Loss: 0.3921031951904297\n",
      "Epoch 1475, Loss: 2.2963790893554688, Final Batch Loss: 0.4708747863769531\n",
      "Epoch 1476, Loss: 2.192413091659546, Final Batch Loss: 0.38989657163619995\n",
      "Epoch 1477, Loss: 2.1023767292499542, Final Batch Loss: 0.425636887550354\n",
      "Epoch 1478, Loss: 2.3120513558387756, Final Batch Loss: 0.5079476237297058\n",
      "Epoch 1479, Loss: 2.2083411514759064, Final Batch Loss: 0.40585818886756897\n",
      "Epoch 1480, Loss: 2.0722848176956177, Final Batch Loss: 0.33915975689888\n",
      "Epoch 1481, Loss: 2.142309248447418, Final Batch Loss: 0.30713310837745667\n",
      "Epoch 1482, Loss: 2.066695749759674, Final Batch Loss: 0.3978619873523712\n",
      "Epoch 1483, Loss: 2.161532759666443, Final Batch Loss: 0.3937024772167206\n",
      "Epoch 1484, Loss: 1.9962284564971924, Final Batch Loss: 0.4421764612197876\n",
      "Epoch 1485, Loss: 2.2410952746868134, Final Batch Loss: 0.4034406542778015\n",
      "Epoch 1486, Loss: 2.2722148001194, Final Batch Loss: 0.5579308271408081\n",
      "Epoch 1487, Loss: 2.134251832962036, Final Batch Loss: 0.4317547082901001\n",
      "Epoch 1488, Loss: 2.132863402366638, Final Batch Loss: 0.4979068338871002\n",
      "Epoch 1489, Loss: 2.2109161615371704, Final Batch Loss: 0.4523336589336395\n",
      "Epoch 1490, Loss: 2.436641275882721, Final Batch Loss: 0.6663538217544556\n",
      "Epoch 1491, Loss: 2.212233245372772, Final Batch Loss: 0.43722906708717346\n",
      "Epoch 1492, Loss: 2.209570735692978, Final Batch Loss: 0.34935781359672546\n",
      "Epoch 1493, Loss: 2.205889731645584, Final Batch Loss: 0.43134579062461853\n",
      "Epoch 1494, Loss: 2.3001554906368256, Final Batch Loss: 0.5671347379684448\n",
      "Epoch 1495, Loss: 2.229430764913559, Final Batch Loss: 0.5329815149307251\n",
      "Epoch 1496, Loss: 2.1479753851890564, Final Batch Loss: 0.3310670852661133\n",
      "Epoch 1497, Loss: 2.1757164001464844, Final Batch Loss: 0.46581926941871643\n",
      "Epoch 1498, Loss: 2.298807919025421, Final Batch Loss: 0.6335117816925049\n",
      "Epoch 1499, Loss: 2.1396108269691467, Final Batch Loss: 0.4284629225730896\n",
      "Epoch 1500, Loss: 2.2621498703956604, Final Batch Loss: 0.4232454299926758\n",
      "Epoch 1501, Loss: 2.169055938720703, Final Batch Loss: 0.5529146194458008\n",
      "Epoch 1502, Loss: 2.125896006822586, Final Batch Loss: 0.4535894989967346\n",
      "Epoch 1503, Loss: 2.1023842990398407, Final Batch Loss: 0.4435425400733948\n",
      "Epoch 1504, Loss: 2.195035070180893, Final Batch Loss: 0.37558209896087646\n",
      "Epoch 1505, Loss: 2.3785633742809296, Final Batch Loss: 0.5937703847885132\n",
      "Epoch 1506, Loss: 2.1016383171081543, Final Batch Loss: 0.3103371560573578\n",
      "Epoch 1507, Loss: 2.3105995655059814, Final Batch Loss: 0.4816437363624573\n",
      "Epoch 1508, Loss: 2.1378505527973175, Final Batch Loss: 0.35454198718070984\n",
      "Epoch 1509, Loss: 2.0592576265335083, Final Batch Loss: 0.2678201198577881\n",
      "Epoch 1510, Loss: 2.1918859481811523, Final Batch Loss: 0.58134925365448\n",
      "Epoch 1511, Loss: 2.2516898214817047, Final Batch Loss: 0.459661066532135\n",
      "Epoch 1512, Loss: 2.0554595291614532, Final Batch Loss: 0.4357679784297943\n",
      "Epoch 1513, Loss: 2.166663557291031, Final Batch Loss: 0.49063724279403687\n",
      "Epoch 1514, Loss: 2.041341096162796, Final Batch Loss: 0.2640805244445801\n",
      "Epoch 1515, Loss: 2.1355504393577576, Final Batch Loss: 0.4051351845264435\n",
      "Epoch 1516, Loss: 2.0903522968292236, Final Batch Loss: 0.3965824544429779\n",
      "Epoch 1517, Loss: 2.2651668787002563, Final Batch Loss: 0.5020407438278198\n",
      "Epoch 1518, Loss: 2.200202226638794, Final Batch Loss: 0.5575258731842041\n",
      "Epoch 1519, Loss: 2.1466422080993652, Final Batch Loss: 0.41389942169189453\n",
      "Epoch 1520, Loss: 2.078083962202072, Final Batch Loss: 0.335406094789505\n",
      "Epoch 1521, Loss: 2.27100932598114, Final Batch Loss: 0.37048014998435974\n",
      "Epoch 1522, Loss: 2.0830588936805725, Final Batch Loss: 0.42686042189598083\n",
      "Epoch 1523, Loss: 2.2230831682682037, Final Batch Loss: 0.5027942657470703\n",
      "Epoch 1524, Loss: 2.4439577162265778, Final Batch Loss: 0.6496477127075195\n",
      "Epoch 1525, Loss: 2.1341190934181213, Final Batch Loss: 0.519223690032959\n",
      "Epoch 1526, Loss: 2.0652056336402893, Final Batch Loss: 0.36996176838874817\n",
      "Epoch 1527, Loss: 2.1749642491340637, Final Batch Loss: 0.4362732172012329\n",
      "Epoch 1528, Loss: 2.349538654088974, Final Batch Loss: 0.3619499206542969\n",
      "Epoch 1529, Loss: 2.2283980548381805, Final Batch Loss: 0.3996329605579376\n",
      "Epoch 1530, Loss: 2.0789784491062164, Final Batch Loss: 0.43708136677742004\n",
      "Epoch 1531, Loss: 2.1568329334259033, Final Batch Loss: 0.45221224427223206\n",
      "Epoch 1532, Loss: 2.2094222605228424, Final Batch Loss: 0.4532727897167206\n",
      "Epoch 1533, Loss: 2.0270666778087616, Final Batch Loss: 0.43372851610183716\n",
      "Epoch 1534, Loss: 2.317934423685074, Final Batch Loss: 0.44901806116104126\n",
      "Epoch 1535, Loss: 2.0542294085025787, Final Batch Loss: 0.3600006699562073\n",
      "Epoch 1536, Loss: 2.1311889588832855, Final Batch Loss: 0.36500051617622375\n",
      "Epoch 1537, Loss: 2.093072921037674, Final Batch Loss: 0.38615021109580994\n",
      "Epoch 1538, Loss: 2.2175776660442352, Final Batch Loss: 0.4821116030216217\n",
      "Epoch 1539, Loss: 2.217147797346115, Final Batch Loss: 0.5262094736099243\n",
      "Epoch 1540, Loss: 2.138458341360092, Final Batch Loss: 0.4574849605560303\n",
      "Epoch 1541, Loss: 2.06432843208313, Final Batch Loss: 0.4681793749332428\n",
      "Epoch 1542, Loss: 2.1889540553092957, Final Batch Loss: 0.43006274104118347\n",
      "Epoch 1543, Loss: 2.135852873325348, Final Batch Loss: 0.4415521025657654\n",
      "Epoch 1544, Loss: 2.094569057226181, Final Batch Loss: 0.3837088942527771\n",
      "Epoch 1545, Loss: 2.0603231489658356, Final Batch Loss: 0.47089841961860657\n",
      "Epoch 1546, Loss: 2.1388843953609467, Final Batch Loss: 0.41456133127212524\n",
      "Epoch 1547, Loss: 2.194903165102005, Final Batch Loss: 0.43643397092819214\n",
      "Epoch 1548, Loss: 2.2736780643463135, Final Batch Loss: 0.586138904094696\n",
      "Epoch 1549, Loss: 2.290933668613434, Final Batch Loss: 0.4217450022697449\n",
      "Epoch 1550, Loss: 2.0227679014205933, Final Batch Loss: 0.3430403470993042\n",
      "Epoch 1551, Loss: 2.220658391714096, Final Batch Loss: 0.5280284881591797\n",
      "Epoch 1552, Loss: 2.183494448661804, Final Batch Loss: 0.4700430929660797\n",
      "Epoch 1553, Loss: 2.1142984330654144, Final Batch Loss: 0.4658622741699219\n",
      "Epoch 1554, Loss: 2.209541529417038, Final Batch Loss: 0.44434505701065063\n",
      "Epoch 1555, Loss: 1.8282750993967056, Final Batch Loss: 0.22015739977359772\n",
      "Epoch 1556, Loss: 2.122854083776474, Final Batch Loss: 0.38425424695014954\n",
      "Epoch 1557, Loss: 2.274030417203903, Final Batch Loss: 0.417519211769104\n",
      "Epoch 1558, Loss: 2.0665716230869293, Final Batch Loss: 0.3873937726020813\n",
      "Epoch 1559, Loss: 2.1418519020080566, Final Batch Loss: 0.44134852290153503\n",
      "Epoch 1560, Loss: 2.079686760902405, Final Batch Loss: 0.385852187871933\n",
      "Epoch 1561, Loss: 2.29744353890419, Final Batch Loss: 0.5280120372772217\n",
      "Epoch 1562, Loss: 2.14878848195076, Final Batch Loss: 0.36608579754829407\n",
      "Epoch 1563, Loss: 2.016899824142456, Final Batch Loss: 0.3728426694869995\n",
      "Epoch 1564, Loss: 2.118575304746628, Final Batch Loss: 0.5126291513442993\n",
      "Epoch 1565, Loss: 2.10230815410614, Final Batch Loss: 0.3605251908302307\n",
      "Epoch 1566, Loss: 2.269721269607544, Final Batch Loss: 0.5406482815742493\n",
      "Epoch 1567, Loss: 2.106809377670288, Final Batch Loss: 0.4408451020717621\n",
      "Epoch 1568, Loss: 1.9891360998153687, Final Batch Loss: 0.3780394196510315\n",
      "Epoch 1569, Loss: 2.065624326467514, Final Batch Loss: 0.36204516887664795\n",
      "Epoch 1570, Loss: 2.3045797646045685, Final Batch Loss: 0.536186695098877\n",
      "Epoch 1571, Loss: 2.051777720451355, Final Batch Loss: 0.4157819449901581\n",
      "Epoch 1572, Loss: 2.014434039592743, Final Batch Loss: 0.4115843176841736\n",
      "Epoch 1573, Loss: 2.056788980960846, Final Batch Loss: 0.400870680809021\n",
      "Epoch 1574, Loss: 2.122982382774353, Final Batch Loss: 0.3913450837135315\n",
      "Epoch 1575, Loss: 1.9056052267551422, Final Batch Loss: 0.32341042160987854\n",
      "Epoch 1576, Loss: 2.0733193457126617, Final Batch Loss: 0.4170245826244354\n",
      "Epoch 1577, Loss: 2.21434423327446, Final Batch Loss: 0.26393264532089233\n",
      "Epoch 1578, Loss: 2.145192563533783, Final Batch Loss: 0.4511041045188904\n",
      "Epoch 1579, Loss: 2.01576691865921, Final Batch Loss: 0.3128097653388977\n",
      "Epoch 1580, Loss: 2.2430832386016846, Final Batch Loss: 0.4134575128555298\n",
      "Epoch 1581, Loss: 2.0112186074256897, Final Batch Loss: 0.4223611652851105\n",
      "Epoch 1582, Loss: 2.2448028922080994, Final Batch Loss: 0.5433059334754944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1583, Loss: 2.0646590888500214, Final Batch Loss: 0.40411487221717834\n",
      "Epoch 1584, Loss: 2.086068779230118, Final Batch Loss: 0.3912355303764343\n",
      "Epoch 1585, Loss: 2.1936884820461273, Final Batch Loss: 0.46994611620903015\n",
      "Epoch 1586, Loss: 2.107100486755371, Final Batch Loss: 0.4777682423591614\n",
      "Epoch 1587, Loss: 2.347424864768982, Final Batch Loss: 0.6233859658241272\n",
      "Epoch 1588, Loss: 1.9609239846467972, Final Batch Loss: 0.23630942404270172\n",
      "Epoch 1589, Loss: 2.2133156061172485, Final Batch Loss: 0.5174171328544617\n",
      "Epoch 1590, Loss: 2.0655042827129364, Final Batch Loss: 0.43764734268188477\n",
      "Epoch 1591, Loss: 2.0800004303455353, Final Batch Loss: 0.3899078369140625\n",
      "Epoch 1592, Loss: 2.0708045661449432, Final Batch Loss: 0.5009584426879883\n",
      "Epoch 1593, Loss: 2.337116688489914, Final Batch Loss: 0.5228016972541809\n",
      "Epoch 1594, Loss: 2.103514790534973, Final Batch Loss: 0.3859167695045471\n",
      "Epoch 1595, Loss: 2.030741572380066, Final Batch Loss: 0.4696974754333496\n",
      "Epoch 1596, Loss: 2.1263473331928253, Final Batch Loss: 0.39744481444358826\n",
      "Epoch 1597, Loss: 2.0683147609233856, Final Batch Loss: 0.413472980260849\n",
      "Epoch 1598, Loss: 2.0906718969345093, Final Batch Loss: 0.33476150035858154\n",
      "Epoch 1599, Loss: 2.1834458708763123, Final Batch Loss: 0.5155063271522522\n",
      "Epoch 1600, Loss: 2.382470339536667, Final Batch Loss: 0.47409138083457947\n",
      "Epoch 1601, Loss: 2.0808790624141693, Final Batch Loss: 0.3822382390499115\n",
      "Epoch 1602, Loss: 1.8683093786239624, Final Batch Loss: 0.2625441253185272\n",
      "Epoch 1603, Loss: 2.074433922767639, Final Batch Loss: 0.39782026410102844\n",
      "Epoch 1604, Loss: 2.2931815683841705, Final Batch Loss: 0.48165592551231384\n",
      "Epoch 1605, Loss: 2.071812093257904, Final Batch Loss: 0.44358083605766296\n",
      "Epoch 1606, Loss: 2.0835093557834625, Final Batch Loss: 0.5129040479660034\n",
      "Epoch 1607, Loss: 2.0548161268234253, Final Batch Loss: 0.36914005875587463\n",
      "Epoch 1608, Loss: 2.0845093429088593, Final Batch Loss: 0.44745776057243347\n",
      "Epoch 1609, Loss: 2.3948966562747955, Final Batch Loss: 0.5875104665756226\n",
      "Epoch 1610, Loss: 2.2880450189113617, Final Batch Loss: 0.3699725270271301\n",
      "Epoch 1611, Loss: 1.8654038459062576, Final Batch Loss: 0.24459432065486908\n",
      "Epoch 1612, Loss: 1.9885244965553284, Final Batch Loss: 0.28650379180908203\n",
      "Epoch 1613, Loss: 2.0188033878803253, Final Batch Loss: 0.4250602126121521\n",
      "Epoch 1614, Loss: 2.2031915485858917, Final Batch Loss: 0.5386151671409607\n",
      "Epoch 1615, Loss: 2.117192029953003, Final Batch Loss: 0.46867451071739197\n",
      "Epoch 1616, Loss: 2.1104968786239624, Final Batch Loss: 0.37180569767951965\n",
      "Epoch 1617, Loss: 2.0865367352962494, Final Batch Loss: 0.5414333343505859\n",
      "Epoch 1618, Loss: 2.0375208258628845, Final Batch Loss: 0.3536470830440521\n",
      "Epoch 1619, Loss: 2.016071379184723, Final Batch Loss: 0.4259265661239624\n",
      "Epoch 1620, Loss: 2.104043185710907, Final Batch Loss: 0.41715335845947266\n",
      "Epoch 1621, Loss: 1.9437095522880554, Final Batch Loss: 0.3167380094528198\n",
      "Epoch 1622, Loss: 1.973305195569992, Final Batch Loss: 0.2967638373374939\n",
      "Epoch 1623, Loss: 2.021643966436386, Final Batch Loss: 0.40588313341140747\n",
      "Epoch 1624, Loss: 2.1587439477443695, Final Batch Loss: 0.48431143164634705\n",
      "Epoch 1625, Loss: 2.087372601032257, Final Batch Loss: 0.5034297704696655\n",
      "Epoch 1626, Loss: 1.9643054604530334, Final Batch Loss: 0.32556086778640747\n",
      "Epoch 1627, Loss: 1.9884410798549652, Final Batch Loss: 0.35647013783454895\n",
      "Epoch 1628, Loss: 2.0031149983406067, Final Batch Loss: 0.4126438796520233\n",
      "Epoch 1629, Loss: 2.0942846536636353, Final Batch Loss: 0.3874015212059021\n",
      "Epoch 1630, Loss: 2.079476982355118, Final Batch Loss: 0.4882424473762512\n",
      "Epoch 1631, Loss: 2.0577462762594223, Final Batch Loss: 0.24450106918811798\n",
      "Epoch 1632, Loss: 2.226017504930496, Final Batch Loss: 0.4149993360042572\n",
      "Epoch 1633, Loss: 2.1264340579509735, Final Batch Loss: 0.45942607522010803\n",
      "Epoch 1634, Loss: 1.9848162829875946, Final Batch Loss: 0.3441409468650818\n",
      "Epoch 1635, Loss: 2.0376405119895935, Final Batch Loss: 0.3361205458641052\n",
      "Epoch 1636, Loss: 1.8600229322910309, Final Batch Loss: 0.3728078603744507\n",
      "Epoch 1637, Loss: 1.9490553736686707, Final Batch Loss: 0.36210519075393677\n",
      "Epoch 1638, Loss: 2.1220843493938446, Final Batch Loss: 0.36178240180015564\n",
      "Epoch 1639, Loss: 2.069887548685074, Final Batch Loss: 0.43245765566825867\n",
      "Epoch 1640, Loss: 1.9760399162769318, Final Batch Loss: 0.31505000591278076\n",
      "Epoch 1641, Loss: 2.0135550498962402, Final Batch Loss: 0.4172723889350891\n",
      "Epoch 1642, Loss: 2.012635976076126, Final Batch Loss: 0.40871119499206543\n",
      "Epoch 1643, Loss: 2.014447510242462, Final Batch Loss: 0.44699275493621826\n",
      "Epoch 1644, Loss: 2.229166328907013, Final Batch Loss: 0.441767156124115\n",
      "Epoch 1645, Loss: 2.1033665537834167, Final Batch Loss: 0.4124125838279724\n",
      "Epoch 1646, Loss: 2.2897198498249054, Final Batch Loss: 0.576256513595581\n",
      "Epoch 1647, Loss: 2.0039633214473724, Final Batch Loss: 0.3284240663051605\n",
      "Epoch 1648, Loss: 1.9619657695293427, Final Batch Loss: 0.3983730375766754\n",
      "Epoch 1649, Loss: 2.3432653546333313, Final Batch Loss: 0.48967427015304565\n",
      "Epoch 1650, Loss: 2.024275302886963, Final Batch Loss: 0.3035902678966522\n",
      "Epoch 1651, Loss: 2.1331804990768433, Final Batch Loss: 0.42698025703430176\n",
      "Epoch 1652, Loss: 2.1609040796756744, Final Batch Loss: 0.470531165599823\n",
      "Epoch 1653, Loss: 1.9790380895137787, Final Batch Loss: 0.34796443581581116\n",
      "Epoch 1654, Loss: 2.1413991451263428, Final Batch Loss: 0.48392096161842346\n",
      "Epoch 1655, Loss: 1.945523887872696, Final Batch Loss: 0.38235369324684143\n",
      "Epoch 1656, Loss: 2.030450850725174, Final Batch Loss: 0.3544817268848419\n",
      "Epoch 1657, Loss: 2.0438433289527893, Final Batch Loss: 0.3369309604167938\n",
      "Epoch 1658, Loss: 1.9804209172725677, Final Batch Loss: 0.33846405148506165\n",
      "Epoch 1659, Loss: 2.1777004301548004, Final Batch Loss: 0.4912208914756775\n",
      "Epoch 1660, Loss: 1.8975659608840942, Final Batch Loss: 0.38308781385421753\n",
      "Epoch 1661, Loss: 2.0513124465942383, Final Batch Loss: 0.27177244424819946\n",
      "Epoch 1662, Loss: 2.0314463675022125, Final Batch Loss: 0.3736623227596283\n",
      "Epoch 1663, Loss: 2.1411730647087097, Final Batch Loss: 0.5441494584083557\n",
      "Epoch 1664, Loss: 1.9677250683307648, Final Batch Loss: 0.29502472281455994\n",
      "Epoch 1665, Loss: 2.1185279488563538, Final Batch Loss: 0.4219547510147095\n",
      "Epoch 1666, Loss: 2.1036487221717834, Final Batch Loss: 0.3965953588485718\n",
      "Epoch 1667, Loss: 2.0755832195281982, Final Batch Loss: 0.33432361483573914\n",
      "Epoch 1668, Loss: 2.16988006234169, Final Batch Loss: 0.38344573974609375\n",
      "Epoch 1669, Loss: 2.007990598678589, Final Batch Loss: 0.31433165073394775\n",
      "Epoch 1670, Loss: 2.132902890443802, Final Batch Loss: 0.3635605573654175\n",
      "Epoch 1671, Loss: 2.183488667011261, Final Batch Loss: 0.5187616944313049\n",
      "Epoch 1672, Loss: 2.117346853017807, Final Batch Loss: 0.4991116225719452\n",
      "Epoch 1673, Loss: 1.9270947575569153, Final Batch Loss: 0.38415542244911194\n",
      "Epoch 1674, Loss: 2.0279465913772583, Final Batch Loss: 0.39140066504478455\n",
      "Epoch 1675, Loss: 1.9821030795574188, Final Batch Loss: 0.5396673679351807\n",
      "Epoch 1676, Loss: 2.1358067393302917, Final Batch Loss: 0.4304405450820923\n",
      "Epoch 1677, Loss: 1.9626417756080627, Final Batch Loss: 0.4111384153366089\n",
      "Epoch 1678, Loss: 2.114063113927841, Final Batch Loss: 0.5394418239593506\n",
      "Epoch 1679, Loss: 2.1683481633663177, Final Batch Loss: 0.45929548144340515\n",
      "Epoch 1680, Loss: 1.9014719724655151, Final Batch Loss: 0.30905628204345703\n",
      "Epoch 1681, Loss: 2.08804452419281, Final Batch Loss: 0.45516273379325867\n",
      "Epoch 1682, Loss: 2.050304263830185, Final Batch Loss: 0.4308626651763916\n",
      "Epoch 1683, Loss: 2.0090304613113403, Final Batch Loss: 0.3460986316204071\n",
      "Epoch 1684, Loss: 2.0605629682540894, Final Batch Loss: 0.38093265891075134\n",
      "Epoch 1685, Loss: 2.053723692893982, Final Batch Loss: 0.45611289143562317\n",
      "Epoch 1686, Loss: 2.0329037606716156, Final Batch Loss: 0.28054481744766235\n",
      "Epoch 1687, Loss: 2.126930683851242, Final Batch Loss: 0.42479056119918823\n",
      "Epoch 1688, Loss: 2.0976960957050323, Final Batch Loss: 0.4130535125732422\n",
      "Epoch 1689, Loss: 2.0388393104076385, Final Batch Loss: 0.35322490334510803\n",
      "Epoch 1690, Loss: 2.0710677206516266, Final Batch Loss: 0.3320850729942322\n",
      "Epoch 1691, Loss: 2.047388017177582, Final Batch Loss: 0.2667781710624695\n",
      "Epoch 1692, Loss: 2.1916487514972687, Final Batch Loss: 0.4361567199230194\n",
      "Epoch 1693, Loss: 2.106072098016739, Final Batch Loss: 0.47411295771598816\n",
      "Epoch 1694, Loss: 2.0215189158916473, Final Batch Loss: 0.5220787525177002\n",
      "Epoch 1695, Loss: 2.088042199611664, Final Batch Loss: 0.41969114542007446\n",
      "Epoch 1696, Loss: 1.972241222858429, Final Batch Loss: 0.43023502826690674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1697, Loss: 2.049939513206482, Final Batch Loss: 0.3327869772911072\n",
      "Epoch 1698, Loss: 2.216952830553055, Final Batch Loss: 0.5565667152404785\n",
      "Epoch 1699, Loss: 2.0083207488059998, Final Batch Loss: 0.337477445602417\n",
      "Epoch 1700, Loss: 2.018696665763855, Final Batch Loss: 0.32021552324295044\n",
      "Epoch 1701, Loss: 2.0636852085590363, Final Batch Loss: 0.38637593388557434\n",
      "Epoch 1702, Loss: 1.8714910745620728, Final Batch Loss: 0.29415494203567505\n",
      "Epoch 1703, Loss: 1.9972142577171326, Final Batch Loss: 0.34409597516059875\n",
      "Epoch 1704, Loss: 2.018144518136978, Final Batch Loss: 0.40400010347366333\n",
      "Epoch 1705, Loss: 2.216343879699707, Final Batch Loss: 0.5365492105484009\n",
      "Epoch 1706, Loss: 2.1411065459251404, Final Batch Loss: 0.48586708307266235\n",
      "Epoch 1707, Loss: 1.8764775097370148, Final Batch Loss: 0.2568628489971161\n",
      "Epoch 1708, Loss: 1.9122358858585358, Final Batch Loss: 0.38076159358024597\n",
      "Epoch 1709, Loss: 1.971142202615738, Final Batch Loss: 0.2459411323070526\n",
      "Epoch 1710, Loss: 1.9909111261367798, Final Batch Loss: 0.2708088159561157\n",
      "Epoch 1711, Loss: 1.954589307308197, Final Batch Loss: 0.22205621004104614\n",
      "Epoch 1712, Loss: 1.8353134095668793, Final Batch Loss: 0.30001986026763916\n",
      "Epoch 1713, Loss: 2.031102806329727, Final Batch Loss: 0.37119391560554504\n",
      "Epoch 1714, Loss: 1.9533594250679016, Final Batch Loss: 0.3318961262702942\n",
      "Epoch 1715, Loss: 2.0221044421195984, Final Batch Loss: 0.3953268826007843\n",
      "Epoch 1716, Loss: 1.9341582357883453, Final Batch Loss: 0.29718413949012756\n",
      "Epoch 1717, Loss: 2.049987703561783, Final Batch Loss: 0.37398815155029297\n",
      "Epoch 1718, Loss: 2.1085083186626434, Final Batch Loss: 0.46726587414741516\n",
      "Epoch 1719, Loss: 2.0299909114837646, Final Batch Loss: 0.3479965031147003\n",
      "Epoch 1720, Loss: 1.996143251657486, Final Batch Loss: 0.396690309047699\n",
      "Epoch 1721, Loss: 2.1162873208522797, Final Batch Loss: 0.3458205461502075\n",
      "Epoch 1722, Loss: 2.0459220707416534, Final Batch Loss: 0.25065600872039795\n",
      "Epoch 1723, Loss: 2.0011784434318542, Final Batch Loss: 0.41012847423553467\n",
      "Epoch 1724, Loss: 1.7830808460712433, Final Batch Loss: 0.29309356212615967\n",
      "Epoch 1725, Loss: 1.9458334743976593, Final Batch Loss: 0.4236774742603302\n",
      "Epoch 1726, Loss: 1.9287959784269333, Final Batch Loss: 0.22149713337421417\n",
      "Epoch 1727, Loss: 2.0100449323654175, Final Batch Loss: 0.35035502910614014\n",
      "Epoch 1728, Loss: 2.1061587929725647, Final Batch Loss: 0.38718992471694946\n",
      "Epoch 1729, Loss: 2.165517896413803, Final Batch Loss: 0.4914373457431793\n",
      "Epoch 1730, Loss: 1.976347178220749, Final Batch Loss: 0.34625333547592163\n",
      "Epoch 1731, Loss: 2.0709710717201233, Final Batch Loss: 0.357941597700119\n",
      "Epoch 1732, Loss: 2.047096073627472, Final Batch Loss: 0.4360196888446808\n",
      "Epoch 1733, Loss: 1.782629370689392, Final Batch Loss: 0.25178205966949463\n",
      "Epoch 1734, Loss: 2.113764375448227, Final Batch Loss: 0.46769794821739197\n",
      "Epoch 1735, Loss: 1.8945678174495697, Final Batch Loss: 0.2891734838485718\n",
      "Epoch 1736, Loss: 2.070000261068344, Final Batch Loss: 0.27311787009239197\n",
      "Epoch 1737, Loss: 2.0121144354343414, Final Batch Loss: 0.3241688311100006\n",
      "Epoch 1738, Loss: 1.857163429260254, Final Batch Loss: 0.29416623711586\n",
      "Epoch 1739, Loss: 2.0675265789031982, Final Batch Loss: 0.41978082060813904\n",
      "Epoch 1740, Loss: 2.0302399396896362, Final Batch Loss: 0.4317018985748291\n",
      "Epoch 1741, Loss: 2.0282177925109863, Final Batch Loss: 0.49594900012016296\n",
      "Epoch 1742, Loss: 2.194914013147354, Final Batch Loss: 0.46643438935279846\n",
      "Epoch 1743, Loss: 2.077163815498352, Final Batch Loss: 0.6629313826560974\n",
      "Epoch 1744, Loss: 2.1942162811756134, Final Batch Loss: 0.5291613340377808\n",
      "Epoch 1745, Loss: 2.1584014892578125, Final Batch Loss: 0.5311193466186523\n",
      "Epoch 1746, Loss: 2.252953886985779, Final Batch Loss: 0.5508735775947571\n",
      "Epoch 1747, Loss: 1.7908077538013458, Final Batch Loss: 0.32080671191215515\n",
      "Epoch 1748, Loss: 2.107267737388611, Final Batch Loss: 0.363386869430542\n",
      "Epoch 1749, Loss: 1.9511390626430511, Final Batch Loss: 0.3756300210952759\n",
      "Epoch 1750, Loss: 2.288942277431488, Final Batch Loss: 0.4466121196746826\n",
      "Epoch 1751, Loss: 1.8439190089702606, Final Batch Loss: 0.3172573149204254\n",
      "Epoch 1752, Loss: 1.9789966344833374, Final Batch Loss: 0.3541859984397888\n",
      "Epoch 1753, Loss: 1.9864075481891632, Final Batch Loss: 0.40444907546043396\n",
      "Epoch 1754, Loss: 1.935213714838028, Final Batch Loss: 0.3717648386955261\n",
      "Epoch 1755, Loss: 1.86586993932724, Final Batch Loss: 0.37690550088882446\n",
      "Epoch 1756, Loss: 1.917534053325653, Final Batch Loss: 0.4031764566898346\n",
      "Epoch 1757, Loss: 1.9690335094928741, Final Batch Loss: 0.40478017926216125\n",
      "Epoch 1758, Loss: 2.0732602775096893, Final Batch Loss: 0.5181175470352173\n",
      "Epoch 1759, Loss: 1.9566077291965485, Final Batch Loss: 0.3735843896865845\n",
      "Epoch 1760, Loss: 2.005863755941391, Final Batch Loss: 0.4182703197002411\n",
      "Epoch 1761, Loss: 1.950732797384262, Final Batch Loss: 0.4791266620159149\n",
      "Epoch 1762, Loss: 1.9476118981838226, Final Batch Loss: 0.31837284564971924\n",
      "Epoch 1763, Loss: 2.045742154121399, Final Batch Loss: 0.35901913046836853\n",
      "Epoch 1764, Loss: 2.5045248568058014, Final Batch Loss: 0.7776842713356018\n",
      "Epoch 1765, Loss: 1.9290140867233276, Final Batch Loss: 0.3318426311016083\n",
      "Epoch 1766, Loss: 2.002089649438858, Final Batch Loss: 0.3512601852416992\n",
      "Epoch 1767, Loss: 1.8466496467590332, Final Batch Loss: 0.3803642690181732\n",
      "Epoch 1768, Loss: 2.155311495065689, Final Batch Loss: 0.4965580403804779\n",
      "Epoch 1769, Loss: 1.9011706113815308, Final Batch Loss: 0.3247797191143036\n",
      "Epoch 1770, Loss: 1.9050937592983246, Final Batch Loss: 0.3879055976867676\n",
      "Epoch 1771, Loss: 1.9838557839393616, Final Batch Loss: 0.48179978132247925\n",
      "Epoch 1772, Loss: 2.0149386823177338, Final Batch Loss: 0.4271637499332428\n",
      "Epoch 1773, Loss: 2.202474921941757, Final Batch Loss: 0.5939502716064453\n",
      "Epoch 1774, Loss: 1.8271611630916595, Final Batch Loss: 0.21511197090148926\n",
      "Epoch 1775, Loss: 2.1730949878692627, Final Batch Loss: 0.5257662534713745\n",
      "Epoch 1776, Loss: 2.0565547049045563, Final Batch Loss: 0.29019469022750854\n",
      "Epoch 1777, Loss: 2.090931326150894, Final Batch Loss: 0.49086761474609375\n",
      "Epoch 1778, Loss: 2.1206333339214325, Final Batch Loss: 0.3591800928115845\n",
      "Epoch 1779, Loss: 2.0071070194244385, Final Batch Loss: 0.3696228265762329\n",
      "Epoch 1780, Loss: 2.0048618614673615, Final Batch Loss: 0.34940311312675476\n",
      "Epoch 1781, Loss: 2.1766446828842163, Final Batch Loss: 0.4456101059913635\n",
      "Epoch 1782, Loss: 1.9145779609680176, Final Batch Loss: 0.3223435878753662\n",
      "Epoch 1783, Loss: 2.1705983877182007, Final Batch Loss: 0.4861800968647003\n",
      "Epoch 1784, Loss: 1.944311112165451, Final Batch Loss: 0.4581514894962311\n",
      "Epoch 1785, Loss: 2.0256905257701874, Final Batch Loss: 0.5649080872535706\n",
      "Epoch 1786, Loss: 2.0173151791095734, Final Batch Loss: 0.4002549350261688\n",
      "Epoch 1787, Loss: 1.981746643781662, Final Batch Loss: 0.38457468152046204\n",
      "Epoch 1788, Loss: 2.0409417748451233, Final Batch Loss: 0.35457223653793335\n",
      "Epoch 1789, Loss: 2.2503758668899536, Final Batch Loss: 0.6148242354393005\n",
      "Epoch 1790, Loss: 2.1217653155326843, Final Batch Loss: 0.42944613099098206\n",
      "Epoch 1791, Loss: 2.207505524158478, Final Batch Loss: 0.515013575553894\n",
      "Epoch 1792, Loss: 2.0688676238059998, Final Batch Loss: 0.2870190441608429\n",
      "Epoch 1793, Loss: 2.0005990266799927, Final Batch Loss: 0.30194422602653503\n",
      "Epoch 1794, Loss: 1.888119786977768, Final Batch Loss: 0.33000311255455017\n",
      "Epoch 1795, Loss: 1.9884101152420044, Final Batch Loss: 0.35061177611351013\n",
      "Epoch 1796, Loss: 2.126650422811508, Final Batch Loss: 0.37728801369667053\n",
      "Epoch 1797, Loss: 2.050764352083206, Final Batch Loss: 0.35377755761146545\n",
      "Epoch 1798, Loss: 2.0991890728473663, Final Batch Loss: 0.315904825925827\n",
      "Epoch 1799, Loss: 1.993036836385727, Final Batch Loss: 0.3425193428993225\n",
      "Epoch 1800, Loss: 2.068216621875763, Final Batch Loss: 0.5119630694389343\n",
      "Epoch 1801, Loss: 2.06353497505188, Final Batch Loss: 0.5141371488571167\n",
      "Epoch 1802, Loss: 1.934664934873581, Final Batch Loss: 0.32452911138534546\n",
      "Epoch 1803, Loss: 2.0222962498664856, Final Batch Loss: 0.30169835686683655\n",
      "Epoch 1804, Loss: 2.018722355365753, Final Batch Loss: 0.3163696527481079\n",
      "Epoch 1805, Loss: 1.9026740193367004, Final Batch Loss: 0.34413012862205505\n",
      "Epoch 1806, Loss: 2.032155692577362, Final Batch Loss: 0.42701393365859985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1807, Loss: 1.9537212550640106, Final Batch Loss: 0.41736969351768494\n",
      "Epoch 1808, Loss: 2.0843357145786285, Final Batch Loss: 0.4851916432380676\n",
      "Epoch 1809, Loss: 1.9794543981552124, Final Batch Loss: 0.3160274028778076\n",
      "Epoch 1810, Loss: 2.0121317207813263, Final Batch Loss: 0.277026891708374\n",
      "Epoch 1811, Loss: 2.0483690798282623, Final Batch Loss: 0.4506232440471649\n",
      "Epoch 1812, Loss: 2.0971644818782806, Final Batch Loss: 0.41394349932670593\n",
      "Epoch 1813, Loss: 2.1396965086460114, Final Batch Loss: 0.5015258193016052\n",
      "Epoch 1814, Loss: 1.999626487493515, Final Batch Loss: 0.43989309668540955\n",
      "Epoch 1815, Loss: 2.1293914914131165, Final Batch Loss: 0.5580366849899292\n",
      "Epoch 1816, Loss: 2.0951634347438812, Final Batch Loss: 0.558824896812439\n",
      "Epoch 1817, Loss: 1.8995684087276459, Final Batch Loss: 0.3643891215324402\n",
      "Epoch 1818, Loss: 1.9676284193992615, Final Batch Loss: 0.309975266456604\n",
      "Epoch 1819, Loss: 2.0559676587581635, Final Batch Loss: 0.3747057318687439\n",
      "Epoch 1820, Loss: 1.9137451350688934, Final Batch Loss: 0.3806858956813812\n",
      "Epoch 1821, Loss: 1.9038054049015045, Final Batch Loss: 0.32782816886901855\n",
      "Epoch 1822, Loss: 1.9437523484230042, Final Batch Loss: 0.3549278974533081\n",
      "Epoch 1823, Loss: 2.1098223328590393, Final Batch Loss: 0.4749034643173218\n",
      "Epoch 1824, Loss: 2.0491040647029877, Final Batch Loss: 0.5041543245315552\n",
      "Epoch 1825, Loss: 1.8839802742004395, Final Batch Loss: 0.26368170976638794\n",
      "Epoch 1826, Loss: 1.8620328605175018, Final Batch Loss: 0.34268587827682495\n",
      "Epoch 1827, Loss: 2.0254863798618317, Final Batch Loss: 0.3938813805580139\n",
      "Epoch 1828, Loss: 2.3454772233963013, Final Batch Loss: 0.40343862771987915\n",
      "Epoch 1829, Loss: 2.017334073781967, Final Batch Loss: 0.5606886148452759\n",
      "Epoch 1830, Loss: 1.9703793227672577, Final Batch Loss: 0.320766806602478\n",
      "Epoch 1831, Loss: 1.9623201787471771, Final Batch Loss: 0.39519813656806946\n",
      "Epoch 1832, Loss: 2.0625940561294556, Final Batch Loss: 0.4169512093067169\n",
      "Epoch 1833, Loss: 1.8371102213859558, Final Batch Loss: 0.42977404594421387\n",
      "Epoch 1834, Loss: 1.958314687013626, Final Batch Loss: 0.3267627954483032\n",
      "Epoch 1835, Loss: 2.0880545377731323, Final Batch Loss: 0.5563850402832031\n",
      "Epoch 1836, Loss: 1.9110730290412903, Final Batch Loss: 0.3336663246154785\n",
      "Epoch 1837, Loss: 1.9855273365974426, Final Batch Loss: 0.32306063175201416\n",
      "Epoch 1838, Loss: 1.9451229572296143, Final Batch Loss: 0.42031607031822205\n",
      "Epoch 1839, Loss: 1.9951042532920837, Final Batch Loss: 0.3736017048358917\n",
      "Epoch 1840, Loss: 2.1762945353984833, Final Batch Loss: 0.40210288763046265\n",
      "Epoch 1841, Loss: 2.0620014369487762, Final Batch Loss: 0.5508013367652893\n",
      "Epoch 1842, Loss: 2.1793215572834015, Final Batch Loss: 0.37293246388435364\n",
      "Epoch 1843, Loss: 2.0090291798114777, Final Batch Loss: 0.280122309923172\n",
      "Epoch 1844, Loss: 1.834733173251152, Final Batch Loss: 0.22678498923778534\n",
      "Epoch 1845, Loss: 2.039205551147461, Final Batch Loss: 0.48263049125671387\n",
      "Epoch 1846, Loss: 2.0053970515727997, Final Batch Loss: 0.3678371012210846\n",
      "Epoch 1847, Loss: 2.03813636302948, Final Batch Loss: 0.42337366938591003\n",
      "Epoch 1848, Loss: 2.1342000663280487, Final Batch Loss: 0.404817670583725\n",
      "Epoch 1849, Loss: 1.8490065336227417, Final Batch Loss: 0.36611711978912354\n",
      "Epoch 1850, Loss: 2.0363386273384094, Final Batch Loss: 0.34048905968666077\n",
      "Epoch 1851, Loss: 2.152850329875946, Final Batch Loss: 0.6199122071266174\n",
      "Epoch 1852, Loss: 1.86098113656044, Final Batch Loss: 0.3209044933319092\n",
      "Epoch 1853, Loss: 2.1435708701610565, Final Batch Loss: 0.5118232369422913\n",
      "Epoch 1854, Loss: 2.122755914926529, Final Batch Loss: 0.42242392897605896\n",
      "Epoch 1855, Loss: 2.121812254190445, Final Batch Loss: 0.5262070894241333\n",
      "Epoch 1856, Loss: 1.8344106674194336, Final Batch Loss: 0.3776616156101227\n",
      "Epoch 1857, Loss: 1.983957588672638, Final Batch Loss: 0.46545013785362244\n",
      "Epoch 1858, Loss: 1.9173982739448547, Final Batch Loss: 0.3739885985851288\n",
      "Epoch 1859, Loss: 1.8360404670238495, Final Batch Loss: 0.29125645756721497\n",
      "Epoch 1860, Loss: 2.020716369152069, Final Batch Loss: 0.4465690851211548\n",
      "Epoch 1861, Loss: 2.0244122445583344, Final Batch Loss: 0.44162288308143616\n",
      "Epoch 1862, Loss: 2.142816871404648, Final Batch Loss: 0.35456737875938416\n",
      "Epoch 1863, Loss: 1.9749538004398346, Final Batch Loss: 0.3163270354270935\n",
      "Epoch 1864, Loss: 2.1306484639644623, Final Batch Loss: 0.5835168361663818\n",
      "Epoch 1865, Loss: 1.9612884223461151, Final Batch Loss: 0.32504603266716003\n",
      "Epoch 1866, Loss: 1.8299230933189392, Final Batch Loss: 0.33794599771499634\n",
      "Epoch 1867, Loss: 1.884057193994522, Final Batch Loss: 0.2721829414367676\n",
      "Epoch 1868, Loss: 1.9612703621387482, Final Batch Loss: 0.4015630781650543\n",
      "Epoch 1869, Loss: 2.116919755935669, Final Batch Loss: 0.4622328579425812\n",
      "Epoch 1870, Loss: 2.1619951128959656, Final Batch Loss: 0.3927391469478607\n",
      "Epoch 1871, Loss: 2.3333049714565277, Final Batch Loss: 0.8311347961425781\n",
      "Epoch 1872, Loss: 1.9381681978702545, Final Batch Loss: 0.35038718581199646\n",
      "Epoch 1873, Loss: 1.8412074148654938, Final Batch Loss: 0.2673829197883606\n",
      "Epoch 1874, Loss: 1.992450013756752, Final Batch Loss: 0.24653439223766327\n",
      "Epoch 1875, Loss: 2.011138379573822, Final Batch Loss: 0.3893137276172638\n",
      "Epoch 1876, Loss: 2.0572668313980103, Final Batch Loss: 0.33283016085624695\n",
      "Epoch 1877, Loss: 1.9665673673152924, Final Batch Loss: 0.33313319087028503\n",
      "Epoch 1878, Loss: 2.1598986387252808, Final Batch Loss: 0.4550321400165558\n",
      "Epoch 1879, Loss: 2.0362768471240997, Final Batch Loss: 0.4531399607658386\n",
      "Epoch 1880, Loss: 1.9274500608444214, Final Batch Loss: 0.2569674551486969\n",
      "Epoch 1881, Loss: 1.9499861598014832, Final Batch Loss: 0.3425867557525635\n",
      "Epoch 1882, Loss: 1.9006829261779785, Final Batch Loss: 0.3400333523750305\n",
      "Epoch 1883, Loss: 2.1537361443042755, Final Batch Loss: 0.5359860062599182\n",
      "Epoch 1884, Loss: 1.867835909128189, Final Batch Loss: 0.2770722508430481\n",
      "Epoch 1885, Loss: 2.1640487909317017, Final Batch Loss: 0.6913920044898987\n",
      "Epoch 1886, Loss: 1.985129565000534, Final Batch Loss: 0.47988876700401306\n",
      "Epoch 1887, Loss: 1.9842625856399536, Final Batch Loss: 0.39581233263015747\n",
      "Epoch 1888, Loss: 2.0414803326129913, Final Batch Loss: 0.4940674602985382\n",
      "Epoch 1889, Loss: 1.8074924945831299, Final Batch Loss: 0.3920899033546448\n",
      "Epoch 1890, Loss: 2.2194829881191254, Final Batch Loss: 0.6119333505630493\n",
      "Epoch 1891, Loss: 2.086274951696396, Final Batch Loss: 0.394910991191864\n",
      "Epoch 1892, Loss: 2.0936518907546997, Final Batch Loss: 0.47924181818962097\n",
      "Epoch 1893, Loss: 2.055559754371643, Final Batch Loss: 0.4753156006336212\n",
      "Epoch 1894, Loss: 1.8993518948554993, Final Batch Loss: 0.42307886481285095\n",
      "Epoch 1895, Loss: 2.0853452682495117, Final Batch Loss: 0.3784952163696289\n",
      "Epoch 1896, Loss: 1.8682607114315033, Final Batch Loss: 0.3135059177875519\n",
      "Epoch 1897, Loss: 2.021561235189438, Final Batch Loss: 0.32468244433403015\n",
      "Epoch 1898, Loss: 2.1089492738246918, Final Batch Loss: 0.5087013840675354\n",
      "Epoch 1899, Loss: 2.104534387588501, Final Batch Loss: 0.48124822974205017\n",
      "Epoch 1900, Loss: 1.9173809885978699, Final Batch Loss: 0.30778002738952637\n",
      "Epoch 1901, Loss: 2.176406145095825, Final Batch Loss: 0.4544220566749573\n",
      "Epoch 1902, Loss: 1.9167516827583313, Final Batch Loss: 0.3955875039100647\n",
      "Epoch 1903, Loss: 1.8580842018127441, Final Batch Loss: 0.2880359888076782\n",
      "Epoch 1904, Loss: 2.0873421132564545, Final Batch Loss: 0.38882675766944885\n",
      "Epoch 1905, Loss: 1.7810882031917572, Final Batch Loss: 0.3430050313472748\n",
      "Epoch 1906, Loss: 2.166271150112152, Final Batch Loss: 0.6413118839263916\n",
      "Epoch 1907, Loss: 1.8805683255195618, Final Batch Loss: 0.463329017162323\n",
      "Epoch 1908, Loss: 1.9102870225906372, Final Batch Loss: 0.41337916254997253\n",
      "Epoch 1909, Loss: 1.8208966553211212, Final Batch Loss: 0.22411388158798218\n",
      "Epoch 1910, Loss: 1.9963949024677277, Final Batch Loss: 0.499595046043396\n",
      "Epoch 1911, Loss: 2.3654609322547913, Final Batch Loss: 0.698749303817749\n",
      "Epoch 1912, Loss: 2.0680805146694183, Final Batch Loss: 0.5270641446113586\n",
      "Epoch 1913, Loss: 2.100480616092682, Final Batch Loss: 0.497667521238327\n",
      "Epoch 1914, Loss: 2.1668223440647125, Final Batch Loss: 0.4410751163959503\n",
      "Epoch 1915, Loss: 1.8005960881710052, Final Batch Loss: 0.26075875759124756\n",
      "Epoch 1916, Loss: 2.2075179517269135, Final Batch Loss: 0.4599764049053192\n",
      "Epoch 1917, Loss: 2.0801574885845184, Final Batch Loss: 0.4939974546432495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1918, Loss: 1.8036998212337494, Final Batch Loss: 0.3049459755420685\n",
      "Epoch 1919, Loss: 1.9652035236358643, Final Batch Loss: 0.31134849786758423\n",
      "Epoch 1920, Loss: 2.1934399604797363, Final Batch Loss: 0.3417334258556366\n",
      "Epoch 1921, Loss: 1.847916156053543, Final Batch Loss: 0.28836309909820557\n",
      "Epoch 1922, Loss: 1.8837261497974396, Final Batch Loss: 0.2785237729549408\n",
      "Epoch 1923, Loss: 2.232905924320221, Final Batch Loss: 0.517626166343689\n",
      "Epoch 1924, Loss: 1.9894816875457764, Final Batch Loss: 0.5167319178581238\n",
      "Epoch 1925, Loss: 1.7487498074769974, Final Batch Loss: 0.2393200546503067\n",
      "Epoch 1926, Loss: 2.1112673580646515, Final Batch Loss: 0.44592800736427307\n",
      "Epoch 1927, Loss: 1.9916604161262512, Final Batch Loss: 0.46942952275276184\n",
      "Epoch 1928, Loss: 1.9321912825107574, Final Batch Loss: 0.3667410612106323\n",
      "Epoch 1929, Loss: 2.0648771226406097, Final Batch Loss: 0.47196027636528015\n",
      "Epoch 1930, Loss: 2.0122907161712646, Final Batch Loss: 0.47330528497695923\n",
      "Epoch 1931, Loss: 1.9205510020256042, Final Batch Loss: 0.31666943430900574\n",
      "Epoch 1932, Loss: 2.2434717416763306, Final Batch Loss: 0.5699036121368408\n",
      "Epoch 1933, Loss: 2.0238181054592133, Final Batch Loss: 0.3824324309825897\n",
      "Epoch 1934, Loss: 1.9594663381576538, Final Batch Loss: 0.3207729756832123\n",
      "Epoch 1935, Loss: 2.121854603290558, Final Batch Loss: 0.43069615960121155\n",
      "Epoch 1936, Loss: 1.896115392446518, Final Batch Loss: 0.32949918508529663\n",
      "Epoch 1937, Loss: 2.02808079123497, Final Batch Loss: 0.49055203795433044\n",
      "Epoch 1938, Loss: 2.02943354845047, Final Batch Loss: 0.3965471684932709\n",
      "Epoch 1939, Loss: 1.868052899837494, Final Batch Loss: 0.35517576336860657\n",
      "Epoch 1940, Loss: 1.9846217632293701, Final Batch Loss: 0.4683055281639099\n",
      "Epoch 1941, Loss: 1.7998602092266083, Final Batch Loss: 0.3926866352558136\n",
      "Epoch 1942, Loss: 1.9296416342258453, Final Batch Loss: 0.35221993923187256\n",
      "Epoch 1943, Loss: 1.9741518795490265, Final Batch Loss: 0.5247299671173096\n",
      "Epoch 1944, Loss: 1.850073665380478, Final Batch Loss: 0.232293963432312\n",
      "Epoch 1945, Loss: 2.1462833881378174, Final Batch Loss: 0.32611265778541565\n",
      "Epoch 1946, Loss: 1.9266443252563477, Final Batch Loss: 0.31766843795776367\n",
      "Epoch 1947, Loss: 2.1185167133808136, Final Batch Loss: 0.4586755335330963\n",
      "Epoch 1948, Loss: 1.9968479573726654, Final Batch Loss: 0.47727257013320923\n",
      "Epoch 1949, Loss: 1.977981150150299, Final Batch Loss: 0.48044532537460327\n",
      "Epoch 1950, Loss: 2.1478569507598877, Final Batch Loss: 0.6338822245597839\n",
      "Epoch 1951, Loss: 1.898268699645996, Final Batch Loss: 0.3318535387516022\n",
      "Epoch 1952, Loss: 2.1558274030685425, Final Batch Loss: 0.5723349452018738\n",
      "Epoch 1953, Loss: 1.946209877729416, Final Batch Loss: 0.42349526286125183\n",
      "Epoch 1954, Loss: 1.8782165944576263, Final Batch Loss: 0.2790277898311615\n",
      "Epoch 1955, Loss: 1.832416445016861, Final Batch Loss: 0.2839736044406891\n",
      "Epoch 1956, Loss: 2.125578224658966, Final Batch Loss: 0.6111839413642883\n",
      "Epoch 1957, Loss: 1.9442542493343353, Final Batch Loss: 0.39198604226112366\n",
      "Epoch 1958, Loss: 1.9993042051792145, Final Batch Loss: 0.4114191234111786\n",
      "Epoch 1959, Loss: 1.8281351327896118, Final Batch Loss: 0.2977669835090637\n",
      "Epoch 1960, Loss: 1.8822068274021149, Final Batch Loss: 0.2925518751144409\n",
      "Epoch 1961, Loss: 1.9806821048259735, Final Batch Loss: 0.4070960581302643\n",
      "Epoch 1962, Loss: 1.7416188716888428, Final Batch Loss: 0.24718493223190308\n",
      "Epoch 1963, Loss: 1.8922291696071625, Final Batch Loss: 0.33271563053131104\n",
      "Epoch 1964, Loss: 1.8067193031311035, Final Batch Loss: 0.23866039514541626\n",
      "Epoch 1965, Loss: 2.015573650598526, Final Batch Loss: 0.43517595529556274\n",
      "Epoch 1966, Loss: 2.2438590824604034, Final Batch Loss: 0.3618510365486145\n",
      "Epoch 1967, Loss: 1.9401777386665344, Final Batch Loss: 0.28388872742652893\n",
      "Epoch 1968, Loss: 1.8316989541053772, Final Batch Loss: 0.3139636218547821\n",
      "Epoch 1969, Loss: 1.8787458539009094, Final Batch Loss: 0.4078013300895691\n",
      "Epoch 1970, Loss: 1.9716443419456482, Final Batch Loss: 0.4198727309703827\n",
      "Epoch 1971, Loss: 2.0967569947242737, Final Batch Loss: 0.546970546245575\n",
      "Epoch 1972, Loss: 1.8961139619350433, Final Batch Loss: 0.38223955035209656\n",
      "Epoch 1973, Loss: 1.9176473915576935, Final Batch Loss: 0.35557109117507935\n",
      "Epoch 1974, Loss: 2.1729021072387695, Final Batch Loss: 0.3525918424129486\n",
      "Epoch 1975, Loss: 1.9903264045715332, Final Batch Loss: 0.3947676420211792\n",
      "Epoch 1976, Loss: 1.8744400143623352, Final Batch Loss: 0.3706969618797302\n",
      "Epoch 1977, Loss: 2.109214723110199, Final Batch Loss: 0.5846734046936035\n",
      "Epoch 1978, Loss: 1.818783938884735, Final Batch Loss: 0.32456034421920776\n",
      "Epoch 1979, Loss: 1.8105835616588593, Final Batch Loss: 0.3224833607673645\n",
      "Epoch 1980, Loss: 2.188661903142929, Final Batch Loss: 0.6200067400932312\n",
      "Epoch 1981, Loss: 1.8627545833587646, Final Batch Loss: 0.31224381923675537\n",
      "Epoch 1982, Loss: 1.9320658147335052, Final Batch Loss: 0.2923758029937744\n",
      "Epoch 1983, Loss: 2.0939877331256866, Final Batch Loss: 0.5042051076889038\n",
      "Epoch 1984, Loss: 1.9618181884288788, Final Batch Loss: 0.41473278403282166\n",
      "Epoch 1985, Loss: 1.9180540144443512, Final Batch Loss: 0.3853527307510376\n",
      "Epoch 1986, Loss: 1.9127176702022552, Final Batch Loss: 0.3638475239276886\n",
      "Epoch 1987, Loss: 2.0017351508140564, Final Batch Loss: 0.5085899829864502\n",
      "Epoch 1988, Loss: 1.8464269936084747, Final Batch Loss: 0.2926415801048279\n",
      "Epoch 1989, Loss: 1.8811593353748322, Final Batch Loss: 0.37815481424331665\n",
      "Epoch 1990, Loss: 2.2841411232948303, Final Batch Loss: 0.5723599791526794\n",
      "Epoch 1991, Loss: 1.877736121416092, Final Batch Loss: 0.4941585958003998\n",
      "Epoch 1992, Loss: 2.07458034157753, Final Batch Loss: 0.3710682988166809\n",
      "Epoch 1993, Loss: 1.8240564465522766, Final Batch Loss: 0.27410274744033813\n",
      "Epoch 1994, Loss: 2.0593268275260925, Final Batch Loss: 0.4745708405971527\n",
      "Epoch 1995, Loss: 2.2583769261837006, Final Batch Loss: 0.459312379360199\n",
      "Epoch 1996, Loss: 1.9319137334823608, Final Batch Loss: 0.3219742476940155\n",
      "Epoch 1997, Loss: 1.9163712859153748, Final Batch Loss: 0.3094537556171417\n",
      "Epoch 1998, Loss: 2.2579580545425415, Final Batch Loss: 0.6800515055656433\n",
      "Epoch 1999, Loss: 1.8936476409435272, Final Batch Loss: 0.3294185996055603\n",
      "Epoch 2000, Loss: 2.0342140793800354, Final Batch Loss: 0.3648216426372528\n",
      "Epoch 2001, Loss: 1.9717106521129608, Final Batch Loss: 0.32149994373321533\n",
      "Epoch 2002, Loss: 1.9159946739673615, Final Batch Loss: 0.2596963047981262\n",
      "Epoch 2003, Loss: 2.091094493865967, Final Batch Loss: 0.48830878734588623\n",
      "Epoch 2004, Loss: 1.9946934878826141, Final Batch Loss: 0.34658122062683105\n",
      "Epoch 2005, Loss: 2.1177403926849365, Final Batch Loss: 0.4313412308692932\n",
      "Epoch 2006, Loss: 1.919877141714096, Final Batch Loss: 0.23795026540756226\n",
      "Epoch 2007, Loss: 1.9832013547420502, Final Batch Loss: 0.3943580389022827\n",
      "Epoch 2008, Loss: 2.0100164115428925, Final Batch Loss: 0.42028912901878357\n",
      "Epoch 2009, Loss: 1.7459291070699692, Final Batch Loss: 0.1459222286939621\n",
      "Epoch 2010, Loss: 1.9390767812728882, Final Batch Loss: 0.38807764649391174\n",
      "Epoch 2011, Loss: 2.2507914900779724, Final Batch Loss: 0.632256031036377\n",
      "Epoch 2012, Loss: 1.8820916712284088, Final Batch Loss: 0.3524560332298279\n",
      "Epoch 2013, Loss: 1.6692040413618088, Final Batch Loss: 0.23979000747203827\n",
      "Epoch 2014, Loss: 2.143723428249359, Final Batch Loss: 0.4811306595802307\n",
      "Epoch 2015, Loss: 1.68503338098526, Final Batch Loss: 0.2983740568161011\n",
      "Epoch 2016, Loss: 2.013929605484009, Final Batch Loss: 0.4557458460330963\n",
      "Epoch 2017, Loss: 1.894819736480713, Final Batch Loss: 0.3900910019874573\n",
      "Epoch 2018, Loss: 2.1088567078113556, Final Batch Loss: 0.4294617772102356\n",
      "Epoch 2019, Loss: 2.174226403236389, Final Batch Loss: 0.4053344130516052\n",
      "Epoch 2020, Loss: 1.9243740737438202, Final Batch Loss: 0.35070452094078064\n",
      "Epoch 2021, Loss: 2.193552613258362, Final Batch Loss: 0.586110532283783\n",
      "Epoch 2022, Loss: 1.9875354766845703, Final Batch Loss: 0.3906095027923584\n",
      "Epoch 2023, Loss: 1.9329991340637207, Final Batch Loss: 0.3852379322052002\n",
      "Epoch 2024, Loss: 2.059601753950119, Final Batch Loss: 0.47780168056488037\n",
      "Epoch 2025, Loss: 2.026199519634247, Final Batch Loss: 0.5418006777763367\n",
      "Epoch 2026, Loss: 2.034136950969696, Final Batch Loss: 0.2772674560546875\n",
      "Epoch 2027, Loss: 1.898728609085083, Final Batch Loss: 0.3675297498703003\n",
      "Epoch 2028, Loss: 1.949247032403946, Final Batch Loss: 0.4479115605354309\n",
      "Epoch 2029, Loss: 1.9007713198661804, Final Batch Loss: 0.41198495030403137\n",
      "Epoch 2030, Loss: 1.8997445106506348, Final Batch Loss: 0.37388384342193604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2031, Loss: 2.0134705305099487, Final Batch Loss: 0.40271854400634766\n",
      "Epoch 2032, Loss: 1.7158831059932709, Final Batch Loss: 0.26680076122283936\n",
      "Epoch 2033, Loss: 2.048921525478363, Final Batch Loss: 0.6497237682342529\n",
      "Epoch 2034, Loss: 1.9011042416095734, Final Batch Loss: 0.3051968216896057\n",
      "Epoch 2035, Loss: 2.096392124891281, Final Batch Loss: 0.45111504197120667\n",
      "Epoch 2036, Loss: 1.9503065645694733, Final Batch Loss: 0.4385835826396942\n",
      "Epoch 2037, Loss: 1.909157007932663, Final Batch Loss: 0.3549819886684418\n",
      "Epoch 2038, Loss: 1.8713497519493103, Final Batch Loss: 0.26491424441337585\n",
      "Epoch 2039, Loss: 1.816226840019226, Final Batch Loss: 0.2635424733161926\n",
      "Epoch 2040, Loss: 1.943719059228897, Final Batch Loss: 0.3969922661781311\n",
      "Epoch 2041, Loss: 2.088927060365677, Final Batch Loss: 0.595841646194458\n",
      "Epoch 2042, Loss: 1.9362965822219849, Final Batch Loss: 0.4028371274471283\n",
      "Epoch 2043, Loss: 2.043961673974991, Final Batch Loss: 0.3553427755832672\n",
      "Epoch 2044, Loss: 1.964285433292389, Final Batch Loss: 0.31578901410102844\n",
      "Epoch 2045, Loss: 2.102807492017746, Final Batch Loss: 0.4652960002422333\n",
      "Epoch 2046, Loss: 1.9682513177394867, Final Batch Loss: 0.38091668486595154\n",
      "Epoch 2047, Loss: 1.9360354244709015, Final Batch Loss: 0.3573606312274933\n",
      "Epoch 2048, Loss: 2.003606379032135, Final Batch Loss: 0.47331702709198\n",
      "Epoch 2049, Loss: 2.0042497515678406, Final Batch Loss: 0.3397914171218872\n",
      "Epoch 2050, Loss: 1.7788539230823517, Final Batch Loss: 0.3116806149482727\n",
      "Epoch 2051, Loss: 1.7869455814361572, Final Batch Loss: 0.20382672548294067\n",
      "Epoch 2052, Loss: 1.9715517461299896, Final Batch Loss: 0.42065712809562683\n",
      "Epoch 2053, Loss: 1.9227532744407654, Final Batch Loss: 0.4188379943370819\n",
      "Epoch 2054, Loss: 1.8964843153953552, Final Batch Loss: 0.33059000968933105\n",
      "Epoch 2055, Loss: 1.7879024147987366, Final Batch Loss: 0.334133118391037\n",
      "Epoch 2056, Loss: 1.968078762292862, Final Batch Loss: 0.3384518623352051\n",
      "Epoch 2057, Loss: 1.8594649732112885, Final Batch Loss: 0.33597660064697266\n",
      "Epoch 2058, Loss: 1.8381649851799011, Final Batch Loss: 0.33068326115608215\n",
      "Epoch 2059, Loss: 2.092298924922943, Final Batch Loss: 0.4266817569732666\n",
      "Epoch 2060, Loss: 1.8995277881622314, Final Batch Loss: 0.3077932596206665\n",
      "Epoch 2061, Loss: 1.8235315382480621, Final Batch Loss: 0.3082863986492157\n",
      "Epoch 2062, Loss: 1.826442927122116, Final Batch Loss: 0.376972496509552\n",
      "Epoch 2063, Loss: 2.167794942855835, Final Batch Loss: 0.5133519768714905\n",
      "Epoch 2064, Loss: 1.8190313279628754, Final Batch Loss: 0.2902027368545532\n",
      "Epoch 2065, Loss: 2.0223211646080017, Final Batch Loss: 0.4882972538471222\n",
      "Epoch 2066, Loss: 1.8771789371967316, Final Batch Loss: 0.3834936320781708\n",
      "Epoch 2067, Loss: 2.03052818775177, Final Batch Loss: 0.40214112401008606\n",
      "Epoch 2068, Loss: 1.9423826038837433, Final Batch Loss: 0.4277726709842682\n",
      "Epoch 2069, Loss: 1.8465586602687836, Final Batch Loss: 0.38214147090911865\n",
      "Epoch 2070, Loss: 1.9453663527965546, Final Batch Loss: 0.42126569151878357\n",
      "Epoch 2071, Loss: 1.9414076507091522, Final Batch Loss: 0.41629597544670105\n",
      "Epoch 2072, Loss: 1.888881891965866, Final Batch Loss: 0.3012659251689911\n",
      "Epoch 2073, Loss: 1.8842875063419342, Final Batch Loss: 0.40744563937187195\n",
      "Epoch 2074, Loss: 1.8823376595973969, Final Batch Loss: 0.2506411671638489\n",
      "Epoch 2075, Loss: 2.051694005727768, Final Batch Loss: 0.3834381699562073\n",
      "Epoch 2076, Loss: 1.9695797264575958, Final Batch Loss: 0.3996583819389343\n",
      "Epoch 2077, Loss: 1.825796753168106, Final Batch Loss: 0.38304227590560913\n",
      "Epoch 2078, Loss: 2.07071316242218, Final Batch Loss: 0.38886889815330505\n",
      "Epoch 2079, Loss: 2.0154952704906464, Final Batch Loss: 0.44582876563072205\n",
      "Epoch 2080, Loss: 2.078327387571335, Final Batch Loss: 0.4808322787284851\n",
      "Epoch 2081, Loss: 2.0600509345531464, Final Batch Loss: 0.4209245443344116\n",
      "Epoch 2082, Loss: 1.822492390871048, Final Batch Loss: 0.38732007145881653\n",
      "Epoch 2083, Loss: 2.0122314393520355, Final Batch Loss: 0.4330548942089081\n",
      "Epoch 2084, Loss: 1.859324336051941, Final Batch Loss: 0.4134337604045868\n",
      "Epoch 2085, Loss: 1.97605961561203, Final Batch Loss: 0.3433104455471039\n",
      "Epoch 2086, Loss: 1.9613690376281738, Final Batch Loss: 0.3950367271900177\n",
      "Epoch 2087, Loss: 1.9360696077346802, Final Batch Loss: 0.4388198256492615\n",
      "Epoch 2088, Loss: 1.7276805937290192, Final Batch Loss: 0.31924086809158325\n",
      "Epoch 2089, Loss: 1.825709730386734, Final Batch Loss: 0.30714473128318787\n",
      "Epoch 2090, Loss: 1.7555835843086243, Final Batch Loss: 0.3214068114757538\n",
      "Epoch 2091, Loss: 1.80414018034935, Final Batch Loss: 0.2850410044193268\n",
      "Epoch 2092, Loss: 1.8405908346176147, Final Batch Loss: 0.3589610159397125\n",
      "Epoch 2093, Loss: 2.1520784497261047, Final Batch Loss: 0.46431857347488403\n",
      "Epoch 2094, Loss: 2.002188354730606, Final Batch Loss: 0.5479636788368225\n",
      "Epoch 2095, Loss: 1.7781852781772614, Final Batch Loss: 0.2942918837070465\n",
      "Epoch 2096, Loss: 1.9798309206962585, Final Batch Loss: 0.3730296492576599\n",
      "Epoch 2097, Loss: 1.891899585723877, Final Batch Loss: 0.3171756863594055\n",
      "Epoch 2098, Loss: 1.705793485045433, Final Batch Loss: 0.17274712026119232\n",
      "Epoch 2099, Loss: 1.9592812061309814, Final Batch Loss: 0.44895440340042114\n",
      "Epoch 2100, Loss: 1.9304652214050293, Final Batch Loss: 0.4652145802974701\n",
      "Epoch 2101, Loss: 2.0220832526683807, Final Batch Loss: 0.4012921452522278\n",
      "Epoch 2102, Loss: 1.870329201221466, Final Batch Loss: 0.28450465202331543\n",
      "Epoch 2103, Loss: 2.23542183637619, Final Batch Loss: 0.5521915555000305\n",
      "Epoch 2104, Loss: 1.7519497573375702, Final Batch Loss: 0.23954197764396667\n",
      "Epoch 2105, Loss: 2.0497012734413147, Final Batch Loss: 0.31212249398231506\n",
      "Epoch 2106, Loss: 1.9836807548999786, Final Batch Loss: 0.5256756544113159\n",
      "Epoch 2107, Loss: 1.7646784782409668, Final Batch Loss: 0.31652194261550903\n",
      "Epoch 2108, Loss: 2.0769603848457336, Final Batch Loss: 0.5319699645042419\n",
      "Epoch 2109, Loss: 1.8338779211044312, Final Batch Loss: 0.436251163482666\n",
      "Epoch 2110, Loss: 2.032794862985611, Final Batch Loss: 0.4493839740753174\n",
      "Epoch 2111, Loss: 1.9419358372688293, Final Batch Loss: 0.47974613308906555\n",
      "Epoch 2112, Loss: 1.7699496448040009, Final Batch Loss: 0.27969905734062195\n",
      "Epoch 2113, Loss: 2.0471191108226776, Final Batch Loss: 0.33913564682006836\n",
      "Epoch 2114, Loss: 1.933466762304306, Final Batch Loss: 0.3246183395385742\n",
      "Epoch 2115, Loss: 1.8810581266880035, Final Batch Loss: 0.32434338331222534\n",
      "Epoch 2116, Loss: 1.9079128801822662, Final Batch Loss: 0.39480891823768616\n",
      "Epoch 2117, Loss: 1.8902587592601776, Final Batch Loss: 0.28052499890327454\n",
      "Epoch 2118, Loss: 1.7244245409965515, Final Batch Loss: 0.3137936592102051\n",
      "Epoch 2119, Loss: 1.7469760477542877, Final Batch Loss: 0.27249330282211304\n",
      "Epoch 2120, Loss: 1.7362239062786102, Final Batch Loss: 0.2773759961128235\n",
      "Epoch 2121, Loss: 2.0558057129383087, Final Batch Loss: 0.61612468957901\n",
      "Epoch 2122, Loss: 1.871970683336258, Final Batch Loss: 0.28315383195877075\n",
      "Epoch 2123, Loss: 2.0926588475704193, Final Batch Loss: 0.3066437542438507\n",
      "Epoch 2124, Loss: 1.767609566450119, Final Batch Loss: 0.40456506609916687\n",
      "Epoch 2125, Loss: 1.8062386512756348, Final Batch Loss: 0.3497713506221771\n",
      "Epoch 2126, Loss: 1.7930428683757782, Final Batch Loss: 0.27242183685302734\n",
      "Epoch 2127, Loss: 1.9675840139389038, Final Batch Loss: 0.5093639492988586\n",
      "Epoch 2128, Loss: 1.7627688944339752, Final Batch Loss: 0.3018903136253357\n",
      "Epoch 2129, Loss: 1.9059930443763733, Final Batch Loss: 0.31719958782196045\n",
      "Epoch 2130, Loss: 1.7200316190719604, Final Batch Loss: 0.27347156405448914\n",
      "Epoch 2131, Loss: 1.8544487953186035, Final Batch Loss: 0.3528543710708618\n",
      "Epoch 2132, Loss: 1.9056397378444672, Final Batch Loss: 0.27207648754119873\n",
      "Epoch 2133, Loss: 1.8051129877567291, Final Batch Loss: 0.3383781611919403\n",
      "Epoch 2134, Loss: 1.6866781413555145, Final Batch Loss: 0.3021194636821747\n",
      "Epoch 2135, Loss: 1.9155215919017792, Final Batch Loss: 0.41975754499435425\n",
      "Epoch 2136, Loss: 1.9166407585144043, Final Batch Loss: 0.44306012988090515\n",
      "Epoch 2137, Loss: 2.160100221633911, Final Batch Loss: 0.4556902050971985\n",
      "Epoch 2138, Loss: 1.9919674396514893, Final Batch Loss: 0.28935205936431885\n",
      "Epoch 2139, Loss: 1.9124462306499481, Final Batch Loss: 0.375068336725235\n",
      "Epoch 2140, Loss: 1.874154806137085, Final Batch Loss: 0.37009450793266296\n",
      "Epoch 2141, Loss: 1.863427221775055, Final Batch Loss: 0.2856386601924896\n",
      "Epoch 2142, Loss: 1.9104024767875671, Final Batch Loss: 0.3726693391799927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2143, Loss: 1.741528481245041, Final Batch Loss: 0.3090919256210327\n",
      "Epoch 2144, Loss: 1.7171120047569275, Final Batch Loss: 0.36193570494651794\n",
      "Epoch 2145, Loss: 2.0121136903762817, Final Batch Loss: 0.4547058641910553\n",
      "Epoch 2146, Loss: 1.8950101137161255, Final Batch Loss: 0.4936580955982208\n",
      "Epoch 2147, Loss: 1.942147433757782, Final Batch Loss: 0.4540988504886627\n",
      "Epoch 2148, Loss: 1.8686756789684296, Final Batch Loss: 0.3128892183303833\n",
      "Epoch 2149, Loss: 1.7539546489715576, Final Batch Loss: 0.2917260527610779\n",
      "Epoch 2150, Loss: 2.096091330051422, Final Batch Loss: 0.4615444242954254\n",
      "Epoch 2151, Loss: 2.0654388070106506, Final Batch Loss: 0.49183714389801025\n",
      "Epoch 2152, Loss: 1.9566348493099213, Final Batch Loss: 0.3907504975795746\n",
      "Epoch 2153, Loss: 1.8514695167541504, Final Batch Loss: 0.2638072371482849\n",
      "Epoch 2154, Loss: 1.8890419006347656, Final Batch Loss: 0.41652625799179077\n",
      "Epoch 2155, Loss: 2.106570541858673, Final Batch Loss: 0.46522626280784607\n",
      "Epoch 2156, Loss: 2.035043954849243, Final Batch Loss: 0.5666752457618713\n",
      "Epoch 2157, Loss: 1.8152604699134827, Final Batch Loss: 0.3661612570285797\n",
      "Epoch 2158, Loss: 1.8924781680107117, Final Batch Loss: 0.33977311849594116\n",
      "Epoch 2159, Loss: 1.7118452191352844, Final Batch Loss: 0.2671620845794678\n",
      "Epoch 2160, Loss: 1.8257515728473663, Final Batch Loss: 0.2895851731300354\n",
      "Epoch 2161, Loss: 2.0506729781627655, Final Batch Loss: 0.41401565074920654\n",
      "Epoch 2162, Loss: 1.9507419764995575, Final Batch Loss: 0.33760184049606323\n",
      "Epoch 2163, Loss: 1.9538874328136444, Final Batch Loss: 0.3431538939476013\n",
      "Epoch 2164, Loss: 1.8806302845478058, Final Batch Loss: 0.3649245798587799\n",
      "Epoch 2165, Loss: 1.8735128939151764, Final Batch Loss: 0.2051577866077423\n",
      "Epoch 2166, Loss: 2.127313017845154, Final Batch Loss: 0.548294186592102\n",
      "Epoch 2167, Loss: 1.7538876235485077, Final Batch Loss: 0.3647749125957489\n",
      "Epoch 2168, Loss: 1.7800039052963257, Final Batch Loss: 0.41016528010368347\n",
      "Epoch 2169, Loss: 1.9716682136058807, Final Batch Loss: 0.4818447530269623\n",
      "Epoch 2170, Loss: 1.9071983397006989, Final Batch Loss: 0.36621206998825073\n",
      "Epoch 2171, Loss: 1.9801283776760101, Final Batch Loss: 0.4915984869003296\n",
      "Epoch 2172, Loss: 1.7981879711151123, Final Batch Loss: 0.35800641775131226\n",
      "Epoch 2173, Loss: 1.9915751516819, Final Batch Loss: 0.45085108280181885\n",
      "Epoch 2174, Loss: 1.8446154594421387, Final Batch Loss: 0.40815889835357666\n",
      "Epoch 2175, Loss: 1.8702501952648163, Final Batch Loss: 0.34053605794906616\n",
      "Epoch 2176, Loss: 1.8354461193084717, Final Batch Loss: 0.2789207398891449\n",
      "Epoch 2177, Loss: 2.1555308997631073, Final Batch Loss: 0.37469589710235596\n",
      "Epoch 2178, Loss: 1.8857367932796478, Final Batch Loss: 0.33956223726272583\n",
      "Epoch 2179, Loss: 2.016030728816986, Final Batch Loss: 0.4814397692680359\n",
      "Epoch 2180, Loss: 1.9600618481636047, Final Batch Loss: 0.5065897107124329\n",
      "Epoch 2181, Loss: 1.9456285536289215, Final Batch Loss: 0.35812270641326904\n",
      "Epoch 2182, Loss: 2.263977289199829, Final Batch Loss: 0.5837872624397278\n",
      "Epoch 2183, Loss: 1.8478163182735443, Final Batch Loss: 0.42564138770103455\n",
      "Epoch 2184, Loss: 1.8330658078193665, Final Batch Loss: 0.32257962226867676\n",
      "Epoch 2185, Loss: 1.9845826625823975, Final Batch Loss: 0.3343544006347656\n",
      "Epoch 2186, Loss: 2.0655620098114014, Final Batch Loss: 0.49677082896232605\n",
      "Epoch 2187, Loss: 1.8877543807029724, Final Batch Loss: 0.2996712327003479\n",
      "Epoch 2188, Loss: 2.0844362378120422, Final Batch Loss: 0.5013849139213562\n",
      "Epoch 2189, Loss: 1.9339162111282349, Final Batch Loss: 0.40302714705467224\n",
      "Epoch 2190, Loss: 2.0063124895095825, Final Batch Loss: 0.4179948568344116\n",
      "Epoch 2191, Loss: 2.0888917446136475, Final Batch Loss: 0.4586799442768097\n",
      "Epoch 2192, Loss: 1.9759849905967712, Final Batch Loss: 0.35706013441085815\n",
      "Epoch 2193, Loss: 1.9720345735549927, Final Batch Loss: 0.5370082855224609\n",
      "Epoch 2194, Loss: 1.6583984941244125, Final Batch Loss: 0.24252651631832123\n",
      "Epoch 2195, Loss: 1.9049998819828033, Final Batch Loss: 0.37514638900756836\n",
      "Epoch 2196, Loss: 2.0607706904411316, Final Batch Loss: 0.49529924988746643\n",
      "Epoch 2197, Loss: 1.9251323342323303, Final Batch Loss: 0.428605318069458\n",
      "Epoch 2198, Loss: 1.8535357117652893, Final Batch Loss: 0.3506438136100769\n",
      "Epoch 2199, Loss: 1.8500319719314575, Final Batch Loss: 0.2786176800727844\n",
      "Epoch 2200, Loss: 2.0700137615203857, Final Batch Loss: 0.4448961615562439\n",
      "Epoch 2201, Loss: 2.0387719571590424, Final Batch Loss: 0.40416285395622253\n",
      "Epoch 2202, Loss: 1.9170613884925842, Final Batch Loss: 0.3375667333602905\n",
      "Epoch 2203, Loss: 1.941048264503479, Final Batch Loss: 0.34566164016723633\n",
      "Epoch 2204, Loss: 2.145732343196869, Final Batch Loss: 0.4617241621017456\n",
      "Epoch 2205, Loss: 1.7851451635360718, Final Batch Loss: 0.40689876675605774\n",
      "Epoch 2206, Loss: 1.8811071515083313, Final Batch Loss: 0.3559989333152771\n",
      "Epoch 2207, Loss: 2.0556872189044952, Final Batch Loss: 0.43988701701164246\n",
      "Epoch 2208, Loss: 1.938301146030426, Final Batch Loss: 0.40259426832199097\n",
      "Epoch 2209, Loss: 1.7877784371376038, Final Batch Loss: 0.29844537377357483\n",
      "Epoch 2210, Loss: 1.8007879257202148, Final Batch Loss: 0.31943944096565247\n",
      "Epoch 2211, Loss: 1.7747957408428192, Final Batch Loss: 0.4564250707626343\n",
      "Epoch 2212, Loss: 1.849841445684433, Final Batch Loss: 0.37886184453964233\n",
      "Epoch 2213, Loss: 1.802053540945053, Final Batch Loss: 0.4236432909965515\n",
      "Epoch 2214, Loss: 1.7596631944179535, Final Batch Loss: 0.31352517008781433\n",
      "Epoch 2215, Loss: 1.9420161843299866, Final Batch Loss: 0.45032253861427307\n",
      "Epoch 2216, Loss: 2.026562064886093, Final Batch Loss: 0.5527281165122986\n",
      "Epoch 2217, Loss: 1.78439599275589, Final Batch Loss: 0.353488564491272\n",
      "Epoch 2218, Loss: 1.8394101858139038, Final Batch Loss: 0.33650854229927063\n",
      "Epoch 2219, Loss: 1.892150640487671, Final Batch Loss: 0.2995440661907196\n",
      "Epoch 2220, Loss: 1.877391368150711, Final Batch Loss: 0.2584051787853241\n",
      "Epoch 2221, Loss: 1.7732017636299133, Final Batch Loss: 0.41420045495033264\n",
      "Epoch 2222, Loss: 1.7810851633548737, Final Batch Loss: 0.33700406551361084\n",
      "Epoch 2223, Loss: 1.8874671906232834, Final Batch Loss: 0.22188366949558258\n",
      "Epoch 2224, Loss: 1.9583317041397095, Final Batch Loss: 0.30460405349731445\n",
      "Epoch 2225, Loss: 1.8018960058689117, Final Batch Loss: 0.35245242714881897\n",
      "Epoch 2226, Loss: 2.108001470565796, Final Batch Loss: 0.44068339467048645\n",
      "Epoch 2227, Loss: 1.8305152654647827, Final Batch Loss: 0.379032701253891\n",
      "Epoch 2228, Loss: 1.8408422768115997, Final Batch Loss: 0.29547855257987976\n",
      "Epoch 2229, Loss: 1.7797230184078217, Final Batch Loss: 0.25917989015579224\n",
      "Epoch 2230, Loss: 1.967235803604126, Final Batch Loss: 0.3725700378417969\n",
      "Epoch 2231, Loss: 1.8703685700893402, Final Batch Loss: 0.2498244047164917\n",
      "Epoch 2232, Loss: 1.9716992378234863, Final Batch Loss: 0.3337137699127197\n",
      "Epoch 2233, Loss: 1.8904799818992615, Final Batch Loss: 0.4072320759296417\n",
      "Epoch 2234, Loss: 1.805825263261795, Final Batch Loss: 0.3388553559780121\n",
      "Epoch 2235, Loss: 1.8609727919101715, Final Batch Loss: 0.2734038233757019\n",
      "Epoch 2236, Loss: 2.022409111261368, Final Batch Loss: 0.4112595021724701\n",
      "Epoch 2237, Loss: 1.8453066051006317, Final Batch Loss: 0.2989986836910248\n",
      "Epoch 2238, Loss: 1.8493866324424744, Final Batch Loss: 0.3171190023422241\n",
      "Epoch 2239, Loss: 1.9125267565250397, Final Batch Loss: 0.4030248522758484\n",
      "Epoch 2240, Loss: 1.8033820688724518, Final Batch Loss: 0.31955674290657043\n",
      "Epoch 2241, Loss: 1.8314015865325928, Final Batch Loss: 0.3839767277240753\n",
      "Epoch 2242, Loss: 1.8620185852050781, Final Batch Loss: 0.31728503108024597\n",
      "Epoch 2243, Loss: 1.7003374993801117, Final Batch Loss: 0.3242667019367218\n",
      "Epoch 2244, Loss: 2.0073111951351166, Final Batch Loss: 0.4509720802307129\n",
      "Epoch 2245, Loss: 2.0497881174087524, Final Batch Loss: 0.5974957942962646\n",
      "Epoch 2246, Loss: 1.9580008387565613, Final Batch Loss: 0.4624793231487274\n",
      "Epoch 2247, Loss: 1.817531794309616, Final Batch Loss: 0.3532775640487671\n",
      "Epoch 2248, Loss: 1.980073094367981, Final Batch Loss: 0.5249523520469666\n",
      "Epoch 2249, Loss: 1.9788944125175476, Final Batch Loss: 0.5251783132553101\n",
      "Epoch 2250, Loss: 1.8973775207996368, Final Batch Loss: 0.4204563796520233\n",
      "Epoch 2251, Loss: 1.9047706127166748, Final Batch Loss: 0.4747535288333893\n",
      "Epoch 2252, Loss: 1.9798479974269867, Final Batch Loss: 0.484646737575531\n",
      "Epoch 2253, Loss: 1.8090970516204834, Final Batch Loss: 0.2126937210559845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2254, Loss: 2.0563701391220093, Final Batch Loss: 0.4175201952457428\n",
      "Epoch 2255, Loss: 1.8778047859668732, Final Batch Loss: 0.3268181085586548\n",
      "Epoch 2256, Loss: 1.7906318008899689, Final Batch Loss: 0.3995361328125\n",
      "Epoch 2257, Loss: 1.9638555645942688, Final Batch Loss: 0.4556519091129303\n",
      "Epoch 2258, Loss: 1.9540654420852661, Final Batch Loss: 0.48502108454704285\n",
      "Epoch 2259, Loss: 1.8318008184432983, Final Batch Loss: 0.38282161951065063\n",
      "Epoch 2260, Loss: 1.8307731598615646, Final Batch Loss: 0.24508632719516754\n",
      "Epoch 2261, Loss: 1.9043417870998383, Final Batch Loss: 0.2511097192764282\n",
      "Epoch 2262, Loss: 2.1361634135246277, Final Batch Loss: 0.6854581236839294\n",
      "Epoch 2263, Loss: 1.9099655151367188, Final Batch Loss: 0.3074982464313507\n",
      "Epoch 2264, Loss: 1.979341208934784, Final Batch Loss: 0.5453790426254272\n",
      "Epoch 2265, Loss: 2.117806226015091, Final Batch Loss: 0.5411669015884399\n",
      "Epoch 2266, Loss: 2.0746128857135773, Final Batch Loss: 0.35370877385139465\n",
      "Epoch 2267, Loss: 2.156846970319748, Final Batch Loss: 0.38621702790260315\n",
      "Epoch 2268, Loss: 2.294147878885269, Final Batch Loss: 0.7349815964698792\n",
      "Epoch 2269, Loss: 1.9717109501361847, Final Batch Loss: 0.4172815680503845\n",
      "Epoch 2270, Loss: 1.8100813925266266, Final Batch Loss: 0.3062729835510254\n",
      "Epoch 2271, Loss: 1.8602762818336487, Final Batch Loss: 0.3511902987957001\n",
      "Epoch 2272, Loss: 1.9571626484394073, Final Batch Loss: 0.31379464268684387\n",
      "Epoch 2273, Loss: 1.8704798519611359, Final Batch Loss: 0.5049663186073303\n",
      "Epoch 2274, Loss: 1.7184885740280151, Final Batch Loss: 0.3006327450275421\n",
      "Epoch 2275, Loss: 1.8135080635547638, Final Batch Loss: 0.302253782749176\n",
      "Epoch 2276, Loss: 1.955784410238266, Final Batch Loss: 0.48329439759254456\n",
      "Epoch 2277, Loss: 2.069278210401535, Final Batch Loss: 0.45140376687049866\n",
      "Epoch 2278, Loss: 2.0763930678367615, Final Batch Loss: 0.5919710397720337\n",
      "Epoch 2279, Loss: 1.787104219198227, Final Batch Loss: 0.2783181071281433\n",
      "Epoch 2280, Loss: 1.8993190824985504, Final Batch Loss: 0.4732078015804291\n",
      "Epoch 2281, Loss: 1.7861768007278442, Final Batch Loss: 0.2500064969062805\n",
      "Epoch 2282, Loss: 1.8343461453914642, Final Batch Loss: 0.32537925243377686\n",
      "Epoch 2283, Loss: 1.9463801383972168, Final Batch Loss: 0.3137630522251129\n",
      "Epoch 2284, Loss: 1.875291645526886, Final Batch Loss: 0.37901002168655396\n",
      "Epoch 2285, Loss: 1.872143417596817, Final Batch Loss: 0.41197633743286133\n",
      "Epoch 2286, Loss: 1.7936009466648102, Final Batch Loss: 0.31911519169807434\n",
      "Epoch 2287, Loss: 1.7993637919425964, Final Batch Loss: 0.32110631465911865\n",
      "Epoch 2288, Loss: 1.7733880281448364, Final Batch Loss: 0.28637218475341797\n",
      "Epoch 2289, Loss: 1.8816152513027191, Final Batch Loss: 0.3682597875595093\n",
      "Epoch 2290, Loss: 1.785181850194931, Final Batch Loss: 0.3280393183231354\n",
      "Epoch 2291, Loss: 2.0009897649288177, Final Batch Loss: 0.3774501383304596\n",
      "Epoch 2292, Loss: 1.773778647184372, Final Batch Loss: 0.28377479314804077\n",
      "Epoch 2293, Loss: 2.1570132970809937, Final Batch Loss: 0.4299607276916504\n",
      "Epoch 2294, Loss: 1.8916756510734558, Final Batch Loss: 0.41786420345306396\n",
      "Epoch 2295, Loss: 1.823323905467987, Final Batch Loss: 0.2731892764568329\n",
      "Epoch 2296, Loss: 1.8417985141277313, Final Batch Loss: 0.34130457043647766\n",
      "Epoch 2297, Loss: 1.8663029074668884, Final Batch Loss: 0.3352662920951843\n",
      "Epoch 2298, Loss: 1.6689429581165314, Final Batch Loss: 0.34603044390678406\n",
      "Epoch 2299, Loss: 1.8385092318058014, Final Batch Loss: 0.38960695266723633\n",
      "Epoch 2300, Loss: 1.9011944234371185, Final Batch Loss: 0.35140153765678406\n",
      "Epoch 2301, Loss: 1.8484047949314117, Final Batch Loss: 0.35576382279396057\n",
      "Epoch 2302, Loss: 1.655542254447937, Final Batch Loss: 0.3911522328853607\n",
      "Epoch 2303, Loss: 1.8541204035282135, Final Batch Loss: 0.3971629738807678\n",
      "Epoch 2304, Loss: 1.8248307406902313, Final Batch Loss: 0.3747480809688568\n",
      "Epoch 2305, Loss: 2.0755876898765564, Final Batch Loss: 0.4763256013393402\n",
      "Epoch 2306, Loss: 1.786875158548355, Final Batch Loss: 0.2691080570220947\n",
      "Epoch 2307, Loss: 1.8889248371124268, Final Batch Loss: 0.42942845821380615\n",
      "Epoch 2308, Loss: 1.7513057887554169, Final Batch Loss: 0.3771427571773529\n",
      "Epoch 2309, Loss: 1.7515933811664581, Final Batch Loss: 0.25547710061073303\n",
      "Epoch 2310, Loss: 1.8272062838077545, Final Batch Loss: 0.3436813950538635\n",
      "Epoch 2311, Loss: 1.9304622411727905, Final Batch Loss: 0.5066247582435608\n",
      "Epoch 2312, Loss: 1.8951537013053894, Final Batch Loss: 0.4201279580593109\n",
      "Epoch 2313, Loss: 2.0369319319725037, Final Batch Loss: 0.5463507771492004\n",
      "Epoch 2314, Loss: 1.9695214629173279, Final Batch Loss: 0.3864416778087616\n",
      "Epoch 2315, Loss: 1.8730020821094513, Final Batch Loss: 0.38575413823127747\n",
      "Epoch 2316, Loss: 1.8596122562885284, Final Batch Loss: 0.2866309881210327\n",
      "Epoch 2317, Loss: 1.8358992040157318, Final Batch Loss: 0.35715892910957336\n",
      "Epoch 2318, Loss: 1.804092139005661, Final Batch Loss: 0.3146343231201172\n",
      "Epoch 2319, Loss: 1.7738049924373627, Final Batch Loss: 0.43951159715652466\n",
      "Epoch 2320, Loss: 1.7536986768245697, Final Batch Loss: 0.3671417832374573\n",
      "Epoch 2321, Loss: 2.006157636642456, Final Batch Loss: 0.490145742893219\n",
      "Epoch 2322, Loss: 1.794150710105896, Final Batch Loss: 0.3119945228099823\n",
      "Epoch 2323, Loss: 1.737153023481369, Final Batch Loss: 0.3667840361595154\n",
      "Epoch 2324, Loss: 1.7868388295173645, Final Batch Loss: 0.35805678367614746\n",
      "Epoch 2325, Loss: 1.804440438747406, Final Batch Loss: 0.3791775405406952\n",
      "Epoch 2326, Loss: 1.9000838100910187, Final Batch Loss: 0.454862117767334\n",
      "Epoch 2327, Loss: 1.7821805477142334, Final Batch Loss: 0.3000491261482239\n",
      "Epoch 2328, Loss: 1.7723524570465088, Final Batch Loss: 0.40491294860839844\n",
      "Epoch 2329, Loss: 1.9545615017414093, Final Batch Loss: 0.4480263292789459\n",
      "Epoch 2330, Loss: 1.7713143825531006, Final Batch Loss: 0.22224107384681702\n",
      "Epoch 2331, Loss: 1.8137613981962204, Final Batch Loss: 0.23553593456745148\n",
      "Epoch 2332, Loss: 1.8412268459796906, Final Batch Loss: 0.2783588171005249\n",
      "Epoch 2333, Loss: 1.8834172785282135, Final Batch Loss: 0.36210134625434875\n",
      "Epoch 2334, Loss: 1.951563149690628, Final Batch Loss: 0.4279387891292572\n",
      "Epoch 2335, Loss: 1.6780157089233398, Final Batch Loss: 0.2625991404056549\n",
      "Epoch 2336, Loss: 1.7592140138149261, Final Batch Loss: 0.2997208833694458\n",
      "Epoch 2337, Loss: 1.925389438867569, Final Batch Loss: 0.3868260681629181\n",
      "Epoch 2338, Loss: 2.039316773414612, Final Batch Loss: 0.4946637451648712\n",
      "Epoch 2339, Loss: 1.728726178407669, Final Batch Loss: 0.35898587107658386\n",
      "Epoch 2340, Loss: 1.791280210018158, Final Batch Loss: 0.3074866235256195\n",
      "Epoch 2341, Loss: 2.062490463256836, Final Batch Loss: 0.552757203578949\n",
      "Epoch 2342, Loss: 2.144303262233734, Final Batch Loss: 0.6664172410964966\n",
      "Epoch 2343, Loss: 1.8024181127548218, Final Batch Loss: 0.35022374987602234\n",
      "Epoch 2344, Loss: 1.8350689262151718, Final Batch Loss: 0.20086757838726044\n",
      "Epoch 2345, Loss: 1.8486929833889008, Final Batch Loss: 0.3007288873195648\n",
      "Epoch 2346, Loss: 1.8534505665302277, Final Batch Loss: 0.4512676000595093\n",
      "Epoch 2347, Loss: 1.9628200829029083, Final Batch Loss: 0.4657666087150574\n",
      "Epoch 2348, Loss: 1.966620296239853, Final Batch Loss: 0.4175238609313965\n",
      "Epoch 2349, Loss: 1.7167284786701202, Final Batch Loss: 0.2556161880493164\n",
      "Epoch 2350, Loss: 1.9492561221122742, Final Batch Loss: 0.3043991029262543\n",
      "Epoch 2351, Loss: 1.7466835975646973, Final Batch Loss: 0.24625363945960999\n",
      "Epoch 2352, Loss: 1.685849815607071, Final Batch Loss: 0.25137847661972046\n",
      "Epoch 2353, Loss: 1.9683368504047394, Final Batch Loss: 0.38050955533981323\n",
      "Epoch 2354, Loss: 1.7802313566207886, Final Batch Loss: 0.3392297029495239\n",
      "Epoch 2355, Loss: 1.7097868621349335, Final Batch Loss: 0.29332923889160156\n",
      "Epoch 2356, Loss: 2.0545497238636017, Final Batch Loss: 0.39549922943115234\n",
      "Epoch 2357, Loss: 1.8429404497146606, Final Batch Loss: 0.33262699842453003\n",
      "Epoch 2358, Loss: 1.910634070634842, Final Batch Loss: 0.3774857819080353\n",
      "Epoch 2359, Loss: 1.7418010830879211, Final Batch Loss: 0.44112345576286316\n",
      "Epoch 2360, Loss: 1.7717034220695496, Final Batch Loss: 0.33378323912620544\n",
      "Epoch 2361, Loss: 2.061522275209427, Final Batch Loss: 0.5019639134407043\n",
      "Epoch 2362, Loss: 1.914973795413971, Final Batch Loss: 0.44654232263565063\n",
      "Epoch 2363, Loss: 1.9675467014312744, Final Batch Loss: 0.4209038019180298\n",
      "Epoch 2364, Loss: 1.8002603948116302, Final Batch Loss: 0.3167760670185089\n",
      "Epoch 2365, Loss: 2.0559431612491608, Final Batch Loss: 0.4734455943107605\n",
      "Epoch 2366, Loss: 1.796383947134018, Final Batch Loss: 0.2874358296394348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2367, Loss: 1.8672474324703217, Final Batch Loss: 0.43233633041381836\n",
      "Epoch 2368, Loss: 1.7743390798568726, Final Batch Loss: 0.3195805251598358\n",
      "Epoch 2369, Loss: 1.7668188214302063, Final Batch Loss: 0.2873494029045105\n",
      "Epoch 2370, Loss: 1.8852846920490265, Final Batch Loss: 0.369344025850296\n",
      "Epoch 2371, Loss: 1.9551791548728943, Final Batch Loss: 0.445870578289032\n",
      "Epoch 2372, Loss: 1.6907066404819489, Final Batch Loss: 0.2564687728881836\n",
      "Epoch 2373, Loss: 1.9450165629386902, Final Batch Loss: 0.40684694051742554\n",
      "Epoch 2374, Loss: 1.7602893710136414, Final Batch Loss: 0.24053379893302917\n",
      "Epoch 2375, Loss: 1.846908152103424, Final Batch Loss: 0.4398999810218811\n",
      "Epoch 2376, Loss: 1.7580232173204422, Final Batch Loss: 0.20551751554012299\n",
      "Epoch 2377, Loss: 1.803022414445877, Final Batch Loss: 0.423556387424469\n",
      "Epoch 2378, Loss: 2.0322833359241486, Final Batch Loss: 0.4304848313331604\n",
      "Epoch 2379, Loss: 1.8246738016605377, Final Batch Loss: 0.3896971046924591\n",
      "Epoch 2380, Loss: 2.0097611248493195, Final Batch Loss: 0.4743892252445221\n",
      "Epoch 2381, Loss: 2.023490846157074, Final Batch Loss: 0.40133264660835266\n",
      "Epoch 2382, Loss: 1.6731432527303696, Final Batch Loss: 0.19889043271541595\n",
      "Epoch 2383, Loss: 1.7644266486167908, Final Batch Loss: 0.32377734780311584\n",
      "Epoch 2384, Loss: 1.6767646372318268, Final Batch Loss: 0.33820345997810364\n",
      "Epoch 2385, Loss: 1.8917024731636047, Final Batch Loss: 0.48228275775909424\n",
      "Epoch 2386, Loss: 1.7619701623916626, Final Batch Loss: 0.4498419463634491\n",
      "Epoch 2387, Loss: 1.9897401928901672, Final Batch Loss: 0.4993370771408081\n",
      "Epoch 2388, Loss: 1.8484980165958405, Final Batch Loss: 0.3175525963306427\n",
      "Epoch 2389, Loss: 1.9482010900974274, Final Batch Loss: 0.4650568664073944\n",
      "Epoch 2390, Loss: 1.838917464017868, Final Batch Loss: 0.3463422954082489\n",
      "Epoch 2391, Loss: 2.1163391172885895, Final Batch Loss: 0.5756638646125793\n",
      "Epoch 2392, Loss: 1.83675816655159, Final Batch Loss: 0.44616442918777466\n",
      "Epoch 2393, Loss: 1.8772015869617462, Final Batch Loss: 0.3437078595161438\n",
      "Epoch 2394, Loss: 1.9073605835437775, Final Batch Loss: 0.3453369736671448\n",
      "Epoch 2395, Loss: 1.7229317724704742, Final Batch Loss: 0.3864607512950897\n",
      "Epoch 2396, Loss: 1.8660053610801697, Final Batch Loss: 0.330640584230423\n",
      "Epoch 2397, Loss: 1.7571722269058228, Final Batch Loss: 0.28993135690689087\n",
      "Epoch 2398, Loss: 2.1311906278133392, Final Batch Loss: 0.3908875584602356\n",
      "Epoch 2399, Loss: 1.742648109793663, Final Batch Loss: 0.19577725231647491\n",
      "Epoch 2400, Loss: 1.8751980364322662, Final Batch Loss: 0.4179791510105133\n",
      "Epoch 2401, Loss: 2.0239091217517853, Final Batch Loss: 0.5127916932106018\n",
      "Epoch 2402, Loss: 1.7606688737869263, Final Batch Loss: 0.32886290550231934\n",
      "Epoch 2403, Loss: 1.6578525006771088, Final Batch Loss: 0.2618626058101654\n",
      "Epoch 2404, Loss: 1.9233227670192719, Final Batch Loss: 0.4703137278556824\n",
      "Epoch 2405, Loss: 1.6839773654937744, Final Batch Loss: 0.29474544525146484\n",
      "Epoch 2406, Loss: 1.875605970621109, Final Batch Loss: 0.31091374158859253\n",
      "Epoch 2407, Loss: 2.026736468076706, Final Batch Loss: 0.4156903326511383\n",
      "Epoch 2408, Loss: 1.8832361996173859, Final Batch Loss: 0.3977038860321045\n",
      "Epoch 2409, Loss: 1.7746029794216156, Final Batch Loss: 0.3038463890552521\n",
      "Epoch 2410, Loss: 1.714885652065277, Final Batch Loss: 0.2981927692890167\n",
      "Epoch 2411, Loss: 1.9810057878494263, Final Batch Loss: 0.331322580575943\n",
      "Epoch 2412, Loss: 1.94096040725708, Final Batch Loss: 0.23903435468673706\n",
      "Epoch 2413, Loss: 2.1924937963485718, Final Batch Loss: 0.6597505211830139\n",
      "Epoch 2414, Loss: 1.847874015569687, Final Batch Loss: 0.3490018844604492\n",
      "Epoch 2415, Loss: 1.8328811824321747, Final Batch Loss: 0.38150259852409363\n",
      "Epoch 2416, Loss: 1.858253389596939, Final Batch Loss: 0.35725030303001404\n",
      "Epoch 2417, Loss: 1.7423396408557892, Final Batch Loss: 0.28121617436408997\n",
      "Epoch 2418, Loss: 1.6676633059978485, Final Batch Loss: 0.22167646884918213\n",
      "Epoch 2419, Loss: 1.7426838874816895, Final Batch Loss: 0.2159547507762909\n",
      "Epoch 2420, Loss: 1.8127496838569641, Final Batch Loss: 0.4741866886615753\n",
      "Epoch 2421, Loss: 1.8190378546714783, Final Batch Loss: 0.4601474702358246\n",
      "Epoch 2422, Loss: 1.7386184334754944, Final Batch Loss: 0.27765899896621704\n",
      "Epoch 2423, Loss: 1.593769520521164, Final Batch Loss: 0.2593887448310852\n",
      "Epoch 2424, Loss: 1.6905517876148224, Final Batch Loss: 0.2501639723777771\n",
      "Epoch 2425, Loss: 1.7496350407600403, Final Batch Loss: 0.23712557554244995\n",
      "Epoch 2426, Loss: 1.7431128919124603, Final Batch Loss: 0.2611050605773926\n",
      "Epoch 2427, Loss: 1.9104831516742706, Final Batch Loss: 0.3989993929862976\n",
      "Epoch 2428, Loss: 1.7605253159999847, Final Batch Loss: 0.3556990325450897\n",
      "Epoch 2429, Loss: 1.7231363356113434, Final Batch Loss: 0.32768714427948\n",
      "Epoch 2430, Loss: 1.871168613433838, Final Batch Loss: 0.3707400858402252\n",
      "Epoch 2431, Loss: 1.9205548763275146, Final Batch Loss: 0.5421435236930847\n",
      "Epoch 2432, Loss: 1.8208656013011932, Final Batch Loss: 0.34911853075027466\n",
      "Epoch 2433, Loss: 1.7511559128761292, Final Batch Loss: 0.37666743993759155\n",
      "Epoch 2434, Loss: 1.982531726360321, Final Batch Loss: 0.3318793475627899\n",
      "Epoch 2435, Loss: 1.9275599420070648, Final Batch Loss: 0.4092338979244232\n",
      "Epoch 2436, Loss: 1.7669614255428314, Final Batch Loss: 0.3853466212749481\n",
      "Epoch 2437, Loss: 1.7122735977172852, Final Batch Loss: 0.2444852888584137\n",
      "Epoch 2438, Loss: 1.802908718585968, Final Batch Loss: 0.4301305115222931\n",
      "Epoch 2439, Loss: 1.8065977692604065, Final Batch Loss: 0.41267141699790955\n",
      "Epoch 2440, Loss: 1.916819155216217, Final Batch Loss: 0.37273526191711426\n",
      "Epoch 2441, Loss: 1.777620553970337, Final Batch Loss: 0.3657374083995819\n",
      "Epoch 2442, Loss: 1.8787333071231842, Final Batch Loss: 0.2923761010169983\n",
      "Epoch 2443, Loss: 1.8214726448059082, Final Batch Loss: 0.3875482380390167\n",
      "Epoch 2444, Loss: 1.8024176955223083, Final Batch Loss: 0.39604973793029785\n",
      "Epoch 2445, Loss: 1.7308173179626465, Final Batch Loss: 0.43239322304725647\n",
      "Epoch 2446, Loss: 1.9329178929328918, Final Batch Loss: 0.3351200520992279\n",
      "Epoch 2447, Loss: 1.976625144481659, Final Batch Loss: 0.40700095891952515\n",
      "Epoch 2448, Loss: 1.8846860229969025, Final Batch Loss: 0.43933728337287903\n",
      "Epoch 2449, Loss: 1.9960220456123352, Final Batch Loss: 0.4808080494403839\n",
      "Epoch 2450, Loss: 1.6571056842803955, Final Batch Loss: 0.34321337938308716\n",
      "Epoch 2451, Loss: 1.7217649817466736, Final Batch Loss: 0.35371333360671997\n",
      "Epoch 2452, Loss: 1.7522088289260864, Final Batch Loss: 0.3547075092792511\n",
      "Epoch 2453, Loss: 1.9558676183223724, Final Batch Loss: 0.3788168132305145\n",
      "Epoch 2454, Loss: 1.7923121452331543, Final Batch Loss: 0.3495216965675354\n",
      "Epoch 2455, Loss: 1.6759087443351746, Final Batch Loss: 0.2633051574230194\n",
      "Epoch 2456, Loss: 1.8102361261844635, Final Batch Loss: 0.30137401819229126\n",
      "Epoch 2457, Loss: 1.7806179821491241, Final Batch Loss: 0.34765955805778503\n",
      "Epoch 2458, Loss: 1.9079264998435974, Final Batch Loss: 0.27151477336883545\n",
      "Epoch 2459, Loss: 1.8648222386837006, Final Batch Loss: 0.3696402907371521\n",
      "Epoch 2460, Loss: 1.9690096378326416, Final Batch Loss: 0.4847939610481262\n",
      "Epoch 2461, Loss: 1.8445167243480682, Final Batch Loss: 0.33601292967796326\n",
      "Epoch 2462, Loss: 2.014279991388321, Final Batch Loss: 0.5637139678001404\n",
      "Epoch 2463, Loss: 1.6815816015005112, Final Batch Loss: 0.23883046209812164\n",
      "Epoch 2464, Loss: 2.114668756723404, Final Batch Loss: 0.5931141376495361\n",
      "Epoch 2465, Loss: 2.3041949570178986, Final Batch Loss: 0.8094488382339478\n",
      "Epoch 2466, Loss: 1.889977216720581, Final Batch Loss: 0.4409978687763214\n",
      "Epoch 2467, Loss: 1.627380907535553, Final Batch Loss: 0.2718893587589264\n",
      "Epoch 2468, Loss: 1.902409017086029, Final Batch Loss: 0.4774700999259949\n",
      "Epoch 2469, Loss: 2.1198723912239075, Final Batch Loss: 0.49654561281204224\n",
      "Epoch 2470, Loss: 1.9839204847812653, Final Batch Loss: 0.4390825629234314\n",
      "Epoch 2471, Loss: 1.9747356176376343, Final Batch Loss: 0.37484410405158997\n",
      "Epoch 2472, Loss: 1.9042057991027832, Final Batch Loss: 0.28593242168426514\n",
      "Epoch 2473, Loss: 1.8681813776493073, Final Batch Loss: 0.3460022509098053\n",
      "Epoch 2474, Loss: 1.872620314359665, Final Batch Loss: 0.4041202664375305\n",
      "Epoch 2475, Loss: 1.741902083158493, Final Batch Loss: 0.24001061916351318\n",
      "Epoch 2476, Loss: 1.8690020442008972, Final Batch Loss: 0.42057210206985474\n",
      "Epoch 2477, Loss: 1.8608398139476776, Final Batch Loss: 0.3520517945289612\n",
      "Epoch 2478, Loss: 1.840583473443985, Final Batch Loss: 0.39040622115135193\n",
      "Epoch 2479, Loss: 2.0728392601013184, Final Batch Loss: 0.46804308891296387\n",
      "Epoch 2480, Loss: 1.8093494772911072, Final Batch Loss: 0.29283225536346436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2481, Loss: 1.8435686528682709, Final Batch Loss: 0.42755207419395447\n",
      "Epoch 2482, Loss: 1.6343575567007065, Final Batch Loss: 0.20010174810886383\n",
      "Epoch 2483, Loss: 1.7649748027324677, Final Batch Loss: 0.3298516571521759\n",
      "Epoch 2484, Loss: 1.7637833952903748, Final Batch Loss: 0.378383994102478\n",
      "Epoch 2485, Loss: 1.9240573346614838, Final Batch Loss: 0.41400769352912903\n",
      "Epoch 2486, Loss: 1.839031994342804, Final Batch Loss: 0.4895022511482239\n",
      "Epoch 2487, Loss: 1.99100661277771, Final Batch Loss: 0.44998279213905334\n",
      "Epoch 2488, Loss: 1.7690600454807281, Final Batch Loss: 0.26740604639053345\n",
      "Epoch 2489, Loss: 1.770457774400711, Final Batch Loss: 0.29877543449401855\n",
      "Epoch 2490, Loss: 1.6588281095027924, Final Batch Loss: 0.21782538294792175\n",
      "Epoch 2491, Loss: 1.890314370393753, Final Batch Loss: 0.35070428252220154\n",
      "Epoch 2492, Loss: 1.8204866647720337, Final Batch Loss: 0.3284844756126404\n",
      "Epoch 2493, Loss: 1.6436764001846313, Final Batch Loss: 0.27315807342529297\n",
      "Epoch 2494, Loss: 1.8664346635341644, Final Batch Loss: 0.3681047558784485\n",
      "Epoch 2495, Loss: 1.9230706691741943, Final Batch Loss: 0.4182775318622589\n",
      "Epoch 2496, Loss: 1.7438247501850128, Final Batch Loss: 0.3270256817340851\n",
      "Epoch 2497, Loss: 1.75772824883461, Final Batch Loss: 0.29345399141311646\n",
      "Epoch 2498, Loss: 1.9028614163398743, Final Batch Loss: 0.48549598455429077\n",
      "Epoch 2499, Loss: 1.905712217092514, Final Batch Loss: 0.4217125475406647\n",
      "Epoch 2500, Loss: 1.6599883437156677, Final Batch Loss: 0.32227542996406555\n",
      "Epoch 2501, Loss: 1.6266875863075256, Final Batch Loss: 0.2672600746154785\n",
      "Epoch 2502, Loss: 1.6097739338874817, Final Batch Loss: 0.2889631390571594\n",
      "Epoch 2503, Loss: 1.6268695145845413, Final Batch Loss: 0.2108522206544876\n",
      "Epoch 2504, Loss: 1.7796880900859833, Final Batch Loss: 0.3945419192314148\n",
      "Epoch 2505, Loss: 1.6962611377239227, Final Batch Loss: 0.2810244858264923\n",
      "Epoch 2506, Loss: 1.8270467519760132, Final Batch Loss: 0.3510473966598511\n",
      "Epoch 2507, Loss: 1.855401188135147, Final Batch Loss: 0.3391490578651428\n",
      "Epoch 2508, Loss: 1.9239530563354492, Final Batch Loss: 0.4622900187969208\n",
      "Epoch 2509, Loss: 1.757095068693161, Final Batch Loss: 0.32551810145378113\n",
      "Epoch 2510, Loss: 1.9630641639232635, Final Batch Loss: 0.5124253630638123\n",
      "Epoch 2511, Loss: 1.8653028905391693, Final Batch Loss: 0.41949254274368286\n",
      "Epoch 2512, Loss: 1.651203215122223, Final Batch Loss: 0.27009838819503784\n",
      "Epoch 2513, Loss: 1.936245173215866, Final Batch Loss: 0.37061169743537903\n",
      "Epoch 2514, Loss: 1.8574938476085663, Final Batch Loss: 0.5370758175849915\n",
      "Epoch 2515, Loss: 1.8795711696147919, Final Batch Loss: 0.28620022535324097\n",
      "Epoch 2516, Loss: 1.93728905916214, Final Batch Loss: 0.3478778302669525\n",
      "Epoch 2517, Loss: 1.7405026257038116, Final Batch Loss: 0.3240106999874115\n",
      "Epoch 2518, Loss: 1.922020047903061, Final Batch Loss: 0.27856191992759705\n",
      "Epoch 2519, Loss: 1.7463295012712479, Final Batch Loss: 0.4176364839076996\n",
      "Epoch 2520, Loss: 1.67428058385849, Final Batch Loss: 0.3634878993034363\n",
      "Epoch 2521, Loss: 1.773987203836441, Final Batch Loss: 0.3099037706851959\n",
      "Epoch 2522, Loss: 2.0605495274066925, Final Batch Loss: 0.3067675828933716\n",
      "Epoch 2523, Loss: 2.02656352519989, Final Batch Loss: 0.4022804796695709\n",
      "Epoch 2524, Loss: 1.8746072947978973, Final Batch Loss: 0.391316294670105\n",
      "Epoch 2525, Loss: 1.9796591103076935, Final Batch Loss: 0.6075254678726196\n",
      "Epoch 2526, Loss: 2.060855358839035, Final Batch Loss: 0.55229252576828\n",
      "Epoch 2527, Loss: 1.8326680660247803, Final Batch Loss: 0.35564517974853516\n",
      "Epoch 2528, Loss: 2.036813974380493, Final Batch Loss: 0.47264689207077026\n",
      "Epoch 2529, Loss: 1.6256746500730515, Final Batch Loss: 0.21158696711063385\n",
      "Epoch 2530, Loss: 1.8363829851150513, Final Batch Loss: 0.2663692235946655\n",
      "Epoch 2531, Loss: 2.1568540930747986, Final Batch Loss: 0.7080392837524414\n",
      "Epoch 2532, Loss: 1.837622970342636, Final Batch Loss: 0.27148035168647766\n",
      "Epoch 2533, Loss: 2.15842342376709, Final Batch Loss: 0.4383522570133209\n",
      "Epoch 2534, Loss: 1.876387506723404, Final Batch Loss: 0.368726521730423\n",
      "Epoch 2535, Loss: 1.6524491906166077, Final Batch Loss: 0.21701842546463013\n",
      "Epoch 2536, Loss: 1.7529197335243225, Final Batch Loss: 0.21795862913131714\n",
      "Epoch 2537, Loss: 1.8443132936954498, Final Batch Loss: 0.438165545463562\n",
      "Epoch 2538, Loss: 1.961894690990448, Final Batch Loss: 0.3793051838874817\n",
      "Epoch 2539, Loss: 1.5795294046401978, Final Batch Loss: 0.246522456407547\n",
      "Epoch 2540, Loss: 1.9231624901294708, Final Batch Loss: 0.36552655696868896\n",
      "Epoch 2541, Loss: 1.8336829841136932, Final Batch Loss: 0.38835081458091736\n",
      "Epoch 2542, Loss: 1.9212676286697388, Final Batch Loss: 0.4121846854686737\n",
      "Epoch 2543, Loss: 2.000350594520569, Final Batch Loss: 0.3820277154445648\n",
      "Epoch 2544, Loss: 1.954165279865265, Final Batch Loss: 0.6089550256729126\n",
      "Epoch 2545, Loss: 1.7287473678588867, Final Batch Loss: 0.22165295481681824\n",
      "Epoch 2546, Loss: 1.9102650582790375, Final Batch Loss: 0.49682003259658813\n",
      "Epoch 2547, Loss: 1.7186016738414764, Final Batch Loss: 0.3345564901828766\n",
      "Epoch 2548, Loss: 1.7419322729110718, Final Batch Loss: 0.2268175184726715\n",
      "Epoch 2549, Loss: 1.8509582579135895, Final Batch Loss: 0.38182133436203003\n",
      "Epoch 2550, Loss: 1.9135461747646332, Final Batch Loss: 0.40259501338005066\n",
      "Epoch 2551, Loss: 1.8529554307460785, Final Batch Loss: 0.36878064274787903\n",
      "Epoch 2552, Loss: 1.796169936656952, Final Batch Loss: 0.31694668531417847\n",
      "Epoch 2553, Loss: 1.666597157716751, Final Batch Loss: 0.2527710199356079\n",
      "Epoch 2554, Loss: 1.8499632477760315, Final Batch Loss: 0.33975109457969666\n",
      "Epoch 2555, Loss: 1.872622698545456, Final Batch Loss: 0.3903884291648865\n",
      "Epoch 2556, Loss: 1.9187315702438354, Final Batch Loss: 0.30990296602249146\n",
      "Epoch 2557, Loss: 1.8331964910030365, Final Batch Loss: 0.2786673903465271\n",
      "Epoch 2558, Loss: 1.7813133597373962, Final Batch Loss: 0.3624124228954315\n",
      "Epoch 2559, Loss: 1.9184257686138153, Final Batch Loss: 0.48546841740608215\n",
      "Epoch 2560, Loss: 1.9803067743778229, Final Batch Loss: 0.5910941362380981\n",
      "Epoch 2561, Loss: 1.6839920580387115, Final Batch Loss: 0.2749345004558563\n",
      "Epoch 2562, Loss: 1.7815287709236145, Final Batch Loss: 0.3738359212875366\n",
      "Epoch 2563, Loss: 1.6976217031478882, Final Batch Loss: 0.28438156843185425\n",
      "Epoch 2564, Loss: 1.7023908495903015, Final Batch Loss: 0.28926798701286316\n",
      "Epoch 2565, Loss: 1.6917283236980438, Final Batch Loss: 0.42140063643455505\n",
      "Epoch 2566, Loss: 1.6361703276634216, Final Batch Loss: 0.26128557324409485\n",
      "Epoch 2567, Loss: 1.6737626492977142, Final Batch Loss: 0.2568405866622925\n",
      "Epoch 2568, Loss: 1.6620622724294662, Final Batch Loss: 0.20266859233379364\n",
      "Epoch 2569, Loss: 1.8373084664344788, Final Batch Loss: 0.32428282499313354\n",
      "Epoch 2570, Loss: 1.7365656793117523, Final Batch Loss: 0.3232528567314148\n",
      "Epoch 2571, Loss: 1.8333429098129272, Final Batch Loss: 0.4223952293395996\n",
      "Epoch 2572, Loss: 1.7772161066532135, Final Batch Loss: 0.30731192231178284\n",
      "Epoch 2573, Loss: 1.9736110866069794, Final Batch Loss: 0.5551953315734863\n",
      "Epoch 2574, Loss: 1.6956688165664673, Final Batch Loss: 0.2998221218585968\n",
      "Epoch 2575, Loss: 1.9940259754657745, Final Batch Loss: 0.49777019023895264\n",
      "Epoch 2576, Loss: 1.8294522762298584, Final Batch Loss: 0.42907723784446716\n",
      "Epoch 2577, Loss: 1.8996370732784271, Final Batch Loss: 0.45464175939559937\n",
      "Epoch 2578, Loss: 1.8818306922912598, Final Batch Loss: 0.3431357443332672\n",
      "Epoch 2579, Loss: 1.7776057422161102, Final Batch Loss: 0.34216487407684326\n",
      "Epoch 2580, Loss: 1.8123407065868378, Final Batch Loss: 0.41684070229530334\n",
      "Epoch 2581, Loss: 1.977404624223709, Final Batch Loss: 0.4060686528682709\n",
      "Epoch 2582, Loss: 1.9074255526065826, Final Batch Loss: 0.31315481662750244\n",
      "Epoch 2583, Loss: 1.7706078886985779, Final Batch Loss: 0.34590116143226624\n",
      "Epoch 2584, Loss: 1.8362988531589508, Final Batch Loss: 0.3020820617675781\n",
      "Epoch 2585, Loss: 1.8030832707881927, Final Batch Loss: 0.3585231900215149\n",
      "Epoch 2586, Loss: 1.879343330860138, Final Batch Loss: 0.33453094959259033\n",
      "Epoch 2587, Loss: 1.7291219532489777, Final Batch Loss: 0.35568058490753174\n",
      "Epoch 2588, Loss: 1.6636579036712646, Final Batch Loss: 0.3115798234939575\n",
      "Epoch 2589, Loss: 1.786629319190979, Final Batch Loss: 0.300789475440979\n",
      "Epoch 2590, Loss: 1.928932785987854, Final Batch Loss: 0.39744117856025696\n",
      "Epoch 2591, Loss: 1.8837169110774994, Final Batch Loss: 0.30719658732414246\n",
      "Epoch 2592, Loss: 1.7320722490549088, Final Batch Loss: 0.24485532939434052\n",
      "Epoch 2593, Loss: 1.9608119130134583, Final Batch Loss: 0.37682947516441345\n",
      "Epoch 2594, Loss: 1.8081589043140411, Final Batch Loss: 0.3109358847141266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2595, Loss: 1.7523002624511719, Final Batch Loss: 0.27757278084754944\n",
      "Epoch 2596, Loss: 1.9203289449214935, Final Batch Loss: 0.4714290499687195\n",
      "Epoch 2597, Loss: 1.8504100143909454, Final Batch Loss: 0.3080292344093323\n",
      "Epoch 2598, Loss: 1.854346752166748, Final Batch Loss: 0.36952173709869385\n",
      "Epoch 2599, Loss: 1.8294655978679657, Final Batch Loss: 0.5269002318382263\n",
      "Epoch 2600, Loss: 1.6993182301521301, Final Batch Loss: 0.267169326543808\n",
      "Epoch 2601, Loss: 2.0698771476745605, Final Batch Loss: 0.44239410758018494\n",
      "Epoch 2602, Loss: 1.8170399963855743, Final Batch Loss: 0.3352205455303192\n",
      "Epoch 2603, Loss: 1.8748127222061157, Final Batch Loss: 0.3911570608615875\n",
      "Epoch 2604, Loss: 2.001909226179123, Final Batch Loss: 0.5281347036361694\n",
      "Epoch 2605, Loss: 1.7381528913974762, Final Batch Loss: 0.29746419191360474\n",
      "Epoch 2606, Loss: 1.8262462317943573, Final Batch Loss: 0.3148963153362274\n",
      "Epoch 2607, Loss: 1.7704696655273438, Final Batch Loss: 0.36907869577407837\n",
      "Epoch 2608, Loss: 1.8260490000247955, Final Batch Loss: 0.4216020703315735\n",
      "Epoch 2609, Loss: 1.6532466113567352, Final Batch Loss: 0.22313296794891357\n",
      "Epoch 2610, Loss: 1.6407414376735687, Final Batch Loss: 0.2683689296245575\n",
      "Epoch 2611, Loss: 1.6858596801757812, Final Batch Loss: 0.3021927773952484\n",
      "Epoch 2612, Loss: 1.7606189250946045, Final Batch Loss: 0.4693416655063629\n",
      "Epoch 2613, Loss: 1.7733561992645264, Final Batch Loss: 0.32995033264160156\n",
      "Epoch 2614, Loss: 1.7456732988357544, Final Batch Loss: 0.35112109780311584\n",
      "Epoch 2615, Loss: 1.7759904265403748, Final Batch Loss: 0.33097586035728455\n",
      "Epoch 2616, Loss: 1.7023839056491852, Final Batch Loss: 0.31308528780937195\n",
      "Epoch 2617, Loss: 1.676571547985077, Final Batch Loss: 0.31153324246406555\n",
      "Epoch 2618, Loss: 1.6730696558952332, Final Batch Loss: 0.30017098784446716\n",
      "Epoch 2619, Loss: 2.229170113801956, Final Batch Loss: 0.5785004496574402\n",
      "Epoch 2620, Loss: 1.9127415418624878, Final Batch Loss: 0.3450240194797516\n",
      "Epoch 2621, Loss: 1.8516394793987274, Final Batch Loss: 0.3203386962413788\n",
      "Epoch 2622, Loss: 1.7789173126220703, Final Batch Loss: 0.31162887811660767\n",
      "Epoch 2623, Loss: 1.8897818326950073, Final Batch Loss: 0.40227246284484863\n",
      "Epoch 2624, Loss: 1.6302059590816498, Final Batch Loss: 0.2768426239490509\n",
      "Epoch 2625, Loss: 1.8531866371631622, Final Batch Loss: 0.34118497371673584\n",
      "Epoch 2626, Loss: 2.0846065878868103, Final Batch Loss: 0.4448889195919037\n",
      "Epoch 2627, Loss: 1.863802969455719, Final Batch Loss: 0.4433298110961914\n",
      "Epoch 2628, Loss: 1.945604532957077, Final Batch Loss: 0.4691248834133148\n",
      "Epoch 2629, Loss: 1.6752919554710388, Final Batch Loss: 0.2758866846561432\n",
      "Epoch 2630, Loss: 1.9067162871360779, Final Batch Loss: 0.29901963472366333\n",
      "Epoch 2631, Loss: 1.9325348138809204, Final Batch Loss: 0.3558548390865326\n",
      "Epoch 2632, Loss: 1.7343182563781738, Final Batch Loss: 0.331402987241745\n",
      "Epoch 2633, Loss: 1.8320364654064178, Final Batch Loss: 0.40147921442985535\n",
      "Epoch 2634, Loss: 2.0434211790561676, Final Batch Loss: 0.640168309211731\n",
      "Epoch 2635, Loss: 1.909978300333023, Final Batch Loss: 0.3808877766132355\n",
      "Epoch 2636, Loss: 1.8153435587882996, Final Batch Loss: 0.4123249351978302\n",
      "Epoch 2637, Loss: 1.9074048399925232, Final Batch Loss: 0.45654362440109253\n",
      "Epoch 2638, Loss: 1.867054671049118, Final Batch Loss: 0.31443434953689575\n",
      "Epoch 2639, Loss: 1.8247843384742737, Final Batch Loss: 0.2804722189903259\n",
      "Epoch 2640, Loss: 1.8590085208415985, Final Batch Loss: 0.4273609220981598\n",
      "Epoch 2641, Loss: 1.8190922737121582, Final Batch Loss: 0.3557528257369995\n",
      "Epoch 2642, Loss: 1.810044288635254, Final Batch Loss: 0.3341590166091919\n",
      "Epoch 2643, Loss: 1.7097930312156677, Final Batch Loss: 0.30727601051330566\n",
      "Epoch 2644, Loss: 2.039297193288803, Final Batch Loss: 0.43775078654289246\n",
      "Epoch 2645, Loss: 1.728723168373108, Final Batch Loss: 0.33253517746925354\n",
      "Epoch 2646, Loss: 1.8331151902675629, Final Batch Loss: 0.3941168189048767\n",
      "Epoch 2647, Loss: 1.839933454990387, Final Batch Loss: 0.4355272054672241\n",
      "Epoch 2648, Loss: 1.8247544169425964, Final Batch Loss: 0.35121726989746094\n",
      "Epoch 2649, Loss: 1.716195523738861, Final Batch Loss: 0.30280616879463196\n",
      "Epoch 2650, Loss: 2.0236066579818726, Final Batch Loss: 0.4574814438819885\n",
      "Epoch 2651, Loss: 1.9561516046524048, Final Batch Loss: 0.439223051071167\n",
      "Epoch 2652, Loss: 2.02983096241951, Final Batch Loss: 0.44809916615486145\n",
      "Epoch 2653, Loss: 2.003430098295212, Final Batch Loss: 0.5310506820678711\n",
      "Epoch 2654, Loss: 1.8729006946086884, Final Batch Loss: 0.3405781090259552\n",
      "Epoch 2655, Loss: 1.8555025309324265, Final Batch Loss: 0.21423496305942535\n",
      "Epoch 2656, Loss: 1.7435119152069092, Final Batch Loss: 0.2703727185726166\n",
      "Epoch 2657, Loss: 1.7924582064151764, Final Batch Loss: 0.3735560178756714\n",
      "Epoch 2658, Loss: 1.6245402097702026, Final Batch Loss: 0.2722930908203125\n",
      "Epoch 2659, Loss: 1.7785693407058716, Final Batch Loss: 0.3423181474208832\n",
      "Epoch 2660, Loss: 1.8271558582782745, Final Batch Loss: 0.3942204713821411\n",
      "Epoch 2661, Loss: 1.743678480386734, Final Batch Loss: 0.2524595558643341\n",
      "Epoch 2662, Loss: 1.8250094950199127, Final Batch Loss: 0.42080503702163696\n",
      "Epoch 2663, Loss: 1.8407021164894104, Final Batch Loss: 0.38793259859085083\n",
      "Epoch 2664, Loss: 1.835894137620926, Final Batch Loss: 0.344651460647583\n",
      "Epoch 2665, Loss: 2.006556272506714, Final Batch Loss: 0.6427616477012634\n",
      "Epoch 2666, Loss: 1.774464100599289, Final Batch Loss: 0.4381372928619385\n",
      "Epoch 2667, Loss: 2.0615480542182922, Final Batch Loss: 0.43945515155792236\n",
      "Epoch 2668, Loss: 1.7144644856452942, Final Batch Loss: 0.2896166443824768\n",
      "Epoch 2669, Loss: 1.741675078868866, Final Batch Loss: 0.3126212954521179\n",
      "Epoch 2670, Loss: 1.7510422766208649, Final Batch Loss: 0.35789674520492554\n",
      "Epoch 2671, Loss: 1.7215077877044678, Final Batch Loss: 0.315591424703598\n",
      "Epoch 2672, Loss: 1.7351606786251068, Final Batch Loss: 0.4104130268096924\n",
      "Epoch 2673, Loss: 1.7736046314239502, Final Batch Loss: 0.36519092321395874\n",
      "Epoch 2674, Loss: 1.703696995973587, Final Batch Loss: 0.34332385659217834\n",
      "Epoch 2675, Loss: 1.8817074298858643, Final Batch Loss: 0.42095139622688293\n",
      "Epoch 2676, Loss: 1.646182894706726, Final Batch Loss: 0.3152371048927307\n",
      "Epoch 2677, Loss: 1.7708435654640198, Final Batch Loss: 0.41035768389701843\n",
      "Epoch 2678, Loss: 2.229553997516632, Final Batch Loss: 0.5707911849021912\n",
      "Epoch 2679, Loss: 1.6433761715888977, Final Batch Loss: 0.29539424180984497\n",
      "Epoch 2680, Loss: 2.1237988770008087, Final Batch Loss: 0.7751166224479675\n",
      "Epoch 2681, Loss: 1.7659125924110413, Final Batch Loss: 0.22528809309005737\n",
      "Epoch 2682, Loss: 1.9368817806243896, Final Batch Loss: 0.40722593665122986\n",
      "Epoch 2683, Loss: 1.7639162242412567, Final Batch Loss: 0.3302678167819977\n",
      "Epoch 2684, Loss: 1.770502656698227, Final Batch Loss: 0.3180627226829529\n",
      "Epoch 2685, Loss: 1.7703097462654114, Final Batch Loss: 0.30052971839904785\n",
      "Epoch 2686, Loss: 2.0091370046138763, Final Batch Loss: 0.3974151015281677\n",
      "Epoch 2687, Loss: 1.8794751465320587, Final Batch Loss: 0.379519522190094\n",
      "Epoch 2688, Loss: 1.65109583735466, Final Batch Loss: 0.37579917907714844\n",
      "Epoch 2689, Loss: 1.7936165630817413, Final Batch Loss: 0.326130211353302\n",
      "Epoch 2690, Loss: 1.8191814124584198, Final Batch Loss: 0.39908042550086975\n",
      "Epoch 2691, Loss: 1.7215519547462463, Final Batch Loss: 0.23475149273872375\n",
      "Epoch 2692, Loss: 1.7673128247261047, Final Batch Loss: 0.3815062642097473\n",
      "Epoch 2693, Loss: 1.8714671730995178, Final Batch Loss: 0.4434358775615692\n",
      "Epoch 2694, Loss: 1.813975214958191, Final Batch Loss: 0.3220193684101105\n",
      "Epoch 2695, Loss: 1.739159733057022, Final Batch Loss: 0.3124801218509674\n",
      "Epoch 2696, Loss: 1.8622329533100128, Final Batch Loss: 0.45248284935951233\n",
      "Epoch 2697, Loss: 1.8848011791706085, Final Batch Loss: 0.28441232442855835\n",
      "Epoch 2698, Loss: 1.78801628947258, Final Batch Loss: 0.3772735297679901\n",
      "Epoch 2699, Loss: 1.9516706466674805, Final Batch Loss: 0.5411636233329773\n",
      "Epoch 2700, Loss: 1.815780371427536, Final Batch Loss: 0.32331863045692444\n",
      "Epoch 2701, Loss: 1.8781075477600098, Final Batch Loss: 0.26222822070121765\n",
      "Epoch 2702, Loss: 1.826482743024826, Final Batch Loss: 0.34798136353492737\n",
      "Epoch 2703, Loss: 1.90017968416214, Final Batch Loss: 0.33072274923324585\n",
      "Epoch 2704, Loss: 1.8402466475963593, Final Batch Loss: 0.36093708872795105\n",
      "Epoch 2705, Loss: 1.866085946559906, Final Batch Loss: 0.48074737191200256\n",
      "Epoch 2706, Loss: 1.7378274649381638, Final Batch Loss: 0.22662155330181122\n",
      "Epoch 2707, Loss: 1.8162671327590942, Final Batch Loss: 0.31025925278663635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2708, Loss: 1.7338224649429321, Final Batch Loss: 0.35241514444351196\n",
      "Epoch 2709, Loss: 1.9247954189777374, Final Batch Loss: 0.3071446120738983\n",
      "Epoch 2710, Loss: 1.8310668766498566, Final Batch Loss: 0.43818730115890503\n",
      "Epoch 2711, Loss: 1.9262590408325195, Final Batch Loss: 0.398702472448349\n",
      "Epoch 2712, Loss: 1.7935323119163513, Final Batch Loss: 0.33078858256340027\n",
      "Epoch 2713, Loss: 1.8032852709293365, Final Batch Loss: 0.41062498092651367\n",
      "Epoch 2714, Loss: 1.8428291380405426, Final Batch Loss: 0.45916876196861267\n",
      "Epoch 2715, Loss: 2.105170875787735, Final Batch Loss: 0.5674096941947937\n",
      "Epoch 2716, Loss: 1.7243765890598297, Final Batch Loss: 0.2693333625793457\n",
      "Epoch 2717, Loss: 1.8847812414169312, Final Batch Loss: 0.4192761480808258\n",
      "Epoch 2718, Loss: 1.8669048249721527, Final Batch Loss: 0.33221614360809326\n",
      "Epoch 2719, Loss: 1.9462656676769257, Final Batch Loss: 0.4042177200317383\n",
      "Epoch 2720, Loss: 1.8470325469970703, Final Batch Loss: 0.28527945280075073\n",
      "Epoch 2721, Loss: 1.686172291636467, Final Batch Loss: 0.22761060297489166\n",
      "Epoch 2722, Loss: 1.7845890522003174, Final Batch Loss: 0.3978113830089569\n",
      "Epoch 2723, Loss: 1.8888106048107147, Final Batch Loss: 0.4687984585762024\n",
      "Epoch 2724, Loss: 1.9336613416671753, Final Batch Loss: 0.5393528342247009\n",
      "Epoch 2725, Loss: 1.5901028513908386, Final Batch Loss: 0.24969860911369324\n",
      "Epoch 2726, Loss: 1.7650794088840485, Final Batch Loss: 0.40574079751968384\n",
      "Epoch 2727, Loss: 1.7979022860527039, Final Batch Loss: 0.38070768117904663\n",
      "Epoch 2728, Loss: 1.8459322452545166, Final Batch Loss: 0.4277594983577728\n",
      "Epoch 2729, Loss: 1.6255324929952621, Final Batch Loss: 0.2181156426668167\n",
      "Epoch 2730, Loss: 2.1646488308906555, Final Batch Loss: 0.6739997267723083\n",
      "Epoch 2731, Loss: 1.8011523187160492, Final Batch Loss: 0.2505097985267639\n",
      "Epoch 2732, Loss: 1.94095179438591, Final Batch Loss: 0.4223206639289856\n",
      "Epoch 2733, Loss: 1.7231755554676056, Final Batch Loss: 0.31519466638565063\n",
      "Epoch 2734, Loss: 1.971352756023407, Final Batch Loss: 0.5177845358848572\n",
      "Epoch 2735, Loss: 1.8441407680511475, Final Batch Loss: 0.4135921001434326\n",
      "Epoch 2736, Loss: 1.5907868444919586, Final Batch Loss: 0.2662195563316345\n",
      "Epoch 2737, Loss: 1.6935831308364868, Final Batch Loss: 0.28445905447006226\n",
      "Epoch 2738, Loss: 1.5220758318901062, Final Batch Loss: 0.26019081473350525\n",
      "Epoch 2739, Loss: 1.7882287502288818, Final Batch Loss: 0.3445045053958893\n",
      "Epoch 2740, Loss: 1.7982451915740967, Final Batch Loss: 0.20945799350738525\n",
      "Epoch 2741, Loss: 1.7495605647563934, Final Batch Loss: 0.4135607182979584\n",
      "Epoch 2742, Loss: 1.6639474928379059, Final Batch Loss: 0.3048231601715088\n",
      "Epoch 2743, Loss: 1.947662889957428, Final Batch Loss: 0.48440155386924744\n",
      "Epoch 2744, Loss: 1.8545268177986145, Final Batch Loss: 0.4228614270687103\n",
      "Epoch 2745, Loss: 1.711885154247284, Final Batch Loss: 0.24510660767555237\n",
      "Epoch 2746, Loss: 1.7753338515758514, Final Batch Loss: 0.29377973079681396\n",
      "Epoch 2747, Loss: 1.9728243350982666, Final Batch Loss: 0.3975844085216522\n",
      "Epoch 2748, Loss: 1.7728749513626099, Final Batch Loss: 0.363799124956131\n",
      "Epoch 2749, Loss: 1.8674075305461884, Final Batch Loss: 0.33667466044425964\n",
      "Epoch 2750, Loss: 1.686605989933014, Final Batch Loss: 0.3629153370857239\n",
      "Epoch 2751, Loss: 1.8570123314857483, Final Batch Loss: 0.499576210975647\n",
      "Epoch 2752, Loss: 2.1475336253643036, Final Batch Loss: 0.5849319696426392\n",
      "Epoch 2753, Loss: 1.8460295498371124, Final Batch Loss: 0.47380006313323975\n",
      "Epoch 2754, Loss: 1.8867298662662506, Final Batch Loss: 0.42045944929122925\n",
      "Epoch 2755, Loss: 1.7122658491134644, Final Batch Loss: 0.3729754388332367\n",
      "Epoch 2756, Loss: 1.8114147782325745, Final Batch Loss: 0.40829044580459595\n",
      "Epoch 2757, Loss: 1.5651163905858994, Final Batch Loss: 0.15930722653865814\n",
      "Epoch 2758, Loss: 1.8147645592689514, Final Batch Loss: 0.31722480058670044\n",
      "Epoch 2759, Loss: 1.5956490486860275, Final Batch Loss: 0.2191707044839859\n",
      "Epoch 2760, Loss: 1.746635526418686, Final Batch Loss: 0.33509039878845215\n",
      "Epoch 2761, Loss: 1.8755992650985718, Final Batch Loss: 0.413882851600647\n",
      "Epoch 2762, Loss: 1.7731868028640747, Final Batch Loss: 0.30788251757621765\n",
      "Epoch 2763, Loss: 1.8651954233646393, Final Batch Loss: 0.3197498619556427\n",
      "Epoch 2764, Loss: 1.7450219094753265, Final Batch Loss: 0.37431544065475464\n",
      "Epoch 2765, Loss: 1.7379092574119568, Final Batch Loss: 0.34011542797088623\n",
      "Epoch 2766, Loss: 1.7337389588356018, Final Batch Loss: 0.3591150641441345\n",
      "Epoch 2767, Loss: 1.8668798804283142, Final Batch Loss: 0.33243343234062195\n",
      "Epoch 2768, Loss: 1.8258672058582306, Final Batch Loss: 0.3813224732875824\n",
      "Epoch 2769, Loss: 1.7857344150543213, Final Batch Loss: 0.2597399353981018\n",
      "Epoch 2770, Loss: 1.8244176506996155, Final Batch Loss: 0.38499265909194946\n",
      "Epoch 2771, Loss: 1.7559080719947815, Final Batch Loss: 0.38023921847343445\n",
      "Epoch 2772, Loss: 1.9370698928833008, Final Batch Loss: 0.38284650444984436\n",
      "Epoch 2773, Loss: 1.974526047706604, Final Batch Loss: 0.537932276725769\n",
      "Epoch 2774, Loss: 1.86155766248703, Final Batch Loss: 0.3886016607284546\n",
      "Epoch 2775, Loss: 1.8817618489265442, Final Batch Loss: 0.48674848675727844\n",
      "Epoch 2776, Loss: 1.7731235921382904, Final Batch Loss: 0.4120226502418518\n",
      "Epoch 2777, Loss: 1.9516398906707764, Final Batch Loss: 0.4269653856754303\n",
      "Epoch 2778, Loss: 1.85177880525589, Final Batch Loss: 0.3825356960296631\n",
      "Epoch 2779, Loss: 1.7573218047618866, Final Batch Loss: 0.3648054599761963\n",
      "Epoch 2780, Loss: 1.810418725013733, Final Batch Loss: 0.4444030523300171\n",
      "Epoch 2781, Loss: 1.8687388002872467, Final Batch Loss: 0.4868541657924652\n",
      "Epoch 2782, Loss: 1.8685088753700256, Final Batch Loss: 0.4080112874507904\n",
      "Epoch 2783, Loss: 1.70129856467247, Final Batch Loss: 0.3877943754196167\n",
      "Epoch 2784, Loss: 1.7230668365955353, Final Batch Loss: 0.2778375446796417\n",
      "Epoch 2785, Loss: 1.6697537451982498, Final Batch Loss: 0.1962452083826065\n",
      "Epoch 2786, Loss: 1.8683500587940216, Final Batch Loss: 0.3303705155849457\n",
      "Epoch 2787, Loss: 1.8267790377140045, Final Batch Loss: 0.5566489696502686\n",
      "Epoch 2788, Loss: 1.8234685361385345, Final Batch Loss: 0.35922548174858093\n",
      "Epoch 2789, Loss: 1.8096466362476349, Final Batch Loss: 0.4405357539653778\n",
      "Epoch 2790, Loss: 1.8882126212120056, Final Batch Loss: 0.3530651926994324\n",
      "Epoch 2791, Loss: 1.8048507422208786, Final Batch Loss: 0.23566119372844696\n",
      "Epoch 2792, Loss: 1.8501565158367157, Final Batch Loss: 0.45837417244911194\n",
      "Epoch 2793, Loss: 1.794690579175949, Final Batch Loss: 0.36423638463020325\n",
      "Epoch 2794, Loss: 1.9673560559749603, Final Batch Loss: 0.5313814282417297\n",
      "Epoch 2795, Loss: 2.2314682602882385, Final Batch Loss: 0.541985809803009\n",
      "Epoch 2796, Loss: 1.723573088645935, Final Batch Loss: 0.31504637002944946\n",
      "Epoch 2797, Loss: 1.8479917645454407, Final Batch Loss: 0.3383368253707886\n",
      "Epoch 2798, Loss: 1.7153449356555939, Final Batch Loss: 0.29323485493659973\n",
      "Epoch 2799, Loss: 1.7012862265110016, Final Batch Loss: 0.30973294377326965\n",
      "Epoch 2800, Loss: 1.8957883417606354, Final Batch Loss: 0.31469568610191345\n",
      "Epoch 2801, Loss: 1.601294755935669, Final Batch Loss: 0.24650555849075317\n",
      "Epoch 2802, Loss: 1.8540723323822021, Final Batch Loss: 0.3115553855895996\n",
      "Epoch 2803, Loss: 1.626218467950821, Final Batch Loss: 0.3218167722225189\n",
      "Epoch 2804, Loss: 1.9007482826709747, Final Batch Loss: 0.42455539107322693\n",
      "Epoch 2805, Loss: 1.9561233520507812, Final Batch Loss: 0.406650185585022\n",
      "Epoch 2806, Loss: 1.8420662879943848, Final Batch Loss: 0.34136876463890076\n",
      "Epoch 2807, Loss: 1.9442218244075775, Final Batch Loss: 0.38144591450691223\n",
      "Epoch 2808, Loss: 1.8186662197113037, Final Batch Loss: 0.37704020738601685\n",
      "Epoch 2809, Loss: 1.7845414578914642, Final Batch Loss: 0.3174700140953064\n",
      "Epoch 2810, Loss: 1.9229594469070435, Final Batch Loss: 0.5627763271331787\n",
      "Epoch 2811, Loss: 1.7423647344112396, Final Batch Loss: 0.2970665395259857\n",
      "Epoch 2812, Loss: 1.7383819818496704, Final Batch Loss: 0.28715822100639343\n",
      "Epoch 2813, Loss: 1.623851254582405, Final Batch Loss: 0.24816842377185822\n",
      "Epoch 2814, Loss: 1.790366381406784, Final Batch Loss: 0.328046590089798\n",
      "Epoch 2815, Loss: 1.831362545490265, Final Batch Loss: 0.24957320094108582\n",
      "Epoch 2816, Loss: 1.7687756717205048, Final Batch Loss: 0.39914774894714355\n",
      "Epoch 2817, Loss: 1.7395906746387482, Final Batch Loss: 0.35330602526664734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2818, Loss: 1.7815177738666534, Final Batch Loss: 0.307188481092453\n",
      "Epoch 2819, Loss: 1.8724519312381744, Final Batch Loss: 0.38099080324172974\n",
      "Epoch 2820, Loss: 1.7036338448524475, Final Batch Loss: 0.2565843462944031\n",
      "Epoch 2821, Loss: 1.5776124894618988, Final Batch Loss: 0.281247079372406\n",
      "Epoch 2822, Loss: 1.653363659977913, Final Batch Loss: 0.23172374069690704\n",
      "Epoch 2823, Loss: 1.805140733718872, Final Batch Loss: 0.30957064032554626\n",
      "Epoch 2824, Loss: 1.8095939457416534, Final Batch Loss: 0.2528899312019348\n",
      "Epoch 2825, Loss: 1.8177287578582764, Final Batch Loss: 0.3946412205696106\n",
      "Epoch 2826, Loss: 1.6754408776760101, Final Batch Loss: 0.38894346356391907\n",
      "Epoch 2827, Loss: 2.081124573945999, Final Batch Loss: 0.6066745519638062\n",
      "Epoch 2828, Loss: 1.8276037573814392, Final Batch Loss: 0.2986319065093994\n",
      "Epoch 2829, Loss: 1.684469223022461, Final Batch Loss: 0.3003181517124176\n",
      "Epoch 2830, Loss: 1.6763291656970978, Final Batch Loss: 0.2622144818305969\n",
      "Epoch 2831, Loss: 1.9788292348384857, Final Batch Loss: 0.5461551547050476\n",
      "Epoch 2832, Loss: 1.8212835490703583, Final Batch Loss: 0.46071353554725647\n",
      "Epoch 2833, Loss: 2.01742485165596, Final Batch Loss: 0.5197257995605469\n",
      "Epoch 2834, Loss: 1.903879463672638, Final Batch Loss: 0.4599776268005371\n",
      "Epoch 2835, Loss: 1.782638132572174, Final Batch Loss: 0.30843353271484375\n",
      "Epoch 2836, Loss: 1.8175445199012756, Final Batch Loss: 0.2771568298339844\n",
      "Epoch 2837, Loss: 1.840255081653595, Final Batch Loss: 0.5036019682884216\n",
      "Epoch 2838, Loss: 1.6634196937084198, Final Batch Loss: 0.27038198709487915\n",
      "Epoch 2839, Loss: 1.7719878554344177, Final Batch Loss: 0.3672633767127991\n",
      "Epoch 2840, Loss: 1.8193019032478333, Final Batch Loss: 0.3184272050857544\n",
      "Epoch 2841, Loss: 1.8512283563613892, Final Batch Loss: 0.34660714864730835\n",
      "Epoch 2842, Loss: 1.555783748626709, Final Batch Loss: 0.21440038084983826\n",
      "Epoch 2843, Loss: 2.0410461723804474, Final Batch Loss: 0.45170116424560547\n",
      "Epoch 2844, Loss: 1.8266463577747345, Final Batch Loss: 0.389347106218338\n",
      "Epoch 2845, Loss: 1.6022078692913055, Final Batch Loss: 0.3585853576660156\n",
      "Epoch 2846, Loss: 1.8994124829769135, Final Batch Loss: 0.39657533168792725\n",
      "Epoch 2847, Loss: 1.771995186805725, Final Batch Loss: 0.34262120723724365\n",
      "Epoch 2848, Loss: 1.9632503390312195, Final Batch Loss: 0.47662973403930664\n",
      "Epoch 2849, Loss: 1.9452177286148071, Final Batch Loss: 0.40693098306655884\n",
      "Epoch 2850, Loss: 1.8548604249954224, Final Batch Loss: 0.490217387676239\n",
      "Epoch 2851, Loss: 1.8366675674915314, Final Batch Loss: 0.34812554717063904\n",
      "Epoch 2852, Loss: 1.833802491426468, Final Batch Loss: 0.36593252420425415\n",
      "Epoch 2853, Loss: 1.77361598610878, Final Batch Loss: 0.2643366754055023\n",
      "Epoch 2854, Loss: 1.7747703790664673, Final Batch Loss: 0.40958014130592346\n",
      "Epoch 2855, Loss: 1.5668129175901413, Final Batch Loss: 0.23574762046337128\n",
      "Epoch 2856, Loss: 1.9448936581611633, Final Batch Loss: 0.3063454031944275\n",
      "Epoch 2857, Loss: 1.7846768200397491, Final Batch Loss: 0.358664870262146\n",
      "Epoch 2858, Loss: 1.7495163679122925, Final Batch Loss: 0.3015269637107849\n",
      "Epoch 2859, Loss: 1.8187673091888428, Final Batch Loss: 0.35679590702056885\n",
      "Epoch 2860, Loss: 1.6415439248085022, Final Batch Loss: 0.31698718667030334\n",
      "Epoch 2861, Loss: 1.6136747300624847, Final Batch Loss: 0.3102914094924927\n",
      "Epoch 2862, Loss: 1.5566724091768265, Final Batch Loss: 0.29315823316574097\n",
      "Epoch 2863, Loss: 1.6128246784210205, Final Batch Loss: 0.29236823320388794\n",
      "Epoch 2864, Loss: 1.605946034193039, Final Batch Loss: 0.34666869044303894\n",
      "Epoch 2865, Loss: 1.7159762978553772, Final Batch Loss: 0.2799678444862366\n",
      "Epoch 2866, Loss: 1.7200957834720612, Final Batch Loss: 0.3803984522819519\n",
      "Epoch 2867, Loss: 2.00344455242157, Final Batch Loss: 0.4229554533958435\n",
      "Epoch 2868, Loss: 1.6940623819828033, Final Batch Loss: 0.3771898150444031\n",
      "Epoch 2869, Loss: 1.5841962695121765, Final Batch Loss: 0.29927879571914673\n",
      "Epoch 2870, Loss: 1.6640535593032837, Final Batch Loss: 0.2355632781982422\n",
      "Epoch 2871, Loss: 1.7811323702335358, Final Batch Loss: 0.365729421377182\n",
      "Epoch 2872, Loss: 1.5852673053741455, Final Batch Loss: 0.2941206395626068\n",
      "Epoch 2873, Loss: 1.8883733451366425, Final Batch Loss: 0.3516909182071686\n",
      "Epoch 2874, Loss: 1.6033188104629517, Final Batch Loss: 0.3260928690433502\n",
      "Epoch 2875, Loss: 1.6089236438274384, Final Batch Loss: 0.33246588706970215\n",
      "Epoch 2876, Loss: 2.1132481396198273, Final Batch Loss: 0.6760013699531555\n",
      "Epoch 2877, Loss: 1.535372018814087, Final Batch Loss: 0.22222566604614258\n",
      "Epoch 2878, Loss: 1.8182706236839294, Final Batch Loss: 0.47863537073135376\n",
      "Epoch 2879, Loss: 1.8211176097393036, Final Batch Loss: 0.34726253151893616\n",
      "Epoch 2880, Loss: 1.742859274148941, Final Batch Loss: 0.37810859084129333\n",
      "Epoch 2881, Loss: 1.8555189669132233, Final Batch Loss: 0.3967333436012268\n",
      "Epoch 2882, Loss: 1.7179823815822601, Final Batch Loss: 0.2521848678588867\n",
      "Epoch 2883, Loss: 1.6923127472400665, Final Batch Loss: 0.4261085093021393\n",
      "Epoch 2884, Loss: 1.7543309926986694, Final Batch Loss: 0.2533714175224304\n",
      "Epoch 2885, Loss: 1.6101947575807571, Final Batch Loss: 0.13698799908161163\n",
      "Epoch 2886, Loss: 1.7051424980163574, Final Batch Loss: 0.31261613965034485\n",
      "Epoch 2887, Loss: 1.6691316366195679, Final Batch Loss: 0.44061946868896484\n",
      "Epoch 2888, Loss: 1.6542053520679474, Final Batch Loss: 0.27899035811424255\n",
      "Epoch 2889, Loss: 1.6430236995220184, Final Batch Loss: 0.2937518060207367\n",
      "Epoch 2890, Loss: 1.5793691277503967, Final Batch Loss: 0.30807259678840637\n",
      "Epoch 2891, Loss: 1.7392328381538391, Final Batch Loss: 0.3352428376674652\n",
      "Epoch 2892, Loss: 1.7409788966178894, Final Batch Loss: 0.3918113112449646\n",
      "Epoch 2893, Loss: 1.855000376701355, Final Batch Loss: 0.35687750577926636\n",
      "Epoch 2894, Loss: 1.7679092586040497, Final Batch Loss: 0.3651862144470215\n",
      "Epoch 2895, Loss: 1.7509432435035706, Final Batch Loss: 0.2591499388217926\n",
      "Epoch 2896, Loss: 1.6503655761480331, Final Batch Loss: 0.1951332539319992\n",
      "Epoch 2897, Loss: 1.6941826045513153, Final Batch Loss: 0.4509473443031311\n",
      "Epoch 2898, Loss: 1.9826940596103668, Final Batch Loss: 0.48463162779808044\n",
      "Epoch 2899, Loss: 1.768139898777008, Final Batch Loss: 0.3014349043369293\n",
      "Epoch 2900, Loss: 1.745141476392746, Final Batch Loss: 0.33223408460617065\n",
      "Epoch 2901, Loss: 1.9439320266246796, Final Batch Loss: 0.27069273591041565\n",
      "Epoch 2902, Loss: 1.738274335861206, Final Batch Loss: 0.42595043778419495\n",
      "Epoch 2903, Loss: 1.8950352370738983, Final Batch Loss: 0.5189440250396729\n",
      "Epoch 2904, Loss: 1.8578430116176605, Final Batch Loss: 0.3910885155200958\n",
      "Epoch 2905, Loss: 2.0289262235164642, Final Batch Loss: 0.6299103498458862\n",
      "Epoch 2906, Loss: 2.0122486650943756, Final Batch Loss: 0.5539682507514954\n",
      "Epoch 2907, Loss: 1.9090169072151184, Final Batch Loss: 0.3027006983757019\n",
      "Epoch 2908, Loss: 2.2592870891094208, Final Batch Loss: 0.6825643181800842\n",
      "Epoch 2909, Loss: 1.9483428597450256, Final Batch Loss: 0.4558437764644623\n",
      "Epoch 2910, Loss: 2.038848876953125, Final Batch Loss: 0.6080252528190613\n",
      "Epoch 2911, Loss: 1.631967082619667, Final Batch Loss: 0.20927567780017853\n",
      "Epoch 2912, Loss: 1.690920203924179, Final Batch Loss: 0.3631187975406647\n",
      "Epoch 2913, Loss: 1.8031024634838104, Final Batch Loss: 0.3390635550022125\n",
      "Epoch 2914, Loss: 1.6627944707870483, Final Batch Loss: 0.33570802211761475\n",
      "Epoch 2915, Loss: 1.818188339471817, Final Batch Loss: 0.45845186710357666\n",
      "Epoch 2916, Loss: 1.5255270600318909, Final Batch Loss: 0.26066678762435913\n",
      "Epoch 2917, Loss: 1.7370987832546234, Final Batch Loss: 0.30275774002075195\n",
      "Epoch 2918, Loss: 1.7873564660549164, Final Batch Loss: 0.36883673071861267\n",
      "Epoch 2919, Loss: 1.6476170718669891, Final Batch Loss: 0.3239259421825409\n",
      "Epoch 2920, Loss: 1.866793930530548, Final Batch Loss: 0.5122900605201721\n",
      "Epoch 2921, Loss: 1.9098962545394897, Final Batch Loss: 0.4233829379081726\n",
      "Epoch 2922, Loss: 2.009328991174698, Final Batch Loss: 0.4792182147502899\n",
      "Epoch 2923, Loss: 1.7631462812423706, Final Batch Loss: 0.3992466330528259\n",
      "Epoch 2924, Loss: 1.7738335132598877, Final Batch Loss: 0.4094802737236023\n",
      "Epoch 2925, Loss: 1.6787072718143463, Final Batch Loss: 0.33453822135925293\n",
      "Epoch 2926, Loss: 1.702512413263321, Final Batch Loss: 0.3622293472290039\n",
      "Epoch 2927, Loss: 1.6102093160152435, Final Batch Loss: 0.14284241199493408\n",
      "Epoch 2928, Loss: 1.9956762492656708, Final Batch Loss: 0.6330238580703735\n",
      "Epoch 2929, Loss: 1.7406405210494995, Final Batch Loss: 0.3985443413257599\n",
      "Epoch 2930, Loss: 1.832999587059021, Final Batch Loss: 0.3892989158630371\n",
      "Epoch 2931, Loss: 1.8858589828014374, Final Batch Loss: 0.4480310380458832\n",
      "Epoch 2932, Loss: 2.0412114560604095, Final Batch Loss: 0.4276176989078522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2933, Loss: 1.7426480948925018, Final Batch Loss: 0.4061848521232605\n",
      "Epoch 2934, Loss: 1.7106999158859253, Final Batch Loss: 0.41634756326675415\n",
      "Epoch 2935, Loss: 1.6378947794437408, Final Batch Loss: 0.23310160636901855\n",
      "Epoch 2936, Loss: 1.7149950861930847, Final Batch Loss: 0.34706392884254456\n",
      "Epoch 2937, Loss: 1.6942461878061295, Final Batch Loss: 0.2155161052942276\n",
      "Epoch 2938, Loss: 1.7534998655319214, Final Batch Loss: 0.40560731291770935\n",
      "Epoch 2939, Loss: 1.5999854505062103, Final Batch Loss: 0.25858667492866516\n",
      "Epoch 2940, Loss: 1.7627432644367218, Final Batch Loss: 0.3430953323841095\n",
      "Epoch 2941, Loss: 1.556016445159912, Final Batch Loss: 0.23530590534210205\n",
      "Epoch 2942, Loss: 1.6932953000068665, Final Batch Loss: 0.35256290435791016\n",
      "Epoch 2943, Loss: 1.5908527076244354, Final Batch Loss: 0.23352524638175964\n",
      "Epoch 2944, Loss: 1.771482527256012, Final Batch Loss: 0.38708004355430603\n",
      "Epoch 2945, Loss: 1.7644267678260803, Final Batch Loss: 0.30336475372314453\n",
      "Epoch 2946, Loss: 1.796566367149353, Final Batch Loss: 0.45951318740844727\n",
      "Epoch 2947, Loss: 1.7223345637321472, Final Batch Loss: 0.29426538944244385\n",
      "Epoch 2948, Loss: 1.5440097451210022, Final Batch Loss: 0.17985311150550842\n",
      "Epoch 2949, Loss: 1.7105436623096466, Final Batch Loss: 0.33069419860839844\n",
      "Epoch 2950, Loss: 2.0607049763202667, Final Batch Loss: 0.5916814208030701\n",
      "Epoch 2951, Loss: 1.759468913078308, Final Batch Loss: 0.35071098804473877\n",
      "Epoch 2952, Loss: 1.7691746652126312, Final Batch Loss: 0.48265209794044495\n",
      "Epoch 2953, Loss: 1.7772977948188782, Final Batch Loss: 0.3350718021392822\n",
      "Epoch 2954, Loss: 1.7606450915336609, Final Batch Loss: 0.2947518229484558\n",
      "Epoch 2955, Loss: 1.668037474155426, Final Batch Loss: 0.2527894973754883\n",
      "Epoch 2956, Loss: 1.7647322416305542, Final Batch Loss: 0.3825838267803192\n",
      "Epoch 2957, Loss: 1.6828136146068573, Final Batch Loss: 0.2612004578113556\n",
      "Epoch 2958, Loss: 1.522894710302353, Final Batch Loss: 0.24407094717025757\n",
      "Epoch 2959, Loss: 1.8244929015636444, Final Batch Loss: 0.3940460979938507\n",
      "Epoch 2960, Loss: 1.7557121515274048, Final Batch Loss: 0.35196805000305176\n",
      "Epoch 2961, Loss: 1.7461627423763275, Final Batch Loss: 0.3537747859954834\n",
      "Epoch 2962, Loss: 1.959026962518692, Final Batch Loss: 0.4625617265701294\n",
      "Epoch 2963, Loss: 1.7503803074359894, Final Batch Loss: 0.26514506340026855\n",
      "Epoch 2964, Loss: 1.7095048427581787, Final Batch Loss: 0.36026036739349365\n",
      "Epoch 2965, Loss: 1.6602060794830322, Final Batch Loss: 0.3241956830024719\n",
      "Epoch 2966, Loss: 1.8110988438129425, Final Batch Loss: 0.46183446049690247\n",
      "Epoch 2967, Loss: 1.7286289632320404, Final Batch Loss: 0.3177854120731354\n",
      "Epoch 2968, Loss: 1.6261424273252487, Final Batch Loss: 0.24788419902324677\n",
      "Epoch 2969, Loss: 1.694666862487793, Final Batch Loss: 0.26296353340148926\n",
      "Epoch 2970, Loss: 1.8521907329559326, Final Batch Loss: 0.3830680847167969\n",
      "Epoch 2971, Loss: 1.8277874886989594, Final Batch Loss: 0.4304348826408386\n",
      "Epoch 2972, Loss: 1.520907074213028, Final Batch Loss: 0.2848033905029297\n",
      "Epoch 2973, Loss: 1.9321613609790802, Final Batch Loss: 0.5007044672966003\n",
      "Epoch 2974, Loss: 1.6215488612651825, Final Batch Loss: 0.33164069056510925\n",
      "Epoch 2975, Loss: 1.7111537009477615, Final Batch Loss: 0.2360394448041916\n",
      "Epoch 2976, Loss: 1.5673179626464844, Final Batch Loss: 0.28744712471961975\n",
      "Epoch 2977, Loss: 1.8177274763584137, Final Batch Loss: 0.40180322527885437\n",
      "Epoch 2978, Loss: 1.906912475824356, Final Batch Loss: 0.5416841506958008\n",
      "Epoch 2979, Loss: 1.733786404132843, Final Batch Loss: 0.31024354696273804\n",
      "Epoch 2980, Loss: 1.733154833316803, Final Batch Loss: 0.3166205585002899\n",
      "Epoch 2981, Loss: 1.8428910374641418, Final Batch Loss: 0.4901725947856903\n",
      "Epoch 2982, Loss: 2.103342056274414, Final Batch Loss: 0.5583545565605164\n",
      "Epoch 2983, Loss: 1.7906114161014557, Final Batch Loss: 0.48873475193977356\n",
      "Epoch 2984, Loss: 1.6521589159965515, Final Batch Loss: 0.3032394349575043\n",
      "Epoch 2985, Loss: 1.680964469909668, Final Batch Loss: 0.3211202323436737\n",
      "Epoch 2986, Loss: 1.722454845905304, Final Batch Loss: 0.4418011009693146\n",
      "Epoch 2987, Loss: 1.785841941833496, Final Batch Loss: 0.41034314036369324\n",
      "Epoch 2988, Loss: 1.7077078521251678, Final Batch Loss: 0.28969570994377136\n",
      "Epoch 2989, Loss: 1.7665286660194397, Final Batch Loss: 0.34408503770828247\n",
      "Epoch 2990, Loss: 1.703978180885315, Final Batch Loss: 0.37245890498161316\n",
      "Epoch 2991, Loss: 1.5961327701807022, Final Batch Loss: 0.2872311770915985\n",
      "Epoch 2992, Loss: 1.7597144544124603, Final Batch Loss: 0.3675076365470886\n",
      "Epoch 2993, Loss: 1.5569101870059967, Final Batch Loss: 0.2544166147708893\n",
      "Epoch 2994, Loss: 1.5091519057750702, Final Batch Loss: 0.1982681155204773\n",
      "Epoch 2995, Loss: 1.876220703125, Final Batch Loss: 0.33049440383911133\n",
      "Epoch 2996, Loss: 1.7717506289482117, Final Batch Loss: 0.33982664346694946\n",
      "Epoch 2997, Loss: 1.980357438325882, Final Batch Loss: 0.3864701986312866\n",
      "Epoch 2998, Loss: 1.7440458685159683, Final Batch Loss: 0.2354470044374466\n",
      "Epoch 2999, Loss: 1.8040198981761932, Final Batch Loss: 0.3574856221675873\n",
      "Epoch 3000, Loss: 1.7385193705558777, Final Batch Loss: 0.269355833530426\n",
      "Epoch 3001, Loss: 1.7428435981273651, Final Batch Loss: 0.37772664427757263\n",
      "Epoch 3002, Loss: 1.8378486037254333, Final Batch Loss: 0.35444414615631104\n",
      "Epoch 3003, Loss: 1.631373941898346, Final Batch Loss: 0.3256782293319702\n",
      "Epoch 3004, Loss: 1.748025894165039, Final Batch Loss: 0.2857988774776459\n",
      "Epoch 3005, Loss: 1.8717690706253052, Final Batch Loss: 0.27711325883865356\n",
      "Epoch 3006, Loss: 1.6173571348190308, Final Batch Loss: 0.22088316082954407\n",
      "Epoch 3007, Loss: 1.7037284672260284, Final Batch Loss: 0.3552519679069519\n",
      "Epoch 3008, Loss: 2.0455381274223328, Final Batch Loss: 0.30039942264556885\n",
      "Epoch 3009, Loss: 1.9372924864292145, Final Batch Loss: 0.4557434320449829\n",
      "Epoch 3010, Loss: 1.7691143453121185, Final Batch Loss: 0.3601371645927429\n",
      "Epoch 3011, Loss: 1.5607320666313171, Final Batch Loss: 0.27692684531211853\n",
      "Epoch 3012, Loss: 1.598090946674347, Final Batch Loss: 0.2751407325267792\n",
      "Epoch 3013, Loss: 1.546583116054535, Final Batch Loss: 0.3151085674762726\n",
      "Epoch 3014, Loss: 1.7441213130950928, Final Batch Loss: 0.3351910710334778\n",
      "Epoch 3015, Loss: 1.9541299939155579, Final Batch Loss: 0.3288954794406891\n",
      "Epoch 3016, Loss: 1.7998699843883514, Final Batch Loss: 0.3477937877178192\n",
      "Epoch 3017, Loss: 1.73906609416008, Final Batch Loss: 0.4088278114795685\n",
      "Epoch 3018, Loss: 1.6549079418182373, Final Batch Loss: 0.352062463760376\n",
      "Epoch 3019, Loss: 1.773827701807022, Final Batch Loss: 0.505747377872467\n",
      "Epoch 3020, Loss: 1.666471928358078, Final Batch Loss: 0.42769351601600647\n",
      "Epoch 3021, Loss: 1.6518965661525726, Final Batch Loss: 0.3075571656227112\n",
      "Epoch 3022, Loss: 1.6294646859169006, Final Batch Loss: 0.2926780879497528\n",
      "Epoch 3023, Loss: 1.773927628993988, Final Batch Loss: 0.30734124779701233\n",
      "Epoch 3024, Loss: 1.6119627952575684, Final Batch Loss: 0.226787269115448\n",
      "Epoch 3025, Loss: 1.61634761095047, Final Batch Loss: 0.4003216624259949\n",
      "Epoch 3026, Loss: 1.6709322929382324, Final Batch Loss: 0.3039027452468872\n",
      "Epoch 3027, Loss: 1.7576830089092255, Final Batch Loss: 0.4009478688240051\n",
      "Epoch 3028, Loss: 1.748810350894928, Final Batch Loss: 0.39129751920700073\n",
      "Epoch 3029, Loss: 1.8449673056602478, Final Batch Loss: 0.4135448932647705\n",
      "Epoch 3030, Loss: 1.6476496458053589, Final Batch Loss: 0.3385155498981476\n",
      "Epoch 3031, Loss: 1.8502806723117828, Final Batch Loss: 0.4669612944126129\n",
      "Epoch 3032, Loss: 1.766536384820938, Final Batch Loss: 0.33422890305519104\n",
      "Epoch 3033, Loss: 1.590443730354309, Final Batch Loss: 0.27098673582077026\n",
      "Epoch 3034, Loss: 2.0588730573654175, Final Batch Loss: 0.4883934259414673\n",
      "Epoch 3035, Loss: 1.9464361369609833, Final Batch Loss: 0.38754504919052124\n",
      "Epoch 3036, Loss: 1.625606894493103, Final Batch Loss: 0.31642791628837585\n",
      "Epoch 3037, Loss: 1.9581747949123383, Final Batch Loss: 0.463850736618042\n",
      "Epoch 3038, Loss: 1.7848427891731262, Final Batch Loss: 0.40515750646591187\n",
      "Epoch 3039, Loss: 1.8347728848457336, Final Batch Loss: 0.4617324471473694\n",
      "Epoch 3040, Loss: 1.85008704662323, Final Batch Loss: 0.3833877146244049\n",
      "Epoch 3041, Loss: 1.681344598531723, Final Batch Loss: 0.41819432377815247\n",
      "Epoch 3042, Loss: 1.6911628544330597, Final Batch Loss: 0.37444761395454407\n",
      "Epoch 3043, Loss: 1.5217450112104416, Final Batch Loss: 0.3161069452762604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3044, Loss: 1.6821793913841248, Final Batch Loss: 0.38313689827919006\n",
      "Epoch 3045, Loss: 1.8129902482032776, Final Batch Loss: 0.3441447913646698\n",
      "Epoch 3046, Loss: 1.611214056611061, Final Batch Loss: 0.23421649634838104\n",
      "Epoch 3047, Loss: 1.6744959652423859, Final Batch Loss: 0.26239240169525146\n",
      "Epoch 3048, Loss: 1.7123511731624603, Final Batch Loss: 0.2868862748146057\n",
      "Epoch 3049, Loss: 1.759201556444168, Final Batch Loss: 0.3085692226886749\n",
      "Epoch 3050, Loss: 1.6983102858066559, Final Batch Loss: 0.4335215091705322\n",
      "Epoch 3051, Loss: 1.591285914182663, Final Batch Loss: 0.32872921228408813\n",
      "Epoch 3052, Loss: 1.6426472663879395, Final Batch Loss: 0.3379194736480713\n",
      "Epoch 3053, Loss: 1.7839289009571075, Final Batch Loss: 0.4048515558242798\n",
      "Epoch 3054, Loss: 1.7717779278755188, Final Batch Loss: 0.3716759979724884\n",
      "Epoch 3055, Loss: 1.666245013475418, Final Batch Loss: 0.40722304582595825\n",
      "Epoch 3056, Loss: 1.784716159105301, Final Batch Loss: 0.3160656988620758\n",
      "Epoch 3057, Loss: 1.5321618914604187, Final Batch Loss: 0.2879447340965271\n",
      "Epoch 3058, Loss: 1.7045871317386627, Final Batch Loss: 0.27863842248916626\n",
      "Epoch 3059, Loss: 1.6943678557872772, Final Batch Loss: 0.38554513454437256\n",
      "Epoch 3060, Loss: 1.8269115686416626, Final Batch Loss: 0.4427334666252136\n",
      "Epoch 3061, Loss: 1.575396716594696, Final Batch Loss: 0.2837933599948883\n",
      "Epoch 3062, Loss: 1.8106046617031097, Final Batch Loss: 0.29072651267051697\n",
      "Epoch 3063, Loss: 1.6857910752296448, Final Batch Loss: 0.32625383138656616\n",
      "Epoch 3064, Loss: 1.6878346502780914, Final Batch Loss: 0.28068429231643677\n",
      "Epoch 3065, Loss: 2.0984106361865997, Final Batch Loss: 0.6069338917732239\n",
      "Epoch 3066, Loss: 1.7468723058700562, Final Batch Loss: 0.3459850549697876\n",
      "Epoch 3067, Loss: 1.7060087323188782, Final Batch Loss: 0.3328385651111603\n",
      "Epoch 3068, Loss: 1.6454884856939316, Final Batch Loss: 0.3276502788066864\n",
      "Epoch 3069, Loss: 1.8463279902935028, Final Batch Loss: 0.4779958426952362\n",
      "Epoch 3070, Loss: 1.6499744057655334, Final Batch Loss: 0.2561153173446655\n",
      "Epoch 3071, Loss: 1.7123616635799408, Final Batch Loss: 0.2966594099998474\n",
      "Epoch 3072, Loss: 1.811610460281372, Final Batch Loss: 0.32458651065826416\n",
      "Epoch 3073, Loss: 1.9875938296318054, Final Batch Loss: 0.5582382678985596\n",
      "Epoch 3074, Loss: 2.9014735221862793, Final Batch Loss: 1.5462496280670166\n",
      "Epoch 3075, Loss: 2.15283864736557, Final Batch Loss: 0.7721392512321472\n",
      "Epoch 3076, Loss: 1.899513840675354, Final Batch Loss: 0.5564157962799072\n",
      "Epoch 3077, Loss: 1.6485476940870285, Final Batch Loss: 0.22695650160312653\n",
      "Epoch 3078, Loss: 1.6429436206817627, Final Batch Loss: 0.33124253153800964\n",
      "Epoch 3079, Loss: 1.8930437564849854, Final Batch Loss: 0.3902879059314728\n",
      "Epoch 3080, Loss: 1.7980799674987793, Final Batch Loss: 0.3734739124774933\n",
      "Epoch 3081, Loss: 1.6068477928638458, Final Batch Loss: 0.37014126777648926\n",
      "Epoch 3082, Loss: 1.6111944317817688, Final Batch Loss: 0.2802535891532898\n",
      "Epoch 3083, Loss: 1.7776774168014526, Final Batch Loss: 0.3695777952671051\n",
      "Epoch 3084, Loss: 2.1002077460289, Final Batch Loss: 0.635210394859314\n",
      "Epoch 3085, Loss: 1.692124217748642, Final Batch Loss: 0.46748119592666626\n",
      "Epoch 3086, Loss: 1.7231086045503616, Final Batch Loss: 0.24710915982723236\n",
      "Epoch 3087, Loss: 1.8722273111343384, Final Batch Loss: 0.44151708483695984\n",
      "Epoch 3088, Loss: 1.9233034551143646, Final Batch Loss: 0.43734830617904663\n",
      "Epoch 3089, Loss: 1.8515633344650269, Final Batch Loss: 0.42690950632095337\n",
      "Epoch 3090, Loss: 1.7665654420852661, Final Batch Loss: 0.3049980401992798\n",
      "Epoch 3091, Loss: 1.669609934091568, Final Batch Loss: 0.33486735820770264\n",
      "Epoch 3092, Loss: 1.6227046847343445, Final Batch Loss: 0.3476406931877136\n",
      "Epoch 3093, Loss: 1.5848288834095001, Final Batch Loss: 0.24511662125587463\n",
      "Epoch 3094, Loss: 1.83802330493927, Final Batch Loss: 0.3369828462600708\n",
      "Epoch 3095, Loss: 1.546533226966858, Final Batch Loss: 0.25932976603507996\n",
      "Epoch 3096, Loss: 1.735800325870514, Final Batch Loss: 0.32605716586112976\n",
      "Epoch 3097, Loss: 1.761817216873169, Final Batch Loss: 0.3272148370742798\n",
      "Epoch 3098, Loss: 2.302372932434082, Final Batch Loss: 0.9792544841766357\n",
      "Epoch 3099, Loss: 1.7337878346443176, Final Batch Loss: 0.39411690831184387\n",
      "Epoch 3100, Loss: 1.6320554912090302, Final Batch Loss: 0.21684032678604126\n",
      "Epoch 3101, Loss: 1.5071043074131012, Final Batch Loss: 0.19013535976409912\n",
      "Epoch 3102, Loss: 1.7015033066272736, Final Batch Loss: 0.39042797684669495\n",
      "Epoch 3103, Loss: 1.7645639777183533, Final Batch Loss: 0.403469443321228\n",
      "Epoch 3104, Loss: 1.7761008441448212, Final Batch Loss: 0.3580528199672699\n",
      "Epoch 3105, Loss: 1.6722437739372253, Final Batch Loss: 0.3059496581554413\n",
      "Epoch 3106, Loss: 1.6301213204860687, Final Batch Loss: 0.2562539875507355\n",
      "Epoch 3107, Loss: 1.6514131277799606, Final Batch Loss: 0.34275656938552856\n",
      "Epoch 3108, Loss: 1.9594865143299103, Final Batch Loss: 0.4034750163555145\n",
      "Epoch 3109, Loss: 1.8050642311573029, Final Batch Loss: 0.4134351909160614\n",
      "Epoch 3110, Loss: 1.8450757563114166, Final Batch Loss: 0.2731529474258423\n",
      "Epoch 3111, Loss: 1.7530645430088043, Final Batch Loss: 0.33877819776535034\n",
      "Epoch 3112, Loss: 1.7577184438705444, Final Batch Loss: 0.3796752691268921\n",
      "Epoch 3113, Loss: 2.2228572964668274, Final Batch Loss: 0.697585940361023\n",
      "Epoch 3114, Loss: 1.7314532995224, Final Batch Loss: 0.36110642552375793\n",
      "Epoch 3115, Loss: 1.731534719467163, Final Batch Loss: 0.3267976641654968\n",
      "Epoch 3116, Loss: 1.6854628920555115, Final Batch Loss: 0.2778782248497009\n",
      "Epoch 3117, Loss: 1.5593668520450592, Final Batch Loss: 0.3458111882209778\n",
      "Epoch 3118, Loss: 1.8987306952476501, Final Batch Loss: 0.4078006446361542\n",
      "Epoch 3119, Loss: 1.5433060377836227, Final Batch Loss: 0.31222856044769287\n",
      "Epoch 3120, Loss: 1.7754085659980774, Final Batch Loss: 0.3474292457103729\n",
      "Epoch 3121, Loss: 1.6940574049949646, Final Batch Loss: 0.29353177547454834\n",
      "Epoch 3122, Loss: 1.7220925390720367, Final Batch Loss: 0.3302324116230011\n",
      "Epoch 3123, Loss: 1.7310341596603394, Final Batch Loss: 0.358318030834198\n",
      "Epoch 3124, Loss: 1.7218519449234009, Final Batch Loss: 0.2917984127998352\n",
      "Epoch 3125, Loss: 1.6446572542190552, Final Batch Loss: 0.3436574935913086\n",
      "Epoch 3126, Loss: 1.6108238399028778, Final Batch Loss: 0.29991671442985535\n",
      "Epoch 3127, Loss: 1.758366584777832, Final Batch Loss: 0.3309699594974518\n",
      "Epoch 3128, Loss: 1.745343029499054, Final Batch Loss: 0.27218273282051086\n",
      "Epoch 3129, Loss: 1.467167854309082, Final Batch Loss: 0.1624135971069336\n",
      "Epoch 3130, Loss: 1.8376454412937164, Final Batch Loss: 0.34546810388565063\n",
      "Epoch 3131, Loss: 1.710245743393898, Final Batch Loss: 0.23268692195415497\n",
      "Epoch 3132, Loss: 1.725249022245407, Final Batch Loss: 0.3216572403907776\n",
      "Epoch 3133, Loss: 1.532889261841774, Final Batch Loss: 0.2853412926197052\n",
      "Epoch 3134, Loss: 1.9119256734848022, Final Batch Loss: 0.4746926426887512\n",
      "Epoch 3135, Loss: 1.672316998243332, Final Batch Loss: 0.35869985818862915\n",
      "Epoch 3136, Loss: 1.6292644739151, Final Batch Loss: 0.2907281517982483\n",
      "Epoch 3137, Loss: 1.9114461243152618, Final Batch Loss: 0.28997546434402466\n",
      "Epoch 3138, Loss: 1.6739864200353622, Final Batch Loss: 0.24171002209186554\n",
      "Epoch 3139, Loss: 1.749147117137909, Final Batch Loss: 0.36326223611831665\n",
      "Epoch 3140, Loss: 1.9265799224376678, Final Batch Loss: 0.5321148037910461\n",
      "Epoch 3141, Loss: 1.5436016917228699, Final Batch Loss: 0.2336936593055725\n",
      "Epoch 3142, Loss: 1.7286801040172577, Final Batch Loss: 0.3444187641143799\n",
      "Epoch 3143, Loss: 1.7938503921031952, Final Batch Loss: 0.4443027079105377\n",
      "Epoch 3144, Loss: 1.5434517562389374, Final Batch Loss: 0.17551115155220032\n",
      "Epoch 3145, Loss: 1.8325832188129425, Final Batch Loss: 0.43057572841644287\n",
      "Epoch 3146, Loss: 1.837152123451233, Final Batch Loss: 0.3603505790233612\n",
      "Epoch 3147, Loss: 1.6056709587574005, Final Batch Loss: 0.3035609722137451\n",
      "Epoch 3148, Loss: 1.8725494146347046, Final Batch Loss: 0.38844698667526245\n",
      "Epoch 3149, Loss: 1.698792725801468, Final Batch Loss: 0.37211278080940247\n",
      "Epoch 3150, Loss: 1.6209674179553986, Final Batch Loss: 0.26643437147140503\n",
      "Epoch 3151, Loss: 1.9620616734027863, Final Batch Loss: 0.4841832220554352\n",
      "Epoch 3152, Loss: 1.8422978818416595, Final Batch Loss: 0.3148152530193329\n",
      "Epoch 3153, Loss: 1.8388800919055939, Final Batch Loss: 0.34401842951774597\n",
      "Epoch 3154, Loss: 1.642946183681488, Final Batch Loss: 0.2815956473350525\n",
      "Epoch 3155, Loss: 1.5763945430517197, Final Batch Loss: 0.3220943808555603\n",
      "Epoch 3156, Loss: 1.8709484934806824, Final Batch Loss: 0.44956326484680176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3157, Loss: 1.8174429833889008, Final Batch Loss: 0.36950650811195374\n",
      "Epoch 3158, Loss: 1.8407061994075775, Final Batch Loss: 0.3618389070034027\n",
      "Epoch 3159, Loss: 1.6529403030872345, Final Batch Loss: 0.2752605676651001\n",
      "Epoch 3160, Loss: 1.6228543519973755, Final Batch Loss: 0.28744587302207947\n",
      "Epoch 3161, Loss: 1.7941206395626068, Final Batch Loss: 0.49850043654441833\n",
      "Epoch 3162, Loss: 1.5989035069942474, Final Batch Loss: 0.4002892076969147\n",
      "Epoch 3163, Loss: 1.5476596355438232, Final Batch Loss: 0.26613596081733704\n",
      "Epoch 3164, Loss: 1.8647326231002808, Final Batch Loss: 0.30712732672691345\n",
      "Epoch 3165, Loss: 1.6795508563518524, Final Batch Loss: 0.281608521938324\n",
      "Epoch 3166, Loss: 1.6580371111631393, Final Batch Loss: 0.28081464767456055\n",
      "Epoch 3167, Loss: 1.7598887979984283, Final Batch Loss: 0.30303332209587097\n",
      "Epoch 3168, Loss: 1.630794495344162, Final Batch Loss: 0.4050782024860382\n",
      "Epoch 3169, Loss: 1.8910906910896301, Final Batch Loss: 0.4384869635105133\n",
      "Epoch 3170, Loss: 1.561898410320282, Final Batch Loss: 0.26995545625686646\n",
      "Epoch 3171, Loss: 1.768809050321579, Final Batch Loss: 0.4413989186286926\n",
      "Epoch 3172, Loss: 1.3828814625740051, Final Batch Loss: 0.3184370994567871\n",
      "Epoch 3173, Loss: 1.5989111512899399, Final Batch Loss: 0.36704978346824646\n",
      "Epoch 3174, Loss: 1.7705126404762268, Final Batch Loss: 0.3191240429878235\n",
      "Epoch 3175, Loss: 1.5593986213207245, Final Batch Loss: 0.40442368388175964\n",
      "Epoch 3176, Loss: 1.6919531524181366, Final Batch Loss: 0.4411482810974121\n",
      "Epoch 3177, Loss: 1.6368375420570374, Final Batch Loss: 0.36487630009651184\n",
      "Epoch 3178, Loss: 1.5834693908691406, Final Batch Loss: 0.4413350522518158\n",
      "Epoch 3179, Loss: 1.8923369646072388, Final Batch Loss: 0.45154625177383423\n",
      "Epoch 3180, Loss: 1.707669973373413, Final Batch Loss: 0.2687736749649048\n",
      "Epoch 3181, Loss: 1.531710535287857, Final Batch Loss: 0.2069174349308014\n",
      "Epoch 3182, Loss: 1.5810325294733047, Final Batch Loss: 0.2052309662103653\n",
      "Epoch 3183, Loss: 1.532523512840271, Final Batch Loss: 0.23993390798568726\n",
      "Epoch 3184, Loss: 1.6475897431373596, Final Batch Loss: 0.29953399300575256\n",
      "Epoch 3185, Loss: 1.7286298871040344, Final Batch Loss: 0.3032585680484772\n",
      "Epoch 3186, Loss: 1.9229993522167206, Final Batch Loss: 0.5726786255836487\n",
      "Epoch 3187, Loss: 1.6899445056915283, Final Batch Loss: 0.2903802692890167\n",
      "Epoch 3188, Loss: 1.8508111834526062, Final Batch Loss: 0.49324730038642883\n",
      "Epoch 3189, Loss: 1.7727789878845215, Final Batch Loss: 0.4040355682373047\n",
      "Epoch 3190, Loss: 1.6588374972343445, Final Batch Loss: 0.3145545423030853\n",
      "Epoch 3191, Loss: 1.825056791305542, Final Batch Loss: 0.42362916469573975\n",
      "Epoch 3192, Loss: 1.6852743327617645, Final Batch Loss: 0.25581803917884827\n",
      "Epoch 3193, Loss: 1.3936517238616943, Final Batch Loss: 0.19251304864883423\n",
      "Epoch 3194, Loss: 1.5604307055473328, Final Batch Loss: 0.32315295934677124\n",
      "Epoch 3195, Loss: 1.741588145494461, Final Batch Loss: 0.3654715120792389\n",
      "Epoch 3196, Loss: 1.6299737691879272, Final Batch Loss: 0.25265225768089294\n",
      "Epoch 3197, Loss: 1.5919422805309296, Final Batch Loss: 0.38575080037117004\n",
      "Epoch 3198, Loss: 1.4683576077222824, Final Batch Loss: 0.3035924732685089\n",
      "Epoch 3199, Loss: 1.686675101518631, Final Batch Loss: 0.330894410610199\n",
      "Epoch 3200, Loss: 1.704496592283249, Final Batch Loss: 0.3240278363227844\n",
      "Epoch 3201, Loss: 1.625837504863739, Final Batch Loss: 0.32455456256866455\n",
      "Epoch 3202, Loss: 1.6431232690811157, Final Batch Loss: 0.2687968909740448\n",
      "Epoch 3203, Loss: 1.7647165954113007, Final Batch Loss: 0.27048757672309875\n",
      "Epoch 3204, Loss: 1.6303815841674805, Final Batch Loss: 0.3901941776275635\n",
      "Epoch 3205, Loss: 1.5484940111637115, Final Batch Loss: 0.16386166214942932\n",
      "Epoch 3206, Loss: 1.6573128402233124, Final Batch Loss: 0.36520645022392273\n",
      "Epoch 3207, Loss: 1.656069666147232, Final Batch Loss: 0.3287239968776703\n",
      "Epoch 3208, Loss: 1.5928983390331268, Final Batch Loss: 0.3317049741744995\n",
      "Epoch 3209, Loss: 1.5909159183502197, Final Batch Loss: 0.3087708055973053\n",
      "Epoch 3210, Loss: 1.871218055486679, Final Batch Loss: 0.46447575092315674\n",
      "Epoch 3211, Loss: 1.538625329732895, Final Batch Loss: 0.33142560720443726\n",
      "Epoch 3212, Loss: 1.6803705394268036, Final Batch Loss: 0.274508535861969\n",
      "Epoch 3213, Loss: 1.7258560359477997, Final Batch Loss: 0.31168168783187866\n",
      "Epoch 3214, Loss: 1.5761185586452484, Final Batch Loss: 0.30535459518432617\n",
      "Epoch 3215, Loss: 1.5845331251621246, Final Batch Loss: 0.26293471455574036\n",
      "Epoch 3216, Loss: 1.641389936208725, Final Batch Loss: 0.284380167722702\n",
      "Epoch 3217, Loss: 1.664331167936325, Final Batch Loss: 0.2666158974170685\n",
      "Epoch 3218, Loss: 1.8219613432884216, Final Batch Loss: 0.5024231672286987\n",
      "Epoch 3219, Loss: 1.8160478472709656, Final Batch Loss: 0.3458097279071808\n",
      "Epoch 3220, Loss: 1.8714241981506348, Final Batch Loss: 0.42829903960227966\n",
      "Epoch 3221, Loss: 1.8112358450889587, Final Batch Loss: 0.37532907724380493\n",
      "Epoch 3222, Loss: 1.4120538830757141, Final Batch Loss: 0.16672900319099426\n",
      "Epoch 3223, Loss: 1.5656450390815735, Final Batch Loss: 0.21927571296691895\n",
      "Epoch 3224, Loss: 1.7256739139556885, Final Batch Loss: 0.42650550603866577\n",
      "Epoch 3225, Loss: 1.5718081444501877, Final Batch Loss: 0.24219800531864166\n",
      "Epoch 3226, Loss: 1.5460021048784256, Final Batch Loss: 0.22545234858989716\n",
      "Epoch 3227, Loss: 1.7906261384487152, Final Batch Loss: 0.424566388130188\n",
      "Epoch 3228, Loss: 1.7677690386772156, Final Batch Loss: 0.36075320839881897\n",
      "Epoch 3229, Loss: 1.7147687077522278, Final Batch Loss: 0.446994423866272\n",
      "Epoch 3230, Loss: 1.6039591431617737, Final Batch Loss: 0.2507351338863373\n",
      "Epoch 3231, Loss: 1.6310278177261353, Final Batch Loss: 0.2591208219528198\n",
      "Epoch 3232, Loss: 1.691475749015808, Final Batch Loss: 0.3493429720401764\n",
      "Epoch 3233, Loss: 1.8883153796195984, Final Batch Loss: 0.5402017831802368\n",
      "Epoch 3234, Loss: 1.7545012533664703, Final Batch Loss: 0.44136106967926025\n",
      "Epoch 3235, Loss: 1.7198575139045715, Final Batch Loss: 0.2869844138622284\n",
      "Epoch 3236, Loss: 1.6644351780414581, Final Batch Loss: 0.32939955592155457\n",
      "Epoch 3237, Loss: 1.8075614869594574, Final Batch Loss: 0.3227727711200714\n",
      "Epoch 3238, Loss: 1.5252225697040558, Final Batch Loss: 0.2270418405532837\n",
      "Epoch 3239, Loss: 1.565588653087616, Final Batch Loss: 0.21815869212150574\n",
      "Epoch 3240, Loss: 1.6887504756450653, Final Batch Loss: 0.3434334993362427\n",
      "Epoch 3241, Loss: 1.8742411136627197, Final Batch Loss: 0.41090577840805054\n",
      "Epoch 3242, Loss: 1.6115059703588486, Final Batch Loss: 0.2057867795228958\n",
      "Epoch 3243, Loss: 1.7997760772705078, Final Batch Loss: 0.49541914463043213\n",
      "Epoch 3244, Loss: 1.655041128396988, Final Batch Loss: 0.3569195866584778\n",
      "Epoch 3245, Loss: 1.7546230852603912, Final Batch Loss: 0.40321314334869385\n",
      "Epoch 3246, Loss: 1.4506887644529343, Final Batch Loss: 0.28113874793052673\n",
      "Epoch 3247, Loss: 1.5750503689050674, Final Batch Loss: 0.22608430683612823\n",
      "Epoch 3248, Loss: 1.7133783549070358, Final Batch Loss: 0.23283042013645172\n",
      "Epoch 3249, Loss: 1.635023832321167, Final Batch Loss: 0.3192277252674103\n",
      "Epoch 3250, Loss: 1.9036837220191956, Final Batch Loss: 0.309947669506073\n",
      "Epoch 3251, Loss: 1.8449921011924744, Final Batch Loss: 0.34916266798973083\n",
      "Epoch 3252, Loss: 1.9231619834899902, Final Batch Loss: 0.44199416041374207\n",
      "Epoch 3253, Loss: 1.6743095219135284, Final Batch Loss: 0.476976215839386\n",
      "Epoch 3254, Loss: 1.5994635224342346, Final Batch Loss: 0.2817299962043762\n",
      "Epoch 3255, Loss: 1.5392355769872665, Final Batch Loss: 0.33844032883644104\n",
      "Epoch 3256, Loss: 1.5621972531080246, Final Batch Loss: 0.33697494864463806\n",
      "Epoch 3257, Loss: 1.6862665265798569, Final Batch Loss: 0.20828507840633392\n",
      "Epoch 3258, Loss: 1.8048471808433533, Final Batch Loss: 0.21509185433387756\n",
      "Epoch 3259, Loss: 2.1918086409568787, Final Batch Loss: 0.5991420745849609\n",
      "Epoch 3260, Loss: 1.6278169751167297, Final Batch Loss: 0.28732261061668396\n",
      "Epoch 3261, Loss: 1.65495166182518, Final Batch Loss: 0.30066317319869995\n",
      "Epoch 3262, Loss: 1.5297083854675293, Final Batch Loss: 0.22860640287399292\n",
      "Epoch 3263, Loss: 2.000309944152832, Final Batch Loss: 0.4599873423576355\n",
      "Epoch 3264, Loss: 1.554203063249588, Final Batch Loss: 0.31745585799217224\n",
      "Epoch 3265, Loss: 1.6051041930913925, Final Batch Loss: 0.24938295781612396\n",
      "Epoch 3266, Loss: 1.5211928337812424, Final Batch Loss: 0.1536748856306076\n",
      "Epoch 3267, Loss: 1.7023040354251862, Final Batch Loss: 0.3580753803253174\n",
      "Epoch 3268, Loss: 1.4263364374637604, Final Batch Loss: 0.268037348985672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3269, Loss: 1.6481934189796448, Final Batch Loss: 0.2697223424911499\n",
      "Epoch 3270, Loss: 1.7648297548294067, Final Batch Loss: 0.3621581196784973\n",
      "Epoch 3271, Loss: 1.5384259223937988, Final Batch Loss: 0.2340092658996582\n",
      "Epoch 3272, Loss: 1.6266273260116577, Final Batch Loss: 0.41272664070129395\n",
      "Epoch 3273, Loss: 1.691987007856369, Final Batch Loss: 0.35937875509262085\n",
      "Epoch 3274, Loss: 1.9865447878837585, Final Batch Loss: 0.5403857231140137\n",
      "Epoch 3275, Loss: 1.7484212517738342, Final Batch Loss: 0.30581364035606384\n",
      "Epoch 3276, Loss: 1.7115826308727264, Final Batch Loss: 0.2983624339103699\n",
      "Epoch 3277, Loss: 1.6525184512138367, Final Batch Loss: 0.34766507148742676\n",
      "Epoch 3278, Loss: 1.774232566356659, Final Batch Loss: 0.3617416322231293\n",
      "Epoch 3279, Loss: 1.8236216604709625, Final Batch Loss: 0.4479844868183136\n",
      "Epoch 3280, Loss: 1.6136270463466644, Final Batch Loss: 0.22724109888076782\n",
      "Epoch 3281, Loss: 1.7067176699638367, Final Batch Loss: 0.39492160081863403\n",
      "Epoch 3282, Loss: 1.600844830274582, Final Batch Loss: 0.3571425974369049\n",
      "Epoch 3283, Loss: 1.5759843587875366, Final Batch Loss: 0.2379876673221588\n",
      "Epoch 3284, Loss: 1.5616392493247986, Final Batch Loss: 0.3006156384944916\n",
      "Epoch 3285, Loss: 1.5860949754714966, Final Batch Loss: 0.3068441152572632\n",
      "Epoch 3286, Loss: 1.69731605052948, Final Batch Loss: 0.338844358921051\n",
      "Epoch 3287, Loss: 1.7257263362407684, Final Batch Loss: 0.35025647282600403\n",
      "Epoch 3288, Loss: 1.7849227488040924, Final Batch Loss: 0.40064409375190735\n",
      "Epoch 3289, Loss: 1.744891881942749, Final Batch Loss: 0.274694561958313\n",
      "Epoch 3290, Loss: 1.5830841362476349, Final Batch Loss: 0.34821170568466187\n",
      "Epoch 3291, Loss: 1.6815990805625916, Final Batch Loss: 0.2922516465187073\n",
      "Epoch 3292, Loss: 1.6706922352313995, Final Batch Loss: 0.32698675990104675\n",
      "Epoch 3293, Loss: 1.7046907544136047, Final Batch Loss: 0.3729398548603058\n",
      "Epoch 3294, Loss: 1.8609755039215088, Final Batch Loss: 0.363660991191864\n",
      "Epoch 3295, Loss: 1.5673630237579346, Final Batch Loss: 0.20839059352874756\n",
      "Epoch 3296, Loss: 1.7986104190349579, Final Batch Loss: 0.3723217248916626\n",
      "Epoch 3297, Loss: 1.518871694803238, Final Batch Loss: 0.3556274473667145\n",
      "Epoch 3298, Loss: 1.6547499597072601, Final Batch Loss: 0.3348636031150818\n",
      "Epoch 3299, Loss: 1.5320758521556854, Final Batch Loss: 0.3085053861141205\n",
      "Epoch 3300, Loss: 1.6994625329971313, Final Batch Loss: 0.3320501148700714\n",
      "Epoch 3301, Loss: 1.7399427890777588, Final Batch Loss: 0.33217787742614746\n",
      "Epoch 3302, Loss: 1.7824037969112396, Final Batch Loss: 0.35743728280067444\n",
      "Epoch 3303, Loss: 1.8171683549880981, Final Batch Loss: 0.5197360515594482\n",
      "Epoch 3304, Loss: 1.5997334718704224, Final Batch Loss: 0.23170825839042664\n",
      "Epoch 3305, Loss: 1.766705572605133, Final Batch Loss: 0.3151622712612152\n",
      "Epoch 3306, Loss: 1.5226044952869415, Final Batch Loss: 0.2859373092651367\n",
      "Epoch 3307, Loss: 1.6870385110378265, Final Batch Loss: 0.34941163659095764\n",
      "Epoch 3308, Loss: 1.6063542664051056, Final Batch Loss: 0.26309269666671753\n",
      "Epoch 3309, Loss: 1.693293035030365, Final Batch Loss: 0.31208449602127075\n",
      "Epoch 3310, Loss: 1.6896056532859802, Final Batch Loss: 0.28803157806396484\n",
      "Epoch 3311, Loss: 1.6582509577274323, Final Batch Loss: 0.3783475458621979\n",
      "Epoch 3312, Loss: 1.7030940055847168, Final Batch Loss: 0.3136384189128876\n",
      "Epoch 3313, Loss: 1.7954480051994324, Final Batch Loss: 0.46923911571502686\n",
      "Epoch 3314, Loss: 1.8942104578018188, Final Batch Loss: 0.5966964960098267\n",
      "Epoch 3315, Loss: 1.6137178242206573, Final Batch Loss: 0.23842912912368774\n",
      "Epoch 3316, Loss: 1.7448998838663101, Final Batch Loss: 0.4022674560546875\n",
      "Epoch 3317, Loss: 1.7087953388690948, Final Batch Loss: 0.310417115688324\n",
      "Epoch 3318, Loss: 1.7757731080055237, Final Batch Loss: 0.4229671359062195\n",
      "Epoch 3319, Loss: 1.764994353055954, Final Batch Loss: 0.36153632402420044\n",
      "Epoch 3320, Loss: 1.6545501947402954, Final Batch Loss: 0.26674410700798035\n",
      "Epoch 3321, Loss: 1.571637749671936, Final Batch Loss: 0.27711546421051025\n",
      "Epoch 3322, Loss: 1.6225886344909668, Final Batch Loss: 0.280244916677475\n",
      "Epoch 3323, Loss: 1.9157899916172028, Final Batch Loss: 0.6810861229896545\n",
      "Epoch 3324, Loss: 1.6724607348442078, Final Batch Loss: 0.33705276250839233\n",
      "Epoch 3325, Loss: 2.0545217990875244, Final Batch Loss: 0.7143149375915527\n",
      "Epoch 3326, Loss: 1.6926172971725464, Final Batch Loss: 0.2589707672595978\n",
      "Epoch 3327, Loss: 1.5459521412849426, Final Batch Loss: 0.31296661496162415\n",
      "Epoch 3328, Loss: 1.5186296701431274, Final Batch Loss: 0.26518920063972473\n",
      "Epoch 3329, Loss: 1.6825232207775116, Final Batch Loss: 0.32642343640327454\n",
      "Epoch 3330, Loss: 1.7212295681238174, Final Batch Loss: 0.41715285181999207\n",
      "Epoch 3331, Loss: 1.6023957133293152, Final Batch Loss: 0.4067024290561676\n",
      "Epoch 3332, Loss: 1.5056121051311493, Final Batch Loss: 0.18370488286018372\n",
      "Epoch 3333, Loss: 1.6354943215847015, Final Batch Loss: 0.22624057531356812\n",
      "Epoch 3334, Loss: 1.4859027564525604, Final Batch Loss: 0.26725104451179504\n",
      "Epoch 3335, Loss: 1.6223244965076447, Final Batch Loss: 0.2607787251472473\n",
      "Epoch 3336, Loss: 1.5494831055402756, Final Batch Loss: 0.24093373119831085\n",
      "Epoch 3337, Loss: 1.794582039117813, Final Batch Loss: 0.4582018256187439\n",
      "Epoch 3338, Loss: 1.8955754935741425, Final Batch Loss: 0.6157426238059998\n",
      "Epoch 3339, Loss: 1.6809486746788025, Final Batch Loss: 0.4207417964935303\n",
      "Epoch 3340, Loss: 1.7929636538028717, Final Batch Loss: 0.3326835334300995\n",
      "Epoch 3341, Loss: 1.888298749923706, Final Batch Loss: 0.35690802335739136\n",
      "Epoch 3342, Loss: 1.5223733484745026, Final Batch Loss: 0.2681034803390503\n",
      "Epoch 3343, Loss: 1.7563098073005676, Final Batch Loss: 0.3738357126712799\n",
      "Epoch 3344, Loss: 1.838192641735077, Final Batch Loss: 0.41814398765563965\n",
      "Epoch 3345, Loss: 1.620896577835083, Final Batch Loss: 0.2808498740196228\n",
      "Epoch 3346, Loss: 1.5593171417713165, Final Batch Loss: 0.2887488901615143\n",
      "Epoch 3347, Loss: 1.628879576921463, Final Batch Loss: 0.22366389632225037\n",
      "Epoch 3348, Loss: 1.848151981830597, Final Batch Loss: 0.4585953652858734\n",
      "Epoch 3349, Loss: 1.6007616519927979, Final Batch Loss: 0.25322040915489197\n",
      "Epoch 3350, Loss: 1.6769664287567139, Final Batch Loss: 0.3746795654296875\n",
      "Epoch 3351, Loss: 1.7861672937870026, Final Batch Loss: 0.3023912012577057\n",
      "Epoch 3352, Loss: 1.8003089129924774, Final Batch Loss: 0.3517340123653412\n",
      "Epoch 3353, Loss: 1.6102140992879868, Final Batch Loss: 0.32132476568222046\n",
      "Epoch 3354, Loss: 1.5339476466178894, Final Batch Loss: 0.2414434850215912\n",
      "Epoch 3355, Loss: 1.592477560043335, Final Batch Loss: 0.2895764410495758\n",
      "Epoch 3356, Loss: 1.5776999592781067, Final Batch Loss: 0.2986125349998474\n",
      "Epoch 3357, Loss: 1.7820383608341217, Final Batch Loss: 0.2855810821056366\n",
      "Epoch 3358, Loss: 1.7866885364055634, Final Batch Loss: 0.3885412812232971\n",
      "Epoch 3359, Loss: 1.8946778178215027, Final Batch Loss: 0.35633882880210876\n",
      "Epoch 3360, Loss: 1.6486723124980927, Final Batch Loss: 0.4051069915294647\n",
      "Epoch 3361, Loss: 1.716166466474533, Final Batch Loss: 0.35173580050468445\n",
      "Epoch 3362, Loss: 1.5299871563911438, Final Batch Loss: 0.27300596237182617\n",
      "Epoch 3363, Loss: 1.8436395227909088, Final Batch Loss: 0.3991875946521759\n",
      "Epoch 3364, Loss: 1.5880004465579987, Final Batch Loss: 0.21929040551185608\n",
      "Epoch 3365, Loss: 1.5597537457942963, Final Batch Loss: 0.3429960012435913\n",
      "Epoch 3366, Loss: 1.6838965713977814, Final Batch Loss: 0.32439273595809937\n",
      "Epoch 3367, Loss: 1.5856482982635498, Final Batch Loss: 0.41236060857772827\n",
      "Epoch 3368, Loss: 1.8177849352359772, Final Batch Loss: 0.3368508815765381\n",
      "Epoch 3369, Loss: 2.016105204820633, Final Batch Loss: 0.5096328258514404\n",
      "Epoch 3370, Loss: 1.7038466036319733, Final Batch Loss: 0.36013010144233704\n",
      "Epoch 3371, Loss: 1.5197900533676147, Final Batch Loss: 0.2766875922679901\n",
      "Epoch 3372, Loss: 1.5968554019927979, Final Batch Loss: 0.3115150034427643\n",
      "Epoch 3373, Loss: 1.9718700051307678, Final Batch Loss: 0.48511117696762085\n",
      "Epoch 3374, Loss: 1.480399563908577, Final Batch Loss: 0.21419931948184967\n",
      "Epoch 3375, Loss: 1.5287767499685287, Final Batch Loss: 0.2797732651233673\n",
      "Epoch 3376, Loss: 1.5710054337978363, Final Batch Loss: 0.27681583166122437\n",
      "Epoch 3377, Loss: 1.5221997797489166, Final Batch Loss: 0.24716243147850037\n",
      "Epoch 3378, Loss: 1.6396833658218384, Final Batch Loss: 0.34287020564079285\n",
      "Epoch 3379, Loss: 1.6251887530088425, Final Batch Loss: 0.35586726665496826\n",
      "Epoch 3380, Loss: 1.4848843067884445, Final Batch Loss: 0.15488718450069427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3381, Loss: 1.6407871544361115, Final Batch Loss: 0.2932756245136261\n",
      "Epoch 3382, Loss: 1.621644377708435, Final Batch Loss: 0.33024075627326965\n",
      "Epoch 3383, Loss: 1.572251945734024, Final Batch Loss: 0.33821478486061096\n",
      "Epoch 3384, Loss: 1.4861321449279785, Final Batch Loss: 0.2627495229244232\n",
      "Epoch 3385, Loss: 1.5338163673877716, Final Batch Loss: 0.24521711468696594\n",
      "Epoch 3386, Loss: 1.6079464852809906, Final Batch Loss: 0.22999554872512817\n",
      "Epoch 3387, Loss: 1.5793680250644684, Final Batch Loss: 0.25798365473747253\n",
      "Epoch 3388, Loss: 1.677911251783371, Final Batch Loss: 0.4703928828239441\n",
      "Epoch 3389, Loss: 1.4736803621053696, Final Batch Loss: 0.18791373074054718\n",
      "Epoch 3390, Loss: 1.78061243891716, Final Batch Loss: 0.42640501260757446\n",
      "Epoch 3391, Loss: 1.3924880176782608, Final Batch Loss: 0.20294691622257233\n",
      "Epoch 3392, Loss: 1.5386671721935272, Final Batch Loss: 0.26127371191978455\n",
      "Epoch 3393, Loss: 1.7504824995994568, Final Batch Loss: 0.3592883050441742\n",
      "Epoch 3394, Loss: 1.3615812361240387, Final Batch Loss: 0.20366328954696655\n",
      "Epoch 3395, Loss: 1.8031672239303589, Final Batch Loss: 0.45247605443000793\n",
      "Epoch 3396, Loss: 1.5334911346435547, Final Batch Loss: 0.3350904881954193\n",
      "Epoch 3397, Loss: 1.6035112589597702, Final Batch Loss: 0.2448488026857376\n",
      "Epoch 3398, Loss: 1.5377230793237686, Final Batch Loss: 0.39426591992378235\n",
      "Epoch 3399, Loss: 1.499095231294632, Final Batch Loss: 0.331330269575119\n",
      "Epoch 3400, Loss: 1.750623643398285, Final Batch Loss: 0.4513750374317169\n",
      "Epoch 3401, Loss: 1.4393739104270935, Final Batch Loss: 0.19234314560890198\n",
      "Epoch 3402, Loss: 1.655847042798996, Final Batch Loss: 0.34075289964675903\n",
      "Epoch 3403, Loss: 1.5233727544546127, Final Batch Loss: 0.21421529352664948\n",
      "Epoch 3404, Loss: 1.6716407239437103, Final Batch Loss: 0.23900723457336426\n",
      "Epoch 3405, Loss: 1.7468550503253937, Final Batch Loss: 0.4826700687408447\n",
      "Epoch 3406, Loss: 1.7433807849884033, Final Batch Loss: 0.297624796628952\n",
      "Epoch 3407, Loss: 1.6683168411254883, Final Batch Loss: 0.3772897720336914\n",
      "Epoch 3408, Loss: 1.699096530675888, Final Batch Loss: 0.31854555010795593\n",
      "Epoch 3409, Loss: 1.7409808784723282, Final Batch Loss: 0.23623810708522797\n",
      "Epoch 3410, Loss: 1.7164852619171143, Final Batch Loss: 0.36156582832336426\n",
      "Epoch 3411, Loss: 1.769256055355072, Final Batch Loss: 0.35845616459846497\n",
      "Epoch 3412, Loss: 1.6223049759864807, Final Batch Loss: 0.32063597440719604\n",
      "Epoch 3413, Loss: 1.5225004255771637, Final Batch Loss: 0.24367159605026245\n",
      "Epoch 3414, Loss: 1.5477949529886246, Final Batch Loss: 0.3726429343223572\n",
      "Epoch 3415, Loss: 1.5213606655597687, Final Batch Loss: 0.29295814037323\n",
      "Epoch 3416, Loss: 1.6290887296199799, Final Batch Loss: 0.35427576303482056\n",
      "Epoch 3417, Loss: 1.6109959185123444, Final Batch Loss: 0.28176426887512207\n",
      "Epoch 3418, Loss: 1.580061137676239, Final Batch Loss: 0.45779532194137573\n",
      "Epoch 3419, Loss: 1.4429163038730621, Final Batch Loss: 0.19257481396198273\n",
      "Epoch 3420, Loss: 1.5860703587532043, Final Batch Loss: 0.3296811580657959\n",
      "Epoch 3421, Loss: 1.644292175769806, Final Batch Loss: 0.2661803960800171\n",
      "Epoch 3422, Loss: 1.6410219967365265, Final Batch Loss: 0.38061603903770447\n",
      "Epoch 3423, Loss: 1.887931078672409, Final Batch Loss: 0.5241029858589172\n",
      "Epoch 3424, Loss: 1.7012110948562622, Final Batch Loss: 0.39453086256980896\n",
      "Epoch 3425, Loss: 1.7878665030002594, Final Batch Loss: 0.4591801166534424\n",
      "Epoch 3426, Loss: 1.5590095818042755, Final Batch Loss: 0.290952205657959\n",
      "Epoch 3427, Loss: 1.6381567120552063, Final Batch Loss: 0.4753229320049286\n",
      "Epoch 3428, Loss: 1.8509730696678162, Final Batch Loss: 0.32886403799057007\n",
      "Epoch 3429, Loss: 1.7073863744735718, Final Batch Loss: 0.43080058693885803\n",
      "Epoch 3430, Loss: 1.4401956349611282, Final Batch Loss: 0.2749185860157013\n",
      "Epoch 3431, Loss: 1.648530751466751, Final Batch Loss: 0.2874796986579895\n",
      "Epoch 3432, Loss: 1.7177321910858154, Final Batch Loss: 0.4521268904209137\n",
      "Epoch 3433, Loss: 1.699154794216156, Final Batch Loss: 0.27571144700050354\n",
      "Epoch 3434, Loss: 1.7973051369190216, Final Batch Loss: 0.4051484167575836\n",
      "Epoch 3435, Loss: 1.9238423705101013, Final Batch Loss: 0.3840371370315552\n",
      "Epoch 3436, Loss: 1.724017158150673, Final Batch Loss: 0.16418768465518951\n",
      "Epoch 3437, Loss: 1.6753837168216705, Final Batch Loss: 0.34007909893989563\n",
      "Epoch 3438, Loss: 1.9506513476371765, Final Batch Loss: 0.3809676468372345\n",
      "Epoch 3439, Loss: 1.5603281557559967, Final Batch Loss: 0.3211172819137573\n",
      "Epoch 3440, Loss: 1.8978221416473389, Final Batch Loss: 0.5413365364074707\n",
      "Epoch 3441, Loss: 1.6483771800994873, Final Batch Loss: 0.3470095992088318\n",
      "Epoch 3442, Loss: 1.6830883026123047, Final Batch Loss: 0.31698423624038696\n",
      "Epoch 3443, Loss: 1.6529763340950012, Final Batch Loss: 0.3840203881263733\n",
      "Epoch 3444, Loss: 1.6775467991828918, Final Batch Loss: 0.3099476099014282\n",
      "Epoch 3445, Loss: 1.6176511347293854, Final Batch Loss: 0.2335059940814972\n",
      "Epoch 3446, Loss: 1.675120860338211, Final Batch Loss: 0.29295238852500916\n",
      "Epoch 3447, Loss: 1.609033852815628, Final Batch Loss: 0.27882930636405945\n",
      "Epoch 3448, Loss: 1.735415279865265, Final Batch Loss: 0.2770623564720154\n",
      "Epoch 3449, Loss: 1.7538829445838928, Final Batch Loss: 0.4154466688632965\n",
      "Epoch 3450, Loss: 1.7129775285720825, Final Batch Loss: 0.38218045234680176\n",
      "Epoch 3451, Loss: 1.7856774032115936, Final Batch Loss: 0.3581995368003845\n",
      "Epoch 3452, Loss: 1.7506568431854248, Final Batch Loss: 0.43181708455085754\n",
      "Epoch 3453, Loss: 1.8644707798957825, Final Batch Loss: 0.6876342296600342\n",
      "Epoch 3454, Loss: 1.7265561819076538, Final Batch Loss: 0.3060046136379242\n",
      "Epoch 3455, Loss: 1.6435653269290924, Final Batch Loss: 0.3214515149593353\n",
      "Epoch 3456, Loss: 1.5737715661525726, Final Batch Loss: 0.3002963066101074\n",
      "Epoch 3457, Loss: 1.608992099761963, Final Batch Loss: 0.2713620662689209\n",
      "Epoch 3458, Loss: 1.964287519454956, Final Batch Loss: 0.5600925087928772\n",
      "Epoch 3459, Loss: 1.8686231076717377, Final Batch Loss: 0.45303866267204285\n",
      "Epoch 3460, Loss: 1.7737330198287964, Final Batch Loss: 0.3754798471927643\n",
      "Epoch 3461, Loss: 1.436334028840065, Final Batch Loss: 0.17551814019680023\n",
      "Epoch 3462, Loss: 1.7321352362632751, Final Batch Loss: 0.4657275080680847\n",
      "Epoch 3463, Loss: 1.6297328025102615, Final Batch Loss: 0.23913995921611786\n",
      "Epoch 3464, Loss: 1.686971127986908, Final Batch Loss: 0.29987597465515137\n",
      "Epoch 3465, Loss: 1.625023975968361, Final Batch Loss: 0.22699947655200958\n",
      "Epoch 3466, Loss: 1.808731496334076, Final Batch Loss: 0.46027806401252747\n",
      "Epoch 3467, Loss: 1.8275100886821747, Final Batch Loss: 0.5376806855201721\n",
      "Epoch 3468, Loss: 1.7770887911319733, Final Batch Loss: 0.32177218794822693\n",
      "Epoch 3469, Loss: 1.7201846837997437, Final Batch Loss: 0.35778024792671204\n",
      "Epoch 3470, Loss: 1.6549678444862366, Final Batch Loss: 0.2118682563304901\n",
      "Epoch 3471, Loss: 1.7044061720371246, Final Batch Loss: 0.26934343576431274\n",
      "Epoch 3472, Loss: 1.6074551343917847, Final Batch Loss: 0.30046966671943665\n",
      "Epoch 3473, Loss: 1.5711148381233215, Final Batch Loss: 0.34537455439567566\n",
      "Epoch 3474, Loss: 1.5580479502677917, Final Batch Loss: 0.23177951574325562\n",
      "Epoch 3475, Loss: 1.5512022972106934, Final Batch Loss: 0.24638143181800842\n",
      "Epoch 3476, Loss: 1.5255035758018494, Final Batch Loss: 0.34674733877182007\n",
      "Epoch 3477, Loss: 1.4500615149736404, Final Batch Loss: 0.2034692019224167\n",
      "Epoch 3478, Loss: 1.6139947772026062, Final Batch Loss: 0.226856529712677\n",
      "Epoch 3479, Loss: 1.7779102623462677, Final Batch Loss: 0.5665968656539917\n",
      "Epoch 3480, Loss: 1.64985990524292, Final Batch Loss: 0.33102184534072876\n",
      "Epoch 3481, Loss: 1.9799190163612366, Final Batch Loss: 0.5367885231971741\n",
      "Epoch 3482, Loss: 1.5506705939769745, Final Batch Loss: 0.3443814814090729\n",
      "Epoch 3483, Loss: 1.543284609913826, Final Batch Loss: 0.21235887706279755\n",
      "Epoch 3484, Loss: 1.6304174959659576, Final Batch Loss: 0.2947627007961273\n",
      "Epoch 3485, Loss: 1.5816212594509125, Final Batch Loss: 0.3335006833076477\n",
      "Epoch 3486, Loss: 2.078093945980072, Final Batch Loss: 0.631412923336029\n",
      "Epoch 3487, Loss: 1.6338408291339874, Final Batch Loss: 0.27435070276260376\n",
      "Epoch 3488, Loss: 1.6857766211032867, Final Batch Loss: 0.3832963705062866\n",
      "Epoch 3489, Loss: 1.9384691715240479, Final Batch Loss: 0.4334011673927307\n",
      "Epoch 3490, Loss: 1.5489628911018372, Final Batch Loss: 0.20587575435638428\n",
      "Epoch 3491, Loss: 1.7264823764562607, Final Batch Loss: 0.435072660446167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3492, Loss: 1.7815883159637451, Final Batch Loss: 0.4016334116458893\n",
      "Epoch 3493, Loss: 1.7004413902759552, Final Batch Loss: 0.3676333725452423\n",
      "Epoch 3494, Loss: 1.839598298072815, Final Batch Loss: 0.45997026562690735\n",
      "Epoch 3495, Loss: 1.5393507778644562, Final Batch Loss: 0.3033770024776459\n",
      "Epoch 3496, Loss: 1.608263224363327, Final Batch Loss: 0.34861424565315247\n",
      "Epoch 3497, Loss: 1.8243007063865662, Final Batch Loss: 0.4548817276954651\n",
      "Epoch 3498, Loss: 1.831827700138092, Final Batch Loss: 0.4698236286640167\n",
      "Epoch 3499, Loss: 1.470651775598526, Final Batch Loss: 0.33640119433403015\n",
      "Epoch 3500, Loss: 1.7006695568561554, Final Batch Loss: 0.3402823507785797\n",
      "Epoch 3501, Loss: 1.7568601965904236, Final Batch Loss: 0.35589930415153503\n",
      "Epoch 3502, Loss: 1.6584390103816986, Final Batch Loss: 0.37508317828178406\n",
      "Epoch 3503, Loss: 1.4972241520881653, Final Batch Loss: 0.28357937932014465\n",
      "Epoch 3504, Loss: 1.730754941701889, Final Batch Loss: 0.41566166281700134\n",
      "Epoch 3505, Loss: 1.4193223863840103, Final Batch Loss: 0.27414724230766296\n",
      "Epoch 3506, Loss: 1.7170024514198303, Final Batch Loss: 0.36614733934402466\n",
      "Epoch 3507, Loss: 1.5589164793491364, Final Batch Loss: 0.3747434616088867\n",
      "Epoch 3508, Loss: 1.6124822497367859, Final Batch Loss: 0.32870039343833923\n",
      "Epoch 3509, Loss: 1.672514170408249, Final Batch Loss: 0.38901472091674805\n",
      "Epoch 3510, Loss: 1.6629362404346466, Final Batch Loss: 0.33751827478408813\n",
      "Epoch 3511, Loss: 1.818529188632965, Final Batch Loss: 0.38681167364120483\n",
      "Epoch 3512, Loss: 1.7119056582450867, Final Batch Loss: 0.2521531879901886\n",
      "Epoch 3513, Loss: 1.5527970045804977, Final Batch Loss: 0.33993086218833923\n",
      "Epoch 3514, Loss: 1.685223013162613, Final Batch Loss: 0.23538747429847717\n",
      "Epoch 3515, Loss: 1.698146492242813, Final Batch Loss: 0.3633121848106384\n",
      "Epoch 3516, Loss: 2.000883638858795, Final Batch Loss: 0.4327593743801117\n",
      "Epoch 3517, Loss: 1.897180199623108, Final Batch Loss: 0.3977613151073456\n",
      "Epoch 3518, Loss: 1.5954488217830658, Final Batch Loss: 0.2555038332939148\n",
      "Epoch 3519, Loss: 1.6412902474403381, Final Batch Loss: 0.31552720069885254\n",
      "Epoch 3520, Loss: 1.5729193985462189, Final Batch Loss: 0.2520830035209656\n",
      "Epoch 3521, Loss: 1.730816513299942, Final Batch Loss: 0.3993218243122101\n",
      "Epoch 3522, Loss: 1.6388068497180939, Final Batch Loss: 0.25856730341911316\n",
      "Epoch 3523, Loss: 1.8347790241241455, Final Batch Loss: 0.45932307839393616\n",
      "Epoch 3524, Loss: 1.6110316812992096, Final Batch Loss: 0.3076208829879761\n",
      "Epoch 3525, Loss: 1.579972118139267, Final Batch Loss: 0.21393367648124695\n",
      "Epoch 3526, Loss: 1.5192903131246567, Final Batch Loss: 0.23742400109767914\n",
      "Epoch 3527, Loss: 1.5825672447681427, Final Batch Loss: 0.2679041922092438\n",
      "Epoch 3528, Loss: 1.7922064065933228, Final Batch Loss: 0.3305380344390869\n",
      "Epoch 3529, Loss: 1.494327336549759, Final Batch Loss: 0.30871686339378357\n",
      "Epoch 3530, Loss: 1.6376931071281433, Final Batch Loss: 0.37237924337387085\n",
      "Epoch 3531, Loss: 1.7059986591339111, Final Batch Loss: 0.19711360335350037\n",
      "Epoch 3532, Loss: 1.677118182182312, Final Batch Loss: 0.3685479462146759\n",
      "Epoch 3533, Loss: 1.5445943474769592, Final Batch Loss: 0.38294050097465515\n",
      "Epoch 3534, Loss: 1.6894229650497437, Final Batch Loss: 0.3021022081375122\n",
      "Epoch 3535, Loss: 1.7453044056892395, Final Batch Loss: 0.5019717812538147\n",
      "Epoch 3536, Loss: 1.7286768555641174, Final Batch Loss: 0.4862680733203888\n",
      "Epoch 3537, Loss: 1.7013398110866547, Final Batch Loss: 0.37186238169670105\n",
      "Epoch 3538, Loss: 1.5306913405656815, Final Batch Loss: 0.23149751126766205\n",
      "Epoch 3539, Loss: 1.503144085407257, Final Batch Loss: 0.19660711288452148\n",
      "Epoch 3540, Loss: 1.7089455127716064, Final Batch Loss: 0.27191048860549927\n",
      "Epoch 3541, Loss: 1.5094942450523376, Final Batch Loss: 0.18485188484191895\n",
      "Epoch 3542, Loss: 1.5887731611728668, Final Batch Loss: 0.2694447636604309\n",
      "Epoch 3543, Loss: 1.5753982663154602, Final Batch Loss: 0.26768797636032104\n",
      "Epoch 3544, Loss: 1.5528258681297302, Final Batch Loss: 0.28539925813674927\n",
      "Epoch 3545, Loss: 1.6770012378692627, Final Batch Loss: 0.39532530307769775\n",
      "Epoch 3546, Loss: 1.765786498785019, Final Batch Loss: 0.3043724596500397\n",
      "Epoch 3547, Loss: 1.6398909091949463, Final Batch Loss: 0.32780370116233826\n",
      "Epoch 3548, Loss: 1.5609112977981567, Final Batch Loss: 0.23270416259765625\n",
      "Epoch 3549, Loss: 1.6648398041725159, Final Batch Loss: 0.3535991907119751\n",
      "Epoch 3550, Loss: 1.630471557378769, Final Batch Loss: 0.25589749217033386\n",
      "Epoch 3551, Loss: 1.6198922395706177, Final Batch Loss: 0.4377342462539673\n",
      "Epoch 3552, Loss: 1.5155614018440247, Final Batch Loss: 0.21857529878616333\n",
      "Epoch 3553, Loss: 1.6947643458843231, Final Batch Loss: 0.3676449656486511\n",
      "Epoch 3554, Loss: 1.6476593911647797, Final Batch Loss: 0.22407516837120056\n",
      "Epoch 3555, Loss: 1.6183517277240753, Final Batch Loss: 0.3280164301395416\n",
      "Epoch 3556, Loss: 1.5998187959194183, Final Batch Loss: 0.3662377595901489\n",
      "Epoch 3557, Loss: 1.7581802010536194, Final Batch Loss: 0.3511574864387512\n",
      "Epoch 3558, Loss: 1.7348075807094574, Final Batch Loss: 0.3610308766365051\n",
      "Epoch 3559, Loss: 1.573013961315155, Final Batch Loss: 0.2525820732116699\n",
      "Epoch 3560, Loss: 1.7682735621929169, Final Batch Loss: 0.39902448654174805\n",
      "Epoch 3561, Loss: 1.5471208989620209, Final Batch Loss: 0.2767121493816376\n",
      "Epoch 3562, Loss: 1.6667585372924805, Final Batch Loss: 0.21874761581420898\n",
      "Epoch 3563, Loss: 1.5557361543178558, Final Batch Loss: 0.3977039158344269\n",
      "Epoch 3564, Loss: 1.7751061618328094, Final Batch Loss: 0.3331605792045593\n",
      "Epoch 3565, Loss: 1.6378477811813354, Final Batch Loss: 0.41949939727783203\n",
      "Epoch 3566, Loss: 1.7414150536060333, Final Batch Loss: 0.4477340877056122\n",
      "Epoch 3567, Loss: 1.6883218437433243, Final Batch Loss: 0.4089420437812805\n",
      "Epoch 3568, Loss: 1.5989283323287964, Final Batch Loss: 0.3311340808868408\n",
      "Epoch 3569, Loss: 1.694623351097107, Final Batch Loss: 0.43289634585380554\n",
      "Epoch 3570, Loss: 1.524468332529068, Final Batch Loss: 0.1640993058681488\n",
      "Epoch 3571, Loss: 1.6220262944698334, Final Batch Loss: 0.28861004114151\n",
      "Epoch 3572, Loss: 1.684416949748993, Final Batch Loss: 0.31054702401161194\n",
      "Epoch 3573, Loss: 1.71061772108078, Final Batch Loss: 0.44266384840011597\n",
      "Epoch 3574, Loss: 1.6384915262460709, Final Batch Loss: 0.26527833938598633\n",
      "Epoch 3575, Loss: 1.829364001750946, Final Batch Loss: 0.4105251133441925\n",
      "Epoch 3576, Loss: 1.4381228387355804, Final Batch Loss: 0.2899317443370819\n",
      "Epoch 3577, Loss: 1.6839305758476257, Final Batch Loss: 0.42760810256004333\n",
      "Epoch 3578, Loss: 1.5685414671897888, Final Batch Loss: 0.26951491832733154\n",
      "Epoch 3579, Loss: 1.6405693888664246, Final Batch Loss: 0.31529971957206726\n",
      "Epoch 3580, Loss: 1.762365698814392, Final Batch Loss: 0.40460580587387085\n",
      "Epoch 3581, Loss: 1.6406139731407166, Final Batch Loss: 0.39008694887161255\n",
      "Epoch 3582, Loss: 1.896373838186264, Final Batch Loss: 0.48572981357574463\n",
      "Epoch 3583, Loss: 1.6825424432754517, Final Batch Loss: 0.2560417652130127\n",
      "Epoch 3584, Loss: 1.6751348674297333, Final Batch Loss: 0.35137051343917847\n",
      "Epoch 3585, Loss: 1.462623804807663, Final Batch Loss: 0.3025500476360321\n",
      "Epoch 3586, Loss: 2.7604042291641235, Final Batch Loss: 1.4567029476165771\n",
      "Epoch 3587, Loss: 1.7240312993526459, Final Batch Loss: 0.48875892162323\n",
      "Epoch 3588, Loss: 1.6349323689937592, Final Batch Loss: 0.18391141295433044\n",
      "Epoch 3589, Loss: 1.8792961239814758, Final Batch Loss: 0.5156087279319763\n",
      "Epoch 3590, Loss: 1.8303203582763672, Final Batch Loss: 0.41102397441864014\n",
      "Epoch 3591, Loss: 1.6033557951450348, Final Batch Loss: 0.22119110822677612\n",
      "Epoch 3592, Loss: 1.9168336689472198, Final Batch Loss: 0.48187047243118286\n",
      "Epoch 3593, Loss: 1.6206453144550323, Final Batch Loss: 0.2612649202346802\n",
      "Epoch 3594, Loss: 1.6418090015649796, Final Batch Loss: 0.20214898884296417\n",
      "Epoch 3595, Loss: 1.5498023182153702, Final Batch Loss: 0.32405370473861694\n",
      "Epoch 3596, Loss: 1.6617542207241058, Final Batch Loss: 0.2735311686992645\n",
      "Epoch 3597, Loss: 1.7171669006347656, Final Batch Loss: 0.3394313454627991\n",
      "Epoch 3598, Loss: 1.686434268951416, Final Batch Loss: 0.4634709060192108\n",
      "Epoch 3599, Loss: 1.5537693500518799, Final Batch Loss: 0.2896379232406616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3600, Loss: 1.550413578748703, Final Batch Loss: 0.3529377579689026\n",
      "Epoch 3601, Loss: 1.7808356881141663, Final Batch Loss: 0.3100820481777191\n",
      "Epoch 3602, Loss: 1.8949872553348541, Final Batch Loss: 0.3959629535675049\n",
      "Epoch 3603, Loss: 1.5202081352472305, Final Batch Loss: 0.2383139282464981\n",
      "Epoch 3604, Loss: 1.608982890844345, Final Batch Loss: 0.3389771282672882\n",
      "Epoch 3605, Loss: 1.5481813848018646, Final Batch Loss: 0.2697957754135132\n",
      "Epoch 3606, Loss: 1.6600503772497177, Final Batch Loss: 0.4395868480205536\n",
      "Epoch 3607, Loss: 1.7670215368270874, Final Batch Loss: 0.34091174602508545\n",
      "Epoch 3608, Loss: 1.7325270473957062, Final Batch Loss: 0.36670899391174316\n",
      "Epoch 3609, Loss: 1.6495984196662903, Final Batch Loss: 0.29415208101272583\n",
      "Epoch 3610, Loss: 1.5661976039409637, Final Batch Loss: 0.34504234790802\n",
      "Epoch 3611, Loss: 1.6833215951919556, Final Batch Loss: 0.3914320170879364\n",
      "Epoch 3612, Loss: 1.4876711964607239, Final Batch Loss: 0.24638590216636658\n",
      "Epoch 3613, Loss: 1.5318565368652344, Final Batch Loss: 0.2551124691963196\n",
      "Epoch 3614, Loss: 1.5816313028335571, Final Batch Loss: 0.26349788904190063\n",
      "Epoch 3615, Loss: 1.9297277927398682, Final Batch Loss: 0.36833322048187256\n",
      "Epoch 3616, Loss: 1.5802991092205048, Final Batch Loss: 0.2745026648044586\n",
      "Epoch 3617, Loss: 1.7357477843761444, Final Batch Loss: 0.3829951286315918\n",
      "Epoch 3618, Loss: 1.4917057156562805, Final Batch Loss: 0.27623745799064636\n",
      "Epoch 3619, Loss: 1.7201615869998932, Final Batch Loss: 0.32976090908050537\n",
      "Epoch 3620, Loss: 1.642040029168129, Final Batch Loss: 0.24456293880939484\n",
      "Epoch 3621, Loss: 1.7311510890722275, Final Batch Loss: 0.23295055329799652\n",
      "Epoch 3622, Loss: 1.5063305795192719, Final Batch Loss: 0.3144494295120239\n",
      "Epoch 3623, Loss: 1.6705913841724396, Final Batch Loss: 0.3768746256828308\n",
      "Epoch 3624, Loss: 1.6223994195461273, Final Batch Loss: 0.3427129089832306\n",
      "Epoch 3625, Loss: 1.6354027390480042, Final Batch Loss: 0.34379616379737854\n",
      "Epoch 3626, Loss: 1.5942495465278625, Final Batch Loss: 0.2995014786720276\n",
      "Epoch 3627, Loss: 1.665412575006485, Final Batch Loss: 0.3035835325717926\n",
      "Epoch 3628, Loss: 1.688619315624237, Final Batch Loss: 0.27535250782966614\n",
      "Epoch 3629, Loss: 1.4302614033222198, Final Batch Loss: 0.2380562126636505\n",
      "Epoch 3630, Loss: 1.6473619937896729, Final Batch Loss: 0.3896513283252716\n",
      "Epoch 3631, Loss: 1.7091323137283325, Final Batch Loss: 0.29038065671920776\n",
      "Epoch 3632, Loss: 1.7623966932296753, Final Batch Loss: 0.4360981583595276\n",
      "Epoch 3633, Loss: 1.7326444685459137, Final Batch Loss: 0.4055255949497223\n",
      "Epoch 3634, Loss: 1.7297770082950592, Final Batch Loss: 0.43109843134880066\n",
      "Epoch 3635, Loss: 1.5162533819675446, Final Batch Loss: 0.2992759346961975\n",
      "Epoch 3636, Loss: 1.4302594363689423, Final Batch Loss: 0.1728406697511673\n",
      "Epoch 3637, Loss: 1.6832565367221832, Final Batch Loss: 0.38375601172447205\n",
      "Epoch 3638, Loss: 1.8910138607025146, Final Batch Loss: 0.5509532690048218\n",
      "Epoch 3639, Loss: 1.6729037165641785, Final Batch Loss: 0.45529675483703613\n",
      "Epoch 3640, Loss: 1.523092806339264, Final Batch Loss: 0.2871662974357605\n",
      "Epoch 3641, Loss: 1.5652770549058914, Final Batch Loss: 0.23231573402881622\n",
      "Epoch 3642, Loss: 1.6417286098003387, Final Batch Loss: 0.38398995995521545\n",
      "Epoch 3643, Loss: 1.614927887916565, Final Batch Loss: 0.274926096200943\n",
      "Epoch 3644, Loss: 1.6125735342502594, Final Batch Loss: 0.2785964906215668\n",
      "Epoch 3645, Loss: 1.6440848112106323, Final Batch Loss: 0.3557389974594116\n",
      "Epoch 3646, Loss: 1.675696074962616, Final Batch Loss: 0.34692418575286865\n",
      "Epoch 3647, Loss: 1.5449799448251724, Final Batch Loss: 0.24200771749019623\n",
      "Epoch 3648, Loss: 1.6511822938919067, Final Batch Loss: 0.4474153518676758\n",
      "Epoch 3649, Loss: 1.541641354560852, Final Batch Loss: 0.30435046553611755\n",
      "Epoch 3650, Loss: 1.610165685415268, Final Batch Loss: 0.3255389630794525\n",
      "Epoch 3651, Loss: 1.5370705127716064, Final Batch Loss: 0.30139562487602234\n",
      "Epoch 3652, Loss: 1.543155997991562, Final Batch Loss: 0.23778215050697327\n",
      "Epoch 3653, Loss: 1.632070779800415, Final Batch Loss: 0.2642412483692169\n",
      "Epoch 3654, Loss: 1.5003861486911774, Final Batch Loss: 0.2559798061847687\n",
      "Epoch 3655, Loss: 1.5969191640615463, Final Batch Loss: 0.2379213124513626\n",
      "Epoch 3656, Loss: 1.6641609370708466, Final Batch Loss: 0.28361332416534424\n",
      "Epoch 3657, Loss: 1.6099407970905304, Final Batch Loss: 0.32973942160606384\n",
      "Epoch 3658, Loss: 1.8356705009937286, Final Batch Loss: 0.5726111531257629\n",
      "Epoch 3659, Loss: 1.6779552102088928, Final Batch Loss: 0.4323268234729767\n",
      "Epoch 3660, Loss: 1.441525787115097, Final Batch Loss: 0.24433936178684235\n",
      "Epoch 3661, Loss: 1.5851299613714218, Final Batch Loss: 0.4666147530078888\n",
      "Epoch 3662, Loss: 1.6424342691898346, Final Batch Loss: 0.2973051369190216\n",
      "Epoch 3663, Loss: 1.5771017968654633, Final Batch Loss: 0.3317066431045532\n",
      "Epoch 3664, Loss: 1.683683305978775, Final Batch Loss: 0.3529963791370392\n",
      "Epoch 3665, Loss: 1.9310640394687653, Final Batch Loss: 0.35721415281295776\n",
      "Epoch 3666, Loss: 1.6235471069812775, Final Batch Loss: 0.34337106347084045\n",
      "Epoch 3667, Loss: 1.6874055564403534, Final Batch Loss: 0.37868624925613403\n",
      "Epoch 3668, Loss: 1.60581436753273, Final Batch Loss: 0.3745367228984833\n",
      "Epoch 3669, Loss: 1.592210203409195, Final Batch Loss: 0.19328293204307556\n",
      "Epoch 3670, Loss: 1.4836901724338531, Final Batch Loss: 0.24454379081726074\n",
      "Epoch 3671, Loss: 1.7012661397457123, Final Batch Loss: 0.3091408610343933\n",
      "Epoch 3672, Loss: 1.5733027160167694, Final Batch Loss: 0.27942100167274475\n",
      "Epoch 3673, Loss: 1.6419352293014526, Final Batch Loss: 0.3318273425102234\n",
      "Epoch 3674, Loss: 1.8906685709953308, Final Batch Loss: 0.5685992240905762\n",
      "Epoch 3675, Loss: 1.5580552220344543, Final Batch Loss: 0.26193803548812866\n",
      "Epoch 3676, Loss: 1.632048100233078, Final Batch Loss: 0.3186609447002411\n",
      "Epoch 3677, Loss: 1.6052265018224716, Final Batch Loss: 0.2198651283979416\n",
      "Epoch 3678, Loss: 1.5856105089187622, Final Batch Loss: 0.4104037582874298\n",
      "Epoch 3679, Loss: 1.6906485259532928, Final Batch Loss: 0.2619135081768036\n",
      "Epoch 3680, Loss: 1.493093103170395, Final Batch Loss: 0.30497124791145325\n",
      "Epoch 3681, Loss: 1.686032697558403, Final Batch Loss: 0.2884398102760315\n",
      "Epoch 3682, Loss: 1.650615781545639, Final Batch Loss: 0.3248581290245056\n",
      "Epoch 3683, Loss: 1.7865645289421082, Final Batch Loss: 0.448547899723053\n",
      "Epoch 3684, Loss: 1.7660747170448303, Final Batch Loss: 0.37113630771636963\n",
      "Epoch 3685, Loss: 1.4619548618793488, Final Batch Loss: 0.19617575407028198\n",
      "Epoch 3686, Loss: 1.6020552515983582, Final Batch Loss: 0.33688223361968994\n",
      "Epoch 3687, Loss: 1.6534403860569, Final Batch Loss: 0.38850441575050354\n",
      "Epoch 3688, Loss: 1.5778752267360687, Final Batch Loss: 0.3349301815032959\n",
      "Epoch 3689, Loss: 1.4581232219934464, Final Batch Loss: 0.1890053004026413\n",
      "Epoch 3690, Loss: 2.0896091163158417, Final Batch Loss: 0.6595402956008911\n",
      "Epoch 3691, Loss: 1.6317150592803955, Final Batch Loss: 0.3074452877044678\n",
      "Epoch 3692, Loss: 1.654725968837738, Final Batch Loss: 0.3191649317741394\n",
      "Epoch 3693, Loss: 1.4300542920827866, Final Batch Loss: 0.2735879123210907\n",
      "Epoch 3694, Loss: 1.518935814499855, Final Batch Loss: 0.21198724210262299\n",
      "Epoch 3695, Loss: 1.7886584401130676, Final Batch Loss: 0.3197978734970093\n",
      "Epoch 3696, Loss: 1.510751724243164, Final Batch Loss: 0.3000929057598114\n",
      "Epoch 3697, Loss: 1.5736688673496246, Final Batch Loss: 0.3402160406112671\n",
      "Epoch 3698, Loss: 1.661341369152069, Final Batch Loss: 0.3337482213973999\n",
      "Epoch 3699, Loss: 1.5634135901927948, Final Batch Loss: 0.29524603486061096\n",
      "Epoch 3700, Loss: 1.9904616177082062, Final Batch Loss: 0.43652379512786865\n",
      "Epoch 3701, Loss: 1.7625778913497925, Final Batch Loss: 0.35388481616973877\n",
      "Epoch 3702, Loss: 1.9356509447097778, Final Batch Loss: 0.4466652274131775\n",
      "Epoch 3703, Loss: 1.736605316400528, Final Batch Loss: 0.30121976137161255\n",
      "Epoch 3704, Loss: 1.7233871817588806, Final Batch Loss: 0.3520408868789673\n",
      "Epoch 3705, Loss: 1.647038072347641, Final Batch Loss: 0.3309932351112366\n",
      "Epoch 3706, Loss: 1.7365003526210785, Final Batch Loss: 0.35588666796684265\n",
      "Epoch 3707, Loss: 1.532851293683052, Final Batch Loss: 0.21121250092983246\n",
      "Epoch 3708, Loss: 1.8005293607711792, Final Batch Loss: 0.5354238748550415\n",
      "Epoch 3709, Loss: 1.6031773388385773, Final Batch Loss: 0.31350117921829224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3710, Loss: 1.8930518627166748, Final Batch Loss: 0.3648349344730377\n",
      "Epoch 3711, Loss: 1.7361651957035065, Final Batch Loss: 0.3383866846561432\n",
      "Epoch 3712, Loss: 1.925444334745407, Final Batch Loss: 0.4688640534877777\n",
      "Epoch 3713, Loss: 1.6832483410835266, Final Batch Loss: 0.37452322244644165\n",
      "Epoch 3714, Loss: 1.596593290567398, Final Batch Loss: 0.27111485600471497\n",
      "Epoch 3715, Loss: 1.512708604335785, Final Batch Loss: 0.232193261384964\n",
      "Epoch 3716, Loss: 1.3564105182886124, Final Batch Loss: 0.1926998794078827\n",
      "Epoch 3717, Loss: 1.8863376080989838, Final Batch Loss: 0.4187627136707306\n",
      "Epoch 3718, Loss: 1.5391606986522675, Final Batch Loss: 0.14946836233139038\n",
      "Epoch 3719, Loss: 2.0751723647117615, Final Batch Loss: 0.6880441904067993\n",
      "Epoch 3720, Loss: 1.918567955493927, Final Batch Loss: 0.5313977599143982\n",
      "Epoch 3721, Loss: 1.60312220454216, Final Batch Loss: 0.390392929315567\n",
      "Epoch 3722, Loss: 1.64593967795372, Final Batch Loss: 0.28907617926597595\n",
      "Epoch 3723, Loss: 1.4841579347848892, Final Batch Loss: 0.2226538509130478\n",
      "Epoch 3724, Loss: 1.464916467666626, Final Batch Loss: 0.29229485988616943\n",
      "Epoch 3725, Loss: 1.6895387470722198, Final Batch Loss: 0.44530412554740906\n",
      "Epoch 3726, Loss: 1.9622041285037994, Final Batch Loss: 0.5528345704078674\n",
      "Epoch 3727, Loss: 1.6972323954105377, Final Batch Loss: 0.44718852639198303\n",
      "Epoch 3728, Loss: 1.4787175059318542, Final Batch Loss: 0.2153787910938263\n",
      "Epoch 3729, Loss: 1.5261497795581818, Final Batch Loss: 0.26249217987060547\n",
      "Epoch 3730, Loss: 1.3983311206102371, Final Batch Loss: 0.2882516384124756\n",
      "Epoch 3731, Loss: 1.580829232931137, Final Batch Loss: 0.3195301294326782\n",
      "Epoch 3732, Loss: 1.5761156380176544, Final Batch Loss: 0.21273845434188843\n",
      "Epoch 3733, Loss: 1.5558368265628815, Final Batch Loss: 0.2615445852279663\n",
      "Epoch 3734, Loss: 1.5980573296546936, Final Batch Loss: 0.4251112639904022\n",
      "Epoch 3735, Loss: 1.7153300940990448, Final Batch Loss: 0.19122132658958435\n",
      "Epoch 3736, Loss: 1.7691960632801056, Final Batch Loss: 0.47947579622268677\n",
      "Epoch 3737, Loss: 1.6473852694034576, Final Batch Loss: 0.28381770849227905\n",
      "Epoch 3738, Loss: 1.780069500207901, Final Batch Loss: 0.49336421489715576\n",
      "Epoch 3739, Loss: 1.4937567114830017, Final Batch Loss: 0.248113751411438\n",
      "Epoch 3740, Loss: 1.6450958102941513, Final Batch Loss: 0.31197217106819153\n",
      "Epoch 3741, Loss: 1.9853128492832184, Final Batch Loss: 0.6002832651138306\n",
      "Epoch 3742, Loss: 1.5936205834150314, Final Batch Loss: 0.20610608160495758\n",
      "Epoch 3743, Loss: 1.832702100276947, Final Batch Loss: 0.4070386290550232\n",
      "Epoch 3744, Loss: 1.5746830701828003, Final Batch Loss: 0.2548315227031708\n",
      "Epoch 3745, Loss: 1.7628813982009888, Final Batch Loss: 0.33945438265800476\n",
      "Epoch 3746, Loss: 1.5471419543027878, Final Batch Loss: 0.16404838860034943\n",
      "Epoch 3747, Loss: 1.5770980417728424, Final Batch Loss: 0.3515702188014984\n",
      "Epoch 3748, Loss: 1.4083632230758667, Final Batch Loss: 0.2920440137386322\n",
      "Epoch 3749, Loss: 1.5596606135368347, Final Batch Loss: 0.2880483865737915\n",
      "Epoch 3750, Loss: 1.5928359180688858, Final Batch Loss: 0.20045365393161774\n",
      "Epoch 3751, Loss: 1.410147801041603, Final Batch Loss: 0.185573011636734\n",
      "Epoch 3752, Loss: 1.6674708724021912, Final Batch Loss: 0.37610816955566406\n",
      "Epoch 3753, Loss: 1.501196265220642, Final Batch Loss: 0.2620537579059601\n",
      "Epoch 3754, Loss: 1.8374953866004944, Final Batch Loss: 0.3171220123767853\n",
      "Epoch 3755, Loss: 1.5747963637113571, Final Batch Loss: 0.32422831654548645\n",
      "Epoch 3756, Loss: 1.5714669525623322, Final Batch Loss: 0.2905956506729126\n",
      "Epoch 3757, Loss: 1.8078050911426544, Final Batch Loss: 0.4127843976020813\n",
      "Epoch 3758, Loss: 1.6981472074985504, Final Batch Loss: 0.34493178129196167\n",
      "Epoch 3759, Loss: 1.95392307639122, Final Batch Loss: 0.6497881412506104\n",
      "Epoch 3760, Loss: 1.7039388716220856, Final Batch Loss: 0.2643386423587799\n",
      "Epoch 3761, Loss: 1.4096912741661072, Final Batch Loss: 0.21262064576148987\n",
      "Epoch 3762, Loss: 1.5462187230587006, Final Batch Loss: 0.31319960951805115\n",
      "Epoch 3763, Loss: 1.6031480729579926, Final Batch Loss: 0.3196773827075958\n",
      "Epoch 3764, Loss: 1.6009981334209442, Final Batch Loss: 0.35308054089546204\n",
      "Epoch 3765, Loss: 1.7304242253303528, Final Batch Loss: 0.3986639082431793\n",
      "Epoch 3766, Loss: 1.6865762174129486, Final Batch Loss: 0.3365325331687927\n",
      "Epoch 3767, Loss: 1.7948413491249084, Final Batch Loss: 0.354561448097229\n",
      "Epoch 3768, Loss: 1.5284886360168457, Final Batch Loss: 0.3440084457397461\n",
      "Epoch 3769, Loss: 1.6172176599502563, Final Batch Loss: 0.2532053291797638\n",
      "Epoch 3770, Loss: 1.4016480147838593, Final Batch Loss: 0.1823497712612152\n",
      "Epoch 3771, Loss: 1.5920026302337646, Final Batch Loss: 0.27213144302368164\n",
      "Epoch 3772, Loss: 1.734226405620575, Final Batch Loss: 0.42190223932266235\n",
      "Epoch 3773, Loss: 1.6469084322452545, Final Batch Loss: 0.35457101464271545\n",
      "Epoch 3774, Loss: 1.7161640524864197, Final Batch Loss: 0.27365657687187195\n",
      "Epoch 3775, Loss: 1.6548239588737488, Final Batch Loss: 0.30029648542404175\n",
      "Epoch 3776, Loss: 1.610956221818924, Final Batch Loss: 0.36651721596717834\n",
      "Epoch 3777, Loss: 1.770541101694107, Final Batch Loss: 0.3632947504520416\n",
      "Epoch 3778, Loss: 1.7221198678016663, Final Batch Loss: 0.36488696932792664\n",
      "Epoch 3779, Loss: 1.6170580387115479, Final Batch Loss: 0.3491910398006439\n",
      "Epoch 3780, Loss: 1.540379449725151, Final Batch Loss: 0.19227783381938934\n",
      "Epoch 3781, Loss: 1.6016438901424408, Final Batch Loss: 0.16860085725784302\n",
      "Epoch 3782, Loss: 1.7433272898197174, Final Batch Loss: 0.3075251579284668\n",
      "Epoch 3783, Loss: 1.6456187963485718, Final Batch Loss: 0.35085996985435486\n",
      "Epoch 3784, Loss: 1.5814447700977325, Final Batch Loss: 0.2946816682815552\n",
      "Epoch 3785, Loss: 1.708662748336792, Final Batch Loss: 0.38982918858528137\n",
      "Epoch 3786, Loss: 1.3739822953939438, Final Batch Loss: 0.21523191034793854\n",
      "Epoch 3787, Loss: 1.7327147424221039, Final Batch Loss: 0.38956472277641296\n",
      "Epoch 3788, Loss: 1.7908011972904205, Final Batch Loss: 0.32778334617614746\n",
      "Epoch 3789, Loss: 1.541293442249298, Final Batch Loss: 0.3308432996273041\n",
      "Epoch 3790, Loss: 1.6351886093616486, Final Batch Loss: 0.42738041281700134\n",
      "Epoch 3791, Loss: 1.4997618347406387, Final Batch Loss: 0.23173563182353973\n",
      "Epoch 3792, Loss: 1.5916551053524017, Final Batch Loss: 0.3023199737071991\n",
      "Epoch 3793, Loss: 1.5532252490520477, Final Batch Loss: 0.27134326100349426\n",
      "Epoch 3794, Loss: 1.5518643707036972, Final Batch Loss: 0.3766297996044159\n",
      "Epoch 3795, Loss: 1.503185287117958, Final Batch Loss: 0.21671254932880402\n",
      "Epoch 3796, Loss: 1.8626459240913391, Final Batch Loss: 0.30660584568977356\n",
      "Epoch 3797, Loss: 1.5783413648605347, Final Batch Loss: 0.2393750250339508\n",
      "Epoch 3798, Loss: 1.580445185303688, Final Batch Loss: 0.36068981885910034\n",
      "Epoch 3799, Loss: 1.4027111381292343, Final Batch Loss: 0.17676298320293427\n",
      "Epoch 3800, Loss: 1.5580273270606995, Final Batch Loss: 0.3530026972293854\n",
      "Epoch 3801, Loss: 1.6844300031661987, Final Batch Loss: 0.43723011016845703\n",
      "Epoch 3802, Loss: 1.7212169915437698, Final Batch Loss: 0.5383619666099548\n",
      "Epoch 3803, Loss: 1.5842543989419937, Final Batch Loss: 0.2393004447221756\n",
      "Epoch 3804, Loss: 1.6202288269996643, Final Batch Loss: 0.28322169184684753\n",
      "Epoch 3805, Loss: 1.739604264497757, Final Batch Loss: 0.3626202940940857\n",
      "Epoch 3806, Loss: 1.400310918688774, Final Batch Loss: 0.2693461775779724\n",
      "Epoch 3807, Loss: 1.5375210642814636, Final Batch Loss: 0.30032145977020264\n",
      "Epoch 3808, Loss: 1.6440497040748596, Final Batch Loss: 0.35132160782814026\n",
      "Epoch 3809, Loss: 1.5631902366876602, Final Batch Loss: 0.24224592745304108\n",
      "Epoch 3810, Loss: 1.8140624165534973, Final Batch Loss: 0.6025184392929077\n",
      "Epoch 3811, Loss: 1.4822446256875992, Final Batch Loss: 0.3417287766933441\n",
      "Epoch 3812, Loss: 1.5464828163385391, Final Batch Loss: 0.1913202553987503\n",
      "Epoch 3813, Loss: 1.4152585566043854, Final Batch Loss: 0.2759103775024414\n",
      "Epoch 3814, Loss: 1.6228680610656738, Final Batch Loss: 0.41112804412841797\n",
      "Epoch 3815, Loss: 1.5838904231786728, Final Batch Loss: 0.21987219154834747\n",
      "Epoch 3816, Loss: 1.4872155785560608, Final Batch Loss: 0.24816471338272095\n",
      "Epoch 3817, Loss: 1.5407036542892456, Final Batch Loss: 0.46270304918289185\n",
      "Epoch 3818, Loss: 1.4821873307228088, Final Batch Loss: 0.25968340039253235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3819, Loss: 1.727467656135559, Final Batch Loss: 0.4164750277996063\n",
      "Epoch 3820, Loss: 1.8538173586130142, Final Batch Loss: 0.6084669828414917\n",
      "Epoch 3821, Loss: 1.4983175992965698, Final Batch Loss: 0.224162757396698\n",
      "Epoch 3822, Loss: 1.551719143986702, Final Batch Loss: 0.2173682302236557\n",
      "Epoch 3823, Loss: 1.6383596807718277, Final Batch Loss: 0.22749750316143036\n",
      "Epoch 3824, Loss: 1.7726887464523315, Final Batch Loss: 0.36535587906837463\n",
      "Epoch 3825, Loss: 1.624022215604782, Final Batch Loss: 0.2854510545730591\n",
      "Epoch 3826, Loss: 1.5454526543617249, Final Batch Loss: 0.3309626281261444\n",
      "Epoch 3827, Loss: 1.579219102859497, Final Batch Loss: 0.33461540937423706\n",
      "Epoch 3828, Loss: 1.6440368294715881, Final Batch Loss: 0.25734493136405945\n",
      "Epoch 3829, Loss: 1.5642379522323608, Final Batch Loss: 0.33699384331703186\n",
      "Epoch 3830, Loss: 1.586301639676094, Final Batch Loss: 0.3018997311592102\n",
      "Epoch 3831, Loss: 1.6223287880420685, Final Batch Loss: 0.3033544719219208\n",
      "Epoch 3832, Loss: 1.4621028900146484, Final Batch Loss: 0.3417532444000244\n",
      "Epoch 3833, Loss: 1.9377288967370987, Final Batch Loss: 0.21761025488376617\n",
      "Epoch 3834, Loss: 1.480432614684105, Final Batch Loss: 0.3757129907608032\n",
      "Epoch 3835, Loss: 1.6102569699287415, Final Batch Loss: 0.3849276900291443\n",
      "Epoch 3836, Loss: 1.6315831243991852, Final Batch Loss: 0.3991268277168274\n",
      "Epoch 3837, Loss: 1.6418929994106293, Final Batch Loss: 0.33265212178230286\n",
      "Epoch 3838, Loss: 1.5601317137479782, Final Batch Loss: 0.2452324479818344\n",
      "Epoch 3839, Loss: 1.6835406124591827, Final Batch Loss: 0.3171745538711548\n",
      "Epoch 3840, Loss: 1.6301053166389465, Final Batch Loss: 0.2878012955188751\n",
      "Epoch 3841, Loss: 1.6157079935073853, Final Batch Loss: 0.35387924313545227\n",
      "Epoch 3842, Loss: 1.535348355770111, Final Batch Loss: 0.3364955484867096\n",
      "Epoch 3843, Loss: 1.5594969540834427, Final Batch Loss: 0.3007601499557495\n",
      "Epoch 3844, Loss: 1.4287921041250229, Final Batch Loss: 0.2670040726661682\n",
      "Epoch 3845, Loss: 1.5680604577064514, Final Batch Loss: 0.3630554676055908\n",
      "Epoch 3846, Loss: 1.600322648882866, Final Batch Loss: 0.2922152876853943\n",
      "Epoch 3847, Loss: 1.4076786041259766, Final Batch Loss: 0.25997066497802734\n",
      "Epoch 3848, Loss: 1.6067009270191193, Final Batch Loss: 0.44002336263656616\n",
      "Epoch 3849, Loss: 1.3905100375413895, Final Batch Loss: 0.19862793385982513\n",
      "Epoch 3850, Loss: 1.954101175069809, Final Batch Loss: 0.5960494875907898\n",
      "Epoch 3851, Loss: 1.6862190663814545, Final Batch Loss: 0.47809576988220215\n",
      "Epoch 3852, Loss: 1.6913118958473206, Final Batch Loss: 0.3908843994140625\n",
      "Epoch 3853, Loss: 1.6011120975017548, Final Batch Loss: 0.3589419722557068\n",
      "Epoch 3854, Loss: 1.4966717958450317, Final Batch Loss: 0.30799904465675354\n",
      "Epoch 3855, Loss: 1.692860096693039, Final Batch Loss: 0.357827365398407\n",
      "Epoch 3856, Loss: 1.5457813888788223, Final Batch Loss: 0.24474704265594482\n",
      "Epoch 3857, Loss: 1.5262760818004608, Final Batch Loss: 0.3282794952392578\n",
      "Epoch 3858, Loss: 1.703748345375061, Final Batch Loss: 0.44445958733558655\n",
      "Epoch 3859, Loss: 1.465268537402153, Final Batch Loss: 0.3266519606113434\n",
      "Epoch 3860, Loss: 1.4746250808238983, Final Batch Loss: 0.3756810128688812\n",
      "Epoch 3861, Loss: 1.5849930346012115, Final Batch Loss: 0.2580919563770294\n",
      "Epoch 3862, Loss: 1.7017783522605896, Final Batch Loss: 0.39737236499786377\n",
      "Epoch 3863, Loss: 1.816009521484375, Final Batch Loss: 0.3501555919647217\n",
      "Epoch 3864, Loss: 1.4900689274072647, Final Batch Loss: 0.23906777799129486\n",
      "Epoch 3865, Loss: 1.4846501052379608, Final Batch Loss: 0.23977121710777283\n",
      "Epoch 3866, Loss: 1.4955479353666306, Final Batch Loss: 0.16528747975826263\n",
      "Epoch 3867, Loss: 1.6190169155597687, Final Batch Loss: 0.30934253334999084\n",
      "Epoch 3868, Loss: 1.6360062211751938, Final Batch Loss: 0.42534878849983215\n",
      "Epoch 3869, Loss: 1.5185132324695587, Final Batch Loss: 0.18824425339698792\n",
      "Epoch 3870, Loss: 1.6473594009876251, Final Batch Loss: 0.3379540741443634\n",
      "Epoch 3871, Loss: 1.482755035161972, Final Batch Loss: 0.19504866003990173\n",
      "Epoch 3872, Loss: 1.5508400797843933, Final Batch Loss: 0.27544164657592773\n",
      "Epoch 3873, Loss: 1.411795899271965, Final Batch Loss: 0.20736312866210938\n",
      "Epoch 3874, Loss: 1.7057251632213593, Final Batch Loss: 0.35508716106414795\n",
      "Epoch 3875, Loss: 1.4003402888774872, Final Batch Loss: 0.2505112290382385\n",
      "Epoch 3876, Loss: 1.507064938545227, Final Batch Loss: 0.35991719365119934\n",
      "Epoch 3877, Loss: 1.6344791054725647, Final Batch Loss: 0.30147501826286316\n",
      "Epoch 3878, Loss: 1.4608164280653, Final Batch Loss: 0.3532256484031677\n",
      "Epoch 3879, Loss: 1.7121683657169342, Final Batch Loss: 0.3414480686187744\n",
      "Epoch 3880, Loss: 1.602256491780281, Final Batch Loss: 0.3093968629837036\n",
      "Epoch 3881, Loss: 1.4496860057115555, Final Batch Loss: 0.34379279613494873\n",
      "Epoch 3882, Loss: 1.6297984421253204, Final Batch Loss: 0.3212085962295532\n",
      "Epoch 3883, Loss: 1.7286235094070435, Final Batch Loss: 0.3250824809074402\n",
      "Epoch 3884, Loss: 1.6130020469427109, Final Batch Loss: 0.23682542145252228\n",
      "Epoch 3885, Loss: 1.5121730268001556, Final Batch Loss: 0.23057779669761658\n",
      "Epoch 3886, Loss: 1.7615749835968018, Final Batch Loss: 0.35570278763771057\n",
      "Epoch 3887, Loss: 1.2795029431581497, Final Batch Loss: 0.13725189864635468\n",
      "Epoch 3888, Loss: 1.5382135808467865, Final Batch Loss: 0.2440851330757141\n",
      "Epoch 3889, Loss: 1.5105197727680206, Final Batch Loss: 0.30187422037124634\n",
      "Epoch 3890, Loss: 1.4313030540943146, Final Batch Loss: 0.2914902865886688\n",
      "Epoch 3891, Loss: 1.534859538078308, Final Batch Loss: 0.4276878535747528\n",
      "Epoch 3892, Loss: 1.7846062779426575, Final Batch Loss: 0.2500007748603821\n",
      "Epoch 3893, Loss: 1.655696451663971, Final Batch Loss: 0.3790373206138611\n",
      "Epoch 3894, Loss: 1.639052927494049, Final Batch Loss: 0.28231167793273926\n",
      "Epoch 3895, Loss: 1.6447088420391083, Final Batch Loss: 0.2955462336540222\n",
      "Epoch 3896, Loss: 1.4718316346406937, Final Batch Loss: 0.22269384562969208\n",
      "Epoch 3897, Loss: 1.614304631948471, Final Batch Loss: 0.3378550410270691\n",
      "Epoch 3898, Loss: 1.5511091649532318, Final Batch Loss: 0.2541205585002899\n",
      "Epoch 3899, Loss: 1.7632095515727997, Final Batch Loss: 0.39326542615890503\n",
      "Epoch 3900, Loss: 1.378934308886528, Final Batch Loss: 0.19281400740146637\n",
      "Epoch 3901, Loss: 1.5748569965362549, Final Batch Loss: 0.28438153862953186\n",
      "Epoch 3902, Loss: 1.6874176561832428, Final Batch Loss: 0.3797109127044678\n",
      "Epoch 3903, Loss: 1.493940219283104, Final Batch Loss: 0.15985207259655\n",
      "Epoch 3904, Loss: 1.8283179104328156, Final Batch Loss: 0.2678552269935608\n",
      "Epoch 3905, Loss: 1.680404245853424, Final Batch Loss: 0.3632698357105255\n",
      "Epoch 3906, Loss: 1.9357002079486847, Final Batch Loss: 0.5143969058990479\n",
      "Epoch 3907, Loss: 1.61760875582695, Final Batch Loss: 0.39204058051109314\n",
      "Epoch 3908, Loss: 1.5095345228910446, Final Batch Loss: 0.24738244712352753\n",
      "Epoch 3909, Loss: 1.5029137432575226, Final Batch Loss: 0.17849019169807434\n",
      "Epoch 3910, Loss: 1.567510962486267, Final Batch Loss: 0.2543933391571045\n",
      "Epoch 3911, Loss: 1.4089100360870361, Final Batch Loss: 0.195139080286026\n",
      "Epoch 3912, Loss: 1.7071059048175812, Final Batch Loss: 0.29783734679222107\n",
      "Epoch 3913, Loss: 1.5709448605775833, Final Batch Loss: 0.1772429645061493\n",
      "Epoch 3914, Loss: 1.6134653091430664, Final Batch Loss: 0.4268997609615326\n",
      "Epoch 3915, Loss: 1.6425987780094147, Final Batch Loss: 0.36204153299331665\n",
      "Epoch 3916, Loss: 1.4193198382854462, Final Batch Loss: 0.22822615504264832\n",
      "Epoch 3917, Loss: 1.3082395792007446, Final Batch Loss: 0.24627557396888733\n",
      "Epoch 3918, Loss: 1.4586607813835144, Final Batch Loss: 0.15762455761432648\n",
      "Epoch 3919, Loss: 1.5570579916238785, Final Batch Loss: 0.2648424804210663\n",
      "Epoch 3920, Loss: 1.7010996490716934, Final Batch Loss: 0.3027788996696472\n",
      "Epoch 3921, Loss: 1.5468979179859161, Final Batch Loss: 0.3120143413543701\n",
      "Epoch 3922, Loss: 1.4913832992315292, Final Batch Loss: 0.19126786291599274\n",
      "Epoch 3923, Loss: 1.5246524661779404, Final Batch Loss: 0.32353535294532776\n",
      "Epoch 3924, Loss: 1.4063284695148468, Final Batch Loss: 0.16870011389255524\n",
      "Epoch 3925, Loss: 1.6167748272418976, Final Batch Loss: 0.3244020938873291\n",
      "Epoch 3926, Loss: 1.731869488954544, Final Batch Loss: 0.42764386534690857\n",
      "Epoch 3927, Loss: 1.8395653367042542, Final Batch Loss: 0.5015743374824524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3928, Loss: 1.6054200530052185, Final Batch Loss: 0.35632839798927307\n",
      "Epoch 3929, Loss: 1.5313538312911987, Final Batch Loss: 0.25102895498275757\n",
      "Epoch 3930, Loss: 1.6725488305091858, Final Batch Loss: 0.22654059529304504\n",
      "Epoch 3931, Loss: 1.664927214384079, Final Batch Loss: 0.2960345149040222\n",
      "Epoch 3932, Loss: 1.7976792752742767, Final Batch Loss: 0.2714204490184784\n",
      "Epoch 3933, Loss: 1.8999108374118805, Final Batch Loss: 0.4400164484977722\n",
      "Epoch 3934, Loss: 1.4625959247350693, Final Batch Loss: 0.20651449263095856\n",
      "Epoch 3935, Loss: 1.5254558324813843, Final Batch Loss: 0.36473944783210754\n",
      "Epoch 3936, Loss: 1.564386397600174, Final Batch Loss: 0.3149736225605011\n",
      "Epoch 3937, Loss: 1.7128807306289673, Final Batch Loss: 0.4580848515033722\n",
      "Epoch 3938, Loss: 1.4611727595329285, Final Batch Loss: 0.3944050371646881\n",
      "Epoch 3939, Loss: 1.6336535215377808, Final Batch Loss: 0.3107057213783264\n",
      "Epoch 3940, Loss: 1.7032907009124756, Final Batch Loss: 0.481924831867218\n",
      "Epoch 3941, Loss: 1.7227639257907867, Final Batch Loss: 0.40753960609436035\n",
      "Epoch 3942, Loss: 1.8277533948421478, Final Batch Loss: 0.4259791076183319\n",
      "Epoch 3943, Loss: 1.5655872523784637, Final Batch Loss: 0.22911953926086426\n",
      "Epoch 3944, Loss: 1.465255692601204, Final Batch Loss: 0.20497380197048187\n",
      "Epoch 3945, Loss: 1.748417317867279, Final Batch Loss: 0.4406050145626068\n",
      "Epoch 3946, Loss: 1.6732904016971588, Final Batch Loss: 0.28540149331092834\n",
      "Epoch 3947, Loss: 1.7218082398176193, Final Batch Loss: 0.4004880487918854\n",
      "Epoch 3948, Loss: 1.604159951210022, Final Batch Loss: 0.3926980197429657\n",
      "Epoch 3949, Loss: 1.5269866585731506, Final Batch Loss: 0.338716059923172\n",
      "Epoch 3950, Loss: 1.5699534118175507, Final Batch Loss: 0.25198325514793396\n",
      "Epoch 3951, Loss: 1.6849357783794403, Final Batch Loss: 0.3858376145362854\n",
      "Epoch 3952, Loss: 1.6336712539196014, Final Batch Loss: 0.4665839970111847\n",
      "Epoch 3953, Loss: 1.8476484715938568, Final Batch Loss: 0.449459969997406\n",
      "Epoch 3954, Loss: 1.604067862033844, Final Batch Loss: 0.2824207544326782\n",
      "Epoch 3955, Loss: 1.5737760812044144, Final Batch Loss: 0.3081749975681305\n",
      "Epoch 3956, Loss: 1.5445406436920166, Final Batch Loss: 0.30212876200675964\n",
      "Epoch 3957, Loss: 1.5420588850975037, Final Batch Loss: 0.3007596433162689\n",
      "Epoch 3958, Loss: 1.5366643220186234, Final Batch Loss: 0.3049883544445038\n",
      "Epoch 3959, Loss: 1.6406728625297546, Final Batch Loss: 0.3879457414150238\n",
      "Epoch 3960, Loss: 1.6012120246887207, Final Batch Loss: 0.27132079005241394\n",
      "Epoch 3961, Loss: 1.7389348149299622, Final Batch Loss: 0.2685885429382324\n",
      "Epoch 3962, Loss: 1.5391980707645416, Final Batch Loss: 0.3721194565296173\n",
      "Epoch 3963, Loss: 1.6260478794574738, Final Batch Loss: 0.27868878841400146\n",
      "Epoch 3964, Loss: 1.5463803708553314, Final Batch Loss: 0.2767360806465149\n",
      "Epoch 3965, Loss: 1.7066665887832642, Final Batch Loss: 0.4216022789478302\n",
      "Epoch 3966, Loss: 1.6237377524375916, Final Batch Loss: 0.2685371935367584\n",
      "Epoch 3967, Loss: 1.6790771484375, Final Batch Loss: 0.34192484617233276\n",
      "Epoch 3968, Loss: 1.5188049972057343, Final Batch Loss: 0.26628151535987854\n",
      "Epoch 3969, Loss: 1.5814909934997559, Final Batch Loss: 0.2465304136276245\n",
      "Epoch 3970, Loss: 1.6475428640842438, Final Batch Loss: 0.502575159072876\n",
      "Epoch 3971, Loss: 1.6383241564035416, Final Batch Loss: 0.3008004426956177\n",
      "Epoch 3972, Loss: 1.8632476031780243, Final Batch Loss: 0.3623234033584595\n",
      "Epoch 3973, Loss: 1.6631463915109634, Final Batch Loss: 0.21062935888767242\n",
      "Epoch 3974, Loss: 1.815710037946701, Final Batch Loss: 0.3750822842121124\n",
      "Epoch 3975, Loss: 1.7650225460529327, Final Batch Loss: 0.49602121114730835\n",
      "Epoch 3976, Loss: 1.6032275557518005, Final Batch Loss: 0.3356923758983612\n",
      "Epoch 3977, Loss: 1.6599556505680084, Final Batch Loss: 0.36252087354660034\n",
      "Epoch 3978, Loss: 1.8038676977157593, Final Batch Loss: 0.32445329427719116\n",
      "Epoch 3979, Loss: 1.633381500840187, Final Batch Loss: 0.37422993779182434\n",
      "Epoch 3980, Loss: 1.8175871670246124, Final Batch Loss: 0.29643288254737854\n",
      "Epoch 3981, Loss: 1.5351136028766632, Final Batch Loss: 0.3404579758644104\n",
      "Epoch 3982, Loss: 1.765385389328003, Final Batch Loss: 0.5209395289421082\n",
      "Epoch 3983, Loss: 1.6277845203876495, Final Batch Loss: 0.35658103227615356\n",
      "Epoch 3984, Loss: 1.5693681240081787, Final Batch Loss: 0.23653635382652283\n",
      "Epoch 3985, Loss: 1.600004494190216, Final Batch Loss: 0.25243890285491943\n",
      "Epoch 3986, Loss: 1.5289831161499023, Final Batch Loss: 0.31035947799682617\n",
      "Epoch 3987, Loss: 1.6166405081748962, Final Batch Loss: 0.30748263001441956\n",
      "Epoch 3988, Loss: 1.5881823599338531, Final Batch Loss: 0.333916038274765\n",
      "Epoch 3989, Loss: 1.5215822458267212, Final Batch Loss: 0.2686716914176941\n",
      "Epoch 3990, Loss: 1.7161506712436676, Final Batch Loss: 0.3107633590698242\n",
      "Epoch 3991, Loss: 1.5431319773197174, Final Batch Loss: 0.32336607575416565\n",
      "Epoch 3992, Loss: 1.7490001022815704, Final Batch Loss: 0.4474433660507202\n",
      "Epoch 3993, Loss: 1.8497295677661896, Final Batch Loss: 0.4717351794242859\n",
      "Epoch 3994, Loss: 1.6763908565044403, Final Batch Loss: 0.41089028120040894\n",
      "Epoch 3995, Loss: 1.4771367609500885, Final Batch Loss: 0.3152294158935547\n",
      "Epoch 3996, Loss: 1.5576201975345612, Final Batch Loss: 0.2056206464767456\n",
      "Epoch 3997, Loss: 1.7495260387659073, Final Batch Loss: 0.5074475407600403\n",
      "Epoch 3998, Loss: 1.5691498816013336, Final Batch Loss: 0.24411144852638245\n",
      "Epoch 3999, Loss: 1.62473464012146, Final Batch Loss: 0.2990334630012512\n",
      "Epoch 4000, Loss: 1.5864714086055756, Final Batch Loss: 0.32459506392478943\n",
      "Epoch 4001, Loss: 1.8226228654384613, Final Batch Loss: 0.6022622585296631\n",
      "Epoch 4002, Loss: 1.939213901758194, Final Batch Loss: 0.4253353178501129\n",
      "Epoch 4003, Loss: 1.6591083109378815, Final Batch Loss: 0.4137430191040039\n",
      "Epoch 4004, Loss: 1.6476779878139496, Final Batch Loss: 0.3214063048362732\n",
      "Epoch 4005, Loss: 1.6170874536037445, Final Batch Loss: 0.26810070872306824\n",
      "Epoch 4006, Loss: 1.4307082444429398, Final Batch Loss: 0.2713990807533264\n",
      "Epoch 4007, Loss: 1.5116049647331238, Final Batch Loss: 0.29954755306243896\n",
      "Epoch 4008, Loss: 1.9382055699825287, Final Batch Loss: 0.6108378767967224\n",
      "Epoch 4009, Loss: 1.479951411485672, Final Batch Loss: 0.28556251525878906\n",
      "Epoch 4010, Loss: 1.6984072625637054, Final Batch Loss: 0.26158955693244934\n",
      "Epoch 4011, Loss: 1.5582621693611145, Final Batch Loss: 0.29855233430862427\n",
      "Epoch 4012, Loss: 1.3923494964838028, Final Batch Loss: 0.22501467168331146\n",
      "Epoch 4013, Loss: 1.492695152759552, Final Batch Loss: 0.2790338695049286\n",
      "Epoch 4014, Loss: 1.5881668627262115, Final Batch Loss: 0.3503545820713043\n",
      "Epoch 4015, Loss: 1.6015865802764893, Final Batch Loss: 0.3376632332801819\n",
      "Epoch 4016, Loss: 1.5027437806129456, Final Batch Loss: 0.2692514657974243\n",
      "Epoch 4017, Loss: 1.5788599699735641, Final Batch Loss: 0.41905587911605835\n",
      "Epoch 4018, Loss: 1.537797063589096, Final Batch Loss: 0.34617555141448975\n",
      "Epoch 4019, Loss: 1.5215934813022614, Final Batch Loss: 0.31977543234825134\n",
      "Epoch 4020, Loss: 1.5094764828681946, Final Batch Loss: 0.29799652099609375\n",
      "Epoch 4021, Loss: 1.3325495421886444, Final Batch Loss: 0.1852360963821411\n",
      "Epoch 4022, Loss: 1.5046792328357697, Final Batch Loss: 0.2823280096054077\n",
      "Epoch 4023, Loss: 1.7916010618209839, Final Batch Loss: 0.30552011728286743\n",
      "Epoch 4024, Loss: 1.4765949547290802, Final Batch Loss: 0.1464344561100006\n",
      "Epoch 4025, Loss: 1.49416846036911, Final Batch Loss: 0.2663888931274414\n",
      "Epoch 4026, Loss: 1.4002000987529755, Final Batch Loss: 0.24711672961711884\n",
      "Epoch 4027, Loss: 1.4428648948669434, Final Batch Loss: 0.2699471116065979\n",
      "Epoch 4028, Loss: 1.5895702540874481, Final Batch Loss: 0.2987228333950043\n",
      "Epoch 4029, Loss: 1.5118274092674255, Final Batch Loss: 0.2639105021953583\n",
      "Epoch 4030, Loss: 1.5261557549238205, Final Batch Loss: 0.23697815835475922\n",
      "Epoch 4031, Loss: 1.621078372001648, Final Batch Loss: 0.2502821981906891\n",
      "Epoch 4032, Loss: 1.598651796579361, Final Batch Loss: 0.3085099458694458\n",
      "Epoch 4033, Loss: 1.4422952681779861, Final Batch Loss: 0.20913274586200714\n",
      "Epoch 4034, Loss: 1.8177573084831238, Final Batch Loss: 0.49817320704460144\n",
      "Epoch 4035, Loss: 1.5175039321184158, Final Batch Loss: 0.24142421782016754\n",
      "Epoch 4036, Loss: 1.6848080456256866, Final Batch Loss: 0.475109338760376\n",
      "Epoch 4037, Loss: 1.5025433897972107, Final Batch Loss: 0.2818191945552826\n",
      "Epoch 4038, Loss: 1.7865507304668427, Final Batch Loss: 0.505997896194458\n",
      "Epoch 4039, Loss: 1.5318611562252045, Final Batch Loss: 0.3250803053379059\n",
      "Epoch 4040, Loss: 1.4978996068239212, Final Batch Loss: 0.20234906673431396\n",
      "Epoch 4041, Loss: 1.6345510482788086, Final Batch Loss: 0.32330724596977234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4042, Loss: 1.6390916407108307, Final Batch Loss: 0.33690711855888367\n",
      "Epoch 4043, Loss: 1.6422357857227325, Final Batch Loss: 0.3352922201156616\n",
      "Epoch 4044, Loss: 1.5195356756448746, Final Batch Loss: 0.22129370272159576\n",
      "Epoch 4045, Loss: 1.5946234315633774, Final Batch Loss: 0.34826016426086426\n",
      "Epoch 4046, Loss: 1.5297505557537079, Final Batch Loss: 0.3084055185317993\n",
      "Epoch 4047, Loss: 1.650803118944168, Final Batch Loss: 0.2808419167995453\n",
      "Epoch 4048, Loss: 1.3666158020496368, Final Batch Loss: 0.17331820726394653\n",
      "Epoch 4049, Loss: 1.7027197182178497, Final Batch Loss: 0.34599459171295166\n",
      "Epoch 4050, Loss: 1.512565940618515, Final Batch Loss: 0.30472660064697266\n",
      "Epoch 4051, Loss: 1.4815818965435028, Final Batch Loss: 0.28309711813926697\n",
      "Epoch 4052, Loss: 1.4096925258636475, Final Batch Loss: 0.3103301227092743\n",
      "Epoch 4053, Loss: 1.6557489037513733, Final Batch Loss: 0.4549502730369568\n",
      "Epoch 4054, Loss: 1.5267482995986938, Final Batch Loss: 0.3312869668006897\n",
      "Epoch 4055, Loss: 1.5638040155172348, Final Batch Loss: 0.3652469217777252\n",
      "Epoch 4056, Loss: 1.5833650529384613, Final Batch Loss: 0.33122187852859497\n",
      "Epoch 4057, Loss: 1.864139199256897, Final Batch Loss: 0.4254325330257416\n",
      "Epoch 4058, Loss: 1.6498746871948242, Final Batch Loss: 0.3242383301258087\n",
      "Epoch 4059, Loss: 1.6415419280529022, Final Batch Loss: 0.3473213315010071\n",
      "Epoch 4060, Loss: 1.4572907090187073, Final Batch Loss: 0.34212470054626465\n",
      "Epoch 4061, Loss: 1.5704639554023743, Final Batch Loss: 0.32516324520111084\n",
      "Epoch 4062, Loss: 1.5784074664115906, Final Batch Loss: 0.3038848638534546\n",
      "Epoch 4063, Loss: 1.4931635111570358, Final Batch Loss: 0.29772964119911194\n",
      "Epoch 4064, Loss: 1.6016628742218018, Final Batch Loss: 0.22021320462226868\n",
      "Epoch 4065, Loss: 1.5253155529499054, Final Batch Loss: 0.37991687655448914\n",
      "Epoch 4066, Loss: 1.5911495387554169, Final Batch Loss: 0.3898681402206421\n",
      "Epoch 4067, Loss: 1.4245310723781586, Final Batch Loss: 0.2623044550418854\n",
      "Epoch 4068, Loss: 1.5010177940130234, Final Batch Loss: 0.18065868318080902\n",
      "Epoch 4069, Loss: 1.5046048611402512, Final Batch Loss: 0.23104135692119598\n",
      "Epoch 4070, Loss: 1.3976226150989532, Final Batch Loss: 0.26729682087898254\n",
      "Epoch 4071, Loss: 1.6889794021844864, Final Batch Loss: 0.24342821538448334\n",
      "Epoch 4072, Loss: 1.7243223190307617, Final Batch Loss: 0.3305594027042389\n",
      "Epoch 4073, Loss: 1.3966549336910248, Final Batch Loss: 0.17238065600395203\n",
      "Epoch 4074, Loss: 1.634519785642624, Final Batch Loss: 0.16647717356681824\n",
      "Epoch 4075, Loss: 1.504205584526062, Final Batch Loss: 0.2834932506084442\n",
      "Epoch 4076, Loss: 1.5510867536067963, Final Batch Loss: 0.2933301329612732\n",
      "Epoch 4077, Loss: 1.502222865819931, Final Batch Loss: 0.3326890170574188\n",
      "Epoch 4078, Loss: 1.383538231253624, Final Batch Loss: 0.1508416086435318\n",
      "Epoch 4079, Loss: 1.5202347338199615, Final Batch Loss: 0.38343486189842224\n",
      "Epoch 4080, Loss: 1.4914761185646057, Final Batch Loss: 0.2775264382362366\n",
      "Epoch 4081, Loss: 1.3595273792743683, Final Batch Loss: 0.1878691166639328\n",
      "Epoch 4082, Loss: 1.4923166781663895, Final Batch Loss: 0.29838070273399353\n",
      "Epoch 4083, Loss: 1.5636652410030365, Final Batch Loss: 0.2975446879863739\n",
      "Epoch 4084, Loss: 1.477271020412445, Final Batch Loss: 0.24189656972885132\n",
      "Epoch 4085, Loss: 1.441767618060112, Final Batch Loss: 0.2357584536075592\n",
      "Epoch 4086, Loss: 1.6418015360832214, Final Batch Loss: 0.35937443375587463\n",
      "Epoch 4087, Loss: 1.983392208814621, Final Batch Loss: 0.3584257662296295\n",
      "Epoch 4088, Loss: 1.6384141743183136, Final Batch Loss: 0.29724550247192383\n",
      "Epoch 4089, Loss: 1.5331859588623047, Final Batch Loss: 0.3052898049354553\n",
      "Epoch 4090, Loss: 1.727106750011444, Final Batch Loss: 0.39913588762283325\n",
      "Epoch 4091, Loss: 1.6935912072658539, Final Batch Loss: 0.3615431487560272\n",
      "Epoch 4092, Loss: 1.6677730679512024, Final Batch Loss: 0.37719571590423584\n",
      "Epoch 4093, Loss: 1.593716025352478, Final Batch Loss: 0.2509342432022095\n",
      "Epoch 4094, Loss: 1.5775533616542816, Final Batch Loss: 0.26782476902008057\n",
      "Epoch 4095, Loss: 1.7066583037376404, Final Batch Loss: 0.30906859040260315\n",
      "Epoch 4096, Loss: 1.810977578163147, Final Batch Loss: 0.500453531742096\n",
      "Epoch 4097, Loss: 1.6767562925815582, Final Batch Loss: 0.32364392280578613\n",
      "Epoch 4098, Loss: 1.6281871795654297, Final Batch Loss: 0.33808109164237976\n",
      "Epoch 4099, Loss: 1.7050195336341858, Final Batch Loss: 0.40380093455314636\n",
      "Epoch 4100, Loss: 1.5829624682664871, Final Batch Loss: 0.22447551786899567\n",
      "Epoch 4101, Loss: 1.8689171373844147, Final Batch Loss: 0.3784860670566559\n",
      "Epoch 4102, Loss: 1.5552648603916168, Final Batch Loss: 0.3616197407245636\n",
      "Epoch 4103, Loss: 1.3797328174114227, Final Batch Loss: 0.23219837248325348\n",
      "Epoch 4104, Loss: 1.5502560138702393, Final Batch Loss: 0.3193947672843933\n",
      "Epoch 4105, Loss: 1.8205144107341766, Final Batch Loss: 0.5002312064170837\n",
      "Epoch 4106, Loss: 1.483282595872879, Final Batch Loss: 0.17174282670021057\n",
      "Epoch 4107, Loss: 1.618594765663147, Final Batch Loss: 0.36827847361564636\n",
      "Epoch 4108, Loss: 1.6837003827095032, Final Batch Loss: 0.2808883786201477\n",
      "Epoch 4109, Loss: 1.5743921995162964, Final Batch Loss: 0.33407971262931824\n",
      "Epoch 4110, Loss: 1.6745427250862122, Final Batch Loss: 0.37524548172950745\n",
      "Epoch 4111, Loss: 1.4860981106758118, Final Batch Loss: 0.3374355137348175\n",
      "Epoch 4112, Loss: 1.6334674954414368, Final Batch Loss: 0.24378952383995056\n",
      "Epoch 4113, Loss: 1.6459389626979828, Final Batch Loss: 0.3151155412197113\n",
      "Epoch 4114, Loss: 1.5765668153762817, Final Batch Loss: 0.261361688375473\n",
      "Epoch 4115, Loss: 1.6468231081962585, Final Batch Loss: 0.3111859858036041\n",
      "Epoch 4116, Loss: 1.6138470321893692, Final Batch Loss: 0.2973123788833618\n",
      "Epoch 4117, Loss: 1.614748865365982, Final Batch Loss: 0.4029766321182251\n",
      "Epoch 4118, Loss: 1.569465696811676, Final Batch Loss: 0.23714259266853333\n",
      "Epoch 4119, Loss: 1.5321645140647888, Final Batch Loss: 0.2560453414916992\n",
      "Epoch 4120, Loss: 1.6995792984962463, Final Batch Loss: 0.35001546144485474\n",
      "Epoch 4121, Loss: 2.0416929125785828, Final Batch Loss: 0.5702828764915466\n",
      "Epoch 4122, Loss: 1.5203447490930557, Final Batch Loss: 0.24660547077655792\n",
      "Epoch 4123, Loss: 1.7272299230098724, Final Batch Loss: 0.35248976945877075\n",
      "Epoch 4124, Loss: 1.5494331270456314, Final Batch Loss: 0.2837803363800049\n",
      "Epoch 4125, Loss: 1.5584737956523895, Final Batch Loss: 0.27586036920547485\n",
      "Epoch 4126, Loss: 1.6213087737560272, Final Batch Loss: 0.2644056975841522\n",
      "Epoch 4127, Loss: 1.7806529104709625, Final Batch Loss: 0.595275342464447\n",
      "Epoch 4128, Loss: 1.613622397184372, Final Batch Loss: 0.37940454483032227\n",
      "Epoch 4129, Loss: 1.4782221913337708, Final Batch Loss: 0.2524225413799286\n",
      "Epoch 4130, Loss: 1.69139164686203, Final Batch Loss: 0.3304017186164856\n",
      "Epoch 4131, Loss: 1.4957551211118698, Final Batch Loss: 0.23519380390644073\n",
      "Epoch 4132, Loss: 1.4483866691589355, Final Batch Loss: 0.22654986381530762\n",
      "Epoch 4133, Loss: 1.5181536674499512, Final Batch Loss: 0.36205020546913147\n",
      "Epoch 4134, Loss: 1.493508294224739, Final Batch Loss: 0.2621401846408844\n",
      "Epoch 4135, Loss: 1.537199929356575, Final Batch Loss: 0.24994786083698273\n",
      "Epoch 4136, Loss: 1.44044890999794, Final Batch Loss: 0.27789106965065\n",
      "Epoch 4137, Loss: 1.5221888422966003, Final Batch Loss: 0.2783231735229492\n",
      "Epoch 4138, Loss: 1.3960504531860352, Final Batch Loss: 0.26753735542297363\n",
      "Epoch 4139, Loss: 1.5711511671543121, Final Batch Loss: 0.2620106339454651\n",
      "Epoch 4140, Loss: 1.8105872869491577, Final Batch Loss: 0.4253363311290741\n",
      "Epoch 4141, Loss: 1.304971143603325, Final Batch Loss: 0.24364669620990753\n",
      "Epoch 4142, Loss: 1.5788603723049164, Final Batch Loss: 0.22722697257995605\n",
      "Epoch 4143, Loss: 1.720549076795578, Final Batch Loss: 0.4321298897266388\n",
      "Epoch 4144, Loss: 1.5898789763450623, Final Batch Loss: 0.359540730714798\n",
      "Epoch 4145, Loss: 1.552290678024292, Final Batch Loss: 0.2729530930519104\n",
      "Epoch 4146, Loss: 1.7535131573677063, Final Batch Loss: 0.41024863719940186\n",
      "Epoch 4147, Loss: 1.593023270368576, Final Batch Loss: 0.3303999900817871\n",
      "Epoch 4148, Loss: 1.536890983581543, Final Batch Loss: 0.39372435212135315\n",
      "Epoch 4149, Loss: 1.4351387023925781, Final Batch Loss: 0.27192267775535583\n",
      "Epoch 4150, Loss: 1.501509040594101, Final Batch Loss: 0.2405484914779663\n",
      "Epoch 4151, Loss: 1.554348111152649, Final Batch Loss: 0.33228540420532227\n",
      "Epoch 4152, Loss: 1.5511122643947601, Final Batch Loss: 0.43539056181907654\n",
      "Epoch 4153, Loss: 1.4965935051441193, Final Batch Loss: 0.4529033899307251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4154, Loss: 1.71170312166214, Final Batch Loss: 0.4528784453868866\n",
      "Epoch 4155, Loss: 1.5250081717967987, Final Batch Loss: 0.3383886516094208\n",
      "Epoch 4156, Loss: 1.3296122252941132, Final Batch Loss: 0.1813904047012329\n",
      "Epoch 4157, Loss: 1.7766092121601105, Final Batch Loss: 0.39724284410476685\n",
      "Epoch 4158, Loss: 1.7248827517032623, Final Batch Loss: 0.3653413951396942\n",
      "Epoch 4159, Loss: 1.534018874168396, Final Batch Loss: 0.3106893301010132\n",
      "Epoch 4160, Loss: 1.519235834479332, Final Batch Loss: 0.3736025094985962\n",
      "Epoch 4161, Loss: 1.5793654024600983, Final Batch Loss: 0.38056665658950806\n",
      "Epoch 4162, Loss: 1.5154846757650375, Final Batch Loss: 0.22749392688274384\n",
      "Epoch 4163, Loss: 1.6999435126781464, Final Batch Loss: 0.38558438420295715\n",
      "Epoch 4164, Loss: 1.5038099884986877, Final Batch Loss: 0.3762539029121399\n",
      "Epoch 4165, Loss: 1.6882226169109344, Final Batch Loss: 0.4893455505371094\n",
      "Epoch 4166, Loss: 1.5993276238441467, Final Batch Loss: 0.3310573697090149\n",
      "Epoch 4167, Loss: 1.5377951562404633, Final Batch Loss: 0.25188013911247253\n",
      "Epoch 4168, Loss: 1.5582837462425232, Final Batch Loss: 0.3459431529045105\n",
      "Epoch 4169, Loss: 1.4718630015850067, Final Batch Loss: 0.27099016308784485\n",
      "Epoch 4170, Loss: 1.4092054963111877, Final Batch Loss: 0.276435524225235\n",
      "Epoch 4171, Loss: 1.4667244404554367, Final Batch Loss: 0.2404000610113144\n",
      "Epoch 4172, Loss: 1.7756666839122772, Final Batch Loss: 0.4028497338294983\n",
      "Epoch 4173, Loss: 1.8083119839429855, Final Batch Loss: 0.5801947712898254\n",
      "Epoch 4174, Loss: 1.4627106338739395, Final Batch Loss: 0.2644859552383423\n",
      "Epoch 4175, Loss: 1.4802294969558716, Final Batch Loss: 0.231585294008255\n",
      "Epoch 4176, Loss: 1.6334925293922424, Final Batch Loss: 0.32552963495254517\n",
      "Epoch 4177, Loss: 1.4957191944122314, Final Batch Loss: 0.3932192027568817\n",
      "Epoch 4178, Loss: 1.4178391844034195, Final Batch Loss: 0.2830968201160431\n",
      "Epoch 4179, Loss: 1.3664564192295074, Final Batch Loss: 0.21968665719032288\n",
      "Epoch 4180, Loss: 1.4526326209306717, Final Batch Loss: 0.23121748864650726\n",
      "Epoch 4181, Loss: 1.6939509212970734, Final Batch Loss: 0.32948073744773865\n",
      "Epoch 4182, Loss: 1.5503427386283875, Final Batch Loss: 0.3387719392776489\n",
      "Epoch 4183, Loss: 1.3609768748283386, Final Batch Loss: 0.23157790303230286\n",
      "Epoch 4184, Loss: 1.6394833028316498, Final Batch Loss: 0.32528260350227356\n",
      "Epoch 4185, Loss: 1.4614689499139786, Final Batch Loss: 0.2491743415594101\n",
      "Epoch 4186, Loss: 1.7110375463962555, Final Batch Loss: 0.37039849162101746\n",
      "Epoch 4187, Loss: 1.8741753101348877, Final Batch Loss: 0.4391680657863617\n",
      "Epoch 4188, Loss: 1.8460506200790405, Final Batch Loss: 0.4016154110431671\n",
      "Epoch 4189, Loss: 1.62949338555336, Final Batch Loss: 0.3529193699359894\n",
      "Epoch 4190, Loss: 1.5965457558631897, Final Batch Loss: 0.3955569565296173\n",
      "Epoch 4191, Loss: 1.390337884426117, Final Batch Loss: 0.20541486144065857\n",
      "Epoch 4192, Loss: 1.3494855165481567, Final Batch Loss: 0.14079153537750244\n",
      "Epoch 4193, Loss: 1.4421380460262299, Final Batch Loss: 0.21093672513961792\n",
      "Epoch 4194, Loss: 1.5859158635139465, Final Batch Loss: 0.39992672204971313\n",
      "Epoch 4195, Loss: 1.5656116604804993, Final Batch Loss: 0.2980223000049591\n",
      "Epoch 4196, Loss: 1.4513894021511078, Final Batch Loss: 0.3112401068210602\n",
      "Epoch 4197, Loss: 1.581922560930252, Final Batch Loss: 0.38701850175857544\n",
      "Epoch 4198, Loss: 1.687347948551178, Final Batch Loss: 0.2836237847805023\n",
      "Epoch 4199, Loss: 1.671383112668991, Final Batch Loss: 0.47758835554122925\n",
      "Epoch 4200, Loss: 1.462460309267044, Final Batch Loss: 0.22572264075279236\n",
      "Epoch 4201, Loss: 1.6323500871658325, Final Batch Loss: 0.322914719581604\n",
      "Epoch 4202, Loss: 1.4916870892047882, Final Batch Loss: 0.3093576431274414\n",
      "Epoch 4203, Loss: 1.4159220457077026, Final Batch Loss: 0.25035786628723145\n",
      "Epoch 4204, Loss: 1.5237942039966583, Final Batch Loss: 0.33143872022628784\n",
      "Epoch 4205, Loss: 1.4384849816560745, Final Batch Loss: 0.28990817070007324\n",
      "Epoch 4206, Loss: 1.5460399687290192, Final Batch Loss: 0.23157939314842224\n",
      "Epoch 4207, Loss: 1.6173030734062195, Final Batch Loss: 0.2982802093029022\n",
      "Epoch 4208, Loss: 1.6318696290254593, Final Batch Loss: 0.43970632553100586\n",
      "Epoch 4209, Loss: 1.8048945665359497, Final Batch Loss: 0.4707675278186798\n",
      "Epoch 4210, Loss: 1.6670435667037964, Final Batch Loss: 0.2966691553592682\n",
      "Epoch 4211, Loss: 1.6100651323795319, Final Batch Loss: 0.23792016506195068\n",
      "Epoch 4212, Loss: 1.6828447580337524, Final Batch Loss: 0.41102156043052673\n",
      "Epoch 4213, Loss: 1.4941828846931458, Final Batch Loss: 0.3042178750038147\n",
      "Epoch 4214, Loss: 1.5066470503807068, Final Batch Loss: 0.17459851503372192\n",
      "Epoch 4215, Loss: 1.4835128635168076, Final Batch Loss: 0.24629530310630798\n",
      "Epoch 4216, Loss: 1.4851789623498917, Final Batch Loss: 0.22532163560390472\n",
      "Epoch 4217, Loss: 1.647478610277176, Final Batch Loss: 0.5018779635429382\n",
      "Epoch 4218, Loss: 1.617948740720749, Final Batch Loss: 0.28285738825798035\n",
      "Epoch 4219, Loss: 1.5154776871204376, Final Batch Loss: 0.27398285269737244\n",
      "Epoch 4220, Loss: 1.6736058592796326, Final Batch Loss: 0.3640251159667969\n",
      "Epoch 4221, Loss: 1.4883218258619308, Final Batch Loss: 0.2452888935804367\n",
      "Epoch 4222, Loss: 1.5310328304767609, Final Batch Loss: 0.29585009813308716\n",
      "Epoch 4223, Loss: 1.4644213914871216, Final Batch Loss: 0.3023681640625\n",
      "Epoch 4224, Loss: 1.3552071154117584, Final Batch Loss: 0.2892746329307556\n",
      "Epoch 4225, Loss: 1.680043876171112, Final Batch Loss: 0.39888352155685425\n",
      "Epoch 4226, Loss: 1.5325556099414825, Final Batch Loss: 0.37419500946998596\n",
      "Epoch 4227, Loss: 1.4437309205532074, Final Batch Loss: 0.2674674391746521\n",
      "Epoch 4228, Loss: 1.4548305422067642, Final Batch Loss: 0.3100750744342804\n",
      "Epoch 4229, Loss: 1.5846421867609024, Final Batch Loss: 0.4626343846321106\n",
      "Epoch 4230, Loss: 1.5566806644201279, Final Batch Loss: 0.3015752136707306\n",
      "Epoch 4231, Loss: 1.5056570321321487, Final Batch Loss: 0.3184468746185303\n",
      "Epoch 4232, Loss: 1.4256135821342468, Final Batch Loss: 0.21048441529273987\n",
      "Epoch 4233, Loss: 1.7031805515289307, Final Batch Loss: 0.3884267807006836\n",
      "Epoch 4234, Loss: 1.5629846155643463, Final Batch Loss: 0.28635174036026\n",
      "Epoch 4235, Loss: 1.72635418176651, Final Batch Loss: 0.482585072517395\n",
      "Epoch 4236, Loss: 1.6566079258918762, Final Batch Loss: 0.3260047733783722\n",
      "Epoch 4237, Loss: 1.7799818515777588, Final Batch Loss: 0.39159464836120605\n",
      "Epoch 4238, Loss: 1.751665621995926, Final Batch Loss: 0.3778526782989502\n",
      "Epoch 4239, Loss: 1.6325518786907196, Final Batch Loss: 0.28174304962158203\n",
      "Epoch 4240, Loss: 1.5701745450496674, Final Batch Loss: 0.37272754311561584\n",
      "Epoch 4241, Loss: 1.50719752907753, Final Batch Loss: 0.2410680055618286\n",
      "Epoch 4242, Loss: 1.5975177586078644, Final Batch Loss: 0.28040674328804016\n",
      "Epoch 4243, Loss: 1.781196653842926, Final Batch Loss: 0.4341666102409363\n",
      "Epoch 4244, Loss: 1.6670291721820831, Final Batch Loss: 0.3795779049396515\n",
      "Epoch 4245, Loss: 1.497124344110489, Final Batch Loss: 0.30363062024116516\n",
      "Epoch 4246, Loss: 1.5597626268863678, Final Batch Loss: 0.272087961435318\n",
      "Epoch 4247, Loss: 1.5493377596139908, Final Batch Loss: 0.20490704476833344\n",
      "Epoch 4248, Loss: 1.434111312031746, Final Batch Loss: 0.31027835607528687\n",
      "Epoch 4249, Loss: 1.5847665667533875, Final Batch Loss: 0.19802334904670715\n",
      "Epoch 4250, Loss: 1.609527826309204, Final Batch Loss: 0.3559413254261017\n",
      "Epoch 4251, Loss: 1.5780169665813446, Final Batch Loss: 0.3571750223636627\n",
      "Epoch 4252, Loss: 1.6011507213115692, Final Batch Loss: 0.28183257579803467\n",
      "Epoch 4253, Loss: 1.7785066664218903, Final Batch Loss: 0.4877314865589142\n",
      "Epoch 4254, Loss: 1.882540225982666, Final Batch Loss: 0.2992407977581024\n",
      "Epoch 4255, Loss: 1.709919959306717, Final Batch Loss: 0.2907474935054779\n",
      "Epoch 4256, Loss: 1.4918576180934906, Final Batch Loss: 0.338532954454422\n",
      "Epoch 4257, Loss: 1.610657960176468, Final Batch Loss: 0.3066330552101135\n",
      "Epoch 4258, Loss: 1.5183961689472198, Final Batch Loss: 0.3376389145851135\n",
      "Epoch 4259, Loss: 1.7159205973148346, Final Batch Loss: 0.3923054039478302\n",
      "Epoch 4260, Loss: 1.6030014604330063, Final Batch Loss: 0.20201577246189117\n",
      "Epoch 4261, Loss: 1.688285931944847, Final Batch Loss: 0.21513693034648895\n",
      "Epoch 4262, Loss: 1.4979016184806824, Final Batch Loss: 0.2801830768585205\n",
      "Epoch 4263, Loss: 1.4655790776014328, Final Batch Loss: 0.24896876513957977\n",
      "Epoch 4264, Loss: 1.2630574405193329, Final Batch Loss: 0.23122341930866241\n",
      "Epoch 4265, Loss: 1.3777395635843277, Final Batch Loss: 0.17453427612781525\n",
      "Epoch 4266, Loss: 1.5971272140741348, Final Batch Loss: 0.4782623052597046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4267, Loss: 1.4011834561824799, Final Batch Loss: 0.17949891090393066\n",
      "Epoch 4268, Loss: 1.4268238544464111, Final Batch Loss: 0.256991446018219\n",
      "Epoch 4269, Loss: 1.5634567588567734, Final Batch Loss: 0.287367045879364\n",
      "Epoch 4270, Loss: 1.3788876831531525, Final Batch Loss: 0.3051488697528839\n",
      "Epoch 4271, Loss: 1.868059903383255, Final Batch Loss: 0.3995070159435272\n",
      "Epoch 4272, Loss: 1.4639363437891006, Final Batch Loss: 0.31185200810432434\n",
      "Epoch 4273, Loss: 1.466433346271515, Final Batch Loss: 0.22150233387947083\n",
      "Epoch 4274, Loss: 1.6900171041488647, Final Batch Loss: 0.25756487250328064\n",
      "Epoch 4275, Loss: 1.4424996376037598, Final Batch Loss: 0.2100498378276825\n",
      "Epoch 4276, Loss: 1.5691953599452972, Final Batch Loss: 0.3320573568344116\n",
      "Epoch 4277, Loss: 1.8394599854946136, Final Batch Loss: 0.45302581787109375\n",
      "Epoch 4278, Loss: 1.9332211911678314, Final Batch Loss: 0.6432757377624512\n",
      "Epoch 4279, Loss: 1.6835368871688843, Final Batch Loss: 0.29082804918289185\n",
      "Epoch 4280, Loss: 1.6242778301239014, Final Batch Loss: 0.27006182074546814\n",
      "Epoch 4281, Loss: 1.577549546957016, Final Batch Loss: 0.31339555978775024\n",
      "Epoch 4282, Loss: 1.5583108365535736, Final Batch Loss: 0.3223692774772644\n",
      "Epoch 4283, Loss: 1.547475516796112, Final Batch Loss: 0.2759544253349304\n",
      "Epoch 4284, Loss: 1.8318090736865997, Final Batch Loss: 0.32875844836235046\n",
      "Epoch 4285, Loss: 1.603538602590561, Final Batch Loss: 0.40403836965560913\n",
      "Epoch 4286, Loss: 1.3545067608356476, Final Batch Loss: 0.22789278626441956\n",
      "Epoch 4287, Loss: 1.5202329903841019, Final Batch Loss: 0.2348717302083969\n",
      "Epoch 4288, Loss: 1.5478178411722183, Final Batch Loss: 0.2497699111700058\n",
      "Epoch 4289, Loss: 1.507743090391159, Final Batch Loss: 0.1702118217945099\n",
      "Epoch 4290, Loss: 1.9832041263580322, Final Batch Loss: 0.4664345681667328\n",
      "Epoch 4291, Loss: 1.7426220178604126, Final Batch Loss: 0.34557273983955383\n",
      "Epoch 4292, Loss: 1.5078331232070923, Final Batch Loss: 0.21400228142738342\n",
      "Epoch 4293, Loss: 1.7034425735473633, Final Batch Loss: 0.3017818331718445\n",
      "Epoch 4294, Loss: 1.5447116792201996, Final Batch Loss: 0.2449028491973877\n",
      "Epoch 4295, Loss: 1.5295336246490479, Final Batch Loss: 0.20011362433433533\n",
      "Epoch 4296, Loss: 1.6792388260364532, Final Batch Loss: 0.26095283031463623\n",
      "Epoch 4297, Loss: 1.4608570486307144, Final Batch Loss: 0.2820301651954651\n",
      "Epoch 4298, Loss: 1.548795461654663, Final Batch Loss: 0.3180517852306366\n",
      "Epoch 4299, Loss: 1.3193446695804596, Final Batch Loss: 0.12636008858680725\n",
      "Epoch 4300, Loss: 1.490045502781868, Final Batch Loss: 0.16750089824199677\n",
      "Epoch 4301, Loss: 1.6030381917953491, Final Batch Loss: 0.41245490312576294\n",
      "Epoch 4302, Loss: 1.7081972360610962, Final Batch Loss: 0.28383228182792664\n",
      "Epoch 4303, Loss: 1.596102625131607, Final Batch Loss: 0.3457620143890381\n",
      "Epoch 4304, Loss: 1.5981136113405228, Final Batch Loss: 0.4399990141391754\n",
      "Epoch 4305, Loss: 1.5253639966249466, Final Batch Loss: 0.285015344619751\n",
      "Epoch 4306, Loss: 1.5431343764066696, Final Batch Loss: 0.2697981595993042\n",
      "Epoch 4307, Loss: 1.6481979489326477, Final Batch Loss: 0.41629141569137573\n",
      "Epoch 4308, Loss: 1.5042142122983932, Final Batch Loss: 0.28671932220458984\n",
      "Epoch 4309, Loss: 1.6113945841789246, Final Batch Loss: 0.25293344259262085\n",
      "Epoch 4310, Loss: 1.5139773041009903, Final Batch Loss: 0.2512062191963196\n",
      "Epoch 4311, Loss: 1.5565617084503174, Final Batch Loss: 0.2939123809337616\n",
      "Epoch 4312, Loss: 1.55681711435318, Final Batch Loss: 0.27400949597358704\n",
      "Epoch 4313, Loss: 1.736217886209488, Final Batch Loss: 0.5531842112541199\n",
      "Epoch 4314, Loss: 1.5447922945022583, Final Batch Loss: 0.3865355849266052\n",
      "Epoch 4315, Loss: 1.5179976522922516, Final Batch Loss: 0.4275359511375427\n",
      "Epoch 4316, Loss: 1.3521459102630615, Final Batch Loss: 0.24302396178245544\n",
      "Epoch 4317, Loss: 1.6234546601772308, Final Batch Loss: 0.2781091034412384\n",
      "Epoch 4318, Loss: 1.5485467314720154, Final Batch Loss: 0.34799087047576904\n",
      "Epoch 4319, Loss: 1.5289162993431091, Final Batch Loss: 0.2909063994884491\n",
      "Epoch 4320, Loss: 1.3971340954303741, Final Batch Loss: 0.20780476927757263\n",
      "Epoch 4321, Loss: 1.7716806530952454, Final Batch Loss: 0.2746151089668274\n",
      "Epoch 4322, Loss: 1.4792540967464447, Final Batch Loss: 0.19187700748443604\n",
      "Epoch 4323, Loss: 1.5659143924713135, Final Batch Loss: 0.3701760470867157\n",
      "Epoch 4324, Loss: 1.5116181522607803, Final Batch Loss: 0.24018381536006927\n",
      "Epoch 4325, Loss: 1.5207300782203674, Final Batch Loss: 0.20845499634742737\n",
      "Epoch 4326, Loss: 1.6824501156806946, Final Batch Loss: 0.3230290412902832\n",
      "Epoch 4327, Loss: 1.4096775949001312, Final Batch Loss: 0.2762671411037445\n",
      "Epoch 4328, Loss: 1.6112465113401413, Final Batch Loss: 0.33906668424606323\n",
      "Epoch 4329, Loss: 1.5960626900196075, Final Batch Loss: 0.46689724922180176\n",
      "Epoch 4330, Loss: 1.492662400007248, Final Batch Loss: 0.2893444895744324\n",
      "Epoch 4331, Loss: 1.4688283205032349, Final Batch Loss: 0.27486979961395264\n",
      "Epoch 4332, Loss: 1.4736898243427277, Final Batch Loss: 0.2953420579433441\n",
      "Epoch 4333, Loss: 1.528344064950943, Final Batch Loss: 0.3250383734703064\n",
      "Epoch 4334, Loss: 1.512935996055603, Final Batch Loss: 0.14370983839035034\n",
      "Epoch 4335, Loss: 1.525791585445404, Final Batch Loss: 0.2739167809486389\n",
      "Epoch 4336, Loss: 1.4718988686800003, Final Batch Loss: 0.13239584863185883\n",
      "Epoch 4337, Loss: 1.6249099671840668, Final Batch Loss: 0.36301663517951965\n",
      "Epoch 4338, Loss: 1.4861548095941544, Final Batch Loss: 0.2027338296175003\n",
      "Epoch 4339, Loss: 1.5090880990028381, Final Batch Loss: 0.27392250299453735\n",
      "Epoch 4340, Loss: 1.4592987298965454, Final Batch Loss: 0.35457757115364075\n",
      "Epoch 4341, Loss: 1.7552073001861572, Final Batch Loss: 0.36229920387268066\n",
      "Epoch 4342, Loss: 1.4699674546718597, Final Batch Loss: 0.25145912170410156\n",
      "Epoch 4343, Loss: 1.5909207910299301, Final Batch Loss: 0.3114936351776123\n",
      "Epoch 4344, Loss: 1.5247319042682648, Final Batch Loss: 0.2919010818004608\n",
      "Epoch 4345, Loss: 1.6032895743846893, Final Batch Loss: 0.357347309589386\n",
      "Epoch 4346, Loss: 1.5979246497154236, Final Batch Loss: 0.27607932686805725\n",
      "Epoch 4347, Loss: 1.855748564004898, Final Batch Loss: 0.3884771168231964\n",
      "Epoch 4348, Loss: 1.5943678617477417, Final Batch Loss: 0.31086453795433044\n",
      "Epoch 4349, Loss: 1.4707887321710587, Final Batch Loss: 0.20109160244464874\n",
      "Epoch 4350, Loss: 1.6789128482341766, Final Batch Loss: 0.372053861618042\n",
      "Epoch 4351, Loss: 1.827766478061676, Final Batch Loss: 0.3645190894603729\n",
      "Epoch 4352, Loss: 1.6524863243103027, Final Batch Loss: 0.24341043829917908\n",
      "Epoch 4353, Loss: 1.5296632498502731, Final Batch Loss: 0.23961646854877472\n",
      "Epoch 4354, Loss: 1.5849527418613434, Final Batch Loss: 0.28945666551589966\n",
      "Epoch 4355, Loss: 1.531048133969307, Final Batch Loss: 0.22804270684719086\n",
      "Epoch 4356, Loss: 1.4992749840021133, Final Batch Loss: 0.3395765423774719\n",
      "Epoch 4357, Loss: 1.4299204796552658, Final Batch Loss: 0.19660697877407074\n",
      "Epoch 4358, Loss: 1.743665188550949, Final Batch Loss: 0.4146556258201599\n",
      "Epoch 4359, Loss: 1.467442274093628, Final Batch Loss: 0.3112732470035553\n",
      "Epoch 4360, Loss: 1.4509291350841522, Final Batch Loss: 0.34445738792419434\n",
      "Epoch 4361, Loss: 1.7588175535202026, Final Batch Loss: 0.4662131369113922\n",
      "Epoch 4362, Loss: 1.6818567514419556, Final Batch Loss: 0.33623206615448\n",
      "Epoch 4363, Loss: 1.3661296963691711, Final Batch Loss: 0.21211400628089905\n",
      "Epoch 4364, Loss: 1.6783823668956757, Final Batch Loss: 0.4692903459072113\n",
      "Epoch 4365, Loss: 1.6090729236602783, Final Batch Loss: 0.40063777565956116\n",
      "Epoch 4366, Loss: 1.4921230375766754, Final Batch Loss: 0.33770614862442017\n",
      "Epoch 4367, Loss: 1.5397440195083618, Final Batch Loss: 0.40053659677505493\n",
      "Epoch 4368, Loss: 1.6793484538793564, Final Batch Loss: 0.525222897529602\n",
      "Epoch 4369, Loss: 1.5463183969259262, Final Batch Loss: 0.2208930104970932\n",
      "Epoch 4370, Loss: 1.278245896100998, Final Batch Loss: 0.2138257920742035\n",
      "Epoch 4371, Loss: 1.568120926618576, Final Batch Loss: 0.4216195046901703\n",
      "Epoch 4372, Loss: 1.5688260793685913, Final Batch Loss: 0.2920117676258087\n",
      "Epoch 4373, Loss: 1.5434821248054504, Final Batch Loss: 0.30313822627067566\n",
      "Epoch 4374, Loss: 1.3817374110221863, Final Batch Loss: 0.18095362186431885\n",
      "Epoch 4375, Loss: 1.3059783726930618, Final Batch Loss: 0.18111123144626617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4376, Loss: 1.71660715341568, Final Batch Loss: 0.3473059833049774\n",
      "Epoch 4377, Loss: 1.5098562240600586, Final Batch Loss: 0.2730509638786316\n",
      "Epoch 4378, Loss: 1.4903626143932343, Final Batch Loss: 0.2521868646144867\n",
      "Epoch 4379, Loss: 1.4382966756820679, Final Batch Loss: 0.25663381814956665\n",
      "Epoch 4380, Loss: 1.4629080295562744, Final Batch Loss: 0.2543099820613861\n",
      "Epoch 4381, Loss: 1.7105997204780579, Final Batch Loss: 0.351215660572052\n",
      "Epoch 4382, Loss: 1.4343453347682953, Final Batch Loss: 0.36637547612190247\n",
      "Epoch 4383, Loss: 1.6060975790023804, Final Batch Loss: 0.3083304166793823\n",
      "Epoch 4384, Loss: 1.5450674146413803, Final Batch Loss: 0.3023025393486023\n",
      "Epoch 4385, Loss: 1.6084570288658142, Final Batch Loss: 0.3480268120765686\n",
      "Epoch 4386, Loss: 1.4618647396564484, Final Batch Loss: 0.261486679315567\n",
      "Epoch 4387, Loss: 1.9338072836399078, Final Batch Loss: 0.40872621536254883\n",
      "Epoch 4388, Loss: 1.5802797377109528, Final Batch Loss: 0.27104443311691284\n",
      "Epoch 4389, Loss: 1.6643817126750946, Final Batch Loss: 0.40529558062553406\n",
      "Epoch 4390, Loss: 1.586521565914154, Final Batch Loss: 0.36920449137687683\n",
      "Epoch 4391, Loss: 1.5373165905475616, Final Batch Loss: 0.24329569935798645\n",
      "Epoch 4392, Loss: 1.4930901229381561, Final Batch Loss: 0.27791789174079895\n",
      "Epoch 4393, Loss: 1.3323463797569275, Final Batch Loss: 0.20477820932865143\n",
      "Epoch 4394, Loss: 1.3656443655490875, Final Batch Loss: 0.23344334959983826\n",
      "Epoch 4395, Loss: 1.4269900023937225, Final Batch Loss: 0.18805208802223206\n",
      "Epoch 4396, Loss: 1.4207851886749268, Final Batch Loss: 0.23621922731399536\n",
      "Epoch 4397, Loss: 1.5284967720508575, Final Batch Loss: 0.28375375270843506\n",
      "Epoch 4398, Loss: 1.6434778571128845, Final Batch Loss: 0.4436720907688141\n",
      "Epoch 4399, Loss: 1.6369389593601227, Final Batch Loss: 0.3232887387275696\n",
      "Epoch 4400, Loss: 1.6231716871261597, Final Batch Loss: 0.3734245002269745\n",
      "Epoch 4401, Loss: 1.6165794730186462, Final Batch Loss: 0.3058582544326782\n",
      "Epoch 4402, Loss: 1.58730947971344, Final Batch Loss: 0.24738427996635437\n",
      "Epoch 4403, Loss: 1.6420184224843979, Final Batch Loss: 0.22038687765598297\n",
      "Epoch 4404, Loss: 1.5684072375297546, Final Batch Loss: 0.3627192974090576\n",
      "Epoch 4405, Loss: 1.6746968030929565, Final Batch Loss: 0.34782105684280396\n",
      "Epoch 4406, Loss: 1.5416942238807678, Final Batch Loss: 0.3182143568992615\n",
      "Epoch 4407, Loss: 1.902244210243225, Final Batch Loss: 0.3580276072025299\n",
      "Epoch 4408, Loss: 1.457346796989441, Final Batch Loss: 0.2826419174671173\n",
      "Epoch 4409, Loss: 1.4492759555578232, Final Batch Loss: 0.33769872784614563\n",
      "Epoch 4410, Loss: 1.5016781985759735, Final Batch Loss: 0.2531832456588745\n",
      "Epoch 4411, Loss: 1.616902083158493, Final Batch Loss: 0.39541491866111755\n",
      "Epoch 4412, Loss: 1.5147965997457504, Final Batch Loss: 0.23006023466587067\n",
      "Epoch 4413, Loss: 1.5713453739881516, Final Batch Loss: 0.4640244245529175\n",
      "Epoch 4414, Loss: 1.5665942877531052, Final Batch Loss: 0.20054559409618378\n",
      "Epoch 4415, Loss: 1.5565237998962402, Final Batch Loss: 0.25917133688926697\n",
      "Epoch 4416, Loss: 1.7874014377593994, Final Batch Loss: 0.5183911323547363\n",
      "Epoch 4417, Loss: 1.6037680804729462, Final Batch Loss: 0.23249876499176025\n",
      "Epoch 4418, Loss: 1.5595029890537262, Final Batch Loss: 0.3064723312854767\n",
      "Epoch 4419, Loss: 1.7687650322914124, Final Batch Loss: 0.5054827332496643\n",
      "Epoch 4420, Loss: 1.483510598540306, Final Batch Loss: 0.2402297705411911\n",
      "Epoch 4421, Loss: 1.7517260909080505, Final Batch Loss: 0.23101836442947388\n",
      "Epoch 4422, Loss: 1.4069411605596542, Final Batch Loss: 0.2621114253997803\n",
      "Epoch 4423, Loss: 1.4687654823064804, Final Batch Loss: 0.23961789906024933\n",
      "Epoch 4424, Loss: 1.780657798051834, Final Batch Loss: 0.38682037591934204\n",
      "Epoch 4425, Loss: 1.7703939229249954, Final Batch Loss: 0.3471800982952118\n",
      "Epoch 4426, Loss: 1.689504235982895, Final Batch Loss: 0.29291263222694397\n",
      "Epoch 4427, Loss: 1.5305665582418442, Final Batch Loss: 0.20220355689525604\n",
      "Epoch 4428, Loss: 1.4628888815641403, Final Batch Loss: 0.358797550201416\n",
      "Epoch 4429, Loss: 1.5746325850486755, Final Batch Loss: 0.2505091726779938\n",
      "Epoch 4430, Loss: 1.5772183239459991, Final Batch Loss: 0.3915902376174927\n",
      "Epoch 4431, Loss: 1.4009058475494385, Final Batch Loss: 0.23013967275619507\n",
      "Epoch 4432, Loss: 1.4196579456329346, Final Batch Loss: 0.23855867981910706\n",
      "Epoch 4433, Loss: 1.5681386888027191, Final Batch Loss: 0.17951437830924988\n",
      "Epoch 4434, Loss: 1.5697408318519592, Final Batch Loss: 0.3961751461029053\n",
      "Epoch 4435, Loss: 1.579362690448761, Final Batch Loss: 0.25881341099739075\n",
      "Epoch 4436, Loss: 1.4855818897485733, Final Batch Loss: 0.28223559260368347\n",
      "Epoch 4437, Loss: 1.4333477467298508, Final Batch Loss: 0.24240146577358246\n",
      "Epoch 4438, Loss: 1.5882773697376251, Final Batch Loss: 0.41894015669822693\n",
      "Epoch 4439, Loss: 1.5705545842647552, Final Batch Loss: 0.27917149662971497\n",
      "Epoch 4440, Loss: 1.3686689138412476, Final Batch Loss: 0.15843909978866577\n",
      "Epoch 4441, Loss: 1.5751672983169556, Final Batch Loss: 0.2966209053993225\n",
      "Epoch 4442, Loss: 1.5968676805496216, Final Batch Loss: 0.4314180910587311\n",
      "Epoch 4443, Loss: 1.6273102462291718, Final Batch Loss: 0.36197060346603394\n",
      "Epoch 4444, Loss: 1.6464948952198029, Final Batch Loss: 0.4222373366355896\n",
      "Epoch 4445, Loss: 1.3942912817001343, Final Batch Loss: 0.2543236315250397\n",
      "Epoch 4446, Loss: 1.5941510945558548, Final Batch Loss: 0.317711740732193\n",
      "Epoch 4447, Loss: 1.6148427426815033, Final Batch Loss: 0.27847781777381897\n",
      "Epoch 4448, Loss: 1.3954768180847168, Final Batch Loss: 0.2203999012708664\n",
      "Epoch 4449, Loss: 1.372543916106224, Final Batch Loss: 0.2436019778251648\n",
      "Epoch 4450, Loss: 1.3954806178808212, Final Batch Loss: 0.308388888835907\n",
      "Epoch 4451, Loss: 1.4334318786859512, Final Batch Loss: 0.15592233836650848\n",
      "Epoch 4452, Loss: 1.6481082141399384, Final Batch Loss: 0.42587795853614807\n",
      "Epoch 4453, Loss: 1.4367473423480988, Final Batch Loss: 0.2991349995136261\n",
      "Epoch 4454, Loss: 1.4556615054607391, Final Batch Loss: 0.35801443457603455\n",
      "Epoch 4455, Loss: 1.607539802789688, Final Batch Loss: 0.3297576308250427\n",
      "Epoch 4456, Loss: 1.672924816608429, Final Batch Loss: 0.37267985939979553\n",
      "Epoch 4457, Loss: 1.456663340330124, Final Batch Loss: 0.2506529688835144\n",
      "Epoch 4458, Loss: 1.5398203432559967, Final Batch Loss: 0.33143264055252075\n",
      "Epoch 4459, Loss: 1.5151901841163635, Final Batch Loss: 0.3804304301738739\n",
      "Epoch 4460, Loss: 1.5915068089962006, Final Batch Loss: 0.28549933433532715\n",
      "Epoch 4461, Loss: 1.818465143442154, Final Batch Loss: 0.6201803684234619\n",
      "Epoch 4462, Loss: 1.5889091044664383, Final Batch Loss: 0.16360805928707123\n",
      "Epoch 4463, Loss: 1.5148458778858185, Final Batch Loss: 0.3185579180717468\n",
      "Epoch 4464, Loss: 1.4054170995950699, Final Batch Loss: 0.17389044165611267\n",
      "Epoch 4465, Loss: 1.38836570084095, Final Batch Loss: 0.25508439540863037\n",
      "Epoch 4466, Loss: 1.495247796177864, Final Batch Loss: 0.23293007910251617\n",
      "Epoch 4467, Loss: 1.4624110460281372, Final Batch Loss: 0.27233001589775085\n",
      "Epoch 4468, Loss: 1.3345032036304474, Final Batch Loss: 0.2325710952281952\n",
      "Epoch 4469, Loss: 1.5933564901351929, Final Batch Loss: 0.35271787643432617\n",
      "Epoch 4470, Loss: 1.5502519011497498, Final Batch Loss: 0.34165436029434204\n",
      "Epoch 4471, Loss: 1.6457454562187195, Final Batch Loss: 0.3802469074726105\n",
      "Epoch 4472, Loss: 1.405162289738655, Final Batch Loss: 0.2760084569454193\n",
      "Epoch 4473, Loss: 1.637560784816742, Final Batch Loss: 0.3794975280761719\n",
      "Epoch 4474, Loss: 1.368283987045288, Final Batch Loss: 0.17918863892555237\n",
      "Epoch 4475, Loss: 1.5228765606880188, Final Batch Loss: 0.28887829184532166\n",
      "Epoch 4476, Loss: 1.6430851519107819, Final Batch Loss: 0.2565467059612274\n",
      "Epoch 4477, Loss: 1.3927763998508453, Final Batch Loss: 0.17428457736968994\n",
      "Epoch 4478, Loss: 1.4373162984848022, Final Batch Loss: 0.22988376021385193\n",
      "Epoch 4479, Loss: 1.67268505692482, Final Batch Loss: 0.3794037699699402\n",
      "Epoch 4480, Loss: 1.6047406196594238, Final Batch Loss: 0.34197697043418884\n",
      "Epoch 4481, Loss: 1.4516971111297607, Final Batch Loss: 0.2738996148109436\n",
      "Epoch 4482, Loss: 1.4940951317548752, Final Batch Loss: 0.23470212519168854\n",
      "Epoch 4483, Loss: 1.5072124302387238, Final Batch Loss: 0.34649649262428284\n",
      "Epoch 4484, Loss: 1.4959256052970886, Final Batch Loss: 0.40404728055000305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4485, Loss: 1.432875156402588, Final Batch Loss: 0.35539400577545166\n",
      "Epoch 4486, Loss: 1.3784946948289871, Final Batch Loss: 0.2178160399198532\n",
      "Epoch 4487, Loss: 1.6909875869750977, Final Batch Loss: 0.37696927785873413\n",
      "Epoch 4488, Loss: 1.5586724132299423, Final Batch Loss: 0.3432197570800781\n",
      "Epoch 4489, Loss: 1.681799292564392, Final Batch Loss: 0.2738228142261505\n",
      "Epoch 4490, Loss: 1.440302699804306, Final Batch Loss: 0.2651030421257019\n",
      "Epoch 4491, Loss: 1.607825294137001, Final Batch Loss: 0.4238402247428894\n",
      "Epoch 4492, Loss: 1.5206026881933212, Final Batch Loss: 0.23056022822856903\n",
      "Epoch 4493, Loss: 1.4423476457595825, Final Batch Loss: 0.2744959592819214\n",
      "Epoch 4494, Loss: 1.5403049141168594, Final Batch Loss: 0.2580733895301819\n",
      "Epoch 4495, Loss: 1.6734522134065628, Final Batch Loss: 0.2452637106180191\n",
      "Epoch 4496, Loss: 1.8341387212276459, Final Batch Loss: 0.2862367630004883\n",
      "Epoch 4497, Loss: 1.3960971236228943, Final Batch Loss: 0.23815682530403137\n",
      "Epoch 4498, Loss: 1.3670395612716675, Final Batch Loss: 0.25901302695274353\n",
      "Epoch 4499, Loss: 1.682420939207077, Final Batch Loss: 0.43620023131370544\n",
      "Epoch 4500, Loss: 1.2616117596626282, Final Batch Loss: 0.17214587330818176\n",
      "Epoch 4501, Loss: 1.5355621874332428, Final Batch Loss: 0.3159904181957245\n",
      "Epoch 4502, Loss: 1.490684449672699, Final Batch Loss: 0.2735840082168579\n",
      "Epoch 4503, Loss: 1.3752939105033875, Final Batch Loss: 0.3332000970840454\n",
      "Epoch 4504, Loss: 1.667451173067093, Final Batch Loss: 0.2849789261817932\n",
      "Epoch 4505, Loss: 1.4044398963451385, Final Batch Loss: 0.2121260017156601\n",
      "Epoch 4506, Loss: 1.6420653313398361, Final Batch Loss: 0.32510972023010254\n",
      "Epoch 4507, Loss: 1.6799925565719604, Final Batch Loss: 0.4927992522716522\n",
      "Epoch 4508, Loss: 1.650098279118538, Final Batch Loss: 0.4147530496120453\n",
      "Epoch 4509, Loss: 1.4975269734859467, Final Batch Loss: 0.2273980975151062\n",
      "Epoch 4510, Loss: 1.6588507890701294, Final Batch Loss: 0.36749759316444397\n",
      "Epoch 4511, Loss: 1.4733999371528625, Final Batch Loss: 0.28004926443099976\n",
      "Epoch 4512, Loss: 1.5916320532560349, Final Batch Loss: 0.4578460156917572\n",
      "Epoch 4513, Loss: 1.455808311700821, Final Batch Loss: 0.26154425740242004\n",
      "Epoch 4514, Loss: 1.488638013601303, Final Batch Loss: 0.26291903853416443\n",
      "Epoch 4515, Loss: 1.6034047156572342, Final Batch Loss: 0.31844890117645264\n",
      "Epoch 4516, Loss: 1.575120747089386, Final Batch Loss: 0.2758809030056\n",
      "Epoch 4517, Loss: 1.7240547835826874, Final Batch Loss: 0.4097880423069\n",
      "Epoch 4518, Loss: 1.514927715063095, Final Batch Loss: 0.23212933540344238\n",
      "Epoch 4519, Loss: 1.5204778611660004, Final Batch Loss: 0.2666614055633545\n",
      "Epoch 4520, Loss: 1.7999935150146484, Final Batch Loss: 0.41629284620285034\n",
      "Epoch 4521, Loss: 1.6069424152374268, Final Batch Loss: 0.3613252341747284\n",
      "Epoch 4522, Loss: 1.5649165213108063, Final Batch Loss: 0.36527976393699646\n",
      "Epoch 4523, Loss: 1.4586107283830643, Final Batch Loss: 0.12905676662921906\n",
      "Epoch 4524, Loss: 1.418745145201683, Final Batch Loss: 0.21113373339176178\n",
      "Epoch 4525, Loss: 1.3188210874795914, Final Batch Loss: 0.13484828174114227\n",
      "Epoch 4526, Loss: 1.4015363603830338, Final Batch Loss: 0.2492557168006897\n",
      "Epoch 4527, Loss: 1.5019058138132095, Final Batch Loss: 0.2139682024717331\n",
      "Epoch 4528, Loss: 1.6267867982387543, Final Batch Loss: 0.38696199655532837\n",
      "Epoch 4529, Loss: 1.802545577287674, Final Batch Loss: 0.5644493103027344\n",
      "Epoch 4530, Loss: 1.5929908752441406, Final Batch Loss: 0.34688422083854675\n",
      "Epoch 4531, Loss: 1.6857640445232391, Final Batch Loss: 0.46364736557006836\n",
      "Epoch 4532, Loss: 1.518694207072258, Final Batch Loss: 0.3236183524131775\n",
      "Epoch 4533, Loss: 1.5543137341737747, Final Batch Loss: 0.22269444167613983\n",
      "Epoch 4534, Loss: 1.6390709280967712, Final Batch Loss: 0.35265281796455383\n",
      "Epoch 4535, Loss: 1.7023711204528809, Final Batch Loss: 0.2986409366130829\n",
      "Epoch 4536, Loss: 1.4240532368421555, Final Batch Loss: 0.21354569494724274\n",
      "Epoch 4537, Loss: 1.5031431317329407, Final Batch Loss: 0.3003987669944763\n",
      "Epoch 4538, Loss: 1.4114654660224915, Final Batch Loss: 0.19512954354286194\n",
      "Epoch 4539, Loss: 1.6454089879989624, Final Batch Loss: 0.34965771436691284\n",
      "Epoch 4540, Loss: 1.4277964681386948, Final Batch Loss: 0.2888905704021454\n",
      "Epoch 4541, Loss: 1.447430968284607, Final Batch Loss: 0.2918575704097748\n",
      "Epoch 4542, Loss: 1.5611741542816162, Final Batch Loss: 0.27944353222846985\n",
      "Epoch 4543, Loss: 1.4593038707971573, Final Batch Loss: 0.2665118873119354\n",
      "Epoch 4544, Loss: 1.3480356633663177, Final Batch Loss: 0.13796056807041168\n",
      "Epoch 4545, Loss: 1.4229120761156082, Final Batch Loss: 0.36460208892822266\n",
      "Epoch 4546, Loss: 1.4958042055368423, Final Batch Loss: 0.22083646059036255\n",
      "Epoch 4547, Loss: 1.657677710056305, Final Batch Loss: 0.27450788021087646\n",
      "Epoch 4548, Loss: 1.4818292260169983, Final Batch Loss: 0.3110150098800659\n",
      "Epoch 4549, Loss: 1.4892308562994003, Final Batch Loss: 0.2320956140756607\n",
      "Epoch 4550, Loss: 1.8084030151367188, Final Batch Loss: 0.4642208218574524\n",
      "Epoch 4551, Loss: 1.4251319617033005, Final Batch Loss: 0.20097477734088898\n",
      "Epoch 4552, Loss: 1.459586277604103, Final Batch Loss: 0.2928353250026703\n",
      "Epoch 4553, Loss: 1.5112777948379517, Final Batch Loss: 0.2527652382850647\n",
      "Epoch 4554, Loss: 1.546383410692215, Final Batch Loss: 0.32006245851516724\n",
      "Epoch 4555, Loss: 1.871552050113678, Final Batch Loss: 0.482477068901062\n",
      "Epoch 4556, Loss: 1.3471393436193466, Final Batch Loss: 0.14914487302303314\n",
      "Epoch 4557, Loss: 1.5574195981025696, Final Batch Loss: 0.3835766613483429\n",
      "Epoch 4558, Loss: 1.6940706372261047, Final Batch Loss: 0.3612733483314514\n",
      "Epoch 4559, Loss: 1.6455612480640411, Final Batch Loss: 0.304697185754776\n",
      "Epoch 4560, Loss: 1.4764418601989746, Final Batch Loss: 0.25933247804641724\n",
      "Epoch 4561, Loss: 1.4830278009176254, Final Batch Loss: 0.35861653089523315\n",
      "Epoch 4562, Loss: 1.3525148332118988, Final Batch Loss: 0.22521932423114777\n",
      "Epoch 4563, Loss: 1.5000377595424652, Final Batch Loss: 0.3470064103603363\n",
      "Epoch 4564, Loss: 1.6712005734443665, Final Batch Loss: 0.36686185002326965\n",
      "Epoch 4565, Loss: 1.5335438251495361, Final Batch Loss: 0.31322604417800903\n",
      "Epoch 4566, Loss: 1.494076132774353, Final Batch Loss: 0.28857913613319397\n",
      "Epoch 4567, Loss: 1.4563369005918503, Final Batch Loss: 0.2671874761581421\n",
      "Epoch 4568, Loss: 1.4621219635009766, Final Batch Loss: 0.22906452417373657\n",
      "Epoch 4569, Loss: 1.5877823233604431, Final Batch Loss: 0.43199989199638367\n",
      "Epoch 4570, Loss: 1.4961769580841064, Final Batch Loss: 0.4559673070907593\n",
      "Epoch 4571, Loss: 1.540179818868637, Final Batch Loss: 0.28349199891090393\n",
      "Epoch 4572, Loss: 1.5928574055433273, Final Batch Loss: 0.42185741662979126\n",
      "Epoch 4573, Loss: 1.6174183189868927, Final Batch Loss: 0.3754390776157379\n",
      "Epoch 4574, Loss: 1.8608922064304352, Final Batch Loss: 0.3742585778236389\n",
      "Epoch 4575, Loss: 1.5105262249708176, Final Batch Loss: 0.4812503159046173\n",
      "Epoch 4576, Loss: 1.7529796361923218, Final Batch Loss: 0.4739978313446045\n",
      "Epoch 4577, Loss: 1.5692384839057922, Final Batch Loss: 0.22724391520023346\n",
      "Epoch 4578, Loss: 1.4228860437870026, Final Batch Loss: 0.35669103264808655\n",
      "Epoch 4579, Loss: 1.6293350458145142, Final Batch Loss: 0.33875909447669983\n",
      "Epoch 4580, Loss: 1.4519532918930054, Final Batch Loss: 0.3542298376560211\n",
      "Epoch 4581, Loss: 1.3972687423229218, Final Batch Loss: 0.1452227234840393\n",
      "Epoch 4582, Loss: 1.3832393884658813, Final Batch Loss: 0.27448800206184387\n",
      "Epoch 4583, Loss: 1.5415548980236053, Final Batch Loss: 0.4620020389556885\n",
      "Epoch 4584, Loss: 1.6532536745071411, Final Batch Loss: 0.29186275601387024\n",
      "Epoch 4585, Loss: 1.4998530447483063, Final Batch Loss: 0.30373573303222656\n",
      "Epoch 4586, Loss: 1.5133333504199982, Final Batch Loss: 0.2928391695022583\n",
      "Epoch 4587, Loss: 1.5230019092559814, Final Batch Loss: 0.2988511621952057\n",
      "Epoch 4588, Loss: 1.765599638223648, Final Batch Loss: 0.4338718354701996\n",
      "Epoch 4589, Loss: 1.5442597717046738, Final Batch Loss: 0.22585256397724152\n",
      "Epoch 4590, Loss: 1.4646437615156174, Final Batch Loss: 0.29965856671333313\n",
      "Epoch 4591, Loss: 1.501866102218628, Final Batch Loss: 0.2543671727180481\n",
      "Epoch 4592, Loss: 1.5309364944696426, Final Batch Loss: 0.28605321049690247\n",
      "Epoch 4593, Loss: 1.5486449599266052, Final Batch Loss: 0.2767946422100067\n",
      "Epoch 4594, Loss: 1.5043646097183228, Final Batch Loss: 0.3399758040904999\n",
      "Epoch 4595, Loss: 1.4631332755088806, Final Batch Loss: 0.24433660507202148\n",
      "Epoch 4596, Loss: 1.6685917377471924, Final Batch Loss: 0.30949512124061584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4597, Loss: 1.660228967666626, Final Batch Loss: 0.35384660959243774\n",
      "Epoch 4598, Loss: 1.5379828214645386, Final Batch Loss: 0.27411893010139465\n",
      "Epoch 4599, Loss: 1.5893291234970093, Final Batch Loss: 0.36825504899024963\n",
      "Epoch 4600, Loss: 1.5536043494939804, Final Batch Loss: 0.21709449589252472\n",
      "Epoch 4601, Loss: 1.5337744504213333, Final Batch Loss: 0.3623094856739044\n",
      "Epoch 4602, Loss: 1.4109115451574326, Final Batch Loss: 0.30884531140327454\n",
      "Epoch 4603, Loss: 1.3574610203504562, Final Batch Loss: 0.23118086159229279\n",
      "Epoch 4604, Loss: 1.664069727063179, Final Batch Loss: 0.44918763637542725\n",
      "Epoch 4605, Loss: 1.5468408316373825, Final Batch Loss: 0.37575674057006836\n",
      "Epoch 4606, Loss: 1.6012803763151169, Final Batch Loss: 0.43137606978416443\n",
      "Epoch 4607, Loss: 1.6281926929950714, Final Batch Loss: 0.3787887990474701\n",
      "Epoch 4608, Loss: 1.6793595850467682, Final Batch Loss: 0.3022451400756836\n",
      "Epoch 4609, Loss: 1.4184350222349167, Final Batch Loss: 0.2533683478832245\n",
      "Epoch 4610, Loss: 1.5562750846147537, Final Batch Loss: 0.39203721284866333\n",
      "Epoch 4611, Loss: 1.5816118121147156, Final Batch Loss: 0.35832980275154114\n",
      "Epoch 4612, Loss: 1.4634701758623123, Final Batch Loss: 0.24707229435443878\n",
      "Epoch 4613, Loss: 1.461194172501564, Final Batch Loss: 0.2084750086069107\n",
      "Epoch 4614, Loss: 1.4297386556863785, Final Batch Loss: 0.22188825905323029\n",
      "Epoch 4615, Loss: 1.5012924075126648, Final Batch Loss: 0.28854358196258545\n",
      "Epoch 4616, Loss: 1.5904646217823029, Final Batch Loss: 0.25093168020248413\n",
      "Epoch 4617, Loss: 1.5933305025100708, Final Batch Loss: 0.3639667332172394\n",
      "Epoch 4618, Loss: 1.7011221945285797, Final Batch Loss: 0.3853496015071869\n",
      "Epoch 4619, Loss: 1.5863290578126907, Final Batch Loss: 0.2344578057527542\n",
      "Epoch 4620, Loss: 1.413145899772644, Final Batch Loss: 0.18946832418441772\n",
      "Epoch 4621, Loss: 1.6599071025848389, Final Batch Loss: 0.41209226846694946\n",
      "Epoch 4622, Loss: 1.5404208898544312, Final Batch Loss: 0.3306379020214081\n",
      "Epoch 4623, Loss: 1.8029628098011017, Final Batch Loss: 0.516747236251831\n",
      "Epoch 4624, Loss: 1.5419350862503052, Final Batch Loss: 0.3333386182785034\n",
      "Epoch 4625, Loss: 1.5999444127082825, Final Batch Loss: 0.3054331839084625\n",
      "Epoch 4626, Loss: 1.4158188253641129, Final Batch Loss: 0.2606910467147827\n",
      "Epoch 4627, Loss: 1.633483186364174, Final Batch Loss: 0.2938763499259949\n",
      "Epoch 4628, Loss: 1.714235931634903, Final Batch Loss: 0.42397353053092957\n",
      "Epoch 4629, Loss: 1.601298600435257, Final Batch Loss: 0.4677676856517792\n",
      "Epoch 4630, Loss: 1.6925311088562012, Final Batch Loss: 0.2798817455768585\n",
      "Epoch 4631, Loss: 1.3933236300945282, Final Batch Loss: 0.28271183371543884\n",
      "Epoch 4632, Loss: 1.6658070087432861, Final Batch Loss: 0.35446247458457947\n",
      "Epoch 4633, Loss: 1.4164440333843231, Final Batch Loss: 0.2919618487358093\n",
      "Epoch 4634, Loss: 1.5012477338314056, Final Batch Loss: 0.29148542881011963\n",
      "Epoch 4635, Loss: 1.4701275378465652, Final Batch Loss: 0.40376538038253784\n",
      "Epoch 4636, Loss: 1.4827316999435425, Final Batch Loss: 0.26231563091278076\n",
      "Epoch 4637, Loss: 1.459581434726715, Final Batch Loss: 0.2650148272514343\n",
      "Epoch 4638, Loss: 1.5676588118076324, Final Batch Loss: 0.42064666748046875\n",
      "Epoch 4639, Loss: 1.5385291874408722, Final Batch Loss: 0.3032492995262146\n",
      "Epoch 4640, Loss: 1.4612691849470139, Final Batch Loss: 0.28700822591781616\n",
      "Epoch 4641, Loss: 1.6642172932624817, Final Batch Loss: 0.31550464034080505\n",
      "Epoch 4642, Loss: 1.5013836920261383, Final Batch Loss: 0.21359547972679138\n",
      "Epoch 4643, Loss: 1.4934931695461273, Final Batch Loss: 0.3099256157875061\n",
      "Epoch 4644, Loss: 1.73311647772789, Final Batch Loss: 0.32343417406082153\n",
      "Epoch 4645, Loss: 1.3485888838768005, Final Batch Loss: 0.18914127349853516\n",
      "Epoch 4646, Loss: 1.3523629754781723, Final Batch Loss: 0.24305425584316254\n",
      "Epoch 4647, Loss: 1.6480934023857117, Final Batch Loss: 0.3591659963130951\n",
      "Epoch 4648, Loss: 1.5574541985988617, Final Batch Loss: 0.252028226852417\n",
      "Epoch 4649, Loss: 1.4870599657297134, Final Batch Loss: 0.2165343016386032\n",
      "Epoch 4650, Loss: 1.586988478899002, Final Batch Loss: 0.3605675995349884\n",
      "Epoch 4651, Loss: 1.5508582890033722, Final Batch Loss: 0.2822144627571106\n",
      "Epoch 4652, Loss: 1.5301807224750519, Final Batch Loss: 0.23334383964538574\n",
      "Epoch 4653, Loss: 1.5571842938661575, Final Batch Loss: 0.48703864216804504\n",
      "Epoch 4654, Loss: 1.3465320020914078, Final Batch Loss: 0.19333000481128693\n",
      "Epoch 4655, Loss: 1.531358152627945, Final Batch Loss: 0.4254845082759857\n",
      "Epoch 4656, Loss: 1.549732118844986, Final Batch Loss: 0.3213322162628174\n",
      "Epoch 4657, Loss: 1.5269334316253662, Final Batch Loss: 0.2881471812725067\n",
      "Epoch 4658, Loss: 1.5461972802877426, Final Batch Loss: 0.3972613215446472\n",
      "Epoch 4659, Loss: 1.4202731102705002, Final Batch Loss: 0.1867024153470993\n",
      "Epoch 4660, Loss: 1.4438323676586151, Final Batch Loss: 0.23367422819137573\n",
      "Epoch 4661, Loss: 1.728623390197754, Final Batch Loss: 0.443417489528656\n",
      "Epoch 4662, Loss: 1.3620449155569077, Final Batch Loss: 0.3132953941822052\n",
      "Epoch 4663, Loss: 1.3324185758829117, Final Batch Loss: 0.1277870386838913\n",
      "Epoch 4664, Loss: 1.6159146428108215, Final Batch Loss: 0.3801517188549042\n",
      "Epoch 4665, Loss: 1.5362798273563385, Final Batch Loss: 0.37327104806900024\n",
      "Epoch 4666, Loss: 1.60245580971241, Final Batch Loss: 0.2807844281196594\n",
      "Epoch 4667, Loss: 1.6571325659751892, Final Batch Loss: 0.3643738031387329\n",
      "Epoch 4668, Loss: 1.7184358835220337, Final Batch Loss: 0.4093663692474365\n",
      "Epoch 4669, Loss: 1.6527144014835358, Final Batch Loss: 0.28353387117385864\n",
      "Epoch 4670, Loss: 1.7040237188339233, Final Batch Loss: 0.37938767671585083\n",
      "Epoch 4671, Loss: 1.451121300458908, Final Batch Loss: 0.24209055304527283\n",
      "Epoch 4672, Loss: 1.6757686883211136, Final Batch Loss: 0.3432532250881195\n",
      "Epoch 4673, Loss: 1.6755589842796326, Final Batch Loss: 0.3121373653411865\n",
      "Epoch 4674, Loss: 1.5989489555358887, Final Batch Loss: 0.3639031946659088\n",
      "Epoch 4675, Loss: 1.577420026063919, Final Batch Loss: 0.3101581931114197\n",
      "Epoch 4676, Loss: 1.541611760854721, Final Batch Loss: 0.2948889434337616\n",
      "Epoch 4677, Loss: 1.49856136739254, Final Batch Loss: 0.22938264906406403\n",
      "Epoch 4678, Loss: 1.6078031063079834, Final Batch Loss: 0.25874945521354675\n",
      "Epoch 4679, Loss: 1.3533705472946167, Final Batch Loss: 0.2696518301963806\n",
      "Epoch 4680, Loss: 1.6281203627586365, Final Batch Loss: 0.3561428189277649\n",
      "Epoch 4681, Loss: 1.673741102218628, Final Batch Loss: 0.5862242579460144\n",
      "Epoch 4682, Loss: 1.635260447859764, Final Batch Loss: 0.508643627166748\n",
      "Epoch 4683, Loss: 1.4377728402614594, Final Batch Loss: 0.22541585564613342\n",
      "Epoch 4684, Loss: 1.5964960157871246, Final Batch Loss: 0.41544970870018005\n",
      "Epoch 4685, Loss: 1.5698926746845245, Final Batch Loss: 0.33447620272636414\n",
      "Epoch 4686, Loss: 1.4029403328895569, Final Batch Loss: 0.16947296261787415\n",
      "Epoch 4687, Loss: 1.4493750035762787, Final Batch Loss: 0.2000274956226349\n",
      "Epoch 4688, Loss: 1.493561029434204, Final Batch Loss: 0.27264195680618286\n",
      "Epoch 4689, Loss: 1.5581040978431702, Final Batch Loss: 0.3063739538192749\n",
      "Epoch 4690, Loss: 1.5827611982822418, Final Batch Loss: 0.33891206979751587\n",
      "Epoch 4691, Loss: 1.4568270146846771, Final Batch Loss: 0.2826882600784302\n",
      "Epoch 4692, Loss: 1.5982299745082855, Final Batch Loss: 0.3998652994632721\n",
      "Epoch 4693, Loss: 1.2926654666662216, Final Batch Loss: 0.13494615256786346\n",
      "Epoch 4694, Loss: 1.3784358501434326, Final Batch Loss: 0.23151659965515137\n",
      "Epoch 4695, Loss: 1.3414108604192734, Final Batch Loss: 0.2582644522190094\n",
      "Epoch 4696, Loss: 1.4926117211580276, Final Batch Loss: 0.3422437012195587\n",
      "Epoch 4697, Loss: 1.5326290726661682, Final Batch Loss: 0.25438928604125977\n",
      "Epoch 4698, Loss: 1.3629150986671448, Final Batch Loss: 0.24024814367294312\n",
      "Epoch 4699, Loss: 1.4625371098518372, Final Batch Loss: 0.3478569984436035\n",
      "Epoch 4700, Loss: 1.4736017882823944, Final Batch Loss: 0.23663845658302307\n",
      "Epoch 4701, Loss: 1.5459722578525543, Final Batch Loss: 0.38199007511138916\n",
      "Epoch 4702, Loss: 1.5266598165035248, Final Batch Loss: 0.34503793716430664\n",
      "Epoch 4703, Loss: 1.635501652956009, Final Batch Loss: 0.3602529764175415\n",
      "Epoch 4704, Loss: 1.6899215579032898, Final Batch Loss: 0.4533459544181824\n",
      "Epoch 4705, Loss: 1.279145509004593, Final Batch Loss: 0.14197619259357452\n",
      "Epoch 4706, Loss: 1.426092118024826, Final Batch Loss: 0.14945191144943237\n",
      "Epoch 4707, Loss: 1.5245898962020874, Final Batch Loss: 0.2629319429397583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4708, Loss: 1.4470634013414383, Final Batch Loss: 0.2995460331439972\n",
      "Epoch 4709, Loss: 1.3567363023757935, Final Batch Loss: 0.28041207790374756\n",
      "Epoch 4710, Loss: 1.524781882762909, Final Batch Loss: 0.322166383266449\n",
      "Epoch 4711, Loss: 1.6290321350097656, Final Batch Loss: 0.3712245523929596\n",
      "Epoch 4712, Loss: 1.7046122252941132, Final Batch Loss: 0.3155785799026489\n",
      "Epoch 4713, Loss: 1.756614476442337, Final Batch Loss: 0.31447213888168335\n",
      "Epoch 4714, Loss: 1.370419755578041, Final Batch Loss: 0.2928529977798462\n",
      "Epoch 4715, Loss: 1.4298617243766785, Final Batch Loss: 0.33583009243011475\n",
      "Epoch 4716, Loss: 1.7442347258329391, Final Batch Loss: 0.33311399817466736\n",
      "Epoch 4717, Loss: 1.5047753304243088, Final Batch Loss: 0.3038581907749176\n",
      "Epoch 4718, Loss: 1.4945415556430817, Final Batch Loss: 0.305823415517807\n",
      "Epoch 4719, Loss: 1.4912221878767014, Final Batch Loss: 0.4002862870693207\n",
      "Epoch 4720, Loss: 1.4719853848218918, Final Batch Loss: 0.3872593939304352\n",
      "Epoch 4721, Loss: 1.4039824306964874, Final Batch Loss: 0.18056190013885498\n",
      "Epoch 4722, Loss: 1.5463876724243164, Final Batch Loss: 0.38755863904953003\n",
      "Epoch 4723, Loss: 1.5028199255466461, Final Batch Loss: 0.2764284610748291\n",
      "Epoch 4724, Loss: 1.4538471400737762, Final Batch Loss: 0.39251089096069336\n",
      "Epoch 4725, Loss: 1.553686410188675, Final Batch Loss: 0.2248668372631073\n",
      "Epoch 4726, Loss: 1.4793561398983002, Final Batch Loss: 0.2924354076385498\n",
      "Epoch 4727, Loss: 1.794594794511795, Final Batch Loss: 0.6213070154190063\n",
      "Epoch 4728, Loss: 1.612694263458252, Final Batch Loss: 0.42681995034217834\n",
      "Epoch 4729, Loss: 1.6353607177734375, Final Batch Loss: 0.35396814346313477\n",
      "Epoch 4730, Loss: 1.403053343296051, Final Batch Loss: 0.30472350120544434\n",
      "Epoch 4731, Loss: 1.7415399551391602, Final Batch Loss: 0.37250012159347534\n",
      "Epoch 4732, Loss: 1.5261653363704681, Final Batch Loss: 0.3076700270175934\n",
      "Epoch 4733, Loss: 1.5647999942302704, Final Batch Loss: 0.3269130289554596\n",
      "Epoch 4734, Loss: 1.7940974086523056, Final Batch Loss: 0.5529286861419678\n",
      "Epoch 4735, Loss: 1.5828202962875366, Final Batch Loss: 0.3247278034687042\n",
      "Epoch 4736, Loss: 1.5332304537296295, Final Batch Loss: 0.2164098024368286\n",
      "Epoch 4737, Loss: 1.6958653032779694, Final Batch Loss: 0.3635825514793396\n",
      "Epoch 4738, Loss: 1.5607115477323532, Final Batch Loss: 0.22484150528907776\n",
      "Epoch 4739, Loss: 1.6332826912403107, Final Batch Loss: 0.4519539773464203\n",
      "Epoch 4740, Loss: 1.560374066233635, Final Batch Loss: 0.19701416790485382\n",
      "Epoch 4741, Loss: 1.5691980123519897, Final Batch Loss: 0.39816462993621826\n",
      "Epoch 4742, Loss: 1.3505113422870636, Final Batch Loss: 0.1948147416114807\n",
      "Epoch 4743, Loss: 1.5648262202739716, Final Batch Loss: 0.3306354284286499\n",
      "Epoch 4744, Loss: 1.6089986860752106, Final Batch Loss: 0.3203471302986145\n",
      "Epoch 4745, Loss: 1.7615608870983124, Final Batch Loss: 0.35412803292274475\n",
      "Epoch 4746, Loss: 1.5170282125473022, Final Batch Loss: 0.20521682500839233\n",
      "Epoch 4747, Loss: 1.5235103368759155, Final Batch Loss: 0.2616831660270691\n",
      "Epoch 4748, Loss: 1.5381339192390442, Final Batch Loss: 0.3826877474784851\n",
      "Epoch 4749, Loss: 1.7665211856365204, Final Batch Loss: 0.4884716868400574\n",
      "Epoch 4750, Loss: 1.5631161332130432, Final Batch Loss: 0.37410587072372437\n",
      "Epoch 4751, Loss: 1.558633267879486, Final Batch Loss: 0.2962503433227539\n",
      "Epoch 4752, Loss: 1.4706641286611557, Final Batch Loss: 0.29973599314689636\n",
      "Epoch 4753, Loss: 1.3910385221242905, Final Batch Loss: 0.2104530781507492\n",
      "Epoch 4754, Loss: 1.4844160974025726, Final Batch Loss: 0.34471598267555237\n",
      "Epoch 4755, Loss: 1.3538692593574524, Final Batch Loss: 0.16721653938293457\n",
      "Epoch 4756, Loss: 1.6978693008422852, Final Batch Loss: 0.4533484876155853\n",
      "Epoch 4757, Loss: 1.5753632485866547, Final Batch Loss: 0.3919377624988556\n",
      "Epoch 4758, Loss: 1.6775439977645874, Final Batch Loss: 0.3845846652984619\n",
      "Epoch 4759, Loss: 1.4006089270114899, Final Batch Loss: 0.2949361503124237\n",
      "Epoch 4760, Loss: 1.6518548727035522, Final Batch Loss: 0.29604291915893555\n",
      "Epoch 4761, Loss: 1.515813559293747, Final Batch Loss: 0.3117313086986542\n",
      "Epoch 4762, Loss: 1.4160002171993256, Final Batch Loss: 0.27707502245903015\n",
      "Epoch 4763, Loss: 1.6572133153676987, Final Batch Loss: 0.23628289997577667\n",
      "Epoch 4764, Loss: 1.6095771044492722, Final Batch Loss: 0.5218738913536072\n",
      "Epoch 4765, Loss: 1.564834088087082, Final Batch Loss: 0.2634795904159546\n",
      "Epoch 4766, Loss: 1.5532843321561813, Final Batch Loss: 0.41753754019737244\n",
      "Epoch 4767, Loss: 1.6332199275493622, Final Batch Loss: 0.32342302799224854\n",
      "Epoch 4768, Loss: 1.3474737852811813, Final Batch Loss: 0.23153485357761383\n",
      "Epoch 4769, Loss: 1.68312007188797, Final Batch Loss: 0.5026886463165283\n",
      "Epoch 4770, Loss: 1.7478090226650238, Final Batch Loss: 0.459104061126709\n",
      "Epoch 4771, Loss: 1.686009407043457, Final Batch Loss: 0.36975735425949097\n",
      "Epoch 4772, Loss: 1.581554114818573, Final Batch Loss: 0.28501462936401367\n",
      "Epoch 4773, Loss: 1.6389534175395966, Final Batch Loss: 0.39579638838768005\n",
      "Epoch 4774, Loss: 1.4496248364448547, Final Batch Loss: 0.23444348573684692\n",
      "Epoch 4775, Loss: 1.5162649750709534, Final Batch Loss: 0.27998676896095276\n",
      "Epoch 4776, Loss: 1.3734129667282104, Final Batch Loss: 0.2200738787651062\n",
      "Epoch 4777, Loss: 1.4424870163202286, Final Batch Loss: 0.23858320713043213\n",
      "Epoch 4778, Loss: 1.6350237429141998, Final Batch Loss: 0.28320789337158203\n",
      "Epoch 4779, Loss: 1.4672008752822876, Final Batch Loss: 0.27316632866859436\n",
      "Epoch 4780, Loss: 1.4583376944065094, Final Batch Loss: 0.37814265489578247\n",
      "Epoch 4781, Loss: 2.042789965867996, Final Batch Loss: 0.6992879509925842\n",
      "Epoch 4782, Loss: 1.6745403110980988, Final Batch Loss: 0.4221515953540802\n",
      "Epoch 4783, Loss: 1.5756294876337051, Final Batch Loss: 0.2878779172897339\n",
      "Epoch 4784, Loss: 1.4949786216020584, Final Batch Loss: 0.22284246981143951\n",
      "Epoch 4785, Loss: 1.4555659294128418, Final Batch Loss: 0.1744840443134308\n",
      "Epoch 4786, Loss: 1.6225855648517609, Final Batch Loss: 0.19736960530281067\n",
      "Epoch 4787, Loss: 1.6982067823410034, Final Batch Loss: 0.37455692887306213\n",
      "Epoch 4788, Loss: 1.7188660502433777, Final Batch Loss: 0.4116436243057251\n",
      "Epoch 4789, Loss: 1.7325659841299057, Final Batch Loss: 0.5692858099937439\n",
      "Epoch 4790, Loss: 1.5956188142299652, Final Batch Loss: 0.38953933119773865\n",
      "Epoch 4791, Loss: 1.505781203508377, Final Batch Loss: 0.3071519732475281\n",
      "Epoch 4792, Loss: 1.417071908712387, Final Batch Loss: 0.27125638723373413\n",
      "Epoch 4793, Loss: 1.5586329698562622, Final Batch Loss: 0.3684389293193817\n",
      "Epoch 4794, Loss: 1.4972590506076813, Final Batch Loss: 0.18300867080688477\n",
      "Epoch 4795, Loss: 1.2797892838716507, Final Batch Loss: 0.16629844903945923\n",
      "Epoch 4796, Loss: 1.454768255352974, Final Batch Loss: 0.21905072033405304\n",
      "Epoch 4797, Loss: 1.493279293179512, Final Batch Loss: 0.20616157352924347\n",
      "Epoch 4798, Loss: 1.808142215013504, Final Batch Loss: 0.5126244425773621\n",
      "Epoch 4799, Loss: 1.5239006280899048, Final Batch Loss: 0.2667824625968933\n",
      "Epoch 4800, Loss: 1.4246096760034561, Final Batch Loss: 0.22132399678230286\n",
      "Epoch 4801, Loss: 1.553827777504921, Final Batch Loss: 0.3121282756328583\n",
      "Epoch 4802, Loss: 1.7108341753482819, Final Batch Loss: 0.35043883323669434\n",
      "Epoch 4803, Loss: 1.5617995858192444, Final Batch Loss: 0.3295298218727112\n",
      "Epoch 4804, Loss: 1.5225673615932465, Final Batch Loss: 0.2958011031150818\n",
      "Epoch 4805, Loss: 1.6123997569084167, Final Batch Loss: 0.3677999973297119\n",
      "Epoch 4806, Loss: 1.6596536487340927, Final Batch Loss: 0.4606125056743622\n",
      "Epoch 4807, Loss: 1.7479608952999115, Final Batch Loss: 0.33511924743652344\n",
      "Epoch 4808, Loss: 1.649488627910614, Final Batch Loss: 0.37592199444770813\n",
      "Epoch 4809, Loss: 1.9810796082019806, Final Batch Loss: 0.5012765526771545\n",
      "Epoch 4810, Loss: 1.838299721479416, Final Batch Loss: 0.5246331691741943\n",
      "Epoch 4811, Loss: 1.469602644443512, Final Batch Loss: 0.3215036690235138\n",
      "Epoch 4812, Loss: 1.559307336807251, Final Batch Loss: 0.416692852973938\n",
      "Epoch 4813, Loss: 1.447666972875595, Final Batch Loss: 0.2367611676454544\n",
      "Epoch 4814, Loss: 1.554123878479004, Final Batch Loss: 0.34458616375923157\n",
      "Epoch 4815, Loss: 1.5241303890943527, Final Batch Loss: 0.21900682151317596\n",
      "Epoch 4816, Loss: 1.3743250668048859, Final Batch Loss: 0.1911276876926422\n",
      "Epoch 4817, Loss: 1.6807471811771393, Final Batch Loss: 0.43033137917518616\n",
      "Epoch 4818, Loss: 1.5208492875099182, Final Batch Loss: 0.3046722114086151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4819, Loss: 1.6919475197792053, Final Batch Loss: 0.41663509607315063\n",
      "Epoch 4820, Loss: 1.3425586521625519, Final Batch Loss: 0.2340836226940155\n",
      "Epoch 4821, Loss: 1.6609599888324738, Final Batch Loss: 0.4127444624900818\n",
      "Epoch 4822, Loss: 1.7814180254936218, Final Batch Loss: 0.3323882818222046\n",
      "Epoch 4823, Loss: 1.7495493292808533, Final Batch Loss: 0.4532787799835205\n",
      "Epoch 4824, Loss: 1.4996471107006073, Final Batch Loss: 0.2897128164768219\n",
      "Epoch 4825, Loss: 1.4634962975978851, Final Batch Loss: 0.2278669774532318\n",
      "Epoch 4826, Loss: 1.4194291681051254, Final Batch Loss: 0.19877701997756958\n",
      "Epoch 4827, Loss: 1.371455654501915, Final Batch Loss: 0.327943354845047\n",
      "Epoch 4828, Loss: 1.65851229429245, Final Batch Loss: 0.40129390358924866\n",
      "Epoch 4829, Loss: 1.5646825730800629, Final Batch Loss: 0.3671248257160187\n",
      "Epoch 4830, Loss: 1.5921364724636078, Final Batch Loss: 0.2975046932697296\n",
      "Epoch 4831, Loss: 1.6739833056926727, Final Batch Loss: 0.4527854323387146\n",
      "Epoch 4832, Loss: 1.9183602184057236, Final Batch Loss: 0.6962493658065796\n",
      "Epoch 4833, Loss: 1.448473185300827, Final Batch Loss: 0.30845892429351807\n",
      "Epoch 4834, Loss: 1.6062169969081879, Final Batch Loss: 0.33795031905174255\n",
      "Epoch 4835, Loss: 1.3481799215078354, Final Batch Loss: 0.2771839201450348\n",
      "Epoch 4836, Loss: 1.517210841178894, Final Batch Loss: 0.28958791494369507\n",
      "Epoch 4837, Loss: 1.5874242782592773, Final Batch Loss: 0.2887168228626251\n",
      "Epoch 4838, Loss: 1.736141324043274, Final Batch Loss: 0.3780071437358856\n",
      "Epoch 4839, Loss: 1.5151843577623367, Final Batch Loss: 0.2947842478752136\n",
      "Epoch 4840, Loss: 1.6307031512260437, Final Batch Loss: 0.31949126720428467\n",
      "Epoch 4841, Loss: 1.517660766839981, Final Batch Loss: 0.2787945866584778\n",
      "Epoch 4842, Loss: 1.6206647455692291, Final Batch Loss: 0.4245719313621521\n",
      "Epoch 4843, Loss: 1.4399741441011429, Final Batch Loss: 0.18419228494167328\n",
      "Epoch 4844, Loss: 1.5497171133756638, Final Batch Loss: 0.23638541996479034\n",
      "Epoch 4845, Loss: 1.3631963431835175, Final Batch Loss: 0.29947882890701294\n",
      "Epoch 4846, Loss: 1.5608810186386108, Final Batch Loss: 0.33951714634895325\n",
      "Epoch 4847, Loss: 1.5125838965177536, Final Batch Loss: 0.4543566107749939\n",
      "Epoch 4848, Loss: 1.5215982347726822, Final Batch Loss: 0.22065423429012299\n",
      "Epoch 4849, Loss: 1.3750748187303543, Final Batch Loss: 0.23286029696464539\n",
      "Epoch 4850, Loss: 1.489424616098404, Final Batch Loss: 0.32250645756721497\n",
      "Epoch 4851, Loss: 1.4403422176837921, Final Batch Loss: 0.28411486744880676\n",
      "Epoch 4852, Loss: 1.5445999801158905, Final Batch Loss: 0.35342442989349365\n",
      "Epoch 4853, Loss: 1.4926141202449799, Final Batch Loss: 0.2796992361545563\n",
      "Epoch 4854, Loss: 1.5544044375419617, Final Batch Loss: 0.29576027393341064\n",
      "Epoch 4855, Loss: 1.383358895778656, Final Batch Loss: 0.26302826404571533\n",
      "Epoch 4856, Loss: 1.3362068384885788, Final Batch Loss: 0.16887889802455902\n",
      "Epoch 4857, Loss: 1.5804627537727356, Final Batch Loss: 0.26701685786247253\n",
      "Epoch 4858, Loss: 1.4654473066329956, Final Batch Loss: 0.2595565617084503\n",
      "Epoch 4859, Loss: 1.4436295330524445, Final Batch Loss: 0.4269789159297943\n",
      "Epoch 4860, Loss: 1.2433666437864304, Final Batch Loss: 0.22415311634540558\n",
      "Epoch 4861, Loss: 1.5335403829813004, Final Batch Loss: 0.3116920292377472\n",
      "Epoch 4862, Loss: 1.4510874450206757, Final Batch Loss: 0.3010527491569519\n",
      "Epoch 4863, Loss: 1.3351048678159714, Final Batch Loss: 0.1520817130804062\n",
      "Epoch 4864, Loss: 1.4189048409461975, Final Batch Loss: 0.2504924237728119\n",
      "Epoch 4865, Loss: 1.6873925924301147, Final Batch Loss: 0.2907305061817169\n",
      "Epoch 4866, Loss: 1.5897444486618042, Final Batch Loss: 0.2784605324268341\n",
      "Epoch 4867, Loss: 1.3989217728376389, Final Batch Loss: 0.230363667011261\n",
      "Epoch 4868, Loss: 1.7399538457393646, Final Batch Loss: 0.2769416868686676\n",
      "Epoch 4869, Loss: 1.3755553364753723, Final Batch Loss: 0.24945591390132904\n",
      "Epoch 4870, Loss: 1.4547870010137558, Final Batch Loss: 0.2229594737291336\n",
      "Epoch 4871, Loss: 1.5641358494758606, Final Batch Loss: 0.3536610007286072\n",
      "Epoch 4872, Loss: 1.5344319343566895, Final Batch Loss: 0.22183078527450562\n",
      "Epoch 4873, Loss: 1.3239178359508514, Final Batch Loss: 0.2523495554924011\n",
      "Epoch 4874, Loss: 1.4526443779468536, Final Batch Loss: 0.2308741956949234\n",
      "Epoch 4875, Loss: 1.358641356229782, Final Batch Loss: 0.25525715947151184\n",
      "Epoch 4876, Loss: 1.995621681213379, Final Batch Loss: 0.6430405974388123\n",
      "Epoch 4877, Loss: 1.5089058130979538, Final Batch Loss: 0.3350832462310791\n",
      "Epoch 4878, Loss: 1.5103197693824768, Final Batch Loss: 0.3115188777446747\n",
      "Epoch 4879, Loss: 1.4399451464414597, Final Batch Loss: 0.3089490234851837\n",
      "Epoch 4880, Loss: 1.3939563184976578, Final Batch Loss: 0.18397554755210876\n",
      "Epoch 4881, Loss: 1.749651461839676, Final Batch Loss: 0.29264646768569946\n",
      "Epoch 4882, Loss: 1.7428144812583923, Final Batch Loss: 0.30926287174224854\n",
      "Epoch 4883, Loss: 1.4714585840702057, Final Batch Loss: 0.3049595057964325\n",
      "Epoch 4884, Loss: 1.6683036386966705, Final Batch Loss: 0.3137424886226654\n",
      "Epoch 4885, Loss: 1.5136333107948303, Final Batch Loss: 0.4035301208496094\n",
      "Epoch 4886, Loss: 1.260192558169365, Final Batch Loss: 0.19400550425052643\n",
      "Epoch 4887, Loss: 1.5315980315208435, Final Batch Loss: 0.2287209928035736\n",
      "Epoch 4888, Loss: 1.5415445268154144, Final Batch Loss: 0.3109566271305084\n",
      "Epoch 4889, Loss: 1.489701047539711, Final Batch Loss: 0.3919869363307953\n",
      "Epoch 4890, Loss: 1.8192003965377808, Final Batch Loss: 0.4212850034236908\n",
      "Epoch 4891, Loss: 1.5210025310516357, Final Batch Loss: 0.31197115778923035\n",
      "Epoch 4892, Loss: 1.4470986425876617, Final Batch Loss: 0.275762677192688\n",
      "Epoch 4893, Loss: 1.5924295485019684, Final Batch Loss: 0.22698241472244263\n",
      "Epoch 4894, Loss: 1.5219095051288605, Final Batch Loss: 0.30758142471313477\n",
      "Epoch 4895, Loss: 1.5540262162685394, Final Batch Loss: 0.42868557572364807\n",
      "Epoch 4896, Loss: 1.6381518840789795, Final Batch Loss: 0.32322782278060913\n",
      "Epoch 4897, Loss: 1.3163738548755646, Final Batch Loss: 0.1726120114326477\n",
      "Epoch 4898, Loss: 1.5933199673891068, Final Batch Loss: 0.1825486272573471\n",
      "Epoch 4899, Loss: 1.5229543447494507, Final Batch Loss: 0.34024330973625183\n",
      "Epoch 4900, Loss: 1.5141120254993439, Final Batch Loss: 0.4084879755973816\n",
      "Epoch 4901, Loss: 1.5997294783592224, Final Batch Loss: 0.3835314214229584\n",
      "Epoch 4902, Loss: 1.4099877774715424, Final Batch Loss: 0.1896161586046219\n",
      "Epoch 4903, Loss: 1.5043409764766693, Final Batch Loss: 0.2905976474285126\n",
      "Epoch 4904, Loss: 1.3698544949293137, Final Batch Loss: 0.20934590697288513\n",
      "Epoch 4905, Loss: 1.3017228990793228, Final Batch Loss: 0.28887060284614563\n",
      "Epoch 4906, Loss: 1.6294778883457184, Final Batch Loss: 0.5225920677185059\n",
      "Epoch 4907, Loss: 1.8816379010677338, Final Batch Loss: 0.3180811107158661\n",
      "Epoch 4908, Loss: 1.4081454277038574, Final Batch Loss: 0.3433597683906555\n",
      "Epoch 4909, Loss: 1.4498492777347565, Final Batch Loss: 0.2650168836116791\n",
      "Epoch 4910, Loss: 1.461226761341095, Final Batch Loss: 0.29101577401161194\n",
      "Epoch 4911, Loss: 1.6411754786968231, Final Batch Loss: 0.3112156093120575\n",
      "Epoch 4912, Loss: 1.5286064147949219, Final Batch Loss: 0.3628586232662201\n",
      "Epoch 4913, Loss: 1.6311553716659546, Final Batch Loss: 0.41323578357696533\n",
      "Epoch 4914, Loss: 1.4406670182943344, Final Batch Loss: 0.25470346212387085\n",
      "Epoch 4915, Loss: 1.4314972907304764, Final Batch Loss: 0.16187505424022675\n",
      "Epoch 4916, Loss: 1.4136447608470917, Final Batch Loss: 0.32592371106147766\n",
      "Epoch 4917, Loss: 1.6447214186191559, Final Batch Loss: 0.35684728622436523\n",
      "Epoch 4918, Loss: 1.6068506240844727, Final Batch Loss: 0.3524528741836548\n",
      "Epoch 4919, Loss: 1.4274981617927551, Final Batch Loss: 0.2710363566875458\n",
      "Epoch 4920, Loss: 1.7359162867069244, Final Batch Loss: 0.5062411427497864\n",
      "Epoch 4921, Loss: 1.6350612491369247, Final Batch Loss: 0.3051776587963104\n",
      "Epoch 4922, Loss: 1.4338027834892273, Final Batch Loss: 0.219253271818161\n",
      "Epoch 4923, Loss: 1.4963375627994537, Final Batch Loss: 0.2502507269382477\n",
      "Epoch 4924, Loss: 1.283832609653473, Final Batch Loss: 0.18190807104110718\n",
      "Epoch 4925, Loss: 1.4413810819387436, Final Batch Loss: 0.3308006525039673\n",
      "Epoch 4926, Loss: 1.4861039370298386, Final Batch Loss: 0.3573201894760132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4927, Loss: 1.6541844010353088, Final Batch Loss: 0.4163780212402344\n",
      "Epoch 4928, Loss: 1.662436157464981, Final Batch Loss: 0.3647578954696655\n",
      "Epoch 4929, Loss: 1.5735437273979187, Final Batch Loss: 0.31929975748062134\n",
      "Epoch 4930, Loss: 1.4928537011146545, Final Batch Loss: 0.21408489346504211\n",
      "Epoch 4931, Loss: 1.3604142516851425, Final Batch Loss: 0.19542600214481354\n",
      "Epoch 4932, Loss: 1.4700383841991425, Final Batch Loss: 0.37557879090309143\n",
      "Epoch 4933, Loss: 1.415162354707718, Final Batch Loss: 0.26397594809532166\n",
      "Epoch 4934, Loss: 1.3837155997753143, Final Batch Loss: 0.2522967457771301\n",
      "Epoch 4935, Loss: 1.3830696791410446, Final Batch Loss: 0.24555161595344543\n",
      "Epoch 4936, Loss: 1.6081182956695557, Final Batch Loss: 0.36460721492767334\n",
      "Epoch 4937, Loss: 1.4327259063720703, Final Batch Loss: 0.2802235782146454\n",
      "Epoch 4938, Loss: 1.2228323370218277, Final Batch Loss: 0.2365102916955948\n",
      "Epoch 4939, Loss: 1.4996512085199356, Final Batch Loss: 0.2974063754081726\n",
      "Epoch 4940, Loss: 1.7044150829315186, Final Batch Loss: 0.5228081345558167\n",
      "Epoch 4941, Loss: 1.5940364748239517, Final Batch Loss: 0.3716571033000946\n",
      "Epoch 4942, Loss: 1.4194448292255402, Final Batch Loss: 0.3259049355983734\n",
      "Epoch 4943, Loss: 1.6278775036334991, Final Batch Loss: 0.4185953438282013\n",
      "Epoch 4944, Loss: 1.361135572195053, Final Batch Loss: 0.2800329327583313\n",
      "Epoch 4945, Loss: 1.5385404080152512, Final Batch Loss: 0.3751313090324402\n",
      "Epoch 4946, Loss: 1.6803087294101715, Final Batch Loss: 0.36439841985702515\n",
      "Epoch 4947, Loss: 1.6214052587747574, Final Batch Loss: 0.5296816229820251\n",
      "Epoch 4948, Loss: 1.5579876899719238, Final Batch Loss: 0.2853538393974304\n",
      "Epoch 4949, Loss: 1.5754791796207428, Final Batch Loss: 0.347216933965683\n",
      "Epoch 4950, Loss: 1.5070420801639557, Final Batch Loss: 0.28045907616615295\n",
      "Epoch 4951, Loss: 1.5890096724033356, Final Batch Loss: 0.3229590058326721\n",
      "Epoch 4952, Loss: 1.4177326411008835, Final Batch Loss: 0.25717630982398987\n",
      "Epoch 4953, Loss: 1.585954487323761, Final Batch Loss: 0.3742418587207794\n",
      "Epoch 4954, Loss: 1.6113439202308655, Final Batch Loss: 0.43133947253227234\n",
      "Epoch 4955, Loss: 1.5235093832015991, Final Batch Loss: 0.3580774962902069\n",
      "Epoch 4956, Loss: 1.7206772565841675, Final Batch Loss: 0.4077056050300598\n",
      "Epoch 4957, Loss: 1.6058992743492126, Final Batch Loss: 0.41965875029563904\n",
      "Epoch 4958, Loss: 1.3868410885334015, Final Batch Loss: 0.20346352458000183\n",
      "Epoch 4959, Loss: 1.5359648764133453, Final Batch Loss: 0.27131226658821106\n",
      "Epoch 4960, Loss: 1.402464598417282, Final Batch Loss: 0.23950745165348053\n",
      "Epoch 4961, Loss: 1.5067997723817825, Final Batch Loss: 0.2597537040710449\n",
      "Epoch 4962, Loss: 1.465844064950943, Final Batch Loss: 0.19908714294433594\n",
      "Epoch 4963, Loss: 1.4146483391523361, Final Batch Loss: 0.22210000455379486\n",
      "Epoch 4964, Loss: 1.537162959575653, Final Batch Loss: 0.3365330994129181\n",
      "Epoch 4965, Loss: 1.4717811197042465, Final Batch Loss: 0.2000465840101242\n",
      "Epoch 4966, Loss: 1.7011531442403793, Final Batch Loss: 0.40884438157081604\n",
      "Epoch 4967, Loss: 1.6830499470233917, Final Batch Loss: 0.4488506019115448\n",
      "Epoch 4968, Loss: 1.7044385969638824, Final Batch Loss: 0.3360913395881653\n",
      "Epoch 4969, Loss: 1.452120453119278, Final Batch Loss: 0.3820686936378479\n",
      "Epoch 4970, Loss: 1.7528460919857025, Final Batch Loss: 0.30909836292266846\n",
      "Epoch 4971, Loss: 1.6741731464862823, Final Batch Loss: 0.35328665375709534\n",
      "Epoch 4972, Loss: 1.407050222158432, Final Batch Loss: 0.247722327709198\n",
      "Epoch 4973, Loss: 1.6164566278457642, Final Batch Loss: 0.3508642017841339\n",
      "Epoch 4974, Loss: 1.4355591833591461, Final Batch Loss: 0.2049318253993988\n",
      "Epoch 4975, Loss: 1.493815690279007, Final Batch Loss: 0.3969733715057373\n",
      "Epoch 4976, Loss: 1.3825567811727524, Final Batch Loss: 0.34043383598327637\n",
      "Epoch 4977, Loss: 1.4149011820554733, Final Batch Loss: 0.32698488235473633\n",
      "Epoch 4978, Loss: 1.3655819445848465, Final Batch Loss: 0.16725878417491913\n",
      "Epoch 4979, Loss: 1.755829781293869, Final Batch Loss: 0.41741234064102173\n",
      "Epoch 4980, Loss: 1.5039322972297668, Final Batch Loss: 0.2942201793193817\n",
      "Epoch 4981, Loss: 1.5350014865398407, Final Batch Loss: 0.27318447828292847\n",
      "Epoch 4982, Loss: 1.4302958250045776, Final Batch Loss: 0.2116910219192505\n",
      "Epoch 4983, Loss: 1.6814008951187134, Final Batch Loss: 0.41004863381385803\n",
      "Epoch 4984, Loss: 1.5454132109880447, Final Batch Loss: 0.24821512401103973\n",
      "Epoch 4985, Loss: 1.7163579612970352, Final Batch Loss: 0.21419061720371246\n",
      "Epoch 4986, Loss: 1.4249971210956573, Final Batch Loss: 0.24512305855751038\n",
      "Epoch 4987, Loss: 1.6915801912546158, Final Batch Loss: 0.4894792139530182\n",
      "Epoch 4988, Loss: 1.580693930387497, Final Batch Loss: 0.34742021560668945\n",
      "Epoch 4989, Loss: 1.4494231045246124, Final Batch Loss: 0.35200753808021545\n",
      "Epoch 4990, Loss: 1.3965590745210648, Final Batch Loss: 0.23544009029865265\n",
      "Epoch 4991, Loss: 1.6377347707748413, Final Batch Loss: 0.39678141474723816\n",
      "Epoch 4992, Loss: 1.4890152215957642, Final Batch Loss: 0.17712455987930298\n",
      "Epoch 4993, Loss: 1.5938405096530914, Final Batch Loss: 0.30811265110969543\n",
      "Epoch 4994, Loss: 1.581264704465866, Final Batch Loss: 0.4335283637046814\n",
      "Epoch 4995, Loss: 1.5841533839702606, Final Batch Loss: 0.39737942814826965\n",
      "Epoch 4996, Loss: 1.6220836341381073, Final Batch Loss: 0.253523051738739\n",
      "Epoch 4997, Loss: 1.6234780251979828, Final Batch Loss: 0.433485209941864\n",
      "Epoch 4998, Loss: 1.4237341582775116, Final Batch Loss: 0.2989393472671509\n",
      "Epoch 4999, Loss: 1.519527107477188, Final Batch Loss: 0.2726408839225769\n",
      "Epoch 5000, Loss: 1.6227611601352692, Final Batch Loss: 0.24288439750671387\n",
      "Epoch 5001, Loss: 1.3505481481552124, Final Batch Loss: 0.2155751883983612\n",
      "Epoch 5002, Loss: 1.4535882771015167, Final Batch Loss: 0.233936607837677\n",
      "Epoch 5003, Loss: 1.4603671431541443, Final Batch Loss: 0.29410383105278015\n",
      "Epoch 5004, Loss: 1.6232976764440536, Final Batch Loss: 0.5420950651168823\n",
      "Epoch 5005, Loss: 1.4746400117874146, Final Batch Loss: 0.25032585859298706\n",
      "Epoch 5006, Loss: 1.431067705154419, Final Batch Loss: 0.3151208460330963\n",
      "Epoch 5007, Loss: 1.4376855492591858, Final Batch Loss: 0.3912205994129181\n",
      "Epoch 5008, Loss: 1.4734288454055786, Final Batch Loss: 0.2833932936191559\n",
      "Epoch 5009, Loss: 1.397760882973671, Final Batch Loss: 0.20183412730693817\n",
      "Epoch 5010, Loss: 1.457161784172058, Final Batch Loss: 0.2563581168651581\n",
      "Epoch 5011, Loss: 1.554495245218277, Final Batch Loss: 0.37085139751434326\n",
      "Epoch 5012, Loss: 1.366615504026413, Final Batch Loss: 0.20257475972175598\n",
      "Epoch 5013, Loss: 1.3707525134086609, Final Batch Loss: 0.3468995988368988\n",
      "Epoch 5014, Loss: 1.554314762353897, Final Batch Loss: 0.3524044156074524\n",
      "Epoch 5015, Loss: 1.3825148791074753, Final Batch Loss: 0.2679210901260376\n",
      "Epoch 5016, Loss: 1.3879062831401825, Final Batch Loss: 0.2838350534439087\n",
      "Epoch 5017, Loss: 1.4268337041139603, Final Batch Loss: 0.2521424889564514\n",
      "Epoch 5018, Loss: 1.4691591560840607, Final Batch Loss: 0.327423632144928\n",
      "Epoch 5019, Loss: 1.602966457605362, Final Batch Loss: 0.39284899830818176\n",
      "Epoch 5020, Loss: 1.423154354095459, Final Batch Loss: 0.31212514638900757\n",
      "Epoch 5021, Loss: 1.5612359941005707, Final Batch Loss: 0.3085518479347229\n",
      "Epoch 5022, Loss: 1.4069014936685562, Final Batch Loss: 0.2460390329360962\n",
      "Epoch 5023, Loss: 1.4732648581266403, Final Batch Loss: 0.29266342520713806\n",
      "Epoch 5024, Loss: 1.5183268785476685, Final Batch Loss: 0.2794291377067566\n",
      "Epoch 5025, Loss: 1.5157803297042847, Final Batch Loss: 0.3412986099720001\n",
      "Epoch 5026, Loss: 1.4431126862764359, Final Batch Loss: 0.21133111417293549\n",
      "Epoch 5027, Loss: 1.4167408049106598, Final Batch Loss: 0.1814405918121338\n",
      "Epoch 5028, Loss: 1.5853080749511719, Final Batch Loss: 0.3174838423728943\n",
      "Epoch 5029, Loss: 1.636249452829361, Final Batch Loss: 0.36272695660591125\n",
      "Epoch 5030, Loss: 1.4930364191532135, Final Batch Loss: 0.31032222509384155\n",
      "Epoch 5031, Loss: 1.422211468219757, Final Batch Loss: 0.19430014491081238\n",
      "Epoch 5032, Loss: 1.4876641482114792, Final Batch Loss: 0.23022405803203583\n",
      "Epoch 5033, Loss: 1.3411518335342407, Final Batch Loss: 0.2960478663444519\n",
      "Epoch 5034, Loss: 1.479409009218216, Final Batch Loss: 0.3048820197582245\n",
      "Epoch 5035, Loss: 1.5840158313512802, Final Batch Loss: 0.41034626960754395\n",
      "Epoch 5036, Loss: 1.4718172550201416, Final Batch Loss: 0.28491517901420593\n",
      "Epoch 5037, Loss: 1.616832971572876, Final Batch Loss: 0.39591386914253235\n",
      "Epoch 5038, Loss: 1.6141185462474823, Final Batch Loss: 0.32740405201911926\n",
      "Epoch 5039, Loss: 1.5146556794643402, Final Batch Loss: 0.26238539814949036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5040, Loss: 1.4056559652090073, Final Batch Loss: 0.22751863300800323\n",
      "Epoch 5041, Loss: 1.3923317641019821, Final Batch Loss: 0.2566088140010834\n",
      "Epoch 5042, Loss: 1.5231988430023193, Final Batch Loss: 0.2906266748905182\n",
      "Epoch 5043, Loss: 1.6379160583019257, Final Batch Loss: 0.44621315598487854\n",
      "Epoch 5044, Loss: 1.503544196486473, Final Batch Loss: 0.3486397862434387\n",
      "Epoch 5045, Loss: 1.5590302795171738, Final Batch Loss: 0.43085429072380066\n",
      "Epoch 5046, Loss: 1.5865270495414734, Final Batch Loss: 0.35911494493484497\n",
      "Epoch 5047, Loss: 1.5990073680877686, Final Batch Loss: 0.3859858512878418\n",
      "Epoch 5048, Loss: 1.5376739501953125, Final Batch Loss: 0.18939241766929626\n",
      "Epoch 5049, Loss: 1.6386570036411285, Final Batch Loss: 0.3589036464691162\n",
      "Epoch 5050, Loss: 1.7270283102989197, Final Batch Loss: 0.42607542872428894\n",
      "Epoch 5051, Loss: 1.433537483215332, Final Batch Loss: 0.21776026487350464\n",
      "Epoch 5052, Loss: 1.6853837221860886, Final Batch Loss: 0.2185441106557846\n",
      "Epoch 5053, Loss: 1.471564844250679, Final Batch Loss: 0.22350962460041046\n",
      "Epoch 5054, Loss: 1.7224911004304886, Final Batch Loss: 0.46914157271385193\n",
      "Epoch 5055, Loss: 1.4602311104536057, Final Batch Loss: 0.23734818398952484\n",
      "Epoch 5056, Loss: 1.3898939341306686, Final Batch Loss: 0.26883870363235474\n",
      "Epoch 5057, Loss: 1.644551396369934, Final Batch Loss: 0.3223285675048828\n",
      "Epoch 5058, Loss: 1.6114163994789124, Final Batch Loss: 0.32884278893470764\n",
      "Epoch 5059, Loss: 1.5353731513023376, Final Batch Loss: 0.3628547191619873\n",
      "Epoch 5060, Loss: 1.4401170015335083, Final Batch Loss: 0.3440941572189331\n",
      "Epoch 5061, Loss: 1.5069195181131363, Final Batch Loss: 0.3424150049686432\n",
      "Epoch 5062, Loss: 1.355845034122467, Final Batch Loss: 0.25307366251945496\n",
      "Epoch 5063, Loss: 1.6288576871156693, Final Batch Loss: 0.5048844814300537\n",
      "Epoch 5064, Loss: 1.4428579211235046, Final Batch Loss: 0.237078458070755\n",
      "Epoch 5065, Loss: 1.4625512659549713, Final Batch Loss: 0.2875153422355652\n",
      "Epoch 5066, Loss: 1.5125637352466583, Final Batch Loss: 0.3309861719608307\n",
      "Epoch 5067, Loss: 1.500124990940094, Final Batch Loss: 0.33577588200569153\n",
      "Epoch 5068, Loss: 1.675826221704483, Final Batch Loss: 0.330312043428421\n",
      "Epoch 5069, Loss: 1.314467892050743, Final Batch Loss: 0.3220537006855011\n",
      "Epoch 5070, Loss: 1.340122014284134, Final Batch Loss: 0.18049409985542297\n",
      "Epoch 5071, Loss: 1.5486932545900345, Final Batch Loss: 0.404030442237854\n",
      "Epoch 5072, Loss: 1.359852910041809, Final Batch Loss: 0.21609514951705933\n",
      "Epoch 5073, Loss: 1.3888182193040848, Final Batch Loss: 0.2753725051879883\n",
      "Epoch 5074, Loss: 1.2784705609083176, Final Batch Loss: 0.1805342733860016\n",
      "Epoch 5075, Loss: 1.754978895187378, Final Batch Loss: 0.37922778725624084\n",
      "Epoch 5076, Loss: 1.4036507904529572, Final Batch Loss: 0.26098376512527466\n",
      "Epoch 5077, Loss: 1.6124045252799988, Final Batch Loss: 0.42597198486328125\n",
      "Epoch 5078, Loss: 1.2747917920351028, Final Batch Loss: 0.19186249375343323\n",
      "Epoch 5079, Loss: 1.6433111429214478, Final Batch Loss: 0.5088925957679749\n",
      "Epoch 5080, Loss: 1.4181514978408813, Final Batch Loss: 0.21022669970989227\n",
      "Epoch 5081, Loss: 1.552476778626442, Final Batch Loss: 0.24563947319984436\n",
      "Epoch 5082, Loss: 1.4016664773225784, Final Batch Loss: 0.20801566541194916\n",
      "Epoch 5083, Loss: 1.5117402523756027, Final Batch Loss: 0.3889986276626587\n",
      "Epoch 5084, Loss: 1.6772664487361908, Final Batch Loss: 0.41751810908317566\n",
      "Epoch 5085, Loss: 1.6751839518547058, Final Batch Loss: 0.3916889429092407\n",
      "Epoch 5086, Loss: 1.5334309488534927, Final Batch Loss: 0.33747607469558716\n",
      "Epoch 5087, Loss: 1.422053411602974, Final Batch Loss: 0.23406563699245453\n",
      "Epoch 5088, Loss: 1.7297053635120392, Final Batch Loss: 0.4232933223247528\n",
      "Epoch 5089, Loss: 1.4343250393867493, Final Batch Loss: 0.297357439994812\n",
      "Epoch 5090, Loss: 1.6986856013536453, Final Batch Loss: 0.5209922790527344\n",
      "Epoch 5091, Loss: 1.5751646310091019, Final Batch Loss: 0.3613472580909729\n",
      "Epoch 5092, Loss: 1.4261670410633087, Final Batch Loss: 0.2376728504896164\n",
      "Epoch 5093, Loss: 1.5986593812704086, Final Batch Loss: 0.35385748744010925\n",
      "Epoch 5094, Loss: 1.2990745604038239, Final Batch Loss: 0.1706591546535492\n",
      "Epoch 5095, Loss: 1.367320790886879, Final Batch Loss: 0.16906940937042236\n",
      "Epoch 5096, Loss: 1.5216692388057709, Final Batch Loss: 0.3609101474285126\n",
      "Epoch 5097, Loss: 1.4573625922203064, Final Batch Loss: 0.24969938397407532\n",
      "Epoch 5098, Loss: 1.5219676941633224, Final Batch Loss: 0.37017694115638733\n",
      "Epoch 5099, Loss: 1.6979535669088364, Final Batch Loss: 0.5251330733299255\n",
      "Epoch 5100, Loss: 1.3406844437122345, Final Batch Loss: 0.3130285441875458\n",
      "Epoch 5101, Loss: 1.3729317486286163, Final Batch Loss: 0.31425607204437256\n",
      "Epoch 5102, Loss: 1.5245115160942078, Final Batch Loss: 0.2727101445198059\n",
      "Epoch 5103, Loss: 1.3746053576469421, Final Batch Loss: 0.20886346697807312\n",
      "Epoch 5104, Loss: 1.7026634812355042, Final Batch Loss: 0.5099058747291565\n",
      "Epoch 5105, Loss: 1.5953697115182877, Final Batch Loss: 0.3946405053138733\n",
      "Epoch 5106, Loss: 1.6022553145885468, Final Batch Loss: 0.43513622879981995\n",
      "Epoch 5107, Loss: 1.7141225636005402, Final Batch Loss: 0.46345123648643494\n",
      "Epoch 5108, Loss: 1.4845568239688873, Final Batch Loss: 0.45815762877464294\n",
      "Epoch 5109, Loss: 1.505712941288948, Final Batch Loss: 0.308337926864624\n",
      "Epoch 5110, Loss: 1.4553254842758179, Final Batch Loss: 0.24886420369148254\n",
      "Epoch 5111, Loss: 1.611496090888977, Final Batch Loss: 0.3237643241882324\n",
      "Epoch 5112, Loss: 1.4893797636032104, Final Batch Loss: 0.30477049946784973\n",
      "Epoch 5113, Loss: 1.6108143031597137, Final Batch Loss: 0.3273634612560272\n",
      "Epoch 5114, Loss: 1.5887735486030579, Final Batch Loss: 0.28039616346359253\n",
      "Epoch 5115, Loss: 1.335816390812397, Final Batch Loss: 0.10587996989488602\n",
      "Epoch 5116, Loss: 1.5175229907035828, Final Batch Loss: 0.26544827222824097\n",
      "Epoch 5117, Loss: 1.579098716378212, Final Batch Loss: 0.3987804651260376\n",
      "Epoch 5118, Loss: 1.3882243186235428, Final Batch Loss: 0.26906269788742065\n",
      "Epoch 5119, Loss: 1.4071549475193024, Final Batch Loss: 0.19872191548347473\n",
      "Epoch 5120, Loss: 1.4223041832447052, Final Batch Loss: 0.37280675768852234\n",
      "Epoch 5121, Loss: 1.5726563483476639, Final Batch Loss: 0.3570409119129181\n",
      "Epoch 5122, Loss: 1.434258759021759, Final Batch Loss: 0.3236096501350403\n",
      "Epoch 5123, Loss: 1.5726359188556671, Final Batch Loss: 0.3262239992618561\n",
      "Epoch 5124, Loss: 1.3484639674425125, Final Batch Loss: 0.30238547921180725\n",
      "Epoch 5125, Loss: 1.5761605352163315, Final Batch Loss: 0.2214096635580063\n",
      "Epoch 5126, Loss: 1.5300824046134949, Final Batch Loss: 0.3324245512485504\n",
      "Epoch 5127, Loss: 1.3393340408802032, Final Batch Loss: 0.2600061595439911\n",
      "Epoch 5128, Loss: 1.390692263841629, Final Batch Loss: 0.2524053752422333\n",
      "Epoch 5129, Loss: 1.3584978729486465, Final Batch Loss: 0.1866462081670761\n",
      "Epoch 5130, Loss: 1.8257200419902802, Final Batch Loss: 0.37668898701667786\n",
      "Epoch 5131, Loss: 1.3525634706020355, Final Batch Loss: 0.22866806387901306\n",
      "Epoch 5132, Loss: 1.4915705025196075, Final Batch Loss: 0.24314910173416138\n",
      "Epoch 5133, Loss: 1.33491250872612, Final Batch Loss: 0.20584695041179657\n",
      "Epoch 5134, Loss: 1.393860101699829, Final Batch Loss: 0.28638431429862976\n",
      "Epoch 5135, Loss: 1.4542726874351501, Final Batch Loss: 0.35341376066207886\n",
      "Epoch 5136, Loss: 1.4385998100042343, Final Batch Loss: 0.3363005220890045\n",
      "Epoch 5137, Loss: 1.3942926228046417, Final Batch Loss: 0.3326883316040039\n",
      "Epoch 5138, Loss: 1.3249312192201614, Final Batch Loss: 0.15992607176303864\n",
      "Epoch 5139, Loss: 1.5299720764160156, Final Batch Loss: 0.40244168043136597\n",
      "Epoch 5140, Loss: 1.339265912771225, Final Batch Loss: 0.30159521102905273\n",
      "Epoch 5141, Loss: 1.4252018481492996, Final Batch Loss: 0.24356038868427277\n",
      "Epoch 5142, Loss: 1.3323087245225906, Final Batch Loss: 0.21625454723834991\n",
      "Epoch 5143, Loss: 1.6697312891483307, Final Batch Loss: 0.5855550169944763\n",
      "Epoch 5144, Loss: 1.2013355195522308, Final Batch Loss: 0.179325670003891\n",
      "Epoch 5145, Loss: 1.4291685223579407, Final Batch Loss: 0.3161311745643616\n",
      "Epoch 5146, Loss: 1.7656937390565872, Final Batch Loss: 0.4841384291648865\n",
      "Epoch 5147, Loss: 1.3303605914115906, Final Batch Loss: 0.2835308313369751\n",
      "Epoch 5148, Loss: 1.5379514396190643, Final Batch Loss: 0.20914044976234436\n",
      "Epoch 5149, Loss: 1.4348430931568146, Final Batch Loss: 0.2638818919658661\n",
      "Epoch 5150, Loss: 1.3448005467653275, Final Batch Loss: 0.19168908894062042\n",
      "Epoch 5151, Loss: 1.5063494890928268, Final Batch Loss: 0.2854614853858948\n",
      "Epoch 5152, Loss: 1.3900990635156631, Final Batch Loss: 0.2997191250324249\n",
      "Epoch 5153, Loss: 1.4687214642763138, Final Batch Loss: 0.27758899331092834\n",
      "Epoch 5154, Loss: 1.4673826098442078, Final Batch Loss: 0.27522721886634827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5155, Loss: 1.4251852482557297, Final Batch Loss: 0.3583388328552246\n",
      "Epoch 5156, Loss: 1.5008545219898224, Final Batch Loss: 0.4034227430820465\n",
      "Epoch 5157, Loss: 1.607044205069542, Final Batch Loss: 0.3383494019508362\n",
      "Epoch 5158, Loss: 1.5762688368558884, Final Batch Loss: 0.2731209397315979\n",
      "Epoch 5159, Loss: 1.4945025891065598, Final Batch Loss: 0.3095451295375824\n",
      "Epoch 5160, Loss: 1.4586781114339828, Final Batch Loss: 0.3629721999168396\n",
      "Epoch 5161, Loss: 1.4868169724941254, Final Batch Loss: 0.3236953020095825\n",
      "Epoch 5162, Loss: 1.5369167178869247, Final Batch Loss: 0.3795851767063141\n",
      "Epoch 5163, Loss: 1.5200302004814148, Final Batch Loss: 0.29892778396606445\n",
      "Epoch 5164, Loss: 1.5465137213468552, Final Batch Loss: 0.4904635548591614\n",
      "Epoch 5165, Loss: 1.4711970686912537, Final Batch Loss: 0.24933576583862305\n",
      "Epoch 5166, Loss: 1.558863252401352, Final Batch Loss: 0.29628950357437134\n",
      "Epoch 5167, Loss: 1.589297503232956, Final Batch Loss: 0.30517271161079407\n",
      "Epoch 5168, Loss: 1.4788156598806381, Final Batch Loss: 0.2851426303386688\n",
      "Epoch 5169, Loss: 1.4697837233543396, Final Batch Loss: 0.2321476936340332\n",
      "Epoch 5170, Loss: 1.3161695748567581, Final Batch Loss: 0.2289188653230667\n",
      "Epoch 5171, Loss: 1.2531198114156723, Final Batch Loss: 0.13454829156398773\n",
      "Epoch 5172, Loss: 1.4622804075479507, Final Batch Loss: 0.22300539910793304\n",
      "Epoch 5173, Loss: 1.3098016381263733, Final Batch Loss: 0.2971140742301941\n",
      "Epoch 5174, Loss: 1.3378737568855286, Final Batch Loss: 0.1655697226524353\n",
      "Epoch 5175, Loss: 1.4969981759786606, Final Batch Loss: 0.32569217681884766\n",
      "Epoch 5176, Loss: 1.4787255823612213, Final Batch Loss: 0.21489492058753967\n",
      "Epoch 5177, Loss: 1.4426902532577515, Final Batch Loss: 0.22218671441078186\n",
      "Epoch 5178, Loss: 1.4533029198646545, Final Batch Loss: 0.3151349127292633\n",
      "Epoch 5179, Loss: 1.555945485830307, Final Batch Loss: 0.32215389609336853\n",
      "Epoch 5180, Loss: 1.4243480116128922, Final Batch Loss: 0.3380557596683502\n",
      "Epoch 5181, Loss: 1.5800198316574097, Final Batch Loss: 0.3855115473270416\n",
      "Epoch 5182, Loss: 1.4431902468204498, Final Batch Loss: 0.24713172018527985\n",
      "Epoch 5183, Loss: 1.4219499230384827, Final Batch Loss: 0.28462329506874084\n",
      "Epoch 5184, Loss: 1.3850309550762177, Final Batch Loss: 0.3056773245334625\n",
      "Epoch 5185, Loss: 1.4153376370668411, Final Batch Loss: 0.25066444277763367\n",
      "Epoch 5186, Loss: 1.2915158718824387, Final Batch Loss: 0.23113220930099487\n",
      "Epoch 5187, Loss: 1.4027565717697144, Final Batch Loss: 0.22688722610473633\n",
      "Epoch 5188, Loss: 1.3042394071817398, Final Batch Loss: 0.284885048866272\n",
      "Epoch 5189, Loss: 1.362713098526001, Final Batch Loss: 0.2527925670146942\n",
      "Epoch 5190, Loss: 1.3846857398748398, Final Batch Loss: 0.19800139963626862\n",
      "Epoch 5191, Loss: 1.3596322536468506, Final Batch Loss: 0.17585358023643494\n",
      "Epoch 5192, Loss: 1.4707249999046326, Final Batch Loss: 0.2791735827922821\n",
      "Epoch 5193, Loss: 1.3500292003154755, Final Batch Loss: 0.31538039445877075\n",
      "Epoch 5194, Loss: 1.3960961252450943, Final Batch Loss: 0.26211073994636536\n",
      "Epoch 5195, Loss: 1.4308316111564636, Final Batch Loss: 0.2306744009256363\n",
      "Epoch 5196, Loss: 1.624332919716835, Final Batch Loss: 0.39870554208755493\n",
      "Epoch 5197, Loss: 1.676696538925171, Final Batch Loss: 0.503463089466095\n",
      "Epoch 5198, Loss: 1.5545684099197388, Final Batch Loss: 0.27642709016799927\n",
      "Epoch 5199, Loss: 1.386027529835701, Final Batch Loss: 0.22731171548366547\n",
      "Epoch 5200, Loss: 1.4077956080436707, Final Batch Loss: 0.31373974680900574\n",
      "Epoch 5201, Loss: 1.4084178507328033, Final Batch Loss: 0.2652478516101837\n",
      "Epoch 5202, Loss: 1.4578837156295776, Final Batch Loss: 0.2972828149795532\n",
      "Epoch 5203, Loss: 1.3805552124977112, Final Batch Loss: 0.27101799845695496\n",
      "Epoch 5204, Loss: 1.5560506880283356, Final Batch Loss: 0.26062023639678955\n",
      "Epoch 5205, Loss: 1.47925266623497, Final Batch Loss: 0.24637556076049805\n",
      "Epoch 5206, Loss: 1.481706976890564, Final Batch Loss: 0.32190757989883423\n",
      "Epoch 5207, Loss: 1.4545714408159256, Final Batch Loss: 0.2682344913482666\n",
      "Epoch 5208, Loss: 1.6267095506191254, Final Batch Loss: 0.32110896706581116\n",
      "Epoch 5209, Loss: 1.5469976663589478, Final Batch Loss: 0.3664418160915375\n",
      "Epoch 5210, Loss: 1.4809255003929138, Final Batch Loss: 0.37781012058258057\n",
      "Epoch 5211, Loss: 1.3979156166315079, Final Batch Loss: 0.1758163720369339\n",
      "Epoch 5212, Loss: 1.4597191661596298, Final Batch Loss: 0.24140305817127228\n",
      "Epoch 5213, Loss: 1.5090834498405457, Final Batch Loss: 0.34607526659965515\n",
      "Epoch 5214, Loss: 1.7555284798145294, Final Batch Loss: 0.3376809060573578\n",
      "Epoch 5215, Loss: 1.4873467534780502, Final Batch Loss: 0.2975672781467438\n",
      "Epoch 5216, Loss: 1.7272565215826035, Final Batch Loss: 0.28110232949256897\n",
      "Epoch 5217, Loss: 1.6360765099525452, Final Batch Loss: 0.2490943819284439\n",
      "Epoch 5218, Loss: 1.3851685374975204, Final Batch Loss: 0.21422170102596283\n",
      "Epoch 5219, Loss: 1.5648199915885925, Final Batch Loss: 0.30247893929481506\n",
      "Epoch 5220, Loss: 1.434202253818512, Final Batch Loss: 0.30907297134399414\n",
      "Epoch 5221, Loss: 1.586839497089386, Final Batch Loss: 0.37509676814079285\n",
      "Epoch 5222, Loss: 1.3444370180368423, Final Batch Loss: 0.2104077786207199\n",
      "Epoch 5223, Loss: 1.5213057696819305, Final Batch Loss: 0.3080902695655823\n",
      "Epoch 5224, Loss: 1.4478813260793686, Final Batch Loss: 0.28544872999191284\n",
      "Epoch 5225, Loss: 1.6200239956378937, Final Batch Loss: 0.30248525738716125\n",
      "Epoch 5226, Loss: 1.42266383767128, Final Batch Loss: 0.32139354944229126\n",
      "Epoch 5227, Loss: 1.3177433907985687, Final Batch Loss: 0.23895293474197388\n",
      "Epoch 5228, Loss: 1.4716301262378693, Final Batch Loss: 0.2774105668067932\n",
      "Epoch 5229, Loss: 1.2655788958072662, Final Batch Loss: 0.24509769678115845\n",
      "Epoch 5230, Loss: 1.41387277841568, Final Batch Loss: 0.21525610983371735\n",
      "Epoch 5231, Loss: 1.300319030880928, Final Batch Loss: 0.24056506156921387\n",
      "Epoch 5232, Loss: 1.555602639913559, Final Batch Loss: 0.36342474818229675\n",
      "Epoch 5233, Loss: 1.5007162690162659, Final Batch Loss: 0.2679137885570526\n",
      "Epoch 5234, Loss: 1.7176265716552734, Final Batch Loss: 0.29896190762519836\n",
      "Epoch 5235, Loss: 1.764198660850525, Final Batch Loss: 0.21107229590415955\n",
      "Epoch 5236, Loss: 1.4637446403503418, Final Batch Loss: 0.2724805474281311\n",
      "Epoch 5237, Loss: 1.2800789773464203, Final Batch Loss: 0.17101362347602844\n",
      "Epoch 5238, Loss: 1.5661426931619644, Final Batch Loss: 0.47766560316085815\n",
      "Epoch 5239, Loss: 1.4042587876319885, Final Batch Loss: 0.2725212275981903\n",
      "Epoch 5240, Loss: 1.4087552577257156, Final Batch Loss: 0.3956608772277832\n",
      "Epoch 5241, Loss: 1.696777105331421, Final Batch Loss: 0.3212193548679352\n",
      "Epoch 5242, Loss: 1.337261140346527, Final Batch Loss: 0.21423886716365814\n",
      "Epoch 5243, Loss: 1.3316267132759094, Final Batch Loss: 0.28902468085289\n",
      "Epoch 5244, Loss: 1.70691978931427, Final Batch Loss: 0.5525549650192261\n",
      "Epoch 5245, Loss: 1.47874154150486, Final Batch Loss: 0.32266971468925476\n",
      "Epoch 5246, Loss: 1.3063563704490662, Final Batch Loss: 0.19043394923210144\n",
      "Epoch 5247, Loss: 1.5668093860149384, Final Batch Loss: 0.3614610433578491\n",
      "Epoch 5248, Loss: 1.3749888241291046, Final Batch Loss: 0.21495234966278076\n",
      "Epoch 5249, Loss: 1.5922753512859344, Final Batch Loss: 0.46340933442115784\n",
      "Epoch 5250, Loss: 1.4442583918571472, Final Batch Loss: 0.25173303484916687\n",
      "Epoch 5251, Loss: 1.553980529308319, Final Batch Loss: 0.28063324093818665\n",
      "Epoch 5252, Loss: 1.8222352266311646, Final Batch Loss: 0.41941967606544495\n",
      "Epoch 5253, Loss: 1.455126665532589, Final Batch Loss: 0.1183956190943718\n",
      "Epoch 5254, Loss: 1.4029324799776077, Final Batch Loss: 0.24890272319316864\n",
      "Epoch 5255, Loss: 1.4553281962871552, Final Batch Loss: 0.34084317088127136\n",
      "Epoch 5256, Loss: 1.355015143752098, Final Batch Loss: 0.29365772008895874\n",
      "Epoch 5257, Loss: 1.389988660812378, Final Batch Loss: 0.173529714345932\n",
      "Epoch 5258, Loss: 1.4389780014753342, Final Batch Loss: 0.21474497020244598\n",
      "Epoch 5259, Loss: 1.318969964981079, Final Batch Loss: 0.18915826082229614\n",
      "Epoch 5260, Loss: 1.5631519258022308, Final Batch Loss: 0.4937564730644226\n",
      "Epoch 5261, Loss: 1.276429682970047, Final Batch Loss: 0.2020251750946045\n",
      "Epoch 5262, Loss: 1.4031975716352463, Final Batch Loss: 0.15404246747493744\n",
      "Epoch 5263, Loss: 1.4789046496152878, Final Batch Loss: 0.3217448592185974\n",
      "Epoch 5264, Loss: 1.2702410817146301, Final Batch Loss: 0.2436700314283371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5265, Loss: 1.5023241937160492, Final Batch Loss: 0.4255804717540741\n",
      "Epoch 5266, Loss: 1.2824762761592865, Final Batch Loss: 0.1971013844013214\n",
      "Epoch 5267, Loss: 1.9163473844528198, Final Batch Loss: 0.5549739599227905\n",
      "Epoch 5268, Loss: 1.398328572511673, Final Batch Loss: 0.2733713984489441\n",
      "Epoch 5269, Loss: 1.375262826681137, Final Batch Loss: 0.3544897735118866\n",
      "Epoch 5270, Loss: 1.4161030948162079, Final Batch Loss: 0.32221344113349915\n",
      "Epoch 5271, Loss: 1.4069297313690186, Final Batch Loss: 0.22136905789375305\n",
      "Epoch 5272, Loss: 1.39077228307724, Final Batch Loss: 0.14045095443725586\n",
      "Epoch 5273, Loss: 1.4994123876094818, Final Batch Loss: 0.1891443431377411\n",
      "Epoch 5274, Loss: 1.3078181147575378, Final Batch Loss: 0.26628872752189636\n",
      "Epoch 5275, Loss: 1.3577012419700623, Final Batch Loss: 0.30808237195014954\n",
      "Epoch 5276, Loss: 1.5051526129245758, Final Batch Loss: 0.37659674882888794\n",
      "Epoch 5277, Loss: 1.3005240708589554, Final Batch Loss: 0.17533649504184723\n",
      "Epoch 5278, Loss: 1.5066743195056915, Final Batch Loss: 0.24207478761672974\n",
      "Epoch 5279, Loss: 1.336495354771614, Final Batch Loss: 0.24148230254650116\n",
      "Epoch 5280, Loss: 1.4597027897834778, Final Batch Loss: 0.25177639722824097\n",
      "Epoch 5281, Loss: 1.50312739610672, Final Batch Loss: 0.30136966705322266\n",
      "Epoch 5282, Loss: 1.5996239483356476, Final Batch Loss: 0.2727641463279724\n",
      "Epoch 5283, Loss: 1.35857655107975, Final Batch Loss: 0.30055147409439087\n",
      "Epoch 5284, Loss: 1.3713707625865936, Final Batch Loss: 0.22118602693080902\n",
      "Epoch 5285, Loss: 1.8223633915185928, Final Batch Loss: 0.405225932598114\n",
      "Epoch 5286, Loss: 1.6608036756515503, Final Batch Loss: 0.4687802195549011\n",
      "Epoch 5287, Loss: 1.7074975222349167, Final Batch Loss: 0.30934643745422363\n",
      "Epoch 5288, Loss: 1.5947028994560242, Final Batch Loss: 0.34842121601104736\n",
      "Epoch 5289, Loss: 1.440820038318634, Final Batch Loss: 0.1812022626399994\n",
      "Epoch 5290, Loss: 1.7365981936454773, Final Batch Loss: 0.32722845673561096\n",
      "Epoch 5291, Loss: 1.4322082102298737, Final Batch Loss: 0.3061308264732361\n",
      "Epoch 5292, Loss: 1.5532336980104446, Final Batch Loss: 0.3355052173137665\n",
      "Epoch 5293, Loss: 1.434287667274475, Final Batch Loss: 0.19595766067504883\n",
      "Epoch 5294, Loss: 1.4677556604146957, Final Batch Loss: 0.29836055636405945\n",
      "Epoch 5295, Loss: 1.5388332605361938, Final Batch Loss: 0.3684559762477875\n",
      "Epoch 5296, Loss: 1.3907599747180939, Final Batch Loss: 0.2925359904766083\n",
      "Epoch 5297, Loss: 1.5791478157043457, Final Batch Loss: 0.34100568294525146\n",
      "Epoch 5298, Loss: 1.3691136687994003, Final Batch Loss: 0.21253032982349396\n",
      "Epoch 5299, Loss: 1.3918762803077698, Final Batch Loss: 0.21643997728824615\n",
      "Epoch 5300, Loss: 1.5871976166963577, Final Batch Loss: 0.3996768593788147\n",
      "Epoch 5301, Loss: 1.3976478427648544, Final Batch Loss: 0.2046917825937271\n",
      "Epoch 5302, Loss: 1.4321656078100204, Final Batch Loss: 0.3728918433189392\n",
      "Epoch 5303, Loss: 1.5017703473567963, Final Batch Loss: 0.37566426396369934\n",
      "Epoch 5304, Loss: 1.631348580121994, Final Batch Loss: 0.3130394518375397\n",
      "Epoch 5305, Loss: 1.6206674873828888, Final Batch Loss: 0.3605884313583374\n",
      "Epoch 5306, Loss: 1.3203071504831314, Final Batch Loss: 0.1702055186033249\n",
      "Epoch 5307, Loss: 1.521464616060257, Final Batch Loss: 0.325267493724823\n",
      "Epoch 5308, Loss: 1.503834992647171, Final Batch Loss: 0.36042359471321106\n",
      "Epoch 5309, Loss: 1.297040268778801, Final Batch Loss: 0.20555581152439117\n",
      "Epoch 5310, Loss: 1.3740519136190414, Final Batch Loss: 0.2323889583349228\n",
      "Epoch 5311, Loss: 1.5730167478322983, Final Batch Loss: 0.2253166288137436\n",
      "Epoch 5312, Loss: 1.7284403890371323, Final Batch Loss: 0.5573922395706177\n",
      "Epoch 5313, Loss: 1.3644661754369736, Final Batch Loss: 0.2729921340942383\n",
      "Epoch 5314, Loss: 1.4447747021913528, Final Batch Loss: 0.2548832893371582\n",
      "Epoch 5315, Loss: 1.8020505458116531, Final Batch Loss: 0.548140823841095\n",
      "Epoch 5316, Loss: 1.345168724656105, Final Batch Loss: 0.22286395728588104\n",
      "Epoch 5317, Loss: 1.5547659546136856, Final Batch Loss: 0.5033228397369385\n",
      "Epoch 5318, Loss: 1.589635193347931, Final Batch Loss: 0.340657114982605\n",
      "Epoch 5319, Loss: 1.4513608813285828, Final Batch Loss: 0.24472621083259583\n",
      "Epoch 5320, Loss: 1.5658270865678787, Final Batch Loss: 0.40063202381134033\n",
      "Epoch 5321, Loss: 1.6358725428581238, Final Batch Loss: 0.32708898186683655\n",
      "Epoch 5322, Loss: 1.8679646849632263, Final Batch Loss: 0.5525173544883728\n",
      "Epoch 5323, Loss: 1.603391706943512, Final Batch Loss: 0.30276551842689514\n",
      "Epoch 5324, Loss: 1.4285652935504913, Final Batch Loss: 0.19975592195987701\n",
      "Epoch 5325, Loss: 1.575231522321701, Final Batch Loss: 0.2910767197608948\n",
      "Epoch 5326, Loss: 1.5196211785078049, Final Batch Loss: 0.3382793366909027\n",
      "Epoch 5327, Loss: 1.335024580359459, Final Batch Loss: 0.2661709785461426\n",
      "Epoch 5328, Loss: 1.3788864463567734, Final Batch Loss: 0.20138634741306305\n",
      "Epoch 5329, Loss: 1.5301664024591446, Final Batch Loss: 0.41109558939933777\n",
      "Epoch 5330, Loss: 1.619734287261963, Final Batch Loss: 0.3603155314922333\n",
      "Epoch 5331, Loss: 1.498327374458313, Final Batch Loss: 0.23365581035614014\n",
      "Epoch 5332, Loss: 1.3585699200630188, Final Batch Loss: 0.24386174976825714\n",
      "Epoch 5333, Loss: 1.5019088238477707, Final Batch Loss: 0.29455122351646423\n",
      "Epoch 5334, Loss: 1.5454761236906052, Final Batch Loss: 0.41894111037254333\n",
      "Epoch 5335, Loss: 1.440463125705719, Final Batch Loss: 0.25147712230682373\n",
      "Epoch 5336, Loss: 1.3907744139432907, Final Batch Loss: 0.19245730340480804\n",
      "Epoch 5337, Loss: 1.3400076180696487, Final Batch Loss: 0.20934751629829407\n",
      "Epoch 5338, Loss: 1.417030468583107, Final Batch Loss: 0.3400641083717346\n",
      "Epoch 5339, Loss: 1.5849129408597946, Final Batch Loss: 0.4296172857284546\n",
      "Epoch 5340, Loss: 1.450594186782837, Final Batch Loss: 0.32534587383270264\n",
      "Epoch 5341, Loss: 1.2655259817838669, Final Batch Loss: 0.15641939640045166\n",
      "Epoch 5342, Loss: 1.5486578792333603, Final Batch Loss: 0.32993999123573303\n",
      "Epoch 5343, Loss: 1.2707156240940094, Final Batch Loss: 0.16011062264442444\n",
      "Epoch 5344, Loss: 1.5325263887643814, Final Batch Loss: 0.28716519474983215\n",
      "Epoch 5345, Loss: 1.2664458006620407, Final Batch Loss: 0.236191987991333\n",
      "Epoch 5346, Loss: 1.303691104054451, Final Batch Loss: 0.23684729635715485\n",
      "Epoch 5347, Loss: 1.8510660827159882, Final Batch Loss: 0.5605769157409668\n",
      "Epoch 5348, Loss: 1.4603189527988434, Final Batch Loss: 0.35792121291160583\n",
      "Epoch 5349, Loss: 1.3856819719076157, Final Batch Loss: 0.27280256152153015\n",
      "Epoch 5350, Loss: 1.547752559185028, Final Batch Loss: 0.33072736859321594\n",
      "Epoch 5351, Loss: 1.3960941135883331, Final Batch Loss: 0.34478527307510376\n",
      "Epoch 5352, Loss: 1.6158596724271774, Final Batch Loss: 0.5666333436965942\n",
      "Epoch 5353, Loss: 1.8324885666370392, Final Batch Loss: 0.44215402007102966\n",
      "Epoch 5354, Loss: 1.284585401415825, Final Batch Loss: 0.15460632741451263\n",
      "Epoch 5355, Loss: 1.4127368330955505, Final Batch Loss: 0.21185201406478882\n",
      "Epoch 5356, Loss: 1.5085753798484802, Final Batch Loss: 0.18486982583999634\n",
      "Epoch 5357, Loss: 1.4204773753881454, Final Batch Loss: 0.3716747462749481\n",
      "Epoch 5358, Loss: 1.4516831189393997, Final Batch Loss: 0.13340257108211517\n",
      "Epoch 5359, Loss: 1.687139630317688, Final Batch Loss: 0.4098570942878723\n",
      "Epoch 5360, Loss: 1.5118816196918488, Final Batch Loss: 0.32323893904685974\n",
      "Epoch 5361, Loss: 1.622147724032402, Final Batch Loss: 0.4126380681991577\n",
      "Epoch 5362, Loss: 1.540213793516159, Final Batch Loss: 0.38273099064826965\n",
      "Epoch 5363, Loss: 1.515151634812355, Final Batch Loss: 0.2346915751695633\n",
      "Epoch 5364, Loss: 1.4378512501716614, Final Batch Loss: 0.23745566606521606\n",
      "Epoch 5365, Loss: 1.5190027356147766, Final Batch Loss: 0.3109942078590393\n",
      "Epoch 5366, Loss: 1.2916726171970367, Final Batch Loss: 0.18085840344429016\n",
      "Epoch 5367, Loss: 1.3803683370351791, Final Batch Loss: 0.3333631753921509\n",
      "Epoch 5368, Loss: 1.397269755601883, Final Batch Loss: 0.26934146881103516\n",
      "Epoch 5369, Loss: 1.3714303076267242, Final Batch Loss: 0.28930702805519104\n",
      "Epoch 5370, Loss: 1.4244134426116943, Final Batch Loss: 0.26928767561912537\n",
      "Epoch 5371, Loss: 1.4845547825098038, Final Batch Loss: 0.24265366792678833\n",
      "Epoch 5372, Loss: 1.325205609202385, Final Batch Loss: 0.3030991852283478\n",
      "Epoch 5373, Loss: 1.5276070833206177, Final Batch Loss: 0.3152170181274414\n",
      "Epoch 5374, Loss: 1.2761294692754745, Final Batch Loss: 0.17428922653198242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5375, Loss: 1.410917118191719, Final Batch Loss: 0.28645625710487366\n",
      "Epoch 5376, Loss: 1.5829215794801712, Final Batch Loss: 0.42463618516921997\n",
      "Epoch 5377, Loss: 1.2627344578504562, Final Batch Loss: 0.2683579623699188\n",
      "Epoch 5378, Loss: 1.4270637482404709, Final Batch Loss: 0.28378862142562866\n",
      "Epoch 5379, Loss: 1.4669056981801987, Final Batch Loss: 0.2937071919441223\n",
      "Epoch 5380, Loss: 1.4296703785657883, Final Batch Loss: 0.1758429855108261\n",
      "Epoch 5381, Loss: 1.648845762014389, Final Batch Loss: 0.373232364654541\n",
      "Epoch 5382, Loss: 1.5826533734798431, Final Batch Loss: 0.37820228934288025\n",
      "Epoch 5383, Loss: 1.274006962776184, Final Batch Loss: 0.24245090782642365\n",
      "Epoch 5384, Loss: 1.4109212309122086, Final Batch Loss: 0.23660273849964142\n",
      "Epoch 5385, Loss: 1.3979355543851852, Final Batch Loss: 0.2284218966960907\n",
      "Epoch 5386, Loss: 1.341342255473137, Final Batch Loss: 0.2449537217617035\n",
      "Epoch 5387, Loss: 1.4616214036941528, Final Batch Loss: 0.4287249445915222\n",
      "Epoch 5388, Loss: 1.3464936465024948, Final Batch Loss: 0.33258721232414246\n",
      "Epoch 5389, Loss: 1.3798404932022095, Final Batch Loss: 0.18266519904136658\n",
      "Epoch 5390, Loss: 1.4970265924930573, Final Batch Loss: 0.1549501121044159\n",
      "Epoch 5391, Loss: 1.171978920698166, Final Batch Loss: 0.13738501071929932\n",
      "Epoch 5392, Loss: 1.5945856869220734, Final Batch Loss: 0.3257947564125061\n",
      "Epoch 5393, Loss: 1.4759276509284973, Final Batch Loss: 0.45578494668006897\n",
      "Epoch 5394, Loss: 1.5806716084480286, Final Batch Loss: 0.35676050186157227\n",
      "Epoch 5395, Loss: 1.442085012793541, Final Batch Loss: 0.23150895535945892\n",
      "Epoch 5396, Loss: 1.4078727662563324, Final Batch Loss: 0.3145720958709717\n",
      "Epoch 5397, Loss: 1.3170869648456573, Final Batch Loss: 0.24057593941688538\n",
      "Epoch 5398, Loss: 1.6851804554462433, Final Batch Loss: 0.47808897495269775\n",
      "Epoch 5399, Loss: 1.393454134464264, Final Batch Loss: 0.2619493305683136\n",
      "Epoch 5400, Loss: 1.375382661819458, Final Batch Loss: 0.25477156043052673\n",
      "Epoch 5401, Loss: 1.549631118774414, Final Batch Loss: 0.3103925883769989\n",
      "Epoch 5402, Loss: 1.3301427066326141, Final Batch Loss: 0.19991667568683624\n",
      "Epoch 5403, Loss: 1.359757050871849, Final Batch Loss: 0.19511885941028595\n",
      "Epoch 5404, Loss: 1.40510293841362, Final Batch Loss: 0.2621968984603882\n",
      "Epoch 5405, Loss: 1.461282193660736, Final Batch Loss: 0.285837322473526\n",
      "Epoch 5406, Loss: 1.2964710146188736, Final Batch Loss: 0.15746895968914032\n",
      "Epoch 5407, Loss: 1.4908371567726135, Final Batch Loss: 0.3275706470012665\n",
      "Epoch 5408, Loss: 1.4093210250139236, Final Batch Loss: 0.2995412349700928\n",
      "Epoch 5409, Loss: 1.5281417965888977, Final Batch Loss: 0.2618304491043091\n",
      "Epoch 5410, Loss: 1.3437797725200653, Final Batch Loss: 0.3045404255390167\n",
      "Epoch 5411, Loss: 1.4029889702796936, Final Batch Loss: 0.19961822032928467\n",
      "Epoch 5412, Loss: 1.3937552869319916, Final Batch Loss: 0.30603477358818054\n",
      "Epoch 5413, Loss: 1.385750025510788, Final Batch Loss: 0.25161340832710266\n",
      "Epoch 5414, Loss: 1.3651955723762512, Final Batch Loss: 0.224348247051239\n",
      "Epoch 5415, Loss: 1.4154366850852966, Final Batch Loss: 0.3205457627773285\n",
      "Epoch 5416, Loss: 1.2623027861118317, Final Batch Loss: 0.12610653042793274\n",
      "Epoch 5417, Loss: 1.47275872528553, Final Batch Loss: 0.3082444965839386\n",
      "Epoch 5418, Loss: 1.689424216747284, Final Batch Loss: 0.4022884666919708\n",
      "Epoch 5419, Loss: 1.5941194295883179, Final Batch Loss: 0.41883260011672974\n",
      "Epoch 5420, Loss: 1.3691060692071915, Final Batch Loss: 0.3645244836807251\n",
      "Epoch 5421, Loss: 1.662229835987091, Final Batch Loss: 0.35482123494148254\n",
      "Epoch 5422, Loss: 1.2733775079250336, Final Batch Loss: 0.259942889213562\n",
      "Epoch 5423, Loss: 1.5780650079250336, Final Batch Loss: 0.2854587435722351\n",
      "Epoch 5424, Loss: 1.4367504715919495, Final Batch Loss: 0.28482457995414734\n",
      "Epoch 5425, Loss: 1.447685182094574, Final Batch Loss: 0.330466091632843\n",
      "Epoch 5426, Loss: 1.2460644394159317, Final Batch Loss: 0.2880823612213135\n",
      "Epoch 5427, Loss: 1.3991812318563461, Final Batch Loss: 0.2824040949344635\n",
      "Epoch 5428, Loss: 1.7511002719402313, Final Batch Loss: 0.45446351170539856\n",
      "Epoch 5429, Loss: 1.3230988830327988, Final Batch Loss: 0.29711461067199707\n",
      "Epoch 5430, Loss: 1.542295053601265, Final Batch Loss: 0.3781967759132385\n",
      "Epoch 5431, Loss: 1.5359944105148315, Final Batch Loss: 0.2429995834827423\n",
      "Epoch 5432, Loss: 1.3769493252038956, Final Batch Loss: 0.22202040255069733\n",
      "Epoch 5433, Loss: 1.6135763227939606, Final Batch Loss: 0.3983622193336487\n",
      "Epoch 5434, Loss: 1.3491189628839493, Final Batch Loss: 0.2560184597969055\n",
      "Epoch 5435, Loss: 1.2873619943857193, Final Batch Loss: 0.2438134104013443\n",
      "Epoch 5436, Loss: 1.3409814983606339, Final Batch Loss: 0.2569282650947571\n",
      "Epoch 5437, Loss: 1.3563690036535263, Final Batch Loss: 0.1780964434146881\n",
      "Epoch 5438, Loss: 1.552743911743164, Final Batch Loss: 0.37244468927383423\n",
      "Epoch 5439, Loss: 1.5845476388931274, Final Batch Loss: 0.3278256356716156\n",
      "Epoch 5440, Loss: 1.4353572726249695, Final Batch Loss: 0.23413434624671936\n",
      "Epoch 5441, Loss: 1.4906144738197327, Final Batch Loss: 0.30638378858566284\n",
      "Epoch 5442, Loss: 1.556762233376503, Final Batch Loss: 0.19469265639781952\n",
      "Epoch 5443, Loss: 1.3805921375751495, Final Batch Loss: 0.23627834022045135\n",
      "Epoch 5444, Loss: 1.6253015398979187, Final Batch Loss: 0.27565258741378784\n",
      "Epoch 5445, Loss: 1.3558287918567657, Final Batch Loss: 0.1968846321105957\n",
      "Epoch 5446, Loss: 1.4009542614221573, Final Batch Loss: 0.22191646695137024\n",
      "Epoch 5447, Loss: 1.3140615224838257, Final Batch Loss: 0.21783722937107086\n",
      "Epoch 5448, Loss: 1.3682847917079926, Final Batch Loss: 0.1235891580581665\n",
      "Epoch 5449, Loss: 1.561564952135086, Final Batch Loss: 0.3134090304374695\n",
      "Epoch 5450, Loss: 1.5344319343566895, Final Batch Loss: 0.23340457677841187\n",
      "Epoch 5451, Loss: 1.4714628010988235, Final Batch Loss: 0.20050062239170074\n",
      "Epoch 5452, Loss: 1.4186607897281647, Final Batch Loss: 0.28134188055992126\n",
      "Epoch 5453, Loss: 1.3466714173555374, Final Batch Loss: 0.26749512553215027\n",
      "Epoch 5454, Loss: 1.3429484218358994, Final Batch Loss: 0.24056415259838104\n",
      "Epoch 5455, Loss: 1.542145922780037, Final Batch Loss: 0.4179871678352356\n",
      "Epoch 5456, Loss: 1.4504600912332535, Final Batch Loss: 0.18431924283504486\n",
      "Epoch 5457, Loss: 1.3509256094694138, Final Batch Loss: 0.2076863795518875\n",
      "Epoch 5458, Loss: 1.3377557396888733, Final Batch Loss: 0.24063076078891754\n",
      "Epoch 5459, Loss: 1.4172904342412949, Final Batch Loss: 0.2588714361190796\n",
      "Epoch 5460, Loss: 1.4705554097890854, Final Batch Loss: 0.25525403022766113\n",
      "Epoch 5461, Loss: 1.520495280623436, Final Batch Loss: 0.17710770666599274\n",
      "Epoch 5462, Loss: 1.5514815747737885, Final Batch Loss: 0.4467357397079468\n",
      "Epoch 5463, Loss: 1.5190548300743103, Final Batch Loss: 0.21413567662239075\n",
      "Epoch 5464, Loss: 1.238445520401001, Final Batch Loss: 0.18388386070728302\n",
      "Epoch 5465, Loss: 1.7071921527385712, Final Batch Loss: 0.3274218440055847\n",
      "Epoch 5466, Loss: 1.6201872676610947, Final Batch Loss: 0.2424079328775406\n",
      "Epoch 5467, Loss: 1.454773098230362, Final Batch Loss: 0.3326975405216217\n",
      "Epoch 5468, Loss: 1.4507868736982346, Final Batch Loss: 0.3142617344856262\n",
      "Epoch 5469, Loss: 1.406116932630539, Final Batch Loss: 0.2848791778087616\n",
      "Epoch 5470, Loss: 1.3710105568170547, Final Batch Loss: 0.2912707030773163\n",
      "Epoch 5471, Loss: 1.3971075862646103, Final Batch Loss: 0.36592549085617065\n",
      "Epoch 5472, Loss: 1.3759773969650269, Final Batch Loss: 0.2196638435125351\n",
      "Epoch 5473, Loss: 1.4284639805555344, Final Batch Loss: 0.22272154688835144\n",
      "Epoch 5474, Loss: 1.3446225821971893, Final Batch Loss: 0.1323060691356659\n",
      "Epoch 5475, Loss: 1.5311829447746277, Final Batch Loss: 0.3025094270706177\n",
      "Epoch 5476, Loss: 1.3035358488559723, Final Batch Loss: 0.29733332991600037\n",
      "Epoch 5477, Loss: 1.501368373632431, Final Batch Loss: 0.22533532977104187\n",
      "Epoch 5478, Loss: 1.688950389623642, Final Batch Loss: 0.450359582901001\n",
      "Epoch 5479, Loss: 1.758512556552887, Final Batch Loss: 0.3779640793800354\n",
      "Epoch 5480, Loss: 1.4901441037654877, Final Batch Loss: 0.2514829933643341\n",
      "Epoch 5481, Loss: 1.495216578245163, Final Batch Loss: 0.2743133008480072\n",
      "Epoch 5482, Loss: 1.289184182882309, Final Batch Loss: 0.2495952695608139\n",
      "Epoch 5483, Loss: 1.4464413970708847, Final Batch Loss: 0.22478173673152924\n",
      "Epoch 5484, Loss: 1.5715443640947342, Final Batch Loss: 0.3803905248641968\n",
      "Epoch 5485, Loss: 1.5729966461658478, Final Batch Loss: 0.32035478949546814\n",
      "Epoch 5486, Loss: 1.398861140012741, Final Batch Loss: 0.3247760832309723\n",
      "Epoch 5487, Loss: 1.3150752484798431, Final Batch Loss: 0.28189224004745483\n",
      "Epoch 5488, Loss: 1.3730576634407043, Final Batch Loss: 0.26715752482414246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5489, Loss: 1.3446481376886368, Final Batch Loss: 0.22812801599502563\n",
      "Epoch 5490, Loss: 1.4105414599180222, Final Batch Loss: 0.2712096571922302\n",
      "Epoch 5491, Loss: 1.3989129811525345, Final Batch Loss: 0.3286447823047638\n",
      "Epoch 5492, Loss: 1.3586671352386475, Final Batch Loss: 0.2924177646636963\n",
      "Epoch 5493, Loss: 1.5055651366710663, Final Batch Loss: 0.18993279337882996\n",
      "Epoch 5494, Loss: 1.3965924084186554, Final Batch Loss: 0.24831873178482056\n",
      "Epoch 5495, Loss: 1.377803087234497, Final Batch Loss: 0.2319684475660324\n",
      "Epoch 5496, Loss: 1.2725131958723068, Final Batch Loss: 0.27077412605285645\n",
      "Epoch 5497, Loss: 1.4164646565914154, Final Batch Loss: 0.35344406962394714\n",
      "Epoch 5498, Loss: 1.5179120004177094, Final Batch Loss: 0.38393571972846985\n",
      "Epoch 5499, Loss: 1.471364974975586, Final Batch Loss: 0.29740190505981445\n",
      "Epoch 5500, Loss: 1.4385470151901245, Final Batch Loss: 0.2957988977432251\n",
      "Epoch 5501, Loss: 1.311863824725151, Final Batch Loss: 0.2729398310184479\n",
      "Epoch 5502, Loss: 1.351870447397232, Final Batch Loss: 0.24822591245174408\n",
      "Epoch 5503, Loss: 1.6041006594896317, Final Batch Loss: 0.5043208599090576\n",
      "Epoch 5504, Loss: 1.7792588770389557, Final Batch Loss: 0.6497441530227661\n",
      "Epoch 5505, Loss: 1.5061125457286835, Final Batch Loss: 0.3018747866153717\n",
      "Epoch 5506, Loss: 1.4008703380823135, Final Batch Loss: 0.3066008388996124\n",
      "Epoch 5507, Loss: 1.414819821715355, Final Batch Loss: 0.31815585494041443\n",
      "Epoch 5508, Loss: 1.4352396428585052, Final Batch Loss: 0.23103350400924683\n",
      "Epoch 5509, Loss: 1.464262455701828, Final Batch Loss: 0.1866566240787506\n",
      "Epoch 5510, Loss: 1.4356781542301178, Final Batch Loss: 0.31582212448120117\n",
      "Epoch 5511, Loss: 1.6916216909885406, Final Batch Loss: 0.3148877024650574\n",
      "Epoch 5512, Loss: 1.4159530699253082, Final Batch Loss: 0.29912829399108887\n",
      "Epoch 5513, Loss: 1.4470638930797577, Final Batch Loss: 0.28381991386413574\n",
      "Epoch 5514, Loss: 1.5305692106485367, Final Batch Loss: 0.39779427647590637\n",
      "Epoch 5515, Loss: 1.4349282830953598, Final Batch Loss: 0.2701356112957001\n",
      "Epoch 5516, Loss: 1.5495579987764359, Final Batch Loss: 0.3474031686782837\n",
      "Epoch 5517, Loss: 1.5212576985359192, Final Batch Loss: 0.3745003342628479\n",
      "Epoch 5518, Loss: 1.295537143945694, Final Batch Loss: 0.16532380878925323\n",
      "Epoch 5519, Loss: 1.431227207183838, Final Batch Loss: 0.3659796416759491\n",
      "Epoch 5520, Loss: 1.3482986241579056, Final Batch Loss: 0.26752668619155884\n",
      "Epoch 5521, Loss: 1.294774129986763, Final Batch Loss: 0.22963835299015045\n",
      "Epoch 5522, Loss: 1.2259970009326935, Final Batch Loss: 0.19334211945533752\n",
      "Epoch 5523, Loss: 1.443777859210968, Final Batch Loss: 0.30488184094429016\n",
      "Epoch 5524, Loss: 1.3236438482999802, Final Batch Loss: 0.2484361231327057\n",
      "Epoch 5525, Loss: 1.318012684583664, Final Batch Loss: 0.2368861585855484\n",
      "Epoch 5526, Loss: 1.3226541578769684, Final Batch Loss: 0.22884532809257507\n",
      "Epoch 5527, Loss: 1.3917718082666397, Final Batch Loss: 0.15933649241924286\n",
      "Epoch 5528, Loss: 1.5778768956661224, Final Batch Loss: 0.4603566825389862\n",
      "Epoch 5529, Loss: 1.3770229518413544, Final Batch Loss: 0.29773351550102234\n",
      "Epoch 5530, Loss: 1.5642278343439102, Final Batch Loss: 0.20049335062503815\n",
      "Epoch 5531, Loss: 1.4104337692260742, Final Batch Loss: 0.18592481315135956\n",
      "Epoch 5532, Loss: 1.536660596728325, Final Batch Loss: 0.3381552994251251\n",
      "Epoch 5533, Loss: 1.5135219097137451, Final Batch Loss: 0.3648451268672943\n",
      "Epoch 5534, Loss: 1.6422894597053528, Final Batch Loss: 0.3302445113658905\n",
      "Epoch 5535, Loss: 1.5633972585201263, Final Batch Loss: 0.21235114336013794\n",
      "Epoch 5536, Loss: 1.2809270024299622, Final Batch Loss: 0.23676128685474396\n",
      "Epoch 5537, Loss: 1.316559799015522, Final Batch Loss: 0.1231011226773262\n",
      "Epoch 5538, Loss: 1.498019814491272, Final Batch Loss: 0.31953907012939453\n",
      "Epoch 5539, Loss: 1.7922340035438538, Final Batch Loss: 0.4667113125324249\n",
      "Epoch 5540, Loss: 1.4136296957731247, Final Batch Loss: 0.1915738731622696\n",
      "Epoch 5541, Loss: 1.6440940499305725, Final Batch Loss: 0.3433331549167633\n",
      "Epoch 5542, Loss: 1.5080834925174713, Final Batch Loss: 0.2811831831932068\n",
      "Epoch 5543, Loss: 1.3779185116291046, Final Batch Loss: 0.21253454685211182\n",
      "Epoch 5544, Loss: 1.3298949897289276, Final Batch Loss: 0.23313024640083313\n",
      "Epoch 5545, Loss: 1.4457692950963974, Final Batch Loss: 0.22808079421520233\n",
      "Epoch 5546, Loss: 1.4198689758777618, Final Batch Loss: 0.21200108528137207\n",
      "Epoch 5547, Loss: 1.4888735711574554, Final Batch Loss: 0.2365264892578125\n",
      "Epoch 5548, Loss: 1.3173852860927582, Final Batch Loss: 0.2802268862724304\n",
      "Epoch 5549, Loss: 1.359848529100418, Final Batch Loss: 0.16096970438957214\n",
      "Epoch 5550, Loss: 1.2167392075061798, Final Batch Loss: 0.160905659198761\n",
      "Epoch 5551, Loss: 1.3277883380651474, Final Batch Loss: 0.19417829811573029\n",
      "Epoch 5552, Loss: 1.545169785618782, Final Batch Loss: 0.4775158762931824\n",
      "Epoch 5553, Loss: 1.5137085914611816, Final Batch Loss: 0.3353857696056366\n",
      "Epoch 5554, Loss: 1.3593492209911346, Final Batch Loss: 0.2825731933116913\n",
      "Epoch 5555, Loss: 1.358524113893509, Final Batch Loss: 0.23476427793502808\n",
      "Epoch 5556, Loss: 1.4601001888513565, Final Batch Loss: 0.29749244451522827\n",
      "Epoch 5557, Loss: 1.446230709552765, Final Batch Loss: 0.28385770320892334\n",
      "Epoch 5558, Loss: 1.6866236925125122, Final Batch Loss: 0.37623119354248047\n",
      "Epoch 5559, Loss: 1.337352529168129, Final Batch Loss: 0.19178269803524017\n",
      "Epoch 5560, Loss: 1.349671646952629, Final Batch Loss: 0.23285597562789917\n",
      "Epoch 5561, Loss: 1.40410515666008, Final Batch Loss: 0.2111946940422058\n",
      "Epoch 5562, Loss: 1.5408309996128082, Final Batch Loss: 0.30653998255729675\n",
      "Epoch 5563, Loss: 1.4501855075359344, Final Batch Loss: 0.24115948379039764\n",
      "Epoch 5564, Loss: 1.4233332127332687, Final Batch Loss: 0.2887953817844391\n",
      "Epoch 5565, Loss: 1.4416921734809875, Final Batch Loss: 0.3950716555118561\n",
      "Epoch 5566, Loss: 1.6493749618530273, Final Batch Loss: 0.33785396814346313\n",
      "Epoch 5567, Loss: 1.660969465970993, Final Batch Loss: 0.37831589579582214\n",
      "Epoch 5568, Loss: 1.4648036062717438, Final Batch Loss: 0.1545456349849701\n",
      "Epoch 5569, Loss: 1.4596144706010818, Final Batch Loss: 0.30948635935783386\n",
      "Epoch 5570, Loss: 1.3515701293945312, Final Batch Loss: 0.313495934009552\n",
      "Epoch 5571, Loss: 1.439646303653717, Final Batch Loss: 0.35273781418800354\n",
      "Epoch 5572, Loss: 1.4806157052516937, Final Batch Loss: 0.3585354685783386\n",
      "Epoch 5573, Loss: 1.6660354435443878, Final Batch Loss: 0.289335697889328\n",
      "Epoch 5574, Loss: 1.7219593822956085, Final Batch Loss: 0.3800583481788635\n",
      "Epoch 5575, Loss: 1.289655551314354, Final Batch Loss: 0.24268105626106262\n",
      "Epoch 5576, Loss: 1.5413996577262878, Final Batch Loss: 0.3620865046977997\n",
      "Epoch 5577, Loss: 1.2821539044380188, Final Batch Loss: 0.2669181227684021\n",
      "Epoch 5578, Loss: 1.488785833120346, Final Batch Loss: 0.4365866482257843\n",
      "Epoch 5579, Loss: 1.309554547071457, Final Batch Loss: 0.3082370162010193\n",
      "Epoch 5580, Loss: 1.4031640440225601, Final Batch Loss: 0.20252786576747894\n",
      "Epoch 5581, Loss: 1.5380791276693344, Final Batch Loss: 0.37182724475860596\n",
      "Epoch 5582, Loss: 1.340286836028099, Final Batch Loss: 0.22420534491539001\n",
      "Epoch 5583, Loss: 1.4919118583202362, Final Batch Loss: 0.3046332001686096\n",
      "Epoch 5584, Loss: 1.7159150540828705, Final Batch Loss: 0.3955596685409546\n",
      "Epoch 5585, Loss: 1.2802045345306396, Final Batch Loss: 0.19446823000907898\n",
      "Epoch 5586, Loss: 1.4150802791118622, Final Batch Loss: 0.2791801393032074\n",
      "Epoch 5587, Loss: 1.362409770488739, Final Batch Loss: 0.2134801149368286\n",
      "Epoch 5588, Loss: 1.422138437628746, Final Batch Loss: 0.290727823972702\n",
      "Epoch 5589, Loss: 1.6731513142585754, Final Batch Loss: 0.3555479645729065\n",
      "Epoch 5590, Loss: 1.3736547976732254, Final Batch Loss: 0.19432798027992249\n",
      "Epoch 5591, Loss: 1.2333560436964035, Final Batch Loss: 0.34264281392097473\n",
      "Epoch 5592, Loss: 1.5662618577480316, Final Batch Loss: 0.3456844389438629\n",
      "Epoch 5593, Loss: 1.5411392152309418, Final Batch Loss: 0.34455832839012146\n",
      "Epoch 5594, Loss: 1.3817074596881866, Final Batch Loss: 0.18123731017112732\n",
      "Epoch 5595, Loss: 1.4956976175308228, Final Batch Loss: 0.3138640522956848\n",
      "Epoch 5596, Loss: 1.3856577426195145, Final Batch Loss: 0.36955952644348145\n",
      "Epoch 5597, Loss: 1.4193442612886429, Final Batch Loss: 0.20769478380680084\n",
      "Epoch 5598, Loss: 1.3742835074663162, Final Batch Loss: 0.2718594968318939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5599, Loss: 1.6475668400526047, Final Batch Loss: 0.49820569157600403\n",
      "Epoch 5600, Loss: 1.3131654858589172, Final Batch Loss: 0.2764478027820587\n",
      "Epoch 5601, Loss: 1.5895458310842514, Final Batch Loss: 0.24972514808177948\n",
      "Epoch 5602, Loss: 1.5961604714393616, Final Batch Loss: 0.4075397849082947\n",
      "Epoch 5603, Loss: 1.4172598719596863, Final Batch Loss: 0.2026003897190094\n",
      "Epoch 5604, Loss: 1.4343518316745758, Final Batch Loss: 0.3223569691181183\n",
      "Epoch 5605, Loss: 1.346550539135933, Final Batch Loss: 0.2707507908344269\n",
      "Epoch 5606, Loss: 1.2853489965200424, Final Batch Loss: 0.2470410317182541\n",
      "Epoch 5607, Loss: 1.3765797019004822, Final Batch Loss: 0.3031664192676544\n",
      "Epoch 5608, Loss: 1.2964849919080734, Final Batch Loss: 0.22954301536083221\n",
      "Epoch 5609, Loss: 1.3574806153774261, Final Batch Loss: 0.28068652749061584\n",
      "Epoch 5610, Loss: 1.458816722035408, Final Batch Loss: 0.29457372426986694\n",
      "Epoch 5611, Loss: 1.5138730257749557, Final Batch Loss: 0.2853522002696991\n",
      "Epoch 5612, Loss: 1.7226433157920837, Final Batch Loss: 0.4622363746166229\n",
      "Epoch 5613, Loss: 1.4398455321788788, Final Batch Loss: 0.28611844778060913\n",
      "Epoch 5614, Loss: 1.4806495159864426, Final Batch Loss: 0.22016648948192596\n",
      "Epoch 5615, Loss: 1.6012956947088242, Final Batch Loss: 0.4170973002910614\n",
      "Epoch 5616, Loss: 1.499652087688446, Final Batch Loss: 0.3637341856956482\n",
      "Epoch 5617, Loss: 1.5411890596151352, Final Batch Loss: 0.20241259038448334\n",
      "Epoch 5618, Loss: 1.4887466132640839, Final Batch Loss: 0.2897742986679077\n",
      "Epoch 5619, Loss: 1.3737228512763977, Final Batch Loss: 0.28347066044807434\n",
      "Epoch 5620, Loss: 1.5496460646390915, Final Batch Loss: 0.3291642963886261\n",
      "Epoch 5621, Loss: 1.6108311116695404, Final Batch Loss: 0.4057413339614868\n",
      "Epoch 5622, Loss: 1.6895015835762024, Final Batch Loss: 0.4892711341381073\n",
      "Epoch 5623, Loss: 1.6003353893756866, Final Batch Loss: 0.4143916368484497\n",
      "Epoch 5624, Loss: 1.4651129692792892, Final Batch Loss: 0.27448293566703796\n",
      "Epoch 5625, Loss: 1.5667882561683655, Final Batch Loss: 0.2873048484325409\n",
      "Epoch 5626, Loss: 1.5118675231933594, Final Batch Loss: 0.30571359395980835\n",
      "Epoch 5627, Loss: 1.5083640962839127, Final Batch Loss: 0.2408626675605774\n",
      "Epoch 5628, Loss: 1.457203984260559, Final Batch Loss: 0.2719019949436188\n",
      "Epoch 5629, Loss: 1.6170324683189392, Final Batch Loss: 0.45758774876594543\n",
      "Epoch 5630, Loss: 1.508790671825409, Final Batch Loss: 0.30381956696510315\n",
      "Epoch 5631, Loss: 1.3601422011852264, Final Batch Loss: 0.21816092729568481\n",
      "Epoch 5632, Loss: 1.5052241086959839, Final Batch Loss: 0.26634323596954346\n",
      "Epoch 5633, Loss: 1.4345182031393051, Final Batch Loss: 0.3724609613418579\n",
      "Epoch 5634, Loss: 1.470420554280281, Final Batch Loss: 0.3048150837421417\n",
      "Epoch 5635, Loss: 1.4809104800224304, Final Batch Loss: 0.275044322013855\n",
      "Epoch 5636, Loss: 1.5086629390716553, Final Batch Loss: 0.298486590385437\n",
      "Epoch 5637, Loss: 1.4060519933700562, Final Batch Loss: 0.31192028522491455\n",
      "Epoch 5638, Loss: 1.4769055843353271, Final Batch Loss: 0.2991393208503723\n",
      "Epoch 5639, Loss: 1.3428852260112762, Final Batch Loss: 0.24364617466926575\n",
      "Epoch 5640, Loss: 1.6015498340129852, Final Batch Loss: 0.3888244926929474\n",
      "Epoch 5641, Loss: 1.3184489607810974, Final Batch Loss: 0.25922495126724243\n",
      "Epoch 5642, Loss: 1.258232057094574, Final Batch Loss: 0.20003628730773926\n",
      "Epoch 5643, Loss: 1.511082112789154, Final Batch Loss: 0.426546573638916\n",
      "Epoch 5644, Loss: 1.3659843504428864, Final Batch Loss: 0.2839514911174774\n",
      "Epoch 5645, Loss: 1.4049898386001587, Final Batch Loss: 0.23551326990127563\n",
      "Epoch 5646, Loss: 1.2479089349508286, Final Batch Loss: 0.16472415626049042\n",
      "Epoch 5647, Loss: 1.2936761230230331, Final Batch Loss: 0.24785670638084412\n",
      "Epoch 5648, Loss: 1.312718689441681, Final Batch Loss: 0.2530834674835205\n",
      "Epoch 5649, Loss: 1.4121571779251099, Final Batch Loss: 0.299023300409317\n",
      "Epoch 5650, Loss: 1.3225342333316803, Final Batch Loss: 0.1954939365386963\n",
      "Epoch 5651, Loss: 1.6536810398101807, Final Batch Loss: 0.37999001145362854\n",
      "Epoch 5652, Loss: 1.502770334482193, Final Batch Loss: 0.39890995621681213\n",
      "Epoch 5653, Loss: 1.2803586423397064, Final Batch Loss: 0.21250508725643158\n",
      "Epoch 5654, Loss: 1.2736267000436783, Final Batch Loss: 0.24005617201328278\n",
      "Epoch 5655, Loss: 1.2407264709472656, Final Batch Loss: 0.2065628618001938\n",
      "Epoch 5656, Loss: 1.2554438710212708, Final Batch Loss: 0.20945538580417633\n",
      "Epoch 5657, Loss: 1.4181267023086548, Final Batch Loss: 0.24246983230113983\n",
      "Epoch 5658, Loss: 1.3868647664785385, Final Batch Loss: 0.22047483921051025\n",
      "Epoch 5659, Loss: 1.7692719101905823, Final Batch Loss: 0.4211063086986542\n",
      "Epoch 5660, Loss: 1.4098730385303497, Final Batch Loss: 0.20038765668869019\n",
      "Epoch 5661, Loss: 1.5330310463905334, Final Batch Loss: 0.4019426107406616\n",
      "Epoch 5662, Loss: 1.432128667831421, Final Batch Loss: 0.2904435992240906\n",
      "Epoch 5663, Loss: 1.494459554553032, Final Batch Loss: 0.44620341062545776\n",
      "Epoch 5664, Loss: 1.310383215546608, Final Batch Loss: 0.27598896622657776\n",
      "Epoch 5665, Loss: 1.3449519723653793, Final Batch Loss: 0.13462425768375397\n",
      "Epoch 5666, Loss: 1.3933230638504028, Final Batch Loss: 0.22557923197746277\n",
      "Epoch 5667, Loss: 1.366824135184288, Final Batch Loss: 0.23141314089298248\n",
      "Epoch 5668, Loss: 1.3883026242256165, Final Batch Loss: 0.29254937171936035\n",
      "Epoch 5669, Loss: 1.362719178199768, Final Batch Loss: 0.2686717212200165\n",
      "Epoch 5670, Loss: 1.3897712230682373, Final Batch Loss: 0.2758404016494751\n",
      "Epoch 5671, Loss: 1.866186410188675, Final Batch Loss: 0.5837641358375549\n",
      "Epoch 5672, Loss: 1.4333189725875854, Final Batch Loss: 0.2243417501449585\n",
      "Epoch 5673, Loss: 1.4145210534334183, Final Batch Loss: 0.3017028570175171\n",
      "Epoch 5674, Loss: 1.3842851519584656, Final Batch Loss: 0.2751425504684448\n",
      "Epoch 5675, Loss: 1.391116440296173, Final Batch Loss: 0.2255050390958786\n",
      "Epoch 5676, Loss: 1.3908643126487732, Final Batch Loss: 0.24906718730926514\n",
      "Epoch 5677, Loss: 1.430453211069107, Final Batch Loss: 0.3233979344367981\n",
      "Epoch 5678, Loss: 1.4994371235370636, Final Batch Loss: 0.45913150906562805\n",
      "Epoch 5679, Loss: 1.29922316968441, Final Batch Loss: 0.16994738578796387\n",
      "Epoch 5680, Loss: 1.2893507182598114, Final Batch Loss: 0.17177265882492065\n",
      "Epoch 5681, Loss: 1.1980223655700684, Final Batch Loss: 0.20146974921226501\n",
      "Epoch 5682, Loss: 1.484542652964592, Final Batch Loss: 0.30263134837150574\n",
      "Epoch 5683, Loss: 1.3253324925899506, Final Batch Loss: 0.2790796458721161\n",
      "Epoch 5684, Loss: 1.298919066786766, Final Batch Loss: 0.2021544873714447\n",
      "Epoch 5685, Loss: 1.6085074245929718, Final Batch Loss: 0.28527456521987915\n",
      "Epoch 5686, Loss: 1.4557615965604782, Final Batch Loss: 0.3658263385295868\n",
      "Epoch 5687, Loss: 1.6399226188659668, Final Batch Loss: 0.3832213580608368\n",
      "Epoch 5688, Loss: 1.4268554896116257, Final Batch Loss: 0.3157057464122772\n",
      "Epoch 5689, Loss: 1.698949158191681, Final Batch Loss: 0.38747474551200867\n",
      "Epoch 5690, Loss: 1.4810053706169128, Final Batch Loss: 0.22500362992286682\n",
      "Epoch 5691, Loss: 1.5511208474636078, Final Batch Loss: 0.2849788963794708\n",
      "Epoch 5692, Loss: 1.6732797026634216, Final Batch Loss: 0.37790408730506897\n",
      "Epoch 5693, Loss: 1.5708076059818268, Final Batch Loss: 0.3344654142856598\n",
      "Epoch 5694, Loss: 1.468602493405342, Final Batch Loss: 0.16279615461826324\n",
      "Epoch 5695, Loss: 1.5116796046495438, Final Batch Loss: 0.3251379728317261\n",
      "Epoch 5696, Loss: 1.4345291256904602, Final Batch Loss: 0.31912460923194885\n",
      "Epoch 5697, Loss: 1.5159087181091309, Final Batch Loss: 0.30984553694725037\n",
      "Epoch 5698, Loss: 1.3629605025053024, Final Batch Loss: 0.2402210235595703\n",
      "Epoch 5699, Loss: 1.4040634036064148, Final Batch Loss: 0.2930702865123749\n",
      "Epoch 5700, Loss: 1.3895128816366196, Final Batch Loss: 0.19462324678897858\n",
      "Epoch 5701, Loss: 1.2317573577165604, Final Batch Loss: 0.3179379105567932\n",
      "Epoch 5702, Loss: 1.3297640308737755, Final Batch Loss: 0.09483005851507187\n",
      "Epoch 5703, Loss: 1.4665738940238953, Final Batch Loss: 0.2788619101047516\n",
      "Epoch 5704, Loss: 1.4530415683984756, Final Batch Loss: 0.44586408138275146\n",
      "Epoch 5705, Loss: 1.3663816154003143, Final Batch Loss: 0.2954314947128296\n",
      "Epoch 5706, Loss: 1.4815733730793, Final Batch Loss: 0.39996230602264404\n",
      "Epoch 5707, Loss: 1.3043275028467178, Final Batch Loss: 0.27134621143341064\n",
      "Epoch 5708, Loss: 1.4070741087198257, Final Batch Loss: 0.29129526019096375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5709, Loss: 1.3761794716119766, Final Batch Loss: 0.17427687346935272\n",
      "Epoch 5710, Loss: 1.3245114982128143, Final Batch Loss: 0.24620571732521057\n",
      "Epoch 5711, Loss: 1.2306678742170334, Final Batch Loss: 0.20111539959907532\n",
      "Epoch 5712, Loss: 1.3921289294958115, Final Batch Loss: 0.31916365027427673\n",
      "Epoch 5713, Loss: 1.3400894552469254, Final Batch Loss: 0.17596425116062164\n",
      "Epoch 5714, Loss: 1.6047888696193695, Final Batch Loss: 0.291830837726593\n",
      "Epoch 5715, Loss: 1.2704188376665115, Final Batch Loss: 0.26978394389152527\n",
      "Epoch 5716, Loss: 1.6150873005390167, Final Batch Loss: 0.3286864459514618\n",
      "Epoch 5717, Loss: 1.5151809453964233, Final Batch Loss: 0.3362213373184204\n",
      "Epoch 5718, Loss: 1.379980280995369, Final Batch Loss: 0.19364863634109497\n",
      "Epoch 5719, Loss: 1.3523407727479935, Final Batch Loss: 0.2284163385629654\n",
      "Epoch 5720, Loss: 1.335809201002121, Final Batch Loss: 0.2219323068857193\n",
      "Epoch 5721, Loss: 1.5847169756889343, Final Batch Loss: 0.47474542260169983\n",
      "Epoch 5722, Loss: 1.6990874111652374, Final Batch Loss: 0.4763486385345459\n",
      "Epoch 5723, Loss: 1.4889467358589172, Final Batch Loss: 0.31725066900253296\n",
      "Epoch 5724, Loss: 1.3868447840213776, Final Batch Loss: 0.17736688256263733\n",
      "Epoch 5725, Loss: 1.5258007645606995, Final Batch Loss: 0.24510687589645386\n",
      "Epoch 5726, Loss: 1.4891098141670227, Final Batch Loss: 0.3138158321380615\n",
      "Epoch 5727, Loss: 1.3389277756214142, Final Batch Loss: 0.22584082186222076\n",
      "Epoch 5728, Loss: 1.5171178877353668, Final Batch Loss: 0.2714543640613556\n",
      "Epoch 5729, Loss: 1.3889032900333405, Final Batch Loss: 0.3067471981048584\n",
      "Epoch 5730, Loss: 1.461075246334076, Final Batch Loss: 0.1805344521999359\n",
      "Epoch 5731, Loss: 1.5935922116041183, Final Batch Loss: 0.25645026564598083\n",
      "Epoch 5732, Loss: 1.6837554574012756, Final Batch Loss: 0.3784771263599396\n",
      "Epoch 5733, Loss: 1.4391738921403885, Final Batch Loss: 0.3082233667373657\n",
      "Epoch 5734, Loss: 1.4960273206233978, Final Batch Loss: 0.3272086977958679\n",
      "Epoch 5735, Loss: 1.462559089064598, Final Batch Loss: 0.31927910447120667\n",
      "Epoch 5736, Loss: 1.5700961649417877, Final Batch Loss: 0.2518063485622406\n",
      "Epoch 5737, Loss: 1.5211073458194733, Final Batch Loss: 0.4021955132484436\n",
      "Epoch 5738, Loss: 1.5597502440214157, Final Batch Loss: 0.13592977821826935\n",
      "Epoch 5739, Loss: 1.4456026703119278, Final Batch Loss: 0.14986573159694672\n",
      "Epoch 5740, Loss: 1.3912760019302368, Final Batch Loss: 0.273952841758728\n",
      "Epoch 5741, Loss: 1.3596218675374985, Final Batch Loss: 0.21376048028469086\n",
      "Epoch 5742, Loss: 1.4422201216220856, Final Batch Loss: 0.27797016501426697\n",
      "Epoch 5743, Loss: 1.377200335264206, Final Batch Loss: 0.2088429033756256\n",
      "Epoch 5744, Loss: 1.2770225703716278, Final Batch Loss: 0.1988954246044159\n",
      "Epoch 5745, Loss: 1.4128726124763489, Final Batch Loss: 0.3348742723464966\n",
      "Epoch 5746, Loss: 1.4693579226732254, Final Batch Loss: 0.24968387186527252\n",
      "Epoch 5747, Loss: 1.4374154210090637, Final Batch Loss: 0.4156528413295746\n",
      "Epoch 5748, Loss: 1.343847319483757, Final Batch Loss: 0.2573007345199585\n",
      "Epoch 5749, Loss: 1.4121113717556, Final Batch Loss: 0.27599474787712097\n",
      "Epoch 5750, Loss: 1.4545805007219315, Final Batch Loss: 0.244698166847229\n",
      "Epoch 5751, Loss: 1.3482789993286133, Final Batch Loss: 0.2698260247707367\n",
      "Epoch 5752, Loss: 1.4037338644266129, Final Batch Loss: 0.271393746137619\n",
      "Epoch 5753, Loss: 1.4489997923374176, Final Batch Loss: 0.35216933488845825\n",
      "Epoch 5754, Loss: 1.5138790607452393, Final Batch Loss: 0.23734638094902039\n",
      "Epoch 5755, Loss: 1.5009761154651642, Final Batch Loss: 0.4217252731323242\n",
      "Epoch 5756, Loss: 1.216086521744728, Final Batch Loss: 0.10492582619190216\n",
      "Epoch 5757, Loss: 1.4436458498239517, Final Batch Loss: 0.33826354146003723\n",
      "Epoch 5758, Loss: 1.525389552116394, Final Batch Loss: 0.22001734375953674\n",
      "Epoch 5759, Loss: 1.3718870282173157, Final Batch Loss: 0.3068356513977051\n",
      "Epoch 5760, Loss: 1.4722330570220947, Final Batch Loss: 0.30037301778793335\n",
      "Epoch 5761, Loss: 1.4431205689907074, Final Batch Loss: 0.2459057718515396\n",
      "Epoch 5762, Loss: 1.3969784826040268, Final Batch Loss: 0.18972444534301758\n",
      "Epoch 5763, Loss: 1.3118298947811127, Final Batch Loss: 0.21997492015361786\n",
      "Epoch 5764, Loss: 1.239090919494629, Final Batch Loss: 0.20691728591918945\n",
      "Epoch 5765, Loss: 1.366780012845993, Final Batch Loss: 0.25767841935157776\n",
      "Epoch 5766, Loss: 1.5468986630439758, Final Batch Loss: 0.38212746381759644\n",
      "Epoch 5767, Loss: 1.222847819328308, Final Batch Loss: 0.2071877270936966\n",
      "Epoch 5768, Loss: 1.4461400508880615, Final Batch Loss: 0.3664954602718353\n",
      "Epoch 5769, Loss: 1.3220010846853256, Final Batch Loss: 0.2934177815914154\n",
      "Epoch 5770, Loss: 1.5678434669971466, Final Batch Loss: 0.3486667275428772\n",
      "Epoch 5771, Loss: 1.6258231103420258, Final Batch Loss: 0.2627406418323517\n",
      "Epoch 5772, Loss: 1.5027559697628021, Final Batch Loss: 0.3169296383857727\n",
      "Epoch 5773, Loss: 1.5094222873449326, Final Batch Loss: 0.343532919883728\n",
      "Epoch 5774, Loss: 1.495627537369728, Final Batch Loss: 0.29967695474624634\n",
      "Epoch 5775, Loss: 1.651745766401291, Final Batch Loss: 0.42835888266563416\n",
      "Epoch 5776, Loss: 1.3550497889518738, Final Batch Loss: 0.2670089602470398\n",
      "Epoch 5777, Loss: 1.2598264962434769, Final Batch Loss: 0.2804908752441406\n",
      "Epoch 5778, Loss: 1.4736420810222626, Final Batch Loss: 0.3313301205635071\n",
      "Epoch 5779, Loss: 1.2982335984706879, Final Batch Loss: 0.1711006462574005\n",
      "Epoch 5780, Loss: 1.3690689206123352, Final Batch Loss: 0.2516537308692932\n",
      "Epoch 5781, Loss: 1.2694188058376312, Final Batch Loss: 0.1794942021369934\n",
      "Epoch 5782, Loss: 1.6185597777366638, Final Batch Loss: 0.45048171281814575\n",
      "Epoch 5783, Loss: 1.4036619514226913, Final Batch Loss: 0.20401136577129364\n",
      "Epoch 5784, Loss: 1.3707990646362305, Final Batch Loss: 0.3524128198623657\n",
      "Epoch 5785, Loss: 1.596795842051506, Final Batch Loss: 0.2553212642669678\n",
      "Epoch 5786, Loss: 1.4462847113609314, Final Batch Loss: 0.214388906955719\n",
      "Epoch 5787, Loss: 1.3586709052324295, Final Batch Loss: 0.1856740415096283\n",
      "Epoch 5788, Loss: 1.2552736401557922, Final Batch Loss: 0.1642473191022873\n",
      "Epoch 5789, Loss: 1.2463766932487488, Final Batch Loss: 0.21799327433109283\n",
      "Epoch 5790, Loss: 1.2458230555057526, Final Batch Loss: 0.12802958488464355\n",
      "Epoch 5791, Loss: 1.5972872376441956, Final Batch Loss: 0.47727951407432556\n",
      "Epoch 5792, Loss: 1.3549475222826004, Final Batch Loss: 0.19538414478302002\n",
      "Epoch 5793, Loss: 1.713842511177063, Final Batch Loss: 0.5067141652107239\n",
      "Epoch 5794, Loss: 1.479252353310585, Final Batch Loss: 0.3040926158428192\n",
      "Epoch 5795, Loss: 1.2461073100566864, Final Batch Loss: 0.1526259332895279\n",
      "Epoch 5796, Loss: 1.3103794306516647, Final Batch Loss: 0.22843672335147858\n",
      "Epoch 5797, Loss: 1.277458295226097, Final Batch Loss: 0.2875835597515106\n",
      "Epoch 5798, Loss: 1.4162769466638565, Final Batch Loss: 0.281842976808548\n",
      "Epoch 5799, Loss: 1.5094092041254044, Final Batch Loss: 0.3116304278373718\n",
      "Epoch 5800, Loss: 1.3737185299396515, Final Batch Loss: 0.24764150381088257\n",
      "Epoch 5801, Loss: 1.4241371899843216, Final Batch Loss: 0.2471102923154831\n",
      "Epoch 5802, Loss: 1.4254276901483536, Final Batch Loss: 0.3966684341430664\n",
      "Epoch 5803, Loss: 1.473626509308815, Final Batch Loss: 0.3882572054862976\n",
      "Epoch 5804, Loss: 1.6696047186851501, Final Batch Loss: 0.2831941246986389\n",
      "Epoch 5805, Loss: 1.4296583831310272, Final Batch Loss: 0.2874692678451538\n",
      "Epoch 5806, Loss: 1.2459262162446976, Final Batch Loss: 0.17841482162475586\n",
      "Epoch 5807, Loss: 1.9146819412708282, Final Batch Loss: 0.7100578546524048\n",
      "Epoch 5808, Loss: 1.3898039758205414, Final Batch Loss: 0.23899675905704498\n",
      "Epoch 5809, Loss: 1.3769170343875885, Final Batch Loss: 0.3671222925186157\n",
      "Epoch 5810, Loss: 1.375754252076149, Final Batch Loss: 0.18796266615390778\n",
      "Epoch 5811, Loss: 1.3936124444007874, Final Batch Loss: 0.137353777885437\n",
      "Epoch 5812, Loss: 1.4622197151184082, Final Batch Loss: 0.34672173857688904\n",
      "Epoch 5813, Loss: 1.3504090309143066, Final Batch Loss: 0.21214421093463898\n",
      "Epoch 5814, Loss: 1.2855582535266876, Final Batch Loss: 0.18099640309810638\n",
      "Epoch 5815, Loss: 1.3357227444648743, Final Batch Loss: 0.17062653601169586\n",
      "Epoch 5816, Loss: 1.351772204041481, Final Batch Loss: 0.40525582432746887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5817, Loss: 1.5704505145549774, Final Batch Loss: 0.17942455410957336\n",
      "Epoch 5818, Loss: 1.5865199863910675, Final Batch Loss: 0.6249709725379944\n",
      "Epoch 5819, Loss: 1.593123346567154, Final Batch Loss: 0.2675144672393799\n",
      "Epoch 5820, Loss: 1.5226119309663773, Final Batch Loss: 0.22133393585681915\n",
      "Epoch 5821, Loss: 1.5998584926128387, Final Batch Loss: 0.4912101924419403\n",
      "Epoch 5822, Loss: 1.6967447102069855, Final Batch Loss: 0.39133960008621216\n",
      "Epoch 5823, Loss: 1.8524826169013977, Final Batch Loss: 0.5409079194068909\n",
      "Epoch 5824, Loss: 1.6324380934238434, Final Batch Loss: 0.3330587148666382\n",
      "Epoch 5825, Loss: 1.6208010613918304, Final Batch Loss: 0.4403141140937805\n",
      "Epoch 5826, Loss: 1.3534071743488312, Final Batch Loss: 0.198378324508667\n",
      "Epoch 5827, Loss: 1.6366715729236603, Final Batch Loss: 0.4351852834224701\n",
      "Epoch 5828, Loss: 1.4200907796621323, Final Batch Loss: 0.2858092486858368\n",
      "Epoch 5829, Loss: 1.3828513026237488, Final Batch Loss: 0.26739922165870667\n",
      "Epoch 5830, Loss: 1.7462511956691742, Final Batch Loss: 0.553550660610199\n",
      "Epoch 5831, Loss: 1.4821168184280396, Final Batch Loss: 0.2949199676513672\n",
      "Epoch 5832, Loss: 1.3431891202926636, Final Batch Loss: 0.23874637484550476\n",
      "Epoch 5833, Loss: 1.3949536681175232, Final Batch Loss: 0.3493921756744385\n",
      "Epoch 5834, Loss: 1.3835728913545609, Final Batch Loss: 0.262434720993042\n",
      "Epoch 5835, Loss: 1.2709980309009552, Final Batch Loss: 0.22014805674552917\n",
      "Epoch 5836, Loss: 1.550158977508545, Final Batch Loss: 0.35690778493881226\n",
      "Epoch 5837, Loss: 1.6129904687404633, Final Batch Loss: 0.2611779272556305\n",
      "Epoch 5838, Loss: 1.316252425312996, Final Batch Loss: 0.16427113115787506\n",
      "Epoch 5839, Loss: 1.6507101953029633, Final Batch Loss: 0.318596214056015\n",
      "Epoch 5840, Loss: 1.4410007745027542, Final Batch Loss: 0.17324040830135345\n",
      "Epoch 5841, Loss: 1.4192257523536682, Final Batch Loss: 0.3352374732494354\n",
      "Epoch 5842, Loss: 1.5482092201709747, Final Batch Loss: 0.25445178151130676\n",
      "Epoch 5843, Loss: 1.3764949291944504, Final Batch Loss: 0.24495245516300201\n",
      "Epoch 5844, Loss: 1.346605896949768, Final Batch Loss: 0.23223312199115753\n",
      "Epoch 5845, Loss: 1.26321642100811, Final Batch Loss: 0.23528622090816498\n",
      "Epoch 5846, Loss: 1.4320465922355652, Final Batch Loss: 0.2516463100910187\n",
      "Epoch 5847, Loss: 1.499500349164009, Final Batch Loss: 0.29682424664497375\n",
      "Epoch 5848, Loss: 1.500209391117096, Final Batch Loss: 0.27498626708984375\n",
      "Epoch 5849, Loss: 1.6356167644262314, Final Batch Loss: 0.37510669231414795\n",
      "Epoch 5850, Loss: 1.188842460513115, Final Batch Loss: 0.19967713952064514\n",
      "Epoch 5851, Loss: 1.4119919389486313, Final Batch Loss: 0.21722100675106049\n",
      "Epoch 5852, Loss: 1.189304918050766, Final Batch Loss: 0.14427247643470764\n",
      "Epoch 5853, Loss: 1.4228775054216385, Final Batch Loss: 0.3138076364994049\n",
      "Epoch 5854, Loss: 1.2570768296718597, Final Batch Loss: 0.18185937404632568\n",
      "Epoch 5855, Loss: 1.4538564383983612, Final Batch Loss: 0.20967359840869904\n",
      "Epoch 5856, Loss: 1.3456453084945679, Final Batch Loss: 0.3211599290370941\n",
      "Epoch 5857, Loss: 1.226609781384468, Final Batch Loss: 0.1793859452009201\n",
      "Epoch 5858, Loss: 1.629934847354889, Final Batch Loss: 0.3510436415672302\n",
      "Epoch 5859, Loss: 1.298292264342308, Final Batch Loss: 0.30062082409858704\n",
      "Epoch 5860, Loss: 1.3874139487743378, Final Batch Loss: 0.2711845338344574\n",
      "Epoch 5861, Loss: 1.2500164955854416, Final Batch Loss: 0.12448929250240326\n",
      "Epoch 5862, Loss: 1.385245069861412, Final Batch Loss: 0.1852223128080368\n",
      "Epoch 5863, Loss: 1.312078595161438, Final Batch Loss: 0.23552560806274414\n",
      "Epoch 5864, Loss: 1.3766558766365051, Final Batch Loss: 0.32448527216911316\n",
      "Epoch 5865, Loss: 1.3177047073841095, Final Batch Loss: 0.2750696837902069\n",
      "Epoch 5866, Loss: 1.4993542581796646, Final Batch Loss: 0.3361196517944336\n",
      "Epoch 5867, Loss: 1.314725324511528, Final Batch Loss: 0.1619851440191269\n",
      "Epoch 5868, Loss: 1.3405148088932037, Final Batch Loss: 0.29862797260284424\n",
      "Epoch 5869, Loss: 1.26701520383358, Final Batch Loss: 0.19679108262062073\n",
      "Epoch 5870, Loss: 1.3268679976463318, Final Batch Loss: 0.21415475010871887\n",
      "Epoch 5871, Loss: 1.2728063315153122, Final Batch Loss: 0.2855391502380371\n",
      "Epoch 5872, Loss: 1.3597896695137024, Final Batch Loss: 0.2563404440879822\n",
      "Epoch 5873, Loss: 1.5404033064842224, Final Batch Loss: 0.3087581396102905\n",
      "Epoch 5874, Loss: 1.488337978720665, Final Batch Loss: 0.3418406546115875\n",
      "Epoch 5875, Loss: 1.5463050156831741, Final Batch Loss: 0.41529160737991333\n",
      "Epoch 5876, Loss: 1.3378820568323135, Final Batch Loss: 0.23129664361476898\n",
      "Epoch 5877, Loss: 1.3630067259073257, Final Batch Loss: 0.3263721168041229\n",
      "Epoch 5878, Loss: 1.1951695084571838, Final Batch Loss: 0.1647845208644867\n",
      "Epoch 5879, Loss: 1.5944246053695679, Final Batch Loss: 0.2821821868419647\n",
      "Epoch 5880, Loss: 1.4078658521175385, Final Batch Loss: 0.283654123544693\n",
      "Epoch 5881, Loss: 1.494961142539978, Final Batch Loss: 0.3064599633216858\n",
      "Epoch 5882, Loss: 1.5071550011634827, Final Batch Loss: 0.22297152876853943\n",
      "Epoch 5883, Loss: 1.4260333180427551, Final Batch Loss: 0.2797852158546448\n",
      "Epoch 5884, Loss: 1.4600597470998764, Final Batch Loss: 0.42831510305404663\n",
      "Epoch 5885, Loss: 1.3807359635829926, Final Batch Loss: 0.2409924864768982\n",
      "Epoch 5886, Loss: 1.5997711718082428, Final Batch Loss: 0.15223312377929688\n",
      "Epoch 5887, Loss: 1.349683940410614, Final Batch Loss: 0.3008449971675873\n",
      "Epoch 5888, Loss: 1.4581243097782135, Final Batch Loss: 0.31536319851875305\n",
      "Epoch 5889, Loss: 1.4523785412311554, Final Batch Loss: 0.3046657145023346\n",
      "Epoch 5890, Loss: 1.6861111521720886, Final Batch Loss: 0.3988659977912903\n",
      "Epoch 5891, Loss: 1.2774042934179306, Final Batch Loss: 0.17580878734588623\n",
      "Epoch 5892, Loss: 1.3184654712677002, Final Batch Loss: 0.27348679304122925\n",
      "Epoch 5893, Loss: 1.5097580552101135, Final Batch Loss: 0.3381507098674774\n",
      "Epoch 5894, Loss: 1.4367398768663406, Final Batch Loss: 0.36010923981666565\n",
      "Epoch 5895, Loss: 1.3016068190336227, Final Batch Loss: 0.20923969149589539\n",
      "Epoch 5896, Loss: 1.5475728809833527, Final Batch Loss: 0.34034815430641174\n",
      "Epoch 5897, Loss: 1.4216312617063522, Final Batch Loss: 0.25641411542892456\n",
      "Epoch 5898, Loss: 1.543252870440483, Final Batch Loss: 0.4564720392227173\n",
      "Epoch 5899, Loss: 1.5243898332118988, Final Batch Loss: 0.3766222894191742\n",
      "Epoch 5900, Loss: 1.3477147221565247, Final Batch Loss: 0.2339845597743988\n",
      "Epoch 5901, Loss: 1.471383973956108, Final Batch Loss: 0.22381483018398285\n",
      "Epoch 5902, Loss: 1.3893349468708038, Final Batch Loss: 0.3515004515647888\n",
      "Epoch 5903, Loss: 1.6249989867210388, Final Batch Loss: 0.3731801509857178\n",
      "Epoch 5904, Loss: 1.5371885299682617, Final Batch Loss: 0.44609180092811584\n",
      "Epoch 5905, Loss: 1.4611133635044098, Final Batch Loss: 0.34417515993118286\n",
      "Epoch 5906, Loss: 1.3738328516483307, Final Batch Loss: 0.3626483678817749\n",
      "Epoch 5907, Loss: 1.575042262673378, Final Batch Loss: 0.3107330799102783\n",
      "Epoch 5908, Loss: 1.3278032541275024, Final Batch Loss: 0.24405673146247864\n",
      "Epoch 5909, Loss: 1.6361856907606125, Final Batch Loss: 0.4301359951496124\n",
      "Epoch 5910, Loss: 1.4440569877624512, Final Batch Loss: 0.358535498380661\n",
      "Epoch 5911, Loss: 1.4136570245027542, Final Batch Loss: 0.14049996435642242\n",
      "Epoch 5912, Loss: 1.5697137117385864, Final Batch Loss: 0.292512983083725\n",
      "Epoch 5913, Loss: 1.62698894739151, Final Batch Loss: 0.3880726099014282\n",
      "Epoch 5914, Loss: 1.600921392440796, Final Batch Loss: 0.28021183609962463\n",
      "Epoch 5915, Loss: 1.3199687153100967, Final Batch Loss: 0.1425836682319641\n",
      "Epoch 5916, Loss: 1.500434398651123, Final Batch Loss: 0.20039111375808716\n",
      "Epoch 5917, Loss: 1.3746680468320847, Final Batch Loss: 0.20747952163219452\n",
      "Epoch 5918, Loss: 1.241265743970871, Final Batch Loss: 0.30655568838119507\n",
      "Epoch 5919, Loss: 1.39906607568264, Final Batch Loss: 0.4291107952594757\n",
      "Epoch 5920, Loss: 1.6353359520435333, Final Batch Loss: 0.5661383867263794\n",
      "Epoch 5921, Loss: 1.5619181394577026, Final Batch Loss: 0.2951079308986664\n",
      "Epoch 5922, Loss: 1.3951355069875717, Final Batch Loss: 0.19167742133140564\n",
      "Epoch 5923, Loss: 1.4391903728246689, Final Batch Loss: 0.3121699094772339\n",
      "Epoch 5924, Loss: 1.465451940894127, Final Batch Loss: 0.22559550404548645\n",
      "Epoch 5925, Loss: 1.3795819133520126, Final Batch Loss: 0.2697296142578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5926, Loss: 1.5128929764032364, Final Batch Loss: 0.4147549271583557\n",
      "Epoch 5927, Loss: 1.4874266982078552, Final Batch Loss: 0.4508373737335205\n",
      "Epoch 5928, Loss: 1.272181823849678, Final Batch Loss: 0.22617857158184052\n",
      "Epoch 5929, Loss: 1.4526777416467667, Final Batch Loss: 0.407715767621994\n",
      "Epoch 5930, Loss: 1.2909750938415527, Final Batch Loss: 0.3759412169456482\n",
      "Epoch 5931, Loss: 1.3359240293502808, Final Batch Loss: 0.18330112099647522\n",
      "Epoch 5932, Loss: 1.2611530870199203, Final Batch Loss: 0.15475498139858246\n",
      "Epoch 5933, Loss: 1.5586523860692978, Final Batch Loss: 0.4843203127384186\n",
      "Epoch 5934, Loss: 1.4234902113676071, Final Batch Loss: 0.27395519614219666\n",
      "Epoch 5935, Loss: 1.5720462650060654, Final Batch Loss: 0.43301308155059814\n",
      "Epoch 5936, Loss: 1.514095425605774, Final Batch Loss: 0.2969730794429779\n",
      "Epoch 5937, Loss: 1.3932179510593414, Final Batch Loss: 0.29657596349716187\n",
      "Epoch 5938, Loss: 1.4656278491020203, Final Batch Loss: 0.3509598970413208\n",
      "Epoch 5939, Loss: 1.459279090166092, Final Batch Loss: 0.3501254916191101\n",
      "Epoch 5940, Loss: 1.482480525970459, Final Batch Loss: 0.2652469575405121\n",
      "Epoch 5941, Loss: 1.5262359976768494, Final Batch Loss: 0.2623799443244934\n",
      "Epoch 5942, Loss: 1.4062968343496323, Final Batch Loss: 0.2938024401664734\n",
      "Epoch 5943, Loss: 1.4753443002700806, Final Batch Loss: 0.3155380189418793\n",
      "Epoch 5944, Loss: 1.4595395922660828, Final Batch Loss: 0.3570193648338318\n",
      "Epoch 5945, Loss: 1.569706454873085, Final Batch Loss: 0.3223421275615692\n",
      "Epoch 5946, Loss: 1.445465698838234, Final Batch Loss: 0.4058668315410614\n",
      "Epoch 5947, Loss: 1.3611594587564468, Final Batch Loss: 0.2373567521572113\n",
      "Epoch 5948, Loss: 1.4854398518800735, Final Batch Loss: 0.3357051610946655\n",
      "Epoch 5949, Loss: 1.3801667392253876, Final Batch Loss: 0.33916905522346497\n",
      "Epoch 5950, Loss: 1.3160711228847504, Final Batch Loss: 0.22093039751052856\n",
      "Epoch 5951, Loss: 1.2450106590986252, Final Batch Loss: 0.22488507628440857\n",
      "Epoch 5952, Loss: 1.3743310123682022, Final Batch Loss: 0.3208644986152649\n",
      "Epoch 5953, Loss: 1.5145141035318375, Final Batch Loss: 0.3387886881828308\n",
      "Epoch 5954, Loss: 1.3652740269899368, Final Batch Loss: 0.22164857387542725\n",
      "Epoch 5955, Loss: 1.2228926122188568, Final Batch Loss: 0.1968349665403366\n",
      "Epoch 5956, Loss: 1.4735616892576218, Final Batch Loss: 0.2360038310289383\n",
      "Epoch 5957, Loss: 1.3845451474189758, Final Batch Loss: 0.22443905472755432\n",
      "Epoch 5958, Loss: 1.544765517115593, Final Batch Loss: 0.32063785195350647\n",
      "Epoch 5959, Loss: 1.395422488451004, Final Batch Loss: 0.1949600875377655\n",
      "Epoch 5960, Loss: 1.3247021585702896, Final Batch Loss: 0.18846945464611053\n",
      "Epoch 5961, Loss: 1.399839162826538, Final Batch Loss: 0.3471281826496124\n",
      "Epoch 5962, Loss: 1.439314991235733, Final Batch Loss: 0.2459740787744522\n",
      "Epoch 5963, Loss: 1.4151078462600708, Final Batch Loss: 0.15393920242786407\n",
      "Epoch 5964, Loss: 1.4916874170303345, Final Batch Loss: 0.30855438113212585\n",
      "Epoch 5965, Loss: 1.4316990226507187, Final Batch Loss: 0.24925415217876434\n",
      "Epoch 5966, Loss: 1.5625163167715073, Final Batch Loss: 0.24140721559524536\n",
      "Epoch 5967, Loss: 1.4340035617351532, Final Batch Loss: 0.44394999742507935\n",
      "Epoch 5968, Loss: 1.371801421046257, Final Batch Loss: 0.30556657910346985\n",
      "Epoch 5969, Loss: 1.3335888236761093, Final Batch Loss: 0.33604204654693604\n",
      "Epoch 5970, Loss: 1.551057904958725, Final Batch Loss: 0.48034706711769104\n",
      "Epoch 5971, Loss: 1.3244384974241257, Final Batch Loss: 0.20865021646022797\n",
      "Epoch 5972, Loss: 1.3957111835479736, Final Batch Loss: 0.18117976188659668\n",
      "Epoch 5973, Loss: 1.2028125673532486, Final Batch Loss: 0.16463559865951538\n",
      "Epoch 5974, Loss: 1.3797230422496796, Final Batch Loss: 0.3022630512714386\n",
      "Epoch 5975, Loss: 1.3070113956928253, Final Batch Loss: 0.16110099852085114\n",
      "Epoch 5976, Loss: 1.3624196946620941, Final Batch Loss: 0.2805519700050354\n",
      "Epoch 5977, Loss: 1.2197548300027847, Final Batch Loss: 0.1951354444026947\n",
      "Epoch 5978, Loss: 1.1961489766836166, Final Batch Loss: 0.18544085323810577\n",
      "Epoch 5979, Loss: 1.515080600976944, Final Batch Loss: 0.26754435896873474\n",
      "Epoch 5980, Loss: 1.4469106942415237, Final Batch Loss: 0.3726671040058136\n",
      "Epoch 5981, Loss: 1.4656455367803574, Final Batch Loss: 0.18381033837795258\n",
      "Epoch 5982, Loss: 1.5417711585760117, Final Batch Loss: 0.3801324963569641\n",
      "Epoch 5983, Loss: 1.449402555823326, Final Batch Loss: 0.3698710501194\n",
      "Epoch 5984, Loss: 1.7846816778182983, Final Batch Loss: 0.5676306486129761\n",
      "Epoch 5985, Loss: 1.406451255083084, Final Batch Loss: 0.25341156125068665\n",
      "Epoch 5986, Loss: 1.6035997867584229, Final Batch Loss: 0.4228934347629547\n",
      "Epoch 5987, Loss: 1.4012134075164795, Final Batch Loss: 0.2140618860721588\n",
      "Epoch 5988, Loss: 1.4474826455116272, Final Batch Loss: 0.24666860699653625\n",
      "Epoch 5989, Loss: 1.4105980545282364, Final Batch Loss: 0.25793537497520447\n",
      "Epoch 5990, Loss: 1.4017880856990814, Final Batch Loss: 0.205020934343338\n",
      "Epoch 5991, Loss: 1.4169373512268066, Final Batch Loss: 0.21384333074092865\n",
      "Epoch 5992, Loss: 1.3230951130390167, Final Batch Loss: 0.31999671459198\n",
      "Epoch 5993, Loss: 1.6954143643379211, Final Batch Loss: 0.5478417277336121\n",
      "Epoch 5994, Loss: 1.3053769767284393, Final Batch Loss: 0.21265855431556702\n",
      "Epoch 5995, Loss: 1.2849023640155792, Final Batch Loss: 0.2725090980529785\n",
      "Epoch 5996, Loss: 1.3520804643630981, Final Batch Loss: 0.23487263917922974\n",
      "Epoch 5997, Loss: 1.4309602677822113, Final Batch Loss: 0.3808648884296417\n",
      "Epoch 5998, Loss: 1.5925760865211487, Final Batch Loss: 0.3947436511516571\n",
      "Epoch 5999, Loss: 1.4806165397167206, Final Batch Loss: 0.2522447407245636\n",
      "Epoch 6000, Loss: 1.4395420402288437, Final Batch Loss: 0.17060889303684235\n",
      "Epoch 6001, Loss: 1.4069358110427856, Final Batch Loss: 0.2976178228855133\n",
      "Epoch 6002, Loss: 1.4616699367761612, Final Batch Loss: 0.23799259960651398\n",
      "Epoch 6003, Loss: 1.238706037402153, Final Batch Loss: 0.12623947858810425\n",
      "Epoch 6004, Loss: 1.4799883961677551, Final Batch Loss: 0.3233008086681366\n",
      "Epoch 6005, Loss: 1.2711511701345444, Final Batch Loss: 0.2285981923341751\n",
      "Epoch 6006, Loss: 1.292552724480629, Final Batch Loss: 0.2400745302438736\n",
      "Epoch 6007, Loss: 1.2867132276296616, Final Batch Loss: 0.2609921395778656\n",
      "Epoch 6008, Loss: 1.4122694432735443, Final Batch Loss: 0.2623118758201599\n",
      "Epoch 6009, Loss: 1.2625715285539627, Final Batch Loss: 0.26102277636528015\n",
      "Epoch 6010, Loss: 1.4524608701467514, Final Batch Loss: 0.3575288951396942\n",
      "Epoch 6011, Loss: 1.3218371719121933, Final Batch Loss: 0.15182244777679443\n",
      "Epoch 6012, Loss: 1.4088924378156662, Final Batch Loss: 0.2202780544757843\n",
      "Epoch 6013, Loss: 1.4042847156524658, Final Batch Loss: 0.3185352087020874\n",
      "Epoch 6014, Loss: 1.317564234137535, Final Batch Loss: 0.2533770203590393\n",
      "Epoch 6015, Loss: 1.5347275882959366, Final Batch Loss: 0.3013562262058258\n",
      "Epoch 6016, Loss: 1.5391889959573746, Final Batch Loss: 0.2363370656967163\n",
      "Epoch 6017, Loss: 1.2168222963809967, Final Batch Loss: 0.2467690259218216\n",
      "Epoch 6018, Loss: 1.3268262147903442, Final Batch Loss: 0.25854477286338806\n",
      "Epoch 6019, Loss: 1.2384207397699356, Final Batch Loss: 0.16276724636554718\n",
      "Epoch 6020, Loss: 1.3176697939634323, Final Batch Loss: 0.27515920996665955\n",
      "Epoch 6021, Loss: 1.4781193286180496, Final Batch Loss: 0.454897940158844\n",
      "Epoch 6022, Loss: 1.448868289589882, Final Batch Loss: 0.43371516466140747\n",
      "Epoch 6023, Loss: 1.430752158164978, Final Batch Loss: 0.32588499784469604\n",
      "Epoch 6024, Loss: 1.3729912042617798, Final Batch Loss: 0.27043086290359497\n",
      "Epoch 6025, Loss: 1.2769761234521866, Final Batch Loss: 0.18516428768634796\n",
      "Epoch 6026, Loss: 1.3798711001873016, Final Batch Loss: 0.19028671085834503\n",
      "Epoch 6027, Loss: 1.4456520974636078, Final Batch Loss: 0.3464924395084381\n",
      "Epoch 6028, Loss: 1.3831664323806763, Final Batch Loss: 0.22761860489845276\n",
      "Epoch 6029, Loss: 1.5091672390699387, Final Batch Loss: 0.3269089162349701\n",
      "Epoch 6030, Loss: 1.3022366762161255, Final Batch Loss: 0.17653697729110718\n",
      "Epoch 6031, Loss: 1.4308178424835205, Final Batch Loss: 0.18964305520057678\n",
      "Epoch 6032, Loss: 1.3349746018648148, Final Batch Loss: 0.16305585205554962\n",
      "Epoch 6033, Loss: 1.2869776487350464, Final Batch Loss: 0.16092941164970398\n",
      "Epoch 6034, Loss: 1.3043493330478668, Final Batch Loss: 0.2132231742143631\n",
      "Epoch 6035, Loss: 1.3988282680511475, Final Batch Loss: 0.31892409920692444\n",
      "Epoch 6036, Loss: 1.4584985971450806, Final Batch Loss: 0.27923583984375\n",
      "Epoch 6037, Loss: 1.4034614861011505, Final Batch Loss: 0.17245382070541382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6038, Loss: 1.2108120173215866, Final Batch Loss: 0.17789612710475922\n",
      "Epoch 6039, Loss: 1.4526661187410355, Final Batch Loss: 0.23493437469005585\n",
      "Epoch 6040, Loss: 1.2845874652266502, Final Batch Loss: 0.08491525799036026\n",
      "Epoch 6041, Loss: 1.377425104379654, Final Batch Loss: 0.25128036737442017\n",
      "Epoch 6042, Loss: 1.5154324769973755, Final Batch Loss: 0.3418850898742676\n",
      "Epoch 6043, Loss: 1.349355086684227, Final Batch Loss: 0.2262319028377533\n",
      "Epoch 6044, Loss: 1.3211799263954163, Final Batch Loss: 0.31815972924232483\n",
      "Epoch 6045, Loss: 1.3315072357654572, Final Batch Loss: 0.21181835234165192\n",
      "Epoch 6046, Loss: 1.404837891459465, Final Batch Loss: 0.28340384364128113\n",
      "Epoch 6047, Loss: 1.5242318511009216, Final Batch Loss: 0.2298073172569275\n",
      "Epoch 6048, Loss: 1.458500012755394, Final Batch Loss: 0.3507024943828583\n",
      "Epoch 6049, Loss: 1.369754046201706, Final Batch Loss: 0.25100255012512207\n",
      "Epoch 6050, Loss: 1.357035830616951, Final Batch Loss: 0.3503131568431854\n",
      "Epoch 6051, Loss: 1.3853197246789932, Final Batch Loss: 0.1982966512441635\n",
      "Epoch 6052, Loss: 1.4943967759609222, Final Batch Loss: 0.3340270221233368\n",
      "Epoch 6053, Loss: 1.3478044867515564, Final Batch Loss: 0.24136610329151154\n",
      "Epoch 6054, Loss: 1.665160894393921, Final Batch Loss: 0.4838933050632477\n",
      "Epoch 6055, Loss: 1.3375545889139175, Final Batch Loss: 0.24423277378082275\n",
      "Epoch 6056, Loss: 1.1604703217744827, Final Batch Loss: 0.16757117211818695\n",
      "Epoch 6057, Loss: 1.4864911288022995, Final Batch Loss: 0.36472004652023315\n",
      "Epoch 6058, Loss: 1.2785396426916122, Final Batch Loss: 0.20067943632602692\n",
      "Epoch 6059, Loss: 1.4778849631547928, Final Batch Loss: 0.24519412219524384\n",
      "Epoch 6060, Loss: 1.344930574297905, Final Batch Loss: 0.28221800923347473\n",
      "Epoch 6061, Loss: 1.2635006606578827, Final Batch Loss: 0.1571723222732544\n",
      "Epoch 6062, Loss: 1.3034881055355072, Final Batch Loss: 0.3429618775844574\n",
      "Epoch 6063, Loss: 1.5282847434282303, Final Batch Loss: 0.3561287522315979\n",
      "Epoch 6064, Loss: 1.288984254002571, Final Batch Loss: 0.2960801422595978\n",
      "Epoch 6065, Loss: 1.3113680928945541, Final Batch Loss: 0.20930904150009155\n",
      "Epoch 6066, Loss: 1.250804379582405, Final Batch Loss: 0.22867026925086975\n",
      "Epoch 6067, Loss: 1.30933278799057, Final Batch Loss: 0.21829600632190704\n",
      "Epoch 6068, Loss: 1.357848733663559, Final Batch Loss: 0.20170816779136658\n",
      "Epoch 6069, Loss: 1.617042750120163, Final Batch Loss: 0.42586657404899597\n",
      "Epoch 6070, Loss: 1.4108432084321976, Final Batch Loss: 0.26271888613700867\n",
      "Epoch 6071, Loss: 1.5227610766887665, Final Batch Loss: 0.2558051347732544\n",
      "Epoch 6072, Loss: 1.296187862753868, Final Batch Loss: 0.16512048244476318\n",
      "Epoch 6073, Loss: 1.5541036128997803, Final Batch Loss: 0.29957276582717896\n",
      "Epoch 6074, Loss: 1.5092533081769943, Final Batch Loss: 0.237184077501297\n",
      "Epoch 6075, Loss: 1.513262778520584, Final Batch Loss: 0.3916865289211273\n",
      "Epoch 6076, Loss: 1.2939982563257217, Final Batch Loss: 0.19010913372039795\n",
      "Epoch 6077, Loss: 1.407057762145996, Final Batch Loss: 0.4156460464000702\n",
      "Epoch 6078, Loss: 1.6539736986160278, Final Batch Loss: 0.5473307371139526\n",
      "Epoch 6079, Loss: 1.3858577907085419, Final Batch Loss: 0.2338443100452423\n",
      "Epoch 6080, Loss: 1.4671573787927628, Final Batch Loss: 0.21544431149959564\n",
      "Epoch 6081, Loss: 1.6029064655303955, Final Batch Loss: 0.31658294796943665\n",
      "Epoch 6082, Loss: 1.544889360666275, Final Batch Loss: 0.2845972776412964\n",
      "Epoch 6083, Loss: 1.428094521164894, Final Batch Loss: 0.3371676802635193\n",
      "Epoch 6084, Loss: 1.4545120596885681, Final Batch Loss: 0.18398389220237732\n",
      "Epoch 6085, Loss: 1.3320635706186295, Final Batch Loss: 0.14267557859420776\n",
      "Epoch 6086, Loss: 1.2926424890756607, Final Batch Loss: 0.1698637306690216\n",
      "Epoch 6087, Loss: 1.5755497813224792, Final Batch Loss: 0.2659151256084442\n",
      "Epoch 6088, Loss: 1.3553644716739655, Final Batch Loss: 0.27254346013069153\n",
      "Epoch 6089, Loss: 1.4414783269166946, Final Batch Loss: 0.26535746455192566\n",
      "Epoch 6090, Loss: 1.3465093076229095, Final Batch Loss: 0.28699731826782227\n",
      "Epoch 6091, Loss: 1.338666096329689, Final Batch Loss: 0.22247788310050964\n",
      "Epoch 6092, Loss: 1.359752282500267, Final Batch Loss: 0.16080167889595032\n",
      "Epoch 6093, Loss: 1.3377198278903961, Final Batch Loss: 0.16604161262512207\n",
      "Epoch 6094, Loss: 1.2558507323265076, Final Batch Loss: 0.2738000154495239\n",
      "Epoch 6095, Loss: 1.26191246509552, Final Batch Loss: 0.18307705223560333\n",
      "Epoch 6096, Loss: 1.3222379386425018, Final Batch Loss: 0.2206692397594452\n",
      "Epoch 6097, Loss: 1.489942952990532, Final Batch Loss: 0.37148115038871765\n",
      "Epoch 6098, Loss: 1.3960711508989334, Final Batch Loss: 0.17934070527553558\n",
      "Epoch 6099, Loss: 1.2478137612342834, Final Batch Loss: 0.2149917334318161\n",
      "Epoch 6100, Loss: 1.5399320721626282, Final Batch Loss: 0.3882642388343811\n",
      "Epoch 6101, Loss: 1.331596851348877, Final Batch Loss: 0.3735745847225189\n",
      "Epoch 6102, Loss: 1.3851161897182465, Final Batch Loss: 0.25013384222984314\n",
      "Epoch 6103, Loss: 1.3684159815311432, Final Batch Loss: 0.2556009292602539\n",
      "Epoch 6104, Loss: 1.3233489841222763, Final Batch Loss: 0.15273407101631165\n",
      "Epoch 6105, Loss: 1.3356627076864243, Final Batch Loss: 0.391225665807724\n",
      "Epoch 6106, Loss: 1.4619233459234238, Final Batch Loss: 0.3066849112510681\n",
      "Epoch 6107, Loss: 1.4050042033195496, Final Batch Loss: 0.1747136116027832\n",
      "Epoch 6108, Loss: 1.4082915782928467, Final Batch Loss: 0.26257091760635376\n",
      "Epoch 6109, Loss: 1.548527404665947, Final Batch Loss: 0.3087100386619568\n",
      "Epoch 6110, Loss: 1.6440144181251526, Final Batch Loss: 0.4832865297794342\n",
      "Epoch 6111, Loss: 1.3068358600139618, Final Batch Loss: 0.21533823013305664\n",
      "Epoch 6112, Loss: 1.3814212679862976, Final Batch Loss: 0.3089795410633087\n",
      "Epoch 6113, Loss: 1.5728942304849625, Final Batch Loss: 0.4617999196052551\n",
      "Epoch 6114, Loss: 1.401535525918007, Final Batch Loss: 0.25982365012168884\n",
      "Epoch 6115, Loss: 1.4634888768196106, Final Batch Loss: 0.2346668541431427\n",
      "Epoch 6116, Loss: 1.737017273902893, Final Batch Loss: 0.27213314175605774\n",
      "Epoch 6117, Loss: 1.5140103548765182, Final Batch Loss: 0.3359512686729431\n",
      "Epoch 6118, Loss: 1.5711119323968887, Final Batch Loss: 0.3478925824165344\n",
      "Epoch 6119, Loss: 1.6293185651302338, Final Batch Loss: 0.36048126220703125\n",
      "Epoch 6120, Loss: 1.566518872976303, Final Batch Loss: 0.3087445795536041\n",
      "Epoch 6121, Loss: 1.905931368470192, Final Batch Loss: 0.23546884953975677\n",
      "Epoch 6122, Loss: 1.4198510944843292, Final Batch Loss: 0.36176279187202454\n",
      "Epoch 6123, Loss: 1.3563528954982758, Final Batch Loss: 0.3673793077468872\n",
      "Epoch 6124, Loss: 1.5121707767248154, Final Batch Loss: 0.3654228746891022\n",
      "Epoch 6125, Loss: 1.4336022585630417, Final Batch Loss: 0.3091963231563568\n",
      "Epoch 6126, Loss: 1.4241745471954346, Final Batch Loss: 0.2522742748260498\n",
      "Epoch 6127, Loss: 1.364351436495781, Final Batch Loss: 0.2513521909713745\n",
      "Epoch 6128, Loss: 1.496128886938095, Final Batch Loss: 0.46707475185394287\n",
      "Epoch 6129, Loss: 1.3077241033315659, Final Batch Loss: 0.2610936164855957\n",
      "Epoch 6130, Loss: 1.3265310376882553, Final Batch Loss: 0.19798888266086578\n",
      "Epoch 6131, Loss: 1.440861701965332, Final Batch Loss: 0.3362484872341156\n",
      "Epoch 6132, Loss: 1.3825314939022064, Final Batch Loss: 0.18194681406021118\n",
      "Epoch 6133, Loss: 1.6076088547706604, Final Batch Loss: 0.4385331869125366\n",
      "Epoch 6134, Loss: 1.5389223545789719, Final Batch Loss: 0.42559221386909485\n",
      "Epoch 6135, Loss: 1.300079584121704, Final Batch Loss: 0.19070696830749512\n",
      "Epoch 6136, Loss: 1.5615949481725693, Final Batch Loss: 0.45964401960372925\n",
      "Epoch 6137, Loss: 1.3367701768875122, Final Batch Loss: 0.24788302183151245\n",
      "Epoch 6138, Loss: 1.3683389276266098, Final Batch Loss: 0.2988566756248474\n",
      "Epoch 6139, Loss: 1.5426876842975616, Final Batch Loss: 0.2569875121116638\n",
      "Epoch 6140, Loss: 1.3276984840631485, Final Batch Loss: 0.3167480230331421\n",
      "Epoch 6141, Loss: 1.4413558840751648, Final Batch Loss: 0.2519162893295288\n",
      "Epoch 6142, Loss: 1.2470017671585083, Final Batch Loss: 0.20311175286769867\n",
      "Epoch 6143, Loss: 1.3991173952817917, Final Batch Loss: 0.33391571044921875\n",
      "Epoch 6144, Loss: 1.6437932848930359, Final Batch Loss: 0.3267170190811157\n",
      "Epoch 6145, Loss: 1.4423489570617676, Final Batch Loss: 0.3050679564476013\n",
      "Epoch 6146, Loss: 1.4583898037672043, Final Batch Loss: 0.4189855456352234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6147, Loss: 1.3736260533332825, Final Batch Loss: 0.22697721421718597\n",
      "Epoch 6148, Loss: 1.4803697317838669, Final Batch Loss: 0.2895043194293976\n",
      "Epoch 6149, Loss: 1.3357798755168915, Final Batch Loss: 0.2708026170730591\n",
      "Epoch 6150, Loss: 1.3738523572683334, Final Batch Loss: 0.3261411786079407\n",
      "Epoch 6151, Loss: 1.4530533701181412, Final Batch Loss: 0.2287602424621582\n",
      "Epoch 6152, Loss: 1.5466500520706177, Final Batch Loss: 0.4414973556995392\n",
      "Epoch 6153, Loss: 1.3534248769283295, Final Batch Loss: 0.27662724256515503\n",
      "Epoch 6154, Loss: 1.3175467401742935, Final Batch Loss: 0.2400200068950653\n",
      "Epoch 6155, Loss: 1.4385484606027603, Final Batch Loss: 0.32879987359046936\n",
      "Epoch 6156, Loss: 1.4294295758008957, Final Batch Loss: 0.26148104667663574\n",
      "Epoch 6157, Loss: 1.4810653030872345, Final Batch Loss: 0.35354453325271606\n",
      "Epoch 6158, Loss: 1.3974117040634155, Final Batch Loss: 0.3265668749809265\n",
      "Epoch 6159, Loss: 1.476276934146881, Final Batch Loss: 0.38503918051719666\n",
      "Epoch 6160, Loss: 1.4320666193962097, Final Batch Loss: 0.19954857230186462\n",
      "Epoch 6161, Loss: 1.3519928753376007, Final Batch Loss: 0.22617438435554504\n",
      "Epoch 6162, Loss: 1.2798952013254166, Final Batch Loss: 0.19210664927959442\n",
      "Epoch 6163, Loss: 1.4389233738183975, Final Batch Loss: 0.34730175137519836\n",
      "Epoch 6164, Loss: 1.6324457228183746, Final Batch Loss: 0.2758869230747223\n",
      "Epoch 6165, Loss: 1.51949542760849, Final Batch Loss: 0.35929587483406067\n",
      "Epoch 6166, Loss: 1.174803465604782, Final Batch Loss: 0.10988578200340271\n",
      "Epoch 6167, Loss: 1.208839237689972, Final Batch Loss: 0.2600526809692383\n",
      "Epoch 6168, Loss: 1.5840437412261963, Final Batch Loss: 0.3227802515029907\n",
      "Epoch 6169, Loss: 1.359106868505478, Final Batch Loss: 0.3238442838191986\n",
      "Epoch 6170, Loss: 1.5002980530261993, Final Batch Loss: 0.23715311288833618\n",
      "Epoch 6171, Loss: 1.2833174765110016, Final Batch Loss: 0.18486805260181427\n",
      "Epoch 6172, Loss: 1.3332549631595612, Final Batch Loss: 0.27723610401153564\n",
      "Epoch 6173, Loss: 1.34410060942173, Final Batch Loss: 0.2760877311229706\n",
      "Epoch 6174, Loss: 1.4764326214790344, Final Batch Loss: 0.3312249481678009\n",
      "Epoch 6175, Loss: 1.3374531418085098, Final Batch Loss: 0.19930647313594818\n",
      "Epoch 6176, Loss: 1.3427935540676117, Final Batch Loss: 0.2428930401802063\n",
      "Epoch 6177, Loss: 1.508322298526764, Final Batch Loss: 0.3727467656135559\n",
      "Epoch 6178, Loss: 1.2485584616661072, Final Batch Loss: 0.2888948321342468\n",
      "Epoch 6179, Loss: 1.4743046015501022, Final Batch Loss: 0.2218802273273468\n",
      "Epoch 6180, Loss: 1.145947813987732, Final Batch Loss: 0.18638716638088226\n",
      "Epoch 6181, Loss: 1.1727834641933441, Final Batch Loss: 0.14536677300930023\n",
      "Epoch 6182, Loss: 1.4175406098365784, Final Batch Loss: 0.20660220086574554\n",
      "Epoch 6183, Loss: 1.4987391233444214, Final Batch Loss: 0.41477906703948975\n",
      "Epoch 6184, Loss: 1.3075692057609558, Final Batch Loss: 0.23061633110046387\n",
      "Epoch 6185, Loss: 1.4148966073989868, Final Batch Loss: 0.25085580348968506\n",
      "Epoch 6186, Loss: 1.4862484633922577, Final Batch Loss: 0.3098081648349762\n",
      "Epoch 6187, Loss: 1.2765987068414688, Final Batch Loss: 0.2256014496088028\n",
      "Epoch 6188, Loss: 1.3111621588468552, Final Batch Loss: 0.25107601284980774\n",
      "Epoch 6189, Loss: 1.3992810100317001, Final Batch Loss: 0.20955806970596313\n",
      "Epoch 6190, Loss: 1.3847076296806335, Final Batch Loss: 0.35587167739868164\n",
      "Epoch 6191, Loss: 1.6379915177822113, Final Batch Loss: 0.48146116733551025\n",
      "Epoch 6192, Loss: 1.407467558979988, Final Batch Loss: 0.20235703885555267\n",
      "Epoch 6193, Loss: 1.4029712975025177, Final Batch Loss: 0.30380818247795105\n",
      "Epoch 6194, Loss: 1.34439617395401, Final Batch Loss: 0.2950429320335388\n",
      "Epoch 6195, Loss: 1.6957040131092072, Final Batch Loss: 0.4046684503555298\n",
      "Epoch 6196, Loss: 1.4776206612586975, Final Batch Loss: 0.24255046248435974\n",
      "Epoch 6197, Loss: 1.3863085508346558, Final Batch Loss: 0.27746596932411194\n",
      "Epoch 6198, Loss: 1.1984675377607346, Final Batch Loss: 0.13739530742168427\n",
      "Epoch 6199, Loss: 1.3134191781282425, Final Batch Loss: 0.24867907166481018\n",
      "Epoch 6200, Loss: 1.4280069321393967, Final Batch Loss: 0.33885905146598816\n",
      "Epoch 6201, Loss: 1.303198590874672, Final Batch Loss: 0.35317081212997437\n",
      "Epoch 6202, Loss: 1.4784800857305527, Final Batch Loss: 0.37291958928108215\n",
      "Epoch 6203, Loss: 1.4343239217996597, Final Batch Loss: 0.33514291048049927\n",
      "Epoch 6204, Loss: 1.233273871243, Final Batch Loss: 0.1106836274266243\n",
      "Epoch 6205, Loss: 1.3645066022872925, Final Batch Loss: 0.19822898507118225\n",
      "Epoch 6206, Loss: 1.3547675758600235, Final Batch Loss: 0.24526768922805786\n",
      "Epoch 6207, Loss: 1.5078760981559753, Final Batch Loss: 0.24102193117141724\n",
      "Epoch 6208, Loss: 1.3952469527721405, Final Batch Loss: 0.34968000650405884\n",
      "Epoch 6209, Loss: 1.4768683165311813, Final Batch Loss: 0.3847268521785736\n",
      "Epoch 6210, Loss: 1.3580637872219086, Final Batch Loss: 0.27943649888038635\n",
      "Epoch 6211, Loss: 1.5067329704761505, Final Batch Loss: 0.2780313491821289\n",
      "Epoch 6212, Loss: 1.317475065588951, Final Batch Loss: 0.16315443813800812\n",
      "Epoch 6213, Loss: 1.4599496126174927, Final Batch Loss: 0.397849977016449\n",
      "Epoch 6214, Loss: 1.4235038757324219, Final Batch Loss: 0.22907979786396027\n",
      "Epoch 6215, Loss: 1.5442966520786285, Final Batch Loss: 0.45960482954978943\n",
      "Epoch 6216, Loss: 1.7590384781360626, Final Batch Loss: 0.515991747379303\n",
      "Epoch 6217, Loss: 2.044980928301811, Final Batch Loss: 1.0008448362350464\n",
      "Epoch 6218, Loss: 1.5288078784942627, Final Batch Loss: 0.3438854217529297\n",
      "Epoch 6219, Loss: 1.2273574322462082, Final Batch Loss: 0.24213479459285736\n",
      "Epoch 6220, Loss: 1.4307619482278824, Final Batch Loss: 0.2338639497756958\n",
      "Epoch 6221, Loss: 1.4421691298484802, Final Batch Loss: 0.252379447221756\n",
      "Epoch 6222, Loss: 1.4782312661409378, Final Batch Loss: 0.220057874917984\n",
      "Epoch 6223, Loss: 1.3723119646310806, Final Batch Loss: 0.269229531288147\n",
      "Epoch 6224, Loss: 1.6489784568548203, Final Batch Loss: 0.4426058530807495\n",
      "Epoch 6225, Loss: 1.4333513379096985, Final Batch Loss: 0.2778627574443817\n",
      "Epoch 6226, Loss: 1.845921367406845, Final Batch Loss: 0.6670669317245483\n",
      "Epoch 6227, Loss: 1.6144437193870544, Final Batch Loss: 0.33452630043029785\n",
      "Epoch 6228, Loss: 1.3086065649986267, Final Batch Loss: 0.2078242152929306\n",
      "Epoch 6229, Loss: 1.358936756849289, Final Batch Loss: 0.25538378953933716\n",
      "Epoch 6230, Loss: 1.2871002405881882, Final Batch Loss: 0.176596000790596\n",
      "Epoch 6231, Loss: 1.2636248171329498, Final Batch Loss: 0.31032174825668335\n",
      "Epoch 6232, Loss: 1.2852802276611328, Final Batch Loss: 0.18608079850673676\n",
      "Epoch 6233, Loss: 1.2483014166355133, Final Batch Loss: 0.1978578269481659\n",
      "Epoch 6234, Loss: 1.3242743164300919, Final Batch Loss: 0.24797117710113525\n",
      "Epoch 6235, Loss: 1.3790330290794373, Final Batch Loss: 0.26863589882850647\n",
      "Epoch 6236, Loss: 1.3562422543764114, Final Batch Loss: 0.2638360559940338\n",
      "Epoch 6237, Loss: 1.4680110663175583, Final Batch Loss: 0.2287466675043106\n",
      "Epoch 6238, Loss: 1.1573748886585236, Final Batch Loss: 0.09925229847431183\n",
      "Epoch 6239, Loss: 1.5110836178064346, Final Batch Loss: 0.47162458300590515\n",
      "Epoch 6240, Loss: 1.2377539426088333, Final Batch Loss: 0.1757672131061554\n",
      "Epoch 6241, Loss: 1.2130548059940338, Final Batch Loss: 0.22282162308692932\n",
      "Epoch 6242, Loss: 1.3660947531461716, Final Batch Loss: 0.37647539377212524\n",
      "Epoch 6243, Loss: 1.5788722187280655, Final Batch Loss: 0.4069981575012207\n",
      "Epoch 6244, Loss: 1.4770633578300476, Final Batch Loss: 0.29221343994140625\n",
      "Epoch 6245, Loss: 1.5529878437519073, Final Batch Loss: 0.35874029994010925\n",
      "Epoch 6246, Loss: 1.3585945069789886, Final Batch Loss: 0.20003469288349152\n",
      "Epoch 6247, Loss: 1.4391898810863495, Final Batch Loss: 0.24894306063652039\n",
      "Epoch 6248, Loss: 1.1635671705007553, Final Batch Loss: 0.21469758450984955\n",
      "Epoch 6249, Loss: 1.437390223145485, Final Batch Loss: 0.34883278608322144\n",
      "Epoch 6250, Loss: 1.1660419255495071, Final Batch Loss: 0.10073304176330566\n",
      "Epoch 6251, Loss: 1.3258187472820282, Final Batch Loss: 0.2400268018245697\n",
      "Epoch 6252, Loss: 1.3037301450967789, Final Batch Loss: 0.22729036211967468\n",
      "Epoch 6253, Loss: 1.5820762515068054, Final Batch Loss: 0.287447065114975\n",
      "Epoch 6254, Loss: 1.3054510205984116, Final Batch Loss: 0.2186119556427002\n",
      "Epoch 6255, Loss: 1.2934091687202454, Final Batch Loss: 0.27049049735069275\n",
      "Epoch 6256, Loss: 1.3488738685846329, Final Batch Loss: 0.1661035269498825\n",
      "Epoch 6257, Loss: 1.5558054894208908, Final Batch Loss: 0.577034592628479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6258, Loss: 1.481445923447609, Final Batch Loss: 0.44374677538871765\n",
      "Epoch 6259, Loss: 1.325729176402092, Final Batch Loss: 0.16997048258781433\n",
      "Epoch 6260, Loss: 1.4379123151302338, Final Batch Loss: 0.20440679788589478\n",
      "Epoch 6261, Loss: 1.5792285799980164, Final Batch Loss: 0.3433164954185486\n",
      "Epoch 6262, Loss: 1.160871461033821, Final Batch Loss: 0.1530379056930542\n",
      "Epoch 6263, Loss: 1.4047410488128662, Final Batch Loss: 0.24948124587535858\n",
      "Epoch 6264, Loss: 1.4867177903652191, Final Batch Loss: 0.2739121913909912\n",
      "Epoch 6265, Loss: 1.2706234455108643, Final Batch Loss: 0.26377856731414795\n",
      "Epoch 6266, Loss: 1.2789429873228073, Final Batch Loss: 0.18181107938289642\n",
      "Epoch 6267, Loss: 1.3382591605186462, Final Batch Loss: 0.29165390133857727\n",
      "Epoch 6268, Loss: 1.20236274600029, Final Batch Loss: 0.14736509323120117\n",
      "Epoch 6269, Loss: 1.6096892058849335, Final Batch Loss: 0.3196920156478882\n",
      "Epoch 6270, Loss: 1.304154857993126, Final Batch Loss: 0.24494881927967072\n",
      "Epoch 6271, Loss: 1.3443414866924286, Final Batch Loss: 0.25610819458961487\n",
      "Epoch 6272, Loss: 1.444542646408081, Final Batch Loss: 0.24581874907016754\n",
      "Epoch 6273, Loss: 1.3842141777276993, Final Batch Loss: 0.2545093894004822\n",
      "Epoch 6274, Loss: 1.2541363835334778, Final Batch Loss: 0.20958825945854187\n",
      "Epoch 6275, Loss: 1.3801652491092682, Final Batch Loss: 0.23532971739768982\n",
      "Epoch 6276, Loss: 1.5062976777553558, Final Batch Loss: 0.33515599370002747\n",
      "Epoch 6277, Loss: 1.5449382066726685, Final Batch Loss: 0.40262851119041443\n",
      "Epoch 6278, Loss: 1.46365088224411, Final Batch Loss: 0.29504963755607605\n",
      "Epoch 6279, Loss: 1.3725179433822632, Final Batch Loss: 0.29001930356025696\n",
      "Epoch 6280, Loss: 1.5367676615715027, Final Batch Loss: 0.33862727880477905\n",
      "Epoch 6281, Loss: 1.3789803236722946, Final Batch Loss: 0.25177377462387085\n",
      "Epoch 6282, Loss: 1.5453422963619232, Final Batch Loss: 0.3802509307861328\n",
      "Epoch 6283, Loss: 1.3909899294376373, Final Batch Loss: 0.32518845796585083\n",
      "Epoch 6284, Loss: 1.481420859694481, Final Batch Loss: 0.23714125156402588\n",
      "Epoch 6285, Loss: 1.4209775179624557, Final Batch Loss: 0.22137564420700073\n",
      "Epoch 6286, Loss: 1.2509095221757889, Final Batch Loss: 0.2647522985935211\n",
      "Epoch 6287, Loss: 1.471347838640213, Final Batch Loss: 0.4300040602684021\n",
      "Epoch 6288, Loss: 1.4944378435611725, Final Batch Loss: 0.38906341791152954\n",
      "Epoch 6289, Loss: 1.4214952141046524, Final Batch Loss: 0.3291943669319153\n",
      "Epoch 6290, Loss: 1.4300993829965591, Final Batch Loss: 0.32690614461898804\n",
      "Epoch 6291, Loss: 1.493645191192627, Final Batch Loss: 0.3243393301963806\n",
      "Epoch 6292, Loss: 1.4562252014875412, Final Batch Loss: 0.21206791698932648\n",
      "Epoch 6293, Loss: 1.2965097427368164, Final Batch Loss: 0.2287559062242508\n",
      "Epoch 6294, Loss: 1.2314889430999756, Final Batch Loss: 0.21796914935112\n",
      "Epoch 6295, Loss: 1.6012619137763977, Final Batch Loss: 0.45613911747932434\n",
      "Epoch 6296, Loss: 1.418378084897995, Final Batch Loss: 0.2665080428123474\n",
      "Epoch 6297, Loss: 1.2938783019781113, Final Batch Loss: 0.24542385339736938\n",
      "Epoch 6298, Loss: 1.3327669948339462, Final Batch Loss: 0.19305332005023956\n",
      "Epoch 6299, Loss: 1.660371571779251, Final Batch Loss: 0.4868764877319336\n",
      "Epoch 6300, Loss: 1.3562896102666855, Final Batch Loss: 0.234854593873024\n",
      "Epoch 6301, Loss: 1.676328331232071, Final Batch Loss: 0.481334924697876\n",
      "Epoch 6302, Loss: 1.4650332778692245, Final Batch Loss: 0.4161246716976166\n",
      "Epoch 6303, Loss: 1.3930388391017914, Final Batch Loss: 0.3029085695743561\n",
      "Epoch 6304, Loss: 1.5657721608877182, Final Batch Loss: 0.450686514377594\n",
      "Epoch 6305, Loss: 1.513788491487503, Final Batch Loss: 0.3509136736392975\n",
      "Epoch 6306, Loss: 1.6555641293525696, Final Batch Loss: 0.3765467405319214\n",
      "Epoch 6307, Loss: 1.450532615184784, Final Batch Loss: 0.352473646402359\n",
      "Epoch 6308, Loss: 1.5520515739917755, Final Batch Loss: 0.29494234919548035\n",
      "Epoch 6309, Loss: 1.4888200759887695, Final Batch Loss: 0.41860511898994446\n",
      "Epoch 6310, Loss: 1.4853071570396423, Final Batch Loss: 0.25461307168006897\n",
      "Epoch 6311, Loss: 1.2178773134946823, Final Batch Loss: 0.12566308677196503\n",
      "Epoch 6312, Loss: 1.1452718526124954, Final Batch Loss: 0.16540053486824036\n",
      "Epoch 6313, Loss: 1.4548981934785843, Final Batch Loss: 0.41283613443374634\n",
      "Epoch 6314, Loss: 1.4436102658510208, Final Batch Loss: 0.3509953022003174\n",
      "Epoch 6315, Loss: 1.3479717373847961, Final Batch Loss: 0.19054269790649414\n",
      "Epoch 6316, Loss: 1.4331049174070358, Final Batch Loss: 0.27429473400115967\n",
      "Epoch 6317, Loss: 1.3930799514055252, Final Batch Loss: 0.21209357678890228\n",
      "Epoch 6318, Loss: 1.4807965010404587, Final Batch Loss: 0.2115805447101593\n",
      "Epoch 6319, Loss: 1.5945477485656738, Final Batch Loss: 0.41861626505851746\n",
      "Epoch 6320, Loss: 1.5086306631565094, Final Batch Loss: 0.2559419572353363\n",
      "Epoch 6321, Loss: 1.512430191040039, Final Batch Loss: 0.2683355510234833\n",
      "Epoch 6322, Loss: 1.2977561354637146, Final Batch Loss: 0.17590908706188202\n",
      "Epoch 6323, Loss: 1.3070636838674545, Final Batch Loss: 0.20105499029159546\n",
      "Epoch 6324, Loss: 1.2586211413145065, Final Batch Loss: 0.2163008600473404\n",
      "Epoch 6325, Loss: 1.5430390238761902, Final Batch Loss: 0.4312025308609009\n",
      "Epoch 6326, Loss: 1.350217118859291, Final Batch Loss: 0.2850634753704071\n",
      "Epoch 6327, Loss: 1.4377872347831726, Final Batch Loss: 0.26045650243759155\n",
      "Epoch 6328, Loss: 1.4267268478870392, Final Batch Loss: 0.20631402730941772\n",
      "Epoch 6329, Loss: 1.374719887971878, Final Batch Loss: 0.11634916067123413\n",
      "Epoch 6330, Loss: 1.3023523688316345, Final Batch Loss: 0.24361760914325714\n",
      "Epoch 6331, Loss: 1.3596855401992798, Final Batch Loss: 0.324396550655365\n",
      "Epoch 6332, Loss: 1.3278087377548218, Final Batch Loss: 0.29002073407173157\n",
      "Epoch 6333, Loss: 1.3812071532011032, Final Batch Loss: 0.3103863596916199\n",
      "Epoch 6334, Loss: 1.3688636124134064, Final Batch Loss: 0.22259365022182465\n",
      "Epoch 6335, Loss: 1.4443345069885254, Final Batch Loss: 0.29030102491378784\n",
      "Epoch 6336, Loss: 1.5156163573265076, Final Batch Loss: 0.2658357620239258\n",
      "Epoch 6337, Loss: 1.2841691374778748, Final Batch Loss: 0.287019282579422\n",
      "Epoch 6338, Loss: 1.354933187365532, Final Batch Loss: 0.2995632588863373\n",
      "Epoch 6339, Loss: 1.4437010288238525, Final Batch Loss: 0.33456653356552124\n",
      "Epoch 6340, Loss: 1.3982502222061157, Final Batch Loss: 0.35633087158203125\n",
      "Epoch 6341, Loss: 1.5866248607635498, Final Batch Loss: 0.5527395009994507\n",
      "Epoch 6342, Loss: 1.4404363334178925, Final Batch Loss: 0.27939218282699585\n",
      "Epoch 6343, Loss: 1.2741440385580063, Final Batch Loss: 0.23669056594371796\n",
      "Epoch 6344, Loss: 1.2043519616127014, Final Batch Loss: 0.21739992499351501\n",
      "Epoch 6345, Loss: 1.386710673570633, Final Batch Loss: 0.23450788855552673\n",
      "Epoch 6346, Loss: 1.362849548459053, Final Batch Loss: 0.29429417848587036\n",
      "Epoch 6347, Loss: 1.800579085946083, Final Batch Loss: 0.6860183477401733\n",
      "Epoch 6348, Loss: 1.3563982099294662, Final Batch Loss: 0.196819007396698\n",
      "Epoch 6349, Loss: 1.2947563827037811, Final Batch Loss: 0.29243022203445435\n",
      "Epoch 6350, Loss: 1.4922581017017365, Final Batch Loss: 0.3157264292240143\n",
      "Epoch 6351, Loss: 1.2490078210830688, Final Batch Loss: 0.22545824944972992\n",
      "Epoch 6352, Loss: 1.3291071355342865, Final Batch Loss: 0.22234919667243958\n",
      "Epoch 6353, Loss: 1.3931025862693787, Final Batch Loss: 0.2468283772468567\n",
      "Epoch 6354, Loss: 1.4201589226722717, Final Batch Loss: 0.27501052618026733\n",
      "Epoch 6355, Loss: 1.367739588022232, Final Batch Loss: 0.2736903727054596\n",
      "Epoch 6356, Loss: 1.3308364301919937, Final Batch Loss: 0.2164943516254425\n",
      "Epoch 6357, Loss: 1.287943720817566, Final Batch Loss: 0.15986797213554382\n",
      "Epoch 6358, Loss: 1.5413403660058975, Final Batch Loss: 0.3354475200176239\n",
      "Epoch 6359, Loss: 1.5913616120815277, Final Batch Loss: 0.32455211877822876\n",
      "Epoch 6360, Loss: 1.4323428869247437, Final Batch Loss: 0.3343506455421448\n",
      "Epoch 6361, Loss: 1.3677798211574554, Final Batch Loss: 0.15572777390480042\n",
      "Epoch 6362, Loss: 1.568964883685112, Final Batch Loss: 0.4057812988758087\n",
      "Epoch 6363, Loss: 1.7505397945642471, Final Batch Loss: 0.5212703347206116\n",
      "Epoch 6364, Loss: 1.3031359314918518, Final Batch Loss: 0.21702858805656433\n",
      "Epoch 6365, Loss: 1.2084153592586517, Final Batch Loss: 0.21800856292247772\n",
      "Epoch 6366, Loss: 1.3376259356737137, Final Batch Loss: 0.22748681902885437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6367, Loss: 1.3344830125570297, Final Batch Loss: 0.2342815101146698\n",
      "Epoch 6368, Loss: 1.3966932445764542, Final Batch Loss: 0.2442769706249237\n",
      "Epoch 6369, Loss: 1.3542663007974625, Final Batch Loss: 0.2080492526292801\n",
      "Epoch 6370, Loss: 1.1689311563968658, Final Batch Loss: 0.17032262682914734\n",
      "Epoch 6371, Loss: 1.3557458817958832, Final Batch Loss: 0.2587610185146332\n",
      "Epoch 6372, Loss: 1.371936097741127, Final Batch Loss: 0.15894143283367157\n",
      "Epoch 6373, Loss: 1.3205689191818237, Final Batch Loss: 0.3193044662475586\n",
      "Epoch 6374, Loss: 1.3595434725284576, Final Batch Loss: 0.21527469158172607\n",
      "Epoch 6375, Loss: 1.2659321129322052, Final Batch Loss: 0.1767781376838684\n",
      "Epoch 6376, Loss: 1.098148450255394, Final Batch Loss: 0.14818549156188965\n",
      "Epoch 6377, Loss: 1.5624665021896362, Final Batch Loss: 0.47781693935394287\n",
      "Epoch 6378, Loss: 1.490662693977356, Final Batch Loss: 0.3641739785671234\n",
      "Epoch 6379, Loss: 1.3860231339931488, Final Batch Loss: 0.21660281717777252\n",
      "Epoch 6380, Loss: 1.4697261154651642, Final Batch Loss: 0.4132770001888275\n",
      "Epoch 6381, Loss: 1.192973107099533, Final Batch Loss: 0.12887635827064514\n",
      "Epoch 6382, Loss: 1.3990366160869598, Final Batch Loss: 0.23120371997356415\n",
      "Epoch 6383, Loss: 1.492366999387741, Final Batch Loss: 0.4134237468242645\n",
      "Epoch 6384, Loss: 1.4220287203788757, Final Batch Loss: 0.2364756166934967\n",
      "Epoch 6385, Loss: 1.3095324337482452, Final Batch Loss: 0.23189732432365417\n",
      "Epoch 6386, Loss: 1.8273409605026245, Final Batch Loss: 0.3308276832103729\n",
      "Epoch 6387, Loss: 1.5680729895830154, Final Batch Loss: 0.24518443644046783\n",
      "Epoch 6388, Loss: 1.2799938023090363, Final Batch Loss: 0.18160094320774078\n",
      "Epoch 6389, Loss: 1.3678817749023438, Final Batch Loss: 0.19042491912841797\n",
      "Epoch 6390, Loss: 1.2647383213043213, Final Batch Loss: 0.33369407057762146\n",
      "Epoch 6391, Loss: 1.2713844925165176, Final Batch Loss: 0.23354585468769073\n",
      "Epoch 6392, Loss: 1.2281513959169388, Final Batch Loss: 0.18618735671043396\n",
      "Epoch 6393, Loss: 1.1745187193155289, Final Batch Loss: 0.16633203625679016\n",
      "Epoch 6394, Loss: 1.3379148840904236, Final Batch Loss: 0.3091668486595154\n",
      "Epoch 6395, Loss: 1.2157335728406906, Final Batch Loss: 0.13800927996635437\n",
      "Epoch 6396, Loss: 1.3088434785604477, Final Batch Loss: 0.23460687696933746\n",
      "Epoch 6397, Loss: 1.3480272442102432, Final Batch Loss: 0.289897084236145\n",
      "Epoch 6398, Loss: 1.483295887708664, Final Batch Loss: 0.2983863353729248\n",
      "Epoch 6399, Loss: 1.283805787563324, Final Batch Loss: 0.1880328208208084\n",
      "Epoch 6400, Loss: 1.4554917812347412, Final Batch Loss: 0.2556213438510895\n",
      "Epoch 6401, Loss: 1.4593730866909027, Final Batch Loss: 0.28003939986228943\n",
      "Epoch 6402, Loss: 1.5384410321712494, Final Batch Loss: 0.24157792329788208\n",
      "Epoch 6403, Loss: 1.3688664734363556, Final Batch Loss: 0.25444602966308594\n",
      "Epoch 6404, Loss: 1.4067032933235168, Final Batch Loss: 0.30034083127975464\n",
      "Epoch 6405, Loss: 1.227276861667633, Final Batch Loss: 0.1702345758676529\n",
      "Epoch 6406, Loss: 1.240467056632042, Final Batch Loss: 0.22320754826068878\n",
      "Epoch 6407, Loss: 1.257656380534172, Final Batch Loss: 0.1584133803844452\n",
      "Epoch 6408, Loss: 1.3850746750831604, Final Batch Loss: 0.3008924424648285\n",
      "Epoch 6409, Loss: 1.5089293718338013, Final Batch Loss: 0.3490772843360901\n",
      "Epoch 6410, Loss: 1.5631809085607529, Final Batch Loss: 0.23417653143405914\n",
      "Epoch 6411, Loss: 1.3138439357280731, Final Batch Loss: 0.20858296751976013\n",
      "Epoch 6412, Loss: 1.568367213010788, Final Batch Loss: 0.3769996166229248\n",
      "Epoch 6413, Loss: 1.3655840158462524, Final Batch Loss: 0.30559512972831726\n",
      "Epoch 6414, Loss: 1.5423162430524826, Final Batch Loss: 0.1670653373003006\n",
      "Epoch 6415, Loss: 1.228256493806839, Final Batch Loss: 0.22942206263542175\n",
      "Epoch 6416, Loss: 1.3800723552703857, Final Batch Loss: 0.257407009601593\n",
      "Epoch 6417, Loss: 1.2428685426712036, Final Batch Loss: 0.2188749611377716\n",
      "Epoch 6418, Loss: 1.4451801776885986, Final Batch Loss: 0.281141996383667\n",
      "Epoch 6419, Loss: 1.3425046801567078, Final Batch Loss: 0.281006395816803\n",
      "Epoch 6420, Loss: 1.5930307060480118, Final Batch Loss: 0.3933064341545105\n",
      "Epoch 6421, Loss: 1.2698580026626587, Final Batch Loss: 0.22019901871681213\n",
      "Epoch 6422, Loss: 1.3165856748819351, Final Batch Loss: 0.23837676644325256\n",
      "Epoch 6423, Loss: 1.4069233536720276, Final Batch Loss: 0.27552422881126404\n",
      "Epoch 6424, Loss: 1.4198669642210007, Final Batch Loss: 0.39747461676597595\n",
      "Epoch 6425, Loss: 1.3918303698301315, Final Batch Loss: 0.25223028659820557\n",
      "Epoch 6426, Loss: 1.2575123608112335, Final Batch Loss: 0.2301424741744995\n",
      "Epoch 6427, Loss: 1.2778030782938004, Final Batch Loss: 0.16023650765419006\n",
      "Epoch 6428, Loss: 1.467237338423729, Final Batch Loss: 0.16239549219608307\n",
      "Epoch 6429, Loss: 1.205162912607193, Final Batch Loss: 0.27739188075065613\n",
      "Epoch 6430, Loss: 1.4157602041959763, Final Batch Loss: 0.1862431913614273\n",
      "Epoch 6431, Loss: 1.3035366833209991, Final Batch Loss: 0.24094046652317047\n",
      "Epoch 6432, Loss: 1.5501451194286346, Final Batch Loss: 0.5489161610603333\n",
      "Epoch 6433, Loss: 1.3054712265729904, Final Batch Loss: 0.2016879916191101\n",
      "Epoch 6434, Loss: 1.3152025640010834, Final Batch Loss: 0.11785155534744263\n",
      "Epoch 6435, Loss: 1.6576795428991318, Final Batch Loss: 0.348235547542572\n",
      "Epoch 6436, Loss: 1.564098596572876, Final Batch Loss: 0.3963036835193634\n",
      "Epoch 6437, Loss: 1.4465020298957825, Final Batch Loss: 0.2993040084838867\n",
      "Epoch 6438, Loss: 1.3688199520111084, Final Batch Loss: 0.31880566477775574\n",
      "Epoch 6439, Loss: 1.5535218864679337, Final Batch Loss: 0.2136833816766739\n",
      "Epoch 6440, Loss: 1.5418286621570587, Final Batch Loss: 0.3721317648887634\n",
      "Epoch 6441, Loss: 1.2182216793298721, Final Batch Loss: 0.2363707274198532\n",
      "Epoch 6442, Loss: 1.6108291447162628, Final Batch Loss: 0.43966054916381836\n",
      "Epoch 6443, Loss: 1.486567735671997, Final Batch Loss: 0.2871497869491577\n",
      "Epoch 6444, Loss: 1.3524322360754013, Final Batch Loss: 0.22010257840156555\n",
      "Epoch 6445, Loss: 1.3553387373685837, Final Batch Loss: 0.18368475139141083\n",
      "Epoch 6446, Loss: 1.3308180123567581, Final Batch Loss: 0.27167901396751404\n",
      "Epoch 6447, Loss: 1.400404930114746, Final Batch Loss: 0.28794336318969727\n",
      "Epoch 6448, Loss: 1.3294094055891037, Final Batch Loss: 0.2522267997264862\n",
      "Epoch 6449, Loss: 1.3680914044380188, Final Batch Loss: 0.27742817997932434\n",
      "Epoch 6450, Loss: 1.8546271622180939, Final Batch Loss: 0.651579737663269\n",
      "Epoch 6451, Loss: 1.5477374494075775, Final Batch Loss: 0.33396676182746887\n",
      "Epoch 6452, Loss: 1.2355225384235382, Final Batch Loss: 0.170554056763649\n",
      "Epoch 6453, Loss: 1.535630390048027, Final Batch Loss: 0.4077422022819519\n",
      "Epoch 6454, Loss: 1.7131692618131638, Final Batch Loss: 0.32007214426994324\n",
      "Epoch 6455, Loss: 1.4716821312904358, Final Batch Loss: 0.27954286336898804\n",
      "Epoch 6456, Loss: 1.3769739121198654, Final Batch Loss: 0.3615806996822357\n",
      "Epoch 6457, Loss: 1.4625476002693176, Final Batch Loss: 0.3475516736507416\n",
      "Epoch 6458, Loss: 1.450158253312111, Final Batch Loss: 0.2158346325159073\n",
      "Epoch 6459, Loss: 1.4492229223251343, Final Batch Loss: 0.31875717639923096\n",
      "Epoch 6460, Loss: 1.3189701437950134, Final Batch Loss: 0.17705653607845306\n",
      "Epoch 6461, Loss: 1.3863157629966736, Final Batch Loss: 0.2300591617822647\n",
      "Epoch 6462, Loss: 1.284913882613182, Final Batch Loss: 0.17629048228263855\n",
      "Epoch 6463, Loss: 1.4399101734161377, Final Batch Loss: 0.32971978187561035\n",
      "Epoch 6464, Loss: 1.3766171783208847, Final Batch Loss: 0.2752823531627655\n",
      "Epoch 6465, Loss: 1.3219726532697678, Final Batch Loss: 0.3304057717323303\n",
      "Epoch 6466, Loss: 1.142612263560295, Final Batch Loss: 0.1428380310535431\n",
      "Epoch 6467, Loss: 1.4207264333963394, Final Batch Loss: 0.2556893229484558\n",
      "Epoch 6468, Loss: 1.2863658219575882, Final Batch Loss: 0.2942253649234772\n",
      "Epoch 6469, Loss: 1.4104119539260864, Final Batch Loss: 0.3668757677078247\n",
      "Epoch 6470, Loss: 1.3362257033586502, Final Batch Loss: 0.25541359186172485\n",
      "Epoch 6471, Loss: 1.3996968567371368, Final Batch Loss: 0.25730234384536743\n",
      "Epoch 6472, Loss: 1.4015311300754547, Final Batch Loss: 0.30527299642562866\n",
      "Epoch 6473, Loss: 1.499933123588562, Final Batch Loss: 0.1807827204465866\n",
      "Epoch 6474, Loss: 1.3666562139987946, Final Batch Loss: 0.28520452976226807\n",
      "Epoch 6475, Loss: 1.5477177947759628, Final Batch Loss: 0.34001466631889343\n",
      "Epoch 6476, Loss: 1.3066683113574982, Final Batch Loss: 0.12999284267425537\n",
      "Epoch 6477, Loss: 1.355901375412941, Final Batch Loss: 0.2290652096271515\n",
      "Epoch 6478, Loss: 1.6652957201004028, Final Batch Loss: 0.46841391921043396\n",
      "Epoch 6479, Loss: 1.4555114954710007, Final Batch Loss: 0.49822333455085754\n",
      "Epoch 6480, Loss: 1.4441721439361572, Final Batch Loss: 0.3310187757015228\n",
      "Epoch 6481, Loss: 1.366849571466446, Final Batch Loss: 0.3088315427303314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6482, Loss: 1.5034897327423096, Final Batch Loss: 0.4056052565574646\n",
      "Epoch 6483, Loss: 1.524065524339676, Final Batch Loss: 0.34611570835113525\n",
      "Epoch 6484, Loss: 1.3767918348312378, Final Batch Loss: 0.21760988235473633\n",
      "Epoch 6485, Loss: 1.3911359757184982, Final Batch Loss: 0.32040470838546753\n",
      "Epoch 6486, Loss: 1.1941033452749252, Final Batch Loss: 0.22565670311450958\n",
      "Epoch 6487, Loss: 1.2078803330659866, Final Batch Loss: 0.2389008104801178\n",
      "Epoch 6488, Loss: 1.3492311537265778, Final Batch Loss: 0.2638241946697235\n",
      "Epoch 6489, Loss: 1.258352130651474, Final Batch Loss: 0.23236903548240662\n",
      "Epoch 6490, Loss: 1.3952294737100601, Final Batch Loss: 0.4072910249233246\n",
      "Epoch 6491, Loss: 1.2397888898849487, Final Batch Loss: 0.13615773618221283\n",
      "Epoch 6492, Loss: 1.2958302646875381, Final Batch Loss: 0.20976731181144714\n",
      "Epoch 6493, Loss: 1.2735214531421661, Final Batch Loss: 0.17302663624286652\n",
      "Epoch 6494, Loss: 1.3549219071865082, Final Batch Loss: 0.1567947268486023\n",
      "Epoch 6495, Loss: 1.276151031255722, Final Batch Loss: 0.20271143317222595\n",
      "Epoch 6496, Loss: 1.4119718819856644, Final Batch Loss: 0.232322096824646\n",
      "Epoch 6497, Loss: 1.235347107052803, Final Batch Loss: 0.21055325865745544\n",
      "Epoch 6498, Loss: 1.2885521054267883, Final Batch Loss: 0.3350563049316406\n",
      "Epoch 6499, Loss: 1.2972820103168488, Final Batch Loss: 0.2893526256084442\n",
      "Epoch 6500, Loss: 1.3864235132932663, Final Batch Loss: 0.21773947775363922\n",
      "Epoch 6501, Loss: 1.5886036306619644, Final Batch Loss: 0.4066842198371887\n",
      "Epoch 6502, Loss: 1.2422206848859787, Final Batch Loss: 0.2154046595096588\n",
      "Epoch 6503, Loss: 1.2721562385559082, Final Batch Loss: 0.21659809350967407\n",
      "Epoch 6504, Loss: 1.372429519891739, Final Batch Loss: 0.3351525366306305\n",
      "Epoch 6505, Loss: 1.533754125237465, Final Batch Loss: 0.3834451735019684\n",
      "Epoch 6506, Loss: 1.5428809225559235, Final Batch Loss: 0.3554198145866394\n",
      "Epoch 6507, Loss: 1.4129206538200378, Final Batch Loss: 0.24818068742752075\n",
      "Epoch 6508, Loss: 1.5772193223237991, Final Batch Loss: 0.520895779132843\n",
      "Epoch 6509, Loss: 1.3537489473819733, Final Batch Loss: 0.2499515414237976\n",
      "Epoch 6510, Loss: 1.2193668186664581, Final Batch Loss: 0.2057310938835144\n",
      "Epoch 6511, Loss: 1.5022306144237518, Final Batch Loss: 0.31387731432914734\n",
      "Epoch 6512, Loss: 1.3436298072338104, Final Batch Loss: 0.26874732971191406\n",
      "Epoch 6513, Loss: 1.5821675211191177, Final Batch Loss: 0.4373834431171417\n",
      "Epoch 6514, Loss: 1.2444956302642822, Final Batch Loss: 0.22932614386081696\n",
      "Epoch 6515, Loss: 1.359413430094719, Final Batch Loss: 0.14993198215961456\n",
      "Epoch 6516, Loss: 1.3073667138814926, Final Batch Loss: 0.18752990663051605\n",
      "Epoch 6517, Loss: 1.1414894759654999, Final Batch Loss: 0.1687229424715042\n",
      "Epoch 6518, Loss: 1.2814074754714966, Final Batch Loss: 0.19039520621299744\n",
      "Epoch 6519, Loss: 1.2350221574306488, Final Batch Loss: 0.1898443102836609\n",
      "Epoch 6520, Loss: 1.3156794309616089, Final Batch Loss: 0.23471970856189728\n",
      "Epoch 6521, Loss: 1.3735976815223694, Final Batch Loss: 0.3199782371520996\n",
      "Epoch 6522, Loss: 1.5824685394763947, Final Batch Loss: 0.3970293700695038\n",
      "Epoch 6523, Loss: 1.1715124696493149, Final Batch Loss: 0.1636403203010559\n",
      "Epoch 6524, Loss: 1.2922490537166595, Final Batch Loss: 0.1953827142715454\n",
      "Epoch 6525, Loss: 1.336440622806549, Final Batch Loss: 0.3010789453983307\n",
      "Epoch 6526, Loss: 1.356614574790001, Final Batch Loss: 0.4252575933933258\n",
      "Epoch 6527, Loss: 1.3727099746465683, Final Batch Loss: 0.22604654729366302\n",
      "Epoch 6528, Loss: 1.350783258676529, Final Batch Loss: 0.20476782321929932\n",
      "Epoch 6529, Loss: 1.3637714982032776, Final Batch Loss: 0.27070358395576477\n",
      "Epoch 6530, Loss: 1.2409343868494034, Final Batch Loss: 0.13950660824775696\n",
      "Epoch 6531, Loss: 1.354743406176567, Final Batch Loss: 0.2149345576763153\n",
      "Epoch 6532, Loss: 1.205093838274479, Final Batch Loss: 0.1107674166560173\n",
      "Epoch 6533, Loss: 1.4173111766576767, Final Batch Loss: 0.20077046751976013\n",
      "Epoch 6534, Loss: 1.2394270449876785, Final Batch Loss: 0.3080333471298218\n",
      "Epoch 6535, Loss: 1.5024677217006683, Final Batch Loss: 0.3144640028476715\n",
      "Epoch 6536, Loss: 1.4285462945699692, Final Batch Loss: 0.22315149009227753\n",
      "Epoch 6537, Loss: 1.4885225892066956, Final Batch Loss: 0.3228319585323334\n",
      "Epoch 6538, Loss: 1.2461544275283813, Final Batch Loss: 0.2196759134531021\n",
      "Epoch 6539, Loss: 1.5403800308704376, Final Batch Loss: 0.36793574690818787\n",
      "Epoch 6540, Loss: 1.4411697536706924, Final Batch Loss: 0.3959138095378876\n",
      "Epoch 6541, Loss: 1.5052355527877808, Final Batch Loss: 0.22121785581111908\n",
      "Epoch 6542, Loss: 1.54843270778656, Final Batch Loss: 0.3745020627975464\n",
      "Epoch 6543, Loss: 1.304970070719719, Final Batch Loss: 0.17160123586654663\n",
      "Epoch 6544, Loss: 1.4410026967525482, Final Batch Loss: 0.28693053126335144\n",
      "Epoch 6545, Loss: 1.4458658397197723, Final Batch Loss: 0.24771687388420105\n",
      "Epoch 6546, Loss: 1.2338172942399979, Final Batch Loss: 0.2536534070968628\n",
      "Epoch 6547, Loss: 1.4762985855340958, Final Batch Loss: 0.1756778508424759\n",
      "Epoch 6548, Loss: 1.29452683031559, Final Batch Loss: 0.2264777272939682\n",
      "Epoch 6549, Loss: 1.5689084231853485, Final Batch Loss: 0.45926904678344727\n",
      "Epoch 6550, Loss: 1.4440918266773224, Final Batch Loss: 0.33790868520736694\n",
      "Epoch 6551, Loss: 1.288732573390007, Final Batch Loss: 0.30490389466285706\n",
      "Epoch 6552, Loss: 1.3483887314796448, Final Batch Loss: 0.20368224382400513\n",
      "Epoch 6553, Loss: 1.2854239791631699, Final Batch Loss: 0.2265910655260086\n",
      "Epoch 6554, Loss: 1.4666638374328613, Final Batch Loss: 0.37673044204711914\n",
      "Epoch 6555, Loss: 1.4760677814483643, Final Batch Loss: 0.3591196537017822\n",
      "Epoch 6556, Loss: 1.5970450639724731, Final Batch Loss: 0.4090356230735779\n",
      "Epoch 6557, Loss: 1.425227627158165, Final Batch Loss: 0.31439879536628723\n",
      "Epoch 6558, Loss: 1.4633475989103317, Final Batch Loss: 0.28711649775505066\n",
      "Epoch 6559, Loss: 1.4907921105623245, Final Batch Loss: 0.35584479570388794\n",
      "Epoch 6560, Loss: 1.4212669730186462, Final Batch Loss: 0.2706236243247986\n",
      "Epoch 6561, Loss: 1.5306329131126404, Final Batch Loss: 0.3808220326900482\n",
      "Epoch 6562, Loss: 1.4545270502567291, Final Batch Loss: 0.30650028586387634\n",
      "Epoch 6563, Loss: 1.4769600927829742, Final Batch Loss: 0.2900424897670746\n",
      "Epoch 6564, Loss: 1.3910114616155624, Final Batch Loss: 0.2811700999736786\n",
      "Epoch 6565, Loss: 1.4960772693157196, Final Batch Loss: 0.2766385078430176\n",
      "Epoch 6566, Loss: 1.8456950187683105, Final Batch Loss: 0.640942394733429\n",
      "Epoch 6567, Loss: 1.5961454063653946, Final Batch Loss: 0.35023096203804016\n",
      "Epoch 6568, Loss: 1.2694516777992249, Final Batch Loss: 0.2712118625640869\n",
      "Epoch 6569, Loss: 1.3557128310203552, Final Batch Loss: 0.2493242770433426\n",
      "Epoch 6570, Loss: 1.295153483748436, Final Batch Loss: 0.1551896184682846\n",
      "Epoch 6571, Loss: 1.2891349345445633, Final Batch Loss: 0.21616074442863464\n",
      "Epoch 6572, Loss: 1.3807260543107986, Final Batch Loss: 0.24911028146743774\n",
      "Epoch 6573, Loss: 1.1274381652474403, Final Batch Loss: 0.08259705454111099\n",
      "Epoch 6574, Loss: 1.143146425485611, Final Batch Loss: 0.11584801971912384\n",
      "Epoch 6575, Loss: 1.2733780890703201, Final Batch Loss: 0.20510199666023254\n",
      "Epoch 6576, Loss: 1.5038075745105743, Final Batch Loss: 0.363071084022522\n",
      "Epoch 6577, Loss: 1.2990051805973053, Final Batch Loss: 0.16311943531036377\n",
      "Epoch 6578, Loss: 1.3578816056251526, Final Batch Loss: 0.3611331582069397\n",
      "Epoch 6579, Loss: 1.3983826786279678, Final Batch Loss: 0.3063424229621887\n",
      "Epoch 6580, Loss: 1.3832498341798782, Final Batch Loss: 0.41961967945098877\n",
      "Epoch 6581, Loss: 1.3088828027248383, Final Batch Loss: 0.24259425699710846\n",
      "Epoch 6582, Loss: 1.4099494218826294, Final Batch Loss: 0.26960834860801697\n",
      "Epoch 6583, Loss: 1.4939080476760864, Final Batch Loss: 0.23122158646583557\n",
      "Epoch 6584, Loss: 1.5799947828054428, Final Batch Loss: 0.5756067633628845\n",
      "Epoch 6585, Loss: 1.42752605676651, Final Batch Loss: 0.22102266550064087\n",
      "Epoch 6586, Loss: 1.80272875726223, Final Batch Loss: 0.5160878896713257\n",
      "Epoch 6587, Loss: 1.4445613324642181, Final Batch Loss: 0.23344570398330688\n",
      "Epoch 6588, Loss: 1.3825052231550217, Final Batch Loss: 0.2859379053115845\n",
      "Epoch 6589, Loss: 1.4725313633680344, Final Batch Loss: 0.23263098299503326\n",
      "Epoch 6590, Loss: 1.178712636232376, Final Batch Loss: 0.21437154710292816\n",
      "Epoch 6591, Loss: 1.5435553640127182, Final Batch Loss: 0.34772560000419617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6592, Loss: 1.4700314402580261, Final Batch Loss: 0.31865400075912476\n",
      "Epoch 6593, Loss: 1.3241740614175797, Final Batch Loss: 0.2695876657962799\n",
      "Epoch 6594, Loss: 1.5072265565395355, Final Batch Loss: 0.4786183834075928\n",
      "Epoch 6595, Loss: 1.3794457614421844, Final Batch Loss: 0.2996470332145691\n",
      "Epoch 6596, Loss: 1.3636487871408463, Final Batch Loss: 0.2285175323486328\n",
      "Epoch 6597, Loss: 1.343756526708603, Final Batch Loss: 0.39419814944267273\n",
      "Epoch 6598, Loss: 1.3236382901668549, Final Batch Loss: 0.1941061019897461\n",
      "Epoch 6599, Loss: 1.6000129878520966, Final Batch Loss: 0.31641289591789246\n",
      "Epoch 6600, Loss: 1.3216766268014908, Final Batch Loss: 0.25569355487823486\n",
      "Epoch 6601, Loss: 1.3738074004650116, Final Batch Loss: 0.2697005867958069\n",
      "Epoch 6602, Loss: 1.235993281006813, Final Batch Loss: 0.18458612263202667\n",
      "Epoch 6603, Loss: 1.6974769681692123, Final Batch Loss: 0.5556752681732178\n",
      "Epoch 6604, Loss: 1.3029583990573883, Final Batch Loss: 0.22219283878803253\n",
      "Epoch 6605, Loss: 1.6246487200260162, Final Batch Loss: 0.32866430282592773\n",
      "Epoch 6606, Loss: 1.5233539193868637, Final Batch Loss: 0.3946415185928345\n",
      "Epoch 6607, Loss: 1.6038839370012283, Final Batch Loss: 0.4405878186225891\n",
      "Epoch 6608, Loss: 1.492753952741623, Final Batch Loss: 0.3650630712509155\n",
      "Epoch 6609, Loss: 1.669097125530243, Final Batch Loss: 0.34021225571632385\n",
      "Epoch 6610, Loss: 1.6717084348201752, Final Batch Loss: 0.4450048506259918\n",
      "Epoch 6611, Loss: 1.445385605096817, Final Batch Loss: 0.25074058771133423\n",
      "Epoch 6612, Loss: 1.4847537279129028, Final Batch Loss: 0.24806693196296692\n",
      "Epoch 6613, Loss: 1.431607872247696, Final Batch Loss: 0.3289318382740021\n",
      "Epoch 6614, Loss: 1.373308315873146, Final Batch Loss: 0.2342495620250702\n",
      "Epoch 6615, Loss: 1.6246438026428223, Final Batch Loss: 0.28296786546707153\n",
      "Epoch 6616, Loss: 1.2368919551372528, Final Batch Loss: 0.17256492376327515\n",
      "Epoch 6617, Loss: 1.4080268144607544, Final Batch Loss: 0.33748576045036316\n",
      "Epoch 6618, Loss: 1.409591168165207, Final Batch Loss: 0.2639550268650055\n",
      "Epoch 6619, Loss: 1.2836318165063858, Final Batch Loss: 0.2189880907535553\n",
      "Epoch 6620, Loss: 1.4833936244249344, Final Batch Loss: 0.3389335870742798\n",
      "Epoch 6621, Loss: 1.272576481103897, Final Batch Loss: 0.19891919195652008\n",
      "Epoch 6622, Loss: 1.145720660686493, Final Batch Loss: 0.1530090868473053\n",
      "Epoch 6623, Loss: 1.3621212095022202, Final Batch Loss: 0.35335633158683777\n",
      "Epoch 6624, Loss: 1.65394988656044, Final Batch Loss: 0.5085387229919434\n",
      "Epoch 6625, Loss: 1.4462777376174927, Final Batch Loss: 0.2667151093482971\n",
      "Epoch 6626, Loss: 1.3424637615680695, Final Batch Loss: 0.24882608652114868\n",
      "Epoch 6627, Loss: 1.3524353057146072, Final Batch Loss: 0.2538154423236847\n",
      "Epoch 6628, Loss: 1.402268573641777, Final Batch Loss: 0.38726383447647095\n",
      "Epoch 6629, Loss: 1.5009731352329254, Final Batch Loss: 0.2012922763824463\n",
      "Epoch 6630, Loss: 1.5090467035770416, Final Batch Loss: 0.15507930517196655\n",
      "Epoch 6631, Loss: 1.3405663520097733, Final Batch Loss: 0.2850539982318878\n",
      "Epoch 6632, Loss: 1.505024015903473, Final Batch Loss: 0.24207249283790588\n",
      "Epoch 6633, Loss: 1.738493651151657, Final Batch Loss: 0.4031376540660858\n",
      "Epoch 6634, Loss: 1.2949572503566742, Final Batch Loss: 0.19526493549346924\n",
      "Epoch 6635, Loss: 1.5022016167640686, Final Batch Loss: 0.3113188147544861\n",
      "Epoch 6636, Loss: 1.467958003282547, Final Batch Loss: 0.3314831852912903\n",
      "Epoch 6637, Loss: 1.4870505779981613, Final Batch Loss: 0.346993088722229\n",
      "Epoch 6638, Loss: 1.5540953129529953, Final Batch Loss: 0.37311699986457825\n",
      "Epoch 6639, Loss: 1.5071545988321304, Final Batch Loss: 0.28948459029197693\n",
      "Epoch 6640, Loss: 1.312280148267746, Final Batch Loss: 0.2131187915802002\n",
      "Epoch 6641, Loss: 1.416407972574234, Final Batch Loss: 0.19979426264762878\n",
      "Epoch 6642, Loss: 1.3722445517778397, Final Batch Loss: 0.2344324141740799\n",
      "Epoch 6643, Loss: 1.6138271987438202, Final Batch Loss: 0.28614410758018494\n",
      "Epoch 6644, Loss: 1.380694255232811, Final Batch Loss: 0.18582205474376678\n",
      "Epoch 6645, Loss: 1.2574051171541214, Final Batch Loss: 0.19334346055984497\n",
      "Epoch 6646, Loss: 1.3733407855033875, Final Batch Loss: 0.3303879499435425\n",
      "Epoch 6647, Loss: 1.3552208840847015, Final Batch Loss: 0.2638244926929474\n",
      "Epoch 6648, Loss: 1.3502077162265778, Final Batch Loss: 0.26686128973960876\n",
      "Epoch 6649, Loss: 1.4044812321662903, Final Batch Loss: 0.3055388331413269\n",
      "Epoch 6650, Loss: 1.3220593929290771, Final Batch Loss: 0.3660656809806824\n",
      "Epoch 6651, Loss: 1.5062644928693771, Final Batch Loss: 0.40304574370384216\n",
      "Epoch 6652, Loss: 1.4853497445583344, Final Batch Loss: 0.3567291498184204\n",
      "Epoch 6653, Loss: 1.4278420507907867, Final Batch Loss: 0.37238675355911255\n",
      "Epoch 6654, Loss: 1.336284726858139, Final Batch Loss: 0.18019944429397583\n",
      "Epoch 6655, Loss: 1.4712859094142914, Final Batch Loss: 0.3172054588794708\n",
      "Epoch 6656, Loss: 1.251803532242775, Final Batch Loss: 0.27178117632865906\n",
      "Epoch 6657, Loss: 1.6088826805353165, Final Batch Loss: 0.23613207042217255\n",
      "Epoch 6658, Loss: 1.404317244887352, Final Batch Loss: 0.27438440918922424\n",
      "Epoch 6659, Loss: 1.3826031982898712, Final Batch Loss: 0.24504205584526062\n",
      "Epoch 6660, Loss: 1.3178571313619614, Final Batch Loss: 0.24125611782073975\n",
      "Epoch 6661, Loss: 1.2090158313512802, Final Batch Loss: 0.2503487467765808\n",
      "Epoch 6662, Loss: 1.3107407838106155, Final Batch Loss: 0.3117368519306183\n",
      "Epoch 6663, Loss: 1.7977946400642395, Final Batch Loss: 0.6379728317260742\n",
      "Epoch 6664, Loss: 1.33185675740242, Final Batch Loss: 0.2786126434803009\n",
      "Epoch 6665, Loss: 1.6341309696435928, Final Batch Loss: 0.39423611760139465\n",
      "Epoch 6666, Loss: 1.3107860088348389, Final Batch Loss: 0.26645851135253906\n",
      "Epoch 6667, Loss: 1.2524236738681793, Final Batch Loss: 0.2531154751777649\n",
      "Epoch 6668, Loss: 1.2339527308940887, Final Batch Loss: 0.1791027933359146\n",
      "Epoch 6669, Loss: 1.3820721060037613, Final Batch Loss: 0.16359803080558777\n",
      "Epoch 6670, Loss: 1.4501427561044693, Final Batch Loss: 0.32367846369743347\n",
      "Epoch 6671, Loss: 1.297118030488491, Final Batch Loss: 0.11832375079393387\n",
      "Epoch 6672, Loss: 1.28609898686409, Final Batch Loss: 0.22473308444023132\n",
      "Epoch 6673, Loss: 1.3324546217918396, Final Batch Loss: 0.2137867510318756\n",
      "Epoch 6674, Loss: 1.2606855183839798, Final Batch Loss: 0.22518591582775116\n",
      "Epoch 6675, Loss: 1.4779336154460907, Final Batch Loss: 0.31264814734458923\n",
      "Epoch 6676, Loss: 1.3517888337373734, Final Batch Loss: 0.25450998544692993\n",
      "Epoch 6677, Loss: 1.239504262804985, Final Batch Loss: 0.1903088241815567\n",
      "Epoch 6678, Loss: 1.3769048750400543, Final Batch Loss: 0.19437964260578156\n",
      "Epoch 6679, Loss: 1.4909243434667587, Final Batch Loss: 0.4364733397960663\n",
      "Epoch 6680, Loss: 1.5453160107135773, Final Batch Loss: 0.21678265929222107\n",
      "Epoch 6681, Loss: 1.2284644097089767, Final Batch Loss: 0.19764570891857147\n",
      "Epoch 6682, Loss: 1.2870169132947922, Final Batch Loss: 0.28252387046813965\n",
      "Epoch 6683, Loss: 1.377464085817337, Final Batch Loss: 0.25348082184791565\n",
      "Epoch 6684, Loss: 1.3118348568677902, Final Batch Loss: 0.23674601316452026\n",
      "Epoch 6685, Loss: 1.2564717829227448, Final Batch Loss: 0.20718593895435333\n",
      "Epoch 6686, Loss: 1.3814100325107574, Final Batch Loss: 0.30370643734931946\n",
      "Epoch 6687, Loss: 1.3246811926364899, Final Batch Loss: 0.3299035131931305\n",
      "Epoch 6688, Loss: 1.596066653728485, Final Batch Loss: 0.3767869472503662\n",
      "Epoch 6689, Loss: 1.2373854666948318, Final Batch Loss: 0.19444674253463745\n",
      "Epoch 6690, Loss: 1.4844108670949936, Final Batch Loss: 0.4288584291934967\n",
      "Epoch 6691, Loss: 1.3792555034160614, Final Batch Loss: 0.3376215696334839\n",
      "Epoch 6692, Loss: 1.3306317329406738, Final Batch Loss: 0.2538702189922333\n",
      "Epoch 6693, Loss: 1.5078221559524536, Final Batch Loss: 0.38322386145591736\n",
      "Epoch 6694, Loss: 1.2883851528167725, Final Batch Loss: 0.2630731165409088\n",
      "Epoch 6695, Loss: 1.2948123216629028, Final Batch Loss: 0.13785958290100098\n",
      "Epoch 6696, Loss: 1.372608706355095, Final Batch Loss: 0.29570719599723816\n",
      "Epoch 6697, Loss: 1.3055408298969269, Final Batch Loss: 0.29672563076019287\n",
      "Epoch 6698, Loss: 1.4933198690414429, Final Batch Loss: 0.44134679436683655\n",
      "Epoch 6699, Loss: 1.2988057732582092, Final Batch Loss: 0.19082072377204895\n",
      "Epoch 6700, Loss: 1.470489114522934, Final Batch Loss: 0.4355948865413666\n",
      "Epoch 6701, Loss: 1.474901169538498, Final Batch Loss: 0.386776328086853\n",
      "Epoch 6702, Loss: 1.3212547302246094, Final Batch Loss: 0.22452695667743683\n",
      "Epoch 6703, Loss: 1.5213674008846283, Final Batch Loss: 0.35797733068466187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6704, Loss: 1.2813627272844315, Final Batch Loss: 0.19418084621429443\n",
      "Epoch 6705, Loss: 1.288193792104721, Final Batch Loss: 0.26397114992141724\n",
      "Epoch 6706, Loss: 1.2926642447710037, Final Batch Loss: 0.2769950032234192\n",
      "Epoch 6707, Loss: 1.4361951053142548, Final Batch Loss: 0.252973347902298\n",
      "Epoch 6708, Loss: 1.504406064748764, Final Batch Loss: 0.29111409187316895\n",
      "Epoch 6709, Loss: 1.3148085922002792, Final Batch Loss: 0.2485937625169754\n",
      "Epoch 6710, Loss: 1.3414853811264038, Final Batch Loss: 0.3464261293411255\n",
      "Epoch 6711, Loss: 1.2305232882499695, Final Batch Loss: 0.23135586082935333\n",
      "Epoch 6712, Loss: 1.348211333155632, Final Batch Loss: 0.27903738617897034\n",
      "Epoch 6713, Loss: 1.1532136648893356, Final Batch Loss: 0.12642452120780945\n",
      "Epoch 6714, Loss: 1.3290483355522156, Final Batch Loss: 0.19256749749183655\n",
      "Epoch 6715, Loss: 1.4250429719686508, Final Batch Loss: 0.31512609124183655\n",
      "Epoch 6716, Loss: 1.2126488238573074, Final Batch Loss: 0.16477800905704498\n",
      "Epoch 6717, Loss: 1.4733968377113342, Final Batch Loss: 0.30255553126335144\n",
      "Epoch 6718, Loss: 1.3251005187630653, Final Batch Loss: 0.09315479546785355\n",
      "Epoch 6719, Loss: 1.4132704734802246, Final Batch Loss: 0.3355855941772461\n",
      "Epoch 6720, Loss: 1.8657229244709015, Final Batch Loss: 0.7288447618484497\n",
      "Epoch 6721, Loss: 1.1581701636314392, Final Batch Loss: 0.1679537147283554\n",
      "Epoch 6722, Loss: 1.373190313577652, Final Batch Loss: 0.20784489810466766\n",
      "Epoch 6723, Loss: 1.3807381391525269, Final Batch Loss: 0.3421458303928375\n",
      "Epoch 6724, Loss: 1.3709711730480194, Final Batch Loss: 0.22792977094650269\n",
      "Epoch 6725, Loss: 1.4338886737823486, Final Batch Loss: 0.2686370611190796\n",
      "Epoch 6726, Loss: 1.4100923836231232, Final Batch Loss: 0.30181968212127686\n",
      "Epoch 6727, Loss: 1.41362726688385, Final Batch Loss: 0.4204065799713135\n",
      "Epoch 6728, Loss: 1.405070036649704, Final Batch Loss: 0.14451786875724792\n",
      "Epoch 6729, Loss: 1.3136544078588486, Final Batch Loss: 0.20513761043548584\n",
      "Epoch 6730, Loss: 1.2918637990951538, Final Batch Loss: 0.1842367947101593\n",
      "Epoch 6731, Loss: 1.262574315071106, Final Batch Loss: 0.26086801290512085\n",
      "Epoch 6732, Loss: 1.4693135023117065, Final Batch Loss: 0.32150110602378845\n",
      "Epoch 6733, Loss: 1.3152553737163544, Final Batch Loss: 0.34241682291030884\n",
      "Epoch 6734, Loss: 1.2164331525564194, Final Batch Loss: 0.16865022480487823\n",
      "Epoch 6735, Loss: 1.3307582139968872, Final Batch Loss: 0.30027300119400024\n",
      "Epoch 6736, Loss: 1.4763657003641129, Final Batch Loss: 0.4601943790912628\n",
      "Epoch 6737, Loss: 1.4630461633205414, Final Batch Loss: 0.3879406452178955\n",
      "Epoch 6738, Loss: 1.3018678724765778, Final Batch Loss: 0.20720240473747253\n",
      "Epoch 6739, Loss: 1.4809224009513855, Final Batch Loss: 0.28904789686203003\n",
      "Epoch 6740, Loss: 1.368500292301178, Final Batch Loss: 0.2250513881444931\n",
      "Epoch 6741, Loss: 1.4437115341424942, Final Batch Loss: 0.1978980153799057\n",
      "Epoch 6742, Loss: 1.2288582026958466, Final Batch Loss: 0.2105216234922409\n",
      "Epoch 6743, Loss: 1.3222365528345108, Final Batch Loss: 0.25405919551849365\n",
      "Epoch 6744, Loss: 1.409765675663948, Final Batch Loss: 0.2912006676197052\n",
      "Epoch 6745, Loss: 1.2258711457252502, Final Batch Loss: 0.1915351152420044\n",
      "Epoch 6746, Loss: 1.3763497322797775, Final Batch Loss: 0.15330134332180023\n",
      "Epoch 6747, Loss: 1.2888512015342712, Final Batch Loss: 0.2677842974662781\n",
      "Epoch 6748, Loss: 1.5384829342365265, Final Batch Loss: 0.2730308175086975\n",
      "Epoch 6749, Loss: 1.3140507489442825, Final Batch Loss: 0.31128862500190735\n",
      "Epoch 6750, Loss: 1.4173523783683777, Final Batch Loss: 0.3346238136291504\n",
      "Epoch 6751, Loss: 1.3137243688106537, Final Batch Loss: 0.24905605614185333\n",
      "Epoch 6752, Loss: 1.2523567080497742, Final Batch Loss: 0.09796030819416046\n",
      "Epoch 6753, Loss: 1.6236655712127686, Final Batch Loss: 0.32835546135902405\n",
      "Epoch 6754, Loss: 1.2972348481416702, Final Batch Loss: 0.3170034885406494\n",
      "Epoch 6755, Loss: 1.526938036084175, Final Batch Loss: 0.2206687480211258\n",
      "Epoch 6756, Loss: 1.4832015335559845, Final Batch Loss: 0.4200810492038727\n",
      "Epoch 6757, Loss: 1.4235083758831024, Final Batch Loss: 0.3301253616809845\n",
      "Epoch 6758, Loss: 1.3058532029390335, Final Batch Loss: 0.2344931811094284\n",
      "Epoch 6759, Loss: 1.2136651128530502, Final Batch Loss: 0.23272499442100525\n",
      "Epoch 6760, Loss: 1.3954933285713196, Final Batch Loss: 0.31819790601730347\n",
      "Epoch 6761, Loss: 1.3325838893651962, Final Batch Loss: 0.31453534960746765\n",
      "Epoch 6762, Loss: 1.3724739998579025, Final Batch Loss: 0.2933790981769562\n",
      "Epoch 6763, Loss: 1.32273468375206, Final Batch Loss: 0.22014454007148743\n",
      "Epoch 6764, Loss: 1.4648596942424774, Final Batch Loss: 0.3613303601741791\n",
      "Epoch 6765, Loss: 1.5315444469451904, Final Batch Loss: 0.3595530688762665\n",
      "Epoch 6766, Loss: 1.5772381573915482, Final Batch Loss: 0.4359092712402344\n",
      "Epoch 6767, Loss: 1.4337086826562881, Final Batch Loss: 0.27592694759368896\n",
      "Epoch 6768, Loss: 1.3702408969402313, Final Batch Loss: 0.16560177505016327\n",
      "Epoch 6769, Loss: 1.4326506108045578, Final Batch Loss: 0.2982172966003418\n",
      "Epoch 6770, Loss: 1.2366679310798645, Final Batch Loss: 0.25149938464164734\n",
      "Epoch 6771, Loss: 1.4539985358715057, Final Batch Loss: 0.3623325526714325\n",
      "Epoch 6772, Loss: 1.298785760998726, Final Batch Loss: 0.25999853014945984\n",
      "Epoch 6773, Loss: 1.5679751932621002, Final Batch Loss: 0.3419627845287323\n",
      "Epoch 6774, Loss: 1.321794018149376, Final Batch Loss: 0.2549331486225128\n",
      "Epoch 6775, Loss: 1.3181971162557602, Final Batch Loss: 0.2221745103597641\n",
      "Epoch 6776, Loss: 1.384553611278534, Final Batch Loss: 0.3390651345252991\n",
      "Epoch 6777, Loss: 1.5432072430849075, Final Batch Loss: 0.4321501553058624\n",
      "Epoch 6778, Loss: 1.301043689250946, Final Batch Loss: 0.21677790582180023\n",
      "Epoch 6779, Loss: 1.1919299364089966, Final Batch Loss: 0.11579188704490662\n",
      "Epoch 6780, Loss: 1.4226764738559723, Final Batch Loss: 0.3108137249946594\n",
      "Epoch 6781, Loss: 1.1924237459897995, Final Batch Loss: 0.13687045872211456\n",
      "Epoch 6782, Loss: 1.392835021018982, Final Batch Loss: 0.21123021841049194\n",
      "Epoch 6783, Loss: 1.1781314611434937, Final Batch Loss: 0.19429531693458557\n",
      "Epoch 6784, Loss: 1.231581762433052, Final Batch Loss: 0.19403137266635895\n",
      "Epoch 6785, Loss: 1.2602181136608124, Final Batch Loss: 0.2840544581413269\n",
      "Epoch 6786, Loss: 1.2489604353904724, Final Batch Loss: 0.16188909113407135\n",
      "Epoch 6787, Loss: 1.3677342981100082, Final Batch Loss: 0.1899668276309967\n",
      "Epoch 6788, Loss: 1.279261365532875, Final Batch Loss: 0.17653395235538483\n",
      "Epoch 6789, Loss: 1.2197940796613693, Final Batch Loss: 0.15558084845542908\n",
      "Epoch 6790, Loss: 1.607386976480484, Final Batch Loss: 0.38769328594207764\n",
      "Epoch 6791, Loss: 1.2923636138439178, Final Batch Loss: 0.13383141160011292\n",
      "Epoch 6792, Loss: 1.191533774137497, Final Batch Loss: 0.14525453746318817\n",
      "Epoch 6793, Loss: 1.3474761247634888, Final Batch Loss: 0.2479475438594818\n",
      "Epoch 6794, Loss: 1.4382508546113968, Final Batch Loss: 0.2893691062927246\n",
      "Epoch 6795, Loss: 1.5757892578840256, Final Batch Loss: 0.4821901321411133\n",
      "Epoch 6796, Loss: 1.5739330649375916, Final Batch Loss: 0.39232033491134644\n",
      "Epoch 6797, Loss: 1.4030259549617767, Final Batch Loss: 0.2663181722164154\n",
      "Epoch 6798, Loss: 1.492904618382454, Final Batch Loss: 0.29722315073013306\n",
      "Epoch 6799, Loss: 1.3885745406150818, Final Batch Loss: 0.2116450071334839\n",
      "Epoch 6800, Loss: 1.732278198003769, Final Batch Loss: 0.45584002137184143\n",
      "Epoch 6801, Loss: 1.485062226653099, Final Batch Loss: 0.4209216237068176\n",
      "Epoch 6802, Loss: 1.4520892202854156, Final Batch Loss: 0.3309774100780487\n",
      "Epoch 6803, Loss: 1.1752825379371643, Final Batch Loss: 0.21652501821517944\n",
      "Epoch 6804, Loss: 1.393917053937912, Final Batch Loss: 0.2914654016494751\n",
      "Epoch 6805, Loss: 1.1784830540418625, Final Batch Loss: 0.12714534997940063\n",
      "Epoch 6806, Loss: 1.3327604532241821, Final Batch Loss: 0.25523921847343445\n",
      "Epoch 6807, Loss: 1.3690522462129593, Final Batch Loss: 0.2000332772731781\n",
      "Epoch 6808, Loss: 1.4516710489988327, Final Batch Loss: 0.3207455277442932\n",
      "Epoch 6809, Loss: 1.4377051889896393, Final Batch Loss: 0.19330769777297974\n",
      "Epoch 6810, Loss: 1.3731147050857544, Final Batch Loss: 0.25848984718322754\n",
      "Epoch 6811, Loss: 1.3793288320302963, Final Batch Loss: 0.22483374178409576\n",
      "Epoch 6812, Loss: 1.3479198813438416, Final Batch Loss: 0.31173765659332275\n",
      "Epoch 6813, Loss: 1.1767453253269196, Final Batch Loss: 0.14396488666534424\n",
      "Epoch 6814, Loss: 1.4408806711435318, Final Batch Loss: 0.35582083463668823\n",
      "Epoch 6815, Loss: 1.2310620695352554, Final Batch Loss: 0.25437772274017334\n",
      "Epoch 6816, Loss: 1.4784955531358719, Final Batch Loss: 0.272809237241745\n",
      "Epoch 6817, Loss: 1.4846224188804626, Final Batch Loss: 0.29448607563972473\n",
      "Epoch 6818, Loss: 1.3646746426820755, Final Batch Loss: 0.3261416554450989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6819, Loss: 1.294764220714569, Final Batch Loss: 0.1898471713066101\n",
      "Epoch 6820, Loss: 1.4591785818338394, Final Batch Loss: 0.35060766339302063\n",
      "Epoch 6821, Loss: 1.3756541907787323, Final Batch Loss: 0.35960832238197327\n",
      "Epoch 6822, Loss: 1.4016234874725342, Final Batch Loss: 0.2847212553024292\n",
      "Epoch 6823, Loss: 1.197419360280037, Final Batch Loss: 0.21423989534378052\n",
      "Epoch 6824, Loss: 1.3122472316026688, Final Batch Loss: 0.29528218507766724\n",
      "Epoch 6825, Loss: 1.5018046200275421, Final Batch Loss: 0.3236037492752075\n",
      "Epoch 6826, Loss: 1.3840762823820114, Final Batch Loss: 0.3788384199142456\n",
      "Epoch 6827, Loss: 1.4240258485078812, Final Batch Loss: 0.30172523856163025\n",
      "Epoch 6828, Loss: 1.5667768716812134, Final Batch Loss: 0.4367815852165222\n",
      "Epoch 6829, Loss: 1.4392355680465698, Final Batch Loss: 0.28092244267463684\n",
      "Epoch 6830, Loss: 1.4333645701408386, Final Batch Loss: 0.281028687953949\n",
      "Epoch 6831, Loss: 1.4123074412345886, Final Batch Loss: 0.39231178164482117\n",
      "Epoch 6832, Loss: 1.1852446049451828, Final Batch Loss: 0.203187495470047\n",
      "Epoch 6833, Loss: 1.3753680139780045, Final Batch Loss: 0.2846570611000061\n",
      "Epoch 6834, Loss: 1.4018585830926895, Final Batch Loss: 0.3258430063724518\n",
      "Epoch 6835, Loss: 1.160213127732277, Final Batch Loss: 0.18988679349422455\n",
      "Epoch 6836, Loss: 1.4684743285179138, Final Batch Loss: 0.2612292468547821\n",
      "Epoch 6837, Loss: 1.3446028530597687, Final Batch Loss: 0.18146511912345886\n",
      "Epoch 6838, Loss: 1.42820605635643, Final Batch Loss: 0.22006918489933014\n",
      "Epoch 6839, Loss: 1.2714355885982513, Final Batch Loss: 0.24442404508590698\n",
      "Epoch 6840, Loss: 1.1505675166845322, Final Batch Loss: 0.1884251981973648\n",
      "Epoch 6841, Loss: 1.1659161746501923, Final Batch Loss: 0.19698458909988403\n",
      "Epoch 6842, Loss: 1.2966318428516388, Final Batch Loss: 0.2308526188135147\n",
      "Epoch 6843, Loss: 1.2127492129802704, Final Batch Loss: 0.18407514691352844\n",
      "Epoch 6844, Loss: 1.2738438695669174, Final Batch Loss: 0.20610521733760834\n",
      "Epoch 6845, Loss: 1.4532352983951569, Final Batch Loss: 0.24476301670074463\n",
      "Epoch 6846, Loss: 1.363745704293251, Final Batch Loss: 0.3216705322265625\n",
      "Epoch 6847, Loss: 1.2974505126476288, Final Batch Loss: 0.23016932606697083\n",
      "Epoch 6848, Loss: 1.6602312326431274, Final Batch Loss: 0.2973969876766205\n",
      "Epoch 6849, Loss: 1.2543227523565292, Final Batch Loss: 0.3466300666332245\n",
      "Epoch 6850, Loss: 1.2339706867933273, Final Batch Loss: 0.24155007302761078\n",
      "Epoch 6851, Loss: 1.3521952629089355, Final Batch Loss: 0.24282415211200714\n",
      "Epoch 6852, Loss: 1.5349857211112976, Final Batch Loss: 0.3194831907749176\n",
      "Epoch 6853, Loss: 1.2444195747375488, Final Batch Loss: 0.2090168446302414\n",
      "Epoch 6854, Loss: 1.3298469483852386, Final Batch Loss: 0.17469090223312378\n",
      "Epoch 6855, Loss: 1.280250072479248, Final Batch Loss: 0.25927358865737915\n",
      "Epoch 6856, Loss: 1.3902502357959747, Final Batch Loss: 0.26932621002197266\n",
      "Epoch 6857, Loss: 1.279902160167694, Final Batch Loss: 0.23592934012413025\n",
      "Epoch 6858, Loss: 1.2317399680614471, Final Batch Loss: 0.22046934068202972\n",
      "Epoch 6859, Loss: 1.4282298684120178, Final Batch Loss: 0.3619076609611511\n",
      "Epoch 6860, Loss: 1.3545577675104141, Final Batch Loss: 0.23410479724407196\n",
      "Epoch 6861, Loss: 1.3746225237846375, Final Batch Loss: 0.28328174352645874\n",
      "Epoch 6862, Loss: 1.168748289346695, Final Batch Loss: 0.2698521316051483\n",
      "Epoch 6863, Loss: 1.3653787970542908, Final Batch Loss: 0.14480042457580566\n",
      "Epoch 6864, Loss: 1.5232502222061157, Final Batch Loss: 0.365495890378952\n",
      "Epoch 6865, Loss: 1.31697379052639, Final Batch Loss: 0.24009379744529724\n",
      "Epoch 6866, Loss: 1.6977331936359406, Final Batch Loss: 0.571670413017273\n",
      "Epoch 6867, Loss: 1.4515486806631088, Final Batch Loss: 0.3069501519203186\n",
      "Epoch 6868, Loss: 1.2473626285791397, Final Batch Loss: 0.10937345027923584\n",
      "Epoch 6869, Loss: 1.3713231831789017, Final Batch Loss: 0.16222690045833588\n",
      "Epoch 6870, Loss: 1.398776650428772, Final Batch Loss: 0.24887904524803162\n",
      "Epoch 6871, Loss: 1.3237285017967224, Final Batch Loss: 0.336669921875\n",
      "Epoch 6872, Loss: 1.395958051085472, Final Batch Loss: 0.322723925113678\n",
      "Epoch 6873, Loss: 1.2460022270679474, Final Batch Loss: 0.2615118622779846\n",
      "Epoch 6874, Loss: 1.3807986676692963, Final Batch Loss: 0.3357393145561218\n",
      "Epoch 6875, Loss: 1.4722995162010193, Final Batch Loss: 0.28132814168930054\n",
      "Epoch 6876, Loss: 1.3326009809970856, Final Batch Loss: 0.21335788071155548\n",
      "Epoch 6877, Loss: 1.3460235446691513, Final Batch Loss: 0.2862262427806854\n",
      "Epoch 6878, Loss: 1.473637118935585, Final Batch Loss: 0.2327403575181961\n",
      "Epoch 6879, Loss: 1.2587235271930695, Final Batch Loss: 0.19172403216362\n",
      "Epoch 6880, Loss: 1.441420465707779, Final Batch Loss: 0.2924381494522095\n",
      "Epoch 6881, Loss: 1.3846344500780106, Final Batch Loss: 0.17619000375270844\n",
      "Epoch 6882, Loss: 1.2795532494783401, Final Batch Loss: 0.27006959915161133\n",
      "Epoch 6883, Loss: 1.404106929898262, Final Batch Loss: 0.26724979281425476\n",
      "Epoch 6884, Loss: 1.2932805716991425, Final Batch Loss: 0.18714997172355652\n",
      "Epoch 6885, Loss: 1.2864906936883926, Final Batch Loss: 0.14598499238491058\n",
      "Epoch 6886, Loss: 1.3048891127109528, Final Batch Loss: 0.13841816782951355\n",
      "Epoch 6887, Loss: 1.6064808368682861, Final Batch Loss: 0.3105553686618805\n",
      "Epoch 6888, Loss: 1.4256083816289902, Final Batch Loss: 0.346462607383728\n",
      "Epoch 6889, Loss: 1.286493495106697, Final Batch Loss: 0.25260525941848755\n",
      "Epoch 6890, Loss: 1.4033931195735931, Final Batch Loss: 0.34807565808296204\n",
      "Epoch 6891, Loss: 1.2970616519451141, Final Batch Loss: 0.34012559056282043\n",
      "Epoch 6892, Loss: 1.338075876235962, Final Batch Loss: 0.2531472444534302\n",
      "Epoch 6893, Loss: 1.2147352993488312, Final Batch Loss: 0.2062554657459259\n",
      "Epoch 6894, Loss: 1.3334195762872696, Final Batch Loss: 0.21281810104846954\n",
      "Epoch 6895, Loss: 1.7158523201942444, Final Batch Loss: 0.461626261472702\n",
      "Epoch 6896, Loss: 1.2225471884012222, Final Batch Loss: 0.23748914897441864\n",
      "Epoch 6897, Loss: 1.2353697568178177, Final Batch Loss: 0.17507238686084747\n",
      "Epoch 6898, Loss: 1.3821151405572891, Final Batch Loss: 0.2586284875869751\n",
      "Epoch 6899, Loss: 1.2784168124198914, Final Batch Loss: 0.18555982410907745\n",
      "Epoch 6900, Loss: 1.382407233119011, Final Batch Loss: 0.3069719672203064\n",
      "Epoch 6901, Loss: 1.3270948827266693, Final Batch Loss: 0.18683278560638428\n",
      "Epoch 6902, Loss: 1.3019190728664398, Final Batch Loss: 0.22720059752464294\n",
      "Epoch 6903, Loss: 1.2845723927021027, Final Batch Loss: 0.22016775608062744\n",
      "Epoch 6904, Loss: 1.4025921672582626, Final Batch Loss: 0.2693789601325989\n",
      "Epoch 6905, Loss: 1.373341143131256, Final Batch Loss: 0.24382859468460083\n",
      "Epoch 6906, Loss: 1.3444822132587433, Final Batch Loss: 0.285047322511673\n",
      "Epoch 6907, Loss: 1.4705656319856644, Final Batch Loss: 0.30986014008522034\n",
      "Epoch 6908, Loss: 1.2739671468734741, Final Batch Loss: 0.2523050010204315\n",
      "Epoch 6909, Loss: 1.3191888183355331, Final Batch Loss: 0.2399044781923294\n",
      "Epoch 6910, Loss: 1.5161657631397247, Final Batch Loss: 0.251869797706604\n",
      "Epoch 6911, Loss: 1.515434056520462, Final Batch Loss: 0.4141247570514679\n",
      "Epoch 6912, Loss: 1.2779686152935028, Final Batch Loss: 0.33957403898239136\n",
      "Epoch 6913, Loss: 1.4165393561124802, Final Batch Loss: 0.23829425871372223\n",
      "Epoch 6914, Loss: 1.4211168885231018, Final Batch Loss: 0.3382529616355896\n",
      "Epoch 6915, Loss: 1.550095558166504, Final Batch Loss: 0.3082442283630371\n",
      "Epoch 6916, Loss: 1.2730358839035034, Final Batch Loss: 0.2261172980070114\n",
      "Epoch 6917, Loss: 1.4906488806009293, Final Batch Loss: 0.2933797836303711\n",
      "Epoch 6918, Loss: 1.3469132781028748, Final Batch Loss: 0.2917829751968384\n",
      "Epoch 6919, Loss: 1.2434069961309433, Final Batch Loss: 0.24910421669483185\n",
      "Epoch 6920, Loss: 1.2406343817710876, Final Batch Loss: 0.20092962682247162\n",
      "Epoch 6921, Loss: 1.191404566168785, Final Batch Loss: 0.2270776331424713\n",
      "Epoch 6922, Loss: 1.3397786021232605, Final Batch Loss: 0.20622041821479797\n",
      "Epoch 6923, Loss: 1.2300028949975967, Final Batch Loss: 0.19723859429359436\n",
      "Epoch 6924, Loss: 1.344239056110382, Final Batch Loss: 0.2699461281299591\n",
      "Epoch 6925, Loss: 1.2242152243852615, Final Batch Loss: 0.15554054081439972\n",
      "Epoch 6926, Loss: 1.1933030188083649, Final Batch Loss: 0.20322009921073914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6927, Loss: 1.4160793721675873, Final Batch Loss: 0.27856892347335815\n",
      "Epoch 6928, Loss: 1.2111465483903885, Final Batch Loss: 0.23547281324863434\n",
      "Epoch 6929, Loss: 1.1787504255771637, Final Batch Loss: 0.20620696246623993\n",
      "Epoch 6930, Loss: 1.3877191990613937, Final Batch Loss: 0.3112535774707794\n",
      "Epoch 6931, Loss: 1.3973639756441116, Final Batch Loss: 0.2597605586051941\n",
      "Epoch 6932, Loss: 1.3731756806373596, Final Batch Loss: 0.37079715728759766\n",
      "Epoch 6933, Loss: 1.3263949006795883, Final Batch Loss: 0.20664216578006744\n",
      "Epoch 6934, Loss: 1.4444594979286194, Final Batch Loss: 0.2105959802865982\n",
      "Epoch 6935, Loss: 1.296335756778717, Final Batch Loss: 0.3042221963405609\n",
      "Epoch 6936, Loss: 1.4464713335037231, Final Batch Loss: 0.27922359108924866\n",
      "Epoch 6937, Loss: 1.3904148489236832, Final Batch Loss: 0.29938045144081116\n",
      "Epoch 6938, Loss: 1.6653298139572144, Final Batch Loss: 0.503044068813324\n",
      "Epoch 6939, Loss: 1.223780483007431, Final Batch Loss: 0.20049796998500824\n",
      "Epoch 6940, Loss: 1.3746159821748734, Final Batch Loss: 0.3174701929092407\n",
      "Epoch 6941, Loss: 1.3697453439235687, Final Batch Loss: 0.16744504868984222\n",
      "Epoch 6942, Loss: 1.2436067163944244, Final Batch Loss: 0.1311674416065216\n",
      "Epoch 6943, Loss: 1.3703855574131012, Final Batch Loss: 0.3391621708869934\n",
      "Epoch 6944, Loss: 1.4994221478700638, Final Batch Loss: 0.2782404124736786\n",
      "Epoch 6945, Loss: 1.5413554161787033, Final Batch Loss: 0.2090875506401062\n",
      "Epoch 6946, Loss: 1.3692715615034103, Final Batch Loss: 0.23496976494789124\n",
      "Epoch 6947, Loss: 1.4551692306995392, Final Batch Loss: 0.25471261143684387\n",
      "Epoch 6948, Loss: 1.2926391810178757, Final Batch Loss: 0.21573007106781006\n",
      "Epoch 6949, Loss: 1.3073098957538605, Final Batch Loss: 0.2506380081176758\n",
      "Epoch 6950, Loss: 1.4146579056978226, Final Batch Loss: 0.27364370226860046\n",
      "Epoch 6951, Loss: 1.21822889149189, Final Batch Loss: 0.24455875158309937\n",
      "Epoch 6952, Loss: 1.2409311830997467, Final Batch Loss: 0.18560712039470673\n",
      "Epoch 6953, Loss: 1.2798297852277756, Final Batch Loss: 0.19030450284481049\n",
      "Epoch 6954, Loss: 1.1875055879354477, Final Batch Loss: 0.2603192925453186\n",
      "Epoch 6955, Loss: 1.3079333156347275, Final Batch Loss: 0.2560761570930481\n",
      "Epoch 6956, Loss: 1.2465392649173737, Final Batch Loss: 0.1604415327310562\n",
      "Epoch 6957, Loss: 1.6003029495477676, Final Batch Loss: 0.5284539461135864\n",
      "Epoch 6958, Loss: 1.3158735036849976, Final Batch Loss: 0.15865013003349304\n",
      "Epoch 6959, Loss: 1.643671452999115, Final Batch Loss: 0.4901787042617798\n",
      "Epoch 6960, Loss: 1.3828963488340378, Final Batch Loss: 0.2635400891304016\n",
      "Epoch 6961, Loss: 1.463110014796257, Final Batch Loss: 0.34081780910491943\n",
      "Epoch 6962, Loss: 1.32805997133255, Final Batch Loss: 0.25867173075675964\n",
      "Epoch 6963, Loss: 1.3123846650123596, Final Batch Loss: 0.17669887840747833\n",
      "Epoch 6964, Loss: 1.4929721504449844, Final Batch Loss: 0.2248329520225525\n",
      "Epoch 6965, Loss: 1.331388533115387, Final Batch Loss: 0.3166755437850952\n",
      "Epoch 6966, Loss: 1.16245499253273, Final Batch Loss: 0.1862654685974121\n",
      "Epoch 6967, Loss: 1.3975879102945328, Final Batch Loss: 0.25318342447280884\n",
      "Epoch 6968, Loss: 1.4930055141448975, Final Batch Loss: 0.21313592791557312\n",
      "Epoch 6969, Loss: 1.2887645810842514, Final Batch Loss: 0.23153352737426758\n",
      "Epoch 6970, Loss: 1.2874009013175964, Final Batch Loss: 0.24769850075244904\n",
      "Epoch 6971, Loss: 1.1908211708068848, Final Batch Loss: 0.15881413221359253\n",
      "Epoch 6972, Loss: 1.3591649383306503, Final Batch Loss: 0.23577940464019775\n",
      "Epoch 6973, Loss: 1.2643246799707413, Final Batch Loss: 0.23116770386695862\n",
      "Epoch 6974, Loss: 1.0806878358125687, Final Batch Loss: 0.17686496675014496\n",
      "Epoch 6975, Loss: 1.1756602078676224, Final Batch Loss: 0.18952420353889465\n",
      "Epoch 6976, Loss: 1.269110083580017, Final Batch Loss: 0.15055705606937408\n",
      "Epoch 6977, Loss: 1.2335674166679382, Final Batch Loss: 0.16410185396671295\n",
      "Epoch 6978, Loss: 1.235392078757286, Final Batch Loss: 0.30328088998794556\n",
      "Epoch 6979, Loss: 1.3479005098342896, Final Batch Loss: 0.20112217962741852\n",
      "Epoch 6980, Loss: 1.2887801229953766, Final Batch Loss: 0.15243485569953918\n",
      "Epoch 6981, Loss: 1.2666861563920975, Final Batch Loss: 0.2105116844177246\n",
      "Epoch 6982, Loss: 1.3324890434741974, Final Batch Loss: 0.23228491842746735\n",
      "Epoch 6983, Loss: 1.3086812943220139, Final Batch Loss: 0.3136507570743561\n",
      "Epoch 6984, Loss: 1.2124653905630112, Final Batch Loss: 0.2173381894826889\n",
      "Epoch 6985, Loss: 1.223653420805931, Final Batch Loss: 0.14784322679042816\n",
      "Epoch 6986, Loss: 1.506602019071579, Final Batch Loss: 0.2786813974380493\n",
      "Epoch 6987, Loss: 1.32472562789917, Final Batch Loss: 0.29377371072769165\n",
      "Epoch 6988, Loss: 1.4040828794240952, Final Batch Loss: 0.24176499247550964\n",
      "Epoch 6989, Loss: 1.3386486023664474, Final Batch Loss: 0.2776432931423187\n",
      "Epoch 6990, Loss: 1.1656043827533722, Final Batch Loss: 0.1407807320356369\n",
      "Epoch 6991, Loss: 1.3873700499534607, Final Batch Loss: 0.2781274914741516\n",
      "Epoch 6992, Loss: 1.2316273599863052, Final Batch Loss: 0.10601909458637238\n",
      "Epoch 6993, Loss: 1.663163185119629, Final Batch Loss: 0.39292091131210327\n",
      "Epoch 6994, Loss: 1.4129064679145813, Final Batch Loss: 0.31379133462905884\n",
      "Epoch 6995, Loss: 1.42934350669384, Final Batch Loss: 0.3507496416568756\n",
      "Epoch 6996, Loss: 1.467665210366249, Final Batch Loss: 0.3669081926345825\n",
      "Epoch 6997, Loss: 1.2348476350307465, Final Batch Loss: 0.1269119530916214\n",
      "Epoch 6998, Loss: 1.2650188356637955, Final Batch Loss: 0.22651654481887817\n",
      "Epoch 6999, Loss: 1.4383635967969894, Final Batch Loss: 0.38555940985679626\n",
      "Epoch 7000, Loss: 1.303930401802063, Final Batch Loss: 0.2622731328010559\n",
      "Epoch 7001, Loss: 1.4800474345684052, Final Batch Loss: 0.27828535437583923\n",
      "Epoch 7002, Loss: 1.4633973836898804, Final Batch Loss: 0.4118407666683197\n",
      "Epoch 7003, Loss: 1.2914787083864212, Final Batch Loss: 0.2563254237174988\n",
      "Epoch 7004, Loss: 1.5893451273441315, Final Batch Loss: 0.2998083531856537\n",
      "Epoch 7005, Loss: 1.30855792760849, Final Batch Loss: 0.25495490431785583\n",
      "Epoch 7006, Loss: 1.3110359162092209, Final Batch Loss: 0.2623669505119324\n",
      "Epoch 7007, Loss: 1.3396962136030197, Final Batch Loss: 0.3041653037071228\n",
      "Epoch 7008, Loss: 1.3759837001562119, Final Batch Loss: 0.18390221893787384\n",
      "Epoch 7009, Loss: 1.4225210398435593, Final Batch Loss: 0.252120703458786\n",
      "Epoch 7010, Loss: 1.1735376715660095, Final Batch Loss: 0.18297326564788818\n",
      "Epoch 7011, Loss: 1.5364467203617096, Final Batch Loss: 0.3825978934764862\n",
      "Epoch 7012, Loss: 1.1489877849817276, Final Batch Loss: 0.2924306094646454\n",
      "Epoch 7013, Loss: 1.6239182949066162, Final Batch Loss: 0.4065493941307068\n",
      "Epoch 7014, Loss: 1.37225241959095, Final Batch Loss: 0.21902517974376678\n",
      "Epoch 7015, Loss: 1.4591519832611084, Final Batch Loss: 0.2735833525657654\n",
      "Epoch 7016, Loss: 1.1616901457309723, Final Batch Loss: 0.13347385823726654\n",
      "Epoch 7017, Loss: 1.3167105317115784, Final Batch Loss: 0.20693840086460114\n",
      "Epoch 7018, Loss: 1.218955159187317, Final Batch Loss: 0.3356071412563324\n",
      "Epoch 7019, Loss: 1.3953426033258438, Final Batch Loss: 0.33396875858306885\n",
      "Epoch 7020, Loss: 1.4331358969211578, Final Batch Loss: 0.3170975148677826\n",
      "Epoch 7021, Loss: 1.5294916480779648, Final Batch Loss: 0.5164477825164795\n",
      "Epoch 7022, Loss: 1.6015642583370209, Final Batch Loss: 0.3912079632282257\n",
      "Epoch 7023, Loss: 1.2511465400457382, Final Batch Loss: 0.1562841236591339\n",
      "Epoch 7024, Loss: 1.3246647268533707, Final Batch Loss: 0.27458587288856506\n",
      "Epoch 7025, Loss: 1.4490866959095001, Final Batch Loss: 0.3707940876483917\n",
      "Epoch 7026, Loss: 1.3593068569898605, Final Batch Loss: 0.4252464473247528\n",
      "Epoch 7027, Loss: 1.2566836327314377, Final Batch Loss: 0.18271856009960175\n",
      "Epoch 7028, Loss: 1.4688169956207275, Final Batch Loss: 0.23440435528755188\n",
      "Epoch 7029, Loss: 1.552252545952797, Final Batch Loss: 0.36421650648117065\n",
      "Epoch 7030, Loss: 1.292451947927475, Final Batch Loss: 0.2516447901725769\n",
      "Epoch 7031, Loss: 1.3407317996025085, Final Batch Loss: 0.3103271722793579\n",
      "Epoch 7032, Loss: 1.3601755797863007, Final Batch Loss: 0.27210187911987305\n",
      "Epoch 7033, Loss: 1.3624665439128876, Final Batch Loss: 0.1795438975095749\n",
      "Epoch 7034, Loss: 1.4754217863082886, Final Batch Loss: 0.40249103307724\n",
      "Epoch 7035, Loss: 1.359836831688881, Final Batch Loss: 0.26348215341567993\n",
      "Epoch 7036, Loss: 1.2816516160964966, Final Batch Loss: 0.19825004041194916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7037, Loss: 1.4057652652263641, Final Batch Loss: 0.25790876150131226\n",
      "Epoch 7038, Loss: 1.4996805489063263, Final Batch Loss: 0.33825019001960754\n",
      "Epoch 7039, Loss: 1.4688507914543152, Final Batch Loss: 0.4260420501232147\n",
      "Epoch 7040, Loss: 1.2252621948719025, Final Batch Loss: 0.23180027306079865\n",
      "Epoch 7041, Loss: 1.428413987159729, Final Batch Loss: 0.2920374274253845\n",
      "Epoch 7042, Loss: 1.1951960772275925, Final Batch Loss: 0.2064814567565918\n",
      "Epoch 7043, Loss: 1.3306578695774078, Final Batch Loss: 0.24015513062477112\n",
      "Epoch 7044, Loss: 1.2101756483316422, Final Batch Loss: 0.17759229242801666\n",
      "Epoch 7045, Loss: 1.2616877406835556, Final Batch Loss: 0.2903434634208679\n",
      "Epoch 7046, Loss: 1.1912733241915703, Final Batch Loss: 0.11896372586488724\n",
      "Epoch 7047, Loss: 1.3804068714380264, Final Batch Loss: 0.23351740837097168\n",
      "Epoch 7048, Loss: 1.311784952878952, Final Batch Loss: 0.1748088300228119\n",
      "Epoch 7049, Loss: 1.3479813188314438, Final Batch Loss: 0.17284436523914337\n",
      "Epoch 7050, Loss: 1.442994087934494, Final Batch Loss: 0.2699642777442932\n",
      "Epoch 7051, Loss: 1.3334221988916397, Final Batch Loss: 0.30206629633903503\n",
      "Epoch 7052, Loss: 1.2984710782766342, Final Batch Loss: 0.28801339864730835\n",
      "Epoch 7053, Loss: 1.4967601299285889, Final Batch Loss: 0.3460790812969208\n",
      "Epoch 7054, Loss: 1.4236007332801819, Final Batch Loss: 0.22695156931877136\n",
      "Epoch 7055, Loss: 1.6580591797828674, Final Batch Loss: 0.36489683389663696\n",
      "Epoch 7056, Loss: 1.5168445259332657, Final Batch Loss: 0.40875282883644104\n",
      "Epoch 7057, Loss: 1.4840379804372787, Final Batch Loss: 0.44756677746772766\n",
      "Epoch 7058, Loss: 1.2882664054632187, Final Batch Loss: 0.17565475404262543\n",
      "Epoch 7059, Loss: 1.3425929844379425, Final Batch Loss: 0.249111145734787\n",
      "Epoch 7060, Loss: 1.3509402573108673, Final Batch Loss: 0.3407003581523895\n",
      "Epoch 7061, Loss: 1.1039191633462906, Final Batch Loss: 0.15091562271118164\n",
      "Epoch 7062, Loss: 1.247061774134636, Final Batch Loss: 0.22428502142429352\n",
      "Epoch 7063, Loss: 1.526223286986351, Final Batch Loss: 0.12700338661670685\n",
      "Epoch 7064, Loss: 1.506594493985176, Final Batch Loss: 0.43395212292671204\n",
      "Epoch 7065, Loss: 1.5770090222358704, Final Batch Loss: 0.4679541289806366\n",
      "Epoch 7066, Loss: 1.426725447177887, Final Batch Loss: 0.38153645396232605\n",
      "Epoch 7067, Loss: 1.4169597923755646, Final Batch Loss: 0.2838703393936157\n",
      "Epoch 7068, Loss: 1.3720713779330254, Final Batch Loss: 0.10973610728979111\n",
      "Epoch 7069, Loss: 1.1846217215061188, Final Batch Loss: 0.1761869639158249\n",
      "Epoch 7070, Loss: 1.2363285720348358, Final Batch Loss: 0.20890535414218903\n",
      "Epoch 7071, Loss: 1.5572001188993454, Final Batch Loss: 0.4338950514793396\n",
      "Epoch 7072, Loss: 1.242618054151535, Final Batch Loss: 0.24326688051223755\n",
      "Epoch 7073, Loss: 1.408913642168045, Final Batch Loss: 0.27396684885025024\n",
      "Epoch 7074, Loss: 1.3770550638437271, Final Batch Loss: 0.24910728633403778\n",
      "Epoch 7075, Loss: 1.5293373614549637, Final Batch Loss: 0.33579114079475403\n",
      "Epoch 7076, Loss: 1.329204335808754, Final Batch Loss: 0.32007357478141785\n",
      "Epoch 7077, Loss: 1.3956422209739685, Final Batch Loss: 0.22766879200935364\n",
      "Epoch 7078, Loss: 1.3020798116922379, Final Batch Loss: 0.21241872012615204\n",
      "Epoch 7079, Loss: 1.334281101822853, Final Batch Loss: 0.20316921174526215\n",
      "Epoch 7080, Loss: 1.3022252917289734, Final Batch Loss: 0.21463625133037567\n",
      "Epoch 7081, Loss: 1.290626659989357, Final Batch Loss: 0.24401208758354187\n",
      "Epoch 7082, Loss: 1.3779995292425156, Final Batch Loss: 0.3107477128505707\n",
      "Epoch 7083, Loss: 1.5127245038747787, Final Batch Loss: 0.45764386653900146\n",
      "Epoch 7084, Loss: 1.3639177829027176, Final Batch Loss: 0.2320387214422226\n",
      "Epoch 7085, Loss: 1.4564385116100311, Final Batch Loss: 0.37367013096809387\n",
      "Epoch 7086, Loss: 1.2539020031690598, Final Batch Loss: 0.23254860937595367\n",
      "Epoch 7087, Loss: 1.4510642886161804, Final Batch Loss: 0.3575020432472229\n",
      "Epoch 7088, Loss: 1.1782650351524353, Final Batch Loss: 0.22211800515651703\n",
      "Epoch 7089, Loss: 1.1268765926361084, Final Batch Loss: 0.13094450533390045\n",
      "Epoch 7090, Loss: 1.5011606812477112, Final Batch Loss: 0.3068852424621582\n",
      "Epoch 7091, Loss: 1.5367775559425354, Final Batch Loss: 0.3210686445236206\n",
      "Epoch 7092, Loss: 1.4051245898008347, Final Batch Loss: 0.189877450466156\n",
      "Epoch 7093, Loss: 1.2787511944770813, Final Batch Loss: 0.23779921233654022\n",
      "Epoch 7094, Loss: 1.1420496702194214, Final Batch Loss: 0.1293441504240036\n",
      "Epoch 7095, Loss: 1.2939007729291916, Final Batch Loss: 0.28184208273887634\n",
      "Epoch 7096, Loss: 1.497236117720604, Final Batch Loss: 0.3914240598678589\n",
      "Epoch 7097, Loss: 1.3061631619930267, Final Batch Loss: 0.24545401334762573\n",
      "Epoch 7098, Loss: 1.4333945959806442, Final Batch Loss: 0.21245448291301727\n",
      "Epoch 7099, Loss: 1.4563015401363373, Final Batch Loss: 0.27373847365379333\n",
      "Epoch 7100, Loss: 1.4864179491996765, Final Batch Loss: 0.28531357645988464\n",
      "Epoch 7101, Loss: 1.4559243023395538, Final Batch Loss: 0.3162206709384918\n",
      "Epoch 7102, Loss: 1.293691873550415, Final Batch Loss: 0.3427754044532776\n",
      "Epoch 7103, Loss: 1.2858270555734634, Final Batch Loss: 0.21311800181865692\n",
      "Epoch 7104, Loss: 1.4016670286655426, Final Batch Loss: 0.2705489695072174\n",
      "Epoch 7105, Loss: 1.6102077662944794, Final Batch Loss: 0.4820728003978729\n",
      "Epoch 7106, Loss: 1.561473086476326, Final Batch Loss: 0.4687758684158325\n",
      "Epoch 7107, Loss: 1.307125762104988, Final Batch Loss: 0.2790890038013458\n",
      "Epoch 7108, Loss: 1.2293467670679092, Final Batch Loss: 0.1895538866519928\n",
      "Epoch 7109, Loss: 1.4743586331605911, Final Batch Loss: 0.24758312106132507\n",
      "Epoch 7110, Loss: 1.3657995760440826, Final Batch Loss: 0.41404449939727783\n",
      "Epoch 7111, Loss: 1.3348741382360458, Final Batch Loss: 0.3119910955429077\n",
      "Epoch 7112, Loss: 1.415471687912941, Final Batch Loss: 0.32073062658309937\n",
      "Epoch 7113, Loss: 1.324892744421959, Final Batch Loss: 0.1948224902153015\n",
      "Epoch 7114, Loss: 1.1010847091674805, Final Batch Loss: 0.17775991559028625\n",
      "Epoch 7115, Loss: 1.1665456295013428, Final Batch Loss: 0.22239895164966583\n",
      "Epoch 7116, Loss: 1.1839978098869324, Final Batch Loss: 0.170816108584404\n",
      "Epoch 7117, Loss: 1.23698590695858, Final Batch Loss: 0.23358501493930817\n",
      "Epoch 7118, Loss: 1.3970693051815033, Final Batch Loss: 0.4074802100658417\n",
      "Epoch 7119, Loss: 1.2959644794464111, Final Batch Loss: 0.21291691064834595\n",
      "Epoch 7120, Loss: 1.3104377835988998, Final Batch Loss: 0.19619275629520416\n",
      "Epoch 7121, Loss: 1.5798432379961014, Final Batch Loss: 0.4151023030281067\n",
      "Epoch 7122, Loss: 1.6495422273874283, Final Batch Loss: 0.39668533205986023\n",
      "Epoch 7123, Loss: 1.3777884095907211, Final Batch Loss: 0.1882571130990982\n",
      "Epoch 7124, Loss: 1.4068254977464676, Final Batch Loss: 0.2544858753681183\n",
      "Epoch 7125, Loss: 1.447946384549141, Final Batch Loss: 0.32256922125816345\n",
      "Epoch 7126, Loss: 1.1881645768880844, Final Batch Loss: 0.20533499121665955\n",
      "Epoch 7127, Loss: 1.3164564222097397, Final Batch Loss: 0.20619018375873566\n",
      "Epoch 7128, Loss: 1.3191155195236206, Final Batch Loss: 0.267699658870697\n",
      "Epoch 7129, Loss: 1.4247192442417145, Final Batch Loss: 0.19667266309261322\n",
      "Epoch 7130, Loss: 1.2866089791059494, Final Batch Loss: 0.1858205795288086\n",
      "Epoch 7131, Loss: 1.5649685710668564, Final Batch Loss: 0.249915212392807\n",
      "Epoch 7132, Loss: 1.515561580657959, Final Batch Loss: 0.32644376158714294\n",
      "Epoch 7133, Loss: 1.5419139564037323, Final Batch Loss: 0.33894941210746765\n",
      "Epoch 7134, Loss: 1.286952793598175, Final Batch Loss: 0.14909476041793823\n",
      "Epoch 7135, Loss: 1.7535497844219208, Final Batch Loss: 0.48186904191970825\n",
      "Epoch 7136, Loss: 1.477518618106842, Final Batch Loss: 0.19157730042934418\n",
      "Epoch 7137, Loss: 1.4577540159225464, Final Batch Loss: 0.34606531262397766\n",
      "Epoch 7138, Loss: 1.3430323600769043, Final Batch Loss: 0.2536458373069763\n",
      "Epoch 7139, Loss: 1.5207851380109787, Final Batch Loss: 0.3292551040649414\n",
      "Epoch 7140, Loss: 1.083478406071663, Final Batch Loss: 0.15522755682468414\n",
      "Epoch 7141, Loss: 1.295848548412323, Final Batch Loss: 0.23742958903312683\n",
      "Epoch 7142, Loss: 1.5014140605926514, Final Batch Loss: 0.4025542140007019\n",
      "Epoch 7143, Loss: 1.2992390394210815, Final Batch Loss: 0.23991280794143677\n",
      "Epoch 7144, Loss: 1.469265267252922, Final Batch Loss: 0.27972376346588135\n",
      "Epoch 7145, Loss: 1.2803034782409668, Final Batch Loss: 0.21705766022205353\n",
      "Epoch 7146, Loss: 1.3106963485479355, Final Batch Loss: 0.20815661549568176\n",
      "Epoch 7147, Loss: 1.2903095930814743, Final Batch Loss: 0.21338437497615814\n",
      "Epoch 7148, Loss: 1.2364038974046707, Final Batch Loss: 0.22452133893966675\n",
      "Epoch 7149, Loss: 1.304182529449463, Final Batch Loss: 0.286949098110199\n",
      "Epoch 7150, Loss: 1.2787621021270752, Final Batch Loss: 0.34632179141044617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7151, Loss: 1.5133942365646362, Final Batch Loss: 0.5017913579940796\n",
      "Epoch 7152, Loss: 1.1892138123512268, Final Batch Loss: 0.16113115847110748\n",
      "Epoch 7153, Loss: 1.344962477684021, Final Batch Loss: 0.4159155488014221\n",
      "Epoch 7154, Loss: 1.4110636860132217, Final Batch Loss: 0.2654612064361572\n",
      "Epoch 7155, Loss: 1.4484044909477234, Final Batch Loss: 0.3320532739162445\n",
      "Epoch 7156, Loss: 1.2066769003868103, Final Batch Loss: 0.1543790102005005\n",
      "Epoch 7157, Loss: 1.3267154544591904, Final Batch Loss: 0.17381316423416138\n",
      "Epoch 7158, Loss: 1.292807713150978, Final Batch Loss: 0.23273150622844696\n",
      "Epoch 7159, Loss: 1.2221787720918655, Final Batch Loss: 0.16665151715278625\n",
      "Epoch 7160, Loss: 1.3640384674072266, Final Batch Loss: 0.238832026720047\n",
      "Epoch 7161, Loss: 1.23760287463665, Final Batch Loss: 0.16648821532726288\n",
      "Epoch 7162, Loss: 1.289157897233963, Final Batch Loss: 0.1903020292520523\n",
      "Epoch 7163, Loss: 1.2716760337352753, Final Batch Loss: 0.2373967319726944\n",
      "Epoch 7164, Loss: 1.328403800725937, Final Batch Loss: 0.4213721454143524\n",
      "Epoch 7165, Loss: 1.4537007510662079, Final Batch Loss: 0.24163669347763062\n",
      "Epoch 7166, Loss: 1.3065721094608307, Final Batch Loss: 0.23617300391197205\n",
      "Epoch 7167, Loss: 1.3426279425621033, Final Batch Loss: 0.21046267449855804\n",
      "Epoch 7168, Loss: 1.3837533742189407, Final Batch Loss: 0.3996209502220154\n",
      "Epoch 7169, Loss: 1.2209301441907883, Final Batch Loss: 0.2201659232378006\n",
      "Epoch 7170, Loss: 1.2177859097719193, Final Batch Loss: 0.169996440410614\n",
      "Epoch 7171, Loss: 1.2826654464006424, Final Batch Loss: 0.18925447762012482\n",
      "Epoch 7172, Loss: 1.3051912188529968, Final Batch Loss: 0.2716571092605591\n",
      "Epoch 7173, Loss: 1.2294493168592453, Final Batch Loss: 0.2650483250617981\n",
      "Epoch 7174, Loss: 1.231247991323471, Final Batch Loss: 0.2357223778963089\n",
      "Epoch 7175, Loss: 1.337602898478508, Final Batch Loss: 0.23749515414237976\n",
      "Epoch 7176, Loss: 1.183010384440422, Final Batch Loss: 0.22201605141162872\n",
      "Epoch 7177, Loss: 1.27145916223526, Final Batch Loss: 0.1796340048313141\n",
      "Epoch 7178, Loss: 1.3958788365125656, Final Batch Loss: 0.287228524684906\n",
      "Epoch 7179, Loss: 1.2449171096086502, Final Batch Loss: 0.19355517625808716\n",
      "Epoch 7180, Loss: 1.3262071162462234, Final Batch Loss: 0.20419827103614807\n",
      "Epoch 7181, Loss: 1.349684551358223, Final Batch Loss: 0.2894154489040375\n",
      "Epoch 7182, Loss: 1.2515804767608643, Final Batch Loss: 0.23491597175598145\n",
      "Epoch 7183, Loss: 1.384114608168602, Final Batch Loss: 0.2866113483905792\n",
      "Epoch 7184, Loss: 1.3335393965244293, Final Batch Loss: 0.23735445737838745\n",
      "Epoch 7185, Loss: 1.4079089611768723, Final Batch Loss: 0.19988861680030823\n",
      "Epoch 7186, Loss: 1.3867643028497696, Final Batch Loss: 0.2465944141149521\n",
      "Epoch 7187, Loss: 1.2611548006534576, Final Batch Loss: 0.31827235221862793\n",
      "Epoch 7188, Loss: 1.2090506851673126, Final Batch Loss: 0.17302581667900085\n",
      "Epoch 7189, Loss: 1.4833171367645264, Final Batch Loss: 0.34615275263786316\n",
      "Epoch 7190, Loss: 1.2851029634475708, Final Batch Loss: 0.2605504095554352\n",
      "Epoch 7191, Loss: 1.2988779246807098, Final Batch Loss: 0.18909645080566406\n",
      "Epoch 7192, Loss: 1.3181894719600677, Final Batch Loss: 0.2886447608470917\n",
      "Epoch 7193, Loss: 1.2818115651607513, Final Batch Loss: 0.2822427749633789\n",
      "Epoch 7194, Loss: 1.459694892168045, Final Batch Loss: 0.350410521030426\n",
      "Epoch 7195, Loss: 1.3489373624324799, Final Batch Loss: 0.26363176107406616\n",
      "Epoch 7196, Loss: 1.280887708067894, Final Batch Loss: 0.2651646137237549\n",
      "Epoch 7197, Loss: 1.3446608781814575, Final Batch Loss: 0.297326922416687\n",
      "Epoch 7198, Loss: 1.2868223190307617, Final Batch Loss: 0.3055167496204376\n",
      "Epoch 7199, Loss: 1.3642267733812332, Final Batch Loss: 0.26208773255348206\n",
      "Epoch 7200, Loss: 1.3290071040391922, Final Batch Loss: 0.3192443251609802\n",
      "Epoch 7201, Loss: 1.3713557869195938, Final Batch Loss: 0.3995189666748047\n",
      "Epoch 7202, Loss: 1.1371348053216934, Final Batch Loss: 0.15446029603481293\n",
      "Epoch 7203, Loss: 1.2617168426513672, Final Batch Loss: 0.24311216175556183\n",
      "Epoch 7204, Loss: 1.3812088370323181, Final Batch Loss: 0.3983341455459595\n",
      "Epoch 7205, Loss: 1.7286051511764526, Final Batch Loss: 0.3575598895549774\n",
      "Epoch 7206, Loss: 1.200334057211876, Final Batch Loss: 0.19092248380184174\n",
      "Epoch 7207, Loss: 1.3706233203411102, Final Batch Loss: 0.22163057327270508\n",
      "Epoch 7208, Loss: 1.3468900471925735, Final Batch Loss: 0.32652509212493896\n",
      "Epoch 7209, Loss: 1.402808427810669, Final Batch Loss: 0.3421834111213684\n",
      "Epoch 7210, Loss: 1.407585009932518, Final Batch Loss: 0.19809967279434204\n",
      "Epoch 7211, Loss: 1.2082616239786148, Final Batch Loss: 0.29035311937332153\n",
      "Epoch 7212, Loss: 1.5245098322629929, Final Batch Loss: 0.3644545078277588\n",
      "Epoch 7213, Loss: 1.512993410229683, Final Batch Loss: 0.32445675134658813\n",
      "Epoch 7214, Loss: 1.5131285637617111, Final Batch Loss: 0.39858368039131165\n",
      "Epoch 7215, Loss: 1.4762290269136429, Final Batch Loss: 0.34223535656929016\n",
      "Epoch 7216, Loss: 1.358325481414795, Final Batch Loss: 0.25909096002578735\n",
      "Epoch 7217, Loss: 1.257717713713646, Final Batch Loss: 0.1352578103542328\n",
      "Epoch 7218, Loss: 1.2266878932714462, Final Batch Loss: 0.2395506203174591\n",
      "Epoch 7219, Loss: 1.3650109618902206, Final Batch Loss: 0.41223791241645813\n",
      "Epoch 7220, Loss: 1.3853033483028412, Final Batch Loss: 0.2892979681491852\n",
      "Epoch 7221, Loss: 1.3465677797794342, Final Batch Loss: 0.23681187629699707\n",
      "Epoch 7222, Loss: 1.293336182832718, Final Batch Loss: 0.22155751287937164\n",
      "Epoch 7223, Loss: 1.2289409339427948, Final Batch Loss: 0.13924521207809448\n",
      "Epoch 7224, Loss: 1.2756407558918, Final Batch Loss: 0.18662530183792114\n",
      "Epoch 7225, Loss: 1.5438041239976883, Final Batch Loss: 0.430542528629303\n",
      "Epoch 7226, Loss: 1.3300035446882248, Final Batch Loss: 0.2336898148059845\n",
      "Epoch 7227, Loss: 1.3620750159025192, Final Batch Loss: 0.31232964992523193\n",
      "Epoch 7228, Loss: 1.3102765828371048, Final Batch Loss: 0.29503321647644043\n",
      "Epoch 7229, Loss: 1.368707299232483, Final Batch Loss: 0.24631235003471375\n",
      "Epoch 7230, Loss: 1.3039927184581757, Final Batch Loss: 0.13799330592155457\n",
      "Epoch 7231, Loss: 1.2753199636936188, Final Batch Loss: 0.23739750683307648\n",
      "Epoch 7232, Loss: 1.341785505414009, Final Batch Loss: 0.2912602126598358\n",
      "Epoch 7233, Loss: 1.6045213639736176, Final Batch Loss: 0.4323851466178894\n",
      "Epoch 7234, Loss: 1.544051006436348, Final Batch Loss: 0.574313759803772\n",
      "Epoch 7235, Loss: 1.4501882195472717, Final Batch Loss: 0.4433092772960663\n",
      "Epoch 7236, Loss: 1.2735936343669891, Final Batch Loss: 0.28216806054115295\n",
      "Epoch 7237, Loss: 1.3244385868310928, Final Batch Loss: 0.2789514362812042\n",
      "Epoch 7238, Loss: 1.3961317986249924, Final Batch Loss: 0.20823225378990173\n",
      "Epoch 7239, Loss: 1.3570505529642105, Final Batch Loss: 0.19985519349575043\n",
      "Epoch 7240, Loss: 1.2397222518920898, Final Batch Loss: 0.1733929067850113\n",
      "Epoch 7241, Loss: 1.4824431389570236, Final Batch Loss: 0.3732854425907135\n",
      "Epoch 7242, Loss: 1.2750570476055145, Final Batch Loss: 0.21652214229106903\n",
      "Epoch 7243, Loss: 1.3420334607362747, Final Batch Loss: 0.3386596441268921\n",
      "Epoch 7244, Loss: 1.306406021118164, Final Batch Loss: 0.2985362410545349\n",
      "Epoch 7245, Loss: 1.5809659212827682, Final Batch Loss: 0.42111310362815857\n",
      "Epoch 7246, Loss: 1.2107849270105362, Final Batch Loss: 0.2772022783756256\n",
      "Epoch 7247, Loss: 1.660666897892952, Final Batch Loss: 0.41325169801712036\n",
      "Epoch 7248, Loss: 1.472517967224121, Final Batch Loss: 0.24340981245040894\n",
      "Epoch 7249, Loss: 1.4132438004016876, Final Batch Loss: 0.3390352129936218\n",
      "Epoch 7250, Loss: 1.2473524808883667, Final Batch Loss: 0.2817620038986206\n",
      "Epoch 7251, Loss: 1.3102289587259293, Final Batch Loss: 0.3193080425262451\n",
      "Epoch 7252, Loss: 1.3618395328521729, Final Batch Loss: 0.2868271470069885\n",
      "Epoch 7253, Loss: 1.2412052750587463, Final Batch Loss: 0.1367582529783249\n",
      "Epoch 7254, Loss: 1.406649500131607, Final Batch Loss: 0.2910831868648529\n",
      "Epoch 7255, Loss: 1.2959225326776505, Final Batch Loss: 0.2703987956047058\n",
      "Epoch 7256, Loss: 1.2502510845661163, Final Batch Loss: 0.2218722552061081\n",
      "Epoch 7257, Loss: 1.4417442232370377, Final Batch Loss: 0.26822829246520996\n",
      "Epoch 7258, Loss: 1.466168686747551, Final Batch Loss: 0.24012576043605804\n",
      "Epoch 7259, Loss: 1.3177977204322815, Final Batch Loss: 0.3105364441871643\n",
      "Epoch 7260, Loss: 1.3496762961149216, Final Batch Loss: 0.22759826481342316\n",
      "Epoch 7261, Loss: 1.2887615114450455, Final Batch Loss: 0.29988792538642883\n",
      "Epoch 7262, Loss: 1.3593143820762634, Final Batch Loss: 0.33553484082221985\n",
      "Epoch 7263, Loss: 1.2487092912197113, Final Batch Loss: 0.17072457075119019\n",
      "Epoch 7264, Loss: 1.3928153812885284, Final Batch Loss: 0.24133166670799255\n",
      "Epoch 7265, Loss: 1.243517816066742, Final Batch Loss: 0.1999170333147049\n",
      "Epoch 7266, Loss: 1.4653989374637604, Final Batch Loss: 0.3292980492115021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7267, Loss: 1.3982545733451843, Final Batch Loss: 0.34951022267341614\n",
      "Epoch 7268, Loss: 1.4912715703248978, Final Batch Loss: 0.4109579920768738\n",
      "Epoch 7269, Loss: 1.3128599524497986, Final Batch Loss: 0.2896217405796051\n",
      "Epoch 7270, Loss: 1.4878999888896942, Final Batch Loss: 0.29288870096206665\n",
      "Epoch 7271, Loss: 1.3519482761621475, Final Batch Loss: 0.37110254168510437\n",
      "Epoch 7272, Loss: 1.4790645092725754, Final Batch Loss: 0.42253339290618896\n",
      "Epoch 7273, Loss: 1.3088567852973938, Final Batch Loss: 0.27236470580101013\n",
      "Epoch 7274, Loss: 1.366848886013031, Final Batch Loss: 0.23781347274780273\n",
      "Epoch 7275, Loss: 1.319891706109047, Final Batch Loss: 0.35514384508132935\n",
      "Epoch 7276, Loss: 1.466881349682808, Final Batch Loss: 0.31461307406425476\n",
      "Epoch 7277, Loss: 1.3228948712348938, Final Batch Loss: 0.2949753403663635\n",
      "Epoch 7278, Loss: 1.2797108739614487, Final Batch Loss: 0.2306770533323288\n",
      "Epoch 7279, Loss: 1.5464222729206085, Final Batch Loss: 0.17217561602592468\n",
      "Epoch 7280, Loss: 1.4882586896419525, Final Batch Loss: 0.31558793783187866\n",
      "Epoch 7281, Loss: 1.4120659679174423, Final Batch Loss: 0.2253033071756363\n",
      "Epoch 7282, Loss: 1.5117087066173553, Final Batch Loss: 0.506718099117279\n",
      "Epoch 7283, Loss: 1.1899049133062363, Final Batch Loss: 0.18699026107788086\n",
      "Epoch 7284, Loss: 1.418557047843933, Final Batch Loss: 0.3349648118019104\n",
      "Epoch 7285, Loss: 1.258741855621338, Final Batch Loss: 0.19656193256378174\n",
      "Epoch 7286, Loss: 1.2947507053613663, Final Batch Loss: 0.2255152463912964\n",
      "Epoch 7287, Loss: 1.203713208436966, Final Batch Loss: 0.21495620906352997\n",
      "Epoch 7288, Loss: 1.30698661506176, Final Batch Loss: 0.3454040288925171\n",
      "Epoch 7289, Loss: 1.362385630607605, Final Batch Loss: 0.30625393986701965\n",
      "Epoch 7290, Loss: 1.312374010682106, Final Batch Loss: 0.28892844915390015\n",
      "Epoch 7291, Loss: 1.525403156876564, Final Batch Loss: 0.37831613421440125\n",
      "Epoch 7292, Loss: 1.4681779593229294, Final Batch Loss: 0.25893712043762207\n",
      "Epoch 7293, Loss: 1.297755867242813, Final Batch Loss: 0.14293232560157776\n",
      "Epoch 7294, Loss: 1.3028444200754166, Final Batch Loss: 0.33788564801216125\n",
      "Epoch 7295, Loss: 1.2441381961107254, Final Batch Loss: 0.26315024495124817\n",
      "Epoch 7296, Loss: 1.4743858873844147, Final Batch Loss: 0.26556575298309326\n",
      "Epoch 7297, Loss: 1.4167925119400024, Final Batch Loss: 0.2579253017902374\n",
      "Epoch 7298, Loss: 1.3887623697519302, Final Batch Loss: 0.4029443562030792\n",
      "Epoch 7299, Loss: 1.4351839870214462, Final Batch Loss: 0.2097872644662857\n",
      "Epoch 7300, Loss: 1.2420979738235474, Final Batch Loss: 0.27598699927330017\n",
      "Epoch 7301, Loss: 1.4493331760168076, Final Batch Loss: 0.24172954261302948\n",
      "Epoch 7302, Loss: 1.3948267996311188, Final Batch Loss: 0.3100886344909668\n",
      "Epoch 7303, Loss: 1.3468412011861801, Final Batch Loss: 0.36340123414993286\n",
      "Epoch 7304, Loss: 1.1570014655590057, Final Batch Loss: 0.13254432380199432\n",
      "Epoch 7305, Loss: 1.3151357173919678, Final Batch Loss: 0.31943827867507935\n",
      "Epoch 7306, Loss: 1.2870963960886002, Final Batch Loss: 0.22081951797008514\n",
      "Epoch 7307, Loss: 1.4215000867843628, Final Batch Loss: 0.40376055240631104\n",
      "Epoch 7308, Loss: 1.2403528839349747, Final Batch Loss: 0.2771347463130951\n",
      "Epoch 7309, Loss: 1.2402608692646027, Final Batch Loss: 0.2460942417383194\n",
      "Epoch 7310, Loss: 1.3157689273357391, Final Batch Loss: 0.19780874252319336\n",
      "Epoch 7311, Loss: 1.3131737560033798, Final Batch Loss: 0.312380850315094\n",
      "Epoch 7312, Loss: 1.3026782870292664, Final Batch Loss: 0.3293592631816864\n",
      "Epoch 7313, Loss: 1.2557906955480576, Final Batch Loss: 0.2611181139945984\n",
      "Epoch 7314, Loss: 1.2751558423042297, Final Batch Loss: 0.22858089208602905\n",
      "Epoch 7315, Loss: 1.3097322434186935, Final Batch Loss: 0.22196011245250702\n",
      "Epoch 7316, Loss: 1.399053543806076, Final Batch Loss: 0.2511851191520691\n",
      "Epoch 7317, Loss: 1.2393086850643158, Final Batch Loss: 0.17601650953292847\n",
      "Epoch 7318, Loss: 1.154623195528984, Final Batch Loss: 0.20434214174747467\n",
      "Epoch 7319, Loss: 1.2867409735918045, Final Batch Loss: 0.2660120725631714\n",
      "Epoch 7320, Loss: 1.3268001675605774, Final Batch Loss: 0.254304975271225\n",
      "Epoch 7321, Loss: 1.1055222153663635, Final Batch Loss: 0.13789476454257965\n",
      "Epoch 7322, Loss: 1.1460084021091461, Final Batch Loss: 0.2215271294116974\n",
      "Epoch 7323, Loss: 1.4659809321165085, Final Batch Loss: 0.4470421075820923\n",
      "Epoch 7324, Loss: 1.283740371465683, Final Batch Loss: 0.19685441255569458\n",
      "Epoch 7325, Loss: 1.355888456106186, Final Batch Loss: 0.2500503361225128\n",
      "Epoch 7326, Loss: 1.2673697471618652, Final Batch Loss: 0.2564428150653839\n",
      "Epoch 7327, Loss: 1.3505696654319763, Final Batch Loss: 0.18245801329612732\n",
      "Epoch 7328, Loss: 1.5302770137786865, Final Batch Loss: 0.33822256326675415\n",
      "Epoch 7329, Loss: 1.2448085695505142, Final Batch Loss: 0.24381481111049652\n",
      "Epoch 7330, Loss: 1.3724138885736465, Final Batch Loss: 0.3056298792362213\n",
      "Epoch 7331, Loss: 1.1968487948179245, Final Batch Loss: 0.18119584023952484\n",
      "Epoch 7332, Loss: 1.5071253329515457, Final Batch Loss: 0.3613072335720062\n",
      "Epoch 7333, Loss: 1.2873628288507462, Final Batch Loss: 0.303192675113678\n",
      "Epoch 7334, Loss: 1.3845251500606537, Final Batch Loss: 0.22040602564811707\n",
      "Epoch 7335, Loss: 1.4254072159528732, Final Batch Loss: 0.3056441843509674\n",
      "Epoch 7336, Loss: 1.35328608751297, Final Batch Loss: 0.30056682229042053\n",
      "Epoch 7337, Loss: 1.2111396193504333, Final Batch Loss: 0.18206557631492615\n",
      "Epoch 7338, Loss: 1.541854664683342, Final Batch Loss: 0.30560240149497986\n",
      "Epoch 7339, Loss: 1.309918001294136, Final Batch Loss: 0.2459205538034439\n",
      "Epoch 7340, Loss: 1.2861883342266083, Final Batch Loss: 0.19842354953289032\n",
      "Epoch 7341, Loss: 1.451348751783371, Final Batch Loss: 0.2070665955543518\n",
      "Epoch 7342, Loss: 1.385131299495697, Final Batch Loss: 0.2224741280078888\n",
      "Epoch 7343, Loss: 1.339206412434578, Final Batch Loss: 0.16926775872707367\n",
      "Epoch 7344, Loss: 1.2919161766767502, Final Batch Loss: 0.23107028007507324\n",
      "Epoch 7345, Loss: 1.5375891029834747, Final Batch Loss: 0.3059947192668915\n",
      "Epoch 7346, Loss: 1.4307477474212646, Final Batch Loss: 0.39397332072257996\n",
      "Epoch 7347, Loss: 1.3327101469039917, Final Batch Loss: 0.2885642349720001\n",
      "Epoch 7348, Loss: 1.328623503446579, Final Batch Loss: 0.24072203040122986\n",
      "Epoch 7349, Loss: 1.275441363453865, Final Batch Loss: 0.08742043375968933\n",
      "Epoch 7350, Loss: 1.1816426366567612, Final Batch Loss: 0.16993552446365356\n",
      "Epoch 7351, Loss: 1.2174201011657715, Final Batch Loss: 0.31800031661987305\n",
      "Epoch 7352, Loss: 1.2870714515447617, Final Batch Loss: 0.32684046030044556\n",
      "Epoch 7353, Loss: 1.1098811775445938, Final Batch Loss: 0.1117536872625351\n",
      "Epoch 7354, Loss: 1.3107007294893265, Final Batch Loss: 0.23129716515541077\n",
      "Epoch 7355, Loss: 1.2896857857704163, Final Batch Loss: 0.2681470215320587\n",
      "Epoch 7356, Loss: 1.307558834552765, Final Batch Loss: 0.20339368283748627\n",
      "Epoch 7357, Loss: 1.4464324712753296, Final Batch Loss: 0.3897094130516052\n",
      "Epoch 7358, Loss: 1.1546562314033508, Final Batch Loss: 0.20350931584835052\n",
      "Epoch 7359, Loss: 1.3597601354122162, Final Batch Loss: 0.21056467294692993\n",
      "Epoch 7360, Loss: 1.2248032540082932, Final Batch Loss: 0.2406557947397232\n",
      "Epoch 7361, Loss: 1.3595884293317795, Final Batch Loss: 0.13487885892391205\n",
      "Epoch 7362, Loss: 1.3971198499202728, Final Batch Loss: 0.2714957296848297\n",
      "Epoch 7363, Loss: 1.2015693932771683, Final Batch Loss: 0.16546531021595\n",
      "Epoch 7364, Loss: 1.2858694642782211, Final Batch Loss: 0.17723174393177032\n",
      "Epoch 7365, Loss: 1.2047391384840012, Final Batch Loss: 0.21415844559669495\n",
      "Epoch 7366, Loss: 1.6020669341087341, Final Batch Loss: 0.3952416777610779\n",
      "Epoch 7367, Loss: 1.3002822995185852, Final Batch Loss: 0.26507923007011414\n",
      "Epoch 7368, Loss: 1.2143092602491379, Final Batch Loss: 0.1882990449666977\n",
      "Epoch 7369, Loss: 1.537919208407402, Final Batch Loss: 0.42188066244125366\n",
      "Epoch 7370, Loss: 1.2051105499267578, Final Batch Loss: 0.10133780539035797\n",
      "Epoch 7371, Loss: 1.254196435213089, Final Batch Loss: 0.14657962322235107\n",
      "Epoch 7372, Loss: 1.4087355434894562, Final Batch Loss: 0.3298083543777466\n",
      "Epoch 7373, Loss: 1.2196247354149818, Final Batch Loss: 0.12405360490083694\n",
      "Epoch 7374, Loss: 1.245287373661995, Final Batch Loss: 0.21275369822978973\n",
      "Epoch 7375, Loss: 1.1572240144014359, Final Batch Loss: 0.20389002561569214\n",
      "Epoch 7376, Loss: 1.1946271061897278, Final Batch Loss: 0.1722470372915268\n",
      "Epoch 7377, Loss: 1.2560993134975433, Final Batch Loss: 0.21786241233348846\n",
      "Epoch 7378, Loss: 1.3268784135580063, Final Batch Loss: 0.22834579646587372\n",
      "Epoch 7379, Loss: 1.3750587850809097, Final Batch Loss: 0.2625274658203125\n",
      "Epoch 7380, Loss: 1.166767731308937, Final Batch Loss: 0.13741347193717957\n",
      "Epoch 7381, Loss: 1.2851758301258087, Final Batch Loss: 0.20191125571727753\n",
      "Epoch 7382, Loss: 1.3387921303510666, Final Batch Loss: 0.16154108941555023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7383, Loss: 1.3784435540437698, Final Batch Loss: 0.2253538817167282\n",
      "Epoch 7384, Loss: 1.335725724697113, Final Batch Loss: 0.2550642490386963\n",
      "Epoch 7385, Loss: 1.391751542687416, Final Batch Loss: 0.2944280207157135\n",
      "Epoch 7386, Loss: 1.45193213224411, Final Batch Loss: 0.19812527298927307\n",
      "Epoch 7387, Loss: 1.1141120046377182, Final Batch Loss: 0.1695929914712906\n",
      "Epoch 7388, Loss: 1.4938670992851257, Final Batch Loss: 0.3238101303577423\n",
      "Epoch 7389, Loss: 1.166325569152832, Final Batch Loss: 0.31331509351730347\n",
      "Epoch 7390, Loss: 1.3806829005479813, Final Batch Loss: 0.1999102681875229\n",
      "Epoch 7391, Loss: 1.3416466861963272, Final Batch Loss: 0.27073031663894653\n",
      "Epoch 7392, Loss: 1.4698253273963928, Final Batch Loss: 0.3863659203052521\n",
      "Epoch 7393, Loss: 1.4252099692821503, Final Batch Loss: 0.34092599153518677\n",
      "Epoch 7394, Loss: 1.05525603890419, Final Batch Loss: 0.1720711886882782\n",
      "Epoch 7395, Loss: 1.2587014734745026, Final Batch Loss: 0.19314512610435486\n",
      "Epoch 7396, Loss: 1.3588720858097076, Final Batch Loss: 0.23828883469104767\n",
      "Epoch 7397, Loss: 1.3244248777627945, Final Batch Loss: 0.2734072506427765\n",
      "Epoch 7398, Loss: 1.3239455819129944, Final Batch Loss: 0.3497436046600342\n",
      "Epoch 7399, Loss: 1.2608179450035095, Final Batch Loss: 0.2574436068534851\n",
      "Epoch 7400, Loss: 1.1850261688232422, Final Batch Loss: 0.2657366394996643\n",
      "Epoch 7401, Loss: 1.2857546508312225, Final Batch Loss: 0.33890947699546814\n",
      "Epoch 7402, Loss: 1.2516887933015823, Final Batch Loss: 0.2628704011440277\n",
      "Epoch 7403, Loss: 1.4049779176712036, Final Batch Loss: 0.29090452194213867\n",
      "Epoch 7404, Loss: 1.6002100259065628, Final Batch Loss: 0.2588195502758026\n",
      "Epoch 7405, Loss: 1.4420523941516876, Final Batch Loss: 0.3388517200946808\n",
      "Epoch 7406, Loss: 1.311067670583725, Final Batch Loss: 0.2128644734621048\n",
      "Epoch 7407, Loss: 1.3385080099105835, Final Batch Loss: 0.21155746281147003\n",
      "Epoch 7408, Loss: 1.1579867005348206, Final Batch Loss: 0.18782421946525574\n",
      "Epoch 7409, Loss: 1.402747631072998, Final Batch Loss: 0.2211865335702896\n",
      "Epoch 7410, Loss: 1.277667909860611, Final Batch Loss: 0.22123399376869202\n",
      "Epoch 7411, Loss: 1.3699941188097, Final Batch Loss: 0.28292712569236755\n",
      "Epoch 7412, Loss: 1.5466862916946411, Final Batch Loss: 0.3864743113517761\n",
      "Epoch 7413, Loss: 1.181166186928749, Final Batch Loss: 0.13865184783935547\n",
      "Epoch 7414, Loss: 1.4540164023637772, Final Batch Loss: 0.2764952778816223\n",
      "Epoch 7415, Loss: 1.3434428721666336, Final Batch Loss: 0.27793189883232117\n",
      "Epoch 7416, Loss: 1.2182128429412842, Final Batch Loss: 0.21085162460803986\n",
      "Epoch 7417, Loss: 1.2009637355804443, Final Batch Loss: 0.177993044257164\n",
      "Epoch 7418, Loss: 1.4573333710432053, Final Batch Loss: 0.3572506308555603\n",
      "Epoch 7419, Loss: 1.2689628154039383, Final Batch Loss: 0.16328135132789612\n",
      "Epoch 7420, Loss: 1.195772334933281, Final Batch Loss: 0.20215792953968048\n",
      "Epoch 7421, Loss: 1.2180266231298447, Final Batch Loss: 0.22220031917095184\n",
      "Epoch 7422, Loss: 1.6250010132789612, Final Batch Loss: 0.3160644769668579\n",
      "Epoch 7423, Loss: 1.1610722839832306, Final Batch Loss: 0.16719084978103638\n",
      "Epoch 7424, Loss: 1.3639094680547714, Final Batch Loss: 0.2715713381767273\n",
      "Epoch 7425, Loss: 1.3109119534492493, Final Batch Loss: 0.24917736649513245\n",
      "Epoch 7426, Loss: 1.4966555535793304, Final Batch Loss: 0.3079214096069336\n",
      "Epoch 7427, Loss: 1.337573066353798, Final Batch Loss: 0.2885397970676422\n",
      "Epoch 7428, Loss: 1.2795652523636818, Final Batch Loss: 0.12187022715806961\n",
      "Epoch 7429, Loss: 1.3157611638307571, Final Batch Loss: 0.3218715786933899\n",
      "Epoch 7430, Loss: 1.4213371276855469, Final Batch Loss: 0.24687232077121735\n",
      "Epoch 7431, Loss: 1.4175722748041153, Final Batch Loss: 0.38574904203414917\n",
      "Epoch 7432, Loss: 1.4246793687343597, Final Batch Loss: 0.3420846164226532\n",
      "Epoch 7433, Loss: 1.2076010555028915, Final Batch Loss: 0.16889865696430206\n",
      "Epoch 7434, Loss: 1.3856472373008728, Final Batch Loss: 0.3473531901836395\n",
      "Epoch 7435, Loss: 1.2945433855056763, Final Batch Loss: 0.2775842845439911\n",
      "Epoch 7436, Loss: 1.4113969653844833, Final Batch Loss: 0.3302062749862671\n",
      "Epoch 7437, Loss: 1.2249637246131897, Final Batch Loss: 0.1398649662733078\n",
      "Epoch 7438, Loss: 1.1238495260477066, Final Batch Loss: 0.1974632740020752\n",
      "Epoch 7439, Loss: 1.291457325220108, Final Batch Loss: 0.23860032856464386\n",
      "Epoch 7440, Loss: 1.3758515119552612, Final Batch Loss: 0.34857937693595886\n",
      "Epoch 7441, Loss: 1.234252855181694, Final Batch Loss: 0.19560781121253967\n",
      "Epoch 7442, Loss: 1.3413398563861847, Final Batch Loss: 0.28610897064208984\n",
      "Epoch 7443, Loss: 1.6058996319770813, Final Batch Loss: 0.18409541249275208\n",
      "Epoch 7444, Loss: 1.1654833555221558, Final Batch Loss: 0.18767596781253815\n",
      "Epoch 7445, Loss: 1.4764595478773117, Final Batch Loss: 0.3843280076980591\n",
      "Epoch 7446, Loss: 1.2396546304225922, Final Batch Loss: 0.19016435742378235\n",
      "Epoch 7447, Loss: 1.3001234382390976, Final Batch Loss: 0.21855051815509796\n",
      "Epoch 7448, Loss: 1.3087878823280334, Final Batch Loss: 0.23476803302764893\n",
      "Epoch 7449, Loss: 1.0870035290718079, Final Batch Loss: 0.1594042032957077\n",
      "Epoch 7450, Loss: 1.2063772231340408, Final Batch Loss: 0.24001634120941162\n",
      "Epoch 7451, Loss: 1.3203880935907364, Final Batch Loss: 0.18314871191978455\n",
      "Epoch 7452, Loss: 1.3751164227724075, Final Batch Loss: 0.2718202471733093\n",
      "Epoch 7453, Loss: 1.5794087797403336, Final Batch Loss: 0.32019731402397156\n",
      "Epoch 7454, Loss: 1.3513342440128326, Final Batch Loss: 0.30947619676589966\n",
      "Epoch 7455, Loss: 1.4377287477254868, Final Batch Loss: 0.3635055422782898\n",
      "Epoch 7456, Loss: 1.25535386800766, Final Batch Loss: 0.34441518783569336\n",
      "Epoch 7457, Loss: 1.2344393879175186, Final Batch Loss: 0.25866618752479553\n",
      "Epoch 7458, Loss: 1.3672024607658386, Final Batch Loss: 0.26503071188926697\n",
      "Epoch 7459, Loss: 1.3707821518182755, Final Batch Loss: 0.1800641119480133\n",
      "Epoch 7460, Loss: 1.162695050239563, Final Batch Loss: 0.1841925084590912\n",
      "Epoch 7461, Loss: 1.3669638633728027, Final Batch Loss: 0.19881996512413025\n",
      "Epoch 7462, Loss: 1.3602073639631271, Final Batch Loss: 0.24746979773044586\n",
      "Epoch 7463, Loss: 1.1645800471305847, Final Batch Loss: 0.24275965988636017\n",
      "Epoch 7464, Loss: 1.3843860924243927, Final Batch Loss: 0.19667166471481323\n",
      "Epoch 7465, Loss: 1.4993274807929993, Final Batch Loss: 0.46485891938209534\n",
      "Epoch 7466, Loss: 1.3437142968177795, Final Batch Loss: 0.25183773040771484\n",
      "Epoch 7467, Loss: 1.3049915432929993, Final Batch Loss: 0.2933886647224426\n",
      "Epoch 7468, Loss: 1.3115753680467606, Final Batch Loss: 0.2967028319835663\n",
      "Epoch 7469, Loss: 1.2160460948944092, Final Batch Loss: 0.1737646907567978\n",
      "Epoch 7470, Loss: 1.438923954963684, Final Batch Loss: 0.3512316048145294\n",
      "Epoch 7471, Loss: 1.2374474257230759, Final Batch Loss: 0.22451891005039215\n",
      "Epoch 7472, Loss: 1.6229287385940552, Final Batch Loss: 0.41397708654403687\n",
      "Epoch 7473, Loss: 1.2931030243635178, Final Batch Loss: 0.24351072311401367\n",
      "Epoch 7474, Loss: 1.2423522472381592, Final Batch Loss: 0.28283917903900146\n",
      "Epoch 7475, Loss: 1.3165273368358612, Final Batch Loss: 0.20415183901786804\n",
      "Epoch 7476, Loss: 1.3824496120214462, Final Batch Loss: 0.2158963829278946\n",
      "Epoch 7477, Loss: 1.5469482839107513, Final Batch Loss: 0.21234524250030518\n",
      "Epoch 7478, Loss: 1.2775403261184692, Final Batch Loss: 0.23662956058979034\n",
      "Epoch 7479, Loss: 1.3120574206113815, Final Batch Loss: 0.23647569119930267\n",
      "Epoch 7480, Loss: 1.2630886137485504, Final Batch Loss: 0.17997267842292786\n",
      "Epoch 7481, Loss: 1.353795200586319, Final Batch Loss: 0.34157124161720276\n",
      "Epoch 7482, Loss: 1.285399928689003, Final Batch Loss: 0.2518308162689209\n",
      "Epoch 7483, Loss: 1.2740650027990341, Final Batch Loss: 0.3092637360095978\n",
      "Epoch 7484, Loss: 1.3313248604536057, Final Batch Loss: 0.2812401354312897\n",
      "Epoch 7485, Loss: 1.438453197479248, Final Batch Loss: 0.23001472651958466\n",
      "Epoch 7486, Loss: 1.3904647678136826, Final Batch Loss: 0.15268850326538086\n",
      "Epoch 7487, Loss: 1.3431412279605865, Final Batch Loss: 0.2672645151615143\n",
      "Epoch 7488, Loss: 1.297270953655243, Final Batch Loss: 0.25076594948768616\n",
      "Epoch 7489, Loss: 1.5893630981445312, Final Batch Loss: 0.4526436924934387\n",
      "Epoch 7490, Loss: 1.3119642585515976, Final Batch Loss: 0.16394780576229095\n",
      "Epoch 7491, Loss: 1.4288312494754791, Final Batch Loss: 0.27589067816734314\n",
      "Epoch 7492, Loss: 1.2697156220674515, Final Batch Loss: 0.20743004977703094\n",
      "Epoch 7493, Loss: 1.3254696130752563, Final Batch Loss: 0.3974241614341736\n",
      "Epoch 7494, Loss: 1.37478868663311, Final Batch Loss: 0.14961622655391693\n",
      "Epoch 7495, Loss: 1.2500887215137482, Final Batch Loss: 0.26634863018989563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7496, Loss: 1.3396054059267044, Final Batch Loss: 0.461292028427124\n",
      "Epoch 7497, Loss: 1.1212159544229507, Final Batch Loss: 0.16901859641075134\n",
      "Epoch 7498, Loss: 1.3118898272514343, Final Batch Loss: 0.2877878248691559\n",
      "Epoch 7499, Loss: 1.2487657517194748, Final Batch Loss: 0.18915429711341858\n",
      "Epoch 7500, Loss: 1.258301392197609, Final Batch Loss: 0.16486772894859314\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  1  0  0  2  0  0  0]\n",
      " [ 0 19  0  0  2  1  1  0]\n",
      " [ 0  1 24  0  0  0  0  0]\n",
      " [ 0  0  0 29  0  0  0  0]\n",
      " [ 1  0  0  0 15  0  0  0]\n",
      " [ 0  0  0  0  0 18  0  0]\n",
      " [ 0  0  0  0  0  0 24  0]\n",
      " [ 0  1  0  0  0  0  0 32]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.952     0.870     0.909        23\n",
      "           1      0.864     0.826     0.844        23\n",
      "           2      1.000     0.960     0.980        25\n",
      "           3      1.000     1.000     1.000        29\n",
      "           4      0.789     0.938     0.857        16\n",
      "           5      0.947     1.000     0.973        18\n",
      "           6      0.960     1.000     0.980        24\n",
      "           7      1.000     0.970     0.985        33\n",
      "\n",
      "    accuracy                          0.948       191\n",
      "   macro avg      0.939     0.945     0.941       191\n",
      "weighted avg      0.950     0.948     0.948       191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'../../../saved_models/UCI 8 User Classifier Ablation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
