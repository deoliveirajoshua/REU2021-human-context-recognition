{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '141 tBodyGyro-iqr()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '434 fBodyGyro-max()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 30),\n",
    "            classifier_block(30, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            nn.Linear(15, 15)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "    \n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_10 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_11 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_12 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_13 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_14 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_15 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9, X_10, X_11, X_12, X_13, X_14, X_15))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9) + [9] * len(X_10) + [10] * len(X_11) + [11] * len(X_12) + [12] * len(X_13) + [13] * len(X_14) + [14] * len(X_15)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5, 7, 8]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 8.177769184112549, Final Batch Loss: 2.7224531173706055\n",
      "Epoch 2, Loss: 8.167686462402344, Final Batch Loss: 2.7219529151916504\n",
      "Epoch 3, Loss: 8.15138554573059, Final Batch Loss: 2.705904722213745\n",
      "Epoch 4, Loss: 8.15267276763916, Final Batch Loss: 2.7183644771575928\n",
      "Epoch 5, Loss: 8.139990329742432, Final Batch Loss: 2.7069091796875\n",
      "Epoch 6, Loss: 8.133266687393188, Final Batch Loss: 2.717188835144043\n",
      "Epoch 7, Loss: 8.126067638397217, Final Batch Loss: 2.7089264392852783\n",
      "Epoch 8, Loss: 8.109956502914429, Final Batch Loss: 2.709545850753784\n",
      "Epoch 9, Loss: 8.093474864959717, Final Batch Loss: 2.6938278675079346\n",
      "Epoch 10, Loss: 8.07895040512085, Final Batch Loss: 2.680131196975708\n",
      "Epoch 11, Loss: 8.040251016616821, Final Batch Loss: 2.658381700515747\n",
      "Epoch 12, Loss: 8.016799926757812, Final Batch Loss: 2.659492015838623\n",
      "Epoch 13, Loss: 8.01409125328064, Final Batch Loss: 2.6736557483673096\n",
      "Epoch 14, Loss: 7.965330600738525, Final Batch Loss: 2.6631548404693604\n",
      "Epoch 15, Loss: 7.888197660446167, Final Batch Loss: 2.6033313274383545\n",
      "Epoch 16, Loss: 7.87068247795105, Final Batch Loss: 2.6146652698516846\n",
      "Epoch 17, Loss: 7.859624862670898, Final Batch Loss: 2.644122838973999\n",
      "Epoch 18, Loss: 7.7572174072265625, Final Batch Loss: 2.595968008041382\n",
      "Epoch 19, Loss: 7.684797525405884, Final Batch Loss: 2.547062635421753\n",
      "Epoch 20, Loss: 7.594206094741821, Final Batch Loss: 2.551877975463867\n",
      "Epoch 21, Loss: 7.585976600646973, Final Batch Loss: 2.56255841255188\n",
      "Epoch 22, Loss: 7.471914291381836, Final Batch Loss: 2.4758477210998535\n",
      "Epoch 23, Loss: 7.33875036239624, Final Batch Loss: 2.3725039958953857\n",
      "Epoch 24, Loss: 7.27415657043457, Final Batch Loss: 2.40080189704895\n",
      "Epoch 25, Loss: 7.184908390045166, Final Batch Loss: 2.3674156665802\n",
      "Epoch 26, Loss: 7.193332195281982, Final Batch Loss: 2.408163070678711\n",
      "Epoch 27, Loss: 7.0185747146606445, Final Batch Loss: 2.3379135131835938\n",
      "Epoch 28, Loss: 6.87817120552063, Final Batch Loss: 2.2683379650115967\n",
      "Epoch 29, Loss: 6.7207722663879395, Final Batch Loss: 2.171830892562866\n",
      "Epoch 30, Loss: 6.61695671081543, Final Batch Loss: 2.1285932064056396\n",
      "Epoch 31, Loss: 6.534668445587158, Final Batch Loss: 2.199145555496216\n",
      "Epoch 32, Loss: 6.418513774871826, Final Batch Loss: 2.1356217861175537\n",
      "Epoch 33, Loss: 6.334488868713379, Final Batch Loss: 2.1010735034942627\n",
      "Epoch 34, Loss: 6.179756164550781, Final Batch Loss: 2.0884451866149902\n",
      "Epoch 35, Loss: 6.058634042739868, Final Batch Loss: 1.9643206596374512\n",
      "Epoch 36, Loss: 5.962082386016846, Final Batch Loss: 1.9329469203948975\n",
      "Epoch 37, Loss: 5.947721242904663, Final Batch Loss: 1.9990872144699097\n",
      "Epoch 38, Loss: 5.753133177757263, Final Batch Loss: 1.8756077289581299\n",
      "Epoch 39, Loss: 5.774552345275879, Final Batch Loss: 1.8944777250289917\n",
      "Epoch 40, Loss: 5.681169629096985, Final Batch Loss: 1.9152755737304688\n",
      "Epoch 41, Loss: 5.562175631523132, Final Batch Loss: 1.862542748451233\n",
      "Epoch 42, Loss: 5.603547811508179, Final Batch Loss: 1.9299355745315552\n",
      "Epoch 43, Loss: 5.421637296676636, Final Batch Loss: 1.794094443321228\n",
      "Epoch 44, Loss: 5.313475728034973, Final Batch Loss: 1.7048529386520386\n",
      "Epoch 45, Loss: 5.177899718284607, Final Batch Loss: 1.658923625946045\n",
      "Epoch 46, Loss: 5.172831058502197, Final Batch Loss: 1.7072641849517822\n",
      "Epoch 47, Loss: 5.109084725379944, Final Batch Loss: 1.687175989151001\n",
      "Epoch 48, Loss: 5.0930880308151245, Final Batch Loss: 1.6962125301361084\n",
      "Epoch 49, Loss: 5.040841460227966, Final Batch Loss: 1.613142967224121\n",
      "Epoch 50, Loss: 5.000101327896118, Final Batch Loss: 1.559769630432129\n",
      "Epoch 51, Loss: 4.937719464302063, Final Batch Loss: 1.614216685295105\n",
      "Epoch 52, Loss: 4.918289542198181, Final Batch Loss: 1.5490747690200806\n",
      "Epoch 53, Loss: 5.009132027626038, Final Batch Loss: 1.728187918663025\n",
      "Epoch 54, Loss: 4.8854005336761475, Final Batch Loss: 1.6383302211761475\n",
      "Epoch 55, Loss: 4.787596940994263, Final Batch Loss: 1.5027638673782349\n",
      "Epoch 56, Loss: 4.840087294578552, Final Batch Loss: 1.6764495372772217\n",
      "Epoch 57, Loss: 4.85789167881012, Final Batch Loss: 1.6017452478408813\n",
      "Epoch 58, Loss: 4.670395851135254, Final Batch Loss: 1.5778306722640991\n",
      "Epoch 59, Loss: 4.687123537063599, Final Batch Loss: 1.593352198600769\n",
      "Epoch 60, Loss: 4.677736282348633, Final Batch Loss: 1.6091424226760864\n",
      "Epoch 61, Loss: 4.562361359596252, Final Batch Loss: 1.4523918628692627\n",
      "Epoch 62, Loss: 4.541875958442688, Final Batch Loss: 1.4414149522781372\n",
      "Epoch 63, Loss: 4.675313591957092, Final Batch Loss: 1.5348471403121948\n",
      "Epoch 64, Loss: 4.502419352531433, Final Batch Loss: 1.4645251035690308\n",
      "Epoch 65, Loss: 4.4946407079696655, Final Batch Loss: 1.4642913341522217\n",
      "Epoch 66, Loss: 4.465443015098572, Final Batch Loss: 1.499533772468567\n",
      "Epoch 67, Loss: 4.361310362815857, Final Batch Loss: 1.4402973651885986\n",
      "Epoch 68, Loss: 4.525070667266846, Final Batch Loss: 1.5574350357055664\n",
      "Epoch 69, Loss: 4.318168640136719, Final Batch Loss: 1.495490550994873\n",
      "Epoch 70, Loss: 4.381088614463806, Final Batch Loss: 1.4294410943984985\n",
      "Epoch 71, Loss: 4.382624268531799, Final Batch Loss: 1.471179485321045\n",
      "Epoch 72, Loss: 4.280541658401489, Final Batch Loss: 1.4350355863571167\n",
      "Epoch 73, Loss: 4.286966562271118, Final Batch Loss: 1.3970133066177368\n",
      "Epoch 74, Loss: 4.161634922027588, Final Batch Loss: 1.2970627546310425\n",
      "Epoch 75, Loss: 4.289954900741577, Final Batch Loss: 1.5125105381011963\n",
      "Epoch 76, Loss: 4.237999320030212, Final Batch Loss: 1.29783034324646\n",
      "Epoch 77, Loss: 4.22356903553009, Final Batch Loss: 1.4238899946212769\n",
      "Epoch 78, Loss: 4.194506764411926, Final Batch Loss: 1.3967393636703491\n",
      "Epoch 79, Loss: 4.139586567878723, Final Batch Loss: 1.4468477964401245\n",
      "Epoch 80, Loss: 4.037928104400635, Final Batch Loss: 1.403930425643921\n",
      "Epoch 81, Loss: 4.0757588148117065, Final Batch Loss: 1.3897212743759155\n",
      "Epoch 82, Loss: 4.056973338127136, Final Batch Loss: 1.272746205329895\n",
      "Epoch 83, Loss: 4.093069076538086, Final Batch Loss: 1.4154813289642334\n",
      "Epoch 84, Loss: 4.072448492050171, Final Batch Loss: 1.353615403175354\n",
      "Epoch 85, Loss: 3.9061264991760254, Final Batch Loss: 1.287606954574585\n",
      "Epoch 86, Loss: 4.027769446372986, Final Batch Loss: 1.3325309753417969\n",
      "Epoch 87, Loss: 4.051054120063782, Final Batch Loss: 1.3051528930664062\n",
      "Epoch 88, Loss: 3.970504403114319, Final Batch Loss: 1.4390450716018677\n",
      "Epoch 89, Loss: 3.90372097492218, Final Batch Loss: 1.229012370109558\n",
      "Epoch 90, Loss: 3.944624185562134, Final Batch Loss: 1.2923821210861206\n",
      "Epoch 91, Loss: 3.9806641340255737, Final Batch Loss: 1.2648838758468628\n",
      "Epoch 92, Loss: 4.034085512161255, Final Batch Loss: 1.3424378633499146\n",
      "Epoch 93, Loss: 4.000279545783997, Final Batch Loss: 1.2990953922271729\n",
      "Epoch 94, Loss: 3.818110942840576, Final Batch Loss: 1.293936014175415\n",
      "Epoch 95, Loss: 3.7708394527435303, Final Batch Loss: 1.2902010679244995\n",
      "Epoch 96, Loss: 3.856873035430908, Final Batch Loss: 1.221105933189392\n",
      "Epoch 97, Loss: 3.7367151975631714, Final Batch Loss: 1.245460033416748\n",
      "Epoch 98, Loss: 3.863235116004944, Final Batch Loss: 1.2661064863204956\n",
      "Epoch 99, Loss: 3.8398447036743164, Final Batch Loss: 1.2248780727386475\n",
      "Epoch 100, Loss: 3.748260021209717, Final Batch Loss: 1.269558072090149\n",
      "Epoch 101, Loss: 3.7280465364456177, Final Batch Loss: 1.2044463157653809\n",
      "Epoch 102, Loss: 3.673851490020752, Final Batch Loss: 1.2274959087371826\n",
      "Epoch 103, Loss: 3.6021440029144287, Final Batch Loss: 1.1582236289978027\n",
      "Epoch 104, Loss: 3.6725659370422363, Final Batch Loss: 1.23917555809021\n",
      "Epoch 105, Loss: 3.646911382675171, Final Batch Loss: 1.1532201766967773\n",
      "Epoch 106, Loss: 3.6901111602783203, Final Batch Loss: 1.2471495866775513\n",
      "Epoch 107, Loss: 3.5955286026000977, Final Batch Loss: 1.179597020149231\n",
      "Epoch 108, Loss: 3.593747615814209, Final Batch Loss: 1.0624092817306519\n",
      "Epoch 109, Loss: 3.746402621269226, Final Batch Loss: 1.2902191877365112\n",
      "Epoch 110, Loss: 3.6189626455307007, Final Batch Loss: 1.2713487148284912\n",
      "Epoch 111, Loss: 3.698426127433777, Final Batch Loss: 1.2284572124481201\n",
      "Epoch 112, Loss: 3.6898670196533203, Final Batch Loss: 1.2405794858932495\n",
      "Epoch 113, Loss: 3.5289233922958374, Final Batch Loss: 1.2662047147750854\n",
      "Epoch 114, Loss: 3.6294045448303223, Final Batch Loss: 1.1597305536270142\n",
      "Epoch 115, Loss: 3.489496946334839, Final Batch Loss: 1.170066475868225\n",
      "Epoch 116, Loss: 3.5018495321273804, Final Batch Loss: 1.1692560911178589\n",
      "Epoch 117, Loss: 3.5999640226364136, Final Batch Loss: 1.2335896492004395\n",
      "Epoch 118, Loss: 3.457703471183777, Final Batch Loss: 1.1882215738296509\n",
      "Epoch 119, Loss: 3.508690595626831, Final Batch Loss: 1.1032744646072388\n",
      "Epoch 120, Loss: 3.47324800491333, Final Batch Loss: 1.1345099210739136\n",
      "Epoch 121, Loss: 3.4158307313919067, Final Batch Loss: 1.1051957607269287\n",
      "Epoch 122, Loss: 3.4124765396118164, Final Batch Loss: 1.1227954626083374\n",
      "Epoch 123, Loss: 3.382441997528076, Final Batch Loss: 1.0975255966186523\n",
      "Epoch 124, Loss: 3.4710159301757812, Final Batch Loss: 1.1440880298614502\n",
      "Epoch 125, Loss: 3.5037893056869507, Final Batch Loss: 1.186222791671753\n",
      "Epoch 126, Loss: 3.476553201675415, Final Batch Loss: 1.2388601303100586\n",
      "Epoch 127, Loss: 3.318466544151306, Final Batch Loss: 1.0606520175933838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128, Loss: 3.4488022327423096, Final Batch Loss: 1.1662291288375854\n",
      "Epoch 129, Loss: 3.360398769378662, Final Batch Loss: 1.1469480991363525\n",
      "Epoch 130, Loss: 3.459819197654724, Final Batch Loss: 1.1565370559692383\n",
      "Epoch 131, Loss: 3.3738791942596436, Final Batch Loss: 1.1661906242370605\n",
      "Epoch 132, Loss: 3.3577369451522827, Final Batch Loss: 1.122756004333496\n",
      "Epoch 133, Loss: 3.361010432243347, Final Batch Loss: 1.1573371887207031\n",
      "Epoch 134, Loss: 3.214348077774048, Final Batch Loss: 1.0492806434631348\n",
      "Epoch 135, Loss: 3.366024613380432, Final Batch Loss: 1.089063286781311\n",
      "Epoch 136, Loss: 3.416367292404175, Final Batch Loss: 1.2142877578735352\n",
      "Epoch 137, Loss: 3.340568423271179, Final Batch Loss: 1.1397546529769897\n",
      "Epoch 138, Loss: 3.4205946922302246, Final Batch Loss: 1.1145747900009155\n",
      "Epoch 139, Loss: 3.3296287059783936, Final Batch Loss: 1.1089563369750977\n",
      "Epoch 140, Loss: 3.320494294166565, Final Batch Loss: 1.1232314109802246\n",
      "Epoch 141, Loss: 3.244964122772217, Final Batch Loss: 1.1378306150436401\n",
      "Epoch 142, Loss: 3.135804772377014, Final Batch Loss: 1.0885953903198242\n",
      "Epoch 143, Loss: 3.3220866918563843, Final Batch Loss: 1.1003774404525757\n",
      "Epoch 144, Loss: 3.3878380060195923, Final Batch Loss: 1.0956357717514038\n",
      "Epoch 145, Loss: 3.2134575843811035, Final Batch Loss: 1.0172797441482544\n",
      "Epoch 146, Loss: 3.2528682947158813, Final Batch Loss: 1.0476189851760864\n",
      "Epoch 147, Loss: 3.145811915397644, Final Batch Loss: 0.9876861572265625\n",
      "Epoch 148, Loss: 3.182479739189148, Final Batch Loss: 1.0260322093963623\n",
      "Epoch 149, Loss: 3.3006815910339355, Final Batch Loss: 1.064451813697815\n",
      "Epoch 150, Loss: 3.293771505355835, Final Batch Loss: 1.0929553508758545\n",
      "Epoch 151, Loss: 3.288378357887268, Final Batch Loss: 1.0387349128723145\n",
      "Epoch 152, Loss: 3.238651990890503, Final Batch Loss: 1.072560429573059\n",
      "Epoch 153, Loss: 3.132761001586914, Final Batch Loss: 1.0300538539886475\n",
      "Epoch 154, Loss: 3.2332231998443604, Final Batch Loss: 1.1154006719589233\n",
      "Epoch 155, Loss: 3.096230447292328, Final Batch Loss: 0.9677714705467224\n",
      "Epoch 156, Loss: 3.2854474782943726, Final Batch Loss: 1.1599125862121582\n",
      "Epoch 157, Loss: 3.120775043964386, Final Batch Loss: 1.1049458980560303\n",
      "Epoch 158, Loss: 3.1061261892318726, Final Batch Loss: 0.974291205406189\n",
      "Epoch 159, Loss: 3.13823664188385, Final Batch Loss: 1.1025993824005127\n",
      "Epoch 160, Loss: 3.201794385910034, Final Batch Loss: 1.1225621700286865\n",
      "Epoch 161, Loss: 3.1598832607269287, Final Batch Loss: 1.1096341609954834\n",
      "Epoch 162, Loss: 3.0539860129356384, Final Batch Loss: 0.9382409453392029\n",
      "Epoch 163, Loss: 3.0678133964538574, Final Batch Loss: 0.9922802448272705\n",
      "Epoch 164, Loss: 3.0595991611480713, Final Batch Loss: 0.9890363216400146\n",
      "Epoch 165, Loss: 3.0680671334266663, Final Batch Loss: 0.9010059237480164\n",
      "Epoch 166, Loss: 3.1314972043037415, Final Batch Loss: 1.1006207466125488\n",
      "Epoch 167, Loss: 3.1142526865005493, Final Batch Loss: 1.0413097143173218\n",
      "Epoch 168, Loss: 3.0871970653533936, Final Batch Loss: 1.0027282238006592\n",
      "Epoch 169, Loss: 3.177855968475342, Final Batch Loss: 1.0976073741912842\n",
      "Epoch 170, Loss: 3.0372724533081055, Final Batch Loss: 1.0362648963928223\n",
      "Epoch 171, Loss: 3.0029930472373962, Final Batch Loss: 0.9463583827018738\n",
      "Epoch 172, Loss: 3.0715301036834717, Final Batch Loss: 0.9401473999023438\n",
      "Epoch 173, Loss: 3.0828033685684204, Final Batch Loss: 1.0499727725982666\n",
      "Epoch 174, Loss: 3.0887691974639893, Final Batch Loss: 1.044358730316162\n",
      "Epoch 175, Loss: 3.120272397994995, Final Batch Loss: 1.012101411819458\n",
      "Epoch 176, Loss: 3.026005804538727, Final Batch Loss: 1.039056658744812\n",
      "Epoch 177, Loss: 3.2200440168380737, Final Batch Loss: 1.0819036960601807\n",
      "Epoch 178, Loss: 3.049159049987793, Final Batch Loss: 0.9935353994369507\n",
      "Epoch 179, Loss: 3.04234379529953, Final Batch Loss: 1.0627340078353882\n",
      "Epoch 180, Loss: 3.045270562171936, Final Batch Loss: 1.0028226375579834\n",
      "Epoch 181, Loss: 2.980983018875122, Final Batch Loss: 0.9095536470413208\n",
      "Epoch 182, Loss: 2.9634299278259277, Final Batch Loss: 0.966346800327301\n",
      "Epoch 183, Loss: 2.976730167865753, Final Batch Loss: 0.9764610528945923\n",
      "Epoch 184, Loss: 2.9493213295936584, Final Batch Loss: 0.9401425719261169\n",
      "Epoch 185, Loss: 2.9694133400917053, Final Batch Loss: 0.9825788140296936\n",
      "Epoch 186, Loss: 3.0142993927001953, Final Batch Loss: 0.9467227458953857\n",
      "Epoch 187, Loss: 2.942677617073059, Final Batch Loss: 0.9779704213142395\n",
      "Epoch 188, Loss: 2.8479522466659546, Final Batch Loss: 0.8860786557197571\n",
      "Epoch 189, Loss: 2.8608208894729614, Final Batch Loss: 0.9022870659828186\n",
      "Epoch 190, Loss: 2.9192737340927124, Final Batch Loss: 1.0194120407104492\n",
      "Epoch 191, Loss: 2.879314124584198, Final Batch Loss: 1.024112343788147\n",
      "Epoch 192, Loss: 2.931502401828766, Final Batch Loss: 0.9750944375991821\n",
      "Epoch 193, Loss: 2.91424697637558, Final Batch Loss: 0.9857841730117798\n",
      "Epoch 194, Loss: 2.924207091331482, Final Batch Loss: 0.9679756164550781\n",
      "Epoch 195, Loss: 2.885851502418518, Final Batch Loss: 0.9573173522949219\n",
      "Epoch 196, Loss: 2.9403576254844666, Final Batch Loss: 1.014032244682312\n",
      "Epoch 197, Loss: 3.048592269420624, Final Batch Loss: 0.9904798269271851\n",
      "Epoch 198, Loss: 2.954798460006714, Final Batch Loss: 0.9725014567375183\n",
      "Epoch 199, Loss: 2.961414337158203, Final Batch Loss: 0.9513034820556641\n",
      "Epoch 200, Loss: 2.8024922013282776, Final Batch Loss: 0.9098090529441833\n",
      "Epoch 201, Loss: 2.9168564081192017, Final Batch Loss: 0.9159149527549744\n",
      "Epoch 202, Loss: 2.9223411679267883, Final Batch Loss: 0.9119862914085388\n",
      "Epoch 203, Loss: 2.9496994018554688, Final Batch Loss: 0.9562221169471741\n",
      "Epoch 204, Loss: 2.8522183299064636, Final Batch Loss: 0.9444637894630432\n",
      "Epoch 205, Loss: 2.9380807280540466, Final Batch Loss: 0.9689505100250244\n",
      "Epoch 206, Loss: 2.8381200432777405, Final Batch Loss: 0.9575412273406982\n",
      "Epoch 207, Loss: 2.9797654151916504, Final Batch Loss: 1.0205042362213135\n",
      "Epoch 208, Loss: 2.9403820633888245, Final Batch Loss: 1.0342128276824951\n",
      "Epoch 209, Loss: 2.8104482293128967, Final Batch Loss: 0.8852535486221313\n",
      "Epoch 210, Loss: 2.8406195640563965, Final Batch Loss: 1.002931833267212\n",
      "Epoch 211, Loss: 2.8041186332702637, Final Batch Loss: 0.8504015803337097\n",
      "Epoch 212, Loss: 2.7988268733024597, Final Batch Loss: 0.9134913682937622\n",
      "Epoch 213, Loss: 2.8028939962387085, Final Batch Loss: 0.9126226305961609\n",
      "Epoch 214, Loss: 2.941298484802246, Final Batch Loss: 0.9881295561790466\n",
      "Epoch 215, Loss: 2.8384868502616882, Final Batch Loss: 0.9722570180892944\n",
      "Epoch 216, Loss: 2.861913502216339, Final Batch Loss: 0.9524564146995544\n",
      "Epoch 217, Loss: 2.8389118313789368, Final Batch Loss: 0.8934910893440247\n",
      "Epoch 218, Loss: 2.9121899008750916, Final Batch Loss: 0.9767138361930847\n",
      "Epoch 219, Loss: 2.8016101717948914, Final Batch Loss: 0.886703610420227\n",
      "Epoch 220, Loss: 2.8305715918540955, Final Batch Loss: 0.9493004083633423\n",
      "Epoch 221, Loss: 2.7977346777915955, Final Batch Loss: 0.9401026964187622\n",
      "Epoch 222, Loss: 2.855763614177704, Final Batch Loss: 0.9362910389900208\n",
      "Epoch 223, Loss: 2.810703217983246, Final Batch Loss: 0.9445961713790894\n",
      "Epoch 224, Loss: 2.878982901573181, Final Batch Loss: 0.9806160926818848\n",
      "Epoch 225, Loss: 2.8231120705604553, Final Batch Loss: 0.9102538228034973\n",
      "Epoch 226, Loss: 2.775095820426941, Final Batch Loss: 0.8562299609184265\n",
      "Epoch 227, Loss: 2.756642520427704, Final Batch Loss: 0.8526765704154968\n",
      "Epoch 228, Loss: 2.7936747670173645, Final Batch Loss: 0.9272332191467285\n",
      "Epoch 229, Loss: 2.847108483314514, Final Batch Loss: 0.969663679599762\n",
      "Epoch 230, Loss: 2.7930092215538025, Final Batch Loss: 0.9336206316947937\n",
      "Epoch 231, Loss: 2.764233410358429, Final Batch Loss: 0.9168110489845276\n",
      "Epoch 232, Loss: 2.9287222623825073, Final Batch Loss: 1.0201793909072876\n",
      "Epoch 233, Loss: 2.803293824195862, Final Batch Loss: 0.943336009979248\n",
      "Epoch 234, Loss: 2.7681198716163635, Final Batch Loss: 0.8958322405815125\n",
      "Epoch 235, Loss: 2.728283643722534, Final Batch Loss: 0.834125816822052\n",
      "Epoch 236, Loss: 2.751371145248413, Final Batch Loss: 0.9306842088699341\n",
      "Epoch 237, Loss: 2.75341135263443, Final Batch Loss: 0.9002202153205872\n",
      "Epoch 238, Loss: 2.7577577233314514, Final Batch Loss: 0.9081096649169922\n",
      "Epoch 239, Loss: 2.858277678489685, Final Batch Loss: 0.9388476014137268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240, Loss: 2.737009108066559, Final Batch Loss: 0.9028817415237427\n",
      "Epoch 241, Loss: 2.7322688698768616, Final Batch Loss: 0.8967946767807007\n",
      "Epoch 242, Loss: 2.7549678087234497, Final Batch Loss: 0.9510049223899841\n",
      "Epoch 243, Loss: 2.6720088720321655, Final Batch Loss: 0.8099027276039124\n",
      "Epoch 244, Loss: 2.7036232352256775, Final Batch Loss: 0.9524038434028625\n",
      "Epoch 245, Loss: 2.7093843817710876, Final Batch Loss: 0.9121677279472351\n",
      "Epoch 246, Loss: 2.8274841904640198, Final Batch Loss: 0.9547707438468933\n",
      "Epoch 247, Loss: 2.8006219267845154, Final Batch Loss: 0.95805823802948\n",
      "Epoch 248, Loss: 2.6386460661888123, Final Batch Loss: 0.808814287185669\n",
      "Epoch 249, Loss: 2.6489736437797546, Final Batch Loss: 0.8418446779251099\n",
      "Epoch 250, Loss: 2.6481334567070007, Final Batch Loss: 0.8883662819862366\n",
      "Epoch 251, Loss: 2.7247522473335266, Final Batch Loss: 0.8031099438667297\n",
      "Epoch 252, Loss: 2.6531792283058167, Final Batch Loss: 0.8879942297935486\n",
      "Epoch 253, Loss: 2.8381585478782654, Final Batch Loss: 0.9539546370506287\n",
      "Epoch 254, Loss: 2.6494680047035217, Final Batch Loss: 0.8033277988433838\n",
      "Epoch 255, Loss: 2.6742308735847473, Final Batch Loss: 0.9276791214942932\n",
      "Epoch 256, Loss: 2.7448466420173645, Final Batch Loss: 0.8590317964553833\n",
      "Epoch 257, Loss: 2.7208303213119507, Final Batch Loss: 0.8700751662254333\n",
      "Epoch 258, Loss: 2.6501993536949158, Final Batch Loss: 0.8769069314002991\n",
      "Epoch 259, Loss: 2.8687856793403625, Final Batch Loss: 0.9501959085464478\n",
      "Epoch 260, Loss: 2.6992276906967163, Final Batch Loss: 0.8662647604942322\n",
      "Epoch 261, Loss: 2.8080472350120544, Final Batch Loss: 0.9532427787780762\n",
      "Epoch 262, Loss: 2.7809334993362427, Final Batch Loss: 0.9924599528312683\n",
      "Epoch 263, Loss: 2.7348079681396484, Final Batch Loss: 0.9824844598770142\n",
      "Epoch 264, Loss: 2.7337805032730103, Final Batch Loss: 0.9441568851470947\n",
      "Epoch 265, Loss: 2.761589467525482, Final Batch Loss: 0.96430903673172\n",
      "Epoch 266, Loss: 2.684341251850128, Final Batch Loss: 0.8711333274841309\n",
      "Epoch 267, Loss: 2.7182604670524597, Final Batch Loss: 0.8684492707252502\n",
      "Epoch 268, Loss: 2.6690667867660522, Final Batch Loss: 0.8305436968803406\n",
      "Epoch 269, Loss: 2.7220365405082703, Final Batch Loss: 0.9336220622062683\n",
      "Epoch 270, Loss: 2.7020678520202637, Final Batch Loss: 0.9507686495780945\n",
      "Epoch 271, Loss: 2.8112913370132446, Final Batch Loss: 1.0302070379257202\n",
      "Epoch 272, Loss: 2.673182249069214, Final Batch Loss: 0.9595748782157898\n",
      "Epoch 273, Loss: 2.726827383041382, Final Batch Loss: 0.8625507950782776\n",
      "Epoch 274, Loss: 2.594662368297577, Final Batch Loss: 0.7752383947372437\n",
      "Epoch 275, Loss: 2.679226338863373, Final Batch Loss: 0.8875069618225098\n",
      "Epoch 276, Loss: 2.6235864758491516, Final Batch Loss: 0.7954384088516235\n",
      "Epoch 277, Loss: 2.662829279899597, Final Batch Loss: 0.8841004371643066\n",
      "Epoch 278, Loss: 2.6679776906967163, Final Batch Loss: 0.8616129755973816\n",
      "Epoch 279, Loss: 2.7645122408866882, Final Batch Loss: 0.9409922957420349\n",
      "Epoch 280, Loss: 2.6654524207115173, Final Batch Loss: 0.9188635945320129\n",
      "Epoch 281, Loss: 2.6617414355278015, Final Batch Loss: 0.8719480633735657\n",
      "Epoch 282, Loss: 2.689722418785095, Final Batch Loss: 0.904853880405426\n",
      "Epoch 283, Loss: 2.732740044593811, Final Batch Loss: 0.8970135450363159\n",
      "Epoch 284, Loss: 2.7410165667533875, Final Batch Loss: 1.008191466331482\n",
      "Epoch 285, Loss: 2.6604363322257996, Final Batch Loss: 0.9264129996299744\n",
      "Epoch 286, Loss: 2.620795786380768, Final Batch Loss: 0.8926501870155334\n",
      "Epoch 287, Loss: 2.684961259365082, Final Batch Loss: 0.9480926394462585\n",
      "Epoch 288, Loss: 2.702539622783661, Final Batch Loss: 0.8522327542304993\n",
      "Epoch 289, Loss: 2.49147492647171, Final Batch Loss: 0.7762876152992249\n",
      "Epoch 290, Loss: 2.6495746970176697, Final Batch Loss: 0.894145131111145\n",
      "Epoch 291, Loss: 2.8101041316986084, Final Batch Loss: 0.941550612449646\n",
      "Epoch 292, Loss: 2.7267003655433655, Final Batch Loss: 0.9390221238136292\n",
      "Epoch 293, Loss: 2.6225839853286743, Final Batch Loss: 1.0038591623306274\n",
      "Epoch 294, Loss: 2.646295666694641, Final Batch Loss: 0.879413902759552\n",
      "Epoch 295, Loss: 2.585080146789551, Final Batch Loss: 0.8725448250770569\n",
      "Epoch 296, Loss: 2.7278847694396973, Final Batch Loss: 0.8699795603752136\n",
      "Epoch 297, Loss: 2.669253945350647, Final Batch Loss: 0.9450031518936157\n",
      "Epoch 298, Loss: 2.5746673345565796, Final Batch Loss: 0.8337123990058899\n",
      "Epoch 299, Loss: 2.5991798043251038, Final Batch Loss: 0.8971091508865356\n",
      "Epoch 300, Loss: 2.6171973943710327, Final Batch Loss: 0.8497815728187561\n",
      "Epoch 301, Loss: 2.6017046570777893, Final Batch Loss: 0.8607198596000671\n",
      "Epoch 302, Loss: 2.501955032348633, Final Batch Loss: 0.8025829792022705\n",
      "Epoch 303, Loss: 2.808016538619995, Final Batch Loss: 0.9885754585266113\n",
      "Epoch 304, Loss: 2.744615614414215, Final Batch Loss: 0.9952852129936218\n",
      "Epoch 305, Loss: 2.6606428027153015, Final Batch Loss: 0.8413922786712646\n",
      "Epoch 306, Loss: 2.5941110253334045, Final Batch Loss: 0.8473834991455078\n",
      "Epoch 307, Loss: 2.607113003730774, Final Batch Loss: 0.9235768914222717\n",
      "Epoch 308, Loss: 2.6520285606384277, Final Batch Loss: 1.0292106866836548\n",
      "Epoch 309, Loss: 2.6744338870048523, Final Batch Loss: 0.968328595161438\n",
      "Epoch 310, Loss: 2.572250485420227, Final Batch Loss: 0.863063395023346\n",
      "Epoch 311, Loss: 2.4878878593444824, Final Batch Loss: 0.7610334753990173\n",
      "Epoch 312, Loss: 2.5156190991401672, Final Batch Loss: 0.832013726234436\n",
      "Epoch 313, Loss: 2.543520987033844, Final Batch Loss: 0.7590922117233276\n",
      "Epoch 314, Loss: 2.642708659172058, Final Batch Loss: 0.823578953742981\n",
      "Epoch 315, Loss: 2.669753849506378, Final Batch Loss: 0.9720177054405212\n",
      "Epoch 316, Loss: 2.5816839933395386, Final Batch Loss: 0.8043999075889587\n",
      "Epoch 317, Loss: 2.6109017729759216, Final Batch Loss: 0.894037127494812\n",
      "Epoch 318, Loss: 2.555097460746765, Final Batch Loss: 0.8041553497314453\n",
      "Epoch 319, Loss: 2.643242657184601, Final Batch Loss: 0.871627151966095\n",
      "Epoch 320, Loss: 2.6621312499046326, Final Batch Loss: 0.8452667593955994\n",
      "Epoch 321, Loss: 2.7090802788734436, Final Batch Loss: 0.8870818614959717\n",
      "Epoch 322, Loss: 2.5580480098724365, Final Batch Loss: 0.8982517123222351\n",
      "Epoch 323, Loss: 2.561726927757263, Final Batch Loss: 0.8578466176986694\n",
      "Epoch 324, Loss: 2.5119743943214417, Final Batch Loss: 0.7241882681846619\n",
      "Epoch 325, Loss: 2.6074212193489075, Final Batch Loss: 0.7992199659347534\n",
      "Epoch 326, Loss: 2.5685441493988037, Final Batch Loss: 0.9235657453536987\n",
      "Epoch 327, Loss: 2.6197440028190613, Final Batch Loss: 0.8472226858139038\n",
      "Epoch 328, Loss: 2.569710850715637, Final Batch Loss: 0.8194306492805481\n",
      "Epoch 329, Loss: 2.5612756609916687, Final Batch Loss: 0.7259432673454285\n",
      "Epoch 330, Loss: 2.4801159501075745, Final Batch Loss: 0.8041512370109558\n",
      "Epoch 331, Loss: 2.5061752796173096, Final Batch Loss: 0.8622384071350098\n",
      "Epoch 332, Loss: 2.506678819656372, Final Batch Loss: 0.8608629703521729\n",
      "Epoch 333, Loss: 2.5692026019096375, Final Batch Loss: 0.8548930287361145\n",
      "Epoch 334, Loss: 2.563567817211151, Final Batch Loss: 0.7822926044464111\n",
      "Epoch 335, Loss: 2.5218291878700256, Final Batch Loss: 0.7628501653671265\n",
      "Epoch 336, Loss: 2.439833343029022, Final Batch Loss: 0.761017382144928\n",
      "Epoch 337, Loss: 2.3846181631088257, Final Batch Loss: 0.7217320799827576\n",
      "Epoch 338, Loss: 2.5733076333999634, Final Batch Loss: 0.9299426078796387\n",
      "Epoch 339, Loss: 2.5287728905677795, Final Batch Loss: 0.8103026747703552\n",
      "Epoch 340, Loss: 2.4686789512634277, Final Batch Loss: 0.8153154253959656\n",
      "Epoch 341, Loss: 2.5241127014160156, Final Batch Loss: 0.8370652198791504\n",
      "Epoch 342, Loss: 2.5507760643959045, Final Batch Loss: 0.8929805755615234\n",
      "Epoch 343, Loss: 2.6080674529075623, Final Batch Loss: 0.8733048439025879\n",
      "Epoch 344, Loss: 2.4569615721702576, Final Batch Loss: 0.8214411735534668\n",
      "Epoch 345, Loss: 2.502255380153656, Final Batch Loss: 0.6810926795005798\n",
      "Epoch 346, Loss: 2.37440687417984, Final Batch Loss: 0.691504716873169\n",
      "Epoch 347, Loss: 2.612230896949768, Final Batch Loss: 0.9622572064399719\n",
      "Epoch 348, Loss: 2.489743709564209, Final Batch Loss: 0.8147445917129517\n",
      "Epoch 349, Loss: 2.59078049659729, Final Batch Loss: 0.8420205116271973\n",
      "Epoch 350, Loss: 2.5716617703437805, Final Batch Loss: 0.8038753867149353\n",
      "Epoch 351, Loss: 2.6412696838378906, Final Batch Loss: 0.853796124458313\n",
      "Epoch 352, Loss: 2.50791335105896, Final Batch Loss: 0.8231328129768372\n",
      "Epoch 353, Loss: 2.5501686334609985, Final Batch Loss: 0.8978266716003418\n",
      "Epoch 354, Loss: 2.462068200111389, Final Batch Loss: 0.818480908870697\n",
      "Epoch 355, Loss: 2.4613503217697144, Final Batch Loss: 0.8214276432991028\n",
      "Epoch 356, Loss: 2.559179902076721, Final Batch Loss: 0.9037506580352783\n",
      "Epoch 357, Loss: 2.5872929096221924, Final Batch Loss: 0.8428670167922974\n",
      "Epoch 358, Loss: 2.562416136264801, Final Batch Loss: 0.817574679851532\n",
      "Epoch 359, Loss: 2.473692834377289, Final Batch Loss: 0.744651198387146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 360, Loss: 2.443763494491577, Final Batch Loss: 0.7702013850212097\n",
      "Epoch 361, Loss: 2.5993043184280396, Final Batch Loss: 0.8261994123458862\n",
      "Epoch 362, Loss: 2.397452652454376, Final Batch Loss: 0.6920777559280396\n",
      "Epoch 363, Loss: 2.4247010350227356, Final Batch Loss: 0.7210518717765808\n",
      "Epoch 364, Loss: 2.385859787464142, Final Batch Loss: 0.789771318435669\n",
      "Epoch 365, Loss: 2.4677241444587708, Final Batch Loss: 0.8002732992172241\n",
      "Epoch 366, Loss: 2.4676854014396667, Final Batch Loss: 0.7766843438148499\n",
      "Epoch 367, Loss: 2.371144413948059, Final Batch Loss: 0.708256185054779\n",
      "Epoch 368, Loss: 2.40841007232666, Final Batch Loss: 0.7706584930419922\n",
      "Epoch 369, Loss: 2.503638803958893, Final Batch Loss: 0.7853737473487854\n",
      "Epoch 370, Loss: 2.5132779479026794, Final Batch Loss: 0.8663626313209534\n",
      "Epoch 371, Loss: 2.4252877831459045, Final Batch Loss: 0.8627345561981201\n",
      "Epoch 372, Loss: 2.5336243510246277, Final Batch Loss: 0.8733600378036499\n",
      "Epoch 373, Loss: 2.32599538564682, Final Batch Loss: 0.6901715993881226\n",
      "Epoch 374, Loss: 2.497986912727356, Final Batch Loss: 0.8510445952415466\n",
      "Epoch 375, Loss: 2.388876259326935, Final Batch Loss: 0.7632989287376404\n",
      "Epoch 376, Loss: 2.4566949009895325, Final Batch Loss: 0.9244601726531982\n",
      "Epoch 377, Loss: 2.5358659625053406, Final Batch Loss: 0.9013989567756653\n",
      "Epoch 378, Loss: 2.478245973587036, Final Batch Loss: 0.8775966763496399\n",
      "Epoch 379, Loss: 2.5822654962539673, Final Batch Loss: 0.9314606189727783\n",
      "Epoch 380, Loss: 2.4113636016845703, Final Batch Loss: 0.8032755255699158\n",
      "Epoch 381, Loss: 2.460488021373749, Final Batch Loss: 0.9050251245498657\n",
      "Epoch 382, Loss: 2.4850249886512756, Final Batch Loss: 0.8426943421363831\n",
      "Epoch 383, Loss: 2.540160655975342, Final Batch Loss: 0.8504537343978882\n",
      "Epoch 384, Loss: 2.428251326084137, Final Batch Loss: 0.7400012612342834\n",
      "Epoch 385, Loss: 2.5894691944122314, Final Batch Loss: 0.9704911708831787\n",
      "Epoch 386, Loss: 2.31953227519989, Final Batch Loss: 0.710979163646698\n",
      "Epoch 387, Loss: 2.328855037689209, Final Batch Loss: 0.6974084377288818\n",
      "Epoch 388, Loss: 2.4245883226394653, Final Batch Loss: 0.8114388585090637\n",
      "Epoch 389, Loss: 2.5124130249023438, Final Batch Loss: 0.9326416850090027\n",
      "Epoch 390, Loss: 2.326465368270874, Final Batch Loss: 0.7503995299339294\n",
      "Epoch 391, Loss: 2.406895160675049, Final Batch Loss: 0.8301814198493958\n",
      "Epoch 392, Loss: 2.4725899696350098, Final Batch Loss: 0.8982447385787964\n",
      "Epoch 393, Loss: 2.3319689631462097, Final Batch Loss: 0.7009584903717041\n",
      "Epoch 394, Loss: 2.3651247024536133, Final Batch Loss: 0.7325552105903625\n",
      "Epoch 395, Loss: 2.39814293384552, Final Batch Loss: 0.8686071634292603\n",
      "Epoch 396, Loss: 2.526766002178192, Final Batch Loss: 0.8222929239273071\n",
      "Epoch 397, Loss: 2.386536717414856, Final Batch Loss: 0.863848090171814\n",
      "Epoch 398, Loss: 2.4525933861732483, Final Batch Loss: 0.7883917093276978\n",
      "Epoch 399, Loss: 2.3626574873924255, Final Batch Loss: 0.725608229637146\n",
      "Epoch 400, Loss: 2.452112376689911, Final Batch Loss: 0.7969194054603577\n",
      "Epoch 401, Loss: 2.354416847229004, Final Batch Loss: 0.7870728969573975\n",
      "Epoch 402, Loss: 2.603819489479065, Final Batch Loss: 0.9471051096916199\n",
      "Epoch 403, Loss: 2.4199382662773132, Final Batch Loss: 0.8021360635757446\n",
      "Epoch 404, Loss: 2.4856178164482117, Final Batch Loss: 0.8415811061859131\n",
      "Epoch 405, Loss: 2.360780954360962, Final Batch Loss: 0.7679499983787537\n",
      "Epoch 406, Loss: 2.3565028309822083, Final Batch Loss: 0.8033363819122314\n",
      "Epoch 407, Loss: 2.4988616704940796, Final Batch Loss: 0.781428873538971\n",
      "Epoch 408, Loss: 2.4260297417640686, Final Batch Loss: 0.8164941072463989\n",
      "Epoch 409, Loss: 2.481023669242859, Final Batch Loss: 0.8621823191642761\n",
      "Epoch 410, Loss: 2.3831700682640076, Final Batch Loss: 0.7107336521148682\n",
      "Epoch 411, Loss: 2.3617743849754333, Final Batch Loss: 0.7130343317985535\n",
      "Epoch 412, Loss: 2.45525199174881, Final Batch Loss: 0.8637365698814392\n",
      "Epoch 413, Loss: 2.3749613761901855, Final Batch Loss: 0.7200019359588623\n",
      "Epoch 414, Loss: 2.555878698825836, Final Batch Loss: 0.9135505557060242\n",
      "Epoch 415, Loss: 2.408923923969269, Final Batch Loss: 0.7948063015937805\n",
      "Epoch 416, Loss: 2.4305259585380554, Final Batch Loss: 0.8713452816009521\n",
      "Epoch 417, Loss: 2.47518789768219, Final Batch Loss: 0.8925150036811829\n",
      "Epoch 418, Loss: 2.427900969982147, Final Batch Loss: 0.8278270363807678\n",
      "Epoch 419, Loss: 2.3477375507354736, Final Batch Loss: 0.8329111337661743\n",
      "Epoch 420, Loss: 2.471375048160553, Final Batch Loss: 0.8578453660011292\n",
      "Epoch 421, Loss: 2.4572790265083313, Final Batch Loss: 0.7912123203277588\n",
      "Epoch 422, Loss: 2.450127601623535, Final Batch Loss: 0.8788339495658875\n",
      "Epoch 423, Loss: 2.397235155105591, Final Batch Loss: 0.8202054500579834\n",
      "Epoch 424, Loss: 2.4371965527534485, Final Batch Loss: 0.8612392544746399\n",
      "Epoch 425, Loss: 2.358988583087921, Final Batch Loss: 0.8104529976844788\n",
      "Epoch 426, Loss: 2.4220070242881775, Final Batch Loss: 0.8590302467346191\n",
      "Epoch 427, Loss: 2.411085605621338, Final Batch Loss: 0.8172782063484192\n",
      "Epoch 428, Loss: 2.3851399421691895, Final Batch Loss: 0.7965672016143799\n",
      "Epoch 429, Loss: 2.3274887800216675, Final Batch Loss: 0.7488712668418884\n",
      "Epoch 430, Loss: 2.369913339614868, Final Batch Loss: 0.7793533205986023\n",
      "Epoch 431, Loss: 2.314221501350403, Final Batch Loss: 0.7233201861381531\n",
      "Epoch 432, Loss: 2.301932692527771, Final Batch Loss: 0.7683584094047546\n",
      "Epoch 433, Loss: 2.437349319458008, Final Batch Loss: 0.8174758553504944\n",
      "Epoch 434, Loss: 2.4417173266410828, Final Batch Loss: 0.8844852447509766\n",
      "Epoch 435, Loss: 2.4538695216178894, Final Batch Loss: 0.792928159236908\n",
      "Epoch 436, Loss: 2.3978524208068848, Final Batch Loss: 0.7164378762245178\n",
      "Epoch 437, Loss: 2.311670660972595, Final Batch Loss: 0.7687996029853821\n",
      "Epoch 438, Loss: 2.4368661046028137, Final Batch Loss: 0.8128686547279358\n",
      "Epoch 439, Loss: 2.342411160469055, Final Batch Loss: 0.7365854382514954\n",
      "Epoch 440, Loss: 2.3687846064567566, Final Batch Loss: 0.7574335336685181\n",
      "Epoch 441, Loss: 2.4428648948669434, Final Batch Loss: 0.7914477586746216\n",
      "Epoch 442, Loss: 2.37568598985672, Final Batch Loss: 0.861259937286377\n",
      "Epoch 443, Loss: 2.3530160188674927, Final Batch Loss: 0.7663677930831909\n",
      "Epoch 444, Loss: 2.367802679538727, Final Batch Loss: 0.7418045401573181\n",
      "Epoch 445, Loss: 2.2542930841445923, Final Batch Loss: 0.6836293935775757\n",
      "Epoch 446, Loss: 2.457957446575165, Final Batch Loss: 0.830858588218689\n",
      "Epoch 447, Loss: 2.3374547958374023, Final Batch Loss: 0.7602706551551819\n",
      "Epoch 448, Loss: 2.387914538383484, Final Batch Loss: 0.7558527588844299\n",
      "Epoch 449, Loss: 2.2276890873908997, Final Batch Loss: 0.6940872073173523\n",
      "Epoch 450, Loss: 2.2452186942100525, Final Batch Loss: 0.7105236649513245\n",
      "Epoch 451, Loss: 2.487621784210205, Final Batch Loss: 0.7869499921798706\n",
      "Epoch 452, Loss: 2.3138956427574158, Final Batch Loss: 0.7817291021347046\n",
      "Epoch 453, Loss: 2.337015211582184, Final Batch Loss: 0.7553231716156006\n",
      "Epoch 454, Loss: 2.3595675826072693, Final Batch Loss: 0.7387663125991821\n",
      "Epoch 455, Loss: 2.382036328315735, Final Batch Loss: 0.7509896755218506\n",
      "Epoch 456, Loss: 2.4046417474746704, Final Batch Loss: 0.8130341172218323\n",
      "Epoch 457, Loss: 2.3338661789894104, Final Batch Loss: 0.7839410305023193\n",
      "Epoch 458, Loss: 2.3257168531417847, Final Batch Loss: 0.7318050861358643\n",
      "Epoch 459, Loss: 2.3789634108543396, Final Batch Loss: 0.8453581929206848\n",
      "Epoch 460, Loss: 2.280705511569977, Final Batch Loss: 0.7308996319770813\n",
      "Epoch 461, Loss: 2.269290268421173, Final Batch Loss: 0.7351393103599548\n",
      "Epoch 462, Loss: 2.436460793018341, Final Batch Loss: 0.8260705471038818\n",
      "Epoch 463, Loss: 2.4253352284431458, Final Batch Loss: 0.8745278120040894\n",
      "Epoch 464, Loss: 2.3432366847991943, Final Batch Loss: 0.8156611919403076\n",
      "Epoch 465, Loss: 2.370745360851288, Final Batch Loss: 0.8732618689537048\n",
      "Epoch 466, Loss: 2.2154884338378906, Final Batch Loss: 0.6960999369621277\n",
      "Epoch 467, Loss: 2.434564173221588, Final Batch Loss: 0.8772122263908386\n",
      "Epoch 468, Loss: 2.331956446170807, Final Batch Loss: 0.8382842540740967\n",
      "Epoch 469, Loss: 2.261327028274536, Final Batch Loss: 0.7649215459823608\n",
      "Epoch 470, Loss: 2.197963833808899, Final Batch Loss: 0.7826052308082581\n",
      "Epoch 471, Loss: 2.27385276556015, Final Batch Loss: 0.6947348713874817\n",
      "Epoch 472, Loss: 2.374206066131592, Final Batch Loss: 0.8809242248535156\n",
      "Epoch 473, Loss: 2.409051239490509, Final Batch Loss: 0.821708083152771\n",
      "Epoch 474, Loss: 2.3052573800086975, Final Batch Loss: 0.8327878713607788\n",
      "Epoch 475, Loss: 2.351977586746216, Final Batch Loss: 0.8048569560050964\n",
      "Epoch 476, Loss: 2.2740780115127563, Final Batch Loss: 0.7589473724365234\n",
      "Epoch 477, Loss: 2.290418326854706, Final Batch Loss: 0.820956289768219\n",
      "Epoch 478, Loss: 2.262023150920868, Final Batch Loss: 0.743073046207428\n",
      "Epoch 479, Loss: 2.3290138244628906, Final Batch Loss: 0.8215264678001404\n",
      "Epoch 480, Loss: 2.45710951089859, Final Batch Loss: 0.9531450867652893\n",
      "Epoch 481, Loss: 2.2716500759124756, Final Batch Loss: 0.7380893230438232\n",
      "Epoch 482, Loss: 2.3004263639450073, Final Batch Loss: 0.7834615111351013\n",
      "Epoch 483, Loss: 2.2870407700538635, Final Batch Loss: 0.7606531381607056\n",
      "Epoch 484, Loss: 2.357107937335968, Final Batch Loss: 0.8065552115440369\n",
      "Epoch 485, Loss: 2.320274591445923, Final Batch Loss: 0.7601836323738098\n",
      "Epoch 486, Loss: 2.380006492137909, Final Batch Loss: 0.8751737475395203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 487, Loss: 2.3509780764579773, Final Batch Loss: 0.8077635765075684\n",
      "Epoch 488, Loss: 2.2793740034103394, Final Batch Loss: 0.8066166639328003\n",
      "Epoch 489, Loss: 2.28692227602005, Final Batch Loss: 0.6711451411247253\n",
      "Epoch 490, Loss: 2.338951826095581, Final Batch Loss: 0.6983197927474976\n",
      "Epoch 491, Loss: 2.3311960101127625, Final Batch Loss: 0.9151055812835693\n",
      "Epoch 492, Loss: 2.204559624195099, Final Batch Loss: 0.7171642780303955\n",
      "Epoch 493, Loss: 2.346126616001129, Final Batch Loss: 0.7988313436508179\n",
      "Epoch 494, Loss: 2.3858678340911865, Final Batch Loss: 0.7599321007728577\n",
      "Epoch 495, Loss: 2.2736161947250366, Final Batch Loss: 0.7483795881271362\n",
      "Epoch 496, Loss: 2.324258506298065, Final Batch Loss: 0.8467038869857788\n",
      "Epoch 497, Loss: 2.269140839576721, Final Batch Loss: 0.7061317563056946\n",
      "Epoch 498, Loss: 2.2995938062667847, Final Batch Loss: 0.7984402775764465\n",
      "Epoch 499, Loss: 2.3305631279945374, Final Batch Loss: 0.8118811845779419\n",
      "Epoch 500, Loss: 2.2971271872520447, Final Batch Loss: 0.7186829447746277\n",
      "Epoch 501, Loss: 2.1563138365745544, Final Batch Loss: 0.661420464515686\n",
      "Epoch 502, Loss: 2.3023507595062256, Final Batch Loss: 0.7805783748626709\n",
      "Epoch 503, Loss: 2.1591956615448, Final Batch Loss: 0.6981322169303894\n",
      "Epoch 504, Loss: 2.28274267911911, Final Batch Loss: 0.7058842778205872\n",
      "Epoch 505, Loss: 2.1819669008255005, Final Batch Loss: 0.7336770296096802\n",
      "Epoch 506, Loss: 2.3022850155830383, Final Batch Loss: 0.78709876537323\n",
      "Epoch 507, Loss: 2.3107678294181824, Final Batch Loss: 0.6754381060600281\n",
      "Epoch 508, Loss: 2.2527600526809692, Final Batch Loss: 0.6887283325195312\n",
      "Epoch 509, Loss: 2.2419278621673584, Final Batch Loss: 0.7909453511238098\n",
      "Epoch 510, Loss: 2.321287751197815, Final Batch Loss: 0.7781733870506287\n",
      "Epoch 511, Loss: 2.2264899611473083, Final Batch Loss: 0.804889976978302\n",
      "Epoch 512, Loss: 2.267823100090027, Final Batch Loss: 0.7808417081832886\n",
      "Epoch 513, Loss: 2.150355279445648, Final Batch Loss: 0.7047919034957886\n",
      "Epoch 514, Loss: 2.2871901988983154, Final Batch Loss: 0.7874032855033875\n",
      "Epoch 515, Loss: 2.2933648228645325, Final Batch Loss: 0.77007657289505\n",
      "Epoch 516, Loss: 2.200673818588257, Final Batch Loss: 0.7030146718025208\n",
      "Epoch 517, Loss: 2.2369982600212097, Final Batch Loss: 0.7056698799133301\n",
      "Epoch 518, Loss: 2.2378294467926025, Final Batch Loss: 0.696006715297699\n",
      "Epoch 519, Loss: 2.3871407508850098, Final Batch Loss: 0.9415488243103027\n",
      "Epoch 520, Loss: 2.3943360447883606, Final Batch Loss: 0.8132243156433105\n",
      "Epoch 521, Loss: 2.3900760412216187, Final Batch Loss: 0.9045556783676147\n",
      "Epoch 522, Loss: 2.1819764971733093, Final Batch Loss: 0.7108669281005859\n",
      "Epoch 523, Loss: 2.1784461736679077, Final Batch Loss: 0.6292233467102051\n",
      "Epoch 524, Loss: 2.270680606365204, Final Batch Loss: 0.7328363060951233\n",
      "Epoch 525, Loss: 2.1830220818519592, Final Batch Loss: 0.7317831516265869\n",
      "Epoch 526, Loss: 2.1697482466697693, Final Batch Loss: 0.6946731209754944\n",
      "Epoch 527, Loss: 2.27319198846817, Final Batch Loss: 0.7770879864692688\n",
      "Epoch 528, Loss: 2.271135091781616, Final Batch Loss: 0.7606207132339478\n",
      "Epoch 529, Loss: 2.2132109999656677, Final Batch Loss: 0.7359811067581177\n",
      "Epoch 530, Loss: 2.264010965824127, Final Batch Loss: 0.7722746133804321\n",
      "Epoch 531, Loss: 2.239928662776947, Final Batch Loss: 0.7647672295570374\n",
      "Epoch 532, Loss: 2.182156562805176, Final Batch Loss: 0.7194360494613647\n",
      "Epoch 533, Loss: 2.17177814245224, Final Batch Loss: 0.682351291179657\n",
      "Epoch 534, Loss: 2.2655832171440125, Final Batch Loss: 0.7559778094291687\n",
      "Epoch 535, Loss: 2.188439905643463, Final Batch Loss: 0.7082347869873047\n",
      "Epoch 536, Loss: 2.3032686710357666, Final Batch Loss: 0.83073890209198\n",
      "Epoch 537, Loss: 2.195378601551056, Final Batch Loss: 0.7746314406394958\n",
      "Epoch 538, Loss: 2.1089008450508118, Final Batch Loss: 0.6941013932228088\n",
      "Epoch 539, Loss: 2.272686004638672, Final Batch Loss: 0.7894246578216553\n",
      "Epoch 540, Loss: 2.229490876197815, Final Batch Loss: 0.6844807267189026\n",
      "Epoch 541, Loss: 2.2924286127090454, Final Batch Loss: 0.7967706322669983\n",
      "Epoch 542, Loss: 2.2093769311904907, Final Batch Loss: 0.6875265836715698\n",
      "Epoch 543, Loss: 2.1367392539978027, Final Batch Loss: 0.6676222681999207\n",
      "Epoch 544, Loss: 2.1172497868537903, Final Batch Loss: 0.7266872525215149\n",
      "Epoch 545, Loss: 2.1770195960998535, Final Batch Loss: 0.738884449005127\n",
      "Epoch 546, Loss: 2.21191668510437, Final Batch Loss: 0.7048182487487793\n",
      "Epoch 547, Loss: 2.1529579162597656, Final Batch Loss: 0.6963037848472595\n",
      "Epoch 548, Loss: 2.1217031478881836, Final Batch Loss: 0.6070143580436707\n",
      "Epoch 549, Loss: 2.2371389865875244, Final Batch Loss: 0.7554337978363037\n",
      "Epoch 550, Loss: 2.1331735849380493, Final Batch Loss: 0.6346803307533264\n",
      "Epoch 551, Loss: 2.3242105841636658, Final Batch Loss: 0.8784101009368896\n",
      "Epoch 552, Loss: 2.181453287601471, Final Batch Loss: 0.7649315595626831\n",
      "Epoch 553, Loss: 2.1309074759483337, Final Batch Loss: 0.6631404757499695\n",
      "Epoch 554, Loss: 2.156361162662506, Final Batch Loss: 0.6817853450775146\n",
      "Epoch 555, Loss: 2.1170151233673096, Final Batch Loss: 0.6890646815299988\n",
      "Epoch 556, Loss: 2.0746546387672424, Final Batch Loss: 0.725199282169342\n",
      "Epoch 557, Loss: 2.173569917678833, Final Batch Loss: 0.7245981097221375\n",
      "Epoch 558, Loss: 2.2756680250167847, Final Batch Loss: 0.7395091652870178\n",
      "Epoch 559, Loss: 2.283895254135132, Final Batch Loss: 0.7506250143051147\n",
      "Epoch 560, Loss: 2.3153852820396423, Final Batch Loss: 0.8010681867599487\n",
      "Epoch 561, Loss: 2.1522266268730164, Final Batch Loss: 0.7325696349143982\n",
      "Epoch 562, Loss: 2.0401864051818848, Final Batch Loss: 0.6775684356689453\n",
      "Epoch 563, Loss: 2.2808187007904053, Final Batch Loss: 0.8550354242324829\n",
      "Epoch 564, Loss: 2.249438524246216, Final Batch Loss: 0.7774413228034973\n",
      "Epoch 565, Loss: 2.1415460109710693, Final Batch Loss: 0.6499816179275513\n",
      "Epoch 566, Loss: 2.1561784148216248, Final Batch Loss: 0.6530039310455322\n",
      "Epoch 567, Loss: 2.135421395301819, Final Batch Loss: 0.7775011658668518\n",
      "Epoch 568, Loss: 2.026543617248535, Final Batch Loss: 0.6649444699287415\n",
      "Epoch 569, Loss: 2.1902260184288025, Final Batch Loss: 0.7111843228340149\n",
      "Epoch 570, Loss: 2.0215646028518677, Final Batch Loss: 0.6714591979980469\n",
      "Epoch 571, Loss: 2.0432143211364746, Final Batch Loss: 0.6507161259651184\n",
      "Epoch 572, Loss: 2.08138769865036, Final Batch Loss: 0.6430270671844482\n",
      "Epoch 573, Loss: 2.0771639347076416, Final Batch Loss: 0.6244081258773804\n",
      "Epoch 574, Loss: 2.078370451927185, Final Batch Loss: 0.6755185723304749\n",
      "Epoch 575, Loss: 2.2139992713928223, Final Batch Loss: 0.7366125583648682\n",
      "Epoch 576, Loss: 2.207807242870331, Final Batch Loss: 0.8152760863304138\n",
      "Epoch 577, Loss: 2.2060887813568115, Final Batch Loss: 0.7888497710227966\n",
      "Epoch 578, Loss: 2.1102415323257446, Final Batch Loss: 0.7359400391578674\n",
      "Epoch 579, Loss: 2.112067937850952, Final Batch Loss: 0.7653695940971375\n",
      "Epoch 580, Loss: 2.1161279678344727, Final Batch Loss: 0.6931608319282532\n",
      "Epoch 581, Loss: 2.1494744420051575, Final Batch Loss: 0.6945407390594482\n",
      "Epoch 582, Loss: 2.0585798621177673, Final Batch Loss: 0.7061485052108765\n",
      "Epoch 583, Loss: 2.121258854866028, Final Batch Loss: 0.7388948202133179\n",
      "Epoch 584, Loss: 2.1599348187446594, Final Batch Loss: 0.7478598952293396\n",
      "Epoch 585, Loss: 2.1135276556015015, Final Batch Loss: 0.6788933277130127\n",
      "Epoch 586, Loss: 2.120602548122406, Final Batch Loss: 0.6869465112686157\n",
      "Epoch 587, Loss: 2.0906007289886475, Final Batch Loss: 0.6756747961044312\n",
      "Epoch 588, Loss: 2.0987952947616577, Final Batch Loss: 0.6292989253997803\n",
      "Epoch 589, Loss: 2.2197336554527283, Final Batch Loss: 0.7383667826652527\n",
      "Epoch 590, Loss: 2.183182656764984, Final Batch Loss: 0.7823954820632935\n",
      "Epoch 591, Loss: 2.0499123334884644, Final Batch Loss: 0.6712409853935242\n",
      "Epoch 592, Loss: 2.125301420688629, Final Batch Loss: 0.7032648921012878\n",
      "Epoch 593, Loss: 2.082683265209198, Final Batch Loss: 0.6847479343414307\n",
      "Epoch 594, Loss: 2.156980335712433, Final Batch Loss: 0.7592249512672424\n",
      "Epoch 595, Loss: 1.9763439297676086, Final Batch Loss: 0.6335816383361816\n",
      "Epoch 596, Loss: 2.1177752017974854, Final Batch Loss: 0.7156279683113098\n",
      "Epoch 597, Loss: 2.0686076879501343, Final Batch Loss: 0.6311801671981812\n",
      "Epoch 598, Loss: 2.0710885524749756, Final Batch Loss: 0.6368538737297058\n",
      "Epoch 599, Loss: 2.096109092235565, Final Batch Loss: 0.6857408881187439\n",
      "Epoch 600, Loss: 2.1837680339813232, Final Batch Loss: 0.7289412021636963\n",
      "Epoch 601, Loss: 1.9572171568870544, Final Batch Loss: 0.5996349453926086\n",
      "Epoch 602, Loss: 2.125277280807495, Final Batch Loss: 0.7722910046577454\n",
      "Epoch 603, Loss: 2.132547438144684, Final Batch Loss: 0.7257785797119141\n",
      "Epoch 604, Loss: 1.9891984462738037, Final Batch Loss: 0.6344093680381775\n",
      "Epoch 605, Loss: 2.2389405369758606, Final Batch Loss: 0.8052805066108704\n",
      "Epoch 606, Loss: 2.047258675098419, Final Batch Loss: 0.7363128066062927\n",
      "Epoch 607, Loss: 2.012392222881317, Final Batch Loss: 0.6000781059265137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 608, Loss: 2.09894722700119, Final Batch Loss: 0.6841323971748352\n",
      "Epoch 609, Loss: 2.1867364048957825, Final Batch Loss: 0.7727410197257996\n",
      "Epoch 610, Loss: 2.10592257976532, Final Batch Loss: 0.751055896282196\n",
      "Epoch 611, Loss: 2.113816499710083, Final Batch Loss: 0.6476213932037354\n",
      "Epoch 612, Loss: 2.042837619781494, Final Batch Loss: 0.6395926475524902\n",
      "Epoch 613, Loss: 2.08310604095459, Final Batch Loss: 0.7190037369728088\n",
      "Epoch 614, Loss: 2.0335901975631714, Final Batch Loss: 0.5940495729446411\n",
      "Epoch 615, Loss: 2.0737550258636475, Final Batch Loss: 0.7554017305374146\n",
      "Epoch 616, Loss: 2.0514251589775085, Final Batch Loss: 0.6244710087776184\n",
      "Epoch 617, Loss: 2.055826485157013, Final Batch Loss: 0.679707944393158\n",
      "Epoch 618, Loss: 1.9247302412986755, Final Batch Loss: 0.5371683835983276\n",
      "Epoch 619, Loss: 2.1273829340934753, Final Batch Loss: 0.6813454627990723\n",
      "Epoch 620, Loss: 2.0325180292129517, Final Batch Loss: 0.6459403038024902\n",
      "Epoch 621, Loss: 2.148327946662903, Final Batch Loss: 0.7777827978134155\n",
      "Epoch 622, Loss: 2.2490299940109253, Final Batch Loss: 0.8086235523223877\n",
      "Epoch 623, Loss: 2.025217890739441, Final Batch Loss: 0.6559804081916809\n",
      "Epoch 624, Loss: 2.059129536151886, Final Batch Loss: 0.6556851863861084\n",
      "Epoch 625, Loss: 2.0695685148239136, Final Batch Loss: 0.7490240335464478\n",
      "Epoch 626, Loss: 1.945459246635437, Final Batch Loss: 0.6862408518791199\n",
      "Epoch 627, Loss: 2.0669673681259155, Final Batch Loss: 0.6825019717216492\n",
      "Epoch 628, Loss: 1.9610812067985535, Final Batch Loss: 0.6480816602706909\n",
      "Epoch 629, Loss: 2.123169720172882, Final Batch Loss: 0.7237300276756287\n",
      "Epoch 630, Loss: 2.13718980550766, Final Batch Loss: 0.7165473103523254\n",
      "Epoch 631, Loss: 2.091813385486603, Final Batch Loss: 0.6443886160850525\n",
      "Epoch 632, Loss: 2.1048200130462646, Final Batch Loss: 0.69916832447052\n",
      "Epoch 633, Loss: 1.981109380722046, Final Batch Loss: 0.5830116271972656\n",
      "Epoch 634, Loss: 1.97879958152771, Final Batch Loss: 0.633867621421814\n",
      "Epoch 635, Loss: 2.001977264881134, Final Batch Loss: 0.618788480758667\n",
      "Epoch 636, Loss: 2.0315827131271362, Final Batch Loss: 0.697909951210022\n",
      "Epoch 637, Loss: 2.068236827850342, Final Batch Loss: 0.6711496114730835\n",
      "Epoch 638, Loss: 2.032735288143158, Final Batch Loss: 0.6924196481704712\n",
      "Epoch 639, Loss: 1.9796603918075562, Final Batch Loss: 0.7013020515441895\n",
      "Epoch 640, Loss: 1.965210497379303, Final Batch Loss: 0.6723669171333313\n",
      "Epoch 641, Loss: 2.0815794467926025, Final Batch Loss: 0.7150880694389343\n",
      "Epoch 642, Loss: 2.0370376110076904, Final Batch Loss: 0.6831479668617249\n",
      "Epoch 643, Loss: 2.1238743662834167, Final Batch Loss: 0.7861459851264954\n",
      "Epoch 644, Loss: 2.027710974216461, Final Batch Loss: 0.6743085384368896\n",
      "Epoch 645, Loss: 2.0380041003227234, Final Batch Loss: 0.6609876751899719\n",
      "Epoch 646, Loss: 2.013043463230133, Final Batch Loss: 0.6519084572792053\n",
      "Epoch 647, Loss: 2.0424906611442566, Final Batch Loss: 0.7336230874061584\n",
      "Epoch 648, Loss: 2.0278804898262024, Final Batch Loss: 0.6516174077987671\n",
      "Epoch 649, Loss: 2.139509379863739, Final Batch Loss: 0.674522876739502\n",
      "Epoch 650, Loss: 2.097200632095337, Final Batch Loss: 0.7110761404037476\n",
      "Epoch 651, Loss: 2.0861803889274597, Final Batch Loss: 0.6643510460853577\n",
      "Epoch 652, Loss: 2.0172324776649475, Final Batch Loss: 0.657145082950592\n",
      "Epoch 653, Loss: 2.0904535055160522, Final Batch Loss: 0.8298882842063904\n",
      "Epoch 654, Loss: 2.067836582660675, Final Batch Loss: 0.7186596393585205\n",
      "Epoch 655, Loss: 2.026614189147949, Final Batch Loss: 0.7033551335334778\n",
      "Epoch 656, Loss: 2.060936689376831, Final Batch Loss: 0.7689014673233032\n",
      "Epoch 657, Loss: 1.9625864624977112, Final Batch Loss: 0.6387737989425659\n",
      "Epoch 658, Loss: 2.088142514228821, Final Batch Loss: 0.7564871311187744\n",
      "Epoch 659, Loss: 2.08767431974411, Final Batch Loss: 0.6513400077819824\n",
      "Epoch 660, Loss: 2.0282198190689087, Final Batch Loss: 0.6628341674804688\n",
      "Epoch 661, Loss: 1.976321280002594, Final Batch Loss: 0.6560993194580078\n",
      "Epoch 662, Loss: 2.0305935740470886, Final Batch Loss: 0.658226728439331\n",
      "Epoch 663, Loss: 2.007882595062256, Final Batch Loss: 0.6663393974304199\n",
      "Epoch 664, Loss: 1.942538321018219, Final Batch Loss: 0.6240206956863403\n",
      "Epoch 665, Loss: 2.0266624093055725, Final Batch Loss: 0.7205304503440857\n",
      "Epoch 666, Loss: 2.061192274093628, Final Batch Loss: 0.7245761156082153\n",
      "Epoch 667, Loss: 2.0548824667930603, Final Batch Loss: 0.6569470763206482\n",
      "Epoch 668, Loss: 1.9430572986602783, Final Batch Loss: 0.6883511543273926\n",
      "Epoch 669, Loss: 1.9333223104476929, Final Batch Loss: 0.6989254355430603\n",
      "Epoch 670, Loss: 2.062113881111145, Final Batch Loss: 0.6533137559890747\n",
      "Epoch 671, Loss: 2.0692288875579834, Final Batch Loss: 0.6954905986785889\n",
      "Epoch 672, Loss: 2.0498147010803223, Final Batch Loss: 0.7779961228370667\n",
      "Epoch 673, Loss: 1.99912691116333, Final Batch Loss: 0.6691378355026245\n",
      "Epoch 674, Loss: 2.081364929676056, Final Batch Loss: 0.7700045108795166\n",
      "Epoch 675, Loss: 1.973019003868103, Final Batch Loss: 0.6290510296821594\n",
      "Epoch 676, Loss: 2.0741093158721924, Final Batch Loss: 0.7081361413002014\n",
      "Epoch 677, Loss: 1.9765207767486572, Final Batch Loss: 0.7497740983963013\n",
      "Epoch 678, Loss: 2.040514290332794, Final Batch Loss: 0.6768486499786377\n",
      "Epoch 679, Loss: 1.9987510442733765, Final Batch Loss: 0.6582725048065186\n",
      "Epoch 680, Loss: 1.9465286135673523, Final Batch Loss: 0.5815573334693909\n",
      "Epoch 681, Loss: 2.116578698158264, Final Batch Loss: 0.75538170337677\n",
      "Epoch 682, Loss: 1.9192867875099182, Final Batch Loss: 0.6759176850318909\n",
      "Epoch 683, Loss: 1.9157013297080994, Final Batch Loss: 0.6152379512786865\n",
      "Epoch 684, Loss: 2.080250859260559, Final Batch Loss: 0.7520219087600708\n",
      "Epoch 685, Loss: 1.9172013401985168, Final Batch Loss: 0.6533911228179932\n",
      "Epoch 686, Loss: 1.8077606558799744, Final Batch Loss: 0.5941355228424072\n",
      "Epoch 687, Loss: 1.911768913269043, Final Batch Loss: 0.5861704349517822\n",
      "Epoch 688, Loss: 1.9348546266555786, Final Batch Loss: 0.693969190120697\n",
      "Epoch 689, Loss: 2.0046080350875854, Final Batch Loss: 0.6285777688026428\n",
      "Epoch 690, Loss: 2.000054955482483, Final Batch Loss: 0.7187085747718811\n",
      "Epoch 691, Loss: 1.9473329782485962, Final Batch Loss: 0.6555033326148987\n",
      "Epoch 692, Loss: 2.0431387424468994, Final Batch Loss: 0.6767664551734924\n",
      "Epoch 693, Loss: 2.0734432339668274, Final Batch Loss: 0.6699607372283936\n",
      "Epoch 694, Loss: 1.97699373960495, Final Batch Loss: 0.6693643927574158\n",
      "Epoch 695, Loss: 1.985059678554535, Final Batch Loss: 0.6533175110816956\n",
      "Epoch 696, Loss: 2.0298011898994446, Final Batch Loss: 0.7157430052757263\n",
      "Epoch 697, Loss: 2.0405685901641846, Final Batch Loss: 0.8072621822357178\n",
      "Epoch 698, Loss: 1.967281460762024, Final Batch Loss: 0.6330179572105408\n",
      "Epoch 699, Loss: 2.00046443939209, Final Batch Loss: 0.6104742288589478\n",
      "Epoch 700, Loss: 2.093208432197571, Final Batch Loss: 0.6988100409507751\n",
      "Epoch 701, Loss: 2.0193338990211487, Final Batch Loss: 0.707893431186676\n",
      "Epoch 702, Loss: 2.052527368068695, Final Batch Loss: 0.6777664422988892\n",
      "Epoch 703, Loss: 1.949592113494873, Final Batch Loss: 0.6530189514160156\n",
      "Epoch 704, Loss: 1.8910192251205444, Final Batch Loss: 0.5745649933815002\n",
      "Epoch 705, Loss: 2.0147727727890015, Final Batch Loss: 0.7377434372901917\n",
      "Epoch 706, Loss: 1.9629068970680237, Final Batch Loss: 0.6783156991004944\n",
      "Epoch 707, Loss: 1.967035174369812, Final Batch Loss: 0.6207907199859619\n",
      "Epoch 708, Loss: 1.9076842665672302, Final Batch Loss: 0.6513016819953918\n",
      "Epoch 709, Loss: 2.0071168541908264, Final Batch Loss: 0.6593384146690369\n",
      "Epoch 710, Loss: 2.0301873683929443, Final Batch Loss: 0.6918050646781921\n",
      "Epoch 711, Loss: 1.9285199642181396, Final Batch Loss: 0.6622315645217896\n",
      "Epoch 712, Loss: 2.014652907848358, Final Batch Loss: 0.6459375023841858\n",
      "Epoch 713, Loss: 2.0461748242378235, Final Batch Loss: 0.6645141243934631\n",
      "Epoch 714, Loss: 1.8710942268371582, Final Batch Loss: 0.5852367877960205\n",
      "Epoch 715, Loss: 2.0155643224716187, Final Batch Loss: 0.6980635523796082\n",
      "Epoch 716, Loss: 2.004677891731262, Final Batch Loss: 0.6973408460617065\n",
      "Epoch 717, Loss: 2.0614060759544373, Final Batch Loss: 0.7282990217208862\n",
      "Epoch 718, Loss: 1.9986122846603394, Final Batch Loss: 0.6687851548194885\n",
      "Epoch 719, Loss: 1.9956018924713135, Final Batch Loss: 0.6637268662452698\n",
      "Epoch 720, Loss: 1.9307428002357483, Final Batch Loss: 0.634696364402771\n",
      "Epoch 721, Loss: 1.9752663969993591, Final Batch Loss: 0.6516972184181213\n",
      "Epoch 722, Loss: 1.9638746976852417, Final Batch Loss: 0.6603043675422668\n",
      "Epoch 723, Loss: 1.853431522846222, Final Batch Loss: 0.5713831782341003\n",
      "Epoch 724, Loss: 1.816671907901764, Final Batch Loss: 0.5085421204566956\n",
      "Epoch 725, Loss: 1.9633760452270508, Final Batch Loss: 0.6476219892501831\n",
      "Epoch 726, Loss: 1.994600236415863, Final Batch Loss: 0.6655606031417847\n",
      "Epoch 727, Loss: 1.953437864780426, Final Batch Loss: 0.6163074970245361\n",
      "Epoch 728, Loss: 1.8740618824958801, Final Batch Loss: 0.6584650278091431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 729, Loss: 1.8906007409095764, Final Batch Loss: 0.6136683821678162\n",
      "Epoch 730, Loss: 1.9112926125526428, Final Batch Loss: 0.6633923053741455\n",
      "Epoch 731, Loss: 1.9029788374900818, Final Batch Loss: 0.6444600224494934\n",
      "Epoch 732, Loss: 1.907447338104248, Final Batch Loss: 0.6371365189552307\n",
      "Epoch 733, Loss: 1.941441833972931, Final Batch Loss: 0.5901906490325928\n",
      "Epoch 734, Loss: 1.9483891725540161, Final Batch Loss: 0.6563403606414795\n",
      "Epoch 735, Loss: 1.898208498954773, Final Batch Loss: 0.7513264417648315\n",
      "Epoch 736, Loss: 1.8975314497947693, Final Batch Loss: 0.6585476398468018\n",
      "Epoch 737, Loss: 2.0489194989204407, Final Batch Loss: 0.7080023288726807\n",
      "Epoch 738, Loss: 1.9362836480140686, Final Batch Loss: 0.5786939263343811\n",
      "Epoch 739, Loss: 1.9292582869529724, Final Batch Loss: 0.6283131241798401\n",
      "Epoch 740, Loss: 1.888034701347351, Final Batch Loss: 0.6300073862075806\n",
      "Epoch 741, Loss: 1.9472249150276184, Final Batch Loss: 0.6165199279785156\n",
      "Epoch 742, Loss: 1.823129415512085, Final Batch Loss: 0.5146294236183167\n",
      "Epoch 743, Loss: 1.8500484228134155, Final Batch Loss: 0.6302090287208557\n",
      "Epoch 744, Loss: 1.978901982307434, Final Batch Loss: 0.7899565100669861\n",
      "Epoch 745, Loss: 1.8772776126861572, Final Batch Loss: 0.5735476613044739\n",
      "Epoch 746, Loss: 1.9274967312812805, Final Batch Loss: 0.6429925560951233\n",
      "Epoch 747, Loss: 1.8535398244857788, Final Batch Loss: 0.6251027584075928\n",
      "Epoch 748, Loss: 1.9676694869995117, Final Batch Loss: 0.6113364100456238\n",
      "Epoch 749, Loss: 1.9659291505813599, Final Batch Loss: 0.6718575358390808\n",
      "Epoch 750, Loss: 1.827893316745758, Final Batch Loss: 0.6633487343788147\n",
      "Epoch 751, Loss: 1.9735373258590698, Final Batch Loss: 0.6543806195259094\n",
      "Epoch 752, Loss: 1.8114115595817566, Final Batch Loss: 0.5698190331459045\n",
      "Epoch 753, Loss: 1.9347442388534546, Final Batch Loss: 0.6785605549812317\n",
      "Epoch 754, Loss: 1.8569807410240173, Final Batch Loss: 0.5439565181732178\n",
      "Epoch 755, Loss: 1.8864943981170654, Final Batch Loss: 0.5928947925567627\n",
      "Epoch 756, Loss: 1.92130047082901, Final Batch Loss: 0.6700469851493835\n",
      "Epoch 757, Loss: 1.8490085005760193, Final Batch Loss: 0.5997418761253357\n",
      "Epoch 758, Loss: 1.8535021543502808, Final Batch Loss: 0.5625227689743042\n",
      "Epoch 759, Loss: 1.9433790445327759, Final Batch Loss: 0.6518653035163879\n",
      "Epoch 760, Loss: 1.8599403500556946, Final Batch Loss: 0.57786625623703\n",
      "Epoch 761, Loss: 1.8415433764457703, Final Batch Loss: 0.5302463173866272\n",
      "Epoch 762, Loss: 1.8362499475479126, Final Batch Loss: 0.5116473436355591\n",
      "Epoch 763, Loss: 1.7783949971199036, Final Batch Loss: 0.5366538166999817\n",
      "Epoch 764, Loss: 1.9620732069015503, Final Batch Loss: 0.6974506974220276\n",
      "Epoch 765, Loss: 1.8502548336982727, Final Batch Loss: 0.5948994159698486\n",
      "Epoch 766, Loss: 1.8660097122192383, Final Batch Loss: 0.6452820897102356\n",
      "Epoch 767, Loss: 1.8018428683280945, Final Batch Loss: 0.5146534442901611\n",
      "Epoch 768, Loss: 1.8810506463050842, Final Batch Loss: 0.5494453310966492\n",
      "Epoch 769, Loss: 1.9280003905296326, Final Batch Loss: 0.6642498970031738\n",
      "Epoch 770, Loss: 1.8573434948921204, Final Batch Loss: 0.6070243716239929\n",
      "Epoch 771, Loss: 1.7929701805114746, Final Batch Loss: 0.584419310092926\n",
      "Epoch 772, Loss: 1.879702091217041, Final Batch Loss: 0.6181274652481079\n",
      "Epoch 773, Loss: 1.899971842765808, Final Batch Loss: 0.59795081615448\n",
      "Epoch 774, Loss: 1.9613043069839478, Final Batch Loss: 0.6854714155197144\n",
      "Epoch 775, Loss: 1.8803918361663818, Final Batch Loss: 0.6848413944244385\n",
      "Epoch 776, Loss: 1.8550288677215576, Final Batch Loss: 0.6327957510948181\n",
      "Epoch 777, Loss: 1.9146283864974976, Final Batch Loss: 0.6691979765892029\n",
      "Epoch 778, Loss: 1.9339362978935242, Final Batch Loss: 0.649652898311615\n",
      "Epoch 779, Loss: 1.8323537111282349, Final Batch Loss: 0.6524749994277954\n",
      "Epoch 780, Loss: 1.9331007599830627, Final Batch Loss: 0.6449151635169983\n",
      "Epoch 781, Loss: 1.8941125273704529, Final Batch Loss: 0.7018539905548096\n",
      "Epoch 782, Loss: 1.8026278018951416, Final Batch Loss: 0.6094055771827698\n",
      "Epoch 783, Loss: 1.9533621072769165, Final Batch Loss: 0.6540199518203735\n",
      "Epoch 784, Loss: 1.9464685320854187, Final Batch Loss: 0.7593850493431091\n",
      "Epoch 785, Loss: 1.9311355948448181, Final Batch Loss: 0.5934239625930786\n",
      "Epoch 786, Loss: 1.7887398600578308, Final Batch Loss: 0.5460898876190186\n",
      "Epoch 787, Loss: 1.880651831626892, Final Batch Loss: 0.6922624707221985\n",
      "Epoch 788, Loss: 1.755004107952118, Final Batch Loss: 0.546023964881897\n",
      "Epoch 789, Loss: 2.0114834904670715, Final Batch Loss: 0.8679518103599548\n",
      "Epoch 790, Loss: 1.8561980724334717, Final Batch Loss: 0.6323375105857849\n",
      "Epoch 791, Loss: 1.8870291709899902, Final Batch Loss: 0.6644736528396606\n",
      "Epoch 792, Loss: 1.9372833967208862, Final Batch Loss: 0.677876889705658\n",
      "Epoch 793, Loss: 1.7864384651184082, Final Batch Loss: 0.5275958776473999\n",
      "Epoch 794, Loss: 1.759988784790039, Final Batch Loss: 0.5145724415779114\n",
      "Epoch 795, Loss: 1.7823827266693115, Final Batch Loss: 0.5440739989280701\n",
      "Epoch 796, Loss: 1.8829401731491089, Final Batch Loss: 0.7135025262832642\n",
      "Epoch 797, Loss: 1.8307019472122192, Final Batch Loss: 0.6437707543373108\n",
      "Epoch 798, Loss: 1.8498573899269104, Final Batch Loss: 0.6385804414749146\n",
      "Epoch 799, Loss: 1.8216286897659302, Final Batch Loss: 0.6231607794761658\n",
      "Epoch 800, Loss: 1.8110982179641724, Final Batch Loss: 0.5715624094009399\n",
      "Epoch 801, Loss: 1.8060877323150635, Final Batch Loss: 0.6079795956611633\n",
      "Epoch 802, Loss: 1.8212859630584717, Final Batch Loss: 0.5351661443710327\n",
      "Epoch 803, Loss: 1.7777743935585022, Final Batch Loss: 0.6169213056564331\n",
      "Epoch 804, Loss: 1.7801769375801086, Final Batch Loss: 0.5110843777656555\n",
      "Epoch 805, Loss: 1.7734951376914978, Final Batch Loss: 0.6053680777549744\n",
      "Epoch 806, Loss: 1.829131841659546, Final Batch Loss: 0.5646913647651672\n",
      "Epoch 807, Loss: 1.8379076719284058, Final Batch Loss: 0.5691385865211487\n",
      "Epoch 808, Loss: 1.9515402913093567, Final Batch Loss: 0.7122159600257874\n",
      "Epoch 809, Loss: 1.794985592365265, Final Batch Loss: 0.5192463994026184\n",
      "Epoch 810, Loss: 1.8634981513023376, Final Batch Loss: 0.6446475386619568\n",
      "Epoch 811, Loss: 1.832029640674591, Final Batch Loss: 0.616524875164032\n",
      "Epoch 812, Loss: 1.753580391407013, Final Batch Loss: 0.5589422583580017\n",
      "Epoch 813, Loss: 1.708343267440796, Final Batch Loss: 0.5779221057891846\n",
      "Epoch 814, Loss: 1.802363932132721, Final Batch Loss: 0.6228668093681335\n",
      "Epoch 815, Loss: 1.8882943987846375, Final Batch Loss: 0.6901863217353821\n",
      "Epoch 816, Loss: 1.8300001621246338, Final Batch Loss: 0.569658637046814\n",
      "Epoch 817, Loss: 1.89572674036026, Final Batch Loss: 0.737533688545227\n",
      "Epoch 818, Loss: 1.8209658861160278, Final Batch Loss: 0.6490631103515625\n",
      "Epoch 819, Loss: 1.8346936702728271, Final Batch Loss: 0.6593853831291199\n",
      "Epoch 820, Loss: 1.6876583099365234, Final Batch Loss: 0.4874371886253357\n",
      "Epoch 821, Loss: 1.801028549671173, Final Batch Loss: 0.6836248636245728\n",
      "Epoch 822, Loss: 1.8625085949897766, Final Batch Loss: 0.7095337510108948\n",
      "Epoch 823, Loss: 1.8233994245529175, Final Batch Loss: 0.6736982464790344\n",
      "Epoch 824, Loss: 1.7482777833938599, Final Batch Loss: 0.538137674331665\n",
      "Epoch 825, Loss: 1.9638082385063171, Final Batch Loss: 0.6274009346961975\n",
      "Epoch 826, Loss: 1.8028093576431274, Final Batch Loss: 0.625465989112854\n",
      "Epoch 827, Loss: 1.8326984643936157, Final Batch Loss: 0.5568908452987671\n",
      "Epoch 828, Loss: 1.8055349588394165, Final Batch Loss: 0.6082924604415894\n",
      "Epoch 829, Loss: 1.8332374095916748, Final Batch Loss: 0.5873009562492371\n",
      "Epoch 830, Loss: 1.8728647232055664, Final Batch Loss: 0.6551679968833923\n",
      "Epoch 831, Loss: 1.9512845873832703, Final Batch Loss: 0.7245884537696838\n",
      "Epoch 832, Loss: 1.7660287022590637, Final Batch Loss: 0.500440776348114\n",
      "Epoch 833, Loss: 1.782161295413971, Final Batch Loss: 0.6318513751029968\n",
      "Epoch 834, Loss: 1.739272117614746, Final Batch Loss: 0.5431851744651794\n",
      "Epoch 835, Loss: 1.8480249643325806, Final Batch Loss: 0.6613651514053345\n",
      "Epoch 836, Loss: 1.8065557479858398, Final Batch Loss: 0.6492961049079895\n",
      "Epoch 837, Loss: 1.808126449584961, Final Batch Loss: 0.6056176424026489\n",
      "Epoch 838, Loss: 1.768290936946869, Final Batch Loss: 0.5057961344718933\n",
      "Epoch 839, Loss: 1.9229968786239624, Final Batch Loss: 0.6878229975700378\n",
      "Epoch 840, Loss: 1.7880300283432007, Final Batch Loss: 0.6339715719223022\n",
      "Epoch 841, Loss: 1.7111083269119263, Final Batch Loss: 0.5746464133262634\n",
      "Epoch 842, Loss: 1.801147222518921, Final Batch Loss: 0.5906955003738403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 843, Loss: 1.8111939430236816, Final Batch Loss: 0.6002733111381531\n",
      "Epoch 844, Loss: 1.6991704106330872, Final Batch Loss: 0.5343387722969055\n",
      "Epoch 845, Loss: 1.8332683444023132, Final Batch Loss: 0.6321994662284851\n",
      "Epoch 846, Loss: 1.7518041133880615, Final Batch Loss: 0.5385273694992065\n",
      "Epoch 847, Loss: 1.8006360530853271, Final Batch Loss: 0.627483606338501\n",
      "Epoch 848, Loss: 1.6818984746932983, Final Batch Loss: 0.5197537541389465\n",
      "Epoch 849, Loss: 1.70390385389328, Final Batch Loss: 0.5230098962783813\n",
      "Epoch 850, Loss: 1.7567777633666992, Final Batch Loss: 0.5325821042060852\n",
      "Epoch 851, Loss: 1.7212566137313843, Final Batch Loss: 0.5658565759658813\n",
      "Epoch 852, Loss: 1.7224323153495789, Final Batch Loss: 0.562100887298584\n",
      "Epoch 853, Loss: 1.6580179035663605, Final Batch Loss: 0.48500898480415344\n",
      "Epoch 854, Loss: 1.8439747095108032, Final Batch Loss: 0.6258485913276672\n",
      "Epoch 855, Loss: 1.7586988806724548, Final Batch Loss: 0.5644014477729797\n",
      "Epoch 856, Loss: 1.8089779019355774, Final Batch Loss: 0.6424217224121094\n",
      "Epoch 857, Loss: 1.7545905113220215, Final Batch Loss: 0.6509456634521484\n",
      "Epoch 858, Loss: 1.8129733204841614, Final Batch Loss: 0.7077910900115967\n",
      "Epoch 859, Loss: 1.7498229146003723, Final Batch Loss: 0.6428178548812866\n",
      "Epoch 860, Loss: 1.8159050345420837, Final Batch Loss: 0.6365026235580444\n",
      "Epoch 861, Loss: 1.8022602200508118, Final Batch Loss: 0.655647337436676\n",
      "Epoch 862, Loss: 1.7091301679611206, Final Batch Loss: 0.6031222343444824\n",
      "Epoch 863, Loss: 1.7435657978057861, Final Batch Loss: 0.5665633082389832\n",
      "Epoch 864, Loss: 1.7459883093833923, Final Batch Loss: 0.567680835723877\n",
      "Epoch 865, Loss: 1.8383753299713135, Final Batch Loss: 0.640675961971283\n",
      "Epoch 866, Loss: 1.7596659660339355, Final Batch Loss: 0.5882731676101685\n",
      "Epoch 867, Loss: 1.7419596314430237, Final Batch Loss: 0.5415095090866089\n",
      "Epoch 868, Loss: 1.8055692315101624, Final Batch Loss: 0.6057639718055725\n",
      "Epoch 869, Loss: 1.759043574333191, Final Batch Loss: 0.6234875321388245\n",
      "Epoch 870, Loss: 1.72405606508255, Final Batch Loss: 0.5769027471542358\n",
      "Epoch 871, Loss: 1.7025394439697266, Final Batch Loss: 0.6334795951843262\n",
      "Epoch 872, Loss: 1.655163437128067, Final Batch Loss: 0.4770403802394867\n",
      "Epoch 873, Loss: 1.7080451250076294, Final Batch Loss: 0.5726067423820496\n",
      "Epoch 874, Loss: 1.7416398525238037, Final Batch Loss: 0.5668513178825378\n",
      "Epoch 875, Loss: 1.7051299810409546, Final Batch Loss: 0.5309404730796814\n",
      "Epoch 876, Loss: 1.68594428896904, Final Batch Loss: 0.4793904721736908\n",
      "Epoch 877, Loss: 1.748438835144043, Final Batch Loss: 0.5302206873893738\n",
      "Epoch 878, Loss: 1.8100716471672058, Final Batch Loss: 0.5367562770843506\n",
      "Epoch 879, Loss: 1.7270273566246033, Final Batch Loss: 0.5662446022033691\n",
      "Epoch 880, Loss: 1.7118321657180786, Final Batch Loss: 0.6090730428695679\n",
      "Epoch 881, Loss: 1.9020264148712158, Final Batch Loss: 0.667433500289917\n",
      "Epoch 882, Loss: 1.7578343152999878, Final Batch Loss: 0.660814642906189\n",
      "Epoch 883, Loss: 1.7231049537658691, Final Batch Loss: 0.564085841178894\n",
      "Epoch 884, Loss: 1.605381816625595, Final Batch Loss: 0.4885950982570648\n",
      "Epoch 885, Loss: 1.6642469465732574, Final Batch Loss: 0.44360294938087463\n",
      "Epoch 886, Loss: 1.7814016938209534, Final Batch Loss: 0.6009151339530945\n",
      "Epoch 887, Loss: 1.6928034126758575, Final Batch Loss: 0.49899712204933167\n",
      "Epoch 888, Loss: 1.7094855904579163, Final Batch Loss: 0.59140545129776\n",
      "Epoch 889, Loss: 1.7045291662216187, Final Batch Loss: 0.5641190409660339\n",
      "Epoch 890, Loss: 1.7322636842727661, Final Batch Loss: 0.6228025555610657\n",
      "Epoch 891, Loss: 1.7433906197547913, Final Batch Loss: 0.6087614297866821\n",
      "Epoch 892, Loss: 1.5740636587142944, Final Batch Loss: 0.4598730206489563\n",
      "Epoch 893, Loss: 1.6993094086647034, Final Batch Loss: 0.5536080598831177\n",
      "Epoch 894, Loss: 1.7059689164161682, Final Batch Loss: 0.6040623784065247\n",
      "Epoch 895, Loss: 1.9162383079528809, Final Batch Loss: 0.7664428949356079\n",
      "Epoch 896, Loss: 1.6145769357681274, Final Batch Loss: 0.47290271520614624\n",
      "Epoch 897, Loss: 1.6549113392829895, Final Batch Loss: 0.5124914050102234\n",
      "Epoch 898, Loss: 1.6520752310752869, Final Batch Loss: 0.5306177735328674\n",
      "Epoch 899, Loss: 1.7536307573318481, Final Batch Loss: 0.5902760624885559\n",
      "Epoch 900, Loss: 1.7378334403038025, Final Batch Loss: 0.6967395544052124\n",
      "Epoch 901, Loss: 1.7156786918640137, Final Batch Loss: 0.5602158904075623\n",
      "Epoch 902, Loss: 1.6908081769943237, Final Batch Loss: 0.5627874135971069\n",
      "Epoch 903, Loss: 1.7815311551094055, Final Batch Loss: 0.5935239195823669\n",
      "Epoch 904, Loss: 1.7742655873298645, Final Batch Loss: 0.5677801966667175\n",
      "Epoch 905, Loss: 1.7703845500946045, Final Batch Loss: 0.6530023813247681\n",
      "Epoch 906, Loss: 1.666867196559906, Final Batch Loss: 0.5353790521621704\n",
      "Epoch 907, Loss: 1.7459003925323486, Final Batch Loss: 0.6201823353767395\n",
      "Epoch 908, Loss: 1.749527096748352, Final Batch Loss: 0.6367067098617554\n",
      "Epoch 909, Loss: 1.7097668051719666, Final Batch Loss: 0.6276834011077881\n",
      "Epoch 910, Loss: 1.7541831135749817, Final Batch Loss: 0.6021732687950134\n",
      "Epoch 911, Loss: 1.662796974182129, Final Batch Loss: 0.6023368835449219\n",
      "Epoch 912, Loss: 1.6209630370140076, Final Batch Loss: 0.5310978889465332\n",
      "Epoch 913, Loss: 1.7933359146118164, Final Batch Loss: 0.59644615650177\n",
      "Epoch 914, Loss: 1.767824113368988, Final Batch Loss: 0.5807487368583679\n",
      "Epoch 915, Loss: 1.7834769487380981, Final Batch Loss: 0.7155317664146423\n",
      "Epoch 916, Loss: 1.6870667934417725, Final Batch Loss: 0.6145716309547424\n",
      "Epoch 917, Loss: 1.7398398518562317, Final Batch Loss: 0.595790684223175\n",
      "Epoch 918, Loss: 1.6564950346946716, Final Batch Loss: 0.5144491791725159\n",
      "Epoch 919, Loss: 1.7954458594322205, Final Batch Loss: 0.6170002818107605\n",
      "Epoch 920, Loss: 1.6094852685928345, Final Batch Loss: 0.5408352613449097\n",
      "Epoch 921, Loss: 1.6326435208320618, Final Batch Loss: 0.5332614183425903\n",
      "Epoch 922, Loss: 1.6112790703773499, Final Batch Loss: 0.4596177935600281\n",
      "Epoch 923, Loss: 1.7447674870491028, Final Batch Loss: 0.6002030968666077\n",
      "Epoch 924, Loss: 1.6624997854232788, Final Batch Loss: 0.5109930634498596\n",
      "Epoch 925, Loss: 1.7206306457519531, Final Batch Loss: 0.5734649300575256\n",
      "Epoch 926, Loss: 1.8791643977165222, Final Batch Loss: 0.6955038905143738\n",
      "Epoch 927, Loss: 1.6905392408370972, Final Batch Loss: 0.5942809581756592\n",
      "Epoch 928, Loss: 1.6803049445152283, Final Batch Loss: 0.6507951617240906\n",
      "Epoch 929, Loss: 1.6431927382946014, Final Batch Loss: 0.5947954058647156\n",
      "Epoch 930, Loss: 1.75438791513443, Final Batch Loss: 0.6368119120597839\n",
      "Epoch 931, Loss: 1.6906716227531433, Final Batch Loss: 0.5095765590667725\n",
      "Epoch 932, Loss: 1.7496719360351562, Final Batch Loss: 0.5681565999984741\n",
      "Epoch 933, Loss: 1.748660922050476, Final Batch Loss: 0.5995259881019592\n",
      "Epoch 934, Loss: 1.8117230534553528, Final Batch Loss: 0.6323513388633728\n",
      "Epoch 935, Loss: 1.5888085961341858, Final Batch Loss: 0.5450076460838318\n",
      "Epoch 936, Loss: 1.67474365234375, Final Batch Loss: 0.5839897394180298\n",
      "Epoch 937, Loss: 1.5142311751842499, Final Batch Loss: 0.4507978856563568\n",
      "Epoch 938, Loss: 1.7162511348724365, Final Batch Loss: 0.6425391435623169\n",
      "Epoch 939, Loss: 1.7130374908447266, Final Batch Loss: 0.5536894202232361\n",
      "Epoch 940, Loss: 1.76475590467453, Final Batch Loss: 0.6377419233322144\n",
      "Epoch 941, Loss: 1.543113261461258, Final Batch Loss: 0.4919002950191498\n",
      "Epoch 942, Loss: 1.6876033246517181, Final Batch Loss: 0.6766383647918701\n",
      "Epoch 943, Loss: 1.692092627286911, Final Batch Loss: 0.5987657308578491\n",
      "Epoch 944, Loss: 1.5971346497535706, Final Batch Loss: 0.566270112991333\n",
      "Epoch 945, Loss: 1.6374942660331726, Final Batch Loss: 0.5816697478294373\n",
      "Epoch 946, Loss: 1.6905677318572998, Final Batch Loss: 0.588791012763977\n",
      "Epoch 947, Loss: 1.7607249021530151, Final Batch Loss: 0.5415326952934265\n",
      "Epoch 948, Loss: 1.5614137053489685, Final Batch Loss: 0.45158886909484863\n",
      "Epoch 949, Loss: 1.6353172659873962, Final Batch Loss: 0.5082122087478638\n",
      "Epoch 950, Loss: 1.6179595291614532, Final Batch Loss: 0.5704602599143982\n",
      "Epoch 951, Loss: 1.6035292148590088, Final Batch Loss: 0.5259092450141907\n",
      "Epoch 952, Loss: 1.5052401721477509, Final Batch Loss: 0.4093897044658661\n",
      "Epoch 953, Loss: 1.5621099472045898, Final Batch Loss: 0.5058144927024841\n",
      "Epoch 954, Loss: 1.6104089617729187, Final Batch Loss: 0.49350637197494507\n",
      "Epoch 955, Loss: 1.6385070085525513, Final Batch Loss: 0.5423298478126526\n",
      "Epoch 956, Loss: 1.5946056842803955, Final Batch Loss: 0.5403549075126648\n",
      "Epoch 957, Loss: 1.6336697041988373, Final Batch Loss: 0.45333752036094666\n",
      "Epoch 958, Loss: 1.5820467174053192, Final Batch Loss: 0.5657187700271606\n",
      "Epoch 959, Loss: 1.7212826609611511, Final Batch Loss: 0.5984811186790466\n",
      "Epoch 960, Loss: 1.590671420097351, Final Batch Loss: 0.4733123779296875\n",
      "Epoch 961, Loss: 1.695544183254242, Final Batch Loss: 0.550929844379425\n",
      "Epoch 962, Loss: 1.627707600593567, Final Batch Loss: 0.6252102851867676\n",
      "Epoch 963, Loss: 1.7503158450126648, Final Batch Loss: 0.5888725519180298\n",
      "Epoch 964, Loss: 1.6100835800170898, Final Batch Loss: 0.5360772013664246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 965, Loss: 1.8018541932106018, Final Batch Loss: 0.7030110955238342\n",
      "Epoch 966, Loss: 1.6584298312664032, Final Batch Loss: 0.6172875761985779\n",
      "Epoch 967, Loss: 1.6182147562503815, Final Batch Loss: 0.5580297112464905\n",
      "Epoch 968, Loss: 1.52303546667099, Final Batch Loss: 0.4694305658340454\n",
      "Epoch 969, Loss: 1.64144629240036, Final Batch Loss: 0.590217113494873\n",
      "Epoch 970, Loss: 1.6208442449569702, Final Batch Loss: 0.5290889143943787\n",
      "Epoch 971, Loss: 1.7025186419487, Final Batch Loss: 0.6120964884757996\n",
      "Epoch 972, Loss: 1.4985686242580414, Final Batch Loss: 0.4977165162563324\n",
      "Epoch 973, Loss: 1.6008245944976807, Final Batch Loss: 0.5856741070747375\n",
      "Epoch 974, Loss: 1.5651436746120453, Final Batch Loss: 0.5290206074714661\n",
      "Epoch 975, Loss: 1.6087527871131897, Final Batch Loss: 0.4856107234954834\n",
      "Epoch 976, Loss: 1.634423553943634, Final Batch Loss: 0.564084529876709\n",
      "Epoch 977, Loss: 1.4670952558517456, Final Batch Loss: 0.3940187692642212\n",
      "Epoch 978, Loss: 1.7211852073669434, Final Batch Loss: 0.49038589000701904\n",
      "Epoch 979, Loss: 1.500238299369812, Final Batch Loss: 0.42571353912353516\n",
      "Epoch 980, Loss: 1.524096816778183, Final Batch Loss: 0.4267692267894745\n",
      "Epoch 981, Loss: 1.62859308719635, Final Batch Loss: 0.5060641169548035\n",
      "Epoch 982, Loss: 1.5978449583053589, Final Batch Loss: 0.49850648641586304\n",
      "Epoch 983, Loss: 1.6631543338298798, Final Batch Loss: 0.6209471225738525\n",
      "Epoch 984, Loss: 1.6223043203353882, Final Batch Loss: 0.5057206749916077\n",
      "Epoch 985, Loss: 1.6674283742904663, Final Batch Loss: 0.5935449004173279\n",
      "Epoch 986, Loss: 1.6050390005111694, Final Batch Loss: 0.5960281491279602\n",
      "Epoch 987, Loss: 1.4787618517875671, Final Batch Loss: 0.4097434878349304\n",
      "Epoch 988, Loss: 1.6664567589759827, Final Batch Loss: 0.5591036677360535\n",
      "Epoch 989, Loss: 1.6899465322494507, Final Batch Loss: 0.6175279021263123\n",
      "Epoch 990, Loss: 1.641166239976883, Final Batch Loss: 0.4970618188381195\n",
      "Epoch 991, Loss: 1.6422749161720276, Final Batch Loss: 0.5759449601173401\n",
      "Epoch 992, Loss: 1.5428265929222107, Final Batch Loss: 0.5233353972434998\n",
      "Epoch 993, Loss: 1.5405715703964233, Final Batch Loss: 0.5364437699317932\n",
      "Epoch 994, Loss: 1.651925504207611, Final Batch Loss: 0.5347991585731506\n",
      "Epoch 995, Loss: 1.6807538866996765, Final Batch Loss: 0.6443278193473816\n",
      "Epoch 996, Loss: 1.5651087760925293, Final Batch Loss: 0.5043423175811768\n",
      "Epoch 997, Loss: 1.5612907111644745, Final Batch Loss: 0.5435943603515625\n",
      "Epoch 998, Loss: 1.685535192489624, Final Batch Loss: 0.5593734979629517\n",
      "Epoch 999, Loss: 1.4815881848335266, Final Batch Loss: 0.46611288189888\n",
      "Epoch 1000, Loss: 1.6188253164291382, Final Batch Loss: 0.5761333703994751\n",
      "Epoch 1001, Loss: 1.6025028824806213, Final Batch Loss: 0.6529488563537598\n",
      "Epoch 1002, Loss: 1.5823327004909515, Final Batch Loss: 0.5384918451309204\n",
      "Epoch 1003, Loss: 1.6295826435089111, Final Batch Loss: 0.5702915787696838\n",
      "Epoch 1004, Loss: 1.5870907306671143, Final Batch Loss: 0.5359958410263062\n",
      "Epoch 1005, Loss: 1.5971968173980713, Final Batch Loss: 0.5059720873832703\n",
      "Epoch 1006, Loss: 1.7782689929008484, Final Batch Loss: 0.6230019330978394\n",
      "Epoch 1007, Loss: 1.5979743003845215, Final Batch Loss: 0.5164021253585815\n",
      "Epoch 1008, Loss: 1.5623648166656494, Final Batch Loss: 0.6076584458351135\n",
      "Epoch 1009, Loss: 1.5823292136192322, Final Batch Loss: 0.512252926826477\n",
      "Epoch 1010, Loss: 1.5281656682491302, Final Batch Loss: 0.4691210687160492\n",
      "Epoch 1011, Loss: 1.624601423740387, Final Batch Loss: 0.5484412908554077\n",
      "Epoch 1012, Loss: 1.5465280413627625, Final Batch Loss: 0.4404571056365967\n",
      "Epoch 1013, Loss: 1.5672609508037567, Final Batch Loss: 0.6145445704460144\n",
      "Epoch 1014, Loss: 1.6237216591835022, Final Batch Loss: 0.5798057317733765\n",
      "Epoch 1015, Loss: 1.5709629952907562, Final Batch Loss: 0.5813395380973816\n",
      "Epoch 1016, Loss: 1.5651173293590546, Final Batch Loss: 0.5139142870903015\n",
      "Epoch 1017, Loss: 1.5855432152748108, Final Batch Loss: 0.5920666456222534\n",
      "Epoch 1018, Loss: 1.6478570699691772, Final Batch Loss: 0.6806620359420776\n",
      "Epoch 1019, Loss: 1.5260260999202728, Final Batch Loss: 0.5047957301139832\n",
      "Epoch 1020, Loss: 1.672821581363678, Final Batch Loss: 0.6079341173171997\n",
      "Epoch 1021, Loss: 1.6024737358093262, Final Batch Loss: 0.5692917704582214\n",
      "Epoch 1022, Loss: 1.720237135887146, Final Batch Loss: 0.5365992784500122\n",
      "Epoch 1023, Loss: 1.5135718882083893, Final Batch Loss: 0.4536103904247284\n",
      "Epoch 1024, Loss: 1.5039782524108887, Final Batch Loss: 0.521775484085083\n",
      "Epoch 1025, Loss: 1.5621499717235565, Final Batch Loss: 0.45089927315711975\n",
      "Epoch 1026, Loss: 1.5338022410869598, Final Batch Loss: 0.5341280698776245\n",
      "Epoch 1027, Loss: 1.6163633465766907, Final Batch Loss: 0.5859476327896118\n",
      "Epoch 1028, Loss: 1.6209271848201752, Final Batch Loss: 0.5895674824714661\n",
      "Epoch 1029, Loss: 1.5289250910282135, Final Batch Loss: 0.47696545720100403\n",
      "Epoch 1030, Loss: 1.6013040840625763, Final Batch Loss: 0.5662727952003479\n",
      "Epoch 1031, Loss: 1.557811200618744, Final Batch Loss: 0.5139755010604858\n",
      "Epoch 1032, Loss: 1.5738560855388641, Final Batch Loss: 0.5323757529258728\n",
      "Epoch 1033, Loss: 1.591789186000824, Final Batch Loss: 0.49340003728866577\n",
      "Epoch 1034, Loss: 1.579417884349823, Final Batch Loss: 0.5798940062522888\n",
      "Epoch 1035, Loss: 1.577272206544876, Final Batch Loss: 0.5447180867195129\n",
      "Epoch 1036, Loss: 1.6322116255760193, Final Batch Loss: 0.5678887367248535\n",
      "Epoch 1037, Loss: 1.4975965321063995, Final Batch Loss: 0.5015407204627991\n",
      "Epoch 1038, Loss: 1.6654270887374878, Final Batch Loss: 0.6346791982650757\n",
      "Epoch 1039, Loss: 1.5387645959854126, Final Batch Loss: 0.5011875629425049\n",
      "Epoch 1040, Loss: 1.5458264648914337, Final Batch Loss: 0.45164820551872253\n",
      "Epoch 1041, Loss: 1.5890137553215027, Final Batch Loss: 0.5136646032333374\n",
      "Epoch 1042, Loss: 1.5071094334125519, Final Batch Loss: 0.45316681265830994\n",
      "Epoch 1043, Loss: 1.5184049904346466, Final Batch Loss: 0.44120079278945923\n",
      "Epoch 1044, Loss: 1.5116625726222992, Final Batch Loss: 0.47790005803108215\n",
      "Epoch 1045, Loss: 1.5068793296813965, Final Batch Loss: 0.4091450572013855\n",
      "Epoch 1046, Loss: 1.504594326019287, Final Batch Loss: 0.4640873074531555\n",
      "Epoch 1047, Loss: 1.475488394498825, Final Batch Loss: 0.4466029703617096\n",
      "Epoch 1048, Loss: 1.5769707560539246, Final Batch Loss: 0.5217640995979309\n",
      "Epoch 1049, Loss: 1.5628421306610107, Final Batch Loss: 0.4576377272605896\n",
      "Epoch 1050, Loss: 1.5920604765415192, Final Batch Loss: 0.5687690377235413\n",
      "Epoch 1051, Loss: 1.514789491891861, Final Batch Loss: 0.4269156754016876\n",
      "Epoch 1052, Loss: 1.5718409419059753, Final Batch Loss: 0.5072325468063354\n",
      "Epoch 1053, Loss: 1.569812685251236, Final Batch Loss: 0.5805959105491638\n",
      "Epoch 1054, Loss: 1.5854641199111938, Final Batch Loss: 0.5758904814720154\n",
      "Epoch 1055, Loss: 1.4310197532176971, Final Batch Loss: 0.4644933044910431\n",
      "Epoch 1056, Loss: 1.5119266510009766, Final Batch Loss: 0.4855634868144989\n",
      "Epoch 1057, Loss: 1.6258565783500671, Final Batch Loss: 0.5534949898719788\n",
      "Epoch 1058, Loss: 1.4749026000499725, Final Batch Loss: 0.5156371593475342\n",
      "Epoch 1059, Loss: 1.487922191619873, Final Batch Loss: 0.464643269777298\n",
      "Epoch 1060, Loss: 1.6471033692359924, Final Batch Loss: 0.5582367777824402\n",
      "Epoch 1061, Loss: 1.5639666020870209, Final Batch Loss: 0.5714231133460999\n",
      "Epoch 1062, Loss: 1.6388247311115265, Final Batch Loss: 0.5810019969940186\n",
      "Epoch 1063, Loss: 1.4862857460975647, Final Batch Loss: 0.5075103640556335\n",
      "Epoch 1064, Loss: 1.572964370250702, Final Batch Loss: 0.5360771417617798\n",
      "Epoch 1065, Loss: 1.5446280539035797, Final Batch Loss: 0.5122735500335693\n",
      "Epoch 1066, Loss: 1.6438220739364624, Final Batch Loss: 0.5504375100135803\n",
      "Epoch 1067, Loss: 1.4866712391376495, Final Batch Loss: 0.5127060413360596\n",
      "Epoch 1068, Loss: 1.4995853006839752, Final Batch Loss: 0.4636765420436859\n",
      "Epoch 1069, Loss: 1.5472303628921509, Final Batch Loss: 0.6093138456344604\n",
      "Epoch 1070, Loss: 1.5309282541275024, Final Batch Loss: 0.5122213959693909\n",
      "Epoch 1071, Loss: 1.6167075335979462, Final Batch Loss: 0.5917201042175293\n",
      "Epoch 1072, Loss: 1.4807044863700867, Final Batch Loss: 0.5118255019187927\n",
      "Epoch 1073, Loss: 1.6412394046783447, Final Batch Loss: 0.5554147362709045\n",
      "Epoch 1074, Loss: 1.5090252757072449, Final Batch Loss: 0.4256916642189026\n",
      "Epoch 1075, Loss: 1.5392957627773285, Final Batch Loss: 0.48247605562210083\n",
      "Epoch 1076, Loss: 1.613258421421051, Final Batch Loss: 0.5819240212440491\n",
      "Epoch 1077, Loss: 1.5752371847629547, Final Batch Loss: 0.6033990383148193\n",
      "Epoch 1078, Loss: 1.4678398072719574, Final Batch Loss: 0.43568241596221924\n",
      "Epoch 1079, Loss: 1.3905634582042694, Final Batch Loss: 0.41517016291618347\n",
      "Epoch 1080, Loss: 1.4424691796302795, Final Batch Loss: 0.4391533136367798\n",
      "Epoch 1081, Loss: 1.4195829033851624, Final Batch Loss: 0.4521228075027466\n",
      "Epoch 1082, Loss: 1.5539170801639557, Final Batch Loss: 0.5632050633430481\n",
      "Epoch 1083, Loss: 1.448717176914215, Final Batch Loss: 0.4528186619281769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1084, Loss: 1.6101904213428497, Final Batch Loss: 0.5976309776306152\n",
      "Epoch 1085, Loss: 1.558056503534317, Final Batch Loss: 0.49716857075691223\n",
      "Epoch 1086, Loss: 1.6336503624916077, Final Batch Loss: 0.5204128623008728\n",
      "Epoch 1087, Loss: 1.6515169143676758, Final Batch Loss: 0.604250431060791\n",
      "Epoch 1088, Loss: 1.5053508877754211, Final Batch Loss: 0.484507292509079\n",
      "Epoch 1089, Loss: 1.5193417072296143, Final Batch Loss: 0.4730941653251648\n",
      "Epoch 1090, Loss: 1.6488005816936493, Final Batch Loss: 0.6202144622802734\n",
      "Epoch 1091, Loss: 1.4769883751869202, Final Batch Loss: 0.5138534903526306\n",
      "Epoch 1092, Loss: 1.5715224146842957, Final Batch Loss: 0.4837416112422943\n",
      "Epoch 1093, Loss: 1.6247621774673462, Final Batch Loss: 0.5617181062698364\n",
      "Epoch 1094, Loss: 1.4994149506092072, Final Batch Loss: 0.5034089088439941\n",
      "Epoch 1095, Loss: 1.4375353753566742, Final Batch Loss: 0.41381171345710754\n",
      "Epoch 1096, Loss: 1.498489648103714, Final Batch Loss: 0.4637294113636017\n",
      "Epoch 1097, Loss: 1.5574460327625275, Final Batch Loss: 0.44672057032585144\n",
      "Epoch 1098, Loss: 1.5919603407382965, Final Batch Loss: 0.5818546414375305\n",
      "Epoch 1099, Loss: 1.6649296879768372, Final Batch Loss: 0.6183062791824341\n",
      "Epoch 1100, Loss: 1.5255182683467865, Final Batch Loss: 0.5530722737312317\n",
      "Epoch 1101, Loss: 1.528880774974823, Final Batch Loss: 0.4401819109916687\n",
      "Epoch 1102, Loss: 1.4568901062011719, Final Batch Loss: 0.42234236001968384\n",
      "Epoch 1103, Loss: 1.5259729623794556, Final Batch Loss: 0.5523948073387146\n",
      "Epoch 1104, Loss: 1.4342013001441956, Final Batch Loss: 0.4398849606513977\n",
      "Epoch 1105, Loss: 1.5018557906150818, Final Batch Loss: 0.4903581738471985\n",
      "Epoch 1106, Loss: 1.4435521066188812, Final Batch Loss: 0.4077565670013428\n",
      "Epoch 1107, Loss: 1.4170611500740051, Final Batch Loss: 0.40470224618911743\n",
      "Epoch 1108, Loss: 1.5221955180168152, Final Batch Loss: 0.5070110559463501\n",
      "Epoch 1109, Loss: 1.453106850385666, Final Batch Loss: 0.455626517534256\n",
      "Epoch 1110, Loss: 1.5942580103874207, Final Batch Loss: 0.5253636240959167\n",
      "Epoch 1111, Loss: 1.5732764303684235, Final Batch Loss: 0.6034449338912964\n",
      "Epoch 1112, Loss: 1.5725868940353394, Final Batch Loss: 0.5361058712005615\n",
      "Epoch 1113, Loss: 1.4252822995185852, Final Batch Loss: 0.4638970196247101\n",
      "Epoch 1114, Loss: 1.4928299188613892, Final Batch Loss: 0.6355960965156555\n",
      "Epoch 1115, Loss: 1.512223482131958, Final Batch Loss: 0.48884961009025574\n",
      "Epoch 1116, Loss: 1.4707467257976532, Final Batch Loss: 0.47831448912620544\n",
      "Epoch 1117, Loss: 1.467638075351715, Final Batch Loss: 0.5599881410598755\n",
      "Epoch 1118, Loss: 1.503868281841278, Final Batch Loss: 0.5858362317085266\n",
      "Epoch 1119, Loss: 1.54524427652359, Final Batch Loss: 0.456881582736969\n",
      "Epoch 1120, Loss: 1.3813740015029907, Final Batch Loss: 0.4695635735988617\n",
      "Epoch 1121, Loss: 1.4957467019557953, Final Batch Loss: 0.4711235463619232\n",
      "Epoch 1122, Loss: 1.4405560493469238, Final Batch Loss: 0.522643506526947\n",
      "Epoch 1123, Loss: 1.5511850416660309, Final Batch Loss: 0.6117777824401855\n",
      "Epoch 1124, Loss: 1.4214976727962494, Final Batch Loss: 0.4153676927089691\n",
      "Epoch 1125, Loss: 1.4501000344753265, Final Batch Loss: 0.4849872589111328\n",
      "Epoch 1126, Loss: 1.365939348936081, Final Batch Loss: 0.4403938949108124\n",
      "Epoch 1127, Loss: 1.3886778354644775, Final Batch Loss: 0.4268444776535034\n",
      "Epoch 1128, Loss: 1.4903705418109894, Final Batch Loss: 0.45098355412483215\n",
      "Epoch 1129, Loss: 1.5035851895809174, Final Batch Loss: 0.5310539603233337\n",
      "Epoch 1130, Loss: 1.4240950644016266, Final Batch Loss: 0.460803747177124\n",
      "Epoch 1131, Loss: 1.4111091792583466, Final Batch Loss: 0.4429793059825897\n",
      "Epoch 1132, Loss: 1.50822514295578, Final Batch Loss: 0.6254823803901672\n",
      "Epoch 1133, Loss: 1.5302942097187042, Final Batch Loss: 0.5280029773712158\n",
      "Epoch 1134, Loss: 1.495428055524826, Final Batch Loss: 0.5351582169532776\n",
      "Epoch 1135, Loss: 1.5190395712852478, Final Batch Loss: 0.48222047090530396\n",
      "Epoch 1136, Loss: 1.6158323287963867, Final Batch Loss: 0.4914137125015259\n",
      "Epoch 1137, Loss: 1.49797323346138, Final Batch Loss: 0.5522004961967468\n",
      "Epoch 1138, Loss: 1.48335862159729, Final Batch Loss: 0.46282535791397095\n",
      "Epoch 1139, Loss: 1.4735767841339111, Final Batch Loss: 0.45644301176071167\n",
      "Epoch 1140, Loss: 1.4404763579368591, Final Batch Loss: 0.505001425743103\n",
      "Epoch 1141, Loss: 1.3984279036521912, Final Batch Loss: 0.4237228333950043\n",
      "Epoch 1142, Loss: 1.3741036653518677, Final Batch Loss: 0.38839587569236755\n",
      "Epoch 1143, Loss: 1.4933820366859436, Final Batch Loss: 0.516343891620636\n",
      "Epoch 1144, Loss: 1.469416081905365, Final Batch Loss: 0.49672266840934753\n",
      "Epoch 1145, Loss: 1.422928124666214, Final Batch Loss: 0.4495220184326172\n",
      "Epoch 1146, Loss: 1.6210864186286926, Final Batch Loss: 0.5516055822372437\n",
      "Epoch 1147, Loss: 1.485031634569168, Final Batch Loss: 0.5015965700149536\n",
      "Epoch 1148, Loss: 1.4866255223751068, Final Batch Loss: 0.49238479137420654\n",
      "Epoch 1149, Loss: 1.4306248724460602, Final Batch Loss: 0.45391446352005005\n",
      "Epoch 1150, Loss: 1.5628281235694885, Final Batch Loss: 0.5304743647575378\n",
      "Epoch 1151, Loss: 1.4336169064044952, Final Batch Loss: 0.5121634602546692\n",
      "Epoch 1152, Loss: 1.4436160326004028, Final Batch Loss: 0.45036041736602783\n",
      "Epoch 1153, Loss: 1.3663325011730194, Final Batch Loss: 0.47720813751220703\n",
      "Epoch 1154, Loss: 1.4529546201229095, Final Batch Loss: 0.4480719268321991\n",
      "Epoch 1155, Loss: 1.4452550113201141, Final Batch Loss: 0.37160202860832214\n",
      "Epoch 1156, Loss: 1.329510360956192, Final Batch Loss: 0.41282904148101807\n",
      "Epoch 1157, Loss: 1.4623571634292603, Final Batch Loss: 0.5163815021514893\n",
      "Epoch 1158, Loss: 1.38884237408638, Final Batch Loss: 0.4317953586578369\n",
      "Epoch 1159, Loss: 1.398942619562149, Final Batch Loss: 0.43464604020118713\n",
      "Epoch 1160, Loss: 1.4176743626594543, Final Batch Loss: 0.4741899371147156\n",
      "Epoch 1161, Loss: 1.5113960206508636, Final Batch Loss: 0.5337491035461426\n",
      "Epoch 1162, Loss: 1.537972778081894, Final Batch Loss: 0.6171000003814697\n",
      "Epoch 1163, Loss: 1.4698219001293182, Final Batch Loss: 0.43204638361930847\n",
      "Epoch 1164, Loss: 1.486573725938797, Final Batch Loss: 0.5844359397888184\n",
      "Epoch 1165, Loss: 1.48147451877594, Final Batch Loss: 0.4567672312259674\n",
      "Epoch 1166, Loss: 1.6235061585903168, Final Batch Loss: 0.5884647965431213\n",
      "Epoch 1167, Loss: 1.4254377484321594, Final Batch Loss: 0.4535461962223053\n",
      "Epoch 1168, Loss: 1.4536873400211334, Final Batch Loss: 0.5005162358283997\n",
      "Epoch 1169, Loss: 1.3190003037452698, Final Batch Loss: 0.3899810016155243\n",
      "Epoch 1170, Loss: 1.4931066632270813, Final Batch Loss: 0.5015729069709778\n",
      "Epoch 1171, Loss: 1.5304374694824219, Final Batch Loss: 0.5261409282684326\n",
      "Epoch 1172, Loss: 1.5074867010116577, Final Batch Loss: 0.556484043598175\n",
      "Epoch 1173, Loss: 1.3342911005020142, Final Batch Loss: 0.37936386466026306\n",
      "Epoch 1174, Loss: 1.4277211725711823, Final Batch Loss: 0.47566303610801697\n",
      "Epoch 1175, Loss: 1.497559905052185, Final Batch Loss: 0.49150344729423523\n",
      "Epoch 1176, Loss: 1.4412072002887726, Final Batch Loss: 0.4775603115558624\n",
      "Epoch 1177, Loss: 1.5111819803714752, Final Batch Loss: 0.5528539419174194\n",
      "Epoch 1178, Loss: 1.5595744252204895, Final Batch Loss: 0.5479083061218262\n",
      "Epoch 1179, Loss: 1.531652718782425, Final Batch Loss: 0.5517329573631287\n",
      "Epoch 1180, Loss: 1.5346744656562805, Final Batch Loss: 0.5186458826065063\n",
      "Epoch 1181, Loss: 1.4982902109622955, Final Batch Loss: 0.5390737652778625\n",
      "Epoch 1182, Loss: 1.4217543303966522, Final Batch Loss: 0.45368894934654236\n",
      "Epoch 1183, Loss: 1.5385063588619232, Final Batch Loss: 0.5381144285202026\n",
      "Epoch 1184, Loss: 1.5448511242866516, Final Batch Loss: 0.602390706539154\n",
      "Epoch 1185, Loss: 1.4415677785873413, Final Batch Loss: 0.38608384132385254\n",
      "Epoch 1186, Loss: 1.4130206406116486, Final Batch Loss: 0.48541954159736633\n",
      "Epoch 1187, Loss: 1.3860782980918884, Final Batch Loss: 0.41581711173057556\n",
      "Epoch 1188, Loss: 1.3637697100639343, Final Batch Loss: 0.4056508243083954\n",
      "Epoch 1189, Loss: 1.431021362543106, Final Batch Loss: 0.5580312609672546\n",
      "Epoch 1190, Loss: 1.4310845136642456, Final Batch Loss: 0.5027912259101868\n",
      "Epoch 1191, Loss: 1.5435601472854614, Final Batch Loss: 0.5456272959709167\n",
      "Epoch 1192, Loss: 1.3929288685321808, Final Batch Loss: 0.45969507098197937\n",
      "Epoch 1193, Loss: 1.443313717842102, Final Batch Loss: 0.5441511869430542\n",
      "Epoch 1194, Loss: 1.5319781303405762, Final Batch Loss: 0.5592506527900696\n",
      "Epoch 1195, Loss: 1.3399417400360107, Final Batch Loss: 0.3440927565097809\n",
      "Epoch 1196, Loss: 1.5116090178489685, Final Batch Loss: 0.5321940183639526\n",
      "Epoch 1197, Loss: 1.4334897696971893, Final Batch Loss: 0.44958221912384033\n",
      "Epoch 1198, Loss: 1.4564133882522583, Final Batch Loss: 0.5387207269668579\n",
      "Epoch 1199, Loss: 1.5465520024299622, Final Batch Loss: 0.6397267580032349\n",
      "Epoch 1200, Loss: 1.4438853859901428, Final Batch Loss: 0.49204814434051514\n",
      "Epoch 1201, Loss: 1.3915722370147705, Final Batch Loss: 0.44792672991752625\n",
      "Epoch 1202, Loss: 1.5850777328014374, Final Batch Loss: 0.6455705761909485\n",
      "Epoch 1203, Loss: 1.5037463307380676, Final Batch Loss: 0.5231519341468811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1204, Loss: 1.404190719127655, Final Batch Loss: 0.43770939111709595\n",
      "Epoch 1205, Loss: 1.3854085505008698, Final Batch Loss: 0.3782373368740082\n",
      "Epoch 1206, Loss: 1.530457764863968, Final Batch Loss: 0.6150949001312256\n",
      "Epoch 1207, Loss: 1.3155688643455505, Final Batch Loss: 0.4684350788593292\n",
      "Epoch 1208, Loss: 1.3672045767307281, Final Batch Loss: 0.4302087128162384\n",
      "Epoch 1209, Loss: 1.2326982915401459, Final Batch Loss: 0.33842402696609497\n",
      "Epoch 1210, Loss: 1.457368791103363, Final Batch Loss: 0.4860696494579315\n",
      "Epoch 1211, Loss: 1.3445027470588684, Final Batch Loss: 0.3921920359134674\n",
      "Epoch 1212, Loss: 1.37932088971138, Final Batch Loss: 0.4992452561855316\n",
      "Epoch 1213, Loss: 1.5312301814556122, Final Batch Loss: 0.4984508156776428\n",
      "Epoch 1214, Loss: 1.4342334270477295, Final Batch Loss: 0.4442340135574341\n",
      "Epoch 1215, Loss: 1.3982194066047668, Final Batch Loss: 0.39306747913360596\n",
      "Epoch 1216, Loss: 1.514561951160431, Final Batch Loss: 0.5700494050979614\n",
      "Epoch 1217, Loss: 1.4956935048103333, Final Batch Loss: 0.555513322353363\n",
      "Epoch 1218, Loss: 1.597131371498108, Final Batch Loss: 0.5889686346054077\n",
      "Epoch 1219, Loss: 1.391091376543045, Final Batch Loss: 0.43217936158180237\n",
      "Epoch 1220, Loss: 1.37421452999115, Final Batch Loss: 0.4528645873069763\n",
      "Epoch 1221, Loss: 1.4794511497020721, Final Batch Loss: 0.5058798789978027\n",
      "Epoch 1222, Loss: 1.4219591617584229, Final Batch Loss: 0.5475543141365051\n",
      "Epoch 1223, Loss: 1.383072406053543, Final Batch Loss: 0.4366922080516815\n",
      "Epoch 1224, Loss: 1.3310596644878387, Final Batch Loss: 0.4058268666267395\n",
      "Epoch 1225, Loss: 1.515390694141388, Final Batch Loss: 0.596792459487915\n",
      "Epoch 1226, Loss: 1.423334687948227, Final Batch Loss: 0.4504849314689636\n",
      "Epoch 1227, Loss: 1.425095796585083, Final Batch Loss: 0.5800403952598572\n",
      "Epoch 1228, Loss: 1.4676332473754883, Final Batch Loss: 0.5313776731491089\n",
      "Epoch 1229, Loss: 1.551471322774887, Final Batch Loss: 0.5633408427238464\n",
      "Epoch 1230, Loss: 1.410315603017807, Final Batch Loss: 0.46753671765327454\n",
      "Epoch 1231, Loss: 1.2890860438346863, Final Batch Loss: 0.3590283691883087\n",
      "Epoch 1232, Loss: 1.3852571845054626, Final Batch Loss: 0.4361487925052643\n",
      "Epoch 1233, Loss: 1.4590066075325012, Final Batch Loss: 0.4978247284889221\n",
      "Epoch 1234, Loss: 1.3851777613162994, Final Batch Loss: 0.46533140540122986\n",
      "Epoch 1235, Loss: 1.4095501899719238, Final Batch Loss: 0.40649572014808655\n",
      "Epoch 1236, Loss: 1.3956920802593231, Final Batch Loss: 0.4509716033935547\n",
      "Epoch 1237, Loss: 1.40861114859581, Final Batch Loss: 0.5641299486160278\n",
      "Epoch 1238, Loss: 1.309810310602188, Final Batch Loss: 0.3987516164779663\n",
      "Epoch 1239, Loss: 1.3475995361804962, Final Batch Loss: 0.4456385672092438\n",
      "Epoch 1240, Loss: 1.4267899096012115, Final Batch Loss: 0.530443549156189\n",
      "Epoch 1241, Loss: 1.4577067196369171, Final Batch Loss: 0.5206621289253235\n",
      "Epoch 1242, Loss: 1.5115906298160553, Final Batch Loss: 0.6126307249069214\n",
      "Epoch 1243, Loss: 1.4104867577552795, Final Batch Loss: 0.515579879283905\n",
      "Epoch 1244, Loss: 1.4353603720664978, Final Batch Loss: 0.48408669233322144\n",
      "Epoch 1245, Loss: 1.3548885583877563, Final Batch Loss: 0.47306886315345764\n",
      "Epoch 1246, Loss: 1.4473236501216888, Final Batch Loss: 0.48379549384117126\n",
      "Epoch 1247, Loss: 1.3614331185817719, Final Batch Loss: 0.41155433654785156\n",
      "Epoch 1248, Loss: 1.3868637382984161, Final Batch Loss: 0.4574538767337799\n",
      "Epoch 1249, Loss: 1.3292218744754791, Final Batch Loss: 0.4818477928638458\n",
      "Epoch 1250, Loss: 1.371693104505539, Final Batch Loss: 0.4154493510723114\n",
      "Epoch 1251, Loss: 1.3020535111427307, Final Batch Loss: 0.45953628420829773\n",
      "Epoch 1252, Loss: 1.2962241470813751, Final Batch Loss: 0.3605910539627075\n",
      "Epoch 1253, Loss: 1.384730041027069, Final Batch Loss: 0.42305031418800354\n",
      "Epoch 1254, Loss: 1.5072880685329437, Final Batch Loss: 0.48006466031074524\n",
      "Epoch 1255, Loss: 1.4283047914505005, Final Batch Loss: 0.595798134803772\n",
      "Epoch 1256, Loss: 1.437557429075241, Final Batch Loss: 0.44324514269828796\n",
      "Epoch 1257, Loss: 1.4008879661560059, Final Batch Loss: 0.43810269236564636\n",
      "Epoch 1258, Loss: 1.3295200765132904, Final Batch Loss: 0.39837950468063354\n",
      "Epoch 1259, Loss: 1.508495181798935, Final Batch Loss: 0.5764517784118652\n",
      "Epoch 1260, Loss: 1.4497980773448944, Final Batch Loss: 0.507893979549408\n",
      "Epoch 1261, Loss: 1.4387559294700623, Final Batch Loss: 0.39860233664512634\n",
      "Epoch 1262, Loss: 1.3267289698123932, Final Batch Loss: 0.4379664659500122\n",
      "Epoch 1263, Loss: 1.3738375306129456, Final Batch Loss: 0.4276195764541626\n",
      "Epoch 1264, Loss: 1.378197431564331, Final Batch Loss: 0.44455188512802124\n",
      "Epoch 1265, Loss: 1.2509149610996246, Final Batch Loss: 0.4079464375972748\n",
      "Epoch 1266, Loss: 1.3264232873916626, Final Batch Loss: 0.3893740475177765\n",
      "Epoch 1267, Loss: 1.3777689337730408, Final Batch Loss: 0.48967209458351135\n",
      "Epoch 1268, Loss: 1.319397658109665, Final Batch Loss: 0.47121351957321167\n",
      "Epoch 1269, Loss: 1.3774921596050262, Final Batch Loss: 0.46601352095603943\n",
      "Epoch 1270, Loss: 1.3934010863304138, Final Batch Loss: 0.5161221027374268\n",
      "Epoch 1271, Loss: 1.503277212381363, Final Batch Loss: 0.5632391571998596\n",
      "Epoch 1272, Loss: 1.4474517107009888, Final Batch Loss: 0.5617994070053101\n",
      "Epoch 1273, Loss: 1.3067384958267212, Final Batch Loss: 0.42937737703323364\n",
      "Epoch 1274, Loss: 1.4000183045864105, Final Batch Loss: 0.48942074179649353\n",
      "Epoch 1275, Loss: 1.3356720507144928, Final Batch Loss: 0.37748783826828003\n",
      "Epoch 1276, Loss: 1.3323231041431427, Final Batch Loss: 0.37955012917518616\n",
      "Epoch 1277, Loss: 1.4128664135932922, Final Batch Loss: 0.533326268196106\n",
      "Epoch 1278, Loss: 1.508887231349945, Final Batch Loss: 0.5537839531898499\n",
      "Epoch 1279, Loss: 1.3681527972221375, Final Batch Loss: 0.47874295711517334\n",
      "Epoch 1280, Loss: 1.3243652284145355, Final Batch Loss: 0.4619302749633789\n",
      "Epoch 1281, Loss: 1.3223628103733063, Final Batch Loss: 0.40513259172439575\n",
      "Epoch 1282, Loss: 1.442615658044815, Final Batch Loss: 0.4879927337169647\n",
      "Epoch 1283, Loss: 1.4100141823291779, Final Batch Loss: 0.39827144145965576\n",
      "Epoch 1284, Loss: 1.2346563935279846, Final Batch Loss: 0.390481561422348\n",
      "Epoch 1285, Loss: 1.318559616804123, Final Batch Loss: 0.3546840250492096\n",
      "Epoch 1286, Loss: 1.4265969097614288, Final Batch Loss: 0.40295594930648804\n",
      "Epoch 1287, Loss: 1.4327474236488342, Final Batch Loss: 0.5078861117362976\n",
      "Epoch 1288, Loss: 1.308266282081604, Final Batch Loss: 0.39726150035858154\n",
      "Epoch 1289, Loss: 1.3587924540042877, Final Batch Loss: 0.3847323954105377\n",
      "Epoch 1290, Loss: 1.424831509590149, Final Batch Loss: 0.508537769317627\n",
      "Epoch 1291, Loss: 1.3743537068367004, Final Batch Loss: 0.44382044672966003\n",
      "Epoch 1292, Loss: 1.354543000459671, Final Batch Loss: 0.5042622685432434\n",
      "Epoch 1293, Loss: 1.3120927214622498, Final Batch Loss: 0.3842071294784546\n",
      "Epoch 1294, Loss: 1.3490366041660309, Final Batch Loss: 0.4390938878059387\n",
      "Epoch 1295, Loss: 1.287109375, Final Batch Loss: 0.4366333782672882\n",
      "Epoch 1296, Loss: 1.2749095857143402, Final Batch Loss: 0.43086859583854675\n",
      "Epoch 1297, Loss: 1.3344103693962097, Final Batch Loss: 0.42376258969306946\n",
      "Epoch 1298, Loss: 1.4103755354881287, Final Batch Loss: 0.46077150106430054\n",
      "Epoch 1299, Loss: 1.3091962039470673, Final Batch Loss: 0.45129504799842834\n",
      "Epoch 1300, Loss: 1.4104665219783783, Final Batch Loss: 0.4472307860851288\n",
      "Epoch 1301, Loss: 1.283517211675644, Final Batch Loss: 0.39515307545661926\n",
      "Epoch 1302, Loss: 1.5632714331150055, Final Batch Loss: 0.5645273327827454\n",
      "Epoch 1303, Loss: 1.3187267184257507, Final Batch Loss: 0.4726544916629791\n",
      "Epoch 1304, Loss: 1.4167356491088867, Final Batch Loss: 0.5712372064590454\n",
      "Epoch 1305, Loss: 1.342142939567566, Final Batch Loss: 0.45672065019607544\n",
      "Epoch 1306, Loss: 1.4176836609840393, Final Batch Loss: 0.5107786655426025\n",
      "Epoch 1307, Loss: 1.3325609266757965, Final Batch Loss: 0.4311048984527588\n",
      "Epoch 1308, Loss: 1.41682830452919, Final Batch Loss: 0.4472322165966034\n",
      "Epoch 1309, Loss: 1.3407571613788605, Final Batch Loss: 0.4836554229259491\n",
      "Epoch 1310, Loss: 1.2943395674228668, Final Batch Loss: 0.33227846026420593\n",
      "Epoch 1311, Loss: 1.2942691445350647, Final Batch Loss: 0.33936113119125366\n",
      "Epoch 1312, Loss: 1.2626680135726929, Final Batch Loss: 0.39393362402915955\n",
      "Epoch 1313, Loss: 1.4377292394638062, Final Batch Loss: 0.5165838003158569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1314, Loss: 1.369333952665329, Final Batch Loss: 0.45176807045936584\n",
      "Epoch 1315, Loss: 1.3570367693901062, Final Batch Loss: 0.46925264596939087\n",
      "Epoch 1316, Loss: 1.3796959817409515, Final Batch Loss: 0.4751984179019928\n",
      "Epoch 1317, Loss: 1.3293504416942596, Final Batch Loss: 0.4219907522201538\n",
      "Epoch 1318, Loss: 1.3111883103847504, Final Batch Loss: 0.49381178617477417\n",
      "Epoch 1319, Loss: 1.3282463550567627, Final Batch Loss: 0.4297923445701599\n",
      "Epoch 1320, Loss: 1.4647769629955292, Final Batch Loss: 0.5523356795310974\n",
      "Epoch 1321, Loss: 1.328370839357376, Final Batch Loss: 0.42416009306907654\n",
      "Epoch 1322, Loss: 1.3645786345005035, Final Batch Loss: 0.40412673354148865\n",
      "Epoch 1323, Loss: 1.376592993736267, Final Batch Loss: 0.47620928287506104\n",
      "Epoch 1324, Loss: 1.153202772140503, Final Batch Loss: 0.2924962341785431\n",
      "Epoch 1325, Loss: 1.3674144446849823, Final Batch Loss: 0.42856279015541077\n",
      "Epoch 1326, Loss: 1.373057633638382, Final Batch Loss: 0.4739731252193451\n",
      "Epoch 1327, Loss: 1.3324846625328064, Final Batch Loss: 0.40779033303260803\n",
      "Epoch 1328, Loss: 1.325185477733612, Final Batch Loss: 0.42988842725753784\n",
      "Epoch 1329, Loss: 1.3085618913173676, Final Batch Loss: 0.4221735894680023\n",
      "Epoch 1330, Loss: 1.252018928527832, Final Batch Loss: 0.42209485173225403\n",
      "Epoch 1331, Loss: 1.3466836512088776, Final Batch Loss: 0.41513529419898987\n",
      "Epoch 1332, Loss: 1.3442776799201965, Final Batch Loss: 0.4158623218536377\n",
      "Epoch 1333, Loss: 1.2419274747371674, Final Batch Loss: 0.41380852460861206\n",
      "Epoch 1334, Loss: 1.2946792244911194, Final Batch Loss: 0.37576958537101746\n",
      "Epoch 1335, Loss: 1.3701924681663513, Final Batch Loss: 0.4661889374256134\n",
      "Epoch 1336, Loss: 1.4810377061367035, Final Batch Loss: 0.5484262704849243\n",
      "Epoch 1337, Loss: 1.3830011487007141, Final Batch Loss: 0.49494558572769165\n",
      "Epoch 1338, Loss: 1.2983260750770569, Final Batch Loss: 0.43495622277259827\n",
      "Epoch 1339, Loss: 1.3986842036247253, Final Batch Loss: 0.5747572183609009\n",
      "Epoch 1340, Loss: 1.2989919781684875, Final Batch Loss: 0.4167565107345581\n",
      "Epoch 1341, Loss: 1.3863033652305603, Final Batch Loss: 0.5170847177505493\n",
      "Epoch 1342, Loss: 1.4448394477367401, Final Batch Loss: 0.5226722955703735\n",
      "Epoch 1343, Loss: 1.3175163269042969, Final Batch Loss: 0.408566415309906\n",
      "Epoch 1344, Loss: 1.271493375301361, Final Batch Loss: 0.29397884011268616\n",
      "Epoch 1345, Loss: 1.2674411535263062, Final Batch Loss: 0.4257866442203522\n",
      "Epoch 1346, Loss: 1.2297392189502716, Final Batch Loss: 0.3311983048915863\n",
      "Epoch 1347, Loss: 1.4375249743461609, Final Batch Loss: 0.45948681235313416\n",
      "Epoch 1348, Loss: 1.2693814039230347, Final Batch Loss: 0.3047807812690735\n",
      "Epoch 1349, Loss: 1.2506611049175262, Final Batch Loss: 0.38740983605384827\n",
      "Epoch 1350, Loss: 1.3535723388195038, Final Batch Loss: 0.43965989351272583\n",
      "Epoch 1351, Loss: 1.4328065514564514, Final Batch Loss: 0.4982931315898895\n",
      "Epoch 1352, Loss: 1.2996408343315125, Final Batch Loss: 0.38196319341659546\n",
      "Epoch 1353, Loss: 1.417091578245163, Final Batch Loss: 0.47697722911834717\n",
      "Epoch 1354, Loss: 1.270687997341156, Final Batch Loss: 0.38622942566871643\n",
      "Epoch 1355, Loss: 1.3054690062999725, Final Batch Loss: 0.4916219115257263\n",
      "Epoch 1356, Loss: 1.258253663778305, Final Batch Loss: 0.41011175513267517\n",
      "Epoch 1357, Loss: 1.3457084596157074, Final Batch Loss: 0.4808831810951233\n",
      "Epoch 1358, Loss: 1.1928353607654572, Final Batch Loss: 0.348136305809021\n",
      "Epoch 1359, Loss: 1.3542158901691437, Final Batch Loss: 0.5437803864479065\n",
      "Epoch 1360, Loss: 1.3187504708766937, Final Batch Loss: 0.4430835247039795\n",
      "Epoch 1361, Loss: 1.316089153289795, Final Batch Loss: 0.4071495234966278\n",
      "Epoch 1362, Loss: 1.3377698361873627, Final Batch Loss: 0.37457340955734253\n",
      "Epoch 1363, Loss: 1.2490149140357971, Final Batch Loss: 0.400167852640152\n",
      "Epoch 1364, Loss: 1.2786905765533447, Final Batch Loss: 0.4282962381839752\n",
      "Epoch 1365, Loss: 1.2716901898384094, Final Batch Loss: 0.4137743413448334\n",
      "Epoch 1366, Loss: 1.3373412191867828, Final Batch Loss: 0.415666788816452\n",
      "Epoch 1367, Loss: 1.3280252814292908, Final Batch Loss: 0.4268825948238373\n",
      "Epoch 1368, Loss: 1.341654121875763, Final Batch Loss: 0.4292105734348297\n",
      "Epoch 1369, Loss: 1.2949285209178925, Final Batch Loss: 0.4262162148952484\n",
      "Epoch 1370, Loss: 1.4025274813175201, Final Batch Loss: 0.5322335958480835\n",
      "Epoch 1371, Loss: 1.2307004928588867, Final Batch Loss: 0.35908639430999756\n",
      "Epoch 1372, Loss: 1.3274862468242645, Final Batch Loss: 0.486198753118515\n",
      "Epoch 1373, Loss: 1.2488738894462585, Final Batch Loss: 0.4213705062866211\n",
      "Epoch 1374, Loss: 1.2653588950634003, Final Batch Loss: 0.42706820368766785\n",
      "Epoch 1375, Loss: 1.261289656162262, Final Batch Loss: 0.42197808623313904\n",
      "Epoch 1376, Loss: 1.320300668478012, Final Batch Loss: 0.4529114067554474\n",
      "Epoch 1377, Loss: 1.3546659350395203, Final Batch Loss: 0.4230329692363739\n",
      "Epoch 1378, Loss: 1.4094300866127014, Final Batch Loss: 0.4820116460323334\n",
      "Epoch 1379, Loss: 1.3939939737319946, Final Batch Loss: 0.6098934412002563\n",
      "Epoch 1380, Loss: 1.328856498003006, Final Batch Loss: 0.4694533944129944\n",
      "Epoch 1381, Loss: 1.4855782389640808, Final Batch Loss: 0.5084697008132935\n",
      "Epoch 1382, Loss: 1.2481251657009125, Final Batch Loss: 0.35212376713752747\n",
      "Epoch 1383, Loss: 1.2837195694446564, Final Batch Loss: 0.44278067350387573\n",
      "Epoch 1384, Loss: 1.320334792137146, Final Batch Loss: 0.5041483044624329\n",
      "Epoch 1385, Loss: 1.1618259847164154, Final Batch Loss: 0.3234196603298187\n",
      "Epoch 1386, Loss: 1.23561891913414, Final Batch Loss: 0.4016205966472626\n",
      "Epoch 1387, Loss: 1.2541985213756561, Final Batch Loss: 0.3600473403930664\n",
      "Epoch 1388, Loss: 1.2819695472717285, Final Batch Loss: 0.4498932659626007\n",
      "Epoch 1389, Loss: 1.2701724469661713, Final Batch Loss: 0.4002377986907959\n",
      "Epoch 1390, Loss: 1.280362755060196, Final Batch Loss: 0.45410627126693726\n",
      "Epoch 1391, Loss: 1.3749144077301025, Final Batch Loss: 0.42912814021110535\n",
      "Epoch 1392, Loss: 1.3282560408115387, Final Batch Loss: 0.37945669889450073\n",
      "Epoch 1393, Loss: 1.2987383008003235, Final Batch Loss: 0.47049447894096375\n",
      "Epoch 1394, Loss: 1.2875443696975708, Final Batch Loss: 0.3450659513473511\n",
      "Epoch 1395, Loss: 1.1609109193086624, Final Batch Loss: 0.2258373349905014\n",
      "Epoch 1396, Loss: 1.3016276955604553, Final Batch Loss: 0.48004379868507385\n",
      "Epoch 1397, Loss: 1.2498423159122467, Final Batch Loss: 0.37825703620910645\n",
      "Epoch 1398, Loss: 1.3520035445690155, Final Batch Loss: 0.45410048961639404\n",
      "Epoch 1399, Loss: 1.340835452079773, Final Batch Loss: 0.422573059797287\n",
      "Epoch 1400, Loss: 1.2721980810165405, Final Batch Loss: 0.31389597058296204\n",
      "Epoch 1401, Loss: 1.31216499209404, Final Batch Loss: 0.39171653985977173\n",
      "Epoch 1402, Loss: 1.3941220939159393, Final Batch Loss: 0.4806058406829834\n",
      "Epoch 1403, Loss: 1.3210211992263794, Final Batch Loss: 0.5118212103843689\n",
      "Epoch 1404, Loss: 1.227483183145523, Final Batch Loss: 0.3629664182662964\n",
      "Epoch 1405, Loss: 1.336205631494522, Final Batch Loss: 0.4448372423648834\n",
      "Epoch 1406, Loss: 1.2252247631549835, Final Batch Loss: 0.3173418343067169\n",
      "Epoch 1407, Loss: 1.2937141358852386, Final Batch Loss: 0.4422469437122345\n",
      "Epoch 1408, Loss: 1.2723052203655243, Final Batch Loss: 0.46077820658683777\n",
      "Epoch 1409, Loss: 1.3347490727901459, Final Batch Loss: 0.4741654694080353\n",
      "Epoch 1410, Loss: 1.444626808166504, Final Batch Loss: 0.5750643610954285\n",
      "Epoch 1411, Loss: 1.3056095242500305, Final Batch Loss: 0.4095854163169861\n",
      "Epoch 1412, Loss: 1.1391312181949615, Final Batch Loss: 0.2756488025188446\n",
      "Epoch 1413, Loss: 1.2603875696659088, Final Batch Loss: 0.4896778464317322\n",
      "Epoch 1414, Loss: 1.1901623904705048, Final Batch Loss: 0.445550799369812\n",
      "Epoch 1415, Loss: 1.2201395332813263, Final Batch Loss: 0.41044241189956665\n",
      "Epoch 1416, Loss: 1.2953012883663177, Final Batch Loss: 0.4442330598831177\n",
      "Epoch 1417, Loss: 1.3168476223945618, Final Batch Loss: 0.4038366377353668\n",
      "Epoch 1418, Loss: 1.2822335064411163, Final Batch Loss: 0.43282151222229004\n",
      "Epoch 1419, Loss: 1.4246417582035065, Final Batch Loss: 0.4035789370536804\n",
      "Epoch 1420, Loss: 1.2697731852531433, Final Batch Loss: 0.3927965462207794\n",
      "Epoch 1421, Loss: 1.189016342163086, Final Batch Loss: 0.39220383763313293\n",
      "Epoch 1422, Loss: 1.3164347410202026, Final Batch Loss: 0.42817986011505127\n",
      "Epoch 1423, Loss: 1.3160105347633362, Final Batch Loss: 0.44612255692481995\n",
      "Epoch 1424, Loss: 1.2946620881557465, Final Batch Loss: 0.4151635766029358\n",
      "Epoch 1425, Loss: 1.3042917549610138, Final Batch Loss: 0.4660918414592743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1426, Loss: 1.2718932628631592, Final Batch Loss: 0.3943706452846527\n",
      "Epoch 1427, Loss: 1.2736295461654663, Final Batch Loss: 0.41224753856658936\n",
      "Epoch 1428, Loss: 1.2991794645786285, Final Batch Loss: 0.482079416513443\n",
      "Epoch 1429, Loss: 1.3107371926307678, Final Batch Loss: 0.45270678400993347\n",
      "Epoch 1430, Loss: 1.3431262969970703, Final Batch Loss: 0.4231441617012024\n",
      "Epoch 1431, Loss: 1.275728702545166, Final Batch Loss: 0.42419877648353577\n",
      "Epoch 1432, Loss: 1.3679827451705933, Final Batch Loss: 0.4918310046195984\n",
      "Epoch 1433, Loss: 1.2123321890830994, Final Batch Loss: 0.3834547698497772\n",
      "Epoch 1434, Loss: 1.2357885539531708, Final Batch Loss: 0.384133517742157\n",
      "Epoch 1435, Loss: 1.361654669046402, Final Batch Loss: 0.5016881823539734\n",
      "Epoch 1436, Loss: 1.2434102296829224, Final Batch Loss: 0.42140716314315796\n",
      "Epoch 1437, Loss: 1.1630459129810333, Final Batch Loss: 0.3478260040283203\n",
      "Epoch 1438, Loss: 1.242801696062088, Final Batch Loss: 0.41791287064552307\n",
      "Epoch 1439, Loss: 1.259776383638382, Final Batch Loss: 0.4279414117336273\n",
      "Epoch 1440, Loss: 1.2679237723350525, Final Batch Loss: 0.37612518668174744\n",
      "Epoch 1441, Loss: 1.2483867406845093, Final Batch Loss: 0.35447171330451965\n",
      "Epoch 1442, Loss: 1.2956512868404388, Final Batch Loss: 0.46314379572868347\n",
      "Epoch 1443, Loss: 1.3204617202281952, Final Batch Loss: 0.5062634944915771\n",
      "Epoch 1444, Loss: 1.2217076420783997, Final Batch Loss: 0.4271278977394104\n",
      "Epoch 1445, Loss: 1.3653555810451508, Final Batch Loss: 0.4203145205974579\n",
      "Epoch 1446, Loss: 1.315897524356842, Final Batch Loss: 0.41144368052482605\n",
      "Epoch 1447, Loss: 1.3607006967067719, Final Batch Loss: 0.5157939195632935\n",
      "Epoch 1448, Loss: 1.2472743690013885, Final Batch Loss: 0.42193812131881714\n",
      "Epoch 1449, Loss: 1.1487172544002533, Final Batch Loss: 0.31242361664772034\n",
      "Epoch 1450, Loss: 1.246078997850418, Final Batch Loss: 0.40451136231422424\n",
      "Epoch 1451, Loss: 1.3193925619125366, Final Batch Loss: 0.4246708154678345\n",
      "Epoch 1452, Loss: 1.1626887619495392, Final Batch Loss: 0.3276238739490509\n",
      "Epoch 1453, Loss: 1.2305383086204529, Final Batch Loss: 0.4397290050983429\n",
      "Epoch 1454, Loss: 1.3184528052806854, Final Batch Loss: 0.4863431453704834\n",
      "Epoch 1455, Loss: 1.354516476392746, Final Batch Loss: 0.5223258137702942\n",
      "Epoch 1456, Loss: 1.350925236940384, Final Batch Loss: 0.5033647418022156\n",
      "Epoch 1457, Loss: 1.171628087759018, Final Batch Loss: 0.3251418471336365\n",
      "Epoch 1458, Loss: 1.2657627761363983, Final Batch Loss: 0.39676132798194885\n",
      "Epoch 1459, Loss: 1.2584153413772583, Final Batch Loss: 0.39619511365890503\n",
      "Epoch 1460, Loss: 1.2732004523277283, Final Batch Loss: 0.4189823865890503\n",
      "Epoch 1461, Loss: 1.231828361749649, Final Batch Loss: 0.41118448972702026\n",
      "Epoch 1462, Loss: 1.4136560261249542, Final Batch Loss: 0.4608026444911957\n",
      "Epoch 1463, Loss: 1.2973105013370514, Final Batch Loss: 0.4477871358394623\n",
      "Epoch 1464, Loss: 1.324926882982254, Final Batch Loss: 0.3954807221889496\n",
      "Epoch 1465, Loss: 1.2172456979751587, Final Batch Loss: 0.40638720989227295\n",
      "Epoch 1466, Loss: 1.3674598336219788, Final Batch Loss: 0.43423405289649963\n",
      "Epoch 1467, Loss: 1.2719513177871704, Final Batch Loss: 0.4140833020210266\n",
      "Epoch 1468, Loss: 1.137471228837967, Final Batch Loss: 0.334720253944397\n",
      "Epoch 1469, Loss: 1.2998862862586975, Final Batch Loss: 0.45537957549095154\n",
      "Epoch 1470, Loss: 1.1817116439342499, Final Batch Loss: 0.36885181069374084\n",
      "Epoch 1471, Loss: 1.271750956773758, Final Batch Loss: 0.4346110224723816\n",
      "Epoch 1472, Loss: 1.3045408427715302, Final Batch Loss: 0.5036858916282654\n",
      "Epoch 1473, Loss: 1.201370745897293, Final Batch Loss: 0.3249894082546234\n",
      "Epoch 1474, Loss: 1.2265520691871643, Final Batch Loss: 0.3846936821937561\n",
      "Epoch 1475, Loss: 1.1997222006320953, Final Batch Loss: 0.36615821719169617\n",
      "Epoch 1476, Loss: 1.2147935926914215, Final Batch Loss: 0.3950175940990448\n",
      "Epoch 1477, Loss: 1.2551657855510712, Final Batch Loss: 0.38775038719177246\n",
      "Epoch 1478, Loss: 1.1984824538230896, Final Batch Loss: 0.3943767845630646\n",
      "Epoch 1479, Loss: 1.3239630162715912, Final Batch Loss: 0.46343931555747986\n",
      "Epoch 1480, Loss: 1.2011043429374695, Final Batch Loss: 0.39407938718795776\n",
      "Epoch 1481, Loss: 1.252465695142746, Final Batch Loss: 0.419345885515213\n",
      "Epoch 1482, Loss: 1.227759212255478, Final Batch Loss: 0.4198843538761139\n",
      "Epoch 1483, Loss: 1.2937783002853394, Final Batch Loss: 0.338796466588974\n",
      "Epoch 1484, Loss: 1.2244616150856018, Final Batch Loss: 0.40714067220687866\n",
      "Epoch 1485, Loss: 1.341263234615326, Final Batch Loss: 0.5628215074539185\n",
      "Epoch 1486, Loss: 1.4052008390426636, Final Batch Loss: 0.5648042559623718\n",
      "Epoch 1487, Loss: 1.1884300708770752, Final Batch Loss: 0.3534661531448364\n",
      "Epoch 1488, Loss: 1.2292738854885101, Final Batch Loss: 0.4469287693500519\n",
      "Epoch 1489, Loss: 1.303373545408249, Final Batch Loss: 0.4953577220439911\n",
      "Epoch 1490, Loss: 1.2964084148406982, Final Batch Loss: 0.4843395948410034\n",
      "Epoch 1491, Loss: 1.2272677421569824, Final Batch Loss: 0.3376888930797577\n",
      "Epoch 1492, Loss: 1.329566478729248, Final Batch Loss: 0.5184587836265564\n",
      "Epoch 1493, Loss: 1.225581407546997, Final Batch Loss: 0.4164733588695526\n",
      "Epoch 1494, Loss: 1.3238482773303986, Final Batch Loss: 0.39878907799720764\n",
      "Epoch 1495, Loss: 1.3818734288215637, Final Batch Loss: 0.6017776727676392\n",
      "Epoch 1496, Loss: 1.1762834191322327, Final Batch Loss: 0.3311348259449005\n",
      "Epoch 1497, Loss: 1.2961862683296204, Final Batch Loss: 0.4706440567970276\n",
      "Epoch 1498, Loss: 1.2245236039161682, Final Batch Loss: 0.4110304117202759\n",
      "Epoch 1499, Loss: 1.1518408954143524, Final Batch Loss: 0.31393545866012573\n",
      "Epoch 1500, Loss: 1.2381987273693085, Final Batch Loss: 0.38644444942474365\n",
      "Epoch 1501, Loss: 1.298035889863968, Final Batch Loss: 0.5159045457839966\n",
      "Epoch 1502, Loss: 1.2069447040557861, Final Batch Loss: 0.3656596541404724\n",
      "Epoch 1503, Loss: 1.2168710827827454, Final Batch Loss: 0.32546108961105347\n",
      "Epoch 1504, Loss: 1.2527428567409515, Final Batch Loss: 0.4192746877670288\n",
      "Epoch 1505, Loss: 1.3260129690170288, Final Batch Loss: 0.4643840789794922\n",
      "Epoch 1506, Loss: 1.2799644470214844, Final Batch Loss: 0.40893471240997314\n",
      "Epoch 1507, Loss: 1.2532747089862823, Final Batch Loss: 0.413125604391098\n",
      "Epoch 1508, Loss: 1.2787116169929504, Final Batch Loss: 0.4513864815235138\n",
      "Epoch 1509, Loss: 1.201831430196762, Final Batch Loss: 0.32070258259773254\n",
      "Epoch 1510, Loss: 1.2819092869758606, Final Batch Loss: 0.3898845613002777\n",
      "Epoch 1511, Loss: 1.2347593903541565, Final Batch Loss: 0.4769045114517212\n",
      "Epoch 1512, Loss: 1.2179568707942963, Final Batch Loss: 0.4605575501918793\n",
      "Epoch 1513, Loss: 1.1505567133426666, Final Batch Loss: 0.39851364493370056\n",
      "Epoch 1514, Loss: 1.2905719876289368, Final Batch Loss: 0.36072924733161926\n",
      "Epoch 1515, Loss: 1.3197100758552551, Final Batch Loss: 0.43675118684768677\n",
      "Epoch 1516, Loss: 1.1747432351112366, Final Batch Loss: 0.3598085939884186\n",
      "Epoch 1517, Loss: 1.226662039756775, Final Batch Loss: 0.39576053619384766\n",
      "Epoch 1518, Loss: 1.3039880692958832, Final Batch Loss: 0.46972477436065674\n",
      "Epoch 1519, Loss: 1.1424476206302643, Final Batch Loss: 0.40014323592185974\n",
      "Epoch 1520, Loss: 1.3365394175052643, Final Batch Loss: 0.46290287375450134\n",
      "Epoch 1521, Loss: 1.245400607585907, Final Batch Loss: 0.43974316120147705\n",
      "Epoch 1522, Loss: 1.3124602735042572, Final Batch Loss: 0.3867514133453369\n",
      "Epoch 1523, Loss: 1.2925008833408356, Final Batch Loss: 0.470692902803421\n",
      "Epoch 1524, Loss: 1.3569454848766327, Final Batch Loss: 0.5032345652580261\n",
      "Epoch 1525, Loss: 1.3746241629123688, Final Batch Loss: 0.5101478695869446\n",
      "Epoch 1526, Loss: 1.1444311738014221, Final Batch Loss: 0.3782508373260498\n",
      "Epoch 1527, Loss: 1.3759998977184296, Final Batch Loss: 0.4979662597179413\n",
      "Epoch 1528, Loss: 1.174782633781433, Final Batch Loss: 0.3327786922454834\n",
      "Epoch 1529, Loss: 1.2749442756175995, Final Batch Loss: 0.42247605323791504\n",
      "Epoch 1530, Loss: 1.2860450148582458, Final Batch Loss: 0.4515989422798157\n",
      "Epoch 1531, Loss: 1.2447285056114197, Final Batch Loss: 0.40291523933410645\n",
      "Epoch 1532, Loss: 1.1821957230567932, Final Batch Loss: 0.40148666501045227\n",
      "Epoch 1533, Loss: 1.3465461134910583, Final Batch Loss: 0.4891849458217621\n",
      "Epoch 1534, Loss: 1.2703741490840912, Final Batch Loss: 0.447020024061203\n",
      "Epoch 1535, Loss: 1.2521013915538788, Final Batch Loss: 0.42716437578201294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1536, Loss: 1.2523778676986694, Final Batch Loss: 0.49323391914367676\n",
      "Epoch 1537, Loss: 1.1691868603229523, Final Batch Loss: 0.39596855640411377\n",
      "Epoch 1538, Loss: 1.2382028996944427, Final Batch Loss: 0.46777814626693726\n",
      "Epoch 1539, Loss: 1.239585816860199, Final Batch Loss: 0.3156667649745941\n",
      "Epoch 1540, Loss: 1.2472331523895264, Final Batch Loss: 0.39491137862205505\n",
      "Epoch 1541, Loss: 1.2193306684494019, Final Batch Loss: 0.33640408515930176\n",
      "Epoch 1542, Loss: 1.2897177338600159, Final Batch Loss: 0.42951810359954834\n",
      "Epoch 1543, Loss: 1.2317546606063843, Final Batch Loss: 0.387961208820343\n",
      "Epoch 1544, Loss: 1.3026338517665863, Final Batch Loss: 0.5049825310707092\n",
      "Epoch 1545, Loss: 1.2473114132881165, Final Batch Loss: 0.455121248960495\n",
      "Epoch 1546, Loss: 1.1569799482822418, Final Batch Loss: 0.33526402711868286\n",
      "Epoch 1547, Loss: 1.2096679508686066, Final Batch Loss: 0.4706183671951294\n",
      "Epoch 1548, Loss: 1.2705738842487335, Final Batch Loss: 0.41227608919143677\n",
      "Epoch 1549, Loss: 1.239007145166397, Final Batch Loss: 0.4737853407859802\n",
      "Epoch 1550, Loss: 1.2943406105041504, Final Batch Loss: 0.540780246257782\n",
      "Epoch 1551, Loss: 1.2025380730628967, Final Batch Loss: 0.32276272773742676\n",
      "Epoch 1552, Loss: 1.235123097896576, Final Batch Loss: 0.3896922469139099\n",
      "Epoch 1553, Loss: 1.2434698343276978, Final Batch Loss: 0.39055511355400085\n",
      "Epoch 1554, Loss: 1.238942712545395, Final Batch Loss: 0.5021584033966064\n",
      "Epoch 1555, Loss: 1.2299147248268127, Final Batch Loss: 0.36735212802886963\n",
      "Epoch 1556, Loss: 1.222626954317093, Final Batch Loss: 0.44500553607940674\n",
      "Epoch 1557, Loss: 1.3987527191638947, Final Batch Loss: 0.5277168154716492\n",
      "Epoch 1558, Loss: 1.3025138974189758, Final Batch Loss: 0.4896852374076843\n",
      "Epoch 1559, Loss: 1.273909479379654, Final Batch Loss: 0.5438904762268066\n",
      "Epoch 1560, Loss: 1.3161579072475433, Final Batch Loss: 0.44307199120521545\n",
      "Epoch 1561, Loss: 1.2094064056873322, Final Batch Loss: 0.40805384516716003\n",
      "Epoch 1562, Loss: 1.2118742763996124, Final Batch Loss: 0.36397987604141235\n",
      "Epoch 1563, Loss: 1.1142070889472961, Final Batch Loss: 0.2931492328643799\n",
      "Epoch 1564, Loss: 1.1439396440982819, Final Batch Loss: 0.286050021648407\n",
      "Epoch 1565, Loss: 1.2192747592926025, Final Batch Loss: 0.3482591509819031\n",
      "Epoch 1566, Loss: 1.2732561230659485, Final Batch Loss: 0.589414656162262\n",
      "Epoch 1567, Loss: 1.3477005660533905, Final Batch Loss: 0.5379025340080261\n",
      "Epoch 1568, Loss: 1.1502257883548737, Final Batch Loss: 0.33942073583602905\n",
      "Epoch 1569, Loss: 1.1516902148723602, Final Batch Loss: 0.34286758303642273\n",
      "Epoch 1570, Loss: 1.2435854375362396, Final Batch Loss: 0.4397674798965454\n",
      "Epoch 1571, Loss: 1.2965663373470306, Final Batch Loss: 0.428780734539032\n",
      "Epoch 1572, Loss: 1.2038949131965637, Final Batch Loss: 0.40913596749305725\n",
      "Epoch 1573, Loss: 1.2697513103485107, Final Batch Loss: 0.4859619438648224\n",
      "Epoch 1574, Loss: 1.2187568247318268, Final Batch Loss: 0.4110196828842163\n",
      "Epoch 1575, Loss: 1.2024878859519958, Final Batch Loss: 0.36573150753974915\n",
      "Epoch 1576, Loss: 1.1764847338199615, Final Batch Loss: 0.3178234100341797\n",
      "Epoch 1577, Loss: 1.2161261439323425, Final Batch Loss: 0.31797975301742554\n",
      "Epoch 1578, Loss: 1.2610571384429932, Final Batch Loss: 0.4919176995754242\n",
      "Epoch 1579, Loss: 1.1695059537887573, Final Batch Loss: 0.35217899084091187\n",
      "Epoch 1580, Loss: 1.218570500612259, Final Batch Loss: 0.4357670843601227\n",
      "Epoch 1581, Loss: 1.2172246873378754, Final Batch Loss: 0.36023610830307007\n",
      "Epoch 1582, Loss: 1.2360834181308746, Final Batch Loss: 0.3479972183704376\n",
      "Epoch 1583, Loss: 1.1928331851959229, Final Batch Loss: 0.4240705072879791\n",
      "Epoch 1584, Loss: 1.2268660962581635, Final Batch Loss: 0.4346010684967041\n",
      "Epoch 1585, Loss: 1.1005227267742157, Final Batch Loss: 0.3582558333873749\n",
      "Epoch 1586, Loss: 1.2888162434101105, Final Batch Loss: 0.41725826263427734\n",
      "Epoch 1587, Loss: 1.1009475588798523, Final Batch Loss: 0.2696615755558014\n",
      "Epoch 1588, Loss: 1.1952087879180908, Final Batch Loss: 0.35797008872032166\n",
      "Epoch 1589, Loss: 1.1286130547523499, Final Batch Loss: 0.3545224368572235\n",
      "Epoch 1590, Loss: 1.1768818199634552, Final Batch Loss: 0.3009062707424164\n",
      "Epoch 1591, Loss: 1.2665621638298035, Final Batch Loss: 0.47609248757362366\n",
      "Epoch 1592, Loss: 1.244019776582718, Final Batch Loss: 0.43671029806137085\n",
      "Epoch 1593, Loss: 1.2182058990001678, Final Batch Loss: 0.46820348501205444\n",
      "Epoch 1594, Loss: 1.1445229053497314, Final Batch Loss: 0.33982372283935547\n",
      "Epoch 1595, Loss: 1.2237756848335266, Final Batch Loss: 0.3905543386936188\n",
      "Epoch 1596, Loss: 1.2385302186012268, Final Batch Loss: 0.4512953460216522\n",
      "Epoch 1597, Loss: 1.2376212179660797, Final Batch Loss: 0.4021967351436615\n",
      "Epoch 1598, Loss: 1.284818023443222, Final Batch Loss: 0.5477351546287537\n",
      "Epoch 1599, Loss: 1.2217766642570496, Final Batch Loss: 0.41683706641197205\n",
      "Epoch 1600, Loss: 1.218384861946106, Final Batch Loss: 0.4385661780834198\n",
      "Epoch 1601, Loss: 1.291521042585373, Final Batch Loss: 0.47346577048301697\n",
      "Epoch 1602, Loss: 1.1451259851455688, Final Batch Loss: 0.37154310941696167\n",
      "Epoch 1603, Loss: 1.2800073325634003, Final Batch Loss: 0.5615403056144714\n",
      "Epoch 1604, Loss: 1.2922942340373993, Final Batch Loss: 0.47031697630882263\n",
      "Epoch 1605, Loss: 1.2064171731472015, Final Batch Loss: 0.41366812586784363\n",
      "Epoch 1606, Loss: 1.2764337956905365, Final Batch Loss: 0.484124094247818\n",
      "Epoch 1607, Loss: 1.2842287123203278, Final Batch Loss: 0.4847545027732849\n",
      "Epoch 1608, Loss: 1.1935870051383972, Final Batch Loss: 0.3938787877559662\n",
      "Epoch 1609, Loss: 1.2376473248004913, Final Batch Loss: 0.42569753527641296\n",
      "Epoch 1610, Loss: 1.2345688939094543, Final Batch Loss: 0.41438359022140503\n",
      "Epoch 1611, Loss: 1.19084632396698, Final Batch Loss: 0.4304632842540741\n",
      "Epoch 1612, Loss: 1.2214692831039429, Final Batch Loss: 0.41783928871154785\n",
      "Epoch 1613, Loss: 1.1969806849956512, Final Batch Loss: 0.3968326151371002\n",
      "Epoch 1614, Loss: 1.2471921741962433, Final Batch Loss: 0.47579506039619446\n",
      "Epoch 1615, Loss: 1.2198677062988281, Final Batch Loss: 0.4095955491065979\n",
      "Epoch 1616, Loss: 1.0906970798969269, Final Batch Loss: 0.25635212659835815\n",
      "Epoch 1617, Loss: 1.1322660744190216, Final Batch Loss: 0.33226126432418823\n",
      "Epoch 1618, Loss: 1.1494214832782745, Final Batch Loss: 0.35161808133125305\n",
      "Epoch 1619, Loss: 1.2095773816108704, Final Batch Loss: 0.44496235251426697\n",
      "Epoch 1620, Loss: 1.1310823559761047, Final Batch Loss: 0.32216161489486694\n",
      "Epoch 1621, Loss: 1.2272269129753113, Final Batch Loss: 0.3969286382198334\n",
      "Epoch 1622, Loss: 1.2099004983901978, Final Batch Loss: 0.3996938169002533\n",
      "Epoch 1623, Loss: 1.1948322057724, Final Batch Loss: 0.3566697835922241\n",
      "Epoch 1624, Loss: 1.158738523721695, Final Batch Loss: 0.38663873076438904\n",
      "Epoch 1625, Loss: 1.121952772140503, Final Batch Loss: 0.3970364034175873\n",
      "Epoch 1626, Loss: 1.110472172498703, Final Batch Loss: 0.35247674584388733\n",
      "Epoch 1627, Loss: 1.2973630726337433, Final Batch Loss: 0.4820343852043152\n",
      "Epoch 1628, Loss: 1.1871289312839508, Final Batch Loss: 0.3838651776313782\n",
      "Epoch 1629, Loss: 1.3143255114555359, Final Batch Loss: 0.38964587450027466\n",
      "Epoch 1630, Loss: 1.2915056645870209, Final Batch Loss: 0.4831344187259674\n",
      "Epoch 1631, Loss: 1.2358351945877075, Final Batch Loss: 0.3857298493385315\n",
      "Epoch 1632, Loss: 1.1841668784618378, Final Batch Loss: 0.33254826068878174\n",
      "Epoch 1633, Loss: 1.210712343454361, Final Batch Loss: 0.45206302404403687\n",
      "Epoch 1634, Loss: 1.227706253528595, Final Batch Loss: 0.3613390624523163\n",
      "Epoch 1635, Loss: 1.2917134165763855, Final Batch Loss: 0.5206049084663391\n",
      "Epoch 1636, Loss: 1.3066693544387817, Final Batch Loss: 0.523348331451416\n",
      "Epoch 1637, Loss: 1.200947493314743, Final Batch Loss: 0.3946802318096161\n",
      "Epoch 1638, Loss: 1.1383641958236694, Final Batch Loss: 0.4283866882324219\n",
      "Epoch 1639, Loss: 1.1783903241157532, Final Batch Loss: 0.389580100774765\n",
      "Epoch 1640, Loss: 1.1956509947776794, Final Batch Loss: 0.40534427762031555\n",
      "Epoch 1641, Loss: 1.2212380170822144, Final Batch Loss: 0.41470757126808167\n",
      "Epoch 1642, Loss: 1.2634522914886475, Final Batch Loss: 0.41139981150627136\n",
      "Epoch 1643, Loss: 1.0897695124149323, Final Batch Loss: 0.2899399995803833\n",
      "Epoch 1644, Loss: 1.239146739244461, Final Batch Loss: 0.4017324149608612\n",
      "Epoch 1645, Loss: 1.2255316972732544, Final Batch Loss: 0.4599166810512543\n",
      "Epoch 1646, Loss: 1.250379502773285, Final Batch Loss: 0.47142258286476135\n",
      "Epoch 1647, Loss: 1.1869617104530334, Final Batch Loss: 0.3739747703075409\n",
      "Epoch 1648, Loss: 1.2082345485687256, Final Batch Loss: 0.40884292125701904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1649, Loss: 1.211243897676468, Final Batch Loss: 0.3086616098880768\n",
      "Epoch 1650, Loss: 1.3681012988090515, Final Batch Loss: 0.5645405054092407\n",
      "Epoch 1651, Loss: 1.2028734385967255, Final Batch Loss: 0.4494418501853943\n",
      "Epoch 1652, Loss: 1.1472530663013458, Final Batch Loss: 0.40226924419403076\n",
      "Epoch 1653, Loss: 1.213865041732788, Final Batch Loss: 0.3400786221027374\n",
      "Epoch 1654, Loss: 1.2155489027500153, Final Batch Loss: 0.353217750787735\n",
      "Epoch 1655, Loss: 1.2556076347827911, Final Batch Loss: 0.3564605116844177\n",
      "Epoch 1656, Loss: 1.1310202181339264, Final Batch Loss: 0.38259512186050415\n",
      "Epoch 1657, Loss: 1.2997530102729797, Final Batch Loss: 0.4648221433162689\n",
      "Epoch 1658, Loss: 1.1851339936256409, Final Batch Loss: 0.34317973256111145\n",
      "Epoch 1659, Loss: 1.1212177872657776, Final Batch Loss: 0.27789953351020813\n",
      "Epoch 1660, Loss: 1.2219352424144745, Final Batch Loss: 0.4195317327976227\n",
      "Epoch 1661, Loss: 1.1889544427394867, Final Batch Loss: 0.4380991756916046\n",
      "Epoch 1662, Loss: 1.1805313229560852, Final Batch Loss: 0.40250056982040405\n",
      "Epoch 1663, Loss: 1.1352427899837494, Final Batch Loss: 0.3712959587574005\n",
      "Epoch 1664, Loss: 1.334898442029953, Final Batch Loss: 0.43814176321029663\n",
      "Epoch 1665, Loss: 1.1293693482875824, Final Batch Loss: 0.34655243158340454\n",
      "Epoch 1666, Loss: 1.2604154348373413, Final Batch Loss: 0.4140751361846924\n",
      "Epoch 1667, Loss: 1.182705968618393, Final Batch Loss: 0.32790443301200867\n",
      "Epoch 1668, Loss: 1.3645429611206055, Final Batch Loss: 0.37731680274009705\n",
      "Epoch 1669, Loss: 1.1551916599273682, Final Batch Loss: 0.30040404200553894\n",
      "Epoch 1670, Loss: 1.1588807106018066, Final Batch Loss: 0.40060490369796753\n",
      "Epoch 1671, Loss: 1.338141292333603, Final Batch Loss: 0.4109669029712677\n",
      "Epoch 1672, Loss: 1.2481037080287933, Final Batch Loss: 0.45415472984313965\n",
      "Epoch 1673, Loss: 1.2291874885559082, Final Batch Loss: 0.3410082161426544\n",
      "Epoch 1674, Loss: 1.2833726108074188, Final Batch Loss: 0.5051827430725098\n",
      "Epoch 1675, Loss: 1.1417484283447266, Final Batch Loss: 0.331260085105896\n",
      "Epoch 1676, Loss: 1.3200345933437347, Final Batch Loss: 0.4799891710281372\n",
      "Epoch 1677, Loss: 1.1409856975078583, Final Batch Loss: 0.42162957787513733\n",
      "Epoch 1678, Loss: 1.2453508079051971, Final Batch Loss: 0.43765750527381897\n",
      "Epoch 1679, Loss: 1.3520601093769073, Final Batch Loss: 0.5958614349365234\n",
      "Epoch 1680, Loss: 1.1699110865592957, Final Batch Loss: 0.38607433438301086\n",
      "Epoch 1681, Loss: 1.2186293005943298, Final Batch Loss: 0.342978835105896\n",
      "Epoch 1682, Loss: 1.1089549362659454, Final Batch Loss: 0.37280014157295227\n",
      "Epoch 1683, Loss: 1.1630067229270935, Final Batch Loss: 0.39046600461006165\n",
      "Epoch 1684, Loss: 1.2149315774440765, Final Batch Loss: 0.35306626558303833\n",
      "Epoch 1685, Loss: 1.0977662205696106, Final Batch Loss: 0.3317829668521881\n",
      "Epoch 1686, Loss: 1.1410294473171234, Final Batch Loss: 0.4117591381072998\n",
      "Epoch 1687, Loss: 1.2711914479732513, Final Batch Loss: 0.479929655790329\n",
      "Epoch 1688, Loss: 1.2170316576957703, Final Batch Loss: 0.42056477069854736\n",
      "Epoch 1689, Loss: 1.2769471406936646, Final Batch Loss: 0.40675088763237\n",
      "Epoch 1690, Loss: 1.1681003868579865, Final Batch Loss: 0.4423772990703583\n",
      "Epoch 1691, Loss: 1.2605109214782715, Final Batch Loss: 0.48484402894973755\n",
      "Epoch 1692, Loss: 1.1576743423938751, Final Batch Loss: 0.37705326080322266\n",
      "Epoch 1693, Loss: 1.1418786644935608, Final Batch Loss: 0.38660991191864014\n",
      "Epoch 1694, Loss: 1.2868588864803314, Final Batch Loss: 0.5152732133865356\n",
      "Epoch 1695, Loss: 1.0677944421768188, Final Batch Loss: 0.3341560661792755\n",
      "Epoch 1696, Loss: 1.1716790199279785, Final Batch Loss: 0.4209020733833313\n",
      "Epoch 1697, Loss: 1.2198667526245117, Final Batch Loss: 0.4082021415233612\n",
      "Epoch 1698, Loss: 1.2313518822193146, Final Batch Loss: 0.40298786759376526\n",
      "Epoch 1699, Loss: 1.1515304148197174, Final Batch Loss: 0.3315117359161377\n",
      "Epoch 1700, Loss: 1.1246886253356934, Final Batch Loss: 0.3303849399089813\n",
      "Epoch 1701, Loss: 1.2715846300125122, Final Batch Loss: 0.457722544670105\n",
      "Epoch 1702, Loss: 1.1639461517333984, Final Batch Loss: 0.38762444257736206\n",
      "Epoch 1703, Loss: 1.231351226568222, Final Batch Loss: 0.3825173079967499\n",
      "Epoch 1704, Loss: 1.2216877043247223, Final Batch Loss: 0.43213117122650146\n",
      "Epoch 1705, Loss: 1.1100201606750488, Final Batch Loss: 0.3577633798122406\n",
      "Epoch 1706, Loss: 1.1654531061649323, Final Batch Loss: 0.3762359321117401\n",
      "Epoch 1707, Loss: 1.284988284111023, Final Batch Loss: 0.4471738934516907\n",
      "Epoch 1708, Loss: 1.2383476197719574, Final Batch Loss: 0.4329601228237152\n",
      "Epoch 1709, Loss: 1.1126458644866943, Final Batch Loss: 0.3402583599090576\n",
      "Epoch 1710, Loss: 1.0986788868904114, Final Batch Loss: 0.2858521640300751\n",
      "Epoch 1711, Loss: 1.183093547821045, Final Batch Loss: 0.4959936738014221\n",
      "Epoch 1712, Loss: 1.2821914553642273, Final Batch Loss: 0.49181675910949707\n",
      "Epoch 1713, Loss: 1.1540573835372925, Final Batch Loss: 0.35923513770103455\n",
      "Epoch 1714, Loss: 1.1640583276748657, Final Batch Loss: 0.4062780439853668\n",
      "Epoch 1715, Loss: 1.2635490894317627, Final Batch Loss: 0.45221173763275146\n",
      "Epoch 1716, Loss: 1.090192049741745, Final Batch Loss: 0.3160225450992584\n",
      "Epoch 1717, Loss: 1.2475488185882568, Final Batch Loss: 0.4983263909816742\n",
      "Epoch 1718, Loss: 1.2059528827667236, Final Batch Loss: 0.37462878227233887\n",
      "Epoch 1719, Loss: 1.1803154051303864, Final Batch Loss: 0.4624774158000946\n",
      "Epoch 1720, Loss: 1.2055899500846863, Final Batch Loss: 0.3666030764579773\n",
      "Epoch 1721, Loss: 1.1422804296016693, Final Batch Loss: 0.3922804892063141\n",
      "Epoch 1722, Loss: 1.1724790334701538, Final Batch Loss: 0.36469000577926636\n",
      "Epoch 1723, Loss: 1.1490522027015686, Final Batch Loss: 0.35216084122657776\n",
      "Epoch 1724, Loss: 1.198876827955246, Final Batch Loss: 0.3580929934978485\n",
      "Epoch 1725, Loss: 1.0699698626995087, Final Batch Loss: 0.2899031937122345\n",
      "Epoch 1726, Loss: 1.1267279088497162, Final Batch Loss: 0.33437904715538025\n",
      "Epoch 1727, Loss: 1.126945436000824, Final Batch Loss: 0.34553763270378113\n",
      "Epoch 1728, Loss: 1.0929076373577118, Final Batch Loss: 0.35794785618782043\n",
      "Epoch 1729, Loss: 1.1985535025596619, Final Batch Loss: 0.489886075258255\n",
      "Epoch 1730, Loss: 1.1831726431846619, Final Batch Loss: 0.38786453008651733\n",
      "Epoch 1731, Loss: 1.2867404222488403, Final Batch Loss: 0.5094959735870361\n",
      "Epoch 1732, Loss: 1.0750944912433624, Final Batch Loss: 0.3797624409198761\n",
      "Epoch 1733, Loss: 1.094009131193161, Final Batch Loss: 0.35687944293022156\n",
      "Epoch 1734, Loss: 1.1922925412654877, Final Batch Loss: 0.34593138098716736\n",
      "Epoch 1735, Loss: 1.1775345504283905, Final Batch Loss: 0.33483925461769104\n",
      "Epoch 1736, Loss: 1.1699604392051697, Final Batch Loss: 0.40112054347991943\n",
      "Epoch 1737, Loss: 1.1498408317565918, Final Batch Loss: 0.3717470169067383\n",
      "Epoch 1738, Loss: 1.1758383512496948, Final Batch Loss: 0.3991737961769104\n",
      "Epoch 1739, Loss: 1.2878016531467438, Final Batch Loss: 0.5982381701469421\n",
      "Epoch 1740, Loss: 1.1907564103603363, Final Batch Loss: 0.46045300364494324\n",
      "Epoch 1741, Loss: 1.1754942536354065, Final Batch Loss: 0.39703160524368286\n",
      "Epoch 1742, Loss: 1.083402931690216, Final Batch Loss: 0.368656724691391\n",
      "Epoch 1743, Loss: 1.1082748770713806, Final Batch Loss: 0.4089350998401642\n",
      "Epoch 1744, Loss: 1.158706247806549, Final Batch Loss: 0.4010165333747864\n",
      "Epoch 1745, Loss: 1.127768874168396, Final Batch Loss: 0.3648507297039032\n",
      "Epoch 1746, Loss: 1.0657716393470764, Final Batch Loss: 0.3052999973297119\n",
      "Epoch 1747, Loss: 1.1340067684650421, Final Batch Loss: 0.3654263913631439\n",
      "Epoch 1748, Loss: 1.2990179061889648, Final Batch Loss: 0.5182202458381653\n",
      "Epoch 1749, Loss: 1.2532064616680145, Final Batch Loss: 0.4181045591831207\n",
      "Epoch 1750, Loss: 1.205452710390091, Final Batch Loss: 0.4151318669319153\n",
      "Epoch 1751, Loss: 1.1284865140914917, Final Batch Loss: 0.36366263031959534\n",
      "Epoch 1752, Loss: 1.1360812187194824, Final Batch Loss: 0.34410786628723145\n",
      "Epoch 1753, Loss: 1.1950834393501282, Final Batch Loss: 0.480998694896698\n",
      "Epoch 1754, Loss: 1.1907480359077454, Final Batch Loss: 0.4253007471561432\n",
      "Epoch 1755, Loss: 1.2256238460540771, Final Batch Loss: 0.4031648635864258\n",
      "Epoch 1756, Loss: 1.0848570764064789, Final Batch Loss: 0.40121161937713623\n",
      "Epoch 1757, Loss: 1.1953732073307037, Final Batch Loss: 0.46877825260162354\n",
      "Epoch 1758, Loss: 1.2228902876377106, Final Batch Loss: 0.43643006682395935\n",
      "Epoch 1759, Loss: 1.1573845744132996, Final Batch Loss: 0.3985067903995514\n",
      "Epoch 1760, Loss: 1.0691737234592438, Final Batch Loss: 0.37153878808021545\n",
      "Epoch 1761, Loss: 1.152590572834015, Final Batch Loss: 0.39584091305732727\n",
      "Epoch 1762, Loss: 1.222578763961792, Final Batch Loss: 0.4017466604709625\n",
      "Epoch 1763, Loss: 1.2395333051681519, Final Batch Loss: 0.41074100136756897\n",
      "Epoch 1764, Loss: 1.1796901226043701, Final Batch Loss: 0.35298413038253784\n",
      "Epoch 1765, Loss: 1.104188323020935, Final Batch Loss: 0.3518517315387726\n",
      "Epoch 1766, Loss: 1.1687518954277039, Final Batch Loss: 0.3830840289592743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1767, Loss: 1.1437179744243622, Final Batch Loss: 0.41822192072868347\n",
      "Epoch 1768, Loss: 1.2624563574790955, Final Batch Loss: 0.4424510896205902\n",
      "Epoch 1769, Loss: 1.116201013326645, Final Batch Loss: 0.330148845911026\n",
      "Epoch 1770, Loss: 1.216859519481659, Final Batch Loss: 0.38929614424705505\n",
      "Epoch 1771, Loss: 1.2657583951950073, Final Batch Loss: 0.44769832491874695\n",
      "Epoch 1772, Loss: 1.1817426085472107, Final Batch Loss: 0.4769929349422455\n",
      "Epoch 1773, Loss: 1.098135083913803, Final Batch Loss: 0.3970405161380768\n",
      "Epoch 1774, Loss: 1.021153211593628, Final Batch Loss: 0.2764480710029602\n",
      "Epoch 1775, Loss: 1.1856994330883026, Final Batch Loss: 0.3844611346721649\n",
      "Epoch 1776, Loss: 1.1405691802501678, Final Batch Loss: 0.396671861410141\n",
      "Epoch 1777, Loss: 1.2697237432003021, Final Batch Loss: 0.39711278676986694\n",
      "Epoch 1778, Loss: 1.1380980908870697, Final Batch Loss: 0.300888329744339\n",
      "Epoch 1779, Loss: 1.2414356768131256, Final Batch Loss: 0.45290857553482056\n",
      "Epoch 1780, Loss: 1.1463198959827423, Final Batch Loss: 0.3874248266220093\n",
      "Epoch 1781, Loss: 1.1215179860591888, Final Batch Loss: 0.29758965969085693\n",
      "Epoch 1782, Loss: 1.0676839053630829, Final Batch Loss: 0.3202076852321625\n",
      "Epoch 1783, Loss: 1.098448783159256, Final Batch Loss: 0.330618292093277\n",
      "Epoch 1784, Loss: 1.1443526446819305, Final Batch Loss: 0.45461374521255493\n",
      "Epoch 1785, Loss: 1.2068266570568085, Final Batch Loss: 0.4417220652103424\n",
      "Epoch 1786, Loss: 1.0960920751094818, Final Batch Loss: 0.36872586607933044\n",
      "Epoch 1787, Loss: 1.1161211729049683, Final Batch Loss: 0.3833581805229187\n",
      "Epoch 1788, Loss: 1.2809455692768097, Final Batch Loss: 0.5281860828399658\n",
      "Epoch 1789, Loss: 1.2896493673324585, Final Batch Loss: 0.46711140871047974\n",
      "Epoch 1790, Loss: 1.2656174302101135, Final Batch Loss: 0.463380366563797\n",
      "Epoch 1791, Loss: 1.1156490743160248, Final Batch Loss: 0.3935457170009613\n",
      "Epoch 1792, Loss: 1.2138477861881256, Final Batch Loss: 0.4596157670021057\n",
      "Epoch 1793, Loss: 1.1675229370594025, Final Batch Loss: 0.4248657822608948\n",
      "Epoch 1794, Loss: 1.1325199007987976, Final Batch Loss: 0.4065436124801636\n",
      "Epoch 1795, Loss: 1.003449261188507, Final Batch Loss: 0.2764013111591339\n",
      "Epoch 1796, Loss: 1.1774373054504395, Final Batch Loss: 0.3278791308403015\n",
      "Epoch 1797, Loss: 1.140731304883957, Final Batch Loss: 0.3907496929168701\n",
      "Epoch 1798, Loss: 1.150113970041275, Final Batch Loss: 0.3446309566497803\n",
      "Epoch 1799, Loss: 1.118247777223587, Final Batch Loss: 0.3035575747489929\n",
      "Epoch 1800, Loss: 1.159326583147049, Final Batch Loss: 0.35710251331329346\n",
      "Epoch 1801, Loss: 1.1253544986248016, Final Batch Loss: 0.3717554807662964\n",
      "Epoch 1802, Loss: 1.1736222505569458, Final Batch Loss: 0.319757342338562\n",
      "Epoch 1803, Loss: 1.1678882539272308, Final Batch Loss: 0.44737353920936584\n",
      "Epoch 1804, Loss: 1.163351446390152, Final Batch Loss: 0.3499666750431061\n",
      "Epoch 1805, Loss: 1.0963448584079742, Final Batch Loss: 0.3926950991153717\n",
      "Epoch 1806, Loss: 1.1203624606132507, Final Batch Loss: 0.3527749478816986\n",
      "Epoch 1807, Loss: 1.2516728937625885, Final Batch Loss: 0.476688414812088\n",
      "Epoch 1808, Loss: 1.1106805205345154, Final Batch Loss: 0.4064529836177826\n",
      "Epoch 1809, Loss: 1.109320729970932, Final Batch Loss: 0.3433062434196472\n",
      "Epoch 1810, Loss: 1.1141175329685211, Final Batch Loss: 0.3370889127254486\n",
      "Epoch 1811, Loss: 1.3599406480789185, Final Batch Loss: 0.5526981353759766\n",
      "Epoch 1812, Loss: 1.1301347315311432, Final Batch Loss: 0.32375162839889526\n",
      "Epoch 1813, Loss: 1.0996871888637543, Final Batch Loss: 0.3620697557926178\n",
      "Epoch 1814, Loss: 1.1104528605937958, Final Batch Loss: 0.3444681167602539\n",
      "Epoch 1815, Loss: 1.2195447385311127, Final Batch Loss: 0.49957168102264404\n",
      "Epoch 1816, Loss: 1.1698694825172424, Final Batch Loss: 0.4619463384151459\n",
      "Epoch 1817, Loss: 1.1042315661907196, Final Batch Loss: 0.37918442487716675\n",
      "Epoch 1818, Loss: 1.2145322263240814, Final Batch Loss: 0.3989831507205963\n",
      "Epoch 1819, Loss: 1.1552261114120483, Final Batch Loss: 0.3915911018848419\n",
      "Epoch 1820, Loss: 1.0538049638271332, Final Batch Loss: 0.28960341215133667\n",
      "Epoch 1821, Loss: 1.1136514544487, Final Batch Loss: 0.36669984459877014\n",
      "Epoch 1822, Loss: 1.1526189744472504, Final Batch Loss: 0.3864612579345703\n",
      "Epoch 1823, Loss: 1.0688462853431702, Final Batch Loss: 0.3168898820877075\n",
      "Epoch 1824, Loss: 1.1982181072235107, Final Batch Loss: 0.4386693239212036\n",
      "Epoch 1825, Loss: 1.234292209148407, Final Batch Loss: 0.3964555859565735\n",
      "Epoch 1826, Loss: 1.0815632939338684, Final Batch Loss: 0.3646570146083832\n",
      "Epoch 1827, Loss: 1.0757415890693665, Final Batch Loss: 0.3251469135284424\n",
      "Epoch 1828, Loss: 1.1610041558742523, Final Batch Loss: 0.27237918972969055\n",
      "Epoch 1829, Loss: 1.1297134160995483, Final Batch Loss: 0.3799302279949188\n",
      "Epoch 1830, Loss: 1.1207463443279266, Final Batch Loss: 0.38441890478134155\n",
      "Epoch 1831, Loss: 1.1307368576526642, Final Batch Loss: 0.39227360486984253\n",
      "Epoch 1832, Loss: 1.1162457168102264, Final Batch Loss: 0.44319531321525574\n",
      "Epoch 1833, Loss: 1.0944378674030304, Final Batch Loss: 0.2558237314224243\n",
      "Epoch 1834, Loss: 1.1870260834693909, Final Batch Loss: 0.3609793782234192\n",
      "Epoch 1835, Loss: 1.109914392232895, Final Batch Loss: 0.41458800435066223\n",
      "Epoch 1836, Loss: 1.185907930135727, Final Batch Loss: 0.361928790807724\n",
      "Epoch 1837, Loss: 1.1158596575260162, Final Batch Loss: 0.35471707582473755\n",
      "Epoch 1838, Loss: 1.0716632008552551, Final Batch Loss: 0.3801422119140625\n",
      "Epoch 1839, Loss: 1.1387757062911987, Final Batch Loss: 0.39199239015579224\n",
      "Epoch 1840, Loss: 1.0748872756958008, Final Batch Loss: 0.3176550269126892\n",
      "Epoch 1841, Loss: 1.3576804399490356, Final Batch Loss: 0.4827802777290344\n",
      "Epoch 1842, Loss: 1.0507945120334625, Final Batch Loss: 0.3277576267719269\n",
      "Epoch 1843, Loss: 1.0606342256069183, Final Batch Loss: 0.31142503023147583\n",
      "Epoch 1844, Loss: 1.2220093607902527, Final Batch Loss: 0.5370654463768005\n",
      "Epoch 1845, Loss: 1.2454071044921875, Final Batch Loss: 0.4111003875732422\n",
      "Epoch 1846, Loss: 1.1523326337337494, Final Batch Loss: 0.3982648253440857\n",
      "Epoch 1847, Loss: 1.1363599598407745, Final Batch Loss: 0.42412707209587097\n",
      "Epoch 1848, Loss: 1.1017566919326782, Final Batch Loss: 0.3781837224960327\n",
      "Epoch 1849, Loss: 1.1301957964897156, Final Batch Loss: 0.3947599530220032\n",
      "Epoch 1850, Loss: 1.3136747479438782, Final Batch Loss: 0.38436323404312134\n",
      "Epoch 1851, Loss: 1.1606092751026154, Final Batch Loss: 0.38598984479904175\n",
      "Epoch 1852, Loss: 1.0876941978931427, Final Batch Loss: 0.3862525224685669\n",
      "Epoch 1853, Loss: 1.1417579352855682, Final Batch Loss: 0.39961105585098267\n",
      "Epoch 1854, Loss: 1.1607923209667206, Final Batch Loss: 0.37231647968292236\n",
      "Epoch 1855, Loss: 1.161792814731598, Final Batch Loss: 0.42683327198028564\n",
      "Epoch 1856, Loss: 1.1279350519180298, Final Batch Loss: 0.35897189378738403\n",
      "Epoch 1857, Loss: 1.0747014582157135, Final Batch Loss: 0.33617106080055237\n",
      "Epoch 1858, Loss: 1.2548169195652008, Final Batch Loss: 0.3927934169769287\n",
      "Epoch 1859, Loss: 1.108540952205658, Final Batch Loss: 0.4118029773235321\n",
      "Epoch 1860, Loss: 1.2106852233409882, Final Batch Loss: 0.4079224765300751\n",
      "Epoch 1861, Loss: 1.193493902683258, Final Batch Loss: 0.42259958386421204\n",
      "Epoch 1862, Loss: 1.1369174122810364, Final Batch Loss: 0.4137623906135559\n",
      "Epoch 1863, Loss: 1.1111444532871246, Final Batch Loss: 0.3328181207180023\n",
      "Epoch 1864, Loss: 1.141175091266632, Final Batch Loss: 0.3401283919811249\n",
      "Epoch 1865, Loss: 1.0800641179084778, Final Batch Loss: 0.3209482431411743\n",
      "Epoch 1866, Loss: 1.089434564113617, Final Batch Loss: 0.3470216393470764\n",
      "Epoch 1867, Loss: 1.2319342195987701, Final Batch Loss: 0.43044808506965637\n",
      "Epoch 1868, Loss: 1.147879421710968, Final Batch Loss: 0.3731948137283325\n",
      "Epoch 1869, Loss: 1.2023165225982666, Final Batch Loss: 0.38910242915153503\n",
      "Epoch 1870, Loss: 1.1948295533657074, Final Batch Loss: 0.48133862018585205\n",
      "Epoch 1871, Loss: 1.1061369478702545, Final Batch Loss: 0.365867555141449\n",
      "Epoch 1872, Loss: 1.0280736088752747, Final Batch Loss: 0.3385612964630127\n",
      "Epoch 1873, Loss: 1.0719814598560333, Final Batch Loss: 0.3484410345554352\n",
      "Epoch 1874, Loss: 1.1710367798805237, Final Batch Loss: 0.4723135232925415\n",
      "Epoch 1875, Loss: 1.1678458750247955, Final Batch Loss: 0.3947197496891022\n",
      "Epoch 1876, Loss: 1.1449461579322815, Final Batch Loss: 0.29176080226898193\n",
      "Epoch 1877, Loss: 1.0660772323608398, Final Batch Loss: 0.2965294420719147\n",
      "Epoch 1878, Loss: 1.1334658563137054, Final Batch Loss: 0.4101443588733673\n",
      "Epoch 1879, Loss: 1.1297984719276428, Final Batch Loss: 0.3924882709980011\n",
      "Epoch 1880, Loss: 1.2134817838668823, Final Batch Loss: 0.44032493233680725\n",
      "Epoch 1881, Loss: 1.096619427204132, Final Batch Loss: 0.3323131799697876\n",
      "Epoch 1882, Loss: 1.2092642188072205, Final Batch Loss: 0.371171772480011\n",
      "Epoch 1883, Loss: 1.0665421783924103, Final Batch Loss: 0.3277672827243805\n",
      "Epoch 1884, Loss: 1.0810907185077667, Final Batch Loss: 0.28658679127693176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1885, Loss: 1.0592540502548218, Final Batch Loss: 0.28632986545562744\n",
      "Epoch 1886, Loss: 1.1544395089149475, Final Batch Loss: 0.38756120204925537\n",
      "Epoch 1887, Loss: 1.2193259596824646, Final Batch Loss: 0.39335909485816956\n",
      "Epoch 1888, Loss: 1.083598554134369, Final Batch Loss: 0.30344346165657043\n",
      "Epoch 1889, Loss: 1.1139287948608398, Final Batch Loss: 0.37621697783470154\n",
      "Epoch 1890, Loss: 1.0231327414512634, Final Batch Loss: 0.35741162300109863\n",
      "Epoch 1891, Loss: 1.018223911523819, Final Batch Loss: 0.32289382815361023\n",
      "Epoch 1892, Loss: 1.1086686551570892, Final Batch Loss: 0.4276590347290039\n",
      "Epoch 1893, Loss: 1.1338678896427155, Final Batch Loss: 0.35366854071617126\n",
      "Epoch 1894, Loss: 1.175850659608841, Final Batch Loss: 0.39436373114585876\n",
      "Epoch 1895, Loss: 1.0848303437232971, Final Batch Loss: 0.3254804313182831\n",
      "Epoch 1896, Loss: 1.090873122215271, Final Batch Loss: 0.30502671003341675\n",
      "Epoch 1897, Loss: 1.0933472514152527, Final Batch Loss: 0.35276368260383606\n",
      "Epoch 1898, Loss: 1.0938761234283447, Final Batch Loss: 0.44521141052246094\n",
      "Epoch 1899, Loss: 1.1092745661735535, Final Batch Loss: 0.30210810899734497\n",
      "Epoch 1900, Loss: 1.0164074897766113, Final Batch Loss: 0.34277382493019104\n",
      "Epoch 1901, Loss: 1.0701920688152313, Final Batch Loss: 0.32312852144241333\n",
      "Epoch 1902, Loss: 1.2072080969810486, Final Batch Loss: 0.49722185730934143\n",
      "Epoch 1903, Loss: 1.0444878041744232, Final Batch Loss: 0.36213773488998413\n",
      "Epoch 1904, Loss: 1.2593415975570679, Final Batch Loss: 0.39407193660736084\n",
      "Epoch 1905, Loss: 1.2757372856140137, Final Batch Loss: 0.34910884499549866\n",
      "Epoch 1906, Loss: 1.1315333247184753, Final Batch Loss: 0.38521552085876465\n",
      "Epoch 1907, Loss: 1.112419068813324, Final Batch Loss: 0.35724571347236633\n",
      "Epoch 1908, Loss: 1.168687105178833, Final Batch Loss: 0.424550861120224\n",
      "Epoch 1909, Loss: 1.1667608618736267, Final Batch Loss: 0.43030163645744324\n",
      "Epoch 1910, Loss: 1.1691365838050842, Final Batch Loss: 0.4155673086643219\n",
      "Epoch 1911, Loss: 1.0873131453990936, Final Batch Loss: 0.4129856526851654\n",
      "Epoch 1912, Loss: 1.2172956764698029, Final Batch Loss: 0.44927796721458435\n",
      "Epoch 1913, Loss: 1.1860974729061127, Final Batch Loss: 0.47735223174095154\n",
      "Epoch 1914, Loss: 1.0474905371665955, Final Batch Loss: 0.3543164134025574\n",
      "Epoch 1915, Loss: 1.1053711771965027, Final Batch Loss: 0.3051067292690277\n",
      "Epoch 1916, Loss: 1.248731404542923, Final Batch Loss: 0.49988678097724915\n",
      "Epoch 1917, Loss: 1.2957881093025208, Final Batch Loss: 0.5187476277351379\n",
      "Epoch 1918, Loss: 1.1372089982032776, Final Batch Loss: 0.32491493225097656\n",
      "Epoch 1919, Loss: 1.1324895322322845, Final Batch Loss: 0.42690035700798035\n",
      "Epoch 1920, Loss: 1.1214661002159119, Final Batch Loss: 0.37280508875846863\n",
      "Epoch 1921, Loss: 1.080449640750885, Final Batch Loss: 0.3262859582901001\n",
      "Epoch 1922, Loss: 1.063119262456894, Final Batch Loss: 0.371662974357605\n",
      "Epoch 1923, Loss: 1.285652995109558, Final Batch Loss: 0.45544829964637756\n",
      "Epoch 1924, Loss: 1.0515462458133698, Final Batch Loss: 0.3167611062526703\n",
      "Epoch 1925, Loss: 1.1066385507583618, Final Batch Loss: 0.393464595079422\n",
      "Epoch 1926, Loss: 1.1376293301582336, Final Batch Loss: 0.37851616740226746\n",
      "Epoch 1927, Loss: 0.9971987307071686, Final Batch Loss: 0.2719307541847229\n",
      "Epoch 1928, Loss: 1.0948207080364227, Final Batch Loss: 0.3840632140636444\n",
      "Epoch 1929, Loss: 1.2333272695541382, Final Batch Loss: 0.5378352999687195\n",
      "Epoch 1930, Loss: 1.090493768453598, Final Batch Loss: 0.33902791142463684\n",
      "Epoch 1931, Loss: 1.0562682747840881, Final Batch Loss: 0.2634008526802063\n",
      "Epoch 1932, Loss: 1.1555282473564148, Final Batch Loss: 0.48171138763427734\n",
      "Epoch 1933, Loss: 1.1275840401649475, Final Batch Loss: 0.4146019518375397\n",
      "Epoch 1934, Loss: 1.0228961110115051, Final Batch Loss: 0.33197131752967834\n",
      "Epoch 1935, Loss: 1.1039310693740845, Final Batch Loss: 0.35014480352401733\n",
      "Epoch 1936, Loss: 1.181739181280136, Final Batch Loss: 0.41313403844833374\n",
      "Epoch 1937, Loss: 1.1076036989688873, Final Batch Loss: 0.3934532105922699\n",
      "Epoch 1938, Loss: 1.0957890450954437, Final Batch Loss: 0.36411359906196594\n",
      "Epoch 1939, Loss: 1.1556563973426819, Final Batch Loss: 0.47644373774528503\n",
      "Epoch 1940, Loss: 1.205582469701767, Final Batch Loss: 0.4673555791378021\n",
      "Epoch 1941, Loss: 1.053526759147644, Final Batch Loss: 0.37578704953193665\n",
      "Epoch 1942, Loss: 1.063505232334137, Final Batch Loss: 0.3387022614479065\n",
      "Epoch 1943, Loss: 1.0563953220844269, Final Batch Loss: 0.3733848035335541\n",
      "Epoch 1944, Loss: 1.0432895421981812, Final Batch Loss: 0.3388616740703583\n",
      "Epoch 1945, Loss: 1.1989020109176636, Final Batch Loss: 0.4290562868118286\n",
      "Epoch 1946, Loss: 1.1366183161735535, Final Batch Loss: 0.38569796085357666\n",
      "Epoch 1947, Loss: 1.2961814403533936, Final Batch Loss: 0.41761982440948486\n",
      "Epoch 1948, Loss: 1.1703259646892548, Final Batch Loss: 0.3883894681930542\n",
      "Epoch 1949, Loss: 1.0778817534446716, Final Batch Loss: 0.31565529108047485\n",
      "Epoch 1950, Loss: 1.0841769874095917, Final Batch Loss: 0.38332024216651917\n",
      "Epoch 1951, Loss: 1.0744063556194305, Final Batch Loss: 0.322556734085083\n",
      "Epoch 1952, Loss: 1.077469676733017, Final Batch Loss: 0.37665823101997375\n",
      "Epoch 1953, Loss: 1.1505487561225891, Final Batch Loss: 0.4330790042877197\n",
      "Epoch 1954, Loss: 1.1179291307926178, Final Batch Loss: 0.2982470989227295\n",
      "Epoch 1955, Loss: 1.135094314813614, Final Batch Loss: 0.3323667049407959\n",
      "Epoch 1956, Loss: 0.9796101897954941, Final Batch Loss: 0.23370717465877533\n",
      "Epoch 1957, Loss: 0.9884203970432281, Final Batch Loss: 0.26666346192359924\n",
      "Epoch 1958, Loss: 1.0935292840003967, Final Batch Loss: 0.33387860655784607\n",
      "Epoch 1959, Loss: 1.142446219921112, Final Batch Loss: 0.38408806920051575\n",
      "Epoch 1960, Loss: 1.085716336965561, Final Batch Loss: 0.307224839925766\n",
      "Epoch 1961, Loss: 1.2172430157661438, Final Batch Loss: 0.46061185002326965\n",
      "Epoch 1962, Loss: 1.0690967738628387, Final Batch Loss: 0.3055229187011719\n",
      "Epoch 1963, Loss: 1.0654903650283813, Final Batch Loss: 0.35910913348197937\n",
      "Epoch 1964, Loss: 1.2081990838050842, Final Batch Loss: 0.4060520529747009\n",
      "Epoch 1965, Loss: 1.110997349023819, Final Batch Loss: 0.301207959651947\n",
      "Epoch 1966, Loss: 1.1151348054409027, Final Batch Loss: 0.3552371561527252\n",
      "Epoch 1967, Loss: 1.1978468596935272, Final Batch Loss: 0.46411654353141785\n",
      "Epoch 1968, Loss: 1.1005226969718933, Final Batch Loss: 0.3722211718559265\n",
      "Epoch 1969, Loss: 1.1376994848251343, Final Batch Loss: 0.4483623206615448\n",
      "Epoch 1970, Loss: 1.1323321759700775, Final Batch Loss: 0.42446520924568176\n",
      "Epoch 1971, Loss: 1.1018518507480621, Final Batch Loss: 0.31645286083221436\n",
      "Epoch 1972, Loss: 1.0689380764961243, Final Batch Loss: 0.3398919999599457\n",
      "Epoch 1973, Loss: 1.1512496769428253, Final Batch Loss: 0.4184749722480774\n",
      "Epoch 1974, Loss: 1.0493026673793793, Final Batch Loss: 0.2823837995529175\n",
      "Epoch 1975, Loss: 1.1645142138004303, Final Batch Loss: 0.28483349084854126\n",
      "Epoch 1976, Loss: 1.153572291135788, Final Batch Loss: 0.3930321931838989\n",
      "Epoch 1977, Loss: 1.0950589179992676, Final Batch Loss: 0.36135807633399963\n",
      "Epoch 1978, Loss: 1.0232978761196136, Final Batch Loss: 0.33660975098609924\n",
      "Epoch 1979, Loss: 1.2075757682323456, Final Batch Loss: 0.45810675621032715\n",
      "Epoch 1980, Loss: 1.0875117778778076, Final Batch Loss: 0.30784159898757935\n",
      "Epoch 1981, Loss: 1.110016644001007, Final Batch Loss: 0.3070486783981323\n",
      "Epoch 1982, Loss: 1.0568823218345642, Final Batch Loss: 0.3299180567264557\n",
      "Epoch 1983, Loss: 1.2185745537281036, Final Batch Loss: 0.34912052750587463\n",
      "Epoch 1984, Loss: 1.1361340880393982, Final Batch Loss: 0.42594292759895325\n",
      "Epoch 1985, Loss: 1.0871350318193436, Final Batch Loss: 0.24967961013317108\n",
      "Epoch 1986, Loss: 1.1962963044643402, Final Batch Loss: 0.49643516540527344\n",
      "Epoch 1987, Loss: 1.1979859471321106, Final Batch Loss: 0.4358367323875427\n",
      "Epoch 1988, Loss: 1.1388571560382843, Final Batch Loss: 0.37268370389938354\n",
      "Epoch 1989, Loss: 0.977653831243515, Final Batch Loss: 0.20634859800338745\n",
      "Epoch 1990, Loss: 1.0294547379016876, Final Batch Loss: 0.28073498606681824\n",
      "Epoch 1991, Loss: 1.13900887966156, Final Batch Loss: 0.3534497320652008\n",
      "Epoch 1992, Loss: 1.1656634211540222, Final Batch Loss: 0.42524945735931396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1993, Loss: 1.2777714133262634, Final Batch Loss: 0.49114876985549927\n",
      "Epoch 1994, Loss: 1.0249994397163391, Final Batch Loss: 0.2671775221824646\n",
      "Epoch 1995, Loss: 1.1469659805297852, Final Batch Loss: 0.2694762647151947\n",
      "Epoch 1996, Loss: 1.183758944272995, Final Batch Loss: 0.42349758744239807\n",
      "Epoch 1997, Loss: 1.1256577670574188, Final Batch Loss: 0.4025353491306305\n",
      "Epoch 1998, Loss: 1.0905870497226715, Final Batch Loss: 0.40798020362854004\n",
      "Epoch 1999, Loss: 1.0373539328575134, Final Batch Loss: 0.29292652010917664\n",
      "Epoch 2000, Loss: 1.1372123658657074, Final Batch Loss: 0.42664095759391785\n",
      "Epoch 2001, Loss: 1.180950790643692, Final Batch Loss: 0.44450223445892334\n",
      "Epoch 2002, Loss: 1.0223223865032196, Final Batch Loss: 0.36232802271842957\n",
      "Epoch 2003, Loss: 1.0781414806842804, Final Batch Loss: 0.3267901539802551\n",
      "Epoch 2004, Loss: 1.0873871147632599, Final Batch Loss: 0.36965975165367126\n",
      "Epoch 2005, Loss: 1.1795975863933563, Final Batch Loss: 0.4771937429904938\n",
      "Epoch 2006, Loss: 1.1846871674060822, Final Batch Loss: 0.4581685960292816\n",
      "Epoch 2007, Loss: 1.1203860938549042, Final Batch Loss: 0.3416510224342346\n",
      "Epoch 2008, Loss: 0.9796862006187439, Final Batch Loss: 0.2763241231441498\n",
      "Epoch 2009, Loss: 1.1815865337848663, Final Batch Loss: 0.41555413603782654\n",
      "Epoch 2010, Loss: 1.220426231622696, Final Batch Loss: 0.43585383892059326\n",
      "Epoch 2011, Loss: 1.0054944455623627, Final Batch Loss: 0.29549935460090637\n",
      "Epoch 2012, Loss: 1.2202454209327698, Final Batch Loss: 0.43101197481155396\n",
      "Epoch 2013, Loss: 1.063811868429184, Final Batch Loss: 0.3462126553058624\n",
      "Epoch 2014, Loss: 1.0096441805362701, Final Batch Loss: 0.2923475503921509\n",
      "Epoch 2015, Loss: 1.1086429357528687, Final Batch Loss: 0.33975768089294434\n",
      "Epoch 2016, Loss: 1.021098405122757, Final Batch Loss: 0.25238490104675293\n",
      "Epoch 2017, Loss: 1.1366951763629913, Final Batch Loss: 0.4142550826072693\n",
      "Epoch 2018, Loss: 1.0890182554721832, Final Batch Loss: 0.3197537064552307\n",
      "Epoch 2019, Loss: 1.0020660161972046, Final Batch Loss: 0.3032492399215698\n",
      "Epoch 2020, Loss: 1.1272659301757812, Final Batch Loss: 0.35518115758895874\n",
      "Epoch 2021, Loss: 1.0699745118618011, Final Batch Loss: 0.34395018219947815\n",
      "Epoch 2022, Loss: 1.147080510854721, Final Batch Loss: 0.4715309739112854\n",
      "Epoch 2023, Loss: 1.0467323064804077, Final Batch Loss: 0.4006551206111908\n",
      "Epoch 2024, Loss: 1.1065336763858795, Final Batch Loss: 0.38349801301956177\n",
      "Epoch 2025, Loss: 1.1072932481765747, Final Batch Loss: 0.38095924258232117\n",
      "Epoch 2026, Loss: 1.190980464220047, Final Batch Loss: 0.519237756729126\n",
      "Epoch 2027, Loss: 1.0146421790122986, Final Batch Loss: 0.32296252250671387\n",
      "Epoch 2028, Loss: 1.0332920849323273, Final Batch Loss: 0.34421005845069885\n",
      "Epoch 2029, Loss: 1.1228805780410767, Final Batch Loss: 0.4331967830657959\n",
      "Epoch 2030, Loss: 1.1420597732067108, Final Batch Loss: 0.4381069540977478\n",
      "Epoch 2031, Loss: 1.2015747427940369, Final Batch Loss: 0.3564409613609314\n",
      "Epoch 2032, Loss: 1.040041297674179, Final Batch Loss: 0.2824961841106415\n",
      "Epoch 2033, Loss: 1.1194669306278229, Final Batch Loss: 0.32766857743263245\n",
      "Epoch 2034, Loss: 1.0835621058940887, Final Batch Loss: 0.35300976037979126\n",
      "Epoch 2035, Loss: 1.183475375175476, Final Batch Loss: 0.49859127402305603\n",
      "Epoch 2036, Loss: 1.1041805744171143, Final Batch Loss: 0.3596457540988922\n",
      "Epoch 2037, Loss: 1.1600027978420258, Final Batch Loss: 0.4633994698524475\n",
      "Epoch 2038, Loss: 1.1698015928268433, Final Batch Loss: 0.4233362078666687\n",
      "Epoch 2039, Loss: 1.1292093098163605, Final Batch Loss: 0.35639891028404236\n",
      "Epoch 2040, Loss: 1.121503084897995, Final Batch Loss: 0.3923478424549103\n",
      "Epoch 2041, Loss: 1.0096765458583832, Final Batch Loss: 0.25432103872299194\n",
      "Epoch 2042, Loss: 1.0624179244041443, Final Batch Loss: 0.35647326707839966\n",
      "Epoch 2043, Loss: 1.0912830829620361, Final Batch Loss: 0.40329545736312866\n",
      "Epoch 2044, Loss: 1.1913021206855774, Final Batch Loss: 0.486305832862854\n",
      "Epoch 2045, Loss: 1.1496333181858063, Final Batch Loss: 0.4658412039279938\n",
      "Epoch 2046, Loss: 1.1088904738426208, Final Batch Loss: 0.3911103308200836\n",
      "Epoch 2047, Loss: 1.1650639772415161, Final Batch Loss: 0.42572689056396484\n",
      "Epoch 2048, Loss: 1.054689884185791, Final Batch Loss: 0.3796005845069885\n",
      "Epoch 2049, Loss: 1.2119194269180298, Final Batch Loss: 0.5413411855697632\n",
      "Epoch 2050, Loss: 1.1750850081443787, Final Batch Loss: 0.43836256861686707\n",
      "Epoch 2051, Loss: 1.1361485421657562, Final Batch Loss: 0.41103503108024597\n",
      "Epoch 2052, Loss: 1.0690028667449951, Final Batch Loss: 0.35806548595428467\n",
      "Epoch 2053, Loss: 1.216382473707199, Final Batch Loss: 0.4704253375530243\n",
      "Epoch 2054, Loss: 1.0744861364364624, Final Batch Loss: 0.3522818982601166\n",
      "Epoch 2055, Loss: 1.051021695137024, Final Batch Loss: 0.3087849020957947\n",
      "Epoch 2056, Loss: 1.094026118516922, Final Batch Loss: 0.3661384880542755\n",
      "Epoch 2057, Loss: 1.1311520040035248, Final Batch Loss: 0.43146154284477234\n",
      "Epoch 2058, Loss: 1.1059927940368652, Final Batch Loss: 0.45425862073898315\n",
      "Epoch 2059, Loss: 1.1134484708309174, Final Batch Loss: 0.3683125078678131\n",
      "Epoch 2060, Loss: 1.1513397693634033, Final Batch Loss: 0.29119130969047546\n",
      "Epoch 2061, Loss: 1.0685595273971558, Final Batch Loss: 0.3777766823768616\n",
      "Epoch 2062, Loss: 1.0729317665100098, Final Batch Loss: 0.3145332932472229\n",
      "Epoch 2063, Loss: 1.0621485710144043, Final Batch Loss: 0.3012133538722992\n",
      "Epoch 2064, Loss: 0.9416706562042236, Final Batch Loss: 0.35514795780181885\n",
      "Epoch 2065, Loss: 1.1896189153194427, Final Batch Loss: 0.426009863615036\n",
      "Epoch 2066, Loss: 1.114342987537384, Final Batch Loss: 0.43095672130584717\n",
      "Epoch 2067, Loss: 1.146420806646347, Final Batch Loss: 0.4572209417819977\n",
      "Epoch 2068, Loss: 1.110326498746872, Final Batch Loss: 0.37053820490837097\n",
      "Epoch 2069, Loss: 1.1199208199977875, Final Batch Loss: 0.41838538646698\n",
      "Epoch 2070, Loss: 1.0139839947223663, Final Batch Loss: 0.3048456907272339\n",
      "Epoch 2071, Loss: 1.0512678623199463, Final Batch Loss: 0.34313100576400757\n",
      "Epoch 2072, Loss: 1.0026764273643494, Final Batch Loss: 0.3275972008705139\n",
      "Epoch 2073, Loss: 1.070557028055191, Final Batch Loss: 0.3611736297607422\n",
      "Epoch 2074, Loss: 1.0635397136211395, Final Batch Loss: 0.3325450122356415\n",
      "Epoch 2075, Loss: 1.269540160894394, Final Batch Loss: 0.4258565306663513\n",
      "Epoch 2076, Loss: 1.066970318555832, Final Batch Loss: 0.3522889018058777\n",
      "Epoch 2077, Loss: 1.0660598874092102, Final Batch Loss: 0.34289151430130005\n",
      "Epoch 2078, Loss: 1.0869502127170563, Final Batch Loss: 0.4037865102291107\n",
      "Epoch 2079, Loss: 1.0946828722953796, Final Batch Loss: 0.2990165054798126\n",
      "Epoch 2080, Loss: 1.1566827893257141, Final Batch Loss: 0.39377984404563904\n",
      "Epoch 2081, Loss: 1.041577935218811, Final Batch Loss: 0.297138512134552\n",
      "Epoch 2082, Loss: 1.1030737459659576, Final Batch Loss: 0.3830014765262604\n",
      "Epoch 2083, Loss: 1.1102759838104248, Final Batch Loss: 0.36909741163253784\n",
      "Epoch 2084, Loss: 1.112691879272461, Final Batch Loss: 0.3623860478401184\n",
      "Epoch 2085, Loss: 1.1905208230018616, Final Batch Loss: 0.4305197298526764\n",
      "Epoch 2086, Loss: 1.105789452791214, Final Batch Loss: 0.37895071506500244\n",
      "Epoch 2087, Loss: 1.1846348643302917, Final Batch Loss: 0.38119038939476013\n",
      "Epoch 2088, Loss: 1.1228919625282288, Final Batch Loss: 0.4063996374607086\n",
      "Epoch 2089, Loss: 1.1571947038173676, Final Batch Loss: 0.44879454374313354\n",
      "Epoch 2090, Loss: 1.0549821555614471, Final Batch Loss: 0.32834139466285706\n",
      "Epoch 2091, Loss: 1.080425649881363, Final Batch Loss: 0.3064540922641754\n",
      "Epoch 2092, Loss: 1.0681199729442596, Final Batch Loss: 0.33849984407424927\n",
      "Epoch 2093, Loss: 1.065221220254898, Final Batch Loss: 0.36742234230041504\n",
      "Epoch 2094, Loss: 1.0483057796955109, Final Batch Loss: 0.33498215675354004\n",
      "Epoch 2095, Loss: 1.081286072731018, Final Batch Loss: 0.3590438663959503\n",
      "Epoch 2096, Loss: 1.1068713068962097, Final Batch Loss: 0.42975133657455444\n",
      "Epoch 2097, Loss: 0.9701603055000305, Final Batch Loss: 0.23699453473091125\n",
      "Epoch 2098, Loss: 1.071535438299179, Final Batch Loss: 0.3192683160305023\n",
      "Epoch 2099, Loss: 1.1185670793056488, Final Batch Loss: 0.38419240713119507\n",
      "Epoch 2100, Loss: 1.0805162191390991, Final Batch Loss: 0.3400109112262726\n",
      "Epoch 2101, Loss: 1.1005374193191528, Final Batch Loss: 0.34926554560661316\n",
      "Epoch 2102, Loss: 1.1411838233470917, Final Batch Loss: 0.41387689113616943\n",
      "Epoch 2103, Loss: 1.082782506942749, Final Batch Loss: 0.3387564718723297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2104, Loss: 1.118221789598465, Final Batch Loss: 0.3294661045074463\n",
      "Epoch 2105, Loss: 1.0925955176353455, Final Batch Loss: 0.37948352098464966\n",
      "Epoch 2106, Loss: 1.1193757951259613, Final Batch Loss: 0.3989255726337433\n",
      "Epoch 2107, Loss: 1.131334513425827, Final Batch Loss: 0.427353173494339\n",
      "Epoch 2108, Loss: 0.9923329055309296, Final Batch Loss: 0.29125428199768066\n",
      "Epoch 2109, Loss: 1.0191764831542969, Final Batch Loss: 0.3684195280075073\n",
      "Epoch 2110, Loss: 1.1134582459926605, Final Batch Loss: 0.3517882823944092\n",
      "Epoch 2111, Loss: 1.1687287092208862, Final Batch Loss: 0.3948034346103668\n",
      "Epoch 2112, Loss: 1.0257095694541931, Final Batch Loss: 0.31268513202667236\n",
      "Epoch 2113, Loss: 0.9325306266546249, Final Batch Loss: 0.23608438670635223\n",
      "Epoch 2114, Loss: 0.9915181696414948, Final Batch Loss: 0.278964102268219\n",
      "Epoch 2115, Loss: 1.0286591351032257, Final Batch Loss: 0.3489973247051239\n",
      "Epoch 2116, Loss: 1.0249308049678802, Final Batch Loss: 0.2810564935207367\n",
      "Epoch 2117, Loss: 1.2088182270526886, Final Batch Loss: 0.448036253452301\n",
      "Epoch 2118, Loss: 1.0203100740909576, Final Batch Loss: 0.34385445713996887\n",
      "Epoch 2119, Loss: 1.0346965193748474, Final Batch Loss: 0.36812618374824524\n",
      "Epoch 2120, Loss: 1.0399932563304901, Final Batch Loss: 0.311474472284317\n",
      "Epoch 2121, Loss: 1.1481167674064636, Final Batch Loss: 0.34977486729621887\n",
      "Epoch 2122, Loss: 1.0751708447933197, Final Batch Loss: 0.3946447968482971\n",
      "Epoch 2123, Loss: 1.0064948201179504, Final Batch Loss: 0.3187602162361145\n",
      "Epoch 2124, Loss: 1.017288327217102, Final Batch Loss: 0.32727932929992676\n",
      "Epoch 2125, Loss: 1.0671788454055786, Final Batch Loss: 0.3445965349674225\n",
      "Epoch 2126, Loss: 1.0807388126850128, Final Batch Loss: 0.387266606092453\n",
      "Epoch 2127, Loss: 1.0328623354434967, Final Batch Loss: 0.3193877339363098\n",
      "Epoch 2128, Loss: 1.041078895330429, Final Batch Loss: 0.35507333278656006\n",
      "Epoch 2129, Loss: 1.0187824964523315, Final Batch Loss: 0.3006841242313385\n",
      "Epoch 2130, Loss: 0.992704838514328, Final Batch Loss: 0.26536163687705994\n",
      "Epoch 2131, Loss: 1.1279876232147217, Final Batch Loss: 0.3597295582294464\n",
      "Epoch 2132, Loss: 1.0018172860145569, Final Batch Loss: 0.31616339087486267\n",
      "Epoch 2133, Loss: 1.0523110032081604, Final Batch Loss: 0.3746638596057892\n",
      "Epoch 2134, Loss: 1.0068401098251343, Final Batch Loss: 0.3580469489097595\n",
      "Epoch 2135, Loss: 1.0538191199302673, Final Batch Loss: 0.30393317341804504\n",
      "Epoch 2136, Loss: 1.2010613083839417, Final Batch Loss: 0.41782572865486145\n",
      "Epoch 2137, Loss: 1.1498545110225677, Final Batch Loss: 0.43929025530815125\n",
      "Epoch 2138, Loss: 1.176096886396408, Final Batch Loss: 0.3853588104248047\n",
      "Epoch 2139, Loss: 0.9926716387271881, Final Batch Loss: 0.2898075580596924\n",
      "Epoch 2140, Loss: 1.0280481278896332, Final Batch Loss: 0.31696566939353943\n",
      "Epoch 2141, Loss: 1.247708410024643, Final Batch Loss: 0.5508295893669128\n",
      "Epoch 2142, Loss: 1.1053880751132965, Final Batch Loss: 0.38913995027542114\n",
      "Epoch 2143, Loss: 0.9831863343715668, Final Batch Loss: 0.31530454754829407\n",
      "Epoch 2144, Loss: 1.0833862125873566, Final Batch Loss: 0.2999509274959564\n",
      "Epoch 2145, Loss: 1.0601855218410492, Final Batch Loss: 0.3736424148082733\n",
      "Epoch 2146, Loss: 0.9918628633022308, Final Batch Loss: 0.2881510853767395\n",
      "Epoch 2147, Loss: 1.1074552237987518, Final Batch Loss: 0.4158775508403778\n",
      "Epoch 2148, Loss: 1.2063953876495361, Final Batch Loss: 0.40935251116752625\n",
      "Epoch 2149, Loss: 1.100635051727295, Final Batch Loss: 0.47158560156822205\n",
      "Epoch 2150, Loss: 1.0946348011493683, Final Batch Loss: 0.31503939628601074\n",
      "Epoch 2151, Loss: 0.936487078666687, Final Batch Loss: 0.29814398288726807\n",
      "Epoch 2152, Loss: 0.9633318185806274, Final Batch Loss: 0.285527765750885\n",
      "Epoch 2153, Loss: 1.0249654054641724, Final Batch Loss: 0.2951977252960205\n",
      "Epoch 2154, Loss: 1.0715855658054352, Final Batch Loss: 0.3014875650405884\n",
      "Epoch 2155, Loss: 1.0831592679023743, Final Batch Loss: 0.2894020080566406\n",
      "Epoch 2156, Loss: 1.0100809335708618, Final Batch Loss: 0.3548276722431183\n",
      "Epoch 2157, Loss: 0.9544954001903534, Final Batch Loss: 0.3176459074020386\n",
      "Epoch 2158, Loss: 0.9830410182476044, Final Batch Loss: 0.2793186604976654\n",
      "Epoch 2159, Loss: 1.113225132226944, Final Batch Loss: 0.4168033003807068\n",
      "Epoch 2160, Loss: 0.9910125136375427, Final Batch Loss: 0.3170573115348816\n",
      "Epoch 2161, Loss: 1.118244618177414, Final Batch Loss: 0.34303805232048035\n",
      "Epoch 2162, Loss: 1.134293109178543, Final Batch Loss: 0.31183725595474243\n",
      "Epoch 2163, Loss: 0.9800726175308228, Final Batch Loss: 0.31067776679992676\n",
      "Epoch 2164, Loss: 1.0195792317390442, Final Batch Loss: 0.31793543696403503\n",
      "Epoch 2165, Loss: 1.0214513838291168, Final Batch Loss: 0.3057170510292053\n",
      "Epoch 2166, Loss: 1.0455683469772339, Final Batch Loss: 0.3483390808105469\n",
      "Epoch 2167, Loss: 1.0495189428329468, Final Batch Loss: 0.3372771441936493\n",
      "Epoch 2168, Loss: 1.0608541667461395, Final Batch Loss: 0.43533632159233093\n",
      "Epoch 2169, Loss: 1.0973894596099854, Final Batch Loss: 0.3998567461967468\n",
      "Epoch 2170, Loss: 0.9936113506555557, Final Batch Loss: 0.2475457638502121\n",
      "Epoch 2171, Loss: 1.1348259449005127, Final Batch Loss: 0.4633382558822632\n",
      "Epoch 2172, Loss: 1.1213982999324799, Final Batch Loss: 0.4458426833152771\n",
      "Epoch 2173, Loss: 1.0557026267051697, Final Batch Loss: 0.34229978919029236\n",
      "Epoch 2174, Loss: 1.1082144975662231, Final Batch Loss: 0.3918944299221039\n",
      "Epoch 2175, Loss: 0.9965044856071472, Final Batch Loss: 0.3134067952632904\n",
      "Epoch 2176, Loss: 0.969416081905365, Final Batch Loss: 0.26252931356430054\n",
      "Epoch 2177, Loss: 1.0865971744060516, Final Batch Loss: 0.35224148631095886\n",
      "Epoch 2178, Loss: 1.0764002203941345, Final Batch Loss: 0.4193321168422699\n",
      "Epoch 2179, Loss: 1.1265837252140045, Final Batch Loss: 0.38783109188079834\n",
      "Epoch 2180, Loss: 1.2013947069644928, Final Batch Loss: 0.47093385457992554\n",
      "Epoch 2181, Loss: 1.0929555892944336, Final Batch Loss: 0.32468900084495544\n",
      "Epoch 2182, Loss: 1.1048321723937988, Final Batch Loss: 0.31316304206848145\n",
      "Epoch 2183, Loss: 1.165701687335968, Final Batch Loss: 0.3244556784629822\n",
      "Epoch 2184, Loss: 1.0070278495550156, Final Batch Loss: 0.2203371673822403\n",
      "Epoch 2185, Loss: 1.0654283165931702, Final Batch Loss: 0.30577054619789124\n",
      "Epoch 2186, Loss: 1.0651991963386536, Final Batch Loss: 0.3419308066368103\n",
      "Epoch 2187, Loss: 1.144110769033432, Final Batch Loss: 0.3575264513492584\n",
      "Epoch 2188, Loss: 1.0526686906814575, Final Batch Loss: 0.31902608275413513\n",
      "Epoch 2189, Loss: 1.214308738708496, Final Batch Loss: 0.49772247672080994\n",
      "Epoch 2190, Loss: 1.0808342099189758, Final Batch Loss: 0.3894319236278534\n",
      "Epoch 2191, Loss: 1.052336573600769, Final Batch Loss: 0.27606290578842163\n",
      "Epoch 2192, Loss: 1.1312629878520966, Final Batch Loss: 0.3960527181625366\n",
      "Epoch 2193, Loss: 1.0573661923408508, Final Batch Loss: 0.3722268044948578\n",
      "Epoch 2194, Loss: 1.142434984445572, Final Batch Loss: 0.38923898339271545\n",
      "Epoch 2195, Loss: 1.0788439214229584, Final Batch Loss: 0.37479838728904724\n",
      "Epoch 2196, Loss: 0.989214301109314, Final Batch Loss: 0.3355095386505127\n",
      "Epoch 2197, Loss: 1.1216553747653961, Final Batch Loss: 0.4164738357067108\n",
      "Epoch 2198, Loss: 1.1602964103221893, Final Batch Loss: 0.45136430859565735\n",
      "Epoch 2199, Loss: 1.069121092557907, Final Batch Loss: 0.3666874170303345\n",
      "Epoch 2200, Loss: 1.0942533016204834, Final Batch Loss: 0.3798280954360962\n",
      "Epoch 2201, Loss: 1.1036316752433777, Final Batch Loss: 0.42498916387557983\n",
      "Epoch 2202, Loss: 1.1200315952301025, Final Batch Loss: 0.4503850042819977\n",
      "Epoch 2203, Loss: 0.957787036895752, Final Batch Loss: 0.2830791771411896\n",
      "Epoch 2204, Loss: 1.1018159687519073, Final Batch Loss: 0.42641928791999817\n",
      "Epoch 2205, Loss: 1.0765345990657806, Final Batch Loss: 0.3050510287284851\n",
      "Epoch 2206, Loss: 1.1783444583415985, Final Batch Loss: 0.39390048384666443\n",
      "Epoch 2207, Loss: 1.1925864219665527, Final Batch Loss: 0.5088657736778259\n",
      "Epoch 2208, Loss: 1.039762705564499, Final Batch Loss: 0.3858824670314789\n",
      "Epoch 2209, Loss: 1.0822179913520813, Final Batch Loss: 0.40532347559928894\n",
      "Epoch 2210, Loss: 1.111329048871994, Final Batch Loss: 0.4269595444202423\n",
      "Epoch 2211, Loss: 0.9262352734804153, Final Batch Loss: 0.19382156431674957\n",
      "Epoch 2212, Loss: 1.069215476512909, Final Batch Loss: 0.3127027153968811\n",
      "Epoch 2213, Loss: 1.1075243949890137, Final Batch Loss: 0.40876394510269165\n",
      "Epoch 2214, Loss: 1.0884559154510498, Final Batch Loss: 0.389402836561203\n",
      "Epoch 2215, Loss: 1.167171835899353, Final Batch Loss: 0.4406140446662903\n",
      "Epoch 2216, Loss: 1.0091094076633453, Final Batch Loss: 0.3066743016242981\n",
      "Epoch 2217, Loss: 1.1900499761104584, Final Batch Loss: 0.4632473886013031\n",
      "Epoch 2218, Loss: 1.2383482456207275, Final Batch Loss: 0.5073030591011047\n",
      "Epoch 2219, Loss: 0.978456661105156, Final Batch Loss: 0.23713113367557526\n",
      "Epoch 2220, Loss: 0.9918597340583801, Final Batch Loss: 0.3322593867778778\n",
      "Epoch 2221, Loss: 1.014273762702942, Final Batch Loss: 0.3159039616584778\n",
      "Epoch 2222, Loss: 0.9794785380363464, Final Batch Loss: 0.2763024866580963\n",
      "Epoch 2223, Loss: 1.167039006948471, Final Batch Loss: 0.4486001431941986\n",
      "Epoch 2224, Loss: 1.0200402736663818, Final Batch Loss: 0.30668559670448303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2225, Loss: 0.9953321516513824, Final Batch Loss: 0.27404868602752686\n",
      "Epoch 2226, Loss: 1.115134835243225, Final Batch Loss: 0.4636978209018707\n",
      "Epoch 2227, Loss: 1.059000939130783, Final Batch Loss: 0.37063178420066833\n",
      "Epoch 2228, Loss: 1.2135541439056396, Final Batch Loss: 0.4805724620819092\n",
      "Epoch 2229, Loss: 1.041066825389862, Final Batch Loss: 0.36617475748062134\n",
      "Epoch 2230, Loss: 0.8979964852333069, Final Batch Loss: 0.2083771824836731\n",
      "Epoch 2231, Loss: 1.0352641642093658, Final Batch Loss: 0.3645251393318176\n",
      "Epoch 2232, Loss: 1.0646869838237762, Final Batch Loss: 0.3316226899623871\n",
      "Epoch 2233, Loss: 1.1657082438468933, Final Batch Loss: 0.484282523393631\n",
      "Epoch 2234, Loss: 1.0089132189750671, Final Batch Loss: 0.2722000777721405\n",
      "Epoch 2235, Loss: 1.1305813789367676, Final Batch Loss: 0.388041615486145\n",
      "Epoch 2236, Loss: 1.1390742659568787, Final Batch Loss: 0.3881969451904297\n",
      "Epoch 2237, Loss: 1.071361631155014, Final Batch Loss: 0.435259073972702\n",
      "Epoch 2238, Loss: 1.19539675116539, Final Batch Loss: 0.49415355920791626\n",
      "Epoch 2239, Loss: 1.01413494348526, Final Batch Loss: 0.3776175081729889\n",
      "Epoch 2240, Loss: 1.0865874886512756, Final Batch Loss: 0.40494558215141296\n",
      "Epoch 2241, Loss: 1.0739019513130188, Final Batch Loss: 0.39465227723121643\n",
      "Epoch 2242, Loss: 0.959618866443634, Final Batch Loss: 0.2785722315311432\n",
      "Epoch 2243, Loss: 0.949623703956604, Final Batch Loss: 0.2783474326133728\n",
      "Epoch 2244, Loss: 0.936901330947876, Final Batch Loss: 0.25662747025489807\n",
      "Epoch 2245, Loss: 1.1166448593139648, Final Batch Loss: 0.3736113905906677\n",
      "Epoch 2246, Loss: 1.0796398520469666, Final Batch Loss: 0.33203014731407166\n",
      "Epoch 2247, Loss: 1.1493998169898987, Final Batch Loss: 0.3430235683917999\n",
      "Epoch 2248, Loss: 0.9548782110214233, Final Batch Loss: 0.3232881426811218\n",
      "Epoch 2249, Loss: 1.005041629076004, Final Batch Loss: 0.308140367269516\n",
      "Epoch 2250, Loss: 0.9836312830448151, Final Batch Loss: 0.37297505140304565\n",
      "Epoch 2251, Loss: 1.0183306634426117, Final Batch Loss: 0.4225255846977234\n",
      "Epoch 2252, Loss: 1.0407269299030304, Final Batch Loss: 0.39366090297698975\n",
      "Epoch 2253, Loss: 1.0240390300750732, Final Batch Loss: 0.3522098660469055\n",
      "Epoch 2254, Loss: 1.0606811046600342, Final Batch Loss: 0.4161827862262726\n",
      "Epoch 2255, Loss: 1.0735783874988556, Final Batch Loss: 0.3348773419857025\n",
      "Epoch 2256, Loss: 1.0013336837291718, Final Batch Loss: 0.33694398403167725\n",
      "Epoch 2257, Loss: 0.9997825920581818, Final Batch Loss: 0.29409298300743103\n",
      "Epoch 2258, Loss: 1.081953227519989, Final Batch Loss: 0.36961838603019714\n",
      "Epoch 2259, Loss: 1.0094091296195984, Final Batch Loss: 0.4074665904045105\n",
      "Epoch 2260, Loss: 1.04296413064003, Final Batch Loss: 0.348600298166275\n",
      "Epoch 2261, Loss: 0.9838902056217194, Final Batch Loss: 0.313923180103302\n",
      "Epoch 2262, Loss: 0.9267036616802216, Final Batch Loss: 0.2664773762226105\n",
      "Epoch 2263, Loss: 1.0140687823295593, Final Batch Loss: 0.3016708493232727\n",
      "Epoch 2264, Loss: 1.0851048827171326, Final Batch Loss: 0.3720378577709198\n",
      "Epoch 2265, Loss: 1.087135374546051, Final Batch Loss: 0.4028014540672302\n",
      "Epoch 2266, Loss: 0.9870835244655609, Final Batch Loss: 0.36930039525032043\n",
      "Epoch 2267, Loss: 1.0265983939170837, Final Batch Loss: 0.3758315145969391\n",
      "Epoch 2268, Loss: 1.020450621843338, Final Batch Loss: 0.34236466884613037\n",
      "Epoch 2269, Loss: 1.0968534350395203, Final Batch Loss: 0.44316208362579346\n",
      "Epoch 2270, Loss: 0.9454349279403687, Final Batch Loss: 0.3006021976470947\n",
      "Epoch 2271, Loss: 1.047630488872528, Final Batch Loss: 0.40164482593536377\n",
      "Epoch 2272, Loss: 1.0728654861450195, Final Batch Loss: 0.3957660496234894\n",
      "Epoch 2273, Loss: 0.995144784450531, Final Batch Loss: 0.36640465259552\n",
      "Epoch 2274, Loss: 1.2154489755630493, Final Batch Loss: 0.49747616052627563\n",
      "Epoch 2275, Loss: 1.0779008567333221, Final Batch Loss: 0.3940476179122925\n",
      "Epoch 2276, Loss: 1.077714204788208, Final Batch Loss: 0.32445278763771057\n",
      "Epoch 2277, Loss: 1.0585367977619171, Final Batch Loss: 0.41053545475006104\n",
      "Epoch 2278, Loss: 1.085651159286499, Final Batch Loss: 0.32581737637519836\n",
      "Epoch 2279, Loss: 0.9975881278514862, Final Batch Loss: 0.28373557329177856\n",
      "Epoch 2280, Loss: 0.9823095202445984, Final Batch Loss: 0.3155997395515442\n",
      "Epoch 2281, Loss: 0.9845379292964935, Final Batch Loss: 0.3576209247112274\n",
      "Epoch 2282, Loss: 0.9960075914859772, Final Batch Loss: 0.2826715111732483\n",
      "Epoch 2283, Loss: 0.9718061685562134, Final Batch Loss: 0.36570626497268677\n",
      "Epoch 2284, Loss: 1.0506300330162048, Final Batch Loss: 0.3870144486427307\n",
      "Epoch 2285, Loss: 0.9991264045238495, Final Batch Loss: 0.385332852602005\n",
      "Epoch 2286, Loss: 1.092556744813919, Final Batch Loss: 0.44151216745376587\n",
      "Epoch 2287, Loss: 1.021636426448822, Final Batch Loss: 0.35261255502700806\n",
      "Epoch 2288, Loss: 1.1152336299419403, Final Batch Loss: 0.35256630182266235\n",
      "Epoch 2289, Loss: 1.029176026582718, Final Batch Loss: 0.37412646412849426\n",
      "Epoch 2290, Loss: 1.0991902351379395, Final Batch Loss: 0.3922431468963623\n",
      "Epoch 2291, Loss: 1.0595982074737549, Final Batch Loss: 0.37826722860336304\n",
      "Epoch 2292, Loss: 0.9169943630695343, Final Batch Loss: 0.3022814393043518\n",
      "Epoch 2293, Loss: 0.9788003265857697, Final Batch Loss: 0.3054959774017334\n",
      "Epoch 2294, Loss: 1.0114365220069885, Final Batch Loss: 0.3928965926170349\n",
      "Epoch 2295, Loss: 1.0374054610729218, Final Batch Loss: 0.31012389063835144\n",
      "Epoch 2296, Loss: 0.9552446603775024, Final Batch Loss: 0.2859780192375183\n",
      "Epoch 2297, Loss: 0.9860342741012573, Final Batch Loss: 0.2769787609577179\n",
      "Epoch 2298, Loss: 0.9524137526750565, Final Batch Loss: 0.22345583140850067\n",
      "Epoch 2299, Loss: 1.04974827170372, Final Batch Loss: 0.3406805098056793\n",
      "Epoch 2300, Loss: 0.9388251900672913, Final Batch Loss: 0.2855805456638336\n",
      "Epoch 2301, Loss: 0.9769946038722992, Final Batch Loss: 0.32182803750038147\n",
      "Epoch 2302, Loss: 1.0880833566188812, Final Batch Loss: 0.45173290371894836\n",
      "Epoch 2303, Loss: 0.9934941232204437, Final Batch Loss: 0.36461395025253296\n",
      "Epoch 2304, Loss: 1.074805498123169, Final Batch Loss: 0.31979915499687195\n",
      "Epoch 2305, Loss: 1.044708013534546, Final Batch Loss: 0.334576278924942\n",
      "Epoch 2306, Loss: 0.9963378608226776, Final Batch Loss: 0.3246476650238037\n",
      "Epoch 2307, Loss: 1.0685353875160217, Final Batch Loss: 0.3922883868217468\n",
      "Epoch 2308, Loss: 0.983863890171051, Final Batch Loss: 0.2605898976325989\n",
      "Epoch 2309, Loss: 1.1253646910190582, Final Batch Loss: 0.38586586713790894\n",
      "Epoch 2310, Loss: 1.0551750659942627, Final Batch Loss: 0.4006858766078949\n",
      "Epoch 2311, Loss: 0.9911727905273438, Final Batch Loss: 0.33461979031562805\n",
      "Epoch 2312, Loss: 0.960046648979187, Final Batch Loss: 0.2907213568687439\n",
      "Epoch 2313, Loss: 0.948432594537735, Final Batch Loss: 0.28037887811660767\n",
      "Epoch 2314, Loss: 1.0478997826576233, Final Batch Loss: 0.32371601462364197\n",
      "Epoch 2315, Loss: 0.9107092022895813, Final Batch Loss: 0.28133222460746765\n",
      "Epoch 2316, Loss: 0.9743528664112091, Final Batch Loss: 0.30480098724365234\n",
      "Epoch 2317, Loss: 1.0127349495887756, Final Batch Loss: 0.27039390802383423\n",
      "Epoch 2318, Loss: 0.9885222613811493, Final Batch Loss: 0.32319608330726624\n",
      "Epoch 2319, Loss: 1.119145780801773, Final Batch Loss: 0.4030519127845764\n",
      "Epoch 2320, Loss: 1.056332528591156, Final Batch Loss: 0.3279167711734772\n",
      "Epoch 2321, Loss: 1.064862310886383, Final Batch Loss: 0.32711559534072876\n",
      "Epoch 2322, Loss: 1.0217984914779663, Final Batch Loss: 0.3660781979560852\n",
      "Epoch 2323, Loss: 1.0935270488262177, Final Batch Loss: 0.42466166615486145\n",
      "Epoch 2324, Loss: 1.052741527557373, Final Batch Loss: 0.4072483479976654\n",
      "Epoch 2325, Loss: 1.0323174595832825, Final Batch Loss: 0.41511353850364685\n",
      "Epoch 2326, Loss: 1.0591756403446198, Final Batch Loss: 0.49251067638397217\n",
      "Epoch 2327, Loss: 1.0332399308681488, Final Batch Loss: 0.4182230234146118\n",
      "Epoch 2328, Loss: 0.9816895425319672, Final Batch Loss: 0.3215605616569519\n",
      "Epoch 2329, Loss: 0.9803034067153931, Final Batch Loss: 0.3033605217933655\n",
      "Epoch 2330, Loss: 1.1210038363933563, Final Batch Loss: 0.43571510910987854\n",
      "Epoch 2331, Loss: 1.0192936956882477, Final Batch Loss: 0.31272754073143005\n",
      "Epoch 2332, Loss: 0.9830544590950012, Final Batch Loss: 0.29726529121398926\n",
      "Epoch 2333, Loss: 1.0624870955944061, Final Batch Loss: 0.39502429962158203\n",
      "Epoch 2334, Loss: 0.987221747636795, Final Batch Loss: 0.3789442479610443\n",
      "Epoch 2335, Loss: 1.0136798918247223, Final Batch Loss: 0.3096783757209778\n",
      "Epoch 2336, Loss: 0.9928648769855499, Final Batch Loss: 0.32885095477104187\n",
      "Epoch 2337, Loss: 0.9262800067663193, Final Batch Loss: 0.23592446744441986\n",
      "Epoch 2338, Loss: 0.9726040959358215, Final Batch Loss: 0.26679161190986633\n",
      "Epoch 2339, Loss: 1.0237453877925873, Final Batch Loss: 0.2504230737686157\n",
      "Epoch 2340, Loss: 0.9356609582901001, Final Batch Loss: 0.21403998136520386\n",
      "Epoch 2341, Loss: 1.0160667896270752, Final Batch Loss: 0.2960817217826843\n",
      "Epoch 2342, Loss: 1.0240607559680939, Final Batch Loss: 0.38016262650489807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2343, Loss: 1.079247772693634, Final Batch Loss: 0.39762821793556213\n",
      "Epoch 2344, Loss: 1.0274212062358856, Final Batch Loss: 0.311998188495636\n",
      "Epoch 2345, Loss: 1.0711674094200134, Final Batch Loss: 0.3430728316307068\n",
      "Epoch 2346, Loss: 0.981732577085495, Final Batch Loss: 0.2744538486003876\n",
      "Epoch 2347, Loss: 1.0170675218105316, Final Batch Loss: 0.34060487151145935\n",
      "Epoch 2348, Loss: 1.1301254034042358, Final Batch Loss: 0.40775054693222046\n",
      "Epoch 2349, Loss: 1.0881660878658295, Final Batch Loss: 0.36009398102760315\n",
      "Epoch 2350, Loss: 0.9698319733142853, Final Batch Loss: 0.33533284068107605\n",
      "Epoch 2351, Loss: 1.0850633382797241, Final Batch Loss: 0.37678268551826477\n",
      "Epoch 2352, Loss: 1.0434797406196594, Final Batch Loss: 0.28811588883399963\n",
      "Epoch 2353, Loss: 1.0667095184326172, Final Batch Loss: 0.35328108072280884\n",
      "Epoch 2354, Loss: 1.005705088376999, Final Batch Loss: 0.3579128384590149\n",
      "Epoch 2355, Loss: 1.0193938910961151, Final Batch Loss: 0.3618864119052887\n",
      "Epoch 2356, Loss: 1.0141128599643707, Final Batch Loss: 0.3646150827407837\n",
      "Epoch 2357, Loss: 0.9664045870304108, Final Batch Loss: 0.3067176938056946\n",
      "Epoch 2358, Loss: 0.9443847984075546, Final Batch Loss: 0.23658053576946259\n",
      "Epoch 2359, Loss: 1.0468712151050568, Final Batch Loss: 0.44186654686927795\n",
      "Epoch 2360, Loss: 1.0051445066928864, Final Batch Loss: 0.3249346911907196\n",
      "Epoch 2361, Loss: 1.0019203126430511, Final Batch Loss: 0.3946835398674011\n",
      "Epoch 2362, Loss: 1.0280165076255798, Final Batch Loss: 0.3285768926143646\n",
      "Epoch 2363, Loss: 1.067750632762909, Final Batch Loss: 0.34943968057632446\n",
      "Epoch 2364, Loss: 1.0678535401821136, Final Batch Loss: 0.3736247420310974\n",
      "Epoch 2365, Loss: 1.0996428430080414, Final Batch Loss: 0.299508273601532\n",
      "Epoch 2366, Loss: 1.0161418616771698, Final Batch Loss: 0.3450079560279846\n",
      "Epoch 2367, Loss: 0.983205109834671, Final Batch Loss: 0.29428452253341675\n",
      "Epoch 2368, Loss: 1.0864693820476532, Final Batch Loss: 0.38534191250801086\n",
      "Epoch 2369, Loss: 0.9662783145904541, Final Batch Loss: 0.3293103277683258\n",
      "Epoch 2370, Loss: 1.0449640452861786, Final Batch Loss: 0.352407306432724\n",
      "Epoch 2371, Loss: 1.0270331799983978, Final Batch Loss: 0.2897798418998718\n",
      "Epoch 2372, Loss: 1.1249632239341736, Final Batch Loss: 0.4054485559463501\n",
      "Epoch 2373, Loss: 1.0505730211734772, Final Batch Loss: 0.4423711597919464\n",
      "Epoch 2374, Loss: 0.9916718006134033, Final Batch Loss: 0.3607149124145508\n",
      "Epoch 2375, Loss: 1.0264155566692352, Final Batch Loss: 0.3037530481815338\n",
      "Epoch 2376, Loss: 1.0071623921394348, Final Batch Loss: 0.28847938776016235\n",
      "Epoch 2377, Loss: 1.0838459730148315, Final Batch Loss: 0.2915762662887573\n",
      "Epoch 2378, Loss: 0.9544446170330048, Final Batch Loss: 0.28006279468536377\n",
      "Epoch 2379, Loss: 1.1457759141921997, Final Batch Loss: 0.42615991830825806\n",
      "Epoch 2380, Loss: 1.0553517937660217, Final Batch Loss: 0.41370299458503723\n",
      "Epoch 2381, Loss: 1.079181283712387, Final Batch Loss: 0.42543825507164\n",
      "Epoch 2382, Loss: 0.9808339178562164, Final Batch Loss: 0.26850825548171997\n",
      "Epoch 2383, Loss: 0.9545595943927765, Final Batch Loss: 0.284223735332489\n",
      "Epoch 2384, Loss: 1.0784595906734467, Final Batch Loss: 0.3651811480522156\n",
      "Epoch 2385, Loss: 0.9661231338977814, Final Batch Loss: 0.2812439203262329\n",
      "Epoch 2386, Loss: 0.9406617283821106, Final Batch Loss: 0.3296054005622864\n",
      "Epoch 2387, Loss: 0.9437752366065979, Final Batch Loss: 0.29197442531585693\n",
      "Epoch 2388, Loss: 1.038359373807907, Final Batch Loss: 0.293738454580307\n",
      "Epoch 2389, Loss: 1.0197942554950714, Final Batch Loss: 0.2657102644443512\n",
      "Epoch 2390, Loss: 0.9237567186355591, Final Batch Loss: 0.26762890815734863\n",
      "Epoch 2391, Loss: 0.9763891696929932, Final Batch Loss: 0.3619430959224701\n",
      "Epoch 2392, Loss: 1.0165022313594818, Final Batch Loss: 0.3301614820957184\n",
      "Epoch 2393, Loss: 1.1125347316265106, Final Batch Loss: 0.3749024569988251\n",
      "Epoch 2394, Loss: 0.9464320838451385, Final Batch Loss: 0.2610582113265991\n",
      "Epoch 2395, Loss: 1.0065268278121948, Final Batch Loss: 0.37160322070121765\n",
      "Epoch 2396, Loss: 1.0357540845870972, Final Batch Loss: 0.41063109040260315\n",
      "Epoch 2397, Loss: 0.9147224426269531, Final Batch Loss: 0.26064902544021606\n",
      "Epoch 2398, Loss: 0.9999746084213257, Final Batch Loss: 0.32075250148773193\n",
      "Epoch 2399, Loss: 0.891034722328186, Final Batch Loss: 0.25581857562065125\n",
      "Epoch 2400, Loss: 1.0209390223026276, Final Batch Loss: 0.2769826650619507\n",
      "Epoch 2401, Loss: 0.9349218308925629, Final Batch Loss: 0.28758907318115234\n",
      "Epoch 2402, Loss: 0.9490087926387787, Final Batch Loss: 0.2914494574069977\n",
      "Epoch 2403, Loss: 0.9268993735313416, Final Batch Loss: 0.2995937466621399\n",
      "Epoch 2404, Loss: 1.1094224154949188, Final Batch Loss: 0.2656655013561249\n",
      "Epoch 2405, Loss: 1.035852313041687, Final Batch Loss: 0.3425597846508026\n",
      "Epoch 2406, Loss: 0.9686739593744278, Final Batch Loss: 0.24780072271823883\n",
      "Epoch 2407, Loss: 1.1457161903381348, Final Batch Loss: 0.438005268573761\n",
      "Epoch 2408, Loss: 1.1786720156669617, Final Batch Loss: 0.42241495847702026\n",
      "Epoch 2409, Loss: 0.991096168756485, Final Batch Loss: 0.35225385427474976\n",
      "Epoch 2410, Loss: 0.9517776668071747, Final Batch Loss: 0.3434239625930786\n",
      "Epoch 2411, Loss: 1.1494999825954437, Final Batch Loss: 0.40517765283584595\n",
      "Epoch 2412, Loss: 0.9803302586078644, Final Batch Loss: 0.2894074022769928\n",
      "Epoch 2413, Loss: 1.0755390226840973, Final Batch Loss: 0.43734484910964966\n",
      "Epoch 2414, Loss: 0.9640741050243378, Final Batch Loss: 0.2895510494709015\n",
      "Epoch 2415, Loss: 0.9402485489845276, Final Batch Loss: 0.2973783612251282\n",
      "Epoch 2416, Loss: 0.8949708342552185, Final Batch Loss: 0.2896381914615631\n",
      "Epoch 2417, Loss: 0.9547410011291504, Final Batch Loss: 0.3034476935863495\n",
      "Epoch 2418, Loss: 0.9611829817295074, Final Batch Loss: 0.26697614789009094\n",
      "Epoch 2419, Loss: 1.0356713235378265, Final Batch Loss: 0.3222768306732178\n",
      "Epoch 2420, Loss: 0.9984455406665802, Final Batch Loss: 0.31283435225486755\n",
      "Epoch 2421, Loss: 0.9576305150985718, Final Batch Loss: 0.3233013153076172\n",
      "Epoch 2422, Loss: 1.0495905876159668, Final Batch Loss: 0.37674471735954285\n",
      "Epoch 2423, Loss: 1.0598670542240143, Final Batch Loss: 0.3836769759654999\n",
      "Epoch 2424, Loss: 0.9872114956378937, Final Batch Loss: 0.26522380113601685\n",
      "Epoch 2425, Loss: 1.1360785067081451, Final Batch Loss: 0.33408284187316895\n",
      "Epoch 2426, Loss: 0.8714211285114288, Final Batch Loss: 0.220818430185318\n",
      "Epoch 2427, Loss: 1.1165993809700012, Final Batch Loss: 0.4396854639053345\n",
      "Epoch 2428, Loss: 1.0463523268699646, Final Batch Loss: 0.3320600390434265\n",
      "Epoch 2429, Loss: 1.0351389348506927, Final Batch Loss: 0.3777593970298767\n",
      "Epoch 2430, Loss: 0.9922578632831573, Final Batch Loss: 0.2991902232170105\n",
      "Epoch 2431, Loss: 0.955136239528656, Final Batch Loss: 0.3222825825214386\n",
      "Epoch 2432, Loss: 0.9831885099411011, Final Batch Loss: 0.3123636543750763\n",
      "Epoch 2433, Loss: 0.930579885840416, Final Batch Loss: 0.23340673744678497\n",
      "Epoch 2434, Loss: 1.0556224584579468, Final Batch Loss: 0.347377747297287\n",
      "Epoch 2435, Loss: 1.030520737171173, Final Batch Loss: 0.3009389042854309\n",
      "Epoch 2436, Loss: 1.0119878053665161, Final Batch Loss: 0.3356005549430847\n",
      "Epoch 2437, Loss: 1.0318963825702667, Final Batch Loss: 0.2998492121696472\n",
      "Epoch 2438, Loss: 1.0260689556598663, Final Batch Loss: 0.35274773836135864\n",
      "Epoch 2439, Loss: 1.0370348691940308, Final Batch Loss: 0.3866094648838043\n",
      "Epoch 2440, Loss: 0.9864777326583862, Final Batch Loss: 0.3201240003108978\n",
      "Epoch 2441, Loss: 0.9296349287033081, Final Batch Loss: 0.30360209941864014\n",
      "Epoch 2442, Loss: 0.9141028821468353, Final Batch Loss: 0.26964136958122253\n",
      "Epoch 2443, Loss: 0.9456293135881424, Final Batch Loss: 0.2376575618982315\n",
      "Epoch 2444, Loss: 0.8681039363145828, Final Batch Loss: 0.2312309890985489\n",
      "Epoch 2445, Loss: 1.006696343421936, Final Batch Loss: 0.3706337511539459\n",
      "Epoch 2446, Loss: 1.0113911628723145, Final Batch Loss: 0.38035914301872253\n",
      "Epoch 2447, Loss: 1.1034106016159058, Final Batch Loss: 0.3883814513683319\n",
      "Epoch 2448, Loss: 1.0099487900733948, Final Batch Loss: 0.37716901302337646\n",
      "Epoch 2449, Loss: 1.0660480558872223, Final Batch Loss: 0.42323416471481323\n",
      "Epoch 2450, Loss: 1.086931437253952, Final Batch Loss: 0.3543242812156677\n",
      "Epoch 2451, Loss: 0.8831998705863953, Final Batch Loss: 0.2648654580116272\n",
      "Epoch 2452, Loss: 0.927600234746933, Final Batch Loss: 0.2693260610103607\n",
      "Epoch 2453, Loss: 0.9505131244659424, Final Batch Loss: 0.2156926989555359\n",
      "Epoch 2454, Loss: 0.9618019163608551, Final Batch Loss: 0.33462682366371155\n",
      "Epoch 2455, Loss: 1.0943222343921661, Final Batch Loss: 0.34225982427597046\n",
      "Epoch 2456, Loss: 0.9892444610595703, Final Batch Loss: 0.3097636103630066\n",
      "Epoch 2457, Loss: 0.9911398589611053, Final Batch Loss: 0.2905423045158386\n",
      "Epoch 2458, Loss: 0.991445392370224, Final Batch Loss: 0.36184507608413696\n",
      "Epoch 2459, Loss: 0.9239277839660645, Final Batch Loss: 0.24046912789344788\n",
      "Epoch 2460, Loss: 1.0988405644893646, Final Batch Loss: 0.29396653175354004\n",
      "Epoch 2461, Loss: 0.9834198951721191, Final Batch Loss: 0.36227351427078247\n",
      "Epoch 2462, Loss: 0.9686780273914337, Final Batch Loss: 0.30758601427078247\n",
      "Epoch 2463, Loss: 0.9696612656116486, Final Batch Loss: 0.3766273856163025\n",
      "Epoch 2464, Loss: 0.9008853435516357, Final Batch Loss: 0.2637077569961548\n",
      "Epoch 2465, Loss: 0.9653535485267639, Final Batch Loss: 0.2913767993450165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2466, Loss: 0.937421441078186, Final Batch Loss: 0.2677600383758545\n",
      "Epoch 2467, Loss: 1.0502695143222809, Final Batch Loss: 0.3457491099834442\n",
      "Epoch 2468, Loss: 1.0426141023635864, Final Batch Loss: 0.32216814160346985\n",
      "Epoch 2469, Loss: 0.8710406124591827, Final Batch Loss: 0.2707253694534302\n",
      "Epoch 2470, Loss: 1.1236677467823029, Final Batch Loss: 0.4069474935531616\n",
      "Epoch 2471, Loss: 0.997194766998291, Final Batch Loss: 0.328836053609848\n",
      "Epoch 2472, Loss: 1.039482057094574, Final Batch Loss: 0.3541581332683563\n",
      "Epoch 2473, Loss: 1.0605020225048065, Final Batch Loss: 0.4345291256904602\n",
      "Epoch 2474, Loss: 1.0644030570983887, Final Batch Loss: 0.3285579979419708\n",
      "Epoch 2475, Loss: 1.0858387351036072, Final Batch Loss: 0.3526766896247864\n",
      "Epoch 2476, Loss: 0.9328598380088806, Final Batch Loss: 0.2955775856971741\n",
      "Epoch 2477, Loss: 0.8975330591201782, Final Batch Loss: 0.2513203024864197\n",
      "Epoch 2478, Loss: 1.023174524307251, Final Batch Loss: 0.3338284194469452\n",
      "Epoch 2479, Loss: 1.164314329624176, Final Batch Loss: 0.4292413592338562\n",
      "Epoch 2480, Loss: 0.9745087623596191, Final Batch Loss: 0.30852681398391724\n",
      "Epoch 2481, Loss: 1.0533662736415863, Final Batch Loss: 0.335999459028244\n",
      "Epoch 2482, Loss: 1.0304390490055084, Final Batch Loss: 0.3956086337566376\n",
      "Epoch 2483, Loss: 1.0134340524673462, Final Batch Loss: 0.31413841247558594\n",
      "Epoch 2484, Loss: 1.1341419219970703, Final Batch Loss: 0.4547455608844757\n",
      "Epoch 2485, Loss: 1.0391841232776642, Final Batch Loss: 0.35472026467323303\n",
      "Epoch 2486, Loss: 0.9899722337722778, Final Batch Loss: 0.34464722871780396\n",
      "Epoch 2487, Loss: 1.1028448939323425, Final Batch Loss: 0.38588568568229675\n",
      "Epoch 2488, Loss: 0.9716385900974274, Final Batch Loss: 0.21729084849357605\n",
      "Epoch 2489, Loss: 0.9050344526767731, Final Batch Loss: 0.24205705523490906\n",
      "Epoch 2490, Loss: 0.9105328917503357, Final Batch Loss: 0.2734469771385193\n",
      "Epoch 2491, Loss: 1.044105589389801, Final Batch Loss: 0.34580281376838684\n",
      "Epoch 2492, Loss: 0.9393959045410156, Final Batch Loss: 0.3041893541812897\n",
      "Epoch 2493, Loss: 1.0261759757995605, Final Batch Loss: 0.27373820543289185\n",
      "Epoch 2494, Loss: 1.0724451839923859, Final Batch Loss: 0.4007522761821747\n",
      "Epoch 2495, Loss: 1.037612110376358, Final Batch Loss: 0.36675161123275757\n",
      "Epoch 2496, Loss: 0.9468412101268768, Final Batch Loss: 0.25575071573257446\n",
      "Epoch 2497, Loss: 0.8034251034259796, Final Batch Loss: 0.2153361439704895\n",
      "Epoch 2498, Loss: 1.071807324886322, Final Batch Loss: 0.3954847455024719\n",
      "Epoch 2499, Loss: 0.9211574047803879, Final Batch Loss: 0.22200597822666168\n",
      "Epoch 2500, Loss: 0.9339262247085571, Final Batch Loss: 0.3300156891345978\n",
      "Epoch 2501, Loss: 0.998483419418335, Final Batch Loss: 0.2701886296272278\n",
      "Epoch 2502, Loss: 1.012574315071106, Final Batch Loss: 0.36494696140289307\n",
      "Epoch 2503, Loss: 0.9294589459896088, Final Batch Loss: 0.27512478828430176\n",
      "Epoch 2504, Loss: 0.9896898567676544, Final Batch Loss: 0.3835536241531372\n",
      "Epoch 2505, Loss: 0.9423880577087402, Final Batch Loss: 0.30053651332855225\n",
      "Epoch 2506, Loss: 0.9628339409828186, Final Batch Loss: 0.29445332288742065\n",
      "Epoch 2507, Loss: 1.007582187652588, Final Batch Loss: 0.3031018078327179\n",
      "Epoch 2508, Loss: 1.0031365752220154, Final Batch Loss: 0.37833014130592346\n",
      "Epoch 2509, Loss: 0.9966035187244415, Final Batch Loss: 0.36843031644821167\n",
      "Epoch 2510, Loss: 0.9998445212841034, Final Batch Loss: 0.30293112993240356\n",
      "Epoch 2511, Loss: 0.9385192692279816, Final Batch Loss: 0.2507161796092987\n",
      "Epoch 2512, Loss: 1.0810921490192413, Final Batch Loss: 0.47489407658576965\n",
      "Epoch 2513, Loss: 0.9671648740768433, Final Batch Loss: 0.3986435532569885\n",
      "Epoch 2514, Loss: 1.0281909108161926, Final Batch Loss: 0.33869102597236633\n",
      "Epoch 2515, Loss: 0.9863793253898621, Final Batch Loss: 0.3271386921405792\n",
      "Epoch 2516, Loss: 0.9382223188877106, Final Batch Loss: 0.31120455265045166\n",
      "Epoch 2517, Loss: 0.9321493804454803, Final Batch Loss: 0.3223687708377838\n",
      "Epoch 2518, Loss: 1.0095406472682953, Final Batch Loss: 0.2663320302963257\n",
      "Epoch 2519, Loss: 1.0045520663261414, Final Batch Loss: 0.41464516520500183\n",
      "Epoch 2520, Loss: 0.8759321868419647, Final Batch Loss: 0.25900059938430786\n",
      "Epoch 2521, Loss: 0.9941588342189789, Final Batch Loss: 0.35343682765960693\n",
      "Epoch 2522, Loss: 0.9777269959449768, Final Batch Loss: 0.28414252400398254\n",
      "Epoch 2523, Loss: 1.064417690038681, Final Batch Loss: 0.3395630121231079\n",
      "Epoch 2524, Loss: 0.9649736285209656, Final Batch Loss: 0.37376388907432556\n",
      "Epoch 2525, Loss: 1.0148367881774902, Final Batch Loss: 0.3073025643825531\n",
      "Epoch 2526, Loss: 1.053166687488556, Final Batch Loss: 0.4084886312484741\n",
      "Epoch 2527, Loss: 0.963711142539978, Final Batch Loss: 0.26374194025993347\n",
      "Epoch 2528, Loss: 1.1746258735656738, Final Batch Loss: 0.4021381139755249\n",
      "Epoch 2529, Loss: 1.0352308750152588, Final Batch Loss: 0.3988305330276489\n",
      "Epoch 2530, Loss: 1.0004279017448425, Final Batch Loss: 0.3082748055458069\n",
      "Epoch 2531, Loss: 0.9381562173366547, Final Batch Loss: 0.2892327904701233\n",
      "Epoch 2532, Loss: 0.9697318077087402, Final Batch Loss: 0.2931763827800751\n",
      "Epoch 2533, Loss: 0.9819731414318085, Final Batch Loss: 0.29270413517951965\n",
      "Epoch 2534, Loss: 1.0198936760425568, Final Batch Loss: 0.2965231239795685\n",
      "Epoch 2535, Loss: 0.9074365496635437, Final Batch Loss: 0.3084894120693207\n",
      "Epoch 2536, Loss: 0.9376212656497955, Final Batch Loss: 0.2981441915035248\n",
      "Epoch 2537, Loss: 0.9278108477592468, Final Batch Loss: 0.2764361798763275\n",
      "Epoch 2538, Loss: 0.9524005353450775, Final Batch Loss: 0.3244558870792389\n",
      "Epoch 2539, Loss: 1.0384722650051117, Final Batch Loss: 0.32718756794929504\n",
      "Epoch 2540, Loss: 1.051542967557907, Final Batch Loss: 0.4244249165058136\n",
      "Epoch 2541, Loss: 0.9922526776790619, Final Batch Loss: 0.3726961314678192\n",
      "Epoch 2542, Loss: 1.0355684459209442, Final Batch Loss: 0.3910695016384125\n",
      "Epoch 2543, Loss: 1.0165874660015106, Final Batch Loss: 0.3339954912662506\n",
      "Epoch 2544, Loss: 0.9569829702377319, Final Batch Loss: 0.354422003030777\n",
      "Epoch 2545, Loss: 1.0094129741191864, Final Batch Loss: 0.3409861624240875\n",
      "Epoch 2546, Loss: 0.9375785738229752, Final Batch Loss: 0.24814878404140472\n",
      "Epoch 2547, Loss: 1.0031215846538544, Final Batch Loss: 0.3349144756793976\n",
      "Epoch 2548, Loss: 1.0013582408428192, Final Batch Loss: 0.42326170206069946\n",
      "Epoch 2549, Loss: 1.1394807994365692, Final Batch Loss: 0.47433966398239136\n",
      "Epoch 2550, Loss: 0.9314133822917938, Final Batch Loss: 0.3735892176628113\n",
      "Epoch 2551, Loss: 0.9018677473068237, Final Batch Loss: 0.22750213742256165\n",
      "Epoch 2552, Loss: 0.9398230016231537, Final Batch Loss: 0.36595618724823\n",
      "Epoch 2553, Loss: 0.9127020537853241, Final Batch Loss: 0.28361043334007263\n",
      "Epoch 2554, Loss: 1.0489098131656647, Final Batch Loss: 0.3999559283256531\n",
      "Epoch 2555, Loss: 0.9527391940355301, Final Batch Loss: 0.24039621651172638\n",
      "Epoch 2556, Loss: 0.9898980259895325, Final Batch Loss: 0.3248460590839386\n",
      "Epoch 2557, Loss: 0.9931105673313141, Final Batch Loss: 0.37868884205818176\n",
      "Epoch 2558, Loss: 1.1253870725631714, Final Batch Loss: 0.3790074586868286\n",
      "Epoch 2559, Loss: 1.0061114132404327, Final Batch Loss: 0.38068458437919617\n",
      "Epoch 2560, Loss: 1.0889824628829956, Final Batch Loss: 0.44269055128097534\n",
      "Epoch 2561, Loss: 1.017942100763321, Final Batch Loss: 0.3416885733604431\n",
      "Epoch 2562, Loss: 0.99151211977005, Final Batch Loss: 0.3556326627731323\n",
      "Epoch 2563, Loss: 0.9526140093803406, Final Batch Loss: 0.2823949158191681\n",
      "Epoch 2564, Loss: 0.9391277134418488, Final Batch Loss: 0.2610802948474884\n",
      "Epoch 2565, Loss: 1.065649837255478, Final Batch Loss: 0.2798805832862854\n",
      "Epoch 2566, Loss: 0.970164567232132, Final Batch Loss: 0.32674822211265564\n",
      "Epoch 2567, Loss: 1.0121752321720123, Final Batch Loss: 0.28936299681663513\n",
      "Epoch 2568, Loss: 1.0924898087978363, Final Batch Loss: 0.36243054270744324\n",
      "Epoch 2569, Loss: 1.0094155669212341, Final Batch Loss: 0.30642634630203247\n",
      "Epoch 2570, Loss: 1.0412361323833466, Final Batch Loss: 0.4248190224170685\n",
      "Epoch 2571, Loss: 0.9780480563640594, Final Batch Loss: 0.2892277240753174\n",
      "Epoch 2572, Loss: 1.070792704820633, Final Batch Loss: 0.4023205637931824\n",
      "Epoch 2573, Loss: 0.9780615866184235, Final Batch Loss: 0.3247460722923279\n",
      "Epoch 2574, Loss: 0.9554120302200317, Final Batch Loss: 0.2768918573856354\n",
      "Epoch 2575, Loss: 0.9920728802680969, Final Batch Loss: 0.3098532557487488\n",
      "Epoch 2576, Loss: 0.9982345104217529, Final Batch Loss: 0.3333992660045624\n",
      "Epoch 2577, Loss: 1.0267888009548187, Final Batch Loss: 0.37658894062042236\n",
      "Epoch 2578, Loss: 0.950341671705246, Final Batch Loss: 0.3220067322254181\n",
      "Epoch 2579, Loss: 0.9560251533985138, Final Batch Loss: 0.2858414351940155\n",
      "Epoch 2580, Loss: 0.9377428591251373, Final Batch Loss: 0.2929481267929077\n",
      "Epoch 2581, Loss: 0.8836039304733276, Final Batch Loss: 0.2674068212509155\n",
      "Epoch 2582, Loss: 0.9568003118038177, Final Batch Loss: 0.2735474109649658\n",
      "Epoch 2583, Loss: 1.0172260105609894, Final Batch Loss: 0.27626320719718933\n",
      "Epoch 2584, Loss: 1.0312657356262207, Final Batch Loss: 0.38342124223709106\n",
      "Epoch 2585, Loss: 0.9037148952484131, Final Batch Loss: 0.32518765330314636\n",
      "Epoch 2586, Loss: 1.0269839465618134, Final Batch Loss: 0.30633604526519775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2587, Loss: 1.038361370563507, Final Batch Loss: 0.3547765612602234\n",
      "Epoch 2588, Loss: 0.9947422295808792, Final Batch Loss: 0.42771396040916443\n",
      "Epoch 2589, Loss: 1.0633017718791962, Final Batch Loss: 0.264486700296402\n",
      "Epoch 2590, Loss: 1.0431574881076813, Final Batch Loss: 0.39621466398239136\n",
      "Epoch 2591, Loss: 0.9523726105690002, Final Batch Loss: 0.3270082473754883\n",
      "Epoch 2592, Loss: 0.9430164694786072, Final Batch Loss: 0.303436279296875\n",
      "Epoch 2593, Loss: 1.0862692892551422, Final Batch Loss: 0.3293645679950714\n",
      "Epoch 2594, Loss: 0.9821389019489288, Final Batch Loss: 0.29844263195991516\n",
      "Epoch 2595, Loss: 0.9773320853710175, Final Batch Loss: 0.3211440443992615\n",
      "Epoch 2596, Loss: 1.097634732723236, Final Batch Loss: 0.31272196769714355\n",
      "Epoch 2597, Loss: 0.9335982799530029, Final Batch Loss: 0.2854456305503845\n",
      "Epoch 2598, Loss: 1.003570318222046, Final Batch Loss: 0.34939998388290405\n",
      "Epoch 2599, Loss: 1.0082283318042755, Final Batch Loss: 0.3542921245098114\n",
      "Epoch 2600, Loss: 1.1298656463623047, Final Batch Loss: 0.41001495718955994\n",
      "Epoch 2601, Loss: 0.9142560362815857, Final Batch Loss: 0.3121316730976105\n",
      "Epoch 2602, Loss: 0.9677218198776245, Final Batch Loss: 0.2905690371990204\n",
      "Epoch 2603, Loss: 0.9690643846988678, Final Batch Loss: 0.3181464374065399\n",
      "Epoch 2604, Loss: 0.8587684482336044, Final Batch Loss: 0.22292225062847137\n",
      "Epoch 2605, Loss: 1.051003098487854, Final Batch Loss: 0.38574445247650146\n",
      "Epoch 2606, Loss: 0.9610565900802612, Final Batch Loss: 0.35966846346855164\n",
      "Epoch 2607, Loss: 0.9882251620292664, Final Batch Loss: 0.34418773651123047\n",
      "Epoch 2608, Loss: 1.0527527928352356, Final Batch Loss: 0.27875736355781555\n",
      "Epoch 2609, Loss: 0.9146769642829895, Final Batch Loss: 0.2517770528793335\n",
      "Epoch 2610, Loss: 1.034155249595642, Final Batch Loss: 0.38939031958580017\n",
      "Epoch 2611, Loss: 1.0643905103206635, Final Batch Loss: 0.37629613280296326\n",
      "Epoch 2612, Loss: 1.0126986801624298, Final Batch Loss: 0.32053595781326294\n",
      "Epoch 2613, Loss: 0.9631564617156982, Final Batch Loss: 0.32280871272087097\n",
      "Epoch 2614, Loss: 1.0361088514328003, Final Batch Loss: 0.3321424424648285\n",
      "Epoch 2615, Loss: 1.0295466184616089, Final Batch Loss: 0.3443410396575928\n",
      "Epoch 2616, Loss: 0.8989531546831131, Final Batch Loss: 0.23635534942150116\n",
      "Epoch 2617, Loss: 0.9416689872741699, Final Batch Loss: 0.27497726678848267\n",
      "Epoch 2618, Loss: 0.9497722089290619, Final Batch Loss: 0.3083256781101227\n",
      "Epoch 2619, Loss: 1.0190049707889557, Final Batch Loss: 0.3504917025566101\n",
      "Epoch 2620, Loss: 1.061656504869461, Final Batch Loss: 0.3748949468135834\n",
      "Epoch 2621, Loss: 0.9444107562303543, Final Batch Loss: 0.24961797893047333\n",
      "Epoch 2622, Loss: 0.9028096050024033, Final Batch Loss: 0.20411022007465363\n",
      "Epoch 2623, Loss: 1.0322091281414032, Final Batch Loss: 0.32067030668258667\n",
      "Epoch 2624, Loss: 0.9686567187309265, Final Batch Loss: 0.3019255995750427\n",
      "Epoch 2625, Loss: 0.9264262318611145, Final Batch Loss: 0.22970381379127502\n",
      "Epoch 2626, Loss: 0.9283445775508881, Final Batch Loss: 0.2732136845588684\n",
      "Epoch 2627, Loss: 0.9062850177288055, Final Batch Loss: 0.3012138307094574\n",
      "Epoch 2628, Loss: 0.9653225243091583, Final Batch Loss: 0.3688157796859741\n",
      "Epoch 2629, Loss: 0.9385102689266205, Final Batch Loss: 0.32516807317733765\n",
      "Epoch 2630, Loss: 1.0305298566818237, Final Batch Loss: 0.42589837312698364\n",
      "Epoch 2631, Loss: 0.981423944234848, Final Batch Loss: 0.3705946207046509\n",
      "Epoch 2632, Loss: 1.0104869604110718, Final Batch Loss: 0.36916106939315796\n",
      "Epoch 2633, Loss: 1.0598121881484985, Final Batch Loss: 0.42761510610580444\n",
      "Epoch 2634, Loss: 1.0017829239368439, Final Batch Loss: 0.33766400814056396\n",
      "Epoch 2635, Loss: 0.9387438297271729, Final Batch Loss: 0.29700160026550293\n",
      "Epoch 2636, Loss: 1.1132715046405792, Final Batch Loss: 0.36316946148872375\n",
      "Epoch 2637, Loss: 0.9746299088001251, Final Batch Loss: 0.3304700553417206\n",
      "Epoch 2638, Loss: 0.9106990098953247, Final Batch Loss: 0.2505967915058136\n",
      "Epoch 2639, Loss: 0.9629504978656769, Final Batch Loss: 0.37025347352027893\n",
      "Epoch 2640, Loss: 0.9931623637676239, Final Batch Loss: 0.2723380923271179\n",
      "Epoch 2641, Loss: 1.0832476913928986, Final Batch Loss: 0.33508768677711487\n",
      "Epoch 2642, Loss: 0.9542544484138489, Final Batch Loss: 0.2998327314853668\n",
      "Epoch 2643, Loss: 1.0729254186153412, Final Batch Loss: 0.4182458817958832\n",
      "Epoch 2644, Loss: 0.9779171347618103, Final Batch Loss: 0.3528960347175598\n",
      "Epoch 2645, Loss: 1.0277524888515472, Final Batch Loss: 0.3221980929374695\n",
      "Epoch 2646, Loss: 1.0346916317939758, Final Batch Loss: 0.39118075370788574\n",
      "Epoch 2647, Loss: 0.9831993579864502, Final Batch Loss: 0.31536173820495605\n",
      "Epoch 2648, Loss: 0.9925558269023895, Final Batch Loss: 0.30648073554039\n",
      "Epoch 2649, Loss: 0.9505225718021393, Final Batch Loss: 0.3226699233055115\n",
      "Epoch 2650, Loss: 0.8850367814302444, Final Batch Loss: 0.19141878187656403\n",
      "Epoch 2651, Loss: 1.0022549033164978, Final Batch Loss: 0.3838426470756531\n",
      "Epoch 2652, Loss: 0.9614801704883575, Final Batch Loss: 0.3054186999797821\n",
      "Epoch 2653, Loss: 1.0008708536624908, Final Batch Loss: 0.3249970078468323\n",
      "Epoch 2654, Loss: 0.9724121689796448, Final Batch Loss: 0.3582887649536133\n",
      "Epoch 2655, Loss: 0.8659434467554092, Final Batch Loss: 0.24574138224124908\n",
      "Epoch 2656, Loss: 0.9537248313426971, Final Batch Loss: 0.38240933418273926\n",
      "Epoch 2657, Loss: 0.9155086874961853, Final Batch Loss: 0.2594476640224457\n",
      "Epoch 2658, Loss: 1.0214896500110626, Final Batch Loss: 0.30912381410598755\n",
      "Epoch 2659, Loss: 0.9637147337198257, Final Batch Loss: 0.23309426009655\n",
      "Epoch 2660, Loss: 0.9077171385288239, Final Batch Loss: 0.2889353334903717\n",
      "Epoch 2661, Loss: 0.9126324653625488, Final Batch Loss: 0.29015374183654785\n",
      "Epoch 2662, Loss: 0.8661519289016724, Final Batch Loss: 0.2594694197177887\n",
      "Epoch 2663, Loss: 0.8527944833040237, Final Batch Loss: 0.24878282845020294\n",
      "Epoch 2664, Loss: 0.9782541394233704, Final Batch Loss: 0.30163317918777466\n",
      "Epoch 2665, Loss: 0.9452714920043945, Final Batch Loss: 0.3421759307384491\n",
      "Epoch 2666, Loss: 0.9317249357700348, Final Batch Loss: 0.2979777455329895\n",
      "Epoch 2667, Loss: 0.9930219948291779, Final Batch Loss: 0.3522596061229706\n",
      "Epoch 2668, Loss: 0.9361360371112823, Final Batch Loss: 0.27821409702301025\n",
      "Epoch 2669, Loss: 0.9113701283931732, Final Batch Loss: 0.25742897391319275\n",
      "Epoch 2670, Loss: 1.1255268454551697, Final Batch Loss: 0.44028714299201965\n",
      "Epoch 2671, Loss: 0.9953423738479614, Final Batch Loss: 0.37930214405059814\n",
      "Epoch 2672, Loss: 1.0202446579933167, Final Batch Loss: 0.3876085877418518\n",
      "Epoch 2673, Loss: 1.0338935256004333, Final Batch Loss: 0.33443114161491394\n",
      "Epoch 2674, Loss: 0.9957097768783569, Final Batch Loss: 0.3689163625240326\n",
      "Epoch 2675, Loss: 0.9153950214385986, Final Batch Loss: 0.3061372637748718\n",
      "Epoch 2676, Loss: 1.0022704750299454, Final Batch Loss: 0.24354244768619537\n",
      "Epoch 2677, Loss: 0.9909496605396271, Final Batch Loss: 0.31888169050216675\n",
      "Epoch 2678, Loss: 1.028768628835678, Final Batch Loss: 0.3812617063522339\n",
      "Epoch 2679, Loss: 0.9863532185554504, Final Batch Loss: 0.31521716713905334\n",
      "Epoch 2680, Loss: 0.8714025914669037, Final Batch Loss: 0.3039267957210541\n",
      "Epoch 2681, Loss: 0.9555750787258148, Final Batch Loss: 0.2880060076713562\n",
      "Epoch 2682, Loss: 0.9766163527965546, Final Batch Loss: 0.19407504796981812\n",
      "Epoch 2683, Loss: 1.0172869265079498, Final Batch Loss: 0.2793598771095276\n",
      "Epoch 2684, Loss: 0.9925248324871063, Final Batch Loss: 0.3085959255695343\n",
      "Epoch 2685, Loss: 0.9237354099750519, Final Batch Loss: 0.3250977098941803\n",
      "Epoch 2686, Loss: 1.0050767064094543, Final Batch Loss: 0.35699573159217834\n",
      "Epoch 2687, Loss: 0.8921446800231934, Final Batch Loss: 0.3079328238964081\n",
      "Epoch 2688, Loss: 0.922250360250473, Final Batch Loss: 0.2800668478012085\n",
      "Epoch 2689, Loss: 0.9466420114040375, Final Batch Loss: 0.24582573771476746\n",
      "Epoch 2690, Loss: 0.9200368523597717, Final Batch Loss: 0.28218379616737366\n",
      "Epoch 2691, Loss: 0.9382546544075012, Final Batch Loss: 0.31716927886009216\n",
      "Epoch 2692, Loss: 0.9435747265815735, Final Batch Loss: 0.36677730083465576\n",
      "Epoch 2693, Loss: 0.9714610874652863, Final Batch Loss: 0.3962661623954773\n",
      "Epoch 2694, Loss: 1.028326392173767, Final Batch Loss: 0.44455158710479736\n",
      "Epoch 2695, Loss: 0.9189991652965546, Final Batch Loss: 0.24285012483596802\n",
      "Epoch 2696, Loss: 0.9736660122871399, Final Batch Loss: 0.3140519857406616\n",
      "Epoch 2697, Loss: 0.9416427314281464, Final Batch Loss: 0.32717087864875793\n",
      "Epoch 2698, Loss: 0.9146183133125305, Final Batch Loss: 0.2519321143627167\n",
      "Epoch 2699, Loss: 1.0852930545806885, Final Batch Loss: 0.40348494052886963\n",
      "Epoch 2700, Loss: 0.9422775506973267, Final Batch Loss: 0.25661608576774597\n",
      "Epoch 2701, Loss: 0.9298117458820343, Final Batch Loss: 0.3396647572517395\n",
      "Epoch 2702, Loss: 1.0205929279327393, Final Batch Loss: 0.4417695999145508\n",
      "Epoch 2703, Loss: 1.0295147597789764, Final Batch Loss: 0.2914477288722992\n",
      "Epoch 2704, Loss: 1.0617228150367737, Final Batch Loss: 0.37407878041267395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2705, Loss: 1.0537500977516174, Final Batch Loss: 0.3683496117591858\n",
      "Epoch 2706, Loss: 1.0012539327144623, Final Batch Loss: 0.3410177528858185\n",
      "Epoch 2707, Loss: 0.9433229863643646, Final Batch Loss: 0.28943538665771484\n",
      "Epoch 2708, Loss: 0.9592659771442413, Final Batch Loss: 0.34667420387268066\n",
      "Epoch 2709, Loss: 0.9311603307723999, Final Batch Loss: 0.3011130392551422\n",
      "Epoch 2710, Loss: 0.9791388809680939, Final Batch Loss: 0.3236328065395355\n",
      "Epoch 2711, Loss: 0.9823164641857147, Final Batch Loss: 0.264631986618042\n",
      "Epoch 2712, Loss: 0.9554345309734344, Final Batch Loss: 0.3687906563282013\n",
      "Epoch 2713, Loss: 1.0453040599822998, Final Batch Loss: 0.37620389461517334\n",
      "Epoch 2714, Loss: 1.0328470468521118, Final Batch Loss: 0.33382171392440796\n",
      "Epoch 2715, Loss: 0.9434504508972168, Final Batch Loss: 0.27167877554893494\n",
      "Epoch 2716, Loss: 1.0625883042812347, Final Batch Loss: 0.38213881850242615\n",
      "Epoch 2717, Loss: 1.0436716973781586, Final Batch Loss: 0.4239804744720459\n",
      "Epoch 2718, Loss: 0.9042344093322754, Final Batch Loss: 0.290199875831604\n",
      "Epoch 2719, Loss: 0.9104067981243134, Final Batch Loss: 0.282229483127594\n",
      "Epoch 2720, Loss: 0.9869018495082855, Final Batch Loss: 0.3969925045967102\n",
      "Epoch 2721, Loss: 0.8896485567092896, Final Batch Loss: 0.2694896459579468\n",
      "Epoch 2722, Loss: 0.9877442717552185, Final Batch Loss: 0.3616653084754944\n",
      "Epoch 2723, Loss: 0.9113694131374359, Final Batch Loss: 0.2645534873008728\n",
      "Epoch 2724, Loss: 0.9774289131164551, Final Batch Loss: 0.33310869336128235\n",
      "Epoch 2725, Loss: 0.9983011782169342, Final Batch Loss: 0.43252381682395935\n",
      "Epoch 2726, Loss: 0.9515822231769562, Final Batch Loss: 0.30742335319519043\n",
      "Epoch 2727, Loss: 1.0631307065486908, Final Batch Loss: 0.2781085968017578\n",
      "Epoch 2728, Loss: 0.9251008331775665, Final Batch Loss: 0.2894487977027893\n",
      "Epoch 2729, Loss: 0.9815320074558258, Final Batch Loss: 0.3317738473415375\n",
      "Epoch 2730, Loss: 0.9288235902786255, Final Batch Loss: 0.30590981245040894\n",
      "Epoch 2731, Loss: 1.0200770199298859, Final Batch Loss: 0.323557585477829\n",
      "Epoch 2732, Loss: 0.9124467074871063, Final Batch Loss: 0.3434053957462311\n",
      "Epoch 2733, Loss: 0.9519242346286774, Final Batch Loss: 0.35753247141838074\n",
      "Epoch 2734, Loss: 1.0267303586006165, Final Batch Loss: 0.3916112184524536\n",
      "Epoch 2735, Loss: 0.9661267399787903, Final Batch Loss: 0.36399391293525696\n",
      "Epoch 2736, Loss: 1.016448974609375, Final Batch Loss: 0.3776845932006836\n",
      "Epoch 2737, Loss: 1.03126659989357, Final Batch Loss: 0.3530735671520233\n",
      "Epoch 2738, Loss: 0.9531484544277191, Final Batch Loss: 0.29100531339645386\n",
      "Epoch 2739, Loss: 1.0138488411903381, Final Batch Loss: 0.30839890241622925\n",
      "Epoch 2740, Loss: 0.9964769780635834, Final Batch Loss: 0.32294780015945435\n",
      "Epoch 2741, Loss: 0.9969450235366821, Final Batch Loss: 0.3268907368183136\n",
      "Epoch 2742, Loss: 0.9741583168506622, Final Batch Loss: 0.3527469336986542\n",
      "Epoch 2743, Loss: 0.892981231212616, Final Batch Loss: 0.23550346493721008\n",
      "Epoch 2744, Loss: 0.9047429859638214, Final Batch Loss: 0.28893032670021057\n",
      "Epoch 2745, Loss: 1.0282323956489563, Final Batch Loss: 0.380447119474411\n",
      "Epoch 2746, Loss: 0.9147438406944275, Final Batch Loss: 0.30862295627593994\n",
      "Epoch 2747, Loss: 0.8868216574192047, Final Batch Loss: 0.24939602613449097\n",
      "Epoch 2748, Loss: 0.940516322851181, Final Batch Loss: 0.28613078594207764\n",
      "Epoch 2749, Loss: 0.9262590408325195, Final Batch Loss: 0.3152649700641632\n",
      "Epoch 2750, Loss: 0.9732993245124817, Final Batch Loss: 0.33999520540237427\n",
      "Epoch 2751, Loss: 1.017197459936142, Final Batch Loss: 0.3049202263355255\n",
      "Epoch 2752, Loss: 0.9125319719314575, Final Batch Loss: 0.28560391068458557\n",
      "Epoch 2753, Loss: 0.8659892827272415, Final Batch Loss: 0.3355540335178375\n",
      "Epoch 2754, Loss: 0.9294430911540985, Final Batch Loss: 0.3202688694000244\n",
      "Epoch 2755, Loss: 0.9826814234256744, Final Batch Loss: 0.36023005843162537\n",
      "Epoch 2756, Loss: 0.9150656461715698, Final Batch Loss: 0.2832285463809967\n",
      "Epoch 2757, Loss: 0.9536409676074982, Final Batch Loss: 0.3230017125606537\n",
      "Epoch 2758, Loss: 0.8294217884540558, Final Batch Loss: 0.2516500949859619\n",
      "Epoch 2759, Loss: 0.9667924046516418, Final Batch Loss: 0.29650622606277466\n",
      "Epoch 2760, Loss: 0.9125527739524841, Final Batch Loss: 0.3101429343223572\n",
      "Epoch 2761, Loss: 0.894000768661499, Final Batch Loss: 0.2626543939113617\n",
      "Epoch 2762, Loss: 1.009477138519287, Final Batch Loss: 0.30538588762283325\n",
      "Epoch 2763, Loss: 0.9259896576404572, Final Batch Loss: 0.2884596586227417\n",
      "Epoch 2764, Loss: 1.0077684819698334, Final Batch Loss: 0.35763585567474365\n",
      "Epoch 2765, Loss: 0.9639551043510437, Final Batch Loss: 0.32956865429878235\n",
      "Epoch 2766, Loss: 0.8463544547557831, Final Batch Loss: 0.30744224786758423\n",
      "Epoch 2767, Loss: 0.9637241959571838, Final Batch Loss: 0.33454659581184387\n",
      "Epoch 2768, Loss: 0.8901963979005814, Final Batch Loss: 0.24407629668712616\n",
      "Epoch 2769, Loss: 0.9544233679771423, Final Batch Loss: 0.4007074534893036\n",
      "Epoch 2770, Loss: 0.9704366624355316, Final Batch Loss: 0.30571287870407104\n",
      "Epoch 2771, Loss: 0.9308948218822479, Final Batch Loss: 0.25060713291168213\n",
      "Epoch 2772, Loss: 0.8903363943099976, Final Batch Loss: 0.2001846432685852\n",
      "Epoch 2773, Loss: 0.9444913566112518, Final Batch Loss: 0.3159130811691284\n",
      "Epoch 2774, Loss: 1.0586735010147095, Final Batch Loss: 0.4313011169433594\n",
      "Epoch 2775, Loss: 0.8244479447603226, Final Batch Loss: 0.21130801737308502\n",
      "Epoch 2776, Loss: 0.8973026871681213, Final Batch Loss: 0.29729393124580383\n",
      "Epoch 2777, Loss: 0.9245959520339966, Final Batch Loss: 0.28126832842826843\n",
      "Epoch 2778, Loss: 1.0145410001277924, Final Batch Loss: 0.3601418733596802\n",
      "Epoch 2779, Loss: 0.9336065500974655, Final Batch Loss: 0.20289267599582672\n",
      "Epoch 2780, Loss: 1.0570232570171356, Final Batch Loss: 0.34505540132522583\n",
      "Epoch 2781, Loss: 0.931145191192627, Final Batch Loss: 0.2590652108192444\n",
      "Epoch 2782, Loss: 0.9394045621156693, Final Batch Loss: 0.20349149405956268\n",
      "Epoch 2783, Loss: 0.9310648739337921, Final Batch Loss: 0.3132396638393402\n",
      "Epoch 2784, Loss: 0.9002954065799713, Final Batch Loss: 0.30987465381622314\n",
      "Epoch 2785, Loss: 0.9445570409297943, Final Batch Loss: 0.34619808197021484\n",
      "Epoch 2786, Loss: 0.8986562043428421, Final Batch Loss: 0.22282563149929047\n",
      "Epoch 2787, Loss: 0.9811867773532867, Final Batch Loss: 0.3283713459968567\n",
      "Epoch 2788, Loss: 0.9302381575107574, Final Batch Loss: 0.2640027403831482\n",
      "Epoch 2789, Loss: 0.9128450453281403, Final Batch Loss: 0.24768349528312683\n",
      "Epoch 2790, Loss: 1.0365361869335175, Final Batch Loss: 0.3180956542491913\n",
      "Epoch 2791, Loss: 0.9463286399841309, Final Batch Loss: 0.2542092204093933\n",
      "Epoch 2792, Loss: 1.013754278421402, Final Batch Loss: 0.34952470660209656\n",
      "Epoch 2793, Loss: 0.8819669336080551, Final Batch Loss: 0.29782208800315857\n",
      "Epoch 2794, Loss: 1.0077094435691833, Final Batch Loss: 0.3926064074039459\n",
      "Epoch 2795, Loss: 1.010080099105835, Final Batch Loss: 0.3274223208427429\n",
      "Epoch 2796, Loss: 1.0292743742465973, Final Batch Loss: 0.4018961787223816\n",
      "Epoch 2797, Loss: 0.9709750264883041, Final Batch Loss: 0.23843128979206085\n",
      "Epoch 2798, Loss: 0.9504327178001404, Final Batch Loss: 0.3160924017429352\n",
      "Epoch 2799, Loss: 0.9724737107753754, Final Batch Loss: 0.2972632050514221\n",
      "Epoch 2800, Loss: 0.8638339340686798, Final Batch Loss: 0.2523166835308075\n",
      "Epoch 2801, Loss: 0.9290019273757935, Final Batch Loss: 0.25974535942077637\n",
      "Epoch 2802, Loss: 1.1266059577465057, Final Batch Loss: 0.3885689973831177\n",
      "Epoch 2803, Loss: 0.8922438621520996, Final Batch Loss: 0.2872215509414673\n",
      "Epoch 2804, Loss: 0.9234516620635986, Final Batch Loss: 0.310911625623703\n",
      "Epoch 2805, Loss: 0.8054555952548981, Final Batch Loss: 0.25493404269218445\n",
      "Epoch 2806, Loss: 1.0021401345729828, Final Batch Loss: 0.34013691544532776\n",
      "Epoch 2807, Loss: 0.980485200881958, Final Batch Loss: 0.3695031702518463\n",
      "Epoch 2808, Loss: 0.8726851046085358, Final Batch Loss: 0.22553449869155884\n",
      "Epoch 2809, Loss: 0.9794667363166809, Final Batch Loss: 0.27629736065864563\n",
      "Epoch 2810, Loss: 0.9743801951408386, Final Batch Loss: 0.3546583354473114\n",
      "Epoch 2811, Loss: 1.11184623837471, Final Batch Loss: 0.4492417871952057\n",
      "Epoch 2812, Loss: 0.9881599843502045, Final Batch Loss: 0.3459506630897522\n",
      "Epoch 2813, Loss: 0.9874056577682495, Final Batch Loss: 0.36575785279273987\n",
      "Epoch 2814, Loss: 0.9284478724002838, Final Batch Loss: 0.33708497881889343\n",
      "Epoch 2815, Loss: 0.9516425728797913, Final Batch Loss: 0.3489270806312561\n",
      "Epoch 2816, Loss: 1.0274487733840942, Final Batch Loss: 0.38924726843833923\n",
      "Epoch 2817, Loss: 1.0472163259983063, Final Batch Loss: 0.2914159893989563\n",
      "Epoch 2818, Loss: 0.8855439722537994, Final Batch Loss: 0.25601962208747864\n",
      "Epoch 2819, Loss: 0.861495703458786, Final Batch Loss: 0.2553933262825012\n",
      "Epoch 2820, Loss: 0.9519136846065521, Final Batch Loss: 0.3087405860424042\n",
      "Epoch 2821, Loss: 0.9046724140644073, Final Batch Loss: 0.2852299213409424\n",
      "Epoch 2822, Loss: 0.9624541997909546, Final Batch Loss: 0.3451984226703644\n",
      "Epoch 2823, Loss: 0.9358291327953339, Final Batch Loss: 0.3143298327922821\n",
      "Epoch 2824, Loss: 0.9820477068424225, Final Batch Loss: 0.35187700390815735\n",
      "Epoch 2825, Loss: 0.9385984539985657, Final Batch Loss: 0.27470824122428894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2826, Loss: 0.9551512598991394, Final Batch Loss: 0.3385005593299866\n",
      "Epoch 2827, Loss: 0.9623027443885803, Final Batch Loss: 0.22143879532814026\n",
      "Epoch 2828, Loss: 0.8962249755859375, Final Batch Loss: 0.27223360538482666\n",
      "Epoch 2829, Loss: 0.9573745429515839, Final Batch Loss: 0.3278796970844269\n",
      "Epoch 2830, Loss: 0.974366307258606, Final Batch Loss: 0.3051273226737976\n",
      "Epoch 2831, Loss: 0.9555200338363647, Final Batch Loss: 0.34762415289878845\n",
      "Epoch 2832, Loss: 1.0153533816337585, Final Batch Loss: 0.3281228840351105\n",
      "Epoch 2833, Loss: 0.8399378806352615, Final Batch Loss: 0.23122094571590424\n",
      "Epoch 2834, Loss: 1.0385884642601013, Final Batch Loss: 0.32071325182914734\n",
      "Epoch 2835, Loss: 0.9272826313972473, Final Batch Loss: 0.18892842531204224\n",
      "Epoch 2836, Loss: 0.8844020068645477, Final Batch Loss: 0.3079591691493988\n",
      "Epoch 2837, Loss: 1.0384545028209686, Final Batch Loss: 0.4050320088863373\n",
      "Epoch 2838, Loss: 1.0927722454071045, Final Batch Loss: 0.3985455632209778\n",
      "Epoch 2839, Loss: 0.9018682241439819, Final Batch Loss: 0.2625804841518402\n",
      "Epoch 2840, Loss: 0.9347434639930725, Final Batch Loss: 0.32123446464538574\n",
      "Epoch 2841, Loss: 0.9639458954334259, Final Batch Loss: 0.3333522379398346\n",
      "Epoch 2842, Loss: 0.9789680242538452, Final Batch Loss: 0.3469735383987427\n",
      "Epoch 2843, Loss: 0.9018608927726746, Final Batch Loss: 0.25521552562713623\n",
      "Epoch 2844, Loss: 1.0899173319339752, Final Batch Loss: 0.428716778755188\n",
      "Epoch 2845, Loss: 0.9294565618038177, Final Batch Loss: 0.3067868649959564\n",
      "Epoch 2846, Loss: 0.9897512197494507, Final Batch Loss: 0.3620949983596802\n",
      "Epoch 2847, Loss: 0.893377348780632, Final Batch Loss: 0.23706655204296112\n",
      "Epoch 2848, Loss: 0.9816372990608215, Final Batch Loss: 0.33954206109046936\n",
      "Epoch 2849, Loss: 0.9136527180671692, Final Batch Loss: 0.32670414447784424\n",
      "Epoch 2850, Loss: 0.9344058632850647, Final Batch Loss: 0.31213870644569397\n",
      "Epoch 2851, Loss: 0.9641338586807251, Final Batch Loss: 0.3308188021183014\n",
      "Epoch 2852, Loss: 0.867216020822525, Final Batch Loss: 0.28076350688934326\n",
      "Epoch 2853, Loss: 0.9566886126995087, Final Batch Loss: 0.3405553102493286\n",
      "Epoch 2854, Loss: 0.9778805673122406, Final Batch Loss: 0.38793981075286865\n",
      "Epoch 2855, Loss: 0.9004493802785873, Final Batch Loss: 0.24999122321605682\n",
      "Epoch 2856, Loss: 0.9046023488044739, Final Batch Loss: 0.27321740984916687\n",
      "Epoch 2857, Loss: 0.8758749663829803, Final Batch Loss: 0.1859062910079956\n",
      "Epoch 2858, Loss: 0.8339011520147324, Final Batch Loss: 0.2900717258453369\n",
      "Epoch 2859, Loss: 0.9483670592308044, Final Batch Loss: 0.3347455859184265\n",
      "Epoch 2860, Loss: 0.9073486328125, Final Batch Loss: 0.26437458395957947\n",
      "Epoch 2861, Loss: 0.8880753219127655, Final Batch Loss: 0.2820274233818054\n",
      "Epoch 2862, Loss: 0.9000042080879211, Final Batch Loss: 0.320882648229599\n",
      "Epoch 2863, Loss: 0.9847920835018158, Final Batch Loss: 0.35522565245628357\n",
      "Epoch 2864, Loss: 0.9270322322845459, Final Batch Loss: 0.29399996995925903\n",
      "Epoch 2865, Loss: 0.8466815799474716, Final Batch Loss: 0.23486687242984772\n",
      "Epoch 2866, Loss: 0.9165363907814026, Final Batch Loss: 0.22824999690055847\n",
      "Epoch 2867, Loss: 0.8651158362627029, Final Batch Loss: 0.17671750485897064\n",
      "Epoch 2868, Loss: 0.9690994620323181, Final Batch Loss: 0.3408927917480469\n",
      "Epoch 2869, Loss: 0.9121424853801727, Final Batch Loss: 0.29436999559402466\n",
      "Epoch 2870, Loss: 1.0882681906223297, Final Batch Loss: 0.46180176734924316\n",
      "Epoch 2871, Loss: 0.8464308530092239, Final Batch Loss: 0.2961914539337158\n",
      "Epoch 2872, Loss: 0.981938511133194, Final Batch Loss: 0.2816542685031891\n",
      "Epoch 2873, Loss: 0.9162852317094803, Final Batch Loss: 0.34469324350357056\n",
      "Epoch 2874, Loss: 1.037463366985321, Final Batch Loss: 0.3413828909397125\n",
      "Epoch 2875, Loss: 0.9043389111757278, Final Batch Loss: 0.21013759076595306\n",
      "Epoch 2876, Loss: 1.0369322299957275, Final Batch Loss: 0.34076225757598877\n",
      "Epoch 2877, Loss: 0.971834659576416, Final Batch Loss: 0.3335229754447937\n",
      "Epoch 2878, Loss: 0.9967888295650482, Final Batch Loss: 0.33126768469810486\n",
      "Epoch 2879, Loss: 0.9739002585411072, Final Batch Loss: 0.34504252672195435\n",
      "Epoch 2880, Loss: 1.040063589811325, Final Batch Loss: 0.40430542826652527\n",
      "Epoch 2881, Loss: 0.9384335577487946, Final Batch Loss: 0.27973082661628723\n",
      "Epoch 2882, Loss: 0.9675014019012451, Final Batch Loss: 0.2806028425693512\n",
      "Epoch 2883, Loss: 0.9912194311618805, Final Batch Loss: 0.27115345001220703\n",
      "Epoch 2884, Loss: 0.9276408851146698, Final Batch Loss: 0.26080265641212463\n",
      "Epoch 2885, Loss: 0.8639696538448334, Final Batch Loss: 0.2571653723716736\n",
      "Epoch 2886, Loss: 0.901816725730896, Final Batch Loss: 0.28360840678215027\n",
      "Epoch 2887, Loss: 0.9561770558357239, Final Batch Loss: 0.3110823333263397\n",
      "Epoch 2888, Loss: 1.0812599062919617, Final Batch Loss: 0.5128398537635803\n",
      "Epoch 2889, Loss: 0.9594731032848358, Final Batch Loss: 0.35475853085517883\n",
      "Epoch 2890, Loss: 1.03106090426445, Final Batch Loss: 0.2839021682739258\n",
      "Epoch 2891, Loss: 0.9008221626281738, Final Batch Loss: 0.28938037157058716\n",
      "Epoch 2892, Loss: 1.028370201587677, Final Batch Loss: 0.4073140323162079\n",
      "Epoch 2893, Loss: 0.9900147914886475, Final Batch Loss: 0.3083501160144806\n",
      "Epoch 2894, Loss: 0.9340628385543823, Final Batch Loss: 0.33117738366127014\n",
      "Epoch 2895, Loss: 0.9466808140277863, Final Batch Loss: 0.3184649646282196\n",
      "Epoch 2896, Loss: 0.9442709684371948, Final Batch Loss: 0.34334927797317505\n",
      "Epoch 2897, Loss: 0.9796421229839325, Final Batch Loss: 0.2981003522872925\n",
      "Epoch 2898, Loss: 1.0491945445537567, Final Batch Loss: 0.37172964215278625\n",
      "Epoch 2899, Loss: 1.067215234041214, Final Batch Loss: 0.4396199882030487\n",
      "Epoch 2900, Loss: 0.955344021320343, Final Batch Loss: 0.3377397060394287\n",
      "Epoch 2901, Loss: 0.9436987340450287, Final Batch Loss: 0.2149258553981781\n",
      "Epoch 2902, Loss: 0.8479199260473251, Final Batch Loss: 0.21080408990383148\n",
      "Epoch 2903, Loss: 0.9820045232772827, Final Batch Loss: 0.293688029050827\n",
      "Epoch 2904, Loss: 0.9326935112476349, Final Batch Loss: 0.2905096411705017\n",
      "Epoch 2905, Loss: 0.9075652211904526, Final Batch Loss: 0.35631462931632996\n",
      "Epoch 2906, Loss: 1.01617893576622, Final Batch Loss: 0.3005894720554352\n",
      "Epoch 2907, Loss: 0.8340564668178558, Final Batch Loss: 0.23098061978816986\n",
      "Epoch 2908, Loss: 0.9001628458499908, Final Batch Loss: 0.28521832823753357\n",
      "Epoch 2909, Loss: 0.8897539675235748, Final Batch Loss: 0.33995288610458374\n",
      "Epoch 2910, Loss: 0.9312619566917419, Final Batch Loss: 0.3212965726852417\n",
      "Epoch 2911, Loss: 0.9576399922370911, Final Batch Loss: 0.33549848198890686\n",
      "Epoch 2912, Loss: 0.8761887848377228, Final Batch Loss: 0.3123689889907837\n",
      "Epoch 2913, Loss: 0.9004471004009247, Final Batch Loss: 0.28533852100372314\n",
      "Epoch 2914, Loss: 0.8844071924686432, Final Batch Loss: 0.2048300802707672\n",
      "Epoch 2915, Loss: 0.8261784017086029, Final Batch Loss: 0.15307998657226562\n",
      "Epoch 2916, Loss: 0.9420657753944397, Final Batch Loss: 0.3000597357749939\n",
      "Epoch 2917, Loss: 0.9638583660125732, Final Batch Loss: 0.3624440133571625\n",
      "Epoch 2918, Loss: 0.9945210516452789, Final Batch Loss: 0.3990029990673065\n",
      "Epoch 2919, Loss: 0.8884153962135315, Final Batch Loss: 0.25066354870796204\n",
      "Epoch 2920, Loss: 0.8591454327106476, Final Batch Loss: 0.2642180025577545\n",
      "Epoch 2921, Loss: 0.9008468687534332, Final Batch Loss: 0.2955503761768341\n",
      "Epoch 2922, Loss: 1.0249915421009064, Final Batch Loss: 0.3902500569820404\n",
      "Epoch 2923, Loss: 1.0132432579994202, Final Batch Loss: 0.3876713514328003\n",
      "Epoch 2924, Loss: 0.9217800796031952, Final Batch Loss: 0.2782827913761139\n",
      "Epoch 2925, Loss: 0.8451293110847473, Final Batch Loss: 0.2844298779964447\n",
      "Epoch 2926, Loss: 1.0219372808933258, Final Batch Loss: 0.38461652398109436\n",
      "Epoch 2927, Loss: 0.9047931134700775, Final Batch Loss: 0.32395705580711365\n",
      "Epoch 2928, Loss: 0.9666965007781982, Final Batch Loss: 0.32626184821128845\n",
      "Epoch 2929, Loss: 0.8418096601963043, Final Batch Loss: 0.224992573261261\n",
      "Epoch 2930, Loss: 1.0299950242042542, Final Batch Loss: 0.3904620110988617\n",
      "Epoch 2931, Loss: 0.9433387517929077, Final Batch Loss: 0.3696521818637848\n",
      "Epoch 2932, Loss: 0.8743730634450912, Final Batch Loss: 0.24216164648532867\n",
      "Epoch 2933, Loss: 0.9922085106372833, Final Batch Loss: 0.2975175976753235\n",
      "Epoch 2934, Loss: 0.9430506527423859, Final Batch Loss: 0.3541852533817291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2935, Loss: 0.9251402616500854, Final Batch Loss: 0.3466838598251343\n",
      "Epoch 2936, Loss: 0.9175924956798553, Final Batch Loss: 0.2545575499534607\n",
      "Epoch 2937, Loss: 0.9543562233448029, Final Batch Loss: 0.29517966508865356\n",
      "Epoch 2938, Loss: 0.9597639441490173, Final Batch Loss: 0.29543107748031616\n",
      "Epoch 2939, Loss: 0.9667491316795349, Final Batch Loss: 0.366110235452652\n",
      "Epoch 2940, Loss: 0.9023755937814713, Final Batch Loss: 0.24482251703739166\n",
      "Epoch 2941, Loss: 1.0004869401454926, Final Batch Loss: 0.40069833397865295\n",
      "Epoch 2942, Loss: 1.035016804933548, Final Batch Loss: 0.29967910051345825\n",
      "Epoch 2943, Loss: 0.9538534283638, Final Batch Loss: 0.25937044620513916\n",
      "Epoch 2944, Loss: 0.9408433735370636, Final Batch Loss: 0.2599834203720093\n",
      "Epoch 2945, Loss: 0.9225284159183502, Final Batch Loss: 0.2576553225517273\n",
      "Epoch 2946, Loss: 0.929583728313446, Final Batch Loss: 0.3194669783115387\n",
      "Epoch 2947, Loss: 1.0823359489440918, Final Batch Loss: 0.4754677414894104\n",
      "Epoch 2948, Loss: 0.8640685379505157, Final Batch Loss: 0.2689749598503113\n",
      "Epoch 2949, Loss: 1.0454888939857483, Final Batch Loss: 0.36275455355644226\n",
      "Epoch 2950, Loss: 0.9825563132762909, Final Batch Loss: 0.35000497102737427\n",
      "Epoch 2951, Loss: 0.9037414342164993, Final Batch Loss: 0.24471549689769745\n",
      "Epoch 2952, Loss: 0.9487678110599518, Final Batch Loss: 0.30893340706825256\n",
      "Epoch 2953, Loss: 0.9822984635829926, Final Batch Loss: 0.42732736468315125\n",
      "Epoch 2954, Loss: 1.1387964189052582, Final Batch Loss: 0.5101016163825989\n",
      "Epoch 2955, Loss: 0.8962172269821167, Final Batch Loss: 0.2165161669254303\n",
      "Epoch 2956, Loss: 0.9170927703380585, Final Batch Loss: 0.32247087359428406\n",
      "Epoch 2957, Loss: 0.8825282752513885, Final Batch Loss: 0.23844781517982483\n",
      "Epoch 2958, Loss: 0.8344316929578781, Final Batch Loss: 0.24865023791790009\n",
      "Epoch 2959, Loss: 1.0363163650035858, Final Batch Loss: 0.36701032519340515\n",
      "Epoch 2960, Loss: 0.8856852948665619, Final Batch Loss: 0.3234352767467499\n",
      "Epoch 2961, Loss: 0.9394802153110504, Final Batch Loss: 0.36301329731941223\n",
      "Epoch 2962, Loss: 1.0450811088085175, Final Batch Loss: 0.3335769474506378\n",
      "Epoch 2963, Loss: 0.9348281621932983, Final Batch Loss: 0.1763072907924652\n",
      "Epoch 2964, Loss: 0.9433113932609558, Final Batch Loss: 0.3554057776927948\n",
      "Epoch 2965, Loss: 0.9410605728626251, Final Batch Loss: 0.27204152941703796\n",
      "Epoch 2966, Loss: 0.8765541911125183, Final Batch Loss: 0.29728102684020996\n",
      "Epoch 2967, Loss: 0.8906924426555634, Final Batch Loss: 0.29796507954597473\n",
      "Epoch 2968, Loss: 0.8504893332719803, Final Batch Loss: 0.22999681532382965\n",
      "Epoch 2969, Loss: 0.8523825705051422, Final Batch Loss: 0.3201632797718048\n",
      "Epoch 2970, Loss: 0.9726942777633667, Final Batch Loss: 0.40230220556259155\n",
      "Epoch 2971, Loss: 0.9414772093296051, Final Batch Loss: 0.30370980501174927\n",
      "Epoch 2972, Loss: 0.9447841942310333, Final Batch Loss: 0.2785953879356384\n",
      "Epoch 2973, Loss: 0.9305368363857269, Final Batch Loss: 0.31945177912712097\n",
      "Epoch 2974, Loss: 0.9587527811527252, Final Batch Loss: 0.3144872784614563\n",
      "Epoch 2975, Loss: 0.8509446978569031, Final Batch Loss: 0.19196227192878723\n",
      "Epoch 2976, Loss: 0.9175488948822021, Final Batch Loss: 0.3603893518447876\n",
      "Epoch 2977, Loss: 0.9784438610076904, Final Batch Loss: 0.32939985394477844\n",
      "Epoch 2978, Loss: 0.9441653192043304, Final Batch Loss: 0.2495320439338684\n",
      "Epoch 2979, Loss: 0.9390581846237183, Final Batch Loss: 0.3447472155094147\n",
      "Epoch 2980, Loss: 0.9654764831066132, Final Batch Loss: 0.28896206617355347\n",
      "Epoch 2981, Loss: 0.8765814304351807, Final Batch Loss: 0.2997536063194275\n",
      "Epoch 2982, Loss: 0.9350625276565552, Final Batch Loss: 0.3868926167488098\n",
      "Epoch 2983, Loss: 0.8996628820896149, Final Batch Loss: 0.25271832942962646\n",
      "Epoch 2984, Loss: 0.8917399942874908, Final Batch Loss: 0.2754444181919098\n",
      "Epoch 2985, Loss: 1.0654821693897247, Final Batch Loss: 0.40132787823677063\n",
      "Epoch 2986, Loss: 0.8804617375135422, Final Batch Loss: 0.24814213812351227\n",
      "Epoch 2987, Loss: 0.9452991783618927, Final Batch Loss: 0.31909435987472534\n",
      "Epoch 2988, Loss: 0.8633257448673248, Final Batch Loss: 0.29578620195388794\n",
      "Epoch 2989, Loss: 0.8942391276359558, Final Batch Loss: 0.31756922602653503\n",
      "Epoch 2990, Loss: 0.9191009998321533, Final Batch Loss: 0.3033756613731384\n",
      "Epoch 2991, Loss: 0.8337763696908951, Final Batch Loss: 0.23926673829555511\n",
      "Epoch 2992, Loss: 1.0265788733959198, Final Batch Loss: 0.33352184295654297\n",
      "Epoch 2993, Loss: 0.8364515155553818, Final Batch Loss: 0.22918732464313507\n",
      "Epoch 2994, Loss: 1.0907354950904846, Final Batch Loss: 0.4756191372871399\n",
      "Epoch 2995, Loss: 0.8302191495895386, Final Batch Loss: 0.26353397965431213\n",
      "Epoch 2996, Loss: 0.9461773931980133, Final Batch Loss: 0.34953072667121887\n",
      "Epoch 2997, Loss: 0.9624865651130676, Final Batch Loss: 0.4495292007923126\n",
      "Epoch 2998, Loss: 0.8851081132888794, Final Batch Loss: 0.3088437616825104\n",
      "Epoch 2999, Loss: 0.9304624795913696, Final Batch Loss: 0.3166414499282837\n",
      "Epoch 3000, Loss: 0.8924264311790466, Final Batch Loss: 0.2639709711074829\n",
      "Epoch 3001, Loss: 1.0668879747390747, Final Batch Loss: 0.4411335587501526\n",
      "Epoch 3002, Loss: 0.8826805949211121, Final Batch Loss: 0.28999900817871094\n",
      "Epoch 3003, Loss: 0.9021194279193878, Final Batch Loss: 0.2830284535884857\n",
      "Epoch 3004, Loss: 0.8689596354961395, Final Batch Loss: 0.34127405285835266\n",
      "Epoch 3005, Loss: 0.983028769493103, Final Batch Loss: 0.31358078122138977\n",
      "Epoch 3006, Loss: 1.0238756239414215, Final Batch Loss: 0.34981483221054077\n",
      "Epoch 3007, Loss: 0.9452953934669495, Final Batch Loss: 0.34122681617736816\n",
      "Epoch 3008, Loss: 0.8907208144664764, Final Batch Loss: 0.27571678161621094\n",
      "Epoch 3009, Loss: 0.9298439025878906, Final Batch Loss: 0.32014232873916626\n",
      "Epoch 3010, Loss: 0.8560847342014313, Final Batch Loss: 0.2779453992843628\n",
      "Epoch 3011, Loss: 0.9388845562934875, Final Batch Loss: 0.341170072555542\n",
      "Epoch 3012, Loss: 0.9992302060127258, Final Batch Loss: 0.3706015646457672\n",
      "Epoch 3013, Loss: 0.8391232043504715, Final Batch Loss: 0.2241811603307724\n",
      "Epoch 3014, Loss: 0.9832329452037811, Final Batch Loss: 0.394172340631485\n",
      "Epoch 3015, Loss: 0.9505287706851959, Final Batch Loss: 0.3414413332939148\n",
      "Epoch 3016, Loss: 0.843583881855011, Final Batch Loss: 0.28424072265625\n",
      "Epoch 3017, Loss: 0.9559408128261566, Final Batch Loss: 0.28702235221862793\n",
      "Epoch 3018, Loss: 0.9320513308048248, Final Batch Loss: 0.32498660683631897\n",
      "Epoch 3019, Loss: 0.8836365938186646, Final Batch Loss: 0.28181716799736023\n",
      "Epoch 3020, Loss: 0.9856688678264618, Final Batch Loss: 0.3420254588127136\n",
      "Epoch 3021, Loss: 0.9089068919420242, Final Batch Loss: 0.249766007065773\n",
      "Epoch 3022, Loss: 0.9085103124380112, Final Batch Loss: 0.38917818665504456\n",
      "Epoch 3023, Loss: 0.8788297772407532, Final Batch Loss: 0.26639485359191895\n",
      "Epoch 3024, Loss: 0.8485507667064667, Final Batch Loss: 0.266519159078598\n",
      "Epoch 3025, Loss: 0.9565292298793793, Final Batch Loss: 0.3575371503829956\n",
      "Epoch 3026, Loss: 0.9794492721557617, Final Batch Loss: 0.38276636600494385\n",
      "Epoch 3027, Loss: 0.8336972296237946, Final Batch Loss: 0.25242096185684204\n",
      "Epoch 3028, Loss: 0.8815643638372421, Final Batch Loss: 0.2246035784482956\n",
      "Epoch 3029, Loss: 0.796483963727951, Final Batch Loss: 0.20457220077514648\n",
      "Epoch 3030, Loss: 0.9560344815254211, Final Batch Loss: 0.3376796245574951\n",
      "Epoch 3031, Loss: 0.9868569076061249, Final Batch Loss: 0.3228430449962616\n",
      "Epoch 3032, Loss: 0.8382259458303452, Final Batch Loss: 0.21804587543010712\n",
      "Epoch 3033, Loss: 0.8591496050357819, Final Batch Loss: 0.2767032980918884\n",
      "Epoch 3034, Loss: 0.9017752110958099, Final Batch Loss: 0.2934766709804535\n",
      "Epoch 3035, Loss: 0.949741780757904, Final Batch Loss: 0.2827361226081848\n",
      "Epoch 3036, Loss: 0.9540657103061676, Final Batch Loss: 0.3570350408554077\n",
      "Epoch 3037, Loss: 0.8504025489091873, Final Batch Loss: 0.24938566982746124\n",
      "Epoch 3038, Loss: 0.8890732228755951, Final Batch Loss: 0.28855133056640625\n",
      "Epoch 3039, Loss: 0.9783214330673218, Final Batch Loss: 0.3715195059776306\n",
      "Epoch 3040, Loss: 0.9033906161785126, Final Batch Loss: 0.3021131753921509\n",
      "Epoch 3041, Loss: 0.9632315933704376, Final Batch Loss: 0.35173681378364563\n",
      "Epoch 3042, Loss: 0.8914593160152435, Final Batch Loss: 0.27983659505844116\n",
      "Epoch 3043, Loss: 0.8543443977832794, Final Batch Loss: 0.2888070046901703\n",
      "Epoch 3044, Loss: 0.9592034965753555, Final Batch Loss: 0.36287540197372437\n",
      "Epoch 3045, Loss: 0.9773834347724915, Final Batch Loss: 0.3376513123512268\n",
      "Epoch 3046, Loss: 0.8650483191013336, Final Batch Loss: 0.2794167697429657\n",
      "Epoch 3047, Loss: 0.8444169908761978, Final Batch Loss: 0.24970664083957672\n",
      "Epoch 3048, Loss: 0.8931903392076492, Final Batch Loss: 0.23358343541622162\n",
      "Epoch 3049, Loss: 0.9237806499004364, Final Batch Loss: 0.35570812225341797\n",
      "Epoch 3050, Loss: 0.8680733144283295, Final Batch Loss: 0.32692375779151917\n",
      "Epoch 3051, Loss: 0.8933258652687073, Final Batch Loss: 0.2524321377277374\n",
      "Epoch 3052, Loss: 0.9727723300457001, Final Batch Loss: 0.38380560278892517\n",
      "Epoch 3053, Loss: 0.8682775497436523, Final Batch Loss: 0.26490479707717896\n",
      "Epoch 3054, Loss: 1.1435782313346863, Final Batch Loss: 0.3386036455631256\n",
      "Epoch 3055, Loss: 0.9307983815670013, Final Batch Loss: 0.31640011072158813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3056, Loss: 0.920884370803833, Final Batch Loss: 0.3384336829185486\n",
      "Epoch 3057, Loss: 0.8133509308099747, Final Batch Loss: 0.2747343182563782\n",
      "Epoch 3058, Loss: 0.7761757522821426, Final Batch Loss: 0.22625233232975006\n",
      "Epoch 3059, Loss: 0.9838606715202332, Final Batch Loss: 0.3557375371456146\n",
      "Epoch 3060, Loss: 0.9000604450702667, Final Batch Loss: 0.29997384548187256\n",
      "Epoch 3061, Loss: 0.9217325150966644, Final Batch Loss: 0.36567479372024536\n",
      "Epoch 3062, Loss: 0.9488590657711029, Final Batch Loss: 0.28243449330329895\n",
      "Epoch 3063, Loss: 0.8918924927711487, Final Batch Loss: 0.3384377360343933\n",
      "Epoch 3064, Loss: 0.9597290754318237, Final Batch Loss: 0.37049928307533264\n",
      "Epoch 3065, Loss: 0.8781103193759918, Final Batch Loss: 0.3068508207798004\n",
      "Epoch 3066, Loss: 0.8731978535652161, Final Batch Loss: 0.33468541502952576\n",
      "Epoch 3067, Loss: 0.9174471199512482, Final Batch Loss: 0.29058343172073364\n",
      "Epoch 3068, Loss: 0.9258885979652405, Final Batch Loss: 0.3618922829627991\n",
      "Epoch 3069, Loss: 1.0193870663642883, Final Batch Loss: 0.4123130440711975\n",
      "Epoch 3070, Loss: 1.0298360884189606, Final Batch Loss: 0.30282121896743774\n",
      "Epoch 3071, Loss: 0.8532573580741882, Final Batch Loss: 0.33235427737236023\n",
      "Epoch 3072, Loss: 0.9082769155502319, Final Batch Loss: 0.2738683521747589\n",
      "Epoch 3073, Loss: 0.9821115732192993, Final Batch Loss: 0.3337301015853882\n",
      "Epoch 3074, Loss: 0.8522130250930786, Final Batch Loss: 0.2468690276145935\n",
      "Epoch 3075, Loss: 0.9692080318927765, Final Batch Loss: 0.3222229778766632\n",
      "Epoch 3076, Loss: 0.9868809580802917, Final Batch Loss: 0.32404443621635437\n",
      "Epoch 3077, Loss: 0.9355156421661377, Final Batch Loss: 0.34657084941864014\n",
      "Epoch 3078, Loss: 0.9060530364513397, Final Batch Loss: 0.28584304451942444\n",
      "Epoch 3079, Loss: 0.9605287611484528, Final Batch Loss: 0.4320836365222931\n",
      "Epoch 3080, Loss: 0.90647292137146, Final Batch Loss: 0.27013882994651794\n",
      "Epoch 3081, Loss: 0.8814883232116699, Final Batch Loss: 0.29588064551353455\n",
      "Epoch 3082, Loss: 0.8434624373912811, Final Batch Loss: 0.266459196805954\n",
      "Epoch 3083, Loss: 0.9050847291946411, Final Batch Loss: 0.28881290555000305\n",
      "Epoch 3084, Loss: 0.9255729615688324, Final Batch Loss: 0.3294610381126404\n",
      "Epoch 3085, Loss: 0.8776793926954269, Final Batch Loss: 0.3475467264652252\n",
      "Epoch 3086, Loss: 0.8815256357192993, Final Batch Loss: 0.3151722550392151\n",
      "Epoch 3087, Loss: 0.8951759189367294, Final Batch Loss: 0.21578951179981232\n",
      "Epoch 3088, Loss: 1.0060935020446777, Final Batch Loss: 0.34055057168006897\n",
      "Epoch 3089, Loss: 0.8652919083833694, Final Batch Loss: 0.3003654181957245\n",
      "Epoch 3090, Loss: 0.8796364367008209, Final Batch Loss: 0.2880571484565735\n",
      "Epoch 3091, Loss: 0.9362920224666595, Final Batch Loss: 0.2625351846218109\n",
      "Epoch 3092, Loss: 0.8246799111366272, Final Batch Loss: 0.2244178056716919\n",
      "Epoch 3093, Loss: 0.9274408221244812, Final Batch Loss: 0.35689425468444824\n",
      "Epoch 3094, Loss: 0.9295676648616791, Final Batch Loss: 0.35845044255256653\n",
      "Epoch 3095, Loss: 0.916991800069809, Final Batch Loss: 0.2894725799560547\n",
      "Epoch 3096, Loss: 0.9412599802017212, Final Batch Loss: 0.32431527972221375\n",
      "Epoch 3097, Loss: 0.9174513816833496, Final Batch Loss: 0.2577279508113861\n",
      "Epoch 3098, Loss: 0.90640988945961, Final Batch Loss: 0.333366334438324\n",
      "Epoch 3099, Loss: 0.9087058007717133, Final Batch Loss: 0.333057165145874\n",
      "Epoch 3100, Loss: 0.9469945430755615, Final Batch Loss: 0.3123137354850769\n",
      "Epoch 3101, Loss: 0.9396163523197174, Final Batch Loss: 0.2774408757686615\n",
      "Epoch 3102, Loss: 0.9296588152647018, Final Batch Loss: 0.24391646683216095\n",
      "Epoch 3103, Loss: 1.0076550543308258, Final Batch Loss: 0.3340301513671875\n",
      "Epoch 3104, Loss: 0.9547984302043915, Final Batch Loss: 0.4010432958602905\n",
      "Epoch 3105, Loss: 0.940589964389801, Final Batch Loss: 0.3118238151073456\n",
      "Epoch 3106, Loss: 0.8920747935771942, Final Batch Loss: 0.2594529986381531\n",
      "Epoch 3107, Loss: 0.923574686050415, Final Batch Loss: 0.3802790939807892\n",
      "Epoch 3108, Loss: 0.8958214819431305, Final Batch Loss: 0.3591462969779968\n",
      "Epoch 3109, Loss: 0.8930906653404236, Final Batch Loss: 0.23597848415374756\n",
      "Epoch 3110, Loss: 1.0082871615886688, Final Batch Loss: 0.37535032629966736\n",
      "Epoch 3111, Loss: 0.9316492080688477, Final Batch Loss: 0.25565800070762634\n",
      "Epoch 3112, Loss: 0.9663687944412231, Final Batch Loss: 0.31086379289627075\n",
      "Epoch 3113, Loss: 0.9141751825809479, Final Batch Loss: 0.29034340381622314\n",
      "Epoch 3114, Loss: 0.9554882347583771, Final Batch Loss: 0.29944688081741333\n",
      "Epoch 3115, Loss: 0.9725041687488556, Final Batch Loss: 0.3316744267940521\n",
      "Epoch 3116, Loss: 0.8381934463977814, Final Batch Loss: 0.25062355399131775\n",
      "Epoch 3117, Loss: 0.953481525182724, Final Batch Loss: 0.330500066280365\n",
      "Epoch 3118, Loss: 0.8723823726177216, Final Batch Loss: 0.25870510935783386\n",
      "Epoch 3119, Loss: 0.8572291433811188, Final Batch Loss: 0.25823572278022766\n",
      "Epoch 3120, Loss: 0.852151945233345, Final Batch Loss: 0.23948051035404205\n",
      "Epoch 3121, Loss: 0.8615990579128265, Final Batch Loss: 0.2876932621002197\n",
      "Epoch 3122, Loss: 0.9750904440879822, Final Batch Loss: 0.3341798186302185\n",
      "Epoch 3123, Loss: 0.9327201545238495, Final Batch Loss: 0.32692497968673706\n",
      "Epoch 3124, Loss: 0.8542298674583435, Final Batch Loss: 0.2535736858844757\n",
      "Epoch 3125, Loss: 0.8594577312469482, Final Batch Loss: 0.29478180408477783\n",
      "Epoch 3126, Loss: 0.8778422772884369, Final Batch Loss: 0.26583003997802734\n",
      "Epoch 3127, Loss: 0.9170659482479095, Final Batch Loss: 0.2429267168045044\n",
      "Epoch 3128, Loss: 0.9180148243904114, Final Batch Loss: 0.29866036772727966\n",
      "Epoch 3129, Loss: 0.7921212464570999, Final Batch Loss: 0.19712482392787933\n",
      "Epoch 3130, Loss: 0.8874712884426117, Final Batch Loss: 0.34478190541267395\n",
      "Epoch 3131, Loss: 0.8898430466651917, Final Batch Loss: 0.2721788287162781\n",
      "Epoch 3132, Loss: 0.7923717945814133, Final Batch Loss: 0.2917484641075134\n",
      "Epoch 3133, Loss: 0.8927968442440033, Final Batch Loss: 0.28839409351348877\n",
      "Epoch 3134, Loss: 0.9479809999465942, Final Batch Loss: 0.35738006234169006\n",
      "Epoch 3135, Loss: 0.9519689977169037, Final Batch Loss: 0.31352344155311584\n",
      "Epoch 3136, Loss: 0.9313995242118835, Final Batch Loss: 0.28519126772880554\n",
      "Epoch 3137, Loss: 0.9592038691043854, Final Batch Loss: 0.3033989667892456\n",
      "Epoch 3138, Loss: 1.0454105734825134, Final Batch Loss: 0.3628290593624115\n",
      "Epoch 3139, Loss: 0.9180402755737305, Final Batch Loss: 0.32824382185935974\n",
      "Epoch 3140, Loss: 0.8823933303356171, Final Batch Loss: 0.30078595876693726\n",
      "Epoch 3141, Loss: 0.7947159111499786, Final Batch Loss: 0.25881099700927734\n",
      "Epoch 3142, Loss: 0.8018937259912491, Final Batch Loss: 0.18716730177402496\n",
      "Epoch 3143, Loss: 0.9538128972053528, Final Batch Loss: 0.353534460067749\n",
      "Epoch 3144, Loss: 0.9109732210636139, Final Batch Loss: 0.2939959168434143\n",
      "Epoch 3145, Loss: 0.9863536357879639, Final Batch Loss: 0.4005880653858185\n",
      "Epoch 3146, Loss: 0.9662302434444427, Final Batch Loss: 0.3475174605846405\n",
      "Epoch 3147, Loss: 0.9139093160629272, Final Batch Loss: 0.3522183299064636\n",
      "Epoch 3148, Loss: 0.895163893699646, Final Batch Loss: 0.2774446904659271\n",
      "Epoch 3149, Loss: 0.8553561568260193, Final Batch Loss: 0.2323373258113861\n",
      "Epoch 3150, Loss: 0.9862290024757385, Final Batch Loss: 0.34077757596969604\n",
      "Epoch 3151, Loss: 0.9745475649833679, Final Batch Loss: 0.349811315536499\n",
      "Epoch 3152, Loss: 0.9385566711425781, Final Batch Loss: 0.2839761972427368\n",
      "Epoch 3153, Loss: 0.9278405606746674, Final Batch Loss: 0.37051668763160706\n",
      "Epoch 3154, Loss: 0.8384695798158646, Final Batch Loss: 0.2173851877450943\n",
      "Epoch 3155, Loss: 0.7863387763500214, Final Batch Loss: 0.25313904881477356\n",
      "Epoch 3156, Loss: 0.8844693899154663, Final Batch Loss: 0.26238635182380676\n",
      "Epoch 3157, Loss: 0.8482075482606888, Final Batch Loss: 0.30082955956459045\n",
      "Epoch 3158, Loss: 0.857766717672348, Final Batch Loss: 0.271292120218277\n",
      "Epoch 3159, Loss: 0.9186583161354065, Final Batch Loss: 0.33235594630241394\n",
      "Epoch 3160, Loss: 0.9822561144828796, Final Batch Loss: 0.37980252504348755\n",
      "Epoch 3161, Loss: 0.8780193328857422, Final Batch Loss: 0.2600381076335907\n",
      "Epoch 3162, Loss: 0.8799820244312286, Final Batch Loss: 0.2979148328304291\n",
      "Epoch 3163, Loss: 0.8570320010185242, Final Batch Loss: 0.25677862763404846\n",
      "Epoch 3164, Loss: 0.9050194472074509, Final Batch Loss: 0.23802010715007782\n",
      "Epoch 3165, Loss: 0.8000452369451523, Final Batch Loss: 0.2163277417421341\n",
      "Epoch 3166, Loss: 0.8099845498800278, Final Batch Loss: 0.2646615207195282\n",
      "Epoch 3167, Loss: 0.9083659052848816, Final Batch Loss: 0.3178041875362396\n",
      "Epoch 3168, Loss: 0.9056178629398346, Final Batch Loss: 0.3107883930206299\n",
      "Epoch 3169, Loss: 0.855610266327858, Final Batch Loss: 0.26964080333709717\n",
      "Epoch 3170, Loss: 0.8816099762916565, Final Batch Loss: 0.37234342098236084\n",
      "Epoch 3171, Loss: 0.8495350033044815, Final Batch Loss: 0.24143819510936737\n",
      "Epoch 3172, Loss: 0.9493960440158844, Final Batch Loss: 0.3522105813026428\n",
      "Epoch 3173, Loss: 0.8858643770217896, Final Batch Loss: 0.36415353417396545\n",
      "Epoch 3174, Loss: 0.9500188231468201, Final Batch Loss: 0.3371752202510834\n",
      "Epoch 3175, Loss: 0.884624183177948, Final Batch Loss: 0.323072612285614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3176, Loss: 0.8330144286155701, Final Batch Loss: 0.31441766023635864\n",
      "Epoch 3177, Loss: 0.8437692224979401, Final Batch Loss: 0.2787424325942993\n",
      "Epoch 3178, Loss: 0.8457551300525665, Final Batch Loss: 0.3018481731414795\n",
      "Epoch 3179, Loss: 0.9147273898124695, Final Batch Loss: 0.3294716775417328\n",
      "Epoch 3180, Loss: 1.0140248537063599, Final Batch Loss: 0.37043315172195435\n",
      "Epoch 3181, Loss: 0.8877156972885132, Final Batch Loss: 0.2014324963092804\n",
      "Epoch 3182, Loss: 0.7417725622653961, Final Batch Loss: 0.1679830700159073\n",
      "Epoch 3183, Loss: 0.8316937237977982, Final Batch Loss: 0.23996327817440033\n",
      "Epoch 3184, Loss: 0.9762759506702423, Final Batch Loss: 0.35834380984306335\n",
      "Epoch 3185, Loss: 0.9933525323867798, Final Batch Loss: 0.3885597288608551\n",
      "Epoch 3186, Loss: 0.9460147321224213, Final Batch Loss: 0.38147372007369995\n",
      "Epoch 3187, Loss: 0.8069976270198822, Final Batch Loss: 0.2618426978588104\n",
      "Epoch 3188, Loss: 0.7980950176715851, Final Batch Loss: 0.24675865471363068\n",
      "Epoch 3189, Loss: 0.8943037241697311, Final Batch Loss: 0.3019488453865051\n",
      "Epoch 3190, Loss: 0.9242740571498871, Final Batch Loss: 0.29430052638053894\n",
      "Epoch 3191, Loss: 0.9781879782676697, Final Batch Loss: 0.3687804937362671\n",
      "Epoch 3192, Loss: 0.8893724083900452, Final Batch Loss: 0.3274657726287842\n",
      "Epoch 3193, Loss: 0.8154979646205902, Final Batch Loss: 0.20423930883407593\n",
      "Epoch 3194, Loss: 0.7283033281564713, Final Batch Loss: 0.2200133055448532\n",
      "Epoch 3195, Loss: 1.002101182937622, Final Batch Loss: 0.33593595027923584\n",
      "Epoch 3196, Loss: 0.9630011916160583, Final Batch Loss: 0.3046683073043823\n",
      "Epoch 3197, Loss: 0.8692530691623688, Final Batch Loss: 0.21963471174240112\n",
      "Epoch 3198, Loss: 0.975538969039917, Final Batch Loss: 0.2824060916900635\n",
      "Epoch 3199, Loss: 0.9136085510253906, Final Batch Loss: 0.31848931312561035\n",
      "Epoch 3200, Loss: 0.9009886980056763, Final Batch Loss: 0.2799682319164276\n",
      "Epoch 3201, Loss: 0.8544221669435501, Final Batch Loss: 0.2824079692363739\n",
      "Epoch 3202, Loss: 0.9108156263828278, Final Batch Loss: 0.3039718270301819\n",
      "Epoch 3203, Loss: 0.9998887479305267, Final Batch Loss: 0.3872459828853607\n",
      "Epoch 3204, Loss: 0.9138695001602173, Final Batch Loss: 0.34349486231803894\n",
      "Epoch 3205, Loss: 0.9686715602874756, Final Batch Loss: 0.3246942460536957\n",
      "Epoch 3206, Loss: 0.9732668101787567, Final Batch Loss: 0.3876247704029083\n",
      "Epoch 3207, Loss: 0.8302271962165833, Final Batch Loss: 0.2751622498035431\n",
      "Epoch 3208, Loss: 0.789436861872673, Final Batch Loss: 0.23668743669986725\n",
      "Epoch 3209, Loss: 0.8022766560316086, Final Batch Loss: 0.22561196982860565\n",
      "Epoch 3210, Loss: 0.8089807033538818, Final Batch Loss: 0.2709825336933136\n",
      "Epoch 3211, Loss: 0.9132355153560638, Final Batch Loss: 0.34102654457092285\n",
      "Epoch 3212, Loss: 0.9746121168136597, Final Batch Loss: 0.36364755034446716\n",
      "Epoch 3213, Loss: 0.9510260224342346, Final Batch Loss: 0.39183294773101807\n",
      "Epoch 3214, Loss: 0.8056934773921967, Final Batch Loss: 0.2325642704963684\n",
      "Epoch 3215, Loss: 0.890838548541069, Final Batch Loss: 0.23521928489208221\n",
      "Epoch 3216, Loss: 0.8940465301275253, Final Batch Loss: 0.3187031149864197\n",
      "Epoch 3217, Loss: 0.944130927324295, Final Batch Loss: 0.26451653242111206\n",
      "Epoch 3218, Loss: 0.8881296813488007, Final Batch Loss: 0.2865549623966217\n",
      "Epoch 3219, Loss: 1.0165347158908844, Final Batch Loss: 0.3873051404953003\n",
      "Epoch 3220, Loss: 0.9022092521190643, Final Batch Loss: 0.2980940341949463\n",
      "Epoch 3221, Loss: 1.020177274942398, Final Batch Loss: 0.398238867521286\n",
      "Epoch 3222, Loss: 0.801843598484993, Final Batch Loss: 0.22987382113933563\n",
      "Epoch 3223, Loss: 0.9228666424751282, Final Batch Loss: 0.2818813621997833\n",
      "Epoch 3224, Loss: 0.8524308800697327, Final Batch Loss: 0.2453029453754425\n",
      "Epoch 3225, Loss: 0.9850526452064514, Final Batch Loss: 0.3845042586326599\n",
      "Epoch 3226, Loss: 0.9555118680000305, Final Batch Loss: 0.2927466630935669\n",
      "Epoch 3227, Loss: 0.9273602664470673, Final Batch Loss: 0.2826688587665558\n",
      "Epoch 3228, Loss: 0.9316409528255463, Final Batch Loss: 0.3698424994945526\n",
      "Epoch 3229, Loss: 0.9199849963188171, Final Batch Loss: 0.4006249010562897\n",
      "Epoch 3230, Loss: 0.9068888425827026, Final Batch Loss: 0.30784842371940613\n",
      "Epoch 3231, Loss: 0.8915624171495438, Final Batch Loss: 0.24550361931324005\n",
      "Epoch 3232, Loss: 0.8668812960386276, Final Batch Loss: 0.23688660562038422\n",
      "Epoch 3233, Loss: 0.9196291267871857, Final Batch Loss: 0.228603333234787\n",
      "Epoch 3234, Loss: 0.9069605767726898, Final Batch Loss: 0.3930608332157135\n",
      "Epoch 3235, Loss: 0.8923787474632263, Final Batch Loss: 0.3125038146972656\n",
      "Epoch 3236, Loss: 0.794531524181366, Final Batch Loss: 0.2535898685455322\n",
      "Epoch 3237, Loss: 0.9201491177082062, Final Batch Loss: 0.28317925333976746\n",
      "Epoch 3238, Loss: 0.9180063009262085, Final Batch Loss: 0.3125063180923462\n",
      "Epoch 3239, Loss: 0.8987946808338165, Final Batch Loss: 0.2816987931728363\n",
      "Epoch 3240, Loss: 0.8717437088489532, Final Batch Loss: 0.3055812120437622\n",
      "Epoch 3241, Loss: 0.7560785412788391, Final Batch Loss: 0.24111992120742798\n",
      "Epoch 3242, Loss: 0.8268173933029175, Final Batch Loss: 0.26145440340042114\n",
      "Epoch 3243, Loss: 0.8712049722671509, Final Batch Loss: 0.2570207715034485\n",
      "Epoch 3244, Loss: 0.9205187857151031, Final Batch Loss: 0.3270590007305145\n",
      "Epoch 3245, Loss: 0.9444742202758789, Final Batch Loss: 0.29239872097969055\n",
      "Epoch 3246, Loss: 0.9859185516834259, Final Batch Loss: 0.3008422553539276\n",
      "Epoch 3247, Loss: 0.8532013893127441, Final Batch Loss: 0.22222596406936646\n",
      "Epoch 3248, Loss: 0.9320391118526459, Final Batch Loss: 0.3084259629249573\n",
      "Epoch 3249, Loss: 0.9014642536640167, Final Batch Loss: 0.2961134612560272\n",
      "Epoch 3250, Loss: 0.8125999569892883, Final Batch Loss: 0.25027117133140564\n",
      "Epoch 3251, Loss: 0.8870119452476501, Final Batch Loss: 0.30237099528312683\n",
      "Epoch 3252, Loss: 0.8297866582870483, Final Batch Loss: 0.2289850115776062\n",
      "Epoch 3253, Loss: 0.8604829013347626, Final Batch Loss: 0.24820756912231445\n",
      "Epoch 3254, Loss: 0.8811025321483612, Final Batch Loss: 0.231174498796463\n",
      "Epoch 3255, Loss: 0.8973463475704193, Final Batch Loss: 0.28356462717056274\n",
      "Epoch 3256, Loss: 0.8328753411769867, Final Batch Loss: 0.3025509715080261\n",
      "Epoch 3257, Loss: 1.007288634777069, Final Batch Loss: 0.4028337597846985\n",
      "Epoch 3258, Loss: 0.8682256042957306, Final Batch Loss: 0.26555201411247253\n",
      "Epoch 3259, Loss: 0.8390942960977554, Final Batch Loss: 0.1809486299753189\n",
      "Epoch 3260, Loss: 0.8507040739059448, Final Batch Loss: 0.2540201246738434\n",
      "Epoch 3261, Loss: 0.8663577735424042, Final Batch Loss: 0.28990912437438965\n",
      "Epoch 3262, Loss: 0.9113661199808121, Final Batch Loss: 0.32663142681121826\n",
      "Epoch 3263, Loss: 0.8521483838558197, Final Batch Loss: 0.30335357785224915\n",
      "Epoch 3264, Loss: 0.7686675488948822, Final Batch Loss: 0.1522156000137329\n",
      "Epoch 3265, Loss: 0.9228216707706451, Final Batch Loss: 0.2942158579826355\n",
      "Epoch 3266, Loss: 0.8209341764450073, Final Batch Loss: 0.2419814169406891\n",
      "Epoch 3267, Loss: 0.8754872828722, Final Batch Loss: 0.31820183992385864\n",
      "Epoch 3268, Loss: 0.758284792304039, Final Batch Loss: 0.21059878170490265\n",
      "Epoch 3269, Loss: 0.8900124132633209, Final Batch Loss: 0.2863585650920868\n",
      "Epoch 3270, Loss: 0.9033873975276947, Final Batch Loss: 0.3133429288864136\n",
      "Epoch 3271, Loss: 0.8581964373588562, Final Batch Loss: 0.2732731103897095\n",
      "Epoch 3272, Loss: 0.9183228015899658, Final Batch Loss: 0.28313055634498596\n",
      "Epoch 3273, Loss: 0.9288736879825592, Final Batch Loss: 0.28825825452804565\n",
      "Epoch 3274, Loss: 0.9766283333301544, Final Batch Loss: 0.37992343306541443\n",
      "Epoch 3275, Loss: 0.9671910405158997, Final Batch Loss: 0.3146147131919861\n",
      "Epoch 3276, Loss: 0.8469807207584381, Final Batch Loss: 0.31238582730293274\n",
      "Epoch 3277, Loss: 0.9950956702232361, Final Batch Loss: 0.3499313294887543\n",
      "Epoch 3278, Loss: 0.8969220817089081, Final Batch Loss: 0.3434666693210602\n",
      "Epoch 3279, Loss: 0.7944202274084091, Final Batch Loss: 0.19808073341846466\n",
      "Epoch 3280, Loss: 0.9544880837202072, Final Batch Loss: 0.37545710802078247\n",
      "Epoch 3281, Loss: 0.8682277202606201, Final Batch Loss: 0.28193336725234985\n",
      "Epoch 3282, Loss: 0.8985425531864166, Final Batch Loss: 0.3524155020713806\n",
      "Epoch 3283, Loss: 0.9604119658470154, Final Batch Loss: 0.3501972556114197\n",
      "Epoch 3284, Loss: 0.9225483536720276, Final Batch Loss: 0.3210746645927429\n",
      "Epoch 3285, Loss: 0.8428095579147339, Final Batch Loss: 0.2711164653301239\n",
      "Epoch 3286, Loss: 0.8897925317287445, Final Batch Loss: 0.2739626467227936\n",
      "Epoch 3287, Loss: 0.8068861365318298, Final Batch Loss: 0.2111840546131134\n",
      "Epoch 3288, Loss: 0.9136106073856354, Final Batch Loss: 0.24649304151535034\n",
      "Epoch 3289, Loss: 1.019493356347084, Final Batch Loss: 0.3791934847831726\n",
      "Epoch 3290, Loss: 0.8766090124845505, Final Batch Loss: 0.22610805928707123\n",
      "Epoch 3291, Loss: 0.8236619681119919, Final Batch Loss: 0.25451308488845825\n",
      "Epoch 3292, Loss: 0.8534052073955536, Final Batch Loss: 0.2666916251182556\n",
      "Epoch 3293, Loss: 0.8576809018850327, Final Batch Loss: 0.24657483398914337\n",
      "Epoch 3294, Loss: 0.8493132591247559, Final Batch Loss: 0.314380407333374\n",
      "Epoch 3295, Loss: 1.0503645241260529, Final Batch Loss: 0.40060046315193176\n",
      "Epoch 3296, Loss: 0.8782590627670288, Final Batch Loss: 0.2695665657520294\n",
      "Epoch 3297, Loss: 0.9003483951091766, Final Batch Loss: 0.2908484935760498\n",
      "Epoch 3298, Loss: 0.9768123030662537, Final Batch Loss: 0.3796727657318115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3299, Loss: 0.8930412530899048, Final Batch Loss: 0.26701727509498596\n",
      "Epoch 3300, Loss: 0.8816770166158676, Final Batch Loss: 0.24069856107234955\n",
      "Epoch 3301, Loss: 0.8083439916372299, Final Batch Loss: 0.17811338603496552\n",
      "Epoch 3302, Loss: 0.8729234635829926, Final Batch Loss: 0.2562025785446167\n",
      "Epoch 3303, Loss: 0.9141955524682999, Final Batch Loss: 0.37708085775375366\n",
      "Epoch 3304, Loss: 0.8641492128372192, Final Batch Loss: 0.30570098757743835\n",
      "Epoch 3305, Loss: 0.8937578797340393, Final Batch Loss: 0.300216943025589\n",
      "Epoch 3306, Loss: 0.892157644033432, Final Batch Loss: 0.25966906547546387\n",
      "Epoch 3307, Loss: 0.9217612445354462, Final Batch Loss: 0.3300681710243225\n",
      "Epoch 3308, Loss: 0.9483957290649414, Final Batch Loss: 0.3087640106678009\n",
      "Epoch 3309, Loss: 0.9718904793262482, Final Batch Loss: 0.44712701439857483\n",
      "Epoch 3310, Loss: 1.0285375714302063, Final Batch Loss: 0.3762739896774292\n",
      "Epoch 3311, Loss: 0.9448081254959106, Final Batch Loss: 0.2954355776309967\n",
      "Epoch 3312, Loss: 0.8349762409925461, Final Batch Loss: 0.28189337253570557\n",
      "Epoch 3313, Loss: 0.8805660605430603, Final Batch Loss: 0.2826279103755951\n",
      "Epoch 3314, Loss: 0.8065416067838669, Final Batch Loss: 0.24618525803089142\n",
      "Epoch 3315, Loss: 0.7889188975095749, Final Batch Loss: 0.24429993331432343\n",
      "Epoch 3316, Loss: 0.9137832224369049, Final Batch Loss: 0.3235156834125519\n",
      "Epoch 3317, Loss: 0.8389175981283188, Final Batch Loss: 0.2445506900548935\n",
      "Epoch 3318, Loss: 0.8923493027687073, Final Batch Loss: 0.2562110424041748\n",
      "Epoch 3319, Loss: 0.8836553245782852, Final Batch Loss: 0.36625027656555176\n",
      "Epoch 3320, Loss: 0.9614598900079727, Final Batch Loss: 0.24502815306186676\n",
      "Epoch 3321, Loss: 0.8500499427318573, Final Batch Loss: 0.2552778422832489\n",
      "Epoch 3322, Loss: 0.8500048369169235, Final Batch Loss: 0.2892969846725464\n",
      "Epoch 3323, Loss: 0.9852303266525269, Final Batch Loss: 0.3440147042274475\n",
      "Epoch 3324, Loss: 0.9053970277309418, Final Batch Loss: 0.3114369809627533\n",
      "Epoch 3325, Loss: 0.8775343298912048, Final Batch Loss: 0.24975889921188354\n",
      "Epoch 3326, Loss: 0.9178822934627533, Final Batch Loss: 0.2965426743030548\n",
      "Epoch 3327, Loss: 0.8054451197385788, Final Batch Loss: 0.2514825165271759\n",
      "Epoch 3328, Loss: 0.99679896235466, Final Batch Loss: 0.3509688973426819\n",
      "Epoch 3329, Loss: 0.9133332371711731, Final Batch Loss: 0.33330851793289185\n",
      "Epoch 3330, Loss: 0.827205628156662, Final Batch Loss: 0.2796160876750946\n",
      "Epoch 3331, Loss: 0.8801511228084564, Final Batch Loss: 0.2814331650733948\n",
      "Epoch 3332, Loss: 0.8581444919109344, Final Batch Loss: 0.28951144218444824\n",
      "Epoch 3333, Loss: 0.8644745349884033, Final Batch Loss: 0.22579678893089294\n",
      "Epoch 3334, Loss: 0.8149716258049011, Final Batch Loss: 0.23366878926753998\n",
      "Epoch 3335, Loss: 0.8881455659866333, Final Batch Loss: 0.3256440758705139\n",
      "Epoch 3336, Loss: 0.8643470704555511, Final Batch Loss: 0.265910804271698\n",
      "Epoch 3337, Loss: 0.8636684715747833, Final Batch Loss: 0.30325525999069214\n",
      "Epoch 3338, Loss: 0.8045590072870255, Final Batch Loss: 0.23212619125843048\n",
      "Epoch 3339, Loss: 0.8974457085132599, Final Batch Loss: 0.2548551857471466\n",
      "Epoch 3340, Loss: 0.9327700138092041, Final Batch Loss: 0.3611348569393158\n",
      "Epoch 3341, Loss: 0.9218433201313019, Final Batch Loss: 0.3518604040145874\n",
      "Epoch 3342, Loss: 0.9339870512485504, Final Batch Loss: 0.32484641671180725\n",
      "Epoch 3343, Loss: 1.0011647641658783, Final Batch Loss: 0.44753149151802063\n",
      "Epoch 3344, Loss: 0.9498802870512009, Final Batch Loss: 0.3601419925689697\n",
      "Epoch 3345, Loss: 1.0026456117630005, Final Batch Loss: 0.4002615213394165\n",
      "Epoch 3346, Loss: 0.9212118685245514, Final Batch Loss: 0.28725212812423706\n",
      "Epoch 3347, Loss: 0.8581703454256058, Final Batch Loss: 0.21989421546459198\n",
      "Epoch 3348, Loss: 0.9078315496444702, Final Batch Loss: 0.2734427750110626\n",
      "Epoch 3349, Loss: 0.8430626839399338, Final Batch Loss: 0.22275550663471222\n",
      "Epoch 3350, Loss: 0.7885638475418091, Final Batch Loss: 0.165800541639328\n",
      "Epoch 3351, Loss: 0.8474030941724777, Final Batch Loss: 0.32436591386795044\n",
      "Epoch 3352, Loss: 0.9347922503948212, Final Batch Loss: 0.362274706363678\n",
      "Epoch 3353, Loss: 0.8641552180051804, Final Batch Loss: 0.22464720904827118\n",
      "Epoch 3354, Loss: 0.9899408668279648, Final Batch Loss: 0.4130091965198517\n",
      "Epoch 3355, Loss: 1.0139034539461136, Final Batch Loss: 0.2496923953294754\n",
      "Epoch 3356, Loss: 0.859753206372261, Final Batch Loss: 0.23190920054912567\n",
      "Epoch 3357, Loss: 1.029233694076538, Final Batch Loss: 0.4086729884147644\n",
      "Epoch 3358, Loss: 0.864184707403183, Final Batch Loss: 0.2999860644340515\n",
      "Epoch 3359, Loss: 0.8338487446308136, Final Batch Loss: 0.2591337263584137\n",
      "Epoch 3360, Loss: 0.8367850929498672, Final Batch Loss: 0.23439939320087433\n",
      "Epoch 3361, Loss: 0.8764404952526093, Final Batch Loss: 0.26577404141426086\n",
      "Epoch 3362, Loss: 0.8326501548290253, Final Batch Loss: 0.25482699275016785\n",
      "Epoch 3363, Loss: 0.9063349962234497, Final Batch Loss: 0.2660742402076721\n",
      "Epoch 3364, Loss: 0.8896812200546265, Final Batch Loss: 0.3319561779499054\n",
      "Epoch 3365, Loss: 0.9513725936412811, Final Batch Loss: 0.3311431109905243\n",
      "Epoch 3366, Loss: 0.9202106893062592, Final Batch Loss: 0.25800013542175293\n",
      "Epoch 3367, Loss: 0.830697700381279, Final Batch Loss: 0.16993235051631927\n",
      "Epoch 3368, Loss: 0.8851265013217926, Final Batch Loss: 0.27614301443099976\n",
      "Epoch 3369, Loss: 0.7759948521852493, Final Batch Loss: 0.2589365541934967\n",
      "Epoch 3370, Loss: 0.9396742284297943, Final Batch Loss: 0.3750768303871155\n",
      "Epoch 3371, Loss: 0.8192583322525024, Final Batch Loss: 0.23730462789535522\n",
      "Epoch 3372, Loss: 0.7826838195323944, Final Batch Loss: 0.2656109035015106\n",
      "Epoch 3373, Loss: 0.8814815878868103, Final Batch Loss: 0.26906639337539673\n",
      "Epoch 3374, Loss: 0.9802475571632385, Final Batch Loss: 0.3545164167881012\n",
      "Epoch 3375, Loss: 0.8576494753360748, Final Batch Loss: 0.2844891846179962\n",
      "Epoch 3376, Loss: 0.8486908972263336, Final Batch Loss: 0.22034147381782532\n",
      "Epoch 3377, Loss: 1.0164211988449097, Final Batch Loss: 0.4611891210079193\n",
      "Epoch 3378, Loss: 0.91825270652771, Final Batch Loss: 0.3105635344982147\n",
      "Epoch 3379, Loss: 0.8769012093544006, Final Batch Loss: 0.2686297595500946\n",
      "Epoch 3380, Loss: 0.9489940404891968, Final Batch Loss: 0.3806709051132202\n",
      "Epoch 3381, Loss: 0.9073451161384583, Final Batch Loss: 0.31284651160240173\n",
      "Epoch 3382, Loss: 0.7984268367290497, Final Batch Loss: 0.21043571829795837\n",
      "Epoch 3383, Loss: 0.8458892703056335, Final Batch Loss: 0.28098756074905396\n",
      "Epoch 3384, Loss: 0.9105711877346039, Final Batch Loss: 0.2877396047115326\n",
      "Epoch 3385, Loss: 0.8760579228401184, Final Batch Loss: 0.28844597935676575\n",
      "Epoch 3386, Loss: 0.8338512629270554, Final Batch Loss: 0.23745448887348175\n",
      "Epoch 3387, Loss: 0.8736221939325333, Final Batch Loss: 0.1629706472158432\n",
      "Epoch 3388, Loss: 0.8215821087360382, Final Batch Loss: 0.2728535532951355\n",
      "Epoch 3389, Loss: 0.9054668247699738, Final Batch Loss: 0.32886987924575806\n",
      "Epoch 3390, Loss: 0.8642192482948303, Final Batch Loss: 0.2558845281600952\n",
      "Epoch 3391, Loss: 0.878926694393158, Final Batch Loss: 0.3411898910999298\n",
      "Epoch 3392, Loss: 0.8536821454763412, Final Batch Loss: 0.2778063714504242\n",
      "Epoch 3393, Loss: 0.8602923154830933, Final Batch Loss: 0.26443326473236084\n",
      "Epoch 3394, Loss: 1.0820661187171936, Final Batch Loss: 0.4097146987915039\n",
      "Epoch 3395, Loss: 0.9074425995349884, Final Batch Loss: 0.2944800555706024\n",
      "Epoch 3396, Loss: 0.8637644201517105, Final Batch Loss: 0.22762857377529144\n",
      "Epoch 3397, Loss: 0.8713372945785522, Final Batch Loss: 0.3193901479244232\n",
      "Epoch 3398, Loss: 0.8551933169364929, Final Batch Loss: 0.33920374512672424\n",
      "Epoch 3399, Loss: 0.8907261788845062, Final Batch Loss: 0.3042456805706024\n",
      "Epoch 3400, Loss: 0.826447606086731, Final Batch Loss: 0.2698424756526947\n",
      "Epoch 3401, Loss: 0.8578335642814636, Final Batch Loss: 0.29124993085861206\n",
      "Epoch 3402, Loss: 0.9945579469203949, Final Batch Loss: 0.39919108152389526\n",
      "Epoch 3403, Loss: 0.8807666599750519, Final Batch Loss: 0.3174675703048706\n",
      "Epoch 3404, Loss: 0.8206281512975693, Final Batch Loss: 0.22777704894542694\n",
      "Epoch 3405, Loss: 0.8657230287790298, Final Batch Loss: 0.333717405796051\n",
      "Epoch 3406, Loss: 0.8212825655937195, Final Batch Loss: 0.3054649829864502\n",
      "Epoch 3407, Loss: 0.8516323864459991, Final Batch Loss: 0.29569947719573975\n",
      "Epoch 3408, Loss: 0.7842644602060318, Final Batch Loss: 0.292558491230011\n",
      "Epoch 3409, Loss: 0.8328128159046173, Final Batch Loss: 0.22113168239593506\n",
      "Epoch 3410, Loss: 1.0150243043899536, Final Batch Loss: 0.4004305303096771\n",
      "Epoch 3411, Loss: 0.8206457197666168, Final Batch Loss: 0.2799879014492035\n",
      "Epoch 3412, Loss: 0.8975764960050583, Final Batch Loss: 0.34434160590171814\n",
      "Epoch 3413, Loss: 0.8824125230312347, Final Batch Loss: 0.26440513134002686\n",
      "Epoch 3414, Loss: 0.8604677319526672, Final Batch Loss: 0.3018170893192291\n",
      "Epoch 3415, Loss: 1.0682633817195892, Final Batch Loss: 0.4607439935207367\n",
      "Epoch 3416, Loss: 0.8213163614273071, Final Batch Loss: 0.2902086675167084\n",
      "Epoch 3417, Loss: 0.9457017183303833, Final Batch Loss: 0.3041436970233917\n",
      "Epoch 3418, Loss: 0.999885767698288, Final Batch Loss: 0.39369481801986694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3419, Loss: 0.9703837037086487, Final Batch Loss: 0.3482001721858978\n",
      "Epoch 3420, Loss: 0.9637207984924316, Final Batch Loss: 0.2548452615737915\n",
      "Epoch 3421, Loss: 0.8170361518859863, Final Batch Loss: 0.31503230333328247\n",
      "Epoch 3422, Loss: 0.9166743457317352, Final Batch Loss: 0.2792794704437256\n",
      "Epoch 3423, Loss: 0.8916924893856049, Final Batch Loss: 0.3659871816635132\n",
      "Epoch 3424, Loss: 0.8972448408603668, Final Batch Loss: 0.35136890411376953\n",
      "Epoch 3425, Loss: 1.0140533149242401, Final Batch Loss: 0.30836454033851624\n",
      "Epoch 3426, Loss: 0.8057378083467484, Final Batch Loss: 0.23348505795001984\n",
      "Epoch 3427, Loss: 0.9529694318771362, Final Batch Loss: 0.2783692479133606\n",
      "Epoch 3428, Loss: 0.9202446341514587, Final Batch Loss: 0.2810925245285034\n",
      "Epoch 3429, Loss: 0.7586941868066788, Final Batch Loss: 0.23988012969493866\n",
      "Epoch 3430, Loss: 0.887458860874176, Final Batch Loss: 0.3107374310493469\n",
      "Epoch 3431, Loss: 0.8672169148921967, Final Batch Loss: 0.2997332513332367\n",
      "Epoch 3432, Loss: 0.89808589220047, Final Batch Loss: 0.24011018872261047\n",
      "Epoch 3433, Loss: 0.8196980804204941, Final Batch Loss: 0.2350551337003708\n",
      "Epoch 3434, Loss: 0.940519168972969, Final Batch Loss: 0.37674736976623535\n",
      "Epoch 3435, Loss: 0.9269499778747559, Final Batch Loss: 0.2891034781932831\n",
      "Epoch 3436, Loss: 0.8534431606531143, Final Batch Loss: 0.30665531754493713\n",
      "Epoch 3437, Loss: 0.8148167133331299, Final Batch Loss: 0.27176526188850403\n",
      "Epoch 3438, Loss: 0.7952444404363632, Final Batch Loss: 0.2272106260061264\n",
      "Epoch 3439, Loss: 0.7965472191572189, Final Batch Loss: 0.23521314561367035\n",
      "Epoch 3440, Loss: 1.0526033341884613, Final Batch Loss: 0.3691454529762268\n",
      "Epoch 3441, Loss: 0.7940506637096405, Final Batch Loss: 0.24212634563446045\n",
      "Epoch 3442, Loss: 0.8328039348125458, Final Batch Loss: 0.3126416802406311\n",
      "Epoch 3443, Loss: 0.925314337015152, Final Batch Loss: 0.31729352474212646\n",
      "Epoch 3444, Loss: 0.8378067910671234, Final Batch Loss: 0.22064223885536194\n",
      "Epoch 3445, Loss: 0.8552262485027313, Final Batch Loss: 0.2768164575099945\n",
      "Epoch 3446, Loss: 0.914187490940094, Final Batch Loss: 0.3052648603916168\n",
      "Epoch 3447, Loss: 0.8556699007749557, Final Batch Loss: 0.31043848395347595\n",
      "Epoch 3448, Loss: 0.8431791365146637, Final Batch Loss: 0.23204800486564636\n",
      "Epoch 3449, Loss: 0.8115404844284058, Final Batch Loss: 0.3016013205051422\n",
      "Epoch 3450, Loss: 0.983056053519249, Final Batch Loss: 0.4644121527671814\n",
      "Epoch 3451, Loss: 0.7769423872232437, Final Batch Loss: 0.2582884430885315\n",
      "Epoch 3452, Loss: 0.8275602459907532, Final Batch Loss: 0.2405967116355896\n",
      "Epoch 3453, Loss: 0.9558623433113098, Final Batch Loss: 0.42877906560897827\n",
      "Epoch 3454, Loss: 0.8336110413074493, Final Batch Loss: 0.27367839217185974\n",
      "Epoch 3455, Loss: 0.8578624725341797, Final Batch Loss: 0.2794579267501831\n",
      "Epoch 3456, Loss: 0.7721318304538727, Final Batch Loss: 0.22549177706241608\n",
      "Epoch 3457, Loss: 0.9572610557079315, Final Batch Loss: 0.30849769711494446\n",
      "Epoch 3458, Loss: 0.7925433814525604, Final Batch Loss: 0.21511216461658478\n",
      "Epoch 3459, Loss: 0.9868374466896057, Final Batch Loss: 0.40710023045539856\n",
      "Epoch 3460, Loss: 0.9257352352142334, Final Batch Loss: 0.3485812246799469\n",
      "Epoch 3461, Loss: 0.7964712977409363, Final Batch Loss: 0.18178996443748474\n",
      "Epoch 3462, Loss: 0.7947438955307007, Final Batch Loss: 0.28066286444664\n",
      "Epoch 3463, Loss: 0.8769982159137726, Final Batch Loss: 0.2946532666683197\n",
      "Epoch 3464, Loss: 0.9147984087467194, Final Batch Loss: 0.37873101234436035\n",
      "Epoch 3465, Loss: 0.7807032316923141, Final Batch Loss: 0.2664206922054291\n",
      "Epoch 3466, Loss: 0.8443820774555206, Final Batch Loss: 0.3303268849849701\n",
      "Epoch 3467, Loss: 0.9163512736558914, Final Batch Loss: 0.3333057463169098\n",
      "Epoch 3468, Loss: 0.8385036587715149, Final Batch Loss: 0.31243884563446045\n",
      "Epoch 3469, Loss: 0.8532404899597168, Final Batch Loss: 0.28518038988113403\n",
      "Epoch 3470, Loss: 0.6836244463920593, Final Batch Loss: 0.18303632736206055\n",
      "Epoch 3471, Loss: 0.7995460033416748, Final Batch Loss: 0.2816080152988434\n",
      "Epoch 3472, Loss: 0.8116938918828964, Final Batch Loss: 0.26533782482147217\n",
      "Epoch 3473, Loss: 0.8679815381765366, Final Batch Loss: 0.22713707387447357\n",
      "Epoch 3474, Loss: 0.826957032084465, Final Batch Loss: 0.19782225787639618\n",
      "Epoch 3475, Loss: 0.8911998569965363, Final Batch Loss: 0.25025010108947754\n",
      "Epoch 3476, Loss: 0.7859510183334351, Final Batch Loss: 0.2525113821029663\n",
      "Epoch 3477, Loss: 1.0194571018218994, Final Batch Loss: 0.44672518968582153\n",
      "Epoch 3478, Loss: 0.907576709985733, Final Batch Loss: 0.3658352494239807\n",
      "Epoch 3479, Loss: 0.9772502481937408, Final Batch Loss: 0.2559344470500946\n",
      "Epoch 3480, Loss: 0.7552857995033264, Final Batch Loss: 0.25809746980667114\n",
      "Epoch 3481, Loss: 0.926128625869751, Final Batch Loss: 0.2927751839160919\n",
      "Epoch 3482, Loss: 0.7879132479429245, Final Batch Loss: 0.26178649067878723\n",
      "Epoch 3483, Loss: 0.9911259412765503, Final Batch Loss: 0.4181155562400818\n",
      "Epoch 3484, Loss: 0.8703284859657288, Final Batch Loss: 0.3109228014945984\n",
      "Epoch 3485, Loss: 0.9512452930212021, Final Batch Loss: 0.24382783472537994\n",
      "Epoch 3486, Loss: 0.833540141582489, Final Batch Loss: 0.28679659962654114\n",
      "Epoch 3487, Loss: 0.8121425807476044, Final Batch Loss: 0.226228266954422\n",
      "Epoch 3488, Loss: 0.8316467702388763, Final Batch Loss: 0.28188449144363403\n",
      "Epoch 3489, Loss: 0.9489786922931671, Final Batch Loss: 0.31229618191719055\n",
      "Epoch 3490, Loss: 0.834391176700592, Final Batch Loss: 0.3111693263053894\n",
      "Epoch 3491, Loss: 0.8709141910076141, Final Batch Loss: 0.28862035274505615\n",
      "Epoch 3492, Loss: 0.7551521509885788, Final Batch Loss: 0.23515966534614563\n",
      "Epoch 3493, Loss: 0.7921072393655777, Final Batch Loss: 0.26497894525527954\n",
      "Epoch 3494, Loss: 1.026365727186203, Final Batch Loss: 0.34060201048851013\n",
      "Epoch 3495, Loss: 1.0176034271717072, Final Batch Loss: 0.35155561566352844\n",
      "Epoch 3496, Loss: 0.8578112870454788, Final Batch Loss: 0.3462415039539337\n",
      "Epoch 3497, Loss: 0.8369383215904236, Final Batch Loss: 0.2510998845100403\n",
      "Epoch 3498, Loss: 0.9384599030017853, Final Batch Loss: 0.3919265866279602\n",
      "Epoch 3499, Loss: 0.8189863860607147, Final Batch Loss: 0.30256563425064087\n",
      "Epoch 3500, Loss: 0.8756537139415741, Final Batch Loss: 0.3418349623680115\n",
      "Epoch 3501, Loss: 0.8521512746810913, Final Batch Loss: 0.24275463819503784\n",
      "Epoch 3502, Loss: 0.8485183417797089, Final Batch Loss: 0.2240331470966339\n",
      "Epoch 3503, Loss: 0.880501002073288, Final Batch Loss: 0.32786741852760315\n",
      "Epoch 3504, Loss: 0.9505094289779663, Final Batch Loss: 0.4161962866783142\n",
      "Epoch 3505, Loss: 0.8388577550649643, Final Batch Loss: 0.34703391790390015\n",
      "Epoch 3506, Loss: 0.8669695258140564, Final Batch Loss: 0.2914060056209564\n",
      "Epoch 3507, Loss: 0.8819797784090042, Final Batch Loss: 0.3736203908920288\n",
      "Epoch 3508, Loss: 1.0609856843948364, Final Batch Loss: 0.40469059348106384\n",
      "Epoch 3509, Loss: 0.9893691539764404, Final Batch Loss: 0.3233298659324646\n",
      "Epoch 3510, Loss: 0.899787187576294, Final Batch Loss: 0.28246548771858215\n",
      "Epoch 3511, Loss: 0.889836922287941, Final Batch Loss: 0.23158498108386993\n",
      "Epoch 3512, Loss: 0.8675674349069595, Final Batch Loss: 0.31735461950302124\n",
      "Epoch 3513, Loss: 0.9529398679733276, Final Batch Loss: 0.34108322858810425\n",
      "Epoch 3514, Loss: 0.8777663856744766, Final Batch Loss: 0.23279030621051788\n",
      "Epoch 3515, Loss: 0.9181948304176331, Final Batch Loss: 0.35661086440086365\n",
      "Epoch 3516, Loss: 0.8257339596748352, Final Batch Loss: 0.2909986078739166\n",
      "Epoch 3517, Loss: 0.9091800451278687, Final Batch Loss: 0.27569183707237244\n",
      "Epoch 3518, Loss: 0.9210205972194672, Final Batch Loss: 0.32261475920677185\n",
      "Epoch 3519, Loss: 0.8873019218444824, Final Batch Loss: 0.30853474140167236\n",
      "Epoch 3520, Loss: 0.8338144719600677, Final Batch Loss: 0.24072018265724182\n",
      "Epoch 3521, Loss: 0.9060006439685822, Final Batch Loss: 0.3241211473941803\n",
      "Epoch 3522, Loss: 0.837676078081131, Final Batch Loss: 0.2775287926197052\n",
      "Epoch 3523, Loss: 0.8591811060905457, Final Batch Loss: 0.24949276447296143\n",
      "Epoch 3524, Loss: 0.8563546538352966, Final Batch Loss: 0.28903621435165405\n",
      "Epoch 3525, Loss: 0.837503582239151, Final Batch Loss: 0.2729373872280121\n",
      "Epoch 3526, Loss: 0.8651496022939682, Final Batch Loss: 0.24861980974674225\n",
      "Epoch 3527, Loss: 0.9162327498197556, Final Batch Loss: 0.3824193775653839\n",
      "Epoch 3528, Loss: 0.8832190036773682, Final Batch Loss: 0.27527210116386414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3529, Loss: 0.8486319482326508, Final Batch Loss: 0.25621646642684937\n",
      "Epoch 3530, Loss: 0.9380120933055878, Final Batch Loss: 0.30818164348602295\n",
      "Epoch 3531, Loss: 0.9049587547779083, Final Batch Loss: 0.27139517664909363\n",
      "Epoch 3532, Loss: 0.9064626395702362, Final Batch Loss: 0.3013085722923279\n",
      "Epoch 3533, Loss: 0.9748422205448151, Final Batch Loss: 0.3383122384548187\n",
      "Epoch 3534, Loss: 0.8388961851596832, Final Batch Loss: 0.2833441495895386\n",
      "Epoch 3535, Loss: 0.8586195856332779, Final Batch Loss: 0.24722321331501007\n",
      "Epoch 3536, Loss: 0.8967643082141876, Final Batch Loss: 0.3396488428115845\n",
      "Epoch 3537, Loss: 0.9497587382793427, Final Batch Loss: 0.37172019481658936\n",
      "Epoch 3538, Loss: 0.9028531759977341, Final Batch Loss: 0.379401832818985\n",
      "Epoch 3539, Loss: 0.8443537354469299, Final Batch Loss: 0.33058303594589233\n",
      "Epoch 3540, Loss: 0.7775941044092178, Final Batch Loss: 0.24833910167217255\n",
      "Epoch 3541, Loss: 0.8646448254585266, Final Batch Loss: 0.28129586577415466\n",
      "Epoch 3542, Loss: 0.8327631056308746, Final Batch Loss: 0.2509998381137848\n",
      "Epoch 3543, Loss: 0.8475660085678101, Final Batch Loss: 0.28532686829566956\n",
      "Epoch 3544, Loss: 0.8833025693893433, Final Batch Loss: 0.2781260013580322\n",
      "Epoch 3545, Loss: 0.885637953877449, Final Batch Loss: 0.3141345977783203\n",
      "Epoch 3546, Loss: 0.8434643447399139, Final Batch Loss: 0.27648600935935974\n",
      "Epoch 3547, Loss: 0.8839840590953827, Final Batch Loss: 0.2736875116825104\n",
      "Epoch 3548, Loss: 0.8614460527896881, Final Batch Loss: 0.27007561922073364\n",
      "Epoch 3549, Loss: 0.8161769509315491, Final Batch Loss: 0.27417051792144775\n",
      "Epoch 3550, Loss: 0.9175644218921661, Final Batch Loss: 0.3425213396549225\n",
      "Epoch 3551, Loss: 0.9827355295419693, Final Batch Loss: 0.40502551198005676\n",
      "Epoch 3552, Loss: 0.8230215758085251, Final Batch Loss: 0.34228283166885376\n",
      "Epoch 3553, Loss: 0.8445231914520264, Final Batch Loss: 0.29906952381134033\n",
      "Epoch 3554, Loss: 0.8686046302318573, Final Batch Loss: 0.2827201187610626\n",
      "Epoch 3555, Loss: 0.7907445728778839, Final Batch Loss: 0.2683926820755005\n",
      "Epoch 3556, Loss: 0.8136545419692993, Final Batch Loss: 0.1915305256843567\n",
      "Epoch 3557, Loss: 0.8795883059501648, Final Batch Loss: 0.252788245677948\n",
      "Epoch 3558, Loss: 0.9438931941986084, Final Batch Loss: 0.35554051399230957\n",
      "Epoch 3559, Loss: 0.7790331691503525, Final Batch Loss: 0.2380668669939041\n",
      "Epoch 3560, Loss: 0.8417986035346985, Final Batch Loss: 0.26332929730415344\n",
      "Epoch 3561, Loss: 0.8174885809421539, Final Batch Loss: 0.2601887285709381\n",
      "Epoch 3562, Loss: 0.8852182328701019, Final Batch Loss: 0.31672340631484985\n",
      "Epoch 3563, Loss: 0.9328297972679138, Final Batch Loss: 0.2563900947570801\n",
      "Epoch 3564, Loss: 1.0166858434677124, Final Batch Loss: 0.36913827061653137\n",
      "Epoch 3565, Loss: 0.8513893038034439, Final Batch Loss: 0.33757659792900085\n",
      "Epoch 3566, Loss: 0.8711089938879013, Final Batch Loss: 0.3007751703262329\n",
      "Epoch 3567, Loss: 0.9040563404560089, Final Batch Loss: 0.3770512342453003\n",
      "Epoch 3568, Loss: 0.8920117616653442, Final Batch Loss: 0.2859209179878235\n",
      "Epoch 3569, Loss: 0.8418093919754028, Final Batch Loss: 0.2764310836791992\n",
      "Epoch 3570, Loss: 0.9548959136009216, Final Batch Loss: 0.3567683696746826\n",
      "Epoch 3571, Loss: 0.9889851212501526, Final Batch Loss: 0.4297029376029968\n",
      "Epoch 3572, Loss: 0.9355377107858658, Final Batch Loss: 0.2484242171049118\n",
      "Epoch 3573, Loss: 0.8823793828487396, Final Batch Loss: 0.29796716570854187\n",
      "Epoch 3574, Loss: 0.9030863642692566, Final Batch Loss: 0.3319602608680725\n",
      "Epoch 3575, Loss: 0.9538520276546478, Final Batch Loss: 0.22226488590240479\n",
      "Epoch 3576, Loss: 0.856538787484169, Final Batch Loss: 0.238186314702034\n",
      "Epoch 3577, Loss: 0.863279715180397, Final Batch Loss: 0.23105598986148834\n",
      "Epoch 3578, Loss: 0.9004206955432892, Final Batch Loss: 0.3292301595211029\n",
      "Epoch 3579, Loss: 0.8375109434127808, Final Batch Loss: 0.28068986535072327\n",
      "Epoch 3580, Loss: 0.8359903246164322, Final Batch Loss: 0.19625793397426605\n",
      "Epoch 3581, Loss: 0.8691055178642273, Final Batch Loss: 0.3184354603290558\n",
      "Epoch 3582, Loss: 0.8878456354141235, Final Batch Loss: 0.31693536043167114\n",
      "Epoch 3583, Loss: 0.9537435173988342, Final Batch Loss: 0.3089767098426819\n",
      "Epoch 3584, Loss: 0.8819468021392822, Final Batch Loss: 0.33210521936416626\n",
      "Epoch 3585, Loss: 0.7842838615179062, Final Batch Loss: 0.22792480885982513\n",
      "Epoch 3586, Loss: 0.9205754995346069, Final Batch Loss: 0.36914581060409546\n",
      "Epoch 3587, Loss: 0.8858471810817719, Final Batch Loss: 0.29333242774009705\n",
      "Epoch 3588, Loss: 0.8940689265727997, Final Batch Loss: 0.3446333706378937\n",
      "Epoch 3589, Loss: 0.9013363718986511, Final Batch Loss: 0.34753167629241943\n",
      "Epoch 3590, Loss: 0.9812020361423492, Final Batch Loss: 0.39232638478279114\n",
      "Epoch 3591, Loss: 0.8878986239433289, Final Batch Loss: 0.30642232298851013\n",
      "Epoch 3592, Loss: 0.7839627265930176, Final Batch Loss: 0.24464499950408936\n",
      "Epoch 3593, Loss: 0.7517229914665222, Final Batch Loss: 0.1820930540561676\n",
      "Epoch 3594, Loss: 0.9196336269378662, Final Batch Loss: 0.2736800014972687\n",
      "Epoch 3595, Loss: 0.8986201286315918, Final Batch Loss: 0.29687732458114624\n",
      "Epoch 3596, Loss: 0.8941137790679932, Final Batch Loss: 0.3017013967037201\n",
      "Epoch 3597, Loss: 0.9645901918411255, Final Batch Loss: 0.27327772974967957\n",
      "Epoch 3598, Loss: 0.8593210130929947, Final Batch Loss: 0.29001009464263916\n",
      "Epoch 3599, Loss: 0.8744618594646454, Final Batch Loss: 0.2808205187320709\n",
      "Epoch 3600, Loss: 0.9075817167758942, Final Batch Loss: 0.3238656222820282\n",
      "Epoch 3601, Loss: 0.7412879765033722, Final Batch Loss: 0.19793161749839783\n",
      "Epoch 3602, Loss: 0.9271188378334045, Final Batch Loss: 0.29980430006980896\n",
      "Epoch 3603, Loss: 0.8652226328849792, Final Batch Loss: 0.2882150113582611\n",
      "Epoch 3604, Loss: 0.8745328336954117, Final Batch Loss: 0.2205408662557602\n",
      "Epoch 3605, Loss: 0.8479280769824982, Final Batch Loss: 0.2898785471916199\n",
      "Epoch 3606, Loss: 0.8769876658916473, Final Batch Loss: 0.25920698046684265\n",
      "Epoch 3607, Loss: 0.9079607427120209, Final Batch Loss: 0.262641042470932\n",
      "Epoch 3608, Loss: 0.8952833414077759, Final Batch Loss: 0.3068787455558777\n",
      "Epoch 3609, Loss: 0.8486837446689606, Final Batch Loss: 0.34696346521377563\n",
      "Epoch 3610, Loss: 0.8489177525043488, Final Batch Loss: 0.29880887269973755\n",
      "Epoch 3611, Loss: 0.7768585532903671, Final Batch Loss: 0.2438039928674698\n",
      "Epoch 3612, Loss: 0.8488206565380096, Final Batch Loss: 0.3184121549129486\n",
      "Epoch 3613, Loss: 0.8612923324108124, Final Batch Loss: 0.34142956137657166\n",
      "Epoch 3614, Loss: 0.903558760881424, Final Batch Loss: 0.2629103362560272\n",
      "Epoch 3615, Loss: 0.8138802498579025, Final Batch Loss: 0.2242286652326584\n",
      "Epoch 3616, Loss: 0.8572676777839661, Final Batch Loss: 0.2541876435279846\n",
      "Epoch 3617, Loss: 0.8838871866464615, Final Batch Loss: 0.33224454522132874\n",
      "Epoch 3618, Loss: 0.8383523970842361, Final Batch Loss: 0.2136564999818802\n",
      "Epoch 3619, Loss: 0.832537829875946, Final Batch Loss: 0.29184940457344055\n",
      "Epoch 3620, Loss: 0.7796057164669037, Final Batch Loss: 0.2590869963169098\n",
      "Epoch 3621, Loss: 0.8089824467897415, Final Batch Loss: 0.3042232394218445\n",
      "Epoch 3622, Loss: 0.9715250730514526, Final Batch Loss: 0.28807660937309265\n",
      "Epoch 3623, Loss: 0.8827821463346481, Final Batch Loss: 0.33345669507980347\n",
      "Epoch 3624, Loss: 1.0061061680316925, Final Batch Loss: 0.40057846903800964\n",
      "Epoch 3625, Loss: 0.7741975039243698, Final Batch Loss: 0.1890685111284256\n",
      "Epoch 3626, Loss: 0.8999259769916534, Final Batch Loss: 0.33785659074783325\n",
      "Epoch 3627, Loss: 0.8960068076848984, Final Batch Loss: 0.35338273644447327\n",
      "Epoch 3628, Loss: 0.9433982670307159, Final Batch Loss: 0.2535426616668701\n",
      "Epoch 3629, Loss: 0.8947566449642181, Final Batch Loss: 0.3044426143169403\n",
      "Epoch 3630, Loss: 0.9310036599636078, Final Batch Loss: 0.36226969957351685\n",
      "Epoch 3631, Loss: 0.9115421772003174, Final Batch Loss: 0.34897711873054504\n",
      "Epoch 3632, Loss: 0.972762256860733, Final Batch Loss: 0.239478200674057\n",
      "Epoch 3633, Loss: 0.8866531997919083, Final Batch Loss: 0.33344167470932007\n",
      "Epoch 3634, Loss: 0.8888070583343506, Final Batch Loss: 0.3748273253440857\n",
      "Epoch 3635, Loss: 0.9762699902057648, Final Batch Loss: 0.40677887201309204\n",
      "Epoch 3636, Loss: 0.8877479583024979, Final Batch Loss: 0.33898162841796875\n",
      "Epoch 3637, Loss: 0.849672332406044, Final Batch Loss: 0.24456451833248138\n",
      "Epoch 3638, Loss: 0.8729169368743896, Final Batch Loss: 0.31300947070121765\n",
      "Epoch 3639, Loss: 0.7881323844194412, Final Batch Loss: 0.2523420453071594\n",
      "Epoch 3640, Loss: 0.8009260594844818, Final Batch Loss: 0.2865069806575775\n",
      "Epoch 3641, Loss: 0.959610790014267, Final Batch Loss: 0.28002485632896423\n",
      "Epoch 3642, Loss: 0.8770244866609573, Final Batch Loss: 0.17069171369075775\n",
      "Epoch 3643, Loss: 0.8203448802232742, Final Batch Loss: 0.3043764531612396\n",
      "Epoch 3644, Loss: 0.8523327708244324, Final Batch Loss: 0.30849489569664\n",
      "Epoch 3645, Loss: 0.8153254091739655, Final Batch Loss: 0.2619054615497589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3646, Loss: 1.0014533400535583, Final Batch Loss: 0.43570733070373535\n",
      "Epoch 3647, Loss: 0.7128819972276688, Final Batch Loss: 0.20741409063339233\n",
      "Epoch 3648, Loss: 0.9308306276798248, Final Batch Loss: 0.3918909430503845\n",
      "Epoch 3649, Loss: 0.82647505402565, Final Batch Loss: 0.2205338180065155\n",
      "Epoch 3650, Loss: 0.8336286246776581, Final Batch Loss: 0.18396222591400146\n",
      "Epoch 3651, Loss: 0.8651931881904602, Final Batch Loss: 0.2964889705181122\n",
      "Epoch 3652, Loss: 0.9059871286153793, Final Batch Loss: 0.39046502113342285\n",
      "Epoch 3653, Loss: 0.8915677070617676, Final Batch Loss: 0.2772877812385559\n",
      "Epoch 3654, Loss: 0.8575327694416046, Final Batch Loss: 0.26026543974876404\n",
      "Epoch 3655, Loss: 0.8019613176584244, Final Batch Loss: 0.2761754095554352\n",
      "Epoch 3656, Loss: 0.8438115119934082, Final Batch Loss: 0.2953602075576782\n",
      "Epoch 3657, Loss: 1.028213083744049, Final Batch Loss: 0.4079902470111847\n",
      "Epoch 3658, Loss: 0.7698413729667664, Final Batch Loss: 0.20941799879074097\n",
      "Epoch 3659, Loss: 0.8483462929725647, Final Batch Loss: 0.24572598934173584\n",
      "Epoch 3660, Loss: 0.8614044785499573, Final Batch Loss: 0.2836504578590393\n",
      "Epoch 3661, Loss: 0.8666397035121918, Final Batch Loss: 0.3440698981285095\n",
      "Epoch 3662, Loss: 0.8532527685165405, Final Batch Loss: 0.24570110440254211\n",
      "Epoch 3663, Loss: 0.8770208358764648, Final Batch Loss: 0.28409937024116516\n",
      "Epoch 3664, Loss: 0.8659874796867371, Final Batch Loss: 0.3128044009208679\n",
      "Epoch 3665, Loss: 0.8613981604576111, Final Batch Loss: 0.2740764319896698\n",
      "Epoch 3666, Loss: 0.8132238984107971, Final Batch Loss: 0.2549992799758911\n",
      "Epoch 3667, Loss: 0.8446725606918335, Final Batch Loss: 0.25321686267852783\n",
      "Epoch 3668, Loss: 0.8421769142150879, Final Batch Loss: 0.2508957087993622\n",
      "Epoch 3669, Loss: 0.8102338314056396, Final Batch Loss: 0.23114022612571716\n",
      "Epoch 3670, Loss: 0.7291282564401627, Final Batch Loss: 0.19785340130329132\n",
      "Epoch 3671, Loss: 0.7275998741388321, Final Batch Loss: 0.20824407041072845\n",
      "Epoch 3672, Loss: 0.7996826022863388, Final Batch Loss: 0.2511771321296692\n",
      "Epoch 3673, Loss: 0.8578890562057495, Final Batch Loss: 0.26452529430389404\n",
      "Epoch 3674, Loss: 0.8021789193153381, Final Batch Loss: 0.27828702330589294\n",
      "Epoch 3675, Loss: 0.887402206659317, Final Batch Loss: 0.30616652965545654\n",
      "Epoch 3676, Loss: 0.815963864326477, Final Batch Loss: 0.24016010761260986\n",
      "Epoch 3677, Loss: 0.7894696146249771, Final Batch Loss: 0.3026057481765747\n",
      "Epoch 3678, Loss: 0.7951332032680511, Final Batch Loss: 0.2843138873577118\n",
      "Epoch 3679, Loss: 0.7728769034147263, Final Batch Loss: 0.20915620028972626\n",
      "Epoch 3680, Loss: 0.9714006185531616, Final Batch Loss: 0.36404627561569214\n",
      "Epoch 3681, Loss: 0.9460359215736389, Final Batch Loss: 0.2915762662887573\n",
      "Epoch 3682, Loss: 0.7710717767477036, Final Batch Loss: 0.2637878954410553\n",
      "Epoch 3683, Loss: 0.7961327135562897, Final Batch Loss: 0.28504112362861633\n",
      "Epoch 3684, Loss: 0.8943001329898834, Final Batch Loss: 0.25117921829223633\n",
      "Epoch 3685, Loss: 0.9360429346561432, Final Batch Loss: 0.34440144896507263\n",
      "Epoch 3686, Loss: 0.7933300584554672, Final Batch Loss: 0.2777400612831116\n",
      "Epoch 3687, Loss: 0.7977999299764633, Final Batch Loss: 0.24122683703899384\n",
      "Epoch 3688, Loss: 0.8088009208440781, Final Batch Loss: 0.2843928635120392\n",
      "Epoch 3689, Loss: 0.8498979061841965, Final Batch Loss: 0.3308899998664856\n",
      "Epoch 3690, Loss: 0.8776420652866364, Final Batch Loss: 0.2880502939224243\n",
      "Epoch 3691, Loss: 0.9183222651481628, Final Batch Loss: 0.3304443657398224\n",
      "Epoch 3692, Loss: 0.8814807534217834, Final Batch Loss: 0.2583758533000946\n",
      "Epoch 3693, Loss: 0.7442976534366608, Final Batch Loss: 0.19809852540493011\n",
      "Epoch 3694, Loss: 0.9036930501461029, Final Batch Loss: 0.2684062421321869\n",
      "Epoch 3695, Loss: 0.9231507480144501, Final Batch Loss: 0.29742762446403503\n",
      "Epoch 3696, Loss: 0.8390258252620697, Final Batch Loss: 0.250140517950058\n",
      "Epoch 3697, Loss: 0.9385869204998016, Final Batch Loss: 0.24509260058403015\n",
      "Epoch 3698, Loss: 0.9188849329948425, Final Batch Loss: 0.3433595299720764\n",
      "Epoch 3699, Loss: 0.87781623005867, Final Batch Loss: 0.3177814781665802\n",
      "Epoch 3700, Loss: 0.8020698130130768, Final Batch Loss: 0.2718597650527954\n",
      "Epoch 3701, Loss: 0.9055331349372864, Final Batch Loss: 0.3801533877849579\n",
      "Epoch 3702, Loss: 0.8187438547611237, Final Batch Loss: 0.2206350564956665\n",
      "Epoch 3703, Loss: 0.9577778577804565, Final Batch Loss: 0.36163708567619324\n",
      "Epoch 3704, Loss: 0.9063878953456879, Final Batch Loss: 0.37095969915390015\n",
      "Epoch 3705, Loss: 0.9053775370121002, Final Batch Loss: 0.34032702445983887\n",
      "Epoch 3706, Loss: 0.812853991985321, Final Batch Loss: 0.2860628366470337\n",
      "Epoch 3707, Loss: 0.8289336562156677, Final Batch Loss: 0.32169631123542786\n",
      "Epoch 3708, Loss: 0.8539441376924515, Final Batch Loss: 0.3158278167247772\n",
      "Epoch 3709, Loss: 0.8811048269271851, Final Batch Loss: 0.2756269872188568\n",
      "Epoch 3710, Loss: 0.8506534844636917, Final Batch Loss: 0.33845818042755127\n",
      "Epoch 3711, Loss: 0.8525505661964417, Final Batch Loss: 0.2684764266014099\n",
      "Epoch 3712, Loss: 0.8185373246669769, Final Batch Loss: 0.2613470256328583\n",
      "Epoch 3713, Loss: 0.84995236992836, Final Batch Loss: 0.3140490651130676\n",
      "Epoch 3714, Loss: 0.8108618706464767, Final Batch Loss: 0.28369152545928955\n",
      "Epoch 3715, Loss: 0.9594340324401855, Final Batch Loss: 0.3767564594745636\n",
      "Epoch 3716, Loss: 0.9554158747196198, Final Batch Loss: 0.2305615246295929\n",
      "Epoch 3717, Loss: 0.9412063360214233, Final Batch Loss: 0.43461596965789795\n",
      "Epoch 3718, Loss: 0.8067001104354858, Final Batch Loss: 0.30601638555526733\n",
      "Epoch 3719, Loss: 0.842560201883316, Final Batch Loss: 0.27844393253326416\n",
      "Epoch 3720, Loss: 0.89993816614151, Final Batch Loss: 0.2263183295726776\n",
      "Epoch 3721, Loss: 0.9203462302684784, Final Batch Loss: 0.3006149232387543\n",
      "Epoch 3722, Loss: 0.8132716119289398, Final Batch Loss: 0.23338758945465088\n",
      "Epoch 3723, Loss: 0.909866064786911, Final Batch Loss: 0.32470107078552246\n",
      "Epoch 3724, Loss: 0.8073609173297882, Final Batch Loss: 0.31178972125053406\n",
      "Epoch 3725, Loss: 0.8124646842479706, Final Batch Loss: 0.257444828748703\n",
      "Epoch 3726, Loss: 0.7956812232732773, Final Batch Loss: 0.2907590866088867\n",
      "Epoch 3727, Loss: 0.8783219158649445, Final Batch Loss: 0.2770156264305115\n",
      "Epoch 3728, Loss: 0.8457739353179932, Final Batch Loss: 0.25004133582115173\n",
      "Epoch 3729, Loss: 0.8682215213775635, Final Batch Loss: 0.32504019141197205\n",
      "Epoch 3730, Loss: 0.9490655064582825, Final Batch Loss: 0.3193477988243103\n",
      "Epoch 3731, Loss: 0.8528158664703369, Final Batch Loss: 0.25630420446395874\n",
      "Epoch 3732, Loss: 0.8212438374757767, Final Batch Loss: 0.3092190623283386\n",
      "Epoch 3733, Loss: 0.9252933263778687, Final Batch Loss: 0.30033624172210693\n",
      "Epoch 3734, Loss: 0.7385820895433426, Final Batch Loss: 0.17271268367767334\n",
      "Epoch 3735, Loss: 0.8014343678951263, Final Batch Loss: 0.25634703040122986\n",
      "Epoch 3736, Loss: 0.8267559111118317, Final Batch Loss: 0.2659744620323181\n",
      "Epoch 3737, Loss: 0.8781404793262482, Final Batch Loss: 0.2553906738758087\n",
      "Epoch 3738, Loss: 0.8033603876829147, Final Batch Loss: 0.14118938148021698\n",
      "Epoch 3739, Loss: 0.8001694679260254, Final Batch Loss: 0.2698071300983429\n",
      "Epoch 3740, Loss: 0.8893151432275772, Final Batch Loss: 0.24600975215435028\n",
      "Epoch 3741, Loss: 0.8742870092391968, Final Batch Loss: 0.37752994894981384\n",
      "Epoch 3742, Loss: 0.7750320136547089, Final Batch Loss: 0.2681881785392761\n",
      "Epoch 3743, Loss: 0.9181455969810486, Final Batch Loss: 0.342464417219162\n",
      "Epoch 3744, Loss: 0.8848229944705963, Final Batch Loss: 0.32071423530578613\n",
      "Epoch 3745, Loss: 0.753115251660347, Final Batch Loss: 0.2177223116159439\n",
      "Epoch 3746, Loss: 0.8154862821102142, Final Batch Loss: 0.24549947679042816\n",
      "Epoch 3747, Loss: 0.8736464530229568, Final Batch Loss: 0.3483879566192627\n",
      "Epoch 3748, Loss: 0.7364546358585358, Final Batch Loss: 0.25110939145088196\n",
      "Epoch 3749, Loss: 0.8713545501232147, Final Batch Loss: 0.29927703738212585\n",
      "Epoch 3750, Loss: 0.8745007812976837, Final Batch Loss: 0.34599021077156067\n",
      "Epoch 3751, Loss: 0.9426150918006897, Final Batch Loss: 0.3385855257511139\n",
      "Epoch 3752, Loss: 0.755247637629509, Final Batch Loss: 0.1881294697523117\n",
      "Epoch 3753, Loss: 0.7549191564321518, Final Batch Loss: 0.2724249064922333\n",
      "Epoch 3754, Loss: 0.8406859189271927, Final Batch Loss: 0.2289305180311203\n",
      "Epoch 3755, Loss: 0.9278303384780884, Final Batch Loss: 0.2954990565776825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3756, Loss: 0.8085876107215881, Final Batch Loss: 0.22838541865348816\n",
      "Epoch 3757, Loss: 0.8476223796606064, Final Batch Loss: 0.3461017906665802\n",
      "Epoch 3758, Loss: 0.7633327841758728, Final Batch Loss: 0.23552371561527252\n",
      "Epoch 3759, Loss: 0.7866669148206711, Final Batch Loss: 0.29612889885902405\n",
      "Epoch 3760, Loss: 0.857161670923233, Final Batch Loss: 0.332404226064682\n",
      "Epoch 3761, Loss: 0.8116893619298935, Final Batch Loss: 0.2980680763721466\n",
      "Epoch 3762, Loss: 0.7985000461339951, Final Batch Loss: 0.20614184439182281\n",
      "Epoch 3763, Loss: 0.8148601651191711, Final Batch Loss: 0.2811206579208374\n",
      "Epoch 3764, Loss: 1.1604787707328796, Final Batch Loss: 0.5993956923484802\n",
      "Epoch 3765, Loss: 0.818787083029747, Final Batch Loss: 0.24987871944904327\n",
      "Epoch 3766, Loss: 0.8821729421615601, Final Batch Loss: 0.3389374017715454\n",
      "Epoch 3767, Loss: 0.894843339920044, Final Batch Loss: 0.3110007643699646\n",
      "Epoch 3768, Loss: 0.7208631485700607, Final Batch Loss: 0.27753913402557373\n",
      "Epoch 3769, Loss: 0.912349134683609, Final Batch Loss: 0.273521363735199\n",
      "Epoch 3770, Loss: 0.8425664007663727, Final Batch Loss: 0.2799762487411499\n",
      "Epoch 3771, Loss: 0.8832131028175354, Final Batch Loss: 0.294012188911438\n",
      "Epoch 3772, Loss: 0.9954076111316681, Final Batch Loss: 0.3348054885864258\n",
      "Epoch 3773, Loss: 0.8622293025255203, Final Batch Loss: 0.241000697016716\n",
      "Epoch 3774, Loss: 0.8032903224229813, Final Batch Loss: 0.2719532549381256\n",
      "Epoch 3775, Loss: 0.9548563063144684, Final Batch Loss: 0.33179938793182373\n",
      "Epoch 3776, Loss: 0.820223018527031, Final Batch Loss: 0.32461825013160706\n",
      "Epoch 3777, Loss: 0.8295364230871201, Final Batch Loss: 0.22790510952472687\n",
      "Epoch 3778, Loss: 0.8560316413640976, Final Batch Loss: 0.23338700830936432\n",
      "Epoch 3779, Loss: 0.8553265929222107, Final Batch Loss: 0.20959314703941345\n",
      "Epoch 3780, Loss: 0.7965004295110703, Final Batch Loss: 0.23728348314762115\n",
      "Epoch 3781, Loss: 0.8497173190116882, Final Batch Loss: 0.29871833324432373\n",
      "Epoch 3782, Loss: 0.7931216955184937, Final Batch Loss: 0.2541610300540924\n",
      "Epoch 3783, Loss: 0.7631785273551941, Final Batch Loss: 0.255397230386734\n",
      "Epoch 3784, Loss: 0.8429338932037354, Final Batch Loss: 0.32151496410369873\n",
      "Epoch 3785, Loss: 0.7723227590322495, Final Batch Loss: 0.25937968492507935\n",
      "Epoch 3786, Loss: 0.8674049079418182, Final Batch Loss: 0.27160847187042236\n",
      "Epoch 3787, Loss: 0.7794201076030731, Final Batch Loss: 0.1768692433834076\n",
      "Epoch 3788, Loss: 0.7656175941228867, Final Batch Loss: 0.23777742683887482\n",
      "Epoch 3789, Loss: 0.8001475483179092, Final Batch Loss: 0.24915029108524323\n",
      "Epoch 3790, Loss: 0.8371809720993042, Final Batch Loss: 0.32348498702049255\n",
      "Epoch 3791, Loss: 0.8235913813114166, Final Batch Loss: 0.30121463537216187\n",
      "Epoch 3792, Loss: 0.8429591208696365, Final Batch Loss: 0.3539649248123169\n",
      "Epoch 3793, Loss: 0.8919626772403717, Final Batch Loss: 0.33804118633270264\n",
      "Epoch 3794, Loss: 0.9076676815748215, Final Batch Loss: 0.4007079303264618\n",
      "Epoch 3795, Loss: 0.8528548926115036, Final Batch Loss: 0.2897931933403015\n",
      "Epoch 3796, Loss: 0.8338944911956787, Final Batch Loss: 0.29199153184890747\n",
      "Epoch 3797, Loss: 0.7867110967636108, Final Batch Loss: 0.17661535739898682\n",
      "Epoch 3798, Loss: 0.8194676339626312, Final Batch Loss: 0.24054130911827087\n",
      "Epoch 3799, Loss: 0.8690657317638397, Final Batch Loss: 0.2390635907649994\n",
      "Epoch 3800, Loss: 0.8492033183574677, Final Batch Loss: 0.30324748158454895\n",
      "Epoch 3801, Loss: 0.800879955291748, Final Batch Loss: 0.25644198060035706\n",
      "Epoch 3802, Loss: 0.9856031835079193, Final Batch Loss: 0.387606680393219\n",
      "Epoch 3803, Loss: 0.90101757645607, Final Batch Loss: 0.28039419651031494\n",
      "Epoch 3804, Loss: 0.8411339968442917, Final Batch Loss: 0.3226199746131897\n",
      "Epoch 3805, Loss: 0.7786568254232407, Final Batch Loss: 0.24011360108852386\n",
      "Epoch 3806, Loss: 0.8601285964250565, Final Batch Loss: 0.34705954790115356\n",
      "Epoch 3807, Loss: 0.8379403948783875, Final Batch Loss: 0.2748349905014038\n",
      "Epoch 3808, Loss: 0.8561381250619888, Final Batch Loss: 0.24575106799602509\n",
      "Epoch 3809, Loss: 0.8315329402685165, Final Batch Loss: 0.2975586950778961\n",
      "Epoch 3810, Loss: 0.8755372166633606, Final Batch Loss: 0.2805876135826111\n",
      "Epoch 3811, Loss: 0.9217730462551117, Final Batch Loss: 0.34515124559402466\n",
      "Epoch 3812, Loss: 0.7827411741018295, Final Batch Loss: 0.21600301563739777\n",
      "Epoch 3813, Loss: 0.8437385559082031, Final Batch Loss: 0.3508586585521698\n",
      "Epoch 3814, Loss: 0.7939801216125488, Final Batch Loss: 0.23052534461021423\n",
      "Epoch 3815, Loss: 0.7421298921108246, Final Batch Loss: 0.27437302470207214\n",
      "Epoch 3816, Loss: 0.7885745614767075, Final Batch Loss: 0.24451790750026703\n",
      "Epoch 3817, Loss: 0.7626220881938934, Final Batch Loss: 0.23078976571559906\n",
      "Epoch 3818, Loss: 0.8468981683254242, Final Batch Loss: 0.3127133250236511\n",
      "Epoch 3819, Loss: 0.8690723776817322, Final Batch Loss: 0.3259240984916687\n",
      "Epoch 3820, Loss: 0.8510312587022781, Final Batch Loss: 0.37354913353919983\n",
      "Epoch 3821, Loss: 0.9432373940944672, Final Batch Loss: 0.294582724571228\n",
      "Epoch 3822, Loss: 0.8050186187028885, Final Batch Loss: 0.21196119487285614\n",
      "Epoch 3823, Loss: 0.8513327538967133, Final Batch Loss: 0.2716091573238373\n",
      "Epoch 3824, Loss: 0.78874871134758, Final Batch Loss: 0.28895705938339233\n",
      "Epoch 3825, Loss: 0.8692588210105896, Final Batch Loss: 0.32919999957084656\n",
      "Epoch 3826, Loss: 0.8822926878929138, Final Batch Loss: 0.3334912955760956\n",
      "Epoch 3827, Loss: 0.7597267478704453, Final Batch Loss: 0.2768540680408478\n",
      "Epoch 3828, Loss: 0.8337285220623016, Final Batch Loss: 0.2837197184562683\n",
      "Epoch 3829, Loss: 0.8401956111192703, Final Batch Loss: 0.2783461809158325\n",
      "Epoch 3830, Loss: 0.9270929992198944, Final Batch Loss: 0.307714581489563\n",
      "Epoch 3831, Loss: 0.918951541185379, Final Batch Loss: 0.3291906416416168\n",
      "Epoch 3832, Loss: 0.9194468110799789, Final Batch Loss: 0.4336540102958679\n",
      "Epoch 3833, Loss: 0.8517892956733704, Final Batch Loss: 0.27228957414627075\n",
      "Epoch 3834, Loss: 0.7932913601398468, Final Batch Loss: 0.26141244173049927\n",
      "Epoch 3835, Loss: 0.8049992322921753, Final Batch Loss: 0.32899489998817444\n",
      "Epoch 3836, Loss: 0.7607338428497314, Final Batch Loss: 0.268984854221344\n",
      "Epoch 3837, Loss: 0.847656786441803, Final Batch Loss: 0.298341304063797\n",
      "Epoch 3838, Loss: 0.8569896221160889, Final Batch Loss: 0.33788028359413147\n",
      "Epoch 3839, Loss: 0.8164388239383698, Final Batch Loss: 0.2812539339065552\n",
      "Epoch 3840, Loss: 0.8454805314540863, Final Batch Loss: 0.280921995639801\n",
      "Epoch 3841, Loss: 0.7297551929950714, Final Batch Loss: 0.2008407711982727\n",
      "Epoch 3842, Loss: 0.8503923118114471, Final Batch Loss: 0.25301411747932434\n",
      "Epoch 3843, Loss: 0.8116665780544281, Final Batch Loss: 0.23949405550956726\n",
      "Epoch 3844, Loss: 0.8126045316457748, Final Batch Loss: 0.3090355694293976\n",
      "Epoch 3845, Loss: 0.8045835345983505, Final Batch Loss: 0.2774966061115265\n",
      "Epoch 3846, Loss: 0.7971855700016022, Final Batch Loss: 0.2899191975593567\n",
      "Epoch 3847, Loss: 0.7788316756486893, Final Batch Loss: 0.22753198444843292\n",
      "Epoch 3848, Loss: 0.801011711359024, Final Batch Loss: 0.18205499649047852\n",
      "Epoch 3849, Loss: 0.752627894282341, Final Batch Loss: 0.2083083540201187\n",
      "Epoch 3850, Loss: 0.8346098065376282, Final Batch Loss: 0.3052288889884949\n",
      "Epoch 3851, Loss: 0.889078676700592, Final Batch Loss: 0.3615817129611969\n",
      "Epoch 3852, Loss: 0.7468022406101227, Final Batch Loss: 0.262777179479599\n",
      "Epoch 3853, Loss: 0.9445452988147736, Final Batch Loss: 0.3441035747528076\n",
      "Epoch 3854, Loss: 0.819627434015274, Final Batch Loss: 0.2880558371543884\n",
      "Epoch 3855, Loss: 0.7763836681842804, Final Batch Loss: 0.24271467328071594\n",
      "Epoch 3856, Loss: 0.8022872507572174, Final Batch Loss: 0.24902260303497314\n",
      "Epoch 3857, Loss: 0.8282869160175323, Final Batch Loss: 0.22540131211280823\n",
      "Epoch 3858, Loss: 0.8731730729341507, Final Batch Loss: 0.35837680101394653\n",
      "Epoch 3859, Loss: 0.8113355785608292, Final Batch Loss: 0.29670608043670654\n",
      "Epoch 3860, Loss: 0.7975189834833145, Final Batch Loss: 0.2729738652706146\n",
      "Epoch 3861, Loss: 0.8162553757429123, Final Batch Loss: 0.24691642820835114\n",
      "Epoch 3862, Loss: 0.8291819244623184, Final Batch Loss: 0.2749585211277008\n",
      "Epoch 3863, Loss: 0.822273463010788, Final Batch Loss: 0.2826525866985321\n",
      "Epoch 3864, Loss: 0.883988618850708, Final Batch Loss: 0.31270524859428406\n",
      "Epoch 3865, Loss: 0.8138696253299713, Final Batch Loss: 0.24813807010650635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3866, Loss: 0.8850438594818115, Final Batch Loss: 0.2727176547050476\n",
      "Epoch 3867, Loss: 0.8826012313365936, Final Batch Loss: 0.3333284556865692\n",
      "Epoch 3868, Loss: 0.7686777859926224, Final Batch Loss: 0.24746160209178925\n",
      "Epoch 3869, Loss: 0.9140085577964783, Final Batch Loss: 0.3391953706741333\n",
      "Epoch 3870, Loss: 0.8761367946863174, Final Batch Loss: 0.20923160016536713\n",
      "Epoch 3871, Loss: 0.8527466654777527, Final Batch Loss: 0.264810174703598\n",
      "Epoch 3872, Loss: 0.7723560780286789, Final Batch Loss: 0.16090644896030426\n",
      "Epoch 3873, Loss: 0.8568158894777298, Final Batch Loss: 0.3115328252315521\n",
      "Epoch 3874, Loss: 1.0287379622459412, Final Batch Loss: 0.35720327496528625\n",
      "Epoch 3875, Loss: 0.7839528024196625, Final Batch Loss: 0.15578657388687134\n",
      "Epoch 3876, Loss: 0.7798162400722504, Final Batch Loss: 0.25213688611984253\n",
      "Epoch 3877, Loss: 0.7934868782758713, Final Batch Loss: 0.22822462022304535\n",
      "Epoch 3878, Loss: 0.8275477290153503, Final Batch Loss: 0.35046449303627014\n",
      "Epoch 3879, Loss: 0.8349444568157196, Final Batch Loss: 0.2978942096233368\n",
      "Epoch 3880, Loss: 0.7883099019527435, Final Batch Loss: 0.2800866961479187\n",
      "Epoch 3881, Loss: 0.7576500326395035, Final Batch Loss: 0.20873898267745972\n",
      "Epoch 3882, Loss: 0.7803615927696228, Final Batch Loss: 0.32366272807121277\n",
      "Epoch 3883, Loss: 0.8339732587337494, Final Batch Loss: 0.3088895380496979\n",
      "Epoch 3884, Loss: 0.8870470821857452, Final Batch Loss: 0.2791242301464081\n",
      "Epoch 3885, Loss: 0.9004237055778503, Final Batch Loss: 0.3267851173877716\n",
      "Epoch 3886, Loss: 0.8231557458639145, Final Batch Loss: 0.24404869973659515\n",
      "Epoch 3887, Loss: 0.93979212641716, Final Batch Loss: 0.29301536083221436\n",
      "Epoch 3888, Loss: 0.8352735787630081, Final Batch Loss: 0.3354179561138153\n",
      "Epoch 3889, Loss: 0.8606942147016525, Final Batch Loss: 0.3503935635089874\n",
      "Epoch 3890, Loss: 0.9419025480747223, Final Batch Loss: 0.2854423224925995\n",
      "Epoch 3891, Loss: 0.8200751394033432, Final Batch Loss: 0.32447221875190735\n",
      "Epoch 3892, Loss: 0.7734440714120865, Final Batch Loss: 0.2744525074958801\n",
      "Epoch 3893, Loss: 0.821465253829956, Final Batch Loss: 0.295688658952713\n",
      "Epoch 3894, Loss: 0.9002855122089386, Final Batch Loss: 0.3225651979446411\n",
      "Epoch 3895, Loss: 0.7683282643556595, Final Batch Loss: 0.24712689220905304\n",
      "Epoch 3896, Loss: 0.9667105078697205, Final Batch Loss: 0.2946877181529999\n",
      "Epoch 3897, Loss: 0.7982566356658936, Final Batch Loss: 0.2939434349536896\n",
      "Epoch 3898, Loss: 0.8973671793937683, Final Batch Loss: 0.3271118700504303\n",
      "Epoch 3899, Loss: 0.7980891466140747, Final Batch Loss: 0.25370660424232483\n",
      "Epoch 3900, Loss: 0.7923950552940369, Final Batch Loss: 0.26401522755622864\n",
      "Epoch 3901, Loss: 0.7998298108577728, Final Batch Loss: 0.21677103638648987\n",
      "Epoch 3902, Loss: 0.6983978301286697, Final Batch Loss: 0.2009461224079132\n",
      "Epoch 3903, Loss: 0.7724406272172928, Final Batch Loss: 0.2280358225107193\n",
      "Epoch 3904, Loss: 0.7959220707416534, Final Batch Loss: 0.32321810722351074\n",
      "Epoch 3905, Loss: 0.8198739737272263, Final Batch Loss: 0.21630053222179413\n",
      "Epoch 3906, Loss: 0.8570458590984344, Final Batch Loss: 0.3029537796974182\n",
      "Epoch 3907, Loss: 0.8009999841451645, Final Batch Loss: 0.23646391928195953\n",
      "Epoch 3908, Loss: 0.8583586513996124, Final Batch Loss: 0.31472140550613403\n",
      "Epoch 3909, Loss: 0.7489933520555496, Final Batch Loss: 0.20071591436862946\n",
      "Epoch 3910, Loss: 0.8660819232463837, Final Batch Loss: 0.24703925848007202\n",
      "Epoch 3911, Loss: 0.7727469205856323, Final Batch Loss: 0.25188273191452026\n",
      "Epoch 3912, Loss: 0.9297506213188171, Final Batch Loss: 0.32140490412712097\n",
      "Epoch 3913, Loss: 0.8709061443805695, Final Batch Loss: 0.27979061007499695\n",
      "Epoch 3914, Loss: 0.7910202443599701, Final Batch Loss: 0.23909717798233032\n",
      "Epoch 3915, Loss: 0.8798470497131348, Final Batch Loss: 0.3259611427783966\n",
      "Epoch 3916, Loss: 0.8859056234359741, Final Batch Loss: 0.3812882602214813\n",
      "Epoch 3917, Loss: 0.7530640810728073, Final Batch Loss: 0.2870883345603943\n",
      "Epoch 3918, Loss: 0.9214251637458801, Final Batch Loss: 0.365945428609848\n",
      "Epoch 3919, Loss: 0.9612365365028381, Final Batch Loss: 0.48095646500587463\n",
      "Epoch 3920, Loss: 0.9620034694671631, Final Batch Loss: 0.42720332741737366\n",
      "Epoch 3921, Loss: 0.7998005151748657, Final Batch Loss: 0.22440478205680847\n",
      "Epoch 3922, Loss: 0.8184477090835571, Final Batch Loss: 0.2622455954551697\n",
      "Epoch 3923, Loss: 0.7860548496246338, Final Batch Loss: 0.25766369700431824\n",
      "Epoch 3924, Loss: 0.8386314809322357, Final Batch Loss: 0.2435847818851471\n",
      "Epoch 3925, Loss: 0.8615738302469254, Final Batch Loss: 0.34295085072517395\n",
      "Epoch 3926, Loss: 0.8278824985027313, Final Batch Loss: 0.28468215465545654\n",
      "Epoch 3927, Loss: 0.8038137704133987, Final Batch Loss: 0.2986240088939667\n",
      "Epoch 3928, Loss: 0.7603136301040649, Final Batch Loss: 0.27737197279930115\n",
      "Epoch 3929, Loss: 0.8422800004482269, Final Batch Loss: 0.2560870945453644\n",
      "Epoch 3930, Loss: 0.870187059044838, Final Batch Loss: 0.21929477155208588\n",
      "Epoch 3931, Loss: 0.8065633475780487, Final Batch Loss: 0.31667864322662354\n",
      "Epoch 3932, Loss: 0.7851142436265945, Final Batch Loss: 0.23494751751422882\n",
      "Epoch 3933, Loss: 0.7799855470657349, Final Batch Loss: 0.24290242791175842\n",
      "Epoch 3934, Loss: 0.8148834109306335, Final Batch Loss: 0.2471657395362854\n",
      "Epoch 3935, Loss: 0.8622861504554749, Final Batch Loss: 0.26335409283638\n",
      "Epoch 3936, Loss: 0.8861398100852966, Final Batch Loss: 0.37803611159324646\n",
      "Epoch 3937, Loss: 0.8291258066892624, Final Batch Loss: 0.2846696972846985\n",
      "Epoch 3938, Loss: 0.9584681689739227, Final Batch Loss: 0.3938113749027252\n",
      "Epoch 3939, Loss: 0.8206041604280472, Final Batch Loss: 0.24949271976947784\n",
      "Epoch 3940, Loss: 0.819510892033577, Final Batch Loss: 0.21381498873233795\n",
      "Epoch 3941, Loss: 0.8652503192424774, Final Batch Loss: 0.2890276312828064\n",
      "Epoch 3942, Loss: 0.752523809671402, Final Batch Loss: 0.19059091806411743\n",
      "Epoch 3943, Loss: 0.8216416239738464, Final Batch Loss: 0.2802110016345978\n",
      "Epoch 3944, Loss: 0.7451324760913849, Final Batch Loss: 0.2762324810028076\n",
      "Epoch 3945, Loss: 0.8837835490703583, Final Batch Loss: 0.33770033717155457\n",
      "Epoch 3946, Loss: 0.7969672232866287, Final Batch Loss: 0.2691260278224945\n",
      "Epoch 3947, Loss: 0.8354063630104065, Final Batch Loss: 0.35779649019241333\n",
      "Epoch 3948, Loss: 0.7414101362228394, Final Batch Loss: 0.18211856484413147\n",
      "Epoch 3949, Loss: 0.7788285315036774, Final Batch Loss: 0.24641358852386475\n",
      "Epoch 3950, Loss: 0.9193588644266129, Final Batch Loss: 0.39730146527290344\n",
      "Epoch 3951, Loss: 0.89409539103508, Final Batch Loss: 0.28745800256729126\n",
      "Epoch 3952, Loss: 0.7671590447425842, Final Batch Loss: 0.25880423188209534\n",
      "Epoch 3953, Loss: 0.8237798362970352, Final Batch Loss: 0.2893255650997162\n",
      "Epoch 3954, Loss: 0.8362136483192444, Final Batch Loss: 0.3534332811832428\n",
      "Epoch 3955, Loss: 0.8636525869369507, Final Batch Loss: 0.29928353428840637\n",
      "Epoch 3956, Loss: 0.7455362379550934, Final Batch Loss: 0.24784351885318756\n",
      "Epoch 3957, Loss: 0.8464148938655853, Final Batch Loss: 0.277567982673645\n",
      "Epoch 3958, Loss: 0.7423211336135864, Final Batch Loss: 0.3042568564414978\n",
      "Epoch 3959, Loss: 0.8620929569005966, Final Batch Loss: 0.3418281674385071\n",
      "Epoch 3960, Loss: 0.8498552739620209, Final Batch Loss: 0.26542145013809204\n",
      "Epoch 3961, Loss: 0.8353692293167114, Final Batch Loss: 0.3387407660484314\n",
      "Epoch 3962, Loss: 0.6993040293455124, Final Batch Loss: 0.205816388130188\n",
      "Epoch 3963, Loss: 0.8122940510511398, Final Batch Loss: 0.3035699725151062\n",
      "Epoch 3964, Loss: 0.904667466878891, Final Batch Loss: 0.3519875407218933\n",
      "Epoch 3965, Loss: 0.8706796169281006, Final Batch Loss: 0.34192389249801636\n",
      "Epoch 3966, Loss: 0.8495498895645142, Final Batch Loss: 0.2578929662704468\n",
      "Epoch 3967, Loss: 0.7749020904302597, Final Batch Loss: 0.2390146404504776\n",
      "Epoch 3968, Loss: 0.8977690637111664, Final Batch Loss: 0.295100599527359\n",
      "Epoch 3969, Loss: 0.726240947842598, Final Batch Loss: 0.1454530507326126\n",
      "Epoch 3970, Loss: 0.7566238045692444, Final Batch Loss: 0.2656872272491455\n",
      "Epoch 3971, Loss: 0.9757862091064453, Final Batch Loss: 0.4112609028816223\n",
      "Epoch 3972, Loss: 0.7004608064889908, Final Batch Loss: 0.21566620469093323\n",
      "Epoch 3973, Loss: 0.781680703163147, Final Batch Loss: 0.23198604583740234\n",
      "Epoch 3974, Loss: 0.7496200650930405, Final Batch Loss: 0.21457992494106293\n",
      "Epoch 3975, Loss: 0.8844318240880966, Final Batch Loss: 0.24006710946559906\n",
      "Epoch 3976, Loss: 0.829735055565834, Final Batch Loss: 0.28568801283836365\n",
      "Epoch 3977, Loss: 0.9306705296039581, Final Batch Loss: 0.2975366413593292\n",
      "Epoch 3978, Loss: 0.8276898711919785, Final Batch Loss: 0.18792633712291718\n",
      "Epoch 3979, Loss: 0.8789865374565125, Final Batch Loss: 0.30434364080429077\n",
      "Epoch 3980, Loss: 0.8089039176702499, Final Batch Loss: 0.2576060891151428\n",
      "Epoch 3981, Loss: 0.8099913746118546, Final Batch Loss: 0.2437344640493393\n",
      "Epoch 3982, Loss: 0.8350923210382462, Final Batch Loss: 0.2935101091861725\n",
      "Epoch 3983, Loss: 0.7739200741052628, Final Batch Loss: 0.24792379140853882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3984, Loss: 0.763766348361969, Final Batch Loss: 0.25275442004203796\n",
      "Epoch 3985, Loss: 0.7478239685297012, Final Batch Loss: 0.24260413646697998\n",
      "Epoch 3986, Loss: 0.8190253376960754, Final Batch Loss: 0.3263876140117645\n",
      "Epoch 3987, Loss: 0.8607748746871948, Final Batch Loss: 0.316748708486557\n",
      "Epoch 3988, Loss: 0.8401710689067841, Final Batch Loss: 0.3249290883541107\n",
      "Epoch 3989, Loss: 0.7826264202594757, Final Batch Loss: 0.23893767595291138\n",
      "Epoch 3990, Loss: 0.7789039313793182, Final Batch Loss: 0.2452661693096161\n",
      "Epoch 3991, Loss: 1.0094310939311981, Final Batch Loss: 0.3605116307735443\n",
      "Epoch 3992, Loss: 0.8091132640838623, Final Batch Loss: 0.27774882316589355\n",
      "Epoch 3993, Loss: 0.8297087401151657, Final Batch Loss: 0.27130770683288574\n",
      "Epoch 3994, Loss: 0.8153912127017975, Final Batch Loss: 0.2233484387397766\n",
      "Epoch 3995, Loss: 0.8103476166725159, Final Batch Loss: 0.2500813901424408\n",
      "Epoch 3996, Loss: 0.8988681733608246, Final Batch Loss: 0.346942275762558\n",
      "Epoch 3997, Loss: 0.7968655228614807, Final Batch Loss: 0.2689703106880188\n",
      "Epoch 3998, Loss: 0.7299666106700897, Final Batch Loss: 0.2146671563386917\n",
      "Epoch 3999, Loss: 0.8111654967069626, Final Batch Loss: 0.2266222983598709\n",
      "Epoch 4000, Loss: 0.7962915003299713, Final Batch Loss: 0.21487045288085938\n",
      "Epoch 4001, Loss: 0.832364559173584, Final Batch Loss: 0.3254631459712982\n",
      "Epoch 4002, Loss: 0.85428087413311, Final Batch Loss: 0.33201831579208374\n",
      "Epoch 4003, Loss: 0.7934012413024902, Final Batch Loss: 0.29010480642318726\n",
      "Epoch 4004, Loss: 0.789966881275177, Final Batch Loss: 0.23220017552375793\n",
      "Epoch 4005, Loss: 0.9470978379249573, Final Batch Loss: 0.28867706656455994\n",
      "Epoch 4006, Loss: 0.8637920320034027, Final Batch Loss: 0.2685903012752533\n",
      "Epoch 4007, Loss: 0.8450588136911392, Final Batch Loss: 0.2998746931552887\n",
      "Epoch 4008, Loss: 0.7974638640880585, Final Batch Loss: 0.22349748015403748\n",
      "Epoch 4009, Loss: 0.8197846412658691, Final Batch Loss: 0.2595442235469818\n",
      "Epoch 4010, Loss: 0.8801272362470627, Final Batch Loss: 0.2655433416366577\n",
      "Epoch 4011, Loss: 0.8955874294042587, Final Batch Loss: 0.36738646030426025\n",
      "Epoch 4012, Loss: 0.8711462318897247, Final Batch Loss: 0.3477237820625305\n",
      "Epoch 4013, Loss: 0.7781752943992615, Final Batch Loss: 0.2451961785554886\n",
      "Epoch 4014, Loss: 0.7510318160057068, Final Batch Loss: 0.22904928028583527\n",
      "Epoch 4015, Loss: 0.732598140835762, Final Batch Loss: 0.2690959572792053\n",
      "Epoch 4016, Loss: 0.8026987016201019, Final Batch Loss: 0.28700077533721924\n",
      "Epoch 4017, Loss: 0.7965750694274902, Final Batch Loss: 0.17163842916488647\n",
      "Epoch 4018, Loss: 0.7532694488763809, Final Batch Loss: 0.23625411093235016\n",
      "Epoch 4019, Loss: 0.7975475937128067, Final Batch Loss: 0.16011478006839752\n",
      "Epoch 4020, Loss: 0.8630534708499908, Final Batch Loss: 0.3432735204696655\n",
      "Epoch 4021, Loss: 0.8560900390148163, Final Batch Loss: 0.21660050749778748\n",
      "Epoch 4022, Loss: 0.7802460789680481, Final Batch Loss: 0.2794300317764282\n",
      "Epoch 4023, Loss: 0.8207820057868958, Final Batch Loss: 0.2503145933151245\n",
      "Epoch 4024, Loss: 0.7702390998601913, Final Batch Loss: 0.2721077501773834\n",
      "Epoch 4025, Loss: 0.813886895775795, Final Batch Loss: 0.2779507040977478\n",
      "Epoch 4026, Loss: 0.691006675362587, Final Batch Loss: 0.15620334446430206\n",
      "Epoch 4027, Loss: 0.7994339764118195, Final Batch Loss: 0.24578696489334106\n",
      "Epoch 4028, Loss: 0.8629569113254547, Final Batch Loss: 0.22419756650924683\n",
      "Epoch 4029, Loss: 0.81088487803936, Final Batch Loss: 0.26793545484542847\n",
      "Epoch 4030, Loss: 0.8320916891098022, Final Batch Loss: 0.35182178020477295\n",
      "Epoch 4031, Loss: 0.830852597951889, Final Batch Loss: 0.17678683996200562\n",
      "Epoch 4032, Loss: 1.0561502873897552, Final Batch Loss: 0.4783518612384796\n",
      "Epoch 4033, Loss: 0.8087805509567261, Final Batch Loss: 0.30834123492240906\n",
      "Epoch 4034, Loss: 0.9141797721385956, Final Batch Loss: 0.3857773244380951\n",
      "Epoch 4035, Loss: 0.767414852976799, Final Batch Loss: 0.2374541312456131\n",
      "Epoch 4036, Loss: 0.8548140227794647, Final Batch Loss: 0.28588154911994934\n",
      "Epoch 4037, Loss: 0.8317570686340332, Final Batch Loss: 0.322042852640152\n",
      "Epoch 4038, Loss: 0.7069703340530396, Final Batch Loss: 0.18518871068954468\n",
      "Epoch 4039, Loss: 0.852453351020813, Final Batch Loss: 0.2961868345737457\n",
      "Epoch 4040, Loss: 0.872066855430603, Final Batch Loss: 0.2853681743144989\n",
      "Epoch 4041, Loss: 0.7741434723138809, Final Batch Loss: 0.19514773786067963\n",
      "Epoch 4042, Loss: 0.7649890780448914, Final Batch Loss: 0.2363424301147461\n",
      "Epoch 4043, Loss: 0.8593878298997879, Final Batch Loss: 0.3705556094646454\n",
      "Epoch 4044, Loss: 0.8739248812198639, Final Batch Loss: 0.2726891338825226\n",
      "Epoch 4045, Loss: 0.8540536016225815, Final Batch Loss: 0.3141425549983978\n",
      "Epoch 4046, Loss: 0.775624692440033, Final Batch Loss: 0.1620890200138092\n",
      "Epoch 4047, Loss: 0.8093128353357315, Final Batch Loss: 0.28152817487716675\n",
      "Epoch 4048, Loss: 0.7334826439619064, Final Batch Loss: 0.2253187745809555\n",
      "Epoch 4049, Loss: 0.7964805513620377, Final Batch Loss: 0.22591517865657806\n",
      "Epoch 4050, Loss: 0.9387189447879791, Final Batch Loss: 0.32163023948669434\n",
      "Epoch 4051, Loss: 0.8007669448852539, Final Batch Loss: 0.2650342285633087\n",
      "Epoch 4052, Loss: 0.7737226188182831, Final Batch Loss: 0.278717964887619\n",
      "Epoch 4053, Loss: 0.8413731455802917, Final Batch Loss: 0.3122331202030182\n",
      "Epoch 4054, Loss: 0.8209875375032425, Final Batch Loss: 0.28298041224479675\n",
      "Epoch 4055, Loss: 0.8617884814739227, Final Batch Loss: 0.2630220353603363\n",
      "Epoch 4056, Loss: 0.7922361791133881, Final Batch Loss: 0.272034615278244\n",
      "Epoch 4057, Loss: 0.7961469292640686, Final Batch Loss: 0.27664265036582947\n",
      "Epoch 4058, Loss: 0.7971633523702621, Final Batch Loss: 0.19548209011554718\n",
      "Epoch 4059, Loss: 0.8032570332288742, Final Batch Loss: 0.24040593206882477\n",
      "Epoch 4060, Loss: 0.7848653346300125, Final Batch Loss: 0.22391347587108612\n",
      "Epoch 4061, Loss: 0.8516854345798492, Final Batch Loss: 0.28667736053466797\n",
      "Epoch 4062, Loss: 0.9388402998447418, Final Batch Loss: 0.33737343549728394\n",
      "Epoch 4063, Loss: 0.8432387411594391, Final Batch Loss: 0.2778953015804291\n",
      "Epoch 4064, Loss: 0.7812571674585342, Final Batch Loss: 0.3062290549278259\n",
      "Epoch 4065, Loss: 0.7902473956346512, Final Batch Loss: 0.20939184725284576\n",
      "Epoch 4066, Loss: 0.7740923911333084, Final Batch Loss: 0.21346183121204376\n",
      "Epoch 4067, Loss: 0.8298980295658112, Final Batch Loss: 0.2724985182285309\n",
      "Epoch 4068, Loss: 0.740859180688858, Final Batch Loss: 0.248182475566864\n",
      "Epoch 4069, Loss: 0.9134933352470398, Final Batch Loss: 0.2597966492176056\n",
      "Epoch 4070, Loss: 0.802010253071785, Final Batch Loss: 0.2972870469093323\n",
      "Epoch 4071, Loss: 0.8117447197437286, Final Batch Loss: 0.28195884823799133\n",
      "Epoch 4072, Loss: 0.837699681520462, Final Batch Loss: 0.277139812707901\n",
      "Epoch 4073, Loss: 0.8147528320550919, Final Batch Loss: 0.26809489727020264\n",
      "Epoch 4074, Loss: 0.7722529172897339, Final Batch Loss: 0.12821456789970398\n",
      "Epoch 4075, Loss: 0.8607391268014908, Final Batch Loss: 0.24321527779102325\n",
      "Epoch 4076, Loss: 0.9074933528900146, Final Batch Loss: 0.2993515431880951\n",
      "Epoch 4077, Loss: 0.8434872180223465, Final Batch Loss: 0.29543665051460266\n",
      "Epoch 4078, Loss: 0.8143035024404526, Final Batch Loss: 0.23509444296360016\n",
      "Epoch 4079, Loss: 0.739916980266571, Final Batch Loss: 0.245491623878479\n",
      "Epoch 4080, Loss: 0.9792019575834274, Final Batch Loss: 0.39369118213653564\n",
      "Epoch 4081, Loss: 0.8320299237966537, Final Batch Loss: 0.30984097719192505\n",
      "Epoch 4082, Loss: 0.8567910194396973, Final Batch Loss: 0.3037152588367462\n",
      "Epoch 4083, Loss: 0.7527371793985367, Final Batch Loss: 0.25334274768829346\n",
      "Epoch 4084, Loss: 0.8017689138650894, Final Batch Loss: 0.2205946296453476\n",
      "Epoch 4085, Loss: 0.751977950334549, Final Batch Loss: 0.27130669355392456\n",
      "Epoch 4086, Loss: 0.8525843322277069, Final Batch Loss: 0.29788464307785034\n",
      "Epoch 4087, Loss: 0.751470223069191, Final Batch Loss: 0.26578062772750854\n",
      "Epoch 4088, Loss: 0.9170742332935333, Final Batch Loss: 0.3380351662635803\n",
      "Epoch 4089, Loss: 0.7640404999256134, Final Batch Loss: 0.24133612215518951\n",
      "Epoch 4090, Loss: 0.8738052248954773, Final Batch Loss: 0.3396783471107483\n",
      "Epoch 4091, Loss: 0.7542034238576889, Final Batch Loss: 0.19390596449375153\n",
      "Epoch 4092, Loss: 0.8903906047344208, Final Batch Loss: 0.3671709895133972\n",
      "Epoch 4093, Loss: 0.8087608814239502, Final Batch Loss: 0.28596869111061096\n",
      "Epoch 4094, Loss: 0.7511700242757797, Final Batch Loss: 0.25088298320770264\n",
      "Epoch 4095, Loss: 0.7793394327163696, Final Batch Loss: 0.22667351365089417\n",
      "Epoch 4096, Loss: 0.7408457100391388, Final Batch Loss: 0.21912935376167297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4097, Loss: 0.774842381477356, Final Batch Loss: 0.20944255590438843\n",
      "Epoch 4098, Loss: 0.8463459610939026, Final Batch Loss: 0.2521788775920868\n",
      "Epoch 4099, Loss: 0.8019880801439285, Final Batch Loss: 0.2931618094444275\n",
      "Epoch 4100, Loss: 0.8307174742221832, Final Batch Loss: 0.26859787106513977\n",
      "Epoch 4101, Loss: 0.7859157025814056, Final Batch Loss: 0.24357019364833832\n",
      "Epoch 4102, Loss: 0.7628224939107895, Final Batch Loss: 0.16749240458011627\n",
      "Epoch 4103, Loss: 0.7639441192150116, Final Batch Loss: 0.2113204002380371\n",
      "Epoch 4104, Loss: 0.8439533114433289, Final Batch Loss: 0.284554123878479\n",
      "Epoch 4105, Loss: 0.7792610079050064, Final Batch Loss: 0.23497238755226135\n",
      "Epoch 4106, Loss: 0.8204860538244247, Final Batch Loss: 0.3248724341392517\n",
      "Epoch 4107, Loss: 0.7499514520168304, Final Batch Loss: 0.19939586520195007\n",
      "Epoch 4108, Loss: 0.7700995653867722, Final Batch Loss: 0.20029707252979279\n",
      "Epoch 4109, Loss: 0.6991387605667114, Final Batch Loss: 0.17957240343093872\n",
      "Epoch 4110, Loss: 0.8791643679141998, Final Batch Loss: 0.3308027684688568\n",
      "Epoch 4111, Loss: 0.8012063801288605, Final Batch Loss: 0.331967294216156\n",
      "Epoch 4112, Loss: 0.7413921803236008, Final Batch Loss: 0.1899910271167755\n",
      "Epoch 4113, Loss: 0.7829771190881729, Final Batch Loss: 0.2895027697086334\n",
      "Epoch 4114, Loss: 0.8047546148300171, Final Batch Loss: 0.2282470166683197\n",
      "Epoch 4115, Loss: 0.8163832724094391, Final Batch Loss: 0.30597439408302307\n",
      "Epoch 4116, Loss: 0.7201409637928009, Final Batch Loss: 0.18985754251480103\n",
      "Epoch 4117, Loss: 0.7831570506095886, Final Batch Loss: 0.2836414575576782\n",
      "Epoch 4118, Loss: 0.8241877555847168, Final Batch Loss: 0.2285405695438385\n",
      "Epoch 4119, Loss: 0.7475080341100693, Final Batch Loss: 0.264277845621109\n",
      "Epoch 4120, Loss: 0.830406129360199, Final Batch Loss: 0.32485586404800415\n",
      "Epoch 4121, Loss: 0.8501156270503998, Final Batch Loss: 0.29311978816986084\n",
      "Epoch 4122, Loss: 0.7063285261392593, Final Batch Loss: 0.2541716396808624\n",
      "Epoch 4123, Loss: 0.7478115260601044, Final Batch Loss: 0.25471824407577515\n",
      "Epoch 4124, Loss: 0.8053733110427856, Final Batch Loss: 0.2949102818965912\n",
      "Epoch 4125, Loss: 0.775426596403122, Final Batch Loss: 0.2856792211532593\n",
      "Epoch 4126, Loss: 0.87081678211689, Final Batch Loss: 0.3697192072868347\n",
      "Epoch 4127, Loss: 0.7833452969789505, Final Batch Loss: 0.23699916899204254\n",
      "Epoch 4128, Loss: 0.7424332201480865, Final Batch Loss: 0.2442386895418167\n",
      "Epoch 4129, Loss: 0.7873429358005524, Final Batch Loss: 0.24107831716537476\n",
      "Epoch 4130, Loss: 0.7073106467723846, Final Batch Loss: 0.19656583666801453\n",
      "Epoch 4131, Loss: 0.8931070566177368, Final Batch Loss: 0.35826465487480164\n",
      "Epoch 4132, Loss: 0.7664154171943665, Final Batch Loss: 0.288726270198822\n",
      "Epoch 4133, Loss: 0.9004429578781128, Final Batch Loss: 0.3303028345108032\n",
      "Epoch 4134, Loss: 0.8122313320636749, Final Batch Loss: 0.24519026279449463\n",
      "Epoch 4135, Loss: 0.7847433239221573, Final Batch Loss: 0.25611206889152527\n",
      "Epoch 4136, Loss: 0.9011669754981995, Final Batch Loss: 0.2839197814464569\n",
      "Epoch 4137, Loss: 0.6989801675081253, Final Batch Loss: 0.22532294690608978\n",
      "Epoch 4138, Loss: 0.7366128265857697, Final Batch Loss: 0.22123226523399353\n",
      "Epoch 4139, Loss: 0.6889173537492752, Final Batch Loss: 0.2078806608915329\n",
      "Epoch 4140, Loss: 0.8918253481388092, Final Batch Loss: 0.2960435152053833\n",
      "Epoch 4141, Loss: 0.8079697638750076, Final Batch Loss: 0.24394191801548004\n",
      "Epoch 4142, Loss: 0.7342883348464966, Final Batch Loss: 0.22404518723487854\n",
      "Epoch 4143, Loss: 0.7908511161804199, Final Batch Loss: 0.23135080933570862\n",
      "Epoch 4144, Loss: 0.861813485622406, Final Batch Loss: 0.26077303290367126\n",
      "Epoch 4145, Loss: 0.7157961875200272, Final Batch Loss: 0.20168109238147736\n",
      "Epoch 4146, Loss: 0.8981320858001709, Final Batch Loss: 0.28667595982551575\n",
      "Epoch 4147, Loss: 0.8217826783657074, Final Batch Loss: 0.29894834756851196\n",
      "Epoch 4148, Loss: 0.7456155717372894, Final Batch Loss: 0.2555953860282898\n",
      "Epoch 4149, Loss: 0.8044666796922684, Final Batch Loss: 0.22411642968654633\n",
      "Epoch 4150, Loss: 0.8567875921726227, Final Batch Loss: 0.27686017751693726\n",
      "Epoch 4151, Loss: 0.7539046257734299, Final Batch Loss: 0.2733921408653259\n",
      "Epoch 4152, Loss: 0.8346506059169769, Final Batch Loss: 0.26484382152557373\n",
      "Epoch 4153, Loss: 0.6998725831508636, Final Batch Loss: 0.20108768343925476\n",
      "Epoch 4154, Loss: 0.7735209465026855, Final Batch Loss: 0.22150161862373352\n",
      "Epoch 4155, Loss: 0.7731080502271652, Final Batch Loss: 0.2294340878725052\n",
      "Epoch 4156, Loss: 0.9804355204105377, Final Batch Loss: 0.2923034727573395\n",
      "Epoch 4157, Loss: 0.7453380227088928, Final Batch Loss: 0.18642988801002502\n",
      "Epoch 4158, Loss: 0.7677581012248993, Final Batch Loss: 0.20527422428131104\n",
      "Epoch 4159, Loss: 0.8191016316413879, Final Batch Loss: 0.21035358309745789\n",
      "Epoch 4160, Loss: 0.901077538728714, Final Batch Loss: 0.31699317693710327\n",
      "Epoch 4161, Loss: 0.9031959772109985, Final Batch Loss: 0.2819966971874237\n",
      "Epoch 4162, Loss: 0.8051266521215439, Final Batch Loss: 0.20258750021457672\n",
      "Epoch 4163, Loss: 0.7687063813209534, Final Batch Loss: 0.2629818320274353\n",
      "Epoch 4164, Loss: 0.7695440202951431, Final Batch Loss: 0.2648106515407562\n",
      "Epoch 4165, Loss: 0.7830227017402649, Final Batch Loss: 0.2726660668849945\n",
      "Epoch 4166, Loss: 0.7353995740413666, Final Batch Loss: 0.2576245367527008\n",
      "Epoch 4167, Loss: 0.785040095448494, Final Batch Loss: 0.2695259153842926\n",
      "Epoch 4168, Loss: 0.818926140666008, Final Batch Loss: 0.25087517499923706\n",
      "Epoch 4169, Loss: 0.8428061306476593, Final Batch Loss: 0.30046531558036804\n",
      "Epoch 4170, Loss: 0.9351101815700531, Final Batch Loss: 0.35410550236701965\n",
      "Epoch 4171, Loss: 0.8477153033018112, Final Batch Loss: 0.2488069087266922\n",
      "Epoch 4172, Loss: 0.8003215938806534, Final Batch Loss: 0.1965171843767166\n",
      "Epoch 4173, Loss: 0.7344826459884644, Final Batch Loss: 0.23675797879695892\n",
      "Epoch 4174, Loss: 0.7206006199121475, Final Batch Loss: 0.2750782370567322\n",
      "Epoch 4175, Loss: 0.831006795167923, Final Batch Loss: 0.27096834778785706\n",
      "Epoch 4176, Loss: 0.8962679803371429, Final Batch Loss: 0.27715036273002625\n",
      "Epoch 4177, Loss: 0.8958622515201569, Final Batch Loss: 0.30337053537368774\n",
      "Epoch 4178, Loss: 0.8040251284837723, Final Batch Loss: 0.23276282846927643\n",
      "Epoch 4179, Loss: 0.803620845079422, Final Batch Loss: 0.23753949999809265\n",
      "Epoch 4180, Loss: 0.7954600751399994, Final Batch Loss: 0.22967329621315002\n",
      "Epoch 4181, Loss: 0.7969332933425903, Final Batch Loss: 0.19818519055843353\n",
      "Epoch 4182, Loss: 0.7400496155023575, Final Batch Loss: 0.22087621688842773\n",
      "Epoch 4183, Loss: 0.7310333997011185, Final Batch Loss: 0.2167525440454483\n",
      "Epoch 4184, Loss: 0.8215354382991791, Final Batch Loss: 0.2951221466064453\n",
      "Epoch 4185, Loss: 0.795212984085083, Final Batch Loss: 0.23501360416412354\n",
      "Epoch 4186, Loss: 0.8854696154594421, Final Batch Loss: 0.27864736318588257\n",
      "Epoch 4187, Loss: 0.7986693978309631, Final Batch Loss: 0.28783687949180603\n",
      "Epoch 4188, Loss: 0.7821462154388428, Final Batch Loss: 0.27629533410072327\n",
      "Epoch 4189, Loss: 0.7311926335096359, Final Batch Loss: 0.1981579214334488\n",
      "Epoch 4190, Loss: 0.748487651348114, Final Batch Loss: 0.270095556974411\n",
      "Epoch 4191, Loss: 0.8758434355258942, Final Batch Loss: 0.26295551657676697\n",
      "Epoch 4192, Loss: 0.8110244274139404, Final Batch Loss: 0.3290860950946808\n",
      "Epoch 4193, Loss: 0.8904560804367065, Final Batch Loss: 0.3597991168498993\n",
      "Epoch 4194, Loss: 0.8321269154548645, Final Batch Loss: 0.2954738140106201\n",
      "Epoch 4195, Loss: 0.7529568821191788, Final Batch Loss: 0.24554689228534698\n",
      "Epoch 4196, Loss: 0.8030692636966705, Final Batch Loss: 0.26406630873680115\n",
      "Epoch 4197, Loss: 0.7786834985017776, Final Batch Loss: 0.22607429325580597\n",
      "Epoch 4198, Loss: 0.683134451508522, Final Batch Loss: 0.21069207787513733\n",
      "Epoch 4199, Loss: 0.7292920053005219, Final Batch Loss: 0.2592489421367645\n",
      "Epoch 4200, Loss: 0.7351275980472565, Final Batch Loss: 0.20320254564285278\n",
      "Epoch 4201, Loss: 0.8099735230207443, Final Batch Loss: 0.32247278094291687\n",
      "Epoch 4202, Loss: 0.8991674184799194, Final Batch Loss: 0.3854958117008209\n",
      "Epoch 4203, Loss: 0.7996068298816681, Final Batch Loss: 0.28870269656181335\n",
      "Epoch 4204, Loss: 0.7690184265375137, Final Batch Loss: 0.26754671335220337\n",
      "Epoch 4205, Loss: 0.7490948736667633, Final Batch Loss: 0.23018470406532288\n",
      "Epoch 4206, Loss: 0.9538706541061401, Final Batch Loss: 0.4059181213378906\n",
      "Epoch 4207, Loss: 0.758107453584671, Final Batch Loss: 0.24264733493328094\n",
      "Epoch 4208, Loss: 0.9405428767204285, Final Batch Loss: 0.419294148683548\n",
      "Epoch 4209, Loss: 0.7465667575597763, Final Batch Loss: 0.23874162137508392\n",
      "Epoch 4210, Loss: 0.8422626554965973, Final Batch Loss: 0.32166972756385803\n",
      "Epoch 4211, Loss: 0.7708649784326553, Final Batch Loss: 0.2500339150428772\n",
      "Epoch 4212, Loss: 0.8365947604179382, Final Batch Loss: 0.2672874331474304\n",
      "Epoch 4213, Loss: 0.8875664323568344, Final Batch Loss: 0.3276641070842743\n",
      "Epoch 4214, Loss: 0.803890585899353, Final Batch Loss: 0.23589766025543213\n",
      "Epoch 4215, Loss: 0.8117000609636307, Final Batch Loss: 0.2556309103965759\n",
      "Epoch 4216, Loss: 0.7246353775262833, Final Batch Loss: 0.21198879182338715\n",
      "Epoch 4217, Loss: 0.8002115488052368, Final Batch Loss: 0.22106114029884338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4218, Loss: 0.8555727005004883, Final Batch Loss: 0.33159056305885315\n",
      "Epoch 4219, Loss: 0.7465472519397736, Final Batch Loss: 0.19971957802772522\n",
      "Epoch 4220, Loss: 0.8667712956666946, Final Batch Loss: 0.3269874155521393\n",
      "Epoch 4221, Loss: 0.8264284133911133, Final Batch Loss: 0.28828248381614685\n",
      "Epoch 4222, Loss: 0.7629043459892273, Final Batch Loss: 0.25123459100723267\n",
      "Epoch 4223, Loss: 0.7736337184906006, Final Batch Loss: 0.23506277799606323\n",
      "Epoch 4224, Loss: 0.7528290450572968, Final Batch Loss: 0.25015291571617126\n",
      "Epoch 4225, Loss: 0.7988169938325882, Final Batch Loss: 0.2146998792886734\n",
      "Epoch 4226, Loss: 0.9664068073034286, Final Batch Loss: 0.4458233714103699\n",
      "Epoch 4227, Loss: 0.8513186573982239, Final Batch Loss: 0.28805848956108093\n",
      "Epoch 4228, Loss: 0.8168675452470779, Final Batch Loss: 0.2997496724128723\n",
      "Epoch 4229, Loss: 0.725785493850708, Final Batch Loss: 0.25503966212272644\n",
      "Epoch 4230, Loss: 0.9072102606296539, Final Batch Loss: 0.20549821853637695\n",
      "Epoch 4231, Loss: 0.7809002995491028, Final Batch Loss: 0.2639212906360626\n",
      "Epoch 4232, Loss: 0.8193228393793106, Final Batch Loss: 0.32002049684524536\n",
      "Epoch 4233, Loss: 0.859434962272644, Final Batch Loss: 0.3830045759677887\n",
      "Epoch 4234, Loss: 0.8747731447219849, Final Batch Loss: 0.3496648073196411\n",
      "Epoch 4235, Loss: 0.7008801400661469, Final Batch Loss: 0.20558395981788635\n",
      "Epoch 4236, Loss: 0.7519290596246719, Final Batch Loss: 0.2627667784690857\n",
      "Epoch 4237, Loss: 0.8032941222190857, Final Batch Loss: 0.252440869808197\n",
      "Epoch 4238, Loss: 0.7667027860879898, Final Batch Loss: 0.28677433729171753\n",
      "Epoch 4239, Loss: 0.7210617810487747, Final Batch Loss: 0.23008586466312408\n",
      "Epoch 4240, Loss: 0.7753540277481079, Final Batch Loss: 0.2661162316799164\n",
      "Epoch 4241, Loss: 0.7111563384532928, Final Batch Loss: 0.2533192038536072\n",
      "Epoch 4242, Loss: 0.786830946803093, Final Batch Loss: 0.24412117898464203\n",
      "Epoch 4243, Loss: 0.6846423149108887, Final Batch Loss: 0.18751777708530426\n",
      "Epoch 4244, Loss: 0.8863550573587418, Final Batch Loss: 0.32272228598594666\n",
      "Epoch 4245, Loss: 0.7074362337589264, Final Batch Loss: 0.2234942764043808\n",
      "Epoch 4246, Loss: 0.6569828242063522, Final Batch Loss: 0.1676778942346573\n",
      "Epoch 4247, Loss: 0.8243170082569122, Final Batch Loss: 0.2544251084327698\n",
      "Epoch 4248, Loss: 0.8147118985652924, Final Batch Loss: 0.3642295002937317\n",
      "Epoch 4249, Loss: 0.8155563473701477, Final Batch Loss: 0.3030420243740082\n",
      "Epoch 4250, Loss: 0.7690677642822266, Final Batch Loss: 0.3052072823047638\n",
      "Epoch 4251, Loss: 0.7782599031925201, Final Batch Loss: 0.227773517370224\n",
      "Epoch 4252, Loss: 0.883930891752243, Final Batch Loss: 0.3110576868057251\n",
      "Epoch 4253, Loss: 0.8175814747810364, Final Batch Loss: 0.2715364396572113\n",
      "Epoch 4254, Loss: 0.7518482357263565, Final Batch Loss: 0.2538667321205139\n",
      "Epoch 4255, Loss: 0.8991668522357941, Final Batch Loss: 0.27046361565589905\n",
      "Epoch 4256, Loss: 0.7063527405261993, Final Batch Loss: 0.2375127524137497\n",
      "Epoch 4257, Loss: 0.894997239112854, Final Batch Loss: 0.3971399664878845\n",
      "Epoch 4258, Loss: 0.7646210342645645, Final Batch Loss: 0.23689110577106476\n",
      "Epoch 4259, Loss: 0.8483068346977234, Final Batch Loss: 0.29239267110824585\n",
      "Epoch 4260, Loss: 0.767935186624527, Final Batch Loss: 0.23285439610481262\n",
      "Epoch 4261, Loss: 0.8159182667732239, Final Batch Loss: 0.32318779826164246\n",
      "Epoch 4262, Loss: 0.756835475564003, Final Batch Loss: 0.2279977649450302\n",
      "Epoch 4263, Loss: 0.8515387773513794, Final Batch Loss: 0.27484700083732605\n",
      "Epoch 4264, Loss: 0.7623806297779083, Final Batch Loss: 0.2722305357456207\n",
      "Epoch 4265, Loss: 0.7674144804477692, Final Batch Loss: 0.2985459864139557\n",
      "Epoch 4266, Loss: 0.8426169455051422, Final Batch Loss: 0.26438382267951965\n",
      "Epoch 4267, Loss: 0.9705381989479065, Final Batch Loss: 0.3541184365749359\n",
      "Epoch 4268, Loss: 0.801103413105011, Final Batch Loss: 0.31462666392326355\n",
      "Epoch 4269, Loss: 0.9483225643634796, Final Batch Loss: 0.40530458092689514\n",
      "Epoch 4270, Loss: 0.8029899299144745, Final Batch Loss: 0.2447797656059265\n",
      "Epoch 4271, Loss: 0.8077691942453384, Final Batch Loss: 0.15580852329730988\n",
      "Epoch 4272, Loss: 0.8231823295354843, Final Batch Loss: 0.22975794970989227\n",
      "Epoch 4273, Loss: 0.9379565417766571, Final Batch Loss: 0.3418871760368347\n",
      "Epoch 4274, Loss: 0.8259429484605789, Final Batch Loss: 0.304930180311203\n",
      "Epoch 4275, Loss: 0.8039704114198685, Final Batch Loss: 0.29236170649528503\n",
      "Epoch 4276, Loss: 0.7643012255430222, Final Batch Loss: 0.29210853576660156\n",
      "Epoch 4277, Loss: 0.8747719675302505, Final Batch Loss: 0.40179991722106934\n",
      "Epoch 4278, Loss: 0.9015006721019745, Final Batch Loss: 0.27467337250709534\n",
      "Epoch 4279, Loss: 0.8584528714418411, Final Batch Loss: 0.3666396737098694\n",
      "Epoch 4280, Loss: 0.7846265882253647, Final Batch Loss: 0.23408450186252594\n",
      "Epoch 4281, Loss: 0.8232252448797226, Final Batch Loss: 0.29311302304267883\n",
      "Epoch 4282, Loss: 0.8221590220928192, Final Batch Loss: 0.23881056904792786\n",
      "Epoch 4283, Loss: 0.8652612715959549, Final Batch Loss: 0.31764280796051025\n",
      "Epoch 4284, Loss: 0.7625078856945038, Final Batch Loss: 0.26101455092430115\n",
      "Epoch 4285, Loss: 0.7782289236783981, Final Batch Loss: 0.3339541554450989\n",
      "Epoch 4286, Loss: 0.7977559566497803, Final Batch Loss: 0.2672218084335327\n",
      "Epoch 4287, Loss: 0.7696237564086914, Final Batch Loss: 0.2543802261352539\n",
      "Epoch 4288, Loss: 0.7521959394216537, Final Batch Loss: 0.19833464920520782\n",
      "Epoch 4289, Loss: 0.7071331143379211, Final Batch Loss: 0.21505770087242126\n",
      "Epoch 4290, Loss: 0.956419438123703, Final Batch Loss: 0.34871020913124084\n",
      "Epoch 4291, Loss: 0.8291101008653641, Final Batch Loss: 0.22351668775081635\n",
      "Epoch 4292, Loss: 0.785169929265976, Final Batch Loss: 0.20882952213287354\n",
      "Epoch 4293, Loss: 0.8048076778650284, Final Batch Loss: 0.29384034872055054\n",
      "Epoch 4294, Loss: 0.8105577081441879, Final Batch Loss: 0.23127274215221405\n",
      "Epoch 4295, Loss: 0.8137183487415314, Final Batch Loss: 0.2328038513660431\n",
      "Epoch 4296, Loss: 0.8006855696439743, Final Batch Loss: 0.3131060302257538\n",
      "Epoch 4297, Loss: 0.7333368211984634, Final Batch Loss: 0.2473803013563156\n",
      "Epoch 4298, Loss: 0.8019582629203796, Final Batch Loss: 0.33361878991127014\n",
      "Epoch 4299, Loss: 0.7862851321697235, Final Batch Loss: 0.28482896089553833\n",
      "Epoch 4300, Loss: 0.7446059584617615, Final Batch Loss: 0.27020394802093506\n",
      "Epoch 4301, Loss: 0.7472657561302185, Final Batch Loss: 0.27404388785362244\n",
      "Epoch 4302, Loss: 0.8104832172393799, Final Batch Loss: 0.2856088876724243\n",
      "Epoch 4303, Loss: 0.8496489226818085, Final Batch Loss: 0.3823983669281006\n",
      "Epoch 4304, Loss: 0.7992113381624222, Final Batch Loss: 0.223252072930336\n",
      "Epoch 4305, Loss: 0.7913178950548172, Final Batch Loss: 0.23118765652179718\n",
      "Epoch 4306, Loss: 0.830491840839386, Final Batch Loss: 0.3739369511604309\n",
      "Epoch 4307, Loss: 0.7976782321929932, Final Batch Loss: 0.2948949337005615\n",
      "Epoch 4308, Loss: 0.9237135052680969, Final Batch Loss: 0.38590073585510254\n",
      "Epoch 4309, Loss: 0.7761665135622025, Final Batch Loss: 0.314784973859787\n",
      "Epoch 4310, Loss: 0.7883691191673279, Final Batch Loss: 0.26599952578544617\n",
      "Epoch 4311, Loss: 0.8556735962629318, Final Batch Loss: 0.27226564288139343\n",
      "Epoch 4312, Loss: 0.8094255477190018, Final Batch Loss: 0.2755962908267975\n",
      "Epoch 4313, Loss: 0.9383738040924072, Final Batch Loss: 0.34673264622688293\n",
      "Epoch 4314, Loss: 0.8874829262495041, Final Batch Loss: 0.3574755787849426\n",
      "Epoch 4315, Loss: 0.8101454377174377, Final Batch Loss: 0.22612407803535461\n",
      "Epoch 4316, Loss: 0.8216930329799652, Final Batch Loss: 0.25079667568206787\n",
      "Epoch 4317, Loss: 0.8193514943122864, Final Batch Loss: 0.2761954367160797\n",
      "Epoch 4318, Loss: 0.8073205649852753, Final Batch Loss: 0.26131996512413025\n",
      "Epoch 4319, Loss: 0.7626336067914963, Final Batch Loss: 0.27246716618537903\n",
      "Epoch 4320, Loss: 0.7630096673965454, Final Batch Loss: 0.23176783323287964\n",
      "Epoch 4321, Loss: 0.8654284179210663, Final Batch Loss: 0.29812389612197876\n",
      "Epoch 4322, Loss: 0.733173057436943, Final Batch Loss: 0.1985744833946228\n",
      "Epoch 4323, Loss: 0.7963927239179611, Final Batch Loss: 0.3319764733314514\n",
      "Epoch 4324, Loss: 0.7140357345342636, Final Batch Loss: 0.2411791831254959\n",
      "Epoch 4325, Loss: 0.8412532955408096, Final Batch Loss: 0.36895930767059326\n",
      "Epoch 4326, Loss: 0.875867947936058, Final Batch Loss: 0.34941980242729187\n",
      "Epoch 4327, Loss: 0.7814825773239136, Final Batch Loss: 0.1919645369052887\n",
      "Epoch 4328, Loss: 0.8152023255825043, Final Batch Loss: 0.24870434403419495\n",
      "Epoch 4329, Loss: 0.80531345307827, Final Batch Loss: 0.31093892455101013\n",
      "Epoch 4330, Loss: 0.801530733704567, Final Batch Loss: 0.2482278198003769\n",
      "Epoch 4331, Loss: 0.7826802134513855, Final Batch Loss: 0.26498574018478394\n",
      "Epoch 4332, Loss: 0.8553236126899719, Final Batch Loss: 0.24383580684661865\n",
      "Epoch 4333, Loss: 0.8181392401456833, Final Batch Loss: 0.25108277797698975\n",
      "Epoch 4334, Loss: 0.743142306804657, Final Batch Loss: 0.22431479394435883\n",
      "Epoch 4335, Loss: 0.7375026792287827, Final Batch Loss: 0.24107298254966736\n",
      "Epoch 4336, Loss: 0.7913115471601486, Final Batch Loss: 0.27745744585990906\n",
      "Epoch 4337, Loss: 0.7889551222324371, Final Batch Loss: 0.16514095664024353\n",
      "Epoch 4338, Loss: 0.8712583780288696, Final Batch Loss: 0.2855757176876068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4339, Loss: 0.7021780014038086, Final Batch Loss: 0.2161203920841217\n",
      "Epoch 4340, Loss: 0.8104497194290161, Final Batch Loss: 0.2560684382915497\n",
      "Epoch 4341, Loss: 0.721000924706459, Final Batch Loss: 0.24602089822292328\n",
      "Epoch 4342, Loss: 0.8563206791877747, Final Batch Loss: 0.35589608550071716\n",
      "Epoch 4343, Loss: 0.858844131231308, Final Batch Loss: 0.2634909749031067\n",
      "Epoch 4344, Loss: 0.8475137203931808, Final Batch Loss: 0.27497848868370056\n",
      "Epoch 4345, Loss: 0.7616080343723297, Final Batch Loss: 0.26526668667793274\n",
      "Epoch 4346, Loss: 0.8269274532794952, Final Batch Loss: 0.2429393082857132\n",
      "Epoch 4347, Loss: 0.8529868721961975, Final Batch Loss: 0.27109506726264954\n",
      "Epoch 4348, Loss: 0.7592027336359024, Final Batch Loss: 0.24424563348293304\n",
      "Epoch 4349, Loss: 0.7754054218530655, Final Batch Loss: 0.2238057553768158\n",
      "Epoch 4350, Loss: 0.8102191835641861, Final Batch Loss: 0.27913039922714233\n",
      "Epoch 4351, Loss: 0.7864732146263123, Final Batch Loss: 0.2998332977294922\n",
      "Epoch 4352, Loss: 0.8599385917186737, Final Batch Loss: 0.2863277792930603\n",
      "Epoch 4353, Loss: 0.8862410485744476, Final Batch Loss: 0.29720738530158997\n",
      "Epoch 4354, Loss: 0.8063644766807556, Final Batch Loss: 0.25630810856819153\n",
      "Epoch 4355, Loss: 0.812420129776001, Final Batch Loss: 0.2356002777814865\n",
      "Epoch 4356, Loss: 0.8274775296449661, Final Batch Loss: 0.36440926790237427\n",
      "Epoch 4357, Loss: 0.7496326118707657, Final Batch Loss: 0.2169489711523056\n",
      "Epoch 4358, Loss: 0.8554242849349976, Final Batch Loss: 0.27042385935783386\n",
      "Epoch 4359, Loss: 0.8285157680511475, Final Batch Loss: 0.296899676322937\n",
      "Epoch 4360, Loss: 0.8233171254396439, Final Batch Loss: 0.32104405760765076\n",
      "Epoch 4361, Loss: 0.7093274742364883, Final Batch Loss: 0.22199787199497223\n",
      "Epoch 4362, Loss: 0.7162194848060608, Final Batch Loss: 0.2173454314470291\n",
      "Epoch 4363, Loss: 1.0049194246530533, Final Batch Loss: 0.5268188714981079\n",
      "Epoch 4364, Loss: 0.722795158624649, Final Batch Loss: 0.18073108792304993\n",
      "Epoch 4365, Loss: 0.7449911236763, Final Batch Loss: 0.18949761986732483\n",
      "Epoch 4366, Loss: 0.796520322561264, Final Batch Loss: 0.26504334807395935\n",
      "Epoch 4367, Loss: 0.7292064726352692, Final Batch Loss: 0.26479247212409973\n",
      "Epoch 4368, Loss: 0.8739822208881378, Final Batch Loss: 0.34864944219589233\n",
      "Epoch 4369, Loss: 0.725148543715477, Final Batch Loss: 0.2873474955558777\n",
      "Epoch 4370, Loss: 0.7549886554479599, Final Batch Loss: 0.2078859657049179\n",
      "Epoch 4371, Loss: 0.7538331896066666, Final Batch Loss: 0.18365927040576935\n",
      "Epoch 4372, Loss: 0.7330246567726135, Final Batch Loss: 0.21702779829502106\n",
      "Epoch 4373, Loss: 0.8224424123764038, Final Batch Loss: 0.25786954164505005\n",
      "Epoch 4374, Loss: 0.8690146952867508, Final Batch Loss: 0.20631955564022064\n",
      "Epoch 4375, Loss: 0.7348185479640961, Final Batch Loss: 0.2560727596282959\n",
      "Epoch 4376, Loss: 0.8169340491294861, Final Batch Loss: 0.21637699007987976\n",
      "Epoch 4377, Loss: 0.8074659258127213, Final Batch Loss: 0.29939430952072144\n",
      "Epoch 4378, Loss: 0.7849474102258682, Final Batch Loss: 0.2454412579536438\n",
      "Epoch 4379, Loss: 0.8370328545570374, Final Batch Loss: 0.2957932949066162\n",
      "Epoch 4380, Loss: 0.8139562904834747, Final Batch Loss: 0.2718667685985565\n",
      "Epoch 4381, Loss: 0.8840956836938858, Final Batch Loss: 0.35416603088378906\n",
      "Epoch 4382, Loss: 0.734997883439064, Final Batch Loss: 0.24102096259593964\n",
      "Epoch 4383, Loss: 0.7252914905548096, Final Batch Loss: 0.17345163226127625\n",
      "Epoch 4384, Loss: 0.8941492140293121, Final Batch Loss: 0.2899008095264435\n",
      "Epoch 4385, Loss: 0.8699875175952911, Final Batch Loss: 0.25832119584083557\n",
      "Epoch 4386, Loss: 0.812290757894516, Final Batch Loss: 0.3409767150878906\n",
      "Epoch 4387, Loss: 0.8800408989191055, Final Batch Loss: 0.28790393471717834\n",
      "Epoch 4388, Loss: 0.7702410221099854, Final Batch Loss: 0.30556294322013855\n",
      "Epoch 4389, Loss: 0.8000294715166092, Final Batch Loss: 0.24939341843128204\n",
      "Epoch 4390, Loss: 0.7467423975467682, Final Batch Loss: 0.20530614256858826\n",
      "Epoch 4391, Loss: 0.774858683347702, Final Batch Loss: 0.22825701534748077\n",
      "Epoch 4392, Loss: 0.8097839951515198, Final Batch Loss: 0.2742859721183777\n",
      "Epoch 4393, Loss: 0.8418298065662384, Final Batch Loss: 0.30951470136642456\n",
      "Epoch 4394, Loss: 0.8287716805934906, Final Batch Loss: 0.26131707429885864\n",
      "Epoch 4395, Loss: 0.7457412779331207, Final Batch Loss: 0.309810072183609\n",
      "Epoch 4396, Loss: 0.7771499007940292, Final Batch Loss: 0.26739996671676636\n",
      "Epoch 4397, Loss: 0.7563274651765823, Final Batch Loss: 0.2675318419933319\n",
      "Epoch 4398, Loss: 0.8159060776233673, Final Batch Loss: 0.27085021138191223\n",
      "Epoch 4399, Loss: 0.7886190712451935, Final Batch Loss: 0.22320565581321716\n",
      "Epoch 4400, Loss: 0.788994699716568, Final Batch Loss: 0.25412002205848694\n",
      "Epoch 4401, Loss: 0.7338467389345169, Final Batch Loss: 0.23369354009628296\n",
      "Epoch 4402, Loss: 0.8257718533277512, Final Batch Loss: 0.24669496715068817\n",
      "Epoch 4403, Loss: 0.8313981890678406, Final Batch Loss: 0.30188998579978943\n",
      "Epoch 4404, Loss: 0.7768523246049881, Final Batch Loss: 0.2058943659067154\n",
      "Epoch 4405, Loss: 0.9154966771602631, Final Batch Loss: 0.36180388927459717\n",
      "Epoch 4406, Loss: 0.6607330143451691, Final Batch Loss: 0.19745364785194397\n",
      "Epoch 4407, Loss: 0.7488133907318115, Final Batch Loss: 0.2658787965774536\n",
      "Epoch 4408, Loss: 0.8115366697311401, Final Batch Loss: 0.25096675753593445\n",
      "Epoch 4409, Loss: 0.7998624593019485, Final Batch Loss: 0.25319570302963257\n",
      "Epoch 4410, Loss: 0.8213062882423401, Final Batch Loss: 0.24095475673675537\n",
      "Epoch 4411, Loss: 0.8964175134897232, Final Batch Loss: 0.24740056693553925\n",
      "Epoch 4412, Loss: 0.6998045593500137, Final Batch Loss: 0.23626549541950226\n",
      "Epoch 4413, Loss: 0.7511700540781021, Final Batch Loss: 0.2356378436088562\n",
      "Epoch 4414, Loss: 0.8659704476594925, Final Batch Loss: 0.2647259831428528\n",
      "Epoch 4415, Loss: 0.7646631300449371, Final Batch Loss: 0.2660352289676666\n",
      "Epoch 4416, Loss: 0.6762381345033646, Final Batch Loss: 0.23039770126342773\n",
      "Epoch 4417, Loss: 0.8052700608968735, Final Batch Loss: 0.24451370537281036\n",
      "Epoch 4418, Loss: 0.8075952529907227, Final Batch Loss: 0.28105980157852173\n",
      "Epoch 4419, Loss: 0.7147522419691086, Final Batch Loss: 0.22543412446975708\n",
      "Epoch 4420, Loss: 0.8575043082237244, Final Batch Loss: 0.37159422039985657\n",
      "Epoch 4421, Loss: 0.7508549690246582, Final Batch Loss: 0.23075202107429504\n",
      "Epoch 4422, Loss: 0.7853337973356247, Final Batch Loss: 0.22443124651908875\n",
      "Epoch 4423, Loss: 0.7472447603940964, Final Batch Loss: 0.1698676496744156\n",
      "Epoch 4424, Loss: 0.8030909895896912, Final Batch Loss: 0.30818724632263184\n",
      "Epoch 4425, Loss: 0.8023999780416489, Final Batch Loss: 0.2297290712594986\n",
      "Epoch 4426, Loss: 0.8322623372077942, Final Batch Loss: 0.32105594873428345\n",
      "Epoch 4427, Loss: 0.841045007109642, Final Batch Loss: 0.31797146797180176\n",
      "Epoch 4428, Loss: 0.8341434299945831, Final Batch Loss: 0.27903497219085693\n",
      "Epoch 4429, Loss: 0.719565287232399, Final Batch Loss: 0.2124205380678177\n",
      "Epoch 4430, Loss: 0.8243545889854431, Final Batch Loss: 0.301948606967926\n",
      "Epoch 4431, Loss: 0.8022420853376389, Final Batch Loss: 0.29770711064338684\n",
      "Epoch 4432, Loss: 0.7317271381616592, Final Batch Loss: 0.2145981639623642\n",
      "Epoch 4433, Loss: 0.7896812856197357, Final Batch Loss: 0.2342299222946167\n",
      "Epoch 4434, Loss: 0.7679472118616104, Final Batch Loss: 0.21820051968097687\n",
      "Epoch 4435, Loss: 0.8467976450920105, Final Batch Loss: 0.3044661581516266\n",
      "Epoch 4436, Loss: 0.9458314180374146, Final Batch Loss: 0.426797479391098\n",
      "Epoch 4437, Loss: 0.7982745319604874, Final Batch Loss: 0.24367143213748932\n",
      "Epoch 4438, Loss: 0.703764408826828, Final Batch Loss: 0.2334253340959549\n",
      "Epoch 4439, Loss: 0.7748947143554688, Final Batch Loss: 0.25106990337371826\n",
      "Epoch 4440, Loss: 0.7344994246959686, Final Batch Loss: 0.1994970738887787\n",
      "Epoch 4441, Loss: 0.8478043973445892, Final Batch Loss: 0.23094680905342102\n",
      "Epoch 4442, Loss: 0.7886488139629364, Final Batch Loss: 0.25979313254356384\n",
      "Epoch 4443, Loss: 0.7181611359119415, Final Batch Loss: 0.22913073003292084\n",
      "Epoch 4444, Loss: 0.7754211127758026, Final Batch Loss: 0.27885982394218445\n",
      "Epoch 4445, Loss: 0.7676955014467239, Final Batch Loss: 0.20817039906978607\n",
      "Epoch 4446, Loss: 0.9061390459537506, Final Batch Loss: 0.34207358956336975\n",
      "Epoch 4447, Loss: 0.6459337323904037, Final Batch Loss: 0.18407641351222992\n",
      "Epoch 4448, Loss: 0.6757868528366089, Final Batch Loss: 0.16435304284095764\n",
      "Epoch 4449, Loss: 0.7582950592041016, Final Batch Loss: 0.23967969417572021\n",
      "Epoch 4450, Loss: 0.7123077809810638, Final Batch Loss: 0.243374302983284\n",
      "Epoch 4451, Loss: 0.76799476146698, Final Batch Loss: 0.20334208011627197\n",
      "Epoch 4452, Loss: 0.8069533854722977, Final Batch Loss: 0.3014996647834778\n",
      "Epoch 4453, Loss: 0.7943037450313568, Final Batch Loss: 0.2503894865512848\n",
      "Epoch 4454, Loss: 0.7593724727630615, Final Batch Loss: 0.23034127056598663\n",
      "Epoch 4455, Loss: 0.6936099380254745, Final Batch Loss: 0.1819944828748703\n",
      "Epoch 4456, Loss: 0.7669142931699753, Final Batch Loss: 0.23217646777629852\n",
      "Epoch 4457, Loss: 0.7625396400690079, Final Batch Loss: 0.23921063542366028\n",
      "Epoch 4458, Loss: 0.7975138872861862, Final Batch Loss: 0.26102787256240845\n",
      "Epoch 4459, Loss: 0.7783637344837189, Final Batch Loss: 0.26384979486465454\n",
      "Epoch 4460, Loss: 0.7223802357912064, Final Batch Loss: 0.22174447774887085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4461, Loss: 0.7379040718078613, Final Batch Loss: 0.1760009378194809\n",
      "Epoch 4462, Loss: 0.8325831145048141, Final Batch Loss: 0.3352440595626831\n",
      "Epoch 4463, Loss: 0.7436998039484024, Final Batch Loss: 0.2553139626979828\n",
      "Epoch 4464, Loss: 0.8465525209903717, Final Batch Loss: 0.3268398940563202\n",
      "Epoch 4465, Loss: 0.8161707073450089, Final Batch Loss: 0.2799432575702667\n",
      "Epoch 4466, Loss: 0.7917259335517883, Final Batch Loss: 0.2616164982318878\n",
      "Epoch 4467, Loss: 0.7378311008214951, Final Batch Loss: 0.2661806643009186\n",
      "Epoch 4468, Loss: 0.7832795977592468, Final Batch Loss: 0.2275277078151703\n",
      "Epoch 4469, Loss: 0.7257529944181442, Final Batch Loss: 0.24828213453292847\n",
      "Epoch 4470, Loss: 0.8859237730503082, Final Batch Loss: 0.2909923493862152\n",
      "Epoch 4471, Loss: 0.724128857254982, Final Batch Loss: 0.21791143715381622\n",
      "Epoch 4472, Loss: 0.8489443957805634, Final Batch Loss: 0.2895049750804901\n",
      "Epoch 4473, Loss: 0.7855512052774429, Final Batch Loss: 0.247922882437706\n",
      "Epoch 4474, Loss: 0.74394391477108, Final Batch Loss: 0.2558083236217499\n",
      "Epoch 4475, Loss: 0.7296137809753418, Final Batch Loss: 0.21582652628421783\n",
      "Epoch 4476, Loss: 0.7746524810791016, Final Batch Loss: 0.21167919039726257\n",
      "Epoch 4477, Loss: 0.7869767844676971, Final Batch Loss: 0.28118646144866943\n",
      "Epoch 4478, Loss: 1.0177215486764908, Final Batch Loss: 0.47599732875823975\n",
      "Epoch 4479, Loss: 0.7012619376182556, Final Batch Loss: 0.17304468154907227\n",
      "Epoch 4480, Loss: 0.841576486825943, Final Batch Loss: 0.24895384907722473\n",
      "Epoch 4481, Loss: 0.7302098870277405, Final Batch Loss: 0.18778479099273682\n",
      "Epoch 4482, Loss: 0.7443028390407562, Final Batch Loss: 0.17652195692062378\n",
      "Epoch 4483, Loss: 0.8601265847682953, Final Batch Loss: 0.2666725218296051\n",
      "Epoch 4484, Loss: 0.91045942902565, Final Batch Loss: 0.3415541350841522\n",
      "Epoch 4485, Loss: 0.7864523231983185, Final Batch Loss: 0.2644031345844269\n",
      "Epoch 4486, Loss: 0.7511629611253738, Final Batch Loss: 0.2786528468132019\n",
      "Epoch 4487, Loss: 0.7568376809358597, Final Batch Loss: 0.22918130457401276\n",
      "Epoch 4488, Loss: 0.7632573246955872, Final Batch Loss: 0.26399287581443787\n",
      "Epoch 4489, Loss: 0.8770742416381836, Final Batch Loss: 0.30624920129776\n",
      "Epoch 4490, Loss: 0.820119708776474, Final Batch Loss: 0.22410327196121216\n",
      "Epoch 4491, Loss: 0.8169668614864349, Final Batch Loss: 0.33216017484664917\n",
      "Epoch 4492, Loss: 0.832886278629303, Final Batch Loss: 0.2595641314983368\n",
      "Epoch 4493, Loss: 0.8588018268346786, Final Batch Loss: 0.24479307234287262\n",
      "Epoch 4494, Loss: 0.6959919780492783, Final Batch Loss: 0.22567889094352722\n",
      "Epoch 4495, Loss: 0.681015744805336, Final Batch Loss: 0.20646093785762787\n",
      "Epoch 4496, Loss: 0.7756475657224655, Final Batch Loss: 0.21548055112361908\n",
      "Epoch 4497, Loss: 0.6406349241733551, Final Batch Loss: 0.19264638423919678\n",
      "Epoch 4498, Loss: 0.7721627354621887, Final Batch Loss: 0.19092008471488953\n",
      "Epoch 4499, Loss: 0.7919483184814453, Final Batch Loss: 0.249577134847641\n",
      "Epoch 4500, Loss: 0.7571929544210434, Final Batch Loss: 0.22011758387088776\n",
      "Epoch 4501, Loss: 0.7112078666687012, Final Batch Loss: 0.2191944271326065\n",
      "Epoch 4502, Loss: 0.6448228806257248, Final Batch Loss: 0.1879616528749466\n",
      "Epoch 4503, Loss: 0.7036833614110947, Final Batch Loss: 0.15847676992416382\n",
      "Epoch 4504, Loss: 0.7459391355514526, Final Batch Loss: 0.3149307072162628\n",
      "Epoch 4505, Loss: 0.7892230153083801, Final Batch Loss: 0.281331866979599\n",
      "Epoch 4506, Loss: 0.7756552249193192, Final Batch Loss: 0.22935719788074493\n",
      "Epoch 4507, Loss: 0.7245628833770752, Final Batch Loss: 0.20660912990570068\n",
      "Epoch 4508, Loss: 0.781934916973114, Final Batch Loss: 0.27728286385536194\n",
      "Epoch 4509, Loss: 0.7234416753053665, Final Batch Loss: 0.17413200438022614\n",
      "Epoch 4510, Loss: 0.723180890083313, Final Batch Loss: 0.23994185030460358\n",
      "Epoch 4511, Loss: 0.6839433312416077, Final Batch Loss: 0.20339997112751007\n",
      "Epoch 4512, Loss: 0.6916134506464005, Final Batch Loss: 0.24095316231250763\n",
      "Epoch 4513, Loss: 0.7555792480707169, Final Batch Loss: 0.26105397939682007\n",
      "Epoch 4514, Loss: 0.7926487624645233, Final Batch Loss: 0.2691086232662201\n",
      "Epoch 4515, Loss: 0.7977647632360458, Final Batch Loss: 0.29222002625465393\n",
      "Epoch 4516, Loss: 0.8508112877607346, Final Batch Loss: 0.3568915128707886\n",
      "Epoch 4517, Loss: 0.7372859120368958, Final Batch Loss: 0.26637959480285645\n",
      "Epoch 4518, Loss: 0.7279200553894043, Final Batch Loss: 0.18372699618339539\n",
      "Epoch 4519, Loss: 0.7329311668872833, Final Batch Loss: 0.20466184616088867\n",
      "Epoch 4520, Loss: 0.7921535670757294, Final Batch Loss: 0.27691328525543213\n",
      "Epoch 4521, Loss: 0.829290360212326, Final Batch Loss: 0.3610062897205353\n",
      "Epoch 4522, Loss: 0.7152334302663803, Final Batch Loss: 0.25968047976493835\n",
      "Epoch 4523, Loss: 0.7804384678602219, Final Batch Loss: 0.3041464686393738\n",
      "Epoch 4524, Loss: 0.7307476848363876, Final Batch Loss: 0.2347193956375122\n",
      "Epoch 4525, Loss: 0.7008752226829529, Final Batch Loss: 0.24294999241828918\n",
      "Epoch 4526, Loss: 0.7258149981498718, Final Batch Loss: 0.21302376687526703\n",
      "Epoch 4527, Loss: 0.76898393034935, Final Batch Loss: 0.3522913157939911\n",
      "Epoch 4528, Loss: 0.6999228149652481, Final Batch Loss: 0.19028609991073608\n",
      "Epoch 4529, Loss: 0.7769318372011185, Final Batch Loss: 0.34055382013320923\n",
      "Epoch 4530, Loss: 0.7645612061023712, Final Batch Loss: 0.2558501958847046\n",
      "Epoch 4531, Loss: 0.7815118432044983, Final Batch Loss: 0.2789601683616638\n",
      "Epoch 4532, Loss: 0.7774444073438644, Final Batch Loss: 0.24107573926448822\n",
      "Epoch 4533, Loss: 0.8595497161149979, Final Batch Loss: 0.3301459550857544\n",
      "Epoch 4534, Loss: 0.7370835691690445, Final Batch Loss: 0.2408123016357422\n",
      "Epoch 4535, Loss: 0.7528715431690216, Final Batch Loss: 0.2555898129940033\n",
      "Epoch 4536, Loss: 0.7622804790735245, Final Batch Loss: 0.19554631412029266\n",
      "Epoch 4537, Loss: 0.6749911159276962, Final Batch Loss: 0.14677537977695465\n",
      "Epoch 4538, Loss: 0.6970365047454834, Final Batch Loss: 0.2132766842842102\n",
      "Epoch 4539, Loss: 0.8366988599300385, Final Batch Loss: 0.2981396019458771\n",
      "Epoch 4540, Loss: 0.7013107091188431, Final Batch Loss: 0.2651608884334564\n",
      "Epoch 4541, Loss: 0.7574615180492401, Final Batch Loss: 0.25871774554252625\n",
      "Epoch 4542, Loss: 0.8717213571071625, Final Batch Loss: 0.28816351294517517\n",
      "Epoch 4543, Loss: 0.8054405152797699, Final Batch Loss: 0.20833122730255127\n",
      "Epoch 4544, Loss: 0.7317350059747696, Final Batch Loss: 0.23765593767166138\n",
      "Epoch 4545, Loss: 0.7644633948802948, Final Batch Loss: 0.19165173172950745\n",
      "Epoch 4546, Loss: 0.922950953245163, Final Batch Loss: 0.3279779553413391\n",
      "Epoch 4547, Loss: 0.8556978851556778, Final Batch Loss: 0.3089129626750946\n",
      "Epoch 4548, Loss: 0.707936093211174, Final Batch Loss: 0.2704281508922577\n",
      "Epoch 4549, Loss: 0.8027602881193161, Final Batch Loss: 0.3222985863685608\n",
      "Epoch 4550, Loss: 0.7994432896375656, Final Batch Loss: 0.27876922488212585\n",
      "Epoch 4551, Loss: 0.8273661285638809, Final Batch Loss: 0.28356149792671204\n",
      "Epoch 4552, Loss: 0.7375419437885284, Final Batch Loss: 0.22659943997859955\n",
      "Epoch 4553, Loss: 0.914366140961647, Final Batch Loss: 0.41450420022010803\n",
      "Epoch 4554, Loss: 0.8550734519958496, Final Batch Loss: 0.2615583539009094\n",
      "Epoch 4555, Loss: 0.7806148529052734, Final Batch Loss: 0.28285640478134155\n",
      "Epoch 4556, Loss: 0.7887716889381409, Final Batch Loss: 0.29929232597351074\n",
      "Epoch 4557, Loss: 0.7788902223110199, Final Batch Loss: 0.29493293166160583\n",
      "Epoch 4558, Loss: 0.8003740906715393, Final Batch Loss: 0.2961280941963196\n",
      "Epoch 4559, Loss: 0.7161020934581757, Final Batch Loss: 0.19783277809619904\n",
      "Epoch 4560, Loss: 0.7373871207237244, Final Batch Loss: 0.2122826874256134\n",
      "Epoch 4561, Loss: 0.7724772095680237, Final Batch Loss: 0.23182469606399536\n",
      "Epoch 4562, Loss: 0.7557272166013718, Final Batch Loss: 0.21216891705989838\n",
      "Epoch 4563, Loss: 0.8057208210229874, Final Batch Loss: 0.2044866532087326\n",
      "Epoch 4564, Loss: 0.8035115152597427, Final Batch Loss: 0.28998199105262756\n",
      "Epoch 4565, Loss: 0.8348066210746765, Final Batch Loss: 0.2787565290927887\n",
      "Epoch 4566, Loss: 0.8003137409687042, Final Batch Loss: 0.2578537166118622\n",
      "Epoch 4567, Loss: 0.7056248039007187, Final Batch Loss: 0.18910223245620728\n",
      "Epoch 4568, Loss: 0.7945152223110199, Final Batch Loss: 0.23535683751106262\n",
      "Epoch 4569, Loss: 0.8462600409984589, Final Batch Loss: 0.28696388006210327\n",
      "Epoch 4570, Loss: 0.7682184725999832, Final Batch Loss: 0.2779340445995331\n",
      "Epoch 4571, Loss: 0.8579561710357666, Final Batch Loss: 0.29515889286994934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4572, Loss: 0.7330328077077866, Final Batch Loss: 0.2365954965353012\n",
      "Epoch 4573, Loss: 0.7809027135372162, Final Batch Loss: 0.2967871129512787\n",
      "Epoch 4574, Loss: 0.776068925857544, Final Batch Loss: 0.19644354283809662\n",
      "Epoch 4575, Loss: 0.7861611396074295, Final Batch Loss: 0.2513754069805145\n",
      "Epoch 4576, Loss: 0.8574210107326508, Final Batch Loss: 0.3522956371307373\n",
      "Epoch 4577, Loss: 0.7273855954408646, Final Batch Loss: 0.2369120568037033\n",
      "Epoch 4578, Loss: 0.7382264882326126, Final Batch Loss: 0.23013606667518616\n",
      "Epoch 4579, Loss: 0.8238526582717896, Final Batch Loss: 0.30826815962791443\n",
      "Epoch 4580, Loss: 0.8190379589796066, Final Batch Loss: 0.3067404329776764\n",
      "Epoch 4581, Loss: 0.6988376080989838, Final Batch Loss: 0.176228329539299\n",
      "Epoch 4582, Loss: 0.7117886543273926, Final Batch Loss: 0.2638768255710602\n",
      "Epoch 4583, Loss: 0.7188486307859421, Final Batch Loss: 0.2486567199230194\n",
      "Epoch 4584, Loss: 0.7578462362289429, Final Batch Loss: 0.2626374661922455\n",
      "Epoch 4585, Loss: 0.6961464136838913, Final Batch Loss: 0.18280097842216492\n",
      "Epoch 4586, Loss: 0.6917875856161118, Final Batch Loss: 0.1511673629283905\n",
      "Epoch 4587, Loss: 0.741213783621788, Final Batch Loss: 0.23927545547485352\n",
      "Epoch 4588, Loss: 0.7552800476551056, Final Batch Loss: 0.20086811482906342\n",
      "Epoch 4589, Loss: 0.7701474130153656, Final Batch Loss: 0.20117446780204773\n",
      "Epoch 4590, Loss: 0.8122257143259048, Final Batch Loss: 0.2945042848587036\n",
      "Epoch 4591, Loss: 0.6972680538892746, Final Batch Loss: 0.200157031416893\n",
      "Epoch 4592, Loss: 0.7739809304475784, Final Batch Loss: 0.22747181355953217\n",
      "Epoch 4593, Loss: 0.7345889508724213, Final Batch Loss: 0.2641541361808777\n",
      "Epoch 4594, Loss: 0.7329180389642715, Final Batch Loss: 0.2256843000650406\n",
      "Epoch 4595, Loss: 0.8475656807422638, Final Batch Loss: 0.337072491645813\n",
      "Epoch 4596, Loss: 0.7402243465185165, Final Batch Loss: 0.21341218054294586\n",
      "Epoch 4597, Loss: 0.646331712603569, Final Batch Loss: 0.1903240978717804\n",
      "Epoch 4598, Loss: 0.7917490303516388, Final Batch Loss: 0.25123873353004456\n",
      "Epoch 4599, Loss: 0.800384908914566, Final Batch Loss: 0.2923371493816376\n",
      "Epoch 4600, Loss: 0.7814920842647552, Final Batch Loss: 0.27453628182411194\n",
      "Epoch 4601, Loss: 0.7404588609933853, Final Batch Loss: 0.31031885743141174\n",
      "Epoch 4602, Loss: 0.7267536818981171, Final Batch Loss: 0.215292289853096\n",
      "Epoch 4603, Loss: 0.7791863083839417, Final Batch Loss: 0.28220134973526\n",
      "Epoch 4604, Loss: 0.8019483387470245, Final Batch Loss: 0.2705391049385071\n",
      "Epoch 4605, Loss: 0.8273964822292328, Final Batch Loss: 0.36246469616889954\n",
      "Epoch 4606, Loss: 0.6818493455648422, Final Batch Loss: 0.1893116533756256\n",
      "Epoch 4607, Loss: 0.7776257991790771, Final Batch Loss: 0.2802819609642029\n",
      "Epoch 4608, Loss: 0.7591147571802139, Final Batch Loss: 0.27641308307647705\n",
      "Epoch 4609, Loss: 0.764918178319931, Final Batch Loss: 0.3054041564464569\n",
      "Epoch 4610, Loss: 0.6903278976678848, Final Batch Loss: 0.21805638074874878\n",
      "Epoch 4611, Loss: 0.7607296705245972, Final Batch Loss: 0.2788483798503876\n",
      "Epoch 4612, Loss: 0.7306408882141113, Final Batch Loss: 0.23251549899578094\n",
      "Epoch 4613, Loss: 0.7534668743610382, Final Batch Loss: 0.23920929431915283\n",
      "Epoch 4614, Loss: 0.778878927230835, Final Batch Loss: 0.2663837671279907\n",
      "Epoch 4615, Loss: 0.7656497806310654, Final Batch Loss: 0.20428504049777985\n",
      "Epoch 4616, Loss: 0.7725155800580978, Final Batch Loss: 0.2209131270647049\n",
      "Epoch 4617, Loss: 0.745621532201767, Final Batch Loss: 0.2865079939365387\n",
      "Epoch 4618, Loss: 0.7892326563596725, Final Batch Loss: 0.19682182371616364\n",
      "Epoch 4619, Loss: 0.7619574517011642, Final Batch Loss: 0.2817464768886566\n",
      "Epoch 4620, Loss: 0.7511090338230133, Final Batch Loss: 0.26615214347839355\n",
      "Epoch 4621, Loss: 0.8075315058231354, Final Batch Loss: 0.23439985513687134\n",
      "Epoch 4622, Loss: 0.7671152055263519, Final Batch Loss: 0.22311803698539734\n",
      "Epoch 4623, Loss: 0.9275332391262054, Final Batch Loss: 0.3863268792629242\n",
      "Epoch 4624, Loss: 0.7616613209247589, Final Batch Loss: 0.25049763917922974\n",
      "Epoch 4625, Loss: 0.8321339786052704, Final Batch Loss: 0.2947911322116852\n",
      "Epoch 4626, Loss: 0.798524022102356, Final Batch Loss: 0.34190428256988525\n",
      "Epoch 4627, Loss: 0.8654917478561401, Final Batch Loss: 0.30627694725990295\n",
      "Epoch 4628, Loss: 0.7599324434995651, Final Batch Loss: 0.24285341799259186\n",
      "Epoch 4629, Loss: 0.9024171829223633, Final Batch Loss: 0.26339346170425415\n",
      "Epoch 4630, Loss: 0.6792199164628983, Final Batch Loss: 0.17714163661003113\n",
      "Epoch 4631, Loss: 0.7340991199016571, Final Batch Loss: 0.26848047971725464\n",
      "Epoch 4632, Loss: 0.845467746257782, Final Batch Loss: 0.31260359287261963\n",
      "Epoch 4633, Loss: 0.8032354116439819, Final Batch Loss: 0.2588336169719696\n",
      "Epoch 4634, Loss: 0.801792562007904, Final Batch Loss: 0.28872424364089966\n",
      "Epoch 4635, Loss: 0.7592244297266006, Final Batch Loss: 0.28588804602622986\n",
      "Epoch 4636, Loss: 0.8240513652563095, Final Batch Loss: 0.30434852838516235\n",
      "Epoch 4637, Loss: 0.8278650939464569, Final Batch Loss: 0.23889227211475372\n",
      "Epoch 4638, Loss: 0.7484238594770432, Final Batch Loss: 0.2223844677209854\n",
      "Epoch 4639, Loss: 0.8554790616035461, Final Batch Loss: 0.2888486385345459\n",
      "Epoch 4640, Loss: 0.7488818019628525, Final Batch Loss: 0.17291517555713654\n",
      "Epoch 4641, Loss: 0.680390328168869, Final Batch Loss: 0.15612782537937164\n",
      "Epoch 4642, Loss: 0.782323807477951, Final Batch Loss: 0.35280659794807434\n",
      "Epoch 4643, Loss: 0.762191891670227, Final Batch Loss: 0.2989850640296936\n",
      "Epoch 4644, Loss: 0.7883510440587997, Final Batch Loss: 0.25214317440986633\n",
      "Epoch 4645, Loss: 0.6548226922750473, Final Batch Loss: 0.23045144975185394\n",
      "Epoch 4646, Loss: 0.7019413262605667, Final Batch Loss: 0.1884513646364212\n",
      "Epoch 4647, Loss: 0.7031416445970535, Final Batch Loss: 0.2101643979549408\n",
      "Epoch 4648, Loss: 0.6936342120170593, Final Batch Loss: 0.24726049602031708\n",
      "Epoch 4649, Loss: 0.7170909345149994, Final Batch Loss: 0.23710986971855164\n",
      "Epoch 4650, Loss: 0.7461797595024109, Final Batch Loss: 0.3094084560871124\n",
      "Epoch 4651, Loss: 0.8398773670196533, Final Batch Loss: 0.26849883794784546\n",
      "Epoch 4652, Loss: 0.7578010261058807, Final Batch Loss: 0.25129950046539307\n",
      "Epoch 4653, Loss: 0.6881589591503143, Final Batch Loss: 0.22863663733005524\n",
      "Epoch 4654, Loss: 0.7215573340654373, Final Batch Loss: 0.24297672510147095\n",
      "Epoch 4655, Loss: 0.9455530792474747, Final Batch Loss: 0.38556745648384094\n",
      "Epoch 4656, Loss: 0.7604303061962128, Final Batch Loss: 0.23845763504505157\n",
      "Epoch 4657, Loss: 0.8121405988931656, Final Batch Loss: 0.28713613748550415\n",
      "Epoch 4658, Loss: 0.9011509120464325, Final Batch Loss: 0.36152929067611694\n",
      "Epoch 4659, Loss: 0.7766465097665787, Final Batch Loss: 0.22602935135364532\n",
      "Epoch 4660, Loss: 0.7434937208890915, Final Batch Loss: 0.3038846552371979\n",
      "Epoch 4661, Loss: 0.8347605615854263, Final Batch Loss: 0.2821629047393799\n",
      "Epoch 4662, Loss: 0.7982455492019653, Final Batch Loss: 0.23590752482414246\n",
      "Epoch 4663, Loss: 0.7310421764850616, Final Batch Loss: 0.23628289997577667\n",
      "Epoch 4664, Loss: 0.7942879498004913, Final Batch Loss: 0.24998798966407776\n",
      "Epoch 4665, Loss: 0.8213478624820709, Final Batch Loss: 0.30078059434890747\n",
      "Epoch 4666, Loss: 0.8950510323047638, Final Batch Loss: 0.35884538292884827\n",
      "Epoch 4667, Loss: 0.6849903911352158, Final Batch Loss: 0.24371682107448578\n",
      "Epoch 4668, Loss: 0.8998795747756958, Final Batch Loss: 0.29706940054893494\n",
      "Epoch 4669, Loss: 0.6803045719861984, Final Batch Loss: 0.21188171207904816\n",
      "Epoch 4670, Loss: 0.7493883371353149, Final Batch Loss: 0.2891376316547394\n",
      "Epoch 4671, Loss: 0.7351880818605423, Final Batch Loss: 0.21932938694953918\n",
      "Epoch 4672, Loss: 0.7197008430957794, Final Batch Loss: 0.2733881175518036\n",
      "Epoch 4673, Loss: 0.8334680944681168, Final Batch Loss: 0.2468177229166031\n",
      "Epoch 4674, Loss: 0.699218288064003, Final Batch Loss: 0.19258950650691986\n",
      "Epoch 4675, Loss: 0.7633157223463058, Final Batch Loss: 0.30760589241981506\n",
      "Epoch 4676, Loss: 0.8428716212511063, Final Batch Loss: 0.33508723974227905\n",
      "Epoch 4677, Loss: 0.7540612667798996, Final Batch Loss: 0.25446057319641113\n",
      "Epoch 4678, Loss: 0.7205718457698822, Final Batch Loss: 0.17545852065086365\n",
      "Epoch 4679, Loss: 0.7274927198886871, Final Batch Loss: 0.26488253474235535\n",
      "Epoch 4680, Loss: 0.7429608404636383, Final Batch Loss: 0.27452364563941956\n",
      "Epoch 4681, Loss: 0.8023153394460678, Final Batch Loss: 0.33338940143585205\n",
      "Epoch 4682, Loss: 0.7563526928424835, Final Batch Loss: 0.22250565886497498\n",
      "Epoch 4683, Loss: 0.7381872236728668, Final Batch Loss: 0.21050703525543213\n",
      "Epoch 4684, Loss: 0.811210423707962, Final Batch Loss: 0.23699095845222473\n",
      "Epoch 4685, Loss: 0.7701700031757355, Final Batch Loss: 0.18556135892868042\n",
      "Epoch 4686, Loss: 0.7140561938285828, Final Batch Loss: 0.2631019055843353\n",
      "Epoch 4687, Loss: 0.7160990089178085, Final Batch Loss: 0.24929256737232208\n",
      "Epoch 4688, Loss: 0.7091509848833084, Final Batch Loss: 0.27934548258781433\n",
      "Epoch 4689, Loss: 0.8039647936820984, Final Batch Loss: 0.2649233341217041\n",
      "Epoch 4690, Loss: 0.7119987607002258, Final Batch Loss: 0.223125159740448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4691, Loss: 0.8444871306419373, Final Batch Loss: 0.21690627932548523\n",
      "Epoch 4692, Loss: 0.7656484395265579, Final Batch Loss: 0.29711419343948364\n",
      "Epoch 4693, Loss: 0.8055001050233841, Final Batch Loss: 0.2700493633747101\n",
      "Epoch 4694, Loss: 0.735363095998764, Final Batch Loss: 0.22505082190036774\n",
      "Epoch 4695, Loss: 0.8185482025146484, Final Batch Loss: 0.263126403093338\n",
      "Epoch 4696, Loss: 0.6819024682044983, Final Batch Loss: 0.2488558143377304\n",
      "Epoch 4697, Loss: 0.6984629184007645, Final Batch Loss: 0.24421273171901703\n",
      "Epoch 4698, Loss: 0.8375874310731888, Final Batch Loss: 0.24571995437145233\n",
      "Epoch 4699, Loss: 0.7607681751251221, Final Batch Loss: 0.2773018479347229\n",
      "Epoch 4700, Loss: 0.7672654241323471, Final Batch Loss: 0.26309528946876526\n",
      "Epoch 4701, Loss: 0.7139180302619934, Final Batch Loss: 0.23584917187690735\n",
      "Epoch 4702, Loss: 0.6609308123588562, Final Batch Loss: 0.23575164377689362\n",
      "Epoch 4703, Loss: 0.7687216103076935, Final Batch Loss: 0.2158411145210266\n",
      "Epoch 4704, Loss: 0.7622942179441452, Final Batch Loss: 0.21790668368339539\n",
      "Epoch 4705, Loss: 0.7805304974317551, Final Batch Loss: 0.29185330867767334\n",
      "Epoch 4706, Loss: 0.7714986056089401, Final Batch Loss: 0.304537832736969\n",
      "Epoch 4707, Loss: 0.6513000130653381, Final Batch Loss: 0.20494863390922546\n",
      "Epoch 4708, Loss: 0.7737698256969452, Final Batch Loss: 0.23968324065208435\n",
      "Epoch 4709, Loss: 0.8279633074998856, Final Batch Loss: 0.26094546914100647\n",
      "Epoch 4710, Loss: 0.7932194322347641, Final Batch Loss: 0.3824094831943512\n",
      "Epoch 4711, Loss: 0.9455922245979309, Final Batch Loss: 0.48401379585266113\n",
      "Epoch 4712, Loss: 0.8115311563014984, Final Batch Loss: 0.27118727564811707\n",
      "Epoch 4713, Loss: 0.8088345974683762, Final Batch Loss: 0.28075599670410156\n",
      "Epoch 4714, Loss: 0.8042569607496262, Final Batch Loss: 0.28479519486427307\n",
      "Epoch 4715, Loss: 0.8590051829814911, Final Batch Loss: 0.2841184437274933\n",
      "Epoch 4716, Loss: 0.7849555909633636, Final Batch Loss: 0.3066818118095398\n",
      "Epoch 4717, Loss: 0.6666385680437088, Final Batch Loss: 0.21751007437705994\n",
      "Epoch 4718, Loss: 0.8007580637931824, Final Batch Loss: 0.2656179964542389\n",
      "Epoch 4719, Loss: 0.7758408784866333, Final Batch Loss: 0.27213138341903687\n",
      "Epoch 4720, Loss: 0.7457357496023178, Final Batch Loss: 0.20940649509429932\n",
      "Epoch 4721, Loss: 0.6874781101942062, Final Batch Loss: 0.1800963431596756\n",
      "Epoch 4722, Loss: 0.7314224094152451, Final Batch Loss: 0.21041692793369293\n",
      "Epoch 4723, Loss: 0.7751959264278412, Final Batch Loss: 0.3048391342163086\n",
      "Epoch 4724, Loss: 0.8624072670936584, Final Batch Loss: 0.27631494402885437\n",
      "Epoch 4725, Loss: 0.8637654930353165, Final Batch Loss: 0.28685063123703003\n",
      "Epoch 4726, Loss: 0.7450765669345856, Final Batch Loss: 0.2511002719402313\n",
      "Epoch 4727, Loss: 0.7618965953588486, Final Batch Loss: 0.23979759216308594\n",
      "Epoch 4728, Loss: 0.7808821052312851, Final Batch Loss: 0.20977921783924103\n",
      "Epoch 4729, Loss: 0.8145987689495087, Final Batch Loss: 0.2660737931728363\n",
      "Epoch 4730, Loss: 0.716468334197998, Final Batch Loss: 0.2317870706319809\n",
      "Epoch 4731, Loss: 0.7722395807504654, Final Batch Loss: 0.20480220019817352\n",
      "Epoch 4732, Loss: 0.7786445915699005, Final Batch Loss: 0.2375987470149994\n",
      "Epoch 4733, Loss: 0.8378021568059921, Final Batch Loss: 0.3163791000843048\n",
      "Epoch 4734, Loss: 0.7917086035013199, Final Batch Loss: 0.2226829081773758\n",
      "Epoch 4735, Loss: 0.6704611331224442, Final Batch Loss: 0.17388154566287994\n",
      "Epoch 4736, Loss: 0.7552655190229416, Final Batch Loss: 0.34299203753471375\n",
      "Epoch 4737, Loss: 0.7743269503116608, Final Batch Loss: 0.256127268075943\n",
      "Epoch 4738, Loss: 0.8501781523227692, Final Batch Loss: 0.28402307629585266\n",
      "Epoch 4739, Loss: 0.7580796778202057, Final Batch Loss: 0.2593366801738739\n",
      "Epoch 4740, Loss: 0.6837834417819977, Final Batch Loss: 0.1649821251630783\n",
      "Epoch 4741, Loss: 0.7131979018449783, Final Batch Loss: 0.2889873683452606\n",
      "Epoch 4742, Loss: 0.6564988046884537, Final Batch Loss: 0.23463484644889832\n",
      "Epoch 4743, Loss: 0.7027226835489273, Final Batch Loss: 0.23521967232227325\n",
      "Epoch 4744, Loss: 0.7470894306898117, Final Batch Loss: 0.20974934101104736\n",
      "Epoch 4745, Loss: 0.7023406326770782, Final Batch Loss: 0.18859121203422546\n",
      "Epoch 4746, Loss: 0.7243903875350952, Final Batch Loss: 0.2401738464832306\n",
      "Epoch 4747, Loss: 0.7948629707098007, Final Batch Loss: 0.2532646656036377\n",
      "Epoch 4748, Loss: 0.7425331771373749, Final Batch Loss: 0.2639860212802887\n",
      "Epoch 4749, Loss: 0.6703490167856216, Final Batch Loss: 0.16967692971229553\n",
      "Epoch 4750, Loss: 0.8123968094587326, Final Batch Loss: 0.3025180697441101\n",
      "Epoch 4751, Loss: 0.7517518252134323, Final Batch Loss: 0.25663790106773376\n",
      "Epoch 4752, Loss: 0.691635251045227, Final Batch Loss: 0.2147684097290039\n",
      "Epoch 4753, Loss: 0.8637421727180481, Final Batch Loss: 0.40399956703186035\n",
      "Epoch 4754, Loss: 0.7616475373506546, Final Batch Loss: 0.17350266873836517\n",
      "Epoch 4755, Loss: 0.7095965147018433, Final Batch Loss: 0.2637946605682373\n",
      "Epoch 4756, Loss: 0.6991576850414276, Final Batch Loss: 0.2349601686000824\n",
      "Epoch 4757, Loss: 1.0364111363887787, Final Batch Loss: 0.4358538091182709\n",
      "Epoch 4758, Loss: 0.6611606627702713, Final Batch Loss: 0.14685657620429993\n",
      "Epoch 4759, Loss: 0.7156337350606918, Final Batch Loss: 0.23493029177188873\n",
      "Epoch 4760, Loss: 0.7867927849292755, Final Batch Loss: 0.30616867542266846\n",
      "Epoch 4761, Loss: 0.7126244455575943, Final Batch Loss: 0.22350846230983734\n",
      "Epoch 4762, Loss: 0.7451881766319275, Final Batch Loss: 0.23923739790916443\n",
      "Epoch 4763, Loss: 0.8103333413600922, Final Batch Loss: 0.3001982271671295\n",
      "Epoch 4764, Loss: 0.8601777106523514, Final Batch Loss: 0.26446592807769775\n",
      "Epoch 4765, Loss: 0.7377408444881439, Final Batch Loss: 0.1860489845275879\n",
      "Epoch 4766, Loss: 0.7811630517244339, Final Batch Loss: 0.29143238067626953\n",
      "Epoch 4767, Loss: 0.7666010409593582, Final Batch Loss: 0.2209388166666031\n",
      "Epoch 4768, Loss: 0.8431813269853592, Final Batch Loss: 0.2792503237724304\n",
      "Epoch 4769, Loss: 0.7340803295373917, Final Batch Loss: 0.16323141753673553\n",
      "Epoch 4770, Loss: 0.7492506057024002, Final Batch Loss: 0.29174908995628357\n",
      "Epoch 4771, Loss: 0.8646029829978943, Final Batch Loss: 0.2943144738674164\n",
      "Epoch 4772, Loss: 0.7443929016590118, Final Batch Loss: 0.2147042602300644\n",
      "Epoch 4773, Loss: 0.8011031299829483, Final Batch Loss: 0.3093216121196747\n",
      "Epoch 4774, Loss: 0.7492519170045853, Final Batch Loss: 0.1957779824733734\n",
      "Epoch 4775, Loss: 0.7434578090906143, Final Batch Loss: 0.21253463625907898\n",
      "Epoch 4776, Loss: 0.8394432365894318, Final Batch Loss: 0.1912960410118103\n",
      "Epoch 4777, Loss: 0.7809930145740509, Final Batch Loss: 0.30139636993408203\n",
      "Epoch 4778, Loss: 0.727106973528862, Final Batch Loss: 0.20570483803749084\n",
      "Epoch 4779, Loss: 0.7790644615888596, Final Batch Loss: 0.2348702847957611\n",
      "Epoch 4780, Loss: 0.6569030433893204, Final Batch Loss: 0.260748028755188\n",
      "Epoch 4781, Loss: 0.784316748380661, Final Batch Loss: 0.2900911867618561\n",
      "Epoch 4782, Loss: 0.8029458671808243, Final Batch Loss: 0.27988046407699585\n",
      "Epoch 4783, Loss: 0.7393958568572998, Final Batch Loss: 0.23442019522190094\n",
      "Epoch 4784, Loss: 0.7999373972415924, Final Batch Loss: 0.2873680889606476\n",
      "Epoch 4785, Loss: 0.8092833906412125, Final Batch Loss: 0.3285718560218811\n",
      "Epoch 4786, Loss: 0.7432956248521805, Final Batch Loss: 0.19602762162685394\n",
      "Epoch 4787, Loss: 0.7032918632030487, Final Batch Loss: 0.23069311678409576\n",
      "Epoch 4788, Loss: 0.7429787963628769, Final Batch Loss: 0.2720244228839874\n",
      "Epoch 4789, Loss: 0.7355614304542542, Final Batch Loss: 0.2809387147426605\n",
      "Epoch 4790, Loss: 0.6947039216756821, Final Batch Loss: 0.21074031293392181\n",
      "Epoch 4791, Loss: 0.7848336845636368, Final Batch Loss: 0.25830721855163574\n",
      "Epoch 4792, Loss: 0.6729036122560501, Final Batch Loss: 0.20182764530181885\n",
      "Epoch 4793, Loss: 0.849476158618927, Final Batch Loss: 0.3788585066795349\n",
      "Epoch 4794, Loss: 0.7249847948551178, Final Batch Loss: 0.22662660479545593\n",
      "Epoch 4795, Loss: 0.8190313577651978, Final Batch Loss: 0.33372601866722107\n",
      "Epoch 4796, Loss: 0.8294967710971832, Final Batch Loss: 0.296067476272583\n",
      "Epoch 4797, Loss: 0.842250794172287, Final Batch Loss: 0.25877103209495544\n",
      "Epoch 4798, Loss: 0.6838037073612213, Final Batch Loss: 0.21546350419521332\n",
      "Epoch 4799, Loss: 0.7720950245857239, Final Batch Loss: 0.2643195390701294\n",
      "Epoch 4800, Loss: 0.7849776446819305, Final Batch Loss: 0.2654094696044922\n",
      "Epoch 4801, Loss: 0.7197423875331879, Final Batch Loss: 0.2823999226093292\n",
      "Epoch 4802, Loss: 0.7146098464727402, Final Batch Loss: 0.20386648178100586\n",
      "Epoch 4803, Loss: 0.6494686156511307, Final Batch Loss: 0.19988057017326355\n",
      "Epoch 4804, Loss: 0.7447614520788193, Final Batch Loss: 0.20758965611457825\n",
      "Epoch 4805, Loss: 0.8456705063581467, Final Batch Loss: 0.23242397606372833\n",
      "Epoch 4806, Loss: 0.7819733619689941, Final Batch Loss: 0.2595958709716797\n",
      "Epoch 4807, Loss: 0.710738405585289, Final Batch Loss: 0.2632724940776825\n",
      "Epoch 4808, Loss: 0.7080304622650146, Final Batch Loss: 0.20727446675300598\n",
      "Epoch 4809, Loss: 0.6875667870044708, Final Batch Loss: 0.23332713544368744\n",
      "Epoch 4810, Loss: 0.71629998087883, Final Batch Loss: 0.21232688426971436\n",
      "Epoch 4811, Loss: 0.830936536192894, Final Batch Loss: 0.2579689621925354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4812, Loss: 0.7085794806480408, Final Batch Loss: 0.21913111209869385\n",
      "Epoch 4813, Loss: 0.8365488350391388, Final Batch Loss: 0.35914885997772217\n",
      "Epoch 4814, Loss: 0.7590373158454895, Final Batch Loss: 0.2718757688999176\n",
      "Epoch 4815, Loss: 0.7911156862974167, Final Batch Loss: 0.2911188006401062\n",
      "Epoch 4816, Loss: 0.8550262302160263, Final Batch Loss: 0.23256663978099823\n",
      "Epoch 4817, Loss: 0.830237939953804, Final Batch Loss: 0.35051438212394714\n",
      "Epoch 4818, Loss: 0.8184681236743927, Final Batch Loss: 0.24284303188323975\n",
      "Epoch 4819, Loss: 0.7103334069252014, Final Batch Loss: 0.1658416986465454\n",
      "Epoch 4820, Loss: 0.8195820599794388, Final Batch Loss: 0.3465299904346466\n",
      "Epoch 4821, Loss: 0.7308111488819122, Final Batch Loss: 0.2802888751029968\n",
      "Epoch 4822, Loss: 0.6844441741704941, Final Batch Loss: 0.23080819845199585\n",
      "Epoch 4823, Loss: 0.7576818615198135, Final Batch Loss: 0.24756354093551636\n",
      "Epoch 4824, Loss: 0.7255326807498932, Final Batch Loss: 0.24702109396457672\n",
      "Epoch 4825, Loss: 0.7362133264541626, Final Batch Loss: 0.22846619784832\n",
      "Epoch 4826, Loss: 0.7647602558135986, Final Batch Loss: 0.18737153708934784\n",
      "Epoch 4827, Loss: 0.6775354146957397, Final Batch Loss: 0.2160635143518448\n",
      "Epoch 4828, Loss: 0.716903880238533, Final Batch Loss: 0.21284498274326324\n",
      "Epoch 4829, Loss: 0.7471137493848801, Final Batch Loss: 0.20679312944412231\n",
      "Epoch 4830, Loss: 0.7478341460227966, Final Batch Loss: 0.17969448864459991\n",
      "Epoch 4831, Loss: 0.6241423040628433, Final Batch Loss: 0.1853037178516388\n",
      "Epoch 4832, Loss: 0.8261602520942688, Final Batch Loss: 0.3173246681690216\n",
      "Epoch 4833, Loss: 0.7245886474847794, Final Batch Loss: 0.2706100642681122\n",
      "Epoch 4834, Loss: 0.8569658696651459, Final Batch Loss: 0.2644278407096863\n",
      "Epoch 4835, Loss: 0.7813864201307297, Final Batch Loss: 0.3188365399837494\n",
      "Epoch 4836, Loss: 0.7604886442422867, Final Batch Loss: 0.2773895561695099\n",
      "Epoch 4837, Loss: 0.8046198189258575, Final Batch Loss: 0.28492337465286255\n",
      "Epoch 4838, Loss: 0.7364775389432907, Final Batch Loss: 0.2466578185558319\n",
      "Epoch 4839, Loss: 0.8590051978826523, Final Batch Loss: 0.3453386127948761\n",
      "Epoch 4840, Loss: 0.7721618562936783, Final Batch Loss: 0.22536863386631012\n",
      "Epoch 4841, Loss: 0.7817644625902176, Final Batch Loss: 0.29045626521110535\n",
      "Epoch 4842, Loss: 0.8799255937337875, Final Batch Loss: 0.31412041187286377\n",
      "Epoch 4843, Loss: 0.7770229578018188, Final Batch Loss: 0.22498464584350586\n",
      "Epoch 4844, Loss: 0.8465806394815445, Final Batch Loss: 0.34738945960998535\n",
      "Epoch 4845, Loss: 0.7210176587104797, Final Batch Loss: 0.1854640245437622\n",
      "Epoch 4846, Loss: 0.6223088130354881, Final Batch Loss: 0.12055859714746475\n",
      "Epoch 4847, Loss: 0.6252249777317047, Final Batch Loss: 0.15939271450042725\n",
      "Epoch 4848, Loss: 0.6688237637281418, Final Batch Loss: 0.1448793262243271\n",
      "Epoch 4849, Loss: 0.7215738445520401, Final Batch Loss: 0.24636059999465942\n",
      "Epoch 4850, Loss: 0.7828133255243301, Final Batch Loss: 0.26532310247421265\n",
      "Epoch 4851, Loss: 0.7966363728046417, Final Batch Loss: 0.29062530398368835\n",
      "Epoch 4852, Loss: 0.7153228223323822, Final Batch Loss: 0.23488663136959076\n",
      "Epoch 4853, Loss: 0.8805668652057648, Final Batch Loss: 0.3768107295036316\n",
      "Epoch 4854, Loss: 0.792212724685669, Final Batch Loss: 0.2748524844646454\n",
      "Epoch 4855, Loss: 0.7792090773582458, Final Batch Loss: 0.1946932077407837\n",
      "Epoch 4856, Loss: 0.7101069092750549, Final Batch Loss: 0.15878108143806458\n",
      "Epoch 4857, Loss: 0.7882238030433655, Final Batch Loss: 0.24780850112438202\n",
      "Epoch 4858, Loss: 0.781774491071701, Final Batch Loss: 0.2749522030353546\n",
      "Epoch 4859, Loss: 0.716551199555397, Final Batch Loss: 0.2632436454296112\n",
      "Epoch 4860, Loss: 0.7720014154911041, Final Batch Loss: 0.22253495454788208\n",
      "Epoch 4861, Loss: 0.76940056681633, Final Batch Loss: 0.2863694727420807\n",
      "Epoch 4862, Loss: 0.7291644364595413, Final Batch Loss: 0.27820879220962524\n",
      "Epoch 4863, Loss: 0.7507183998823166, Final Batch Loss: 0.2291044443845749\n",
      "Epoch 4864, Loss: 0.7603778839111328, Final Batch Loss: 0.25262200832366943\n",
      "Epoch 4865, Loss: 0.7984275370836258, Final Batch Loss: 0.29340487718582153\n",
      "Epoch 4866, Loss: 0.7947293967008591, Final Batch Loss: 0.3214608132839203\n",
      "Epoch 4867, Loss: 0.7127586603164673, Final Batch Loss: 0.22877803444862366\n",
      "Epoch 4868, Loss: 0.8939871191978455, Final Batch Loss: 0.43143925070762634\n",
      "Epoch 4869, Loss: 0.7225112020969391, Final Batch Loss: 0.2244437336921692\n",
      "Epoch 4870, Loss: 0.7006050050258636, Final Batch Loss: 0.24279899895191193\n",
      "Epoch 4871, Loss: 0.955818846821785, Final Batch Loss: 0.1996087282896042\n",
      "Epoch 4872, Loss: 0.6156077235937119, Final Batch Loss: 0.18484582006931305\n",
      "Epoch 4873, Loss: 0.7614448517560959, Final Batch Loss: 0.27808305621147156\n",
      "Epoch 4874, Loss: 0.7934059649705887, Final Batch Loss: 0.30731046199798584\n",
      "Epoch 4875, Loss: 0.6908624768257141, Final Batch Loss: 0.23344114422798157\n",
      "Epoch 4876, Loss: 0.9332712888717651, Final Batch Loss: 0.316663920879364\n",
      "Epoch 4877, Loss: 0.7242753207683563, Final Batch Loss: 0.2449059933423996\n",
      "Epoch 4878, Loss: 0.7320057153701782, Final Batch Loss: 0.1782507598400116\n",
      "Epoch 4879, Loss: 0.7441432625055313, Final Batch Loss: 0.26889893412590027\n",
      "Epoch 4880, Loss: 0.7795872539281845, Final Batch Loss: 0.2936800718307495\n",
      "Epoch 4881, Loss: 0.6620459407567978, Final Batch Loss: 0.16424931585788727\n",
      "Epoch 4882, Loss: 0.83428855240345, Final Batch Loss: 0.3545578420162201\n",
      "Epoch 4883, Loss: 0.7797982692718506, Final Batch Loss: 0.22879008948802948\n",
      "Epoch 4884, Loss: 0.8156192600727081, Final Batch Loss: 0.23838940262794495\n",
      "Epoch 4885, Loss: 0.7681241929531097, Final Batch Loss: 0.25832176208496094\n",
      "Epoch 4886, Loss: 0.786655604839325, Final Batch Loss: 0.2129541039466858\n",
      "Epoch 4887, Loss: 0.7103106528520584, Final Batch Loss: 0.24093545973300934\n",
      "Epoch 4888, Loss: 0.7748761028051376, Final Batch Loss: 0.28645452857017517\n",
      "Epoch 4889, Loss: 0.7467992305755615, Final Batch Loss: 0.23158031702041626\n",
      "Epoch 4890, Loss: 0.7410221844911575, Final Batch Loss: 0.27102336287498474\n",
      "Epoch 4891, Loss: 0.6629074960947037, Final Batch Loss: 0.2011154443025589\n",
      "Epoch 4892, Loss: 0.7749142944812775, Final Batch Loss: 0.26671862602233887\n",
      "Epoch 4893, Loss: 0.654204323887825, Final Batch Loss: 0.20028109848499298\n",
      "Epoch 4894, Loss: 0.7503846138715744, Final Batch Loss: 0.23171697556972504\n",
      "Epoch 4895, Loss: 0.7668474167585373, Final Batch Loss: 0.29481351375579834\n",
      "Epoch 4896, Loss: 0.7291311472654343, Final Batch Loss: 0.2777040898799896\n",
      "Epoch 4897, Loss: 0.7998470813035965, Final Batch Loss: 0.2432093769311905\n",
      "Epoch 4898, Loss: 0.798641711473465, Final Batch Loss: 0.3310171663761139\n",
      "Epoch 4899, Loss: 0.7064420878887177, Final Batch Loss: 0.19790874421596527\n",
      "Epoch 4900, Loss: 0.8570339679718018, Final Batch Loss: 0.23001280426979065\n",
      "Epoch 4901, Loss: 0.8286781162023544, Final Batch Loss: 0.2424817532300949\n",
      "Epoch 4902, Loss: 0.7878997176885605, Final Batch Loss: 0.24437515437602997\n",
      "Epoch 4903, Loss: 0.800421729683876, Final Batch Loss: 0.2942967712879181\n",
      "Epoch 4904, Loss: 0.7303821444511414, Final Batch Loss: 0.2316809594631195\n",
      "Epoch 4905, Loss: 0.8029355704784393, Final Batch Loss: 0.26416242122650146\n",
      "Epoch 4906, Loss: 0.8519923239946365, Final Batch Loss: 0.3140333592891693\n",
      "Epoch 4907, Loss: 0.7219976633787155, Final Batch Loss: 0.26801666617393494\n",
      "Epoch 4908, Loss: 0.8444285541772842, Final Batch Loss: 0.33242908120155334\n",
      "Epoch 4909, Loss: 0.7231611758470535, Final Batch Loss: 0.22955277562141418\n",
      "Epoch 4910, Loss: 0.7322126924991608, Final Batch Loss: 0.25315988063812256\n",
      "Epoch 4911, Loss: 0.6851026117801666, Final Batch Loss: 0.2310335785150528\n",
      "Epoch 4912, Loss: 0.6487938761711121, Final Batch Loss: 0.13097313046455383\n",
      "Epoch 4913, Loss: 0.7734638452529907, Final Batch Loss: 0.28691592812538147\n",
      "Epoch 4914, Loss: 0.7347055673599243, Final Batch Loss: 0.25261190533638\n",
      "Epoch 4915, Loss: 0.7495447546243668, Final Batch Loss: 0.2572818398475647\n",
      "Epoch 4916, Loss: 0.7573194205760956, Final Batch Loss: 0.20864680409431458\n",
      "Epoch 4917, Loss: 0.8325971513986588, Final Batch Loss: 0.24933163821697235\n",
      "Epoch 4918, Loss: 0.7739438563585281, Final Batch Loss: 0.27730846405029297\n",
      "Epoch 4919, Loss: 0.7741214036941528, Final Batch Loss: 0.3070089817047119\n",
      "Epoch 4920, Loss: 0.8306501805782318, Final Batch Loss: 0.36253637075424194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4921, Loss: 0.7148543149232864, Final Batch Loss: 0.2487582117319107\n",
      "Epoch 4922, Loss: 0.7267736047506332, Final Batch Loss: 0.22472982108592987\n",
      "Epoch 4923, Loss: 0.7244439721107483, Final Batch Loss: 0.247938334941864\n",
      "Epoch 4924, Loss: 0.7598877549171448, Final Batch Loss: 0.30810827016830444\n",
      "Epoch 4925, Loss: 0.7350207269191742, Final Batch Loss: 0.26926642656326294\n",
      "Epoch 4926, Loss: 0.7252707034349442, Final Batch Loss: 0.21072803437709808\n",
      "Epoch 4927, Loss: 0.7860008031129837, Final Batch Loss: 0.24512846767902374\n",
      "Epoch 4928, Loss: 0.7593415379524231, Final Batch Loss: 0.244456484913826\n",
      "Epoch 4929, Loss: 0.9252462983131409, Final Batch Loss: 0.33915936946868896\n",
      "Epoch 4930, Loss: 0.8831250220537186, Final Batch Loss: 0.39524295926094055\n",
      "Epoch 4931, Loss: 0.6563548296689987, Final Batch Loss: 0.22554779052734375\n",
      "Epoch 4932, Loss: 0.7673037201166153, Final Batch Loss: 0.27026599645614624\n",
      "Epoch 4933, Loss: 0.6420511454343796, Final Batch Loss: 0.21909216046333313\n",
      "Epoch 4934, Loss: 0.7621314823627472, Final Batch Loss: 0.21540354192256927\n",
      "Epoch 4935, Loss: 0.8058252334594727, Final Batch Loss: 0.2981187701225281\n",
      "Epoch 4936, Loss: 0.8012436032295227, Final Batch Loss: 0.2534049153327942\n",
      "Epoch 4937, Loss: 0.806198388338089, Final Batch Loss: 0.2874493896961212\n",
      "Epoch 4938, Loss: 0.6583049148321152, Final Batch Loss: 0.2118314802646637\n",
      "Epoch 4939, Loss: 0.792252853512764, Final Batch Loss: 0.26753586530685425\n",
      "Epoch 4940, Loss: 0.7567373663187027, Final Batch Loss: 0.21660487353801727\n",
      "Epoch 4941, Loss: 0.8436225056648254, Final Batch Loss: 0.3179226517677307\n",
      "Epoch 4942, Loss: 0.7652754783630371, Final Batch Loss: 0.2794542908668518\n",
      "Epoch 4943, Loss: 0.708533838391304, Final Batch Loss: 0.2577819228172302\n",
      "Epoch 4944, Loss: 0.7268952876329422, Final Batch Loss: 0.19341959059238434\n",
      "Epoch 4945, Loss: 0.7054921537637711, Final Batch Loss: 0.20551738142967224\n",
      "Epoch 4946, Loss: 0.7782087326049805, Final Batch Loss: 0.25255826115608215\n",
      "Epoch 4947, Loss: 0.7978985607624054, Final Batch Loss: 0.30213168263435364\n",
      "Epoch 4948, Loss: 0.8463512659072876, Final Batch Loss: 0.2611362636089325\n",
      "Epoch 4949, Loss: 0.7883454263210297, Final Batch Loss: 0.26915132999420166\n",
      "Epoch 4950, Loss: 0.6933381855487823, Final Batch Loss: 0.24655532836914062\n",
      "Epoch 4951, Loss: 0.8006023615598679, Final Batch Loss: 0.20233429968357086\n",
      "Epoch 4952, Loss: 0.6803703755140305, Final Batch Loss: 0.1847022920846939\n",
      "Epoch 4953, Loss: 0.7337159216403961, Final Batch Loss: 0.197341650724411\n",
      "Epoch 4954, Loss: 0.8461896479129791, Final Batch Loss: 0.27110639214515686\n",
      "Epoch 4955, Loss: 0.8090263158082962, Final Batch Loss: 0.28329360485076904\n",
      "Epoch 4956, Loss: 0.7358378767967224, Final Batch Loss: 0.17330694198608398\n",
      "Epoch 4957, Loss: 0.714664027094841, Final Batch Loss: 0.17091678082942963\n",
      "Epoch 4958, Loss: 0.8195495754480362, Final Batch Loss: 0.287132203578949\n",
      "Epoch 4959, Loss: 0.7199775278568268, Final Batch Loss: 0.27806347608566284\n",
      "Epoch 4960, Loss: 0.8308349996805191, Final Batch Loss: 0.3781045079231262\n",
      "Epoch 4961, Loss: 0.7578015178442001, Final Batch Loss: 0.24994629621505737\n",
      "Epoch 4962, Loss: 0.7904437780380249, Final Batch Loss: 0.27816081047058105\n",
      "Epoch 4963, Loss: 0.7348960638046265, Final Batch Loss: 0.3065344989299774\n",
      "Epoch 4964, Loss: 0.6769369393587112, Final Batch Loss: 0.22685517370700836\n",
      "Epoch 4965, Loss: 0.7460138648748398, Final Batch Loss: 0.2742569148540497\n",
      "Epoch 4966, Loss: 0.7479072511196136, Final Batch Loss: 0.2253320813179016\n",
      "Epoch 4967, Loss: 0.8015065789222717, Final Batch Loss: 0.2351561188697815\n",
      "Epoch 4968, Loss: 0.8408844619989395, Final Batch Loss: 0.23118339478969574\n",
      "Epoch 4969, Loss: 0.6684987992048264, Final Batch Loss: 0.1955745369195938\n",
      "Epoch 4970, Loss: 0.6693056225776672, Final Batch Loss: 0.21102072298526764\n",
      "Epoch 4971, Loss: 0.7782275676727295, Final Batch Loss: 0.2025357335805893\n",
      "Epoch 4972, Loss: 0.7832368016242981, Final Batch Loss: 0.33810287714004517\n",
      "Epoch 4973, Loss: 0.8160833120346069, Final Batch Loss: 0.29352277517318726\n",
      "Epoch 4974, Loss: 0.6836786866188049, Final Batch Loss: 0.20474426448345184\n",
      "Epoch 4975, Loss: 0.700583204627037, Final Batch Loss: 0.3043384552001953\n",
      "Epoch 4976, Loss: 0.7505445778369904, Final Batch Loss: 0.2657669484615326\n",
      "Epoch 4977, Loss: 0.7167986333370209, Final Batch Loss: 0.23881226778030396\n",
      "Epoch 4978, Loss: 0.6696567684412003, Final Batch Loss: 0.22375580668449402\n",
      "Epoch 4979, Loss: 0.8114520460367203, Final Batch Loss: 0.2366720736026764\n",
      "Epoch 4980, Loss: 0.6542995125055313, Final Batch Loss: 0.14871865510940552\n",
      "Epoch 4981, Loss: 0.7221104651689529, Final Batch Loss: 0.22509337961673737\n",
      "Epoch 4982, Loss: 0.7330599874258041, Final Batch Loss: 0.21005822718143463\n",
      "Epoch 4983, Loss: 0.7566262036561966, Final Batch Loss: 0.24932582676410675\n",
      "Epoch 4984, Loss: 0.6780661344528198, Final Batch Loss: 0.18113668262958527\n",
      "Epoch 4985, Loss: 0.745497465133667, Final Batch Loss: 0.18117907643318176\n",
      "Epoch 4986, Loss: 0.8125273734331131, Final Batch Loss: 0.26829978823661804\n",
      "Epoch 4987, Loss: 0.6418291479349136, Final Batch Loss: 0.14605502784252167\n",
      "Epoch 4988, Loss: 0.7488387376070023, Final Batch Loss: 0.28252559900283813\n",
      "Epoch 4989, Loss: 0.8325846791267395, Final Batch Loss: 0.24971377849578857\n",
      "Epoch 4990, Loss: 0.8031859844923019, Final Batch Loss: 0.1965274214744568\n",
      "Epoch 4991, Loss: 0.7305799126625061, Final Batch Loss: 0.19344665110111237\n",
      "Epoch 4992, Loss: 0.7337176352739334, Final Batch Loss: 0.1630118489265442\n",
      "Epoch 4993, Loss: 0.7547838687896729, Final Batch Loss: 0.253243625164032\n",
      "Epoch 4994, Loss: 0.6558430790901184, Final Batch Loss: 0.21281348168849945\n",
      "Epoch 4995, Loss: 0.6887224912643433, Final Batch Loss: 0.2962615191936493\n",
      "Epoch 4996, Loss: 0.731794685125351, Final Batch Loss: 0.27926450967788696\n",
      "Epoch 4997, Loss: 0.7587261199951172, Final Batch Loss: 0.22851786017417908\n",
      "Epoch 4998, Loss: 0.7536853849887848, Final Batch Loss: 0.29727303981781006\n",
      "Epoch 4999, Loss: 0.7456654906272888, Final Batch Loss: 0.22787518799304962\n",
      "Epoch 5000, Loss: 0.7038107514381409, Final Batch Loss: 0.24205653369426727\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  5  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 13  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 11  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  0  4  0  0  4  0  0  0  0  0  2]\n",
      " [ 0  0  0  0  0  0 11  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  5  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  1  0  0  0 12  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  7  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 12  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  7  0  0]\n",
      " [ 1  1  0  0  0  0  0  0  0  0  1  0  0  5  0]\n",
      " [ 0  0  2  0  0  3  0  0  0  0  0  0  0  0  5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.94737   1.00000   0.97297        18\n",
      "           1    0.90000   0.90000   0.90000        10\n",
      "           2    0.55556   1.00000   0.71429         5\n",
      "           3    1.00000   1.00000   1.00000        13\n",
      "           4    1.00000   1.00000   1.00000        11\n",
      "           5    0.50000   0.33333   0.40000        12\n",
      "           6    1.00000   0.91667   0.95652        12\n",
      "           7    0.90000   1.00000   0.94737         9\n",
      "           8    0.55556   0.50000   0.52632        10\n",
      "           9    1.00000   0.92308   0.96000        13\n",
      "          10    0.87500   1.00000   0.93333         7\n",
      "          11    1.00000   1.00000   1.00000        12\n",
      "          12    1.00000   1.00000   1.00000         7\n",
      "          13    0.83333   0.62500   0.71429         8\n",
      "          14    0.41667   0.50000   0.45455        10\n",
      "\n",
      "    accuracy                        0.84713       157\n",
      "   macro avg    0.83223   0.84654   0.83198       157\n",
      "weighted avg    0.84996   0.84713   0.84348       157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_1 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_1 = np.zeros(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_2 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_2 = np.ones(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_3 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_3 = np.ones(n_samples) + 1\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_4 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_4 = np.ones(n_samples) + 2\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_5 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_5 = np.ones(n_samples) + 3\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_6 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_6 = np.ones(n_samples) + 4\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_7 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_7 = np.ones(n_samples) + 5\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_8 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_8 = np.ones(n_samples) + 6\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_9 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_9 = np.ones(n_samples) + 7\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U3A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_10 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_10 = np.ones(n_samples) + 8\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U3A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_11 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_11 = np.ones(n_samples) + 9\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U3A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_12 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_12 = np.ones(n_samples) + 10\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U4A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_13 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_13 = np.ones(n_samples) + 11\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U4A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_14 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_14 = np.ones(n_samples) + 12\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U4A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_15 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_15 = np.ones(n_samples) + 13\n",
    "\n",
    "\n",
    "fake_features = np.concatenate((fake_features_1, fake_features_2, fake_features_3, fake_features_4, fake_features_5, fake_features_6,\n",
    "                         fake_features_7, fake_features_8, fake_features_9, fake_features_10, fake_features_11, fake_features_12,\n",
    "                               fake_features_13, fake_features_14, fake_features_15))\n",
    "fake_labels = np.concatenate((y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9, y_10, y_11, y_12, y_13, y_14, y_15))\n",
    "\n",
    "fake_features = torch.Tensor(fake_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 20  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  0  0  3  0  0  0  0  0  4  0  0  7]\n",
      " [ 0  0  0 19  0  0  0  0  0  1  0  0  0  0  0]\n",
      " [ 0  2  0  0 16  0  0  0  0  0  2  0  0  0  0]\n",
      " [ 0  0  5  0  0  7  0  0  2  0  0  2  0  0  4]\n",
      " [ 0  0  0  0  0  0 20  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0  0  8  0  0  0  0]\n",
      " [ 0  0  0  0  0  4  0  0 10  0  0  2  0  0  4]\n",
      " [ 0  1  0  0  0  0  0  0  0  9  7  0  1  2  0]\n",
      " [ 0  0  0  1  4  0  0  2  0  0 13  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  3  0  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  0  0 19  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0  4  0  0 15  0]\n",
      " [ 0  0  9  0  0  3  0  0  2  0  0  0  0  0  6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    1.00000   1.00000   1.00000        20\n",
      "         1.0    0.86957   1.00000   0.93023        20\n",
      "         2.0    0.30000   0.30000   0.30000        20\n",
      "         3.0    0.95000   0.95000   0.95000        20\n",
      "         4.0    0.80000   0.80000   0.80000        20\n",
      "         5.0    0.26923   0.35000   0.30435        20\n",
      "         6.0    1.00000   1.00000   1.00000        20\n",
      "         7.0    0.80000   0.60000   0.68571        20\n",
      "         8.0    0.58824   0.50000   0.54054        20\n",
      "         9.0    0.81818   0.45000   0.58065        20\n",
      "        10.0    0.38235   0.65000   0.48148        20\n",
      "        11.0    0.50000   0.40000   0.44444        20\n",
      "        12.0    0.95000   0.95000   0.95000        20\n",
      "        13.0    0.88235   0.75000   0.81081        20\n",
      "        14.0    0.28571   0.30000   0.29268        20\n",
      "\n",
      "    accuracy                        0.66667       300\n",
      "   macro avg    0.69304   0.66667   0.67139       300\n",
      "weighted avg    0.69304   0.66667   0.67139       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
