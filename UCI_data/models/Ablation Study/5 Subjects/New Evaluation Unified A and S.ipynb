{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '90 tBodyAccJerk-max()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 30),\n",
    "            classifier_block(30, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            nn.Linear(15, 15)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def get_act_matrix(batch_size, a_dim):\n",
    "    indexes = np.random.randint(a_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((len(indexes), indexes.max()+1))\n",
    "    one_hot[np.arange(len(indexes)),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "    \n",
    "def get_usr_matrix(batch_size, u_dim):\n",
    "    indexes = np.random.randint(u_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((indexes.size, indexes.max()+1))\n",
    "    one_hot[np.arange(indexes.size),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Real Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "    \n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_10 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_11 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_12 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_13 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_14 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_15 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9, X_10, X_11, X_12, X_13, X_14, X_15))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9) + [9] * len(X_10) + [10] * len(X_11) + [11] * len(X_12) + [12] * len(X_13) + [13] * len(X_14) + [14] * len(X_15)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5, 7, 8]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 8.172544479370117, Final Batch Loss: 2.7316579818725586\n",
      "Epoch 2, Loss: 8.154963731765747, Final Batch Loss: 2.7064859867095947\n",
      "Epoch 3, Loss: 8.150194644927979, Final Batch Loss: 2.721109390258789\n",
      "Epoch 4, Loss: 8.146137237548828, Final Batch Loss: 2.7202839851379395\n",
      "Epoch 5, Loss: 8.13899564743042, Final Batch Loss: 2.712228536605835\n",
      "Epoch 6, Loss: 8.130512952804565, Final Batch Loss: 2.7118113040924072\n",
      "Epoch 7, Loss: 8.131357908248901, Final Batch Loss: 2.725954294204712\n",
      "Epoch 8, Loss: 8.118293523788452, Final Batch Loss: 2.7083590030670166\n",
      "Epoch 9, Loss: 8.096697092056274, Final Batch Loss: 2.679025650024414\n",
      "Epoch 10, Loss: 8.099243640899658, Final Batch Loss: 2.6928298473358154\n",
      "Epoch 11, Loss: 8.09138536453247, Final Batch Loss: 2.6920089721679688\n",
      "Epoch 12, Loss: 8.081949949264526, Final Batch Loss: 2.6925458908081055\n",
      "Epoch 13, Loss: 8.074125528335571, Final Batch Loss: 2.6950323581695557\n",
      "Epoch 14, Loss: 8.050451517105103, Final Batch Loss: 2.6773452758789062\n",
      "Epoch 15, Loss: 8.04484248161316, Final Batch Loss: 2.675572395324707\n",
      "Epoch 16, Loss: 8.029961347579956, Final Batch Loss: 2.6819522380828857\n",
      "Epoch 17, Loss: 8.00314474105835, Final Batch Loss: 2.672429323196411\n",
      "Epoch 18, Loss: 7.959460258483887, Final Batch Loss: 2.6447913646698\n",
      "Epoch 19, Loss: 7.935824155807495, Final Batch Loss: 2.6438257694244385\n",
      "Epoch 20, Loss: 7.883853197097778, Final Batch Loss: 2.616957187652588\n",
      "Epoch 21, Loss: 7.835865020751953, Final Batch Loss: 2.623889207839966\n",
      "Epoch 22, Loss: 7.728097438812256, Final Batch Loss: 2.547832727432251\n",
      "Epoch 23, Loss: 7.6567816734313965, Final Batch Loss: 2.541876792907715\n",
      "Epoch 24, Loss: 7.502463340759277, Final Batch Loss: 2.4475908279418945\n",
      "Epoch 25, Loss: 7.416446924209595, Final Batch Loss: 2.4640297889709473\n",
      "Epoch 26, Loss: 7.336658954620361, Final Batch Loss: 2.4676425457000732\n",
      "Epoch 27, Loss: 7.2271411418914795, Final Batch Loss: 2.44682240486145\n",
      "Epoch 28, Loss: 7.027284145355225, Final Batch Loss: 2.3092434406280518\n",
      "Epoch 29, Loss: 6.969812393188477, Final Batch Loss: 2.3833298683166504\n",
      "Epoch 30, Loss: 6.890485048294067, Final Batch Loss: 2.333622694015503\n",
      "Epoch 31, Loss: 6.742147922515869, Final Batch Loss: 2.233330726623535\n",
      "Epoch 32, Loss: 6.639372110366821, Final Batch Loss: 2.184446334838867\n",
      "Epoch 33, Loss: 6.632594347000122, Final Batch Loss: 2.2131187915802\n",
      "Epoch 34, Loss: 6.616696834564209, Final Batch Loss: 2.2808995246887207\n",
      "Epoch 35, Loss: 6.451551675796509, Final Batch Loss: 2.1630518436431885\n",
      "Epoch 36, Loss: 6.361805438995361, Final Batch Loss: 2.0730936527252197\n",
      "Epoch 37, Loss: 6.326802968978882, Final Batch Loss: 2.1081604957580566\n",
      "Epoch 38, Loss: 6.293971061706543, Final Batch Loss: 2.090940237045288\n",
      "Epoch 39, Loss: 6.269470453262329, Final Batch Loss: 2.0728163719177246\n",
      "Epoch 40, Loss: 6.222512483596802, Final Batch Loss: 2.0649192333221436\n",
      "Epoch 41, Loss: 6.196240663528442, Final Batch Loss: 2.0628645420074463\n",
      "Epoch 42, Loss: 6.037617206573486, Final Batch Loss: 1.9844300746917725\n",
      "Epoch 43, Loss: 6.095816135406494, Final Batch Loss: 2.0769779682159424\n",
      "Epoch 44, Loss: 6.0157705545425415, Final Batch Loss: 1.9822489023208618\n",
      "Epoch 45, Loss: 5.91721498966217, Final Batch Loss: 1.9759223461151123\n",
      "Epoch 46, Loss: 5.825561165809631, Final Batch Loss: 1.8978489637374878\n",
      "Epoch 47, Loss: 5.886363387107849, Final Batch Loss: 1.9976016283035278\n",
      "Epoch 48, Loss: 5.8590710163116455, Final Batch Loss: 1.9668946266174316\n",
      "Epoch 49, Loss: 5.730609178543091, Final Batch Loss: 1.9115419387817383\n",
      "Epoch 50, Loss: 5.736767411231995, Final Batch Loss: 1.8716940879821777\n",
      "Epoch 51, Loss: 5.665494918823242, Final Batch Loss: 1.8900481462478638\n",
      "Epoch 52, Loss: 5.616899132728577, Final Batch Loss: 1.8544931411743164\n",
      "Epoch 53, Loss: 5.544333100318909, Final Batch Loss: 1.849149227142334\n",
      "Epoch 54, Loss: 5.4342591762542725, Final Batch Loss: 1.8023191690444946\n",
      "Epoch 55, Loss: 5.26792299747467, Final Batch Loss: 1.7306654453277588\n",
      "Epoch 56, Loss: 5.3900169134140015, Final Batch Loss: 1.810524821281433\n",
      "Epoch 57, Loss: 5.30022406578064, Final Batch Loss: 1.780922532081604\n",
      "Epoch 58, Loss: 5.248066186904907, Final Batch Loss: 1.748111367225647\n",
      "Epoch 59, Loss: 5.291914105415344, Final Batch Loss: 1.8207489252090454\n",
      "Epoch 60, Loss: 5.181183815002441, Final Batch Loss: 1.7300915718078613\n",
      "Epoch 61, Loss: 5.122349739074707, Final Batch Loss: 1.7405152320861816\n",
      "Epoch 62, Loss: 5.067795634269714, Final Batch Loss: 1.646037220954895\n",
      "Epoch 63, Loss: 5.064015626907349, Final Batch Loss: 1.7030881643295288\n",
      "Epoch 64, Loss: 5.010908484458923, Final Batch Loss: 1.6876420974731445\n",
      "Epoch 65, Loss: 4.950707316398621, Final Batch Loss: 1.596938133239746\n",
      "Epoch 66, Loss: 4.8989986181259155, Final Batch Loss: 1.6526105403900146\n",
      "Epoch 67, Loss: 5.070302248001099, Final Batch Loss: 1.7578786611557007\n",
      "Epoch 68, Loss: 4.872250437736511, Final Batch Loss: 1.6712645292282104\n",
      "Epoch 69, Loss: 4.9143383502960205, Final Batch Loss: 1.6374586820602417\n",
      "Epoch 70, Loss: 4.7109527587890625, Final Batch Loss: 1.4905500411987305\n",
      "Epoch 71, Loss: 4.907216906547546, Final Batch Loss: 1.620233416557312\n",
      "Epoch 72, Loss: 4.754801392555237, Final Batch Loss: 1.636475920677185\n",
      "Epoch 73, Loss: 4.653984546661377, Final Batch Loss: 1.4639276266098022\n",
      "Epoch 74, Loss: 4.750025749206543, Final Batch Loss: 1.6296883821487427\n",
      "Epoch 75, Loss: 4.704162955284119, Final Batch Loss: 1.5580754280090332\n",
      "Epoch 76, Loss: 4.646705746650696, Final Batch Loss: 1.4861401319503784\n",
      "Epoch 77, Loss: 4.581013202667236, Final Batch Loss: 1.525386095046997\n",
      "Epoch 78, Loss: 4.662536382675171, Final Batch Loss: 1.6134871244430542\n",
      "Epoch 79, Loss: 4.534064531326294, Final Batch Loss: 1.45612633228302\n",
      "Epoch 80, Loss: 4.484102725982666, Final Batch Loss: 1.525349736213684\n",
      "Epoch 81, Loss: 4.469081878662109, Final Batch Loss: 1.4664794206619263\n",
      "Epoch 82, Loss: 4.473940014839172, Final Batch Loss: 1.4727461338043213\n",
      "Epoch 83, Loss: 4.404268383979797, Final Batch Loss: 1.4107052087783813\n",
      "Epoch 84, Loss: 4.339597821235657, Final Batch Loss: 1.448311686515808\n",
      "Epoch 85, Loss: 4.37233293056488, Final Batch Loss: 1.4901179075241089\n",
      "Epoch 86, Loss: 4.386523962020874, Final Batch Loss: 1.4388607740402222\n",
      "Epoch 87, Loss: 4.329365253448486, Final Batch Loss: 1.495619773864746\n",
      "Epoch 88, Loss: 4.137629508972168, Final Batch Loss: 1.3406322002410889\n",
      "Epoch 89, Loss: 4.319507837295532, Final Batch Loss: 1.4479395151138306\n",
      "Epoch 90, Loss: 4.2783414125442505, Final Batch Loss: 1.4883992671966553\n",
      "Epoch 91, Loss: 4.221864104270935, Final Batch Loss: 1.4665580987930298\n",
      "Epoch 92, Loss: 4.268527030944824, Final Batch Loss: 1.446110486984253\n",
      "Epoch 93, Loss: 4.2257291078567505, Final Batch Loss: 1.4414085149765015\n",
      "Epoch 94, Loss: 4.166974782943726, Final Batch Loss: 1.4076261520385742\n",
      "Epoch 95, Loss: 4.137771725654602, Final Batch Loss: 1.3472970724105835\n",
      "Epoch 96, Loss: 4.1696765422821045, Final Batch Loss: 1.3448070287704468\n",
      "Epoch 97, Loss: 4.132849097251892, Final Batch Loss: 1.3767043352127075\n",
      "Epoch 98, Loss: 4.141132235527039, Final Batch Loss: 1.3516632318496704\n",
      "Epoch 99, Loss: 4.203006386756897, Final Batch Loss: 1.4605191946029663\n",
      "Epoch 100, Loss: 4.193362712860107, Final Batch Loss: 1.460188865661621\n",
      "Epoch 101, Loss: 4.120084047317505, Final Batch Loss: 1.4175498485565186\n",
      "Epoch 102, Loss: 4.135242700576782, Final Batch Loss: 1.3709266185760498\n",
      "Epoch 103, Loss: 4.062960863113403, Final Batch Loss: 1.3232485055923462\n",
      "Epoch 104, Loss: 3.9370232820510864, Final Batch Loss: 1.302996277809143\n",
      "Epoch 105, Loss: 4.0139405727386475, Final Batch Loss: 1.3308097124099731\n",
      "Epoch 106, Loss: 4.11795699596405, Final Batch Loss: 1.4226583242416382\n",
      "Epoch 107, Loss: 4.20175313949585, Final Batch Loss: 1.4598968029022217\n",
      "Epoch 108, Loss: 3.997735381126404, Final Batch Loss: 1.3978177309036255\n",
      "Epoch 109, Loss: 3.9170100688934326, Final Batch Loss: 1.3533581495285034\n",
      "Epoch 110, Loss: 3.819763422012329, Final Batch Loss: 1.291517734527588\n",
      "Epoch 111, Loss: 3.8766801357269287, Final Batch Loss: 1.2912455797195435\n",
      "Epoch 112, Loss: 3.9494019746780396, Final Batch Loss: 1.3746757507324219\n",
      "Epoch 113, Loss: 3.8447993993759155, Final Batch Loss: 1.2657407522201538\n",
      "Epoch 114, Loss: 3.913973569869995, Final Batch Loss: 1.3524686098098755\n",
      "Epoch 115, Loss: 3.8432434797286987, Final Batch Loss: 1.263344645500183\n",
      "Epoch 116, Loss: 3.882299065589905, Final Batch Loss: 1.2550920248031616\n",
      "Epoch 117, Loss: 3.8137892484664917, Final Batch Loss: 1.3084238767623901\n",
      "Epoch 118, Loss: 3.761747360229492, Final Batch Loss: 1.2057377099990845\n",
      "Epoch 119, Loss: 3.7476165294647217, Final Batch Loss: 1.2762356996536255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120, Loss: 3.767132878303528, Final Batch Loss: 1.1788127422332764\n",
      "Epoch 121, Loss: 3.860102415084839, Final Batch Loss: 1.3423594236373901\n",
      "Epoch 122, Loss: 3.7901018857955933, Final Batch Loss: 1.198707103729248\n",
      "Epoch 123, Loss: 3.726729154586792, Final Batch Loss: 1.2427669763565063\n",
      "Epoch 124, Loss: 3.717448353767395, Final Batch Loss: 1.257447600364685\n",
      "Epoch 125, Loss: 3.760271430015564, Final Batch Loss: 1.2860336303710938\n",
      "Epoch 126, Loss: 3.76931369304657, Final Batch Loss: 1.1978704929351807\n",
      "Epoch 127, Loss: 3.7508513927459717, Final Batch Loss: 1.2299760580062866\n",
      "Epoch 128, Loss: 3.661033272743225, Final Batch Loss: 1.202179193496704\n",
      "Epoch 129, Loss: 3.7312577962875366, Final Batch Loss: 1.2545747756958008\n",
      "Epoch 130, Loss: 3.682947039604187, Final Batch Loss: 1.1804968118667603\n",
      "Epoch 131, Loss: 3.745893359184265, Final Batch Loss: 1.294352412223816\n",
      "Epoch 132, Loss: 3.623727560043335, Final Batch Loss: 1.3163801431655884\n",
      "Epoch 133, Loss: 3.6399019956588745, Final Batch Loss: 1.1425729990005493\n",
      "Epoch 134, Loss: 3.7762752771377563, Final Batch Loss: 1.2580331563949585\n",
      "Epoch 135, Loss: 3.604753613471985, Final Batch Loss: 1.1989747285842896\n",
      "Epoch 136, Loss: 3.6322587728500366, Final Batch Loss: 1.176855206489563\n",
      "Epoch 137, Loss: 3.7281278371810913, Final Batch Loss: 1.2520838975906372\n",
      "Epoch 138, Loss: 3.6340737342834473, Final Batch Loss: 1.2566702365875244\n",
      "Epoch 139, Loss: 3.599348545074463, Final Batch Loss: 1.2384039163589478\n",
      "Epoch 140, Loss: 3.4913913011550903, Final Batch Loss: 1.1181386709213257\n",
      "Epoch 141, Loss: 3.3862991333007812, Final Batch Loss: 1.0570549964904785\n",
      "Epoch 142, Loss: 3.431332588195801, Final Batch Loss: 1.1322400569915771\n",
      "Epoch 143, Loss: 3.5462573766708374, Final Batch Loss: 1.1655279397964478\n",
      "Epoch 144, Loss: 3.479444980621338, Final Batch Loss: 1.1303716897964478\n",
      "Epoch 145, Loss: 3.45683753490448, Final Batch Loss: 1.0833735466003418\n",
      "Epoch 146, Loss: 3.5855437517166138, Final Batch Loss: 1.1147189140319824\n",
      "Epoch 147, Loss: 3.50417959690094, Final Batch Loss: 1.1759235858917236\n",
      "Epoch 148, Loss: 3.4294393062591553, Final Batch Loss: 1.1162245273590088\n",
      "Epoch 149, Loss: 3.402770519256592, Final Batch Loss: 1.200561761856079\n",
      "Epoch 150, Loss: 3.562005043029785, Final Batch Loss: 1.20022714138031\n",
      "Epoch 151, Loss: 3.365955948829651, Final Batch Loss: 1.0588339567184448\n",
      "Epoch 152, Loss: 3.334609866142273, Final Batch Loss: 1.0066770315170288\n",
      "Epoch 153, Loss: 3.430092215538025, Final Batch Loss: 1.187045693397522\n",
      "Epoch 154, Loss: 3.393345594406128, Final Batch Loss: 1.1339043378829956\n",
      "Epoch 155, Loss: 3.430017352104187, Final Batch Loss: 1.1632522344589233\n",
      "Epoch 156, Loss: 3.265913724899292, Final Batch Loss: 1.0312714576721191\n",
      "Epoch 157, Loss: 3.3798294067382812, Final Batch Loss: 1.1224435567855835\n",
      "Epoch 158, Loss: 3.3324838876724243, Final Batch Loss: 1.1002551317214966\n",
      "Epoch 159, Loss: 3.277176022529602, Final Batch Loss: 1.0605517625808716\n",
      "Epoch 160, Loss: 3.406979560852051, Final Batch Loss: 1.1464296579360962\n",
      "Epoch 161, Loss: 3.2820571660995483, Final Batch Loss: 0.9921377897262573\n",
      "Epoch 162, Loss: 3.396685004234314, Final Batch Loss: 1.1546971797943115\n",
      "Epoch 163, Loss: 3.3856626749038696, Final Batch Loss: 1.2541532516479492\n",
      "Epoch 164, Loss: 3.3209872245788574, Final Batch Loss: 1.0707359313964844\n",
      "Epoch 165, Loss: 3.3995012044906616, Final Batch Loss: 1.1104254722595215\n",
      "Epoch 166, Loss: 3.201052188873291, Final Batch Loss: 1.0362590551376343\n",
      "Epoch 167, Loss: 3.275004744529724, Final Batch Loss: 1.0851401090621948\n",
      "Epoch 168, Loss: 3.335804581642151, Final Batch Loss: 1.1541109085083008\n",
      "Epoch 169, Loss: 3.2417490482330322, Final Batch Loss: 1.0527573823928833\n",
      "Epoch 170, Loss: 3.224255681037903, Final Batch Loss: 1.143324613571167\n",
      "Epoch 171, Loss: 3.2334564924240112, Final Batch Loss: 1.0412983894348145\n",
      "Epoch 172, Loss: 3.2554742097854614, Final Batch Loss: 1.0691100358963013\n",
      "Epoch 173, Loss: 3.257836699485779, Final Batch Loss: 1.0683397054672241\n",
      "Epoch 174, Loss: 3.141668438911438, Final Batch Loss: 1.008034586906433\n",
      "Epoch 175, Loss: 3.2653340101242065, Final Batch Loss: 1.1265044212341309\n",
      "Epoch 176, Loss: 3.3021297454833984, Final Batch Loss: 1.1124179363250732\n",
      "Epoch 177, Loss: 3.236767053604126, Final Batch Loss: 1.0593291521072388\n",
      "Epoch 178, Loss: 3.186716079711914, Final Batch Loss: 1.0528494119644165\n",
      "Epoch 179, Loss: 3.1728663444519043, Final Batch Loss: 1.0863665342330933\n",
      "Epoch 180, Loss: 3.1337281465530396, Final Batch Loss: 1.0135127305984497\n",
      "Epoch 181, Loss: 3.2608131170272827, Final Batch Loss: 1.1338473558425903\n",
      "Epoch 182, Loss: 3.2863837480545044, Final Batch Loss: 1.1106938123703003\n",
      "Epoch 183, Loss: 3.199008584022522, Final Batch Loss: 1.1339830160140991\n",
      "Epoch 184, Loss: 3.1238219141960144, Final Batch Loss: 1.127586007118225\n",
      "Epoch 185, Loss: 3.292538046836853, Final Batch Loss: 1.11201012134552\n",
      "Epoch 186, Loss: 3.1240113973617554, Final Batch Loss: 1.0158571004867554\n",
      "Epoch 187, Loss: 3.117734134197235, Final Batch Loss: 0.9875994324684143\n",
      "Epoch 188, Loss: 3.094936966896057, Final Batch Loss: 1.0167242288589478\n",
      "Epoch 189, Loss: 3.155248761177063, Final Batch Loss: 1.1174594163894653\n",
      "Epoch 190, Loss: 3.1089629530906677, Final Batch Loss: 0.9700292944908142\n",
      "Epoch 191, Loss: 3.0481483936309814, Final Batch Loss: 0.9768584966659546\n",
      "Epoch 192, Loss: 3.1029348373413086, Final Batch Loss: 1.0141619443893433\n",
      "Epoch 193, Loss: 3.063990592956543, Final Batch Loss: 1.0237520933151245\n",
      "Epoch 194, Loss: 2.993625044822693, Final Batch Loss: 0.9016128778457642\n",
      "Epoch 195, Loss: 3.1428000926971436, Final Batch Loss: 1.0023704767227173\n",
      "Epoch 196, Loss: 3.0592544078826904, Final Batch Loss: 0.9909156560897827\n",
      "Epoch 197, Loss: 2.909635603427887, Final Batch Loss: 0.9690800309181213\n",
      "Epoch 198, Loss: 3.1452348232269287, Final Batch Loss: 1.0621486902236938\n",
      "Epoch 199, Loss: 3.1749292612075806, Final Batch Loss: 1.1648368835449219\n",
      "Epoch 200, Loss: 3.2069345712661743, Final Batch Loss: 1.0749343633651733\n",
      "Epoch 201, Loss: 3.0259909629821777, Final Batch Loss: 0.9220690727233887\n",
      "Epoch 202, Loss: 2.9762232899665833, Final Batch Loss: 0.9576815366744995\n",
      "Epoch 203, Loss: 3.0293226838111877, Final Batch Loss: 1.1299643516540527\n",
      "Epoch 204, Loss: 2.977977454662323, Final Batch Loss: 1.0010764598846436\n",
      "Epoch 205, Loss: 2.9664878249168396, Final Batch Loss: 0.9296007752418518\n",
      "Epoch 206, Loss: 3.1308224201202393, Final Batch Loss: 1.0167946815490723\n",
      "Epoch 207, Loss: 3.0901259183883667, Final Batch Loss: 1.0545344352722168\n",
      "Epoch 208, Loss: 3.0003532767295837, Final Batch Loss: 1.1050634384155273\n",
      "Epoch 209, Loss: 2.9741321206092834, Final Batch Loss: 1.0519623756408691\n",
      "Epoch 210, Loss: 3.0553847551345825, Final Batch Loss: 1.0044736862182617\n",
      "Epoch 211, Loss: 2.9597139954566956, Final Batch Loss: 0.9926965832710266\n",
      "Epoch 212, Loss: 3.0270700454711914, Final Batch Loss: 1.0700279474258423\n",
      "Epoch 213, Loss: 2.93959504365921, Final Batch Loss: 0.9677960276603699\n",
      "Epoch 214, Loss: 2.871798098087311, Final Batch Loss: 0.9594073295593262\n",
      "Epoch 215, Loss: 3.0365601181983948, Final Batch Loss: 0.9555798172950745\n",
      "Epoch 216, Loss: 2.8479188084602356, Final Batch Loss: 0.9111976623535156\n",
      "Epoch 217, Loss: 3.0109118223190308, Final Batch Loss: 0.974711537361145\n",
      "Epoch 218, Loss: 2.8984355330467224, Final Batch Loss: 0.9696532487869263\n",
      "Epoch 219, Loss: 2.89115047454834, Final Batch Loss: 1.0319929122924805\n",
      "Epoch 220, Loss: 2.805757761001587, Final Batch Loss: 0.8246419429779053\n",
      "Epoch 221, Loss: 2.9105746150016785, Final Batch Loss: 1.0059475898742676\n",
      "Epoch 222, Loss: 2.945133328437805, Final Batch Loss: 1.0006303787231445\n",
      "Epoch 223, Loss: 2.8452990651130676, Final Batch Loss: 0.9674713015556335\n",
      "Epoch 224, Loss: 2.810976266860962, Final Batch Loss: 0.9631438851356506\n",
      "Epoch 225, Loss: 2.874794900417328, Final Batch Loss: 0.970710039138794\n",
      "Epoch 226, Loss: 2.971053957939148, Final Batch Loss: 1.0007867813110352\n",
      "Epoch 227, Loss: 2.87685227394104, Final Batch Loss: 0.9887506365776062\n",
      "Epoch 228, Loss: 2.959097743034363, Final Batch Loss: 1.0893945693969727\n",
      "Epoch 229, Loss: 2.804656147956848, Final Batch Loss: 0.9586459994316101\n",
      "Epoch 230, Loss: 2.8875967860221863, Final Batch Loss: 0.9739546179771423\n",
      "Epoch 231, Loss: 2.7876517176628113, Final Batch Loss: 0.8851929903030396\n",
      "Epoch 232, Loss: 2.771401584148407, Final Batch Loss: 0.9421581029891968\n",
      "Epoch 233, Loss: 2.9013161659240723, Final Batch Loss: 0.9286028742790222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 234, Loss: 2.8332161903381348, Final Batch Loss: 0.93327796459198\n",
      "Epoch 235, Loss: 2.7774727940559387, Final Batch Loss: 0.8563329577445984\n",
      "Epoch 236, Loss: 2.86604380607605, Final Batch Loss: 0.9028850197792053\n",
      "Epoch 237, Loss: 2.742743670940399, Final Batch Loss: 0.8988276720046997\n",
      "Epoch 238, Loss: 2.902869999408722, Final Batch Loss: 1.0384595394134521\n",
      "Epoch 239, Loss: 2.8143069744110107, Final Batch Loss: 0.9495493769645691\n",
      "Epoch 240, Loss: 2.8119568824768066, Final Batch Loss: 0.8934794068336487\n",
      "Epoch 241, Loss: 2.8764721751213074, Final Batch Loss: 1.0197991132736206\n",
      "Epoch 242, Loss: 2.7660643458366394, Final Batch Loss: 0.891944944858551\n",
      "Epoch 243, Loss: 2.7524913549423218, Final Batch Loss: 0.8946846723556519\n",
      "Epoch 244, Loss: 2.790008306503296, Final Batch Loss: 0.977592945098877\n",
      "Epoch 245, Loss: 2.764802634716034, Final Batch Loss: 0.8512153029441833\n",
      "Epoch 246, Loss: 2.6365898847579956, Final Batch Loss: 0.835214376449585\n",
      "Epoch 247, Loss: 2.7810025811195374, Final Batch Loss: 0.9643915295600891\n",
      "Epoch 248, Loss: 2.8308804035186768, Final Batch Loss: 0.9351257085800171\n",
      "Epoch 249, Loss: 2.7704708576202393, Final Batch Loss: 0.9062964916229248\n",
      "Epoch 250, Loss: 2.782211124897003, Final Batch Loss: 1.0078154802322388\n",
      "Epoch 251, Loss: 2.7694711089134216, Final Batch Loss: 0.8816679120063782\n",
      "Epoch 252, Loss: 2.724332630634308, Final Batch Loss: 0.8585540652275085\n",
      "Epoch 253, Loss: 2.688987374305725, Final Batch Loss: 0.9266764521598816\n",
      "Epoch 254, Loss: 2.619594395160675, Final Batch Loss: 0.7635865807533264\n",
      "Epoch 255, Loss: 2.851243495941162, Final Batch Loss: 0.9788949489593506\n",
      "Epoch 256, Loss: 2.8174407482147217, Final Batch Loss: 1.036563754081726\n",
      "Epoch 257, Loss: 2.622672438621521, Final Batch Loss: 0.8221181035041809\n",
      "Epoch 258, Loss: 2.676178812980652, Final Batch Loss: 0.9060040712356567\n",
      "Epoch 259, Loss: 2.7157790064811707, Final Batch Loss: 0.8946999311447144\n",
      "Epoch 260, Loss: 2.8126189708709717, Final Batch Loss: 0.9869025945663452\n",
      "Epoch 261, Loss: 2.589124321937561, Final Batch Loss: 0.8480727672576904\n",
      "Epoch 262, Loss: 2.7239680886268616, Final Batch Loss: 0.8760495781898499\n",
      "Epoch 263, Loss: 2.6207245588302612, Final Batch Loss: 0.8700916767120361\n",
      "Epoch 264, Loss: 2.716425836086273, Final Batch Loss: 0.9737324714660645\n",
      "Epoch 265, Loss: 2.7771605253219604, Final Batch Loss: 0.9425841569900513\n",
      "Epoch 266, Loss: 2.7099727988243103, Final Batch Loss: 0.865367591381073\n",
      "Epoch 267, Loss: 2.6565340757369995, Final Batch Loss: 0.7807297110557556\n",
      "Epoch 268, Loss: 2.712525725364685, Final Batch Loss: 0.8581516742706299\n",
      "Epoch 269, Loss: 2.6451226472854614, Final Batch Loss: 0.9091683030128479\n",
      "Epoch 270, Loss: 2.650079846382141, Final Batch Loss: 0.7821618318557739\n",
      "Epoch 271, Loss: 2.6852757334709167, Final Batch Loss: 0.8389288783073425\n",
      "Epoch 272, Loss: 2.602854549884796, Final Batch Loss: 0.7729546427726746\n",
      "Epoch 273, Loss: 2.627080023288727, Final Batch Loss: 0.8974718451499939\n",
      "Epoch 274, Loss: 2.6690104007720947, Final Batch Loss: 0.9261427521705627\n",
      "Epoch 275, Loss: 2.659975528717041, Final Batch Loss: 0.9130279421806335\n",
      "Epoch 276, Loss: 2.591369390487671, Final Batch Loss: 0.7827326059341431\n",
      "Epoch 277, Loss: 2.584988832473755, Final Batch Loss: 0.9361710548400879\n",
      "Epoch 278, Loss: 2.6873552799224854, Final Batch Loss: 0.9099692106246948\n",
      "Epoch 279, Loss: 2.7808656692504883, Final Batch Loss: 0.9834385514259338\n",
      "Epoch 280, Loss: 2.7481499910354614, Final Batch Loss: 0.923060953617096\n",
      "Epoch 281, Loss: 2.5688769221305847, Final Batch Loss: 0.7995663285255432\n",
      "Epoch 282, Loss: 2.5709856152534485, Final Batch Loss: 0.8212295770645142\n",
      "Epoch 283, Loss: 2.75473290681839, Final Batch Loss: 0.9131653308868408\n",
      "Epoch 284, Loss: 2.418636977672577, Final Batch Loss: 0.7052006125450134\n",
      "Epoch 285, Loss: 2.5920430421829224, Final Batch Loss: 0.8377965688705444\n",
      "Epoch 286, Loss: 2.6232029795646667, Final Batch Loss: 0.8799335360527039\n",
      "Epoch 287, Loss: 2.6679360270500183, Final Batch Loss: 0.8372467756271362\n",
      "Epoch 288, Loss: 2.6270853877067566, Final Batch Loss: 0.9037913680076599\n",
      "Epoch 289, Loss: 2.577383577823639, Final Batch Loss: 0.8698307275772095\n",
      "Epoch 290, Loss: 2.590188980102539, Final Batch Loss: 0.8958371877670288\n",
      "Epoch 291, Loss: 2.649397552013397, Final Batch Loss: 0.9280160069465637\n",
      "Epoch 292, Loss: 2.612752914428711, Final Batch Loss: 0.7784658074378967\n",
      "Epoch 293, Loss: 2.5988157987594604, Final Batch Loss: 0.8304982781410217\n",
      "Epoch 294, Loss: 2.4984492659568787, Final Batch Loss: 0.7720045447349548\n",
      "Epoch 295, Loss: 2.530139207839966, Final Batch Loss: 0.798936665058136\n",
      "Epoch 296, Loss: 2.728253185749054, Final Batch Loss: 0.9893366098403931\n",
      "Epoch 297, Loss: 2.5937129855155945, Final Batch Loss: 0.8113692402839661\n",
      "Epoch 298, Loss: 2.658697187900543, Final Batch Loss: 1.010817050933838\n",
      "Epoch 299, Loss: 2.6887335181236267, Final Batch Loss: 0.9915796518325806\n",
      "Epoch 300, Loss: 2.467627704143524, Final Batch Loss: 0.7649734020233154\n",
      "Epoch 301, Loss: 2.521828532218933, Final Batch Loss: 0.8198530673980713\n",
      "Epoch 302, Loss: 2.7197004556655884, Final Batch Loss: 1.0442460775375366\n",
      "Epoch 303, Loss: 2.452902138233185, Final Batch Loss: 0.8057587742805481\n",
      "Epoch 304, Loss: 2.703801155090332, Final Batch Loss: 0.9019472599029541\n",
      "Epoch 305, Loss: 2.566882848739624, Final Batch Loss: 0.8103333115577698\n",
      "Epoch 306, Loss: 2.552085757255554, Final Batch Loss: 0.8212249279022217\n",
      "Epoch 307, Loss: 2.594316303730011, Final Batch Loss: 0.8654123544692993\n",
      "Epoch 308, Loss: 2.511943817138672, Final Batch Loss: 0.7929622530937195\n",
      "Epoch 309, Loss: 2.422739565372467, Final Batch Loss: 0.7202265858650208\n",
      "Epoch 310, Loss: 2.598464012145996, Final Batch Loss: 0.8625708818435669\n",
      "Epoch 311, Loss: 2.5614534616470337, Final Batch Loss: 0.809721052646637\n",
      "Epoch 312, Loss: 2.4665295481681824, Final Batch Loss: 0.8555176854133606\n",
      "Epoch 313, Loss: 2.590758502483368, Final Batch Loss: 0.8181190490722656\n",
      "Epoch 314, Loss: 2.4224668741226196, Final Batch Loss: 0.8016424775123596\n",
      "Epoch 315, Loss: 2.5520569682121277, Final Batch Loss: 0.8866832852363586\n",
      "Epoch 316, Loss: 2.5798349380493164, Final Batch Loss: 0.8484582304954529\n",
      "Epoch 317, Loss: 2.5139212608337402, Final Batch Loss: 0.7849321961402893\n",
      "Epoch 318, Loss: 2.487572491168976, Final Batch Loss: 0.8618005514144897\n",
      "Epoch 319, Loss: 2.5226128697395325, Final Batch Loss: 0.9295437335968018\n",
      "Epoch 320, Loss: 2.48944753408432, Final Batch Loss: 0.865264892578125\n",
      "Epoch 321, Loss: 2.500651776790619, Final Batch Loss: 0.7904520630836487\n",
      "Epoch 322, Loss: 2.5188127756118774, Final Batch Loss: 0.8750287890434265\n",
      "Epoch 323, Loss: 2.517533779144287, Final Batch Loss: 0.9152321815490723\n",
      "Epoch 324, Loss: 2.5300996899604797, Final Batch Loss: 0.9168832898139954\n",
      "Epoch 325, Loss: 2.4446564316749573, Final Batch Loss: 0.7090240120887756\n",
      "Epoch 326, Loss: 2.5642706155776978, Final Batch Loss: 0.9386191964149475\n",
      "Epoch 327, Loss: 2.3719465732574463, Final Batch Loss: 0.8454684019088745\n",
      "Epoch 328, Loss: 2.4531977772712708, Final Batch Loss: 0.8110031485557556\n",
      "Epoch 329, Loss: 2.403521776199341, Final Batch Loss: 0.8383952975273132\n",
      "Epoch 330, Loss: 2.5107096433639526, Final Batch Loss: 0.8983218669891357\n",
      "Epoch 331, Loss: 2.4185230135917664, Final Batch Loss: 0.8734994530677795\n",
      "Epoch 332, Loss: 2.470718502998352, Final Batch Loss: 0.8686195611953735\n",
      "Epoch 333, Loss: 2.414322316646576, Final Batch Loss: 0.8247817158699036\n",
      "Epoch 334, Loss: 2.490249812602997, Final Batch Loss: 0.7566197514533997\n",
      "Epoch 335, Loss: 2.3627254962921143, Final Batch Loss: 0.7557060122489929\n",
      "Epoch 336, Loss: 2.486521363258362, Final Batch Loss: 0.8836137652397156\n",
      "Epoch 337, Loss: 2.4642242789268494, Final Batch Loss: 0.8773322701454163\n",
      "Epoch 338, Loss: 2.372837722301483, Final Batch Loss: 0.7505564093589783\n",
      "Epoch 339, Loss: 2.4920367002487183, Final Batch Loss: 0.8190761804580688\n",
      "Epoch 340, Loss: 2.3884421586990356, Final Batch Loss: 0.7783203721046448\n",
      "Epoch 341, Loss: 2.4573070406913757, Final Batch Loss: 0.7668520212173462\n",
      "Epoch 342, Loss: 2.4795539379119873, Final Batch Loss: 0.8602600693702698\n",
      "Epoch 343, Loss: 2.4746137261390686, Final Batch Loss: 0.8972903490066528\n",
      "Epoch 344, Loss: 2.3397796154022217, Final Batch Loss: 0.7691483497619629\n",
      "Epoch 345, Loss: 2.393177807331085, Final Batch Loss: 0.762562096118927\n",
      "Epoch 346, Loss: 2.392043888568878, Final Batch Loss: 0.7808696031570435\n",
      "Epoch 347, Loss: 2.4566681385040283, Final Batch Loss: 0.8975090384483337\n",
      "Epoch 348, Loss: 2.248266816139221, Final Batch Loss: 0.6839541792869568\n",
      "Epoch 349, Loss: 2.3779152035713196, Final Batch Loss: 0.8079638481140137\n",
      "Epoch 350, Loss: 2.5301244258880615, Final Batch Loss: 0.8320440649986267\n",
      "Epoch 351, Loss: 2.326058268547058, Final Batch Loss: 0.748033344745636\n",
      "Epoch 352, Loss: 2.294296681880951, Final Batch Loss: 0.7340047955513\n",
      "Epoch 353, Loss: 2.277670681476593, Final Batch Loss: 0.769193708896637\n",
      "Epoch 354, Loss: 2.317730665206909, Final Batch Loss: 0.6698428392410278\n",
      "Epoch 355, Loss: 2.4997552633285522, Final Batch Loss: 0.861813485622406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 356, Loss: 2.4448389410972595, Final Batch Loss: 0.9184064269065857\n",
      "Epoch 357, Loss: 2.4483324885368347, Final Batch Loss: 0.8516925573348999\n",
      "Epoch 358, Loss: 2.3489471077919006, Final Batch Loss: 0.7277833819389343\n",
      "Epoch 359, Loss: 2.2969375252723694, Final Batch Loss: 0.7043113112449646\n",
      "Epoch 360, Loss: 2.3862507343292236, Final Batch Loss: 0.8550324440002441\n",
      "Epoch 361, Loss: 2.3815714716911316, Final Batch Loss: 0.7526801228523254\n",
      "Epoch 362, Loss: 2.280105471611023, Final Batch Loss: 0.7295131683349609\n",
      "Epoch 363, Loss: 2.2767646312713623, Final Batch Loss: 0.7097166776657104\n",
      "Epoch 364, Loss: 2.278864026069641, Final Batch Loss: 0.6604459881782532\n",
      "Epoch 365, Loss: 2.3162500858306885, Final Batch Loss: 0.7495490312576294\n",
      "Epoch 366, Loss: 2.2945291996002197, Final Batch Loss: 0.815077543258667\n",
      "Epoch 367, Loss: 2.379838287830353, Final Batch Loss: 0.8473616242408752\n",
      "Epoch 368, Loss: 2.3146650791168213, Final Batch Loss: 0.688154399394989\n",
      "Epoch 369, Loss: 2.257830262184143, Final Batch Loss: 0.7409639954566956\n",
      "Epoch 370, Loss: 2.331884980201721, Final Batch Loss: 0.7013180255889893\n",
      "Epoch 371, Loss: 2.3262369632720947, Final Batch Loss: 0.7940734624862671\n",
      "Epoch 372, Loss: 2.2946484684944153, Final Batch Loss: 0.747780442237854\n",
      "Epoch 373, Loss: 2.327867567539215, Final Batch Loss: 0.7696727514266968\n",
      "Epoch 374, Loss: 2.3680452704429626, Final Batch Loss: 0.8499489426612854\n",
      "Epoch 375, Loss: 2.2116552591323853, Final Batch Loss: 0.6893733739852905\n",
      "Epoch 376, Loss: 2.302378475666046, Final Batch Loss: 0.7575452923774719\n",
      "Epoch 377, Loss: 2.3477157950401306, Final Batch Loss: 0.8000550270080566\n",
      "Epoch 378, Loss: 2.271565556526184, Final Batch Loss: 0.7771509885787964\n",
      "Epoch 379, Loss: 2.264941990375519, Final Batch Loss: 0.683217465877533\n",
      "Epoch 380, Loss: 2.2418511509895325, Final Batch Loss: 0.702494740486145\n",
      "Epoch 381, Loss: 2.3115114569664, Final Batch Loss: 0.7589468359947205\n",
      "Epoch 382, Loss: 2.232210397720337, Final Batch Loss: 0.6714551448822021\n",
      "Epoch 383, Loss: 2.414715528488159, Final Batch Loss: 0.7686177492141724\n",
      "Epoch 384, Loss: 2.358439862728119, Final Batch Loss: 0.775978684425354\n",
      "Epoch 385, Loss: 2.3242358565330505, Final Batch Loss: 0.8194841146469116\n",
      "Epoch 386, Loss: 2.265630543231964, Final Batch Loss: 0.7442134022712708\n",
      "Epoch 387, Loss: 2.218340277671814, Final Batch Loss: 0.8139239549636841\n",
      "Epoch 388, Loss: 2.1367207169532776, Final Batch Loss: 0.6541628241539001\n",
      "Epoch 389, Loss: 2.3050761818885803, Final Batch Loss: 0.81087327003479\n",
      "Epoch 390, Loss: 2.33467698097229, Final Batch Loss: 0.8004291653633118\n",
      "Epoch 391, Loss: 2.287389099597931, Final Batch Loss: 0.7192490696907043\n",
      "Epoch 392, Loss: 2.313830077648163, Final Batch Loss: 0.831117570400238\n",
      "Epoch 393, Loss: 2.1938095688819885, Final Batch Loss: 0.7556655406951904\n",
      "Epoch 394, Loss: 2.3125012516975403, Final Batch Loss: 0.8451730012893677\n",
      "Epoch 395, Loss: 2.320796549320221, Final Batch Loss: 0.8038797974586487\n",
      "Epoch 396, Loss: 2.2263007760047913, Final Batch Loss: 0.7267161011695862\n",
      "Epoch 397, Loss: 2.18696790933609, Final Batch Loss: 0.708564817905426\n",
      "Epoch 398, Loss: 2.214358389377594, Final Batch Loss: 0.7498443722724915\n",
      "Epoch 399, Loss: 2.1601579189300537, Final Batch Loss: 0.6561018228530884\n",
      "Epoch 400, Loss: 2.2993698716163635, Final Batch Loss: 0.7756177186965942\n",
      "Epoch 401, Loss: 2.1574482321739197, Final Batch Loss: 0.68503338098526\n",
      "Epoch 402, Loss: 2.1887577176094055, Final Batch Loss: 0.7486785650253296\n",
      "Epoch 403, Loss: 2.2714751958847046, Final Batch Loss: 0.7144867777824402\n",
      "Epoch 404, Loss: 2.1383864283561707, Final Batch Loss: 0.7271304726600647\n",
      "Epoch 405, Loss: 2.2535144090652466, Final Batch Loss: 0.8349772095680237\n",
      "Epoch 406, Loss: 2.24164617061615, Final Batch Loss: 0.7207568287849426\n",
      "Epoch 407, Loss: 2.2024735808372498, Final Batch Loss: 0.7403064370155334\n",
      "Epoch 408, Loss: 2.2512542009353638, Final Batch Loss: 0.7615000009536743\n",
      "Epoch 409, Loss: 2.306981146335602, Final Batch Loss: 0.7201167345046997\n",
      "Epoch 410, Loss: 2.2265546917915344, Final Batch Loss: 0.7921903133392334\n",
      "Epoch 411, Loss: 2.275605082511902, Final Batch Loss: 0.77818763256073\n",
      "Epoch 412, Loss: 2.200708270072937, Final Batch Loss: 0.8141812086105347\n",
      "Epoch 413, Loss: 2.144783616065979, Final Batch Loss: 0.6257069110870361\n",
      "Epoch 414, Loss: 2.4024654030799866, Final Batch Loss: 0.9189440608024597\n",
      "Epoch 415, Loss: 2.095944404602051, Final Batch Loss: 0.6367107629776001\n",
      "Epoch 416, Loss: 2.223586857318878, Final Batch Loss: 0.7645267248153687\n",
      "Epoch 417, Loss: 2.2391693592071533, Final Batch Loss: 0.8157912492752075\n",
      "Epoch 418, Loss: 2.33654522895813, Final Batch Loss: 0.8095868229866028\n",
      "Epoch 419, Loss: 2.2613377571105957, Final Batch Loss: 0.732592761516571\n",
      "Epoch 420, Loss: 2.1909741163253784, Final Batch Loss: 0.6799976229667664\n",
      "Epoch 421, Loss: 2.197508931159973, Final Batch Loss: 0.7534444332122803\n",
      "Epoch 422, Loss: 2.1058802604675293, Final Batch Loss: 0.6729927659034729\n",
      "Epoch 423, Loss: 2.0696340799331665, Final Batch Loss: 0.6301274299621582\n",
      "Epoch 424, Loss: 2.246661365032196, Final Batch Loss: 0.6967965364456177\n",
      "Epoch 425, Loss: 2.27250999212265, Final Batch Loss: 0.7538246512413025\n",
      "Epoch 426, Loss: 2.1558194160461426, Final Batch Loss: 0.7211561799049377\n",
      "Epoch 427, Loss: 2.2859294414520264, Final Batch Loss: 0.8242077231407166\n",
      "Epoch 428, Loss: 2.2396769523620605, Final Batch Loss: 0.7899711728096008\n",
      "Epoch 429, Loss: 2.0731260776519775, Final Batch Loss: 0.7096489071846008\n",
      "Epoch 430, Loss: 2.3663870096206665, Final Batch Loss: 0.8515292406082153\n",
      "Epoch 431, Loss: 2.0027899146080017, Final Batch Loss: 0.6772381663322449\n",
      "Epoch 432, Loss: 2.0942280292510986, Final Batch Loss: 0.7070724964141846\n",
      "Epoch 433, Loss: 2.200924038887024, Final Batch Loss: 0.845243513584137\n",
      "Epoch 434, Loss: 2.189251661300659, Final Batch Loss: 0.6997038125991821\n",
      "Epoch 435, Loss: 2.125076413154602, Final Batch Loss: 0.6800568103790283\n",
      "Epoch 436, Loss: 2.171975612640381, Final Batch Loss: 0.7260192036628723\n",
      "Epoch 437, Loss: 2.1250844597816467, Final Batch Loss: 0.7245923280715942\n",
      "Epoch 438, Loss: 2.1922162771224976, Final Batch Loss: 0.8090794086456299\n",
      "Epoch 439, Loss: 2.195762872695923, Final Batch Loss: 0.792212963104248\n",
      "Epoch 440, Loss: 2.2677762508392334, Final Batch Loss: 0.7204616069793701\n",
      "Epoch 441, Loss: 2.087134599685669, Final Batch Loss: 0.7234530448913574\n",
      "Epoch 442, Loss: 2.1675203442573547, Final Batch Loss: 0.7684472799301147\n",
      "Epoch 443, Loss: 2.125419497489929, Final Batch Loss: 0.6731482148170471\n",
      "Epoch 444, Loss: 2.230485260486603, Final Batch Loss: 0.6980333924293518\n",
      "Epoch 445, Loss: 2.2157920598983765, Final Batch Loss: 0.771271288394928\n",
      "Epoch 446, Loss: 2.233115553855896, Final Batch Loss: 0.7581619620323181\n",
      "Epoch 447, Loss: 2.1051860451698303, Final Batch Loss: 0.6871762275695801\n",
      "Epoch 448, Loss: 2.220522403717041, Final Batch Loss: 0.7850934863090515\n",
      "Epoch 449, Loss: 2.0250855684280396, Final Batch Loss: 0.6363875269889832\n",
      "Epoch 450, Loss: 2.1365081667900085, Final Batch Loss: 0.7098801732063293\n",
      "Epoch 451, Loss: 2.1905935406684875, Final Batch Loss: 0.8042112588882446\n",
      "Epoch 452, Loss: 2.1212340593338013, Final Batch Loss: 0.7249615788459778\n",
      "Epoch 453, Loss: 2.1935737133026123, Final Batch Loss: 0.7220535278320312\n",
      "Epoch 454, Loss: 2.2567930817604065, Final Batch Loss: 0.8278141617774963\n",
      "Epoch 455, Loss: 2.187322199344635, Final Batch Loss: 0.7326405644416809\n",
      "Epoch 456, Loss: 1.9943550825119019, Final Batch Loss: 0.5828806757926941\n",
      "Epoch 457, Loss: 2.0417040586471558, Final Batch Loss: 0.647024393081665\n",
      "Epoch 458, Loss: 2.091247260570526, Final Batch Loss: 0.6754238605499268\n",
      "Epoch 459, Loss: 2.1777668595314026, Final Batch Loss: 0.683810830116272\n",
      "Epoch 460, Loss: 2.1532079577445984, Final Batch Loss: 0.7178545594215393\n",
      "Epoch 461, Loss: 2.010032594203949, Final Batch Loss: 0.6464990377426147\n",
      "Epoch 462, Loss: 2.113446295261383, Final Batch Loss: 0.6364588737487793\n",
      "Epoch 463, Loss: 2.127313196659088, Final Batch Loss: 0.6680380702018738\n",
      "Epoch 464, Loss: 2.0387991666793823, Final Batch Loss: 0.7028132081031799\n",
      "Epoch 465, Loss: 2.1817490458488464, Final Batch Loss: 0.812423586845398\n",
      "Epoch 466, Loss: 2.0600024461746216, Final Batch Loss: 0.685806393623352\n",
      "Epoch 467, Loss: 2.1089213490486145, Final Batch Loss: 0.8049178719520569\n",
      "Epoch 468, Loss: 2.130677878856659, Final Batch Loss: 0.66666179895401\n",
      "Epoch 469, Loss: 2.0786606669425964, Final Batch Loss: 0.7388170957565308\n",
      "Epoch 470, Loss: 2.1973604559898376, Final Batch Loss: 0.7412973046302795\n",
      "Epoch 471, Loss: 2.0221495032310486, Final Batch Loss: 0.7058312892913818\n",
      "Epoch 472, Loss: 2.198025345802307, Final Batch Loss: 0.7953087091445923\n",
      "Epoch 473, Loss: 2.0328365564346313, Final Batch Loss: 0.629397451877594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 474, Loss: 1.9142173528671265, Final Batch Loss: 0.5282555818557739\n",
      "Epoch 475, Loss: 2.1625670194625854, Final Batch Loss: 0.7613092660903931\n",
      "Epoch 476, Loss: 1.9102672338485718, Final Batch Loss: 0.6157441139221191\n",
      "Epoch 477, Loss: 2.1284517645835876, Final Batch Loss: 0.7180635929107666\n",
      "Epoch 478, Loss: 2.2188428044319153, Final Batch Loss: 0.7475114464759827\n",
      "Epoch 479, Loss: 2.1360787749290466, Final Batch Loss: 0.7964493632316589\n",
      "Epoch 480, Loss: 2.0402934551239014, Final Batch Loss: 0.6016120910644531\n",
      "Epoch 481, Loss: 2.1094512343406677, Final Batch Loss: 0.7281701564788818\n",
      "Epoch 482, Loss: 2.1761869192123413, Final Batch Loss: 0.7564082145690918\n",
      "Epoch 483, Loss: 2.063679277896881, Final Batch Loss: 0.6582834124565125\n",
      "Epoch 484, Loss: 2.0744786858558655, Final Batch Loss: 0.600000262260437\n",
      "Epoch 485, Loss: 2.057546854019165, Final Batch Loss: 0.7688714265823364\n",
      "Epoch 486, Loss: 2.060306966304779, Final Batch Loss: 0.6763064861297607\n",
      "Epoch 487, Loss: 2.0538359880447388, Final Batch Loss: 0.6501349210739136\n",
      "Epoch 488, Loss: 2.1030613780021667, Final Batch Loss: 0.6590346097946167\n",
      "Epoch 489, Loss: 2.075071156024933, Final Batch Loss: 0.6842309236526489\n",
      "Epoch 490, Loss: 1.988854706287384, Final Batch Loss: 0.6147645711898804\n",
      "Epoch 491, Loss: 2.168900966644287, Final Batch Loss: 0.7726595401763916\n",
      "Epoch 492, Loss: 2.1243311762809753, Final Batch Loss: 0.6447278261184692\n",
      "Epoch 493, Loss: 1.9716402292251587, Final Batch Loss: 0.6181223392486572\n",
      "Epoch 494, Loss: 2.029772400856018, Final Batch Loss: 0.6557609438896179\n",
      "Epoch 495, Loss: 2.014092206954956, Final Batch Loss: 0.6139320135116577\n",
      "Epoch 496, Loss: 2.0265299677848816, Final Batch Loss: 0.6008709669113159\n",
      "Epoch 497, Loss: 2.173970937728882, Final Batch Loss: 0.7052980065345764\n",
      "Epoch 498, Loss: 1.9874451756477356, Final Batch Loss: 0.6387620568275452\n",
      "Epoch 499, Loss: 2.070352017879486, Final Batch Loss: 0.7513293027877808\n",
      "Epoch 500, Loss: 2.1551252603530884, Final Batch Loss: 0.7161896824836731\n",
      "Epoch 501, Loss: 1.9953081011772156, Final Batch Loss: 0.6301918625831604\n",
      "Epoch 502, Loss: 2.0470064282417297, Final Batch Loss: 0.6951788067817688\n",
      "Epoch 503, Loss: 2.003656268119812, Final Batch Loss: 0.6457430720329285\n",
      "Epoch 504, Loss: 2.143398404121399, Final Batch Loss: 0.748250424861908\n",
      "Epoch 505, Loss: 2.139177680015564, Final Batch Loss: 0.7199033498764038\n",
      "Epoch 506, Loss: 2.055339992046356, Final Batch Loss: 0.7392063140869141\n",
      "Epoch 507, Loss: 2.0103840231895447, Final Batch Loss: 0.6936437487602234\n",
      "Epoch 508, Loss: 1.9594494104385376, Final Batch Loss: 0.6441745162010193\n",
      "Epoch 509, Loss: 2.128836512565613, Final Batch Loss: 0.6660768985748291\n",
      "Epoch 510, Loss: 2.0080661177635193, Final Batch Loss: 0.6358965635299683\n",
      "Epoch 511, Loss: 2.1815277338027954, Final Batch Loss: 0.7456985116004944\n",
      "Epoch 512, Loss: 1.9961177110671997, Final Batch Loss: 0.6981323957443237\n",
      "Epoch 513, Loss: 2.055886745452881, Final Batch Loss: 0.7209118008613586\n",
      "Epoch 514, Loss: 2.0540830492973328, Final Batch Loss: 0.6668552160263062\n",
      "Epoch 515, Loss: 1.995079755783081, Final Batch Loss: 0.6306835412979126\n",
      "Epoch 516, Loss: 1.972981035709381, Final Batch Loss: 0.6090179681777954\n",
      "Epoch 517, Loss: 1.9456260204315186, Final Batch Loss: 0.6512630581855774\n",
      "Epoch 518, Loss: 2.017902195453644, Final Batch Loss: 0.6940198540687561\n",
      "Epoch 519, Loss: 2.004693865776062, Final Batch Loss: 0.6694653034210205\n",
      "Epoch 520, Loss: 1.9621172547340393, Final Batch Loss: 0.6624819040298462\n",
      "Epoch 521, Loss: 2.00762277841568, Final Batch Loss: 0.6916905641555786\n",
      "Epoch 522, Loss: 2.1464791893959045, Final Batch Loss: 0.7605552077293396\n",
      "Epoch 523, Loss: 2.044145703315735, Final Batch Loss: 0.7335910201072693\n",
      "Epoch 524, Loss: 2.1557827591896057, Final Batch Loss: 0.6202107071876526\n",
      "Epoch 525, Loss: 1.99666166305542, Final Batch Loss: 0.6431848406791687\n",
      "Epoch 526, Loss: 2.008246958255768, Final Batch Loss: 0.6120643615722656\n",
      "Epoch 527, Loss: 2.064179539680481, Final Batch Loss: 0.6797295808792114\n",
      "Epoch 528, Loss: 2.094839334487915, Final Batch Loss: 0.798968493938446\n",
      "Epoch 529, Loss: 2.0041752457618713, Final Batch Loss: 0.6666053533554077\n",
      "Epoch 530, Loss: 2.0065905451774597, Final Batch Loss: 0.6444190144538879\n",
      "Epoch 531, Loss: 1.9916287064552307, Final Batch Loss: 0.5905534029006958\n",
      "Epoch 532, Loss: 1.9867462515830994, Final Batch Loss: 0.6465150713920593\n",
      "Epoch 533, Loss: 2.041805863380432, Final Batch Loss: 0.6736157536506653\n",
      "Epoch 534, Loss: 1.9822646975517273, Final Batch Loss: 0.6694223284721375\n",
      "Epoch 535, Loss: 2.0777742862701416, Final Batch Loss: 0.7261263728141785\n",
      "Epoch 536, Loss: 2.005346417427063, Final Batch Loss: 0.6346847414970398\n",
      "Epoch 537, Loss: 2.002230644226074, Final Batch Loss: 0.633816659450531\n",
      "Epoch 538, Loss: 1.934119999408722, Final Batch Loss: 0.6295188665390015\n",
      "Epoch 539, Loss: 2.0839375853538513, Final Batch Loss: 0.6352072358131409\n",
      "Epoch 540, Loss: 1.915755569934845, Final Batch Loss: 0.5837542414665222\n",
      "Epoch 541, Loss: 1.9426965117454529, Final Batch Loss: 0.5968723893165588\n",
      "Epoch 542, Loss: 1.9176254868507385, Final Batch Loss: 0.5985053181648254\n",
      "Epoch 543, Loss: 1.9951946139335632, Final Batch Loss: 0.6543299555778503\n",
      "Epoch 544, Loss: 1.9399875402450562, Final Batch Loss: 0.5931373834609985\n",
      "Epoch 545, Loss: 2.021169364452362, Final Batch Loss: 0.7022036910057068\n",
      "Epoch 546, Loss: 2.0044689178466797, Final Batch Loss: 0.6184433698654175\n",
      "Epoch 547, Loss: 1.9856508374214172, Final Batch Loss: 0.650547444820404\n",
      "Epoch 548, Loss: 1.9301940202713013, Final Batch Loss: 0.6253950595855713\n",
      "Epoch 549, Loss: 1.9157916903495789, Final Batch Loss: 0.6160986423492432\n",
      "Epoch 550, Loss: 2.0565194487571716, Final Batch Loss: 0.5628055930137634\n",
      "Epoch 551, Loss: 2.0961288809776306, Final Batch Loss: 0.7853441834449768\n",
      "Epoch 552, Loss: 1.9035342335700989, Final Batch Loss: 0.6237761974334717\n",
      "Epoch 553, Loss: 1.879044771194458, Final Batch Loss: 0.5887869000434875\n",
      "Epoch 554, Loss: 2.054919421672821, Final Batch Loss: 0.7009390592575073\n",
      "Epoch 555, Loss: 1.9227864146232605, Final Batch Loss: 0.5592424273490906\n",
      "Epoch 556, Loss: 1.9250760674476624, Final Batch Loss: 0.5860061645507812\n",
      "Epoch 557, Loss: 2.0555943846702576, Final Batch Loss: 0.6912556886672974\n",
      "Epoch 558, Loss: 1.9908573627471924, Final Batch Loss: 0.6610761880874634\n",
      "Epoch 559, Loss: 1.917099118232727, Final Batch Loss: 0.6262430548667908\n",
      "Epoch 560, Loss: 2.018148362636566, Final Batch Loss: 0.6143078804016113\n",
      "Epoch 561, Loss: 2.018145501613617, Final Batch Loss: 0.724159300327301\n",
      "Epoch 562, Loss: 1.9932789206504822, Final Batch Loss: 0.6376001834869385\n",
      "Epoch 563, Loss: 1.9214416146278381, Final Batch Loss: 0.6223331689834595\n",
      "Epoch 564, Loss: 2.144756555557251, Final Batch Loss: 0.7416899800300598\n",
      "Epoch 565, Loss: 2.0362786054611206, Final Batch Loss: 0.7690826654434204\n",
      "Epoch 566, Loss: 2.0006654262542725, Final Batch Loss: 0.7061787247657776\n",
      "Epoch 567, Loss: 1.984827995300293, Final Batch Loss: 0.7150769829750061\n",
      "Epoch 568, Loss: 2.1349949836730957, Final Batch Loss: 0.7502322793006897\n",
      "Epoch 569, Loss: 1.9010177850723267, Final Batch Loss: 0.5675317645072937\n",
      "Epoch 570, Loss: 2.0033006072044373, Final Batch Loss: 0.6793977618217468\n",
      "Epoch 571, Loss: 1.9586853981018066, Final Batch Loss: 0.6397706270217896\n",
      "Epoch 572, Loss: 2.1436330676078796, Final Batch Loss: 0.7719658613204956\n",
      "Epoch 573, Loss: 1.942956030368805, Final Batch Loss: 0.633813738822937\n",
      "Epoch 574, Loss: 2.056406557559967, Final Batch Loss: 0.6806271076202393\n",
      "Epoch 575, Loss: 1.8929262161254883, Final Batch Loss: 0.627036452293396\n",
      "Epoch 576, Loss: 1.9800889492034912, Final Batch Loss: 0.6719346046447754\n",
      "Epoch 577, Loss: 1.7862728536128998, Final Batch Loss: 0.4745519459247589\n",
      "Epoch 578, Loss: 1.9378775358200073, Final Batch Loss: 0.560684859752655\n",
      "Epoch 579, Loss: 1.867988407611847, Final Batch Loss: 0.5824413895606995\n",
      "Epoch 580, Loss: 2.0323119163513184, Final Batch Loss: 0.6451133489608765\n",
      "Epoch 581, Loss: 1.9206135272979736, Final Batch Loss: 0.5342541337013245\n",
      "Epoch 582, Loss: 1.9627169370651245, Final Batch Loss: 0.7149841785430908\n",
      "Epoch 583, Loss: 1.8253365755081177, Final Batch Loss: 0.6074961423873901\n",
      "Epoch 584, Loss: 2.0133306980133057, Final Batch Loss: 0.7676652073860168\n",
      "Epoch 585, Loss: 1.8372888565063477, Final Batch Loss: 0.5539271235466003\n",
      "Epoch 586, Loss: 1.817861020565033, Final Batch Loss: 0.5014481544494629\n",
      "Epoch 587, Loss: 1.9184284806251526, Final Batch Loss: 0.6543867588043213\n",
      "Epoch 588, Loss: 1.9543019533157349, Final Batch Loss: 0.6875348687171936\n",
      "Epoch 589, Loss: 2.012703835964203, Final Batch Loss: 0.7093133330345154\n",
      "Epoch 590, Loss: 1.9495320916175842, Final Batch Loss: 0.6833674907684326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 591, Loss: 2.003218650817871, Final Batch Loss: 0.6273826360702515\n",
      "Epoch 592, Loss: 1.967721939086914, Final Batch Loss: 0.6463049054145813\n",
      "Epoch 593, Loss: 1.9505062103271484, Final Batch Loss: 0.6754606366157532\n",
      "Epoch 594, Loss: 1.9974892139434814, Final Batch Loss: 0.6009758114814758\n",
      "Epoch 595, Loss: 1.9727471470832825, Final Batch Loss: 0.6658316254615784\n",
      "Epoch 596, Loss: 1.8704244494438171, Final Batch Loss: 0.5966702699661255\n",
      "Epoch 597, Loss: 1.9823570847511292, Final Batch Loss: 0.5833512544631958\n",
      "Epoch 598, Loss: 2.0205695629119873, Final Batch Loss: 0.7651020884513855\n",
      "Epoch 599, Loss: 1.803687572479248, Final Batch Loss: 0.5665073394775391\n",
      "Epoch 600, Loss: 1.9058947563171387, Final Batch Loss: 0.5898243188858032\n",
      "Epoch 601, Loss: 1.898456633090973, Final Batch Loss: 0.5575721263885498\n",
      "Epoch 602, Loss: 1.8263522386550903, Final Batch Loss: 0.5312960147857666\n",
      "Epoch 603, Loss: 1.9868908524513245, Final Batch Loss: 0.6887505054473877\n",
      "Epoch 604, Loss: 1.9629116654396057, Final Batch Loss: 0.6273802518844604\n",
      "Epoch 605, Loss: 2.034461498260498, Final Batch Loss: 0.7110074162483215\n",
      "Epoch 606, Loss: 2.0393552780151367, Final Batch Loss: 0.747715413570404\n",
      "Epoch 607, Loss: 1.9212377667427063, Final Batch Loss: 0.5519499778747559\n",
      "Epoch 608, Loss: 1.8499473929405212, Final Batch Loss: 0.5445647835731506\n",
      "Epoch 609, Loss: 1.8849958777427673, Final Batch Loss: 0.5557588934898376\n",
      "Epoch 610, Loss: 1.9413703083992004, Final Batch Loss: 0.7489301562309265\n",
      "Epoch 611, Loss: 1.9547563791275024, Final Batch Loss: 0.6994330883026123\n",
      "Epoch 612, Loss: 1.9714990258216858, Final Batch Loss: 0.6600692272186279\n",
      "Epoch 613, Loss: 1.934377372264862, Final Batch Loss: 0.6108459234237671\n",
      "Epoch 614, Loss: 1.93364679813385, Final Batch Loss: 0.6270792484283447\n",
      "Epoch 615, Loss: 1.8647950887680054, Final Batch Loss: 0.6172154545783997\n",
      "Epoch 616, Loss: 1.9015498161315918, Final Batch Loss: 0.6695140600204468\n",
      "Epoch 617, Loss: 1.9241141080856323, Final Batch Loss: 0.6287930607795715\n",
      "Epoch 618, Loss: 1.9614851474761963, Final Batch Loss: 0.6123878955841064\n",
      "Epoch 619, Loss: 1.8736581802368164, Final Batch Loss: 0.5948584675788879\n",
      "Epoch 620, Loss: 1.7980332970619202, Final Batch Loss: 0.6282840967178345\n",
      "Epoch 621, Loss: 1.8599708080291748, Final Batch Loss: 0.6048534512519836\n",
      "Epoch 622, Loss: 1.9354546070098877, Final Batch Loss: 0.6211274266242981\n",
      "Epoch 623, Loss: 1.9279727339744568, Final Batch Loss: 0.7343170046806335\n",
      "Epoch 624, Loss: 1.9096782803535461, Final Batch Loss: 0.6334752440452576\n",
      "Epoch 625, Loss: 1.911767065525055, Final Batch Loss: 0.6296074986457825\n",
      "Epoch 626, Loss: 1.9819726347923279, Final Batch Loss: 0.6788092255592346\n",
      "Epoch 627, Loss: 1.971024215221405, Final Batch Loss: 0.6451780200004578\n",
      "Epoch 628, Loss: 1.912255883216858, Final Batch Loss: 0.5750701427459717\n",
      "Epoch 629, Loss: 1.8405430316925049, Final Batch Loss: 0.5702018141746521\n",
      "Epoch 630, Loss: 1.9046971201896667, Final Batch Loss: 0.600121796131134\n",
      "Epoch 631, Loss: 1.9661864042282104, Final Batch Loss: 0.754231870174408\n",
      "Epoch 632, Loss: 1.905376672744751, Final Batch Loss: 0.6530184745788574\n",
      "Epoch 633, Loss: 1.806169867515564, Final Batch Loss: 0.579917848110199\n",
      "Epoch 634, Loss: 1.9392059445381165, Final Batch Loss: 0.6445291042327881\n",
      "Epoch 635, Loss: 1.8030316233634949, Final Batch Loss: 0.5308353304862976\n",
      "Epoch 636, Loss: 1.8232635259628296, Final Batch Loss: 0.5578286647796631\n",
      "Epoch 637, Loss: 1.972430169582367, Final Batch Loss: 0.702603280544281\n",
      "Epoch 638, Loss: 1.9738736748695374, Final Batch Loss: 0.6450402140617371\n",
      "Epoch 639, Loss: 1.9309597611427307, Final Batch Loss: 0.6574610471725464\n",
      "Epoch 640, Loss: 1.8904098868370056, Final Batch Loss: 0.6729757189750671\n",
      "Epoch 641, Loss: 1.9008176922798157, Final Batch Loss: 0.6115238070487976\n",
      "Epoch 642, Loss: 1.8772087693214417, Final Batch Loss: 0.6208406686782837\n",
      "Epoch 643, Loss: 1.7922769784927368, Final Batch Loss: 0.49005818367004395\n",
      "Epoch 644, Loss: 1.9943548440933228, Final Batch Loss: 0.6682733297348022\n",
      "Epoch 645, Loss: 1.9131745100021362, Final Batch Loss: 0.657275915145874\n",
      "Epoch 646, Loss: 1.9463101029396057, Final Batch Loss: 0.7460882663726807\n",
      "Epoch 647, Loss: 1.9042332768440247, Final Batch Loss: 0.5956963300704956\n",
      "Epoch 648, Loss: 1.7909237146377563, Final Batch Loss: 0.6716453433036804\n",
      "Epoch 649, Loss: 1.8791508078575134, Final Batch Loss: 0.6084567904472351\n",
      "Epoch 650, Loss: 1.8296173810958862, Final Batch Loss: 0.5358996391296387\n",
      "Epoch 651, Loss: 1.989139199256897, Final Batch Loss: 0.6545543670654297\n",
      "Epoch 652, Loss: 1.8323136568069458, Final Batch Loss: 0.5804230570793152\n",
      "Epoch 653, Loss: 1.7782063484191895, Final Batch Loss: 0.510402500629425\n",
      "Epoch 654, Loss: 1.8854078650474548, Final Batch Loss: 0.5386812686920166\n",
      "Epoch 655, Loss: 1.8654240369796753, Final Batch Loss: 0.6290505528450012\n",
      "Epoch 656, Loss: 2.01349276304245, Final Batch Loss: 0.6395328640937805\n",
      "Epoch 657, Loss: 1.8196869492530823, Final Batch Loss: 0.658733606338501\n",
      "Epoch 658, Loss: 1.8298816680908203, Final Batch Loss: 0.5482328534126282\n",
      "Epoch 659, Loss: 1.865946114063263, Final Batch Loss: 0.6494839191436768\n",
      "Epoch 660, Loss: 1.8852943778038025, Final Batch Loss: 0.5448215007781982\n",
      "Epoch 661, Loss: 1.9469497799873352, Final Batch Loss: 0.6469877362251282\n",
      "Epoch 662, Loss: 1.8625844717025757, Final Batch Loss: 0.5841042399406433\n",
      "Epoch 663, Loss: 1.8030633330345154, Final Batch Loss: 0.6467570662498474\n",
      "Epoch 664, Loss: 1.8570786118507385, Final Batch Loss: 0.4984908699989319\n",
      "Epoch 665, Loss: 1.7466983795166016, Final Batch Loss: 0.5462155342102051\n",
      "Epoch 666, Loss: 1.9260779023170471, Final Batch Loss: 0.7204477190971375\n",
      "Epoch 667, Loss: 1.7793076634407043, Final Batch Loss: 0.5785347819328308\n",
      "Epoch 668, Loss: 1.7429430484771729, Final Batch Loss: 0.5990738272666931\n",
      "Epoch 669, Loss: 1.9893790483474731, Final Batch Loss: 0.6721089482307434\n",
      "Epoch 670, Loss: 1.8612061738967896, Final Batch Loss: 0.5572017431259155\n",
      "Epoch 671, Loss: 1.8979164361953735, Final Batch Loss: 0.7604179978370667\n",
      "Epoch 672, Loss: 1.8414572477340698, Final Batch Loss: 0.5712780952453613\n",
      "Epoch 673, Loss: 1.8586488962173462, Final Batch Loss: 0.5471174716949463\n",
      "Epoch 674, Loss: 2.022790789604187, Final Batch Loss: 0.6796796321868896\n",
      "Epoch 675, Loss: 1.9618269205093384, Final Batch Loss: 0.7157374620437622\n",
      "Epoch 676, Loss: 1.8832554817199707, Final Batch Loss: 0.535148024559021\n",
      "Epoch 677, Loss: 1.9150723814964294, Final Batch Loss: 0.7390741109848022\n",
      "Epoch 678, Loss: 1.8765401244163513, Final Batch Loss: 0.5999463796615601\n",
      "Epoch 679, Loss: 1.8332301378250122, Final Batch Loss: 0.5476394891738892\n",
      "Epoch 680, Loss: 1.9354812502861023, Final Batch Loss: 0.6685383915901184\n",
      "Epoch 681, Loss: 1.8925772309303284, Final Batch Loss: 0.6675268411636353\n",
      "Epoch 682, Loss: 1.7458229064941406, Final Batch Loss: 0.5640870332717896\n",
      "Epoch 683, Loss: 1.802726149559021, Final Batch Loss: 0.6022898554801941\n",
      "Epoch 684, Loss: 1.9051984548568726, Final Batch Loss: 0.762603223323822\n",
      "Epoch 685, Loss: 1.976963758468628, Final Batch Loss: 0.7970180511474609\n",
      "Epoch 686, Loss: 1.8804541230201721, Final Batch Loss: 0.6975695490837097\n",
      "Epoch 687, Loss: 1.8948540687561035, Final Batch Loss: 0.668117880821228\n",
      "Epoch 688, Loss: 1.7991751730442047, Final Batch Loss: 0.6055600643157959\n",
      "Epoch 689, Loss: 1.8642045855522156, Final Batch Loss: 0.6431499719619751\n",
      "Epoch 690, Loss: 1.8725310564041138, Final Batch Loss: 0.5647141933441162\n",
      "Epoch 691, Loss: 1.9169800877571106, Final Batch Loss: 0.745273768901825\n",
      "Epoch 692, Loss: 1.9266597628593445, Final Batch Loss: 0.6993586421012878\n",
      "Epoch 693, Loss: 1.848451018333435, Final Batch Loss: 0.6346534490585327\n",
      "Epoch 694, Loss: 1.795259416103363, Final Batch Loss: 0.5633593797683716\n",
      "Epoch 695, Loss: 1.9021454453468323, Final Batch Loss: 0.768108069896698\n",
      "Epoch 696, Loss: 1.8038009405136108, Final Batch Loss: 0.5091127753257751\n",
      "Epoch 697, Loss: 1.8281575441360474, Final Batch Loss: 0.6044536232948303\n",
      "Epoch 698, Loss: 1.9484285116195679, Final Batch Loss: 0.6797893643379211\n",
      "Epoch 699, Loss: 1.7309197783470154, Final Batch Loss: 0.517991840839386\n",
      "Epoch 700, Loss: 1.8501689434051514, Final Batch Loss: 0.6459429264068604\n",
      "Epoch 701, Loss: 1.804010033607483, Final Batch Loss: 0.631669819355011\n",
      "Epoch 702, Loss: 1.8698513507843018, Final Batch Loss: 0.65282142162323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 703, Loss: 1.9140849709510803, Final Batch Loss: 0.6705939173698425\n",
      "Epoch 704, Loss: 1.9287217259407043, Final Batch Loss: 0.6540219187736511\n",
      "Epoch 705, Loss: 1.8464373350143433, Final Batch Loss: 0.5625162720680237\n",
      "Epoch 706, Loss: 1.741518259048462, Final Batch Loss: 0.5357884168624878\n",
      "Epoch 707, Loss: 1.905007541179657, Final Batch Loss: 0.680324137210846\n",
      "Epoch 708, Loss: 1.7527890801429749, Final Batch Loss: 0.5167347192764282\n",
      "Epoch 709, Loss: 1.9516916275024414, Final Batch Loss: 0.7210370302200317\n",
      "Epoch 710, Loss: 1.8781087398529053, Final Batch Loss: 0.6081765294075012\n",
      "Epoch 711, Loss: 1.9337509274482727, Final Batch Loss: 0.7022164463996887\n",
      "Epoch 712, Loss: 1.87209153175354, Final Batch Loss: 0.6407055258750916\n",
      "Epoch 713, Loss: 1.8055580258369446, Final Batch Loss: 0.5858380198478699\n",
      "Epoch 714, Loss: 1.7935259342193604, Final Batch Loss: 0.6056904792785645\n",
      "Epoch 715, Loss: 1.8704957962036133, Final Batch Loss: 0.6068050265312195\n",
      "Epoch 716, Loss: 1.792637586593628, Final Batch Loss: 0.583727240562439\n",
      "Epoch 717, Loss: 1.8166184425354004, Final Batch Loss: 0.6167820692062378\n",
      "Epoch 718, Loss: 1.8610699772834778, Final Batch Loss: 0.6171160936355591\n",
      "Epoch 719, Loss: 1.8617121577262878, Final Batch Loss: 0.6546235084533691\n",
      "Epoch 720, Loss: 1.9089946746826172, Final Batch Loss: 0.5798295736312866\n",
      "Epoch 721, Loss: 1.8180208206176758, Final Batch Loss: 0.5513647794723511\n",
      "Epoch 722, Loss: 1.7204940915107727, Final Batch Loss: 0.5631918907165527\n",
      "Epoch 723, Loss: 1.8097901940345764, Final Batch Loss: 0.587404727935791\n",
      "Epoch 724, Loss: 1.832678496837616, Final Batch Loss: 0.5864701271057129\n",
      "Epoch 725, Loss: 1.8165333271026611, Final Batch Loss: 0.6422675251960754\n",
      "Epoch 726, Loss: 1.9757894277572632, Final Batch Loss: 0.6863102912902832\n",
      "Epoch 727, Loss: 1.8261120915412903, Final Batch Loss: 0.6000749468803406\n",
      "Epoch 728, Loss: 1.7879393696784973, Final Batch Loss: 0.6305773854255676\n",
      "Epoch 729, Loss: 1.8220720291137695, Final Batch Loss: 0.6103481650352478\n",
      "Epoch 730, Loss: 1.917191505432129, Final Batch Loss: 0.6852366924285889\n",
      "Epoch 731, Loss: 1.8719374537467957, Final Batch Loss: 0.651835024356842\n",
      "Epoch 732, Loss: 1.909721553325653, Final Batch Loss: 0.6660868525505066\n",
      "Epoch 733, Loss: 1.8258842825889587, Final Batch Loss: 0.6548693180084229\n",
      "Epoch 734, Loss: 1.8796045184135437, Final Batch Loss: 0.6498314738273621\n",
      "Epoch 735, Loss: 1.8866996765136719, Final Batch Loss: 0.6176076531410217\n",
      "Epoch 736, Loss: 1.8170654773712158, Final Batch Loss: 0.6342555284500122\n",
      "Epoch 737, Loss: 1.9007208943367004, Final Batch Loss: 0.5873015522956848\n",
      "Epoch 738, Loss: 1.722486972808838, Final Batch Loss: 0.5542603135108948\n",
      "Epoch 739, Loss: 1.7836166024208069, Final Batch Loss: 0.6170522570610046\n",
      "Epoch 740, Loss: 1.7287775874137878, Final Batch Loss: 0.6163172721862793\n",
      "Epoch 741, Loss: 1.7448945045471191, Final Batch Loss: 0.6175966858863831\n",
      "Epoch 742, Loss: 1.9369240999221802, Final Batch Loss: 0.7071197032928467\n",
      "Epoch 743, Loss: 1.8880849480628967, Final Batch Loss: 0.6512671709060669\n",
      "Epoch 744, Loss: 1.729174792766571, Final Batch Loss: 0.5742405652999878\n",
      "Epoch 745, Loss: 1.8941243290901184, Final Batch Loss: 0.6261569261550903\n",
      "Epoch 746, Loss: 1.8009988069534302, Final Batch Loss: 0.5458956360816956\n",
      "Epoch 747, Loss: 1.851279854774475, Final Batch Loss: 0.5835420489311218\n",
      "Epoch 748, Loss: 1.898878574371338, Final Batch Loss: 0.577004075050354\n",
      "Epoch 749, Loss: 1.8222535252571106, Final Batch Loss: 0.6488218903541565\n",
      "Epoch 750, Loss: 1.74718177318573, Final Batch Loss: 0.5767927765846252\n",
      "Epoch 751, Loss: 1.7770676016807556, Final Batch Loss: 0.5973585844039917\n",
      "Epoch 752, Loss: 1.9106353521347046, Final Batch Loss: 0.6736170649528503\n",
      "Epoch 753, Loss: 1.9040610790252686, Final Batch Loss: 0.6499868035316467\n",
      "Epoch 754, Loss: 1.7029539942741394, Final Batch Loss: 0.5464724898338318\n",
      "Epoch 755, Loss: 1.7178984880447388, Final Batch Loss: 0.5597682595252991\n",
      "Epoch 756, Loss: 1.780362218618393, Final Batch Loss: 0.47023507952690125\n",
      "Epoch 757, Loss: 1.7726741433143616, Final Batch Loss: 0.5217946767807007\n",
      "Epoch 758, Loss: 1.8036898970603943, Final Batch Loss: 0.6503962278366089\n",
      "Epoch 759, Loss: 1.8067889213562012, Final Batch Loss: 0.6945943236351013\n",
      "Epoch 760, Loss: 1.7926493287086487, Final Batch Loss: 0.6003667116165161\n",
      "Epoch 761, Loss: 1.7893024682998657, Final Batch Loss: 0.6227191686630249\n",
      "Epoch 762, Loss: 1.8756446838378906, Final Batch Loss: 0.5729594230651855\n",
      "Epoch 763, Loss: 1.8285616636276245, Final Batch Loss: 0.5737454295158386\n",
      "Epoch 764, Loss: 1.8469574451446533, Final Batch Loss: 0.5625406503677368\n",
      "Epoch 765, Loss: 1.8687928318977356, Final Batch Loss: 0.5625317096710205\n",
      "Epoch 766, Loss: 1.8658655285835266, Final Batch Loss: 0.6482154130935669\n",
      "Epoch 767, Loss: 1.8342066407203674, Final Batch Loss: 0.642577588558197\n",
      "Epoch 768, Loss: 1.764447271823883, Final Batch Loss: 0.4967234134674072\n",
      "Epoch 769, Loss: 1.8117082118988037, Final Batch Loss: 0.5559591054916382\n",
      "Epoch 770, Loss: 1.7500914335250854, Final Batch Loss: 0.5045639872550964\n",
      "Epoch 771, Loss: 1.8072842955589294, Final Batch Loss: 0.6231293678283691\n",
      "Epoch 772, Loss: 1.7499257326126099, Final Batch Loss: 0.5279156565666199\n",
      "Epoch 773, Loss: 1.6648755967617035, Final Batch Loss: 0.4788025915622711\n",
      "Epoch 774, Loss: 1.768718808889389, Final Batch Loss: 0.4839186370372772\n",
      "Epoch 775, Loss: 1.7399457097053528, Final Batch Loss: 0.5372583270072937\n",
      "Epoch 776, Loss: 1.8483806252479553, Final Batch Loss: 0.6816439032554626\n",
      "Epoch 777, Loss: 1.7446017265319824, Final Batch Loss: 0.5389696955680847\n",
      "Epoch 778, Loss: 1.6821032762527466, Final Batch Loss: 0.5153440833091736\n",
      "Epoch 779, Loss: 1.886930227279663, Final Batch Loss: 0.7225606441497803\n",
      "Epoch 780, Loss: 1.685524582862854, Final Batch Loss: 0.5107924938201904\n",
      "Epoch 781, Loss: 1.8247342705726624, Final Batch Loss: 0.6529714465141296\n",
      "Epoch 782, Loss: 1.829149454832077, Final Batch Loss: 0.6271854043006897\n",
      "Epoch 783, Loss: 1.8720099329948425, Final Batch Loss: 0.6549667119979858\n",
      "Epoch 784, Loss: 1.6732389628887177, Final Batch Loss: 0.454563707113266\n",
      "Epoch 785, Loss: 1.8083257675170898, Final Batch Loss: 0.5400465130805969\n",
      "Epoch 786, Loss: 1.729531705379486, Final Batch Loss: 0.589610755443573\n",
      "Epoch 787, Loss: 1.7292518615722656, Final Batch Loss: 0.5966153740882874\n",
      "Epoch 788, Loss: 1.7147005200386047, Final Batch Loss: 0.5935898423194885\n",
      "Epoch 789, Loss: 1.746852457523346, Final Batch Loss: 0.5426660180091858\n",
      "Epoch 790, Loss: 1.7364476919174194, Final Batch Loss: 0.5904561281204224\n",
      "Epoch 791, Loss: 1.8862176537513733, Final Batch Loss: 0.7272753715515137\n",
      "Epoch 792, Loss: 1.8174076676368713, Final Batch Loss: 0.6154083609580994\n",
      "Epoch 793, Loss: 1.878151535987854, Final Batch Loss: 0.5938339233398438\n",
      "Epoch 794, Loss: 1.7828277945518494, Final Batch Loss: 0.6511278748512268\n",
      "Epoch 795, Loss: 1.7250739932060242, Final Batch Loss: 0.6457783579826355\n",
      "Epoch 796, Loss: 1.7605515122413635, Final Batch Loss: 0.5399317741394043\n",
      "Epoch 797, Loss: 1.830527901649475, Final Batch Loss: 0.6451471447944641\n",
      "Epoch 798, Loss: 1.8635751008987427, Final Batch Loss: 0.6404668688774109\n",
      "Epoch 799, Loss: 1.6990373730659485, Final Batch Loss: 0.550240159034729\n",
      "Epoch 800, Loss: 1.784794270992279, Final Batch Loss: 0.6396762132644653\n",
      "Epoch 801, Loss: 1.6293355822563171, Final Batch Loss: 0.4707125425338745\n",
      "Epoch 802, Loss: 1.7429569363594055, Final Batch Loss: 0.597070038318634\n",
      "Epoch 803, Loss: 1.739671766757965, Final Batch Loss: 0.5792973041534424\n",
      "Epoch 804, Loss: 1.8454111218452454, Final Batch Loss: 0.6372756361961365\n",
      "Epoch 805, Loss: 1.8654652833938599, Final Batch Loss: 0.6477450728416443\n",
      "Epoch 806, Loss: 1.70113605260849, Final Batch Loss: 0.5440482497215271\n",
      "Epoch 807, Loss: 1.8165954947471619, Final Batch Loss: 0.5910181999206543\n",
      "Epoch 808, Loss: 1.7967770099639893, Final Batch Loss: 0.62043297290802\n",
      "Epoch 809, Loss: 1.7869285941123962, Final Batch Loss: 0.5325005650520325\n",
      "Epoch 810, Loss: 1.739097774028778, Final Batch Loss: 0.5023308992385864\n",
      "Epoch 811, Loss: 1.7058236002922058, Final Batch Loss: 0.5093740224838257\n",
      "Epoch 812, Loss: 1.7973975539207458, Final Batch Loss: 0.5402988791465759\n",
      "Epoch 813, Loss: 1.6654217839241028, Final Batch Loss: 0.46626824140548706\n",
      "Epoch 814, Loss: 1.7744324803352356, Final Batch Loss: 0.6364859938621521\n",
      "Epoch 815, Loss: 1.789329171180725, Final Batch Loss: 0.6184149384498596\n",
      "Epoch 816, Loss: 1.8474054336547852, Final Batch Loss: 0.6643524169921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 817, Loss: 1.7838906645774841, Final Batch Loss: 0.5333042144775391\n",
      "Epoch 818, Loss: 1.863203763961792, Final Batch Loss: 0.6187549829483032\n",
      "Epoch 819, Loss: 1.688971757888794, Final Batch Loss: 0.5216308832168579\n",
      "Epoch 820, Loss: 1.810042142868042, Final Batch Loss: 0.6076733469963074\n",
      "Epoch 821, Loss: 1.629643440246582, Final Batch Loss: 0.5566657185554504\n",
      "Epoch 822, Loss: 1.7006301879882812, Final Batch Loss: 0.5169587135314941\n",
      "Epoch 823, Loss: 1.617156058549881, Final Batch Loss: 0.46423789858818054\n",
      "Epoch 824, Loss: 1.8140574097633362, Final Batch Loss: 0.6623001098632812\n",
      "Epoch 825, Loss: 1.7356840372085571, Final Batch Loss: 0.6338241100311279\n",
      "Epoch 826, Loss: 1.7253955602645874, Final Batch Loss: 0.6456999778747559\n",
      "Epoch 827, Loss: 1.7539731860160828, Final Batch Loss: 0.5552443265914917\n",
      "Epoch 828, Loss: 1.8701585531234741, Final Batch Loss: 0.6417496800422668\n",
      "Epoch 829, Loss: 1.6761547923088074, Final Batch Loss: 0.497205913066864\n",
      "Epoch 830, Loss: 1.697570025920868, Final Batch Loss: 0.5109575390815735\n",
      "Epoch 831, Loss: 1.6987431645393372, Final Batch Loss: 0.6008173823356628\n",
      "Epoch 832, Loss: 1.8235043287277222, Final Batch Loss: 0.6420083045959473\n",
      "Epoch 833, Loss: 1.8419110178947449, Final Batch Loss: 0.681688666343689\n",
      "Epoch 834, Loss: 1.6634816527366638, Final Batch Loss: 0.5954334735870361\n",
      "Epoch 835, Loss: 1.8345279097557068, Final Batch Loss: 0.6245101690292358\n",
      "Epoch 836, Loss: 1.8032801151275635, Final Batch Loss: 0.5767692923545837\n",
      "Epoch 837, Loss: 1.7664958834648132, Final Batch Loss: 0.5794813632965088\n",
      "Epoch 838, Loss: 1.7808284163475037, Final Batch Loss: 0.5646586418151855\n",
      "Epoch 839, Loss: 1.836439311504364, Final Batch Loss: 0.6688534617424011\n",
      "Epoch 840, Loss: 1.7350226044654846, Final Batch Loss: 0.5818155407905579\n",
      "Epoch 841, Loss: 1.6737393736839294, Final Batch Loss: 0.5288771986961365\n",
      "Epoch 842, Loss: 1.724044382572174, Final Batch Loss: 0.5647944808006287\n",
      "Epoch 843, Loss: 1.7154119610786438, Final Batch Loss: 0.6161434650421143\n",
      "Epoch 844, Loss: 1.8735327124595642, Final Batch Loss: 0.7242160439491272\n",
      "Epoch 845, Loss: 1.7355065941810608, Final Batch Loss: 0.5404508709907532\n",
      "Epoch 846, Loss: 1.8646417260169983, Final Batch Loss: 0.6399180889129639\n",
      "Epoch 847, Loss: 1.7264116406440735, Final Batch Loss: 0.5951700806617737\n",
      "Epoch 848, Loss: 1.9523524641990662, Final Batch Loss: 0.7227575182914734\n",
      "Epoch 849, Loss: 1.7134122848510742, Final Batch Loss: 0.49508482217788696\n",
      "Epoch 850, Loss: 1.796004831790924, Final Batch Loss: 0.6897875666618347\n",
      "Epoch 851, Loss: 1.723673164844513, Final Batch Loss: 0.5374704599380493\n",
      "Epoch 852, Loss: 1.7182689309120178, Final Batch Loss: 0.5209130644798279\n",
      "Epoch 853, Loss: 1.68209308385849, Final Batch Loss: 0.5317474007606506\n",
      "Epoch 854, Loss: 1.7816507816314697, Final Batch Loss: 0.5847731232643127\n",
      "Epoch 855, Loss: 1.691128134727478, Final Batch Loss: 0.5842032432556152\n",
      "Epoch 856, Loss: 1.6591712534427643, Final Batch Loss: 0.6118118762969971\n",
      "Epoch 857, Loss: 1.6898924112319946, Final Batch Loss: 0.4641708731651306\n",
      "Epoch 858, Loss: 1.8035158514976501, Final Batch Loss: 0.578619658946991\n",
      "Epoch 859, Loss: 1.808348000049591, Final Batch Loss: 0.6806992888450623\n",
      "Epoch 860, Loss: 1.6958505511283875, Final Batch Loss: 0.5792730450630188\n",
      "Epoch 861, Loss: 1.5692200064659119, Final Batch Loss: 0.43320560455322266\n",
      "Epoch 862, Loss: 1.7224846482276917, Final Batch Loss: 0.5986257195472717\n",
      "Epoch 863, Loss: 1.7041152715682983, Final Batch Loss: 0.49855226278305054\n",
      "Epoch 864, Loss: 1.7308186888694763, Final Batch Loss: 0.5881090760231018\n",
      "Epoch 865, Loss: 1.8118783235549927, Final Batch Loss: 0.6814677119255066\n",
      "Epoch 866, Loss: 1.7139994502067566, Final Batch Loss: 0.517688512802124\n",
      "Epoch 867, Loss: 1.6889907121658325, Final Batch Loss: 0.5546938180923462\n",
      "Epoch 868, Loss: 1.7224149107933044, Final Batch Loss: 0.5527364015579224\n",
      "Epoch 869, Loss: 1.7352440357208252, Final Batch Loss: 0.5900943279266357\n",
      "Epoch 870, Loss: 1.7357838153839111, Final Batch Loss: 0.5529202222824097\n",
      "Epoch 871, Loss: 1.7011802196502686, Final Batch Loss: 0.633195698261261\n",
      "Epoch 872, Loss: 1.688443273305893, Final Batch Loss: 0.4836396872997284\n",
      "Epoch 873, Loss: 1.684228539466858, Final Batch Loss: 0.49700164794921875\n",
      "Epoch 874, Loss: 1.7836118936538696, Final Batch Loss: 0.6009738445281982\n",
      "Epoch 875, Loss: 1.820243000984192, Final Batch Loss: 0.6276599168777466\n",
      "Epoch 876, Loss: 1.7713326811790466, Final Batch Loss: 0.6242577433586121\n",
      "Epoch 877, Loss: 1.656715452671051, Final Batch Loss: 0.577262282371521\n",
      "Epoch 878, Loss: 1.7300724387168884, Final Batch Loss: 0.5632440447807312\n",
      "Epoch 879, Loss: 1.6457059979438782, Final Batch Loss: 0.5379537343978882\n",
      "Epoch 880, Loss: 1.79731023311615, Final Batch Loss: 0.6795953512191772\n",
      "Epoch 881, Loss: 1.7351056337356567, Final Batch Loss: 0.6236612796783447\n",
      "Epoch 882, Loss: 1.7822723984718323, Final Batch Loss: 0.604026734828949\n",
      "Epoch 883, Loss: 1.8130912780761719, Final Batch Loss: 0.5970798134803772\n",
      "Epoch 884, Loss: 1.6633142232894897, Final Batch Loss: 0.5552011132240295\n",
      "Epoch 885, Loss: 1.638241946697235, Final Batch Loss: 0.5070213675498962\n",
      "Epoch 886, Loss: 1.7240711450576782, Final Batch Loss: 0.5238103270530701\n",
      "Epoch 887, Loss: 1.6807989180088043, Final Batch Loss: 0.49463459849357605\n",
      "Epoch 888, Loss: 1.738027572631836, Final Batch Loss: 0.5842640995979309\n",
      "Epoch 889, Loss: 1.7065329551696777, Final Batch Loss: 0.5067521333694458\n",
      "Epoch 890, Loss: 1.7658848762512207, Final Batch Loss: 0.60484379529953\n",
      "Epoch 891, Loss: 1.6074929535388947, Final Batch Loss: 0.5199646353721619\n",
      "Epoch 892, Loss: 1.7167099714279175, Final Batch Loss: 0.5868580937385559\n",
      "Epoch 893, Loss: 1.744626224040985, Final Batch Loss: 0.6581529974937439\n",
      "Epoch 894, Loss: 1.8502201437950134, Final Batch Loss: 0.7200397253036499\n",
      "Epoch 895, Loss: 1.714383602142334, Final Batch Loss: 0.6344560384750366\n",
      "Epoch 896, Loss: 1.8072054386138916, Final Batch Loss: 0.7724230885505676\n",
      "Epoch 897, Loss: 1.681277334690094, Final Batch Loss: 0.5322281718254089\n",
      "Epoch 898, Loss: 1.8128591179847717, Final Batch Loss: 0.6746523976325989\n",
      "Epoch 899, Loss: 1.6042636036872864, Final Batch Loss: 0.5492913722991943\n",
      "Epoch 900, Loss: 1.651233732700348, Final Batch Loss: 0.4643034338951111\n",
      "Epoch 901, Loss: 1.7818446159362793, Final Batch Loss: 0.5641418099403381\n",
      "Epoch 902, Loss: 1.6385785341262817, Final Batch Loss: 0.5248242616653442\n",
      "Epoch 903, Loss: 1.6181638836860657, Final Batch Loss: 0.4945332407951355\n",
      "Epoch 904, Loss: 1.6523425579071045, Final Batch Loss: 0.5059994459152222\n",
      "Epoch 905, Loss: 1.731097161769867, Final Batch Loss: 0.6513456702232361\n",
      "Epoch 906, Loss: 1.7219803929328918, Final Batch Loss: 0.6022545099258423\n",
      "Epoch 907, Loss: 1.6789366006851196, Final Batch Loss: 0.5417163372039795\n",
      "Epoch 908, Loss: 1.7181639671325684, Final Batch Loss: 0.5390090346336365\n",
      "Epoch 909, Loss: 1.7209096550941467, Final Batch Loss: 0.6118512153625488\n",
      "Epoch 910, Loss: 1.7869693636894226, Final Batch Loss: 0.656445324420929\n",
      "Epoch 911, Loss: 1.6720450520515442, Final Batch Loss: 0.6155967116355896\n",
      "Epoch 912, Loss: 1.631580501794815, Final Batch Loss: 0.5495216250419617\n",
      "Epoch 913, Loss: 1.6456395983695984, Final Batch Loss: 0.5352252125740051\n",
      "Epoch 914, Loss: 1.8819296956062317, Final Batch Loss: 0.6938029527664185\n",
      "Epoch 915, Loss: 1.6984099745750427, Final Batch Loss: 0.48392194509506226\n",
      "Epoch 916, Loss: 1.68656724691391, Final Batch Loss: 0.5726651549339294\n",
      "Epoch 917, Loss: 1.7114315629005432, Final Batch Loss: 0.6094181537628174\n",
      "Epoch 918, Loss: 1.7063099145889282, Final Batch Loss: 0.5976639986038208\n",
      "Epoch 919, Loss: 1.6980603337287903, Final Batch Loss: 0.5585370063781738\n",
      "Epoch 920, Loss: 1.8034366965293884, Final Batch Loss: 0.654232919216156\n",
      "Epoch 921, Loss: 1.6657772660255432, Final Batch Loss: 0.5333936214447021\n",
      "Epoch 922, Loss: 1.64590185880661, Final Batch Loss: 0.5193428993225098\n",
      "Epoch 923, Loss: 1.7272161841392517, Final Batch Loss: 0.6286149024963379\n",
      "Epoch 924, Loss: 1.7317872643470764, Final Batch Loss: 0.5951313972473145\n",
      "Epoch 925, Loss: 1.6660020351409912, Final Batch Loss: 0.5615442395210266\n",
      "Epoch 926, Loss: 1.7067362070083618, Final Batch Loss: 0.5758057832717896\n",
      "Epoch 927, Loss: 1.6654778122901917, Final Batch Loss: 0.6399524211883545\n",
      "Epoch 928, Loss: 1.5708352029323578, Final Batch Loss: 0.4147126376628876\n",
      "Epoch 929, Loss: 1.7189536690711975, Final Batch Loss: 0.5109111666679382\n",
      "Epoch 930, Loss: 1.7110270261764526, Final Batch Loss: 0.5973846912384033\n",
      "Epoch 931, Loss: 1.7128701210021973, Final Batch Loss: 0.5947425961494446\n",
      "Epoch 932, Loss: 1.7597593665122986, Final Batch Loss: 0.5941345691680908\n",
      "Epoch 933, Loss: 1.7015204429626465, Final Batch Loss: 0.5331242680549622\n",
      "Epoch 934, Loss: 1.6483502686023712, Final Batch Loss: 0.4752456843852997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 935, Loss: 1.7127403616905212, Final Batch Loss: 0.558320164680481\n",
      "Epoch 936, Loss: 1.7730284929275513, Final Batch Loss: 0.7135956883430481\n",
      "Epoch 937, Loss: 1.746475338935852, Final Batch Loss: 0.558392345905304\n",
      "Epoch 938, Loss: 1.7074007391929626, Final Batch Loss: 0.6332675814628601\n",
      "Epoch 939, Loss: 1.714130938053131, Final Batch Loss: 0.5426071882247925\n",
      "Epoch 940, Loss: 1.630406677722931, Final Batch Loss: 0.5385478138923645\n",
      "Epoch 941, Loss: 1.7109392881393433, Final Batch Loss: 0.6685203909873962\n",
      "Epoch 942, Loss: 1.6466946005821228, Final Batch Loss: 0.5864050388336182\n",
      "Epoch 943, Loss: 1.651004135608673, Final Batch Loss: 0.5830458402633667\n",
      "Epoch 944, Loss: 1.6825930774211884, Final Batch Loss: 0.6547259092330933\n",
      "Epoch 945, Loss: 1.5984808504581451, Final Batch Loss: 0.4905037581920624\n",
      "Epoch 946, Loss: 1.6926530599594116, Final Batch Loss: 0.5882996320724487\n",
      "Epoch 947, Loss: 1.656865656375885, Final Batch Loss: 0.49675339460372925\n",
      "Epoch 948, Loss: 1.6952839493751526, Final Batch Loss: 0.6171314120292664\n",
      "Epoch 949, Loss: 1.6895610094070435, Final Batch Loss: 0.508284330368042\n",
      "Epoch 950, Loss: 1.7558806538581848, Final Batch Loss: 0.6292291283607483\n",
      "Epoch 951, Loss: 1.7768003940582275, Final Batch Loss: 0.6548207998275757\n",
      "Epoch 952, Loss: 1.680719405412674, Final Batch Loss: 0.49671831727027893\n",
      "Epoch 953, Loss: 1.7500897645950317, Final Batch Loss: 0.5253645777702332\n",
      "Epoch 954, Loss: 1.79140967130661, Final Batch Loss: 0.6026930809020996\n",
      "Epoch 955, Loss: 1.6519715785980225, Final Batch Loss: 0.5568916201591492\n",
      "Epoch 956, Loss: 1.6400325894355774, Final Batch Loss: 0.5925885438919067\n",
      "Epoch 957, Loss: 1.7096038460731506, Final Batch Loss: 0.6518712639808655\n",
      "Epoch 958, Loss: 1.7200538516044617, Final Batch Loss: 0.592820942401886\n",
      "Epoch 959, Loss: 1.709700107574463, Final Batch Loss: 0.5967238545417786\n",
      "Epoch 960, Loss: 1.4847941398620605, Final Batch Loss: 0.45159679651260376\n",
      "Epoch 961, Loss: 1.73092520236969, Final Batch Loss: 0.5172358751296997\n",
      "Epoch 962, Loss: 1.6138697862625122, Final Batch Loss: 0.559739887714386\n",
      "Epoch 963, Loss: 1.5634761154651642, Final Batch Loss: 0.4846327602863312\n",
      "Epoch 964, Loss: 1.5565354228019714, Final Batch Loss: 0.42965757846832275\n",
      "Epoch 965, Loss: 1.6904041767120361, Final Batch Loss: 0.5499275922775269\n",
      "Epoch 966, Loss: 1.6604132056236267, Final Batch Loss: 0.5042916536331177\n",
      "Epoch 967, Loss: 1.726434588432312, Final Batch Loss: 0.6039769649505615\n",
      "Epoch 968, Loss: 1.5831563174724579, Final Batch Loss: 0.46242788434028625\n",
      "Epoch 969, Loss: 1.763917088508606, Final Batch Loss: 0.598953366279602\n",
      "Epoch 970, Loss: 1.6589487493038177, Final Batch Loss: 0.4984031617641449\n",
      "Epoch 971, Loss: 1.6232241988182068, Final Batch Loss: 0.5524832010269165\n",
      "Epoch 972, Loss: 1.5990245342254639, Final Batch Loss: 0.4984355568885803\n",
      "Epoch 973, Loss: 1.6570478677749634, Final Batch Loss: 0.5808117389678955\n",
      "Epoch 974, Loss: 1.713939368724823, Final Batch Loss: 0.5613652467727661\n",
      "Epoch 975, Loss: 1.760504961013794, Final Batch Loss: 0.5166202783584595\n",
      "Epoch 976, Loss: 1.7158101201057434, Final Batch Loss: 0.5559875965118408\n",
      "Epoch 977, Loss: 1.6215329766273499, Final Batch Loss: 0.5617640614509583\n",
      "Epoch 978, Loss: 1.646289885044098, Final Batch Loss: 0.5624285340309143\n",
      "Epoch 979, Loss: 1.6729193329811096, Final Batch Loss: 0.5995690226554871\n",
      "Epoch 980, Loss: 1.6832337081432343, Final Batch Loss: 0.5918852090835571\n",
      "Epoch 981, Loss: 1.6238430738449097, Final Batch Loss: 0.49148041009902954\n",
      "Epoch 982, Loss: 1.599502682685852, Final Batch Loss: 0.5275363922119141\n",
      "Epoch 983, Loss: 1.6863868236541748, Final Batch Loss: 0.5882716774940491\n",
      "Epoch 984, Loss: 1.6729329228401184, Final Batch Loss: 0.5063666105270386\n",
      "Epoch 985, Loss: 1.7370909452438354, Final Batch Loss: 0.6036794781684875\n",
      "Epoch 986, Loss: 1.7759263515472412, Final Batch Loss: 0.6357397437095642\n",
      "Epoch 987, Loss: 1.5583254396915436, Final Batch Loss: 0.46638593077659607\n",
      "Epoch 988, Loss: 1.7194864153862, Final Batch Loss: 0.5639530420303345\n",
      "Epoch 989, Loss: 1.6231534481048584, Final Batch Loss: 0.5135778188705444\n",
      "Epoch 990, Loss: 1.5966196060180664, Final Batch Loss: 0.5043328404426575\n",
      "Epoch 991, Loss: 1.7228266596794128, Final Batch Loss: 0.5771018862724304\n",
      "Epoch 992, Loss: 1.6705538034439087, Final Batch Loss: 0.5872835516929626\n",
      "Epoch 993, Loss: 1.6972628235816956, Final Batch Loss: 0.6247295141220093\n",
      "Epoch 994, Loss: 1.6526533961296082, Final Batch Loss: 0.6042095422744751\n",
      "Epoch 995, Loss: 1.688558578491211, Final Batch Loss: 0.5480702519416809\n",
      "Epoch 996, Loss: 1.6449506282806396, Final Batch Loss: 0.5406466126441956\n",
      "Epoch 997, Loss: 1.6951733827590942, Final Batch Loss: 0.5616772174835205\n",
      "Epoch 998, Loss: 1.6768088340759277, Final Batch Loss: 0.6118658781051636\n",
      "Epoch 999, Loss: 1.567398488521576, Final Batch Loss: 0.47625261545181274\n",
      "Epoch 1000, Loss: 1.563859224319458, Final Batch Loss: 0.5277361869812012\n",
      "Epoch 1001, Loss: 1.6172438859939575, Final Batch Loss: 0.5432501435279846\n",
      "Epoch 1002, Loss: 1.6070497930049896, Final Batch Loss: 0.4799763262271881\n",
      "Epoch 1003, Loss: 1.8112857937812805, Final Batch Loss: 0.7182716131210327\n",
      "Epoch 1004, Loss: 1.5831860899925232, Final Batch Loss: 0.513274073600769\n",
      "Epoch 1005, Loss: 1.659395158290863, Final Batch Loss: 0.5923252105712891\n",
      "Epoch 1006, Loss: 1.5689788162708282, Final Batch Loss: 0.49442312121391296\n",
      "Epoch 1007, Loss: 1.5046058297157288, Final Batch Loss: 0.4754120111465454\n",
      "Epoch 1008, Loss: 1.6463871598243713, Final Batch Loss: 0.5236999988555908\n",
      "Epoch 1009, Loss: 1.6987972259521484, Final Batch Loss: 0.503851056098938\n",
      "Epoch 1010, Loss: 1.5987504720687866, Final Batch Loss: 0.5223662853240967\n",
      "Epoch 1011, Loss: 1.7011059522628784, Final Batch Loss: 0.5899714231491089\n",
      "Epoch 1012, Loss: 1.6076325178146362, Final Batch Loss: 0.5707985758781433\n",
      "Epoch 1013, Loss: 1.645864188671112, Final Batch Loss: 0.4588592052459717\n",
      "Epoch 1014, Loss: 1.7274320125579834, Final Batch Loss: 0.5435325503349304\n",
      "Epoch 1015, Loss: 1.6017410457134247, Final Batch Loss: 0.5617541074752808\n",
      "Epoch 1016, Loss: 1.5995061993598938, Final Batch Loss: 0.5328824520111084\n",
      "Epoch 1017, Loss: 1.7832671999931335, Final Batch Loss: 0.7034078240394592\n",
      "Epoch 1018, Loss: 1.6504575312137604, Final Batch Loss: 0.5562249422073364\n",
      "Epoch 1019, Loss: 1.7102273106575012, Final Batch Loss: 0.6718738675117493\n",
      "Epoch 1020, Loss: 1.697959303855896, Final Batch Loss: 0.6165591478347778\n",
      "Epoch 1021, Loss: 1.579387903213501, Final Batch Loss: 0.49714386463165283\n",
      "Epoch 1022, Loss: 1.856106698513031, Final Batch Loss: 0.6710141897201538\n",
      "Epoch 1023, Loss: 1.6991890668869019, Final Batch Loss: 0.5877112746238708\n",
      "Epoch 1024, Loss: 1.548581749200821, Final Batch Loss: 0.4748865067958832\n",
      "Epoch 1025, Loss: 1.5634406805038452, Final Batch Loss: 0.5122575163841248\n",
      "Epoch 1026, Loss: 1.6562332212924957, Final Batch Loss: 0.6111112833023071\n",
      "Epoch 1027, Loss: 1.6413972675800323, Final Batch Loss: 0.4862503111362457\n",
      "Epoch 1028, Loss: 1.58286452293396, Final Batch Loss: 0.5101853609085083\n",
      "Epoch 1029, Loss: 1.6855809390544891, Final Batch Loss: 0.6179695725440979\n",
      "Epoch 1030, Loss: 1.6073571741580963, Final Batch Loss: 0.49808362126350403\n",
      "Epoch 1031, Loss: 1.5883027017116547, Final Batch Loss: 0.4337438642978668\n",
      "Epoch 1032, Loss: 1.6829370856285095, Final Batch Loss: 0.607011079788208\n",
      "Epoch 1033, Loss: 1.5692103505134583, Final Batch Loss: 0.5587214827537537\n",
      "Epoch 1034, Loss: 1.5955289602279663, Final Batch Loss: 0.4814779758453369\n",
      "Epoch 1035, Loss: 1.5846479535102844, Final Batch Loss: 0.5375590324401855\n",
      "Epoch 1036, Loss: 1.5031573176383972, Final Batch Loss: 0.4054505228996277\n",
      "Epoch 1037, Loss: 1.5917604565620422, Final Batch Loss: 0.5401303172111511\n",
      "Epoch 1038, Loss: 1.666407585144043, Final Batch Loss: 0.6358926296234131\n",
      "Epoch 1039, Loss: 1.743952214717865, Final Batch Loss: 0.6336387395858765\n",
      "Epoch 1040, Loss: 1.546119749546051, Final Batch Loss: 0.563125729560852\n",
      "Epoch 1041, Loss: 1.7052944898605347, Final Batch Loss: 0.5974385738372803\n",
      "Epoch 1042, Loss: 1.660692572593689, Final Batch Loss: 0.5289021134376526\n",
      "Epoch 1043, Loss: 1.7179427742958069, Final Batch Loss: 0.6557120084762573\n",
      "Epoch 1044, Loss: 1.6893912553787231, Final Batch Loss: 0.5698055624961853\n",
      "Epoch 1045, Loss: 1.6488226056098938, Final Batch Loss: 0.5549380779266357\n",
      "Epoch 1046, Loss: 1.7225284576416016, Final Batch Loss: 0.5155918598175049\n",
      "Epoch 1047, Loss: 1.713726818561554, Final Batch Loss: 0.5120514631271362\n",
      "Epoch 1048, Loss: 1.553463339805603, Final Batch Loss: 0.5305603742599487\n",
      "Epoch 1049, Loss: 1.674655020236969, Final Batch Loss: 0.5809369683265686\n",
      "Epoch 1050, Loss: 1.635756015777588, Final Batch Loss: 0.5397453308105469\n",
      "Epoch 1051, Loss: 1.7584351301193237, Final Batch Loss: 0.5525509715080261\n",
      "Epoch 1052, Loss: 1.6453789472579956, Final Batch Loss: 0.5276928544044495\n",
      "Epoch 1053, Loss: 1.5563983917236328, Final Batch Loss: 0.4650375545024872\n",
      "Epoch 1054, Loss: 1.5903648436069489, Final Batch Loss: 0.49436166882514954\n",
      "Epoch 1055, Loss: 1.7295307517051697, Final Batch Loss: 0.6106414198875427\n",
      "Epoch 1056, Loss: 1.6308873295783997, Final Batch Loss: 0.54576176404953\n",
      "Epoch 1057, Loss: 1.5673560500144958, Final Batch Loss: 0.5453656911849976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1058, Loss: 1.5632986724376678, Final Batch Loss: 0.46816781163215637\n",
      "Epoch 1059, Loss: 1.5518963038921356, Final Batch Loss: 0.5410499572753906\n",
      "Epoch 1060, Loss: 1.5838813483715057, Final Batch Loss: 0.4587641656398773\n",
      "Epoch 1061, Loss: 1.6771401166915894, Final Batch Loss: 0.5307093858718872\n",
      "Epoch 1062, Loss: 1.5504142343997955, Final Batch Loss: 0.5586334466934204\n",
      "Epoch 1063, Loss: 1.567980796098709, Final Batch Loss: 0.5871986150741577\n",
      "Epoch 1064, Loss: 1.496870905160904, Final Batch Loss: 0.4759104251861572\n",
      "Epoch 1065, Loss: 1.5512603521347046, Final Batch Loss: 0.4798584580421448\n",
      "Epoch 1066, Loss: 1.6727537214756012, Final Batch Loss: 0.5708171725273132\n",
      "Epoch 1067, Loss: 1.5772323310375214, Final Batch Loss: 0.5212849974632263\n",
      "Epoch 1068, Loss: 1.6599713563919067, Final Batch Loss: 0.5489434599876404\n",
      "Epoch 1069, Loss: 1.7136269211769104, Final Batch Loss: 0.5734935402870178\n",
      "Epoch 1070, Loss: 1.6616805791854858, Final Batch Loss: 0.5007427930831909\n",
      "Epoch 1071, Loss: 1.5382992625236511, Final Batch Loss: 0.5357427000999451\n",
      "Epoch 1072, Loss: 1.6176666021347046, Final Batch Loss: 0.5507911443710327\n",
      "Epoch 1073, Loss: 1.693764567375183, Final Batch Loss: 0.6549625396728516\n",
      "Epoch 1074, Loss: 1.6137345135211945, Final Batch Loss: 0.5693293213844299\n",
      "Epoch 1075, Loss: 1.6039491593837738, Final Batch Loss: 0.42153868079185486\n",
      "Epoch 1076, Loss: 1.6738594770431519, Final Batch Loss: 0.5796337723731995\n",
      "Epoch 1077, Loss: 1.561418503522873, Final Batch Loss: 0.4914339482784271\n",
      "Epoch 1078, Loss: 1.5191697776317596, Final Batch Loss: 0.5115455389022827\n",
      "Epoch 1079, Loss: 1.5693750083446503, Final Batch Loss: 0.6043856143951416\n",
      "Epoch 1080, Loss: 1.530525803565979, Final Batch Loss: 0.47575390338897705\n",
      "Epoch 1081, Loss: 1.5702187418937683, Final Batch Loss: 0.6049963235855103\n",
      "Epoch 1082, Loss: 1.564466267824173, Final Batch Loss: 0.5650585889816284\n",
      "Epoch 1083, Loss: 1.6561224460601807, Final Batch Loss: 0.5161887407302856\n",
      "Epoch 1084, Loss: 1.58580681681633, Final Batch Loss: 0.44989797472953796\n",
      "Epoch 1085, Loss: 1.6003766655921936, Final Batch Loss: 0.5491759777069092\n",
      "Epoch 1086, Loss: 1.5539014339447021, Final Batch Loss: 0.5336937308311462\n",
      "Epoch 1087, Loss: 1.5421808958053589, Final Batch Loss: 0.45774024724960327\n",
      "Epoch 1088, Loss: 1.564819872379303, Final Batch Loss: 0.49950850009918213\n",
      "Epoch 1089, Loss: 1.8018677234649658, Final Batch Loss: 0.7144627571105957\n",
      "Epoch 1090, Loss: 1.5269573330879211, Final Batch Loss: 0.48607391119003296\n",
      "Epoch 1091, Loss: 1.514054298400879, Final Batch Loss: 0.5211628079414368\n",
      "Epoch 1092, Loss: 1.5943132638931274, Final Batch Loss: 0.6138445734977722\n",
      "Epoch 1093, Loss: 1.7034672498703003, Final Batch Loss: 0.698165774345398\n",
      "Epoch 1094, Loss: 1.5874255001544952, Final Batch Loss: 0.4825775921344757\n",
      "Epoch 1095, Loss: 1.6930891275405884, Final Batch Loss: 0.5526027083396912\n",
      "Epoch 1096, Loss: 1.6403943598270416, Final Batch Loss: 0.6417032480239868\n",
      "Epoch 1097, Loss: 1.5828123688697815, Final Batch Loss: 0.5787114500999451\n",
      "Epoch 1098, Loss: 1.5179438889026642, Final Batch Loss: 0.5245655179023743\n",
      "Epoch 1099, Loss: 1.5546736419200897, Final Batch Loss: 0.5455850958824158\n",
      "Epoch 1100, Loss: 1.5346346497535706, Final Batch Loss: 0.4808082580566406\n",
      "Epoch 1101, Loss: 1.6543654203414917, Final Batch Loss: 0.5652431845664978\n",
      "Epoch 1102, Loss: 1.548149824142456, Final Batch Loss: 0.5191985964775085\n",
      "Epoch 1103, Loss: 1.646594762802124, Final Batch Loss: 0.5930426716804504\n",
      "Epoch 1104, Loss: 1.4456167817115784, Final Batch Loss: 0.4295564591884613\n",
      "Epoch 1105, Loss: 1.551131933927536, Final Batch Loss: 0.43052640557289124\n",
      "Epoch 1106, Loss: 1.6295744180679321, Final Batch Loss: 0.5580934286117554\n",
      "Epoch 1107, Loss: 1.618481993675232, Final Batch Loss: 0.5445148944854736\n",
      "Epoch 1108, Loss: 1.5896590948104858, Final Batch Loss: 0.5065535306930542\n",
      "Epoch 1109, Loss: 1.5548140108585358, Final Batch Loss: 0.5565775036811829\n",
      "Epoch 1110, Loss: 1.5242629647254944, Final Batch Loss: 0.5523185133934021\n",
      "Epoch 1111, Loss: 1.4814440608024597, Final Batch Loss: 0.46003109216690063\n",
      "Epoch 1112, Loss: 1.5454697608947754, Final Batch Loss: 0.5357006788253784\n",
      "Epoch 1113, Loss: 1.514615386724472, Final Batch Loss: 0.4854249656200409\n",
      "Epoch 1114, Loss: 1.6310291290283203, Final Batch Loss: 0.5813333988189697\n",
      "Epoch 1115, Loss: 1.635697066783905, Final Batch Loss: 0.588720440864563\n",
      "Epoch 1116, Loss: 1.5650601387023926, Final Batch Loss: 0.5238728523254395\n",
      "Epoch 1117, Loss: 1.575217217206955, Final Batch Loss: 0.5812405347824097\n",
      "Epoch 1118, Loss: 1.5403737127780914, Final Batch Loss: 0.5496819019317627\n",
      "Epoch 1119, Loss: 1.652529537677765, Final Batch Loss: 0.6435182690620422\n",
      "Epoch 1120, Loss: 1.4732217192649841, Final Batch Loss: 0.4611635208129883\n",
      "Epoch 1121, Loss: 1.5229092240333557, Final Batch Loss: 0.4490564465522766\n",
      "Epoch 1122, Loss: 1.5822168588638306, Final Batch Loss: 0.47388118505477905\n",
      "Epoch 1123, Loss: 1.5177803039550781, Final Batch Loss: 0.5201039910316467\n",
      "Epoch 1124, Loss: 1.5722788274288177, Final Batch Loss: 0.4670739471912384\n",
      "Epoch 1125, Loss: 1.7042182087898254, Final Batch Loss: 0.5464461445808411\n",
      "Epoch 1126, Loss: 1.5403062403202057, Final Batch Loss: 0.45211395621299744\n",
      "Epoch 1127, Loss: 1.6263822317123413, Final Batch Loss: 0.5962966084480286\n",
      "Epoch 1128, Loss: 1.570957064628601, Final Batch Loss: 0.5052320957183838\n",
      "Epoch 1129, Loss: 1.6192913353443146, Final Batch Loss: 0.6138883233070374\n",
      "Epoch 1130, Loss: 1.6655651926994324, Final Batch Loss: 0.5903287529945374\n",
      "Epoch 1131, Loss: 1.548920750617981, Final Batch Loss: 0.5152420401573181\n",
      "Epoch 1132, Loss: 1.5723329782485962, Final Batch Loss: 0.5358189940452576\n",
      "Epoch 1133, Loss: 1.5136480927467346, Final Batch Loss: 0.4252573847770691\n",
      "Epoch 1134, Loss: 1.5493891835212708, Final Batch Loss: 0.539040207862854\n",
      "Epoch 1135, Loss: 1.535877287387848, Final Batch Loss: 0.4334740936756134\n",
      "Epoch 1136, Loss: 1.665607511997223, Final Batch Loss: 0.5902411937713623\n",
      "Epoch 1137, Loss: 1.5336848199367523, Final Batch Loss: 0.4722301661968231\n",
      "Epoch 1138, Loss: 1.4719358682632446, Final Batch Loss: 0.4862387180328369\n",
      "Epoch 1139, Loss: 1.4793687164783478, Final Batch Loss: 0.46278658509254456\n",
      "Epoch 1140, Loss: 1.5537746250629425, Final Batch Loss: 0.4833123981952667\n",
      "Epoch 1141, Loss: 1.597115010023117, Final Batch Loss: 0.5919979214668274\n",
      "Epoch 1142, Loss: 1.558108925819397, Final Batch Loss: 0.6059632897377014\n",
      "Epoch 1143, Loss: 1.6484189629554749, Final Batch Loss: 0.6091604828834534\n",
      "Epoch 1144, Loss: 1.5880406498908997, Final Batch Loss: 0.49711376428604126\n",
      "Epoch 1145, Loss: 1.5543068945407867, Final Batch Loss: 0.5949288606643677\n",
      "Epoch 1146, Loss: 1.4727687239646912, Final Batch Loss: 0.4583395719528198\n",
      "Epoch 1147, Loss: 1.4870080351829529, Final Batch Loss: 0.4756537079811096\n",
      "Epoch 1148, Loss: 1.5080086886882782, Final Batch Loss: 0.4995381534099579\n",
      "Epoch 1149, Loss: 1.5864530801773071, Final Batch Loss: 0.5681527853012085\n",
      "Epoch 1150, Loss: 1.5021770596504211, Final Batch Loss: 0.4667956233024597\n",
      "Epoch 1151, Loss: 1.4699231684207916, Final Batch Loss: 0.41070058941841125\n",
      "Epoch 1152, Loss: 1.603305459022522, Final Batch Loss: 0.5994449257850647\n",
      "Epoch 1153, Loss: 1.5508653819561005, Final Batch Loss: 0.48291322588920593\n",
      "Epoch 1154, Loss: 1.5616001188755035, Final Batch Loss: 0.5457497835159302\n",
      "Epoch 1155, Loss: 1.669811725616455, Final Batch Loss: 0.5519190430641174\n",
      "Epoch 1156, Loss: 1.5194163918495178, Final Batch Loss: 0.4729945659637451\n",
      "Epoch 1157, Loss: 1.4361701011657715, Final Batch Loss: 0.49799779057502747\n",
      "Epoch 1158, Loss: 1.5000207126140594, Final Batch Loss: 0.513791561126709\n",
      "Epoch 1159, Loss: 1.5564752221107483, Final Batch Loss: 0.4926527142524719\n",
      "Epoch 1160, Loss: 1.5758437514305115, Final Batch Loss: 0.5736858248710632\n",
      "Epoch 1161, Loss: 1.5888026058673859, Final Batch Loss: 0.6093965172767639\n",
      "Epoch 1162, Loss: 1.5615264773368835, Final Batch Loss: 0.5080822706222534\n",
      "Epoch 1163, Loss: 1.7151433229446411, Final Batch Loss: 0.5344974994659424\n",
      "Epoch 1164, Loss: 1.5303538739681244, Final Batch Loss: 0.5662853121757507\n",
      "Epoch 1165, Loss: 1.5398090481758118, Final Batch Loss: 0.5593720078468323\n",
      "Epoch 1166, Loss: 1.494744598865509, Final Batch Loss: 0.5060957074165344\n",
      "Epoch 1167, Loss: 1.4839358925819397, Final Batch Loss: 0.5576916933059692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1168, Loss: 1.5413342714309692, Final Batch Loss: 0.5261094570159912\n",
      "Epoch 1169, Loss: 1.5735223591327667, Final Batch Loss: 0.580756425857544\n",
      "Epoch 1170, Loss: 1.5201578736305237, Final Batch Loss: 0.528308629989624\n",
      "Epoch 1171, Loss: 1.4764267206192017, Final Batch Loss: 0.4069138169288635\n",
      "Epoch 1172, Loss: 1.574380338191986, Final Batch Loss: 0.5815039277076721\n",
      "Epoch 1173, Loss: 1.5605137348175049, Final Batch Loss: 0.5826126337051392\n",
      "Epoch 1174, Loss: 1.5897480845451355, Final Batch Loss: 0.5356722474098206\n",
      "Epoch 1175, Loss: 1.6265904307365417, Final Batch Loss: 0.5117766261100769\n",
      "Epoch 1176, Loss: 1.471914827823639, Final Batch Loss: 0.4686499834060669\n",
      "Epoch 1177, Loss: 1.5733113586902618, Final Batch Loss: 0.4749195873737335\n",
      "Epoch 1178, Loss: 1.5031547546386719, Final Batch Loss: 0.4280564785003662\n",
      "Epoch 1179, Loss: 1.6439819931983948, Final Batch Loss: 0.5934615731239319\n",
      "Epoch 1180, Loss: 1.4697628319263458, Final Batch Loss: 0.46104124188423157\n",
      "Epoch 1181, Loss: 1.5304611027240753, Final Batch Loss: 0.4908316433429718\n",
      "Epoch 1182, Loss: 1.4777128994464874, Final Batch Loss: 0.4550699293613434\n",
      "Epoch 1183, Loss: 1.615548312664032, Final Batch Loss: 0.6076056361198425\n",
      "Epoch 1184, Loss: 1.6032677292823792, Final Batch Loss: 0.5607971549034119\n",
      "Epoch 1185, Loss: 1.6025147438049316, Final Batch Loss: 0.5003643035888672\n",
      "Epoch 1186, Loss: 1.692304015159607, Final Batch Loss: 0.6492620706558228\n",
      "Epoch 1187, Loss: 1.522475689649582, Final Batch Loss: 0.6036818623542786\n",
      "Epoch 1188, Loss: 1.4019789695739746, Final Batch Loss: 0.4016801118850708\n",
      "Epoch 1189, Loss: 1.492917776107788, Final Batch Loss: 0.49238115549087524\n",
      "Epoch 1190, Loss: 1.617216408252716, Final Batch Loss: 0.5938833951950073\n",
      "Epoch 1191, Loss: 1.51267009973526, Final Batch Loss: 0.5159024000167847\n",
      "Epoch 1192, Loss: 1.4465231597423553, Final Batch Loss: 0.41550078988075256\n",
      "Epoch 1193, Loss: 1.5147813558578491, Final Batch Loss: 0.5085093379020691\n",
      "Epoch 1194, Loss: 1.5636807084083557, Final Batch Loss: 0.5488563179969788\n",
      "Epoch 1195, Loss: 1.5134579241275787, Final Batch Loss: 0.4626038372516632\n",
      "Epoch 1196, Loss: 1.4574465155601501, Final Batch Loss: 0.44252172112464905\n",
      "Epoch 1197, Loss: 1.4712982475757599, Final Batch Loss: 0.47256386280059814\n",
      "Epoch 1198, Loss: 1.4315686225891113, Final Batch Loss: 0.47952184081077576\n",
      "Epoch 1199, Loss: 1.4876380562782288, Final Batch Loss: 0.5230433940887451\n",
      "Epoch 1200, Loss: 1.5562938749790192, Final Batch Loss: 0.5168527364730835\n",
      "Epoch 1201, Loss: 1.4807980358600616, Final Batch Loss: 0.4504501521587372\n",
      "Epoch 1202, Loss: 1.4454092979431152, Final Batch Loss: 0.5368907451629639\n",
      "Epoch 1203, Loss: 1.6456336975097656, Final Batch Loss: 0.4896150827407837\n",
      "Epoch 1204, Loss: 1.5654033422470093, Final Batch Loss: 0.5155662894248962\n",
      "Epoch 1205, Loss: 1.4438707530498505, Final Batch Loss: 0.4637540280818939\n",
      "Epoch 1206, Loss: 1.5017416179180145, Final Batch Loss: 0.45210713148117065\n",
      "Epoch 1207, Loss: 1.5180425345897675, Final Batch Loss: 0.5806334018707275\n",
      "Epoch 1208, Loss: 1.5496078431606293, Final Batch Loss: 0.5307011008262634\n",
      "Epoch 1209, Loss: 1.5186141729354858, Final Batch Loss: 0.4594491720199585\n",
      "Epoch 1210, Loss: 1.500461220741272, Final Batch Loss: 0.5062039494514465\n",
      "Epoch 1211, Loss: 1.5294810831546783, Final Batch Loss: 0.4841102063655853\n",
      "Epoch 1212, Loss: 1.4462922811508179, Final Batch Loss: 0.5143017172813416\n",
      "Epoch 1213, Loss: 1.4822221100330353, Final Batch Loss: 0.5126750469207764\n",
      "Epoch 1214, Loss: 1.4775235652923584, Final Batch Loss: 0.5167502164840698\n",
      "Epoch 1215, Loss: 1.4805950820446014, Final Batch Loss: 0.5096630454063416\n",
      "Epoch 1216, Loss: 1.5078375041484833, Final Batch Loss: 0.5122784972190857\n",
      "Epoch 1217, Loss: 1.5487943887710571, Final Batch Loss: 0.5347940325737\n",
      "Epoch 1218, Loss: 1.527592122554779, Final Batch Loss: 0.5325061082839966\n",
      "Epoch 1219, Loss: 1.4228887259960175, Final Batch Loss: 0.4720010459423065\n",
      "Epoch 1220, Loss: 1.4079444110393524, Final Batch Loss: 0.4503726065158844\n",
      "Epoch 1221, Loss: 1.4597597420215607, Final Batch Loss: 0.5142605304718018\n",
      "Epoch 1222, Loss: 1.555392861366272, Final Batch Loss: 0.5458106398582458\n",
      "Epoch 1223, Loss: 1.4780931174755096, Final Batch Loss: 0.513316810131073\n",
      "Epoch 1224, Loss: 1.3744792640209198, Final Batch Loss: 0.4861137568950653\n",
      "Epoch 1225, Loss: 1.4285856783390045, Final Batch Loss: 0.4648263454437256\n",
      "Epoch 1226, Loss: 1.3795660138130188, Final Batch Loss: 0.4077608585357666\n",
      "Epoch 1227, Loss: 1.559433400630951, Final Batch Loss: 0.5413830876350403\n",
      "Epoch 1228, Loss: 1.499121516942978, Final Batch Loss: 0.4255698323249817\n",
      "Epoch 1229, Loss: 1.5243913531303406, Final Batch Loss: 0.5239746570587158\n",
      "Epoch 1230, Loss: 1.4277665615081787, Final Batch Loss: 0.43087562918663025\n",
      "Epoch 1231, Loss: 1.4477426409721375, Final Batch Loss: 0.4526378810405731\n",
      "Epoch 1232, Loss: 1.5497393012046814, Final Batch Loss: 0.5102437138557434\n",
      "Epoch 1233, Loss: 1.5107747614383698, Final Batch Loss: 0.5258482098579407\n",
      "Epoch 1234, Loss: 1.5339278280735016, Final Batch Loss: 0.45027294754981995\n",
      "Epoch 1235, Loss: 1.4962108731269836, Final Batch Loss: 0.5579781532287598\n",
      "Epoch 1236, Loss: 1.566983699798584, Final Batch Loss: 0.5054882764816284\n",
      "Epoch 1237, Loss: 1.436083734035492, Final Batch Loss: 0.4313696324825287\n",
      "Epoch 1238, Loss: 1.524865597486496, Final Batch Loss: 0.5384342670440674\n",
      "Epoch 1239, Loss: 1.4843252301216125, Final Batch Loss: 0.4832408130168915\n",
      "Epoch 1240, Loss: 1.5550869703292847, Final Batch Loss: 0.5548298358917236\n",
      "Epoch 1241, Loss: 1.5659994781017303, Final Batch Loss: 0.5113063454627991\n",
      "Epoch 1242, Loss: 1.6282190680503845, Final Batch Loss: 0.5634113550186157\n",
      "Epoch 1243, Loss: 1.612240970134735, Final Batch Loss: 0.5047305226325989\n",
      "Epoch 1244, Loss: 1.6339900493621826, Final Batch Loss: 0.6318240165710449\n",
      "Epoch 1245, Loss: 1.593231439590454, Final Batch Loss: 0.5546127557754517\n",
      "Epoch 1246, Loss: 1.582962155342102, Final Batch Loss: 0.6123696565628052\n",
      "Epoch 1247, Loss: 1.4857966303825378, Final Batch Loss: 0.549002468585968\n",
      "Epoch 1248, Loss: 1.515547662973404, Final Batch Loss: 0.5360885262489319\n",
      "Epoch 1249, Loss: 1.5001151859760284, Final Batch Loss: 0.4964933693408966\n",
      "Epoch 1250, Loss: 1.4220626950263977, Final Batch Loss: 0.36178553104400635\n",
      "Epoch 1251, Loss: 1.451341837644577, Final Batch Loss: 0.4839552044868469\n",
      "Epoch 1252, Loss: 1.4396353662014008, Final Batch Loss: 0.5257648229598999\n",
      "Epoch 1253, Loss: 1.4899978935718536, Final Batch Loss: 0.5694471597671509\n",
      "Epoch 1254, Loss: 1.494638979434967, Final Batch Loss: 0.45545607805252075\n",
      "Epoch 1255, Loss: 1.439732313156128, Final Batch Loss: 0.48276039958000183\n",
      "Epoch 1256, Loss: 1.4139065146446228, Final Batch Loss: 0.46113133430480957\n",
      "Epoch 1257, Loss: 1.52588951587677, Final Batch Loss: 0.5552641749382019\n",
      "Epoch 1258, Loss: 1.5159934163093567, Final Batch Loss: 0.5912758708000183\n",
      "Epoch 1259, Loss: 1.4262575209140778, Final Batch Loss: 0.4392078220844269\n",
      "Epoch 1260, Loss: 1.5489972233772278, Final Batch Loss: 0.6024061441421509\n",
      "Epoch 1261, Loss: 1.4545895755290985, Final Batch Loss: 0.3683091104030609\n",
      "Epoch 1262, Loss: 1.5224697589874268, Final Batch Loss: 0.4751890301704407\n",
      "Epoch 1263, Loss: 1.5695761442184448, Final Batch Loss: 0.5563675761222839\n",
      "Epoch 1264, Loss: 1.5675859451293945, Final Batch Loss: 0.615519642829895\n",
      "Epoch 1265, Loss: 1.3656761348247528, Final Batch Loss: 0.43755459785461426\n",
      "Epoch 1266, Loss: 1.454315960407257, Final Batch Loss: 0.4543444514274597\n",
      "Epoch 1267, Loss: 1.4439125061035156, Final Batch Loss: 0.42722469568252563\n",
      "Epoch 1268, Loss: 1.4717426896095276, Final Batch Loss: 0.5399343967437744\n",
      "Epoch 1269, Loss: 1.5060023963451385, Final Batch Loss: 0.5032548308372498\n",
      "Epoch 1270, Loss: 1.4467496275901794, Final Batch Loss: 0.48668473958969116\n",
      "Epoch 1271, Loss: 1.5159481465816498, Final Batch Loss: 0.5319895148277283\n",
      "Epoch 1272, Loss: 1.4029697477817535, Final Batch Loss: 0.46275827288627625\n",
      "Epoch 1273, Loss: 1.457493633031845, Final Batch Loss: 0.4980045557022095\n",
      "Epoch 1274, Loss: 1.4682129323482513, Final Batch Loss: 0.5181241035461426\n",
      "Epoch 1275, Loss: 1.3199423253536224, Final Batch Loss: 0.38757964968681335\n",
      "Epoch 1276, Loss: 1.456689715385437, Final Batch Loss: 0.5078610777854919\n",
      "Epoch 1277, Loss: 1.4241822063922882, Final Batch Loss: 0.45366358757019043\n",
      "Epoch 1278, Loss: 1.4198144376277924, Final Batch Loss: 0.39514264464378357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1279, Loss: 1.48725026845932, Final Batch Loss: 0.513020932674408\n",
      "Epoch 1280, Loss: 1.4920774400234222, Final Batch Loss: 0.501801609992981\n",
      "Epoch 1281, Loss: 1.4249746799468994, Final Batch Loss: 0.47116655111312866\n",
      "Epoch 1282, Loss: 1.46098393201828, Final Batch Loss: 0.5148897767066956\n",
      "Epoch 1283, Loss: 1.4894943535327911, Final Batch Loss: 0.5925344228744507\n",
      "Epoch 1284, Loss: 1.4932770133018494, Final Batch Loss: 0.6046027541160583\n",
      "Epoch 1285, Loss: 1.4247500896453857, Final Batch Loss: 0.4866585433483124\n",
      "Epoch 1286, Loss: 1.4712961614131927, Final Batch Loss: 0.5500603914260864\n",
      "Epoch 1287, Loss: 1.509556919336319, Final Batch Loss: 0.5764959454536438\n",
      "Epoch 1288, Loss: 1.4708691239356995, Final Batch Loss: 0.561170756816864\n",
      "Epoch 1289, Loss: 1.5667774677276611, Final Batch Loss: 0.5359274744987488\n",
      "Epoch 1290, Loss: 1.3458493947982788, Final Batch Loss: 0.4721081256866455\n",
      "Epoch 1291, Loss: 1.567894607782364, Final Batch Loss: 0.632504940032959\n",
      "Epoch 1292, Loss: 1.4308805763721466, Final Batch Loss: 0.4547356367111206\n",
      "Epoch 1293, Loss: 1.4902238547801971, Final Batch Loss: 0.5179346799850464\n",
      "Epoch 1294, Loss: 1.446950912475586, Final Batch Loss: 0.4852704107761383\n",
      "Epoch 1295, Loss: 1.4463313221931458, Final Batch Loss: 0.47021764516830444\n",
      "Epoch 1296, Loss: 1.487892597913742, Final Batch Loss: 0.4052552878856659\n",
      "Epoch 1297, Loss: 1.3451146483421326, Final Batch Loss: 0.40415632724761963\n",
      "Epoch 1298, Loss: 1.423755705356598, Final Batch Loss: 0.502383828163147\n",
      "Epoch 1299, Loss: 1.4269877672195435, Final Batch Loss: 0.45033323764801025\n",
      "Epoch 1300, Loss: 1.4037895202636719, Final Batch Loss: 0.4474538564682007\n",
      "Epoch 1301, Loss: 1.4538600742816925, Final Batch Loss: 0.45232129096984863\n",
      "Epoch 1302, Loss: 1.4798172116279602, Final Batch Loss: 0.5632724165916443\n",
      "Epoch 1303, Loss: 1.4583556652069092, Final Batch Loss: 0.4711052179336548\n",
      "Epoch 1304, Loss: 1.4342701137065887, Final Batch Loss: 0.41584187746047974\n",
      "Epoch 1305, Loss: 1.353844553232193, Final Batch Loss: 0.3784531354904175\n",
      "Epoch 1306, Loss: 1.4539252817630768, Final Batch Loss: 0.48721814155578613\n",
      "Epoch 1307, Loss: 1.4407030642032623, Final Batch Loss: 0.4451639950275421\n",
      "Epoch 1308, Loss: 1.5684577822685242, Final Batch Loss: 0.6101675629615784\n",
      "Epoch 1309, Loss: 1.380648821592331, Final Batch Loss: 0.42864108085632324\n",
      "Epoch 1310, Loss: 1.458267480134964, Final Batch Loss: 0.49789923429489136\n",
      "Epoch 1311, Loss: 1.4891369044780731, Final Batch Loss: 0.5346572399139404\n",
      "Epoch 1312, Loss: 1.443247228860855, Final Batch Loss: 0.4220086932182312\n",
      "Epoch 1313, Loss: 1.4566561579704285, Final Batch Loss: 0.44739651679992676\n",
      "Epoch 1314, Loss: 1.3924702107906342, Final Batch Loss: 0.4766063392162323\n",
      "Epoch 1315, Loss: 1.4446826577186584, Final Batch Loss: 0.5908078551292419\n",
      "Epoch 1316, Loss: 1.5409619212150574, Final Batch Loss: 0.5528313517570496\n",
      "Epoch 1317, Loss: 1.4344178438186646, Final Batch Loss: 0.5293281674385071\n",
      "Epoch 1318, Loss: 1.3868632912635803, Final Batch Loss: 0.44832608103752136\n",
      "Epoch 1319, Loss: 1.4880515038967133, Final Batch Loss: 0.483574777841568\n",
      "Epoch 1320, Loss: 1.4351190328598022, Final Batch Loss: 0.5180608630180359\n",
      "Epoch 1321, Loss: 1.4465903043746948, Final Batch Loss: 0.5265977382659912\n",
      "Epoch 1322, Loss: 1.475767344236374, Final Batch Loss: 0.46759113669395447\n",
      "Epoch 1323, Loss: 1.346923828125, Final Batch Loss: 0.4512246549129486\n",
      "Epoch 1324, Loss: 1.499486356973648, Final Batch Loss: 0.5782226324081421\n",
      "Epoch 1325, Loss: 1.4596142768859863, Final Batch Loss: 0.47190722823143005\n",
      "Epoch 1326, Loss: 1.3415753245353699, Final Batch Loss: 0.4529821574687958\n",
      "Epoch 1327, Loss: 1.4297739267349243, Final Batch Loss: 0.4907042980194092\n",
      "Epoch 1328, Loss: 1.4695169031620026, Final Batch Loss: 0.4746866524219513\n",
      "Epoch 1329, Loss: 1.3787667155265808, Final Batch Loss: 0.3792482018470764\n",
      "Epoch 1330, Loss: 1.388664573431015, Final Batch Loss: 0.4552333652973175\n",
      "Epoch 1331, Loss: 1.3984015882015228, Final Batch Loss: 0.47173309326171875\n",
      "Epoch 1332, Loss: 1.4463674128055573, Final Batch Loss: 0.5054475665092468\n",
      "Epoch 1333, Loss: 1.5714816749095917, Final Batch Loss: 0.6356868743896484\n",
      "Epoch 1334, Loss: 1.3425694704055786, Final Batch Loss: 0.40212884545326233\n",
      "Epoch 1335, Loss: 1.4692239165306091, Final Batch Loss: 0.5435893535614014\n",
      "Epoch 1336, Loss: 1.3332097232341766, Final Batch Loss: 0.4242022931575775\n",
      "Epoch 1337, Loss: 1.4931082427501678, Final Batch Loss: 0.5905011892318726\n",
      "Epoch 1338, Loss: 1.4579191207885742, Final Batch Loss: 0.4793434143066406\n",
      "Epoch 1339, Loss: 1.5937249064445496, Final Batch Loss: 0.6631800532341003\n",
      "Epoch 1340, Loss: 1.4493739604949951, Final Batch Loss: 0.5571906566619873\n",
      "Epoch 1341, Loss: 1.4318652749061584, Final Batch Loss: 0.4473358988761902\n",
      "Epoch 1342, Loss: 1.33934286236763, Final Batch Loss: 0.42995578050613403\n",
      "Epoch 1343, Loss: 1.373817354440689, Final Batch Loss: 0.3270368278026581\n",
      "Epoch 1344, Loss: 1.358036071062088, Final Batch Loss: 0.40237095952033997\n",
      "Epoch 1345, Loss: 1.3511163592338562, Final Batch Loss: 0.4486654996871948\n",
      "Epoch 1346, Loss: 1.3239223659038544, Final Batch Loss: 0.40132594108581543\n",
      "Epoch 1347, Loss: 1.4360806345939636, Final Batch Loss: 0.464699387550354\n",
      "Epoch 1348, Loss: 1.4366956949234009, Final Batch Loss: 0.42187947034835815\n",
      "Epoch 1349, Loss: 1.5383185744285583, Final Batch Loss: 0.512532114982605\n",
      "Epoch 1350, Loss: 1.4814032018184662, Final Batch Loss: 0.5013970732688904\n",
      "Epoch 1351, Loss: 1.4048341810703278, Final Batch Loss: 0.5435703992843628\n",
      "Epoch 1352, Loss: 1.4472147524356842, Final Batch Loss: 0.4960874021053314\n",
      "Epoch 1353, Loss: 1.3899070024490356, Final Batch Loss: 0.4679831266403198\n",
      "Epoch 1354, Loss: 1.3681948781013489, Final Batch Loss: 0.5032094717025757\n",
      "Epoch 1355, Loss: 1.3497190177440643, Final Batch Loss: 0.40802231431007385\n",
      "Epoch 1356, Loss: 1.3246209025382996, Final Batch Loss: 0.46570727229118347\n",
      "Epoch 1357, Loss: 1.46016326546669, Final Batch Loss: 0.47680750489234924\n",
      "Epoch 1358, Loss: 1.4028777778148651, Final Batch Loss: 0.46140098571777344\n",
      "Epoch 1359, Loss: 1.5739823579788208, Final Batch Loss: 0.5480112433433533\n",
      "Epoch 1360, Loss: 1.3313654363155365, Final Batch Loss: 0.46763876080513\n",
      "Epoch 1361, Loss: 1.4268772900104523, Final Batch Loss: 0.4624226689338684\n",
      "Epoch 1362, Loss: 1.4665841460227966, Final Batch Loss: 0.5900985598564148\n",
      "Epoch 1363, Loss: 1.323895275592804, Final Batch Loss: 0.4222542941570282\n",
      "Epoch 1364, Loss: 1.4210644364356995, Final Batch Loss: 0.5362024903297424\n",
      "Epoch 1365, Loss: 1.3793282508850098, Final Batch Loss: 0.5117304921150208\n",
      "Epoch 1366, Loss: 1.4011524319648743, Final Batch Loss: 0.4495157301425934\n",
      "Epoch 1367, Loss: 1.5149637460708618, Final Batch Loss: 0.5248898267745972\n",
      "Epoch 1368, Loss: 1.379721999168396, Final Batch Loss: 0.42768341302871704\n",
      "Epoch 1369, Loss: 1.4124999046325684, Final Batch Loss: 0.47030338644981384\n",
      "Epoch 1370, Loss: 1.5146631002426147, Final Batch Loss: 0.5489713549613953\n",
      "Epoch 1371, Loss: 1.3923605382442474, Final Batch Loss: 0.5197947025299072\n",
      "Epoch 1372, Loss: 1.2850852608680725, Final Batch Loss: 0.35056793689727783\n",
      "Epoch 1373, Loss: 1.4679045975208282, Final Batch Loss: 0.5184987783432007\n",
      "Epoch 1374, Loss: 1.4534722864627838, Final Batch Loss: 0.47147253155708313\n",
      "Epoch 1375, Loss: 1.3331836462020874, Final Batch Loss: 0.4000743627548218\n",
      "Epoch 1376, Loss: 1.4954001307487488, Final Batch Loss: 0.5260990262031555\n",
      "Epoch 1377, Loss: 1.3489853143692017, Final Batch Loss: 0.4124559462070465\n",
      "Epoch 1378, Loss: 1.3816469609737396, Final Batch Loss: 0.44120079278945923\n",
      "Epoch 1379, Loss: 1.4037433564662933, Final Batch Loss: 0.4938516616821289\n",
      "Epoch 1380, Loss: 1.4374492168426514, Final Batch Loss: 0.5013297200202942\n",
      "Epoch 1381, Loss: 1.3081251680850983, Final Batch Loss: 0.42546623945236206\n",
      "Epoch 1382, Loss: 1.4239319860935211, Final Batch Loss: 0.43295711278915405\n",
      "Epoch 1383, Loss: 1.366569072008133, Final Batch Loss: 0.4371729791164398\n",
      "Epoch 1384, Loss: 1.4092137217521667, Final Batch Loss: 0.48133233189582825\n",
      "Epoch 1385, Loss: 1.4310236275196075, Final Batch Loss: 0.5127220153808594\n",
      "Epoch 1386, Loss: 1.4417167007923126, Final Batch Loss: 0.4537498354911804\n",
      "Epoch 1387, Loss: 1.3857546746730804, Final Batch Loss: 0.4546269476413727\n",
      "Epoch 1388, Loss: 1.3528222739696503, Final Batch Loss: 0.42412087321281433\n",
      "Epoch 1389, Loss: 1.4828580617904663, Final Batch Loss: 0.5415831208229065\n",
      "Epoch 1390, Loss: 1.3245018124580383, Final Batch Loss: 0.41355860233306885\n",
      "Epoch 1391, Loss: 1.224782556295395, Final Batch Loss: 0.35529959201812744\n",
      "Epoch 1392, Loss: 1.338897168636322, Final Batch Loss: 0.45572686195373535\n",
      "Epoch 1393, Loss: 1.3085945546627045, Final Batch Loss: 0.47526222467422485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1394, Loss: 1.5754462480545044, Final Batch Loss: 0.5745276212692261\n",
      "Epoch 1395, Loss: 1.4462473392486572, Final Batch Loss: 0.4635414183139801\n",
      "Epoch 1396, Loss: 1.4529820680618286, Final Batch Loss: 0.5517776608467102\n",
      "Epoch 1397, Loss: 1.4215809106826782, Final Batch Loss: 0.5399667024612427\n",
      "Epoch 1398, Loss: 1.4426392316818237, Final Batch Loss: 0.5819438695907593\n",
      "Epoch 1399, Loss: 1.5327998101711273, Final Batch Loss: 0.4793780744075775\n",
      "Epoch 1400, Loss: 1.366258978843689, Final Batch Loss: 0.4301433265209198\n",
      "Epoch 1401, Loss: 1.4659741520881653, Final Batch Loss: 0.4506246745586395\n",
      "Epoch 1402, Loss: 1.3507967591285706, Final Batch Loss: 0.4293937385082245\n",
      "Epoch 1403, Loss: 1.2700325548648834, Final Batch Loss: 0.3504020869731903\n",
      "Epoch 1404, Loss: 1.391806960105896, Final Batch Loss: 0.49779772758483887\n",
      "Epoch 1405, Loss: 1.3849481046199799, Final Batch Loss: 0.45556336641311646\n",
      "Epoch 1406, Loss: 1.449956476688385, Final Batch Loss: 0.5195580124855042\n",
      "Epoch 1407, Loss: 1.4912269413471222, Final Batch Loss: 0.5171254277229309\n",
      "Epoch 1408, Loss: 1.404083400964737, Final Batch Loss: 0.3922041058540344\n",
      "Epoch 1409, Loss: 1.4185770452022552, Final Batch Loss: 0.4972658157348633\n",
      "Epoch 1410, Loss: 1.2695418894290924, Final Batch Loss: 0.4383920431137085\n",
      "Epoch 1411, Loss: 1.4064350724220276, Final Batch Loss: 0.47055694460868835\n",
      "Epoch 1412, Loss: 1.3165337145328522, Final Batch Loss: 0.4132583737373352\n",
      "Epoch 1413, Loss: 1.3748477101325989, Final Batch Loss: 0.4897741377353668\n",
      "Epoch 1414, Loss: 1.3677410185337067, Final Batch Loss: 0.4296235740184784\n",
      "Epoch 1415, Loss: 1.4334681034088135, Final Batch Loss: 0.5134848356246948\n",
      "Epoch 1416, Loss: 1.4549509286880493, Final Batch Loss: 0.437522292137146\n",
      "Epoch 1417, Loss: 1.4080651700496674, Final Batch Loss: 0.49216753244400024\n",
      "Epoch 1418, Loss: 1.3764917850494385, Final Batch Loss: 0.43268486857414246\n",
      "Epoch 1419, Loss: 1.350200891494751, Final Batch Loss: 0.5078532695770264\n",
      "Epoch 1420, Loss: 1.4320695996284485, Final Batch Loss: 0.46454793214797974\n",
      "Epoch 1421, Loss: 1.5743655264377594, Final Batch Loss: 0.6186366081237793\n",
      "Epoch 1422, Loss: 1.360966056585312, Final Batch Loss: 0.4074125289916992\n",
      "Epoch 1423, Loss: 1.4066204726696014, Final Batch Loss: 0.4665403366088867\n",
      "Epoch 1424, Loss: 1.5051246881484985, Final Batch Loss: 0.5322932600975037\n",
      "Epoch 1425, Loss: 1.3559498488903046, Final Batch Loss: 0.4298722743988037\n",
      "Epoch 1426, Loss: 1.2808676660060883, Final Batch Loss: 0.31318745017051697\n",
      "Epoch 1427, Loss: 1.3877186477184296, Final Batch Loss: 0.4551180303096771\n",
      "Epoch 1428, Loss: 1.2356588542461395, Final Batch Loss: 0.33194440603256226\n",
      "Epoch 1429, Loss: 1.358850210905075, Final Batch Loss: 0.42105624079704285\n",
      "Epoch 1430, Loss: 1.4766715168952942, Final Batch Loss: 0.6159951686859131\n",
      "Epoch 1431, Loss: 1.3264465928077698, Final Batch Loss: 0.4674316942691803\n",
      "Epoch 1432, Loss: 1.32680144906044, Final Batch Loss: 0.4181285500526428\n",
      "Epoch 1433, Loss: 1.3354081809520721, Final Batch Loss: 0.434758722782135\n",
      "Epoch 1434, Loss: 1.3233825862407684, Final Batch Loss: 0.4559224843978882\n",
      "Epoch 1435, Loss: 1.4293088018894196, Final Batch Loss: 0.5241760015487671\n",
      "Epoch 1436, Loss: 1.410276085138321, Final Batch Loss: 0.4133918583393097\n",
      "Epoch 1437, Loss: 1.2788945734500885, Final Batch Loss: 0.44087523221969604\n",
      "Epoch 1438, Loss: 1.380307912826538, Final Batch Loss: 0.48053085803985596\n",
      "Epoch 1439, Loss: 1.2846789360046387, Final Batch Loss: 0.43881234526634216\n",
      "Epoch 1440, Loss: 1.1654899716377258, Final Batch Loss: 0.3663882911205292\n",
      "Epoch 1441, Loss: 1.5694420039653778, Final Batch Loss: 0.43342724442481995\n",
      "Epoch 1442, Loss: 1.6135801374912262, Final Batch Loss: 0.580475926399231\n",
      "Epoch 1443, Loss: 1.2980444133281708, Final Batch Loss: 0.34944504499435425\n",
      "Epoch 1444, Loss: 1.265512228012085, Final Batch Loss: 0.3758717179298401\n",
      "Epoch 1445, Loss: 1.3789745569229126, Final Batch Loss: 0.4963979721069336\n",
      "Epoch 1446, Loss: 1.376760184764862, Final Batch Loss: 0.4471887946128845\n",
      "Epoch 1447, Loss: 1.325654298067093, Final Batch Loss: 0.5267387628555298\n",
      "Epoch 1448, Loss: 1.4495839774608612, Final Batch Loss: 0.5318437814712524\n",
      "Epoch 1449, Loss: 1.331175535917282, Final Batch Loss: 0.4404342770576477\n",
      "Epoch 1450, Loss: 1.4873130023479462, Final Batch Loss: 0.612790048122406\n",
      "Epoch 1451, Loss: 1.3194145858287811, Final Batch Loss: 0.45506104826927185\n",
      "Epoch 1452, Loss: 1.4834067225456238, Final Batch Loss: 0.5797408819198608\n",
      "Epoch 1453, Loss: 1.3482942581176758, Final Batch Loss: 0.43833935260772705\n",
      "Epoch 1454, Loss: 1.3134136199951172, Final Batch Loss: 0.3839636743068695\n",
      "Epoch 1455, Loss: 1.3150011003017426, Final Batch Loss: 0.4561620354652405\n",
      "Epoch 1456, Loss: 1.3113372325897217, Final Batch Loss: 0.40275198221206665\n",
      "Epoch 1457, Loss: 1.281341701745987, Final Batch Loss: 0.38529470562934875\n",
      "Epoch 1458, Loss: 1.465354710817337, Final Batch Loss: 0.5915473103523254\n",
      "Epoch 1459, Loss: 1.3442056477069855, Final Batch Loss: 0.44279786944389343\n",
      "Epoch 1460, Loss: 1.3701137602329254, Final Batch Loss: 0.4574158191680908\n",
      "Epoch 1461, Loss: 1.4887825846672058, Final Batch Loss: 0.5776861310005188\n",
      "Epoch 1462, Loss: 1.22540882229805, Final Batch Loss: 0.40985986590385437\n",
      "Epoch 1463, Loss: 1.3651143908500671, Final Batch Loss: 0.4951687753200531\n",
      "Epoch 1464, Loss: 1.3483704328536987, Final Batch Loss: 0.49751681089401245\n",
      "Epoch 1465, Loss: 1.2933717966079712, Final Batch Loss: 0.40191614627838135\n",
      "Epoch 1466, Loss: 1.3683975636959076, Final Batch Loss: 0.5624009966850281\n",
      "Epoch 1467, Loss: 1.310866802930832, Final Batch Loss: 0.42660054564476013\n",
      "Epoch 1468, Loss: 1.4564526677131653, Final Batch Loss: 0.535010576248169\n",
      "Epoch 1469, Loss: 1.3945159614086151, Final Batch Loss: 0.48194652795791626\n",
      "Epoch 1470, Loss: 1.3970185220241547, Final Batch Loss: 0.5062766075134277\n",
      "Epoch 1471, Loss: 1.3573155105113983, Final Batch Loss: 0.4474700391292572\n",
      "Epoch 1472, Loss: 1.3923483788967133, Final Batch Loss: 0.41320914030075073\n",
      "Epoch 1473, Loss: 1.3620051741600037, Final Batch Loss: 0.3389183580875397\n",
      "Epoch 1474, Loss: 1.2315220534801483, Final Batch Loss: 0.3098427355289459\n",
      "Epoch 1475, Loss: 1.302295595407486, Final Batch Loss: 0.42763665318489075\n",
      "Epoch 1476, Loss: 1.392234444618225, Final Batch Loss: 0.3748054802417755\n",
      "Epoch 1477, Loss: 1.3816068768501282, Final Batch Loss: 0.5602676272392273\n",
      "Epoch 1478, Loss: 1.30653578042984, Final Batch Loss: 0.49330446124076843\n",
      "Epoch 1479, Loss: 1.4357766509056091, Final Batch Loss: 0.5492691993713379\n",
      "Epoch 1480, Loss: 1.4555973708629608, Final Batch Loss: 0.5948952436447144\n",
      "Epoch 1481, Loss: 1.442303866147995, Final Batch Loss: 0.46955519914627075\n",
      "Epoch 1482, Loss: 1.3448637127876282, Final Batch Loss: 0.36986857652664185\n",
      "Epoch 1483, Loss: 1.2787628769874573, Final Batch Loss: 0.3350699245929718\n",
      "Epoch 1484, Loss: 1.4114484190940857, Final Batch Loss: 0.4598556160926819\n",
      "Epoch 1485, Loss: 1.4128743410110474, Final Batch Loss: 0.5834124088287354\n",
      "Epoch 1486, Loss: 1.2683121263980865, Final Batch Loss: 0.4001692235469818\n",
      "Epoch 1487, Loss: 1.4211696088314056, Final Batch Loss: 0.5134401917457581\n",
      "Epoch 1488, Loss: 1.4437107145786285, Final Batch Loss: 0.5556735396385193\n",
      "Epoch 1489, Loss: 1.3817781805992126, Final Batch Loss: 0.4712536931037903\n",
      "Epoch 1490, Loss: 1.3102494180202484, Final Batch Loss: 0.3848245441913605\n",
      "Epoch 1491, Loss: 1.2676146030426025, Final Batch Loss: 0.38073447346687317\n",
      "Epoch 1492, Loss: 1.4106459617614746, Final Batch Loss: 0.4863308370113373\n",
      "Epoch 1493, Loss: 1.3329695463180542, Final Batch Loss: 0.445781946182251\n",
      "Epoch 1494, Loss: 1.2780296802520752, Final Batch Loss: 0.3560505509376526\n",
      "Epoch 1495, Loss: 1.3014607727527618, Final Batch Loss: 0.41932690143585205\n",
      "Epoch 1496, Loss: 1.272747963666916, Final Batch Loss: 0.5052614212036133\n",
      "Epoch 1497, Loss: 1.2067356407642365, Final Batch Loss: 0.394902765750885\n",
      "Epoch 1498, Loss: 1.3727554380893707, Final Batch Loss: 0.45728832483291626\n",
      "Epoch 1499, Loss: 1.2511144280433655, Final Batch Loss: 0.3916791081428528\n",
      "Epoch 1500, Loss: 1.4383818805217743, Final Batch Loss: 0.4263235628604889\n",
      "Epoch 1501, Loss: 1.4246001541614532, Final Batch Loss: 0.5902157425880432\n",
      "Epoch 1502, Loss: 1.435472458600998, Final Batch Loss: 0.5163086652755737\n",
      "Epoch 1503, Loss: 1.4373553395271301, Final Batch Loss: 0.4881947338581085\n",
      "Epoch 1504, Loss: 1.267927646636963, Final Batch Loss: 0.5116521716117859\n",
      "Epoch 1505, Loss: 1.307795137166977, Final Batch Loss: 0.4110587537288666\n",
      "Epoch 1506, Loss: 1.2295741438865662, Final Batch Loss: 0.384956032037735\n",
      "Epoch 1507, Loss: 1.236318588256836, Final Batch Loss: 0.35320401191711426\n",
      "Epoch 1508, Loss: 1.2019623219966888, Final Batch Loss: 0.3159010112285614\n",
      "Epoch 1509, Loss: 1.3747147023677826, Final Batch Loss: 0.4678647816181183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1510, Loss: 1.3717180788516998, Final Batch Loss: 0.4665420651435852\n",
      "Epoch 1511, Loss: 1.3357373774051666, Final Batch Loss: 0.40818294882774353\n",
      "Epoch 1512, Loss: 1.3116466999053955, Final Batch Loss: 0.40049752593040466\n",
      "Epoch 1513, Loss: 1.2463806569576263, Final Batch Loss: 0.3224014341831207\n",
      "Epoch 1514, Loss: 1.4589254260063171, Final Batch Loss: 0.5297242403030396\n",
      "Epoch 1515, Loss: 1.4197054505348206, Final Batch Loss: 0.4914674460887909\n",
      "Epoch 1516, Loss: 1.3551121652126312, Final Batch Loss: 0.41501450538635254\n",
      "Epoch 1517, Loss: 1.480592131614685, Final Batch Loss: 0.5662887096405029\n",
      "Epoch 1518, Loss: 1.28575000166893, Final Batch Loss: 0.3857150673866272\n",
      "Epoch 1519, Loss: 1.3185069561004639, Final Batch Loss: 0.4047640264034271\n",
      "Epoch 1520, Loss: 1.3661229610443115, Final Batch Loss: 0.5474115014076233\n",
      "Epoch 1521, Loss: 1.3359033465385437, Final Batch Loss: 0.37995079159736633\n",
      "Epoch 1522, Loss: 1.3198961913585663, Final Batch Loss: 0.41555875539779663\n",
      "Epoch 1523, Loss: 1.535906046628952, Final Batch Loss: 0.5697449445724487\n",
      "Epoch 1524, Loss: 1.3303547501564026, Final Batch Loss: 0.4363068640232086\n",
      "Epoch 1525, Loss: 1.474767416715622, Final Batch Loss: 0.49675673246383667\n",
      "Epoch 1526, Loss: 1.3717796206474304, Final Batch Loss: 0.5466532111167908\n",
      "Epoch 1527, Loss: 1.4792092144489288, Final Batch Loss: 0.5614274144172668\n",
      "Epoch 1528, Loss: 1.3462525606155396, Final Batch Loss: 0.484870582818985\n",
      "Epoch 1529, Loss: 1.259569227695465, Final Batch Loss: 0.44954097270965576\n",
      "Epoch 1530, Loss: 1.3480217456817627, Final Batch Loss: 0.4557807445526123\n",
      "Epoch 1531, Loss: 1.2407325208187103, Final Batch Loss: 0.31199389696121216\n",
      "Epoch 1532, Loss: 1.405578464269638, Final Batch Loss: 0.5608402490615845\n",
      "Epoch 1533, Loss: 1.282706767320633, Final Batch Loss: 0.45469412207603455\n",
      "Epoch 1534, Loss: 1.2647937536239624, Final Batch Loss: 0.440902441740036\n",
      "Epoch 1535, Loss: 1.360083431005478, Final Batch Loss: 0.4724665880203247\n",
      "Epoch 1536, Loss: 1.3061589300632477, Final Batch Loss: 0.38413792848587036\n",
      "Epoch 1537, Loss: 1.3029637038707733, Final Batch Loss: 0.3908277451992035\n",
      "Epoch 1538, Loss: 1.3229102492332458, Final Batch Loss: 0.4242904484272003\n",
      "Epoch 1539, Loss: 1.2879822552204132, Final Batch Loss: 0.44460180401802063\n",
      "Epoch 1540, Loss: 1.3541447520256042, Final Batch Loss: 0.5010848045349121\n",
      "Epoch 1541, Loss: 1.28371262550354, Final Batch Loss: 0.42862072587013245\n",
      "Epoch 1542, Loss: 1.339789718389511, Final Batch Loss: 0.39647141098976135\n",
      "Epoch 1543, Loss: 1.403145581483841, Final Batch Loss: 0.4927539527416229\n",
      "Epoch 1544, Loss: 1.3222020268440247, Final Batch Loss: 0.37083593010902405\n",
      "Epoch 1545, Loss: 1.2582339644432068, Final Batch Loss: 0.4104306399822235\n",
      "Epoch 1546, Loss: 1.355776995420456, Final Batch Loss: 0.4833317995071411\n",
      "Epoch 1547, Loss: 1.2941592633724213, Final Batch Loss: 0.36475491523742676\n",
      "Epoch 1548, Loss: 1.3445817232131958, Final Batch Loss: 0.4631703495979309\n",
      "Epoch 1549, Loss: 1.4016385972499847, Final Batch Loss: 0.4005071818828583\n",
      "Epoch 1550, Loss: 1.3277001082897186, Final Batch Loss: 0.4423169791698456\n",
      "Epoch 1551, Loss: 1.3602946400642395, Final Batch Loss: 0.44269147515296936\n",
      "Epoch 1552, Loss: 1.3053671717643738, Final Batch Loss: 0.40725842118263245\n",
      "Epoch 1553, Loss: 1.21781986951828, Final Batch Loss: 0.414796382188797\n",
      "Epoch 1554, Loss: 1.3421764969825745, Final Batch Loss: 0.553800106048584\n",
      "Epoch 1555, Loss: 1.2241619527339935, Final Batch Loss: 0.35132190585136414\n",
      "Epoch 1556, Loss: 1.2382221817970276, Final Batch Loss: 0.35862624645233154\n",
      "Epoch 1557, Loss: 1.3433300256729126, Final Batch Loss: 0.36304694414138794\n",
      "Epoch 1558, Loss: 1.5553596019744873, Final Batch Loss: 0.5948357582092285\n",
      "Epoch 1559, Loss: 1.3993958532810211, Final Batch Loss: 0.4593656361103058\n",
      "Epoch 1560, Loss: 1.289404273033142, Final Batch Loss: 0.43279972672462463\n",
      "Epoch 1561, Loss: 1.3886827528476715, Final Batch Loss: 0.5157243609428406\n",
      "Epoch 1562, Loss: 1.2036676108837128, Final Batch Loss: 0.31893306970596313\n",
      "Epoch 1563, Loss: 1.3496991395950317, Final Batch Loss: 0.47221148014068604\n",
      "Epoch 1564, Loss: 1.313057005405426, Final Batch Loss: 0.4494864046573639\n",
      "Epoch 1565, Loss: 1.1894131898880005, Final Batch Loss: 0.38805413246154785\n",
      "Epoch 1566, Loss: 1.2207784354686737, Final Batch Loss: 0.3573380708694458\n",
      "Epoch 1567, Loss: 1.3136737048625946, Final Batch Loss: 0.40328770875930786\n",
      "Epoch 1568, Loss: 1.400063931941986, Final Batch Loss: 0.5777567625045776\n",
      "Epoch 1569, Loss: 1.3153120279312134, Final Batch Loss: 0.5010840892791748\n",
      "Epoch 1570, Loss: 1.2884657979011536, Final Batch Loss: 0.45340269804000854\n",
      "Epoch 1571, Loss: 1.2322431802749634, Final Batch Loss: 0.40773633122444153\n",
      "Epoch 1572, Loss: 1.2619055211544037, Final Batch Loss: 0.42519301176071167\n",
      "Epoch 1573, Loss: 1.3188158571720123, Final Batch Loss: 0.43859922885894775\n",
      "Epoch 1574, Loss: 1.3298909962177277, Final Batch Loss: 0.5200581550598145\n",
      "Epoch 1575, Loss: 1.2574017941951752, Final Batch Loss: 0.3883037269115448\n",
      "Epoch 1576, Loss: 1.4465688169002533, Final Batch Loss: 0.6022892594337463\n",
      "Epoch 1577, Loss: 1.319679707288742, Final Batch Loss: 0.44410163164138794\n",
      "Epoch 1578, Loss: 1.2523474395275116, Final Batch Loss: 0.3122580051422119\n",
      "Epoch 1579, Loss: 1.2854026556015015, Final Batch Loss: 0.4691712558269501\n",
      "Epoch 1580, Loss: 1.3269183933734894, Final Batch Loss: 0.43024057149887085\n",
      "Epoch 1581, Loss: 1.3276028037071228, Final Batch Loss: 0.4545398950576782\n",
      "Epoch 1582, Loss: 1.2468831241130829, Final Batch Loss: 0.3773307204246521\n",
      "Epoch 1583, Loss: 1.2060861885547638, Final Batch Loss: 0.3634265661239624\n",
      "Epoch 1584, Loss: 1.276429831981659, Final Batch Loss: 0.3970716893672943\n",
      "Epoch 1585, Loss: 1.257213443517685, Final Batch Loss: 0.3744610548019409\n",
      "Epoch 1586, Loss: 1.3908406496047974, Final Batch Loss: 0.5211548209190369\n",
      "Epoch 1587, Loss: 1.2959243953227997, Final Batch Loss: 0.4366314113140106\n",
      "Epoch 1588, Loss: 1.3460004329681396, Final Batch Loss: 0.46898648142814636\n",
      "Epoch 1589, Loss: 1.3167903125286102, Final Batch Loss: 0.4573647379875183\n",
      "Epoch 1590, Loss: 1.2796517610549927, Final Batch Loss: 0.4697667062282562\n",
      "Epoch 1591, Loss: 1.4170603454113007, Final Batch Loss: 0.5506986379623413\n",
      "Epoch 1592, Loss: 1.2079100012779236, Final Batch Loss: 0.391315221786499\n",
      "Epoch 1593, Loss: 1.2851731479167938, Final Batch Loss: 0.4210719168186188\n",
      "Epoch 1594, Loss: 1.3702958226203918, Final Batch Loss: 0.5432059168815613\n",
      "Epoch 1595, Loss: 1.3625260293483734, Final Batch Loss: 0.5193328857421875\n",
      "Epoch 1596, Loss: 1.2690277695655823, Final Batch Loss: 0.3877280354499817\n",
      "Epoch 1597, Loss: 1.2008757591247559, Final Batch Loss: 0.34865739941596985\n",
      "Epoch 1598, Loss: 1.3180238902568817, Final Batch Loss: 0.402584046125412\n",
      "Epoch 1599, Loss: 1.354990541934967, Final Batch Loss: 0.47497811913490295\n",
      "Epoch 1600, Loss: 1.1258145868778229, Final Batch Loss: 0.32354241609573364\n",
      "Epoch 1601, Loss: 1.2801483571529388, Final Batch Loss: 0.45645877718925476\n",
      "Epoch 1602, Loss: 1.2338067591190338, Final Batch Loss: 0.41264015436172485\n",
      "Epoch 1603, Loss: 1.2008818089962006, Final Batch Loss: 0.4072131812572479\n",
      "Epoch 1604, Loss: 1.239468514919281, Final Batch Loss: 0.4225936830043793\n",
      "Epoch 1605, Loss: 1.2539376020431519, Final Batch Loss: 0.3548363149166107\n",
      "Epoch 1606, Loss: 1.3542497754096985, Final Batch Loss: 0.4682738780975342\n",
      "Epoch 1607, Loss: 1.3683988451957703, Final Batch Loss: 0.4901203513145447\n",
      "Epoch 1608, Loss: 1.3034862875938416, Final Batch Loss: 0.4246842563152313\n",
      "Epoch 1609, Loss: 1.3211111724376678, Final Batch Loss: 0.4400666654109955\n",
      "Epoch 1610, Loss: 1.3486243188381195, Final Batch Loss: 0.530685305595398\n",
      "Epoch 1611, Loss: 1.235495388507843, Final Batch Loss: 0.4365752041339874\n",
      "Epoch 1612, Loss: 1.2698128819465637, Final Batch Loss: 0.48705756664276123\n",
      "Epoch 1613, Loss: 1.3905566334724426, Final Batch Loss: 0.536466121673584\n",
      "Epoch 1614, Loss: 1.2769362032413483, Final Batch Loss: 0.3697492480278015\n",
      "Epoch 1615, Loss: 1.298571527004242, Final Batch Loss: 0.39317062497138977\n",
      "Epoch 1616, Loss: 1.3789967000484467, Final Batch Loss: 0.44746506214141846\n",
      "Epoch 1617, Loss: 1.4241331815719604, Final Batch Loss: 0.45040541887283325\n",
      "Epoch 1618, Loss: 1.1862106919288635, Final Batch Loss: 0.31066226959228516\n",
      "Epoch 1619, Loss: 1.3275408744812012, Final Batch Loss: 0.5120112299919128\n",
      "Epoch 1620, Loss: 1.3724333345890045, Final Batch Loss: 0.43034788966178894\n",
      "Epoch 1621, Loss: 1.3293825089931488, Final Batch Loss: 0.41082286834716797\n",
      "Epoch 1622, Loss: 1.1651085019111633, Final Batch Loss: 0.37898242473602295\n",
      "Epoch 1623, Loss: 1.2319886982440948, Final Batch Loss: 0.4071720540523529\n",
      "Epoch 1624, Loss: 1.2243361175060272, Final Batch Loss: 0.382066935300827\n",
      "Epoch 1625, Loss: 1.3449847996234894, Final Batch Loss: 0.46432361006736755\n",
      "Epoch 1626, Loss: 1.2217001616954803, Final Batch Loss: 0.3814859390258789\n",
      "Epoch 1627, Loss: 1.279366821050644, Final Batch Loss: 0.427222341299057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1628, Loss: 1.3669540286064148, Final Batch Loss: 0.4764339327812195\n",
      "Epoch 1629, Loss: 1.2666305303573608, Final Batch Loss: 0.441142201423645\n",
      "Epoch 1630, Loss: 1.2990702092647552, Final Batch Loss: 0.4584847688674927\n",
      "Epoch 1631, Loss: 1.312565803527832, Final Batch Loss: 0.4449924826622009\n",
      "Epoch 1632, Loss: 1.311709702014923, Final Batch Loss: 0.3769998252391815\n",
      "Epoch 1633, Loss: 1.291011482477188, Final Batch Loss: 0.5230124592781067\n",
      "Epoch 1634, Loss: 1.3585399687290192, Final Batch Loss: 0.5586491227149963\n",
      "Epoch 1635, Loss: 1.3537329137325287, Final Batch Loss: 0.45317140221595764\n",
      "Epoch 1636, Loss: 1.2644112408161163, Final Batch Loss: 0.5061708688735962\n",
      "Epoch 1637, Loss: 1.3182513415813446, Final Batch Loss: 0.3829263746738434\n",
      "Epoch 1638, Loss: 1.2920316755771637, Final Batch Loss: 0.4101579785346985\n",
      "Epoch 1639, Loss: 1.3321237564086914, Final Batch Loss: 0.4259496033191681\n",
      "Epoch 1640, Loss: 1.2697851061820984, Final Batch Loss: 0.4658676087856293\n",
      "Epoch 1641, Loss: 1.2243646383285522, Final Batch Loss: 0.43071043491363525\n",
      "Epoch 1642, Loss: 1.3107795715332031, Final Batch Loss: 0.3908827304840088\n",
      "Epoch 1643, Loss: 1.3023573756217957, Final Batch Loss: 0.41036751866340637\n",
      "Epoch 1644, Loss: 1.1675112843513489, Final Batch Loss: 0.34002557396888733\n",
      "Epoch 1645, Loss: 1.2331543266773224, Final Batch Loss: 0.3676961064338684\n",
      "Epoch 1646, Loss: 1.3705072700977325, Final Batch Loss: 0.40890127420425415\n",
      "Epoch 1647, Loss: 1.281924068927765, Final Batch Loss: 0.4467791020870209\n",
      "Epoch 1648, Loss: 1.3689630627632141, Final Batch Loss: 0.5680365562438965\n",
      "Epoch 1649, Loss: 1.1684615314006805, Final Batch Loss: 0.3366190195083618\n",
      "Epoch 1650, Loss: 1.3298067450523376, Final Batch Loss: 0.37667912244796753\n",
      "Epoch 1651, Loss: 1.3084879517555237, Final Batch Loss: 0.40272054076194763\n",
      "Epoch 1652, Loss: 1.249155580997467, Final Batch Loss: 0.38100340962409973\n",
      "Epoch 1653, Loss: 1.4103819131851196, Final Batch Loss: 0.39671841263771057\n",
      "Epoch 1654, Loss: 1.253480225801468, Final Batch Loss: 0.4780580401420593\n",
      "Epoch 1655, Loss: 1.3543702960014343, Final Batch Loss: 0.5596905946731567\n",
      "Epoch 1656, Loss: 1.1501833200454712, Final Batch Loss: 0.39930668473243713\n",
      "Epoch 1657, Loss: 1.3515803515911102, Final Batch Loss: 0.49780136346817017\n",
      "Epoch 1658, Loss: 1.3733798265457153, Final Batch Loss: 0.49870625138282776\n",
      "Epoch 1659, Loss: 1.2374844551086426, Final Batch Loss: 0.355411559343338\n",
      "Epoch 1660, Loss: 1.176479995250702, Final Batch Loss: 0.4426979720592499\n",
      "Epoch 1661, Loss: 1.2797001600265503, Final Batch Loss: 0.35715821385383606\n",
      "Epoch 1662, Loss: 1.3886059820652008, Final Batch Loss: 0.5524941682815552\n",
      "Epoch 1663, Loss: 1.3294603824615479, Final Batch Loss: 0.42490091919898987\n",
      "Epoch 1664, Loss: 1.2212976515293121, Final Batch Loss: 0.4186190068721771\n",
      "Epoch 1665, Loss: 1.2930895686149597, Final Batch Loss: 0.46472981572151184\n",
      "Epoch 1666, Loss: 1.3622134327888489, Final Batch Loss: 0.5120759010314941\n",
      "Epoch 1667, Loss: 1.2407152652740479, Final Batch Loss: 0.3751647174358368\n",
      "Epoch 1668, Loss: 1.3578296303749084, Final Batch Loss: 0.46213826537132263\n",
      "Epoch 1669, Loss: 1.194648653268814, Final Batch Loss: 0.3092668354511261\n",
      "Epoch 1670, Loss: 1.3049793541431427, Final Batch Loss: 0.43149131536483765\n",
      "Epoch 1671, Loss: 1.374135285615921, Final Batch Loss: 0.5054187178611755\n",
      "Epoch 1672, Loss: 1.259393334388733, Final Batch Loss: 0.4280742406845093\n",
      "Epoch 1673, Loss: 1.403168946504593, Final Batch Loss: 0.5732735395431519\n",
      "Epoch 1674, Loss: 1.3192892372608185, Final Batch Loss: 0.46331384778022766\n",
      "Epoch 1675, Loss: 1.33314710855484, Final Batch Loss: 0.38920724391937256\n",
      "Epoch 1676, Loss: 1.2905349135398865, Final Batch Loss: 0.4921012222766876\n",
      "Epoch 1677, Loss: 1.2019782364368439, Final Batch Loss: 0.35951370000839233\n",
      "Epoch 1678, Loss: 1.3117036521434784, Final Batch Loss: 0.5209887623786926\n",
      "Epoch 1679, Loss: 1.2410034537315369, Final Batch Loss: 0.35733941197395325\n",
      "Epoch 1680, Loss: 1.159312903881073, Final Batch Loss: 0.3360808193683624\n",
      "Epoch 1681, Loss: 1.213129073381424, Final Batch Loss: 0.4064042866230011\n",
      "Epoch 1682, Loss: 1.3315571546554565, Final Batch Loss: 0.42748165130615234\n",
      "Epoch 1683, Loss: 1.3221850395202637, Final Batch Loss: 0.42775171995162964\n",
      "Epoch 1684, Loss: 1.207251250743866, Final Batch Loss: 0.47272640466690063\n",
      "Epoch 1685, Loss: 1.2646056115627289, Final Batch Loss: 0.352030485868454\n",
      "Epoch 1686, Loss: 1.2282779216766357, Final Batch Loss: 0.4494691789150238\n",
      "Epoch 1687, Loss: 1.3626348078250885, Final Batch Loss: 0.5046199560165405\n",
      "Epoch 1688, Loss: 1.3258226215839386, Final Batch Loss: 0.48279377818107605\n",
      "Epoch 1689, Loss: 1.2575774490833282, Final Batch Loss: 0.3933149576187134\n",
      "Epoch 1690, Loss: 1.2967980802059174, Final Batch Loss: 0.4430008828639984\n",
      "Epoch 1691, Loss: 1.3139508664608002, Final Batch Loss: 0.3711507320404053\n",
      "Epoch 1692, Loss: 1.1768161952495575, Final Batch Loss: 0.4031406342983246\n",
      "Epoch 1693, Loss: 1.2258759140968323, Final Batch Loss: 0.46772444248199463\n",
      "Epoch 1694, Loss: 1.224750280380249, Final Batch Loss: 0.3934424817562103\n",
      "Epoch 1695, Loss: 1.2542706727981567, Final Batch Loss: 0.46377885341644287\n",
      "Epoch 1696, Loss: 1.3021869957447052, Final Batch Loss: 0.41409316658973694\n",
      "Epoch 1697, Loss: 1.2897609770298004, Final Batch Loss: 0.40569332242012024\n",
      "Epoch 1698, Loss: 1.3012844622135162, Final Batch Loss: 0.43106338381767273\n",
      "Epoch 1699, Loss: 1.2570362091064453, Final Batch Loss: 0.4058401584625244\n",
      "Epoch 1700, Loss: 1.216721624135971, Final Batch Loss: 0.4035261571407318\n",
      "Epoch 1701, Loss: 1.2059627175331116, Final Batch Loss: 0.3485857844352722\n",
      "Epoch 1702, Loss: 1.2878981828689575, Final Batch Loss: 0.472750723361969\n",
      "Epoch 1703, Loss: 1.229387104511261, Final Batch Loss: 0.36510685086250305\n",
      "Epoch 1704, Loss: 1.3280300498008728, Final Batch Loss: 0.4876580536365509\n",
      "Epoch 1705, Loss: 1.1968849897384644, Final Batch Loss: 0.41542866826057434\n",
      "Epoch 1706, Loss: 1.2864773869514465, Final Batch Loss: 0.464218407869339\n",
      "Epoch 1707, Loss: 1.2280725240707397, Final Batch Loss: 0.28552451729774475\n",
      "Epoch 1708, Loss: 1.2838842868804932, Final Batch Loss: 0.5116708278656006\n",
      "Epoch 1709, Loss: 1.1633398234844208, Final Batch Loss: 0.39722940325737\n",
      "Epoch 1710, Loss: 1.1878414452075958, Final Batch Loss: 0.41089197993278503\n",
      "Epoch 1711, Loss: 1.3627902269363403, Final Batch Loss: 0.43619582056999207\n",
      "Epoch 1712, Loss: 1.2327215671539307, Final Batch Loss: 0.40737631916999817\n",
      "Epoch 1713, Loss: 1.2382758855819702, Final Batch Loss: 0.41625818610191345\n",
      "Epoch 1714, Loss: 1.2125557363033295, Final Batch Loss: 0.3441896140575409\n",
      "Epoch 1715, Loss: 1.2531992495059967, Final Batch Loss: 0.42703744769096375\n",
      "Epoch 1716, Loss: 1.2976185083389282, Final Batch Loss: 0.41284552216529846\n",
      "Epoch 1717, Loss: 1.2166204452514648, Final Batch Loss: 0.41339725255966187\n",
      "Epoch 1718, Loss: 1.1779040098190308, Final Batch Loss: 0.37447255849838257\n",
      "Epoch 1719, Loss: 1.2951575815677643, Final Batch Loss: 0.39146503806114197\n",
      "Epoch 1720, Loss: 1.2813904285430908, Final Batch Loss: 0.4928610026836395\n",
      "Epoch 1721, Loss: 1.1284794509410858, Final Batch Loss: 0.3965073525905609\n",
      "Epoch 1722, Loss: 1.166902780532837, Final Batch Loss: 0.4102243483066559\n",
      "Epoch 1723, Loss: 1.2020198702812195, Final Batch Loss: 0.45322704315185547\n",
      "Epoch 1724, Loss: 1.1664265990257263, Final Batch Loss: 0.4675203859806061\n",
      "Epoch 1725, Loss: 1.222610741853714, Final Batch Loss: 0.39682185649871826\n",
      "Epoch 1726, Loss: 1.3269988596439362, Final Batch Loss: 0.4521455764770508\n",
      "Epoch 1727, Loss: 1.1569108664989471, Final Batch Loss: 0.3456331193447113\n",
      "Epoch 1728, Loss: 1.1642570495605469, Final Batch Loss: 0.3553316295146942\n",
      "Epoch 1729, Loss: 1.1630850434303284, Final Batch Loss: 0.4086996912956238\n",
      "Epoch 1730, Loss: 1.1512786149978638, Final Batch Loss: 0.372999370098114\n",
      "Epoch 1731, Loss: 1.2990012764930725, Final Batch Loss: 0.42474788427352905\n",
      "Epoch 1732, Loss: 1.325911521911621, Final Batch Loss: 0.4071451723575592\n",
      "Epoch 1733, Loss: 1.228033185005188, Final Batch Loss: 0.465679407119751\n",
      "Epoch 1734, Loss: 1.3923679888248444, Final Batch Loss: 0.45409074425697327\n",
      "Epoch 1735, Loss: 1.2495452761650085, Final Batch Loss: 0.40102142095565796\n",
      "Epoch 1736, Loss: 1.1355597376823425, Final Batch Loss: 0.3399587571620941\n",
      "Epoch 1737, Loss: 1.190129667520523, Final Batch Loss: 0.36077719926834106\n",
      "Epoch 1738, Loss: 1.1955525577068329, Final Batch Loss: 0.4365561306476593\n",
      "Epoch 1739, Loss: 1.272825449705124, Final Batch Loss: 0.40002912282943726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1740, Loss: 1.2009511590003967, Final Batch Loss: 0.4464866518974304\n",
      "Epoch 1741, Loss: 1.2333423793315887, Final Batch Loss: 0.4045960307121277\n",
      "Epoch 1742, Loss: 1.2759263515472412, Final Batch Loss: 0.38632047176361084\n",
      "Epoch 1743, Loss: 1.3009451031684875, Final Batch Loss: 0.4850335419178009\n",
      "Epoch 1744, Loss: 1.3018669188022614, Final Batch Loss: 0.373599112033844\n",
      "Epoch 1745, Loss: 1.2064041495323181, Final Batch Loss: 0.31277865171432495\n",
      "Epoch 1746, Loss: 1.277673363685608, Final Batch Loss: 0.3907541334629059\n",
      "Epoch 1747, Loss: 1.2085735201835632, Final Batch Loss: 0.4255025386810303\n",
      "Epoch 1748, Loss: 1.132598876953125, Final Batch Loss: 0.385989785194397\n",
      "Epoch 1749, Loss: 1.2422136068344116, Final Batch Loss: 0.4666508436203003\n",
      "Epoch 1750, Loss: 1.1855781376361847, Final Batch Loss: 0.3687938153743744\n",
      "Epoch 1751, Loss: 1.2441189587116241, Final Batch Loss: 0.4069894552230835\n",
      "Epoch 1752, Loss: 1.243381142616272, Final Batch Loss: 0.36432576179504395\n",
      "Epoch 1753, Loss: 1.1625067293643951, Final Batch Loss: 0.4344351887702942\n",
      "Epoch 1754, Loss: 1.1667053699493408, Final Batch Loss: 0.32498303055763245\n",
      "Epoch 1755, Loss: 1.2297011017799377, Final Batch Loss: 0.43337640166282654\n",
      "Epoch 1756, Loss: 1.2017990350723267, Final Batch Loss: 0.40341460704803467\n",
      "Epoch 1757, Loss: 1.325604498386383, Final Batch Loss: 0.4364517629146576\n",
      "Epoch 1758, Loss: 1.1139470040798187, Final Batch Loss: 0.2816542387008667\n",
      "Epoch 1759, Loss: 1.4526877105236053, Final Batch Loss: 0.5259562134742737\n",
      "Epoch 1760, Loss: 1.237453132867813, Final Batch Loss: 0.44372954964637756\n",
      "Epoch 1761, Loss: 1.2089763581752777, Final Batch Loss: 0.44319504499435425\n",
      "Epoch 1762, Loss: 1.2241190373897552, Final Batch Loss: 0.36422500014305115\n",
      "Epoch 1763, Loss: 1.3401761651039124, Final Batch Loss: 0.4267158508300781\n",
      "Epoch 1764, Loss: 1.3713738918304443, Final Batch Loss: 0.4605651795864105\n",
      "Epoch 1765, Loss: 1.2456382811069489, Final Batch Loss: 0.4547778069972992\n",
      "Epoch 1766, Loss: 1.3060151934623718, Final Batch Loss: 0.40303555130958557\n",
      "Epoch 1767, Loss: 1.231296956539154, Final Batch Loss: 0.40003326535224915\n",
      "Epoch 1768, Loss: 1.2648638486862183, Final Batch Loss: 0.39571836590766907\n",
      "Epoch 1769, Loss: 1.3074820339679718, Final Batch Loss: 0.4751743972301483\n",
      "Epoch 1770, Loss: 1.2355710864067078, Final Batch Loss: 0.3786558210849762\n",
      "Epoch 1771, Loss: 1.2736965715885162, Final Batch Loss: 0.3708702027797699\n",
      "Epoch 1772, Loss: 1.0844318866729736, Final Batch Loss: 0.33733025193214417\n",
      "Epoch 1773, Loss: 1.1652267277240753, Final Batch Loss: 0.4185413718223572\n",
      "Epoch 1774, Loss: 1.3480674028396606, Final Batch Loss: 0.5412170886993408\n",
      "Epoch 1775, Loss: 1.1868540048599243, Final Batch Loss: 0.33794093132019043\n",
      "Epoch 1776, Loss: 1.3615853488445282, Final Batch Loss: 0.47275441884994507\n",
      "Epoch 1777, Loss: 1.1978445053100586, Final Batch Loss: 0.39872419834136963\n",
      "Epoch 1778, Loss: 1.2126432657241821, Final Batch Loss: 0.3530579209327698\n",
      "Epoch 1779, Loss: 1.3494949638843536, Final Batch Loss: 0.49853476881980896\n",
      "Epoch 1780, Loss: 1.1879188418388367, Final Batch Loss: 0.41751590371131897\n",
      "Epoch 1781, Loss: 1.29636150598526, Final Batch Loss: 0.4482271075248718\n",
      "Epoch 1782, Loss: 1.2284223437309265, Final Batch Loss: 0.37075158953666687\n",
      "Epoch 1783, Loss: 1.1473068594932556, Final Batch Loss: 0.3731769621372223\n",
      "Epoch 1784, Loss: 1.2122884392738342, Final Batch Loss: 0.3425232768058777\n",
      "Epoch 1785, Loss: 1.2797673344612122, Final Batch Loss: 0.44242221117019653\n",
      "Epoch 1786, Loss: 1.198108047246933, Final Batch Loss: 0.37977334856987\n",
      "Epoch 1787, Loss: 1.1473608016967773, Final Batch Loss: 0.33727192878723145\n",
      "Epoch 1788, Loss: 1.3422978222370148, Final Batch Loss: 0.547714352607727\n",
      "Epoch 1789, Loss: 1.1862861812114716, Final Batch Loss: 0.3629823327064514\n",
      "Epoch 1790, Loss: 1.1829270720481873, Final Batch Loss: 0.36906698346138\n",
      "Epoch 1791, Loss: 1.1695962250232697, Final Batch Loss: 0.3759174942970276\n",
      "Epoch 1792, Loss: 1.2678224444389343, Final Batch Loss: 0.42163723707199097\n",
      "Epoch 1793, Loss: 1.136908769607544, Final Batch Loss: 0.4151904284954071\n",
      "Epoch 1794, Loss: 1.2275325953960419, Final Batch Loss: 0.3640502095222473\n",
      "Epoch 1795, Loss: 1.1109907925128937, Final Batch Loss: 0.32366469502449036\n",
      "Epoch 1796, Loss: 1.2031701803207397, Final Batch Loss: 0.42272359132766724\n",
      "Epoch 1797, Loss: 1.164250671863556, Final Batch Loss: 0.31828248500823975\n",
      "Epoch 1798, Loss: 1.2295361161231995, Final Batch Loss: 0.4510786533355713\n",
      "Epoch 1799, Loss: 1.2368279099464417, Final Batch Loss: 0.475169837474823\n",
      "Epoch 1800, Loss: 1.25772425532341, Final Batch Loss: 0.37786877155303955\n",
      "Epoch 1801, Loss: 1.237141191959381, Final Batch Loss: 0.40942078828811646\n",
      "Epoch 1802, Loss: 1.2523427307605743, Final Batch Loss: 0.4517193138599396\n",
      "Epoch 1803, Loss: 1.0798950791358948, Final Batch Loss: 0.3567976653575897\n",
      "Epoch 1804, Loss: 1.1959987878799438, Final Batch Loss: 0.4313928484916687\n",
      "Epoch 1805, Loss: 1.3013435900211334, Final Batch Loss: 0.4201834499835968\n",
      "Epoch 1806, Loss: 1.1911267340183258, Final Batch Loss: 0.3668554127216339\n",
      "Epoch 1807, Loss: 1.2974048256874084, Final Batch Loss: 0.49515652656555176\n",
      "Epoch 1808, Loss: 1.197222650051117, Final Batch Loss: 0.4266696274280548\n",
      "Epoch 1809, Loss: 1.225793182849884, Final Batch Loss: 0.4064491093158722\n",
      "Epoch 1810, Loss: 1.3007155060768127, Final Batch Loss: 0.45123109221458435\n",
      "Epoch 1811, Loss: 1.277278572320938, Final Batch Loss: 0.44302433729171753\n",
      "Epoch 1812, Loss: 1.1194846034049988, Final Batch Loss: 0.3501608073711395\n",
      "Epoch 1813, Loss: 1.3880708813667297, Final Batch Loss: 0.5254020094871521\n",
      "Epoch 1814, Loss: 1.3091918528079987, Final Batch Loss: 0.4399946630001068\n",
      "Epoch 1815, Loss: 1.1286297142505646, Final Batch Loss: 0.3201037347316742\n",
      "Epoch 1816, Loss: 1.1884896755218506, Final Batch Loss: 0.39298272132873535\n",
      "Epoch 1817, Loss: 1.158565491437912, Final Batch Loss: 0.3105151653289795\n",
      "Epoch 1818, Loss: 1.2736549377441406, Final Batch Loss: 0.48272860050201416\n",
      "Epoch 1819, Loss: 1.2076627314090729, Final Batch Loss: 0.39761486649513245\n",
      "Epoch 1820, Loss: 1.2225896716117859, Final Batch Loss: 0.4652417302131653\n",
      "Epoch 1821, Loss: 1.1236472129821777, Final Batch Loss: 0.3338119387626648\n",
      "Epoch 1822, Loss: 1.1677593886852264, Final Batch Loss: 0.3675214946269989\n",
      "Epoch 1823, Loss: 1.1822664737701416, Final Batch Loss: 0.3961738646030426\n",
      "Epoch 1824, Loss: 1.1800791919231415, Final Batch Loss: 0.3719921112060547\n",
      "Epoch 1825, Loss: 1.1969470381736755, Final Batch Loss: 0.41056180000305176\n",
      "Epoch 1826, Loss: 1.156505823135376, Final Batch Loss: 0.34967949986457825\n",
      "Epoch 1827, Loss: 1.2011058926582336, Final Batch Loss: 0.37606966495513916\n",
      "Epoch 1828, Loss: 1.095902442932129, Final Batch Loss: 0.27040353417396545\n",
      "Epoch 1829, Loss: 1.214122623205185, Final Batch Loss: 0.43970930576324463\n",
      "Epoch 1830, Loss: 1.174143224954605, Final Batch Loss: 0.36162665486335754\n",
      "Epoch 1831, Loss: 1.3191500008106232, Final Batch Loss: 0.4665631353855133\n",
      "Epoch 1832, Loss: 1.2028183937072754, Final Batch Loss: 0.3992939889431\n",
      "Epoch 1833, Loss: 1.0730956494808197, Final Batch Loss: 0.28190112113952637\n",
      "Epoch 1834, Loss: 1.2224439680576324, Final Batch Loss: 0.39016684889793396\n",
      "Epoch 1835, Loss: 1.1216141283512115, Final Batch Loss: 0.3426951766014099\n",
      "Epoch 1836, Loss: 1.1826873123645782, Final Batch Loss: 0.37165367603302\n",
      "Epoch 1837, Loss: 1.2602872550487518, Final Batch Loss: 0.4268380403518677\n",
      "Epoch 1838, Loss: 1.1568545997142792, Final Batch Loss: 0.39822113513946533\n",
      "Epoch 1839, Loss: 1.178985357284546, Final Batch Loss: 0.4438092112541199\n",
      "Epoch 1840, Loss: 1.2820313274860382, Final Batch Loss: 0.424504816532135\n",
      "Epoch 1841, Loss: 1.1270771324634552, Final Batch Loss: 0.4114929735660553\n",
      "Epoch 1842, Loss: 1.2315747141838074, Final Batch Loss: 0.4344444274902344\n",
      "Epoch 1843, Loss: 1.2788447737693787, Final Batch Loss: 0.4980441629886627\n",
      "Epoch 1844, Loss: 1.2718392312526703, Final Batch Loss: 0.4515826404094696\n",
      "Epoch 1845, Loss: 1.2311372756958008, Final Batch Loss: 0.4099372327327728\n",
      "Epoch 1846, Loss: 1.2883493602275848, Final Batch Loss: 0.33889976143836975\n",
      "Epoch 1847, Loss: 1.0948008596897125, Final Batch Loss: 0.3373309373855591\n",
      "Epoch 1848, Loss: 1.2150975167751312, Final Batch Loss: 0.3743533790111542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1849, Loss: 1.2256740927696228, Final Batch Loss: 0.4653998911380768\n",
      "Epoch 1850, Loss: 1.150513470172882, Final Batch Loss: 0.3935542702674866\n",
      "Epoch 1851, Loss: 1.1241753995418549, Final Batch Loss: 0.44626736640930176\n",
      "Epoch 1852, Loss: 1.3088566064834595, Final Batch Loss: 0.523725152015686\n",
      "Epoch 1853, Loss: 1.2253901362419128, Final Batch Loss: 0.4394114315509796\n",
      "Epoch 1854, Loss: 1.233234018087387, Final Batch Loss: 0.48780253529548645\n",
      "Epoch 1855, Loss: 1.166176199913025, Final Batch Loss: 0.48390206694602966\n",
      "Epoch 1856, Loss: 1.1709155440330505, Final Batch Loss: 0.3861350417137146\n",
      "Epoch 1857, Loss: 1.2725048065185547, Final Batch Loss: 0.47167840600013733\n",
      "Epoch 1858, Loss: 1.151684433221817, Final Batch Loss: 0.3408488631248474\n",
      "Epoch 1859, Loss: 1.246095448732376, Final Batch Loss: 0.4291355311870575\n",
      "Epoch 1860, Loss: 1.2362125217914581, Final Batch Loss: 0.3976920247077942\n",
      "Epoch 1861, Loss: 1.2029250860214233, Final Batch Loss: 0.4838212728500366\n",
      "Epoch 1862, Loss: 1.1265140771865845, Final Batch Loss: 0.39748281240463257\n",
      "Epoch 1863, Loss: 1.1217187941074371, Final Batch Loss: 0.2816353142261505\n",
      "Epoch 1864, Loss: 1.1310604214668274, Final Batch Loss: 0.3549768030643463\n",
      "Epoch 1865, Loss: 1.1049917340278625, Final Batch Loss: 0.38577818870544434\n",
      "Epoch 1866, Loss: 1.2712596654891968, Final Batch Loss: 0.45113980770111084\n",
      "Epoch 1867, Loss: 1.1574758887290955, Final Batch Loss: 0.33158138394355774\n",
      "Epoch 1868, Loss: 1.282185584306717, Final Batch Loss: 0.5231998562812805\n",
      "Epoch 1869, Loss: 1.203300565481186, Final Batch Loss: 0.45313191413879395\n",
      "Epoch 1870, Loss: 1.1891289353370667, Final Batch Loss: 0.4074985086917877\n",
      "Epoch 1871, Loss: 1.1419110894203186, Final Batch Loss: 0.39077240228652954\n",
      "Epoch 1872, Loss: 1.1361747682094574, Final Batch Loss: 0.462112158536911\n",
      "Epoch 1873, Loss: 1.074872374534607, Final Batch Loss: 0.33690792322158813\n",
      "Epoch 1874, Loss: 1.1314858496189117, Final Batch Loss: 0.3263116180896759\n",
      "Epoch 1875, Loss: 1.1703082621097565, Final Batch Loss: 0.44300299882888794\n",
      "Epoch 1876, Loss: 1.1895975172519684, Final Batch Loss: 0.3678942918777466\n",
      "Epoch 1877, Loss: 1.094726413488388, Final Batch Loss: 0.3124939501285553\n",
      "Epoch 1878, Loss: 1.1580438911914825, Final Batch Loss: 0.37027570605278015\n",
      "Epoch 1879, Loss: 1.2127586901187897, Final Batch Loss: 0.46999651193618774\n",
      "Epoch 1880, Loss: 1.1678345799446106, Final Batch Loss: 0.4077760875225067\n",
      "Epoch 1881, Loss: 1.1985875070095062, Final Batch Loss: 0.3569052219390869\n",
      "Epoch 1882, Loss: 1.1910904049873352, Final Batch Loss: 0.41231417655944824\n",
      "Epoch 1883, Loss: 1.3253768384456635, Final Batch Loss: 0.39497295022010803\n",
      "Epoch 1884, Loss: 1.2218250930309296, Final Batch Loss: 0.45661255717277527\n",
      "Epoch 1885, Loss: 1.1334420442581177, Final Batch Loss: 0.39058247208595276\n",
      "Epoch 1886, Loss: 1.196437805891037, Final Batch Loss: 0.3521033227443695\n",
      "Epoch 1887, Loss: 1.2276427745819092, Final Batch Loss: 0.4121766984462738\n",
      "Epoch 1888, Loss: 1.2591593265533447, Final Batch Loss: 0.48385384678840637\n",
      "Epoch 1889, Loss: 1.1600249111652374, Final Batch Loss: 0.40003976225852966\n",
      "Epoch 1890, Loss: 1.194609135389328, Final Batch Loss: 0.4564146399497986\n",
      "Epoch 1891, Loss: 1.3867902755737305, Final Batch Loss: 0.5697077512741089\n",
      "Epoch 1892, Loss: 1.1341837644577026, Final Batch Loss: 0.3570249676704407\n",
      "Epoch 1893, Loss: 1.148299217224121, Final Batch Loss: 0.4013802111148834\n",
      "Epoch 1894, Loss: 1.220295399427414, Final Batch Loss: 0.5019536018371582\n",
      "Epoch 1895, Loss: 1.2007359266281128, Final Batch Loss: 0.38460272550582886\n",
      "Epoch 1896, Loss: 1.2709961831569672, Final Batch Loss: 0.3785251975059509\n",
      "Epoch 1897, Loss: 1.2289324402809143, Final Batch Loss: 0.4215969741344452\n",
      "Epoch 1898, Loss: 1.0997208654880524, Final Batch Loss: 0.31104162335395813\n",
      "Epoch 1899, Loss: 1.2003660798072815, Final Batch Loss: 0.42472824454307556\n",
      "Epoch 1900, Loss: 1.254142940044403, Final Batch Loss: 0.4549867808818817\n",
      "Epoch 1901, Loss: 1.2856219410896301, Final Batch Loss: 0.4956705868244171\n",
      "Epoch 1902, Loss: 1.0698895454406738, Final Batch Loss: 0.31468749046325684\n",
      "Epoch 1903, Loss: 1.1546772420406342, Final Batch Loss: 0.34893283247947693\n",
      "Epoch 1904, Loss: 1.1383762061595917, Final Batch Loss: 0.3849787414073944\n",
      "Epoch 1905, Loss: 1.2212507128715515, Final Batch Loss: 0.375502347946167\n",
      "Epoch 1906, Loss: 1.16978058218956, Final Batch Loss: 0.3898817002773285\n",
      "Epoch 1907, Loss: 1.1711246073246002, Final Batch Loss: 0.4120081663131714\n",
      "Epoch 1908, Loss: 1.211106777191162, Final Batch Loss: 0.39968401193618774\n",
      "Epoch 1909, Loss: 1.2203896641731262, Final Batch Loss: 0.4139213263988495\n",
      "Epoch 1910, Loss: 1.2618748843669891, Final Batch Loss: 0.4380171597003937\n",
      "Epoch 1911, Loss: 1.2109653949737549, Final Batch Loss: 0.4504709839820862\n",
      "Epoch 1912, Loss: 1.179958015680313, Final Batch Loss: 0.4082912504673004\n",
      "Epoch 1913, Loss: 1.2119245827198029, Final Batch Loss: 0.3594570755958557\n",
      "Epoch 1914, Loss: 1.1133491694927216, Final Batch Loss: 0.3273237347602844\n",
      "Epoch 1915, Loss: 1.1047624945640564, Final Batch Loss: 0.39765670895576477\n",
      "Epoch 1916, Loss: 1.0662652850151062, Final Batch Loss: 0.2530972957611084\n",
      "Epoch 1917, Loss: 1.2206936180591583, Final Batch Loss: 0.484340637922287\n",
      "Epoch 1918, Loss: 1.2305129766464233, Final Batch Loss: 0.3620746433734894\n",
      "Epoch 1919, Loss: 1.1536125540733337, Final Batch Loss: 0.40700408816337585\n",
      "Epoch 1920, Loss: 1.053917795419693, Final Batch Loss: 0.2787550687789917\n",
      "Epoch 1921, Loss: 1.099361389875412, Final Batch Loss: 0.35889363288879395\n",
      "Epoch 1922, Loss: 1.1848737597465515, Final Batch Loss: 0.4384193420410156\n",
      "Epoch 1923, Loss: 1.0914531350135803, Final Batch Loss: 0.4054752290248871\n",
      "Epoch 1924, Loss: 1.0628213286399841, Final Batch Loss: 0.2864900529384613\n",
      "Epoch 1925, Loss: 1.148299217224121, Final Batch Loss: 0.4222743511199951\n",
      "Epoch 1926, Loss: 1.25585275888443, Final Batch Loss: 0.49369919300079346\n",
      "Epoch 1927, Loss: 1.249462366104126, Final Batch Loss: 0.5356979370117188\n",
      "Epoch 1928, Loss: 1.163328856229782, Final Batch Loss: 0.39290568232536316\n",
      "Epoch 1929, Loss: 1.169882893562317, Final Batch Loss: 0.39243704080581665\n",
      "Epoch 1930, Loss: 1.09724760055542, Final Batch Loss: 0.3423842787742615\n",
      "Epoch 1931, Loss: 1.1968985795974731, Final Batch Loss: 0.4076738655567169\n",
      "Epoch 1932, Loss: 1.1080816090106964, Final Batch Loss: 0.35680335760116577\n",
      "Epoch 1933, Loss: 1.0810960531234741, Final Batch Loss: 0.3268921375274658\n",
      "Epoch 1934, Loss: 1.1818689703941345, Final Batch Loss: 0.33666858077049255\n",
      "Epoch 1935, Loss: 1.248130202293396, Final Batch Loss: 0.482649564743042\n",
      "Epoch 1936, Loss: 1.170729011297226, Final Batch Loss: 0.3625257611274719\n",
      "Epoch 1937, Loss: 1.173274964094162, Final Batch Loss: 0.33216148614883423\n",
      "Epoch 1938, Loss: 1.191089540719986, Final Batch Loss: 0.4052000045776367\n",
      "Epoch 1939, Loss: 1.1156846731901169, Final Batch Loss: 0.24426166713237762\n",
      "Epoch 1940, Loss: 1.1217524111270905, Final Batch Loss: 0.39983829855918884\n",
      "Epoch 1941, Loss: 1.247699111700058, Final Batch Loss: 0.4748607277870178\n",
      "Epoch 1942, Loss: 1.1823177933692932, Final Batch Loss: 0.38301315903663635\n",
      "Epoch 1943, Loss: 1.191441148519516, Final Batch Loss: 0.4217463731765747\n",
      "Epoch 1944, Loss: 1.14430633187294, Final Batch Loss: 0.3702068328857422\n",
      "Epoch 1945, Loss: 1.137445718050003, Final Batch Loss: 0.390272855758667\n",
      "Epoch 1946, Loss: 1.1361494958400726, Final Batch Loss: 0.36975109577178955\n",
      "Epoch 1947, Loss: 1.110201507806778, Final Batch Loss: 0.36173439025878906\n",
      "Epoch 1948, Loss: 1.1329310834407806, Final Batch Loss: 0.30643230676651\n",
      "Epoch 1949, Loss: 1.173616886138916, Final Batch Loss: 0.3896656036376953\n",
      "Epoch 1950, Loss: 1.1828589737415314, Final Batch Loss: 0.38676807284355164\n",
      "Epoch 1951, Loss: 1.334873914718628, Final Batch Loss: 0.5169134736061096\n",
      "Epoch 1952, Loss: 1.1198487877845764, Final Batch Loss: 0.34668490290641785\n",
      "Epoch 1953, Loss: 1.1134376227855682, Final Batch Loss: 0.4176609218120575\n",
      "Epoch 1954, Loss: 1.1446250975131989, Final Batch Loss: 0.39504313468933105\n",
      "Epoch 1955, Loss: 1.1957311630249023, Final Batch Loss: 0.35776761174201965\n",
      "Epoch 1956, Loss: 1.1531740128993988, Final Batch Loss: 0.43980899453163147\n",
      "Epoch 1957, Loss: 1.1500059962272644, Final Batch Loss: 0.35538122057914734\n",
      "Epoch 1958, Loss: 1.274224191904068, Final Batch Loss: 0.47924306988716125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1959, Loss: 1.1281071305274963, Final Batch Loss: 0.36587849259376526\n",
      "Epoch 1960, Loss: 1.2167852222919464, Final Batch Loss: 0.3844510614871979\n",
      "Epoch 1961, Loss: 1.1415138244628906, Final Batch Loss: 0.4093644917011261\n",
      "Epoch 1962, Loss: 1.1702775657176971, Final Batch Loss: 0.3876549303531647\n",
      "Epoch 1963, Loss: 1.0424213707447052, Final Batch Loss: 0.3196961581707001\n",
      "Epoch 1964, Loss: 1.2382190227508545, Final Batch Loss: 0.40006840229034424\n",
      "Epoch 1965, Loss: 1.1515713334083557, Final Batch Loss: 0.3137936294078827\n",
      "Epoch 1966, Loss: 0.9933348596096039, Final Batch Loss: 0.2860932946205139\n",
      "Epoch 1967, Loss: 1.150851458311081, Final Batch Loss: 0.4407970905303955\n",
      "Epoch 1968, Loss: 1.1230846047401428, Final Batch Loss: 0.34641578793525696\n",
      "Epoch 1969, Loss: 1.1158879399299622, Final Batch Loss: 0.3346404731273651\n",
      "Epoch 1970, Loss: 1.0523601472377777, Final Batch Loss: 0.3707551956176758\n",
      "Epoch 1971, Loss: 1.1552431285381317, Final Batch Loss: 0.35146957635879517\n",
      "Epoch 1972, Loss: 1.19940584897995, Final Batch Loss: 0.47988027334213257\n",
      "Epoch 1973, Loss: 1.2643744945526123, Final Batch Loss: 0.49620112776756287\n",
      "Epoch 1974, Loss: 0.9799836277961731, Final Batch Loss: 0.2594417631626129\n",
      "Epoch 1975, Loss: 1.1650661826133728, Final Batch Loss: 0.4128181040287018\n",
      "Epoch 1976, Loss: 1.1306766867637634, Final Batch Loss: 0.3511296510696411\n",
      "Epoch 1977, Loss: 1.225638061761856, Final Batch Loss: 0.43547743558883667\n",
      "Epoch 1978, Loss: 1.1761605441570282, Final Batch Loss: 0.4171519875526428\n",
      "Epoch 1979, Loss: 1.2214916348457336, Final Batch Loss: 0.3993690609931946\n",
      "Epoch 1980, Loss: 1.2276903688907623, Final Batch Loss: 0.5016769766807556\n",
      "Epoch 1981, Loss: 1.2082294821739197, Final Batch Loss: 0.44780638813972473\n",
      "Epoch 1982, Loss: 1.0677663087844849, Final Batch Loss: 0.2683461904525757\n",
      "Epoch 1983, Loss: 1.2500975131988525, Final Batch Loss: 0.40072134137153625\n",
      "Epoch 1984, Loss: 1.1829476654529572, Final Batch Loss: 0.4443739950656891\n",
      "Epoch 1985, Loss: 1.1472015678882599, Final Batch Loss: 0.2926998734474182\n",
      "Epoch 1986, Loss: 1.1765254735946655, Final Batch Loss: 0.40147003531455994\n",
      "Epoch 1987, Loss: 1.1355226933956146, Final Batch Loss: 0.35353437066078186\n",
      "Epoch 1988, Loss: 1.128519594669342, Final Batch Loss: 0.37637385725975037\n",
      "Epoch 1989, Loss: 1.0704931020736694, Final Batch Loss: 0.2966806888580322\n",
      "Epoch 1990, Loss: 1.0457946360111237, Final Batch Loss: 0.35617369413375854\n",
      "Epoch 1991, Loss: 1.1883047223091125, Final Batch Loss: 0.36440524458885193\n",
      "Epoch 1992, Loss: 1.1712283790111542, Final Batch Loss: 0.36071568727493286\n",
      "Epoch 1993, Loss: 1.2039274275302887, Final Batch Loss: 0.4759112000465393\n",
      "Epoch 1994, Loss: 1.1625615358352661, Final Batch Loss: 0.32393378019332886\n",
      "Epoch 1995, Loss: 1.1282833814620972, Final Batch Loss: 0.33379581570625305\n",
      "Epoch 1996, Loss: 1.173119843006134, Final Batch Loss: 0.4502461552619934\n",
      "Epoch 1997, Loss: 1.1135607957839966, Final Batch Loss: 0.31260573863983154\n",
      "Epoch 1998, Loss: 1.218718558549881, Final Batch Loss: 0.46544939279556274\n",
      "Epoch 1999, Loss: 1.106951892375946, Final Batch Loss: 0.35126158595085144\n",
      "Epoch 2000, Loss: 1.2159520983695984, Final Batch Loss: 0.44159069657325745\n",
      "Epoch 2001, Loss: 1.2133689522743225, Final Batch Loss: 0.40475723147392273\n",
      "Epoch 2002, Loss: 1.0528773367404938, Final Batch Loss: 0.3120850920677185\n",
      "Epoch 2003, Loss: 1.0557433068752289, Final Batch Loss: 0.36378926038742065\n",
      "Epoch 2004, Loss: 1.0306482315063477, Final Batch Loss: 0.2986832559108734\n",
      "Epoch 2005, Loss: 1.0954983532428741, Final Batch Loss: 0.3055008351802826\n",
      "Epoch 2006, Loss: 1.1008448600769043, Final Batch Loss: 0.29386621713638306\n",
      "Epoch 2007, Loss: 1.1415329277515411, Final Batch Loss: 0.36984875798225403\n",
      "Epoch 2008, Loss: 1.1181027591228485, Final Batch Loss: 0.3424012362957001\n",
      "Epoch 2009, Loss: 1.0866515338420868, Final Batch Loss: 0.317930668592453\n",
      "Epoch 2010, Loss: 1.0974884927272797, Final Batch Loss: 0.3584952652454376\n",
      "Epoch 2011, Loss: 1.1867420077323914, Final Batch Loss: 0.41645607352256775\n",
      "Epoch 2012, Loss: 1.1432364284992218, Final Batch Loss: 0.43328922986984253\n",
      "Epoch 2013, Loss: 1.1328890025615692, Final Batch Loss: 0.3554479777812958\n",
      "Epoch 2014, Loss: 1.1304645538330078, Final Batch Loss: 0.42954763770103455\n",
      "Epoch 2015, Loss: 1.1263314485549927, Final Batch Loss: 0.37475258111953735\n",
      "Epoch 2016, Loss: 1.1089016199111938, Final Batch Loss: 0.3708637058734894\n",
      "Epoch 2017, Loss: 1.1688998937606812, Final Batch Loss: 0.3571833670139313\n",
      "Epoch 2018, Loss: 1.089319884777069, Final Batch Loss: 0.38856804370880127\n",
      "Epoch 2019, Loss: 1.1174080669879913, Final Batch Loss: 0.392706036567688\n",
      "Epoch 2020, Loss: 1.0289227366447449, Final Batch Loss: 0.37671521306037903\n",
      "Epoch 2021, Loss: 1.1402409672737122, Final Batch Loss: 0.3783164620399475\n",
      "Epoch 2022, Loss: 1.1399438083171844, Final Batch Loss: 0.41563376784324646\n",
      "Epoch 2023, Loss: 1.091914415359497, Final Batch Loss: 0.35570982098579407\n",
      "Epoch 2024, Loss: 1.1296794414520264, Final Batch Loss: 0.40054452419281006\n",
      "Epoch 2025, Loss: 1.2699860632419586, Final Batch Loss: 0.45507586002349854\n",
      "Epoch 2026, Loss: 1.3207947313785553, Final Batch Loss: 0.5820504426956177\n",
      "Epoch 2027, Loss: 1.1207033097743988, Final Batch Loss: 0.41083499789237976\n",
      "Epoch 2028, Loss: 1.209520548582077, Final Batch Loss: 0.4542580842971802\n",
      "Epoch 2029, Loss: 1.1035707592964172, Final Batch Loss: 0.38464412093162537\n",
      "Epoch 2030, Loss: 1.1691138446331024, Final Batch Loss: 0.5145111680030823\n",
      "Epoch 2031, Loss: 1.1697587966918945, Final Batch Loss: 0.3775033950805664\n",
      "Epoch 2032, Loss: 1.1760912239551544, Final Batch Loss: 0.3786153793334961\n",
      "Epoch 2033, Loss: 1.2261859774589539, Final Batch Loss: 0.4137190580368042\n",
      "Epoch 2034, Loss: 1.1237499415874481, Final Batch Loss: 0.34490475058555603\n",
      "Epoch 2035, Loss: 1.0981409549713135, Final Batch Loss: 0.3850153386592865\n",
      "Epoch 2036, Loss: 1.122980535030365, Final Batch Loss: 0.3952949643135071\n",
      "Epoch 2037, Loss: 1.1084244847297668, Final Batch Loss: 0.42891833186149597\n",
      "Epoch 2038, Loss: 1.0522826313972473, Final Batch Loss: 0.3220949172973633\n",
      "Epoch 2039, Loss: 1.1446689665317535, Final Batch Loss: 0.3709741532802582\n",
      "Epoch 2040, Loss: 1.1525635123252869, Final Batch Loss: 0.35635560750961304\n",
      "Epoch 2041, Loss: 1.189137041568756, Final Batch Loss: 0.40003708004951477\n",
      "Epoch 2042, Loss: 1.224905550479889, Final Batch Loss: 0.4249923527240753\n",
      "Epoch 2043, Loss: 1.089176207780838, Final Batch Loss: 0.3582051396369934\n",
      "Epoch 2044, Loss: 1.17079558968544, Final Batch Loss: 0.39493483304977417\n",
      "Epoch 2045, Loss: 1.144578903913498, Final Batch Loss: 0.42737317085266113\n",
      "Epoch 2046, Loss: 1.079676777124405, Final Batch Loss: 0.3201010525226593\n",
      "Epoch 2047, Loss: 1.143581211566925, Final Batch Loss: 0.44643738865852356\n",
      "Epoch 2048, Loss: 1.0955798029899597, Final Batch Loss: 0.33730998635292053\n",
      "Epoch 2049, Loss: 1.06229367852211, Final Batch Loss: 0.3917488157749176\n",
      "Epoch 2050, Loss: 1.1484130918979645, Final Batch Loss: 0.37881895899772644\n",
      "Epoch 2051, Loss: 1.1187783777713776, Final Batch Loss: 0.3034077286720276\n",
      "Epoch 2052, Loss: 1.1058598756790161, Final Batch Loss: 0.4240931570529938\n",
      "Epoch 2053, Loss: 1.123970240354538, Final Batch Loss: 0.3304729461669922\n",
      "Epoch 2054, Loss: 1.1169212758541107, Final Batch Loss: 0.37920984625816345\n",
      "Epoch 2055, Loss: 1.1245139241218567, Final Batch Loss: 0.33139386773109436\n",
      "Epoch 2056, Loss: 1.1375283598899841, Final Batch Loss: 0.321023553609848\n",
      "Epoch 2057, Loss: 1.1094560027122498, Final Batch Loss: 0.3458312749862671\n",
      "Epoch 2058, Loss: 1.110822319984436, Final Batch Loss: 0.28462091088294983\n",
      "Epoch 2059, Loss: 1.211633026599884, Final Batch Loss: 0.4238284230232239\n",
      "Epoch 2060, Loss: 1.0752202868461609, Final Batch Loss: 0.3108697533607483\n",
      "Epoch 2061, Loss: 1.1025413274765015, Final Batch Loss: 0.4370722472667694\n",
      "Epoch 2062, Loss: 1.1894943118095398, Final Batch Loss: 0.4065345823764801\n",
      "Epoch 2063, Loss: 1.1802763938903809, Final Batch Loss: 0.4173685908317566\n",
      "Epoch 2064, Loss: 1.1219691932201385, Final Batch Loss: 0.33191806077957153\n",
      "Epoch 2065, Loss: 1.1428176164627075, Final Batch Loss: 0.3998039662837982\n",
      "Epoch 2066, Loss: 1.2037879824638367, Final Batch Loss: 0.42443954944610596\n",
      "Epoch 2067, Loss: 1.1256436705589294, Final Batch Loss: 0.4067539870738983\n",
      "Epoch 2068, Loss: 1.11385840177536, Final Batch Loss: 0.3027057349681854\n",
      "Epoch 2069, Loss: 1.0999889373779297, Final Batch Loss: 0.37643682956695557\n",
      "Epoch 2070, Loss: 0.9214156568050385, Final Batch Loss: 0.26573488116264343\n",
      "Epoch 2071, Loss: 1.2720071375370026, Final Batch Loss: 0.4334190785884857\n",
      "Epoch 2072, Loss: 1.0682456493377686, Final Batch Loss: 0.42241859436035156\n",
      "Epoch 2073, Loss: 1.2362995147705078, Final Batch Loss: 0.40636980533599854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2074, Loss: 1.1244205832481384, Final Batch Loss: 0.4077197313308716\n",
      "Epoch 2075, Loss: 1.0157796442508698, Final Batch Loss: 0.33572134375572205\n",
      "Epoch 2076, Loss: 1.1356833577156067, Final Batch Loss: 0.42390522360801697\n",
      "Epoch 2077, Loss: 1.0817241668701172, Final Batch Loss: 0.39006564021110535\n",
      "Epoch 2078, Loss: 1.0999740064144135, Final Batch Loss: 0.358688622713089\n",
      "Epoch 2079, Loss: 1.1415833234786987, Final Batch Loss: 0.41270172595977783\n",
      "Epoch 2080, Loss: 1.1218194961547852, Final Batch Loss: 0.41086387634277344\n",
      "Epoch 2081, Loss: 1.0392606556415558, Final Batch Loss: 0.333467036485672\n",
      "Epoch 2082, Loss: 1.060653269290924, Final Batch Loss: 0.4131786525249481\n",
      "Epoch 2083, Loss: 1.0947213768959045, Final Batch Loss: 0.3385200798511505\n",
      "Epoch 2084, Loss: 1.1127856373786926, Final Batch Loss: 0.4120177626609802\n",
      "Epoch 2085, Loss: 1.1805491745471954, Final Batch Loss: 0.42073142528533936\n",
      "Epoch 2086, Loss: 1.1686503291130066, Final Batch Loss: 0.44634291529655457\n",
      "Epoch 2087, Loss: 1.2216356694698334, Final Batch Loss: 0.40661656856536865\n",
      "Epoch 2088, Loss: 1.2138560116291046, Final Batch Loss: 0.48201078176498413\n",
      "Epoch 2089, Loss: 1.1291318833827972, Final Batch Loss: 0.35594430565834045\n",
      "Epoch 2090, Loss: 1.093113750219345, Final Batch Loss: 0.36665624380111694\n",
      "Epoch 2091, Loss: 1.1280979812145233, Final Batch Loss: 0.3672862946987152\n",
      "Epoch 2092, Loss: 0.9934052526950836, Final Batch Loss: 0.24515947699546814\n",
      "Epoch 2093, Loss: 1.2835645079612732, Final Batch Loss: 0.4277626574039459\n",
      "Epoch 2094, Loss: 1.1520243287086487, Final Batch Loss: 0.3926239013671875\n",
      "Epoch 2095, Loss: 1.2560333907604218, Final Batch Loss: 0.4798458516597748\n",
      "Epoch 2096, Loss: 1.102898269891739, Final Batch Loss: 0.36821678280830383\n",
      "Epoch 2097, Loss: 1.1300953924655914, Final Batch Loss: 0.3750961124897003\n",
      "Epoch 2098, Loss: 1.0961053371429443, Final Batch Loss: 0.3575817346572876\n",
      "Epoch 2099, Loss: 1.0439358353614807, Final Batch Loss: 0.32425159215927124\n",
      "Epoch 2100, Loss: 1.0723746418952942, Final Batch Loss: 0.3474365174770355\n",
      "Epoch 2101, Loss: 1.2690915167331696, Final Batch Loss: 0.5564119815826416\n",
      "Epoch 2102, Loss: 1.0268151462078094, Final Batch Loss: 0.32463619112968445\n",
      "Epoch 2103, Loss: 1.104319989681244, Final Batch Loss: 0.3865032196044922\n",
      "Epoch 2104, Loss: 1.0294920802116394, Final Batch Loss: 0.3751913011074066\n",
      "Epoch 2105, Loss: 1.0958472192287445, Final Batch Loss: 0.3989778459072113\n",
      "Epoch 2106, Loss: 1.1930363476276398, Final Batch Loss: 0.33265000581741333\n",
      "Epoch 2107, Loss: 1.0237607061862946, Final Batch Loss: 0.31659719347953796\n",
      "Epoch 2108, Loss: 1.1520723402500153, Final Batch Loss: 0.3533068597316742\n",
      "Epoch 2109, Loss: 1.1523807048797607, Final Batch Loss: 0.39026299118995667\n",
      "Epoch 2110, Loss: 1.1240401566028595, Final Batch Loss: 0.47493433952331543\n",
      "Epoch 2111, Loss: 1.2078329622745514, Final Batch Loss: 0.3903552293777466\n",
      "Epoch 2112, Loss: 1.11364084482193, Final Batch Loss: 0.33670076727867126\n",
      "Epoch 2113, Loss: 1.0527534484863281, Final Batch Loss: 0.3542923331260681\n",
      "Epoch 2114, Loss: 1.1006892621517181, Final Batch Loss: 0.34580886363983154\n",
      "Epoch 2115, Loss: 1.2033064365386963, Final Batch Loss: 0.44175493717193604\n",
      "Epoch 2116, Loss: 1.1240772306919098, Final Batch Loss: 0.4290938079357147\n",
      "Epoch 2117, Loss: 1.1423383951187134, Final Batch Loss: 0.37814009189605713\n",
      "Epoch 2118, Loss: 1.1321338713169098, Final Batch Loss: 0.42568719387054443\n",
      "Epoch 2119, Loss: 1.1462969481945038, Final Batch Loss: 0.4572403132915497\n",
      "Epoch 2120, Loss: 1.1051631569862366, Final Batch Loss: 0.3469961881637573\n",
      "Epoch 2121, Loss: 1.1084493696689606, Final Batch Loss: 0.4012812376022339\n",
      "Epoch 2122, Loss: 1.0839791297912598, Final Batch Loss: 0.3112177550792694\n",
      "Epoch 2123, Loss: 1.1270399391651154, Final Batch Loss: 0.4080083966255188\n",
      "Epoch 2124, Loss: 1.1491238176822662, Final Batch Loss: 0.39453834295272827\n",
      "Epoch 2125, Loss: 1.0126245319843292, Final Batch Loss: 0.3237099349498749\n",
      "Epoch 2126, Loss: 1.1602035760879517, Final Batch Loss: 0.34864482283592224\n",
      "Epoch 2127, Loss: 1.0160521864891052, Final Batch Loss: 0.33329230546951294\n",
      "Epoch 2128, Loss: 1.1590820848941803, Final Batch Loss: 0.44379910826683044\n",
      "Epoch 2129, Loss: 1.1500030159950256, Final Batch Loss: 0.4181782007217407\n",
      "Epoch 2130, Loss: 0.9698197543621063, Final Batch Loss: 0.33632007241249084\n",
      "Epoch 2131, Loss: 1.0923793613910675, Final Batch Loss: 0.39861711859703064\n",
      "Epoch 2132, Loss: 0.9987594783306122, Final Batch Loss: 0.2572457492351532\n",
      "Epoch 2133, Loss: 1.0777976512908936, Final Batch Loss: 0.44223612546920776\n",
      "Epoch 2134, Loss: 1.0403099656105042, Final Batch Loss: 0.3285790979862213\n",
      "Epoch 2135, Loss: 1.110657513141632, Final Batch Loss: 0.32882192730903625\n",
      "Epoch 2136, Loss: 1.0480615198612213, Final Batch Loss: 0.36246272921562195\n",
      "Epoch 2137, Loss: 0.9713053703308105, Final Batch Loss: 0.33453962206840515\n",
      "Epoch 2138, Loss: 1.0822893381118774, Final Batch Loss: 0.35637104511260986\n",
      "Epoch 2139, Loss: 1.1687785685062408, Final Batch Loss: 0.4547780752182007\n",
      "Epoch 2140, Loss: 1.111502081155777, Final Batch Loss: 0.36378535628318787\n",
      "Epoch 2141, Loss: 1.0167969167232513, Final Batch Loss: 0.3470800518989563\n",
      "Epoch 2142, Loss: 1.0289304852485657, Final Batch Loss: 0.2970462143421173\n",
      "Epoch 2143, Loss: 1.0945526361465454, Final Batch Loss: 0.36175471544265747\n",
      "Epoch 2144, Loss: 0.9865512847900391, Final Batch Loss: 0.2801382839679718\n",
      "Epoch 2145, Loss: 1.1604327261447906, Final Batch Loss: 0.42559921741485596\n",
      "Epoch 2146, Loss: 1.0808948874473572, Final Batch Loss: 0.3089805245399475\n",
      "Epoch 2147, Loss: 1.065889447927475, Final Batch Loss: 0.3600037097930908\n",
      "Epoch 2148, Loss: 1.0900845229625702, Final Batch Loss: 0.3022943139076233\n",
      "Epoch 2149, Loss: 1.0602731704711914, Final Batch Loss: 0.3363480865955353\n",
      "Epoch 2150, Loss: 1.0671769082546234, Final Batch Loss: 0.347547709941864\n",
      "Epoch 2151, Loss: 1.059700608253479, Final Batch Loss: 0.3069215714931488\n",
      "Epoch 2152, Loss: 1.0506339371204376, Final Batch Loss: 0.25193294882774353\n",
      "Epoch 2153, Loss: 0.9515859186649323, Final Batch Loss: 0.23075884580612183\n",
      "Epoch 2154, Loss: 1.093989759683609, Final Batch Loss: 0.30091989040374756\n",
      "Epoch 2155, Loss: 1.090023398399353, Final Batch Loss: 0.33976176381111145\n",
      "Epoch 2156, Loss: 1.1893626153469086, Final Batch Loss: 0.4248066842556\n",
      "Epoch 2157, Loss: 1.2134765982627869, Final Batch Loss: 0.46331435441970825\n",
      "Epoch 2158, Loss: 1.0485332608222961, Final Batch Loss: 0.3949663043022156\n",
      "Epoch 2159, Loss: 1.1233251988887787, Final Batch Loss: 0.3612799346446991\n",
      "Epoch 2160, Loss: 1.1136605441570282, Final Batch Loss: 0.4044337868690491\n",
      "Epoch 2161, Loss: 1.0373523235321045, Final Batch Loss: 0.3122480809688568\n",
      "Epoch 2162, Loss: 0.9536670446395874, Final Batch Loss: 0.28462573885917664\n",
      "Epoch 2163, Loss: 1.0627736449241638, Final Batch Loss: 0.4042997658252716\n",
      "Epoch 2164, Loss: 1.196020931005478, Final Batch Loss: 0.3955345153808594\n",
      "Epoch 2165, Loss: 1.0028504133224487, Final Batch Loss: 0.28420424461364746\n",
      "Epoch 2166, Loss: 1.1356058418750763, Final Batch Loss: 0.47464174032211304\n",
      "Epoch 2167, Loss: 1.1248026192188263, Final Batch Loss: 0.41345587372779846\n",
      "Epoch 2168, Loss: 1.1026759147644043, Final Batch Loss: 0.30673858523368835\n",
      "Epoch 2169, Loss: 1.053001493215561, Final Batch Loss: 0.30010464787483215\n",
      "Epoch 2170, Loss: 1.1384311020374298, Final Batch Loss: 0.4512454569339752\n",
      "Epoch 2171, Loss: 1.1140690445899963, Final Batch Loss: 0.33286744356155396\n",
      "Epoch 2172, Loss: 1.1124858260154724, Final Batch Loss: 0.3640699088573456\n",
      "Epoch 2173, Loss: 1.126471370458603, Final Batch Loss: 0.3987255394458771\n",
      "Epoch 2174, Loss: 1.2016292810440063, Final Batch Loss: 0.43985143303871155\n",
      "Epoch 2175, Loss: 1.075987696647644, Final Batch Loss: 0.36875417828559875\n",
      "Epoch 2176, Loss: 1.1400148570537567, Final Batch Loss: 0.3849082887172699\n",
      "Epoch 2177, Loss: 1.1562474370002747, Final Batch Loss: 0.2839985489845276\n",
      "Epoch 2178, Loss: 1.0834063589572906, Final Batch Loss: 0.3676271140575409\n",
      "Epoch 2179, Loss: 0.971823662519455, Final Batch Loss: 0.3479140102863312\n",
      "Epoch 2180, Loss: 1.1057527661323547, Final Batch Loss: 0.36889663338661194\n",
      "Epoch 2181, Loss: 1.1568902134895325, Final Batch Loss: 0.3510711193084717\n",
      "Epoch 2182, Loss: 1.14366415143013, Final Batch Loss: 0.37413397431373596\n",
      "Epoch 2183, Loss: 1.0364521741867065, Final Batch Loss: 0.3633699417114258\n",
      "Epoch 2184, Loss: 1.1379449665546417, Final Batch Loss: 0.41037166118621826\n",
      "Epoch 2185, Loss: 1.0576781332492828, Final Batch Loss: 0.32494279742240906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2186, Loss: 1.0339671075344086, Final Batch Loss: 0.3073977828025818\n",
      "Epoch 2187, Loss: 1.0802370607852936, Final Batch Loss: 0.3562770187854767\n",
      "Epoch 2188, Loss: 1.0429154336452484, Final Batch Loss: 0.35405606031417847\n",
      "Epoch 2189, Loss: 1.0051790177822113, Final Batch Loss: 0.31167927384376526\n",
      "Epoch 2190, Loss: 1.003860980272293, Final Batch Loss: 0.3686549961566925\n",
      "Epoch 2191, Loss: 1.14120152592659, Final Batch Loss: 0.45439839363098145\n",
      "Epoch 2192, Loss: 1.0169870853424072, Final Batch Loss: 0.37560927867889404\n",
      "Epoch 2193, Loss: 0.9459743499755859, Final Batch Loss: 0.3150688707828522\n",
      "Epoch 2194, Loss: 1.0540109872817993, Final Batch Loss: 0.4290187358856201\n",
      "Epoch 2195, Loss: 1.0891427397727966, Final Batch Loss: 0.36881768703460693\n",
      "Epoch 2196, Loss: 1.0854456424713135, Final Batch Loss: 0.45297977328300476\n",
      "Epoch 2197, Loss: 1.0096609592437744, Final Batch Loss: 0.32761937379837036\n",
      "Epoch 2198, Loss: 1.1442208290100098, Final Batch Loss: 0.45533889532089233\n",
      "Epoch 2199, Loss: 1.0188703536987305, Final Batch Loss: 0.2704586684703827\n",
      "Epoch 2200, Loss: 1.0155002772808075, Final Batch Loss: 0.32881662249565125\n",
      "Epoch 2201, Loss: 1.1580906212329865, Final Batch Loss: 0.4234299063682556\n",
      "Epoch 2202, Loss: 1.0676837861537933, Final Batch Loss: 0.3702208399772644\n",
      "Epoch 2203, Loss: 1.2093233168125153, Final Batch Loss: 0.3798671066761017\n",
      "Epoch 2204, Loss: 1.100845843553543, Final Batch Loss: 0.3282674551010132\n",
      "Epoch 2205, Loss: 1.0571455657482147, Final Batch Loss: 0.35468974709510803\n",
      "Epoch 2206, Loss: 1.1244920790195465, Final Batch Loss: 0.43579593300819397\n",
      "Epoch 2207, Loss: 1.0928277671337128, Final Batch Loss: 0.3835887610912323\n",
      "Epoch 2208, Loss: 1.1315386295318604, Final Batch Loss: 0.46459871530532837\n",
      "Epoch 2209, Loss: 1.022463709115982, Final Batch Loss: 0.2716933786869049\n",
      "Epoch 2210, Loss: 1.1639889478683472, Final Batch Loss: 0.44727176427841187\n",
      "Epoch 2211, Loss: 1.0382395684719086, Final Batch Loss: 0.31237390637397766\n",
      "Epoch 2212, Loss: 1.1845397651195526, Final Batch Loss: 0.32825449109077454\n",
      "Epoch 2213, Loss: 1.1040829122066498, Final Batch Loss: 0.3391130566596985\n",
      "Epoch 2214, Loss: 1.0836822390556335, Final Batch Loss: 0.3531019985675812\n",
      "Epoch 2215, Loss: 1.184099406003952, Final Batch Loss: 0.43934866786003113\n",
      "Epoch 2216, Loss: 1.0995814800262451, Final Batch Loss: 0.32854488492012024\n",
      "Epoch 2217, Loss: 1.0361485183238983, Final Batch Loss: 0.2638354003429413\n",
      "Epoch 2218, Loss: 1.1335135400295258, Final Batch Loss: 0.351248562335968\n",
      "Epoch 2219, Loss: 1.1569209694862366, Final Batch Loss: 0.4790777862071991\n",
      "Epoch 2220, Loss: 1.0675211548805237, Final Batch Loss: 0.41971781849861145\n",
      "Epoch 2221, Loss: 1.0121230483055115, Final Batch Loss: 0.3471900522708893\n",
      "Epoch 2222, Loss: 1.0194308757781982, Final Batch Loss: 0.2795296311378479\n",
      "Epoch 2223, Loss: 1.1078810095787048, Final Batch Loss: 0.3695859909057617\n",
      "Epoch 2224, Loss: 1.1246152222156525, Final Batch Loss: 0.3405750095844269\n",
      "Epoch 2225, Loss: 1.1556792855262756, Final Batch Loss: 0.4243548512458801\n",
      "Epoch 2226, Loss: 1.1470768749713898, Final Batch Loss: 0.38544678688049316\n",
      "Epoch 2227, Loss: 1.05815389752388, Final Batch Loss: 0.3008902966976166\n",
      "Epoch 2228, Loss: 1.1917333006858826, Final Batch Loss: 0.4376855194568634\n",
      "Epoch 2229, Loss: 1.0712940990924835, Final Batch Loss: 0.45733171701431274\n",
      "Epoch 2230, Loss: 1.027248203754425, Final Batch Loss: 0.28825777769088745\n",
      "Epoch 2231, Loss: 1.1404739320278168, Final Batch Loss: 0.4044143557548523\n",
      "Epoch 2232, Loss: 1.0441900491714478, Final Batch Loss: 0.4318622946739197\n",
      "Epoch 2233, Loss: 1.0208757817745209, Final Batch Loss: 0.29187923669815063\n",
      "Epoch 2234, Loss: 0.9459248781204224, Final Batch Loss: 0.3010138273239136\n",
      "Epoch 2235, Loss: 1.0803176760673523, Final Batch Loss: 0.34204551577568054\n",
      "Epoch 2236, Loss: 1.0171714127063751, Final Batch Loss: 0.26932692527770996\n",
      "Epoch 2237, Loss: 1.0320886671543121, Final Batch Loss: 0.2876044809818268\n",
      "Epoch 2238, Loss: 1.099466472864151, Final Batch Loss: 0.3290071189403534\n",
      "Epoch 2239, Loss: 0.9936352074146271, Final Batch Loss: 0.36145612597465515\n",
      "Epoch 2240, Loss: 1.0469008386135101, Final Batch Loss: 0.3898104131221771\n",
      "Epoch 2241, Loss: 0.963322103023529, Final Batch Loss: 0.2734519839286804\n",
      "Epoch 2242, Loss: 0.9727333188056946, Final Batch Loss: 0.282151460647583\n",
      "Epoch 2243, Loss: 0.9761412143707275, Final Batch Loss: 0.34439414739608765\n",
      "Epoch 2244, Loss: 0.9914110898971558, Final Batch Loss: 0.28476887941360474\n",
      "Epoch 2245, Loss: 1.1112718284130096, Final Batch Loss: 0.41898906230926514\n",
      "Epoch 2246, Loss: 1.0497239828109741, Final Batch Loss: 0.3246705234050751\n",
      "Epoch 2247, Loss: 0.9269710779190063, Final Batch Loss: 0.2746966481208801\n",
      "Epoch 2248, Loss: 1.079721063375473, Final Batch Loss: 0.3792624771595001\n",
      "Epoch 2249, Loss: 0.995947390794754, Final Batch Loss: 0.32172319293022156\n",
      "Epoch 2250, Loss: 0.9877080917358398, Final Batch Loss: 0.3117481768131256\n",
      "Epoch 2251, Loss: 0.9941443800926208, Final Batch Loss: 0.3058280348777771\n",
      "Epoch 2252, Loss: 1.0652765035629272, Final Batch Loss: 0.33356982469558716\n",
      "Epoch 2253, Loss: 1.0067146122455597, Final Batch Loss: 0.31605619192123413\n",
      "Epoch 2254, Loss: 1.05618816614151, Final Batch Loss: 0.35119879245758057\n",
      "Epoch 2255, Loss: 1.00851371884346, Final Batch Loss: 0.2874891757965088\n",
      "Epoch 2256, Loss: 1.0446109473705292, Final Batch Loss: 0.3595273196697235\n",
      "Epoch 2257, Loss: 1.0789709389209747, Final Batch Loss: 0.3184267580509186\n",
      "Epoch 2258, Loss: 1.170777291059494, Final Batch Loss: 0.4984298348426819\n",
      "Epoch 2259, Loss: 1.0140362083911896, Final Batch Loss: 0.31759411096572876\n",
      "Epoch 2260, Loss: 1.0210323333740234, Final Batch Loss: 0.3730104863643646\n",
      "Epoch 2261, Loss: 1.19313183426857, Final Batch Loss: 0.40598955750465393\n",
      "Epoch 2262, Loss: 1.1109518110752106, Final Batch Loss: 0.3700779974460602\n",
      "Epoch 2263, Loss: 0.9418767392635345, Final Batch Loss: 0.2853919267654419\n",
      "Epoch 2264, Loss: 1.0583271384239197, Final Batch Loss: 0.3399796783924103\n",
      "Epoch 2265, Loss: 1.0764621794223785, Final Batch Loss: 0.3364925980567932\n",
      "Epoch 2266, Loss: 1.049634724855423, Final Batch Loss: 0.3370404839515686\n",
      "Epoch 2267, Loss: 1.1221440732479095, Final Batch Loss: 0.3326321840286255\n",
      "Epoch 2268, Loss: 1.0709026455879211, Final Batch Loss: 0.3727160692214966\n",
      "Epoch 2269, Loss: 1.0323915481567383, Final Batch Loss: 0.27552396059036255\n",
      "Epoch 2270, Loss: 1.1072286367416382, Final Batch Loss: 0.3317546844482422\n",
      "Epoch 2271, Loss: 0.9989154040813446, Final Batch Loss: 0.29230427742004395\n",
      "Epoch 2272, Loss: 1.083128273487091, Final Batch Loss: 0.3959159255027771\n",
      "Epoch 2273, Loss: 0.9999541342258453, Final Batch Loss: 0.3101286292076111\n",
      "Epoch 2274, Loss: 0.9794981628656387, Final Batch Loss: 0.24896611273288727\n",
      "Epoch 2275, Loss: 0.9957810938358307, Final Batch Loss: 0.3434509038925171\n",
      "Epoch 2276, Loss: 1.0281806886196136, Final Batch Loss: 0.35139521956443787\n",
      "Epoch 2277, Loss: 1.0057000517845154, Final Batch Loss: 0.3493739068508148\n",
      "Epoch 2278, Loss: 1.093191236257553, Final Batch Loss: 0.41040918231010437\n",
      "Epoch 2279, Loss: 1.0853170454502106, Final Batch Loss: 0.4209021031856537\n",
      "Epoch 2280, Loss: 0.9078505039215088, Final Batch Loss: 0.27018728852272034\n",
      "Epoch 2281, Loss: 1.1786923110485077, Final Batch Loss: 0.43117663264274597\n",
      "Epoch 2282, Loss: 1.055012047290802, Final Batch Loss: 0.49104243516921997\n",
      "Epoch 2283, Loss: 1.0601244568824768, Final Batch Loss: 0.3427314758300781\n",
      "Epoch 2284, Loss: 1.0812866687774658, Final Batch Loss: 0.361022412776947\n",
      "Epoch 2285, Loss: 1.0277905762195587, Final Batch Loss: 0.3820558190345764\n",
      "Epoch 2286, Loss: 1.1018190681934357, Final Batch Loss: 0.37976813316345215\n",
      "Epoch 2287, Loss: 1.0047356188297272, Final Batch Loss: 0.33894649147987366\n",
      "Epoch 2288, Loss: 0.9856227934360504, Final Batch Loss: 0.29215601086616516\n",
      "Epoch 2289, Loss: 1.0688201189041138, Final Batch Loss: 0.324532687664032\n",
      "Epoch 2290, Loss: 1.0448496043682098, Final Batch Loss: 0.34938082098960876\n",
      "Epoch 2291, Loss: 1.0147170424461365, Final Batch Loss: 0.33840689063072205\n",
      "Epoch 2292, Loss: 1.0342498123645782, Final Batch Loss: 0.29511716961860657\n",
      "Epoch 2293, Loss: 0.9961607456207275, Final Batch Loss: 0.33513712882995605\n",
      "Epoch 2294, Loss: 1.0718469023704529, Final Batch Loss: 0.4012586772441864\n",
      "Epoch 2295, Loss: 1.0168933272361755, Final Batch Loss: 0.3983355164527893\n",
      "Epoch 2296, Loss: 1.0076030790805817, Final Batch Loss: 0.3237791955471039\n",
      "Epoch 2297, Loss: 1.013229250907898, Final Batch Loss: 0.3327314555644989\n",
      "Epoch 2298, Loss: 0.9742345213890076, Final Batch Loss: 0.3386533558368683\n",
      "Epoch 2299, Loss: 1.1066392362117767, Final Batch Loss: 0.4682444930076599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2300, Loss: 1.0426084995269775, Final Batch Loss: 0.38361606001853943\n",
      "Epoch 2301, Loss: 1.2500193119049072, Final Batch Loss: 0.35950931906700134\n",
      "Epoch 2302, Loss: 1.0820361077785492, Final Batch Loss: 0.37995901703834534\n",
      "Epoch 2303, Loss: 0.9703304469585419, Final Batch Loss: 0.3524785339832306\n",
      "Epoch 2304, Loss: 0.8799535036087036, Final Batch Loss: 0.2633892595767975\n",
      "Epoch 2305, Loss: 0.9774988293647766, Final Batch Loss: 0.2927649915218353\n",
      "Epoch 2306, Loss: 1.1028645038604736, Final Batch Loss: 0.34478554129600525\n",
      "Epoch 2307, Loss: 1.0838144421577454, Final Batch Loss: 0.37303927540779114\n",
      "Epoch 2308, Loss: 1.0666109919548035, Final Batch Loss: 0.32293418049812317\n",
      "Epoch 2309, Loss: 0.9749431014060974, Final Batch Loss: 0.30169573426246643\n",
      "Epoch 2310, Loss: 1.0116037130355835, Final Batch Loss: 0.34468090534210205\n",
      "Epoch 2311, Loss: 1.0658970475196838, Final Batch Loss: 0.40108755230903625\n",
      "Epoch 2312, Loss: 1.177390456199646, Final Batch Loss: 0.41686397790908813\n",
      "Epoch 2313, Loss: 1.0399406254291534, Final Batch Loss: 0.3320365250110626\n",
      "Epoch 2314, Loss: 1.028528869152069, Final Batch Loss: 0.30339038372039795\n",
      "Epoch 2315, Loss: 1.0559468865394592, Final Batch Loss: 0.4583105146884918\n",
      "Epoch 2316, Loss: 1.0810169279575348, Final Batch Loss: 0.4016110897064209\n",
      "Epoch 2317, Loss: 1.028885394334793, Final Batch Loss: 0.32877466082572937\n",
      "Epoch 2318, Loss: 0.9813748598098755, Final Batch Loss: 0.30830517411231995\n",
      "Epoch 2319, Loss: 1.0162520110607147, Final Batch Loss: 0.3253856599330902\n",
      "Epoch 2320, Loss: 1.0122037827968597, Final Batch Loss: 0.3686305284500122\n",
      "Epoch 2321, Loss: 1.1300600171089172, Final Batch Loss: 0.4325968623161316\n",
      "Epoch 2322, Loss: 0.9298291206359863, Final Batch Loss: 0.2870122194290161\n",
      "Epoch 2323, Loss: 1.0649735629558563, Final Batch Loss: 0.4175031781196594\n",
      "Epoch 2324, Loss: 1.0365048050880432, Final Batch Loss: 0.2800043821334839\n",
      "Epoch 2325, Loss: 1.01473468542099, Final Batch Loss: 0.3222256600856781\n",
      "Epoch 2326, Loss: 1.0853668749332428, Final Batch Loss: 0.41334736347198486\n",
      "Epoch 2327, Loss: 1.0890560150146484, Final Batch Loss: 0.35618966817855835\n",
      "Epoch 2328, Loss: 1.0321525633335114, Final Batch Loss: 0.32131505012512207\n",
      "Epoch 2329, Loss: 1.1603772044181824, Final Batch Loss: 0.399147093296051\n",
      "Epoch 2330, Loss: 1.0672278106212616, Final Batch Loss: 0.45874154567718506\n",
      "Epoch 2331, Loss: 1.0552011728286743, Final Batch Loss: 0.31111446022987366\n",
      "Epoch 2332, Loss: 1.1457533240318298, Final Batch Loss: 0.43042072653770447\n",
      "Epoch 2333, Loss: 0.9875609874725342, Final Batch Loss: 0.30194151401519775\n",
      "Epoch 2334, Loss: 1.0689168274402618, Final Batch Loss: 0.35812994837760925\n",
      "Epoch 2335, Loss: 0.9380130767822266, Final Batch Loss: 0.29388412833213806\n",
      "Epoch 2336, Loss: 1.1556131839752197, Final Batch Loss: 0.49314284324645996\n",
      "Epoch 2337, Loss: 1.022842824459076, Final Batch Loss: 0.34418416023254395\n",
      "Epoch 2338, Loss: 1.0530942380428314, Final Batch Loss: 0.35353174805641174\n",
      "Epoch 2339, Loss: 0.995683342218399, Final Batch Loss: 0.29577669501304626\n",
      "Epoch 2340, Loss: 0.9791419506072998, Final Batch Loss: 0.32146769762039185\n",
      "Epoch 2341, Loss: 0.9517644047737122, Final Batch Loss: 0.28154462575912476\n",
      "Epoch 2342, Loss: 0.9489213526248932, Final Batch Loss: 0.25351181626319885\n",
      "Epoch 2343, Loss: 0.9145351946353912, Final Batch Loss: 0.2972346544265747\n",
      "Epoch 2344, Loss: 0.90676349401474, Final Batch Loss: 0.33549126982688904\n",
      "Epoch 2345, Loss: 1.023599088191986, Final Batch Loss: 0.28986451029777527\n",
      "Epoch 2346, Loss: 1.0044912099838257, Final Batch Loss: 0.3416147530078888\n",
      "Epoch 2347, Loss: 1.0428598523139954, Final Batch Loss: 0.36375585198402405\n",
      "Epoch 2348, Loss: 1.0784322321414948, Final Batch Loss: 0.37459519505500793\n",
      "Epoch 2349, Loss: 0.9887846112251282, Final Batch Loss: 0.31498435139656067\n",
      "Epoch 2350, Loss: 0.9714262783527374, Final Batch Loss: 0.2770603895187378\n",
      "Epoch 2351, Loss: 1.0226014256477356, Final Batch Loss: 0.35921162366867065\n",
      "Epoch 2352, Loss: 1.0373855531215668, Final Batch Loss: 0.3620849549770355\n",
      "Epoch 2353, Loss: 0.8771326094865799, Final Batch Loss: 0.23574618995189667\n",
      "Epoch 2354, Loss: 1.0509198307991028, Final Batch Loss: 0.32599857449531555\n",
      "Epoch 2355, Loss: 0.9796786904335022, Final Batch Loss: 0.2534236013889313\n",
      "Epoch 2356, Loss: 1.0804231762886047, Final Batch Loss: 0.3804692327976227\n",
      "Epoch 2357, Loss: 0.972777783870697, Final Batch Loss: 0.30923786759376526\n",
      "Epoch 2358, Loss: 0.9644671082496643, Final Batch Loss: 0.3113388419151306\n",
      "Epoch 2359, Loss: 1.0692611634731293, Final Batch Loss: 0.30445215106010437\n",
      "Epoch 2360, Loss: 0.9387570023536682, Final Batch Loss: 0.2857135236263275\n",
      "Epoch 2361, Loss: 0.9182642698287964, Final Batch Loss: 0.27582141757011414\n",
      "Epoch 2362, Loss: 0.9680660665035248, Final Batch Loss: 0.33814355731010437\n",
      "Epoch 2363, Loss: 1.0375519096851349, Final Batch Loss: 0.3029897212982178\n",
      "Epoch 2364, Loss: 1.0204154551029205, Final Batch Loss: 0.3302876055240631\n",
      "Epoch 2365, Loss: 0.927294909954071, Final Batch Loss: 0.294219970703125\n",
      "Epoch 2366, Loss: 1.0008793473243713, Final Batch Loss: 0.31940633058547974\n",
      "Epoch 2367, Loss: 0.9500977993011475, Final Batch Loss: 0.2723811864852905\n",
      "Epoch 2368, Loss: 0.9903141856193542, Final Batch Loss: 0.39291197061538696\n",
      "Epoch 2369, Loss: 1.1964693069458008, Final Batch Loss: 0.4006285071372986\n",
      "Epoch 2370, Loss: 0.9303644001483917, Final Batch Loss: 0.3230876624584198\n",
      "Epoch 2371, Loss: 1.1533264517784119, Final Batch Loss: 0.365214467048645\n",
      "Epoch 2372, Loss: 1.0552650094032288, Final Batch Loss: 0.4049955904483795\n",
      "Epoch 2373, Loss: 1.0384726524353027, Final Batch Loss: 0.3108886182308197\n",
      "Epoch 2374, Loss: 0.9877669215202332, Final Batch Loss: 0.35932719707489014\n",
      "Epoch 2375, Loss: 1.0298155844211578, Final Batch Loss: 0.4027150273323059\n",
      "Epoch 2376, Loss: 0.935311883687973, Final Batch Loss: 0.35194793343544006\n",
      "Epoch 2377, Loss: 1.0709115862846375, Final Batch Loss: 0.35088813304901123\n",
      "Epoch 2378, Loss: 1.03249990940094, Final Batch Loss: 0.3238098621368408\n",
      "Epoch 2379, Loss: 0.9759194850921631, Final Batch Loss: 0.33159714937210083\n",
      "Epoch 2380, Loss: 1.0897773206233978, Final Batch Loss: 0.3635173738002777\n",
      "Epoch 2381, Loss: 1.0370590686798096, Final Batch Loss: 0.3326481878757477\n",
      "Epoch 2382, Loss: 1.0100899040699005, Final Batch Loss: 0.407088965177536\n",
      "Epoch 2383, Loss: 1.0519837737083435, Final Batch Loss: 0.26814234256744385\n",
      "Epoch 2384, Loss: 0.988311380147934, Final Batch Loss: 0.27336499094963074\n",
      "Epoch 2385, Loss: 1.0505762696266174, Final Batch Loss: 0.3312917947769165\n",
      "Epoch 2386, Loss: 1.0358621776103973, Final Batch Loss: 0.3507730960845947\n",
      "Epoch 2387, Loss: 0.985785186290741, Final Batch Loss: 0.2995390295982361\n",
      "Epoch 2388, Loss: 1.1035550236701965, Final Batch Loss: 0.39253515005111694\n",
      "Epoch 2389, Loss: 1.0744850039482117, Final Batch Loss: 0.3896145522594452\n",
      "Epoch 2390, Loss: 1.154253363609314, Final Batch Loss: 0.45278260111808777\n",
      "Epoch 2391, Loss: 0.9823399186134338, Final Batch Loss: 0.35911548137664795\n",
      "Epoch 2392, Loss: 1.017696350812912, Final Batch Loss: 0.3513784110546112\n",
      "Epoch 2393, Loss: 1.0615711212158203, Final Batch Loss: 0.417997807264328\n",
      "Epoch 2394, Loss: 0.9956551492214203, Final Batch Loss: 0.3869006335735321\n",
      "Epoch 2395, Loss: 0.9677256047725677, Final Batch Loss: 0.30058732628822327\n",
      "Epoch 2396, Loss: 1.0819036364555359, Final Batch Loss: 0.3290402293205261\n",
      "Epoch 2397, Loss: 0.9460488557815552, Final Batch Loss: 0.29897186160087585\n",
      "Epoch 2398, Loss: 1.102548986673355, Final Batch Loss: 0.4459645748138428\n",
      "Epoch 2399, Loss: 0.9307031035423279, Final Batch Loss: 0.32798969745635986\n",
      "Epoch 2400, Loss: 0.9300774037837982, Final Batch Loss: 0.30176639556884766\n",
      "Epoch 2401, Loss: 0.9810498058795929, Final Batch Loss: 0.35032331943511963\n",
      "Epoch 2402, Loss: 0.9464503824710846, Final Batch Loss: 0.34463170170783997\n",
      "Epoch 2403, Loss: 1.0292108356952667, Final Batch Loss: 0.3408929705619812\n",
      "Epoch 2404, Loss: 1.0669622719287872, Final Batch Loss: 0.43100959062576294\n",
      "Epoch 2405, Loss: 0.9950478971004486, Final Batch Loss: 0.27454596757888794\n",
      "Epoch 2406, Loss: 1.055567741394043, Final Batch Loss: 0.33652907609939575\n",
      "Epoch 2407, Loss: 1.0169615745544434, Final Batch Loss: 0.3463042080402374\n",
      "Epoch 2408, Loss: 0.9796625971794128, Final Batch Loss: 0.30017778277397156\n",
      "Epoch 2409, Loss: 0.9247307479381561, Final Batch Loss: 0.31464311480522156\n",
      "Epoch 2410, Loss: 1.2171784937381744, Final Batch Loss: 0.4138302206993103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2411, Loss: 1.0597960948944092, Final Batch Loss: 0.38694167137145996\n",
      "Epoch 2412, Loss: 0.8766005784273148, Final Batch Loss: 0.2195543497800827\n",
      "Epoch 2413, Loss: 0.9934277832508087, Final Batch Loss: 0.3303182125091553\n",
      "Epoch 2414, Loss: 0.9493021667003632, Final Batch Loss: 0.3132150173187256\n",
      "Epoch 2415, Loss: 0.9384255111217499, Final Batch Loss: 0.2900146543979645\n",
      "Epoch 2416, Loss: 1.0140110850334167, Final Batch Loss: 0.3571215867996216\n",
      "Epoch 2417, Loss: 0.8806672543287277, Final Batch Loss: 0.2072763293981552\n",
      "Epoch 2418, Loss: 0.9027466773986816, Final Batch Loss: 0.3160974979400635\n",
      "Epoch 2419, Loss: 0.9939479827880859, Final Batch Loss: 0.3251100480556488\n",
      "Epoch 2420, Loss: 1.1683330833911896, Final Batch Loss: 0.462761253118515\n",
      "Epoch 2421, Loss: 1.0999409556388855, Final Batch Loss: 0.38161733746528625\n",
      "Epoch 2422, Loss: 0.9949196875095367, Final Batch Loss: 0.3407866656780243\n",
      "Epoch 2423, Loss: 0.9900109469890594, Final Batch Loss: 0.3188450038433075\n",
      "Epoch 2424, Loss: 1.0503637194633484, Final Batch Loss: 0.33991801738739014\n",
      "Epoch 2425, Loss: 1.0071105659008026, Final Batch Loss: 0.36667704582214355\n",
      "Epoch 2426, Loss: 1.0104786157608032, Final Batch Loss: 0.32624512910842896\n",
      "Epoch 2427, Loss: 0.8837884664535522, Final Batch Loss: 0.25842106342315674\n",
      "Epoch 2428, Loss: 1.0523376166820526, Final Batch Loss: 0.38661327958106995\n",
      "Epoch 2429, Loss: 1.070168524980545, Final Batch Loss: 0.35322049260139465\n",
      "Epoch 2430, Loss: 0.926368236541748, Final Batch Loss: 0.3522723317146301\n",
      "Epoch 2431, Loss: 1.0465094149112701, Final Batch Loss: 0.45096585154533386\n",
      "Epoch 2432, Loss: 0.9845934510231018, Final Batch Loss: 0.31209227442741394\n",
      "Epoch 2433, Loss: 1.1658161282539368, Final Batch Loss: 0.4447062015533447\n",
      "Epoch 2434, Loss: 0.9090028703212738, Final Batch Loss: 0.3064820468425751\n",
      "Epoch 2435, Loss: 1.0088689029216766, Final Batch Loss: 0.3586690425872803\n",
      "Epoch 2436, Loss: 0.9652144908905029, Final Batch Loss: 0.3037700057029724\n",
      "Epoch 2437, Loss: 0.9571224451065063, Final Batch Loss: 0.32422253489494324\n",
      "Epoch 2438, Loss: 0.9470500349998474, Final Batch Loss: 0.2944899797439575\n",
      "Epoch 2439, Loss: 1.011418104171753, Final Batch Loss: 0.29523521661758423\n",
      "Epoch 2440, Loss: 0.9789480865001678, Final Batch Loss: 0.3864698112010956\n",
      "Epoch 2441, Loss: 1.0099044442176819, Final Batch Loss: 0.2632620632648468\n",
      "Epoch 2442, Loss: 1.0563538074493408, Final Batch Loss: 0.33818984031677246\n",
      "Epoch 2443, Loss: 0.9521328210830688, Final Batch Loss: 0.2950836718082428\n",
      "Epoch 2444, Loss: 1.062442421913147, Final Batch Loss: 0.36882996559143066\n",
      "Epoch 2445, Loss: 0.8995647728443146, Final Batch Loss: 0.31900566816329956\n",
      "Epoch 2446, Loss: 0.9674516320228577, Final Batch Loss: 0.31644412875175476\n",
      "Epoch 2447, Loss: 1.0305469930171967, Final Batch Loss: 0.34500086307525635\n",
      "Epoch 2448, Loss: 0.9357993006706238, Final Batch Loss: 0.3342685401439667\n",
      "Epoch 2449, Loss: 0.9756695032119751, Final Batch Loss: 0.36215412616729736\n",
      "Epoch 2450, Loss: 0.9802390038967133, Final Batch Loss: 0.3086242973804474\n",
      "Epoch 2451, Loss: 1.0291042923927307, Final Batch Loss: 0.3506699204444885\n",
      "Epoch 2452, Loss: 1.1355811059474945, Final Batch Loss: 0.4369916319847107\n",
      "Epoch 2453, Loss: 1.0121159553527832, Final Batch Loss: 0.30921471118927\n",
      "Epoch 2454, Loss: 0.9481366872787476, Final Batch Loss: 0.2899850904941559\n",
      "Epoch 2455, Loss: 1.005819708108902, Final Batch Loss: 0.34912532567977905\n",
      "Epoch 2456, Loss: 0.9370825737714767, Final Batch Loss: 0.2380533665418625\n",
      "Epoch 2457, Loss: 1.0306302309036255, Final Batch Loss: 0.3836500942707062\n",
      "Epoch 2458, Loss: 0.9357729554176331, Final Batch Loss: 0.31733107566833496\n",
      "Epoch 2459, Loss: 1.0191822946071625, Final Batch Loss: 0.3240264654159546\n",
      "Epoch 2460, Loss: 1.100705087184906, Final Batch Loss: 0.3986007571220398\n",
      "Epoch 2461, Loss: 0.9372298121452332, Final Batch Loss: 0.29971185326576233\n",
      "Epoch 2462, Loss: 0.8962592035531998, Final Batch Loss: 0.2929214835166931\n",
      "Epoch 2463, Loss: 1.0416154563426971, Final Batch Loss: 0.3912990093231201\n",
      "Epoch 2464, Loss: 0.8790566027164459, Final Batch Loss: 0.33307838439941406\n",
      "Epoch 2465, Loss: 0.9627224504947662, Final Batch Loss: 0.29961735010147095\n",
      "Epoch 2466, Loss: 0.9310100972652435, Final Batch Loss: 0.25835591554641724\n",
      "Epoch 2467, Loss: 0.967720091342926, Final Batch Loss: 0.37245017290115356\n",
      "Epoch 2468, Loss: 1.1173899471759796, Final Batch Loss: 0.4332546889781952\n",
      "Epoch 2469, Loss: 0.918326199054718, Final Batch Loss: 0.3055568337440491\n",
      "Epoch 2470, Loss: 0.9990204870700836, Final Batch Loss: 0.3738253712654114\n",
      "Epoch 2471, Loss: 1.0090060532093048, Final Batch Loss: 0.3158305883407593\n",
      "Epoch 2472, Loss: 0.9222977161407471, Final Batch Loss: 0.2634167969226837\n",
      "Epoch 2473, Loss: 1.067781001329422, Final Batch Loss: 0.36894211173057556\n",
      "Epoch 2474, Loss: 0.9910538047552109, Final Batch Loss: 0.24258030951023102\n",
      "Epoch 2475, Loss: 1.0439730882644653, Final Batch Loss: 0.37840384244918823\n",
      "Epoch 2476, Loss: 0.9856053590774536, Final Batch Loss: 0.3331436812877655\n",
      "Epoch 2477, Loss: 0.9755427539348602, Final Batch Loss: 0.321277379989624\n",
      "Epoch 2478, Loss: 0.8952013254165649, Final Batch Loss: 0.26961055397987366\n",
      "Epoch 2479, Loss: 1.125448226928711, Final Batch Loss: 0.39175453782081604\n",
      "Epoch 2480, Loss: 0.9500753730535507, Final Batch Loss: 0.2240099459886551\n",
      "Epoch 2481, Loss: 1.0067166686058044, Final Batch Loss: 0.3152027130126953\n",
      "Epoch 2482, Loss: 0.9606714844703674, Final Batch Loss: 0.35176852345466614\n",
      "Epoch 2483, Loss: 1.0258437991142273, Final Batch Loss: 0.35997578501701355\n",
      "Epoch 2484, Loss: 0.88370281457901, Final Batch Loss: 0.26570865511894226\n",
      "Epoch 2485, Loss: 0.9660213589668274, Final Batch Loss: 0.3159445822238922\n",
      "Epoch 2486, Loss: 0.9364345073699951, Final Batch Loss: 0.3304939568042755\n",
      "Epoch 2487, Loss: 0.8495383262634277, Final Batch Loss: 0.22705405950546265\n",
      "Epoch 2488, Loss: 0.9915595948696136, Final Batch Loss: 0.3707951605319977\n",
      "Epoch 2489, Loss: 1.0504328608512878, Final Batch Loss: 0.40884295105934143\n",
      "Epoch 2490, Loss: 0.9325434267520905, Final Batch Loss: 0.2605848014354706\n",
      "Epoch 2491, Loss: 0.9946811497211456, Final Batch Loss: 0.2542632222175598\n",
      "Epoch 2492, Loss: 0.9439182579517365, Final Batch Loss: 0.25135061144828796\n",
      "Epoch 2493, Loss: 1.1238133013248444, Final Batch Loss: 0.3847769796848297\n",
      "Epoch 2494, Loss: 1.0217041373252869, Final Batch Loss: 0.34296172857284546\n",
      "Epoch 2495, Loss: 1.0236576199531555, Final Batch Loss: 0.3735067844390869\n",
      "Epoch 2496, Loss: 1.0968308448791504, Final Batch Loss: 0.43396803736686707\n",
      "Epoch 2497, Loss: 0.906573161482811, Final Batch Loss: 0.21684058010578156\n",
      "Epoch 2498, Loss: 1.120068222284317, Final Batch Loss: 0.34987327456474304\n",
      "Epoch 2499, Loss: 1.0373743772506714, Final Batch Loss: 0.3121986985206604\n",
      "Epoch 2500, Loss: 0.9832564294338226, Final Batch Loss: 0.29755744338035583\n",
      "Epoch 2501, Loss: 1.0337589979171753, Final Batch Loss: 0.36395183205604553\n",
      "Epoch 2502, Loss: 0.9190032184123993, Final Batch Loss: 0.26069098711013794\n",
      "Epoch 2503, Loss: 0.9470513164997101, Final Batch Loss: 0.2614348232746124\n",
      "Epoch 2504, Loss: 0.9891418814659119, Final Batch Loss: 0.35602468252182007\n",
      "Epoch 2505, Loss: 0.9581111669540405, Final Batch Loss: 0.3532271087169647\n",
      "Epoch 2506, Loss: 1.0800217390060425, Final Batch Loss: 0.2835642099380493\n",
      "Epoch 2507, Loss: 1.046608030796051, Final Batch Loss: 0.3584847152233124\n",
      "Epoch 2508, Loss: 0.9605840146541595, Final Batch Loss: 0.26601698994636536\n",
      "Epoch 2509, Loss: 0.958824634552002, Final Batch Loss: 0.3053615093231201\n",
      "Epoch 2510, Loss: 0.8960257470607758, Final Batch Loss: 0.32068073749542236\n",
      "Epoch 2511, Loss: 0.8998707681894302, Final Batch Loss: 0.2462529093027115\n",
      "Epoch 2512, Loss: 1.0121966004371643, Final Batch Loss: 0.3688599467277527\n",
      "Epoch 2513, Loss: 0.9955305755138397, Final Batch Loss: 0.34637680649757385\n",
      "Epoch 2514, Loss: 1.012597143650055, Final Batch Loss: 0.40069013833999634\n",
      "Epoch 2515, Loss: 0.9130637049674988, Final Batch Loss: 0.30040958523750305\n",
      "Epoch 2516, Loss: 1.041896641254425, Final Batch Loss: 0.4530012607574463\n",
      "Epoch 2517, Loss: 1.0944416522979736, Final Batch Loss: 0.41626331210136414\n",
      "Epoch 2518, Loss: 0.9962852597236633, Final Batch Loss: 0.32244205474853516\n",
      "Epoch 2519, Loss: 0.9972945749759674, Final Batch Loss: 0.30180445313453674\n",
      "Epoch 2520, Loss: 1.0426065027713776, Final Batch Loss: 0.3145926296710968\n",
      "Epoch 2521, Loss: 0.9351005405187607, Final Batch Loss: 0.20413707196712494\n",
      "Epoch 2522, Loss: 0.8837283849716187, Final Batch Loss: 0.3292999565601349\n",
      "Epoch 2523, Loss: 1.0521205365657806, Final Batch Loss: 0.36200249195098877\n",
      "Epoch 2524, Loss: 1.1022345125675201, Final Batch Loss: 0.40913307666778564\n",
      "Epoch 2525, Loss: 1.0270894765853882, Final Batch Loss: 0.4257320165634155\n",
      "Epoch 2526, Loss: 0.9738934934139252, Final Batch Loss: 0.31275564432144165\n",
      "Epoch 2527, Loss: 1.0117658078670502, Final Batch Loss: 0.3672054409980774\n",
      "Epoch 2528, Loss: 1.0051122307777405, Final Batch Loss: 0.3059448003768921\n",
      "Epoch 2529, Loss: 1.0522077679634094, Final Batch Loss: 0.37977135181427\n",
      "Epoch 2530, Loss: 1.0694211423397064, Final Batch Loss: 0.4088512659072876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2531, Loss: 1.021821916103363, Final Batch Loss: 0.33185943961143494\n",
      "Epoch 2532, Loss: 1.0585534870624542, Final Batch Loss: 0.3468668460845947\n",
      "Epoch 2533, Loss: 0.9836945235729218, Final Batch Loss: 0.3417907953262329\n",
      "Epoch 2534, Loss: 1.0761246979236603, Final Batch Loss: 0.3669425845146179\n",
      "Epoch 2535, Loss: 0.9959000051021576, Final Batch Loss: 0.3380849063396454\n",
      "Epoch 2536, Loss: 0.9253019392490387, Final Batch Loss: 0.3027551472187042\n",
      "Epoch 2537, Loss: 0.9561507105827332, Final Batch Loss: 0.38099610805511475\n",
      "Epoch 2538, Loss: 0.9634036719799042, Final Batch Loss: 0.34204262495040894\n",
      "Epoch 2539, Loss: 0.8371579349040985, Final Batch Loss: 0.285592645406723\n",
      "Epoch 2540, Loss: 0.953517735004425, Final Batch Loss: 0.4234723150730133\n",
      "Epoch 2541, Loss: 1.1513091921806335, Final Batch Loss: 0.5249322056770325\n",
      "Epoch 2542, Loss: 0.9027948975563049, Final Batch Loss: 0.2572614848613739\n",
      "Epoch 2543, Loss: 0.9389426112174988, Final Batch Loss: 0.31599754095077515\n",
      "Epoch 2544, Loss: 0.8976439833641052, Final Batch Loss: 0.31880494952201843\n",
      "Epoch 2545, Loss: 0.9677299559116364, Final Batch Loss: 0.3182440996170044\n",
      "Epoch 2546, Loss: 0.9684320986270905, Final Batch Loss: 0.31796032190322876\n",
      "Epoch 2547, Loss: 1.1329425275325775, Final Batch Loss: 0.45217177271842957\n",
      "Epoch 2548, Loss: 0.9692725837230682, Final Batch Loss: 0.339642733335495\n",
      "Epoch 2549, Loss: 0.860084742307663, Final Batch Loss: 0.29156190156936646\n",
      "Epoch 2550, Loss: 0.8401341736316681, Final Batch Loss: 0.2667795419692993\n",
      "Epoch 2551, Loss: 0.9457232654094696, Final Batch Loss: 0.22700676321983337\n",
      "Epoch 2552, Loss: 1.0340855121612549, Final Batch Loss: 0.4493183195590973\n",
      "Epoch 2553, Loss: 0.9232592582702637, Final Batch Loss: 0.31608331203460693\n",
      "Epoch 2554, Loss: 1.0691912770271301, Final Batch Loss: 0.38251808285713196\n",
      "Epoch 2555, Loss: 0.9553595781326294, Final Batch Loss: 0.3604402244091034\n",
      "Epoch 2556, Loss: 0.8898758292198181, Final Batch Loss: 0.28385651111602783\n",
      "Epoch 2557, Loss: 1.0150988698005676, Final Batch Loss: 0.3476923108100891\n",
      "Epoch 2558, Loss: 1.0671721994876862, Final Batch Loss: 0.3968019187450409\n",
      "Epoch 2559, Loss: 1.0152157545089722, Final Batch Loss: 0.34017735719680786\n",
      "Epoch 2560, Loss: 0.9803924858570099, Final Batch Loss: 0.30519649386405945\n",
      "Epoch 2561, Loss: 0.9616271257400513, Final Batch Loss: 0.292595237493515\n",
      "Epoch 2562, Loss: 1.0346708297729492, Final Batch Loss: 0.31512024998664856\n",
      "Epoch 2563, Loss: 1.1035360991954803, Final Batch Loss: 0.3873651325702667\n",
      "Epoch 2564, Loss: 0.9701805412769318, Final Batch Loss: 0.2651081681251526\n",
      "Epoch 2565, Loss: 0.9456654489040375, Final Batch Loss: 0.3349926173686981\n",
      "Epoch 2566, Loss: 0.9288933277130127, Final Batch Loss: 0.29310837388038635\n",
      "Epoch 2567, Loss: 0.9397042989730835, Final Batch Loss: 0.35185953974723816\n",
      "Epoch 2568, Loss: 0.9697273671627045, Final Batch Loss: 0.3594857156276703\n",
      "Epoch 2569, Loss: 0.9469199180603027, Final Batch Loss: 0.3513263165950775\n",
      "Epoch 2570, Loss: 0.9760257601737976, Final Batch Loss: 0.3751237392425537\n",
      "Epoch 2571, Loss: 1.025594800710678, Final Batch Loss: 0.40160268545150757\n",
      "Epoch 2572, Loss: 0.9838967621326447, Final Batch Loss: 0.37831243872642517\n",
      "Epoch 2573, Loss: 0.9453295171260834, Final Batch Loss: 0.39486759901046753\n",
      "Epoch 2574, Loss: 0.9698731601238251, Final Batch Loss: 0.2999768555164337\n",
      "Epoch 2575, Loss: 1.0000137686729431, Final Batch Loss: 0.372904509305954\n",
      "Epoch 2576, Loss: 0.9099016189575195, Final Batch Loss: 0.3211800456047058\n",
      "Epoch 2577, Loss: 1.0472585558891296, Final Batch Loss: 0.3298877775669098\n",
      "Epoch 2578, Loss: 0.9437636435031891, Final Batch Loss: 0.31379103660583496\n",
      "Epoch 2579, Loss: 0.9612738788127899, Final Batch Loss: 0.2971680462360382\n",
      "Epoch 2580, Loss: 0.9653311669826508, Final Batch Loss: 0.31002941727638245\n",
      "Epoch 2581, Loss: 0.9168716967105865, Final Batch Loss: 0.247694194316864\n",
      "Epoch 2582, Loss: 0.9104671478271484, Final Batch Loss: 0.2738107442855835\n",
      "Epoch 2583, Loss: 0.990737795829773, Final Batch Loss: 0.31265026330947876\n",
      "Epoch 2584, Loss: 0.9687412083148956, Final Batch Loss: 0.34472066164016724\n",
      "Epoch 2585, Loss: 0.967372477054596, Final Batch Loss: 0.3164556622505188\n",
      "Epoch 2586, Loss: 1.0123673677444458, Final Batch Loss: 0.34467998147010803\n",
      "Epoch 2587, Loss: 0.9234268367290497, Final Batch Loss: 0.307308167219162\n",
      "Epoch 2588, Loss: 0.9314374923706055, Final Batch Loss: 0.2809068262577057\n",
      "Epoch 2589, Loss: 1.0349106192588806, Final Batch Loss: 0.3545711934566498\n",
      "Epoch 2590, Loss: 0.9648299515247345, Final Batch Loss: 0.30711838603019714\n",
      "Epoch 2591, Loss: 0.8431822061538696, Final Batch Loss: 0.2592967748641968\n",
      "Epoch 2592, Loss: 0.949778750538826, Final Batch Loss: 0.24038951098918915\n",
      "Epoch 2593, Loss: 0.9640806913375854, Final Batch Loss: 0.28804853558540344\n",
      "Epoch 2594, Loss: 0.8375600874423981, Final Batch Loss: 0.20106923580169678\n",
      "Epoch 2595, Loss: 0.8869293928146362, Final Batch Loss: 0.2773846685886383\n",
      "Epoch 2596, Loss: 0.9393619000911713, Final Batch Loss: 0.36919888854026794\n",
      "Epoch 2597, Loss: 0.9443968087434769, Final Batch Loss: 0.2243437021970749\n",
      "Epoch 2598, Loss: 1.1787694096565247, Final Batch Loss: 0.4794405400753021\n",
      "Epoch 2599, Loss: 0.9844009280204773, Final Batch Loss: 0.26383623480796814\n",
      "Epoch 2600, Loss: 1.0715133845806122, Final Batch Loss: 0.3677336275577545\n",
      "Epoch 2601, Loss: 0.93700310587883, Final Batch Loss: 0.2867954671382904\n",
      "Epoch 2602, Loss: 0.8827354311943054, Final Batch Loss: 0.2599031925201416\n",
      "Epoch 2603, Loss: 0.9906058311462402, Final Batch Loss: 0.2839438021183014\n",
      "Epoch 2604, Loss: 0.8825661540031433, Final Batch Loss: 0.20728906989097595\n",
      "Epoch 2605, Loss: 0.9612258076667786, Final Batch Loss: 0.28609415888786316\n",
      "Epoch 2606, Loss: 0.8963648974895477, Final Batch Loss: 0.3208449184894562\n",
      "Epoch 2607, Loss: 0.9253287017345428, Final Batch Loss: 0.3315700590610504\n",
      "Epoch 2608, Loss: 0.9158181548118591, Final Batch Loss: 0.2434980273246765\n",
      "Epoch 2609, Loss: 0.926288902759552, Final Batch Loss: 0.2845163345336914\n",
      "Epoch 2610, Loss: 0.9657619297504425, Final Batch Loss: 0.2806488871574402\n",
      "Epoch 2611, Loss: 0.9346563071012497, Final Batch Loss: 0.35338255763053894\n",
      "Epoch 2612, Loss: 0.8841475248336792, Final Batch Loss: 0.27470603585243225\n",
      "Epoch 2613, Loss: 0.9587211310863495, Final Batch Loss: 0.3020448386669159\n",
      "Epoch 2614, Loss: 0.8797414302825928, Final Batch Loss: 0.2551329433917999\n",
      "Epoch 2615, Loss: 0.905822366476059, Final Batch Loss: 0.2676753103733063\n",
      "Epoch 2616, Loss: 0.9400213658809662, Final Batch Loss: 0.2735411524772644\n",
      "Epoch 2617, Loss: 0.9827418029308319, Final Batch Loss: 0.36842867732048035\n",
      "Epoch 2618, Loss: 0.8979951739311218, Final Batch Loss: 0.31763967871665955\n",
      "Epoch 2619, Loss: 0.9008103609085083, Final Batch Loss: 0.2635408341884613\n",
      "Epoch 2620, Loss: 0.8658409863710403, Final Batch Loss: 0.24824751913547516\n",
      "Epoch 2621, Loss: 0.9178242087364197, Final Batch Loss: 0.2271004021167755\n",
      "Epoch 2622, Loss: 0.9062447547912598, Final Batch Loss: 0.25617653131484985\n",
      "Epoch 2623, Loss: 1.103637933731079, Final Batch Loss: 0.28147637844085693\n",
      "Epoch 2624, Loss: 0.9726353585720062, Final Batch Loss: 0.3342672884464264\n",
      "Epoch 2625, Loss: 1.0136117935180664, Final Batch Loss: 0.3270435929298401\n",
      "Epoch 2626, Loss: 0.9529620707035065, Final Batch Loss: 0.27917778491973877\n",
      "Epoch 2627, Loss: 0.8679803609848022, Final Batch Loss: 0.3005930483341217\n",
      "Epoch 2628, Loss: 1.0289994478225708, Final Batch Loss: 0.33616673946380615\n",
      "Epoch 2629, Loss: 0.9089145511388779, Final Batch Loss: 0.3121533989906311\n",
      "Epoch 2630, Loss: 1.0105663239955902, Final Batch Loss: 0.3492043614387512\n",
      "Epoch 2631, Loss: 1.0516956746578217, Final Batch Loss: 0.42463937401771545\n",
      "Epoch 2632, Loss: 0.9554131925106049, Final Batch Loss: 0.2793126702308655\n",
      "Epoch 2633, Loss: 0.9600994884967804, Final Batch Loss: 0.3546862304210663\n",
      "Epoch 2634, Loss: 1.016314685344696, Final Batch Loss: 0.3908974826335907\n",
      "Epoch 2635, Loss: 0.9634239077568054, Final Batch Loss: 0.38571497797966003\n",
      "Epoch 2636, Loss: 0.9417544603347778, Final Batch Loss: 0.30828845500946045\n",
      "Epoch 2637, Loss: 0.8887773156166077, Final Batch Loss: 0.2513488233089447\n",
      "Epoch 2638, Loss: 0.9416372179985046, Final Batch Loss: 0.34448879957199097\n",
      "Epoch 2639, Loss: 0.9146811664104462, Final Batch Loss: 0.3071404695510864\n",
      "Epoch 2640, Loss: 1.0277116000652313, Final Batch Loss: 0.26117444038391113\n",
      "Epoch 2641, Loss: 1.045022040605545, Final Batch Loss: 0.3978145718574524\n",
      "Epoch 2642, Loss: 1.0121307373046875, Final Batch Loss: 0.3118424117565155\n",
      "Epoch 2643, Loss: 1.0298225283622742, Final Batch Loss: 0.4071999490261078\n",
      "Epoch 2644, Loss: 0.9124726355075836, Final Batch Loss: 0.28878694772720337\n",
      "Epoch 2645, Loss: 0.971224457025528, Final Batch Loss: 0.3435710370540619\n",
      "Epoch 2646, Loss: 0.9621524810791016, Final Batch Loss: 0.3549143970012665\n",
      "Epoch 2647, Loss: 1.1674066185951233, Final Batch Loss: 0.4616467356681824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2648, Loss: 0.9490828663110733, Final Batch Loss: 0.3792248070240021\n",
      "Epoch 2649, Loss: 1.1599248945713043, Final Batch Loss: 0.3685820698738098\n",
      "Epoch 2650, Loss: 0.9720732569694519, Final Batch Loss: 0.33848458528518677\n",
      "Epoch 2651, Loss: 1.0839413106441498, Final Batch Loss: 0.491691529750824\n",
      "Epoch 2652, Loss: 1.0573865175247192, Final Batch Loss: 0.398093581199646\n",
      "Epoch 2653, Loss: 1.0085206627845764, Final Batch Loss: 0.31036442518234253\n",
      "Epoch 2654, Loss: 0.9233782589435577, Final Batch Loss: 0.2332029640674591\n",
      "Epoch 2655, Loss: 0.9651956856250763, Final Batch Loss: 0.35105645656585693\n",
      "Epoch 2656, Loss: 0.9648058712482452, Final Batch Loss: 0.25022441148757935\n",
      "Epoch 2657, Loss: 0.8476122319698334, Final Batch Loss: 0.2737540602684021\n",
      "Epoch 2658, Loss: 0.902390792965889, Final Batch Loss: 0.2430795282125473\n",
      "Epoch 2659, Loss: 0.9877064824104309, Final Batch Loss: 0.39611777663230896\n",
      "Epoch 2660, Loss: 0.9859797954559326, Final Batch Loss: 0.297004371881485\n",
      "Epoch 2661, Loss: 0.903237909078598, Final Batch Loss: 0.280614972114563\n",
      "Epoch 2662, Loss: 0.9979124665260315, Final Batch Loss: 0.3183448910713196\n",
      "Epoch 2663, Loss: 0.8842786550521851, Final Batch Loss: 0.2795511484146118\n",
      "Epoch 2664, Loss: 0.9915372431278229, Final Batch Loss: 0.32132208347320557\n",
      "Epoch 2665, Loss: 1.0261417031288147, Final Batch Loss: 0.4174894392490387\n",
      "Epoch 2666, Loss: 0.9804832935333252, Final Batch Loss: 0.40986478328704834\n",
      "Epoch 2667, Loss: 0.995311826467514, Final Batch Loss: 0.40323883295059204\n",
      "Epoch 2668, Loss: 0.9665633738040924, Final Batch Loss: 0.3511313498020172\n",
      "Epoch 2669, Loss: 0.989277571439743, Final Batch Loss: 0.2934238016605377\n",
      "Epoch 2670, Loss: 0.923979640007019, Final Batch Loss: 0.31778326630592346\n",
      "Epoch 2671, Loss: 0.9639890789985657, Final Batch Loss: 0.37250810861587524\n",
      "Epoch 2672, Loss: 1.024758517742157, Final Batch Loss: 0.38571926951408386\n",
      "Epoch 2673, Loss: 0.8563241362571716, Final Batch Loss: 0.2705940008163452\n",
      "Epoch 2674, Loss: 1.0011871457099915, Final Batch Loss: 0.29665157198905945\n",
      "Epoch 2675, Loss: 1.0588311553001404, Final Batch Loss: 0.39610329270362854\n",
      "Epoch 2676, Loss: 0.8819215595722198, Final Batch Loss: 0.26273226737976074\n",
      "Epoch 2677, Loss: 0.9971445202827454, Final Batch Loss: 0.31469401717185974\n",
      "Epoch 2678, Loss: 0.978251188993454, Final Batch Loss: 0.32851871848106384\n",
      "Epoch 2679, Loss: 0.979843407869339, Final Batch Loss: 0.33448559045791626\n",
      "Epoch 2680, Loss: 0.834262415766716, Final Batch Loss: 0.20221678912639618\n",
      "Epoch 2681, Loss: 0.9734451323747635, Final Batch Loss: 0.23998214304447174\n",
      "Epoch 2682, Loss: 0.9093995690345764, Final Batch Loss: 0.2548375129699707\n",
      "Epoch 2683, Loss: 0.998190850019455, Final Batch Loss: 0.295229971408844\n",
      "Epoch 2684, Loss: 1.0093494653701782, Final Batch Loss: 0.36580151319503784\n",
      "Epoch 2685, Loss: 0.9326262474060059, Final Batch Loss: 0.3063157796859741\n",
      "Epoch 2686, Loss: 0.8729411661624908, Final Batch Loss: 0.27304607629776\n",
      "Epoch 2687, Loss: 0.9207736998796463, Final Batch Loss: 0.35864943265914917\n",
      "Epoch 2688, Loss: 0.9181860387325287, Final Batch Loss: 0.3324955701828003\n",
      "Epoch 2689, Loss: 1.1482456028461456, Final Batch Loss: 0.4558179974555969\n",
      "Epoch 2690, Loss: 0.9694176018238068, Final Batch Loss: 0.4190652072429657\n",
      "Epoch 2691, Loss: 0.7862551808357239, Final Batch Loss: 0.2537802755832672\n",
      "Epoch 2692, Loss: 0.9649551063776016, Final Batch Loss: 0.24141518771648407\n",
      "Epoch 2693, Loss: 0.9298805296421051, Final Batch Loss: 0.2851425111293793\n",
      "Epoch 2694, Loss: 0.852492019534111, Final Batch Loss: 0.24638377130031586\n",
      "Epoch 2695, Loss: 0.8651305437088013, Final Batch Loss: 0.25194019079208374\n",
      "Epoch 2696, Loss: 1.0832272469997406, Final Batch Loss: 0.3767763674259186\n",
      "Epoch 2697, Loss: 0.9172995686531067, Final Batch Loss: 0.2510735094547272\n",
      "Epoch 2698, Loss: 0.9731017053127289, Final Batch Loss: 0.33138594031333923\n",
      "Epoch 2699, Loss: 0.8914577662944794, Final Batch Loss: 0.28187909722328186\n",
      "Epoch 2700, Loss: 0.9649825394153595, Final Batch Loss: 0.3264663815498352\n",
      "Epoch 2701, Loss: 1.0320549309253693, Final Batch Loss: 0.31397852301597595\n",
      "Epoch 2702, Loss: 0.9596051275730133, Final Batch Loss: 0.31997519731521606\n",
      "Epoch 2703, Loss: 0.911359041929245, Final Batch Loss: 0.34505078196525574\n",
      "Epoch 2704, Loss: 0.8807192146778107, Final Batch Loss: 0.2558005154132843\n",
      "Epoch 2705, Loss: 0.9753585159778595, Final Batch Loss: 0.3595331907272339\n",
      "Epoch 2706, Loss: 0.9867355078458786, Final Batch Loss: 0.35190609097480774\n",
      "Epoch 2707, Loss: 1.0228091180324554, Final Batch Loss: 0.34254521131515503\n",
      "Epoch 2708, Loss: 0.9448417276144028, Final Batch Loss: 0.43216222524642944\n",
      "Epoch 2709, Loss: 1.021878033876419, Final Batch Loss: 0.3579358458518982\n",
      "Epoch 2710, Loss: 1.0645436942577362, Final Batch Loss: 0.3661988377571106\n",
      "Epoch 2711, Loss: 0.8892417550086975, Final Batch Loss: 0.2516111731529236\n",
      "Epoch 2712, Loss: 0.9989053308963776, Final Batch Loss: 0.3042443096637726\n",
      "Epoch 2713, Loss: 0.9800662100315094, Final Batch Loss: 0.3715827167034149\n",
      "Epoch 2714, Loss: 0.9058283120393753, Final Batch Loss: 0.2046986073255539\n",
      "Epoch 2715, Loss: 1.1239123046398163, Final Batch Loss: 0.45002833008766174\n",
      "Epoch 2716, Loss: 0.8662278354167938, Final Batch Loss: 0.2643033564090729\n",
      "Epoch 2717, Loss: 0.9359323382377625, Final Batch Loss: 0.30377209186553955\n",
      "Epoch 2718, Loss: 1.0368058681488037, Final Batch Loss: 0.383439302444458\n",
      "Epoch 2719, Loss: 0.9626093804836273, Final Batch Loss: 0.376327246427536\n",
      "Epoch 2720, Loss: 1.075839102268219, Final Batch Loss: 0.39200878143310547\n",
      "Epoch 2721, Loss: 0.878690093755722, Final Batch Loss: 0.28095540404319763\n",
      "Epoch 2722, Loss: 0.9546623826026917, Final Batch Loss: 0.26315703988075256\n",
      "Epoch 2723, Loss: 0.9206762909889221, Final Batch Loss: 0.31790944933891296\n",
      "Epoch 2724, Loss: 0.9535926282405853, Final Batch Loss: 0.2734435796737671\n",
      "Epoch 2725, Loss: 0.979268491268158, Final Batch Loss: 0.4072739779949188\n",
      "Epoch 2726, Loss: 0.8972575664520264, Final Batch Loss: 0.28474411368370056\n",
      "Epoch 2727, Loss: 1.0943513810634613, Final Batch Loss: 0.35760828852653503\n",
      "Epoch 2728, Loss: 0.909846693277359, Final Batch Loss: 0.2511337995529175\n",
      "Epoch 2729, Loss: 1.0081607401371002, Final Batch Loss: 0.32502514123916626\n",
      "Epoch 2730, Loss: 0.8980789482593536, Final Batch Loss: 0.2556312382221222\n",
      "Epoch 2731, Loss: 0.9713959693908691, Final Batch Loss: 0.29948902130126953\n",
      "Epoch 2732, Loss: 0.8733140826225281, Final Batch Loss: 0.3312664330005646\n",
      "Epoch 2733, Loss: 0.9709420502185822, Final Batch Loss: 0.28142958879470825\n",
      "Epoch 2734, Loss: 0.9704974591732025, Final Batch Loss: 0.3531910181045532\n",
      "Epoch 2735, Loss: 0.9997398853302002, Final Batch Loss: 0.3052426278591156\n",
      "Epoch 2736, Loss: 0.8546479344367981, Final Batch Loss: 0.2149796485900879\n",
      "Epoch 2737, Loss: 0.9853717088699341, Final Batch Loss: 0.2901591956615448\n",
      "Epoch 2738, Loss: 1.0148028135299683, Final Batch Loss: 0.3145895302295685\n",
      "Epoch 2739, Loss: 1.0006864815950394, Final Batch Loss: 0.2390558272600174\n",
      "Epoch 2740, Loss: 0.8959960639476776, Final Batch Loss: 0.315135657787323\n",
      "Epoch 2741, Loss: 0.948856383562088, Final Batch Loss: 0.3432086110115051\n",
      "Epoch 2742, Loss: 0.8542808890342712, Final Batch Loss: 0.26411595940589905\n",
      "Epoch 2743, Loss: 0.86390021443367, Final Batch Loss: 0.30730655789375305\n",
      "Epoch 2744, Loss: 0.9205460250377655, Final Batch Loss: 0.25162267684936523\n",
      "Epoch 2745, Loss: 0.9898991286754608, Final Batch Loss: 0.37879478931427\n",
      "Epoch 2746, Loss: 0.985464334487915, Final Batch Loss: 0.3869626820087433\n",
      "Epoch 2747, Loss: 0.8538215458393097, Final Batch Loss: 0.28845909237861633\n",
      "Epoch 2748, Loss: 0.9091828763484955, Final Batch Loss: 0.2317083179950714\n",
      "Epoch 2749, Loss: 0.8310035765171051, Final Batch Loss: 0.22911271452903748\n",
      "Epoch 2750, Loss: 0.9109571576118469, Final Batch Loss: 0.31814226508140564\n",
      "Epoch 2751, Loss: 0.8898840397596359, Final Batch Loss: 0.24922989308834076\n",
      "Epoch 2752, Loss: 0.8684077858924866, Final Batch Loss: 0.25000298023223877\n",
      "Epoch 2753, Loss: 0.9795583188533783, Final Batch Loss: 0.4374918043613434\n",
      "Epoch 2754, Loss: 0.9797554314136505, Final Batch Loss: 0.3564392030239105\n",
      "Epoch 2755, Loss: 0.825804591178894, Final Batch Loss: 0.2759932577610016\n",
      "Epoch 2756, Loss: 0.9040902853012085, Final Batch Loss: 0.2564210891723633\n",
      "Epoch 2757, Loss: 0.8288724422454834, Final Batch Loss: 0.248940110206604\n",
      "Epoch 2758, Loss: 0.8635978549718857, Final Batch Loss: 0.2168043702840805\n",
      "Epoch 2759, Loss: 0.8787315487861633, Final Batch Loss: 0.2568395137786865\n",
      "Epoch 2760, Loss: 0.8145953863859177, Final Batch Loss: 0.22586967051029205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2761, Loss: 0.8696816563606262, Final Batch Loss: 0.258465439081192\n",
      "Epoch 2762, Loss: 0.8698613047599792, Final Batch Loss: 0.3061234652996063\n",
      "Epoch 2763, Loss: 0.9299570620059967, Final Batch Loss: 0.37944111227989197\n",
      "Epoch 2764, Loss: 0.9293157756328583, Final Batch Loss: 0.3189684748649597\n",
      "Epoch 2765, Loss: 1.100271224975586, Final Batch Loss: 0.4246002733707428\n",
      "Epoch 2766, Loss: 0.9304872155189514, Final Batch Loss: 0.2829399108886719\n",
      "Epoch 2767, Loss: 0.8968491554260254, Final Batch Loss: 0.3112412095069885\n",
      "Epoch 2768, Loss: 0.8830357789993286, Final Batch Loss: 0.3009069561958313\n",
      "Epoch 2769, Loss: 0.9436651766300201, Final Batch Loss: 0.3352358341217041\n",
      "Epoch 2770, Loss: 1.0670010447502136, Final Batch Loss: 0.38303595781326294\n",
      "Epoch 2771, Loss: 0.9417377412319183, Final Batch Loss: 0.28212568163871765\n",
      "Epoch 2772, Loss: 0.9635914862155914, Final Batch Loss: 0.29746749997138977\n",
      "Epoch 2773, Loss: 0.9517773687839508, Final Batch Loss: 0.30552157759666443\n",
      "Epoch 2774, Loss: 1.0042582750320435, Final Batch Loss: 0.46604469418525696\n",
      "Epoch 2775, Loss: 1.015320599079132, Final Batch Loss: 0.4056856334209442\n",
      "Epoch 2776, Loss: 0.8925946056842804, Final Batch Loss: 0.3444705903530121\n",
      "Epoch 2777, Loss: 0.9340464174747467, Final Batch Loss: 0.36899325251579285\n",
      "Epoch 2778, Loss: 0.9979143440723419, Final Batch Loss: 0.34307122230529785\n",
      "Epoch 2779, Loss: 0.9329284131526947, Final Batch Loss: 0.3251557946205139\n",
      "Epoch 2780, Loss: 0.9212360680103302, Final Batch Loss: 0.24653509259223938\n",
      "Epoch 2781, Loss: 1.0135450065135956, Final Batch Loss: 0.3073626160621643\n",
      "Epoch 2782, Loss: 0.9105595052242279, Final Batch Loss: 0.3321254253387451\n",
      "Epoch 2783, Loss: 0.9923061430454254, Final Batch Loss: 0.34357964992523193\n",
      "Epoch 2784, Loss: 0.9289214909076691, Final Batch Loss: 0.3111957013607025\n",
      "Epoch 2785, Loss: 0.8842290341854095, Final Batch Loss: 0.2710694968700409\n",
      "Epoch 2786, Loss: 0.8382314294576645, Final Batch Loss: 0.20974428951740265\n",
      "Epoch 2787, Loss: 1.025280624628067, Final Batch Loss: 0.35940197110176086\n",
      "Epoch 2788, Loss: 0.887899249792099, Final Batch Loss: 0.2678353786468506\n",
      "Epoch 2789, Loss: 0.9091221988201141, Final Batch Loss: 0.332876592874527\n",
      "Epoch 2790, Loss: 1.055856853723526, Final Batch Loss: 0.36053845286369324\n",
      "Epoch 2791, Loss: 0.9313014447689056, Final Batch Loss: 0.29784122109413147\n",
      "Epoch 2792, Loss: 1.0082816183567047, Final Batch Loss: 0.40477290749549866\n",
      "Epoch 2793, Loss: 0.8694764077663422, Final Batch Loss: 0.30472710728645325\n",
      "Epoch 2794, Loss: 0.8906731903553009, Final Batch Loss: 0.25046679377555847\n",
      "Epoch 2795, Loss: 0.8846835494041443, Final Batch Loss: 0.2609359622001648\n",
      "Epoch 2796, Loss: 0.9133681356906891, Final Batch Loss: 0.29455575346946716\n",
      "Epoch 2797, Loss: 0.9847396910190582, Final Batch Loss: 0.3446723222732544\n",
      "Epoch 2798, Loss: 1.0066657066345215, Final Batch Loss: 0.3895680606365204\n",
      "Epoch 2799, Loss: 1.0635676681995392, Final Batch Loss: 0.43457236886024475\n",
      "Epoch 2800, Loss: 0.7637601643800735, Final Batch Loss: 0.25751909613609314\n",
      "Epoch 2801, Loss: 0.9632171094417572, Final Batch Loss: 0.34694886207580566\n",
      "Epoch 2802, Loss: 0.789503663778305, Final Batch Loss: 0.20010221004486084\n",
      "Epoch 2803, Loss: 0.9747364372014999, Final Batch Loss: 0.39394375681877136\n",
      "Epoch 2804, Loss: 0.9026984870433807, Final Batch Loss: 0.2704662084579468\n",
      "Epoch 2805, Loss: 0.9257230460643768, Final Batch Loss: 0.3040164113044739\n",
      "Epoch 2806, Loss: 0.9570857882499695, Final Batch Loss: 0.3217776119709015\n",
      "Epoch 2807, Loss: 0.8687945604324341, Final Batch Loss: 0.26865264773368835\n",
      "Epoch 2808, Loss: 0.8875625729560852, Final Batch Loss: 0.27915602922439575\n",
      "Epoch 2809, Loss: 0.9470459669828415, Final Batch Loss: 0.38140714168548584\n",
      "Epoch 2810, Loss: 0.9567825198173523, Final Batch Loss: 0.3015545606613159\n",
      "Epoch 2811, Loss: 0.8724904209375381, Final Batch Loss: 0.24939729273319244\n",
      "Epoch 2812, Loss: 0.9786337912082672, Final Batch Loss: 0.3406921923160553\n",
      "Epoch 2813, Loss: 1.0560266822576523, Final Batch Loss: 0.4937678873538971\n",
      "Epoch 2814, Loss: 0.8957478702068329, Final Batch Loss: 0.2875460684299469\n",
      "Epoch 2815, Loss: 0.7862409502267838, Final Batch Loss: 0.25133010745048523\n",
      "Epoch 2816, Loss: 0.9693801999092102, Final Batch Loss: 0.3646795451641083\n",
      "Epoch 2817, Loss: 0.9808251559734344, Final Batch Loss: 0.344029039144516\n",
      "Epoch 2818, Loss: 1.019419640302658, Final Batch Loss: 0.3466154932975769\n",
      "Epoch 2819, Loss: 1.0148134231567383, Final Batch Loss: 0.3577965199947357\n",
      "Epoch 2820, Loss: 0.9778302907943726, Final Batch Loss: 0.3337094783782959\n",
      "Epoch 2821, Loss: 0.8988052308559418, Final Batch Loss: 0.26174837350845337\n",
      "Epoch 2822, Loss: 1.0357835590839386, Final Batch Loss: 0.42461878061294556\n",
      "Epoch 2823, Loss: 1.0873592793941498, Final Batch Loss: 0.43788570165634155\n",
      "Epoch 2824, Loss: 0.909261167049408, Final Batch Loss: 0.298216313123703\n",
      "Epoch 2825, Loss: 0.8068297803401947, Final Batch Loss: 0.24170881509780884\n",
      "Epoch 2826, Loss: 0.9796946942806244, Final Batch Loss: 0.35378775000572205\n",
      "Epoch 2827, Loss: 0.8775998651981354, Final Batch Loss: 0.23080024123191833\n",
      "Epoch 2828, Loss: 0.903870016336441, Final Batch Loss: 0.23829495906829834\n",
      "Epoch 2829, Loss: 0.9246821999549866, Final Batch Loss: 0.28513532876968384\n",
      "Epoch 2830, Loss: 0.9743426442146301, Final Batch Loss: 0.32924240827560425\n",
      "Epoch 2831, Loss: 0.9587630331516266, Final Batch Loss: 0.3305164873600006\n",
      "Epoch 2832, Loss: 0.8613187819719315, Final Batch Loss: 0.30179348587989807\n",
      "Epoch 2833, Loss: 0.9659773409366608, Final Batch Loss: 0.42885375022888184\n",
      "Epoch 2834, Loss: 0.8122025430202484, Final Batch Loss: 0.2572562098503113\n",
      "Epoch 2835, Loss: 0.8944139182567596, Final Batch Loss: 0.273954838514328\n",
      "Epoch 2836, Loss: 1.0144599676132202, Final Batch Loss: 0.3391689658164978\n",
      "Epoch 2837, Loss: 0.8974003791809082, Final Batch Loss: 0.3116849362850189\n",
      "Epoch 2838, Loss: 0.9297713041305542, Final Batch Loss: 0.25120803713798523\n",
      "Epoch 2839, Loss: 0.9598492085933685, Final Batch Loss: 0.34475943446159363\n",
      "Epoch 2840, Loss: 0.9799619913101196, Final Batch Loss: 0.348196417093277\n",
      "Epoch 2841, Loss: 0.8503727912902832, Final Batch Loss: 0.31945502758026123\n",
      "Epoch 2842, Loss: 1.0735987424850464, Final Batch Loss: 0.39174214005470276\n",
      "Epoch 2843, Loss: 0.8764179944992065, Final Batch Loss: 0.2542315423488617\n",
      "Epoch 2844, Loss: 0.9657153785228729, Final Batch Loss: 0.3853728473186493\n",
      "Epoch 2845, Loss: 0.8419183045625687, Final Batch Loss: 0.276995986700058\n",
      "Epoch 2846, Loss: 0.9082631170749664, Final Batch Loss: 0.36025160551071167\n",
      "Epoch 2847, Loss: 0.8134275674819946, Final Batch Loss: 0.28196999430656433\n",
      "Epoch 2848, Loss: 0.8959343582391739, Final Batch Loss: 0.22205151617527008\n",
      "Epoch 2849, Loss: 1.0876043736934662, Final Batch Loss: 0.4625995457172394\n",
      "Epoch 2850, Loss: 0.9581968188285828, Final Batch Loss: 0.38728177547454834\n",
      "Epoch 2851, Loss: 0.9008768796920776, Final Batch Loss: 0.3472396731376648\n",
      "Epoch 2852, Loss: 0.9780799746513367, Final Batch Loss: 0.3678080439567566\n",
      "Epoch 2853, Loss: 0.9286908805370331, Final Batch Loss: 0.3267168402671814\n",
      "Epoch 2854, Loss: 0.8959749639034271, Final Batch Loss: 0.28456270694732666\n",
      "Epoch 2855, Loss: 0.842998743057251, Final Batch Loss: 0.2591429054737091\n",
      "Epoch 2856, Loss: 0.8834680318832397, Final Batch Loss: 0.2649804353713989\n",
      "Epoch 2857, Loss: 0.9088294506072998, Final Batch Loss: 0.3056887686252594\n",
      "Epoch 2858, Loss: 0.8293042033910751, Final Batch Loss: 0.2423856407403946\n",
      "Epoch 2859, Loss: 0.8338996469974518, Final Batch Loss: 0.25959834456443787\n",
      "Epoch 2860, Loss: 0.8610498160123825, Final Batch Loss: 0.24782554805278778\n",
      "Epoch 2861, Loss: 0.9007821083068848, Final Batch Loss: 0.30188947916030884\n",
      "Epoch 2862, Loss: 0.9593751430511475, Final Batch Loss: 0.29258477687835693\n",
      "Epoch 2863, Loss: 0.9665981531143188, Final Batch Loss: 0.2874869108200073\n",
      "Epoch 2864, Loss: 0.8392035067081451, Final Batch Loss: 0.2551533281803131\n",
      "Epoch 2865, Loss: 0.884428858757019, Final Batch Loss: 0.29592058062553406\n",
      "Epoch 2866, Loss: 1.1411990970373154, Final Batch Loss: 0.5181260704994202\n",
      "Epoch 2867, Loss: 0.8215015381574631, Final Batch Loss: 0.2971385419368744\n",
      "Epoch 2868, Loss: 0.9031002521514893, Final Batch Loss: 0.2791096568107605\n",
      "Epoch 2869, Loss: 0.9471527338027954, Final Batch Loss: 0.3649573028087616\n",
      "Epoch 2870, Loss: 0.9391124248504639, Final Batch Loss: 0.27229878306388855\n",
      "Epoch 2871, Loss: 0.8728460669517517, Final Batch Loss: 0.25913214683532715\n",
      "Epoch 2872, Loss: 0.8957389891147614, Final Batch Loss: 0.2811460494995117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2873, Loss: 0.9690617024898529, Final Batch Loss: 0.3457482159137726\n",
      "Epoch 2874, Loss: 0.8120615184307098, Final Batch Loss: 0.2589936852455139\n",
      "Epoch 2875, Loss: 0.8480155169963837, Final Batch Loss: 0.3283713757991791\n",
      "Epoch 2876, Loss: 0.8979443609714508, Final Batch Loss: 0.27386584877967834\n",
      "Epoch 2877, Loss: 0.9594076573848724, Final Batch Loss: 0.36420387029647827\n",
      "Epoch 2878, Loss: 0.9313211292028427, Final Batch Loss: 0.3828592002391815\n",
      "Epoch 2879, Loss: 0.8813155889511108, Final Batch Loss: 0.2883892357349396\n",
      "Epoch 2880, Loss: 0.840243473649025, Final Batch Loss: 0.25628000497817993\n",
      "Epoch 2881, Loss: 0.8871284425258636, Final Batch Loss: 0.281057208776474\n",
      "Epoch 2882, Loss: 0.9508486092090607, Final Batch Loss: 0.32521629333496094\n",
      "Epoch 2883, Loss: 0.8405127227306366, Final Batch Loss: 0.257503867149353\n",
      "Epoch 2884, Loss: 0.8735874891281128, Final Batch Loss: 0.2977553606033325\n",
      "Epoch 2885, Loss: 0.9723428785800934, Final Batch Loss: 0.34930968284606934\n",
      "Epoch 2886, Loss: 0.8222275674343109, Final Batch Loss: 0.2003345787525177\n",
      "Epoch 2887, Loss: 0.8473566621541977, Final Batch Loss: 0.23540405929088593\n",
      "Epoch 2888, Loss: 0.881608784198761, Final Batch Loss: 0.2943001687526703\n",
      "Epoch 2889, Loss: 0.945380687713623, Final Batch Loss: 0.3214215338230133\n",
      "Epoch 2890, Loss: 0.9663227796554565, Final Batch Loss: 0.34962624311447144\n",
      "Epoch 2891, Loss: 0.9022528529167175, Final Batch Loss: 0.3044791519641876\n",
      "Epoch 2892, Loss: 0.855369970202446, Final Batch Loss: 0.35116705298423767\n",
      "Epoch 2893, Loss: 0.8542360365390778, Final Batch Loss: 0.2712247967720032\n",
      "Epoch 2894, Loss: 0.8359962701797485, Final Batch Loss: 0.2947847545146942\n",
      "Epoch 2895, Loss: 0.9860166609287262, Final Batch Loss: 0.3402496576309204\n",
      "Epoch 2896, Loss: 0.8513605445623398, Final Batch Loss: 0.24793364107608795\n",
      "Epoch 2897, Loss: 0.9314955174922943, Final Batch Loss: 0.26739567518234253\n",
      "Epoch 2898, Loss: 0.8345318734645844, Final Batch Loss: 0.2597495913505554\n",
      "Epoch 2899, Loss: 0.7791145443916321, Final Batch Loss: 0.2428920567035675\n",
      "Epoch 2900, Loss: 0.9267613589763641, Final Batch Loss: 0.2558910548686981\n",
      "Epoch 2901, Loss: 0.896308183670044, Final Batch Loss: 0.3083899915218353\n",
      "Epoch 2902, Loss: 0.8486187160015106, Final Batch Loss: 0.2023724615573883\n",
      "Epoch 2903, Loss: 0.9791528284549713, Final Batch Loss: 0.3223874568939209\n",
      "Epoch 2904, Loss: 0.8302373886108398, Final Batch Loss: 0.2523019015789032\n",
      "Epoch 2905, Loss: 0.9688845276832581, Final Batch Loss: 0.3510408401489258\n",
      "Epoch 2906, Loss: 0.9909386038780212, Final Batch Loss: 0.28848761320114136\n",
      "Epoch 2907, Loss: 0.859541267156601, Final Batch Loss: 0.3016780912876129\n",
      "Epoch 2908, Loss: 0.9078592658042908, Final Batch Loss: 0.3093152642250061\n",
      "Epoch 2909, Loss: 0.938801109790802, Final Batch Loss: 0.30151501297950745\n",
      "Epoch 2910, Loss: 0.976714164018631, Final Batch Loss: 0.3336714506149292\n",
      "Epoch 2911, Loss: 0.8612574338912964, Final Batch Loss: 0.30125853419303894\n",
      "Epoch 2912, Loss: 0.9076841771602631, Final Batch Loss: 0.3200555145740509\n",
      "Epoch 2913, Loss: 0.9557821452617645, Final Batch Loss: 0.31902366876602173\n",
      "Epoch 2914, Loss: 0.8652933239936829, Final Batch Loss: 0.32376131415367126\n",
      "Epoch 2915, Loss: 0.9742951989173889, Final Batch Loss: 0.3265269994735718\n",
      "Epoch 2916, Loss: 0.926354169845581, Final Batch Loss: 0.30930447578430176\n",
      "Epoch 2917, Loss: 1.0175912082195282, Final Batch Loss: 0.4141119420528412\n",
      "Epoch 2918, Loss: 0.946731835603714, Final Batch Loss: 0.3414773643016815\n",
      "Epoch 2919, Loss: 0.8942754566669464, Final Batch Loss: 0.28328585624694824\n",
      "Epoch 2920, Loss: 0.8539712727069855, Final Batch Loss: 0.25667962431907654\n",
      "Epoch 2921, Loss: 0.8640967905521393, Final Batch Loss: 0.28015410900115967\n",
      "Epoch 2922, Loss: 0.8637663125991821, Final Batch Loss: 0.30217063426971436\n",
      "Epoch 2923, Loss: 0.8941856473684311, Final Batch Loss: 0.32976558804512024\n",
      "Epoch 2924, Loss: 1.0252211093902588, Final Batch Loss: 0.35351741313934326\n",
      "Epoch 2925, Loss: 1.0495505034923553, Final Batch Loss: 0.3774208128452301\n",
      "Epoch 2926, Loss: 0.8902851939201355, Final Batch Loss: 0.31018945574760437\n",
      "Epoch 2927, Loss: 0.8951071500778198, Final Batch Loss: 0.2556155323982239\n",
      "Epoch 2928, Loss: 0.9423773884773254, Final Batch Loss: 0.3060760200023651\n",
      "Epoch 2929, Loss: 0.8343437612056732, Final Batch Loss: 0.2914731502532959\n",
      "Epoch 2930, Loss: 0.8217041194438934, Final Batch Loss: 0.2355322241783142\n",
      "Epoch 2931, Loss: 0.8663783073425293, Final Batch Loss: 0.2753063142299652\n",
      "Epoch 2932, Loss: 0.9783955514431, Final Batch Loss: 0.38767439126968384\n",
      "Epoch 2933, Loss: 0.9032209515571594, Final Batch Loss: 0.305823415517807\n",
      "Epoch 2934, Loss: 0.8270091116428375, Final Batch Loss: 0.2570802867412567\n",
      "Epoch 2935, Loss: 0.7944981306791306, Final Batch Loss: 0.23639069497585297\n",
      "Epoch 2936, Loss: 0.9127496778964996, Final Batch Loss: 0.299478679895401\n",
      "Epoch 2937, Loss: 0.899825781583786, Final Batch Loss: 0.36312952637672424\n",
      "Epoch 2938, Loss: 0.9374463260173798, Final Batch Loss: 0.32070350646972656\n",
      "Epoch 2939, Loss: 0.9032730758190155, Final Batch Loss: 0.32342270016670227\n",
      "Epoch 2940, Loss: 0.9274088740348816, Final Batch Loss: 0.29203230142593384\n",
      "Epoch 2941, Loss: 1.0255739986896515, Final Batch Loss: 0.2789967954158783\n",
      "Epoch 2942, Loss: 0.9743347465991974, Final Batch Loss: 0.3026789128780365\n",
      "Epoch 2943, Loss: 0.9673701524734497, Final Batch Loss: 0.31924015283584595\n",
      "Epoch 2944, Loss: 0.95176762342453, Final Batch Loss: 0.3214082419872284\n",
      "Epoch 2945, Loss: 0.8562095612287521, Final Batch Loss: 0.24600188434123993\n",
      "Epoch 2946, Loss: 0.9277411103248596, Final Batch Loss: 0.3383449912071228\n",
      "Epoch 2947, Loss: 0.9674734175205231, Final Batch Loss: 0.37127119302749634\n",
      "Epoch 2948, Loss: 0.8474906831979752, Final Batch Loss: 0.23762084543704987\n",
      "Epoch 2949, Loss: 0.9155761301517487, Final Batch Loss: 0.3116483986377716\n",
      "Epoch 2950, Loss: 0.8022184371948242, Final Batch Loss: 0.25701504945755005\n",
      "Epoch 2951, Loss: 0.8409150540828705, Final Batch Loss: 0.2791025638580322\n",
      "Epoch 2952, Loss: 0.8309352546930313, Final Batch Loss: 0.2390771061182022\n",
      "Epoch 2953, Loss: 0.8493044227361679, Final Batch Loss: 0.2290857583284378\n",
      "Epoch 2954, Loss: 0.9057305455207825, Final Batch Loss: 0.36215683817863464\n",
      "Epoch 2955, Loss: 0.9614993929862976, Final Batch Loss: 0.32766425609588623\n",
      "Epoch 2956, Loss: 0.8269919604063034, Final Batch Loss: 0.29811352491378784\n",
      "Epoch 2957, Loss: 0.9725122451782227, Final Batch Loss: 0.21618905663490295\n",
      "Epoch 2958, Loss: 0.9716371297836304, Final Batch Loss: 0.38202595710754395\n",
      "Epoch 2959, Loss: 0.8645032644271851, Final Batch Loss: 0.2603435218334198\n",
      "Epoch 2960, Loss: 0.9749688804149628, Final Batch Loss: 0.3931702673435211\n",
      "Epoch 2961, Loss: 0.9706987142562866, Final Batch Loss: 0.41012194752693176\n",
      "Epoch 2962, Loss: 0.8971781432628632, Final Batch Loss: 0.304600328207016\n",
      "Epoch 2963, Loss: 0.8057790994644165, Final Batch Loss: 0.2868421673774719\n",
      "Epoch 2964, Loss: 0.8200783133506775, Final Batch Loss: 0.28857889771461487\n",
      "Epoch 2965, Loss: 0.9520894885063171, Final Batch Loss: 0.3280757963657379\n",
      "Epoch 2966, Loss: 0.9585645496845245, Final Batch Loss: 0.3225348889827728\n",
      "Epoch 2967, Loss: 1.1495909094810486, Final Batch Loss: 0.4521019458770752\n",
      "Epoch 2968, Loss: 0.9734086394309998, Final Batch Loss: 0.3897213041782379\n",
      "Epoch 2969, Loss: 0.9662312567234039, Final Batch Loss: 0.39429035782814026\n",
      "Epoch 2970, Loss: 0.8852874040603638, Final Batch Loss: 0.3001805245876312\n",
      "Epoch 2971, Loss: 0.9413000047206879, Final Batch Loss: 0.3719322383403778\n",
      "Epoch 2972, Loss: 0.8905632495880127, Final Batch Loss: 0.28617095947265625\n",
      "Epoch 2973, Loss: 0.9180399775505066, Final Batch Loss: 0.38245829939842224\n",
      "Epoch 2974, Loss: 0.8853850960731506, Final Batch Loss: 0.2772563695907593\n",
      "Epoch 2975, Loss: 0.8473434746265411, Final Batch Loss: 0.23863938450813293\n",
      "Epoch 2976, Loss: 0.8640409409999847, Final Batch Loss: 0.2662902772426605\n",
      "Epoch 2977, Loss: 1.0081028938293457, Final Batch Loss: 0.2906836271286011\n",
      "Epoch 2978, Loss: 0.9250206053256989, Final Batch Loss: 0.3541652262210846\n",
      "Epoch 2979, Loss: 0.8909686207771301, Final Batch Loss: 0.32889416813850403\n",
      "Epoch 2980, Loss: 0.7532040774822235, Final Batch Loss: 0.20756685733795166\n",
      "Epoch 2981, Loss: 1.0268913507461548, Final Batch Loss: 0.3976433277130127\n",
      "Epoch 2982, Loss: 0.897268146276474, Final Batch Loss: 0.25181636214256287\n",
      "Epoch 2983, Loss: 0.9535422921180725, Final Batch Loss: 0.321063756942749\n",
      "Epoch 2984, Loss: 1.0467317402362823, Final Batch Loss: 0.4057962894439697\n",
      "Epoch 2985, Loss: 0.8389523774385452, Final Batch Loss: 0.277913361787796\n",
      "Epoch 2986, Loss: 0.8247740119695663, Final Batch Loss: 0.35191717743873596\n",
      "Epoch 2987, Loss: 0.8647485226392746, Final Batch Loss: 0.2275363653898239\n",
      "Epoch 2988, Loss: 0.8614448010921478, Final Batch Loss: 0.3088066279888153\n",
      "Epoch 2989, Loss: 0.8071140795946121, Final Batch Loss: 0.22976572811603546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2990, Loss: 0.8592327833175659, Final Batch Loss: 0.268992155790329\n",
      "Epoch 2991, Loss: 0.8368620276451111, Final Batch Loss: 0.2587851285934448\n",
      "Epoch 2992, Loss: 0.8269953429698944, Final Batch Loss: 0.2557230591773987\n",
      "Epoch 2993, Loss: 0.8817707896232605, Final Batch Loss: 0.3345889449119568\n",
      "Epoch 2994, Loss: 0.925086110830307, Final Batch Loss: 0.2907469570636749\n",
      "Epoch 2995, Loss: 0.9453659057617188, Final Batch Loss: 0.30554473400115967\n",
      "Epoch 2996, Loss: 0.8935354351997375, Final Batch Loss: 0.2580890357494354\n",
      "Epoch 2997, Loss: 1.0263828039169312, Final Batch Loss: 0.3739165961742401\n",
      "Epoch 2998, Loss: 0.9105357229709625, Final Batch Loss: 0.29606592655181885\n",
      "Epoch 2999, Loss: 0.9666467607021332, Final Batch Loss: 0.2861209511756897\n",
      "Epoch 3000, Loss: 0.978520005941391, Final Batch Loss: 0.4055395722389221\n",
      "Epoch 3001, Loss: 0.8247286677360535, Final Batch Loss: 0.2635948956012726\n",
      "Epoch 3002, Loss: 0.9415105283260345, Final Batch Loss: 0.3437488377094269\n",
      "Epoch 3003, Loss: 0.9043232798576355, Final Batch Loss: 0.3096955418586731\n",
      "Epoch 3004, Loss: 0.7807946801185608, Final Batch Loss: 0.23488202691078186\n",
      "Epoch 3005, Loss: 0.886128157377243, Final Batch Loss: 0.2765836715698242\n",
      "Epoch 3006, Loss: 0.8412606716156006, Final Batch Loss: 0.32025229930877686\n",
      "Epoch 3007, Loss: 0.8948094248771667, Final Batch Loss: 0.32754141092300415\n",
      "Epoch 3008, Loss: 0.8403069376945496, Final Batch Loss: 0.27693384885787964\n",
      "Epoch 3009, Loss: 0.8936920762062073, Final Batch Loss: 0.31438469886779785\n",
      "Epoch 3010, Loss: 0.8750997483730316, Final Batch Loss: 0.25264009833335876\n",
      "Epoch 3011, Loss: 0.9001937210559845, Final Batch Loss: 0.3514458239078522\n",
      "Epoch 3012, Loss: 0.9964368343353271, Final Batch Loss: 0.4228597581386566\n",
      "Epoch 3013, Loss: 0.9326375126838684, Final Batch Loss: 0.2769969701766968\n",
      "Epoch 3014, Loss: 0.8862199783325195, Final Batch Loss: 0.31533491611480713\n",
      "Epoch 3015, Loss: 0.896202877163887, Final Batch Loss: 0.24866826832294464\n",
      "Epoch 3016, Loss: 1.0269558727741241, Final Batch Loss: 0.46664687991142273\n",
      "Epoch 3017, Loss: 0.8346112370491028, Final Batch Loss: 0.27241748571395874\n",
      "Epoch 3018, Loss: 0.9545233249664307, Final Batch Loss: 0.3159363567829132\n",
      "Epoch 3019, Loss: 0.8155967146158218, Final Batch Loss: 0.2057354599237442\n",
      "Epoch 3020, Loss: 0.9848742783069611, Final Batch Loss: 0.28420525789260864\n",
      "Epoch 3021, Loss: 0.870827704668045, Final Batch Loss: 0.25896117091178894\n",
      "Epoch 3022, Loss: 0.9023291170597076, Final Batch Loss: 0.2837406098842621\n",
      "Epoch 3023, Loss: 0.8527710735797882, Final Batch Loss: 0.27404820919036865\n",
      "Epoch 3024, Loss: 0.887343019247055, Final Batch Loss: 0.2826683521270752\n",
      "Epoch 3025, Loss: 0.9292317032814026, Final Batch Loss: 0.2719845175743103\n",
      "Epoch 3026, Loss: 0.9596439003944397, Final Batch Loss: 0.41783517599105835\n",
      "Epoch 3027, Loss: 0.8536168932914734, Final Batch Loss: 0.2743769586086273\n",
      "Epoch 3028, Loss: 0.8273440450429916, Final Batch Loss: 0.21646834909915924\n",
      "Epoch 3029, Loss: 0.8000268787145615, Final Batch Loss: 0.2587830126285553\n",
      "Epoch 3030, Loss: 0.9037471413612366, Final Batch Loss: 0.32844358682632446\n",
      "Epoch 3031, Loss: 0.9442132413387299, Final Batch Loss: 0.35940104722976685\n",
      "Epoch 3032, Loss: 0.9439875930547714, Final Batch Loss: 0.2360178381204605\n",
      "Epoch 3033, Loss: 0.9252346456050873, Final Batch Loss: 0.38597238063812256\n",
      "Epoch 3034, Loss: 0.9541653990745544, Final Batch Loss: 0.40573811531066895\n",
      "Epoch 3035, Loss: 0.9019657075405121, Final Batch Loss: 0.2916412055492401\n",
      "Epoch 3036, Loss: 0.961665689945221, Final Batch Loss: 0.39004984498023987\n",
      "Epoch 3037, Loss: 0.8465394377708435, Final Batch Loss: 0.29487356543540955\n",
      "Epoch 3038, Loss: 0.9385582208633423, Final Batch Loss: 0.3910306692123413\n",
      "Epoch 3039, Loss: 0.8651303648948669, Final Batch Loss: 0.30807578563690186\n",
      "Epoch 3040, Loss: 0.8176904916763306, Final Batch Loss: 0.2705990970134735\n",
      "Epoch 3041, Loss: 0.9251025319099426, Final Batch Loss: 0.3205694854259491\n",
      "Epoch 3042, Loss: 0.813796728849411, Final Batch Loss: 0.22585466504096985\n",
      "Epoch 3043, Loss: 0.9068499207496643, Final Batch Loss: 0.35843679308891296\n",
      "Epoch 3044, Loss: 0.7928868681192398, Final Batch Loss: 0.26158949732780457\n",
      "Epoch 3045, Loss: 0.876318946480751, Final Batch Loss: 0.19553767144680023\n",
      "Epoch 3046, Loss: 0.9553126990795135, Final Batch Loss: 0.31316807866096497\n",
      "Epoch 3047, Loss: 0.8469683825969696, Final Batch Loss: 0.24454933404922485\n",
      "Epoch 3048, Loss: 0.8853683769702911, Final Batch Loss: 0.2745763957500458\n",
      "Epoch 3049, Loss: 0.8379844576120377, Final Batch Loss: 0.24922862648963928\n",
      "Epoch 3050, Loss: 0.7643298655748367, Final Batch Loss: 0.2882772982120514\n",
      "Epoch 3051, Loss: 0.7657963186502457, Final Batch Loss: 0.22351853549480438\n",
      "Epoch 3052, Loss: 0.9151282608509064, Final Batch Loss: 0.30848196148872375\n",
      "Epoch 3053, Loss: 0.9059122800827026, Final Batch Loss: 0.2743605375289917\n",
      "Epoch 3054, Loss: 0.9538675397634506, Final Batch Loss: 0.22540177404880524\n",
      "Epoch 3055, Loss: 0.872369796037674, Final Batch Loss: 0.29027968645095825\n",
      "Epoch 3056, Loss: 0.9614737927913666, Final Batch Loss: 0.31530362367630005\n",
      "Epoch 3057, Loss: 0.8294048756361008, Final Batch Loss: 0.28892093896865845\n",
      "Epoch 3058, Loss: 0.8987435698509216, Final Batch Loss: 0.3522338569164276\n",
      "Epoch 3059, Loss: 0.8651156723499298, Final Batch Loss: 0.3098703920841217\n",
      "Epoch 3060, Loss: 0.9157120883464813, Final Batch Loss: 0.3004222512245178\n",
      "Epoch 3061, Loss: 0.8830253779888153, Final Batch Loss: 0.32867714762687683\n",
      "Epoch 3062, Loss: 0.8966610133647919, Final Batch Loss: 0.3068271577358246\n",
      "Epoch 3063, Loss: 0.8436038196086884, Final Batch Loss: 0.21256235241889954\n",
      "Epoch 3064, Loss: 0.8439616858959198, Final Batch Loss: 0.3016144633293152\n",
      "Epoch 3065, Loss: 0.8570170551538467, Final Batch Loss: 0.24939538538455963\n",
      "Epoch 3066, Loss: 0.9436906278133392, Final Batch Loss: 0.25175532698631287\n",
      "Epoch 3067, Loss: 1.2269700765609741, Final Batch Loss: 0.5667078495025635\n",
      "Epoch 3068, Loss: 0.8319287896156311, Final Batch Loss: 0.22653427720069885\n",
      "Epoch 3069, Loss: 0.9466325640678406, Final Batch Loss: 0.3289729356765747\n",
      "Epoch 3070, Loss: 0.9120991826057434, Final Batch Loss: 0.24859359860420227\n",
      "Epoch 3071, Loss: 0.8366012573242188, Final Batch Loss: 0.22417452931404114\n",
      "Epoch 3072, Loss: 0.8748649656772614, Final Batch Loss: 0.27963048219680786\n",
      "Epoch 3073, Loss: 0.8933875560760498, Final Batch Loss: 0.31321847438812256\n",
      "Epoch 3074, Loss: 0.8672219216823578, Final Batch Loss: 0.2965206205844879\n",
      "Epoch 3075, Loss: 0.769392654299736, Final Batch Loss: 0.21664150059223175\n",
      "Epoch 3076, Loss: 0.9964596331119537, Final Batch Loss: 0.29569435119628906\n",
      "Epoch 3077, Loss: 0.8783369064331055, Final Batch Loss: 0.25547826290130615\n",
      "Epoch 3078, Loss: 1.0026530921459198, Final Batch Loss: 0.3099682033061981\n",
      "Epoch 3079, Loss: 0.9107469916343689, Final Batch Loss: 0.2698342800140381\n",
      "Epoch 3080, Loss: 0.88040591776371, Final Batch Loss: 0.23290149867534637\n",
      "Epoch 3081, Loss: 1.0751249492168427, Final Batch Loss: 0.3954636752605438\n",
      "Epoch 3082, Loss: 0.8065609037876129, Final Batch Loss: 0.2993640899658203\n",
      "Epoch 3083, Loss: 0.9756685346364975, Final Batch Loss: 0.35783851146698\n",
      "Epoch 3084, Loss: 0.9177089035511017, Final Batch Loss: 0.3404127359390259\n",
      "Epoch 3085, Loss: 0.8847694098949432, Final Batch Loss: 0.2624882459640503\n",
      "Epoch 3086, Loss: 0.8426636457443237, Final Batch Loss: 0.2213152050971985\n",
      "Epoch 3087, Loss: 0.9427410960197449, Final Batch Loss: 0.3389369547367096\n",
      "Epoch 3088, Loss: 0.882577195763588, Final Batch Loss: 0.3493482768535614\n",
      "Epoch 3089, Loss: 0.8430209904909134, Final Batch Loss: 0.24942748248577118\n",
      "Epoch 3090, Loss: 0.932794839143753, Final Batch Loss: 0.3111414313316345\n",
      "Epoch 3091, Loss: 0.8670225590467453, Final Batch Loss: 0.2301139384508133\n",
      "Epoch 3092, Loss: 0.9266057908535004, Final Batch Loss: 0.2864146828651428\n",
      "Epoch 3093, Loss: 0.8661366701126099, Final Batch Loss: 0.28144556283950806\n",
      "Epoch 3094, Loss: 0.8577743619680405, Final Batch Loss: 0.23185186088085175\n",
      "Epoch 3095, Loss: 0.824812725186348, Final Batch Loss: 0.24925152957439423\n",
      "Epoch 3096, Loss: 0.9871872365474701, Final Batch Loss: 0.3815227746963501\n",
      "Epoch 3097, Loss: 0.9948272407054901, Final Batch Loss: 0.35722577571868896\n",
      "Epoch 3098, Loss: 0.9120178818702698, Final Batch Loss: 0.3178182542324066\n",
      "Epoch 3099, Loss: 0.8751921057701111, Final Batch Loss: 0.26429322361946106\n",
      "Epoch 3100, Loss: 0.8189791738986969, Final Batch Loss: 0.18487802147865295\n",
      "Epoch 3101, Loss: 0.7928384095430374, Final Batch Loss: 0.2531830668449402\n",
      "Epoch 3102, Loss: 0.7734819501638412, Final Batch Loss: 0.18347369134426117\n",
      "Epoch 3103, Loss: 0.95375856757164, Final Batch Loss: 0.36073657870292664\n",
      "Epoch 3104, Loss: 0.9787570834159851, Final Batch Loss: 0.3628334701061249\n",
      "Epoch 3105, Loss: 0.8246854990720749, Final Batch Loss: 0.316320538520813\n",
      "Epoch 3106, Loss: 0.9023540318012238, Final Batch Loss: 0.34259575605392456\n",
      "Epoch 3107, Loss: 0.8815456181764603, Final Batch Loss: 0.20980562269687653\n",
      "Epoch 3108, Loss: 0.938855767250061, Final Batch Loss: 0.29937276244163513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3109, Loss: 0.923026978969574, Final Batch Loss: 0.2899940013885498\n",
      "Epoch 3110, Loss: 0.9045383334159851, Final Batch Loss: 0.3643083870410919\n",
      "Epoch 3111, Loss: 0.8476307094097137, Final Batch Loss: 0.2140122652053833\n",
      "Epoch 3112, Loss: 0.8690114617347717, Final Batch Loss: 0.2780042886734009\n",
      "Epoch 3113, Loss: 0.9967591464519501, Final Batch Loss: 0.4032793641090393\n",
      "Epoch 3114, Loss: 0.8776110261678696, Final Batch Loss: 0.3466419279575348\n",
      "Epoch 3115, Loss: 0.9288569092750549, Final Batch Loss: 0.33304744958877563\n",
      "Epoch 3116, Loss: 1.0747336149215698, Final Batch Loss: 0.4687406122684479\n",
      "Epoch 3117, Loss: 0.925796166062355, Final Batch Loss: 0.34245598316192627\n",
      "Epoch 3118, Loss: 0.8814516365528107, Final Batch Loss: 0.37126624584198\n",
      "Epoch 3119, Loss: 0.9885579943656921, Final Batch Loss: 0.327049195766449\n",
      "Epoch 3120, Loss: 0.9971007406711578, Final Batch Loss: 0.3304286599159241\n",
      "Epoch 3121, Loss: 0.8312959671020508, Final Batch Loss: 0.3302568197250366\n",
      "Epoch 3122, Loss: 0.9496986716985703, Final Batch Loss: 0.24783317744731903\n",
      "Epoch 3123, Loss: 0.8289977163076401, Final Batch Loss: 0.14891317486763\n",
      "Epoch 3124, Loss: 0.9196644723415375, Final Batch Loss: 0.3736725151538849\n",
      "Epoch 3125, Loss: 0.8627007603645325, Final Batch Loss: 0.32063111662864685\n",
      "Epoch 3126, Loss: 0.9455918669700623, Final Batch Loss: 0.29092177748680115\n",
      "Epoch 3127, Loss: 0.8414176106452942, Final Batch Loss: 0.26234227418899536\n",
      "Epoch 3128, Loss: 0.81691974401474, Final Batch Loss: 0.27173614501953125\n",
      "Epoch 3129, Loss: 0.923361212015152, Final Batch Loss: 0.3241885304450989\n",
      "Epoch 3130, Loss: 0.9459318816661835, Final Batch Loss: 0.3263103663921356\n",
      "Epoch 3131, Loss: 0.9109274446964264, Final Batch Loss: 0.3272048830986023\n",
      "Epoch 3132, Loss: 0.831908643245697, Final Batch Loss: 0.2967366576194763\n",
      "Epoch 3133, Loss: 0.8870023488998413, Final Batch Loss: 0.3214024305343628\n",
      "Epoch 3134, Loss: 0.852619469165802, Final Batch Loss: 0.26158279180526733\n",
      "Epoch 3135, Loss: 0.8646852076053619, Final Batch Loss: 0.27039504051208496\n",
      "Epoch 3136, Loss: 0.9680182635784149, Final Batch Loss: 0.3715219795703888\n",
      "Epoch 3137, Loss: 0.8520728945732117, Final Batch Loss: 0.31196463108062744\n",
      "Epoch 3138, Loss: 0.9367606341838837, Final Batch Loss: 0.31993260979652405\n",
      "Epoch 3139, Loss: 0.8693956583738327, Final Batch Loss: 0.23371978104114532\n",
      "Epoch 3140, Loss: 0.803076758980751, Final Batch Loss: 0.18963752686977386\n",
      "Epoch 3141, Loss: 0.7796033769845963, Final Batch Loss: 0.20864082872867584\n",
      "Epoch 3142, Loss: 0.881253182888031, Final Batch Loss: 0.337528795003891\n",
      "Epoch 3143, Loss: 0.8406570553779602, Final Batch Loss: 0.29436931014060974\n",
      "Epoch 3144, Loss: 0.8195292502641678, Final Batch Loss: 0.27599671483039856\n",
      "Epoch 3145, Loss: 0.9084952473640442, Final Batch Loss: 0.2649904191493988\n",
      "Epoch 3146, Loss: 0.8664729297161102, Final Batch Loss: 0.2782668173313141\n",
      "Epoch 3147, Loss: 0.8979642391204834, Final Batch Loss: 0.2781471014022827\n",
      "Epoch 3148, Loss: 0.9781996011734009, Final Batch Loss: 0.3654916286468506\n",
      "Epoch 3149, Loss: 0.7826760858297348, Final Batch Loss: 0.27730828523635864\n",
      "Epoch 3150, Loss: 0.7899913191795349, Final Batch Loss: 0.2243611216545105\n",
      "Epoch 3151, Loss: 0.8827521502971649, Final Batch Loss: 0.3649435043334961\n",
      "Epoch 3152, Loss: 0.8769073188304901, Final Batch Loss: 0.279986172914505\n",
      "Epoch 3153, Loss: 0.9390960931777954, Final Batch Loss: 0.36564409732818604\n",
      "Epoch 3154, Loss: 0.7816804498434067, Final Batch Loss: 0.2560116946697235\n",
      "Epoch 3155, Loss: 0.9114180505275726, Final Batch Loss: 0.34400805830955505\n",
      "Epoch 3156, Loss: 0.8504125773906708, Final Batch Loss: 0.30126088857650757\n",
      "Epoch 3157, Loss: 0.88163922727108, Final Batch Loss: 0.21640296280384064\n",
      "Epoch 3158, Loss: 0.837813675403595, Final Batch Loss: 0.23579508066177368\n",
      "Epoch 3159, Loss: 0.859027773141861, Final Batch Loss: 0.26439493894577026\n",
      "Epoch 3160, Loss: 0.8233894407749176, Final Batch Loss: 0.22591382265090942\n",
      "Epoch 3161, Loss: 0.9617902040481567, Final Batch Loss: 0.3777875602245331\n",
      "Epoch 3162, Loss: 0.9019441902637482, Final Batch Loss: 0.30937790870666504\n",
      "Epoch 3163, Loss: 0.881674736738205, Final Batch Loss: 0.3475399613380432\n",
      "Epoch 3164, Loss: 0.9217573404312134, Final Batch Loss: 0.3208719789981842\n",
      "Epoch 3165, Loss: 0.8542575240135193, Final Batch Loss: 0.27630615234375\n",
      "Epoch 3166, Loss: 1.0317943394184113, Final Batch Loss: 0.3944121301174164\n",
      "Epoch 3167, Loss: 0.8885816037654877, Final Batch Loss: 0.31284335255622864\n",
      "Epoch 3168, Loss: 0.838069498538971, Final Batch Loss: 0.2674465775489807\n",
      "Epoch 3169, Loss: 0.957348108291626, Final Batch Loss: 0.2792336344718933\n",
      "Epoch 3170, Loss: 0.7748501598834991, Final Batch Loss: 0.2327020764350891\n",
      "Epoch 3171, Loss: 0.9683355689048767, Final Batch Loss: 0.35237857699394226\n",
      "Epoch 3172, Loss: 0.9060610681772232, Final Batch Loss: 0.35926541686058044\n",
      "Epoch 3173, Loss: 0.7867262661457062, Final Batch Loss: 0.2555410861968994\n",
      "Epoch 3174, Loss: 0.9234705567359924, Final Batch Loss: 0.2849990725517273\n",
      "Epoch 3175, Loss: 0.8756331205368042, Final Batch Loss: 0.29450440406799316\n",
      "Epoch 3176, Loss: 0.8943697214126587, Final Batch Loss: 0.30584657192230225\n",
      "Epoch 3177, Loss: 0.9075214713811874, Final Batch Loss: 0.23803360760211945\n",
      "Epoch 3178, Loss: 0.8527502119541168, Final Batch Loss: 0.2920864522457123\n",
      "Epoch 3179, Loss: 0.8689683079719543, Final Batch Loss: 0.31314021348953247\n",
      "Epoch 3180, Loss: 0.8768731653690338, Final Batch Loss: 0.33029839396476746\n",
      "Epoch 3181, Loss: 0.8894007205963135, Final Batch Loss: 0.3442595601081848\n",
      "Epoch 3182, Loss: 0.8695577085018158, Final Batch Loss: 0.31969067454338074\n",
      "Epoch 3183, Loss: 1.0645159184932709, Final Batch Loss: 0.4179106056690216\n",
      "Epoch 3184, Loss: 0.8300189971923828, Final Batch Loss: 0.27994304895401\n",
      "Epoch 3185, Loss: 0.8969150185585022, Final Batch Loss: 0.2946475148200989\n",
      "Epoch 3186, Loss: 0.8694618940353394, Final Batch Loss: 0.2939501702785492\n",
      "Epoch 3187, Loss: 0.8460069000720978, Final Batch Loss: 0.2958575189113617\n",
      "Epoch 3188, Loss: 0.8091401308774948, Final Batch Loss: 0.25722455978393555\n",
      "Epoch 3189, Loss: 0.9357497543096542, Final Batch Loss: 0.3769385516643524\n",
      "Epoch 3190, Loss: 0.8593283593654633, Final Batch Loss: 0.22569456696510315\n",
      "Epoch 3191, Loss: 0.8535655736923218, Final Batch Loss: 0.21492242813110352\n",
      "Epoch 3192, Loss: 0.9576341509819031, Final Batch Loss: 0.3543159067630768\n",
      "Epoch 3193, Loss: 0.8464471995830536, Final Batch Loss: 0.2967419922351837\n",
      "Epoch 3194, Loss: 0.8311515152454376, Final Batch Loss: 0.24373817443847656\n",
      "Epoch 3195, Loss: 0.8820647895336151, Final Batch Loss: 0.3764336109161377\n",
      "Epoch 3196, Loss: 0.8166115432977676, Final Batch Loss: 0.2529163956642151\n",
      "Epoch 3197, Loss: 0.7584107667207718, Final Batch Loss: 0.2904384434223175\n",
      "Epoch 3198, Loss: 0.8868984878063202, Final Batch Loss: 0.30421751737594604\n",
      "Epoch 3199, Loss: 0.8983784019947052, Final Batch Loss: 0.3203139901161194\n",
      "Epoch 3200, Loss: 0.8479172438383102, Final Batch Loss: 0.35120293498039246\n",
      "Epoch 3201, Loss: 0.8847239017486572, Final Batch Loss: 0.30732256174087524\n",
      "Epoch 3202, Loss: 0.8341703712940216, Final Batch Loss: 0.26830732822418213\n",
      "Epoch 3203, Loss: 0.8712272644042969, Final Batch Loss: 0.32193756103515625\n",
      "Epoch 3204, Loss: 0.8120025843381882, Final Batch Loss: 0.23289652168750763\n",
      "Epoch 3205, Loss: 0.8458182215690613, Final Batch Loss: 0.2564023435115814\n",
      "Epoch 3206, Loss: 0.7497351914644241, Final Batch Loss: 0.22605343163013458\n",
      "Epoch 3207, Loss: 0.8751994371414185, Final Batch Loss: 0.29214125871658325\n",
      "Epoch 3208, Loss: 0.9453823268413544, Final Batch Loss: 0.3416968286037445\n",
      "Epoch 3209, Loss: 0.891807347536087, Final Batch Loss: 0.28162330389022827\n",
      "Epoch 3210, Loss: 0.9107802212238312, Final Batch Loss: 0.3219957947731018\n",
      "Epoch 3211, Loss: 0.8218357264995575, Final Batch Loss: 0.21908262372016907\n",
      "Epoch 3212, Loss: 0.7965594083070755, Final Batch Loss: 0.27716436982154846\n",
      "Epoch 3213, Loss: 0.8560800552368164, Final Batch Loss: 0.28758177161216736\n",
      "Epoch 3214, Loss: 0.8335977643728256, Final Batch Loss: 0.215149387717247\n",
      "Epoch 3215, Loss: 0.8174830377101898, Final Batch Loss: 0.24339303374290466\n",
      "Epoch 3216, Loss: 0.8317022025585175, Final Batch Loss: 0.2522834539413452\n",
      "Epoch 3217, Loss: 0.941495269536972, Final Batch Loss: 0.3355552852153778\n",
      "Epoch 3218, Loss: 0.8546816855669022, Final Batch Loss: 0.21463795006275177\n",
      "Epoch 3219, Loss: 0.9103077352046967, Final Batch Loss: 0.3020794987678528\n",
      "Epoch 3220, Loss: 0.9044012129306793, Final Batch Loss: 0.3147529065608978\n",
      "Epoch 3221, Loss: 0.8624125272035599, Final Batch Loss: 0.33883410692214966\n",
      "Epoch 3222, Loss: 0.9004893600940704, Final Batch Loss: 0.3193354904651642\n",
      "Epoch 3223, Loss: 0.9394974112510681, Final Batch Loss: 0.30073827505111694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3224, Loss: 0.9461399167776108, Final Batch Loss: 0.32322970032691956\n",
      "Epoch 3225, Loss: 0.8643904626369476, Final Batch Loss: 0.2636682987213135\n",
      "Epoch 3226, Loss: 0.8839439749717712, Final Batch Loss: 0.291332483291626\n",
      "Epoch 3227, Loss: 0.8858331143856049, Final Batch Loss: 0.2984342575073242\n",
      "Epoch 3228, Loss: 0.9851772785186768, Final Batch Loss: 0.35253337025642395\n",
      "Epoch 3229, Loss: 0.9582695364952087, Final Batch Loss: 0.36176639795303345\n",
      "Epoch 3230, Loss: 0.9122109115123749, Final Batch Loss: 0.34048521518707275\n",
      "Epoch 3231, Loss: 0.9831963181495667, Final Batch Loss: 0.4133305847644806\n",
      "Epoch 3232, Loss: 0.803707018494606, Final Batch Loss: 0.22038891911506653\n",
      "Epoch 3233, Loss: 0.8595393300056458, Final Batch Loss: 0.24624153971672058\n",
      "Epoch 3234, Loss: 0.9553286731243134, Final Batch Loss: 0.32234713435173035\n",
      "Epoch 3235, Loss: 0.8324330747127533, Final Batch Loss: 0.2209276556968689\n",
      "Epoch 3236, Loss: 0.8347896933555603, Final Batch Loss: 0.272888720035553\n",
      "Epoch 3237, Loss: 0.7998901158571243, Final Batch Loss: 0.15870200097560883\n",
      "Epoch 3238, Loss: 0.8436779379844666, Final Batch Loss: 0.2917746603488922\n",
      "Epoch 3239, Loss: 0.9231801331043243, Final Batch Loss: 0.38198786973953247\n",
      "Epoch 3240, Loss: 1.0019606351852417, Final Batch Loss: 0.31301283836364746\n",
      "Epoch 3241, Loss: 0.889479249715805, Final Batch Loss: 0.2732177972793579\n",
      "Epoch 3242, Loss: 0.9616342782974243, Final Batch Loss: 0.3844083547592163\n",
      "Epoch 3243, Loss: 0.9168461263179779, Final Batch Loss: 0.38080301880836487\n",
      "Epoch 3244, Loss: 0.8959454894065857, Final Batch Loss: 0.289472758769989\n",
      "Epoch 3245, Loss: 0.8874066472053528, Final Batch Loss: 0.3009868264198303\n",
      "Epoch 3246, Loss: 0.8594314754009247, Final Batch Loss: 0.32630011439323425\n",
      "Epoch 3247, Loss: 0.7731307744979858, Final Batch Loss: 0.24282044172286987\n",
      "Epoch 3248, Loss: 0.8527916073799133, Final Batch Loss: 0.2759285867214203\n",
      "Epoch 3249, Loss: 0.8096101880073547, Final Batch Loss: 0.26007431745529175\n",
      "Epoch 3250, Loss: 0.8488025069236755, Final Batch Loss: 0.21936103701591492\n",
      "Epoch 3251, Loss: 0.8964288532733917, Final Batch Loss: 0.28189828991889954\n",
      "Epoch 3252, Loss: 0.9288019835948944, Final Batch Loss: 0.3379482626914978\n",
      "Epoch 3253, Loss: 0.9064005017280579, Final Batch Loss: 0.3237881362438202\n",
      "Epoch 3254, Loss: 0.8757243305444717, Final Batch Loss: 0.2773182690143585\n",
      "Epoch 3255, Loss: 0.8525132387876511, Final Batch Loss: 0.24560461938381195\n",
      "Epoch 3256, Loss: 0.990926206111908, Final Batch Loss: 0.39958861470222473\n",
      "Epoch 3257, Loss: 0.9132261574268341, Final Batch Loss: 0.2960550785064697\n",
      "Epoch 3258, Loss: 0.9062756896018982, Final Batch Loss: 0.2652825117111206\n",
      "Epoch 3259, Loss: 0.868882566690445, Final Batch Loss: 0.25974804162979126\n",
      "Epoch 3260, Loss: 0.8666852414608002, Final Batch Loss: 0.25478821992874146\n",
      "Epoch 3261, Loss: 0.8937885165214539, Final Batch Loss: 0.2729871869087219\n",
      "Epoch 3262, Loss: 0.9413789808750153, Final Batch Loss: 0.3598516285419464\n",
      "Epoch 3263, Loss: 0.8930274993181229, Final Batch Loss: 0.37832221388816833\n",
      "Epoch 3264, Loss: 0.8327686637639999, Final Batch Loss: 0.31734853982925415\n",
      "Epoch 3265, Loss: 0.9551638662815094, Final Batch Loss: 0.330681711435318\n",
      "Epoch 3266, Loss: 0.7965148836374283, Final Batch Loss: 0.2604956328868866\n",
      "Epoch 3267, Loss: 0.8963994979858398, Final Batch Loss: 0.31101104617118835\n",
      "Epoch 3268, Loss: 0.8673549890518188, Final Batch Loss: 0.28381532430648804\n",
      "Epoch 3269, Loss: 0.8517282009124756, Final Batch Loss: 0.23484733700752258\n",
      "Epoch 3270, Loss: 0.8635789901018143, Final Batch Loss: 0.3469526171684265\n",
      "Epoch 3271, Loss: 0.9375515878200531, Final Batch Loss: 0.31697162985801697\n",
      "Epoch 3272, Loss: 0.875781923532486, Final Batch Loss: 0.3197915256023407\n",
      "Epoch 3273, Loss: 0.9572021812200546, Final Batch Loss: 0.4301443099975586\n",
      "Epoch 3274, Loss: 0.9007979333400726, Final Batch Loss: 0.285184770822525\n",
      "Epoch 3275, Loss: 0.7789888978004456, Final Batch Loss: 0.28164637088775635\n",
      "Epoch 3276, Loss: 0.9024471342563629, Final Batch Loss: 0.3332778513431549\n",
      "Epoch 3277, Loss: 1.059370681643486, Final Batch Loss: 0.5199485421180725\n",
      "Epoch 3278, Loss: 0.888667106628418, Final Batch Loss: 0.29757118225097656\n",
      "Epoch 3279, Loss: 0.9664263129234314, Final Batch Loss: 0.2823066711425781\n",
      "Epoch 3280, Loss: 0.8288635909557343, Final Batch Loss: 0.3247063457965851\n",
      "Epoch 3281, Loss: 0.8332944512367249, Final Batch Loss: 0.25354307889938354\n",
      "Epoch 3282, Loss: 0.8779293894767761, Final Batch Loss: 0.2155563235282898\n",
      "Epoch 3283, Loss: 0.8452140241861343, Final Batch Loss: 0.2599351108074188\n",
      "Epoch 3284, Loss: 0.7327663451433182, Final Batch Loss: 0.2574195861816406\n",
      "Epoch 3285, Loss: 0.7549851387739182, Final Batch Loss: 0.24553506076335907\n",
      "Epoch 3286, Loss: 0.989189088344574, Final Batch Loss: 0.32071226835250854\n",
      "Epoch 3287, Loss: 0.9193784892559052, Final Batch Loss: 0.34657612442970276\n",
      "Epoch 3288, Loss: 0.8560195863246918, Final Batch Loss: 0.3058679401874542\n",
      "Epoch 3289, Loss: 0.9360146969556808, Final Batch Loss: 0.37510424852371216\n",
      "Epoch 3290, Loss: 0.838273823261261, Final Batch Loss: 0.19758033752441406\n",
      "Epoch 3291, Loss: 0.8421266078948975, Final Batch Loss: 0.2992683947086334\n",
      "Epoch 3292, Loss: 0.814634844660759, Final Batch Loss: 0.25511935353279114\n",
      "Epoch 3293, Loss: 0.8429621160030365, Final Batch Loss: 0.28909704089164734\n",
      "Epoch 3294, Loss: 0.8990926444530487, Final Batch Loss: 0.3743952512741089\n",
      "Epoch 3295, Loss: 0.971728652715683, Final Batch Loss: 0.39221930503845215\n",
      "Epoch 3296, Loss: 0.825271263718605, Final Batch Loss: 0.24662457406520844\n",
      "Epoch 3297, Loss: 0.8625472486019135, Final Batch Loss: 0.25309890508651733\n",
      "Epoch 3298, Loss: 0.9582554996013641, Final Batch Loss: 0.35571494698524475\n",
      "Epoch 3299, Loss: 0.8884648531675339, Final Batch Loss: 0.34184905886650085\n",
      "Epoch 3300, Loss: 0.9003758728504181, Final Batch Loss: 0.36196568608283997\n",
      "Epoch 3301, Loss: 1.0425925105810165, Final Batch Loss: 0.48052749037742615\n",
      "Epoch 3302, Loss: 0.8949626088142395, Final Batch Loss: 0.24665117263793945\n",
      "Epoch 3303, Loss: 0.8322018831968307, Final Batch Loss: 0.23883281648159027\n",
      "Epoch 3304, Loss: 0.7896796464920044, Final Batch Loss: 0.20095211267471313\n",
      "Epoch 3305, Loss: 1.0094600915908813, Final Batch Loss: 0.4080599546432495\n",
      "Epoch 3306, Loss: 0.7928506433963776, Final Batch Loss: 0.3070407509803772\n",
      "Epoch 3307, Loss: 0.812497466802597, Final Batch Loss: 0.2799416184425354\n",
      "Epoch 3308, Loss: 0.821569412946701, Final Batch Loss: 0.30088749527931213\n",
      "Epoch 3309, Loss: 0.9451691806316376, Final Batch Loss: 0.36879289150238037\n",
      "Epoch 3310, Loss: 0.818269670009613, Final Batch Loss: 0.22883284091949463\n",
      "Epoch 3311, Loss: 0.8250100016593933, Final Batch Loss: 0.25770673155784607\n",
      "Epoch 3312, Loss: 0.9892174601554871, Final Batch Loss: 0.38965529203414917\n",
      "Epoch 3313, Loss: 0.9490042626857758, Final Batch Loss: 0.30701813101768494\n",
      "Epoch 3314, Loss: 0.8177334070205688, Final Batch Loss: 0.2511528432369232\n",
      "Epoch 3315, Loss: 0.8944915533065796, Final Batch Loss: 0.3765482008457184\n",
      "Epoch 3316, Loss: 0.8268032968044281, Final Batch Loss: 0.3147664964199066\n",
      "Epoch 3317, Loss: 0.7094363421201706, Final Batch Loss: 0.2102983444929123\n",
      "Epoch 3318, Loss: 0.7855285108089447, Final Batch Loss: 0.25263649225234985\n",
      "Epoch 3319, Loss: 0.8201486468315125, Final Batch Loss: 0.3175892233848572\n",
      "Epoch 3320, Loss: 0.9154064655303955, Final Batch Loss: 0.31361111998558044\n",
      "Epoch 3321, Loss: 0.9345700740814209, Final Batch Loss: 0.3826640248298645\n",
      "Epoch 3322, Loss: 0.8044124245643616, Final Batch Loss: 0.25610896944999695\n",
      "Epoch 3323, Loss: 0.8628503382205963, Final Batch Loss: 0.31222468614578247\n",
      "Epoch 3324, Loss: 0.9088146686553955, Final Batch Loss: 0.301693856716156\n",
      "Epoch 3325, Loss: 0.8688139617443085, Final Batch Loss: 0.2746453881263733\n",
      "Epoch 3326, Loss: 0.8419121205806732, Final Batch Loss: 0.28708407282829285\n",
      "Epoch 3327, Loss: 0.8730715811252594, Final Batch Loss: 0.31642550230026245\n",
      "Epoch 3328, Loss: 0.9087176620960236, Final Batch Loss: 0.3019357919692993\n",
      "Epoch 3329, Loss: 0.85088911652565, Final Batch Loss: 0.28223922848701477\n",
      "Epoch 3330, Loss: 0.8591828644275665, Final Batch Loss: 0.35448476672172546\n",
      "Epoch 3331, Loss: 0.85030797123909, Final Batch Loss: 0.23772621154785156\n",
      "Epoch 3332, Loss: 0.9395244121551514, Final Batch Loss: 0.29816287755966187\n",
      "Epoch 3333, Loss: 0.9242172539234161, Final Batch Loss: 0.31738945841789246\n",
      "Epoch 3334, Loss: 0.7814859449863434, Final Batch Loss: 0.2566383183002472\n",
      "Epoch 3335, Loss: 0.8217997699975967, Final Batch Loss: 0.2529352009296417\n",
      "Epoch 3336, Loss: 0.8856714367866516, Final Batch Loss: 0.29333382844924927\n",
      "Epoch 3337, Loss: 0.794636994600296, Final Batch Loss: 0.2618165612220764\n",
      "Epoch 3338, Loss: 0.8472267240285873, Final Batch Loss: 0.21777714788913727\n",
      "Epoch 3339, Loss: 0.9155722558498383, Final Batch Loss: 0.3223409950733185\n",
      "Epoch 3340, Loss: 0.8188932687044144, Final Batch Loss: 0.33503857254981995\n",
      "Epoch 3341, Loss: 0.7937034070491791, Final Batch Loss: 0.28522491455078125\n",
      "Epoch 3342, Loss: 1.0237115919589996, Final Batch Loss: 0.4259839951992035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3343, Loss: 0.932746171951294, Final Batch Loss: 0.35316887497901917\n",
      "Epoch 3344, Loss: 0.8833388984203339, Final Batch Loss: 0.28257882595062256\n",
      "Epoch 3345, Loss: 0.9071907997131348, Final Batch Loss: 0.28639844059944153\n",
      "Epoch 3346, Loss: 0.9219700247049332, Final Batch Loss: 0.30191633105278015\n",
      "Epoch 3347, Loss: 0.8638332635164261, Final Batch Loss: 0.28168195486068726\n",
      "Epoch 3348, Loss: 0.7997486740350723, Final Batch Loss: 0.22341082990169525\n",
      "Epoch 3349, Loss: 0.7744764685630798, Final Batch Loss: 0.22175857424736023\n",
      "Epoch 3350, Loss: 0.874654084444046, Final Batch Loss: 0.32043302059173584\n",
      "Epoch 3351, Loss: 0.8990723788738251, Final Batch Loss: 0.35350990295410156\n",
      "Epoch 3352, Loss: 0.8908856213092804, Final Batch Loss: 0.37184369564056396\n",
      "Epoch 3353, Loss: 1.0374074280261993, Final Batch Loss: 0.39122432470321655\n",
      "Epoch 3354, Loss: 0.8607726097106934, Final Batch Loss: 0.2721283435821533\n",
      "Epoch 3355, Loss: 0.9241508841514587, Final Batch Loss: 0.23977714776992798\n",
      "Epoch 3356, Loss: 0.8359832763671875, Final Batch Loss: 0.29572445154190063\n",
      "Epoch 3357, Loss: 0.7879405319690704, Final Batch Loss: 0.2628004252910614\n",
      "Epoch 3358, Loss: 0.861630767583847, Final Batch Loss: 0.26155397295951843\n",
      "Epoch 3359, Loss: 0.8102937936782837, Final Batch Loss: 0.15973025560379028\n",
      "Epoch 3360, Loss: 0.9251324534416199, Final Batch Loss: 0.2837473452091217\n",
      "Epoch 3361, Loss: 0.9428698718547821, Final Batch Loss: 0.3315993845462799\n",
      "Epoch 3362, Loss: 0.8324232846498489, Final Batch Loss: 0.19890569150447845\n",
      "Epoch 3363, Loss: 0.8939683735370636, Final Batch Loss: 0.32328808307647705\n",
      "Epoch 3364, Loss: 0.7129059433937073, Final Batch Loss: 0.24154964089393616\n",
      "Epoch 3365, Loss: 0.9462895691394806, Final Batch Loss: 0.38227811455726624\n",
      "Epoch 3366, Loss: 0.8131377846002579, Final Batch Loss: 0.20845435559749603\n",
      "Epoch 3367, Loss: 0.9976231157779694, Final Batch Loss: 0.4136808514595032\n",
      "Epoch 3368, Loss: 0.7481930702924728, Final Batch Loss: 0.1688469499349594\n",
      "Epoch 3369, Loss: 0.7907814234495163, Final Batch Loss: 0.30115827918052673\n",
      "Epoch 3370, Loss: 0.9425713121891022, Final Batch Loss: 0.32356205582618713\n",
      "Epoch 3371, Loss: 0.8436733782291412, Final Batch Loss: 0.2689620852470398\n",
      "Epoch 3372, Loss: 0.8900316059589386, Final Batch Loss: 0.3456805944442749\n",
      "Epoch 3373, Loss: 0.9111421406269073, Final Batch Loss: 0.3290858864784241\n",
      "Epoch 3374, Loss: 0.8711062073707581, Final Batch Loss: 0.3319719433784485\n",
      "Epoch 3375, Loss: 0.8613612353801727, Final Batch Loss: 0.29854294657707214\n",
      "Epoch 3376, Loss: 0.8948210179805756, Final Batch Loss: 0.33043473958969116\n",
      "Epoch 3377, Loss: 0.8616398274898529, Final Batch Loss: 0.2796282172203064\n",
      "Epoch 3378, Loss: 0.8571742624044418, Final Batch Loss: 0.2514381408691406\n",
      "Epoch 3379, Loss: 0.9504972994327545, Final Batch Loss: 0.3772893249988556\n",
      "Epoch 3380, Loss: 0.8843486607074738, Final Batch Loss: 0.3237321674823761\n",
      "Epoch 3381, Loss: 0.7498069554567337, Final Batch Loss: 0.18582044541835785\n",
      "Epoch 3382, Loss: 1.0030454397201538, Final Batch Loss: 0.41034096479415894\n",
      "Epoch 3383, Loss: 0.7998432964086533, Final Batch Loss: 0.1907189041376114\n",
      "Epoch 3384, Loss: 0.7937655746936798, Final Batch Loss: 0.21767768263816833\n",
      "Epoch 3385, Loss: 0.8221559524536133, Final Batch Loss: 0.2858181297779083\n",
      "Epoch 3386, Loss: 0.7959412634372711, Final Batch Loss: 0.2500409781932831\n",
      "Epoch 3387, Loss: 0.961589515209198, Final Batch Loss: 0.2760967016220093\n",
      "Epoch 3388, Loss: 0.8928236067295074, Final Batch Loss: 0.358146607875824\n",
      "Epoch 3389, Loss: 0.7261025756597519, Final Batch Loss: 0.22766192257404327\n",
      "Epoch 3390, Loss: 0.9159634411334991, Final Batch Loss: 0.29854121804237366\n",
      "Epoch 3391, Loss: 0.8474431186914444, Final Batch Loss: 0.31612643599510193\n",
      "Epoch 3392, Loss: 0.8286883234977722, Final Batch Loss: 0.2527688145637512\n",
      "Epoch 3393, Loss: 0.7099912911653519, Final Batch Loss: 0.18289947509765625\n",
      "Epoch 3394, Loss: 0.9758577644824982, Final Batch Loss: 0.44516971707344055\n",
      "Epoch 3395, Loss: 0.8031164854764938, Final Batch Loss: 0.2568047344684601\n",
      "Epoch 3396, Loss: 0.8054697811603546, Final Batch Loss: 0.25601205229759216\n",
      "Epoch 3397, Loss: 0.8048424273729324, Final Batch Loss: 0.22839508950710297\n",
      "Epoch 3398, Loss: 0.8225791752338409, Final Batch Loss: 0.22716253995895386\n",
      "Epoch 3399, Loss: 0.8332130014896393, Final Batch Loss: 0.3122512996196747\n",
      "Epoch 3400, Loss: 0.8645375967025757, Final Batch Loss: 0.33273178339004517\n",
      "Epoch 3401, Loss: 0.8609879612922668, Final Batch Loss: 0.25829270482063293\n",
      "Epoch 3402, Loss: 0.8648993670940399, Final Batch Loss: 0.2935059070587158\n",
      "Epoch 3403, Loss: 0.8771772384643555, Final Batch Loss: 0.31623268127441406\n",
      "Epoch 3404, Loss: 0.8135238587856293, Final Batch Loss: 0.29098257422447205\n",
      "Epoch 3405, Loss: 0.8901814222335815, Final Batch Loss: 0.29393041133880615\n",
      "Epoch 3406, Loss: 0.9062906801700592, Final Batch Loss: 0.29003942012786865\n",
      "Epoch 3407, Loss: 0.9057627171278, Final Batch Loss: 0.3714647889137268\n",
      "Epoch 3408, Loss: 0.9116576611995697, Final Batch Loss: 0.32673192024230957\n",
      "Epoch 3409, Loss: 0.791283130645752, Final Batch Loss: 0.26821067929267883\n",
      "Epoch 3410, Loss: 0.8801805973052979, Final Batch Loss: 0.28824013471603394\n",
      "Epoch 3411, Loss: 0.818786546587944, Final Batch Loss: 0.2869921624660492\n",
      "Epoch 3412, Loss: 0.8755401074886322, Final Batch Loss: 0.2481231689453125\n",
      "Epoch 3413, Loss: 0.8574807643890381, Final Batch Loss: 0.2897338569164276\n",
      "Epoch 3414, Loss: 0.9081669747829437, Final Batch Loss: 0.304460346698761\n",
      "Epoch 3415, Loss: 0.8398078233003616, Final Batch Loss: 0.30002138018608093\n",
      "Epoch 3416, Loss: 0.8756953179836273, Final Batch Loss: 0.29294553399086\n",
      "Epoch 3417, Loss: 0.9140853583812714, Final Batch Loss: 0.2757985293865204\n",
      "Epoch 3418, Loss: 0.8675650805234909, Final Batch Loss: 0.24908317625522614\n",
      "Epoch 3419, Loss: 0.8502350747585297, Final Batch Loss: 0.2726083993911743\n",
      "Epoch 3420, Loss: 0.753433033823967, Final Batch Loss: 0.1875176578760147\n",
      "Epoch 3421, Loss: 0.8941647708415985, Final Batch Loss: 0.34864553809165955\n",
      "Epoch 3422, Loss: 0.770262598991394, Final Batch Loss: 0.2950834035873413\n",
      "Epoch 3423, Loss: 0.867850124835968, Final Batch Loss: 0.30294185876846313\n",
      "Epoch 3424, Loss: 0.9032811224460602, Final Batch Loss: 0.30920103192329407\n",
      "Epoch 3425, Loss: 0.8286989629268646, Final Batch Loss: 0.289303183555603\n",
      "Epoch 3426, Loss: 0.9103996306657791, Final Batch Loss: 0.31858566403388977\n",
      "Epoch 3427, Loss: 0.7909469604492188, Final Batch Loss: 0.25534117221832275\n",
      "Epoch 3428, Loss: 0.766411229968071, Final Batch Loss: 0.2746513783931732\n",
      "Epoch 3429, Loss: 0.9495494961738586, Final Batch Loss: 0.422784686088562\n",
      "Epoch 3430, Loss: 0.9278010725975037, Final Batch Loss: 0.35401853919029236\n",
      "Epoch 3431, Loss: 0.8523002117872238, Final Batch Loss: 0.3430333435535431\n",
      "Epoch 3432, Loss: 0.7772857546806335, Final Batch Loss: 0.2511056959629059\n",
      "Epoch 3433, Loss: 0.777845948934555, Final Batch Loss: 0.2392529845237732\n",
      "Epoch 3434, Loss: 0.897056832909584, Final Batch Loss: 0.38904455304145813\n",
      "Epoch 3435, Loss: 0.8925634026527405, Final Batch Loss: 0.28090739250183105\n",
      "Epoch 3436, Loss: 0.8809939026832581, Final Batch Loss: 0.32062917947769165\n",
      "Epoch 3437, Loss: 0.8538370281457901, Final Batch Loss: 0.24071742594242096\n",
      "Epoch 3438, Loss: 0.8590514659881592, Final Batch Loss: 0.28450462222099304\n",
      "Epoch 3439, Loss: 0.7806456983089447, Final Batch Loss: 0.2041233479976654\n",
      "Epoch 3440, Loss: 0.86561119556427, Final Batch Loss: 0.2695159614086151\n",
      "Epoch 3441, Loss: 0.7567296773195267, Final Batch Loss: 0.2239421159029007\n",
      "Epoch 3442, Loss: 0.8970603942871094, Final Batch Loss: 0.32254502177238464\n",
      "Epoch 3443, Loss: 0.9014399945735931, Final Batch Loss: 0.2564830183982849\n",
      "Epoch 3444, Loss: 0.8557195067405701, Final Batch Loss: 0.2655079960823059\n",
      "Epoch 3445, Loss: 0.9074492156505585, Final Batch Loss: 0.2633979618549347\n",
      "Epoch 3446, Loss: 0.7825247347354889, Final Batch Loss: 0.2518119513988495\n",
      "Epoch 3447, Loss: 0.8131354749202728, Final Batch Loss: 0.26511287689208984\n",
      "Epoch 3448, Loss: 0.8427773863077164, Final Batch Loss: 0.3156551718711853\n",
      "Epoch 3449, Loss: 0.8130486905574799, Final Batch Loss: 0.2464088797569275\n",
      "Epoch 3450, Loss: 0.8406869918107986, Final Batch Loss: 0.30978646874427795\n",
      "Epoch 3451, Loss: 0.8153004050254822, Final Batch Loss: 0.32396256923675537\n",
      "Epoch 3452, Loss: 0.8274418264627457, Final Batch Loss: 0.2687101662158966\n",
      "Epoch 3453, Loss: 0.9178980588912964, Final Batch Loss: 0.32614487409591675\n",
      "Epoch 3454, Loss: 0.8793370127677917, Final Batch Loss: 0.32800742983818054\n",
      "Epoch 3455, Loss: 0.8666316568851471, Final Batch Loss: 0.2564878761768341\n",
      "Epoch 3456, Loss: 0.8677289038896561, Final Batch Loss: 0.3254876434803009\n",
      "Epoch 3457, Loss: 0.743842363357544, Final Batch Loss: 0.22683635354042053\n",
      "Epoch 3458, Loss: 0.8479474931955338, Final Batch Loss: 0.23698030412197113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3459, Loss: 0.8382146060466766, Final Batch Loss: 0.39525917172431946\n",
      "Epoch 3460, Loss: 0.9079798758029938, Final Batch Loss: 0.37949490547180176\n",
      "Epoch 3461, Loss: 0.9139202237129211, Final Batch Loss: 0.3051357865333557\n",
      "Epoch 3462, Loss: 0.8639865517616272, Final Batch Loss: 0.28046759963035583\n",
      "Epoch 3463, Loss: 0.7838200628757477, Final Batch Loss: 0.24687573313713074\n",
      "Epoch 3464, Loss: 0.8312851786613464, Final Batch Loss: 0.26939457654953003\n",
      "Epoch 3465, Loss: 0.810303658246994, Final Batch Loss: 0.23192442953586578\n",
      "Epoch 3466, Loss: 0.8303749710321426, Final Batch Loss: 0.1864011138677597\n",
      "Epoch 3467, Loss: 0.9194088578224182, Final Batch Loss: 0.366706520318985\n",
      "Epoch 3468, Loss: 0.8082553595304489, Final Batch Loss: 0.25908660888671875\n",
      "Epoch 3469, Loss: 0.8642816245555878, Final Batch Loss: 0.3071053624153137\n",
      "Epoch 3470, Loss: 0.8465263843536377, Final Batch Loss: 0.26058825850486755\n",
      "Epoch 3471, Loss: 0.7784989178180695, Final Batch Loss: 0.19856971502304077\n",
      "Epoch 3472, Loss: 0.836094856262207, Final Batch Loss: 0.2874085605144501\n",
      "Epoch 3473, Loss: 0.8161084055900574, Final Batch Loss: 0.25919800996780396\n",
      "Epoch 3474, Loss: 0.8105634897947311, Final Batch Loss: 0.2694736123085022\n",
      "Epoch 3475, Loss: 0.8665859401226044, Final Batch Loss: 0.25137192010879517\n",
      "Epoch 3476, Loss: 0.8903945982456207, Final Batch Loss: 0.25989532470703125\n",
      "Epoch 3477, Loss: 0.886792778968811, Final Batch Loss: 0.3155234456062317\n",
      "Epoch 3478, Loss: 0.8059519082307816, Final Batch Loss: 0.2588340938091278\n",
      "Epoch 3479, Loss: 0.8975858986377716, Final Batch Loss: 0.23402389883995056\n",
      "Epoch 3480, Loss: 0.7392176687717438, Final Batch Loss: 0.23437990248203278\n",
      "Epoch 3481, Loss: 0.8810339719057083, Final Batch Loss: 0.3062487542629242\n",
      "Epoch 3482, Loss: 0.7658254206180573, Final Batch Loss: 0.16599562764167786\n",
      "Epoch 3483, Loss: 0.8930060565471649, Final Batch Loss: 0.30096927285194397\n",
      "Epoch 3484, Loss: 0.8669326305389404, Final Batch Loss: 0.29992586374282837\n",
      "Epoch 3485, Loss: 0.999580442905426, Final Batch Loss: 0.37859222292900085\n",
      "Epoch 3486, Loss: 0.9622862488031387, Final Batch Loss: 0.36565661430358887\n",
      "Epoch 3487, Loss: 0.7445192486047745, Final Batch Loss: 0.14890645444393158\n",
      "Epoch 3488, Loss: 0.8116779178380966, Final Batch Loss: 0.31368234753608704\n",
      "Epoch 3489, Loss: 1.0751141011714935, Final Batch Loss: 0.46448057889938354\n",
      "Epoch 3490, Loss: 0.7857516258955002, Final Batch Loss: 0.1995648890733719\n",
      "Epoch 3491, Loss: 0.8205332458019257, Final Batch Loss: 0.26572293043136597\n",
      "Epoch 3492, Loss: 0.7632699906826019, Final Batch Loss: 0.25987064838409424\n",
      "Epoch 3493, Loss: 0.9127856194972992, Final Batch Loss: 0.3924250900745392\n",
      "Epoch 3494, Loss: 1.0552628338336945, Final Batch Loss: 0.4496215283870697\n",
      "Epoch 3495, Loss: 0.767486497759819, Final Batch Loss: 0.2513190805912018\n",
      "Epoch 3496, Loss: 0.7746108770370483, Final Batch Loss: 0.2141789346933365\n",
      "Epoch 3497, Loss: 0.8698606789112091, Final Batch Loss: 0.3036215305328369\n",
      "Epoch 3498, Loss: 0.8241424858570099, Final Batch Loss: 0.2558535933494568\n",
      "Epoch 3499, Loss: 0.9394767880439758, Final Batch Loss: 0.2985067069530487\n",
      "Epoch 3500, Loss: 0.8996364176273346, Final Batch Loss: 0.3438848555088043\n",
      "Epoch 3501, Loss: 0.8121042996644974, Final Batch Loss: 0.20667250454425812\n",
      "Epoch 3502, Loss: 0.8596860319375992, Final Batch Loss: 0.33415931463241577\n",
      "Epoch 3503, Loss: 0.8583536893129349, Final Batch Loss: 0.33380740880966187\n",
      "Epoch 3504, Loss: 0.8687765896320343, Final Batch Loss: 0.2567598819732666\n",
      "Epoch 3505, Loss: 0.9562219381332397, Final Batch Loss: 0.38700324296951294\n",
      "Epoch 3506, Loss: 0.8356158286333084, Final Batch Loss: 0.28256621956825256\n",
      "Epoch 3507, Loss: 0.8539636433124542, Final Batch Loss: 0.2905910015106201\n",
      "Epoch 3508, Loss: 0.8858226984739304, Final Batch Loss: 0.2996446192264557\n",
      "Epoch 3509, Loss: 0.8592653870582581, Final Batch Loss: 0.28799575567245483\n",
      "Epoch 3510, Loss: 0.848466083407402, Final Batch Loss: 0.3211674392223358\n",
      "Epoch 3511, Loss: 0.7432485222816467, Final Batch Loss: 0.28220200538635254\n",
      "Epoch 3512, Loss: 0.8707947582006454, Final Batch Loss: 0.2356439083814621\n",
      "Epoch 3513, Loss: 0.8707228153944016, Final Batch Loss: 0.34150218963623047\n",
      "Epoch 3514, Loss: 0.8532207608222961, Final Batch Loss: 0.28478190302848816\n",
      "Epoch 3515, Loss: 0.8962201178073883, Final Batch Loss: 0.2932308316230774\n",
      "Epoch 3516, Loss: 0.9240568280220032, Final Batch Loss: 0.3002206087112427\n",
      "Epoch 3517, Loss: 0.9720313549041748, Final Batch Loss: 0.33120328187942505\n",
      "Epoch 3518, Loss: 0.8563793152570724, Final Batch Loss: 0.3288077712059021\n",
      "Epoch 3519, Loss: 0.8056299835443497, Final Batch Loss: 0.24004186689853668\n",
      "Epoch 3520, Loss: 0.9412790834903717, Final Batch Loss: 0.32108765840530396\n",
      "Epoch 3521, Loss: 0.6764515340328217, Final Batch Loss: 0.17454583942890167\n",
      "Epoch 3522, Loss: 0.8729928135871887, Final Batch Loss: 0.27730950713157654\n",
      "Epoch 3523, Loss: 0.8038092255592346, Final Batch Loss: 0.2878289818763733\n",
      "Epoch 3524, Loss: 0.8111521750688553, Final Batch Loss: 0.22844655811786652\n",
      "Epoch 3525, Loss: 0.887781947851181, Final Batch Loss: 0.28344103693962097\n",
      "Epoch 3526, Loss: 0.9178806394338608, Final Batch Loss: 0.24524010717868805\n",
      "Epoch 3527, Loss: 0.7737725377082825, Final Batch Loss: 0.2125793695449829\n",
      "Epoch 3528, Loss: 0.894978791475296, Final Batch Loss: 0.2504342794418335\n",
      "Epoch 3529, Loss: 0.8311023116111755, Final Batch Loss: 0.19986620545387268\n",
      "Epoch 3530, Loss: 0.803687572479248, Final Batch Loss: 0.28908875584602356\n",
      "Epoch 3531, Loss: 0.8078672736883163, Final Batch Loss: 0.29728347063064575\n",
      "Epoch 3532, Loss: 0.8227312862873077, Final Batch Loss: 0.31920987367630005\n",
      "Epoch 3533, Loss: 0.760375052690506, Final Batch Loss: 0.25183913111686707\n",
      "Epoch 3534, Loss: 0.8595214784145355, Final Batch Loss: 0.2312522828578949\n",
      "Epoch 3535, Loss: 0.7814366966485977, Final Batch Loss: 0.3010713756084442\n",
      "Epoch 3536, Loss: 0.8408670425415039, Final Batch Loss: 0.27881932258605957\n",
      "Epoch 3537, Loss: 0.7987940609455109, Final Batch Loss: 0.27857980132102966\n",
      "Epoch 3538, Loss: 0.8118087947368622, Final Batch Loss: 0.31810441613197327\n",
      "Epoch 3539, Loss: 0.8028946220874786, Final Batch Loss: 0.31395313143730164\n",
      "Epoch 3540, Loss: 0.9412510097026825, Final Batch Loss: 0.28251782059669495\n",
      "Epoch 3541, Loss: 0.9186203479766846, Final Batch Loss: 0.36820152401924133\n",
      "Epoch 3542, Loss: 0.7433802038431168, Final Batch Loss: 0.2742892801761627\n",
      "Epoch 3543, Loss: 0.8630172908306122, Final Batch Loss: 0.2755938470363617\n",
      "Epoch 3544, Loss: 0.7746139615774155, Final Batch Loss: 0.2118145078420639\n",
      "Epoch 3545, Loss: 0.8694093823432922, Final Batch Loss: 0.3433113098144531\n",
      "Epoch 3546, Loss: 0.7606756687164307, Final Batch Loss: 0.2456020563840866\n",
      "Epoch 3547, Loss: 0.8119380176067352, Final Batch Loss: 0.2307043969631195\n",
      "Epoch 3548, Loss: 0.8091665208339691, Final Batch Loss: 0.2542041540145874\n",
      "Epoch 3549, Loss: 0.8591517210006714, Final Batch Loss: 0.2861490249633789\n",
      "Epoch 3550, Loss: 0.8908518254756927, Final Batch Loss: 0.3043891489505768\n",
      "Epoch 3551, Loss: 0.703060045838356, Final Batch Loss: 0.1895454227924347\n",
      "Epoch 3552, Loss: 0.7589192986488342, Final Batch Loss: 0.24638287723064423\n",
      "Epoch 3553, Loss: 0.9304418563842773, Final Batch Loss: 0.3298322260379791\n",
      "Epoch 3554, Loss: 0.8638196289539337, Final Batch Loss: 0.27116459608078003\n",
      "Epoch 3555, Loss: 0.9720565676689148, Final Batch Loss: 0.3704763650894165\n",
      "Epoch 3556, Loss: 0.9032894670963287, Final Batch Loss: 0.24791398644447327\n",
      "Epoch 3557, Loss: 0.8157993704080582, Final Batch Loss: 0.28287407755851746\n",
      "Epoch 3558, Loss: 0.8404715657234192, Final Batch Loss: 0.3193924129009247\n",
      "Epoch 3559, Loss: 0.9024432301521301, Final Batch Loss: 0.3663085103034973\n",
      "Epoch 3560, Loss: 0.7948229610919952, Final Batch Loss: 0.24444833397865295\n",
      "Epoch 3561, Loss: 0.7398998737335205, Final Batch Loss: 0.2219134271144867\n",
      "Epoch 3562, Loss: 0.8960399329662323, Final Batch Loss: 0.39327141642570496\n",
      "Epoch 3563, Loss: 0.8979671448469162, Final Batch Loss: 0.3431793451309204\n",
      "Epoch 3564, Loss: 1.0353150218725204, Final Batch Loss: 0.4284929633140564\n",
      "Epoch 3565, Loss: 0.7952076345682144, Final Batch Loss: 0.21319447457790375\n",
      "Epoch 3566, Loss: 0.8369666337966919, Final Batch Loss: 0.3155291676521301\n",
      "Epoch 3567, Loss: 0.8905706852674484, Final Batch Loss: 0.40470483899116516\n",
      "Epoch 3568, Loss: 0.8064269870519638, Final Batch Loss: 0.2591933608055115\n",
      "Epoch 3569, Loss: 0.8189012408256531, Final Batch Loss: 0.22138537466526031\n",
      "Epoch 3570, Loss: 0.8387901186943054, Final Batch Loss: 0.309093713760376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3571, Loss: 0.8977134823799133, Final Batch Loss: 0.3682501018047333\n",
      "Epoch 3572, Loss: 0.7578529417514801, Final Batch Loss: 0.21057257056236267\n",
      "Epoch 3573, Loss: 0.8441394120454788, Final Batch Loss: 0.28186365962028503\n",
      "Epoch 3574, Loss: 0.8401155769824982, Final Batch Loss: 0.33658143877983093\n",
      "Epoch 3575, Loss: 0.7951843738555908, Final Batch Loss: 0.26278355717658997\n",
      "Epoch 3576, Loss: 0.8770633041858673, Final Batch Loss: 0.33445075154304504\n",
      "Epoch 3577, Loss: 0.8379513621330261, Final Batch Loss: 0.22186526656150818\n",
      "Epoch 3578, Loss: 0.9390210211277008, Final Batch Loss: 0.36619728803634644\n",
      "Epoch 3579, Loss: 0.8399327993392944, Final Batch Loss: 0.31345176696777344\n",
      "Epoch 3580, Loss: 0.8582490086555481, Final Batch Loss: 0.29169487953186035\n",
      "Epoch 3581, Loss: 0.8727698624134064, Final Batch Loss: 0.30942386388778687\n",
      "Epoch 3582, Loss: 0.7218895256519318, Final Batch Loss: 0.2252349853515625\n",
      "Epoch 3583, Loss: 0.7879695296287537, Final Batch Loss: 0.29018881916999817\n",
      "Epoch 3584, Loss: 0.8063226640224457, Final Batch Loss: 0.348281592130661\n",
      "Epoch 3585, Loss: 0.8917407393455505, Final Batch Loss: 0.3188617527484894\n",
      "Epoch 3586, Loss: 0.8177312165498734, Final Batch Loss: 0.21575908362865448\n",
      "Epoch 3587, Loss: 0.7725169062614441, Final Batch Loss: 0.22038725018501282\n",
      "Epoch 3588, Loss: 0.8811464011669159, Final Batch Loss: 0.383512407541275\n",
      "Epoch 3589, Loss: 0.8541346192359924, Final Batch Loss: 0.259275883436203\n",
      "Epoch 3590, Loss: 0.8489259481430054, Final Batch Loss: 0.24900701642036438\n",
      "Epoch 3591, Loss: 0.7537244260311127, Final Batch Loss: 0.2609561085700989\n",
      "Epoch 3592, Loss: 0.8756587654352188, Final Batch Loss: 0.2418164163827896\n",
      "Epoch 3593, Loss: 0.9027676284313202, Final Batch Loss: 0.3219883441925049\n",
      "Epoch 3594, Loss: 0.8190504610538483, Final Batch Loss: 0.2550872266292572\n",
      "Epoch 3595, Loss: 0.7727913409471512, Final Batch Loss: 0.19186623394489288\n",
      "Epoch 3596, Loss: 0.8301494121551514, Final Batch Loss: 0.28277578949928284\n",
      "Epoch 3597, Loss: 0.7778015285730362, Final Batch Loss: 0.2645241320133209\n",
      "Epoch 3598, Loss: 0.885445773601532, Final Batch Loss: 0.25876370072364807\n",
      "Epoch 3599, Loss: 0.8632870316505432, Final Batch Loss: 0.29936483502388\n",
      "Epoch 3600, Loss: 0.7721899747848511, Final Batch Loss: 0.2676156163215637\n",
      "Epoch 3601, Loss: 1.0004704296588898, Final Batch Loss: 0.4199521541595459\n",
      "Epoch 3602, Loss: 0.8353103399276733, Final Batch Loss: 0.33036744594573975\n",
      "Epoch 3603, Loss: 0.8642032444477081, Final Batch Loss: 0.30486300587654114\n",
      "Epoch 3604, Loss: 0.749632865190506, Final Batch Loss: 0.1937008500099182\n",
      "Epoch 3605, Loss: 0.812842458486557, Final Batch Loss: 0.24659103155136108\n",
      "Epoch 3606, Loss: 0.8792821764945984, Final Batch Loss: 0.2978137135505676\n",
      "Epoch 3607, Loss: 0.8437567204236984, Final Batch Loss: 0.2722747027873993\n",
      "Epoch 3608, Loss: 0.8772235810756683, Final Batch Loss: 0.32656586170196533\n",
      "Epoch 3609, Loss: 0.8995944559574127, Final Batch Loss: 0.3730051815509796\n",
      "Epoch 3610, Loss: 0.740375429391861, Final Batch Loss: 0.2226235568523407\n",
      "Epoch 3611, Loss: 0.7613374888896942, Final Batch Loss: 0.24042323231697083\n",
      "Epoch 3612, Loss: 0.9357733130455017, Final Batch Loss: 0.3791045844554901\n",
      "Epoch 3613, Loss: 0.7922863364219666, Final Batch Loss: 0.2213878631591797\n",
      "Epoch 3614, Loss: 0.7451976537704468, Final Batch Loss: 0.2617228031158447\n",
      "Epoch 3615, Loss: 0.906414270401001, Final Batch Loss: 0.31651270389556885\n",
      "Epoch 3616, Loss: 0.8876586556434631, Final Batch Loss: 0.282427042722702\n",
      "Epoch 3617, Loss: 0.8585293591022491, Final Batch Loss: 0.3260895907878876\n",
      "Epoch 3618, Loss: 0.7638161033391953, Final Batch Loss: 0.2555462121963501\n",
      "Epoch 3619, Loss: 0.8491152673959732, Final Batch Loss: 0.2709203064441681\n",
      "Epoch 3620, Loss: 0.7991746068000793, Final Batch Loss: 0.3118706941604614\n",
      "Epoch 3621, Loss: 0.7707379758358002, Final Batch Loss: 0.24583499133586884\n",
      "Epoch 3622, Loss: 0.7283864319324493, Final Batch Loss: 0.17507997155189514\n",
      "Epoch 3623, Loss: 0.8173566609621048, Final Batch Loss: 0.3442336916923523\n",
      "Epoch 3624, Loss: 0.7463776916265488, Final Batch Loss: 0.24597744643688202\n",
      "Epoch 3625, Loss: 0.8818329870700836, Final Batch Loss: 0.3448609411716461\n",
      "Epoch 3626, Loss: 0.7490286231040955, Final Batch Loss: 0.18420882523059845\n",
      "Epoch 3627, Loss: 0.7695588618516922, Final Batch Loss: 0.20986603200435638\n",
      "Epoch 3628, Loss: 0.7143533676862717, Final Batch Loss: 0.19680947065353394\n",
      "Epoch 3629, Loss: 0.7553878426551819, Final Batch Loss: 0.2603934705257416\n",
      "Epoch 3630, Loss: 0.7936220467090607, Final Batch Loss: 0.18999436497688293\n",
      "Epoch 3631, Loss: 0.8945020884275436, Final Batch Loss: 0.3538605868816376\n",
      "Epoch 3632, Loss: 0.8005049228668213, Final Batch Loss: 0.26522019505500793\n",
      "Epoch 3633, Loss: 0.7895170003175735, Final Batch Loss: 0.25470277667045593\n",
      "Epoch 3634, Loss: 0.8225847035646439, Final Batch Loss: 0.24574105441570282\n",
      "Epoch 3635, Loss: 0.7644510865211487, Final Batch Loss: 0.22644606232643127\n",
      "Epoch 3636, Loss: 0.8466873168945312, Final Batch Loss: 0.37260109186172485\n",
      "Epoch 3637, Loss: 0.8511967062950134, Final Batch Loss: 0.26238882541656494\n",
      "Epoch 3638, Loss: 0.8220617920160294, Final Batch Loss: 0.2136567384004593\n",
      "Epoch 3639, Loss: 0.8149402439594269, Final Batch Loss: 0.27424609661102295\n",
      "Epoch 3640, Loss: 0.7884650230407715, Final Batch Loss: 0.29237526655197144\n",
      "Epoch 3641, Loss: 0.7882901281118393, Final Batch Loss: 0.2883342504501343\n",
      "Epoch 3642, Loss: 0.7180052548646927, Final Batch Loss: 0.22753174602985382\n",
      "Epoch 3643, Loss: 0.7759560346603394, Final Batch Loss: 0.2158125340938568\n",
      "Epoch 3644, Loss: 0.866441935300827, Final Batch Loss: 0.31193476915359497\n",
      "Epoch 3645, Loss: 0.7386337667703629, Final Batch Loss: 0.1879885345697403\n",
      "Epoch 3646, Loss: 0.8294831216335297, Final Batch Loss: 0.2584342658519745\n",
      "Epoch 3647, Loss: 0.7449478209018707, Final Batch Loss: 0.16955575346946716\n",
      "Epoch 3648, Loss: 0.874883845448494, Final Batch Loss: 0.3112236559391022\n",
      "Epoch 3649, Loss: 0.829262375831604, Final Batch Loss: 0.29191815853118896\n",
      "Epoch 3650, Loss: 0.8167884349822998, Final Batch Loss: 0.2403157651424408\n",
      "Epoch 3651, Loss: 0.8383392244577408, Final Batch Loss: 0.25968673825263977\n",
      "Epoch 3652, Loss: 0.7295424342155457, Final Batch Loss: 0.20331859588623047\n",
      "Epoch 3653, Loss: 0.8322986215353012, Final Batch Loss: 0.2840460538864136\n",
      "Epoch 3654, Loss: 0.8770259916782379, Final Batch Loss: 0.20296543836593628\n",
      "Epoch 3655, Loss: 0.8717465996742249, Final Batch Loss: 0.2530478537082672\n",
      "Epoch 3656, Loss: 0.8441772758960724, Final Batch Loss: 0.2503686845302582\n",
      "Epoch 3657, Loss: 0.7730617672204971, Final Batch Loss: 0.3004220426082611\n",
      "Epoch 3658, Loss: 0.8609821796417236, Final Batch Loss: 0.3443022668361664\n",
      "Epoch 3659, Loss: 0.8313543200492859, Final Batch Loss: 0.2903061509132385\n",
      "Epoch 3660, Loss: 0.8002327680587769, Final Batch Loss: 0.2750696837902069\n",
      "Epoch 3661, Loss: 0.8034517019987106, Final Batch Loss: 0.29650646448135376\n",
      "Epoch 3662, Loss: 0.7743675112724304, Final Batch Loss: 0.23683904111385345\n",
      "Epoch 3663, Loss: 0.6820706427097321, Final Batch Loss: 0.2021230310201645\n",
      "Epoch 3664, Loss: 0.6940555423498154, Final Batch Loss: 0.19625914096832275\n",
      "Epoch 3665, Loss: 0.763490304350853, Final Batch Loss: 0.25516945123672485\n",
      "Epoch 3666, Loss: 0.7874681353569031, Final Batch Loss: 0.30701348185539246\n",
      "Epoch 3667, Loss: 0.8707744777202606, Final Batch Loss: 0.3008507490158081\n",
      "Epoch 3668, Loss: 0.8117713779211044, Final Batch Loss: 0.30272728204727173\n",
      "Epoch 3669, Loss: 0.9307430386543274, Final Batch Loss: 0.3318733274936676\n",
      "Epoch 3670, Loss: 0.859128326177597, Final Batch Loss: 0.2728114426136017\n",
      "Epoch 3671, Loss: 0.729585811495781, Final Batch Loss: 0.1803463101387024\n",
      "Epoch 3672, Loss: 0.7376587241888046, Final Batch Loss: 0.20075295865535736\n",
      "Epoch 3673, Loss: 0.7839852869510651, Final Batch Loss: 0.23161429166793823\n",
      "Epoch 3674, Loss: 0.8360586017370224, Final Batch Loss: 0.2672996520996094\n",
      "Epoch 3675, Loss: 0.7505785524845123, Final Batch Loss: 0.21991917490959167\n",
      "Epoch 3676, Loss: 0.8358911275863647, Final Batch Loss: 0.24730068445205688\n",
      "Epoch 3677, Loss: 0.7758881151676178, Final Batch Loss: 0.2626497149467468\n",
      "Epoch 3678, Loss: 0.9479123055934906, Final Batch Loss: 0.4588353931903839\n",
      "Epoch 3679, Loss: 0.7556274831295013, Final Batch Loss: 0.2634318172931671\n",
      "Epoch 3680, Loss: 0.8267587870359421, Final Batch Loss: 0.2480447143316269\n",
      "Epoch 3681, Loss: 0.8368743062019348, Final Batch Loss: 0.3066108822822571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3682, Loss: 0.7498132586479187, Final Batch Loss: 0.270457923412323\n",
      "Epoch 3683, Loss: 0.7606626749038696, Final Batch Loss: 0.25348562002182007\n",
      "Epoch 3684, Loss: 0.8019406497478485, Final Batch Loss: 0.2669404447078705\n",
      "Epoch 3685, Loss: 0.8315259665250778, Final Batch Loss: 0.17364172637462616\n",
      "Epoch 3686, Loss: 0.7469308823347092, Final Batch Loss: 0.18449003994464874\n",
      "Epoch 3687, Loss: 0.8029255867004395, Final Batch Loss: 0.2630700170993805\n",
      "Epoch 3688, Loss: 0.7790097296237946, Final Batch Loss: 0.2821238934993744\n",
      "Epoch 3689, Loss: 0.7915714979171753, Final Batch Loss: 0.2706342339515686\n",
      "Epoch 3690, Loss: 0.8634312152862549, Final Batch Loss: 0.2696245610713959\n",
      "Epoch 3691, Loss: 0.7823450267314911, Final Batch Loss: 0.24496468901634216\n",
      "Epoch 3692, Loss: 0.7822245955467224, Final Batch Loss: 0.2613890767097473\n",
      "Epoch 3693, Loss: 0.8787684589624405, Final Batch Loss: 0.3006482720375061\n",
      "Epoch 3694, Loss: 0.733366921544075, Final Batch Loss: 0.2425633668899536\n",
      "Epoch 3695, Loss: 0.8129894435405731, Final Batch Loss: 0.2763049006462097\n",
      "Epoch 3696, Loss: 0.796940878033638, Final Batch Loss: 0.22876717150211334\n",
      "Epoch 3697, Loss: 0.8358388543128967, Final Batch Loss: 0.28890272974967957\n",
      "Epoch 3698, Loss: 0.8443354666233063, Final Batch Loss: 0.2827162444591522\n",
      "Epoch 3699, Loss: 0.8404791802167892, Final Batch Loss: 0.29325973987579346\n",
      "Epoch 3700, Loss: 0.8242779672145844, Final Batch Loss: 0.3572220206260681\n",
      "Epoch 3701, Loss: 0.8142104744911194, Final Batch Loss: 0.24257908761501312\n",
      "Epoch 3702, Loss: 0.7984298169612885, Final Batch Loss: 0.24023312330245972\n",
      "Epoch 3703, Loss: 0.7953912615776062, Final Batch Loss: 0.24804574251174927\n",
      "Epoch 3704, Loss: 0.6761702597141266, Final Batch Loss: 0.2220793217420578\n",
      "Epoch 3705, Loss: 0.8700517863035202, Final Batch Loss: 0.28574100136756897\n",
      "Epoch 3706, Loss: 0.7732850909233093, Final Batch Loss: 0.2668835520744324\n",
      "Epoch 3707, Loss: 0.8309901654720306, Final Batch Loss: 0.2948194444179535\n",
      "Epoch 3708, Loss: 0.8133063018321991, Final Batch Loss: 0.27533990144729614\n",
      "Epoch 3709, Loss: 0.8832189738750458, Final Batch Loss: 0.35551825165748596\n",
      "Epoch 3710, Loss: 0.768034502863884, Final Batch Loss: 0.2047320455312729\n",
      "Epoch 3711, Loss: 0.8397738933563232, Final Batch Loss: 0.3200235664844513\n",
      "Epoch 3712, Loss: 0.8466085195541382, Final Batch Loss: 0.3014912009239197\n",
      "Epoch 3713, Loss: 0.7755599021911621, Final Batch Loss: 0.25586894154548645\n",
      "Epoch 3714, Loss: 0.7942719161510468, Final Batch Loss: 0.254721075296402\n",
      "Epoch 3715, Loss: 0.8754235208034515, Final Batch Loss: 0.29755744338035583\n",
      "Epoch 3716, Loss: 0.8225328624248505, Final Batch Loss: 0.27123114466667175\n",
      "Epoch 3717, Loss: 0.6981024891138077, Final Batch Loss: 0.20842309296131134\n",
      "Epoch 3718, Loss: 0.7574161440134048, Final Batch Loss: 0.2547006905078888\n",
      "Epoch 3719, Loss: 0.8885612785816193, Final Batch Loss: 0.40226173400878906\n",
      "Epoch 3720, Loss: 0.7900251001119614, Final Batch Loss: 0.2712115943431854\n",
      "Epoch 3721, Loss: 0.825631633400917, Final Batch Loss: 0.26532769203186035\n",
      "Epoch 3722, Loss: 0.8654931783676147, Final Batch Loss: 0.3898003101348877\n",
      "Epoch 3723, Loss: 0.7705865800380707, Final Batch Loss: 0.26204776763916016\n",
      "Epoch 3724, Loss: 0.7792212665081024, Final Batch Loss: 0.24796758592128754\n",
      "Epoch 3725, Loss: 0.73365218937397, Final Batch Loss: 0.21484039723873138\n",
      "Epoch 3726, Loss: 0.752114400267601, Final Batch Loss: 0.21079067885875702\n",
      "Epoch 3727, Loss: 0.7786538898944855, Final Batch Loss: 0.2869572341442108\n",
      "Epoch 3728, Loss: 0.8131785988807678, Final Batch Loss: 0.2314867377281189\n",
      "Epoch 3729, Loss: 0.7575335651636124, Final Batch Loss: 0.23520325124263763\n",
      "Epoch 3730, Loss: 0.8015796989202499, Final Batch Loss: 0.2673196792602539\n",
      "Epoch 3731, Loss: 0.8609984815120697, Final Batch Loss: 0.23934921622276306\n",
      "Epoch 3732, Loss: 0.794177919626236, Final Batch Loss: 0.3333152234554291\n",
      "Epoch 3733, Loss: 0.824390098452568, Final Batch Loss: 0.2287842333316803\n",
      "Epoch 3734, Loss: 0.7407210320234299, Final Batch Loss: 0.18198229372501373\n",
      "Epoch 3735, Loss: 0.8623939454555511, Final Batch Loss: 0.24658459424972534\n",
      "Epoch 3736, Loss: 0.8981829583644867, Final Batch Loss: 0.33326229453086853\n",
      "Epoch 3737, Loss: 0.791596457362175, Final Batch Loss: 0.31397151947021484\n",
      "Epoch 3738, Loss: 0.7343059629201889, Final Batch Loss: 0.24749603867530823\n",
      "Epoch 3739, Loss: 0.8119827806949615, Final Batch Loss: 0.24164769053459167\n",
      "Epoch 3740, Loss: 0.9161179065704346, Final Batch Loss: 0.3263051509857178\n",
      "Epoch 3741, Loss: 0.8074635565280914, Final Batch Loss: 0.29897549748420715\n",
      "Epoch 3742, Loss: 0.7563023418188095, Final Batch Loss: 0.23768694698810577\n",
      "Epoch 3743, Loss: 0.7908725589513779, Final Batch Loss: 0.24818989634513855\n",
      "Epoch 3744, Loss: 0.7640361338853836, Final Batch Loss: 0.2770179510116577\n",
      "Epoch 3745, Loss: 0.7366810888051987, Final Batch Loss: 0.20954503118991852\n",
      "Epoch 3746, Loss: 0.7506669759750366, Final Batch Loss: 0.23802633583545685\n",
      "Epoch 3747, Loss: 0.7807387709617615, Final Batch Loss: 0.20995056629180908\n",
      "Epoch 3748, Loss: 0.7564361542463303, Final Batch Loss: 0.24471253156661987\n",
      "Epoch 3749, Loss: 0.6775752156972885, Final Batch Loss: 0.17846418917179108\n",
      "Epoch 3750, Loss: 0.7832738608121872, Final Batch Loss: 0.2736589014530182\n",
      "Epoch 3751, Loss: 0.7243281155824661, Final Batch Loss: 0.1661505550146103\n",
      "Epoch 3752, Loss: 0.8116670250892639, Final Batch Loss: 0.2963100075721741\n",
      "Epoch 3753, Loss: 0.8464271724224091, Final Batch Loss: 0.28466787934303284\n",
      "Epoch 3754, Loss: 0.7165118157863617, Final Batch Loss: 0.25344711542129517\n",
      "Epoch 3755, Loss: 0.6593341678380966, Final Batch Loss: 0.1948099434375763\n",
      "Epoch 3756, Loss: 0.7311755418777466, Final Batch Loss: 0.20081478357315063\n",
      "Epoch 3757, Loss: 0.8149226605892181, Final Batch Loss: 0.24105757474899292\n",
      "Epoch 3758, Loss: 0.8670088797807693, Final Batch Loss: 0.37289661169052124\n",
      "Epoch 3759, Loss: 0.7216836661100388, Final Batch Loss: 0.2528747022151947\n",
      "Epoch 3760, Loss: 0.810520276427269, Final Batch Loss: 0.2739185094833374\n",
      "Epoch 3761, Loss: 0.8031903505325317, Final Batch Loss: 0.23072975873947144\n",
      "Epoch 3762, Loss: 0.8465971946716309, Final Batch Loss: 0.3034237027168274\n",
      "Epoch 3763, Loss: 0.8282607197761536, Final Batch Loss: 0.3136914074420929\n",
      "Epoch 3764, Loss: 0.781597450375557, Final Batch Loss: 0.2226095348596573\n",
      "Epoch 3765, Loss: 0.8708547651767731, Final Batch Loss: 0.2984400987625122\n",
      "Epoch 3766, Loss: 0.8505150973796844, Final Batch Loss: 0.3216812014579773\n",
      "Epoch 3767, Loss: 0.8344895243644714, Final Batch Loss: 0.24007782340049744\n",
      "Epoch 3768, Loss: 0.7770850211381912, Final Batch Loss: 0.2892145812511444\n",
      "Epoch 3769, Loss: 0.9675802886486053, Final Batch Loss: 0.36818721890449524\n",
      "Epoch 3770, Loss: 0.897552028298378, Final Batch Loss: 0.2458898276090622\n",
      "Epoch 3771, Loss: 0.8654596507549286, Final Batch Loss: 0.3007238507270813\n",
      "Epoch 3772, Loss: 0.8421173393726349, Final Batch Loss: 0.2553722560405731\n",
      "Epoch 3773, Loss: 0.8158361166715622, Final Batch Loss: 0.3365747034549713\n",
      "Epoch 3774, Loss: 0.755342110991478, Final Batch Loss: 0.23218868672847748\n",
      "Epoch 3775, Loss: 0.7823207676410675, Final Batch Loss: 0.22440645098686218\n",
      "Epoch 3776, Loss: 0.7825729697942734, Final Batch Loss: 0.24879486858844757\n",
      "Epoch 3777, Loss: 0.8652295172214508, Final Batch Loss: 0.3149113357067108\n",
      "Epoch 3778, Loss: 0.8279996514320374, Final Batch Loss: 0.27237868309020996\n",
      "Epoch 3779, Loss: 0.7629647105932236, Final Batch Loss: 0.19546152651309967\n",
      "Epoch 3780, Loss: 0.8119252324104309, Final Batch Loss: 0.2922605872154236\n",
      "Epoch 3781, Loss: 0.7344584465026855, Final Batch Loss: 0.21454955637454987\n",
      "Epoch 3782, Loss: 0.8510404229164124, Final Batch Loss: 0.35597559809684753\n",
      "Epoch 3783, Loss: 0.7408615052700043, Final Batch Loss: 0.3066854178905487\n",
      "Epoch 3784, Loss: 0.8116039633750916, Final Batch Loss: 0.2481142282485962\n",
      "Epoch 3785, Loss: 0.7667794972658157, Final Batch Loss: 0.26107868552207947\n",
      "Epoch 3786, Loss: 0.770456850528717, Final Batch Loss: 0.2314988374710083\n",
      "Epoch 3787, Loss: 0.7482065409421921, Final Batch Loss: 0.2527257204055786\n",
      "Epoch 3788, Loss: 0.8174818903207779, Final Batch Loss: 0.23830671608448029\n",
      "Epoch 3789, Loss: 0.8116867244243622, Final Batch Loss: 0.25171759724617004\n",
      "Epoch 3790, Loss: 0.7133006155490875, Final Batch Loss: 0.16212236881256104\n",
      "Epoch 3791, Loss: 0.7317700684070587, Final Batch Loss: 0.2032039314508438\n",
      "Epoch 3792, Loss: 0.7917963713407516, Final Batch Loss: 0.30973413586616516\n",
      "Epoch 3793, Loss: 0.8743166625499725, Final Batch Loss: 0.3045065104961395\n",
      "Epoch 3794, Loss: 0.7867627441883087, Final Batch Loss: 0.2655479609966278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3795, Loss: 0.7572328597307205, Final Batch Loss: 0.2089618444442749\n",
      "Epoch 3796, Loss: 0.8310891091823578, Final Batch Loss: 0.2839032709598541\n",
      "Epoch 3797, Loss: 0.746191143989563, Final Batch Loss: 0.24339324235916138\n",
      "Epoch 3798, Loss: 0.852694183588028, Final Batch Loss: 0.3062417805194855\n",
      "Epoch 3799, Loss: 0.8037118911743164, Final Batch Loss: 0.27422064542770386\n",
      "Epoch 3800, Loss: 0.7473374456167221, Final Batch Loss: 0.2689862847328186\n",
      "Epoch 3801, Loss: 0.8591102063655853, Final Batch Loss: 0.2901572585105896\n",
      "Epoch 3802, Loss: 0.7856085747480392, Final Batch Loss: 0.23515520989894867\n",
      "Epoch 3803, Loss: 0.7256570607423782, Final Batch Loss: 0.20592059195041656\n",
      "Epoch 3804, Loss: 0.7683268785476685, Final Batch Loss: 0.29503291845321655\n",
      "Epoch 3805, Loss: 0.7550264596939087, Final Batch Loss: 0.18048211932182312\n",
      "Epoch 3806, Loss: 0.8940450251102448, Final Batch Loss: 0.34824368357658386\n",
      "Epoch 3807, Loss: 0.8548243343830109, Final Batch Loss: 0.3214997947216034\n",
      "Epoch 3808, Loss: 0.8345692753791809, Final Batch Loss: 0.22656691074371338\n",
      "Epoch 3809, Loss: 0.8222255855798721, Final Batch Loss: 0.24844883382320404\n",
      "Epoch 3810, Loss: 0.8081671297550201, Final Batch Loss: 0.2620989680290222\n",
      "Epoch 3811, Loss: 0.7256108820438385, Final Batch Loss: 0.2280503213405609\n",
      "Epoch 3812, Loss: 0.7918754816055298, Final Batch Loss: 0.2918046712875366\n",
      "Epoch 3813, Loss: 0.8370928913354874, Final Batch Loss: 0.24866731464862823\n",
      "Epoch 3814, Loss: 0.8011542856693268, Final Batch Loss: 0.25884148478507996\n",
      "Epoch 3815, Loss: 0.7363753914833069, Final Batch Loss: 0.24573983252048492\n",
      "Epoch 3816, Loss: 0.9013874232769012, Final Batch Loss: 0.4057163596153259\n",
      "Epoch 3817, Loss: 0.7723838835954666, Final Batch Loss: 0.19949249923229218\n",
      "Epoch 3818, Loss: 0.817779466509819, Final Batch Loss: 0.30706462264060974\n",
      "Epoch 3819, Loss: 0.7681889235973358, Final Batch Loss: 0.21447452902793884\n",
      "Epoch 3820, Loss: 0.8185057044029236, Final Batch Loss: 0.28024905920028687\n",
      "Epoch 3821, Loss: 0.7657884806394577, Final Batch Loss: 0.26919299364089966\n",
      "Epoch 3822, Loss: 0.7358888685703278, Final Batch Loss: 0.292179673910141\n",
      "Epoch 3823, Loss: 0.7385851591825485, Final Batch Loss: 0.1718478947877884\n",
      "Epoch 3824, Loss: 0.7741127610206604, Final Batch Loss: 0.17994949221611023\n",
      "Epoch 3825, Loss: 0.8989047110080719, Final Batch Loss: 0.2993127107620239\n",
      "Epoch 3826, Loss: 0.8273626416921616, Final Batch Loss: 0.3252086341381073\n",
      "Epoch 3827, Loss: 0.8570516854524612, Final Batch Loss: 0.30756866931915283\n",
      "Epoch 3828, Loss: 0.7735174596309662, Final Batch Loss: 0.2503894865512848\n",
      "Epoch 3829, Loss: 0.7887039929628372, Final Batch Loss: 0.31430917978286743\n",
      "Epoch 3830, Loss: 0.9345119297504425, Final Batch Loss: 0.3014214336872101\n",
      "Epoch 3831, Loss: 0.7449976056814194, Final Batch Loss: 0.23685817420482635\n",
      "Epoch 3832, Loss: 0.802088588476181, Final Batch Loss: 0.28710538148880005\n",
      "Epoch 3833, Loss: 0.8209212273359299, Final Batch Loss: 0.30046722292900085\n",
      "Epoch 3834, Loss: 0.8481086939573288, Final Batch Loss: 0.3256000876426697\n",
      "Epoch 3835, Loss: 0.8595607876777649, Final Batch Loss: 0.3111155033111572\n",
      "Epoch 3836, Loss: 0.7917437702417374, Final Batch Loss: 0.3425043821334839\n",
      "Epoch 3837, Loss: 0.8282698392868042, Final Batch Loss: 0.256155788898468\n",
      "Epoch 3838, Loss: 0.7655814439058304, Final Batch Loss: 0.2521035373210907\n",
      "Epoch 3839, Loss: 0.7615992277860641, Final Batch Loss: 0.3137901723384857\n",
      "Epoch 3840, Loss: 0.7480560839176178, Final Batch Loss: 0.2760063409805298\n",
      "Epoch 3841, Loss: 0.8008306920528412, Final Batch Loss: 0.2019246220588684\n",
      "Epoch 3842, Loss: 0.8528739511966705, Final Batch Loss: 0.2927532196044922\n",
      "Epoch 3843, Loss: 0.8781937956809998, Final Batch Loss: 0.28734979033470154\n",
      "Epoch 3844, Loss: 0.7920444905757904, Final Batch Loss: 0.2258848249912262\n",
      "Epoch 3845, Loss: 0.8725602924823761, Final Batch Loss: 0.337654173374176\n",
      "Epoch 3846, Loss: 0.8096638768911362, Final Batch Loss: 0.22280867397785187\n",
      "Epoch 3847, Loss: 0.8431988656520844, Final Batch Loss: 0.20717760920524597\n",
      "Epoch 3848, Loss: 0.8180225640535355, Final Batch Loss: 0.21266533434391022\n",
      "Epoch 3849, Loss: 0.8597062826156616, Final Batch Loss: 0.3418460488319397\n",
      "Epoch 3850, Loss: 0.8175494372844696, Final Batch Loss: 0.25496435165405273\n",
      "Epoch 3851, Loss: 0.8330414742231369, Final Batch Loss: 0.24040259420871735\n",
      "Epoch 3852, Loss: 0.868552565574646, Final Batch Loss: 0.29122886061668396\n",
      "Epoch 3853, Loss: 0.8687095940113068, Final Batch Loss: 0.33535829186439514\n",
      "Epoch 3854, Loss: 0.8725045323371887, Final Batch Loss: 0.3249707520008087\n",
      "Epoch 3855, Loss: 0.7130932360887527, Final Batch Loss: 0.2810864746570587\n",
      "Epoch 3856, Loss: 0.7680531740188599, Final Batch Loss: 0.25377580523490906\n",
      "Epoch 3857, Loss: 0.8280302435159683, Final Batch Loss: 0.2345474511384964\n",
      "Epoch 3858, Loss: 0.7810617983341217, Final Batch Loss: 0.26682248711586\n",
      "Epoch 3859, Loss: 0.7531183660030365, Final Batch Loss: 0.2764400541782379\n",
      "Epoch 3860, Loss: 0.7708708792924881, Final Batch Loss: 0.3197247087955475\n",
      "Epoch 3861, Loss: 0.7966935038566589, Final Batch Loss: 0.30359965562820435\n",
      "Epoch 3862, Loss: 0.6674996912479401, Final Batch Loss: 0.16912958025932312\n",
      "Epoch 3863, Loss: 0.7885014712810516, Final Batch Loss: 0.3302832841873169\n",
      "Epoch 3864, Loss: 0.7599635273218155, Final Batch Loss: 0.2615923285484314\n",
      "Epoch 3865, Loss: 0.9539277851581573, Final Batch Loss: 0.25623664259910583\n",
      "Epoch 3866, Loss: 0.9024709165096283, Final Batch Loss: 0.2932452857494354\n",
      "Epoch 3867, Loss: 0.7836123108863831, Final Batch Loss: 0.31121981143951416\n",
      "Epoch 3868, Loss: 0.8007736951112747, Final Batch Loss: 0.19466887414455414\n",
      "Epoch 3869, Loss: 0.8580314517021179, Final Batch Loss: 0.3303811848163605\n",
      "Epoch 3870, Loss: 0.8569742143154144, Final Batch Loss: 0.33697617053985596\n",
      "Epoch 3871, Loss: 0.7924316078424454, Final Batch Loss: 0.28766345977783203\n",
      "Epoch 3872, Loss: 0.8487522304058075, Final Batch Loss: 0.2938337028026581\n",
      "Epoch 3873, Loss: 0.8141370266675949, Final Batch Loss: 0.3053513765335083\n",
      "Epoch 3874, Loss: 0.7234506011009216, Final Batch Loss: 0.18857532739639282\n",
      "Epoch 3875, Loss: 0.8746732473373413, Final Batch Loss: 0.32317960262298584\n",
      "Epoch 3876, Loss: 0.8131379336118698, Final Batch Loss: 0.21970756351947784\n",
      "Epoch 3877, Loss: 0.8229702115058899, Final Batch Loss: 0.28553828597068787\n",
      "Epoch 3878, Loss: 0.7747069746255875, Final Batch Loss: 0.2470158189535141\n",
      "Epoch 3879, Loss: 0.8837489187717438, Final Batch Loss: 0.35631677508354187\n",
      "Epoch 3880, Loss: 0.7049035727977753, Final Batch Loss: 0.2144363969564438\n",
      "Epoch 3881, Loss: 0.7857393026351929, Final Batch Loss: 0.2467610239982605\n",
      "Epoch 3882, Loss: 0.8831032663583755, Final Batch Loss: 0.257674902677536\n",
      "Epoch 3883, Loss: 0.8864998370409012, Final Batch Loss: 0.31783542037010193\n",
      "Epoch 3884, Loss: 0.8410580307245255, Final Batch Loss: 0.24183757603168488\n",
      "Epoch 3885, Loss: 0.8984680622816086, Final Batch Loss: 0.363877534866333\n",
      "Epoch 3886, Loss: 0.8073436766862869, Final Batch Loss: 0.30385226011276245\n",
      "Epoch 3887, Loss: 0.8236484229564667, Final Batch Loss: 0.2766093611717224\n",
      "Epoch 3888, Loss: 0.7779442667961121, Final Batch Loss: 0.20869800448417664\n",
      "Epoch 3889, Loss: 0.7666051238775253, Final Batch Loss: 0.31050509214401245\n",
      "Epoch 3890, Loss: 0.6785060167312622, Final Batch Loss: 0.2025137096643448\n",
      "Epoch 3891, Loss: 0.8205500841140747, Final Batch Loss: 0.2793237268924713\n",
      "Epoch 3892, Loss: 0.6796924918889999, Final Batch Loss: 0.198688343167305\n",
      "Epoch 3893, Loss: 0.7798375487327576, Final Batch Loss: 0.25013381242752075\n",
      "Epoch 3894, Loss: 0.7750558108091354, Final Batch Loss: 0.2183343917131424\n",
      "Epoch 3895, Loss: 0.7989804148674011, Final Batch Loss: 0.29729896783828735\n",
      "Epoch 3896, Loss: 0.8149830847978592, Final Batch Loss: 0.3148229420185089\n",
      "Epoch 3897, Loss: 0.771176815032959, Final Batch Loss: 0.2584504187107086\n",
      "Epoch 3898, Loss: 0.799241840839386, Final Batch Loss: 0.2830408811569214\n",
      "Epoch 3899, Loss: 0.714219942688942, Final Batch Loss: 0.2097531408071518\n",
      "Epoch 3900, Loss: 0.7770090401172638, Final Batch Loss: 0.26167407631874084\n",
      "Epoch 3901, Loss: 0.7815702557563782, Final Batch Loss: 0.24559332430362701\n",
      "Epoch 3902, Loss: 0.7081667482852936, Final Batch Loss: 0.19587093591690063\n",
      "Epoch 3903, Loss: 0.7447070628404617, Final Batch Loss: 0.24150511622428894\n",
      "Epoch 3904, Loss: 0.8364365547895432, Final Batch Loss: 0.2537313997745514\n",
      "Epoch 3905, Loss: 0.7847839593887329, Final Batch Loss: 0.2869550287723541\n",
      "Epoch 3906, Loss: 0.8869628012180328, Final Batch Loss: 0.3724563419818878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3907, Loss: 0.6933958679437637, Final Batch Loss: 0.1951955109834671\n",
      "Epoch 3908, Loss: 0.8852911591529846, Final Batch Loss: 0.2240208387374878\n",
      "Epoch 3909, Loss: 0.8660494089126587, Final Batch Loss: 0.35037317872047424\n",
      "Epoch 3910, Loss: 0.8504611253738403, Final Batch Loss: 0.34692710638046265\n",
      "Epoch 3911, Loss: 0.8001286089420319, Final Batch Loss: 0.31179139018058777\n",
      "Epoch 3912, Loss: 0.8248106092214584, Final Batch Loss: 0.31068336963653564\n",
      "Epoch 3913, Loss: 0.8116351664066315, Final Batch Loss: 0.31609565019607544\n",
      "Epoch 3914, Loss: 0.7642088979482651, Final Batch Loss: 0.2394222766160965\n",
      "Epoch 3915, Loss: 0.8559158146381378, Final Batch Loss: 0.327869176864624\n",
      "Epoch 3916, Loss: 0.865003764629364, Final Batch Loss: 0.27551430463790894\n",
      "Epoch 3917, Loss: 0.7817957550287247, Final Batch Loss: 0.29750531911849976\n",
      "Epoch 3918, Loss: 0.8577985763549805, Final Batch Loss: 0.30658432841300964\n",
      "Epoch 3919, Loss: 0.7328027188777924, Final Batch Loss: 0.20791849493980408\n",
      "Epoch 3920, Loss: 0.8822395503520966, Final Batch Loss: 0.3597785532474518\n",
      "Epoch 3921, Loss: 0.8556249141693115, Final Batch Loss: 0.26917779445648193\n",
      "Epoch 3922, Loss: 0.7403881847858429, Final Batch Loss: 0.224246084690094\n",
      "Epoch 3923, Loss: 0.830743819475174, Final Batch Loss: 0.30055534839630127\n",
      "Epoch 3924, Loss: 0.8346088081598282, Final Batch Loss: 0.2966324985027313\n",
      "Epoch 3925, Loss: 0.7357888966798782, Final Batch Loss: 0.1940837949514389\n",
      "Epoch 3926, Loss: 0.7829337269067764, Final Batch Loss: 0.2640993297100067\n",
      "Epoch 3927, Loss: 0.7467282265424728, Final Batch Loss: 0.23443947732448578\n",
      "Epoch 3928, Loss: 0.7662907540798187, Final Batch Loss: 0.21126997470855713\n",
      "Epoch 3929, Loss: 0.8802010416984558, Final Batch Loss: 0.2875765264034271\n",
      "Epoch 3930, Loss: 0.8241489827632904, Final Batch Loss: 0.28663170337677\n",
      "Epoch 3931, Loss: 0.7067669332027435, Final Batch Loss: 0.19958768784999847\n",
      "Epoch 3932, Loss: 0.8580363988876343, Final Batch Loss: 0.2577172815799713\n",
      "Epoch 3933, Loss: 0.7319888919591904, Final Batch Loss: 0.20711280405521393\n",
      "Epoch 3934, Loss: 0.796103224158287, Final Batch Loss: 0.23540130257606506\n",
      "Epoch 3935, Loss: 0.8940404653549194, Final Batch Loss: 0.31759175658226013\n",
      "Epoch 3936, Loss: 0.8106296062469482, Final Batch Loss: 0.2704789340496063\n",
      "Epoch 3937, Loss: 0.7800917476415634, Final Batch Loss: 0.23489417135715485\n",
      "Epoch 3938, Loss: 0.7532530277967453, Final Batch Loss: 0.20894764363765717\n",
      "Epoch 3939, Loss: 0.7744803875684738, Final Batch Loss: 0.217908576130867\n",
      "Epoch 3940, Loss: 0.8953803181648254, Final Batch Loss: 0.3600117862224579\n",
      "Epoch 3941, Loss: 0.6862484365701675, Final Batch Loss: 0.23531849682331085\n",
      "Epoch 3942, Loss: 0.9193904399871826, Final Batch Loss: 0.31336522102355957\n",
      "Epoch 3943, Loss: 0.7563190013170242, Final Batch Loss: 0.21995031833648682\n",
      "Epoch 3944, Loss: 0.8030568659305573, Final Batch Loss: 0.2589653432369232\n",
      "Epoch 3945, Loss: 0.8760361671447754, Final Batch Loss: 0.3332977592945099\n",
      "Epoch 3946, Loss: 0.774201050400734, Final Batch Loss: 0.2072470784187317\n",
      "Epoch 3947, Loss: 0.8944860100746155, Final Batch Loss: 0.33005255460739136\n",
      "Epoch 3948, Loss: 0.8357380330562592, Final Batch Loss: 0.27075788378715515\n",
      "Epoch 3949, Loss: 0.7054305672645569, Final Batch Loss: 0.22321470081806183\n",
      "Epoch 3950, Loss: 0.8268263638019562, Final Batch Loss: 0.34173643589019775\n",
      "Epoch 3951, Loss: 0.8068654835224152, Final Batch Loss: 0.24371474981307983\n",
      "Epoch 3952, Loss: 0.9248129427433014, Final Batch Loss: 0.2516481876373291\n",
      "Epoch 3953, Loss: 0.9611871540546417, Final Batch Loss: 0.31836360692977905\n",
      "Epoch 3954, Loss: 0.75482477247715, Final Batch Loss: 0.23450212180614471\n",
      "Epoch 3955, Loss: 0.7840306758880615, Final Batch Loss: 0.263276070356369\n",
      "Epoch 3956, Loss: 0.8087235987186432, Final Batch Loss: 0.3012630343437195\n",
      "Epoch 3957, Loss: 0.9056134819984436, Final Batch Loss: 0.3405081331729889\n",
      "Epoch 3958, Loss: 0.7619706988334656, Final Batch Loss: 0.225502610206604\n",
      "Epoch 3959, Loss: 0.7563956677913666, Final Batch Loss: 0.24197044968605042\n",
      "Epoch 3960, Loss: 0.7002168893814087, Final Batch Loss: 0.19845996797084808\n",
      "Epoch 3961, Loss: 0.8900626003742218, Final Batch Loss: 0.29975447058677673\n",
      "Epoch 3962, Loss: 0.7916371077299118, Final Batch Loss: 0.25339820981025696\n",
      "Epoch 3963, Loss: 0.810058519244194, Final Batch Loss: 0.26247328519821167\n",
      "Epoch 3964, Loss: 0.7937767952680588, Final Batch Loss: 0.3005373179912567\n",
      "Epoch 3965, Loss: 0.7467389702796936, Final Batch Loss: 0.1814115345478058\n",
      "Epoch 3966, Loss: 0.7288422584533691, Final Batch Loss: 0.22233092784881592\n",
      "Epoch 3967, Loss: 0.8194045722484589, Final Batch Loss: 0.27890968322753906\n",
      "Epoch 3968, Loss: 0.7783892899751663, Final Batch Loss: 0.3111136853694916\n",
      "Epoch 3969, Loss: 0.6697990298271179, Final Batch Loss: 0.18911953270435333\n",
      "Epoch 3970, Loss: 0.7511006891727448, Final Batch Loss: 0.22196608781814575\n",
      "Epoch 3971, Loss: 0.6905950158834457, Final Batch Loss: 0.20658579468727112\n",
      "Epoch 3972, Loss: 0.7692300081253052, Final Batch Loss: 0.2552134096622467\n",
      "Epoch 3973, Loss: 0.7785899937152863, Final Batch Loss: 0.2957291901111603\n",
      "Epoch 3974, Loss: 0.9112686216831207, Final Batch Loss: 0.3264749348163605\n",
      "Epoch 3975, Loss: 0.7481299489736557, Final Batch Loss: 0.2300088107585907\n",
      "Epoch 3976, Loss: 0.7052667587995529, Final Batch Loss: 0.19203777611255646\n",
      "Epoch 3977, Loss: 0.7967337816953659, Final Batch Loss: 0.3052583336830139\n",
      "Epoch 3978, Loss: 0.7273731529712677, Final Batch Loss: 0.17237377166748047\n",
      "Epoch 3979, Loss: 0.7420993000268936, Final Batch Loss: 0.18498051166534424\n",
      "Epoch 3980, Loss: 0.8428689986467361, Final Batch Loss: 0.3303152322769165\n",
      "Epoch 3981, Loss: 0.7814988195896149, Final Batch Loss: 0.3116021752357483\n",
      "Epoch 3982, Loss: 0.7859367281198502, Final Batch Loss: 0.29786354303359985\n",
      "Epoch 3983, Loss: 0.8528060913085938, Final Batch Loss: 0.2951762080192566\n",
      "Epoch 3984, Loss: 0.8616079688072205, Final Batch Loss: 0.2796037197113037\n",
      "Epoch 3985, Loss: 0.794579416513443, Final Batch Loss: 0.27633535861968994\n",
      "Epoch 3986, Loss: 0.7421246618032455, Final Batch Loss: 0.3108287453651428\n",
      "Epoch 3987, Loss: 0.8230065405368805, Final Batch Loss: 0.262090265750885\n",
      "Epoch 3988, Loss: 0.8622272610664368, Final Batch Loss: 0.27323463559150696\n",
      "Epoch 3989, Loss: 0.8416350185871124, Final Batch Loss: 0.2706277072429657\n",
      "Epoch 3990, Loss: 0.9813378751277924, Final Batch Loss: 0.34793156385421753\n",
      "Epoch 3991, Loss: 1.0440916419029236, Final Batch Loss: 0.46880391240119934\n",
      "Epoch 3992, Loss: 0.6892742961645126, Final Batch Loss: 0.18210870027542114\n",
      "Epoch 3993, Loss: 0.7863150238990784, Final Batch Loss: 0.25184017419815063\n",
      "Epoch 3994, Loss: 0.7829268127679825, Final Batch Loss: 0.25557711720466614\n",
      "Epoch 3995, Loss: 0.7543582618236542, Final Batch Loss: 0.2457209676504135\n",
      "Epoch 3996, Loss: 0.7102020084857941, Final Batch Loss: 0.2423619031906128\n",
      "Epoch 3997, Loss: 0.7452977895736694, Final Batch Loss: 0.28130337595939636\n",
      "Epoch 3998, Loss: 0.6635588109493256, Final Batch Loss: 0.16393257677555084\n",
      "Epoch 3999, Loss: 0.7865310311317444, Final Batch Loss: 0.2767152190208435\n",
      "Epoch 4000, Loss: 0.7425857931375504, Final Batch Loss: 0.27751246094703674\n",
      "Epoch 4001, Loss: 0.6889534443616867, Final Batch Loss: 0.21466761827468872\n",
      "Epoch 4002, Loss: 0.7610670924186707, Final Batch Loss: 0.22417668998241425\n",
      "Epoch 4003, Loss: 0.762570396065712, Final Batch Loss: 0.24486799538135529\n",
      "Epoch 4004, Loss: 0.7821941077709198, Final Batch Loss: 0.30653440952301025\n",
      "Epoch 4005, Loss: 0.7850546836853027, Final Batch Loss: 0.2612019181251526\n",
      "Epoch 4006, Loss: 0.7781572788953781, Final Batch Loss: 0.22670596837997437\n",
      "Epoch 4007, Loss: 0.8469713032245636, Final Batch Loss: 0.29500430822372437\n",
      "Epoch 4008, Loss: 0.7318272739648819, Final Batch Loss: 0.21526160836219788\n",
      "Epoch 4009, Loss: 0.8768905699253082, Final Batch Loss: 0.3593650162220001\n",
      "Epoch 4010, Loss: 0.8648590594530106, Final Batch Loss: 0.2493494600057602\n",
      "Epoch 4011, Loss: 0.788741260766983, Final Batch Loss: 0.2738211452960968\n",
      "Epoch 4012, Loss: 0.7393375337123871, Final Batch Loss: 0.22246414422988892\n",
      "Epoch 4013, Loss: 0.8012667149305344, Final Batch Loss: 0.27620455622673035\n",
      "Epoch 4014, Loss: 0.8206343054771423, Final Batch Loss: 0.3001115620136261\n",
      "Epoch 4015, Loss: 0.8311961740255356, Final Batch Loss: 0.24492384493350983\n",
      "Epoch 4016, Loss: 0.7265001535415649, Final Batch Loss: 0.21319743990898132\n",
      "Epoch 4017, Loss: 0.7858773767948151, Final Batch Loss: 0.2728666067123413\n",
      "Epoch 4018, Loss: 0.7949111312627792, Final Batch Loss: 0.2885904908180237\n",
      "Epoch 4019, Loss: 0.8118521571159363, Final Batch Loss: 0.29315340518951416\n",
      "Epoch 4020, Loss: 0.7514795362949371, Final Batch Loss: 0.2486267238855362\n",
      "Epoch 4021, Loss: 0.7472013980150223, Final Batch Loss: 0.27024123072624207\n",
      "Epoch 4022, Loss: 0.7834672629833221, Final Batch Loss: 0.2201743721961975\n",
      "Epoch 4023, Loss: 0.7757187783718109, Final Batch Loss: 0.22984981536865234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4024, Loss: 0.8348540216684341, Final Batch Loss: 0.23990117013454437\n",
      "Epoch 4025, Loss: 0.7212922871112823, Final Batch Loss: 0.2461344599723816\n",
      "Epoch 4026, Loss: 0.713983491063118, Final Batch Loss: 0.21837332844734192\n",
      "Epoch 4027, Loss: 0.7856639474630356, Final Batch Loss: 0.27817052602767944\n",
      "Epoch 4028, Loss: 0.8556147217750549, Final Batch Loss: 0.332060843706131\n",
      "Epoch 4029, Loss: 0.9149899780750275, Final Batch Loss: 0.2377091348171234\n",
      "Epoch 4030, Loss: 0.828292578458786, Final Batch Loss: 0.30496516823768616\n",
      "Epoch 4031, Loss: 0.8072906881570816, Final Batch Loss: 0.23177416622638702\n",
      "Epoch 4032, Loss: 0.8075851052999496, Final Batch Loss: 0.24029329419136047\n",
      "Epoch 4033, Loss: 0.8155259937047958, Final Batch Loss: 0.3214142322540283\n",
      "Epoch 4034, Loss: 0.7877394706010818, Final Batch Loss: 0.2590465545654297\n",
      "Epoch 4035, Loss: 0.8447071462869644, Final Batch Loss: 0.22990353405475616\n",
      "Epoch 4036, Loss: 0.7807764112949371, Final Batch Loss: 0.23515765368938446\n",
      "Epoch 4037, Loss: 0.784123882651329, Final Batch Loss: 0.20888926088809967\n",
      "Epoch 4038, Loss: 0.8015444725751877, Final Batch Loss: 0.19161497056484222\n",
      "Epoch 4039, Loss: 0.7785846441984177, Final Batch Loss: 0.27163830399513245\n",
      "Epoch 4040, Loss: 0.8602284044027328, Final Batch Loss: 0.2345229834318161\n",
      "Epoch 4041, Loss: 0.8313701152801514, Final Batch Loss: 0.28782549500465393\n",
      "Epoch 4042, Loss: 0.7429222613573074, Final Batch Loss: 0.2115866094827652\n",
      "Epoch 4043, Loss: 0.750764012336731, Final Batch Loss: 0.24645474553108215\n",
      "Epoch 4044, Loss: 0.7178260535001755, Final Batch Loss: 0.19622795283794403\n",
      "Epoch 4045, Loss: 0.6799332946538925, Final Batch Loss: 0.16926482319831848\n",
      "Epoch 4046, Loss: 0.7567932307720184, Final Batch Loss: 0.27580058574676514\n",
      "Epoch 4047, Loss: 0.7489185631275177, Final Batch Loss: 0.326168030500412\n",
      "Epoch 4048, Loss: 0.7002619057893753, Final Batch Loss: 0.17508363723754883\n",
      "Epoch 4049, Loss: 0.8256140053272247, Final Batch Loss: 0.2566570043563843\n",
      "Epoch 4050, Loss: 0.8206084370613098, Final Batch Loss: 0.284878134727478\n",
      "Epoch 4051, Loss: 0.7197981029748917, Final Batch Loss: 0.2294309288263321\n",
      "Epoch 4052, Loss: 0.7297252863645554, Final Batch Loss: 0.2256486713886261\n",
      "Epoch 4053, Loss: 0.7797892391681671, Final Batch Loss: 0.29250290989875793\n",
      "Epoch 4054, Loss: 0.6658179461956024, Final Batch Loss: 0.20389379560947418\n",
      "Epoch 4055, Loss: 0.7036149203777313, Final Batch Loss: 0.21910041570663452\n",
      "Epoch 4056, Loss: 0.7117960751056671, Final Batch Loss: 0.16996070742607117\n",
      "Epoch 4057, Loss: 0.7158375084400177, Final Batch Loss: 0.23907963931560516\n",
      "Epoch 4058, Loss: 0.773991584777832, Final Batch Loss: 0.32471764087677\n",
      "Epoch 4059, Loss: 0.7960163652896881, Final Batch Loss: 0.25786569714546204\n",
      "Epoch 4060, Loss: 0.7028540670871735, Final Batch Loss: 0.18625298142433167\n",
      "Epoch 4061, Loss: 0.777776449918747, Final Batch Loss: 0.22617283463478088\n",
      "Epoch 4062, Loss: 0.8655701130628586, Final Batch Loss: 0.3088775873184204\n",
      "Epoch 4063, Loss: 0.6803915500640869, Final Batch Loss: 0.15589244663715363\n",
      "Epoch 4064, Loss: 0.8101666122674942, Final Batch Loss: 0.20162872970104218\n",
      "Epoch 4065, Loss: 0.7950157225131989, Final Batch Loss: 0.2672973573207855\n",
      "Epoch 4066, Loss: 0.7437202781438828, Final Batch Loss: 0.21288548409938812\n",
      "Epoch 4067, Loss: 0.8193995654582977, Final Batch Loss: 0.3607247769832611\n",
      "Epoch 4068, Loss: 0.6934951096773148, Final Batch Loss: 0.23064735531806946\n",
      "Epoch 4069, Loss: 0.7581468820571899, Final Batch Loss: 0.2577671408653259\n",
      "Epoch 4070, Loss: 0.8034278750419617, Final Batch Loss: 0.2286878526210785\n",
      "Epoch 4071, Loss: 0.7788222879171371, Final Batch Loss: 0.2605108618736267\n",
      "Epoch 4072, Loss: 0.8234027326107025, Final Batch Loss: 0.3759516179561615\n",
      "Epoch 4073, Loss: 0.8579549044370651, Final Batch Loss: 0.34435105323791504\n",
      "Epoch 4074, Loss: 0.7174694389104843, Final Batch Loss: 0.2427396923303604\n",
      "Epoch 4075, Loss: 0.8095756471157074, Final Batch Loss: 0.2468399703502655\n",
      "Epoch 4076, Loss: 0.7248565107584, Final Batch Loss: 0.2673178017139435\n",
      "Epoch 4077, Loss: 0.7318409532308578, Final Batch Loss: 0.27252036333084106\n",
      "Epoch 4078, Loss: 0.7481721937656403, Final Batch Loss: 0.2466779202222824\n",
      "Epoch 4079, Loss: 0.7707471549510956, Final Batch Loss: 0.26380249857902527\n",
      "Epoch 4080, Loss: 0.8778056353330612, Final Batch Loss: 0.3333978056907654\n",
      "Epoch 4081, Loss: 0.8307112157344818, Final Batch Loss: 0.29491904377937317\n",
      "Epoch 4082, Loss: 0.7315696477890015, Final Batch Loss: 0.3115934431552887\n",
      "Epoch 4083, Loss: 0.7095098346471786, Final Batch Loss: 0.24209868907928467\n",
      "Epoch 4084, Loss: 0.7785844802856445, Final Batch Loss: 0.22305616736412048\n",
      "Epoch 4085, Loss: 0.755485787987709, Final Batch Loss: 0.21977080404758453\n",
      "Epoch 4086, Loss: 0.6931025087833405, Final Batch Loss: 0.20153610408306122\n",
      "Epoch 4087, Loss: 0.8820569813251495, Final Batch Loss: 0.2660498023033142\n",
      "Epoch 4088, Loss: 0.7876593172550201, Final Batch Loss: 0.24878782033920288\n",
      "Epoch 4089, Loss: 0.8688910007476807, Final Batch Loss: 0.3718220293521881\n",
      "Epoch 4090, Loss: 0.7329588085412979, Final Batch Loss: 0.14298613369464874\n",
      "Epoch 4091, Loss: 0.8000320345163345, Final Batch Loss: 0.20453985035419464\n",
      "Epoch 4092, Loss: 0.7563294768333435, Final Batch Loss: 0.2181081771850586\n",
      "Epoch 4093, Loss: 0.7848576456308365, Final Batch Loss: 0.2617768347263336\n",
      "Epoch 4094, Loss: 0.7278532385826111, Final Batch Loss: 0.204080730676651\n",
      "Epoch 4095, Loss: 0.857160896062851, Final Batch Loss: 0.32874053716659546\n",
      "Epoch 4096, Loss: 0.818789929151535, Final Batch Loss: 0.27301010489463806\n",
      "Epoch 4097, Loss: 0.6880859881639481, Final Batch Loss: 0.19594916701316833\n",
      "Epoch 4098, Loss: 0.7804109901189804, Final Batch Loss: 0.3145918548107147\n",
      "Epoch 4099, Loss: 0.7725903689861298, Final Batch Loss: 0.26804348826408386\n",
      "Epoch 4100, Loss: 0.86219821870327, Final Batch Loss: 0.36969834566116333\n",
      "Epoch 4101, Loss: 0.8309974819421768, Final Batch Loss: 0.28640806674957275\n",
      "Epoch 4102, Loss: 0.8098305016756058, Final Batch Loss: 0.3206506073474884\n",
      "Epoch 4103, Loss: 0.8658589124679565, Final Batch Loss: 0.2597007155418396\n",
      "Epoch 4104, Loss: 0.824155792593956, Final Batch Loss: 0.22909341752529144\n",
      "Epoch 4105, Loss: 0.8105004131793976, Final Batch Loss: 0.29918769001960754\n",
      "Epoch 4106, Loss: 0.8371268659830093, Final Batch Loss: 0.3366643488407135\n",
      "Epoch 4107, Loss: 0.8561229258775711, Final Batch Loss: 0.3773949444293976\n",
      "Epoch 4108, Loss: 0.8130271732807159, Final Batch Loss: 0.2645057737827301\n",
      "Epoch 4109, Loss: 0.7884741425514221, Final Batch Loss: 0.2755609154701233\n",
      "Epoch 4110, Loss: 0.7383912205696106, Final Batch Loss: 0.27547380328178406\n",
      "Epoch 4111, Loss: 0.6710160821676254, Final Batch Loss: 0.1870352029800415\n",
      "Epoch 4112, Loss: 0.7687675058841705, Final Batch Loss: 0.23525583744049072\n",
      "Epoch 4113, Loss: 0.9011256247758865, Final Batch Loss: 0.43983936309814453\n",
      "Epoch 4114, Loss: 0.7893336415290833, Final Batch Loss: 0.2773267328739166\n",
      "Epoch 4115, Loss: 0.7791418433189392, Final Batch Loss: 0.25135883688926697\n",
      "Epoch 4116, Loss: 0.7423939108848572, Final Batch Loss: 0.28989294171333313\n",
      "Epoch 4117, Loss: 0.8489292562007904, Final Batch Loss: 0.2306007742881775\n",
      "Epoch 4118, Loss: 0.9196993410587311, Final Batch Loss: 0.35915273427963257\n",
      "Epoch 4119, Loss: 0.7343967705965042, Final Batch Loss: 0.23138095438480377\n",
      "Epoch 4120, Loss: 0.8602514863014221, Final Batch Loss: 0.3043670356273651\n",
      "Epoch 4121, Loss: 0.7507428228855133, Final Batch Loss: 0.296368271112442\n",
      "Epoch 4122, Loss: 0.8540587723255157, Final Batch Loss: 0.22927585244178772\n",
      "Epoch 4123, Loss: 0.7467838525772095, Final Batch Loss: 0.23883503675460815\n",
      "Epoch 4124, Loss: 0.806044191122055, Final Batch Loss: 0.20197293162345886\n",
      "Epoch 4125, Loss: 0.805056020617485, Final Batch Loss: 0.3079487979412079\n",
      "Epoch 4126, Loss: 0.8668167591094971, Final Batch Loss: 0.2844621241092682\n",
      "Epoch 4127, Loss: 0.7661325633525848, Final Batch Loss: 0.2688312232494354\n",
      "Epoch 4128, Loss: 0.7458796948194504, Final Batch Loss: 0.19828493893146515\n",
      "Epoch 4129, Loss: 0.8128585964441299, Final Batch Loss: 0.3257901668548584\n",
      "Epoch 4130, Loss: 0.9025993049144745, Final Batch Loss: 0.3681163489818573\n",
      "Epoch 4131, Loss: 0.8088506609201431, Final Batch Loss: 0.3282279670238495\n",
      "Epoch 4132, Loss: 0.7367295622825623, Final Batch Loss: 0.2690373659133911\n",
      "Epoch 4133, Loss: 0.7508817911148071, Final Batch Loss: 0.2367321103811264\n",
      "Epoch 4134, Loss: 0.8026547431945801, Final Batch Loss: 0.23053626716136932\n",
      "Epoch 4135, Loss: 0.7833448052406311, Final Batch Loss: 0.19606521725654602\n",
      "Epoch 4136, Loss: 0.8384557962417603, Final Batch Loss: 0.28861695528030396\n",
      "Epoch 4137, Loss: 0.7640482783317566, Final Batch Loss: 0.30610600113868713\n",
      "Epoch 4138, Loss: 0.78306445479393, Final Batch Loss: 0.21197426319122314\n",
      "Epoch 4139, Loss: 0.8256117850542068, Final Batch Loss: 0.35309919714927673\n",
      "Epoch 4140, Loss: 0.7396608889102936, Final Batch Loss: 0.21522247791290283\n",
      "Epoch 4141, Loss: 0.7631295472383499, Final Batch Loss: 0.21367348730564117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4142, Loss: 0.8390304148197174, Final Batch Loss: 0.30455508828163147\n",
      "Epoch 4143, Loss: 0.6936569511890411, Final Batch Loss: 0.15339668095111847\n",
      "Epoch 4144, Loss: 0.762272521853447, Final Batch Loss: 0.2521775960922241\n",
      "Epoch 4145, Loss: 0.8053397387266159, Final Batch Loss: 0.24594835937023163\n",
      "Epoch 4146, Loss: 0.8826922178268433, Final Batch Loss: 0.3401358723640442\n",
      "Epoch 4147, Loss: 0.876027524471283, Final Batch Loss: 0.33920854330062866\n",
      "Epoch 4148, Loss: 0.8413310796022415, Final Batch Loss: 0.3511648178100586\n",
      "Epoch 4149, Loss: 0.6758224964141846, Final Batch Loss: 0.18639346957206726\n",
      "Epoch 4150, Loss: 0.8493539392948151, Final Batch Loss: 0.32813161611557007\n",
      "Epoch 4151, Loss: 0.7142706513404846, Final Batch Loss: 0.22604452073574066\n",
      "Epoch 4152, Loss: 0.7626034766435623, Final Batch Loss: 0.2550336420536041\n",
      "Epoch 4153, Loss: 0.7037730813026428, Final Batch Loss: 0.23297187685966492\n",
      "Epoch 4154, Loss: 0.7892758846282959, Final Batch Loss: 0.23587557673454285\n",
      "Epoch 4155, Loss: 0.6824272125959396, Final Batch Loss: 0.2474701702594757\n",
      "Epoch 4156, Loss: 0.7680321931838989, Final Batch Loss: 0.1913355588912964\n",
      "Epoch 4157, Loss: 0.7786769270896912, Final Batch Loss: 0.33851566910743713\n",
      "Epoch 4158, Loss: 0.7403480410575867, Final Batch Loss: 0.2144598662853241\n",
      "Epoch 4159, Loss: 0.7225429117679596, Final Batch Loss: 0.21824458241462708\n",
      "Epoch 4160, Loss: 0.7543816864490509, Final Batch Loss: 0.2138088047504425\n",
      "Epoch 4161, Loss: 0.8203029185533524, Final Batch Loss: 0.2965843081474304\n",
      "Epoch 4162, Loss: 0.8408143073320389, Final Batch Loss: 0.31410035490989685\n",
      "Epoch 4163, Loss: 0.6550678014755249, Final Batch Loss: 0.20571622252464294\n",
      "Epoch 4164, Loss: 0.7994518131017685, Final Batch Loss: 0.22975872457027435\n",
      "Epoch 4165, Loss: 0.7493421137332916, Final Batch Loss: 0.29268625378608704\n",
      "Epoch 4166, Loss: 0.696177750825882, Final Batch Loss: 0.19633422791957855\n",
      "Epoch 4167, Loss: 0.6958263963460922, Final Batch Loss: 0.17921972274780273\n",
      "Epoch 4168, Loss: 0.7639721035957336, Final Batch Loss: 0.29472652077674866\n",
      "Epoch 4169, Loss: 0.8047768473625183, Final Batch Loss: 0.3089463412761688\n",
      "Epoch 4170, Loss: 0.749660313129425, Final Batch Loss: 0.24097925424575806\n",
      "Epoch 4171, Loss: 0.8371121436357498, Final Batch Loss: 0.30489128828048706\n",
      "Epoch 4172, Loss: 0.7567699253559113, Final Batch Loss: 0.26892802119255066\n",
      "Epoch 4173, Loss: 0.7430310696363449, Final Batch Loss: 0.263282835483551\n",
      "Epoch 4174, Loss: 0.6997630000114441, Final Batch Loss: 0.22466056048870087\n",
      "Epoch 4175, Loss: 0.9192193448543549, Final Batch Loss: 0.31281349062919617\n",
      "Epoch 4176, Loss: 0.7526047974824905, Final Batch Loss: 0.17333103716373444\n",
      "Epoch 4177, Loss: 0.8172850012779236, Final Batch Loss: 0.26088523864746094\n",
      "Epoch 4178, Loss: 0.7241994887590408, Final Batch Loss: 0.2636566758155823\n",
      "Epoch 4179, Loss: 0.7570556551218033, Final Batch Loss: 0.32010695338249207\n",
      "Epoch 4180, Loss: 0.6862176209688187, Final Batch Loss: 0.17931705713272095\n",
      "Epoch 4181, Loss: 0.7306034117937088, Final Batch Loss: 0.22622984647750854\n",
      "Epoch 4182, Loss: 0.7256496697664261, Final Batch Loss: 0.297254353761673\n",
      "Epoch 4183, Loss: 0.7531348466873169, Final Batch Loss: 0.3210068941116333\n",
      "Epoch 4184, Loss: 0.8414664715528488, Final Batch Loss: 0.2871209979057312\n",
      "Epoch 4185, Loss: 0.8789212256669998, Final Batch Loss: 0.34915944933891296\n",
      "Epoch 4186, Loss: 0.8434160947799683, Final Batch Loss: 0.27330607175827026\n",
      "Epoch 4187, Loss: 0.7341316938400269, Final Batch Loss: 0.22106632590293884\n",
      "Epoch 4188, Loss: 0.6386111676692963, Final Batch Loss: 0.19561998546123505\n",
      "Epoch 4189, Loss: 0.9943794310092926, Final Batch Loss: 0.33385607600212097\n",
      "Epoch 4190, Loss: 0.8428152501583099, Final Batch Loss: 0.3277870714664459\n",
      "Epoch 4191, Loss: 0.7216581106185913, Final Batch Loss: 0.22877570986747742\n",
      "Epoch 4192, Loss: 0.7876904904842377, Final Batch Loss: 0.3662164807319641\n",
      "Epoch 4193, Loss: 0.8001339733600616, Final Batch Loss: 0.2763945460319519\n",
      "Epoch 4194, Loss: 0.7502555400133133, Final Batch Loss: 0.18660230934619904\n",
      "Epoch 4195, Loss: 0.6662678420543671, Final Batch Loss: 0.1566990464925766\n",
      "Epoch 4196, Loss: 0.8741610050201416, Final Batch Loss: 0.2529905140399933\n",
      "Epoch 4197, Loss: 0.8216435611248016, Final Batch Loss: 0.30091559886932373\n",
      "Epoch 4198, Loss: 0.8432924747467041, Final Batch Loss: 0.29118624329566956\n",
      "Epoch 4199, Loss: 0.885623574256897, Final Batch Loss: 0.33293482661247253\n",
      "Epoch 4200, Loss: 0.8029381036758423, Final Batch Loss: 0.23574501276016235\n",
      "Epoch 4201, Loss: 0.805140882730484, Final Batch Loss: 0.33270418643951416\n",
      "Epoch 4202, Loss: 0.7545064240694046, Final Batch Loss: 0.286943256855011\n",
      "Epoch 4203, Loss: 0.7425989955663681, Final Batch Loss: 0.23828378319740295\n",
      "Epoch 4204, Loss: 0.8478267043828964, Final Batch Loss: 0.2837584912776947\n",
      "Epoch 4205, Loss: 0.8035343587398529, Final Batch Loss: 0.2677728235721588\n",
      "Epoch 4206, Loss: 0.7521824389696121, Final Batch Loss: 0.20500142872333527\n",
      "Epoch 4207, Loss: 0.8735490888357162, Final Batch Loss: 0.3804892301559448\n",
      "Epoch 4208, Loss: 0.8447644114494324, Final Batch Loss: 0.289986789226532\n",
      "Epoch 4209, Loss: 0.7168984562158585, Final Batch Loss: 0.2723490297794342\n",
      "Epoch 4210, Loss: 0.9354991763830185, Final Batch Loss: 0.40482044219970703\n",
      "Epoch 4211, Loss: 0.7482995092868805, Final Batch Loss: 0.19897478818893433\n",
      "Epoch 4212, Loss: 0.7096162736415863, Final Batch Loss: 0.20310038328170776\n",
      "Epoch 4213, Loss: 0.7376706004142761, Final Batch Loss: 0.2577088475227356\n",
      "Epoch 4214, Loss: 0.6935171037912369, Final Batch Loss: 0.18189333379268646\n",
      "Epoch 4215, Loss: 0.8241477161645889, Final Batch Loss: 0.3187160789966583\n",
      "Epoch 4216, Loss: 0.7063601911067963, Final Batch Loss: 0.20399540662765503\n",
      "Epoch 4217, Loss: 0.7132380157709122, Final Batch Loss: 0.1992848813533783\n",
      "Epoch 4218, Loss: 0.7204490303993225, Final Batch Loss: 0.18547821044921875\n",
      "Epoch 4219, Loss: 0.6682108342647552, Final Batch Loss: 0.1897771656513214\n",
      "Epoch 4220, Loss: 0.7260362356901169, Final Batch Loss: 0.20543162524700165\n",
      "Epoch 4221, Loss: 0.8061068952083588, Final Batch Loss: 0.25040921568870544\n",
      "Epoch 4222, Loss: 0.7070427238941193, Final Batch Loss: 0.23878109455108643\n",
      "Epoch 4223, Loss: 0.7919636815786362, Final Batch Loss: 0.25866925716400146\n",
      "Epoch 4224, Loss: 0.8495670557022095, Final Batch Loss: 0.3380717635154724\n",
      "Epoch 4225, Loss: 0.6580359786748886, Final Batch Loss: 0.2074565589427948\n",
      "Epoch 4226, Loss: 0.7335768342018127, Final Batch Loss: 0.26556289196014404\n",
      "Epoch 4227, Loss: 0.797017827630043, Final Batch Loss: 0.2987087368965149\n",
      "Epoch 4228, Loss: 0.7140344530344009, Final Batch Loss: 0.1883968710899353\n",
      "Epoch 4229, Loss: 0.790777251124382, Final Batch Loss: 0.2498522847890854\n",
      "Epoch 4230, Loss: 0.8777782171964645, Final Batch Loss: 0.3887145221233368\n",
      "Epoch 4231, Loss: 0.7370375990867615, Final Batch Loss: 0.3003232777118683\n",
      "Epoch 4232, Loss: 0.7687990367412567, Final Batch Loss: 0.21975019574165344\n",
      "Epoch 4233, Loss: 0.7426453083753586, Final Batch Loss: 0.2247370183467865\n",
      "Epoch 4234, Loss: 0.824650451540947, Final Batch Loss: 0.23884209990501404\n",
      "Epoch 4235, Loss: 0.7217858284711838, Final Batch Loss: 0.24777314066886902\n",
      "Epoch 4236, Loss: 0.8112279027700424, Final Batch Loss: 0.30181506276130676\n",
      "Epoch 4237, Loss: 0.9535638689994812, Final Batch Loss: 0.3483334183692932\n",
      "Epoch 4238, Loss: 0.8560318797826767, Final Batch Loss: 0.3396618366241455\n",
      "Epoch 4239, Loss: 0.6894791275262833, Final Batch Loss: 0.1986646205186844\n",
      "Epoch 4240, Loss: 0.829629972577095, Final Batch Loss: 0.32545748353004456\n",
      "Epoch 4241, Loss: 0.7927112281322479, Final Batch Loss: 0.312942773103714\n",
      "Epoch 4242, Loss: 0.7529929429292679, Final Batch Loss: 0.19361181557178497\n",
      "Epoch 4243, Loss: 0.8119323998689651, Final Batch Loss: 0.2235567718744278\n",
      "Epoch 4244, Loss: 0.7659413665533066, Final Batch Loss: 0.237590029835701\n",
      "Epoch 4245, Loss: 0.8981459438800812, Final Batch Loss: 0.28030407428741455\n",
      "Epoch 4246, Loss: 0.8173925280570984, Final Batch Loss: 0.281759649515152\n",
      "Epoch 4247, Loss: 0.7647382467985153, Final Batch Loss: 0.266696035861969\n",
      "Epoch 4248, Loss: 0.6940919458866119, Final Batch Loss: 0.24647285044193268\n",
      "Epoch 4249, Loss: 0.7830715924501419, Final Batch Loss: 0.2645851671695709\n",
      "Epoch 4250, Loss: 0.8418706357479095, Final Batch Loss: 0.2546378970146179\n",
      "Epoch 4251, Loss: 0.6945576965808868, Final Batch Loss: 0.18150785565376282\n",
      "Epoch 4252, Loss: 0.7412062138319016, Final Batch Loss: 0.2492215782403946\n",
      "Epoch 4253, Loss: 0.7299761921167374, Final Batch Loss: 0.2160966694355011\n",
      "Epoch 4254, Loss: 0.7956315129995346, Final Batch Loss: 0.3433169424533844\n",
      "Epoch 4255, Loss: 0.8433007001876831, Final Batch Loss: 0.24674922227859497\n",
      "Epoch 4256, Loss: 0.8300333619117737, Final Batch Loss: 0.3253440260887146\n",
      "Epoch 4257, Loss: 0.8351227641105652, Final Batch Loss: 0.28209859132766724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4258, Loss: 0.7477068454027176, Final Batch Loss: 0.23765932023525238\n",
      "Epoch 4259, Loss: 0.7731995582580566, Final Batch Loss: 0.23570427298545837\n",
      "Epoch 4260, Loss: 0.7199828624725342, Final Batch Loss: 0.24941569566726685\n",
      "Epoch 4261, Loss: 0.8190098404884338, Final Batch Loss: 0.2681276500225067\n",
      "Epoch 4262, Loss: 0.8687039911746979, Final Batch Loss: 0.2563146650791168\n",
      "Epoch 4263, Loss: 0.7781480103731155, Final Batch Loss: 0.3214530050754547\n",
      "Epoch 4264, Loss: 0.7631241530179977, Final Batch Loss: 0.25712206959724426\n",
      "Epoch 4265, Loss: 0.6717965751886368, Final Batch Loss: 0.2169899344444275\n",
      "Epoch 4266, Loss: 0.8287010788917542, Final Batch Loss: 0.3103347718715668\n",
      "Epoch 4267, Loss: 0.7236185818910599, Final Batch Loss: 0.1586330085992813\n",
      "Epoch 4268, Loss: 0.8131440430879593, Final Batch Loss: 0.2776734530925751\n",
      "Epoch 4269, Loss: 0.7037977278232574, Final Batch Loss: 0.22650673985481262\n",
      "Epoch 4270, Loss: 0.7653341442346573, Final Batch Loss: 0.24308054149150848\n",
      "Epoch 4271, Loss: 0.8493360877037048, Final Batch Loss: 0.3340056538581848\n",
      "Epoch 4272, Loss: 0.8694911301136017, Final Batch Loss: 0.37455272674560547\n",
      "Epoch 4273, Loss: 0.8244951963424683, Final Batch Loss: 0.2884329557418823\n",
      "Epoch 4274, Loss: 0.6135373264551163, Final Batch Loss: 0.21461929380893707\n",
      "Epoch 4275, Loss: 0.7444649487733841, Final Batch Loss: 0.24759772419929504\n",
      "Epoch 4276, Loss: 0.7262367606163025, Final Batch Loss: 0.27014222741127014\n",
      "Epoch 4277, Loss: 0.7964666038751602, Final Batch Loss: 0.3123713433742523\n",
      "Epoch 4278, Loss: 0.8734080195426941, Final Batch Loss: 0.2995840609073639\n",
      "Epoch 4279, Loss: 0.7767023593187332, Final Batch Loss: 0.2727946937084198\n",
      "Epoch 4280, Loss: 0.6898326277732849, Final Batch Loss: 0.23275578022003174\n",
      "Epoch 4281, Loss: 0.7760172784328461, Final Batch Loss: 0.273542582988739\n",
      "Epoch 4282, Loss: 0.7345071732997894, Final Batch Loss: 0.24378803372383118\n",
      "Epoch 4283, Loss: 0.6883791089057922, Final Batch Loss: 0.17347322404384613\n",
      "Epoch 4284, Loss: 0.8021159023046494, Final Batch Loss: 0.28097614645957947\n",
      "Epoch 4285, Loss: 0.7317880243062973, Final Batch Loss: 0.2538829445838928\n",
      "Epoch 4286, Loss: 0.7894469350576401, Final Batch Loss: 0.3404322564601898\n",
      "Epoch 4287, Loss: 0.8226398080587387, Final Batch Loss: 0.2572665512561798\n",
      "Epoch 4288, Loss: 0.6604590713977814, Final Batch Loss: 0.1881493330001831\n",
      "Epoch 4289, Loss: 0.7271931767463684, Final Batch Loss: 0.249019593000412\n",
      "Epoch 4290, Loss: 0.6425494402647018, Final Batch Loss: 0.1630580723285675\n",
      "Epoch 4291, Loss: 0.7268654257059097, Final Batch Loss: 0.2505568563938141\n",
      "Epoch 4292, Loss: 0.6971303224563599, Final Batch Loss: 0.2440563291311264\n",
      "Epoch 4293, Loss: 0.896172821521759, Final Batch Loss: 0.37798845767974854\n",
      "Epoch 4294, Loss: 0.748657613992691, Final Batch Loss: 0.20838725566864014\n",
      "Epoch 4295, Loss: 0.7373591512441635, Final Batch Loss: 0.27168187499046326\n",
      "Epoch 4296, Loss: 0.8268272578716278, Final Batch Loss: 0.26346224546432495\n",
      "Epoch 4297, Loss: 0.8029396831989288, Final Batch Loss: 0.28897637128829956\n",
      "Epoch 4298, Loss: 0.8071779012680054, Final Batch Loss: 0.3479405343532562\n",
      "Epoch 4299, Loss: 0.8352110236883163, Final Batch Loss: 0.3255889415740967\n",
      "Epoch 4300, Loss: 0.6610224694013596, Final Batch Loss: 0.19592668116092682\n",
      "Epoch 4301, Loss: 0.8086052536964417, Final Batch Loss: 0.3030494451522827\n",
      "Epoch 4302, Loss: 0.7691494524478912, Final Batch Loss: 0.14568635821342468\n",
      "Epoch 4303, Loss: 0.6798896044492722, Final Batch Loss: 0.16509248316287994\n",
      "Epoch 4304, Loss: 0.8613283187150955, Final Batch Loss: 0.19074617326259613\n",
      "Epoch 4305, Loss: 0.683728501200676, Final Batch Loss: 0.22479254007339478\n",
      "Epoch 4306, Loss: 0.7539698630571365, Final Batch Loss: 0.22855111956596375\n",
      "Epoch 4307, Loss: 0.8246832340955734, Final Batch Loss: 0.31278201937675476\n",
      "Epoch 4308, Loss: 0.7876120507717133, Final Batch Loss: 0.23662039637565613\n",
      "Epoch 4309, Loss: 0.8416981846094131, Final Batch Loss: 0.28470391035079956\n",
      "Epoch 4310, Loss: 0.8104294687509537, Final Batch Loss: 0.3014562726020813\n",
      "Epoch 4311, Loss: 0.803569421172142, Final Batch Loss: 0.33441269397735596\n",
      "Epoch 4312, Loss: 0.6698710471391678, Final Batch Loss: 0.1680976003408432\n",
      "Epoch 4313, Loss: 0.9260316640138626, Final Batch Loss: 0.412748247385025\n",
      "Epoch 4314, Loss: 0.7170575261116028, Final Batch Loss: 0.2044759839773178\n",
      "Epoch 4315, Loss: 0.6915912479162216, Final Batch Loss: 0.2502571940422058\n",
      "Epoch 4316, Loss: 0.7426229268312454, Final Batch Loss: 0.21649567782878876\n",
      "Epoch 4317, Loss: 0.7229730188846588, Final Batch Loss: 0.2568919360637665\n",
      "Epoch 4318, Loss: 0.6822638362646103, Final Batch Loss: 0.21918885409832\n",
      "Epoch 4319, Loss: 0.7551364600658417, Final Batch Loss: 0.2641530930995941\n",
      "Epoch 4320, Loss: 0.6701091676950455, Final Batch Loss: 0.20606505870819092\n",
      "Epoch 4321, Loss: 0.7296086698770523, Final Batch Loss: 0.2845951020717621\n",
      "Epoch 4322, Loss: 0.7948788106441498, Final Batch Loss: 0.2844778299331665\n",
      "Epoch 4323, Loss: 0.80022794008255, Final Batch Loss: 0.21153733134269714\n",
      "Epoch 4324, Loss: 0.7839407324790955, Final Batch Loss: 0.2428208589553833\n",
      "Epoch 4325, Loss: 0.7991611957550049, Final Batch Loss: 0.24546197056770325\n",
      "Epoch 4326, Loss: 0.750413253903389, Final Batch Loss: 0.29888489842414856\n",
      "Epoch 4327, Loss: 0.8120192736387253, Final Batch Loss: 0.34032100439071655\n",
      "Epoch 4328, Loss: 0.7999460697174072, Final Batch Loss: 0.2538772225379944\n",
      "Epoch 4329, Loss: 0.8118386268615723, Final Batch Loss: 0.26242056488990784\n",
      "Epoch 4330, Loss: 0.7437564432621002, Final Batch Loss: 0.18560156226158142\n",
      "Epoch 4331, Loss: 0.7755791395902634, Final Batch Loss: 0.2710677981376648\n",
      "Epoch 4332, Loss: 0.627324566245079, Final Batch Loss: 0.17093095183372498\n",
      "Epoch 4333, Loss: 0.728490874171257, Final Batch Loss: 0.24711647629737854\n",
      "Epoch 4334, Loss: 0.713798999786377, Final Batch Loss: 0.26849082112312317\n",
      "Epoch 4335, Loss: 0.8756872117519379, Final Batch Loss: 0.34568285942077637\n",
      "Epoch 4336, Loss: 0.6727856546640396, Final Batch Loss: 0.20418304204940796\n",
      "Epoch 4337, Loss: 0.775918111205101, Final Batch Loss: 0.18743519484996796\n",
      "Epoch 4338, Loss: 0.7742814123630524, Final Batch Loss: 0.2255585491657257\n",
      "Epoch 4339, Loss: 0.735169842839241, Final Batch Loss: 0.22102828323841095\n",
      "Epoch 4340, Loss: 0.6204709410667419, Final Batch Loss: 0.15121202170848846\n",
      "Epoch 4341, Loss: 0.694355919957161, Final Batch Loss: 0.17153795063495636\n",
      "Epoch 4342, Loss: 0.7517776787281036, Final Batch Loss: 0.19072887301445007\n",
      "Epoch 4343, Loss: 0.7200596779584885, Final Batch Loss: 0.26048824191093445\n",
      "Epoch 4344, Loss: 0.8539229035377502, Final Batch Loss: 0.28053197264671326\n",
      "Epoch 4345, Loss: 0.7875889539718628, Final Batch Loss: 0.27267616987228394\n",
      "Epoch 4346, Loss: 0.7172221839427948, Final Batch Loss: 0.21599450707435608\n",
      "Epoch 4347, Loss: 0.700147807598114, Final Batch Loss: 0.20447076857089996\n",
      "Epoch 4348, Loss: 0.7075086086988449, Final Batch Loss: 0.1806296855211258\n",
      "Epoch 4349, Loss: 0.7604862749576569, Final Batch Loss: 0.2770466208457947\n",
      "Epoch 4350, Loss: 0.983234241604805, Final Batch Loss: 0.4690330922603607\n",
      "Epoch 4351, Loss: 0.8154260069131851, Final Batch Loss: 0.23364849388599396\n",
      "Epoch 4352, Loss: 0.7643755078315735, Final Batch Loss: 0.2500782310962677\n",
      "Epoch 4353, Loss: 0.8408172130584717, Final Batch Loss: 0.2756507098674774\n",
      "Epoch 4354, Loss: 0.8816182911396027, Final Batch Loss: 0.3488784432411194\n",
      "Epoch 4355, Loss: 0.8254783749580383, Final Batch Loss: 0.3097822368144989\n",
      "Epoch 4356, Loss: 0.9661080837249756, Final Batch Loss: 0.41781744360923767\n",
      "Epoch 4357, Loss: 0.7881887555122375, Final Batch Loss: 0.2554527819156647\n",
      "Epoch 4358, Loss: 0.809375673532486, Final Batch Loss: 0.1880781054496765\n",
      "Epoch 4359, Loss: 0.7823200076818466, Final Batch Loss: 0.30344390869140625\n",
      "Epoch 4360, Loss: 0.6780270934104919, Final Batch Loss: 0.22702164947986603\n",
      "Epoch 4361, Loss: 0.7767149806022644, Final Batch Loss: 0.2895069718360901\n",
      "Epoch 4362, Loss: 0.7706895172595978, Final Batch Loss: 0.31725847721099854\n",
      "Epoch 4363, Loss: 0.7903172671794891, Final Batch Loss: 0.3205828368663788\n",
      "Epoch 4364, Loss: 0.7285734862089157, Final Batch Loss: 0.24111348390579224\n",
      "Epoch 4365, Loss: 0.7447129338979721, Final Batch Loss: 0.177725687623024\n",
      "Epoch 4366, Loss: 0.7433827668428421, Final Batch Loss: 0.3144476115703583\n",
      "Epoch 4367, Loss: 0.8301213383674622, Final Batch Loss: 0.2659863531589508\n",
      "Epoch 4368, Loss: 0.7212863862514496, Final Batch Loss: 0.20978862047195435\n",
      "Epoch 4369, Loss: 0.7122102081775665, Final Batch Loss: 0.25499245524406433\n",
      "Epoch 4370, Loss: 0.8322761952877045, Final Batch Loss: 0.26850712299346924\n",
      "Epoch 4371, Loss: 0.8362386524677277, Final Batch Loss: 0.3012421429157257\n",
      "Epoch 4372, Loss: 0.7543199062347412, Final Batch Loss: 0.2705191373825073\n",
      "Epoch 4373, Loss: 0.7781559079885483, Final Batch Loss: 0.26903846859931946\n",
      "Epoch 4374, Loss: 0.7293896675109863, Final Batch Loss: 0.19242696464061737\n",
      "Epoch 4375, Loss: 0.7868121862411499, Final Batch Loss: 0.2804815173149109\n",
      "Epoch 4376, Loss: 0.7145591378211975, Final Batch Loss: 0.27694642543792725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4377, Loss: 0.6954779028892517, Final Batch Loss: 0.2159997820854187\n",
      "Epoch 4378, Loss: 0.724113404750824, Final Batch Loss: 0.2511133849620819\n",
      "Epoch 4379, Loss: 0.7849486172199249, Final Batch Loss: 0.2818499803543091\n",
      "Epoch 4380, Loss: 0.7990346401929855, Final Batch Loss: 0.2722703218460083\n",
      "Epoch 4381, Loss: 0.7276855409145355, Final Batch Loss: 0.19803106784820557\n",
      "Epoch 4382, Loss: 0.6756511628627777, Final Batch Loss: 0.19694431126117706\n",
      "Epoch 4383, Loss: 0.7343343496322632, Final Batch Loss: 0.22703136503696442\n",
      "Epoch 4384, Loss: 0.7587416917085648, Final Batch Loss: 0.22367914021015167\n",
      "Epoch 4385, Loss: 0.8901284486055374, Final Batch Loss: 0.3878135085105896\n",
      "Epoch 4386, Loss: 0.6978987753391266, Final Batch Loss: 0.20051239430904388\n",
      "Epoch 4387, Loss: 0.7233682870864868, Final Batch Loss: 0.2462167590856552\n",
      "Epoch 4388, Loss: 0.7255571335554123, Final Batch Loss: 0.3004568815231323\n",
      "Epoch 4389, Loss: 0.9497349709272385, Final Batch Loss: 0.39720410108566284\n",
      "Epoch 4390, Loss: 0.7230813503265381, Final Batch Loss: 0.24225856363773346\n",
      "Epoch 4391, Loss: 0.8621698617935181, Final Batch Loss: 0.326933354139328\n",
      "Epoch 4392, Loss: 0.7467342615127563, Final Batch Loss: 0.2128254771232605\n",
      "Epoch 4393, Loss: 0.7375078499317169, Final Batch Loss: 0.2555406093597412\n",
      "Epoch 4394, Loss: 0.8730439990758896, Final Batch Loss: 0.27760207653045654\n",
      "Epoch 4395, Loss: 0.7996240258216858, Final Batch Loss: 0.22001077234745026\n",
      "Epoch 4396, Loss: 0.6903514117002487, Final Batch Loss: 0.21516290307044983\n",
      "Epoch 4397, Loss: 0.7164819985628128, Final Batch Loss: 0.24036219716072083\n",
      "Epoch 4398, Loss: 0.8494309037923813, Final Batch Loss: 0.32783403992652893\n",
      "Epoch 4399, Loss: 0.7199790626764297, Final Batch Loss: 0.18133710324764252\n",
      "Epoch 4400, Loss: 0.8155499398708344, Final Batch Loss: 0.22400158643722534\n",
      "Epoch 4401, Loss: 0.7359364479780197, Final Batch Loss: 0.2791699171066284\n",
      "Epoch 4402, Loss: 0.7835959941148758, Final Batch Loss: 0.27572402358055115\n",
      "Epoch 4403, Loss: 0.7449244111776352, Final Batch Loss: 0.23703236877918243\n",
      "Epoch 4404, Loss: 0.7755354046821594, Final Batch Loss: 0.27447372674942017\n",
      "Epoch 4405, Loss: 0.7244134843349457, Final Batch Loss: 0.2944995164871216\n",
      "Epoch 4406, Loss: 0.9142376184463501, Final Batch Loss: 0.376634418964386\n",
      "Epoch 4407, Loss: 0.6482747495174408, Final Batch Loss: 0.16075459122657776\n",
      "Epoch 4408, Loss: 0.834860548377037, Final Batch Loss: 0.3450101613998413\n",
      "Epoch 4409, Loss: 0.8035498857498169, Final Batch Loss: 0.31079939007759094\n",
      "Epoch 4410, Loss: 0.7620662450790405, Final Batch Loss: 0.29925742745399475\n",
      "Epoch 4411, Loss: 0.7550693154335022, Final Batch Loss: 0.301626592874527\n",
      "Epoch 4412, Loss: 0.8267424404621124, Final Batch Loss: 0.2491196244955063\n",
      "Epoch 4413, Loss: 0.7100179046392441, Final Batch Loss: 0.20858748257160187\n",
      "Epoch 4414, Loss: 0.741292417049408, Final Batch Loss: 0.26566606760025024\n",
      "Epoch 4415, Loss: 0.8871077597141266, Final Batch Loss: 0.34905070066452026\n",
      "Epoch 4416, Loss: 0.7509618103504181, Final Batch Loss: 0.25327879190444946\n",
      "Epoch 4417, Loss: 0.77634397149086, Final Batch Loss: 0.30585378408432007\n",
      "Epoch 4418, Loss: 0.6384579390287399, Final Batch Loss: 0.2527264654636383\n",
      "Epoch 4419, Loss: 0.6082692295312881, Final Batch Loss: 0.15758037567138672\n",
      "Epoch 4420, Loss: 0.7630354315042496, Final Batch Loss: 0.2383720427751541\n",
      "Epoch 4421, Loss: 0.7951431274414062, Final Batch Loss: 0.26603519916534424\n",
      "Epoch 4422, Loss: 0.7250398844480515, Final Batch Loss: 0.19657543301582336\n",
      "Epoch 4423, Loss: 0.7345409840345383, Final Batch Loss: 0.22742021083831787\n",
      "Epoch 4424, Loss: 0.7383577823638916, Final Batch Loss: 0.2123446762561798\n",
      "Epoch 4425, Loss: 0.8184731006622314, Final Batch Loss: 0.27507874369621277\n",
      "Epoch 4426, Loss: 0.7680813819169998, Final Batch Loss: 0.23368708789348602\n",
      "Epoch 4427, Loss: 0.7047903388738632, Final Batch Loss: 0.1878790259361267\n",
      "Epoch 4428, Loss: 0.7849812507629395, Final Batch Loss: 0.251183420419693\n",
      "Epoch 4429, Loss: 0.7670798897743225, Final Batch Loss: 0.23763135075569153\n",
      "Epoch 4430, Loss: 0.7581801414489746, Final Batch Loss: 0.30911457538604736\n",
      "Epoch 4431, Loss: 0.6743632405996323, Final Batch Loss: 0.23394180834293365\n",
      "Epoch 4432, Loss: 0.7363689690828323, Final Batch Loss: 0.2719161808490753\n",
      "Epoch 4433, Loss: 0.8642018139362335, Final Batch Loss: 0.2822207808494568\n",
      "Epoch 4434, Loss: 0.7375629991292953, Final Batch Loss: 0.3162027597427368\n",
      "Epoch 4435, Loss: 0.7179738283157349, Final Batch Loss: 0.22883714735507965\n",
      "Epoch 4436, Loss: 0.6610561311244965, Final Batch Loss: 0.1977943480014801\n",
      "Epoch 4437, Loss: 0.7156515717506409, Final Batch Loss: 0.2744690775871277\n",
      "Epoch 4438, Loss: 0.7490233182907104, Final Batch Loss: 0.32173535227775574\n",
      "Epoch 4439, Loss: 0.6738058775663376, Final Batch Loss: 0.23738272488117218\n",
      "Epoch 4440, Loss: 0.7746010571718216, Final Batch Loss: 0.23686063289642334\n",
      "Epoch 4441, Loss: 0.7407196164131165, Final Batch Loss: 0.22089599072933197\n",
      "Epoch 4442, Loss: 0.6802081018686295, Final Batch Loss: 0.23221179842948914\n",
      "Epoch 4443, Loss: 0.6933203488588333, Final Batch Loss: 0.2130734771490097\n",
      "Epoch 4444, Loss: 0.6099252253770828, Final Batch Loss: 0.18844419717788696\n",
      "Epoch 4445, Loss: 0.7509641498327255, Final Batch Loss: 0.26869386434555054\n",
      "Epoch 4446, Loss: 0.7963504493236542, Final Batch Loss: 0.28014063835144043\n",
      "Epoch 4447, Loss: 0.8764962255954742, Final Batch Loss: 0.3235531747341156\n",
      "Epoch 4448, Loss: 0.8133139759302139, Final Batch Loss: 0.26922035217285156\n",
      "Epoch 4449, Loss: 0.7544845044612885, Final Batch Loss: 0.29986634850502014\n",
      "Epoch 4450, Loss: 0.6987365037202835, Final Batch Loss: 0.17777565121650696\n",
      "Epoch 4451, Loss: 0.7517983913421631, Final Batch Loss: 0.2700311243534088\n",
      "Epoch 4452, Loss: 0.6359078586101532, Final Batch Loss: 0.18684755265712738\n",
      "Epoch 4453, Loss: 0.6455690562725067, Final Batch Loss: 0.21244370937347412\n",
      "Epoch 4454, Loss: 0.6839024871587753, Final Batch Loss: 0.2119007110595703\n",
      "Epoch 4455, Loss: 0.7170146405696869, Final Batch Loss: 0.23397627472877502\n",
      "Epoch 4456, Loss: 0.7127504646778107, Final Batch Loss: 0.26409780979156494\n",
      "Epoch 4457, Loss: 0.7605301290750504, Final Batch Loss: 0.23088671267032623\n",
      "Epoch 4458, Loss: 0.7762579470872879, Final Batch Loss: 0.24085742235183716\n",
      "Epoch 4459, Loss: 0.8582332134246826, Final Batch Loss: 0.29332235455513\n",
      "Epoch 4460, Loss: 0.7491299659013748, Final Batch Loss: 0.26885005831718445\n",
      "Epoch 4461, Loss: 0.6324831694364548, Final Batch Loss: 0.16800998151302338\n",
      "Epoch 4462, Loss: 0.8120266050100327, Final Batch Loss: 0.3103279173374176\n",
      "Epoch 4463, Loss: 0.7602707147598267, Final Batch Loss: 0.256137877702713\n",
      "Epoch 4464, Loss: 0.6937370598316193, Final Batch Loss: 0.23028108477592468\n",
      "Epoch 4465, Loss: 0.6728069633245468, Final Batch Loss: 0.2487538754940033\n",
      "Epoch 4466, Loss: 0.7489721029996872, Final Batch Loss: 0.2451896071434021\n",
      "Epoch 4467, Loss: 0.7212518155574799, Final Batch Loss: 0.18822193145751953\n",
      "Epoch 4468, Loss: 0.7198760360479355, Final Batch Loss: 0.22924605011940002\n",
      "Epoch 4469, Loss: 0.7483564764261246, Final Batch Loss: 0.22417186200618744\n",
      "Epoch 4470, Loss: 0.764248326420784, Final Batch Loss: 0.20495858788490295\n",
      "Epoch 4471, Loss: 0.7407064884901047, Final Batch Loss: 0.29711106419563293\n",
      "Epoch 4472, Loss: 0.7101273387670517, Final Batch Loss: 0.20898129045963287\n",
      "Epoch 4473, Loss: 0.771306574344635, Final Batch Loss: 0.22148889303207397\n",
      "Epoch 4474, Loss: 0.8162726759910583, Final Batch Loss: 0.23299124836921692\n",
      "Epoch 4475, Loss: 0.6399665623903275, Final Batch Loss: 0.21295948326587677\n",
      "Epoch 4476, Loss: 0.686824381351471, Final Batch Loss: 0.18777376413345337\n",
      "Epoch 4477, Loss: 0.855364665389061, Final Batch Loss: 0.24012796580791473\n",
      "Epoch 4478, Loss: 0.7231544256210327, Final Batch Loss: 0.20844894647598267\n",
      "Epoch 4479, Loss: 0.7827301919460297, Final Batch Loss: 0.2931509017944336\n",
      "Epoch 4480, Loss: 0.7771061211824417, Final Batch Loss: 0.2440112680196762\n",
      "Epoch 4481, Loss: 0.7363974303007126, Final Batch Loss: 0.24967864155769348\n",
      "Epoch 4482, Loss: 0.7885901778936386, Final Batch Loss: 0.306923508644104\n",
      "Epoch 4483, Loss: 0.6986830085515976, Final Batch Loss: 0.22528034448623657\n",
      "Epoch 4484, Loss: 0.7165292501449585, Final Batch Loss: 0.22680549323558807\n",
      "Epoch 4485, Loss: 0.7552333921194077, Final Batch Loss: 0.21734453737735748\n",
      "Epoch 4486, Loss: 0.8114450871944427, Final Batch Loss: 0.2708051800727844\n",
      "Epoch 4487, Loss: 0.7137712389230728, Final Batch Loss: 0.180982306599617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4488, Loss: 0.7785325348377228, Final Batch Loss: 0.25793102383613586\n",
      "Epoch 4489, Loss: 0.7168120592832565, Final Batch Loss: 0.27530989050865173\n",
      "Epoch 4490, Loss: 0.7096008658409119, Final Batch Loss: 0.17846094071865082\n",
      "Epoch 4491, Loss: 0.824307307600975, Final Batch Loss: 0.36485156416893005\n",
      "Epoch 4492, Loss: 0.7981959283351898, Final Batch Loss: 0.26599669456481934\n",
      "Epoch 4493, Loss: 0.8534530848264694, Final Batch Loss: 0.33836662769317627\n",
      "Epoch 4494, Loss: 0.7075465768575668, Final Batch Loss: 0.26575836539268494\n",
      "Epoch 4495, Loss: 0.7902192324399948, Final Batch Loss: 0.3312723636627197\n",
      "Epoch 4496, Loss: 0.7490160167217255, Final Batch Loss: 0.28913596272468567\n",
      "Epoch 4497, Loss: 0.724363312125206, Final Batch Loss: 0.30348241329193115\n",
      "Epoch 4498, Loss: 0.7713930159807205, Final Batch Loss: 0.25084105134010315\n",
      "Epoch 4499, Loss: 0.8842328637838364, Final Batch Loss: 0.39145320653915405\n",
      "Epoch 4500, Loss: 0.8319536447525024, Final Batch Loss: 0.2969273626804352\n",
      "Epoch 4501, Loss: 0.7860555797815323, Final Batch Loss: 0.27512285113334656\n",
      "Epoch 4502, Loss: 0.6596700102090836, Final Batch Loss: 0.18390874564647675\n",
      "Epoch 4503, Loss: 0.799494132399559, Final Batch Loss: 0.30392494797706604\n",
      "Epoch 4504, Loss: 0.8211202174425125, Final Batch Loss: 0.2660858929157257\n",
      "Epoch 4505, Loss: 0.7968341410160065, Final Batch Loss: 0.2564809322357178\n",
      "Epoch 4506, Loss: 0.7604160606861115, Final Batch Loss: 0.2889194190502167\n",
      "Epoch 4507, Loss: 0.7028131633996964, Final Batch Loss: 0.24911479651927948\n",
      "Epoch 4508, Loss: 0.7020166665315628, Final Batch Loss: 0.2205030620098114\n",
      "Epoch 4509, Loss: 0.7805440127849579, Final Batch Loss: 0.25042858719825745\n",
      "Epoch 4510, Loss: 0.742177814245224, Final Batch Loss: 0.27077868580818176\n",
      "Epoch 4511, Loss: 0.6850730925798416, Final Batch Loss: 0.19374990463256836\n",
      "Epoch 4512, Loss: 0.7651047259569168, Final Batch Loss: 0.22328953444957733\n",
      "Epoch 4513, Loss: 0.7089596837759018, Final Batch Loss: 0.2094581425189972\n",
      "Epoch 4514, Loss: 0.7519876509904861, Final Batch Loss: 0.2480379343032837\n",
      "Epoch 4515, Loss: 0.8681125640869141, Final Batch Loss: 0.28105857968330383\n",
      "Epoch 4516, Loss: 0.8440891802310944, Final Batch Loss: 0.2955619692802429\n",
      "Epoch 4517, Loss: 0.7729391604661942, Final Batch Loss: 0.23043285310268402\n",
      "Epoch 4518, Loss: 0.7011456191539764, Final Batch Loss: 0.16681960225105286\n",
      "Epoch 4519, Loss: 0.7419434785842896, Final Batch Loss: 0.2115287184715271\n",
      "Epoch 4520, Loss: 0.8187880963087082, Final Batch Loss: 0.22992129623889923\n",
      "Epoch 4521, Loss: 0.7804774194955826, Final Batch Loss: 0.26831290125846863\n",
      "Epoch 4522, Loss: 0.7775018364191055, Final Batch Loss: 0.2446567863225937\n",
      "Epoch 4523, Loss: 0.6733928620815277, Final Batch Loss: 0.19764263927936554\n",
      "Epoch 4524, Loss: 0.9058336615562439, Final Batch Loss: 0.40745386481285095\n",
      "Epoch 4525, Loss: 0.7881985604763031, Final Batch Loss: 0.2642499804496765\n",
      "Epoch 4526, Loss: 0.7146983295679092, Final Batch Loss: 0.21245701611042023\n",
      "Epoch 4527, Loss: 0.7246217131614685, Final Batch Loss: 0.24857325851917267\n",
      "Epoch 4528, Loss: 0.8420892655849457, Final Batch Loss: 0.33837267756462097\n",
      "Epoch 4529, Loss: 0.7215041965246201, Final Batch Loss: 0.2635389268398285\n",
      "Epoch 4530, Loss: 0.7197904884815216, Final Batch Loss: 0.25898224115371704\n",
      "Epoch 4531, Loss: 0.8731943070888519, Final Batch Loss: 0.33531033992767334\n",
      "Epoch 4532, Loss: 0.867572009563446, Final Batch Loss: 0.30202633142471313\n",
      "Epoch 4533, Loss: 0.8041233569383621, Final Batch Loss: 0.25980979204177856\n",
      "Epoch 4534, Loss: 0.7918772548437119, Final Batch Loss: 0.2515704929828644\n",
      "Epoch 4535, Loss: 0.7927085161209106, Final Batch Loss: 0.2824283242225647\n",
      "Epoch 4536, Loss: 0.662071704864502, Final Batch Loss: 0.22039732336997986\n",
      "Epoch 4537, Loss: 0.6888466775417328, Final Batch Loss: 0.2123938351869583\n",
      "Epoch 4538, Loss: 0.7569150626659393, Final Batch Loss: 0.18211963772773743\n",
      "Epoch 4539, Loss: 0.7234892249107361, Final Batch Loss: 0.21340543031692505\n",
      "Epoch 4540, Loss: 0.7180917859077454, Final Batch Loss: 0.21349555253982544\n",
      "Epoch 4541, Loss: 0.7409722059965134, Final Batch Loss: 0.22209078073501587\n",
      "Epoch 4542, Loss: 0.7465261369943619, Final Batch Loss: 0.2233119010925293\n",
      "Epoch 4543, Loss: 0.79513780772686, Final Batch Loss: 0.285396546125412\n",
      "Epoch 4544, Loss: 0.7579050809144974, Final Batch Loss: 0.2351757138967514\n",
      "Epoch 4545, Loss: 0.7166591286659241, Final Batch Loss: 0.2720877528190613\n",
      "Epoch 4546, Loss: 0.7091477513313293, Final Batch Loss: 0.21742188930511475\n",
      "Epoch 4547, Loss: 0.7865867018699646, Final Batch Loss: 0.2583855092525482\n",
      "Epoch 4548, Loss: 0.6776504367589951, Final Batch Loss: 0.1993643045425415\n",
      "Epoch 4549, Loss: 0.7356849610805511, Final Batch Loss: 0.22385098040103912\n",
      "Epoch 4550, Loss: 0.6805024594068527, Final Batch Loss: 0.23772871494293213\n",
      "Epoch 4551, Loss: 0.7711020410060883, Final Batch Loss: 0.24670962989330292\n",
      "Epoch 4552, Loss: 0.758846640586853, Final Batch Loss: 0.26415151357650757\n",
      "Epoch 4553, Loss: 0.7666808068752289, Final Batch Loss: 0.27521201968193054\n",
      "Epoch 4554, Loss: 0.770049661397934, Final Batch Loss: 0.17553216218948364\n",
      "Epoch 4555, Loss: 0.6969442665576935, Final Batch Loss: 0.23278240859508514\n",
      "Epoch 4556, Loss: 0.7758450955152512, Final Batch Loss: 0.22382661700248718\n",
      "Epoch 4557, Loss: 0.7973553389310837, Final Batch Loss: 0.32427555322647095\n",
      "Epoch 4558, Loss: 0.7394587695598602, Final Batch Loss: 0.2821657657623291\n",
      "Epoch 4559, Loss: 0.8823332190513611, Final Batch Loss: 0.2795446813106537\n",
      "Epoch 4560, Loss: 0.7107528001070023, Final Batch Loss: 0.26822277903556824\n",
      "Epoch 4561, Loss: 0.721148282289505, Final Batch Loss: 0.2705223262310028\n",
      "Epoch 4562, Loss: 0.791169673204422, Final Batch Loss: 0.2773866057395935\n",
      "Epoch 4563, Loss: 0.7248753905296326, Final Batch Loss: 0.21284101903438568\n",
      "Epoch 4564, Loss: 0.7903548032045364, Final Batch Loss: 0.3108207583427429\n",
      "Epoch 4565, Loss: 0.741931363940239, Final Batch Loss: 0.3057974576950073\n",
      "Epoch 4566, Loss: 0.8906216025352478, Final Batch Loss: 0.3019299805164337\n",
      "Epoch 4567, Loss: 0.7458712309598923, Final Batch Loss: 0.18441835045814514\n",
      "Epoch 4568, Loss: 0.7966691255569458, Final Batch Loss: 0.3372346758842468\n",
      "Epoch 4569, Loss: 0.7794231325387955, Final Batch Loss: 0.29677730798721313\n",
      "Epoch 4570, Loss: 0.7679088711738586, Final Batch Loss: 0.23861919343471527\n",
      "Epoch 4571, Loss: 0.6601842641830444, Final Batch Loss: 0.18594729900360107\n",
      "Epoch 4572, Loss: 0.8103986531496048, Final Batch Loss: 0.30032095313072205\n",
      "Epoch 4573, Loss: 0.8075854182243347, Final Batch Loss: 0.2718048393726349\n",
      "Epoch 4574, Loss: 0.7273479700088501, Final Batch Loss: 0.2525075674057007\n",
      "Epoch 4575, Loss: 0.6990689933300018, Final Batch Loss: 0.17055371403694153\n",
      "Epoch 4576, Loss: 0.7368555963039398, Final Batch Loss: 0.2722538113594055\n",
      "Epoch 4577, Loss: 0.8021366745233536, Final Batch Loss: 0.17204438149929047\n",
      "Epoch 4578, Loss: 0.8341859579086304, Final Batch Loss: 0.2248069941997528\n",
      "Epoch 4579, Loss: 0.6791135966777802, Final Batch Loss: 0.17556826770305634\n",
      "Epoch 4580, Loss: 0.760557621717453, Final Batch Loss: 0.3141835331916809\n",
      "Epoch 4581, Loss: 0.7354609221220016, Final Batch Loss: 0.22182831168174744\n",
      "Epoch 4582, Loss: 0.7029263824224472, Final Batch Loss: 0.2328706979751587\n",
      "Epoch 4583, Loss: 0.6600371301174164, Final Batch Loss: 0.1866336166858673\n",
      "Epoch 4584, Loss: 0.6760623157024384, Final Batch Loss: 0.2162390649318695\n",
      "Epoch 4585, Loss: 0.7432855814695358, Final Batch Loss: 0.2783720791339874\n",
      "Epoch 4586, Loss: 0.7272844463586807, Final Batch Loss: 0.2191794067621231\n",
      "Epoch 4587, Loss: 0.6719769388437271, Final Batch Loss: 0.23695515096187592\n",
      "Epoch 4588, Loss: 0.739336296916008, Final Batch Loss: 0.2346803992986679\n",
      "Epoch 4589, Loss: 0.6657486259937286, Final Batch Loss: 0.1914498656988144\n",
      "Epoch 4590, Loss: 0.6863332837820053, Final Batch Loss: 0.21038885414600372\n",
      "Epoch 4591, Loss: 0.7527285218238831, Final Batch Loss: 0.34182071685791016\n",
      "Epoch 4592, Loss: 0.8042477369308472, Final Batch Loss: 0.31819427013397217\n",
      "Epoch 4593, Loss: 0.7378609627485275, Final Batch Loss: 0.2500417232513428\n",
      "Epoch 4594, Loss: 0.7159226089715958, Final Batch Loss: 0.24981452524662018\n",
      "Epoch 4595, Loss: 0.7455630302429199, Final Batch Loss: 0.21744298934936523\n",
      "Epoch 4596, Loss: 0.6297159790992737, Final Batch Loss: 0.17720481753349304\n",
      "Epoch 4597, Loss: 0.7783624976873398, Final Batch Loss: 0.24739177525043488\n",
      "Epoch 4598, Loss: 0.7536981403827667, Final Batch Loss: 0.2758536636829376\n",
      "Epoch 4599, Loss: 0.7217491716146469, Final Batch Loss: 0.26915794610977173\n",
      "Epoch 4600, Loss: 0.6950008273124695, Final Batch Loss: 0.2303125113248825\n",
      "Epoch 4601, Loss: 0.8116599023342133, Final Batch Loss: 0.23669597506523132\n",
      "Epoch 4602, Loss: 0.7521854639053345, Final Batch Loss: 0.21304160356521606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4603, Loss: 0.7793871909379959, Final Batch Loss: 0.24719345569610596\n",
      "Epoch 4604, Loss: 0.7138343155384064, Final Batch Loss: 0.2881453037261963\n",
      "Epoch 4605, Loss: 0.5806925296783447, Final Batch Loss: 0.18596552312374115\n",
      "Epoch 4606, Loss: 0.7470577508211136, Final Batch Loss: 0.25367292761802673\n",
      "Epoch 4607, Loss: 0.8018348217010498, Final Batch Loss: 0.2947711944580078\n",
      "Epoch 4608, Loss: 0.8330143690109253, Final Batch Loss: 0.2816441059112549\n",
      "Epoch 4609, Loss: 0.6103885322809219, Final Batch Loss: 0.1696217954158783\n",
      "Epoch 4610, Loss: 0.7808433473110199, Final Batch Loss: 0.2523907721042633\n",
      "Epoch 4611, Loss: 0.6392036229372025, Final Batch Loss: 0.164871945977211\n",
      "Epoch 4612, Loss: 0.6398729830980301, Final Batch Loss: 0.18104201555252075\n",
      "Epoch 4613, Loss: 0.7834833711385727, Final Batch Loss: 0.3034244179725647\n",
      "Epoch 4614, Loss: 0.8361353874206543, Final Batch Loss: 0.30908405780792236\n",
      "Epoch 4615, Loss: 0.7131297141313553, Final Batch Loss: 0.22278675436973572\n",
      "Epoch 4616, Loss: 0.7759941667318344, Final Batch Loss: 0.20249466598033905\n",
      "Epoch 4617, Loss: 0.740352526307106, Final Batch Loss: 0.231161430478096\n",
      "Epoch 4618, Loss: 0.7461335211992264, Final Batch Loss: 0.2290617674589157\n",
      "Epoch 4619, Loss: 0.7253334522247314, Final Batch Loss: 0.24710588157176971\n",
      "Epoch 4620, Loss: 0.7392850667238235, Final Batch Loss: 0.27607473731040955\n",
      "Epoch 4621, Loss: 0.8588675856590271, Final Batch Loss: 0.3018510043621063\n",
      "Epoch 4622, Loss: 0.7154329121112823, Final Batch Loss: 0.1873888224363327\n",
      "Epoch 4623, Loss: 0.7356575727462769, Final Batch Loss: 0.24561263620853424\n",
      "Epoch 4624, Loss: 0.717006504535675, Final Batch Loss: 0.17941239476203918\n",
      "Epoch 4625, Loss: 0.8317881077528, Final Batch Loss: 0.24766473472118378\n",
      "Epoch 4626, Loss: 0.7471303641796112, Final Batch Loss: 0.2311183214187622\n",
      "Epoch 4627, Loss: 0.7095627635717392, Final Batch Loss: 0.26017093658447266\n",
      "Epoch 4628, Loss: 0.7199121862649918, Final Batch Loss: 0.2723424732685089\n",
      "Epoch 4629, Loss: 0.7407499998807907, Final Batch Loss: 0.30328699946403503\n",
      "Epoch 4630, Loss: 0.8437947779893875, Final Batch Loss: 0.3315671682357788\n",
      "Epoch 4631, Loss: 0.6312509477138519, Final Batch Loss: 0.1745108962059021\n",
      "Epoch 4632, Loss: 0.7522683292627335, Final Batch Loss: 0.24153538048267365\n",
      "Epoch 4633, Loss: 0.6809448748826981, Final Batch Loss: 0.2380467653274536\n",
      "Epoch 4634, Loss: 0.796448603272438, Final Batch Loss: 0.2441524714231491\n",
      "Epoch 4635, Loss: 0.724405899643898, Final Batch Loss: 0.2331216186285019\n",
      "Epoch 4636, Loss: 0.750482514500618, Final Batch Loss: 0.29246628284454346\n",
      "Epoch 4637, Loss: 0.757412001490593, Final Batch Loss: 0.22917000949382782\n",
      "Epoch 4638, Loss: 0.7232275754213333, Final Batch Loss: 0.2568804919719696\n",
      "Epoch 4639, Loss: 0.7293507158756256, Final Batch Loss: 0.17510828375816345\n",
      "Epoch 4640, Loss: 0.7162591218948364, Final Batch Loss: 0.21500757336616516\n",
      "Epoch 4641, Loss: 0.6947361379861832, Final Batch Loss: 0.19572748243808746\n",
      "Epoch 4642, Loss: 0.8114927858114243, Final Batch Loss: 0.32269757986068726\n",
      "Epoch 4643, Loss: 0.6644265651702881, Final Batch Loss: 0.23899884521961212\n",
      "Epoch 4644, Loss: 0.726743295788765, Final Batch Loss: 0.21091704070568085\n",
      "Epoch 4645, Loss: 0.7365550845861435, Final Batch Loss: 0.22750599682331085\n",
      "Epoch 4646, Loss: 0.7185526490211487, Final Batch Loss: 0.1907416433095932\n",
      "Epoch 4647, Loss: 0.7922683656215668, Final Batch Loss: 0.25623929500579834\n",
      "Epoch 4648, Loss: 0.8034076988697052, Final Batch Loss: 0.3324808180332184\n",
      "Epoch 4649, Loss: 0.7102794647216797, Final Batch Loss: 0.2378798872232437\n",
      "Epoch 4650, Loss: 0.7484094053506851, Final Batch Loss: 0.3460843861103058\n",
      "Epoch 4651, Loss: 0.8314664661884308, Final Batch Loss: 0.30297133326530457\n",
      "Epoch 4652, Loss: 0.6620527356863022, Final Batch Loss: 0.16916242241859436\n",
      "Epoch 4653, Loss: 0.7421948164701462, Final Batch Loss: 0.20815227925777435\n",
      "Epoch 4654, Loss: 0.653996467590332, Final Batch Loss: 0.2017192542552948\n",
      "Epoch 4655, Loss: 0.6855617314577103, Final Batch Loss: 0.17573024332523346\n",
      "Epoch 4656, Loss: 0.7278283089399338, Final Batch Loss: 0.29702845215797424\n",
      "Epoch 4657, Loss: 0.6417738795280457, Final Batch Loss: 0.19500620663166046\n",
      "Epoch 4658, Loss: 0.746217280626297, Final Batch Loss: 0.17651624977588654\n",
      "Epoch 4659, Loss: 0.7662437111139297, Final Batch Loss: 0.22647953033447266\n",
      "Epoch 4660, Loss: 0.7200698107481003, Final Batch Loss: 0.24215874075889587\n",
      "Epoch 4661, Loss: 0.7004705667495728, Final Batch Loss: 0.2380499541759491\n",
      "Epoch 4662, Loss: 0.6975633949041367, Final Batch Loss: 0.171953022480011\n",
      "Epoch 4663, Loss: 0.7180402278900146, Final Batch Loss: 0.26842454075813293\n",
      "Epoch 4664, Loss: 0.7349311858415604, Final Batch Loss: 0.25781136751174927\n",
      "Epoch 4665, Loss: 0.6642754673957825, Final Batch Loss: 0.17549188435077667\n",
      "Epoch 4666, Loss: 0.8046827614307404, Final Batch Loss: 0.24167725443840027\n",
      "Epoch 4667, Loss: 0.6760017722845078, Final Batch Loss: 0.18542015552520752\n",
      "Epoch 4668, Loss: 0.6787343323230743, Final Batch Loss: 0.235237255692482\n",
      "Epoch 4669, Loss: 0.7386472970247269, Final Batch Loss: 0.3223443329334259\n",
      "Epoch 4670, Loss: 0.8635154813528061, Final Batch Loss: 0.34331902861595154\n",
      "Epoch 4671, Loss: 0.7484340965747833, Final Batch Loss: 0.24007651209831238\n",
      "Epoch 4672, Loss: 0.7196492254734039, Final Batch Loss: 0.3141571283340454\n",
      "Epoch 4673, Loss: 0.648389607667923, Final Batch Loss: 0.17430010437965393\n",
      "Epoch 4674, Loss: 0.6502449214458466, Final Batch Loss: 0.18986457586288452\n",
      "Epoch 4675, Loss: 0.777283325791359, Final Batch Loss: 0.19731132686138153\n",
      "Epoch 4676, Loss: 0.7860416173934937, Final Batch Loss: 0.2681278884410858\n",
      "Epoch 4677, Loss: 0.6833061128854752, Final Batch Loss: 0.2343217432498932\n",
      "Epoch 4678, Loss: 0.7410862892866135, Final Batch Loss: 0.27284085750579834\n",
      "Epoch 4679, Loss: 0.834690049290657, Final Batch Loss: 0.39994844794273376\n",
      "Epoch 4680, Loss: 0.7434623092412949, Final Batch Loss: 0.25313127040863037\n",
      "Epoch 4681, Loss: 0.7661677449941635, Final Batch Loss: 0.2095835655927658\n",
      "Epoch 4682, Loss: 0.6494702398777008, Final Batch Loss: 0.20576636493206024\n",
      "Epoch 4683, Loss: 0.6331611126661301, Final Batch Loss: 0.22818602621555328\n",
      "Epoch 4684, Loss: 0.6683310270309448, Final Batch Loss: 0.21144145727157593\n",
      "Epoch 4685, Loss: 0.7362433075904846, Final Batch Loss: 0.22802862524986267\n",
      "Epoch 4686, Loss: 0.6915101408958435, Final Batch Loss: 0.2418731302022934\n",
      "Epoch 4687, Loss: 0.7409738898277283, Final Batch Loss: 0.22608163952827454\n",
      "Epoch 4688, Loss: 0.7609877437353134, Final Batch Loss: 0.3025321960449219\n",
      "Epoch 4689, Loss: 0.6919348388910294, Final Batch Loss: 0.15460793673992157\n",
      "Epoch 4690, Loss: 0.7611547857522964, Final Batch Loss: 0.22292912006378174\n",
      "Epoch 4691, Loss: 0.8717024922370911, Final Batch Loss: 0.29963502287864685\n",
      "Epoch 4692, Loss: 0.7384078204631805, Final Batch Loss: 0.2839377820491791\n",
      "Epoch 4693, Loss: 0.7609075009822845, Final Batch Loss: 0.284075528383255\n",
      "Epoch 4694, Loss: 0.6652262210845947, Final Batch Loss: 0.1786937713623047\n",
      "Epoch 4695, Loss: 0.787254273891449, Final Batch Loss: 0.22887879610061646\n",
      "Epoch 4696, Loss: 0.8008916825056076, Final Batch Loss: 0.2944355010986328\n",
      "Epoch 4697, Loss: 0.7902507036924362, Final Batch Loss: 0.27388375997543335\n",
      "Epoch 4698, Loss: 0.6508247256278992, Final Batch Loss: 0.20772045850753784\n",
      "Epoch 4699, Loss: 0.763104647397995, Final Batch Loss: 0.24631193280220032\n",
      "Epoch 4700, Loss: 0.793882355093956, Final Batch Loss: 0.3325178325176239\n",
      "Epoch 4701, Loss: 0.7100514322519302, Final Batch Loss: 0.28364014625549316\n",
      "Epoch 4702, Loss: 0.794464498758316, Final Batch Loss: 0.3264065980911255\n",
      "Epoch 4703, Loss: 0.7354104667901993, Final Batch Loss: 0.26161810755729675\n",
      "Epoch 4704, Loss: 0.7792235314846039, Final Batch Loss: 0.27523937821388245\n",
      "Epoch 4705, Loss: 0.659058541059494, Final Batch Loss: 0.19551226496696472\n",
      "Epoch 4706, Loss: 0.7201903909444809, Final Batch Loss: 0.24711689352989197\n",
      "Epoch 4707, Loss: 0.7771120667457581, Final Batch Loss: 0.29973334074020386\n",
      "Epoch 4708, Loss: 0.7747886031866074, Final Batch Loss: 0.28770712018013\n",
      "Epoch 4709, Loss: 0.7111390829086304, Final Batch Loss: 0.22505605220794678\n",
      "Epoch 4710, Loss: 0.7842702120542526, Final Batch Loss: 0.2392391413450241\n",
      "Epoch 4711, Loss: 0.7868986278772354, Final Batch Loss: 0.37242546677589417\n",
      "Epoch 4712, Loss: 0.7403874695301056, Final Batch Loss: 0.2339027374982834\n",
      "Epoch 4713, Loss: 0.6067002713680267, Final Batch Loss: 0.20169058442115784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4714, Loss: 0.7308946996927261, Final Batch Loss: 0.18814577162265778\n",
      "Epoch 4715, Loss: 0.6463800966739655, Final Batch Loss: 0.23137475550174713\n",
      "Epoch 4716, Loss: 0.7221604287624359, Final Batch Loss: 0.23130559921264648\n",
      "Epoch 4717, Loss: 0.8132513463497162, Final Batch Loss: 0.30070972442626953\n",
      "Epoch 4718, Loss: 0.7444133758544922, Final Batch Loss: 0.24137762188911438\n",
      "Epoch 4719, Loss: 0.7610114812850952, Final Batch Loss: 0.2518845796585083\n",
      "Epoch 4720, Loss: 0.8297595530748367, Final Batch Loss: 0.3376579284667969\n",
      "Epoch 4721, Loss: 0.75572569668293, Final Batch Loss: 0.24043698608875275\n",
      "Epoch 4722, Loss: 0.81389020383358, Final Batch Loss: 0.3113775849342346\n",
      "Epoch 4723, Loss: 0.7463622093200684, Final Batch Loss: 0.23644351959228516\n",
      "Epoch 4724, Loss: 0.7606267333030701, Final Batch Loss: 0.2391050010919571\n",
      "Epoch 4725, Loss: 0.7234482616186142, Final Batch Loss: 0.2058858722448349\n",
      "Epoch 4726, Loss: 0.7021554708480835, Final Batch Loss: 0.19174273312091827\n",
      "Epoch 4727, Loss: 0.8175434917211533, Final Batch Loss: 0.22216911613941193\n",
      "Epoch 4728, Loss: 0.6467105150222778, Final Batch Loss: 0.26746058464050293\n",
      "Epoch 4729, Loss: 0.7068169713020325, Final Batch Loss: 0.23534858226776123\n",
      "Epoch 4730, Loss: 0.7236804515123367, Final Batch Loss: 0.2950001358985901\n",
      "Epoch 4731, Loss: 0.8481530696153641, Final Batch Loss: 0.3517610728740692\n",
      "Epoch 4732, Loss: 0.6824852675199509, Final Batch Loss: 0.2466249167919159\n",
      "Epoch 4733, Loss: 0.731001615524292, Final Batch Loss: 0.2542550265789032\n",
      "Epoch 4734, Loss: 0.727245569229126, Final Batch Loss: 0.2719259262084961\n",
      "Epoch 4735, Loss: 0.6109885275363922, Final Batch Loss: 0.18736156821250916\n",
      "Epoch 4736, Loss: 0.7474788874387741, Final Batch Loss: 0.3468024730682373\n",
      "Epoch 4737, Loss: 0.6818866282701492, Final Batch Loss: 0.1936035007238388\n",
      "Epoch 4738, Loss: 0.710205003619194, Final Batch Loss: 0.2070709466934204\n",
      "Epoch 4739, Loss: 0.7368962168693542, Final Batch Loss: 0.2842322885990143\n",
      "Epoch 4740, Loss: 0.7444192916154861, Final Batch Loss: 0.2235361784696579\n",
      "Epoch 4741, Loss: 0.7763015925884247, Final Batch Loss: 0.19597536325454712\n",
      "Epoch 4742, Loss: 0.6905460506677628, Final Batch Loss: 0.18739229440689087\n",
      "Epoch 4743, Loss: 0.6984144151210785, Final Batch Loss: 0.2032819241285324\n",
      "Epoch 4744, Loss: 0.6404726803302765, Final Batch Loss: 0.2714315950870514\n",
      "Epoch 4745, Loss: 0.7955479174852371, Final Batch Loss: 0.2610343098640442\n",
      "Epoch 4746, Loss: 0.7081567794084549, Final Batch Loss: 0.18895511329174042\n",
      "Epoch 4747, Loss: 0.7200459688901901, Final Batch Loss: 0.149953231215477\n",
      "Epoch 4748, Loss: 0.579491451382637, Final Batch Loss: 0.16677255928516388\n",
      "Epoch 4749, Loss: 0.7093277275562286, Final Batch Loss: 0.1880626678466797\n",
      "Epoch 4750, Loss: 0.6199623942375183, Final Batch Loss: 0.15100952982902527\n",
      "Epoch 4751, Loss: 0.6730604022741318, Final Batch Loss: 0.21979723870754242\n",
      "Epoch 4752, Loss: 0.7493596374988556, Final Batch Loss: 0.26315534114837646\n",
      "Epoch 4753, Loss: 0.7307963073253632, Final Batch Loss: 0.3117484152317047\n",
      "Epoch 4754, Loss: 0.8356011211872101, Final Batch Loss: 0.3194786310195923\n",
      "Epoch 4755, Loss: 0.6116272658109665, Final Batch Loss: 0.15926797688007355\n",
      "Epoch 4756, Loss: 0.8355190008878708, Final Batch Loss: 0.2054636925458908\n",
      "Epoch 4757, Loss: 0.7235793471336365, Final Batch Loss: 0.18882402777671814\n",
      "Epoch 4758, Loss: 0.6625692546367645, Final Batch Loss: 0.22097492218017578\n",
      "Epoch 4759, Loss: 0.6897306740283966, Final Batch Loss: 0.26537176966667175\n",
      "Epoch 4760, Loss: 0.7485416978597641, Final Batch Loss: 0.3069317042827606\n",
      "Epoch 4761, Loss: 0.6744256615638733, Final Batch Loss: 0.21541301906108856\n",
      "Epoch 4762, Loss: 0.7474395334720612, Final Batch Loss: 0.26435956358909607\n",
      "Epoch 4763, Loss: 0.7298546731472015, Final Batch Loss: 0.3470219671726227\n",
      "Epoch 4764, Loss: 0.6724862158298492, Final Batch Loss: 0.22849182784557343\n",
      "Epoch 4765, Loss: 0.8074533343315125, Final Batch Loss: 0.30030712485313416\n",
      "Epoch 4766, Loss: 0.7602235823869705, Final Batch Loss: 0.29094669222831726\n",
      "Epoch 4767, Loss: 0.687973752617836, Final Batch Loss: 0.1823560744524002\n",
      "Epoch 4768, Loss: 0.5810182988643646, Final Batch Loss: 0.12373945116996765\n",
      "Epoch 4769, Loss: 0.7613669335842133, Final Batch Loss: 0.26566770672798157\n",
      "Epoch 4770, Loss: 0.6603168696165085, Final Batch Loss: 0.2532636225223541\n",
      "Epoch 4771, Loss: 0.6760383546352386, Final Batch Loss: 0.19029249250888824\n",
      "Epoch 4772, Loss: 0.6199630796909332, Final Batch Loss: 0.18391269445419312\n",
      "Epoch 4773, Loss: 0.7311621010303497, Final Batch Loss: 0.20567522943019867\n",
      "Epoch 4774, Loss: 0.7160749733448029, Final Batch Loss: 0.22622057795524597\n",
      "Epoch 4775, Loss: 0.6588037610054016, Final Batch Loss: 0.25829631090164185\n",
      "Epoch 4776, Loss: 0.7680455595254898, Final Batch Loss: 0.21700216829776764\n",
      "Epoch 4777, Loss: 0.6465897858142853, Final Batch Loss: 0.1797022819519043\n",
      "Epoch 4778, Loss: 0.8241973966360092, Final Batch Loss: 0.276664137840271\n",
      "Epoch 4779, Loss: 0.7696928381919861, Final Batch Loss: 0.2310105562210083\n",
      "Epoch 4780, Loss: 0.7572363913059235, Final Batch Loss: 0.26857542991638184\n",
      "Epoch 4781, Loss: 0.7380316853523254, Final Batch Loss: 0.2230769693851471\n",
      "Epoch 4782, Loss: 0.6511100083589554, Final Batch Loss: 0.20211008191108704\n",
      "Epoch 4783, Loss: 0.7246010154485703, Final Batch Loss: 0.25046306848526\n",
      "Epoch 4784, Loss: 0.7296906560659409, Final Batch Loss: 0.22155357897281647\n",
      "Epoch 4785, Loss: 0.5986740291118622, Final Batch Loss: 0.20034724473953247\n",
      "Epoch 4786, Loss: 0.7537692338228226, Final Batch Loss: 0.2809458076953888\n",
      "Epoch 4787, Loss: 0.7402972280979156, Final Batch Loss: 0.29324814677238464\n",
      "Epoch 4788, Loss: 0.6874679625034332, Final Batch Loss: 0.2550365924835205\n",
      "Epoch 4789, Loss: 0.7524347454309464, Final Batch Loss: 0.21568426489830017\n",
      "Epoch 4790, Loss: 0.8164450377225876, Final Batch Loss: 0.32325971126556396\n",
      "Epoch 4791, Loss: 0.6714085638523102, Final Batch Loss: 0.20899170637130737\n",
      "Epoch 4792, Loss: 0.7619525194168091, Final Batch Loss: 0.2670639753341675\n",
      "Epoch 4793, Loss: 0.6469290405511856, Final Batch Loss: 0.22979499399662018\n",
      "Epoch 4794, Loss: 0.788892388343811, Final Batch Loss: 0.22851964831352234\n",
      "Epoch 4795, Loss: 0.6965924948453903, Final Batch Loss: 0.24499736726284027\n",
      "Epoch 4796, Loss: 0.7866620421409607, Final Batch Loss: 0.32386571168899536\n",
      "Epoch 4797, Loss: 0.7346528023481369, Final Batch Loss: 0.28447839617729187\n",
      "Epoch 4798, Loss: 0.7845954447984695, Final Batch Loss: 0.2926161587238312\n",
      "Epoch 4799, Loss: 0.793678805232048, Final Batch Loss: 0.34326452016830444\n",
      "Epoch 4800, Loss: 0.7558212727308273, Final Batch Loss: 0.2270510345697403\n",
      "Epoch 4801, Loss: 0.7276518493890762, Final Batch Loss: 0.23261818289756775\n",
      "Epoch 4802, Loss: 0.6976646184921265, Final Batch Loss: 0.22825153172016144\n",
      "Epoch 4803, Loss: 0.6883634030818939, Final Batch Loss: 0.20613807439804077\n",
      "Epoch 4804, Loss: 0.7361607402563095, Final Batch Loss: 0.2538037896156311\n",
      "Epoch 4805, Loss: 0.8085924685001373, Final Batch Loss: 0.2841275930404663\n",
      "Epoch 4806, Loss: 0.7277659028768539, Final Batch Loss: 0.3152911067008972\n",
      "Epoch 4807, Loss: 0.8115708380937576, Final Batch Loss: 0.3148500919342041\n",
      "Epoch 4808, Loss: 0.8356311321258545, Final Batch Loss: 0.26124072074890137\n",
      "Epoch 4809, Loss: 0.8148402571678162, Final Batch Loss: 0.31346002221107483\n",
      "Epoch 4810, Loss: 0.6226424127817154, Final Batch Loss: 0.1934126317501068\n",
      "Epoch 4811, Loss: 0.6473269015550613, Final Batch Loss: 0.22779914736747742\n",
      "Epoch 4812, Loss: 0.6732415407896042, Final Batch Loss: 0.16682645678520203\n",
      "Epoch 4813, Loss: 0.7619400322437286, Final Batch Loss: 0.21785445511341095\n",
      "Epoch 4814, Loss: 0.677415207028389, Final Batch Loss: 0.2656368911266327\n",
      "Epoch 4815, Loss: 0.7560621947050095, Final Batch Loss: 0.2931528687477112\n",
      "Epoch 4816, Loss: 0.7491587698459625, Final Batch Loss: 0.2931203842163086\n",
      "Epoch 4817, Loss: 0.8214154988527298, Final Batch Loss: 0.3758104145526886\n",
      "Epoch 4818, Loss: 0.7869129478931427, Final Batch Loss: 0.26641374826431274\n",
      "Epoch 4819, Loss: 0.5595262497663498, Final Batch Loss: 0.19433477520942688\n",
      "Epoch 4820, Loss: 0.7385115325450897, Final Batch Loss: 0.20885293185710907\n",
      "Epoch 4821, Loss: 0.7378498613834381, Final Batch Loss: 0.2629074454307556\n",
      "Epoch 4822, Loss: 0.6631142944097519, Final Batch Loss: 0.1967843919992447\n",
      "Epoch 4823, Loss: 0.8331481218338013, Final Batch Loss: 0.32172611355781555\n",
      "Epoch 4824, Loss: 0.697268083691597, Final Batch Loss: 0.13430698215961456\n",
      "Epoch 4825, Loss: 0.8521970063447952, Final Batch Loss: 0.35793399810791016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4826, Loss: 0.6537554711103439, Final Batch Loss: 0.14755196869373322\n",
      "Epoch 4827, Loss: 0.6553096175193787, Final Batch Loss: 0.14792640507221222\n",
      "Epoch 4828, Loss: 0.7269390970468521, Final Batch Loss: 0.1815422922372818\n",
      "Epoch 4829, Loss: 0.6711417138576508, Final Batch Loss: 0.1923018991947174\n",
      "Epoch 4830, Loss: 0.754102349281311, Final Batch Loss: 0.2537227272987366\n",
      "Epoch 4831, Loss: 0.6906582713127136, Final Batch Loss: 0.1375775933265686\n",
      "Epoch 4832, Loss: 0.7251473665237427, Final Batch Loss: 0.2782171666622162\n",
      "Epoch 4833, Loss: 0.6459711790084839, Final Batch Loss: 0.2010246068239212\n",
      "Epoch 4834, Loss: 0.7085820436477661, Final Batch Loss: 0.26825010776519775\n",
      "Epoch 4835, Loss: 0.679128959774971, Final Batch Loss: 0.25339028239250183\n",
      "Epoch 4836, Loss: 0.6646818965673447, Final Batch Loss: 0.14528656005859375\n",
      "Epoch 4837, Loss: 0.676857978105545, Final Batch Loss: 0.2330850511789322\n",
      "Epoch 4838, Loss: 0.749723419547081, Final Batch Loss: 0.21671853959560394\n",
      "Epoch 4839, Loss: 0.6697686314582825, Final Batch Loss: 0.2104009985923767\n",
      "Epoch 4840, Loss: 0.5938724279403687, Final Batch Loss: 0.1522822082042694\n",
      "Epoch 4841, Loss: 0.7730268090963364, Final Batch Loss: 0.21441909670829773\n",
      "Epoch 4842, Loss: 0.7500961124897003, Final Batch Loss: 0.2885565459728241\n",
      "Epoch 4843, Loss: 0.860624372959137, Final Batch Loss: 0.2795041799545288\n",
      "Epoch 4844, Loss: 0.7356601655483246, Final Batch Loss: 0.17919185757637024\n",
      "Epoch 4845, Loss: 0.9521062672138214, Final Batch Loss: 0.322743684053421\n",
      "Epoch 4846, Loss: 0.6977518796920776, Final Batch Loss: 0.17643100023269653\n",
      "Epoch 4847, Loss: 0.674086183309555, Final Batch Loss: 0.18086378276348114\n",
      "Epoch 4848, Loss: 0.8177655339241028, Final Batch Loss: 0.3469790518283844\n",
      "Epoch 4849, Loss: 0.7373985350131989, Final Batch Loss: 0.2246345430612564\n",
      "Epoch 4850, Loss: 0.7293293923139572, Final Batch Loss: 0.240838885307312\n",
      "Epoch 4851, Loss: 0.7464427202939987, Final Batch Loss: 0.2609342634677887\n",
      "Epoch 4852, Loss: 0.7047728151082993, Final Batch Loss: 0.16186828911304474\n",
      "Epoch 4853, Loss: 0.7487515062093735, Final Batch Loss: 0.2042424976825714\n",
      "Epoch 4854, Loss: 0.7321787327528, Final Batch Loss: 0.2507927417755127\n",
      "Epoch 4855, Loss: 0.848013162612915, Final Batch Loss: 0.3232291638851166\n",
      "Epoch 4856, Loss: 0.7750834673643112, Final Batch Loss: 0.2961808443069458\n",
      "Epoch 4857, Loss: 0.6294675767421722, Final Batch Loss: 0.25149640440940857\n",
      "Epoch 4858, Loss: 0.7221364974975586, Final Batch Loss: 0.2519189417362213\n",
      "Epoch 4859, Loss: 0.7404185086488724, Final Batch Loss: 0.21956545114517212\n",
      "Epoch 4860, Loss: 0.7298771291971207, Final Batch Loss: 0.21890877187252045\n",
      "Epoch 4861, Loss: 0.8178987950086594, Final Batch Loss: 0.1942826360464096\n",
      "Epoch 4862, Loss: 0.8523038923740387, Final Batch Loss: 0.24545374512672424\n",
      "Epoch 4863, Loss: 0.7332348972558975, Final Batch Loss: 0.2289837896823883\n",
      "Epoch 4864, Loss: 0.7311447262763977, Final Batch Loss: 0.2832939028739929\n",
      "Epoch 4865, Loss: 0.7183466553688049, Final Batch Loss: 0.2806141674518585\n",
      "Epoch 4866, Loss: 0.8484100550413132, Final Batch Loss: 0.30806800723075867\n",
      "Epoch 4867, Loss: 0.6888071000576019, Final Batch Loss: 0.2785758972167969\n",
      "Epoch 4868, Loss: 0.706776961684227, Final Batch Loss: 0.21998478472232819\n",
      "Epoch 4869, Loss: 0.7330279052257538, Final Batch Loss: 0.24583911895751953\n",
      "Epoch 4870, Loss: 0.7009121626615524, Final Batch Loss: 0.2522082030773163\n",
      "Epoch 4871, Loss: 0.6871131956577301, Final Batch Loss: 0.21202073991298676\n",
      "Epoch 4872, Loss: 0.769285261631012, Final Batch Loss: 0.23656585812568665\n",
      "Epoch 4873, Loss: 0.7353254556655884, Final Batch Loss: 0.23165515065193176\n",
      "Epoch 4874, Loss: 0.7179738283157349, Final Batch Loss: 0.21693550050258636\n",
      "Epoch 4875, Loss: 0.6684346944093704, Final Batch Loss: 0.22106300294399261\n",
      "Epoch 4876, Loss: 0.6741044074296951, Final Batch Loss: 0.2193259447813034\n",
      "Epoch 4877, Loss: 0.79543337225914, Final Batch Loss: 0.3266696631908417\n",
      "Epoch 4878, Loss: 0.7240214198827744, Final Batch Loss: 0.2778162658214569\n",
      "Epoch 4879, Loss: 0.743336632847786, Final Batch Loss: 0.24857130646705627\n",
      "Epoch 4880, Loss: 0.7512400448322296, Final Batch Loss: 0.29632893204689026\n",
      "Epoch 4881, Loss: 0.655639111995697, Final Batch Loss: 0.16368238627910614\n",
      "Epoch 4882, Loss: 0.7859646826982498, Final Batch Loss: 0.26681065559387207\n",
      "Epoch 4883, Loss: 0.7674910575151443, Final Batch Loss: 0.34022241830825806\n",
      "Epoch 4884, Loss: 0.7569012194871902, Final Batch Loss: 0.237369105219841\n",
      "Epoch 4885, Loss: 0.7288495451211929, Final Batch Loss: 0.2207806259393692\n",
      "Epoch 4886, Loss: 0.7485110908746719, Final Batch Loss: 0.2118256390094757\n",
      "Epoch 4887, Loss: 0.8205893933773041, Final Batch Loss: 0.266034871339798\n",
      "Epoch 4888, Loss: 0.7322069853544235, Final Batch Loss: 0.23973670601844788\n",
      "Epoch 4889, Loss: 0.7653203010559082, Final Batch Loss: 0.2675172686576843\n",
      "Epoch 4890, Loss: 0.7462535351514816, Final Batch Loss: 0.21094457805156708\n",
      "Epoch 4891, Loss: 0.7872394621372223, Final Batch Loss: 0.2634844481945038\n",
      "Epoch 4892, Loss: 0.7855822145938873, Final Batch Loss: 0.2526548504829407\n",
      "Epoch 4893, Loss: 0.6046787351369858, Final Batch Loss: 0.1577962338924408\n",
      "Epoch 4894, Loss: 0.6972670257091522, Final Batch Loss: 0.21566534042358398\n",
      "Epoch 4895, Loss: 0.772331491112709, Final Batch Loss: 0.18331532180309296\n",
      "Epoch 4896, Loss: 0.7900149673223495, Final Batch Loss: 0.2983219027519226\n",
      "Epoch 4897, Loss: 0.8057041168212891, Final Batch Loss: 0.341731995344162\n",
      "Epoch 4898, Loss: 0.6848891079425812, Final Batch Loss: 0.22114640474319458\n",
      "Epoch 4899, Loss: 0.7528590112924576, Final Batch Loss: 0.24865883588790894\n",
      "Epoch 4900, Loss: 0.748620942234993, Final Batch Loss: 0.30761125683784485\n",
      "Epoch 4901, Loss: 0.7148168385028839, Final Batch Loss: 0.2044793963432312\n",
      "Epoch 4902, Loss: 0.676838293671608, Final Batch Loss: 0.16227570176124573\n",
      "Epoch 4903, Loss: 0.7739341408014297, Final Batch Loss: 0.23192721605300903\n",
      "Epoch 4904, Loss: 0.6941622942686081, Final Batch Loss: 0.1882595717906952\n",
      "Epoch 4905, Loss: 0.7286666482686996, Final Batch Loss: 0.26376134157180786\n",
      "Epoch 4906, Loss: 0.7895884960889816, Final Batch Loss: 0.33574095368385315\n",
      "Epoch 4907, Loss: 0.7981500774621964, Final Batch Loss: 0.3136097192764282\n",
      "Epoch 4908, Loss: 0.7363565862178802, Final Batch Loss: 0.22006559371948242\n",
      "Epoch 4909, Loss: 0.6774571239948273, Final Batch Loss: 0.233437642455101\n",
      "Epoch 4910, Loss: 0.7318390309810638, Final Batch Loss: 0.29322680830955505\n",
      "Epoch 4911, Loss: 0.7315008640289307, Final Batch Loss: 0.24240495264530182\n",
      "Epoch 4912, Loss: 0.7051556706428528, Final Batch Loss: 0.2666863203048706\n",
      "Epoch 4913, Loss: 0.8100773096084595, Final Batch Loss: 0.2624610662460327\n",
      "Epoch 4914, Loss: 0.6314784437417984, Final Batch Loss: 0.124042809009552\n",
      "Epoch 4915, Loss: 0.6465512663125992, Final Batch Loss: 0.1998559981584549\n",
      "Epoch 4916, Loss: 0.7883525937795639, Final Batch Loss: 0.24720536172389984\n",
      "Epoch 4917, Loss: 0.7480363547801971, Final Batch Loss: 0.25213000178337097\n",
      "Epoch 4918, Loss: 0.6682258248329163, Final Batch Loss: 0.20620891451835632\n",
      "Epoch 4919, Loss: 0.6563989073038101, Final Batch Loss: 0.20086713135242462\n",
      "Epoch 4920, Loss: 0.742409273982048, Final Batch Loss: 0.23048105835914612\n",
      "Epoch 4921, Loss: 0.7583389431238174, Final Batch Loss: 0.313948392868042\n",
      "Epoch 4922, Loss: 0.7633505761623383, Final Batch Loss: 0.25061917304992676\n",
      "Epoch 4923, Loss: 0.7718375027179718, Final Batch Loss: 0.25177904963493347\n",
      "Epoch 4924, Loss: 0.6877532452344894, Final Batch Loss: 0.20455104112625122\n",
      "Epoch 4925, Loss: 0.6326195299625397, Final Batch Loss: 0.16830793023109436\n",
      "Epoch 4926, Loss: 0.6567146629095078, Final Batch Loss: 0.24713729321956635\n",
      "Epoch 4927, Loss: 0.9001084715127945, Final Batch Loss: 0.4438428580760956\n",
      "Epoch 4928, Loss: 0.6173851788043976, Final Batch Loss: 0.16733604669570923\n",
      "Epoch 4929, Loss: 0.8092340677976608, Final Batch Loss: 0.30011922121047974\n",
      "Epoch 4930, Loss: 0.6283107697963715, Final Batch Loss: 0.165433868765831\n",
      "Epoch 4931, Loss: 0.6698943674564362, Final Batch Loss: 0.23523344099521637\n",
      "Epoch 4932, Loss: 0.73804971575737, Final Batch Loss: 0.2732034921646118\n",
      "Epoch 4933, Loss: 0.8207649886608124, Final Batch Loss: 0.2619725167751312\n",
      "Epoch 4934, Loss: 0.8241845220327377, Final Batch Loss: 0.34348803758621216\n",
      "Epoch 4935, Loss: 0.7265750914812088, Final Batch Loss: 0.19130633771419525\n",
      "Epoch 4936, Loss: 0.8144903182983398, Final Batch Loss: 0.2830231487751007\n",
      "Epoch 4937, Loss: 0.7219550311565399, Final Batch Loss: 0.2245081067085266\n",
      "Epoch 4938, Loss: 0.6688237637281418, Final Batch Loss: 0.25001323223114014\n",
      "Epoch 4939, Loss: 0.6749110668897629, Final Batch Loss: 0.1999242901802063\n",
      "Epoch 4940, Loss: 0.6572629511356354, Final Batch Loss: 0.16667035222053528\n",
      "Epoch 4941, Loss: 0.7086239159107208, Final Batch Loss: 0.230083167552948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4942, Loss: 0.7496336251497269, Final Batch Loss: 0.26205840706825256\n",
      "Epoch 4943, Loss: 0.7423306852579117, Final Batch Loss: 0.20664416253566742\n",
      "Epoch 4944, Loss: 0.7830425947904587, Final Batch Loss: 0.3018244504928589\n",
      "Epoch 4945, Loss: 0.758674681186676, Final Batch Loss: 0.25853294134140015\n",
      "Epoch 4946, Loss: 0.8174761235713959, Final Batch Loss: 0.26994240283966064\n",
      "Epoch 4947, Loss: 0.7215414792299271, Final Batch Loss: 0.19475112855434418\n",
      "Epoch 4948, Loss: 0.6812148541212082, Final Batch Loss: 0.2039886862039566\n",
      "Epoch 4949, Loss: 0.6973604559898376, Final Batch Loss: 0.20105914771556854\n",
      "Epoch 4950, Loss: 0.7070957273244858, Final Batch Loss: 0.22125962376594543\n",
      "Epoch 4951, Loss: 0.6791283786296844, Final Batch Loss: 0.2526906132698059\n",
      "Epoch 4952, Loss: 0.7925958633422852, Final Batch Loss: 0.27858489751815796\n",
      "Epoch 4953, Loss: 0.6533331274986267, Final Batch Loss: 0.2651267945766449\n",
      "Epoch 4954, Loss: 0.7353804409503937, Final Batch Loss: 0.22510996460914612\n",
      "Epoch 4955, Loss: 0.794523760676384, Final Batch Loss: 0.2656957507133484\n",
      "Epoch 4956, Loss: 0.6517579704523087, Final Batch Loss: 0.1623745858669281\n",
      "Epoch 4957, Loss: 0.7173685282468796, Final Batch Loss: 0.2406490445137024\n",
      "Epoch 4958, Loss: 0.8869008421897888, Final Batch Loss: 0.43191829323768616\n",
      "Epoch 4959, Loss: 0.6610102504491806, Final Batch Loss: 0.2068781852722168\n",
      "Epoch 4960, Loss: 0.6603284776210785, Final Batch Loss: 0.15325519442558289\n",
      "Epoch 4961, Loss: 0.6860610544681549, Final Batch Loss: 0.1947186291217804\n",
      "Epoch 4962, Loss: 0.8061231523752213, Final Batch Loss: 0.32647421956062317\n",
      "Epoch 4963, Loss: 0.7758548259735107, Final Batch Loss: 0.25484418869018555\n",
      "Epoch 4964, Loss: 0.8519549518823624, Final Batch Loss: 0.29637420177459717\n",
      "Epoch 4965, Loss: 0.6971486061811447, Final Batch Loss: 0.25910431146621704\n",
      "Epoch 4966, Loss: 0.6533423215150833, Final Batch Loss: 0.20813025534152985\n",
      "Epoch 4967, Loss: 0.7224376946687698, Final Batch Loss: 0.21506084501743317\n",
      "Epoch 4968, Loss: 0.6321202367544174, Final Batch Loss: 0.20426273345947266\n",
      "Epoch 4969, Loss: 0.7742765545845032, Final Batch Loss: 0.2800438404083252\n",
      "Epoch 4970, Loss: 0.8023846596479416, Final Batch Loss: 0.2987653315067291\n",
      "Epoch 4971, Loss: 0.7595148533582687, Final Batch Loss: 0.2104727327823639\n",
      "Epoch 4972, Loss: 0.74103182554245, Final Batch Loss: 0.26069191098213196\n",
      "Epoch 4973, Loss: 0.8483959585428238, Final Batch Loss: 0.27891239523887634\n",
      "Epoch 4974, Loss: 0.6994776278734207, Final Batch Loss: 0.1605173498392105\n",
      "Epoch 4975, Loss: 0.640867680311203, Final Batch Loss: 0.1762493997812271\n",
      "Epoch 4976, Loss: 0.7803327739238739, Final Batch Loss: 0.25339275598526\n",
      "Epoch 4977, Loss: 0.7016344517469406, Final Batch Loss: 0.1640681028366089\n",
      "Epoch 4978, Loss: 0.73194719851017, Final Batch Loss: 0.2953976094722748\n",
      "Epoch 4979, Loss: 0.7118988484144211, Final Batch Loss: 0.18072538077831268\n",
      "Epoch 4980, Loss: 0.7335165143013, Final Batch Loss: 0.24242864549160004\n",
      "Epoch 4981, Loss: 0.7892169505357742, Final Batch Loss: 0.2919814884662628\n",
      "Epoch 4982, Loss: 0.7112824022769928, Final Batch Loss: 0.19319811463356018\n",
      "Epoch 4983, Loss: 0.7623908668756485, Final Batch Loss: 0.2855123281478882\n",
      "Epoch 4984, Loss: 0.901699960231781, Final Batch Loss: 0.39615413546562195\n",
      "Epoch 4985, Loss: 0.7222162634134293, Final Batch Loss: 0.278279185295105\n",
      "Epoch 4986, Loss: 0.7365928143262863, Final Batch Loss: 0.13532321155071259\n",
      "Epoch 4987, Loss: 0.7858470529317856, Final Batch Loss: 0.29741978645324707\n",
      "Epoch 4988, Loss: 0.5791692733764648, Final Batch Loss: 0.15111835300922394\n",
      "Epoch 4989, Loss: 0.628600150346756, Final Batch Loss: 0.1856509894132614\n",
      "Epoch 4990, Loss: 0.6593794524669647, Final Batch Loss: 0.22683857381343842\n",
      "Epoch 4991, Loss: 0.7150097191333771, Final Batch Loss: 0.20321577787399292\n",
      "Epoch 4992, Loss: 0.7313204854726791, Final Batch Loss: 0.291210412979126\n",
      "Epoch 4993, Loss: 0.7599256187677383, Final Batch Loss: 0.25824061036109924\n",
      "Epoch 4994, Loss: 0.838845506310463, Final Batch Loss: 0.34372544288635254\n",
      "Epoch 4995, Loss: 0.7351500242948532, Final Batch Loss: 0.29912078380584717\n",
      "Epoch 4996, Loss: 0.6589650064706802, Final Batch Loss: 0.17558209598064423\n",
      "Epoch 4997, Loss: 0.7410822808742523, Final Batch Loss: 0.23632478713989258\n",
      "Epoch 4998, Loss: 0.6761713325977325, Final Batch Loss: 0.2185613065958023\n",
      "Epoch 4999, Loss: 0.8518111258745193, Final Batch Loss: 0.3931662142276764\n",
      "Epoch 5000, Loss: 0.8502168953418732, Final Batch Loss: 0.3194199204444885\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0  0  0  0  2  0]\n",
      " [ 0  0  7  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  6  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  8  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  0  6  0  0  0  0  0  0  0  0  7]\n",
      " [ 0  0  0  0  0  0  9  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  5  0  0  0  0  0  1]\n",
      " [ 0  1  0  0  0  0  0  0  0 12  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0 10  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0  6  0]\n",
      " [ 0  0  4  0  0  0  0  0  0  0  0  0  0  0  3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        23\n",
      "           1    0.85714   0.85714   0.85714        14\n",
      "           2    0.53846   1.00000   0.70000         7\n",
      "           3    1.00000   0.85714   0.92308         7\n",
      "           4    0.88889   1.00000   0.94118         8\n",
      "           5    0.85714   0.40000   0.54545        15\n",
      "           6    1.00000   0.90000   0.94737        10\n",
      "           7    0.81818   1.00000   0.90000         9\n",
      "           8    1.00000   0.71429   0.83333         7\n",
      "           9    1.00000   0.92308   0.96000        13\n",
      "          10    1.00000   0.90909   0.95238        11\n",
      "          11    1.00000   1.00000   1.00000        10\n",
      "          12    1.00000   1.00000   1.00000         9\n",
      "          13    0.75000   0.85714   0.80000         7\n",
      "          14    0.27273   0.42857   0.33333         7\n",
      "\n",
      "    accuracy                        0.85987       157\n",
      "   macro avg    0.86550   0.85643   0.84622       157\n",
      "weighted avg    0.89338   0.85987   0.86222       157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Fake Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20,000 epochs DEFAULT PARAMETERS FOR THRESHOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (gen): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=108, out_features=80, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=80, out_features=60, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=60, out_features=50, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Linear(in_features=50, out_features=30, bias=True)\n",
       "    (4): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator(z_dim = 108)\n",
    "load_model(gen, \"3 Label 5 Subject GAN Ablation_gen.param\")\n",
    "gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(X_test)\n",
    "latent_vectors = get_noise(size, 100)\n",
    "act_vectors = get_act_matrix(size, 3)\n",
    "usr_vectors = get_usr_matrix(size, 5)\n",
    "\n",
    "fake_labels = []\n",
    "\n",
    "for k in range(size):\n",
    "    if act_vectors[0][k] == 0 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(0)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(1)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(2)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(3)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(4)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(5)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(6)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(7)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(8)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 3:\n",
    "        fake_labels.append(9)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 3:\n",
    "        fake_labels.append(10)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 3:\n",
    "        fake_labels.append(11)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 4:\n",
    "        fake_labels.append(12)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 4:\n",
    "        fake_labels.append(13)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 4:\n",
    "        fake_labels.append(14)\n",
    "        \n",
    "fake_labels = np.asarray(fake_labels)\n",
    "to_gen = torch.cat((latent_vectors, act_vectors[1], usr_vectors[1]), 1)\n",
    "fake_features = gen(to_gen).detach()\n",
    "#fake_features = gen(to_gen).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 11  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 13  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 16  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 19  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  0]\n",
      " [ 0  0  0  0  0  0  0  0 13  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 11  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  8  0  0  0  0  0]\n",
      " [ 0  7  0  0  1  0  0  0  0  0  2  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         8\n",
      "           1    0.63158   1.00000   0.77419        12\n",
      "           2    1.00000   1.00000   1.00000        11\n",
      "           3    1.00000   1.00000   1.00000        13\n",
      "           4    0.94118   1.00000   0.96970        16\n",
      "           5    1.00000   1.00000   1.00000        19\n",
      "           6    0.00000   0.00000   0.00000        10\n",
      "           7    0.00000   0.00000   0.00000         3\n",
      "           8    1.00000   1.00000   1.00000        13\n",
      "           9    0.37931   1.00000   0.55000        11\n",
      "          10    0.66667   1.00000   0.80000         4\n",
      "          11    1.00000   1.00000   1.00000         9\n",
      "          12    0.00000   0.00000   0.00000         8\n",
      "          13    0.00000   0.00000   0.00000        10\n",
      "          14    1.00000   1.00000   1.00000        10\n",
      "\n",
      "    accuracy                        0.80255       157\n",
      "   macro avg    0.64125   0.73333   0.67293       157\n",
      "weighted avg    0.71641   0.80255   0.74558       157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5, zero_division = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
