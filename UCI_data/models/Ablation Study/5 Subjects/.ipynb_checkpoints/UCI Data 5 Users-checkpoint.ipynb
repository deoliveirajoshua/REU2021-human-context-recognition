{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '90 tBodyAccJerk-max()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>42 tGravityAcc-mean()-Y</th>\n",
       "      <th>43 tGravityAcc-mean()-Z</th>\n",
       "      <th>51 tGravityAcc-max()-Y</th>\n",
       "      <th>52 tGravityAcc-max()-Z</th>\n",
       "      <th>54 tGravityAcc-min()-Y</th>\n",
       "      <th>55 tGravityAcc-min()-Z</th>\n",
       "      <th>56 tGravityAcc-sma()</th>\n",
       "      <th>58 tGravityAcc-energy()-Y</th>\n",
       "      <th>475 fBodyGyro-bandsEnergy()-1,8</th>\n",
       "      <th>483 fBodyGyro-bandsEnergy()-1,16</th>\n",
       "      <th>...</th>\n",
       "      <th>272 fBodyAcc-mad()-X</th>\n",
       "      <th>282 fBodyAcc-energy()-X</th>\n",
       "      <th>303 fBodyAcc-bandsEnergy()-1,8</th>\n",
       "      <th>311 fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <th>315 fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <th>504 fBodyAccMag-std()</th>\n",
       "      <th>505 fBodyAccMag-mad()</th>\n",
       "      <th>509 fBodyAccMag-energy()</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.140840</td>\n",
       "      <td>0.115375</td>\n",
       "      <td>-0.161265</td>\n",
       "      <td>0.124660</td>\n",
       "      <td>-0.123213</td>\n",
       "      <td>0.056483</td>\n",
       "      <td>-0.375426</td>\n",
       "      <td>-0.970905</td>\n",
       "      <td>-0.999454</td>\n",
       "      <td>-0.999619</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.996889</td>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-0.956134</td>\n",
       "      <td>-0.948870</td>\n",
       "      <td>-0.998285</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.141551</td>\n",
       "      <td>0.109379</td>\n",
       "      <td>-0.161343</td>\n",
       "      <td>0.122586</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.383430</td>\n",
       "      <td>-0.970583</td>\n",
       "      <td>-0.999856</td>\n",
       "      <td>-0.999897</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.997890</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.975866</td>\n",
       "      <td>-0.975777</td>\n",
       "      <td>-0.999472</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.142010</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.094566</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.401602</td>\n",
       "      <td>-0.970368</td>\n",
       "      <td>-0.999954</td>\n",
       "      <td>-0.999962</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.994097</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.999972</td>\n",
       "      <td>-0.989015</td>\n",
       "      <td>-0.985594</td>\n",
       "      <td>-0.999807</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.143976</td>\n",
       "      <td>0.099850</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.093425</td>\n",
       "      <td>-0.121336</td>\n",
       "      <td>0.095753</td>\n",
       "      <td>-0.400278</td>\n",
       "      <td>-0.969400</td>\n",
       "      <td>-0.999931</td>\n",
       "      <td>-0.999947</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.994547</td>\n",
       "      <td>-0.999975</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-0.986742</td>\n",
       "      <td>-0.983524</td>\n",
       "      <td>-0.999770</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.148750</td>\n",
       "      <td>0.094486</td>\n",
       "      <td>-0.166786</td>\n",
       "      <td>0.091682</td>\n",
       "      <td>-0.121834</td>\n",
       "      <td>0.094059</td>\n",
       "      <td>-0.400477</td>\n",
       "      <td>-0.967051</td>\n",
       "      <td>-0.999926</td>\n",
       "      <td>-0.999946</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.997725</td>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.990063</td>\n",
       "      <td>-0.992324</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>-0.222004</td>\n",
       "      <td>-0.039492</td>\n",
       "      <td>-0.214233</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.071977</td>\n",
       "      <td>-0.405132</td>\n",
       "      <td>-0.918375</td>\n",
       "      <td>-0.053258</td>\n",
       "      <td>-0.307101</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050748</td>\n",
       "      <td>-0.674230</td>\n",
       "      <td>-0.684177</td>\n",
       "      <td>-0.666429</td>\n",
       "      <td>-0.668164</td>\n",
       "      <td>-0.232600</td>\n",
       "      <td>-0.007392</td>\n",
       "      <td>-0.584282</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>-0.242054</td>\n",
       "      <td>-0.039863</td>\n",
       "      <td>-0.231477</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.358934</td>\n",
       "      <td>-0.902880</td>\n",
       "      <td>-0.029411</td>\n",
       "      <td>-0.286728</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177661</td>\n",
       "      <td>-0.705580</td>\n",
       "      <td>-0.726986</td>\n",
       "      <td>-0.704444</td>\n",
       "      <td>-0.705435</td>\n",
       "      <td>-0.275373</td>\n",
       "      <td>-0.172448</td>\n",
       "      <td>-0.632536</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>-0.236950</td>\n",
       "      <td>-0.026805</td>\n",
       "      <td>-0.249134</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.216004</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.377025</td>\n",
       "      <td>-0.907561</td>\n",
       "      <td>0.161404</td>\n",
       "      <td>-0.164197</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.249486</td>\n",
       "      <td>-0.692379</td>\n",
       "      <td>-0.655263</td>\n",
       "      <td>-0.674515</td>\n",
       "      <td>-0.684729</td>\n",
       "      <td>-0.220288</td>\n",
       "      <td>-0.216074</td>\n",
       "      <td>-0.641170</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>-0.233230</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>-0.244267</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.210542</td>\n",
       "      <td>-0.040009</td>\n",
       "      <td>-0.440050</td>\n",
       "      <td>-0.910648</td>\n",
       "      <td>0.193585</td>\n",
       "      <td>-0.155644</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.247028</td>\n",
       "      <td>-0.693098</td>\n",
       "      <td>-0.643425</td>\n",
       "      <td>-0.677215</td>\n",
       "      <td>-0.685088</td>\n",
       "      <td>-0.234539</td>\n",
       "      <td>-0.220443</td>\n",
       "      <td>-0.663579</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>-0.233292</td>\n",
       "      <td>-0.020954</td>\n",
       "      <td>-0.240956</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.212149</td>\n",
       "      <td>-0.047491</td>\n",
       "      <td>-0.432003</td>\n",
       "      <td>-0.910579</td>\n",
       "      <td>-0.129277</td>\n",
       "      <td>-0.384693</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114475</td>\n",
       "      <td>-0.731037</td>\n",
       "      <td>-0.709495</td>\n",
       "      <td>-0.728519</td>\n",
       "      <td>-0.727441</td>\n",
       "      <td>-0.342670</td>\n",
       "      <td>-0.146649</td>\n",
       "      <td>-0.698087</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      42 tGravityAcc-mean()-Y  43 tGravityAcc-mean()-Z  \\\n",
       "0                   -0.140840                 0.115375   \n",
       "1                   -0.141551                 0.109379   \n",
       "2                   -0.142010                 0.101884   \n",
       "3                   -0.143976                 0.099850   \n",
       "4                   -0.148750                 0.094486   \n",
       "...                       ...                      ...   \n",
       "7347                -0.222004                -0.039492   \n",
       "7348                -0.242054                -0.039863   \n",
       "7349                -0.236950                -0.026805   \n",
       "7350                -0.233230                -0.004984   \n",
       "7351                -0.233292                -0.020954   \n",
       "\n",
       "      51 tGravityAcc-max()-Y  52 tGravityAcc-max()-Z  54 tGravityAcc-min()-Y  \\\n",
       "0                  -0.161265                0.124660               -0.123213   \n",
       "1                  -0.161343                0.122586               -0.114893   \n",
       "2                  -0.163711                0.094566               -0.114893   \n",
       "3                  -0.163711                0.093425               -0.121336   \n",
       "4                  -0.166786                0.091682               -0.121834   \n",
       "...                      ...                     ...                     ...   \n",
       "7347               -0.214233               -0.016391               -0.234998   \n",
       "7348               -0.231477               -0.016391               -0.234998   \n",
       "7349               -0.249134                0.024684               -0.216004   \n",
       "7350               -0.244267                0.024684               -0.210542   \n",
       "7351               -0.240956                0.003031               -0.212149   \n",
       "\n",
       "      55 tGravityAcc-min()-Z  56 tGravityAcc-sma()  58 tGravityAcc-energy()-Y  \\\n",
       "0                   0.056483             -0.375426                  -0.970905   \n",
       "1                   0.102764             -0.383430                  -0.970583   \n",
       "2                   0.102764             -0.401602                  -0.970368   \n",
       "3                   0.095753             -0.400278                  -0.969400   \n",
       "4                   0.094059             -0.400477                  -0.967051   \n",
       "...                      ...                   ...                        ...   \n",
       "7347               -0.071977             -0.405132                  -0.918375   \n",
       "7348               -0.068919             -0.358934                  -0.902880   \n",
       "7349               -0.068919             -0.377025                  -0.907561   \n",
       "7350               -0.040009             -0.440050                  -0.910648   \n",
       "7351               -0.047491             -0.432003                  -0.910579   \n",
       "\n",
       "      475 fBodyGyro-bandsEnergy()-1,8  483 fBodyGyro-bandsEnergy()-1,16  ...  \\\n",
       "0                           -0.999454                         -0.999619  ...   \n",
       "1                           -0.999856                         -0.999897  ...   \n",
       "2                           -0.999954                         -0.999962  ...   \n",
       "3                           -0.999931                         -0.999947  ...   \n",
       "4                           -0.999926                         -0.999946  ...   \n",
       "...                               ...                               ...  ...   \n",
       "7347                        -0.053258                         -0.307101  ...   \n",
       "7348                        -0.029411                         -0.286728  ...   \n",
       "7349                         0.161404                         -0.164197  ...   \n",
       "7350                         0.193585                         -0.155644  ...   \n",
       "7351                        -0.129277                         -0.384693  ...   \n",
       "\n",
       "      272 fBodyAcc-mad()-X  282 fBodyAcc-energy()-X  \\\n",
       "0                -0.996889                -0.999968   \n",
       "1                -0.997890                -0.999991   \n",
       "2                -0.994097                -0.999969   \n",
       "3                -0.994547                -0.999975   \n",
       "4                -0.997725                -0.999990   \n",
       "...                    ...                      ...   \n",
       "7347             -0.050748                -0.674230   \n",
       "7348             -0.177661                -0.705580   \n",
       "7349             -0.249486                -0.692379   \n",
       "7350             -0.247028                -0.693098   \n",
       "7351             -0.114475                -0.731037   \n",
       "\n",
       "      303 fBodyAcc-bandsEnergy()-1,8  311 fBodyAcc-bandsEnergy()-1,16  \\\n",
       "0                          -0.999963                        -0.999969   \n",
       "1                          -0.999996                        -0.999994   \n",
       "2                          -0.999989                        -0.999983   \n",
       "3                          -0.999989                        -0.999986   \n",
       "4                          -0.999994                        -0.999993   \n",
       "...                              ...                              ...   \n",
       "7347                       -0.684177                        -0.666429   \n",
       "7348                       -0.726986                        -0.704444   \n",
       "7349                       -0.655263                        -0.674515   \n",
       "7350                       -0.643425                        -0.677215   \n",
       "7351                       -0.709495                        -0.728519   \n",
       "\n",
       "      315 fBodyAcc-bandsEnergy()-1,24  504 fBodyAccMag-std()  \\\n",
       "0                           -0.999971              -0.956134   \n",
       "1                           -0.999992              -0.975866   \n",
       "2                           -0.999972              -0.989015   \n",
       "3                           -0.999977              -0.986742   \n",
       "4                           -0.999991              -0.990063   \n",
       "...                               ...                    ...   \n",
       "7347                        -0.668164              -0.232600   \n",
       "7348                        -0.705435              -0.275373   \n",
       "7349                        -0.684729              -0.220288   \n",
       "7350                        -0.685088              -0.234539   \n",
       "7351                        -0.727441              -0.342670   \n",
       "\n",
       "      505 fBodyAccMag-mad()  509 fBodyAccMag-energy()  Subject  Activity  \n",
       "0                 -0.948870                 -0.998285        1         5  \n",
       "1                 -0.975777                 -0.999472        1         5  \n",
       "2                 -0.985594                 -0.999807        1         5  \n",
       "3                 -0.983524                 -0.999770        1         5  \n",
       "4                 -0.992324                 -0.999873        1         5  \n",
       "...                     ...                       ...      ...       ...  \n",
       "7347              -0.007392                 -0.584282       30         2  \n",
       "7348              -0.172448                 -0.632536       30         2  \n",
       "7349              -0.216074                 -0.641170       30         2  \n",
       "7350              -0.220443                 -0.663579       30         2  \n",
       "7351              -0.146649                 -0.698087       30         2  \n",
       "\n",
       "[7352 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_names = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "train_column_names = train_names.values.tolist()\n",
    "train_column_names = [k for row in train_column_names for k in row]\n",
    "\n",
    "train_data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None)\n",
    "train_data.columns = train_column_names\n",
    "\n",
    "### Single dataframe column\n",
    "\n",
    "y_train = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "y_train.columns = ['Subject']\n",
    "\n",
    "y_train_activity = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "y_train_activity.columns = ['Activity']\n",
    "\n",
    "X_train_1 = train_data[sub_features]\n",
    "X_train_2 = train_data[act_features]\n",
    "X_train_data = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "X_train_data = pd.concat([X_train_data, y_train, y_train_activity], axis = 1)\n",
    "X_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_data[(X_train_data['Subject'].isin([1, 3, 5, 7, 8])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,:-2].values\n",
    "y_train = X_train_data[(X_train_data['Subject'].isin([1, 3, 5, 7, 8])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,-2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y_train)):\n",
    "    if y_train[k] == 1:\n",
    "        y_train[k] = 0\n",
    "    elif y_train[k] == 3:\n",
    "        y_train[k] = 1\n",
    "    elif y_train[k] == 5:\n",
    "        y_train[k] = 2\n",
    "    elif y_train[k] == 7:\n",
    "        y_train[k] = 3\n",
    "    else:\n",
    "        y_train[k] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 30),\n",
    "            classifier_block(30, 20),\n",
    "            classifier_block(20, 20),\n",
    "            classifier_block(20, 10),\n",
    "            nn.Linear(10, 5)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 7500\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.901568055152893, Final Batch Loss: 1.6216896772384644\n",
      "Epoch 2, Loss: 4.9057323932647705, Final Batch Loss: 1.6585233211517334\n",
      "Epoch 3, Loss: 4.889178991317749, Final Batch Loss: 1.6292177438735962\n",
      "Epoch 4, Loss: 4.882572889328003, Final Batch Loss: 1.6380025148391724\n",
      "Epoch 5, Loss: 4.871217489242554, Final Batch Loss: 1.6309311389923096\n",
      "Epoch 6, Loss: 4.862881660461426, Final Batch Loss: 1.602850079536438\n",
      "Epoch 7, Loss: 4.8588597774505615, Final Batch Loss: 1.623079776763916\n",
      "Epoch 8, Loss: 4.847156524658203, Final Batch Loss: 1.5948899984359741\n",
      "Epoch 9, Loss: 4.859623551368713, Final Batch Loss: 1.627223253250122\n",
      "Epoch 10, Loss: 4.844614386558533, Final Batch Loss: 1.6101192235946655\n",
      "Epoch 11, Loss: 4.843320965766907, Final Batch Loss: 1.6028560400009155\n",
      "Epoch 12, Loss: 4.8378989696502686, Final Batch Loss: 1.6072643995285034\n",
      "Epoch 13, Loss: 4.841763377189636, Final Batch Loss: 1.6190848350524902\n",
      "Epoch 14, Loss: 4.840689182281494, Final Batch Loss: 1.6151691675186157\n",
      "Epoch 15, Loss: 4.830887079238892, Final Batch Loss: 1.5941848754882812\n",
      "Epoch 16, Loss: 4.837149381637573, Final Batch Loss: 1.6289010047912598\n",
      "Epoch 17, Loss: 4.834504723548889, Final Batch Loss: 1.6135197877883911\n",
      "Epoch 18, Loss: 4.827222108840942, Final Batch Loss: 1.6190780401229858\n",
      "Epoch 19, Loss: 4.823490738868713, Final Batch Loss: 1.6008261442184448\n",
      "Epoch 20, Loss: 4.8140764236450195, Final Batch Loss: 1.6102190017700195\n",
      "Epoch 21, Loss: 4.811922073364258, Final Batch Loss: 1.6121258735656738\n",
      "Epoch 22, Loss: 4.804661154747009, Final Batch Loss: 1.5995227098464966\n",
      "Epoch 23, Loss: 4.798040509223938, Final Batch Loss: 1.6029623746871948\n",
      "Epoch 24, Loss: 4.7821351289749146, Final Batch Loss: 1.5911567211151123\n",
      "Epoch 25, Loss: 4.781949996948242, Final Batch Loss: 1.5861172676086426\n",
      "Epoch 26, Loss: 4.767077326774597, Final Batch Loss: 1.589691400527954\n",
      "Epoch 27, Loss: 4.746425271034241, Final Batch Loss: 1.5705772638320923\n",
      "Epoch 28, Loss: 4.730269551277161, Final Batch Loss: 1.5704301595687866\n",
      "Epoch 29, Loss: 4.7099655866622925, Final Batch Loss: 1.568306565284729\n",
      "Epoch 30, Loss: 4.7109997272491455, Final Batch Loss: 1.5808519124984741\n",
      "Epoch 31, Loss: 4.678570985794067, Final Batch Loss: 1.5572443008422852\n",
      "Epoch 32, Loss: 4.637005686759949, Final Batch Loss: 1.5204120874404907\n",
      "Epoch 33, Loss: 4.6301246881484985, Final Batch Loss: 1.5512672662734985\n",
      "Epoch 34, Loss: 4.597790360450745, Final Batch Loss: 1.5255883932113647\n",
      "Epoch 35, Loss: 4.564792156219482, Final Batch Loss: 1.5158852338790894\n",
      "Epoch 36, Loss: 4.501461029052734, Final Batch Loss: 1.4841973781585693\n",
      "Epoch 37, Loss: 4.510951519012451, Final Batch Loss: 1.4965801239013672\n",
      "Epoch 38, Loss: 4.431566953659058, Final Batch Loss: 1.485478162765503\n",
      "Epoch 39, Loss: 4.475775957107544, Final Batch Loss: 1.4931145906448364\n",
      "Epoch 40, Loss: 4.392588376998901, Final Batch Loss: 1.421591877937317\n",
      "Epoch 41, Loss: 4.399067044258118, Final Batch Loss: 1.4734725952148438\n",
      "Epoch 42, Loss: 4.359201073646545, Final Batch Loss: 1.4567062854766846\n",
      "Epoch 43, Loss: 4.293843626976013, Final Batch Loss: 1.4448741674423218\n",
      "Epoch 44, Loss: 4.316326260566711, Final Batch Loss: 1.4308375120162964\n",
      "Epoch 45, Loss: 4.2910367250442505, Final Batch Loss: 1.4810079336166382\n",
      "Epoch 46, Loss: 4.229245901107788, Final Batch Loss: 1.4020320177078247\n",
      "Epoch 47, Loss: 4.224083185195923, Final Batch Loss: 1.4199072122573853\n",
      "Epoch 48, Loss: 4.2293455600738525, Final Batch Loss: 1.4525375366210938\n",
      "Epoch 49, Loss: 4.2178332805633545, Final Batch Loss: 1.3844499588012695\n",
      "Epoch 50, Loss: 4.143147349357605, Final Batch Loss: 1.4074560403823853\n",
      "Epoch 51, Loss: 4.129309773445129, Final Batch Loss: 1.347016453742981\n",
      "Epoch 52, Loss: 4.098948836326599, Final Batch Loss: 1.3499385118484497\n",
      "Epoch 53, Loss: 4.1103997230529785, Final Batch Loss: 1.4182802438735962\n",
      "Epoch 54, Loss: 4.08336329460144, Final Batch Loss: 1.3436145782470703\n",
      "Epoch 55, Loss: 4.058429718017578, Final Batch Loss: 1.3503985404968262\n",
      "Epoch 56, Loss: 4.019027948379517, Final Batch Loss: 1.3438405990600586\n",
      "Epoch 57, Loss: 4.018317818641663, Final Batch Loss: 1.3002139329910278\n",
      "Epoch 58, Loss: 3.9628894329071045, Final Batch Loss: 1.3769071102142334\n",
      "Epoch 59, Loss: 3.972688674926758, Final Batch Loss: 1.339949131011963\n",
      "Epoch 60, Loss: 3.880083680152893, Final Batch Loss: 1.2703546285629272\n",
      "Epoch 61, Loss: 3.8748804330825806, Final Batch Loss: 1.2309049367904663\n",
      "Epoch 62, Loss: 3.9276877641677856, Final Batch Loss: 1.2969510555267334\n",
      "Epoch 63, Loss: 3.899719476699829, Final Batch Loss: 1.3352636098861694\n",
      "Epoch 64, Loss: 3.849016785621643, Final Batch Loss: 1.2578279972076416\n",
      "Epoch 65, Loss: 3.9105799198150635, Final Batch Loss: 1.3544862270355225\n",
      "Epoch 66, Loss: 3.8493518829345703, Final Batch Loss: 1.30709707736969\n",
      "Epoch 67, Loss: 3.806484580039978, Final Batch Loss: 1.2445926666259766\n",
      "Epoch 68, Loss: 3.830007553100586, Final Batch Loss: 1.3398798704147339\n",
      "Epoch 69, Loss: 3.797982335090637, Final Batch Loss: 1.273192286491394\n",
      "Epoch 70, Loss: 3.7431788444519043, Final Batch Loss: 1.275036334991455\n",
      "Epoch 71, Loss: 3.773404598236084, Final Batch Loss: 1.2817432880401611\n",
      "Epoch 72, Loss: 3.7158998250961304, Final Batch Loss: 1.1916005611419678\n",
      "Epoch 73, Loss: 3.7222468852996826, Final Batch Loss: 1.2470017671585083\n",
      "Epoch 74, Loss: 3.737443685531616, Final Batch Loss: 1.2500325441360474\n",
      "Epoch 75, Loss: 3.633962035179138, Final Batch Loss: 1.2065043449401855\n",
      "Epoch 76, Loss: 3.5893067121505737, Final Batch Loss: 1.1469751596450806\n",
      "Epoch 77, Loss: 3.6196991205215454, Final Batch Loss: 1.1800339221954346\n",
      "Epoch 78, Loss: 3.5674655437469482, Final Batch Loss: 1.1948318481445312\n",
      "Epoch 79, Loss: 3.5819400548934937, Final Batch Loss: 1.0988800525665283\n",
      "Epoch 80, Loss: 3.5887116193771362, Final Batch Loss: 1.185874104499817\n",
      "Epoch 81, Loss: 3.583071231842041, Final Batch Loss: 1.1817524433135986\n",
      "Epoch 82, Loss: 3.5022298097610474, Final Batch Loss: 1.161409854888916\n",
      "Epoch 83, Loss: 3.495060443878174, Final Batch Loss: 1.1220492124557495\n",
      "Epoch 84, Loss: 3.4653875827789307, Final Batch Loss: 1.1371091604232788\n",
      "Epoch 85, Loss: 3.4815982580184937, Final Batch Loss: 1.191445231437683\n",
      "Epoch 86, Loss: 3.412493348121643, Final Batch Loss: 1.1455113887786865\n",
      "Epoch 87, Loss: 3.452854037284851, Final Batch Loss: 1.231406331062317\n",
      "Epoch 88, Loss: 3.435266375541687, Final Batch Loss: 1.188072681427002\n",
      "Epoch 89, Loss: 3.305096387863159, Final Batch Loss: 1.0970892906188965\n",
      "Epoch 90, Loss: 3.320290684700012, Final Batch Loss: 1.1355576515197754\n",
      "Epoch 91, Loss: 3.2439568042755127, Final Batch Loss: 1.0742599964141846\n",
      "Epoch 92, Loss: 3.257143259048462, Final Batch Loss: 1.1148234605789185\n",
      "Epoch 93, Loss: 3.2337130308151245, Final Batch Loss: 1.0842636823654175\n",
      "Epoch 94, Loss: 3.17559152841568, Final Batch Loss: 1.0962013006210327\n",
      "Epoch 95, Loss: 3.129663407802582, Final Batch Loss: 0.9784778952598572\n",
      "Epoch 96, Loss: 3.1002212166786194, Final Batch Loss: 0.9905326962471008\n",
      "Epoch 97, Loss: 3.08574777841568, Final Batch Loss: 0.9500269293785095\n",
      "Epoch 98, Loss: 3.0833210945129395, Final Batch Loss: 1.0601645708084106\n",
      "Epoch 99, Loss: 3.0093366503715515, Final Batch Loss: 0.9595887064933777\n",
      "Epoch 100, Loss: 3.0336338877677917, Final Batch Loss: 1.0692403316497803\n",
      "Epoch 101, Loss: 3.0087876319885254, Final Batch Loss: 1.0318251848220825\n",
      "Epoch 102, Loss: 2.87565016746521, Final Batch Loss: 0.9077361822128296\n",
      "Epoch 103, Loss: 2.9135934114456177, Final Batch Loss: 1.0266658067703247\n",
      "Epoch 104, Loss: 2.913235366344452, Final Batch Loss: 0.9928098320960999\n",
      "Epoch 105, Loss: 2.8264962434768677, Final Batch Loss: 0.854542076587677\n",
      "Epoch 106, Loss: 2.845102548599243, Final Batch Loss: 0.915177583694458\n",
      "Epoch 107, Loss: 2.882560670375824, Final Batch Loss: 1.0115329027175903\n",
      "Epoch 108, Loss: 2.842811644077301, Final Batch Loss: 0.9644243717193604\n",
      "Epoch 109, Loss: 2.790425658226013, Final Batch Loss: 0.9269335865974426\n",
      "Epoch 110, Loss: 2.835903227329254, Final Batch Loss: 0.968525230884552\n",
      "Epoch 111, Loss: 2.9362385272979736, Final Batch Loss: 1.070320725440979\n",
      "Epoch 112, Loss: 2.7087690830230713, Final Batch Loss: 0.8965150117874146\n",
      "Epoch 113, Loss: 2.6820638179779053, Final Batch Loss: 0.867294430732727\n",
      "Epoch 114, Loss: 2.7554927468299866, Final Batch Loss: 0.9634683728218079\n",
      "Epoch 115, Loss: 2.7142622470855713, Final Batch Loss: 0.8515249490737915\n",
      "Epoch 116, Loss: 2.6659218072891235, Final Batch Loss: 0.8916159868240356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117, Loss: 2.682243525981903, Final Batch Loss: 0.8801479935646057\n",
      "Epoch 118, Loss: 2.699390709400177, Final Batch Loss: 0.9007763266563416\n",
      "Epoch 119, Loss: 2.642258882522583, Final Batch Loss: 0.9156909584999084\n",
      "Epoch 120, Loss: 2.732376754283905, Final Batch Loss: 0.9401359558105469\n",
      "Epoch 121, Loss: 2.6884477734565735, Final Batch Loss: 0.9654163718223572\n",
      "Epoch 122, Loss: 2.6603047847747803, Final Batch Loss: 0.8705951571464539\n",
      "Epoch 123, Loss: 2.6441264152526855, Final Batch Loss: 0.8912725448608398\n",
      "Epoch 124, Loss: 2.551886796951294, Final Batch Loss: 0.8756913542747498\n",
      "Epoch 125, Loss: 2.5871490240097046, Final Batch Loss: 0.8637918829917908\n",
      "Epoch 126, Loss: 2.64469051361084, Final Batch Loss: 0.917938232421875\n",
      "Epoch 127, Loss: 2.513135612010956, Final Batch Loss: 0.8211983442306519\n",
      "Epoch 128, Loss: 2.6586586236953735, Final Batch Loss: 0.9673818349838257\n",
      "Epoch 129, Loss: 2.58806836605072, Final Batch Loss: 0.8297412991523743\n",
      "Epoch 130, Loss: 2.4445075392723083, Final Batch Loss: 0.7649651169776917\n",
      "Epoch 131, Loss: 2.5482720732688904, Final Batch Loss: 0.7936926484107971\n",
      "Epoch 132, Loss: 2.4693689942359924, Final Batch Loss: 0.8744864463806152\n",
      "Epoch 133, Loss: 2.5069071650505066, Final Batch Loss: 0.8265739679336548\n",
      "Epoch 134, Loss: 2.5122532844543457, Final Batch Loss: 0.899147629737854\n",
      "Epoch 135, Loss: 2.4281702041625977, Final Batch Loss: 0.8258890509605408\n",
      "Epoch 136, Loss: 2.440133512020111, Final Batch Loss: 0.7065792679786682\n",
      "Epoch 137, Loss: 2.4768521785736084, Final Batch Loss: 0.7787450551986694\n",
      "Epoch 138, Loss: 2.4899078607559204, Final Batch Loss: 0.7708356380462646\n",
      "Epoch 139, Loss: 2.5136391520500183, Final Batch Loss: 0.8668747544288635\n",
      "Epoch 140, Loss: 2.4245612025260925, Final Batch Loss: 0.7509909868240356\n",
      "Epoch 141, Loss: 2.465456962585449, Final Batch Loss: 0.8072341084480286\n",
      "Epoch 142, Loss: 2.434842526912689, Final Batch Loss: 0.9063634872436523\n",
      "Epoch 143, Loss: 2.4098910093307495, Final Batch Loss: 0.8024263978004456\n",
      "Epoch 144, Loss: 2.450884222984314, Final Batch Loss: 0.7980269193649292\n",
      "Epoch 145, Loss: 2.4534688591957092, Final Batch Loss: 0.8288080096244812\n",
      "Epoch 146, Loss: 2.4046823978424072, Final Batch Loss: 0.8253593444824219\n",
      "Epoch 147, Loss: 2.404461622238159, Final Batch Loss: 0.8111700415611267\n",
      "Epoch 148, Loss: 2.416470944881439, Final Batch Loss: 0.7771833539009094\n",
      "Epoch 149, Loss: 2.398081600666046, Final Batch Loss: 0.8527935743331909\n",
      "Epoch 150, Loss: 2.4296790957450867, Final Batch Loss: 0.8354756832122803\n",
      "Epoch 151, Loss: 2.371742844581604, Final Batch Loss: 0.7544437646865845\n",
      "Epoch 152, Loss: 2.4130155444145203, Final Batch Loss: 0.8312640190124512\n",
      "Epoch 153, Loss: 2.3807075023651123, Final Batch Loss: 0.7872099876403809\n",
      "Epoch 154, Loss: 2.270480453968048, Final Batch Loss: 0.6912691593170166\n",
      "Epoch 155, Loss: 2.3937103748321533, Final Batch Loss: 0.7733736634254456\n",
      "Epoch 156, Loss: 2.431611955165863, Final Batch Loss: 0.7450152635574341\n",
      "Epoch 157, Loss: 2.384193539619446, Final Batch Loss: 0.8073033094406128\n",
      "Epoch 158, Loss: 2.4079551100730896, Final Batch Loss: 0.866756796836853\n",
      "Epoch 159, Loss: 2.3575724959373474, Final Batch Loss: 0.8171582221984863\n",
      "Epoch 160, Loss: 2.262792944908142, Final Batch Loss: 0.729308009147644\n",
      "Epoch 161, Loss: 2.2893452644348145, Final Batch Loss: 0.7041370868682861\n",
      "Epoch 162, Loss: 2.3572256565093994, Final Batch Loss: 0.7680383324623108\n",
      "Epoch 163, Loss: 2.247401773929596, Final Batch Loss: 0.7558037042617798\n",
      "Epoch 164, Loss: 2.2933225631713867, Final Batch Loss: 0.7532498240470886\n",
      "Epoch 165, Loss: 2.335166335105896, Final Batch Loss: 0.8556192517280579\n",
      "Epoch 166, Loss: 2.255917549133301, Final Batch Loss: 0.7518226504325867\n",
      "Epoch 167, Loss: 2.1861338019371033, Final Batch Loss: 0.7482827305793762\n",
      "Epoch 168, Loss: 2.2855712175369263, Final Batch Loss: 0.7475049495697021\n",
      "Epoch 169, Loss: 2.224226653575897, Final Batch Loss: 0.6837366223335266\n",
      "Epoch 170, Loss: 2.175561487674713, Final Batch Loss: 0.7634990811347961\n",
      "Epoch 171, Loss: 2.199548900127411, Final Batch Loss: 0.6400047540664673\n",
      "Epoch 172, Loss: 2.200652003288269, Final Batch Loss: 0.7458856701850891\n",
      "Epoch 173, Loss: 2.246151030063629, Final Batch Loss: 0.7293494343757629\n",
      "Epoch 174, Loss: 2.2655426263809204, Final Batch Loss: 0.7195808291435242\n",
      "Epoch 175, Loss: 2.177382528781891, Final Batch Loss: 0.7541248798370361\n",
      "Epoch 176, Loss: 2.18049019575119, Final Batch Loss: 0.7863562703132629\n",
      "Epoch 177, Loss: 2.155536413192749, Final Batch Loss: 0.6988238096237183\n",
      "Epoch 178, Loss: 2.1502405405044556, Final Batch Loss: 0.6846389174461365\n",
      "Epoch 179, Loss: 2.193365752696991, Final Batch Loss: 0.7522315979003906\n",
      "Epoch 180, Loss: 2.205804705619812, Final Batch Loss: 0.696787416934967\n",
      "Epoch 181, Loss: 2.1769379377365112, Final Batch Loss: 0.7644755244255066\n",
      "Epoch 182, Loss: 2.1814749240875244, Final Batch Loss: 0.743168294429779\n",
      "Epoch 183, Loss: 2.1518837809562683, Final Batch Loss: 0.7297088503837585\n",
      "Epoch 184, Loss: 2.1502519249916077, Final Batch Loss: 0.7040860652923584\n",
      "Epoch 185, Loss: 2.1405030488967896, Final Batch Loss: 0.6986392140388489\n",
      "Epoch 186, Loss: 2.1219103932380676, Final Batch Loss: 0.7188939452171326\n",
      "Epoch 187, Loss: 2.1761963963508606, Final Batch Loss: 0.6856985092163086\n",
      "Epoch 188, Loss: 2.1304051876068115, Final Batch Loss: 0.645374059677124\n",
      "Epoch 189, Loss: 2.1048598289489746, Final Batch Loss: 0.7563772797584534\n",
      "Epoch 190, Loss: 2.016723930835724, Final Batch Loss: 0.6380059123039246\n",
      "Epoch 191, Loss: 2.076864242553711, Final Batch Loss: 0.621548593044281\n",
      "Epoch 192, Loss: 2.094779849052429, Final Batch Loss: 0.7417937517166138\n",
      "Epoch 193, Loss: 2.1475778818130493, Final Batch Loss: 0.7105655074119568\n",
      "Epoch 194, Loss: 2.055076837539673, Final Batch Loss: 0.7299845814704895\n",
      "Epoch 195, Loss: 2.1767261624336243, Final Batch Loss: 0.7840197682380676\n",
      "Epoch 196, Loss: 2.0156403183937073, Final Batch Loss: 0.6368964910507202\n",
      "Epoch 197, Loss: 2.0376172065734863, Final Batch Loss: 0.6674479842185974\n",
      "Epoch 198, Loss: 2.0210965871810913, Final Batch Loss: 0.6451096534729004\n",
      "Epoch 199, Loss: 2.1264942288398743, Final Batch Loss: 0.7464330196380615\n",
      "Epoch 200, Loss: 2.0360613465309143, Final Batch Loss: 0.6579955220222473\n",
      "Epoch 201, Loss: 2.0685372948646545, Final Batch Loss: 0.7131890654563904\n",
      "Epoch 202, Loss: 2.1039764881134033, Final Batch Loss: 0.6379842758178711\n",
      "Epoch 203, Loss: 2.001360058784485, Final Batch Loss: 0.6342706680297852\n",
      "Epoch 204, Loss: 2.0445777773857117, Final Batch Loss: 0.6697491407394409\n",
      "Epoch 205, Loss: 1.9386725425720215, Final Batch Loss: 0.5801092982292175\n",
      "Epoch 206, Loss: 1.9346529245376587, Final Batch Loss: 0.5826436877250671\n",
      "Epoch 207, Loss: 1.963175117969513, Final Batch Loss: 0.6044297218322754\n",
      "Epoch 208, Loss: 2.005097210407257, Final Batch Loss: 0.667328417301178\n",
      "Epoch 209, Loss: 1.9887035489082336, Final Batch Loss: 0.6562354564666748\n",
      "Epoch 210, Loss: 1.9218953251838684, Final Batch Loss: 0.7109016180038452\n",
      "Epoch 211, Loss: 1.9437673687934875, Final Batch Loss: 0.6693457961082458\n",
      "Epoch 212, Loss: 1.961115539073944, Final Batch Loss: 0.7062856554985046\n",
      "Epoch 213, Loss: 1.9289674162864685, Final Batch Loss: 0.7447630763053894\n",
      "Epoch 214, Loss: 1.931133508682251, Final Batch Loss: 0.5921505689620972\n",
      "Epoch 215, Loss: 1.9856131672859192, Final Batch Loss: 0.6649153828620911\n",
      "Epoch 216, Loss: 1.9054735898971558, Final Batch Loss: 0.6260514259338379\n",
      "Epoch 217, Loss: 1.9131260514259338, Final Batch Loss: 0.6708217263221741\n",
      "Epoch 218, Loss: 1.9310484528541565, Final Batch Loss: 0.6092284321784973\n",
      "Epoch 219, Loss: 1.9060183763504028, Final Batch Loss: 0.5555102229118347\n",
      "Epoch 220, Loss: 1.8934470415115356, Final Batch Loss: 0.5677422881126404\n",
      "Epoch 221, Loss: 1.8425865769386292, Final Batch Loss: 0.6479207277297974\n",
      "Epoch 222, Loss: 1.9050800204277039, Final Batch Loss: 0.6611242890357971\n",
      "Epoch 223, Loss: 1.8364102840423584, Final Batch Loss: 0.5108707547187805\n",
      "Epoch 224, Loss: 1.7997516989707947, Final Batch Loss: 0.6023957133293152\n",
      "Epoch 225, Loss: 1.8198122382164001, Final Batch Loss: 0.5944736003875732\n",
      "Epoch 226, Loss: 1.7989159226417542, Final Batch Loss: 0.5491337776184082\n",
      "Epoch 227, Loss: 1.9627725481987, Final Batch Loss: 0.7162818908691406\n",
      "Epoch 228, Loss: 1.9043081402778625, Final Batch Loss: 0.6858964562416077\n",
      "Epoch 229, Loss: 1.9271160960197449, Final Batch Loss: 0.6905250549316406\n",
      "Epoch 230, Loss: 1.734679400920868, Final Batch Loss: 0.6123339533805847\n",
      "Epoch 231, Loss: 1.8074034452438354, Final Batch Loss: 0.5610448718070984\n",
      "Epoch 232, Loss: 1.8982038497924805, Final Batch Loss: 0.6033419966697693\n",
      "Epoch 233, Loss: 1.8265214562416077, Final Batch Loss: 0.6100316643714905\n",
      "Epoch 234, Loss: 1.7959169745445251, Final Batch Loss: 0.6463004946708679\n",
      "Epoch 235, Loss: 1.8359829187393188, Final Batch Loss: 0.6845901012420654\n",
      "Epoch 236, Loss: 1.8445228934288025, Final Batch Loss: 0.5924954414367676\n",
      "Epoch 237, Loss: 1.8463759422302246, Final Batch Loss: 0.6701188683509827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238, Loss: 1.802367925643921, Final Batch Loss: 0.6153924465179443\n",
      "Epoch 239, Loss: 1.8050596714019775, Final Batch Loss: 0.6184664368629456\n",
      "Epoch 240, Loss: 1.6862937808036804, Final Batch Loss: 0.5218042731285095\n",
      "Epoch 241, Loss: 1.7541447281837463, Final Batch Loss: 0.5904802083969116\n",
      "Epoch 242, Loss: 1.8352892994880676, Final Batch Loss: 0.5614187121391296\n",
      "Epoch 243, Loss: 1.723652720451355, Final Batch Loss: 0.5710246562957764\n",
      "Epoch 244, Loss: 1.7119999527931213, Final Batch Loss: 0.5188656449317932\n",
      "Epoch 245, Loss: 1.7001781165599823, Final Batch Loss: 0.48970451951026917\n",
      "Epoch 246, Loss: 1.6680594086647034, Final Batch Loss: 0.5188875198364258\n",
      "Epoch 247, Loss: 1.6027894020080566, Final Batch Loss: 0.525425374507904\n",
      "Epoch 248, Loss: 1.690609872341156, Final Batch Loss: 0.5018102526664734\n",
      "Epoch 249, Loss: 1.7736746072769165, Final Batch Loss: 0.6271088719367981\n",
      "Epoch 250, Loss: 1.7160799503326416, Final Batch Loss: 0.6262131929397583\n",
      "Epoch 251, Loss: 1.665577471256256, Final Batch Loss: 0.5934495329856873\n",
      "Epoch 252, Loss: 1.6082226037979126, Final Batch Loss: 0.5150811076164246\n",
      "Epoch 253, Loss: 1.756216585636139, Final Batch Loss: 0.574543297290802\n",
      "Epoch 254, Loss: 1.6316975355148315, Final Batch Loss: 0.5303322672843933\n",
      "Epoch 255, Loss: 1.7578412294387817, Final Batch Loss: 0.5723161697387695\n",
      "Epoch 256, Loss: 1.725369930267334, Final Batch Loss: 0.5939822793006897\n",
      "Epoch 257, Loss: 1.6112843453884125, Final Batch Loss: 0.5818055272102356\n",
      "Epoch 258, Loss: 1.5602892637252808, Final Batch Loss: 0.4844517111778259\n",
      "Epoch 259, Loss: 1.6793695092201233, Final Batch Loss: 0.6045880913734436\n",
      "Epoch 260, Loss: 1.6898372769355774, Final Batch Loss: 0.5556403398513794\n",
      "Epoch 261, Loss: 1.566453367471695, Final Batch Loss: 0.4728626310825348\n",
      "Epoch 262, Loss: 1.6859592199325562, Final Batch Loss: 0.6289694905281067\n",
      "Epoch 263, Loss: 1.572867900133133, Final Batch Loss: 0.5501205921173096\n",
      "Epoch 264, Loss: 1.6221436262130737, Final Batch Loss: 0.5022887587547302\n",
      "Epoch 265, Loss: 1.7714754939079285, Final Batch Loss: 0.6042132377624512\n",
      "Epoch 266, Loss: 1.6719582080841064, Final Batch Loss: 0.5053793787956238\n",
      "Epoch 267, Loss: 1.72605299949646, Final Batch Loss: 0.5366855263710022\n",
      "Epoch 268, Loss: 1.6993723511695862, Final Batch Loss: 0.5382750034332275\n",
      "Epoch 269, Loss: 1.6521536111831665, Final Batch Loss: 0.55954909324646\n",
      "Epoch 270, Loss: 1.6846494674682617, Final Batch Loss: 0.5302183032035828\n",
      "Epoch 271, Loss: 1.6751821637153625, Final Batch Loss: 0.5434592366218567\n",
      "Epoch 272, Loss: 1.6648167371749878, Final Batch Loss: 0.5284752249717712\n",
      "Epoch 273, Loss: 1.621831476688385, Final Batch Loss: 0.579562246799469\n",
      "Epoch 274, Loss: 1.6299676299095154, Final Batch Loss: 0.5960651636123657\n",
      "Epoch 275, Loss: 1.6670390367507935, Final Batch Loss: 0.5736194252967834\n",
      "Epoch 276, Loss: 1.5703813135623932, Final Batch Loss: 0.4943796694278717\n",
      "Epoch 277, Loss: 1.6589085757732391, Final Batch Loss: 0.5336503386497498\n",
      "Epoch 278, Loss: 1.6446866393089294, Final Batch Loss: 0.5183511972427368\n",
      "Epoch 279, Loss: 1.6488450169563293, Final Batch Loss: 0.5395147204399109\n",
      "Epoch 280, Loss: 1.6032306551933289, Final Batch Loss: 0.5086624622344971\n",
      "Epoch 281, Loss: 1.6346880793571472, Final Batch Loss: 0.5403085350990295\n",
      "Epoch 282, Loss: 1.6073141694068909, Final Batch Loss: 0.5310415625572205\n",
      "Epoch 283, Loss: 1.6346091628074646, Final Batch Loss: 0.4790475368499756\n",
      "Epoch 284, Loss: 1.6510823965072632, Final Batch Loss: 0.5568909645080566\n",
      "Epoch 285, Loss: 1.5459312498569489, Final Batch Loss: 0.4820317029953003\n",
      "Epoch 286, Loss: 1.6614221930503845, Final Batch Loss: 0.5016408562660217\n",
      "Epoch 287, Loss: 1.6346886456012726, Final Batch Loss: 0.4880295693874359\n",
      "Epoch 288, Loss: 1.5943273305892944, Final Batch Loss: 0.5893155336380005\n",
      "Epoch 289, Loss: 1.5255950689315796, Final Batch Loss: 0.5211455225944519\n",
      "Epoch 290, Loss: 1.5100453197956085, Final Batch Loss: 0.5227538347244263\n",
      "Epoch 291, Loss: 1.500402569770813, Final Batch Loss: 0.4308260679244995\n",
      "Epoch 292, Loss: 1.5764974653720856, Final Batch Loss: 0.5215650796890259\n",
      "Epoch 293, Loss: 1.6491810977458954, Final Batch Loss: 0.5441781878471375\n",
      "Epoch 294, Loss: 1.595774233341217, Final Batch Loss: 0.5575149655342102\n",
      "Epoch 295, Loss: 1.5478968918323517, Final Batch Loss: 0.5753725171089172\n",
      "Epoch 296, Loss: 1.5878914296627045, Final Batch Loss: 0.5571368336677551\n",
      "Epoch 297, Loss: 1.5539324581623077, Final Batch Loss: 0.47681212425231934\n",
      "Epoch 298, Loss: 1.56144118309021, Final Batch Loss: 0.4816654920578003\n",
      "Epoch 299, Loss: 1.5272629261016846, Final Batch Loss: 0.4919334948062897\n",
      "Epoch 300, Loss: 1.5691656470298767, Final Batch Loss: 0.5488569736480713\n",
      "Epoch 301, Loss: 1.5996975600719452, Final Batch Loss: 0.5645827054977417\n",
      "Epoch 302, Loss: 1.5009953677654266, Final Batch Loss: 0.4857332110404968\n",
      "Epoch 303, Loss: 1.4836836159229279, Final Batch Loss: 0.43598946928977966\n",
      "Epoch 304, Loss: 1.4703081250190735, Final Batch Loss: 0.45267754793167114\n",
      "Epoch 305, Loss: 1.5701924860477448, Final Batch Loss: 0.4510023891925812\n",
      "Epoch 306, Loss: 1.4065817296504974, Final Batch Loss: 0.4501434862613678\n",
      "Epoch 307, Loss: 1.5330659449100494, Final Batch Loss: 0.49463340640068054\n",
      "Epoch 308, Loss: 1.561966896057129, Final Batch Loss: 0.5656449198722839\n",
      "Epoch 309, Loss: 1.665003478527069, Final Batch Loss: 0.5340290069580078\n",
      "Epoch 310, Loss: 1.5692801475524902, Final Batch Loss: 0.5152896642684937\n",
      "Epoch 311, Loss: 1.5170223712921143, Final Batch Loss: 0.49012577533721924\n",
      "Epoch 312, Loss: 1.56788370013237, Final Batch Loss: 0.6174566149711609\n",
      "Epoch 313, Loss: 1.4738818109035492, Final Batch Loss: 0.4698249101638794\n",
      "Epoch 314, Loss: 1.572108805179596, Final Batch Loss: 0.5281910300254822\n",
      "Epoch 315, Loss: 1.6405883133411407, Final Batch Loss: 0.6686322093009949\n",
      "Epoch 316, Loss: 1.5195652842521667, Final Batch Loss: 0.5103848576545715\n",
      "Epoch 317, Loss: 1.4963308572769165, Final Batch Loss: 0.45023322105407715\n",
      "Epoch 318, Loss: 1.4862544536590576, Final Batch Loss: 0.5366265773773193\n",
      "Epoch 319, Loss: 1.5219744145870209, Final Batch Loss: 0.5105652809143066\n",
      "Epoch 320, Loss: 1.5939199924468994, Final Batch Loss: 0.5449999570846558\n",
      "Epoch 321, Loss: 1.437933474779129, Final Batch Loss: 0.4711206257343292\n",
      "Epoch 322, Loss: 1.3901058435440063, Final Batch Loss: 0.4414058327674866\n",
      "Epoch 323, Loss: 1.5471519827842712, Final Batch Loss: 0.5619844198226929\n",
      "Epoch 324, Loss: 1.591834008693695, Final Batch Loss: 0.5264770984649658\n",
      "Epoch 325, Loss: 1.5339501202106476, Final Batch Loss: 0.4843895137310028\n",
      "Epoch 326, Loss: 1.532946139574051, Final Batch Loss: 0.4497201442718506\n",
      "Epoch 327, Loss: 1.4915655255317688, Final Batch Loss: 0.4948309361934662\n",
      "Epoch 328, Loss: 1.4610298871994019, Final Batch Loss: 0.48749351501464844\n",
      "Epoch 329, Loss: 1.488749384880066, Final Batch Loss: 0.47605443000793457\n",
      "Epoch 330, Loss: 1.3901031911373138, Final Batch Loss: 0.4494571387767792\n",
      "Epoch 331, Loss: 1.4881232678890228, Final Batch Loss: 0.5279770493507385\n",
      "Epoch 332, Loss: 1.4414390623569489, Final Batch Loss: 0.47254428267478943\n",
      "Epoch 333, Loss: 1.5147781372070312, Final Batch Loss: 0.49395644664764404\n",
      "Epoch 334, Loss: 1.4963966012001038, Final Batch Loss: 0.5134841799736023\n",
      "Epoch 335, Loss: 1.4570491313934326, Final Batch Loss: 0.4329153895378113\n",
      "Epoch 336, Loss: 1.5333071649074554, Final Batch Loss: 0.5867112278938293\n",
      "Epoch 337, Loss: 1.417478084564209, Final Batch Loss: 0.4968392252922058\n",
      "Epoch 338, Loss: 1.4963151216506958, Final Batch Loss: 0.5028643012046814\n",
      "Epoch 339, Loss: 1.503777951002121, Final Batch Loss: 0.50506192445755\n",
      "Epoch 340, Loss: 1.4858635663986206, Final Batch Loss: 0.4828892946243286\n",
      "Epoch 341, Loss: 1.3537809252738953, Final Batch Loss: 0.38893797993659973\n",
      "Epoch 342, Loss: 1.485548734664917, Final Batch Loss: 0.4548700749874115\n",
      "Epoch 343, Loss: 1.5345434844493866, Final Batch Loss: 0.448848694562912\n",
      "Epoch 344, Loss: 1.4086208045482635, Final Batch Loss: 0.46567562222480774\n",
      "Epoch 345, Loss: 1.50171560049057, Final Batch Loss: 0.5995177030563354\n",
      "Epoch 346, Loss: 1.428942620754242, Final Batch Loss: 0.5676118731498718\n",
      "Epoch 347, Loss: 1.427463322877884, Final Batch Loss: 0.4744701683521271\n",
      "Epoch 348, Loss: 1.39760422706604, Final Batch Loss: 0.4736919701099396\n",
      "Epoch 349, Loss: 1.3817000687122345, Final Batch Loss: 0.4587380290031433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350, Loss: 1.4373565018177032, Final Batch Loss: 0.44295641779899597\n",
      "Epoch 351, Loss: 1.398557186126709, Final Batch Loss: 0.4735576808452606\n",
      "Epoch 352, Loss: 1.3773577213287354, Final Batch Loss: 0.44377079606056213\n",
      "Epoch 353, Loss: 1.4509322345256805, Final Batch Loss: 0.4746485650539398\n",
      "Epoch 354, Loss: 1.490724802017212, Final Batch Loss: 0.4596511721611023\n",
      "Epoch 355, Loss: 1.4248591661453247, Final Batch Loss: 0.4229174554347992\n",
      "Epoch 356, Loss: 1.5103453695774078, Final Batch Loss: 0.484640896320343\n",
      "Epoch 357, Loss: 1.4165081679821014, Final Batch Loss: 0.41348695755004883\n",
      "Epoch 358, Loss: 1.4484745562076569, Final Batch Loss: 0.5045866966247559\n",
      "Epoch 359, Loss: 1.4773278832435608, Final Batch Loss: 0.5152537822723389\n",
      "Epoch 360, Loss: 1.4410142600536346, Final Batch Loss: 0.4688650071620941\n",
      "Epoch 361, Loss: 1.410400778055191, Final Batch Loss: 0.4825444221496582\n",
      "Epoch 362, Loss: 1.5225830972194672, Final Batch Loss: 0.5209389328956604\n",
      "Epoch 363, Loss: 1.3476468920707703, Final Batch Loss: 0.40502217411994934\n",
      "Epoch 364, Loss: 1.4083870649337769, Final Batch Loss: 0.45653435587882996\n",
      "Epoch 365, Loss: 1.4786346554756165, Final Batch Loss: 0.5287450551986694\n",
      "Epoch 366, Loss: 1.5588922798633575, Final Batch Loss: 0.5756227970123291\n",
      "Epoch 367, Loss: 1.4633421301841736, Final Batch Loss: 0.5522220134735107\n",
      "Epoch 368, Loss: 1.491523802280426, Final Batch Loss: 0.5145735144615173\n",
      "Epoch 369, Loss: 1.4878452122211456, Final Batch Loss: 0.5112190842628479\n",
      "Epoch 370, Loss: 1.4283856749534607, Final Batch Loss: 0.5420373678207397\n",
      "Epoch 371, Loss: 1.4160901010036469, Final Batch Loss: 0.4215714633464813\n",
      "Epoch 372, Loss: 1.4615370333194733, Final Batch Loss: 0.5274413228034973\n",
      "Epoch 373, Loss: 1.2896566689014435, Final Batch Loss: 0.4010789692401886\n",
      "Epoch 374, Loss: 1.3487157225608826, Final Batch Loss: 0.4311216473579407\n",
      "Epoch 375, Loss: 1.4366772174835205, Final Batch Loss: 0.47339022159576416\n",
      "Epoch 376, Loss: 1.3646589517593384, Final Batch Loss: 0.45916318893432617\n",
      "Epoch 377, Loss: 1.449792206287384, Final Batch Loss: 0.44676095247268677\n",
      "Epoch 378, Loss: 1.427302211523056, Final Batch Loss: 0.5312000513076782\n",
      "Epoch 379, Loss: 1.420650601387024, Final Batch Loss: 0.45625343918800354\n",
      "Epoch 380, Loss: 1.4649147689342499, Final Batch Loss: 0.41005557775497437\n",
      "Epoch 381, Loss: 1.412354737520218, Final Batch Loss: 0.46903812885284424\n",
      "Epoch 382, Loss: 1.3705491721630096, Final Batch Loss: 0.40440279245376587\n",
      "Epoch 383, Loss: 1.3611911237239838, Final Batch Loss: 0.5071188807487488\n",
      "Epoch 384, Loss: 1.4327830076217651, Final Batch Loss: 0.49490711092948914\n",
      "Epoch 385, Loss: 1.4868426322937012, Final Batch Loss: 0.6049549579620361\n",
      "Epoch 386, Loss: 1.375661164522171, Final Batch Loss: 0.4885061979293823\n",
      "Epoch 387, Loss: 1.38409885764122, Final Batch Loss: 0.4834927022457123\n",
      "Epoch 388, Loss: 1.3260496258735657, Final Batch Loss: 0.3843686282634735\n",
      "Epoch 389, Loss: 1.4326358139514923, Final Batch Loss: 0.5186023116111755\n",
      "Epoch 390, Loss: 1.3976662755012512, Final Batch Loss: 0.38133013248443604\n",
      "Epoch 391, Loss: 1.373024046421051, Final Batch Loss: 0.4903504550457001\n",
      "Epoch 392, Loss: 1.3441712856292725, Final Batch Loss: 0.4341294467449188\n",
      "Epoch 393, Loss: 1.3943485021591187, Final Batch Loss: 0.4748416841030121\n",
      "Epoch 394, Loss: 1.4297311305999756, Final Batch Loss: 0.5469483733177185\n",
      "Epoch 395, Loss: 1.505157709121704, Final Batch Loss: 0.6164728403091431\n",
      "Epoch 396, Loss: 1.4053589403629303, Final Batch Loss: 0.47802358865737915\n",
      "Epoch 397, Loss: 1.3537494838237762, Final Batch Loss: 0.46883726119995117\n",
      "Epoch 398, Loss: 1.5171427130699158, Final Batch Loss: 0.5076623558998108\n",
      "Epoch 399, Loss: 1.3720589578151703, Final Batch Loss: 0.4194962680339813\n",
      "Epoch 400, Loss: 1.371913731098175, Final Batch Loss: 0.44745054841041565\n",
      "Epoch 401, Loss: 1.3000479936599731, Final Batch Loss: 0.4117947518825531\n",
      "Epoch 402, Loss: 1.4168325662612915, Final Batch Loss: 0.5104235410690308\n",
      "Epoch 403, Loss: 1.3703339099884033, Final Batch Loss: 0.48989811539649963\n",
      "Epoch 404, Loss: 1.3087651133537292, Final Batch Loss: 0.41349753737449646\n",
      "Epoch 405, Loss: 1.3387160003185272, Final Batch Loss: 0.4365033209323883\n",
      "Epoch 406, Loss: 1.2822076678276062, Final Batch Loss: 0.430328369140625\n",
      "Epoch 407, Loss: 1.377361536026001, Final Batch Loss: 0.39143019914627075\n",
      "Epoch 408, Loss: 1.4040833413600922, Final Batch Loss: 0.4892103970050812\n",
      "Epoch 409, Loss: 1.3912467062473297, Final Batch Loss: 0.402023047208786\n",
      "Epoch 410, Loss: 1.4370867013931274, Final Batch Loss: 0.47763362526893616\n",
      "Epoch 411, Loss: 1.3141694366931915, Final Batch Loss: 0.3634575307369232\n",
      "Epoch 412, Loss: 1.2716996669769287, Final Batch Loss: 0.37123343348503113\n",
      "Epoch 413, Loss: 1.3592947721481323, Final Batch Loss: 0.40996190905570984\n",
      "Epoch 414, Loss: 1.4762485921382904, Final Batch Loss: 0.4764333665370941\n",
      "Epoch 415, Loss: 1.3180177509784698, Final Batch Loss: 0.5092302560806274\n",
      "Epoch 416, Loss: 1.3078174889087677, Final Batch Loss: 0.448635458946228\n",
      "Epoch 417, Loss: 1.3278554677963257, Final Batch Loss: 0.4603724479675293\n",
      "Epoch 418, Loss: 1.3265304565429688, Final Batch Loss: 0.42652738094329834\n",
      "Epoch 419, Loss: 1.3670405745506287, Final Batch Loss: 0.4474415183067322\n",
      "Epoch 420, Loss: 1.3467881381511688, Final Batch Loss: 0.4414396584033966\n",
      "Epoch 421, Loss: 1.3531205654144287, Final Batch Loss: 0.4750392436981201\n",
      "Epoch 422, Loss: 1.2754211127758026, Final Batch Loss: 0.438527911901474\n",
      "Epoch 423, Loss: 1.441482037305832, Final Batch Loss: 0.5124766230583191\n",
      "Epoch 424, Loss: 1.2867786884307861, Final Batch Loss: 0.3641182780265808\n",
      "Epoch 425, Loss: 1.3267439305782318, Final Batch Loss: 0.49961400032043457\n",
      "Epoch 426, Loss: 1.3717950880527496, Final Batch Loss: 0.4378189742565155\n",
      "Epoch 427, Loss: 1.5076726078987122, Final Batch Loss: 0.5759342312812805\n",
      "Epoch 428, Loss: 1.2922967970371246, Final Batch Loss: 0.3844333291053772\n",
      "Epoch 429, Loss: 1.4083428084850311, Final Batch Loss: 0.5269609093666077\n",
      "Epoch 430, Loss: 1.3839172720909119, Final Batch Loss: 0.4988662600517273\n",
      "Epoch 431, Loss: 1.2929346561431885, Final Batch Loss: 0.4329547882080078\n",
      "Epoch 432, Loss: 1.3764694929122925, Final Batch Loss: 0.4440426528453827\n",
      "Epoch 433, Loss: 1.4029499888420105, Final Batch Loss: 0.4289466142654419\n",
      "Epoch 434, Loss: 1.24526709318161, Final Batch Loss: 0.35366886854171753\n",
      "Epoch 435, Loss: 1.3596911430358887, Final Batch Loss: 0.44791772961616516\n",
      "Epoch 436, Loss: 1.2715167999267578, Final Batch Loss: 0.37354984879493713\n",
      "Epoch 437, Loss: 1.3901622593402863, Final Batch Loss: 0.49680933356285095\n",
      "Epoch 438, Loss: 1.3507980704307556, Final Batch Loss: 0.43109309673309326\n",
      "Epoch 439, Loss: 1.2378390431404114, Final Batch Loss: 0.32963016629219055\n",
      "Epoch 440, Loss: 1.2731407582759857, Final Batch Loss: 0.4011893570423126\n",
      "Epoch 441, Loss: 1.381085753440857, Final Batch Loss: 0.5225822329521179\n",
      "Epoch 442, Loss: 1.3872984945774078, Final Batch Loss: 0.5162517428398132\n",
      "Epoch 443, Loss: 1.3408694565296173, Final Batch Loss: 0.4052983820438385\n",
      "Epoch 444, Loss: 1.318023145198822, Final Batch Loss: 0.40037041902542114\n",
      "Epoch 445, Loss: 1.3158946633338928, Final Batch Loss: 0.37677931785583496\n",
      "Epoch 446, Loss: 1.3095330893993378, Final Batch Loss: 0.3962253928184509\n",
      "Epoch 447, Loss: 1.2702866792678833, Final Batch Loss: 0.3964214324951172\n",
      "Epoch 448, Loss: 1.4664580523967743, Final Batch Loss: 0.5988590121269226\n",
      "Epoch 449, Loss: 1.3489983975887299, Final Batch Loss: 0.524247407913208\n",
      "Epoch 450, Loss: 1.3348156213760376, Final Batch Loss: 0.45227840542793274\n",
      "Epoch 451, Loss: 1.2526761889457703, Final Batch Loss: 0.38172927498817444\n",
      "Epoch 452, Loss: 1.2978509068489075, Final Batch Loss: 0.465015709400177\n",
      "Epoch 453, Loss: 1.2798759043216705, Final Batch Loss: 0.4273006319999695\n",
      "Epoch 454, Loss: 1.2959217727184296, Final Batch Loss: 0.42344868183135986\n",
      "Epoch 455, Loss: 1.358839362859726, Final Batch Loss: 0.5585451722145081\n",
      "Epoch 456, Loss: 1.3168854415416718, Final Batch Loss: 0.3914739787578583\n",
      "Epoch 457, Loss: 1.3731791079044342, Final Batch Loss: 0.45119723677635193\n",
      "Epoch 458, Loss: 1.3055466711521149, Final Batch Loss: 0.3743487596511841\n",
      "Epoch 459, Loss: 1.2546371221542358, Final Batch Loss: 0.3895123302936554\n",
      "Epoch 460, Loss: 1.300698161125183, Final Batch Loss: 0.45535436272621155\n",
      "Epoch 461, Loss: 1.2621445655822754, Final Batch Loss: 0.36248576641082764\n",
      "Epoch 462, Loss: 1.2828662395477295, Final Batch Loss: 0.4007427990436554\n",
      "Epoch 463, Loss: 1.3227224349975586, Final Batch Loss: 0.447900652885437\n",
      "Epoch 464, Loss: 1.289164513349533, Final Batch Loss: 0.4618033766746521\n",
      "Epoch 465, Loss: 1.2982963025569916, Final Batch Loss: 0.47816574573516846\n",
      "Epoch 466, Loss: 1.4178301692008972, Final Batch Loss: 0.4795495271682739\n",
      "Epoch 467, Loss: 1.3555461168289185, Final Batch Loss: 0.4621601700782776\n",
      "Epoch 468, Loss: 1.387463003396988, Final Batch Loss: 0.45176565647125244\n",
      "Epoch 469, Loss: 1.257725179195404, Final Batch Loss: 0.3967705965042114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470, Loss: 1.268854945898056, Final Batch Loss: 0.4094243347644806\n",
      "Epoch 471, Loss: 1.3693192303180695, Final Batch Loss: 0.44996702671051025\n",
      "Epoch 472, Loss: 1.3348695635795593, Final Batch Loss: 0.4116398096084595\n",
      "Epoch 473, Loss: 1.2391521334648132, Final Batch Loss: 0.46619072556495667\n",
      "Epoch 474, Loss: 1.2727529406547546, Final Batch Loss: 0.48455968499183655\n",
      "Epoch 475, Loss: 1.3472613394260406, Final Batch Loss: 0.47556746006011963\n",
      "Epoch 476, Loss: 1.343614012002945, Final Batch Loss: 0.4376334547996521\n",
      "Epoch 477, Loss: 1.2374318838119507, Final Batch Loss: 0.4100606143474579\n",
      "Epoch 478, Loss: 1.2849087119102478, Final Batch Loss: 0.3456926643848419\n",
      "Epoch 479, Loss: 1.2785455584526062, Final Batch Loss: 0.4392121136188507\n",
      "Epoch 480, Loss: 1.3450663685798645, Final Batch Loss: 0.47575846314430237\n",
      "Epoch 481, Loss: 1.3041541874408722, Final Batch Loss: 0.45426735281944275\n",
      "Epoch 482, Loss: 1.3555095195770264, Final Batch Loss: 0.5252493023872375\n",
      "Epoch 483, Loss: 1.229950100183487, Final Batch Loss: 0.3591030538082123\n",
      "Epoch 484, Loss: 1.243328332901001, Final Batch Loss: 0.40140604972839355\n",
      "Epoch 485, Loss: 1.2440117299556732, Final Batch Loss: 0.448022723197937\n",
      "Epoch 486, Loss: 1.1957529783248901, Final Batch Loss: 0.4004599452018738\n",
      "Epoch 487, Loss: 1.276425302028656, Final Batch Loss: 0.37368476390838623\n",
      "Epoch 488, Loss: 1.3241561949253082, Final Batch Loss: 0.491499125957489\n",
      "Epoch 489, Loss: 1.2683183550834656, Final Batch Loss: 0.4016292095184326\n",
      "Epoch 490, Loss: 1.3330081403255463, Final Batch Loss: 0.49385154247283936\n",
      "Epoch 491, Loss: 1.358807921409607, Final Batch Loss: 0.46955356001853943\n",
      "Epoch 492, Loss: 1.296221762895584, Final Batch Loss: 0.46055707335472107\n",
      "Epoch 493, Loss: 1.3005797266960144, Final Batch Loss: 0.4756454825401306\n",
      "Epoch 494, Loss: 1.3135254085063934, Final Batch Loss: 0.4804302155971527\n",
      "Epoch 495, Loss: 1.236102044582367, Final Batch Loss: 0.43582603335380554\n",
      "Epoch 496, Loss: 1.3508608043193817, Final Batch Loss: 0.4937104880809784\n",
      "Epoch 497, Loss: 1.2198962271213531, Final Batch Loss: 0.3709551692008972\n",
      "Epoch 498, Loss: 1.3169109225273132, Final Batch Loss: 0.4065673351287842\n",
      "Epoch 499, Loss: 1.2518517076969147, Final Batch Loss: 0.4242880344390869\n",
      "Epoch 500, Loss: 1.3843126595020294, Final Batch Loss: 0.48398399353027344\n",
      "Epoch 501, Loss: 1.2472908794879913, Final Batch Loss: 0.4282829463481903\n",
      "Epoch 502, Loss: 1.2502108812332153, Final Batch Loss: 0.35758495330810547\n",
      "Epoch 503, Loss: 1.354441910982132, Final Batch Loss: 0.4811896085739136\n",
      "Epoch 504, Loss: 1.2151916325092316, Final Batch Loss: 0.36722832918167114\n",
      "Epoch 505, Loss: 1.2634246349334717, Final Batch Loss: 0.49780160188674927\n",
      "Epoch 506, Loss: 1.2413011491298676, Final Batch Loss: 0.4788481295108795\n",
      "Epoch 507, Loss: 1.279911369085312, Final Batch Loss: 0.430711567401886\n",
      "Epoch 508, Loss: 1.1863830089569092, Final Batch Loss: 0.3716956079006195\n",
      "Epoch 509, Loss: 1.2878309488296509, Final Batch Loss: 0.43171945214271545\n",
      "Epoch 510, Loss: 1.3106184899806976, Final Batch Loss: 0.4219660758972168\n",
      "Epoch 511, Loss: 1.207903802394867, Final Batch Loss: 0.3468630313873291\n",
      "Epoch 512, Loss: 1.3068949282169342, Final Batch Loss: 0.3992628753185272\n",
      "Epoch 513, Loss: 1.2549211084842682, Final Batch Loss: 0.437625914812088\n",
      "Epoch 514, Loss: 1.1887258887290955, Final Batch Loss: 0.3108557164669037\n",
      "Epoch 515, Loss: 1.2553184032440186, Final Batch Loss: 0.4610472023487091\n",
      "Epoch 516, Loss: 1.2650065422058105, Final Batch Loss: 0.451404869556427\n",
      "Epoch 517, Loss: 1.1725502014160156, Final Batch Loss: 0.4042704701423645\n",
      "Epoch 518, Loss: 1.3136540353298187, Final Batch Loss: 0.46762117743492126\n",
      "Epoch 519, Loss: 1.1958746910095215, Final Batch Loss: 0.3881875276565552\n",
      "Epoch 520, Loss: 1.250130534172058, Final Batch Loss: 0.4569949805736542\n",
      "Epoch 521, Loss: 1.3218953013420105, Final Batch Loss: 0.45997101068496704\n",
      "Epoch 522, Loss: 1.2467146217823029, Final Batch Loss: 0.4041684865951538\n",
      "Epoch 523, Loss: 1.240448772907257, Final Batch Loss: 0.368864506483078\n",
      "Epoch 524, Loss: 1.2049556374549866, Final Batch Loss: 0.3603335916996002\n",
      "Epoch 525, Loss: 1.281814694404602, Final Batch Loss: 0.4451107680797577\n",
      "Epoch 526, Loss: 1.3135842978954315, Final Batch Loss: 0.5040143132209778\n",
      "Epoch 527, Loss: 1.2115547955036163, Final Batch Loss: 0.3897453844547272\n",
      "Epoch 528, Loss: 1.2223133444786072, Final Batch Loss: 0.3928152024745941\n",
      "Epoch 529, Loss: 1.1769515872001648, Final Batch Loss: 0.3831227719783783\n",
      "Epoch 530, Loss: 1.251959204673767, Final Batch Loss: 0.41454750299453735\n",
      "Epoch 531, Loss: 1.2309774458408356, Final Batch Loss: 0.4346770644187927\n",
      "Epoch 532, Loss: 1.2069010734558105, Final Batch Loss: 0.3935525417327881\n",
      "Epoch 533, Loss: 1.2758877277374268, Final Batch Loss: 0.4336691200733185\n",
      "Epoch 534, Loss: 1.21690234541893, Final Batch Loss: 0.402682900428772\n",
      "Epoch 535, Loss: 1.2387650310993195, Final Batch Loss: 0.39899763464927673\n",
      "Epoch 536, Loss: 1.191877394914627, Final Batch Loss: 0.42062729597091675\n",
      "Epoch 537, Loss: 1.2376218140125275, Final Batch Loss: 0.3973967730998993\n",
      "Epoch 538, Loss: 1.2270696461200714, Final Batch Loss: 0.3947332203388214\n",
      "Epoch 539, Loss: 1.2970825135707855, Final Batch Loss: 0.4293838143348694\n",
      "Epoch 540, Loss: 1.2737034559249878, Final Batch Loss: 0.4538297951221466\n",
      "Epoch 541, Loss: 1.1593557000160217, Final Batch Loss: 0.4836871325969696\n",
      "Epoch 542, Loss: 1.2928206324577332, Final Batch Loss: 0.4323902130126953\n",
      "Epoch 543, Loss: 1.2467608451843262, Final Batch Loss: 0.42941877245903015\n",
      "Epoch 544, Loss: 1.1971604228019714, Final Batch Loss: 0.357826292514801\n",
      "Epoch 545, Loss: 1.251655101776123, Final Batch Loss: 0.43695682287216187\n",
      "Epoch 546, Loss: 1.2088165581226349, Final Batch Loss: 0.41885271668434143\n",
      "Epoch 547, Loss: 1.173392415046692, Final Batch Loss: 0.4101467430591583\n",
      "Epoch 548, Loss: 1.248914897441864, Final Batch Loss: 0.4435071051120758\n",
      "Epoch 549, Loss: 1.2314218282699585, Final Batch Loss: 0.38430795073509216\n",
      "Epoch 550, Loss: 1.205568790435791, Final Batch Loss: 0.4016543924808502\n",
      "Epoch 551, Loss: 1.2068018317222595, Final Batch Loss: 0.3781166672706604\n",
      "Epoch 552, Loss: 1.2569242119789124, Final Batch Loss: 0.5115869641304016\n",
      "Epoch 553, Loss: 1.1629155576229095, Final Batch Loss: 0.40138188004493713\n",
      "Epoch 554, Loss: 1.2701534628868103, Final Batch Loss: 0.46557414531707764\n",
      "Epoch 555, Loss: 1.1303873360157013, Final Batch Loss: 0.3317457437515259\n",
      "Epoch 556, Loss: 1.1884759366512299, Final Batch Loss: 0.38620901107788086\n",
      "Epoch 557, Loss: 1.2166960537433624, Final Batch Loss: 0.3683900833129883\n",
      "Epoch 558, Loss: 1.2093498706817627, Final Batch Loss: 0.3607284128665924\n",
      "Epoch 559, Loss: 1.2568985223770142, Final Batch Loss: 0.4254773259162903\n",
      "Epoch 560, Loss: 1.1821407079696655, Final Batch Loss: 0.3685661852359772\n",
      "Epoch 561, Loss: 1.208761751651764, Final Batch Loss: 0.3096707761287689\n",
      "Epoch 562, Loss: 1.1495475471019745, Final Batch Loss: 0.4167303442955017\n",
      "Epoch 563, Loss: 1.1637296080589294, Final Batch Loss: 0.4036390483379364\n",
      "Epoch 564, Loss: 1.327296644449234, Final Batch Loss: 0.47764456272125244\n",
      "Epoch 565, Loss: 1.1965463757514954, Final Batch Loss: 0.3776712119579315\n",
      "Epoch 566, Loss: 1.2162517607212067, Final Batch Loss: 0.44515007734298706\n",
      "Epoch 567, Loss: 1.1922716796398163, Final Batch Loss: 0.4248133897781372\n",
      "Epoch 568, Loss: 1.2068136632442474, Final Batch Loss: 0.4657793641090393\n",
      "Epoch 569, Loss: 1.2189621925354004, Final Batch Loss: 0.3900456130504608\n",
      "Epoch 570, Loss: 1.2118144929409027, Final Batch Loss: 0.42141610383987427\n",
      "Epoch 571, Loss: 1.2324903905391693, Final Batch Loss: 0.4765286445617676\n",
      "Epoch 572, Loss: 1.2581956684589386, Final Batch Loss: 0.44015026092529297\n",
      "Epoch 573, Loss: 1.244843304157257, Final Batch Loss: 0.34849685430526733\n",
      "Epoch 574, Loss: 1.2315576672554016, Final Batch Loss: 0.3536950647830963\n",
      "Epoch 575, Loss: 1.2674652934074402, Final Batch Loss: 0.4384141266345978\n",
      "Epoch 576, Loss: 1.2088129222393036, Final Batch Loss: 0.4156155586242676\n",
      "Epoch 577, Loss: 1.2262088060379028, Final Batch Loss: 0.39838287234306335\n",
      "Epoch 578, Loss: 1.1852706968784332, Final Batch Loss: 0.36486363410949707\n",
      "Epoch 579, Loss: 1.1750693619251251, Final Batch Loss: 0.3666450083255768\n",
      "Epoch 580, Loss: 1.1736475825309753, Final Batch Loss: 0.38380661606788635\n",
      "Epoch 581, Loss: 1.2805914878845215, Final Batch Loss: 0.4664485454559326\n",
      "Epoch 582, Loss: 1.214376151561737, Final Batch Loss: 0.391728937625885\n",
      "Epoch 583, Loss: 1.2530423402786255, Final Batch Loss: 0.4008220434188843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 584, Loss: 1.1542721688747406, Final Batch Loss: 0.3182385563850403\n",
      "Epoch 585, Loss: 1.1341266930103302, Final Batch Loss: 0.3992699980735779\n",
      "Epoch 586, Loss: 1.1107942759990692, Final Batch Loss: 0.3320329487323761\n",
      "Epoch 587, Loss: 1.2240577042102814, Final Batch Loss: 0.39724868535995483\n",
      "Epoch 588, Loss: 1.2223874926567078, Final Batch Loss: 0.3848330080509186\n",
      "Epoch 589, Loss: 1.1511072516441345, Final Batch Loss: 0.3416248559951782\n",
      "Epoch 590, Loss: 1.1702132523059845, Final Batch Loss: 0.39578187465667725\n",
      "Epoch 591, Loss: 1.1790203750133514, Final Batch Loss: 0.414902001619339\n",
      "Epoch 592, Loss: 1.113038808107376, Final Batch Loss: 0.30927395820617676\n",
      "Epoch 593, Loss: 1.143089473247528, Final Batch Loss: 0.347859263420105\n",
      "Epoch 594, Loss: 1.1924435198307037, Final Batch Loss: 0.40810728073120117\n",
      "Epoch 595, Loss: 1.1905739307403564, Final Batch Loss: 0.4117901921272278\n",
      "Epoch 596, Loss: 1.2082643806934357, Final Batch Loss: 0.4935528337955475\n",
      "Epoch 597, Loss: 1.1621869802474976, Final Batch Loss: 0.3508540689945221\n",
      "Epoch 598, Loss: 1.2267062366008759, Final Batch Loss: 0.3818660378456116\n",
      "Epoch 599, Loss: 1.182521790266037, Final Batch Loss: 0.44310086965560913\n",
      "Epoch 600, Loss: 1.1534614562988281, Final Batch Loss: 0.36749905347824097\n",
      "Epoch 601, Loss: 1.1597967147827148, Final Batch Loss: 0.41497644782066345\n",
      "Epoch 602, Loss: 1.1592814922332764, Final Batch Loss: 0.45595359802246094\n",
      "Epoch 603, Loss: 1.1602784097194672, Final Batch Loss: 0.3519105911254883\n",
      "Epoch 604, Loss: 1.202875792980194, Final Batch Loss: 0.4090927243232727\n",
      "Epoch 605, Loss: 1.174985647201538, Final Batch Loss: 0.3925533890724182\n",
      "Epoch 606, Loss: 1.1866783499717712, Final Batch Loss: 0.34459561109542847\n",
      "Epoch 607, Loss: 1.268309235572815, Final Batch Loss: 0.5284616947174072\n",
      "Epoch 608, Loss: 1.2272157669067383, Final Batch Loss: 0.4151856601238251\n",
      "Epoch 609, Loss: 1.2072233259677887, Final Batch Loss: 0.3826654851436615\n",
      "Epoch 610, Loss: 1.1434113085269928, Final Batch Loss: 0.44528165459632874\n",
      "Epoch 611, Loss: 1.1120725274085999, Final Batch Loss: 0.37171363830566406\n",
      "Epoch 612, Loss: 1.193264663219452, Final Batch Loss: 0.45965221524238586\n",
      "Epoch 613, Loss: 1.1541836261749268, Final Batch Loss: 0.3493572771549225\n",
      "Epoch 614, Loss: 1.0949967205524445, Final Batch Loss: 0.3483617901802063\n",
      "Epoch 615, Loss: 1.1114884316921234, Final Batch Loss: 0.3494257926940918\n",
      "Epoch 616, Loss: 1.1897539496421814, Final Batch Loss: 0.38930240273475647\n",
      "Epoch 617, Loss: 1.142897754907608, Final Batch Loss: 0.3121839761734009\n",
      "Epoch 618, Loss: 1.0744647681713104, Final Batch Loss: 0.38605213165283203\n",
      "Epoch 619, Loss: 1.2861164510250092, Final Batch Loss: 0.4479095935821533\n",
      "Epoch 620, Loss: 1.2104988992214203, Final Batch Loss: 0.37399935722351074\n",
      "Epoch 621, Loss: 1.21199831366539, Final Batch Loss: 0.4665781855583191\n",
      "Epoch 622, Loss: 1.1568333506584167, Final Batch Loss: 0.35965225100517273\n",
      "Epoch 623, Loss: 1.1805985867977142, Final Batch Loss: 0.39127790927886963\n",
      "Epoch 624, Loss: 1.103716254234314, Final Batch Loss: 0.31988874077796936\n",
      "Epoch 625, Loss: 1.1305063962936401, Final Batch Loss: 0.47218501567840576\n",
      "Epoch 626, Loss: 1.1085134148597717, Final Batch Loss: 0.37900564074516296\n",
      "Epoch 627, Loss: 1.1254426836967468, Final Batch Loss: 0.4002189040184021\n",
      "Epoch 628, Loss: 1.1443738341331482, Final Batch Loss: 0.3722032308578491\n",
      "Epoch 629, Loss: 1.1365349292755127, Final Batch Loss: 0.33790120482444763\n",
      "Epoch 630, Loss: 1.1679887175559998, Final Batch Loss: 0.3317103981971741\n",
      "Epoch 631, Loss: 1.1842018067836761, Final Batch Loss: 0.4229245185852051\n",
      "Epoch 632, Loss: 1.079394370317459, Final Batch Loss: 0.34926676750183105\n",
      "Epoch 633, Loss: 1.1356589496135712, Final Batch Loss: 0.31430184841156006\n",
      "Epoch 634, Loss: 1.1897072792053223, Final Batch Loss: 0.4268641471862793\n",
      "Epoch 635, Loss: 1.2995213270187378, Final Batch Loss: 0.5834113359451294\n",
      "Epoch 636, Loss: 1.1906704902648926, Final Batch Loss: 0.37083402276039124\n",
      "Epoch 637, Loss: 1.1575477421283722, Final Batch Loss: 0.42464306950569153\n",
      "Epoch 638, Loss: 1.0698771178722382, Final Batch Loss: 0.33506572246551514\n",
      "Epoch 639, Loss: 1.1471127569675446, Final Batch Loss: 0.4101627469062805\n",
      "Epoch 640, Loss: 1.1305303871631622, Final Batch Loss: 0.3430725634098053\n",
      "Epoch 641, Loss: 1.1761807799339294, Final Batch Loss: 0.3794613480567932\n",
      "Epoch 642, Loss: 1.2889043092727661, Final Batch Loss: 0.39843815565109253\n",
      "Epoch 643, Loss: 1.2082969844341278, Final Batch Loss: 0.4417082369327545\n",
      "Epoch 644, Loss: 1.2051993310451508, Final Batch Loss: 0.328682541847229\n",
      "Epoch 645, Loss: 1.2099845707416534, Final Batch Loss: 0.3675391674041748\n",
      "Epoch 646, Loss: 1.1101883947849274, Final Batch Loss: 0.2777571976184845\n",
      "Epoch 647, Loss: 1.153320997953415, Final Batch Loss: 0.3512420356273651\n",
      "Epoch 648, Loss: 1.1445007920265198, Final Batch Loss: 0.3554517924785614\n",
      "Epoch 649, Loss: 1.1409250497817993, Final Batch Loss: 0.36965981125831604\n",
      "Epoch 650, Loss: 1.1777615249156952, Final Batch Loss: 0.33284422755241394\n",
      "Epoch 651, Loss: 1.2541975677013397, Final Batch Loss: 0.4083808958530426\n",
      "Epoch 652, Loss: 1.1242282688617706, Final Batch Loss: 0.3994351327419281\n",
      "Epoch 653, Loss: 1.201850414276123, Final Batch Loss: 0.3612706959247589\n",
      "Epoch 654, Loss: 1.0834442973136902, Final Batch Loss: 0.3315125107765198\n",
      "Epoch 655, Loss: 1.2237852215766907, Final Batch Loss: 0.5092912316322327\n",
      "Epoch 656, Loss: 1.2041304409503937, Final Batch Loss: 0.45379549264907837\n",
      "Epoch 657, Loss: 1.1320514678955078, Final Batch Loss: 0.39299437403678894\n",
      "Epoch 658, Loss: 1.1324554979801178, Final Batch Loss: 0.4206008017063141\n",
      "Epoch 659, Loss: 1.1321683526039124, Final Batch Loss: 0.35659316182136536\n",
      "Epoch 660, Loss: 1.2303999662399292, Final Batch Loss: 0.4073852002620697\n",
      "Epoch 661, Loss: 1.1004442274570465, Final Batch Loss: 0.407907634973526\n",
      "Epoch 662, Loss: 1.2111260890960693, Final Batch Loss: 0.4376518428325653\n",
      "Epoch 663, Loss: 1.0991837084293365, Final Batch Loss: 0.3977481424808502\n",
      "Epoch 664, Loss: 1.1480532884597778, Final Batch Loss: 0.4324875771999359\n",
      "Epoch 665, Loss: 1.1088511943817139, Final Batch Loss: 0.3977232873439789\n",
      "Epoch 666, Loss: 1.1140949726104736, Final Batch Loss: 0.37592247128486633\n",
      "Epoch 667, Loss: 1.0934962630271912, Final Batch Loss: 0.30828437209129333\n",
      "Epoch 668, Loss: 1.1379700005054474, Final Batch Loss: 0.424945205450058\n",
      "Epoch 669, Loss: 1.1093784868717194, Final Batch Loss: 0.3654294013977051\n",
      "Epoch 670, Loss: 1.1176528334617615, Final Batch Loss: 0.3718324899673462\n",
      "Epoch 671, Loss: 1.1019402146339417, Final Batch Loss: 0.3378833532333374\n",
      "Epoch 672, Loss: 1.0551214218139648, Final Batch Loss: 0.3067105710506439\n",
      "Epoch 673, Loss: 1.1011333763599396, Final Batch Loss: 0.3890686333179474\n",
      "Epoch 674, Loss: 1.2176821529865265, Final Batch Loss: 0.39235812425613403\n",
      "Epoch 675, Loss: 1.0779328346252441, Final Batch Loss: 0.3303515911102295\n",
      "Epoch 676, Loss: 1.1625811755657196, Final Batch Loss: 0.4169885218143463\n",
      "Epoch 677, Loss: 1.0606383383274078, Final Batch Loss: 0.3661409318447113\n",
      "Epoch 678, Loss: 1.1934904754161835, Final Batch Loss: 0.39862632751464844\n",
      "Epoch 679, Loss: 1.1864907145500183, Final Batch Loss: 0.34575217962265015\n",
      "Epoch 680, Loss: 1.1729722619056702, Final Batch Loss: 0.3839680850505829\n",
      "Epoch 681, Loss: 1.0963129699230194, Final Batch Loss: 0.3193623721599579\n",
      "Epoch 682, Loss: 1.1543269753456116, Final Batch Loss: 0.4172084927558899\n",
      "Epoch 683, Loss: 1.116005688905716, Final Batch Loss: 0.31171104311943054\n",
      "Epoch 684, Loss: 1.1609424650669098, Final Batch Loss: 0.36235150694847107\n",
      "Epoch 685, Loss: 1.0486218631267548, Final Batch Loss: 0.33722904324531555\n",
      "Epoch 686, Loss: 1.1610715687274933, Final Batch Loss: 0.3863270580768585\n",
      "Epoch 687, Loss: 1.0392690002918243, Final Batch Loss: 0.31272849440574646\n",
      "Epoch 688, Loss: 1.0858194828033447, Final Batch Loss: 0.38506484031677246\n",
      "Epoch 689, Loss: 1.12052521109581, Final Batch Loss: 0.3215842545032501\n",
      "Epoch 690, Loss: 1.0807115733623505, Final Batch Loss: 0.34272173047065735\n",
      "Epoch 691, Loss: 1.1918199956417084, Final Batch Loss: 0.38058701157569885\n",
      "Epoch 692, Loss: 1.1714603900909424, Final Batch Loss: 0.4014156460762024\n",
      "Epoch 693, Loss: 1.1567317843437195, Final Batch Loss: 0.31964266300201416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 694, Loss: 1.1735049486160278, Final Batch Loss: 0.4415125846862793\n",
      "Epoch 695, Loss: 1.0326893627643585, Final Batch Loss: 0.3714055120944977\n",
      "Epoch 696, Loss: 1.115902155637741, Final Batch Loss: 0.3973005414009094\n",
      "Epoch 697, Loss: 1.2281044125556946, Final Batch Loss: 0.47282320261001587\n",
      "Epoch 698, Loss: 1.1857226192951202, Final Batch Loss: 0.4602838158607483\n",
      "Epoch 699, Loss: 1.1107154488563538, Final Batch Loss: 0.3437274992465973\n",
      "Epoch 700, Loss: 1.165249615907669, Final Batch Loss: 0.4026485085487366\n",
      "Epoch 701, Loss: 1.1438749730587006, Final Batch Loss: 0.3846837282180786\n",
      "Epoch 702, Loss: 1.1486191749572754, Final Batch Loss: 0.3870141804218292\n",
      "Epoch 703, Loss: 1.0727360248565674, Final Batch Loss: 0.38264429569244385\n",
      "Epoch 704, Loss: 1.0670359432697296, Final Batch Loss: 0.3414516746997833\n",
      "Epoch 705, Loss: 1.103170484304428, Final Batch Loss: 0.37272173166275024\n",
      "Epoch 706, Loss: 1.086960345506668, Final Batch Loss: 0.33793649077415466\n",
      "Epoch 707, Loss: 1.1661920249462128, Final Batch Loss: 0.41965582966804504\n",
      "Epoch 708, Loss: 1.1165090203285217, Final Batch Loss: 0.3820963501930237\n",
      "Epoch 709, Loss: 1.0467321574687958, Final Batch Loss: 0.34578800201416016\n",
      "Epoch 710, Loss: 1.1217412650585175, Final Batch Loss: 0.33996450901031494\n",
      "Epoch 711, Loss: 1.0979891121387482, Final Batch Loss: 0.3427354693412781\n",
      "Epoch 712, Loss: 1.1545577347278595, Final Batch Loss: 0.34555724263191223\n",
      "Epoch 713, Loss: 1.0728334784507751, Final Batch Loss: 0.3348942697048187\n",
      "Epoch 714, Loss: 1.104425311088562, Final Batch Loss: 0.35184288024902344\n",
      "Epoch 715, Loss: 1.0530337989330292, Final Batch Loss: 0.3181900382041931\n",
      "Epoch 716, Loss: 1.217917561531067, Final Batch Loss: 0.44043001532554626\n",
      "Epoch 717, Loss: 1.077456533908844, Final Batch Loss: 0.37701377272605896\n",
      "Epoch 718, Loss: 1.0960860848426819, Final Batch Loss: 0.3865191638469696\n",
      "Epoch 719, Loss: 1.1125586032867432, Final Batch Loss: 0.33983004093170166\n",
      "Epoch 720, Loss: 1.0460218489170074, Final Batch Loss: 0.3706218898296356\n",
      "Epoch 721, Loss: 0.9777667820453644, Final Batch Loss: 0.2743319272994995\n",
      "Epoch 722, Loss: 1.0203083455562592, Final Batch Loss: 0.31902873516082764\n",
      "Epoch 723, Loss: 1.0053286254405975, Final Batch Loss: 0.3429143726825714\n",
      "Epoch 724, Loss: 1.11428764462471, Final Batch Loss: 0.36522236466407776\n",
      "Epoch 725, Loss: 1.094119369983673, Final Batch Loss: 0.39385223388671875\n",
      "Epoch 726, Loss: 1.1059744656085968, Final Batch Loss: 0.3970189094543457\n",
      "Epoch 727, Loss: 1.1003439128398895, Final Batch Loss: 0.39355945587158203\n",
      "Epoch 728, Loss: 1.1114279627799988, Final Batch Loss: 0.3482431173324585\n",
      "Epoch 729, Loss: 1.108134925365448, Final Batch Loss: 0.34710851311683655\n",
      "Epoch 730, Loss: 1.1382821798324585, Final Batch Loss: 0.3640814423561096\n",
      "Epoch 731, Loss: 1.162654459476471, Final Batch Loss: 0.3443310856819153\n",
      "Epoch 732, Loss: 1.0497321784496307, Final Batch Loss: 0.35671740770339966\n",
      "Epoch 733, Loss: 1.1808306574821472, Final Batch Loss: 0.47113969922065735\n",
      "Epoch 734, Loss: 1.0384222567081451, Final Batch Loss: 0.4109117388725281\n",
      "Epoch 735, Loss: 1.055891990661621, Final Batch Loss: 0.3381463587284088\n",
      "Epoch 736, Loss: 1.171424388885498, Final Batch Loss: 0.4259606599807739\n",
      "Epoch 737, Loss: 1.0572310984134674, Final Batch Loss: 0.3953750729560852\n",
      "Epoch 738, Loss: 1.0899431109428406, Final Batch Loss: 0.38587382435798645\n",
      "Epoch 739, Loss: 1.107961356639862, Final Batch Loss: 0.3358848989009857\n",
      "Epoch 740, Loss: 1.1360940635204315, Final Batch Loss: 0.31523624062538147\n",
      "Epoch 741, Loss: 1.107081413269043, Final Batch Loss: 0.39160236716270447\n",
      "Epoch 742, Loss: 1.1177351772785187, Final Batch Loss: 0.33774909377098083\n",
      "Epoch 743, Loss: 1.102122038602829, Final Batch Loss: 0.3440869450569153\n",
      "Epoch 744, Loss: 1.149580866098404, Final Batch Loss: 0.45005273818969727\n",
      "Epoch 745, Loss: 1.1908735036849976, Final Batch Loss: 0.36071279644966125\n",
      "Epoch 746, Loss: 1.0668030083179474, Final Batch Loss: 0.3626711666584015\n",
      "Epoch 747, Loss: 1.0302714705467224, Final Batch Loss: 0.3028567135334015\n",
      "Epoch 748, Loss: 1.0808088779449463, Final Batch Loss: 0.35716694593429565\n",
      "Epoch 749, Loss: 1.0741418600082397, Final Batch Loss: 0.3486328423023224\n",
      "Epoch 750, Loss: 1.083918184041977, Final Batch Loss: 0.4466559886932373\n",
      "Epoch 751, Loss: 1.110599845647812, Final Batch Loss: 0.4337923526763916\n",
      "Epoch 752, Loss: 1.2636307179927826, Final Batch Loss: 0.47084105014801025\n",
      "Epoch 753, Loss: 1.1815292537212372, Final Batch Loss: 0.46342140436172485\n",
      "Epoch 754, Loss: 1.0397199094295502, Final Batch Loss: 0.31162670254707336\n",
      "Epoch 755, Loss: 1.129381000995636, Final Batch Loss: 0.41139623522758484\n",
      "Epoch 756, Loss: 1.0883133709430695, Final Batch Loss: 0.39613547921180725\n",
      "Epoch 757, Loss: 1.0783924460411072, Final Batch Loss: 0.34291285276412964\n",
      "Epoch 758, Loss: 1.0034852921962738, Final Batch Loss: 0.3308562636375427\n",
      "Epoch 759, Loss: 1.118366003036499, Final Batch Loss: 0.3628048002719879\n",
      "Epoch 760, Loss: 1.006211906671524, Final Batch Loss: 0.29568856954574585\n",
      "Epoch 761, Loss: 1.1318648159503937, Final Batch Loss: 0.42691129446029663\n",
      "Epoch 762, Loss: 1.0965029001235962, Final Batch Loss: 0.3472380042076111\n",
      "Epoch 763, Loss: 1.0932657122612, Final Batch Loss: 0.3124862313270569\n",
      "Epoch 764, Loss: 1.138977974653244, Final Batch Loss: 0.37633904814720154\n",
      "Epoch 765, Loss: 1.0421354472637177, Final Batch Loss: 0.33648473024368286\n",
      "Epoch 766, Loss: 1.1720918118953705, Final Batch Loss: 0.3811308741569519\n",
      "Epoch 767, Loss: 1.05715212225914, Final Batch Loss: 0.33656978607177734\n",
      "Epoch 768, Loss: 1.0601136684417725, Final Batch Loss: 0.330689400434494\n",
      "Epoch 769, Loss: 1.092921108007431, Final Batch Loss: 0.372131884098053\n",
      "Epoch 770, Loss: 1.0123938024044037, Final Batch Loss: 0.2813413143157959\n",
      "Epoch 771, Loss: 1.0839446485042572, Final Batch Loss: 0.3739076852798462\n",
      "Epoch 772, Loss: 0.9805921614170074, Final Batch Loss: 0.3199602961540222\n",
      "Epoch 773, Loss: 1.1826818883419037, Final Batch Loss: 0.42723602056503296\n",
      "Epoch 774, Loss: 1.0742153525352478, Final Batch Loss: 0.33946385979652405\n",
      "Epoch 775, Loss: 1.0812337398529053, Final Batch Loss: 0.3423314392566681\n",
      "Epoch 776, Loss: 1.1280999183654785, Final Batch Loss: 0.44044479727745056\n",
      "Epoch 777, Loss: 1.000680387020111, Final Batch Loss: 0.3195556104183197\n",
      "Epoch 778, Loss: 0.9629979431629181, Final Batch Loss: 0.29412537813186646\n",
      "Epoch 779, Loss: 1.040414184331894, Final Batch Loss: 0.39581260085105896\n",
      "Epoch 780, Loss: 1.1012383997440338, Final Batch Loss: 0.3442603051662445\n",
      "Epoch 781, Loss: 1.0347588956356049, Final Batch Loss: 0.37128356099128723\n",
      "Epoch 782, Loss: 0.9991233646869659, Final Batch Loss: 0.28888580203056335\n",
      "Epoch 783, Loss: 1.1405839920043945, Final Batch Loss: 0.370229572057724\n",
      "Epoch 784, Loss: 1.0725979208946228, Final Batch Loss: 0.2688665986061096\n",
      "Epoch 785, Loss: 1.0720226764678955, Final Batch Loss: 0.31210145354270935\n",
      "Epoch 786, Loss: 1.0799021124839783, Final Batch Loss: 0.39969512820243835\n",
      "Epoch 787, Loss: 1.0086374580860138, Final Batch Loss: 0.2763146162033081\n",
      "Epoch 788, Loss: 1.084726870059967, Final Batch Loss: 0.4228101372718811\n",
      "Epoch 789, Loss: 1.0580747723579407, Final Batch Loss: 0.3282585144042969\n",
      "Epoch 790, Loss: 1.0984725058078766, Final Batch Loss: 0.33084970712661743\n",
      "Epoch 791, Loss: 1.1287625432014465, Final Batch Loss: 0.36885741353034973\n",
      "Epoch 792, Loss: 1.074546992778778, Final Batch Loss: 0.3684438467025757\n",
      "Epoch 793, Loss: 1.0659370124340057, Final Batch Loss: 0.4224628508090973\n",
      "Epoch 794, Loss: 1.1563236117362976, Final Batch Loss: 0.31887364387512207\n",
      "Epoch 795, Loss: 1.0827271938323975, Final Batch Loss: 0.3220239579677582\n",
      "Epoch 796, Loss: 1.1487984359264374, Final Batch Loss: 0.39971038699150085\n",
      "Epoch 797, Loss: 1.1434477865695953, Final Batch Loss: 0.381436824798584\n",
      "Epoch 798, Loss: 1.0358334481716156, Final Batch Loss: 0.28066983819007874\n",
      "Epoch 799, Loss: 0.9703421592712402, Final Batch Loss: 0.3430969715118408\n",
      "Epoch 800, Loss: 0.9799717664718628, Final Batch Loss: 0.2950124442577362\n",
      "Epoch 801, Loss: 1.0300392508506775, Final Batch Loss: 0.32277804613113403\n",
      "Epoch 802, Loss: 1.0960704982280731, Final Batch Loss: 0.32557839155197144\n",
      "Epoch 803, Loss: 1.0155182778835297, Final Batch Loss: 0.3345121741294861\n",
      "Epoch 804, Loss: 1.0485413074493408, Final Batch Loss: 0.3433725833892822\n",
      "Epoch 805, Loss: 1.0603724718093872, Final Batch Loss: 0.40006616711616516\n",
      "Epoch 806, Loss: 1.0993870198726654, Final Batch Loss: 0.37084782123565674\n",
      "Epoch 807, Loss: 1.0692610144615173, Final Batch Loss: 0.3359902799129486\n",
      "Epoch 808, Loss: 1.1001484990119934, Final Batch Loss: 0.35031598806381226\n",
      "Epoch 809, Loss: 1.0864337384700775, Final Batch Loss: 0.3959125876426697\n",
      "Epoch 810, Loss: 1.0671600997447968, Final Batch Loss: 0.30719295144081116\n",
      "Epoch 811, Loss: 1.1210189759731293, Final Batch Loss: 0.40063387155532837\n",
      "Epoch 812, Loss: 1.0385929346084595, Final Batch Loss: 0.3300553560256958\n",
      "Epoch 813, Loss: 1.0672787725925446, Final Batch Loss: 0.2802112102508545\n",
      "Epoch 814, Loss: 1.0526678264141083, Final Batch Loss: 0.3132152557373047\n",
      "Epoch 815, Loss: 1.04153111577034, Final Batch Loss: 0.3553958833217621\n",
      "Epoch 816, Loss: 0.9964221119880676, Final Batch Loss: 0.29775577783584595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 817, Loss: 1.0146051347255707, Final Batch Loss: 0.3363792896270752\n",
      "Epoch 818, Loss: 1.0481752157211304, Final Batch Loss: 0.37594518065452576\n",
      "Epoch 819, Loss: 1.090454488992691, Final Batch Loss: 0.31887540221214294\n",
      "Epoch 820, Loss: 1.029409259557724, Final Batch Loss: 0.283102810382843\n",
      "Epoch 821, Loss: 1.1153567731380463, Final Batch Loss: 0.39225372672080994\n",
      "Epoch 822, Loss: 1.0191228091716766, Final Batch Loss: 0.3326723575592041\n",
      "Epoch 823, Loss: 0.9579044282436371, Final Batch Loss: 0.287171334028244\n",
      "Epoch 824, Loss: 1.0203227400779724, Final Batch Loss: 0.3451128900051117\n",
      "Epoch 825, Loss: 1.0242902636528015, Final Batch Loss: 0.2999090254306793\n",
      "Epoch 826, Loss: 1.0570409297943115, Final Batch Loss: 0.3521880805492401\n",
      "Epoch 827, Loss: 1.0576729476451874, Final Batch Loss: 0.37521040439605713\n",
      "Epoch 828, Loss: 0.9764663577079773, Final Batch Loss: 0.3197788894176483\n",
      "Epoch 829, Loss: 1.0677745044231415, Final Batch Loss: 0.3430406153202057\n",
      "Epoch 830, Loss: 1.1213746666908264, Final Batch Loss: 0.3933221399784088\n",
      "Epoch 831, Loss: 1.0046589970588684, Final Batch Loss: 0.24093550443649292\n",
      "Epoch 832, Loss: 0.9810463786125183, Final Batch Loss: 0.3466129004955292\n",
      "Epoch 833, Loss: 1.1188852787017822, Final Batch Loss: 0.4282184839248657\n",
      "Epoch 834, Loss: 0.9763776957988739, Final Batch Loss: 0.2788926959037781\n",
      "Epoch 835, Loss: 1.013850837945938, Final Batch Loss: 0.34081530570983887\n",
      "Epoch 836, Loss: 1.0670095682144165, Final Batch Loss: 0.3666963279247284\n",
      "Epoch 837, Loss: 1.0242153108119965, Final Batch Loss: 0.26233556866645813\n",
      "Epoch 838, Loss: 1.003077208995819, Final Batch Loss: 0.34615078568458557\n",
      "Epoch 839, Loss: 1.0204462110996246, Final Batch Loss: 0.33549419045448303\n",
      "Epoch 840, Loss: 1.0497520565986633, Final Batch Loss: 0.3747979700565338\n",
      "Epoch 841, Loss: 1.0383292138576508, Final Batch Loss: 0.3275040090084076\n",
      "Epoch 842, Loss: 1.0117219686508179, Final Batch Loss: 0.3274933099746704\n",
      "Epoch 843, Loss: 1.0745331645011902, Final Batch Loss: 0.3228348195552826\n",
      "Epoch 844, Loss: 1.0079716742038727, Final Batch Loss: 0.3476860523223877\n",
      "Epoch 845, Loss: 1.021045744419098, Final Batch Loss: 0.2730652391910553\n",
      "Epoch 846, Loss: 0.9646497368812561, Final Batch Loss: 0.3226839303970337\n",
      "Epoch 847, Loss: 0.9692339301109314, Final Batch Loss: 0.30552005767822266\n",
      "Epoch 848, Loss: 0.9655620455741882, Final Batch Loss: 0.32097795605659485\n",
      "Epoch 849, Loss: 0.9724158048629761, Final Batch Loss: 0.31443023681640625\n",
      "Epoch 850, Loss: 1.063322752714157, Final Batch Loss: 0.42932331562042236\n",
      "Epoch 851, Loss: 1.0358181297779083, Final Batch Loss: 0.35672736167907715\n",
      "Epoch 852, Loss: 1.0579473078250885, Final Batch Loss: 0.35763832926750183\n",
      "Epoch 853, Loss: 1.0638618171215057, Final Batch Loss: 0.36453765630722046\n",
      "Epoch 854, Loss: 0.9775364398956299, Final Batch Loss: 0.3134375512599945\n",
      "Epoch 855, Loss: 0.9968541860580444, Final Batch Loss: 0.27089107036590576\n",
      "Epoch 856, Loss: 1.0147954821586609, Final Batch Loss: 0.25683465600013733\n",
      "Epoch 857, Loss: 1.1056696772575378, Final Batch Loss: 0.39395156502723694\n",
      "Epoch 858, Loss: 1.1320567429065704, Final Batch Loss: 0.3957973122596741\n",
      "Epoch 859, Loss: 1.058887630701065, Final Batch Loss: 0.40593627095222473\n",
      "Epoch 860, Loss: 0.944098949432373, Final Batch Loss: 0.29442891478538513\n",
      "Epoch 861, Loss: 1.0520163476467133, Final Batch Loss: 0.4058021605014801\n",
      "Epoch 862, Loss: 1.0558997690677643, Final Batch Loss: 0.3808788061141968\n",
      "Epoch 863, Loss: 0.96150803565979, Final Batch Loss: 0.320842444896698\n",
      "Epoch 864, Loss: 0.9936035573482513, Final Batch Loss: 0.283875972032547\n",
      "Epoch 865, Loss: 1.0940773785114288, Final Batch Loss: 0.3611758351325989\n",
      "Epoch 866, Loss: 1.0331403017044067, Final Batch Loss: 0.3042428195476532\n",
      "Epoch 867, Loss: 1.0322157144546509, Final Batch Loss: 0.3051407039165497\n",
      "Epoch 868, Loss: 1.0277637243270874, Final Batch Loss: 0.3398173749446869\n",
      "Epoch 869, Loss: 1.1145820319652557, Final Batch Loss: 0.42255067825317383\n",
      "Epoch 870, Loss: 1.004721075296402, Final Batch Loss: 0.3125692903995514\n",
      "Epoch 871, Loss: 0.9872586131095886, Final Batch Loss: 0.3218945860862732\n",
      "Epoch 872, Loss: 0.9859065115451813, Final Batch Loss: 0.2733381390571594\n",
      "Epoch 873, Loss: 1.0578457415103912, Final Batch Loss: 0.3832654058933258\n",
      "Epoch 874, Loss: 0.8428027927875519, Final Batch Loss: 0.20680966973304749\n",
      "Epoch 875, Loss: 1.0919708609580994, Final Batch Loss: 0.5007545948028564\n",
      "Epoch 876, Loss: 1.0142875611782074, Final Batch Loss: 0.3845249116420746\n",
      "Epoch 877, Loss: 1.0416328608989716, Final Batch Loss: 0.4083746671676636\n",
      "Epoch 878, Loss: 1.0089449882507324, Final Batch Loss: 0.3169049918651581\n",
      "Epoch 879, Loss: 1.0664241313934326, Final Batch Loss: 0.3568362891674042\n",
      "Epoch 880, Loss: 0.9654231071472168, Final Batch Loss: 0.29331475496292114\n",
      "Epoch 881, Loss: 1.0116057395935059, Final Batch Loss: 0.3471052646636963\n",
      "Epoch 882, Loss: 1.0404908061027527, Final Batch Loss: 0.33560094237327576\n",
      "Epoch 883, Loss: 0.9191247522830963, Final Batch Loss: 0.27577728033065796\n",
      "Epoch 884, Loss: 1.0321694314479828, Final Batch Loss: 0.33161741495132446\n",
      "Epoch 885, Loss: 1.0600236058235168, Final Batch Loss: 0.38760918378829956\n",
      "Epoch 886, Loss: 0.9627340137958527, Final Batch Loss: 0.35736358165740967\n",
      "Epoch 887, Loss: 0.976741373538971, Final Batch Loss: 0.36372947692871094\n",
      "Epoch 888, Loss: 1.1131427884101868, Final Batch Loss: 0.3778168261051178\n",
      "Epoch 889, Loss: 0.9934596121311188, Final Batch Loss: 0.32926204800605774\n",
      "Epoch 890, Loss: 0.9673683941364288, Final Batch Loss: 0.3014592230319977\n",
      "Epoch 891, Loss: 0.9238199591636658, Final Batch Loss: 0.3217480182647705\n",
      "Epoch 892, Loss: 0.9724059402942657, Final Batch Loss: 0.35653170943260193\n",
      "Epoch 893, Loss: 1.0465849339962006, Final Batch Loss: 0.33267930150032043\n",
      "Epoch 894, Loss: 1.0538554787635803, Final Batch Loss: 0.30525442957878113\n",
      "Epoch 895, Loss: 1.0451084673404694, Final Batch Loss: 0.367299348115921\n",
      "Epoch 896, Loss: 0.9945493340492249, Final Batch Loss: 0.3763105571269989\n",
      "Epoch 897, Loss: 0.9476817548274994, Final Batch Loss: 0.27635276317596436\n",
      "Epoch 898, Loss: 0.9391831755638123, Final Batch Loss: 0.27082377672195435\n",
      "Epoch 899, Loss: 0.9621672928333282, Final Batch Loss: 0.34769150614738464\n",
      "Epoch 900, Loss: 0.9374789893627167, Final Batch Loss: 0.3131537437438965\n",
      "Epoch 901, Loss: 1.0002746880054474, Final Batch Loss: 0.2528354823589325\n",
      "Epoch 902, Loss: 1.093986600637436, Final Batch Loss: 0.4186449646949768\n",
      "Epoch 903, Loss: 0.8615608811378479, Final Batch Loss: 0.27851229906082153\n",
      "Epoch 904, Loss: 1.0384703278541565, Final Batch Loss: 0.32519716024398804\n",
      "Epoch 905, Loss: 1.013411045074463, Final Batch Loss: 0.37692761421203613\n",
      "Epoch 906, Loss: 0.945866197347641, Final Batch Loss: 0.3722512423992157\n",
      "Epoch 907, Loss: 1.0281978249549866, Final Batch Loss: 0.35109949111938477\n",
      "Epoch 908, Loss: 1.0608095526695251, Final Batch Loss: 0.39570534229278564\n",
      "Epoch 909, Loss: 0.9841888546943665, Final Batch Loss: 0.31485575437545776\n",
      "Epoch 910, Loss: 1.061431348323822, Final Batch Loss: 0.29415085911750793\n",
      "Epoch 911, Loss: 0.9132210910320282, Final Batch Loss: 0.35721850395202637\n",
      "Epoch 912, Loss: 0.9810858070850372, Final Batch Loss: 0.34883254766464233\n",
      "Epoch 913, Loss: 0.9494589865207672, Final Batch Loss: 0.3656959533691406\n",
      "Epoch 914, Loss: 1.0990974605083466, Final Batch Loss: 0.40892767906188965\n",
      "Epoch 915, Loss: 0.9161068499088287, Final Batch Loss: 0.3030601441860199\n",
      "Epoch 916, Loss: 0.9893204867839813, Final Batch Loss: 0.3609316945075989\n",
      "Epoch 917, Loss: 0.9654493033885956, Final Batch Loss: 0.2787063419818878\n",
      "Epoch 918, Loss: 1.0465666055679321, Final Batch Loss: 0.37751567363739014\n",
      "Epoch 919, Loss: 0.9263119101524353, Final Batch Loss: 0.27426043152809143\n",
      "Epoch 920, Loss: 1.0177976787090302, Final Batch Loss: 0.318059504032135\n",
      "Epoch 921, Loss: 1.0204207301139832, Final Batch Loss: 0.3711770474910736\n",
      "Epoch 922, Loss: 1.02833890914917, Final Batch Loss: 0.3894754648208618\n",
      "Epoch 923, Loss: 0.9364737868309021, Final Batch Loss: 0.2502437233924866\n",
      "Epoch 924, Loss: 0.952642023563385, Final Batch Loss: 0.32126209139823914\n",
      "Epoch 925, Loss: 0.9706104695796967, Final Batch Loss: 0.2966322600841522\n",
      "Epoch 926, Loss: 0.9550243616104126, Final Batch Loss: 0.3192650377750397\n",
      "Epoch 927, Loss: 0.9057598412036896, Final Batch Loss: 0.27392837405204773\n",
      "Epoch 928, Loss: 1.0069122612476349, Final Batch Loss: 0.256306916475296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 929, Loss: 1.0190492570400238, Final Batch Loss: 0.2921125590801239\n",
      "Epoch 930, Loss: 0.8983453065156937, Final Batch Loss: 0.23959629237651825\n",
      "Epoch 931, Loss: 1.0201398432254791, Final Batch Loss: 0.30479589104652405\n",
      "Epoch 932, Loss: 0.8361857831478119, Final Batch Loss: 0.20377102494239807\n",
      "Epoch 933, Loss: 0.9998650550842285, Final Batch Loss: 0.37795522809028625\n",
      "Epoch 934, Loss: 1.062103033065796, Final Batch Loss: 0.3769362270832062\n",
      "Epoch 935, Loss: 0.966390460729599, Final Batch Loss: 0.3397673964500427\n",
      "Epoch 936, Loss: 0.9732951670885086, Final Batch Loss: 0.3889901638031006\n",
      "Epoch 937, Loss: 0.9219063222408295, Final Batch Loss: 0.25627294182777405\n",
      "Epoch 938, Loss: 1.0407128036022186, Final Batch Loss: 0.3488743305206299\n",
      "Epoch 939, Loss: 0.9792009890079498, Final Batch Loss: 0.2526590824127197\n",
      "Epoch 940, Loss: 0.9181310534477234, Final Batch Loss: 0.28016793727874756\n",
      "Epoch 941, Loss: 0.939166247844696, Final Batch Loss: 0.31384187936782837\n",
      "Epoch 942, Loss: 1.020187497138977, Final Batch Loss: 0.41137734055519104\n",
      "Epoch 943, Loss: 0.9514504373073578, Final Batch Loss: 0.31937742233276367\n",
      "Epoch 944, Loss: 0.9861643612384796, Final Batch Loss: 0.3000127971172333\n",
      "Epoch 945, Loss: 0.9609801173210144, Final Batch Loss: 0.3039724826812744\n",
      "Epoch 946, Loss: 1.0269697606563568, Final Batch Loss: 0.29531723260879517\n",
      "Epoch 947, Loss: 0.9488652646541595, Final Batch Loss: 0.32210859656333923\n",
      "Epoch 948, Loss: 0.9738963544368744, Final Batch Loss: 0.3007704019546509\n",
      "Epoch 949, Loss: 0.9164019227027893, Final Batch Loss: 0.31397074460983276\n",
      "Epoch 950, Loss: 1.0577861964702606, Final Batch Loss: 0.3217695355415344\n",
      "Epoch 951, Loss: 0.9210894703865051, Final Batch Loss: 0.313389390707016\n",
      "Epoch 952, Loss: 1.0072782933712006, Final Batch Loss: 0.3440655469894409\n",
      "Epoch 953, Loss: 0.9478062987327576, Final Batch Loss: 0.34732285141944885\n",
      "Epoch 954, Loss: 0.9400376379489899, Final Batch Loss: 0.28064417839050293\n",
      "Epoch 955, Loss: 1.0118280947208405, Final Batch Loss: 0.37163108587265015\n",
      "Epoch 956, Loss: 0.9725279808044434, Final Batch Loss: 0.30342087149620056\n",
      "Epoch 957, Loss: 0.9377519190311432, Final Batch Loss: 0.30485934019088745\n",
      "Epoch 958, Loss: 0.9182263314723969, Final Batch Loss: 0.3183783292770386\n",
      "Epoch 959, Loss: 0.9554811716079712, Final Batch Loss: 0.31410303711891174\n",
      "Epoch 960, Loss: 0.9288889467716217, Final Batch Loss: 0.32667797803878784\n",
      "Epoch 961, Loss: 1.0441066026687622, Final Batch Loss: 0.3766842484474182\n",
      "Epoch 962, Loss: 0.9346857964992523, Final Batch Loss: 0.3121413588523865\n",
      "Epoch 963, Loss: 1.0185841023921967, Final Batch Loss: 0.30053332448005676\n",
      "Epoch 964, Loss: 1.0129992961883545, Final Batch Loss: 0.31778088212013245\n",
      "Epoch 965, Loss: 1.0372925102710724, Final Batch Loss: 0.2950947880744934\n",
      "Epoch 966, Loss: 1.0064038932323456, Final Batch Loss: 0.4067079424858093\n",
      "Epoch 967, Loss: 0.9360667616128922, Final Batch Loss: 0.2461921125650406\n",
      "Epoch 968, Loss: 0.9775398969650269, Final Batch Loss: 0.34951090812683105\n",
      "Epoch 969, Loss: 1.0032776594161987, Final Batch Loss: 0.36187058687210083\n",
      "Epoch 970, Loss: 1.0717881917953491, Final Batch Loss: 0.4371313154697418\n",
      "Epoch 971, Loss: 1.0490073263645172, Final Batch Loss: 0.30203714966773987\n",
      "Epoch 972, Loss: 0.9674147963523865, Final Batch Loss: 0.2611871361732483\n",
      "Epoch 973, Loss: 0.8372024595737457, Final Batch Loss: 0.32264629006385803\n",
      "Epoch 974, Loss: 0.934438169002533, Final Batch Loss: 0.2516472637653351\n",
      "Epoch 975, Loss: 0.9326416850090027, Final Batch Loss: 0.3428349494934082\n",
      "Epoch 976, Loss: 0.9472258388996124, Final Batch Loss: 0.2475907802581787\n",
      "Epoch 977, Loss: 0.9648526310920715, Final Batch Loss: 0.31696227192878723\n",
      "Epoch 978, Loss: 0.9539330303668976, Final Batch Loss: 0.298845499753952\n",
      "Epoch 979, Loss: 0.955070436000824, Final Batch Loss: 0.3046641945838928\n",
      "Epoch 980, Loss: 1.0840030312538147, Final Batch Loss: 0.3254089951515198\n",
      "Epoch 981, Loss: 1.0201372802257538, Final Batch Loss: 0.359404593706131\n",
      "Epoch 982, Loss: 0.9945921897888184, Final Batch Loss: 0.3145991265773773\n",
      "Epoch 983, Loss: 0.9772290885448456, Final Batch Loss: 0.2791629433631897\n",
      "Epoch 984, Loss: 1.1129507422447205, Final Batch Loss: 0.43509232997894287\n",
      "Epoch 985, Loss: 0.9140927493572235, Final Batch Loss: 0.2643711268901825\n",
      "Epoch 986, Loss: 1.0477132499217987, Final Batch Loss: 0.3912847936153412\n",
      "Epoch 987, Loss: 0.9830948114395142, Final Batch Loss: 0.3238689601421356\n",
      "Epoch 988, Loss: 0.9626699090003967, Final Batch Loss: 0.2989945411682129\n",
      "Epoch 989, Loss: 1.0073846578598022, Final Batch Loss: 0.37259525060653687\n",
      "Epoch 990, Loss: 0.9997541308403015, Final Batch Loss: 0.39471763372421265\n",
      "Epoch 991, Loss: 1.000765472650528, Final Batch Loss: 0.3997498154640198\n",
      "Epoch 992, Loss: 1.0217851400375366, Final Batch Loss: 0.28431469202041626\n",
      "Epoch 993, Loss: 0.9787115752696991, Final Batch Loss: 0.2867863178253174\n",
      "Epoch 994, Loss: 0.965870201587677, Final Batch Loss: 0.2739675045013428\n",
      "Epoch 995, Loss: 1.0023571252822876, Final Batch Loss: 0.37155768275260925\n",
      "Epoch 996, Loss: 0.9008517563343048, Final Batch Loss: 0.29211676120758057\n",
      "Epoch 997, Loss: 1.0351958870887756, Final Batch Loss: 0.38729405403137207\n",
      "Epoch 998, Loss: 0.9148341715335846, Final Batch Loss: 0.2841073274612427\n",
      "Epoch 999, Loss: 0.9898169487714767, Final Batch Loss: 0.34493955969810486\n",
      "Epoch 1000, Loss: 0.9150100350379944, Final Batch Loss: 0.31399083137512207\n",
      "Epoch 1001, Loss: 0.8541048467159271, Final Batch Loss: 0.2561836838722229\n",
      "Epoch 1002, Loss: 0.8976857364177704, Final Batch Loss: 0.2936047315597534\n",
      "Epoch 1003, Loss: 0.987325519323349, Final Batch Loss: 0.32645705342292786\n",
      "Epoch 1004, Loss: 1.035331130027771, Final Batch Loss: 0.2654268145561218\n",
      "Epoch 1005, Loss: 0.977744072675705, Final Batch Loss: 0.3189992606639862\n",
      "Epoch 1006, Loss: 0.94344362616539, Final Batch Loss: 0.31197476387023926\n",
      "Epoch 1007, Loss: 0.8749976456165314, Final Batch Loss: 0.31910276412963867\n",
      "Epoch 1008, Loss: 0.9605716168880463, Final Batch Loss: 0.36172643303871155\n",
      "Epoch 1009, Loss: 0.9154954254627228, Final Batch Loss: 0.27884796261787415\n",
      "Epoch 1010, Loss: 0.9169224202632904, Final Batch Loss: 0.2837323546409607\n",
      "Epoch 1011, Loss: 0.9624802470207214, Final Batch Loss: 0.2874831557273865\n",
      "Epoch 1012, Loss: 0.9433220326900482, Final Batch Loss: 0.30823007225990295\n",
      "Epoch 1013, Loss: 0.9736667275428772, Final Batch Loss: 0.3315856456756592\n",
      "Epoch 1014, Loss: 0.8873065114021301, Final Batch Loss: 0.32785430550575256\n",
      "Epoch 1015, Loss: 1.0066933631896973, Final Batch Loss: 0.3875345289707184\n",
      "Epoch 1016, Loss: 0.9043546617031097, Final Batch Loss: 0.2713357210159302\n",
      "Epoch 1017, Loss: 0.9109740555286407, Final Batch Loss: 0.3410995304584503\n",
      "Epoch 1018, Loss: 0.8521179556846619, Final Batch Loss: 0.22878104448318481\n",
      "Epoch 1019, Loss: 0.9602348804473877, Final Batch Loss: 0.3537971079349518\n",
      "Epoch 1020, Loss: 0.9566995799541473, Final Batch Loss: 0.3792758882045746\n",
      "Epoch 1021, Loss: 1.0162771046161652, Final Batch Loss: 0.402168869972229\n",
      "Epoch 1022, Loss: 0.942162811756134, Final Batch Loss: 0.3202742040157318\n",
      "Epoch 1023, Loss: 0.9652547836303711, Final Batch Loss: 0.27870962023735046\n",
      "Epoch 1024, Loss: 0.9508505761623383, Final Batch Loss: 0.3302263021469116\n",
      "Epoch 1025, Loss: 0.9347938895225525, Final Batch Loss: 0.32676979899406433\n",
      "Epoch 1026, Loss: 0.9129984378814697, Final Batch Loss: 0.2606448829174042\n",
      "Epoch 1027, Loss: 0.9344882667064667, Final Batch Loss: 0.2826557457447052\n",
      "Epoch 1028, Loss: 0.925691619515419, Final Batch Loss: 0.35990193486213684\n",
      "Epoch 1029, Loss: 0.9901160597801208, Final Batch Loss: 0.3104982376098633\n",
      "Epoch 1030, Loss: 0.9195108413696289, Final Batch Loss: 0.3632166087627411\n",
      "Epoch 1031, Loss: 0.9407139122486115, Final Batch Loss: 0.32755231857299805\n",
      "Epoch 1032, Loss: 0.9259653985500336, Final Batch Loss: 0.31592631340026855\n",
      "Epoch 1033, Loss: 0.9160778820514679, Final Batch Loss: 0.2902592122554779\n",
      "Epoch 1034, Loss: 0.8864490538835526, Final Batch Loss: 0.3348847031593323\n",
      "Epoch 1035, Loss: 0.9241380095481873, Final Batch Loss: 0.2934134602546692\n",
      "Epoch 1036, Loss: 0.9173164665699005, Final Batch Loss: 0.34052062034606934\n",
      "Epoch 1037, Loss: 0.9086986482143402, Final Batch Loss: 0.27974677085876465\n",
      "Epoch 1038, Loss: 0.968962550163269, Final Batch Loss: 0.34535664319992065\n",
      "Epoch 1039, Loss: 0.9901643991470337, Final Batch Loss: 0.36377471685409546\n",
      "Epoch 1040, Loss: 0.8214237689971924, Final Batch Loss: 0.25463294982910156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1041, Loss: 0.9116880595684052, Final Batch Loss: 0.3064856231212616\n",
      "Epoch 1042, Loss: 0.9054050445556641, Final Batch Loss: 0.29825642704963684\n",
      "Epoch 1043, Loss: 0.9209827184677124, Final Batch Loss: 0.2895154058933258\n",
      "Epoch 1044, Loss: 0.9402011632919312, Final Batch Loss: 0.311683714389801\n",
      "Epoch 1045, Loss: 0.8881492614746094, Final Batch Loss: 0.3204783499240875\n",
      "Epoch 1046, Loss: 0.8402242362499237, Final Batch Loss: 0.30108943581581116\n",
      "Epoch 1047, Loss: 0.9129895567893982, Final Batch Loss: 0.33797168731689453\n",
      "Epoch 1048, Loss: 0.9616598188877106, Final Batch Loss: 0.3684183359146118\n",
      "Epoch 1049, Loss: 0.9201795756816864, Final Batch Loss: 0.3077445328235626\n",
      "Epoch 1050, Loss: 0.8515809029340744, Final Batch Loss: 0.2243540734052658\n",
      "Epoch 1051, Loss: 0.8913744986057281, Final Batch Loss: 0.35651251673698425\n",
      "Epoch 1052, Loss: 0.9344160258769989, Final Batch Loss: 0.2731676995754242\n",
      "Epoch 1053, Loss: 0.8870209455490112, Final Batch Loss: 0.2686295211315155\n",
      "Epoch 1054, Loss: 0.839503139257431, Final Batch Loss: 0.27582424879074097\n",
      "Epoch 1055, Loss: 0.9175894260406494, Final Batch Loss: 0.32908880710601807\n",
      "Epoch 1056, Loss: 0.9163478314876556, Final Batch Loss: 0.285176545381546\n",
      "Epoch 1057, Loss: 0.8913328945636749, Final Batch Loss: 0.28458186984062195\n",
      "Epoch 1058, Loss: 0.8484791815280914, Final Batch Loss: 0.2862307131290436\n",
      "Epoch 1059, Loss: 0.8412725329399109, Final Batch Loss: 0.2907682955265045\n",
      "Epoch 1060, Loss: 0.8596502095460892, Final Batch Loss: 0.24050427973270416\n",
      "Epoch 1061, Loss: 0.9112178385257721, Final Batch Loss: 0.319972962141037\n",
      "Epoch 1062, Loss: 0.8111019879579544, Final Batch Loss: 0.21730126440525055\n",
      "Epoch 1063, Loss: 1.0002698302268982, Final Batch Loss: 0.327370822429657\n",
      "Epoch 1064, Loss: 0.9666763842105865, Final Batch Loss: 0.3394242823123932\n",
      "Epoch 1065, Loss: 0.822539746761322, Final Batch Loss: 0.2511829137802124\n",
      "Epoch 1066, Loss: 1.0183145403862, Final Batch Loss: 0.2852879464626312\n",
      "Epoch 1067, Loss: 0.983008861541748, Final Batch Loss: 0.38194704055786133\n",
      "Epoch 1068, Loss: 0.879434272646904, Final Batch Loss: 0.2904760539531708\n",
      "Epoch 1069, Loss: 0.9416635930538177, Final Batch Loss: 0.3108624517917633\n",
      "Epoch 1070, Loss: 0.9237077832221985, Final Batch Loss: 0.3524780571460724\n",
      "Epoch 1071, Loss: 0.9394286274909973, Final Batch Loss: 0.2519177496433258\n",
      "Epoch 1072, Loss: 1.0222259759902954, Final Batch Loss: 0.3700233995914459\n",
      "Epoch 1073, Loss: 0.859639972448349, Final Batch Loss: 0.2953936755657196\n",
      "Epoch 1074, Loss: 0.9265486299991608, Final Batch Loss: 0.2709837257862091\n",
      "Epoch 1075, Loss: 0.8812589049339294, Final Batch Loss: 0.3685890734195709\n",
      "Epoch 1076, Loss: 0.9032832682132721, Final Batch Loss: 0.32857879996299744\n",
      "Epoch 1077, Loss: 0.924347996711731, Final Batch Loss: 0.31798025965690613\n",
      "Epoch 1078, Loss: 0.9106869101524353, Final Batch Loss: 0.1899462342262268\n",
      "Epoch 1079, Loss: 0.8926509320735931, Final Batch Loss: 0.33391568064689636\n",
      "Epoch 1080, Loss: 0.915571391582489, Final Batch Loss: 0.29119938611984253\n",
      "Epoch 1081, Loss: 0.9508317410945892, Final Batch Loss: 0.37633901834487915\n",
      "Epoch 1082, Loss: 0.8831259310245514, Final Batch Loss: 0.2224581241607666\n",
      "Epoch 1083, Loss: 0.869652271270752, Final Batch Loss: 0.2523258328437805\n",
      "Epoch 1084, Loss: 0.9496123492717743, Final Batch Loss: 0.38823118805885315\n",
      "Epoch 1085, Loss: 0.9285532534122467, Final Batch Loss: 0.30674776434898376\n",
      "Epoch 1086, Loss: 0.9169464409351349, Final Batch Loss: 0.31433790922164917\n",
      "Epoch 1087, Loss: 0.9932364821434021, Final Batch Loss: 0.38185223937034607\n",
      "Epoch 1088, Loss: 0.8215814828872681, Final Batch Loss: 0.2892685532569885\n",
      "Epoch 1089, Loss: 0.9198061227798462, Final Batch Loss: 0.26030343770980835\n",
      "Epoch 1090, Loss: 0.8105518370866776, Final Batch Loss: 0.23925666511058807\n",
      "Epoch 1091, Loss: 0.9419171512126923, Final Batch Loss: 0.3265174329280853\n",
      "Epoch 1092, Loss: 0.8190352618694305, Final Batch Loss: 0.2687327265739441\n",
      "Epoch 1093, Loss: 0.8964871168136597, Final Batch Loss: 0.28926655650138855\n",
      "Epoch 1094, Loss: 0.8173749446868896, Final Batch Loss: 0.2566090524196625\n",
      "Epoch 1095, Loss: 0.9024679064750671, Final Batch Loss: 0.3667007088661194\n",
      "Epoch 1096, Loss: 0.9110287129878998, Final Batch Loss: 0.290845662355423\n",
      "Epoch 1097, Loss: 0.844522550702095, Final Batch Loss: 0.3106333017349243\n",
      "Epoch 1098, Loss: 0.932385265827179, Final Batch Loss: 0.25178903341293335\n",
      "Epoch 1099, Loss: 0.9324497878551483, Final Batch Loss: 0.3196348249912262\n",
      "Epoch 1100, Loss: 0.9036979824304581, Final Batch Loss: 0.2800706624984741\n",
      "Epoch 1101, Loss: 0.876323401927948, Final Batch Loss: 0.28501835465431213\n",
      "Epoch 1102, Loss: 0.9603858292102814, Final Batch Loss: 0.3260895013809204\n",
      "Epoch 1103, Loss: 0.8631702363491058, Final Batch Loss: 0.2754618525505066\n",
      "Epoch 1104, Loss: 0.913200318813324, Final Batch Loss: 0.24935391545295715\n",
      "Epoch 1105, Loss: 0.8836313933134079, Final Batch Loss: 0.30716896057128906\n",
      "Epoch 1106, Loss: 0.9131686091423035, Final Batch Loss: 0.28483152389526367\n",
      "Epoch 1107, Loss: 0.8610544800758362, Final Batch Loss: 0.2760290503501892\n",
      "Epoch 1108, Loss: 0.9153487980365753, Final Batch Loss: 0.28295740485191345\n",
      "Epoch 1109, Loss: 0.9448194205760956, Final Batch Loss: 0.2947314381599426\n",
      "Epoch 1110, Loss: 0.8984175622463226, Final Batch Loss: 0.2621152698993683\n",
      "Epoch 1111, Loss: 0.8166390359401703, Final Batch Loss: 0.26186108589172363\n",
      "Epoch 1112, Loss: 0.8174450993537903, Final Batch Loss: 0.29318997263908386\n",
      "Epoch 1113, Loss: 0.8004569411277771, Final Batch Loss: 0.2462891936302185\n",
      "Epoch 1114, Loss: 0.8910853862762451, Final Batch Loss: 0.3421821892261505\n",
      "Epoch 1115, Loss: 0.8311276435852051, Final Batch Loss: 0.27448880672454834\n",
      "Epoch 1116, Loss: 0.8721964657306671, Final Batch Loss: 0.27159255743026733\n",
      "Epoch 1117, Loss: 0.9007174968719482, Final Batch Loss: 0.3202453553676605\n",
      "Epoch 1118, Loss: 0.8686735033988953, Final Batch Loss: 0.28740784525871277\n",
      "Epoch 1119, Loss: 0.8518551588058472, Final Batch Loss: 0.2944794297218323\n",
      "Epoch 1120, Loss: 0.9909977614879608, Final Batch Loss: 0.33882981538772583\n",
      "Epoch 1121, Loss: 0.9805696904659271, Final Batch Loss: 0.38414138555526733\n",
      "Epoch 1122, Loss: 0.9059131443500519, Final Batch Loss: 0.28306692838668823\n",
      "Epoch 1123, Loss: 0.9500091075897217, Final Batch Loss: 0.35150402784347534\n",
      "Epoch 1124, Loss: 0.8997251689434052, Final Batch Loss: 0.2981545329093933\n",
      "Epoch 1125, Loss: 0.8684814870357513, Final Batch Loss: 0.311493843793869\n",
      "Epoch 1126, Loss: 0.9981072843074799, Final Batch Loss: 0.35566627979278564\n",
      "Epoch 1127, Loss: 0.8813024163246155, Final Batch Loss: 0.3036351203918457\n",
      "Epoch 1128, Loss: 0.9404511153697968, Final Batch Loss: 0.349935382604599\n",
      "Epoch 1129, Loss: 0.8893844485282898, Final Batch Loss: 0.32498109340667725\n",
      "Epoch 1130, Loss: 0.9256550371646881, Final Batch Loss: 0.3304480016231537\n",
      "Epoch 1131, Loss: 0.8909203708171844, Final Batch Loss: 0.2585865259170532\n",
      "Epoch 1132, Loss: 0.9519150257110596, Final Batch Loss: 0.31520897150039673\n",
      "Epoch 1133, Loss: 0.8176282346248627, Final Batch Loss: 0.24399465322494507\n",
      "Epoch 1134, Loss: 0.8700251430273056, Final Batch Loss: 0.3221699297428131\n",
      "Epoch 1135, Loss: 0.8901188969612122, Final Batch Loss: 0.28581467270851135\n",
      "Epoch 1136, Loss: 0.9283663034439087, Final Batch Loss: 0.31969600915908813\n",
      "Epoch 1137, Loss: 0.8635218888521194, Final Batch Loss: 0.34110409021377563\n",
      "Epoch 1138, Loss: 0.8966075479984283, Final Batch Loss: 0.3162153959274292\n",
      "Epoch 1139, Loss: 0.843192994594574, Final Batch Loss: 0.2893070578575134\n",
      "Epoch 1140, Loss: 0.8892620503902435, Final Batch Loss: 0.2756517827510834\n",
      "Epoch 1141, Loss: 0.8875749111175537, Final Batch Loss: 0.32437148690223694\n",
      "Epoch 1142, Loss: 0.9203034043312073, Final Batch Loss: 0.28854501247406006\n",
      "Epoch 1143, Loss: 0.9312186539173126, Final Batch Loss: 0.38232436776161194\n",
      "Epoch 1144, Loss: 0.8516377210617065, Final Batch Loss: 0.2506538927555084\n",
      "Epoch 1145, Loss: 0.8729869425296783, Final Batch Loss: 0.2991412580013275\n",
      "Epoch 1146, Loss: 0.9637851119041443, Final Batch Loss: 0.34572169184684753\n",
      "Epoch 1147, Loss: 0.8217926025390625, Final Batch Loss: 0.2723120152950287\n",
      "Epoch 1148, Loss: 1.0026588141918182, Final Batch Loss: 0.38648590445518494\n",
      "Epoch 1149, Loss: 0.8027744293212891, Final Batch Loss: 0.27098044753074646\n",
      "Epoch 1150, Loss: 0.907198429107666, Final Batch Loss: 0.3033069670200348\n",
      "Epoch 1151, Loss: 0.8647134304046631, Final Batch Loss: 0.2604232132434845\n",
      "Epoch 1152, Loss: 0.8686579316854477, Final Batch Loss: 0.3318207263946533\n",
      "Epoch 1153, Loss: 0.8538916409015656, Final Batch Loss: 0.2971076965332031\n",
      "Epoch 1154, Loss: 0.7974268049001694, Final Batch Loss: 0.22297073900699615\n",
      "Epoch 1155, Loss: 0.8580292910337448, Final Batch Loss: 0.3565593659877777\n",
      "Epoch 1156, Loss: 0.8521748185157776, Final Batch Loss: 0.34136995673179626\n",
      "Epoch 1157, Loss: 0.8379022181034088, Final Batch Loss: 0.29898613691329956\n",
      "Epoch 1158, Loss: 0.9230254888534546, Final Batch Loss: 0.3102956712245941\n",
      "Epoch 1159, Loss: 0.915459468960762, Final Batch Loss: 0.3353672921657562\n",
      "Epoch 1160, Loss: 0.8382462561130524, Final Batch Loss: 0.29428625106811523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1161, Loss: 0.8183296620845795, Final Batch Loss: 0.2792970836162567\n",
      "Epoch 1162, Loss: 0.9413166046142578, Final Batch Loss: 0.26184603571891785\n",
      "Epoch 1163, Loss: 0.8537408113479614, Final Batch Loss: 0.2978786528110504\n",
      "Epoch 1164, Loss: 0.8516028374433517, Final Batch Loss: 0.2291591614484787\n",
      "Epoch 1165, Loss: 0.8778523802757263, Final Batch Loss: 0.3257383406162262\n",
      "Epoch 1166, Loss: 0.898279994726181, Final Batch Loss: 0.28652164340019226\n",
      "Epoch 1167, Loss: 0.9682474434375763, Final Batch Loss: 0.299040287733078\n",
      "Epoch 1168, Loss: 0.928392231464386, Final Batch Loss: 0.30460354685783386\n",
      "Epoch 1169, Loss: 0.8043112605810165, Final Batch Loss: 0.24897705018520355\n",
      "Epoch 1170, Loss: 0.921286553144455, Final Batch Loss: 0.30161288380622864\n",
      "Epoch 1171, Loss: 0.8943337053060532, Final Batch Loss: 0.2792186439037323\n",
      "Epoch 1172, Loss: 0.8992136120796204, Final Batch Loss: 0.2974869906902313\n",
      "Epoch 1173, Loss: 0.81240114569664, Final Batch Loss: 0.26567190885543823\n",
      "Epoch 1174, Loss: 0.8538139164447784, Final Batch Loss: 0.27148160338401794\n",
      "Epoch 1175, Loss: 0.9262490272521973, Final Batch Loss: 0.2511662542819977\n",
      "Epoch 1176, Loss: 0.8611516356468201, Final Batch Loss: 0.24764645099639893\n",
      "Epoch 1177, Loss: 0.9812127947807312, Final Batch Loss: 0.33843889832496643\n",
      "Epoch 1178, Loss: 0.8857031315565109, Final Batch Loss: 0.32553771138191223\n",
      "Epoch 1179, Loss: 0.8662955164909363, Final Batch Loss: 0.33649885654449463\n",
      "Epoch 1180, Loss: 0.811657577753067, Final Batch Loss: 0.25039252638816833\n",
      "Epoch 1181, Loss: 0.8576901406049728, Final Batch Loss: 0.290057897567749\n",
      "Epoch 1182, Loss: 0.8952718526124954, Final Batch Loss: 0.32660651206970215\n",
      "Epoch 1183, Loss: 0.9432460069656372, Final Batch Loss: 0.2987721860408783\n",
      "Epoch 1184, Loss: 0.8581518679857254, Final Batch Loss: 0.3149929344654083\n",
      "Epoch 1185, Loss: 0.7614115178585052, Final Batch Loss: 0.2030959129333496\n",
      "Epoch 1186, Loss: 0.9140786826610565, Final Batch Loss: 0.29724839329719543\n",
      "Epoch 1187, Loss: 0.9092075228691101, Final Batch Loss: 0.3174777030944824\n",
      "Epoch 1188, Loss: 0.8489201068878174, Final Batch Loss: 0.3069436550140381\n",
      "Epoch 1189, Loss: 0.8673675954341888, Final Batch Loss: 0.31687766313552856\n",
      "Epoch 1190, Loss: 0.8747342228889465, Final Batch Loss: 0.28944075107574463\n",
      "Epoch 1191, Loss: 0.8018420934677124, Final Batch Loss: 0.2603458762168884\n",
      "Epoch 1192, Loss: 0.8398844748735428, Final Batch Loss: 0.34118571877479553\n",
      "Epoch 1193, Loss: 0.8942002058029175, Final Batch Loss: 0.37773454189300537\n",
      "Epoch 1194, Loss: 0.7718613296747208, Final Batch Loss: 0.21858245134353638\n",
      "Epoch 1195, Loss: 0.8244717717170715, Final Batch Loss: 0.299087256193161\n",
      "Epoch 1196, Loss: 0.9520710110664368, Final Batch Loss: 0.3853612542152405\n",
      "Epoch 1197, Loss: 0.8577377200126648, Final Batch Loss: 0.29751694202423096\n",
      "Epoch 1198, Loss: 0.867360919713974, Final Batch Loss: 0.34156009554862976\n",
      "Epoch 1199, Loss: 0.8790247738361359, Final Batch Loss: 0.32291826605796814\n",
      "Epoch 1200, Loss: 0.7967871874570847, Final Batch Loss: 0.23521654307842255\n",
      "Epoch 1201, Loss: 0.7850888520479202, Final Batch Loss: 0.2928004860877991\n",
      "Epoch 1202, Loss: 0.8352562636137009, Final Batch Loss: 0.22305355966091156\n",
      "Epoch 1203, Loss: 0.8513081967830658, Final Batch Loss: 0.26179465651512146\n",
      "Epoch 1204, Loss: 0.8645403683185577, Final Batch Loss: 0.28897392749786377\n",
      "Epoch 1205, Loss: 0.851746067404747, Final Batch Loss: 0.32219991087913513\n",
      "Epoch 1206, Loss: 0.8480910956859589, Final Batch Loss: 0.2551223039627075\n",
      "Epoch 1207, Loss: 0.8942599594593048, Final Batch Loss: 0.3100789189338684\n",
      "Epoch 1208, Loss: 0.80190509557724, Final Batch Loss: 0.28403326869010925\n",
      "Epoch 1209, Loss: 0.8857633173465729, Final Batch Loss: 0.2896067500114441\n",
      "Epoch 1210, Loss: 0.8981548845767975, Final Batch Loss: 0.30207324028015137\n",
      "Epoch 1211, Loss: 0.8464995622634888, Final Batch Loss: 0.28391313552856445\n",
      "Epoch 1212, Loss: 0.8924539685249329, Final Batch Loss: 0.27785977721214294\n",
      "Epoch 1213, Loss: 0.9244529753923416, Final Batch Loss: 0.41402721405029297\n",
      "Epoch 1214, Loss: 0.77508245408535, Final Batch Loss: 0.2356865108013153\n",
      "Epoch 1215, Loss: 0.8589901626110077, Final Batch Loss: 0.3096536695957184\n",
      "Epoch 1216, Loss: 0.8862980902194977, Final Batch Loss: 0.27153050899505615\n",
      "Epoch 1217, Loss: 0.8919724524021149, Final Batch Loss: 0.37182000279426575\n",
      "Epoch 1218, Loss: 0.8931445330381393, Final Batch Loss: 0.3179604113101959\n",
      "Epoch 1219, Loss: 0.8228375613689423, Final Batch Loss: 0.32209768891334534\n",
      "Epoch 1220, Loss: 0.8131207227706909, Final Batch Loss: 0.24819311499595642\n",
      "Epoch 1221, Loss: 0.8772722333669662, Final Batch Loss: 0.35154369473457336\n",
      "Epoch 1222, Loss: 0.920215904712677, Final Batch Loss: 0.3995792269706726\n",
      "Epoch 1223, Loss: 0.870789647102356, Final Batch Loss: 0.3023473620414734\n",
      "Epoch 1224, Loss: 0.7974743247032166, Final Batch Loss: 0.2418290078639984\n",
      "Epoch 1225, Loss: 0.8479621559381485, Final Batch Loss: 0.18416793644428253\n",
      "Epoch 1226, Loss: 0.754025787115097, Final Batch Loss: 0.19318684935569763\n",
      "Epoch 1227, Loss: 0.873721182346344, Final Batch Loss: 0.3138864040374756\n",
      "Epoch 1228, Loss: 0.8380035907030106, Final Batch Loss: 0.21243302524089813\n",
      "Epoch 1229, Loss: 0.729382261633873, Final Batch Loss: 0.199836865067482\n",
      "Epoch 1230, Loss: 0.7593671381473541, Final Batch Loss: 0.18773603439331055\n",
      "Epoch 1231, Loss: 0.8042138069868088, Final Batch Loss: 0.32385119795799255\n",
      "Epoch 1232, Loss: 0.8791505098342896, Final Batch Loss: 0.319758802652359\n",
      "Epoch 1233, Loss: 0.9436115622520447, Final Batch Loss: 0.4009868800640106\n",
      "Epoch 1234, Loss: 0.9031459987163544, Final Batch Loss: 0.3340783417224884\n",
      "Epoch 1235, Loss: 0.7707121670246124, Final Batch Loss: 0.2773111164569855\n",
      "Epoch 1236, Loss: 0.9021729528903961, Final Batch Loss: 0.3526363670825958\n",
      "Epoch 1237, Loss: 0.8618403375148773, Final Batch Loss: 0.3097582459449768\n",
      "Epoch 1238, Loss: 0.8954083025455475, Final Batch Loss: 0.33114781975746155\n",
      "Epoch 1239, Loss: 0.9142445921897888, Final Batch Loss: 0.3313661217689514\n",
      "Epoch 1240, Loss: 0.8075924217700958, Final Batch Loss: 0.2800147533416748\n",
      "Epoch 1241, Loss: 0.7840958535671234, Final Batch Loss: 0.2720113694667816\n",
      "Epoch 1242, Loss: 0.8523043990135193, Final Batch Loss: 0.22967493534088135\n",
      "Epoch 1243, Loss: 0.7292467355728149, Final Batch Loss: 0.2614273428916931\n",
      "Epoch 1244, Loss: 0.910046860575676, Final Batch Loss: 0.32068461179733276\n",
      "Epoch 1245, Loss: 0.8796538412570953, Final Batch Loss: 0.2833569049835205\n",
      "Epoch 1246, Loss: 0.9356689453125, Final Batch Loss: 0.30694815516471863\n",
      "Epoch 1247, Loss: 0.7234737277030945, Final Batch Loss: 0.19710491597652435\n",
      "Epoch 1248, Loss: 0.8786150962114334, Final Batch Loss: 0.3241690993309021\n",
      "Epoch 1249, Loss: 0.7436122596263885, Final Batch Loss: 0.24487148225307465\n",
      "Epoch 1250, Loss: 0.7774921357631683, Final Batch Loss: 0.2285386323928833\n",
      "Epoch 1251, Loss: 0.7925108373165131, Final Batch Loss: 0.27847784757614136\n",
      "Epoch 1252, Loss: 0.7708859145641327, Final Batch Loss: 0.244436115026474\n",
      "Epoch 1253, Loss: 0.751066267490387, Final Batch Loss: 0.21049857139587402\n",
      "Epoch 1254, Loss: 0.8127020299434662, Final Batch Loss: 0.24158185720443726\n",
      "Epoch 1255, Loss: 0.8194452375173569, Final Batch Loss: 0.31169766187667847\n",
      "Epoch 1256, Loss: 0.804369643330574, Final Batch Loss: 0.24989478290081024\n",
      "Epoch 1257, Loss: 0.9631932973861694, Final Batch Loss: 0.28220197558403015\n",
      "Epoch 1258, Loss: 0.8892651796340942, Final Batch Loss: 0.33664461970329285\n",
      "Epoch 1259, Loss: 0.8200159519910812, Final Batch Loss: 0.2470579296350479\n",
      "Epoch 1260, Loss: 0.7936114072799683, Final Batch Loss: 0.3049253225326538\n",
      "Epoch 1261, Loss: 0.8298061788082123, Final Batch Loss: 0.2724079191684723\n",
      "Epoch 1262, Loss: 0.8507068306207657, Final Batch Loss: 0.22946704924106598\n",
      "Epoch 1263, Loss: 0.8309496939182281, Final Batch Loss: 0.2710331082344055\n",
      "Epoch 1264, Loss: 0.7918275147676468, Final Batch Loss: 0.22211690247058868\n",
      "Epoch 1265, Loss: 0.7720385789871216, Final Batch Loss: 0.24514102935791016\n",
      "Epoch 1266, Loss: 0.892753005027771, Final Batch Loss: 0.2800714671611786\n",
      "Epoch 1267, Loss: 0.8381933569908142, Final Batch Loss: 0.3218444287776947\n",
      "Epoch 1268, Loss: 0.9436428248882294, Final Batch Loss: 0.29447489976882935\n",
      "Epoch 1269, Loss: 0.7271740138530731, Final Batch Loss: 0.2262141853570938\n",
      "Epoch 1270, Loss: 0.8277480453252792, Final Batch Loss: 0.31537115573883057\n",
      "Epoch 1271, Loss: 0.8184657692909241, Final Batch Loss: 0.2708117365837097\n",
      "Epoch 1272, Loss: 0.7880531996488571, Final Batch Loss: 0.24840103089809418\n",
      "Epoch 1273, Loss: 0.7639908790588379, Final Batch Loss: 0.2090107798576355\n",
      "Epoch 1274, Loss: 0.7559093236923218, Final Batch Loss: 0.2503480613231659\n",
      "Epoch 1275, Loss: 0.8270315378904343, Final Batch Loss: 0.3614186942577362\n",
      "Epoch 1276, Loss: 0.7958692908287048, Final Batch Loss: 0.2119063436985016\n",
      "Epoch 1277, Loss: 0.7578493356704712, Final Batch Loss: 0.22305601835250854\n",
      "Epoch 1278, Loss: 0.7721401005983353, Final Batch Loss: 0.2626461982727051\n",
      "Epoch 1279, Loss: 0.8197583556175232, Final Batch Loss: 0.27979618310928345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1280, Loss: 0.8049763888120651, Final Batch Loss: 0.23230834305286407\n",
      "Epoch 1281, Loss: 0.8165119588375092, Final Batch Loss: 0.3271920382976532\n",
      "Epoch 1282, Loss: 0.7483667880296707, Final Batch Loss: 0.183612659573555\n",
      "Epoch 1283, Loss: 0.7490430325269699, Final Batch Loss: 0.19322556257247925\n",
      "Epoch 1284, Loss: 0.7020507901906967, Final Batch Loss: 0.2190423309803009\n",
      "Epoch 1285, Loss: 0.7273955345153809, Final Batch Loss: 0.23701563477516174\n",
      "Epoch 1286, Loss: 0.7944430261850357, Final Batch Loss: 0.3120824992656708\n",
      "Epoch 1287, Loss: 0.8426742106676102, Final Batch Loss: 0.29468148946762085\n",
      "Epoch 1288, Loss: 0.8021887987852097, Final Batch Loss: 0.35688886046409607\n",
      "Epoch 1289, Loss: 0.6959069967269897, Final Batch Loss: 0.24673321843147278\n",
      "Epoch 1290, Loss: 0.8316447287797928, Final Batch Loss: 0.24361731112003326\n",
      "Epoch 1291, Loss: 0.960046648979187, Final Batch Loss: 0.3578774034976959\n",
      "Epoch 1292, Loss: 0.7446457743644714, Final Batch Loss: 0.300869882106781\n",
      "Epoch 1293, Loss: 0.7126629948616028, Final Batch Loss: 0.1780390441417694\n",
      "Epoch 1294, Loss: 0.8470662236213684, Final Batch Loss: 0.2977829873561859\n",
      "Epoch 1295, Loss: 0.8539671897888184, Final Batch Loss: 0.29688847064971924\n",
      "Epoch 1296, Loss: 0.7253254801034927, Final Batch Loss: 0.2374046891927719\n",
      "Epoch 1297, Loss: 0.8098923563957214, Final Batch Loss: 0.2784980833530426\n",
      "Epoch 1298, Loss: 0.890583723783493, Final Batch Loss: 0.28510069847106934\n",
      "Epoch 1299, Loss: 0.7520009577274323, Final Batch Loss: 0.23747511208057404\n",
      "Epoch 1300, Loss: 0.7599120885133743, Final Batch Loss: 0.2861286699771881\n",
      "Epoch 1301, Loss: 0.8759750425815582, Final Batch Loss: 0.32246994972229004\n",
      "Epoch 1302, Loss: 0.8454053699970245, Final Batch Loss: 0.2676672339439392\n",
      "Epoch 1303, Loss: 0.69246406853199, Final Batch Loss: 0.20649710297584534\n",
      "Epoch 1304, Loss: 0.7261383533477783, Final Batch Loss: 0.24183663725852966\n",
      "Epoch 1305, Loss: 0.7569358348846436, Final Batch Loss: 0.30352962017059326\n",
      "Epoch 1306, Loss: 0.7160940617322922, Final Batch Loss: 0.24161238968372345\n",
      "Epoch 1307, Loss: 0.8607284426689148, Final Batch Loss: 0.36051851511001587\n",
      "Epoch 1308, Loss: 0.7091230154037476, Final Batch Loss: 0.2321649044752121\n",
      "Epoch 1309, Loss: 0.8005146384239197, Final Batch Loss: 0.2800035774707794\n",
      "Epoch 1310, Loss: 0.8223096430301666, Final Batch Loss: 0.32123303413391113\n",
      "Epoch 1311, Loss: 0.7911151051521301, Final Batch Loss: 0.25331762433052063\n",
      "Epoch 1312, Loss: 0.6400750130414963, Final Batch Loss: 0.21284599602222443\n",
      "Epoch 1313, Loss: 0.7980633527040482, Final Batch Loss: 0.323268860578537\n",
      "Epoch 1314, Loss: 0.8409062922000885, Final Batch Loss: 0.3110758364200592\n",
      "Epoch 1315, Loss: 0.8311321586370468, Final Batch Loss: 0.23585034906864166\n",
      "Epoch 1316, Loss: 0.9552282989025116, Final Batch Loss: 0.35534337162971497\n",
      "Epoch 1317, Loss: 0.720150887966156, Final Batch Loss: 0.24385890364646912\n",
      "Epoch 1318, Loss: 0.8140939474105835, Final Batch Loss: 0.2615757882595062\n",
      "Epoch 1319, Loss: 0.7230414748191833, Final Batch Loss: 0.2272946834564209\n",
      "Epoch 1320, Loss: 0.9050775170326233, Final Batch Loss: 0.3734346628189087\n",
      "Epoch 1321, Loss: 0.7925692647695541, Final Batch Loss: 0.20397670567035675\n",
      "Epoch 1322, Loss: 0.7371898889541626, Final Batch Loss: 0.21276111900806427\n",
      "Epoch 1323, Loss: 0.7512108683586121, Final Batch Loss: 0.19104060530662537\n",
      "Epoch 1324, Loss: 0.7929969727993011, Final Batch Loss: 0.3117355406284332\n",
      "Epoch 1325, Loss: 0.7864714413881302, Final Batch Loss: 0.23295433819293976\n",
      "Epoch 1326, Loss: 0.7484249919652939, Final Batch Loss: 0.2899388372898102\n",
      "Epoch 1327, Loss: 0.9615976512432098, Final Batch Loss: 0.3725632131099701\n",
      "Epoch 1328, Loss: 0.890976220369339, Final Batch Loss: 0.38726216554641724\n",
      "Epoch 1329, Loss: 0.7914958745241165, Final Batch Loss: 0.3246794044971466\n",
      "Epoch 1330, Loss: 0.7734773457050323, Final Batch Loss: 0.28755706548690796\n",
      "Epoch 1331, Loss: 0.7984062433242798, Final Batch Loss: 0.2608279287815094\n",
      "Epoch 1332, Loss: 0.7189400345087051, Final Batch Loss: 0.22029998898506165\n",
      "Epoch 1333, Loss: 0.7174818962812424, Final Batch Loss: 0.23493550717830658\n",
      "Epoch 1334, Loss: 0.7315817028284073, Final Batch Loss: 0.2023886889219284\n",
      "Epoch 1335, Loss: 0.861375629901886, Final Batch Loss: 0.38094562292099\n",
      "Epoch 1336, Loss: 0.74398373067379, Final Batch Loss: 0.22626377642154694\n",
      "Epoch 1337, Loss: 0.7843703925609589, Final Batch Loss: 0.30500471591949463\n",
      "Epoch 1338, Loss: 0.7300957590341568, Final Batch Loss: 0.2588193416595459\n",
      "Epoch 1339, Loss: 0.8307581543922424, Final Batch Loss: 0.2704614996910095\n",
      "Epoch 1340, Loss: 0.781505286693573, Final Batch Loss: 0.19039350748062134\n",
      "Epoch 1341, Loss: 0.795053020119667, Final Batch Loss: 0.3077385723590851\n",
      "Epoch 1342, Loss: 0.8176818042993546, Final Batch Loss: 0.3080229163169861\n",
      "Epoch 1343, Loss: 0.82717664539814, Final Batch Loss: 0.2992214262485504\n",
      "Epoch 1344, Loss: 0.730943962931633, Final Batch Loss: 0.2538049519062042\n",
      "Epoch 1345, Loss: 0.6919465363025665, Final Batch Loss: 0.20851454138755798\n",
      "Epoch 1346, Loss: 0.7971857637166977, Final Batch Loss: 0.24299438297748566\n",
      "Epoch 1347, Loss: 0.7136401683092117, Final Batch Loss: 0.21645991504192352\n",
      "Epoch 1348, Loss: 0.7209996581077576, Final Batch Loss: 0.28086695075035095\n",
      "Epoch 1349, Loss: 0.8638464957475662, Final Batch Loss: 0.26957616209983826\n",
      "Epoch 1350, Loss: 0.7430950701236725, Final Batch Loss: 0.1682819426059723\n",
      "Epoch 1351, Loss: 0.7193829417228699, Final Batch Loss: 0.22063376009464264\n",
      "Epoch 1352, Loss: 0.7111340761184692, Final Batch Loss: 0.20477166771888733\n",
      "Epoch 1353, Loss: 0.8693200945854187, Final Batch Loss: 0.376271516084671\n",
      "Epoch 1354, Loss: 0.7778726518154144, Final Batch Loss: 0.2139068841934204\n",
      "Epoch 1355, Loss: 0.6905104219913483, Final Batch Loss: 0.25573986768722534\n",
      "Epoch 1356, Loss: 0.9389215707778931, Final Batch Loss: 0.2798875570297241\n",
      "Epoch 1357, Loss: 0.7552202641963959, Final Batch Loss: 0.22163978219032288\n",
      "Epoch 1358, Loss: 0.7708049267530441, Final Batch Loss: 0.19450359046459198\n",
      "Epoch 1359, Loss: 0.7129158973693848, Final Batch Loss: 0.21872776746749878\n",
      "Epoch 1360, Loss: 0.700199082493782, Final Batch Loss: 0.2948853373527527\n",
      "Epoch 1361, Loss: 0.7970366030931473, Final Batch Loss: 0.2199791520833969\n",
      "Epoch 1362, Loss: 0.7253898829221725, Final Batch Loss: 0.19934706389904022\n",
      "Epoch 1363, Loss: 0.7859039455652237, Final Batch Loss: 0.22034336626529694\n",
      "Epoch 1364, Loss: 0.829512745141983, Final Batch Loss: 0.26289403438568115\n",
      "Epoch 1365, Loss: 0.7007731944322586, Final Batch Loss: 0.1982761174440384\n",
      "Epoch 1366, Loss: 0.749260738492012, Final Batch Loss: 0.22134314477443695\n",
      "Epoch 1367, Loss: 0.749546229839325, Final Batch Loss: 0.2831558883190155\n",
      "Epoch 1368, Loss: 0.8735293000936508, Final Batch Loss: 0.32715389132499695\n",
      "Epoch 1369, Loss: 0.8172674179077148, Final Batch Loss: 0.22550204396247864\n",
      "Epoch 1370, Loss: 0.7813444882631302, Final Batch Loss: 0.2801108658313751\n",
      "Epoch 1371, Loss: 0.855892539024353, Final Batch Loss: 0.318989098072052\n",
      "Epoch 1372, Loss: 0.8141365349292755, Final Batch Loss: 0.27785080671310425\n",
      "Epoch 1373, Loss: 0.8040598630905151, Final Batch Loss: 0.27473410964012146\n",
      "Epoch 1374, Loss: 0.6374595612287521, Final Batch Loss: 0.22836607694625854\n",
      "Epoch 1375, Loss: 0.8027983009815216, Final Batch Loss: 0.2793537974357605\n",
      "Epoch 1376, Loss: 0.7837862968444824, Final Batch Loss: 0.29035401344299316\n",
      "Epoch 1377, Loss: 0.7276832908391953, Final Batch Loss: 0.18822281062602997\n",
      "Epoch 1378, Loss: 0.7385950237512589, Final Batch Loss: 0.2708556056022644\n",
      "Epoch 1379, Loss: 0.7269332706928253, Final Batch Loss: 0.28525421023368835\n",
      "Epoch 1380, Loss: 0.6446234583854675, Final Batch Loss: 0.1518239974975586\n",
      "Epoch 1381, Loss: 0.8041123151779175, Final Batch Loss: 0.32776641845703125\n",
      "Epoch 1382, Loss: 0.6910912841558456, Final Batch Loss: 0.17443370819091797\n",
      "Epoch 1383, Loss: 0.7268004715442657, Final Batch Loss: 0.2138477861881256\n",
      "Epoch 1384, Loss: 0.7759155929088593, Final Batch Loss: 0.3111303448677063\n",
      "Epoch 1385, Loss: 0.7365215122699738, Final Batch Loss: 0.26917412877082825\n",
      "Epoch 1386, Loss: 0.8521115928888321, Final Batch Loss: 0.22870595753192902\n",
      "Epoch 1387, Loss: 0.8675310462713242, Final Batch Loss: 0.24755339324474335\n",
      "Epoch 1388, Loss: 0.721953809261322, Final Batch Loss: 0.2781670093536377\n",
      "Epoch 1389, Loss: 0.7052353620529175, Final Batch Loss: 0.26096251606941223\n",
      "Epoch 1390, Loss: 0.7093333005905151, Final Batch Loss: 0.24855458736419678\n",
      "Epoch 1391, Loss: 0.6684198826551437, Final Batch Loss: 0.21037612855434418\n",
      "Epoch 1392, Loss: 0.8392658084630966, Final Batch Loss: 0.303583025932312\n",
      "Epoch 1393, Loss: 0.887396365404129, Final Batch Loss: 0.305641233921051\n",
      "Epoch 1394, Loss: 0.7850363403558731, Final Batch Loss: 0.30635374784469604\n",
      "Epoch 1395, Loss: 0.7492564767599106, Final Batch Loss: 0.2120814472436905\n",
      "Epoch 1396, Loss: 0.7877859473228455, Final Batch Loss: 0.33089208602905273\n",
      "Epoch 1397, Loss: 0.6922156661748886, Final Batch Loss: 0.22576721012592316\n",
      "Epoch 1398, Loss: 0.7305012345314026, Final Batch Loss: 0.2662106156349182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1399, Loss: 0.7344994246959686, Final Batch Loss: 0.2105000764131546\n",
      "Epoch 1400, Loss: 0.6490370631217957, Final Batch Loss: 0.16971679031848907\n",
      "Epoch 1401, Loss: 0.7400412857532501, Final Batch Loss: 0.32150936126708984\n",
      "Epoch 1402, Loss: 0.7729923874139786, Final Batch Loss: 0.23877720534801483\n",
      "Epoch 1403, Loss: 0.796311765909195, Final Batch Loss: 0.23936167359352112\n",
      "Epoch 1404, Loss: 0.8859139680862427, Final Batch Loss: 0.36198610067367554\n",
      "Epoch 1405, Loss: 0.6561246365308762, Final Batch Loss: 0.24497994780540466\n",
      "Epoch 1406, Loss: 0.9367095232009888, Final Batch Loss: 0.3816799521446228\n",
      "Epoch 1407, Loss: 0.7174969166517258, Final Batch Loss: 0.19884368777275085\n",
      "Epoch 1408, Loss: 0.8074663281440735, Final Batch Loss: 0.23584550619125366\n",
      "Epoch 1409, Loss: 0.6617847084999084, Final Batch Loss: 0.2619978189468384\n",
      "Epoch 1410, Loss: 0.7703942209482193, Final Batch Loss: 0.2574719488620758\n",
      "Epoch 1411, Loss: 0.823366105556488, Final Batch Loss: 0.24914714694023132\n",
      "Epoch 1412, Loss: 0.799290731549263, Final Batch Loss: 0.2948119044303894\n",
      "Epoch 1413, Loss: 0.8002335429191589, Final Batch Loss: 0.22720304131507874\n",
      "Epoch 1414, Loss: 0.765703022480011, Final Batch Loss: 0.25561195611953735\n",
      "Epoch 1415, Loss: 0.6760146021842957, Final Batch Loss: 0.2440967708826065\n",
      "Epoch 1416, Loss: 0.716942235827446, Final Batch Loss: 0.25432634353637695\n",
      "Epoch 1417, Loss: 0.6323544234037399, Final Batch Loss: 0.18284262716770172\n",
      "Epoch 1418, Loss: 0.6681641787290573, Final Batch Loss: 0.20320695638656616\n",
      "Epoch 1419, Loss: 0.7559615969657898, Final Batch Loss: 0.22265681624412537\n",
      "Epoch 1420, Loss: 0.7337213307619095, Final Batch Loss: 0.27701258659362793\n",
      "Epoch 1421, Loss: 0.829929769039154, Final Batch Loss: 0.2605363428592682\n",
      "Epoch 1422, Loss: 0.8847236335277557, Final Batch Loss: 0.26602694392204285\n",
      "Epoch 1423, Loss: 0.6435062289237976, Final Batch Loss: 0.2102012187242508\n",
      "Epoch 1424, Loss: 0.7502107471227646, Final Batch Loss: 0.2539793848991394\n",
      "Epoch 1425, Loss: 0.8078725636005402, Final Batch Loss: 0.30551087856292725\n",
      "Epoch 1426, Loss: 0.6705551743507385, Final Batch Loss: 0.19935888051986694\n",
      "Epoch 1427, Loss: 0.8509311228990555, Final Batch Loss: 0.4092119038105011\n",
      "Epoch 1428, Loss: 0.746127188205719, Final Batch Loss: 0.24081957340240479\n",
      "Epoch 1429, Loss: 0.7935483008623123, Final Batch Loss: 0.3331029415130615\n",
      "Epoch 1430, Loss: 0.754422515630722, Final Batch Loss: 0.29227709770202637\n",
      "Epoch 1431, Loss: 0.6861692816019058, Final Batch Loss: 0.18686170876026154\n",
      "Epoch 1432, Loss: 0.7942183017730713, Final Batch Loss: 0.30364957451820374\n",
      "Epoch 1433, Loss: 0.7860988974571228, Final Batch Loss: 0.31367623805999756\n",
      "Epoch 1434, Loss: 0.8140087723731995, Final Batch Loss: 0.25809797644615173\n",
      "Epoch 1435, Loss: 0.7285071015357971, Final Batch Loss: 0.2621515393257141\n",
      "Epoch 1436, Loss: 0.7895419299602509, Final Batch Loss: 0.2889527976512909\n",
      "Epoch 1437, Loss: 0.7279299646615982, Final Batch Loss: 0.2642139196395874\n",
      "Epoch 1438, Loss: 0.7226984351873398, Final Batch Loss: 0.2521417438983917\n",
      "Epoch 1439, Loss: 0.8004003912210464, Final Batch Loss: 0.3123396635055542\n",
      "Epoch 1440, Loss: 0.805137038230896, Final Batch Loss: 0.2593207359313965\n",
      "Epoch 1441, Loss: 0.6754125505685806, Final Batch Loss: 0.22097282111644745\n",
      "Epoch 1442, Loss: 0.7223271727561951, Final Batch Loss: 0.22830551862716675\n",
      "Epoch 1443, Loss: 0.7836633622646332, Final Batch Loss: 0.26099348068237305\n",
      "Epoch 1444, Loss: 0.7070122063159943, Final Batch Loss: 0.2040099799633026\n",
      "Epoch 1445, Loss: 0.6598447114229202, Final Batch Loss: 0.1983281522989273\n",
      "Epoch 1446, Loss: 0.7433984726667404, Final Batch Loss: 0.13187240064144135\n",
      "Epoch 1447, Loss: 0.7513760030269623, Final Batch Loss: 0.24789440631866455\n",
      "Epoch 1448, Loss: 0.7519790083169937, Final Batch Loss: 0.23152008652687073\n",
      "Epoch 1449, Loss: 0.8081532716751099, Final Batch Loss: 0.25837549567222595\n",
      "Epoch 1450, Loss: 0.6578321158885956, Final Batch Loss: 0.22856636345386505\n",
      "Epoch 1451, Loss: 0.7501661032438278, Final Batch Loss: 0.24366505444049835\n",
      "Epoch 1452, Loss: 0.6759618371725082, Final Batch Loss: 0.2530369460582733\n",
      "Epoch 1453, Loss: 0.8047395199537277, Final Batch Loss: 0.21285583078861237\n",
      "Epoch 1454, Loss: 0.6962478309869766, Final Batch Loss: 0.29248374700546265\n",
      "Epoch 1455, Loss: 0.7506344765424728, Final Batch Loss: 0.30891871452331543\n",
      "Epoch 1456, Loss: 0.6360854655504227, Final Batch Loss: 0.1913476586341858\n",
      "Epoch 1457, Loss: 0.707096740603447, Final Batch Loss: 0.17557168006896973\n",
      "Epoch 1458, Loss: 0.6981281787157059, Final Batch Loss: 0.1909654587507248\n",
      "Epoch 1459, Loss: 0.7314549833536148, Final Batch Loss: 0.2531554698944092\n",
      "Epoch 1460, Loss: 0.7302346080541611, Final Batch Loss: 0.21302352845668793\n",
      "Epoch 1461, Loss: 0.681813582777977, Final Batch Loss: 0.21996234357357025\n",
      "Epoch 1462, Loss: 0.7135036438703537, Final Batch Loss: 0.18181094527244568\n",
      "Epoch 1463, Loss: 0.7882282584905624, Final Batch Loss: 0.20031099021434784\n",
      "Epoch 1464, Loss: 0.6952526718378067, Final Batch Loss: 0.2296099215745926\n",
      "Epoch 1465, Loss: 0.7470071464776993, Final Batch Loss: 0.3119313418865204\n",
      "Epoch 1466, Loss: 0.7008060067892075, Final Batch Loss: 0.2659490704536438\n",
      "Epoch 1467, Loss: 0.6921507716178894, Final Batch Loss: 0.2379796952009201\n",
      "Epoch 1468, Loss: 0.6772628724575043, Final Batch Loss: 0.23322957754135132\n",
      "Epoch 1469, Loss: 0.7423340678215027, Final Batch Loss: 0.29031267762184143\n",
      "Epoch 1470, Loss: 0.6565171927213669, Final Batch Loss: 0.24227048456668854\n",
      "Epoch 1471, Loss: 0.7958046048879623, Final Batch Loss: 0.29897621273994446\n",
      "Epoch 1472, Loss: 0.7657140344381332, Final Batch Loss: 0.31569916009902954\n",
      "Epoch 1473, Loss: 0.6357007324695587, Final Batch Loss: 0.24197585880756378\n",
      "Epoch 1474, Loss: 0.7463143169879913, Final Batch Loss: 0.2924446165561676\n",
      "Epoch 1475, Loss: 0.7836861312389374, Final Batch Loss: 0.2886643707752228\n",
      "Epoch 1476, Loss: 0.7944966107606888, Final Batch Loss: 0.32820865511894226\n",
      "Epoch 1477, Loss: 0.6778886020183563, Final Batch Loss: 0.23415283858776093\n",
      "Epoch 1478, Loss: 0.8800914585590363, Final Batch Loss: 0.3084704279899597\n",
      "Epoch 1479, Loss: 0.6587552130222321, Final Batch Loss: 0.27208077907562256\n",
      "Epoch 1480, Loss: 0.6880506426095963, Final Batch Loss: 0.2379971593618393\n",
      "Epoch 1481, Loss: 0.7650260627269745, Final Batch Loss: 0.24432267248630524\n",
      "Epoch 1482, Loss: 0.7608145028352737, Final Batch Loss: 0.19355584681034088\n",
      "Epoch 1483, Loss: 0.6868767738342285, Final Batch Loss: 0.2439056634902954\n",
      "Epoch 1484, Loss: 0.8129749000072479, Final Batch Loss: 0.2646061182022095\n",
      "Epoch 1485, Loss: 0.6187076270580292, Final Batch Loss: 0.20246949791908264\n",
      "Epoch 1486, Loss: 0.7613999843597412, Final Batch Loss: 0.2645498216152191\n",
      "Epoch 1487, Loss: 0.6753621101379395, Final Batch Loss: 0.23981845378875732\n",
      "Epoch 1488, Loss: 0.7613931447267532, Final Batch Loss: 0.21643532812595367\n",
      "Epoch 1489, Loss: 0.7755238860845566, Final Batch Loss: 0.24937836825847626\n",
      "Epoch 1490, Loss: 0.7677135318517685, Final Batch Loss: 0.2665182650089264\n",
      "Epoch 1491, Loss: 0.6311142593622208, Final Batch Loss: 0.20685270428657532\n",
      "Epoch 1492, Loss: 0.7523611783981323, Final Batch Loss: 0.19303035736083984\n",
      "Epoch 1493, Loss: 0.6941546946763992, Final Batch Loss: 0.23736019432544708\n",
      "Epoch 1494, Loss: 0.7879935652017593, Final Batch Loss: 0.2806648313999176\n",
      "Epoch 1495, Loss: 0.7512082457542419, Final Batch Loss: 0.2755429148674011\n",
      "Epoch 1496, Loss: 0.7100068777799606, Final Batch Loss: 0.21060357987880707\n",
      "Epoch 1497, Loss: 0.802953913807869, Final Batch Loss: 0.2770274579524994\n",
      "Epoch 1498, Loss: 0.753751739859581, Final Batch Loss: 0.2517520785331726\n",
      "Epoch 1499, Loss: 0.7485100328922272, Final Batch Loss: 0.27064642310142517\n",
      "Epoch 1500, Loss: 0.7231334745883942, Final Batch Loss: 0.22724007070064545\n",
      "Epoch 1501, Loss: 0.7097245752811432, Final Batch Loss: 0.20113138854503632\n",
      "Epoch 1502, Loss: 0.7959233522415161, Final Batch Loss: 0.31289586424827576\n",
      "Epoch 1503, Loss: 0.7102975845336914, Final Batch Loss: 0.19660308957099915\n",
      "Epoch 1504, Loss: 0.7395302057266235, Final Batch Loss: 0.2710946798324585\n",
      "Epoch 1505, Loss: 0.8301247954368591, Final Batch Loss: 0.30098384618759155\n",
      "Epoch 1506, Loss: 0.7471837848424911, Final Batch Loss: 0.2863326966762543\n",
      "Epoch 1507, Loss: 0.7811301648616791, Final Batch Loss: 0.31207478046417236\n",
      "Epoch 1508, Loss: 0.8415501713752747, Final Batch Loss: 0.3014318645000458\n",
      "Epoch 1509, Loss: 0.7298647463321686, Final Batch Loss: 0.207878977060318\n",
      "Epoch 1510, Loss: 0.6334417760372162, Final Batch Loss: 0.2150585651397705\n",
      "Epoch 1511, Loss: 0.688270628452301, Final Batch Loss: 0.1695994734764099\n",
      "Epoch 1512, Loss: 0.6965228617191315, Final Batch Loss: 0.19017452001571655\n",
      "Epoch 1513, Loss: 0.6876078546047211, Final Batch Loss: 0.19162751734256744\n",
      "Epoch 1514, Loss: 0.6918887794017792, Final Batch Loss: 0.23504161834716797\n",
      "Epoch 1515, Loss: 0.6790021508932114, Final Batch Loss: 0.20839641988277435\n",
      "Epoch 1516, Loss: 0.6874755322933197, Final Batch Loss: 0.24976471066474915\n",
      "Epoch 1517, Loss: 0.6541603207588196, Final Batch Loss: 0.2874182164669037\n",
      "Epoch 1518, Loss: 0.7267463058233261, Final Batch Loss: 0.28717854619026184\n",
      "Epoch 1519, Loss: 0.6737072020769119, Final Batch Loss: 0.22187440097332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1520, Loss: 0.6615228056907654, Final Batch Loss: 0.1612759679555893\n",
      "Epoch 1521, Loss: 0.7342938333749771, Final Batch Loss: 0.2071278840303421\n",
      "Epoch 1522, Loss: 0.7906106263399124, Final Batch Loss: 0.2614729106426239\n",
      "Epoch 1523, Loss: 0.7327043861150742, Final Batch Loss: 0.25462600588798523\n",
      "Epoch 1524, Loss: 0.6795202493667603, Final Batch Loss: 0.2034231573343277\n",
      "Epoch 1525, Loss: 0.713075190782547, Final Batch Loss: 0.20270311832427979\n",
      "Epoch 1526, Loss: 0.6603453755378723, Final Batch Loss: 0.22398914396762848\n",
      "Epoch 1527, Loss: 0.6325282901525497, Final Batch Loss: 0.17381888628005981\n",
      "Epoch 1528, Loss: 0.819095566868782, Final Batch Loss: 0.2782396972179413\n",
      "Epoch 1529, Loss: 0.6931159347295761, Final Batch Loss: 0.25363168120384216\n",
      "Epoch 1530, Loss: 0.7893318086862564, Final Batch Loss: 0.3504463732242584\n",
      "Epoch 1531, Loss: 0.5678800195455551, Final Batch Loss: 0.14885619282722473\n",
      "Epoch 1532, Loss: 0.7690176963806152, Final Batch Loss: 0.2652030289173126\n",
      "Epoch 1533, Loss: 0.6271510869264603, Final Batch Loss: 0.19066469371318817\n",
      "Epoch 1534, Loss: 0.619821310043335, Final Batch Loss: 0.19879692792892456\n",
      "Epoch 1535, Loss: 0.622382789850235, Final Batch Loss: 0.19358673691749573\n",
      "Epoch 1536, Loss: 0.6937184184789658, Final Batch Loss: 0.1852533519268036\n",
      "Epoch 1537, Loss: 0.6809221059083939, Final Batch Loss: 0.2207452803850174\n",
      "Epoch 1538, Loss: 0.6240092515945435, Final Batch Loss: 0.1722453385591507\n",
      "Epoch 1539, Loss: 0.7443278431892395, Final Batch Loss: 0.2515273988246918\n",
      "Epoch 1540, Loss: 0.5483304262161255, Final Batch Loss: 0.16864998638629913\n",
      "Epoch 1541, Loss: 0.7179179191589355, Final Batch Loss: 0.28049159049987793\n",
      "Epoch 1542, Loss: 0.6143012940883636, Final Batch Loss: 0.1734825223684311\n",
      "Epoch 1543, Loss: 0.6277798116207123, Final Batch Loss: 0.2029626965522766\n",
      "Epoch 1544, Loss: 0.7993765473365784, Final Batch Loss: 0.296493798494339\n",
      "Epoch 1545, Loss: 0.6789923757314682, Final Batch Loss: 0.23606859147548676\n",
      "Epoch 1546, Loss: 0.7560165375471115, Final Batch Loss: 0.2561471164226532\n",
      "Epoch 1547, Loss: 0.6746412813663483, Final Batch Loss: 0.28360000252723694\n",
      "Epoch 1548, Loss: 0.7318178564310074, Final Batch Loss: 0.24741105735301971\n",
      "Epoch 1549, Loss: 0.7495265305042267, Final Batch Loss: 0.26364678144454956\n",
      "Epoch 1550, Loss: 0.851907804608345, Final Batch Loss: 0.34572863578796387\n",
      "Epoch 1551, Loss: 0.7850959748029709, Final Batch Loss: 0.22838819026947021\n",
      "Epoch 1552, Loss: 0.6979970037937164, Final Batch Loss: 0.24036459624767303\n",
      "Epoch 1553, Loss: 0.6726072877645493, Final Batch Loss: 0.22531796991825104\n",
      "Epoch 1554, Loss: 0.5741650760173798, Final Batch Loss: 0.16907624900341034\n",
      "Epoch 1555, Loss: 0.8102585673332214, Final Batch Loss: 0.3324235677719116\n",
      "Epoch 1556, Loss: 0.7497332245111465, Final Batch Loss: 0.29120713472366333\n",
      "Epoch 1557, Loss: 0.7824463546276093, Final Batch Loss: 0.24230250716209412\n",
      "Epoch 1558, Loss: 0.742903858423233, Final Batch Loss: 0.2263699173927307\n",
      "Epoch 1559, Loss: 0.7879801690578461, Final Batch Loss: 0.20112906396389008\n",
      "Epoch 1560, Loss: 0.6997308731079102, Final Batch Loss: 0.264404833316803\n",
      "Epoch 1561, Loss: 0.6581602543592453, Final Batch Loss: 0.25173377990722656\n",
      "Epoch 1562, Loss: 0.6588806658983231, Final Batch Loss: 0.21887284517288208\n",
      "Epoch 1563, Loss: 0.8125838935375214, Final Batch Loss: 0.2715826630592346\n",
      "Epoch 1564, Loss: 0.8581317961215973, Final Batch Loss: 0.3872520923614502\n",
      "Epoch 1565, Loss: 0.8332183659076691, Final Batch Loss: 0.2578737139701843\n",
      "Epoch 1566, Loss: 0.839270144701004, Final Batch Loss: 0.28767240047454834\n",
      "Epoch 1567, Loss: 0.6574961245059967, Final Batch Loss: 0.21124190092086792\n",
      "Epoch 1568, Loss: 0.6936430037021637, Final Batch Loss: 0.2412833869457245\n",
      "Epoch 1569, Loss: 0.7733675837516785, Final Batch Loss: 0.21785461902618408\n",
      "Epoch 1570, Loss: 0.7219279259443283, Final Batch Loss: 0.26728203892707825\n",
      "Epoch 1571, Loss: 0.6348565220832825, Final Batch Loss: 0.23668478429317474\n",
      "Epoch 1572, Loss: 0.8194707334041595, Final Batch Loss: 0.25658899545669556\n",
      "Epoch 1573, Loss: 0.6819599568843842, Final Batch Loss: 0.2755763828754425\n",
      "Epoch 1574, Loss: 0.6988514810800552, Final Batch Loss: 0.21282073855400085\n",
      "Epoch 1575, Loss: 0.6011056900024414, Final Batch Loss: 0.16971369087696075\n",
      "Epoch 1576, Loss: 0.7333753854036331, Final Batch Loss: 0.29342222213745117\n",
      "Epoch 1577, Loss: 0.6613203287124634, Final Batch Loss: 0.19284267723560333\n",
      "Epoch 1578, Loss: 0.7036155611276627, Final Batch Loss: 0.1991262137889862\n",
      "Epoch 1579, Loss: 0.7394299209117889, Final Batch Loss: 0.3203825354576111\n",
      "Epoch 1580, Loss: 0.5868871212005615, Final Batch Loss: 0.15566585958003998\n",
      "Epoch 1581, Loss: 0.6544491797685623, Final Batch Loss: 0.21422061324119568\n",
      "Epoch 1582, Loss: 0.7013266533613205, Final Batch Loss: 0.3665042817592621\n",
      "Epoch 1583, Loss: 0.5946169644594193, Final Batch Loss: 0.2531081438064575\n",
      "Epoch 1584, Loss: 0.674990102648735, Final Batch Loss: 0.2281215339899063\n",
      "Epoch 1585, Loss: 0.7695756554603577, Final Batch Loss: 0.21642938256263733\n",
      "Epoch 1586, Loss: 0.7209069132804871, Final Batch Loss: 0.22933053970336914\n",
      "Epoch 1587, Loss: 0.6622811555862427, Final Batch Loss: 0.20332179963588715\n",
      "Epoch 1588, Loss: 0.7155003547668457, Final Batch Loss: 0.2558191120624542\n",
      "Epoch 1589, Loss: 0.6331129819154739, Final Batch Loss: 0.2066734880208969\n",
      "Epoch 1590, Loss: 0.7120962888002396, Final Batch Loss: 0.22993062436580658\n",
      "Epoch 1591, Loss: 0.5609217584133148, Final Batch Loss: 0.2183302938938141\n",
      "Epoch 1592, Loss: 0.7565418034791946, Final Batch Loss: 0.25548940896987915\n",
      "Epoch 1593, Loss: 0.701820969581604, Final Batch Loss: 0.3304803669452667\n",
      "Epoch 1594, Loss: 0.6608948260545731, Final Batch Loss: 0.2058844119310379\n",
      "Epoch 1595, Loss: 0.6459873765707016, Final Batch Loss: 0.1838119477033615\n",
      "Epoch 1596, Loss: 0.6727763116359711, Final Batch Loss: 0.23678982257843018\n",
      "Epoch 1597, Loss: 0.6210765242576599, Final Batch Loss: 0.2247181236743927\n",
      "Epoch 1598, Loss: 0.5513050556182861, Final Batch Loss: 0.1506531536579132\n",
      "Epoch 1599, Loss: 0.641940250992775, Final Batch Loss: 0.2840113639831543\n",
      "Epoch 1600, Loss: 0.6593012064695358, Final Batch Loss: 0.24313634634017944\n",
      "Epoch 1601, Loss: 0.6705185174942017, Final Batch Loss: 0.2718648910522461\n",
      "Epoch 1602, Loss: 0.6444419324398041, Final Batch Loss: 0.21540950238704681\n",
      "Epoch 1603, Loss: 0.6866009682416916, Final Batch Loss: 0.23312000930309296\n",
      "Epoch 1604, Loss: 0.7161073237657547, Final Batch Loss: 0.2636171579360962\n",
      "Epoch 1605, Loss: 0.6914816796779633, Final Batch Loss: 0.18200789391994476\n",
      "Epoch 1606, Loss: 0.7275985032320023, Final Batch Loss: 0.22828388214111328\n",
      "Epoch 1607, Loss: 0.7004338204860687, Final Batch Loss: 0.22492119669914246\n",
      "Epoch 1608, Loss: 0.8403201252222061, Final Batch Loss: 0.4038671553134918\n",
      "Epoch 1609, Loss: 0.6281747967004776, Final Batch Loss: 0.23492977023124695\n",
      "Epoch 1610, Loss: 0.5842116326093674, Final Batch Loss: 0.16787587106227875\n",
      "Epoch 1611, Loss: 0.7037410289049149, Final Batch Loss: 0.2700463831424713\n",
      "Epoch 1612, Loss: 0.6352034211158752, Final Batch Loss: 0.17798003554344177\n",
      "Epoch 1613, Loss: 0.6665936559438705, Final Batch Loss: 0.20277619361877441\n",
      "Epoch 1614, Loss: 0.6994696855545044, Final Batch Loss: 0.25115570425987244\n",
      "Epoch 1615, Loss: 0.6525559425354004, Final Batch Loss: 0.24264757335186005\n",
      "Epoch 1616, Loss: 0.6367112100124359, Final Batch Loss: 0.21816445887088776\n",
      "Epoch 1617, Loss: 0.6350902020931244, Final Batch Loss: 0.24075330793857574\n",
      "Epoch 1618, Loss: 0.6795489937067032, Final Batch Loss: 0.20735451579093933\n",
      "Epoch 1619, Loss: 0.7507072538137436, Final Batch Loss: 0.22832715511322021\n",
      "Epoch 1620, Loss: 0.6659158915281296, Final Batch Loss: 0.27484890818595886\n",
      "Epoch 1621, Loss: 0.6341711133718491, Final Batch Loss: 0.17226499319076538\n",
      "Epoch 1622, Loss: 0.6943326592445374, Final Batch Loss: 0.17217090725898743\n",
      "Epoch 1623, Loss: 0.6927695721387863, Final Batch Loss: 0.22044192254543304\n",
      "Epoch 1624, Loss: 0.7840058207511902, Final Batch Loss: 0.2925897538661957\n",
      "Epoch 1625, Loss: 0.6292916089296341, Final Batch Loss: 0.23871468007564545\n",
      "Epoch 1626, Loss: 0.744650274515152, Final Batch Loss: 0.2372615784406662\n",
      "Epoch 1627, Loss: 0.7226915210485458, Final Batch Loss: 0.167380228638649\n",
      "Epoch 1628, Loss: 0.7499870657920837, Final Batch Loss: 0.1536373496055603\n",
      "Epoch 1629, Loss: 0.708904504776001, Final Batch Loss: 0.25812309980392456\n",
      "Epoch 1630, Loss: 0.7429520934820175, Final Batch Loss: 0.21851320564746857\n",
      "Epoch 1631, Loss: 0.7421535551548004, Final Batch Loss: 0.28225424885749817\n",
      "Epoch 1632, Loss: 0.6543235778808594, Final Batch Loss: 0.2451937347650528\n",
      "Epoch 1633, Loss: 0.8203023970127106, Final Batch Loss: 0.30662357807159424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1634, Loss: 0.7417278587818146, Final Batch Loss: 0.21732079982757568\n",
      "Epoch 1635, Loss: 0.6894677728414536, Final Batch Loss: 0.2161075323820114\n",
      "Epoch 1636, Loss: 0.6097261905670166, Final Batch Loss: 0.20275050401687622\n",
      "Epoch 1637, Loss: 0.7490654587745667, Final Batch Loss: 0.327280730009079\n",
      "Epoch 1638, Loss: 0.7035030424594879, Final Batch Loss: 0.2216460406780243\n",
      "Epoch 1639, Loss: 0.7421223968267441, Final Batch Loss: 0.30890944600105286\n",
      "Epoch 1640, Loss: 0.7928261458873749, Final Batch Loss: 0.3231029212474823\n",
      "Epoch 1641, Loss: 0.7896120101213455, Final Batch Loss: 0.3387773633003235\n",
      "Epoch 1642, Loss: 0.7284284830093384, Final Batch Loss: 0.1912912279367447\n",
      "Epoch 1643, Loss: 0.8284847289323807, Final Batch Loss: 0.35277390480041504\n",
      "Epoch 1644, Loss: 0.627349853515625, Final Batch Loss: 0.20671434700489044\n",
      "Epoch 1645, Loss: 0.624810665845871, Final Batch Loss: 0.21604028344154358\n",
      "Epoch 1646, Loss: 0.6980214864015579, Final Batch Loss: 0.2663494646549225\n",
      "Epoch 1647, Loss: 0.7332619726657867, Final Batch Loss: 0.21862578392028809\n",
      "Epoch 1648, Loss: 0.6857983022928238, Final Batch Loss: 0.2026166319847107\n",
      "Epoch 1649, Loss: 0.6612608730792999, Final Batch Loss: 0.2166324406862259\n",
      "Epoch 1650, Loss: 0.7092941850423813, Final Batch Loss: 0.30094459652900696\n",
      "Epoch 1651, Loss: 0.6943899244070053, Final Batch Loss: 0.27870652079582214\n",
      "Epoch 1652, Loss: 0.7164885252714157, Final Batch Loss: 0.2674276530742645\n",
      "Epoch 1653, Loss: 0.6884975284337997, Final Batch Loss: 0.20575635135173798\n",
      "Epoch 1654, Loss: 0.6463183164596558, Final Batch Loss: 0.205049529671669\n",
      "Epoch 1655, Loss: 0.8282274752855301, Final Batch Loss: 0.25969189405441284\n",
      "Epoch 1656, Loss: 0.580255463719368, Final Batch Loss: 0.1634644865989685\n",
      "Epoch 1657, Loss: 0.6953681260347366, Final Batch Loss: 0.28143051266670227\n",
      "Epoch 1658, Loss: 0.586303785443306, Final Batch Loss: 0.1770671010017395\n",
      "Epoch 1659, Loss: 0.6962878555059433, Final Batch Loss: 0.26961541175842285\n",
      "Epoch 1660, Loss: 0.6062882393598557, Final Batch Loss: 0.1859399527311325\n",
      "Epoch 1661, Loss: 0.7525554746389389, Final Batch Loss: 0.332352876663208\n",
      "Epoch 1662, Loss: 0.6865820586681366, Final Batch Loss: 0.2594950795173645\n",
      "Epoch 1663, Loss: 0.6523902118206024, Final Batch Loss: 0.2442604899406433\n",
      "Epoch 1664, Loss: 0.6341670602560043, Final Batch Loss: 0.2657601237297058\n",
      "Epoch 1665, Loss: 0.7902806401252747, Final Batch Loss: 0.2836313545703888\n",
      "Epoch 1666, Loss: 0.5810584425926208, Final Batch Loss: 0.1946873515844345\n",
      "Epoch 1667, Loss: 0.6195578873157501, Final Batch Loss: 0.23715266585350037\n",
      "Epoch 1668, Loss: 0.7890661358833313, Final Batch Loss: 0.273132860660553\n",
      "Epoch 1669, Loss: 0.5972093641757965, Final Batch Loss: 0.19619983434677124\n",
      "Epoch 1670, Loss: 0.7315203249454498, Final Batch Loss: 0.16954156756401062\n",
      "Epoch 1671, Loss: 0.687895804643631, Final Batch Loss: 0.17775686085224152\n",
      "Epoch 1672, Loss: 0.7054188400506973, Final Batch Loss: 0.16395586729049683\n",
      "Epoch 1673, Loss: 0.6503975838422775, Final Batch Loss: 0.2363685667514801\n",
      "Epoch 1674, Loss: 0.7069722414016724, Final Batch Loss: 0.24768532812595367\n",
      "Epoch 1675, Loss: 0.6546716243028641, Final Batch Loss: 0.24702921509742737\n",
      "Epoch 1676, Loss: 0.6634277254343033, Final Batch Loss: 0.24455852806568146\n",
      "Epoch 1677, Loss: 0.6655326932668686, Final Batch Loss: 0.17104953527450562\n",
      "Epoch 1678, Loss: 0.6570766121149063, Final Batch Loss: 0.18854758143424988\n",
      "Epoch 1679, Loss: 0.7376829236745834, Final Batch Loss: 0.2947654128074646\n",
      "Epoch 1680, Loss: 0.628461942076683, Final Batch Loss: 0.2079877108335495\n",
      "Epoch 1681, Loss: 0.6422000974416733, Final Batch Loss: 0.1946626454591751\n",
      "Epoch 1682, Loss: 0.53628870844841, Final Batch Loss: 0.1287834495306015\n",
      "Epoch 1683, Loss: 0.6145928353071213, Final Batch Loss: 0.19937606155872345\n",
      "Epoch 1684, Loss: 0.586840346455574, Final Batch Loss: 0.1716039478778839\n",
      "Epoch 1685, Loss: 0.7038692682981491, Final Batch Loss: 0.22400295734405518\n",
      "Epoch 1686, Loss: 0.647278219461441, Final Batch Loss: 0.18979336321353912\n",
      "Epoch 1687, Loss: 0.577283188700676, Final Batch Loss: 0.17288461327552795\n",
      "Epoch 1688, Loss: 0.7060440927743912, Final Batch Loss: 0.2319827675819397\n",
      "Epoch 1689, Loss: 0.5838954299688339, Final Batch Loss: 0.14102447032928467\n",
      "Epoch 1690, Loss: 0.5481715500354767, Final Batch Loss: 0.17205165326595306\n",
      "Epoch 1691, Loss: 0.6987247169017792, Final Batch Loss: 0.23883409798145294\n",
      "Epoch 1692, Loss: 0.6222902834415436, Final Batch Loss: 0.22700032591819763\n",
      "Epoch 1693, Loss: 0.7610499858856201, Final Batch Loss: 0.29659220576286316\n",
      "Epoch 1694, Loss: 0.654007762670517, Final Batch Loss: 0.20723727345466614\n",
      "Epoch 1695, Loss: 0.5638201087713242, Final Batch Loss: 0.15887269377708435\n",
      "Epoch 1696, Loss: 0.6889251619577408, Final Batch Loss: 0.2531357407569885\n",
      "Epoch 1697, Loss: 0.5776004195213318, Final Batch Loss: 0.19037799537181854\n",
      "Epoch 1698, Loss: 0.6815325915813446, Final Batch Loss: 0.1728743016719818\n",
      "Epoch 1699, Loss: 0.6427409648895264, Final Batch Loss: 0.20890402793884277\n",
      "Epoch 1700, Loss: 0.6572463363409042, Final Batch Loss: 0.18291574716567993\n",
      "Epoch 1701, Loss: 0.6062236875295639, Final Batch Loss: 0.2383269965648651\n",
      "Epoch 1702, Loss: 0.5359368473291397, Final Batch Loss: 0.18558330833911896\n",
      "Epoch 1703, Loss: 0.601076602935791, Final Batch Loss: 0.17815060913562775\n",
      "Epoch 1704, Loss: 0.6321871131658554, Final Batch Loss: 0.19676946103572845\n",
      "Epoch 1705, Loss: 0.5781833529472351, Final Batch Loss: 0.16322028636932373\n",
      "Epoch 1706, Loss: 0.558037519454956, Final Batch Loss: 0.18234212696552277\n",
      "Epoch 1707, Loss: 0.6723664402961731, Final Batch Loss: 0.17818908393383026\n",
      "Epoch 1708, Loss: 0.5155532956123352, Final Batch Loss: 0.1483784317970276\n",
      "Epoch 1709, Loss: 0.5893363207578659, Final Batch Loss: 0.18642933666706085\n",
      "Epoch 1710, Loss: 0.6031918674707413, Final Batch Loss: 0.22382645308971405\n",
      "Epoch 1711, Loss: 0.560518279671669, Final Batch Loss: 0.20251773297786713\n",
      "Epoch 1712, Loss: 0.621278926730156, Final Batch Loss: 0.18828405439853668\n",
      "Epoch 1713, Loss: 0.739082396030426, Final Batch Loss: 0.3411048352718353\n",
      "Epoch 1714, Loss: 0.621685802936554, Final Batch Loss: 0.22439849376678467\n",
      "Epoch 1715, Loss: 0.628560483455658, Final Batch Loss: 0.257000207901001\n",
      "Epoch 1716, Loss: 0.7861217111349106, Final Batch Loss: 0.41121363639831543\n",
      "Epoch 1717, Loss: 0.6061891466379166, Final Batch Loss: 0.2123992145061493\n",
      "Epoch 1718, Loss: 0.6064392775297165, Final Batch Loss: 0.1674506813287735\n",
      "Epoch 1719, Loss: 0.6859916597604752, Final Batch Loss: 0.2394963651895523\n",
      "Epoch 1720, Loss: 0.6189680993556976, Final Batch Loss: 0.2522326409816742\n",
      "Epoch 1721, Loss: 0.6615712940692902, Final Batch Loss: 0.2637106478214264\n",
      "Epoch 1722, Loss: 0.6269759833812714, Final Batch Loss: 0.23324838280677795\n",
      "Epoch 1723, Loss: 0.6733378916978836, Final Batch Loss: 0.26082944869995117\n",
      "Epoch 1724, Loss: 0.6761986464262009, Final Batch Loss: 0.22199630737304688\n",
      "Epoch 1725, Loss: 0.5955751091241837, Final Batch Loss: 0.15821050107479095\n",
      "Epoch 1726, Loss: 0.6305260360240936, Final Batch Loss: 0.20492488145828247\n",
      "Epoch 1727, Loss: 0.6235698312520981, Final Batch Loss: 0.22874762117862701\n",
      "Epoch 1728, Loss: 0.6670303046703339, Final Batch Loss: 0.19210252165794373\n",
      "Epoch 1729, Loss: 0.6155078709125519, Final Batch Loss: 0.21449320018291473\n",
      "Epoch 1730, Loss: 0.7411207407712936, Final Batch Loss: 0.3549063801765442\n",
      "Epoch 1731, Loss: 0.7373108714818954, Final Batch Loss: 0.23753921687602997\n",
      "Epoch 1732, Loss: 0.6531912982463837, Final Batch Loss: 0.2067974954843521\n",
      "Epoch 1733, Loss: 0.6008849591016769, Final Batch Loss: 0.17447218298912048\n",
      "Epoch 1734, Loss: 0.6020039021968842, Final Batch Loss: 0.1579713523387909\n",
      "Epoch 1735, Loss: 0.7004048228263855, Final Batch Loss: 0.2230709344148636\n",
      "Epoch 1736, Loss: 0.6633390337228775, Final Batch Loss: 0.17903268337249756\n",
      "Epoch 1737, Loss: 0.5697823613882065, Final Batch Loss: 0.17237496376037598\n",
      "Epoch 1738, Loss: 0.5926550775766373, Final Batch Loss: 0.16754402220249176\n",
      "Epoch 1739, Loss: 0.678562730550766, Final Batch Loss: 0.22116126120090485\n",
      "Epoch 1740, Loss: 0.6006862372159958, Final Batch Loss: 0.1761895716190338\n",
      "Epoch 1741, Loss: 0.7565031796693802, Final Batch Loss: 0.29427990317344666\n",
      "Epoch 1742, Loss: 0.6476239114999771, Final Batch Loss: 0.2239476591348648\n",
      "Epoch 1743, Loss: 0.5211470872163773, Final Batch Loss: 0.14878712594509125\n",
      "Epoch 1744, Loss: 0.6070337742567062, Final Batch Loss: 0.21523793041706085\n",
      "Epoch 1745, Loss: 0.5923660695552826, Final Batch Loss: 0.19476228952407837\n",
      "Epoch 1746, Loss: 0.6829054653644562, Final Batch Loss: 0.24074524641036987\n",
      "Epoch 1747, Loss: 0.6530359536409378, Final Batch Loss: 0.22617122530937195\n",
      "Epoch 1748, Loss: 0.6499358415603638, Final Batch Loss: 0.17513975501060486\n",
      "Epoch 1749, Loss: 0.6312213093042374, Final Batch Loss: 0.21574023365974426\n",
      "Epoch 1750, Loss: 0.7182545512914658, Final Batch Loss: 0.3105401396751404\n",
      "Epoch 1751, Loss: 0.6487970352172852, Final Batch Loss: 0.22326336801052094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1752, Loss: 0.581770583987236, Final Batch Loss: 0.16393010318279266\n",
      "Epoch 1753, Loss: 0.6374561190605164, Final Batch Loss: 0.2455393373966217\n",
      "Epoch 1754, Loss: 0.5922492891550064, Final Batch Loss: 0.164032444357872\n",
      "Epoch 1755, Loss: 0.66265968978405, Final Batch Loss: 0.19358769059181213\n",
      "Epoch 1756, Loss: 0.6332096308469772, Final Batch Loss: 0.21383219957351685\n",
      "Epoch 1757, Loss: 0.7095105051994324, Final Batch Loss: 0.21241877973079681\n",
      "Epoch 1758, Loss: 0.6188783943653107, Final Batch Loss: 0.18152029812335968\n",
      "Epoch 1759, Loss: 0.6684257090091705, Final Batch Loss: 0.2576698362827301\n",
      "Epoch 1760, Loss: 0.7039127349853516, Final Batch Loss: 0.22722770273685455\n",
      "Epoch 1761, Loss: 0.639039933681488, Final Batch Loss: 0.2733054459095001\n",
      "Epoch 1762, Loss: 0.5764036774635315, Final Batch Loss: 0.22446565330028534\n",
      "Epoch 1763, Loss: 0.6630585342645645, Final Batch Loss: 0.2340000420808792\n",
      "Epoch 1764, Loss: 0.6662020534276962, Final Batch Loss: 0.24508975446224213\n",
      "Epoch 1765, Loss: 0.5422401428222656, Final Batch Loss: 0.19683648645877838\n",
      "Epoch 1766, Loss: 0.577711895108223, Final Batch Loss: 0.21293476223945618\n",
      "Epoch 1767, Loss: 0.5319582670927048, Final Batch Loss: 0.1627482771873474\n",
      "Epoch 1768, Loss: 0.5798989683389664, Final Batch Loss: 0.205702543258667\n",
      "Epoch 1769, Loss: 0.6925659626722336, Final Batch Loss: 0.2402886301279068\n",
      "Epoch 1770, Loss: 0.702105924487114, Final Batch Loss: 0.19348180294036865\n",
      "Epoch 1771, Loss: 0.5847688019275665, Final Batch Loss: 0.20862025022506714\n",
      "Epoch 1772, Loss: 0.5912143290042877, Final Batch Loss: 0.18088413774967194\n",
      "Epoch 1773, Loss: 0.5645322501659393, Final Batch Loss: 0.1928369402885437\n",
      "Epoch 1774, Loss: 0.6752948462963104, Final Batch Loss: 0.20650626718997955\n",
      "Epoch 1775, Loss: 0.6198622584342957, Final Batch Loss: 0.17974649369716644\n",
      "Epoch 1776, Loss: 0.6220614314079285, Final Batch Loss: 0.19725382328033447\n",
      "Epoch 1777, Loss: 0.5747725516557693, Final Batch Loss: 0.1658414900302887\n",
      "Epoch 1778, Loss: 0.6701814383268356, Final Batch Loss: 0.15615031123161316\n",
      "Epoch 1779, Loss: 0.7128476649522781, Final Batch Loss: 0.27471640706062317\n",
      "Epoch 1780, Loss: 0.6378789097070694, Final Batch Loss: 0.1725003719329834\n",
      "Epoch 1781, Loss: 0.5920490920543671, Final Batch Loss: 0.163936585187912\n",
      "Epoch 1782, Loss: 0.6323828995227814, Final Batch Loss: 0.19393505156040192\n",
      "Epoch 1783, Loss: 0.6877599805593491, Final Batch Loss: 0.247481569647789\n",
      "Epoch 1784, Loss: 0.590577706694603, Final Batch Loss: 0.20288537442684174\n",
      "Epoch 1785, Loss: 0.5584015548229218, Final Batch Loss: 0.1916293054819107\n",
      "Epoch 1786, Loss: 0.6659772098064423, Final Batch Loss: 0.2790006995201111\n",
      "Epoch 1787, Loss: 0.5469735711812973, Final Batch Loss: 0.1474657952785492\n",
      "Epoch 1788, Loss: 0.7183842658996582, Final Batch Loss: 0.28847670555114746\n",
      "Epoch 1789, Loss: 0.5742795467376709, Final Batch Loss: 0.24464423954486847\n",
      "Epoch 1790, Loss: 0.4978130906820297, Final Batch Loss: 0.1550045907497406\n",
      "Epoch 1791, Loss: 0.603762611746788, Final Batch Loss: 0.23956122994422913\n",
      "Epoch 1792, Loss: 0.6974756866693497, Final Batch Loss: 0.17162355780601501\n",
      "Epoch 1793, Loss: 0.6118675172328949, Final Batch Loss: 0.16752871870994568\n",
      "Epoch 1794, Loss: 0.6211287975311279, Final Batch Loss: 0.20060966908931732\n",
      "Epoch 1795, Loss: 0.6074051558971405, Final Batch Loss: 0.26230910420417786\n",
      "Epoch 1796, Loss: 0.6570932269096375, Final Batch Loss: 0.21107129752635956\n",
      "Epoch 1797, Loss: 0.6103604286909103, Final Batch Loss: 0.1866641491651535\n",
      "Epoch 1798, Loss: 0.5646266937255859, Final Batch Loss: 0.18633484840393066\n",
      "Epoch 1799, Loss: 0.5987074077129364, Final Batch Loss: 0.2111486941576004\n",
      "Epoch 1800, Loss: 0.6235815137624741, Final Batch Loss: 0.22979460656642914\n",
      "Epoch 1801, Loss: 0.6486756801605225, Final Batch Loss: 0.14082017540931702\n",
      "Epoch 1802, Loss: 0.6383855938911438, Final Batch Loss: 0.17970824241638184\n",
      "Epoch 1803, Loss: 0.5257620811462402, Final Batch Loss: 0.15838934481143951\n",
      "Epoch 1804, Loss: 0.6291287839412689, Final Batch Loss: 0.26676055788993835\n",
      "Epoch 1805, Loss: 0.5922510176897049, Final Batch Loss: 0.19725380837917328\n",
      "Epoch 1806, Loss: 0.7249876707792282, Final Batch Loss: 0.19570888578891754\n",
      "Epoch 1807, Loss: 0.6466491222381592, Final Batch Loss: 0.2131483405828476\n",
      "Epoch 1808, Loss: 0.6519375592470169, Final Batch Loss: 0.23744681477546692\n",
      "Epoch 1809, Loss: 0.7127484828233719, Final Batch Loss: 0.2378169596195221\n",
      "Epoch 1810, Loss: 0.6657331585884094, Final Batch Loss: 0.30909180641174316\n",
      "Epoch 1811, Loss: 0.6520112007856369, Final Batch Loss: 0.2032981663942337\n",
      "Epoch 1812, Loss: 0.6926111578941345, Final Batch Loss: 0.20636551082134247\n",
      "Epoch 1813, Loss: 0.6460392326116562, Final Batch Loss: 0.22356323897838593\n",
      "Epoch 1814, Loss: 0.7089470773935318, Final Batch Loss: 0.29459792375564575\n",
      "Epoch 1815, Loss: 0.68571437895298, Final Batch Loss: 0.20494891703128815\n",
      "Epoch 1816, Loss: 0.6118378043174744, Final Batch Loss: 0.12935639917850494\n",
      "Epoch 1817, Loss: 0.6242004483938217, Final Batch Loss: 0.2592555284500122\n",
      "Epoch 1818, Loss: 0.6245445013046265, Final Batch Loss: 0.16043543815612793\n",
      "Epoch 1819, Loss: 0.6188103705644608, Final Batch Loss: 0.19308845698833466\n",
      "Epoch 1820, Loss: 0.6655156165361404, Final Batch Loss: 0.3141935169696808\n",
      "Epoch 1821, Loss: 0.5265444070100784, Final Batch Loss: 0.20696862041950226\n",
      "Epoch 1822, Loss: 0.6309761703014374, Final Batch Loss: 0.2389957755804062\n",
      "Epoch 1823, Loss: 0.598200261592865, Final Batch Loss: 0.19623585045337677\n",
      "Epoch 1824, Loss: 0.6547465622425079, Final Batch Loss: 0.21809764206409454\n",
      "Epoch 1825, Loss: 0.6722470968961716, Final Batch Loss: 0.2647087872028351\n",
      "Epoch 1826, Loss: 0.631857693195343, Final Batch Loss: 0.18889881670475006\n",
      "Epoch 1827, Loss: 0.5538628548383713, Final Batch Loss: 0.22473034262657166\n",
      "Epoch 1828, Loss: 0.6399965584278107, Final Batch Loss: 0.186513751745224\n",
      "Epoch 1829, Loss: 0.5844521224498749, Final Batch Loss: 0.12843631207942963\n",
      "Epoch 1830, Loss: 0.5533416420221329, Final Batch Loss: 0.1583380252122879\n",
      "Epoch 1831, Loss: 0.6430406868457794, Final Batch Loss: 0.19221019744873047\n",
      "Epoch 1832, Loss: 0.671580359339714, Final Batch Loss: 0.2171267420053482\n",
      "Epoch 1833, Loss: 0.6665136814117432, Final Batch Loss: 0.22368232905864716\n",
      "Epoch 1834, Loss: 0.5734416991472244, Final Batch Loss: 0.165336012840271\n",
      "Epoch 1835, Loss: 0.5250243991613388, Final Batch Loss: 0.13230830430984497\n",
      "Epoch 1836, Loss: 0.6300410181283951, Final Batch Loss: 0.20491406321525574\n",
      "Epoch 1837, Loss: 0.608792319893837, Final Batch Loss: 0.1792687326669693\n",
      "Epoch 1838, Loss: 0.6551687866449356, Final Batch Loss: 0.18190380930900574\n",
      "Epoch 1839, Loss: 0.6471674293279648, Final Batch Loss: 0.24329662322998047\n",
      "Epoch 1840, Loss: 0.6071267873048782, Final Batch Loss: 0.1864769607782364\n",
      "Epoch 1841, Loss: 0.6769062578678131, Final Batch Loss: 0.2033134400844574\n",
      "Epoch 1842, Loss: 0.6231299489736557, Final Batch Loss: 0.20870129764080048\n",
      "Epoch 1843, Loss: 0.5921935141086578, Final Batch Loss: 0.16995002329349518\n",
      "Epoch 1844, Loss: 0.6420658081769943, Final Batch Loss: 0.23436950147151947\n",
      "Epoch 1845, Loss: 0.5664219111204147, Final Batch Loss: 0.15329299867153168\n",
      "Epoch 1846, Loss: 0.6120195984840393, Final Batch Loss: 0.26701685786247253\n",
      "Epoch 1847, Loss: 0.557235598564148, Final Batch Loss: 0.1819642335176468\n",
      "Epoch 1848, Loss: 0.5772835910320282, Final Batch Loss: 0.2044859081506729\n",
      "Epoch 1849, Loss: 0.7128020972013474, Final Batch Loss: 0.3022975027561188\n",
      "Epoch 1850, Loss: 0.5786323696374893, Final Batch Loss: 0.22141988575458527\n",
      "Epoch 1851, Loss: 0.6199515908956528, Final Batch Loss: 0.2295871526002884\n",
      "Epoch 1852, Loss: 0.6694658845663071, Final Batch Loss: 0.24126316606998444\n",
      "Epoch 1853, Loss: 0.6544468402862549, Final Batch Loss: 0.2673092484474182\n",
      "Epoch 1854, Loss: 0.5413455069065094, Final Batch Loss: 0.1850264072418213\n",
      "Epoch 1855, Loss: 0.6123838126659393, Final Batch Loss: 0.17355501651763916\n",
      "Epoch 1856, Loss: 0.5671062469482422, Final Batch Loss: 0.15449754893779755\n",
      "Epoch 1857, Loss: 0.5841666162014008, Final Batch Loss: 0.23171493411064148\n",
      "Epoch 1858, Loss: 0.5677201747894287, Final Batch Loss: 0.26108747720718384\n",
      "Epoch 1859, Loss: 0.5821446031332016, Final Batch Loss: 0.18025800585746765\n",
      "Epoch 1860, Loss: 0.6270590424537659, Final Batch Loss: 0.18986085057258606\n",
      "Epoch 1861, Loss: 0.615085631608963, Final Batch Loss: 0.2076440155506134\n",
      "Epoch 1862, Loss: 0.5350914150476456, Final Batch Loss: 0.1577545404434204\n",
      "Epoch 1863, Loss: 0.583109438419342, Final Batch Loss: 0.21396350860595703\n",
      "Epoch 1864, Loss: 0.6345807462930679, Final Batch Loss: 0.23186546564102173\n",
      "Epoch 1865, Loss: 0.48359325528144836, Final Batch Loss: 0.15541045367717743\n",
      "Epoch 1866, Loss: 0.6188941299915314, Final Batch Loss: 0.2495252937078476\n",
      "Epoch 1867, Loss: 0.5971899181604385, Final Batch Loss: 0.23159560561180115\n",
      "Epoch 1868, Loss: 0.5672838538885117, Final Batch Loss: 0.13444560766220093\n",
      "Epoch 1869, Loss: 0.6889615207910538, Final Batch Loss: 0.270999550819397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1870, Loss: 0.5868500620126724, Final Batch Loss: 0.20451228320598602\n",
      "Epoch 1871, Loss: 0.5859047919511795, Final Batch Loss: 0.23913517594337463\n",
      "Epoch 1872, Loss: 0.576723039150238, Final Batch Loss: 0.16629737615585327\n",
      "Epoch 1873, Loss: 0.6824101805686951, Final Batch Loss: 0.23441675305366516\n",
      "Epoch 1874, Loss: 0.6222840547561646, Final Batch Loss: 0.23828743398189545\n",
      "Epoch 1875, Loss: 0.5626135021448135, Final Batch Loss: 0.1328432559967041\n",
      "Epoch 1876, Loss: 0.6551477611064911, Final Batch Loss: 0.2569991946220398\n",
      "Epoch 1877, Loss: 0.6125670075416565, Final Batch Loss: 0.20637355744838715\n",
      "Epoch 1878, Loss: 0.6297675371170044, Final Batch Loss: 0.17507262527942657\n",
      "Epoch 1879, Loss: 0.5348261892795563, Final Batch Loss: 0.1408601701259613\n",
      "Epoch 1880, Loss: 0.6410327255725861, Final Batch Loss: 0.1505073457956314\n",
      "Epoch 1881, Loss: 0.7033867388963699, Final Batch Loss: 0.2936612665653229\n",
      "Epoch 1882, Loss: 0.5558501034975052, Final Batch Loss: 0.1324748992919922\n",
      "Epoch 1883, Loss: 0.49217547476291656, Final Batch Loss: 0.13385654985904694\n",
      "Epoch 1884, Loss: 0.5694281756877899, Final Batch Loss: 0.20688937604427338\n",
      "Epoch 1885, Loss: 0.5321965888142586, Final Batch Loss: 0.1135328933596611\n",
      "Epoch 1886, Loss: 0.6073939204216003, Final Batch Loss: 0.20866163074970245\n",
      "Epoch 1887, Loss: 0.67099629342556, Final Batch Loss: 0.27814754843711853\n",
      "Epoch 1888, Loss: 0.5341906622052193, Final Batch Loss: 0.10723965615034103\n",
      "Epoch 1889, Loss: 0.7178242355585098, Final Batch Loss: 0.31770995259284973\n",
      "Epoch 1890, Loss: 0.6502455770969391, Final Batch Loss: 0.19980280101299286\n",
      "Epoch 1891, Loss: 0.6204988807439804, Final Batch Loss: 0.20634689927101135\n",
      "Epoch 1892, Loss: 0.6334541589021683, Final Batch Loss: 0.17702950537204742\n",
      "Epoch 1893, Loss: 0.4855612516403198, Final Batch Loss: 0.1321161836385727\n",
      "Epoch 1894, Loss: 0.513502836227417, Final Batch Loss: 0.21437843143939972\n",
      "Epoch 1895, Loss: 0.5891093909740448, Final Batch Loss: 0.17335855960845947\n",
      "Epoch 1896, Loss: 0.5892728269100189, Final Batch Loss: 0.23959921300411224\n",
      "Epoch 1897, Loss: 0.6364052295684814, Final Batch Loss: 0.2532195746898651\n",
      "Epoch 1898, Loss: 0.674756646156311, Final Batch Loss: 0.20092138648033142\n",
      "Epoch 1899, Loss: 0.6214939802885056, Final Batch Loss: 0.1799626350402832\n",
      "Epoch 1900, Loss: 0.6514294743537903, Final Batch Loss: 0.2121412307024002\n",
      "Epoch 1901, Loss: 0.5327577292919159, Final Batch Loss: 0.18373334407806396\n",
      "Epoch 1902, Loss: 0.6567835211753845, Final Batch Loss: 0.1557512879371643\n",
      "Epoch 1903, Loss: 0.5271081626415253, Final Batch Loss: 0.14278018474578857\n",
      "Epoch 1904, Loss: 0.5207110643386841, Final Batch Loss: 0.1833081692457199\n",
      "Epoch 1905, Loss: 0.6884392201900482, Final Batch Loss: 0.30546247959136963\n",
      "Epoch 1906, Loss: 0.5807434916496277, Final Batch Loss: 0.2564598619937897\n",
      "Epoch 1907, Loss: 0.5567992329597473, Final Batch Loss: 0.16756103932857513\n",
      "Epoch 1908, Loss: 0.6578804552555084, Final Batch Loss: 0.2322985678911209\n",
      "Epoch 1909, Loss: 0.5498250424861908, Final Batch Loss: 0.22304995357990265\n",
      "Epoch 1910, Loss: 0.5864910930395126, Final Batch Loss: 0.14664074778556824\n",
      "Epoch 1911, Loss: 0.4866674542427063, Final Batch Loss: 0.15807579457759857\n",
      "Epoch 1912, Loss: 0.5912657082080841, Final Batch Loss: 0.19036073982715607\n",
      "Epoch 1913, Loss: 0.505318358540535, Final Batch Loss: 0.19871366024017334\n",
      "Epoch 1914, Loss: 0.588912695646286, Final Batch Loss: 0.16456323862075806\n",
      "Epoch 1915, Loss: 0.6769944727420807, Final Batch Loss: 0.3317320644855499\n",
      "Epoch 1916, Loss: 0.6166574358940125, Final Batch Loss: 0.15469767153263092\n",
      "Epoch 1917, Loss: 0.6491060554981232, Final Batch Loss: 0.22125333547592163\n",
      "Epoch 1918, Loss: 0.5829639285802841, Final Batch Loss: 0.2631164491176605\n",
      "Epoch 1919, Loss: 0.5691001415252686, Final Batch Loss: 0.1824016273021698\n",
      "Epoch 1920, Loss: 0.6147804260253906, Final Batch Loss: 0.26566436886787415\n",
      "Epoch 1921, Loss: 0.5933476984500885, Final Batch Loss: 0.207816943526268\n",
      "Epoch 1922, Loss: 0.6582248508930206, Final Batch Loss: 0.15193511545658112\n",
      "Epoch 1923, Loss: 0.6079446971416473, Final Batch Loss: 0.12201674282550812\n",
      "Epoch 1924, Loss: 0.6902985870838165, Final Batch Loss: 0.29335322976112366\n",
      "Epoch 1925, Loss: 0.5404853671789169, Final Batch Loss: 0.18089327216148376\n",
      "Epoch 1926, Loss: 0.6303771585226059, Final Batch Loss: 0.2829113006591797\n",
      "Epoch 1927, Loss: 0.7172025889158249, Final Batch Loss: 0.2511264681816101\n",
      "Epoch 1928, Loss: 0.6144361793994904, Final Batch Loss: 0.19800953567028046\n",
      "Epoch 1929, Loss: 0.6894921362400055, Final Batch Loss: 0.28020787239074707\n",
      "Epoch 1930, Loss: 0.6308576464653015, Final Batch Loss: 0.24415209889411926\n",
      "Epoch 1931, Loss: 0.6094639301300049, Final Batch Loss: 0.23090657591819763\n",
      "Epoch 1932, Loss: 0.5524919480085373, Final Batch Loss: 0.1885489672422409\n",
      "Epoch 1933, Loss: 0.6030033230781555, Final Batch Loss: 0.1733882874250412\n",
      "Epoch 1934, Loss: 0.6054681837558746, Final Batch Loss: 0.2427567094564438\n",
      "Epoch 1935, Loss: 0.5571034103631973, Final Batch Loss: 0.21784181892871857\n",
      "Epoch 1936, Loss: 0.6154574602842331, Final Batch Loss: 0.25231513381004333\n",
      "Epoch 1937, Loss: 0.7158700674772263, Final Batch Loss: 0.24830476939678192\n",
      "Epoch 1938, Loss: 0.6503042131662369, Final Batch Loss: 0.17903949320316315\n",
      "Epoch 1939, Loss: 0.6543447971343994, Final Batch Loss: 0.2580942213535309\n",
      "Epoch 1940, Loss: 0.711408331990242, Final Batch Loss: 0.23723959922790527\n",
      "Epoch 1941, Loss: 0.6678584963083267, Final Batch Loss: 0.2709280550479889\n",
      "Epoch 1942, Loss: 0.5895378440618515, Final Batch Loss: 0.17038004100322723\n",
      "Epoch 1943, Loss: 0.5099056959152222, Final Batch Loss: 0.14699998497962952\n",
      "Epoch 1944, Loss: 0.5895069390535355, Final Batch Loss: 0.156534343957901\n",
      "Epoch 1945, Loss: 0.5610846132040024, Final Batch Loss: 0.18816189467906952\n",
      "Epoch 1946, Loss: 0.7219313979148865, Final Batch Loss: 0.26010167598724365\n",
      "Epoch 1947, Loss: 0.616603285074234, Final Batch Loss: 0.22281904518604279\n",
      "Epoch 1948, Loss: 0.6905100047588348, Final Batch Loss: 0.24902011454105377\n",
      "Epoch 1949, Loss: 0.5272565335035324, Final Batch Loss: 0.16782808303833008\n",
      "Epoch 1950, Loss: 0.6359187662601471, Final Batch Loss: 0.19537965953350067\n",
      "Epoch 1951, Loss: 0.5960643589496613, Final Batch Loss: 0.15627087652683258\n",
      "Epoch 1952, Loss: 0.6080007553100586, Final Batch Loss: 0.14051373302936554\n",
      "Epoch 1953, Loss: 0.7678895145654678, Final Batch Loss: 0.2942708730697632\n",
      "Epoch 1954, Loss: 0.6139096319675446, Final Batch Loss: 0.1879628598690033\n",
      "Epoch 1955, Loss: 0.5918197333812714, Final Batch Loss: 0.21661102771759033\n",
      "Epoch 1956, Loss: 0.5539200007915497, Final Batch Loss: 0.18538738787174225\n",
      "Epoch 1957, Loss: 0.5854921340942383, Final Batch Loss: 0.12513867020606995\n",
      "Epoch 1958, Loss: 0.5669616162776947, Final Batch Loss: 0.21731911599636078\n",
      "Epoch 1959, Loss: 0.5649821013212204, Final Batch Loss: 0.24657337367534637\n",
      "Epoch 1960, Loss: 0.6040659546852112, Final Batch Loss: 0.211578831076622\n",
      "Epoch 1961, Loss: 0.5188128650188446, Final Batch Loss: 0.16004520654678345\n",
      "Epoch 1962, Loss: 0.6269145905971527, Final Batch Loss: 0.23652678728103638\n",
      "Epoch 1963, Loss: 0.542113408446312, Final Batch Loss: 0.2175949364900589\n",
      "Epoch 1964, Loss: 0.5835633873939514, Final Batch Loss: 0.20785999298095703\n",
      "Epoch 1965, Loss: 0.6287888884544373, Final Batch Loss: 0.22044812142848969\n",
      "Epoch 1966, Loss: 0.5388092249631882, Final Batch Loss: 0.1447097659111023\n",
      "Epoch 1967, Loss: 0.5236321538686752, Final Batch Loss: 0.15488126873970032\n",
      "Epoch 1968, Loss: 0.5888187736272812, Final Batch Loss: 0.16376836597919464\n",
      "Epoch 1969, Loss: 0.5419455617666245, Final Batch Loss: 0.19646018743515015\n",
      "Epoch 1970, Loss: 0.6049054563045502, Final Batch Loss: 0.1918991059064865\n",
      "Epoch 1971, Loss: 0.6567282676696777, Final Batch Loss: 0.230449378490448\n",
      "Epoch 1972, Loss: 0.6700917929410934, Final Batch Loss: 0.17924728989601135\n",
      "Epoch 1973, Loss: 0.5503723919391632, Final Batch Loss: 0.13605308532714844\n",
      "Epoch 1974, Loss: 0.5495208501815796, Final Batch Loss: 0.1846933215856552\n",
      "Epoch 1975, Loss: 0.5920178145170212, Final Batch Loss: 0.14329025149345398\n",
      "Epoch 1976, Loss: 0.5628780126571655, Final Batch Loss: 0.218726247549057\n",
      "Epoch 1977, Loss: 0.5846829861402512, Final Batch Loss: 0.17073792219161987\n",
      "Epoch 1978, Loss: 0.718080997467041, Final Batch Loss: 0.332588255405426\n",
      "Epoch 1979, Loss: 0.6422174423933029, Final Batch Loss: 0.31126779317855835\n",
      "Epoch 1980, Loss: 0.5464366525411606, Final Batch Loss: 0.27231982350349426\n",
      "Epoch 1981, Loss: 0.5590020418167114, Final Batch Loss: 0.1692734956741333\n",
      "Epoch 1982, Loss: 0.6131457835435867, Final Batch Loss: 0.15653614699840546\n",
      "Epoch 1983, Loss: 0.560321256518364, Final Batch Loss: 0.21098929643630981\n",
      "Epoch 1984, Loss: 0.5052992552518845, Final Batch Loss: 0.15328238904476166\n",
      "Epoch 1985, Loss: 0.5232508480548859, Final Batch Loss: 0.19087056815624237\n",
      "Epoch 1986, Loss: 0.5511346310377121, Final Batch Loss: 0.18655265867710114\n",
      "Epoch 1987, Loss: 0.5752513408660889, Final Batch Loss: 0.21455347537994385\n",
      "Epoch 1988, Loss: 0.5423817038536072, Final Batch Loss: 0.1796150952577591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1989, Loss: 0.6639904379844666, Final Batch Loss: 0.23133635520935059\n",
      "Epoch 1990, Loss: 0.6385880708694458, Final Batch Loss: 0.20081305503845215\n",
      "Epoch 1991, Loss: 0.6453800052404404, Final Batch Loss: 0.18724747002124786\n",
      "Epoch 1992, Loss: 0.5691805779933929, Final Batch Loss: 0.19917801022529602\n",
      "Epoch 1993, Loss: 0.7087936997413635, Final Batch Loss: 0.2503986358642578\n",
      "Epoch 1994, Loss: 0.5386016070842743, Final Batch Loss: 0.13304488360881805\n",
      "Epoch 1995, Loss: 0.5735483765602112, Final Batch Loss: 0.18117327988147736\n",
      "Epoch 1996, Loss: 0.5787383168935776, Final Batch Loss: 0.15821677446365356\n",
      "Epoch 1997, Loss: 0.5677160173654556, Final Batch Loss: 0.18109716475009918\n",
      "Epoch 1998, Loss: 0.5076707154512405, Final Batch Loss: 0.1746845692396164\n",
      "Epoch 1999, Loss: 0.5328928977251053, Final Batch Loss: 0.12604369223117828\n",
      "Epoch 2000, Loss: 0.546423465013504, Final Batch Loss: 0.22688782215118408\n",
      "Epoch 2001, Loss: 0.6508748829364777, Final Batch Loss: 0.24733273684978485\n",
      "Epoch 2002, Loss: 0.6194417029619217, Final Batch Loss: 0.294988751411438\n",
      "Epoch 2003, Loss: 0.6015632599592209, Final Batch Loss: 0.15625472366809845\n",
      "Epoch 2004, Loss: 0.5275867730379105, Final Batch Loss: 0.18594422936439514\n",
      "Epoch 2005, Loss: 0.548019215464592, Final Batch Loss: 0.2074047178030014\n",
      "Epoch 2006, Loss: 0.5055551677942276, Final Batch Loss: 0.15324841439723969\n",
      "Epoch 2007, Loss: 0.4824199080467224, Final Batch Loss: 0.14666475355625153\n",
      "Epoch 2008, Loss: 0.5457797199487686, Final Batch Loss: 0.17428769171237946\n",
      "Epoch 2009, Loss: 0.595431387424469, Final Batch Loss: 0.2063308209180832\n",
      "Epoch 2010, Loss: 0.5855007171630859, Final Batch Loss: 0.14668767154216766\n",
      "Epoch 2011, Loss: 0.5923189669847488, Final Batch Loss: 0.1995818018913269\n",
      "Epoch 2012, Loss: 0.6294993460178375, Final Batch Loss: 0.19973741471767426\n",
      "Epoch 2013, Loss: 0.5145661234855652, Final Batch Loss: 0.17608387768268585\n",
      "Epoch 2014, Loss: 0.5124199390411377, Final Batch Loss: 0.18988573551177979\n",
      "Epoch 2015, Loss: 0.5785564184188843, Final Batch Loss: 0.21171221137046814\n",
      "Epoch 2016, Loss: 0.47802019119262695, Final Batch Loss: 0.15700648725032806\n",
      "Epoch 2017, Loss: 0.6295433193445206, Final Batch Loss: 0.17811483144760132\n",
      "Epoch 2018, Loss: 0.5204180181026459, Final Batch Loss: 0.18376216292381287\n",
      "Epoch 2019, Loss: 0.6419217139482498, Final Batch Loss: 0.28954190015792847\n",
      "Epoch 2020, Loss: 0.6755843907594681, Final Batch Loss: 0.19058267772197723\n",
      "Epoch 2021, Loss: 0.5332401692867279, Final Batch Loss: 0.1685187965631485\n",
      "Epoch 2022, Loss: 0.4294050857424736, Final Batch Loss: 0.11457408219575882\n",
      "Epoch 2023, Loss: 0.5425393879413605, Final Batch Loss: 0.18026180565357208\n",
      "Epoch 2024, Loss: 0.6151796579360962, Final Batch Loss: 0.22878192365169525\n",
      "Epoch 2025, Loss: 0.6923193037509918, Final Batch Loss: 0.20243065059185028\n",
      "Epoch 2026, Loss: 0.5247737467288971, Final Batch Loss: 0.15928129851818085\n",
      "Epoch 2027, Loss: 0.6147799044847488, Final Batch Loss: 0.22448225319385529\n",
      "Epoch 2028, Loss: 0.5690878927707672, Final Batch Loss: 0.18216560781002045\n",
      "Epoch 2029, Loss: 0.5665283501148224, Final Batch Loss: 0.1278742551803589\n",
      "Epoch 2030, Loss: 0.5264513492584229, Final Batch Loss: 0.2042781114578247\n",
      "Epoch 2031, Loss: 0.5997613966464996, Final Batch Loss: 0.18299712240695953\n",
      "Epoch 2032, Loss: 0.5206616744399071, Final Batch Loss: 0.11675430089235306\n",
      "Epoch 2033, Loss: 0.5099294036626816, Final Batch Loss: 0.17990033328533173\n",
      "Epoch 2034, Loss: 0.5575238615274429, Final Batch Loss: 0.23971015214920044\n",
      "Epoch 2035, Loss: 0.5045876801013947, Final Batch Loss: 0.1265770047903061\n",
      "Epoch 2036, Loss: 0.6354954540729523, Final Batch Loss: 0.17689310014247894\n",
      "Epoch 2037, Loss: 0.6912193149328232, Final Batch Loss: 0.23057810962200165\n",
      "Epoch 2038, Loss: 0.6160782426595688, Final Batch Loss: 0.24617944657802582\n",
      "Epoch 2039, Loss: 0.5631612986326218, Final Batch Loss: 0.1679690033197403\n",
      "Epoch 2040, Loss: 0.556971088051796, Final Batch Loss: 0.16521015763282776\n",
      "Epoch 2041, Loss: 0.5808759182691574, Final Batch Loss: 0.1776435822248459\n",
      "Epoch 2042, Loss: 0.5955550223588943, Final Batch Loss: 0.22260284423828125\n",
      "Epoch 2043, Loss: 0.6246641129255295, Final Batch Loss: 0.22387482225894928\n",
      "Epoch 2044, Loss: 0.6380537152290344, Final Batch Loss: 0.24680915474891663\n",
      "Epoch 2045, Loss: 0.6581318378448486, Final Batch Loss: 0.21822041273117065\n",
      "Epoch 2046, Loss: 0.5441096276044846, Final Batch Loss: 0.14245480298995972\n",
      "Epoch 2047, Loss: 0.6181690692901611, Final Batch Loss: 0.1661992222070694\n",
      "Epoch 2048, Loss: 0.5179188698530197, Final Batch Loss: 0.17579884827136993\n",
      "Epoch 2049, Loss: 0.707695260643959, Final Batch Loss: 0.2768661677837372\n",
      "Epoch 2050, Loss: 0.6758970767259598, Final Batch Loss: 0.251748263835907\n",
      "Epoch 2051, Loss: 0.6408723443746567, Final Batch Loss: 0.16500058770179749\n",
      "Epoch 2052, Loss: 0.6365046352148056, Final Batch Loss: 0.2395021617412567\n",
      "Epoch 2053, Loss: 0.5125094801187515, Final Batch Loss: 0.17749647796154022\n",
      "Epoch 2054, Loss: 0.5491803586483002, Final Batch Loss: 0.23501496016979218\n",
      "Epoch 2055, Loss: 0.5852077007293701, Final Batch Loss: 0.23030021786689758\n",
      "Epoch 2056, Loss: 0.6356373429298401, Final Batch Loss: 0.23982763290405273\n",
      "Epoch 2057, Loss: 0.6206704825162888, Final Batch Loss: 0.2294270545244217\n",
      "Epoch 2058, Loss: 0.6600154787302017, Final Batch Loss: 0.2394351214170456\n",
      "Epoch 2059, Loss: 0.5269570052623749, Final Batch Loss: 0.164909228682518\n",
      "Epoch 2060, Loss: 0.4790819585323334, Final Batch Loss: 0.17762617766857147\n",
      "Epoch 2061, Loss: 0.6855059713125229, Final Batch Loss: 0.2160731554031372\n",
      "Epoch 2062, Loss: 0.5773245841264725, Final Batch Loss: 0.20632819831371307\n",
      "Epoch 2063, Loss: 0.6317148357629776, Final Batch Loss: 0.187194362282753\n",
      "Epoch 2064, Loss: 0.5906059145927429, Final Batch Loss: 0.19828279316425323\n",
      "Epoch 2065, Loss: 0.6108350902795792, Final Batch Loss: 0.18788453936576843\n",
      "Epoch 2066, Loss: 0.5039257109165192, Final Batch Loss: 0.16418708860874176\n",
      "Epoch 2067, Loss: 0.6432858556509018, Final Batch Loss: 0.22951442003250122\n",
      "Epoch 2068, Loss: 0.5832590758800507, Final Batch Loss: 0.134145587682724\n",
      "Epoch 2069, Loss: 0.6229292452335358, Final Batch Loss: 0.1718457192182541\n",
      "Epoch 2070, Loss: 0.6225146353244781, Final Batch Loss: 0.23731175065040588\n",
      "Epoch 2071, Loss: 0.6388044506311417, Final Batch Loss: 0.26813358068466187\n",
      "Epoch 2072, Loss: 0.5474079698324203, Final Batch Loss: 0.1745705008506775\n",
      "Epoch 2073, Loss: 0.6009968817234039, Final Batch Loss: 0.1447843760251999\n",
      "Epoch 2074, Loss: 0.6249241679906845, Final Batch Loss: 0.1585465967655182\n",
      "Epoch 2075, Loss: 0.5662834495306015, Final Batch Loss: 0.21964173018932343\n",
      "Epoch 2076, Loss: 0.5499002784490585, Final Batch Loss: 0.18456196784973145\n",
      "Epoch 2077, Loss: 0.48086485266685486, Final Batch Loss: 0.14636555314064026\n",
      "Epoch 2078, Loss: 0.5200864523649216, Final Batch Loss: 0.1690521389245987\n",
      "Epoch 2079, Loss: 0.5366889238357544, Final Batch Loss: 0.2113865166902542\n",
      "Epoch 2080, Loss: 0.5639114528894424, Final Batch Loss: 0.1292612999677658\n",
      "Epoch 2081, Loss: 0.518579512834549, Final Batch Loss: 0.142364040017128\n",
      "Epoch 2082, Loss: 0.5351762473583221, Final Batch Loss: 0.18953026831150055\n",
      "Epoch 2083, Loss: 0.5817103683948517, Final Batch Loss: 0.16399379074573517\n",
      "Epoch 2084, Loss: 0.6523824334144592, Final Batch Loss: 0.20572522282600403\n",
      "Epoch 2085, Loss: 0.5186727792024612, Final Batch Loss: 0.14016185700893402\n",
      "Epoch 2086, Loss: 0.5500818192958832, Final Batch Loss: 0.169773668050766\n",
      "Epoch 2087, Loss: 0.47002749145030975, Final Batch Loss: 0.14250235259532928\n",
      "Epoch 2088, Loss: 0.5748883932828903, Final Batch Loss: 0.1856386512517929\n",
      "Epoch 2089, Loss: 0.5201527923345566, Final Batch Loss: 0.14980188012123108\n",
      "Epoch 2090, Loss: 0.5469856560230255, Final Batch Loss: 0.16570119559764862\n",
      "Epoch 2091, Loss: 0.6428154110908508, Final Batch Loss: 0.23470105230808258\n",
      "Epoch 2092, Loss: 0.5228123068809509, Final Batch Loss: 0.19403067231178284\n",
      "Epoch 2093, Loss: 0.6464625597000122, Final Batch Loss: 0.2617543339729309\n",
      "Epoch 2094, Loss: 0.5553252846002579, Final Batch Loss: 0.22859959304332733\n",
      "Epoch 2095, Loss: 0.4688543304800987, Final Batch Loss: 0.19821679592132568\n",
      "Epoch 2096, Loss: 0.5373791605234146, Final Batch Loss: 0.18591855466365814\n",
      "Epoch 2097, Loss: 0.5796700268983841, Final Batch Loss: 0.19009146094322205\n",
      "Epoch 2098, Loss: 0.4959460198879242, Final Batch Loss: 0.2400103658437729\n",
      "Epoch 2099, Loss: 0.4349115490913391, Final Batch Loss: 0.1456345021724701\n",
      "Epoch 2100, Loss: 0.4828629791736603, Final Batch Loss: 0.17521055042743683\n",
      "Epoch 2101, Loss: 0.5818981230258942, Final Batch Loss: 0.21346013247966766\n",
      "Epoch 2102, Loss: 0.4474860429763794, Final Batch Loss: 0.10297299921512604\n",
      "Epoch 2103, Loss: 0.4962541311979294, Final Batch Loss: 0.1279585361480713\n",
      "Epoch 2104, Loss: 0.5613031983375549, Final Batch Loss: 0.19301378726959229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2105, Loss: 0.6122536659240723, Final Batch Loss: 0.17633821070194244\n",
      "Epoch 2106, Loss: 0.49596525728702545, Final Batch Loss: 0.1464412957429886\n",
      "Epoch 2107, Loss: 0.5805883407592773, Final Batch Loss: 0.24363917112350464\n",
      "Epoch 2108, Loss: 0.5574590712785721, Final Batch Loss: 0.14215636253356934\n",
      "Epoch 2109, Loss: 0.4448975846171379, Final Batch Loss: 0.13095194101333618\n",
      "Epoch 2110, Loss: 0.6094558984041214, Final Batch Loss: 0.24712638556957245\n",
      "Epoch 2111, Loss: 0.5537290200591087, Final Batch Loss: 0.23463939130306244\n",
      "Epoch 2112, Loss: 0.5570091903209686, Final Batch Loss: 0.16258783638477325\n",
      "Epoch 2113, Loss: 0.6327133476734161, Final Batch Loss: 0.16816093027591705\n",
      "Epoch 2114, Loss: 0.5417125821113586, Final Batch Loss: 0.14446499943733215\n",
      "Epoch 2115, Loss: 0.6373937129974365, Final Batch Loss: 0.2280084639787674\n",
      "Epoch 2116, Loss: 0.6137152016162872, Final Batch Loss: 0.15756207704544067\n",
      "Epoch 2117, Loss: 0.5136770829558372, Final Batch Loss: 0.11351989954710007\n",
      "Epoch 2118, Loss: 0.7381227910518646, Final Batch Loss: 0.19877131283283234\n",
      "Epoch 2119, Loss: 0.603495255112648, Final Batch Loss: 0.20968559384346008\n",
      "Epoch 2120, Loss: 0.533770352602005, Final Batch Loss: 0.18595196306705475\n",
      "Epoch 2121, Loss: 0.6269353330135345, Final Batch Loss: 0.20099332928657532\n",
      "Epoch 2122, Loss: 0.5515500754117966, Final Batch Loss: 0.25245025753974915\n",
      "Epoch 2123, Loss: 0.6587348580360413, Final Batch Loss: 0.1878848373889923\n",
      "Epoch 2124, Loss: 0.588970273733139, Final Batch Loss: 0.19071577489376068\n",
      "Epoch 2125, Loss: 0.5030159503221512, Final Batch Loss: 0.148732528090477\n",
      "Epoch 2126, Loss: 0.5526593923568726, Final Batch Loss: 0.21912060678005219\n",
      "Epoch 2127, Loss: 0.4892565906047821, Final Batch Loss: 0.12548552453517914\n",
      "Epoch 2128, Loss: 0.603313222527504, Final Batch Loss: 0.19634020328521729\n",
      "Epoch 2129, Loss: 0.5226586610078812, Final Batch Loss: 0.15783007442951202\n",
      "Epoch 2130, Loss: 0.6584824174642563, Final Batch Loss: 0.24231204390525818\n",
      "Epoch 2131, Loss: 0.5131834149360657, Final Batch Loss: 0.17276760935783386\n",
      "Epoch 2132, Loss: 0.6832947582006454, Final Batch Loss: 0.27783724665641785\n",
      "Epoch 2133, Loss: 0.6144406646490097, Final Batch Loss: 0.23403634130954742\n",
      "Epoch 2134, Loss: 0.6679409742355347, Final Batch Loss: 0.2332448661327362\n",
      "Epoch 2135, Loss: 0.4832817614078522, Final Batch Loss: 0.11642871797084808\n",
      "Epoch 2136, Loss: 0.49399009346961975, Final Batch Loss: 0.13481777906417847\n",
      "Epoch 2137, Loss: 0.5431751012802124, Final Batch Loss: 0.12649181485176086\n",
      "Epoch 2138, Loss: 0.5975027531385422, Final Batch Loss: 0.22241772711277008\n",
      "Epoch 2139, Loss: 0.5924278050661087, Final Batch Loss: 0.1779138445854187\n",
      "Epoch 2140, Loss: 0.5734632164239883, Final Batch Loss: 0.17232540249824524\n",
      "Epoch 2141, Loss: 0.5985851436853409, Final Batch Loss: 0.18439745903015137\n",
      "Epoch 2142, Loss: 0.6213706582784653, Final Batch Loss: 0.198322132229805\n",
      "Epoch 2143, Loss: 0.5958220511674881, Final Batch Loss: 0.1603519767522812\n",
      "Epoch 2144, Loss: 0.5970520973205566, Final Batch Loss: 0.14974160492420197\n",
      "Epoch 2145, Loss: 0.6531690508127213, Final Batch Loss: 0.24332650005817413\n",
      "Epoch 2146, Loss: 0.5467918366193771, Final Batch Loss: 0.2186293601989746\n",
      "Epoch 2147, Loss: 0.5260947644710541, Final Batch Loss: 0.17083483934402466\n",
      "Epoch 2148, Loss: 0.686040610074997, Final Batch Loss: 0.2740687131881714\n",
      "Epoch 2149, Loss: 0.5431433171033859, Final Batch Loss: 0.20783917605876923\n",
      "Epoch 2150, Loss: 0.5512023866176605, Final Batch Loss: 0.18488635122776031\n",
      "Epoch 2151, Loss: 0.6713179796934128, Final Batch Loss: 0.13424675166606903\n",
      "Epoch 2152, Loss: 0.529198169708252, Final Batch Loss: 0.1940973997116089\n",
      "Epoch 2153, Loss: 0.5907914191484451, Final Batch Loss: 0.266615629196167\n",
      "Epoch 2154, Loss: 0.5422768443822861, Final Batch Loss: 0.19310615956783295\n",
      "Epoch 2155, Loss: 0.5821939706802368, Final Batch Loss: 0.17209024727344513\n",
      "Epoch 2156, Loss: 0.5571512132883072, Final Batch Loss: 0.19002050161361694\n",
      "Epoch 2157, Loss: 0.4062504321336746, Final Batch Loss: 0.13900525867938995\n",
      "Epoch 2158, Loss: 0.616198793053627, Final Batch Loss: 0.2939804494380951\n",
      "Epoch 2159, Loss: 0.632244348526001, Final Batch Loss: 0.213512122631073\n",
      "Epoch 2160, Loss: 0.6219246536493301, Final Batch Loss: 0.25586873292922974\n",
      "Epoch 2161, Loss: 0.615550696849823, Final Batch Loss: 0.18353766202926636\n",
      "Epoch 2162, Loss: 0.5379670411348343, Final Batch Loss: 0.1255401372909546\n",
      "Epoch 2163, Loss: 0.5558087378740311, Final Batch Loss: 0.23922376334667206\n",
      "Epoch 2164, Loss: 0.5285840034484863, Final Batch Loss: 0.17483757436275482\n",
      "Epoch 2165, Loss: 0.5391916707158089, Final Batch Loss: 0.123458631336689\n",
      "Epoch 2166, Loss: 0.4946676790714264, Final Batch Loss: 0.17471441626548767\n",
      "Epoch 2167, Loss: 0.6240675151348114, Final Batch Loss: 0.18803811073303223\n",
      "Epoch 2168, Loss: 0.5940379798412323, Final Batch Loss: 0.23896047472953796\n",
      "Epoch 2169, Loss: 0.60873843729496, Final Batch Loss: 0.22507594525814056\n",
      "Epoch 2170, Loss: 0.5431088954210281, Final Batch Loss: 0.1428195685148239\n",
      "Epoch 2171, Loss: 0.5669740438461304, Final Batch Loss: 0.1468532830476761\n",
      "Epoch 2172, Loss: 0.571900874376297, Final Batch Loss: 0.2223087102174759\n",
      "Epoch 2173, Loss: 0.5879445523023605, Final Batch Loss: 0.21273963153362274\n",
      "Epoch 2174, Loss: 0.5366180390119553, Final Batch Loss: 0.12155571579933167\n",
      "Epoch 2175, Loss: 0.6348136961460114, Final Batch Loss: 0.17925554513931274\n",
      "Epoch 2176, Loss: 0.5998962372541428, Final Batch Loss: 0.19309602677822113\n",
      "Epoch 2177, Loss: 0.5308656096458435, Final Batch Loss: 0.13652533292770386\n",
      "Epoch 2178, Loss: 0.6094416230916977, Final Batch Loss: 0.23797500133514404\n",
      "Epoch 2179, Loss: 0.550797700881958, Final Batch Loss: 0.17326699197292328\n",
      "Epoch 2180, Loss: 0.5371412038803101, Final Batch Loss: 0.19525030255317688\n",
      "Epoch 2181, Loss: 0.5999816358089447, Final Batch Loss: 0.22054298222064972\n",
      "Epoch 2182, Loss: 0.5705149322748184, Final Batch Loss: 0.20036229491233826\n",
      "Epoch 2183, Loss: 0.49255600571632385, Final Batch Loss: 0.16842494904994965\n",
      "Epoch 2184, Loss: 0.5540667921304703, Final Batch Loss: 0.167496919631958\n",
      "Epoch 2185, Loss: 0.5321175158023834, Final Batch Loss: 0.19313107430934906\n",
      "Epoch 2186, Loss: 0.5176490098237991, Final Batch Loss: 0.17234010994434357\n",
      "Epoch 2187, Loss: 0.5060884207487106, Final Batch Loss: 0.1360751837491989\n",
      "Epoch 2188, Loss: 0.5537361949682236, Final Batch Loss: 0.20735934376716614\n",
      "Epoch 2189, Loss: 0.5236699134111404, Final Batch Loss: 0.1440374106168747\n",
      "Epoch 2190, Loss: 0.6420836597681046, Final Batch Loss: 0.17676584422588348\n",
      "Epoch 2191, Loss: 0.5195086449384689, Final Batch Loss: 0.1662696897983551\n",
      "Epoch 2192, Loss: 0.5153163522481918, Final Batch Loss: 0.15639783442020416\n",
      "Epoch 2193, Loss: 0.5351573675870895, Final Batch Loss: 0.1366543173789978\n",
      "Epoch 2194, Loss: 0.5494758039712906, Final Batch Loss: 0.18472613394260406\n",
      "Epoch 2195, Loss: 0.603970468044281, Final Batch Loss: 0.2350609004497528\n",
      "Epoch 2196, Loss: 0.5466219931840897, Final Batch Loss: 0.20196861028671265\n",
      "Epoch 2197, Loss: 0.5690943449735641, Final Batch Loss: 0.14411547780036926\n",
      "Epoch 2198, Loss: 0.5795735418796539, Final Batch Loss: 0.2001541405916214\n",
      "Epoch 2199, Loss: 0.592083290219307, Final Batch Loss: 0.17901328206062317\n",
      "Epoch 2200, Loss: 0.5248260498046875, Final Batch Loss: 0.18654301762580872\n",
      "Epoch 2201, Loss: 0.6161143928766251, Final Batch Loss: 0.2146262228488922\n",
      "Epoch 2202, Loss: 0.4122077226638794, Final Batch Loss: 0.10004658997058868\n",
      "Epoch 2203, Loss: 0.5120769590139389, Final Batch Loss: 0.17377763986587524\n",
      "Epoch 2204, Loss: 0.5392774939537048, Final Batch Loss: 0.19429661333560944\n",
      "Epoch 2205, Loss: 0.5991589725017548, Final Batch Loss: 0.2285463958978653\n",
      "Epoch 2206, Loss: 0.5041216760873795, Final Batch Loss: 0.1799803376197815\n",
      "Epoch 2207, Loss: 0.5700356215238571, Final Batch Loss: 0.1843736171722412\n",
      "Epoch 2208, Loss: 0.620266318321228, Final Batch Loss: 0.1599149852991104\n",
      "Epoch 2209, Loss: 0.5884124636650085, Final Batch Loss: 0.25030064582824707\n",
      "Epoch 2210, Loss: 0.5744937062263489, Final Batch Loss: 0.19485148787498474\n",
      "Epoch 2211, Loss: 0.4882976710796356, Final Batch Loss: 0.19020810723304749\n",
      "Epoch 2212, Loss: 0.607956200838089, Final Batch Loss: 0.2494593858718872\n",
      "Epoch 2213, Loss: 0.4825107157230377, Final Batch Loss: 0.14812195301055908\n",
      "Epoch 2214, Loss: 0.4964854568243027, Final Batch Loss: 0.17185243964195251\n",
      "Epoch 2215, Loss: 0.5103133469820023, Final Batch Loss: 0.18432864546775818\n",
      "Epoch 2216, Loss: 0.4947218745946884, Final Batch Loss: 0.1313379853963852\n",
      "Epoch 2217, Loss: 0.665639340877533, Final Batch Loss: 0.2770303189754486\n",
      "Epoch 2218, Loss: 0.48798584192991257, Final Batch Loss: 0.17968373000621796\n",
      "Epoch 2219, Loss: 0.4904526472091675, Final Batch Loss: 0.17072544991970062\n",
      "Epoch 2220, Loss: 0.5844054073095322, Final Batch Loss: 0.15102951228618622\n",
      "Epoch 2221, Loss: 0.5084673166275024, Final Batch Loss: 0.21387958526611328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2222, Loss: 0.578939899802208, Final Batch Loss: 0.1648688167333603\n",
      "Epoch 2223, Loss: 0.5397052019834518, Final Batch Loss: 0.20278359949588776\n",
      "Epoch 2224, Loss: 0.48277226090431213, Final Batch Loss: 0.13029126822948456\n",
      "Epoch 2225, Loss: 0.6136905997991562, Final Batch Loss: 0.19458022713661194\n",
      "Epoch 2226, Loss: 0.497979074716568, Final Batch Loss: 0.18099308013916016\n",
      "Epoch 2227, Loss: 0.6889587044715881, Final Batch Loss: 0.23789694905281067\n",
      "Epoch 2228, Loss: 0.5974670052528381, Final Batch Loss: 0.24888409674167633\n",
      "Epoch 2229, Loss: 0.6419283747673035, Final Batch Loss: 0.31419268250465393\n",
      "Epoch 2230, Loss: 0.5723214894533157, Final Batch Loss: 0.2088198959827423\n",
      "Epoch 2231, Loss: 0.5291098356246948, Final Batch Loss: 0.16944865882396698\n",
      "Epoch 2232, Loss: 0.7480354011058807, Final Batch Loss: 0.27683284878730774\n",
      "Epoch 2233, Loss: 0.5642877221107483, Final Batch Loss: 0.18676364421844482\n",
      "Epoch 2234, Loss: 0.46358418464660645, Final Batch Loss: 0.1469126045703888\n",
      "Epoch 2235, Loss: 0.6212224811315536, Final Batch Loss: 0.19180867075920105\n",
      "Epoch 2236, Loss: 0.4661344811320305, Final Batch Loss: 0.11237653344869614\n",
      "Epoch 2237, Loss: 0.5733544677495956, Final Batch Loss: 0.21948648989200592\n",
      "Epoch 2238, Loss: 0.4827350303530693, Final Batch Loss: 0.1929883509874344\n",
      "Epoch 2239, Loss: 0.5533689558506012, Final Batch Loss: 0.1576865166425705\n",
      "Epoch 2240, Loss: 0.6235762685537338, Final Batch Loss: 0.2662299573421478\n",
      "Epoch 2241, Loss: 0.5590529590845108, Final Batch Loss: 0.2206069827079773\n",
      "Epoch 2242, Loss: 0.6747839450836182, Final Batch Loss: 0.22919242084026337\n",
      "Epoch 2243, Loss: 0.4562113285064697, Final Batch Loss: 0.13105688989162445\n",
      "Epoch 2244, Loss: 0.5964374095201492, Final Batch Loss: 0.19543872773647308\n",
      "Epoch 2245, Loss: 0.5475804656744003, Final Batch Loss: 0.18785762786865234\n",
      "Epoch 2246, Loss: 0.6228899955749512, Final Batch Loss: 0.15926863253116608\n",
      "Epoch 2247, Loss: 0.5705953389406204, Final Batch Loss: 0.2026538997888565\n",
      "Epoch 2248, Loss: 0.5589517056941986, Final Batch Loss: 0.15374432504177094\n",
      "Epoch 2249, Loss: 0.5342736095190048, Final Batch Loss: 0.17845629155635834\n",
      "Epoch 2250, Loss: 0.5299975126981735, Final Batch Loss: 0.16873477399349213\n",
      "Epoch 2251, Loss: 0.5120029747486115, Final Batch Loss: 0.1560550183057785\n",
      "Epoch 2252, Loss: 0.44862955808639526, Final Batch Loss: 0.07563327252864838\n",
      "Epoch 2253, Loss: 0.5049166232347488, Final Batch Loss: 0.14900796115398407\n",
      "Epoch 2254, Loss: 0.5037650465965271, Final Batch Loss: 0.17637883126735687\n",
      "Epoch 2255, Loss: 0.5650521665811539, Final Batch Loss: 0.14390526711940765\n",
      "Epoch 2256, Loss: 0.490504153072834, Final Batch Loss: 0.2044435739517212\n",
      "Epoch 2257, Loss: 0.468304842710495, Final Batch Loss: 0.11335383355617523\n",
      "Epoch 2258, Loss: 0.5920881927013397, Final Batch Loss: 0.24074669182300568\n",
      "Epoch 2259, Loss: 0.6025315672159195, Final Batch Loss: 0.22291545569896698\n",
      "Epoch 2260, Loss: 0.5643640607595444, Final Batch Loss: 0.16759221255779266\n",
      "Epoch 2261, Loss: 0.5107163339853287, Final Batch Loss: 0.21086373925209045\n",
      "Epoch 2262, Loss: 0.5520975887775421, Final Batch Loss: 0.165449857711792\n",
      "Epoch 2263, Loss: 0.46768471598625183, Final Batch Loss: 0.1189693957567215\n",
      "Epoch 2264, Loss: 0.5221558064222336, Final Batch Loss: 0.15469612181186676\n",
      "Epoch 2265, Loss: 0.4577146917581558, Final Batch Loss: 0.12613338232040405\n",
      "Epoch 2266, Loss: 0.5606735646724701, Final Batch Loss: 0.20494817197322845\n",
      "Epoch 2267, Loss: 0.6521286070346832, Final Batch Loss: 0.23507583141326904\n",
      "Epoch 2268, Loss: 0.5424518883228302, Final Batch Loss: 0.1960325390100479\n",
      "Epoch 2269, Loss: 0.6288184374570847, Final Batch Loss: 0.15857765078544617\n",
      "Epoch 2270, Loss: 0.5459800362586975, Final Batch Loss: 0.16380411386489868\n",
      "Epoch 2271, Loss: 0.5942167118191719, Final Batch Loss: 0.08752182871103287\n",
      "Epoch 2272, Loss: 0.6333366632461548, Final Batch Loss: 0.2686415910720825\n",
      "Epoch 2273, Loss: 0.5208937972784042, Final Batch Loss: 0.13597889244556427\n",
      "Epoch 2274, Loss: 0.48622437566518784, Final Batch Loss: 0.22873830795288086\n",
      "Epoch 2275, Loss: 0.5412308722734451, Final Batch Loss: 0.13863243162631989\n",
      "Epoch 2276, Loss: 0.49914149940013885, Final Batch Loss: 0.20145070552825928\n",
      "Epoch 2277, Loss: 0.49333716928958893, Final Batch Loss: 0.17797139286994934\n",
      "Epoch 2278, Loss: 0.5696262121200562, Final Batch Loss: 0.20292291045188904\n",
      "Epoch 2279, Loss: 0.4771018922328949, Final Batch Loss: 0.14204250276088715\n",
      "Epoch 2280, Loss: 0.45878300070762634, Final Batch Loss: 0.15268540382385254\n",
      "Epoch 2281, Loss: 0.464120090007782, Final Batch Loss: 0.2106713205575943\n",
      "Epoch 2282, Loss: 0.5596895664930344, Final Batch Loss: 0.21896804869174957\n",
      "Epoch 2283, Loss: 0.5130416601896286, Final Batch Loss: 0.1802721470594406\n",
      "Epoch 2284, Loss: 0.5293692499399185, Final Batch Loss: 0.15685003995895386\n",
      "Epoch 2285, Loss: 0.4784209877252579, Final Batch Loss: 0.12946172058582306\n",
      "Epoch 2286, Loss: 0.6565768122673035, Final Batch Loss: 0.25050199031829834\n",
      "Epoch 2287, Loss: 0.5057629495859146, Final Batch Loss: 0.1228027492761612\n",
      "Epoch 2288, Loss: 0.5938939899206161, Final Batch Loss: 0.27393001317977905\n",
      "Epoch 2289, Loss: 0.4912101775407791, Final Batch Loss: 0.1603400856256485\n",
      "Epoch 2290, Loss: 0.5557871609926224, Final Batch Loss: 0.19565747678279877\n",
      "Epoch 2291, Loss: 0.4949833154678345, Final Batch Loss: 0.13236163556575775\n",
      "Epoch 2292, Loss: 0.5833474695682526, Final Batch Loss: 0.22005264461040497\n",
      "Epoch 2293, Loss: 0.45014798641204834, Final Batch Loss: 0.12283051013946533\n",
      "Epoch 2294, Loss: 0.5620869994163513, Final Batch Loss: 0.15228885412216187\n",
      "Epoch 2295, Loss: 0.5638962164521217, Final Batch Loss: 0.18000900745391846\n",
      "Epoch 2296, Loss: 0.5348154231905937, Final Batch Loss: 0.12429799884557724\n",
      "Epoch 2297, Loss: 0.582177072763443, Final Batch Loss: 0.21542136371135712\n",
      "Epoch 2298, Loss: 0.5535922050476074, Final Batch Loss: 0.21324579417705536\n",
      "Epoch 2299, Loss: 0.5840204060077667, Final Batch Loss: 0.27429065108299255\n",
      "Epoch 2300, Loss: 0.552541472017765, Final Batch Loss: 0.11456207185983658\n",
      "Epoch 2301, Loss: 0.43416082859039307, Final Batch Loss: 0.13536378741264343\n",
      "Epoch 2302, Loss: 0.5143350064754486, Final Batch Loss: 0.2228713035583496\n",
      "Epoch 2303, Loss: 0.4599006772041321, Final Batch Loss: 0.11994720995426178\n",
      "Epoch 2304, Loss: 0.6404189616441727, Final Batch Loss: 0.2073797881603241\n",
      "Epoch 2305, Loss: 0.7355408370494843, Final Batch Loss: 0.38188809156417847\n",
      "Epoch 2306, Loss: 0.5278985649347305, Final Batch Loss: 0.17461037635803223\n",
      "Epoch 2307, Loss: 0.6031053066253662, Final Batch Loss: 0.1418042927980423\n",
      "Epoch 2308, Loss: 0.5494763553142548, Final Batch Loss: 0.14514358341693878\n",
      "Epoch 2309, Loss: 0.6001510471105576, Final Batch Loss: 0.22103172540664673\n",
      "Epoch 2310, Loss: 0.49124059826135635, Final Batch Loss: 0.11300132423639297\n",
      "Epoch 2311, Loss: 0.5964062958955765, Final Batch Loss: 0.14896553754806519\n",
      "Epoch 2312, Loss: 0.44098706543445587, Final Batch Loss: 0.11260060966014862\n",
      "Epoch 2313, Loss: 0.4577484205365181, Final Batch Loss: 0.12233736366033554\n",
      "Epoch 2314, Loss: 0.6094740405678749, Final Batch Loss: 0.2781379818916321\n",
      "Epoch 2315, Loss: 0.5668439120054245, Final Batch Loss: 0.1794053465127945\n",
      "Epoch 2316, Loss: 0.49456965923309326, Final Batch Loss: 0.15039469301700592\n",
      "Epoch 2317, Loss: 0.46693722158670425, Final Batch Loss: 0.1128648892045021\n",
      "Epoch 2318, Loss: 0.5128210857510567, Final Batch Loss: 0.20834288001060486\n",
      "Epoch 2319, Loss: 0.5883143246173859, Final Batch Loss: 0.20850805938243866\n",
      "Epoch 2320, Loss: 0.5600499510765076, Final Batch Loss: 0.1730436235666275\n",
      "Epoch 2321, Loss: 0.5537443310022354, Final Batch Loss: 0.2117365449666977\n",
      "Epoch 2322, Loss: 0.4957168251276016, Final Batch Loss: 0.1831159144639969\n",
      "Epoch 2323, Loss: 0.5359740853309631, Final Batch Loss: 0.2018444985151291\n",
      "Epoch 2324, Loss: 0.6854477226734161, Final Batch Loss: 0.32485389709472656\n",
      "Epoch 2325, Loss: 0.5344030559062958, Final Batch Loss: 0.151678666472435\n",
      "Epoch 2326, Loss: 0.5992505252361298, Final Batch Loss: 0.23449769616127014\n",
      "Epoch 2327, Loss: 0.5258852392435074, Final Batch Loss: 0.14070163667201996\n",
      "Epoch 2328, Loss: 0.5604933947324753, Final Batch Loss: 0.22840090095996857\n",
      "Epoch 2329, Loss: 0.5379156321287155, Final Batch Loss: 0.14640118181705475\n",
      "Epoch 2330, Loss: 0.45876066386699677, Final Batch Loss: 0.10171034932136536\n",
      "Epoch 2331, Loss: 0.5743445605039597, Final Batch Loss: 0.31524813175201416\n",
      "Epoch 2332, Loss: 0.5046956390142441, Final Batch Loss: 0.18461693823337555\n",
      "Epoch 2333, Loss: 0.489338681101799, Final Batch Loss: 0.18170122802257538\n",
      "Epoch 2334, Loss: 0.5298014879226685, Final Batch Loss: 0.20668719708919525\n",
      "Epoch 2335, Loss: 0.5299787223339081, Final Batch Loss: 0.18406805396080017\n",
      "Epoch 2336, Loss: 0.4854564815759659, Final Batch Loss: 0.12122562527656555\n",
      "Epoch 2337, Loss: 0.6530948877334595, Final Batch Loss: 0.2606317102909088\n",
      "Epoch 2338, Loss: 0.4881610572338104, Final Batch Loss: 0.11487835645675659\n",
      "Epoch 2339, Loss: 0.6089425981044769, Final Batch Loss: 0.19181066751480103\n",
      "Epoch 2340, Loss: 0.5608955770730972, Final Batch Loss: 0.19775895774364471\n",
      "Epoch 2341, Loss: 0.6248634308576584, Final Batch Loss: 0.16948802769184113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2342, Loss: 0.6077365875244141, Final Batch Loss: 0.16986137628555298\n",
      "Epoch 2343, Loss: 0.49690572917461395, Final Batch Loss: 0.1751038134098053\n",
      "Epoch 2344, Loss: 0.5480000972747803, Final Batch Loss: 0.16901296377182007\n",
      "Epoch 2345, Loss: 0.5141801238059998, Final Batch Loss: 0.20672282576560974\n",
      "Epoch 2346, Loss: 0.5631136894226074, Final Batch Loss: 0.24331624805927277\n",
      "Epoch 2347, Loss: 0.600112184882164, Final Batch Loss: 0.2853200137615204\n",
      "Epoch 2348, Loss: 0.6224735230207443, Final Batch Loss: 0.20691296458244324\n",
      "Epoch 2349, Loss: 0.5997728109359741, Final Batch Loss: 0.2407112866640091\n",
      "Epoch 2350, Loss: 0.4781477600336075, Final Batch Loss: 0.10353639721870422\n",
      "Epoch 2351, Loss: 0.486093133687973, Final Batch Loss: 0.1346493661403656\n",
      "Epoch 2352, Loss: 0.45784927904605865, Final Batch Loss: 0.14440496265888214\n",
      "Epoch 2353, Loss: 0.5076013505458832, Final Batch Loss: 0.19114892184734344\n",
      "Epoch 2354, Loss: 0.5011177882552147, Final Batch Loss: 0.1955859512090683\n",
      "Epoch 2355, Loss: 0.5253327488899231, Final Batch Loss: 0.13477416336536407\n",
      "Epoch 2356, Loss: 0.5524679273366928, Final Batch Loss: 0.18702413141727448\n",
      "Epoch 2357, Loss: 0.6499295979738235, Final Batch Loss: 0.19359759986400604\n",
      "Epoch 2358, Loss: 0.56373031437397, Final Batch Loss: 0.1687435805797577\n",
      "Epoch 2359, Loss: 0.5514672696590424, Final Batch Loss: 0.2445739507675171\n",
      "Epoch 2360, Loss: 0.6043479442596436, Final Batch Loss: 0.1970931887626648\n",
      "Epoch 2361, Loss: 0.6191592663526535, Final Batch Loss: 0.1817302405834198\n",
      "Epoch 2362, Loss: 0.5211779177188873, Final Batch Loss: 0.18625611066818237\n",
      "Epoch 2363, Loss: 0.5871095508337021, Final Batch Loss: 0.20180200040340424\n",
      "Epoch 2364, Loss: 0.46349038928747177, Final Batch Loss: 0.10516176372766495\n",
      "Epoch 2365, Loss: 0.5863809883594513, Final Batch Loss: 0.20621047914028168\n",
      "Epoch 2366, Loss: 0.49021802842617035, Final Batch Loss: 0.1724497377872467\n",
      "Epoch 2367, Loss: 0.49569451808929443, Final Batch Loss: 0.16207516193389893\n",
      "Epoch 2368, Loss: 0.49930986762046814, Final Batch Loss: 0.10922364890575409\n",
      "Epoch 2369, Loss: 0.5656192004680634, Final Batch Loss: 0.1785431206226349\n",
      "Epoch 2370, Loss: 0.5828161835670471, Final Batch Loss: 0.2652023732662201\n",
      "Epoch 2371, Loss: 0.5220092833042145, Final Batch Loss: 0.1859370917081833\n",
      "Epoch 2372, Loss: 0.49207378923892975, Final Batch Loss: 0.17821772396564484\n",
      "Epoch 2373, Loss: 0.5251998454332352, Final Batch Loss: 0.19846545159816742\n",
      "Epoch 2374, Loss: 0.44465768337249756, Final Batch Loss: 0.17664119601249695\n",
      "Epoch 2375, Loss: 0.5664566457271576, Final Batch Loss: 0.19326256215572357\n",
      "Epoch 2376, Loss: 0.5346313267946243, Final Batch Loss: 0.2088477462530136\n",
      "Epoch 2377, Loss: 0.4721565693616867, Final Batch Loss: 0.18054138123989105\n",
      "Epoch 2378, Loss: 0.5427039116621017, Final Batch Loss: 0.1914711743593216\n",
      "Epoch 2379, Loss: 0.5529415160417557, Final Batch Loss: 0.17200639843940735\n",
      "Epoch 2380, Loss: 0.5054605901241302, Final Batch Loss: 0.1630673110485077\n",
      "Epoch 2381, Loss: 0.5368983447551727, Final Batch Loss: 0.25545990467071533\n",
      "Epoch 2382, Loss: 0.5394610613584518, Final Batch Loss: 0.15440483391284943\n",
      "Epoch 2383, Loss: 0.5384650826454163, Final Batch Loss: 0.15044541656970978\n",
      "Epoch 2384, Loss: 0.48479361832141876, Final Batch Loss: 0.13600848615169525\n",
      "Epoch 2385, Loss: 0.4923718571662903, Final Batch Loss: 0.12118683755397797\n",
      "Epoch 2386, Loss: 0.5253742933273315, Final Batch Loss: 0.18393588066101074\n",
      "Epoch 2387, Loss: 0.5523305982351303, Final Batch Loss: 0.13680996000766754\n",
      "Epoch 2388, Loss: 0.5034278109669685, Final Batch Loss: 0.25335749983787537\n",
      "Epoch 2389, Loss: 0.6113206446170807, Final Batch Loss: 0.21495725214481354\n",
      "Epoch 2390, Loss: 0.45910604298114777, Final Batch Loss: 0.13753505051136017\n",
      "Epoch 2391, Loss: 0.6054379642009735, Final Batch Loss: 0.21449914574623108\n",
      "Epoch 2392, Loss: 0.5412270873785019, Final Batch Loss: 0.19633109867572784\n",
      "Epoch 2393, Loss: 0.675565168261528, Final Batch Loss: 0.2607969045639038\n",
      "Epoch 2394, Loss: 0.4656188189983368, Final Batch Loss: 0.18087317049503326\n",
      "Epoch 2395, Loss: 0.5409468412399292, Final Batch Loss: 0.15772981941699982\n",
      "Epoch 2396, Loss: 0.5550987273454666, Final Batch Loss: 0.211296945810318\n",
      "Epoch 2397, Loss: 0.5191021710634232, Final Batch Loss: 0.18474383652210236\n",
      "Epoch 2398, Loss: 0.6254499703645706, Final Batch Loss: 0.23410817980766296\n",
      "Epoch 2399, Loss: 0.509149819612503, Final Batch Loss: 0.1839873194694519\n",
      "Epoch 2400, Loss: 0.5429872274398804, Final Batch Loss: 0.15810005366802216\n",
      "Epoch 2401, Loss: 0.4842945635318756, Final Batch Loss: 0.10121633112430573\n",
      "Epoch 2402, Loss: 0.6356755942106247, Final Batch Loss: 0.2553068995475769\n",
      "Epoch 2403, Loss: 0.5027646422386169, Final Batch Loss: 0.15368524193763733\n",
      "Epoch 2404, Loss: 0.4673066884279251, Final Batch Loss: 0.09760650992393494\n",
      "Epoch 2405, Loss: 0.43126973509788513, Final Batch Loss: 0.11400212347507477\n",
      "Epoch 2406, Loss: 0.4531274735927582, Final Batch Loss: 0.16415683925151825\n",
      "Epoch 2407, Loss: 0.6339169144630432, Final Batch Loss: 0.28885379433631897\n",
      "Epoch 2408, Loss: 0.4586748257279396, Final Batch Loss: 0.10592404752969742\n",
      "Epoch 2409, Loss: 0.5389996767044067, Final Batch Loss: 0.15750768780708313\n",
      "Epoch 2410, Loss: 0.5213652998209, Final Batch Loss: 0.18914742767810822\n",
      "Epoch 2411, Loss: 0.47191737592220306, Final Batch Loss: 0.1376621127128601\n",
      "Epoch 2412, Loss: 0.5413166880607605, Final Batch Loss: 0.19907338917255402\n",
      "Epoch 2413, Loss: 0.48725465685129166, Final Batch Loss: 0.10067816823720932\n",
      "Epoch 2414, Loss: 0.5317400246858597, Final Batch Loss: 0.15683132410049438\n",
      "Epoch 2415, Loss: 0.4818066507577896, Final Batch Loss: 0.1507914811372757\n",
      "Epoch 2416, Loss: 0.4800746887922287, Final Batch Loss: 0.1765935868024826\n",
      "Epoch 2417, Loss: 0.4975007697939873, Final Batch Loss: 0.11225386708974838\n",
      "Epoch 2418, Loss: 0.5629480928182602, Final Batch Loss: 0.1892247349023819\n",
      "Epoch 2419, Loss: 0.5236162692308426, Final Batch Loss: 0.13536617159843445\n",
      "Epoch 2420, Loss: 0.6319646239280701, Final Batch Loss: 0.14092960953712463\n",
      "Epoch 2421, Loss: 0.44741928577423096, Final Batch Loss: 0.11910825967788696\n",
      "Epoch 2422, Loss: 0.45531390607357025, Final Batch Loss: 0.12240403890609741\n",
      "Epoch 2423, Loss: 0.5714680999517441, Final Batch Loss: 0.16953429579734802\n",
      "Epoch 2424, Loss: 0.47734613716602325, Final Batch Loss: 0.13823184370994568\n",
      "Epoch 2425, Loss: 0.5615706741809845, Final Batch Loss: 0.2155781388282776\n",
      "Epoch 2426, Loss: 0.510094478726387, Final Batch Loss: 0.21846778690814972\n",
      "Epoch 2427, Loss: 0.480014368891716, Final Batch Loss: 0.14475874602794647\n",
      "Epoch 2428, Loss: 0.5693048387765884, Final Batch Loss: 0.1268894076347351\n",
      "Epoch 2429, Loss: 0.5293754488229752, Final Batch Loss: 0.1536247879266739\n",
      "Epoch 2430, Loss: 0.5323762744665146, Final Batch Loss: 0.20003952085971832\n",
      "Epoch 2431, Loss: 0.483368456363678, Final Batch Loss: 0.1994328647851944\n",
      "Epoch 2432, Loss: 0.5573373138904572, Final Batch Loss: 0.21454192698001862\n",
      "Epoch 2433, Loss: 0.5753356963396072, Final Batch Loss: 0.1677892655134201\n",
      "Epoch 2434, Loss: 0.5052616149187088, Final Batch Loss: 0.1631057858467102\n",
      "Epoch 2435, Loss: 0.4575563818216324, Final Batch Loss: 0.1407865285873413\n",
      "Epoch 2436, Loss: 0.5407457649707794, Final Batch Loss: 0.15934138000011444\n",
      "Epoch 2437, Loss: 0.5402348935604095, Final Batch Loss: 0.18939685821533203\n",
      "Epoch 2438, Loss: 0.6031472086906433, Final Batch Loss: 0.2627064287662506\n",
      "Epoch 2439, Loss: 0.6164605170488358, Final Batch Loss: 0.2799288332462311\n",
      "Epoch 2440, Loss: 0.5148705989122391, Final Batch Loss: 0.21778401732444763\n",
      "Epoch 2441, Loss: 0.491764634847641, Final Batch Loss: 0.2045980989933014\n",
      "Epoch 2442, Loss: 0.5522514581680298, Final Batch Loss: 0.218965545296669\n",
      "Epoch 2443, Loss: 0.5428149253129959, Final Batch Loss: 0.14418365061283112\n",
      "Epoch 2444, Loss: 0.5227648317813873, Final Batch Loss: 0.1931469738483429\n",
      "Epoch 2445, Loss: 0.5482850074768066, Final Batch Loss: 0.20380766689777374\n",
      "Epoch 2446, Loss: 0.44955745339393616, Final Batch Loss: 0.1564256250858307\n",
      "Epoch 2447, Loss: 0.49978819489479065, Final Batch Loss: 0.16354478895664215\n",
      "Epoch 2448, Loss: 0.5395087748765945, Final Batch Loss: 0.21484941244125366\n",
      "Epoch 2449, Loss: 0.48936134576797485, Final Batch Loss: 0.1535230129957199\n",
      "Epoch 2450, Loss: 0.5175260305404663, Final Batch Loss: 0.20873510837554932\n",
      "Epoch 2451, Loss: 0.49943700432777405, Final Batch Loss: 0.08798223733901978\n",
      "Epoch 2452, Loss: 0.5729047656059265, Final Batch Loss: 0.17129676043987274\n",
      "Epoch 2453, Loss: 0.5018678307533264, Final Batch Loss: 0.13725703954696655\n",
      "Epoch 2454, Loss: 0.6834967583417892, Final Batch Loss: 0.24822935461997986\n",
      "Epoch 2455, Loss: 0.5281549990177155, Final Batch Loss: 0.08817702531814575\n",
      "Epoch 2456, Loss: 0.6684893369674683, Final Batch Loss: 0.28451424837112427\n",
      "Epoch 2457, Loss: 0.504058837890625, Final Batch Loss: 0.18717937171459198\n",
      "Epoch 2458, Loss: 0.5574628263711929, Final Batch Loss: 0.17181144654750824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2459, Loss: 0.5738472193479538, Final Batch Loss: 0.2008788287639618\n",
      "Epoch 2460, Loss: 0.6794615387916565, Final Batch Loss: 0.3065929412841797\n",
      "Epoch 2461, Loss: 0.4643121212720871, Final Batch Loss: 0.14261183142662048\n",
      "Epoch 2462, Loss: 0.5745814591646194, Final Batch Loss: 0.12086901068687439\n",
      "Epoch 2463, Loss: 0.5093564987182617, Final Batch Loss: 0.18090282380580902\n",
      "Epoch 2464, Loss: 0.45155447721481323, Final Batch Loss: 0.1514441967010498\n",
      "Epoch 2465, Loss: 0.522170215845108, Final Batch Loss: 0.23091498017311096\n",
      "Epoch 2466, Loss: 0.5606153309345245, Final Batch Loss: 0.2555217742919922\n",
      "Epoch 2467, Loss: 0.4950604662299156, Final Batch Loss: 0.0963122621178627\n",
      "Epoch 2468, Loss: 0.5445048660039902, Final Batch Loss: 0.2417047917842865\n",
      "Epoch 2469, Loss: 0.453089639544487, Final Batch Loss: 0.14060015976428986\n",
      "Epoch 2470, Loss: 0.6010755300521851, Final Batch Loss: 0.24903017282485962\n",
      "Epoch 2471, Loss: 0.5158867537975311, Final Batch Loss: 0.15455158054828644\n",
      "Epoch 2472, Loss: 0.5516664981842041, Final Batch Loss: 0.20108041167259216\n",
      "Epoch 2473, Loss: 0.44322480261325836, Final Batch Loss: 0.13825857639312744\n",
      "Epoch 2474, Loss: 0.45410436391830444, Final Batch Loss: 0.13269037008285522\n",
      "Epoch 2475, Loss: 0.5648080557584763, Final Batch Loss: 0.213809072971344\n",
      "Epoch 2476, Loss: 0.6030988842248917, Final Batch Loss: 0.2158050537109375\n",
      "Epoch 2477, Loss: 0.5667773485183716, Final Batch Loss: 0.22452254593372345\n",
      "Epoch 2478, Loss: 0.4895261526107788, Final Batch Loss: 0.1656763106584549\n",
      "Epoch 2479, Loss: 0.5340417772531509, Final Batch Loss: 0.14256992936134338\n",
      "Epoch 2480, Loss: 0.5356056243181229, Final Batch Loss: 0.21930865943431854\n",
      "Epoch 2481, Loss: 0.5426360964775085, Final Batch Loss: 0.1958622932434082\n",
      "Epoch 2482, Loss: 0.5230599939823151, Final Batch Loss: 0.18804095685482025\n",
      "Epoch 2483, Loss: 0.48217834532260895, Final Batch Loss: 0.12548989057540894\n",
      "Epoch 2484, Loss: 0.5644449293613434, Final Batch Loss: 0.19343343377113342\n",
      "Epoch 2485, Loss: 0.5214319229125977, Final Batch Loss: 0.17733721435070038\n",
      "Epoch 2486, Loss: 0.4569869935512543, Final Batch Loss: 0.15977942943572998\n",
      "Epoch 2487, Loss: 0.5522972345352173, Final Batch Loss: 0.15896089375019073\n",
      "Epoch 2488, Loss: 0.43950948119163513, Final Batch Loss: 0.10130266845226288\n",
      "Epoch 2489, Loss: 0.5454651862382889, Final Batch Loss: 0.19748778641223907\n",
      "Epoch 2490, Loss: 0.460577130317688, Final Batch Loss: 0.1300664246082306\n",
      "Epoch 2491, Loss: 0.48531483113765717, Final Batch Loss: 0.17269665002822876\n",
      "Epoch 2492, Loss: 0.4388653114438057, Final Batch Loss: 0.09166938811540604\n",
      "Epoch 2493, Loss: 0.43857699632644653, Final Batch Loss: 0.1535143405199051\n",
      "Epoch 2494, Loss: 0.4741627424955368, Final Batch Loss: 0.1322147697210312\n",
      "Epoch 2495, Loss: 0.5610947832465172, Final Batch Loss: 0.10953345149755478\n",
      "Epoch 2496, Loss: 0.4849829226732254, Final Batch Loss: 0.14958710968494415\n",
      "Epoch 2497, Loss: 0.4339142292737961, Final Batch Loss: 0.16101036965847015\n",
      "Epoch 2498, Loss: 0.4674580618739128, Final Batch Loss: 0.2145773321390152\n",
      "Epoch 2499, Loss: 0.5228457897901535, Final Batch Loss: 0.14945758879184723\n",
      "Epoch 2500, Loss: 0.5265278667211533, Final Batch Loss: 0.16613532602787018\n",
      "Epoch 2501, Loss: 0.5344776213169098, Final Batch Loss: 0.2195337414741516\n",
      "Epoch 2502, Loss: 0.4446147233247757, Final Batch Loss: 0.17220774292945862\n",
      "Epoch 2503, Loss: 0.5010792836546898, Final Batch Loss: 0.21421025693416595\n",
      "Epoch 2504, Loss: 0.5392298400402069, Final Batch Loss: 0.19618402421474457\n",
      "Epoch 2505, Loss: 0.49922728538513184, Final Batch Loss: 0.1785021871328354\n",
      "Epoch 2506, Loss: 0.4908301830291748, Final Batch Loss: 0.15588289499282837\n",
      "Epoch 2507, Loss: 0.3975926488637924, Final Batch Loss: 0.1397559642791748\n",
      "Epoch 2508, Loss: 0.5457681715488434, Final Batch Loss: 0.17895656824111938\n",
      "Epoch 2509, Loss: 0.42326951026916504, Final Batch Loss: 0.1497284471988678\n",
      "Epoch 2510, Loss: 0.4558340609073639, Final Batch Loss: 0.12930026650428772\n",
      "Epoch 2511, Loss: 0.44054874777793884, Final Batch Loss: 0.10967907309532166\n",
      "Epoch 2512, Loss: 0.5144744962453842, Final Batch Loss: 0.14611539244651794\n",
      "Epoch 2513, Loss: 0.49131208658218384, Final Batch Loss: 0.18248692154884338\n",
      "Epoch 2514, Loss: 0.535148561000824, Final Batch Loss: 0.1737542599439621\n",
      "Epoch 2515, Loss: 0.4924396276473999, Final Batch Loss: 0.18290826678276062\n",
      "Epoch 2516, Loss: 0.4860105663537979, Final Batch Loss: 0.1726265251636505\n",
      "Epoch 2517, Loss: 0.6121296286582947, Final Batch Loss: 0.18396538496017456\n",
      "Epoch 2518, Loss: 0.45411309599876404, Final Batch Loss: 0.1293240785598755\n",
      "Epoch 2519, Loss: 0.4306025356054306, Final Batch Loss: 0.1478271335363388\n",
      "Epoch 2520, Loss: 0.5137400329113007, Final Batch Loss: 0.16703803837299347\n",
      "Epoch 2521, Loss: 0.5547532513737679, Final Batch Loss: 0.18794409930706024\n",
      "Epoch 2522, Loss: 0.4403153210878372, Final Batch Loss: 0.16398361325263977\n",
      "Epoch 2523, Loss: 0.5123602673411369, Final Batch Loss: 0.20048043131828308\n",
      "Epoch 2524, Loss: 0.4235372692346573, Final Batch Loss: 0.17338114976882935\n",
      "Epoch 2525, Loss: 0.42381881922483444, Final Batch Loss: 0.10669585317373276\n",
      "Epoch 2526, Loss: 0.5618407428264618, Final Batch Loss: 0.19998329877853394\n",
      "Epoch 2527, Loss: 0.5114534646272659, Final Batch Loss: 0.1539137214422226\n",
      "Epoch 2528, Loss: 0.6319474279880524, Final Batch Loss: 0.1383453607559204\n",
      "Epoch 2529, Loss: 0.5501112043857574, Final Batch Loss: 0.157077819108963\n",
      "Epoch 2530, Loss: 0.6390953809022903, Final Batch Loss: 0.27935758233070374\n",
      "Epoch 2531, Loss: 0.47787490487098694, Final Batch Loss: 0.1534181535243988\n",
      "Epoch 2532, Loss: 0.4546676576137543, Final Batch Loss: 0.12080270051956177\n",
      "Epoch 2533, Loss: 0.5961999148130417, Final Batch Loss: 0.21493640542030334\n",
      "Epoch 2534, Loss: 0.5936439633369446, Final Batch Loss: 0.23979446291923523\n",
      "Epoch 2535, Loss: 0.4979613870382309, Final Batch Loss: 0.18841254711151123\n",
      "Epoch 2536, Loss: 0.5416734963655472, Final Batch Loss: 0.16657878458499908\n",
      "Epoch 2537, Loss: 0.5954259037971497, Final Batch Loss: 0.22619125247001648\n",
      "Epoch 2538, Loss: 0.46136002242565155, Final Batch Loss: 0.14121894538402557\n",
      "Epoch 2539, Loss: 0.5995558500289917, Final Batch Loss: 0.16744670271873474\n",
      "Epoch 2540, Loss: 0.47257380187511444, Final Batch Loss: 0.147857204079628\n",
      "Epoch 2541, Loss: 0.5395699441432953, Final Batch Loss: 0.17405365407466888\n",
      "Epoch 2542, Loss: 0.4106731489300728, Final Batch Loss: 0.13186517357826233\n",
      "Epoch 2543, Loss: 0.4712202697992325, Final Batch Loss: 0.13486433029174805\n",
      "Epoch 2544, Loss: 0.42955728620290756, Final Batch Loss: 0.11281838268041611\n",
      "Epoch 2545, Loss: 0.5537223666906357, Final Batch Loss: 0.1894540935754776\n",
      "Epoch 2546, Loss: 0.4598424509167671, Final Batch Loss: 0.12223520129919052\n",
      "Epoch 2547, Loss: 0.5088780522346497, Final Batch Loss: 0.21497070789337158\n",
      "Epoch 2548, Loss: 0.5506498515605927, Final Batch Loss: 0.1824527084827423\n",
      "Epoch 2549, Loss: 0.4907945543527603, Final Batch Loss: 0.12177357077598572\n",
      "Epoch 2550, Loss: 0.5923388004302979, Final Batch Loss: 0.20394808053970337\n",
      "Epoch 2551, Loss: 0.5139683932065964, Final Batch Loss: 0.22561776638031006\n",
      "Epoch 2552, Loss: 0.4352511763572693, Final Batch Loss: 0.14846394956111908\n",
      "Epoch 2553, Loss: 0.4910576492547989, Final Batch Loss: 0.1887957602739334\n",
      "Epoch 2554, Loss: 0.4820152223110199, Final Batch Loss: 0.1424063891172409\n",
      "Epoch 2555, Loss: 0.4888654425740242, Final Batch Loss: 0.19240765273571014\n",
      "Epoch 2556, Loss: 0.4624692052602768, Final Batch Loss: 0.17071080207824707\n",
      "Epoch 2557, Loss: 0.4373503625392914, Final Batch Loss: 0.1271347850561142\n",
      "Epoch 2558, Loss: 0.4515674263238907, Final Batch Loss: 0.13995876908302307\n",
      "Epoch 2559, Loss: 0.43962041288614273, Final Batch Loss: 0.13876889646053314\n",
      "Epoch 2560, Loss: 0.5445905923843384, Final Batch Loss: 0.24622009694576263\n",
      "Epoch 2561, Loss: 0.47306397557258606, Final Batch Loss: 0.1641693413257599\n",
      "Epoch 2562, Loss: 0.4935189336538315, Final Batch Loss: 0.16215738654136658\n",
      "Epoch 2563, Loss: 0.48114699870347977, Final Batch Loss: 0.20317323505878448\n",
      "Epoch 2564, Loss: 0.514671266078949, Final Batch Loss: 0.1624344438314438\n",
      "Epoch 2565, Loss: 0.48177962750196457, Final Batch Loss: 0.12256626039743423\n",
      "Epoch 2566, Loss: 0.47810812294483185, Final Batch Loss: 0.1658070981502533\n",
      "Epoch 2567, Loss: 0.5677646845579147, Final Batch Loss: 0.1451500505208969\n",
      "Epoch 2568, Loss: 0.43343204259872437, Final Batch Loss: 0.13456963002681732\n",
      "Epoch 2569, Loss: 0.5957315564155579, Final Batch Loss: 0.25454139709472656\n",
      "Epoch 2570, Loss: 0.46467718482017517, Final Batch Loss: 0.11634209752082825\n",
      "Epoch 2571, Loss: 0.503303050994873, Final Batch Loss: 0.17928752303123474\n",
      "Epoch 2572, Loss: 0.4388831928372383, Final Batch Loss: 0.17467278242111206\n",
      "Epoch 2573, Loss: 0.48248979449272156, Final Batch Loss: 0.17802006006240845\n",
      "Epoch 2574, Loss: 0.5588763505220413, Final Batch Loss: 0.1665937900543213\n",
      "Epoch 2575, Loss: 0.4825294464826584, Final Batch Loss: 0.15558750927448273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2576, Loss: 0.4539248198270798, Final Batch Loss: 0.1328275203704834\n",
      "Epoch 2577, Loss: 0.5514470338821411, Final Batch Loss: 0.2168704718351364\n",
      "Epoch 2578, Loss: 0.4277901127934456, Final Batch Loss: 0.18211492896080017\n",
      "Epoch 2579, Loss: 0.48780959844589233, Final Batch Loss: 0.19909007847309113\n",
      "Epoch 2580, Loss: 0.5731427073478699, Final Batch Loss: 0.13649481534957886\n",
      "Epoch 2581, Loss: 0.5447676777839661, Final Batch Loss: 0.1357468068599701\n",
      "Epoch 2582, Loss: 0.49718959629535675, Final Batch Loss: 0.18745899200439453\n",
      "Epoch 2583, Loss: 0.5253370553255081, Final Batch Loss: 0.14030174911022186\n",
      "Epoch 2584, Loss: 0.46408383548259735, Final Batch Loss: 0.1644953042268753\n",
      "Epoch 2585, Loss: 0.5385797470808029, Final Batch Loss: 0.15169192850589752\n",
      "Epoch 2586, Loss: 0.4226883351802826, Final Batch Loss: 0.15676458179950714\n",
      "Epoch 2587, Loss: 0.5114548578858376, Final Batch Loss: 0.09794294089078903\n",
      "Epoch 2588, Loss: 0.45242931693792343, Final Batch Loss: 0.12032952159643173\n",
      "Epoch 2589, Loss: 0.5496821478009224, Final Batch Loss: 0.12313181906938553\n",
      "Epoch 2590, Loss: 0.5726927667856216, Final Batch Loss: 0.1899276226758957\n",
      "Epoch 2591, Loss: 0.576357826590538, Final Batch Loss: 0.15076999366283417\n",
      "Epoch 2592, Loss: 0.49840886890888214, Final Batch Loss: 0.17610575258731842\n",
      "Epoch 2593, Loss: 0.4280049800872803, Final Batch Loss: 0.08706121146678925\n",
      "Epoch 2594, Loss: 0.5116505324840546, Final Batch Loss: 0.15538416802883148\n",
      "Epoch 2595, Loss: 0.505979061126709, Final Batch Loss: 0.15896520018577576\n",
      "Epoch 2596, Loss: 0.48998381197452545, Final Batch Loss: 0.1459973305463791\n",
      "Epoch 2597, Loss: 0.5559205561876297, Final Batch Loss: 0.20158348977565765\n",
      "Epoch 2598, Loss: 0.5766284167766571, Final Batch Loss: 0.1632208526134491\n",
      "Epoch 2599, Loss: 0.5653175860643387, Final Batch Loss: 0.17664629220962524\n",
      "Epoch 2600, Loss: 0.5049097985029221, Final Batch Loss: 0.16376955807209015\n",
      "Epoch 2601, Loss: 0.5498020350933075, Final Batch Loss: 0.15920516848564148\n",
      "Epoch 2602, Loss: 0.5312429219484329, Final Batch Loss: 0.22711996734142303\n",
      "Epoch 2603, Loss: 0.43065133690834045, Final Batch Loss: 0.1358194649219513\n",
      "Epoch 2604, Loss: 0.4526243507862091, Final Batch Loss: 0.1466040164232254\n",
      "Epoch 2605, Loss: 0.4091009125113487, Final Batch Loss: 0.13833637535572052\n",
      "Epoch 2606, Loss: 0.43595289438962936, Final Batch Loss: 0.2129545658826828\n",
      "Epoch 2607, Loss: 0.4669898450374603, Final Batch Loss: 0.21047531068325043\n",
      "Epoch 2608, Loss: 0.5568488091230392, Final Batch Loss: 0.2426561862230301\n",
      "Epoch 2609, Loss: 0.4353276789188385, Final Batch Loss: 0.19747334718704224\n",
      "Epoch 2610, Loss: 0.4910382777452469, Final Batch Loss: 0.17867903411388397\n",
      "Epoch 2611, Loss: 0.6817643344402313, Final Batch Loss: 0.21830515563488007\n",
      "Epoch 2612, Loss: 0.5193132162094116, Final Batch Loss: 0.13353459537029266\n",
      "Epoch 2613, Loss: 0.4918556809425354, Final Batch Loss: 0.16366274654865265\n",
      "Epoch 2614, Loss: 0.5107534527778625, Final Batch Loss: 0.19665901362895966\n",
      "Epoch 2615, Loss: 0.47459371387958527, Final Batch Loss: 0.1959598809480667\n",
      "Epoch 2616, Loss: 0.5652954131364822, Final Batch Loss: 0.19444376230239868\n",
      "Epoch 2617, Loss: 0.522498294711113, Final Batch Loss: 0.21503673493862152\n",
      "Epoch 2618, Loss: 0.45257145911455154, Final Batch Loss: 0.11407136172056198\n",
      "Epoch 2619, Loss: 0.4883591756224632, Final Batch Loss: 0.18360374867916107\n",
      "Epoch 2620, Loss: 0.5662479400634766, Final Batch Loss: 0.17528030276298523\n",
      "Epoch 2621, Loss: 0.5626749694347382, Final Batch Loss: 0.13177819550037384\n",
      "Epoch 2622, Loss: 0.45209720730781555, Final Batch Loss: 0.13455568253993988\n",
      "Epoch 2623, Loss: 0.5522553622722626, Final Batch Loss: 0.21539978682994843\n",
      "Epoch 2624, Loss: 0.49857020378112793, Final Batch Loss: 0.14427052438259125\n",
      "Epoch 2625, Loss: 0.5567654669284821, Final Batch Loss: 0.2054504007101059\n",
      "Epoch 2626, Loss: 0.5366077870130539, Final Batch Loss: 0.22704462707042694\n",
      "Epoch 2627, Loss: 0.5126230418682098, Final Batch Loss: 0.15809886157512665\n",
      "Epoch 2628, Loss: 0.5019761919975281, Final Batch Loss: 0.2075614035129547\n",
      "Epoch 2629, Loss: 0.4692560285329819, Final Batch Loss: 0.1666967123746872\n",
      "Epoch 2630, Loss: 0.5165806114673615, Final Batch Loss: 0.1197492927312851\n",
      "Epoch 2631, Loss: 0.38754135370254517, Final Batch Loss: 0.09119394421577454\n",
      "Epoch 2632, Loss: 0.5443674176931381, Final Batch Loss: 0.22972628474235535\n",
      "Epoch 2633, Loss: 0.4271240159869194, Final Batch Loss: 0.17546817660331726\n",
      "Epoch 2634, Loss: 0.5232822149991989, Final Batch Loss: 0.18223445117473602\n",
      "Epoch 2635, Loss: 0.6400673091411591, Final Batch Loss: 0.1921343207359314\n",
      "Epoch 2636, Loss: 0.4802528917789459, Final Batch Loss: 0.1999344378709793\n",
      "Epoch 2637, Loss: 0.5067635700106621, Final Batch Loss: 0.11529501527547836\n",
      "Epoch 2638, Loss: 0.4929465353488922, Final Batch Loss: 0.18004098534584045\n",
      "Epoch 2639, Loss: 0.47759659588336945, Final Batch Loss: 0.15762674808502197\n",
      "Epoch 2640, Loss: 0.5936733484268188, Final Batch Loss: 0.18985311686992645\n",
      "Epoch 2641, Loss: 0.4935673326253891, Final Batch Loss: 0.14549551904201508\n",
      "Epoch 2642, Loss: 0.6176495403051376, Final Batch Loss: 0.18964862823486328\n",
      "Epoch 2643, Loss: 0.4921772629022598, Final Batch Loss: 0.15286529064178467\n",
      "Epoch 2644, Loss: 0.444745808839798, Final Batch Loss: 0.10690785944461823\n",
      "Epoch 2645, Loss: 0.476076602935791, Final Batch Loss: 0.1819329708814621\n",
      "Epoch 2646, Loss: 0.4192853718996048, Final Batch Loss: 0.1395115703344345\n",
      "Epoch 2647, Loss: 0.5416392982006073, Final Batch Loss: 0.2032242715358734\n",
      "Epoch 2648, Loss: 0.5426203534007072, Final Batch Loss: 0.25582048296928406\n",
      "Epoch 2649, Loss: 0.4957713633775711, Final Batch Loss: 0.15676915645599365\n",
      "Epoch 2650, Loss: 0.473242424428463, Final Batch Loss: 0.12296874076128006\n",
      "Epoch 2651, Loss: 0.5635028779506683, Final Batch Loss: 0.22855578362941742\n",
      "Epoch 2652, Loss: 0.464500367641449, Final Batch Loss: 0.12941160798072815\n",
      "Epoch 2653, Loss: 0.40408916026353836, Final Batch Loss: 0.13511443138122559\n",
      "Epoch 2654, Loss: 0.4538693279027939, Final Batch Loss: 0.19540570676326752\n",
      "Epoch 2655, Loss: 0.565537616610527, Final Batch Loss: 0.14672167599201202\n",
      "Epoch 2656, Loss: 0.44098546355962753, Final Batch Loss: 0.12295322865247726\n",
      "Epoch 2657, Loss: 0.42375408858060837, Final Batch Loss: 0.10519564896821976\n",
      "Epoch 2658, Loss: 0.5530133694410324, Final Batch Loss: 0.22263115644454956\n",
      "Epoch 2659, Loss: 0.520738497376442, Final Batch Loss: 0.1652640551328659\n",
      "Epoch 2660, Loss: 0.5091025531291962, Final Batch Loss: 0.1674533486366272\n",
      "Epoch 2661, Loss: 0.5269613415002823, Final Batch Loss: 0.20509204268455505\n",
      "Epoch 2662, Loss: 0.5814897269010544, Final Batch Loss: 0.1929750144481659\n",
      "Epoch 2663, Loss: 0.5574915558099747, Final Batch Loss: 0.16916204988956451\n",
      "Epoch 2664, Loss: 0.48498673737049103, Final Batch Loss: 0.19954602420330048\n",
      "Epoch 2665, Loss: 0.4992000535130501, Final Batch Loss: 0.18382418155670166\n",
      "Epoch 2666, Loss: 0.4730193465948105, Final Batch Loss: 0.16402962803840637\n",
      "Epoch 2667, Loss: 0.4406721442937851, Final Batch Loss: 0.16400748491287231\n",
      "Epoch 2668, Loss: 0.5483351498842239, Final Batch Loss: 0.17990931868553162\n",
      "Epoch 2669, Loss: 0.5501024276018143, Final Batch Loss: 0.2205362170934677\n",
      "Epoch 2670, Loss: 0.46831947565078735, Final Batch Loss: 0.13942468166351318\n",
      "Epoch 2671, Loss: 0.4842712730169296, Final Batch Loss: 0.13418354094028473\n",
      "Epoch 2672, Loss: 0.49964646995067596, Final Batch Loss: 0.1523558795452118\n",
      "Epoch 2673, Loss: 0.44362571835517883, Final Batch Loss: 0.16249088943004608\n",
      "Epoch 2674, Loss: 0.4283553212881088, Final Batch Loss: 0.13886725902557373\n",
      "Epoch 2675, Loss: 0.49813112616539, Final Batch Loss: 0.15561357140541077\n",
      "Epoch 2676, Loss: 0.4437158405780792, Final Batch Loss: 0.17619094252586365\n",
      "Epoch 2677, Loss: 0.47860657423734665, Final Batch Loss: 0.13234040141105652\n",
      "Epoch 2678, Loss: 0.5435347259044647, Final Batch Loss: 0.20162177085876465\n",
      "Epoch 2679, Loss: 0.5964566469192505, Final Batch Loss: 0.18004074692726135\n",
      "Epoch 2680, Loss: 0.47521546483039856, Final Batch Loss: 0.17718029022216797\n",
      "Epoch 2681, Loss: 0.5071005746722221, Final Batch Loss: 0.12029940634965897\n",
      "Epoch 2682, Loss: 0.4378347396850586, Final Batch Loss: 0.13346703350543976\n",
      "Epoch 2683, Loss: 0.5501547157764435, Final Batch Loss: 0.21654917299747467\n",
      "Epoch 2684, Loss: 0.49076642096042633, Final Batch Loss: 0.1789308488368988\n",
      "Epoch 2685, Loss: 0.3709346204996109, Final Batch Loss: 0.10564640164375305\n",
      "Epoch 2686, Loss: 0.45779983699321747, Final Batch Loss: 0.1546083390712738\n",
      "Epoch 2687, Loss: 0.48756270110607147, Final Batch Loss: 0.1278391182422638\n",
      "Epoch 2688, Loss: 0.450065940618515, Final Batch Loss: 0.19234001636505127\n",
      "Epoch 2689, Loss: 0.3875841945409775, Final Batch Loss: 0.0970974862575531\n",
      "Epoch 2690, Loss: 0.6000556200742722, Final Batch Loss: 0.2278696596622467\n",
      "Epoch 2691, Loss: 0.5083109885454178, Final Batch Loss: 0.1785663217306137\n",
      "Epoch 2692, Loss: 0.573635458946228, Final Batch Loss: 0.13143548369407654\n",
      "Epoch 2693, Loss: 0.5378539562225342, Final Batch Loss: 0.24839381873607635\n",
      "Epoch 2694, Loss: 0.4817463904619217, Final Batch Loss: 0.12101539969444275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2695, Loss: 0.449382945895195, Final Batch Loss: 0.14235667884349823\n",
      "Epoch 2696, Loss: 0.4369461461901665, Final Batch Loss: 0.11993477493524551\n",
      "Epoch 2697, Loss: 0.4928344190120697, Final Batch Loss: 0.1111629456281662\n",
      "Epoch 2698, Loss: 0.47561757266521454, Final Batch Loss: 0.16760557889938354\n",
      "Epoch 2699, Loss: 0.4735260158777237, Final Batch Loss: 0.13191427290439606\n",
      "Epoch 2700, Loss: 0.5528393685817719, Final Batch Loss: 0.1894388198852539\n",
      "Epoch 2701, Loss: 0.5803417265415192, Final Batch Loss: 0.17360346019268036\n",
      "Epoch 2702, Loss: 0.540174663066864, Final Batch Loss: 0.2216530293226242\n",
      "Epoch 2703, Loss: 0.47398680448532104, Final Batch Loss: 0.18161050975322723\n",
      "Epoch 2704, Loss: 0.6034278124570847, Final Batch Loss: 0.10032020509243011\n",
      "Epoch 2705, Loss: 0.6412006914615631, Final Batch Loss: 0.32544058561325073\n",
      "Epoch 2706, Loss: 0.4622591435909271, Final Batch Loss: 0.15790493786334991\n",
      "Epoch 2707, Loss: 0.4537225067615509, Final Batch Loss: 0.15243174135684967\n",
      "Epoch 2708, Loss: 0.40169671922922134, Final Batch Loss: 0.11509064584970474\n",
      "Epoch 2709, Loss: 0.45746905356645584, Final Batch Loss: 0.08430441468954086\n",
      "Epoch 2710, Loss: 0.43808020651340485, Final Batch Loss: 0.13655546307563782\n",
      "Epoch 2711, Loss: 0.46691858023405075, Final Batch Loss: 0.10282105952501297\n",
      "Epoch 2712, Loss: 0.41938263922929764, Final Batch Loss: 0.1089346632361412\n",
      "Epoch 2713, Loss: 0.513944685459137, Final Batch Loss: 0.11522173881530762\n",
      "Epoch 2714, Loss: 0.570548303425312, Final Batch Loss: 0.12022293359041214\n",
      "Epoch 2715, Loss: 0.4952004551887512, Final Batch Loss: 0.15978898108005524\n",
      "Epoch 2716, Loss: 0.5181765407323837, Final Batch Loss: 0.15258623659610748\n",
      "Epoch 2717, Loss: 0.515463188290596, Final Batch Loss: 0.21577322483062744\n",
      "Epoch 2718, Loss: 0.491661474108696, Final Batch Loss: 0.12974917888641357\n",
      "Epoch 2719, Loss: 0.563924603164196, Final Batch Loss: 0.27048686146736145\n",
      "Epoch 2720, Loss: 0.4706023037433624, Final Batch Loss: 0.18598856031894684\n",
      "Epoch 2721, Loss: 0.5127323269844055, Final Batch Loss: 0.14547546207904816\n",
      "Epoch 2722, Loss: 0.47091925144195557, Final Batch Loss: 0.18415291607379913\n",
      "Epoch 2723, Loss: 0.4675934836268425, Final Batch Loss: 0.20090414583683014\n",
      "Epoch 2724, Loss: 0.43429771065711975, Final Batch Loss: 0.12986443936824799\n",
      "Epoch 2725, Loss: 0.4532686546444893, Final Batch Loss: 0.18226300179958344\n",
      "Epoch 2726, Loss: 0.3450476825237274, Final Batch Loss: 0.072209931910038\n",
      "Epoch 2727, Loss: 0.5160023272037506, Final Batch Loss: 0.18369106948375702\n",
      "Epoch 2728, Loss: 0.5093480721116066, Final Batch Loss: 0.1749049425125122\n",
      "Epoch 2729, Loss: 0.5191207826137543, Final Batch Loss: 0.23446102440357208\n",
      "Epoch 2730, Loss: 0.49624012410640717, Final Batch Loss: 0.19985981285572052\n",
      "Epoch 2731, Loss: 0.4582129493355751, Final Batch Loss: 0.10480468720197678\n",
      "Epoch 2732, Loss: 0.5600543767213821, Final Batch Loss: 0.14165756106376648\n",
      "Epoch 2733, Loss: 0.47433630377054214, Final Batch Loss: 0.09277205914258957\n",
      "Epoch 2734, Loss: 0.47717544436454773, Final Batch Loss: 0.16888606548309326\n",
      "Epoch 2735, Loss: 0.49843648821115494, Final Batch Loss: 0.21578170359134674\n",
      "Epoch 2736, Loss: 0.5209332555532455, Final Batch Loss: 0.21667586266994476\n",
      "Epoch 2737, Loss: 0.5562936216592789, Final Batch Loss: 0.20861764252185822\n",
      "Epoch 2738, Loss: 0.6027394831180573, Final Batch Loss: 0.304379403591156\n",
      "Epoch 2739, Loss: 0.39556460082530975, Final Batch Loss: 0.11216489970684052\n",
      "Epoch 2740, Loss: 0.38146331906318665, Final Batch Loss: 0.10528908669948578\n",
      "Epoch 2741, Loss: 0.5110302716493607, Final Batch Loss: 0.19190120697021484\n",
      "Epoch 2742, Loss: 0.5862819999456406, Final Batch Loss: 0.20937885344028473\n",
      "Epoch 2743, Loss: 0.4390115737915039, Final Batch Loss: 0.12601560354232788\n",
      "Epoch 2744, Loss: 0.5275651216506958, Final Batch Loss: 0.13667471706867218\n",
      "Epoch 2745, Loss: 0.5632044672966003, Final Batch Loss: 0.15580351650714874\n",
      "Epoch 2746, Loss: 0.5260637104511261, Final Batch Loss: 0.21111896634101868\n",
      "Epoch 2747, Loss: 0.6258516907691956, Final Batch Loss: 0.1708282232284546\n",
      "Epoch 2748, Loss: 0.4807380735874176, Final Batch Loss: 0.19417235255241394\n",
      "Epoch 2749, Loss: 0.47244951128959656, Final Batch Loss: 0.21131035685539246\n",
      "Epoch 2750, Loss: 0.46889324486255646, Final Batch Loss: 0.17952971160411835\n",
      "Epoch 2751, Loss: 0.47736408561468124, Final Batch Loss: 0.17988687753677368\n",
      "Epoch 2752, Loss: 0.41265761107206345, Final Batch Loss: 0.15320813655853271\n",
      "Epoch 2753, Loss: 0.4296697527170181, Final Batch Loss: 0.13754737377166748\n",
      "Epoch 2754, Loss: 0.6029714494943619, Final Batch Loss: 0.21165798604488373\n",
      "Epoch 2755, Loss: 0.508684441447258, Final Batch Loss: 0.23564450442790985\n",
      "Epoch 2756, Loss: 0.5642994195222855, Final Batch Loss: 0.24113354086875916\n",
      "Epoch 2757, Loss: 0.4880923703312874, Final Batch Loss: 0.2187502086162567\n",
      "Epoch 2758, Loss: 0.4472416192293167, Final Batch Loss: 0.1360473483800888\n",
      "Epoch 2759, Loss: 0.41385507583618164, Final Batch Loss: 0.10618330538272858\n",
      "Epoch 2760, Loss: 0.49039193987846375, Final Batch Loss: 0.12551984190940857\n",
      "Epoch 2761, Loss: 0.5646391957998276, Final Batch Loss: 0.24739620089530945\n",
      "Epoch 2762, Loss: 0.43507523834705353, Final Batch Loss: 0.16380906105041504\n",
      "Epoch 2763, Loss: 0.5013451725244522, Final Batch Loss: 0.23480363190174103\n",
      "Epoch 2764, Loss: 0.5258867591619492, Final Batch Loss: 0.20965027809143066\n",
      "Epoch 2765, Loss: 0.4692820832133293, Final Batch Loss: 0.11943132430315018\n",
      "Epoch 2766, Loss: 0.6061408817768097, Final Batch Loss: 0.16466806828975677\n",
      "Epoch 2767, Loss: 0.5034456849098206, Final Batch Loss: 0.13486292958259583\n",
      "Epoch 2768, Loss: 0.47297465801239014, Final Batch Loss: 0.1556486189365387\n",
      "Epoch 2769, Loss: 0.5604317337274551, Final Batch Loss: 0.1837783008813858\n",
      "Epoch 2770, Loss: 0.5515990257263184, Final Batch Loss: 0.15171535313129425\n",
      "Epoch 2771, Loss: 0.4270113930106163, Final Batch Loss: 0.16791735589504242\n",
      "Epoch 2772, Loss: 0.43701504170894623, Final Batch Loss: 0.12894374132156372\n",
      "Epoch 2773, Loss: 0.5269842594861984, Final Batch Loss: 0.19284501671791077\n",
      "Epoch 2774, Loss: 0.5276584774255753, Final Batch Loss: 0.23812197148799896\n",
      "Epoch 2775, Loss: 0.4976915866136551, Final Batch Loss: 0.16234450042247772\n",
      "Epoch 2776, Loss: 0.4939528927206993, Final Batch Loss: 0.1171502098441124\n",
      "Epoch 2777, Loss: 0.4818052425980568, Final Batch Loss: 0.1769576072692871\n",
      "Epoch 2778, Loss: 0.5285305976867676, Final Batch Loss: 0.13816918432712555\n",
      "Epoch 2779, Loss: 0.47668538987636566, Final Batch Loss: 0.1911383718252182\n",
      "Epoch 2780, Loss: 0.44990406930446625, Final Batch Loss: 0.15943902730941772\n",
      "Epoch 2781, Loss: 0.505066379904747, Final Batch Loss: 0.18735040724277496\n",
      "Epoch 2782, Loss: 0.4775742292404175, Final Batch Loss: 0.2428158074617386\n",
      "Epoch 2783, Loss: 0.4750993698835373, Final Batch Loss: 0.09133096039295197\n",
      "Epoch 2784, Loss: 0.544895127415657, Final Batch Loss: 0.1457872986793518\n",
      "Epoch 2785, Loss: 0.49813106656074524, Final Batch Loss: 0.12420783936977386\n",
      "Epoch 2786, Loss: 0.5258653461933136, Final Batch Loss: 0.20591941475868225\n",
      "Epoch 2787, Loss: 0.43063807487487793, Final Batch Loss: 0.15877138078212738\n",
      "Epoch 2788, Loss: 0.5426129549741745, Final Batch Loss: 0.21935132145881653\n",
      "Epoch 2789, Loss: 0.42511770129203796, Final Batch Loss: 0.1291777342557907\n",
      "Epoch 2790, Loss: 0.4342852532863617, Final Batch Loss: 0.17764557898044586\n",
      "Epoch 2791, Loss: 0.46430253982543945, Final Batch Loss: 0.15725940465927124\n",
      "Epoch 2792, Loss: 0.470855176448822, Final Batch Loss: 0.15385259687900543\n",
      "Epoch 2793, Loss: 0.5240865498781204, Final Batch Loss: 0.16437527537345886\n",
      "Epoch 2794, Loss: 0.4609565883874893, Final Batch Loss: 0.14595016837120056\n",
      "Epoch 2795, Loss: 0.4439680203795433, Final Batch Loss: 0.1238960549235344\n",
      "Epoch 2796, Loss: 0.46356286108493805, Final Batch Loss: 0.12996140122413635\n",
      "Epoch 2797, Loss: 0.4942883402109146, Final Batch Loss: 0.12578606605529785\n",
      "Epoch 2798, Loss: 0.4640096426010132, Final Batch Loss: 0.1678057163953781\n",
      "Epoch 2799, Loss: 0.534197136759758, Final Batch Loss: 0.23534242808818817\n",
      "Epoch 2800, Loss: 0.5916131436824799, Final Batch Loss: 0.1816171109676361\n",
      "Epoch 2801, Loss: 0.44627175480127335, Final Batch Loss: 0.1114390566945076\n",
      "Epoch 2802, Loss: 0.4935320019721985, Final Batch Loss: 0.1259305477142334\n",
      "Epoch 2803, Loss: 0.4061014950275421, Final Batch Loss: 0.12674984335899353\n",
      "Epoch 2804, Loss: 0.5700209885835648, Final Batch Loss: 0.19105766713619232\n",
      "Epoch 2805, Loss: 0.4424029141664505, Final Batch Loss: 0.10333152860403061\n",
      "Epoch 2806, Loss: 0.42036594450473785, Final Batch Loss: 0.10932953655719757\n",
      "Epoch 2807, Loss: 0.5306398272514343, Final Batch Loss: 0.1171044409275055\n",
      "Epoch 2808, Loss: 0.4900270700454712, Final Batch Loss: 0.18905679881572723\n",
      "Epoch 2809, Loss: 0.5811474323272705, Final Batch Loss: 0.2452162504196167\n",
      "Epoch 2810, Loss: 0.4453115612268448, Final Batch Loss: 0.13705995678901672\n",
      "Epoch 2811, Loss: 0.5467593222856522, Final Batch Loss: 0.22332341969013214\n",
      "Epoch 2812, Loss: 0.43466153740882874, Final Batch Loss: 0.12766627967357635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2813, Loss: 0.537755936384201, Final Batch Loss: 0.14873279631137848\n",
      "Epoch 2814, Loss: 0.4863921254873276, Final Batch Loss: 0.1307060569524765\n",
      "Epoch 2815, Loss: 0.5067623555660248, Final Batch Loss: 0.19186697900295258\n",
      "Epoch 2816, Loss: 0.5875217467546463, Final Batch Loss: 0.2600996792316437\n",
      "Epoch 2817, Loss: 0.5934648662805557, Final Batch Loss: 0.20473983883857727\n",
      "Epoch 2818, Loss: 0.46604661643505096, Final Batch Loss: 0.17800273001194\n",
      "Epoch 2819, Loss: 0.4996180534362793, Final Batch Loss: 0.1743592619895935\n",
      "Epoch 2820, Loss: 0.40074795484542847, Final Batch Loss: 0.13894884288311005\n",
      "Epoch 2821, Loss: 0.4903070032596588, Final Batch Loss: 0.14476364850997925\n",
      "Epoch 2822, Loss: 0.4871499240398407, Final Batch Loss: 0.16886620223522186\n",
      "Epoch 2823, Loss: 0.5738049000501633, Final Batch Loss: 0.19519095122814178\n",
      "Epoch 2824, Loss: 0.5078492611646652, Final Batch Loss: 0.21315009891986847\n",
      "Epoch 2825, Loss: 0.4995634853839874, Final Batch Loss: 0.13154082000255585\n",
      "Epoch 2826, Loss: 0.4498777538537979, Final Batch Loss: 0.12556922435760498\n",
      "Epoch 2827, Loss: 0.4452426806092262, Final Batch Loss: 0.1985747516155243\n",
      "Epoch 2828, Loss: 0.5226880311965942, Final Batch Loss: 0.1822918951511383\n",
      "Epoch 2829, Loss: 0.4636906310915947, Final Batch Loss: 0.17472149431705475\n",
      "Epoch 2830, Loss: 0.47625935077667236, Final Batch Loss: 0.14155364036560059\n",
      "Epoch 2831, Loss: 0.4642868787050247, Final Batch Loss: 0.15989720821380615\n",
      "Epoch 2832, Loss: 0.4480973482131958, Final Batch Loss: 0.1507437527179718\n",
      "Epoch 2833, Loss: 0.530097983777523, Final Batch Loss: 0.23229476809501648\n",
      "Epoch 2834, Loss: 0.49383214116096497, Final Batch Loss: 0.13117873668670654\n",
      "Epoch 2835, Loss: 0.4841749221086502, Final Batch Loss: 0.12757913768291473\n",
      "Epoch 2836, Loss: 0.5242727845907211, Final Batch Loss: 0.1579996794462204\n",
      "Epoch 2837, Loss: 0.5205948650836945, Final Batch Loss: 0.21125805377960205\n",
      "Epoch 2838, Loss: 0.47529079020023346, Final Batch Loss: 0.1206217110157013\n",
      "Epoch 2839, Loss: 0.5546792596578598, Final Batch Loss: 0.1525118052959442\n",
      "Epoch 2840, Loss: 0.5790189802646637, Final Batch Loss: 0.13986992835998535\n",
      "Epoch 2841, Loss: 0.39365189522504807, Final Batch Loss: 0.10952090471982956\n",
      "Epoch 2842, Loss: 0.4155538082122803, Final Batch Loss: 0.1283365786075592\n",
      "Epoch 2843, Loss: 0.6503905057907104, Final Batch Loss: 0.2553113102912903\n",
      "Epoch 2844, Loss: 0.4766276031732559, Final Batch Loss: 0.1794028878211975\n",
      "Epoch 2845, Loss: 0.4037627801299095, Final Batch Loss: 0.11232265084981918\n",
      "Epoch 2846, Loss: 0.534009724855423, Final Batch Loss: 0.15211518108844757\n",
      "Epoch 2847, Loss: 0.5052333995699883, Final Batch Loss: 0.1871703565120697\n",
      "Epoch 2848, Loss: 0.4307383969426155, Final Batch Loss: 0.1478208601474762\n",
      "Epoch 2849, Loss: 0.4508003741502762, Final Batch Loss: 0.15478242933750153\n",
      "Epoch 2850, Loss: 0.42743244767189026, Final Batch Loss: 0.14799174666404724\n",
      "Epoch 2851, Loss: 0.50443484634161, Final Batch Loss: 0.10353750735521317\n",
      "Epoch 2852, Loss: 0.45196958631277084, Final Batch Loss: 0.1084863618016243\n",
      "Epoch 2853, Loss: 0.49509139358997345, Final Batch Loss: 0.15375320613384247\n",
      "Epoch 2854, Loss: 0.47144994139671326, Final Batch Loss: 0.1603364199399948\n",
      "Epoch 2855, Loss: 0.4729045182466507, Final Batch Loss: 0.22585465013980865\n",
      "Epoch 2856, Loss: 0.43377692997455597, Final Batch Loss: 0.19394689798355103\n",
      "Epoch 2857, Loss: 0.4765986353158951, Final Batch Loss: 0.2282290756702423\n",
      "Epoch 2858, Loss: 0.507214218378067, Final Batch Loss: 0.15654441714286804\n",
      "Epoch 2859, Loss: 0.4496396407485008, Final Batch Loss: 0.11721061915159225\n",
      "Epoch 2860, Loss: 0.515926256775856, Final Batch Loss: 0.22770801186561584\n",
      "Epoch 2861, Loss: 0.5384720265865326, Final Batch Loss: 0.1968459188938141\n",
      "Epoch 2862, Loss: 0.462436243891716, Final Batch Loss: 0.14034409821033478\n",
      "Epoch 2863, Loss: 0.4237280711531639, Final Batch Loss: 0.14750975370407104\n",
      "Epoch 2864, Loss: 0.30612725019454956, Final Batch Loss: 0.11229062080383301\n",
      "Epoch 2865, Loss: 0.43397408723831177, Final Batch Loss: 0.1493169218301773\n",
      "Epoch 2866, Loss: 0.4167858734726906, Final Batch Loss: 0.0952274426817894\n",
      "Epoch 2867, Loss: 0.5966658443212509, Final Batch Loss: 0.23808105289936066\n",
      "Epoch 2868, Loss: 0.4316767752170563, Final Batch Loss: 0.14498919248580933\n",
      "Epoch 2869, Loss: 0.5031483173370361, Final Batch Loss: 0.22401763498783112\n",
      "Epoch 2870, Loss: 0.46533165872097015, Final Batch Loss: 0.09665513038635254\n",
      "Epoch 2871, Loss: 0.45725657045841217, Final Batch Loss: 0.13736550509929657\n",
      "Epoch 2872, Loss: 0.4464803636074066, Final Batch Loss: 0.1863567978143692\n",
      "Epoch 2873, Loss: 0.4791005700826645, Final Batch Loss: 0.16894862055778503\n",
      "Epoch 2874, Loss: 0.49415670335292816, Final Batch Loss: 0.1474183350801468\n",
      "Epoch 2875, Loss: 0.5044433027505875, Final Batch Loss: 0.1668539047241211\n",
      "Epoch 2876, Loss: 0.48178280889987946, Final Batch Loss: 0.21001064777374268\n",
      "Epoch 2877, Loss: 0.46836714446544647, Final Batch Loss: 0.19121107459068298\n",
      "Epoch 2878, Loss: 0.42621684074401855, Final Batch Loss: 0.1445605307817459\n",
      "Epoch 2879, Loss: 0.3931261673569679, Final Batch Loss: 0.08086325973272324\n",
      "Epoch 2880, Loss: 0.46593179553747177, Final Batch Loss: 0.12369685620069504\n",
      "Epoch 2881, Loss: 0.448720321059227, Final Batch Loss: 0.17895521223545074\n",
      "Epoch 2882, Loss: 0.46714264899492264, Final Batch Loss: 0.12242624908685684\n",
      "Epoch 2883, Loss: 0.45418572425842285, Final Batch Loss: 0.13914650678634644\n",
      "Epoch 2884, Loss: 0.46402858942747116, Final Batch Loss: 0.18313069641590118\n",
      "Epoch 2885, Loss: 0.47549328207969666, Final Batch Loss: 0.14638787508010864\n",
      "Epoch 2886, Loss: 0.6284858882427216, Final Batch Loss: 0.14781981706619263\n",
      "Epoch 2887, Loss: 0.5683738738298416, Final Batch Loss: 0.21182526648044586\n",
      "Epoch 2888, Loss: 0.5276354849338531, Final Batch Loss: 0.21577371656894684\n",
      "Epoch 2889, Loss: 0.5379771590232849, Final Batch Loss: 0.24725037813186646\n",
      "Epoch 2890, Loss: 0.5888552516698837, Final Batch Loss: 0.1593882292509079\n",
      "Epoch 2891, Loss: 0.4696841984987259, Final Batch Loss: 0.12339058518409729\n",
      "Epoch 2892, Loss: 0.454444944858551, Final Batch Loss: 0.11628787219524384\n",
      "Epoch 2893, Loss: 0.5959692299365997, Final Batch Loss: 0.24924375116825104\n",
      "Epoch 2894, Loss: 0.3933483734726906, Final Batch Loss: 0.10675159096717834\n",
      "Epoch 2895, Loss: 0.481101855635643, Final Batch Loss: 0.14772437512874603\n",
      "Epoch 2896, Loss: 0.4401494115591049, Final Batch Loss: 0.1479150950908661\n",
      "Epoch 2897, Loss: 0.458395853638649, Final Batch Loss: 0.12411829829216003\n",
      "Epoch 2898, Loss: 0.44752146303653717, Final Batch Loss: 0.1334179788827896\n",
      "Epoch 2899, Loss: 0.481885626912117, Final Batch Loss: 0.17459213733673096\n",
      "Epoch 2900, Loss: 0.5482767075300217, Final Batch Loss: 0.24945376813411713\n",
      "Epoch 2901, Loss: 0.4505641832947731, Final Batch Loss: 0.16971543431282043\n",
      "Epoch 2902, Loss: 0.4994639754295349, Final Batch Loss: 0.1695626676082611\n",
      "Epoch 2903, Loss: 0.5643719807267189, Final Batch Loss: 0.22371312975883484\n",
      "Epoch 2904, Loss: 0.5046548321843147, Final Batch Loss: 0.13749642670154572\n",
      "Epoch 2905, Loss: 0.4894651547074318, Final Batch Loss: 0.20954293012619019\n",
      "Epoch 2906, Loss: 0.43651191145181656, Final Batch Loss: 0.1596686989068985\n",
      "Epoch 2907, Loss: 0.508849173784256, Final Batch Loss: 0.24191848933696747\n",
      "Epoch 2908, Loss: 0.41559652984142303, Final Batch Loss: 0.13961230218410492\n",
      "Epoch 2909, Loss: 0.40241842716932297, Final Batch Loss: 0.15964053571224213\n",
      "Epoch 2910, Loss: 0.43415187299251556, Final Batch Loss: 0.17247274518013\n",
      "Epoch 2911, Loss: 0.6711579859256744, Final Batch Loss: 0.32799479365348816\n",
      "Epoch 2912, Loss: 0.4692008048295975, Final Batch Loss: 0.13877494633197784\n",
      "Epoch 2913, Loss: 0.4943239986896515, Final Batch Loss: 0.1563991755247116\n",
      "Epoch 2914, Loss: 0.4648839682340622, Final Batch Loss: 0.1693115532398224\n",
      "Epoch 2915, Loss: 0.5296760052442551, Final Batch Loss: 0.20901678502559662\n",
      "Epoch 2916, Loss: 0.4557892084121704, Final Batch Loss: 0.1464913785457611\n",
      "Epoch 2917, Loss: 0.4380848780274391, Final Batch Loss: 0.11239734292030334\n",
      "Epoch 2918, Loss: 0.355670303106308, Final Batch Loss: 0.08801799267530441\n",
      "Epoch 2919, Loss: 0.45074623078107834, Final Batch Loss: 0.12379316240549088\n",
      "Epoch 2920, Loss: 0.39234356582164764, Final Batch Loss: 0.10175584256649017\n",
      "Epoch 2921, Loss: 0.5458164811134338, Final Batch Loss: 0.2217976450920105\n",
      "Epoch 2922, Loss: 0.5823704302310944, Final Batch Loss: 0.2347469925880432\n",
      "Epoch 2923, Loss: 0.5252038240432739, Final Batch Loss: 0.20018625259399414\n",
      "Epoch 2924, Loss: 0.500185564160347, Final Batch Loss: 0.20644202828407288\n",
      "Epoch 2925, Loss: 0.4307069182395935, Final Batch Loss: 0.06992140412330627\n",
      "Epoch 2926, Loss: 0.4339608997106552, Final Batch Loss: 0.14245913922786713\n",
      "Epoch 2927, Loss: 0.3810831606388092, Final Batch Loss: 0.11085520684719086\n",
      "Epoch 2928, Loss: 0.4157288074493408, Final Batch Loss: 0.1301708072423935\n",
      "Epoch 2929, Loss: 0.515126571059227, Final Batch Loss: 0.1858450323343277\n",
      "Epoch 2930, Loss: 0.5047866702079773, Final Batch Loss: 0.08035434782505035\n",
      "Epoch 2931, Loss: 0.4764389842748642, Final Batch Loss: 0.16917714476585388\n",
      "Epoch 2932, Loss: 0.34189222753047943, Final Batch Loss: 0.09631440043449402\n",
      "Epoch 2933, Loss: 0.5510251969099045, Final Batch Loss: 0.14821715652942657\n",
      "Epoch 2934, Loss: 0.4501713365316391, Final Batch Loss: 0.14043645560741425\n",
      "Epoch 2935, Loss: 0.4190637320280075, Final Batch Loss: 0.11779823899269104\n",
      "Epoch 2936, Loss: 0.4896969497203827, Final Batch Loss: 0.17076706886291504\n",
      "Epoch 2937, Loss: 0.48667222261428833, Final Batch Loss: 0.19911420345306396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2938, Loss: 0.46922650933265686, Final Batch Loss: 0.16652734577655792\n",
      "Epoch 2939, Loss: 0.42066704481840134, Final Batch Loss: 0.11446741968393326\n",
      "Epoch 2940, Loss: 0.4776948392391205, Final Batch Loss: 0.14889128506183624\n",
      "Epoch 2941, Loss: 0.512953594326973, Final Batch Loss: 0.18797680735588074\n",
      "Epoch 2942, Loss: 0.45136912167072296, Final Batch Loss: 0.14143957197666168\n",
      "Epoch 2943, Loss: 0.48008014261722565, Final Batch Loss: 0.09317563474178314\n",
      "Epoch 2944, Loss: 0.40555743128061295, Final Batch Loss: 0.10142461210489273\n",
      "Epoch 2945, Loss: 0.48875290155410767, Final Batch Loss: 0.163374662399292\n",
      "Epoch 2946, Loss: 0.651969313621521, Final Batch Loss: 0.24884213507175446\n",
      "Epoch 2947, Loss: 0.4270431324839592, Final Batch Loss: 0.11054445058107376\n",
      "Epoch 2948, Loss: 0.5893253535032272, Final Batch Loss: 0.18380723893642426\n",
      "Epoch 2949, Loss: 0.5590286105871201, Final Batch Loss: 0.19413895905017853\n",
      "Epoch 2950, Loss: 0.3950831815600395, Final Batch Loss: 0.0936216488480568\n",
      "Epoch 2951, Loss: 0.44687601923942566, Final Batch Loss: 0.17099769413471222\n",
      "Epoch 2952, Loss: 0.5326557755470276, Final Batch Loss: 0.1925947666168213\n",
      "Epoch 2953, Loss: 0.5340067744255066, Final Batch Loss: 0.20036762952804565\n",
      "Epoch 2954, Loss: 0.46308282017707825, Final Batch Loss: 0.1675461232662201\n",
      "Epoch 2955, Loss: 0.45953287929296494, Final Batch Loss: 0.10616151243448257\n",
      "Epoch 2956, Loss: 0.35530661791563034, Final Batch Loss: 0.0999002754688263\n",
      "Epoch 2957, Loss: 0.47472013533115387, Final Batch Loss: 0.15152089297771454\n",
      "Epoch 2958, Loss: 0.402127243578434, Final Batch Loss: 0.1294863224029541\n",
      "Epoch 2959, Loss: 0.5805713087320328, Final Batch Loss: 0.19285613298416138\n",
      "Epoch 2960, Loss: 0.4829021543264389, Final Batch Loss: 0.11999472975730896\n",
      "Epoch 2961, Loss: 0.4727618843317032, Final Batch Loss: 0.1527477502822876\n",
      "Epoch 2962, Loss: 0.4719974249601364, Final Batch Loss: 0.11016735434532166\n",
      "Epoch 2963, Loss: 0.4013770744204521, Final Batch Loss: 0.07596548646688461\n",
      "Epoch 2964, Loss: 0.45959821343421936, Final Batch Loss: 0.16029180586338043\n",
      "Epoch 2965, Loss: 0.4634833112359047, Final Batch Loss: 0.1121990904211998\n",
      "Epoch 2966, Loss: 0.4100515991449356, Final Batch Loss: 0.13933371007442474\n",
      "Epoch 2967, Loss: 0.5187528878450394, Final Batch Loss: 0.15341204404830933\n",
      "Epoch 2968, Loss: 0.46598854660987854, Final Batch Loss: 0.15987104177474976\n",
      "Epoch 2969, Loss: 0.5177649259567261, Final Batch Loss: 0.1643674373626709\n",
      "Epoch 2970, Loss: 0.4861960783600807, Final Batch Loss: 0.1023344174027443\n",
      "Epoch 2971, Loss: 0.44987696409225464, Final Batch Loss: 0.12646321952342987\n",
      "Epoch 2972, Loss: 0.4165923297405243, Final Batch Loss: 0.12746070325374603\n",
      "Epoch 2973, Loss: 0.5038165301084518, Final Batch Loss: 0.1453842669725418\n",
      "Epoch 2974, Loss: 0.37177015095949173, Final Batch Loss: 0.12098980695009232\n",
      "Epoch 2975, Loss: 0.5421899855136871, Final Batch Loss: 0.17129381000995636\n",
      "Epoch 2976, Loss: 0.45991164445877075, Final Batch Loss: 0.14933954179286957\n",
      "Epoch 2977, Loss: 0.5858867168426514, Final Batch Loss: 0.20176614820957184\n",
      "Epoch 2978, Loss: 0.5086247026920319, Final Batch Loss: 0.14929525554180145\n",
      "Epoch 2979, Loss: 0.4535078853368759, Final Batch Loss: 0.15116116404533386\n",
      "Epoch 2980, Loss: 0.5018999427556992, Final Batch Loss: 0.14250043034553528\n",
      "Epoch 2981, Loss: 0.5868037194013596, Final Batch Loss: 0.22057132422924042\n",
      "Epoch 2982, Loss: 0.47831030189991, Final Batch Loss: 0.11777938902378082\n",
      "Epoch 2983, Loss: 0.5118138939142227, Final Batch Loss: 0.18566285073757172\n",
      "Epoch 2984, Loss: 0.6225303113460541, Final Batch Loss: 0.07887992262840271\n",
      "Epoch 2985, Loss: 0.44799694418907166, Final Batch Loss: 0.143663227558136\n",
      "Epoch 2986, Loss: 0.42525913566350937, Final Batch Loss: 0.1628868281841278\n",
      "Epoch 2987, Loss: 0.49812711775302887, Final Batch Loss: 0.13791868090629578\n",
      "Epoch 2988, Loss: 0.5733019411563873, Final Batch Loss: 0.12217870354652405\n",
      "Epoch 2989, Loss: 0.5860597938299179, Final Batch Loss: 0.21002614498138428\n",
      "Epoch 2990, Loss: 0.5513865053653717, Final Batch Loss: 0.2158694863319397\n",
      "Epoch 2991, Loss: 0.4867340475320816, Final Batch Loss: 0.17835402488708496\n",
      "Epoch 2992, Loss: 0.4576824754476547, Final Batch Loss: 0.13531622290611267\n",
      "Epoch 2993, Loss: 0.5116518437862396, Final Batch Loss: 0.20235097408294678\n",
      "Epoch 2994, Loss: 0.5617026090621948, Final Batch Loss: 0.1897794008255005\n",
      "Epoch 2995, Loss: 0.5778681337833405, Final Batch Loss: 0.18654462695121765\n",
      "Epoch 2996, Loss: 0.44999605417251587, Final Batch Loss: 0.14802590012550354\n",
      "Epoch 2997, Loss: 0.5615848749876022, Final Batch Loss: 0.16278840601444244\n",
      "Epoch 2998, Loss: 0.6010345071554184, Final Batch Loss: 0.28497615456581116\n",
      "Epoch 2999, Loss: 0.5097073838114738, Final Batch Loss: 0.17710469663143158\n",
      "Epoch 3000, Loss: 0.5405516475439072, Final Batch Loss: 0.1561802178621292\n",
      "Epoch 3001, Loss: 0.7188143879175186, Final Batch Loss: 0.27038687467575073\n",
      "Epoch 3002, Loss: 0.4001297801733017, Final Batch Loss: 0.12518849968910217\n",
      "Epoch 3003, Loss: 0.4955121800303459, Final Batch Loss: 0.16674789786338806\n",
      "Epoch 3004, Loss: 0.5437479913234711, Final Batch Loss: 0.14299291372299194\n",
      "Epoch 3005, Loss: 0.4396028220653534, Final Batch Loss: 0.17775914072990417\n",
      "Epoch 3006, Loss: 0.4500046744942665, Final Batch Loss: 0.09966254979372025\n",
      "Epoch 3007, Loss: 0.5058902949094772, Final Batch Loss: 0.14278161525726318\n",
      "Epoch 3008, Loss: 0.5021671652793884, Final Batch Loss: 0.14996083080768585\n",
      "Epoch 3009, Loss: 0.46771515905857086, Final Batch Loss: 0.12962035834789276\n",
      "Epoch 3010, Loss: 0.5226376950740814, Final Batch Loss: 0.13593566417694092\n",
      "Epoch 3011, Loss: 0.45484694838523865, Final Batch Loss: 0.15374526381492615\n",
      "Epoch 3012, Loss: 0.4792834147810936, Final Batch Loss: 0.1684199869632721\n",
      "Epoch 3013, Loss: 0.4106702581048012, Final Batch Loss: 0.08636849373579025\n",
      "Epoch 3014, Loss: 0.389469712972641, Final Batch Loss: 0.08883248269557953\n",
      "Epoch 3015, Loss: 0.5474423468112946, Final Batch Loss: 0.20485195517539978\n",
      "Epoch 3016, Loss: 0.33680152148008347, Final Batch Loss: 0.07834955304861069\n",
      "Epoch 3017, Loss: 0.45641452819108963, Final Batch Loss: 0.18242783844470978\n",
      "Epoch 3018, Loss: 0.5148825794458389, Final Batch Loss: 0.17871271073818207\n",
      "Epoch 3019, Loss: 0.41580627113580704, Final Batch Loss: 0.10238835960626602\n",
      "Epoch 3020, Loss: 0.447227418422699, Final Batch Loss: 0.16142739355564117\n",
      "Epoch 3021, Loss: 0.4668992981314659, Final Batch Loss: 0.10616301000118256\n",
      "Epoch 3022, Loss: 0.4550718739628792, Final Batch Loss: 0.10192359983921051\n",
      "Epoch 3023, Loss: 0.4946300834417343, Final Batch Loss: 0.21771572530269623\n",
      "Epoch 3024, Loss: 0.4341379404067993, Final Batch Loss: 0.17757023870944977\n",
      "Epoch 3025, Loss: 0.46411097049713135, Final Batch Loss: 0.18806608021259308\n",
      "Epoch 3026, Loss: 0.4958534240722656, Final Batch Loss: 0.15605835616588593\n",
      "Epoch 3027, Loss: 0.38380787521600723, Final Batch Loss: 0.09891893714666367\n",
      "Epoch 3028, Loss: 0.48437342047691345, Final Batch Loss: 0.14497926831245422\n",
      "Epoch 3029, Loss: 0.4902833551168442, Final Batch Loss: 0.14354918897151947\n",
      "Epoch 3030, Loss: 0.4031069949269295, Final Batch Loss: 0.11944788694381714\n",
      "Epoch 3031, Loss: 0.36945758759975433, Final Batch Loss: 0.10452358424663544\n",
      "Epoch 3032, Loss: 0.455273874104023, Final Batch Loss: 0.1734914630651474\n",
      "Epoch 3033, Loss: 0.47908611595630646, Final Batch Loss: 0.19468314945697784\n",
      "Epoch 3034, Loss: 0.44525400549173355, Final Batch Loss: 0.16226600110530853\n",
      "Epoch 3035, Loss: 0.4954191595315933, Final Batch Loss: 0.22494666278362274\n",
      "Epoch 3036, Loss: 0.6021222621202469, Final Batch Loss: 0.23787792026996613\n",
      "Epoch 3037, Loss: 0.4082167074084282, Final Batch Loss: 0.09476805478334427\n",
      "Epoch 3038, Loss: 0.380743108689785, Final Batch Loss: 0.12121681123971939\n",
      "Epoch 3039, Loss: 0.4712640047073364, Final Batch Loss: 0.1752191036939621\n",
      "Epoch 3040, Loss: 0.42918457835912704, Final Batch Loss: 0.16257108747959137\n",
      "Epoch 3041, Loss: 0.44555576145648956, Final Batch Loss: 0.16532161831855774\n",
      "Epoch 3042, Loss: 0.3733939230442047, Final Batch Loss: 0.07363297045230865\n",
      "Epoch 3043, Loss: 0.4959281384944916, Final Batch Loss: 0.1574646234512329\n",
      "Epoch 3044, Loss: 0.4652569890022278, Final Batch Loss: 0.14524570107460022\n",
      "Epoch 3045, Loss: 0.4179878979921341, Final Batch Loss: 0.09832870960235596\n",
      "Epoch 3046, Loss: 0.47451767325401306, Final Batch Loss: 0.1469651758670807\n",
      "Epoch 3047, Loss: 0.4633806049823761, Final Batch Loss: 0.07984651625156403\n",
      "Epoch 3048, Loss: 0.45417477935552597, Final Batch Loss: 0.12249743193387985\n",
      "Epoch 3049, Loss: 0.45822329819202423, Final Batch Loss: 0.09670714288949966\n",
      "Epoch 3050, Loss: 0.4518653303384781, Final Batch Loss: 0.1673925518989563\n",
      "Epoch 3051, Loss: 0.4228302836418152, Final Batch Loss: 0.1395300030708313\n",
      "Epoch 3052, Loss: 0.5171899050474167, Final Batch Loss: 0.15109707415103912\n",
      "Epoch 3053, Loss: 0.45900775492191315, Final Batch Loss: 0.13731394708156586\n",
      "Epoch 3054, Loss: 0.49183548986911774, Final Batch Loss: 0.18715237081050873\n",
      "Epoch 3055, Loss: 0.41955386847257614, Final Batch Loss: 0.19865089654922485\n",
      "Epoch 3056, Loss: 0.44391393661499023, Final Batch Loss: 0.1106964647769928\n",
      "Epoch 3057, Loss: 0.4606214165687561, Final Batch Loss: 0.13962960243225098\n",
      "Epoch 3058, Loss: 0.5396030694246292, Final Batch Loss: 0.17766878008842468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3059, Loss: 0.5030552968382835, Final Batch Loss: 0.20517344772815704\n",
      "Epoch 3060, Loss: 0.4986293166875839, Final Batch Loss: 0.21963746845722198\n",
      "Epoch 3061, Loss: 0.48335691541433334, Final Batch Loss: 0.21813558042049408\n",
      "Epoch 3062, Loss: 0.44854892045259476, Final Batch Loss: 0.12062714248895645\n",
      "Epoch 3063, Loss: 0.4834282249212265, Final Batch Loss: 0.15262337028980255\n",
      "Epoch 3064, Loss: 0.4283781200647354, Final Batch Loss: 0.14512930810451508\n",
      "Epoch 3065, Loss: 0.40151340514421463, Final Batch Loss: 0.1227012351155281\n",
      "Epoch 3066, Loss: 0.5326612889766693, Final Batch Loss: 0.13865487277507782\n",
      "Epoch 3067, Loss: 0.3748791590332985, Final Batch Loss: 0.0913485512137413\n",
      "Epoch 3068, Loss: 0.4849471226334572, Final Batch Loss: 0.21547354757785797\n",
      "Epoch 3069, Loss: 0.47820694744586945, Final Batch Loss: 0.13081814348697662\n",
      "Epoch 3070, Loss: 0.41791536659002304, Final Batch Loss: 0.16719090938568115\n",
      "Epoch 3071, Loss: 0.5169452130794525, Final Batch Loss: 0.1627705991268158\n",
      "Epoch 3072, Loss: 0.48587799072265625, Final Batch Loss: 0.15121613442897797\n",
      "Epoch 3073, Loss: 0.5653921514749527, Final Batch Loss: 0.1930316835641861\n",
      "Epoch 3074, Loss: 0.44173380732536316, Final Batch Loss: 0.18422311544418335\n",
      "Epoch 3075, Loss: 0.437078058719635, Final Batch Loss: 0.15300191938877106\n",
      "Epoch 3076, Loss: 0.46382254362106323, Final Batch Loss: 0.13716235756874084\n",
      "Epoch 3077, Loss: 0.49611999094486237, Final Batch Loss: 0.14979015290737152\n",
      "Epoch 3078, Loss: 0.40702418982982635, Final Batch Loss: 0.13591082394123077\n",
      "Epoch 3079, Loss: 0.4055885225534439, Final Batch Loss: 0.17949974536895752\n",
      "Epoch 3080, Loss: 0.5019439309835434, Final Batch Loss: 0.2576703727245331\n",
      "Epoch 3081, Loss: 0.4112342670559883, Final Batch Loss: 0.11614427715539932\n",
      "Epoch 3082, Loss: 0.3644828572869301, Final Batch Loss: 0.11863502860069275\n",
      "Epoch 3083, Loss: 0.45932891964912415, Final Batch Loss: 0.1490868479013443\n",
      "Epoch 3084, Loss: 0.4094875901937485, Final Batch Loss: 0.12636196613311768\n",
      "Epoch 3085, Loss: 0.4111576974391937, Final Batch Loss: 0.11312450468540192\n",
      "Epoch 3086, Loss: 0.4896284192800522, Final Batch Loss: 0.14753209054470062\n",
      "Epoch 3087, Loss: 0.413530558347702, Final Batch Loss: 0.12917883694171906\n",
      "Epoch 3088, Loss: 0.4206522926688194, Final Batch Loss: 0.07453583925962448\n",
      "Epoch 3089, Loss: 0.38787951320409775, Final Batch Loss: 0.11232396960258484\n",
      "Epoch 3090, Loss: 0.44044654071331024, Final Batch Loss: 0.17285631597042084\n",
      "Epoch 3091, Loss: 0.48992832005023956, Final Batch Loss: 0.17928186058998108\n",
      "Epoch 3092, Loss: 0.3912479877471924, Final Batch Loss: 0.12067686021327972\n",
      "Epoch 3093, Loss: 0.4216463342308998, Final Batch Loss: 0.11091692000627518\n",
      "Epoch 3094, Loss: 0.37065768986940384, Final Batch Loss: 0.07193910330533981\n",
      "Epoch 3095, Loss: 0.5658724457025528, Final Batch Loss: 0.2176874279975891\n",
      "Epoch 3096, Loss: 0.5094994306564331, Final Batch Loss: 0.1556285172700882\n",
      "Epoch 3097, Loss: 0.5712815821170807, Final Batch Loss: 0.17113910615444183\n",
      "Epoch 3098, Loss: 0.49313318729400635, Final Batch Loss: 0.17471027374267578\n",
      "Epoch 3099, Loss: 0.42797841131687164, Final Batch Loss: 0.1374950408935547\n",
      "Epoch 3100, Loss: 0.37822769582271576, Final Batch Loss: 0.11955006420612335\n",
      "Epoch 3101, Loss: 0.4118903651833534, Final Batch Loss: 0.12205825001001358\n",
      "Epoch 3102, Loss: 0.5465747267007828, Final Batch Loss: 0.17061924934387207\n",
      "Epoch 3103, Loss: 0.5127213522791862, Final Batch Loss: 0.10839306563138962\n",
      "Epoch 3104, Loss: 0.48474009335041046, Final Batch Loss: 0.15299855172634125\n",
      "Epoch 3105, Loss: 0.47964783012866974, Final Batch Loss: 0.12077003717422485\n",
      "Epoch 3106, Loss: 0.4181332588195801, Final Batch Loss: 0.12534360587596893\n",
      "Epoch 3107, Loss: 0.4083278924226761, Final Batch Loss: 0.0949329286813736\n",
      "Epoch 3108, Loss: 0.5106372162699699, Final Batch Loss: 0.22003786265850067\n",
      "Epoch 3109, Loss: 0.5228279754519463, Final Batch Loss: 0.2210700809955597\n",
      "Epoch 3110, Loss: 0.3882938027381897, Final Batch Loss: 0.14441688358783722\n",
      "Epoch 3111, Loss: 0.464070700109005, Final Batch Loss: 0.2249959409236908\n",
      "Epoch 3112, Loss: 0.5379463285207748, Final Batch Loss: 0.2679029405117035\n",
      "Epoch 3113, Loss: 0.4997362643480301, Final Batch Loss: 0.23113325238227844\n",
      "Epoch 3114, Loss: 0.4276665449142456, Final Batch Loss: 0.14404746890068054\n",
      "Epoch 3115, Loss: 0.49812575429677963, Final Batch Loss: 0.16064926981925964\n",
      "Epoch 3116, Loss: 0.45178766548633575, Final Batch Loss: 0.1377260684967041\n",
      "Epoch 3117, Loss: 0.4327724128961563, Final Batch Loss: 0.152265727519989\n",
      "Epoch 3118, Loss: 0.4617436081171036, Final Batch Loss: 0.16434822976589203\n",
      "Epoch 3119, Loss: 0.4857934042811394, Final Batch Loss: 0.24797332286834717\n",
      "Epoch 3120, Loss: 0.4070221483707428, Final Batch Loss: 0.12696635723114014\n",
      "Epoch 3121, Loss: 0.4820954203605652, Final Batch Loss: 0.2182731330394745\n",
      "Epoch 3122, Loss: 0.4277126416563988, Final Batch Loss: 0.1224028691649437\n",
      "Epoch 3123, Loss: 0.3748711571097374, Final Batch Loss: 0.1174253597855568\n",
      "Epoch 3124, Loss: 0.4819822460412979, Final Batch Loss: 0.19144350290298462\n",
      "Epoch 3125, Loss: 0.5130338370800018, Final Batch Loss: 0.24520735442638397\n",
      "Epoch 3126, Loss: 0.4446820169687271, Final Batch Loss: 0.15499241650104523\n",
      "Epoch 3127, Loss: 0.3885151147842407, Final Batch Loss: 0.13456328213214874\n",
      "Epoch 3128, Loss: 0.39577271044254303, Final Batch Loss: 0.12520340085029602\n",
      "Epoch 3129, Loss: 0.46845580637454987, Final Batch Loss: 0.14695949852466583\n",
      "Epoch 3130, Loss: 0.49018168449401855, Final Batch Loss: 0.12780053913593292\n",
      "Epoch 3131, Loss: 0.5108245983719826, Final Batch Loss: 0.23835597932338715\n",
      "Epoch 3132, Loss: 0.4919198155403137, Final Batch Loss: 0.21217219531536102\n",
      "Epoch 3133, Loss: 0.4502212852239609, Final Batch Loss: 0.17194677889347076\n",
      "Epoch 3134, Loss: 0.41277043521404266, Final Batch Loss: 0.1559077948331833\n",
      "Epoch 3135, Loss: 0.36362776905298233, Final Batch Loss: 0.1100090742111206\n",
      "Epoch 3136, Loss: 0.4275590777397156, Final Batch Loss: 0.1702069342136383\n",
      "Epoch 3137, Loss: 0.4419306665658951, Final Batch Loss: 0.13257144391536713\n",
      "Epoch 3138, Loss: 0.5105201154947281, Final Batch Loss: 0.21702538430690765\n",
      "Epoch 3139, Loss: 0.524671345949173, Final Batch Loss: 0.21117503941059113\n",
      "Epoch 3140, Loss: 0.5179759413003922, Final Batch Loss: 0.1677856743335724\n",
      "Epoch 3141, Loss: 0.44727623462677, Final Batch Loss: 0.130977064371109\n",
      "Epoch 3142, Loss: 0.5503140985965729, Final Batch Loss: 0.13214315474033356\n",
      "Epoch 3143, Loss: 0.48710793256759644, Final Batch Loss: 0.136877179145813\n",
      "Epoch 3144, Loss: 0.39529646188020706, Final Batch Loss: 0.15358427166938782\n",
      "Epoch 3145, Loss: 0.5502432435750961, Final Batch Loss: 0.17030145227909088\n",
      "Epoch 3146, Loss: 0.5338182002305984, Final Batch Loss: 0.16872921586036682\n",
      "Epoch 3147, Loss: 0.4555470645427704, Final Batch Loss: 0.13268758356571198\n",
      "Epoch 3148, Loss: 0.5286411046981812, Final Batch Loss: 0.1588214635848999\n",
      "Epoch 3149, Loss: 0.49242210388183594, Final Batch Loss: 0.16464978456497192\n",
      "Epoch 3150, Loss: 0.4849754571914673, Final Batch Loss: 0.14605064690113068\n",
      "Epoch 3151, Loss: 0.518446072936058, Final Batch Loss: 0.18894636631011963\n",
      "Epoch 3152, Loss: 0.4091726690530777, Final Batch Loss: 0.201126828789711\n",
      "Epoch 3153, Loss: 0.4332698807120323, Final Batch Loss: 0.1711663156747818\n",
      "Epoch 3154, Loss: 0.4042307510972023, Final Batch Loss: 0.12303061038255692\n",
      "Epoch 3155, Loss: 0.41433925926685333, Final Batch Loss: 0.07718828320503235\n",
      "Epoch 3156, Loss: 0.5215873569250107, Final Batch Loss: 0.1614222675561905\n",
      "Epoch 3157, Loss: 0.40547334402799606, Final Batch Loss: 0.15757396817207336\n",
      "Epoch 3158, Loss: 0.5877644568681717, Final Batch Loss: 0.16618984937667847\n",
      "Epoch 3159, Loss: 0.4319726824760437, Final Batch Loss: 0.16701865196228027\n",
      "Epoch 3160, Loss: 0.530302420258522, Final Batch Loss: 0.19871333241462708\n",
      "Epoch 3161, Loss: 0.5497814267873764, Final Batch Loss: 0.18387337028980255\n",
      "Epoch 3162, Loss: 0.381981760263443, Final Batch Loss: 0.10118548572063446\n",
      "Epoch 3163, Loss: 0.48949822038412094, Final Batch Loss: 0.13303840160369873\n",
      "Epoch 3164, Loss: 0.5473706424236298, Final Batch Loss: 0.21873946487903595\n",
      "Epoch 3165, Loss: 0.4308703765273094, Final Batch Loss: 0.1500554084777832\n",
      "Epoch 3166, Loss: 0.5009594857692719, Final Batch Loss: 0.15276511013507843\n",
      "Epoch 3167, Loss: 0.5005054771900177, Final Batch Loss: 0.1774221658706665\n",
      "Epoch 3168, Loss: 0.434023916721344, Final Batch Loss: 0.13583317399024963\n",
      "Epoch 3169, Loss: 0.5050691217184067, Final Batch Loss: 0.113991379737854\n",
      "Epoch 3170, Loss: 0.4407540559768677, Final Batch Loss: 0.10756264626979828\n",
      "Epoch 3171, Loss: 0.44553349912166595, Final Batch Loss: 0.12108056247234344\n",
      "Epoch 3172, Loss: 0.4074554741382599, Final Batch Loss: 0.11950428783893585\n",
      "Epoch 3173, Loss: 0.540099486708641, Final Batch Loss: 0.2160084992647171\n",
      "Epoch 3174, Loss: 0.5661807954311371, Final Batch Loss: 0.2520802319049835\n",
      "Epoch 3175, Loss: 0.4714599698781967, Final Batch Loss: 0.14798948168754578\n",
      "Epoch 3176, Loss: 0.46048206090927124, Final Batch Loss: 0.12385495007038116\n",
      "Epoch 3177, Loss: 0.5325103998184204, Final Batch Loss: 0.1462516486644745\n",
      "Epoch 3178, Loss: 0.44133590161800385, Final Batch Loss: 0.12293416261672974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3179, Loss: 0.47650929540395737, Final Batch Loss: 0.09738969057798386\n",
      "Epoch 3180, Loss: 0.43236567080020905, Final Batch Loss: 0.149275004863739\n",
      "Epoch 3181, Loss: 0.44033273309469223, Final Batch Loss: 0.11788507550954819\n",
      "Epoch 3182, Loss: 0.4390546455979347, Final Batch Loss: 0.23253004252910614\n",
      "Epoch 3183, Loss: 0.4876798093318939, Final Batch Loss: 0.12607616186141968\n",
      "Epoch 3184, Loss: 0.48186254501342773, Final Batch Loss: 0.1290985345840454\n",
      "Epoch 3185, Loss: 0.4254647269845009, Final Batch Loss: 0.10349718481302261\n",
      "Epoch 3186, Loss: 0.45677249133586884, Final Batch Loss: 0.09345342218875885\n",
      "Epoch 3187, Loss: 0.5876524746417999, Final Batch Loss: 0.2744308412075043\n",
      "Epoch 3188, Loss: 0.44290055334568024, Final Batch Loss: 0.13130106031894684\n",
      "Epoch 3189, Loss: 0.4653610289096832, Final Batch Loss: 0.16680745780467987\n",
      "Epoch 3190, Loss: 0.3812972158193588, Final Batch Loss: 0.1057361513376236\n",
      "Epoch 3191, Loss: 0.5744039118289948, Final Batch Loss: 0.26589468121528625\n",
      "Epoch 3192, Loss: 0.3974669873714447, Final Batch Loss: 0.12989197671413422\n",
      "Epoch 3193, Loss: 0.4532872214913368, Final Batch Loss: 0.13613834977149963\n",
      "Epoch 3194, Loss: 0.3929283693432808, Final Batch Loss: 0.12016599625349045\n",
      "Epoch 3195, Loss: 0.49005142599344254, Final Batch Loss: 0.0983681008219719\n",
      "Epoch 3196, Loss: 0.551435187458992, Final Batch Loss: 0.2215491682291031\n",
      "Epoch 3197, Loss: 0.4909181147813797, Final Batch Loss: 0.1744585484266281\n",
      "Epoch 3198, Loss: 0.3576529324054718, Final Batch Loss: 0.08118300884962082\n",
      "Epoch 3199, Loss: 0.49503812193870544, Final Batch Loss: 0.15561623871326447\n",
      "Epoch 3200, Loss: 0.5080306380987167, Final Batch Loss: 0.1599220484495163\n",
      "Epoch 3201, Loss: 0.3927789181470871, Final Batch Loss: 0.13500389456748962\n",
      "Epoch 3202, Loss: 0.43762435019016266, Final Batch Loss: 0.1284041553735733\n",
      "Epoch 3203, Loss: 0.4387764185667038, Final Batch Loss: 0.15817780792713165\n",
      "Epoch 3204, Loss: 0.408943772315979, Final Batch Loss: 0.14857199788093567\n",
      "Epoch 3205, Loss: 0.45053138583898544, Final Batch Loss: 0.17019584774971008\n",
      "Epoch 3206, Loss: 0.43032507598400116, Final Batch Loss: 0.14685963094234467\n",
      "Epoch 3207, Loss: 0.4956023842096329, Final Batch Loss: 0.1740826517343521\n",
      "Epoch 3208, Loss: 0.547075629234314, Final Batch Loss: 0.1581900715827942\n",
      "Epoch 3209, Loss: 0.7004850506782532, Final Batch Loss: 0.2892603576183319\n",
      "Epoch 3210, Loss: 0.47436413168907166, Final Batch Loss: 0.13887172937393188\n",
      "Epoch 3211, Loss: 0.5607158690690994, Final Batch Loss: 0.23166081309318542\n",
      "Epoch 3212, Loss: 0.6471403241157532, Final Batch Loss: 0.1825082153081894\n",
      "Epoch 3213, Loss: 0.47188519686460495, Final Batch Loss: 0.18923287093639374\n",
      "Epoch 3214, Loss: 0.46418336033821106, Final Batch Loss: 0.1495249718427658\n",
      "Epoch 3215, Loss: 0.5006165653467178, Final Batch Loss: 0.1745370328426361\n",
      "Epoch 3216, Loss: 0.4182792603969574, Final Batch Loss: 0.14314335584640503\n",
      "Epoch 3217, Loss: 0.47978560626506805, Final Batch Loss: 0.1600286066532135\n",
      "Epoch 3218, Loss: 0.5446407049894333, Final Batch Loss: 0.13861185312271118\n",
      "Epoch 3219, Loss: 0.5063446760177612, Final Batch Loss: 0.16829431056976318\n",
      "Epoch 3220, Loss: 0.3868621289730072, Final Batch Loss: 0.0768325924873352\n",
      "Epoch 3221, Loss: 0.4046986699104309, Final Batch Loss: 0.1110912412405014\n",
      "Epoch 3222, Loss: 0.47810132801532745, Final Batch Loss: 0.16278637945652008\n",
      "Epoch 3223, Loss: 0.3763227164745331, Final Batch Loss: 0.1441626101732254\n",
      "Epoch 3224, Loss: 0.5150949209928513, Final Batch Loss: 0.1816796213388443\n",
      "Epoch 3225, Loss: 0.4846571683883667, Final Batch Loss: 0.21583309769630432\n",
      "Epoch 3226, Loss: 0.45044585317373276, Final Batch Loss: 0.12104836851358414\n",
      "Epoch 3227, Loss: 0.5073870569467545, Final Batch Loss: 0.1713680773973465\n",
      "Epoch 3228, Loss: 0.47717510163784027, Final Batch Loss: 0.17753778398036957\n",
      "Epoch 3229, Loss: 0.5429786294698715, Final Batch Loss: 0.19975829124450684\n",
      "Epoch 3230, Loss: 0.4517262205481529, Final Batch Loss: 0.18728679418563843\n",
      "Epoch 3231, Loss: 0.379700630903244, Final Batch Loss: 0.12519927322864532\n",
      "Epoch 3232, Loss: 0.29933009296655655, Final Batch Loss: 0.09136851131916046\n",
      "Epoch 3233, Loss: 0.646571546792984, Final Batch Loss: 0.181527242064476\n",
      "Epoch 3234, Loss: 0.4779265448451042, Final Batch Loss: 0.12471383064985275\n",
      "Epoch 3235, Loss: 0.46410733461380005, Final Batch Loss: 0.13460367918014526\n",
      "Epoch 3236, Loss: 0.4643750488758087, Final Batch Loss: 0.1270875185728073\n",
      "Epoch 3237, Loss: 0.41767360270023346, Final Batch Loss: 0.11594967544078827\n",
      "Epoch 3238, Loss: 0.5930994898080826, Final Batch Loss: 0.17769695818424225\n",
      "Epoch 3239, Loss: 0.5256423801183701, Final Batch Loss: 0.23871074616909027\n",
      "Epoch 3240, Loss: 0.48980163037776947, Final Batch Loss: 0.1680576503276825\n",
      "Epoch 3241, Loss: 0.4619397968053818, Final Batch Loss: 0.09848196804523468\n",
      "Epoch 3242, Loss: 0.491963267326355, Final Batch Loss: 0.15343286097049713\n",
      "Epoch 3243, Loss: 0.4838133454322815, Final Batch Loss: 0.1800459772348404\n",
      "Epoch 3244, Loss: 0.3638428673148155, Final Batch Loss: 0.08677058666944504\n",
      "Epoch 3245, Loss: 0.36979590356349945, Final Batch Loss: 0.1110013946890831\n",
      "Epoch 3246, Loss: 0.4328857511281967, Final Batch Loss: 0.1242438480257988\n",
      "Epoch 3247, Loss: 0.43118953704833984, Final Batch Loss: 0.12354402244091034\n",
      "Epoch 3248, Loss: 0.4335222840309143, Final Batch Loss: 0.12727303802967072\n",
      "Epoch 3249, Loss: 0.4423169121146202, Final Batch Loss: 0.1312834918498993\n",
      "Epoch 3250, Loss: 0.42456023395061493, Final Batch Loss: 0.15018457174301147\n",
      "Epoch 3251, Loss: 0.3352201506495476, Final Batch Loss: 0.10682640969753265\n",
      "Epoch 3252, Loss: 0.48193205893039703, Final Batch Loss: 0.13451777398586273\n",
      "Epoch 3253, Loss: 0.4381054490804672, Final Batch Loss: 0.10229383409023285\n",
      "Epoch 3254, Loss: 0.43223146349191666, Final Batch Loss: 0.08847116678953171\n",
      "Epoch 3255, Loss: 0.4077885076403618, Final Batch Loss: 0.14411427080631256\n",
      "Epoch 3256, Loss: 0.4030568525195122, Final Batch Loss: 0.10937723517417908\n",
      "Epoch 3257, Loss: 0.3497442230582237, Final Batch Loss: 0.1000623032450676\n",
      "Epoch 3258, Loss: 0.45235825330018997, Final Batch Loss: 0.11428252607584\n",
      "Epoch 3259, Loss: 0.4149535149335861, Final Batch Loss: 0.1574491709470749\n",
      "Epoch 3260, Loss: 0.3681908920407295, Final Batch Loss: 0.09805146604776382\n",
      "Epoch 3261, Loss: 0.4801298528909683, Final Batch Loss: 0.15887056291103363\n",
      "Epoch 3262, Loss: 0.5030048787593842, Final Batch Loss: 0.21342748403549194\n",
      "Epoch 3263, Loss: 0.39761873334646225, Final Batch Loss: 0.0793832316994667\n",
      "Epoch 3264, Loss: 0.4251151531934738, Final Batch Loss: 0.14690043032169342\n",
      "Epoch 3265, Loss: 0.4581616669893265, Final Batch Loss: 0.188057541847229\n",
      "Epoch 3266, Loss: 0.4531908631324768, Final Batch Loss: 0.1261250078678131\n",
      "Epoch 3267, Loss: 0.44745831936597824, Final Batch Loss: 0.12385552376508713\n",
      "Epoch 3268, Loss: 0.3815182149410248, Final Batch Loss: 0.10177083313465118\n",
      "Epoch 3269, Loss: 0.4395906776189804, Final Batch Loss: 0.09909519553184509\n",
      "Epoch 3270, Loss: 0.46330997347831726, Final Batch Loss: 0.12266437709331512\n",
      "Epoch 3271, Loss: 0.3679174706339836, Final Batch Loss: 0.10336533188819885\n",
      "Epoch 3272, Loss: 0.5255115628242493, Final Batch Loss: 0.1288020759820938\n",
      "Epoch 3273, Loss: 0.43064403533935547, Final Batch Loss: 0.11820492148399353\n",
      "Epoch 3274, Loss: 0.4372156634926796, Final Batch Loss: 0.12486454099416733\n",
      "Epoch 3275, Loss: 0.4636669382452965, Final Batch Loss: 0.10671640187501907\n",
      "Epoch 3276, Loss: 0.41065845638513565, Final Batch Loss: 0.14820007979869843\n",
      "Epoch 3277, Loss: 0.45452314615249634, Final Batch Loss: 0.17574821412563324\n",
      "Epoch 3278, Loss: 0.4823436141014099, Final Batch Loss: 0.19690391421318054\n",
      "Epoch 3279, Loss: 0.5594166815280914, Final Batch Loss: 0.16355551779270172\n",
      "Epoch 3280, Loss: 0.43674755841493607, Final Batch Loss: 0.14483001828193665\n",
      "Epoch 3281, Loss: 0.467537522315979, Final Batch Loss: 0.14425872266292572\n",
      "Epoch 3282, Loss: 0.40612131357192993, Final Batch Loss: 0.14956505596637726\n",
      "Epoch 3283, Loss: 0.43489302694797516, Final Batch Loss: 0.19129057228565216\n",
      "Epoch 3284, Loss: 0.47549422085285187, Final Batch Loss: 0.15564872324466705\n",
      "Epoch 3285, Loss: 0.42563172429800034, Final Batch Loss: 0.09074880182743073\n",
      "Epoch 3286, Loss: 0.5338355675339699, Final Batch Loss: 0.12027143687009811\n",
      "Epoch 3287, Loss: 0.4728427901864052, Final Batch Loss: 0.16850751638412476\n",
      "Epoch 3288, Loss: 0.40982770174741745, Final Batch Loss: 0.0818248763680458\n",
      "Epoch 3289, Loss: 0.38642866164445877, Final Batch Loss: 0.15464821457862854\n",
      "Epoch 3290, Loss: 0.3766876384615898, Final Batch Loss: 0.10669507831335068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3291, Loss: 0.5482455641031265, Final Batch Loss: 0.16465270519256592\n",
      "Epoch 3292, Loss: 0.36553388088941574, Final Batch Loss: 0.10793187469244003\n",
      "Epoch 3293, Loss: 0.5523569285869598, Final Batch Loss: 0.15876543521881104\n",
      "Epoch 3294, Loss: 0.4604260325431824, Final Batch Loss: 0.14254134893417358\n",
      "Epoch 3295, Loss: 0.3478930965065956, Final Batch Loss: 0.08570833504199982\n",
      "Epoch 3296, Loss: 0.4506133273243904, Final Batch Loss: 0.12022851407527924\n",
      "Epoch 3297, Loss: 0.5755540207028389, Final Batch Loss: 0.1212349608540535\n",
      "Epoch 3298, Loss: 0.5029305219650269, Final Batch Loss: 0.1589701771736145\n",
      "Epoch 3299, Loss: 0.4428657740354538, Final Batch Loss: 0.1310151219367981\n",
      "Epoch 3300, Loss: 0.3980639725923538, Final Batch Loss: 0.10509558022022247\n",
      "Epoch 3301, Loss: 0.587857075035572, Final Batch Loss: 0.2765147089958191\n",
      "Epoch 3302, Loss: 0.4605666995048523, Final Batch Loss: 0.11301401257514954\n",
      "Epoch 3303, Loss: 0.5025100409984589, Final Batch Loss: 0.162417471408844\n",
      "Epoch 3304, Loss: 0.4640895128250122, Final Batch Loss: 0.16252274811267853\n",
      "Epoch 3305, Loss: 0.4147513583302498, Final Batch Loss: 0.1627359390258789\n",
      "Epoch 3306, Loss: 0.46560361981391907, Final Batch Loss: 0.15959566831588745\n",
      "Epoch 3307, Loss: 0.3394020050764084, Final Batch Loss: 0.0969950407743454\n",
      "Epoch 3308, Loss: 0.40980247408151627, Final Batch Loss: 0.17437049746513367\n",
      "Epoch 3309, Loss: 0.4465246647596359, Final Batch Loss: 0.14388687908649445\n",
      "Epoch 3310, Loss: 0.48752836138010025, Final Batch Loss: 0.20198658108711243\n",
      "Epoch 3311, Loss: 0.3744100332260132, Final Batch Loss: 0.07131125032901764\n",
      "Epoch 3312, Loss: 0.5093580335378647, Final Batch Loss: 0.2727789878845215\n",
      "Epoch 3313, Loss: 0.5478523969650269, Final Batch Loss: 0.18690072000026703\n",
      "Epoch 3314, Loss: 0.3337783217430115, Final Batch Loss: 0.07743355631828308\n",
      "Epoch 3315, Loss: 0.5127702206373215, Final Batch Loss: 0.2365543097257614\n",
      "Epoch 3316, Loss: 0.4467192143201828, Final Batch Loss: 0.09058921039104462\n",
      "Epoch 3317, Loss: 0.42954792082309723, Final Batch Loss: 0.16059114038944244\n",
      "Epoch 3318, Loss: 0.4522821605205536, Final Batch Loss: 0.193936288356781\n",
      "Epoch 3319, Loss: 0.4911041036248207, Final Batch Loss: 0.17469699680805206\n",
      "Epoch 3320, Loss: 0.4696846902370453, Final Batch Loss: 0.1641012728214264\n",
      "Epoch 3321, Loss: 0.4844805598258972, Final Batch Loss: 0.18259383738040924\n",
      "Epoch 3322, Loss: 0.34953492879867554, Final Batch Loss: 0.08595658838748932\n",
      "Epoch 3323, Loss: 0.4283471256494522, Final Batch Loss: 0.1466647982597351\n",
      "Epoch 3324, Loss: 0.41248719394207, Final Batch Loss: 0.1407719999551773\n",
      "Epoch 3325, Loss: 0.5181425288319588, Final Batch Loss: 0.19471508264541626\n",
      "Epoch 3326, Loss: 0.39248212426900864, Final Batch Loss: 0.14628742635250092\n",
      "Epoch 3327, Loss: 0.39485636353492737, Final Batch Loss: 0.1274564266204834\n",
      "Epoch 3328, Loss: 0.4740818291902542, Final Batch Loss: 0.1626061350107193\n",
      "Epoch 3329, Loss: 0.44403018802404404, Final Batch Loss: 0.20669342577457428\n",
      "Epoch 3330, Loss: 0.46492427587509155, Final Batch Loss: 0.15498000383377075\n",
      "Epoch 3331, Loss: 0.3433907926082611, Final Batch Loss: 0.11148424446582794\n",
      "Epoch 3332, Loss: 0.48028646409511566, Final Batch Loss: 0.18694771826267242\n",
      "Epoch 3333, Loss: 0.5041777491569519, Final Batch Loss: 0.12676861882209778\n",
      "Epoch 3334, Loss: 0.5322229117155075, Final Batch Loss: 0.16636785864830017\n",
      "Epoch 3335, Loss: 0.5550081133842468, Final Batch Loss: 0.17368203401565552\n",
      "Epoch 3336, Loss: 0.46716153621673584, Final Batch Loss: 0.17611217498779297\n",
      "Epoch 3337, Loss: 0.4721953868865967, Final Batch Loss: 0.11873643100261688\n",
      "Epoch 3338, Loss: 0.3846657797694206, Final Batch Loss: 0.10369037836790085\n",
      "Epoch 3339, Loss: 0.4741435796022415, Final Batch Loss: 0.1668625771999359\n",
      "Epoch 3340, Loss: 0.5305906385183334, Final Batch Loss: 0.19611330330371857\n",
      "Epoch 3341, Loss: 0.3678289130330086, Final Batch Loss: 0.13544084131717682\n",
      "Epoch 3342, Loss: 0.4034302979707718, Final Batch Loss: 0.11719947308301926\n",
      "Epoch 3343, Loss: 0.3851966857910156, Final Batch Loss: 0.09077011048793793\n",
      "Epoch 3344, Loss: 0.3954797014594078, Final Batch Loss: 0.1427876353263855\n",
      "Epoch 3345, Loss: 0.4705374985933304, Final Batch Loss: 0.1817607432603836\n",
      "Epoch 3346, Loss: 0.4831141009926796, Final Batch Loss: 0.1558023989200592\n",
      "Epoch 3347, Loss: 0.40060076862573624, Final Batch Loss: 0.1017475351691246\n",
      "Epoch 3348, Loss: 0.42321114987134933, Final Batch Loss: 0.11607160419225693\n",
      "Epoch 3349, Loss: 0.49669747054576874, Final Batch Loss: 0.1402360200881958\n",
      "Epoch 3350, Loss: 0.4268090948462486, Final Batch Loss: 0.15060976147651672\n",
      "Epoch 3351, Loss: 0.4116126224398613, Final Batch Loss: 0.11841020733118057\n",
      "Epoch 3352, Loss: 0.4663083702325821, Final Batch Loss: 0.1868908703327179\n",
      "Epoch 3353, Loss: 0.384442076086998, Final Batch Loss: 0.11627586930990219\n",
      "Epoch 3354, Loss: 0.46142779290676117, Final Batch Loss: 0.11514395475387573\n",
      "Epoch 3355, Loss: 0.475452683866024, Final Batch Loss: 0.14425332844257355\n",
      "Epoch 3356, Loss: 0.41116172075271606, Final Batch Loss: 0.10275711119174957\n",
      "Epoch 3357, Loss: 0.4153595045208931, Final Batch Loss: 0.13958147168159485\n",
      "Epoch 3358, Loss: 0.45686329901218414, Final Batch Loss: 0.14723747968673706\n",
      "Epoch 3359, Loss: 0.45039981603622437, Final Batch Loss: 0.09292382001876831\n",
      "Epoch 3360, Loss: 0.5191395729780197, Final Batch Loss: 0.19871298968791962\n",
      "Epoch 3361, Loss: 0.4142032787203789, Final Batch Loss: 0.14693023264408112\n",
      "Epoch 3362, Loss: 0.4393068850040436, Final Batch Loss: 0.11423750221729279\n",
      "Epoch 3363, Loss: 0.45435622334480286, Final Batch Loss: 0.13580170273780823\n",
      "Epoch 3364, Loss: 0.456722155213356, Final Batch Loss: 0.18099389970302582\n",
      "Epoch 3365, Loss: 0.48095114529132843, Final Batch Loss: 0.15002693235874176\n",
      "Epoch 3366, Loss: 0.4009157121181488, Final Batch Loss: 0.14154934883117676\n",
      "Epoch 3367, Loss: 0.5315888971090317, Final Batch Loss: 0.14337798953056335\n",
      "Epoch 3368, Loss: 0.5328165292739868, Final Batch Loss: 0.15890604257583618\n",
      "Epoch 3369, Loss: 0.472697488963604, Final Batch Loss: 0.15271151065826416\n",
      "Epoch 3370, Loss: 0.45939502120018005, Final Batch Loss: 0.1301136314868927\n",
      "Epoch 3371, Loss: 0.3715750500559807, Final Batch Loss: 0.07391489297151566\n",
      "Epoch 3372, Loss: 0.39435378462076187, Final Batch Loss: 0.16504359245300293\n",
      "Epoch 3373, Loss: 0.40830032527446747, Final Batch Loss: 0.1433286964893341\n",
      "Epoch 3374, Loss: 0.44574275612831116, Final Batch Loss: 0.17113874852657318\n",
      "Epoch 3375, Loss: 0.5186878591775894, Final Batch Loss: 0.11964547634124756\n",
      "Epoch 3376, Loss: 0.4422449991106987, Final Batch Loss: 0.20523513853549957\n",
      "Epoch 3377, Loss: 0.4330083578824997, Final Batch Loss: 0.1318705528974533\n",
      "Epoch 3378, Loss: 0.4094080328941345, Final Batch Loss: 0.0962357223033905\n",
      "Epoch 3379, Loss: 0.4058636277914047, Final Batch Loss: 0.18162283301353455\n",
      "Epoch 3380, Loss: 0.40378522127866745, Final Batch Loss: 0.12510743737220764\n",
      "Epoch 3381, Loss: 0.40389250963926315, Final Batch Loss: 0.1332184374332428\n",
      "Epoch 3382, Loss: 0.5389044731855392, Final Batch Loss: 0.2551948130130768\n",
      "Epoch 3383, Loss: 0.40038926154375076, Final Batch Loss: 0.11235795170068741\n",
      "Epoch 3384, Loss: 0.5374389886856079, Final Batch Loss: 0.23047716915607452\n",
      "Epoch 3385, Loss: 0.4815719723701477, Final Batch Loss: 0.1953878551721573\n",
      "Epoch 3386, Loss: 0.5360191613435745, Final Batch Loss: 0.164714515209198\n",
      "Epoch 3387, Loss: 0.3858799785375595, Final Batch Loss: 0.1727459579706192\n",
      "Epoch 3388, Loss: 0.43228956311941147, Final Batch Loss: 0.16449548304080963\n",
      "Epoch 3389, Loss: 0.4702938199043274, Final Batch Loss: 0.15315772593021393\n",
      "Epoch 3390, Loss: 0.40633998066186905, Final Batch Loss: 0.17495103180408478\n",
      "Epoch 3391, Loss: 0.4848809540271759, Final Batch Loss: 0.16102838516235352\n",
      "Epoch 3392, Loss: 0.4226092994213104, Final Batch Loss: 0.12353962659835815\n",
      "Epoch 3393, Loss: 0.47123703360557556, Final Batch Loss: 0.17335481941699982\n",
      "Epoch 3394, Loss: 0.40089718997478485, Final Batch Loss: 0.1331108808517456\n",
      "Epoch 3395, Loss: 0.46385735273361206, Final Batch Loss: 0.15726110339164734\n",
      "Epoch 3396, Loss: 0.42730680853128433, Final Batch Loss: 0.12045801430940628\n",
      "Epoch 3397, Loss: 0.4123220890760422, Final Batch Loss: 0.11855117231607437\n",
      "Epoch 3398, Loss: 0.4945356845855713, Final Batch Loss: 0.20630872249603271\n",
      "Epoch 3399, Loss: 0.3756161853671074, Final Batch Loss: 0.17137975990772247\n",
      "Epoch 3400, Loss: 0.5027336031198502, Final Batch Loss: 0.1559729129076004\n",
      "Epoch 3401, Loss: 0.5179561078548431, Final Batch Loss: 0.22016440331935883\n",
      "Epoch 3402, Loss: 0.4717264547944069, Final Batch Loss: 0.16890473663806915\n",
      "Epoch 3403, Loss: 0.43220916390419006, Final Batch Loss: 0.14274844527244568\n",
      "Epoch 3404, Loss: 0.5180070549249649, Final Batch Loss: 0.15496166050434113\n",
      "Epoch 3405, Loss: 0.528707817196846, Final Batch Loss: 0.24308905005455017\n",
      "Epoch 3406, Loss: 0.5167095959186554, Final Batch Loss: 0.18058376014232635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3407, Loss: 0.5063087940216064, Final Batch Loss: 0.20064441859722137\n",
      "Epoch 3408, Loss: 0.4715329706668854, Final Batch Loss: 0.1449328362941742\n",
      "Epoch 3409, Loss: 0.3771955296397209, Final Batch Loss: 0.15663708746433258\n",
      "Epoch 3410, Loss: 0.4139266312122345, Final Batch Loss: 0.15204477310180664\n",
      "Epoch 3411, Loss: 0.490638330578804, Final Batch Loss: 0.1386203020811081\n",
      "Epoch 3412, Loss: 0.4303353428840637, Final Batch Loss: 0.13632330298423767\n",
      "Epoch 3413, Loss: 0.39937925338745117, Final Batch Loss: 0.10741844773292542\n",
      "Epoch 3414, Loss: 0.4661501348018646, Final Batch Loss: 0.15486276149749756\n",
      "Epoch 3415, Loss: 0.48559217154979706, Final Batch Loss: 0.2159470170736313\n",
      "Epoch 3416, Loss: 0.421781525015831, Final Batch Loss: 0.14406773447990417\n",
      "Epoch 3417, Loss: 0.49586835503578186, Final Batch Loss: 0.15913592278957367\n",
      "Epoch 3418, Loss: 0.4140876233577728, Final Batch Loss: 0.14601901173591614\n",
      "Epoch 3419, Loss: 0.412141777575016, Final Batch Loss: 0.09512654691934586\n",
      "Epoch 3420, Loss: 0.44155237823724747, Final Batch Loss: 0.16883863508701324\n",
      "Epoch 3421, Loss: 0.45586851984262466, Final Batch Loss: 0.16065065562725067\n",
      "Epoch 3422, Loss: 0.43306319415569305, Final Batch Loss: 0.13200260698795319\n",
      "Epoch 3423, Loss: 0.45831355452537537, Final Batch Loss: 0.10738950967788696\n",
      "Epoch 3424, Loss: 0.4510139971971512, Final Batch Loss: 0.12643446028232574\n",
      "Epoch 3425, Loss: 0.45421232283115387, Final Batch Loss: 0.14440126717090607\n",
      "Epoch 3426, Loss: 0.4667156934738159, Final Batch Loss: 0.19091078639030457\n",
      "Epoch 3427, Loss: 0.3365071192383766, Final Batch Loss: 0.14359831809997559\n",
      "Epoch 3428, Loss: 0.45324669778347015, Final Batch Loss: 0.16822901368141174\n",
      "Epoch 3429, Loss: 0.447675384581089, Final Batch Loss: 0.1083814725279808\n",
      "Epoch 3430, Loss: 0.40792547166347504, Final Batch Loss: 0.09489716589450836\n",
      "Epoch 3431, Loss: 0.3763076812028885, Final Batch Loss: 0.15050356090068817\n",
      "Epoch 3432, Loss: 0.49705496430397034, Final Batch Loss: 0.17317722737789154\n",
      "Epoch 3433, Loss: 0.35800375789403915, Final Batch Loss: 0.06424961239099503\n",
      "Epoch 3434, Loss: 0.5544829219579697, Final Batch Loss: 0.2048274129629135\n",
      "Epoch 3435, Loss: 0.5013454705476761, Final Batch Loss: 0.18592604994773865\n",
      "Epoch 3436, Loss: 0.4943044036626816, Final Batch Loss: 0.1440548449754715\n",
      "Epoch 3437, Loss: 0.46416206657886505, Final Batch Loss: 0.1450733095407486\n",
      "Epoch 3438, Loss: 0.3816976025700569, Final Batch Loss: 0.05908378213644028\n",
      "Epoch 3439, Loss: 0.40554043650627136, Final Batch Loss: 0.09166411310434341\n",
      "Epoch 3440, Loss: 0.4068744108080864, Final Batch Loss: 0.1700451672077179\n",
      "Epoch 3441, Loss: 0.4404771402478218, Final Batch Loss: 0.19167868793010712\n",
      "Epoch 3442, Loss: 0.4365280196070671, Final Batch Loss: 0.20495663583278656\n",
      "Epoch 3443, Loss: 0.4367826357483864, Final Batch Loss: 0.11803042143583298\n",
      "Epoch 3444, Loss: 0.3991257920861244, Final Batch Loss: 0.13614799082279205\n",
      "Epoch 3445, Loss: 0.43892697244882584, Final Batch Loss: 0.19000905752182007\n",
      "Epoch 3446, Loss: 0.3893878310918808, Final Batch Loss: 0.12317593395709991\n",
      "Epoch 3447, Loss: 0.4312936142086983, Final Batch Loss: 0.09351355582475662\n",
      "Epoch 3448, Loss: 0.46297604218125343, Final Batch Loss: 0.05618497356772423\n",
      "Epoch 3449, Loss: 0.45861655473709106, Final Batch Loss: 0.11140234768390656\n",
      "Epoch 3450, Loss: 0.3998880237340927, Final Batch Loss: 0.0877753496170044\n",
      "Epoch 3451, Loss: 0.49935176968574524, Final Batch Loss: 0.1867382824420929\n",
      "Epoch 3452, Loss: 0.46246782690286636, Final Batch Loss: 0.08072399348020554\n",
      "Epoch 3453, Loss: 0.3696379289031029, Final Batch Loss: 0.1060723140835762\n",
      "Epoch 3454, Loss: 0.39982573688030243, Final Batch Loss: 0.12654174864292145\n",
      "Epoch 3455, Loss: 0.3703143075108528, Final Batch Loss: 0.08683405816555023\n",
      "Epoch 3456, Loss: 0.49047277867794037, Final Batch Loss: 0.15068334341049194\n",
      "Epoch 3457, Loss: 0.3992050364613533, Final Batch Loss: 0.13290034234523773\n",
      "Epoch 3458, Loss: 0.36036011576652527, Final Batch Loss: 0.11085230857133865\n",
      "Epoch 3459, Loss: 0.4581493213772774, Final Batch Loss: 0.19430942833423615\n",
      "Epoch 3460, Loss: 0.46908364444971085, Final Batch Loss: 0.11988107115030289\n",
      "Epoch 3461, Loss: 0.3958292454481125, Final Batch Loss: 0.1074695810675621\n",
      "Epoch 3462, Loss: 0.32966893166303635, Final Batch Loss: 0.07933138310909271\n",
      "Epoch 3463, Loss: 0.5135667026042938, Final Batch Loss: 0.17828351259231567\n",
      "Epoch 3464, Loss: 0.3722734749317169, Final Batch Loss: 0.11096632480621338\n",
      "Epoch 3465, Loss: 0.5135275423526764, Final Batch Loss: 0.17878057062625885\n",
      "Epoch 3466, Loss: 0.4557759165763855, Final Batch Loss: 0.1852932721376419\n",
      "Epoch 3467, Loss: 0.5012747794389725, Final Batch Loss: 0.17353808879852295\n",
      "Epoch 3468, Loss: 0.36896568536758423, Final Batch Loss: 0.09755770862102509\n",
      "Epoch 3469, Loss: 0.4420946091413498, Final Batch Loss: 0.12145604193210602\n",
      "Epoch 3470, Loss: 0.45272351056337357, Final Batch Loss: 0.09633653610944748\n",
      "Epoch 3471, Loss: 0.5280848145484924, Final Batch Loss: 0.25335338711738586\n",
      "Epoch 3472, Loss: 0.3844563141465187, Final Batch Loss: 0.13669562339782715\n",
      "Epoch 3473, Loss: 0.5820396989583969, Final Batch Loss: 0.24605625867843628\n",
      "Epoch 3474, Loss: 0.42383813858032227, Final Batch Loss: 0.12333694100379944\n",
      "Epoch 3475, Loss: 0.48069460690021515, Final Batch Loss: 0.16197437047958374\n",
      "Epoch 3476, Loss: 0.36028748005628586, Final Batch Loss: 0.08222344517707825\n",
      "Epoch 3477, Loss: 0.42913100123405457, Final Batch Loss: 0.09438949823379517\n",
      "Epoch 3478, Loss: 0.3373269587755203, Final Batch Loss: 0.06324683129787445\n",
      "Epoch 3479, Loss: 0.4318050444126129, Final Batch Loss: 0.1101417988538742\n",
      "Epoch 3480, Loss: 0.44172602891921997, Final Batch Loss: 0.13118533790111542\n",
      "Epoch 3481, Loss: 0.5203657746315002, Final Batch Loss: 0.1694934070110321\n",
      "Epoch 3482, Loss: 0.4599009230732918, Final Batch Loss: 0.24865272641181946\n",
      "Epoch 3483, Loss: 0.4448685348033905, Final Batch Loss: 0.17206476628780365\n",
      "Epoch 3484, Loss: 0.42315346747636795, Final Batch Loss: 0.11970854550600052\n",
      "Epoch 3485, Loss: 0.4313320964574814, Final Batch Loss: 0.1737033575773239\n",
      "Epoch 3486, Loss: 0.3751329407095909, Final Batch Loss: 0.10166602581739426\n",
      "Epoch 3487, Loss: 0.42391084134578705, Final Batch Loss: 0.1413474828004837\n",
      "Epoch 3488, Loss: 0.444053515791893, Final Batch Loss: 0.10115651786327362\n",
      "Epoch 3489, Loss: 0.3477788120508194, Final Batch Loss: 0.10120972245931625\n",
      "Epoch 3490, Loss: 0.41228047013282776, Final Batch Loss: 0.13681909441947937\n",
      "Epoch 3491, Loss: 0.47807665169239044, Final Batch Loss: 0.15712565183639526\n",
      "Epoch 3492, Loss: 0.4045911580324173, Final Batch Loss: 0.12035965174436569\n",
      "Epoch 3493, Loss: 0.42491720616817474, Final Batch Loss: 0.16930806636810303\n",
      "Epoch 3494, Loss: 0.4839225709438324, Final Batch Loss: 0.22104443609714508\n",
      "Epoch 3495, Loss: 0.41037363559007645, Final Batch Loss: 0.09224774688482285\n",
      "Epoch 3496, Loss: 0.4252060204744339, Final Batch Loss: 0.1493188440799713\n",
      "Epoch 3497, Loss: 0.38354168087244034, Final Batch Loss: 0.1499636471271515\n",
      "Epoch 3498, Loss: 0.4349162429571152, Final Batch Loss: 0.10585582256317139\n",
      "Epoch 3499, Loss: 0.4359050989151001, Final Batch Loss: 0.14214272797107697\n",
      "Epoch 3500, Loss: 0.3583287298679352, Final Batch Loss: 0.09570254385471344\n",
      "Epoch 3501, Loss: 0.5019496977329254, Final Batch Loss: 0.21098999679088593\n",
      "Epoch 3502, Loss: 0.5562064349651337, Final Batch Loss: 0.235281303524971\n",
      "Epoch 3503, Loss: 0.4676678031682968, Final Batch Loss: 0.1618935912847519\n",
      "Epoch 3504, Loss: 0.39848826080560684, Final Batch Loss: 0.11797221750020981\n",
      "Epoch 3505, Loss: 0.44041819125413895, Final Batch Loss: 0.1107829287648201\n",
      "Epoch 3506, Loss: 0.4201001301407814, Final Batch Loss: 0.17603041231632233\n",
      "Epoch 3507, Loss: 0.504484236240387, Final Batch Loss: 0.13467180728912354\n",
      "Epoch 3508, Loss: 0.37032394856214523, Final Batch Loss: 0.0930926576256752\n",
      "Epoch 3509, Loss: 0.45717060565948486, Final Batch Loss: 0.1299903690814972\n",
      "Epoch 3510, Loss: 0.45603009313344955, Final Batch Loss: 0.15960906445980072\n",
      "Epoch 3511, Loss: 0.5491384863853455, Final Batch Loss: 0.18687491118907928\n",
      "Epoch 3512, Loss: 0.44237256795167923, Final Batch Loss: 0.15924035012722015\n",
      "Epoch 3513, Loss: 0.4496135041117668, Final Batch Loss: 0.13523900508880615\n",
      "Epoch 3514, Loss: 0.3382803276181221, Final Batch Loss: 0.09595433622598648\n",
      "Epoch 3515, Loss: 0.4057522788643837, Final Batch Loss: 0.08068621903657913\n",
      "Epoch 3516, Loss: 0.42605067044496536, Final Batch Loss: 0.12150261551141739\n",
      "Epoch 3517, Loss: 0.3470607399940491, Final Batch Loss: 0.11882119625806808\n",
      "Epoch 3518, Loss: 0.4461791068315506, Final Batch Loss: 0.1364651322364807\n",
      "Epoch 3519, Loss: 0.5225083380937576, Final Batch Loss: 0.14496107399463654\n",
      "Epoch 3520, Loss: 0.46690618991851807, Final Batch Loss: 0.14829331636428833\n",
      "Epoch 3521, Loss: 0.37533895671367645, Final Batch Loss: 0.107545405626297\n",
      "Epoch 3522, Loss: 0.3958081901073456, Final Batch Loss: 0.14506001770496368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3523, Loss: 0.42586129903793335, Final Batch Loss: 0.1359381526708603\n",
      "Epoch 3524, Loss: 0.4467947483062744, Final Batch Loss: 0.12266780436038971\n",
      "Epoch 3525, Loss: 0.3747993931174278, Final Batch Loss: 0.09450729936361313\n",
      "Epoch 3526, Loss: 0.5017170533537865, Final Batch Loss: 0.12008469551801682\n",
      "Epoch 3527, Loss: 0.39418598264455795, Final Batch Loss: 0.16923929750919342\n",
      "Epoch 3528, Loss: 0.4038763791322708, Final Batch Loss: 0.1439438760280609\n",
      "Epoch 3529, Loss: 0.48494309186935425, Final Batch Loss: 0.18359404802322388\n",
      "Epoch 3530, Loss: 0.3632609024643898, Final Batch Loss: 0.14563530683517456\n",
      "Epoch 3531, Loss: 0.4508814290165901, Final Batch Loss: 0.15352120995521545\n",
      "Epoch 3532, Loss: 0.37055984884500504, Final Batch Loss: 0.08372525125741959\n",
      "Epoch 3533, Loss: 0.43835172057151794, Final Batch Loss: 0.10823331773281097\n",
      "Epoch 3534, Loss: 0.4531508684158325, Final Batch Loss: 0.13199618458747864\n",
      "Epoch 3535, Loss: 0.4042595997452736, Final Batch Loss: 0.15333285927772522\n",
      "Epoch 3536, Loss: 0.4402179718017578, Final Batch Loss: 0.09345057606697083\n",
      "Epoch 3537, Loss: 0.4237823709845543, Final Batch Loss: 0.17698077857494354\n",
      "Epoch 3538, Loss: 0.5096927583217621, Final Batch Loss: 0.20290835201740265\n",
      "Epoch 3539, Loss: 0.4013879969716072, Final Batch Loss: 0.11029639095067978\n",
      "Epoch 3540, Loss: 0.38916563242673874, Final Batch Loss: 0.10836180299520493\n",
      "Epoch 3541, Loss: 0.4357624426484108, Final Batch Loss: 0.17790471017360687\n",
      "Epoch 3542, Loss: 0.3763378784060478, Final Batch Loss: 0.07783133536577225\n",
      "Epoch 3543, Loss: 0.4661782532930374, Final Batch Loss: 0.2360409051179886\n",
      "Epoch 3544, Loss: 0.40394701063632965, Final Batch Loss: 0.13743333518505096\n",
      "Epoch 3545, Loss: 0.325551338493824, Final Batch Loss: 0.1268816888332367\n",
      "Epoch 3546, Loss: 0.410000815987587, Final Batch Loss: 0.13445180654525757\n",
      "Epoch 3547, Loss: 0.4330897629261017, Final Batch Loss: 0.1544947773218155\n",
      "Epoch 3548, Loss: 0.36246469616889954, Final Batch Loss: 0.06773857772350311\n",
      "Epoch 3549, Loss: 0.4403502568602562, Final Batch Loss: 0.14364156126976013\n",
      "Epoch 3550, Loss: 0.4422300457954407, Final Batch Loss: 0.14048326015472412\n",
      "Epoch 3551, Loss: 0.38319266587495804, Final Batch Loss: 0.0965474396944046\n",
      "Epoch 3552, Loss: 0.3949829414486885, Final Batch Loss: 0.16135621070861816\n",
      "Epoch 3553, Loss: 0.43031276762485504, Final Batch Loss: 0.13823576271533966\n",
      "Epoch 3554, Loss: 0.3970189765095711, Final Batch Loss: 0.08777272701263428\n",
      "Epoch 3555, Loss: 0.4098694771528244, Final Batch Loss: 0.07283905148506165\n",
      "Epoch 3556, Loss: 0.49558253586292267, Final Batch Loss: 0.1402388960123062\n",
      "Epoch 3557, Loss: 0.49330152571201324, Final Batch Loss: 0.16643846035003662\n",
      "Epoch 3558, Loss: 0.4104730114340782, Final Batch Loss: 0.17573970556259155\n",
      "Epoch 3559, Loss: 0.4194643422961235, Final Batch Loss: 0.08811549097299576\n",
      "Epoch 3560, Loss: 0.44077345728874207, Final Batch Loss: 0.14495395123958588\n",
      "Epoch 3561, Loss: 0.46258819103240967, Final Batch Loss: 0.14677976071834564\n",
      "Epoch 3562, Loss: 0.5250946283340454, Final Batch Loss: 0.1962776929140091\n",
      "Epoch 3563, Loss: 0.4359697997570038, Final Batch Loss: 0.16945254802703857\n",
      "Epoch 3564, Loss: 0.4254268929362297, Final Batch Loss: 0.133875772356987\n",
      "Epoch 3565, Loss: 0.45617295056581497, Final Batch Loss: 0.1839706003665924\n",
      "Epoch 3566, Loss: 0.4828629717230797, Final Batch Loss: 0.14953607320785522\n",
      "Epoch 3567, Loss: 0.4833495616912842, Final Batch Loss: 0.161741703748703\n",
      "Epoch 3568, Loss: 0.4922209084033966, Final Batch Loss: 0.12355825304985046\n",
      "Epoch 3569, Loss: 0.43399836122989655, Final Batch Loss: 0.18755821883678436\n",
      "Epoch 3570, Loss: 0.2931613549590111, Final Batch Loss: 0.0774424821138382\n",
      "Epoch 3571, Loss: 0.417197085916996, Final Batch Loss: 0.09389438480138779\n",
      "Epoch 3572, Loss: 0.4495651498436928, Final Batch Loss: 0.2110394835472107\n",
      "Epoch 3573, Loss: 0.5596231073141098, Final Batch Loss: 0.19262531399726868\n",
      "Epoch 3574, Loss: 0.46381381154060364, Final Batch Loss: 0.15206378698349\n",
      "Epoch 3575, Loss: 0.415973424911499, Final Batch Loss: 0.16560006141662598\n",
      "Epoch 3576, Loss: 0.3978300243616104, Final Batch Loss: 0.12388323992490768\n",
      "Epoch 3577, Loss: 0.4916199743747711, Final Batch Loss: 0.1747230738401413\n",
      "Epoch 3578, Loss: 0.42324621975421906, Final Batch Loss: 0.09458792954683304\n",
      "Epoch 3579, Loss: 0.35119348019361496, Final Batch Loss: 0.10752648860216141\n",
      "Epoch 3580, Loss: 0.4102295711636543, Final Batch Loss: 0.11943023651838303\n",
      "Epoch 3581, Loss: 0.3608528897166252, Final Batch Loss: 0.11660022288560867\n",
      "Epoch 3582, Loss: 0.42076001316308975, Final Batch Loss: 0.09822254627943039\n",
      "Epoch 3583, Loss: 0.3463006690144539, Final Batch Loss: 0.11750584095716476\n",
      "Epoch 3584, Loss: 0.4148673489689827, Final Batch Loss: 0.16948653757572174\n",
      "Epoch 3585, Loss: 0.5294690281152725, Final Batch Loss: 0.20069138705730438\n",
      "Epoch 3586, Loss: 0.4512142837047577, Final Batch Loss: 0.11264391243457794\n",
      "Epoch 3587, Loss: 0.4580518454313278, Final Batch Loss: 0.12473760545253754\n",
      "Epoch 3588, Loss: 0.3845263943076134, Final Batch Loss: 0.11812412738800049\n",
      "Epoch 3589, Loss: 0.40970735251903534, Final Batch Loss: 0.13060350716114044\n",
      "Epoch 3590, Loss: 0.48945821076631546, Final Batch Loss: 0.2141023427248001\n",
      "Epoch 3591, Loss: 0.46018463373184204, Final Batch Loss: 0.1744048148393631\n",
      "Epoch 3592, Loss: 0.5160801187157631, Final Batch Loss: 0.11352989822626114\n",
      "Epoch 3593, Loss: 0.4182977229356766, Final Batch Loss: 0.1255975216627121\n",
      "Epoch 3594, Loss: 0.4364434480667114, Final Batch Loss: 0.15623509883880615\n",
      "Epoch 3595, Loss: 0.4565265327692032, Final Batch Loss: 0.08221971988677979\n",
      "Epoch 3596, Loss: 0.47635769098997116, Final Batch Loss: 0.1058836504817009\n",
      "Epoch 3597, Loss: 0.41671322286129, Final Batch Loss: 0.13676966726779938\n",
      "Epoch 3598, Loss: 0.4564077630639076, Final Batch Loss: 0.0813271626830101\n",
      "Epoch 3599, Loss: 0.46270133554935455, Final Batch Loss: 0.14839163422584534\n",
      "Epoch 3600, Loss: 0.41743485629558563, Final Batch Loss: 0.140236034989357\n",
      "Epoch 3601, Loss: 0.4341208338737488, Final Batch Loss: 0.1405181735754013\n",
      "Epoch 3602, Loss: 0.40142184495925903, Final Batch Loss: 0.14408142864704132\n",
      "Epoch 3603, Loss: 0.46869969367980957, Final Batch Loss: 0.1825154721736908\n",
      "Epoch 3604, Loss: 0.4217199757695198, Final Batch Loss: 0.11592302471399307\n",
      "Epoch 3605, Loss: 0.34762462973594666, Final Batch Loss: 0.06274793297052383\n",
      "Epoch 3606, Loss: 0.36021433770656586, Final Batch Loss: 0.08168082684278488\n",
      "Epoch 3607, Loss: 0.4714103490114212, Final Batch Loss: 0.10695505142211914\n",
      "Epoch 3608, Loss: 0.424507200717926, Final Batch Loss: 0.09935206174850464\n",
      "Epoch 3609, Loss: 0.3551015704870224, Final Batch Loss: 0.10603129118680954\n",
      "Epoch 3610, Loss: 0.3773225173354149, Final Batch Loss: 0.1322816163301468\n",
      "Epoch 3611, Loss: 0.4497236907482147, Final Batch Loss: 0.15908800065517426\n",
      "Epoch 3612, Loss: 0.4774867445230484, Final Batch Loss: 0.19899894297122955\n",
      "Epoch 3613, Loss: 0.4232621490955353, Final Batch Loss: 0.17455078661441803\n",
      "Epoch 3614, Loss: 0.5262363851070404, Final Batch Loss: 0.18236540257930756\n",
      "Epoch 3615, Loss: 0.6225666552782059, Final Batch Loss: 0.1532958745956421\n",
      "Epoch 3616, Loss: 0.43869737535715103, Final Batch Loss: 0.1224498525261879\n",
      "Epoch 3617, Loss: 0.4665812849998474, Final Batch Loss: 0.12544560432434082\n",
      "Epoch 3618, Loss: 0.4374838173389435, Final Batch Loss: 0.14969561994075775\n",
      "Epoch 3619, Loss: 0.478375107049942, Final Batch Loss: 0.17884579300880432\n",
      "Epoch 3620, Loss: 0.5041098222136497, Final Batch Loss: 0.1823178082704544\n",
      "Epoch 3621, Loss: 0.49504654854536057, Final Batch Loss: 0.2103915810585022\n",
      "Epoch 3622, Loss: 0.372354693710804, Final Batch Loss: 0.08390987664461136\n",
      "Epoch 3623, Loss: 0.4484686106443405, Final Batch Loss: 0.14801830053329468\n",
      "Epoch 3624, Loss: 0.47194574773311615, Final Batch Loss: 0.1595405787229538\n",
      "Epoch 3625, Loss: 0.4536961019039154, Final Batch Loss: 0.10803696513175964\n",
      "Epoch 3626, Loss: 0.4852941781282425, Final Batch Loss: 0.18454979360103607\n",
      "Epoch 3627, Loss: 0.3953336328268051, Final Batch Loss: 0.11063957214355469\n",
      "Epoch 3628, Loss: 0.45600199699401855, Final Batch Loss: 0.1667465716600418\n",
      "Epoch 3629, Loss: 0.5113303065299988, Final Batch Loss: 0.19578930735588074\n",
      "Epoch 3630, Loss: 0.40643151849508286, Final Batch Loss: 0.1669706404209137\n",
      "Epoch 3631, Loss: 0.46242430806159973, Final Batch Loss: 0.12815254926681519\n",
      "Epoch 3632, Loss: 0.5486573874950409, Final Batch Loss: 0.20043276250362396\n",
      "Epoch 3633, Loss: 0.4784824699163437, Final Batch Loss: 0.1705136001110077\n",
      "Epoch 3634, Loss: 0.4195489138364792, Final Batch Loss: 0.14491593837738037\n",
      "Epoch 3635, Loss: 0.41006044298410416, Final Batch Loss: 0.1569984406232834\n",
      "Epoch 3636, Loss: 0.5434297472238541, Final Batch Loss: 0.18536821007728577\n",
      "Epoch 3637, Loss: 0.450094610452652, Final Batch Loss: 0.15193969011306763\n",
      "Epoch 3638, Loss: 0.391488678753376, Final Batch Loss: 0.16887040436267853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3639, Loss: 0.3268756791949272, Final Batch Loss: 0.08667982369661331\n",
      "Epoch 3640, Loss: 0.5174207091331482, Final Batch Loss: 0.20429548621177673\n",
      "Epoch 3641, Loss: 0.4026281014084816, Final Batch Loss: 0.11950499564409256\n",
      "Epoch 3642, Loss: 0.3893241658806801, Final Batch Loss: 0.11190781742334366\n",
      "Epoch 3643, Loss: 0.3744087666273117, Final Batch Loss: 0.13695980608463287\n",
      "Epoch 3644, Loss: 0.400802880525589, Final Batch Loss: 0.13294003903865814\n",
      "Epoch 3645, Loss: 0.34565655142068863, Final Batch Loss: 0.10452617704868317\n",
      "Epoch 3646, Loss: 0.434508353471756, Final Batch Loss: 0.1251942217350006\n",
      "Epoch 3647, Loss: 0.37209029495716095, Final Batch Loss: 0.06738312542438507\n",
      "Epoch 3648, Loss: 0.5652962774038315, Final Batch Loss: 0.15869154036045074\n",
      "Epoch 3649, Loss: 0.4440072700381279, Final Batch Loss: 0.10156957060098648\n",
      "Epoch 3650, Loss: 0.42790767550468445, Final Batch Loss: 0.10891048610210419\n",
      "Epoch 3651, Loss: 0.5198312550783157, Final Batch Loss: 0.17694079875946045\n",
      "Epoch 3652, Loss: 0.44964150339365005, Final Batch Loss: 0.21498283743858337\n",
      "Epoch 3653, Loss: 0.42235617339611053, Final Batch Loss: 0.1321869194507599\n",
      "Epoch 3654, Loss: 0.43223971128463745, Final Batch Loss: 0.13636967539787292\n",
      "Epoch 3655, Loss: 0.41126590967178345, Final Batch Loss: 0.1462249904870987\n",
      "Epoch 3656, Loss: 0.3959256485104561, Final Batch Loss: 0.12123531848192215\n",
      "Epoch 3657, Loss: 0.38405942916870117, Final Batch Loss: 0.1148596704006195\n",
      "Epoch 3658, Loss: 0.4581884890794754, Final Batch Loss: 0.1516561657190323\n",
      "Epoch 3659, Loss: 0.38462477177381516, Final Batch Loss: 0.16661575436592102\n",
      "Epoch 3660, Loss: 0.4714202582836151, Final Batch Loss: 0.1812792271375656\n",
      "Epoch 3661, Loss: 0.4497279226779938, Final Batch Loss: 0.15483754873275757\n",
      "Epoch 3662, Loss: 0.3855990394949913, Final Batch Loss: 0.14569686353206635\n",
      "Epoch 3663, Loss: 0.43695298582315445, Final Batch Loss: 0.20215117931365967\n",
      "Epoch 3664, Loss: 0.5104281306266785, Final Batch Loss: 0.19805777072906494\n",
      "Epoch 3665, Loss: 0.44830240309238434, Final Batch Loss: 0.16597771644592285\n",
      "Epoch 3666, Loss: 0.4732823222875595, Final Batch Loss: 0.13705070316791534\n",
      "Epoch 3667, Loss: 0.46623391658067703, Final Batch Loss: 0.10147447139024734\n",
      "Epoch 3668, Loss: 0.3754119798541069, Final Batch Loss: 0.12327868491411209\n",
      "Epoch 3669, Loss: 0.4756770730018616, Final Batch Loss: 0.17772376537322998\n",
      "Epoch 3670, Loss: 0.4418167769908905, Final Batch Loss: 0.16493691504001617\n",
      "Epoch 3671, Loss: 0.5059131979942322, Final Batch Loss: 0.22371649742126465\n",
      "Epoch 3672, Loss: 0.4601436108350754, Final Batch Loss: 0.1374611258506775\n",
      "Epoch 3673, Loss: 0.5349456369876862, Final Batch Loss: 0.22186614573001862\n",
      "Epoch 3674, Loss: 0.4289761856198311, Final Batch Loss: 0.11338728666305542\n",
      "Epoch 3675, Loss: 0.39789576828479767, Final Batch Loss: 0.12163738161325455\n",
      "Epoch 3676, Loss: 0.47376875579357147, Final Batch Loss: 0.13039499521255493\n",
      "Epoch 3677, Loss: 0.4976145327091217, Final Batch Loss: 0.14932125806808472\n",
      "Epoch 3678, Loss: 0.36670784652233124, Final Batch Loss: 0.10201235115528107\n",
      "Epoch 3679, Loss: 0.3334031254053116, Final Batch Loss: 0.12338068336248398\n",
      "Epoch 3680, Loss: 0.4026021510362625, Final Batch Loss: 0.17376984655857086\n",
      "Epoch 3681, Loss: 0.4359581544995308, Final Batch Loss: 0.13007806241512299\n",
      "Epoch 3682, Loss: 0.4627703130245209, Final Batch Loss: 0.14410708844661713\n",
      "Epoch 3683, Loss: 0.43731608986854553, Final Batch Loss: 0.11586886644363403\n",
      "Epoch 3684, Loss: 0.4520304501056671, Final Batch Loss: 0.13868992030620575\n",
      "Epoch 3685, Loss: 0.5054176598787308, Final Batch Loss: 0.19789952039718628\n",
      "Epoch 3686, Loss: 0.4278038591146469, Final Batch Loss: 0.2083556354045868\n",
      "Epoch 3687, Loss: 0.4199470579624176, Final Batch Loss: 0.11037738621234894\n",
      "Epoch 3688, Loss: 0.3615137115120888, Final Batch Loss: 0.08896656334400177\n",
      "Epoch 3689, Loss: 0.3994591757655144, Final Batch Loss: 0.13139744102954865\n",
      "Epoch 3690, Loss: 0.3660096004605293, Final Batch Loss: 0.13198994100093842\n",
      "Epoch 3691, Loss: 0.379163034260273, Final Batch Loss: 0.14440731704235077\n",
      "Epoch 3692, Loss: 0.5754207968711853, Final Batch Loss: 0.2744387984275818\n",
      "Epoch 3693, Loss: 0.41281772404909134, Final Batch Loss: 0.1747715175151825\n",
      "Epoch 3694, Loss: 0.45795856416225433, Final Batch Loss: 0.14820612967014313\n",
      "Epoch 3695, Loss: 0.43830911815166473, Final Batch Loss: 0.15665340423583984\n",
      "Epoch 3696, Loss: 0.4749828726053238, Final Batch Loss: 0.2021891325712204\n",
      "Epoch 3697, Loss: 0.4213942885398865, Final Batch Loss: 0.10538609325885773\n",
      "Epoch 3698, Loss: 0.3679977208375931, Final Batch Loss: 0.09723697602748871\n",
      "Epoch 3699, Loss: 0.3940936401486397, Final Batch Loss: 0.13885970413684845\n",
      "Epoch 3700, Loss: 0.41979527473449707, Final Batch Loss: 0.14017952978610992\n",
      "Epoch 3701, Loss: 0.48410729318857193, Final Batch Loss: 0.07641924172639847\n",
      "Epoch 3702, Loss: 0.4591974541544914, Final Batch Loss: 0.20008473098278046\n",
      "Epoch 3703, Loss: 0.46610313653945923, Final Batch Loss: 0.17541714012622833\n",
      "Epoch 3704, Loss: 0.49970151484012604, Final Batch Loss: 0.13452118635177612\n",
      "Epoch 3705, Loss: 0.4191691502928734, Final Batch Loss: 0.15991558134555817\n",
      "Epoch 3706, Loss: 0.3988681510090828, Final Batch Loss: 0.07665462046861649\n",
      "Epoch 3707, Loss: 0.41610677540302277, Final Batch Loss: 0.13407553732395172\n",
      "Epoch 3708, Loss: 0.3986367955803871, Final Batch Loss: 0.10847776383161545\n",
      "Epoch 3709, Loss: 0.44587454199790955, Final Batch Loss: 0.17674382030963898\n",
      "Epoch 3710, Loss: 0.41773588955402374, Final Batch Loss: 0.10494105517864227\n",
      "Epoch 3711, Loss: 0.46957410126924515, Final Batch Loss: 0.12020391970872879\n",
      "Epoch 3712, Loss: 0.37661968171596527, Final Batch Loss: 0.07977984845638275\n",
      "Epoch 3713, Loss: 0.34547919034957886, Final Batch Loss: 0.13946634531021118\n",
      "Epoch 3714, Loss: 0.3888220191001892, Final Batch Loss: 0.1628858596086502\n",
      "Epoch 3715, Loss: 0.41868725419044495, Final Batch Loss: 0.14821100234985352\n",
      "Epoch 3716, Loss: 0.3710052967071533, Final Batch Loss: 0.14092369377613068\n",
      "Epoch 3717, Loss: 0.446536049246788, Final Batch Loss: 0.15430495142936707\n",
      "Epoch 3718, Loss: 0.4133181795477867, Final Batch Loss: 0.09783577173948288\n",
      "Epoch 3719, Loss: 0.46901215612888336, Final Batch Loss: 0.15541993081569672\n",
      "Epoch 3720, Loss: 0.39540335536003113, Final Batch Loss: 0.1290661245584488\n",
      "Epoch 3721, Loss: 0.40017424523830414, Final Batch Loss: 0.15637385845184326\n",
      "Epoch 3722, Loss: 0.4206491559743881, Final Batch Loss: 0.12295565009117126\n",
      "Epoch 3723, Loss: 0.4540432542562485, Final Batch Loss: 0.15051773190498352\n",
      "Epoch 3724, Loss: 0.36254842579364777, Final Batch Loss: 0.09136026352643967\n",
      "Epoch 3725, Loss: 0.36870044469833374, Final Batch Loss: 0.11854854971170425\n",
      "Epoch 3726, Loss: 0.39983924478292465, Final Batch Loss: 0.1284024715423584\n",
      "Epoch 3727, Loss: 0.3722062259912491, Final Batch Loss: 0.14022523164749146\n",
      "Epoch 3728, Loss: 0.3815997615456581, Final Batch Loss: 0.1179555207490921\n",
      "Epoch 3729, Loss: 0.44066617637872696, Final Batch Loss: 0.2289530336856842\n",
      "Epoch 3730, Loss: 0.45542268455028534, Final Batch Loss: 0.23884977400302887\n",
      "Epoch 3731, Loss: 0.36644768714904785, Final Batch Loss: 0.09593755006790161\n",
      "Epoch 3732, Loss: 0.3998260572552681, Final Batch Loss: 0.12372028827667236\n",
      "Epoch 3733, Loss: 0.5111124068498611, Final Batch Loss: 0.1479543000459671\n",
      "Epoch 3734, Loss: 0.571581244468689, Final Batch Loss: 0.16501384973526\n",
      "Epoch 3735, Loss: 0.5252409130334854, Final Batch Loss: 0.15861953794956207\n",
      "Epoch 3736, Loss: 0.47769100964069366, Final Batch Loss: 0.1484786421060562\n",
      "Epoch 3737, Loss: 0.4913415014743805, Final Batch Loss: 0.19580191373825073\n",
      "Epoch 3738, Loss: 0.37554270774126053, Final Batch Loss: 0.12013043463230133\n",
      "Epoch 3739, Loss: 0.4717298746109009, Final Batch Loss: 0.18594785034656525\n",
      "Epoch 3740, Loss: 0.4616662859916687, Final Batch Loss: 0.1467333883047104\n",
      "Epoch 3741, Loss: 0.382677398622036, Final Batch Loss: 0.15092535316944122\n",
      "Epoch 3742, Loss: 0.5083582177758217, Final Batch Loss: 0.20010071992874146\n",
      "Epoch 3743, Loss: 0.47861336171627045, Final Batch Loss: 0.15066976845264435\n",
      "Epoch 3744, Loss: 0.37089288234710693, Final Batch Loss: 0.15784549713134766\n",
      "Epoch 3745, Loss: 0.3198956996202469, Final Batch Loss: 0.08952774107456207\n",
      "Epoch 3746, Loss: 0.530961386859417, Final Batch Loss: 0.21847140789031982\n",
      "Epoch 3747, Loss: 0.4541070908308029, Final Batch Loss: 0.15517258644104004\n",
      "Epoch 3748, Loss: 0.38974685966968536, Final Batch Loss: 0.10134360939264297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3749, Loss: 0.37089522927999496, Final Batch Loss: 0.11900854110717773\n",
      "Epoch 3750, Loss: 0.5036512762308121, Final Batch Loss: 0.27040809392929077\n",
      "Epoch 3751, Loss: 0.41673239320516586, Final Batch Loss: 0.15908314287662506\n",
      "Epoch 3752, Loss: 0.45710454881191254, Final Batch Loss: 0.1292116791009903\n",
      "Epoch 3753, Loss: 0.5907868444919586, Final Batch Loss: 0.14006014168262482\n",
      "Epoch 3754, Loss: 0.3617497459053993, Final Batch Loss: 0.11699111014604568\n",
      "Epoch 3755, Loss: 0.3816226124763489, Final Batch Loss: 0.12819847464561462\n",
      "Epoch 3756, Loss: 0.4799385294318199, Final Batch Loss: 0.091879703104496\n",
      "Epoch 3757, Loss: 0.5386538133025169, Final Batch Loss: 0.25236350297927856\n",
      "Epoch 3758, Loss: 0.3628328666090965, Final Batch Loss: 0.094052754342556\n",
      "Epoch 3759, Loss: 0.44456829130649567, Final Batch Loss: 0.1813381016254425\n",
      "Epoch 3760, Loss: 0.3996935114264488, Final Batch Loss: 0.12292859703302383\n",
      "Epoch 3761, Loss: 0.43476031720638275, Final Batch Loss: 0.15412914752960205\n",
      "Epoch 3762, Loss: 0.38853420317173004, Final Batch Loss: 0.14803744852542877\n",
      "Epoch 3763, Loss: 0.394980788230896, Final Batch Loss: 0.11431267857551575\n",
      "Epoch 3764, Loss: 0.4112129509449005, Final Batch Loss: 0.13803218305110931\n",
      "Epoch 3765, Loss: 0.4399169385433197, Final Batch Loss: 0.1408708393573761\n",
      "Epoch 3766, Loss: 0.35854481160640717, Final Batch Loss: 0.08830058574676514\n",
      "Epoch 3767, Loss: 0.4444963112473488, Final Batch Loss: 0.09851012378931046\n",
      "Epoch 3768, Loss: 0.47246164828538895, Final Batch Loss: 0.20095054805278778\n",
      "Epoch 3769, Loss: 0.41617055237293243, Final Batch Loss: 0.1395123302936554\n",
      "Epoch 3770, Loss: 0.5768991708755493, Final Batch Loss: 0.18544787168502808\n",
      "Epoch 3771, Loss: 0.40241652727127075, Final Batch Loss: 0.16288603842258453\n",
      "Epoch 3772, Loss: 0.38577038049697876, Final Batch Loss: 0.10707508027553558\n",
      "Epoch 3773, Loss: 0.42414798587560654, Final Batch Loss: 0.11817040294408798\n",
      "Epoch 3774, Loss: 0.3961479738354683, Final Batch Loss: 0.09647905081510544\n",
      "Epoch 3775, Loss: 0.42618653923273087, Final Batch Loss: 0.10993409901857376\n",
      "Epoch 3776, Loss: 0.36733198910951614, Final Batch Loss: 0.16564932465553284\n",
      "Epoch 3777, Loss: 0.5309472754597664, Final Batch Loss: 0.24577035009860992\n",
      "Epoch 3778, Loss: 0.4083855599164963, Final Batch Loss: 0.12104795128107071\n",
      "Epoch 3779, Loss: 0.4704151004552841, Final Batch Loss: 0.14753778278827667\n",
      "Epoch 3780, Loss: 0.4638126492500305, Final Batch Loss: 0.23127377033233643\n",
      "Epoch 3781, Loss: 0.4101206138730049, Final Batch Loss: 0.13274386525154114\n",
      "Epoch 3782, Loss: 0.43823350220918655, Final Batch Loss: 0.17496542632579803\n",
      "Epoch 3783, Loss: 0.3590311333537102, Final Batch Loss: 0.11090008169412613\n",
      "Epoch 3784, Loss: 0.3932531923055649, Final Batch Loss: 0.13932164013385773\n",
      "Epoch 3785, Loss: 0.4634603336453438, Final Batch Loss: 0.08672048896551132\n",
      "Epoch 3786, Loss: 0.3832012191414833, Final Batch Loss: 0.11977963149547577\n",
      "Epoch 3787, Loss: 0.41166795045137405, Final Batch Loss: 0.18990570306777954\n",
      "Epoch 3788, Loss: 0.5353603586554527, Final Batch Loss: 0.22446437180042267\n",
      "Epoch 3789, Loss: 0.35161667689681053, Final Batch Loss: 0.062044259160757065\n",
      "Epoch 3790, Loss: 0.42259738594293594, Final Batch Loss: 0.13956142961978912\n",
      "Epoch 3791, Loss: 0.45960529148578644, Final Batch Loss: 0.13467508554458618\n",
      "Epoch 3792, Loss: 0.3282661959528923, Final Batch Loss: 0.09498739242553711\n",
      "Epoch 3793, Loss: 0.4226609095931053, Final Batch Loss: 0.12151982635259628\n",
      "Epoch 3794, Loss: 0.43694327771663666, Final Batch Loss: 0.14114795625209808\n",
      "Epoch 3795, Loss: 0.40213924646377563, Final Batch Loss: 0.11897726356983185\n",
      "Epoch 3796, Loss: 0.4541186988353729, Final Batch Loss: 0.18106234073638916\n",
      "Epoch 3797, Loss: 0.4695761054754257, Final Batch Loss: 0.09374868869781494\n",
      "Epoch 3798, Loss: 0.380935475230217, Final Batch Loss: 0.1129399761557579\n",
      "Epoch 3799, Loss: 0.38171765208244324, Final Batch Loss: 0.10180473327636719\n",
      "Epoch 3800, Loss: 0.37656065821647644, Final Batch Loss: 0.13580088317394257\n",
      "Epoch 3801, Loss: 0.465490460395813, Final Batch Loss: 0.15428130328655243\n",
      "Epoch 3802, Loss: 0.37173209339380264, Final Batch Loss: 0.13152723014354706\n",
      "Epoch 3803, Loss: 0.3970289006829262, Final Batch Loss: 0.11796595901250839\n",
      "Epoch 3804, Loss: 0.4734594076871872, Final Batch Loss: 0.14374196529388428\n",
      "Epoch 3805, Loss: 0.3388852998614311, Final Batch Loss: 0.07908574491739273\n",
      "Epoch 3806, Loss: 0.44811971485614777, Final Batch Loss: 0.12775501608848572\n",
      "Epoch 3807, Loss: 0.35407647490501404, Final Batch Loss: 0.10246586054563522\n",
      "Epoch 3808, Loss: 0.3614351153373718, Final Batch Loss: 0.11895020306110382\n",
      "Epoch 3809, Loss: 0.3757685124874115, Final Batch Loss: 0.11141069233417511\n",
      "Epoch 3810, Loss: 0.46696658432483673, Final Batch Loss: 0.12511879205703735\n",
      "Epoch 3811, Loss: 0.42156194150447845, Final Batch Loss: 0.08757529407739639\n",
      "Epoch 3812, Loss: 0.40301014482975006, Final Batch Loss: 0.10891077667474747\n",
      "Epoch 3813, Loss: 0.3508666604757309, Final Batch Loss: 0.11613485217094421\n",
      "Epoch 3814, Loss: 0.6554482132196426, Final Batch Loss: 0.1460394561290741\n",
      "Epoch 3815, Loss: 0.3601860925555229, Final Batch Loss: 0.051084838807582855\n",
      "Epoch 3816, Loss: 0.3695315942168236, Final Batch Loss: 0.14966176450252533\n",
      "Epoch 3817, Loss: 0.5275920033454895, Final Batch Loss: 0.20739181339740753\n",
      "Epoch 3818, Loss: 0.47585727274417877, Final Batch Loss: 0.19001184403896332\n",
      "Epoch 3819, Loss: 0.4168124496936798, Final Batch Loss: 0.19374065101146698\n",
      "Epoch 3820, Loss: 0.5601427108049393, Final Batch Loss: 0.16249486804008484\n",
      "Epoch 3821, Loss: 0.4208894446492195, Final Batch Loss: 0.12207398563623428\n",
      "Epoch 3822, Loss: 0.42313581705093384, Final Batch Loss: 0.1257040947675705\n",
      "Epoch 3823, Loss: 0.3965928703546524, Final Batch Loss: 0.13575689494609833\n",
      "Epoch 3824, Loss: 0.3591427654027939, Final Batch Loss: 0.10621611773967743\n",
      "Epoch 3825, Loss: 0.4077286943793297, Final Batch Loss: 0.10237792134284973\n",
      "Epoch 3826, Loss: 0.40567514300346375, Final Batch Loss: 0.11457576602697372\n",
      "Epoch 3827, Loss: 0.3945206478238106, Final Batch Loss: 0.12158685177564621\n",
      "Epoch 3828, Loss: 0.36378271877765656, Final Batch Loss: 0.119051493704319\n",
      "Epoch 3829, Loss: 0.46422456204891205, Final Batch Loss: 0.1557389199733734\n",
      "Epoch 3830, Loss: 0.4250619485974312, Final Batch Loss: 0.12031660228967667\n",
      "Epoch 3831, Loss: 0.3967079371213913, Final Batch Loss: 0.13755539059638977\n",
      "Epoch 3832, Loss: 0.43403448164463043, Final Batch Loss: 0.14470814168453217\n",
      "Epoch 3833, Loss: 0.40010593831539154, Final Batch Loss: 0.14591968059539795\n",
      "Epoch 3834, Loss: 0.38582126051187515, Final Batch Loss: 0.13392546772956848\n",
      "Epoch 3835, Loss: 0.41107623279094696, Final Batch Loss: 0.10901974141597748\n",
      "Epoch 3836, Loss: 0.4142667353153229, Final Batch Loss: 0.13732948899269104\n",
      "Epoch 3837, Loss: 0.3747333809733391, Final Batch Loss: 0.09378829598426819\n",
      "Epoch 3838, Loss: 0.4992547035217285, Final Batch Loss: 0.15698495507240295\n",
      "Epoch 3839, Loss: 0.41233765333890915, Final Batch Loss: 0.16400136053562164\n",
      "Epoch 3840, Loss: 0.39171629399061203, Final Batch Loss: 0.1542779952287674\n",
      "Epoch 3841, Loss: 0.4212217628955841, Final Batch Loss: 0.15593580901622772\n",
      "Epoch 3842, Loss: 0.5151689052581787, Final Batch Loss: 0.20766334235668182\n",
      "Epoch 3843, Loss: 0.64491967856884, Final Batch Loss: 0.20828691124916077\n",
      "Epoch 3844, Loss: 0.3609186187386513, Final Batch Loss: 0.14309601485729218\n",
      "Epoch 3845, Loss: 0.5307508856058121, Final Batch Loss: 0.16252140700817108\n",
      "Epoch 3846, Loss: 0.434132844209671, Final Batch Loss: 0.17444837093353271\n",
      "Epoch 3847, Loss: 0.31902069598436356, Final Batch Loss: 0.08831612020730972\n",
      "Epoch 3848, Loss: 0.38378380239009857, Final Batch Loss: 0.13233444094657898\n",
      "Epoch 3849, Loss: 0.3865218535065651, Final Batch Loss: 0.18876294791698456\n",
      "Epoch 3850, Loss: 0.46423642337322235, Final Batch Loss: 0.1347014605998993\n",
      "Epoch 3851, Loss: 0.45716480910778046, Final Batch Loss: 0.1640268862247467\n",
      "Epoch 3852, Loss: 0.36107269674539566, Final Batch Loss: 0.09176314622163773\n",
      "Epoch 3853, Loss: 0.4478243812918663, Final Batch Loss: 0.15877509117126465\n",
      "Epoch 3854, Loss: 0.3387269079685211, Final Batch Loss: 0.11180892586708069\n",
      "Epoch 3855, Loss: 0.41989199817180634, Final Batch Loss: 0.09830206632614136\n",
      "Epoch 3856, Loss: 0.33145616948604584, Final Batch Loss: 0.11084531992673874\n",
      "Epoch 3857, Loss: 0.5345505177974701, Final Batch Loss: 0.1434279829263687\n",
      "Epoch 3858, Loss: 0.3059685304760933, Final Batch Loss: 0.09339891374111176\n",
      "Epoch 3859, Loss: 0.3903835713863373, Final Batch Loss: 0.1872662454843521\n",
      "Epoch 3860, Loss: 0.40850670635700226, Final Batch Loss: 0.1520310491323471\n",
      "Epoch 3861, Loss: 0.3932751715183258, Final Batch Loss: 0.1198585033416748\n",
      "Epoch 3862, Loss: 0.5101668536663055, Final Batch Loss: 0.1649298071861267\n",
      "Epoch 3863, Loss: 0.41124188154935837, Final Batch Loss: 0.2065674066543579\n",
      "Epoch 3864, Loss: 0.4246159642934799, Final Batch Loss: 0.15007926523685455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3865, Loss: 0.47639200091362, Final Batch Loss: 0.11637851595878601\n",
      "Epoch 3866, Loss: 0.41789180040359497, Final Batch Loss: 0.12676072120666504\n",
      "Epoch 3867, Loss: 0.3889887407422066, Final Batch Loss: 0.17450770735740662\n",
      "Epoch 3868, Loss: 0.46695221960544586, Final Batch Loss: 0.16540460288524628\n",
      "Epoch 3869, Loss: 0.4407508224248886, Final Batch Loss: 0.15947306156158447\n",
      "Epoch 3870, Loss: 0.4428121745586395, Final Batch Loss: 0.14580370485782623\n",
      "Epoch 3871, Loss: 0.41301439702510834, Final Batch Loss: 0.08293315768241882\n",
      "Epoch 3872, Loss: 0.43005186319351196, Final Batch Loss: 0.18273292481899261\n",
      "Epoch 3873, Loss: 0.3756731301546097, Final Batch Loss: 0.12796097993850708\n",
      "Epoch 3874, Loss: 0.40397343784570694, Final Batch Loss: 0.10275537520647049\n",
      "Epoch 3875, Loss: 0.4089756980538368, Final Batch Loss: 0.10484699159860611\n",
      "Epoch 3876, Loss: 0.48579712212085724, Final Batch Loss: 0.14486920833587646\n",
      "Epoch 3877, Loss: 0.3782476708292961, Final Batch Loss: 0.07018808275461197\n",
      "Epoch 3878, Loss: 0.4788765609264374, Final Batch Loss: 0.19843561947345734\n",
      "Epoch 3879, Loss: 0.37620995938777924, Final Batch Loss: 0.14170518517494202\n",
      "Epoch 3880, Loss: 0.3273437172174454, Final Batch Loss: 0.09292984753847122\n",
      "Epoch 3881, Loss: 0.44727324694395065, Final Batch Loss: 0.1543598175048828\n",
      "Epoch 3882, Loss: 0.3511756807565689, Final Batch Loss: 0.10265448689460754\n",
      "Epoch 3883, Loss: 0.4099683091044426, Final Batch Loss: 0.16287115216255188\n",
      "Epoch 3884, Loss: 0.37147363275289536, Final Batch Loss: 0.1397857367992401\n",
      "Epoch 3885, Loss: 0.40007008612155914, Final Batch Loss: 0.13087907433509827\n",
      "Epoch 3886, Loss: 0.4540921300649643, Final Batch Loss: 0.14632675051689148\n",
      "Epoch 3887, Loss: 0.3198602944612503, Final Batch Loss: 0.11132705956697464\n",
      "Epoch 3888, Loss: 0.36781052500009537, Final Batch Loss: 0.08841634541749954\n",
      "Epoch 3889, Loss: 0.3843447342514992, Final Batch Loss: 0.1145896315574646\n",
      "Epoch 3890, Loss: 0.41862769424915314, Final Batch Loss: 0.1461678296327591\n",
      "Epoch 3891, Loss: 0.37438905239105225, Final Batch Loss: 0.16454185545444489\n",
      "Epoch 3892, Loss: 0.49337922036647797, Final Batch Loss: 0.24053257703781128\n",
      "Epoch 3893, Loss: 0.43660667538642883, Final Batch Loss: 0.13373927772045135\n",
      "Epoch 3894, Loss: 0.35715046525001526, Final Batch Loss: 0.08494055271148682\n",
      "Epoch 3895, Loss: 0.41890115290880203, Final Batch Loss: 0.15401555597782135\n",
      "Epoch 3896, Loss: 0.3525741547346115, Final Batch Loss: 0.0877392366528511\n",
      "Epoch 3897, Loss: 0.43177931010723114, Final Batch Loss: 0.10191351175308228\n",
      "Epoch 3898, Loss: 0.5226014330983162, Final Batch Loss: 0.11224325746297836\n",
      "Epoch 3899, Loss: 0.4217837452888489, Final Batch Loss: 0.17975880205631256\n",
      "Epoch 3900, Loss: 0.4286172389984131, Final Batch Loss: 0.18900641798973083\n",
      "Epoch 3901, Loss: 0.4131632596254349, Final Batch Loss: 0.1250361055135727\n",
      "Epoch 3902, Loss: 0.4697052985429764, Final Batch Loss: 0.14143502712249756\n",
      "Epoch 3903, Loss: 0.42216186970472336, Final Batch Loss: 0.08870605379343033\n",
      "Epoch 3904, Loss: 0.5018393248319626, Final Batch Loss: 0.203560009598732\n",
      "Epoch 3905, Loss: 0.3885277882218361, Final Batch Loss: 0.106700100004673\n",
      "Epoch 3906, Loss: 0.3998957648873329, Final Batch Loss: 0.09784841537475586\n",
      "Epoch 3907, Loss: 0.3362091928720474, Final Batch Loss: 0.062434449791908264\n",
      "Epoch 3908, Loss: 0.37988246232271194, Final Batch Loss: 0.07967445999383926\n",
      "Epoch 3909, Loss: 0.3376956358551979, Final Batch Loss: 0.07921253144741058\n",
      "Epoch 3910, Loss: 0.35185805708169937, Final Batch Loss: 0.14882703125476837\n",
      "Epoch 3911, Loss: 0.34004607796669006, Final Batch Loss: 0.0647927075624466\n",
      "Epoch 3912, Loss: 0.49248847365379333, Final Batch Loss: 0.18341825902462006\n",
      "Epoch 3913, Loss: 0.5189978182315826, Final Batch Loss: 0.26157256960868835\n",
      "Epoch 3914, Loss: 0.4312732219696045, Final Batch Loss: 0.06896704435348511\n",
      "Epoch 3915, Loss: 0.4063141420483589, Final Batch Loss: 0.09052153676748276\n",
      "Epoch 3916, Loss: 0.4378187358379364, Final Batch Loss: 0.1406383216381073\n",
      "Epoch 3917, Loss: 0.38414308428764343, Final Batch Loss: 0.07777038216590881\n",
      "Epoch 3918, Loss: 0.3909541890025139, Final Batch Loss: 0.16423337161540985\n",
      "Epoch 3919, Loss: 0.37933318316936493, Final Batch Loss: 0.08431905508041382\n",
      "Epoch 3920, Loss: 0.48165594041347504, Final Batch Loss: 0.10154412686824799\n",
      "Epoch 3921, Loss: 0.3791714832186699, Final Batch Loss: 0.10170922428369522\n",
      "Epoch 3922, Loss: 0.4046703055500984, Final Batch Loss: 0.12509772181510925\n",
      "Epoch 3923, Loss: 0.29529358446598053, Final Batch Loss: 0.07622994482517242\n",
      "Epoch 3924, Loss: 0.433545283973217, Final Batch Loss: 0.1447075754404068\n",
      "Epoch 3925, Loss: 0.36791596561670303, Final Batch Loss: 0.12301229685544968\n",
      "Epoch 3926, Loss: 0.4701550453901291, Final Batch Loss: 0.18575632572174072\n",
      "Epoch 3927, Loss: 0.41173306107521057, Final Batch Loss: 0.14870646595954895\n",
      "Epoch 3928, Loss: 0.4283987507224083, Final Batch Loss: 0.1943754255771637\n",
      "Epoch 3929, Loss: 0.3575897887349129, Final Batch Loss: 0.11301052570343018\n",
      "Epoch 3930, Loss: 0.49869440495967865, Final Batch Loss: 0.13726253807544708\n",
      "Epoch 3931, Loss: 0.4352140352129936, Final Batch Loss: 0.1645294427871704\n",
      "Epoch 3932, Loss: 0.4250449687242508, Final Batch Loss: 0.15986283123493195\n",
      "Epoch 3933, Loss: 0.45619818568229675, Final Batch Loss: 0.21765391528606415\n",
      "Epoch 3934, Loss: 0.36008360981941223, Final Batch Loss: 0.14175014197826385\n",
      "Epoch 3935, Loss: 0.4552416205406189, Final Batch Loss: 0.17841802537441254\n",
      "Epoch 3936, Loss: 0.4958023875951767, Final Batch Loss: 0.1330489218235016\n",
      "Epoch 3937, Loss: 0.30617424100637436, Final Batch Loss: 0.09187667071819305\n",
      "Epoch 3938, Loss: 0.40704114735126495, Final Batch Loss: 0.13579578697681427\n",
      "Epoch 3939, Loss: 0.4628477245569229, Final Batch Loss: 0.1978452056646347\n",
      "Epoch 3940, Loss: 0.4253438636660576, Final Batch Loss: 0.1743042916059494\n",
      "Epoch 3941, Loss: 0.3597613424062729, Final Batch Loss: 0.12122597545385361\n",
      "Epoch 3942, Loss: 0.4239595830440521, Final Batch Loss: 0.16625145077705383\n",
      "Epoch 3943, Loss: 0.34868699312210083, Final Batch Loss: 0.08711892366409302\n",
      "Epoch 3944, Loss: 0.3719305247068405, Final Batch Loss: 0.1198001578450203\n",
      "Epoch 3945, Loss: 0.40080440789461136, Final Batch Loss: 0.09804686158895493\n",
      "Epoch 3946, Loss: 0.44861868023872375, Final Batch Loss: 0.15185244381427765\n",
      "Epoch 3947, Loss: 0.31818483024835587, Final Batch Loss: 0.08656354993581772\n",
      "Epoch 3948, Loss: 0.47270607203245163, Final Batch Loss: 0.1419450342655182\n",
      "Epoch 3949, Loss: 0.3927065208554268, Final Batch Loss: 0.12375570833683014\n",
      "Epoch 3950, Loss: 0.46909424662590027, Final Batch Loss: 0.12741290032863617\n",
      "Epoch 3951, Loss: 0.4617837369441986, Final Batch Loss: 0.2026088386774063\n",
      "Epoch 3952, Loss: 0.555510863661766, Final Batch Loss: 0.2656627595424652\n",
      "Epoch 3953, Loss: 0.42734653502702713, Final Batch Loss: 0.07848171144723892\n",
      "Epoch 3954, Loss: 0.4844668060541153, Final Batch Loss: 0.1943114846944809\n",
      "Epoch 3955, Loss: 0.43287187814712524, Final Batch Loss: 0.14760711789131165\n",
      "Epoch 3956, Loss: 0.4639980345964432, Final Batch Loss: 0.1656644344329834\n",
      "Epoch 3957, Loss: 0.3092738837003708, Final Batch Loss: 0.11271537840366364\n",
      "Epoch 3958, Loss: 0.4075561612844467, Final Batch Loss: 0.12208828330039978\n",
      "Epoch 3959, Loss: 0.3659387156367302, Final Batch Loss: 0.1388835608959198\n",
      "Epoch 3960, Loss: 0.42362863570451736, Final Batch Loss: 0.11164165288209915\n",
      "Epoch 3961, Loss: 0.4401324391365051, Final Batch Loss: 0.16527098417282104\n",
      "Epoch 3962, Loss: 0.4748171865940094, Final Batch Loss: 0.19280490279197693\n",
      "Epoch 3963, Loss: 0.3936648890376091, Final Batch Loss: 0.1305869221687317\n",
      "Epoch 3964, Loss: 0.4399339184165001, Final Batch Loss: 0.1806085854768753\n",
      "Epoch 3965, Loss: 0.4418027698993683, Final Batch Loss: 0.12157011032104492\n",
      "Epoch 3966, Loss: 0.38689766079187393, Final Batch Loss: 0.0899273082613945\n",
      "Epoch 3967, Loss: 0.345241017639637, Final Batch Loss: 0.12485352158546448\n",
      "Epoch 3968, Loss: 0.42887144535779953, Final Batch Loss: 0.1633758544921875\n",
      "Epoch 3969, Loss: 0.4757049158215523, Final Batch Loss: 0.11458263546228409\n",
      "Epoch 3970, Loss: 0.329204224050045, Final Batch Loss: 0.10313446074724197\n",
      "Epoch 3971, Loss: 0.34142063558101654, Final Batch Loss: 0.10467173904180527\n",
      "Epoch 3972, Loss: 0.35352788865566254, Final Batch Loss: 0.06707651913166046\n",
      "Epoch 3973, Loss: 0.34360919147729874, Final Batch Loss: 0.14755402505397797\n",
      "Epoch 3974, Loss: 0.4329438880085945, Final Batch Loss: 0.1193896010518074\n",
      "Epoch 3975, Loss: 0.40508100390434265, Final Batch Loss: 0.17118246853351593\n",
      "Epoch 3976, Loss: 0.4010402634739876, Final Batch Loss: 0.1271035224199295\n",
      "Epoch 3977, Loss: 0.45220036804676056, Final Batch Loss: 0.13517805933952332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3978, Loss: 0.5161795020103455, Final Batch Loss: 0.15572986006736755\n",
      "Epoch 3979, Loss: 0.42487121373414993, Final Batch Loss: 0.15164706110954285\n",
      "Epoch 3980, Loss: 0.4035682827234268, Final Batch Loss: 0.1362060308456421\n",
      "Epoch 3981, Loss: 0.27930857241153717, Final Batch Loss: 0.09991806000471115\n",
      "Epoch 3982, Loss: 0.37219085544347763, Final Batch Loss: 0.13393743336200714\n",
      "Epoch 3983, Loss: 0.30415570735931396, Final Batch Loss: 0.09485240280628204\n",
      "Epoch 3984, Loss: 0.32201438397169113, Final Batch Loss: 0.07818754017353058\n",
      "Epoch 3985, Loss: 0.3838253691792488, Final Batch Loss: 0.12375464290380478\n",
      "Epoch 3986, Loss: 0.4730101302266121, Final Batch Loss: 0.20011761784553528\n",
      "Epoch 3987, Loss: 0.38229794055223465, Final Batch Loss: 0.12771816551685333\n",
      "Epoch 3988, Loss: 0.4163210242986679, Final Batch Loss: 0.14194105565547943\n",
      "Epoch 3989, Loss: 0.3737751543521881, Final Batch Loss: 0.12739810347557068\n",
      "Epoch 3990, Loss: 0.47723303735256195, Final Batch Loss: 0.1541365534067154\n",
      "Epoch 3991, Loss: 0.38429633527994156, Final Batch Loss: 0.1139359176158905\n",
      "Epoch 3992, Loss: 0.47022590041160583, Final Batch Loss: 0.15429243445396423\n",
      "Epoch 3993, Loss: 0.5025151818990707, Final Batch Loss: 0.1398889422416687\n",
      "Epoch 3994, Loss: 0.4432532787322998, Final Batch Loss: 0.19354656338691711\n",
      "Epoch 3995, Loss: 0.38140586018562317, Final Batch Loss: 0.104369156062603\n",
      "Epoch 3996, Loss: 0.343608520925045, Final Batch Loss: 0.08232717216014862\n",
      "Epoch 3997, Loss: 0.37498030066490173, Final Batch Loss: 0.10854075103998184\n",
      "Epoch 3998, Loss: 0.3805772066116333, Final Batch Loss: 0.11910682171583176\n",
      "Epoch 3999, Loss: 0.4151296317577362, Final Batch Loss: 0.1324184685945511\n",
      "Epoch 4000, Loss: 0.5582931786775589, Final Batch Loss: 0.29178494215011597\n",
      "Epoch 4001, Loss: 0.4142775982618332, Final Batch Loss: 0.1281285285949707\n",
      "Epoch 4002, Loss: 0.4229823499917984, Final Batch Loss: 0.12891130149364471\n",
      "Epoch 4003, Loss: 0.44949252903461456, Final Batch Loss: 0.13978764414787292\n",
      "Epoch 4004, Loss: 0.4095645844936371, Final Batch Loss: 0.13944435119628906\n",
      "Epoch 4005, Loss: 0.47368021309375763, Final Batch Loss: 0.16913670301437378\n",
      "Epoch 4006, Loss: 0.4269517511129379, Final Batch Loss: 0.15871939063072205\n",
      "Epoch 4007, Loss: 0.4143673777580261, Final Batch Loss: 0.10252845287322998\n",
      "Epoch 4008, Loss: 0.5360671430826187, Final Batch Loss: 0.20249685645103455\n",
      "Epoch 4009, Loss: 0.3337578549981117, Final Batch Loss: 0.08343371003866196\n",
      "Epoch 4010, Loss: 0.45246224105358124, Final Batch Loss: 0.12559309601783752\n",
      "Epoch 4011, Loss: 0.42793579399585724, Final Batch Loss: 0.17710012197494507\n",
      "Epoch 4012, Loss: 0.38193193078041077, Final Batch Loss: 0.13681837916374207\n",
      "Epoch 4013, Loss: 0.361872598528862, Final Batch Loss: 0.0803922489285469\n",
      "Epoch 4014, Loss: 0.3525456264615059, Final Batch Loss: 0.1499594748020172\n",
      "Epoch 4015, Loss: 0.4393646642565727, Final Batch Loss: 0.09335824102163315\n",
      "Epoch 4016, Loss: 0.4468081071972847, Final Batch Loss: 0.09924156218767166\n",
      "Epoch 4017, Loss: 0.42896105349063873, Final Batch Loss: 0.13117578625679016\n",
      "Epoch 4018, Loss: 0.3830162510275841, Final Batch Loss: 0.12891432642936707\n",
      "Epoch 4019, Loss: 0.3576309382915497, Final Batch Loss: 0.10761277377605438\n",
      "Epoch 4020, Loss: 0.371861033141613, Final Batch Loss: 0.1065574511885643\n",
      "Epoch 4021, Loss: 0.3996683955192566, Final Batch Loss: 0.11189807951450348\n",
      "Epoch 4022, Loss: 0.3741692677140236, Final Batch Loss: 0.08530906587839127\n",
      "Epoch 4023, Loss: 0.5031592547893524, Final Batch Loss: 0.17877958714962006\n",
      "Epoch 4024, Loss: 0.4435929208993912, Final Batch Loss: 0.1989927440881729\n",
      "Epoch 4025, Loss: 0.3258861154317856, Final Batch Loss: 0.0721345990896225\n",
      "Epoch 4026, Loss: 0.4395690783858299, Final Batch Loss: 0.161210298538208\n",
      "Epoch 4027, Loss: 0.39434852451086044, Final Batch Loss: 0.11875880509614944\n",
      "Epoch 4028, Loss: 0.4331945776939392, Final Batch Loss: 0.11320190131664276\n",
      "Epoch 4029, Loss: 0.3976981192827225, Final Batch Loss: 0.17038723826408386\n",
      "Epoch 4030, Loss: 0.4270210415124893, Final Batch Loss: 0.12952426075935364\n",
      "Epoch 4031, Loss: 0.39608798921108246, Final Batch Loss: 0.11372745037078857\n",
      "Epoch 4032, Loss: 0.4132312685251236, Final Batch Loss: 0.12622666358947754\n",
      "Epoch 4033, Loss: 0.3251446262001991, Final Batch Loss: 0.10463298857212067\n",
      "Epoch 4034, Loss: 0.41653120517730713, Final Batch Loss: 0.13821931183338165\n",
      "Epoch 4035, Loss: 0.394376203417778, Final Batch Loss: 0.16390803456306458\n",
      "Epoch 4036, Loss: 0.42579294741153717, Final Batch Loss: 0.13319802284240723\n",
      "Epoch 4037, Loss: 0.4111113026738167, Final Batch Loss: 0.1653607189655304\n",
      "Epoch 4038, Loss: 0.41302619129419327, Final Batch Loss: 0.16614392399787903\n",
      "Epoch 4039, Loss: 0.28581662476062775, Final Batch Loss: 0.0943388119339943\n",
      "Epoch 4040, Loss: 0.46123117953538895, Final Batch Loss: 0.19671794772148132\n",
      "Epoch 4041, Loss: 0.40835513919591904, Final Batch Loss: 0.10248776525259018\n",
      "Epoch 4042, Loss: 0.2986195236444473, Final Batch Loss: 0.07383818179368973\n",
      "Epoch 4043, Loss: 0.3903057798743248, Final Batch Loss: 0.12929193675518036\n",
      "Epoch 4044, Loss: 0.4124074876308441, Final Batch Loss: 0.1276226043701172\n",
      "Epoch 4045, Loss: 0.4030435308814049, Final Batch Loss: 0.16800615191459656\n",
      "Epoch 4046, Loss: 0.37469103932380676, Final Batch Loss: 0.10128243267536163\n",
      "Epoch 4047, Loss: 0.3229304999113083, Final Batch Loss: 0.09217584133148193\n",
      "Epoch 4048, Loss: 0.4752706289291382, Final Batch Loss: 0.19859087467193604\n",
      "Epoch 4049, Loss: 0.42089033126831055, Final Batch Loss: 0.12267079949378967\n",
      "Epoch 4050, Loss: 0.4094880670309067, Final Batch Loss: 0.15447105467319489\n",
      "Epoch 4051, Loss: 0.4659804552793503, Final Batch Loss: 0.15787503123283386\n",
      "Epoch 4052, Loss: 0.4077019989490509, Final Batch Loss: 0.14814789593219757\n",
      "Epoch 4053, Loss: 0.357948936522007, Final Batch Loss: 0.06636019796133041\n",
      "Epoch 4054, Loss: 0.4421166628599167, Final Batch Loss: 0.1926974505186081\n",
      "Epoch 4055, Loss: 0.40536047518253326, Final Batch Loss: 0.12607452273368835\n",
      "Epoch 4056, Loss: 0.4081958830356598, Final Batch Loss: 0.13520103693008423\n",
      "Epoch 4057, Loss: 0.5204581543803215, Final Batch Loss: 0.22325663268566132\n",
      "Epoch 4058, Loss: 0.5020239278674126, Final Batch Loss: 0.11684588342905045\n",
      "Epoch 4059, Loss: 0.3746345192193985, Final Batch Loss: 0.14478206634521484\n",
      "Epoch 4060, Loss: 0.4017426446080208, Final Batch Loss: 0.14620201289653778\n",
      "Epoch 4061, Loss: 0.3617289587855339, Final Batch Loss: 0.12122365087270737\n",
      "Epoch 4062, Loss: 0.46724072098731995, Final Batch Loss: 0.1980421394109726\n",
      "Epoch 4063, Loss: 0.5350687503814697, Final Batch Loss: 0.1562170535326004\n",
      "Epoch 4064, Loss: 0.3534190058708191, Final Batch Loss: 0.12601882219314575\n",
      "Epoch 4065, Loss: 0.4651280641555786, Final Batch Loss: 0.14398778975009918\n",
      "Epoch 4066, Loss: 0.4408636689186096, Final Batch Loss: 0.13147617876529694\n",
      "Epoch 4067, Loss: 0.2853815108537674, Final Batch Loss: 0.07232476770877838\n",
      "Epoch 4068, Loss: 0.4740494638681412, Final Batch Loss: 0.1855105608701706\n",
      "Epoch 4069, Loss: 0.33467745035886765, Final Batch Loss: 0.09922369569540024\n",
      "Epoch 4070, Loss: 0.38075705617666245, Final Batch Loss: 0.11323445290327072\n",
      "Epoch 4071, Loss: 0.37748128920793533, Final Batch Loss: 0.14655061066150665\n",
      "Epoch 4072, Loss: 0.46650801599025726, Final Batch Loss: 0.0861521065235138\n",
      "Epoch 4073, Loss: 0.36425262689590454, Final Batch Loss: 0.11446626484394073\n",
      "Epoch 4074, Loss: 0.4435039907693863, Final Batch Loss: 0.1503189504146576\n",
      "Epoch 4075, Loss: 0.3625906929373741, Final Batch Loss: 0.10434050112962723\n",
      "Epoch 4076, Loss: 0.4052898436784744, Final Batch Loss: 0.14695462584495544\n",
      "Epoch 4077, Loss: 0.4791673719882965, Final Batch Loss: 0.23064184188842773\n",
      "Epoch 4078, Loss: 0.3976483941078186, Final Batch Loss: 0.12840667366981506\n",
      "Epoch 4079, Loss: 0.36650393158197403, Final Batch Loss: 0.09565155953168869\n",
      "Epoch 4080, Loss: 0.45476900041103363, Final Batch Loss: 0.20311495661735535\n",
      "Epoch 4081, Loss: 0.35105885565280914, Final Batch Loss: 0.07699339091777802\n",
      "Epoch 4082, Loss: 0.414127953350544, Final Batch Loss: 0.16346938908100128\n",
      "Epoch 4083, Loss: 0.36740072071552277, Final Batch Loss: 0.1301114410161972\n",
      "Epoch 4084, Loss: 0.4022999182343483, Final Batch Loss: 0.12485671788454056\n",
      "Epoch 4085, Loss: 0.47760386765003204, Final Batch Loss: 0.15483717620372772\n",
      "Epoch 4086, Loss: 0.4494476467370987, Final Batch Loss: 0.12706468999385834\n",
      "Epoch 4087, Loss: 0.5049520134925842, Final Batch Loss: 0.20514854788780212\n",
      "Epoch 4088, Loss: 0.48724156618118286, Final Batch Loss: 0.15721173584461212\n",
      "Epoch 4089, Loss: 0.4209957718849182, Final Batch Loss: 0.11800003051757812\n",
      "Epoch 4090, Loss: 0.4113715589046478, Final Batch Loss: 0.14681993424892426\n",
      "Epoch 4091, Loss: 0.5335290729999542, Final Batch Loss: 0.21544691920280457\n",
      "Epoch 4092, Loss: 0.39499620348215103, Final Batch Loss: 0.15090452134609222\n",
      "Epoch 4093, Loss: 0.4295344054698944, Final Batch Loss: 0.16963230073451996\n",
      "Epoch 4094, Loss: 0.4089217558503151, Final Batch Loss: 0.1346179097890854\n",
      "Epoch 4095, Loss: 0.3309166505932808, Final Batch Loss: 0.08104687929153442\n",
      "Epoch 4096, Loss: 0.38080988824367523, Final Batch Loss: 0.15195108950138092\n",
      "Epoch 4097, Loss: 0.40963445603847504, Final Batch Loss: 0.10331451892852783\n",
      "Epoch 4098, Loss: 0.3824068084359169, Final Batch Loss: 0.15967756509780884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4099, Loss: 0.3184060603380203, Final Batch Loss: 0.11041037738323212\n",
      "Epoch 4100, Loss: 0.5835898444056511, Final Batch Loss: 0.25973081588745117\n",
      "Epoch 4101, Loss: 0.4348364770412445, Final Batch Loss: 0.09456191956996918\n",
      "Epoch 4102, Loss: 0.3451767936348915, Final Batch Loss: 0.08708437532186508\n",
      "Epoch 4103, Loss: 0.44527454674243927, Final Batch Loss: 0.12753337621688843\n",
      "Epoch 4104, Loss: 0.3305722773075104, Final Batch Loss: 0.11728483438491821\n",
      "Epoch 4105, Loss: 0.39230021834373474, Final Batch Loss: 0.08789317309856415\n",
      "Epoch 4106, Loss: 0.35987287014722824, Final Batch Loss: 0.13032235205173492\n",
      "Epoch 4107, Loss: 0.36281100660562515, Final Batch Loss: 0.13574586808681488\n",
      "Epoch 4108, Loss: 0.3896898999810219, Final Batch Loss: 0.15038137137889862\n",
      "Epoch 4109, Loss: 0.43148043006658554, Final Batch Loss: 0.12257563322782516\n",
      "Epoch 4110, Loss: 0.3950129300355911, Final Batch Loss: 0.13038580119609833\n",
      "Epoch 4111, Loss: 0.3282826617360115, Final Batch Loss: 0.09695927798748016\n",
      "Epoch 4112, Loss: 0.3567420393228531, Final Batch Loss: 0.18057183921337128\n",
      "Epoch 4113, Loss: 0.40395669639110565, Final Batch Loss: 0.11415623128414154\n",
      "Epoch 4114, Loss: 0.40496746450662613, Final Batch Loss: 0.1773119568824768\n",
      "Epoch 4115, Loss: 0.3454100489616394, Final Batch Loss: 0.17757409811019897\n",
      "Epoch 4116, Loss: 0.3761446624994278, Final Batch Loss: 0.12194403260946274\n",
      "Epoch 4117, Loss: 0.36072760820388794, Final Batch Loss: 0.11293360590934753\n",
      "Epoch 4118, Loss: 0.43046388030052185, Final Batch Loss: 0.11942450702190399\n",
      "Epoch 4119, Loss: 0.4479198530316353, Final Batch Loss: 0.15787273645401\n",
      "Epoch 4120, Loss: 0.43349961936473846, Final Batch Loss: 0.16309596598148346\n",
      "Epoch 4121, Loss: 0.41381923854351044, Final Batch Loss: 0.16463124752044678\n",
      "Epoch 4122, Loss: 0.35120538622140884, Final Batch Loss: 0.1616421937942505\n",
      "Epoch 4123, Loss: 0.5449699759483337, Final Batch Loss: 0.19162435829639435\n",
      "Epoch 4124, Loss: 0.4302782043814659, Final Batch Loss: 0.09228328615427017\n",
      "Epoch 4125, Loss: 0.4228053614497185, Final Batch Loss: 0.10357173532247543\n",
      "Epoch 4126, Loss: 0.45658761262893677, Final Batch Loss: 0.18644356727600098\n",
      "Epoch 4127, Loss: 0.4233795553445816, Final Batch Loss: 0.12621819972991943\n",
      "Epoch 4128, Loss: 0.37731876224279404, Final Batch Loss: 0.12839622795581818\n",
      "Epoch 4129, Loss: 0.3844708353281021, Final Batch Loss: 0.13946151733398438\n",
      "Epoch 4130, Loss: 0.3283704146742821, Final Batch Loss: 0.08092767000198364\n",
      "Epoch 4131, Loss: 0.4389263838529587, Final Batch Loss: 0.1826982945203781\n",
      "Epoch 4132, Loss: 0.3568989261984825, Final Batch Loss: 0.13484176993370056\n",
      "Epoch 4133, Loss: 0.4307038113474846, Final Batch Loss: 0.15627416968345642\n",
      "Epoch 4134, Loss: 0.34346941858530045, Final Batch Loss: 0.11120816320180893\n",
      "Epoch 4135, Loss: 0.3737986460328102, Final Batch Loss: 0.14279505610466003\n",
      "Epoch 4136, Loss: 0.4450911730527878, Final Batch Loss: 0.20009289681911469\n",
      "Epoch 4137, Loss: 0.4362150505185127, Final Batch Loss: 0.193475604057312\n",
      "Epoch 4138, Loss: 0.41108477860689163, Final Batch Loss: 0.08550591766834259\n",
      "Epoch 4139, Loss: 0.5221783518791199, Final Batch Loss: 0.19413571059703827\n",
      "Epoch 4140, Loss: 0.41874417662620544, Final Batch Loss: 0.15151207149028778\n",
      "Epoch 4141, Loss: 0.3585692197084427, Final Batch Loss: 0.07274715602397919\n",
      "Epoch 4142, Loss: 0.31986989453434944, Final Batch Loss: 0.125880628824234\n",
      "Epoch 4143, Loss: 0.44303464889526367, Final Batch Loss: 0.1251693069934845\n",
      "Epoch 4144, Loss: 0.36143866926431656, Final Batch Loss: 0.10155525803565979\n",
      "Epoch 4145, Loss: 0.40110407769680023, Final Batch Loss: 0.1286691129207611\n",
      "Epoch 4146, Loss: 0.42196547985076904, Final Batch Loss: 0.14580096304416656\n",
      "Epoch 4147, Loss: 0.4475947171449661, Final Batch Loss: 0.1401756852865219\n",
      "Epoch 4148, Loss: 0.3394962251186371, Final Batch Loss: 0.10275889933109283\n",
      "Epoch 4149, Loss: 0.3559502884745598, Final Batch Loss: 0.15538187325000763\n",
      "Epoch 4150, Loss: 0.36537956446409225, Final Batch Loss: 0.15543362498283386\n",
      "Epoch 4151, Loss: 0.4305194765329361, Final Batch Loss: 0.14485691487789154\n",
      "Epoch 4152, Loss: 0.41276155412197113, Final Batch Loss: 0.12651918828487396\n",
      "Epoch 4153, Loss: 0.4593872055411339, Final Batch Loss: 0.110728420317173\n",
      "Epoch 4154, Loss: 0.3808215856552124, Final Batch Loss: 0.10628041625022888\n",
      "Epoch 4155, Loss: 0.45056140422821045, Final Batch Loss: 0.15544076263904572\n",
      "Epoch 4156, Loss: 0.4390575513243675, Final Batch Loss: 0.11618275195360184\n",
      "Epoch 4157, Loss: 0.3652594983577728, Final Batch Loss: 0.15190386772155762\n",
      "Epoch 4158, Loss: 0.42244138568639755, Final Batch Loss: 0.1192154660820961\n",
      "Epoch 4159, Loss: 0.3641127645969391, Final Batch Loss: 0.1689579039812088\n",
      "Epoch 4160, Loss: 0.48698779940605164, Final Batch Loss: 0.22561505436897278\n",
      "Epoch 4161, Loss: 0.38640420883893967, Final Batch Loss: 0.16330721974372864\n",
      "Epoch 4162, Loss: 0.3302853927016258, Final Batch Loss: 0.12837189435958862\n",
      "Epoch 4163, Loss: 0.37721484154462814, Final Batch Loss: 0.07776935398578644\n",
      "Epoch 4164, Loss: 0.44359326362609863, Final Batch Loss: 0.1757388710975647\n",
      "Epoch 4165, Loss: 0.4433727189898491, Final Batch Loss: 0.2006819099187851\n",
      "Epoch 4166, Loss: 0.36537688225507736, Final Batch Loss: 0.20941121876239777\n",
      "Epoch 4167, Loss: 0.4555675610899925, Final Batch Loss: 0.20024347305297852\n",
      "Epoch 4168, Loss: 0.5519512742757797, Final Batch Loss: 0.21417325735092163\n",
      "Epoch 4169, Loss: 0.3725561872124672, Final Batch Loss: 0.09392432123422623\n",
      "Epoch 4170, Loss: 0.4503054618835449, Final Batch Loss: 0.16115407645702362\n",
      "Epoch 4171, Loss: 0.3709266781806946, Final Batch Loss: 0.11101630330085754\n",
      "Epoch 4172, Loss: 0.39672140032052994, Final Batch Loss: 0.06847197562456131\n",
      "Epoch 4173, Loss: 0.3473846912384033, Final Batch Loss: 0.11498937010765076\n",
      "Epoch 4174, Loss: 0.42597363889217377, Final Batch Loss: 0.136561781167984\n",
      "Epoch 4175, Loss: 0.37251748889684677, Final Batch Loss: 0.09891095757484436\n",
      "Epoch 4176, Loss: 0.39873840659856796, Final Batch Loss: 0.12581340968608856\n",
      "Epoch 4177, Loss: 0.4732866734266281, Final Batch Loss: 0.1249954104423523\n",
      "Epoch 4178, Loss: 0.26600152626633644, Final Batch Loss: 0.04368403181433678\n",
      "Epoch 4179, Loss: 0.38829631358385086, Final Batch Loss: 0.1351584494113922\n",
      "Epoch 4180, Loss: 0.40613454580307007, Final Batch Loss: 0.13685397803783417\n",
      "Epoch 4181, Loss: 0.3229106590151787, Final Batch Loss: 0.13424135744571686\n",
      "Epoch 4182, Loss: 0.41574500501155853, Final Batch Loss: 0.1324474811553955\n",
      "Epoch 4183, Loss: 0.5130035281181335, Final Batch Loss: 0.2529928386211395\n",
      "Epoch 4184, Loss: 0.30623960494995117, Final Batch Loss: 0.11506948620080948\n",
      "Epoch 4185, Loss: 0.4814978316426277, Final Batch Loss: 0.21986904740333557\n",
      "Epoch 4186, Loss: 0.48518940806388855, Final Batch Loss: 0.17996275424957275\n",
      "Epoch 4187, Loss: 0.39481737464666367, Final Batch Loss: 0.13644321262836456\n",
      "Epoch 4188, Loss: 0.43276767432689667, Final Batch Loss: 0.1133730337023735\n",
      "Epoch 4189, Loss: 0.35109594836831093, Final Batch Loss: 0.046330977231264114\n",
      "Epoch 4190, Loss: 0.3282089978456497, Final Batch Loss: 0.14783340692520142\n",
      "Epoch 4191, Loss: 0.40931978821754456, Final Batch Loss: 0.09673291444778442\n",
      "Epoch 4192, Loss: 0.38999947160482407, Final Batch Loss: 0.10408229380846024\n",
      "Epoch 4193, Loss: 0.41349632292985916, Final Batch Loss: 0.1113693043589592\n",
      "Epoch 4194, Loss: 0.4155209809541702, Final Batch Loss: 0.13910545408725739\n",
      "Epoch 4195, Loss: 0.32696398347616196, Final Batch Loss: 0.06639989465475082\n",
      "Epoch 4196, Loss: 0.3321637213230133, Final Batch Loss: 0.12130969017744064\n",
      "Epoch 4197, Loss: 0.3338635712862015, Final Batch Loss: 0.08454138040542603\n",
      "Epoch 4198, Loss: 0.492499977350235, Final Batch Loss: 0.210829496383667\n",
      "Epoch 4199, Loss: 0.4099953696131706, Final Batch Loss: 0.11735738068819046\n",
      "Epoch 4200, Loss: 0.34421001374721527, Final Batch Loss: 0.1217382550239563\n",
      "Epoch 4201, Loss: 0.4233882799744606, Final Batch Loss: 0.17207251489162445\n",
      "Epoch 4202, Loss: 0.46270516514778137, Final Batch Loss: 0.17125415802001953\n",
      "Epoch 4203, Loss: 0.4246825575828552, Final Batch Loss: 0.13544771075248718\n",
      "Epoch 4204, Loss: 0.4348471611738205, Final Batch Loss: 0.1352422684431076\n",
      "Epoch 4205, Loss: 0.3512222617864609, Final Batch Loss: 0.12896791100502014\n",
      "Epoch 4206, Loss: 0.41628874838352203, Final Batch Loss: 0.1386197805404663\n",
      "Epoch 4207, Loss: 0.4041345864534378, Final Batch Loss: 0.12068971991539001\n",
      "Epoch 4208, Loss: 0.49275559186935425, Final Batch Loss: 0.2269691675901413\n",
      "Epoch 4209, Loss: 0.41736193001270294, Final Batch Loss: 0.15262693166732788\n",
      "Epoch 4210, Loss: 0.43032485246658325, Final Batch Loss: 0.1383591890335083\n",
      "Epoch 4211, Loss: 0.4953310340642929, Final Batch Loss: 0.16638313233852386\n",
      "Epoch 4212, Loss: 0.4020529240369797, Final Batch Loss: 0.12350847572088242\n",
      "Epoch 4213, Loss: 0.3902062401175499, Final Batch Loss: 0.11925452947616577\n",
      "Epoch 4214, Loss: 0.46910230815410614, Final Batch Loss: 0.1112046018242836\n",
      "Epoch 4215, Loss: 0.3545204848051071, Final Batch Loss: 0.09471102058887482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4216, Loss: 0.46251076459884644, Final Batch Loss: 0.17236150801181793\n",
      "Epoch 4217, Loss: 0.4694548696279526, Final Batch Loss: 0.15312868356704712\n",
      "Epoch 4218, Loss: 0.39455657452344894, Final Batch Loss: 0.13158555328845978\n",
      "Epoch 4219, Loss: 0.3856864795088768, Final Batch Loss: 0.1106388196349144\n",
      "Epoch 4220, Loss: 0.4013434201478958, Final Batch Loss: 0.10283039510250092\n",
      "Epoch 4221, Loss: 0.4279927536845207, Final Batch Loss: 0.10381946712732315\n",
      "Epoch 4222, Loss: 0.4136776700615883, Final Batch Loss: 0.13022956252098083\n",
      "Epoch 4223, Loss: 0.29112767428159714, Final Batch Loss: 0.12009940296411514\n",
      "Epoch 4224, Loss: 0.38518523424863815, Final Batch Loss: 0.14212019741535187\n",
      "Epoch 4225, Loss: 0.3985045477747917, Final Batch Loss: 0.16097861528396606\n",
      "Epoch 4226, Loss: 0.4411400556564331, Final Batch Loss: 0.13079026341438293\n",
      "Epoch 4227, Loss: 0.2993985489010811, Final Batch Loss: 0.1084429994225502\n",
      "Epoch 4228, Loss: 0.34632453322410583, Final Batch Loss: 0.118052639067173\n",
      "Epoch 4229, Loss: 0.4466613084077835, Final Batch Loss: 0.15130728483200073\n",
      "Epoch 4230, Loss: 0.3524266555905342, Final Batch Loss: 0.11377377808094025\n",
      "Epoch 4231, Loss: 0.4171200916171074, Final Batch Loss: 0.07892759889364243\n",
      "Epoch 4232, Loss: 0.37911489605903625, Final Batch Loss: 0.08907105028629303\n",
      "Epoch 4233, Loss: 0.4027765542268753, Final Batch Loss: 0.12026727944612503\n",
      "Epoch 4234, Loss: 0.39492472261190414, Final Batch Loss: 0.08321959525346756\n",
      "Epoch 4235, Loss: 0.39025185257196426, Final Batch Loss: 0.09099024534225464\n",
      "Epoch 4236, Loss: 0.39953096956014633, Final Batch Loss: 0.08164595812559128\n",
      "Epoch 4237, Loss: 0.397610567510128, Final Batch Loss: 0.12717287242412567\n",
      "Epoch 4238, Loss: 0.4881342798471451, Final Batch Loss: 0.20123110711574554\n",
      "Epoch 4239, Loss: 0.37316062301397324, Final Batch Loss: 0.07244587689638138\n",
      "Epoch 4240, Loss: 0.391996294260025, Final Batch Loss: 0.15589481592178345\n",
      "Epoch 4241, Loss: 0.41062380373477936, Final Batch Loss: 0.14491015672683716\n",
      "Epoch 4242, Loss: 0.4341696873307228, Final Batch Loss: 0.18547925353050232\n",
      "Epoch 4243, Loss: 0.35049164295196533, Final Batch Loss: 0.11516547948122025\n",
      "Epoch 4244, Loss: 0.48398563265800476, Final Batch Loss: 0.18461371958255768\n",
      "Epoch 4245, Loss: 0.499581053853035, Final Batch Loss: 0.1603216975927353\n",
      "Epoch 4246, Loss: 0.6245218068361282, Final Batch Loss: 0.2398502677679062\n",
      "Epoch 4247, Loss: 0.5988203436136246, Final Batch Loss: 0.18176013231277466\n",
      "Epoch 4248, Loss: 0.47843533754348755, Final Batch Loss: 0.21219618618488312\n",
      "Epoch 4249, Loss: 0.4114520028233528, Final Batch Loss: 0.12682513892650604\n",
      "Epoch 4250, Loss: 0.46547339111566544, Final Batch Loss: 0.15906211733818054\n",
      "Epoch 4251, Loss: 0.3932395949959755, Final Batch Loss: 0.14392997324466705\n",
      "Epoch 4252, Loss: 0.4269206002354622, Final Batch Loss: 0.1194177195429802\n",
      "Epoch 4253, Loss: 0.4979131370782852, Final Batch Loss: 0.19783131778240204\n",
      "Epoch 4254, Loss: 0.490669384598732, Final Batch Loss: 0.23978595435619354\n",
      "Epoch 4255, Loss: 0.437527172267437, Final Batch Loss: 0.12159394472837448\n",
      "Epoch 4256, Loss: 0.39912259578704834, Final Batch Loss: 0.17038792371749878\n",
      "Epoch 4257, Loss: 0.4375965893268585, Final Batch Loss: 0.09015616774559021\n",
      "Epoch 4258, Loss: 0.476867213845253, Final Batch Loss: 0.14948509633541107\n",
      "Epoch 4259, Loss: 0.4288003742694855, Final Batch Loss: 0.14009098708629608\n",
      "Epoch 4260, Loss: 0.5171070396900177, Final Batch Loss: 0.16933555901050568\n",
      "Epoch 4261, Loss: 0.38052672892808914, Final Batch Loss: 0.11891362816095352\n",
      "Epoch 4262, Loss: 0.35500268638134, Final Batch Loss: 0.08523127436637878\n",
      "Epoch 4263, Loss: 0.34499210864305496, Final Batch Loss: 0.09768310189247131\n",
      "Epoch 4264, Loss: 0.39972659945487976, Final Batch Loss: 0.12869925796985626\n",
      "Epoch 4265, Loss: 0.5348699539899826, Final Batch Loss: 0.1796831488609314\n",
      "Epoch 4266, Loss: 0.31756605207920074, Final Batch Loss: 0.07531687617301941\n",
      "Epoch 4267, Loss: 0.4529682844877243, Final Batch Loss: 0.17366668581962585\n",
      "Epoch 4268, Loss: 0.3223169595003128, Final Batch Loss: 0.11402206122875214\n",
      "Epoch 4269, Loss: 0.5667890161275864, Final Batch Loss: 0.1437297761440277\n",
      "Epoch 4270, Loss: 0.34410085529088974, Final Batch Loss: 0.11179552972316742\n",
      "Epoch 4271, Loss: 0.4505026042461395, Final Batch Loss: 0.17837820947170258\n",
      "Epoch 4272, Loss: 0.36575042456388474, Final Batch Loss: 0.16373416781425476\n",
      "Epoch 4273, Loss: 0.4183141216635704, Final Batch Loss: 0.17801013588905334\n",
      "Epoch 4274, Loss: 0.36144737899303436, Final Batch Loss: 0.12830455601215363\n",
      "Epoch 4275, Loss: 0.44394926726818085, Final Batch Loss: 0.1566651314496994\n",
      "Epoch 4276, Loss: 0.35577066242694855, Final Batch Loss: 0.09678591787815094\n",
      "Epoch 4277, Loss: 0.3467015251517296, Final Batch Loss: 0.08102171868085861\n",
      "Epoch 4278, Loss: 0.4069547578692436, Final Batch Loss: 0.13784323632717133\n",
      "Epoch 4279, Loss: 0.5135684162378311, Final Batch Loss: 0.14769698679447174\n",
      "Epoch 4280, Loss: 0.5020945817232132, Final Batch Loss: 0.13458462059497833\n",
      "Epoch 4281, Loss: 0.3980300724506378, Final Batch Loss: 0.14712271094322205\n",
      "Epoch 4282, Loss: 0.43233218789100647, Final Batch Loss: 0.1260579377412796\n",
      "Epoch 4283, Loss: 0.4276099279522896, Final Batch Loss: 0.17345888912677765\n",
      "Epoch 4284, Loss: 0.49232205748558044, Final Batch Loss: 0.20330214500427246\n",
      "Epoch 4285, Loss: 0.40242867171764374, Final Batch Loss: 0.12733931839466095\n",
      "Epoch 4286, Loss: 0.3730129897594452, Final Batch Loss: 0.1299169361591339\n",
      "Epoch 4287, Loss: 0.3934328407049179, Final Batch Loss: 0.09543343633413315\n",
      "Epoch 4288, Loss: 0.40258531272411346, Final Batch Loss: 0.09420934319496155\n",
      "Epoch 4289, Loss: 0.3587513789534569, Final Batch Loss: 0.09175846725702286\n",
      "Epoch 4290, Loss: 0.4550635367631912, Final Batch Loss: 0.14967669546604156\n",
      "Epoch 4291, Loss: 0.5303672552108765, Final Batch Loss: 0.2494988739490509\n",
      "Epoch 4292, Loss: 0.4459045007824898, Final Batch Loss: 0.06346962600946426\n",
      "Epoch 4293, Loss: 0.36352039128541946, Final Batch Loss: 0.12936663627624512\n",
      "Epoch 4294, Loss: 0.29646556824445724, Final Batch Loss: 0.10520174354314804\n",
      "Epoch 4295, Loss: 0.3357684090733528, Final Batch Loss: 0.0845739021897316\n",
      "Epoch 4296, Loss: 0.44759343564510345, Final Batch Loss: 0.21202018857002258\n",
      "Epoch 4297, Loss: 0.3558949679136276, Final Batch Loss: 0.09204878658056259\n",
      "Epoch 4298, Loss: 0.3737126439809799, Final Batch Loss: 0.10122110694646835\n",
      "Epoch 4299, Loss: 0.5088704824447632, Final Batch Loss: 0.13388341665267944\n",
      "Epoch 4300, Loss: 0.38910333812236786, Final Batch Loss: 0.135330930352211\n",
      "Epoch 4301, Loss: 0.386565700173378, Final Batch Loss: 0.07900369167327881\n",
      "Epoch 4302, Loss: 0.33941130340099335, Final Batch Loss: 0.15034282207489014\n",
      "Epoch 4303, Loss: 0.3650508224964142, Final Batch Loss: 0.07655838131904602\n",
      "Epoch 4304, Loss: 0.454298734664917, Final Batch Loss: 0.12108662724494934\n",
      "Epoch 4305, Loss: 0.38875387609004974, Final Batch Loss: 0.14463956654071808\n",
      "Epoch 4306, Loss: 0.3754906505346298, Final Batch Loss: 0.09077130258083344\n",
      "Epoch 4307, Loss: 0.42273208498954773, Final Batch Loss: 0.13361014425754547\n",
      "Epoch 4308, Loss: 0.40622344613075256, Final Batch Loss: 0.1661667376756668\n",
      "Epoch 4309, Loss: 0.40565353631973267, Final Batch Loss: 0.09667931497097015\n",
      "Epoch 4310, Loss: 0.41922421008348465, Final Batch Loss: 0.11715137958526611\n",
      "Epoch 4311, Loss: 0.32344548404216766, Final Batch Loss: 0.10122604668140411\n",
      "Epoch 4312, Loss: 0.42693136632442474, Final Batch Loss: 0.1290108561515808\n",
      "Epoch 4313, Loss: 0.37667175382375717, Final Batch Loss: 0.13479790091514587\n",
      "Epoch 4314, Loss: 0.3935004323720932, Final Batch Loss: 0.09647990763187408\n",
      "Epoch 4315, Loss: 0.38733696937561035, Final Batch Loss: 0.12931416928768158\n",
      "Epoch 4316, Loss: 0.34092792868614197, Final Batch Loss: 0.12610571086406708\n",
      "Epoch 4317, Loss: 0.32895611226558685, Final Batch Loss: 0.13722531497478485\n",
      "Epoch 4318, Loss: 0.43262164294719696, Final Batch Loss: 0.15096543729305267\n",
      "Epoch 4319, Loss: 0.414984792470932, Final Batch Loss: 0.10611468553543091\n",
      "Epoch 4320, Loss: 0.3363848924636841, Final Batch Loss: 0.13836389780044556\n",
      "Epoch 4321, Loss: 0.4324423223733902, Final Batch Loss: 0.12115702033042908\n",
      "Epoch 4322, Loss: 0.3487958684563637, Final Batch Loss: 0.11885251849889755\n",
      "Epoch 4323, Loss: 0.33474599197506905, Final Batch Loss: 0.0587124265730381\n",
      "Epoch 4324, Loss: 0.43359334021806717, Final Batch Loss: 0.10488922148942947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4325, Loss: 0.36514416337013245, Final Batch Loss: 0.1601041853427887\n",
      "Epoch 4326, Loss: 0.3644457682967186, Final Batch Loss: 0.06843524426221848\n",
      "Epoch 4327, Loss: 0.40566007047891617, Final Batch Loss: 0.14146709442138672\n",
      "Epoch 4328, Loss: 0.4727778211236, Final Batch Loss: 0.12308705598115921\n",
      "Epoch 4329, Loss: 0.3904280960559845, Final Batch Loss: 0.1285550743341446\n",
      "Epoch 4330, Loss: 0.4282771497964859, Final Batch Loss: 0.13658355176448822\n",
      "Epoch 4331, Loss: 0.42160726338624954, Final Batch Loss: 0.19243745505809784\n",
      "Epoch 4332, Loss: 0.3257101699709892, Final Batch Loss: 0.09710835665464401\n",
      "Epoch 4333, Loss: 0.4604284018278122, Final Batch Loss: 0.16765859723091125\n",
      "Epoch 4334, Loss: 0.42218732833862305, Final Batch Loss: 0.12156431376934052\n",
      "Epoch 4335, Loss: 0.46382153034210205, Final Batch Loss: 0.15556365251541138\n",
      "Epoch 4336, Loss: 0.3174484521150589, Final Batch Loss: 0.08861589431762695\n",
      "Epoch 4337, Loss: 0.49204401671886444, Final Batch Loss: 0.1271744817495346\n",
      "Epoch 4338, Loss: 0.4539406895637512, Final Batch Loss: 0.1409946233034134\n",
      "Epoch 4339, Loss: 0.4019043520092964, Final Batch Loss: 0.12747810781002045\n",
      "Epoch 4340, Loss: 0.38106612861156464, Final Batch Loss: 0.14350710809230804\n",
      "Epoch 4341, Loss: 0.4403790161013603, Final Batch Loss: 0.21556435525417328\n",
      "Epoch 4342, Loss: 0.32378336042165756, Final Batch Loss: 0.13650934398174286\n",
      "Epoch 4343, Loss: 0.4098764434456825, Final Batch Loss: 0.12190008908510208\n",
      "Epoch 4344, Loss: 0.30778900533914566, Final Batch Loss: 0.07069894671440125\n",
      "Epoch 4345, Loss: 0.3448238745331764, Final Batch Loss: 0.09794885665178299\n",
      "Epoch 4346, Loss: 0.3832232169806957, Final Batch Loss: 0.05743392929434776\n",
      "Epoch 4347, Loss: 0.33847080916166306, Final Batch Loss: 0.08530322462320328\n",
      "Epoch 4348, Loss: 0.3803195133805275, Final Batch Loss: 0.14667850732803345\n",
      "Epoch 4349, Loss: 0.4017211124300957, Final Batch Loss: 0.1602555811405182\n",
      "Epoch 4350, Loss: 0.44671551138162613, Final Batch Loss: 0.16259494423866272\n",
      "Epoch 4351, Loss: 0.4558422565460205, Final Batch Loss: 0.12372426688671112\n",
      "Epoch 4352, Loss: 0.32195084542036057, Final Batch Loss: 0.10939084738492966\n",
      "Epoch 4353, Loss: 0.29723750799894333, Final Batch Loss: 0.10012347996234894\n",
      "Epoch 4354, Loss: 0.5069582089781761, Final Batch Loss: 0.2361600399017334\n",
      "Epoch 4355, Loss: 0.29843930900096893, Final Batch Loss: 0.13360098004341125\n",
      "Epoch 4356, Loss: 0.3889009431004524, Final Batch Loss: 0.10121037811040878\n",
      "Epoch 4357, Loss: 0.41499338299036026, Final Batch Loss: 0.1079932227730751\n",
      "Epoch 4358, Loss: 0.42436428368091583, Final Batch Loss: 0.13984739780426025\n",
      "Epoch 4359, Loss: 0.3988329768180847, Final Batch Loss: 0.1012808233499527\n",
      "Epoch 4360, Loss: 0.45171845704317093, Final Batch Loss: 0.12114975601434708\n",
      "Epoch 4361, Loss: 0.4299275055527687, Final Batch Loss: 0.16169866919517517\n",
      "Epoch 4362, Loss: 0.5148579329252243, Final Batch Loss: 0.1824253648519516\n",
      "Epoch 4363, Loss: 0.3817439377307892, Final Batch Loss: 0.12094920128583908\n",
      "Epoch 4364, Loss: 0.5364516079425812, Final Batch Loss: 0.19277022778987885\n",
      "Epoch 4365, Loss: 0.35661063343286514, Final Batch Loss: 0.09429977089166641\n",
      "Epoch 4366, Loss: 0.3818589895963669, Final Batch Loss: 0.06849408894777298\n",
      "Epoch 4367, Loss: 0.3673538565635681, Final Batch Loss: 0.10052771121263504\n",
      "Epoch 4368, Loss: 0.34027601033449173, Final Batch Loss: 0.1373598873615265\n",
      "Epoch 4369, Loss: 0.4274647533893585, Final Batch Loss: 0.11701513826847076\n",
      "Epoch 4370, Loss: 0.28590405732393265, Final Batch Loss: 0.0666520744562149\n",
      "Epoch 4371, Loss: 0.37039653211832047, Final Batch Loss: 0.08751992136240005\n",
      "Epoch 4372, Loss: 0.47106024622917175, Final Batch Loss: 0.17775972187519073\n",
      "Epoch 4373, Loss: 0.48397835344076157, Final Batch Loss: 0.16623549163341522\n",
      "Epoch 4374, Loss: 0.3938809931278229, Final Batch Loss: 0.15446066856384277\n",
      "Epoch 4375, Loss: 0.4547114968299866, Final Batch Loss: 0.19031749665737152\n",
      "Epoch 4376, Loss: 0.3392820507287979, Final Batch Loss: 0.08100339025259018\n",
      "Epoch 4377, Loss: 0.3338257446885109, Final Batch Loss: 0.09629818052053452\n",
      "Epoch 4378, Loss: 0.36721865832805634, Final Batch Loss: 0.11159923672676086\n",
      "Epoch 4379, Loss: 0.3968680873513222, Final Batch Loss: 0.0933927372097969\n",
      "Epoch 4380, Loss: 0.37364211678504944, Final Batch Loss: 0.1002173125743866\n",
      "Epoch 4381, Loss: 0.4269702881574631, Final Batch Loss: 0.15414400398731232\n",
      "Epoch 4382, Loss: 0.38027632236480713, Final Batch Loss: 0.19410328567028046\n",
      "Epoch 4383, Loss: 0.4491233602166176, Final Batch Loss: 0.16537711024284363\n",
      "Epoch 4384, Loss: 0.38397540152072906, Final Batch Loss: 0.10430888086557388\n",
      "Epoch 4385, Loss: 0.4411631226539612, Final Batch Loss: 0.1496986597776413\n",
      "Epoch 4386, Loss: 0.39409444481134415, Final Batch Loss: 0.14832289516925812\n",
      "Epoch 4387, Loss: 0.45445338636636734, Final Batch Loss: 0.1738693118095398\n",
      "Epoch 4388, Loss: 0.35896583646535873, Final Batch Loss: 0.09480582177639008\n",
      "Epoch 4389, Loss: 0.3285241574048996, Final Batch Loss: 0.1061018854379654\n",
      "Epoch 4390, Loss: 0.31179793924093246, Final Batch Loss: 0.06729667633771896\n",
      "Epoch 4391, Loss: 0.3909350410103798, Final Batch Loss: 0.13512156903743744\n",
      "Epoch 4392, Loss: 0.33722374588251114, Final Batch Loss: 0.110779769718647\n",
      "Epoch 4393, Loss: 0.32675229758024216, Final Batch Loss: 0.08266089111566544\n",
      "Epoch 4394, Loss: 0.4518860876560211, Final Batch Loss: 0.1399315744638443\n",
      "Epoch 4395, Loss: 0.41782132536172867, Final Batch Loss: 0.132773756980896\n",
      "Epoch 4396, Loss: 0.37373441457748413, Final Batch Loss: 0.12291087955236435\n",
      "Epoch 4397, Loss: 0.38659007102251053, Final Batch Loss: 0.11980964988470078\n",
      "Epoch 4398, Loss: 0.4373835623264313, Final Batch Loss: 0.14679645001888275\n",
      "Epoch 4399, Loss: 0.4210398495197296, Final Batch Loss: 0.20785503089427948\n",
      "Epoch 4400, Loss: 0.42223361879587173, Final Batch Loss: 0.20470136404037476\n",
      "Epoch 4401, Loss: 0.3509161099791527, Final Batch Loss: 0.08764632046222687\n",
      "Epoch 4402, Loss: 0.38678590953350067, Final Batch Loss: 0.13165637850761414\n",
      "Epoch 4403, Loss: 0.44770529121160507, Final Batch Loss: 0.2546021640300751\n",
      "Epoch 4404, Loss: 0.49450335651636124, Final Batch Loss: 0.11924692243337631\n",
      "Epoch 4405, Loss: 0.47860440611839294, Final Batch Loss: 0.18120023608207703\n",
      "Epoch 4406, Loss: 0.34426385909318924, Final Batch Loss: 0.11142217367887497\n",
      "Epoch 4407, Loss: 0.44985391199588776, Final Batch Loss: 0.11329938471317291\n",
      "Epoch 4408, Loss: 0.35683852434158325, Final Batch Loss: 0.12144392728805542\n",
      "Epoch 4409, Loss: 0.3684666007757187, Final Batch Loss: 0.10430436581373215\n",
      "Epoch 4410, Loss: 0.47373440861701965, Final Batch Loss: 0.12046346068382263\n",
      "Epoch 4411, Loss: 0.39941174536943436, Final Batch Loss: 0.14107215404510498\n",
      "Epoch 4412, Loss: 0.3068470433354378, Final Batch Loss: 0.0579337477684021\n",
      "Epoch 4413, Loss: 0.47146035730838776, Final Batch Loss: 0.21449202299118042\n",
      "Epoch 4414, Loss: 0.37418028712272644, Final Batch Loss: 0.1339142769575119\n",
      "Epoch 4415, Loss: 0.3555167689919472, Final Batch Loss: 0.10615859925746918\n",
      "Epoch 4416, Loss: 0.3903942182660103, Final Batch Loss: 0.14057691395282745\n",
      "Epoch 4417, Loss: 0.4451999291777611, Final Batch Loss: 0.1522778868675232\n",
      "Epoch 4418, Loss: 0.2868671864271164, Final Batch Loss: 0.11976383626461029\n",
      "Epoch 4419, Loss: 0.38511238247156143, Final Batch Loss: 0.13934753835201263\n",
      "Epoch 4420, Loss: 0.321609303355217, Final Batch Loss: 0.09339307248592377\n",
      "Epoch 4421, Loss: 0.41918519139289856, Final Batch Loss: 0.12406860291957855\n",
      "Epoch 4422, Loss: 0.303662545979023, Final Batch Loss: 0.06700468063354492\n",
      "Epoch 4423, Loss: 0.38439951837062836, Final Batch Loss: 0.1348593831062317\n",
      "Epoch 4424, Loss: 0.4109477251768112, Final Batch Loss: 0.15817230939865112\n",
      "Epoch 4425, Loss: 0.33564936369657516, Final Batch Loss: 0.12272116541862488\n",
      "Epoch 4426, Loss: 0.3249322846531868, Final Batch Loss: 0.10777485370635986\n",
      "Epoch 4427, Loss: 0.43046194314956665, Final Batch Loss: 0.1699618548154831\n",
      "Epoch 4428, Loss: 0.285400353372097, Final Batch Loss: 0.1088513433933258\n",
      "Epoch 4429, Loss: 0.33328500390052795, Final Batch Loss: 0.14188256859779358\n",
      "Epoch 4430, Loss: 0.2828003391623497, Final Batch Loss: 0.07026036083698273\n",
      "Epoch 4431, Loss: 0.5208213031291962, Final Batch Loss: 0.1324727088212967\n",
      "Epoch 4432, Loss: 0.3728209510445595, Final Batch Loss: 0.11815762519836426\n",
      "Epoch 4433, Loss: 0.3210052475333214, Final Batch Loss: 0.12467503547668457\n",
      "Epoch 4434, Loss: 0.3374352753162384, Final Batch Loss: 0.10085977613925934\n",
      "Epoch 4435, Loss: 0.388163797557354, Final Batch Loss: 0.1309608519077301\n",
      "Epoch 4436, Loss: 0.49299611896276474, Final Batch Loss: 0.08381026238203049\n",
      "Epoch 4437, Loss: 0.4193871319293976, Final Batch Loss: 0.1778946965932846\n",
      "Epoch 4438, Loss: 0.47472353279590607, Final Batch Loss: 0.1373974084854126\n",
      "Epoch 4439, Loss: 0.4637068510055542, Final Batch Loss: 0.17986321449279785\n",
      "Epoch 4440, Loss: 0.4309823364019394, Final Batch Loss: 0.11013779789209366\n",
      "Epoch 4441, Loss: 0.40767137706279755, Final Batch Loss: 0.14139360189437866\n",
      "Epoch 4442, Loss: 0.5058232992887497, Final Batch Loss: 0.1366884559392929\n",
      "Epoch 4443, Loss: 0.36097534000873566, Final Batch Loss: 0.11189503222703934\n",
      "Epoch 4444, Loss: 0.4340931251645088, Final Batch Loss: 0.10034780949354172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4445, Loss: 0.4597736597061157, Final Batch Loss: 0.12050870060920715\n",
      "Epoch 4446, Loss: 0.3806557059288025, Final Batch Loss: 0.15403971076011658\n",
      "Epoch 4447, Loss: 0.334081694483757, Final Batch Loss: 0.1044871062040329\n",
      "Epoch 4448, Loss: 0.4150414317846298, Final Batch Loss: 0.14252863824367523\n",
      "Epoch 4449, Loss: 0.3665856868028641, Final Batch Loss: 0.08008439838886261\n",
      "Epoch 4450, Loss: 0.41674043238162994, Final Batch Loss: 0.1807517260313034\n",
      "Epoch 4451, Loss: 0.39102766662836075, Final Batch Loss: 0.1239304319024086\n",
      "Epoch 4452, Loss: 0.4624306261539459, Final Batch Loss: 0.14448848366737366\n",
      "Epoch 4453, Loss: 0.512330949306488, Final Batch Loss: 0.1610879749059677\n",
      "Epoch 4454, Loss: 0.45984310656785965, Final Batch Loss: 0.10321258753538132\n",
      "Epoch 4455, Loss: 0.37567172199487686, Final Batch Loss: 0.06914366781711578\n",
      "Epoch 4456, Loss: 0.5051219686865807, Final Batch Loss: 0.15211588144302368\n",
      "Epoch 4457, Loss: 0.3165453374385834, Final Batch Loss: 0.10899584740400314\n",
      "Epoch 4458, Loss: 0.6902842372655869, Final Batch Loss: 0.30974677205085754\n",
      "Epoch 4459, Loss: 0.3959556519985199, Final Batch Loss: 0.1347055733203888\n",
      "Epoch 4460, Loss: 0.3986119255423546, Final Batch Loss: 0.09035556763410568\n",
      "Epoch 4461, Loss: 0.4874016344547272, Final Batch Loss: 0.11776231229305267\n",
      "Epoch 4462, Loss: 0.4182639941573143, Final Batch Loss: 0.16311395168304443\n",
      "Epoch 4463, Loss: 0.4471013993024826, Final Batch Loss: 0.08638474345207214\n",
      "Epoch 4464, Loss: 0.40177227556705475, Final Batch Loss: 0.13275034725666046\n",
      "Epoch 4465, Loss: 0.3802000507712364, Final Batch Loss: 0.1586470603942871\n",
      "Epoch 4466, Loss: 0.41664736717939377, Final Batch Loss: 0.1495954543352127\n",
      "Epoch 4467, Loss: 0.46688854694366455, Final Batch Loss: 0.18261738121509552\n",
      "Epoch 4468, Loss: 0.39134176075458527, Final Batch Loss: 0.12423451989889145\n",
      "Epoch 4469, Loss: 0.38839006423950195, Final Batch Loss: 0.11385365575551987\n",
      "Epoch 4470, Loss: 0.3445998728275299, Final Batch Loss: 0.13564084470272064\n",
      "Epoch 4471, Loss: 0.4004666954278946, Final Batch Loss: 0.11241429299116135\n",
      "Epoch 4472, Loss: 0.3390408083796501, Final Batch Loss: 0.1257971078157425\n",
      "Epoch 4473, Loss: 0.37714437395334244, Final Batch Loss: 0.09543552249670029\n",
      "Epoch 4474, Loss: 0.33568190038204193, Final Batch Loss: 0.10922326892614365\n",
      "Epoch 4475, Loss: 0.34660953283309937, Final Batch Loss: 0.0809871256351471\n",
      "Epoch 4476, Loss: 0.3952644318342209, Final Batch Loss: 0.1299581527709961\n",
      "Epoch 4477, Loss: 0.3894824758172035, Final Batch Loss: 0.15121059119701385\n",
      "Epoch 4478, Loss: 0.36595071107149124, Final Batch Loss: 0.11359559744596481\n",
      "Epoch 4479, Loss: 0.35109201073646545, Final Batch Loss: 0.12955978512763977\n",
      "Epoch 4480, Loss: 0.4763142913579941, Final Batch Loss: 0.17733986675739288\n",
      "Epoch 4481, Loss: 0.4524455666542053, Final Batch Loss: 0.21179969608783722\n",
      "Epoch 4482, Loss: 0.3472307622432709, Final Batch Loss: 0.1077072024345398\n",
      "Epoch 4483, Loss: 0.3920962065458298, Final Batch Loss: 0.12368328869342804\n",
      "Epoch 4484, Loss: 0.3310384675860405, Final Batch Loss: 0.09080753475427628\n",
      "Epoch 4485, Loss: 0.37271786481142044, Final Batch Loss: 0.13400477170944214\n",
      "Epoch 4486, Loss: 0.40180034935474396, Final Batch Loss: 0.15220782160758972\n",
      "Epoch 4487, Loss: 0.3071948289871216, Final Batch Loss: 0.09588809311389923\n",
      "Epoch 4488, Loss: 0.4384559988975525, Final Batch Loss: 0.15570533275604248\n",
      "Epoch 4489, Loss: 0.33643609285354614, Final Batch Loss: 0.08111538738012314\n",
      "Epoch 4490, Loss: 0.31651999056339264, Final Batch Loss: 0.12533316016197205\n",
      "Epoch 4491, Loss: 0.3901642858982086, Final Batch Loss: 0.16612409055233002\n",
      "Epoch 4492, Loss: 0.36222682148218155, Final Batch Loss: 0.1443924754858017\n",
      "Epoch 4493, Loss: 0.4009455516934395, Final Batch Loss: 0.16102734208106995\n",
      "Epoch 4494, Loss: 0.3802390322089195, Final Batch Loss: 0.11966987699270248\n",
      "Epoch 4495, Loss: 0.4218343198299408, Final Batch Loss: 0.16620630025863647\n",
      "Epoch 4496, Loss: 0.4948424696922302, Final Batch Loss: 0.23715750873088837\n",
      "Epoch 4497, Loss: 0.3487587571144104, Final Batch Loss: 0.14922760426998138\n",
      "Epoch 4498, Loss: 0.36210622638463974, Final Batch Loss: 0.1631748080253601\n",
      "Epoch 4499, Loss: 0.33112289756536484, Final Batch Loss: 0.11488969624042511\n",
      "Epoch 4500, Loss: 0.3811488449573517, Final Batch Loss: 0.12698514759540558\n",
      "Epoch 4501, Loss: 0.3512996882200241, Final Batch Loss: 0.1120210662484169\n",
      "Epoch 4502, Loss: 0.40329186618328094, Final Batch Loss: 0.10438937693834305\n",
      "Epoch 4503, Loss: 0.2885920852422714, Final Batch Loss: 0.10082641243934631\n",
      "Epoch 4504, Loss: 0.4367200583219528, Final Batch Loss: 0.19360771775245667\n",
      "Epoch 4505, Loss: 0.37530021369457245, Final Batch Loss: 0.1801462173461914\n",
      "Epoch 4506, Loss: 0.4843563437461853, Final Batch Loss: 0.1780078262090683\n",
      "Epoch 4507, Loss: 0.33346473425626755, Final Batch Loss: 0.1285725086927414\n",
      "Epoch 4508, Loss: 0.36351584643125534, Final Batch Loss: 0.17140096426010132\n",
      "Epoch 4509, Loss: 0.4002876430749893, Final Batch Loss: 0.08715683221817017\n",
      "Epoch 4510, Loss: 0.3770054876804352, Final Batch Loss: 0.12549778819084167\n",
      "Epoch 4511, Loss: 0.47930964827537537, Final Batch Loss: 0.14161746203899384\n",
      "Epoch 4512, Loss: 0.3573364093899727, Final Batch Loss: 0.1688491702079773\n",
      "Epoch 4513, Loss: 0.4113154113292694, Final Batch Loss: 0.12240669876337051\n",
      "Epoch 4514, Loss: 0.5464708358049393, Final Batch Loss: 0.23679840564727783\n",
      "Epoch 4515, Loss: 0.3977220728993416, Final Batch Loss: 0.17025192081928253\n",
      "Epoch 4516, Loss: 0.3905048966407776, Final Batch Loss: 0.14405933022499084\n",
      "Epoch 4517, Loss: 0.31779634207487106, Final Batch Loss: 0.11510559916496277\n",
      "Epoch 4518, Loss: 0.33094535022974014, Final Batch Loss: 0.0809001624584198\n",
      "Epoch 4519, Loss: 0.38626766204833984, Final Batch Loss: 0.10984852910041809\n",
      "Epoch 4520, Loss: 0.41107990965247154, Final Batch Loss: 0.1750030219554901\n",
      "Epoch 4521, Loss: 0.30948346853256226, Final Batch Loss: 0.09048440307378769\n",
      "Epoch 4522, Loss: 0.37338869273662567, Final Batch Loss: 0.1350797712802887\n",
      "Epoch 4523, Loss: 0.4158932864665985, Final Batch Loss: 0.10942137241363525\n",
      "Epoch 4524, Loss: 0.4120825529098511, Final Batch Loss: 0.16635426878929138\n",
      "Epoch 4525, Loss: 0.5069801658391953, Final Batch Loss: 0.15342779457569122\n",
      "Epoch 4526, Loss: 0.40743159502744675, Final Batch Loss: 0.1135317012667656\n",
      "Epoch 4527, Loss: 0.4812227636575699, Final Batch Loss: 0.17411206662654877\n",
      "Epoch 4528, Loss: 0.3652840033173561, Final Batch Loss: 0.1180378720164299\n",
      "Epoch 4529, Loss: 0.4093814939260483, Final Batch Loss: 0.0964183360338211\n",
      "Epoch 4530, Loss: 0.4089841917157173, Final Batch Loss: 0.11246771365404129\n",
      "Epoch 4531, Loss: 0.3182321712374687, Final Batch Loss: 0.08684027940034866\n",
      "Epoch 4532, Loss: 0.3841560110449791, Final Batch Loss: 0.15501342713832855\n",
      "Epoch 4533, Loss: 0.46456319838762283, Final Batch Loss: 0.24849997460842133\n",
      "Epoch 4534, Loss: 0.383567675948143, Final Batch Loss: 0.1583968549966812\n",
      "Epoch 4535, Loss: 0.3666667491197586, Final Batch Loss: 0.11321613937616348\n",
      "Epoch 4536, Loss: 0.34085162729024887, Final Batch Loss: 0.15693895518779755\n",
      "Epoch 4537, Loss: 0.49277108907699585, Final Batch Loss: 0.1629611700773239\n",
      "Epoch 4538, Loss: 0.469288669526577, Final Batch Loss: 0.19405825436115265\n",
      "Epoch 4539, Loss: 0.3540823236107826, Final Batch Loss: 0.12342609465122223\n",
      "Epoch 4540, Loss: 0.3433428332209587, Final Batch Loss: 0.11528728902339935\n",
      "Epoch 4541, Loss: 0.3827323243021965, Final Batch Loss: 0.13167966902256012\n",
      "Epoch 4542, Loss: 0.3581037372350693, Final Batch Loss: 0.11250211298465729\n",
      "Epoch 4543, Loss: 0.459119513630867, Final Batch Loss: 0.06351827085018158\n",
      "Epoch 4544, Loss: 0.404341883957386, Final Batch Loss: 0.11560370773077011\n",
      "Epoch 4545, Loss: 0.38621190190315247, Final Batch Loss: 0.15966692566871643\n",
      "Epoch 4546, Loss: 0.3304916098713875, Final Batch Loss: 0.12218189984560013\n",
      "Epoch 4547, Loss: 0.3035593703389168, Final Batch Loss: 0.10766827315092087\n",
      "Epoch 4548, Loss: 0.4458075314760208, Final Batch Loss: 0.2146955132484436\n",
      "Epoch 4549, Loss: 0.40345508605241776, Final Batch Loss: 0.1771058440208435\n",
      "Epoch 4550, Loss: 0.31139035895466805, Final Batch Loss: 0.05749927833676338\n",
      "Epoch 4551, Loss: 0.29441552609205246, Final Batch Loss: 0.07596646249294281\n",
      "Epoch 4552, Loss: 0.31988680362701416, Final Batch Loss: 0.12215760350227356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4553, Loss: 0.37874704599380493, Final Batch Loss: 0.10177046060562134\n",
      "Epoch 4554, Loss: 0.3949616625905037, Final Batch Loss: 0.14818914234638214\n",
      "Epoch 4555, Loss: 0.4142145365476608, Final Batch Loss: 0.16993819177150726\n",
      "Epoch 4556, Loss: 0.3412635996937752, Final Batch Loss: 0.11455035209655762\n",
      "Epoch 4557, Loss: 0.28269144892692566, Final Batch Loss: 0.06765570491552353\n",
      "Epoch 4558, Loss: 0.35490550845861435, Final Batch Loss: 0.12462027370929718\n",
      "Epoch 4559, Loss: 0.38837580382823944, Final Batch Loss: 0.1472565084695816\n",
      "Epoch 4560, Loss: 0.4021605849266052, Final Batch Loss: 0.12384320795536041\n",
      "Epoch 4561, Loss: 0.3422664664685726, Final Batch Loss: 0.05547645315527916\n",
      "Epoch 4562, Loss: 0.42552807182073593, Final Batch Loss: 0.08664726465940475\n",
      "Epoch 4563, Loss: 0.41677578538656235, Final Batch Loss: 0.10397433489561081\n",
      "Epoch 4564, Loss: 0.41489803045988083, Final Batch Loss: 0.10684642940759659\n",
      "Epoch 4565, Loss: 0.4241664856672287, Final Batch Loss: 0.11298304796218872\n",
      "Epoch 4566, Loss: 0.3837667182087898, Final Batch Loss: 0.15912161767482758\n",
      "Epoch 4567, Loss: 0.37824568897485733, Final Batch Loss: 0.13201619684696198\n",
      "Epoch 4568, Loss: 0.5122760683298111, Final Batch Loss: 0.1756712794303894\n",
      "Epoch 4569, Loss: 0.3929563835263252, Final Batch Loss: 0.09967301040887833\n",
      "Epoch 4570, Loss: 0.4744368940591812, Final Batch Loss: 0.17432008683681488\n",
      "Epoch 4571, Loss: 0.40061479806900024, Final Batch Loss: 0.13690122961997986\n",
      "Epoch 4572, Loss: 0.35984377562999725, Final Batch Loss: 0.09161469340324402\n",
      "Epoch 4573, Loss: 0.3858906105160713, Final Batch Loss: 0.11946425586938858\n",
      "Epoch 4574, Loss: 0.4207450672984123, Final Batch Loss: 0.16723373532295227\n",
      "Epoch 4575, Loss: 0.40073103457689285, Final Batch Loss: 0.1550956517457962\n",
      "Epoch 4576, Loss: 0.3310961648821831, Final Batch Loss: 0.11331336945295334\n",
      "Epoch 4577, Loss: 0.36621589213609695, Final Batch Loss: 0.16068826615810394\n",
      "Epoch 4578, Loss: 0.36943093687295914, Final Batch Loss: 0.08826901763677597\n",
      "Epoch 4579, Loss: 0.44234748929739, Final Batch Loss: 0.1246645525097847\n",
      "Epoch 4580, Loss: 0.34733927994966507, Final Batch Loss: 0.13434664905071259\n",
      "Epoch 4581, Loss: 0.3628604859113693, Final Batch Loss: 0.1752857118844986\n",
      "Epoch 4582, Loss: 0.40608949959278107, Final Batch Loss: 0.10927233099937439\n",
      "Epoch 4583, Loss: 0.36101703345775604, Final Batch Loss: 0.10275793820619583\n",
      "Epoch 4584, Loss: 0.45735399425029755, Final Batch Loss: 0.1883222907781601\n",
      "Epoch 4585, Loss: 0.4416477307677269, Final Batch Loss: 0.18619130551815033\n",
      "Epoch 4586, Loss: 0.38288524746894836, Final Batch Loss: 0.09647925198078156\n",
      "Epoch 4587, Loss: 0.23819124698638916, Final Batch Loss: 0.0769897848367691\n",
      "Epoch 4588, Loss: 0.39242203533649445, Final Batch Loss: 0.11219249665737152\n",
      "Epoch 4589, Loss: 0.3698706701397896, Final Batch Loss: 0.1332237273454666\n",
      "Epoch 4590, Loss: 0.35919545590877533, Final Batch Loss: 0.12180031090974808\n",
      "Epoch 4591, Loss: 0.3686436116695404, Final Batch Loss: 0.12443472445011139\n",
      "Epoch 4592, Loss: 0.4082869663834572, Final Batch Loss: 0.1862182319164276\n",
      "Epoch 4593, Loss: 0.43233080208301544, Final Batch Loss: 0.11069203913211823\n",
      "Epoch 4594, Loss: 0.3763623982667923, Final Batch Loss: 0.09610182791948318\n",
      "Epoch 4595, Loss: 0.32426954060792923, Final Batch Loss: 0.06544370204210281\n",
      "Epoch 4596, Loss: 0.31216277182102203, Final Batch Loss: 0.11626339703798294\n",
      "Epoch 4597, Loss: 0.31045883893966675, Final Batch Loss: 0.06508360058069229\n",
      "Epoch 4598, Loss: 0.36179082095623016, Final Batch Loss: 0.09067673236131668\n",
      "Epoch 4599, Loss: 0.3470936492085457, Final Batch Loss: 0.09675727039575577\n",
      "Epoch 4600, Loss: 0.4556909650564194, Final Batch Loss: 0.16696731746196747\n",
      "Epoch 4601, Loss: 0.34597450494766235, Final Batch Loss: 0.08097164332866669\n",
      "Epoch 4602, Loss: 0.40046028792858124, Final Batch Loss: 0.11699070036411285\n",
      "Epoch 4603, Loss: 0.37827470153570175, Final Batch Loss: 0.11823011934757233\n",
      "Epoch 4604, Loss: 0.3382980599999428, Final Batch Loss: 0.1167079359292984\n",
      "Epoch 4605, Loss: 0.4453129470348358, Final Batch Loss: 0.17126218974590302\n",
      "Epoch 4606, Loss: 0.3434963971376419, Final Batch Loss: 0.10104886442422867\n",
      "Epoch 4607, Loss: 0.3025806322693825, Final Batch Loss: 0.10976656526327133\n",
      "Epoch 4608, Loss: 0.3909008577466011, Final Batch Loss: 0.16191591322422028\n",
      "Epoch 4609, Loss: 0.4198540151119232, Final Batch Loss: 0.16480569541454315\n",
      "Epoch 4610, Loss: 0.3720560446381569, Final Batch Loss: 0.11979630589485168\n",
      "Epoch 4611, Loss: 0.3903530538082123, Final Batch Loss: 0.12874077260494232\n",
      "Epoch 4612, Loss: 0.4130188822746277, Final Batch Loss: 0.09998750686645508\n",
      "Epoch 4613, Loss: 0.4472835808992386, Final Batch Loss: 0.13677076995372772\n",
      "Epoch 4614, Loss: 0.5276414901018143, Final Batch Loss: 0.20349450409412384\n",
      "Epoch 4615, Loss: 0.34388188272714615, Final Batch Loss: 0.11179932206869125\n",
      "Epoch 4616, Loss: 0.35250124335289, Final Batch Loss: 0.10338373482227325\n",
      "Epoch 4617, Loss: 0.46829216182231903, Final Batch Loss: 0.21998658776283264\n",
      "Epoch 4618, Loss: 0.25064124166965485, Final Batch Loss: 0.08075392246246338\n",
      "Epoch 4619, Loss: 0.3382882699370384, Final Batch Loss: 0.10885234922170639\n",
      "Epoch 4620, Loss: 0.4421394467353821, Final Batch Loss: 0.16025203466415405\n",
      "Epoch 4621, Loss: 0.38174498826265335, Final Batch Loss: 0.14402681589126587\n",
      "Epoch 4622, Loss: 0.43041105568408966, Final Batch Loss: 0.1508430689573288\n",
      "Epoch 4623, Loss: 0.34149879217147827, Final Batch Loss: 0.11039194464683533\n",
      "Epoch 4624, Loss: 0.445047527551651, Final Batch Loss: 0.16451707482337952\n",
      "Epoch 4625, Loss: 0.3873308375477791, Final Batch Loss: 0.13607987761497498\n",
      "Epoch 4626, Loss: 0.39833514392375946, Final Batch Loss: 0.12597031891345978\n",
      "Epoch 4627, Loss: 0.3554224818944931, Final Batch Loss: 0.1440238654613495\n",
      "Epoch 4628, Loss: 0.4195731356739998, Final Batch Loss: 0.13916006684303284\n",
      "Epoch 4629, Loss: 0.4309050291776657, Final Batch Loss: 0.17376600205898285\n",
      "Epoch 4630, Loss: 0.37167155742645264, Final Batch Loss: 0.13935647904872894\n",
      "Epoch 4631, Loss: 0.3888334482908249, Final Batch Loss: 0.19805343449115753\n",
      "Epoch 4632, Loss: 0.33667463064193726, Final Batch Loss: 0.10361751914024353\n",
      "Epoch 4633, Loss: 0.35529469698667526, Final Batch Loss: 0.10436117649078369\n",
      "Epoch 4634, Loss: 0.3591552749276161, Final Batch Loss: 0.12100418657064438\n",
      "Epoch 4635, Loss: 0.48968199640512466, Final Batch Loss: 0.27908626198768616\n",
      "Epoch 4636, Loss: 0.38275032490491867, Final Batch Loss: 0.14267612993717194\n",
      "Epoch 4637, Loss: 0.3330616131424904, Final Batch Loss: 0.08648651093244553\n",
      "Epoch 4638, Loss: 0.4213552922010422, Final Batch Loss: 0.19252395629882812\n",
      "Epoch 4639, Loss: 0.40732527524232864, Final Batch Loss: 0.19041280448436737\n",
      "Epoch 4640, Loss: 0.373431496322155, Final Batch Loss: 0.1459653526544571\n",
      "Epoch 4641, Loss: 0.3001195266842842, Final Batch Loss: 0.11742052435874939\n",
      "Epoch 4642, Loss: 0.31338977068662643, Final Batch Loss: 0.11010272800922394\n",
      "Epoch 4643, Loss: 0.3372054174542427, Final Batch Loss: 0.14607267081737518\n",
      "Epoch 4644, Loss: 0.3043031021952629, Final Batch Loss: 0.10676640272140503\n",
      "Epoch 4645, Loss: 0.5073618069291115, Final Batch Loss: 0.24117450416088104\n",
      "Epoch 4646, Loss: 0.4884808361530304, Final Batch Loss: 0.14402765035629272\n",
      "Epoch 4647, Loss: 0.33607330173254013, Final Batch Loss: 0.13126462697982788\n",
      "Epoch 4648, Loss: 0.4198926314711571, Final Batch Loss: 0.17933869361877441\n",
      "Epoch 4649, Loss: 0.34923287481069565, Final Batch Loss: 0.10121242702007294\n",
      "Epoch 4650, Loss: 0.4303199350833893, Final Batch Loss: 0.16647858917713165\n",
      "Epoch 4651, Loss: 0.3214787691831589, Final Batch Loss: 0.10780578851699829\n",
      "Epoch 4652, Loss: 0.43883564323186874, Final Batch Loss: 0.17043110728263855\n",
      "Epoch 4653, Loss: 0.4232071191072464, Final Batch Loss: 0.11334887146949768\n",
      "Epoch 4654, Loss: 0.43022068589925766, Final Batch Loss: 0.15668419003486633\n",
      "Epoch 4655, Loss: 0.398770734667778, Final Batch Loss: 0.15706267952919006\n",
      "Epoch 4656, Loss: 0.5045774430036545, Final Batch Loss: 0.15990392863750458\n",
      "Epoch 4657, Loss: 0.35400815308094025, Final Batch Loss: 0.09443838894367218\n",
      "Epoch 4658, Loss: 0.33355800807476044, Final Batch Loss: 0.12988343834877014\n",
      "Epoch 4659, Loss: 0.3293217495083809, Final Batch Loss: 0.11160747706890106\n",
      "Epoch 4660, Loss: 0.37127677351236343, Final Batch Loss: 0.106190524995327\n",
      "Epoch 4661, Loss: 0.333176389336586, Final Batch Loss: 0.0939624235033989\n",
      "Epoch 4662, Loss: 0.40116146951913834, Final Batch Loss: 0.17067821323871613\n",
      "Epoch 4663, Loss: 0.37815603613853455, Final Batch Loss: 0.13898707926273346\n",
      "Epoch 4664, Loss: 0.5021120458841324, Final Batch Loss: 0.22478562593460083\n",
      "Epoch 4665, Loss: 0.36820075660943985, Final Batch Loss: 0.14523470401763916\n",
      "Epoch 4666, Loss: 0.4440910369157791, Final Batch Loss: 0.14761535823345184\n",
      "Epoch 4667, Loss: 0.41981812566518784, Final Batch Loss: 0.16653531789779663\n",
      "Epoch 4668, Loss: 0.5263270065188408, Final Batch Loss: 0.2688659131526947\n",
      "Epoch 4669, Loss: 0.397954985499382, Final Batch Loss: 0.0988144800066948\n",
      "Epoch 4670, Loss: 0.31902386248111725, Final Batch Loss: 0.1636846512556076\n",
      "Epoch 4671, Loss: 0.44196081161499023, Final Batch Loss: 0.19328442215919495\n",
      "Epoch 4672, Loss: 0.39061975851655006, Final Batch Loss: 0.05997519567608833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4673, Loss: 0.3310749679803848, Final Batch Loss: 0.07908979058265686\n",
      "Epoch 4674, Loss: 0.35434920340776443, Final Batch Loss: 0.12960731983184814\n",
      "Epoch 4675, Loss: 0.40115098655223846, Final Batch Loss: 0.14497706294059753\n",
      "Epoch 4676, Loss: 0.43268781155347824, Final Batch Loss: 0.2064066231250763\n",
      "Epoch 4677, Loss: 0.39723309874534607, Final Batch Loss: 0.10643480718135834\n",
      "Epoch 4678, Loss: 0.34236907213926315, Final Batch Loss: 0.11903718113899231\n",
      "Epoch 4679, Loss: 0.4783296212553978, Final Batch Loss: 0.2033974528312683\n",
      "Epoch 4680, Loss: 0.36340948939323425, Final Batch Loss: 0.07128633558750153\n",
      "Epoch 4681, Loss: 0.4608820676803589, Final Batch Loss: 0.17463715374469757\n",
      "Epoch 4682, Loss: 0.43178944289684296, Final Batch Loss: 0.1800350844860077\n",
      "Epoch 4683, Loss: 0.5304381847381592, Final Batch Loss: 0.18763957917690277\n",
      "Epoch 4684, Loss: 0.39710814505815506, Final Batch Loss: 0.10937323421239853\n",
      "Epoch 4685, Loss: 0.40269211679697037, Final Batch Loss: 0.14883364737033844\n",
      "Epoch 4686, Loss: 0.43486257642507553, Final Batch Loss: 0.15083061158657074\n",
      "Epoch 4687, Loss: 0.4627574682235718, Final Batch Loss: 0.19946815073490143\n",
      "Epoch 4688, Loss: 0.4841092526912689, Final Batch Loss: 0.1382186859846115\n",
      "Epoch 4689, Loss: 0.5238697230815887, Final Batch Loss: 0.19506894052028656\n",
      "Epoch 4690, Loss: 0.4272908642888069, Final Batch Loss: 0.17012828588485718\n",
      "Epoch 4691, Loss: 0.29631292819976807, Final Batch Loss: 0.12432307004928589\n",
      "Epoch 4692, Loss: 0.3657076731324196, Final Batch Loss: 0.11812985688447952\n",
      "Epoch 4693, Loss: 0.4224280044436455, Final Batch Loss: 0.1898922175168991\n",
      "Epoch 4694, Loss: 0.3806687816977501, Final Batch Loss: 0.11196696013212204\n",
      "Epoch 4695, Loss: 0.3207940571010113, Final Batch Loss: 0.05925319716334343\n",
      "Epoch 4696, Loss: 0.30349036306142807, Final Batch Loss: 0.07252418249845505\n",
      "Epoch 4697, Loss: 0.31346552819013596, Final Batch Loss: 0.11735216528177261\n",
      "Epoch 4698, Loss: 0.41129277646541595, Final Batch Loss: 0.16290132701396942\n",
      "Epoch 4699, Loss: 0.4656768813729286, Final Batch Loss: 0.07932419329881668\n",
      "Epoch 4700, Loss: 0.3408486545085907, Final Batch Loss: 0.09544672816991806\n",
      "Epoch 4701, Loss: 0.3503720313310623, Final Batch Loss: 0.11108601838350296\n",
      "Epoch 4702, Loss: 0.3348350524902344, Final Batch Loss: 0.11142105609178543\n",
      "Epoch 4703, Loss: 0.3577192947268486, Final Batch Loss: 0.11596445739269257\n",
      "Epoch 4704, Loss: 0.3997846841812134, Final Batch Loss: 0.11951951682567596\n",
      "Epoch 4705, Loss: 0.4178420528769493, Final Batch Loss: 0.2075621783733368\n",
      "Epoch 4706, Loss: 0.3524257242679596, Final Batch Loss: 0.11775380373001099\n",
      "Epoch 4707, Loss: 0.4109038636088371, Final Batch Loss: 0.07733490318059921\n",
      "Epoch 4708, Loss: 0.40468641370534897, Final Batch Loss: 0.08833148330450058\n",
      "Epoch 4709, Loss: 0.4542774185538292, Final Batch Loss: 0.18617591261863708\n",
      "Epoch 4710, Loss: 0.30622102320194244, Final Batch Loss: 0.09067922830581665\n",
      "Epoch 4711, Loss: 0.35247568786144257, Final Batch Loss: 0.15751396119594574\n",
      "Epoch 4712, Loss: 0.46782857924699783, Final Batch Loss: 0.11832725256681442\n",
      "Epoch 4713, Loss: 0.38337980955839157, Final Batch Loss: 0.130313903093338\n",
      "Epoch 4714, Loss: 0.44245579093694687, Final Batch Loss: 0.2105298936367035\n",
      "Epoch 4715, Loss: 0.3605169951915741, Final Batch Loss: 0.07750426232814789\n",
      "Epoch 4716, Loss: 0.38771406561136246, Final Batch Loss: 0.14637094736099243\n",
      "Epoch 4717, Loss: 0.4117462858557701, Final Batch Loss: 0.12203013151884079\n",
      "Epoch 4718, Loss: 0.4934500753879547, Final Batch Loss: 0.17322516441345215\n",
      "Epoch 4719, Loss: 0.4157435968518257, Final Batch Loss: 0.15862131118774414\n",
      "Epoch 4720, Loss: 0.41574839502573013, Final Batch Loss: 0.15905055403709412\n",
      "Epoch 4721, Loss: 0.3452686071395874, Final Batch Loss: 0.13709937036037445\n",
      "Epoch 4722, Loss: 0.41604427248239517, Final Batch Loss: 0.14857156574726105\n",
      "Epoch 4723, Loss: 0.3588536009192467, Final Batch Loss: 0.11442141234874725\n",
      "Epoch 4724, Loss: 0.3180706724524498, Final Batch Loss: 0.08706361055374146\n",
      "Epoch 4725, Loss: 0.32989296317100525, Final Batch Loss: 0.10819850862026215\n",
      "Epoch 4726, Loss: 0.316568061709404, Final Batch Loss: 0.09345448017120361\n",
      "Epoch 4727, Loss: 0.4720214083790779, Final Batch Loss: 0.1968436986207962\n",
      "Epoch 4728, Loss: 0.36434822529554367, Final Batch Loss: 0.12720273435115814\n",
      "Epoch 4729, Loss: 0.35333213955163956, Final Batch Loss: 0.1446351706981659\n",
      "Epoch 4730, Loss: 0.4301169067621231, Final Batch Loss: 0.07838606834411621\n",
      "Epoch 4731, Loss: 0.39517155289649963, Final Batch Loss: 0.17300094664096832\n",
      "Epoch 4732, Loss: 0.4202933609485626, Final Batch Loss: 0.1742638498544693\n",
      "Epoch 4733, Loss: 0.35019420832395554, Final Batch Loss: 0.11973115056753159\n",
      "Epoch 4734, Loss: 0.3147916719317436, Final Batch Loss: 0.10625497251749039\n",
      "Epoch 4735, Loss: 0.3267108127474785, Final Batch Loss: 0.09166188538074493\n",
      "Epoch 4736, Loss: 0.37702223658561707, Final Batch Loss: 0.19185592234134674\n",
      "Epoch 4737, Loss: 0.41309620440006256, Final Batch Loss: 0.12683068215847015\n",
      "Epoch 4738, Loss: 0.40748515725135803, Final Batch Loss: 0.12741035223007202\n",
      "Epoch 4739, Loss: 0.37681709229946136, Final Batch Loss: 0.13342493772506714\n",
      "Epoch 4740, Loss: 0.3451554775238037, Final Batch Loss: 0.12572690844535828\n",
      "Epoch 4741, Loss: 0.44712428748607635, Final Batch Loss: 0.14971022307872772\n",
      "Epoch 4742, Loss: 0.3701908700168133, Final Batch Loss: 0.1381993144750595\n",
      "Epoch 4743, Loss: 0.3585169166326523, Final Batch Loss: 0.10853496938943863\n",
      "Epoch 4744, Loss: 0.4606126844882965, Final Batch Loss: 0.1668122112751007\n",
      "Epoch 4745, Loss: 0.42244692891836166, Final Batch Loss: 0.09752447158098221\n",
      "Epoch 4746, Loss: 0.33744753897190094, Final Batch Loss: 0.1314508318901062\n",
      "Epoch 4747, Loss: 0.3939886540174484, Final Batch Loss: 0.11895474791526794\n",
      "Epoch 4748, Loss: 0.3289989233016968, Final Batch Loss: 0.12847767770290375\n",
      "Epoch 4749, Loss: 0.3699936270713806, Final Batch Loss: 0.10791273415088654\n",
      "Epoch 4750, Loss: 0.46836434304714203, Final Batch Loss: 0.15037479996681213\n",
      "Epoch 4751, Loss: 0.4456402212381363, Final Batch Loss: 0.1740039587020874\n",
      "Epoch 4752, Loss: 0.3990621194243431, Final Batch Loss: 0.16080836951732635\n",
      "Epoch 4753, Loss: 0.461948499083519, Final Batch Loss: 0.08097255229949951\n",
      "Epoch 4754, Loss: 0.30294185876846313, Final Batch Loss: 0.1228286623954773\n",
      "Epoch 4755, Loss: 0.3659738823771477, Final Batch Loss: 0.14030148088932037\n",
      "Epoch 4756, Loss: 0.43308955430984497, Final Batch Loss: 0.17204198241233826\n",
      "Epoch 4757, Loss: 0.29670391231775284, Final Batch Loss: 0.11347172409296036\n",
      "Epoch 4758, Loss: 0.3659190386533737, Final Batch Loss: 0.10599370300769806\n",
      "Epoch 4759, Loss: 0.33791114389896393, Final Batch Loss: 0.09382043778896332\n",
      "Epoch 4760, Loss: 0.33801180869340897, Final Batch Loss: 0.10474207252264023\n",
      "Epoch 4761, Loss: 0.3325571119785309, Final Batch Loss: 0.09977295994758606\n",
      "Epoch 4762, Loss: 0.36859582364559174, Final Batch Loss: 0.14983169734477997\n",
      "Epoch 4763, Loss: 0.3489895984530449, Final Batch Loss: 0.12257182598114014\n",
      "Epoch 4764, Loss: 0.3979982063174248, Final Batch Loss: 0.1638709157705307\n",
      "Epoch 4765, Loss: 0.27701201289892197, Final Batch Loss: 0.08401316404342651\n",
      "Epoch 4766, Loss: 0.3809017166495323, Final Batch Loss: 0.09304279834032059\n",
      "Epoch 4767, Loss: 0.37964875251054764, Final Batch Loss: 0.06317630410194397\n",
      "Epoch 4768, Loss: 0.2769760414958, Final Batch Loss: 0.06474748998880386\n",
      "Epoch 4769, Loss: 0.41874201595783234, Final Batch Loss: 0.06746810674667358\n",
      "Epoch 4770, Loss: 0.3117770403623581, Final Batch Loss: 0.09631779044866562\n",
      "Epoch 4771, Loss: 0.3491957113146782, Final Batch Loss: 0.07334015518426895\n",
      "Epoch 4772, Loss: 0.438529372215271, Final Batch Loss: 0.1439490169286728\n",
      "Epoch 4773, Loss: 0.33149734884500504, Final Batch Loss: 0.11567758023738861\n",
      "Epoch 4774, Loss: 0.43083834648132324, Final Batch Loss: 0.14068913459777832\n",
      "Epoch 4775, Loss: 0.4696030616760254, Final Batch Loss: 0.1430482268333435\n",
      "Epoch 4776, Loss: 0.4074772968888283, Final Batch Loss: 0.14560885727405548\n",
      "Epoch 4777, Loss: 0.4093061611056328, Final Batch Loss: 0.14183610677719116\n",
      "Epoch 4778, Loss: 0.42595626413822174, Final Batch Loss: 0.14911441504955292\n",
      "Epoch 4779, Loss: 0.30836009979248047, Final Batch Loss: 0.08061233162879944\n",
      "Epoch 4780, Loss: 0.3598150834441185, Final Batch Loss: 0.12300471216440201\n",
      "Epoch 4781, Loss: 0.38797173649072647, Final Batch Loss: 0.07571249455213547\n",
      "Epoch 4782, Loss: 0.31677908450365067, Final Batch Loss: 0.08965323120355606\n",
      "Epoch 4783, Loss: 0.35444992780685425, Final Batch Loss: 0.12186435610055923\n",
      "Epoch 4784, Loss: 0.39724889397621155, Final Batch Loss: 0.16795749962329865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4785, Loss: 0.4264320284128189, Final Batch Loss: 0.14383138716220856\n",
      "Epoch 4786, Loss: 0.287615068256855, Final Batch Loss: 0.09015501290559769\n",
      "Epoch 4787, Loss: 0.3531683534383774, Final Batch Loss: 0.1029796153306961\n",
      "Epoch 4788, Loss: 0.34566911309957504, Final Batch Loss: 0.15054331719875336\n",
      "Epoch 4789, Loss: 0.3888108432292938, Final Batch Loss: 0.11209976673126221\n",
      "Epoch 4790, Loss: 0.29183299094438553, Final Batch Loss: 0.11718104034662247\n",
      "Epoch 4791, Loss: 0.32862284034490585, Final Batch Loss: 0.06402173638343811\n",
      "Epoch 4792, Loss: 0.314285047352314, Final Batch Loss: 0.08828192949295044\n",
      "Epoch 4793, Loss: 0.3617926687002182, Final Batch Loss: 0.15156219899654388\n",
      "Epoch 4794, Loss: 0.43209265172481537, Final Batch Loss: 0.15278971195220947\n",
      "Epoch 4795, Loss: 0.36003095656633377, Final Batch Loss: 0.10173676162958145\n",
      "Epoch 4796, Loss: 0.4202394112944603, Final Batch Loss: 0.18747316300868988\n",
      "Epoch 4797, Loss: 0.3609646409749985, Final Batch Loss: 0.10260409116744995\n",
      "Epoch 4798, Loss: 0.38981373608112335, Final Batch Loss: 0.16396455466747284\n",
      "Epoch 4799, Loss: 0.3536652401089668, Final Batch Loss: 0.0978332906961441\n",
      "Epoch 4800, Loss: 0.2714262269437313, Final Batch Loss: 0.09145017713308334\n",
      "Epoch 4801, Loss: 0.39538098871707916, Final Batch Loss: 0.1131892278790474\n",
      "Epoch 4802, Loss: 0.3638097196817398, Final Batch Loss: 0.12984691560268402\n",
      "Epoch 4803, Loss: 0.29368143528699875, Final Batch Loss: 0.07599923759698868\n",
      "Epoch 4804, Loss: 0.45170480012893677, Final Batch Loss: 0.17596803605556488\n",
      "Epoch 4805, Loss: 0.3766147568821907, Final Batch Loss: 0.13816654682159424\n",
      "Epoch 4806, Loss: 0.3460986986756325, Final Batch Loss: 0.11082495003938675\n",
      "Epoch 4807, Loss: 0.44440317153930664, Final Batch Loss: 0.1537497639656067\n",
      "Epoch 4808, Loss: 0.39779679477214813, Final Batch Loss: 0.18134304881095886\n",
      "Epoch 4809, Loss: 0.37074728310108185, Final Batch Loss: 0.14890168607234955\n",
      "Epoch 4810, Loss: 0.4025612249970436, Final Batch Loss: 0.15066154301166534\n",
      "Epoch 4811, Loss: 0.35096338391304016, Final Batch Loss: 0.12043297290802002\n",
      "Epoch 4812, Loss: 0.3894250839948654, Final Batch Loss: 0.07994139194488525\n",
      "Epoch 4813, Loss: 0.40195730328559875, Final Batch Loss: 0.14232878386974335\n",
      "Epoch 4814, Loss: 0.4494287744164467, Final Batch Loss: 0.17781879007816315\n",
      "Epoch 4815, Loss: 0.33940841257572174, Final Batch Loss: 0.11870504915714264\n",
      "Epoch 4816, Loss: 0.35884416103363037, Final Batch Loss: 0.08953922241926193\n",
      "Epoch 4817, Loss: 0.3239269554615021, Final Batch Loss: 0.13240425288677216\n",
      "Epoch 4818, Loss: 0.4300682321190834, Final Batch Loss: 0.18047340214252472\n",
      "Epoch 4819, Loss: 0.43841929733753204, Final Batch Loss: 0.14997977018356323\n",
      "Epoch 4820, Loss: 0.2875917926430702, Final Batch Loss: 0.10538627952337265\n",
      "Epoch 4821, Loss: 0.4615028649568558, Final Batch Loss: 0.15026208758354187\n",
      "Epoch 4822, Loss: 0.32921021431684494, Final Batch Loss: 0.12007047235965729\n",
      "Epoch 4823, Loss: 0.3658074662089348, Final Batch Loss: 0.16037839651107788\n",
      "Epoch 4824, Loss: 0.33561455458402634, Final Batch Loss: 0.045027412474155426\n",
      "Epoch 4825, Loss: 0.40314366668462753, Final Batch Loss: 0.1190548911690712\n",
      "Epoch 4826, Loss: 0.3330995813012123, Final Batch Loss: 0.14846031367778778\n",
      "Epoch 4827, Loss: 0.42640432715415955, Final Batch Loss: 0.13682812452316284\n",
      "Epoch 4828, Loss: 0.4679032489657402, Final Batch Loss: 0.208573579788208\n",
      "Epoch 4829, Loss: 0.4097716510295868, Final Batch Loss: 0.187990203499794\n",
      "Epoch 4830, Loss: 0.3785296753048897, Final Batch Loss: 0.0961088165640831\n",
      "Epoch 4831, Loss: 0.33624839037656784, Final Batch Loss: 0.0700814500451088\n",
      "Epoch 4832, Loss: 0.497398778796196, Final Batch Loss: 0.1480393260717392\n",
      "Epoch 4833, Loss: 0.3531910702586174, Final Batch Loss: 0.12668778002262115\n",
      "Epoch 4834, Loss: 0.28118355572223663, Final Batch Loss: 0.08765128254890442\n",
      "Epoch 4835, Loss: 0.4466669261455536, Final Batch Loss: 0.15895956754684448\n",
      "Epoch 4836, Loss: 0.3805641531944275, Final Batch Loss: 0.1536467969417572\n",
      "Epoch 4837, Loss: 0.35967598110437393, Final Batch Loss: 0.1456586867570877\n",
      "Epoch 4838, Loss: 0.4041043594479561, Final Batch Loss: 0.13755285739898682\n",
      "Epoch 4839, Loss: 0.3914223983883858, Final Batch Loss: 0.15291939675807953\n",
      "Epoch 4840, Loss: 0.4289775937795639, Final Batch Loss: 0.14707480370998383\n",
      "Epoch 4841, Loss: 0.33266468346118927, Final Batch Loss: 0.06994770467281342\n",
      "Epoch 4842, Loss: 0.2966773584485054, Final Batch Loss: 0.07586036622524261\n",
      "Epoch 4843, Loss: 0.3671923950314522, Final Batch Loss: 0.10945907235145569\n",
      "Epoch 4844, Loss: 0.29939763620495796, Final Batch Loss: 0.14220812916755676\n",
      "Epoch 4845, Loss: 0.34557168185710907, Final Batch Loss: 0.09671460837125778\n",
      "Epoch 4846, Loss: 0.35505832731723785, Final Batch Loss: 0.088629812002182\n",
      "Epoch 4847, Loss: 0.3782767578959465, Final Batch Loss: 0.12163467705249786\n",
      "Epoch 4848, Loss: 0.4173935651779175, Final Batch Loss: 0.12345913052558899\n",
      "Epoch 4849, Loss: 0.441298209130764, Final Batch Loss: 0.17557388544082642\n",
      "Epoch 4850, Loss: 0.27609027922153473, Final Batch Loss: 0.10825598239898682\n",
      "Epoch 4851, Loss: 0.3199029639363289, Final Batch Loss: 0.09319789707660675\n",
      "Epoch 4852, Loss: 0.3430415987968445, Final Batch Loss: 0.10398056358098984\n",
      "Epoch 4853, Loss: 0.3996560424566269, Final Batch Loss: 0.12486345320940018\n",
      "Epoch 4854, Loss: 0.4315531849861145, Final Batch Loss: 0.13336730003356934\n",
      "Epoch 4855, Loss: 0.4200832098722458, Final Batch Loss: 0.1404314637184143\n",
      "Epoch 4856, Loss: 0.35186193138360977, Final Batch Loss: 0.09457807242870331\n",
      "Epoch 4857, Loss: 0.3246772065758705, Final Batch Loss: 0.09992002695798874\n",
      "Epoch 4858, Loss: 0.34729643911123276, Final Batch Loss: 0.10272668302059174\n",
      "Epoch 4859, Loss: 0.35586074739694595, Final Batch Loss: 0.1444040834903717\n",
      "Epoch 4860, Loss: 0.28959739953279495, Final Batch Loss: 0.11173281073570251\n",
      "Epoch 4861, Loss: 0.4037110507488251, Final Batch Loss: 0.1317945420742035\n",
      "Epoch 4862, Loss: 0.2864038795232773, Final Batch Loss: 0.09113442152738571\n",
      "Epoch 4863, Loss: 0.4595925658941269, Final Batch Loss: 0.14589163661003113\n",
      "Epoch 4864, Loss: 0.4523707702755928, Final Batch Loss: 0.17913439869880676\n",
      "Epoch 4865, Loss: 0.41016943752765656, Final Batch Loss: 0.17084935307502747\n",
      "Epoch 4866, Loss: 0.42622484266757965, Final Batch Loss: 0.1845432072877884\n",
      "Epoch 4867, Loss: 0.33849384635686874, Final Batch Loss: 0.11244519799947739\n",
      "Epoch 4868, Loss: 0.4315936043858528, Final Batch Loss: 0.15226894617080688\n",
      "Epoch 4869, Loss: 0.31356415152549744, Final Batch Loss: 0.0801667720079422\n",
      "Epoch 4870, Loss: 0.36493443697690964, Final Batch Loss: 0.094876728951931\n",
      "Epoch 4871, Loss: 0.47833530604839325, Final Batch Loss: 0.1316378116607666\n",
      "Epoch 4872, Loss: 0.3548264093697071, Final Batch Loss: 0.14715968072414398\n",
      "Epoch 4873, Loss: 0.33798791468143463, Final Batch Loss: 0.10372491925954819\n",
      "Epoch 4874, Loss: 0.30283381789922714, Final Batch Loss: 0.07255003601312637\n",
      "Epoch 4875, Loss: 0.33184440433979034, Final Batch Loss: 0.09945972263813019\n",
      "Epoch 4876, Loss: 0.4621022492647171, Final Batch Loss: 0.21619053184986115\n",
      "Epoch 4877, Loss: 0.3642924502491951, Final Batch Loss: 0.08193560689687729\n",
      "Epoch 4878, Loss: 0.38574739545583725, Final Batch Loss: 0.10804564505815506\n",
      "Epoch 4879, Loss: 0.37222401052713394, Final Batch Loss: 0.0718841627240181\n",
      "Epoch 4880, Loss: 0.3882906138896942, Final Batch Loss: 0.11364300549030304\n",
      "Epoch 4881, Loss: 0.5178397297859192, Final Batch Loss: 0.15126986801624298\n",
      "Epoch 4882, Loss: 0.3700765371322632, Final Batch Loss: 0.1262698918581009\n",
      "Epoch 4883, Loss: 0.30805929750204086, Final Batch Loss: 0.06349166482686996\n",
      "Epoch 4884, Loss: 0.35440192371606827, Final Batch Loss: 0.1039133220911026\n",
      "Epoch 4885, Loss: 0.4628036171197891, Final Batch Loss: 0.13299448788166046\n",
      "Epoch 4886, Loss: 0.38748836517333984, Final Batch Loss: 0.13639812171459198\n",
      "Epoch 4887, Loss: 0.3301101326942444, Final Batch Loss: 0.16360004246234894\n",
      "Epoch 4888, Loss: 0.31280049681663513, Final Batch Loss: 0.11599741876125336\n",
      "Epoch 4889, Loss: 0.3869183212518692, Final Batch Loss: 0.1631244719028473\n",
      "Epoch 4890, Loss: 0.32722271233797073, Final Batch Loss: 0.1248408704996109\n",
      "Epoch 4891, Loss: 0.2843852937221527, Final Batch Loss: 0.06693552434444427\n",
      "Epoch 4892, Loss: 0.4112781956791878, Final Batch Loss: 0.10722833126783371\n",
      "Epoch 4893, Loss: 0.32509543746709824, Final Batch Loss: 0.09427434206008911\n",
      "Epoch 4894, Loss: 0.3774886876344681, Final Batch Loss: 0.0875641256570816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4895, Loss: 0.31558558344841003, Final Batch Loss: 0.10431356728076935\n",
      "Epoch 4896, Loss: 0.39703379571437836, Final Batch Loss: 0.14430676400661469\n",
      "Epoch 4897, Loss: 0.3723282143473625, Final Batch Loss: 0.12631693482398987\n",
      "Epoch 4898, Loss: 0.28998252004384995, Final Batch Loss: 0.05806216597557068\n",
      "Epoch 4899, Loss: 0.34346944838762283, Final Batch Loss: 0.1569623351097107\n",
      "Epoch 4900, Loss: 0.2960667461156845, Final Batch Loss: 0.05733253061771393\n",
      "Epoch 4901, Loss: 0.4989154785871506, Final Batch Loss: 0.17367856204509735\n",
      "Epoch 4902, Loss: 0.3934416249394417, Final Batch Loss: 0.1308659464120865\n",
      "Epoch 4903, Loss: 0.3724682703614235, Final Batch Loss: 0.1354898065328598\n",
      "Epoch 4904, Loss: 0.3221558853983879, Final Batch Loss: 0.07398000359535217\n",
      "Epoch 4905, Loss: 0.3935416303575039, Final Batch Loss: 0.04645503684878349\n",
      "Epoch 4906, Loss: 0.35451046377420425, Final Batch Loss: 0.10244451463222504\n",
      "Epoch 4907, Loss: 0.3754950910806656, Final Batch Loss: 0.18243268132209778\n",
      "Epoch 4908, Loss: 0.28338201344013214, Final Batch Loss: 0.08627340942621231\n",
      "Epoch 4909, Loss: 0.3882552906870842, Final Batch Loss: 0.2008291631937027\n",
      "Epoch 4910, Loss: 0.3234156370162964, Final Batch Loss: 0.12187166512012482\n",
      "Epoch 4911, Loss: 0.3678487539291382, Final Batch Loss: 0.10200285166501999\n",
      "Epoch 4912, Loss: 0.3023887649178505, Final Batch Loss: 0.07725810259580612\n",
      "Epoch 4913, Loss: 0.38578642904758453, Final Batch Loss: 0.09292060136795044\n",
      "Epoch 4914, Loss: 0.34195534884929657, Final Batch Loss: 0.11381065100431442\n",
      "Epoch 4915, Loss: 0.3712849020957947, Final Batch Loss: 0.07553714513778687\n",
      "Epoch 4916, Loss: 0.4502589628100395, Final Batch Loss: 0.22619806230068207\n",
      "Epoch 4917, Loss: 0.359227292239666, Final Batch Loss: 0.07087063044309616\n",
      "Epoch 4918, Loss: 0.3654792234301567, Final Batch Loss: 0.16338875889778137\n",
      "Epoch 4919, Loss: 0.2955377623438835, Final Batch Loss: 0.11466872692108154\n",
      "Epoch 4920, Loss: 0.45073816180229187, Final Batch Loss: 0.16659171879291534\n",
      "Epoch 4921, Loss: 0.4610787034034729, Final Batch Loss: 0.18341583013534546\n",
      "Epoch 4922, Loss: 0.3603157252073288, Final Batch Loss: 0.18024544417858124\n",
      "Epoch 4923, Loss: 0.3120630830526352, Final Batch Loss: 0.10863744467496872\n",
      "Epoch 4924, Loss: 0.3928169161081314, Final Batch Loss: 0.11900952458381653\n",
      "Epoch 4925, Loss: 0.36958377808332443, Final Batch Loss: 0.09112302213907242\n",
      "Epoch 4926, Loss: 0.3459196761250496, Final Batch Loss: 0.18148589134216309\n",
      "Epoch 4927, Loss: 0.31820670515298843, Final Batch Loss: 0.132182776927948\n",
      "Epoch 4928, Loss: 0.3195943310856819, Final Batch Loss: 0.11011612415313721\n",
      "Epoch 4929, Loss: 0.49511849880218506, Final Batch Loss: 0.1650974154472351\n",
      "Epoch 4930, Loss: 0.3693682998418808, Final Batch Loss: 0.13235051929950714\n",
      "Epoch 4931, Loss: 0.38130971789360046, Final Batch Loss: 0.09353702515363693\n",
      "Epoch 4932, Loss: 0.4695378914475441, Final Batch Loss: 0.183682382106781\n",
      "Epoch 4933, Loss: 0.41562602669000626, Final Batch Loss: 0.1156238242983818\n",
      "Epoch 4934, Loss: 0.4128880575299263, Final Batch Loss: 0.08526190370321274\n",
      "Epoch 4935, Loss: 0.44298534095287323, Final Batch Loss: 0.12329214811325073\n",
      "Epoch 4936, Loss: 0.3700820729136467, Final Batch Loss: 0.10953215509653091\n",
      "Epoch 4937, Loss: 0.36948974430561066, Final Batch Loss: 0.14065460860729218\n",
      "Epoch 4938, Loss: 0.36421722173690796, Final Batch Loss: 0.11242034286260605\n",
      "Epoch 4939, Loss: 0.39617639034986496, Final Batch Loss: 0.1977936178445816\n",
      "Epoch 4940, Loss: 0.3021496832370758, Final Batch Loss: 0.0775635689496994\n",
      "Epoch 4941, Loss: 0.37715642899274826, Final Batch Loss: 0.11584796756505966\n",
      "Epoch 4942, Loss: 0.33895033597946167, Final Batch Loss: 0.17882058024406433\n",
      "Epoch 4943, Loss: 0.34489601850509644, Final Batch Loss: 0.15037801861763\n",
      "Epoch 4944, Loss: 0.4038451537489891, Final Batch Loss: 0.11072394996881485\n",
      "Epoch 4945, Loss: 0.3125823438167572, Final Batch Loss: 0.11231793463230133\n",
      "Epoch 4946, Loss: 0.461897611618042, Final Batch Loss: 0.15004409849643707\n",
      "Epoch 4947, Loss: 0.3064124658703804, Final Batch Loss: 0.08741786330938339\n",
      "Epoch 4948, Loss: 0.3959719315171242, Final Batch Loss: 0.11441703885793686\n",
      "Epoch 4949, Loss: 0.3069668859243393, Final Batch Loss: 0.07456143945455551\n",
      "Epoch 4950, Loss: 0.42087820172309875, Final Batch Loss: 0.13311438262462616\n",
      "Epoch 4951, Loss: 0.319765105843544, Final Batch Loss: 0.06689348816871643\n",
      "Epoch 4952, Loss: 0.3732610195875168, Final Batch Loss: 0.12659025192260742\n",
      "Epoch 4953, Loss: 0.3899499475955963, Final Batch Loss: 0.13818134367465973\n",
      "Epoch 4954, Loss: 0.2973972409963608, Final Batch Loss: 0.09453530609607697\n",
      "Epoch 4955, Loss: 0.30656201392412186, Final Batch Loss: 0.10156437009572983\n",
      "Epoch 4956, Loss: 0.34742017835378647, Final Batch Loss: 0.11018654704093933\n",
      "Epoch 4957, Loss: 0.3044399246573448, Final Batch Loss: 0.10777450352907181\n",
      "Epoch 4958, Loss: 0.3932972848415375, Final Batch Loss: 0.1674560308456421\n",
      "Epoch 4959, Loss: 0.3257569447159767, Final Batch Loss: 0.08450455218553543\n",
      "Epoch 4960, Loss: 0.319422110915184, Final Batch Loss: 0.08811599016189575\n",
      "Epoch 4961, Loss: 0.3674599900841713, Final Batch Loss: 0.19763818383216858\n",
      "Epoch 4962, Loss: 0.4195514917373657, Final Batch Loss: 0.1596052199602127\n",
      "Epoch 4963, Loss: 0.3201169818639755, Final Batch Loss: 0.1216830387711525\n",
      "Epoch 4964, Loss: 0.35061103105545044, Final Batch Loss: 0.11889339238405228\n",
      "Epoch 4965, Loss: 0.397232286632061, Final Batch Loss: 0.18089231848716736\n",
      "Epoch 4966, Loss: 0.37364138662815094, Final Batch Loss: 0.14322485029697418\n",
      "Epoch 4967, Loss: 0.41916489601135254, Final Batch Loss: 0.2016160935163498\n",
      "Epoch 4968, Loss: 0.42503343895077705, Final Batch Loss: 0.2088707536458969\n",
      "Epoch 4969, Loss: 0.41368449479341507, Final Batch Loss: 0.13973979651927948\n",
      "Epoch 4970, Loss: 0.273643858730793, Final Batch Loss: 0.06374254077672958\n",
      "Epoch 4971, Loss: 0.35373149812221527, Final Batch Loss: 0.16245900094509125\n",
      "Epoch 4972, Loss: 0.3392205908894539, Final Batch Loss: 0.06521370261907578\n",
      "Epoch 4973, Loss: 0.3274698257446289, Final Batch Loss: 0.1425020694732666\n",
      "Epoch 4974, Loss: 0.3918500617146492, Final Batch Loss: 0.0901975929737091\n",
      "Epoch 4975, Loss: 0.39419986307621, Final Batch Loss: 0.15460681915283203\n",
      "Epoch 4976, Loss: 0.37634510546922684, Final Batch Loss: 0.15262432396411896\n",
      "Epoch 4977, Loss: 0.2587050646543503, Final Batch Loss: 0.09134203940629959\n",
      "Epoch 4978, Loss: 0.33848707377910614, Final Batch Loss: 0.12647908926010132\n",
      "Epoch 4979, Loss: 0.4368509203195572, Final Batch Loss: 0.19203783571720123\n",
      "Epoch 4980, Loss: 0.3607817068696022, Final Batch Loss: 0.14207275211811066\n",
      "Epoch 4981, Loss: 0.31419631838798523, Final Batch Loss: 0.11359651386737823\n",
      "Epoch 4982, Loss: 0.38042208552360535, Final Batch Loss: 0.12311386317014694\n",
      "Epoch 4983, Loss: 0.3798741027712822, Final Batch Loss: 0.10853297263383865\n",
      "Epoch 4984, Loss: 0.48413708806037903, Final Batch Loss: 0.15786539018154144\n",
      "Epoch 4985, Loss: 0.34970853477716446, Final Batch Loss: 0.1666042059659958\n",
      "Epoch 4986, Loss: 0.4229252338409424, Final Batch Loss: 0.1348349004983902\n",
      "Epoch 4987, Loss: 0.47921887040138245, Final Batch Loss: 0.2087649703025818\n",
      "Epoch 4988, Loss: 0.5016441941261292, Final Batch Loss: 0.12896335124969482\n",
      "Epoch 4989, Loss: 0.38064926862716675, Final Batch Loss: 0.0984780490398407\n",
      "Epoch 4990, Loss: 0.3251055032014847, Final Batch Loss: 0.11742378026247025\n",
      "Epoch 4991, Loss: 0.3094325363636017, Final Batch Loss: 0.07299689203500748\n",
      "Epoch 4992, Loss: 0.28312356397509575, Final Batch Loss: 0.04856792464852333\n",
      "Epoch 4993, Loss: 0.4287729188799858, Final Batch Loss: 0.16111324727535248\n",
      "Epoch 4994, Loss: 0.4626412093639374, Final Batch Loss: 0.13785263895988464\n",
      "Epoch 4995, Loss: 0.39620260149240494, Final Batch Loss: 0.13170483708381653\n",
      "Epoch 4996, Loss: 0.36722513288259506, Final Batch Loss: 0.1269509196281433\n",
      "Epoch 4997, Loss: 0.315665602684021, Final Batch Loss: 0.08121877908706665\n",
      "Epoch 4998, Loss: 0.35855990648269653, Final Batch Loss: 0.13325101137161255\n",
      "Epoch 4999, Loss: 0.344352550804615, Final Batch Loss: 0.12641841173171997\n",
      "Epoch 5000, Loss: 0.4316221922636032, Final Batch Loss: 0.21284016966819763\n",
      "Epoch 5001, Loss: 0.4254300966858864, Final Batch Loss: 0.11116711050271988\n",
      "Epoch 5002, Loss: 0.4013928174972534, Final Batch Loss: 0.11137357354164124\n",
      "Epoch 5003, Loss: 0.3047422170639038, Final Batch Loss: 0.14626266062259674\n",
      "Epoch 5004, Loss: 0.3128240928053856, Final Batch Loss: 0.08520812541246414\n",
      "Epoch 5005, Loss: 0.33516331762075424, Final Batch Loss: 0.09059318900108337\n",
      "Epoch 5006, Loss: 0.3185215890407562, Final Batch Loss: 0.08039332926273346\n",
      "Epoch 5007, Loss: 0.3589445427060127, Final Batch Loss: 0.12777674198150635\n",
      "Epoch 5008, Loss: 0.3119155205786228, Final Batch Loss: 0.059202130883932114\n",
      "Epoch 5009, Loss: 0.2739205062389374, Final Batch Loss: 0.10666974633932114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5010, Loss: 0.2942301630973816, Final Batch Loss: 0.08961117267608643\n",
      "Epoch 5011, Loss: 0.3859574794769287, Final Batch Loss: 0.11212299764156342\n",
      "Epoch 5012, Loss: 0.4157618060708046, Final Batch Loss: 0.15070003271102905\n",
      "Epoch 5013, Loss: 0.3199218325316906, Final Batch Loss: 0.13094563782215118\n",
      "Epoch 5014, Loss: 0.40712587535381317, Final Batch Loss: 0.12179689109325409\n",
      "Epoch 5015, Loss: 0.4181928336620331, Final Batch Loss: 0.09484630823135376\n",
      "Epoch 5016, Loss: 0.32859358936548233, Final Batch Loss: 0.08344779908657074\n",
      "Epoch 5017, Loss: 0.3427211716771126, Final Batch Loss: 0.097675621509552\n",
      "Epoch 5018, Loss: 0.38602307438850403, Final Batch Loss: 0.1360982209444046\n",
      "Epoch 5019, Loss: 0.31089697033166885, Final Batch Loss: 0.11599651724100113\n",
      "Epoch 5020, Loss: 0.4084830582141876, Final Batch Loss: 0.09213277697563171\n",
      "Epoch 5021, Loss: 0.288151316344738, Final Batch Loss: 0.10089649260044098\n",
      "Epoch 5022, Loss: 0.41565828770399094, Final Batch Loss: 0.14239557087421417\n",
      "Epoch 5023, Loss: 0.40066393464803696, Final Batch Loss: 0.17155590653419495\n",
      "Epoch 5024, Loss: 0.39708737283945084, Final Batch Loss: 0.13027532398700714\n",
      "Epoch 5025, Loss: 0.4956846237182617, Final Batch Loss: 0.18819285929203033\n",
      "Epoch 5026, Loss: 0.3523489534854889, Final Batch Loss: 0.12391482293605804\n",
      "Epoch 5027, Loss: 0.31048574298620224, Final Batch Loss: 0.0824722871184349\n",
      "Epoch 5028, Loss: 0.48611511290073395, Final Batch Loss: 0.22679130733013153\n",
      "Epoch 5029, Loss: 0.3802502751350403, Final Batch Loss: 0.11766009032726288\n",
      "Epoch 5030, Loss: 0.537651926279068, Final Batch Loss: 0.2865358889102936\n",
      "Epoch 5031, Loss: 0.4227279797196388, Final Batch Loss: 0.12377969175577164\n",
      "Epoch 5032, Loss: 0.39224308729171753, Final Batch Loss: 0.13266314566135406\n",
      "Epoch 5033, Loss: 0.4184470474720001, Final Batch Loss: 0.12713991105556488\n",
      "Epoch 5034, Loss: 0.38050875067710876, Final Batch Loss: 0.11553575098514557\n",
      "Epoch 5035, Loss: 0.3915175274014473, Final Batch Loss: 0.10817105323076248\n",
      "Epoch 5036, Loss: 0.5023946613073349, Final Batch Loss: 0.14680111408233643\n",
      "Epoch 5037, Loss: 0.3219910115003586, Final Batch Loss: 0.08344139903783798\n",
      "Epoch 5038, Loss: 0.40729570388793945, Final Batch Loss: 0.14412696659564972\n",
      "Epoch 5039, Loss: 0.5770089998841286, Final Batch Loss: 0.25929784774780273\n",
      "Epoch 5040, Loss: 0.4142330065369606, Final Batch Loss: 0.16582751274108887\n",
      "Epoch 5041, Loss: 0.36544039100408554, Final Batch Loss: 0.127908855676651\n",
      "Epoch 5042, Loss: 0.32638441771268845, Final Batch Loss: 0.11421909183263779\n",
      "Epoch 5043, Loss: 0.37314800173044205, Final Batch Loss: 0.12043991684913635\n",
      "Epoch 5044, Loss: 0.36657263338565826, Final Batch Loss: 0.09233029186725616\n",
      "Epoch 5045, Loss: 0.28968291729688644, Final Batch Loss: 0.10914460569620132\n",
      "Epoch 5046, Loss: 0.36885693669319153, Final Batch Loss: 0.11616634577512741\n",
      "Epoch 5047, Loss: 0.3377343490719795, Final Batch Loss: 0.09702467918395996\n",
      "Epoch 5048, Loss: 0.36815647035837173, Final Batch Loss: 0.10336235910654068\n",
      "Epoch 5049, Loss: 0.3475004769861698, Final Batch Loss: 0.1269425004720688\n",
      "Epoch 5050, Loss: 0.46386685967445374, Final Batch Loss: 0.177093505859375\n",
      "Epoch 5051, Loss: 0.323421947658062, Final Batch Loss: 0.12648431956768036\n",
      "Epoch 5052, Loss: 0.28124507516622543, Final Batch Loss: 0.09000447392463684\n",
      "Epoch 5053, Loss: 0.30737587809562683, Final Batch Loss: 0.09083401411771774\n",
      "Epoch 5054, Loss: 0.28762664645910263, Final Batch Loss: 0.0937979444861412\n",
      "Epoch 5055, Loss: 0.2933764159679413, Final Batch Loss: 0.06355270743370056\n",
      "Epoch 5056, Loss: 0.36631616950035095, Final Batch Loss: 0.15303783118724823\n",
      "Epoch 5057, Loss: 0.3711419031023979, Final Batch Loss: 0.1266970932483673\n",
      "Epoch 5058, Loss: 0.31783897429704666, Final Batch Loss: 0.10646498203277588\n",
      "Epoch 5059, Loss: 0.3737894520163536, Final Batch Loss: 0.07813196629285812\n",
      "Epoch 5060, Loss: 0.3668840304017067, Final Batch Loss: 0.14042186737060547\n",
      "Epoch 5061, Loss: 0.32375217229127884, Final Batch Loss: 0.08858754485845566\n",
      "Epoch 5062, Loss: 0.34666144102811813, Final Batch Loss: 0.07109392434358597\n",
      "Epoch 5063, Loss: 0.38093242794275284, Final Batch Loss: 0.14208929240703583\n",
      "Epoch 5064, Loss: 0.3888886421918869, Final Batch Loss: 0.12963645160198212\n",
      "Epoch 5065, Loss: 0.34403616189956665, Final Batch Loss: 0.13855843245983124\n",
      "Epoch 5066, Loss: 0.2544088810682297, Final Batch Loss: 0.11956766992807388\n",
      "Epoch 5067, Loss: 0.3538759723305702, Final Batch Loss: 0.0629078671336174\n",
      "Epoch 5068, Loss: 0.42807920277118683, Final Batch Loss: 0.15017446875572205\n",
      "Epoch 5069, Loss: 0.3476886823773384, Final Batch Loss: 0.12059767544269562\n",
      "Epoch 5070, Loss: 0.3303372487425804, Final Batch Loss: 0.09528033435344696\n",
      "Epoch 5071, Loss: 0.40726329386234283, Final Batch Loss: 0.14568687975406647\n",
      "Epoch 5072, Loss: 0.3736872971057892, Final Batch Loss: 0.0911511778831482\n",
      "Epoch 5073, Loss: 0.3519027456641197, Final Batch Loss: 0.1356339454650879\n",
      "Epoch 5074, Loss: 0.29100190103054047, Final Batch Loss: 0.08212334662675858\n",
      "Epoch 5075, Loss: 0.42177408933639526, Final Batch Loss: 0.12136764824390411\n",
      "Epoch 5076, Loss: 0.28625626116991043, Final Batch Loss: 0.04376272112131119\n",
      "Epoch 5077, Loss: 0.3564475253224373, Final Batch Loss: 0.10781505703926086\n",
      "Epoch 5078, Loss: 0.39731117337942123, Final Batch Loss: 0.14650815725326538\n",
      "Epoch 5079, Loss: 0.31836000084877014, Final Batch Loss: 0.08804541081190109\n",
      "Epoch 5080, Loss: 0.2794340252876282, Final Batch Loss: 0.06549839675426483\n",
      "Epoch 5081, Loss: 0.37915463745594025, Final Batch Loss: 0.16286584734916687\n",
      "Epoch 5082, Loss: 0.4428984075784683, Final Batch Loss: 0.22201202809810638\n",
      "Epoch 5083, Loss: 0.37053168565034866, Final Batch Loss: 0.09695810079574585\n",
      "Epoch 5084, Loss: 0.3579171821475029, Final Batch Loss: 0.1668710708618164\n",
      "Epoch 5085, Loss: 0.3976825699210167, Final Batch Loss: 0.15034596621990204\n",
      "Epoch 5086, Loss: 0.3631735295057297, Final Batch Loss: 0.16852690279483795\n",
      "Epoch 5087, Loss: 0.4711745083332062, Final Batch Loss: 0.202861949801445\n",
      "Epoch 5088, Loss: 0.2967389449477196, Final Batch Loss: 0.0788416713476181\n",
      "Epoch 5089, Loss: 0.3967467471957207, Final Batch Loss: 0.1608683317899704\n",
      "Epoch 5090, Loss: 0.4115540534257889, Final Batch Loss: 0.11685340106487274\n",
      "Epoch 5091, Loss: 0.2741478830575943, Final Batch Loss: 0.0847550705075264\n",
      "Epoch 5092, Loss: 0.4015898108482361, Final Batch Loss: 0.14975854754447937\n",
      "Epoch 5093, Loss: 0.3681399002671242, Final Batch Loss: 0.08201056718826294\n",
      "Epoch 5094, Loss: 0.3255366086959839, Final Batch Loss: 0.10797673463821411\n",
      "Epoch 5095, Loss: 0.339031457901001, Final Batch Loss: 0.10674535483121872\n",
      "Epoch 5096, Loss: 0.4006221666932106, Final Batch Loss: 0.1306709200143814\n",
      "Epoch 5097, Loss: 0.3729991614818573, Final Batch Loss: 0.10419560223817825\n",
      "Epoch 5098, Loss: 0.3473009392619133, Final Batch Loss: 0.12971650063991547\n",
      "Epoch 5099, Loss: 0.2996508553624153, Final Batch Loss: 0.07258783280849457\n",
      "Epoch 5100, Loss: 0.4099554196000099, Final Batch Loss: 0.15709729492664337\n",
      "Epoch 5101, Loss: 0.3341059684753418, Final Batch Loss: 0.12619446218013763\n",
      "Epoch 5102, Loss: 0.35981639474630356, Final Batch Loss: 0.1172541156411171\n",
      "Epoch 5103, Loss: 0.42948857694864273, Final Batch Loss: 0.09967377036809921\n",
      "Epoch 5104, Loss: 0.37186937034130096, Final Batch Loss: 0.11962112784385681\n",
      "Epoch 5105, Loss: 0.44518931210041046, Final Batch Loss: 0.19115915894508362\n",
      "Epoch 5106, Loss: 0.34863337129354477, Final Batch Loss: 0.09236886352300644\n",
      "Epoch 5107, Loss: 0.4061596095561981, Final Batch Loss: 0.13034412264823914\n",
      "Epoch 5108, Loss: 0.33741747587919235, Final Batch Loss: 0.15587906539440155\n",
      "Epoch 5109, Loss: 0.39330756664276123, Final Batch Loss: 0.1513885259628296\n",
      "Epoch 5110, Loss: 0.4034167155623436, Final Batch Loss: 0.11694600433111191\n",
      "Epoch 5111, Loss: 0.3652160242199898, Final Batch Loss: 0.07547513395547867\n",
      "Epoch 5112, Loss: 0.36015769094228745, Final Batch Loss: 0.09292944520711899\n",
      "Epoch 5113, Loss: 0.3115699663758278, Final Batch Loss: 0.09096634387969971\n",
      "Epoch 5114, Loss: 0.4577063173055649, Final Batch Loss: 0.20594890415668488\n",
      "Epoch 5115, Loss: 0.31418292224407196, Final Batch Loss: 0.13627411425113678\n",
      "Epoch 5116, Loss: 0.3241875171661377, Final Batch Loss: 0.07278560847043991\n",
      "Epoch 5117, Loss: 0.39103303849697113, Final Batch Loss: 0.09552811086177826\n",
      "Epoch 5118, Loss: 0.4713949039578438, Final Batch Loss: 0.1189187541604042\n",
      "Epoch 5119, Loss: 0.3585463687777519, Final Batch Loss: 0.09218419343233109\n",
      "Epoch 5120, Loss: 0.37765493988990784, Final Batch Loss: 0.23036637902259827\n",
      "Epoch 5121, Loss: 0.5384658724069595, Final Batch Loss: 0.22898679971694946\n",
      "Epoch 5122, Loss: 0.4376072585582733, Final Batch Loss: 0.16844914853572845\n",
      "Epoch 5123, Loss: 0.5101732835173607, Final Batch Loss: 0.2061871737241745\n",
      "Epoch 5124, Loss: 0.339754119515419, Final Batch Loss: 0.14512357115745544\n",
      "Epoch 5125, Loss: 0.432209774851799, Final Batch Loss: 0.12234055995941162\n",
      "Epoch 5126, Loss: 0.3569990545511246, Final Batch Loss: 0.11720593273639679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5127, Loss: 0.35207218676805496, Final Batch Loss: 0.09965722262859344\n",
      "Epoch 5128, Loss: 0.37083419412374496, Final Batch Loss: 0.1177995353937149\n",
      "Epoch 5129, Loss: 0.3129206672310829, Final Batch Loss: 0.14139153063297272\n",
      "Epoch 5130, Loss: 0.3197649046778679, Final Batch Loss: 0.11222650110721588\n",
      "Epoch 5131, Loss: 0.36565902829170227, Final Batch Loss: 0.18436789512634277\n",
      "Epoch 5132, Loss: 0.37507012486457825, Final Batch Loss: 0.10594015568494797\n",
      "Epoch 5133, Loss: 0.3352738991379738, Final Batch Loss: 0.12143583595752716\n",
      "Epoch 5134, Loss: 0.32033246010541916, Final Batch Loss: 0.15068656206130981\n",
      "Epoch 5135, Loss: 0.28355448693037033, Final Batch Loss: 0.10736830532550812\n",
      "Epoch 5136, Loss: 0.2857315093278885, Final Batch Loss: 0.07004507631063461\n",
      "Epoch 5137, Loss: 0.449050135910511, Final Batch Loss: 0.18583321571350098\n",
      "Epoch 5138, Loss: 0.35078154504299164, Final Batch Loss: 0.13898925483226776\n",
      "Epoch 5139, Loss: 0.40613823384046555, Final Batch Loss: 0.09372594207525253\n",
      "Epoch 5140, Loss: 0.2956434562802315, Final Batch Loss: 0.08954880386590958\n",
      "Epoch 5141, Loss: 0.35855818539857864, Final Batch Loss: 0.14897195994853973\n",
      "Epoch 5142, Loss: 0.38357800245285034, Final Batch Loss: 0.1541944444179535\n",
      "Epoch 5143, Loss: 0.3913620412349701, Final Batch Loss: 0.17470373213291168\n",
      "Epoch 5144, Loss: 0.34234002977609634, Final Batch Loss: 0.06375844031572342\n",
      "Epoch 5145, Loss: 0.334771029651165, Final Batch Loss: 0.12762784957885742\n",
      "Epoch 5146, Loss: 0.31269602477550507, Final Batch Loss: 0.07548946887254715\n",
      "Epoch 5147, Loss: 0.3402719646692276, Final Batch Loss: 0.08503696322441101\n",
      "Epoch 5148, Loss: 0.403508223593235, Final Batch Loss: 0.17195795476436615\n",
      "Epoch 5149, Loss: 0.3119637742638588, Final Batch Loss: 0.11854559928178787\n",
      "Epoch 5150, Loss: 0.3241260424256325, Final Batch Loss: 0.1008773148059845\n",
      "Epoch 5151, Loss: 0.3490382172167301, Final Batch Loss: 0.11474613845348358\n",
      "Epoch 5152, Loss: 0.3769853115081787, Final Batch Loss: 0.07624030858278275\n",
      "Epoch 5153, Loss: 0.35307879000902176, Final Batch Loss: 0.09867284446954727\n",
      "Epoch 5154, Loss: 0.3906843960285187, Final Batch Loss: 0.1990845799446106\n",
      "Epoch 5155, Loss: 0.2825460284948349, Final Batch Loss: 0.0721040591597557\n",
      "Epoch 5156, Loss: 0.38887955993413925, Final Batch Loss: 0.06760028749704361\n",
      "Epoch 5157, Loss: 0.37953102588653564, Final Batch Loss: 0.1314515620470047\n",
      "Epoch 5158, Loss: 0.37371549755334854, Final Batch Loss: 0.08053765445947647\n",
      "Epoch 5159, Loss: 0.36089300364255905, Final Batch Loss: 0.17140178382396698\n",
      "Epoch 5160, Loss: 0.3427625298500061, Final Batch Loss: 0.1796296238899231\n",
      "Epoch 5161, Loss: 0.4040847420692444, Final Batch Loss: 0.13882608711719513\n",
      "Epoch 5162, Loss: 0.3859189376235008, Final Batch Loss: 0.11941163241863251\n",
      "Epoch 5163, Loss: 0.3642183318734169, Final Batch Loss: 0.08936115354299545\n",
      "Epoch 5164, Loss: 0.3599329888820648, Final Batch Loss: 0.11214061081409454\n",
      "Epoch 5165, Loss: 0.3650968000292778, Final Batch Loss: 0.17811143398284912\n",
      "Epoch 5166, Loss: 0.4048982188105583, Final Batch Loss: 0.10848461836576462\n",
      "Epoch 5167, Loss: 0.30068259686231613, Final Batch Loss: 0.07631376385688782\n",
      "Epoch 5168, Loss: 0.31397123634815216, Final Batch Loss: 0.06498635560274124\n",
      "Epoch 5169, Loss: 0.3776325061917305, Final Batch Loss: 0.13791236281394958\n",
      "Epoch 5170, Loss: 0.3619791194796562, Final Batch Loss: 0.1380811631679535\n",
      "Epoch 5171, Loss: 0.37018371373414993, Final Batch Loss: 0.18392576277256012\n",
      "Epoch 5172, Loss: 0.399373821914196, Final Batch Loss: 0.1584414839744568\n",
      "Epoch 5173, Loss: 0.423211045563221, Final Batch Loss: 0.17286472022533417\n",
      "Epoch 5174, Loss: 0.4046785458922386, Final Batch Loss: 0.15007174015045166\n",
      "Epoch 5175, Loss: 0.4507548063993454, Final Batch Loss: 0.16264529526233673\n",
      "Epoch 5176, Loss: 0.3984205946326256, Final Batch Loss: 0.17869196832180023\n",
      "Epoch 5177, Loss: 0.3014148101210594, Final Batch Loss: 0.12092003971338272\n",
      "Epoch 5178, Loss: 0.45103976130485535, Final Batch Loss: 0.1761891394853592\n",
      "Epoch 5179, Loss: 0.4876556098461151, Final Batch Loss: 0.16369599103927612\n",
      "Epoch 5180, Loss: 0.3370916023850441, Final Batch Loss: 0.17400485277175903\n",
      "Epoch 5181, Loss: 0.388225682079792, Final Batch Loss: 0.14101360738277435\n",
      "Epoch 5182, Loss: 0.37782030552625656, Final Batch Loss: 0.07836901396512985\n",
      "Epoch 5183, Loss: 0.408296674489975, Final Batch Loss: 0.12219700217247009\n",
      "Epoch 5184, Loss: 0.4506266713142395, Final Batch Loss: 0.196195587515831\n",
      "Epoch 5185, Loss: 0.39781690388917923, Final Batch Loss: 0.14162741601467133\n",
      "Epoch 5186, Loss: 0.369293212890625, Final Batch Loss: 0.10824458301067352\n",
      "Epoch 5187, Loss: 0.32277463376522064, Final Batch Loss: 0.07538129389286041\n",
      "Epoch 5188, Loss: 0.4381931871175766, Final Batch Loss: 0.16097354888916016\n",
      "Epoch 5189, Loss: 0.2716265469789505, Final Batch Loss: 0.09610327333211899\n",
      "Epoch 5190, Loss: 0.35191406309604645, Final Batch Loss: 0.17772924900054932\n",
      "Epoch 5191, Loss: 0.31772278249263763, Final Batch Loss: 0.1460975855588913\n",
      "Epoch 5192, Loss: 0.29920685291290283, Final Batch Loss: 0.06968021392822266\n",
      "Epoch 5193, Loss: 0.2585744261741638, Final Batch Loss: 0.08129098266363144\n",
      "Epoch 5194, Loss: 0.3321203514933586, Final Batch Loss: 0.13517440855503082\n",
      "Epoch 5195, Loss: 0.3350406363606453, Final Batch Loss: 0.11814036965370178\n",
      "Epoch 5196, Loss: 0.3488875478506088, Final Batch Loss: 0.1488412618637085\n",
      "Epoch 5197, Loss: 0.3288060277700424, Final Batch Loss: 0.13565964996814728\n",
      "Epoch 5198, Loss: 0.32271523028612137, Final Batch Loss: 0.1011756956577301\n",
      "Epoch 5199, Loss: 0.38229114562273026, Final Batch Loss: 0.06673809140920639\n",
      "Epoch 5200, Loss: 0.2794206812977791, Final Batch Loss: 0.1123412549495697\n",
      "Epoch 5201, Loss: 0.31738606095314026, Final Batch Loss: 0.1314602941274643\n",
      "Epoch 5202, Loss: 0.37279587239027023, Final Batch Loss: 0.1821441948413849\n",
      "Epoch 5203, Loss: 0.37488628178834915, Final Batch Loss: 0.13181748986244202\n",
      "Epoch 5204, Loss: 0.38841407001018524, Final Batch Loss: 0.13145187497138977\n",
      "Epoch 5205, Loss: 0.4226732477545738, Final Batch Loss: 0.14713358879089355\n",
      "Epoch 5206, Loss: 0.41029222309589386, Final Batch Loss: 0.15629734098911285\n",
      "Epoch 5207, Loss: 0.3926817998290062, Final Batch Loss: 0.16065211594104767\n",
      "Epoch 5208, Loss: 0.36213836818933487, Final Batch Loss: 0.129177063703537\n",
      "Epoch 5209, Loss: 0.3832482099533081, Final Batch Loss: 0.1139940395951271\n",
      "Epoch 5210, Loss: 0.4414939805865288, Final Batch Loss: 0.16834238171577454\n",
      "Epoch 5211, Loss: 0.3808935806155205, Final Batch Loss: 0.156948983669281\n",
      "Epoch 5212, Loss: 0.44270990043878555, Final Batch Loss: 0.17503152787685394\n",
      "Epoch 5213, Loss: 0.32359449192881584, Final Batch Loss: 0.05845264717936516\n",
      "Epoch 5214, Loss: 0.3371424973011017, Final Batch Loss: 0.10702823847532272\n",
      "Epoch 5215, Loss: 0.2783430516719818, Final Batch Loss: 0.06841176748275757\n",
      "Epoch 5216, Loss: 0.3753728047013283, Final Batch Loss: 0.1202893778681755\n",
      "Epoch 5217, Loss: 0.35039734840393066, Final Batch Loss: 0.06797121465206146\n",
      "Epoch 5218, Loss: 0.32240428030490875, Final Batch Loss: 0.08024230599403381\n",
      "Epoch 5219, Loss: 0.35269082337617874, Final Batch Loss: 0.08996212482452393\n",
      "Epoch 5220, Loss: 0.3243282735347748, Final Batch Loss: 0.07157621532678604\n",
      "Epoch 5221, Loss: 0.3377368524670601, Final Batch Loss: 0.10696467012166977\n",
      "Epoch 5222, Loss: 0.3669007271528244, Final Batch Loss: 0.1240324005484581\n",
      "Epoch 5223, Loss: 0.310975581407547, Final Batch Loss: 0.09897331148386002\n",
      "Epoch 5224, Loss: 0.3179836794734001, Final Batch Loss: 0.07650458812713623\n",
      "Epoch 5225, Loss: 0.25697026401758194, Final Batch Loss: 0.07653390616178513\n",
      "Epoch 5226, Loss: 0.3015217036008835, Final Batch Loss: 0.09179738163948059\n",
      "Epoch 5227, Loss: 0.33288250863552094, Final Batch Loss: 0.16856501996517181\n",
      "Epoch 5228, Loss: 0.34613996744155884, Final Batch Loss: 0.10855086147785187\n",
      "Epoch 5229, Loss: 0.41019637882709503, Final Batch Loss: 0.10414248704910278\n",
      "Epoch 5230, Loss: 0.38148733228445053, Final Batch Loss: 0.10119619220495224\n",
      "Epoch 5231, Loss: 0.3492061495780945, Final Batch Loss: 0.11637222766876221\n",
      "Epoch 5232, Loss: 0.29037177562713623, Final Batch Loss: 0.07577307522296906\n",
      "Epoch 5233, Loss: 0.36041784286499023, Final Batch Loss: 0.09960018843412399\n",
      "Epoch 5234, Loss: 0.42064736783504486, Final Batch Loss: 0.21880599856376648\n",
      "Epoch 5235, Loss: 0.33598097413778305, Final Batch Loss: 0.12292523682117462\n",
      "Epoch 5236, Loss: 0.33257869631052017, Final Batch Loss: 0.12895730137825012\n",
      "Epoch 5237, Loss: 0.3345509171485901, Final Batch Loss: 0.10216909646987915\n",
      "Epoch 5238, Loss: 0.39740727096796036, Final Batch Loss: 0.11388058215379715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5239, Loss: 0.358745239675045, Final Batch Loss: 0.14575134217739105\n",
      "Epoch 5240, Loss: 0.37924908101558685, Final Batch Loss: 0.1616680920124054\n",
      "Epoch 5241, Loss: 0.2913582772016525, Final Batch Loss: 0.08218315988779068\n",
      "Epoch 5242, Loss: 0.3500324636697769, Final Batch Loss: 0.14624716341495514\n",
      "Epoch 5243, Loss: 0.43153296411037445, Final Batch Loss: 0.22667397558689117\n",
      "Epoch 5244, Loss: 0.42760948836803436, Final Batch Loss: 0.11052955687046051\n",
      "Epoch 5245, Loss: 0.4069620221853256, Final Batch Loss: 0.16011083126068115\n",
      "Epoch 5246, Loss: 0.3465755507349968, Final Batch Loss: 0.09822969138622284\n",
      "Epoch 5247, Loss: 0.40904442220926285, Final Batch Loss: 0.12474854290485382\n",
      "Epoch 5248, Loss: 0.4677526652812958, Final Batch Loss: 0.12091696262359619\n",
      "Epoch 5249, Loss: 0.29988277703523636, Final Batch Loss: 0.09057489037513733\n",
      "Epoch 5250, Loss: 0.33449821919202805, Final Batch Loss: 0.10615703463554382\n",
      "Epoch 5251, Loss: 0.2886480391025543, Final Batch Loss: 0.10974053293466568\n",
      "Epoch 5252, Loss: 0.489877812564373, Final Batch Loss: 0.1418255716562271\n",
      "Epoch 5253, Loss: 0.40514470264315605, Final Batch Loss: 0.062133338302373886\n",
      "Epoch 5254, Loss: 0.3932066187262535, Final Batch Loss: 0.11069279164075851\n",
      "Epoch 5255, Loss: 0.3892093300819397, Final Batch Loss: 0.18075403571128845\n",
      "Epoch 5256, Loss: 0.29051197692751884, Final Batch Loss: 0.05003529414534569\n",
      "Epoch 5257, Loss: 0.30366837978363037, Final Batch Loss: 0.112843357026577\n",
      "Epoch 5258, Loss: 0.27133703231811523, Final Batch Loss: 0.053989432752132416\n",
      "Epoch 5259, Loss: 0.3479369133710861, Final Batch Loss: 0.08221422135829926\n",
      "Epoch 5260, Loss: 0.46856382489204407, Final Batch Loss: 0.14419519901275635\n",
      "Epoch 5261, Loss: 0.33771200478076935, Final Batch Loss: 0.07536515593528748\n",
      "Epoch 5262, Loss: 0.3226267993450165, Final Batch Loss: 0.08750584721565247\n",
      "Epoch 5263, Loss: 0.34499623626470566, Final Batch Loss: 0.0652652457356453\n",
      "Epoch 5264, Loss: 0.36004798114299774, Final Batch Loss: 0.12633958458900452\n",
      "Epoch 5265, Loss: 0.4447028189897537, Final Batch Loss: 0.1777331531047821\n",
      "Epoch 5266, Loss: 0.2869463488459587, Final Batch Loss: 0.08308684080839157\n",
      "Epoch 5267, Loss: 0.34972843527793884, Final Batch Loss: 0.08420637249946594\n",
      "Epoch 5268, Loss: 0.27481261640787125, Final Batch Loss: 0.08556124567985535\n",
      "Epoch 5269, Loss: 0.39139045029878616, Final Batch Loss: 0.11204198747873306\n",
      "Epoch 5270, Loss: 0.36915794759988785, Final Batch Loss: 0.10738071799278259\n",
      "Epoch 5271, Loss: 0.45874737203121185, Final Batch Loss: 0.14089396595954895\n",
      "Epoch 5272, Loss: 0.28957074880599976, Final Batch Loss: 0.09458235651254654\n",
      "Epoch 5273, Loss: 0.2808338850736618, Final Batch Loss: 0.08705390244722366\n",
      "Epoch 5274, Loss: 0.3523516356945038, Final Batch Loss: 0.10392884165048599\n",
      "Epoch 5275, Loss: 0.3484482243657112, Final Batch Loss: 0.07579924166202545\n",
      "Epoch 5276, Loss: 0.2836906835436821, Final Batch Loss: 0.061962321400642395\n",
      "Epoch 5277, Loss: 0.36046623438596725, Final Batch Loss: 0.13710416853427887\n",
      "Epoch 5278, Loss: 0.33220428228378296, Final Batch Loss: 0.12297499179840088\n",
      "Epoch 5279, Loss: 0.41845574975013733, Final Batch Loss: 0.1524580717086792\n",
      "Epoch 5280, Loss: 0.40139516443014145, Final Batch Loss: 0.10856074839830399\n",
      "Epoch 5281, Loss: 0.34863031655550003, Final Batch Loss: 0.12220074981451035\n",
      "Epoch 5282, Loss: 0.5230287313461304, Final Batch Loss: 0.23127423226833344\n",
      "Epoch 5283, Loss: 0.3350345939397812, Final Batch Loss: 0.14338265359401703\n",
      "Epoch 5284, Loss: 0.33955228328704834, Final Batch Loss: 0.09102103859186172\n",
      "Epoch 5285, Loss: 0.38779183477163315, Final Batch Loss: 0.09483306854963303\n",
      "Epoch 5286, Loss: 0.34755997359752655, Final Batch Loss: 0.12944290041923523\n",
      "Epoch 5287, Loss: 0.29583805054426193, Final Batch Loss: 0.07037591189146042\n",
      "Epoch 5288, Loss: 0.3054453283548355, Final Batch Loss: 0.09005378186702728\n",
      "Epoch 5289, Loss: 0.34235963970422745, Final Batch Loss: 0.10962837189435959\n",
      "Epoch 5290, Loss: 0.3829306364059448, Final Batch Loss: 0.161531001329422\n",
      "Epoch 5291, Loss: 0.31302621960639954, Final Batch Loss: 0.0839197188615799\n",
      "Epoch 5292, Loss: 0.35608480870723724, Final Batch Loss: 0.15300758183002472\n",
      "Epoch 5293, Loss: 0.2930532470345497, Final Batch Loss: 0.08297044783830643\n",
      "Epoch 5294, Loss: 0.2706228718161583, Final Batch Loss: 0.07120900601148605\n",
      "Epoch 5295, Loss: 0.34372539818286896, Final Batch Loss: 0.09036703407764435\n",
      "Epoch 5296, Loss: 0.40564194321632385, Final Batch Loss: 0.14165329933166504\n",
      "Epoch 5297, Loss: 0.3251391500234604, Final Batch Loss: 0.11460047960281372\n",
      "Epoch 5298, Loss: 0.35026542842388153, Final Batch Loss: 0.15316055715084076\n",
      "Epoch 5299, Loss: 0.4383324682712555, Final Batch Loss: 0.16513042151927948\n",
      "Epoch 5300, Loss: 0.3431401625275612, Final Batch Loss: 0.11023250967264175\n",
      "Epoch 5301, Loss: 0.40568163245916367, Final Batch Loss: 0.12502002716064453\n",
      "Epoch 5302, Loss: 0.3196141645312309, Final Batch Loss: 0.10181373357772827\n",
      "Epoch 5303, Loss: 0.33753084391355515, Final Batch Loss: 0.09942637383937836\n",
      "Epoch 5304, Loss: 0.29733630269765854, Final Batch Loss: 0.09193527698516846\n",
      "Epoch 5305, Loss: 0.3213863745331764, Final Batch Loss: 0.12496606260538101\n",
      "Epoch 5306, Loss: 0.43960826843976974, Final Batch Loss: 0.11219238489866257\n",
      "Epoch 5307, Loss: 0.24890848249197006, Final Batch Loss: 0.06496063619852066\n",
      "Epoch 5308, Loss: 0.3683876171708107, Final Batch Loss: 0.1336233913898468\n",
      "Epoch 5309, Loss: 0.35722851008176804, Final Batch Loss: 0.1839383840560913\n",
      "Epoch 5310, Loss: 0.3867355212569237, Final Batch Loss: 0.06506145745515823\n",
      "Epoch 5311, Loss: 0.3591054677963257, Final Batch Loss: 0.1345406174659729\n",
      "Epoch 5312, Loss: 0.31210242211818695, Final Batch Loss: 0.09978873282670975\n",
      "Epoch 5313, Loss: 0.3368224874138832, Final Batch Loss: 0.08062088489532471\n",
      "Epoch 5314, Loss: 0.39191269874572754, Final Batch Loss: 0.07971598207950592\n",
      "Epoch 5315, Loss: 0.35961516946554184, Final Batch Loss: 0.08113234490156174\n",
      "Epoch 5316, Loss: 0.3240203410387039, Final Batch Loss: 0.09141712635755539\n",
      "Epoch 5317, Loss: 0.35912831127643585, Final Batch Loss: 0.14347824454307556\n",
      "Epoch 5318, Loss: 0.32915448397397995, Final Batch Loss: 0.09926116466522217\n",
      "Epoch 5319, Loss: 0.4116571247577667, Final Batch Loss: 0.12855154275894165\n",
      "Epoch 5320, Loss: 0.4536682516336441, Final Batch Loss: 0.13793261349201202\n",
      "Epoch 5321, Loss: 0.2538309693336487, Final Batch Loss: 0.08701060712337494\n",
      "Epoch 5322, Loss: 0.30955595523118973, Final Batch Loss: 0.0676812008023262\n",
      "Epoch 5323, Loss: 0.3496370241045952, Final Batch Loss: 0.14873044192790985\n",
      "Epoch 5324, Loss: 0.480780228972435, Final Batch Loss: 0.1363525688648224\n",
      "Epoch 5325, Loss: 0.31972536444664, Final Batch Loss: 0.09557891637086868\n",
      "Epoch 5326, Loss: 0.387056902050972, Final Batch Loss: 0.1206686794757843\n",
      "Epoch 5327, Loss: 0.45589467883110046, Final Batch Loss: 0.13187073171138763\n",
      "Epoch 5328, Loss: 0.42879974842071533, Final Batch Loss: 0.12503044307231903\n",
      "Epoch 5329, Loss: 0.40470360219478607, Final Batch Loss: 0.10848231613636017\n",
      "Epoch 5330, Loss: 0.389635369181633, Final Batch Loss: 0.11179380118846893\n",
      "Epoch 5331, Loss: 0.41160740703344345, Final Batch Loss: 0.12331726402044296\n",
      "Epoch 5332, Loss: 0.3869774714112282, Final Batch Loss: 0.16284404695034027\n",
      "Epoch 5333, Loss: 0.3189745545387268, Final Batch Loss: 0.10832462459802628\n",
      "Epoch 5334, Loss: 0.29148994386196136, Final Batch Loss: 0.13858643174171448\n",
      "Epoch 5335, Loss: 0.3726131245493889, Final Batch Loss: 0.12487206608057022\n",
      "Epoch 5336, Loss: 0.3478301018476486, Final Batch Loss: 0.14273880422115326\n",
      "Epoch 5337, Loss: 0.4588775932788849, Final Batch Loss: 0.23270346224308014\n",
      "Epoch 5338, Loss: 0.2909682169556618, Final Batch Loss: 0.13063113391399384\n",
      "Epoch 5339, Loss: 0.29595910385251045, Final Batch Loss: 0.05111466720700264\n",
      "Epoch 5340, Loss: 0.36670927703380585, Final Batch Loss: 0.12697555124759674\n",
      "Epoch 5341, Loss: 0.3508519008755684, Final Batch Loss: 0.10122627019882202\n",
      "Epoch 5342, Loss: 0.24762608855962753, Final Batch Loss: 0.0900435671210289\n",
      "Epoch 5343, Loss: 0.4001407027244568, Final Batch Loss: 0.11265420913696289\n",
      "Epoch 5344, Loss: 0.35709797590970993, Final Batch Loss: 0.06618575006723404\n",
      "Epoch 5345, Loss: 0.31987686455249786, Final Batch Loss: 0.10615579038858414\n",
      "Epoch 5346, Loss: 0.3493085205554962, Final Batch Loss: 0.11664648354053497\n",
      "Epoch 5347, Loss: 0.33979227393865585, Final Batch Loss: 0.06313902884721756\n",
      "Epoch 5348, Loss: 0.3352681025862694, Final Batch Loss: 0.07118488848209381\n",
      "Epoch 5349, Loss: 0.36993367969989777, Final Batch Loss: 0.19044171273708344\n",
      "Epoch 5350, Loss: 0.37017175555229187, Final Batch Loss: 0.1486094743013382\n",
      "Epoch 5351, Loss: 0.46290118992328644, Final Batch Loss: 0.13415738940238953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5352, Loss: 0.2844568155705929, Final Batch Loss: 0.05673656985163689\n",
      "Epoch 5353, Loss: 0.24545682221651077, Final Batch Loss: 0.05893673747777939\n",
      "Epoch 5354, Loss: 0.29651062190532684, Final Batch Loss: 0.09696163982152939\n",
      "Epoch 5355, Loss: 0.38574472069740295, Final Batch Loss: 0.1415523737668991\n",
      "Epoch 5356, Loss: 0.2533825412392616, Final Batch Loss: 0.07124267518520355\n",
      "Epoch 5357, Loss: 0.361552432179451, Final Batch Loss: 0.14968344569206238\n",
      "Epoch 5358, Loss: 0.3824792504310608, Final Batch Loss: 0.12576796114444733\n",
      "Epoch 5359, Loss: 0.40259818732738495, Final Batch Loss: 0.08603827655315399\n",
      "Epoch 5360, Loss: 0.3030872717499733, Final Batch Loss: 0.13091103732585907\n",
      "Epoch 5361, Loss: 0.3262123763561249, Final Batch Loss: 0.1031377837061882\n",
      "Epoch 5362, Loss: 0.3319977819919586, Final Batch Loss: 0.061531826853752136\n",
      "Epoch 5363, Loss: 0.33107971400022507, Final Batch Loss: 0.08393441885709763\n",
      "Epoch 5364, Loss: 0.3071342781186104, Final Batch Loss: 0.08831924945116043\n",
      "Epoch 5365, Loss: 0.28736795485019684, Final Batch Loss: 0.13211458921432495\n",
      "Epoch 5366, Loss: 0.3006853088736534, Final Batch Loss: 0.12852397561073303\n",
      "Epoch 5367, Loss: 0.3190496414899826, Final Batch Loss: 0.12542971968650818\n",
      "Epoch 5368, Loss: 0.4657467231154442, Final Batch Loss: 0.12442737072706223\n",
      "Epoch 5369, Loss: 0.29555216431617737, Final Batch Loss: 0.1167948916554451\n",
      "Epoch 5370, Loss: 0.33188077062368393, Final Batch Loss: 0.0937393307685852\n",
      "Epoch 5371, Loss: 0.4078603982925415, Final Batch Loss: 0.10798954963684082\n",
      "Epoch 5372, Loss: 0.3279650881886482, Final Batch Loss: 0.11528685688972473\n",
      "Epoch 5373, Loss: 0.34097423404455185, Final Batch Loss: 0.12683403491973877\n",
      "Epoch 5374, Loss: 0.38852768391370773, Final Batch Loss: 0.17963150143623352\n",
      "Epoch 5375, Loss: 0.4839535355567932, Final Batch Loss: 0.17019449174404144\n",
      "Epoch 5376, Loss: 0.4063630923628807, Final Batch Loss: 0.14651139080524445\n",
      "Epoch 5377, Loss: 0.43768659234046936, Final Batch Loss: 0.13023951649665833\n",
      "Epoch 5378, Loss: 0.3673514351248741, Final Batch Loss: 0.08005786687135696\n",
      "Epoch 5379, Loss: 0.4301701486110687, Final Batch Loss: 0.1382112205028534\n",
      "Epoch 5380, Loss: 0.313625305891037, Final Batch Loss: 0.09735927730798721\n",
      "Epoch 5381, Loss: 0.2995058670639992, Final Batch Loss: 0.10779013484716415\n",
      "Epoch 5382, Loss: 0.3938988223671913, Final Batch Loss: 0.09243660420179367\n",
      "Epoch 5383, Loss: 0.3602427616715431, Final Batch Loss: 0.1533617377281189\n",
      "Epoch 5384, Loss: 0.3137885183095932, Final Batch Loss: 0.09371543675661087\n",
      "Epoch 5385, Loss: 0.4724271446466446, Final Batch Loss: 0.11404387652873993\n",
      "Epoch 5386, Loss: 0.38292407244443893, Final Batch Loss: 0.14151756465435028\n",
      "Epoch 5387, Loss: 0.34428200125694275, Final Batch Loss: 0.14921218156814575\n",
      "Epoch 5388, Loss: 0.29049231112003326, Final Batch Loss: 0.08819927275180817\n",
      "Epoch 5389, Loss: 0.2951202020049095, Final Batch Loss: 0.07787945866584778\n",
      "Epoch 5390, Loss: 0.34230180084705353, Final Batch Loss: 0.06264473497867584\n",
      "Epoch 5391, Loss: 0.2693367712199688, Final Batch Loss: 0.08620599657297134\n",
      "Epoch 5392, Loss: 0.28406357765197754, Final Batch Loss: 0.09817394614219666\n",
      "Epoch 5393, Loss: 0.3092448562383652, Final Batch Loss: 0.09883062541484833\n",
      "Epoch 5394, Loss: 0.345683291554451, Final Batch Loss: 0.1118142306804657\n",
      "Epoch 5395, Loss: 0.39435677975416183, Final Batch Loss: 0.18993881344795227\n",
      "Epoch 5396, Loss: 0.2901121452450752, Final Batch Loss: 0.10330305248498917\n",
      "Epoch 5397, Loss: 0.35394566506147385, Final Batch Loss: 0.10051260143518448\n",
      "Epoch 5398, Loss: 0.35146043449640274, Final Batch Loss: 0.1274442970752716\n",
      "Epoch 5399, Loss: 0.38190553337335587, Final Batch Loss: 0.1059502586722374\n",
      "Epoch 5400, Loss: 0.30405258387327194, Final Batch Loss: 0.08843088150024414\n",
      "Epoch 5401, Loss: 0.35280660539865494, Final Batch Loss: 0.1483425796031952\n",
      "Epoch 5402, Loss: 0.40024638921022415, Final Batch Loss: 0.1743118166923523\n",
      "Epoch 5403, Loss: 0.35571736097335815, Final Batch Loss: 0.09366437792778015\n",
      "Epoch 5404, Loss: 0.3765157535672188, Final Batch Loss: 0.08294139057397842\n",
      "Epoch 5405, Loss: 0.41775791347026825, Final Batch Loss: 0.1419162154197693\n",
      "Epoch 5406, Loss: 0.34672797471284866, Final Batch Loss: 0.04980260878801346\n",
      "Epoch 5407, Loss: 0.34956517070531845, Final Batch Loss: 0.15596400201320648\n",
      "Epoch 5408, Loss: 0.23609570041298866, Final Batch Loss: 0.04896201565861702\n",
      "Epoch 5409, Loss: 0.31358833611011505, Final Batch Loss: 0.06416107714176178\n",
      "Epoch 5410, Loss: 0.36309418082237244, Final Batch Loss: 0.11032601445913315\n",
      "Epoch 5411, Loss: 0.2659224644303322, Final Batch Loss: 0.10279823839664459\n",
      "Epoch 5412, Loss: 0.4017777070403099, Final Batch Loss: 0.14436671137809753\n",
      "Epoch 5413, Loss: 0.42408349364995956, Final Batch Loss: 0.1211339458823204\n",
      "Epoch 5414, Loss: 0.3762856796383858, Final Batch Loss: 0.14221706986427307\n",
      "Epoch 5415, Loss: 0.45776404440402985, Final Batch Loss: 0.17321695387363434\n",
      "Epoch 5416, Loss: 0.37512653321027756, Final Batch Loss: 0.16275767982006073\n",
      "Epoch 5417, Loss: 0.40880610048770905, Final Batch Loss: 0.12095119059085846\n",
      "Epoch 5418, Loss: 0.30089832469820976, Final Batch Loss: 0.061041925102472305\n",
      "Epoch 5419, Loss: 0.33138587325811386, Final Batch Loss: 0.11637093126773834\n",
      "Epoch 5420, Loss: 0.29688067734241486, Final Batch Loss: 0.09107068181037903\n",
      "Epoch 5421, Loss: 0.2978840246796608, Final Batch Loss: 0.1011354997754097\n",
      "Epoch 5422, Loss: 0.353865422308445, Final Batch Loss: 0.07406748086214066\n",
      "Epoch 5423, Loss: 0.32963093370199203, Final Batch Loss: 0.08886656165122986\n",
      "Epoch 5424, Loss: 0.4328274354338646, Final Batch Loss: 0.1124829575419426\n",
      "Epoch 5425, Loss: 0.2879602238535881, Final Batch Loss: 0.09651515632867813\n",
      "Epoch 5426, Loss: 0.26409411057829857, Final Batch Loss: 0.058660488575696945\n",
      "Epoch 5427, Loss: 0.33913444727659225, Final Batch Loss: 0.07473480701446533\n",
      "Epoch 5428, Loss: 0.2931049168109894, Final Batch Loss: 0.11044032871723175\n",
      "Epoch 5429, Loss: 0.3451991081237793, Final Batch Loss: 0.042046040296554565\n",
      "Epoch 5430, Loss: 0.35955245792865753, Final Batch Loss: 0.14333496987819672\n",
      "Epoch 5431, Loss: 0.2883930951356888, Final Batch Loss: 0.0924689769744873\n",
      "Epoch 5432, Loss: 0.37584850937128067, Final Batch Loss: 0.11650098115205765\n",
      "Epoch 5433, Loss: 0.3404112309217453, Final Batch Loss: 0.13510018587112427\n",
      "Epoch 5434, Loss: 0.5563878417015076, Final Batch Loss: 0.29609155654907227\n",
      "Epoch 5435, Loss: 0.4035884067416191, Final Batch Loss: 0.1552615910768509\n",
      "Epoch 5436, Loss: 0.3738902807235718, Final Batch Loss: 0.135859876871109\n",
      "Epoch 5437, Loss: 0.3382410481572151, Final Batch Loss: 0.15177354216575623\n",
      "Epoch 5438, Loss: 0.38085006922483444, Final Batch Loss: 0.15282152593135834\n",
      "Epoch 5439, Loss: 0.3909805491566658, Final Batch Loss: 0.11099429428577423\n",
      "Epoch 5440, Loss: 0.3826393559575081, Final Batch Loss: 0.10884814709424973\n",
      "Epoch 5441, Loss: 0.39782923460006714, Final Batch Loss: 0.19619403779506683\n",
      "Epoch 5442, Loss: 0.2707907110452652, Final Batch Loss: 0.08039990812540054\n",
      "Epoch 5443, Loss: 0.3996554762125015, Final Batch Loss: 0.14941193163394928\n",
      "Epoch 5444, Loss: 0.41636668145656586, Final Batch Loss: 0.13440990447998047\n",
      "Epoch 5445, Loss: 0.29192426800727844, Final Batch Loss: 0.09208066016435623\n",
      "Epoch 5446, Loss: 0.3469555079936981, Final Batch Loss: 0.07166397571563721\n",
      "Epoch 5447, Loss: 0.336619570851326, Final Batch Loss: 0.12021102011203766\n",
      "Epoch 5448, Loss: 0.3068024218082428, Final Batch Loss: 0.12314236164093018\n",
      "Epoch 5449, Loss: 0.3056877478957176, Final Batch Loss: 0.06576666235923767\n",
      "Epoch 5450, Loss: 0.3285836800932884, Final Batch Loss: 0.11339612305164337\n",
      "Epoch 5451, Loss: 0.2901327982544899, Final Batch Loss: 0.09549815207719803\n",
      "Epoch 5452, Loss: 0.3059941753745079, Final Batch Loss: 0.09496600180864334\n",
      "Epoch 5453, Loss: 0.2586546242237091, Final Batch Loss: 0.06936567276716232\n",
      "Epoch 5454, Loss: 0.31659381091594696, Final Batch Loss: 0.10660809278488159\n",
      "Epoch 5455, Loss: 0.4311632066965103, Final Batch Loss: 0.16155415773391724\n",
      "Epoch 5456, Loss: 0.2934636026620865, Final Batch Loss: 0.07765773683786392\n",
      "Epoch 5457, Loss: 0.3145838528871536, Final Batch Loss: 0.09213212877511978\n",
      "Epoch 5458, Loss: 0.45537103712558746, Final Batch Loss: 0.16332721710205078\n",
      "Epoch 5459, Loss: 0.38259247690439224, Final Batch Loss: 0.09816721826791763\n",
      "Epoch 5460, Loss: 0.31593695282936096, Final Batch Loss: 0.1187584176659584\n",
      "Epoch 5461, Loss: 0.38357672095298767, Final Batch Loss: 0.16431021690368652\n",
      "Epoch 5462, Loss: 0.40826427191495895, Final Batch Loss: 0.1476099044084549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5463, Loss: 0.37342319637537, Final Batch Loss: 0.18243949115276337\n",
      "Epoch 5464, Loss: 0.3511291518807411, Final Batch Loss: 0.11669058352708817\n",
      "Epoch 5465, Loss: 0.3761764317750931, Final Batch Loss: 0.0958198606967926\n",
      "Epoch 5466, Loss: 0.2609892189502716, Final Batch Loss: 0.09500620514154434\n",
      "Epoch 5467, Loss: 0.4028363674879074, Final Batch Loss: 0.17481845617294312\n",
      "Epoch 5468, Loss: 0.2796868681907654, Final Batch Loss: 0.0797279104590416\n",
      "Epoch 5469, Loss: 0.3117729499936104, Final Batch Loss: 0.10172341018915176\n",
      "Epoch 5470, Loss: 0.3917566239833832, Final Batch Loss: 0.09607729315757751\n",
      "Epoch 5471, Loss: 0.5086669623851776, Final Batch Loss: 0.23733897507190704\n",
      "Epoch 5472, Loss: 0.4118258133530617, Final Batch Loss: 0.13486406207084656\n",
      "Epoch 5473, Loss: 0.3550627678632736, Final Batch Loss: 0.13962291181087494\n",
      "Epoch 5474, Loss: 0.30257850885391235, Final Batch Loss: 0.1094098687171936\n",
      "Epoch 5475, Loss: 0.4498113542795181, Final Batch Loss: 0.1478664129972458\n",
      "Epoch 5476, Loss: 0.3241584822535515, Final Batch Loss: 0.12317833304405212\n",
      "Epoch 5477, Loss: 0.332028292119503, Final Batch Loss: 0.0755901113152504\n",
      "Epoch 5478, Loss: 0.350485123693943, Final Batch Loss: 0.157146155834198\n",
      "Epoch 5479, Loss: 0.36913421005010605, Final Batch Loss: 0.11616355925798416\n",
      "Epoch 5480, Loss: 0.3746119290590286, Final Batch Loss: 0.09819012135267258\n",
      "Epoch 5481, Loss: 0.38484781235456467, Final Batch Loss: 0.10886179655790329\n",
      "Epoch 5482, Loss: 0.36383437365293503, Final Batch Loss: 0.1344730406999588\n",
      "Epoch 5483, Loss: 0.28407851606607437, Final Batch Loss: 0.10562650114297867\n",
      "Epoch 5484, Loss: 0.3640326038002968, Final Batch Loss: 0.13138045370578766\n",
      "Epoch 5485, Loss: 0.32886872440576553, Final Batch Loss: 0.08680222183465958\n",
      "Epoch 5486, Loss: 0.38936396688222885, Final Batch Loss: 0.13550302386283875\n",
      "Epoch 5487, Loss: 0.31056687235832214, Final Batch Loss: 0.09166207909584045\n",
      "Epoch 5488, Loss: 0.3871360421180725, Final Batch Loss: 0.12616312503814697\n",
      "Epoch 5489, Loss: 0.30683818459510803, Final Batch Loss: 0.14281944930553436\n",
      "Epoch 5490, Loss: 0.36241134256124496, Final Batch Loss: 0.11995352059602737\n",
      "Epoch 5491, Loss: 0.29597388952970505, Final Batch Loss: 0.13967832922935486\n",
      "Epoch 5492, Loss: 0.3554948791861534, Final Batch Loss: 0.14946956932544708\n",
      "Epoch 5493, Loss: 0.4103148803114891, Final Batch Loss: 0.14726139605045319\n",
      "Epoch 5494, Loss: 0.5103067457675934, Final Batch Loss: 0.22163529694080353\n",
      "Epoch 5495, Loss: 0.29718102142214775, Final Batch Loss: 0.08777178823947906\n",
      "Epoch 5496, Loss: 0.31505801156163216, Final Batch Loss: 0.05748012289404869\n",
      "Epoch 5497, Loss: 0.29581570625305176, Final Batch Loss: 0.06907272338867188\n",
      "Epoch 5498, Loss: 0.34117887914180756, Final Batch Loss: 0.10267805308103561\n",
      "Epoch 5499, Loss: 0.3407208137214184, Final Batch Loss: 0.16964420676231384\n",
      "Epoch 5500, Loss: 0.40255577862262726, Final Batch Loss: 0.06469166278839111\n",
      "Epoch 5501, Loss: 0.31118930131196976, Final Batch Loss: 0.0694328024983406\n",
      "Epoch 5502, Loss: 0.3596740551292896, Final Batch Loss: 0.04578002169728279\n",
      "Epoch 5503, Loss: 0.40720234811306, Final Batch Loss: 0.10076402127742767\n",
      "Epoch 5504, Loss: 0.3633826822042465, Final Batch Loss: 0.07579578459262848\n",
      "Epoch 5505, Loss: 0.38046976178884506, Final Batch Loss: 0.11390550434589386\n",
      "Epoch 5506, Loss: 0.4638923928141594, Final Batch Loss: 0.17974156141281128\n",
      "Epoch 5507, Loss: 0.3622530475258827, Final Batch Loss: 0.1262732595205307\n",
      "Epoch 5508, Loss: 0.3903840035200119, Final Batch Loss: 0.13913694024085999\n",
      "Epoch 5509, Loss: 0.5195962190628052, Final Batch Loss: 0.22977669537067413\n",
      "Epoch 5510, Loss: 0.3573680743575096, Final Batch Loss: 0.0981750637292862\n",
      "Epoch 5511, Loss: 0.36280110478401184, Final Batch Loss: 0.1225452795624733\n",
      "Epoch 5512, Loss: 0.42456045001745224, Final Batch Loss: 0.15662303566932678\n",
      "Epoch 5513, Loss: 0.36306074261665344, Final Batch Loss: 0.09051354229450226\n",
      "Epoch 5514, Loss: 0.3582414910197258, Final Batch Loss: 0.09623519331216812\n",
      "Epoch 5515, Loss: 0.36935554444789886, Final Batch Loss: 0.10098932683467865\n",
      "Epoch 5516, Loss: 0.44893427193164825, Final Batch Loss: 0.13161836564540863\n",
      "Epoch 5517, Loss: 0.46630851179361343, Final Batch Loss: 0.12259664386510849\n",
      "Epoch 5518, Loss: 0.37993474304676056, Final Batch Loss: 0.12708474695682526\n",
      "Epoch 5519, Loss: 0.4618796333670616, Final Batch Loss: 0.10158617049455643\n",
      "Epoch 5520, Loss: 0.3611755967140198, Final Batch Loss: 0.11883941292762756\n",
      "Epoch 5521, Loss: 0.5042476952075958, Final Batch Loss: 0.15025660395622253\n",
      "Epoch 5522, Loss: 0.4826864153146744, Final Batch Loss: 0.1572355180978775\n",
      "Epoch 5523, Loss: 0.4295951575040817, Final Batch Loss: 0.1122857928276062\n",
      "Epoch 5524, Loss: 0.401959590613842, Final Batch Loss: 0.12915243208408356\n",
      "Epoch 5525, Loss: 0.2897135019302368, Final Batch Loss: 0.07726459950208664\n",
      "Epoch 5526, Loss: 0.2991309389472008, Final Batch Loss: 0.08686279505491257\n",
      "Epoch 5527, Loss: 0.3380577489733696, Final Batch Loss: 0.10582039505243301\n",
      "Epoch 5528, Loss: 0.3377458229660988, Final Batch Loss: 0.1413974165916443\n",
      "Epoch 5529, Loss: 0.3521770238876343, Final Batch Loss: 0.16131027042865753\n",
      "Epoch 5530, Loss: 0.4130396470427513, Final Batch Loss: 0.11930812150239944\n",
      "Epoch 5531, Loss: 0.3328903689980507, Final Batch Loss: 0.09189120680093765\n",
      "Epoch 5532, Loss: 0.38364632427692413, Final Batch Loss: 0.11407109349966049\n",
      "Epoch 5533, Loss: 0.4154067486524582, Final Batch Loss: 0.16993361711502075\n",
      "Epoch 5534, Loss: 0.32240448147058487, Final Batch Loss: 0.08578161150217056\n",
      "Epoch 5535, Loss: 0.38277845084667206, Final Batch Loss: 0.12931767106056213\n",
      "Epoch 5536, Loss: 0.28571829944849014, Final Batch Loss: 0.10188864171504974\n",
      "Epoch 5537, Loss: 0.39314185827970505, Final Batch Loss: 0.09605420380830765\n",
      "Epoch 5538, Loss: 0.2960965633392334, Final Batch Loss: 0.11123370379209518\n",
      "Epoch 5539, Loss: 0.34983057528734207, Final Batch Loss: 0.13171641528606415\n",
      "Epoch 5540, Loss: 0.32976359874010086, Final Batch Loss: 0.09735757112503052\n",
      "Epoch 5541, Loss: 0.372354656457901, Final Batch Loss: 0.13191907107830048\n",
      "Epoch 5542, Loss: 0.35611769556999207, Final Batch Loss: 0.15244661271572113\n",
      "Epoch 5543, Loss: 0.30745895206928253, Final Batch Loss: 0.08204758167266846\n",
      "Epoch 5544, Loss: 0.44124214351177216, Final Batch Loss: 0.15131956338882446\n",
      "Epoch 5545, Loss: 0.2882610112428665, Final Batch Loss: 0.08034873008728027\n",
      "Epoch 5546, Loss: 0.3348764628171921, Final Batch Loss: 0.1321575939655304\n",
      "Epoch 5547, Loss: 0.32488659024238586, Final Batch Loss: 0.07420173287391663\n",
      "Epoch 5548, Loss: 0.35428377240896225, Final Batch Loss: 0.09194527566432953\n",
      "Epoch 5549, Loss: 0.3127133920788765, Final Batch Loss: 0.10060469806194305\n",
      "Epoch 5550, Loss: 0.32603156566619873, Final Batch Loss: 0.10205548256635666\n",
      "Epoch 5551, Loss: 0.2832644060254097, Final Batch Loss: 0.10247629880905151\n",
      "Epoch 5552, Loss: 0.36132413148880005, Final Batch Loss: 0.12489878386259079\n",
      "Epoch 5553, Loss: 0.28989531844854355, Final Batch Loss: 0.08811350166797638\n",
      "Epoch 5554, Loss: 0.3206179440021515, Final Batch Loss: 0.10873518139123917\n",
      "Epoch 5555, Loss: 0.3255656212568283, Final Batch Loss: 0.156904399394989\n",
      "Epoch 5556, Loss: 0.30442893505096436, Final Batch Loss: 0.0763518214225769\n",
      "Epoch 5557, Loss: 0.3696407452225685, Final Batch Loss: 0.14525166153907776\n",
      "Epoch 5558, Loss: 0.37732846289873123, Final Batch Loss: 0.11837019771337509\n",
      "Epoch 5559, Loss: 0.39277440309524536, Final Batch Loss: 0.1390150636434555\n",
      "Epoch 5560, Loss: 0.36031361669301987, Final Batch Loss: 0.14071379601955414\n",
      "Epoch 5561, Loss: 0.40571699291467667, Final Batch Loss: 0.07605375349521637\n",
      "Epoch 5562, Loss: 0.33102303743362427, Final Batch Loss: 0.1376948207616806\n",
      "Epoch 5563, Loss: 0.4672776609659195, Final Batch Loss: 0.11279667913913727\n",
      "Epoch 5564, Loss: 0.303972490131855, Final Batch Loss: 0.09959185868501663\n",
      "Epoch 5565, Loss: 0.35125330090522766, Final Batch Loss: 0.09585180133581161\n",
      "Epoch 5566, Loss: 0.30040840804576874, Final Batch Loss: 0.08337818086147308\n",
      "Epoch 5567, Loss: 0.34870319068431854, Final Batch Loss: 0.13699640333652496\n",
      "Epoch 5568, Loss: 0.42996978759765625, Final Batch Loss: 0.0968266949057579\n",
      "Epoch 5569, Loss: 0.39700889587402344, Final Batch Loss: 0.1708545982837677\n",
      "Epoch 5570, Loss: 0.42649461328983307, Final Batch Loss: 0.1599520891904831\n",
      "Epoch 5571, Loss: 0.3831413611769676, Final Batch Loss: 0.13245820999145508\n",
      "Epoch 5572, Loss: 0.24337970092892647, Final Batch Loss: 0.09196390211582184\n",
      "Epoch 5573, Loss: 0.3267704173922539, Final Batch Loss: 0.08249348402023315\n",
      "Epoch 5574, Loss: 0.31139035522937775, Final Batch Loss: 0.10193715989589691\n",
      "Epoch 5575, Loss: 0.26313964277505875, Final Batch Loss: 0.1186731830239296\n",
      "Epoch 5576, Loss: 0.28080088645219803, Final Batch Loss: 0.06799927353858948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5577, Loss: 0.35695796459913254, Final Batch Loss: 0.1565973460674286\n",
      "Epoch 5578, Loss: 0.34633609652519226, Final Batch Loss: 0.09976834058761597\n",
      "Epoch 5579, Loss: 0.3743315190076828, Final Batch Loss: 0.15015307068824768\n",
      "Epoch 5580, Loss: 0.37996261566877365, Final Batch Loss: 0.11986394226551056\n",
      "Epoch 5581, Loss: 0.29375576972961426, Final Batch Loss: 0.05432526022195816\n",
      "Epoch 5582, Loss: 0.3516702800989151, Final Batch Loss: 0.1279294341802597\n",
      "Epoch 5583, Loss: 0.29859136044979095, Final Batch Loss: 0.09380096197128296\n",
      "Epoch 5584, Loss: 0.3210248425602913, Final Batch Loss: 0.07408407330513\n",
      "Epoch 5585, Loss: 0.3418811336159706, Final Batch Loss: 0.14844879508018494\n",
      "Epoch 5586, Loss: 0.3774075210094452, Final Batch Loss: 0.13194909691810608\n",
      "Epoch 5587, Loss: 0.23283661156892776, Final Batch Loss: 0.07218652963638306\n",
      "Epoch 5588, Loss: 0.33735577017068863, Final Batch Loss: 0.05423205345869064\n",
      "Epoch 5589, Loss: 0.3027159124612808, Final Batch Loss: 0.07973343878984451\n",
      "Epoch 5590, Loss: 0.38553721457719803, Final Batch Loss: 0.08165328949689865\n",
      "Epoch 5591, Loss: 0.3536544218659401, Final Batch Loss: 0.1356174349784851\n",
      "Epoch 5592, Loss: 0.38745660334825516, Final Batch Loss: 0.08948663622140884\n",
      "Epoch 5593, Loss: 0.3177279829978943, Final Batch Loss: 0.09995511174201965\n",
      "Epoch 5594, Loss: 0.3146173804998398, Final Batch Loss: 0.0790170207619667\n",
      "Epoch 5595, Loss: 0.31151244044303894, Final Batch Loss: 0.08030868321657181\n",
      "Epoch 5596, Loss: 0.33068570494651794, Final Batch Loss: 0.07038275897502899\n",
      "Epoch 5597, Loss: 0.22252003848552704, Final Batch Loss: 0.04677265137434006\n",
      "Epoch 5598, Loss: 0.3520233482122421, Final Batch Loss: 0.13477329909801483\n",
      "Epoch 5599, Loss: 0.31999538838863373, Final Batch Loss: 0.1288619041442871\n",
      "Epoch 5600, Loss: 0.5416454970836639, Final Batch Loss: 0.23843249678611755\n",
      "Epoch 5601, Loss: 0.3181886002421379, Final Batch Loss: 0.08566780388355255\n",
      "Epoch 5602, Loss: 0.30920568108558655, Final Batch Loss: 0.10513067990541458\n",
      "Epoch 5603, Loss: 0.28256844729185104, Final Batch Loss: 0.11710652709007263\n",
      "Epoch 5604, Loss: 0.30950067192316055, Final Batch Loss: 0.07643276453018188\n",
      "Epoch 5605, Loss: 0.35067182034254074, Final Batch Loss: 0.12005329877138138\n",
      "Epoch 5606, Loss: 0.32238466292619705, Final Batch Loss: 0.11148309707641602\n",
      "Epoch 5607, Loss: 0.3031684048473835, Final Batch Loss: 0.14046764373779297\n",
      "Epoch 5608, Loss: 0.3331916704773903, Final Batch Loss: 0.09680953621864319\n",
      "Epoch 5609, Loss: 0.3140498399734497, Final Batch Loss: 0.0954936072230339\n",
      "Epoch 5610, Loss: 0.3307773545384407, Final Batch Loss: 0.1269366592168808\n",
      "Epoch 5611, Loss: 0.39845307171344757, Final Batch Loss: 0.12659026682376862\n",
      "Epoch 5612, Loss: 0.3786447197198868, Final Batch Loss: 0.1088600605726242\n",
      "Epoch 5613, Loss: 0.4987216964364052, Final Batch Loss: 0.27925798296928406\n",
      "Epoch 5614, Loss: 0.3310815244913101, Final Batch Loss: 0.09954891353845596\n",
      "Epoch 5615, Loss: 0.35726460069417953, Final Batch Loss: 0.11347596347332001\n",
      "Epoch 5616, Loss: 0.37376152724027634, Final Batch Loss: 0.10964276641607285\n",
      "Epoch 5617, Loss: 0.30703090131282806, Final Batch Loss: 0.11259066313505173\n",
      "Epoch 5618, Loss: 0.4867355264723301, Final Batch Loss: 0.21431054174900055\n",
      "Epoch 5619, Loss: 0.40783990919589996, Final Batch Loss: 0.10463476181030273\n",
      "Epoch 5620, Loss: 0.39797136932611465, Final Batch Loss: 0.10613832622766495\n",
      "Epoch 5621, Loss: 0.3839212879538536, Final Batch Loss: 0.13150757551193237\n",
      "Epoch 5622, Loss: 0.30277352780103683, Final Batch Loss: 0.07786966115236282\n",
      "Epoch 5623, Loss: 0.340475894510746, Final Batch Loss: 0.10535942018032074\n",
      "Epoch 5624, Loss: 0.41400182247161865, Final Batch Loss: 0.2559473514556885\n",
      "Epoch 5625, Loss: 0.3172939419746399, Final Batch Loss: 0.11654577404260635\n",
      "Epoch 5626, Loss: 0.29734597355127335, Final Batch Loss: 0.09994422644376755\n",
      "Epoch 5627, Loss: 0.36264097690582275, Final Batch Loss: 0.1006632149219513\n",
      "Epoch 5628, Loss: 0.4177738428115845, Final Batch Loss: 0.14441967010498047\n",
      "Epoch 5629, Loss: 0.31779860705137253, Final Batch Loss: 0.10376981645822525\n",
      "Epoch 5630, Loss: 0.4073888510465622, Final Batch Loss: 0.1776556819677353\n",
      "Epoch 5631, Loss: 0.3359777256846428, Final Batch Loss: 0.1417747288942337\n",
      "Epoch 5632, Loss: 0.3138168454170227, Final Batch Loss: 0.13240182399749756\n",
      "Epoch 5633, Loss: 0.3089147210121155, Final Batch Loss: 0.10010723769664764\n",
      "Epoch 5634, Loss: 0.22500620037317276, Final Batch Loss: 0.08209514617919922\n",
      "Epoch 5635, Loss: 0.45855461806058884, Final Batch Loss: 0.17400558292865753\n",
      "Epoch 5636, Loss: 0.24867486953735352, Final Batch Loss: 0.06348902732133865\n",
      "Epoch 5637, Loss: 0.3647509217262268, Final Batch Loss: 0.09160581976175308\n",
      "Epoch 5638, Loss: 0.3041848838329315, Final Batch Loss: 0.07333465665578842\n",
      "Epoch 5639, Loss: 0.38309066742658615, Final Batch Loss: 0.07292314618825912\n",
      "Epoch 5640, Loss: 0.4695163518190384, Final Batch Loss: 0.07225847244262695\n",
      "Epoch 5641, Loss: 0.38554947823286057, Final Batch Loss: 0.14780141413211823\n",
      "Epoch 5642, Loss: 0.36707524210214615, Final Batch Loss: 0.07142578065395355\n",
      "Epoch 5643, Loss: 0.32674892246723175, Final Batch Loss: 0.0970533937215805\n",
      "Epoch 5644, Loss: 0.29940109699964523, Final Batch Loss: 0.07923755794763565\n",
      "Epoch 5645, Loss: 0.365088053047657, Final Batch Loss: 0.1267850548028946\n",
      "Epoch 5646, Loss: 0.47463739663362503, Final Batch Loss: 0.21228867769241333\n",
      "Epoch 5647, Loss: 0.41748758405447006, Final Batch Loss: 0.13112975656986237\n",
      "Epoch 5648, Loss: 0.38316551595926285, Final Batch Loss: 0.1385975331068039\n",
      "Epoch 5649, Loss: 0.3389761671423912, Final Batch Loss: 0.10578276962041855\n",
      "Epoch 5650, Loss: 0.29357824474573135, Final Batch Loss: 0.09452570229768753\n",
      "Epoch 5651, Loss: 0.329045407474041, Final Batch Loss: 0.1008211150765419\n",
      "Epoch 5652, Loss: 0.35971905291080475, Final Batch Loss: 0.13884934782981873\n",
      "Epoch 5653, Loss: 0.3516114801168442, Final Batch Loss: 0.13752251863479614\n",
      "Epoch 5654, Loss: 0.4112830385565758, Final Batch Loss: 0.11697667092084885\n",
      "Epoch 5655, Loss: 0.3528604656457901, Final Batch Loss: 0.06431753188371658\n",
      "Epoch 5656, Loss: 0.32540471851825714, Final Batch Loss: 0.1173485815525055\n",
      "Epoch 5657, Loss: 0.31847425550222397, Final Batch Loss: 0.10224240273237228\n",
      "Epoch 5658, Loss: 0.3100973069667816, Final Batch Loss: 0.1203315481543541\n",
      "Epoch 5659, Loss: 0.2757859453558922, Final Batch Loss: 0.08024097979068756\n",
      "Epoch 5660, Loss: 0.3489234149456024, Final Batch Loss: 0.14491094648838043\n",
      "Epoch 5661, Loss: 0.344034880399704, Final Batch Loss: 0.10245392471551895\n",
      "Epoch 5662, Loss: 0.35409320518374443, Final Batch Loss: 0.08093468099832535\n",
      "Epoch 5663, Loss: 0.2882016524672508, Final Batch Loss: 0.10936204344034195\n",
      "Epoch 5664, Loss: 0.3177240267395973, Final Batch Loss: 0.08353070914745331\n",
      "Epoch 5665, Loss: 0.43866388499736786, Final Batch Loss: 0.1575442999601364\n",
      "Epoch 5666, Loss: 0.3333894684910774, Final Batch Loss: 0.13331933319568634\n",
      "Epoch 5667, Loss: 0.3384399637579918, Final Batch Loss: 0.08116412162780762\n",
      "Epoch 5668, Loss: 0.33525726944208145, Final Batch Loss: 0.14113809168338776\n",
      "Epoch 5669, Loss: 0.33644692599773407, Final Batch Loss: 0.08522780984640121\n",
      "Epoch 5670, Loss: 0.31932151317596436, Final Batch Loss: 0.1264473795890808\n",
      "Epoch 5671, Loss: 0.32441361248493195, Final Batch Loss: 0.15104681253433228\n",
      "Epoch 5672, Loss: 0.33696532994508743, Final Batch Loss: 0.11098800599575043\n",
      "Epoch 5673, Loss: 0.3088634982705116, Final Batch Loss: 0.07841666787862778\n",
      "Epoch 5674, Loss: 0.3106120228767395, Final Batch Loss: 0.06376079469919205\n",
      "Epoch 5675, Loss: 0.3720250129699707, Final Batch Loss: 0.1404075026512146\n",
      "Epoch 5676, Loss: 0.3925350084900856, Final Batch Loss: 0.14122742414474487\n",
      "Epoch 5677, Loss: 0.3797147050499916, Final Batch Loss: 0.11630376428365707\n",
      "Epoch 5678, Loss: 0.31474942713975906, Final Batch Loss: 0.08512013405561447\n",
      "Epoch 5679, Loss: 0.30591533705592155, Final Batch Loss: 0.05459677800536156\n",
      "Epoch 5680, Loss: 0.3103369176387787, Final Batch Loss: 0.10587700456380844\n",
      "Epoch 5681, Loss: 0.2630827948451042, Final Batch Loss: 0.04853811115026474\n",
      "Epoch 5682, Loss: 0.26639988273382187, Final Batch Loss: 0.05302581191062927\n",
      "Epoch 5683, Loss: 0.39146703481674194, Final Batch Loss: 0.09633779525756836\n",
      "Epoch 5684, Loss: 0.38516105711460114, Final Batch Loss: 0.13415312767028809\n",
      "Epoch 5685, Loss: 0.2637169137597084, Final Batch Loss: 0.062425389885902405\n",
      "Epoch 5686, Loss: 0.2990421801805496, Final Batch Loss: 0.06568530946969986\n",
      "Epoch 5687, Loss: 0.3432222977280617, Final Batch Loss: 0.08312845230102539\n",
      "Epoch 5688, Loss: 0.3905233293771744, Final Batch Loss: 0.07004357874393463\n",
      "Epoch 5689, Loss: 0.3282545581459999, Final Batch Loss: 0.12912821769714355\n",
      "Epoch 5690, Loss: 0.3815232962369919, Final Batch Loss: 0.11355133354663849\n",
      "Epoch 5691, Loss: 0.4082120358943939, Final Batch Loss: 0.23629797995090485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5692, Loss: 0.3368064984679222, Final Batch Loss: 0.09493345022201538\n",
      "Epoch 5693, Loss: 0.3539183512330055, Final Batch Loss: 0.11261118203401566\n",
      "Epoch 5694, Loss: 0.3671472407877445, Final Batch Loss: 0.046352531760931015\n",
      "Epoch 5695, Loss: 0.5069678425788879, Final Batch Loss: 0.24259644746780396\n",
      "Epoch 5696, Loss: 0.4377630650997162, Final Batch Loss: 0.17271959781646729\n",
      "Epoch 5697, Loss: 0.32670828700065613, Final Batch Loss: 0.10318050533533096\n",
      "Epoch 5698, Loss: 0.3237500786781311, Final Batch Loss: 0.1139521449804306\n",
      "Epoch 5699, Loss: 0.40380217880010605, Final Batch Loss: 0.17893631756305695\n",
      "Epoch 5700, Loss: 0.34391258656978607, Final Batch Loss: 0.16609816253185272\n",
      "Epoch 5701, Loss: 0.2862502783536911, Final Batch Loss: 0.08654192835092545\n",
      "Epoch 5702, Loss: 0.5029940456151962, Final Batch Loss: 0.2523472011089325\n",
      "Epoch 5703, Loss: 0.39612458646297455, Final Batch Loss: 0.16687707602977753\n",
      "Epoch 5704, Loss: 0.40641917288303375, Final Batch Loss: 0.10119540989398956\n",
      "Epoch 5705, Loss: 0.4054187759757042, Final Batch Loss: 0.13518382608890533\n",
      "Epoch 5706, Loss: 0.3152729943394661, Final Batch Loss: 0.14515219628810883\n",
      "Epoch 5707, Loss: 0.3772182837128639, Final Batch Loss: 0.08710423856973648\n",
      "Epoch 5708, Loss: 0.4269321411848068, Final Batch Loss: 0.18037307262420654\n",
      "Epoch 5709, Loss: 0.3353758975863457, Final Batch Loss: 0.11035296320915222\n",
      "Epoch 5710, Loss: 0.4767161011695862, Final Batch Loss: 0.18037110567092896\n",
      "Epoch 5711, Loss: 0.3681707829236984, Final Batch Loss: 0.129331573843956\n",
      "Epoch 5712, Loss: 0.3258293718099594, Final Batch Loss: 0.08884597569704056\n",
      "Epoch 5713, Loss: 0.2921299561858177, Final Batch Loss: 0.08849625289440155\n",
      "Epoch 5714, Loss: 0.36127209663391113, Final Batch Loss: 0.11070030182600021\n",
      "Epoch 5715, Loss: 0.3302932381629944, Final Batch Loss: 0.11286444216966629\n",
      "Epoch 5716, Loss: 0.3023681342601776, Final Batch Loss: 0.09527979046106339\n",
      "Epoch 5717, Loss: 0.5090482532978058, Final Batch Loss: 0.19910620152950287\n",
      "Epoch 5718, Loss: 0.41951558738946915, Final Batch Loss: 0.12123704701662064\n",
      "Epoch 5719, Loss: 0.3145962581038475, Final Batch Loss: 0.09993983805179596\n",
      "Epoch 5720, Loss: 0.31265418231487274, Final Batch Loss: 0.07282563298940659\n",
      "Epoch 5721, Loss: 0.2552727088332176, Final Batch Loss: 0.06505762040615082\n",
      "Epoch 5722, Loss: 0.40393178164958954, Final Batch Loss: 0.19027355313301086\n",
      "Epoch 5723, Loss: 0.2820151075720787, Final Batch Loss: 0.09617945551872253\n",
      "Epoch 5724, Loss: 0.32669517397880554, Final Batch Loss: 0.08758224546909332\n",
      "Epoch 5725, Loss: 0.30621296912431717, Final Batch Loss: 0.10668562352657318\n",
      "Epoch 5726, Loss: 0.3037831634283066, Final Batch Loss: 0.06501626968383789\n",
      "Epoch 5727, Loss: 0.3541698008775711, Final Batch Loss: 0.13077412545681\n",
      "Epoch 5728, Loss: 0.3104875758290291, Final Batch Loss: 0.06094183027744293\n",
      "Epoch 5729, Loss: 0.33323758095502853, Final Batch Loss: 0.10692229866981506\n",
      "Epoch 5730, Loss: 0.32766253501176834, Final Batch Loss: 0.0884636864066124\n",
      "Epoch 5731, Loss: 0.3008015528321266, Final Batch Loss: 0.08600827306509018\n",
      "Epoch 5732, Loss: 0.46620728075504303, Final Batch Loss: 0.15303491055965424\n",
      "Epoch 5733, Loss: 0.2576141580939293, Final Batch Loss: 0.07761626690626144\n",
      "Epoch 5734, Loss: 0.2997887656092644, Final Batch Loss: 0.10999179631471634\n",
      "Epoch 5735, Loss: 0.33725183457136154, Final Batch Loss: 0.10109765827655792\n",
      "Epoch 5736, Loss: 0.3274889513850212, Final Batch Loss: 0.11049266159534454\n",
      "Epoch 5737, Loss: 0.3893015384674072, Final Batch Loss: 0.15616321563720703\n",
      "Epoch 5738, Loss: 0.3102484941482544, Final Batch Loss: 0.13343504071235657\n",
      "Epoch 5739, Loss: 0.40241789072752, Final Batch Loss: 0.08488021045923233\n",
      "Epoch 5740, Loss: 0.4282119572162628, Final Batch Loss: 0.13989843428134918\n",
      "Epoch 5741, Loss: 0.37106936424970627, Final Batch Loss: 0.08841108530759811\n",
      "Epoch 5742, Loss: 0.29090849310159683, Final Batch Loss: 0.0942896157503128\n",
      "Epoch 5743, Loss: 0.2878975346684456, Final Batch Loss: 0.11209315061569214\n",
      "Epoch 5744, Loss: 0.3571303188800812, Final Batch Loss: 0.18614858388900757\n",
      "Epoch 5745, Loss: 0.3380727395415306, Final Batch Loss: 0.09273523092269897\n",
      "Epoch 5746, Loss: 0.37323736399412155, Final Batch Loss: 0.08627323061227798\n",
      "Epoch 5747, Loss: 0.4419018626213074, Final Batch Loss: 0.15697912871837616\n",
      "Epoch 5748, Loss: 0.31752200424671173, Final Batch Loss: 0.08912581205368042\n",
      "Epoch 5749, Loss: 0.31882859021425247, Final Batch Loss: 0.10610352456569672\n",
      "Epoch 5750, Loss: 0.34269504994153976, Final Batch Loss: 0.08703792840242386\n",
      "Epoch 5751, Loss: 0.4126812294125557, Final Batch Loss: 0.09789081662893295\n",
      "Epoch 5752, Loss: 0.3046087175607681, Final Batch Loss: 0.07146268337965012\n",
      "Epoch 5753, Loss: 0.2070346400141716, Final Batch Loss: 0.05249246209859848\n",
      "Epoch 5754, Loss: 0.3563363701105118, Final Batch Loss: 0.09269396960735321\n",
      "Epoch 5755, Loss: 0.34213460236787796, Final Batch Loss: 0.0714816078543663\n",
      "Epoch 5756, Loss: 0.42806608974933624, Final Batch Loss: 0.10851413011550903\n",
      "Epoch 5757, Loss: 0.34648751467466354, Final Batch Loss: 0.14548486471176147\n",
      "Epoch 5758, Loss: 0.33550528436899185, Final Batch Loss: 0.1286882907152176\n",
      "Epoch 5759, Loss: 0.35394105315208435, Final Batch Loss: 0.10497786104679108\n",
      "Epoch 5760, Loss: 0.28453677892684937, Final Batch Loss: 0.07874519377946854\n",
      "Epoch 5761, Loss: 0.3162328526377678, Final Batch Loss: 0.10532903671264648\n",
      "Epoch 5762, Loss: 0.5211538672447205, Final Batch Loss: 0.20251448452472687\n",
      "Epoch 5763, Loss: 0.341042622923851, Final Batch Loss: 0.11390972882509232\n",
      "Epoch 5764, Loss: 0.33972760289907455, Final Batch Loss: 0.10442829877138138\n",
      "Epoch 5765, Loss: 0.36018693447113037, Final Batch Loss: 0.11723282933235168\n",
      "Epoch 5766, Loss: 0.34934233129024506, Final Batch Loss: 0.06869073212146759\n",
      "Epoch 5767, Loss: 0.38488680869340897, Final Batch Loss: 0.1762159764766693\n",
      "Epoch 5768, Loss: 0.2990756928920746, Final Batch Loss: 0.08257954567670822\n",
      "Epoch 5769, Loss: 0.4168534204363823, Final Batch Loss: 0.16424202919006348\n",
      "Epoch 5770, Loss: 0.3266407400369644, Final Batch Loss: 0.08629396557807922\n",
      "Epoch 5771, Loss: 0.24672628939151764, Final Batch Loss: 0.06998465955257416\n",
      "Epoch 5772, Loss: 0.39386286586523056, Final Batch Loss: 0.08861454576253891\n",
      "Epoch 5773, Loss: 0.2708573415875435, Final Batch Loss: 0.07811929285526276\n",
      "Epoch 5774, Loss: 0.36405234783887863, Final Batch Loss: 0.10229291766881943\n",
      "Epoch 5775, Loss: 0.4108833223581314, Final Batch Loss: 0.13231047987937927\n",
      "Epoch 5776, Loss: 0.2905793935060501, Final Batch Loss: 0.12707382440567017\n",
      "Epoch 5777, Loss: 0.32423367351293564, Final Batch Loss: 0.079994797706604\n",
      "Epoch 5778, Loss: 0.4057433158159256, Final Batch Loss: 0.16422344744205475\n",
      "Epoch 5779, Loss: 0.37254849076271057, Final Batch Loss: 0.13993485271930695\n",
      "Epoch 5780, Loss: 0.3540654554963112, Final Batch Loss: 0.11709518730640411\n",
      "Epoch 5781, Loss: 0.37094157189130783, Final Batch Loss: 0.12077841907739639\n",
      "Epoch 5782, Loss: 0.34498804807662964, Final Batch Loss: 0.12688636779785156\n",
      "Epoch 5783, Loss: 0.29916127026081085, Final Batch Loss: 0.0712411031126976\n",
      "Epoch 5784, Loss: 0.3150152117013931, Final Batch Loss: 0.11913774907588959\n",
      "Epoch 5785, Loss: 0.30783507972955704, Final Batch Loss: 0.1447313278913498\n",
      "Epoch 5786, Loss: 0.2445479929447174, Final Batch Loss: 0.11573164165019989\n",
      "Epoch 5787, Loss: 0.36844442784786224, Final Batch Loss: 0.15209722518920898\n",
      "Epoch 5788, Loss: 0.4864402860403061, Final Batch Loss: 0.19768507778644562\n",
      "Epoch 5789, Loss: 0.32706447690725327, Final Batch Loss: 0.08665643632411957\n",
      "Epoch 5790, Loss: 0.3209320679306984, Final Batch Loss: 0.12937869131565094\n",
      "Epoch 5791, Loss: 0.31158745288848877, Final Batch Loss: 0.12759113311767578\n",
      "Epoch 5792, Loss: 0.3108275905251503, Final Batch Loss: 0.11701557040214539\n",
      "Epoch 5793, Loss: 0.34929944574832916, Final Batch Loss: 0.10406999289989471\n",
      "Epoch 5794, Loss: 0.34014442563056946, Final Batch Loss: 0.11483602225780487\n",
      "Epoch 5795, Loss: 0.3897085562348366, Final Batch Loss: 0.18860623240470886\n",
      "Epoch 5796, Loss: 0.26528922468423843, Final Batch Loss: 0.0683918371796608\n",
      "Epoch 5797, Loss: 0.3485342934727669, Final Batch Loss: 0.11532266438007355\n",
      "Epoch 5798, Loss: 0.451157845556736, Final Batch Loss: 0.19353444874286652\n",
      "Epoch 5799, Loss: 0.30148186534643173, Final Batch Loss: 0.08812686800956726\n",
      "Epoch 5800, Loss: 0.2768321633338928, Final Batch Loss: 0.12210937589406967\n",
      "Epoch 5801, Loss: 0.47936202585697174, Final Batch Loss: 0.16967995464801788\n",
      "Epoch 5802, Loss: 0.32756366580724716, Final Batch Loss: 0.09777901321649551\n",
      "Epoch 5803, Loss: 0.3318922072649002, Final Batch Loss: 0.07576517760753632\n",
      "Epoch 5804, Loss: 0.3211500197649002, Final Batch Loss: 0.08085063844919205\n",
      "Epoch 5805, Loss: 0.2805812731385231, Final Batch Loss: 0.11398132890462875\n",
      "Epoch 5806, Loss: 0.31729649007320404, Final Batch Loss: 0.1315234899520874\n",
      "Epoch 5807, Loss: 0.37112829089164734, Final Batch Loss: 0.09607967734336853\n",
      "Epoch 5808, Loss: 0.35405183583498, Final Batch Loss: 0.13475236296653748\n",
      "Epoch 5809, Loss: 0.3477766290307045, Final Batch Loss: 0.06722522526979446\n",
      "Epoch 5810, Loss: 0.35247500240802765, Final Batch Loss: 0.14625373482704163\n",
      "Epoch 5811, Loss: 0.3704417794942856, Final Batch Loss: 0.101000115275383\n",
      "Epoch 5812, Loss: 0.4296545088291168, Final Batch Loss: 0.14032785594463348\n",
      "Epoch 5813, Loss: 0.3284169062972069, Final Batch Loss: 0.11706586182117462\n",
      "Epoch 5814, Loss: 0.2468578964471817, Final Batch Loss: 0.0672866478562355\n",
      "Epoch 5815, Loss: 0.32058560103178024, Final Batch Loss: 0.06865189224481583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5816, Loss: 0.36885055154561996, Final Batch Loss: 0.14055858552455902\n",
      "Epoch 5817, Loss: 0.43566957116127014, Final Batch Loss: 0.16648469865322113\n",
      "Epoch 5818, Loss: 0.38602399080991745, Final Batch Loss: 0.07509847730398178\n",
      "Epoch 5819, Loss: 0.39995548129081726, Final Batch Loss: 0.1430700570344925\n",
      "Epoch 5820, Loss: 0.332229420542717, Final Batch Loss: 0.1196083351969719\n",
      "Epoch 5821, Loss: 0.30130136013031006, Final Batch Loss: 0.07853518426418304\n",
      "Epoch 5822, Loss: 0.28744660317897797, Final Batch Loss: 0.06659866124391556\n",
      "Epoch 5823, Loss: 0.23539359867572784, Final Batch Loss: 0.05592883378267288\n",
      "Epoch 5824, Loss: 0.3574977442622185, Final Batch Loss: 0.11537589132785797\n",
      "Epoch 5825, Loss: 0.49978288263082504, Final Batch Loss: 0.24880515038967133\n",
      "Epoch 5826, Loss: 0.3962574899196625, Final Batch Loss: 0.1739683300256729\n",
      "Epoch 5827, Loss: 0.38715760409832, Final Batch Loss: 0.12910722196102142\n",
      "Epoch 5828, Loss: 0.3584112972021103, Final Batch Loss: 0.12797428667545319\n",
      "Epoch 5829, Loss: 0.3164892941713333, Final Batch Loss: 0.09289483726024628\n",
      "Epoch 5830, Loss: 0.3224448561668396, Final Batch Loss: 0.1032889112830162\n",
      "Epoch 5831, Loss: 0.3803122565150261, Final Batch Loss: 0.08253473788499832\n",
      "Epoch 5832, Loss: 0.35436470061540604, Final Batch Loss: 0.13871456682682037\n",
      "Epoch 5833, Loss: 0.327926330268383, Final Batch Loss: 0.12314002215862274\n",
      "Epoch 5834, Loss: 0.41890785843133926, Final Batch Loss: 0.10146873444318771\n",
      "Epoch 5835, Loss: 0.3021632358431816, Final Batch Loss: 0.11544063687324524\n",
      "Epoch 5836, Loss: 0.3224557340145111, Final Batch Loss: 0.06729539483785629\n",
      "Epoch 5837, Loss: 0.42692482471466064, Final Batch Loss: 0.16005469858646393\n",
      "Epoch 5838, Loss: 0.35188838839530945, Final Batch Loss: 0.06383993476629257\n",
      "Epoch 5839, Loss: 0.3620825633406639, Final Batch Loss: 0.12724901735782623\n",
      "Epoch 5840, Loss: 0.3491946831345558, Final Batch Loss: 0.11969595402479172\n",
      "Epoch 5841, Loss: 0.3545445576310158, Final Batch Loss: 0.10864058881998062\n",
      "Epoch 5842, Loss: 0.31948672235012054, Final Batch Loss: 0.08072603493928909\n",
      "Epoch 5843, Loss: 0.39997170865535736, Final Batch Loss: 0.101743683218956\n",
      "Epoch 5844, Loss: 0.31242091953754425, Final Batch Loss: 0.07887449115514755\n",
      "Epoch 5845, Loss: 0.3401564434170723, Final Batch Loss: 0.11311990767717361\n",
      "Epoch 5846, Loss: 0.35451028496026993, Final Batch Loss: 0.10782790929079056\n",
      "Epoch 5847, Loss: 0.4114532321691513, Final Batch Loss: 0.1147124320268631\n",
      "Epoch 5848, Loss: 0.4034431502223015, Final Batch Loss: 0.11822409182786942\n",
      "Epoch 5849, Loss: 0.34926342964172363, Final Batch Loss: 0.10321048647165298\n",
      "Epoch 5850, Loss: 0.3660963550209999, Final Batch Loss: 0.11415436118841171\n",
      "Epoch 5851, Loss: 0.39887984097003937, Final Batch Loss: 0.17469178140163422\n",
      "Epoch 5852, Loss: 0.30869168788194656, Final Batch Loss: 0.06891747564077377\n",
      "Epoch 5853, Loss: 0.3440302535891533, Final Batch Loss: 0.12759259343147278\n",
      "Epoch 5854, Loss: 0.42027683556079865, Final Batch Loss: 0.19366887211799622\n",
      "Epoch 5855, Loss: 0.38715270161628723, Final Batch Loss: 0.08800359070301056\n",
      "Epoch 5856, Loss: 0.37682947516441345, Final Batch Loss: 0.11150173097848892\n",
      "Epoch 5857, Loss: 0.4380556493997574, Final Batch Loss: 0.1710798293352127\n",
      "Epoch 5858, Loss: 0.4184275418519974, Final Batch Loss: 0.18483492732048035\n",
      "Epoch 5859, Loss: 0.36172694712877274, Final Batch Loss: 0.127422034740448\n",
      "Epoch 5860, Loss: 0.3683398514986038, Final Batch Loss: 0.13908444344997406\n",
      "Epoch 5861, Loss: 0.3338626101613045, Final Batch Loss: 0.07661216706037521\n",
      "Epoch 5862, Loss: 0.4460434913635254, Final Batch Loss: 0.18535678088665009\n",
      "Epoch 5863, Loss: 0.32752712815999985, Final Batch Loss: 0.11689002066850662\n",
      "Epoch 5864, Loss: 0.375428281724453, Final Batch Loss: 0.11372750252485275\n",
      "Epoch 5865, Loss: 0.28659744933247566, Final Batch Loss: 0.11843421310186386\n",
      "Epoch 5866, Loss: 0.31779371201992035, Final Batch Loss: 0.1381550133228302\n",
      "Epoch 5867, Loss: 0.36315207183361053, Final Batch Loss: 0.153159961104393\n",
      "Epoch 5868, Loss: 0.39033573120832443, Final Batch Loss: 0.1518276333808899\n",
      "Epoch 5869, Loss: 0.4610069543123245, Final Batch Loss: 0.1004425436258316\n",
      "Epoch 5870, Loss: 0.3635910227894783, Final Batch Loss: 0.10120879858732224\n",
      "Epoch 5871, Loss: 0.32180432975292206, Final Batch Loss: 0.08740824460983276\n",
      "Epoch 5872, Loss: 0.2912657931447029, Final Batch Loss: 0.09981963038444519\n",
      "Epoch 5873, Loss: 0.25683021545410156, Final Batch Loss: 0.057498082518577576\n",
      "Epoch 5874, Loss: 0.31699899956583977, Final Batch Loss: 0.15252278745174408\n",
      "Epoch 5875, Loss: 0.30231425166130066, Final Batch Loss: 0.10394830256700516\n",
      "Epoch 5876, Loss: 0.49882539361715317, Final Batch Loss: 0.19920513033866882\n",
      "Epoch 5877, Loss: 0.5073616355657578, Final Batch Loss: 0.13182570040225983\n",
      "Epoch 5878, Loss: 0.325897678732872, Final Batch Loss: 0.10704469680786133\n",
      "Epoch 5879, Loss: 0.31040938198566437, Final Batch Loss: 0.08086909353733063\n",
      "Epoch 5880, Loss: 0.3620356246829033, Final Batch Loss: 0.11009901016950607\n",
      "Epoch 5881, Loss: 0.2906264141201973, Final Batch Loss: 0.06831996142864227\n",
      "Epoch 5882, Loss: 0.3157845363020897, Final Batch Loss: 0.11867106705904007\n",
      "Epoch 5883, Loss: 0.32922276854515076, Final Batch Loss: 0.10099011659622192\n",
      "Epoch 5884, Loss: 0.34735122323036194, Final Batch Loss: 0.1405053734779358\n",
      "Epoch 5885, Loss: 0.28939275443553925, Final Batch Loss: 0.08401937782764435\n",
      "Epoch 5886, Loss: 0.26703114435076714, Final Batch Loss: 0.10491254180669785\n",
      "Epoch 5887, Loss: 0.24339105933904648, Final Batch Loss: 0.07516548037528992\n",
      "Epoch 5888, Loss: 0.295616939663887, Final Batch Loss: 0.0972931906580925\n",
      "Epoch 5889, Loss: 0.3373940885066986, Final Batch Loss: 0.10964445024728775\n",
      "Epoch 5890, Loss: 0.33504990488290787, Final Batch Loss: 0.11719851940870285\n",
      "Epoch 5891, Loss: 0.42020081728696823, Final Batch Loss: 0.2181245982646942\n",
      "Epoch 5892, Loss: 0.3408273234963417, Final Batch Loss: 0.1165785863995552\n",
      "Epoch 5893, Loss: 0.3606696054339409, Final Batch Loss: 0.1435326635837555\n",
      "Epoch 5894, Loss: 0.25693289563059807, Final Batch Loss: 0.04605979844927788\n",
      "Epoch 5895, Loss: 0.34547798335552216, Final Batch Loss: 0.14454469084739685\n",
      "Epoch 5896, Loss: 0.2975110486149788, Final Batch Loss: 0.14004012942314148\n",
      "Epoch 5897, Loss: 0.3849315270781517, Final Batch Loss: 0.1244138553738594\n",
      "Epoch 5898, Loss: 0.3325820490717888, Final Batch Loss: 0.0889776274561882\n",
      "Epoch 5899, Loss: 0.31423021107912064, Final Batch Loss: 0.15592758357524872\n",
      "Epoch 5900, Loss: 0.2832559645175934, Final Batch Loss: 0.07908995449542999\n",
      "Epoch 5901, Loss: 0.3659379333257675, Final Batch Loss: 0.10239104926586151\n",
      "Epoch 5902, Loss: 0.31574977934360504, Final Batch Loss: 0.10659287124872208\n",
      "Epoch 5903, Loss: 0.4124870225787163, Final Batch Loss: 0.0916251465678215\n",
      "Epoch 5904, Loss: 0.35374416410923004, Final Batch Loss: 0.13153021037578583\n",
      "Epoch 5905, Loss: 0.4668772220611572, Final Batch Loss: 0.21008571982383728\n",
      "Epoch 5906, Loss: 0.3678356856107712, Final Batch Loss: 0.10547652840614319\n",
      "Epoch 5907, Loss: 0.28390511125326157, Final Batch Loss: 0.10890772193670273\n",
      "Epoch 5908, Loss: 0.3194220997393131, Final Batch Loss: 0.04812674596905708\n",
      "Epoch 5909, Loss: 0.40473248809576035, Final Batch Loss: 0.14762400090694427\n",
      "Epoch 5910, Loss: 0.37792733311653137, Final Batch Loss: 0.1208791434764862\n",
      "Epoch 5911, Loss: 0.1992332600057125, Final Batch Loss: 0.03228196129202843\n",
      "Epoch 5912, Loss: 0.33343419432640076, Final Batch Loss: 0.09757931530475616\n",
      "Epoch 5913, Loss: 0.35856879502534866, Final Batch Loss: 0.105735182762146\n",
      "Epoch 5914, Loss: 0.32862619310617447, Final Batch Loss: 0.09051956236362457\n",
      "Epoch 5915, Loss: 0.27983054891228676, Final Batch Loss: 0.1416795253753662\n",
      "Epoch 5916, Loss: 0.35732945799827576, Final Batch Loss: 0.13043387234210968\n",
      "Epoch 5917, Loss: 0.3501439318060875, Final Batch Loss: 0.1190904974937439\n",
      "Epoch 5918, Loss: 0.3352924510836601, Final Batch Loss: 0.14494173228740692\n",
      "Epoch 5919, Loss: 0.34108907729387283, Final Batch Loss: 0.07545474916696548\n",
      "Epoch 5920, Loss: 0.312077172100544, Final Batch Loss: 0.12568745017051697\n",
      "Epoch 5921, Loss: 0.38238711655139923, Final Batch Loss: 0.14678306877613068\n",
      "Epoch 5922, Loss: 0.3994007557630539, Final Batch Loss: 0.08491504192352295\n",
      "Epoch 5923, Loss: 0.3732921630144119, Final Batch Loss: 0.16910414397716522\n",
      "Epoch 5924, Loss: 0.32182958349585533, Final Batch Loss: 0.061347704380750656\n",
      "Epoch 5925, Loss: 0.3850844278931618, Final Batch Loss: 0.10604983568191528\n",
      "Epoch 5926, Loss: 0.27049441635608673, Final Batch Loss: 0.130900576710701\n",
      "Epoch 5927, Loss: 0.3455583453178406, Final Batch Loss: 0.11634550243616104\n",
      "Epoch 5928, Loss: 0.2509034723043442, Final Batch Loss: 0.10007847845554352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5929, Loss: 0.3989329859614372, Final Batch Loss: 0.12417387217283249\n",
      "Epoch 5930, Loss: 0.34553902596235275, Final Batch Loss: 0.08546824008226395\n",
      "Epoch 5931, Loss: 0.3642158657312393, Final Batch Loss: 0.14086806774139404\n",
      "Epoch 5932, Loss: 0.2911907248198986, Final Batch Loss: 0.05184384807944298\n",
      "Epoch 5933, Loss: 0.3720381185412407, Final Batch Loss: 0.13394112884998322\n",
      "Epoch 5934, Loss: 0.255212664604187, Final Batch Loss: 0.05788704752922058\n",
      "Epoch 5935, Loss: 0.35937172919511795, Final Batch Loss: 0.13090212643146515\n",
      "Epoch 5936, Loss: 0.2572220414876938, Final Batch Loss: 0.08740127831697464\n",
      "Epoch 5937, Loss: 0.46372514963150024, Final Batch Loss: 0.15437139570713043\n",
      "Epoch 5938, Loss: 0.34581877291202545, Final Batch Loss: 0.0926307663321495\n",
      "Epoch 5939, Loss: 0.4070517495274544, Final Batch Loss: 0.2011430859565735\n",
      "Epoch 5940, Loss: 0.3366265445947647, Final Batch Loss: 0.13373659551143646\n",
      "Epoch 5941, Loss: 0.4778297320008278, Final Batch Loss: 0.20508599281311035\n",
      "Epoch 5942, Loss: 0.3725743517279625, Final Batch Loss: 0.18593770265579224\n",
      "Epoch 5943, Loss: 0.37913845479488373, Final Batch Loss: 0.1462215632200241\n",
      "Epoch 5944, Loss: 0.32642238587141037, Final Batch Loss: 0.11019973456859589\n",
      "Epoch 5945, Loss: 0.28703565150499344, Final Batch Loss: 0.10826658457517624\n",
      "Epoch 5946, Loss: 0.40150419622659683, Final Batch Loss: 0.10499940067529678\n",
      "Epoch 5947, Loss: 0.3387834280729294, Final Batch Loss: 0.140674889087677\n",
      "Epoch 5948, Loss: 0.3793131858110428, Final Batch Loss: 0.13791637122631073\n",
      "Epoch 5949, Loss: 0.42300713062286377, Final Batch Loss: 0.19127966463565826\n",
      "Epoch 5950, Loss: 0.4885411411523819, Final Batch Loss: 0.1592082381248474\n",
      "Epoch 5951, Loss: 0.32694412022829056, Final Batch Loss: 0.09964606165885925\n",
      "Epoch 5952, Loss: 0.35137882456183434, Final Batch Loss: 0.15523409843444824\n",
      "Epoch 5953, Loss: 0.32865578681230545, Final Batch Loss: 0.09961915761232376\n",
      "Epoch 5954, Loss: 0.30587294697761536, Final Batch Loss: 0.10130225867033005\n",
      "Epoch 5955, Loss: 0.3712320700287819, Final Batch Loss: 0.15504951775074005\n",
      "Epoch 5956, Loss: 0.3014368861913681, Final Batch Loss: 0.12373615056276321\n",
      "Epoch 5957, Loss: 0.33333881944417953, Final Batch Loss: 0.08858583122491837\n",
      "Epoch 5958, Loss: 0.3848981186747551, Final Batch Loss: 0.13294488191604614\n",
      "Epoch 5959, Loss: 0.308308944106102, Final Batch Loss: 0.07276623696088791\n",
      "Epoch 5960, Loss: 0.319257952272892, Final Batch Loss: 0.1071452721953392\n",
      "Epoch 5961, Loss: 0.2780133746564388, Final Batch Loss: 0.054109591990709305\n",
      "Epoch 5962, Loss: 0.31601980328559875, Final Batch Loss: 0.11465376615524292\n",
      "Epoch 5963, Loss: 0.3344945162534714, Final Batch Loss: 0.08140988647937775\n",
      "Epoch 5964, Loss: 0.24278289824724197, Final Batch Loss: 0.07971297949552536\n",
      "Epoch 5965, Loss: 0.29966992884874344, Final Batch Loss: 0.0668717697262764\n",
      "Epoch 5966, Loss: 0.26812268793582916, Final Batch Loss: 0.10854899138212204\n",
      "Epoch 5967, Loss: 0.42004989087581635, Final Batch Loss: 0.18974949419498444\n",
      "Epoch 5968, Loss: 0.32676146924495697, Final Batch Loss: 0.12278076261281967\n",
      "Epoch 5969, Loss: 0.2880067452788353, Final Batch Loss: 0.06492743641138077\n",
      "Epoch 5970, Loss: 0.2937273755669594, Final Batch Loss: 0.08114747703075409\n",
      "Epoch 5971, Loss: 0.22604501247406006, Final Batch Loss: 0.06434882432222366\n",
      "Epoch 5972, Loss: 0.3655972182750702, Final Batch Loss: 0.10961548984050751\n",
      "Epoch 5973, Loss: 0.3630778416991234, Final Batch Loss: 0.10908036679029465\n",
      "Epoch 5974, Loss: 0.2953941076993942, Final Batch Loss: 0.1344803273677826\n",
      "Epoch 5975, Loss: 0.3245902583003044, Final Batch Loss: 0.0899026021361351\n",
      "Epoch 5976, Loss: 0.30934086441993713, Final Batch Loss: 0.11031708866357803\n",
      "Epoch 5977, Loss: 0.3934776037931442, Final Batch Loss: 0.1710495799779892\n",
      "Epoch 5978, Loss: 0.3066680505871773, Final Batch Loss: 0.10601173341274261\n",
      "Epoch 5979, Loss: 0.3692449927330017, Final Batch Loss: 0.15042318403720856\n",
      "Epoch 5980, Loss: 0.3220517039299011, Final Batch Loss: 0.07490333914756775\n",
      "Epoch 5981, Loss: 0.2909448370337486, Final Batch Loss: 0.09577114880084991\n",
      "Epoch 5982, Loss: 0.28364839032292366, Final Batch Loss: 0.10406435281038284\n",
      "Epoch 5983, Loss: 0.25460443645715714, Final Batch Loss: 0.08565162867307663\n",
      "Epoch 5984, Loss: 0.3385623097419739, Final Batch Loss: 0.07799291610717773\n",
      "Epoch 5985, Loss: 0.2561344914138317, Final Batch Loss: 0.1040690690279007\n",
      "Epoch 5986, Loss: 0.3086480349302292, Final Batch Loss: 0.1447843760251999\n",
      "Epoch 5987, Loss: 0.34730713814496994, Final Batch Loss: 0.11043825000524521\n",
      "Epoch 5988, Loss: 0.3820452094078064, Final Batch Loss: 0.1337960660457611\n",
      "Epoch 5989, Loss: 0.34071359038352966, Final Batch Loss: 0.1391240954399109\n",
      "Epoch 5990, Loss: 0.29478973895311356, Final Batch Loss: 0.09211520850658417\n",
      "Epoch 5991, Loss: 0.2837415374815464, Final Batch Loss: 0.09917940199375153\n",
      "Epoch 5992, Loss: 0.3108764737844467, Final Batch Loss: 0.10067367553710938\n",
      "Epoch 5993, Loss: 0.4538927227258682, Final Batch Loss: 0.14714570343494415\n",
      "Epoch 5994, Loss: 0.3522641733288765, Final Batch Loss: 0.11405366659164429\n",
      "Epoch 5995, Loss: 0.335850827395916, Final Batch Loss: 0.08118456602096558\n",
      "Epoch 5996, Loss: 0.2378430739045143, Final Batch Loss: 0.07710246741771698\n",
      "Epoch 5997, Loss: 0.32763827592134476, Final Batch Loss: 0.1269848793745041\n",
      "Epoch 5998, Loss: 0.22897586226463318, Final Batch Loss: 0.0898451879620552\n",
      "Epoch 5999, Loss: 0.2626020833849907, Final Batch Loss: 0.07045713067054749\n",
      "Epoch 6000, Loss: 0.3698331154882908, Final Batch Loss: 0.20020022988319397\n",
      "Epoch 6001, Loss: 0.3150764927268028, Final Batch Loss: 0.1151161789894104\n",
      "Epoch 6002, Loss: 0.42270679771900177, Final Batch Loss: 0.11438632011413574\n",
      "Epoch 6003, Loss: 0.3519775792956352, Final Batch Loss: 0.09501281380653381\n",
      "Epoch 6004, Loss: 0.28522252291440964, Final Batch Loss: 0.09327754378318787\n",
      "Epoch 6005, Loss: 0.2331875115633011, Final Batch Loss: 0.07109025120735168\n",
      "Epoch 6006, Loss: 0.37120621651411057, Final Batch Loss: 0.14918482303619385\n",
      "Epoch 6007, Loss: 0.3436347171664238, Final Batch Loss: 0.06765737384557724\n",
      "Epoch 6008, Loss: 0.24564896523952484, Final Batch Loss: 0.06887130439281464\n",
      "Epoch 6009, Loss: 0.2991938441991806, Final Batch Loss: 0.0757826566696167\n",
      "Epoch 6010, Loss: 0.33907825499773026, Final Batch Loss: 0.1305774301290512\n",
      "Epoch 6011, Loss: 0.43556421995162964, Final Batch Loss: 0.11524200439453125\n",
      "Epoch 6012, Loss: 0.34078022837638855, Final Batch Loss: 0.12571637332439423\n",
      "Epoch 6013, Loss: 0.34632445126771927, Final Batch Loss: 0.10801803320646286\n",
      "Epoch 6014, Loss: 0.30475864559412, Final Batch Loss: 0.13074196875095367\n",
      "Epoch 6015, Loss: 0.40508463978767395, Final Batch Loss: 0.1523573398590088\n",
      "Epoch 6016, Loss: 0.3076602891087532, Final Batch Loss: 0.12833723425865173\n",
      "Epoch 6017, Loss: 0.3290962055325508, Final Batch Loss: 0.1022154837846756\n",
      "Epoch 6018, Loss: 0.38297584652900696, Final Batch Loss: 0.09340350329875946\n",
      "Epoch 6019, Loss: 0.3728073984384537, Final Batch Loss: 0.15115799009799957\n",
      "Epoch 6020, Loss: 0.5871767401695251, Final Batch Loss: 0.24640384316444397\n",
      "Epoch 6021, Loss: 0.3342730179429054, Final Batch Loss: 0.14586250483989716\n",
      "Epoch 6022, Loss: 0.3779694736003876, Final Batch Loss: 0.1205160841345787\n",
      "Epoch 6023, Loss: 0.3388347402215004, Final Batch Loss: 0.090021513402462\n",
      "Epoch 6024, Loss: 0.29552479833364487, Final Batch Loss: 0.06405452638864517\n",
      "Epoch 6025, Loss: 0.37775788456201553, Final Batch Loss: 0.12351197749376297\n",
      "Epoch 6026, Loss: 0.44142962247133255, Final Batch Loss: 0.18579429388046265\n",
      "Epoch 6027, Loss: 0.37182098627090454, Final Batch Loss: 0.14621031284332275\n",
      "Epoch 6028, Loss: 0.3265044242143631, Final Batch Loss: 0.068233922123909\n",
      "Epoch 6029, Loss: 0.34698386490345, Final Batch Loss: 0.12041075527667999\n",
      "Epoch 6030, Loss: 0.3958418294787407, Final Batch Loss: 0.1362912654876709\n",
      "Epoch 6031, Loss: 0.3429352715611458, Final Batch Loss: 0.08692535758018494\n",
      "Epoch 6032, Loss: 0.23635931685566902, Final Batch Loss: 0.055955756455659866\n",
      "Epoch 6033, Loss: 0.2572036013007164, Final Batch Loss: 0.08327559381723404\n",
      "Epoch 6034, Loss: 0.2907168082892895, Final Batch Loss: 0.05704905465245247\n",
      "Epoch 6035, Loss: 0.32278481870889664, Final Batch Loss: 0.07688210904598236\n",
      "Epoch 6036, Loss: 0.2767816036939621, Final Batch Loss: 0.13954675197601318\n",
      "Epoch 6037, Loss: 0.30272094160318375, Final Batch Loss: 0.09270088374614716\n",
      "Epoch 6038, Loss: 0.23179183155298233, Final Batch Loss: 0.06215054541826248\n",
      "Epoch 6039, Loss: 0.326596699655056, Final Batch Loss: 0.0570359081029892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6040, Loss: 0.3945101872086525, Final Batch Loss: 0.10665711015462875\n",
      "Epoch 6041, Loss: 0.3633652403950691, Final Batch Loss: 0.1505052000284195\n",
      "Epoch 6042, Loss: 0.31934593617916107, Final Batch Loss: 0.10562225431203842\n",
      "Epoch 6043, Loss: 0.3316754698753357, Final Batch Loss: 0.1246226578950882\n",
      "Epoch 6044, Loss: 0.2747175768017769, Final Batch Loss: 0.08991095423698425\n",
      "Epoch 6045, Loss: 0.27207354456186295, Final Batch Loss: 0.10090532153844833\n",
      "Epoch 6046, Loss: 0.4571069926023483, Final Batch Loss: 0.16637228429317474\n",
      "Epoch 6047, Loss: 0.2688922956585884, Final Batch Loss: 0.09141573309898376\n",
      "Epoch 6048, Loss: 0.29108183085918427, Final Batch Loss: 0.08153527230024338\n",
      "Epoch 6049, Loss: 0.2538638114929199, Final Batch Loss: 0.06353851407766342\n",
      "Epoch 6050, Loss: 0.2869250476360321, Final Batch Loss: 0.08355989307165146\n",
      "Epoch 6051, Loss: 0.26950687915086746, Final Batch Loss: 0.07314427942037582\n",
      "Epoch 6052, Loss: 0.30772417038679123, Final Batch Loss: 0.07447654753923416\n",
      "Epoch 6053, Loss: 0.25214502960443497, Final Batch Loss: 0.10725907236337662\n",
      "Epoch 6054, Loss: 0.2247762270271778, Final Batch Loss: 0.054294321686029434\n",
      "Epoch 6055, Loss: 0.3195415511727333, Final Batch Loss: 0.0744972974061966\n",
      "Epoch 6056, Loss: 0.2780558317899704, Final Batch Loss: 0.04008851200342178\n",
      "Epoch 6057, Loss: 0.3685693219304085, Final Batch Loss: 0.11611367762088776\n",
      "Epoch 6058, Loss: 0.3459564968943596, Final Batch Loss: 0.12418730556964874\n",
      "Epoch 6059, Loss: 0.2553444281220436, Final Batch Loss: 0.09848316013813019\n",
      "Epoch 6060, Loss: 0.30692486092448235, Final Batch Loss: 0.12918001413345337\n",
      "Epoch 6061, Loss: 0.4005048796534538, Final Batch Loss: 0.17936848104000092\n",
      "Epoch 6062, Loss: 0.3811366334557533, Final Batch Loss: 0.09026256948709488\n",
      "Epoch 6063, Loss: 0.3025323525071144, Final Batch Loss: 0.10729830712080002\n",
      "Epoch 6064, Loss: 0.405692495405674, Final Batch Loss: 0.13481391966342926\n",
      "Epoch 6065, Loss: 0.44701941311359406, Final Batch Loss: 0.12778472900390625\n",
      "Epoch 6066, Loss: 0.4101921617984772, Final Batch Loss: 0.11578412353992462\n",
      "Epoch 6067, Loss: 0.4457579255104065, Final Batch Loss: 0.1606648862361908\n",
      "Epoch 6068, Loss: 0.40640365332365036, Final Batch Loss: 0.10198421031236649\n",
      "Epoch 6069, Loss: 0.43456365168094635, Final Batch Loss: 0.15083283185958862\n",
      "Epoch 6070, Loss: 0.4709572345018387, Final Batch Loss: 0.16568751633167267\n",
      "Epoch 6071, Loss: 0.3850589394569397, Final Batch Loss: 0.08580896258354187\n",
      "Epoch 6072, Loss: 0.2555922791361809, Final Batch Loss: 0.06405523419380188\n",
      "Epoch 6073, Loss: 0.43268317729234695, Final Batch Loss: 0.10706979781389236\n",
      "Epoch 6074, Loss: 0.38289399445056915, Final Batch Loss: 0.08651383221149445\n",
      "Epoch 6075, Loss: 0.27036071568727493, Final Batch Loss: 0.05823440104722977\n",
      "Epoch 6076, Loss: 0.38318242132663727, Final Batch Loss: 0.1808658093214035\n",
      "Epoch 6077, Loss: 0.333477508276701, Final Batch Loss: 0.05669763311743736\n",
      "Epoch 6078, Loss: 0.3784470446407795, Final Batch Loss: 0.20523427426815033\n",
      "Epoch 6079, Loss: 0.34195079654455185, Final Batch Loss: 0.12951956689357758\n",
      "Epoch 6080, Loss: 0.38306643068790436, Final Batch Loss: 0.049369096755981445\n",
      "Epoch 6081, Loss: 0.22100204229354858, Final Batch Loss: 0.0644131749868393\n",
      "Epoch 6082, Loss: 0.35163359344005585, Final Batch Loss: 0.10768312215805054\n",
      "Epoch 6083, Loss: 0.44678977131843567, Final Batch Loss: 0.1900695264339447\n",
      "Epoch 6084, Loss: 0.3582114949822426, Final Batch Loss: 0.1264474093914032\n",
      "Epoch 6085, Loss: 0.3838265389204025, Final Batch Loss: 0.12780390679836273\n",
      "Epoch 6086, Loss: 0.2950301393866539, Final Batch Loss: 0.09473175555467606\n",
      "Epoch 6087, Loss: 0.3922140449285507, Final Batch Loss: 0.1771453619003296\n",
      "Epoch 6088, Loss: 0.28166482225060463, Final Batch Loss: 0.05107099190354347\n",
      "Epoch 6089, Loss: 0.315982460975647, Final Batch Loss: 0.11834482848644257\n",
      "Epoch 6090, Loss: 0.29005392640829086, Final Batch Loss: 0.07438165694475174\n",
      "Epoch 6091, Loss: 0.36953385174274445, Final Batch Loss: 0.1069733202457428\n",
      "Epoch 6092, Loss: 0.3123563826084137, Final Batch Loss: 0.07482167333364487\n",
      "Epoch 6093, Loss: 0.2595553919672966, Final Batch Loss: 0.08999697118997574\n",
      "Epoch 6094, Loss: 0.2937837913632393, Final Batch Loss: 0.12129886448383331\n",
      "Epoch 6095, Loss: 0.30954138934612274, Final Batch Loss: 0.10186384618282318\n",
      "Epoch 6096, Loss: 0.4153238609433174, Final Batch Loss: 0.2094198614358902\n",
      "Epoch 6097, Loss: 0.4083491414785385, Final Batch Loss: 0.11187393963336945\n",
      "Epoch 6098, Loss: 0.4009343236684799, Final Batch Loss: 0.12627719342708588\n",
      "Epoch 6099, Loss: 0.30701083689928055, Final Batch Loss: 0.061570629477500916\n",
      "Epoch 6100, Loss: 0.46142254769802094, Final Batch Loss: 0.1251446157693863\n",
      "Epoch 6101, Loss: 0.35113487392663956, Final Batch Loss: 0.09805996716022491\n",
      "Epoch 6102, Loss: 0.3019229918718338, Final Batch Loss: 0.12333841621875763\n",
      "Epoch 6103, Loss: 0.28493507578969, Final Batch Loss: 0.05662881210446358\n",
      "Epoch 6104, Loss: 0.3676573187112808, Final Batch Loss: 0.1476607769727707\n",
      "Epoch 6105, Loss: 0.31594469398260117, Final Batch Loss: 0.08255201578140259\n",
      "Epoch 6106, Loss: 0.37763192504644394, Final Batch Loss: 0.12264204025268555\n",
      "Epoch 6107, Loss: 0.31577325612306595, Final Batch Loss: 0.06309543550014496\n",
      "Epoch 6108, Loss: 0.6217043697834015, Final Batch Loss: 0.3411312997341156\n",
      "Epoch 6109, Loss: 0.37118448317050934, Final Batch Loss: 0.11789478361606598\n",
      "Epoch 6110, Loss: 0.29805082082748413, Final Batch Loss: 0.12422127276659012\n",
      "Epoch 6111, Loss: 0.29919832199811935, Final Batch Loss: 0.09350670129060745\n",
      "Epoch 6112, Loss: 0.4014994278550148, Final Batch Loss: 0.12938165664672852\n",
      "Epoch 6113, Loss: 0.3903719410300255, Final Batch Loss: 0.19494736194610596\n",
      "Epoch 6114, Loss: 0.2909177392721176, Final Batch Loss: 0.07856535166501999\n",
      "Epoch 6115, Loss: 0.3311180844902992, Final Batch Loss: 0.11693072319030762\n",
      "Epoch 6116, Loss: 0.254791222512722, Final Batch Loss: 0.05769895017147064\n",
      "Epoch 6117, Loss: 0.26686494797468185, Final Batch Loss: 0.09723043441772461\n",
      "Epoch 6118, Loss: 0.3691176027059555, Final Batch Loss: 0.13049925863742828\n",
      "Epoch 6119, Loss: 0.29986750334501266, Final Batch Loss: 0.1001758947968483\n",
      "Epoch 6120, Loss: 0.31537695974111557, Final Batch Loss: 0.11627309769392014\n",
      "Epoch 6121, Loss: 0.3533577471971512, Final Batch Loss: 0.16053061187267303\n",
      "Epoch 6122, Loss: 0.37777286767959595, Final Batch Loss: 0.11874202638864517\n",
      "Epoch 6123, Loss: 0.3150579631328583, Final Batch Loss: 0.1051560714840889\n",
      "Epoch 6124, Loss: 0.3025271147489548, Final Batch Loss: 0.10043254494667053\n",
      "Epoch 6125, Loss: 0.2775195315480232, Final Batch Loss: 0.0921027734875679\n",
      "Epoch 6126, Loss: 0.28791358321905136, Final Batch Loss: 0.06127883493900299\n",
      "Epoch 6127, Loss: 0.3412340357899666, Final Batch Loss: 0.11227722465991974\n",
      "Epoch 6128, Loss: 0.3443497121334076, Final Batch Loss: 0.12983378767967224\n",
      "Epoch 6129, Loss: 0.3862045183777809, Final Batch Loss: 0.11266693472862244\n",
      "Epoch 6130, Loss: 0.362426333129406, Final Batch Loss: 0.11666765064001083\n",
      "Epoch 6131, Loss: 0.22641333937644958, Final Batch Loss: 0.05279887467622757\n",
      "Epoch 6132, Loss: 0.26723136007785797, Final Batch Loss: 0.07289119064807892\n",
      "Epoch 6133, Loss: 0.3020973205566406, Final Batch Loss: 0.06726479530334473\n",
      "Epoch 6134, Loss: 0.32912562042474747, Final Batch Loss: 0.1419760137796402\n",
      "Epoch 6135, Loss: 0.3188157379627228, Final Batch Loss: 0.13494224846363068\n",
      "Epoch 6136, Loss: 0.3131312131881714, Final Batch Loss: 0.08467313647270203\n",
      "Epoch 6137, Loss: 0.2786680534482002, Final Batch Loss: 0.0705920159816742\n",
      "Epoch 6138, Loss: 0.3244512975215912, Final Batch Loss: 0.12576541304588318\n",
      "Epoch 6139, Loss: 0.2680234909057617, Final Batch Loss: 0.07614542543888092\n",
      "Epoch 6140, Loss: 0.306624099612236, Final Batch Loss: 0.07830079644918442\n",
      "Epoch 6141, Loss: 0.30338937044143677, Final Batch Loss: 0.06802037358283997\n",
      "Epoch 6142, Loss: 0.35956843942403793, Final Batch Loss: 0.09853331744670868\n",
      "Epoch 6143, Loss: 0.3369905427098274, Final Batch Loss: 0.11007975786924362\n",
      "Epoch 6144, Loss: 0.34638671576976776, Final Batch Loss: 0.11900122463703156\n",
      "Epoch 6145, Loss: 0.37320592254400253, Final Batch Loss: 0.14433029294013977\n",
      "Epoch 6146, Loss: 0.34355906397104263, Final Batch Loss: 0.11590851098299026\n",
      "Epoch 6147, Loss: 0.3598291054368019, Final Batch Loss: 0.17141443490982056\n",
      "Epoch 6148, Loss: 0.26911985129117966, Final Batch Loss: 0.07099249213933945\n",
      "Epoch 6149, Loss: 0.33800412714481354, Final Batch Loss: 0.14289800822734833\n",
      "Epoch 6150, Loss: 0.32448792457580566, Final Batch Loss: 0.14810210466384888\n",
      "Epoch 6151, Loss: 0.46331849694252014, Final Batch Loss: 0.11972285807132721\n",
      "Epoch 6152, Loss: 0.3421546518802643, Final Batch Loss: 0.11109011620283127\n",
      "Epoch 6153, Loss: 0.33029478788375854, Final Batch Loss: 0.11287178099155426\n",
      "Epoch 6154, Loss: 0.3201238289475441, Final Batch Loss: 0.11425571888685226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6155, Loss: 0.3417174369096756, Final Batch Loss: 0.10415958613157272\n",
      "Epoch 6156, Loss: 0.3208260089159012, Final Batch Loss: 0.10530658811330795\n",
      "Epoch 6157, Loss: 0.3337511420249939, Final Batch Loss: 0.08530178666114807\n",
      "Epoch 6158, Loss: 0.3128323033452034, Final Batch Loss: 0.11993520706892014\n",
      "Epoch 6159, Loss: 0.2851327061653137, Final Batch Loss: 0.08869600296020508\n",
      "Epoch 6160, Loss: 0.32155485451221466, Final Batch Loss: 0.12697485089302063\n",
      "Epoch 6161, Loss: 0.3135208785533905, Final Batch Loss: 0.13615532219409943\n",
      "Epoch 6162, Loss: 0.38058044761419296, Final Batch Loss: 0.13690902292728424\n",
      "Epoch 6163, Loss: 0.3325100764632225, Final Batch Loss: 0.143737331032753\n",
      "Epoch 6164, Loss: 0.3203202709555626, Final Batch Loss: 0.06780402362346649\n",
      "Epoch 6165, Loss: 0.3546391725540161, Final Batch Loss: 0.13143134117126465\n",
      "Epoch 6166, Loss: 0.37832261621952057, Final Batch Loss: 0.18514199554920197\n",
      "Epoch 6167, Loss: 0.30060290545225143, Final Batch Loss: 0.09987130016088486\n",
      "Epoch 6168, Loss: 0.22548244893550873, Final Batch Loss: 0.06366756558418274\n",
      "Epoch 6169, Loss: 0.32658617943525314, Final Batch Loss: 0.07541466504335403\n",
      "Epoch 6170, Loss: 0.3688516765832901, Final Batch Loss: 0.10701310634613037\n",
      "Epoch 6171, Loss: 0.25704865902662277, Final Batch Loss: 0.0714690238237381\n",
      "Epoch 6172, Loss: 0.36167415231466293, Final Batch Loss: 0.11108390241861343\n",
      "Epoch 6173, Loss: 0.43206746876239777, Final Batch Loss: 0.13939650356769562\n",
      "Epoch 6174, Loss: 0.2639561891555786, Final Batch Loss: 0.08322174847126007\n",
      "Epoch 6175, Loss: 0.30241283029317856, Final Batch Loss: 0.06393978744745255\n",
      "Epoch 6176, Loss: 0.246421467512846, Final Batch Loss: 0.08236947655677795\n",
      "Epoch 6177, Loss: 0.31310804188251495, Final Batch Loss: 0.08042972534894943\n",
      "Epoch 6178, Loss: 0.4067259505391121, Final Batch Loss: 0.08279048651456833\n",
      "Epoch 6179, Loss: 0.2816810607910156, Final Batch Loss: 0.0693652406334877\n",
      "Epoch 6180, Loss: 0.26851023733615875, Final Batch Loss: 0.037131890654563904\n",
      "Epoch 6181, Loss: 0.2712221257388592, Final Batch Loss: 0.08256504684686661\n",
      "Epoch 6182, Loss: 0.37585216760635376, Final Batch Loss: 0.09837166965007782\n",
      "Epoch 6183, Loss: 0.33574600517749786, Final Batch Loss: 0.08428110927343369\n",
      "Epoch 6184, Loss: 0.3683645948767662, Final Batch Loss: 0.18141596019268036\n",
      "Epoch 6185, Loss: 0.38304055482149124, Final Batch Loss: 0.15667977929115295\n",
      "Epoch 6186, Loss: 0.4277612268924713, Final Batch Loss: 0.16733074188232422\n",
      "Epoch 6187, Loss: 0.5054321810603142, Final Batch Loss: 0.24387438595294952\n",
      "Epoch 6188, Loss: 0.33577659726142883, Final Batch Loss: 0.09318336844444275\n",
      "Epoch 6189, Loss: 0.4605691209435463, Final Batch Loss: 0.19026100635528564\n",
      "Epoch 6190, Loss: 0.3925464302301407, Final Batch Loss: 0.17490795254707336\n",
      "Epoch 6191, Loss: 0.42015035450458527, Final Batch Loss: 0.09516221284866333\n",
      "Epoch 6192, Loss: 0.4261666014790535, Final Batch Loss: 0.23665174841880798\n",
      "Epoch 6193, Loss: 0.3942042663693428, Final Batch Loss: 0.17292428016662598\n",
      "Epoch 6194, Loss: 0.30058693140745163, Final Batch Loss: 0.08610763400793076\n",
      "Epoch 6195, Loss: 0.3891003802418709, Final Batch Loss: 0.1342582255601883\n",
      "Epoch 6196, Loss: 0.30002087354660034, Final Batch Loss: 0.07026246190071106\n",
      "Epoch 6197, Loss: 0.4333264157176018, Final Batch Loss: 0.17421403527259827\n",
      "Epoch 6198, Loss: 0.3378562033176422, Final Batch Loss: 0.06989113986492157\n",
      "Epoch 6199, Loss: 0.46522845327854156, Final Batch Loss: 0.17632009088993073\n",
      "Epoch 6200, Loss: 0.34706050902605057, Final Batch Loss: 0.0851130560040474\n",
      "Epoch 6201, Loss: 0.3302777037024498, Final Batch Loss: 0.09146416187286377\n",
      "Epoch 6202, Loss: 0.3236343041062355, Final Batch Loss: 0.11082480847835541\n",
      "Epoch 6203, Loss: 0.3869641609489918, Final Batch Loss: 0.059049639850854874\n",
      "Epoch 6204, Loss: 0.3752186894416809, Final Batch Loss: 0.14302434027194977\n",
      "Epoch 6205, Loss: 0.3721003159880638, Final Batch Loss: 0.09810137003660202\n",
      "Epoch 6206, Loss: 0.38231857120990753, Final Batch Loss: 0.05224846303462982\n",
      "Epoch 6207, Loss: 0.3441297113895416, Final Batch Loss: 0.1462395042181015\n",
      "Epoch 6208, Loss: 0.3424501046538353, Final Batch Loss: 0.1287810355424881\n",
      "Epoch 6209, Loss: 0.3354036808013916, Final Batch Loss: 0.09392853826284409\n",
      "Epoch 6210, Loss: 0.32119733840227127, Final Batch Loss: 0.07712621986865997\n",
      "Epoch 6211, Loss: 0.3963969796895981, Final Batch Loss: 0.14822441339492798\n",
      "Epoch 6212, Loss: 0.3319709226489067, Final Batch Loss: 0.11778222769498825\n",
      "Epoch 6213, Loss: 0.3349663093686104, Final Batch Loss: 0.10435432940721512\n",
      "Epoch 6214, Loss: 0.35592561960220337, Final Batch Loss: 0.12504823505878448\n",
      "Epoch 6215, Loss: 0.3836689442396164, Final Batch Loss: 0.12324609607458115\n",
      "Epoch 6216, Loss: 0.2811208665370941, Final Batch Loss: 0.0857074111700058\n",
      "Epoch 6217, Loss: 0.3739413097500801, Final Batch Loss: 0.13583824038505554\n",
      "Epoch 6218, Loss: 0.33381637930870056, Final Batch Loss: 0.10895706713199615\n",
      "Epoch 6219, Loss: 0.2752065286040306, Final Batch Loss: 0.11404313892126083\n",
      "Epoch 6220, Loss: 0.3903995752334595, Final Batch Loss: 0.19685572385787964\n",
      "Epoch 6221, Loss: 0.3103722706437111, Final Batch Loss: 0.08412616699934006\n",
      "Epoch 6222, Loss: 0.30961647629737854, Final Batch Loss: 0.078092560172081\n",
      "Epoch 6223, Loss: 0.2731430232524872, Final Batch Loss: 0.09081824868917465\n",
      "Epoch 6224, Loss: 0.4763833284378052, Final Batch Loss: 0.20029206573963165\n",
      "Epoch 6225, Loss: 0.31401339918375015, Final Batch Loss: 0.13239289820194244\n",
      "Epoch 6226, Loss: 0.3655138835310936, Final Batch Loss: 0.10301307588815689\n",
      "Epoch 6227, Loss: 0.33020877838134766, Final Batch Loss: 0.0998057872056961\n",
      "Epoch 6228, Loss: 0.3651702031493187, Final Batch Loss: 0.13677217066287994\n",
      "Epoch 6229, Loss: 0.38908716291189194, Final Batch Loss: 0.13256756961345673\n",
      "Epoch 6230, Loss: 0.2900495007634163, Final Batch Loss: 0.11259294301271439\n",
      "Epoch 6231, Loss: 0.39578188210725784, Final Batch Loss: 0.12115379422903061\n",
      "Epoch 6232, Loss: 0.27299147099256516, Final Batch Loss: 0.0910780057311058\n",
      "Epoch 6233, Loss: 0.2807111255824566, Final Batch Loss: 0.12711884081363678\n",
      "Epoch 6234, Loss: 0.33774483948946, Final Batch Loss: 0.14335283637046814\n",
      "Epoch 6235, Loss: 0.2504807710647583, Final Batch Loss: 0.07599719613790512\n",
      "Epoch 6236, Loss: 0.2666681334376335, Final Batch Loss: 0.07232290506362915\n",
      "Epoch 6237, Loss: 0.2681080028414726, Final Batch Loss: 0.08106081187725067\n",
      "Epoch 6238, Loss: 0.3535868898034096, Final Batch Loss: 0.19049875438213348\n",
      "Epoch 6239, Loss: 0.3822791874408722, Final Batch Loss: 0.1702938675880432\n",
      "Epoch 6240, Loss: 0.35517166554927826, Final Batch Loss: 0.09507331997156143\n",
      "Epoch 6241, Loss: 0.3292296677827835, Final Batch Loss: 0.10671974718570709\n",
      "Epoch 6242, Loss: 0.31990932673215866, Final Batch Loss: 0.11911714822053909\n",
      "Epoch 6243, Loss: 0.41710861027240753, Final Batch Loss: 0.1066150814294815\n",
      "Epoch 6244, Loss: 0.2768183834850788, Final Batch Loss: 0.11397483199834824\n",
      "Epoch 6245, Loss: 0.31783366948366165, Final Batch Loss: 0.12292774021625519\n",
      "Epoch 6246, Loss: 0.28611455485224724, Final Batch Loss: 0.05514944717288017\n",
      "Epoch 6247, Loss: 0.3490256816148758, Final Batch Loss: 0.08800987899303436\n",
      "Epoch 6248, Loss: 0.24412820860743523, Final Batch Loss: 0.058204907923936844\n",
      "Epoch 6249, Loss: 0.4121086746454239, Final Batch Loss: 0.15289553999900818\n",
      "Epoch 6250, Loss: 0.2645769789814949, Final Batch Loss: 0.02528677135705948\n",
      "Epoch 6251, Loss: 0.2702202871441841, Final Batch Loss: 0.0840965136885643\n",
      "Epoch 6252, Loss: 0.3491210713982582, Final Batch Loss: 0.127887561917305\n",
      "Epoch 6253, Loss: 0.23375652357935905, Final Batch Loss: 0.08807262778282166\n",
      "Epoch 6254, Loss: 0.24003274738788605, Final Batch Loss: 0.09055878967046738\n",
      "Epoch 6255, Loss: 0.27406587451696396, Final Batch Loss: 0.09064517915248871\n",
      "Epoch 6256, Loss: 0.3038749396800995, Final Batch Loss: 0.0886538028717041\n",
      "Epoch 6257, Loss: 0.38239171728491783, Final Batch Loss: 0.046620260924100876\n",
      "Epoch 6258, Loss: 0.41152356564998627, Final Batch Loss: 0.13257178664207458\n",
      "Epoch 6259, Loss: 0.3733183965086937, Final Batch Loss: 0.12599250674247742\n",
      "Epoch 6260, Loss: 0.29937857389450073, Final Batch Loss: 0.09934405237436295\n",
      "Epoch 6261, Loss: 0.3115905746817589, Final Batch Loss: 0.13898363709449768\n",
      "Epoch 6262, Loss: 0.3365609124302864, Final Batch Loss: 0.09703832119703293\n",
      "Epoch 6263, Loss: 0.3938230127096176, Final Batch Loss: 0.2102232128381729\n",
      "Epoch 6264, Loss: 0.2906148433685303, Final Batch Loss: 0.08484092354774475\n",
      "Epoch 6265, Loss: 0.44186773896217346, Final Batch Loss: 0.20215456187725067\n",
      "Epoch 6266, Loss: 0.3597627431154251, Final Batch Loss: 0.1243690624833107\n",
      "Epoch 6267, Loss: 0.3440828174352646, Final Batch Loss: 0.11165407299995422\n",
      "Epoch 6268, Loss: 0.33782990276813507, Final Batch Loss: 0.08681126683950424\n",
      "Epoch 6269, Loss: 0.3199281692504883, Final Batch Loss: 0.08504526317119598\n",
      "Epoch 6270, Loss: 0.3716629520058632, Final Batch Loss: 0.14588294923305511\n",
      "Epoch 6271, Loss: 0.3342432230710983, Final Batch Loss: 0.0929313525557518\n",
      "Epoch 6272, Loss: 0.398494154214859, Final Batch Loss: 0.24537642300128937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6273, Loss: 0.3209471181035042, Final Batch Loss: 0.09762726724147797\n",
      "Epoch 6274, Loss: 0.3555428981781006, Final Batch Loss: 0.16782481968402863\n",
      "Epoch 6275, Loss: 0.27803686261177063, Final Batch Loss: 0.07806899398565292\n",
      "Epoch 6276, Loss: 0.3776467368006706, Final Batch Loss: 0.10052596777677536\n",
      "Epoch 6277, Loss: 0.3230527341365814, Final Batch Loss: 0.1562928557395935\n",
      "Epoch 6278, Loss: 0.3704928159713745, Final Batch Loss: 0.16607758402824402\n",
      "Epoch 6279, Loss: 0.3373008891940117, Final Batch Loss: 0.07287599891424179\n",
      "Epoch 6280, Loss: 0.3838737830519676, Final Batch Loss: 0.08724439889192581\n",
      "Epoch 6281, Loss: 0.28051096200942993, Final Batch Loss: 0.08614531904459\n",
      "Epoch 6282, Loss: 0.32777317613363266, Final Batch Loss: 0.12754492461681366\n",
      "Epoch 6283, Loss: 0.38056106120347977, Final Batch Loss: 0.19210191071033478\n",
      "Epoch 6284, Loss: 0.31025467813014984, Final Batch Loss: 0.11598192900419235\n",
      "Epoch 6285, Loss: 0.3347567990422249, Final Batch Loss: 0.12642675638198853\n",
      "Epoch 6286, Loss: 0.2703463211655617, Final Batch Loss: 0.07331764698028564\n",
      "Epoch 6287, Loss: 0.3262053206562996, Final Batch Loss: 0.1425469070672989\n",
      "Epoch 6288, Loss: 0.3433602750301361, Final Batch Loss: 0.05804423987865448\n",
      "Epoch 6289, Loss: 0.3365500792860985, Final Batch Loss: 0.10075099766254425\n",
      "Epoch 6290, Loss: 0.2375260628759861, Final Batch Loss: 0.05704193934798241\n",
      "Epoch 6291, Loss: 0.4153534322977066, Final Batch Loss: 0.13244296610355377\n",
      "Epoch 6292, Loss: 0.281820822507143, Final Batch Loss: 0.12631118297576904\n",
      "Epoch 6293, Loss: 0.3036603480577469, Final Batch Loss: 0.1143580824136734\n",
      "Epoch 6294, Loss: 0.4426543265581131, Final Batch Loss: 0.1926305890083313\n",
      "Epoch 6295, Loss: 0.35085535794496536, Final Batch Loss: 0.14272020757198334\n",
      "Epoch 6296, Loss: 0.3689236119389534, Final Batch Loss: 0.14048682153224945\n",
      "Epoch 6297, Loss: 0.36320751905441284, Final Batch Loss: 0.11873093247413635\n",
      "Epoch 6298, Loss: 0.30634981393814087, Final Batch Loss: 0.08345330506563187\n",
      "Epoch 6299, Loss: 0.26656973361968994, Final Batch Loss: 0.07968048006296158\n",
      "Epoch 6300, Loss: 0.23317719250917435, Final Batch Loss: 0.06448084861040115\n",
      "Epoch 6301, Loss: 0.2715880870819092, Final Batch Loss: 0.11508267372846603\n",
      "Epoch 6302, Loss: 0.37180789560079575, Final Batch Loss: 0.10180819779634476\n",
      "Epoch 6303, Loss: 0.31457453966140747, Final Batch Loss: 0.09315025061368942\n",
      "Epoch 6304, Loss: 0.28833502531051636, Final Batch Loss: 0.09065646678209305\n",
      "Epoch 6305, Loss: 0.38601259142160416, Final Batch Loss: 0.11584078520536423\n",
      "Epoch 6306, Loss: 0.3443637415766716, Final Batch Loss: 0.0724269226193428\n",
      "Epoch 6307, Loss: 0.423676997423172, Final Batch Loss: 0.15695200860500336\n",
      "Epoch 6308, Loss: 0.4351511299610138, Final Batch Loss: 0.12124153226613998\n",
      "Epoch 6309, Loss: 0.3456675261259079, Final Batch Loss: 0.13534396886825562\n",
      "Epoch 6310, Loss: 0.4113478437066078, Final Batch Loss: 0.12333061546087265\n",
      "Epoch 6311, Loss: 0.4611382931470871, Final Batch Loss: 0.19273440539836884\n",
      "Epoch 6312, Loss: 0.3239515572786331, Final Batch Loss: 0.12722180783748627\n",
      "Epoch 6313, Loss: 0.409635454416275, Final Batch Loss: 0.17292295396327972\n",
      "Epoch 6314, Loss: 0.41432321071624756, Final Batch Loss: 0.1140609011054039\n",
      "Epoch 6315, Loss: 0.34365011006593704, Final Batch Loss: 0.09671103954315186\n",
      "Epoch 6316, Loss: 0.4329783543944359, Final Batch Loss: 0.18186146020889282\n",
      "Epoch 6317, Loss: 0.3651134744286537, Final Batch Loss: 0.13448745012283325\n",
      "Epoch 6318, Loss: 0.32169072329998016, Final Batch Loss: 0.11726726591587067\n",
      "Epoch 6319, Loss: 0.2843784913420677, Final Batch Loss: 0.09627196937799454\n",
      "Epoch 6320, Loss: 0.4951779767870903, Final Batch Loss: 0.11652054637670517\n",
      "Epoch 6321, Loss: 0.2754354104399681, Final Batch Loss: 0.10345737636089325\n",
      "Epoch 6322, Loss: 0.29221823439002037, Final Batch Loss: 0.04582927003502846\n",
      "Epoch 6323, Loss: 0.5464251488447189, Final Batch Loss: 0.2513194978237152\n",
      "Epoch 6324, Loss: 0.31212498992681503, Final Batch Loss: 0.08994722366333008\n",
      "Epoch 6325, Loss: 0.3607448488473892, Final Batch Loss: 0.16407078504562378\n",
      "Epoch 6326, Loss: 0.2907370924949646, Final Batch Loss: 0.07756136357784271\n",
      "Epoch 6327, Loss: 0.28393568843603134, Final Batch Loss: 0.08305640518665314\n",
      "Epoch 6328, Loss: 0.33264539390802383, Final Batch Loss: 0.09237413108348846\n",
      "Epoch 6329, Loss: 0.32563627511262894, Final Batch Loss: 0.12250682711601257\n",
      "Epoch 6330, Loss: 0.2940877228975296, Final Batch Loss: 0.1114949882030487\n",
      "Epoch 6331, Loss: 0.42087240517139435, Final Batch Loss: 0.19766412675380707\n",
      "Epoch 6332, Loss: 0.2967541515827179, Final Batch Loss: 0.08591226488351822\n",
      "Epoch 6333, Loss: 0.3930033668875694, Final Batch Loss: 0.1196976751089096\n",
      "Epoch 6334, Loss: 0.3422396779060364, Final Batch Loss: 0.1711411327123642\n",
      "Epoch 6335, Loss: 0.36243337392807007, Final Batch Loss: 0.1364147961139679\n",
      "Epoch 6336, Loss: 0.31469691544771194, Final Batch Loss: 0.10997604578733444\n",
      "Epoch 6337, Loss: 0.2472931668162346, Final Batch Loss: 0.0969931110739708\n",
      "Epoch 6338, Loss: 0.34327543526887894, Final Batch Loss: 0.09108999371528625\n",
      "Epoch 6339, Loss: 0.2878495529294014, Final Batch Loss: 0.07306623458862305\n",
      "Epoch 6340, Loss: 0.32070861756801605, Final Batch Loss: 0.1604963093996048\n",
      "Epoch 6341, Loss: 0.3661915361881256, Final Batch Loss: 0.13558191061019897\n",
      "Epoch 6342, Loss: 0.3347124382853508, Final Batch Loss: 0.12464474141597748\n",
      "Epoch 6343, Loss: 0.3404271677136421, Final Batch Loss: 0.11440697312355042\n",
      "Epoch 6344, Loss: 0.3131862208247185, Final Batch Loss: 0.08629690110683441\n",
      "Epoch 6345, Loss: 0.32429632544517517, Final Batch Loss: 0.12779749929904938\n",
      "Epoch 6346, Loss: 0.3757781758904457, Final Batch Loss: 0.13877266645431519\n",
      "Epoch 6347, Loss: 0.2870389521121979, Final Batch Loss: 0.09984225779771805\n",
      "Epoch 6348, Loss: 0.34705857187509537, Final Batch Loss: 0.10279842466115952\n",
      "Epoch 6349, Loss: 0.3010800704360008, Final Batch Loss: 0.13886107504367828\n",
      "Epoch 6350, Loss: 0.32129648327827454, Final Batch Loss: 0.12562258541584015\n",
      "Epoch 6351, Loss: 0.3948798030614853, Final Batch Loss: 0.1674799919128418\n",
      "Epoch 6352, Loss: 0.38700687885284424, Final Batch Loss: 0.12101862579584122\n",
      "Epoch 6353, Loss: 0.41655825078487396, Final Batch Loss: 0.13995163142681122\n",
      "Epoch 6354, Loss: 0.3771580308675766, Final Batch Loss: 0.12230559438467026\n",
      "Epoch 6355, Loss: 0.39543794840574265, Final Batch Loss: 0.11507420241832733\n",
      "Epoch 6356, Loss: 0.28701841086149216, Final Batch Loss: 0.09476330131292343\n",
      "Epoch 6357, Loss: 0.41877472400665283, Final Batch Loss: 0.11846590042114258\n",
      "Epoch 6358, Loss: 0.3965507373213768, Final Batch Loss: 0.13518422842025757\n",
      "Epoch 6359, Loss: 0.42259685695171356, Final Batch Loss: 0.11549850553274155\n",
      "Epoch 6360, Loss: 0.2835889980196953, Final Batch Loss: 0.06903229653835297\n",
      "Epoch 6361, Loss: 0.30122964829206467, Final Batch Loss: 0.07430901378393173\n",
      "Epoch 6362, Loss: 0.31710582226514816, Final Batch Loss: 0.11245094239711761\n",
      "Epoch 6363, Loss: 0.3418210297822952, Final Batch Loss: 0.09088055044412613\n",
      "Epoch 6364, Loss: 0.3922399505972862, Final Batch Loss: 0.1762179583311081\n",
      "Epoch 6365, Loss: 0.4073938876390457, Final Batch Loss: 0.11999139934778214\n",
      "Epoch 6366, Loss: 0.301373727619648, Final Batch Loss: 0.08007191121578217\n",
      "Epoch 6367, Loss: 0.39760902523994446, Final Batch Loss: 0.0993342399597168\n",
      "Epoch 6368, Loss: 0.3447848707437515, Final Batch Loss: 0.10362514108419418\n",
      "Epoch 6369, Loss: 0.3693646565079689, Final Batch Loss: 0.1207745298743248\n",
      "Epoch 6370, Loss: 0.29065898060798645, Final Batch Loss: 0.06682607531547546\n",
      "Epoch 6371, Loss: 0.34383024275302887, Final Batch Loss: 0.07660992443561554\n",
      "Epoch 6372, Loss: 0.46543705463409424, Final Batch Loss: 0.1193571388721466\n",
      "Epoch 6373, Loss: 0.3770083487033844, Final Batch Loss: 0.15611547231674194\n",
      "Epoch 6374, Loss: 0.29787150025367737, Final Batch Loss: 0.13482026755809784\n",
      "Epoch 6375, Loss: 0.35169927030801773, Final Batch Loss: 0.11411485821008682\n",
      "Epoch 6376, Loss: 0.35336947441101074, Final Batch Loss: 0.13332517445087433\n",
      "Epoch 6377, Loss: 0.3873857334256172, Final Batch Loss: 0.14797423779964447\n",
      "Epoch 6378, Loss: 0.35844536870718, Final Batch Loss: 0.1064457818865776\n",
      "Epoch 6379, Loss: 0.3602462112903595, Final Batch Loss: 0.09059056639671326\n",
      "Epoch 6380, Loss: 0.3980288431048393, Final Batch Loss: 0.19282694160938263\n",
      "Epoch 6381, Loss: 0.2951432764530182, Final Batch Loss: 0.06502918154001236\n",
      "Epoch 6382, Loss: 0.29346299916505814, Final Batch Loss: 0.09609746932983398\n",
      "Epoch 6383, Loss: 0.434523306787014, Final Batch Loss: 0.19427859783172607\n",
      "Epoch 6384, Loss: 0.32898328453302383, Final Batch Loss: 0.12067735195159912\n",
      "Epoch 6385, Loss: 0.26053984835743904, Final Batch Loss: 0.0707443356513977\n",
      "Epoch 6386, Loss: 0.33848242461681366, Final Batch Loss: 0.10900290310382843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6387, Loss: 0.3727834075689316, Final Batch Loss: 0.1698051393032074\n",
      "Epoch 6388, Loss: 0.2750825881958008, Final Batch Loss: 0.09309963881969452\n",
      "Epoch 6389, Loss: 0.3290179744362831, Final Batch Loss: 0.10418656468391418\n",
      "Epoch 6390, Loss: 0.31669236719608307, Final Batch Loss: 0.08974376320838928\n",
      "Epoch 6391, Loss: 0.22534262388944626, Final Batch Loss: 0.07601955533027649\n",
      "Epoch 6392, Loss: 0.31540174782276154, Final Batch Loss: 0.12533675134181976\n",
      "Epoch 6393, Loss: 0.41973165422677994, Final Batch Loss: 0.09205438941717148\n",
      "Epoch 6394, Loss: 0.342017225921154, Final Batch Loss: 0.05965445935726166\n",
      "Epoch 6395, Loss: 0.36686205491423607, Final Batch Loss: 0.13398391008377075\n",
      "Epoch 6396, Loss: 0.4978586733341217, Final Batch Loss: 0.2133190929889679\n",
      "Epoch 6397, Loss: 0.40187303721904755, Final Batch Loss: 0.09687343239784241\n",
      "Epoch 6398, Loss: 0.3914109244942665, Final Batch Loss: 0.15354517102241516\n",
      "Epoch 6399, Loss: 0.3742903396487236, Final Batch Loss: 0.17567181587219238\n",
      "Epoch 6400, Loss: 0.41461070626974106, Final Batch Loss: 0.202550008893013\n",
      "Epoch 6401, Loss: 0.37374403327703476, Final Batch Loss: 0.1460975855588913\n",
      "Epoch 6402, Loss: 0.28346914052963257, Final Batch Loss: 0.09751711040735245\n",
      "Epoch 6403, Loss: 0.2689885199069977, Final Batch Loss: 0.07497264444828033\n",
      "Epoch 6404, Loss: 0.35459818691015244, Final Batch Loss: 0.09803275763988495\n",
      "Epoch 6405, Loss: 0.20165855437517166, Final Batch Loss: 0.06018820405006409\n",
      "Epoch 6406, Loss: 0.3644094318151474, Final Batch Loss: 0.06156052649021149\n",
      "Epoch 6407, Loss: 0.39759135246276855, Final Batch Loss: 0.13143569231033325\n",
      "Epoch 6408, Loss: 0.2923450581729412, Final Batch Loss: 0.05905049666762352\n",
      "Epoch 6409, Loss: 0.3549837991595268, Final Batch Loss: 0.10726702958345413\n",
      "Epoch 6410, Loss: 0.30276570469141006, Final Batch Loss: 0.12476299703121185\n",
      "Epoch 6411, Loss: 0.3727768287062645, Final Batch Loss: 0.1180887296795845\n",
      "Epoch 6412, Loss: 0.299241840839386, Final Batch Loss: 0.06094963103532791\n",
      "Epoch 6413, Loss: 0.3415783643722534, Final Batch Loss: 0.09350146353244781\n",
      "Epoch 6414, Loss: 0.36780815571546555, Final Batch Loss: 0.1266489326953888\n",
      "Epoch 6415, Loss: 0.26208454743027687, Final Batch Loss: 0.12882083654403687\n",
      "Epoch 6416, Loss: 0.3005039691925049, Final Batch Loss: 0.08348460495471954\n",
      "Epoch 6417, Loss: 0.30066267400979996, Final Batch Loss: 0.1255355030298233\n",
      "Epoch 6418, Loss: 0.39668821543455124, Final Batch Loss: 0.1944437026977539\n",
      "Epoch 6419, Loss: 0.4731229767203331, Final Batch Loss: 0.14998114109039307\n",
      "Epoch 6420, Loss: 0.39077283442020416, Final Batch Loss: 0.13523654639720917\n",
      "Epoch 6421, Loss: 0.28368768841028214, Final Batch Loss: 0.08624418824911118\n",
      "Epoch 6422, Loss: 0.3674699068069458, Final Batch Loss: 0.10725938528776169\n",
      "Epoch 6423, Loss: 0.3206307366490364, Final Batch Loss: 0.08533959090709686\n",
      "Epoch 6424, Loss: 0.3055713623762131, Final Batch Loss: 0.058271653950214386\n",
      "Epoch 6425, Loss: 0.39541805535554886, Final Batch Loss: 0.14486664533615112\n",
      "Epoch 6426, Loss: 0.2457536831498146, Final Batch Loss: 0.0507698580622673\n",
      "Epoch 6427, Loss: 0.33775006234645844, Final Batch Loss: 0.12593452632427216\n",
      "Epoch 6428, Loss: 0.3488191142678261, Final Batch Loss: 0.10733824223279953\n",
      "Epoch 6429, Loss: 0.27461472153663635, Final Batch Loss: 0.050989918410778046\n",
      "Epoch 6430, Loss: 0.537532702088356, Final Batch Loss: 0.1371399462223053\n",
      "Epoch 6431, Loss: 0.3346656933426857, Final Batch Loss: 0.09168452024459839\n",
      "Epoch 6432, Loss: 0.316190242767334, Final Batch Loss: 0.11165641248226166\n",
      "Epoch 6433, Loss: 0.2971618175506592, Final Batch Loss: 0.11436538398265839\n",
      "Epoch 6434, Loss: 0.3646671026945114, Final Batch Loss: 0.11886294931173325\n",
      "Epoch 6435, Loss: 0.32295724749565125, Final Batch Loss: 0.09358786791563034\n",
      "Epoch 6436, Loss: 0.4094335436820984, Final Batch Loss: 0.1964816302061081\n",
      "Epoch 6437, Loss: 0.3224935978651047, Final Batch Loss: 0.10964801907539368\n",
      "Epoch 6438, Loss: 0.44223831593990326, Final Batch Loss: 0.18973858654499054\n",
      "Epoch 6439, Loss: 0.280480582267046, Final Batch Loss: 0.06131432577967644\n",
      "Epoch 6440, Loss: 0.403894878923893, Final Batch Loss: 0.13926063477993011\n",
      "Epoch 6441, Loss: 0.3073589392006397, Final Batch Loss: 0.11292402446269989\n",
      "Epoch 6442, Loss: 0.26903267949819565, Final Batch Loss: 0.08308979123830795\n",
      "Epoch 6443, Loss: 0.37124212086200714, Final Batch Loss: 0.1496029794216156\n",
      "Epoch 6444, Loss: 0.24973836541175842, Final Batch Loss: 0.04233091324567795\n",
      "Epoch 6445, Loss: 0.398842953145504, Final Batch Loss: 0.0972236767411232\n",
      "Epoch 6446, Loss: 0.26424603909254074, Final Batch Loss: 0.04281056672334671\n",
      "Epoch 6447, Loss: 0.2761002704501152, Final Batch Loss: 0.07707469165325165\n",
      "Epoch 6448, Loss: 0.28046051412820816, Final Batch Loss: 0.09743877500295639\n",
      "Epoch 6449, Loss: 0.3143158406019211, Final Batch Loss: 0.11294719576835632\n",
      "Epoch 6450, Loss: 0.2098093070089817, Final Batch Loss: 0.04635533317923546\n",
      "Epoch 6451, Loss: 0.26804500073194504, Final Batch Loss: 0.11260802298784256\n",
      "Epoch 6452, Loss: 0.29071666300296783, Final Batch Loss: 0.10785049200057983\n",
      "Epoch 6453, Loss: 0.3018069267272949, Final Batch Loss: 0.14712928235530853\n",
      "Epoch 6454, Loss: 0.41082997620105743, Final Batch Loss: 0.1396593600511551\n",
      "Epoch 6455, Loss: 0.3250854015350342, Final Batch Loss: 0.09036543220281601\n",
      "Epoch 6456, Loss: 0.2439679466187954, Final Batch Loss: 0.06228579208254814\n",
      "Epoch 6457, Loss: 0.41323501616716385, Final Batch Loss: 0.1804400235414505\n",
      "Epoch 6458, Loss: 0.3304625302553177, Final Batch Loss: 0.13571946322917938\n",
      "Epoch 6459, Loss: 0.36501434445381165, Final Batch Loss: 0.10981277376413345\n",
      "Epoch 6460, Loss: 0.3400304801762104, Final Batch Loss: 0.05585690215229988\n",
      "Epoch 6461, Loss: 0.36536961048841476, Final Batch Loss: 0.14337791502475739\n",
      "Epoch 6462, Loss: 0.3228994756937027, Final Batch Loss: 0.08193068951368332\n",
      "Epoch 6463, Loss: 0.3955404981970787, Final Batch Loss: 0.17377230525016785\n",
      "Epoch 6464, Loss: 0.26478834077715874, Final Batch Loss: 0.09633783251047134\n",
      "Epoch 6465, Loss: 0.3100309371948242, Final Batch Loss: 0.07321049273014069\n",
      "Epoch 6466, Loss: 0.3899204954504967, Final Batch Loss: 0.10932214558124542\n",
      "Epoch 6467, Loss: 0.38950344175100327, Final Batch Loss: 0.14794988930225372\n",
      "Epoch 6468, Loss: 0.31233348697423935, Final Batch Loss: 0.08223451673984528\n",
      "Epoch 6469, Loss: 0.2777973785996437, Final Batch Loss: 0.06894660741090775\n",
      "Epoch 6470, Loss: 0.5053861439228058, Final Batch Loss: 0.18758133053779602\n",
      "Epoch 6471, Loss: 0.3891923129558563, Final Batch Loss: 0.11089944839477539\n",
      "Epoch 6472, Loss: 0.3077601492404938, Final Batch Loss: 0.11476810276508331\n",
      "Epoch 6473, Loss: 0.2986164204776287, Final Batch Loss: 0.0995381698012352\n",
      "Epoch 6474, Loss: 0.3534107059240341, Final Batch Loss: 0.09903015196323395\n",
      "Epoch 6475, Loss: 0.34465567767620087, Final Batch Loss: 0.133669912815094\n",
      "Epoch 6476, Loss: 0.275294229388237, Final Batch Loss: 0.08727524429559708\n",
      "Epoch 6477, Loss: 0.3570217341184616, Final Batch Loss: 0.13131210207939148\n",
      "Epoch 6478, Loss: 0.3566029518842697, Final Batch Loss: 0.1251344382762909\n",
      "Epoch 6479, Loss: 0.39231038093566895, Final Batch Loss: 0.10289530456066132\n",
      "Epoch 6480, Loss: 0.2980048656463623, Final Batch Loss: 0.09682244807481766\n",
      "Epoch 6481, Loss: 0.4098941758275032, Final Batch Loss: 0.18593329191207886\n",
      "Epoch 6482, Loss: 0.32254521548748016, Final Batch Loss: 0.12899599969387054\n",
      "Epoch 6483, Loss: 0.3362678289413452, Final Batch Loss: 0.12203893065452576\n",
      "Epoch 6484, Loss: 0.3296377658843994, Final Batch Loss: 0.10998157411813736\n",
      "Epoch 6485, Loss: 0.3207218125462532, Final Batch Loss: 0.13195271790027618\n",
      "Epoch 6486, Loss: 0.4073245972394943, Final Batch Loss: 0.19907954335212708\n",
      "Epoch 6487, Loss: 0.39526253938674927, Final Batch Loss: 0.13643527030944824\n",
      "Epoch 6488, Loss: 0.3170670419931412, Final Batch Loss: 0.07728534191846848\n",
      "Epoch 6489, Loss: 0.29435861110687256, Final Batch Loss: 0.1309676170349121\n",
      "Epoch 6490, Loss: 0.26596878468990326, Final Batch Loss: 0.1113654300570488\n",
      "Epoch 6491, Loss: 0.3510468080639839, Final Batch Loss: 0.08900126069784164\n",
      "Epoch 6492, Loss: 0.35448070615530014, Final Batch Loss: 0.11787876486778259\n",
      "Epoch 6493, Loss: 0.37384068220853806, Final Batch Loss: 0.08517397195100784\n",
      "Epoch 6494, Loss: 0.47838766872882843, Final Batch Loss: 0.16293717920780182\n",
      "Epoch 6495, Loss: 0.3449588045477867, Final Batch Loss: 0.12510332465171814\n",
      "Epoch 6496, Loss: 0.29248183220624924, Final Batch Loss: 0.11470308899879456\n",
      "Epoch 6497, Loss: 0.2993793413043022, Final Batch Loss: 0.07843847572803497\n",
      "Epoch 6498, Loss: 0.3541795685887337, Final Batch Loss: 0.12317432463169098\n",
      "Epoch 6499, Loss: 0.27330025285482407, Final Batch Loss: 0.09977331012487411\n",
      "Epoch 6500, Loss: 0.27031852304935455, Final Batch Loss: 0.1069950982928276\n",
      "Epoch 6501, Loss: 0.2931315377354622, Final Batch Loss: 0.13395486772060394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6502, Loss: 0.33334530144929886, Final Batch Loss: 0.09997183084487915\n",
      "Epoch 6503, Loss: 0.26076459884643555, Final Batch Loss: 0.1167040765285492\n",
      "Epoch 6504, Loss: 0.34838220477104187, Final Batch Loss: 0.12913835048675537\n",
      "Epoch 6505, Loss: 0.29930294305086136, Final Batch Loss: 0.07552483677864075\n",
      "Epoch 6506, Loss: 0.2867606654763222, Final Batch Loss: 0.13903099298477173\n",
      "Epoch 6507, Loss: 0.31763870269060135, Final Batch Loss: 0.08721726387739182\n",
      "Epoch 6508, Loss: 0.27854688465595245, Final Batch Loss: 0.10974395275115967\n",
      "Epoch 6509, Loss: 0.47373166680336, Final Batch Loss: 0.17329910397529602\n",
      "Epoch 6510, Loss: 0.3214949071407318, Final Batch Loss: 0.14501617848873138\n",
      "Epoch 6511, Loss: 0.2660791501402855, Final Batch Loss: 0.09991009533405304\n",
      "Epoch 6512, Loss: 0.3546096049249172, Final Batch Loss: 0.052244748920202255\n",
      "Epoch 6513, Loss: 0.34652014821767807, Final Batch Loss: 0.08200284838676453\n",
      "Epoch 6514, Loss: 0.28716521710157394, Final Batch Loss: 0.09026750177145004\n",
      "Epoch 6515, Loss: 0.2638862356543541, Final Batch Loss: 0.08591596782207489\n",
      "Epoch 6516, Loss: 0.30244946479797363, Final Batch Loss: 0.0680825486779213\n",
      "Epoch 6517, Loss: 0.24218573048710823, Final Batch Loss: 0.1096063032746315\n",
      "Epoch 6518, Loss: 0.35650472342967987, Final Batch Loss: 0.10617892444133759\n",
      "Epoch 6519, Loss: 0.33453863859176636, Final Batch Loss: 0.1051829531788826\n",
      "Epoch 6520, Loss: 0.3028023913502693, Final Batch Loss: 0.09532006829977036\n",
      "Epoch 6521, Loss: 0.401328980922699, Final Batch Loss: 0.1151685044169426\n",
      "Epoch 6522, Loss: 0.4009251669049263, Final Batch Loss: 0.1730036437511444\n",
      "Epoch 6523, Loss: 0.33525189757347107, Final Batch Loss: 0.13448195159435272\n",
      "Epoch 6524, Loss: 0.2915641739964485, Final Batch Loss: 0.11365798860788345\n",
      "Epoch 6525, Loss: 0.3295065015554428, Final Batch Loss: 0.14224664866924286\n",
      "Epoch 6526, Loss: 0.5007780343294144, Final Batch Loss: 0.23754927515983582\n",
      "Epoch 6527, Loss: 0.32459793239831924, Final Batch Loss: 0.06715423613786697\n",
      "Epoch 6528, Loss: 0.3174003139138222, Final Batch Loss: 0.07354018092155457\n",
      "Epoch 6529, Loss: 0.34797751903533936, Final Batch Loss: 0.08858625590801239\n",
      "Epoch 6530, Loss: 0.44288044422864914, Final Batch Loss: 0.1496075987815857\n",
      "Epoch 6531, Loss: 0.26486750692129135, Final Batch Loss: 0.07002297788858414\n",
      "Epoch 6532, Loss: 0.2978902757167816, Final Batch Loss: 0.09266923367977142\n",
      "Epoch 6533, Loss: 0.3248496949672699, Final Batch Loss: 0.06713584810495377\n",
      "Epoch 6534, Loss: 0.2981407791376114, Final Batch Loss: 0.09893009066581726\n",
      "Epoch 6535, Loss: 0.2510177604854107, Final Batch Loss: 0.04168849065899849\n",
      "Epoch 6536, Loss: 0.2683204934000969, Final Batch Loss: 0.09794764965772629\n",
      "Epoch 6537, Loss: 0.3331902548670769, Final Batch Loss: 0.08683249354362488\n",
      "Epoch 6538, Loss: 0.3040515184402466, Final Batch Loss: 0.08501262962818146\n",
      "Epoch 6539, Loss: 0.3220515623688698, Final Batch Loss: 0.15706604719161987\n",
      "Epoch 6540, Loss: 0.29117192327976227, Final Batch Loss: 0.05246336758136749\n",
      "Epoch 6541, Loss: 0.3464813604950905, Final Batch Loss: 0.0954025536775589\n",
      "Epoch 6542, Loss: 0.2681374251842499, Final Batch Loss: 0.07762517035007477\n",
      "Epoch 6543, Loss: 0.4053001254796982, Final Batch Loss: 0.11762478947639465\n",
      "Epoch 6544, Loss: 0.32516729831695557, Final Batch Loss: 0.08606702089309692\n",
      "Epoch 6545, Loss: 0.3085906431078911, Final Batch Loss: 0.10108199715614319\n",
      "Epoch 6546, Loss: 0.403748519718647, Final Batch Loss: 0.14359533786773682\n",
      "Epoch 6547, Loss: 0.319366991519928, Final Batch Loss: 0.14409101009368896\n",
      "Epoch 6548, Loss: 0.3124101236462593, Final Batch Loss: 0.10816411674022675\n",
      "Epoch 6549, Loss: 0.28044455498456955, Final Batch Loss: 0.10389725863933563\n",
      "Epoch 6550, Loss: 0.3700397461652756, Final Batch Loss: 0.12914973497390747\n",
      "Epoch 6551, Loss: 0.3952467441558838, Final Batch Loss: 0.1750035583972931\n",
      "Epoch 6552, Loss: 0.2857983708381653, Final Batch Loss: 0.06078043580055237\n",
      "Epoch 6553, Loss: 0.2887791469693184, Final Batch Loss: 0.06668943166732788\n",
      "Epoch 6554, Loss: 0.3216252252459526, Final Batch Loss: 0.09176641702651978\n",
      "Epoch 6555, Loss: 0.40517501533031464, Final Batch Loss: 0.09564856439828873\n",
      "Epoch 6556, Loss: 0.24728235602378845, Final Batch Loss: 0.09400039166212082\n",
      "Epoch 6557, Loss: 0.3727959915995598, Final Batch Loss: 0.14757075905799866\n",
      "Epoch 6558, Loss: 0.2348608821630478, Final Batch Loss: 0.08883960545063019\n",
      "Epoch 6559, Loss: 0.29790206998586655, Final Batch Loss: 0.12920916080474854\n",
      "Epoch 6560, Loss: 0.33211446553468704, Final Batch Loss: 0.08341744542121887\n",
      "Epoch 6561, Loss: 0.2854134291410446, Final Batch Loss: 0.06463784724473953\n",
      "Epoch 6562, Loss: 0.2162911519408226, Final Batch Loss: 0.06978083401918411\n",
      "Epoch 6563, Loss: 0.32610632479190826, Final Batch Loss: 0.12680859863758087\n",
      "Epoch 6564, Loss: 0.36345864832401276, Final Batch Loss: 0.11151685565710068\n",
      "Epoch 6565, Loss: 0.35780762508511543, Final Batch Loss: 0.10125207155942917\n",
      "Epoch 6566, Loss: 0.3604850322008133, Final Batch Loss: 0.10096009075641632\n",
      "Epoch 6567, Loss: 0.2798365652561188, Final Batch Loss: 0.11413450539112091\n",
      "Epoch 6568, Loss: 0.34339746087789536, Final Batch Loss: 0.09112424403429031\n",
      "Epoch 6569, Loss: 0.38071632385253906, Final Batch Loss: 0.12105773389339447\n",
      "Epoch 6570, Loss: 0.3953687325119972, Final Batch Loss: 0.11089129745960236\n",
      "Epoch 6571, Loss: 0.32920224964618683, Final Batch Loss: 0.1141839325428009\n",
      "Epoch 6572, Loss: 0.26635121926665306, Final Batch Loss: 0.052030909806489944\n",
      "Epoch 6573, Loss: 0.32348962873220444, Final Batch Loss: 0.09708879888057709\n",
      "Epoch 6574, Loss: 0.42744939029216766, Final Batch Loss: 0.13251498341560364\n",
      "Epoch 6575, Loss: 0.3345096483826637, Final Batch Loss: 0.12439583986997604\n",
      "Epoch 6576, Loss: 0.31480424851179123, Final Batch Loss: 0.11403574049472809\n",
      "Epoch 6577, Loss: 0.27750319242477417, Final Batch Loss: 0.11474552005529404\n",
      "Epoch 6578, Loss: 0.3663286864757538, Final Batch Loss: 0.11763420701026917\n",
      "Epoch 6579, Loss: 0.2950833737850189, Final Batch Loss: 0.09422286599874496\n",
      "Epoch 6580, Loss: 0.3920129984617233, Final Batch Loss: 0.188942551612854\n",
      "Epoch 6581, Loss: 0.26817063614726067, Final Batch Loss: 0.05809398368000984\n",
      "Epoch 6582, Loss: 0.299503818154335, Final Batch Loss: 0.048130445182323456\n",
      "Epoch 6583, Loss: 0.39260973781347275, Final Batch Loss: 0.105169378221035\n",
      "Epoch 6584, Loss: 0.35692986100912094, Final Batch Loss: 0.15100346505641937\n",
      "Epoch 6585, Loss: 0.3510836735367775, Final Batch Loss: 0.15440160036087036\n",
      "Epoch 6586, Loss: 0.32836823910474777, Final Batch Loss: 0.10433391481637955\n",
      "Epoch 6587, Loss: 0.3448807820677757, Final Batch Loss: 0.13110631704330444\n",
      "Epoch 6588, Loss: 0.2564101628959179, Final Batch Loss: 0.05807613208889961\n",
      "Epoch 6589, Loss: 0.3006901443004608, Final Batch Loss: 0.08570166677236557\n",
      "Epoch 6590, Loss: 0.3645523265004158, Final Batch Loss: 0.12908531725406647\n",
      "Epoch 6591, Loss: 0.34167713671922684, Final Batch Loss: 0.06885795295238495\n",
      "Epoch 6592, Loss: 0.267727792263031, Final Batch Loss: 0.05474693328142166\n",
      "Epoch 6593, Loss: 0.2782861739397049, Final Batch Loss: 0.11544877290725708\n",
      "Epoch 6594, Loss: 0.27198269963264465, Final Batch Loss: 0.062144383788108826\n",
      "Epoch 6595, Loss: 0.34373825788497925, Final Batch Loss: 0.1458606868982315\n",
      "Epoch 6596, Loss: 0.23975329101085663, Final Batch Loss: 0.045815564692020416\n",
      "Epoch 6597, Loss: 0.39372511208057404, Final Batch Loss: 0.08755641430616379\n",
      "Epoch 6598, Loss: 0.3383776769042015, Final Batch Loss: 0.07770861685276031\n",
      "Epoch 6599, Loss: 0.3219630494713783, Final Batch Loss: 0.07157588750123978\n",
      "Epoch 6600, Loss: 0.2817752957344055, Final Batch Loss: 0.10155613720417023\n",
      "Epoch 6601, Loss: 0.30472488701343536, Final Batch Loss: 0.09344369918107986\n",
      "Epoch 6602, Loss: 0.24104372039437294, Final Batch Loss: 0.05734287574887276\n",
      "Epoch 6603, Loss: 0.29320409148931503, Final Batch Loss: 0.07908762246370316\n",
      "Epoch 6604, Loss: 0.27335742861032486, Final Batch Loss: 0.07781063765287399\n",
      "Epoch 6605, Loss: 0.3302813395857811, Final Batch Loss: 0.10291382670402527\n",
      "Epoch 6606, Loss: 0.37389884144067764, Final Batch Loss: 0.1326490342617035\n",
      "Epoch 6607, Loss: 0.2995535805821419, Final Batch Loss: 0.07101315259933472\n",
      "Epoch 6608, Loss: 0.3746565654873848, Final Batch Loss: 0.13709375262260437\n",
      "Epoch 6609, Loss: 0.2610826790332794, Final Batch Loss: 0.0473637580871582\n",
      "Epoch 6610, Loss: 0.21354606375098228, Final Batch Loss: 0.06989928334951401\n",
      "Epoch 6611, Loss: 0.309270441532135, Final Batch Loss: 0.09426916390657425\n",
      "Epoch 6612, Loss: 0.31004486232995987, Final Batch Loss: 0.10946720838546753\n",
      "Epoch 6613, Loss: 0.2820613980293274, Final Batch Loss: 0.09258130192756653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6614, Loss: 0.3625992387533188, Final Batch Loss: 0.1032533124089241\n",
      "Epoch 6615, Loss: 0.2879326567053795, Final Batch Loss: 0.0955517515540123\n",
      "Epoch 6616, Loss: 0.3067900612950325, Final Batch Loss: 0.0905877947807312\n",
      "Epoch 6617, Loss: 0.3144015148282051, Final Batch Loss: 0.10071748495101929\n",
      "Epoch 6618, Loss: 0.40108518302440643, Final Batch Loss: 0.12599915266036987\n",
      "Epoch 6619, Loss: 0.3810092657804489, Final Batch Loss: 0.14532430469989777\n",
      "Epoch 6620, Loss: 0.330509677529335, Final Batch Loss: 0.09576912224292755\n",
      "Epoch 6621, Loss: 0.2941563352942467, Final Batch Loss: 0.09568694978952408\n",
      "Epoch 6622, Loss: 0.33739228546619415, Final Batch Loss: 0.175197571516037\n",
      "Epoch 6623, Loss: 0.2642316669225693, Final Batch Loss: 0.06461525708436966\n",
      "Epoch 6624, Loss: 0.3117687553167343, Final Batch Loss: 0.06891908496618271\n",
      "Epoch 6625, Loss: 0.33624885976314545, Final Batch Loss: 0.08756609261035919\n",
      "Epoch 6626, Loss: 0.2973535284399986, Final Batch Loss: 0.10563548654317856\n",
      "Epoch 6627, Loss: 0.28273941576480865, Final Batch Loss: 0.07151495665311813\n",
      "Epoch 6628, Loss: 0.3506879210472107, Final Batch Loss: 0.12256138026714325\n",
      "Epoch 6629, Loss: 0.28738198429346085, Final Batch Loss: 0.07728967815637589\n",
      "Epoch 6630, Loss: 0.2556178942322731, Final Batch Loss: 0.08574315905570984\n",
      "Epoch 6631, Loss: 0.2773379981517792, Final Batch Loss: 0.12154841423034668\n",
      "Epoch 6632, Loss: 0.2014138363301754, Final Batch Loss: 0.07212819904088974\n",
      "Epoch 6633, Loss: 0.27889905869960785, Final Batch Loss: 0.06413997709751129\n",
      "Epoch 6634, Loss: 0.2399844527244568, Final Batch Loss: 0.05299745500087738\n",
      "Epoch 6635, Loss: 0.24153930321335793, Final Batch Loss: 0.04434628412127495\n",
      "Epoch 6636, Loss: 0.3127046823501587, Final Batch Loss: 0.09694225341081619\n",
      "Epoch 6637, Loss: 0.32831422984600067, Final Batch Loss: 0.09169778227806091\n",
      "Epoch 6638, Loss: 0.3679875284433365, Final Batch Loss: 0.14391008019447327\n",
      "Epoch 6639, Loss: 0.28666144609451294, Final Batch Loss: 0.11465411633253098\n",
      "Epoch 6640, Loss: 0.3349556252360344, Final Batch Loss: 0.10889597237110138\n",
      "Epoch 6641, Loss: 0.2997182533144951, Final Batch Loss: 0.10520890355110168\n",
      "Epoch 6642, Loss: 0.26008330285549164, Final Batch Loss: 0.095740906894207\n",
      "Epoch 6643, Loss: 0.3548746183514595, Final Batch Loss: 0.08591613918542862\n",
      "Epoch 6644, Loss: 0.39143332839012146, Final Batch Loss: 0.12428529560565948\n",
      "Epoch 6645, Loss: 0.29964546859264374, Final Batch Loss: 0.09839486330747604\n",
      "Epoch 6646, Loss: 0.2962292805314064, Final Batch Loss: 0.12624499201774597\n",
      "Epoch 6647, Loss: 0.4964854419231415, Final Batch Loss: 0.18691615760326385\n",
      "Epoch 6648, Loss: 0.3167785182595253, Final Batch Loss: 0.10584332793951035\n",
      "Epoch 6649, Loss: 0.37829364091157913, Final Batch Loss: 0.09818435460329056\n",
      "Epoch 6650, Loss: 0.35150690376758575, Final Batch Loss: 0.1228782907128334\n",
      "Epoch 6651, Loss: 0.268353633582592, Final Batch Loss: 0.10322488844394684\n",
      "Epoch 6652, Loss: 0.3954354599118233, Final Batch Loss: 0.16468973457813263\n",
      "Epoch 6653, Loss: 0.3670404255390167, Final Batch Loss: 0.08921933174133301\n",
      "Epoch 6654, Loss: 0.40747953951358795, Final Batch Loss: 0.14897480607032776\n",
      "Epoch 6655, Loss: 0.38802681118249893, Final Batch Loss: 0.16254237294197083\n",
      "Epoch 6656, Loss: 0.42018453776836395, Final Batch Loss: 0.18648377060890198\n",
      "Epoch 6657, Loss: 0.40840277820825577, Final Batch Loss: 0.11205402761697769\n",
      "Epoch 6658, Loss: 0.3379877954721451, Final Batch Loss: 0.07617753744125366\n",
      "Epoch 6659, Loss: 0.23175926133990288, Final Batch Loss: 0.08036481589078903\n",
      "Epoch 6660, Loss: 0.3543098047375679, Final Batch Loss: 0.10992211848497391\n",
      "Epoch 6661, Loss: 0.25785770267248154, Final Batch Loss: 0.06342849135398865\n",
      "Epoch 6662, Loss: 0.2704349756240845, Final Batch Loss: 0.07522562891244888\n",
      "Epoch 6663, Loss: 0.38928353041410446, Final Batch Loss: 0.15680690109729767\n",
      "Epoch 6664, Loss: 0.3119466304779053, Final Batch Loss: 0.11303556710481644\n",
      "Epoch 6665, Loss: 0.38097723573446274, Final Batch Loss: 0.08294153958559036\n",
      "Epoch 6666, Loss: 0.28163668513298035, Final Batch Loss: 0.06714851409196854\n",
      "Epoch 6667, Loss: 0.3108482137322426, Final Batch Loss: 0.13034655153751373\n",
      "Epoch 6668, Loss: 0.28742992132902145, Final Batch Loss: 0.07920993864536285\n",
      "Epoch 6669, Loss: 0.26363158226013184, Final Batch Loss: 0.06562542915344238\n",
      "Epoch 6670, Loss: 0.28561369329690933, Final Batch Loss: 0.12199229747056961\n",
      "Epoch 6671, Loss: 0.18977566435933113, Final Batch Loss: 0.061592891812324524\n",
      "Epoch 6672, Loss: 0.2637842297554016, Final Batch Loss: 0.08925358206033707\n",
      "Epoch 6673, Loss: 0.3141435235738754, Final Batch Loss: 0.12738898396492004\n",
      "Epoch 6674, Loss: 0.3068701848387718, Final Batch Loss: 0.13260476291179657\n",
      "Epoch 6675, Loss: 0.2851578891277313, Final Batch Loss: 0.10287263989448547\n",
      "Epoch 6676, Loss: 0.3259245455265045, Final Batch Loss: 0.11853993684053421\n",
      "Epoch 6677, Loss: 0.32195233553647995, Final Batch Loss: 0.16444988548755646\n",
      "Epoch 6678, Loss: 0.3326575234532356, Final Batch Loss: 0.13216742873191833\n",
      "Epoch 6679, Loss: 0.2611504942178726, Final Batch Loss: 0.06533659249544144\n",
      "Epoch 6680, Loss: 0.31109965592622757, Final Batch Loss: 0.06288626044988632\n",
      "Epoch 6681, Loss: 0.45089149475097656, Final Batch Loss: 0.16889195144176483\n",
      "Epoch 6682, Loss: 0.27738434076309204, Final Batch Loss: 0.11257452517747879\n",
      "Epoch 6683, Loss: 0.21680761128664017, Final Batch Loss: 0.07094904780387878\n",
      "Epoch 6684, Loss: 0.3178483843803406, Final Batch Loss: 0.08211866766214371\n",
      "Epoch 6685, Loss: 0.34605564549565315, Final Batch Loss: 0.060277435928583145\n",
      "Epoch 6686, Loss: 0.34254544228315353, Final Batch Loss: 0.08056779950857162\n",
      "Epoch 6687, Loss: 0.3488811254501343, Final Batch Loss: 0.18135397136211395\n",
      "Epoch 6688, Loss: 0.31851569563150406, Final Batch Loss: 0.09128139913082123\n",
      "Epoch 6689, Loss: 0.3289176821708679, Final Batch Loss: 0.17801062762737274\n",
      "Epoch 6690, Loss: 0.35167986154556274, Final Batch Loss: 0.09134384989738464\n",
      "Epoch 6691, Loss: 0.38916730135679245, Final Batch Loss: 0.1683470904827118\n",
      "Epoch 6692, Loss: 0.24692324176430702, Final Batch Loss: 0.11631770431995392\n",
      "Epoch 6693, Loss: 0.37379171699285507, Final Batch Loss: 0.1442132443189621\n",
      "Epoch 6694, Loss: 0.38529597967863083, Final Batch Loss: 0.11261429637670517\n",
      "Epoch 6695, Loss: 0.37999075651168823, Final Batch Loss: 0.15104839205741882\n",
      "Epoch 6696, Loss: 0.3992563933134079, Final Batch Loss: 0.15456420183181763\n",
      "Epoch 6697, Loss: 0.41920555382966995, Final Batch Loss: 0.19102557003498077\n",
      "Epoch 6698, Loss: 0.4315449967980385, Final Batch Loss: 0.18508380651474\n",
      "Epoch 6699, Loss: 0.3446923941373825, Final Batch Loss: 0.08521314710378647\n",
      "Epoch 6700, Loss: 0.2784564793109894, Final Batch Loss: 0.09143372625112534\n",
      "Epoch 6701, Loss: 0.3179662302136421, Final Batch Loss: 0.13535621762275696\n",
      "Epoch 6702, Loss: 0.31932880729436874, Final Batch Loss: 0.11398869752883911\n",
      "Epoch 6703, Loss: 0.31951168179512024, Final Batch Loss: 0.11809436976909637\n",
      "Epoch 6704, Loss: 0.3421959728002548, Final Batch Loss: 0.1147325187921524\n",
      "Epoch 6705, Loss: 0.2968211844563484, Final Batch Loss: 0.10737665742635727\n",
      "Epoch 6706, Loss: 0.3065996989607811, Final Batch Loss: 0.08599811792373657\n",
      "Epoch 6707, Loss: 0.2736240550875664, Final Batch Loss: 0.0699295625090599\n",
      "Epoch 6708, Loss: 0.3560742288827896, Final Batch Loss: 0.07766593992710114\n",
      "Epoch 6709, Loss: 0.2669112756848335, Final Batch Loss: 0.08134191483259201\n",
      "Epoch 6710, Loss: 0.27473529428243637, Final Batch Loss: 0.07363584637641907\n",
      "Epoch 6711, Loss: 0.3749447464942932, Final Batch Loss: 0.08808290958404541\n",
      "Epoch 6712, Loss: 0.37495002150535583, Final Batch Loss: 0.08077678829431534\n",
      "Epoch 6713, Loss: 0.31514541059732437, Final Batch Loss: 0.11175353080034256\n",
      "Epoch 6714, Loss: 0.32495758682489395, Final Batch Loss: 0.17491331696510315\n",
      "Epoch 6715, Loss: 0.30530498921871185, Final Batch Loss: 0.10415733605623245\n",
      "Epoch 6716, Loss: 0.3528863340616226, Final Batch Loss: 0.16764391958713531\n",
      "Epoch 6717, Loss: 0.28621263802051544, Final Batch Loss: 0.06852589547634125\n",
      "Epoch 6718, Loss: 0.325860857963562, Final Batch Loss: 0.07679623365402222\n",
      "Epoch 6719, Loss: 0.33349891006946564, Final Batch Loss: 0.13111895322799683\n",
      "Epoch 6720, Loss: 0.38101980835199356, Final Batch Loss: 0.13605840504169464\n",
      "Epoch 6721, Loss: 0.32102787494659424, Final Batch Loss: 0.06920518726110458\n",
      "Epoch 6722, Loss: 0.38271744549274445, Final Batch Loss: 0.06777611374855042\n",
      "Epoch 6723, Loss: 0.2951458916068077, Final Batch Loss: 0.10692290961742401\n",
      "Epoch 6724, Loss: 0.2851191908121109, Final Batch Loss: 0.08913611620664597\n",
      "Epoch 6725, Loss: 0.3132084608078003, Final Batch Loss: 0.06471485644578934\n",
      "Epoch 6726, Loss: 0.23921462893486023, Final Batch Loss: 0.07492558658123016\n",
      "Epoch 6727, Loss: 0.4309103563427925, Final Batch Loss: 0.18726162612438202\n",
      "Epoch 6728, Loss: 0.3270808979868889, Final Batch Loss: 0.08789069205522537\n",
      "Epoch 6729, Loss: 0.2988482862710953, Final Batch Loss: 0.12046215683221817\n",
      "Epoch 6730, Loss: 0.3036547675728798, Final Batch Loss: 0.10303980857133865\n",
      "Epoch 6731, Loss: 0.27180077880620956, Final Batch Loss: 0.06256303936243057\n",
      "Epoch 6732, Loss: 0.28071141988039017, Final Batch Loss: 0.09944263100624084\n",
      "Epoch 6733, Loss: 0.2891005873680115, Final Batch Loss: 0.05255987495183945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6734, Loss: 0.25390663743019104, Final Batch Loss: 0.07973142713308334\n",
      "Epoch 6735, Loss: 0.23861443996429443, Final Batch Loss: 0.056582145392894745\n",
      "Epoch 6736, Loss: 0.3637351840734482, Final Batch Loss: 0.15583296120166779\n",
      "Epoch 6737, Loss: 0.33220909535884857, Final Batch Loss: 0.1483774185180664\n",
      "Epoch 6738, Loss: 0.41498203575611115, Final Batch Loss: 0.15684743225574493\n",
      "Epoch 6739, Loss: 0.35626357048749924, Final Batch Loss: 0.1183406412601471\n",
      "Epoch 6740, Loss: 0.2764284089207649, Final Batch Loss: 0.09941655397415161\n",
      "Epoch 6741, Loss: 0.35745324939489365, Final Batch Loss: 0.11492312699556351\n",
      "Epoch 6742, Loss: 0.4589274823665619, Final Batch Loss: 0.18852707743644714\n",
      "Epoch 6743, Loss: 0.3508104383945465, Final Batch Loss: 0.11245313286781311\n",
      "Epoch 6744, Loss: 0.2662048190832138, Final Batch Loss: 0.08522601425647736\n",
      "Epoch 6745, Loss: 0.3333960771560669, Final Batch Loss: 0.1726013422012329\n",
      "Epoch 6746, Loss: 0.43304286897182465, Final Batch Loss: 0.1774197816848755\n",
      "Epoch 6747, Loss: 0.36544524878263474, Final Batch Loss: 0.12904298305511475\n",
      "Epoch 6748, Loss: 0.3865198791027069, Final Batch Loss: 0.11723067611455917\n",
      "Epoch 6749, Loss: 0.3415968641638756, Final Batch Loss: 0.12717457115650177\n",
      "Epoch 6750, Loss: 0.3566237539052963, Final Batch Loss: 0.15179023146629333\n",
      "Epoch 6751, Loss: 0.3429681584239006, Final Batch Loss: 0.12568290531635284\n",
      "Epoch 6752, Loss: 0.34915778040885925, Final Batch Loss: 0.14967654645442963\n",
      "Epoch 6753, Loss: 0.32750219106674194, Final Batch Loss: 0.09439809620380402\n",
      "Epoch 6754, Loss: 0.37411635369062424, Final Batch Loss: 0.07441999763250351\n",
      "Epoch 6755, Loss: 0.28941765427589417, Final Batch Loss: 0.035814836621284485\n",
      "Epoch 6756, Loss: 0.3508971780538559, Final Batch Loss: 0.1272699385881424\n",
      "Epoch 6757, Loss: 0.4258672893047333, Final Batch Loss: 0.17192046344280243\n",
      "Epoch 6758, Loss: 0.2194574922323227, Final Batch Loss: 0.042901165783405304\n",
      "Epoch 6759, Loss: 0.34556587785482407, Final Batch Loss: 0.08759726583957672\n",
      "Epoch 6760, Loss: 0.3420741707086563, Final Batch Loss: 0.10348057746887207\n",
      "Epoch 6761, Loss: 0.39462965726852417, Final Batch Loss: 0.08108536899089813\n",
      "Epoch 6762, Loss: 0.37347351759672165, Final Batch Loss: 0.11979539692401886\n",
      "Epoch 6763, Loss: 0.25574903562664986, Final Batch Loss: 0.05254187807440758\n",
      "Epoch 6764, Loss: 0.41383901983499527, Final Batch Loss: 0.20029540359973907\n",
      "Epoch 6765, Loss: 0.37863893806934357, Final Batch Loss: 0.09136487543582916\n",
      "Epoch 6766, Loss: 0.2815147265791893, Final Batch Loss: 0.08189159631729126\n",
      "Epoch 6767, Loss: 0.32441912591457367, Final Batch Loss: 0.10150330513715744\n",
      "Epoch 6768, Loss: 0.3119628429412842, Final Batch Loss: 0.09799542278051376\n",
      "Epoch 6769, Loss: 0.3180933892726898, Final Batch Loss: 0.08595160394906998\n",
      "Epoch 6770, Loss: 0.33942870795726776, Final Batch Loss: 0.09606014937162399\n",
      "Epoch 6771, Loss: 0.32244042307138443, Final Batch Loss: 0.1379700005054474\n",
      "Epoch 6772, Loss: 0.33289503306150436, Final Batch Loss: 0.13819155097007751\n",
      "Epoch 6773, Loss: 0.3911692872643471, Final Batch Loss: 0.07420559972524643\n",
      "Epoch 6774, Loss: 0.33327703922986984, Final Batch Loss: 0.12628014385700226\n",
      "Epoch 6775, Loss: 0.2637616842985153, Final Batch Loss: 0.11312050372362137\n",
      "Epoch 6776, Loss: 0.34279005974531174, Final Batch Loss: 0.14730437099933624\n",
      "Epoch 6777, Loss: 0.2672029957175255, Final Batch Loss: 0.06739062070846558\n",
      "Epoch 6778, Loss: 0.3019282668828964, Final Batch Loss: 0.13366910815238953\n",
      "Epoch 6779, Loss: 0.30453967303037643, Final Batch Loss: 0.09880246967077255\n",
      "Epoch 6780, Loss: 0.333940789103508, Final Batch Loss: 0.05724972486495972\n",
      "Epoch 6781, Loss: 0.42669373005628586, Final Batch Loss: 0.13133792579174042\n",
      "Epoch 6782, Loss: 0.3441453278064728, Final Batch Loss: 0.1476331204175949\n",
      "Epoch 6783, Loss: 0.3427567780017853, Final Batch Loss: 0.1276838332414627\n",
      "Epoch 6784, Loss: 0.23532037436962128, Final Batch Loss: 0.08226971328258514\n",
      "Epoch 6785, Loss: 0.33624186366796494, Final Batch Loss: 0.13945350050926208\n",
      "Epoch 6786, Loss: 0.2877235785126686, Final Batch Loss: 0.0542871430516243\n",
      "Epoch 6787, Loss: 0.3018227815628052, Final Batch Loss: 0.10567143559455872\n",
      "Epoch 6788, Loss: 0.28373754769563675, Final Batch Loss: 0.10608061403036118\n",
      "Epoch 6789, Loss: 0.37962114810943604, Final Batch Loss: 0.09327170252799988\n",
      "Epoch 6790, Loss: 0.32586077600717545, Final Batch Loss: 0.0754740908741951\n",
      "Epoch 6791, Loss: 0.27583610638976097, Final Batch Loss: 0.05678066983819008\n",
      "Epoch 6792, Loss: 0.26308999583125114, Final Batch Loss: 0.05499628558754921\n",
      "Epoch 6793, Loss: 0.36468029767274857, Final Batch Loss: 0.11661767214536667\n",
      "Epoch 6794, Loss: 0.24851227551698685, Final Batch Loss: 0.06710457801818848\n",
      "Epoch 6795, Loss: 0.2361632063984871, Final Batch Loss: 0.07027910649776459\n",
      "Epoch 6796, Loss: 0.23897280544042587, Final Batch Loss: 0.07301422208547592\n",
      "Epoch 6797, Loss: 0.30562297254800797, Final Batch Loss: 0.14612898230552673\n",
      "Epoch 6798, Loss: 0.3784348964691162, Final Batch Loss: 0.17101933062076569\n",
      "Epoch 6799, Loss: 0.3395061045885086, Final Batch Loss: 0.15948310494422913\n",
      "Epoch 6800, Loss: 0.3936096131801605, Final Batch Loss: 0.1955861896276474\n",
      "Epoch 6801, Loss: 0.40898608416318893, Final Batch Loss: 0.1425260603427887\n",
      "Epoch 6802, Loss: 0.33446404337882996, Final Batch Loss: 0.09744467586278915\n",
      "Epoch 6803, Loss: 0.30412499606609344, Final Batch Loss: 0.10657951980829239\n",
      "Epoch 6804, Loss: 0.3305015489459038, Final Batch Loss: 0.1289641559123993\n",
      "Epoch 6805, Loss: 0.2983100786805153, Final Batch Loss: 0.07134012877941132\n",
      "Epoch 6806, Loss: 0.3916112333536148, Final Batch Loss: 0.11948537826538086\n",
      "Epoch 6807, Loss: 0.30955031514167786, Final Batch Loss: 0.11839783191680908\n",
      "Epoch 6808, Loss: 0.2871043235063553, Final Batch Loss: 0.14827914535999298\n",
      "Epoch 6809, Loss: 0.3662979081273079, Final Batch Loss: 0.16057798266410828\n",
      "Epoch 6810, Loss: 0.3085391968488693, Final Batch Loss: 0.08168026804924011\n",
      "Epoch 6811, Loss: 0.38042790442705154, Final Batch Loss: 0.18030673265457153\n",
      "Epoch 6812, Loss: 0.408054381608963, Final Batch Loss: 0.14378949999809265\n",
      "Epoch 6813, Loss: 0.26843680441379547, Final Batch Loss: 0.07223901897668839\n",
      "Epoch 6814, Loss: 0.30894381552934647, Final Batch Loss: 0.10294415801763535\n",
      "Epoch 6815, Loss: 0.3504088968038559, Final Batch Loss: 0.14299839735031128\n",
      "Epoch 6816, Loss: 0.3124927207827568, Final Batch Loss: 0.11554744094610214\n",
      "Epoch 6817, Loss: 0.3133373185992241, Final Batch Loss: 0.08698053658008575\n",
      "Epoch 6818, Loss: 0.2986196503043175, Final Batch Loss: 0.13793908059597015\n",
      "Epoch 6819, Loss: 0.3775385841727257, Final Batch Loss: 0.18161852657794952\n",
      "Epoch 6820, Loss: 0.39733879268169403, Final Batch Loss: 0.15641342103481293\n",
      "Epoch 6821, Loss: 0.25844450294971466, Final Batch Loss: 0.10111682116985321\n",
      "Epoch 6822, Loss: 0.32979271933436394, Final Batch Loss: 0.10944660007953644\n",
      "Epoch 6823, Loss: 0.2950137034058571, Final Batch Loss: 0.08173815906047821\n",
      "Epoch 6824, Loss: 0.31471675634384155, Final Batch Loss: 0.1305282562971115\n",
      "Epoch 6825, Loss: 0.30652351677417755, Final Batch Loss: 0.1281309574842453\n",
      "Epoch 6826, Loss: 0.34239455312490463, Final Batch Loss: 0.1503078043460846\n",
      "Epoch 6827, Loss: 0.26757100224494934, Final Batch Loss: 0.07345099747180939\n",
      "Epoch 6828, Loss: 0.2996772527694702, Final Batch Loss: 0.1067936047911644\n",
      "Epoch 6829, Loss: 0.26451100409030914, Final Batch Loss: 0.09558470547199249\n",
      "Epoch 6830, Loss: 0.4793969690799713, Final Batch Loss: 0.14066295325756073\n",
      "Epoch 6831, Loss: 0.34798602759838104, Final Batch Loss: 0.13293857872486115\n",
      "Epoch 6832, Loss: 0.2885550856590271, Final Batch Loss: 0.07625284790992737\n",
      "Epoch 6833, Loss: 0.3190987557172775, Final Batch Loss: 0.0821940153837204\n",
      "Epoch 6834, Loss: 0.28271837532520294, Final Batch Loss: 0.07364577800035477\n",
      "Epoch 6835, Loss: 0.3418727219104767, Final Batch Loss: 0.11235399544239044\n",
      "Epoch 6836, Loss: 0.3064325749874115, Final Batch Loss: 0.12438751012086868\n",
      "Epoch 6837, Loss: 0.2846357598900795, Final Batch Loss: 0.07935275137424469\n",
      "Epoch 6838, Loss: 0.35212258249521255, Final Batch Loss: 0.0818144679069519\n",
      "Epoch 6839, Loss: 0.29669328033924103, Final Batch Loss: 0.10850007832050323\n",
      "Epoch 6840, Loss: 0.3931021988391876, Final Batch Loss: 0.12281623482704163\n",
      "Epoch 6841, Loss: 0.2879544645547867, Final Batch Loss: 0.06618210673332214\n",
      "Epoch 6842, Loss: 0.3420102447271347, Final Batch Loss: 0.09123329073190689\n",
      "Epoch 6843, Loss: 0.269753634929657, Final Batch Loss: 0.12866833806037903\n",
      "Epoch 6844, Loss: 0.3607446700334549, Final Batch Loss: 0.14883840084075928\n",
      "Epoch 6845, Loss: 0.35684872418642044, Final Batch Loss: 0.1653289794921875\n",
      "Epoch 6846, Loss: 0.3713160455226898, Final Batch Loss: 0.09003889560699463\n",
      "Epoch 6847, Loss: 0.477290078997612, Final Batch Loss: 0.19999146461486816\n",
      "Epoch 6848, Loss: 0.33268190920352936, Final Batch Loss: 0.1038709506392479\n",
      "Epoch 6849, Loss: 0.2945586368441582, Final Batch Loss: 0.08100855350494385\n",
      "Epoch 6850, Loss: 0.4002624973654747, Final Batch Loss: 0.15201927721500397\n",
      "Epoch 6851, Loss: 0.4143148809671402, Final Batch Loss: 0.07599003612995148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6852, Loss: 0.2737649157643318, Final Batch Loss: 0.08106344193220139\n",
      "Epoch 6853, Loss: 0.3160867914557457, Final Batch Loss: 0.12294302135705948\n",
      "Epoch 6854, Loss: 0.32083268091082573, Final Batch Loss: 0.08710544556379318\n",
      "Epoch 6855, Loss: 0.31309176981449127, Final Batch Loss: 0.08749400824308395\n",
      "Epoch 6856, Loss: 0.2998228892683983, Final Batch Loss: 0.07468748092651367\n",
      "Epoch 6857, Loss: 0.292784608900547, Final Batch Loss: 0.07796894013881683\n",
      "Epoch 6858, Loss: 0.34209731966257095, Final Batch Loss: 0.1581026315689087\n",
      "Epoch 6859, Loss: 0.26623033732175827, Final Batch Loss: 0.06779015809297562\n",
      "Epoch 6860, Loss: 0.506727859377861, Final Batch Loss: 0.24336108565330505\n",
      "Epoch 6861, Loss: 0.32878924906253815, Final Batch Loss: 0.0915762335062027\n",
      "Epoch 6862, Loss: 0.33937058597803116, Final Batch Loss: 0.09932614117860794\n",
      "Epoch 6863, Loss: 0.2870827689766884, Final Batch Loss: 0.14261659979820251\n",
      "Epoch 6864, Loss: 0.2910059317946434, Final Batch Loss: 0.09492992609739304\n",
      "Epoch 6865, Loss: 0.29333093762397766, Final Batch Loss: 0.1447029709815979\n",
      "Epoch 6866, Loss: 0.3497772887349129, Final Batch Loss: 0.09707710146903992\n",
      "Epoch 6867, Loss: 0.2571557089686394, Final Batch Loss: 0.08880947530269623\n",
      "Epoch 6868, Loss: 0.3137601390480995, Final Batch Loss: 0.1311580389738083\n",
      "Epoch 6869, Loss: 0.2912631072103977, Final Batch Loss: 0.053496625274419785\n",
      "Epoch 6870, Loss: 0.36733149737119675, Final Batch Loss: 0.1303698718547821\n",
      "Epoch 6871, Loss: 0.29278717190027237, Final Batch Loss: 0.0961356908082962\n",
      "Epoch 6872, Loss: 0.4316100403666496, Final Batch Loss: 0.1551283299922943\n",
      "Epoch 6873, Loss: 0.3215026706457138, Final Batch Loss: 0.09035435318946838\n",
      "Epoch 6874, Loss: 0.3137533590197563, Final Batch Loss: 0.12084321677684784\n",
      "Epoch 6875, Loss: 0.3166595660150051, Final Batch Loss: 0.05963161215186119\n",
      "Epoch 6876, Loss: 0.236521665006876, Final Batch Loss: 0.08731129765510559\n",
      "Epoch 6877, Loss: 0.3129363879561424, Final Batch Loss: 0.1477828025817871\n",
      "Epoch 6878, Loss: 0.21183732897043228, Final Batch Loss: 0.11335058510303497\n",
      "Epoch 6879, Loss: 0.28136567026376724, Final Batch Loss: 0.07615690678358078\n",
      "Epoch 6880, Loss: 0.33651016652584076, Final Batch Loss: 0.10479072481393814\n",
      "Epoch 6881, Loss: 0.2954386919736862, Final Batch Loss: 0.09072685986757278\n",
      "Epoch 6882, Loss: 0.46362245827913284, Final Batch Loss: 0.19929766654968262\n",
      "Epoch 6883, Loss: 0.3081747815012932, Final Batch Loss: 0.07449639588594437\n",
      "Epoch 6884, Loss: 0.3211108222603798, Final Batch Loss: 0.1206488236784935\n",
      "Epoch 6885, Loss: 0.24046370387077332, Final Batch Loss: 0.09377352893352509\n",
      "Epoch 6886, Loss: 0.32222259044647217, Final Batch Loss: 0.05722282826900482\n",
      "Epoch 6887, Loss: 0.2627868875861168, Final Batch Loss: 0.08235445618629456\n",
      "Epoch 6888, Loss: 0.39751821756362915, Final Batch Loss: 0.17037349939346313\n",
      "Epoch 6889, Loss: 0.2667124271392822, Final Batch Loss: 0.04949942231178284\n",
      "Epoch 6890, Loss: 0.36374279111623764, Final Batch Loss: 0.11112898588180542\n",
      "Epoch 6891, Loss: 0.3572690933942795, Final Batch Loss: 0.10288556665182114\n",
      "Epoch 6892, Loss: 0.40138130635023117, Final Batch Loss: 0.1973470002412796\n",
      "Epoch 6893, Loss: 0.23922617733478546, Final Batch Loss: 0.09472695738077164\n",
      "Epoch 6894, Loss: 0.36647694557905197, Final Batch Loss: 0.11744245141744614\n",
      "Epoch 6895, Loss: 0.25418294966220856, Final Batch Loss: 0.05433201789855957\n",
      "Epoch 6896, Loss: 0.3212302103638649, Final Batch Loss: 0.12372749298810959\n",
      "Epoch 6897, Loss: 0.39173003286123276, Final Batch Loss: 0.13658711314201355\n",
      "Epoch 6898, Loss: 0.299177810549736, Final Batch Loss: 0.08134519308805466\n",
      "Epoch 6899, Loss: 0.24332697689533234, Final Batch Loss: 0.06947702914476395\n",
      "Epoch 6900, Loss: 0.3052268996834755, Final Batch Loss: 0.0745345950126648\n",
      "Epoch 6901, Loss: 0.26007726415991783, Final Batch Loss: 0.042145583778619766\n",
      "Epoch 6902, Loss: 0.43152105063199997, Final Batch Loss: 0.1656852513551712\n",
      "Epoch 6903, Loss: 0.28150760382413864, Final Batch Loss: 0.06411326676607132\n",
      "Epoch 6904, Loss: 0.27855097502470016, Final Batch Loss: 0.08204083889722824\n",
      "Epoch 6905, Loss: 0.3358462452888489, Final Batch Loss: 0.07942051440477371\n",
      "Epoch 6906, Loss: 0.3972881957888603, Final Batch Loss: 0.15435801446437836\n",
      "Epoch 6907, Loss: 0.2633349895477295, Final Batch Loss: 0.06963787227869034\n",
      "Epoch 6908, Loss: 0.25584618002176285, Final Batch Loss: 0.05978892743587494\n",
      "Epoch 6909, Loss: 0.3123881369829178, Final Batch Loss: 0.08767152577638626\n",
      "Epoch 6910, Loss: 0.3065868988633156, Final Batch Loss: 0.10359247028827667\n",
      "Epoch 6911, Loss: 0.2703338637948036, Final Batch Loss: 0.07928957045078278\n",
      "Epoch 6912, Loss: 0.2213940992951393, Final Batch Loss: 0.07259660214185715\n",
      "Epoch 6913, Loss: 0.4088016599416733, Final Batch Loss: 0.15827472507953644\n",
      "Epoch 6914, Loss: 0.4237847924232483, Final Batch Loss: 0.15253929793834686\n",
      "Epoch 6915, Loss: 0.3977298438549042, Final Batch Loss: 0.13875015079975128\n",
      "Epoch 6916, Loss: 0.31003228574991226, Final Batch Loss: 0.06258377432823181\n",
      "Epoch 6917, Loss: 0.34979982674121857, Final Batch Loss: 0.09402509033679962\n",
      "Epoch 6918, Loss: 0.40867139399051666, Final Batch Loss: 0.18750163912773132\n",
      "Epoch 6919, Loss: 0.2976125702261925, Final Batch Loss: 0.13108037412166595\n",
      "Epoch 6920, Loss: 0.2925253063440323, Final Batch Loss: 0.0862213745713234\n",
      "Epoch 6921, Loss: 0.3028677850961685, Final Batch Loss: 0.06976574659347534\n",
      "Epoch 6922, Loss: 0.346737377345562, Final Batch Loss: 0.11882929503917694\n",
      "Epoch 6923, Loss: 0.3035947233438492, Final Batch Loss: 0.09229591488838196\n",
      "Epoch 6924, Loss: 0.3302401527762413, Final Batch Loss: 0.10279949009418488\n",
      "Epoch 6925, Loss: 0.31599706411361694, Final Batch Loss: 0.1395232081413269\n",
      "Epoch 6926, Loss: 0.3561280518770218, Final Batch Loss: 0.15450812876224518\n",
      "Epoch 6927, Loss: 0.3748483285307884, Final Batch Loss: 0.14090879261493683\n",
      "Epoch 6928, Loss: 0.3477889746427536, Final Batch Loss: 0.17394042015075684\n",
      "Epoch 6929, Loss: 0.352688267827034, Final Batch Loss: 0.09554742276668549\n",
      "Epoch 6930, Loss: 0.4520483762025833, Final Batch Loss: 0.12703727185726166\n",
      "Epoch 6931, Loss: 0.3768850564956665, Final Batch Loss: 0.118157297372818\n",
      "Epoch 6932, Loss: 0.304167777299881, Final Batch Loss: 0.1175166666507721\n",
      "Epoch 6933, Loss: 0.40016767382621765, Final Batch Loss: 0.11500052362680435\n",
      "Epoch 6934, Loss: 0.43069932609796524, Final Batch Loss: 0.11087552458047867\n",
      "Epoch 6935, Loss: 0.2871665135025978, Final Batch Loss: 0.08984321355819702\n",
      "Epoch 6936, Loss: 0.3353564068675041, Final Batch Loss: 0.08870455622673035\n",
      "Epoch 6937, Loss: 0.349502757191658, Final Batch Loss: 0.1489047259092331\n",
      "Epoch 6938, Loss: 0.3576201722025871, Final Batch Loss: 0.11013133823871613\n",
      "Epoch 6939, Loss: 0.3439335823059082, Final Batch Loss: 0.09789638221263885\n",
      "Epoch 6940, Loss: 0.41734834015369415, Final Batch Loss: 0.17003491520881653\n",
      "Epoch 6941, Loss: 0.3463268168270588, Final Batch Loss: 0.05827916041016579\n",
      "Epoch 6942, Loss: 0.3049113228917122, Final Batch Loss: 0.13968683779239655\n",
      "Epoch 6943, Loss: 0.31171808391809464, Final Batch Loss: 0.07811153680086136\n",
      "Epoch 6944, Loss: 0.3511166349053383, Final Batch Loss: 0.15009130537509918\n",
      "Epoch 6945, Loss: 0.3386056125164032, Final Batch Loss: 0.08366351574659348\n",
      "Epoch 6946, Loss: 0.2698271870613098, Final Batch Loss: 0.09459633380174637\n",
      "Epoch 6947, Loss: 0.3603852242231369, Final Batch Loss: 0.1847730427980423\n",
      "Epoch 6948, Loss: 0.3175501823425293, Final Batch Loss: 0.07803243398666382\n",
      "Epoch 6949, Loss: 0.41550634801387787, Final Batch Loss: 0.16297486424446106\n",
      "Epoch 6950, Loss: 0.4382507801055908, Final Batch Loss: 0.2351481020450592\n",
      "Epoch 6951, Loss: 0.3224498927593231, Final Batch Loss: 0.10087589174509048\n",
      "Epoch 6952, Loss: 0.44774559885263443, Final Batch Loss: 0.21162185072898865\n",
      "Epoch 6953, Loss: 0.45768220722675323, Final Batch Loss: 0.1469811201095581\n",
      "Epoch 6954, Loss: 0.276678241789341, Final Batch Loss: 0.11396677047014236\n",
      "Epoch 6955, Loss: 0.3785660043358803, Final Batch Loss: 0.16055776178836823\n",
      "Epoch 6956, Loss: 0.3208775222301483, Final Batch Loss: 0.14790405333042145\n",
      "Epoch 6957, Loss: 0.5104424580931664, Final Batch Loss: 0.2417077124118805\n",
      "Epoch 6958, Loss: 0.34708072245121, Final Batch Loss: 0.13954858481884003\n",
      "Epoch 6959, Loss: 0.3552556410431862, Final Batch Loss: 0.126669242978096\n",
      "Epoch 6960, Loss: 0.5019327998161316, Final Batch Loss: 0.23491951823234558\n",
      "Epoch 6961, Loss: 0.3174188733100891, Final Batch Loss: 0.10719502717256546\n",
      "Epoch 6962, Loss: 0.36861734092235565, Final Batch Loss: 0.13909922540187836\n",
      "Epoch 6963, Loss: 0.3102835714817047, Final Batch Loss: 0.14651629328727722\n",
      "Epoch 6964, Loss: 0.27394236624240875, Final Batch Loss: 0.08951076865196228\n",
      "Epoch 6965, Loss: 0.33117593079805374, Final Batch Loss: 0.1469356268644333\n",
      "Epoch 6966, Loss: 0.33814533054828644, Final Batch Loss: 0.12250909954309464\n",
      "Epoch 6967, Loss: 0.2637608274817467, Final Batch Loss: 0.08812394738197327\n",
      "Epoch 6968, Loss: 0.3131544664502144, Final Batch Loss: 0.10068140923976898\n",
      "Epoch 6969, Loss: 0.2593872621655464, Final Batch Loss: 0.08923178911209106\n",
      "Epoch 6970, Loss: 0.35705892741680145, Final Batch Loss: 0.1331513673067093\n",
      "Epoch 6971, Loss: 0.3064848706126213, Final Batch Loss: 0.10935486108064651\n",
      "Epoch 6972, Loss: 0.4281558468937874, Final Batch Loss: 0.20261432230472565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6973, Loss: 0.2584000304341316, Final Batch Loss: 0.08933430910110474\n",
      "Epoch 6974, Loss: 0.24147772043943405, Final Batch Loss: 0.08978106826543808\n",
      "Epoch 6975, Loss: 0.35230913758277893, Final Batch Loss: 0.10763660073280334\n",
      "Epoch 6976, Loss: 0.2677266523241997, Final Batch Loss: 0.07106894254684448\n",
      "Epoch 6977, Loss: 0.3199850246310234, Final Batch Loss: 0.10822685807943344\n",
      "Epoch 6978, Loss: 0.2554694600403309, Final Batch Loss: 0.05695933476090431\n",
      "Epoch 6979, Loss: 0.3246418237686157, Final Batch Loss: 0.12495245784521103\n",
      "Epoch 6980, Loss: 0.3138650432229042, Final Batch Loss: 0.06670712679624557\n",
      "Epoch 6981, Loss: 0.31256283819675446, Final Batch Loss: 0.16391126811504364\n",
      "Epoch 6982, Loss: 0.43729446828365326, Final Batch Loss: 0.12060213088989258\n",
      "Epoch 6983, Loss: 0.2730840928852558, Final Batch Loss: 0.06093503162264824\n",
      "Epoch 6984, Loss: 0.274670273065567, Final Batch Loss: 0.11426834017038345\n",
      "Epoch 6985, Loss: 0.29099805653095245, Final Batch Loss: 0.12252725660800934\n",
      "Epoch 6986, Loss: 0.28464794158935547, Final Batch Loss: 0.0555710643529892\n",
      "Epoch 6987, Loss: 0.35823605954647064, Final Batch Loss: 0.11680018156766891\n",
      "Epoch 6988, Loss: 0.22277461364865303, Final Batch Loss: 0.08326549082994461\n",
      "Epoch 6989, Loss: 0.29135793447494507, Final Batch Loss: 0.05087495595216751\n",
      "Epoch 6990, Loss: 0.37856028228998184, Final Batch Loss: 0.14752736687660217\n",
      "Epoch 6991, Loss: 0.36245544999837875, Final Batch Loss: 0.06345144659280777\n",
      "Epoch 6992, Loss: 0.3249405696988106, Final Batch Loss: 0.17120938003063202\n",
      "Epoch 6993, Loss: 0.34767623245716095, Final Batch Loss: 0.097538523375988\n",
      "Epoch 6994, Loss: 0.33027466386556625, Final Batch Loss: 0.1028539165854454\n",
      "Epoch 6995, Loss: 0.30224836990237236, Final Batch Loss: 0.05423701927065849\n",
      "Epoch 6996, Loss: 0.3177374601364136, Final Batch Loss: 0.16280688345432281\n",
      "Epoch 6997, Loss: 0.24370110780000687, Final Batch Loss: 0.07177931070327759\n",
      "Epoch 6998, Loss: 0.2732424959540367, Final Batch Loss: 0.07918759435415268\n",
      "Epoch 6999, Loss: 0.2762060761451721, Final Batch Loss: 0.08517389744520187\n",
      "Epoch 7000, Loss: 0.3278743103146553, Final Batch Loss: 0.11681295931339264\n",
      "Epoch 7001, Loss: 0.33321159332990646, Final Batch Loss: 0.14483582973480225\n",
      "Epoch 7002, Loss: 0.22010590508580208, Final Batch Loss: 0.04899764060974121\n",
      "Epoch 7003, Loss: 0.3021853491663933, Final Batch Loss: 0.15389898419380188\n",
      "Epoch 7004, Loss: 0.33362995833158493, Final Batch Loss: 0.07339991629123688\n",
      "Epoch 7005, Loss: 0.2749355584383011, Final Batch Loss: 0.0982518345117569\n",
      "Epoch 7006, Loss: 0.30626143887639046, Final Batch Loss: 0.0621926374733448\n",
      "Epoch 7007, Loss: 0.2739820033311844, Final Batch Loss: 0.07792612910270691\n",
      "Epoch 7008, Loss: 0.34562697261571884, Final Batch Loss: 0.11180417984724045\n",
      "Epoch 7009, Loss: 0.3341556787490845, Final Batch Loss: 0.10681938380002975\n",
      "Epoch 7010, Loss: 0.31420282274484634, Final Batch Loss: 0.14529038965702057\n",
      "Epoch 7011, Loss: 0.32604003697633743, Final Batch Loss: 0.14125527441501617\n",
      "Epoch 7012, Loss: 0.3478006273508072, Final Batch Loss: 0.12301404029130936\n",
      "Epoch 7013, Loss: 0.30968689173460007, Final Batch Loss: 0.10822610557079315\n",
      "Epoch 7014, Loss: 0.29949116706848145, Final Batch Loss: 0.07399380207061768\n",
      "Epoch 7015, Loss: 0.3812934160232544, Final Batch Loss: 0.0794672966003418\n",
      "Epoch 7016, Loss: 0.26454247906804085, Final Batch Loss: 0.09080509841442108\n",
      "Epoch 7017, Loss: 0.40722332149744034, Final Batch Loss: 0.14059196412563324\n",
      "Epoch 7018, Loss: 0.4547639787197113, Final Batch Loss: 0.1965673416852951\n",
      "Epoch 7019, Loss: 0.4023979604244232, Final Batch Loss: 0.18603725731372833\n",
      "Epoch 7020, Loss: 0.4324216768145561, Final Batch Loss: 0.17888674139976501\n",
      "Epoch 7021, Loss: 0.4956401512026787, Final Batch Loss: 0.24456222355365753\n",
      "Epoch 7022, Loss: 0.27660464495420456, Final Batch Loss: 0.07616438716650009\n",
      "Epoch 7023, Loss: 0.473006971180439, Final Batch Loss: 0.21557800471782684\n",
      "Epoch 7024, Loss: 0.3371920809149742, Final Batch Loss: 0.0753997191786766\n",
      "Epoch 7025, Loss: 0.32461097836494446, Final Batch Loss: 0.07226577401161194\n",
      "Epoch 7026, Loss: 0.3064878210425377, Final Batch Loss: 0.08978861570358276\n",
      "Epoch 7027, Loss: 0.4117951765656471, Final Batch Loss: 0.16126196086406708\n",
      "Epoch 7028, Loss: 0.36646953225135803, Final Batch Loss: 0.08730936050415039\n",
      "Epoch 7029, Loss: 0.4649689421057701, Final Batch Loss: 0.1714155226945877\n",
      "Epoch 7030, Loss: 0.39422721415758133, Final Batch Loss: 0.14050772786140442\n",
      "Epoch 7031, Loss: 0.43276698142290115, Final Batch Loss: 0.1456262469291687\n",
      "Epoch 7032, Loss: 0.5550681501626968, Final Batch Loss: 0.13700610399246216\n",
      "Epoch 7033, Loss: 0.5238075405359268, Final Batch Loss: 0.21539388597011566\n",
      "Epoch 7034, Loss: 0.2902561277151108, Final Batch Loss: 0.0784284695982933\n",
      "Epoch 7035, Loss: 0.49745874106884, Final Batch Loss: 0.11447976529598236\n",
      "Epoch 7036, Loss: 0.3853773772716522, Final Batch Loss: 0.1531219482421875\n",
      "Epoch 7037, Loss: 0.5045442432165146, Final Batch Loss: 0.12122295796871185\n",
      "Epoch 7038, Loss: 0.29830797016620636, Final Batch Loss: 0.08308728039264679\n",
      "Epoch 7039, Loss: 0.3773891255259514, Final Batch Loss: 0.13671043515205383\n",
      "Epoch 7040, Loss: 0.42775896936655045, Final Batch Loss: 0.21639612317085266\n",
      "Epoch 7041, Loss: 0.31565137952566147, Final Batch Loss: 0.07618003338575363\n",
      "Epoch 7042, Loss: 0.3071990832686424, Final Batch Loss: 0.1168796569108963\n",
      "Epoch 7043, Loss: 0.34474412351846695, Final Batch Loss: 0.07442503422498703\n",
      "Epoch 7044, Loss: 0.3296026736497879, Final Batch Loss: 0.11061865836381912\n",
      "Epoch 7045, Loss: 0.32621005177497864, Final Batch Loss: 0.1320292055606842\n",
      "Epoch 7046, Loss: 0.3510889559984207, Final Batch Loss: 0.10851985216140747\n",
      "Epoch 7047, Loss: 0.3240938037633896, Final Batch Loss: 0.09584730863571167\n",
      "Epoch 7048, Loss: 0.32542797923088074, Final Batch Loss: 0.08185341209173203\n",
      "Epoch 7049, Loss: 0.35770416259765625, Final Batch Loss: 0.09902115166187286\n",
      "Epoch 7050, Loss: 0.38729121536016464, Final Batch Loss: 0.1519010365009308\n",
      "Epoch 7051, Loss: 0.3077981546521187, Final Batch Loss: 0.10997466742992401\n",
      "Epoch 7052, Loss: 0.3598335161805153, Final Batch Loss: 0.1322391778230667\n",
      "Epoch 7053, Loss: 0.40005358308553696, Final Batch Loss: 0.11553077399730682\n",
      "Epoch 7054, Loss: 0.2836320176720619, Final Batch Loss: 0.11417515575885773\n",
      "Epoch 7055, Loss: 0.3146500363945961, Final Batch Loss: 0.09336838871240616\n",
      "Epoch 7056, Loss: 0.2339448481798172, Final Batch Loss: 0.09157366305589676\n",
      "Epoch 7057, Loss: 0.3804103508591652, Final Batch Loss: 0.13984675705432892\n",
      "Epoch 7058, Loss: 0.30602260679006577, Final Batch Loss: 0.09231571108102798\n",
      "Epoch 7059, Loss: 0.2628129757940769, Final Batch Loss: 0.06203243508934975\n",
      "Epoch 7060, Loss: 0.367813378572464, Final Batch Loss: 0.15347924828529358\n",
      "Epoch 7061, Loss: 0.2289174199104309, Final Batch Loss: 0.07781517505645752\n",
      "Epoch 7062, Loss: 0.2464214563369751, Final Batch Loss: 0.09372355788946152\n",
      "Epoch 7063, Loss: 0.23288030177354813, Final Batch Loss: 0.061747632920742035\n",
      "Epoch 7064, Loss: 0.2724437527358532, Final Batch Loss: 0.10878093540668488\n",
      "Epoch 7065, Loss: 0.3746894896030426, Final Batch Loss: 0.14819040894508362\n",
      "Epoch 7066, Loss: 0.32008476182818413, Final Batch Loss: 0.05160973593592644\n",
      "Epoch 7067, Loss: 0.23857975751161575, Final Batch Loss: 0.05516054481267929\n",
      "Epoch 7068, Loss: 0.2836972251534462, Final Batch Loss: 0.09879829734563828\n",
      "Epoch 7069, Loss: 0.3193718045949936, Final Batch Loss: 0.07638966292142868\n",
      "Epoch 7070, Loss: 0.3209080547094345, Final Batch Loss: 0.10495225340127945\n",
      "Epoch 7071, Loss: 0.278278112411499, Final Batch Loss: 0.09845631569623947\n",
      "Epoch 7072, Loss: 0.3297037184238434, Final Batch Loss: 0.1500067263841629\n",
      "Epoch 7073, Loss: 0.2664499692618847, Final Batch Loss: 0.08739287406206131\n",
      "Epoch 7074, Loss: 0.3320293501019478, Final Batch Loss: 0.09754364937543869\n",
      "Epoch 7075, Loss: 0.2867388427257538, Final Batch Loss: 0.0787305235862732\n",
      "Epoch 7076, Loss: 0.32447976619005203, Final Batch Loss: 0.07783485949039459\n",
      "Epoch 7077, Loss: 0.3366084173321724, Final Batch Loss: 0.10566049069166183\n",
      "Epoch 7078, Loss: 0.34189462661743164, Final Batch Loss: 0.07316088676452637\n",
      "Epoch 7079, Loss: 0.3048992082476616, Final Batch Loss: 0.14983248710632324\n",
      "Epoch 7080, Loss: 0.2799665406346321, Final Batch Loss: 0.0969049260020256\n",
      "Epoch 7081, Loss: 0.31646110117435455, Final Batch Loss: 0.08542150259017944\n",
      "Epoch 7082, Loss: 0.30083658546209335, Final Batch Loss: 0.06624427437782288\n",
      "Epoch 7083, Loss: 0.3247150629758835, Final Batch Loss: 0.14525188505649567\n",
      "Epoch 7084, Loss: 0.39644405245780945, Final Batch Loss: 0.142636239528656\n",
      "Epoch 7085, Loss: 0.2833257243037224, Final Batch Loss: 0.09255141019821167\n",
      "Epoch 7086, Loss: 0.3668057844042778, Final Batch Loss: 0.12257786840200424\n",
      "Epoch 7087, Loss: 0.30823440104722977, Final Batch Loss: 0.07008547335863113\n",
      "Epoch 7088, Loss: 0.3440232053399086, Final Batch Loss: 0.12097280472517014\n",
      "Epoch 7089, Loss: 0.27749088034033775, Final Batch Loss: 0.0596819631755352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7090, Loss: 0.36189092695713043, Final Batch Loss: 0.10189060121774673\n",
      "Epoch 7091, Loss: 0.32001592218875885, Final Batch Loss: 0.07159650325775146\n",
      "Epoch 7092, Loss: 0.31908831745386124, Final Batch Loss: 0.0867721289396286\n",
      "Epoch 7093, Loss: 0.30858440697193146, Final Batch Loss: 0.1033484935760498\n",
      "Epoch 7094, Loss: 0.37321067601442337, Final Batch Loss: 0.14911717176437378\n",
      "Epoch 7095, Loss: 0.32834333926439285, Final Batch Loss: 0.11515506356954575\n",
      "Epoch 7096, Loss: 0.4616669863462448, Final Batch Loss: 0.15830326080322266\n",
      "Epoch 7097, Loss: 0.31427448242902756, Final Batch Loss: 0.09247704595327377\n",
      "Epoch 7098, Loss: 0.4446321874856949, Final Batch Loss: 0.23393894731998444\n",
      "Epoch 7099, Loss: 0.3408070430159569, Final Batch Loss: 0.08769595623016357\n",
      "Epoch 7100, Loss: 0.2752075716853142, Final Batch Loss: 0.07476002722978592\n",
      "Epoch 7101, Loss: 0.35287298262119293, Final Batch Loss: 0.11102461069822311\n",
      "Epoch 7102, Loss: 0.36678793281316757, Final Batch Loss: 0.13655106723308563\n",
      "Epoch 7103, Loss: 0.38677291199564934, Final Batch Loss: 0.21218986809253693\n",
      "Epoch 7104, Loss: 0.2569461427628994, Final Batch Loss: 0.07563906908035278\n",
      "Epoch 7105, Loss: 0.28020134568214417, Final Batch Loss: 0.07207381725311279\n",
      "Epoch 7106, Loss: 0.24168555811047554, Final Batch Loss: 0.05235123261809349\n",
      "Epoch 7107, Loss: 0.39199909567832947, Final Batch Loss: 0.06751102209091187\n",
      "Epoch 7108, Loss: 0.3036072850227356, Final Batch Loss: 0.08369161188602448\n",
      "Epoch 7109, Loss: 0.32769643515348434, Final Batch Loss: 0.14136385917663574\n",
      "Epoch 7110, Loss: 0.2846996337175369, Final Batch Loss: 0.1134062334895134\n",
      "Epoch 7111, Loss: 0.3595701977610588, Final Batch Loss: 0.153189018368721\n",
      "Epoch 7112, Loss: 0.3374980315566063, Final Batch Loss: 0.09722626209259033\n",
      "Epoch 7113, Loss: 0.31252358108758926, Final Batch Loss: 0.08133113384246826\n",
      "Epoch 7114, Loss: 0.32670604437589645, Final Batch Loss: 0.13896077871322632\n",
      "Epoch 7115, Loss: 0.3503536805510521, Final Batch Loss: 0.14322611689567566\n",
      "Epoch 7116, Loss: 0.25239497423171997, Final Batch Loss: 0.09840206801891327\n",
      "Epoch 7117, Loss: 0.28575360774993896, Final Batch Loss: 0.06371646374464035\n",
      "Epoch 7118, Loss: 0.3042837455868721, Final Batch Loss: 0.08880480378866196\n",
      "Epoch 7119, Loss: 0.28561752289533615, Final Batch Loss: 0.10916851460933685\n",
      "Epoch 7120, Loss: 0.28131549805402756, Final Batch Loss: 0.10539819300174713\n",
      "Epoch 7121, Loss: 0.2989979013800621, Final Batch Loss: 0.06566458195447922\n",
      "Epoch 7122, Loss: 0.4298805221915245, Final Batch Loss: 0.10409574955701828\n",
      "Epoch 7123, Loss: 0.42907126247882843, Final Batch Loss: 0.19057707488536835\n",
      "Epoch 7124, Loss: 0.2975738123059273, Final Batch Loss: 0.11458694934844971\n",
      "Epoch 7125, Loss: 0.29561154544353485, Final Batch Loss: 0.08336237818002701\n",
      "Epoch 7126, Loss: 0.3045928329229355, Final Batch Loss: 0.11239331215620041\n",
      "Epoch 7127, Loss: 0.3207925632596016, Final Batch Loss: 0.09284774959087372\n",
      "Epoch 7128, Loss: 0.29979512095451355, Final Batch Loss: 0.08224319666624069\n",
      "Epoch 7129, Loss: 0.28542014211416245, Final Batch Loss: 0.06867101043462753\n",
      "Epoch 7130, Loss: 0.3571353778243065, Final Batch Loss: 0.0928761214017868\n",
      "Epoch 7131, Loss: 0.33051320165395737, Final Batch Loss: 0.15610109269618988\n",
      "Epoch 7132, Loss: 0.25286436825990677, Final Batch Loss: 0.059178002178668976\n",
      "Epoch 7133, Loss: 0.3406801223754883, Final Batch Loss: 0.15209029614925385\n",
      "Epoch 7134, Loss: 0.2887955829501152, Final Batch Loss: 0.09800173342227936\n",
      "Epoch 7135, Loss: 0.2790541350841522, Final Batch Loss: 0.07886900007724762\n",
      "Epoch 7136, Loss: 0.21697848290205002, Final Batch Loss: 0.06634216755628586\n",
      "Epoch 7137, Loss: 0.324855737388134, Final Batch Loss: 0.06803145259618759\n",
      "Epoch 7138, Loss: 0.26686715334653854, Final Batch Loss: 0.0654759332537651\n",
      "Epoch 7139, Loss: 0.2944330424070358, Final Batch Loss: 0.1210811585187912\n",
      "Epoch 7140, Loss: 0.30443616956472397, Final Batch Loss: 0.11581246554851532\n",
      "Epoch 7141, Loss: 0.3244132697582245, Final Batch Loss: 0.10576379299163818\n",
      "Epoch 7142, Loss: 0.4115873649716377, Final Batch Loss: 0.08481806516647339\n",
      "Epoch 7143, Loss: 0.34108586609363556, Final Batch Loss: 0.12425795942544937\n",
      "Epoch 7144, Loss: 0.2920297682285309, Final Batch Loss: 0.09552884101867676\n",
      "Epoch 7145, Loss: 0.3077116757631302, Final Batch Loss: 0.1468857079744339\n",
      "Epoch 7146, Loss: 0.3226550295948982, Final Batch Loss: 0.0801134929060936\n",
      "Epoch 7147, Loss: 0.353191576898098, Final Batch Loss: 0.13206499814987183\n",
      "Epoch 7148, Loss: 0.27012744173407555, Final Batch Loss: 0.0435788668692112\n",
      "Epoch 7149, Loss: 0.2808603048324585, Final Batch Loss: 0.08884590864181519\n",
      "Epoch 7150, Loss: 0.2975265681743622, Final Batch Loss: 0.08414182811975479\n",
      "Epoch 7151, Loss: 0.2656604051589966, Final Batch Loss: 0.08326170593500137\n",
      "Epoch 7152, Loss: 0.28032926470041275, Final Batch Loss: 0.10606826841831207\n",
      "Epoch 7153, Loss: 0.4344407990574837, Final Batch Loss: 0.13270576298236847\n",
      "Epoch 7154, Loss: 0.2569427564740181, Final Batch Loss: 0.09678926318883896\n",
      "Epoch 7155, Loss: 0.33754339441657066, Final Batch Loss: 0.049254123121500015\n",
      "Epoch 7156, Loss: 0.33114606887102127, Final Batch Loss: 0.09911336749792099\n",
      "Epoch 7157, Loss: 0.2520376406610012, Final Batch Loss: 0.047022391110658646\n",
      "Epoch 7158, Loss: 0.35964884981513023, Final Batch Loss: 0.17368437349796295\n",
      "Epoch 7159, Loss: 0.2774961590766907, Final Batch Loss: 0.11196786165237427\n",
      "Epoch 7160, Loss: 0.3629307970404625, Final Batch Loss: 0.11068013310432434\n",
      "Epoch 7161, Loss: 0.32543614506721497, Final Batch Loss: 0.08074677735567093\n",
      "Epoch 7162, Loss: 0.35513080656528473, Final Batch Loss: 0.13771790266036987\n",
      "Epoch 7163, Loss: 0.3142474740743637, Final Batch Loss: 0.08706595748662949\n",
      "Epoch 7164, Loss: 0.2615106925368309, Final Batch Loss: 0.09315581619739532\n",
      "Epoch 7165, Loss: 0.21301599964499474, Final Batch Loss: 0.08222374320030212\n",
      "Epoch 7166, Loss: 0.23051412776112556, Final Batch Loss: 0.04824261739850044\n",
      "Epoch 7167, Loss: 0.31162161380052567, Final Batch Loss: 0.12138234823942184\n",
      "Epoch 7168, Loss: 0.2325860857963562, Final Batch Loss: 0.0845818817615509\n",
      "Epoch 7169, Loss: 0.25536439195275307, Final Batch Loss: 0.06211944296956062\n",
      "Epoch 7170, Loss: 0.34220585227012634, Final Batch Loss: 0.07641880959272385\n",
      "Epoch 7171, Loss: 0.2950191795825958, Final Batch Loss: 0.04188133031129837\n",
      "Epoch 7172, Loss: 0.31629136949777603, Final Batch Loss: 0.1089482307434082\n",
      "Epoch 7173, Loss: 0.34295136481523514, Final Batch Loss: 0.11013267189264297\n",
      "Epoch 7174, Loss: 0.2639724686741829, Final Batch Loss: 0.10380382090806961\n",
      "Epoch 7175, Loss: 0.3412378951907158, Final Batch Loss: 0.10126234591007233\n",
      "Epoch 7176, Loss: 0.30590103566646576, Final Batch Loss: 0.1071309894323349\n",
      "Epoch 7177, Loss: 0.23097315430641174, Final Batch Loss: 0.07037486135959625\n",
      "Epoch 7178, Loss: 0.29447314888238907, Final Batch Loss: 0.09034281224012375\n",
      "Epoch 7179, Loss: 0.31321704387664795, Final Batch Loss: 0.08493774384260178\n",
      "Epoch 7180, Loss: 0.3265349641442299, Final Batch Loss: 0.12138921022415161\n",
      "Epoch 7181, Loss: 0.31892208755016327, Final Batch Loss: 0.14369498193264008\n",
      "Epoch 7182, Loss: 0.24387270212173462, Final Batch Loss: 0.0511399544775486\n",
      "Epoch 7183, Loss: 0.3108031898736954, Final Batch Loss: 0.08084344863891602\n",
      "Epoch 7184, Loss: 0.27502497285604477, Final Batch Loss: 0.09404190629720688\n",
      "Epoch 7185, Loss: 0.3425860106945038, Final Batch Loss: 0.1397479772567749\n",
      "Epoch 7186, Loss: 0.3227536827325821, Final Batch Loss: 0.10416541248559952\n",
      "Epoch 7187, Loss: 0.3258252888917923, Final Batch Loss: 0.08941581100225449\n",
      "Epoch 7188, Loss: 0.3159084841609001, Final Batch Loss: 0.14194746315479279\n",
      "Epoch 7189, Loss: 0.33417005464434624, Final Batch Loss: 0.15374498069286346\n",
      "Epoch 7190, Loss: 0.28037138283252716, Final Batch Loss: 0.07237448543310165\n",
      "Epoch 7191, Loss: 0.24832753837108612, Final Batch Loss: 0.05356279015541077\n",
      "Epoch 7192, Loss: 0.35096684098243713, Final Batch Loss: 0.10141776502132416\n",
      "Epoch 7193, Loss: 0.23809818178415298, Final Batch Loss: 0.09323630481958389\n",
      "Epoch 7194, Loss: 0.3699215017259121, Final Batch Loss: 0.15236110985279083\n",
      "Epoch 7195, Loss: 0.28785981237888336, Final Batch Loss: 0.09463172405958176\n",
      "Epoch 7196, Loss: 0.41504620015621185, Final Batch Loss: 0.16607247292995453\n",
      "Epoch 7197, Loss: 0.3436001166701317, Final Batch Loss: 0.10646424442529678\n",
      "Epoch 7198, Loss: 0.24686846137046814, Final Batch Loss: 0.04978866130113602\n",
      "Epoch 7199, Loss: 0.47733528912067413, Final Batch Loss: 0.17118807137012482\n",
      "Epoch 7200, Loss: 0.26319678127765656, Final Batch Loss: 0.09313999116420746\n",
      "Epoch 7201, Loss: 0.2922919914126396, Final Batch Loss: 0.11836173385381699\n",
      "Epoch 7202, Loss: 0.23446695879101753, Final Batch Loss: 0.03809879347681999\n",
      "Epoch 7203, Loss: 0.2816454544663429, Final Batch Loss: 0.08748037368059158\n",
      "Epoch 7204, Loss: 0.2685806006193161, Final Batch Loss: 0.12589019536972046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7205, Loss: 0.23967793211340904, Final Batch Loss: 0.13132880628108978\n",
      "Epoch 7206, Loss: 0.3235303983092308, Final Batch Loss: 0.11310181766748428\n",
      "Epoch 7207, Loss: 0.41928109526634216, Final Batch Loss: 0.13123001158237457\n",
      "Epoch 7208, Loss: 0.3612795025110245, Final Batch Loss: 0.15519261360168457\n",
      "Epoch 7209, Loss: 0.42688556760549545, Final Batch Loss: 0.12075062841176987\n",
      "Epoch 7210, Loss: 0.2711128294467926, Final Batch Loss: 0.07799432426691055\n",
      "Epoch 7211, Loss: 0.3950212523341179, Final Batch Loss: 0.16934101283550262\n",
      "Epoch 7212, Loss: 0.3191375955939293, Final Batch Loss: 0.07157079130411148\n",
      "Epoch 7213, Loss: 0.2896561175584793, Final Batch Loss: 0.08620275557041168\n",
      "Epoch 7214, Loss: 0.3594762086868286, Final Batch Loss: 0.13504095375537872\n",
      "Epoch 7215, Loss: 0.3152570500969887, Final Batch Loss: 0.08160229027271271\n",
      "Epoch 7216, Loss: 0.27646975964307785, Final Batch Loss: 0.10495895147323608\n",
      "Epoch 7217, Loss: 0.35636525601148605, Final Batch Loss: 0.08000615239143372\n",
      "Epoch 7218, Loss: 0.2367321513593197, Final Batch Loss: 0.07359454780817032\n",
      "Epoch 7219, Loss: 0.26432398706674576, Final Batch Loss: 0.09269889444112778\n",
      "Epoch 7220, Loss: 0.29742611944675446, Final Batch Loss: 0.07119480520486832\n",
      "Epoch 7221, Loss: 0.3637660965323448, Final Batch Loss: 0.06695576757192612\n",
      "Epoch 7222, Loss: 0.3504822552204132, Final Batch Loss: 0.1041065976023674\n",
      "Epoch 7223, Loss: 0.3046768680214882, Final Batch Loss: 0.10404190421104431\n",
      "Epoch 7224, Loss: 0.24053139612078667, Final Batch Loss: 0.05724913999438286\n",
      "Epoch 7225, Loss: 0.23212122917175293, Final Batch Loss: 0.061573438346385956\n",
      "Epoch 7226, Loss: 0.26253338158130646, Final Batch Loss: 0.08210034668445587\n",
      "Epoch 7227, Loss: 0.2683110237121582, Final Batch Loss: 0.0833175778388977\n",
      "Epoch 7228, Loss: 0.3109219968318939, Final Batch Loss: 0.09129081666469574\n",
      "Epoch 7229, Loss: 0.2616712301969528, Final Batch Loss: 0.0602959543466568\n",
      "Epoch 7230, Loss: 0.3797845058143139, Final Batch Loss: 0.1684015542268753\n",
      "Epoch 7231, Loss: 0.25474729761481285, Final Batch Loss: 0.035567622631788254\n",
      "Epoch 7232, Loss: 0.2752455398440361, Final Batch Loss: 0.07208570837974548\n",
      "Epoch 7233, Loss: 0.2913532555103302, Final Batch Loss: 0.064176544547081\n",
      "Epoch 7234, Loss: 0.29060281813144684, Final Batch Loss: 0.12162862718105316\n",
      "Epoch 7235, Loss: 0.29706087335944176, Final Batch Loss: 0.11128837615251541\n",
      "Epoch 7236, Loss: 0.3286681994795799, Final Batch Loss: 0.11384522914886475\n",
      "Epoch 7237, Loss: 0.22885879129171371, Final Batch Loss: 0.0313221737742424\n",
      "Epoch 7238, Loss: 0.2749546766281128, Final Batch Loss: 0.08530548214912415\n",
      "Epoch 7239, Loss: 0.31427932530641556, Final Batch Loss: 0.1288105547428131\n",
      "Epoch 7240, Loss: 0.30981308221817017, Final Batch Loss: 0.11205898225307465\n",
      "Epoch 7241, Loss: 0.3209129273891449, Final Batch Loss: 0.07691194117069244\n",
      "Epoch 7242, Loss: 0.3235388547182083, Final Batch Loss: 0.098861925303936\n",
      "Epoch 7243, Loss: 0.18266592919826508, Final Batch Loss: 0.04080183804035187\n",
      "Epoch 7244, Loss: 0.18792006373405457, Final Batch Loss: 0.037910934537649155\n",
      "Epoch 7245, Loss: 0.4016277641057968, Final Batch Loss: 0.10191251337528229\n",
      "Epoch 7246, Loss: 0.3172089606523514, Final Batch Loss: 0.10943792760372162\n",
      "Epoch 7247, Loss: 0.2858102172613144, Final Batch Loss: 0.08075094223022461\n",
      "Epoch 7248, Loss: 0.36493465304374695, Final Batch Loss: 0.18955090641975403\n",
      "Epoch 7249, Loss: 0.26036349684000015, Final Batch Loss: 0.08767680078744888\n",
      "Epoch 7250, Loss: 0.40488509088754654, Final Batch Loss: 0.07411227375268936\n",
      "Epoch 7251, Loss: 0.3144781216979027, Final Batch Loss: 0.10990382730960846\n",
      "Epoch 7252, Loss: 0.26450837403535843, Final Batch Loss: 0.10039352625608444\n",
      "Epoch 7253, Loss: 0.26264631748199463, Final Batch Loss: 0.09795019030570984\n",
      "Epoch 7254, Loss: 0.3636538237333298, Final Batch Loss: 0.11226747930049896\n",
      "Epoch 7255, Loss: 0.2842223271727562, Final Batch Loss: 0.1211085319519043\n",
      "Epoch 7256, Loss: 0.2961304634809494, Final Batch Loss: 0.10118215531110764\n",
      "Epoch 7257, Loss: 0.3031691238284111, Final Batch Loss: 0.14138264954090118\n",
      "Epoch 7258, Loss: 0.31997258216142654, Final Batch Loss: 0.09635095298290253\n",
      "Epoch 7259, Loss: 0.2608689144253731, Final Batch Loss: 0.08786490559577942\n",
      "Epoch 7260, Loss: 0.3590284138917923, Final Batch Loss: 0.11040964722633362\n",
      "Epoch 7261, Loss: 0.2987520545721054, Final Batch Loss: 0.06507496535778046\n",
      "Epoch 7262, Loss: 0.3771166056394577, Final Batch Loss: 0.12082955241203308\n",
      "Epoch 7263, Loss: 0.24484383687376976, Final Batch Loss: 0.09739191830158234\n",
      "Epoch 7264, Loss: 0.38926080614328384, Final Batch Loss: 0.21992164850234985\n",
      "Epoch 7265, Loss: 0.4162174388766289, Final Batch Loss: 0.2136014699935913\n",
      "Epoch 7266, Loss: 0.2683384642004967, Final Batch Loss: 0.07958081364631653\n",
      "Epoch 7267, Loss: 0.28917209059000015, Final Batch Loss: 0.11907598376274109\n",
      "Epoch 7268, Loss: 0.29239288717508316, Final Batch Loss: 0.11651894450187683\n",
      "Epoch 7269, Loss: 0.23854492604732513, Final Batch Loss: 0.0845789983868599\n",
      "Epoch 7270, Loss: 0.31058766692876816, Final Batch Loss: 0.12944461405277252\n",
      "Epoch 7271, Loss: 0.3300275206565857, Final Batch Loss: 0.0722421407699585\n",
      "Epoch 7272, Loss: 0.3563290387392044, Final Batch Loss: 0.12059676647186279\n",
      "Epoch 7273, Loss: 0.3968633934855461, Final Batch Loss: 0.16702355444431305\n",
      "Epoch 7274, Loss: 0.3164302706718445, Final Batch Loss: 0.08026901632547379\n",
      "Epoch 7275, Loss: 0.3131887838244438, Final Batch Loss: 0.08954959362745285\n",
      "Epoch 7276, Loss: 0.30845871567726135, Final Batch Loss: 0.13019248843193054\n",
      "Epoch 7277, Loss: 0.2925049737095833, Final Batch Loss: 0.0817641019821167\n",
      "Epoch 7278, Loss: 0.37096448987722397, Final Batch Loss: 0.13831399381160736\n",
      "Epoch 7279, Loss: 0.28627530485391617, Final Batch Loss: 0.08861085027456284\n",
      "Epoch 7280, Loss: 0.4967779517173767, Final Batch Loss: 0.1598801463842392\n",
      "Epoch 7281, Loss: 0.31867630779743195, Final Batch Loss: 0.11337994039058685\n",
      "Epoch 7282, Loss: 0.37479884177446365, Final Batch Loss: 0.13697494566440582\n",
      "Epoch 7283, Loss: 0.29374557733535767, Final Batch Loss: 0.10314005613327026\n",
      "Epoch 7284, Loss: 0.2791531905531883, Final Batch Loss: 0.054225243628025055\n",
      "Epoch 7285, Loss: 0.34487195312976837, Final Batch Loss: 0.16584259271621704\n",
      "Epoch 7286, Loss: 0.3438274636864662, Final Batch Loss: 0.09895721077919006\n",
      "Epoch 7287, Loss: 0.2828635200858116, Final Batch Loss: 0.11690295487642288\n",
      "Epoch 7288, Loss: 0.2861279770731926, Final Batch Loss: 0.05182383954524994\n",
      "Epoch 7289, Loss: 0.2783595286309719, Final Batch Loss: 0.12836137413978577\n",
      "Epoch 7290, Loss: 0.3469541668891907, Final Batch Loss: 0.10913082212209702\n",
      "Epoch 7291, Loss: 0.3606621325016022, Final Batch Loss: 0.09583641588687897\n",
      "Epoch 7292, Loss: 0.19881615042686462, Final Batch Loss: 0.04317048192024231\n",
      "Epoch 7293, Loss: 0.3035694435238838, Final Batch Loss: 0.11096753180027008\n",
      "Epoch 7294, Loss: 0.3024693839251995, Final Batch Loss: 0.14492955803871155\n",
      "Epoch 7295, Loss: 0.2654123604297638, Final Batch Loss: 0.09618676453828812\n",
      "Epoch 7296, Loss: 0.36717022210359573, Final Batch Loss: 0.111604243516922\n",
      "Epoch 7297, Loss: 0.29747406393289566, Final Batch Loss: 0.09299349784851074\n",
      "Epoch 7298, Loss: 0.2804530933499336, Final Batch Loss: 0.07437906414270401\n",
      "Epoch 7299, Loss: 0.2816318944096565, Final Batch Loss: 0.09000901877880096\n",
      "Epoch 7300, Loss: 0.34379371628165245, Final Batch Loss: 0.11898764967918396\n",
      "Epoch 7301, Loss: 0.26379335299134254, Final Batch Loss: 0.08509783446788788\n",
      "Epoch 7302, Loss: 0.2640538215637207, Final Batch Loss: 0.09000054746866226\n",
      "Epoch 7303, Loss: 0.3054579570889473, Final Batch Loss: 0.15644320845603943\n",
      "Epoch 7304, Loss: 0.37033241242170334, Final Batch Loss: 0.12635226547718048\n",
      "Epoch 7305, Loss: 0.3160461336374283, Final Batch Loss: 0.11046237498521805\n",
      "Epoch 7306, Loss: 0.3374871239066124, Final Batch Loss: 0.08975858241319656\n",
      "Epoch 7307, Loss: 0.30094827711582184, Final Batch Loss: 0.08431723713874817\n",
      "Epoch 7308, Loss: 0.2983948476612568, Final Batch Loss: 0.11541477590799332\n",
      "Epoch 7309, Loss: 0.3337497338652611, Final Batch Loss: 0.1382516622543335\n",
      "Epoch 7310, Loss: 0.3160177245736122, Final Batch Loss: 0.09349292516708374\n",
      "Epoch 7311, Loss: 0.28433556854724884, Final Batch Loss: 0.0872553288936615\n",
      "Epoch 7312, Loss: 0.2890467345714569, Final Batch Loss: 0.09677333384752274\n",
      "Epoch 7313, Loss: 0.3280852288007736, Final Batch Loss: 0.11971338838338852\n",
      "Epoch 7314, Loss: 0.26252342760562897, Final Batch Loss: 0.06369082629680634\n",
      "Epoch 7315, Loss: 0.35311636328697205, Final Batch Loss: 0.17436891794204712\n",
      "Epoch 7316, Loss: 0.2511681318283081, Final Batch Loss: 0.08541912585496902\n",
      "Epoch 7317, Loss: 0.30189861357212067, Final Batch Loss: 0.09291402250528336\n",
      "Epoch 7318, Loss: 0.29531367123126984, Final Batch Loss: 0.0936468169093132\n",
      "Epoch 7319, Loss: 0.31302981078624725, Final Batch Loss: 0.1729779690504074\n",
      "Epoch 7320, Loss: 0.2643972337245941, Final Batch Loss: 0.07771153748035431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7321, Loss: 0.34024207293987274, Final Batch Loss: 0.08649294823408127\n",
      "Epoch 7322, Loss: 0.3114653378725052, Final Batch Loss: 0.1099485531449318\n",
      "Epoch 7323, Loss: 0.39929115027189255, Final Batch Loss: 0.08841542154550552\n",
      "Epoch 7324, Loss: 0.32475724816322327, Final Batch Loss: 0.09589996188879013\n",
      "Epoch 7325, Loss: 0.24248458817601204, Final Batch Loss: 0.03892165794968605\n",
      "Epoch 7326, Loss: 0.30547139048576355, Final Batch Loss: 0.11467929929494858\n",
      "Epoch 7327, Loss: 0.26603494584560394, Final Batch Loss: 0.0723535344004631\n",
      "Epoch 7328, Loss: 0.28086449950933456, Final Batch Loss: 0.07623746246099472\n",
      "Epoch 7329, Loss: 0.2750852033495903, Final Batch Loss: 0.05061531066894531\n",
      "Epoch 7330, Loss: 0.33668647706508636, Final Batch Loss: 0.12161947786808014\n",
      "Epoch 7331, Loss: 0.26784225553274155, Final Batch Loss: 0.0920214056968689\n",
      "Epoch 7332, Loss: 0.2834034562110901, Final Batch Loss: 0.07728181779384613\n",
      "Epoch 7333, Loss: 0.3217248246073723, Final Batch Loss: 0.14903464913368225\n",
      "Epoch 7334, Loss: 0.4190935716032982, Final Batch Loss: 0.1594342440366745\n",
      "Epoch 7335, Loss: 0.35817573964595795, Final Batch Loss: 0.17490622401237488\n",
      "Epoch 7336, Loss: 0.2527381703257561, Final Batch Loss: 0.08271752297878265\n",
      "Epoch 7337, Loss: 0.28647513687610626, Final Batch Loss: 0.11082007735967636\n",
      "Epoch 7338, Loss: 0.3404553607106209, Final Batch Loss: 0.14121659100055695\n",
      "Epoch 7339, Loss: 0.2771642357110977, Final Batch Loss: 0.07912306487560272\n",
      "Epoch 7340, Loss: 0.2912367209792137, Final Batch Loss: 0.09281981736421585\n",
      "Epoch 7341, Loss: 0.35836388915777206, Final Batch Loss: 0.12878306210041046\n",
      "Epoch 7342, Loss: 0.31808381900191307, Final Batch Loss: 0.055671896785497665\n",
      "Epoch 7343, Loss: 0.29367493093013763, Final Batch Loss: 0.07331881672143936\n",
      "Epoch 7344, Loss: 0.33249185234308243, Final Batch Loss: 0.1646844744682312\n",
      "Epoch 7345, Loss: 0.3228512406349182, Final Batch Loss: 0.1321650743484497\n",
      "Epoch 7346, Loss: 0.2927067019045353, Final Batch Loss: 0.05449124053120613\n",
      "Epoch 7347, Loss: 0.3173760548233986, Final Batch Loss: 0.09762156009674072\n",
      "Epoch 7348, Loss: 0.26966142654418945, Final Batch Loss: 0.06622540950775146\n",
      "Epoch 7349, Loss: 0.37181060016155243, Final Batch Loss: 0.15910960733890533\n",
      "Epoch 7350, Loss: 0.36658935993909836, Final Batch Loss: 0.07214873284101486\n",
      "Epoch 7351, Loss: 0.4656427204608917, Final Batch Loss: 0.13761118054389954\n",
      "Epoch 7352, Loss: 0.3719540983438492, Final Batch Loss: 0.13749010860919952\n",
      "Epoch 7353, Loss: 0.3017103746533394, Final Batch Loss: 0.09714938700199127\n",
      "Epoch 7354, Loss: 0.28510140627622604, Final Batch Loss: 0.11935919523239136\n",
      "Epoch 7355, Loss: 0.2688906788825989, Final Batch Loss: 0.07596918940544128\n",
      "Epoch 7356, Loss: 0.27517085522413254, Final Batch Loss: 0.12387663871049881\n",
      "Epoch 7357, Loss: 0.29970504343509674, Final Batch Loss: 0.07625482976436615\n",
      "Epoch 7358, Loss: 0.3349008895456791, Final Batch Loss: 0.1482369303703308\n",
      "Epoch 7359, Loss: 0.39922797679901123, Final Batch Loss: 0.17786994576454163\n",
      "Epoch 7360, Loss: 0.23803755640983582, Final Batch Loss: 0.11498397588729858\n",
      "Epoch 7361, Loss: 0.27989622950553894, Final Batch Loss: 0.1076141893863678\n",
      "Epoch 7362, Loss: 0.3134893625974655, Final Batch Loss: 0.07702115923166275\n",
      "Epoch 7363, Loss: 0.250503022223711, Final Batch Loss: 0.09653712064027786\n",
      "Epoch 7364, Loss: 0.22330424934625626, Final Batch Loss: 0.07319551706314087\n",
      "Epoch 7365, Loss: 0.3066154196858406, Final Batch Loss: 0.07856270670890808\n",
      "Epoch 7366, Loss: 0.2866651192307472, Final Batch Loss: 0.08769907802343369\n",
      "Epoch 7367, Loss: 0.32047421485185623, Final Batch Loss: 0.13806749880313873\n",
      "Epoch 7368, Loss: 0.35429805517196655, Final Batch Loss: 0.1250755935907364\n",
      "Epoch 7369, Loss: 0.3954725116491318, Final Batch Loss: 0.12035183608531952\n",
      "Epoch 7370, Loss: 0.2780950367450714, Final Batch Loss: 0.08987321704626083\n",
      "Epoch 7371, Loss: 0.41593749821186066, Final Batch Loss: 0.10074186325073242\n",
      "Epoch 7372, Loss: 0.2919171378016472, Final Batch Loss: 0.1080673560500145\n",
      "Epoch 7373, Loss: 0.2974609509110451, Final Batch Loss: 0.0730443000793457\n",
      "Epoch 7374, Loss: 0.39197197556495667, Final Batch Loss: 0.1226053535938263\n",
      "Epoch 7375, Loss: 0.29551034420728683, Final Batch Loss: 0.055794984102249146\n",
      "Epoch 7376, Loss: 0.2733750119805336, Final Batch Loss: 0.08744172006845474\n",
      "Epoch 7377, Loss: 0.27940961718559265, Final Batch Loss: 0.068735770881176\n",
      "Epoch 7378, Loss: 0.3279491290450096, Final Batch Loss: 0.061251938343048096\n",
      "Epoch 7379, Loss: 0.26326528564095497, Final Batch Loss: 0.11361990123987198\n",
      "Epoch 7380, Loss: 0.3340086042881012, Final Batch Loss: 0.13763852417469025\n",
      "Epoch 7381, Loss: 0.35519302636384964, Final Batch Loss: 0.15764012932777405\n",
      "Epoch 7382, Loss: 0.34069424122571945, Final Batch Loss: 0.14789873361587524\n",
      "Epoch 7383, Loss: 0.33242084085941315, Final Batch Loss: 0.1175895407795906\n",
      "Epoch 7384, Loss: 0.31594567745923996, Final Batch Loss: 0.059323497116565704\n",
      "Epoch 7385, Loss: 0.2089419923722744, Final Batch Loss: 0.0411611907184124\n",
      "Epoch 7386, Loss: 0.24085497111082077, Final Batch Loss: 0.09977137297391891\n",
      "Epoch 7387, Loss: 0.2745990604162216, Final Batch Loss: 0.10049924999475479\n",
      "Epoch 7388, Loss: 0.3581251911818981, Final Batch Loss: 0.1411556750535965\n",
      "Epoch 7389, Loss: 0.3690093159675598, Final Batch Loss: 0.1196102425456047\n",
      "Epoch 7390, Loss: 0.3799119219183922, Final Batch Loss: 0.10123662650585175\n",
      "Epoch 7391, Loss: 0.32222506403923035, Final Batch Loss: 0.11633889377117157\n",
      "Epoch 7392, Loss: 0.2888283282518387, Final Batch Loss: 0.11108541488647461\n",
      "Epoch 7393, Loss: 0.26674238219857216, Final Batch Loss: 0.05513713136315346\n",
      "Epoch 7394, Loss: 0.29895471408963203, Final Batch Loss: 0.03693457320332527\n",
      "Epoch 7395, Loss: 0.34238286316394806, Final Batch Loss: 0.115206278860569\n",
      "Epoch 7396, Loss: 0.33791477233171463, Final Batch Loss: 0.10193374007940292\n",
      "Epoch 7397, Loss: 0.38666554540395737, Final Batch Loss: 0.13652846217155457\n",
      "Epoch 7398, Loss: 0.3627130463719368, Final Batch Loss: 0.14051783084869385\n",
      "Epoch 7399, Loss: 0.37429939210414886, Final Batch Loss: 0.1192023754119873\n",
      "Epoch 7400, Loss: 0.3775700628757477, Final Batch Loss: 0.16248196363449097\n",
      "Epoch 7401, Loss: 0.2980620302259922, Final Batch Loss: 0.04759212210774422\n",
      "Epoch 7402, Loss: 0.30485428869724274, Final Batch Loss: 0.13558150827884674\n",
      "Epoch 7403, Loss: 0.24085462093353271, Final Batch Loss: 0.08797445148229599\n",
      "Epoch 7404, Loss: 0.35187115520238876, Final Batch Loss: 0.12692399322986603\n",
      "Epoch 7405, Loss: 0.30142077803611755, Final Batch Loss: 0.13298790156841278\n",
      "Epoch 7406, Loss: 0.33785348385572433, Final Batch Loss: 0.09815538674592972\n",
      "Epoch 7407, Loss: 0.305073119699955, Final Batch Loss: 0.08753477782011032\n",
      "Epoch 7408, Loss: 0.35429490730166435, Final Batch Loss: 0.05462672933936119\n",
      "Epoch 7409, Loss: 0.28729382529854774, Final Batch Loss: 0.11281261593103409\n",
      "Epoch 7410, Loss: 0.34044545143842697, Final Batch Loss: 0.10690532624721527\n",
      "Epoch 7411, Loss: 0.25755010545253754, Final Batch Loss: 0.09006187319755554\n",
      "Epoch 7412, Loss: 0.31362493336200714, Final Batch Loss: 0.11885479092597961\n",
      "Epoch 7413, Loss: 0.35324324667453766, Final Batch Loss: 0.16417737305164337\n",
      "Epoch 7414, Loss: 0.3211008384823799, Final Batch Loss: 0.09214510768651962\n",
      "Epoch 7415, Loss: 0.2666296139359474, Final Batch Loss: 0.07552476227283478\n",
      "Epoch 7416, Loss: 0.2784988060593605, Final Batch Loss: 0.08506480604410172\n",
      "Epoch 7417, Loss: 0.3572463318705559, Final Batch Loss: 0.1136389747262001\n",
      "Epoch 7418, Loss: 0.3277938589453697, Final Batch Loss: 0.0960104763507843\n",
      "Epoch 7419, Loss: 0.3611723855137825, Final Batch Loss: 0.16876263916492462\n",
      "Epoch 7420, Loss: 0.18488622084259987, Final Batch Loss: 0.05658741667866707\n",
      "Epoch 7421, Loss: 0.2793152555823326, Final Batch Loss: 0.08992110192775726\n",
      "Epoch 7422, Loss: 0.24505767971277237, Final Batch Loss: 0.07013306021690369\n",
      "Epoch 7423, Loss: 0.43615203350782394, Final Batch Loss: 0.19920635223388672\n",
      "Epoch 7424, Loss: 0.2629176750779152, Final Batch Loss: 0.09092661738395691\n",
      "Epoch 7425, Loss: 0.34421157091856003, Final Batch Loss: 0.10041245073080063\n",
      "Epoch 7426, Loss: 0.3412066474556923, Final Batch Loss: 0.06499810516834259\n",
      "Epoch 7427, Loss: 0.28558776527643204, Final Batch Loss: 0.07610728591680527\n",
      "Epoch 7428, Loss: 0.292668953537941, Final Batch Loss: 0.11086679250001907\n",
      "Epoch 7429, Loss: 0.2493286281824112, Final Batch Loss: 0.07408429682254791\n",
      "Epoch 7430, Loss: 0.3513251096010208, Final Batch Loss: 0.09462909400463104\n",
      "Epoch 7431, Loss: 0.2552914209663868, Final Batch Loss: 0.08049985766410828\n",
      "Epoch 7432, Loss: 0.31889959424734116, Final Batch Loss: 0.11333058029413223\n",
      "Epoch 7433, Loss: 0.223564513027668, Final Batch Loss: 0.07386677712202072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7434, Loss: 0.2830953896045685, Final Batch Loss: 0.10886414349079132\n",
      "Epoch 7435, Loss: 0.2774876207113266, Final Batch Loss: 0.09843682497739792\n",
      "Epoch 7436, Loss: 0.3221694529056549, Final Batch Loss: 0.1002642884850502\n",
      "Epoch 7437, Loss: 0.24313290417194366, Final Batch Loss: 0.05048880726099014\n",
      "Epoch 7438, Loss: 0.3074406534433365, Final Batch Loss: 0.10516322404146194\n",
      "Epoch 7439, Loss: 0.31838933378458023, Final Batch Loss: 0.12998926639556885\n",
      "Epoch 7440, Loss: 0.4217112511396408, Final Batch Loss: 0.1342920958995819\n",
      "Epoch 7441, Loss: 0.24615608528256416, Final Batch Loss: 0.05086180195212364\n",
      "Epoch 7442, Loss: 0.32266274094581604, Final Batch Loss: 0.14218147099018097\n",
      "Epoch 7443, Loss: 0.28768355771899223, Final Batch Loss: 0.056799713522195816\n",
      "Epoch 7444, Loss: 0.34042640775442123, Final Batch Loss: 0.10658755898475647\n",
      "Epoch 7445, Loss: 0.22563040629029274, Final Batch Loss: 0.05184543505311012\n",
      "Epoch 7446, Loss: 0.3621576875448227, Final Batch Loss: 0.16445042192935944\n",
      "Epoch 7447, Loss: 0.2850673198699951, Final Batch Loss: 0.08224937319755554\n",
      "Epoch 7448, Loss: 0.28800168633461, Final Batch Loss: 0.12076316028833389\n",
      "Epoch 7449, Loss: 0.3502695858478546, Final Batch Loss: 0.08195723593235016\n",
      "Epoch 7450, Loss: 0.36327747255563736, Final Batch Loss: 0.10099232196807861\n",
      "Epoch 7451, Loss: 0.42477428913116455, Final Batch Loss: 0.18151605129241943\n",
      "Epoch 7452, Loss: 0.3093572035431862, Final Batch Loss: 0.058173537254333496\n",
      "Epoch 7453, Loss: 0.37731532752513885, Final Batch Loss: 0.19875039160251617\n",
      "Epoch 7454, Loss: 0.31660937517881393, Final Batch Loss: 0.08886656165122986\n",
      "Epoch 7455, Loss: 0.2805417627096176, Final Batch Loss: 0.08029914647340775\n",
      "Epoch 7456, Loss: 0.2874462157487869, Final Batch Loss: 0.07852631062269211\n",
      "Epoch 7457, Loss: 0.25855306535959244, Final Batch Loss: 0.06900908052921295\n",
      "Epoch 7458, Loss: 0.2333233468234539, Final Batch Loss: 0.04359797015786171\n",
      "Epoch 7459, Loss: 0.24224205315113068, Final Batch Loss: 0.08816442638635635\n",
      "Epoch 7460, Loss: 0.3021637089550495, Final Batch Loss: 0.05095728859305382\n",
      "Epoch 7461, Loss: 0.34317782521247864, Final Batch Loss: 0.13528819382190704\n",
      "Epoch 7462, Loss: 0.3131180629134178, Final Batch Loss: 0.10220886766910553\n",
      "Epoch 7463, Loss: 0.3384939581155777, Final Batch Loss: 0.14946500957012177\n",
      "Epoch 7464, Loss: 0.23472749814391136, Final Batch Loss: 0.032061610370874405\n",
      "Epoch 7465, Loss: 0.33375047892332077, Final Batch Loss: 0.09316276013851166\n",
      "Epoch 7466, Loss: 0.39576125144958496, Final Batch Loss: 0.1471724808216095\n",
      "Epoch 7467, Loss: 0.28579341620206833, Final Batch Loss: 0.09087818115949631\n",
      "Epoch 7468, Loss: 0.19454095512628555, Final Batch Loss: 0.03492943197488785\n",
      "Epoch 7469, Loss: 0.21631668880581856, Final Batch Loss: 0.07811851799488068\n",
      "Epoch 7470, Loss: 0.375650092959404, Final Batch Loss: 0.1666976362466812\n",
      "Epoch 7471, Loss: 0.2754952311515808, Final Batch Loss: 0.08836029469966888\n",
      "Epoch 7472, Loss: 0.27537135034799576, Final Batch Loss: 0.08170505613088608\n",
      "Epoch 7473, Loss: 0.3601393550634384, Final Batch Loss: 0.17931713163852692\n",
      "Epoch 7474, Loss: 0.2702776566147804, Final Batch Loss: 0.09226547181606293\n",
      "Epoch 7475, Loss: 0.3146926313638687, Final Batch Loss: 0.15400977432727814\n",
      "Epoch 7476, Loss: 0.31796128302812576, Final Batch Loss: 0.12082157284021378\n",
      "Epoch 7477, Loss: 0.2900402918457985, Final Batch Loss: 0.09971503913402557\n",
      "Epoch 7478, Loss: 0.2374635636806488, Final Batch Loss: 0.09464462846517563\n",
      "Epoch 7479, Loss: 0.2538125067949295, Final Batch Loss: 0.05044744163751602\n",
      "Epoch 7480, Loss: 0.29786067456007004, Final Batch Loss: 0.14655886590480804\n",
      "Epoch 7481, Loss: 0.2247392237186432, Final Batch Loss: 0.08520179986953735\n",
      "Epoch 7482, Loss: 0.29074450582265854, Final Batch Loss: 0.12864895164966583\n",
      "Epoch 7483, Loss: 0.2863430380821228, Final Batch Loss: 0.10814271122217178\n",
      "Epoch 7484, Loss: 0.21572646126151085, Final Batch Loss: 0.05484633520245552\n",
      "Epoch 7485, Loss: 0.3948376476764679, Final Batch Loss: 0.1952112466096878\n",
      "Epoch 7486, Loss: 0.3068751096725464, Final Batch Loss: 0.14131537079811096\n",
      "Epoch 7487, Loss: 0.3559322729706764, Final Batch Loss: 0.1487278789281845\n",
      "Epoch 7488, Loss: 0.3280392810702324, Final Batch Loss: 0.11467574536800385\n",
      "Epoch 7489, Loss: 0.330917090177536, Final Batch Loss: 0.12308573722839355\n",
      "Epoch 7490, Loss: 0.2851682975888252, Final Batch Loss: 0.09754029661417007\n",
      "Epoch 7491, Loss: 0.36874503642320633, Final Batch Loss: 0.11960016936063766\n",
      "Epoch 7492, Loss: 0.2506445161998272, Final Batch Loss: 0.05436001345515251\n",
      "Epoch 7493, Loss: 0.3110245130956173, Final Batch Loss: 0.048216868191957474\n",
      "Epoch 7494, Loss: 0.2679620012640953, Final Batch Loss: 0.08747368305921555\n",
      "Epoch 7495, Loss: 0.3951202556490898, Final Batch Loss: 0.17742347717285156\n",
      "Epoch 7496, Loss: 0.3789495825767517, Final Batch Loss: 0.13315775990486145\n",
      "Epoch 7497, Loss: 0.2529129385948181, Final Batch Loss: 0.10614604502916336\n",
      "Epoch 7498, Loss: 0.36220231652259827, Final Batch Loss: 0.1322258710861206\n",
      "Epoch 7499, Loss: 0.2258089743554592, Final Batch Loss: 0.07895626127719879\n",
      "Epoch 7500, Loss: 0.26991454139351845, Final Batch Loss: 0.13728706538677216\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24  0  0  0  0]\n",
      " [ 0 27  0  0  0]\n",
      " [ 0  0 26  1  0]\n",
      " [ 0  0  2 20  0]\n",
      " [ 0  1  1  0 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        24\n",
      "           1      0.964     1.000     0.982        27\n",
      "           2      0.897     0.963     0.929        27\n",
      "           3      0.952     0.909     0.930        22\n",
      "           4      1.000     0.889     0.941        18\n",
      "\n",
      "    accuracy                          0.958       118\n",
      "   macro avg      0.963     0.952     0.956       118\n",
      "weighted avg      0.959     0.958     0.958       118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'../../../saved_models/UCI 5 User Classifier Ablation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
