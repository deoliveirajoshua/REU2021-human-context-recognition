{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '141 tBodyGyro-iqr()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '434 fBodyGyro-max()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 35),\n",
    "            classifier_block(35, 30),\n",
    "            classifier_block(30, 25),\n",
    "            classifier_block(25, 25),\n",
    "            nn.Linear(25, 27)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "    \n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_10 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_11 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_12 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_13 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_14 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_15 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_16 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_17 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_18 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_19 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_20 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_21 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_22 = GAN_data[(GAN_data['Subject'] == 17) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_23 = GAN_data[(GAN_data['Subject'] == 17) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_24 = GAN_data[(GAN_data['Subject'] == 17) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_25 = GAN_data[(GAN_data['Subject'] == 19) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_26 = GAN_data[(GAN_data['Subject'] == 19) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_27 = GAN_data[(GAN_data['Subject'] == 19) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9, X_10, X_11, X_12, X_13, X_14, X_15, X_16, X_17, X_18, X_19, X_20, X_21, X_22, X_23, X_24, X_25, X_26, X_27))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9) + [9] * len(X_10) + [10] * len(X_11) + [11] * len(X_12) + [12] * len(X_13) + [13] * len(X_14) + [14] * len(X_15) + [15] * len(X_16) + [16] * len(X_17) + [17] * len(X_18) + [18] * len(X_19) + [19] * len(X_20) + [20] * len(X_21) + [21] * len(X_22) + [22] * len(X_23) + [23] * len(X_24) + [24] * len(X_25) + [25] * len(X_26) + [26] * len(X_27)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5, 7, 8, 11, 14, 17, 19]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 16.51865029335022, Final Batch Loss: 3.3084299564361572\n",
      "Epoch 2, Loss: 16.499032497406006, Final Batch Loss: 3.3034024238586426\n",
      "Epoch 3, Loss: 16.481985569000244, Final Batch Loss: 3.3024635314941406\n",
      "Epoch 4, Loss: 16.457619667053223, Final Batch Loss: 3.2869317531585693\n",
      "Epoch 5, Loss: 16.4397292137146, Final Batch Loss: 3.2896595001220703\n",
      "Epoch 6, Loss: 16.418330192565918, Final Batch Loss: 3.2882168292999268\n",
      "Epoch 7, Loss: 16.394782066345215, Final Batch Loss: 3.284086227416992\n",
      "Epoch 8, Loss: 16.360390424728394, Final Batch Loss: 3.2761383056640625\n",
      "Epoch 9, Loss: 16.29083752632141, Final Batch Loss: 3.2474937438964844\n",
      "Epoch 10, Loss: 16.219391584396362, Final Batch Loss: 3.2442996501922607\n",
      "Epoch 11, Loss: 16.08209538459778, Final Batch Loss: 3.202280282974243\n",
      "Epoch 12, Loss: 15.87822151184082, Final Batch Loss: 3.130915403366089\n",
      "Epoch 13, Loss: 15.633298397064209, Final Batch Loss: 3.0895514488220215\n",
      "Epoch 14, Loss: 15.258543968200684, Final Batch Loss: 3.039942979812622\n",
      "Epoch 15, Loss: 14.685226440429688, Final Batch Loss: 2.852900743484497\n",
      "Epoch 16, Loss: 14.160707473754883, Final Batch Loss: 2.7715282440185547\n",
      "Epoch 17, Loss: 13.51527190208435, Final Batch Loss: 2.6386563777923584\n",
      "Epoch 18, Loss: 12.776530027389526, Final Batch Loss: 2.469977378845215\n",
      "Epoch 19, Loss: 12.253190755844116, Final Batch Loss: 2.4036264419555664\n",
      "Epoch 20, Loss: 11.970293283462524, Final Batch Loss: 2.4059810638427734\n",
      "Epoch 21, Loss: 11.485756874084473, Final Batch Loss: 2.3050155639648438\n",
      "Epoch 22, Loss: 11.2425537109375, Final Batch Loss: 2.251526117324829\n",
      "Epoch 23, Loss: 10.987876176834106, Final Batch Loss: 2.1262001991271973\n",
      "Epoch 24, Loss: 10.802254915237427, Final Batch Loss: 2.195160388946533\n",
      "Epoch 25, Loss: 10.658272504806519, Final Batch Loss: 2.056809186935425\n",
      "Epoch 26, Loss: 10.52451753616333, Final Batch Loss: 2.046349048614502\n",
      "Epoch 27, Loss: 10.37169599533081, Final Batch Loss: 2.1423654556274414\n",
      "Epoch 28, Loss: 10.244442701339722, Final Batch Loss: 2.0203826427459717\n",
      "Epoch 29, Loss: 10.0981285572052, Final Batch Loss: 2.069460153579712\n",
      "Epoch 30, Loss: 9.833356022834778, Final Batch Loss: 1.9128081798553467\n",
      "Epoch 31, Loss: 9.799630403518677, Final Batch Loss: 1.9697644710540771\n",
      "Epoch 32, Loss: 9.705752849578857, Final Batch Loss: 1.8851065635681152\n",
      "Epoch 33, Loss: 9.661605834960938, Final Batch Loss: 1.9080290794372559\n",
      "Epoch 34, Loss: 9.445686936378479, Final Batch Loss: 1.90061354637146\n",
      "Epoch 35, Loss: 9.387084245681763, Final Batch Loss: 1.95892333984375\n",
      "Epoch 36, Loss: 9.166685342788696, Final Batch Loss: 1.8213844299316406\n",
      "Epoch 37, Loss: 9.351366877555847, Final Batch Loss: 1.8975588083267212\n",
      "Epoch 38, Loss: 9.187711119651794, Final Batch Loss: 1.8300929069519043\n",
      "Epoch 39, Loss: 9.094681024551392, Final Batch Loss: 1.9190311431884766\n",
      "Epoch 40, Loss: 8.963361024856567, Final Batch Loss: 1.7870362997055054\n",
      "Epoch 41, Loss: 8.81200122833252, Final Batch Loss: 1.6646766662597656\n",
      "Epoch 42, Loss: 8.875231623649597, Final Batch Loss: 1.7819173336029053\n",
      "Epoch 43, Loss: 8.82930874824524, Final Batch Loss: 1.7958247661590576\n",
      "Epoch 44, Loss: 8.726901054382324, Final Batch Loss: 1.8058608770370483\n",
      "Epoch 45, Loss: 8.645387649536133, Final Batch Loss: 1.672182321548462\n",
      "Epoch 46, Loss: 8.459277629852295, Final Batch Loss: 1.7420048713684082\n",
      "Epoch 47, Loss: 8.517892718315125, Final Batch Loss: 1.729275107383728\n",
      "Epoch 48, Loss: 8.436171054840088, Final Batch Loss: 1.5685210227966309\n",
      "Epoch 49, Loss: 8.370599746704102, Final Batch Loss: 1.623401165008545\n",
      "Epoch 50, Loss: 8.203656077384949, Final Batch Loss: 1.6403696537017822\n",
      "Epoch 51, Loss: 8.53562080860138, Final Batch Loss: 1.7540969848632812\n",
      "Epoch 52, Loss: 8.227167129516602, Final Batch Loss: 1.6122194528579712\n",
      "Epoch 53, Loss: 8.095927715301514, Final Batch Loss: 1.4782700538635254\n",
      "Epoch 54, Loss: 7.885881304740906, Final Batch Loss: 1.5833733081817627\n",
      "Epoch 55, Loss: 8.011420369148254, Final Batch Loss: 1.6169301271438599\n",
      "Epoch 56, Loss: 7.9827752113342285, Final Batch Loss: 1.632448434829712\n",
      "Epoch 57, Loss: 8.032391548156738, Final Batch Loss: 1.6226296424865723\n",
      "Epoch 58, Loss: 7.819499611854553, Final Batch Loss: 1.5296176671981812\n",
      "Epoch 59, Loss: 7.816276431083679, Final Batch Loss: 1.5606409311294556\n",
      "Epoch 60, Loss: 7.811862587928772, Final Batch Loss: 1.4656299352645874\n",
      "Epoch 61, Loss: 7.679388046264648, Final Batch Loss: 1.488769292831421\n",
      "Epoch 62, Loss: 7.71358585357666, Final Batch Loss: 1.436133861541748\n",
      "Epoch 63, Loss: 7.716830372810364, Final Batch Loss: 1.5399800539016724\n",
      "Epoch 64, Loss: 7.665408492088318, Final Batch Loss: 1.6638028621673584\n",
      "Epoch 65, Loss: 7.5229960680007935, Final Batch Loss: 1.5992908477783203\n",
      "Epoch 66, Loss: 7.651493549346924, Final Batch Loss: 1.661356806755066\n",
      "Epoch 67, Loss: 7.594279766082764, Final Batch Loss: 1.5147840976715088\n",
      "Epoch 68, Loss: 7.47909688949585, Final Batch Loss: 1.598158359527588\n",
      "Epoch 69, Loss: 7.557691931724548, Final Batch Loss: 1.5469468832015991\n",
      "Epoch 70, Loss: 7.564711928367615, Final Batch Loss: 1.5519282817840576\n",
      "Epoch 71, Loss: 7.352200388908386, Final Batch Loss: 1.3772556781768799\n",
      "Epoch 72, Loss: 7.3797115087509155, Final Batch Loss: 1.5146805047988892\n",
      "Epoch 73, Loss: 7.34407114982605, Final Batch Loss: 1.4935603141784668\n",
      "Epoch 74, Loss: 7.457049250602722, Final Batch Loss: 1.51872718334198\n",
      "Epoch 75, Loss: 7.33772087097168, Final Batch Loss: 1.4058215618133545\n",
      "Epoch 76, Loss: 7.286990404129028, Final Batch Loss: 1.5555351972579956\n",
      "Epoch 77, Loss: 7.215381383895874, Final Batch Loss: 1.3699448108673096\n",
      "Epoch 78, Loss: 7.133425354957581, Final Batch Loss: 1.4599533081054688\n",
      "Epoch 79, Loss: 7.202944397926331, Final Batch Loss: 1.4061394929885864\n",
      "Epoch 80, Loss: 7.3259429931640625, Final Batch Loss: 1.482113003730774\n",
      "Epoch 81, Loss: 7.091087579727173, Final Batch Loss: 1.3262114524841309\n",
      "Epoch 82, Loss: 7.0830559730529785, Final Batch Loss: 1.3455029726028442\n",
      "Epoch 83, Loss: 7.175360918045044, Final Batch Loss: 1.423538088798523\n",
      "Epoch 84, Loss: 7.098158836364746, Final Batch Loss: 1.516715407371521\n",
      "Epoch 85, Loss: 7.069985270500183, Final Batch Loss: 1.346441388130188\n",
      "Epoch 86, Loss: 7.056685566902161, Final Batch Loss: 1.3722983598709106\n",
      "Epoch 87, Loss: 7.013073086738586, Final Batch Loss: 1.437677264213562\n",
      "Epoch 88, Loss: 7.004415988922119, Final Batch Loss: 1.4170355796813965\n",
      "Epoch 89, Loss: 6.96561336517334, Final Batch Loss: 1.4539506435394287\n",
      "Epoch 90, Loss: 6.995378375053406, Final Batch Loss: 1.416916012763977\n",
      "Epoch 91, Loss: 6.873140335083008, Final Batch Loss: 1.415880799293518\n",
      "Epoch 92, Loss: 6.929581642150879, Final Batch Loss: 1.3264087438583374\n",
      "Epoch 93, Loss: 6.607247233390808, Final Batch Loss: 1.214312195777893\n",
      "Epoch 94, Loss: 6.993557691574097, Final Batch Loss: 1.4631158113479614\n",
      "Epoch 95, Loss: 6.825879335403442, Final Batch Loss: 1.297293782234192\n",
      "Epoch 96, Loss: 6.820074200630188, Final Batch Loss: 1.4675461053848267\n",
      "Epoch 97, Loss: 6.752301096916199, Final Batch Loss: 1.3794093132019043\n",
      "Epoch 98, Loss: 6.792430400848389, Final Batch Loss: 1.3234578371047974\n",
      "Epoch 99, Loss: 6.77043616771698, Final Batch Loss: 1.271949052810669\n",
      "Epoch 100, Loss: 6.678849816322327, Final Batch Loss: 1.2350549697875977\n",
      "Epoch 101, Loss: 6.678738832473755, Final Batch Loss: 1.400369644165039\n",
      "Epoch 102, Loss: 6.574724316596985, Final Batch Loss: 1.3805707693099976\n",
      "Epoch 103, Loss: 6.584299683570862, Final Batch Loss: 1.3999584913253784\n",
      "Epoch 104, Loss: 6.454308032989502, Final Batch Loss: 1.2351804971694946\n",
      "Epoch 105, Loss: 6.537357211112976, Final Batch Loss: 1.3451145887374878\n",
      "Epoch 106, Loss: 6.616872429847717, Final Batch Loss: 1.413135290145874\n",
      "Epoch 107, Loss: 6.500185012817383, Final Batch Loss: 1.3214311599731445\n",
      "Epoch 108, Loss: 6.640215754508972, Final Batch Loss: 1.3858461380004883\n",
      "Epoch 109, Loss: 6.558869004249573, Final Batch Loss: 1.2922463417053223\n",
      "Epoch 110, Loss: 6.634012222290039, Final Batch Loss: 1.397947907447815\n",
      "Epoch 111, Loss: 6.416613340377808, Final Batch Loss: 1.3468729257583618\n",
      "Epoch 112, Loss: 6.59363853931427, Final Batch Loss: 1.3840785026550293\n",
      "Epoch 113, Loss: 6.458372950553894, Final Batch Loss: 1.2752991914749146\n",
      "Epoch 114, Loss: 6.401658773422241, Final Batch Loss: 1.3138514757156372\n",
      "Epoch 115, Loss: 6.443922162055969, Final Batch Loss: 1.280680537223816\n",
      "Epoch 116, Loss: 6.467693328857422, Final Batch Loss: 1.2178266048431396\n",
      "Epoch 117, Loss: 6.4178712368011475, Final Batch Loss: 1.285427451133728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118, Loss: 6.324379801750183, Final Batch Loss: 1.2336806058883667\n",
      "Epoch 119, Loss: 6.383435964584351, Final Batch Loss: 1.2514631748199463\n",
      "Epoch 120, Loss: 6.453417539596558, Final Batch Loss: 1.350162386894226\n",
      "Epoch 121, Loss: 6.345674157142639, Final Batch Loss: 1.2155007123947144\n",
      "Epoch 122, Loss: 6.466821312904358, Final Batch Loss: 1.3336788415908813\n",
      "Epoch 123, Loss: 6.472315549850464, Final Batch Loss: 1.3754163980484009\n",
      "Epoch 124, Loss: 6.381137490272522, Final Batch Loss: 1.2379134893417358\n",
      "Epoch 125, Loss: 6.23167085647583, Final Batch Loss: 1.2013087272644043\n",
      "Epoch 126, Loss: 6.210725426673889, Final Batch Loss: 1.1850301027297974\n",
      "Epoch 127, Loss: 6.221542716026306, Final Batch Loss: 1.3040125370025635\n",
      "Epoch 128, Loss: 6.090165138244629, Final Batch Loss: 1.2332496643066406\n",
      "Epoch 129, Loss: 6.296495795249939, Final Batch Loss: 1.2611626386642456\n",
      "Epoch 130, Loss: 6.379622220993042, Final Batch Loss: 1.3722459077835083\n",
      "Epoch 131, Loss: 6.1735217571258545, Final Batch Loss: 1.2759054899215698\n",
      "Epoch 132, Loss: 6.223761796951294, Final Batch Loss: 1.2207707166671753\n",
      "Epoch 133, Loss: 6.271099090576172, Final Batch Loss: 1.1597256660461426\n",
      "Epoch 134, Loss: 6.099055171012878, Final Batch Loss: 1.0868215560913086\n",
      "Epoch 135, Loss: 6.008387565612793, Final Batch Loss: 1.1659846305847168\n",
      "Epoch 136, Loss: 6.0837050676345825, Final Batch Loss: 1.2690078020095825\n",
      "Epoch 137, Loss: 6.263389229774475, Final Batch Loss: 1.3215745687484741\n",
      "Epoch 138, Loss: 6.167163014411926, Final Batch Loss: 1.2118982076644897\n",
      "Epoch 139, Loss: 6.074447393417358, Final Batch Loss: 1.2913907766342163\n",
      "Epoch 140, Loss: 6.044777750968933, Final Batch Loss: 1.126139760017395\n",
      "Epoch 141, Loss: 6.01349937915802, Final Batch Loss: 1.2454116344451904\n",
      "Epoch 142, Loss: 6.152225732803345, Final Batch Loss: 1.2639546394348145\n",
      "Epoch 143, Loss: 5.943198561668396, Final Batch Loss: 1.1894519329071045\n",
      "Epoch 144, Loss: 6.195953011512756, Final Batch Loss: 1.3102900981903076\n",
      "Epoch 145, Loss: 6.002030372619629, Final Batch Loss: 1.1510050296783447\n",
      "Epoch 146, Loss: 6.050258040428162, Final Batch Loss: 1.193752408027649\n",
      "Epoch 147, Loss: 5.919585347175598, Final Batch Loss: 1.18916654586792\n",
      "Epoch 148, Loss: 6.1970391273498535, Final Batch Loss: 1.2895910739898682\n",
      "Epoch 149, Loss: 5.985635280609131, Final Batch Loss: 1.225313663482666\n",
      "Epoch 150, Loss: 5.9461294412612915, Final Batch Loss: 1.172076940536499\n",
      "Epoch 151, Loss: 5.933444976806641, Final Batch Loss: 1.1048375368118286\n",
      "Epoch 152, Loss: 5.948277950286865, Final Batch Loss: 1.197556734085083\n",
      "Epoch 153, Loss: 5.959127187728882, Final Batch Loss: 1.1910699605941772\n",
      "Epoch 154, Loss: 6.034781455993652, Final Batch Loss: 1.286896824836731\n",
      "Epoch 155, Loss: 5.97382116317749, Final Batch Loss: 1.2737716436386108\n",
      "Epoch 156, Loss: 5.883687257766724, Final Batch Loss: 1.152168869972229\n",
      "Epoch 157, Loss: 5.881354093551636, Final Batch Loss: 1.171127200126648\n",
      "Epoch 158, Loss: 6.005175352096558, Final Batch Loss: 1.2863718271255493\n",
      "Epoch 159, Loss: 5.830021858215332, Final Batch Loss: 1.2148367166519165\n",
      "Epoch 160, Loss: 5.932626247406006, Final Batch Loss: 1.1586500406265259\n",
      "Epoch 161, Loss: 5.763453960418701, Final Batch Loss: 1.1687697172164917\n",
      "Epoch 162, Loss: 5.78096878528595, Final Batch Loss: 1.150004506111145\n",
      "Epoch 163, Loss: 5.798323750495911, Final Batch Loss: 1.0708725452423096\n",
      "Epoch 164, Loss: 5.841307997703552, Final Batch Loss: 1.2042078971862793\n",
      "Epoch 165, Loss: 5.71504807472229, Final Batch Loss: 1.1857573986053467\n",
      "Epoch 166, Loss: 5.861480712890625, Final Batch Loss: 1.1198471784591675\n",
      "Epoch 167, Loss: 5.731978535652161, Final Batch Loss: 1.1493992805480957\n",
      "Epoch 168, Loss: 5.886350035667419, Final Batch Loss: 1.2174997329711914\n",
      "Epoch 169, Loss: 5.6307960748672485, Final Batch Loss: 1.0824707746505737\n",
      "Epoch 170, Loss: 5.830762624740601, Final Batch Loss: 1.2337141036987305\n",
      "Epoch 171, Loss: 5.856865286827087, Final Batch Loss: 1.1496268510818481\n",
      "Epoch 172, Loss: 5.613818407058716, Final Batch Loss: 1.0376060009002686\n",
      "Epoch 173, Loss: 5.753682971000671, Final Batch Loss: 1.1417739391326904\n",
      "Epoch 174, Loss: 5.800531983375549, Final Batch Loss: 1.2688878774642944\n",
      "Epoch 175, Loss: 5.654022932052612, Final Batch Loss: 1.1514098644256592\n",
      "Epoch 176, Loss: 5.7776782512664795, Final Batch Loss: 1.0860626697540283\n",
      "Epoch 177, Loss: 5.70986533164978, Final Batch Loss: 1.0908136367797852\n",
      "Epoch 178, Loss: 5.850077867507935, Final Batch Loss: 1.2558542490005493\n",
      "Epoch 179, Loss: 5.758994817733765, Final Batch Loss: 1.1846792697906494\n",
      "Epoch 180, Loss: 5.663741707801819, Final Batch Loss: 1.110655426979065\n",
      "Epoch 181, Loss: 5.560497045516968, Final Batch Loss: 1.145464301109314\n",
      "Epoch 182, Loss: 5.4573721289634705, Final Batch Loss: 0.9424368739128113\n",
      "Epoch 183, Loss: 5.634698271751404, Final Batch Loss: 1.0286246538162231\n",
      "Epoch 184, Loss: 5.566286087036133, Final Batch Loss: 1.0720884799957275\n",
      "Epoch 185, Loss: 5.600621700286865, Final Batch Loss: 1.0499110221862793\n",
      "Epoch 186, Loss: 5.640293717384338, Final Batch Loss: 1.198860764503479\n",
      "Epoch 187, Loss: 5.762703895568848, Final Batch Loss: 1.1735659837722778\n",
      "Epoch 188, Loss: 5.548735737800598, Final Batch Loss: 1.1361384391784668\n",
      "Epoch 189, Loss: 5.534869909286499, Final Batch Loss: 1.1403706073760986\n",
      "Epoch 190, Loss: 5.400594353675842, Final Batch Loss: 1.0434269905090332\n",
      "Epoch 191, Loss: 5.5003814697265625, Final Batch Loss: 1.0785609483718872\n",
      "Epoch 192, Loss: 5.680776476860046, Final Batch Loss: 1.185775876045227\n",
      "Epoch 193, Loss: 5.575169563293457, Final Batch Loss: 1.166334629058838\n",
      "Epoch 194, Loss: 5.50370180606842, Final Batch Loss: 1.0621404647827148\n",
      "Epoch 195, Loss: 5.4271721839904785, Final Batch Loss: 1.1100976467132568\n",
      "Epoch 196, Loss: 5.439209461212158, Final Batch Loss: 1.070128083229065\n",
      "Epoch 197, Loss: 5.580047965049744, Final Batch Loss: 1.1866497993469238\n",
      "Epoch 198, Loss: 5.498754262924194, Final Batch Loss: 1.0649031400680542\n",
      "Epoch 199, Loss: 5.407423973083496, Final Batch Loss: 1.0520967245101929\n",
      "Epoch 200, Loss: 5.483031630516052, Final Batch Loss: 1.049791932106018\n",
      "Epoch 201, Loss: 5.5426048040390015, Final Batch Loss: 1.1994622945785522\n",
      "Epoch 202, Loss: 5.364989638328552, Final Batch Loss: 1.0073431730270386\n",
      "Epoch 203, Loss: 5.416794717311859, Final Batch Loss: 1.0852138996124268\n",
      "Epoch 204, Loss: 5.399290144443512, Final Batch Loss: 1.0852854251861572\n",
      "Epoch 205, Loss: 5.396665334701538, Final Batch Loss: 1.1218979358673096\n",
      "Epoch 206, Loss: 5.3215362429618835, Final Batch Loss: 0.9745199084281921\n",
      "Epoch 207, Loss: 5.280327796936035, Final Batch Loss: 0.8976050019264221\n",
      "Epoch 208, Loss: 5.410863816738129, Final Batch Loss: 0.9728290438652039\n",
      "Epoch 209, Loss: 5.367684602737427, Final Batch Loss: 1.105830192565918\n",
      "Epoch 210, Loss: 5.413374304771423, Final Batch Loss: 1.000749111175537\n",
      "Epoch 211, Loss: 5.271836459636688, Final Batch Loss: 1.0706701278686523\n",
      "Epoch 212, Loss: 5.372727990150452, Final Batch Loss: 1.1806219816207886\n",
      "Epoch 213, Loss: 5.369349539279938, Final Batch Loss: 1.0287119150161743\n",
      "Epoch 214, Loss: 5.400021910667419, Final Batch Loss: 1.1358951330184937\n",
      "Epoch 215, Loss: 5.378408789634705, Final Batch Loss: 1.0906106233596802\n",
      "Epoch 216, Loss: 5.353019714355469, Final Batch Loss: 1.1015958786010742\n",
      "Epoch 217, Loss: 5.298113107681274, Final Batch Loss: 1.0406562089920044\n",
      "Epoch 218, Loss: 5.291593253612518, Final Batch Loss: 0.955619752407074\n",
      "Epoch 219, Loss: 5.249289035797119, Final Batch Loss: 1.1061172485351562\n",
      "Epoch 220, Loss: 5.519397020339966, Final Batch Loss: 1.0378015041351318\n",
      "Epoch 221, Loss: 5.320087552070618, Final Batch Loss: 1.0876483917236328\n",
      "Epoch 222, Loss: 5.438624024391174, Final Batch Loss: 1.0366432666778564\n",
      "Epoch 223, Loss: 5.2978954911231995, Final Batch Loss: 0.9785670638084412\n",
      "Epoch 224, Loss: 5.213747203350067, Final Batch Loss: 0.945271909236908\n",
      "Epoch 225, Loss: 5.216911554336548, Final Batch Loss: 1.0628914833068848\n",
      "Epoch 226, Loss: 5.209874629974365, Final Batch Loss: 1.0353856086730957\n",
      "Epoch 227, Loss: 5.314967513084412, Final Batch Loss: 1.1317447423934937\n",
      "Epoch 228, Loss: 5.344144940376282, Final Batch Loss: 1.1346508264541626\n",
      "Epoch 229, Loss: 5.3167542815208435, Final Batch Loss: 1.1097686290740967\n",
      "Epoch 230, Loss: 5.3854674100875854, Final Batch Loss: 1.0380947589874268\n",
      "Epoch 231, Loss: 5.196036219596863, Final Batch Loss: 1.005828619003296\n",
      "Epoch 232, Loss: 5.203657031059265, Final Batch Loss: 1.0371323823928833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233, Loss: 5.2325217723846436, Final Batch Loss: 1.0627408027648926\n",
      "Epoch 234, Loss: 5.325114846229553, Final Batch Loss: 1.014003038406372\n",
      "Epoch 235, Loss: 5.253874480724335, Final Batch Loss: 0.9781785607337952\n",
      "Epoch 236, Loss: 5.213246524333954, Final Batch Loss: 1.105910062789917\n",
      "Epoch 237, Loss: 5.087533533573151, Final Batch Loss: 0.9966485500335693\n",
      "Epoch 238, Loss: 5.155000686645508, Final Batch Loss: 1.0377845764160156\n",
      "Epoch 239, Loss: 5.376210331916809, Final Batch Loss: 1.1086232662200928\n",
      "Epoch 240, Loss: 5.202962815761566, Final Batch Loss: 0.9037516713142395\n",
      "Epoch 241, Loss: 5.253943622112274, Final Batch Loss: 1.205979824066162\n",
      "Epoch 242, Loss: 5.1993032693862915, Final Batch Loss: 1.0824140310287476\n",
      "Epoch 243, Loss: 5.117581069469452, Final Batch Loss: 0.9635845422744751\n",
      "Epoch 244, Loss: 5.277454257011414, Final Batch Loss: 1.067566990852356\n",
      "Epoch 245, Loss: 5.067261815071106, Final Batch Loss: 0.9252457022666931\n",
      "Epoch 246, Loss: 5.128319680690765, Final Batch Loss: 1.0253453254699707\n",
      "Epoch 247, Loss: 5.0385255217552185, Final Batch Loss: 0.9359438419342041\n",
      "Epoch 248, Loss: 5.255947470664978, Final Batch Loss: 1.0704152584075928\n",
      "Epoch 249, Loss: 5.1814863085746765, Final Batch Loss: 1.0839471817016602\n",
      "Epoch 250, Loss: 5.057518720626831, Final Batch Loss: 0.9975419640541077\n",
      "Epoch 251, Loss: 5.145411252975464, Final Batch Loss: 1.0427653789520264\n",
      "Epoch 252, Loss: 5.0548394322395325, Final Batch Loss: 1.0848674774169922\n",
      "Epoch 253, Loss: 5.146904289722443, Final Batch Loss: 1.070297360420227\n",
      "Epoch 254, Loss: 5.0892903208732605, Final Batch Loss: 1.0177412033081055\n",
      "Epoch 255, Loss: 5.0181527733802795, Final Batch Loss: 0.9804396033287048\n",
      "Epoch 256, Loss: 5.123623549938202, Final Batch Loss: 1.0646988153457642\n",
      "Epoch 257, Loss: 5.070366442203522, Final Batch Loss: 1.0467427968978882\n",
      "Epoch 258, Loss: 5.016772270202637, Final Batch Loss: 1.0002795457839966\n",
      "Epoch 259, Loss: 5.05543977022171, Final Batch Loss: 1.0329434871673584\n",
      "Epoch 260, Loss: 5.021972298622131, Final Batch Loss: 1.0090899467468262\n",
      "Epoch 261, Loss: 5.033589243888855, Final Batch Loss: 0.975098192691803\n",
      "Epoch 262, Loss: 5.105383217334747, Final Batch Loss: 0.9800916910171509\n",
      "Epoch 263, Loss: 4.989427983760834, Final Batch Loss: 0.9236138463020325\n",
      "Epoch 264, Loss: 5.0743356347084045, Final Batch Loss: 1.0280176401138306\n",
      "Epoch 265, Loss: 5.158481478691101, Final Batch Loss: 1.0145641565322876\n",
      "Epoch 266, Loss: 5.009156584739685, Final Batch Loss: 0.9890226125717163\n",
      "Epoch 267, Loss: 5.110107243061066, Final Batch Loss: 1.1255338191986084\n",
      "Epoch 268, Loss: 4.8936654925346375, Final Batch Loss: 0.9140905737876892\n",
      "Epoch 269, Loss: 5.004812121391296, Final Batch Loss: 1.0312013626098633\n",
      "Epoch 270, Loss: 5.20853579044342, Final Batch Loss: 1.1177618503570557\n",
      "Epoch 271, Loss: 4.902569472789764, Final Batch Loss: 0.9358773231506348\n",
      "Epoch 272, Loss: 4.915240228176117, Final Batch Loss: 0.9970117807388306\n",
      "Epoch 273, Loss: 4.8784539103508, Final Batch Loss: 0.8677845597267151\n",
      "Epoch 274, Loss: 4.923154175281525, Final Batch Loss: 1.089755654335022\n",
      "Epoch 275, Loss: 4.898714601993561, Final Batch Loss: 1.042638897895813\n",
      "Epoch 276, Loss: 4.83173269033432, Final Batch Loss: 1.0065878629684448\n",
      "Epoch 277, Loss: 4.962054193019867, Final Batch Loss: 1.0141468048095703\n",
      "Epoch 278, Loss: 4.945205092430115, Final Batch Loss: 1.0450910329818726\n",
      "Epoch 279, Loss: 5.046680569648743, Final Batch Loss: 0.9688862562179565\n",
      "Epoch 280, Loss: 5.0253618359565735, Final Batch Loss: 1.0153107643127441\n",
      "Epoch 281, Loss: 5.0230095982551575, Final Batch Loss: 1.1250096559524536\n",
      "Epoch 282, Loss: 4.8872416615486145, Final Batch Loss: 0.9160850644111633\n",
      "Epoch 283, Loss: 4.906470477581024, Final Batch Loss: 0.9422675967216492\n",
      "Epoch 284, Loss: 4.97179239988327, Final Batch Loss: 0.98670893907547\n",
      "Epoch 285, Loss: 4.924998700618744, Final Batch Loss: 1.0976835489273071\n",
      "Epoch 286, Loss: 4.941406726837158, Final Batch Loss: 1.0348490476608276\n",
      "Epoch 287, Loss: 4.87512332201004, Final Batch Loss: 0.940043568611145\n",
      "Epoch 288, Loss: 5.022854328155518, Final Batch Loss: 1.065933108329773\n",
      "Epoch 289, Loss: 4.968662977218628, Final Batch Loss: 1.1051146984100342\n",
      "Epoch 290, Loss: 4.948712646961212, Final Batch Loss: 1.0341954231262207\n",
      "Epoch 291, Loss: 4.998890042304993, Final Batch Loss: 1.0310386419296265\n",
      "Epoch 292, Loss: 5.054018914699554, Final Batch Loss: 1.015243411064148\n",
      "Epoch 293, Loss: 4.85318386554718, Final Batch Loss: 1.0239890813827515\n",
      "Epoch 294, Loss: 4.758135437965393, Final Batch Loss: 0.8779900670051575\n",
      "Epoch 295, Loss: 4.907764196395874, Final Batch Loss: 1.0354269742965698\n",
      "Epoch 296, Loss: 4.816738426685333, Final Batch Loss: 1.007716178894043\n",
      "Epoch 297, Loss: 4.878072619438171, Final Batch Loss: 1.0143320560455322\n",
      "Epoch 298, Loss: 4.898643255233765, Final Batch Loss: 1.0157711505889893\n",
      "Epoch 299, Loss: 4.909677624702454, Final Batch Loss: 0.832319438457489\n",
      "Epoch 300, Loss: 5.067452251911163, Final Batch Loss: 1.176910638809204\n",
      "Epoch 301, Loss: 4.807215213775635, Final Batch Loss: 0.9742289781570435\n",
      "Epoch 302, Loss: 4.911893308162689, Final Batch Loss: 1.018343448638916\n",
      "Epoch 303, Loss: 4.763708770275116, Final Batch Loss: 0.8868780732154846\n",
      "Epoch 304, Loss: 5.016452372074127, Final Batch Loss: 1.2171612977981567\n",
      "Epoch 305, Loss: 4.943983793258667, Final Batch Loss: 1.0329290628433228\n",
      "Epoch 306, Loss: 4.9798866510391235, Final Batch Loss: 1.0233045816421509\n",
      "Epoch 307, Loss: 4.888241469860077, Final Batch Loss: 0.9988828301429749\n",
      "Epoch 308, Loss: 4.719463467597961, Final Batch Loss: 0.9478464126586914\n",
      "Epoch 309, Loss: 4.703160643577576, Final Batch Loss: 0.9479483962059021\n",
      "Epoch 310, Loss: 4.860915303230286, Final Batch Loss: 0.9953018426895142\n",
      "Epoch 311, Loss: 4.7194876074790955, Final Batch Loss: 0.9377688765525818\n",
      "Epoch 312, Loss: 4.718523025512695, Final Batch Loss: 0.9673666954040527\n",
      "Epoch 313, Loss: 4.918686032295227, Final Batch Loss: 1.0583235025405884\n",
      "Epoch 314, Loss: 4.729822397232056, Final Batch Loss: 0.9220679998397827\n",
      "Epoch 315, Loss: 4.7813573479652405, Final Batch Loss: 0.916330873966217\n",
      "Epoch 316, Loss: 4.72666209936142, Final Batch Loss: 0.79320228099823\n",
      "Epoch 317, Loss: 4.654750108718872, Final Batch Loss: 0.8700316548347473\n",
      "Epoch 318, Loss: 4.677576184272766, Final Batch Loss: 0.9453219771385193\n",
      "Epoch 319, Loss: 4.825464069843292, Final Batch Loss: 0.9779787063598633\n",
      "Epoch 320, Loss: 4.894177198410034, Final Batch Loss: 1.123012900352478\n",
      "Epoch 321, Loss: 4.896325707435608, Final Batch Loss: 1.0748589038848877\n",
      "Epoch 322, Loss: 4.71485036611557, Final Batch Loss: 0.9040665030479431\n",
      "Epoch 323, Loss: 4.952823102474213, Final Batch Loss: 1.0423952341079712\n",
      "Epoch 324, Loss: 4.730249404907227, Final Batch Loss: 0.9136981964111328\n",
      "Epoch 325, Loss: 4.651910722255707, Final Batch Loss: 0.9334504008293152\n",
      "Epoch 326, Loss: 4.612527370452881, Final Batch Loss: 0.8366819024085999\n",
      "Epoch 327, Loss: 4.70098614692688, Final Batch Loss: 0.9871536493301392\n",
      "Epoch 328, Loss: 4.870463192462921, Final Batch Loss: 1.0555379390716553\n",
      "Epoch 329, Loss: 4.769031643867493, Final Batch Loss: 1.0643399953842163\n",
      "Epoch 330, Loss: 4.604490518569946, Final Batch Loss: 0.9355277419090271\n",
      "Epoch 331, Loss: 4.7873281836509705, Final Batch Loss: 0.861798107624054\n",
      "Epoch 332, Loss: 4.617803990840912, Final Batch Loss: 0.8471941947937012\n",
      "Epoch 333, Loss: 4.654478967189789, Final Batch Loss: 0.9055027365684509\n",
      "Epoch 334, Loss: 4.633081912994385, Final Batch Loss: 0.9221439957618713\n",
      "Epoch 335, Loss: 4.7044899463653564, Final Batch Loss: 1.0299988985061646\n",
      "Epoch 336, Loss: 4.609663963317871, Final Batch Loss: 0.8211091756820679\n",
      "Epoch 337, Loss: 4.671435832977295, Final Batch Loss: 1.0491008758544922\n",
      "Epoch 338, Loss: 4.681955814361572, Final Batch Loss: 1.0384265184402466\n",
      "Epoch 339, Loss: 4.485829949378967, Final Batch Loss: 0.7814230918884277\n",
      "Epoch 340, Loss: 4.711654245853424, Final Batch Loss: 0.8711465001106262\n",
      "Epoch 341, Loss: 4.752936780452728, Final Batch Loss: 0.8804025650024414\n",
      "Epoch 342, Loss: 4.651949644088745, Final Batch Loss: 0.9112839102745056\n",
      "Epoch 343, Loss: 4.562174141407013, Final Batch Loss: 0.9087286591529846\n",
      "Epoch 344, Loss: 4.687757790088654, Final Batch Loss: 1.0245921611785889\n",
      "Epoch 345, Loss: 4.527636706829071, Final Batch Loss: 0.9751402735710144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 346, Loss: 4.593384265899658, Final Batch Loss: 0.9878965020179749\n",
      "Epoch 347, Loss: 4.73816978931427, Final Batch Loss: 0.9912282824516296\n",
      "Epoch 348, Loss: 4.672610282897949, Final Batch Loss: 0.9195464253425598\n",
      "Epoch 349, Loss: 4.720049142837524, Final Batch Loss: 0.941234290599823\n",
      "Epoch 350, Loss: 4.65991336107254, Final Batch Loss: 0.9518075585365295\n",
      "Epoch 351, Loss: 4.590609073638916, Final Batch Loss: 0.9984705448150635\n",
      "Epoch 352, Loss: 4.633797585964203, Final Batch Loss: 0.9922706484794617\n",
      "Epoch 353, Loss: 4.574462890625, Final Batch Loss: 0.9740756154060364\n",
      "Epoch 354, Loss: 4.625207245349884, Final Batch Loss: 0.9405415654182434\n",
      "Epoch 355, Loss: 4.587603390216827, Final Batch Loss: 0.8843066096305847\n",
      "Epoch 356, Loss: 4.6506956815719604, Final Batch Loss: 1.03427255153656\n",
      "Epoch 357, Loss: 4.5676658153533936, Final Batch Loss: 0.9082335233688354\n",
      "Epoch 358, Loss: 4.661130607128143, Final Batch Loss: 0.9000326991081238\n",
      "Epoch 359, Loss: 4.499461531639099, Final Batch Loss: 0.9593616127967834\n",
      "Epoch 360, Loss: 4.792378902435303, Final Batch Loss: 0.9506695866584778\n",
      "Epoch 361, Loss: 4.568712413311005, Final Batch Loss: 0.8095365166664124\n",
      "Epoch 362, Loss: 4.5056867599487305, Final Batch Loss: 0.7305055856704712\n",
      "Epoch 363, Loss: 4.46900200843811, Final Batch Loss: 0.8347212672233582\n",
      "Epoch 364, Loss: 4.463054060935974, Final Batch Loss: 0.8533874750137329\n",
      "Epoch 365, Loss: 4.699148237705231, Final Batch Loss: 1.0505530834197998\n",
      "Epoch 366, Loss: 4.606651961803436, Final Batch Loss: 0.9948363304138184\n",
      "Epoch 367, Loss: 4.66039365530014, Final Batch Loss: 0.991456151008606\n",
      "Epoch 368, Loss: 4.3914971351623535, Final Batch Loss: 0.89857017993927\n",
      "Epoch 369, Loss: 4.444411754608154, Final Batch Loss: 0.8719698786735535\n",
      "Epoch 370, Loss: 4.562335431575775, Final Batch Loss: 1.0069912672042847\n",
      "Epoch 371, Loss: 4.448082447052002, Final Batch Loss: 0.773973822593689\n",
      "Epoch 372, Loss: 4.577609658241272, Final Batch Loss: 0.88300621509552\n",
      "Epoch 373, Loss: 4.4381988644599915, Final Batch Loss: 0.8983656167984009\n",
      "Epoch 374, Loss: 4.623757898807526, Final Batch Loss: 1.0439798831939697\n",
      "Epoch 375, Loss: 4.571818470954895, Final Batch Loss: 0.9233229160308838\n",
      "Epoch 376, Loss: 4.412954330444336, Final Batch Loss: 0.7584545016288757\n",
      "Epoch 377, Loss: 4.475214600563049, Final Batch Loss: 0.8564258217811584\n",
      "Epoch 378, Loss: 4.379298388957977, Final Batch Loss: 0.9151723980903625\n",
      "Epoch 379, Loss: 4.525096774101257, Final Batch Loss: 0.865326464176178\n",
      "Epoch 380, Loss: 4.574181318283081, Final Batch Loss: 1.0936129093170166\n",
      "Epoch 381, Loss: 4.56621378660202, Final Batch Loss: 0.9887216687202454\n",
      "Epoch 382, Loss: 4.483274042606354, Final Batch Loss: 0.8669676780700684\n",
      "Epoch 383, Loss: 4.471434772014618, Final Batch Loss: 0.9012908935546875\n",
      "Epoch 384, Loss: 4.4668161273002625, Final Batch Loss: 0.9578558802604675\n",
      "Epoch 385, Loss: 4.409971058368683, Final Batch Loss: 0.7586844563484192\n",
      "Epoch 386, Loss: 4.3417059779167175, Final Batch Loss: 0.7824726104736328\n",
      "Epoch 387, Loss: 4.398244082927704, Final Batch Loss: 0.9617800712585449\n",
      "Epoch 388, Loss: 4.429081976413727, Final Batch Loss: 0.8879354596138\n",
      "Epoch 389, Loss: 4.435914993286133, Final Batch Loss: 0.8607013821601868\n",
      "Epoch 390, Loss: 4.673343658447266, Final Batch Loss: 1.076429009437561\n",
      "Epoch 391, Loss: 4.375143468379974, Final Batch Loss: 0.8443690538406372\n",
      "Epoch 392, Loss: 4.636174261569977, Final Batch Loss: 1.072779655456543\n",
      "Epoch 393, Loss: 4.395094096660614, Final Batch Loss: 0.9380893111228943\n",
      "Epoch 394, Loss: 4.4167545437812805, Final Batch Loss: 0.9382248520851135\n",
      "Epoch 395, Loss: 4.370899021625519, Final Batch Loss: 0.955070972442627\n",
      "Epoch 396, Loss: 4.398640394210815, Final Batch Loss: 0.9242652654647827\n",
      "Epoch 397, Loss: 4.4703075885772705, Final Batch Loss: 0.9208589792251587\n",
      "Epoch 398, Loss: 4.376205205917358, Final Batch Loss: 0.8732531666755676\n",
      "Epoch 399, Loss: 4.437746047973633, Final Batch Loss: 0.8182413578033447\n",
      "Epoch 400, Loss: 4.391361057758331, Final Batch Loss: 0.8120455145835876\n",
      "Epoch 401, Loss: 4.264086663722992, Final Batch Loss: 1.0065993070602417\n",
      "Epoch 402, Loss: 4.451637089252472, Final Batch Loss: 0.9235846996307373\n",
      "Epoch 403, Loss: 4.403164565563202, Final Batch Loss: 0.773093581199646\n",
      "Epoch 404, Loss: 4.396880507469177, Final Batch Loss: 0.8961951732635498\n",
      "Epoch 405, Loss: 4.526700377464294, Final Batch Loss: 0.958355188369751\n",
      "Epoch 406, Loss: 4.377634644508362, Final Batch Loss: 0.8830429315567017\n",
      "Epoch 407, Loss: 4.377604305744171, Final Batch Loss: 0.8086637854576111\n",
      "Epoch 408, Loss: 4.477235674858093, Final Batch Loss: 0.9500952363014221\n",
      "Epoch 409, Loss: 4.3490835428237915, Final Batch Loss: 0.9275151491165161\n",
      "Epoch 410, Loss: 4.473756909370422, Final Batch Loss: 0.9453060626983643\n",
      "Epoch 411, Loss: 4.504035413265228, Final Batch Loss: 0.9074309468269348\n",
      "Epoch 412, Loss: 4.264274418354034, Final Batch Loss: 0.9474034905433655\n",
      "Epoch 413, Loss: 4.294699549674988, Final Batch Loss: 0.8589375615119934\n",
      "Epoch 414, Loss: 4.292460858821869, Final Batch Loss: 0.8198149800300598\n",
      "Epoch 415, Loss: 4.518161594867706, Final Batch Loss: 0.9325554370880127\n",
      "Epoch 416, Loss: 4.425391614437103, Final Batch Loss: 0.9339108467102051\n",
      "Epoch 417, Loss: 4.24132764339447, Final Batch Loss: 0.7789233326911926\n",
      "Epoch 418, Loss: 4.4533586502075195, Final Batch Loss: 0.9352176785469055\n",
      "Epoch 419, Loss: 4.426853716373444, Final Batch Loss: 0.8647786378860474\n",
      "Epoch 420, Loss: 4.395870804786682, Final Batch Loss: 0.8367617130279541\n",
      "Epoch 421, Loss: 4.410676896572113, Final Batch Loss: 0.9563811421394348\n",
      "Epoch 422, Loss: 4.228501975536346, Final Batch Loss: 0.8116471767425537\n",
      "Epoch 423, Loss: 4.456131756305695, Final Batch Loss: 0.8327080607414246\n",
      "Epoch 424, Loss: 4.396663963794708, Final Batch Loss: 0.9246774315834045\n",
      "Epoch 425, Loss: 4.37647157907486, Final Batch Loss: 0.9033864140510559\n",
      "Epoch 426, Loss: 4.2629353404045105, Final Batch Loss: 0.7888035178184509\n",
      "Epoch 427, Loss: 4.243676722049713, Final Batch Loss: 0.8305863738059998\n",
      "Epoch 428, Loss: 4.3529375195503235, Final Batch Loss: 0.9587413668632507\n",
      "Epoch 429, Loss: 4.420758485794067, Final Batch Loss: 0.889614462852478\n",
      "Epoch 430, Loss: 4.450423061847687, Final Batch Loss: 0.9653358459472656\n",
      "Epoch 431, Loss: 4.171905755996704, Final Batch Loss: 0.8047375679016113\n",
      "Epoch 432, Loss: 4.2764288783073425, Final Batch Loss: 0.918655514717102\n",
      "Epoch 433, Loss: 4.320689260959625, Final Batch Loss: 0.928145706653595\n",
      "Epoch 434, Loss: 4.421260952949524, Final Batch Loss: 0.8065701127052307\n",
      "Epoch 435, Loss: 4.226679980754852, Final Batch Loss: 0.8925297260284424\n",
      "Epoch 436, Loss: 4.125153958797455, Final Batch Loss: 0.7416994571685791\n",
      "Epoch 437, Loss: 4.337567627429962, Final Batch Loss: 0.8665233254432678\n",
      "Epoch 438, Loss: 4.261017799377441, Final Batch Loss: 0.7712903618812561\n",
      "Epoch 439, Loss: 4.063735544681549, Final Batch Loss: 0.8014649152755737\n",
      "Epoch 440, Loss: 4.313360929489136, Final Batch Loss: 0.8780986666679382\n",
      "Epoch 441, Loss: 4.256966710090637, Final Batch Loss: 0.9099701046943665\n",
      "Epoch 442, Loss: 4.324848115444183, Final Batch Loss: 0.8346496820449829\n",
      "Epoch 443, Loss: 4.224382221698761, Final Batch Loss: 0.7466772794723511\n",
      "Epoch 444, Loss: 4.237906396389008, Final Batch Loss: 0.9707226157188416\n",
      "Epoch 445, Loss: 4.338363766670227, Final Batch Loss: 0.8535512089729309\n",
      "Epoch 446, Loss: 4.373565435409546, Final Batch Loss: 0.9161431789398193\n",
      "Epoch 447, Loss: 4.192739248275757, Final Batch Loss: 0.8792132139205933\n",
      "Epoch 448, Loss: 4.216351091861725, Final Batch Loss: 0.8848358988761902\n",
      "Epoch 449, Loss: 4.351380825042725, Final Batch Loss: 0.8452924489974976\n",
      "Epoch 450, Loss: 4.2970499992370605, Final Batch Loss: 0.8598635196685791\n",
      "Epoch 451, Loss: 4.151575207710266, Final Batch Loss: 0.835731565952301\n",
      "Epoch 452, Loss: 4.072441816329956, Final Batch Loss: 0.7728829383850098\n",
      "Epoch 453, Loss: 4.161394417285919, Final Batch Loss: 0.8625034093856812\n",
      "Epoch 454, Loss: 4.291359841823578, Final Batch Loss: 0.8479175567626953\n",
      "Epoch 455, Loss: 4.2317163944244385, Final Batch Loss: 0.8013037443161011\n",
      "Epoch 456, Loss: 4.464016973972321, Final Batch Loss: 0.9024843573570251\n",
      "Epoch 457, Loss: 4.163305461406708, Final Batch Loss: 0.7926821708679199\n",
      "Epoch 458, Loss: 4.07828813791275, Final Batch Loss: 0.7661929130554199\n",
      "Epoch 459, Loss: 4.205883622169495, Final Batch Loss: 0.883797287940979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 460, Loss: 4.105958044528961, Final Batch Loss: 0.8430819511413574\n",
      "Epoch 461, Loss: 4.181272029876709, Final Batch Loss: 0.8585652112960815\n",
      "Epoch 462, Loss: 4.119334638118744, Final Batch Loss: 0.7842534780502319\n",
      "Epoch 463, Loss: 4.044599652290344, Final Batch Loss: 0.8109779357910156\n",
      "Epoch 464, Loss: 4.209657192230225, Final Batch Loss: 0.8759328126907349\n",
      "Epoch 465, Loss: 4.2595720291137695, Final Batch Loss: 0.8836892247200012\n",
      "Epoch 466, Loss: 4.088912665843964, Final Batch Loss: 0.76793372631073\n",
      "Epoch 467, Loss: 4.166687726974487, Final Batch Loss: 0.9545195698738098\n",
      "Epoch 468, Loss: 4.1547539830207825, Final Batch Loss: 0.8440369963645935\n",
      "Epoch 469, Loss: 4.07789146900177, Final Batch Loss: 0.9059070348739624\n",
      "Epoch 470, Loss: 4.141560614109039, Final Batch Loss: 0.925796627998352\n",
      "Epoch 471, Loss: 3.999624729156494, Final Batch Loss: 0.77281254529953\n",
      "Epoch 472, Loss: 4.073522627353668, Final Batch Loss: 0.8424702882766724\n",
      "Epoch 473, Loss: 4.179626703262329, Final Batch Loss: 0.893688440322876\n",
      "Epoch 474, Loss: 4.1441041231155396, Final Batch Loss: 0.783632755279541\n",
      "Epoch 475, Loss: 4.103470861911774, Final Batch Loss: 0.7512086033821106\n",
      "Epoch 476, Loss: 4.003822326660156, Final Batch Loss: 0.8092741370201111\n",
      "Epoch 477, Loss: 4.050652205944061, Final Batch Loss: 0.9071150422096252\n",
      "Epoch 478, Loss: 4.065708816051483, Final Batch Loss: 0.6800285577774048\n",
      "Epoch 479, Loss: 4.200407445430756, Final Batch Loss: 0.829227089881897\n",
      "Epoch 480, Loss: 4.1180331110954285, Final Batch Loss: 0.928684651851654\n",
      "Epoch 481, Loss: 4.107168257236481, Final Batch Loss: 0.8931955099105835\n",
      "Epoch 482, Loss: 4.15780782699585, Final Batch Loss: 0.87933349609375\n",
      "Epoch 483, Loss: 4.039002060890198, Final Batch Loss: 0.7707609534263611\n",
      "Epoch 484, Loss: 4.027796506881714, Final Batch Loss: 0.6179262399673462\n",
      "Epoch 485, Loss: 3.90618097782135, Final Batch Loss: 0.7669296264648438\n",
      "Epoch 486, Loss: 3.9871014952659607, Final Batch Loss: 0.9255825877189636\n",
      "Epoch 487, Loss: 4.14621114730835, Final Batch Loss: 0.874667227268219\n",
      "Epoch 488, Loss: 4.000738441944122, Final Batch Loss: 0.7882029414176941\n",
      "Epoch 489, Loss: 4.127138793468475, Final Batch Loss: 0.7585910558700562\n",
      "Epoch 490, Loss: 3.9935858249664307, Final Batch Loss: 0.7619900107383728\n",
      "Epoch 491, Loss: 3.9453080892562866, Final Batch Loss: 0.7895954847335815\n",
      "Epoch 492, Loss: 4.112864971160889, Final Batch Loss: 0.8010886311531067\n",
      "Epoch 493, Loss: 4.1027180552482605, Final Batch Loss: 0.8266329765319824\n",
      "Epoch 494, Loss: 4.12547755241394, Final Batch Loss: 0.7785255908966064\n",
      "Epoch 495, Loss: 4.008418500423431, Final Batch Loss: 0.8464340567588806\n",
      "Epoch 496, Loss: 3.9727025032043457, Final Batch Loss: 0.7864718437194824\n",
      "Epoch 497, Loss: 4.032740831375122, Final Batch Loss: 0.8500373959541321\n",
      "Epoch 498, Loss: 4.144431889057159, Final Batch Loss: 0.8480370044708252\n",
      "Epoch 499, Loss: 4.037743270397186, Final Batch Loss: 0.7575280070304871\n",
      "Epoch 500, Loss: 4.081579148769379, Final Batch Loss: 0.8374150991439819\n",
      "Epoch 501, Loss: 4.06735098361969, Final Batch Loss: 0.8148690462112427\n",
      "Epoch 502, Loss: 4.0375120639801025, Final Batch Loss: 0.7162115573883057\n",
      "Epoch 503, Loss: 3.934128165245056, Final Batch Loss: 0.7749854326248169\n",
      "Epoch 504, Loss: 3.971682071685791, Final Batch Loss: 0.7299675345420837\n",
      "Epoch 505, Loss: 3.9707719683647156, Final Batch Loss: 0.8331527709960938\n",
      "Epoch 506, Loss: 4.01520174741745, Final Batch Loss: 0.8777725100517273\n",
      "Epoch 507, Loss: 4.025282263755798, Final Batch Loss: 0.8244088292121887\n",
      "Epoch 508, Loss: 3.949471175670624, Final Batch Loss: 0.7325568795204163\n",
      "Epoch 509, Loss: 3.9631367921829224, Final Batch Loss: 0.833389937877655\n",
      "Epoch 510, Loss: 3.919900953769684, Final Batch Loss: 0.7671026587486267\n",
      "Epoch 511, Loss: 3.983461081981659, Final Batch Loss: 0.8177693486213684\n",
      "Epoch 512, Loss: 3.945349395275116, Final Batch Loss: 0.7611517310142517\n",
      "Epoch 513, Loss: 4.02946662902832, Final Batch Loss: 0.8604522943496704\n",
      "Epoch 514, Loss: 4.099429070949554, Final Batch Loss: 0.9274699091911316\n",
      "Epoch 515, Loss: 4.007391273975372, Final Batch Loss: 0.8036426901817322\n",
      "Epoch 516, Loss: 3.8088765740394592, Final Batch Loss: 0.6145320534706116\n",
      "Epoch 517, Loss: 3.9451985955238342, Final Batch Loss: 0.8160906434059143\n",
      "Epoch 518, Loss: 3.9661478996276855, Final Batch Loss: 0.8184080123901367\n",
      "Epoch 519, Loss: 3.900988221168518, Final Batch Loss: 0.7499671578407288\n",
      "Epoch 520, Loss: 3.8823368549346924, Final Batch Loss: 0.8223793506622314\n",
      "Epoch 521, Loss: 3.8480533957481384, Final Batch Loss: 0.7628791332244873\n",
      "Epoch 522, Loss: 4.144733905792236, Final Batch Loss: 0.96702641248703\n",
      "Epoch 523, Loss: 3.980585515499115, Final Batch Loss: 0.7658300399780273\n",
      "Epoch 524, Loss: 3.9856098890304565, Final Batch Loss: 0.8196903467178345\n",
      "Epoch 525, Loss: 3.981554687023163, Final Batch Loss: 0.8341807126998901\n",
      "Epoch 526, Loss: 4.0703917145729065, Final Batch Loss: 0.8688279986381531\n",
      "Epoch 527, Loss: 3.957117736339569, Final Batch Loss: 0.7679728865623474\n",
      "Epoch 528, Loss: 3.9143314361572266, Final Batch Loss: 0.8618180155754089\n",
      "Epoch 529, Loss: 3.810712993144989, Final Batch Loss: 0.7343173027038574\n",
      "Epoch 530, Loss: 3.788019359111786, Final Batch Loss: 0.7529661655426025\n",
      "Epoch 531, Loss: 3.843676269054413, Final Batch Loss: 0.7981411218643188\n",
      "Epoch 532, Loss: 3.991147994995117, Final Batch Loss: 0.7827604413032532\n",
      "Epoch 533, Loss: 3.883690595626831, Final Batch Loss: 0.7207520008087158\n",
      "Epoch 534, Loss: 3.8730342984199524, Final Batch Loss: 0.8162674903869629\n",
      "Epoch 535, Loss: 3.8721256852149963, Final Batch Loss: 0.8079138994216919\n",
      "Epoch 536, Loss: 3.9089934825897217, Final Batch Loss: 0.7182345390319824\n",
      "Epoch 537, Loss: 3.841567039489746, Final Batch Loss: 0.7121475338935852\n",
      "Epoch 538, Loss: 3.7071605920791626, Final Batch Loss: 0.7528606653213501\n",
      "Epoch 539, Loss: 3.888693153858185, Final Batch Loss: 0.6878114342689514\n",
      "Epoch 540, Loss: 3.8134827613830566, Final Batch Loss: 0.752399742603302\n",
      "Epoch 541, Loss: 3.859814465045929, Final Batch Loss: 0.7367000579833984\n",
      "Epoch 542, Loss: 3.827687621116638, Final Batch Loss: 0.8108716011047363\n",
      "Epoch 543, Loss: 3.8219939470291138, Final Batch Loss: 0.7770141959190369\n",
      "Epoch 544, Loss: 3.821771740913391, Final Batch Loss: 0.7913458347320557\n",
      "Epoch 545, Loss: 3.8668206930160522, Final Batch Loss: 0.792360246181488\n",
      "Epoch 546, Loss: 3.8689324259757996, Final Batch Loss: 0.7795265913009644\n",
      "Epoch 547, Loss: 3.7189345955848694, Final Batch Loss: 0.7845345735549927\n",
      "Epoch 548, Loss: 3.746143877506256, Final Batch Loss: 0.7717798352241516\n",
      "Epoch 549, Loss: 3.905577838420868, Final Batch Loss: 0.8775755763053894\n",
      "Epoch 550, Loss: 3.888574481010437, Final Batch Loss: 0.764295220375061\n",
      "Epoch 551, Loss: 3.895192503929138, Final Batch Loss: 0.8431503176689148\n",
      "Epoch 552, Loss: 3.755199909210205, Final Batch Loss: 0.6980326771736145\n",
      "Epoch 553, Loss: 3.7497279047966003, Final Batch Loss: 0.7830734848976135\n",
      "Epoch 554, Loss: 3.732623338699341, Final Batch Loss: 0.8186036944389343\n",
      "Epoch 555, Loss: 3.86784565448761, Final Batch Loss: 0.7654129862785339\n",
      "Epoch 556, Loss: 3.950450122356415, Final Batch Loss: 0.9167625904083252\n",
      "Epoch 557, Loss: 3.7352156043052673, Final Batch Loss: 0.7789216637611389\n",
      "Epoch 558, Loss: 3.7536269426345825, Final Batch Loss: 0.733142077922821\n",
      "Epoch 559, Loss: 3.890110969543457, Final Batch Loss: 0.7182139158248901\n",
      "Epoch 560, Loss: 3.734815299510956, Final Batch Loss: 0.687488853931427\n",
      "Epoch 561, Loss: 3.888702154159546, Final Batch Loss: 0.758572518825531\n",
      "Epoch 562, Loss: 3.7449418902397156, Final Batch Loss: 0.7233496308326721\n",
      "Epoch 563, Loss: 3.8282077312469482, Final Batch Loss: 0.7624837160110474\n",
      "Epoch 564, Loss: 3.7143245935440063, Final Batch Loss: 0.8081098794937134\n",
      "Epoch 565, Loss: 3.8573076128959656, Final Batch Loss: 0.8443412184715271\n",
      "Epoch 566, Loss: 3.7481801509857178, Final Batch Loss: 0.8683029413223267\n",
      "Epoch 567, Loss: 3.8233830332756042, Final Batch Loss: 0.7305588722229004\n",
      "Epoch 568, Loss: 3.740676164627075, Final Batch Loss: 0.8254342079162598\n",
      "Epoch 569, Loss: 3.7167199850082397, Final Batch Loss: 0.6488304734230042\n",
      "Epoch 570, Loss: 3.826919376850128, Final Batch Loss: 0.8912250399589539\n",
      "Epoch 571, Loss: 3.6440019607543945, Final Batch Loss: 0.6659391522407532\n",
      "Epoch 572, Loss: 3.7239511013031006, Final Batch Loss: 0.7834055423736572\n",
      "Epoch 573, Loss: 3.6094672679901123, Final Batch Loss: 0.6754084825515747\n",
      "Epoch 574, Loss: 3.7012899518013, Final Batch Loss: 0.7629885673522949\n",
      "Epoch 575, Loss: 3.828301250934601, Final Batch Loss: 0.7564355134963989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 576, Loss: 3.9891406297683716, Final Batch Loss: 0.8388732075691223\n",
      "Epoch 577, Loss: 3.8215097188949585, Final Batch Loss: 0.8473060727119446\n",
      "Epoch 578, Loss: 3.801194727420807, Final Batch Loss: 0.7762963175773621\n",
      "Epoch 579, Loss: 3.612856447696686, Final Batch Loss: 0.8170294761657715\n",
      "Epoch 580, Loss: 3.6810083389282227, Final Batch Loss: 0.6411733031272888\n",
      "Epoch 581, Loss: 3.9209068417549133, Final Batch Loss: 0.7536606192588806\n",
      "Epoch 582, Loss: 3.6552850008010864, Final Batch Loss: 0.6932148933410645\n",
      "Epoch 583, Loss: 3.7426857948303223, Final Batch Loss: 0.8032821416854858\n",
      "Epoch 584, Loss: 3.6388655304908752, Final Batch Loss: 0.6944725513458252\n",
      "Epoch 585, Loss: 3.7115538120269775, Final Batch Loss: 0.686207115650177\n",
      "Epoch 586, Loss: 3.7720797061920166, Final Batch Loss: 0.7204811573028564\n",
      "Epoch 587, Loss: 3.7027573585510254, Final Batch Loss: 0.6409943103790283\n",
      "Epoch 588, Loss: 3.6815685033798218, Final Batch Loss: 0.7140889167785645\n",
      "Epoch 589, Loss: 3.6264621019363403, Final Batch Loss: 0.6148684024810791\n",
      "Epoch 590, Loss: 3.7362293004989624, Final Batch Loss: 0.8250046372413635\n",
      "Epoch 591, Loss: 3.7664710879325867, Final Batch Loss: 0.7285630106925964\n",
      "Epoch 592, Loss: 3.7128360271453857, Final Batch Loss: 0.7505641579627991\n",
      "Epoch 593, Loss: 3.6749200224876404, Final Batch Loss: 0.7547618746757507\n",
      "Epoch 594, Loss: 3.864513397216797, Final Batch Loss: 0.8621594309806824\n",
      "Epoch 595, Loss: 3.6383659839630127, Final Batch Loss: 0.7301117181777954\n",
      "Epoch 596, Loss: 3.679730713367462, Final Batch Loss: 0.7626219391822815\n",
      "Epoch 597, Loss: 3.7067543268203735, Final Batch Loss: 0.7992220520973206\n",
      "Epoch 598, Loss: 3.734949231147766, Final Batch Loss: 0.7512804865837097\n",
      "Epoch 599, Loss: 3.555259048938751, Final Batch Loss: 0.7297917008399963\n",
      "Epoch 600, Loss: 3.5518970489501953, Final Batch Loss: 0.6551703810691833\n",
      "Epoch 601, Loss: 3.629721522331238, Final Batch Loss: 0.7865596413612366\n",
      "Epoch 602, Loss: 3.795756995677948, Final Batch Loss: 0.7786139845848083\n",
      "Epoch 603, Loss: 3.644997000694275, Final Batch Loss: 0.788053035736084\n",
      "Epoch 604, Loss: 3.606069266796112, Final Batch Loss: 0.7068769931793213\n",
      "Epoch 605, Loss: 3.7556031942367554, Final Batch Loss: 0.7413027286529541\n",
      "Epoch 606, Loss: 3.694524645805359, Final Batch Loss: 0.6757487058639526\n",
      "Epoch 607, Loss: 3.6448583602905273, Final Batch Loss: 0.7239059209823608\n",
      "Epoch 608, Loss: 3.6517648100852966, Final Batch Loss: 0.8114708662033081\n",
      "Epoch 609, Loss: 3.7946475744247437, Final Batch Loss: 0.781010627746582\n",
      "Epoch 610, Loss: 3.7239790558815002, Final Batch Loss: 0.7922399044036865\n",
      "Epoch 611, Loss: 3.5636860728263855, Final Batch Loss: 0.8339069485664368\n",
      "Epoch 612, Loss: 3.6282694935798645, Final Batch Loss: 0.6743109226226807\n",
      "Epoch 613, Loss: 3.6427133083343506, Final Batch Loss: 0.6441274881362915\n",
      "Epoch 614, Loss: 3.719409763813019, Final Batch Loss: 0.7420631051063538\n",
      "Epoch 615, Loss: 3.771556854248047, Final Batch Loss: 0.8013812303543091\n",
      "Epoch 616, Loss: 3.6869620084762573, Final Batch Loss: 0.6767219305038452\n",
      "Epoch 617, Loss: 3.6552894115448, Final Batch Loss: 0.7483580708503723\n",
      "Epoch 618, Loss: 3.533733546733856, Final Batch Loss: 0.6245435476303101\n",
      "Epoch 619, Loss: 3.770115613937378, Final Batch Loss: 0.7595659494400024\n",
      "Epoch 620, Loss: 3.62479168176651, Final Batch Loss: 0.733674168586731\n",
      "Epoch 621, Loss: 3.6229668855667114, Final Batch Loss: 0.7186647057533264\n",
      "Epoch 622, Loss: 3.6375787258148193, Final Batch Loss: 0.8186140060424805\n",
      "Epoch 623, Loss: 3.683413505554199, Final Batch Loss: 0.7600215673446655\n",
      "Epoch 624, Loss: 3.5860158801078796, Final Batch Loss: 0.6466930508613586\n",
      "Epoch 625, Loss: 3.5643128156661987, Final Batch Loss: 0.761715292930603\n",
      "Epoch 626, Loss: 3.6436163187026978, Final Batch Loss: 0.8163484334945679\n",
      "Epoch 627, Loss: 3.6354219913482666, Final Batch Loss: 0.6522344946861267\n",
      "Epoch 628, Loss: 3.5429038405418396, Final Batch Loss: 0.7357523441314697\n",
      "Epoch 629, Loss: 3.634778320789337, Final Batch Loss: 0.5603775978088379\n",
      "Epoch 630, Loss: 3.6098950505256653, Final Batch Loss: 0.7876019477844238\n",
      "Epoch 631, Loss: 3.785457491874695, Final Batch Loss: 0.834322988986969\n",
      "Epoch 632, Loss: 3.4791590571403503, Final Batch Loss: 0.6057353615760803\n",
      "Epoch 633, Loss: 3.560331106185913, Final Batch Loss: 0.7297415137290955\n",
      "Epoch 634, Loss: 3.7035006284713745, Final Batch Loss: 0.7850143313407898\n",
      "Epoch 635, Loss: 3.544670820236206, Final Batch Loss: 0.7590072154998779\n",
      "Epoch 636, Loss: 3.4214804768562317, Final Batch Loss: 0.581378161907196\n",
      "Epoch 637, Loss: 3.59360671043396, Final Batch Loss: 0.7008837461471558\n",
      "Epoch 638, Loss: 3.4551637768745422, Final Batch Loss: 0.6399691700935364\n",
      "Epoch 639, Loss: 3.5637059211730957, Final Batch Loss: 0.661705493927002\n",
      "Epoch 640, Loss: 3.4833068251609802, Final Batch Loss: 0.6872377395629883\n",
      "Epoch 641, Loss: 3.547869026660919, Final Batch Loss: 0.6731867790222168\n",
      "Epoch 642, Loss: 3.6551303267478943, Final Batch Loss: 0.781674325466156\n",
      "Epoch 643, Loss: 3.72425240278244, Final Batch Loss: 0.7345456480979919\n",
      "Epoch 644, Loss: 3.5775285959243774, Final Batch Loss: 0.6527827382087708\n",
      "Epoch 645, Loss: 3.6362577080726624, Final Batch Loss: 0.712992250919342\n",
      "Epoch 646, Loss: 3.6504980325698853, Final Batch Loss: 0.7761186957359314\n",
      "Epoch 647, Loss: 3.541538655757904, Final Batch Loss: 0.7091467380523682\n",
      "Epoch 648, Loss: 3.6296578645706177, Final Batch Loss: 0.7194904685020447\n",
      "Epoch 649, Loss: 3.5539527535438538, Final Batch Loss: 0.7381828427314758\n",
      "Epoch 650, Loss: 3.7203672528266907, Final Batch Loss: 0.756013810634613\n",
      "Epoch 651, Loss: 3.548235774040222, Final Batch Loss: 0.7745147347450256\n",
      "Epoch 652, Loss: 3.6112974286079407, Final Batch Loss: 0.6992435455322266\n",
      "Epoch 653, Loss: 3.5429667830467224, Final Batch Loss: 0.7271367311477661\n",
      "Epoch 654, Loss: 3.5905558466911316, Final Batch Loss: 0.6747100353240967\n",
      "Epoch 655, Loss: 3.5180842876434326, Final Batch Loss: 0.6249029040336609\n",
      "Epoch 656, Loss: 3.6049081087112427, Final Batch Loss: 0.8303020596504211\n",
      "Epoch 657, Loss: 3.5172258615493774, Final Batch Loss: 0.7174487709999084\n",
      "Epoch 658, Loss: 3.572268068790436, Final Batch Loss: 0.7698372602462769\n",
      "Epoch 659, Loss: 3.5981082916259766, Final Batch Loss: 0.6943594217300415\n",
      "Epoch 660, Loss: 3.548954665660858, Final Batch Loss: 0.7186101675033569\n",
      "Epoch 661, Loss: 3.516010880470276, Final Batch Loss: 0.6087470054626465\n",
      "Epoch 662, Loss: 3.7342767119407654, Final Batch Loss: 0.8363070487976074\n",
      "Epoch 663, Loss: 3.7100724577903748, Final Batch Loss: 0.7494885921478271\n",
      "Epoch 664, Loss: 3.505149483680725, Final Batch Loss: 0.6776809692382812\n",
      "Epoch 665, Loss: 3.8724523782730103, Final Batch Loss: 0.9387000799179077\n",
      "Epoch 666, Loss: 3.5311477184295654, Final Batch Loss: 0.7000653147697449\n",
      "Epoch 667, Loss: 3.639219582080841, Final Batch Loss: 0.7178850769996643\n",
      "Epoch 668, Loss: 3.5782576203346252, Final Batch Loss: 0.7174175381660461\n",
      "Epoch 669, Loss: 3.4028561115264893, Final Batch Loss: 0.6392943859100342\n",
      "Epoch 670, Loss: 3.4250736832618713, Final Batch Loss: 0.6752105951309204\n",
      "Epoch 671, Loss: 3.525106191635132, Final Batch Loss: 0.6304610371589661\n",
      "Epoch 672, Loss: 3.5095155239105225, Final Batch Loss: 0.7200348377227783\n",
      "Epoch 673, Loss: 3.6456159949302673, Final Batch Loss: 0.818938136100769\n",
      "Epoch 674, Loss: 3.5140500664711, Final Batch Loss: 0.6239653825759888\n",
      "Epoch 675, Loss: 3.537589371204376, Final Batch Loss: 0.6576123833656311\n",
      "Epoch 676, Loss: 3.4696425199508667, Final Batch Loss: 0.6254485249519348\n",
      "Epoch 677, Loss: 3.476855993270874, Final Batch Loss: 0.5196468234062195\n",
      "Epoch 678, Loss: 3.585667610168457, Final Batch Loss: 0.7764220237731934\n",
      "Epoch 679, Loss: 3.456775724887848, Final Batch Loss: 0.7091000080108643\n",
      "Epoch 680, Loss: 3.772209405899048, Final Batch Loss: 0.8739390969276428\n",
      "Epoch 681, Loss: 3.6138768792152405, Final Batch Loss: 0.8730464577674866\n",
      "Epoch 682, Loss: 3.4206186532974243, Final Batch Loss: 0.6935998797416687\n",
      "Epoch 683, Loss: 3.5327789187431335, Final Batch Loss: 0.6891082525253296\n",
      "Epoch 684, Loss: 3.6611651182174683, Final Batch Loss: 0.7807449102401733\n",
      "Epoch 685, Loss: 3.4872973561286926, Final Batch Loss: 0.5473593473434448\n",
      "Epoch 686, Loss: 3.3636456727981567, Final Batch Loss: 0.5847650766372681\n",
      "Epoch 687, Loss: 3.357993960380554, Final Batch Loss: 0.6531448364257812\n",
      "Epoch 688, Loss: 3.3877457976341248, Final Batch Loss: 0.6156653165817261\n",
      "Epoch 689, Loss: 3.397382378578186, Final Batch Loss: 0.7107585668563843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 690, Loss: 3.4703813195228577, Final Batch Loss: 0.7026211619377136\n",
      "Epoch 691, Loss: 3.5078244805336, Final Batch Loss: 0.6992488503456116\n",
      "Epoch 692, Loss: 3.497689425945282, Final Batch Loss: 0.6407924890518188\n",
      "Epoch 693, Loss: 3.4572041630744934, Final Batch Loss: 0.7150943875312805\n",
      "Epoch 694, Loss: 3.5311856865882874, Final Batch Loss: 0.7187933325767517\n",
      "Epoch 695, Loss: 3.5883724093437195, Final Batch Loss: 0.7644985318183899\n",
      "Epoch 696, Loss: 3.518391788005829, Final Batch Loss: 0.664462149143219\n",
      "Epoch 697, Loss: 3.4933077096939087, Final Batch Loss: 0.8032315969467163\n",
      "Epoch 698, Loss: 3.5236605405807495, Final Batch Loss: 0.6537470817565918\n",
      "Epoch 699, Loss: 3.652052402496338, Final Batch Loss: 0.7069443464279175\n",
      "Epoch 700, Loss: 3.478081226348877, Final Batch Loss: 0.6644697785377502\n",
      "Epoch 701, Loss: 3.528026521205902, Final Batch Loss: 0.7775005102157593\n",
      "Epoch 702, Loss: 3.3733946084976196, Final Batch Loss: 0.6399145722389221\n",
      "Epoch 703, Loss: 3.5485334992408752, Final Batch Loss: 0.730678379535675\n",
      "Epoch 704, Loss: 3.444718599319458, Final Batch Loss: 0.7761367559432983\n",
      "Epoch 705, Loss: 3.374123990535736, Final Batch Loss: 0.663038432598114\n",
      "Epoch 706, Loss: 3.3222626447677612, Final Batch Loss: 0.6079164147377014\n",
      "Epoch 707, Loss: 3.4542691111564636, Final Batch Loss: 0.6601119637489319\n",
      "Epoch 708, Loss: 3.3601053953170776, Final Batch Loss: 0.639601469039917\n",
      "Epoch 709, Loss: 3.5713939666748047, Final Batch Loss: 0.7269600033760071\n",
      "Epoch 710, Loss: 3.338944673538208, Final Batch Loss: 0.5885350704193115\n",
      "Epoch 711, Loss: 3.495166599750519, Final Batch Loss: 0.8076086044311523\n",
      "Epoch 712, Loss: 3.5835129022598267, Final Batch Loss: 0.8007755279541016\n",
      "Epoch 713, Loss: 3.5669644474983215, Final Batch Loss: 0.8079791069030762\n",
      "Epoch 714, Loss: 3.5791961550712585, Final Batch Loss: 0.705119788646698\n",
      "Epoch 715, Loss: 3.4640274047851562, Final Batch Loss: 0.7357026934623718\n",
      "Epoch 716, Loss: 3.613350749015808, Final Batch Loss: 0.8275876641273499\n",
      "Epoch 717, Loss: 3.5767956972122192, Final Batch Loss: 0.7654514908790588\n",
      "Epoch 718, Loss: 3.5348294973373413, Final Batch Loss: 0.723899781703949\n",
      "Epoch 719, Loss: 3.457570433616638, Final Batch Loss: 0.7092404365539551\n",
      "Epoch 720, Loss: 3.382149338722229, Final Batch Loss: 0.7115312814712524\n",
      "Epoch 721, Loss: 3.2874740958213806, Final Batch Loss: 0.6370217204093933\n",
      "Epoch 722, Loss: 3.3634113669395447, Final Batch Loss: 0.6241585612297058\n",
      "Epoch 723, Loss: 3.3253984451293945, Final Batch Loss: 0.5597774386405945\n",
      "Epoch 724, Loss: 3.528667449951172, Final Batch Loss: 0.6328028440475464\n",
      "Epoch 725, Loss: 3.4270118474960327, Final Batch Loss: 0.6268447637557983\n",
      "Epoch 726, Loss: 3.411485433578491, Final Batch Loss: 0.7149683833122253\n",
      "Epoch 727, Loss: 3.6282509565353394, Final Batch Loss: 0.7630594968795776\n",
      "Epoch 728, Loss: 3.4089735746383667, Final Batch Loss: 0.597771167755127\n",
      "Epoch 729, Loss: 3.4855891466140747, Final Batch Loss: 0.5753819346427917\n",
      "Epoch 730, Loss: 3.6306281685829163, Final Batch Loss: 0.8769044280052185\n",
      "Epoch 731, Loss: 3.386898159980774, Final Batch Loss: 0.665152370929718\n",
      "Epoch 732, Loss: 3.3468716144561768, Final Batch Loss: 0.683133602142334\n",
      "Epoch 733, Loss: 3.3924976587295532, Final Batch Loss: 0.6782349348068237\n",
      "Epoch 734, Loss: 3.49449360370636, Final Batch Loss: 0.7500891089439392\n",
      "Epoch 735, Loss: 3.552557349205017, Final Batch Loss: 0.7469692826271057\n",
      "Epoch 736, Loss: 3.364443361759186, Final Batch Loss: 0.7553715705871582\n",
      "Epoch 737, Loss: 3.283940374851227, Final Batch Loss: 0.8273013234138489\n",
      "Epoch 738, Loss: 3.4055440425872803, Final Batch Loss: 0.7652392387390137\n",
      "Epoch 739, Loss: 3.5620117783546448, Final Batch Loss: 0.7415405511856079\n",
      "Epoch 740, Loss: 3.4272725582122803, Final Batch Loss: 0.6795680522918701\n",
      "Epoch 741, Loss: 3.2370393872261047, Final Batch Loss: 0.597967803478241\n",
      "Epoch 742, Loss: 3.3697683215141296, Final Batch Loss: 0.614591658115387\n",
      "Epoch 743, Loss: 3.277526080608368, Final Batch Loss: 0.6611214280128479\n",
      "Epoch 744, Loss: 3.3353219628334045, Final Batch Loss: 0.6701276302337646\n",
      "Epoch 745, Loss: 3.424397647380829, Final Batch Loss: 0.6489210724830627\n",
      "Epoch 746, Loss: 3.412900686264038, Final Batch Loss: 0.7672635316848755\n",
      "Epoch 747, Loss: 3.378320097923279, Final Batch Loss: 0.6153808832168579\n",
      "Epoch 748, Loss: 3.57539439201355, Final Batch Loss: 0.7604192495346069\n",
      "Epoch 749, Loss: 3.4301151633262634, Final Batch Loss: 0.6638412475585938\n",
      "Epoch 750, Loss: 3.546090006828308, Final Batch Loss: 0.7300710082054138\n",
      "Epoch 751, Loss: 3.214427888393402, Final Batch Loss: 0.5190879106521606\n",
      "Epoch 752, Loss: 3.3324766755104065, Final Batch Loss: 0.7286904454231262\n",
      "Epoch 753, Loss: 3.4238044023513794, Final Batch Loss: 0.605249285697937\n",
      "Epoch 754, Loss: 3.3742213249206543, Final Batch Loss: 0.6039384603500366\n",
      "Epoch 755, Loss: 3.3417357802391052, Final Batch Loss: 0.6284196376800537\n",
      "Epoch 756, Loss: 3.463957190513611, Final Batch Loss: 0.6859078407287598\n",
      "Epoch 757, Loss: 3.2977656722068787, Final Batch Loss: 0.6011805534362793\n",
      "Epoch 758, Loss: 3.249511420726776, Final Batch Loss: 0.6398763060569763\n",
      "Epoch 759, Loss: 3.4523975253105164, Final Batch Loss: 0.7540585398674011\n",
      "Epoch 760, Loss: 3.365690290927887, Final Batch Loss: 0.6871160268783569\n",
      "Epoch 761, Loss: 3.3524866700172424, Final Batch Loss: 0.627440869808197\n",
      "Epoch 762, Loss: 3.4939847588539124, Final Batch Loss: 0.8277156352996826\n",
      "Epoch 763, Loss: 3.477432608604431, Final Batch Loss: 0.671938955783844\n",
      "Epoch 764, Loss: 3.3608131408691406, Final Batch Loss: 0.7632530331611633\n",
      "Epoch 765, Loss: 3.3044384121894836, Final Batch Loss: 0.6231799125671387\n",
      "Epoch 766, Loss: 3.4060436487197876, Final Batch Loss: 0.599189817905426\n",
      "Epoch 767, Loss: 3.4464633464813232, Final Batch Loss: 0.8250747919082642\n",
      "Epoch 768, Loss: 3.3319844603538513, Final Batch Loss: 0.657260000705719\n",
      "Epoch 769, Loss: 3.343185305595398, Final Batch Loss: 0.7366855144500732\n",
      "Epoch 770, Loss: 3.279768228530884, Final Batch Loss: 0.5858155488967896\n",
      "Epoch 771, Loss: 3.3603532910346985, Final Batch Loss: 0.6544431447982788\n",
      "Epoch 772, Loss: 3.219158709049225, Final Batch Loss: 0.6177259087562561\n",
      "Epoch 773, Loss: 3.314961612224579, Final Batch Loss: 0.7431406378746033\n",
      "Epoch 774, Loss: 3.38303279876709, Final Batch Loss: 0.6528930068016052\n",
      "Epoch 775, Loss: 3.4169177412986755, Final Batch Loss: 0.7167043685913086\n",
      "Epoch 776, Loss: 3.1597763299942017, Final Batch Loss: 0.6649298667907715\n",
      "Epoch 777, Loss: 3.345448315143585, Final Batch Loss: 0.70955491065979\n",
      "Epoch 778, Loss: 3.304652512073517, Final Batch Loss: 0.5961793065071106\n",
      "Epoch 779, Loss: 3.300923466682434, Final Batch Loss: 0.6573184132575989\n",
      "Epoch 780, Loss: 3.430316925048828, Final Batch Loss: 0.8527522683143616\n",
      "Epoch 781, Loss: 3.303082585334778, Final Batch Loss: 0.6362572908401489\n",
      "Epoch 782, Loss: 3.2286201119422913, Final Batch Loss: 0.5648147463798523\n",
      "Epoch 783, Loss: 3.2997778058052063, Final Batch Loss: 0.6785231232643127\n",
      "Epoch 784, Loss: 3.264835476875305, Final Batch Loss: 0.6043708324432373\n",
      "Epoch 785, Loss: 3.4293092489242554, Final Batch Loss: 0.7296262979507446\n",
      "Epoch 786, Loss: 3.494359791278839, Final Batch Loss: 0.6188535094261169\n",
      "Epoch 787, Loss: 3.418177604675293, Final Batch Loss: 0.7475724816322327\n",
      "Epoch 788, Loss: 3.2704336047172546, Final Batch Loss: 0.7082623243331909\n",
      "Epoch 789, Loss: 3.598381519317627, Final Batch Loss: 0.7097681164741516\n",
      "Epoch 790, Loss: 3.2557584047317505, Final Batch Loss: 0.7541602849960327\n",
      "Epoch 791, Loss: 3.4392898082733154, Final Batch Loss: 0.640756368637085\n",
      "Epoch 792, Loss: 3.368236541748047, Final Batch Loss: 0.6796039342880249\n",
      "Epoch 793, Loss: 3.4672515988349915, Final Batch Loss: 0.7413896918296814\n",
      "Epoch 794, Loss: 3.155221402645111, Final Batch Loss: 0.5681657195091248\n",
      "Epoch 795, Loss: 3.3453723788261414, Final Batch Loss: 0.6309406757354736\n",
      "Epoch 796, Loss: 3.3898845314979553, Final Batch Loss: 0.6518319249153137\n",
      "Epoch 797, Loss: 3.3530982732772827, Final Batch Loss: 0.6839753985404968\n",
      "Epoch 798, Loss: 3.147943913936615, Final Batch Loss: 0.6352692246437073\n",
      "Epoch 799, Loss: 3.437494397163391, Final Batch Loss: 0.7225098609924316\n",
      "Epoch 800, Loss: 3.2512471079826355, Final Batch Loss: 0.6434673070907593\n",
      "Epoch 801, Loss: 3.2956182956695557, Final Batch Loss: 0.6135367751121521\n",
      "Epoch 802, Loss: 3.1865002512931824, Final Batch Loss: 0.5954209566116333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 803, Loss: 3.2679280638694763, Final Batch Loss: 0.5852056741714478\n",
      "Epoch 804, Loss: 3.344961166381836, Final Batch Loss: 0.847909152507782\n",
      "Epoch 805, Loss: 3.426101803779602, Final Batch Loss: 0.8469237089157104\n",
      "Epoch 806, Loss: 3.3201765418052673, Final Batch Loss: 0.6940537691116333\n",
      "Epoch 807, Loss: 3.238840878009796, Final Batch Loss: 0.6219926476478577\n",
      "Epoch 808, Loss: 3.285288393497467, Final Batch Loss: 0.5965558886528015\n",
      "Epoch 809, Loss: 3.2505361437797546, Final Batch Loss: 0.691232442855835\n",
      "Epoch 810, Loss: 3.419051170349121, Final Batch Loss: 0.6747460961341858\n",
      "Epoch 811, Loss: 3.3934741616249084, Final Batch Loss: 0.7170225977897644\n",
      "Epoch 812, Loss: 3.4462578296661377, Final Batch Loss: 0.7349873185157776\n",
      "Epoch 813, Loss: 3.2434300780296326, Final Batch Loss: 0.6702880859375\n",
      "Epoch 814, Loss: 3.1356546878814697, Final Batch Loss: 0.5923664569854736\n",
      "Epoch 815, Loss: 3.2657418847084045, Final Batch Loss: 0.6760657429695129\n",
      "Epoch 816, Loss: 3.289364278316498, Final Batch Loss: 0.6517964005470276\n",
      "Epoch 817, Loss: 3.213331878185272, Final Batch Loss: 0.6668850779533386\n",
      "Epoch 818, Loss: 3.34180611371994, Final Batch Loss: 0.6881694197654724\n",
      "Epoch 819, Loss: 3.3594972491264343, Final Batch Loss: 0.7466462254524231\n",
      "Epoch 820, Loss: 3.3984882831573486, Final Batch Loss: 0.6107633113861084\n",
      "Epoch 821, Loss: 3.328821837902069, Final Batch Loss: 0.7579069137573242\n",
      "Epoch 822, Loss: 3.3634634613990784, Final Batch Loss: 0.7812013030052185\n",
      "Epoch 823, Loss: 3.299939751625061, Final Batch Loss: 0.6999228000640869\n",
      "Epoch 824, Loss: 3.217148780822754, Final Batch Loss: 0.5920282006263733\n",
      "Epoch 825, Loss: 3.2368064522743225, Final Batch Loss: 0.5909376740455627\n",
      "Epoch 826, Loss: 3.3455244302749634, Final Batch Loss: 0.7180958986282349\n",
      "Epoch 827, Loss: 3.262165069580078, Final Batch Loss: 0.7264583110809326\n",
      "Epoch 828, Loss: 3.2708247303962708, Final Batch Loss: 0.7442814111709595\n",
      "Epoch 829, Loss: 3.1216426491737366, Final Batch Loss: 0.5453447103500366\n",
      "Epoch 830, Loss: 3.38496470451355, Final Batch Loss: 0.7081041932106018\n",
      "Epoch 831, Loss: 3.3033732771873474, Final Batch Loss: 0.6741231679916382\n",
      "Epoch 832, Loss: 3.1432708501815796, Final Batch Loss: 0.610845148563385\n",
      "Epoch 833, Loss: 3.2773671746253967, Final Batch Loss: 0.6360759139060974\n",
      "Epoch 834, Loss: 3.303345024585724, Final Batch Loss: 0.8347378373146057\n",
      "Epoch 835, Loss: 3.305849552154541, Final Batch Loss: 0.6711094379425049\n",
      "Epoch 836, Loss: 3.2818814516067505, Final Batch Loss: 0.6866379976272583\n",
      "Epoch 837, Loss: 3.3232333660125732, Final Batch Loss: 0.6928191184997559\n",
      "Epoch 838, Loss: 3.281137704849243, Final Batch Loss: 0.7037643790245056\n",
      "Epoch 839, Loss: 3.1229666471481323, Final Batch Loss: 0.6307070255279541\n",
      "Epoch 840, Loss: 3.2075365781784058, Final Batch Loss: 0.548492968082428\n",
      "Epoch 841, Loss: 3.1956170797348022, Final Batch Loss: 0.620518147945404\n",
      "Epoch 842, Loss: 3.098605692386627, Final Batch Loss: 0.4537525177001953\n",
      "Epoch 843, Loss: 3.3061740398406982, Final Batch Loss: 0.6686998009681702\n",
      "Epoch 844, Loss: 3.176911473274231, Final Batch Loss: 0.544648289680481\n",
      "Epoch 845, Loss: 3.2144041061401367, Final Batch Loss: 0.6866488456726074\n",
      "Epoch 846, Loss: 3.291484773159027, Final Batch Loss: 0.588493287563324\n",
      "Epoch 847, Loss: 3.251866579055786, Final Batch Loss: 0.671541154384613\n",
      "Epoch 848, Loss: 3.2896774411201477, Final Batch Loss: 0.6645469665527344\n",
      "Epoch 849, Loss: 3.3162805438041687, Final Batch Loss: 0.6631555557250977\n",
      "Epoch 850, Loss: 3.3633291125297546, Final Batch Loss: 0.6530697345733643\n",
      "Epoch 851, Loss: 3.19685161113739, Final Batch Loss: 0.598427414894104\n",
      "Epoch 852, Loss: 3.2992957830429077, Final Batch Loss: 0.6709731817245483\n",
      "Epoch 853, Loss: 3.3508039712905884, Final Batch Loss: 0.7097488045692444\n",
      "Epoch 854, Loss: 3.1243977546691895, Final Batch Loss: 0.5016555190086365\n",
      "Epoch 855, Loss: 3.1730127930641174, Final Batch Loss: 0.6179885268211365\n",
      "Epoch 856, Loss: 3.063068926334381, Final Batch Loss: 0.5527433753013611\n",
      "Epoch 857, Loss: 3.216969072818756, Final Batch Loss: 0.7372415661811829\n",
      "Epoch 858, Loss: 3.2580573558807373, Final Batch Loss: 0.6874913573265076\n",
      "Epoch 859, Loss: 3.0854470133781433, Final Batch Loss: 0.6010434627532959\n",
      "Epoch 860, Loss: 3.182655394077301, Final Batch Loss: 0.680362343788147\n",
      "Epoch 861, Loss: 3.252468466758728, Final Batch Loss: 0.6049307584762573\n",
      "Epoch 862, Loss: 3.2131298184394836, Final Batch Loss: 0.6270507574081421\n",
      "Epoch 863, Loss: 3.1504146456718445, Final Batch Loss: 0.5821264386177063\n",
      "Epoch 864, Loss: 3.1427345871925354, Final Batch Loss: 0.7005594968795776\n",
      "Epoch 865, Loss: 3.182966887950897, Final Batch Loss: 0.7199952602386475\n",
      "Epoch 866, Loss: 3.2522103786468506, Final Batch Loss: 0.7220787405967712\n",
      "Epoch 867, Loss: 3.2200462222099304, Final Batch Loss: 0.5662429332733154\n",
      "Epoch 868, Loss: 3.0443124771118164, Final Batch Loss: 0.579692006111145\n",
      "Epoch 869, Loss: 3.1535208225250244, Final Batch Loss: 0.6928802728652954\n",
      "Epoch 870, Loss: 3.0808398723602295, Final Batch Loss: 0.5183421969413757\n",
      "Epoch 871, Loss: 3.3023674488067627, Final Batch Loss: 0.7375187873840332\n",
      "Epoch 872, Loss: 3.254061698913574, Final Batch Loss: 0.7853135466575623\n",
      "Epoch 873, Loss: 3.179288923740387, Final Batch Loss: 0.5843701362609863\n",
      "Epoch 874, Loss: 3.2512073516845703, Final Batch Loss: 0.7286370992660522\n",
      "Epoch 875, Loss: 3.2225935459136963, Final Batch Loss: 0.6398923397064209\n",
      "Epoch 876, Loss: 3.172716796398163, Final Batch Loss: 0.6851522922515869\n",
      "Epoch 877, Loss: 3.155450224876404, Final Batch Loss: 0.568575382232666\n",
      "Epoch 878, Loss: 3.256540834903717, Final Batch Loss: 0.6356445550918579\n",
      "Epoch 879, Loss: 3.2878562211990356, Final Batch Loss: 0.6706783175468445\n",
      "Epoch 880, Loss: 3.2434399724006653, Final Batch Loss: 0.5483526587486267\n",
      "Epoch 881, Loss: 3.1376779079437256, Final Batch Loss: 0.5944607257843018\n",
      "Epoch 882, Loss: 3.0594080686569214, Final Batch Loss: 0.6233829259872437\n",
      "Epoch 883, Loss: 3.216112792491913, Final Batch Loss: 0.6516417264938354\n",
      "Epoch 884, Loss: 3.3039150834083557, Final Batch Loss: 0.756533682346344\n",
      "Epoch 885, Loss: 3.213765501976013, Final Batch Loss: 0.6574661731719971\n",
      "Epoch 886, Loss: 3.0528088212013245, Final Batch Loss: 0.6036058068275452\n",
      "Epoch 887, Loss: 3.269017815589905, Final Batch Loss: 0.6323742866516113\n",
      "Epoch 888, Loss: 3.4077627658843994, Final Batch Loss: 0.8773132562637329\n",
      "Epoch 889, Loss: 3.2649484276771545, Final Batch Loss: 0.5948723554611206\n",
      "Epoch 890, Loss: 3.1765080094337463, Final Batch Loss: 0.5953799486160278\n",
      "Epoch 891, Loss: 3.270919680595398, Final Batch Loss: 0.684699296951294\n",
      "Epoch 892, Loss: 3.329638421535492, Final Batch Loss: 0.7125318050384521\n",
      "Epoch 893, Loss: 3.065648078918457, Final Batch Loss: 0.5300340056419373\n",
      "Epoch 894, Loss: 3.2790764570236206, Final Batch Loss: 0.6955947279930115\n",
      "Epoch 895, Loss: 3.1784607768058777, Final Batch Loss: 0.6381085515022278\n",
      "Epoch 896, Loss: 3.142951190471649, Final Batch Loss: 0.7047988772392273\n",
      "Epoch 897, Loss: 3.3324506282806396, Final Batch Loss: 0.6548380851745605\n",
      "Epoch 898, Loss: 3.125915288925171, Final Batch Loss: 0.6029324531555176\n",
      "Epoch 899, Loss: 3.0560763478279114, Final Batch Loss: 0.6132174730300903\n",
      "Epoch 900, Loss: 3.2979456782341003, Final Batch Loss: 0.7946527600288391\n",
      "Epoch 901, Loss: 3.1163015961647034, Final Batch Loss: 0.6574938893318176\n",
      "Epoch 902, Loss: 3.1496448516845703, Final Batch Loss: 0.6130056977272034\n",
      "Epoch 903, Loss: 3.0894683599472046, Final Batch Loss: 0.6166985034942627\n",
      "Epoch 904, Loss: 3.115414619445801, Final Batch Loss: 0.5515708923339844\n",
      "Epoch 905, Loss: 3.137935519218445, Final Batch Loss: 0.6592487692832947\n",
      "Epoch 906, Loss: 3.1666740775108337, Final Batch Loss: 0.6175617575645447\n",
      "Epoch 907, Loss: 3.077686131000519, Final Batch Loss: 0.5683295726776123\n",
      "Epoch 908, Loss: 3.1740809082984924, Final Batch Loss: 0.6624630093574524\n",
      "Epoch 909, Loss: 3.024744212627411, Final Batch Loss: 0.5867835283279419\n",
      "Epoch 910, Loss: 3.1087692379951477, Final Batch Loss: 0.5406496524810791\n",
      "Epoch 911, Loss: 3.2177364826202393, Final Batch Loss: 0.6472423076629639\n",
      "Epoch 912, Loss: 3.1298352479934692, Final Batch Loss: 0.644497811794281\n",
      "Epoch 913, Loss: 3.1299449801445007, Final Batch Loss: 0.6556767821311951\n",
      "Epoch 914, Loss: 3.124077796936035, Final Batch Loss: 0.565002977848053\n",
      "Epoch 915, Loss: 3.174774646759033, Final Batch Loss: 0.8146533966064453\n",
      "Epoch 916, Loss: 3.023138552904129, Final Batch Loss: 0.5273112654685974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 917, Loss: 3.2096500396728516, Final Batch Loss: 0.6847214698791504\n",
      "Epoch 918, Loss: 3.0966250896453857, Final Batch Loss: 0.5640316009521484\n",
      "Epoch 919, Loss: 3.1671429872512817, Final Batch Loss: 0.6390873193740845\n",
      "Epoch 920, Loss: 3.156384289264679, Final Batch Loss: 0.5211060643196106\n",
      "Epoch 921, Loss: 3.1388734579086304, Final Batch Loss: 0.7048202157020569\n",
      "Epoch 922, Loss: 3.150359094142914, Final Batch Loss: 0.6735984086990356\n",
      "Epoch 923, Loss: 3.270435929298401, Final Batch Loss: 0.7106526494026184\n",
      "Epoch 924, Loss: 3.1355329751968384, Final Batch Loss: 0.5725194215774536\n",
      "Epoch 925, Loss: 3.1789504289627075, Final Batch Loss: 0.6988000273704529\n",
      "Epoch 926, Loss: 2.922378122806549, Final Batch Loss: 0.6086644530296326\n",
      "Epoch 927, Loss: 3.1258227229118347, Final Batch Loss: 0.6445907950401306\n",
      "Epoch 928, Loss: 3.148897647857666, Final Batch Loss: 0.6331968903541565\n",
      "Epoch 929, Loss: 3.272076427936554, Final Batch Loss: 0.6687982678413391\n",
      "Epoch 930, Loss: 3.1307849884033203, Final Batch Loss: 0.6241005063056946\n",
      "Epoch 931, Loss: 3.1929428577423096, Final Batch Loss: 0.667082667350769\n",
      "Epoch 932, Loss: 3.0211135745048523, Final Batch Loss: 0.5674458742141724\n",
      "Epoch 933, Loss: 3.1867611408233643, Final Batch Loss: 0.6809648275375366\n",
      "Epoch 934, Loss: 3.1610398292541504, Final Batch Loss: 0.7487419843673706\n",
      "Epoch 935, Loss: 3.1757397651672363, Final Batch Loss: 0.6721879243850708\n",
      "Epoch 936, Loss: 3.255874276161194, Final Batch Loss: 0.6329511404037476\n",
      "Epoch 937, Loss: 3.14026415348053, Final Batch Loss: 0.7392405271530151\n",
      "Epoch 938, Loss: 3.182380437850952, Final Batch Loss: 0.6175011396408081\n",
      "Epoch 939, Loss: 3.055187940597534, Final Batch Loss: 0.5613563656806946\n",
      "Epoch 940, Loss: 2.9610392451286316, Final Batch Loss: 0.5148141384124756\n",
      "Epoch 941, Loss: 2.998542070388794, Final Batch Loss: 0.5403979420661926\n",
      "Epoch 942, Loss: 3.1066172122955322, Final Batch Loss: 0.6101663708686829\n",
      "Epoch 943, Loss: 2.9636422991752625, Final Batch Loss: 0.5049571990966797\n",
      "Epoch 944, Loss: 3.070670187473297, Final Batch Loss: 0.559848964214325\n",
      "Epoch 945, Loss: 3.000666856765747, Final Batch Loss: 0.5988592505455017\n",
      "Epoch 946, Loss: 3.2066447734832764, Final Batch Loss: 0.6324968934059143\n",
      "Epoch 947, Loss: 3.072113335132599, Final Batch Loss: 0.6599334478378296\n",
      "Epoch 948, Loss: 3.239928424358368, Final Batch Loss: 0.6941924691200256\n",
      "Epoch 949, Loss: 2.9700392484664917, Final Batch Loss: 0.5405980348587036\n",
      "Epoch 950, Loss: 3.159958600997925, Final Batch Loss: 0.7001053094863892\n",
      "Epoch 951, Loss: 3.082419693470001, Final Batch Loss: 0.542231559753418\n",
      "Epoch 952, Loss: 3.1065260767936707, Final Batch Loss: 0.5814609527587891\n",
      "Epoch 953, Loss: 3.15475994348526, Final Batch Loss: 0.6197964549064636\n",
      "Epoch 954, Loss: 3.1817139387130737, Final Batch Loss: 0.6264195442199707\n",
      "Epoch 955, Loss: 3.076012372970581, Final Batch Loss: 0.6257326006889343\n",
      "Epoch 956, Loss: 3.180833578109741, Final Batch Loss: 0.6746551394462585\n",
      "Epoch 957, Loss: 2.950861871242523, Final Batch Loss: 0.5763291716575623\n",
      "Epoch 958, Loss: 3.1161758303642273, Final Batch Loss: 0.6356057524681091\n",
      "Epoch 959, Loss: 2.9613773822784424, Final Batch Loss: 0.5328640937805176\n",
      "Epoch 960, Loss: 3.0647287368774414, Final Batch Loss: 0.6398699283599854\n",
      "Epoch 961, Loss: 3.03871488571167, Final Batch Loss: 0.5504611134529114\n",
      "Epoch 962, Loss: 3.234109044075012, Final Batch Loss: 0.6571363210678101\n",
      "Epoch 963, Loss: 3.093058407306671, Final Batch Loss: 0.6009039878845215\n",
      "Epoch 964, Loss: 3.203050434589386, Final Batch Loss: 0.6119754910469055\n",
      "Epoch 965, Loss: 2.992091715335846, Final Batch Loss: 0.5605816841125488\n",
      "Epoch 966, Loss: 3.0660510659217834, Final Batch Loss: 0.5779219269752502\n",
      "Epoch 967, Loss: 3.043356776237488, Final Batch Loss: 0.5714513063430786\n",
      "Epoch 968, Loss: 3.0799518823623657, Final Batch Loss: 0.6285664439201355\n",
      "Epoch 969, Loss: 2.9126390516757965, Final Batch Loss: 0.4767322838306427\n",
      "Epoch 970, Loss: 3.065701484680176, Final Batch Loss: 0.6809536814689636\n",
      "Epoch 971, Loss: 2.935110628604889, Final Batch Loss: 0.6394900679588318\n",
      "Epoch 972, Loss: 3.112384021282196, Final Batch Loss: 0.5800716280937195\n",
      "Epoch 973, Loss: 3.164664387702942, Final Batch Loss: 0.6887218356132507\n",
      "Epoch 974, Loss: 2.929928183555603, Final Batch Loss: 0.5049068331718445\n",
      "Epoch 975, Loss: 3.140007972717285, Final Batch Loss: 0.6559095978736877\n",
      "Epoch 976, Loss: 3.0828294157981873, Final Batch Loss: 0.6094118356704712\n",
      "Epoch 977, Loss: 3.031088173389435, Final Batch Loss: 0.6732788681983948\n",
      "Epoch 978, Loss: 2.9582526981830597, Final Batch Loss: 0.5886067152023315\n",
      "Epoch 979, Loss: 2.995808780193329, Final Batch Loss: 0.5646032094955444\n",
      "Epoch 980, Loss: 3.0721550583839417, Final Batch Loss: 0.6530965566635132\n",
      "Epoch 981, Loss: 3.0753709077835083, Final Batch Loss: 0.6633996367454529\n",
      "Epoch 982, Loss: 3.143378973007202, Final Batch Loss: 0.6798926591873169\n",
      "Epoch 983, Loss: 3.0090630650520325, Final Batch Loss: 0.7008498907089233\n",
      "Epoch 984, Loss: 3.069295644760132, Final Batch Loss: 0.5816256999969482\n",
      "Epoch 985, Loss: 3.230944335460663, Final Batch Loss: 0.7103103399276733\n",
      "Epoch 986, Loss: 3.1716200709342957, Final Batch Loss: 0.7431284189224243\n",
      "Epoch 987, Loss: 2.9253799319267273, Final Batch Loss: 0.5857115387916565\n",
      "Epoch 988, Loss: 2.9194689989089966, Final Batch Loss: 0.5292079448699951\n",
      "Epoch 989, Loss: 3.0149078965187073, Final Batch Loss: 0.6412471532821655\n",
      "Epoch 990, Loss: 3.013218343257904, Final Batch Loss: 0.5214311480522156\n",
      "Epoch 991, Loss: 3.1602829694747925, Final Batch Loss: 0.6606209874153137\n",
      "Epoch 992, Loss: 3.1224806904792786, Final Batch Loss: 0.6729386448860168\n",
      "Epoch 993, Loss: 3.127610504627228, Final Batch Loss: 0.6451280117034912\n",
      "Epoch 994, Loss: 3.05198335647583, Final Batch Loss: 0.6261072158813477\n",
      "Epoch 995, Loss: 3.00478732585907, Final Batch Loss: 0.6020840406417847\n",
      "Epoch 996, Loss: 3.105844557285309, Final Batch Loss: 0.6540138125419617\n",
      "Epoch 997, Loss: 3.0731972455978394, Final Batch Loss: 0.5770450830459595\n",
      "Epoch 998, Loss: 3.019860178232193, Final Batch Loss: 0.6734175086021423\n",
      "Epoch 999, Loss: 3.0481096506118774, Final Batch Loss: 0.6499067544937134\n",
      "Epoch 1000, Loss: 2.9203317165374756, Final Batch Loss: 0.5301592350006104\n",
      "Epoch 1001, Loss: 3.066954553127289, Final Batch Loss: 0.6804690361022949\n",
      "Epoch 1002, Loss: 3.042663872241974, Final Batch Loss: 0.6473116874694824\n",
      "Epoch 1003, Loss: 3.05906218290329, Final Batch Loss: 0.6114311218261719\n",
      "Epoch 1004, Loss: 3.1143208146095276, Final Batch Loss: 0.5720400810241699\n",
      "Epoch 1005, Loss: 2.9793859720230103, Final Batch Loss: 0.6291505098342896\n",
      "Epoch 1006, Loss: 3.0188255310058594, Final Batch Loss: 0.609467089176178\n",
      "Epoch 1007, Loss: 3.0488266944885254, Final Batch Loss: 0.6862683296203613\n",
      "Epoch 1008, Loss: 2.980950891971588, Final Batch Loss: 0.5203285217285156\n",
      "Epoch 1009, Loss: 3.1382699608802795, Final Batch Loss: 0.58326655626297\n",
      "Epoch 1010, Loss: 2.895246207714081, Final Batch Loss: 0.6284478306770325\n",
      "Epoch 1011, Loss: 3.0695863366127014, Final Batch Loss: 0.7188582420349121\n",
      "Epoch 1012, Loss: 3.1561062335968018, Final Batch Loss: 0.6986785531044006\n",
      "Epoch 1013, Loss: 2.9296563863754272, Final Batch Loss: 0.6101170778274536\n",
      "Epoch 1014, Loss: 3.0425438284873962, Final Batch Loss: 0.5966261625289917\n",
      "Epoch 1015, Loss: 3.1299793124198914, Final Batch Loss: 0.5945934653282166\n",
      "Epoch 1016, Loss: 3.076500654220581, Final Batch Loss: 0.6525669693946838\n",
      "Epoch 1017, Loss: 2.8895695209503174, Final Batch Loss: 0.5508401989936829\n",
      "Epoch 1018, Loss: 2.939857602119446, Final Batch Loss: 0.6350041627883911\n",
      "Epoch 1019, Loss: 3.026148200035095, Final Batch Loss: 0.61165851354599\n",
      "Epoch 1020, Loss: 2.9207319021224976, Final Batch Loss: 0.6314966082572937\n",
      "Epoch 1021, Loss: 3.0072290897369385, Final Batch Loss: 0.6020772457122803\n",
      "Epoch 1022, Loss: 2.971765637397766, Final Batch Loss: 0.5345209836959839\n",
      "Epoch 1023, Loss: 2.9129795134067535, Final Batch Loss: 0.6247278451919556\n",
      "Epoch 1024, Loss: 3.005509316921234, Final Batch Loss: 0.6227147579193115\n",
      "Epoch 1025, Loss: 3.20765882730484, Final Batch Loss: 0.5929573774337769\n",
      "Epoch 1026, Loss: 3.044569194316864, Final Batch Loss: 0.7040303945541382\n",
      "Epoch 1027, Loss: 2.9681312441825867, Final Batch Loss: 0.5650781989097595\n",
      "Epoch 1028, Loss: 3.0151562094688416, Final Batch Loss: 0.651314914226532\n",
      "Epoch 1029, Loss: 3.0458694100379944, Final Batch Loss: 0.5872549414634705\n",
      "Epoch 1030, Loss: 2.9058704376220703, Final Batch Loss: 0.5944305658340454\n",
      "Epoch 1031, Loss: 2.9543736577033997, Final Batch Loss: 0.6193785071372986\n",
      "Epoch 1032, Loss: 3.1563082933425903, Final Batch Loss: 0.7249575257301331\n",
      "Epoch 1033, Loss: 2.882372498512268, Final Batch Loss: 0.5187616348266602\n",
      "Epoch 1034, Loss: 3.0175845623016357, Final Batch Loss: 0.6195786595344543\n",
      "Epoch 1035, Loss: 3.0482150614261627, Final Batch Loss: 0.733832836151123\n",
      "Epoch 1036, Loss: 3.0414008498191833, Final Batch Loss: 0.64356529712677\n",
      "Epoch 1037, Loss: 2.933265507221222, Final Batch Loss: 0.6842176914215088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1038, Loss: 2.983969569206238, Final Batch Loss: 0.5472350120544434\n",
      "Epoch 1039, Loss: 2.916545033454895, Final Batch Loss: 0.6294137835502625\n",
      "Epoch 1040, Loss: 2.9611597061157227, Final Batch Loss: 0.483043372631073\n",
      "Epoch 1041, Loss: 2.944149672985077, Final Batch Loss: 0.6395263671875\n",
      "Epoch 1042, Loss: 3.135923981666565, Final Batch Loss: 0.5584151148796082\n",
      "Epoch 1043, Loss: 3.0209116339683533, Final Batch Loss: 0.5540137887001038\n",
      "Epoch 1044, Loss: 2.832429826259613, Final Batch Loss: 0.5750191807746887\n",
      "Epoch 1045, Loss: 2.9466801285743713, Final Batch Loss: 0.5510988831520081\n",
      "Epoch 1046, Loss: 2.9761521220207214, Final Batch Loss: 0.5356805920600891\n",
      "Epoch 1047, Loss: 2.9940434098243713, Final Batch Loss: 0.5850998759269714\n",
      "Epoch 1048, Loss: 2.9387949407100677, Final Batch Loss: 0.5087617635726929\n",
      "Epoch 1049, Loss: 2.941792905330658, Final Batch Loss: 0.5454269647598267\n",
      "Epoch 1050, Loss: 2.9746886491775513, Final Batch Loss: 0.6560880541801453\n",
      "Epoch 1051, Loss: 2.9648011922836304, Final Batch Loss: 0.5962659120559692\n",
      "Epoch 1052, Loss: 2.976694107055664, Final Batch Loss: 0.6193832755088806\n",
      "Epoch 1053, Loss: 2.967617392539978, Final Batch Loss: 0.5715208053588867\n",
      "Epoch 1054, Loss: 2.9289673268795013, Final Batch Loss: 0.6253175139427185\n",
      "Epoch 1055, Loss: 2.8928636014461517, Final Batch Loss: 0.4768041670322418\n",
      "Epoch 1056, Loss: 2.9262054562568665, Final Batch Loss: 0.5747775435447693\n",
      "Epoch 1057, Loss: 2.968459904193878, Final Batch Loss: 0.6673880815505981\n",
      "Epoch 1058, Loss: 3.068417727947235, Final Batch Loss: 0.5890580415725708\n",
      "Epoch 1059, Loss: 2.9606581926345825, Final Batch Loss: 0.5623902082443237\n",
      "Epoch 1060, Loss: 3.1029842495918274, Final Batch Loss: 0.6058725118637085\n",
      "Epoch 1061, Loss: 2.9043978452682495, Final Batch Loss: 0.5497746467590332\n",
      "Epoch 1062, Loss: 2.7312115728855133, Final Batch Loss: 0.43503084778785706\n",
      "Epoch 1063, Loss: 2.982862949371338, Final Batch Loss: 0.5597783327102661\n",
      "Epoch 1064, Loss: 2.870508372783661, Final Batch Loss: 0.6051862239837646\n",
      "Epoch 1065, Loss: 2.8955782055854797, Final Batch Loss: 0.5964471697807312\n",
      "Epoch 1066, Loss: 2.849489450454712, Final Batch Loss: 0.5884783267974854\n",
      "Epoch 1067, Loss: 3.1389413475990295, Final Batch Loss: 0.6654873490333557\n",
      "Epoch 1068, Loss: 3.0639392137527466, Final Batch Loss: 0.680232048034668\n",
      "Epoch 1069, Loss: 2.7824178338050842, Final Batch Loss: 0.5147318840026855\n",
      "Epoch 1070, Loss: 3.015533924102783, Final Batch Loss: 0.6346108317375183\n",
      "Epoch 1071, Loss: 2.879700630903244, Final Batch Loss: 0.4183177649974823\n",
      "Epoch 1072, Loss: 2.8845012187957764, Final Batch Loss: 0.5387158393859863\n",
      "Epoch 1073, Loss: 3.0479522049427032, Final Batch Loss: 0.7579648494720459\n",
      "Epoch 1074, Loss: 2.897136092185974, Final Batch Loss: 0.5539296865463257\n",
      "Epoch 1075, Loss: 3.030986785888672, Final Batch Loss: 0.6822099089622498\n",
      "Epoch 1076, Loss: 3.2512835264205933, Final Batch Loss: 0.7685977220535278\n",
      "Epoch 1077, Loss: 2.9455353021621704, Final Batch Loss: 0.6390120983123779\n",
      "Epoch 1078, Loss: 2.941766142845154, Final Batch Loss: 0.673467755317688\n",
      "Epoch 1079, Loss: 2.9044540524482727, Final Batch Loss: 0.6026474833488464\n",
      "Epoch 1080, Loss: 2.937550961971283, Final Batch Loss: 0.6362488865852356\n",
      "Epoch 1081, Loss: 2.8977169394493103, Final Batch Loss: 0.46449732780456543\n",
      "Epoch 1082, Loss: 2.926708072423935, Final Batch Loss: 0.66170734167099\n",
      "Epoch 1083, Loss: 2.9586049616336823, Final Batch Loss: 0.5929884314537048\n",
      "Epoch 1084, Loss: 2.8980221450328827, Final Batch Loss: 0.6138783097267151\n",
      "Epoch 1085, Loss: 2.901156187057495, Final Batch Loss: 0.5206468105316162\n",
      "Epoch 1086, Loss: 3.003267288208008, Final Batch Loss: 0.6473096609115601\n",
      "Epoch 1087, Loss: 2.8440992534160614, Final Batch Loss: 0.48851296305656433\n",
      "Epoch 1088, Loss: 2.9168612360954285, Final Batch Loss: 0.644551694393158\n",
      "Epoch 1089, Loss: 2.8536031246185303, Final Batch Loss: 0.5521963238716125\n",
      "Epoch 1090, Loss: 2.8851237893104553, Final Batch Loss: 0.6921603083610535\n",
      "Epoch 1091, Loss: 2.874739110469818, Final Batch Loss: 0.5186600685119629\n",
      "Epoch 1092, Loss: 2.887199819087982, Final Batch Loss: 0.5705780386924744\n",
      "Epoch 1093, Loss: 2.717581033706665, Final Batch Loss: 0.5043098330497742\n",
      "Epoch 1094, Loss: 3.001903235912323, Final Batch Loss: 0.6788402795791626\n",
      "Epoch 1095, Loss: 2.835937023162842, Final Batch Loss: 0.6678749322891235\n",
      "Epoch 1096, Loss: 2.903262972831726, Final Batch Loss: 0.5196229219436646\n",
      "Epoch 1097, Loss: 2.853189140558243, Final Batch Loss: 0.6177713871002197\n",
      "Epoch 1098, Loss: 2.937281847000122, Final Batch Loss: 0.5954597592353821\n",
      "Epoch 1099, Loss: 2.9009101390838623, Final Batch Loss: 0.6173941493034363\n",
      "Epoch 1100, Loss: 3.076275587081909, Final Batch Loss: 0.6335837244987488\n",
      "Epoch 1101, Loss: 2.8032416105270386, Final Batch Loss: 0.5312572717666626\n",
      "Epoch 1102, Loss: 2.899465262889862, Final Batch Loss: 0.5356014966964722\n",
      "Epoch 1103, Loss: 3.012910157442093, Final Batch Loss: 0.7158012986183167\n",
      "Epoch 1104, Loss: 2.9286986589431763, Final Batch Loss: 0.6635375022888184\n",
      "Epoch 1105, Loss: 2.97400563955307, Final Batch Loss: 0.634394645690918\n",
      "Epoch 1106, Loss: 3.065671980381012, Final Batch Loss: 0.784584105014801\n",
      "Epoch 1107, Loss: 2.8849061727523804, Final Batch Loss: 0.5190094113349915\n",
      "Epoch 1108, Loss: 2.833406090736389, Final Batch Loss: 0.5580354332923889\n",
      "Epoch 1109, Loss: 2.8923218846321106, Final Batch Loss: 0.5022976994514465\n",
      "Epoch 1110, Loss: 2.82463401556015, Final Batch Loss: 0.5859847068786621\n",
      "Epoch 1111, Loss: 2.8898645639419556, Final Batch Loss: 0.6295759677886963\n",
      "Epoch 1112, Loss: 2.932568371295929, Final Batch Loss: 0.46199846267700195\n",
      "Epoch 1113, Loss: 3.0068072080612183, Final Batch Loss: 0.6678854823112488\n",
      "Epoch 1114, Loss: 2.9116395711898804, Final Batch Loss: 0.5072450041770935\n",
      "Epoch 1115, Loss: 2.987814247608185, Final Batch Loss: 0.6563974022865295\n",
      "Epoch 1116, Loss: 2.778907597064972, Final Batch Loss: 0.6311310529708862\n",
      "Epoch 1117, Loss: 2.9328807592391968, Final Batch Loss: 0.6112849116325378\n",
      "Epoch 1118, Loss: 2.8766375184059143, Final Batch Loss: 0.5531092882156372\n",
      "Epoch 1119, Loss: 2.9133625626564026, Final Batch Loss: 0.5815243721008301\n",
      "Epoch 1120, Loss: 2.874932825565338, Final Batch Loss: 0.5926344394683838\n",
      "Epoch 1121, Loss: 2.779923975467682, Final Batch Loss: 0.5525416731834412\n",
      "Epoch 1122, Loss: 2.9370222687721252, Final Batch Loss: 0.6903682947158813\n",
      "Epoch 1123, Loss: 2.895289957523346, Final Batch Loss: 0.5172988176345825\n",
      "Epoch 1124, Loss: 2.885011851787567, Final Batch Loss: 0.6408001780509949\n",
      "Epoch 1125, Loss: 2.8524477183818817, Final Batch Loss: 0.6053191423416138\n",
      "Epoch 1126, Loss: 2.8988490104675293, Final Batch Loss: 0.5669990181922913\n",
      "Epoch 1127, Loss: 2.932980000972748, Final Batch Loss: 0.5724103450775146\n",
      "Epoch 1128, Loss: 2.861428141593933, Final Batch Loss: 0.60041743516922\n",
      "Epoch 1129, Loss: 2.8216673731803894, Final Batch Loss: 0.5526145696640015\n",
      "Epoch 1130, Loss: 2.971819519996643, Final Batch Loss: 0.6482378840446472\n",
      "Epoch 1131, Loss: 2.6836231350898743, Final Batch Loss: 0.5788024067878723\n",
      "Epoch 1132, Loss: 2.7838887870311737, Final Batch Loss: 0.5672281980514526\n",
      "Epoch 1133, Loss: 2.7615070939064026, Final Batch Loss: 0.5623423457145691\n",
      "Epoch 1134, Loss: 2.9071966111660004, Final Batch Loss: 0.6305942535400391\n",
      "Epoch 1135, Loss: 3.0361632108688354, Final Batch Loss: 0.608601450920105\n",
      "Epoch 1136, Loss: 2.9629812240600586, Final Batch Loss: 0.6626186966896057\n",
      "Epoch 1137, Loss: 2.891602575778961, Final Batch Loss: 0.6160428524017334\n",
      "Epoch 1138, Loss: 2.812711477279663, Final Batch Loss: 0.48963063955307007\n",
      "Epoch 1139, Loss: 2.781063675880432, Final Batch Loss: 0.5002883076667786\n",
      "Epoch 1140, Loss: 2.901802361011505, Final Batch Loss: 0.5626929402351379\n",
      "Epoch 1141, Loss: 2.7454913854599, Final Batch Loss: 0.5183358788490295\n",
      "Epoch 1142, Loss: 2.8135742843151093, Final Batch Loss: 0.4905739724636078\n",
      "Epoch 1143, Loss: 2.939598500728607, Final Batch Loss: 0.5424647331237793\n",
      "Epoch 1144, Loss: 2.8460556864738464, Final Batch Loss: 0.5731989741325378\n",
      "Epoch 1145, Loss: 2.73930686712265, Final Batch Loss: 0.5916826128959656\n",
      "Epoch 1146, Loss: 2.8327895998954773, Final Batch Loss: 0.6721314787864685\n",
      "Epoch 1147, Loss: 2.785458564758301, Final Batch Loss: 0.5325824618339539\n",
      "Epoch 1148, Loss: 2.8339158296585083, Final Batch Loss: 0.5032192468643188\n",
      "Epoch 1149, Loss: 2.921201467514038, Final Batch Loss: 0.5729240775108337\n",
      "Epoch 1150, Loss: 3.0846100449562073, Final Batch Loss: 0.7507646083831787\n",
      "Epoch 1151, Loss: 2.7234906554222107, Final Batch Loss: 0.5000908970832825\n",
      "Epoch 1152, Loss: 2.9881375432014465, Final Batch Loss: 0.7946677803993225\n",
      "Epoch 1153, Loss: 2.8806894719600677, Final Batch Loss: 0.6052275896072388\n",
      "Epoch 1154, Loss: 2.7657611966133118, Final Batch Loss: 0.5808276534080505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1155, Loss: 2.896301358938217, Final Batch Loss: 0.4809984862804413\n",
      "Epoch 1156, Loss: 2.867403745651245, Final Batch Loss: 0.5683942437171936\n",
      "Epoch 1157, Loss: 2.7376624643802643, Final Batch Loss: 0.5613679885864258\n",
      "Epoch 1158, Loss: 2.7685019075870514, Final Batch Loss: 0.6312790513038635\n",
      "Epoch 1159, Loss: 2.833237648010254, Final Batch Loss: 0.5665853023529053\n",
      "Epoch 1160, Loss: 2.9841222167015076, Final Batch Loss: 0.6387326717376709\n",
      "Epoch 1161, Loss: 2.8596465587615967, Final Batch Loss: 0.6205906271934509\n",
      "Epoch 1162, Loss: 2.9290090203285217, Final Batch Loss: 0.630828320980072\n",
      "Epoch 1163, Loss: 2.821967363357544, Final Batch Loss: 0.5437429547309875\n",
      "Epoch 1164, Loss: 2.7418545484542847, Final Batch Loss: 0.6266277432441711\n",
      "Epoch 1165, Loss: 3.0604113340377808, Final Batch Loss: 0.7690367102622986\n",
      "Epoch 1166, Loss: 2.7810705602169037, Final Batch Loss: 0.47342249751091003\n",
      "Epoch 1167, Loss: 2.841449588537216, Final Batch Loss: 0.534015417098999\n",
      "Epoch 1168, Loss: 2.8724295496940613, Final Batch Loss: 0.5402694344520569\n",
      "Epoch 1169, Loss: 2.9673346281051636, Final Batch Loss: 0.6088531613349915\n",
      "Epoch 1170, Loss: 2.6814636290073395, Final Batch Loss: 0.6146438121795654\n",
      "Epoch 1171, Loss: 2.979351222515106, Final Batch Loss: 0.635037362575531\n",
      "Epoch 1172, Loss: 2.754518449306488, Final Batch Loss: 0.5528038740158081\n",
      "Epoch 1173, Loss: 2.871877431869507, Final Batch Loss: 0.6102234721183777\n",
      "Epoch 1174, Loss: 2.8232618868350983, Final Batch Loss: 0.6621105670928955\n",
      "Epoch 1175, Loss: 2.691660761833191, Final Batch Loss: 0.4753236472606659\n",
      "Epoch 1176, Loss: 2.7177881598472595, Final Batch Loss: 0.5588400363922119\n",
      "Epoch 1177, Loss: 2.82809516787529, Final Batch Loss: 0.6726762056350708\n",
      "Epoch 1178, Loss: 2.94338321685791, Final Batch Loss: 0.6590412259101868\n",
      "Epoch 1179, Loss: 2.849799692630768, Final Batch Loss: 0.6214370727539062\n",
      "Epoch 1180, Loss: 2.761454939842224, Final Batch Loss: 0.6024960279464722\n",
      "Epoch 1181, Loss: 2.714713156223297, Final Batch Loss: 0.5791677832603455\n",
      "Epoch 1182, Loss: 2.7600143551826477, Final Batch Loss: 0.5613312125205994\n",
      "Epoch 1183, Loss: 2.7481487095355988, Final Batch Loss: 0.5553246140480042\n",
      "Epoch 1184, Loss: 2.6764405965805054, Final Batch Loss: 0.535480260848999\n",
      "Epoch 1185, Loss: 2.7464677691459656, Final Batch Loss: 0.5589719414710999\n",
      "Epoch 1186, Loss: 2.7849608659744263, Final Batch Loss: 0.5414531826972961\n",
      "Epoch 1187, Loss: 2.9055325984954834, Final Batch Loss: 0.508074164390564\n",
      "Epoch 1188, Loss: 2.830732434988022, Final Batch Loss: 0.7273098230361938\n",
      "Epoch 1189, Loss: 2.925029218196869, Final Batch Loss: 0.6412420868873596\n",
      "Epoch 1190, Loss: 2.7036211788654327, Final Batch Loss: 0.5224431157112122\n",
      "Epoch 1191, Loss: 2.7190503776073456, Final Batch Loss: 0.5530940890312195\n",
      "Epoch 1192, Loss: 2.709075778722763, Final Batch Loss: 0.5368881225585938\n",
      "Epoch 1193, Loss: 2.8025126457214355, Final Batch Loss: 0.5512531399726868\n",
      "Epoch 1194, Loss: 2.7946700751781464, Final Batch Loss: 0.5812803506851196\n",
      "Epoch 1195, Loss: 2.8333149552345276, Final Batch Loss: 0.6114091873168945\n",
      "Epoch 1196, Loss: 2.938664197921753, Final Batch Loss: 0.5252591967582703\n",
      "Epoch 1197, Loss: 2.717047542333603, Final Batch Loss: 0.5381323099136353\n",
      "Epoch 1198, Loss: 2.7411195039749146, Final Batch Loss: 0.5280600190162659\n",
      "Epoch 1199, Loss: 2.7414864003658295, Final Batch Loss: 0.45443135499954224\n",
      "Epoch 1200, Loss: 2.714076280593872, Final Batch Loss: 0.5031336545944214\n",
      "Epoch 1201, Loss: 2.7053649723529816, Final Batch Loss: 0.49242904782295227\n",
      "Epoch 1202, Loss: 2.742908924818039, Final Batch Loss: 0.5306617021560669\n",
      "Epoch 1203, Loss: 2.8366445899009705, Final Batch Loss: 0.6315649151802063\n",
      "Epoch 1204, Loss: 2.6918838918209076, Final Batch Loss: 0.45428046584129333\n",
      "Epoch 1205, Loss: 2.737459361553192, Final Batch Loss: 0.4948183298110962\n",
      "Epoch 1206, Loss: 2.7847880721092224, Final Batch Loss: 0.6055184006690979\n",
      "Epoch 1207, Loss: 2.8579564690589905, Final Batch Loss: 0.5850661993026733\n",
      "Epoch 1208, Loss: 2.8017414212226868, Final Batch Loss: 0.5651063323020935\n",
      "Epoch 1209, Loss: 2.8410784304142, Final Batch Loss: 0.6486294865608215\n",
      "Epoch 1210, Loss: 2.8832740783691406, Final Batch Loss: 0.562375009059906\n",
      "Epoch 1211, Loss: 2.736787438392639, Final Batch Loss: 0.6220883727073669\n",
      "Epoch 1212, Loss: 2.7336525917053223, Final Batch Loss: 0.5429062843322754\n",
      "Epoch 1213, Loss: 2.850023627281189, Final Batch Loss: 0.600053608417511\n",
      "Epoch 1214, Loss: 2.6538149416446686, Final Batch Loss: 0.5404037237167358\n",
      "Epoch 1215, Loss: 2.88788765668869, Final Batch Loss: 0.61464923620224\n",
      "Epoch 1216, Loss: 2.6443484127521515, Final Batch Loss: 0.5283367037773132\n",
      "Epoch 1217, Loss: 2.7693629264831543, Final Batch Loss: 0.5051220655441284\n",
      "Epoch 1218, Loss: 2.751576602458954, Final Batch Loss: 0.5503396987915039\n",
      "Epoch 1219, Loss: 2.7955187559127808, Final Batch Loss: 0.42442500591278076\n",
      "Epoch 1220, Loss: 2.682314872741699, Final Batch Loss: 0.5645924806594849\n",
      "Epoch 1221, Loss: 2.717603027820587, Final Batch Loss: 0.5470820069313049\n",
      "Epoch 1222, Loss: 2.9032469987869263, Final Batch Loss: 0.6753079295158386\n",
      "Epoch 1223, Loss: 2.8015225529670715, Final Batch Loss: 0.41961073875427246\n",
      "Epoch 1224, Loss: 2.663182884454727, Final Batch Loss: 0.5541468858718872\n",
      "Epoch 1225, Loss: 2.8137571811676025, Final Batch Loss: 0.5707162022590637\n",
      "Epoch 1226, Loss: 2.6217707991600037, Final Batch Loss: 0.5065880417823792\n",
      "Epoch 1227, Loss: 2.701190173625946, Final Batch Loss: 0.5403821468353271\n",
      "Epoch 1228, Loss: 2.61926805973053, Final Batch Loss: 0.44256865978240967\n",
      "Epoch 1229, Loss: 2.7514894902706146, Final Batch Loss: 0.6168803572654724\n",
      "Epoch 1230, Loss: 2.5884608030319214, Final Batch Loss: 0.5123838782310486\n",
      "Epoch 1231, Loss: 2.769366145133972, Final Batch Loss: 0.5882819294929504\n",
      "Epoch 1232, Loss: 2.7293827533721924, Final Batch Loss: 0.5725047588348389\n",
      "Epoch 1233, Loss: 2.7955956161022186, Final Batch Loss: 0.487569659948349\n",
      "Epoch 1234, Loss: 2.628221720457077, Final Batch Loss: 0.5061608552932739\n",
      "Epoch 1235, Loss: 2.7161649465560913, Final Batch Loss: 0.47559598088264465\n",
      "Epoch 1236, Loss: 2.6854390799999237, Final Batch Loss: 0.5730642676353455\n",
      "Epoch 1237, Loss: 2.5887745916843414, Final Batch Loss: 0.47874513268470764\n",
      "Epoch 1238, Loss: 2.7703136205673218, Final Batch Loss: 0.54438716173172\n",
      "Epoch 1239, Loss: 2.780287206172943, Final Batch Loss: 0.5857111215591431\n",
      "Epoch 1240, Loss: 2.592684805393219, Final Batch Loss: 0.4284234642982483\n",
      "Epoch 1241, Loss: 2.738908886909485, Final Batch Loss: 0.5396613478660583\n",
      "Epoch 1242, Loss: 2.8206986784934998, Final Batch Loss: 0.5716846585273743\n",
      "Epoch 1243, Loss: 2.4764836132526398, Final Batch Loss: 0.3801218569278717\n",
      "Epoch 1244, Loss: 2.948981463909149, Final Batch Loss: 0.6291131973266602\n",
      "Epoch 1245, Loss: 2.7713002264499664, Final Batch Loss: 0.5868453979492188\n",
      "Epoch 1246, Loss: 2.8044790029525757, Final Batch Loss: 0.6141793131828308\n",
      "Epoch 1247, Loss: 2.7810704112052917, Final Batch Loss: 0.5678261518478394\n",
      "Epoch 1248, Loss: 2.867280066013336, Final Batch Loss: 0.6145457625389099\n",
      "Epoch 1249, Loss: 2.583222061395645, Final Batch Loss: 0.5161864757537842\n",
      "Epoch 1250, Loss: 2.666256695985794, Final Batch Loss: 0.6124814748764038\n",
      "Epoch 1251, Loss: 2.708553433418274, Final Batch Loss: 0.6335092186927795\n",
      "Epoch 1252, Loss: 2.677533209323883, Final Batch Loss: 0.5532865524291992\n",
      "Epoch 1253, Loss: 2.643949955701828, Final Batch Loss: 0.45700761675834656\n",
      "Epoch 1254, Loss: 2.6485891938209534, Final Batch Loss: 0.5458609461784363\n",
      "Epoch 1255, Loss: 2.7060374319553375, Final Batch Loss: 0.5142331719398499\n",
      "Epoch 1256, Loss: 2.7531175017356873, Final Batch Loss: 0.5289723873138428\n",
      "Epoch 1257, Loss: 2.7318532168865204, Final Batch Loss: 0.6117608547210693\n",
      "Epoch 1258, Loss: 2.861026167869568, Final Batch Loss: 0.6026577949523926\n",
      "Epoch 1259, Loss: 2.637983411550522, Final Batch Loss: 0.4539479613304138\n",
      "Epoch 1260, Loss: 2.904444992542267, Final Batch Loss: 0.5823599696159363\n",
      "Epoch 1261, Loss: 2.7533134818077087, Final Batch Loss: 0.55439293384552\n",
      "Epoch 1262, Loss: 2.863337755203247, Final Batch Loss: 0.5696713924407959\n",
      "Epoch 1263, Loss: 2.75161150097847, Final Batch Loss: 0.5946069359779358\n",
      "Epoch 1264, Loss: 2.62955304980278, Final Batch Loss: 0.5787612795829773\n",
      "Epoch 1265, Loss: 2.50817608833313, Final Batch Loss: 0.48915794491767883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1266, Loss: 2.6798292994499207, Final Batch Loss: 0.5488294363021851\n",
      "Epoch 1267, Loss: 2.6336103677749634, Final Batch Loss: 0.5457361340522766\n",
      "Epoch 1268, Loss: 2.760995388031006, Final Batch Loss: 0.6101383566856384\n",
      "Epoch 1269, Loss: 2.8635519444942474, Final Batch Loss: 0.5371689200401306\n",
      "Epoch 1270, Loss: 2.770381510257721, Final Batch Loss: 0.6156631112098694\n",
      "Epoch 1271, Loss: 2.7472756803035736, Final Batch Loss: 0.5478475689888\n",
      "Epoch 1272, Loss: 2.673118472099304, Final Batch Loss: 0.4543797969818115\n",
      "Epoch 1273, Loss: 2.5950333774089813, Final Batch Loss: 0.5628858804702759\n",
      "Epoch 1274, Loss: 2.614733576774597, Final Batch Loss: 0.6144158244132996\n",
      "Epoch 1275, Loss: 2.73029625415802, Final Batch Loss: 0.6021170020103455\n",
      "Epoch 1276, Loss: 2.7080191373825073, Final Batch Loss: 0.5523021817207336\n",
      "Epoch 1277, Loss: 2.7109895944595337, Final Batch Loss: 0.6091856360435486\n",
      "Epoch 1278, Loss: 2.7268825471401215, Final Batch Loss: 0.6799845695495605\n",
      "Epoch 1279, Loss: 2.730205237865448, Final Batch Loss: 0.5643156170845032\n",
      "Epoch 1280, Loss: 2.7945188879966736, Final Batch Loss: 0.6386908888816833\n",
      "Epoch 1281, Loss: 2.64451265335083, Final Batch Loss: 0.6387003064155579\n",
      "Epoch 1282, Loss: 2.62508025765419, Final Batch Loss: 0.522838294506073\n",
      "Epoch 1283, Loss: 2.67145773768425, Final Batch Loss: 0.5246454477310181\n",
      "Epoch 1284, Loss: 2.7502982020378113, Final Batch Loss: 0.570722758769989\n",
      "Epoch 1285, Loss: 2.6851807236671448, Final Batch Loss: 0.4348757266998291\n",
      "Epoch 1286, Loss: 2.8994662761688232, Final Batch Loss: 0.6952793598175049\n",
      "Epoch 1287, Loss: 2.8003598749637604, Final Batch Loss: 0.5999228358268738\n",
      "Epoch 1288, Loss: 2.761815309524536, Final Batch Loss: 0.5433297157287598\n",
      "Epoch 1289, Loss: 2.70021715760231, Final Batch Loss: 0.5644062757492065\n",
      "Epoch 1290, Loss: 2.6342727541923523, Final Batch Loss: 0.5028204917907715\n",
      "Epoch 1291, Loss: 2.7475095689296722, Final Batch Loss: 0.5941855311393738\n",
      "Epoch 1292, Loss: 2.6141751408576965, Final Batch Loss: 0.5203513503074646\n",
      "Epoch 1293, Loss: 2.7894893884658813, Final Batch Loss: 0.5467948317527771\n",
      "Epoch 1294, Loss: 2.564221531152725, Final Batch Loss: 0.4277508556842804\n",
      "Epoch 1295, Loss: 2.460969865322113, Final Batch Loss: 0.45620590448379517\n",
      "Epoch 1296, Loss: 2.657746762037277, Final Batch Loss: 0.48378095030784607\n",
      "Epoch 1297, Loss: 2.702479839324951, Final Batch Loss: 0.6117327809333801\n",
      "Epoch 1298, Loss: 2.6654073894023895, Final Batch Loss: 0.5804902911186218\n",
      "Epoch 1299, Loss: 2.6961063146591187, Final Batch Loss: 0.5980738997459412\n",
      "Epoch 1300, Loss: 2.6288156509399414, Final Batch Loss: 0.5416374802589417\n",
      "Epoch 1301, Loss: 2.60361310839653, Final Batch Loss: 0.5150659680366516\n",
      "Epoch 1302, Loss: 2.6890051662921906, Final Batch Loss: 0.5811659693717957\n",
      "Epoch 1303, Loss: 2.7218457460403442, Final Batch Loss: 0.5483300685882568\n",
      "Epoch 1304, Loss: 2.6612859964370728, Final Batch Loss: 0.5700336694717407\n",
      "Epoch 1305, Loss: 2.6249575316905975, Final Batch Loss: 0.5846010446548462\n",
      "Epoch 1306, Loss: 2.7547926902770996, Final Batch Loss: 0.5991899371147156\n",
      "Epoch 1307, Loss: 2.6805225610733032, Final Batch Loss: 0.5724716186523438\n",
      "Epoch 1308, Loss: 2.537107288837433, Final Batch Loss: 0.41064733266830444\n",
      "Epoch 1309, Loss: 2.6978269517421722, Final Batch Loss: 0.5281397700309753\n",
      "Epoch 1310, Loss: 2.7760503590106964, Final Batch Loss: 0.6217488646507263\n",
      "Epoch 1311, Loss: 2.7474509477615356, Final Batch Loss: 0.6003720760345459\n",
      "Epoch 1312, Loss: 2.6433854401111603, Final Batch Loss: 0.45072320103645325\n",
      "Epoch 1313, Loss: 2.6759945452213287, Final Batch Loss: 0.5104861855506897\n",
      "Epoch 1314, Loss: 2.637248992919922, Final Batch Loss: 0.5480680465698242\n",
      "Epoch 1315, Loss: 2.619987040758133, Final Batch Loss: 0.6157007813453674\n",
      "Epoch 1316, Loss: 2.7401408553123474, Final Batch Loss: 0.6455526351928711\n",
      "Epoch 1317, Loss: 2.6530202627182007, Final Batch Loss: 0.5051835179328918\n",
      "Epoch 1318, Loss: 2.6841476559638977, Final Batch Loss: 0.577614963054657\n",
      "Epoch 1319, Loss: 2.6330393254756927, Final Batch Loss: 0.540072500705719\n",
      "Epoch 1320, Loss: 2.800185948610306, Final Batch Loss: 0.5809013843536377\n",
      "Epoch 1321, Loss: 2.667027086019516, Final Batch Loss: 0.5627295970916748\n",
      "Epoch 1322, Loss: 2.6473950147628784, Final Batch Loss: 0.5096452236175537\n",
      "Epoch 1323, Loss: 2.7388209402561188, Final Batch Loss: 0.549899160861969\n",
      "Epoch 1324, Loss: 2.6243516504764557, Final Batch Loss: 0.5363305807113647\n",
      "Epoch 1325, Loss: 2.543584495782852, Final Batch Loss: 0.429866760969162\n",
      "Epoch 1326, Loss: 2.5845919847488403, Final Batch Loss: 0.4642353057861328\n",
      "Epoch 1327, Loss: 2.5876711010932922, Final Batch Loss: 0.44660475850105286\n",
      "Epoch 1328, Loss: 2.524728626012802, Final Batch Loss: 0.5295438170433044\n",
      "Epoch 1329, Loss: 2.5661013424396515, Final Batch Loss: 0.4506317675113678\n",
      "Epoch 1330, Loss: 2.521270275115967, Final Batch Loss: 0.5140131711959839\n",
      "Epoch 1331, Loss: 2.6385488510131836, Final Batch Loss: 0.4640994369983673\n",
      "Epoch 1332, Loss: 2.571031928062439, Final Batch Loss: 0.5009126663208008\n",
      "Epoch 1333, Loss: 2.594335198402405, Final Batch Loss: 0.564643383026123\n",
      "Epoch 1334, Loss: 2.5589618384838104, Final Batch Loss: 0.545266330242157\n",
      "Epoch 1335, Loss: 2.6972928941249847, Final Batch Loss: 0.721576452255249\n",
      "Epoch 1336, Loss: 2.7879273295402527, Final Batch Loss: 0.5103389620780945\n",
      "Epoch 1337, Loss: 2.625761777162552, Final Batch Loss: 0.4568256735801697\n",
      "Epoch 1338, Loss: 2.712992310523987, Final Batch Loss: 0.5176380276679993\n",
      "Epoch 1339, Loss: 2.535503000020981, Final Batch Loss: 0.39561811089515686\n",
      "Epoch 1340, Loss: 2.6416384279727936, Final Batch Loss: 0.5919336080551147\n",
      "Epoch 1341, Loss: 2.637335926294327, Final Batch Loss: 0.49357372522354126\n",
      "Epoch 1342, Loss: 2.5593194663524628, Final Batch Loss: 0.5140672922134399\n",
      "Epoch 1343, Loss: 2.5692284405231476, Final Batch Loss: 0.5676950216293335\n",
      "Epoch 1344, Loss: 2.600298583507538, Final Batch Loss: 0.4980272352695465\n",
      "Epoch 1345, Loss: 2.599082887172699, Final Batch Loss: 0.6718463897705078\n",
      "Epoch 1346, Loss: 2.5317312479019165, Final Batch Loss: 0.4832340180873871\n",
      "Epoch 1347, Loss: 2.6242667138576508, Final Batch Loss: 0.566240131855011\n",
      "Epoch 1348, Loss: 2.6364902555942535, Final Batch Loss: 0.5950738787651062\n",
      "Epoch 1349, Loss: 2.469234198331833, Final Batch Loss: 0.3999864459037781\n",
      "Epoch 1350, Loss: 2.6572630405426025, Final Batch Loss: 0.4977293312549591\n",
      "Epoch 1351, Loss: 2.488968461751938, Final Batch Loss: 0.45520514249801636\n",
      "Epoch 1352, Loss: 2.6101096868515015, Final Batch Loss: 0.4871925115585327\n",
      "Epoch 1353, Loss: 2.4833072423934937, Final Batch Loss: 0.5120512247085571\n",
      "Epoch 1354, Loss: 2.646070510149002, Final Batch Loss: 0.4660498797893524\n",
      "Epoch 1355, Loss: 2.7481140196323395, Final Batch Loss: 0.6400216221809387\n",
      "Epoch 1356, Loss: 2.726962983608246, Final Batch Loss: 0.5075662732124329\n",
      "Epoch 1357, Loss: 2.587462902069092, Final Batch Loss: 0.7010172009468079\n",
      "Epoch 1358, Loss: 2.5167544782161713, Final Batch Loss: 0.4608567953109741\n",
      "Epoch 1359, Loss: 2.5411900877952576, Final Batch Loss: 0.5629581809043884\n",
      "Epoch 1360, Loss: 2.676341950893402, Final Batch Loss: 0.6494961380958557\n",
      "Epoch 1361, Loss: 2.5084711611270905, Final Batch Loss: 0.5058966875076294\n",
      "Epoch 1362, Loss: 2.682302087545395, Final Batch Loss: 0.6117553114891052\n",
      "Epoch 1363, Loss: 2.607280969619751, Final Batch Loss: 0.5192082524299622\n",
      "Epoch 1364, Loss: 2.6668070554733276, Final Batch Loss: 0.4867885708808899\n",
      "Epoch 1365, Loss: 2.8295962810516357, Final Batch Loss: 0.8244101405143738\n",
      "Epoch 1366, Loss: 2.472615748643875, Final Batch Loss: 0.41966482996940613\n",
      "Epoch 1367, Loss: 2.743556886911392, Final Batch Loss: 0.6283493041992188\n",
      "Epoch 1368, Loss: 2.613925725221634, Final Batch Loss: 0.5018059015274048\n",
      "Epoch 1369, Loss: 2.5730301439762115, Final Batch Loss: 0.4403740465641022\n",
      "Epoch 1370, Loss: 2.6097192466259003, Final Batch Loss: 0.3964359164237976\n",
      "Epoch 1371, Loss: 2.7171091735363007, Final Batch Loss: 0.549890398979187\n",
      "Epoch 1372, Loss: 2.6766237616539, Final Batch Loss: 0.5801119208335876\n",
      "Epoch 1373, Loss: 2.6599684357643127, Final Batch Loss: 0.5585692524909973\n",
      "Epoch 1374, Loss: 2.558537244796753, Final Batch Loss: 0.4342491030693054\n",
      "Epoch 1375, Loss: 2.593533217906952, Final Batch Loss: 0.5135302543640137\n",
      "Epoch 1376, Loss: 2.3900267481803894, Final Batch Loss: 0.4770593047142029\n",
      "Epoch 1377, Loss: 2.5687951743602753, Final Batch Loss: 0.5546594262123108\n",
      "Epoch 1378, Loss: 2.590259850025177, Final Batch Loss: 0.5508131980895996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1379, Loss: 2.5139873325824738, Final Batch Loss: 0.5132690668106079\n",
      "Epoch 1380, Loss: 2.7807315587997437, Final Batch Loss: 0.7417539954185486\n",
      "Epoch 1381, Loss: 2.729605495929718, Final Batch Loss: 0.568581223487854\n",
      "Epoch 1382, Loss: 2.6692433059215546, Final Batch Loss: 0.49862897396087646\n",
      "Epoch 1383, Loss: 2.8238660991191864, Final Batch Loss: 0.7199200391769409\n",
      "Epoch 1384, Loss: 2.6768602430820465, Final Batch Loss: 0.517471432685852\n",
      "Epoch 1385, Loss: 2.6742600798606873, Final Batch Loss: 0.48369309306144714\n",
      "Epoch 1386, Loss: 2.5994542837142944, Final Batch Loss: 0.5620205402374268\n",
      "Epoch 1387, Loss: 2.6737462282180786, Final Batch Loss: 0.5860697031021118\n",
      "Epoch 1388, Loss: 2.576831340789795, Final Batch Loss: 0.5621113181114197\n",
      "Epoch 1389, Loss: 2.5800249874591827, Final Batch Loss: 0.536300003528595\n",
      "Epoch 1390, Loss: 2.4950228333473206, Final Batch Loss: 0.4747304320335388\n",
      "Epoch 1391, Loss: 2.6132689118385315, Final Batch Loss: 0.5422723293304443\n",
      "Epoch 1392, Loss: 2.493190675973892, Final Batch Loss: 0.44612905383110046\n",
      "Epoch 1393, Loss: 2.52238729596138, Final Batch Loss: 0.500442385673523\n",
      "Epoch 1394, Loss: 2.5780255794525146, Final Batch Loss: 0.4365044832229614\n",
      "Epoch 1395, Loss: 2.612082213163376, Final Batch Loss: 0.4659951627254486\n",
      "Epoch 1396, Loss: 2.6041166484355927, Final Batch Loss: 0.4661805331707001\n",
      "Epoch 1397, Loss: 2.463021397590637, Final Batch Loss: 0.5028054118156433\n",
      "Epoch 1398, Loss: 2.7553759813308716, Final Batch Loss: 0.5036270022392273\n",
      "Epoch 1399, Loss: 2.628086894750595, Final Batch Loss: 0.5237924456596375\n",
      "Epoch 1400, Loss: 2.525492548942566, Final Batch Loss: 0.5200456976890564\n",
      "Epoch 1401, Loss: 2.7073971331119537, Final Batch Loss: 0.5526121258735657\n",
      "Epoch 1402, Loss: 2.5679308474063873, Final Batch Loss: 0.5853570699691772\n",
      "Epoch 1403, Loss: 2.6173932552337646, Final Batch Loss: 0.5517535209655762\n",
      "Epoch 1404, Loss: 2.5984871685504913, Final Batch Loss: 0.4738125503063202\n",
      "Epoch 1405, Loss: 2.574145346879959, Final Batch Loss: 0.43818435072898865\n",
      "Epoch 1406, Loss: 2.47719269990921, Final Batch Loss: 0.44673025608062744\n",
      "Epoch 1407, Loss: 2.5015085637569427, Final Batch Loss: 0.4397251605987549\n",
      "Epoch 1408, Loss: 2.617315262556076, Final Batch Loss: 0.5241928100585938\n",
      "Epoch 1409, Loss: 2.5414240956306458, Final Batch Loss: 0.5286922454833984\n",
      "Epoch 1410, Loss: 2.637390047311783, Final Batch Loss: 0.541018009185791\n",
      "Epoch 1411, Loss: 2.52365306019783, Final Batch Loss: 0.45221689343452454\n",
      "Epoch 1412, Loss: 2.4635952711105347, Final Batch Loss: 0.44274523854255676\n",
      "Epoch 1413, Loss: 2.4649788439273834, Final Batch Loss: 0.4986535906791687\n",
      "Epoch 1414, Loss: 2.5909295678138733, Final Batch Loss: 0.5231392979621887\n",
      "Epoch 1415, Loss: 2.5732729732990265, Final Batch Loss: 0.47669413685798645\n",
      "Epoch 1416, Loss: 2.728726327419281, Final Batch Loss: 0.5915749073028564\n",
      "Epoch 1417, Loss: 2.5901432633399963, Final Batch Loss: 0.4748058319091797\n",
      "Epoch 1418, Loss: 2.5092835128307343, Final Batch Loss: 0.5105407238006592\n",
      "Epoch 1419, Loss: 2.504070609807968, Final Batch Loss: 0.46087947487831116\n",
      "Epoch 1420, Loss: 2.5562825798988342, Final Batch Loss: 0.5412620306015015\n",
      "Epoch 1421, Loss: 2.4219280779361725, Final Batch Loss: 0.5392745137214661\n",
      "Epoch 1422, Loss: 2.4832754731178284, Final Batch Loss: 0.44446831941604614\n",
      "Epoch 1423, Loss: 2.5185945630073547, Final Batch Loss: 0.42761480808258057\n",
      "Epoch 1424, Loss: 2.6817659735679626, Final Batch Loss: 0.6605527400970459\n",
      "Epoch 1425, Loss: 2.591837227344513, Final Batch Loss: 0.5914580225944519\n",
      "Epoch 1426, Loss: 2.531964272260666, Final Batch Loss: 0.5218607187271118\n",
      "Epoch 1427, Loss: 2.4682449102401733, Final Batch Loss: 0.5241300463676453\n",
      "Epoch 1428, Loss: 2.494061589241028, Final Batch Loss: 0.5052900910377502\n",
      "Epoch 1429, Loss: 2.5880039036273956, Final Batch Loss: 0.4632833003997803\n",
      "Epoch 1430, Loss: 2.682140827178955, Final Batch Loss: 0.5653201341629028\n",
      "Epoch 1431, Loss: 2.548259973526001, Final Batch Loss: 0.44496774673461914\n",
      "Epoch 1432, Loss: 2.6586191058158875, Final Batch Loss: 0.5417805910110474\n",
      "Epoch 1433, Loss: 2.6972025334835052, Final Batch Loss: 0.5547441244125366\n",
      "Epoch 1434, Loss: 2.4946294128894806, Final Batch Loss: 0.47244903445243835\n",
      "Epoch 1435, Loss: 2.449143350124359, Final Batch Loss: 0.47863075137138367\n",
      "Epoch 1436, Loss: 2.552610367536545, Final Batch Loss: 0.5210868716239929\n",
      "Epoch 1437, Loss: 2.3797034323215485, Final Batch Loss: 0.3734361231327057\n",
      "Epoch 1438, Loss: 2.6011027693748474, Final Batch Loss: 0.50425124168396\n",
      "Epoch 1439, Loss: 2.4779259264469147, Final Batch Loss: 0.4226699769496918\n",
      "Epoch 1440, Loss: 2.4204801321029663, Final Batch Loss: 0.5292621850967407\n",
      "Epoch 1441, Loss: 2.4331931471824646, Final Batch Loss: 0.4004811942577362\n",
      "Epoch 1442, Loss: 2.5139228105545044, Final Batch Loss: 0.542279839515686\n",
      "Epoch 1443, Loss: 2.6455573737621307, Final Batch Loss: 0.6213911175727844\n",
      "Epoch 1444, Loss: 2.4604448676109314, Final Batch Loss: 0.5373867750167847\n",
      "Epoch 1445, Loss: 2.483140617609024, Final Batch Loss: 0.4619676172733307\n",
      "Epoch 1446, Loss: 2.5602591037750244, Final Batch Loss: 0.4259929656982422\n",
      "Epoch 1447, Loss: 2.516684800386429, Final Batch Loss: 0.4083143174648285\n",
      "Epoch 1448, Loss: 2.549056053161621, Final Batch Loss: 0.531604528427124\n",
      "Epoch 1449, Loss: 2.4300642609596252, Final Batch Loss: 0.4767721891403198\n",
      "Epoch 1450, Loss: 2.586360901594162, Final Batch Loss: 0.5140634179115295\n",
      "Epoch 1451, Loss: 2.3862329721450806, Final Batch Loss: 0.4150128662586212\n",
      "Epoch 1452, Loss: 2.432176649570465, Final Batch Loss: 0.4570472538471222\n",
      "Epoch 1453, Loss: 2.588741213083267, Final Batch Loss: 0.551079511642456\n",
      "Epoch 1454, Loss: 2.6173974871635437, Final Batch Loss: 0.5460222959518433\n",
      "Epoch 1455, Loss: 2.5300212800502777, Final Batch Loss: 0.4478719234466553\n",
      "Epoch 1456, Loss: 2.6766919791698456, Final Batch Loss: 0.5324443578720093\n",
      "Epoch 1457, Loss: 2.4905203878879547, Final Batch Loss: 0.48060861229896545\n",
      "Epoch 1458, Loss: 2.552649140357971, Final Batch Loss: 0.4199085235595703\n",
      "Epoch 1459, Loss: 2.494178682565689, Final Batch Loss: 0.49324050545692444\n",
      "Epoch 1460, Loss: 2.621999144554138, Final Batch Loss: 0.5427149534225464\n",
      "Epoch 1461, Loss: 2.4942492246627808, Final Batch Loss: 0.47212886810302734\n",
      "Epoch 1462, Loss: 2.6296797692775726, Final Batch Loss: 0.5337473154067993\n",
      "Epoch 1463, Loss: 2.515030711889267, Final Batch Loss: 0.4936913847923279\n",
      "Epoch 1464, Loss: 2.5972610414028168, Final Batch Loss: 0.4993492364883423\n",
      "Epoch 1465, Loss: 2.55334210395813, Final Batch Loss: 0.5386472344398499\n",
      "Epoch 1466, Loss: 2.5081969797611237, Final Batch Loss: 0.4871583580970764\n",
      "Epoch 1467, Loss: 2.4436421394348145, Final Batch Loss: 0.5213108062744141\n",
      "Epoch 1468, Loss: 2.614720940589905, Final Batch Loss: 0.5713431239128113\n",
      "Epoch 1469, Loss: 2.5326240062713623, Final Batch Loss: 0.5313892960548401\n",
      "Epoch 1470, Loss: 2.4078763127326965, Final Batch Loss: 0.43553128838539124\n",
      "Epoch 1471, Loss: 2.562357723712921, Final Batch Loss: 0.5471215844154358\n",
      "Epoch 1472, Loss: 2.5172902941703796, Final Batch Loss: 0.5462877750396729\n",
      "Epoch 1473, Loss: 2.676696389913559, Final Batch Loss: 0.6330069899559021\n",
      "Epoch 1474, Loss: 2.4022292494773865, Final Batch Loss: 0.3225221335887909\n",
      "Epoch 1475, Loss: 2.5770203173160553, Final Batch Loss: 0.6081748008728027\n",
      "Epoch 1476, Loss: 2.481878399848938, Final Batch Loss: 0.5362647175788879\n",
      "Epoch 1477, Loss: 2.541117489337921, Final Batch Loss: 0.4294755458831787\n",
      "Epoch 1478, Loss: 2.5110042691230774, Final Batch Loss: 0.5360791087150574\n",
      "Epoch 1479, Loss: 2.567730665206909, Final Batch Loss: 0.6420983076095581\n",
      "Epoch 1480, Loss: 2.5136984288692474, Final Batch Loss: 0.4339483976364136\n",
      "Epoch 1481, Loss: 2.43713316321373, Final Batch Loss: 0.5282424688339233\n",
      "Epoch 1482, Loss: 2.575597256422043, Final Batch Loss: 0.4086669087409973\n",
      "Epoch 1483, Loss: 2.521404594182968, Final Batch Loss: 0.5296346545219421\n",
      "Epoch 1484, Loss: 2.5797911882400513, Final Batch Loss: 0.5169218182563782\n",
      "Epoch 1485, Loss: 2.5350480377674103, Final Batch Loss: 0.6381591558456421\n",
      "Epoch 1486, Loss: 2.56482070684433, Final Batch Loss: 0.5268197655677795\n",
      "Epoch 1487, Loss: 2.671890050172806, Final Batch Loss: 0.5018544793128967\n",
      "Epoch 1488, Loss: 2.409260094165802, Final Batch Loss: 0.4149167835712433\n",
      "Epoch 1489, Loss: 2.5209938883781433, Final Batch Loss: 0.5780873894691467\n",
      "Epoch 1490, Loss: 2.419065445661545, Final Batch Loss: 0.5594019889831543\n",
      "Epoch 1491, Loss: 2.574944704771042, Final Batch Loss: 0.43381166458129883\n",
      "Epoch 1492, Loss: 2.559626132249832, Final Batch Loss: 0.5531325340270996\n",
      "Epoch 1493, Loss: 2.3305167853832245, Final Batch Loss: 0.39312148094177246\n",
      "Epoch 1494, Loss: 2.481712907552719, Final Batch Loss: 0.4158475995063782\n",
      "Epoch 1495, Loss: 2.4634319245815277, Final Batch Loss: 0.47618818283081055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1496, Loss: 2.5504289865493774, Final Batch Loss: 0.5119957327842712\n",
      "Epoch 1497, Loss: 2.547998160123825, Final Batch Loss: 0.4045208990573883\n",
      "Epoch 1498, Loss: 2.5185290575027466, Final Batch Loss: 0.580098569393158\n",
      "Epoch 1499, Loss: 2.5189481377601624, Final Batch Loss: 0.5731987357139587\n",
      "Epoch 1500, Loss: 2.4288200736045837, Final Batch Loss: 0.4160624146461487\n",
      "Epoch 1501, Loss: 2.59072482585907, Final Batch Loss: 0.51444411277771\n",
      "Epoch 1502, Loss: 2.444754034280777, Final Batch Loss: 0.49860209226608276\n",
      "Epoch 1503, Loss: 2.5076126754283905, Final Batch Loss: 0.47035443782806396\n",
      "Epoch 1504, Loss: 2.422204375267029, Final Batch Loss: 0.55707186460495\n",
      "Epoch 1505, Loss: 2.510306477546692, Final Batch Loss: 0.4222504794597626\n",
      "Epoch 1506, Loss: 2.429598391056061, Final Batch Loss: 0.4461164176464081\n",
      "Epoch 1507, Loss: 2.4433659613132477, Final Batch Loss: 0.5335915684700012\n",
      "Epoch 1508, Loss: 2.566917836666107, Final Batch Loss: 0.5226025581359863\n",
      "Epoch 1509, Loss: 2.3406518697738647, Final Batch Loss: 0.48556792736053467\n",
      "Epoch 1510, Loss: 2.4729436933994293, Final Batch Loss: 0.5447773337364197\n",
      "Epoch 1511, Loss: 2.4757420122623444, Final Batch Loss: 0.4819011688232422\n",
      "Epoch 1512, Loss: 2.460190922021866, Final Batch Loss: 0.5342159867286682\n",
      "Epoch 1513, Loss: 2.454036444425583, Final Batch Loss: 0.5450595021247864\n",
      "Epoch 1514, Loss: 2.488899201154709, Final Batch Loss: 0.5212871432304382\n",
      "Epoch 1515, Loss: 2.486792504787445, Final Batch Loss: 0.43188759684562683\n",
      "Epoch 1516, Loss: 2.4975600838661194, Final Batch Loss: 0.449812114238739\n",
      "Epoch 1517, Loss: 2.5941317975521088, Final Batch Loss: 0.4838353097438812\n",
      "Epoch 1518, Loss: 2.467367857694626, Final Batch Loss: 0.42457661032676697\n",
      "Epoch 1519, Loss: 2.4930770099163055, Final Batch Loss: 0.4605451822280884\n",
      "Epoch 1520, Loss: 2.490222603082657, Final Batch Loss: 0.4390757977962494\n",
      "Epoch 1521, Loss: 2.5657801926136017, Final Batch Loss: 0.554672122001648\n",
      "Epoch 1522, Loss: 2.3773248195648193, Final Batch Loss: 0.5387198328971863\n",
      "Epoch 1523, Loss: 2.382506102323532, Final Batch Loss: 0.45959699153900146\n",
      "Epoch 1524, Loss: 2.6168307065963745, Final Batch Loss: 0.5447447299957275\n",
      "Epoch 1525, Loss: 2.492372363805771, Final Batch Loss: 0.5172613263130188\n",
      "Epoch 1526, Loss: 2.526819497346878, Final Batch Loss: 0.5227193832397461\n",
      "Epoch 1527, Loss: 2.3645215034484863, Final Batch Loss: 0.5410703420639038\n",
      "Epoch 1528, Loss: 2.427362382411957, Final Batch Loss: 0.6062146425247192\n",
      "Epoch 1529, Loss: 2.3952954709529877, Final Batch Loss: 0.38871997594833374\n",
      "Epoch 1530, Loss: 2.586012303829193, Final Batch Loss: 0.5099243521690369\n",
      "Epoch 1531, Loss: 2.347934752702713, Final Batch Loss: 0.45814570784568787\n",
      "Epoch 1532, Loss: 2.37094309926033, Final Batch Loss: 0.39635801315307617\n",
      "Epoch 1533, Loss: 2.5803425014019012, Final Batch Loss: 0.47234612703323364\n",
      "Epoch 1534, Loss: 2.4765637814998627, Final Batch Loss: 0.4970826208591461\n",
      "Epoch 1535, Loss: 2.418545037508011, Final Batch Loss: 0.3902629315853119\n",
      "Epoch 1536, Loss: 2.422255903482437, Final Batch Loss: 0.3788720667362213\n",
      "Epoch 1537, Loss: 2.4425446689128876, Final Batch Loss: 0.46709319949150085\n",
      "Epoch 1538, Loss: 2.5286344289779663, Final Batch Loss: 0.5412389039993286\n",
      "Epoch 1539, Loss: 2.4577005207538605, Final Batch Loss: 0.471078485250473\n",
      "Epoch 1540, Loss: 2.410463899374008, Final Batch Loss: 0.4551439583301544\n",
      "Epoch 1541, Loss: 2.485210508108139, Final Batch Loss: 0.4808025360107422\n",
      "Epoch 1542, Loss: 2.387899786233902, Final Batch Loss: 0.461294561624527\n",
      "Epoch 1543, Loss: 2.4503441751003265, Final Batch Loss: 0.5680835247039795\n",
      "Epoch 1544, Loss: 2.614824265241623, Final Batch Loss: 0.6213800311088562\n",
      "Epoch 1545, Loss: 2.5249049067497253, Final Batch Loss: 0.44826698303222656\n",
      "Epoch 1546, Loss: 2.5062217712402344, Final Batch Loss: 0.5302929878234863\n",
      "Epoch 1547, Loss: 2.4593693017959595, Final Batch Loss: 0.5359219312667847\n",
      "Epoch 1548, Loss: 2.5135703086853027, Final Batch Loss: 0.4884442090988159\n",
      "Epoch 1549, Loss: 2.453693240880966, Final Batch Loss: 0.4865042269229889\n",
      "Epoch 1550, Loss: 2.4682535529136658, Final Batch Loss: 0.5305740833282471\n",
      "Epoch 1551, Loss: 2.349378824234009, Final Batch Loss: 0.4511437714099884\n",
      "Epoch 1552, Loss: 2.3772048354148865, Final Batch Loss: 0.47721731662750244\n",
      "Epoch 1553, Loss: 2.39225971698761, Final Batch Loss: 0.46682772040367126\n",
      "Epoch 1554, Loss: 2.371752679347992, Final Batch Loss: 0.4789476990699768\n",
      "Epoch 1555, Loss: 2.363541692495346, Final Batch Loss: 0.4471873939037323\n",
      "Epoch 1556, Loss: 2.504576236009598, Final Batch Loss: 0.5423994064331055\n",
      "Epoch 1557, Loss: 2.3572501242160797, Final Batch Loss: 0.4523215889930725\n",
      "Epoch 1558, Loss: 2.374178171157837, Final Batch Loss: 0.4521327614784241\n",
      "Epoch 1559, Loss: 2.376959502696991, Final Batch Loss: 0.4210852086544037\n",
      "Epoch 1560, Loss: 2.5209275782108307, Final Batch Loss: 0.4407367706298828\n",
      "Epoch 1561, Loss: 2.4030576944351196, Final Batch Loss: 0.4067530333995819\n",
      "Epoch 1562, Loss: 2.5764093101024628, Final Batch Loss: 0.5419321656227112\n",
      "Epoch 1563, Loss: 2.5895063877105713, Final Batch Loss: 0.5117281675338745\n",
      "Epoch 1564, Loss: 2.5835816264152527, Final Batch Loss: 0.6078108549118042\n",
      "Epoch 1565, Loss: 2.4498525261878967, Final Batch Loss: 0.5312771797180176\n",
      "Epoch 1566, Loss: 2.4072110652923584, Final Batch Loss: 0.4385916590690613\n",
      "Epoch 1567, Loss: 2.555109828710556, Final Batch Loss: 0.6368863582611084\n",
      "Epoch 1568, Loss: 2.395976275205612, Final Batch Loss: 0.420990526676178\n",
      "Epoch 1569, Loss: 2.3847652971744537, Final Batch Loss: 0.3832088112831116\n",
      "Epoch 1570, Loss: 2.331967681646347, Final Batch Loss: 0.3825699985027313\n",
      "Epoch 1571, Loss: 2.445892244577408, Final Batch Loss: 0.5004802942276001\n",
      "Epoch 1572, Loss: 2.4685755372047424, Final Batch Loss: 0.46051910519599915\n",
      "Epoch 1573, Loss: 2.4546519219875336, Final Batch Loss: 0.4735560417175293\n",
      "Epoch 1574, Loss: 2.4182673394680023, Final Batch Loss: 0.5549846887588501\n",
      "Epoch 1575, Loss: 2.3240791261196136, Final Batch Loss: 0.40509337186813354\n",
      "Epoch 1576, Loss: 2.411073535680771, Final Batch Loss: 0.48409444093704224\n",
      "Epoch 1577, Loss: 2.3899922370910645, Final Batch Loss: 0.4040335416793823\n",
      "Epoch 1578, Loss: 2.4906791746616364, Final Batch Loss: 0.4937334656715393\n",
      "Epoch 1579, Loss: 2.431651920080185, Final Batch Loss: 0.4685589373111725\n",
      "Epoch 1580, Loss: 2.3924088776111603, Final Batch Loss: 0.4883711338043213\n",
      "Epoch 1581, Loss: 2.4196980595588684, Final Batch Loss: 0.49492886662483215\n",
      "Epoch 1582, Loss: 2.599795699119568, Final Batch Loss: 0.6564970016479492\n",
      "Epoch 1583, Loss: 2.478642851114273, Final Batch Loss: 0.47677159309387207\n",
      "Epoch 1584, Loss: 2.27082160115242, Final Batch Loss: 0.40304455161094666\n",
      "Epoch 1585, Loss: 2.5550276339054108, Final Batch Loss: 0.5485394597053528\n",
      "Epoch 1586, Loss: 2.4861113727092743, Final Batch Loss: 0.6156942248344421\n",
      "Epoch 1587, Loss: 2.4749514758586884, Final Batch Loss: 0.5582894682884216\n",
      "Epoch 1588, Loss: 2.462565302848816, Final Batch Loss: 0.41867977380752563\n",
      "Epoch 1589, Loss: 2.473032146692276, Final Batch Loss: 0.4900919497013092\n",
      "Epoch 1590, Loss: 2.339729964733124, Final Batch Loss: 0.44083601236343384\n",
      "Epoch 1591, Loss: 2.4108810126781464, Final Batch Loss: 0.4192555546760559\n",
      "Epoch 1592, Loss: 2.4871440827846527, Final Batch Loss: 0.5619844198226929\n",
      "Epoch 1593, Loss: 2.3919213116168976, Final Batch Loss: 0.5130932927131653\n",
      "Epoch 1594, Loss: 2.5349228978157043, Final Batch Loss: 0.7067034840583801\n",
      "Epoch 1595, Loss: 2.546470433473587, Final Batch Loss: 0.521292507648468\n",
      "Epoch 1596, Loss: 2.399853765964508, Final Batch Loss: 0.40513908863067627\n",
      "Epoch 1597, Loss: 2.3580870628356934, Final Batch Loss: 0.4318893551826477\n",
      "Epoch 1598, Loss: 2.4340803921222687, Final Batch Loss: 0.5234251618385315\n",
      "Epoch 1599, Loss: 2.2964427769184113, Final Batch Loss: 0.44949978590011597\n",
      "Epoch 1600, Loss: 2.5389733612537384, Final Batch Loss: 0.44935891032218933\n",
      "Epoch 1601, Loss: 2.4768108427524567, Final Batch Loss: 0.5028874278068542\n",
      "Epoch 1602, Loss: 2.4007153809070587, Final Batch Loss: 0.4483262300491333\n",
      "Epoch 1603, Loss: 2.4966051876544952, Final Batch Loss: 0.5453655123710632\n",
      "Epoch 1604, Loss: 2.4487435817718506, Final Batch Loss: 0.4842585027217865\n",
      "Epoch 1605, Loss: 2.322774589061737, Final Batch Loss: 0.42256370186805725\n",
      "Epoch 1606, Loss: 2.4334059059619904, Final Batch Loss: 0.5178900361061096\n",
      "Epoch 1607, Loss: 2.4255291521549225, Final Batch Loss: 0.5723831057548523\n",
      "Epoch 1608, Loss: 2.5360747575759888, Final Batch Loss: 0.623434841632843\n",
      "Epoch 1609, Loss: 2.474182277917862, Final Batch Loss: 0.5685095191001892\n",
      "Epoch 1610, Loss: 2.332544893026352, Final Batch Loss: 0.4033122658729553\n",
      "Epoch 1611, Loss: 2.238067328929901, Final Batch Loss: 0.42263323068618774\n",
      "Epoch 1612, Loss: 2.3962963819503784, Final Batch Loss: 0.44830870628356934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1613, Loss: 2.3015725016593933, Final Batch Loss: 0.487686425447464\n",
      "Epoch 1614, Loss: 2.4276354014873505, Final Batch Loss: 0.48247671127319336\n",
      "Epoch 1615, Loss: 2.2419897317886353, Final Batch Loss: 0.4678119122982025\n",
      "Epoch 1616, Loss: 2.3208191096782684, Final Batch Loss: 0.44398725032806396\n",
      "Epoch 1617, Loss: 2.339080899953842, Final Batch Loss: 0.480180948972702\n",
      "Epoch 1618, Loss: 2.295423775911331, Final Batch Loss: 0.39889419078826904\n",
      "Epoch 1619, Loss: 2.371717244386673, Final Batch Loss: 0.4542298913002014\n",
      "Epoch 1620, Loss: 2.4045105278491974, Final Batch Loss: 0.5459504127502441\n",
      "Epoch 1621, Loss: 2.2233402132987976, Final Batch Loss: 0.39367878437042236\n",
      "Epoch 1622, Loss: 2.326455593109131, Final Batch Loss: 0.42914560437202454\n",
      "Epoch 1623, Loss: 2.144169360399246, Final Batch Loss: 0.3678790032863617\n",
      "Epoch 1624, Loss: 2.355922818183899, Final Batch Loss: 0.4722745716571808\n",
      "Epoch 1625, Loss: 2.4873277246952057, Final Batch Loss: 0.4543198049068451\n",
      "Epoch 1626, Loss: 2.418139159679413, Final Batch Loss: 0.5520118474960327\n",
      "Epoch 1627, Loss: 2.426010638475418, Final Batch Loss: 0.5952329039573669\n",
      "Epoch 1628, Loss: 2.465414345264435, Final Batch Loss: 0.5648146867752075\n",
      "Epoch 1629, Loss: 2.406797468662262, Final Batch Loss: 0.43030881881713867\n",
      "Epoch 1630, Loss: 2.449477881193161, Final Batch Loss: 0.46964022517204285\n",
      "Epoch 1631, Loss: 2.38944935798645, Final Batch Loss: 0.4430232048034668\n",
      "Epoch 1632, Loss: 2.30465367436409, Final Batch Loss: 0.47053712606430054\n",
      "Epoch 1633, Loss: 2.294219493865967, Final Batch Loss: 0.43293437361717224\n",
      "Epoch 1634, Loss: 2.349411219358444, Final Batch Loss: 0.527304470539093\n",
      "Epoch 1635, Loss: 2.377045691013336, Final Batch Loss: 0.4260087311267853\n",
      "Epoch 1636, Loss: 2.290391504764557, Final Batch Loss: 0.36966630816459656\n",
      "Epoch 1637, Loss: 2.251807004213333, Final Batch Loss: 0.45998451113700867\n",
      "Epoch 1638, Loss: 2.28497177362442, Final Batch Loss: 0.48676514625549316\n",
      "Epoch 1639, Loss: 2.426624298095703, Final Batch Loss: 0.46916624903678894\n",
      "Epoch 1640, Loss: 2.400637000799179, Final Batch Loss: 0.49764156341552734\n",
      "Epoch 1641, Loss: 2.354086071252823, Final Batch Loss: 0.48253458738327026\n",
      "Epoch 1642, Loss: 2.328970968723297, Final Batch Loss: 0.4122914671897888\n",
      "Epoch 1643, Loss: 2.447341203689575, Final Batch Loss: 0.5385459661483765\n",
      "Epoch 1644, Loss: 2.5877253711223602, Final Batch Loss: 0.5621882081031799\n",
      "Epoch 1645, Loss: 2.4278650879859924, Final Batch Loss: 0.46712562441825867\n",
      "Epoch 1646, Loss: 2.4359895288944244, Final Batch Loss: 0.48750174045562744\n",
      "Epoch 1647, Loss: 2.570859372615814, Final Batch Loss: 0.516169011592865\n",
      "Epoch 1648, Loss: 2.284917414188385, Final Batch Loss: 0.3925207853317261\n",
      "Epoch 1649, Loss: 2.4332432448863983, Final Batch Loss: 0.41092604398727417\n",
      "Epoch 1650, Loss: 2.423060715198517, Final Batch Loss: 0.46907737851142883\n",
      "Epoch 1651, Loss: 2.5764975249767303, Final Batch Loss: 0.6783161759376526\n",
      "Epoch 1652, Loss: 2.5383092164993286, Final Batch Loss: 0.5482668876647949\n",
      "Epoch 1653, Loss: 2.2928289473056793, Final Batch Loss: 0.4767349362373352\n",
      "Epoch 1654, Loss: 2.3666542768478394, Final Batch Loss: 0.36712971329689026\n",
      "Epoch 1655, Loss: 2.4495193660259247, Final Batch Loss: 0.5097150802612305\n",
      "Epoch 1656, Loss: 2.269382894039154, Final Batch Loss: 0.45963364839553833\n",
      "Epoch 1657, Loss: 2.4087119102478027, Final Batch Loss: 0.4843730628490448\n",
      "Epoch 1658, Loss: 2.227080225944519, Final Batch Loss: 0.3946843147277832\n",
      "Epoch 1659, Loss: 2.406686544418335, Final Batch Loss: 0.534992516040802\n",
      "Epoch 1660, Loss: 2.3605569899082184, Final Batch Loss: 0.44879335165023804\n",
      "Epoch 1661, Loss: 2.4141911268234253, Final Batch Loss: 0.5360989570617676\n",
      "Epoch 1662, Loss: 2.419624775648117, Final Batch Loss: 0.5679280757904053\n",
      "Epoch 1663, Loss: 2.5085586607456207, Final Batch Loss: 0.47392725944519043\n",
      "Epoch 1664, Loss: 2.1598183512687683, Final Batch Loss: 0.30646729469299316\n",
      "Epoch 1665, Loss: 2.353999972343445, Final Batch Loss: 0.49265363812446594\n",
      "Epoch 1666, Loss: 2.372274696826935, Final Batch Loss: 0.46127957105636597\n",
      "Epoch 1667, Loss: 2.383364200592041, Final Batch Loss: 0.5070816278457642\n",
      "Epoch 1668, Loss: 2.329832911491394, Final Batch Loss: 0.43487825989723206\n",
      "Epoch 1669, Loss: 2.3733644783496857, Final Batch Loss: 0.5121387839317322\n",
      "Epoch 1670, Loss: 2.482586830854416, Final Batch Loss: 0.5388860702514648\n",
      "Epoch 1671, Loss: 2.497196078300476, Final Batch Loss: 0.5808886289596558\n",
      "Epoch 1672, Loss: 2.38135427236557, Final Batch Loss: 0.5666149258613586\n",
      "Epoch 1673, Loss: 2.3222152292728424, Final Batch Loss: 0.4637826979160309\n",
      "Epoch 1674, Loss: 2.4051860570907593, Final Batch Loss: 0.4231508672237396\n",
      "Epoch 1675, Loss: 2.4183216392993927, Final Batch Loss: 0.46368491649627686\n",
      "Epoch 1676, Loss: 2.5763162672519684, Final Batch Loss: 0.5820600986480713\n",
      "Epoch 1677, Loss: 2.4113950431346893, Final Batch Loss: 0.4984826445579529\n",
      "Epoch 1678, Loss: 2.3282483220100403, Final Batch Loss: 0.45488834381103516\n",
      "Epoch 1679, Loss: 2.356887012720108, Final Batch Loss: 0.47850435972213745\n",
      "Epoch 1680, Loss: 2.331371009349823, Final Batch Loss: 0.5039498209953308\n",
      "Epoch 1681, Loss: 2.4044412970542908, Final Batch Loss: 0.5385679006576538\n",
      "Epoch 1682, Loss: 2.3167662620544434, Final Batch Loss: 0.47550055384635925\n",
      "Epoch 1683, Loss: 2.256813257932663, Final Batch Loss: 0.33477821946144104\n",
      "Epoch 1684, Loss: 2.3796992897987366, Final Batch Loss: 0.515525221824646\n",
      "Epoch 1685, Loss: 2.3531911373138428, Final Batch Loss: 0.37363332509994507\n",
      "Epoch 1686, Loss: 2.4901958405971527, Final Batch Loss: 0.527720034122467\n",
      "Epoch 1687, Loss: 2.399873822927475, Final Batch Loss: 0.45899710059165955\n",
      "Epoch 1688, Loss: 2.3158975541591644, Final Batch Loss: 0.5044384598731995\n",
      "Epoch 1689, Loss: 2.3767883479595184, Final Batch Loss: 0.4509316384792328\n",
      "Epoch 1690, Loss: 2.2629068791866302, Final Batch Loss: 0.40277132391929626\n",
      "Epoch 1691, Loss: 2.229936718940735, Final Batch Loss: 0.42842262983322144\n",
      "Epoch 1692, Loss: 2.4847167432308197, Final Batch Loss: 0.5440320372581482\n",
      "Epoch 1693, Loss: 2.3342716991901398, Final Batch Loss: 0.3700324296951294\n",
      "Epoch 1694, Loss: 2.4502148032188416, Final Batch Loss: 0.4695589244365692\n",
      "Epoch 1695, Loss: 2.390516608953476, Final Batch Loss: 0.4792490005493164\n",
      "Epoch 1696, Loss: 2.309681922197342, Final Batch Loss: 0.42866650223731995\n",
      "Epoch 1697, Loss: 2.2670691907405853, Final Batch Loss: 0.413089781999588\n",
      "Epoch 1698, Loss: 2.3199402689933777, Final Batch Loss: 0.4698881506919861\n",
      "Epoch 1699, Loss: 2.3098317086696625, Final Batch Loss: 0.4808211624622345\n",
      "Epoch 1700, Loss: 2.267268270254135, Final Batch Loss: 0.38879072666168213\n",
      "Epoch 1701, Loss: 2.25087508559227, Final Batch Loss: 0.39066457748413086\n",
      "Epoch 1702, Loss: 2.155481070280075, Final Batch Loss: 0.3503773510456085\n",
      "Epoch 1703, Loss: 2.2720110416412354, Final Batch Loss: 0.44627413153648376\n",
      "Epoch 1704, Loss: 2.3623529970645905, Final Batch Loss: 0.47878143191337585\n",
      "Epoch 1705, Loss: 2.3705891370773315, Final Batch Loss: 0.5274147391319275\n",
      "Epoch 1706, Loss: 2.4208977222442627, Final Batch Loss: 0.5616879463195801\n",
      "Epoch 1707, Loss: 2.228568434715271, Final Batch Loss: 0.38236114382743835\n",
      "Epoch 1708, Loss: 2.2804932594299316, Final Batch Loss: 0.5096068978309631\n",
      "Epoch 1709, Loss: 2.203772634267807, Final Batch Loss: 0.486312597990036\n",
      "Epoch 1710, Loss: 2.3265515565872192, Final Batch Loss: 0.5020561218261719\n",
      "Epoch 1711, Loss: 2.302855223417282, Final Batch Loss: 0.41374126076698303\n",
      "Epoch 1712, Loss: 2.2361100018024445, Final Batch Loss: 0.3914954960346222\n",
      "Epoch 1713, Loss: 2.383913815021515, Final Batch Loss: 0.4306475818157196\n",
      "Epoch 1714, Loss: 2.1809397637844086, Final Batch Loss: 0.3633131682872772\n",
      "Epoch 1715, Loss: 2.3211074769496918, Final Batch Loss: 0.5537201166152954\n",
      "Epoch 1716, Loss: 2.3359222412109375, Final Batch Loss: 0.4321841597557068\n",
      "Epoch 1717, Loss: 2.3146116137504578, Final Batch Loss: 0.44251197576522827\n",
      "Epoch 1718, Loss: 2.195283830165863, Final Batch Loss: 0.47603780031204224\n",
      "Epoch 1719, Loss: 2.2859977185726166, Final Batch Loss: 0.48327136039733887\n",
      "Epoch 1720, Loss: 2.331006556749344, Final Batch Loss: 0.4453530013561249\n",
      "Epoch 1721, Loss: 2.451898008584976, Final Batch Loss: 0.5467689037322998\n",
      "Epoch 1722, Loss: 2.361646056175232, Final Batch Loss: 0.5666384696960449\n",
      "Epoch 1723, Loss: 2.2557327449321747, Final Batch Loss: 0.3280488848686218\n",
      "Epoch 1724, Loss: 2.3206394612789154, Final Batch Loss: 0.42071980237960815\n",
      "Epoch 1725, Loss: 2.2447278201580048, Final Batch Loss: 0.4891520142555237\n",
      "Epoch 1726, Loss: 2.2958357334136963, Final Batch Loss: 0.4584023356437683\n",
      "Epoch 1727, Loss: 2.2712399065494537, Final Batch Loss: 0.43178635835647583\n",
      "Epoch 1728, Loss: 2.37247297167778, Final Batch Loss: 0.38141554594039917\n",
      "Epoch 1729, Loss: 2.292430192232132, Final Batch Loss: 0.382046639919281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1730, Loss: 2.2707930505275726, Final Batch Loss: 0.47852203249931335\n",
      "Epoch 1731, Loss: 2.3467207551002502, Final Batch Loss: 0.4980868101119995\n",
      "Epoch 1732, Loss: 2.3360427916049957, Final Batch Loss: 0.457805335521698\n",
      "Epoch 1733, Loss: 2.2703398764133453, Final Batch Loss: 0.40819254517555237\n",
      "Epoch 1734, Loss: 2.409203886985779, Final Batch Loss: 0.5114878416061401\n",
      "Epoch 1735, Loss: 2.317803055047989, Final Batch Loss: 0.4563794434070587\n",
      "Epoch 1736, Loss: 2.182372748851776, Final Batch Loss: 0.4646655321121216\n",
      "Epoch 1737, Loss: 2.3540885150432587, Final Batch Loss: 0.49831274151802063\n",
      "Epoch 1738, Loss: 2.3473004698753357, Final Batch Loss: 0.5782650709152222\n",
      "Epoch 1739, Loss: 2.3812696039676666, Final Batch Loss: 0.5573617815971375\n",
      "Epoch 1740, Loss: 2.3325891196727753, Final Batch Loss: 0.4783039093017578\n",
      "Epoch 1741, Loss: 2.3402983844280243, Final Batch Loss: 0.45291948318481445\n",
      "Epoch 1742, Loss: 2.386193633079529, Final Batch Loss: 0.5165918469429016\n",
      "Epoch 1743, Loss: 2.2989481985569, Final Batch Loss: 0.4661061763763428\n",
      "Epoch 1744, Loss: 2.2359791696071625, Final Batch Loss: 0.46962064504623413\n",
      "Epoch 1745, Loss: 2.218038409948349, Final Batch Loss: 0.4698021113872528\n",
      "Epoch 1746, Loss: 2.360574573278427, Final Batch Loss: 0.47069165110588074\n",
      "Epoch 1747, Loss: 2.380637466907501, Final Batch Loss: 0.5563175082206726\n",
      "Epoch 1748, Loss: 2.3140035569667816, Final Batch Loss: 0.5478299856185913\n",
      "Epoch 1749, Loss: 2.3028685450553894, Final Batch Loss: 0.48461076617240906\n",
      "Epoch 1750, Loss: 2.266059249639511, Final Batch Loss: 0.38673365116119385\n",
      "Epoch 1751, Loss: 2.3773865401744843, Final Batch Loss: 0.5476521253585815\n",
      "Epoch 1752, Loss: 2.4061228930950165, Final Batch Loss: 0.3829452395439148\n",
      "Epoch 1753, Loss: 2.533279240131378, Final Batch Loss: 0.45134618878364563\n",
      "Epoch 1754, Loss: 2.204605460166931, Final Batch Loss: 0.45639821887016296\n",
      "Epoch 1755, Loss: 2.351740449666977, Final Batch Loss: 0.4830479621887207\n",
      "Epoch 1756, Loss: 2.235350579023361, Final Batch Loss: 0.4201243817806244\n",
      "Epoch 1757, Loss: 2.2371692061424255, Final Batch Loss: 0.43523314595222473\n",
      "Epoch 1758, Loss: 2.211118072271347, Final Batch Loss: 0.459504634141922\n",
      "Epoch 1759, Loss: 2.354438066482544, Final Batch Loss: 0.474605530500412\n",
      "Epoch 1760, Loss: 2.220632880926132, Final Batch Loss: 0.38510823249816895\n",
      "Epoch 1761, Loss: 2.345471143722534, Final Batch Loss: 0.5377368330955505\n",
      "Epoch 1762, Loss: 2.377707779407501, Final Batch Loss: 0.37966087460517883\n",
      "Epoch 1763, Loss: 2.3064375519752502, Final Batch Loss: 0.43620017170906067\n",
      "Epoch 1764, Loss: 2.24471777677536, Final Batch Loss: 0.34422919154167175\n",
      "Epoch 1765, Loss: 2.170205742120743, Final Batch Loss: 0.37083542346954346\n",
      "Epoch 1766, Loss: 2.2463975846767426, Final Batch Loss: 0.39105844497680664\n",
      "Epoch 1767, Loss: 2.3359287679195404, Final Batch Loss: 0.5290161967277527\n",
      "Epoch 1768, Loss: 2.337670385837555, Final Batch Loss: 0.49457892775535583\n",
      "Epoch 1769, Loss: 2.2764889001846313, Final Batch Loss: 0.3512229025363922\n",
      "Epoch 1770, Loss: 2.2473556995391846, Final Batch Loss: 0.35718679428100586\n",
      "Epoch 1771, Loss: 2.4519758820533752, Final Batch Loss: 0.5690008997917175\n",
      "Epoch 1772, Loss: 2.282083958387375, Final Batch Loss: 0.5286889672279358\n",
      "Epoch 1773, Loss: 2.2998328804969788, Final Batch Loss: 0.4867706894874573\n",
      "Epoch 1774, Loss: 2.291523367166519, Final Batch Loss: 0.41712337732315063\n",
      "Epoch 1775, Loss: 2.3297118842601776, Final Batch Loss: 0.43423962593078613\n",
      "Epoch 1776, Loss: 2.389050453901291, Final Batch Loss: 0.4564724564552307\n",
      "Epoch 1777, Loss: 2.2871078848838806, Final Batch Loss: 0.5039604902267456\n",
      "Epoch 1778, Loss: 2.350369304418564, Final Batch Loss: 0.5767995715141296\n",
      "Epoch 1779, Loss: 2.2763134837150574, Final Batch Loss: 0.45320287346839905\n",
      "Epoch 1780, Loss: 2.4515220522880554, Final Batch Loss: 0.4983338415622711\n",
      "Epoch 1781, Loss: 2.3475701808929443, Final Batch Loss: 0.4319445788860321\n",
      "Epoch 1782, Loss: 2.2618786692619324, Final Batch Loss: 0.5029846429824829\n",
      "Epoch 1783, Loss: 2.357104241847992, Final Batch Loss: 0.4852282404899597\n",
      "Epoch 1784, Loss: 2.252063363790512, Final Batch Loss: 0.4367120862007141\n",
      "Epoch 1785, Loss: 2.2944087982177734, Final Batch Loss: 0.40376943349838257\n",
      "Epoch 1786, Loss: 2.2272181808948517, Final Batch Loss: 0.39503511786460876\n",
      "Epoch 1787, Loss: 2.1614255607128143, Final Batch Loss: 0.37337613105773926\n",
      "Epoch 1788, Loss: 2.357301414012909, Final Batch Loss: 0.5091937184333801\n",
      "Epoch 1789, Loss: 2.2900012731552124, Final Batch Loss: 0.41492989659309387\n",
      "Epoch 1790, Loss: 2.380126178264618, Final Batch Loss: 0.436253160238266\n",
      "Epoch 1791, Loss: 2.223924845457077, Final Batch Loss: 0.41961470246315\n",
      "Epoch 1792, Loss: 2.304395854473114, Final Batch Loss: 0.47571811079978943\n",
      "Epoch 1793, Loss: 2.192903459072113, Final Batch Loss: 0.35790374875068665\n",
      "Epoch 1794, Loss: 2.2013233602046967, Final Batch Loss: 0.3543606102466583\n",
      "Epoch 1795, Loss: 2.184244751930237, Final Batch Loss: 0.39417263865470886\n",
      "Epoch 1796, Loss: 2.4606403410434723, Final Batch Loss: 0.5683987736701965\n",
      "Epoch 1797, Loss: 2.269147604703903, Final Batch Loss: 0.5181493163108826\n",
      "Epoch 1798, Loss: 2.3113734126091003, Final Batch Loss: 0.43680518865585327\n",
      "Epoch 1799, Loss: 2.3413082659244537, Final Batch Loss: 0.4520282745361328\n",
      "Epoch 1800, Loss: 2.3171172738075256, Final Batch Loss: 0.4844331443309784\n",
      "Epoch 1801, Loss: 2.2536779642105103, Final Batch Loss: 0.5744938850402832\n",
      "Epoch 1802, Loss: 2.4399569928646088, Final Batch Loss: 0.5858463644981384\n",
      "Epoch 1803, Loss: 2.2031993567943573, Final Batch Loss: 0.4083285331726074\n",
      "Epoch 1804, Loss: 2.2511489391326904, Final Batch Loss: 0.40715375542640686\n",
      "Epoch 1805, Loss: 2.214932382106781, Final Batch Loss: 0.4724940359592438\n",
      "Epoch 1806, Loss: 2.3272273540496826, Final Batch Loss: 0.572360098361969\n",
      "Epoch 1807, Loss: 2.302351236343384, Final Batch Loss: 0.504214346408844\n",
      "Epoch 1808, Loss: 2.2449434399604797, Final Batch Loss: 0.5132158398628235\n",
      "Epoch 1809, Loss: 2.2764912843704224, Final Batch Loss: 0.488392174243927\n",
      "Epoch 1810, Loss: 2.360787719488144, Final Batch Loss: 0.556276261806488\n",
      "Epoch 1811, Loss: 2.2330168187618256, Final Batch Loss: 0.4828493297100067\n",
      "Epoch 1812, Loss: 2.2427251040935516, Final Batch Loss: 0.2960418164730072\n",
      "Epoch 1813, Loss: 2.1750066578388214, Final Batch Loss: 0.4521864354610443\n",
      "Epoch 1814, Loss: 2.231319159269333, Final Batch Loss: 0.3984970152378082\n",
      "Epoch 1815, Loss: 2.266971856355667, Final Batch Loss: 0.3697168827056885\n",
      "Epoch 1816, Loss: 2.2979621589183807, Final Batch Loss: 0.4922144412994385\n",
      "Epoch 1817, Loss: 2.1516274511814117, Final Batch Loss: 0.49638035893440247\n",
      "Epoch 1818, Loss: 2.1649582982063293, Final Batch Loss: 0.4595842659473419\n",
      "Epoch 1819, Loss: 2.202767014503479, Final Batch Loss: 0.40048107504844666\n",
      "Epoch 1820, Loss: 2.366141527891159, Final Batch Loss: 0.45620155334472656\n",
      "Epoch 1821, Loss: 2.173875331878662, Final Batch Loss: 0.4220915138721466\n",
      "Epoch 1822, Loss: 2.3563084304332733, Final Batch Loss: 0.45794129371643066\n",
      "Epoch 1823, Loss: 2.2847419679164886, Final Batch Loss: 0.43427616357803345\n",
      "Epoch 1824, Loss: 2.2996707558631897, Final Batch Loss: 0.4349982440471649\n",
      "Epoch 1825, Loss: 2.2973251044750214, Final Batch Loss: 0.5119945406913757\n",
      "Epoch 1826, Loss: 2.2502535581588745, Final Batch Loss: 0.5326133966445923\n",
      "Epoch 1827, Loss: 2.320666879415512, Final Batch Loss: 0.4142376184463501\n",
      "Epoch 1828, Loss: 2.2287099361419678, Final Batch Loss: 0.5066617131233215\n",
      "Epoch 1829, Loss: 2.2086448073387146, Final Batch Loss: 0.4725468158721924\n",
      "Epoch 1830, Loss: 2.1897288858890533, Final Batch Loss: 0.39368966221809387\n",
      "Epoch 1831, Loss: 2.3308143615722656, Final Batch Loss: 0.4973730146884918\n",
      "Epoch 1832, Loss: 2.2567046880722046, Final Batch Loss: 0.3966591954231262\n",
      "Epoch 1833, Loss: 2.208921432495117, Final Batch Loss: 0.3943369686603546\n",
      "Epoch 1834, Loss: 2.297796905040741, Final Batch Loss: 0.5715053677558899\n",
      "Epoch 1835, Loss: 2.2676146924495697, Final Batch Loss: 0.5301448702812195\n",
      "Epoch 1836, Loss: 2.2329567968845367, Final Batch Loss: 0.4483320116996765\n",
      "Epoch 1837, Loss: 2.228222370147705, Final Batch Loss: 0.3782670497894287\n",
      "Epoch 1838, Loss: 2.286092072725296, Final Batch Loss: 0.475312739610672\n",
      "Epoch 1839, Loss: 2.266317218542099, Final Batch Loss: 0.5591648817062378\n",
      "Epoch 1840, Loss: 2.190741866827011, Final Batch Loss: 0.45067837834358215\n",
      "Epoch 1841, Loss: 2.182621419429779, Final Batch Loss: 0.3855959177017212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1842, Loss: 2.3538782596588135, Final Batch Loss: 0.4392275810241699\n",
      "Epoch 1843, Loss: 2.3160497546195984, Final Batch Loss: 0.4304245710372925\n",
      "Epoch 1844, Loss: 2.2658776938915253, Final Batch Loss: 0.4722163677215576\n",
      "Epoch 1845, Loss: 2.2681615948677063, Final Batch Loss: 0.4542742073535919\n",
      "Epoch 1846, Loss: 2.218611478805542, Final Batch Loss: 0.4413095712661743\n",
      "Epoch 1847, Loss: 2.1418673396110535, Final Batch Loss: 0.35489538311958313\n",
      "Epoch 1848, Loss: 2.269545257091522, Final Batch Loss: 0.4532531201839447\n",
      "Epoch 1849, Loss: 2.199875622987747, Final Batch Loss: 0.4589178264141083\n",
      "Epoch 1850, Loss: 2.2290307879447937, Final Batch Loss: 0.4051359295845032\n",
      "Epoch 1851, Loss: 2.1879079937934875, Final Batch Loss: 0.4149283766746521\n",
      "Epoch 1852, Loss: 2.341221898794174, Final Batch Loss: 0.40547776222229004\n",
      "Epoch 1853, Loss: 2.367886543273926, Final Batch Loss: 0.3900957405567169\n",
      "Epoch 1854, Loss: 2.222986578941345, Final Batch Loss: 0.4334370195865631\n",
      "Epoch 1855, Loss: 2.1912454664707184, Final Batch Loss: 0.4089522659778595\n",
      "Epoch 1856, Loss: 2.2403936088085175, Final Batch Loss: 0.4541698694229126\n",
      "Epoch 1857, Loss: 2.2915819883346558, Final Batch Loss: 0.4129908084869385\n",
      "Epoch 1858, Loss: 2.095950812101364, Final Batch Loss: 0.3994769752025604\n",
      "Epoch 1859, Loss: 2.3364342749118805, Final Batch Loss: 0.49467599391937256\n",
      "Epoch 1860, Loss: 2.21484237909317, Final Batch Loss: 0.488884299993515\n",
      "Epoch 1861, Loss: 2.2202998995780945, Final Batch Loss: 0.4763025641441345\n",
      "Epoch 1862, Loss: 2.095768064260483, Final Batch Loss: 0.38754671812057495\n",
      "Epoch 1863, Loss: 2.133269965648651, Final Batch Loss: 0.3445456027984619\n",
      "Epoch 1864, Loss: 2.2167408168315887, Final Batch Loss: 0.4366123676300049\n",
      "Epoch 1865, Loss: 2.527935177087784, Final Batch Loss: 0.47560834884643555\n",
      "Epoch 1866, Loss: 2.3461111783981323, Final Batch Loss: 0.5057082176208496\n",
      "Epoch 1867, Loss: 2.2867674827575684, Final Batch Loss: 0.4595755636692047\n",
      "Epoch 1868, Loss: 2.133353680372238, Final Batch Loss: 0.47412729263305664\n",
      "Epoch 1869, Loss: 2.2687965631484985, Final Batch Loss: 0.47115957736968994\n",
      "Epoch 1870, Loss: 2.2625729143619537, Final Batch Loss: 0.44847455620765686\n",
      "Epoch 1871, Loss: 2.2399949431419373, Final Batch Loss: 0.5121706128120422\n",
      "Epoch 1872, Loss: 2.3575360476970673, Final Batch Loss: 0.5622292160987854\n",
      "Epoch 1873, Loss: 2.174138605594635, Final Batch Loss: 0.5174492597579956\n",
      "Epoch 1874, Loss: 2.3680744767189026, Final Batch Loss: 0.5966442823410034\n",
      "Epoch 1875, Loss: 2.0705337822437286, Final Batch Loss: 0.36400192975997925\n",
      "Epoch 1876, Loss: 2.2026429772377014, Final Batch Loss: 0.5040751099586487\n",
      "Epoch 1877, Loss: 2.1681590378284454, Final Batch Loss: 0.3760361671447754\n",
      "Epoch 1878, Loss: 2.0925974249839783, Final Batch Loss: 0.4909561276435852\n",
      "Epoch 1879, Loss: 2.1950255930423737, Final Batch Loss: 0.4311371445655823\n",
      "Epoch 1880, Loss: 2.200081527233124, Final Batch Loss: 0.44261080026626587\n",
      "Epoch 1881, Loss: 2.0999531745910645, Final Batch Loss: 0.3709626793861389\n",
      "Epoch 1882, Loss: 2.1112319827079773, Final Batch Loss: 0.4087681472301483\n",
      "Epoch 1883, Loss: 2.101696252822876, Final Batch Loss: 0.39089295268058777\n",
      "Epoch 1884, Loss: 2.239581197500229, Final Batch Loss: 0.38414984941482544\n",
      "Epoch 1885, Loss: 2.26216584444046, Final Batch Loss: 0.4304506182670593\n",
      "Epoch 1886, Loss: 2.276632696390152, Final Batch Loss: 0.5089386105537415\n",
      "Epoch 1887, Loss: 2.2475055754184723, Final Batch Loss: 0.4785117208957672\n",
      "Epoch 1888, Loss: 2.2283941507339478, Final Batch Loss: 0.4942787289619446\n",
      "Epoch 1889, Loss: 2.3983209431171417, Final Batch Loss: 0.5037323236465454\n",
      "Epoch 1890, Loss: 2.3006646931171417, Final Batch Loss: 0.4014971852302551\n",
      "Epoch 1891, Loss: 2.1800813376903534, Final Batch Loss: 0.45856723189353943\n",
      "Epoch 1892, Loss: 2.126652628183365, Final Batch Loss: 0.34600692987442017\n",
      "Epoch 1893, Loss: 2.376124322414398, Final Batch Loss: 0.4778992831707001\n",
      "Epoch 1894, Loss: 2.196274757385254, Final Batch Loss: 0.48497697710990906\n",
      "Epoch 1895, Loss: 2.2836445569992065, Final Batch Loss: 0.4777737259864807\n",
      "Epoch 1896, Loss: 2.3057620227336884, Final Batch Loss: 0.44757580757141113\n",
      "Epoch 1897, Loss: 2.245376616716385, Final Batch Loss: 0.5070345401763916\n",
      "Epoch 1898, Loss: 2.075735032558441, Final Batch Loss: 0.4816139340400696\n",
      "Epoch 1899, Loss: 2.2218077182769775, Final Batch Loss: 0.42927050590515137\n",
      "Epoch 1900, Loss: 2.1666216254234314, Final Batch Loss: 0.3435080945491791\n",
      "Epoch 1901, Loss: 2.047342538833618, Final Batch Loss: 0.2738451063632965\n",
      "Epoch 1902, Loss: 2.142567664384842, Final Batch Loss: 0.5137248635292053\n",
      "Epoch 1903, Loss: 2.119675487279892, Final Batch Loss: 0.29916197061538696\n",
      "Epoch 1904, Loss: 2.136186867952347, Final Batch Loss: 0.3685753345489502\n",
      "Epoch 1905, Loss: 2.1941020488739014, Final Batch Loss: 0.46353206038475037\n",
      "Epoch 1906, Loss: 2.099878668785095, Final Batch Loss: 0.37770721316337585\n",
      "Epoch 1907, Loss: 2.3304175436496735, Final Batch Loss: 0.4640904366970062\n",
      "Epoch 1908, Loss: 2.033268064260483, Final Batch Loss: 0.41164740920066833\n",
      "Epoch 1909, Loss: 2.0950721502304077, Final Batch Loss: 0.3286329507827759\n",
      "Epoch 1910, Loss: 2.1519483029842377, Final Batch Loss: 0.3763967454433441\n",
      "Epoch 1911, Loss: 2.178311973810196, Final Batch Loss: 0.4144788980484009\n",
      "Epoch 1912, Loss: 2.288239359855652, Final Batch Loss: 0.4220673143863678\n",
      "Epoch 1913, Loss: 2.1555499732494354, Final Batch Loss: 0.4630628526210785\n",
      "Epoch 1914, Loss: 2.278396636247635, Final Batch Loss: 0.4996141195297241\n",
      "Epoch 1915, Loss: 2.294315129518509, Final Batch Loss: 0.47619107365608215\n",
      "Epoch 1916, Loss: 2.3835111260414124, Final Batch Loss: 0.5535631775856018\n",
      "Epoch 1917, Loss: 2.1261095106601715, Final Batch Loss: 0.4152657091617584\n",
      "Epoch 1918, Loss: 2.175847828388214, Final Batch Loss: 0.4433518648147583\n",
      "Epoch 1919, Loss: 2.4081134498119354, Final Batch Loss: 0.5759598016738892\n",
      "Epoch 1920, Loss: 2.1373895704746246, Final Batch Loss: 0.43200361728668213\n",
      "Epoch 1921, Loss: 1.9926210641860962, Final Batch Loss: 0.30712810158729553\n",
      "Epoch 1922, Loss: 2.2378575205802917, Final Batch Loss: 0.41736796498298645\n",
      "Epoch 1923, Loss: 2.1716811656951904, Final Batch Loss: 0.5124419927597046\n",
      "Epoch 1924, Loss: 2.1702417731285095, Final Batch Loss: 0.46190789341926575\n",
      "Epoch 1925, Loss: 2.2335632741451263, Final Batch Loss: 0.5566741228103638\n",
      "Epoch 1926, Loss: 2.303649812936783, Final Batch Loss: 0.5364561080932617\n",
      "Epoch 1927, Loss: 2.24146169424057, Final Batch Loss: 0.5016002058982849\n",
      "Epoch 1928, Loss: 2.216594308614731, Final Batch Loss: 0.37243783473968506\n",
      "Epoch 1929, Loss: 2.160389393568039, Final Batch Loss: 0.42924824357032776\n",
      "Epoch 1930, Loss: 2.324180781841278, Final Batch Loss: 0.5457785129547119\n",
      "Epoch 1931, Loss: 2.179939925670624, Final Batch Loss: 0.35414835810661316\n",
      "Epoch 1932, Loss: 1.9695082902908325, Final Batch Loss: 0.34757328033447266\n",
      "Epoch 1933, Loss: 2.205945998430252, Final Batch Loss: 0.3161378800868988\n",
      "Epoch 1934, Loss: 2.046513944864273, Final Batch Loss: 0.44301968812942505\n",
      "Epoch 1935, Loss: 2.259758025407791, Final Batch Loss: 0.4687427878379822\n",
      "Epoch 1936, Loss: 2.239709198474884, Final Batch Loss: 0.4877519905567169\n",
      "Epoch 1937, Loss: 2.288741707801819, Final Batch Loss: 0.46510493755340576\n",
      "Epoch 1938, Loss: 2.191622316837311, Final Batch Loss: 0.40360045433044434\n",
      "Epoch 1939, Loss: 2.2748981714248657, Final Batch Loss: 0.514821469783783\n",
      "Epoch 1940, Loss: 2.211779773235321, Final Batch Loss: 0.3343642055988312\n",
      "Epoch 1941, Loss: 2.263826608657837, Final Batch Loss: 0.5017507672309875\n",
      "Epoch 1942, Loss: 2.143229454755783, Final Batch Loss: 0.5194773077964783\n",
      "Epoch 1943, Loss: 2.1056479811668396, Final Batch Loss: 0.40555524826049805\n",
      "Epoch 1944, Loss: 2.2596882581710815, Final Batch Loss: 0.44900134205818176\n",
      "Epoch 1945, Loss: 2.242239832878113, Final Batch Loss: 0.4588466286659241\n",
      "Epoch 1946, Loss: 2.251315712928772, Final Batch Loss: 0.4137553572654724\n",
      "Epoch 1947, Loss: 2.1841650009155273, Final Batch Loss: 0.40330255031585693\n",
      "Epoch 1948, Loss: 2.178486317396164, Final Batch Loss: 0.41287961602211\n",
      "Epoch 1949, Loss: 2.1747909784317017, Final Batch Loss: 0.4980793297290802\n",
      "Epoch 1950, Loss: 2.203240215778351, Final Batch Loss: 0.4116557240486145\n",
      "Epoch 1951, Loss: 2.2710973620414734, Final Batch Loss: 0.44162219762802124\n",
      "Epoch 1952, Loss: 2.1704649329185486, Final Batch Loss: 0.4737549424171448\n",
      "Epoch 1953, Loss: 2.0419060587882996, Final Batch Loss: 0.3629036843776703\n",
      "Epoch 1954, Loss: 2.178289979696274, Final Batch Loss: 0.4010460674762726\n",
      "Epoch 1955, Loss: 2.3278879821300507, Final Batch Loss: 0.480961412191391\n",
      "Epoch 1956, Loss: 2.0672901272773743, Final Batch Loss: 0.4278453290462494\n",
      "Epoch 1957, Loss: 2.2544884085655212, Final Batch Loss: 0.5459843873977661\n",
      "Epoch 1958, Loss: 2.1391884088516235, Final Batch Loss: 0.4023735523223877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1959, Loss: 2.1620437800884247, Final Batch Loss: 0.45380133390426636\n",
      "Epoch 1960, Loss: 2.107515722513199, Final Batch Loss: 0.39990121126174927\n",
      "Epoch 1961, Loss: 2.084010750055313, Final Batch Loss: 0.43472281098365784\n",
      "Epoch 1962, Loss: 2.2782518565654755, Final Batch Loss: 0.5506422519683838\n",
      "Epoch 1963, Loss: 2.333941251039505, Final Batch Loss: 0.4386296570301056\n",
      "Epoch 1964, Loss: 2.3217695355415344, Final Batch Loss: 0.4858100712299347\n",
      "Epoch 1965, Loss: 2.2509800493717194, Final Batch Loss: 0.48650819063186646\n",
      "Epoch 1966, Loss: 2.1465941071510315, Final Batch Loss: 0.38453200459480286\n",
      "Epoch 1967, Loss: 2.201029598712921, Final Batch Loss: 0.4821159243583679\n",
      "Epoch 1968, Loss: 2.1248872578144073, Final Batch Loss: 0.3738066852092743\n",
      "Epoch 1969, Loss: 2.2246818840503693, Final Batch Loss: 0.3957241475582123\n",
      "Epoch 1970, Loss: 2.138340711593628, Final Batch Loss: 0.4869958162307739\n",
      "Epoch 1971, Loss: 2.1861800849437714, Final Batch Loss: 0.3889397382736206\n",
      "Epoch 1972, Loss: 2.061315953731537, Final Batch Loss: 0.3131334185600281\n",
      "Epoch 1973, Loss: 2.1468633711338043, Final Batch Loss: 0.33909326791763306\n",
      "Epoch 1974, Loss: 2.172261029481888, Final Batch Loss: 0.45109647512435913\n",
      "Epoch 1975, Loss: 2.252200126647949, Final Batch Loss: 0.4931119680404663\n",
      "Epoch 1976, Loss: 2.2522581815719604, Final Batch Loss: 0.5642386674880981\n",
      "Epoch 1977, Loss: 2.1580686569213867, Final Batch Loss: 0.3818955719470978\n",
      "Epoch 1978, Loss: 2.140276074409485, Final Batch Loss: 0.33972451090812683\n",
      "Epoch 1979, Loss: 2.1337550580501556, Final Batch Loss: 0.41678375005722046\n",
      "Epoch 1980, Loss: 2.245563507080078, Final Batch Loss: 0.47144418954849243\n",
      "Epoch 1981, Loss: 2.2891794443130493, Final Batch Loss: 0.6036665439605713\n",
      "Epoch 1982, Loss: 2.390817016363144, Final Batch Loss: 0.5093780755996704\n",
      "Epoch 1983, Loss: 2.2701364159584045, Final Batch Loss: 0.48136553168296814\n",
      "Epoch 1984, Loss: 2.202351540327072, Final Batch Loss: 0.5613278746604919\n",
      "Epoch 1985, Loss: 2.1187270283699036, Final Batch Loss: 0.45454832911491394\n",
      "Epoch 1986, Loss: 2.1458987295627594, Final Batch Loss: 0.36778339743614197\n",
      "Epoch 1987, Loss: 2.1659608483314514, Final Batch Loss: 0.38586175441741943\n",
      "Epoch 1988, Loss: 2.1179320216178894, Final Batch Loss: 0.28979411721229553\n",
      "Epoch 1989, Loss: 2.281076103448868, Final Batch Loss: 0.5940239429473877\n",
      "Epoch 1990, Loss: 2.1079480051994324, Final Batch Loss: 0.38606980443000793\n",
      "Epoch 1991, Loss: 2.19138041138649, Final Batch Loss: 0.43332505226135254\n",
      "Epoch 1992, Loss: 2.041457384824753, Final Batch Loss: 0.3529179096221924\n",
      "Epoch 1993, Loss: 2.1469343304634094, Final Batch Loss: 0.3376525044441223\n",
      "Epoch 1994, Loss: 2.146756738424301, Final Batch Loss: 0.43704625964164734\n",
      "Epoch 1995, Loss: 2.150284022092819, Final Batch Loss: 0.4761952757835388\n",
      "Epoch 1996, Loss: 2.177401304244995, Final Batch Loss: 0.4129372835159302\n",
      "Epoch 1997, Loss: 2.127303957939148, Final Batch Loss: 0.3620140552520752\n",
      "Epoch 1998, Loss: 2.065582722425461, Final Batch Loss: 0.3524928092956543\n",
      "Epoch 1999, Loss: 2.0701023936271667, Final Batch Loss: 0.43054255843162537\n",
      "Epoch 2000, Loss: 2.152816265821457, Final Batch Loss: 0.38987502455711365\n",
      "Epoch 2001, Loss: 2.240713953971863, Final Batch Loss: 0.42760205268859863\n",
      "Epoch 2002, Loss: 2.2069251239299774, Final Batch Loss: 0.3892141282558441\n",
      "Epoch 2003, Loss: 2.264580726623535, Final Batch Loss: 0.36920344829559326\n",
      "Epoch 2004, Loss: 2.272532343864441, Final Batch Loss: 0.49621033668518066\n",
      "Epoch 2005, Loss: 1.9981767535209656, Final Batch Loss: 0.440224289894104\n",
      "Epoch 2006, Loss: 2.055856853723526, Final Batch Loss: 0.34027889370918274\n",
      "Epoch 2007, Loss: 2.1847641468048096, Final Batch Loss: 0.46665158867836\n",
      "Epoch 2008, Loss: 2.2622238099575043, Final Batch Loss: 0.449859619140625\n",
      "Epoch 2009, Loss: 2.144415497779846, Final Batch Loss: 0.49924030900001526\n",
      "Epoch 2010, Loss: 2.0754568576812744, Final Batch Loss: 0.4121367633342743\n",
      "Epoch 2011, Loss: 2.1897628009319305, Final Batch Loss: 0.5294267535209656\n",
      "Epoch 2012, Loss: 2.323417216539383, Final Batch Loss: 0.5367239117622375\n",
      "Epoch 2013, Loss: 2.1113004982471466, Final Batch Loss: 0.44562941789627075\n",
      "Epoch 2014, Loss: 2.104631394147873, Final Batch Loss: 0.32844892144203186\n",
      "Epoch 2015, Loss: 2.138511747121811, Final Batch Loss: 0.46887442469596863\n",
      "Epoch 2016, Loss: 2.2116527259349823, Final Batch Loss: 0.427618145942688\n",
      "Epoch 2017, Loss: 2.0667929351329803, Final Batch Loss: 0.3972225785255432\n",
      "Epoch 2018, Loss: 2.2466316521167755, Final Batch Loss: 0.4894489049911499\n",
      "Epoch 2019, Loss: 2.1906585097312927, Final Batch Loss: 0.4745849668979645\n",
      "Epoch 2020, Loss: 2.180495649576187, Final Batch Loss: 0.43230825662612915\n",
      "Epoch 2021, Loss: 2.2091077268123627, Final Batch Loss: 0.43624207377433777\n",
      "Epoch 2022, Loss: 2.1835348308086395, Final Batch Loss: 0.4561595916748047\n",
      "Epoch 2023, Loss: 2.0183155834674835, Final Batch Loss: 0.43717730045318604\n",
      "Epoch 2024, Loss: 2.2369269728660583, Final Batch Loss: 0.559211254119873\n",
      "Epoch 2025, Loss: 2.0910738706588745, Final Batch Loss: 0.48666417598724365\n",
      "Epoch 2026, Loss: 2.077914148569107, Final Batch Loss: 0.3887897729873657\n",
      "Epoch 2027, Loss: 2.217855393886566, Final Batch Loss: 0.50822514295578\n",
      "Epoch 2028, Loss: 2.158591568470001, Final Batch Loss: 0.5104767084121704\n",
      "Epoch 2029, Loss: 2.172523945569992, Final Batch Loss: 0.5490097999572754\n",
      "Epoch 2030, Loss: 2.403147876262665, Final Batch Loss: 0.47732681035995483\n",
      "Epoch 2031, Loss: 2.106199562549591, Final Batch Loss: 0.4374547600746155\n",
      "Epoch 2032, Loss: 2.1148916482925415, Final Batch Loss: 0.3775642514228821\n",
      "Epoch 2033, Loss: 2.2276387810707092, Final Batch Loss: 0.43929004669189453\n",
      "Epoch 2034, Loss: 2.050798624753952, Final Batch Loss: 0.42474454641342163\n",
      "Epoch 2035, Loss: 2.2672547101974487, Final Batch Loss: 0.5193511843681335\n",
      "Epoch 2036, Loss: 2.181779623031616, Final Batch Loss: 0.39066773653030396\n",
      "Epoch 2037, Loss: 2.1428838670253754, Final Batch Loss: 0.4603674113750458\n",
      "Epoch 2038, Loss: 2.125668317079544, Final Batch Loss: 0.4226006865501404\n",
      "Epoch 2039, Loss: 2.143655449151993, Final Batch Loss: 0.45751285552978516\n",
      "Epoch 2040, Loss: 2.193634867668152, Final Batch Loss: 0.514456570148468\n",
      "Epoch 2041, Loss: 2.0718503296375275, Final Batch Loss: 0.33592572808265686\n",
      "Epoch 2042, Loss: 2.199037492275238, Final Batch Loss: 0.5074158310890198\n",
      "Epoch 2043, Loss: 2.1825548112392426, Final Batch Loss: 0.42998063564300537\n",
      "Epoch 2044, Loss: 2.1258903443813324, Final Batch Loss: 0.47456833720207214\n",
      "Epoch 2045, Loss: 2.198764145374298, Final Batch Loss: 0.42085129022598267\n",
      "Epoch 2046, Loss: 2.0283626317977905, Final Batch Loss: 0.38589340448379517\n",
      "Epoch 2047, Loss: 2.1983408629894257, Final Batch Loss: 0.4424286484718323\n",
      "Epoch 2048, Loss: 2.181124120950699, Final Batch Loss: 0.5364497900009155\n",
      "Epoch 2049, Loss: 2.120775043964386, Final Batch Loss: 0.42709583044052124\n",
      "Epoch 2050, Loss: 2.1073594987392426, Final Batch Loss: 0.42223823070526123\n",
      "Epoch 2051, Loss: 2.2179887890815735, Final Batch Loss: 0.567796528339386\n",
      "Epoch 2052, Loss: 2.035710483789444, Final Batch Loss: 0.3577330708503723\n",
      "Epoch 2053, Loss: 2.075297325849533, Final Batch Loss: 0.5016158223152161\n",
      "Epoch 2054, Loss: 2.1467027366161346, Final Batch Loss: 0.44957274198532104\n",
      "Epoch 2055, Loss: 2.131651371717453, Final Batch Loss: 0.5304679870605469\n",
      "Epoch 2056, Loss: 2.2107061445713043, Final Batch Loss: 0.5020977854728699\n",
      "Epoch 2057, Loss: 2.2578903138637543, Final Batch Loss: 0.5364835262298584\n",
      "Epoch 2058, Loss: 2.0208776593208313, Final Batch Loss: 0.4012608826160431\n",
      "Epoch 2059, Loss: 2.1003339290618896, Final Batch Loss: 0.35841426253318787\n",
      "Epoch 2060, Loss: 2.0326695442199707, Final Batch Loss: 0.37121957540512085\n",
      "Epoch 2061, Loss: 2.15223091840744, Final Batch Loss: 0.4513542056083679\n",
      "Epoch 2062, Loss: 2.1575608253479004, Final Batch Loss: 0.45261767506599426\n",
      "Epoch 2063, Loss: 2.2132706940174103, Final Batch Loss: 0.4695170819759369\n",
      "Epoch 2064, Loss: 2.1064580380916595, Final Batch Loss: 0.3988021910190582\n",
      "Epoch 2065, Loss: 2.0117774307727814, Final Batch Loss: 0.3219495117664337\n",
      "Epoch 2066, Loss: 2.0197426080703735, Final Batch Loss: 0.3673768937587738\n",
      "Epoch 2067, Loss: 2.188632071018219, Final Batch Loss: 0.42491698265075684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2068, Loss: 2.072165071964264, Final Batch Loss: 0.4188911020755768\n",
      "Epoch 2069, Loss: 2.2598383724689484, Final Batch Loss: 0.5374971628189087\n",
      "Epoch 2070, Loss: 2.1079470217227936, Final Batch Loss: 0.4322104752063751\n",
      "Epoch 2071, Loss: 2.257730633020401, Final Batch Loss: 0.5180248022079468\n",
      "Epoch 2072, Loss: 2.324182152748108, Final Batch Loss: 0.4252528250217438\n",
      "Epoch 2073, Loss: 2.092294991016388, Final Batch Loss: 0.46053656935691833\n",
      "Epoch 2074, Loss: 2.1438955664634705, Final Batch Loss: 0.38296622037887573\n",
      "Epoch 2075, Loss: 2.134736806154251, Final Batch Loss: 0.4556888937950134\n",
      "Epoch 2076, Loss: 2.0672321915626526, Final Batch Loss: 0.3442355990409851\n",
      "Epoch 2077, Loss: 2.09468537569046, Final Batch Loss: 0.5096994638442993\n",
      "Epoch 2078, Loss: 1.9338926076889038, Final Batch Loss: 0.32864898443222046\n",
      "Epoch 2079, Loss: 2.0818930566310883, Final Batch Loss: 0.3549273908138275\n",
      "Epoch 2080, Loss: 2.149921417236328, Final Batch Loss: 0.5010178685188293\n",
      "Epoch 2081, Loss: 2.163866341114044, Final Batch Loss: 0.3803340792655945\n",
      "Epoch 2082, Loss: 2.113609194755554, Final Batch Loss: 0.4014544188976288\n",
      "Epoch 2083, Loss: 2.070590078830719, Final Batch Loss: 0.3823451101779938\n",
      "Epoch 2084, Loss: 2.074574202299118, Final Batch Loss: 0.38503187894821167\n",
      "Epoch 2085, Loss: 2.0520545542240143, Final Batch Loss: 0.437804639339447\n",
      "Epoch 2086, Loss: 2.0527939200401306, Final Batch Loss: 0.3919405937194824\n",
      "Epoch 2087, Loss: 2.1734328866004944, Final Batch Loss: 0.5322446823120117\n",
      "Epoch 2088, Loss: 2.2211025059223175, Final Batch Loss: 0.44919008016586304\n",
      "Epoch 2089, Loss: 2.1260730922222137, Final Batch Loss: 0.4518304169178009\n",
      "Epoch 2090, Loss: 2.0981545448303223, Final Batch Loss: 0.39825522899627686\n",
      "Epoch 2091, Loss: 2.1430590450763702, Final Batch Loss: 0.42234185338020325\n",
      "Epoch 2092, Loss: 2.164819449186325, Final Batch Loss: 0.5402064323425293\n",
      "Epoch 2093, Loss: 2.140451729297638, Final Batch Loss: 0.4563848674297333\n",
      "Epoch 2094, Loss: 2.274056315422058, Final Batch Loss: 0.5231613516807556\n",
      "Epoch 2095, Loss: 2.1855293214321136, Final Batch Loss: 0.49695760011672974\n",
      "Epoch 2096, Loss: 2.2180700600147247, Final Batch Loss: 0.5059618949890137\n",
      "Epoch 2097, Loss: 2.173536777496338, Final Batch Loss: 0.4019756317138672\n",
      "Epoch 2098, Loss: 2.06100594997406, Final Batch Loss: 0.3278726637363434\n",
      "Epoch 2099, Loss: 2.13883239030838, Final Batch Loss: 0.4817933738231659\n",
      "Epoch 2100, Loss: 2.1131386160850525, Final Batch Loss: 0.42144575715065\n",
      "Epoch 2101, Loss: 2.2459574341773987, Final Batch Loss: 0.5034024715423584\n",
      "Epoch 2102, Loss: 2.166232019662857, Final Batch Loss: 0.43428531289100647\n",
      "Epoch 2103, Loss: 2.0874398350715637, Final Batch Loss: 0.4372803568840027\n",
      "Epoch 2104, Loss: 2.110518455505371, Final Batch Loss: 0.48215389251708984\n",
      "Epoch 2105, Loss: 2.0736586451530457, Final Batch Loss: 0.4194643199443817\n",
      "Epoch 2106, Loss: 2.113267421722412, Final Batch Loss: 0.47306686639785767\n",
      "Epoch 2107, Loss: 2.149339646100998, Final Batch Loss: 0.384724885225296\n",
      "Epoch 2108, Loss: 2.179048091173172, Final Batch Loss: 0.40549784898757935\n",
      "Epoch 2109, Loss: 1.907685399055481, Final Batch Loss: 0.2999119460582733\n",
      "Epoch 2110, Loss: 2.1310179829597473, Final Batch Loss: 0.46918413043022156\n",
      "Epoch 2111, Loss: 2.1724732518196106, Final Batch Loss: 0.37808099389076233\n",
      "Epoch 2112, Loss: 2.03790682554245, Final Batch Loss: 0.40074434876441956\n",
      "Epoch 2113, Loss: 1.9859260618686676, Final Batch Loss: 0.3644656538963318\n",
      "Epoch 2114, Loss: 2.13840052485466, Final Batch Loss: 0.3952997326850891\n",
      "Epoch 2115, Loss: 2.121816873550415, Final Batch Loss: 0.5314269065856934\n",
      "Epoch 2116, Loss: 2.0152100324630737, Final Batch Loss: 0.3165650963783264\n",
      "Epoch 2117, Loss: 2.1009658575057983, Final Batch Loss: 0.3712318539619446\n",
      "Epoch 2118, Loss: 2.1509463489055634, Final Batch Loss: 0.5097796320915222\n",
      "Epoch 2119, Loss: 2.0987769067287445, Final Batch Loss: 0.30297034978866577\n",
      "Epoch 2120, Loss: 2.1752508878707886, Final Batch Loss: 0.4824331998825073\n",
      "Epoch 2121, Loss: 2.194448858499527, Final Batch Loss: 0.4073340892791748\n",
      "Epoch 2122, Loss: 2.111315906047821, Final Batch Loss: 0.32535627484321594\n",
      "Epoch 2123, Loss: 2.054925113916397, Final Batch Loss: 0.3821272552013397\n",
      "Epoch 2124, Loss: 2.1611394584178925, Final Batch Loss: 0.48570147156715393\n",
      "Epoch 2125, Loss: 2.2149218022823334, Final Batch Loss: 0.48375317454338074\n",
      "Epoch 2126, Loss: 1.9916995465755463, Final Batch Loss: 0.3832350969314575\n",
      "Epoch 2127, Loss: 2.168084532022476, Final Batch Loss: 0.5099048018455505\n",
      "Epoch 2128, Loss: 2.1287581622600555, Final Batch Loss: 0.42624276876449585\n",
      "Epoch 2129, Loss: 2.0767619907855988, Final Batch Loss: 0.3618292510509491\n",
      "Epoch 2130, Loss: 2.304215282201767, Final Batch Loss: 0.5646891593933105\n",
      "Epoch 2131, Loss: 2.1007762253284454, Final Batch Loss: 0.3411543369293213\n",
      "Epoch 2132, Loss: 2.2594290673732758, Final Batch Loss: 0.4302555024623871\n",
      "Epoch 2133, Loss: 2.2670065462589264, Final Batch Loss: 0.4043284058570862\n",
      "Epoch 2134, Loss: 2.0199500918388367, Final Batch Loss: 0.42040953040122986\n",
      "Epoch 2135, Loss: 2.1832688450813293, Final Batch Loss: 0.4548706114292145\n",
      "Epoch 2136, Loss: 2.071547418832779, Final Batch Loss: 0.39630213379859924\n",
      "Epoch 2137, Loss: 2.033396989107132, Final Batch Loss: 0.32987236976623535\n",
      "Epoch 2138, Loss: 2.0665465891361237, Final Batch Loss: 0.42957231402397156\n",
      "Epoch 2139, Loss: 2.029373973608017, Final Batch Loss: 0.4049004316329956\n",
      "Epoch 2140, Loss: 2.2290132343769073, Final Batch Loss: 0.552884042263031\n",
      "Epoch 2141, Loss: 2.1545711755752563, Final Batch Loss: 0.3779560923576355\n",
      "Epoch 2142, Loss: 2.221896767616272, Final Batch Loss: 0.503768265247345\n",
      "Epoch 2143, Loss: 2.0354378521442413, Final Batch Loss: 0.38604065775871277\n",
      "Epoch 2144, Loss: 2.0772368013858795, Final Batch Loss: 0.5618153810501099\n",
      "Epoch 2145, Loss: 2.1856289207935333, Final Batch Loss: 0.5012381076812744\n",
      "Epoch 2146, Loss: 2.026011049747467, Final Batch Loss: 0.4580802023410797\n",
      "Epoch 2147, Loss: 2.0861293375492096, Final Batch Loss: 0.395929217338562\n",
      "Epoch 2148, Loss: 2.1358551681041718, Final Batch Loss: 0.4857465624809265\n",
      "Epoch 2149, Loss: 2.1579755544662476, Final Batch Loss: 0.4582258462905884\n",
      "Epoch 2150, Loss: 2.0935981273651123, Final Batch Loss: 0.4258553981781006\n",
      "Epoch 2151, Loss: 2.149428904056549, Final Batch Loss: 0.5433126091957092\n",
      "Epoch 2152, Loss: 2.1224773824214935, Final Batch Loss: 0.4938894212245941\n",
      "Epoch 2153, Loss: 1.9744616150856018, Final Batch Loss: 0.3235768675804138\n",
      "Epoch 2154, Loss: 2.1253316700458527, Final Batch Loss: 0.39359572529792786\n",
      "Epoch 2155, Loss: 2.0078024864196777, Final Batch Loss: 0.4078920781612396\n",
      "Epoch 2156, Loss: 2.0764447450637817, Final Batch Loss: 0.39774826169013977\n",
      "Epoch 2157, Loss: 2.080183655023575, Final Batch Loss: 0.41528087854385376\n",
      "Epoch 2158, Loss: 2.1284605264663696, Final Batch Loss: 0.38133880496025085\n",
      "Epoch 2159, Loss: 2.013224959373474, Final Batch Loss: 0.38702675700187683\n",
      "Epoch 2160, Loss: 2.1235454380512238, Final Batch Loss: 0.44283348321914673\n",
      "Epoch 2161, Loss: 2.250137060880661, Final Batch Loss: 0.47989434003829956\n",
      "Epoch 2162, Loss: 2.0460511445999146, Final Batch Loss: 0.39260271191596985\n",
      "Epoch 2163, Loss: 2.1878927648067474, Final Batch Loss: 0.34694546461105347\n",
      "Epoch 2164, Loss: 2.1085526049137115, Final Batch Loss: 0.3640274405479431\n",
      "Epoch 2165, Loss: 2.06488236784935, Final Batch Loss: 0.4666987359523773\n",
      "Epoch 2166, Loss: 2.173346221446991, Final Batch Loss: 0.34756964445114136\n",
      "Epoch 2167, Loss: 2.1273832619190216, Final Batch Loss: 0.4721912741661072\n",
      "Epoch 2168, Loss: 2.0536335706710815, Final Batch Loss: 0.45566463470458984\n",
      "Epoch 2169, Loss: 2.1568034291267395, Final Batch Loss: 0.538556694984436\n",
      "Epoch 2170, Loss: 2.2233871519565582, Final Batch Loss: 0.45140165090560913\n",
      "Epoch 2171, Loss: 2.117705464363098, Final Batch Loss: 0.41371041536331177\n",
      "Epoch 2172, Loss: 2.2783166468143463, Final Batch Loss: 0.46877995133399963\n",
      "Epoch 2173, Loss: 2.2006025910377502, Final Batch Loss: 0.5773192048072815\n",
      "Epoch 2174, Loss: 2.038645774126053, Final Batch Loss: 0.44554200768470764\n",
      "Epoch 2175, Loss: 2.024328887462616, Final Batch Loss: 0.39338061213493347\n",
      "Epoch 2176, Loss: 2.2828535735607147, Final Batch Loss: 0.4914986491203308\n",
      "Epoch 2177, Loss: 1.9718508124351501, Final Batch Loss: 0.37028154730796814\n",
      "Epoch 2178, Loss: 2.074457496404648, Final Batch Loss: 0.39606329798698425\n",
      "Epoch 2179, Loss: 2.020797073841095, Final Batch Loss: 0.3071412444114685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2180, Loss: 2.1243695318698883, Final Batch Loss: 0.4810382127761841\n",
      "Epoch 2181, Loss: 2.0017228424549103, Final Batch Loss: 0.4197772145271301\n",
      "Epoch 2182, Loss: 1.9635884761810303, Final Batch Loss: 0.4004864990711212\n",
      "Epoch 2183, Loss: 1.96416574716568, Final Batch Loss: 0.35952672362327576\n",
      "Epoch 2184, Loss: 2.1151328682899475, Final Batch Loss: 0.4350433051586151\n",
      "Epoch 2185, Loss: 2.066477209329605, Final Batch Loss: 0.35895201563835144\n",
      "Epoch 2186, Loss: 1.9214104413986206, Final Batch Loss: 0.38722729682922363\n",
      "Epoch 2187, Loss: 2.1066968739032745, Final Batch Loss: 0.5165290832519531\n",
      "Epoch 2188, Loss: 2.1619486212730408, Final Batch Loss: 0.37367600202560425\n",
      "Epoch 2189, Loss: 2.0828867852687836, Final Batch Loss: 0.46384376287460327\n",
      "Epoch 2190, Loss: 2.156149446964264, Final Batch Loss: 0.46581748127937317\n",
      "Epoch 2191, Loss: 2.023241400718689, Final Batch Loss: 0.3355523347854614\n",
      "Epoch 2192, Loss: 2.2150996923446655, Final Batch Loss: 0.4791609048843384\n",
      "Epoch 2193, Loss: 2.193223536014557, Final Batch Loss: 0.4454267919063568\n",
      "Epoch 2194, Loss: 2.1740138232707977, Final Batch Loss: 0.5115737915039062\n",
      "Epoch 2195, Loss: 2.151063919067383, Final Batch Loss: 0.41897255182266235\n",
      "Epoch 2196, Loss: 1.955354928970337, Final Batch Loss: 0.2922576367855072\n",
      "Epoch 2197, Loss: 2.107530176639557, Final Batch Loss: 0.4943351447582245\n",
      "Epoch 2198, Loss: 2.1004080772399902, Final Batch Loss: 0.5364198684692383\n",
      "Epoch 2199, Loss: 2.107419937849045, Final Batch Loss: 0.3364960849285126\n",
      "Epoch 2200, Loss: 2.017470359802246, Final Batch Loss: 0.40467017889022827\n",
      "Epoch 2201, Loss: 2.1133936941623688, Final Batch Loss: 0.4031967520713806\n",
      "Epoch 2202, Loss: 2.0733121931552887, Final Batch Loss: 0.35567036271095276\n",
      "Epoch 2203, Loss: 2.065869987010956, Final Batch Loss: 0.36826571822166443\n",
      "Epoch 2204, Loss: 2.1166844367980957, Final Batch Loss: 0.47871652245521545\n",
      "Epoch 2205, Loss: 2.1193141639232635, Final Batch Loss: 0.49358490109443665\n",
      "Epoch 2206, Loss: 2.1074970364570618, Final Batch Loss: 0.34153619408607483\n",
      "Epoch 2207, Loss: 2.0925124883651733, Final Batch Loss: 0.4924266040325165\n",
      "Epoch 2208, Loss: 2.2415896356105804, Final Batch Loss: 0.48803141713142395\n",
      "Epoch 2209, Loss: 1.9780563712120056, Final Batch Loss: 0.326821506023407\n",
      "Epoch 2210, Loss: 2.0146041810512543, Final Batch Loss: 0.2798226773738861\n",
      "Epoch 2211, Loss: 1.9159488379955292, Final Batch Loss: 0.303429514169693\n",
      "Epoch 2212, Loss: 2.0727067291736603, Final Batch Loss: 0.3892926275730133\n",
      "Epoch 2213, Loss: 2.1182688176631927, Final Batch Loss: 0.3829265534877777\n",
      "Epoch 2214, Loss: 2.271478772163391, Final Batch Loss: 0.42484036087989807\n",
      "Epoch 2215, Loss: 2.1614446341991425, Final Batch Loss: 0.4538743495941162\n",
      "Epoch 2216, Loss: 2.1374735832214355, Final Batch Loss: 0.41830793023109436\n",
      "Epoch 2217, Loss: 2.003807693719864, Final Batch Loss: 0.3207755386829376\n",
      "Epoch 2218, Loss: 2.0544152557849884, Final Batch Loss: 0.37099871039390564\n",
      "Epoch 2219, Loss: 2.0338222980499268, Final Batch Loss: 0.3971136808395386\n",
      "Epoch 2220, Loss: 2.1264597177505493, Final Batch Loss: 0.3942028284072876\n",
      "Epoch 2221, Loss: 2.090492933988571, Final Batch Loss: 0.48383498191833496\n",
      "Epoch 2222, Loss: 2.1257519125938416, Final Batch Loss: 0.4469507038593292\n",
      "Epoch 2223, Loss: 2.0846657156944275, Final Batch Loss: 0.41730958223342896\n",
      "Epoch 2224, Loss: 2.2359657287597656, Final Batch Loss: 0.4816165268421173\n",
      "Epoch 2225, Loss: 2.0513502955436707, Final Batch Loss: 0.4500555694103241\n",
      "Epoch 2226, Loss: 2.0609652400016785, Final Batch Loss: 0.42846331000328064\n",
      "Epoch 2227, Loss: 2.0394957661628723, Final Batch Loss: 0.35224029421806335\n",
      "Epoch 2228, Loss: 2.0483022034168243, Final Batch Loss: 0.4740072190761566\n",
      "Epoch 2229, Loss: 2.0065287947654724, Final Batch Loss: 0.40130850672721863\n",
      "Epoch 2230, Loss: 2.1581102311611176, Final Batch Loss: 0.5265064835548401\n",
      "Epoch 2231, Loss: 2.151546448469162, Final Batch Loss: 0.4668608009815216\n",
      "Epoch 2232, Loss: 2.027011752128601, Final Batch Loss: 0.42366984486579895\n",
      "Epoch 2233, Loss: 2.1314046382904053, Final Batch Loss: 0.4734589159488678\n",
      "Epoch 2234, Loss: 1.9481398165225983, Final Batch Loss: 0.4046593904495239\n",
      "Epoch 2235, Loss: 2.06349840760231, Final Batch Loss: 0.36521580815315247\n",
      "Epoch 2236, Loss: 2.0330409109592438, Final Batch Loss: 0.3402363359928131\n",
      "Epoch 2237, Loss: 2.0594632625579834, Final Batch Loss: 0.40607649087905884\n",
      "Epoch 2238, Loss: 2.0458482801914215, Final Batch Loss: 0.3873942792415619\n",
      "Epoch 2239, Loss: 1.9609335362911224, Final Batch Loss: 0.4307839870452881\n",
      "Epoch 2240, Loss: 2.1484996378421783, Final Batch Loss: 0.534630298614502\n",
      "Epoch 2241, Loss: 2.214495986700058, Final Batch Loss: 0.5163614749908447\n",
      "Epoch 2242, Loss: 1.9549328088760376, Final Batch Loss: 0.42539745569229126\n",
      "Epoch 2243, Loss: 2.190857231616974, Final Batch Loss: 0.46719890832901\n",
      "Epoch 2244, Loss: 2.235247880220413, Final Batch Loss: 0.4787653982639313\n",
      "Epoch 2245, Loss: 2.0572556853294373, Final Batch Loss: 0.329862117767334\n",
      "Epoch 2246, Loss: 1.9541528224945068, Final Batch Loss: 0.26561546325683594\n",
      "Epoch 2247, Loss: 1.9693496227264404, Final Batch Loss: 0.44932714104652405\n",
      "Epoch 2248, Loss: 2.0176381170749664, Final Batch Loss: 0.37329354882240295\n",
      "Epoch 2249, Loss: 2.1258616149425507, Final Batch Loss: 0.38931307196617126\n",
      "Epoch 2250, Loss: 2.072124034166336, Final Batch Loss: 0.39497649669647217\n",
      "Epoch 2251, Loss: 2.168736606836319, Final Batch Loss: 0.4524809718132019\n",
      "Epoch 2252, Loss: 2.108262538909912, Final Batch Loss: 0.4771033525466919\n",
      "Epoch 2253, Loss: 2.0805753767490387, Final Batch Loss: 0.3861932158470154\n",
      "Epoch 2254, Loss: 2.0575083792209625, Final Batch Loss: 0.37733784317970276\n",
      "Epoch 2255, Loss: 2.0470941066741943, Final Batch Loss: 0.3694119453430176\n",
      "Epoch 2256, Loss: 2.0691879093647003, Final Batch Loss: 0.43680962920188904\n",
      "Epoch 2257, Loss: 2.153673768043518, Final Batch Loss: 0.4032934606075287\n",
      "Epoch 2258, Loss: 1.9988690614700317, Final Batch Loss: 0.3901296854019165\n",
      "Epoch 2259, Loss: 2.0885655879974365, Final Batch Loss: 0.5679368376731873\n",
      "Epoch 2260, Loss: 1.9699545502662659, Final Batch Loss: 0.35773593187332153\n",
      "Epoch 2261, Loss: 1.948451966047287, Final Batch Loss: 0.337841272354126\n",
      "Epoch 2262, Loss: 1.9814127385616302, Final Batch Loss: 0.3646167516708374\n",
      "Epoch 2263, Loss: 2.100220710039139, Final Batch Loss: 0.5129043459892273\n",
      "Epoch 2264, Loss: 1.8766245245933533, Final Batch Loss: 0.3656482696533203\n",
      "Epoch 2265, Loss: 2.1274584531784058, Final Batch Loss: 0.44259676337242126\n",
      "Epoch 2266, Loss: 2.1587880849838257, Final Batch Loss: 0.4359947443008423\n",
      "Epoch 2267, Loss: 1.944080114364624, Final Batch Loss: 0.3741346597671509\n",
      "Epoch 2268, Loss: 2.0729921460151672, Final Batch Loss: 0.36629465222358704\n",
      "Epoch 2269, Loss: 2.0616436302661896, Final Batch Loss: 0.39951857924461365\n",
      "Epoch 2270, Loss: 1.9593366086483002, Final Batch Loss: 0.3547285199165344\n",
      "Epoch 2271, Loss: 2.023820251226425, Final Batch Loss: 0.33359235525131226\n",
      "Epoch 2272, Loss: 2.1361806392669678, Final Batch Loss: 0.5103309154510498\n",
      "Epoch 2273, Loss: 2.1181154251098633, Final Batch Loss: 0.4319586753845215\n",
      "Epoch 2274, Loss: 2.040703922510147, Final Batch Loss: 0.33499792218208313\n",
      "Epoch 2275, Loss: 2.087041735649109, Final Batch Loss: 0.4656301438808441\n",
      "Epoch 2276, Loss: 2.0682014524936676, Final Batch Loss: 0.36359941959381104\n",
      "Epoch 2277, Loss: 2.147386372089386, Final Batch Loss: 0.5434600710868835\n",
      "Epoch 2278, Loss: 2.0358294546604156, Final Batch Loss: 0.48879769444465637\n",
      "Epoch 2279, Loss: 1.9804617166519165, Final Batch Loss: 0.38493695855140686\n",
      "Epoch 2280, Loss: 1.9553672075271606, Final Batch Loss: 0.43260520696640015\n",
      "Epoch 2281, Loss: 2.0032236576080322, Final Batch Loss: 0.4564870297908783\n",
      "Epoch 2282, Loss: 2.009210377931595, Final Batch Loss: 0.3645605444908142\n",
      "Epoch 2283, Loss: 1.983318716287613, Final Batch Loss: 0.3333006203174591\n",
      "Epoch 2284, Loss: 2.0521152913570404, Final Batch Loss: 0.43313586711883545\n",
      "Epoch 2285, Loss: 2.1012178659439087, Final Batch Loss: 0.4300960600376129\n",
      "Epoch 2286, Loss: 2.05141082406044, Final Batch Loss: 0.3566049635410309\n",
      "Epoch 2287, Loss: 2.1208072006702423, Final Batch Loss: 0.4387427568435669\n",
      "Epoch 2288, Loss: 1.9628023207187653, Final Batch Loss: 0.39820581674575806\n",
      "Epoch 2289, Loss: 2.036654770374298, Final Batch Loss: 0.446809321641922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2290, Loss: 2.0504174530506134, Final Batch Loss: 0.43829917907714844\n",
      "Epoch 2291, Loss: 2.047602266073227, Final Batch Loss: 0.3802958130836487\n",
      "Epoch 2292, Loss: 2.2171027958393097, Final Batch Loss: 0.521375298500061\n",
      "Epoch 2293, Loss: 1.9091802537441254, Final Batch Loss: 0.41659078001976013\n",
      "Epoch 2294, Loss: 2.0568751394748688, Final Batch Loss: 0.46150192618370056\n",
      "Epoch 2295, Loss: 2.0300568342208862, Final Batch Loss: 0.34049320220947266\n",
      "Epoch 2296, Loss: 1.917504459619522, Final Batch Loss: 0.3742755353450775\n",
      "Epoch 2297, Loss: 1.9052834212779999, Final Batch Loss: 0.28986167907714844\n",
      "Epoch 2298, Loss: 2.0951481759548187, Final Batch Loss: 0.411938339471817\n",
      "Epoch 2299, Loss: 1.9615396857261658, Final Batch Loss: 0.35895276069641113\n",
      "Epoch 2300, Loss: 2.0035449862480164, Final Batch Loss: 0.37856438755989075\n",
      "Epoch 2301, Loss: 2.041113644838333, Final Batch Loss: 0.4328244626522064\n",
      "Epoch 2302, Loss: 2.165011703968048, Final Batch Loss: 0.4636552035808563\n",
      "Epoch 2303, Loss: 1.9867319166660309, Final Batch Loss: 0.37414970993995667\n",
      "Epoch 2304, Loss: 2.0136992931365967, Final Batch Loss: 0.3560064435005188\n",
      "Epoch 2305, Loss: 2.035897433757782, Final Batch Loss: 0.45756709575653076\n",
      "Epoch 2306, Loss: 2.1906854808330536, Final Batch Loss: 0.3662901222705841\n",
      "Epoch 2307, Loss: 1.8936017751693726, Final Batch Loss: 0.3608786463737488\n",
      "Epoch 2308, Loss: 2.1079472303390503, Final Batch Loss: 0.43155819177627563\n",
      "Epoch 2309, Loss: 2.03400319814682, Final Batch Loss: 0.4875842332839966\n",
      "Epoch 2310, Loss: 1.9580650925636292, Final Batch Loss: 0.4030548930168152\n",
      "Epoch 2311, Loss: 2.103901445865631, Final Batch Loss: 0.39185190200805664\n",
      "Epoch 2312, Loss: 2.1123155653476715, Final Batch Loss: 0.5113639831542969\n",
      "Epoch 2313, Loss: 1.9387504756450653, Final Batch Loss: 0.4150458872318268\n",
      "Epoch 2314, Loss: 2.1760576367378235, Final Batch Loss: 0.4338170289993286\n",
      "Epoch 2315, Loss: 2.1204104721546173, Final Batch Loss: 0.3749941885471344\n",
      "Epoch 2316, Loss: 1.8189295828342438, Final Batch Loss: 0.38046783208847046\n",
      "Epoch 2317, Loss: 2.1664711236953735, Final Batch Loss: 0.42734494805336\n",
      "Epoch 2318, Loss: 2.104525089263916, Final Batch Loss: 0.4640214443206787\n",
      "Epoch 2319, Loss: 2.091607689857483, Final Batch Loss: 0.39413556456565857\n",
      "Epoch 2320, Loss: 1.9277147352695465, Final Batch Loss: 0.29618293046951294\n",
      "Epoch 2321, Loss: 1.9831505417823792, Final Batch Loss: 0.5138754844665527\n",
      "Epoch 2322, Loss: 2.0691035091876984, Final Batch Loss: 0.41389796137809753\n",
      "Epoch 2323, Loss: 2.1355718076229095, Final Batch Loss: 0.40781378746032715\n",
      "Epoch 2324, Loss: 2.1587462425231934, Final Batch Loss: 0.5395975112915039\n",
      "Epoch 2325, Loss: 1.9528616666793823, Final Batch Loss: 0.43485867977142334\n",
      "Epoch 2326, Loss: 1.9906252920627594, Final Batch Loss: 0.42712870240211487\n",
      "Epoch 2327, Loss: 2.0041883885860443, Final Batch Loss: 0.34148988127708435\n",
      "Epoch 2328, Loss: 1.9944427013397217, Final Batch Loss: 0.4705057144165039\n",
      "Epoch 2329, Loss: 2.1397261321544647, Final Batch Loss: 0.3852960169315338\n",
      "Epoch 2330, Loss: 2.0153488516807556, Final Batch Loss: 0.36835771799087524\n",
      "Epoch 2331, Loss: 2.0615656971931458, Final Batch Loss: 0.44060418009757996\n",
      "Epoch 2332, Loss: 2.1236476004123688, Final Batch Loss: 0.5501587986946106\n",
      "Epoch 2333, Loss: 1.9316848814487457, Final Batch Loss: 0.45318445563316345\n",
      "Epoch 2334, Loss: 1.956989049911499, Final Batch Loss: 0.37966805696487427\n",
      "Epoch 2335, Loss: 1.884726107120514, Final Batch Loss: 0.41438859701156616\n",
      "Epoch 2336, Loss: 1.9141892790794373, Final Batch Loss: 0.38342371582984924\n",
      "Epoch 2337, Loss: 1.9161973595619202, Final Batch Loss: 0.3352224826812744\n",
      "Epoch 2338, Loss: 2.0411789417266846, Final Batch Loss: 0.3715621531009674\n",
      "Epoch 2339, Loss: 2.1537256240844727, Final Batch Loss: 0.4116683900356293\n",
      "Epoch 2340, Loss: 2.028579145669937, Final Batch Loss: 0.4061782956123352\n",
      "Epoch 2341, Loss: 2.14533793926239, Final Batch Loss: 0.39143404364585876\n",
      "Epoch 2342, Loss: 2.005847930908203, Final Batch Loss: 0.45328378677368164\n",
      "Epoch 2343, Loss: 1.9814935326576233, Final Batch Loss: 0.43330469727516174\n",
      "Epoch 2344, Loss: 1.998236060142517, Final Batch Loss: 0.37669023871421814\n",
      "Epoch 2345, Loss: 2.088204711675644, Final Batch Loss: 0.3609248995780945\n",
      "Epoch 2346, Loss: 1.8751354217529297, Final Batch Loss: 0.35704758763313293\n",
      "Epoch 2347, Loss: 1.988979011774063, Final Batch Loss: 0.37121251225471497\n",
      "Epoch 2348, Loss: 2.1321698129177094, Final Batch Loss: 0.5080606341362\n",
      "Epoch 2349, Loss: 2.1063106656074524, Final Batch Loss: 0.4579646587371826\n",
      "Epoch 2350, Loss: 2.1509540379047394, Final Batch Loss: 0.33484265208244324\n",
      "Epoch 2351, Loss: 1.9153143763542175, Final Batch Loss: 0.3898772597312927\n",
      "Epoch 2352, Loss: 1.98453289270401, Final Batch Loss: 0.3282475173473358\n",
      "Epoch 2353, Loss: 1.995300143957138, Final Batch Loss: 0.2885782718658447\n",
      "Epoch 2354, Loss: 1.985321819782257, Final Batch Loss: 0.4622776508331299\n",
      "Epoch 2355, Loss: 1.908987045288086, Final Batch Loss: 0.3584713637828827\n",
      "Epoch 2356, Loss: 2.109565854072571, Final Batch Loss: 0.44154974818229675\n",
      "Epoch 2357, Loss: 1.910806953907013, Final Batch Loss: 0.3899983763694763\n",
      "Epoch 2358, Loss: 1.9845035672187805, Final Batch Loss: 0.3820006251335144\n",
      "Epoch 2359, Loss: 1.8869968056678772, Final Batch Loss: 0.36291542649269104\n",
      "Epoch 2360, Loss: 2.1157672703266144, Final Batch Loss: 0.4437669515609741\n",
      "Epoch 2361, Loss: 1.969414472579956, Final Batch Loss: 0.35556888580322266\n",
      "Epoch 2362, Loss: 1.9407798647880554, Final Batch Loss: 0.35043859481811523\n",
      "Epoch 2363, Loss: 1.9992823600769043, Final Batch Loss: 0.3754095137119293\n",
      "Epoch 2364, Loss: 2.1652276813983917, Final Batch Loss: 0.4725702702999115\n",
      "Epoch 2365, Loss: 2.053626626729965, Final Batch Loss: 0.4684561491012573\n",
      "Epoch 2366, Loss: 2.1422902941703796, Final Batch Loss: 0.3894266188144684\n",
      "Epoch 2367, Loss: 2.002815216779709, Final Batch Loss: 0.3029610812664032\n",
      "Epoch 2368, Loss: 1.9757385551929474, Final Batch Loss: 0.3902962803840637\n",
      "Epoch 2369, Loss: 1.9588951468467712, Final Batch Loss: 0.47079187631607056\n",
      "Epoch 2370, Loss: 2.0781589448451996, Final Batch Loss: 0.4630869925022125\n",
      "Epoch 2371, Loss: 1.9961799681186676, Final Batch Loss: 0.37932586669921875\n",
      "Epoch 2372, Loss: 1.9314928948879242, Final Batch Loss: 0.35748550295829773\n",
      "Epoch 2373, Loss: 1.9712397456169128, Final Batch Loss: 0.4621695578098297\n",
      "Epoch 2374, Loss: 2.1495562493801117, Final Batch Loss: 0.3763638734817505\n",
      "Epoch 2375, Loss: 1.9649280309677124, Final Batch Loss: 0.3995456397533417\n",
      "Epoch 2376, Loss: 2.0593332052230835, Final Batch Loss: 0.4549320638179779\n",
      "Epoch 2377, Loss: 2.029737025499344, Final Batch Loss: 0.38311567902565\n",
      "Epoch 2378, Loss: 2.0582116842269897, Final Batch Loss: 0.4254809021949768\n",
      "Epoch 2379, Loss: 2.0736940503120422, Final Batch Loss: 0.5019351243972778\n",
      "Epoch 2380, Loss: 1.8549819588661194, Final Batch Loss: 0.38416793942451477\n",
      "Epoch 2381, Loss: 2.1513149440288544, Final Batch Loss: 0.47377824783325195\n",
      "Epoch 2382, Loss: 2.006333589553833, Final Batch Loss: 0.44673246145248413\n",
      "Epoch 2383, Loss: 1.9268180131912231, Final Batch Loss: 0.3451330065727234\n",
      "Epoch 2384, Loss: 1.8739170730113983, Final Batch Loss: 0.36981481313705444\n",
      "Epoch 2385, Loss: 1.965915024280548, Final Batch Loss: 0.5099145174026489\n",
      "Epoch 2386, Loss: 2.037347733974457, Final Batch Loss: 0.4087180495262146\n",
      "Epoch 2387, Loss: 1.9330906569957733, Final Batch Loss: 0.2998497784137726\n",
      "Epoch 2388, Loss: 2.078604578971863, Final Batch Loss: 0.4555828273296356\n",
      "Epoch 2389, Loss: 2.0422243177890778, Final Batch Loss: 0.5034652352333069\n",
      "Epoch 2390, Loss: 2.0339209735393524, Final Batch Loss: 0.36662203073501587\n",
      "Epoch 2391, Loss: 2.049799472093582, Final Batch Loss: 0.3490103781223297\n",
      "Epoch 2392, Loss: 2.0645233392715454, Final Batch Loss: 0.39630648493766785\n",
      "Epoch 2393, Loss: 2.031622529029846, Final Batch Loss: 0.4202312231063843\n",
      "Epoch 2394, Loss: 2.0308361053466797, Final Batch Loss: 0.38596296310424805\n",
      "Epoch 2395, Loss: 2.0742306113243103, Final Batch Loss: 0.3393417298793793\n",
      "Epoch 2396, Loss: 1.9491759836673737, Final Batch Loss: 0.3221660852432251\n",
      "Epoch 2397, Loss: 2.015512451529503, Final Batch Loss: 0.24776767194271088\n",
      "Epoch 2398, Loss: 2.161377191543579, Final Batch Loss: 0.4560278058052063\n",
      "Epoch 2399, Loss: 2.1855755746364594, Final Batch Loss: 0.46963852643966675\n",
      "Epoch 2400, Loss: 2.130224883556366, Final Batch Loss: 0.39847898483276367\n",
      "Epoch 2401, Loss: 2.049505352973938, Final Batch Loss: 0.3365707993507385\n",
      "Epoch 2402, Loss: 2.0841562151908875, Final Batch Loss: 0.5055091977119446\n",
      "Epoch 2403, Loss: 2.2366431653499603, Final Batch Loss: 0.33305203914642334\n",
      "Epoch 2404, Loss: 2.0539722442626953, Final Batch Loss: 0.4472676217556\n",
      "Epoch 2405, Loss: 1.9577073454856873, Final Batch Loss: 0.43564996123313904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2406, Loss: 2.004734545946121, Final Batch Loss: 0.35865628719329834\n",
      "Epoch 2407, Loss: 2.075960338115692, Final Batch Loss: 0.4212433397769928\n",
      "Epoch 2408, Loss: 2.132019877433777, Final Batch Loss: 0.43659621477127075\n",
      "Epoch 2409, Loss: 1.9922654032707214, Final Batch Loss: 0.27529507875442505\n",
      "Epoch 2410, Loss: 1.992895096540451, Final Batch Loss: 0.41482219099998474\n",
      "Epoch 2411, Loss: 1.955756425857544, Final Batch Loss: 0.3520379960536957\n",
      "Epoch 2412, Loss: 1.8509755432605743, Final Batch Loss: 0.3425289988517761\n",
      "Epoch 2413, Loss: 2.020754873752594, Final Batch Loss: 0.4148494303226471\n",
      "Epoch 2414, Loss: 2.038659483194351, Final Batch Loss: 0.41650041937828064\n",
      "Epoch 2415, Loss: 1.8447979986667633, Final Batch Loss: 0.3479297459125519\n",
      "Epoch 2416, Loss: 2.0223206281661987, Final Batch Loss: 0.41040194034576416\n",
      "Epoch 2417, Loss: 1.8508460521697998, Final Batch Loss: 0.3763361871242523\n",
      "Epoch 2418, Loss: 1.9485937058925629, Final Batch Loss: 0.3520301878452301\n",
      "Epoch 2419, Loss: 1.951717734336853, Final Batch Loss: 0.3352773189544678\n",
      "Epoch 2420, Loss: 1.9611207246780396, Final Batch Loss: 0.3888104259967804\n",
      "Epoch 2421, Loss: 2.0772607028484344, Final Batch Loss: 0.40087488293647766\n",
      "Epoch 2422, Loss: 2.0562873482704163, Final Batch Loss: 0.5403503775596619\n",
      "Epoch 2423, Loss: 2.1286151111125946, Final Batch Loss: 0.46707969903945923\n",
      "Epoch 2424, Loss: 1.9839656054973602, Final Batch Loss: 0.4216623902320862\n",
      "Epoch 2425, Loss: 1.8171002268791199, Final Batch Loss: 0.3575000464916229\n",
      "Epoch 2426, Loss: 2.064395308494568, Final Batch Loss: 0.4448848366737366\n",
      "Epoch 2427, Loss: 1.9095156788825989, Final Batch Loss: 0.3506159782409668\n",
      "Epoch 2428, Loss: 2.1753030121326447, Final Batch Loss: 0.3752402067184448\n",
      "Epoch 2429, Loss: 1.9344770908355713, Final Batch Loss: 0.3768598139286041\n",
      "Epoch 2430, Loss: 1.9211238026618958, Final Batch Loss: 0.37834930419921875\n",
      "Epoch 2431, Loss: 2.073928117752075, Final Batch Loss: 0.5017503499984741\n",
      "Epoch 2432, Loss: 1.9919072687625885, Final Batch Loss: 0.350087970495224\n",
      "Epoch 2433, Loss: 1.9612914025783539, Final Batch Loss: 0.3025595247745514\n",
      "Epoch 2434, Loss: 1.9030258357524872, Final Batch Loss: 0.31441810727119446\n",
      "Epoch 2435, Loss: 1.8742919266223907, Final Batch Loss: 0.371697336435318\n",
      "Epoch 2436, Loss: 2.1660039722919464, Final Batch Loss: 0.48267778754234314\n",
      "Epoch 2437, Loss: 1.921002060174942, Final Batch Loss: 0.36540067195892334\n",
      "Epoch 2438, Loss: 2.1136370599269867, Final Batch Loss: 0.4568862318992615\n",
      "Epoch 2439, Loss: 2.019685208797455, Final Batch Loss: 0.388676255941391\n",
      "Epoch 2440, Loss: 2.164510279893875, Final Batch Loss: 0.49036476016044617\n",
      "Epoch 2441, Loss: 1.996463268995285, Final Batch Loss: 0.4560219645500183\n",
      "Epoch 2442, Loss: 2.225934863090515, Final Batch Loss: 0.45971012115478516\n",
      "Epoch 2443, Loss: 2.0078539550304413, Final Batch Loss: 0.3753790557384491\n",
      "Epoch 2444, Loss: 1.9256480038166046, Final Batch Loss: 0.4332349896430969\n",
      "Epoch 2445, Loss: 1.8648386299610138, Final Batch Loss: 0.2933037281036377\n",
      "Epoch 2446, Loss: 1.9788782894611359, Final Batch Loss: 0.3487470746040344\n",
      "Epoch 2447, Loss: 1.9717511236667633, Final Batch Loss: 0.3841993510723114\n",
      "Epoch 2448, Loss: 1.8941636979579926, Final Batch Loss: 0.3941173553466797\n",
      "Epoch 2449, Loss: 1.9145539999008179, Final Batch Loss: 0.3653779625892639\n",
      "Epoch 2450, Loss: 1.9222728908061981, Final Batch Loss: 0.39084476232528687\n",
      "Epoch 2451, Loss: 2.026109367609024, Final Batch Loss: 0.4211435914039612\n",
      "Epoch 2452, Loss: 2.0259232223033905, Final Batch Loss: 0.3658702075481415\n",
      "Epoch 2453, Loss: 1.9742065966129303, Final Batch Loss: 0.32394614815711975\n",
      "Epoch 2454, Loss: 1.9980112314224243, Final Batch Loss: 0.3355734944343567\n",
      "Epoch 2455, Loss: 2.1730979084968567, Final Batch Loss: 0.498555451631546\n",
      "Epoch 2456, Loss: 2.1168573200702667, Final Batch Loss: 0.4254079759120941\n",
      "Epoch 2457, Loss: 2.062882125377655, Final Batch Loss: 0.36525553464889526\n",
      "Epoch 2458, Loss: 2.035371094942093, Final Batch Loss: 0.40379613637924194\n",
      "Epoch 2459, Loss: 2.08405077457428, Final Batch Loss: 0.466404527425766\n",
      "Epoch 2460, Loss: 1.954503446817398, Final Batch Loss: 0.3182951807975769\n",
      "Epoch 2461, Loss: 1.9995368421077728, Final Batch Loss: 0.38084790110588074\n",
      "Epoch 2462, Loss: 1.924580156803131, Final Batch Loss: 0.37903323769569397\n",
      "Epoch 2463, Loss: 1.9557829201221466, Final Batch Loss: 0.30618834495544434\n",
      "Epoch 2464, Loss: 2.0045520961284637, Final Batch Loss: 0.3711264133453369\n",
      "Epoch 2465, Loss: 1.9198302328586578, Final Batch Loss: 0.3637198805809021\n",
      "Epoch 2466, Loss: 2.058944284915924, Final Batch Loss: 0.3160332441329956\n",
      "Epoch 2467, Loss: 1.9148965179920197, Final Batch Loss: 0.4144924581050873\n",
      "Epoch 2468, Loss: 2.0217990279197693, Final Batch Loss: 0.3626527488231659\n",
      "Epoch 2469, Loss: 1.9497714340686798, Final Batch Loss: 0.44805455207824707\n",
      "Epoch 2470, Loss: 2.004138559103012, Final Batch Loss: 0.4012356400489807\n",
      "Epoch 2471, Loss: 1.8434233963489532, Final Batch Loss: 0.39223602414131165\n",
      "Epoch 2472, Loss: 1.9872756600379944, Final Batch Loss: 0.30541232228279114\n",
      "Epoch 2473, Loss: 1.93265500664711, Final Batch Loss: 0.3569149971008301\n",
      "Epoch 2474, Loss: 1.8988712131977081, Final Batch Loss: 0.3348347544670105\n",
      "Epoch 2475, Loss: 2.1197520196437836, Final Batch Loss: 0.4394610524177551\n",
      "Epoch 2476, Loss: 2.0313901007175446, Final Batch Loss: 0.43764224648475647\n",
      "Epoch 2477, Loss: 1.8767109513282776, Final Batch Loss: 0.30220046639442444\n",
      "Epoch 2478, Loss: 2.0433310866355896, Final Batch Loss: 0.4278469979763031\n",
      "Epoch 2479, Loss: 2.086677372455597, Final Batch Loss: 0.4815046787261963\n",
      "Epoch 2480, Loss: 2.073263704776764, Final Batch Loss: 0.418753981590271\n",
      "Epoch 2481, Loss: 2.005412131547928, Final Batch Loss: 0.31119951605796814\n",
      "Epoch 2482, Loss: 1.9896433651447296, Final Batch Loss: 0.3852761685848236\n",
      "Epoch 2483, Loss: 2.0057212710380554, Final Batch Loss: 0.4220218360424042\n",
      "Epoch 2484, Loss: 2.0820031464099884, Final Batch Loss: 0.4312542974948883\n",
      "Epoch 2485, Loss: 1.9903329014778137, Final Batch Loss: 0.38563185930252075\n",
      "Epoch 2486, Loss: 1.9923818707466125, Final Batch Loss: 0.3705763518810272\n",
      "Epoch 2487, Loss: 2.1089755594730377, Final Batch Loss: 0.4427771270275116\n",
      "Epoch 2488, Loss: 1.8223392069339752, Final Batch Loss: 0.39625537395477295\n",
      "Epoch 2489, Loss: 1.874747782945633, Final Batch Loss: 0.3197319805622101\n",
      "Epoch 2490, Loss: 1.889083057641983, Final Batch Loss: 0.40205445885658264\n",
      "Epoch 2491, Loss: 1.9936420917510986, Final Batch Loss: 0.42045578360557556\n",
      "Epoch 2492, Loss: 2.0512377619743347, Final Batch Loss: 0.3834034502506256\n",
      "Epoch 2493, Loss: 2.023116409778595, Final Batch Loss: 0.42166629433631897\n",
      "Epoch 2494, Loss: 1.9078335762023926, Final Batch Loss: 0.4443276822566986\n",
      "Epoch 2495, Loss: 1.9004281163215637, Final Batch Loss: 0.35614991188049316\n",
      "Epoch 2496, Loss: 1.9464390873908997, Final Batch Loss: 0.5376636981964111\n",
      "Epoch 2497, Loss: 1.7793219685554504, Final Batch Loss: 0.3432677984237671\n",
      "Epoch 2498, Loss: 2.1559228003025055, Final Batch Loss: 0.40236696600914\n",
      "Epoch 2499, Loss: 1.9283780455589294, Final Batch Loss: 0.4490983486175537\n",
      "Epoch 2500, Loss: 1.9144929349422455, Final Batch Loss: 0.2667543888092041\n",
      "Epoch 2501, Loss: 1.9125065207481384, Final Batch Loss: 0.3659230172634125\n",
      "Epoch 2502, Loss: 1.9552899897098541, Final Batch Loss: 0.3634859025478363\n",
      "Epoch 2503, Loss: 2.041433811187744, Final Batch Loss: 0.5365269184112549\n",
      "Epoch 2504, Loss: 1.9881337881088257, Final Batch Loss: 0.4571235477924347\n",
      "Epoch 2505, Loss: 2.0096474289894104, Final Batch Loss: 0.5276846885681152\n",
      "Epoch 2506, Loss: 2.1266531944274902, Final Batch Loss: 0.48768314719200134\n",
      "Epoch 2507, Loss: 2.0327785313129425, Final Batch Loss: 0.45736032724380493\n",
      "Epoch 2508, Loss: 1.9923149049282074, Final Batch Loss: 0.37117472290992737\n",
      "Epoch 2509, Loss: 2.045259475708008, Final Batch Loss: 0.3369053602218628\n",
      "Epoch 2510, Loss: 2.030867278575897, Final Batch Loss: 0.4144860804080963\n",
      "Epoch 2511, Loss: 2.0883805751800537, Final Batch Loss: 0.4130195677280426\n",
      "Epoch 2512, Loss: 1.8478932976722717, Final Batch Loss: 0.2951393723487854\n",
      "Epoch 2513, Loss: 1.8649995028972626, Final Batch Loss: 0.35495108366012573\n",
      "Epoch 2514, Loss: 1.926622599363327, Final Batch Loss: 0.3277021646499634\n",
      "Epoch 2515, Loss: 2.1190071403980255, Final Batch Loss: 0.45159679651260376\n",
      "Epoch 2516, Loss: 1.9318617582321167, Final Batch Loss: 0.35950997471809387\n",
      "Epoch 2517, Loss: 1.9160038828849792, Final Batch Loss: 0.35646510124206543\n",
      "Epoch 2518, Loss: 2.0496213734149933, Final Batch Loss: 0.4540795683860779\n",
      "Epoch 2519, Loss: 1.9898645281791687, Final Batch Loss: 0.45222797989845276\n",
      "Epoch 2520, Loss: 1.8578086495399475, Final Batch Loss: 0.3422681391239166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2521, Loss: 1.9733741283416748, Final Batch Loss: 0.3766345977783203\n",
      "Epoch 2522, Loss: 1.958930492401123, Final Batch Loss: 0.4694306552410126\n",
      "Epoch 2523, Loss: 2.0479871928691864, Final Batch Loss: 0.5020045638084412\n",
      "Epoch 2524, Loss: 1.9203213155269623, Final Batch Loss: 0.4171474277973175\n",
      "Epoch 2525, Loss: 2.0801279842853546, Final Batch Loss: 0.3966872990131378\n",
      "Epoch 2526, Loss: 2.0066387355327606, Final Batch Loss: 0.3632608652114868\n",
      "Epoch 2527, Loss: 1.9687344431877136, Final Batch Loss: 0.47918701171875\n",
      "Epoch 2528, Loss: 2.0530896484851837, Final Batch Loss: 0.36593377590179443\n",
      "Epoch 2529, Loss: 2.005842387676239, Final Batch Loss: 0.3654079735279083\n",
      "Epoch 2530, Loss: 1.9897390305995941, Final Batch Loss: 0.40612849593162537\n",
      "Epoch 2531, Loss: 2.010698437690735, Final Batch Loss: 0.29541146755218506\n",
      "Epoch 2532, Loss: 2.1347692906856537, Final Batch Loss: 0.46038761734962463\n",
      "Epoch 2533, Loss: 2.0230338275432587, Final Batch Loss: 0.4351002871990204\n",
      "Epoch 2534, Loss: 1.9325138330459595, Final Batch Loss: 0.40076616406440735\n",
      "Epoch 2535, Loss: 2.0008396804332733, Final Batch Loss: 0.42010822892189026\n",
      "Epoch 2536, Loss: 1.8735594749450684, Final Batch Loss: 0.2534695565700531\n",
      "Epoch 2537, Loss: 1.8883306682109833, Final Batch Loss: 0.320341557264328\n",
      "Epoch 2538, Loss: 1.9922878444194794, Final Batch Loss: 0.45606115460395813\n",
      "Epoch 2539, Loss: 1.8218328952789307, Final Batch Loss: 0.31355834007263184\n",
      "Epoch 2540, Loss: 1.994376689195633, Final Batch Loss: 0.4163682758808136\n",
      "Epoch 2541, Loss: 2.1089003682136536, Final Batch Loss: 0.630616307258606\n",
      "Epoch 2542, Loss: 2.0068813860416412, Final Batch Loss: 0.35382091999053955\n",
      "Epoch 2543, Loss: 1.9182632267475128, Final Batch Loss: 0.35500189661979675\n",
      "Epoch 2544, Loss: 1.8899385333061218, Final Batch Loss: 0.4040200710296631\n",
      "Epoch 2545, Loss: 2.053176522254944, Final Batch Loss: 0.42425641417503357\n",
      "Epoch 2546, Loss: 1.988142967224121, Final Batch Loss: 0.4635062515735626\n",
      "Epoch 2547, Loss: 2.0271702706813812, Final Batch Loss: 0.43602266907691956\n",
      "Epoch 2548, Loss: 1.896424412727356, Final Batch Loss: 0.42987391352653503\n",
      "Epoch 2549, Loss: 2.0306995809078217, Final Batch Loss: 0.37915360927581787\n",
      "Epoch 2550, Loss: 2.040532648563385, Final Batch Loss: 0.5147602558135986\n",
      "Epoch 2551, Loss: 1.8471039533615112, Final Batch Loss: 0.33577778935432434\n",
      "Epoch 2552, Loss: 1.8258309066295624, Final Batch Loss: 0.351840078830719\n",
      "Epoch 2553, Loss: 1.953169345855713, Final Batch Loss: 0.36111363768577576\n",
      "Epoch 2554, Loss: 1.7844057828187943, Final Batch Loss: 0.24513860046863556\n",
      "Epoch 2555, Loss: 1.8205402791500092, Final Batch Loss: 0.40971267223358154\n",
      "Epoch 2556, Loss: 2.025191515684128, Final Batch Loss: 0.4085991382598877\n",
      "Epoch 2557, Loss: 1.8778232038021088, Final Batch Loss: 0.4349503219127655\n",
      "Epoch 2558, Loss: 1.981450378894806, Final Batch Loss: 0.3562634587287903\n",
      "Epoch 2559, Loss: 1.914732962846756, Final Batch Loss: 0.4079816937446594\n",
      "Epoch 2560, Loss: 2.033582091331482, Final Batch Loss: 0.4747243821620941\n",
      "Epoch 2561, Loss: 2.067058354616165, Final Batch Loss: 0.42974579334259033\n",
      "Epoch 2562, Loss: 1.9159560203552246, Final Batch Loss: 0.40322908759117126\n",
      "Epoch 2563, Loss: 2.073007255792618, Final Batch Loss: 0.540574312210083\n",
      "Epoch 2564, Loss: 1.9773023128509521, Final Batch Loss: 0.35735851526260376\n",
      "Epoch 2565, Loss: 2.0699639916419983, Final Batch Loss: 0.4474315345287323\n",
      "Epoch 2566, Loss: 1.9351098239421844, Final Batch Loss: 0.3642553985118866\n",
      "Epoch 2567, Loss: 1.9710870385169983, Final Batch Loss: 0.5779251456260681\n",
      "Epoch 2568, Loss: 2.005973696708679, Final Batch Loss: 0.40304887294769287\n",
      "Epoch 2569, Loss: 1.9497552812099457, Final Batch Loss: 0.3466947376728058\n",
      "Epoch 2570, Loss: 1.9653037786483765, Final Batch Loss: 0.4153999984264374\n",
      "Epoch 2571, Loss: 2.0786518156528473, Final Batch Loss: 0.5430862903594971\n",
      "Epoch 2572, Loss: 1.9573173522949219, Final Batch Loss: 0.38096854090690613\n",
      "Epoch 2573, Loss: 2.0005073845386505, Final Batch Loss: 0.43453752994537354\n",
      "Epoch 2574, Loss: 1.8685947954654694, Final Batch Loss: 0.42244625091552734\n",
      "Epoch 2575, Loss: 1.9852436780929565, Final Batch Loss: 0.4528447687625885\n",
      "Epoch 2576, Loss: 1.8890051543712616, Final Batch Loss: 0.38777703046798706\n",
      "Epoch 2577, Loss: 2.0200260281562805, Final Batch Loss: 0.49316027760505676\n",
      "Epoch 2578, Loss: 1.8721821308135986, Final Batch Loss: 0.3671019375324249\n",
      "Epoch 2579, Loss: 1.9381105303764343, Final Batch Loss: 0.44270041584968567\n",
      "Epoch 2580, Loss: 1.9035526514053345, Final Batch Loss: 0.45509469509124756\n",
      "Epoch 2581, Loss: 2.0145341753959656, Final Batch Loss: 0.4293932318687439\n",
      "Epoch 2582, Loss: 1.995675414800644, Final Batch Loss: 0.4192889332771301\n",
      "Epoch 2583, Loss: 2.025874227285385, Final Batch Loss: 0.394096702337265\n",
      "Epoch 2584, Loss: 1.9682926535606384, Final Batch Loss: 0.38568174839019775\n",
      "Epoch 2585, Loss: 1.8840818107128143, Final Batch Loss: 0.381366103887558\n",
      "Epoch 2586, Loss: 1.978479951620102, Final Batch Loss: 0.4030785858631134\n",
      "Epoch 2587, Loss: 1.9294278025627136, Final Batch Loss: 0.3660091161727905\n",
      "Epoch 2588, Loss: 1.9821574687957764, Final Batch Loss: 0.47388574481010437\n",
      "Epoch 2589, Loss: 1.8106746971607208, Final Batch Loss: 0.3595656156539917\n",
      "Epoch 2590, Loss: 1.9346762597560883, Final Batch Loss: 0.3731881380081177\n",
      "Epoch 2591, Loss: 1.9610414803028107, Final Batch Loss: 0.4273240864276886\n",
      "Epoch 2592, Loss: 2.0224004089832306, Final Batch Loss: 0.4044337272644043\n",
      "Epoch 2593, Loss: 1.9756483435630798, Final Batch Loss: 0.3943672776222229\n",
      "Epoch 2594, Loss: 1.8944758772850037, Final Batch Loss: 0.3296337127685547\n",
      "Epoch 2595, Loss: 2.0613542795181274, Final Batch Loss: 0.4162052273750305\n",
      "Epoch 2596, Loss: 1.891219437122345, Final Batch Loss: 0.374459832906723\n",
      "Epoch 2597, Loss: 1.915908694267273, Final Batch Loss: 0.3300391733646393\n",
      "Epoch 2598, Loss: 2.0574724078178406, Final Batch Loss: 0.442495197057724\n",
      "Epoch 2599, Loss: 1.9783207774162292, Final Batch Loss: 0.4271346926689148\n",
      "Epoch 2600, Loss: 1.9738166332244873, Final Batch Loss: 0.4180518388748169\n",
      "Epoch 2601, Loss: 1.8968547880649567, Final Batch Loss: 0.385575532913208\n",
      "Epoch 2602, Loss: 2.0070548057556152, Final Batch Loss: 0.39671239256858826\n",
      "Epoch 2603, Loss: 1.9848202466964722, Final Batch Loss: 0.3699178397655487\n",
      "Epoch 2604, Loss: 1.9969233870506287, Final Batch Loss: 0.3948460519313812\n",
      "Epoch 2605, Loss: 1.880366325378418, Final Batch Loss: 0.3545890152454376\n",
      "Epoch 2606, Loss: 1.9338840544223785, Final Batch Loss: 0.3805064857006073\n",
      "Epoch 2607, Loss: 1.9111612141132355, Final Batch Loss: 0.34887343645095825\n",
      "Epoch 2608, Loss: 1.9204915463924408, Final Batch Loss: 0.3619617521762848\n",
      "Epoch 2609, Loss: 1.9582312405109406, Final Batch Loss: 0.396089106798172\n",
      "Epoch 2610, Loss: 1.898364543914795, Final Batch Loss: 0.34164243936538696\n",
      "Epoch 2611, Loss: 1.9129315912723541, Final Batch Loss: 0.47827643156051636\n",
      "Epoch 2612, Loss: 1.9217492043972015, Final Batch Loss: 0.475944459438324\n",
      "Epoch 2613, Loss: 2.0298335552215576, Final Batch Loss: 0.3639686703681946\n",
      "Epoch 2614, Loss: 1.920691192150116, Final Batch Loss: 0.4500863552093506\n",
      "Epoch 2615, Loss: 1.9572487771511078, Final Batch Loss: 0.3489621579647064\n",
      "Epoch 2616, Loss: 1.9663719534873962, Final Batch Loss: 0.3454144299030304\n",
      "Epoch 2617, Loss: 1.9521655440330505, Final Batch Loss: 0.38545551896095276\n",
      "Epoch 2618, Loss: 1.9505717754364014, Final Batch Loss: 0.3626234233379364\n",
      "Epoch 2619, Loss: 1.9378970265388489, Final Batch Loss: 0.40156251192092896\n",
      "Epoch 2620, Loss: 1.9387498199939728, Final Batch Loss: 0.37798652052879333\n",
      "Epoch 2621, Loss: 2.0056484639644623, Final Batch Loss: 0.4197985827922821\n",
      "Epoch 2622, Loss: 1.8687452673912048, Final Batch Loss: 0.3232972323894501\n",
      "Epoch 2623, Loss: 1.9267456233501434, Final Batch Loss: 0.3739498555660248\n",
      "Epoch 2624, Loss: 1.8779833018779755, Final Batch Loss: 0.35234037041664124\n",
      "Epoch 2625, Loss: 1.8521389663219452, Final Batch Loss: 0.43322330713272095\n",
      "Epoch 2626, Loss: 2.001937508583069, Final Batch Loss: 0.3554929792881012\n",
      "Epoch 2627, Loss: 1.9052189886569977, Final Batch Loss: 0.3906877040863037\n",
      "Epoch 2628, Loss: 1.91081964969635, Final Batch Loss: 0.4152635335922241\n",
      "Epoch 2629, Loss: 1.8928686380386353, Final Batch Loss: 0.3660932779312134\n",
      "Epoch 2630, Loss: 1.78256356716156, Final Batch Loss: 0.34097838401794434\n",
      "Epoch 2631, Loss: 1.8818968832492828, Final Batch Loss: 0.34507355093955994\n",
      "Epoch 2632, Loss: 2.003005862236023, Final Batch Loss: 0.4283502399921417\n",
      "Epoch 2633, Loss: 2.027721107006073, Final Batch Loss: 0.4077632427215576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2634, Loss: 1.8636903166770935, Final Batch Loss: 0.3044089078903198\n",
      "Epoch 2635, Loss: 1.8652081191539764, Final Batch Loss: 0.4076193869113922\n",
      "Epoch 2636, Loss: 1.8730967342853546, Final Batch Loss: 0.29980793595314026\n",
      "Epoch 2637, Loss: 1.773716926574707, Final Batch Loss: 0.4103841483592987\n",
      "Epoch 2638, Loss: 2.105531245470047, Final Batch Loss: 0.40682610869407654\n",
      "Epoch 2639, Loss: 1.958034098148346, Final Batch Loss: 0.42544081807136536\n",
      "Epoch 2640, Loss: 1.8840610086917877, Final Batch Loss: 0.2608497142791748\n",
      "Epoch 2641, Loss: 1.9282330870628357, Final Batch Loss: 0.40733176469802856\n",
      "Epoch 2642, Loss: 1.937995582818985, Final Batch Loss: 0.3762161433696747\n",
      "Epoch 2643, Loss: 1.9124414026737213, Final Batch Loss: 0.43095871806144714\n",
      "Epoch 2644, Loss: 1.8895091712474823, Final Batch Loss: 0.39581671357154846\n",
      "Epoch 2645, Loss: 1.889990895986557, Final Batch Loss: 0.41194960474967957\n",
      "Epoch 2646, Loss: 1.880478322505951, Final Batch Loss: 0.35912883281707764\n",
      "Epoch 2647, Loss: 2.169492393732071, Final Batch Loss: 0.3597145974636078\n",
      "Epoch 2648, Loss: 1.9715774059295654, Final Batch Loss: 0.4675661325454712\n",
      "Epoch 2649, Loss: 1.9084547758102417, Final Batch Loss: 0.3938746750354767\n",
      "Epoch 2650, Loss: 1.836868941783905, Final Batch Loss: 0.3484237790107727\n",
      "Epoch 2651, Loss: 1.7432999312877655, Final Batch Loss: 0.34473586082458496\n",
      "Epoch 2652, Loss: 1.927981823682785, Final Batch Loss: 0.43307653069496155\n",
      "Epoch 2653, Loss: 1.8677197694778442, Final Batch Loss: 0.42235007882118225\n",
      "Epoch 2654, Loss: 2.0436886847019196, Final Batch Loss: 0.4364662766456604\n",
      "Epoch 2655, Loss: 2.0026620626449585, Final Batch Loss: 0.3950692415237427\n",
      "Epoch 2656, Loss: 1.8928419351577759, Final Batch Loss: 0.32931819558143616\n",
      "Epoch 2657, Loss: 1.9199620485305786, Final Batch Loss: 0.3733157217502594\n",
      "Epoch 2658, Loss: 1.958421528339386, Final Batch Loss: 0.36301568150520325\n",
      "Epoch 2659, Loss: 2.01677268743515, Final Batch Loss: 0.49591532349586487\n",
      "Epoch 2660, Loss: 1.9621661603450775, Final Batch Loss: 0.43716949224472046\n",
      "Epoch 2661, Loss: 2.064680427312851, Final Batch Loss: 0.5235514640808105\n",
      "Epoch 2662, Loss: 1.892737478017807, Final Batch Loss: 0.36905789375305176\n",
      "Epoch 2663, Loss: 1.835137814283371, Final Batch Loss: 0.3948017656803131\n",
      "Epoch 2664, Loss: 2.061383843421936, Final Batch Loss: 0.5497416257858276\n",
      "Epoch 2665, Loss: 1.8794506788253784, Final Batch Loss: 0.361092746257782\n",
      "Epoch 2666, Loss: 1.9084109365940094, Final Batch Loss: 0.36825501918792725\n",
      "Epoch 2667, Loss: 1.9297490119934082, Final Batch Loss: 0.4091740548610687\n",
      "Epoch 2668, Loss: 1.7884780168533325, Final Batch Loss: 0.33252623677253723\n",
      "Epoch 2669, Loss: 1.88461235165596, Final Batch Loss: 0.4224076569080353\n",
      "Epoch 2670, Loss: 2.0498715341091156, Final Batch Loss: 0.4049510061740875\n",
      "Epoch 2671, Loss: 1.9335716664791107, Final Batch Loss: 0.3654390275478363\n",
      "Epoch 2672, Loss: 1.9204701483249664, Final Batch Loss: 0.2993491291999817\n",
      "Epoch 2673, Loss: 1.8627945482730865, Final Batch Loss: 0.40996959805488586\n",
      "Epoch 2674, Loss: 1.9740256071090698, Final Batch Loss: 0.4648960530757904\n",
      "Epoch 2675, Loss: 2.1366193890571594, Final Batch Loss: 0.521029531955719\n",
      "Epoch 2676, Loss: 1.9231303036212921, Final Batch Loss: 0.39304134249687195\n",
      "Epoch 2677, Loss: 1.949093371629715, Final Batch Loss: 0.46012425422668457\n",
      "Epoch 2678, Loss: 1.9499869644641876, Final Batch Loss: 0.39402252435684204\n",
      "Epoch 2679, Loss: 1.8921675086021423, Final Batch Loss: 0.3334440290927887\n",
      "Epoch 2680, Loss: 1.8238424956798553, Final Batch Loss: 0.3672190308570862\n",
      "Epoch 2681, Loss: 1.9779964089393616, Final Batch Loss: 0.3878670930862427\n",
      "Epoch 2682, Loss: 1.898651510477066, Final Batch Loss: 0.33770036697387695\n",
      "Epoch 2683, Loss: 1.8940946459770203, Final Batch Loss: 0.3484283685684204\n",
      "Epoch 2684, Loss: 1.8978489339351654, Final Batch Loss: 0.40078186988830566\n",
      "Epoch 2685, Loss: 1.8696492612361908, Final Batch Loss: 0.33193856477737427\n",
      "Epoch 2686, Loss: 1.8003389239311218, Final Batch Loss: 0.3350965082645416\n",
      "Epoch 2687, Loss: 1.7949222028255463, Final Batch Loss: 0.39241859316825867\n",
      "Epoch 2688, Loss: 1.9843773543834686, Final Batch Loss: 0.44174250960350037\n",
      "Epoch 2689, Loss: 1.966133713722229, Final Batch Loss: 0.4793567657470703\n",
      "Epoch 2690, Loss: 1.752004325389862, Final Batch Loss: 0.25471338629722595\n",
      "Epoch 2691, Loss: 1.938455879688263, Final Batch Loss: 0.3872709274291992\n",
      "Epoch 2692, Loss: 1.7755740284919739, Final Batch Loss: 0.31230488419532776\n",
      "Epoch 2693, Loss: 1.888288676738739, Final Batch Loss: 0.30826637148857117\n",
      "Epoch 2694, Loss: 1.9437043964862823, Final Batch Loss: 0.4844381809234619\n",
      "Epoch 2695, Loss: 1.946261078119278, Final Batch Loss: 0.35784274339675903\n",
      "Epoch 2696, Loss: 2.000421494245529, Final Batch Loss: 0.4298957884311676\n",
      "Epoch 2697, Loss: 1.8959275484085083, Final Batch Loss: 0.34659579396247864\n",
      "Epoch 2698, Loss: 1.9762287437915802, Final Batch Loss: 0.42125189304351807\n",
      "Epoch 2699, Loss: 1.976121723651886, Final Batch Loss: 0.4171973466873169\n",
      "Epoch 2700, Loss: 1.8579829633235931, Final Batch Loss: 0.3823685348033905\n",
      "Epoch 2701, Loss: 2.004012167453766, Final Batch Loss: 0.5170688033103943\n",
      "Epoch 2702, Loss: 1.963824063539505, Final Batch Loss: 0.3451654613018036\n",
      "Epoch 2703, Loss: 1.848625361919403, Final Batch Loss: 0.30345308780670166\n",
      "Epoch 2704, Loss: 1.9430510699748993, Final Batch Loss: 0.4334524869918823\n",
      "Epoch 2705, Loss: 1.7842386066913605, Final Batch Loss: 0.35855668783187866\n",
      "Epoch 2706, Loss: 1.925002783536911, Final Batch Loss: 0.428175151348114\n",
      "Epoch 2707, Loss: 1.8819907009601593, Final Batch Loss: 0.3891400694847107\n",
      "Epoch 2708, Loss: 1.8861935138702393, Final Batch Loss: 0.44922110438346863\n",
      "Epoch 2709, Loss: 1.8849367797374725, Final Batch Loss: 0.36764851212501526\n",
      "Epoch 2710, Loss: 1.9812557995319366, Final Batch Loss: 0.3967214524745941\n",
      "Epoch 2711, Loss: 1.8920787870883942, Final Batch Loss: 0.48587653040885925\n",
      "Epoch 2712, Loss: 1.7973517775535583, Final Batch Loss: 0.28815957903862\n",
      "Epoch 2713, Loss: 2.015942692756653, Final Batch Loss: 0.4244673252105713\n",
      "Epoch 2714, Loss: 1.8415579795837402, Final Batch Loss: 0.2897126078605652\n",
      "Epoch 2715, Loss: 1.9568148851394653, Final Batch Loss: 0.38906657695770264\n",
      "Epoch 2716, Loss: 1.8310515582561493, Final Batch Loss: 0.2725140154361725\n",
      "Epoch 2717, Loss: 1.8705781698226929, Final Batch Loss: 0.3037753403186798\n",
      "Epoch 2718, Loss: 1.9042127132415771, Final Batch Loss: 0.4055468738079071\n",
      "Epoch 2719, Loss: 1.765845149755478, Final Batch Loss: 0.36272287368774414\n",
      "Epoch 2720, Loss: 1.9866129159927368, Final Batch Loss: 0.3908138871192932\n",
      "Epoch 2721, Loss: 1.8956289291381836, Final Batch Loss: 0.4099304974079132\n",
      "Epoch 2722, Loss: 1.8533946871757507, Final Batch Loss: 0.3137710690498352\n",
      "Epoch 2723, Loss: 2.138487845659256, Final Batch Loss: 0.39953407645225525\n",
      "Epoch 2724, Loss: 1.8421458899974823, Final Batch Loss: 0.40274178981781006\n",
      "Epoch 2725, Loss: 2.001061648130417, Final Batch Loss: 0.4415040612220764\n",
      "Epoch 2726, Loss: 1.8227612376213074, Final Batch Loss: 0.4391942024230957\n",
      "Epoch 2727, Loss: 1.826120913028717, Final Batch Loss: 0.3106968402862549\n",
      "Epoch 2728, Loss: 1.9020195603370667, Final Batch Loss: 0.47533637285232544\n",
      "Epoch 2729, Loss: 1.9072302281856537, Final Batch Loss: 0.30997025966644287\n",
      "Epoch 2730, Loss: 1.7920808792114258, Final Batch Loss: 0.37533265352249146\n",
      "Epoch 2731, Loss: 1.9183114171028137, Final Batch Loss: 0.35169702768325806\n",
      "Epoch 2732, Loss: 1.8544011116027832, Final Batch Loss: 0.3697475492954254\n",
      "Epoch 2733, Loss: 1.7962377071380615, Final Batch Loss: 0.3097156882286072\n",
      "Epoch 2734, Loss: 1.8472537994384766, Final Batch Loss: 0.3473668694496155\n",
      "Epoch 2735, Loss: 1.8045706152915955, Final Batch Loss: 0.3240678310394287\n",
      "Epoch 2736, Loss: 2.140951842069626, Final Batch Loss: 0.4103096127510071\n",
      "Epoch 2737, Loss: 2.0502989590168, Final Batch Loss: 0.4305347502231598\n",
      "Epoch 2738, Loss: 2.0695061087608337, Final Batch Loss: 0.41947075724601746\n",
      "Epoch 2739, Loss: 1.9608192443847656, Final Batch Loss: 0.34843721985816956\n",
      "Epoch 2740, Loss: 1.8419898748397827, Final Batch Loss: 0.34506121277809143\n",
      "Epoch 2741, Loss: 1.9490387737751007, Final Batch Loss: 0.43205124139785767\n",
      "Epoch 2742, Loss: 1.8785271644592285, Final Batch Loss: 0.29736092686653137\n",
      "Epoch 2743, Loss: 1.9517385065555573, Final Batch Loss: 0.3606503903865814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2744, Loss: 1.9639984667301178, Final Batch Loss: 0.4100292921066284\n",
      "Epoch 2745, Loss: 1.9069938659667969, Final Batch Loss: 0.3713228404521942\n",
      "Epoch 2746, Loss: 1.9232973754405975, Final Batch Loss: 0.40215274691581726\n",
      "Epoch 2747, Loss: 1.9971303939819336, Final Batch Loss: 0.534013032913208\n",
      "Epoch 2748, Loss: 1.8184510171413422, Final Batch Loss: 0.3435094952583313\n",
      "Epoch 2749, Loss: 1.8045083582401276, Final Batch Loss: 0.38092267513275146\n",
      "Epoch 2750, Loss: 1.8176154494285583, Final Batch Loss: 0.3699552118778229\n",
      "Epoch 2751, Loss: 1.8237963020801544, Final Batch Loss: 0.3339129388332367\n",
      "Epoch 2752, Loss: 1.8341984450817108, Final Batch Loss: 0.31829139590263367\n",
      "Epoch 2753, Loss: 2.010354310274124, Final Batch Loss: 0.44941890239715576\n",
      "Epoch 2754, Loss: 1.9219582080841064, Final Batch Loss: 0.355569064617157\n",
      "Epoch 2755, Loss: 1.8377066850662231, Final Batch Loss: 0.3955465257167816\n",
      "Epoch 2756, Loss: 1.9607917070388794, Final Batch Loss: 0.36384034156799316\n",
      "Epoch 2757, Loss: 1.8982642889022827, Final Batch Loss: 0.3518659472465515\n",
      "Epoch 2758, Loss: 1.8696417808532715, Final Batch Loss: 0.3567603826522827\n",
      "Epoch 2759, Loss: 1.8323837220668793, Final Batch Loss: 0.322986900806427\n",
      "Epoch 2760, Loss: 1.828182727098465, Final Batch Loss: 0.336610347032547\n",
      "Epoch 2761, Loss: 1.8812820315361023, Final Batch Loss: 0.45515206456184387\n",
      "Epoch 2762, Loss: 1.8961414694786072, Final Batch Loss: 0.32011187076568604\n",
      "Epoch 2763, Loss: 1.9232313632965088, Final Batch Loss: 0.414826899766922\n",
      "Epoch 2764, Loss: 1.9471465647220612, Final Batch Loss: 0.40147724747657776\n",
      "Epoch 2765, Loss: 1.8118758797645569, Final Batch Loss: 0.3280785083770752\n",
      "Epoch 2766, Loss: 1.8060352802276611, Final Batch Loss: 0.41490939259529114\n",
      "Epoch 2767, Loss: 1.8973885476589203, Final Batch Loss: 0.38598448038101196\n",
      "Epoch 2768, Loss: 2.0831001102924347, Final Batch Loss: 0.6100084781646729\n",
      "Epoch 2769, Loss: 1.9141210317611694, Final Batch Loss: 0.3583424687385559\n",
      "Epoch 2770, Loss: 1.7902900874614716, Final Batch Loss: 0.27887293696403503\n",
      "Epoch 2771, Loss: 1.746498703956604, Final Batch Loss: 0.32420089840888977\n",
      "Epoch 2772, Loss: 1.9886431396007538, Final Batch Loss: 0.3811931014060974\n",
      "Epoch 2773, Loss: 1.710206925868988, Final Batch Loss: 0.3350663483142853\n",
      "Epoch 2774, Loss: 1.88405379652977, Final Batch Loss: 0.4502135217189789\n",
      "Epoch 2775, Loss: 1.9296954274177551, Final Batch Loss: 0.3352862000465393\n",
      "Epoch 2776, Loss: 1.70231893658638, Final Batch Loss: 0.32544052600860596\n",
      "Epoch 2777, Loss: 1.8434984683990479, Final Batch Loss: 0.43548381328582764\n",
      "Epoch 2778, Loss: 1.79006627202034, Final Batch Loss: 0.2911536991596222\n",
      "Epoch 2779, Loss: 1.877121090888977, Final Batch Loss: 0.2704675793647766\n",
      "Epoch 2780, Loss: 1.8147380948066711, Final Batch Loss: 0.41198334097862244\n",
      "Epoch 2781, Loss: 1.897448867559433, Final Batch Loss: 0.33371251821517944\n",
      "Epoch 2782, Loss: 1.8856565058231354, Final Batch Loss: 0.4655757546424866\n",
      "Epoch 2783, Loss: 1.7971101999282837, Final Batch Loss: 0.27670443058013916\n",
      "Epoch 2784, Loss: 1.8230900764465332, Final Batch Loss: 0.39167481660842896\n",
      "Epoch 2785, Loss: 1.9850660860538483, Final Batch Loss: 0.5008219480514526\n",
      "Epoch 2786, Loss: 1.8276258409023285, Final Batch Loss: 0.38392361998558044\n",
      "Epoch 2787, Loss: 1.803871899843216, Final Batch Loss: 0.36679574847221375\n",
      "Epoch 2788, Loss: 1.8309106826782227, Final Batch Loss: 0.42731979489326477\n",
      "Epoch 2789, Loss: 1.9915598630905151, Final Batch Loss: 0.39688724279403687\n",
      "Epoch 2790, Loss: 1.9359487295150757, Final Batch Loss: 0.36835333704948425\n",
      "Epoch 2791, Loss: 1.8045944571495056, Final Batch Loss: 0.41520366072654724\n",
      "Epoch 2792, Loss: 1.8881790041923523, Final Batch Loss: 0.4445013105869293\n",
      "Epoch 2793, Loss: 1.866955816745758, Final Batch Loss: 0.3885248601436615\n",
      "Epoch 2794, Loss: 1.7148197889328003, Final Batch Loss: 0.31703370809555054\n",
      "Epoch 2795, Loss: 1.8554781079292297, Final Batch Loss: 0.33946678042411804\n",
      "Epoch 2796, Loss: 1.9160229563713074, Final Batch Loss: 0.4597931504249573\n",
      "Epoch 2797, Loss: 1.7504933178424835, Final Batch Loss: 0.2821815609931946\n",
      "Epoch 2798, Loss: 1.7602294981479645, Final Batch Loss: 0.3743915557861328\n",
      "Epoch 2799, Loss: 1.8875440657138824, Final Batch Loss: 0.3816649913787842\n",
      "Epoch 2800, Loss: 1.8786450028419495, Final Batch Loss: 0.32283174991607666\n",
      "Epoch 2801, Loss: 1.6898443102836609, Final Batch Loss: 0.35339149832725525\n",
      "Epoch 2802, Loss: 2.0268391966819763, Final Batch Loss: 0.35037535429000854\n",
      "Epoch 2803, Loss: 1.9519138038158417, Final Batch Loss: 0.37115105986595154\n",
      "Epoch 2804, Loss: 1.9523068368434906, Final Batch Loss: 0.33581599593162537\n",
      "Epoch 2805, Loss: 1.9587352871894836, Final Batch Loss: 0.41729676723480225\n",
      "Epoch 2806, Loss: 1.8145601153373718, Final Batch Loss: 0.2923707962036133\n",
      "Epoch 2807, Loss: 1.8935161232948303, Final Batch Loss: 0.36014553904533386\n",
      "Epoch 2808, Loss: 1.8125623762607574, Final Batch Loss: 0.41774606704711914\n",
      "Epoch 2809, Loss: 1.8518993854522705, Final Batch Loss: 0.32361724972724915\n",
      "Epoch 2810, Loss: 1.9148057401180267, Final Batch Loss: 0.43869709968566895\n",
      "Epoch 2811, Loss: 1.7826991975307465, Final Batch Loss: 0.3494527041912079\n",
      "Epoch 2812, Loss: 1.9146498143672943, Final Batch Loss: 0.3555230498313904\n",
      "Epoch 2813, Loss: 1.8093312084674835, Final Batch Loss: 0.3848516047000885\n",
      "Epoch 2814, Loss: 1.782006025314331, Final Batch Loss: 0.3370887339115143\n",
      "Epoch 2815, Loss: 1.958553522825241, Final Batch Loss: 0.32102665305137634\n",
      "Epoch 2816, Loss: 1.8504918217658997, Final Batch Loss: 0.31559446454048157\n",
      "Epoch 2817, Loss: 1.8083419501781464, Final Batch Loss: 0.31858178973197937\n",
      "Epoch 2818, Loss: 1.921316683292389, Final Batch Loss: 0.43208047747612\n",
      "Epoch 2819, Loss: 1.8277740627527237, Final Batch Loss: 0.23570550978183746\n",
      "Epoch 2820, Loss: 1.9318539500236511, Final Batch Loss: 0.35534530878067017\n",
      "Epoch 2821, Loss: 1.8171873986721039, Final Batch Loss: 0.4261024594306946\n",
      "Epoch 2822, Loss: 1.8350498676300049, Final Batch Loss: 0.325590580701828\n",
      "Epoch 2823, Loss: 1.8543983697891235, Final Batch Loss: 0.40092554688453674\n",
      "Epoch 2824, Loss: 1.7507728934288025, Final Batch Loss: 0.287852942943573\n",
      "Epoch 2825, Loss: 1.9351894855499268, Final Batch Loss: 0.4100956320762634\n",
      "Epoch 2826, Loss: 1.8157199025154114, Final Batch Loss: 0.318516343832016\n",
      "Epoch 2827, Loss: 1.9578470289707184, Final Batch Loss: 0.3483990728855133\n",
      "Epoch 2828, Loss: 1.885780930519104, Final Batch Loss: 0.4585321247577667\n",
      "Epoch 2829, Loss: 1.8206679224967957, Final Batch Loss: 0.3425767421722412\n",
      "Epoch 2830, Loss: 1.7978512048721313, Final Batch Loss: 0.36820387840270996\n",
      "Epoch 2831, Loss: 1.8481959849596024, Final Batch Loss: 0.24838562309741974\n",
      "Epoch 2832, Loss: 1.8447145223617554, Final Batch Loss: 0.41967782378196716\n",
      "Epoch 2833, Loss: 1.8541716039180756, Final Batch Loss: 0.3877190053462982\n",
      "Epoch 2834, Loss: 1.740048199892044, Final Batch Loss: 0.3071550130844116\n",
      "Epoch 2835, Loss: 1.745286613702774, Final Batch Loss: 0.3599053621292114\n",
      "Epoch 2836, Loss: 1.868445247411728, Final Batch Loss: 0.2722220718860626\n",
      "Epoch 2837, Loss: 1.9662627875804901, Final Batch Loss: 0.3704846203327179\n",
      "Epoch 2838, Loss: 1.922332227230072, Final Batch Loss: 0.3606511354446411\n",
      "Epoch 2839, Loss: 1.8926224410533905, Final Batch Loss: 0.332824170589447\n",
      "Epoch 2840, Loss: 1.850778877735138, Final Batch Loss: 0.3385753631591797\n",
      "Epoch 2841, Loss: 1.7982459366321564, Final Batch Loss: 0.3327200412750244\n",
      "Epoch 2842, Loss: 1.9332748353481293, Final Batch Loss: 0.4042797088623047\n",
      "Epoch 2843, Loss: 1.8486672639846802, Final Batch Loss: 0.4872690737247467\n",
      "Epoch 2844, Loss: 1.7656011283397675, Final Batch Loss: 0.44576138257980347\n",
      "Epoch 2845, Loss: 1.7975367307662964, Final Batch Loss: 0.4441879391670227\n",
      "Epoch 2846, Loss: 1.9868852198123932, Final Batch Loss: 0.41895899176597595\n",
      "Epoch 2847, Loss: 1.7725708186626434, Final Batch Loss: 0.35930681228637695\n",
      "Epoch 2848, Loss: 1.8664442002773285, Final Batch Loss: 0.3788006901741028\n",
      "Epoch 2849, Loss: 1.878525823354721, Final Batch Loss: 0.33954429626464844\n",
      "Epoch 2850, Loss: 1.7050567269325256, Final Batch Loss: 0.2890954315662384\n",
      "Epoch 2851, Loss: 1.886548936367035, Final Batch Loss: 0.4185114800930023\n",
      "Epoch 2852, Loss: 1.8769320249557495, Final Batch Loss: 0.41425538063049316\n",
      "Epoch 2853, Loss: 1.9112440049648285, Final Batch Loss: 0.3700399100780487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2854, Loss: 1.8212741911411285, Final Batch Loss: 0.34635791182518005\n",
      "Epoch 2855, Loss: 1.8861948549747467, Final Batch Loss: 0.35933515429496765\n",
      "Epoch 2856, Loss: 1.9348656833171844, Final Batch Loss: 0.4288400113582611\n",
      "Epoch 2857, Loss: 1.9025917053222656, Final Batch Loss: 0.41567328572273254\n",
      "Epoch 2858, Loss: 1.9235267341136932, Final Batch Loss: 0.4085145592689514\n",
      "Epoch 2859, Loss: 1.8372356593608856, Final Batch Loss: 0.3536002039909363\n",
      "Epoch 2860, Loss: 1.9584171175956726, Final Batch Loss: 0.4206010699272156\n",
      "Epoch 2861, Loss: 1.9570406079292297, Final Batch Loss: 0.40936633944511414\n",
      "Epoch 2862, Loss: 2.0720687806606293, Final Batch Loss: 0.3745979964733124\n",
      "Epoch 2863, Loss: 2.0299974381923676, Final Batch Loss: 0.5720052123069763\n",
      "Epoch 2864, Loss: 1.7928487658500671, Final Batch Loss: 0.3960138261318207\n",
      "Epoch 2865, Loss: 1.8508362770080566, Final Batch Loss: 0.4024449288845062\n",
      "Epoch 2866, Loss: 1.826490432024002, Final Batch Loss: 0.3191388249397278\n",
      "Epoch 2867, Loss: 1.7874804735183716, Final Batch Loss: 0.3746664524078369\n",
      "Epoch 2868, Loss: 1.7348860800266266, Final Batch Loss: 0.3303804099559784\n",
      "Epoch 2869, Loss: 1.7238745987415314, Final Batch Loss: 0.3453913927078247\n",
      "Epoch 2870, Loss: 1.7734748125076294, Final Batch Loss: 0.3787232041358948\n",
      "Epoch 2871, Loss: 1.7770790755748749, Final Batch Loss: 0.331504762172699\n",
      "Epoch 2872, Loss: 1.8922081887722015, Final Batch Loss: 0.505862295627594\n",
      "Epoch 2873, Loss: 1.9008533358573914, Final Batch Loss: 0.45981431007385254\n",
      "Epoch 2874, Loss: 1.8643536567687988, Final Batch Loss: 0.35517382621765137\n",
      "Epoch 2875, Loss: 1.7041650116443634, Final Batch Loss: 0.2947585880756378\n",
      "Epoch 2876, Loss: 1.7817203402519226, Final Batch Loss: 0.32009613513946533\n",
      "Epoch 2877, Loss: 1.6990701854228973, Final Batch Loss: 0.32022982835769653\n",
      "Epoch 2878, Loss: 2.00846067070961, Final Batch Loss: 0.4039059281349182\n",
      "Epoch 2879, Loss: 1.7540400624275208, Final Batch Loss: 0.36853957176208496\n",
      "Epoch 2880, Loss: 1.8463325500488281, Final Batch Loss: 0.3694537580013275\n",
      "Epoch 2881, Loss: 1.8581096827983856, Final Batch Loss: 0.4427042007446289\n",
      "Epoch 2882, Loss: 1.8315336406230927, Final Batch Loss: 0.3859971761703491\n",
      "Epoch 2883, Loss: 1.875886619091034, Final Batch Loss: 0.38031700253486633\n",
      "Epoch 2884, Loss: 1.7814319133758545, Final Batch Loss: 0.4091548025608063\n",
      "Epoch 2885, Loss: 1.7287479043006897, Final Batch Loss: 0.23194748163223267\n",
      "Epoch 2886, Loss: 1.818844050168991, Final Batch Loss: 0.28608018159866333\n",
      "Epoch 2887, Loss: 1.8106406927108765, Final Batch Loss: 0.2834297716617584\n",
      "Epoch 2888, Loss: 1.781946212053299, Final Batch Loss: 0.30173009634017944\n",
      "Epoch 2889, Loss: 1.9105511903762817, Final Batch Loss: 0.3056619167327881\n",
      "Epoch 2890, Loss: 1.7354351580142975, Final Batch Loss: 0.278215229511261\n",
      "Epoch 2891, Loss: 1.8174455165863037, Final Batch Loss: 0.3751862943172455\n",
      "Epoch 2892, Loss: 1.8766565918922424, Final Batch Loss: 0.35435569286346436\n",
      "Epoch 2893, Loss: 1.8578948974609375, Final Batch Loss: 0.4305380582809448\n",
      "Epoch 2894, Loss: 1.8018128871917725, Final Batch Loss: 0.3221816420555115\n",
      "Epoch 2895, Loss: 1.8438304364681244, Final Batch Loss: 0.35377633571624756\n",
      "Epoch 2896, Loss: 1.999843955039978, Final Batch Loss: 0.45159879326820374\n",
      "Epoch 2897, Loss: 1.8891027569770813, Final Batch Loss: 0.3551842272281647\n",
      "Epoch 2898, Loss: 1.7317203879356384, Final Batch Loss: 0.41525593400001526\n",
      "Epoch 2899, Loss: 1.8042477071285248, Final Batch Loss: 0.3283084034919739\n",
      "Epoch 2900, Loss: 1.6145635843276978, Final Batch Loss: 0.3006857633590698\n",
      "Epoch 2901, Loss: 1.8495203852653503, Final Batch Loss: 0.40975382924079895\n",
      "Epoch 2902, Loss: 1.7865805327892303, Final Batch Loss: 0.36656126379966736\n",
      "Epoch 2903, Loss: 1.8058911263942719, Final Batch Loss: 0.3555483818054199\n",
      "Epoch 2904, Loss: 1.877140611410141, Final Batch Loss: 0.42816194891929626\n",
      "Epoch 2905, Loss: 1.6859633922576904, Final Batch Loss: 0.26387301087379456\n",
      "Epoch 2906, Loss: 1.7848296463489532, Final Batch Loss: 0.40282532572746277\n",
      "Epoch 2907, Loss: 1.8470008969306946, Final Batch Loss: 0.3439803719520569\n",
      "Epoch 2908, Loss: 1.7913559973239899, Final Batch Loss: 0.3676251769065857\n",
      "Epoch 2909, Loss: 1.8796518743038177, Final Batch Loss: 0.3828832507133484\n",
      "Epoch 2910, Loss: 1.9489130973815918, Final Batch Loss: 0.4065743386745453\n",
      "Epoch 2911, Loss: 1.8406400084495544, Final Batch Loss: 0.33008861541748047\n",
      "Epoch 2912, Loss: 1.8128329813480377, Final Batch Loss: 0.28709012269973755\n",
      "Epoch 2913, Loss: 1.8603967726230621, Final Batch Loss: 0.2848227024078369\n",
      "Epoch 2914, Loss: 1.9465087354183197, Final Batch Loss: 0.36188462376594543\n",
      "Epoch 2915, Loss: 1.8184016048908234, Final Batch Loss: 0.3487749695777893\n",
      "Epoch 2916, Loss: 1.8370956778526306, Final Batch Loss: 0.349508136510849\n",
      "Epoch 2917, Loss: 1.7590622305870056, Final Batch Loss: 0.3532906174659729\n",
      "Epoch 2918, Loss: 1.8140727281570435, Final Batch Loss: 0.32341688871383667\n",
      "Epoch 2919, Loss: 1.8310722708702087, Final Batch Loss: 0.3907540440559387\n",
      "Epoch 2920, Loss: 1.8619423508644104, Final Batch Loss: 0.3415514826774597\n",
      "Epoch 2921, Loss: 1.9089712798595428, Final Batch Loss: 0.3746073246002197\n",
      "Epoch 2922, Loss: 1.7222025692462921, Final Batch Loss: 0.3286411166191101\n",
      "Epoch 2923, Loss: 1.864291787147522, Final Batch Loss: 0.3065696358680725\n",
      "Epoch 2924, Loss: 1.8111837804317474, Final Batch Loss: 0.397597074508667\n",
      "Epoch 2925, Loss: 1.823072373867035, Final Batch Loss: 0.3257661461830139\n",
      "Epoch 2926, Loss: 1.7381649613380432, Final Batch Loss: 0.29840439558029175\n",
      "Epoch 2927, Loss: 1.9177259802818298, Final Batch Loss: 0.3260042667388916\n",
      "Epoch 2928, Loss: 1.985866129398346, Final Batch Loss: 0.3884982466697693\n",
      "Epoch 2929, Loss: 1.7090975642204285, Final Batch Loss: 0.298125296831131\n",
      "Epoch 2930, Loss: 1.868457406759262, Final Batch Loss: 0.343747615814209\n",
      "Epoch 2931, Loss: 1.9601851403713226, Final Batch Loss: 0.416701078414917\n",
      "Epoch 2932, Loss: 1.8038332164287567, Final Batch Loss: 0.38068798184394836\n",
      "Epoch 2933, Loss: 1.6173072159290314, Final Batch Loss: 0.29411742091178894\n",
      "Epoch 2934, Loss: 1.6910737454891205, Final Batch Loss: 0.31484749913215637\n",
      "Epoch 2935, Loss: 1.9197155237197876, Final Batch Loss: 0.37003645300865173\n",
      "Epoch 2936, Loss: 1.773223727941513, Final Batch Loss: 0.36858800053596497\n",
      "Epoch 2937, Loss: 1.9302655756473541, Final Batch Loss: 0.4233505427837372\n",
      "Epoch 2938, Loss: 1.8922983407974243, Final Batch Loss: 0.3303999602794647\n",
      "Epoch 2939, Loss: 1.8287140727043152, Final Batch Loss: 0.3740311861038208\n",
      "Epoch 2940, Loss: 1.7671975493431091, Final Batch Loss: 0.38958123326301575\n",
      "Epoch 2941, Loss: 1.9249374270439148, Final Batch Loss: 0.3676132261753082\n",
      "Epoch 2942, Loss: 1.7558754980564117, Final Batch Loss: 0.30063343048095703\n",
      "Epoch 2943, Loss: 1.847091794013977, Final Batch Loss: 0.3677692413330078\n",
      "Epoch 2944, Loss: 1.825915277004242, Final Batch Loss: 0.4825020730495453\n",
      "Epoch 2945, Loss: 1.9989246129989624, Final Batch Loss: 0.5029367208480835\n",
      "Epoch 2946, Loss: 1.7981853485107422, Final Batch Loss: 0.34890270233154297\n",
      "Epoch 2947, Loss: 1.848472535610199, Final Batch Loss: 0.3861375153064728\n",
      "Epoch 2948, Loss: 1.8155122697353363, Final Batch Loss: 0.33247804641723633\n",
      "Epoch 2949, Loss: 1.8762574195861816, Final Batch Loss: 0.3539131283760071\n",
      "Epoch 2950, Loss: 1.8507654666900635, Final Batch Loss: 0.4157825708389282\n",
      "Epoch 2951, Loss: 1.8374064564704895, Final Batch Loss: 0.40541979670524597\n",
      "Epoch 2952, Loss: 1.8763531744480133, Final Batch Loss: 0.5090328454971313\n",
      "Epoch 2953, Loss: 1.7848879098892212, Final Batch Loss: 0.3159332275390625\n",
      "Epoch 2954, Loss: 1.8098621368408203, Final Batch Loss: 0.33703377842903137\n",
      "Epoch 2955, Loss: 1.8916314840316772, Final Batch Loss: 0.46442490816116333\n",
      "Epoch 2956, Loss: 1.9130010306835175, Final Batch Loss: 0.41629844903945923\n",
      "Epoch 2957, Loss: 2.036596029996872, Final Batch Loss: 0.37914717197418213\n",
      "Epoch 2958, Loss: 1.6154977679252625, Final Batch Loss: 0.25667980313301086\n",
      "Epoch 2959, Loss: 1.8465177118778229, Final Batch Loss: 0.3803924322128296\n",
      "Epoch 2960, Loss: 1.8663533627986908, Final Batch Loss: 0.4428849220275879\n",
      "Epoch 2961, Loss: 1.919968456029892, Final Batch Loss: 0.4001847207546234\n",
      "Epoch 2962, Loss: 1.9970985054969788, Final Batch Loss: 0.4570147693157196\n",
      "Epoch 2963, Loss: 1.8509761095046997, Final Batch Loss: 0.3718146085739136\n",
      "Epoch 2964, Loss: 1.8749395608901978, Final Batch Loss: 0.4660109877586365\n",
      "Epoch 2965, Loss: 1.9087408483028412, Final Batch Loss: 0.269627183675766\n",
      "Epoch 2966, Loss: 1.7764867842197418, Final Batch Loss: 0.29767921566963196\n",
      "Epoch 2967, Loss: 1.7899454534053802, Final Batch Loss: 0.4219283163547516\n",
      "Epoch 2968, Loss: 1.6792449951171875, Final Batch Loss: 0.3236926794052124\n",
      "Epoch 2969, Loss: 1.8091301620006561, Final Batch Loss: 0.3520621955394745\n",
      "Epoch 2970, Loss: 1.8482721745967865, Final Batch Loss: 0.27025106549263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2971, Loss: 1.9278774559497833, Final Batch Loss: 0.3910370469093323\n",
      "Epoch 2972, Loss: 1.821533054113388, Final Batch Loss: 0.35806748270988464\n",
      "Epoch 2973, Loss: 1.9838829338550568, Final Batch Loss: 0.4535943865776062\n",
      "Epoch 2974, Loss: 1.8934164941310883, Final Batch Loss: 0.38133081793785095\n",
      "Epoch 2975, Loss: 1.855663925409317, Final Batch Loss: 0.39557546377182007\n",
      "Epoch 2976, Loss: 1.7254669666290283, Final Batch Loss: 0.32674676179885864\n",
      "Epoch 2977, Loss: 1.8421262800693512, Final Batch Loss: 0.40645188093185425\n",
      "Epoch 2978, Loss: 1.776589810848236, Final Batch Loss: 0.23695707321166992\n",
      "Epoch 2979, Loss: 1.9268370270729065, Final Batch Loss: 0.28977981209754944\n",
      "Epoch 2980, Loss: 1.8494203090667725, Final Batch Loss: 0.4049064517021179\n",
      "Epoch 2981, Loss: 1.8444837927818298, Final Batch Loss: 0.3203730285167694\n",
      "Epoch 2982, Loss: 1.7882354259490967, Final Batch Loss: 0.3058076798915863\n",
      "Epoch 2983, Loss: 1.9227352142333984, Final Batch Loss: 0.4201297163963318\n",
      "Epoch 2984, Loss: 1.826394945383072, Final Batch Loss: 0.34993696212768555\n",
      "Epoch 2985, Loss: 1.849570095539093, Final Batch Loss: 0.38619866967201233\n",
      "Epoch 2986, Loss: 1.758194386959076, Final Batch Loss: 0.4104914665222168\n",
      "Epoch 2987, Loss: 1.8287211060523987, Final Batch Loss: 0.34279724955558777\n",
      "Epoch 2988, Loss: 1.6547768414020538, Final Batch Loss: 0.25402024388313293\n",
      "Epoch 2989, Loss: 1.8447380065917969, Final Batch Loss: 0.3948823809623718\n",
      "Epoch 2990, Loss: 1.8880062401294708, Final Batch Loss: 0.48306411504745483\n",
      "Epoch 2991, Loss: 1.7261322140693665, Final Batch Loss: 0.4081677198410034\n",
      "Epoch 2992, Loss: 1.7587601840496063, Final Batch Loss: 0.3216193914413452\n",
      "Epoch 2993, Loss: 1.7606033384799957, Final Batch Loss: 0.43257951736450195\n",
      "Epoch 2994, Loss: 1.8655243217945099, Final Batch Loss: 0.4741271436214447\n",
      "Epoch 2995, Loss: 1.8195926249027252, Final Batch Loss: 0.26668447256088257\n",
      "Epoch 2996, Loss: 1.7100284397602081, Final Batch Loss: 0.3275255858898163\n",
      "Epoch 2997, Loss: 1.673659086227417, Final Batch Loss: 0.34363824129104614\n",
      "Epoch 2998, Loss: 1.7941368520259857, Final Batch Loss: 0.3944553732872009\n",
      "Epoch 2999, Loss: 1.7848788499832153, Final Batch Loss: 0.35833877325057983\n",
      "Epoch 3000, Loss: 1.9919824004173279, Final Batch Loss: 0.3722746968269348\n",
      "Epoch 3001, Loss: 1.7954305410385132, Final Batch Loss: 0.3634127974510193\n",
      "Epoch 3002, Loss: 1.7940743565559387, Final Batch Loss: 0.43173882365226746\n",
      "Epoch 3003, Loss: 1.9119395315647125, Final Batch Loss: 0.35721057653427124\n",
      "Epoch 3004, Loss: 1.9256973564624786, Final Batch Loss: 0.27483993768692017\n",
      "Epoch 3005, Loss: 1.797975331544876, Final Batch Loss: 0.3539983332157135\n",
      "Epoch 3006, Loss: 1.778054028749466, Final Batch Loss: 0.4514147937297821\n",
      "Epoch 3007, Loss: 1.7718243896961212, Final Batch Loss: 0.2254239320755005\n",
      "Epoch 3008, Loss: 1.8658840358257294, Final Batch Loss: 0.36328673362731934\n",
      "Epoch 3009, Loss: 1.8928277790546417, Final Batch Loss: 0.46456199884414673\n",
      "Epoch 3010, Loss: 1.8878351151943207, Final Batch Loss: 0.4372296929359436\n",
      "Epoch 3011, Loss: 1.99282506108284, Final Batch Loss: 0.4198283851146698\n",
      "Epoch 3012, Loss: 1.81336110830307, Final Batch Loss: 0.3390803635120392\n",
      "Epoch 3013, Loss: 1.7211856842041016, Final Batch Loss: 0.38420042395591736\n",
      "Epoch 3014, Loss: 1.6205199360847473, Final Batch Loss: 0.30367887020111084\n",
      "Epoch 3015, Loss: 1.9662615358829498, Final Batch Loss: 0.38313066959381104\n",
      "Epoch 3016, Loss: 1.8035992980003357, Final Batch Loss: 0.3830932676792145\n",
      "Epoch 3017, Loss: 1.625706136226654, Final Batch Loss: 0.3263830542564392\n",
      "Epoch 3018, Loss: 1.8154998421669006, Final Batch Loss: 0.3516504168510437\n",
      "Epoch 3019, Loss: 1.747547835111618, Final Batch Loss: 0.3344670534133911\n",
      "Epoch 3020, Loss: 1.9151878952980042, Final Batch Loss: 0.4630577266216278\n",
      "Epoch 3021, Loss: 1.6694042384624481, Final Batch Loss: 0.2800295948982239\n",
      "Epoch 3022, Loss: 1.7109015882015228, Final Batch Loss: 0.33558422327041626\n",
      "Epoch 3023, Loss: 1.7578376233577728, Final Batch Loss: 0.3600432872772217\n",
      "Epoch 3024, Loss: 1.6350614130496979, Final Batch Loss: 0.3016683757305145\n",
      "Epoch 3025, Loss: 1.8260081112384796, Final Batch Loss: 0.3921701908111572\n",
      "Epoch 3026, Loss: 1.7898563742637634, Final Batch Loss: 0.3621944189071655\n",
      "Epoch 3027, Loss: 1.7937474846839905, Final Batch Loss: 0.28824663162231445\n",
      "Epoch 3028, Loss: 1.7331444323062897, Final Batch Loss: 0.3087840974330902\n",
      "Epoch 3029, Loss: 1.8561739027500153, Final Batch Loss: 0.37542983889579773\n",
      "Epoch 3030, Loss: 1.8357421159744263, Final Batch Loss: 0.4228416085243225\n",
      "Epoch 3031, Loss: 1.8868134915828705, Final Batch Loss: 0.3855099678039551\n",
      "Epoch 3032, Loss: 1.8784697651863098, Final Batch Loss: 0.3864079415798187\n",
      "Epoch 3033, Loss: 1.7807520925998688, Final Batch Loss: 0.3752128779888153\n",
      "Epoch 3034, Loss: 1.6362938284873962, Final Batch Loss: 0.2503122091293335\n",
      "Epoch 3035, Loss: 1.826377421617508, Final Batch Loss: 0.36912018060684204\n",
      "Epoch 3036, Loss: 1.8038441240787506, Final Batch Loss: 0.36586567759513855\n",
      "Epoch 3037, Loss: 1.732884794473648, Final Batch Loss: 0.4180043041706085\n",
      "Epoch 3038, Loss: 1.7069316506385803, Final Batch Loss: 0.276368647813797\n",
      "Epoch 3039, Loss: 1.6981914341449738, Final Batch Loss: 0.29897770285606384\n",
      "Epoch 3040, Loss: 1.8505840301513672, Final Batch Loss: 0.40291455388069153\n",
      "Epoch 3041, Loss: 1.7062106728553772, Final Batch Loss: 0.35481780767440796\n",
      "Epoch 3042, Loss: 1.6942044496536255, Final Batch Loss: 0.31791234016418457\n",
      "Epoch 3043, Loss: 1.7990662455558777, Final Batch Loss: 0.4625200033187866\n",
      "Epoch 3044, Loss: 1.8115670680999756, Final Batch Loss: 0.3337363600730896\n",
      "Epoch 3045, Loss: 1.8348891735076904, Final Batch Loss: 0.3058229386806488\n",
      "Epoch 3046, Loss: 1.7587556838989258, Final Batch Loss: 0.28409722447395325\n",
      "Epoch 3047, Loss: 1.7460018396377563, Final Batch Loss: 0.3289661407470703\n",
      "Epoch 3048, Loss: 1.774692416191101, Final Batch Loss: 0.3577370345592499\n",
      "Epoch 3049, Loss: 1.7324596643447876, Final Batch Loss: 0.2729371190071106\n",
      "Epoch 3050, Loss: 1.7119150459766388, Final Batch Loss: 0.34649255871772766\n",
      "Epoch 3051, Loss: 1.8107292652130127, Final Batch Loss: 0.35963019728660583\n",
      "Epoch 3052, Loss: 1.6969883441925049, Final Batch Loss: 0.37211933732032776\n",
      "Epoch 3053, Loss: 1.7152643501758575, Final Batch Loss: 0.30673274397850037\n",
      "Epoch 3054, Loss: 1.8136969804763794, Final Batch Loss: 0.3656890094280243\n",
      "Epoch 3055, Loss: 1.8308354914188385, Final Batch Loss: 0.3506607711315155\n",
      "Epoch 3056, Loss: 1.8411241173744202, Final Batch Loss: 0.3034970462322235\n",
      "Epoch 3057, Loss: 1.647237092256546, Final Batch Loss: 0.33000701665878296\n",
      "Epoch 3058, Loss: 1.8181311786174774, Final Batch Loss: 0.3463609218597412\n",
      "Epoch 3059, Loss: 1.8429477214813232, Final Batch Loss: 0.41580596566200256\n",
      "Epoch 3060, Loss: 1.8169145584106445, Final Batch Loss: 0.4467123746871948\n",
      "Epoch 3061, Loss: 1.794800043106079, Final Batch Loss: 0.40559646487236023\n",
      "Epoch 3062, Loss: 1.778259426355362, Final Batch Loss: 0.3530501425266266\n",
      "Epoch 3063, Loss: 1.802659422159195, Final Batch Loss: 0.37228158116340637\n",
      "Epoch 3064, Loss: 1.6673713028430939, Final Batch Loss: 0.258096307516098\n",
      "Epoch 3065, Loss: 1.6980337500572205, Final Batch Loss: 0.2862337827682495\n",
      "Epoch 3066, Loss: 1.6157239079475403, Final Batch Loss: 0.29677367210388184\n",
      "Epoch 3067, Loss: 1.8926951885223389, Final Batch Loss: 0.4266068637371063\n",
      "Epoch 3068, Loss: 1.7752272188663483, Final Batch Loss: 0.2568654417991638\n",
      "Epoch 3069, Loss: 1.795051246881485, Final Batch Loss: 0.3926008641719818\n",
      "Epoch 3070, Loss: 1.8173981308937073, Final Batch Loss: 0.34115347266197205\n",
      "Epoch 3071, Loss: 1.752103716135025, Final Batch Loss: 0.3558461368083954\n",
      "Epoch 3072, Loss: 1.6972457468509674, Final Batch Loss: 0.3145385682582855\n",
      "Epoch 3073, Loss: 1.770964652299881, Final Batch Loss: 0.43113282322883606\n",
      "Epoch 3074, Loss: 1.9156053960323334, Final Batch Loss: 0.3749719560146332\n",
      "Epoch 3075, Loss: 1.7228101789951324, Final Batch Loss: 0.2507636845111847\n",
      "Epoch 3076, Loss: 1.8664580881595612, Final Batch Loss: 0.3672231137752533\n",
      "Epoch 3077, Loss: 1.786282628774643, Final Batch Loss: 0.30303633213043213\n",
      "Epoch 3078, Loss: 1.996289610862732, Final Batch Loss: 0.3947322964668274\n",
      "Epoch 3079, Loss: 1.7191708981990814, Final Batch Loss: 0.31149035692214966\n",
      "Epoch 3080, Loss: 1.854076623916626, Final Batch Loss: 0.34775108098983765\n",
      "Epoch 3081, Loss: 1.7963863909244537, Final Batch Loss: 0.2931986451148987\n",
      "Epoch 3082, Loss: 1.774576723575592, Final Batch Loss: 0.35016459226608276\n",
      "Epoch 3083, Loss: 1.7615019083023071, Final Batch Loss: 0.2949528694152832\n",
      "Epoch 3084, Loss: 1.8010611832141876, Final Batch Loss: 0.3301948308944702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3085, Loss: 1.8092223703861237, Final Batch Loss: 0.2766498029232025\n",
      "Epoch 3086, Loss: 1.945445030927658, Final Batch Loss: 0.3855271637439728\n",
      "Epoch 3087, Loss: 1.8283304274082184, Final Batch Loss: 0.395987331867218\n",
      "Epoch 3088, Loss: 1.8071104288101196, Final Batch Loss: 0.4650859832763672\n",
      "Epoch 3089, Loss: 1.7041714191436768, Final Batch Loss: 0.32418352365493774\n",
      "Epoch 3090, Loss: 1.8923267424106598, Final Batch Loss: 0.4329880177974701\n",
      "Epoch 3091, Loss: 1.6638755798339844, Final Batch Loss: 0.32612326741218567\n",
      "Epoch 3092, Loss: 1.9149151146411896, Final Batch Loss: 0.36055466532707214\n",
      "Epoch 3093, Loss: 1.6837786138057709, Final Batch Loss: 0.3390626013278961\n",
      "Epoch 3094, Loss: 1.7571033239364624, Final Batch Loss: 0.24570289254188538\n",
      "Epoch 3095, Loss: 1.7411967813968658, Final Batch Loss: 0.3074231743812561\n",
      "Epoch 3096, Loss: 1.8856978714466095, Final Batch Loss: 0.4805588722229004\n",
      "Epoch 3097, Loss: 1.8502003252506256, Final Batch Loss: 0.38718700408935547\n",
      "Epoch 3098, Loss: 1.8348602652549744, Final Batch Loss: 0.38878461718559265\n",
      "Epoch 3099, Loss: 1.7925621271133423, Final Batch Loss: 0.32622644305229187\n",
      "Epoch 3100, Loss: 1.9768234193325043, Final Batch Loss: 0.4642062485218048\n",
      "Epoch 3101, Loss: 1.8409284949302673, Final Batch Loss: 0.2678960859775543\n",
      "Epoch 3102, Loss: 1.898861825466156, Final Batch Loss: 0.394565612077713\n",
      "Epoch 3103, Loss: 1.829090192914009, Final Batch Loss: 0.44506463408470154\n",
      "Epoch 3104, Loss: 1.8867971897125244, Final Batch Loss: 0.3841567039489746\n",
      "Epoch 3105, Loss: 1.8426827490329742, Final Batch Loss: 0.36697521805763245\n",
      "Epoch 3106, Loss: 1.8533300757408142, Final Batch Loss: 0.44596847891807556\n",
      "Epoch 3107, Loss: 1.8126928508281708, Final Batch Loss: 0.360160768032074\n",
      "Epoch 3108, Loss: 1.7401015758514404, Final Batch Loss: 0.354899138212204\n",
      "Epoch 3109, Loss: 1.657194048166275, Final Batch Loss: 0.3040708899497986\n",
      "Epoch 3110, Loss: 1.7001465260982513, Final Batch Loss: 0.3251868486404419\n",
      "Epoch 3111, Loss: 1.7089033126831055, Final Batch Loss: 0.3174099624156952\n",
      "Epoch 3112, Loss: 1.7198777198791504, Final Batch Loss: 0.27753886580467224\n",
      "Epoch 3113, Loss: 1.6752445697784424, Final Batch Loss: 0.2575914263725281\n",
      "Epoch 3114, Loss: 1.683659553527832, Final Batch Loss: 0.3242601156234741\n",
      "Epoch 3115, Loss: 1.8549799025058746, Final Batch Loss: 0.3265705108642578\n",
      "Epoch 3116, Loss: 1.7090541422367096, Final Batch Loss: 0.32994067668914795\n",
      "Epoch 3117, Loss: 1.7778772413730621, Final Batch Loss: 0.39830684661865234\n",
      "Epoch 3118, Loss: 1.8441439270973206, Final Batch Loss: 0.42239782214164734\n",
      "Epoch 3119, Loss: 1.6644510924816132, Final Batch Loss: 0.33789345622062683\n",
      "Epoch 3120, Loss: 1.680263727903366, Final Batch Loss: 0.30723685026168823\n",
      "Epoch 3121, Loss: 1.8082911968231201, Final Batch Loss: 0.3836444914340973\n",
      "Epoch 3122, Loss: 1.7921198308467865, Final Batch Loss: 0.28928297758102417\n",
      "Epoch 3123, Loss: 1.8169444501399994, Final Batch Loss: 0.3921636939048767\n",
      "Epoch 3124, Loss: 1.9122637212276459, Final Batch Loss: 0.382344514131546\n",
      "Epoch 3125, Loss: 1.7517189383506775, Final Batch Loss: 0.37046945095062256\n",
      "Epoch 3126, Loss: 1.5966130495071411, Final Batch Loss: 0.2793261408805847\n",
      "Epoch 3127, Loss: 1.619727373123169, Final Batch Loss: 0.2854641377925873\n",
      "Epoch 3128, Loss: 1.782621055841446, Final Batch Loss: 0.2682608366012573\n",
      "Epoch 3129, Loss: 1.8003756701946259, Final Batch Loss: 0.36703139543533325\n",
      "Epoch 3130, Loss: 1.784585416316986, Final Batch Loss: 0.48679453134536743\n",
      "Epoch 3131, Loss: 1.6981011927127838, Final Batch Loss: 0.3496045172214508\n",
      "Epoch 3132, Loss: 1.6402297765016556, Final Batch Loss: 0.361087828874588\n",
      "Epoch 3133, Loss: 1.673217535018921, Final Batch Loss: 0.317851722240448\n",
      "Epoch 3134, Loss: 1.6946547329425812, Final Batch Loss: 0.3862231969833374\n",
      "Epoch 3135, Loss: 1.80327570438385, Final Batch Loss: 0.41905054450035095\n",
      "Epoch 3136, Loss: 1.7634074985980988, Final Batch Loss: 0.3444017469882965\n",
      "Epoch 3137, Loss: 1.7982352375984192, Final Batch Loss: 0.30433329939842224\n",
      "Epoch 3138, Loss: 1.7462442815303802, Final Batch Loss: 0.35963281989097595\n",
      "Epoch 3139, Loss: 1.8318202495574951, Final Batch Loss: 0.38113299012184143\n",
      "Epoch 3140, Loss: 1.7765313684940338, Final Batch Loss: 0.3002767562866211\n",
      "Epoch 3141, Loss: 1.5954331457614899, Final Batch Loss: 0.2902863621711731\n",
      "Epoch 3142, Loss: 1.8760946989059448, Final Batch Loss: 0.4183708727359772\n",
      "Epoch 3143, Loss: 1.7244222462177277, Final Batch Loss: 0.290579617023468\n",
      "Epoch 3144, Loss: 1.8046196401119232, Final Batch Loss: 0.4308435320854187\n",
      "Epoch 3145, Loss: 1.7262994349002838, Final Batch Loss: 0.2817099392414093\n",
      "Epoch 3146, Loss: 1.8995847404003143, Final Batch Loss: 0.33001261949539185\n",
      "Epoch 3147, Loss: 1.8915071487426758, Final Batch Loss: 0.37231674790382385\n",
      "Epoch 3148, Loss: 1.832165241241455, Final Batch Loss: 0.37567758560180664\n",
      "Epoch 3149, Loss: 1.8163257837295532, Final Batch Loss: 0.4293910562992096\n",
      "Epoch 3150, Loss: 1.8101993799209595, Final Batch Loss: 0.3507425785064697\n",
      "Epoch 3151, Loss: 1.8356651663780212, Final Batch Loss: 0.3963603675365448\n",
      "Epoch 3152, Loss: 1.6852469444274902, Final Batch Loss: 0.3527388274669647\n",
      "Epoch 3153, Loss: 1.8640358746051788, Final Batch Loss: 0.3765023350715637\n",
      "Epoch 3154, Loss: 1.6921375691890717, Final Batch Loss: 0.3248804807662964\n",
      "Epoch 3155, Loss: 1.7249376773834229, Final Batch Loss: 0.3365212678909302\n",
      "Epoch 3156, Loss: 1.7701871395111084, Final Batch Loss: 0.3592545986175537\n",
      "Epoch 3157, Loss: 1.7480111122131348, Final Batch Loss: 0.3186565935611725\n",
      "Epoch 3158, Loss: 1.8811701834201813, Final Batch Loss: 0.44168269634246826\n",
      "Epoch 3159, Loss: 1.8201650381088257, Final Batch Loss: 0.399941086769104\n",
      "Epoch 3160, Loss: 1.8011939823627472, Final Batch Loss: 0.42190128564834595\n",
      "Epoch 3161, Loss: 1.944909244775772, Final Batch Loss: 0.4980258345603943\n",
      "Epoch 3162, Loss: 1.6603768467903137, Final Batch Loss: 0.29057615995407104\n",
      "Epoch 3163, Loss: 1.832298070192337, Final Batch Loss: 0.38666701316833496\n",
      "Epoch 3164, Loss: 1.815324753522873, Final Batch Loss: 0.30920979380607605\n",
      "Epoch 3165, Loss: 1.6913027465343475, Final Batch Loss: 0.3313741087913513\n",
      "Epoch 3166, Loss: 1.6076893210411072, Final Batch Loss: 0.3162154257297516\n",
      "Epoch 3167, Loss: 1.6086843311786652, Final Batch Loss: 0.2672778069972992\n",
      "Epoch 3168, Loss: 1.7328863441944122, Final Batch Loss: 0.35504135489463806\n",
      "Epoch 3169, Loss: 1.6207447350025177, Final Batch Loss: 0.3283376395702362\n",
      "Epoch 3170, Loss: 1.8199543952941895, Final Batch Loss: 0.36851513385772705\n",
      "Epoch 3171, Loss: 1.6868893951177597, Final Batch Loss: 0.24769823253154755\n",
      "Epoch 3172, Loss: 1.7102924585342407, Final Batch Loss: 0.31846141815185547\n",
      "Epoch 3173, Loss: 1.775545358657837, Final Batch Loss: 0.38529568910598755\n",
      "Epoch 3174, Loss: 1.6918504536151886, Final Batch Loss: 0.32446345686912537\n",
      "Epoch 3175, Loss: 1.6804039478302002, Final Batch Loss: 0.3636995851993561\n",
      "Epoch 3176, Loss: 1.8209432661533356, Final Batch Loss: 0.38559284806251526\n",
      "Epoch 3177, Loss: 1.6378271877765656, Final Batch Loss: 0.2967332601547241\n",
      "Epoch 3178, Loss: 1.8667108714580536, Final Batch Loss: 0.30156442523002625\n",
      "Epoch 3179, Loss: 1.615658164024353, Final Batch Loss: 0.35805660486221313\n",
      "Epoch 3180, Loss: 1.7713104784488678, Final Batch Loss: 0.43913644552230835\n",
      "Epoch 3181, Loss: 1.5992697179317474, Final Batch Loss: 0.2629719078540802\n",
      "Epoch 3182, Loss: 1.882151871919632, Final Batch Loss: 0.30079731345176697\n",
      "Epoch 3183, Loss: 1.7571057081222534, Final Batch Loss: 0.30380454659461975\n",
      "Epoch 3184, Loss: 1.6275766491889954, Final Batch Loss: 0.3270399570465088\n",
      "Epoch 3185, Loss: 1.7164361774921417, Final Batch Loss: 0.37384945154190063\n",
      "Epoch 3186, Loss: 1.634434849023819, Final Batch Loss: 0.2775954604148865\n",
      "Epoch 3187, Loss: 1.8466679751873016, Final Batch Loss: 0.4350837469100952\n",
      "Epoch 3188, Loss: 1.8292036950588226, Final Batch Loss: 0.3970641791820526\n",
      "Epoch 3189, Loss: 1.6646924018859863, Final Batch Loss: 0.36192914843559265\n",
      "Epoch 3190, Loss: 1.8380384147167206, Final Batch Loss: 0.4757350981235504\n",
      "Epoch 3191, Loss: 1.8943696022033691, Final Batch Loss: 0.37568485736846924\n",
      "Epoch 3192, Loss: 1.7147609293460846, Final Batch Loss: 0.4452233612537384\n",
      "Epoch 3193, Loss: 1.6747799217700958, Final Batch Loss: 0.32094699144363403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3194, Loss: 1.9359756410121918, Final Batch Loss: 0.3897136151790619\n",
      "Epoch 3195, Loss: 1.6752776503562927, Final Batch Loss: 0.2953375577926636\n",
      "Epoch 3196, Loss: 1.7372260391712189, Final Batch Loss: 0.3544325828552246\n",
      "Epoch 3197, Loss: 1.689888834953308, Final Batch Loss: 0.33508536219596863\n",
      "Epoch 3198, Loss: 1.6056427657604218, Final Batch Loss: 0.3197827935218811\n",
      "Epoch 3199, Loss: 1.7071026861667633, Final Batch Loss: 0.34042850136756897\n",
      "Epoch 3200, Loss: 1.6527771651744843, Final Batch Loss: 0.18982630968093872\n",
      "Epoch 3201, Loss: 1.7804910838603973, Final Batch Loss: 0.3969881534576416\n",
      "Epoch 3202, Loss: 1.679111123085022, Final Batch Loss: 0.355839341878891\n",
      "Epoch 3203, Loss: 1.7588126063346863, Final Batch Loss: 0.29465726017951965\n",
      "Epoch 3204, Loss: 1.8208002150058746, Final Batch Loss: 0.3780581057071686\n",
      "Epoch 3205, Loss: 1.617369756102562, Final Batch Loss: 0.220518097281456\n",
      "Epoch 3206, Loss: 1.8894569277763367, Final Batch Loss: 0.40374985337257385\n",
      "Epoch 3207, Loss: 1.6775038242340088, Final Batch Loss: 0.36385321617126465\n",
      "Epoch 3208, Loss: 1.7710350155830383, Final Batch Loss: 0.356277197599411\n",
      "Epoch 3209, Loss: 1.7086160480976105, Final Batch Loss: 0.36918261647224426\n",
      "Epoch 3210, Loss: 1.7151180803775787, Final Batch Loss: 0.3786460757255554\n",
      "Epoch 3211, Loss: 1.7302277088165283, Final Batch Loss: 0.3670671880245209\n",
      "Epoch 3212, Loss: 1.6757762432098389, Final Batch Loss: 0.3541397452354431\n",
      "Epoch 3213, Loss: 1.6458332240581512, Final Batch Loss: 0.31159862875938416\n",
      "Epoch 3214, Loss: 1.7623254358768463, Final Batch Loss: 0.395338237285614\n",
      "Epoch 3215, Loss: 1.7485122382640839, Final Batch Loss: 0.37189221382141113\n",
      "Epoch 3216, Loss: 1.8231584131717682, Final Batch Loss: 0.4250220060348511\n",
      "Epoch 3217, Loss: 1.713105469942093, Final Batch Loss: 0.3099929392337799\n",
      "Epoch 3218, Loss: 1.577749252319336, Final Batch Loss: 0.26790735125541687\n",
      "Epoch 3219, Loss: 1.7595767378807068, Final Batch Loss: 0.2675488293170929\n",
      "Epoch 3220, Loss: 1.7605423033237457, Final Batch Loss: 0.3653596043586731\n",
      "Epoch 3221, Loss: 1.6852316558361053, Final Batch Loss: 0.35792702436447144\n",
      "Epoch 3222, Loss: 1.8457163274288177, Final Batch Loss: 0.3837876617908478\n",
      "Epoch 3223, Loss: 1.6110007166862488, Final Batch Loss: 0.3088188171386719\n",
      "Epoch 3224, Loss: 1.6600681394338608, Final Batch Loss: 0.2428203970193863\n",
      "Epoch 3225, Loss: 1.600835382938385, Final Batch Loss: 0.3692432641983032\n",
      "Epoch 3226, Loss: 1.6738097071647644, Final Batch Loss: 0.2936636805534363\n",
      "Epoch 3227, Loss: 1.7603794038295746, Final Batch Loss: 0.33375468850135803\n",
      "Epoch 3228, Loss: 1.7173862755298615, Final Batch Loss: 0.2818087339401245\n",
      "Epoch 3229, Loss: 1.7208268642425537, Final Batch Loss: 0.3305756449699402\n",
      "Epoch 3230, Loss: 1.6877959668636322, Final Batch Loss: 0.2840656340122223\n",
      "Epoch 3231, Loss: 1.7012110650539398, Final Batch Loss: 0.34524548053741455\n",
      "Epoch 3232, Loss: 1.8548004031181335, Final Batch Loss: 0.3835129737854004\n",
      "Epoch 3233, Loss: 1.775890201330185, Final Batch Loss: 0.3974229395389557\n",
      "Epoch 3234, Loss: 1.6918962895870209, Final Batch Loss: 0.29507750272750854\n",
      "Epoch 3235, Loss: 1.8689770996570587, Final Batch Loss: 0.33095479011535645\n",
      "Epoch 3236, Loss: 1.6749961972236633, Final Batch Loss: 0.36150190234184265\n",
      "Epoch 3237, Loss: 1.7631031274795532, Final Batch Loss: 0.3424156904220581\n",
      "Epoch 3238, Loss: 1.7016069293022156, Final Batch Loss: 0.37592625617980957\n",
      "Epoch 3239, Loss: 1.83885857462883, Final Batch Loss: 0.3506825268268585\n",
      "Epoch 3240, Loss: 1.7106881439685822, Final Batch Loss: 0.3769066333770752\n",
      "Epoch 3241, Loss: 1.7290212213993073, Final Batch Loss: 0.3763354420661926\n",
      "Epoch 3242, Loss: 1.714580088853836, Final Batch Loss: 0.4684380888938904\n",
      "Epoch 3243, Loss: 1.6958957612514496, Final Batch Loss: 0.29574790596961975\n",
      "Epoch 3244, Loss: 1.7124190926551819, Final Batch Loss: 0.3492780327796936\n",
      "Epoch 3245, Loss: 1.6797175705432892, Final Batch Loss: 0.2912271022796631\n",
      "Epoch 3246, Loss: 1.7583828568458557, Final Batch Loss: 0.3612802028656006\n",
      "Epoch 3247, Loss: 1.7138514518737793, Final Batch Loss: 0.35356029868125916\n",
      "Epoch 3248, Loss: 1.747429996728897, Final Batch Loss: 0.339680939912796\n",
      "Epoch 3249, Loss: 1.8412390351295471, Final Batch Loss: 0.41985204815864563\n",
      "Epoch 3250, Loss: 1.713798612356186, Final Batch Loss: 0.3969401717185974\n",
      "Epoch 3251, Loss: 1.6998545825481415, Final Batch Loss: 0.29157909750938416\n",
      "Epoch 3252, Loss: 1.6978875696659088, Final Batch Loss: 0.32567039132118225\n",
      "Epoch 3253, Loss: 1.7139028012752533, Final Batch Loss: 0.3010269105434418\n",
      "Epoch 3254, Loss: 1.6825624406337738, Final Batch Loss: 0.3948904573917389\n",
      "Epoch 3255, Loss: 1.709037333726883, Final Batch Loss: 0.3022761344909668\n",
      "Epoch 3256, Loss: 1.9309203922748566, Final Batch Loss: 0.46131211519241333\n",
      "Epoch 3257, Loss: 1.7847057282924652, Final Batch Loss: 0.4253389239311218\n",
      "Epoch 3258, Loss: 1.7459028661251068, Final Batch Loss: 0.30420607328414917\n",
      "Epoch 3259, Loss: 1.7378197312355042, Final Batch Loss: 0.37200304865837097\n",
      "Epoch 3260, Loss: 1.670269399881363, Final Batch Loss: 0.29347699880599976\n",
      "Epoch 3261, Loss: 1.758553385734558, Final Batch Loss: 0.27509644627571106\n",
      "Epoch 3262, Loss: 1.8155818581581116, Final Batch Loss: 0.3603476285934448\n",
      "Epoch 3263, Loss: 1.7528263628482819, Final Batch Loss: 0.2855830490589142\n",
      "Epoch 3264, Loss: 1.7208259403705597, Final Batch Loss: 0.3055865466594696\n",
      "Epoch 3265, Loss: 1.744998186826706, Final Batch Loss: 0.37546584010124207\n",
      "Epoch 3266, Loss: 1.9026886820793152, Final Batch Loss: 0.3504781126976013\n",
      "Epoch 3267, Loss: 1.7790791392326355, Final Batch Loss: 0.377949059009552\n",
      "Epoch 3268, Loss: 1.8191542029380798, Final Batch Loss: 0.3845737874507904\n",
      "Epoch 3269, Loss: 1.7357982993125916, Final Batch Loss: 0.25227972865104675\n",
      "Epoch 3270, Loss: 1.7262875735759735, Final Batch Loss: 0.384630411863327\n",
      "Epoch 3271, Loss: 1.8844150006771088, Final Batch Loss: 0.35222816467285156\n",
      "Epoch 3272, Loss: 1.764136403799057, Final Batch Loss: 0.3925654888153076\n",
      "Epoch 3273, Loss: 1.7124707996845245, Final Batch Loss: 0.3533611297607422\n",
      "Epoch 3274, Loss: 1.763090044260025, Final Batch Loss: 0.4112448990345001\n",
      "Epoch 3275, Loss: 1.772873729467392, Final Batch Loss: 0.3268272578716278\n",
      "Epoch 3276, Loss: 1.7822459638118744, Final Batch Loss: 0.3036114573478699\n",
      "Epoch 3277, Loss: 1.734813004732132, Final Batch Loss: 0.2651049792766571\n",
      "Epoch 3278, Loss: 1.7160101234912872, Final Batch Loss: 0.2539214789867401\n",
      "Epoch 3279, Loss: 1.6981319785118103, Final Batch Loss: 0.3136378228664398\n",
      "Epoch 3280, Loss: 1.6640314012765884, Final Batch Loss: 0.2105775624513626\n",
      "Epoch 3281, Loss: 1.7953741550445557, Final Batch Loss: 0.3777344524860382\n",
      "Epoch 3282, Loss: 1.6616921424865723, Final Batch Loss: 0.3827663064002991\n",
      "Epoch 3283, Loss: 1.677226036787033, Final Batch Loss: 0.38123175501823425\n",
      "Epoch 3284, Loss: 1.8751420378684998, Final Batch Loss: 0.3963860869407654\n",
      "Epoch 3285, Loss: 1.6186361014842987, Final Batch Loss: 0.2747471034526825\n",
      "Epoch 3286, Loss: 1.7897289991378784, Final Batch Loss: 0.28876206278800964\n",
      "Epoch 3287, Loss: 1.6723060011863708, Final Batch Loss: 0.279844731092453\n",
      "Epoch 3288, Loss: 1.6917183995246887, Final Batch Loss: 0.402195543050766\n",
      "Epoch 3289, Loss: 1.6789976358413696, Final Batch Loss: 0.3541342616081238\n",
      "Epoch 3290, Loss: 1.801227182149887, Final Batch Loss: 0.3573469817638397\n",
      "Epoch 3291, Loss: 1.7991551458835602, Final Batch Loss: 0.4254572093486786\n",
      "Epoch 3292, Loss: 1.7506792843341827, Final Batch Loss: 0.3574999272823334\n",
      "Epoch 3293, Loss: 1.7092229127883911, Final Batch Loss: 0.2698955535888672\n",
      "Epoch 3294, Loss: 1.8182317018508911, Final Batch Loss: 0.44014620780944824\n",
      "Epoch 3295, Loss: 1.698200911283493, Final Batch Loss: 0.4551462233066559\n",
      "Epoch 3296, Loss: 1.6954689025878906, Final Batch Loss: 0.37849029898643494\n",
      "Epoch 3297, Loss: 1.719080775976181, Final Batch Loss: 0.3245018422603607\n",
      "Epoch 3298, Loss: 1.856037363409996, Final Batch Loss: 0.4345262944698334\n",
      "Epoch 3299, Loss: 1.7628556936979294, Final Batch Loss: 0.23746256530284882\n",
      "Epoch 3300, Loss: 1.634609967470169, Final Batch Loss: 0.2975385785102844\n",
      "Epoch 3301, Loss: 1.7418526411056519, Final Batch Loss: 0.38598939776420593\n",
      "Epoch 3302, Loss: 1.6033343374729156, Final Batch Loss: 0.3007183372974396\n",
      "Epoch 3303, Loss: 1.737850397825241, Final Batch Loss: 0.30173665285110474\n",
      "Epoch 3304, Loss: 1.800391137599945, Final Batch Loss: 0.3070451319217682\n",
      "Epoch 3305, Loss: 1.758764162659645, Final Batch Loss: 0.22970329225063324\n",
      "Epoch 3306, Loss: 1.7180837392807007, Final Batch Loss: 0.33054330945014954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3307, Loss: 1.7144098281860352, Final Batch Loss: 0.3515118658542633\n",
      "Epoch 3308, Loss: 1.90824294090271, Final Batch Loss: 0.5176568627357483\n",
      "Epoch 3309, Loss: 1.7097582519054413, Final Batch Loss: 0.3455108404159546\n",
      "Epoch 3310, Loss: 1.7143060863018036, Final Batch Loss: 0.34498023986816406\n",
      "Epoch 3311, Loss: 1.8501959145069122, Final Batch Loss: 0.4563188850879669\n",
      "Epoch 3312, Loss: 1.592365950345993, Final Batch Loss: 0.3846476376056671\n",
      "Epoch 3313, Loss: 1.7471624612808228, Final Batch Loss: 0.25372421741485596\n",
      "Epoch 3314, Loss: 1.7976059019565582, Final Batch Loss: 0.4187335968017578\n",
      "Epoch 3315, Loss: 1.5968177616596222, Final Batch Loss: 0.3040493428707123\n",
      "Epoch 3316, Loss: 1.580128252506256, Final Batch Loss: 0.2962084114551544\n",
      "Epoch 3317, Loss: 1.844907432794571, Final Batch Loss: 0.32965028285980225\n",
      "Epoch 3318, Loss: 1.7334744930267334, Final Batch Loss: 0.3302430808544159\n",
      "Epoch 3319, Loss: 1.9498476088047028, Final Batch Loss: 0.40219244360923767\n",
      "Epoch 3320, Loss: 1.5707466155290604, Final Batch Loss: 0.2247764617204666\n",
      "Epoch 3321, Loss: 1.636099398136139, Final Batch Loss: 0.322142094373703\n",
      "Epoch 3322, Loss: 1.6184015274047852, Final Batch Loss: 0.25099387764930725\n",
      "Epoch 3323, Loss: 1.6267605125904083, Final Batch Loss: 0.30536949634552\n",
      "Epoch 3324, Loss: 1.6996907889842987, Final Batch Loss: 0.3093014359474182\n",
      "Epoch 3325, Loss: 1.6808813214302063, Final Batch Loss: 0.37688368558883667\n",
      "Epoch 3326, Loss: 1.7159318029880524, Final Batch Loss: 0.279681921005249\n",
      "Epoch 3327, Loss: 1.7253028452396393, Final Batch Loss: 0.323676735162735\n",
      "Epoch 3328, Loss: 1.8815002739429474, Final Batch Loss: 0.4848795533180237\n",
      "Epoch 3329, Loss: 1.7067604064941406, Final Batch Loss: 0.2864212393760681\n",
      "Epoch 3330, Loss: 1.6567104160785675, Final Batch Loss: 0.28196412324905396\n",
      "Epoch 3331, Loss: 1.7850120961666107, Final Batch Loss: 0.2909279465675354\n",
      "Epoch 3332, Loss: 1.7373228669166565, Final Batch Loss: 0.25853490829467773\n",
      "Epoch 3333, Loss: 1.6819396615028381, Final Batch Loss: 0.35878226161003113\n",
      "Epoch 3334, Loss: 1.796297311782837, Final Batch Loss: 0.3582717478275299\n",
      "Epoch 3335, Loss: 1.568066269159317, Final Batch Loss: 0.2884814143180847\n",
      "Epoch 3336, Loss: 1.837561011314392, Final Batch Loss: 0.45792677998542786\n",
      "Epoch 3337, Loss: 1.7735161483287811, Final Batch Loss: 0.4207942485809326\n",
      "Epoch 3338, Loss: 1.6687528491020203, Final Batch Loss: 0.35722723603248596\n",
      "Epoch 3339, Loss: 1.8050325810909271, Final Batch Loss: 0.4795093834400177\n",
      "Epoch 3340, Loss: 1.6713488399982452, Final Batch Loss: 0.2973749041557312\n",
      "Epoch 3341, Loss: 1.731687992811203, Final Batch Loss: 0.37499678134918213\n",
      "Epoch 3342, Loss: 1.784453123807907, Final Batch Loss: 0.4189321994781494\n",
      "Epoch 3343, Loss: 1.8642302751541138, Final Batch Loss: 0.31039682030677795\n",
      "Epoch 3344, Loss: 1.6712965071201324, Final Batch Loss: 0.2788798213005066\n",
      "Epoch 3345, Loss: 1.794738084077835, Final Batch Loss: 0.3477790057659149\n",
      "Epoch 3346, Loss: 1.7526247799396515, Final Batch Loss: 0.4134398400783539\n",
      "Epoch 3347, Loss: 1.8063555657863617, Final Batch Loss: 0.30512991547584534\n",
      "Epoch 3348, Loss: 1.7689448595046997, Final Batch Loss: 0.41548025608062744\n",
      "Epoch 3349, Loss: 1.927783727645874, Final Batch Loss: 0.3381224274635315\n",
      "Epoch 3350, Loss: 1.7034921646118164, Final Batch Loss: 0.31875401735305786\n",
      "Epoch 3351, Loss: 1.6731052696704865, Final Batch Loss: 0.3037659525871277\n",
      "Epoch 3352, Loss: 1.7912635505199432, Final Batch Loss: 0.32377609610557556\n",
      "Epoch 3353, Loss: 1.7678373754024506, Final Batch Loss: 0.2794322073459625\n",
      "Epoch 3354, Loss: 1.889902800321579, Final Batch Loss: 0.49048352241516113\n",
      "Epoch 3355, Loss: 1.6551703363656998, Final Batch Loss: 0.2140505164861679\n",
      "Epoch 3356, Loss: 1.7622966170310974, Final Batch Loss: 0.31992411613464355\n",
      "Epoch 3357, Loss: 1.6477414667606354, Final Batch Loss: 0.29859092831611633\n",
      "Epoch 3358, Loss: 1.7476953566074371, Final Batch Loss: 0.3110477328300476\n",
      "Epoch 3359, Loss: 1.6001120805740356, Final Batch Loss: 0.3069213330745697\n",
      "Epoch 3360, Loss: 1.7802890241146088, Final Batch Loss: 0.4398779571056366\n",
      "Epoch 3361, Loss: 1.7806098461151123, Final Batch Loss: 0.36177387833595276\n",
      "Epoch 3362, Loss: 1.7256634533405304, Final Batch Loss: 0.28402456641197205\n",
      "Epoch 3363, Loss: 1.7070807218551636, Final Batch Loss: 0.342053085565567\n",
      "Epoch 3364, Loss: 1.6891613602638245, Final Batch Loss: 0.31504347920417786\n",
      "Epoch 3365, Loss: 1.7340736091136932, Final Batch Loss: 0.35381096601486206\n",
      "Epoch 3366, Loss: 1.6493493914604187, Final Batch Loss: 0.30796366930007935\n",
      "Epoch 3367, Loss: 1.7070027887821198, Final Batch Loss: 0.33574527502059937\n",
      "Epoch 3368, Loss: 1.6636928915977478, Final Batch Loss: 0.331089049577713\n",
      "Epoch 3369, Loss: 1.6570966392755508, Final Batch Loss: 0.23032857477664948\n",
      "Epoch 3370, Loss: 1.673407405614853, Final Batch Loss: 0.30952247977256775\n",
      "Epoch 3371, Loss: 1.6638697981834412, Final Batch Loss: 0.29665660858154297\n",
      "Epoch 3372, Loss: 1.6258852779865265, Final Batch Loss: 0.29785722494125366\n",
      "Epoch 3373, Loss: 1.7465028762817383, Final Batch Loss: 0.3187623918056488\n",
      "Epoch 3374, Loss: 1.6701984107494354, Final Batch Loss: 0.3590351343154907\n",
      "Epoch 3375, Loss: 1.7424739003181458, Final Batch Loss: 0.39329099655151367\n",
      "Epoch 3376, Loss: 1.738327443599701, Final Batch Loss: 0.31539449095726013\n",
      "Epoch 3377, Loss: 1.5696556866168976, Final Batch Loss: 0.33361944556236267\n",
      "Epoch 3378, Loss: 1.7291098833084106, Final Batch Loss: 0.3534209728240967\n",
      "Epoch 3379, Loss: 1.7490366697311401, Final Batch Loss: 0.30808225274086\n",
      "Epoch 3380, Loss: 1.854872077703476, Final Batch Loss: 0.33377018570899963\n",
      "Epoch 3381, Loss: 1.796322375535965, Final Batch Loss: 0.2970629036426544\n",
      "Epoch 3382, Loss: 1.7016532719135284, Final Batch Loss: 0.33622220158576965\n",
      "Epoch 3383, Loss: 1.6767596304416656, Final Batch Loss: 0.285822331905365\n",
      "Epoch 3384, Loss: 1.8457001745700836, Final Batch Loss: 0.4575432538986206\n",
      "Epoch 3385, Loss: 1.7517891228199005, Final Batch Loss: 0.3508523404598236\n",
      "Epoch 3386, Loss: 1.7461856007575989, Final Batch Loss: 0.31047323346138\n",
      "Epoch 3387, Loss: 1.6860594302415848, Final Batch Loss: 0.3825415372848511\n",
      "Epoch 3388, Loss: 1.574097916483879, Final Batch Loss: 0.23652775585651398\n",
      "Epoch 3389, Loss: 1.5914115607738495, Final Batch Loss: 0.3214268982410431\n",
      "Epoch 3390, Loss: 1.7687682807445526, Final Batch Loss: 0.43972980976104736\n",
      "Epoch 3391, Loss: 1.7354690730571747, Final Batch Loss: 0.32017460465431213\n",
      "Epoch 3392, Loss: 1.7316819876432419, Final Batch Loss: 0.4079819321632385\n",
      "Epoch 3393, Loss: 1.658529132604599, Final Batch Loss: 0.3020760416984558\n",
      "Epoch 3394, Loss: 1.6746777594089508, Final Batch Loss: 0.29554057121276855\n",
      "Epoch 3395, Loss: 1.85215625166893, Final Batch Loss: 0.3256082534790039\n",
      "Epoch 3396, Loss: 1.7882587015628815, Final Batch Loss: 0.4412820339202881\n",
      "Epoch 3397, Loss: 1.63399937748909, Final Batch Loss: 0.27161678671836853\n",
      "Epoch 3398, Loss: 1.6884801387786865, Final Batch Loss: 0.340206116437912\n",
      "Epoch 3399, Loss: 1.6716962456703186, Final Batch Loss: 0.33317747712135315\n",
      "Epoch 3400, Loss: 1.697618544101715, Final Batch Loss: 0.26245957612991333\n",
      "Epoch 3401, Loss: 1.8642663359642029, Final Batch Loss: 0.3437865078449249\n",
      "Epoch 3402, Loss: 1.7094145119190216, Final Batch Loss: 0.3441928029060364\n",
      "Epoch 3403, Loss: 1.6729058623313904, Final Batch Loss: 0.3131479322910309\n",
      "Epoch 3404, Loss: 1.5955840051174164, Final Batch Loss: 0.24581971764564514\n",
      "Epoch 3405, Loss: 1.6867161393165588, Final Batch Loss: 0.3461143374443054\n",
      "Epoch 3406, Loss: 1.5894314050674438, Final Batch Loss: 0.33363473415374756\n",
      "Epoch 3407, Loss: 1.5259884297847748, Final Batch Loss: 0.3129180073738098\n",
      "Epoch 3408, Loss: 1.7305798828601837, Final Batch Loss: 0.3958946764469147\n",
      "Epoch 3409, Loss: 1.623937338590622, Final Batch Loss: 0.25495246052742004\n",
      "Epoch 3410, Loss: 1.9546116292476654, Final Batch Loss: 0.5567963123321533\n",
      "Epoch 3411, Loss: 1.8317529559135437, Final Batch Loss: 0.4570590853691101\n",
      "Epoch 3412, Loss: 1.6314738094806671, Final Batch Loss: 0.3629315495491028\n",
      "Epoch 3413, Loss: 1.6941732466220856, Final Batch Loss: 0.33085834980010986\n",
      "Epoch 3414, Loss: 1.7176334857940674, Final Batch Loss: 0.36605870723724365\n",
      "Epoch 3415, Loss: 1.6916712820529938, Final Batch Loss: 0.4017934203147888\n",
      "Epoch 3416, Loss: 1.6682376265525818, Final Batch Loss: 0.20358088612556458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3417, Loss: 1.7383429408073425, Final Batch Loss: 0.2429129183292389\n",
      "Epoch 3418, Loss: 1.7056687474250793, Final Batch Loss: 0.34259045124053955\n",
      "Epoch 3419, Loss: 1.8627020418643951, Final Batch Loss: 0.45575571060180664\n",
      "Epoch 3420, Loss: 1.9667935073375702, Final Batch Loss: 0.280673623085022\n",
      "Epoch 3421, Loss: 1.696498692035675, Final Batch Loss: 0.2748139500617981\n",
      "Epoch 3422, Loss: 1.5752204358577728, Final Batch Loss: 0.34948810935020447\n",
      "Epoch 3423, Loss: 1.7623867988586426, Final Batch Loss: 0.3754240572452545\n",
      "Epoch 3424, Loss: 1.6434553861618042, Final Batch Loss: 0.2657123804092407\n",
      "Epoch 3425, Loss: 1.7107273638248444, Final Batch Loss: 0.3496990203857422\n",
      "Epoch 3426, Loss: 1.6948069334030151, Final Batch Loss: 0.38793277740478516\n",
      "Epoch 3427, Loss: 1.7043877243995667, Final Batch Loss: 0.3733902871608734\n",
      "Epoch 3428, Loss: 1.6713273227214813, Final Batch Loss: 0.2865926921367645\n",
      "Epoch 3429, Loss: 1.6281038373708725, Final Batch Loss: 0.3959789276123047\n",
      "Epoch 3430, Loss: 1.6479621529579163, Final Batch Loss: 0.32178500294685364\n",
      "Epoch 3431, Loss: 1.6270866394042969, Final Batch Loss: 0.29363423585891724\n",
      "Epoch 3432, Loss: 1.6655412912368774, Final Batch Loss: 0.2806636691093445\n",
      "Epoch 3433, Loss: 1.8111568689346313, Final Batch Loss: 0.36759960651397705\n",
      "Epoch 3434, Loss: 1.775644063949585, Final Batch Loss: 0.3388963043689728\n",
      "Epoch 3435, Loss: 1.752505898475647, Final Batch Loss: 0.42886218428611755\n",
      "Epoch 3436, Loss: 1.7974729239940643, Final Batch Loss: 0.4659762680530548\n",
      "Epoch 3437, Loss: 1.6434796154499054, Final Batch Loss: 0.3444305658340454\n",
      "Epoch 3438, Loss: 1.6290424168109894, Final Batch Loss: 0.34519875049591064\n",
      "Epoch 3439, Loss: 1.663799226284027, Final Batch Loss: 0.4299662709236145\n",
      "Epoch 3440, Loss: 1.6285556256771088, Final Batch Loss: 0.3153948485851288\n",
      "Epoch 3441, Loss: 1.711804449558258, Final Batch Loss: 0.40312105417251587\n",
      "Epoch 3442, Loss: 1.6967063546180725, Final Batch Loss: 0.3660716116428375\n",
      "Epoch 3443, Loss: 1.6691378057003021, Final Batch Loss: 0.26514869928359985\n",
      "Epoch 3444, Loss: 1.7439419329166412, Final Batch Loss: 0.39665326476097107\n",
      "Epoch 3445, Loss: 1.6411520540714264, Final Batch Loss: 0.2606966197490692\n",
      "Epoch 3446, Loss: 1.7529426515102386, Final Batch Loss: 0.46457773447036743\n",
      "Epoch 3447, Loss: 1.7218214869499207, Final Batch Loss: 0.3418460786342621\n",
      "Epoch 3448, Loss: 1.7748742699623108, Final Batch Loss: 0.3916082978248596\n",
      "Epoch 3449, Loss: 1.9613483548164368, Final Batch Loss: 0.4791460633277893\n",
      "Epoch 3450, Loss: 1.618153601884842, Final Batch Loss: 0.27986791729927063\n",
      "Epoch 3451, Loss: 1.7202723622322083, Final Batch Loss: 0.31979039311408997\n",
      "Epoch 3452, Loss: 1.5887710452079773, Final Batch Loss: 0.28432697057724\n",
      "Epoch 3453, Loss: 1.6797460913658142, Final Batch Loss: 0.3390745520591736\n",
      "Epoch 3454, Loss: 1.6908855140209198, Final Batch Loss: 0.3425940275192261\n",
      "Epoch 3455, Loss: 1.5740381628274918, Final Batch Loss: 0.24695326387882233\n",
      "Epoch 3456, Loss: 1.6527552157640457, Final Batch Loss: 0.19749753177165985\n",
      "Epoch 3457, Loss: 1.6385329961776733, Final Batch Loss: 0.2976961135864258\n",
      "Epoch 3458, Loss: 1.6643856465816498, Final Batch Loss: 0.3248387575149536\n",
      "Epoch 3459, Loss: 1.7331110835075378, Final Batch Loss: 0.3475114405155182\n",
      "Epoch 3460, Loss: 1.6613193452358246, Final Batch Loss: 0.29576364159584045\n",
      "Epoch 3461, Loss: 1.834282636642456, Final Batch Loss: 0.38003817200660706\n",
      "Epoch 3462, Loss: 1.7120719850063324, Final Batch Loss: 0.39144057035446167\n",
      "Epoch 3463, Loss: 1.6894142925739288, Final Batch Loss: 0.2554466724395752\n",
      "Epoch 3464, Loss: 1.7986224591732025, Final Batch Loss: 0.5238214135169983\n",
      "Epoch 3465, Loss: 1.6941224932670593, Final Batch Loss: 0.34837964177131653\n",
      "Epoch 3466, Loss: 1.764894276857376, Final Batch Loss: 0.42013657093048096\n",
      "Epoch 3467, Loss: 1.623995840549469, Final Batch Loss: 0.30153751373291016\n",
      "Epoch 3468, Loss: 1.8116533756256104, Final Batch Loss: 0.3836801052093506\n",
      "Epoch 3469, Loss: 1.6460364162921906, Final Batch Loss: 0.31684741377830505\n",
      "Epoch 3470, Loss: 1.749316245317459, Final Batch Loss: 0.354899138212204\n",
      "Epoch 3471, Loss: 1.686902016401291, Final Batch Loss: 0.39765429496765137\n",
      "Epoch 3472, Loss: 1.680455356836319, Final Batch Loss: 0.30605995655059814\n",
      "Epoch 3473, Loss: 1.567988932132721, Final Batch Loss: 0.22834280133247375\n",
      "Epoch 3474, Loss: 1.7072063386440277, Final Batch Loss: 0.2533521354198456\n",
      "Epoch 3475, Loss: 1.6478078365325928, Final Batch Loss: 0.32527321577072144\n",
      "Epoch 3476, Loss: 1.569581538438797, Final Batch Loss: 0.3439861536026001\n",
      "Epoch 3477, Loss: 1.7651426196098328, Final Batch Loss: 0.36723142862319946\n",
      "Epoch 3478, Loss: 1.8089053332805634, Final Batch Loss: 0.34185028076171875\n",
      "Epoch 3479, Loss: 1.620457798242569, Final Batch Loss: 0.3264395594596863\n",
      "Epoch 3480, Loss: 1.7186230719089508, Final Batch Loss: 0.38714084029197693\n",
      "Epoch 3481, Loss: 1.639670044183731, Final Batch Loss: 0.35581445693969727\n",
      "Epoch 3482, Loss: 1.713972419500351, Final Batch Loss: 0.36385592818260193\n",
      "Epoch 3483, Loss: 1.6160526126623154, Final Batch Loss: 0.26896801590919495\n",
      "Epoch 3484, Loss: 1.677402839064598, Final Batch Loss: 0.2813873887062073\n",
      "Epoch 3485, Loss: 1.8050349950790405, Final Batch Loss: 0.3216313421726227\n",
      "Epoch 3486, Loss: 1.6487419605255127, Final Batch Loss: 0.3731352686882019\n",
      "Epoch 3487, Loss: 1.7957445085048676, Final Batch Loss: 0.3684908151626587\n",
      "Epoch 3488, Loss: 1.6056398451328278, Final Batch Loss: 0.3065256178379059\n",
      "Epoch 3489, Loss: 1.6758520603179932, Final Batch Loss: 0.304498553276062\n",
      "Epoch 3490, Loss: 1.6514192521572113, Final Batch Loss: 0.3234550952911377\n",
      "Epoch 3491, Loss: 1.6770773530006409, Final Batch Loss: 0.3592360019683838\n",
      "Epoch 3492, Loss: 1.6162136495113373, Final Batch Loss: 0.3836096227169037\n",
      "Epoch 3493, Loss: 1.6662465929985046, Final Batch Loss: 0.3484019637107849\n",
      "Epoch 3494, Loss: 1.7629583775997162, Final Batch Loss: 0.3816151022911072\n",
      "Epoch 3495, Loss: 1.7122028172016144, Final Batch Loss: 0.3658730089664459\n",
      "Epoch 3496, Loss: 1.7189252972602844, Final Batch Loss: 0.4027578830718994\n",
      "Epoch 3497, Loss: 1.5338290184736252, Final Batch Loss: 0.28849533200263977\n",
      "Epoch 3498, Loss: 1.711256355047226, Final Batch Loss: 0.32816293835639954\n",
      "Epoch 3499, Loss: 1.6567672193050385, Final Batch Loss: 0.2813885509967804\n",
      "Epoch 3500, Loss: 1.6421309113502502, Final Batch Loss: 0.37002819776535034\n",
      "Epoch 3501, Loss: 1.5966638624668121, Final Batch Loss: 0.2587374150753021\n",
      "Epoch 3502, Loss: 1.6558043658733368, Final Batch Loss: 0.3236221969127655\n",
      "Epoch 3503, Loss: 1.6602306067943573, Final Batch Loss: 0.3863740563392639\n",
      "Epoch 3504, Loss: 1.7352485060691833, Final Batch Loss: 0.4412856996059418\n",
      "Epoch 3505, Loss: 1.650514006614685, Final Batch Loss: 0.3713727593421936\n",
      "Epoch 3506, Loss: 1.6089757978916168, Final Batch Loss: 0.3383418619632721\n",
      "Epoch 3507, Loss: 1.6673299968242645, Final Batch Loss: 0.281463086605072\n",
      "Epoch 3508, Loss: 1.6285011917352676, Final Batch Loss: 0.3535812497138977\n",
      "Epoch 3509, Loss: 1.7530755996704102, Final Batch Loss: 0.4019967019557953\n",
      "Epoch 3510, Loss: 1.6357499957084656, Final Batch Loss: 0.3345997631549835\n",
      "Epoch 3511, Loss: 1.6282319724559784, Final Batch Loss: 0.3095243573188782\n",
      "Epoch 3512, Loss: 1.6314473152160645, Final Batch Loss: 0.3406606614589691\n",
      "Epoch 3513, Loss: 1.6497951447963715, Final Batch Loss: 0.26923123002052307\n",
      "Epoch 3514, Loss: 1.6781703531742096, Final Batch Loss: 0.32650646567344666\n",
      "Epoch 3515, Loss: 1.5063254237174988, Final Batch Loss: 0.21653243899345398\n",
      "Epoch 3516, Loss: 1.5815891027450562, Final Batch Loss: 0.30588147044181824\n",
      "Epoch 3517, Loss: 1.6339119374752045, Final Batch Loss: 0.3352982997894287\n",
      "Epoch 3518, Loss: 1.524594783782959, Final Batch Loss: 0.28653714060783386\n",
      "Epoch 3519, Loss: 1.5758358836174011, Final Batch Loss: 0.37268614768981934\n",
      "Epoch 3520, Loss: 1.697712481021881, Final Batch Loss: 0.3817797005176544\n",
      "Epoch 3521, Loss: 1.741195946931839, Final Batch Loss: 0.35947471857070923\n",
      "Epoch 3522, Loss: 1.7624779641628265, Final Batch Loss: 0.3710475265979767\n",
      "Epoch 3523, Loss: 1.6963480114936829, Final Batch Loss: 0.4289134442806244\n",
      "Epoch 3524, Loss: 1.8128523230552673, Final Batch Loss: 0.4454154074192047\n",
      "Epoch 3525, Loss: 1.5140870213508606, Final Batch Loss: 0.30751511454582214\n",
      "Epoch 3526, Loss: 1.7922129034996033, Final Batch Loss: 0.44371989369392395\n",
      "Epoch 3527, Loss: 1.8381665349006653, Final Batch Loss: 0.5764411091804504\n",
      "Epoch 3528, Loss: 1.6077608168125153, Final Batch Loss: 0.29800885915756226\n",
      "Epoch 3529, Loss: 1.6621996462345123, Final Batch Loss: 0.30610060691833496\n",
      "Epoch 3530, Loss: 1.5639322102069855, Final Batch Loss: 0.3018348813056946\n",
      "Epoch 3531, Loss: 1.5437179505825043, Final Batch Loss: 0.3167133629322052\n",
      "Epoch 3532, Loss: 1.7264517545700073, Final Batch Loss: 0.42791852355003357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3533, Loss: 1.6044450998306274, Final Batch Loss: 0.28002485632896423\n",
      "Epoch 3534, Loss: 1.8372678458690643, Final Batch Loss: 0.447200745344162\n",
      "Epoch 3535, Loss: 1.646617829799652, Final Batch Loss: 0.3285689651966095\n",
      "Epoch 3536, Loss: 1.6184371709823608, Final Batch Loss: 0.2914758324623108\n",
      "Epoch 3537, Loss: 1.617447316646576, Final Batch Loss: 0.32969653606414795\n",
      "Epoch 3538, Loss: 1.6149920523166656, Final Batch Loss: 0.31917044520378113\n",
      "Epoch 3539, Loss: 1.5918653905391693, Final Batch Loss: 0.29308634996414185\n",
      "Epoch 3540, Loss: 1.722521722316742, Final Batch Loss: 0.4374491572380066\n",
      "Epoch 3541, Loss: 1.707977518439293, Final Batch Loss: 0.31000223755836487\n",
      "Epoch 3542, Loss: 1.6084019243717194, Final Batch Loss: 0.3405430018901825\n",
      "Epoch 3543, Loss: 1.7107023298740387, Final Batch Loss: 0.3213801383972168\n",
      "Epoch 3544, Loss: 1.5681557059288025, Final Batch Loss: 0.26564401388168335\n",
      "Epoch 3545, Loss: 1.7143234312534332, Final Batch Loss: 0.38994675874710083\n",
      "Epoch 3546, Loss: 1.6295352578163147, Final Batch Loss: 0.25888901948928833\n",
      "Epoch 3547, Loss: 1.7956455647945404, Final Batch Loss: 0.4175652265548706\n",
      "Epoch 3548, Loss: 1.7078417241573334, Final Batch Loss: 0.3650342524051666\n",
      "Epoch 3549, Loss: 1.565994918346405, Final Batch Loss: 0.38524559140205383\n",
      "Epoch 3550, Loss: 1.597870111465454, Final Batch Loss: 0.3618296682834625\n",
      "Epoch 3551, Loss: 1.6858619153499603, Final Batch Loss: 0.3219384253025055\n",
      "Epoch 3552, Loss: 1.6009374260902405, Final Batch Loss: 0.3239908516407013\n",
      "Epoch 3553, Loss: 1.6164193153381348, Final Batch Loss: 0.2536470890045166\n",
      "Epoch 3554, Loss: 1.812726616859436, Final Batch Loss: 0.3061904311180115\n",
      "Epoch 3555, Loss: 1.619387537240982, Final Batch Loss: 0.35747405886650085\n",
      "Epoch 3556, Loss: 1.728661686182022, Final Batch Loss: 0.3800642192363739\n",
      "Epoch 3557, Loss: 1.7161471247673035, Final Batch Loss: 0.3412238657474518\n",
      "Epoch 3558, Loss: 1.6949362456798553, Final Batch Loss: 0.33679232001304626\n",
      "Epoch 3559, Loss: 1.8439833521842957, Final Batch Loss: 0.37871813774108887\n",
      "Epoch 3560, Loss: 1.6231287717819214, Final Batch Loss: 0.2977990508079529\n",
      "Epoch 3561, Loss: 1.6188697814941406, Final Batch Loss: 0.31504225730895996\n",
      "Epoch 3562, Loss: 1.6460103392601013, Final Batch Loss: 0.3346891701221466\n",
      "Epoch 3563, Loss: 1.5962269604206085, Final Batch Loss: 0.26805099844932556\n",
      "Epoch 3564, Loss: 1.6117411851882935, Final Batch Loss: 0.3409218490123749\n",
      "Epoch 3565, Loss: 1.6964903771877289, Final Batch Loss: 0.3090950846672058\n",
      "Epoch 3566, Loss: 1.6979358494281769, Final Batch Loss: 0.31871724128723145\n",
      "Epoch 3567, Loss: 1.573767215013504, Final Batch Loss: 0.24480348825454712\n",
      "Epoch 3568, Loss: 1.6279732882976532, Final Batch Loss: 0.290534108877182\n",
      "Epoch 3569, Loss: 1.6735427975654602, Final Batch Loss: 0.29358142614364624\n",
      "Epoch 3570, Loss: 1.6135058999061584, Final Batch Loss: 0.2765214741230011\n",
      "Epoch 3571, Loss: 1.5481828451156616, Final Batch Loss: 0.31178009510040283\n",
      "Epoch 3572, Loss: 1.813543975353241, Final Batch Loss: 0.47824370861053467\n",
      "Epoch 3573, Loss: 1.6933687329292297, Final Batch Loss: 0.261740118265152\n",
      "Epoch 3574, Loss: 1.73428475856781, Final Batch Loss: 0.34511396288871765\n",
      "Epoch 3575, Loss: 1.6126594245433807, Final Batch Loss: 0.3190474510192871\n",
      "Epoch 3576, Loss: 1.7999195754528046, Final Batch Loss: 0.39635035395622253\n",
      "Epoch 3577, Loss: 1.5914560556411743, Final Batch Loss: 0.33921870589256287\n",
      "Epoch 3578, Loss: 1.5206596553325653, Final Batch Loss: 0.27984917163848877\n",
      "Epoch 3579, Loss: 1.6131844520568848, Final Batch Loss: 0.3580564558506012\n",
      "Epoch 3580, Loss: 1.7116940021514893, Final Batch Loss: 0.3558975160121918\n",
      "Epoch 3581, Loss: 1.834936261177063, Final Batch Loss: 0.3684057891368866\n",
      "Epoch 3582, Loss: 1.736201286315918, Final Batch Loss: 0.36717504262924194\n",
      "Epoch 3583, Loss: 1.7201469838619232, Final Batch Loss: 0.3187373876571655\n",
      "Epoch 3584, Loss: 1.699051707983017, Final Batch Loss: 0.3748495280742645\n",
      "Epoch 3585, Loss: 1.6926957368850708, Final Batch Loss: 0.3727487623691559\n",
      "Epoch 3586, Loss: 1.747706264257431, Final Batch Loss: 0.34758904576301575\n",
      "Epoch 3587, Loss: 1.7463387846946716, Final Batch Loss: 0.41564956307411194\n",
      "Epoch 3588, Loss: 1.6057022511959076, Final Batch Loss: 0.2722046673297882\n",
      "Epoch 3589, Loss: 1.5911558270454407, Final Batch Loss: 0.30555692315101624\n",
      "Epoch 3590, Loss: 1.7391635477542877, Final Batch Loss: 0.44558069109916687\n",
      "Epoch 3591, Loss: 1.9368226528167725, Final Batch Loss: 0.360333114862442\n",
      "Epoch 3592, Loss: 1.5740588009357452, Final Batch Loss: 0.294766366481781\n",
      "Epoch 3593, Loss: 1.6199442446231842, Final Batch Loss: 0.2801443040370941\n",
      "Epoch 3594, Loss: 1.5533238500356674, Final Batch Loss: 0.23121298849582672\n",
      "Epoch 3595, Loss: 1.8057708442211151, Final Batch Loss: 0.3819749057292938\n",
      "Epoch 3596, Loss: 1.691378653049469, Final Batch Loss: 0.28645002841949463\n",
      "Epoch 3597, Loss: 1.5589824169874191, Final Batch Loss: 0.27221277356147766\n",
      "Epoch 3598, Loss: 1.6675459742546082, Final Batch Loss: 0.36761656403541565\n",
      "Epoch 3599, Loss: 1.750666081905365, Final Batch Loss: 0.32878080010414124\n",
      "Epoch 3600, Loss: 1.696652501821518, Final Batch Loss: 0.3316231071949005\n",
      "Epoch 3601, Loss: 1.6635282635688782, Final Batch Loss: 0.3551584780216217\n",
      "Epoch 3602, Loss: 1.6260125637054443, Final Batch Loss: 0.32196909189224243\n",
      "Epoch 3603, Loss: 1.6878559291362762, Final Batch Loss: 0.24171802401542664\n",
      "Epoch 3604, Loss: 1.6170139014720917, Final Batch Loss: 0.3142586052417755\n",
      "Epoch 3605, Loss: 1.723505049943924, Final Batch Loss: 0.3525576889514923\n",
      "Epoch 3606, Loss: 1.5170568525791168, Final Batch Loss: 0.2714696228504181\n",
      "Epoch 3607, Loss: 1.677910566329956, Final Batch Loss: 0.29137176275253296\n",
      "Epoch 3608, Loss: 1.7474535405635834, Final Batch Loss: 0.38363170623779297\n",
      "Epoch 3609, Loss: 1.628869503736496, Final Batch Loss: 0.27664169669151306\n",
      "Epoch 3610, Loss: 1.5544134378433228, Final Batch Loss: 0.291527658700943\n",
      "Epoch 3611, Loss: 1.589251920580864, Final Batch Loss: 0.39934730529785156\n",
      "Epoch 3612, Loss: 1.6593505144119263, Final Batch Loss: 0.3241456151008606\n",
      "Epoch 3613, Loss: 1.5078730583190918, Final Batch Loss: 0.2381807565689087\n",
      "Epoch 3614, Loss: 1.6597057580947876, Final Batch Loss: 0.4733830392360687\n",
      "Epoch 3615, Loss: 1.5772274434566498, Final Batch Loss: 0.25585100054740906\n",
      "Epoch 3616, Loss: 1.6044475585222244, Final Batch Loss: 0.36461836099624634\n",
      "Epoch 3617, Loss: 1.5887326896190643, Final Batch Loss: 0.3816030025482178\n",
      "Epoch 3618, Loss: 1.774871975183487, Final Batch Loss: 0.4585686922073364\n",
      "Epoch 3619, Loss: 1.5449699759483337, Final Batch Loss: 0.3204388916492462\n",
      "Epoch 3620, Loss: 1.7071812748908997, Final Batch Loss: 0.38586515188217163\n",
      "Epoch 3621, Loss: 1.5258528590202332, Final Batch Loss: 0.250542014837265\n",
      "Epoch 3622, Loss: 1.6077476739883423, Final Batch Loss: 0.35664311051368713\n",
      "Epoch 3623, Loss: 1.5123428404331207, Final Batch Loss: 0.2566995620727539\n",
      "Epoch 3624, Loss: 1.7534700781106949, Final Batch Loss: 0.43674197793006897\n",
      "Epoch 3625, Loss: 1.961060881614685, Final Batch Loss: 0.43311387300491333\n",
      "Epoch 3626, Loss: 1.764815479516983, Final Batch Loss: 0.37819749116897583\n",
      "Epoch 3627, Loss: 1.5380294471979141, Final Batch Loss: 0.22927357256412506\n",
      "Epoch 3628, Loss: 1.6390198171138763, Final Batch Loss: 0.3259347677230835\n",
      "Epoch 3629, Loss: 1.4975904822349548, Final Batch Loss: 0.2764788269996643\n",
      "Epoch 3630, Loss: 1.5539024770259857, Final Batch Loss: 0.29308944940567017\n",
      "Epoch 3631, Loss: 1.6808339059352875, Final Batch Loss: 0.34810400009155273\n",
      "Epoch 3632, Loss: 1.6512328088283539, Final Batch Loss: 0.36638766527175903\n",
      "Epoch 3633, Loss: 1.6880908012390137, Final Batch Loss: 0.3812655508518219\n",
      "Epoch 3634, Loss: 1.6751569360494614, Final Batch Loss: 0.4903344511985779\n",
      "Epoch 3635, Loss: 1.6278179287910461, Final Batch Loss: 0.4160862863063812\n",
      "Epoch 3636, Loss: 1.6596965491771698, Final Batch Loss: 0.3750483989715576\n",
      "Epoch 3637, Loss: 1.663865476846695, Final Batch Loss: 0.34302037954330444\n",
      "Epoch 3638, Loss: 1.7725547850131989, Final Batch Loss: 0.3619695007801056\n",
      "Epoch 3639, Loss: 1.573317974805832, Final Batch Loss: 0.35134729743003845\n",
      "Epoch 3640, Loss: 1.7607749700546265, Final Batch Loss: 0.34271708130836487\n",
      "Epoch 3641, Loss: 1.632068157196045, Final Batch Loss: 0.2833901643753052\n",
      "Epoch 3642, Loss: 1.7765754461288452, Final Batch Loss: 0.4320352077484131\n",
      "Epoch 3643, Loss: 1.657319039106369, Final Batch Loss: 0.3239201605319977\n",
      "Epoch 3644, Loss: 1.6130179017782211, Final Batch Loss: 0.37651723623275757\n",
      "Epoch 3645, Loss: 1.6628709733486176, Final Batch Loss: 0.27956682443618774\n",
      "Epoch 3646, Loss: 1.745271772146225, Final Batch Loss: 0.41940993070602417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3647, Loss: 1.5893534421920776, Final Batch Loss: 0.2824138402938843\n",
      "Epoch 3648, Loss: 1.6131053268909454, Final Batch Loss: 0.3659568727016449\n",
      "Epoch 3649, Loss: 1.6724371016025543, Final Batch Loss: 0.3634447455406189\n",
      "Epoch 3650, Loss: 1.657440960407257, Final Batch Loss: 0.27713897824287415\n",
      "Epoch 3651, Loss: 1.6636223495006561, Final Batch Loss: 0.3203555643558502\n",
      "Epoch 3652, Loss: 1.6829732954502106, Final Batch Loss: 0.32445427775382996\n",
      "Epoch 3653, Loss: 1.6415081918239594, Final Batch Loss: 0.316895991563797\n",
      "Epoch 3654, Loss: 1.602024793624878, Final Batch Loss: 0.3127496540546417\n",
      "Epoch 3655, Loss: 1.6392186880111694, Final Batch Loss: 0.3015300929546356\n",
      "Epoch 3656, Loss: 1.5920311212539673, Final Batch Loss: 0.3325378894805908\n",
      "Epoch 3657, Loss: 1.7318584024906158, Final Batch Loss: 0.45666977763175964\n",
      "Epoch 3658, Loss: 1.6165792047977448, Final Batch Loss: 0.30843257904052734\n",
      "Epoch 3659, Loss: 1.6108593046665192, Final Batch Loss: 0.34306979179382324\n",
      "Epoch 3660, Loss: 1.6447764039039612, Final Batch Loss: 0.3279484212398529\n",
      "Epoch 3661, Loss: 1.746258795261383, Final Batch Loss: 0.2990979552268982\n",
      "Epoch 3662, Loss: 1.586514174938202, Final Batch Loss: 0.34798479080200195\n",
      "Epoch 3663, Loss: 1.7131012380123138, Final Batch Loss: 0.2868770658969879\n",
      "Epoch 3664, Loss: 1.5702889561653137, Final Batch Loss: 0.31293877959251404\n",
      "Epoch 3665, Loss: 1.638221025466919, Final Batch Loss: 0.2865067720413208\n",
      "Epoch 3666, Loss: 1.6209308952093124, Final Batch Loss: 0.22029198706150055\n",
      "Epoch 3667, Loss: 1.684274673461914, Final Batch Loss: 0.3315746486186981\n",
      "Epoch 3668, Loss: 1.5652799606323242, Final Batch Loss: 0.2819443941116333\n",
      "Epoch 3669, Loss: 1.672321230173111, Final Batch Loss: 0.3696657717227936\n",
      "Epoch 3670, Loss: 1.7429798543453217, Final Batch Loss: 0.44561418890953064\n",
      "Epoch 3671, Loss: 1.5964494347572327, Final Batch Loss: 0.3374258279800415\n",
      "Epoch 3672, Loss: 1.7891710996627808, Final Batch Loss: 0.34954336285591125\n",
      "Epoch 3673, Loss: 1.6818552911281586, Final Batch Loss: 0.3636525571346283\n",
      "Epoch 3674, Loss: 1.66072678565979, Final Batch Loss: 0.35407888889312744\n",
      "Epoch 3675, Loss: 1.6317790746688843, Final Batch Loss: 0.25915056467056274\n",
      "Epoch 3676, Loss: 1.7233769595623016, Final Batch Loss: 0.4206889569759369\n",
      "Epoch 3677, Loss: 1.6647650301456451, Final Batch Loss: 0.25595641136169434\n",
      "Epoch 3678, Loss: 1.7497077584266663, Final Batch Loss: 0.33196964859962463\n",
      "Epoch 3679, Loss: 1.62411630153656, Final Batch Loss: 0.3316805362701416\n",
      "Epoch 3680, Loss: 1.6556355357170105, Final Batch Loss: 0.21724280714988708\n",
      "Epoch 3681, Loss: 1.7203512489795685, Final Batch Loss: 0.34243497252464294\n",
      "Epoch 3682, Loss: 1.6918857395648956, Final Batch Loss: 0.31694597005844116\n",
      "Epoch 3683, Loss: 1.587117612361908, Final Batch Loss: 0.2891618013381958\n",
      "Epoch 3684, Loss: 1.6661414802074432, Final Batch Loss: 0.3405141234397888\n",
      "Epoch 3685, Loss: 1.76423779129982, Final Batch Loss: 0.35869500041007996\n",
      "Epoch 3686, Loss: 1.8181695938110352, Final Batch Loss: 0.39197418093681335\n",
      "Epoch 3687, Loss: 1.6821652352809906, Final Batch Loss: 0.3751760423183441\n",
      "Epoch 3688, Loss: 1.5994440615177155, Final Batch Loss: 0.4162482023239136\n",
      "Epoch 3689, Loss: 1.7844190001487732, Final Batch Loss: 0.3634060025215149\n",
      "Epoch 3690, Loss: 1.600287288427353, Final Batch Loss: 0.26888880133628845\n",
      "Epoch 3691, Loss: 1.6770565211772919, Final Batch Loss: 0.3031879663467407\n",
      "Epoch 3692, Loss: 1.5988262593746185, Final Batch Loss: 0.33773449063301086\n",
      "Epoch 3693, Loss: 1.6332599073648453, Final Batch Loss: 0.21617399156093597\n",
      "Epoch 3694, Loss: 1.617518424987793, Final Batch Loss: 0.4121233820915222\n",
      "Epoch 3695, Loss: 1.6222313344478607, Final Batch Loss: 0.27455514669418335\n",
      "Epoch 3696, Loss: 1.6721493899822235, Final Batch Loss: 0.3153775930404663\n",
      "Epoch 3697, Loss: 1.5906473398208618, Final Batch Loss: 0.33164849877357483\n",
      "Epoch 3698, Loss: 1.653923660516739, Final Batch Loss: 0.3260171413421631\n",
      "Epoch 3699, Loss: 1.577527791261673, Final Batch Loss: 0.28758004307746887\n",
      "Epoch 3700, Loss: 1.7293092906475067, Final Batch Loss: 0.38766610622406006\n",
      "Epoch 3701, Loss: 1.6553482115268707, Final Batch Loss: 0.2915838062763214\n",
      "Epoch 3702, Loss: 1.8420180678367615, Final Batch Loss: 0.36586931347846985\n",
      "Epoch 3703, Loss: 1.627491533756256, Final Batch Loss: 0.3083738386631012\n",
      "Epoch 3704, Loss: 1.8886157870292664, Final Batch Loss: 0.459747850894928\n",
      "Epoch 3705, Loss: 1.6447467654943466, Final Batch Loss: 0.2088136225938797\n",
      "Epoch 3706, Loss: 1.6491921842098236, Final Batch Loss: 0.28177207708358765\n",
      "Epoch 3707, Loss: 1.6162009835243225, Final Batch Loss: 0.30728599429130554\n",
      "Epoch 3708, Loss: 1.7867466807365417, Final Batch Loss: 0.3313513398170471\n",
      "Epoch 3709, Loss: 1.773264467716217, Final Batch Loss: 0.3602063059806824\n",
      "Epoch 3710, Loss: 1.671113759279251, Final Batch Loss: 0.357430100440979\n",
      "Epoch 3711, Loss: 1.5959319174289703, Final Batch Loss: 0.30834028124809265\n",
      "Epoch 3712, Loss: 1.7550662159919739, Final Batch Loss: 0.2976717948913574\n",
      "Epoch 3713, Loss: 1.627417415380478, Final Batch Loss: 0.2899828553199768\n",
      "Epoch 3714, Loss: 1.6798486709594727, Final Batch Loss: 0.2921297252178192\n",
      "Epoch 3715, Loss: 1.7405399978160858, Final Batch Loss: 0.2859027087688446\n",
      "Epoch 3716, Loss: 1.632261574268341, Final Batch Loss: 0.33568572998046875\n",
      "Epoch 3717, Loss: 1.8023592978715897, Final Batch Loss: 0.4759413003921509\n",
      "Epoch 3718, Loss: 1.6778915226459503, Final Batch Loss: 0.31664326786994934\n",
      "Epoch 3719, Loss: 1.5301247239112854, Final Batch Loss: 0.2750360369682312\n",
      "Epoch 3720, Loss: 1.652476042509079, Final Batch Loss: 0.2681714594364166\n",
      "Epoch 3721, Loss: 1.4520877301692963, Final Batch Loss: 0.29095107316970825\n",
      "Epoch 3722, Loss: 1.6697192788124084, Final Batch Loss: 0.33380886912345886\n",
      "Epoch 3723, Loss: 1.6268413960933685, Final Batch Loss: 0.2680681645870209\n",
      "Epoch 3724, Loss: 1.6159996092319489, Final Batch Loss: 0.2908194661140442\n",
      "Epoch 3725, Loss: 1.5931872725486755, Final Batch Loss: 0.3035975992679596\n",
      "Epoch 3726, Loss: 1.6646705567836761, Final Batch Loss: 0.26738518476486206\n",
      "Epoch 3727, Loss: 1.606388971209526, Final Batch Loss: 0.2870464026927948\n",
      "Epoch 3728, Loss: 1.6638180613517761, Final Batch Loss: 0.3106729984283447\n",
      "Epoch 3729, Loss: 1.7404132783412933, Final Batch Loss: 0.32677990198135376\n",
      "Epoch 3730, Loss: 1.687421590089798, Final Batch Loss: 0.3437829911708832\n",
      "Epoch 3731, Loss: 1.705830603837967, Final Batch Loss: 0.3736184239387512\n",
      "Epoch 3732, Loss: 1.6825790703296661, Final Batch Loss: 0.3945119380950928\n",
      "Epoch 3733, Loss: 1.6534176468849182, Final Batch Loss: 0.3585752248764038\n",
      "Epoch 3734, Loss: 1.656266063451767, Final Batch Loss: 0.3494066894054413\n",
      "Epoch 3735, Loss: 1.695820391178131, Final Batch Loss: 0.30895504355430603\n",
      "Epoch 3736, Loss: 1.7321825921535492, Final Batch Loss: 0.3415878713130951\n",
      "Epoch 3737, Loss: 1.5858688354492188, Final Batch Loss: 0.31080278754234314\n",
      "Epoch 3738, Loss: 1.5707311034202576, Final Batch Loss: 0.2512507438659668\n",
      "Epoch 3739, Loss: 1.6485950648784637, Final Batch Loss: 0.31893905997276306\n",
      "Epoch 3740, Loss: 1.5489463806152344, Final Batch Loss: 0.283698707818985\n",
      "Epoch 3741, Loss: 1.622939258813858, Final Batch Loss: 0.3687947988510132\n",
      "Epoch 3742, Loss: 1.7162107825279236, Final Batch Loss: 0.3473662734031677\n",
      "Epoch 3743, Loss: 1.6199788451194763, Final Batch Loss: 0.3132210969924927\n",
      "Epoch 3744, Loss: 1.595221221446991, Final Batch Loss: 0.3631160259246826\n",
      "Epoch 3745, Loss: 1.605786144733429, Final Batch Loss: 0.375482439994812\n",
      "Epoch 3746, Loss: 1.7455205917358398, Final Batch Loss: 0.3451555073261261\n",
      "Epoch 3747, Loss: 1.608398824930191, Final Batch Loss: 0.26948031783103943\n",
      "Epoch 3748, Loss: 1.5491209924221039, Final Batch Loss: 0.2770099639892578\n",
      "Epoch 3749, Loss: 1.7775513529777527, Final Batch Loss: 0.382373183965683\n",
      "Epoch 3750, Loss: 1.5419571101665497, Final Batch Loss: 0.34070253372192383\n",
      "Epoch 3751, Loss: 1.6180613040924072, Final Batch Loss: 0.30692940950393677\n",
      "Epoch 3752, Loss: 1.5215495824813843, Final Batch Loss: 0.3561903238296509\n",
      "Epoch 3753, Loss: 1.5756979286670685, Final Batch Loss: 0.21024799346923828\n",
      "Epoch 3754, Loss: 1.5551035404205322, Final Batch Loss: 0.27498602867126465\n",
      "Epoch 3755, Loss: 1.6318312287330627, Final Batch Loss: 0.28151077032089233\n",
      "Epoch 3756, Loss: 1.6537027657032013, Final Batch Loss: 0.3474234342575073\n",
      "Epoch 3757, Loss: 1.7206924259662628, Final Batch Loss: 0.3624512851238251\n",
      "Epoch 3758, Loss: 1.6732138395309448, Final Batch Loss: 0.3298233151435852\n",
      "Epoch 3759, Loss: 1.533046007156372, Final Batch Loss: 0.34974366426467896\n",
      "Epoch 3760, Loss: 1.4899653494358063, Final Batch Loss: 0.2334112524986267\n",
      "Epoch 3761, Loss: 1.5044823288917542, Final Batch Loss: 0.23236283659934998\n",
      "Epoch 3762, Loss: 1.5252889096736908, Final Batch Loss: 0.305936723947525\n",
      "Epoch 3763, Loss: 1.563326209783554, Final Batch Loss: 0.39236709475517273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3764, Loss: 1.623240888118744, Final Batch Loss: 0.3191482424736023\n",
      "Epoch 3765, Loss: 1.6372426450252533, Final Batch Loss: 0.2964414954185486\n",
      "Epoch 3766, Loss: 1.7056411802768707, Final Batch Loss: 0.3405155837535858\n",
      "Epoch 3767, Loss: 1.6646181046962738, Final Batch Loss: 0.34878671169281006\n",
      "Epoch 3768, Loss: 1.6206935048103333, Final Batch Loss: 0.4255625307559967\n",
      "Epoch 3769, Loss: 1.6522716581821442, Final Batch Loss: 0.39379552006721497\n",
      "Epoch 3770, Loss: 1.6700285971164703, Final Batch Loss: 0.3558099865913391\n",
      "Epoch 3771, Loss: 1.7259228825569153, Final Batch Loss: 0.4510965347290039\n",
      "Epoch 3772, Loss: 1.6896282732486725, Final Batch Loss: 0.2561272382736206\n",
      "Epoch 3773, Loss: 1.4548173546791077, Final Batch Loss: 0.2793136239051819\n",
      "Epoch 3774, Loss: 1.578551173210144, Final Batch Loss: 0.3788759708404541\n",
      "Epoch 3775, Loss: 1.689352810382843, Final Batch Loss: 0.319834440946579\n",
      "Epoch 3776, Loss: 1.7007513642311096, Final Batch Loss: 0.3132742941379547\n",
      "Epoch 3777, Loss: 1.521410197019577, Final Batch Loss: 0.2867148518562317\n",
      "Epoch 3778, Loss: 1.6894539594650269, Final Batch Loss: 0.40270379185676575\n",
      "Epoch 3779, Loss: 1.6326630115509033, Final Batch Loss: 0.29973119497299194\n",
      "Epoch 3780, Loss: 1.56165012717247, Final Batch Loss: 0.3011484444141388\n",
      "Epoch 3781, Loss: 1.6890390813350677, Final Batch Loss: 0.38210928440093994\n",
      "Epoch 3782, Loss: 1.628021940588951, Final Batch Loss: 0.33589431643486023\n",
      "Epoch 3783, Loss: 1.5278847366571426, Final Batch Loss: 0.33795666694641113\n",
      "Epoch 3784, Loss: 1.5923044383525848, Final Batch Loss: 0.26542526483535767\n",
      "Epoch 3785, Loss: 1.6923274993896484, Final Batch Loss: 0.42351752519607544\n",
      "Epoch 3786, Loss: 1.608548879623413, Final Batch Loss: 0.29025959968566895\n",
      "Epoch 3787, Loss: 1.6356314420700073, Final Batch Loss: 0.37552857398986816\n",
      "Epoch 3788, Loss: 1.617522418498993, Final Batch Loss: 0.3192095160484314\n",
      "Epoch 3789, Loss: 1.604940414428711, Final Batch Loss: 0.29542940855026245\n",
      "Epoch 3790, Loss: 1.6489541232585907, Final Batch Loss: 0.3002428114414215\n",
      "Epoch 3791, Loss: 1.5688299983739853, Final Batch Loss: 0.24070076644420624\n",
      "Epoch 3792, Loss: 1.5595581829547882, Final Batch Loss: 0.3521806001663208\n",
      "Epoch 3793, Loss: 1.6536705195903778, Final Batch Loss: 0.2988726794719696\n",
      "Epoch 3794, Loss: 1.5403291285037994, Final Batch Loss: 0.31255507469177246\n",
      "Epoch 3795, Loss: 1.571623682975769, Final Batch Loss: 0.38344523310661316\n",
      "Epoch 3796, Loss: 1.4989148080348969, Final Batch Loss: 0.32619667053222656\n",
      "Epoch 3797, Loss: 1.798470914363861, Final Batch Loss: 0.37083104252815247\n",
      "Epoch 3798, Loss: 1.5404872596263885, Final Batch Loss: 0.2986834943294525\n",
      "Epoch 3799, Loss: 1.648075520992279, Final Batch Loss: 0.41079676151275635\n",
      "Epoch 3800, Loss: 1.578881561756134, Final Batch Loss: 0.3510147035121918\n",
      "Epoch 3801, Loss: 1.6683745384216309, Final Batch Loss: 0.36811456084251404\n",
      "Epoch 3802, Loss: 1.4569615423679352, Final Batch Loss: 0.2364690899848938\n",
      "Epoch 3803, Loss: 1.518900990486145, Final Batch Loss: 0.2038186490535736\n",
      "Epoch 3804, Loss: 1.6959095299243927, Final Batch Loss: 0.3104926347732544\n",
      "Epoch 3805, Loss: 1.7725668549537659, Final Batch Loss: 0.45341524481773376\n",
      "Epoch 3806, Loss: 1.5923964083194733, Final Batch Loss: 0.3232172131538391\n",
      "Epoch 3807, Loss: 1.7022094428539276, Final Batch Loss: 0.39206528663635254\n",
      "Epoch 3808, Loss: 1.6393969655036926, Final Batch Loss: 0.33499807119369507\n",
      "Epoch 3809, Loss: 1.630187213420868, Final Batch Loss: 0.26014596223831177\n",
      "Epoch 3810, Loss: 1.6125013828277588, Final Batch Loss: 0.3452984392642975\n",
      "Epoch 3811, Loss: 1.7000941336154938, Final Batch Loss: 0.36001989245414734\n",
      "Epoch 3812, Loss: 1.6207144856452942, Final Batch Loss: 0.3420417308807373\n",
      "Epoch 3813, Loss: 1.6027523279190063, Final Batch Loss: 0.3133230209350586\n",
      "Epoch 3814, Loss: 1.6818273663520813, Final Batch Loss: 0.32752078771591187\n",
      "Epoch 3815, Loss: 1.671414852142334, Final Batch Loss: 0.36943212151527405\n",
      "Epoch 3816, Loss: 1.6296366155147552, Final Batch Loss: 0.30832168459892273\n",
      "Epoch 3817, Loss: 1.66637521982193, Final Batch Loss: 0.35223081707954407\n",
      "Epoch 3818, Loss: 1.5244270712137222, Final Batch Loss: 0.22407357394695282\n",
      "Epoch 3819, Loss: 1.75777205824852, Final Batch Loss: 0.4494425356388092\n",
      "Epoch 3820, Loss: 1.6321195363998413, Final Batch Loss: 0.3312819302082062\n",
      "Epoch 3821, Loss: 1.5565404891967773, Final Batch Loss: 0.32022786140441895\n",
      "Epoch 3822, Loss: 1.8148527145385742, Final Batch Loss: 0.29775288701057434\n",
      "Epoch 3823, Loss: 1.4852297455072403, Final Batch Loss: 0.3133994936943054\n",
      "Epoch 3824, Loss: 1.5400859713554382, Final Batch Loss: 0.25508755445480347\n",
      "Epoch 3825, Loss: 1.8603887557983398, Final Batch Loss: 0.4570588171482086\n",
      "Epoch 3826, Loss: 1.6370681524276733, Final Batch Loss: 0.31258368492126465\n",
      "Epoch 3827, Loss: 1.533291608095169, Final Batch Loss: 0.32808491587638855\n",
      "Epoch 3828, Loss: 1.6374504268169403, Final Batch Loss: 0.3662188947200775\n",
      "Epoch 3829, Loss: 1.6493191719055176, Final Batch Loss: 0.2481989562511444\n",
      "Epoch 3830, Loss: 1.5938376635313034, Final Batch Loss: 0.2461806982755661\n",
      "Epoch 3831, Loss: 1.903839886188507, Final Batch Loss: 0.5566359162330627\n",
      "Epoch 3832, Loss: 1.636876493692398, Final Batch Loss: 0.30904659628868103\n",
      "Epoch 3833, Loss: 1.5482808351516724, Final Batch Loss: 0.27527520060539246\n",
      "Epoch 3834, Loss: 1.5596171021461487, Final Batch Loss: 0.3755171000957489\n",
      "Epoch 3835, Loss: 1.629668414592743, Final Batch Loss: 0.2985004484653473\n",
      "Epoch 3836, Loss: 1.589106872677803, Final Batch Loss: 0.230318084359169\n",
      "Epoch 3837, Loss: 1.6350723206996918, Final Batch Loss: 0.2537822425365448\n",
      "Epoch 3838, Loss: 1.6937856078147888, Final Batch Loss: 0.3617687225341797\n",
      "Epoch 3839, Loss: 1.7114814817905426, Final Batch Loss: 0.3406023383140564\n",
      "Epoch 3840, Loss: 1.5051970779895782, Final Batch Loss: 0.308797150850296\n",
      "Epoch 3841, Loss: 1.7489669919013977, Final Batch Loss: 0.3610890805721283\n",
      "Epoch 3842, Loss: 1.649921327829361, Final Batch Loss: 0.3417772352695465\n",
      "Epoch 3843, Loss: 1.597252368927002, Final Batch Loss: 0.2899574041366577\n",
      "Epoch 3844, Loss: 1.6588111519813538, Final Batch Loss: 0.3081967830657959\n",
      "Epoch 3845, Loss: 1.6221213638782501, Final Batch Loss: 0.3762614130973816\n",
      "Epoch 3846, Loss: 1.4458175003528595, Final Batch Loss: 0.2475428581237793\n",
      "Epoch 3847, Loss: 1.5485393106937408, Final Batch Loss: 0.27733245491981506\n",
      "Epoch 3848, Loss: 1.5515452176332474, Final Batch Loss: 0.3457321226596832\n",
      "Epoch 3849, Loss: 1.6090008616447449, Final Batch Loss: 0.3683418035507202\n",
      "Epoch 3850, Loss: 1.594766080379486, Final Batch Loss: 0.405988484621048\n",
      "Epoch 3851, Loss: 1.672042340040207, Final Batch Loss: 0.3457808196544647\n",
      "Epoch 3852, Loss: 1.6330723762512207, Final Batch Loss: 0.3312172293663025\n",
      "Epoch 3853, Loss: 1.4921188652515411, Final Batch Loss: 0.2825550138950348\n",
      "Epoch 3854, Loss: 1.551555097103119, Final Batch Loss: 0.2081248164176941\n",
      "Epoch 3855, Loss: 1.6550459563732147, Final Batch Loss: 0.3348843455314636\n",
      "Epoch 3856, Loss: 1.6317615807056427, Final Batch Loss: 0.17250198125839233\n",
      "Epoch 3857, Loss: 1.6058366894721985, Final Batch Loss: 0.34244638681411743\n",
      "Epoch 3858, Loss: 1.5539232790470123, Final Batch Loss: 0.31318238377571106\n",
      "Epoch 3859, Loss: 1.4853543043136597, Final Batch Loss: 0.3337746858596802\n",
      "Epoch 3860, Loss: 1.5508596748113632, Final Batch Loss: 0.35081690549850464\n",
      "Epoch 3861, Loss: 1.503290742635727, Final Batch Loss: 0.2771172821521759\n",
      "Epoch 3862, Loss: 1.6211101859807968, Final Batch Loss: 0.22356154024600983\n",
      "Epoch 3863, Loss: 1.6221500039100647, Final Batch Loss: 0.30095139145851135\n",
      "Epoch 3864, Loss: 1.7074970602989197, Final Batch Loss: 0.2899833619594574\n",
      "Epoch 3865, Loss: 1.527983844280243, Final Batch Loss: 0.23642534017562866\n",
      "Epoch 3866, Loss: 1.7114989459514618, Final Batch Loss: 0.4008685350418091\n",
      "Epoch 3867, Loss: 1.652843326330185, Final Batch Loss: 0.32276734709739685\n",
      "Epoch 3868, Loss: 1.7577233612537384, Final Batch Loss: 0.38738754391670227\n",
      "Epoch 3869, Loss: 1.5347936153411865, Final Batch Loss: 0.3266456127166748\n",
      "Epoch 3870, Loss: 1.5592861473560333, Final Batch Loss: 0.3116914927959442\n",
      "Epoch 3871, Loss: 1.603485107421875, Final Batch Loss: 0.36617061495780945\n",
      "Epoch 3872, Loss: 1.568587601184845, Final Batch Loss: 0.2127397656440735\n",
      "Epoch 3873, Loss: 1.6497746109962463, Final Batch Loss: 0.33627599477767944\n",
      "Epoch 3874, Loss: 1.5189160406589508, Final Batch Loss: 0.28416723012924194\n",
      "Epoch 3875, Loss: 1.6244911849498749, Final Batch Loss: 0.21694552898406982\n",
      "Epoch 3876, Loss: 1.6699315011501312, Final Batch Loss: 0.2434995472431183\n",
      "Epoch 3877, Loss: 1.5980310142040253, Final Batch Loss: 0.27207812666893005\n",
      "Epoch 3878, Loss: 1.595147728919983, Final Batch Loss: 0.2986737787723541\n",
      "Epoch 3879, Loss: 1.6428486704826355, Final Batch Loss: 0.32064658403396606\n",
      "Epoch 3880, Loss: 1.6809942424297333, Final Batch Loss: 0.2787763774394989\n",
      "Epoch 3881, Loss: 1.7288316190242767, Final Batch Loss: 0.3051125705242157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3882, Loss: 1.6098380386829376, Final Batch Loss: 0.35134920477867126\n",
      "Epoch 3883, Loss: 1.6368629038333893, Final Batch Loss: 0.32056573033332825\n",
      "Epoch 3884, Loss: 1.5331489145755768, Final Batch Loss: 0.26381921768188477\n",
      "Epoch 3885, Loss: 1.6439024209976196, Final Batch Loss: 0.2758411765098572\n",
      "Epoch 3886, Loss: 1.526919275522232, Final Batch Loss: 0.2915390431880951\n",
      "Epoch 3887, Loss: 1.8123224377632141, Final Batch Loss: 0.5006094574928284\n",
      "Epoch 3888, Loss: 1.7192007005214691, Final Batch Loss: 0.3730681538581848\n",
      "Epoch 3889, Loss: 1.5965105295181274, Final Batch Loss: 0.32514187693595886\n",
      "Epoch 3890, Loss: 1.4171639531850815, Final Batch Loss: 0.22033514082431793\n",
      "Epoch 3891, Loss: 1.5771112442016602, Final Batch Loss: 0.3182540237903595\n",
      "Epoch 3892, Loss: 1.4902214109897614, Final Batch Loss: 0.3269467353820801\n",
      "Epoch 3893, Loss: 1.4797279387712479, Final Batch Loss: 0.31561988592147827\n",
      "Epoch 3894, Loss: 1.7051752805709839, Final Batch Loss: 0.33617648482322693\n",
      "Epoch 3895, Loss: 1.618416577577591, Final Batch Loss: 0.4288824200630188\n",
      "Epoch 3896, Loss: 1.580631583929062, Final Batch Loss: 0.28784793615341187\n",
      "Epoch 3897, Loss: 1.6645818948745728, Final Batch Loss: 0.369522362947464\n",
      "Epoch 3898, Loss: 1.7621000409126282, Final Batch Loss: 0.405428022146225\n",
      "Epoch 3899, Loss: 1.5349034816026688, Final Batch Loss: 0.27465468645095825\n",
      "Epoch 3900, Loss: 1.5965547263622284, Final Batch Loss: 0.23703140020370483\n",
      "Epoch 3901, Loss: 1.6976744830608368, Final Batch Loss: 0.35321465134620667\n",
      "Epoch 3902, Loss: 1.7528524994850159, Final Batch Loss: 0.4027005732059479\n",
      "Epoch 3903, Loss: 1.5889851450920105, Final Batch Loss: 0.30177339911460876\n",
      "Epoch 3904, Loss: 1.6053771674633026, Final Batch Loss: 0.3664869964122772\n",
      "Epoch 3905, Loss: 1.6004355549812317, Final Batch Loss: 0.33169984817504883\n",
      "Epoch 3906, Loss: 1.54209703207016, Final Batch Loss: 0.3106389343738556\n",
      "Epoch 3907, Loss: 1.6042938530445099, Final Batch Loss: 0.35353341698646545\n",
      "Epoch 3908, Loss: 1.498851478099823, Final Batch Loss: 0.2594063878059387\n",
      "Epoch 3909, Loss: 1.6146490275859833, Final Batch Loss: 0.28977206349372864\n",
      "Epoch 3910, Loss: 1.4349605739116669, Final Batch Loss: 0.2854824364185333\n",
      "Epoch 3911, Loss: 1.709703952074051, Final Batch Loss: 0.32326740026474\n",
      "Epoch 3912, Loss: 1.7405509948730469, Final Batch Loss: 0.42327046394348145\n",
      "Epoch 3913, Loss: 1.6974458694458008, Final Batch Loss: 0.48706427216529846\n",
      "Epoch 3914, Loss: 1.6834270060062408, Final Batch Loss: 0.2840651869773865\n",
      "Epoch 3915, Loss: 1.5620045065879822, Final Batch Loss: 0.3102734386920929\n",
      "Epoch 3916, Loss: 1.6892065405845642, Final Batch Loss: 0.30434972047805786\n",
      "Epoch 3917, Loss: 1.5298072844743729, Final Batch Loss: 0.316671758890152\n",
      "Epoch 3918, Loss: 1.6010775566101074, Final Batch Loss: 0.3205288350582123\n",
      "Epoch 3919, Loss: 1.5838631093502045, Final Batch Loss: 0.2939399778842926\n",
      "Epoch 3920, Loss: 1.4839434772729874, Final Batch Loss: 0.2266845703125\n",
      "Epoch 3921, Loss: 1.7065781354904175, Final Batch Loss: 0.3691031336784363\n",
      "Epoch 3922, Loss: 1.617294192314148, Final Batch Loss: 0.35927504301071167\n",
      "Epoch 3923, Loss: 1.6465244889259338, Final Batch Loss: 0.43044301867485046\n",
      "Epoch 3924, Loss: 1.7245671451091766, Final Batch Loss: 0.32193318009376526\n",
      "Epoch 3925, Loss: 1.543702483177185, Final Batch Loss: 0.2353690266609192\n",
      "Epoch 3926, Loss: 1.5066551566123962, Final Batch Loss: 0.2776210606098175\n",
      "Epoch 3927, Loss: 1.5228805840015411, Final Batch Loss: 0.2963417172431946\n",
      "Epoch 3928, Loss: 1.6115120351314545, Final Batch Loss: 0.31564125418663025\n",
      "Epoch 3929, Loss: 1.6183147877454758, Final Batch Loss: 0.3670276701450348\n",
      "Epoch 3930, Loss: 1.7087018489837646, Final Batch Loss: 0.3353884220123291\n",
      "Epoch 3931, Loss: 1.6303526163101196, Final Batch Loss: 0.22359955310821533\n",
      "Epoch 3932, Loss: 1.612952321767807, Final Batch Loss: 0.32540586590766907\n",
      "Epoch 3933, Loss: 1.5294128358364105, Final Batch Loss: 0.3348917067050934\n",
      "Epoch 3934, Loss: 1.6300679743289948, Final Batch Loss: 0.3447132706642151\n",
      "Epoch 3935, Loss: 1.6662727892398834, Final Batch Loss: 0.33029982447624207\n",
      "Epoch 3936, Loss: 1.5150824785232544, Final Batch Loss: 0.32231488823890686\n",
      "Epoch 3937, Loss: 1.581563800573349, Final Batch Loss: 0.33690717816352844\n",
      "Epoch 3938, Loss: 1.593179613351822, Final Batch Loss: 0.303567498922348\n",
      "Epoch 3939, Loss: 1.792675107717514, Final Batch Loss: 0.4223135709762573\n",
      "Epoch 3940, Loss: 1.585914671421051, Final Batch Loss: 0.21627533435821533\n",
      "Epoch 3941, Loss: 1.6265347599983215, Final Batch Loss: 0.28837350010871887\n",
      "Epoch 3942, Loss: 1.5350277721881866, Final Batch Loss: 0.28869521617889404\n",
      "Epoch 3943, Loss: 1.6342355012893677, Final Batch Loss: 0.33429285883903503\n",
      "Epoch 3944, Loss: 1.765825629234314, Final Batch Loss: 0.375233918428421\n",
      "Epoch 3945, Loss: 1.667047142982483, Final Batch Loss: 0.3656006455421448\n",
      "Epoch 3946, Loss: 1.5432479679584503, Final Batch Loss: 0.2637387812137604\n",
      "Epoch 3947, Loss: 1.7783584892749786, Final Batch Loss: 0.4269174635410309\n",
      "Epoch 3948, Loss: 1.530698001384735, Final Batch Loss: 0.2628965377807617\n",
      "Epoch 3949, Loss: 1.4982395470142365, Final Batch Loss: 0.33240318298339844\n",
      "Epoch 3950, Loss: 1.5877330303192139, Final Batch Loss: 0.31776663661003113\n",
      "Epoch 3951, Loss: 1.590681791305542, Final Batch Loss: 0.2998897433280945\n",
      "Epoch 3952, Loss: 1.5988487601280212, Final Batch Loss: 0.3762877881526947\n",
      "Epoch 3953, Loss: 1.5918931365013123, Final Batch Loss: 0.3166464865207672\n",
      "Epoch 3954, Loss: 1.7154688239097595, Final Batch Loss: 0.37571805715560913\n",
      "Epoch 3955, Loss: 1.7001955807209015, Final Batch Loss: 0.3879181742668152\n",
      "Epoch 3956, Loss: 1.6666560471057892, Final Batch Loss: 0.22202301025390625\n",
      "Epoch 3957, Loss: 1.6810639798641205, Final Batch Loss: 0.357215017080307\n",
      "Epoch 3958, Loss: 1.686467468738556, Final Batch Loss: 0.34257176518440247\n",
      "Epoch 3959, Loss: 1.6244469285011292, Final Batch Loss: 0.2392728328704834\n",
      "Epoch 3960, Loss: 1.5009922236204147, Final Batch Loss: 0.3717135488986969\n",
      "Epoch 3961, Loss: 1.6185645461082458, Final Batch Loss: 0.34346261620521545\n",
      "Epoch 3962, Loss: 1.6980418860912323, Final Batch Loss: 0.35332223773002625\n",
      "Epoch 3963, Loss: 1.528786152601242, Final Batch Loss: 0.2859002351760864\n",
      "Epoch 3964, Loss: 1.5551660358905792, Final Batch Loss: 0.2913825213909149\n",
      "Epoch 3965, Loss: 1.5668247789144516, Final Batch Loss: 0.3589622378349304\n",
      "Epoch 3966, Loss: 1.5930196344852448, Final Batch Loss: 0.24874544143676758\n",
      "Epoch 3967, Loss: 1.6513074040412903, Final Batch Loss: 0.3950142562389374\n",
      "Epoch 3968, Loss: 1.6192417740821838, Final Batch Loss: 0.3553984463214874\n",
      "Epoch 3969, Loss: 1.5281532108783722, Final Batch Loss: 0.2844153344631195\n",
      "Epoch 3970, Loss: 1.8533680438995361, Final Batch Loss: 0.39721646904945374\n",
      "Epoch 3971, Loss: 1.6462154984474182, Final Batch Loss: 0.37497153878211975\n",
      "Epoch 3972, Loss: 1.6257641017436981, Final Batch Loss: 0.27064254879951477\n",
      "Epoch 3973, Loss: 1.4852327704429626, Final Batch Loss: 0.22797414660453796\n",
      "Epoch 3974, Loss: 1.8131587505340576, Final Batch Loss: 0.45142221450805664\n",
      "Epoch 3975, Loss: 1.7341499328613281, Final Batch Loss: 0.3767847418785095\n",
      "Epoch 3976, Loss: 1.5841058790683746, Final Batch Loss: 0.3180982172489166\n",
      "Epoch 3977, Loss: 1.6118900775909424, Final Batch Loss: 0.25976452231407166\n",
      "Epoch 3978, Loss: 1.634630262851715, Final Batch Loss: 0.26589348912239075\n",
      "Epoch 3979, Loss: 1.473821222782135, Final Batch Loss: 0.2695693373680115\n",
      "Epoch 3980, Loss: 1.4886982589960098, Final Batch Loss: 0.2088056057691574\n",
      "Epoch 3981, Loss: 1.6305524706840515, Final Batch Loss: 0.34043964743614197\n",
      "Epoch 3982, Loss: 1.6298634558916092, Final Batch Loss: 0.24947412312030792\n",
      "Epoch 3983, Loss: 1.566247671842575, Final Batch Loss: 0.37027955055236816\n",
      "Epoch 3984, Loss: 1.5283405780792236, Final Batch Loss: 0.31886789202690125\n",
      "Epoch 3985, Loss: 1.5525630116462708, Final Batch Loss: 0.2899450957775116\n",
      "Epoch 3986, Loss: 1.5576318800449371, Final Batch Loss: 0.31636354327201843\n",
      "Epoch 3987, Loss: 1.5641978085041046, Final Batch Loss: 0.2803715169429779\n",
      "Epoch 3988, Loss: 1.5720421373844147, Final Batch Loss: 0.34944918751716614\n",
      "Epoch 3989, Loss: 1.5657610595226288, Final Batch Loss: 0.34935593605041504\n",
      "Epoch 3990, Loss: 1.5204239785671234, Final Batch Loss: 0.24136152863502502\n",
      "Epoch 3991, Loss: 1.5699363350868225, Final Batch Loss: 0.38833335041999817\n",
      "Epoch 3992, Loss: 1.578517585992813, Final Batch Loss: 0.3794633150100708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3993, Loss: 1.5303592681884766, Final Batch Loss: 0.25602537393569946\n",
      "Epoch 3994, Loss: 1.5942959189414978, Final Batch Loss: 0.4139366149902344\n",
      "Epoch 3995, Loss: 1.5646875500679016, Final Batch Loss: 0.26125213503837585\n",
      "Epoch 3996, Loss: 1.6470147967338562, Final Batch Loss: 0.2888490557670593\n",
      "Epoch 3997, Loss: 1.524969220161438, Final Batch Loss: 0.30604615807533264\n",
      "Epoch 3998, Loss: 1.6119298934936523, Final Batch Loss: 0.30062830448150635\n",
      "Epoch 3999, Loss: 1.5537039637565613, Final Batch Loss: 0.32309964299201965\n",
      "Epoch 4000, Loss: 1.6019835472106934, Final Batch Loss: 0.3493110239505768\n",
      "Epoch 4001, Loss: 1.5819084346294403, Final Batch Loss: 0.336603045463562\n",
      "Epoch 4002, Loss: 1.6390732824802399, Final Batch Loss: 0.3935081362724304\n",
      "Epoch 4003, Loss: 1.5622508823871613, Final Batch Loss: 0.34757092595100403\n",
      "Epoch 4004, Loss: 1.4874303191900253, Final Batch Loss: 0.2403457909822464\n",
      "Epoch 4005, Loss: 1.67164945602417, Final Batch Loss: 0.3611412048339844\n",
      "Epoch 4006, Loss: 1.6835074722766876, Final Batch Loss: 0.43360215425491333\n",
      "Epoch 4007, Loss: 1.6507957875728607, Final Batch Loss: 0.3196401596069336\n",
      "Epoch 4008, Loss: 1.5973726212978363, Final Batch Loss: 0.251043438911438\n",
      "Epoch 4009, Loss: 1.6698533594608307, Final Batch Loss: 0.3125622570514679\n",
      "Epoch 4010, Loss: 1.5891586244106293, Final Batch Loss: 0.2966504991054535\n",
      "Epoch 4011, Loss: 1.6990899443626404, Final Batch Loss: 0.39504098892211914\n",
      "Epoch 4012, Loss: 1.570353239774704, Final Batch Loss: 0.341969758272171\n",
      "Epoch 4013, Loss: 1.6480674147605896, Final Batch Loss: 0.3757128417491913\n",
      "Epoch 4014, Loss: 1.5639120489358902, Final Batch Loss: 0.238188698887825\n",
      "Epoch 4015, Loss: 1.4672215580940247, Final Batch Loss: 0.3048189878463745\n",
      "Epoch 4016, Loss: 1.5619411766529083, Final Batch Loss: 0.2760154902935028\n",
      "Epoch 4017, Loss: 1.6835626363754272, Final Batch Loss: 0.36762720346450806\n",
      "Epoch 4018, Loss: 1.653705656528473, Final Batch Loss: 0.28987008333206177\n",
      "Epoch 4019, Loss: 1.980676293373108, Final Batch Loss: 0.45145314931869507\n",
      "Epoch 4020, Loss: 1.6105321794748306, Final Batch Loss: 0.23597483336925507\n",
      "Epoch 4021, Loss: 1.5584109127521515, Final Batch Loss: 0.2854969799518585\n",
      "Epoch 4022, Loss: 1.6605259776115417, Final Batch Loss: 0.27756938338279724\n",
      "Epoch 4023, Loss: 1.672160655260086, Final Batch Loss: 0.3100590407848358\n",
      "Epoch 4024, Loss: 1.6036642491817474, Final Batch Loss: 0.29139113426208496\n",
      "Epoch 4025, Loss: 1.5747640132904053, Final Batch Loss: 0.3846256136894226\n",
      "Epoch 4026, Loss: 1.5399949550628662, Final Batch Loss: 0.29095110297203064\n",
      "Epoch 4027, Loss: 1.51425901055336, Final Batch Loss: 0.24910911917686462\n",
      "Epoch 4028, Loss: 1.72450190782547, Final Batch Loss: 0.40123865008354187\n",
      "Epoch 4029, Loss: 1.5170469880104065, Final Batch Loss: 0.28513702750205994\n",
      "Epoch 4030, Loss: 1.6877343356609344, Final Batch Loss: 0.43498197197914124\n",
      "Epoch 4031, Loss: 1.5879876762628555, Final Batch Loss: 0.39940351247787476\n",
      "Epoch 4032, Loss: 1.582033857703209, Final Batch Loss: 0.24582163989543915\n",
      "Epoch 4033, Loss: 1.518908515572548, Final Batch Loss: 0.24618367850780487\n",
      "Epoch 4034, Loss: 1.6411911249160767, Final Batch Loss: 0.33731627464294434\n",
      "Epoch 4035, Loss: 1.619053065776825, Final Batch Loss: 0.26768866181373596\n",
      "Epoch 4036, Loss: 1.6440602540969849, Final Batch Loss: 0.385638952255249\n",
      "Epoch 4037, Loss: 1.5861110091209412, Final Batch Loss: 0.3235309422016144\n",
      "Epoch 4038, Loss: 1.4567366689443588, Final Batch Loss: 0.3224595785140991\n",
      "Epoch 4039, Loss: 1.5321191549301147, Final Batch Loss: 0.30336204171180725\n",
      "Epoch 4040, Loss: 1.6892156600952148, Final Batch Loss: 0.3047753870487213\n",
      "Epoch 4041, Loss: 1.520611196756363, Final Batch Loss: 0.18770721554756165\n",
      "Epoch 4042, Loss: 1.4817594587802887, Final Batch Loss: 0.22595658898353577\n",
      "Epoch 4043, Loss: 1.4681324660778046, Final Batch Loss: 0.246419757604599\n",
      "Epoch 4044, Loss: 1.5563506186008453, Final Batch Loss: 0.2553654909133911\n",
      "Epoch 4045, Loss: 1.7398070693016052, Final Batch Loss: 0.34880438446998596\n",
      "Epoch 4046, Loss: 1.6656787693500519, Final Batch Loss: 0.22349977493286133\n",
      "Epoch 4047, Loss: 1.51019287109375, Final Batch Loss: 0.2827123999595642\n",
      "Epoch 4048, Loss: 1.7226681113243103, Final Batch Loss: 0.36880171298980713\n",
      "Epoch 4049, Loss: 1.499757081270218, Final Batch Loss: 0.2844102382659912\n",
      "Epoch 4050, Loss: 1.4006760120391846, Final Batch Loss: 0.29213330149650574\n",
      "Epoch 4051, Loss: 1.4366027861833572, Final Batch Loss: 0.29137930274009705\n",
      "Epoch 4052, Loss: 1.601011335849762, Final Batch Loss: 0.4008557200431824\n",
      "Epoch 4053, Loss: 1.6149134635925293, Final Batch Loss: 0.4092505872249603\n",
      "Epoch 4054, Loss: 1.536224126815796, Final Batch Loss: 0.34236735105514526\n",
      "Epoch 4055, Loss: 1.5751891881227493, Final Batch Loss: 0.2480238825082779\n",
      "Epoch 4056, Loss: 1.6895872056484222, Final Batch Loss: 0.351481169462204\n",
      "Epoch 4057, Loss: 1.519053190946579, Final Batch Loss: 0.26485612988471985\n",
      "Epoch 4058, Loss: 1.5884253978729248, Final Batch Loss: 0.28527235984802246\n",
      "Epoch 4059, Loss: 1.582144170999527, Final Batch Loss: 0.25901469588279724\n",
      "Epoch 4060, Loss: 1.6151823103427887, Final Batch Loss: 0.25374555587768555\n",
      "Epoch 4061, Loss: 1.5061455368995667, Final Batch Loss: 0.31717509031295776\n",
      "Epoch 4062, Loss: 1.5442950129508972, Final Batch Loss: 0.2750949561595917\n",
      "Epoch 4063, Loss: 1.6226049959659576, Final Batch Loss: 0.3764345943927765\n",
      "Epoch 4064, Loss: 1.7114851474761963, Final Batch Loss: 0.3886259198188782\n",
      "Epoch 4065, Loss: 1.6512816548347473, Final Batch Loss: 0.37519532442092896\n",
      "Epoch 4066, Loss: 1.5248361676931381, Final Batch Loss: 0.3626549541950226\n",
      "Epoch 4067, Loss: 1.6178974211215973, Final Batch Loss: 0.2516428828239441\n",
      "Epoch 4068, Loss: 1.5705446898937225, Final Batch Loss: 0.30186721682548523\n",
      "Epoch 4069, Loss: 1.6318561434745789, Final Batch Loss: 0.34467533230781555\n",
      "Epoch 4070, Loss: 1.5553838908672333, Final Batch Loss: 0.2671176791191101\n",
      "Epoch 4071, Loss: 1.595273032784462, Final Batch Loss: 0.3905254900455475\n",
      "Epoch 4072, Loss: 1.6619406640529633, Final Batch Loss: 0.42300742864608765\n",
      "Epoch 4073, Loss: 1.5596313178539276, Final Batch Loss: 0.3337669372558594\n",
      "Epoch 4074, Loss: 1.5423967242240906, Final Batch Loss: 0.3562524914741516\n",
      "Epoch 4075, Loss: 1.4350972175598145, Final Batch Loss: 0.31388312578201294\n",
      "Epoch 4076, Loss: 1.4717722833156586, Final Batch Loss: 0.253620445728302\n",
      "Epoch 4077, Loss: 1.6111225336790085, Final Batch Loss: 0.30918434262275696\n",
      "Epoch 4078, Loss: 1.4950986951589584, Final Batch Loss: 0.28716132044792175\n",
      "Epoch 4079, Loss: 1.629553034901619, Final Batch Loss: 0.238935187458992\n",
      "Epoch 4080, Loss: 1.6598646938800812, Final Batch Loss: 0.34930419921875\n",
      "Epoch 4081, Loss: 1.4256319552659988, Final Batch Loss: 0.2806200087070465\n",
      "Epoch 4082, Loss: 1.5923079699277878, Final Batch Loss: 0.3806685507297516\n",
      "Epoch 4083, Loss: 1.7551024556159973, Final Batch Loss: 0.42507949471473694\n",
      "Epoch 4084, Loss: 1.5627807080745697, Final Batch Loss: 0.28773656487464905\n",
      "Epoch 4085, Loss: 1.5125343948602676, Final Batch Loss: 0.25005972385406494\n",
      "Epoch 4086, Loss: 1.5040477812290192, Final Batch Loss: 0.3229517936706543\n",
      "Epoch 4087, Loss: 1.5708061754703522, Final Batch Loss: 0.38865554332733154\n",
      "Epoch 4088, Loss: 1.6112878024578094, Final Batch Loss: 0.32919842004776\n",
      "Epoch 4089, Loss: 1.7451848685741425, Final Batch Loss: 0.30740636587142944\n",
      "Epoch 4090, Loss: 1.5833449065685272, Final Batch Loss: 0.324576735496521\n",
      "Epoch 4091, Loss: 1.6128890216350555, Final Batch Loss: 0.3320560157299042\n",
      "Epoch 4092, Loss: 1.52957084774971, Final Batch Loss: 0.33402398228645325\n",
      "Epoch 4093, Loss: 1.5753290951251984, Final Batch Loss: 0.3037639558315277\n",
      "Epoch 4094, Loss: 1.6350106298923492, Final Batch Loss: 0.2702544629573822\n",
      "Epoch 4095, Loss: 1.4960492551326752, Final Batch Loss: 0.26114413142204285\n",
      "Epoch 4096, Loss: 1.6123944222927094, Final Batch Loss: 0.3473597764968872\n",
      "Epoch 4097, Loss: 1.3951627314090729, Final Batch Loss: 0.3608369529247284\n",
      "Epoch 4098, Loss: 1.5857796669006348, Final Batch Loss: 0.40245354175567627\n",
      "Epoch 4099, Loss: 1.648749828338623, Final Batch Loss: 0.3280894160270691\n",
      "Epoch 4100, Loss: 1.6278841197490692, Final Batch Loss: 0.3408806622028351\n",
      "Epoch 4101, Loss: 1.560497522354126, Final Batch Loss: 0.2804046869277954\n",
      "Epoch 4102, Loss: 1.535147249698639, Final Batch Loss: 0.2716505229473114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4103, Loss: 1.6388662457466125, Final Batch Loss: 0.3080672323703766\n",
      "Epoch 4104, Loss: 1.555438369512558, Final Batch Loss: 0.3100751042366028\n",
      "Epoch 4105, Loss: 1.5599547624588013, Final Batch Loss: 0.2859090268611908\n",
      "Epoch 4106, Loss: 1.547389343380928, Final Batch Loss: 0.3282276690006256\n",
      "Epoch 4107, Loss: 1.562401831150055, Final Batch Loss: 0.2221427857875824\n",
      "Epoch 4108, Loss: 1.5775679051876068, Final Batch Loss: 0.3001042902469635\n",
      "Epoch 4109, Loss: 1.625179648399353, Final Batch Loss: 0.3360505700111389\n",
      "Epoch 4110, Loss: 1.5780083239078522, Final Batch Loss: 0.3440944254398346\n",
      "Epoch 4111, Loss: 1.645841896533966, Final Batch Loss: 0.49735987186431885\n",
      "Epoch 4112, Loss: 1.6036901772022247, Final Batch Loss: 0.33133664727211\n",
      "Epoch 4113, Loss: 1.651741236448288, Final Batch Loss: 0.3938038945198059\n",
      "Epoch 4114, Loss: 1.4651961624622345, Final Batch Loss: 0.26382651925086975\n",
      "Epoch 4115, Loss: 1.7308047115802765, Final Batch Loss: 0.33844366669654846\n",
      "Epoch 4116, Loss: 1.6359935104846954, Final Batch Loss: 0.3590911626815796\n",
      "Epoch 4117, Loss: 1.5076184570789337, Final Batch Loss: 0.28692498803138733\n",
      "Epoch 4118, Loss: 1.5742670744657516, Final Batch Loss: 0.41533559560775757\n",
      "Epoch 4119, Loss: 1.642579346895218, Final Batch Loss: 0.3503439426422119\n",
      "Epoch 4120, Loss: 1.4847813546657562, Final Batch Loss: 0.32122689485549927\n",
      "Epoch 4121, Loss: 1.5530124306678772, Final Batch Loss: 0.31063464283943176\n",
      "Epoch 4122, Loss: 1.6677754819393158, Final Batch Loss: 0.3672451972961426\n",
      "Epoch 4123, Loss: 1.5700255632400513, Final Batch Loss: 0.3088173568248749\n",
      "Epoch 4124, Loss: 1.428489238023758, Final Batch Loss: 0.3251609206199646\n",
      "Epoch 4125, Loss: 1.5963978320360184, Final Batch Loss: 0.3642158806324005\n",
      "Epoch 4126, Loss: 1.5721509158611298, Final Batch Loss: 0.26753947138786316\n",
      "Epoch 4127, Loss: 1.5361644178628922, Final Batch Loss: 0.2212420254945755\n",
      "Epoch 4128, Loss: 1.6413194239139557, Final Batch Loss: 0.31802698969841003\n",
      "Epoch 4129, Loss: 1.5762296319007874, Final Batch Loss: 0.21076902747154236\n",
      "Epoch 4130, Loss: 1.6088462471961975, Final Batch Loss: 0.38765138387680054\n",
      "Epoch 4131, Loss: 1.6749212145805359, Final Batch Loss: 0.31456083059310913\n",
      "Epoch 4132, Loss: 1.625656247138977, Final Batch Loss: 0.3817656338214874\n",
      "Epoch 4133, Loss: 1.607293039560318, Final Batch Loss: 0.31849533319473267\n",
      "Epoch 4134, Loss: 1.5889980494976044, Final Batch Loss: 0.29783955216407776\n",
      "Epoch 4135, Loss: 1.4684607982635498, Final Batch Loss: 0.2748747766017914\n",
      "Epoch 4136, Loss: 1.6117149591445923, Final Batch Loss: 0.24659478664398193\n",
      "Epoch 4137, Loss: 1.6380751132965088, Final Batch Loss: 0.32161107659339905\n",
      "Epoch 4138, Loss: 1.550955891609192, Final Batch Loss: 0.36687755584716797\n",
      "Epoch 4139, Loss: 1.412168264389038, Final Batch Loss: 0.27468568086624146\n",
      "Epoch 4140, Loss: 1.6927601397037506, Final Batch Loss: 0.39261311292648315\n",
      "Epoch 4141, Loss: 1.660244107246399, Final Batch Loss: 0.2629871964454651\n",
      "Epoch 4142, Loss: 1.5043215155601501, Final Batch Loss: 0.36516377329826355\n",
      "Epoch 4143, Loss: 1.4768647849559784, Final Batch Loss: 0.255564421415329\n",
      "Epoch 4144, Loss: 1.5435365736484528, Final Batch Loss: 0.25933587551116943\n",
      "Epoch 4145, Loss: 1.6022507548332214, Final Batch Loss: 0.3835432529449463\n",
      "Epoch 4146, Loss: 1.4687651693820953, Final Batch Loss: 0.31861868500709534\n",
      "Epoch 4147, Loss: 1.5889568626880646, Final Batch Loss: 0.31176498532295227\n",
      "Epoch 4148, Loss: 1.566189020872116, Final Batch Loss: 0.33728498220443726\n",
      "Epoch 4149, Loss: 1.513640210032463, Final Batch Loss: 0.23441414535045624\n",
      "Epoch 4150, Loss: 1.6824869513511658, Final Batch Loss: 0.34545648097991943\n",
      "Epoch 4151, Loss: 1.7789565920829773, Final Batch Loss: 0.4495614469051361\n",
      "Epoch 4152, Loss: 1.584678739309311, Final Batch Loss: 0.31277668476104736\n",
      "Epoch 4153, Loss: 1.434399053454399, Final Batch Loss: 0.29049479961395264\n",
      "Epoch 4154, Loss: 1.5471051633358002, Final Batch Loss: 0.32131022214889526\n",
      "Epoch 4155, Loss: 1.577722817659378, Final Batch Loss: 0.3278881013393402\n",
      "Epoch 4156, Loss: 1.5218375325202942, Final Batch Loss: 0.21825915575027466\n",
      "Epoch 4157, Loss: 1.5119537711143494, Final Batch Loss: 0.2788873314857483\n",
      "Epoch 4158, Loss: 1.5615180730819702, Final Batch Loss: 0.26815706491470337\n",
      "Epoch 4159, Loss: 1.674052894115448, Final Batch Loss: 0.29298725724220276\n",
      "Epoch 4160, Loss: 1.6987605690956116, Final Batch Loss: 0.3924620747566223\n",
      "Epoch 4161, Loss: 1.4830538928508759, Final Batch Loss: 0.2917335629463196\n",
      "Epoch 4162, Loss: 1.5763082802295685, Final Batch Loss: 0.31894806027412415\n",
      "Epoch 4163, Loss: 1.5497825145721436, Final Batch Loss: 0.3075084090232849\n",
      "Epoch 4164, Loss: 1.710003525018692, Final Batch Loss: 0.3500499725341797\n",
      "Epoch 4165, Loss: 1.5342276692390442, Final Batch Loss: 0.25324374437332153\n",
      "Epoch 4166, Loss: 1.6530359387397766, Final Batch Loss: 0.35764575004577637\n",
      "Epoch 4167, Loss: 1.4518475532531738, Final Batch Loss: 0.2965107858181\n",
      "Epoch 4168, Loss: 1.5411073863506317, Final Batch Loss: 0.26342737674713135\n",
      "Epoch 4169, Loss: 1.733382672071457, Final Batch Loss: 0.46746522188186646\n",
      "Epoch 4170, Loss: 1.5554827749729156, Final Batch Loss: 0.25412240624427795\n",
      "Epoch 4171, Loss: 1.6134771406650543, Final Batch Loss: 0.318418949842453\n",
      "Epoch 4172, Loss: 1.5174060463905334, Final Batch Loss: 0.2693624496459961\n",
      "Epoch 4173, Loss: 1.5858899354934692, Final Batch Loss: 0.36022835969924927\n",
      "Epoch 4174, Loss: 1.6241836249828339, Final Batch Loss: 0.3236035108566284\n",
      "Epoch 4175, Loss: 1.5855022370815277, Final Batch Loss: 0.2934347689151764\n",
      "Epoch 4176, Loss: 1.6071378290653229, Final Batch Loss: 0.3138643205165863\n",
      "Epoch 4177, Loss: 1.5296704918146133, Final Batch Loss: 0.43437859416007996\n",
      "Epoch 4178, Loss: 1.6019670367240906, Final Batch Loss: 0.32445618510246277\n",
      "Epoch 4179, Loss: 1.642522007226944, Final Batch Loss: 0.33088406920433044\n",
      "Epoch 4180, Loss: 1.566589117050171, Final Batch Loss: 0.26872479915618896\n",
      "Epoch 4181, Loss: 1.4612041860818863, Final Batch Loss: 0.24268324673175812\n",
      "Epoch 4182, Loss: 1.7295000553131104, Final Batch Loss: 0.2735188603401184\n",
      "Epoch 4183, Loss: 1.6303688883781433, Final Batch Loss: 0.2751443684101105\n",
      "Epoch 4184, Loss: 1.676425814628601, Final Batch Loss: 0.3228380084037781\n",
      "Epoch 4185, Loss: 1.7006109356880188, Final Batch Loss: 0.37566617131233215\n",
      "Epoch 4186, Loss: 1.487213522195816, Final Batch Loss: 0.27617108821868896\n",
      "Epoch 4187, Loss: 1.5937259197235107, Final Batch Loss: 0.25806745886802673\n",
      "Epoch 4188, Loss: 1.6553781628608704, Final Batch Loss: 0.3219413161277771\n",
      "Epoch 4189, Loss: 1.5881372392177582, Final Batch Loss: 0.24512499570846558\n",
      "Epoch 4190, Loss: 1.4482027888298035, Final Batch Loss: 0.29807502031326294\n",
      "Epoch 4191, Loss: 1.6004183888435364, Final Batch Loss: 0.3410222828388214\n",
      "Epoch 4192, Loss: 1.5205038487911224, Final Batch Loss: 0.2975251376628876\n",
      "Epoch 4193, Loss: 1.4937500655651093, Final Batch Loss: 0.3399181663990021\n",
      "Epoch 4194, Loss: 1.588531732559204, Final Batch Loss: 0.3415549397468567\n",
      "Epoch 4195, Loss: 1.5822699666023254, Final Batch Loss: 0.4007773697376251\n",
      "Epoch 4196, Loss: 1.5148282647132874, Final Batch Loss: 0.24967420101165771\n",
      "Epoch 4197, Loss: 1.5887061953544617, Final Batch Loss: 0.2761291265487671\n",
      "Epoch 4198, Loss: 1.5499965250492096, Final Batch Loss: 0.3281043469905853\n",
      "Epoch 4199, Loss: 1.5691740661859512, Final Batch Loss: 0.4678368866443634\n",
      "Epoch 4200, Loss: 1.5950040817260742, Final Batch Loss: 0.39599552750587463\n",
      "Epoch 4201, Loss: 1.5302402824163437, Final Batch Loss: 0.20663399994373322\n",
      "Epoch 4202, Loss: 1.6985467076301575, Final Batch Loss: 0.3482663631439209\n",
      "Epoch 4203, Loss: 1.5490398108959198, Final Batch Loss: 0.2779051661491394\n",
      "Epoch 4204, Loss: 1.6428641080856323, Final Batch Loss: 0.2625899612903595\n",
      "Epoch 4205, Loss: 1.6137270629405975, Final Batch Loss: 0.3029804229736328\n",
      "Epoch 4206, Loss: 1.5290985107421875, Final Batch Loss: 0.3043937385082245\n",
      "Epoch 4207, Loss: 1.4847418069839478, Final Batch Loss: 0.32761940360069275\n",
      "Epoch 4208, Loss: 1.5708727836608887, Final Batch Loss: 0.2867041230201721\n",
      "Epoch 4209, Loss: 1.7072051763534546, Final Batch Loss: 0.4480845630168915\n",
      "Epoch 4210, Loss: 1.6802745461463928, Final Batch Loss: 0.320186048746109\n",
      "Epoch 4211, Loss: 1.4670184552669525, Final Batch Loss: 0.26359817385673523\n",
      "Epoch 4212, Loss: 1.5607308149337769, Final Batch Loss: 0.2926742434501648\n",
      "Epoch 4213, Loss: 1.7527526319026947, Final Batch Loss: 0.30069002509117126\n",
      "Epoch 4214, Loss: 1.6786349713802338, Final Batch Loss: 0.40452367067337036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4215, Loss: 1.6104565858840942, Final Batch Loss: 0.3135000169277191\n",
      "Epoch 4216, Loss: 1.5415964126586914, Final Batch Loss: 0.2612016201019287\n",
      "Epoch 4217, Loss: 1.5416976511478424, Final Batch Loss: 0.2908226549625397\n",
      "Epoch 4218, Loss: 1.4502519071102142, Final Batch Loss: 0.25039952993392944\n",
      "Epoch 4219, Loss: 1.758113145828247, Final Batch Loss: 0.3761826753616333\n",
      "Epoch 4220, Loss: 1.568777710199356, Final Batch Loss: 0.41479140520095825\n",
      "Epoch 4221, Loss: 1.4907592982053757, Final Batch Loss: 0.23275543749332428\n",
      "Epoch 4222, Loss: 1.6943023800849915, Final Batch Loss: 0.34733179211616516\n",
      "Epoch 4223, Loss: 1.6629145741462708, Final Batch Loss: 0.3022019863128662\n",
      "Epoch 4224, Loss: 1.590882658958435, Final Batch Loss: 0.33476653695106506\n",
      "Epoch 4225, Loss: 1.6257632076740265, Final Batch Loss: 0.34801292419433594\n",
      "Epoch 4226, Loss: 1.6340498626232147, Final Batch Loss: 0.3636086881160736\n",
      "Epoch 4227, Loss: 1.607483297586441, Final Batch Loss: 0.37933236360549927\n",
      "Epoch 4228, Loss: 1.6847281455993652, Final Batch Loss: 0.26364654302597046\n",
      "Epoch 4229, Loss: 1.5387876480817795, Final Batch Loss: 0.30544307827949524\n",
      "Epoch 4230, Loss: 1.625624880194664, Final Batch Loss: 0.24178911745548248\n",
      "Epoch 4231, Loss: 1.6777276992797852, Final Batch Loss: 0.4038310647010803\n",
      "Epoch 4232, Loss: 1.6211170256137848, Final Batch Loss: 0.381680428981781\n",
      "Epoch 4233, Loss: 1.5407356023788452, Final Batch Loss: 0.28898635506629944\n",
      "Epoch 4234, Loss: 1.569800317287445, Final Batch Loss: 0.24354714155197144\n",
      "Epoch 4235, Loss: 1.425446256995201, Final Batch Loss: 0.2491558939218521\n",
      "Epoch 4236, Loss: 1.5343419164419174, Final Batch Loss: 0.2819124162197113\n",
      "Epoch 4237, Loss: 1.5398188829421997, Final Batch Loss: 0.2786381244659424\n",
      "Epoch 4238, Loss: 1.5492339134216309, Final Batch Loss: 0.28674352169036865\n",
      "Epoch 4239, Loss: 1.7143456041812897, Final Batch Loss: 0.3393647372722626\n",
      "Epoch 4240, Loss: 1.4295173734426498, Final Batch Loss: 0.3801402449607849\n",
      "Epoch 4241, Loss: 1.4825225174427032, Final Batch Loss: 0.3355250954627991\n",
      "Epoch 4242, Loss: 1.5271302461624146, Final Batch Loss: 0.2560289204120636\n",
      "Epoch 4243, Loss: 1.5364319682121277, Final Batch Loss: 0.21814781427383423\n",
      "Epoch 4244, Loss: 1.570101946592331, Final Batch Loss: 0.34929490089416504\n",
      "Epoch 4245, Loss: 1.5629274994134903, Final Batch Loss: 0.4156687259674072\n",
      "Epoch 4246, Loss: 1.6462450921535492, Final Batch Loss: 0.38674813508987427\n",
      "Epoch 4247, Loss: 1.5911560654640198, Final Batch Loss: 0.24193412065505981\n",
      "Epoch 4248, Loss: 1.4395388662815094, Final Batch Loss: 0.33690497279167175\n",
      "Epoch 4249, Loss: 1.7368907928466797, Final Batch Loss: 0.507542610168457\n",
      "Epoch 4250, Loss: 1.7319272756576538, Final Batch Loss: 0.3428729176521301\n",
      "Epoch 4251, Loss: 1.534818023443222, Final Batch Loss: 0.22548896074295044\n",
      "Epoch 4252, Loss: 1.50334133207798, Final Batch Loss: 0.22705332934856415\n",
      "Epoch 4253, Loss: 1.5698500722646713, Final Batch Loss: 0.3217550218105316\n",
      "Epoch 4254, Loss: 1.5569965541362762, Final Batch Loss: 0.33169734477996826\n",
      "Epoch 4255, Loss: 1.5148786902427673, Final Batch Loss: 0.2755662798881531\n",
      "Epoch 4256, Loss: 1.4817659258842468, Final Batch Loss: 0.30783146619796753\n",
      "Epoch 4257, Loss: 1.4997444152832031, Final Batch Loss: 0.2838685214519501\n",
      "Epoch 4258, Loss: 1.5507103502750397, Final Batch Loss: 0.34074166417121887\n",
      "Epoch 4259, Loss: 1.6436000615358353, Final Batch Loss: 0.2378799170255661\n",
      "Epoch 4260, Loss: 1.6279552578926086, Final Batch Loss: 0.34636732935905457\n",
      "Epoch 4261, Loss: 1.609486848115921, Final Batch Loss: 0.3261176347732544\n",
      "Epoch 4262, Loss: 1.6449746191501617, Final Batch Loss: 0.2804623544216156\n",
      "Epoch 4263, Loss: 1.5725743472576141, Final Batch Loss: 0.3383008539676666\n",
      "Epoch 4264, Loss: 1.4763414859771729, Final Batch Loss: 0.31315991282463074\n",
      "Epoch 4265, Loss: 1.6579095423221588, Final Batch Loss: 0.4096030592918396\n",
      "Epoch 4266, Loss: 1.659628301858902, Final Batch Loss: 0.4073266088962555\n",
      "Epoch 4267, Loss: 1.5631240010261536, Final Batch Loss: 0.3780272603034973\n",
      "Epoch 4268, Loss: 1.5130281299352646, Final Batch Loss: 0.2028421312570572\n",
      "Epoch 4269, Loss: 1.6612863540649414, Final Batch Loss: 0.27634552121162415\n",
      "Epoch 4270, Loss: 1.5385619401931763, Final Batch Loss: 0.3204125165939331\n",
      "Epoch 4271, Loss: 1.6292982995510101, Final Batch Loss: 0.3160659074783325\n",
      "Epoch 4272, Loss: 1.4643856137990952, Final Batch Loss: 0.20000766217708588\n",
      "Epoch 4273, Loss: 1.6305204033851624, Final Batch Loss: 0.2647918164730072\n",
      "Epoch 4274, Loss: 1.6456551849842072, Final Batch Loss: 0.3026875853538513\n",
      "Epoch 4275, Loss: 1.5895119905471802, Final Batch Loss: 0.3377988636493683\n",
      "Epoch 4276, Loss: 1.5158314406871796, Final Batch Loss: 0.23358532786369324\n",
      "Epoch 4277, Loss: 1.5251656472682953, Final Batch Loss: 0.3528400957584381\n",
      "Epoch 4278, Loss: 1.4091996103525162, Final Batch Loss: 0.29243794083595276\n",
      "Epoch 4279, Loss: 1.519442617893219, Final Batch Loss: 0.2579769790172577\n",
      "Epoch 4280, Loss: 1.4828417897224426, Final Batch Loss: 0.2519461214542389\n",
      "Epoch 4281, Loss: 1.5086177587509155, Final Batch Loss: 0.3087911307811737\n",
      "Epoch 4282, Loss: 1.4552951753139496, Final Batch Loss: 0.23672917485237122\n",
      "Epoch 4283, Loss: 1.4084215462207794, Final Batch Loss: 0.25323250889778137\n",
      "Epoch 4284, Loss: 1.5235454738140106, Final Batch Loss: 0.2970218062400818\n",
      "Epoch 4285, Loss: 1.4646508395671844, Final Batch Loss: 0.2638467848300934\n",
      "Epoch 4286, Loss: 1.4070141911506653, Final Batch Loss: 0.29575395584106445\n",
      "Epoch 4287, Loss: 1.3787037432193756, Final Batch Loss: 0.2534445524215698\n",
      "Epoch 4288, Loss: 1.5151031762361526, Final Batch Loss: 0.2283063381910324\n",
      "Epoch 4289, Loss: 1.5099289566278458, Final Batch Loss: 0.23550690710544586\n",
      "Epoch 4290, Loss: 1.5250065326690674, Final Batch Loss: 0.2995521128177643\n",
      "Epoch 4291, Loss: 1.6142579913139343, Final Batch Loss: 0.3740093410015106\n",
      "Epoch 4292, Loss: 1.6349700093269348, Final Batch Loss: 0.34325334429740906\n",
      "Epoch 4293, Loss: 1.6202634274959564, Final Batch Loss: 0.2990005314350128\n",
      "Epoch 4294, Loss: 1.5813233256340027, Final Batch Loss: 0.32889094948768616\n",
      "Epoch 4295, Loss: 1.7019038796424866, Final Batch Loss: 0.347585529088974\n",
      "Epoch 4296, Loss: 1.641120657324791, Final Batch Loss: 0.3629756569862366\n",
      "Epoch 4297, Loss: 1.5640147030353546, Final Batch Loss: 0.30149421095848083\n",
      "Epoch 4298, Loss: 1.500623345375061, Final Batch Loss: 0.2377690076828003\n",
      "Epoch 4299, Loss: 1.534766137599945, Final Batch Loss: 0.20115140080451965\n",
      "Epoch 4300, Loss: 1.4414424002170563, Final Batch Loss: 0.29998674988746643\n",
      "Epoch 4301, Loss: 1.628420740365982, Final Batch Loss: 0.38405197858810425\n",
      "Epoch 4302, Loss: 1.4925936609506607, Final Batch Loss: 0.29453524947166443\n",
      "Epoch 4303, Loss: 1.6000473201274872, Final Batch Loss: 0.30793675780296326\n",
      "Epoch 4304, Loss: 1.5528314113616943, Final Batch Loss: 0.27507907152175903\n",
      "Epoch 4305, Loss: 1.583401381969452, Final Batch Loss: 0.3564036786556244\n",
      "Epoch 4306, Loss: 1.5260099172592163, Final Batch Loss: 0.33851784467697144\n",
      "Epoch 4307, Loss: 1.5637371838092804, Final Batch Loss: 0.30850836634635925\n",
      "Epoch 4308, Loss: 1.5656806975603104, Final Batch Loss: 0.22811473906040192\n",
      "Epoch 4309, Loss: 1.5753662288188934, Final Batch Loss: 0.2944851219654083\n",
      "Epoch 4310, Loss: 1.4948925971984863, Final Batch Loss: 0.3038685619831085\n",
      "Epoch 4311, Loss: 1.5824562013149261, Final Batch Loss: 0.35287031531333923\n",
      "Epoch 4312, Loss: 1.4702323377132416, Final Batch Loss: 0.25084540247917175\n",
      "Epoch 4313, Loss: 1.5966424643993378, Final Batch Loss: 0.3207094073295593\n",
      "Epoch 4314, Loss: 1.4020142406225204, Final Batch Loss: 0.24386678636074066\n",
      "Epoch 4315, Loss: 1.6213694214820862, Final Batch Loss: 0.34712931513786316\n",
      "Epoch 4316, Loss: 1.6182913482189178, Final Batch Loss: 0.29598256945610046\n",
      "Epoch 4317, Loss: 1.6325080394744873, Final Batch Loss: 0.41372668743133545\n",
      "Epoch 4318, Loss: 1.432517722249031, Final Batch Loss: 0.23657019436359406\n",
      "Epoch 4319, Loss: 1.5564564168453217, Final Batch Loss: 0.3757306933403015\n",
      "Epoch 4320, Loss: 1.4636766910552979, Final Batch Loss: 0.24927052855491638\n",
      "Epoch 4321, Loss: 1.5890810191631317, Final Batch Loss: 0.30759957432746887\n",
      "Epoch 4322, Loss: 1.53409942984581, Final Batch Loss: 0.3066849410533905\n",
      "Epoch 4323, Loss: 1.6896664798259735, Final Batch Loss: 0.36177873611450195\n",
      "Epoch 4324, Loss: 1.5109464824199677, Final Batch Loss: 0.32236766815185547\n",
      "Epoch 4325, Loss: 1.5948493778705597, Final Batch Loss: 0.40185582637786865\n",
      "Epoch 4326, Loss: 1.6490910351276398, Final Batch Loss: 0.30692100524902344\n",
      "Epoch 4327, Loss: 1.5570530891418457, Final Batch Loss: 0.3245322108268738\n",
      "Epoch 4328, Loss: 1.448434054851532, Final Batch Loss: 0.2666962146759033\n",
      "Epoch 4329, Loss: 1.4916830360889435, Final Batch Loss: 0.2747775614261627\n",
      "Epoch 4330, Loss: 1.4548256695270538, Final Batch Loss: 0.2749398946762085\n",
      "Epoch 4331, Loss: 1.4876671731472015, Final Batch Loss: 0.25465741753578186\n",
      "Epoch 4332, Loss: 1.5429351329803467, Final Batch Loss: 0.30467891693115234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4333, Loss: 1.538008838891983, Final Batch Loss: 0.2289128601551056\n",
      "Epoch 4334, Loss: 1.5355258584022522, Final Batch Loss: 0.3067739009857178\n",
      "Epoch 4335, Loss: 1.7293224036693573, Final Batch Loss: 0.29334092140197754\n",
      "Epoch 4336, Loss: 1.5454411208629608, Final Batch Loss: 0.34363827109336853\n",
      "Epoch 4337, Loss: 1.6469912230968475, Final Batch Loss: 0.34493717551231384\n",
      "Epoch 4338, Loss: 1.542087882757187, Final Batch Loss: 0.2995111346244812\n",
      "Epoch 4339, Loss: 1.7005720734596252, Final Batch Loss: 0.3803667724132538\n",
      "Epoch 4340, Loss: 1.6774545758962631, Final Batch Loss: 0.39027896523475647\n",
      "Epoch 4341, Loss: 1.4667755961418152, Final Batch Loss: 0.25707677006721497\n",
      "Epoch 4342, Loss: 1.548860251903534, Final Batch Loss: 0.3548881411552429\n",
      "Epoch 4343, Loss: 1.4888352155685425, Final Batch Loss: 0.31583255529403687\n",
      "Epoch 4344, Loss: 1.577609732747078, Final Batch Loss: 0.47719430923461914\n",
      "Epoch 4345, Loss: 1.5143021494150162, Final Batch Loss: 0.2474784106016159\n",
      "Epoch 4346, Loss: 1.5385140925645828, Final Batch Loss: 0.36822959780693054\n",
      "Epoch 4347, Loss: 1.551020473241806, Final Batch Loss: 0.2815631628036499\n",
      "Epoch 4348, Loss: 1.5691134631633759, Final Batch Loss: 0.312911719083786\n",
      "Epoch 4349, Loss: 1.5407319962978363, Final Batch Loss: 0.2973461449146271\n",
      "Epoch 4350, Loss: 1.6085841357707977, Final Batch Loss: 0.32879766821861267\n",
      "Epoch 4351, Loss: 1.605378270149231, Final Batch Loss: 0.2936340570449829\n",
      "Epoch 4352, Loss: 1.4571904093027115, Final Batch Loss: 0.2329546958208084\n",
      "Epoch 4353, Loss: 1.6013530492782593, Final Batch Loss: 0.31607604026794434\n",
      "Epoch 4354, Loss: 1.6624994575977325, Final Batch Loss: 0.33842191100120544\n",
      "Epoch 4355, Loss: 1.411343976855278, Final Batch Loss: 0.3610342741012573\n",
      "Epoch 4356, Loss: 1.5612245202064514, Final Batch Loss: 0.3245750665664673\n",
      "Epoch 4357, Loss: 1.3841995149850845, Final Batch Loss: 0.2660696506500244\n",
      "Epoch 4358, Loss: 1.6131068170070648, Final Batch Loss: 0.3042714297771454\n",
      "Epoch 4359, Loss: 1.4559304118156433, Final Batch Loss: 0.2684730291366577\n",
      "Epoch 4360, Loss: 1.6457318514585495, Final Batch Loss: 0.49921301007270813\n",
      "Epoch 4361, Loss: 1.549278050661087, Final Batch Loss: 0.35728874802589417\n",
      "Epoch 4362, Loss: 1.5985415875911713, Final Batch Loss: 0.3630375564098358\n",
      "Epoch 4363, Loss: 1.424013689160347, Final Batch Loss: 0.28162550926208496\n",
      "Epoch 4364, Loss: 1.5114883482456207, Final Batch Loss: 0.3333553373813629\n",
      "Epoch 4365, Loss: 1.6924138367176056, Final Batch Loss: 0.41965141892433167\n",
      "Epoch 4366, Loss: 1.4906502664089203, Final Batch Loss: 0.2904302775859833\n",
      "Epoch 4367, Loss: 1.6365296840667725, Final Batch Loss: 0.38495519757270813\n",
      "Epoch 4368, Loss: 1.5730867683887482, Final Batch Loss: 0.39753755927085876\n",
      "Epoch 4369, Loss: 1.43157097697258, Final Batch Loss: 0.2729688584804535\n",
      "Epoch 4370, Loss: 1.572733849287033, Final Batch Loss: 0.315569669008255\n",
      "Epoch 4371, Loss: 1.5924448370933533, Final Batch Loss: 0.2758828103542328\n",
      "Epoch 4372, Loss: 1.407058447599411, Final Batch Loss: 0.2719299793243408\n",
      "Epoch 4373, Loss: 1.4781276136636734, Final Batch Loss: 0.3303747773170471\n",
      "Epoch 4374, Loss: 1.5783672779798508, Final Batch Loss: 0.39370444416999817\n",
      "Epoch 4375, Loss: 1.4344242066144943, Final Batch Loss: 0.2404768317937851\n",
      "Epoch 4376, Loss: 1.5418472737073898, Final Batch Loss: 0.2914041578769684\n",
      "Epoch 4377, Loss: 1.4320831000804901, Final Batch Loss: 0.282166063785553\n",
      "Epoch 4378, Loss: 1.5696936249732971, Final Batch Loss: 0.2669513523578644\n",
      "Epoch 4379, Loss: 1.4081721156835556, Final Batch Loss: 0.24466350674629211\n",
      "Epoch 4380, Loss: 1.6556717604398727, Final Batch Loss: 0.4209569990634918\n",
      "Epoch 4381, Loss: 1.6310291588306427, Final Batch Loss: 0.41228964924812317\n",
      "Epoch 4382, Loss: 1.5202661156654358, Final Batch Loss: 0.2997572422027588\n",
      "Epoch 4383, Loss: 1.5357069969177246, Final Batch Loss: 0.3068465292453766\n",
      "Epoch 4384, Loss: 1.5016111731529236, Final Batch Loss: 0.3310224711894989\n",
      "Epoch 4385, Loss: 1.5435574054718018, Final Batch Loss: 0.3019965887069702\n",
      "Epoch 4386, Loss: 1.5031044781208038, Final Batch Loss: 0.2586633563041687\n",
      "Epoch 4387, Loss: 1.4981743395328522, Final Batch Loss: 0.3165803551673889\n",
      "Epoch 4388, Loss: 1.6688703000545502, Final Batch Loss: 0.27030885219573975\n",
      "Epoch 4389, Loss: 1.5686754286289215, Final Batch Loss: 0.3000169098377228\n",
      "Epoch 4390, Loss: 1.4841034263372421, Final Batch Loss: 0.2688451409339905\n",
      "Epoch 4391, Loss: 1.5322388708591461, Final Batch Loss: 0.27885109186172485\n",
      "Epoch 4392, Loss: 1.5142067968845367, Final Batch Loss: 0.28214722871780396\n",
      "Epoch 4393, Loss: 1.6108198165893555, Final Batch Loss: 0.32241079211235046\n",
      "Epoch 4394, Loss: 1.6085334718227386, Final Batch Loss: 0.3316468596458435\n",
      "Epoch 4395, Loss: 1.8921119570732117, Final Batch Loss: 0.5809265375137329\n",
      "Epoch 4396, Loss: 1.575826272368431, Final Batch Loss: 0.47368931770324707\n",
      "Epoch 4397, Loss: 1.6862202882766724, Final Batch Loss: 0.2735706567764282\n",
      "Epoch 4398, Loss: 1.6990193724632263, Final Batch Loss: 0.4269789457321167\n",
      "Epoch 4399, Loss: 1.4535460770130157, Final Batch Loss: 0.28342416882514954\n",
      "Epoch 4400, Loss: 1.5345531105995178, Final Batch Loss: 0.26927709579467773\n",
      "Epoch 4401, Loss: 1.4962575733661652, Final Batch Loss: 0.34698590636253357\n",
      "Epoch 4402, Loss: 1.6109336912631989, Final Batch Loss: 0.3383048474788666\n",
      "Epoch 4403, Loss: 1.486696794629097, Final Batch Loss: 0.2564898729324341\n",
      "Epoch 4404, Loss: 1.5598732233047485, Final Batch Loss: 0.31800249218940735\n",
      "Epoch 4405, Loss: 1.599036067724228, Final Batch Loss: 0.34936678409576416\n",
      "Epoch 4406, Loss: 1.505819320678711, Final Batch Loss: 0.34356608986854553\n",
      "Epoch 4407, Loss: 1.5661399364471436, Final Batch Loss: 0.35305505990982056\n",
      "Epoch 4408, Loss: 1.5076561570167542, Final Batch Loss: 0.2588575482368469\n",
      "Epoch 4409, Loss: 1.4702107459306717, Final Batch Loss: 0.2789864242076874\n",
      "Epoch 4410, Loss: 1.6943203508853912, Final Batch Loss: 0.35456719994544983\n",
      "Epoch 4411, Loss: 1.5528661906719208, Final Batch Loss: 0.3195250332355499\n",
      "Epoch 4412, Loss: 1.5418949574232101, Final Batch Loss: 0.23820705711841583\n",
      "Epoch 4413, Loss: 1.6443968415260315, Final Batch Loss: 0.3540342450141907\n",
      "Epoch 4414, Loss: 1.5064746141433716, Final Batch Loss: 0.3624957501888275\n",
      "Epoch 4415, Loss: 1.7784432470798492, Final Batch Loss: 0.31690216064453125\n",
      "Epoch 4416, Loss: 1.5186512768268585, Final Batch Loss: 0.3681561052799225\n",
      "Epoch 4417, Loss: 1.5300040245056152, Final Batch Loss: 0.34060701727867126\n",
      "Epoch 4418, Loss: 1.460631713271141, Final Batch Loss: 0.2913316786289215\n",
      "Epoch 4419, Loss: 1.6138920187950134, Final Batch Loss: 0.3496003746986389\n",
      "Epoch 4420, Loss: 1.549456626176834, Final Batch Loss: 0.30522480607032776\n",
      "Epoch 4421, Loss: 1.588054895401001, Final Batch Loss: 0.34626826643943787\n",
      "Epoch 4422, Loss: 1.555439829826355, Final Batch Loss: 0.3045962452888489\n",
      "Epoch 4423, Loss: 1.5100562870502472, Final Batch Loss: 0.2999466061592102\n",
      "Epoch 4424, Loss: 1.4417116940021515, Final Batch Loss: 0.22865569591522217\n",
      "Epoch 4425, Loss: 1.6401512324810028, Final Batch Loss: 0.2815561890602112\n",
      "Epoch 4426, Loss: 1.4270626455545425, Final Batch Loss: 0.2736400067806244\n",
      "Epoch 4427, Loss: 1.4797888398170471, Final Batch Loss: 0.2250058948993683\n",
      "Epoch 4428, Loss: 1.58539317548275, Final Batch Loss: 0.39886078238487244\n",
      "Epoch 4429, Loss: 1.524696171283722, Final Batch Loss: 0.29659730195999146\n",
      "Epoch 4430, Loss: 1.6235871016979218, Final Batch Loss: 0.3703448474407196\n",
      "Epoch 4431, Loss: 1.5174027681350708, Final Batch Loss: 0.2859324812889099\n",
      "Epoch 4432, Loss: 1.5488573610782623, Final Batch Loss: 0.2322898656129837\n",
      "Epoch 4433, Loss: 1.5354884564876556, Final Batch Loss: 0.29779303073883057\n",
      "Epoch 4434, Loss: 1.5192433893680573, Final Batch Loss: 0.37756040692329407\n",
      "Epoch 4435, Loss: 1.559419959783554, Final Batch Loss: 0.2951716482639313\n",
      "Epoch 4436, Loss: 1.4480148404836655, Final Batch Loss: 0.21559379994869232\n",
      "Epoch 4437, Loss: 1.4845829159021378, Final Batch Loss: 0.27649033069610596\n",
      "Epoch 4438, Loss: 1.5647050142288208, Final Batch Loss: 0.274291455745697\n",
      "Epoch 4439, Loss: 1.4518214017152786, Final Batch Loss: 0.19038845598697662\n",
      "Epoch 4440, Loss: 1.624483436346054, Final Batch Loss: 0.30563339591026306\n",
      "Epoch 4441, Loss: 1.52955262362957, Final Batch Loss: 0.23315350711345673\n",
      "Epoch 4442, Loss: 1.4513239711523056, Final Batch Loss: 0.2314336895942688\n",
      "Epoch 4443, Loss: 1.680831104516983, Final Batch Loss: 0.36924803256988525\n",
      "Epoch 4444, Loss: 1.4608439803123474, Final Batch Loss: 0.27510619163513184\n",
      "Epoch 4445, Loss: 1.5989830493927002, Final Batch Loss: 0.3548955023288727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4446, Loss: 1.4745599627494812, Final Batch Loss: 0.3279169201850891\n",
      "Epoch 4447, Loss: 1.5417613089084625, Final Batch Loss: 0.2750065326690674\n",
      "Epoch 4448, Loss: 1.5227485001087189, Final Batch Loss: 0.2934340238571167\n",
      "Epoch 4449, Loss: 1.484318733215332, Final Batch Loss: 0.2680252492427826\n",
      "Epoch 4450, Loss: 1.5020960867404938, Final Batch Loss: 0.33509084582328796\n",
      "Epoch 4451, Loss: 1.5281532108783722, Final Batch Loss: 0.39329230785369873\n",
      "Epoch 4452, Loss: 1.4947488754987717, Final Batch Loss: 0.2363169938325882\n",
      "Epoch 4453, Loss: 1.5692496001720428, Final Batch Loss: 0.2651326358318329\n",
      "Epoch 4454, Loss: 1.5132962763309479, Final Batch Loss: 0.2605228126049042\n",
      "Epoch 4455, Loss: 1.5006347447633743, Final Batch Loss: 0.19032855331897736\n",
      "Epoch 4456, Loss: 1.5182502269744873, Final Batch Loss: 0.20278409123420715\n",
      "Epoch 4457, Loss: 1.4991008043289185, Final Batch Loss: 0.255262166261673\n",
      "Epoch 4458, Loss: 1.749768853187561, Final Batch Loss: 0.2694774866104126\n",
      "Epoch 4459, Loss: 1.5271576046943665, Final Batch Loss: 0.35765984654426575\n",
      "Epoch 4460, Loss: 1.4706736207008362, Final Batch Loss: 0.2686607837677002\n",
      "Epoch 4461, Loss: 1.5909987688064575, Final Batch Loss: 0.31921541690826416\n",
      "Epoch 4462, Loss: 1.6576588451862335, Final Batch Loss: 0.3755275011062622\n",
      "Epoch 4463, Loss: 1.5504190325737, Final Batch Loss: 0.3752440810203552\n",
      "Epoch 4464, Loss: 1.4521033316850662, Final Batch Loss: 0.3000175356864929\n",
      "Epoch 4465, Loss: 1.6575466394424438, Final Batch Loss: 0.2606183588504791\n",
      "Epoch 4466, Loss: 1.4714142680168152, Final Batch Loss: 0.3571559488773346\n",
      "Epoch 4467, Loss: 1.4717959016561508, Final Batch Loss: 0.23755817115306854\n",
      "Epoch 4468, Loss: 1.3834095150232315, Final Batch Loss: 0.27656233310699463\n",
      "Epoch 4469, Loss: 1.4840403497219086, Final Batch Loss: 0.20678144693374634\n",
      "Epoch 4470, Loss: 1.4896898567676544, Final Batch Loss: 0.29485592246055603\n",
      "Epoch 4471, Loss: 1.4368817210197449, Final Batch Loss: 0.2822951376438141\n",
      "Epoch 4472, Loss: 1.49476857483387, Final Batch Loss: 0.31518426537513733\n",
      "Epoch 4473, Loss: 1.4514466673135757, Final Batch Loss: 0.1978190392255783\n",
      "Epoch 4474, Loss: 1.4926314502954483, Final Batch Loss: 0.21890677511692047\n",
      "Epoch 4475, Loss: 1.5191570967435837, Final Batch Loss: 0.32744336128234863\n",
      "Epoch 4476, Loss: 1.4709516316652298, Final Batch Loss: 0.2934112250804901\n",
      "Epoch 4477, Loss: 1.5212672352790833, Final Batch Loss: 0.3514143228530884\n",
      "Epoch 4478, Loss: 1.5593991875648499, Final Batch Loss: 0.26312097907066345\n",
      "Epoch 4479, Loss: 1.5674720704555511, Final Batch Loss: 0.3004758059978485\n",
      "Epoch 4480, Loss: 1.6161946505308151, Final Batch Loss: 0.24569089710712433\n",
      "Epoch 4481, Loss: 1.4983133375644684, Final Batch Loss: 0.23939520120620728\n",
      "Epoch 4482, Loss: 1.5613845884799957, Final Batch Loss: 0.2983127236366272\n",
      "Epoch 4483, Loss: 1.5468637943267822, Final Batch Loss: 0.3500750660896301\n",
      "Epoch 4484, Loss: 1.5562753081321716, Final Batch Loss: 0.3088829517364502\n",
      "Epoch 4485, Loss: 1.448392629623413, Final Batch Loss: 0.235885351896286\n",
      "Epoch 4486, Loss: 1.4972272217273712, Final Batch Loss: 0.25897538661956787\n",
      "Epoch 4487, Loss: 1.5286338478326797, Final Batch Loss: 0.24983851611614227\n",
      "Epoch 4488, Loss: 1.4310754537582397, Final Batch Loss: 0.25484347343444824\n",
      "Epoch 4489, Loss: 1.4807578176259995, Final Batch Loss: 0.3824591040611267\n",
      "Epoch 4490, Loss: 1.5203238129615784, Final Batch Loss: 0.31805434823036194\n",
      "Epoch 4491, Loss: 1.4841312617063522, Final Batch Loss: 0.3426797389984131\n",
      "Epoch 4492, Loss: 1.572811484336853, Final Batch Loss: 0.25613799691200256\n",
      "Epoch 4493, Loss: 1.4532488137483597, Final Batch Loss: 0.2742496728897095\n",
      "Epoch 4494, Loss: 1.3854264467954636, Final Batch Loss: 0.29924991726875305\n",
      "Epoch 4495, Loss: 1.543667495250702, Final Batch Loss: 0.3885807394981384\n",
      "Epoch 4496, Loss: 1.584827482700348, Final Batch Loss: 0.28674378991127014\n",
      "Epoch 4497, Loss: 1.643647462129593, Final Batch Loss: 0.305071622133255\n",
      "Epoch 4498, Loss: 1.4725979417562485, Final Batch Loss: 0.1655774861574173\n",
      "Epoch 4499, Loss: 1.5007227957248688, Final Batch Loss: 0.33211204409599304\n",
      "Epoch 4500, Loss: 1.579409271478653, Final Batch Loss: 0.2727993130683899\n",
      "Epoch 4501, Loss: 1.491131067276001, Final Batch Loss: 0.30714067816734314\n",
      "Epoch 4502, Loss: 1.4270148426294327, Final Batch Loss: 0.2691056430339813\n",
      "Epoch 4503, Loss: 1.528572365641594, Final Batch Loss: 0.29062384366989136\n",
      "Epoch 4504, Loss: 1.6726526319980621, Final Batch Loss: 0.39051803946495056\n",
      "Epoch 4505, Loss: 1.396371379494667, Final Batch Loss: 0.20666135847568512\n",
      "Epoch 4506, Loss: 1.4367159307003021, Final Batch Loss: 0.28169530630111694\n",
      "Epoch 4507, Loss: 1.5323287397623062, Final Batch Loss: 0.34088820219039917\n",
      "Epoch 4508, Loss: 1.45870740711689, Final Batch Loss: 0.3319665491580963\n",
      "Epoch 4509, Loss: 1.6599108278751373, Final Batch Loss: 0.410874605178833\n",
      "Epoch 4510, Loss: 1.5205333232879639, Final Batch Loss: 0.32359248399734497\n",
      "Epoch 4511, Loss: 1.4306608885526657, Final Batch Loss: 0.3517340421676636\n",
      "Epoch 4512, Loss: 1.5144047737121582, Final Batch Loss: 0.23929943144321442\n",
      "Epoch 4513, Loss: 1.3973031044006348, Final Batch Loss: 0.2996739447116852\n",
      "Epoch 4514, Loss: 1.5882323235273361, Final Batch Loss: 0.3853816092014313\n",
      "Epoch 4515, Loss: 1.478121280670166, Final Batch Loss: 0.3169316053390503\n",
      "Epoch 4516, Loss: 1.4876099228858948, Final Batch Loss: 0.27779555320739746\n",
      "Epoch 4517, Loss: 1.4858912527561188, Final Batch Loss: 0.22999098896980286\n",
      "Epoch 4518, Loss: 1.4655416011810303, Final Batch Loss: 0.27493491768836975\n",
      "Epoch 4519, Loss: 1.5393990278244019, Final Batch Loss: 0.36248669028282166\n",
      "Epoch 4520, Loss: 1.5009650588035583, Final Batch Loss: 0.3681607246398926\n",
      "Epoch 4521, Loss: 1.6074059903621674, Final Batch Loss: 0.35446709394454956\n",
      "Epoch 4522, Loss: 1.5343196392059326, Final Batch Loss: 0.282330721616745\n",
      "Epoch 4523, Loss: 1.5438432097434998, Final Batch Loss: 0.24844923615455627\n",
      "Epoch 4524, Loss: 1.5030038356781006, Final Batch Loss: 0.27193349599838257\n",
      "Epoch 4525, Loss: 1.5226832628250122, Final Batch Loss: 0.2674145996570587\n",
      "Epoch 4526, Loss: 1.5497025847434998, Final Batch Loss: 0.3388262093067169\n",
      "Epoch 4527, Loss: 1.4492997229099274, Final Batch Loss: 0.3126726746559143\n",
      "Epoch 4528, Loss: 1.455996036529541, Final Batch Loss: 0.29415860772132874\n",
      "Epoch 4529, Loss: 1.4459533989429474, Final Batch Loss: 0.23595669865608215\n",
      "Epoch 4530, Loss: 1.4730990082025528, Final Batch Loss: 0.23619453608989716\n",
      "Epoch 4531, Loss: 1.4728101193904877, Final Batch Loss: 0.2768447995185852\n",
      "Epoch 4532, Loss: 1.505774274468422, Final Batch Loss: 0.2357856184244156\n",
      "Epoch 4533, Loss: 1.5343218892812729, Final Batch Loss: 0.23607514798641205\n",
      "Epoch 4534, Loss: 1.3791156709194183, Final Batch Loss: 0.18347898125648499\n",
      "Epoch 4535, Loss: 1.4032095819711685, Final Batch Loss: 0.2336447685956955\n",
      "Epoch 4536, Loss: 1.527650147676468, Final Batch Loss: 0.2402791976928711\n",
      "Epoch 4537, Loss: 1.5033025741577148, Final Batch Loss: 0.20904719829559326\n",
      "Epoch 4538, Loss: 1.6293147802352905, Final Batch Loss: 0.2893335223197937\n",
      "Epoch 4539, Loss: 1.4471324384212494, Final Batch Loss: 0.25769612193107605\n",
      "Epoch 4540, Loss: 1.633974850177765, Final Batch Loss: 0.30970364809036255\n",
      "Epoch 4541, Loss: 1.57669797539711, Final Batch Loss: 0.27571746706962585\n",
      "Epoch 4542, Loss: 1.646850824356079, Final Batch Loss: 0.39176392555236816\n",
      "Epoch 4543, Loss: 1.5192390978336334, Final Batch Loss: 0.28814733028411865\n",
      "Epoch 4544, Loss: 1.5067401379346848, Final Batch Loss: 0.2295578271150589\n",
      "Epoch 4545, Loss: 1.534829095005989, Final Batch Loss: 0.33630338311195374\n",
      "Epoch 4546, Loss: 1.5164920091629028, Final Batch Loss: 0.2963676154613495\n",
      "Epoch 4547, Loss: 1.466597557067871, Final Batch Loss: 0.29319140315055847\n",
      "Epoch 4548, Loss: 1.4361916035413742, Final Batch Loss: 0.24656014144420624\n",
      "Epoch 4549, Loss: 1.4572689831256866, Final Batch Loss: 0.31352847814559937\n",
      "Epoch 4550, Loss: 1.642792284488678, Final Batch Loss: 0.3051125705242157\n",
      "Epoch 4551, Loss: 1.4070348143577576, Final Batch Loss: 0.298149973154068\n",
      "Epoch 4552, Loss: 1.5445959270000458, Final Batch Loss: 0.35600507259368896\n",
      "Epoch 4553, Loss: 1.541080802679062, Final Batch Loss: 0.33914652466773987\n",
      "Epoch 4554, Loss: 1.531172901391983, Final Batch Loss: 0.27322712540626526\n",
      "Epoch 4555, Loss: 1.6144893914461136, Final Batch Loss: 0.3882635831832886\n",
      "Epoch 4556, Loss: 1.5284889042377472, Final Batch Loss: 0.3344847559928894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4557, Loss: 1.5295469909906387, Final Batch Loss: 0.38624107837677\n",
      "Epoch 4558, Loss: 1.6464846432209015, Final Batch Loss: 0.33322906494140625\n",
      "Epoch 4559, Loss: 1.6628901660442352, Final Batch Loss: 0.34354662895202637\n",
      "Epoch 4560, Loss: 1.654188334941864, Final Batch Loss: 0.41383183002471924\n",
      "Epoch 4561, Loss: 1.562137395143509, Final Batch Loss: 0.36355987191200256\n",
      "Epoch 4562, Loss: 1.6884532570838928, Final Batch Loss: 0.34286367893218994\n",
      "Epoch 4563, Loss: 1.4500410556793213, Final Batch Loss: 0.2985404133796692\n",
      "Epoch 4564, Loss: 1.4814487099647522, Final Batch Loss: 0.2802024185657501\n",
      "Epoch 4565, Loss: 1.470576524734497, Final Batch Loss: 0.2788658142089844\n",
      "Epoch 4566, Loss: 1.5229551196098328, Final Batch Loss: 0.2886596620082855\n",
      "Epoch 4567, Loss: 1.6723839044570923, Final Batch Loss: 0.40474510192871094\n",
      "Epoch 4568, Loss: 1.6032925844192505, Final Batch Loss: 0.35376855731010437\n",
      "Epoch 4569, Loss: 1.5338089019060135, Final Batch Loss: 0.34129464626312256\n",
      "Epoch 4570, Loss: 1.5931648313999176, Final Batch Loss: 0.3949350416660309\n",
      "Epoch 4571, Loss: 1.5267675817012787, Final Batch Loss: 0.272590309381485\n",
      "Epoch 4572, Loss: 1.4329200983047485, Final Batch Loss: 0.2550587058067322\n",
      "Epoch 4573, Loss: 1.5264443308115005, Final Batch Loss: 0.3852907419204712\n",
      "Epoch 4574, Loss: 1.4250373244285583, Final Batch Loss: 0.27307257056236267\n",
      "Epoch 4575, Loss: 1.3510236144065857, Final Batch Loss: 0.24002483487129211\n",
      "Epoch 4576, Loss: 1.4000614285469055, Final Batch Loss: 0.2253160923719406\n",
      "Epoch 4577, Loss: 1.5269371420145035, Final Batch Loss: 0.24486881494522095\n",
      "Epoch 4578, Loss: 1.5335140228271484, Final Batch Loss: 0.28729745745658875\n",
      "Epoch 4579, Loss: 1.5672926902770996, Final Batch Loss: 0.378206342458725\n",
      "Epoch 4580, Loss: 1.625338852405548, Final Batch Loss: 0.3737489879131317\n",
      "Epoch 4581, Loss: 1.3157344162464142, Final Batch Loss: 0.20516681671142578\n",
      "Epoch 4582, Loss: 1.4646457433700562, Final Batch Loss: 0.25005853176116943\n",
      "Epoch 4583, Loss: 1.5508699268102646, Final Batch Loss: 0.3174263834953308\n",
      "Epoch 4584, Loss: 1.5386823415756226, Final Batch Loss: 0.2978937029838562\n",
      "Epoch 4585, Loss: 1.408225029706955, Final Batch Loss: 0.24782802164554596\n",
      "Epoch 4586, Loss: 1.4621995836496353, Final Batch Loss: 0.244986429810524\n",
      "Epoch 4587, Loss: 1.4519521445035934, Final Batch Loss: 0.3578437566757202\n",
      "Epoch 4588, Loss: 1.4432331770658493, Final Batch Loss: 0.2342582494020462\n",
      "Epoch 4589, Loss: 1.6635754704475403, Final Batch Loss: 0.27566900849342346\n",
      "Epoch 4590, Loss: 1.503464251756668, Final Batch Loss: 0.2905135154724121\n",
      "Epoch 4591, Loss: 1.498286560177803, Final Batch Loss: 0.31957682967185974\n",
      "Epoch 4592, Loss: 1.6315309703350067, Final Batch Loss: 0.42829594016075134\n",
      "Epoch 4593, Loss: 1.4866254031658173, Final Batch Loss: 0.2466464638710022\n",
      "Epoch 4594, Loss: 1.5916290581226349, Final Batch Loss: 0.30188271403312683\n",
      "Epoch 4595, Loss: 1.3992156684398651, Final Batch Loss: 0.3171607553958893\n",
      "Epoch 4596, Loss: 1.480645090341568, Final Batch Loss: 0.28936079144477844\n",
      "Epoch 4597, Loss: 1.531433492898941, Final Batch Loss: 0.2923462986946106\n",
      "Epoch 4598, Loss: 1.4003339409828186, Final Batch Loss: 0.2270776927471161\n",
      "Epoch 4599, Loss: 1.4856385290622711, Final Batch Loss: 0.347648948431015\n",
      "Epoch 4600, Loss: 1.4995966255664825, Final Batch Loss: 0.28557923436164856\n",
      "Epoch 4601, Loss: 1.3765668123960495, Final Batch Loss: 0.3116014301776886\n",
      "Epoch 4602, Loss: 1.475651741027832, Final Batch Loss: 0.2946506440639496\n",
      "Epoch 4603, Loss: 1.5557115077972412, Final Batch Loss: 0.3943071663379669\n",
      "Epoch 4604, Loss: 1.4385575652122498, Final Batch Loss: 0.30250126123428345\n",
      "Epoch 4605, Loss: 1.5943935215473175, Final Batch Loss: 0.35740926861763\n",
      "Epoch 4606, Loss: 1.6101074814796448, Final Batch Loss: 0.25758132338523865\n",
      "Epoch 4607, Loss: 1.5491577684879303, Final Batch Loss: 0.3549809157848358\n",
      "Epoch 4608, Loss: 1.4449224919080734, Final Batch Loss: 0.22446291148662567\n",
      "Epoch 4609, Loss: 1.5900047421455383, Final Batch Loss: 0.4152664840221405\n",
      "Epoch 4610, Loss: 1.5776946246623993, Final Batch Loss: 0.25606802105903625\n",
      "Epoch 4611, Loss: 1.447958305478096, Final Batch Loss: 0.22002674639225006\n",
      "Epoch 4612, Loss: 1.7272886335849762, Final Batch Loss: 0.5386368036270142\n",
      "Epoch 4613, Loss: 1.547197550535202, Final Batch Loss: 0.33141234517097473\n",
      "Epoch 4614, Loss: 1.5050234943628311, Final Batch Loss: 0.354367196559906\n",
      "Epoch 4615, Loss: 1.4070795476436615, Final Batch Loss: 0.29472485184669495\n",
      "Epoch 4616, Loss: 1.4273636937141418, Final Batch Loss: 0.3396419584751129\n",
      "Epoch 4617, Loss: 1.487791270017624, Final Batch Loss: 0.26470476388931274\n",
      "Epoch 4618, Loss: 1.6385803818702698, Final Batch Loss: 0.38640516996383667\n",
      "Epoch 4619, Loss: 1.4927019774913788, Final Batch Loss: 0.3175247609615326\n",
      "Epoch 4620, Loss: 1.511053055524826, Final Batch Loss: 0.28853559494018555\n",
      "Epoch 4621, Loss: 1.5589340329170227, Final Batch Loss: 0.3091646432876587\n",
      "Epoch 4622, Loss: 1.5908903777599335, Final Batch Loss: 0.3950980007648468\n",
      "Epoch 4623, Loss: 1.520915612578392, Final Batch Loss: 0.20721815526485443\n",
      "Epoch 4624, Loss: 1.4365030229091644, Final Batch Loss: 0.29577720165252686\n",
      "Epoch 4625, Loss: 1.4257497191429138, Final Batch Loss: 0.24793872237205505\n",
      "Epoch 4626, Loss: 1.597536861896515, Final Batch Loss: 0.42971205711364746\n",
      "Epoch 4627, Loss: 1.4981927871704102, Final Batch Loss: 0.257610023021698\n",
      "Epoch 4628, Loss: 1.439446747303009, Final Batch Loss: 0.3009597361087799\n",
      "Epoch 4629, Loss: 1.538994699716568, Final Batch Loss: 0.293999582529068\n",
      "Epoch 4630, Loss: 1.6634807586669922, Final Batch Loss: 0.2697933316230774\n",
      "Epoch 4631, Loss: 1.568819284439087, Final Batch Loss: 0.26169922947883606\n",
      "Epoch 4632, Loss: 1.5873975157737732, Final Batch Loss: 0.41472044587135315\n",
      "Epoch 4633, Loss: 1.4138916432857513, Final Batch Loss: 0.2668624222278595\n",
      "Epoch 4634, Loss: 1.6453151106834412, Final Batch Loss: 0.4116818308830261\n",
      "Epoch 4635, Loss: 1.526643767952919, Final Batch Loss: 0.35575857758522034\n",
      "Epoch 4636, Loss: 1.4192569255828857, Final Batch Loss: 0.34398216009140015\n",
      "Epoch 4637, Loss: 1.594923734664917, Final Batch Loss: 0.33898210525512695\n",
      "Epoch 4638, Loss: 1.6260770857334137, Final Batch Loss: 0.4124055504798889\n",
      "Epoch 4639, Loss: 1.532957136631012, Final Batch Loss: 0.34072738885879517\n",
      "Epoch 4640, Loss: 1.5170359015464783, Final Batch Loss: 0.25008484721183777\n",
      "Epoch 4641, Loss: 1.4992113411426544, Final Batch Loss: 0.2812446653842926\n",
      "Epoch 4642, Loss: 1.4486112892627716, Final Batch Loss: 0.3172883987426758\n",
      "Epoch 4643, Loss: 1.3662220239639282, Final Batch Loss: 0.2835693359375\n",
      "Epoch 4644, Loss: 1.5039772689342499, Final Batch Loss: 0.35042664408683777\n",
      "Epoch 4645, Loss: 1.4035316407680511, Final Batch Loss: 0.26277390122413635\n",
      "Epoch 4646, Loss: 1.4828703999519348, Final Batch Loss: 0.29999637603759766\n",
      "Epoch 4647, Loss: 1.5254228115081787, Final Batch Loss: 0.28666871786117554\n",
      "Epoch 4648, Loss: 1.3837288171052933, Final Batch Loss: 0.2744673490524292\n",
      "Epoch 4649, Loss: 1.4377829730510712, Final Batch Loss: 0.3627047836780548\n",
      "Epoch 4650, Loss: 1.4001957327127457, Final Batch Loss: 0.31225866079330444\n",
      "Epoch 4651, Loss: 1.543937936425209, Final Batch Loss: 0.2481066733598709\n",
      "Epoch 4652, Loss: 1.5813687145709991, Final Batch Loss: 0.28793469071388245\n",
      "Epoch 4653, Loss: 1.4717997312545776, Final Batch Loss: 0.3207433521747589\n",
      "Epoch 4654, Loss: 1.538498431444168, Final Batch Loss: 0.3117164671421051\n",
      "Epoch 4655, Loss: 1.5017509013414383, Final Batch Loss: 0.2422933131456375\n",
      "Epoch 4656, Loss: 1.4284875094890594, Final Batch Loss: 0.2965639531612396\n",
      "Epoch 4657, Loss: 1.5510578453540802, Final Batch Loss: 0.2963401675224304\n",
      "Epoch 4658, Loss: 1.5859666466712952, Final Batch Loss: 0.28194501996040344\n",
      "Epoch 4659, Loss: 1.5050042271614075, Final Batch Loss: 0.37897977232933044\n",
      "Epoch 4660, Loss: 1.4556991159915924, Final Batch Loss: 0.31096944212913513\n",
      "Epoch 4661, Loss: 1.4102991223335266, Final Batch Loss: 0.28626748919487\n",
      "Epoch 4662, Loss: 1.435770407319069, Final Batch Loss: 0.24617794156074524\n",
      "Epoch 4663, Loss: 1.5685014128684998, Final Batch Loss: 0.3629505932331085\n",
      "Epoch 4664, Loss: 1.5265724956989288, Final Batch Loss: 0.28262031078338623\n",
      "Epoch 4665, Loss: 1.5778400599956512, Final Batch Loss: 0.3162183165550232\n",
      "Epoch 4666, Loss: 1.4433605521917343, Final Batch Loss: 0.3342534601688385\n",
      "Epoch 4667, Loss: 1.5167866945266724, Final Batch Loss: 0.30895286798477173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4668, Loss: 1.5715585947036743, Final Batch Loss: 0.37366294860839844\n",
      "Epoch 4669, Loss: 1.433859258890152, Final Batch Loss: 0.2478058934211731\n",
      "Epoch 4670, Loss: 1.402038797736168, Final Batch Loss: 0.1914699673652649\n",
      "Epoch 4671, Loss: 1.42134228348732, Final Batch Loss: 0.27342337369918823\n",
      "Epoch 4672, Loss: 1.4588309824466705, Final Batch Loss: 0.28858256340026855\n",
      "Epoch 4673, Loss: 1.3796714842319489, Final Batch Loss: 0.20296329259872437\n",
      "Epoch 4674, Loss: 1.5394415259361267, Final Batch Loss: 0.2682466208934784\n",
      "Epoch 4675, Loss: 1.453486606478691, Final Batch Loss: 0.3621176779270172\n",
      "Epoch 4676, Loss: 1.440458819270134, Final Batch Loss: 0.2413293570280075\n",
      "Epoch 4677, Loss: 1.5350977778434753, Final Batch Loss: 0.2644006013870239\n",
      "Epoch 4678, Loss: 1.528694987297058, Final Batch Loss: 0.29655271768569946\n",
      "Epoch 4679, Loss: 1.421266257762909, Final Batch Loss: 0.2740080654621124\n",
      "Epoch 4680, Loss: 1.432625189423561, Final Batch Loss: 0.19358135759830475\n",
      "Epoch 4681, Loss: 1.4902851432561874, Final Batch Loss: 0.31199774146080017\n",
      "Epoch 4682, Loss: 1.5502208769321442, Final Batch Loss: 0.3105063736438751\n",
      "Epoch 4683, Loss: 1.559291034936905, Final Batch Loss: 0.3086050748825073\n",
      "Epoch 4684, Loss: 1.486951857805252, Final Batch Loss: 0.28019478917121887\n",
      "Epoch 4685, Loss: 1.5228035748004913, Final Batch Loss: 0.33428218960762024\n",
      "Epoch 4686, Loss: 1.5408762097358704, Final Batch Loss: 0.3576750159263611\n",
      "Epoch 4687, Loss: 1.4222796708345413, Final Batch Loss: 0.21041437983512878\n",
      "Epoch 4688, Loss: 1.4074601382017136, Final Batch Loss: 0.28058117628097534\n",
      "Epoch 4689, Loss: 1.5123783349990845, Final Batch Loss: 0.30565834045410156\n",
      "Epoch 4690, Loss: 1.4930316358804703, Final Batch Loss: 0.3133605122566223\n",
      "Epoch 4691, Loss: 1.490739643573761, Final Batch Loss: 0.2501785159111023\n",
      "Epoch 4692, Loss: 1.4824073910713196, Final Batch Loss: 0.27036193013191223\n",
      "Epoch 4693, Loss: 1.466110959649086, Final Batch Loss: 0.3322395086288452\n",
      "Epoch 4694, Loss: 1.7351893782615662, Final Batch Loss: 0.4355568587779999\n",
      "Epoch 4695, Loss: 1.4111890494823456, Final Batch Loss: 0.28420740365982056\n",
      "Epoch 4696, Loss: 1.4313250482082367, Final Batch Loss: 0.2876729667186737\n",
      "Epoch 4697, Loss: 1.6094895601272583, Final Batch Loss: 0.3097374141216278\n",
      "Epoch 4698, Loss: 1.4446093142032623, Final Batch Loss: 0.2236786186695099\n",
      "Epoch 4699, Loss: 1.5485863834619522, Final Batch Loss: 0.24728403985500336\n",
      "Epoch 4700, Loss: 1.7732278406620026, Final Batch Loss: 0.4818805456161499\n",
      "Epoch 4701, Loss: 1.4843155145645142, Final Batch Loss: 0.28872403502464294\n",
      "Epoch 4702, Loss: 1.583930343389511, Final Batch Loss: 0.3438827395439148\n",
      "Epoch 4703, Loss: 1.533592849969864, Final Batch Loss: 0.29588282108306885\n",
      "Epoch 4704, Loss: 1.4311992526054382, Final Batch Loss: 0.21800753474235535\n",
      "Epoch 4705, Loss: 1.4420100897550583, Final Batch Loss: 0.22787238657474518\n",
      "Epoch 4706, Loss: 1.408074751496315, Final Batch Loss: 0.2296128123998642\n",
      "Epoch 4707, Loss: 1.45945343375206, Final Batch Loss: 0.2636808454990387\n",
      "Epoch 4708, Loss: 1.5294477343559265, Final Batch Loss: 0.2627842128276825\n",
      "Epoch 4709, Loss: 1.480847418308258, Final Batch Loss: 0.2820153832435608\n",
      "Epoch 4710, Loss: 1.5929379761219025, Final Batch Loss: 0.44621098041534424\n",
      "Epoch 4711, Loss: 1.5368966162204742, Final Batch Loss: 0.3364521563053131\n",
      "Epoch 4712, Loss: 1.4813295006752014, Final Batch Loss: 0.271283358335495\n",
      "Epoch 4713, Loss: 1.4771276265382767, Final Batch Loss: 0.29278647899627686\n",
      "Epoch 4714, Loss: 1.791894555091858, Final Batch Loss: 0.3375106453895569\n",
      "Epoch 4715, Loss: 1.4627436250448227, Final Batch Loss: 0.21019403636455536\n",
      "Epoch 4716, Loss: 1.4367204308509827, Final Batch Loss: 0.3240205943584442\n",
      "Epoch 4717, Loss: 1.6106137931346893, Final Batch Loss: 0.36515748500823975\n",
      "Epoch 4718, Loss: 1.4325798004865646, Final Batch Loss: 0.40284547209739685\n",
      "Epoch 4719, Loss: 1.6037680506706238, Final Batch Loss: 0.34806028008461\n",
      "Epoch 4720, Loss: 1.488524168729782, Final Batch Loss: 0.2877918779850006\n",
      "Epoch 4721, Loss: 1.4374788999557495, Final Batch Loss: 0.30296334624290466\n",
      "Epoch 4722, Loss: 1.458436518907547, Final Batch Loss: 0.28526467084884644\n",
      "Epoch 4723, Loss: 1.5050025135278702, Final Batch Loss: 0.2234431803226471\n",
      "Epoch 4724, Loss: 1.4543114751577377, Final Batch Loss: 0.3646753430366516\n",
      "Epoch 4725, Loss: 1.425682544708252, Final Batch Loss: 0.2456403225660324\n",
      "Epoch 4726, Loss: 1.5353697538375854, Final Batch Loss: 0.3117380738258362\n",
      "Epoch 4727, Loss: 1.4716542065143585, Final Batch Loss: 0.28498581051826477\n",
      "Epoch 4728, Loss: 1.3634540140628815, Final Batch Loss: 0.26257777214050293\n",
      "Epoch 4729, Loss: 1.4487512111663818, Final Batch Loss: 0.3389531970024109\n",
      "Epoch 4730, Loss: 1.453980267047882, Final Batch Loss: 0.293626070022583\n",
      "Epoch 4731, Loss: 1.5765973329544067, Final Batch Loss: 0.31450843811035156\n",
      "Epoch 4732, Loss: 1.434474989771843, Final Batch Loss: 0.22395235300064087\n",
      "Epoch 4733, Loss: 1.5536712408065796, Final Batch Loss: 0.4028177559375763\n",
      "Epoch 4734, Loss: 1.4558120965957642, Final Batch Loss: 0.3299788236618042\n",
      "Epoch 4735, Loss: 1.4878788441419601, Final Batch Loss: 0.3804997205734253\n",
      "Epoch 4736, Loss: 1.594225823879242, Final Batch Loss: 0.33165374398231506\n",
      "Epoch 4737, Loss: 1.4568787962198257, Final Batch Loss: 0.3542152941226959\n",
      "Epoch 4738, Loss: 1.454989716410637, Final Batch Loss: 0.3696530759334564\n",
      "Epoch 4739, Loss: 1.5079330950975418, Final Batch Loss: 0.24594129621982574\n",
      "Epoch 4740, Loss: 1.4924869239330292, Final Batch Loss: 0.2627790570259094\n",
      "Epoch 4741, Loss: 1.4139746129512787, Final Batch Loss: 0.2712520360946655\n",
      "Epoch 4742, Loss: 1.4853207617998123, Final Batch Loss: 0.31738629937171936\n",
      "Epoch 4743, Loss: 1.3987989872694016, Final Batch Loss: 0.24161399900913239\n",
      "Epoch 4744, Loss: 1.4847235083580017, Final Batch Loss: 0.2627059519290924\n",
      "Epoch 4745, Loss: 1.4243161380290985, Final Batch Loss: 0.29098302125930786\n",
      "Epoch 4746, Loss: 1.4487222582101822, Final Batch Loss: 0.345084011554718\n",
      "Epoch 4747, Loss: 1.5245226919651031, Final Batch Loss: 0.38411787152290344\n",
      "Epoch 4748, Loss: 1.565658524632454, Final Batch Loss: 0.4139057397842407\n",
      "Epoch 4749, Loss: 1.4352790415287018, Final Batch Loss: 0.33856987953186035\n",
      "Epoch 4750, Loss: 1.6080701649188995, Final Batch Loss: 0.3021194636821747\n",
      "Epoch 4751, Loss: 1.5682020485401154, Final Batch Loss: 0.3509104549884796\n",
      "Epoch 4752, Loss: 1.4291532188653946, Final Batch Loss: 0.23712031543254852\n",
      "Epoch 4753, Loss: 1.3252239525318146, Final Batch Loss: 0.2138761430978775\n",
      "Epoch 4754, Loss: 1.5005934834480286, Final Batch Loss: 0.3463180363178253\n",
      "Epoch 4755, Loss: 1.5538425296545029, Final Batch Loss: 0.5009465217590332\n",
      "Epoch 4756, Loss: 1.4534714818000793, Final Batch Loss: 0.2589970827102661\n",
      "Epoch 4757, Loss: 1.4482493847608566, Final Batch Loss: 0.3489246964454651\n",
      "Epoch 4758, Loss: 1.5590325891971588, Final Batch Loss: 0.3217567801475525\n",
      "Epoch 4759, Loss: 1.4306801110506058, Final Batch Loss: 0.3090205788612366\n",
      "Epoch 4760, Loss: 1.386708214879036, Final Batch Loss: 0.1994454711675644\n",
      "Epoch 4761, Loss: 1.4533888697624207, Final Batch Loss: 0.30362242460250854\n",
      "Epoch 4762, Loss: 1.4517299830913544, Final Batch Loss: 0.30962714552879333\n",
      "Epoch 4763, Loss: 1.5769756734371185, Final Batch Loss: 0.3194439709186554\n",
      "Epoch 4764, Loss: 1.3501275777816772, Final Batch Loss: 0.22998233139514923\n",
      "Epoch 4765, Loss: 1.420848309993744, Final Batch Loss: 0.27841630578041077\n",
      "Epoch 4766, Loss: 1.578900158405304, Final Batch Loss: 0.25060173869132996\n",
      "Epoch 4767, Loss: 1.4386425465345383, Final Batch Loss: 0.29884037375450134\n",
      "Epoch 4768, Loss: 1.5953166782855988, Final Batch Loss: 0.2803525924682617\n",
      "Epoch 4769, Loss: 1.4284435361623764, Final Batch Loss: 0.30352792143821716\n",
      "Epoch 4770, Loss: 1.5373504757881165, Final Batch Loss: 0.3360389471054077\n",
      "Epoch 4771, Loss: 1.4512081146240234, Final Batch Loss: 0.26810044050216675\n",
      "Epoch 4772, Loss: 1.6193992495536804, Final Batch Loss: 0.44195976853370667\n",
      "Epoch 4773, Loss: 1.4405753165483475, Final Batch Loss: 0.2538444399833679\n",
      "Epoch 4774, Loss: 1.472361445426941, Final Batch Loss: 0.2032589316368103\n",
      "Epoch 4775, Loss: 1.6053735613822937, Final Batch Loss: 0.30194786190986633\n",
      "Epoch 4776, Loss: 1.6028816401958466, Final Batch Loss: 0.41125956177711487\n",
      "Epoch 4777, Loss: 1.5136067569255829, Final Batch Loss: 0.4148011803627014\n",
      "Epoch 4778, Loss: 1.4561339616775513, Final Batch Loss: 0.22134406864643097\n",
      "Epoch 4779, Loss: 1.4402250796556473, Final Batch Loss: 0.2353111058473587\n",
      "Epoch 4780, Loss: 1.4559823870658875, Final Batch Loss: 0.2561500072479248\n",
      "Epoch 4781, Loss: 1.4458844512701035, Final Batch Loss: 0.2218277007341385\n",
      "Epoch 4782, Loss: 1.4625713229179382, Final Batch Loss: 0.3169409930706024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4783, Loss: 1.399259552359581, Final Batch Loss: 0.27611714601516724\n",
      "Epoch 4784, Loss: 1.5578072369098663, Final Batch Loss: 0.4568544626235962\n",
      "Epoch 4785, Loss: 1.4453002512454987, Final Batch Loss: 0.2311132550239563\n",
      "Epoch 4786, Loss: 1.4799969494342804, Final Batch Loss: 0.2785034775733948\n",
      "Epoch 4787, Loss: 1.485088512301445, Final Batch Loss: 0.2734719514846802\n",
      "Epoch 4788, Loss: 1.5620944499969482, Final Batch Loss: 0.33280640840530396\n",
      "Epoch 4789, Loss: 1.5600031316280365, Final Batch Loss: 0.29860544204711914\n",
      "Epoch 4790, Loss: 1.5459003448486328, Final Batch Loss: 0.2730174660682678\n",
      "Epoch 4791, Loss: 1.4507386982440948, Final Batch Loss: 0.28971150517463684\n",
      "Epoch 4792, Loss: 1.4157838225364685, Final Batch Loss: 0.23796221613883972\n",
      "Epoch 4793, Loss: 1.5461823046207428, Final Batch Loss: 0.3697250485420227\n",
      "Epoch 4794, Loss: 1.3767091929912567, Final Batch Loss: 0.29551300406455994\n",
      "Epoch 4795, Loss: 1.3328776061534882, Final Batch Loss: 0.27152326703071594\n",
      "Epoch 4796, Loss: 1.53006973862648, Final Batch Loss: 0.4138084650039673\n",
      "Epoch 4797, Loss: 1.4078876972198486, Final Batch Loss: 0.19662609696388245\n",
      "Epoch 4798, Loss: 1.3999153077602386, Final Batch Loss: 0.2574995160102844\n",
      "Epoch 4799, Loss: 1.3172800689935684, Final Batch Loss: 0.2455338090658188\n",
      "Epoch 4800, Loss: 1.4413131475448608, Final Batch Loss: 0.3457357883453369\n",
      "Epoch 4801, Loss: 1.4374192655086517, Final Batch Loss: 0.26444360613822937\n",
      "Epoch 4802, Loss: 1.5698368847370148, Final Batch Loss: 0.46739184856414795\n",
      "Epoch 4803, Loss: 1.421696126461029, Final Batch Loss: 0.22656142711639404\n",
      "Epoch 4804, Loss: 1.5924552977085114, Final Batch Loss: 0.20089158415794373\n",
      "Epoch 4805, Loss: 1.5908087342977524, Final Batch Loss: 0.35400158166885376\n",
      "Epoch 4806, Loss: 1.4641727954149246, Final Batch Loss: 0.2651600241661072\n",
      "Epoch 4807, Loss: 1.4403166472911835, Final Batch Loss: 0.3272215723991394\n",
      "Epoch 4808, Loss: 1.3590251207351685, Final Batch Loss: 0.2680385708808899\n",
      "Epoch 4809, Loss: 1.4674264192581177, Final Batch Loss: 0.26093751192092896\n",
      "Epoch 4810, Loss: 1.4641446322202682, Final Batch Loss: 0.2686150372028351\n",
      "Epoch 4811, Loss: 1.3501326739788055, Final Batch Loss: 0.22301675379276276\n",
      "Epoch 4812, Loss: 1.5177147090435028, Final Batch Loss: 0.31914475560188293\n",
      "Epoch 4813, Loss: 1.439121037721634, Final Batch Loss: 0.31691423058509827\n",
      "Epoch 4814, Loss: 1.5934240520000458, Final Batch Loss: 0.3109254837036133\n",
      "Epoch 4815, Loss: 1.5454707145690918, Final Batch Loss: 0.30724838376045227\n",
      "Epoch 4816, Loss: 1.4675932079553604, Final Batch Loss: 0.258815735578537\n",
      "Epoch 4817, Loss: 1.4863140136003494, Final Batch Loss: 0.241712287068367\n",
      "Epoch 4818, Loss: 1.4668876826763153, Final Batch Loss: 0.2621202766895294\n",
      "Epoch 4819, Loss: 1.3552975803613663, Final Batch Loss: 0.24660199880599976\n",
      "Epoch 4820, Loss: 1.3788245767354965, Final Batch Loss: 0.23858900368213654\n",
      "Epoch 4821, Loss: 1.5794291496276855, Final Batch Loss: 0.4194866716861725\n",
      "Epoch 4822, Loss: 1.4096395820379257, Final Batch Loss: 0.34071362018585205\n",
      "Epoch 4823, Loss: 1.4026738703250885, Final Batch Loss: 0.3329507112503052\n",
      "Epoch 4824, Loss: 1.5776033997535706, Final Batch Loss: 0.37560099363327026\n",
      "Epoch 4825, Loss: 1.5360664129257202, Final Batch Loss: 0.30860352516174316\n",
      "Epoch 4826, Loss: 1.4357075095176697, Final Batch Loss: 0.20018717646598816\n",
      "Epoch 4827, Loss: 1.4181545972824097, Final Batch Loss: 0.25245946645736694\n",
      "Epoch 4828, Loss: 1.4353566616773605, Final Batch Loss: 0.34368351101875305\n",
      "Epoch 4829, Loss: 1.5000381171703339, Final Batch Loss: 0.2834846079349518\n",
      "Epoch 4830, Loss: 1.3431951999664307, Final Batch Loss: 0.24771025776863098\n",
      "Epoch 4831, Loss: 1.453954890370369, Final Batch Loss: 0.25859618186950684\n",
      "Epoch 4832, Loss: 1.4735988080501556, Final Batch Loss: 0.2933080196380615\n",
      "Epoch 4833, Loss: 1.4161497056484222, Final Batch Loss: 0.25967955589294434\n",
      "Epoch 4834, Loss: 1.5220005214214325, Final Batch Loss: 0.29577746987342834\n",
      "Epoch 4835, Loss: 1.4537807703018188, Final Batch Loss: 0.29078492522239685\n",
      "Epoch 4836, Loss: 1.4800158441066742, Final Batch Loss: 0.3046451210975647\n",
      "Epoch 4837, Loss: 1.4518641382455826, Final Batch Loss: 0.23314405977725983\n",
      "Epoch 4838, Loss: 1.3710117191076279, Final Batch Loss: 0.2443680614233017\n",
      "Epoch 4839, Loss: 1.3740350157022476, Final Batch Loss: 0.2487208992242813\n",
      "Epoch 4840, Loss: 1.3892413228750229, Final Batch Loss: 0.26015615463256836\n",
      "Epoch 4841, Loss: 1.4611088931560516, Final Batch Loss: 0.2827751338481903\n",
      "Epoch 4842, Loss: 1.6000816226005554, Final Batch Loss: 0.40022534132003784\n",
      "Epoch 4843, Loss: 1.6580399572849274, Final Batch Loss: 0.41762274503707886\n",
      "Epoch 4844, Loss: 1.4529998302459717, Final Batch Loss: 0.3019689917564392\n",
      "Epoch 4845, Loss: 1.5950333923101425, Final Batch Loss: 0.43374645709991455\n",
      "Epoch 4846, Loss: 1.4567291140556335, Final Batch Loss: 0.22302088141441345\n",
      "Epoch 4847, Loss: 1.4944716840982437, Final Batch Loss: 0.24420084059238434\n",
      "Epoch 4848, Loss: 1.4618504792451859, Final Batch Loss: 0.2827650308609009\n",
      "Epoch 4849, Loss: 1.496895670890808, Final Batch Loss: 0.24563083052635193\n",
      "Epoch 4850, Loss: 1.5632520020008087, Final Batch Loss: 0.37000030279159546\n",
      "Epoch 4851, Loss: 1.471230387687683, Final Batch Loss: 0.25719061493873596\n",
      "Epoch 4852, Loss: 1.4184003174304962, Final Batch Loss: 0.2655886709690094\n",
      "Epoch 4853, Loss: 1.3842423409223557, Final Batch Loss: 0.24638472497463226\n",
      "Epoch 4854, Loss: 1.5266324281692505, Final Batch Loss: 0.310077428817749\n",
      "Epoch 4855, Loss: 1.4378365725278854, Final Batch Loss: 0.3395948112010956\n",
      "Epoch 4856, Loss: 1.397192195057869, Final Batch Loss: 0.30677953362464905\n",
      "Epoch 4857, Loss: 1.4928017854690552, Final Batch Loss: 0.3015381693840027\n",
      "Epoch 4858, Loss: 1.5826684534549713, Final Batch Loss: 0.3259879946708679\n",
      "Epoch 4859, Loss: 1.5227853655815125, Final Batch Loss: 0.35593366622924805\n",
      "Epoch 4860, Loss: 1.3624934703111649, Final Batch Loss: 0.2705825865268707\n",
      "Epoch 4861, Loss: 1.6637543439865112, Final Batch Loss: 0.43300580978393555\n",
      "Epoch 4862, Loss: 1.4781820476055145, Final Batch Loss: 0.21200183033943176\n",
      "Epoch 4863, Loss: 1.3384351432323456, Final Batch Loss: 0.22656068205833435\n",
      "Epoch 4864, Loss: 1.3414899557828903, Final Batch Loss: 0.31039297580718994\n",
      "Epoch 4865, Loss: 1.38929982483387, Final Batch Loss: 0.29690513014793396\n",
      "Epoch 4866, Loss: 1.362883985042572, Final Batch Loss: 0.25224944949150085\n",
      "Epoch 4867, Loss: 1.3620451837778091, Final Batch Loss: 0.19772425293922424\n",
      "Epoch 4868, Loss: 1.4352503269910812, Final Batch Loss: 0.297823965549469\n",
      "Epoch 4869, Loss: 1.4020445942878723, Final Batch Loss: 0.34722405672073364\n",
      "Epoch 4870, Loss: 1.7090867161750793, Final Batch Loss: 0.4025379717350006\n",
      "Epoch 4871, Loss: 1.5227816104888916, Final Batch Loss: 0.3284672200679779\n",
      "Epoch 4872, Loss: 1.3295675814151764, Final Batch Loss: 0.28785985708236694\n",
      "Epoch 4873, Loss: 1.4839586019515991, Final Batch Loss: 0.2817448377609253\n",
      "Epoch 4874, Loss: 1.4647423923015594, Final Batch Loss: 0.22537562251091003\n",
      "Epoch 4875, Loss: 1.3595638424158096, Final Batch Loss: 0.2388361692428589\n",
      "Epoch 4876, Loss: 1.6167558431625366, Final Batch Loss: 0.3470700681209564\n",
      "Epoch 4877, Loss: 1.6026999652385712, Final Batch Loss: 0.33616045117378235\n",
      "Epoch 4878, Loss: 1.4868941009044647, Final Batch Loss: 0.31007421016693115\n",
      "Epoch 4879, Loss: 1.4772734493017197, Final Batch Loss: 0.3689160943031311\n",
      "Epoch 4880, Loss: 1.4033361077308655, Final Batch Loss: 0.26352405548095703\n",
      "Epoch 4881, Loss: 1.5318593680858612, Final Batch Loss: 0.3392375409603119\n",
      "Epoch 4882, Loss: 1.462438017129898, Final Batch Loss: 0.24376899003982544\n",
      "Epoch 4883, Loss: 1.4829691499471664, Final Batch Loss: 0.376800537109375\n",
      "Epoch 4884, Loss: 1.3779568076133728, Final Batch Loss: 0.21756547689437866\n",
      "Epoch 4885, Loss: 1.5917344987392426, Final Batch Loss: 0.3465416729450226\n",
      "Epoch 4886, Loss: 1.549654170870781, Final Batch Loss: 0.2406943291425705\n",
      "Epoch 4887, Loss: 1.586555078625679, Final Batch Loss: 0.36764639616012573\n",
      "Epoch 4888, Loss: 1.41377791762352, Final Batch Loss: 0.27122458815574646\n",
      "Epoch 4889, Loss: 1.4018453061580658, Final Batch Loss: 0.2415626496076584\n",
      "Epoch 4890, Loss: 1.3398821502923965, Final Batch Loss: 0.2191316783428192\n",
      "Epoch 4891, Loss: 1.3913793861865997, Final Batch Loss: 0.23441901803016663\n",
      "Epoch 4892, Loss: 1.2918441891670227, Final Batch Loss: 0.24852460622787476\n",
      "Epoch 4893, Loss: 1.4574535638093948, Final Batch Loss: 0.3199855089187622\n",
      "Epoch 4894, Loss: 1.3615771681070328, Final Batch Loss: 0.21924839913845062\n",
      "Epoch 4895, Loss: 1.5389783680438995, Final Batch Loss: 0.31021827459335327\n",
      "Epoch 4896, Loss: 1.5419415980577469, Final Batch Loss: 0.46338433027267456\n",
      "Epoch 4897, Loss: 1.6656243205070496, Final Batch Loss: 0.30079206824302673\n",
      "Epoch 4898, Loss: 1.399041011929512, Final Batch Loss: 0.32211145758628845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4899, Loss: 1.3313915729522705, Final Batch Loss: 0.2054586410522461\n",
      "Epoch 4900, Loss: 1.3315812796354294, Final Batch Loss: 0.22072666883468628\n",
      "Epoch 4901, Loss: 1.4288370162248611, Final Batch Loss: 0.23836186528205872\n",
      "Epoch 4902, Loss: 1.5102572739124298, Final Batch Loss: 0.36805322766304016\n",
      "Epoch 4903, Loss: 1.4766472578048706, Final Batch Loss: 0.32413628697395325\n",
      "Epoch 4904, Loss: 1.633031278848648, Final Batch Loss: 0.3744012117385864\n",
      "Epoch 4905, Loss: 1.4846477508544922, Final Batch Loss: 0.26442596316337585\n",
      "Epoch 4906, Loss: 1.5449443608522415, Final Batch Loss: 0.3756556808948517\n",
      "Epoch 4907, Loss: 1.6674858331680298, Final Batch Loss: 0.3445225954055786\n",
      "Epoch 4908, Loss: 1.3730235546827316, Final Batch Loss: 0.26482877135276794\n",
      "Epoch 4909, Loss: 1.53050197660923, Final Batch Loss: 0.27321121096611023\n",
      "Epoch 4910, Loss: 1.4160068333148956, Final Batch Loss: 0.3219578266143799\n",
      "Epoch 4911, Loss: 1.5384137630462646, Final Batch Loss: 0.2554576098918915\n",
      "Epoch 4912, Loss: 1.414039134979248, Final Batch Loss: 0.24495792388916016\n",
      "Epoch 4913, Loss: 1.3363610655069351, Final Batch Loss: 0.27866631746292114\n",
      "Epoch 4914, Loss: 1.5752846002578735, Final Batch Loss: 0.27129465341567993\n",
      "Epoch 4915, Loss: 1.5166246592998505, Final Batch Loss: 0.2727106809616089\n",
      "Epoch 4916, Loss: 1.4588367640972137, Final Batch Loss: 0.2991245985031128\n",
      "Epoch 4917, Loss: 1.4350555539131165, Final Batch Loss: 0.294604629278183\n",
      "Epoch 4918, Loss: 1.465343788266182, Final Batch Loss: 0.3087756633758545\n",
      "Epoch 4919, Loss: 1.5533736646175385, Final Batch Loss: 0.33723729848861694\n",
      "Epoch 4920, Loss: 1.365835651755333, Final Batch Loss: 0.3470134735107422\n",
      "Epoch 4921, Loss: 1.4767178297042847, Final Batch Loss: 0.2830463647842407\n",
      "Epoch 4922, Loss: 1.3911566585302353, Final Batch Loss: 0.2973383069038391\n",
      "Epoch 4923, Loss: 1.5028753727674484, Final Batch Loss: 0.23096607625484467\n",
      "Epoch 4924, Loss: 1.467337042093277, Final Batch Loss: 0.28707650303840637\n",
      "Epoch 4925, Loss: 1.5962426960468292, Final Batch Loss: 0.28112685680389404\n",
      "Epoch 4926, Loss: 1.433458149433136, Final Batch Loss: 0.21193274855613708\n",
      "Epoch 4927, Loss: 1.4469541162252426, Final Batch Loss: 0.2352311760187149\n",
      "Epoch 4928, Loss: 1.3884994983673096, Final Batch Loss: 0.2441350668668747\n",
      "Epoch 4929, Loss: 1.4004357904195786, Final Batch Loss: 0.36076489090919495\n",
      "Epoch 4930, Loss: 1.5888735055923462, Final Batch Loss: 0.37952348589897156\n",
      "Epoch 4931, Loss: 1.4037987142801285, Final Batch Loss: 0.2997760474681854\n",
      "Epoch 4932, Loss: 1.4625443816184998, Final Batch Loss: 0.23528867959976196\n",
      "Epoch 4933, Loss: 1.443430781364441, Final Batch Loss: 0.28242194652557373\n",
      "Epoch 4934, Loss: 1.276536911725998, Final Batch Loss: 0.27218565344810486\n",
      "Epoch 4935, Loss: 1.5578041672706604, Final Batch Loss: 0.32900774478912354\n",
      "Epoch 4936, Loss: 1.5261916816234589, Final Batch Loss: 0.23485878109931946\n",
      "Epoch 4937, Loss: 1.280687764286995, Final Batch Loss: 0.24988992512226105\n",
      "Epoch 4938, Loss: 1.4626654237508774, Final Batch Loss: 0.3426685333251953\n",
      "Epoch 4939, Loss: 1.43197700381279, Final Batch Loss: 0.3227607011795044\n",
      "Epoch 4940, Loss: 1.4701668620109558, Final Batch Loss: 0.30117741227149963\n",
      "Epoch 4941, Loss: 1.6980528831481934, Final Batch Loss: 0.30321797728538513\n",
      "Epoch 4942, Loss: 1.5314282178878784, Final Batch Loss: 0.40273866057395935\n",
      "Epoch 4943, Loss: 1.498188704252243, Final Batch Loss: 0.3307652473449707\n",
      "Epoch 4944, Loss: 1.4158058166503906, Final Batch Loss: 0.3022107779979706\n",
      "Epoch 4945, Loss: 1.4249334186315536, Final Batch Loss: 0.3517371416091919\n",
      "Epoch 4946, Loss: 1.5286410748958588, Final Batch Loss: 0.2697855532169342\n",
      "Epoch 4947, Loss: 1.45234976708889, Final Batch Loss: 0.3617081344127655\n",
      "Epoch 4948, Loss: 1.5333543121814728, Final Batch Loss: 0.3149016797542572\n",
      "Epoch 4949, Loss: 1.4078377336263657, Final Batch Loss: 0.2386588752269745\n",
      "Epoch 4950, Loss: 1.4446256458759308, Final Batch Loss: 0.2826088070869446\n",
      "Epoch 4951, Loss: 1.4088698327541351, Final Batch Loss: 0.2581529915332794\n",
      "Epoch 4952, Loss: 1.5605089366436005, Final Batch Loss: 0.3786908984184265\n",
      "Epoch 4953, Loss: 1.4619541019201279, Final Batch Loss: 0.25553178787231445\n",
      "Epoch 4954, Loss: 1.411984145641327, Final Batch Loss: 0.2282823771238327\n",
      "Epoch 4955, Loss: 1.5632933378219604, Final Batch Loss: 0.28461670875549316\n",
      "Epoch 4956, Loss: 1.3635364770889282, Final Batch Loss: 0.31453725695610046\n",
      "Epoch 4957, Loss: 1.5441429913043976, Final Batch Loss: 0.39912426471710205\n",
      "Epoch 4958, Loss: 1.5355505645275116, Final Batch Loss: 0.3565792441368103\n",
      "Epoch 4959, Loss: 1.5809630751609802, Final Batch Loss: 0.29939937591552734\n",
      "Epoch 4960, Loss: 1.5492767691612244, Final Batch Loss: 0.3303573727607727\n",
      "Epoch 4961, Loss: 1.5309657752513885, Final Batch Loss: 0.20380550622940063\n",
      "Epoch 4962, Loss: 1.4278618097305298, Final Batch Loss: 0.30813872814178467\n",
      "Epoch 4963, Loss: 1.4373585730791092, Final Batch Loss: 0.2915278971195221\n",
      "Epoch 4964, Loss: 1.364957720041275, Final Batch Loss: 0.24788469076156616\n",
      "Epoch 4965, Loss: 1.384300023317337, Final Batch Loss: 0.24573829770088196\n",
      "Epoch 4966, Loss: 1.5968801826238632, Final Batch Loss: 0.4323536157608032\n",
      "Epoch 4967, Loss: 1.3551879674196243, Final Batch Loss: 0.35692182183265686\n",
      "Epoch 4968, Loss: 1.5023848414421082, Final Batch Loss: 0.28322795033454895\n",
      "Epoch 4969, Loss: 1.397426038980484, Final Batch Loss: 0.3253289759159088\n",
      "Epoch 4970, Loss: 1.4250240325927734, Final Batch Loss: 0.3226676285266876\n",
      "Epoch 4971, Loss: 1.433578982949257, Final Batch Loss: 0.3338853120803833\n",
      "Epoch 4972, Loss: 1.47822767496109, Final Batch Loss: 0.31993958353996277\n",
      "Epoch 4973, Loss: 1.4915973991155624, Final Batch Loss: 0.23966346681118011\n",
      "Epoch 4974, Loss: 1.4176999926567078, Final Batch Loss: 0.2670010030269623\n",
      "Epoch 4975, Loss: 1.4897908568382263, Final Batch Loss: 0.3054331839084625\n",
      "Epoch 4976, Loss: 1.426974132657051, Final Batch Loss: 0.31967681646347046\n",
      "Epoch 4977, Loss: 1.5436790436506271, Final Batch Loss: 0.3033015727996826\n",
      "Epoch 4978, Loss: 1.3504296243190765, Final Batch Loss: 0.25388583540916443\n",
      "Epoch 4979, Loss: 1.437944233417511, Final Batch Loss: 0.28244465589523315\n",
      "Epoch 4980, Loss: 1.3842002898454666, Final Batch Loss: 0.21829108893871307\n",
      "Epoch 4981, Loss: 1.3810203671455383, Final Batch Loss: 0.25952455401420593\n",
      "Epoch 4982, Loss: 1.4490006566047668, Final Batch Loss: 0.2917367219924927\n",
      "Epoch 4983, Loss: 1.540585771203041, Final Batch Loss: 0.43395891785621643\n",
      "Epoch 4984, Loss: 1.5215570330619812, Final Batch Loss: 0.43537071347236633\n",
      "Epoch 4985, Loss: 1.3588927686214447, Final Batch Loss: 0.2820942997932434\n",
      "Epoch 4986, Loss: 1.6069869846105576, Final Batch Loss: 0.41702547669410706\n",
      "Epoch 4987, Loss: 1.2897847890853882, Final Batch Loss: 0.26199135184288025\n",
      "Epoch 4988, Loss: 1.3357678055763245, Final Batch Loss: 0.2459174394607544\n",
      "Epoch 4989, Loss: 1.4605446755886078, Final Batch Loss: 0.30346736311912537\n",
      "Epoch 4990, Loss: 1.3956342786550522, Final Batch Loss: 0.29514986276626587\n",
      "Epoch 4991, Loss: 1.4379267394542694, Final Batch Loss: 0.30165761709213257\n",
      "Epoch 4992, Loss: 1.4168633818626404, Final Batch Loss: 0.2766091227531433\n",
      "Epoch 4993, Loss: 1.5726022124290466, Final Batch Loss: 0.3314073085784912\n",
      "Epoch 4994, Loss: 1.4954001009464264, Final Batch Loss: 0.35136541724205017\n",
      "Epoch 4995, Loss: 1.401021271944046, Final Batch Loss: 0.22454731166362762\n",
      "Epoch 4996, Loss: 1.416138395667076, Final Batch Loss: 0.372966468334198\n",
      "Epoch 4997, Loss: 1.2775493264198303, Final Batch Loss: 0.22910133004188538\n",
      "Epoch 4998, Loss: 1.4445395767688751, Final Batch Loss: 0.27404871582984924\n",
      "Epoch 4999, Loss: 1.3444289863109589, Final Batch Loss: 0.2533368766307831\n",
      "Epoch 5000, Loss: 1.442491203546524, Final Batch Loss: 0.30441200733184814\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  1  0]\n",
      " [ 0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   1  0  0]\n",
      " [ 0  0  2  0  0  4  0  0  0  0  0  0  0  0  2  0  0  1  0  0  0  0  0  1\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  6  0  0  0  0  0  3  0  0  0  0  0  1  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 13  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 13  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0 13  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0\n",
      "   0  0  2]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0 16\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  13  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  3  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0 12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        20\n",
      "           1    0.92308   0.92308   0.92308        13\n",
      "           2    0.80000   1.00000   0.88889         8\n",
      "           3    1.00000   0.93333   0.96552        15\n",
      "           4    1.00000   0.90000   0.94737        10\n",
      "           5    0.57143   0.40000   0.47059        10\n",
      "           6    1.00000   1.00000   1.00000        17\n",
      "           7    1.00000   1.00000   1.00000        11\n",
      "           8    1.00000   0.54545   0.70588        11\n",
      "           9    1.00000   1.00000   1.00000        13\n",
      "          10    1.00000   1.00000   1.00000        13\n",
      "          11    0.88889   1.00000   0.94118         8\n",
      "          12    1.00000   1.00000   1.00000         5\n",
      "          13    1.00000   0.92857   0.96296        14\n",
      "          14    0.58333   0.77778   0.66667         9\n",
      "          15    1.00000   1.00000   1.00000        10\n",
      "          16    1.00000   1.00000   1.00000        11\n",
      "          17    0.87500   1.00000   0.93333         7\n",
      "          18    1.00000   1.00000   1.00000         5\n",
      "          19    1.00000   1.00000   1.00000         4\n",
      "          20    0.90000   0.90000   0.90000        10\n",
      "          21    0.90909   1.00000   0.95238        10\n",
      "          22    1.00000   1.00000   1.00000         7\n",
      "          23    0.94118   0.88889   0.91429        18\n",
      "          24    0.92857   1.00000   0.96296        13\n",
      "          25    0.75000   1.00000   0.85714         3\n",
      "          26    0.85714   1.00000   0.92308        12\n",
      "\n",
      "    accuracy                        0.93031       287\n",
      "   macro avg    0.92325   0.93323   0.92279       287\n",
      "weighted avg    0.93463   0.93031   0.92763       287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_1 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_1 = np.zeros(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_2 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_2 = np.ones(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_3 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_3 = np.ones(n_samples) + 1\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_4 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_4 = np.ones(n_samples) + 2\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_5 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_5 = np.ones(n_samples) + 3\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_6 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_6 = np.ones(n_samples) + 4\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_7 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_7 = np.ones(n_samples) + 5\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_8 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_8 = np.ones(n_samples) + 6\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_9 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_9 = np.ones(n_samples) + 7\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U3A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_10 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_10 = np.ones(n_samples) + 8\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U3A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_11 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_11 = np.ones(n_samples) + 9\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U3A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_12 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_12 = np.ones(n_samples) + 10\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U4A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_13 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_13 = np.ones(n_samples) + 11\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U4A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_14 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_14 = np.ones(n_samples) + 12\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U4A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_15 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_15 = np.ones(n_samples) + 13\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U5A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_16 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_16 = np.ones(n_samples) + 14\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U5A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_17 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_17 = np.ones(n_samples) + 15\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U5A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_18 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_18 = np.ones(n_samples) + 16\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U6A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_19 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_19 = np.ones(n_samples) + 17\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U6A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_20 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_20 = np.ones(n_samples) + 18\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U6A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_21 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_21 = np.ones(n_samples) + 19\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U7A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_22 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_22 = np.ones(n_samples) + 20\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U7A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_23 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_23 = np.ones(n_samples) + 21\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U7A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_24 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_24 = np.ones(n_samples) + 22\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U8A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_25 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_25 = np.ones(n_samples) + 23\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U8A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_26 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_26 = np.ones(n_samples) + 24\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U8A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_27 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_27 = np.ones(n_samples) + 25\n",
    "\n",
    "fake_features = np.concatenate((fake_features_1, fake_features_2, fake_features_3, fake_features_4, fake_features_5, fake_features_6,\n",
    "                         fake_features_7, fake_features_8, fake_features_9, fake_features_10, fake_features_11, fake_features_12,\n",
    "                               fake_features_13, fake_features_14, fake_features_15, fake_features_16, fake_features_17, fake_features_18,\n",
    "                               fake_features_19, fake_features_20, fake_features_21, fake_features_22, fake_features_23, fake_features_24,\n",
    "                               fake_features_25, fake_features_26, fake_features_27))\n",
    "fake_labels = np.concatenate((y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9, y_10, y_11, y_12, y_13, y_14, y_15, y_16, y_17, y_18, y_19, y_20, y_21, y_22, y_23, y_24, y_25, y_26, y_27))\n",
    "\n",
    "fake_features = torch.Tensor(fake_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  6  0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0\n",
      "   0  0  8]\n",
      " [ 0  0  0 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0 16  0  2  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   1  0  0]\n",
      " [ 0  0  5  0  0  1  0  0  0  0  0  2  0  0  1  0  0  9  0  0  1  0  0  0\n",
      "   0  0  1]\n",
      " [ 0  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  2  0  0  9  0  0  5  0  0  0  0  0  2  0  0  2  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  0 16  3  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  3  0  0  0  0  0 17  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  4  0  0  9  0  0  2  0  0  0  0  0  0  0  0  5\n",
      "   0  0  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  2  8  0  0\n",
      "   0  0  0]\n",
      " [ 0  2  0  0  0  0  0  0  0  0  5  0  1  8  0  0  0  0  0  0  0  0  0  0\n",
      "   4  0  0]\n",
      " [ 0  0 13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0\n",
      "   0  0  4]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 20  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0 17  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  2  0  0  0  0  0  2  0  0  0  0  0  4  0  0 10  0  0  0  0  0  0\n",
      "   0  0  2]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 20  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 20  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  4  0  0  8  0  0  7\n",
      "   0  0  0]\n",
      " [ 0  0  0  7  0  0  0  0  0  2  0  0  3  0  0  1  0  1  0  0  0  6  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0 10  0\n",
      "   0  3  0]\n",
      " [ 0  0  0  0  0  0  1  0  2  0  0  1  0  0  0  0  0  1  0  0  7  0  0  8\n",
      "   0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  3  0  0  1  0  0  0  0  0  0  0  0\n",
      "  15  0  0]\n",
      " [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0 17  0]\n",
      " [ 0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0 19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    0.95238   1.00000   0.97561        20\n",
      "         1.0    0.86957   1.00000   0.93023        20\n",
      "         2.0    0.22222   0.30000   0.25532        20\n",
      "         3.0    0.74074   1.00000   0.85106        20\n",
      "         4.0    0.80000   0.80000   0.80000        20\n",
      "         5.0    0.25000   0.05000   0.08333        20\n",
      "         6.0    0.83333   1.00000   0.90909        20\n",
      "         7.0    0.66667   0.60000   0.63158        20\n",
      "         8.0    0.52941   0.45000   0.48649        20\n",
      "         9.0    0.59259   0.80000   0.68085        20\n",
      "        10.0    0.42500   0.85000   0.56667        20\n",
      "        11.0    0.52941   0.45000   0.48649        20\n",
      "        12.0    0.00000   0.00000   0.00000        20\n",
      "        13.0    1.00000   0.40000   0.57143        20\n",
      "        14.0    0.00000   0.00000   0.00000        20\n",
      "        15.0    0.90909   1.00000   0.95238        20\n",
      "        16.0    1.00000   0.85000   0.91892        20\n",
      "        17.0    0.33333   0.50000   0.40000        20\n",
      "        18.0    1.00000   1.00000   1.00000        20\n",
      "        19.0    1.00000   1.00000   1.00000        20\n",
      "        20.0    0.40000   0.40000   0.40000        20\n",
      "        21.0    0.42857   0.30000   0.35294        20\n",
      "        22.0    1.00000   0.50000   0.66667        20\n",
      "        23.0    0.40000   0.40000   0.40000        20\n",
      "        24.0    0.75000   0.75000   0.75000        20\n",
      "        25.0    0.85000   0.85000   0.85000        20\n",
      "        26.0    0.55882   0.95000   0.70370        20\n",
      "\n",
      "    accuracy                        0.63704       540\n",
      "   macro avg    0.63115   0.63704   0.61566       540\n",
      "weighted avg    0.63115   0.63704   0.61566       540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5, zero_division = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
