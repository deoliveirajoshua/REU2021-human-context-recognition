{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>42 tGravityAcc-mean()-Y</th>\n",
       "      <th>43 tGravityAcc-mean()-Z</th>\n",
       "      <th>51 tGravityAcc-max()-Y</th>\n",
       "      <th>52 tGravityAcc-max()-Z</th>\n",
       "      <th>54 tGravityAcc-min()-Y</th>\n",
       "      <th>55 tGravityAcc-min()-Z</th>\n",
       "      <th>56 tGravityAcc-sma()</th>\n",
       "      <th>58 tGravityAcc-energy()-Y</th>\n",
       "      <th>59 tGravityAcc-energy()-Z</th>\n",
       "      <th>475 fBodyGyro-bandsEnergy()-1,8</th>\n",
       "      <th>...</th>\n",
       "      <th>272 fBodyAcc-mad()-X</th>\n",
       "      <th>282 fBodyAcc-energy()-X</th>\n",
       "      <th>311 fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <th>315 fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <th>382 fBodyAccJerk-bandsEnergy()-1,8</th>\n",
       "      <th>504 fBodyAccMag-std()</th>\n",
       "      <th>505 fBodyAccMag-mad()</th>\n",
       "      <th>509 fBodyAccMag-energy()</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.140840</td>\n",
       "      <td>0.115375</td>\n",
       "      <td>-0.161265</td>\n",
       "      <td>0.124660</td>\n",
       "      <td>-0.123213</td>\n",
       "      <td>0.056483</td>\n",
       "      <td>-0.375426</td>\n",
       "      <td>-0.970905</td>\n",
       "      <td>-0.975510</td>\n",
       "      <td>-0.999454</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.996889</td>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.956134</td>\n",
       "      <td>-0.948870</td>\n",
       "      <td>-0.998285</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.141551</td>\n",
       "      <td>0.109379</td>\n",
       "      <td>-0.161343</td>\n",
       "      <td>0.122586</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.383430</td>\n",
       "      <td>-0.970583</td>\n",
       "      <td>-0.978500</td>\n",
       "      <td>-0.999856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.997890</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.975866</td>\n",
       "      <td>-0.975777</td>\n",
       "      <td>-0.999472</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.142010</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.094566</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.401602</td>\n",
       "      <td>-0.970368</td>\n",
       "      <td>-0.981672</td>\n",
       "      <td>-0.999954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.994097</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.999972</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.989015</td>\n",
       "      <td>-0.985594</td>\n",
       "      <td>-0.999807</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.143976</td>\n",
       "      <td>0.099850</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.093425</td>\n",
       "      <td>-0.121336</td>\n",
       "      <td>0.095753</td>\n",
       "      <td>-0.400278</td>\n",
       "      <td>-0.969400</td>\n",
       "      <td>-0.982420</td>\n",
       "      <td>-0.999931</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.994547</td>\n",
       "      <td>-0.999975</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-0.986742</td>\n",
       "      <td>-0.983524</td>\n",
       "      <td>-0.999770</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.148750</td>\n",
       "      <td>0.094486</td>\n",
       "      <td>-0.166786</td>\n",
       "      <td>0.091682</td>\n",
       "      <td>-0.121834</td>\n",
       "      <td>0.094059</td>\n",
       "      <td>-0.400477</td>\n",
       "      <td>-0.967051</td>\n",
       "      <td>-0.984363</td>\n",
       "      <td>-0.999926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.997725</td>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-0.990063</td>\n",
       "      <td>-0.992324</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>-0.222004</td>\n",
       "      <td>-0.039492</td>\n",
       "      <td>-0.214233</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.071977</td>\n",
       "      <td>-0.405132</td>\n",
       "      <td>-0.918375</td>\n",
       "      <td>-0.995193</td>\n",
       "      <td>-0.053258</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050748</td>\n",
       "      <td>-0.674230</td>\n",
       "      <td>-0.666429</td>\n",
       "      <td>-0.668164</td>\n",
       "      <td>-0.839256</td>\n",
       "      <td>-0.232600</td>\n",
       "      <td>-0.007392</td>\n",
       "      <td>-0.584282</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>-0.242054</td>\n",
       "      <td>-0.039863</td>\n",
       "      <td>-0.231477</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.358934</td>\n",
       "      <td>-0.902880</td>\n",
       "      <td>-0.995151</td>\n",
       "      <td>-0.029411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177661</td>\n",
       "      <td>-0.705580</td>\n",
       "      <td>-0.704444</td>\n",
       "      <td>-0.705435</td>\n",
       "      <td>-0.854278</td>\n",
       "      <td>-0.275373</td>\n",
       "      <td>-0.172448</td>\n",
       "      <td>-0.632536</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>-0.236950</td>\n",
       "      <td>-0.026805</td>\n",
       "      <td>-0.249134</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.216004</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.377025</td>\n",
       "      <td>-0.907561</td>\n",
       "      <td>-0.995450</td>\n",
       "      <td>0.161404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.249486</td>\n",
       "      <td>-0.692379</td>\n",
       "      <td>-0.674515</td>\n",
       "      <td>-0.684729</td>\n",
       "      <td>-0.815380</td>\n",
       "      <td>-0.220288</td>\n",
       "      <td>-0.216074</td>\n",
       "      <td>-0.641170</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>-0.233230</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>-0.244267</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.210542</td>\n",
       "      <td>-0.040009</td>\n",
       "      <td>-0.440050</td>\n",
       "      <td>-0.910648</td>\n",
       "      <td>-0.998824</td>\n",
       "      <td>0.193585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.247028</td>\n",
       "      <td>-0.693098</td>\n",
       "      <td>-0.677215</td>\n",
       "      <td>-0.685088</td>\n",
       "      <td>-0.822905</td>\n",
       "      <td>-0.234539</td>\n",
       "      <td>-0.220443</td>\n",
       "      <td>-0.663579</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>-0.233292</td>\n",
       "      <td>-0.020954</td>\n",
       "      <td>-0.240956</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.212149</td>\n",
       "      <td>-0.047491</td>\n",
       "      <td>-0.432003</td>\n",
       "      <td>-0.910579</td>\n",
       "      <td>-0.998144</td>\n",
       "      <td>-0.129277</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114475</td>\n",
       "      <td>-0.731037</td>\n",
       "      <td>-0.728519</td>\n",
       "      <td>-0.727441</td>\n",
       "      <td>-0.834215</td>\n",
       "      <td>-0.342670</td>\n",
       "      <td>-0.146649</td>\n",
       "      <td>-0.698087</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      42 tGravityAcc-mean()-Y  43 tGravityAcc-mean()-Z  \\\n",
       "0                   -0.140840                 0.115375   \n",
       "1                   -0.141551                 0.109379   \n",
       "2                   -0.142010                 0.101884   \n",
       "3                   -0.143976                 0.099850   \n",
       "4                   -0.148750                 0.094486   \n",
       "...                       ...                      ...   \n",
       "7347                -0.222004                -0.039492   \n",
       "7348                -0.242054                -0.039863   \n",
       "7349                -0.236950                -0.026805   \n",
       "7350                -0.233230                -0.004984   \n",
       "7351                -0.233292                -0.020954   \n",
       "\n",
       "      51 tGravityAcc-max()-Y  52 tGravityAcc-max()-Z  54 tGravityAcc-min()-Y  \\\n",
       "0                  -0.161265                0.124660               -0.123213   \n",
       "1                  -0.161343                0.122586               -0.114893   \n",
       "2                  -0.163711                0.094566               -0.114893   \n",
       "3                  -0.163711                0.093425               -0.121336   \n",
       "4                  -0.166786                0.091682               -0.121834   \n",
       "...                      ...                     ...                     ...   \n",
       "7347               -0.214233               -0.016391               -0.234998   \n",
       "7348               -0.231477               -0.016391               -0.234998   \n",
       "7349               -0.249134                0.024684               -0.216004   \n",
       "7350               -0.244267                0.024684               -0.210542   \n",
       "7351               -0.240956                0.003031               -0.212149   \n",
       "\n",
       "      55 tGravityAcc-min()-Z  56 tGravityAcc-sma()  58 tGravityAcc-energy()-Y  \\\n",
       "0                   0.056483             -0.375426                  -0.970905   \n",
       "1                   0.102764             -0.383430                  -0.970583   \n",
       "2                   0.102764             -0.401602                  -0.970368   \n",
       "3                   0.095753             -0.400278                  -0.969400   \n",
       "4                   0.094059             -0.400477                  -0.967051   \n",
       "...                      ...                   ...                        ...   \n",
       "7347               -0.071977             -0.405132                  -0.918375   \n",
       "7348               -0.068919             -0.358934                  -0.902880   \n",
       "7349               -0.068919             -0.377025                  -0.907561   \n",
       "7350               -0.040009             -0.440050                  -0.910648   \n",
       "7351               -0.047491             -0.432003                  -0.910579   \n",
       "\n",
       "      59 tGravityAcc-energy()-Z  475 fBodyGyro-bandsEnergy()-1,8  ...  \\\n",
       "0                     -0.975510                        -0.999454  ...   \n",
       "1                     -0.978500                        -0.999856  ...   \n",
       "2                     -0.981672                        -0.999954  ...   \n",
       "3                     -0.982420                        -0.999931  ...   \n",
       "4                     -0.984363                        -0.999926  ...   \n",
       "...                         ...                              ...  ...   \n",
       "7347                  -0.995193                        -0.053258  ...   \n",
       "7348                  -0.995151                        -0.029411  ...   \n",
       "7349                  -0.995450                         0.161404  ...   \n",
       "7350                  -0.998824                         0.193585  ...   \n",
       "7351                  -0.998144                        -0.129277  ...   \n",
       "\n",
       "      272 fBodyAcc-mad()-X  282 fBodyAcc-energy()-X  \\\n",
       "0                -0.996889                -0.999968   \n",
       "1                -0.997890                -0.999991   \n",
       "2                -0.994097                -0.999969   \n",
       "3                -0.994547                -0.999975   \n",
       "4                -0.997725                -0.999990   \n",
       "...                    ...                      ...   \n",
       "7347             -0.050748                -0.674230   \n",
       "7348             -0.177661                -0.705580   \n",
       "7349             -0.249486                -0.692379   \n",
       "7350             -0.247028                -0.693098   \n",
       "7351             -0.114475                -0.731037   \n",
       "\n",
       "      311 fBodyAcc-bandsEnergy()-1,16  315 fBodyAcc-bandsEnergy()-1,24  \\\n",
       "0                           -0.999969                        -0.999971   \n",
       "1                           -0.999994                        -0.999992   \n",
       "2                           -0.999983                        -0.999972   \n",
       "3                           -0.999986                        -0.999977   \n",
       "4                           -0.999993                        -0.999991   \n",
       "...                               ...                              ...   \n",
       "7347                        -0.666429                        -0.668164   \n",
       "7348                        -0.704444                        -0.705435   \n",
       "7349                        -0.674515                        -0.684729   \n",
       "7350                        -0.677215                        -0.685088   \n",
       "7351                        -0.728519                        -0.727441   \n",
       "\n",
       "      382 fBodyAccJerk-bandsEnergy()-1,8  504 fBodyAccMag-std()  \\\n",
       "0                              -0.999986              -0.956134   \n",
       "1                              -0.999996              -0.975866   \n",
       "2                              -0.999994              -0.989015   \n",
       "3                              -0.999998              -0.986742   \n",
       "4                              -0.999995              -0.990063   \n",
       "...                                  ...                    ...   \n",
       "7347                           -0.839256              -0.232600   \n",
       "7348                           -0.854278              -0.275373   \n",
       "7349                           -0.815380              -0.220288   \n",
       "7350                           -0.822905              -0.234539   \n",
       "7351                           -0.834215              -0.342670   \n",
       "\n",
       "      505 fBodyAccMag-mad()  509 fBodyAccMag-energy()  Activity  Subject  \n",
       "0                 -0.948870                 -0.998285         5        1  \n",
       "1                 -0.975777                 -0.999472         5        1  \n",
       "2                 -0.985594                 -0.999807         5        1  \n",
       "3                 -0.983524                 -0.999770         5        1  \n",
       "4                 -0.992324                 -0.999873         5        1  \n",
       "...                     ...                       ...       ...      ...  \n",
       "7347              -0.007392                 -0.584282         2       30  \n",
       "7348              -0.172448                 -0.632536         2       30  \n",
       "7349              -0.216074                 -0.641170         2       30  \n",
       "7350              -0.220443                 -0.663579         2       30  \n",
       "7351              -0.146649                 -0.698087         2       30  \n",
       "\n",
       "[7352 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_names = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "train_column_names = train_names.values.tolist()\n",
    "train_column_names = [k for row in train_column_names for k in row]\n",
    "\n",
    "train_data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None)\n",
    "train_data.columns = train_column_names\n",
    "\n",
    "### Single dataframe column\n",
    "y_train = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "y_train.columns = ['Activity']\n",
    "\n",
    "y_train_subject = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "y_train_subject.columns = ['Subject']\n",
    "\n",
    "X_train_1 = train_data[sub_features]\n",
    "X_train_2 = train_data[act_features]\n",
    "X_train_data = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "X_train_data = pd.concat([X_train_data, y_train, y_train_subject], axis = 1)\n",
    "X_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_data[(X_train_data['Subject'].isin([1, 3, 5, 7, 8, 11, 14, 17, 19])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,:-2].values\n",
    "y_train = X_train_data[(X_train_data['Subject'].isin([1, 3, 5, 7, 8, 11, 14, 17, 19])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,-2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y_train)):\n",
    "    if y_train[k] == 1:\n",
    "        y_train[k] = 0\n",
    "    elif y_train[k] == 3:\n",
    "        y_train[k] = 1\n",
    "    else:\n",
    "        y_train[k] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 30),\n",
    "            classifier_block(30, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.6273252964019775, Final Batch Loss: 1.1055189371109009\n",
      "Epoch 2, Loss: 5.593998432159424, Final Batch Loss: 1.1066615581512451\n",
      "Epoch 3, Loss: 5.567805290222168, Final Batch Loss: 1.1108728647232056\n",
      "Epoch 4, Loss: 5.5329978466033936, Final Batch Loss: 1.1051020622253418\n",
      "Epoch 5, Loss: 5.491449594497681, Final Batch Loss: 1.0931514501571655\n",
      "Epoch 6, Loss: 5.435864329338074, Final Batch Loss: 1.0840715169906616\n",
      "Epoch 7, Loss: 5.359064936637878, Final Batch Loss: 1.0606184005737305\n",
      "Epoch 8, Loss: 5.253962516784668, Final Batch Loss: 1.032117486000061\n",
      "Epoch 9, Loss: 5.118733763694763, Final Batch Loss: 1.0104141235351562\n",
      "Epoch 10, Loss: 4.939127326011658, Final Batch Loss: 0.9741478562355042\n",
      "Epoch 11, Loss: 4.724250018596649, Final Batch Loss: 0.9215981364250183\n",
      "Epoch 12, Loss: 4.410037338733673, Final Batch Loss: 0.8475560545921326\n",
      "Epoch 13, Loss: 4.104855120182037, Final Batch Loss: 0.834884762763977\n",
      "Epoch 14, Loss: 3.7705323696136475, Final Batch Loss: 0.68147873878479\n",
      "Epoch 15, Loss: 3.475633680820465, Final Batch Loss: 0.7062365412712097\n",
      "Epoch 16, Loss: 3.2308165431022644, Final Batch Loss: 0.6375243663787842\n",
      "Epoch 17, Loss: 3.0214598774909973, Final Batch Loss: 0.6219761371612549\n",
      "Epoch 18, Loss: 2.8179209232330322, Final Batch Loss: 0.530910313129425\n",
      "Epoch 19, Loss: 2.652842700481415, Final Batch Loss: 0.5127267241477966\n",
      "Epoch 20, Loss: 2.544580966234207, Final Batch Loss: 0.549629807472229\n",
      "Epoch 21, Loss: 2.3307969868183136, Final Batch Loss: 0.5006789565086365\n",
      "Epoch 22, Loss: 2.1387990713119507, Final Batch Loss: 0.40083742141723633\n",
      "Epoch 23, Loss: 1.9802795350551605, Final Batch Loss: 0.3770981431007385\n",
      "Epoch 24, Loss: 1.790009468793869, Final Batch Loss: 0.35605308413505554\n",
      "Epoch 25, Loss: 1.5775029063224792, Final Batch Loss: 0.28505685925483704\n",
      "Epoch 26, Loss: 1.4433822929859161, Final Batch Loss: 0.26530903577804565\n",
      "Epoch 27, Loss: 1.3011984825134277, Final Batch Loss: 0.21984313428401947\n",
      "Epoch 28, Loss: 1.1992523223161697, Final Batch Loss: 0.19281922280788422\n",
      "Epoch 29, Loss: 1.1154054403305054, Final Batch Loss: 0.2573665976524353\n",
      "Epoch 30, Loss: 1.1370925158262253, Final Batch Loss: 0.3130579888820648\n",
      "Epoch 31, Loss: 1.0943787097930908, Final Batch Loss: 0.23202522099018097\n",
      "Epoch 32, Loss: 1.1347460001707077, Final Batch Loss: 0.23998145759105682\n",
      "Epoch 33, Loss: 1.0165738761425018, Final Batch Loss: 0.22034823894500732\n",
      "Epoch 34, Loss: 1.0455861985683441, Final Batch Loss: 0.24103346467018127\n",
      "Epoch 35, Loss: 0.9271988272666931, Final Batch Loss: 0.1841576248407364\n",
      "Epoch 36, Loss: 0.9624494910240173, Final Batch Loss: 0.20696450769901276\n",
      "Epoch 37, Loss: 0.9524117559194565, Final Batch Loss: 0.19508546590805054\n",
      "Epoch 38, Loss: 0.9677155911922455, Final Batch Loss: 0.18126912415027618\n",
      "Epoch 39, Loss: 0.8633885160088539, Final Batch Loss: 0.1192823126912117\n",
      "Epoch 40, Loss: 0.9131466001272202, Final Batch Loss: 0.1702946275472641\n",
      "Epoch 41, Loss: 0.969581812620163, Final Batch Loss: 0.1804565042257309\n",
      "Epoch 42, Loss: 0.913994699716568, Final Batch Loss: 0.20980916917324066\n",
      "Epoch 43, Loss: 0.8575897663831711, Final Batch Loss: 0.12601523101329803\n",
      "Epoch 44, Loss: 0.8618058562278748, Final Batch Loss: 0.17434126138687134\n",
      "Epoch 45, Loss: 0.8990685343742371, Final Batch Loss: 0.1786884367465973\n",
      "Epoch 46, Loss: 0.8213638216257095, Final Batch Loss: 0.15170350670814514\n",
      "Epoch 47, Loss: 0.8601935133337975, Final Batch Loss: 0.2157590240240097\n",
      "Epoch 48, Loss: 0.8215453922748566, Final Batch Loss: 0.1424904614686966\n",
      "Epoch 49, Loss: 0.9266050904989243, Final Batch Loss: 0.20248089730739594\n",
      "Epoch 50, Loss: 0.7725555896759033, Final Batch Loss: 0.16172365844249725\n",
      "Epoch 51, Loss: 0.8286960273981094, Final Batch Loss: 0.2100292146205902\n",
      "Epoch 52, Loss: 0.7404097691178322, Final Batch Loss: 0.10781662911176682\n",
      "Epoch 53, Loss: 0.7818108648061752, Final Batch Loss: 0.16419970989227295\n",
      "Epoch 54, Loss: 0.7757987976074219, Final Batch Loss: 0.23741622269153595\n",
      "Epoch 55, Loss: 0.7578013017773628, Final Batch Loss: 0.10461606830358505\n",
      "Epoch 56, Loss: 0.8552147597074509, Final Batch Loss: 0.16503015160560608\n",
      "Epoch 57, Loss: 0.7255383729934692, Final Batch Loss: 0.12622928619384766\n",
      "Epoch 58, Loss: 0.746622622013092, Final Batch Loss: 0.1641477793455124\n",
      "Epoch 59, Loss: 0.7384824752807617, Final Batch Loss: 0.16717159748077393\n",
      "Epoch 60, Loss: 0.7608961910009384, Final Batch Loss: 0.1357864886522293\n",
      "Epoch 61, Loss: 0.7833388298749924, Final Batch Loss: 0.1339978724718094\n",
      "Epoch 62, Loss: 0.7546110898256302, Final Batch Loss: 0.10203927755355835\n",
      "Epoch 63, Loss: 0.7758170515298843, Final Batch Loss: 0.17043238878250122\n",
      "Epoch 64, Loss: 0.7275471016764641, Final Batch Loss: 0.17713548243045807\n",
      "Epoch 65, Loss: 0.722547747194767, Final Batch Loss: 0.15969964861869812\n",
      "Epoch 66, Loss: 0.8008550852537155, Final Batch Loss: 0.15249747037887573\n",
      "Epoch 67, Loss: 0.7092732340097427, Final Batch Loss: 0.1252364218235016\n",
      "Epoch 68, Loss: 0.6991779878735542, Final Batch Loss: 0.15438443422317505\n",
      "Epoch 69, Loss: 0.7482464611530304, Final Batch Loss: 0.1302979439496994\n",
      "Epoch 70, Loss: 0.6768329739570618, Final Batch Loss: 0.1362987458705902\n",
      "Epoch 71, Loss: 0.7013784721493721, Final Batch Loss: 0.12713097035884857\n",
      "Epoch 72, Loss: 0.6794189512729645, Final Batch Loss: 0.09845424443483353\n",
      "Epoch 73, Loss: 0.6977111175656319, Final Batch Loss: 0.15931551158428192\n",
      "Epoch 74, Loss: 0.6238240003585815, Final Batch Loss: 0.09637581557035446\n",
      "Epoch 75, Loss: 0.6881023347377777, Final Batch Loss: 0.1275818943977356\n",
      "Epoch 76, Loss: 0.6404567807912827, Final Batch Loss: 0.15439675748348236\n",
      "Epoch 77, Loss: 0.6972529143095016, Final Batch Loss: 0.1762426495552063\n",
      "Epoch 78, Loss: 0.6862408295273781, Final Batch Loss: 0.17426346242427826\n",
      "Epoch 79, Loss: 0.6784382238984108, Final Batch Loss: 0.09058555215597153\n",
      "Epoch 80, Loss: 0.6957831382751465, Final Batch Loss: 0.08888118714094162\n",
      "Epoch 81, Loss: 0.6915101706981659, Final Batch Loss: 0.15493585169315338\n",
      "Epoch 82, Loss: 0.6302583143115044, Final Batch Loss: 0.09347765147686005\n",
      "Epoch 83, Loss: 0.6433743834495544, Final Batch Loss: 0.19847087562084198\n",
      "Epoch 84, Loss: 0.6588258296251297, Final Batch Loss: 0.11562691628932953\n",
      "Epoch 85, Loss: 0.6598510444164276, Final Batch Loss: 0.14182496070861816\n",
      "Epoch 86, Loss: 0.6572898030281067, Final Batch Loss: 0.15017016232013702\n",
      "Epoch 87, Loss: 0.7083533182740211, Final Batch Loss: 0.19974394142627716\n",
      "Epoch 88, Loss: 0.636517271399498, Final Batch Loss: 0.13170360028743744\n",
      "Epoch 89, Loss: 0.5907460376620293, Final Batch Loss: 0.11454660445451736\n",
      "Epoch 90, Loss: 0.6364318430423737, Final Batch Loss: 0.08413413912057877\n",
      "Epoch 91, Loss: 0.6153097823262215, Final Batch Loss: 0.15060202777385712\n",
      "Epoch 92, Loss: 0.6367732211947441, Final Batch Loss: 0.13623584806919098\n",
      "Epoch 93, Loss: 0.6663888245820999, Final Batch Loss: 0.16491247713565826\n",
      "Epoch 94, Loss: 0.6309231892228127, Final Batch Loss: 0.10848143696784973\n",
      "Epoch 95, Loss: 0.5684148147702217, Final Batch Loss: 0.12939418852329254\n",
      "Epoch 96, Loss: 0.5426427945494652, Final Batch Loss: 0.11354710161685944\n",
      "Epoch 97, Loss: 0.5664789453148842, Final Batch Loss: 0.1316838264465332\n",
      "Epoch 98, Loss: 0.5888028293848038, Final Batch Loss: 0.10433007776737213\n",
      "Epoch 99, Loss: 0.6256929263472557, Final Batch Loss: 0.13842761516571045\n",
      "Epoch 100, Loss: 0.5557862594723701, Final Batch Loss: 0.13724754750728607\n",
      "Epoch 101, Loss: 0.6009085848927498, Final Batch Loss: 0.14021478593349457\n",
      "Epoch 102, Loss: 0.5825053304433823, Final Batch Loss: 0.1204514279961586\n",
      "Epoch 103, Loss: 0.5689888745546341, Final Batch Loss: 0.16844451427459717\n",
      "Epoch 104, Loss: 0.5676147639751434, Final Batch Loss: 0.06512750685214996\n",
      "Epoch 105, Loss: 0.577284961938858, Final Batch Loss: 0.12517862021923065\n",
      "Epoch 106, Loss: 0.5846744105219841, Final Batch Loss: 0.1492973417043686\n",
      "Epoch 107, Loss: 0.5863248407840729, Final Batch Loss: 0.14002779126167297\n",
      "Epoch 108, Loss: 0.5647476688027382, Final Batch Loss: 0.1187324970960617\n",
      "Epoch 109, Loss: 0.5859031900763512, Final Batch Loss: 0.09313550591468811\n",
      "Epoch 110, Loss: 0.5440462231636047, Final Batch Loss: 0.09632531553506851\n",
      "Epoch 111, Loss: 0.6204005926847458, Final Batch Loss: 0.1291348785161972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112, Loss: 0.5862623676657677, Final Batch Loss: 0.12663833796977997\n",
      "Epoch 113, Loss: 0.5640844441950321, Final Batch Loss: 0.11607928574085236\n",
      "Epoch 114, Loss: 0.6065390035510063, Final Batch Loss: 0.23162774741649628\n",
      "Epoch 115, Loss: 0.5776652693748474, Final Batch Loss: 0.11576519161462784\n",
      "Epoch 116, Loss: 0.5298555195331573, Final Batch Loss: 0.11600598692893982\n",
      "Epoch 117, Loss: 0.6052809879183769, Final Batch Loss: 0.12256278097629547\n",
      "Epoch 118, Loss: 0.5345054119825363, Final Batch Loss: 0.0787634551525116\n",
      "Epoch 119, Loss: 0.5339070484042168, Final Batch Loss: 0.11096978932619095\n",
      "Epoch 120, Loss: 0.561246745288372, Final Batch Loss: 0.1073949933052063\n",
      "Epoch 121, Loss: 0.5623194575309753, Final Batch Loss: 0.1354391872882843\n",
      "Epoch 122, Loss: 0.5347049385309219, Final Batch Loss: 0.11120527237653732\n",
      "Epoch 123, Loss: 0.556316003203392, Final Batch Loss: 0.11952684074640274\n",
      "Epoch 124, Loss: 0.5145123451948166, Final Batch Loss: 0.09066924452781677\n",
      "Epoch 125, Loss: 0.5339871309697628, Final Batch Loss: 0.09209323674440384\n",
      "Epoch 126, Loss: 0.49346494674682617, Final Batch Loss: 0.10225824266672134\n",
      "Epoch 127, Loss: 0.5019307285547256, Final Batch Loss: 0.11191880702972412\n",
      "Epoch 128, Loss: 0.4813937693834305, Final Batch Loss: 0.15103991329669952\n",
      "Epoch 129, Loss: 0.49812568724155426, Final Batch Loss: 0.08441983908414841\n",
      "Epoch 130, Loss: 0.5455885231494904, Final Batch Loss: 0.1285950094461441\n",
      "Epoch 131, Loss: 0.5111590474843979, Final Batch Loss: 0.14293396472930908\n",
      "Epoch 132, Loss: 0.5019800290465355, Final Batch Loss: 0.08654546737670898\n",
      "Epoch 133, Loss: 0.5350376591086388, Final Batch Loss: 0.09133153408765793\n",
      "Epoch 134, Loss: 0.5392224341630936, Final Batch Loss: 0.1194109246134758\n",
      "Epoch 135, Loss: 0.5237768441438675, Final Batch Loss: 0.14329291880130768\n",
      "Epoch 136, Loss: 0.49996214359998703, Final Batch Loss: 0.09410934895277023\n",
      "Epoch 137, Loss: 0.482375830411911, Final Batch Loss: 0.09345152974128723\n",
      "Epoch 138, Loss: 0.4956594333052635, Final Batch Loss: 0.08171707391738892\n",
      "Epoch 139, Loss: 0.5018901750445366, Final Batch Loss: 0.08065890520811081\n",
      "Epoch 140, Loss: 0.5074267089366913, Final Batch Loss: 0.11550780385732651\n",
      "Epoch 141, Loss: 0.5914200693368912, Final Batch Loss: 0.1676107496023178\n",
      "Epoch 142, Loss: 0.519758827984333, Final Batch Loss: 0.11610450595617294\n",
      "Epoch 143, Loss: 0.4504138454794884, Final Batch Loss: 0.12361910939216614\n",
      "Epoch 144, Loss: 0.5229455642402172, Final Batch Loss: 0.05874848738312721\n",
      "Epoch 145, Loss: 0.47800201177597046, Final Batch Loss: 0.06316706538200378\n",
      "Epoch 146, Loss: 0.4589467868208885, Final Batch Loss: 0.09889078140258789\n",
      "Epoch 147, Loss: 0.517775759100914, Final Batch Loss: 0.08966952562332153\n",
      "Epoch 148, Loss: 0.46679212525486946, Final Batch Loss: 0.05585269257426262\n",
      "Epoch 149, Loss: 0.4695652648806572, Final Batch Loss: 0.08669901639223099\n",
      "Epoch 150, Loss: 0.4914291799068451, Final Batch Loss: 0.09700652956962585\n",
      "Epoch 151, Loss: 0.47391002252697945, Final Batch Loss: 0.08213692903518677\n",
      "Epoch 152, Loss: 0.4777431860566139, Final Batch Loss: 0.13716767728328705\n",
      "Epoch 153, Loss: 0.4815303459763527, Final Batch Loss: 0.12109432369470596\n",
      "Epoch 154, Loss: 0.48049013316631317, Final Batch Loss: 0.061603643000125885\n",
      "Epoch 155, Loss: 0.4665057696402073, Final Batch Loss: 0.05787854269146919\n",
      "Epoch 156, Loss: 0.4429181441664696, Final Batch Loss: 0.10926055163145065\n",
      "Epoch 157, Loss: 0.4145007096230984, Final Batch Loss: 0.05504266545176506\n",
      "Epoch 158, Loss: 0.46407105028629303, Final Batch Loss: 0.05345667898654938\n",
      "Epoch 159, Loss: 0.50770653039217, Final Batch Loss: 0.09841612726449966\n",
      "Epoch 160, Loss: 0.5050351470708847, Final Batch Loss: 0.13993960618972778\n",
      "Epoch 161, Loss: 0.47598183155059814, Final Batch Loss: 0.09528781473636627\n",
      "Epoch 162, Loss: 0.44869548827409744, Final Batch Loss: 0.09271646291017532\n",
      "Epoch 163, Loss: 0.48538514226675034, Final Batch Loss: 0.11521313339471817\n",
      "Epoch 164, Loss: 0.4728105440735817, Final Batch Loss: 0.10728920251131058\n",
      "Epoch 165, Loss: 0.41944124549627304, Final Batch Loss: 0.0753750130534172\n",
      "Epoch 166, Loss: 0.44899632036685944, Final Batch Loss: 0.08858651667833328\n",
      "Epoch 167, Loss: 0.45354432612657547, Final Batch Loss: 0.09061944484710693\n",
      "Epoch 168, Loss: 0.43049290776252747, Final Batch Loss: 0.04968806356191635\n",
      "Epoch 169, Loss: 0.46317390352487564, Final Batch Loss: 0.07392098754644394\n",
      "Epoch 170, Loss: 0.43389419838786125, Final Batch Loss: 0.1042541041970253\n",
      "Epoch 171, Loss: 0.4143671318888664, Final Batch Loss: 0.09391672164201736\n",
      "Epoch 172, Loss: 0.4543381556868553, Final Batch Loss: 0.10404703766107559\n",
      "Epoch 173, Loss: 0.4382930099964142, Final Batch Loss: 0.07859671860933304\n",
      "Epoch 174, Loss: 0.45835620164871216, Final Batch Loss: 0.062086209654808044\n",
      "Epoch 175, Loss: 0.4339042827486992, Final Batch Loss: 0.0874757468700409\n",
      "Epoch 176, Loss: 0.4374769888818264, Final Batch Loss: 0.0987151637673378\n",
      "Epoch 177, Loss: 0.41591154783964157, Final Batch Loss: 0.06461258977651596\n",
      "Epoch 178, Loss: 0.4009833559393883, Final Batch Loss: 0.09103721380233765\n",
      "Epoch 179, Loss: 0.4237867295742035, Final Batch Loss: 0.09858965873718262\n",
      "Epoch 180, Loss: 0.4713965654373169, Final Batch Loss: 0.12511955201625824\n",
      "Epoch 181, Loss: 0.4170788824558258, Final Batch Loss: 0.08789444714784622\n",
      "Epoch 182, Loss: 0.4474225118756294, Final Batch Loss: 0.13368482887744904\n",
      "Epoch 183, Loss: 0.4391670599579811, Final Batch Loss: 0.06927882134914398\n",
      "Epoch 184, Loss: 0.3874160647392273, Final Batch Loss: 0.07029208540916443\n",
      "Epoch 185, Loss: 0.47385911643505096, Final Batch Loss: 0.11247269809246063\n",
      "Epoch 186, Loss: 0.4340749494731426, Final Batch Loss: 0.057981979101896286\n",
      "Epoch 187, Loss: 0.4798475131392479, Final Batch Loss: 0.09050336480140686\n",
      "Epoch 188, Loss: 0.42003945633769035, Final Batch Loss: 0.12048608064651489\n",
      "Epoch 189, Loss: 0.4821574240922928, Final Batch Loss: 0.09260110557079315\n",
      "Epoch 190, Loss: 0.43667975813150406, Final Batch Loss: 0.10206708312034607\n",
      "Epoch 191, Loss: 0.44436806067824364, Final Batch Loss: 0.13511024415493011\n",
      "Epoch 192, Loss: 0.40094930678606033, Final Batch Loss: 0.06223202496767044\n",
      "Epoch 193, Loss: 0.4163891524076462, Final Batch Loss: 0.063261017203331\n",
      "Epoch 194, Loss: 0.4497942812740803, Final Batch Loss: 0.09590540081262589\n",
      "Epoch 195, Loss: 0.44438064098358154, Final Batch Loss: 0.10700426250696182\n",
      "Epoch 196, Loss: 0.3940351977944374, Final Batch Loss: 0.0557500496506691\n",
      "Epoch 197, Loss: 0.41835760697722435, Final Batch Loss: 0.05573189631104469\n",
      "Epoch 198, Loss: 0.4373827427625656, Final Batch Loss: 0.06332693994045258\n",
      "Epoch 199, Loss: 0.42572205513715744, Final Batch Loss: 0.06289597600698471\n",
      "Epoch 200, Loss: 0.43610861897468567, Final Batch Loss: 0.06024734675884247\n",
      "Epoch 201, Loss: 0.43486832827329636, Final Batch Loss: 0.08131599426269531\n",
      "Epoch 202, Loss: 0.3910132870078087, Final Batch Loss: 0.07820191979408264\n",
      "Epoch 203, Loss: 0.41900917515158653, Final Batch Loss: 0.09535108506679535\n",
      "Epoch 204, Loss: 0.41673099249601364, Final Batch Loss: 0.09845896065235138\n",
      "Epoch 205, Loss: 0.3765758350491524, Final Batch Loss: 0.06918510794639587\n",
      "Epoch 206, Loss: 0.38703834265470505, Final Batch Loss: 0.07642903923988342\n",
      "Epoch 207, Loss: 0.39240356534719467, Final Batch Loss: 0.06100712716579437\n",
      "Epoch 208, Loss: 0.3999693989753723, Final Batch Loss: 0.056365445256233215\n",
      "Epoch 209, Loss: 0.40198060870170593, Final Batch Loss: 0.12089889496564865\n",
      "Epoch 210, Loss: 0.37750567495822906, Final Batch Loss: 0.06484700739383698\n",
      "Epoch 211, Loss: 0.37816012278199196, Final Batch Loss: 0.0350976400077343\n",
      "Epoch 212, Loss: 0.41353337466716766, Final Batch Loss: 0.12217743694782257\n",
      "Epoch 213, Loss: 0.39610423520207405, Final Batch Loss: 0.07778146862983704\n",
      "Epoch 214, Loss: 0.39858679100871086, Final Batch Loss: 0.056922782212495804\n",
      "Epoch 215, Loss: 0.38009992614388466, Final Batch Loss: 0.08108428120613098\n",
      "Epoch 216, Loss: 0.3729971759021282, Final Batch Loss: 0.061598535627126694\n",
      "Epoch 217, Loss: 0.4129798486828804, Final Batch Loss: 0.1094246357679367\n",
      "Epoch 218, Loss: 0.4033384397625923, Final Batch Loss: 0.09910792857408524\n",
      "Epoch 219, Loss: 0.3841230235993862, Final Batch Loss: 0.045300696045160294\n",
      "Epoch 220, Loss: 0.3531432971358299, Final Batch Loss: 0.07855061441659927\n",
      "Epoch 221, Loss: 0.3789811134338379, Final Batch Loss: 0.06806832551956177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222, Loss: 0.36138417944312096, Final Batch Loss: 0.0612335279583931\n",
      "Epoch 223, Loss: 0.4005111902952194, Final Batch Loss: 0.06307248771190643\n",
      "Epoch 224, Loss: 0.39319758862257004, Final Batch Loss: 0.05297313630580902\n",
      "Epoch 225, Loss: 0.3634440265595913, Final Batch Loss: 0.1099458709359169\n",
      "Epoch 226, Loss: 0.3625034764409065, Final Batch Loss: 0.05259678140282631\n",
      "Epoch 227, Loss: 0.36761413142085075, Final Batch Loss: 0.07929835468530655\n",
      "Epoch 228, Loss: 0.3892736881971359, Final Batch Loss: 0.1000627726316452\n",
      "Epoch 229, Loss: 0.39317726716399193, Final Batch Loss: 0.06672403961420059\n",
      "Epoch 230, Loss: 0.3730679340660572, Final Batch Loss: 0.08341873437166214\n",
      "Epoch 231, Loss: 0.36201004311442375, Final Batch Loss: 0.06857755035161972\n",
      "Epoch 232, Loss: 0.36124537512660027, Final Batch Loss: 0.06192297115921974\n",
      "Epoch 233, Loss: 0.36846399307250977, Final Batch Loss: 0.06894687563180923\n",
      "Epoch 234, Loss: 0.40374915301799774, Final Batch Loss: 0.09684537351131439\n",
      "Epoch 235, Loss: 0.37486206367611885, Final Batch Loss: 0.03750760480761528\n",
      "Epoch 236, Loss: 0.35323766991496086, Final Batch Loss: 0.07719507813453674\n",
      "Epoch 237, Loss: 0.3348721079528332, Final Batch Loss: 0.054874975234270096\n",
      "Epoch 238, Loss: 0.3872663788497448, Final Batch Loss: 0.0900423675775528\n",
      "Epoch 239, Loss: 0.3864712454378605, Final Batch Loss: 0.08001727610826492\n",
      "Epoch 240, Loss: 0.37726079300045967, Final Batch Loss: 0.07643194496631622\n",
      "Epoch 241, Loss: 0.38083820417523384, Final Batch Loss: 0.08246567845344543\n",
      "Epoch 242, Loss: 0.35257287696003914, Final Batch Loss: 0.04543871060013771\n",
      "Epoch 243, Loss: 0.36156628653407097, Final Batch Loss: 0.10788363963365555\n",
      "Epoch 244, Loss: 0.3377816192805767, Final Batch Loss: 0.07345917075872421\n",
      "Epoch 245, Loss: 0.3432331867516041, Final Batch Loss: 0.0798153206706047\n",
      "Epoch 246, Loss: 0.325927946716547, Final Batch Loss: 0.07877765595912933\n",
      "Epoch 247, Loss: 0.32057005539536476, Final Batch Loss: 0.07214032113552094\n",
      "Epoch 248, Loss: 0.385831493884325, Final Batch Loss: 0.11091357469558716\n",
      "Epoch 249, Loss: 0.3556358627974987, Final Batch Loss: 0.05330406501889229\n",
      "Epoch 250, Loss: 0.3356751427054405, Final Batch Loss: 0.06563008576631546\n",
      "Epoch 251, Loss: 0.3489506207406521, Final Batch Loss: 0.05488205328583717\n",
      "Epoch 252, Loss: 0.31937279365956783, Final Batch Loss: 0.028574952855706215\n",
      "Epoch 253, Loss: 0.31523655354976654, Final Batch Loss: 0.045579805970191956\n",
      "Epoch 254, Loss: 0.34841611608862877, Final Batch Loss: 0.07134125381708145\n",
      "Epoch 255, Loss: 0.42236512154340744, Final Batch Loss: 0.09978780150413513\n",
      "Epoch 256, Loss: 0.36637256294488907, Final Batch Loss: 0.04768078401684761\n",
      "Epoch 257, Loss: 0.37241142615675926, Final Batch Loss: 0.05094577372074127\n",
      "Epoch 258, Loss: 0.3314705118536949, Final Batch Loss: 0.05585223063826561\n",
      "Epoch 259, Loss: 0.3143678233027458, Final Batch Loss: 0.061743736267089844\n",
      "Epoch 260, Loss: 0.35355518758296967, Final Batch Loss: 0.07417815178632736\n",
      "Epoch 261, Loss: 0.35312946513295174, Final Batch Loss: 0.08176311105489731\n",
      "Epoch 262, Loss: 0.33137062564492226, Final Batch Loss: 0.058493491262197495\n",
      "Epoch 263, Loss: 0.3473152071237564, Final Batch Loss: 0.038360703736543655\n",
      "Epoch 264, Loss: 0.3443378023803234, Final Batch Loss: 0.07268007099628448\n",
      "Epoch 265, Loss: 0.34866709262132645, Final Batch Loss: 0.05877833813428879\n",
      "Epoch 266, Loss: 0.3560269959270954, Final Batch Loss: 0.045779552310705185\n",
      "Epoch 267, Loss: 0.32644929364323616, Final Batch Loss: 0.09894701838493347\n",
      "Epoch 268, Loss: 0.375052023679018, Final Batch Loss: 0.09121394157409668\n",
      "Epoch 269, Loss: 0.3351540006697178, Final Batch Loss: 0.06369815021753311\n",
      "Epoch 270, Loss: 0.35463837906718254, Final Batch Loss: 0.07158295810222626\n",
      "Epoch 271, Loss: 0.34811585023999214, Final Batch Loss: 0.05052512139081955\n",
      "Epoch 272, Loss: 0.35189439728856087, Final Batch Loss: 0.08673061430454254\n",
      "Epoch 273, Loss: 0.3631379120051861, Final Batch Loss: 0.0871124342083931\n",
      "Epoch 274, Loss: 0.3421189785003662, Final Batch Loss: 0.10744071006774902\n",
      "Epoch 275, Loss: 0.3368920460343361, Final Batch Loss: 0.10829046368598938\n",
      "Epoch 276, Loss: 0.31079481542110443, Final Batch Loss: 0.07942239195108414\n",
      "Epoch 277, Loss: 0.3803866244852543, Final Batch Loss: 0.07507937401533127\n",
      "Epoch 278, Loss: 0.3161725252866745, Final Batch Loss: 0.059045080095529556\n",
      "Epoch 279, Loss: 0.3594363294541836, Final Batch Loss: 0.08704137057065964\n",
      "Epoch 280, Loss: 0.3695410266518593, Final Batch Loss: 0.07978019118309021\n",
      "Epoch 281, Loss: 0.3296111188828945, Final Batch Loss: 0.06890195608139038\n",
      "Epoch 282, Loss: 0.36601339280605316, Final Batch Loss: 0.08667796105146408\n",
      "Epoch 283, Loss: 0.33439857326447964, Final Batch Loss: 0.07055926322937012\n",
      "Epoch 284, Loss: 0.35567593201994896, Final Batch Loss: 0.10158538818359375\n",
      "Epoch 285, Loss: 0.3156203590333462, Final Batch Loss: 0.03388839215040207\n",
      "Epoch 286, Loss: 0.3189077340066433, Final Batch Loss: 0.08179081231355667\n",
      "Epoch 287, Loss: 0.343282837420702, Final Batch Loss: 0.06262654066085815\n",
      "Epoch 288, Loss: 0.3142784759402275, Final Batch Loss: 0.05954848602414131\n",
      "Epoch 289, Loss: 0.31686898320913315, Final Batch Loss: 0.050846077501773834\n",
      "Epoch 290, Loss: 0.30988891422748566, Final Batch Loss: 0.04465694725513458\n",
      "Epoch 291, Loss: 0.33201872557401657, Final Batch Loss: 0.10690511018037796\n",
      "Epoch 292, Loss: 0.31806275248527527, Final Batch Loss: 0.052729684859514236\n",
      "Epoch 293, Loss: 0.349863987416029, Final Batch Loss: 0.05491575226187706\n",
      "Epoch 294, Loss: 0.2882041446864605, Final Batch Loss: 0.03393625468015671\n",
      "Epoch 295, Loss: 0.27448466047644615, Final Batch Loss: 0.07275671511888504\n",
      "Epoch 296, Loss: 0.3080731891095638, Final Batch Loss: 0.04320088028907776\n",
      "Epoch 297, Loss: 0.31275779381394386, Final Batch Loss: 0.05376099795103073\n",
      "Epoch 298, Loss: 0.3240693174302578, Final Batch Loss: 0.09973304718732834\n",
      "Epoch 299, Loss: 0.30896082520484924, Final Batch Loss: 0.05869879946112633\n",
      "Epoch 300, Loss: 0.322005957365036, Final Batch Loss: 0.09168941527605057\n",
      "Epoch 301, Loss: 0.3428064249455929, Final Batch Loss: 0.05613686144351959\n",
      "Epoch 302, Loss: 0.3218464367091656, Final Batch Loss: 0.11509808897972107\n",
      "Epoch 303, Loss: 0.3112538829445839, Final Batch Loss: 0.06685902178287506\n",
      "Epoch 304, Loss: 0.29018931835889816, Final Batch Loss: 0.07027489691972733\n",
      "Epoch 305, Loss: 0.33516576141119003, Final Batch Loss: 0.08025841414928436\n",
      "Epoch 306, Loss: 0.30636680871248245, Final Batch Loss: 0.06258370727300644\n",
      "Epoch 307, Loss: 0.31222405284643173, Final Batch Loss: 0.041841547936201096\n",
      "Epoch 308, Loss: 0.3142278417944908, Final Batch Loss: 0.040720563381910324\n",
      "Epoch 309, Loss: 0.3266581892967224, Final Batch Loss: 0.03527873754501343\n",
      "Epoch 310, Loss: 0.28182774037122726, Final Batch Loss: 0.04937569051980972\n",
      "Epoch 311, Loss: 0.30366213247179985, Final Batch Loss: 0.03711901977658272\n",
      "Epoch 312, Loss: 0.2775699533522129, Final Batch Loss: 0.038253460079431534\n",
      "Epoch 313, Loss: 0.2913837768137455, Final Batch Loss: 0.03171626850962639\n",
      "Epoch 314, Loss: 0.2956421561539173, Final Batch Loss: 0.05719641223549843\n",
      "Epoch 315, Loss: 0.3010432571172714, Final Batch Loss: 0.054910529404878616\n",
      "Epoch 316, Loss: 0.31072671338915825, Final Batch Loss: 0.07378944754600525\n",
      "Epoch 317, Loss: 0.2911744900047779, Final Batch Loss: 0.05977003648877144\n",
      "Epoch 318, Loss: 0.29320982471108437, Final Batch Loss: 0.04644816741347313\n",
      "Epoch 319, Loss: 0.29248009249567986, Final Batch Loss: 0.025403831154108047\n",
      "Epoch 320, Loss: 0.25099804624915123, Final Batch Loss: 0.04084918648004532\n",
      "Epoch 321, Loss: 0.3116352818906307, Final Batch Loss: 0.06055467948317528\n",
      "Epoch 322, Loss: 0.30065909400582314, Final Batch Loss: 0.061168570071458817\n",
      "Epoch 323, Loss: 0.2624748107045889, Final Batch Loss: 0.02766631357371807\n",
      "Epoch 324, Loss: 0.28539247065782547, Final Batch Loss: 0.06791237741708755\n",
      "Epoch 325, Loss: 0.27729417383670807, Final Batch Loss: 0.06281992793083191\n",
      "Epoch 326, Loss: 0.29070720821619034, Final Batch Loss: 0.10217486321926117\n",
      "Epoch 327, Loss: 0.31068160757422447, Final Batch Loss: 0.04332106560468674\n",
      "Epoch 328, Loss: 0.315200824290514, Final Batch Loss: 0.0650942325592041\n",
      "Epoch 329, Loss: 0.2687807120382786, Final Batch Loss: 0.04578302428126335\n",
      "Epoch 330, Loss: 0.24980521202087402, Final Batch Loss: 0.04369676858186722\n",
      "Epoch 331, Loss: 0.27482911199331284, Final Batch Loss: 0.0621265172958374\n",
      "Epoch 332, Loss: 0.29729608446359634, Final Batch Loss: 0.06521553546190262\n",
      "Epoch 333, Loss: 0.2646904829889536, Final Batch Loss: 0.0999319925904274\n",
      "Epoch 334, Loss: 0.3295975923538208, Final Batch Loss: 0.0602714866399765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 335, Loss: 0.3158494383096695, Final Batch Loss: 0.06843142956495285\n",
      "Epoch 336, Loss: 0.26952353306114674, Final Batch Loss: 0.030816374346613884\n",
      "Epoch 337, Loss: 0.3028636537492275, Final Batch Loss: 0.06372731178998947\n",
      "Epoch 338, Loss: 0.2638937048614025, Final Batch Loss: 0.03529584780335426\n",
      "Epoch 339, Loss: 0.26802763156592846, Final Batch Loss: 0.050624776631593704\n",
      "Epoch 340, Loss: 0.27666639909148216, Final Batch Loss: 0.03920166939496994\n",
      "Epoch 341, Loss: 0.2876715660095215, Final Batch Loss: 0.07030637562274933\n",
      "Epoch 342, Loss: 0.2897091247141361, Final Batch Loss: 0.024241521954536438\n",
      "Epoch 343, Loss: 0.2641124874353409, Final Batch Loss: 0.081362783908844\n",
      "Epoch 344, Loss: 0.2527453638613224, Final Batch Loss: 0.03694338724017143\n",
      "Epoch 345, Loss: 0.28609425388276577, Final Batch Loss: 0.0733586922287941\n",
      "Epoch 346, Loss: 0.2595460768789053, Final Batch Loss: 0.049331024289131165\n",
      "Epoch 347, Loss: 0.27886340022087097, Final Batch Loss: 0.0444498248398304\n",
      "Epoch 348, Loss: 0.27022002451121807, Final Batch Loss: 0.03148908168077469\n",
      "Epoch 349, Loss: 0.27202364057302475, Final Batch Loss: 0.04535114765167236\n",
      "Epoch 350, Loss: 0.2738429009914398, Final Batch Loss: 0.05755562335252762\n",
      "Epoch 351, Loss: 0.268884040415287, Final Batch Loss: 0.039992425590753555\n",
      "Epoch 352, Loss: 0.24752221256494522, Final Batch Loss: 0.04950268194079399\n",
      "Epoch 353, Loss: 0.2463892549276352, Final Batch Loss: 0.033249370753765106\n",
      "Epoch 354, Loss: 0.2707907650619745, Final Batch Loss: 0.03027566522359848\n",
      "Epoch 355, Loss: 0.2682098522782326, Final Batch Loss: 0.05089220032095909\n",
      "Epoch 356, Loss: 0.27218069694936275, Final Batch Loss: 0.09229353070259094\n",
      "Epoch 357, Loss: 0.27474334090948105, Final Batch Loss: 0.06561725586652756\n",
      "Epoch 358, Loss: 0.28865102492272854, Final Batch Loss: 0.07093687355518341\n",
      "Epoch 359, Loss: 0.2820093184709549, Final Batch Loss: 0.03968438506126404\n",
      "Epoch 360, Loss: 0.25965550169348717, Final Batch Loss: 0.04106166213750839\n",
      "Epoch 361, Loss: 0.25571489706635475, Final Batch Loss: 0.029689352959394455\n",
      "Epoch 362, Loss: 0.24800578318536282, Final Batch Loss: 0.04399730637669563\n",
      "Epoch 363, Loss: 0.2744208239018917, Final Batch Loss: 0.0783466324210167\n",
      "Epoch 364, Loss: 0.26954472437500954, Final Batch Loss: 0.056107182055711746\n",
      "Epoch 365, Loss: 0.2951532043516636, Final Batch Loss: 0.081763356924057\n",
      "Epoch 366, Loss: 0.24038598872721195, Final Batch Loss: 0.049399398267269135\n",
      "Epoch 367, Loss: 0.3165493868291378, Final Batch Loss: 0.09007594734430313\n",
      "Epoch 368, Loss: 0.24846677109599113, Final Batch Loss: 0.033946238458156586\n",
      "Epoch 369, Loss: 0.25992101058363914, Final Batch Loss: 0.05297601595520973\n",
      "Epoch 370, Loss: 0.23405469581484795, Final Batch Loss: 0.017162058502435684\n",
      "Epoch 371, Loss: 0.23320995271205902, Final Batch Loss: 0.050368137657642365\n",
      "Epoch 372, Loss: 0.2573562189936638, Final Batch Loss: 0.04630913585424423\n",
      "Epoch 373, Loss: 0.2508241571485996, Final Batch Loss: 0.05027013644576073\n",
      "Epoch 374, Loss: 0.2309669991955161, Final Batch Loss: 0.012333638034760952\n",
      "Epoch 375, Loss: 0.24473217502236366, Final Batch Loss: 0.05158984288573265\n",
      "Epoch 376, Loss: 0.2559158690273762, Final Batch Loss: 0.06272691488265991\n",
      "Epoch 377, Loss: 0.24496988952159882, Final Batch Loss: 0.04373020678758621\n",
      "Epoch 378, Loss: 0.2187122292816639, Final Batch Loss: 0.037221670150756836\n",
      "Epoch 379, Loss: 0.280253030359745, Final Batch Loss: 0.0700773224234581\n",
      "Epoch 380, Loss: 0.2583471015095711, Final Batch Loss: 0.03131181001663208\n",
      "Epoch 381, Loss: 0.2733788453042507, Final Batch Loss: 0.04711713641881943\n",
      "Epoch 382, Loss: 0.23928657732903957, Final Batch Loss: 0.028151074424386024\n",
      "Epoch 383, Loss: 0.23028602823615074, Final Batch Loss: 0.047324180603027344\n",
      "Epoch 384, Loss: 0.21226853132247925, Final Batch Loss: 0.03449390456080437\n",
      "Epoch 385, Loss: 0.23397118225693703, Final Batch Loss: 0.03143389895558357\n",
      "Epoch 386, Loss: 0.24475260823965073, Final Batch Loss: 0.057402968406677246\n",
      "Epoch 387, Loss: 0.2725476324558258, Final Batch Loss: 0.08475107699632645\n",
      "Epoch 388, Loss: 0.27234020456671715, Final Batch Loss: 0.06883706152439117\n",
      "Epoch 389, Loss: 0.21965776197612286, Final Batch Loss: 0.038326289504766464\n",
      "Epoch 390, Loss: 0.20976629853248596, Final Batch Loss: 0.055163439363241196\n",
      "Epoch 391, Loss: 0.2495337314903736, Final Batch Loss: 0.012244723737239838\n",
      "Epoch 392, Loss: 0.2527789957821369, Final Batch Loss: 0.03904024884104729\n",
      "Epoch 393, Loss: 0.2295810878276825, Final Batch Loss: 0.03992030769586563\n",
      "Epoch 394, Loss: 0.2376327570527792, Final Batch Loss: 0.05964140594005585\n",
      "Epoch 395, Loss: 0.264849029481411, Final Batch Loss: 0.040487609803676605\n",
      "Epoch 396, Loss: 0.24619919806718826, Final Batch Loss: 0.08325988799333572\n",
      "Epoch 397, Loss: 0.20389092899858952, Final Batch Loss: 0.044124435633420944\n",
      "Epoch 398, Loss: 0.2531278654932976, Final Batch Loss: 0.055408526211977005\n",
      "Epoch 399, Loss: 0.2492352444678545, Final Batch Loss: 0.03994433954358101\n",
      "Epoch 400, Loss: 0.20986225828528404, Final Batch Loss: 0.028593147173523903\n",
      "Epoch 401, Loss: 0.22576423734426498, Final Batch Loss: 0.03443600609898567\n",
      "Epoch 402, Loss: 0.22484654746949673, Final Batch Loss: 0.031204653903841972\n",
      "Epoch 403, Loss: 0.22992616519331932, Final Batch Loss: 0.03391881659626961\n",
      "Epoch 404, Loss: 0.19137919507920742, Final Batch Loss: 0.04560608044266701\n",
      "Epoch 405, Loss: 0.19710670597851276, Final Batch Loss: 0.041586291044950485\n",
      "Epoch 406, Loss: 0.23065933771431446, Final Batch Loss: 0.05696067214012146\n",
      "Epoch 407, Loss: 0.19723580032587051, Final Batch Loss: 0.0274062380194664\n",
      "Epoch 408, Loss: 0.21578801237046719, Final Batch Loss: 0.05627632513642311\n",
      "Epoch 409, Loss: 0.23515283316373825, Final Batch Loss: 0.07149431854486465\n",
      "Epoch 410, Loss: 0.2697095200419426, Final Batch Loss: 0.06770294904708862\n",
      "Epoch 411, Loss: 0.2541169747710228, Final Batch Loss: 0.04942598193883896\n",
      "Epoch 412, Loss: 0.24952083639800549, Final Batch Loss: 0.041766680777072906\n",
      "Epoch 413, Loss: 0.2332649640738964, Final Batch Loss: 0.04078331217169762\n",
      "Epoch 414, Loss: 0.2624995559453964, Final Batch Loss: 0.06573420763015747\n",
      "Epoch 415, Loss: 0.20892125740647316, Final Batch Loss: 0.03939178213477135\n",
      "Epoch 416, Loss: 0.24344244040548801, Final Batch Loss: 0.0801558867096901\n",
      "Epoch 417, Loss: 0.21026036143302917, Final Batch Loss: 0.037295762449502945\n",
      "Epoch 418, Loss: 0.18720612488687038, Final Batch Loss: 0.026923799887299538\n",
      "Epoch 419, Loss: 0.1899249665439129, Final Batch Loss: 0.03442324325442314\n",
      "Epoch 420, Loss: 0.23328180983662605, Final Batch Loss: 0.026581313461065292\n",
      "Epoch 421, Loss: 0.1779529619961977, Final Batch Loss: 0.023455660790205002\n",
      "Epoch 422, Loss: 0.22448694333434105, Final Batch Loss: 0.057732533663511276\n",
      "Epoch 423, Loss: 0.22665654867887497, Final Batch Loss: 0.026338450610637665\n",
      "Epoch 424, Loss: 0.2083979044109583, Final Batch Loss: 0.05142402648925781\n",
      "Epoch 425, Loss: 0.2236027866601944, Final Batch Loss: 0.020358720794320107\n",
      "Epoch 426, Loss: 0.20566939003765583, Final Batch Loss: 0.02134147845208645\n",
      "Epoch 427, Loss: 0.20442208088934422, Final Batch Loss: 0.04589098319411278\n",
      "Epoch 428, Loss: 0.20811312645673752, Final Batch Loss: 0.0489274337887764\n",
      "Epoch 429, Loss: 0.19834592752158642, Final Batch Loss: 0.052306074649095535\n",
      "Epoch 430, Loss: 0.19373083300888538, Final Batch Loss: 0.03502155840396881\n",
      "Epoch 431, Loss: 0.1908831037580967, Final Batch Loss: 0.06826908141374588\n",
      "Epoch 432, Loss: 0.2289401162415743, Final Batch Loss: 0.0454607792198658\n",
      "Epoch 433, Loss: 0.21082712523639202, Final Batch Loss: 0.018368138000369072\n",
      "Epoch 434, Loss: 0.20263751409947872, Final Batch Loss: 0.02151833288371563\n",
      "Epoch 435, Loss: 0.2027801126241684, Final Batch Loss: 0.041126955300569534\n",
      "Epoch 436, Loss: 0.2177899219095707, Final Batch Loss: 0.039618320763111115\n",
      "Epoch 437, Loss: 0.20172787830233574, Final Batch Loss: 0.032593391835689545\n",
      "Epoch 438, Loss: 0.21317201759666204, Final Batch Loss: 0.04019494354724884\n",
      "Epoch 439, Loss: 0.19831747375428677, Final Batch Loss: 0.02261069230735302\n",
      "Epoch 440, Loss: 0.2250299071893096, Final Batch Loss: 0.05673820152878761\n",
      "Epoch 441, Loss: 0.19229348003864288, Final Batch Loss: 0.04153686389327049\n",
      "Epoch 442, Loss: 0.19002694450318813, Final Batch Loss: 0.023914115503430367\n",
      "Epoch 443, Loss: 0.2013505306094885, Final Batch Loss: 0.04021957144141197\n",
      "Epoch 444, Loss: 0.2105220202356577, Final Batch Loss: 0.030520258471369743\n",
      "Epoch 445, Loss: 0.19200676679611206, Final Batch Loss: 0.05308225378394127\n",
      "Epoch 446, Loss: 0.1847247676923871, Final Batch Loss: 0.011103988625109196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 447, Loss: 0.2057687994092703, Final Batch Loss: 0.01504947803914547\n",
      "Epoch 448, Loss: 0.1870811991393566, Final Batch Loss: 0.03157661110162735\n",
      "Epoch 449, Loss: 0.19966794550418854, Final Batch Loss: 0.026344217360019684\n",
      "Epoch 450, Loss: 0.1956045664846897, Final Batch Loss: 0.021397683769464493\n",
      "Epoch 451, Loss: 0.18463610112667084, Final Batch Loss: 0.059421394020318985\n",
      "Epoch 452, Loss: 0.19392164796590805, Final Batch Loss: 0.027171868830919266\n",
      "Epoch 453, Loss: 0.19438658095896244, Final Batch Loss: 0.04860417917370796\n",
      "Epoch 454, Loss: 0.16781146079301834, Final Batch Loss: 0.013828612864017487\n",
      "Epoch 455, Loss: 0.1987756285816431, Final Batch Loss: 0.01958339847624302\n",
      "Epoch 456, Loss: 0.1641897652298212, Final Batch Loss: 0.020530974492430687\n",
      "Epoch 457, Loss: 0.17488665226846933, Final Batch Loss: 0.048583656549453735\n",
      "Epoch 458, Loss: 0.16031772829592228, Final Batch Loss: 0.026210196316242218\n",
      "Epoch 459, Loss: 0.19297188334167004, Final Batch Loss: 0.04670186713337898\n",
      "Epoch 460, Loss: 0.2057325690984726, Final Batch Loss: 0.039231736212968826\n",
      "Epoch 461, Loss: 0.15349336713552475, Final Batch Loss: 0.03833812102675438\n",
      "Epoch 462, Loss: 0.193169217556715, Final Batch Loss: 0.02854437753558159\n",
      "Epoch 463, Loss: 0.18983755819499493, Final Batch Loss: 0.04248286038637161\n",
      "Epoch 464, Loss: 0.2009133007377386, Final Batch Loss: 0.029517969116568565\n",
      "Epoch 465, Loss: 0.21365505456924438, Final Batch Loss: 0.02483188547194004\n",
      "Epoch 466, Loss: 0.1712856125086546, Final Batch Loss: 0.041071873158216476\n",
      "Epoch 467, Loss: 0.2111009657382965, Final Batch Loss: 0.061048660427331924\n",
      "Epoch 468, Loss: 0.18098942190408707, Final Batch Loss: 0.03582318127155304\n",
      "Epoch 469, Loss: 0.22314500249922276, Final Batch Loss: 0.05004226788878441\n",
      "Epoch 470, Loss: 0.16423407662659883, Final Batch Loss: 0.0483122281730175\n",
      "Epoch 471, Loss: 0.2082033883780241, Final Batch Loss: 0.06459386646747589\n",
      "Epoch 472, Loss: 0.1692295651882887, Final Batch Loss: 0.03846186026930809\n",
      "Epoch 473, Loss: 0.2218751721084118, Final Batch Loss: 0.035988908261060715\n",
      "Epoch 474, Loss: 0.18859238922595978, Final Batch Loss: 0.01707339659333229\n",
      "Epoch 475, Loss: 0.18895143270492554, Final Batch Loss: 0.028976093977689743\n",
      "Epoch 476, Loss: 0.21608191542327404, Final Batch Loss: 0.07191334664821625\n",
      "Epoch 477, Loss: 0.153203297406435, Final Batch Loss: 0.01694290153682232\n",
      "Epoch 478, Loss: 0.19206097535789013, Final Batch Loss: 0.03337021917104721\n",
      "Epoch 479, Loss: 0.17806517146527767, Final Batch Loss: 0.018960027024149895\n",
      "Epoch 480, Loss: 0.14833207055926323, Final Batch Loss: 0.033295512199401855\n",
      "Epoch 481, Loss: 0.16634218581020832, Final Batch Loss: 0.02045164816081524\n",
      "Epoch 482, Loss: 0.13144163973629475, Final Batch Loss: 0.025530999526381493\n",
      "Epoch 483, Loss: 0.16093916818499565, Final Batch Loss: 0.03705712407827377\n",
      "Epoch 484, Loss: 0.17415791377425194, Final Batch Loss: 0.016968324780464172\n",
      "Epoch 485, Loss: 0.20639550872147083, Final Batch Loss: 0.05204667150974274\n",
      "Epoch 486, Loss: 0.20577523484826088, Final Batch Loss: 0.02806093543767929\n",
      "Epoch 487, Loss: 0.16826538927853107, Final Batch Loss: 0.03161095082759857\n",
      "Epoch 488, Loss: 0.18629245273768902, Final Batch Loss: 0.028489813208580017\n",
      "Epoch 489, Loss: 0.18606656976044178, Final Batch Loss: 0.048371605575084686\n",
      "Epoch 490, Loss: 0.18445801176130772, Final Batch Loss: 0.02120286226272583\n",
      "Epoch 491, Loss: 0.16500396840274334, Final Batch Loss: 0.03035696968436241\n",
      "Epoch 492, Loss: 0.2017071209847927, Final Batch Loss: 0.03039725497364998\n",
      "Epoch 493, Loss: 0.17158060893416405, Final Batch Loss: 0.04776792600750923\n",
      "Epoch 494, Loss: 0.17278142645955086, Final Batch Loss: 0.025754528120160103\n",
      "Epoch 495, Loss: 0.13825536519289017, Final Batch Loss: 0.02668950706720352\n",
      "Epoch 496, Loss: 0.1600357536226511, Final Batch Loss: 0.03124721348285675\n",
      "Epoch 497, Loss: 0.1852465495467186, Final Batch Loss: 0.023074502125382423\n",
      "Epoch 498, Loss: 0.17973759584128857, Final Batch Loss: 0.03764330968260765\n",
      "Epoch 499, Loss: 0.14739517495036125, Final Batch Loss: 0.04020697623491287\n",
      "Epoch 500, Loss: 0.18256915174424648, Final Batch Loss: 0.012611521407961845\n",
      "Epoch 501, Loss: 0.18477517738938332, Final Batch Loss: 0.0362345315515995\n",
      "Epoch 502, Loss: 0.12329199071973562, Final Batch Loss: 0.006265940144658089\n",
      "Epoch 503, Loss: 0.14012428745627403, Final Batch Loss: 0.035186171531677246\n",
      "Epoch 504, Loss: 0.1996208280324936, Final Batch Loss: 0.027266012504696846\n",
      "Epoch 505, Loss: 0.16986492555588484, Final Batch Loss: 0.03321828320622444\n",
      "Epoch 506, Loss: 0.1625020494684577, Final Batch Loss: 0.05394028127193451\n",
      "Epoch 507, Loss: 0.15709004923701286, Final Batch Loss: 0.011626597493886948\n",
      "Epoch 508, Loss: 0.18777289148420095, Final Batch Loss: 0.059980712831020355\n",
      "Epoch 509, Loss: 0.18729708157479763, Final Batch Loss: 0.031247856095433235\n",
      "Epoch 510, Loss: 0.16774292942136526, Final Batch Loss: 0.014707089401781559\n",
      "Epoch 511, Loss: 0.1657175812870264, Final Batch Loss: 0.02064397558569908\n",
      "Epoch 512, Loss: 0.17051725834608078, Final Batch Loss: 0.01505009550601244\n",
      "Epoch 513, Loss: 0.1661681579425931, Final Batch Loss: 0.036830391734838486\n",
      "Epoch 514, Loss: 0.2455495297908783, Final Batch Loss: 0.01599515974521637\n",
      "Epoch 515, Loss: 0.15481833554804325, Final Batch Loss: 0.021139152348041534\n",
      "Epoch 516, Loss: 0.16177769470959902, Final Batch Loss: 0.01354757510125637\n",
      "Epoch 517, Loss: 0.14569890312850475, Final Batch Loss: 0.020888149738311768\n",
      "Epoch 518, Loss: 0.18416409939527512, Final Batch Loss: 0.05276354029774666\n",
      "Epoch 519, Loss: 0.17687488067895174, Final Batch Loss: 0.03400791436433792\n",
      "Epoch 520, Loss: 0.15386341698467731, Final Batch Loss: 0.024631688371300697\n",
      "Epoch 521, Loss: 0.15382666140794754, Final Batch Loss: 0.020044995471835136\n",
      "Epoch 522, Loss: 0.1566024897620082, Final Batch Loss: 0.05080978572368622\n",
      "Epoch 523, Loss: 0.19331847690045834, Final Batch Loss: 0.05564715713262558\n",
      "Epoch 524, Loss: 0.14248285628855228, Final Batch Loss: 0.04639165848493576\n",
      "Epoch 525, Loss: 0.1401137402281165, Final Batch Loss: 0.010778947733342648\n",
      "Epoch 526, Loss: 0.1757936906069517, Final Batch Loss: 0.04524507373571396\n",
      "Epoch 527, Loss: 0.14925596304237843, Final Batch Loss: 0.025553803890943527\n",
      "Epoch 528, Loss: 0.17202169448137283, Final Batch Loss: 0.035665806382894516\n",
      "Epoch 529, Loss: 0.18626269325613976, Final Batch Loss: 0.032454963773489\n",
      "Epoch 530, Loss: 0.14276664331555367, Final Batch Loss: 0.017997149378061295\n",
      "Epoch 531, Loss: 0.14264524821192026, Final Batch Loss: 0.023674171417951584\n",
      "Epoch 532, Loss: 0.15868991240859032, Final Batch Loss: 0.0374445803463459\n",
      "Epoch 533, Loss: 0.19243850652128458, Final Batch Loss: 0.01554932538419962\n",
      "Epoch 534, Loss: 0.1690701637417078, Final Batch Loss: 0.029499832540750504\n",
      "Epoch 535, Loss: 0.14300516806542873, Final Batch Loss: 0.02366868406534195\n",
      "Epoch 536, Loss: 0.16634582169353962, Final Batch Loss: 0.01637481525540352\n",
      "Epoch 537, Loss: 0.16800767369568348, Final Batch Loss: 0.029743075370788574\n",
      "Epoch 538, Loss: 0.1361127719283104, Final Batch Loss: 0.029185336083173752\n",
      "Epoch 539, Loss: 0.15830897726118565, Final Batch Loss: 0.041301943361759186\n",
      "Epoch 540, Loss: 0.1379279736429453, Final Batch Loss: 0.0276657585054636\n",
      "Epoch 541, Loss: 0.1295965015888214, Final Batch Loss: 0.03373366594314575\n",
      "Epoch 542, Loss: 0.19034800119698048, Final Batch Loss: 0.04568907618522644\n",
      "Epoch 543, Loss: 0.12589786108583212, Final Batch Loss: 0.020159216597676277\n",
      "Epoch 544, Loss: 0.16086958907544613, Final Batch Loss: 0.03134326636791229\n",
      "Epoch 545, Loss: 0.11805247887969017, Final Batch Loss: 0.039610154926776886\n",
      "Epoch 546, Loss: 0.12554213497787714, Final Batch Loss: 0.03532317280769348\n",
      "Epoch 547, Loss: 0.14466287195682526, Final Batch Loss: 0.04549895599484444\n",
      "Epoch 548, Loss: 0.1819417867809534, Final Batch Loss: 0.05988223850727081\n",
      "Epoch 549, Loss: 0.13493015803396702, Final Batch Loss: 0.04940599948167801\n",
      "Epoch 550, Loss: 0.178669149056077, Final Batch Loss: 0.014779964461922646\n",
      "Epoch 551, Loss: 0.15591307543218136, Final Batch Loss: 0.04165906831622124\n",
      "Epoch 552, Loss: 0.10811002179980278, Final Batch Loss: 0.01867777481675148\n",
      "Epoch 553, Loss: 0.13600081019103527, Final Batch Loss: 0.017848024144768715\n",
      "Epoch 554, Loss: 0.1178937777876854, Final Batch Loss: 0.01088332012295723\n",
      "Epoch 555, Loss: 0.16592495702207088, Final Batch Loss: 0.02888653799891472\n",
      "Epoch 556, Loss: 0.12555214297026396, Final Batch Loss: 0.017725173383951187\n",
      "Epoch 557, Loss: 0.14358078502118587, Final Batch Loss: 0.027305442839860916\n",
      "Epoch 558, Loss: 0.132695275824517, Final Batch Loss: 0.016399797052145004\n",
      "Epoch 559, Loss: 0.11708777956664562, Final Batch Loss: 0.016832634806632996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 560, Loss: 0.13016096409410238, Final Batch Loss: 0.02519080601632595\n",
      "Epoch 561, Loss: 0.14785302616655827, Final Batch Loss: 0.045687608420848846\n",
      "Epoch 562, Loss: 0.11542329471558332, Final Batch Loss: 0.010909494943916798\n",
      "Epoch 563, Loss: 0.16239681467413902, Final Batch Loss: 0.03961940482258797\n",
      "Epoch 564, Loss: 0.17887025699019432, Final Batch Loss: 0.02510135993361473\n",
      "Epoch 565, Loss: 0.15226646699011326, Final Batch Loss: 0.0392138734459877\n",
      "Epoch 566, Loss: 0.17133611999452114, Final Batch Loss: 0.028140852227807045\n",
      "Epoch 567, Loss: 0.12661507539451122, Final Batch Loss: 0.017223453149199486\n",
      "Epoch 568, Loss: 0.16141690872609615, Final Batch Loss: 0.028897209092974663\n",
      "Epoch 569, Loss: 0.15887358970940113, Final Batch Loss: 0.024479612708091736\n",
      "Epoch 570, Loss: 0.15871035866439342, Final Batch Loss: 0.05853905528783798\n",
      "Epoch 571, Loss: 0.16602545883506536, Final Batch Loss: 0.05023651197552681\n",
      "Epoch 572, Loss: 0.12837001588195562, Final Batch Loss: 0.013529184274375439\n",
      "Epoch 573, Loss: 0.1198481391184032, Final Batch Loss: 0.012801728211343288\n",
      "Epoch 574, Loss: 0.1276881955564022, Final Batch Loss: 0.024405166506767273\n",
      "Epoch 575, Loss: 0.127514673396945, Final Batch Loss: 0.020686395466327667\n",
      "Epoch 576, Loss: 0.15132109075784683, Final Batch Loss: 0.031954266130924225\n",
      "Epoch 577, Loss: 0.13854912482202053, Final Batch Loss: 0.03351873159408569\n",
      "Epoch 578, Loss: 0.1378952395170927, Final Batch Loss: 0.01597720757126808\n",
      "Epoch 579, Loss: 0.1480502663180232, Final Batch Loss: 0.013924642466008663\n",
      "Epoch 580, Loss: 0.12885531317442656, Final Batch Loss: 0.010354296304285526\n",
      "Epoch 581, Loss: 0.17273163795471191, Final Batch Loss: 0.04688863828778267\n",
      "Epoch 582, Loss: 0.17331243492662907, Final Batch Loss: 0.02176130935549736\n",
      "Epoch 583, Loss: 0.1860837396234274, Final Batch Loss: 0.053741563111543655\n",
      "Epoch 584, Loss: 0.13353831507265568, Final Batch Loss: 0.04186958074569702\n",
      "Epoch 585, Loss: 0.18323297891765833, Final Batch Loss: 0.04254292696714401\n",
      "Epoch 586, Loss: 0.15078599005937576, Final Batch Loss: 0.0342438742518425\n",
      "Epoch 587, Loss: 0.16550939809530973, Final Batch Loss: 0.03910728171467781\n",
      "Epoch 588, Loss: 0.15821373835206032, Final Batch Loss: 0.03125504404306412\n",
      "Epoch 589, Loss: 0.14877893961966038, Final Batch Loss: 0.0381007082760334\n",
      "Epoch 590, Loss: 0.17298365850001574, Final Batch Loss: 0.014699677936732769\n",
      "Epoch 591, Loss: 0.1459196638315916, Final Batch Loss: 0.019645188003778458\n",
      "Epoch 592, Loss: 0.14403120055794716, Final Batch Loss: 0.03365437686443329\n",
      "Epoch 593, Loss: 0.15657276287674904, Final Batch Loss: 0.018417896702885628\n",
      "Epoch 594, Loss: 0.16109741013497114, Final Batch Loss: 0.03165660426020622\n",
      "Epoch 595, Loss: 0.1334887035191059, Final Batch Loss: 0.016419831663370132\n",
      "Epoch 596, Loss: 0.11930642044171691, Final Batch Loss: 0.03031541220843792\n",
      "Epoch 597, Loss: 0.1425937321037054, Final Batch Loss: 0.025187980383634567\n",
      "Epoch 598, Loss: 0.1314520174637437, Final Batch Loss: 0.012844028882682323\n",
      "Epoch 599, Loss: 0.10590474074706435, Final Batch Loss: 0.010341648943722248\n",
      "Epoch 600, Loss: 0.12253531347960234, Final Batch Loss: 0.021096862852573395\n",
      "Epoch 601, Loss: 0.14315367303788662, Final Batch Loss: 0.028879258781671524\n",
      "Epoch 602, Loss: 0.14379512518644333, Final Batch Loss: 0.05333947762846947\n",
      "Epoch 603, Loss: 0.12668124120682478, Final Batch Loss: 0.01633288525044918\n",
      "Epoch 604, Loss: 0.127272910438478, Final Batch Loss: 0.024594604969024658\n",
      "Epoch 605, Loss: 0.1412203349173069, Final Batch Loss: 0.023962851613759995\n",
      "Epoch 606, Loss: 0.1438894048333168, Final Batch Loss: 0.02110672928392887\n",
      "Epoch 607, Loss: 0.12789777480065823, Final Batch Loss: 0.022050607949495316\n",
      "Epoch 608, Loss: 0.16509661078453064, Final Batch Loss: 0.03366251289844513\n",
      "Epoch 609, Loss: 0.14653234835714102, Final Batch Loss: 0.009380926378071308\n",
      "Epoch 610, Loss: 0.13222589809447527, Final Batch Loss: 0.015335737727582455\n",
      "Epoch 611, Loss: 0.12040074821561575, Final Batch Loss: 0.01311490684747696\n",
      "Epoch 612, Loss: 0.11881848983466625, Final Batch Loss: 0.02087058313190937\n",
      "Epoch 613, Loss: 0.15255036484450102, Final Batch Loss: 0.03689343109726906\n",
      "Epoch 614, Loss: 0.15500499308109283, Final Batch Loss: 0.015415100380778313\n",
      "Epoch 615, Loss: 0.14552095159888268, Final Batch Loss: 0.010023972019553185\n",
      "Epoch 616, Loss: 0.12411588616669178, Final Batch Loss: 0.016006112098693848\n",
      "Epoch 617, Loss: 0.13489161245524883, Final Batch Loss: 0.0420137494802475\n",
      "Epoch 618, Loss: 0.10559015907347202, Final Batch Loss: 0.029587075114250183\n",
      "Epoch 619, Loss: 0.11684364546090364, Final Batch Loss: 0.013047650456428528\n",
      "Epoch 620, Loss: 0.09356574807316065, Final Batch Loss: 0.010961147956550121\n",
      "Epoch 621, Loss: 0.11385904625058174, Final Batch Loss: 0.014996493235230446\n",
      "Epoch 622, Loss: 0.11993097327649593, Final Batch Loss: 0.02262185886502266\n",
      "Epoch 623, Loss: 0.10919033922255039, Final Batch Loss: 0.009143354371190071\n",
      "Epoch 624, Loss: 0.11703731771558523, Final Batch Loss: 0.03136438503861427\n",
      "Epoch 625, Loss: 0.1065463051199913, Final Batch Loss: 0.027842599898576736\n",
      "Epoch 626, Loss: 0.10289081372320652, Final Batch Loss: 0.012182272039353848\n",
      "Epoch 627, Loss: 0.12306377850472927, Final Batch Loss: 0.05408330634236336\n",
      "Epoch 628, Loss: 0.12077698111534119, Final Batch Loss: 0.02318762242794037\n",
      "Epoch 629, Loss: 0.11010169703513384, Final Batch Loss: 0.03130527213215828\n",
      "Epoch 630, Loss: 0.0970935788936913, Final Batch Loss: 0.007646890822798014\n",
      "Epoch 631, Loss: 0.08762279711663723, Final Batch Loss: 0.012105491012334824\n",
      "Epoch 632, Loss: 0.13593222200870514, Final Batch Loss: 0.06151801347732544\n",
      "Epoch 633, Loss: 0.12723413668572903, Final Batch Loss: 0.024503707885742188\n",
      "Epoch 634, Loss: 0.12068616412580013, Final Batch Loss: 0.010973096825182438\n",
      "Epoch 635, Loss: 0.11181456875056028, Final Batch Loss: 0.02664257027208805\n",
      "Epoch 636, Loss: 0.14637063443660736, Final Batch Loss: 0.01360439695417881\n",
      "Epoch 637, Loss: 0.10410756152123213, Final Batch Loss: 0.010134857147932053\n",
      "Epoch 638, Loss: 0.0969166960567236, Final Batch Loss: 0.03162071853876114\n",
      "Epoch 639, Loss: 0.13177438639104366, Final Batch Loss: 0.033962029963731766\n",
      "Epoch 640, Loss: 0.11794130224734545, Final Batch Loss: 0.014125113375484943\n",
      "Epoch 641, Loss: 0.17295363266021013, Final Batch Loss: 0.06938908994197845\n",
      "Epoch 642, Loss: 0.12778997048735619, Final Batch Loss: 0.019871488213539124\n",
      "Epoch 643, Loss: 0.10881088767200708, Final Batch Loss: 0.018725644797086716\n",
      "Epoch 644, Loss: 0.14039781037718058, Final Batch Loss: 0.013056701049208641\n",
      "Epoch 645, Loss: 0.129250499419868, Final Batch Loss: 0.013018121011555195\n",
      "Epoch 646, Loss: 0.13183549884706736, Final Batch Loss: 0.04952342435717583\n",
      "Epoch 647, Loss: 0.1140376003459096, Final Batch Loss: 0.02511143498122692\n",
      "Epoch 648, Loss: 0.1131718335673213, Final Batch Loss: 0.018525157123804092\n",
      "Epoch 649, Loss: 0.13101862743496895, Final Batch Loss: 0.04773547500371933\n",
      "Epoch 650, Loss: 0.16304020024836063, Final Batch Loss: 0.0618639700114727\n",
      "Epoch 651, Loss: 0.10723119415342808, Final Batch Loss: 0.012638635002076626\n",
      "Epoch 652, Loss: 0.12413956550881267, Final Batch Loss: 0.04248104616999626\n",
      "Epoch 653, Loss: 0.11807307321578264, Final Batch Loss: 0.01401586551219225\n",
      "Epoch 654, Loss: 0.14248444186523557, Final Batch Loss: 0.02494872361421585\n",
      "Epoch 655, Loss: 0.11205114889889956, Final Batch Loss: 0.0279178898781538\n",
      "Epoch 656, Loss: 0.1221414152532816, Final Batch Loss: 0.02016068436205387\n",
      "Epoch 657, Loss: 0.09436414809897542, Final Batch Loss: 0.027617618441581726\n",
      "Epoch 658, Loss: 0.11678850557655096, Final Batch Loss: 0.03974844142794609\n",
      "Epoch 659, Loss: 0.1281738579273224, Final Batch Loss: 0.01817099004983902\n",
      "Epoch 660, Loss: 0.10076720174401999, Final Batch Loss: 0.021405193954706192\n",
      "Epoch 661, Loss: 0.16779216844588518, Final Batch Loss: 0.023859480395913124\n",
      "Epoch 662, Loss: 0.09511805418878794, Final Batch Loss: 0.022632993757724762\n",
      "Epoch 663, Loss: 0.09619214339181781, Final Batch Loss: 0.00544720096513629\n",
      "Epoch 664, Loss: 0.10331754479557276, Final Batch Loss: 0.03181704133749008\n",
      "Epoch 665, Loss: 0.10568212252110243, Final Batch Loss: 0.014659821055829525\n",
      "Epoch 666, Loss: 0.14234184939414263, Final Batch Loss: 0.027276623994112015\n",
      "Epoch 667, Loss: 0.08629429806023836, Final Batch Loss: 0.00804347824305296\n",
      "Epoch 668, Loss: 0.13592119794338942, Final Batch Loss: 0.02752419374883175\n",
      "Epoch 669, Loss: 0.096115049906075, Final Batch Loss: 0.019685881212353706\n",
      "Epoch 670, Loss: 0.10319976881146431, Final Batch Loss: 0.026662971824407578\n",
      "Epoch 671, Loss: 0.1203104869928211, Final Batch Loss: 0.003102020127698779\n",
      "Epoch 672, Loss: 0.12323231995105743, Final Batch Loss: 0.027753083035349846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 673, Loss: 0.1436766441911459, Final Batch Loss: 0.020020239055156708\n",
      "Epoch 674, Loss: 0.14200991299003363, Final Batch Loss: 0.030702458694577217\n",
      "Epoch 675, Loss: 0.12473136372864246, Final Batch Loss: 0.05531039834022522\n",
      "Epoch 676, Loss: 0.12338917143642902, Final Batch Loss: 0.009194510988891125\n",
      "Epoch 677, Loss: 0.09291194938123226, Final Batch Loss: 0.006233330816030502\n",
      "Epoch 678, Loss: 0.1116203274577856, Final Batch Loss: 0.02434791438281536\n",
      "Epoch 679, Loss: 0.11644741240888834, Final Batch Loss: 0.032352786511182785\n",
      "Epoch 680, Loss: 0.09946616552770138, Final Batch Loss: 0.027028115466237068\n",
      "Epoch 681, Loss: 0.11291574686765671, Final Batch Loss: 0.025562066584825516\n",
      "Epoch 682, Loss: 0.11935717100277543, Final Batch Loss: 0.005529975984245539\n",
      "Epoch 683, Loss: 0.15085648791864514, Final Batch Loss: 0.041173551231622696\n",
      "Epoch 684, Loss: 0.11239293217658997, Final Batch Loss: 0.03084608167409897\n",
      "Epoch 685, Loss: 0.10372424218803644, Final Batch Loss: 0.02091515064239502\n",
      "Epoch 686, Loss: 0.1101633096113801, Final Batch Loss: 0.02907869592308998\n",
      "Epoch 687, Loss: 0.09390895999968052, Final Batch Loss: 0.0247198473662138\n",
      "Epoch 688, Loss: 0.10798145877197385, Final Batch Loss: 0.024304648861289024\n",
      "Epoch 689, Loss: 0.08539066277444363, Final Batch Loss: 0.021268323063850403\n",
      "Epoch 690, Loss: 0.0906480411067605, Final Batch Loss: 0.00932069681584835\n",
      "Epoch 691, Loss: 0.12501164432615042, Final Batch Loss: 0.032317645847797394\n",
      "Epoch 692, Loss: 0.10350483655929565, Final Batch Loss: 0.01410948671400547\n",
      "Epoch 693, Loss: 0.13114050030708313, Final Batch Loss: 0.022226734086871147\n",
      "Epoch 694, Loss: 0.10093638626858592, Final Batch Loss: 0.006474134977906942\n",
      "Epoch 695, Loss: 0.09916078206151724, Final Batch Loss: 0.02125331200659275\n",
      "Epoch 696, Loss: 0.10599176026880741, Final Batch Loss: 0.04025707766413689\n",
      "Epoch 697, Loss: 0.15239182580262423, Final Batch Loss: 0.06444470584392548\n",
      "Epoch 698, Loss: 0.11116656474769115, Final Batch Loss: 0.018999939784407616\n",
      "Epoch 699, Loss: 0.17206513695418835, Final Batch Loss: 0.010654988698661327\n",
      "Epoch 700, Loss: 0.10984380170702934, Final Batch Loss: 0.016044197604060173\n",
      "Epoch 701, Loss: 0.1140994057059288, Final Batch Loss: 0.013670073822140694\n",
      "Epoch 702, Loss: 0.0968355592340231, Final Batch Loss: 0.03141384199261665\n",
      "Epoch 703, Loss: 0.1122676832601428, Final Batch Loss: 0.021335409954190254\n",
      "Epoch 704, Loss: 0.08334356173872948, Final Batch Loss: 0.016768183559179306\n",
      "Epoch 705, Loss: 0.12758022733032703, Final Batch Loss: 0.06925901770591736\n",
      "Epoch 706, Loss: 0.10175774246454239, Final Batch Loss: 0.019680669531226158\n",
      "Epoch 707, Loss: 0.11721269227564335, Final Batch Loss: 0.01808299869298935\n",
      "Epoch 708, Loss: 0.11171719897538424, Final Batch Loss: 0.023761749267578125\n",
      "Epoch 709, Loss: 0.13512554951012135, Final Batch Loss: 0.022696763277053833\n",
      "Epoch 710, Loss: 0.12583240028470755, Final Batch Loss: 0.02976074069738388\n",
      "Epoch 711, Loss: 0.14441335387527943, Final Batch Loss: 0.03795116767287254\n",
      "Epoch 712, Loss: 0.1477008517831564, Final Batch Loss: 0.03251473978161812\n",
      "Epoch 713, Loss: 0.11023142747581005, Final Batch Loss: 0.022621089592576027\n",
      "Epoch 714, Loss: 0.09721471089869738, Final Batch Loss: 0.017821986228227615\n",
      "Epoch 715, Loss: 0.1334598772227764, Final Batch Loss: 0.015761511400341988\n",
      "Epoch 716, Loss: 0.11005172692239285, Final Batch Loss: 0.03586586192250252\n",
      "Epoch 717, Loss: 0.1366314822807908, Final Batch Loss: 0.04591111093759537\n",
      "Epoch 718, Loss: 0.13441320322453976, Final Batch Loss: 0.03175286203622818\n",
      "Epoch 719, Loss: 0.13546608947217464, Final Batch Loss: 0.0243211779743433\n",
      "Epoch 720, Loss: 0.09687473904341459, Final Batch Loss: 0.0371319055557251\n",
      "Epoch 721, Loss: 0.09359998069703579, Final Batch Loss: 0.020081644877791405\n",
      "Epoch 722, Loss: 0.09564440697431564, Final Batch Loss: 0.020724469795823097\n",
      "Epoch 723, Loss: 0.105256212875247, Final Batch Loss: 0.026901934295892715\n",
      "Epoch 724, Loss: 0.09476293809711933, Final Batch Loss: 0.011264109052717686\n",
      "Epoch 725, Loss: 0.1346610290929675, Final Batch Loss: 0.01254183892160654\n",
      "Epoch 726, Loss: 0.0909468587487936, Final Batch Loss: 0.018792981281876564\n",
      "Epoch 727, Loss: 0.12432095594704151, Final Batch Loss: 0.0073186978697776794\n",
      "Epoch 728, Loss: 0.09966972190886736, Final Batch Loss: 0.022022824734449387\n",
      "Epoch 729, Loss: 0.11308467574417591, Final Batch Loss: 0.011999993585050106\n",
      "Epoch 730, Loss: 0.10179947037249804, Final Batch Loss: 0.012344270013272762\n",
      "Epoch 731, Loss: 0.1834558630362153, Final Batch Loss: 0.09234092384576797\n",
      "Epoch 732, Loss: 0.13175014313310385, Final Batch Loss: 0.031257741153240204\n",
      "Epoch 733, Loss: 0.10895123146474361, Final Batch Loss: 0.03023397922515869\n",
      "Epoch 734, Loss: 0.15096639469265938, Final Batch Loss: 0.04681422933936119\n",
      "Epoch 735, Loss: 0.11140003986656666, Final Batch Loss: 0.022297469899058342\n",
      "Epoch 736, Loss: 0.13548242393881083, Final Batch Loss: 0.014208300039172173\n",
      "Epoch 737, Loss: 0.08587126154452562, Final Batch Loss: 0.01562568172812462\n",
      "Epoch 738, Loss: 0.11889543477445841, Final Batch Loss: 0.020999688655138016\n",
      "Epoch 739, Loss: 0.09935325756669044, Final Batch Loss: 0.01986679621040821\n",
      "Epoch 740, Loss: 0.15310162492096424, Final Batch Loss: 0.018500905483961105\n",
      "Epoch 741, Loss: 0.08160697296261787, Final Batch Loss: 0.018373403698205948\n",
      "Epoch 742, Loss: 0.09886226337403059, Final Batch Loss: 0.030512336641550064\n",
      "Epoch 743, Loss: 0.12371320743113756, Final Batch Loss: 0.03281418979167938\n",
      "Epoch 744, Loss: 0.10825346503406763, Final Batch Loss: 0.008031690493226051\n",
      "Epoch 745, Loss: 0.10316990036517382, Final Batch Loss: 0.02244485542178154\n",
      "Epoch 746, Loss: 0.1117798862978816, Final Batch Loss: 0.03606509417295456\n",
      "Epoch 747, Loss: 0.0965008158236742, Final Batch Loss: 0.019016696140170097\n",
      "Epoch 748, Loss: 0.12969236075878143, Final Batch Loss: 0.04283389821648598\n",
      "Epoch 749, Loss: 0.11125945765525103, Final Batch Loss: 0.03615059703588486\n",
      "Epoch 750, Loss: 0.11542968451976776, Final Batch Loss: 0.027113081887364388\n",
      "Epoch 751, Loss: 0.08388872351497412, Final Batch Loss: 0.029586035758256912\n",
      "Epoch 752, Loss: 0.14157177228480577, Final Batch Loss: 0.062262699007987976\n",
      "Epoch 753, Loss: 0.13665313459932804, Final Batch Loss: 0.02343331091105938\n",
      "Epoch 754, Loss: 0.1002445686608553, Final Batch Loss: 0.013813004828989506\n",
      "Epoch 755, Loss: 0.15519029460847378, Final Batch Loss: 0.014262383803725243\n",
      "Epoch 756, Loss: 0.13814687822014093, Final Batch Loss: 0.029812797904014587\n",
      "Epoch 757, Loss: 0.11901695560663939, Final Batch Loss: 0.03918914496898651\n",
      "Epoch 758, Loss: 0.12334227934479713, Final Batch Loss: 0.04679179564118385\n",
      "Epoch 759, Loss: 0.10600094264373183, Final Batch Loss: 0.04194817692041397\n",
      "Epoch 760, Loss: 0.08370941877365112, Final Batch Loss: 0.03185391053557396\n",
      "Epoch 761, Loss: 0.10192798543721437, Final Batch Loss: 0.012715805321931839\n",
      "Epoch 762, Loss: 0.1616819454357028, Final Batch Loss: 0.05364029482007027\n",
      "Epoch 763, Loss: 0.1215715017169714, Final Batch Loss: 0.026562070474028587\n",
      "Epoch 764, Loss: 0.12079134583473206, Final Batch Loss: 0.022885285317897797\n",
      "Epoch 765, Loss: 0.12449092976748943, Final Batch Loss: 0.022731952369213104\n",
      "Epoch 766, Loss: 0.12302720174193382, Final Batch Loss: 0.026118110865354538\n",
      "Epoch 767, Loss: 0.11754687037318945, Final Batch Loss: 0.015260874293744564\n",
      "Epoch 768, Loss: 0.1257943594828248, Final Batch Loss: 0.03721313178539276\n",
      "Epoch 769, Loss: 0.08539025485515594, Final Batch Loss: 0.008002241142094135\n",
      "Epoch 770, Loss: 0.08952161949127913, Final Batch Loss: 0.027148688212037086\n",
      "Epoch 771, Loss: 0.09721792209893465, Final Batch Loss: 0.008305275812745094\n",
      "Epoch 772, Loss: 0.12725444789975882, Final Batch Loss: 0.043400075286626816\n",
      "Epoch 773, Loss: 0.1108431052416563, Final Batch Loss: 0.027078310027718544\n",
      "Epoch 774, Loss: 0.09357934491708875, Final Batch Loss: 0.0076499511487782\n",
      "Epoch 775, Loss: 0.09377205837517977, Final Batch Loss: 0.02088085561990738\n",
      "Epoch 776, Loss: 0.10136414971202612, Final Batch Loss: 0.0200759656727314\n",
      "Epoch 777, Loss: 0.07818814925849438, Final Batch Loss: 0.022022346034646034\n",
      "Epoch 778, Loss: 0.10025320295244455, Final Batch Loss: 0.019272156059741974\n",
      "Epoch 779, Loss: 0.0725594381801784, Final Batch Loss: 0.012819948606193066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 780, Loss: 0.0820594597607851, Final Batch Loss: 0.02736804448068142\n",
      "Epoch 781, Loss: 0.09291808679699898, Final Batch Loss: 0.01562123280018568\n",
      "Epoch 782, Loss: 0.0920362276956439, Final Batch Loss: 0.02912048064172268\n",
      "Epoch 783, Loss: 0.0916233491152525, Final Batch Loss: 0.01442254614084959\n",
      "Epoch 784, Loss: 0.0932161370292306, Final Batch Loss: 0.008432251401245594\n",
      "Epoch 785, Loss: 0.07903457805514336, Final Batch Loss: 0.021727144718170166\n",
      "Epoch 786, Loss: 0.10025329561904073, Final Batch Loss: 0.020824188366532326\n",
      "Epoch 787, Loss: 0.0684732161462307, Final Batch Loss: 0.01335224136710167\n",
      "Epoch 788, Loss: 0.08921200386248529, Final Batch Loss: 0.03269483894109726\n",
      "Epoch 789, Loss: 0.07696216460317373, Final Batch Loss: 0.01718343421816826\n",
      "Epoch 790, Loss: 0.09857330331578851, Final Batch Loss: 0.020961154252290726\n",
      "Epoch 791, Loss: 0.10026881983503699, Final Batch Loss: 0.010001345537602901\n",
      "Epoch 792, Loss: 0.15399645501747727, Final Batch Loss: 0.008442843332886696\n",
      "Epoch 793, Loss: 0.08316998602822423, Final Batch Loss: 0.03340959921479225\n",
      "Epoch 794, Loss: 0.09552657604217529, Final Batch Loss: 0.012087012641131878\n",
      "Epoch 795, Loss: 0.09077549632638693, Final Batch Loss: 0.026313796639442444\n",
      "Epoch 796, Loss: 0.09996559005230665, Final Batch Loss: 0.01893503963947296\n",
      "Epoch 797, Loss: 0.13373233936727047, Final Batch Loss: 0.027314310893416405\n",
      "Epoch 798, Loss: 0.09893189370632172, Final Batch Loss: 0.006676261778920889\n",
      "Epoch 799, Loss: 0.0807022899389267, Final Batch Loss: 0.013882225379347801\n",
      "Epoch 800, Loss: 0.09490249399095774, Final Batch Loss: 0.010512197390198708\n",
      "Epoch 801, Loss: 0.07439289055764675, Final Batch Loss: 0.021570561453700066\n",
      "Epoch 802, Loss: 0.07650424633175135, Final Batch Loss: 0.012635523453354836\n",
      "Epoch 803, Loss: 0.08139499556273222, Final Batch Loss: 0.019472897052764893\n",
      "Epoch 804, Loss: 0.11004417529329658, Final Batch Loss: 0.011347991414368153\n",
      "Epoch 805, Loss: 0.11049376660957932, Final Batch Loss: 0.009984169155359268\n",
      "Epoch 806, Loss: 0.1374154295772314, Final Batch Loss: 0.04198531433939934\n",
      "Epoch 807, Loss: 0.08990684896707535, Final Batch Loss: 0.01315496675670147\n",
      "Epoch 808, Loss: 0.0987333245575428, Final Batch Loss: 0.019864482805132866\n",
      "Epoch 809, Loss: 0.07789774006232619, Final Batch Loss: 0.018775278702378273\n",
      "Epoch 810, Loss: 0.08370283618569374, Final Batch Loss: 0.008394033648073673\n",
      "Epoch 811, Loss: 0.07674290798604488, Final Batch Loss: 0.02321428805589676\n",
      "Epoch 812, Loss: 0.08949474059045315, Final Batch Loss: 0.042736511677503586\n",
      "Epoch 813, Loss: 0.07197585934773088, Final Batch Loss: 0.0028859800659120083\n",
      "Epoch 814, Loss: 0.1471424214541912, Final Batch Loss: 0.0571131594479084\n",
      "Epoch 815, Loss: 0.11569122411310673, Final Batch Loss: 0.029997456818819046\n",
      "Epoch 816, Loss: 0.09334144787862897, Final Batch Loss: 0.019841201603412628\n",
      "Epoch 817, Loss: 0.09308891883119941, Final Batch Loss: 0.014587249606847763\n",
      "Epoch 818, Loss: 0.07865466177463531, Final Batch Loss: 0.019084064289927483\n",
      "Epoch 819, Loss: 0.10194431990385056, Final Batch Loss: 0.014973216690123081\n",
      "Epoch 820, Loss: 0.1157687846571207, Final Batch Loss: 0.023070814087986946\n",
      "Epoch 821, Loss: 0.0826630718074739, Final Batch Loss: 0.008758480660617352\n",
      "Epoch 822, Loss: 0.11124216020107269, Final Batch Loss: 0.03230643644928932\n",
      "Epoch 823, Loss: 0.11034725699573755, Final Batch Loss: 0.032605286687612534\n",
      "Epoch 824, Loss: 0.09619527962058783, Final Batch Loss: 0.011963524855673313\n",
      "Epoch 825, Loss: 0.11567425495013595, Final Batch Loss: 0.07238662987947464\n",
      "Epoch 826, Loss: 0.09422842506319284, Final Batch Loss: 0.01940779760479927\n",
      "Epoch 827, Loss: 0.07892523473128676, Final Batch Loss: 0.03960764408111572\n",
      "Epoch 828, Loss: 0.07723447680473328, Final Batch Loss: 0.010605303570628166\n",
      "Epoch 829, Loss: 0.09495894517749548, Final Batch Loss: 0.017675727605819702\n",
      "Epoch 830, Loss: 0.07987428363412619, Final Batch Loss: 0.031522486358881\n",
      "Epoch 831, Loss: 0.1308448752388358, Final Batch Loss: 0.06610634922981262\n",
      "Epoch 832, Loss: 0.1067947493866086, Final Batch Loss: 0.0024832659400999546\n",
      "Epoch 833, Loss: 0.16277847648598254, Final Batch Loss: 0.02963510900735855\n",
      "Epoch 834, Loss: 0.13121095020323992, Final Batch Loss: 0.009589179418981075\n",
      "Epoch 835, Loss: 0.08033855631947517, Final Batch Loss: 0.015731971710920334\n",
      "Epoch 836, Loss: 0.09146386152133346, Final Batch Loss: 0.010178969241678715\n",
      "Epoch 837, Loss: 0.08314392622560263, Final Batch Loss: 0.016333740204572678\n",
      "Epoch 838, Loss: 0.09275610884651542, Final Batch Loss: 0.016414087265729904\n",
      "Epoch 839, Loss: 0.06425172928720713, Final Batch Loss: 0.0053555588237941265\n",
      "Epoch 840, Loss: 0.07579458504915237, Final Batch Loss: 0.009898350574076176\n",
      "Epoch 841, Loss: 0.11104178801178932, Final Batch Loss: 0.014694735407829285\n",
      "Epoch 842, Loss: 0.07900422811508179, Final Batch Loss: 0.005175149068236351\n",
      "Epoch 843, Loss: 0.08452999778091908, Final Batch Loss: 0.023156125098466873\n",
      "Epoch 844, Loss: 0.07736402191221714, Final Batch Loss: 0.014293558895587921\n",
      "Epoch 845, Loss: 0.08133955695666373, Final Batch Loss: 0.025905238464474678\n",
      "Epoch 846, Loss: 0.0773666026070714, Final Batch Loss: 0.007696702145040035\n",
      "Epoch 847, Loss: 0.06980992015451193, Final Batch Loss: 0.008583527989685535\n",
      "Epoch 848, Loss: 0.07240448333323002, Final Batch Loss: 0.010546007193624973\n",
      "Epoch 849, Loss: 0.06674412498250604, Final Batch Loss: 0.01360795646905899\n",
      "Epoch 850, Loss: 0.13022959558293223, Final Batch Loss: 0.040887754410505295\n",
      "Epoch 851, Loss: 0.10219103563576937, Final Batch Loss: 0.0110337994992733\n",
      "Epoch 852, Loss: 0.06718776747584343, Final Batch Loss: 0.004960112273693085\n",
      "Epoch 853, Loss: 0.07194081041961908, Final Batch Loss: 0.021415380761027336\n",
      "Epoch 854, Loss: 0.08368886262178421, Final Batch Loss: 0.01387092936784029\n",
      "Epoch 855, Loss: 0.10945314727723598, Final Batch Loss: 0.03367890790104866\n",
      "Epoch 856, Loss: 0.06662641000002623, Final Batch Loss: 0.027272799983620644\n",
      "Epoch 857, Loss: 0.11087286192923784, Final Batch Loss: 0.023207256570458412\n",
      "Epoch 858, Loss: 0.11858823127113283, Final Batch Loss: 0.027273790910840034\n",
      "Epoch 859, Loss: 0.0805254599545151, Final Batch Loss: 0.0036712356377393007\n",
      "Epoch 860, Loss: 0.07985329627990723, Final Batch Loss: 0.028548697009682655\n",
      "Epoch 861, Loss: 0.11341230385005474, Final Batch Loss: 0.004092260263860226\n",
      "Epoch 862, Loss: 0.08846413344144821, Final Batch Loss: 0.014679976738989353\n",
      "Epoch 863, Loss: 0.07959301769733429, Final Batch Loss: 0.009942260570824146\n",
      "Epoch 864, Loss: 0.07790766190737486, Final Batch Loss: 0.022582069039344788\n",
      "Epoch 865, Loss: 0.07508943136781454, Final Batch Loss: 0.009202323853969574\n",
      "Epoch 866, Loss: 0.084932423196733, Final Batch Loss: 0.003728325944393873\n",
      "Epoch 867, Loss: 0.07217256631702185, Final Batch Loss: 0.02450493350625038\n",
      "Epoch 868, Loss: 0.1027808585204184, Final Batch Loss: 0.011302337050437927\n",
      "Epoch 869, Loss: 0.06446794047951698, Final Batch Loss: 0.009659809991717339\n",
      "Epoch 870, Loss: 0.08275648392736912, Final Batch Loss: 0.019504781812429428\n",
      "Epoch 871, Loss: 0.056145175360143185, Final Batch Loss: 0.011393445543944836\n",
      "Epoch 872, Loss: 0.08791938982903957, Final Batch Loss: 0.026519248262047768\n",
      "Epoch 873, Loss: 0.0677540092729032, Final Batch Loss: 0.015839288011193275\n",
      "Epoch 874, Loss: 0.053925516083836555, Final Batch Loss: 0.010658320970833302\n",
      "Epoch 875, Loss: 0.06818847078830004, Final Batch Loss: 0.0063746836967766285\n",
      "Epoch 876, Loss: 0.08727380260825157, Final Batch Loss: 0.027727508917450905\n",
      "Epoch 877, Loss: 0.07368971966207027, Final Batch Loss: 0.021813834086060524\n",
      "Epoch 878, Loss: 0.10574518237262964, Final Batch Loss: 0.01796760782599449\n",
      "Epoch 879, Loss: 0.09137062495574355, Final Batch Loss: 0.009105994366109371\n",
      "Epoch 880, Loss: 0.07304504932835698, Final Batch Loss: 0.03108501434326172\n",
      "Epoch 881, Loss: 0.10174433328211308, Final Batch Loss: 0.02654842473566532\n",
      "Epoch 882, Loss: 0.05943193985149264, Final Batch Loss: 0.00460036052390933\n",
      "Epoch 883, Loss: 0.06474153860472143, Final Batch Loss: 0.003567780600860715\n",
      "Epoch 884, Loss: 0.0554603454656899, Final Batch Loss: 0.008327804505825043\n",
      "Epoch 885, Loss: 0.086094681173563, Final Batch Loss: 0.008136439137160778\n",
      "Epoch 886, Loss: 0.06595518044196069, Final Batch Loss: 0.0048568532802164555\n",
      "Epoch 887, Loss: 0.054983717389404774, Final Batch Loss: 0.008030712604522705\n",
      "Epoch 888, Loss: 0.11897660652175546, Final Batch Loss: 0.04983225092291832\n",
      "Epoch 889, Loss: 0.060591545421630144, Final Batch Loss: 0.00447485176846385\n",
      "Epoch 890, Loss: 0.06826931959949434, Final Batch Loss: 0.0024008832406252623\n",
      "Epoch 891, Loss: 0.08980253431946039, Final Batch Loss: 0.0362880602478981\n",
      "Epoch 892, Loss: 0.12422862555831671, Final Batch Loss: 0.008881280198693275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 893, Loss: 0.10500240721739829, Final Batch Loss: 0.012651193886995316\n",
      "Epoch 894, Loss: 0.0815933474805206, Final Batch Loss: 0.013694718480110168\n",
      "Epoch 895, Loss: 0.05904011428356171, Final Batch Loss: 0.006417061202228069\n",
      "Epoch 896, Loss: 0.04038305813446641, Final Batch Loss: 0.012908870354294777\n",
      "Epoch 897, Loss: 0.13785723014734685, Final Batch Loss: 0.06759829819202423\n",
      "Epoch 898, Loss: 0.06409308197908103, Final Batch Loss: 0.002948545617982745\n",
      "Epoch 899, Loss: 0.07507067825645208, Final Batch Loss: 0.008876030333340168\n",
      "Epoch 900, Loss: 0.077533436473459, Final Batch Loss: 0.02097582258284092\n",
      "Epoch 901, Loss: 0.07289547380059958, Final Batch Loss: 0.009212047792971134\n",
      "Epoch 902, Loss: 0.07192762359045446, Final Batch Loss: 0.03204858675599098\n",
      "Epoch 903, Loss: 0.09018874960020185, Final Batch Loss: 0.0174233578145504\n",
      "Epoch 904, Loss: 0.07447406579740345, Final Batch Loss: 0.0020411519799381495\n",
      "Epoch 905, Loss: 0.06054013315588236, Final Batch Loss: 0.021946467459201813\n",
      "Epoch 906, Loss: 0.06722237030044198, Final Batch Loss: 0.006236367858946323\n",
      "Epoch 907, Loss: 0.09091390762478113, Final Batch Loss: 0.02335011586546898\n",
      "Epoch 908, Loss: 0.04159387154504657, Final Batch Loss: 0.009064526297152042\n",
      "Epoch 909, Loss: 0.06950593180954456, Final Batch Loss: 0.010239551775157452\n",
      "Epoch 910, Loss: 0.061293683014810085, Final Batch Loss: 0.02913636341691017\n",
      "Epoch 911, Loss: 0.04742353642359376, Final Batch Loss: 0.002013587858527899\n",
      "Epoch 912, Loss: 0.08520337101072073, Final Batch Loss: 0.008900020271539688\n",
      "Epoch 913, Loss: 0.08411631779745221, Final Batch Loss: 0.01957850158214569\n",
      "Epoch 914, Loss: 0.048952518263831735, Final Batch Loss: 0.003141972003504634\n",
      "Epoch 915, Loss: 0.07345264474861324, Final Batch Loss: 0.02637050487101078\n",
      "Epoch 916, Loss: 0.050951847224496305, Final Batch Loss: 0.0021045985631644726\n",
      "Epoch 917, Loss: 0.10111875552684069, Final Batch Loss: 0.027511995285749435\n",
      "Epoch 918, Loss: 0.0928553375415504, Final Batch Loss: 0.03431440144777298\n",
      "Epoch 919, Loss: 0.13559848722070456, Final Batch Loss: 0.020547883585095406\n",
      "Epoch 920, Loss: 0.09186114137992263, Final Batch Loss: 0.02716219238936901\n",
      "Epoch 921, Loss: 0.05494979443028569, Final Batch Loss: 0.008861048147082329\n",
      "Epoch 922, Loss: 0.08437227038666606, Final Batch Loss: 0.0077432128600776196\n",
      "Epoch 923, Loss: 0.06296377163380384, Final Batch Loss: 0.011193622834980488\n",
      "Epoch 924, Loss: 0.10721187479794025, Final Batch Loss: 0.02975207380950451\n",
      "Epoch 925, Loss: 0.058055365923792124, Final Batch Loss: 0.01976659521460533\n",
      "Epoch 926, Loss: 0.08021143777295947, Final Batch Loss: 0.020340250805020332\n",
      "Epoch 927, Loss: 0.08522431645542383, Final Batch Loss: 0.04021429643034935\n",
      "Epoch 928, Loss: 0.10399592900648713, Final Batch Loss: 0.007401070091873407\n",
      "Epoch 929, Loss: 0.07093423139303923, Final Batch Loss: 0.00979339238256216\n",
      "Epoch 930, Loss: 0.05660392518620938, Final Batch Loss: 0.016953786835074425\n",
      "Epoch 931, Loss: 0.06896746065467596, Final Batch Loss: 0.010311463847756386\n",
      "Epoch 932, Loss: 0.05024926504120231, Final Batch Loss: 0.003932374529540539\n",
      "Epoch 933, Loss: 0.11265695467591286, Final Batch Loss: 0.04805608466267586\n",
      "Epoch 934, Loss: 0.06544117536395788, Final Batch Loss: 0.014249785803258419\n",
      "Epoch 935, Loss: 0.07146881008520722, Final Batch Loss: 0.005803182255476713\n",
      "Epoch 936, Loss: 0.08396740350872278, Final Batch Loss: 0.006682013161480427\n",
      "Epoch 937, Loss: 0.055632613599300385, Final Batch Loss: 0.005021045450121164\n",
      "Epoch 938, Loss: 0.09584549441933632, Final Batch Loss: 0.02283593825995922\n",
      "Epoch 939, Loss: 0.08382184151560068, Final Batch Loss: 0.03219166770577431\n",
      "Epoch 940, Loss: 0.051438285037875175, Final Batch Loss: 0.008721455000340939\n",
      "Epoch 941, Loss: 0.042935925303027034, Final Batch Loss: 0.00435990933328867\n",
      "Epoch 942, Loss: 0.08510650880634785, Final Batch Loss: 0.01435249112546444\n",
      "Epoch 943, Loss: 0.1037156234961003, Final Batch Loss: 0.03193603828549385\n",
      "Epoch 944, Loss: 0.04645031085237861, Final Batch Loss: 0.0056431894190609455\n",
      "Epoch 945, Loss: 0.04718355601653457, Final Batch Loss: 0.005778813268989325\n",
      "Epoch 946, Loss: 0.04582676012068987, Final Batch Loss: 0.015979617834091187\n",
      "Epoch 947, Loss: 0.05834192084148526, Final Batch Loss: 0.02357124350965023\n",
      "Epoch 948, Loss: 0.06995491730049253, Final Batch Loss: 0.014079957269132137\n",
      "Epoch 949, Loss: 0.047997108194977045, Final Batch Loss: 0.003738938132300973\n",
      "Epoch 950, Loss: 0.04838040051981807, Final Batch Loss: 0.0031464346684515476\n",
      "Epoch 951, Loss: 0.04621097398921847, Final Batch Loss: 0.005413050297647715\n",
      "Epoch 952, Loss: 0.07033294253051281, Final Batch Loss: 0.014184259809553623\n",
      "Epoch 953, Loss: 0.09019720554351807, Final Batch Loss: 0.017338506877422333\n",
      "Epoch 954, Loss: 0.055692432913929224, Final Batch Loss: 0.008989221416413784\n",
      "Epoch 955, Loss: 0.03516848245635629, Final Batch Loss: 0.007112984079867601\n",
      "Epoch 956, Loss: 0.0800743387080729, Final Batch Loss: 0.022732915356755257\n",
      "Epoch 957, Loss: 0.07163180597126484, Final Batch Loss: 0.017783060669898987\n",
      "Epoch 958, Loss: 0.0739789754152298, Final Batch Loss: 0.017007263377308846\n",
      "Epoch 959, Loss: 0.05323476204648614, Final Batch Loss: 0.006115295458585024\n",
      "Epoch 960, Loss: 0.08987428597174585, Final Batch Loss: 0.0032803646754473448\n",
      "Epoch 961, Loss: 0.03916324838064611, Final Batch Loss: 0.005232435651123524\n",
      "Epoch 962, Loss: 0.1131023212801665, Final Batch Loss: 0.015085441991686821\n",
      "Epoch 963, Loss: 0.058975315652787685, Final Batch Loss: 0.009979387745261192\n",
      "Epoch 964, Loss: 0.11046029115095735, Final Batch Loss: 0.009075082838535309\n",
      "Epoch 965, Loss: 0.07136574853211641, Final Batch Loss: 0.007068007718771696\n",
      "Epoch 966, Loss: 0.05350577412173152, Final Batch Loss: 0.013890733011066914\n",
      "Epoch 967, Loss: 0.13018062338232994, Final Batch Loss: 0.0077891163527965546\n",
      "Epoch 968, Loss: 0.04517464269883931, Final Batch Loss: 0.007151379715651274\n",
      "Epoch 969, Loss: 0.047494683181867, Final Batch Loss: 0.003074100473895669\n",
      "Epoch 970, Loss: 0.05369416205212474, Final Batch Loss: 0.005067677702754736\n",
      "Epoch 971, Loss: 0.059367429465055466, Final Batch Loss: 0.006232960615307093\n",
      "Epoch 972, Loss: 0.0820886674337089, Final Batch Loss: 0.017777852714061737\n",
      "Epoch 973, Loss: 0.05315078585408628, Final Batch Loss: 0.01789763942360878\n",
      "Epoch 974, Loss: 0.0615378119982779, Final Batch Loss: 0.020617397502064705\n",
      "Epoch 975, Loss: 0.07027611904777586, Final Batch Loss: 0.005943352822214365\n",
      "Epoch 976, Loss: 0.09735672338865697, Final Batch Loss: 0.002814166946336627\n",
      "Epoch 977, Loss: 0.11455202009528875, Final Batch Loss: 0.01775878109037876\n",
      "Epoch 978, Loss: 0.06694557284936309, Final Batch Loss: 0.016976499930024147\n",
      "Epoch 979, Loss: 0.13518665311858058, Final Batch Loss: 0.01373340655118227\n",
      "Epoch 980, Loss: 0.06021210225299001, Final Batch Loss: 0.012215811759233475\n",
      "Epoch 981, Loss: 0.10341657511889935, Final Batch Loss: 0.011251792311668396\n",
      "Epoch 982, Loss: 0.06676431559026241, Final Batch Loss: 0.010009716264903545\n",
      "Epoch 983, Loss: 0.11322554107755423, Final Batch Loss: 0.022725610062479973\n",
      "Epoch 984, Loss: 0.08613248681649566, Final Batch Loss: 0.01851588487625122\n",
      "Epoch 985, Loss: 0.09715373488143086, Final Batch Loss: 0.04952346906065941\n",
      "Epoch 986, Loss: 0.05516621074639261, Final Batch Loss: 0.0022619024384766817\n",
      "Epoch 987, Loss: 0.05809808545745909, Final Batch Loss: 0.00599720049649477\n",
      "Epoch 988, Loss: 0.0963855660520494, Final Batch Loss: 0.0016224156133830547\n",
      "Epoch 989, Loss: 0.06682980293408036, Final Batch Loss: 0.013987382873892784\n",
      "Epoch 990, Loss: 0.05596145335584879, Final Batch Loss: 0.010422755964100361\n",
      "Epoch 991, Loss: 0.09127573482692242, Final Batch Loss: 0.007177365943789482\n",
      "Epoch 992, Loss: 0.07552468357607722, Final Batch Loss: 0.021049680188298225\n",
      "Epoch 993, Loss: 0.10095582017675042, Final Batch Loss: 0.040321946144104004\n",
      "Epoch 994, Loss: 0.06465506600216031, Final Batch Loss: 0.008089614100754261\n",
      "Epoch 995, Loss: 0.03873154707252979, Final Batch Loss: 0.013344880193471909\n",
      "Epoch 996, Loss: 0.04988388845231384, Final Batch Loss: 0.020077195018529892\n",
      "Epoch 997, Loss: 0.0660546263679862, Final Batch Loss: 0.005111560691148043\n",
      "Epoch 998, Loss: 0.04921579919755459, Final Batch Loss: 0.006443859077990055\n",
      "Epoch 999, Loss: 0.06183742848224938, Final Batch Loss: 0.0023562368005514145\n",
      "Epoch 1000, Loss: 0.0665729702450335, Final Batch Loss: 0.015845980495214462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1001, Loss: 0.08732329215854406, Final Batch Loss: 0.006605484522879124\n",
      "Epoch 1002, Loss: 0.059540183283388615, Final Batch Loss: 0.008218741975724697\n",
      "Epoch 1003, Loss: 0.04934736015275121, Final Batch Loss: 0.006487449165433645\n",
      "Epoch 1004, Loss: 0.04767990508116782, Final Batch Loss: 0.0038447559345513582\n",
      "Epoch 1005, Loss: 0.06076162587851286, Final Batch Loss: 0.024252742528915405\n",
      "Epoch 1006, Loss: 0.04839339666068554, Final Batch Loss: 0.00884160678833723\n",
      "Epoch 1007, Loss: 0.0326794998254627, Final Batch Loss: 0.004955267999321222\n",
      "Epoch 1008, Loss: 0.043909638188779354, Final Batch Loss: 0.005624192766845226\n",
      "Epoch 1009, Loss: 0.07362237758934498, Final Batch Loss: 0.03386969864368439\n",
      "Epoch 1010, Loss: 0.06690161547157913, Final Batch Loss: 0.03197874873876572\n",
      "Epoch 1011, Loss: 0.08948054653592408, Final Batch Loss: 0.013976194895803928\n",
      "Epoch 1012, Loss: 0.10411583445966244, Final Batch Loss: 0.025099802762269974\n",
      "Epoch 1013, Loss: 0.07571911485865712, Final Batch Loss: 0.02136707305908203\n",
      "Epoch 1014, Loss: 0.07037194236181676, Final Batch Loss: 0.0067295171320438385\n",
      "Epoch 1015, Loss: 0.13745216187089682, Final Batch Loss: 0.015798399224877357\n",
      "Epoch 1016, Loss: 0.053616206627339125, Final Batch Loss: 0.0021490135695785284\n",
      "Epoch 1017, Loss: 0.05035258864518255, Final Batch Loss: 0.008808230981230736\n",
      "Epoch 1018, Loss: 0.049139734357595444, Final Batch Loss: 0.005329687613993883\n",
      "Epoch 1019, Loss: 0.06829375447705388, Final Batch Loss: 0.01631123758852482\n",
      "Epoch 1020, Loss: 0.05621142452582717, Final Batch Loss: 0.0034746224991977215\n",
      "Epoch 1021, Loss: 0.05287946085445583, Final Batch Loss: 0.002152148401364684\n",
      "Epoch 1022, Loss: 0.05638542911037803, Final Batch Loss: 0.008254535496234894\n",
      "Epoch 1023, Loss: 0.046315440675243735, Final Batch Loss: 0.008549310266971588\n",
      "Epoch 1024, Loss: 0.06251011556014419, Final Batch Loss: 0.00451783649623394\n",
      "Epoch 1025, Loss: 0.08177757216617465, Final Batch Loss: 0.004853463731706142\n",
      "Epoch 1026, Loss: 0.03567473334260285, Final Batch Loss: 0.002718430245295167\n",
      "Epoch 1027, Loss: 0.0627322569489479, Final Batch Loss: 0.008301693946123123\n",
      "Epoch 1028, Loss: 0.04552124999463558, Final Batch Loss: 0.005061426665633917\n",
      "Epoch 1029, Loss: 0.06766239763237536, Final Batch Loss: 0.0019006242509931326\n",
      "Epoch 1030, Loss: 0.07482761656865478, Final Batch Loss: 0.016432294622063637\n",
      "Epoch 1031, Loss: 0.08349652076140046, Final Batch Loss: 0.012013386003673077\n",
      "Epoch 1032, Loss: 0.0422550227958709, Final Batch Loss: 0.00237620179541409\n",
      "Epoch 1033, Loss: 0.02757705538533628, Final Batch Loss: 0.0036913766525685787\n",
      "Epoch 1034, Loss: 0.04157058079726994, Final Batch Loss: 0.011335226707160473\n",
      "Epoch 1035, Loss: 0.04132501548156142, Final Batch Loss: 0.004420523997396231\n",
      "Epoch 1036, Loss: 0.07572402339428663, Final Batch Loss: 0.004171121399849653\n",
      "Epoch 1037, Loss: 0.05300487345084548, Final Batch Loss: 0.004634329117834568\n",
      "Epoch 1038, Loss: 0.027434482937678695, Final Batch Loss: 0.00139753264375031\n",
      "Epoch 1039, Loss: 0.06885789567604661, Final Batch Loss: 0.02196372114121914\n",
      "Epoch 1040, Loss: 0.06458760262466967, Final Batch Loss: 0.010028813034296036\n",
      "Epoch 1041, Loss: 0.053898663027212024, Final Batch Loss: 0.0022679234389215708\n",
      "Epoch 1042, Loss: 0.05595741420984268, Final Batch Loss: 0.010800210759043694\n",
      "Epoch 1043, Loss: 0.0709396917372942, Final Batch Loss: 0.017447970807552338\n",
      "Epoch 1044, Loss: 0.10055013792589307, Final Batch Loss: 0.004285889212042093\n",
      "Epoch 1045, Loss: 0.08847932098433375, Final Batch Loss: 0.01784701459109783\n",
      "Epoch 1046, Loss: 0.06371139036491513, Final Batch Loss: 0.01008214708417654\n",
      "Epoch 1047, Loss: 0.045167345087975264, Final Batch Loss: 0.017367517575621605\n",
      "Epoch 1048, Loss: 0.10096617927774787, Final Batch Loss: 0.008268623612821102\n",
      "Epoch 1049, Loss: 0.05975064309313893, Final Batch Loss: 0.0077734654769301414\n",
      "Epoch 1050, Loss: 0.034300585743039846, Final Batch Loss: 0.007568923756480217\n",
      "Epoch 1051, Loss: 0.08188261813484132, Final Batch Loss: 0.0034062189515680075\n",
      "Epoch 1052, Loss: 0.04968575364910066, Final Batch Loss: 0.00865152757614851\n",
      "Epoch 1053, Loss: 0.06371891684830189, Final Batch Loss: 0.0044133346527814865\n",
      "Epoch 1054, Loss: 0.06217161472886801, Final Batch Loss: 0.011794008314609528\n",
      "Epoch 1055, Loss: 0.042052230797708035, Final Batch Loss: 0.0029743225313723087\n",
      "Epoch 1056, Loss: 0.04002950340509415, Final Batch Loss: 0.003454830264672637\n",
      "Epoch 1057, Loss: 0.04012599331326783, Final Batch Loss: 0.007866970263421535\n",
      "Epoch 1058, Loss: 0.12060741242021322, Final Batch Loss: 0.022422388195991516\n",
      "Epoch 1059, Loss: 0.03883990412577987, Final Batch Loss: 0.004939399193972349\n",
      "Epoch 1060, Loss: 0.07003086898475885, Final Batch Loss: 0.009483782574534416\n",
      "Epoch 1061, Loss: 0.05443711020052433, Final Batch Loss: 0.004576226696372032\n",
      "Epoch 1062, Loss: 0.06786835892125964, Final Batch Loss: 0.0070072319358587265\n",
      "Epoch 1063, Loss: 0.07585206883959472, Final Batch Loss: 0.031316738575696945\n",
      "Epoch 1064, Loss: 0.09215775039047003, Final Batch Loss: 0.029717225581407547\n",
      "Epoch 1065, Loss: 0.06820690305903554, Final Batch Loss: 0.006930085830390453\n",
      "Epoch 1066, Loss: 0.05761405639350414, Final Batch Loss: 0.020980309695005417\n",
      "Epoch 1067, Loss: 0.05728345736861229, Final Batch Loss: 0.012236056849360466\n",
      "Epoch 1068, Loss: 0.06441342597827315, Final Batch Loss: 0.0050392658449709415\n",
      "Epoch 1069, Loss: 0.05901427986100316, Final Batch Loss: 0.03158368542790413\n",
      "Epoch 1070, Loss: 0.07624034327454865, Final Batch Loss: 0.010906611569225788\n",
      "Epoch 1071, Loss: 0.06425305921584368, Final Batch Loss: 0.0243426151573658\n",
      "Epoch 1072, Loss: 0.05071955779567361, Final Batch Loss: 0.009123923256993294\n",
      "Epoch 1073, Loss: 0.04257814702577889, Final Batch Loss: 0.0019515806343406439\n",
      "Epoch 1074, Loss: 0.06568577326834202, Final Batch Loss: 0.012796231545507908\n",
      "Epoch 1075, Loss: 0.05040515947621316, Final Batch Loss: 0.01670244336128235\n",
      "Epoch 1076, Loss: 0.07307594921439886, Final Batch Loss: 0.004203526768833399\n",
      "Epoch 1077, Loss: 0.057749729487113655, Final Batch Loss: 0.0011041642865166068\n",
      "Epoch 1078, Loss: 0.040954826632514596, Final Batch Loss: 0.00472407229244709\n",
      "Epoch 1079, Loss: 0.10070011531934142, Final Batch Loss: 0.02443590760231018\n",
      "Epoch 1080, Loss: 0.049804302398115396, Final Batch Loss: 0.008306375704705715\n",
      "Epoch 1081, Loss: 0.03838652605190873, Final Batch Loss: 0.003400728339329362\n",
      "Epoch 1082, Loss: 0.04460217687301338, Final Batch Loss: 0.01343543827533722\n",
      "Epoch 1083, Loss: 0.06558491382747889, Final Batch Loss: 0.012727687135338783\n",
      "Epoch 1084, Loss: 0.105126210488379, Final Batch Loss: 0.012427814304828644\n",
      "Epoch 1085, Loss: 0.07992897590156645, Final Batch Loss: 0.032074879854917526\n",
      "Epoch 1086, Loss: 0.08406427130103111, Final Batch Loss: 0.010096295736730099\n",
      "Epoch 1087, Loss: 0.074164206860587, Final Batch Loss: 0.003486083587631583\n",
      "Epoch 1088, Loss: 0.05235449457541108, Final Batch Loss: 0.015198745764791965\n",
      "Epoch 1089, Loss: 0.03521091025322676, Final Batch Loss: 0.004683705046772957\n",
      "Epoch 1090, Loss: 0.0634826379828155, Final Batch Loss: 0.009824949316680431\n",
      "Epoch 1091, Loss: 0.08666792791336775, Final Batch Loss: 0.008511362597346306\n",
      "Epoch 1092, Loss: 0.03276595915667713, Final Batch Loss: 0.004581826739013195\n",
      "Epoch 1093, Loss: 0.061535286949947476, Final Batch Loss: 0.0028937729075551033\n",
      "Epoch 1094, Loss: 0.04821757401805371, Final Batch Loss: 0.00374641758389771\n",
      "Epoch 1095, Loss: 0.07692684582434595, Final Batch Loss: 0.04438760504126549\n",
      "Epoch 1096, Loss: 0.06096820125821978, Final Batch Loss: 0.005204466171562672\n",
      "Epoch 1097, Loss: 0.03914479026570916, Final Batch Loss: 0.006603873334825039\n",
      "Epoch 1098, Loss: 0.04162011109292507, Final Batch Loss: 0.009153923019766808\n",
      "Epoch 1099, Loss: 0.052510341396555305, Final Batch Loss: 0.007879499346017838\n",
      "Epoch 1100, Loss: 0.04149212525226176, Final Batch Loss: 0.0023268761578947306\n",
      "Epoch 1101, Loss: 0.08475935319438577, Final Batch Loss: 0.026531219482421875\n",
      "Epoch 1102, Loss: 0.03823072323575616, Final Batch Loss: 0.013494051992893219\n",
      "Epoch 1103, Loss: 0.058298420161008835, Final Batch Loss: 0.030475221574306488\n",
      "Epoch 1104, Loss: 0.04138303780928254, Final Batch Loss: 0.002967351581901312\n",
      "Epoch 1105, Loss: 0.04528028704226017, Final Batch Loss: 0.006022254936397076\n",
      "Epoch 1106, Loss: 0.0485225438605994, Final Batch Loss: 0.01070340909063816\n",
      "Epoch 1107, Loss: 0.03449071547947824, Final Batch Loss: 0.008085099048912525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1108, Loss: 0.06293407804332674, Final Batch Loss: 0.011617850512266159\n",
      "Epoch 1109, Loss: 0.05409301444888115, Final Batch Loss: 0.005199723411351442\n",
      "Epoch 1110, Loss: 0.044397514779120684, Final Batch Loss: 0.0038932603783905506\n",
      "Epoch 1111, Loss: 0.042470631655305624, Final Batch Loss: 0.0029925028793513775\n",
      "Epoch 1112, Loss: 0.04424214921891689, Final Batch Loss: 0.016330447047948837\n",
      "Epoch 1113, Loss: 0.12922222586348653, Final Batch Loss: 0.029962442815303802\n",
      "Epoch 1114, Loss: 0.09806906362064183, Final Batch Loss: 0.008401882834732533\n",
      "Epoch 1115, Loss: 0.035104490583762527, Final Batch Loss: 0.007031346671283245\n",
      "Epoch 1116, Loss: 0.05689477897249162, Final Batch Loss: 0.028022902086377144\n",
      "Epoch 1117, Loss: 0.06316840555518866, Final Batch Loss: 0.005008598789572716\n",
      "Epoch 1118, Loss: 0.0743433793541044, Final Batch Loss: 0.03086361102759838\n",
      "Epoch 1119, Loss: 0.095388013869524, Final Batch Loss: 0.013055936433374882\n",
      "Epoch 1120, Loss: 0.05054420465603471, Final Batch Loss: 0.007186150178313255\n",
      "Epoch 1121, Loss: 0.0884217913262546, Final Batch Loss: 0.00821442250162363\n",
      "Epoch 1122, Loss: 0.07458491018041968, Final Batch Loss: 0.027260584756731987\n",
      "Epoch 1123, Loss: 0.03809485211968422, Final Batch Loss: 0.006401735357940197\n",
      "Epoch 1124, Loss: 0.08318129181861877, Final Batch Loss: 0.015713024884462357\n",
      "Epoch 1125, Loss: 0.07587772863917053, Final Batch Loss: 0.01149914413690567\n",
      "Epoch 1126, Loss: 0.06959779956378043, Final Batch Loss: 0.01417675893753767\n",
      "Epoch 1127, Loss: 0.06257167737931013, Final Batch Loss: 0.009476467035710812\n",
      "Epoch 1128, Loss: 0.11291815247386694, Final Batch Loss: 0.017474036663770676\n",
      "Epoch 1129, Loss: 0.058079677168279886, Final Batch Loss: 0.017317794263362885\n",
      "Epoch 1130, Loss: 0.07950238231569529, Final Batch Loss: 0.004957458004355431\n",
      "Epoch 1131, Loss: 0.09244172368198633, Final Batch Loss: 0.006170492619276047\n",
      "Epoch 1132, Loss: 0.05071357451379299, Final Batch Loss: 0.006372861098498106\n",
      "Epoch 1133, Loss: 0.04187473619822413, Final Batch Loss: 0.0011688339291140437\n",
      "Epoch 1134, Loss: 0.03462187072727829, Final Batch Loss: 0.0015882971929386258\n",
      "Epoch 1135, Loss: 0.03289702767506242, Final Batch Loss: 0.010607950389385223\n",
      "Epoch 1136, Loss: 0.07813984854146838, Final Batch Loss: 0.021926162764430046\n",
      "Epoch 1137, Loss: 0.04287507454864681, Final Batch Loss: 0.014671152457594872\n",
      "Epoch 1138, Loss: 0.04159733117558062, Final Batch Loss: 0.01584448292851448\n",
      "Epoch 1139, Loss: 0.06912988889962435, Final Batch Loss: 0.03306852653622627\n",
      "Epoch 1140, Loss: 0.04408081597648561, Final Batch Loss: 0.0014835139736533165\n",
      "Epoch 1141, Loss: 0.054367141565307975, Final Batch Loss: 0.0026457442436367273\n",
      "Epoch 1142, Loss: 0.04603692074306309, Final Batch Loss: 0.0032249020878225565\n",
      "Epoch 1143, Loss: 0.05079307081177831, Final Batch Loss: 0.013851569034159184\n",
      "Epoch 1144, Loss: 0.037364523159340024, Final Batch Loss: 0.002036873484030366\n",
      "Epoch 1145, Loss: 0.1143601443618536, Final Batch Loss: 0.05890524387359619\n",
      "Epoch 1146, Loss: 0.06527715444099158, Final Batch Loss: 0.0014566321624442935\n",
      "Epoch 1147, Loss: 0.05394196114502847, Final Batch Loss: 0.0020495483186095953\n",
      "Epoch 1148, Loss: 0.05483349587302655, Final Batch Loss: 0.02173866704106331\n",
      "Epoch 1149, Loss: 0.0379605779889971, Final Batch Loss: 0.0032668092753738165\n",
      "Epoch 1150, Loss: 0.061059636529535055, Final Batch Loss: 0.005040159914642572\n",
      "Epoch 1151, Loss: 0.07895604183431715, Final Batch Loss: 0.0073871552012860775\n",
      "Epoch 1152, Loss: 0.08989281300455332, Final Batch Loss: 0.0201176255941391\n",
      "Epoch 1153, Loss: 0.06149290967732668, Final Batch Loss: 0.007293888367712498\n",
      "Epoch 1154, Loss: 0.04458129371050745, Final Batch Loss: 0.0009318456286564469\n",
      "Epoch 1155, Loss: 0.05914671719074249, Final Batch Loss: 0.006812605075538158\n",
      "Epoch 1156, Loss: 0.08150891633704305, Final Batch Loss: 0.002890071365982294\n",
      "Epoch 1157, Loss: 0.044470178661867976, Final Batch Loss: 0.010258697904646397\n",
      "Epoch 1158, Loss: 0.08524173125624657, Final Batch Loss: 0.015572890639305115\n",
      "Epoch 1159, Loss: 0.06236644834280014, Final Batch Loss: 0.012102263048291206\n",
      "Epoch 1160, Loss: 0.045156576205044985, Final Batch Loss: 0.002147563500329852\n",
      "Epoch 1161, Loss: 0.06466906890273094, Final Batch Loss: 0.004654719494283199\n",
      "Epoch 1162, Loss: 0.035675095743499696, Final Batch Loss: 0.0016734058735892177\n",
      "Epoch 1163, Loss: 0.03589963633567095, Final Batch Loss: 0.008303619921207428\n",
      "Epoch 1164, Loss: 0.05916611501015723, Final Batch Loss: 0.0033364142291247845\n",
      "Epoch 1165, Loss: 0.04937291517853737, Final Batch Loss: 0.02443963848054409\n",
      "Epoch 1166, Loss: 0.041776257334277034, Final Batch Loss: 0.013684683479368687\n",
      "Epoch 1167, Loss: 0.035922591807320714, Final Batch Loss: 0.010420449078083038\n",
      "Epoch 1168, Loss: 0.03578820312395692, Final Batch Loss: 0.01445760577917099\n",
      "Epoch 1169, Loss: 0.03688089794013649, Final Batch Loss: 0.013004003092646599\n",
      "Epoch 1170, Loss: 0.029754182789474726, Final Batch Loss: 0.012550413608551025\n",
      "Epoch 1171, Loss: 0.0377773295622319, Final Batch Loss: 0.005065069999545813\n",
      "Epoch 1172, Loss: 0.021451774518936872, Final Batch Loss: 0.0048240600153803825\n",
      "Epoch 1173, Loss: 0.04823394725099206, Final Batch Loss: 0.017109103500843048\n",
      "Epoch 1174, Loss: 0.03613576106727123, Final Batch Loss: 0.00998570118099451\n",
      "Epoch 1175, Loss: 0.047501587541773915, Final Batch Loss: 0.012877685017883778\n",
      "Epoch 1176, Loss: 0.06204411690123379, Final Batch Loss: 0.02163625694811344\n",
      "Epoch 1177, Loss: 0.04649343993514776, Final Batch Loss: 0.00514095276594162\n",
      "Epoch 1178, Loss: 0.09364878328051418, Final Batch Loss: 0.006136795971542597\n",
      "Epoch 1179, Loss: 0.05309503176249564, Final Batch Loss: 0.010613715276122093\n",
      "Epoch 1180, Loss: 0.12587073631584644, Final Batch Loss: 0.03956389054656029\n",
      "Epoch 1181, Loss: 0.08098300220444798, Final Batch Loss: 0.005340539384633303\n",
      "Epoch 1182, Loss: 0.043127435725182295, Final Batch Loss: 0.01686449721455574\n",
      "Epoch 1183, Loss: 0.10978651093319058, Final Batch Loss: 0.054566264152526855\n",
      "Epoch 1184, Loss: 0.06850329134613276, Final Batch Loss: 0.023202894255518913\n",
      "Epoch 1185, Loss: 0.11629315558820963, Final Batch Loss: 0.04821107164025307\n",
      "Epoch 1186, Loss: 0.04478205391205847, Final Batch Loss: 0.0016353812534362078\n",
      "Epoch 1187, Loss: 0.08433864041580819, Final Batch Loss: 0.01568671688437462\n",
      "Epoch 1188, Loss: 0.1048891730606556, Final Batch Loss: 0.010465615428984165\n",
      "Epoch 1189, Loss: 0.06639816053211689, Final Batch Loss: 0.01274691428989172\n",
      "Epoch 1190, Loss: 0.06850226980168372, Final Batch Loss: 0.006533817388117313\n",
      "Epoch 1191, Loss: 0.11493277130648494, Final Batch Loss: 0.08045735210180283\n",
      "Epoch 1192, Loss: 0.05486938031390309, Final Batch Loss: 0.013432917185127735\n",
      "Epoch 1193, Loss: 0.08768162608612329, Final Batch Loss: 0.043081384152173996\n",
      "Epoch 1194, Loss: 0.06621305854059756, Final Batch Loss: 0.024578509852290154\n",
      "Epoch 1195, Loss: 0.0412800470367074, Final Batch Loss: 0.009842893108725548\n",
      "Epoch 1196, Loss: 0.04489874327555299, Final Batch Loss: 0.004082987550646067\n",
      "Epoch 1197, Loss: 0.04931418574415147, Final Batch Loss: 0.01599457859992981\n",
      "Epoch 1198, Loss: 0.06189556419849396, Final Batch Loss: 0.004779793322086334\n",
      "Epoch 1199, Loss: 0.0404855259694159, Final Batch Loss: 0.0020143911242485046\n",
      "Epoch 1200, Loss: 0.08604593854397535, Final Batch Loss: 0.018653638660907745\n",
      "Epoch 1201, Loss: 0.037060776725411415, Final Batch Loss: 0.0044182115234434605\n",
      "Epoch 1202, Loss: 0.04019955359399319, Final Batch Loss: 0.005612707696855068\n",
      "Epoch 1203, Loss: 0.04880675405729562, Final Batch Loss: 0.004170933272689581\n",
      "Epoch 1204, Loss: 0.05088367243297398, Final Batch Loss: 0.0293098334223032\n",
      "Epoch 1205, Loss: 0.03423070104327053, Final Batch Loss: 0.019000142812728882\n",
      "Epoch 1206, Loss: 0.047523499582894146, Final Batch Loss: 0.016847534105181694\n",
      "Epoch 1207, Loss: 0.04682405013591051, Final Batch Loss: 0.016056645661592484\n",
      "Epoch 1208, Loss: 0.04243867681361735, Final Batch Loss: 0.002954935422167182\n",
      "Epoch 1209, Loss: 0.05608226452022791, Final Batch Loss: 0.010814100503921509\n",
      "Epoch 1210, Loss: 0.03693940211087465, Final Batch Loss: 0.001899871975183487\n",
      "Epoch 1211, Loss: 0.058944756048731506, Final Batch Loss: 0.012134196236729622\n",
      "Epoch 1212, Loss: 0.05848247231915593, Final Batch Loss: 0.0032592895440757275\n",
      "Epoch 1213, Loss: 0.06918757245875895, Final Batch Loss: 0.04022130370140076\n",
      "Epoch 1214, Loss: 0.053539885208010674, Final Batch Loss: 0.0022022768389433622\n",
      "Epoch 1215, Loss: 0.06814879924058914, Final Batch Loss: 0.023685207590460777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1216, Loss: 0.06751704798080027, Final Batch Loss: 0.003141801105812192\n",
      "Epoch 1217, Loss: 0.03675630991347134, Final Batch Loss: 0.005344280507415533\n",
      "Epoch 1218, Loss: 0.08684373134747148, Final Batch Loss: 0.002556666499003768\n",
      "Epoch 1219, Loss: 0.03392684110440314, Final Batch Loss: 0.006660180632025003\n",
      "Epoch 1220, Loss: 0.03304987004958093, Final Batch Loss: 0.008019212633371353\n",
      "Epoch 1221, Loss: 0.06464002304710448, Final Batch Loss: 0.01674279198050499\n",
      "Epoch 1222, Loss: 0.049229615833610296, Final Batch Loss: 0.0092829754576087\n",
      "Epoch 1223, Loss: 0.09801142709329724, Final Batch Loss: 0.018720336258411407\n",
      "Epoch 1224, Loss: 0.06973594380542636, Final Batch Loss: 0.009926649741828442\n",
      "Epoch 1225, Loss: 0.07966790650971234, Final Batch Loss: 0.003728886367753148\n",
      "Epoch 1226, Loss: 0.06540693249553442, Final Batch Loss: 0.009989933110773563\n",
      "Epoch 1227, Loss: 0.04423512052744627, Final Batch Loss: 0.01584807224571705\n",
      "Epoch 1228, Loss: 0.08389117941260338, Final Batch Loss: 0.011010118760168552\n",
      "Epoch 1229, Loss: 0.07502788887359202, Final Batch Loss: 0.005097298417240381\n",
      "Epoch 1230, Loss: 0.04274038318544626, Final Batch Loss: 0.004908534232527018\n",
      "Epoch 1231, Loss: 0.04829224478453398, Final Batch Loss: 0.001984863542020321\n",
      "Epoch 1232, Loss: 0.0587904192507267, Final Batch Loss: 0.030159717425704002\n",
      "Epoch 1233, Loss: 0.04805095819756389, Final Batch Loss: 0.030729766935110092\n",
      "Epoch 1234, Loss: 0.05211748066358268, Final Batch Loss: 0.004941775929182768\n",
      "Epoch 1235, Loss: 0.033306971890851855, Final Batch Loss: 0.008874057792127132\n",
      "Epoch 1236, Loss: 0.06273280782625079, Final Batch Loss: 0.004512877203524113\n",
      "Epoch 1237, Loss: 0.05329073476605117, Final Batch Loss: 0.00910110492259264\n",
      "Epoch 1238, Loss: 0.0674722061958164, Final Batch Loss: 0.028359925374388695\n",
      "Epoch 1239, Loss: 0.040684035047888756, Final Batch Loss: 0.004233630374073982\n",
      "Epoch 1240, Loss: 0.09003400709480047, Final Batch Loss: 0.0036521712318062782\n",
      "Epoch 1241, Loss: 0.047850025934167206, Final Batch Loss: 0.012774674221873283\n",
      "Epoch 1242, Loss: 0.08280954509973526, Final Batch Loss: 0.00471861008554697\n",
      "Epoch 1243, Loss: 0.03993533970788121, Final Batch Loss: 0.012972937896847725\n",
      "Epoch 1244, Loss: 0.036067876033484936, Final Batch Loss: 0.004466477781534195\n",
      "Epoch 1245, Loss: 0.04252916807308793, Final Batch Loss: 0.007078859489411116\n",
      "Epoch 1246, Loss: 0.03365002735517919, Final Batch Loss: 0.002464168006554246\n",
      "Epoch 1247, Loss: 0.024399269837886095, Final Batch Loss: 0.007931054569780827\n",
      "Epoch 1248, Loss: 0.0489269788376987, Final Batch Loss: 0.01272607035934925\n",
      "Epoch 1249, Loss: 0.09113533981144428, Final Batch Loss: 0.02133813686668873\n",
      "Epoch 1250, Loss: 0.043188760755583644, Final Batch Loss: 0.004314882215112448\n",
      "Epoch 1251, Loss: 0.047413200431037694, Final Batch Loss: 0.0009336346411146224\n",
      "Epoch 1252, Loss: 0.049221265013329685, Final Batch Loss: 0.015960490331053734\n",
      "Epoch 1253, Loss: 0.0406205328181386, Final Batch Loss: 0.0037574847228825092\n",
      "Epoch 1254, Loss: 0.05534636043012142, Final Batch Loss: 0.005302503239363432\n",
      "Epoch 1255, Loss: 0.07089082151651382, Final Batch Loss: 0.012419194914400578\n",
      "Epoch 1256, Loss: 0.03336033457890153, Final Batch Loss: 0.003365342738106847\n",
      "Epoch 1257, Loss: 0.03711613407358527, Final Batch Loss: 0.003972885198891163\n",
      "Epoch 1258, Loss: 0.09063157578930259, Final Batch Loss: 0.012398844584822655\n",
      "Epoch 1259, Loss: 0.04056496964767575, Final Batch Loss: 0.02066894620656967\n",
      "Epoch 1260, Loss: 0.040877963649109006, Final Batch Loss: 0.006500651128590107\n",
      "Epoch 1261, Loss: 0.05748461140319705, Final Batch Loss: 0.003538487944751978\n",
      "Epoch 1262, Loss: 0.019723560893908143, Final Batch Loss: 0.00357053242623806\n",
      "Epoch 1263, Loss: 0.028823278727941215, Final Batch Loss: 0.0015742338728159666\n",
      "Epoch 1264, Loss: 0.022810743539594114, Final Batch Loss: 0.003536157077178359\n",
      "Epoch 1265, Loss: 0.06131879542954266, Final Batch Loss: 0.0026422529481351376\n",
      "Epoch 1266, Loss: 0.03958921320736408, Final Batch Loss: 0.011339779943227768\n",
      "Epoch 1267, Loss: 0.05113995913416147, Final Batch Loss: 0.01529853604733944\n",
      "Epoch 1268, Loss: 0.0875229308148846, Final Batch Loss: 0.03272762522101402\n",
      "Epoch 1269, Loss: 0.060915905982255936, Final Batch Loss: 0.015438913367688656\n",
      "Epoch 1270, Loss: 0.056429714895784855, Final Batch Loss: 0.01270305085927248\n",
      "Epoch 1271, Loss: 0.038826134288683534, Final Batch Loss: 0.0015077153220772743\n",
      "Epoch 1272, Loss: 0.05639955820515752, Final Batch Loss: 0.007086264435201883\n",
      "Epoch 1273, Loss: 0.09738062415271997, Final Batch Loss: 0.02256835252046585\n",
      "Epoch 1274, Loss: 0.049871893133968115, Final Batch Loss: 0.028385020792484283\n",
      "Epoch 1275, Loss: 0.041465304559096694, Final Batch Loss: 0.00946325995028019\n",
      "Epoch 1276, Loss: 0.06083391420543194, Final Batch Loss: 0.00398850254714489\n",
      "Epoch 1277, Loss: 0.037956637563183904, Final Batch Loss: 0.009083337150514126\n",
      "Epoch 1278, Loss: 0.1056165846530348, Final Batch Loss: 0.04435966536402702\n",
      "Epoch 1279, Loss: 0.04960952512919903, Final Batch Loss: 0.015970293432474136\n",
      "Epoch 1280, Loss: 0.08303388906642795, Final Batch Loss: 0.0074859969317913055\n",
      "Epoch 1281, Loss: 0.024403021088801324, Final Batch Loss: 0.006618533283472061\n",
      "Epoch 1282, Loss: 0.019816881278529763, Final Batch Loss: 0.0022013746201992035\n",
      "Epoch 1283, Loss: 0.045713381143286824, Final Batch Loss: 0.007887172512710094\n",
      "Epoch 1284, Loss: 0.07084641128312796, Final Batch Loss: 0.0011733978753909469\n",
      "Epoch 1285, Loss: 0.027264003176242113, Final Batch Loss: 0.005124757997691631\n",
      "Epoch 1286, Loss: 0.03309454838745296, Final Batch Loss: 0.0052810790948569775\n",
      "Epoch 1287, Loss: 0.050766572123393416, Final Batch Loss: 0.011543109081685543\n",
      "Epoch 1288, Loss: 0.045797604601830244, Final Batch Loss: 0.013870268128812313\n",
      "Epoch 1289, Loss: 0.0170651440275833, Final Batch Loss: 0.0020068001467734575\n",
      "Epoch 1290, Loss: 0.04871275497134775, Final Batch Loss: 0.010420674458146095\n",
      "Epoch 1291, Loss: 0.04145440342836082, Final Batch Loss: 0.01284309383481741\n",
      "Epoch 1292, Loss: 0.05225835647433996, Final Batch Loss: 0.0026029786095023155\n",
      "Epoch 1293, Loss: 0.034635893418453634, Final Batch Loss: 0.00746806338429451\n",
      "Epoch 1294, Loss: 0.04188104724744335, Final Batch Loss: 0.0006204108358360827\n",
      "Epoch 1295, Loss: 0.03121205442585051, Final Batch Loss: 0.008184642530977726\n",
      "Epoch 1296, Loss: 0.036164339166134596, Final Batch Loss: 0.012117074802517891\n",
      "Epoch 1297, Loss: 0.03097999340388924, Final Batch Loss: 0.001173193915747106\n",
      "Epoch 1298, Loss: 0.05131015332881361, Final Batch Loss: 0.0014572638319805264\n",
      "Epoch 1299, Loss: 0.030362901743501425, Final Batch Loss: 0.003969608340412378\n",
      "Epoch 1300, Loss: 0.04195429524406791, Final Batch Loss: 0.0014179060235619545\n",
      "Epoch 1301, Loss: 0.04651606781408191, Final Batch Loss: 0.012919710949063301\n",
      "Epoch 1302, Loss: 0.029124057095032185, Final Batch Loss: 0.0011448467848822474\n",
      "Epoch 1303, Loss: 0.03817926382180303, Final Batch Loss: 0.0014694888377562165\n",
      "Epoch 1304, Loss: 0.048131830990314484, Final Batch Loss: 0.003640242386609316\n",
      "Epoch 1305, Loss: 0.05042681144550443, Final Batch Loss: 0.0022975504398345947\n",
      "Epoch 1306, Loss: 0.05216734204441309, Final Batch Loss: 0.014020845293998718\n",
      "Epoch 1307, Loss: 0.058396986685693264, Final Batch Loss: 0.015488826669752598\n",
      "Epoch 1308, Loss: 0.052297035697847605, Final Batch Loss: 0.01842329651117325\n",
      "Epoch 1309, Loss: 0.029846043908037245, Final Batch Loss: 0.0030489303171634674\n",
      "Epoch 1310, Loss: 0.027372919721528888, Final Batch Loss: 0.002505411859601736\n",
      "Epoch 1311, Loss: 0.044949304894544184, Final Batch Loss: 0.007867289707064629\n",
      "Epoch 1312, Loss: 0.03953198506496847, Final Batch Loss: 0.014789491891860962\n",
      "Epoch 1313, Loss: 0.039179271552711725, Final Batch Loss: 0.013142537325620651\n",
      "Epoch 1314, Loss: 0.07333743246272206, Final Batch Loss: 0.01612907089293003\n",
      "Epoch 1315, Loss: 0.036405757535248995, Final Batch Loss: 0.002721402794122696\n",
      "Epoch 1316, Loss: 0.055909412214532495, Final Batch Loss: 0.0016928231343626976\n",
      "Epoch 1317, Loss: 0.061010728823021054, Final Batch Loss: 0.009028580971062183\n",
      "Epoch 1318, Loss: 0.08501102635636926, Final Batch Loss: 0.031098758801817894\n",
      "Epoch 1319, Loss: 0.03575923456810415, Final Batch Loss: 0.0024303181562572718\n",
      "Epoch 1320, Loss: 0.022325445315800607, Final Batch Loss: 0.0026778627652674913\n",
      "Epoch 1321, Loss: 0.05877894232980907, Final Batch Loss: 0.01726355031132698\n",
      "Epoch 1322, Loss: 0.01808385062031448, Final Batch Loss: 0.0045513007789850235\n",
      "Epoch 1323, Loss: 0.03844071738421917, Final Batch Loss: 0.005802453961223364\n",
      "Epoch 1324, Loss: 0.035802849219180644, Final Batch Loss: 0.0077889044769108295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1325, Loss: 0.04519366554450244, Final Batch Loss: 0.0011087782913818955\n",
      "Epoch 1326, Loss: 0.05895089451223612, Final Batch Loss: 0.009699368849396706\n",
      "Epoch 1327, Loss: 0.03689890744863078, Final Batch Loss: 0.0009085322380997241\n",
      "Epoch 1328, Loss: 0.02218325389549136, Final Batch Loss: 0.0026013783644884825\n",
      "Epoch 1329, Loss: 0.030453615996520966, Final Batch Loss: 0.005024727433919907\n",
      "Epoch 1330, Loss: 0.03063593286788091, Final Batch Loss: 0.0009882394224405289\n",
      "Epoch 1331, Loss: 0.11303630657494068, Final Batch Loss: 0.026708200573921204\n",
      "Epoch 1332, Loss: 0.05465195479337126, Final Batch Loss: 0.011376087553799152\n",
      "Epoch 1333, Loss: 0.04889184865169227, Final Batch Loss: 0.003656202694401145\n",
      "Epoch 1334, Loss: 0.07903814769815654, Final Batch Loss: 0.0053060175850987434\n",
      "Epoch 1335, Loss: 0.04376966995187104, Final Batch Loss: 0.002023434964939952\n",
      "Epoch 1336, Loss: 0.027358809835277498, Final Batch Loss: 0.0024615200236439705\n",
      "Epoch 1337, Loss: 0.017966881743632257, Final Batch Loss: 0.0031216582283377647\n",
      "Epoch 1338, Loss: 0.03012115927413106, Final Batch Loss: 0.001694013481028378\n",
      "Epoch 1339, Loss: 0.048808479215949774, Final Batch Loss: 0.01896199956536293\n",
      "Epoch 1340, Loss: 0.02798107755370438, Final Batch Loss: 0.0012802040437236428\n",
      "Epoch 1341, Loss: 0.06905513675883412, Final Batch Loss: 0.007828403264284134\n",
      "Epoch 1342, Loss: 0.02787524415180087, Final Batch Loss: 0.0026680342853069305\n",
      "Epoch 1343, Loss: 0.05665782745927572, Final Batch Loss: 0.0037799235433340073\n",
      "Epoch 1344, Loss: 0.06609807466156781, Final Batch Loss: 0.014099176973104477\n",
      "Epoch 1345, Loss: 0.025723897153511643, Final Batch Loss: 0.004648702684789896\n",
      "Epoch 1346, Loss: 0.04723445954732597, Final Batch Loss: 0.009704594500362873\n",
      "Epoch 1347, Loss: 0.023620268795639277, Final Batch Loss: 0.0055729397572577\n",
      "Epoch 1348, Loss: 0.03370530891697854, Final Batch Loss: 0.024700939655303955\n",
      "Epoch 1349, Loss: 0.046143127139657736, Final Batch Loss: 0.010438711382448673\n",
      "Epoch 1350, Loss: 0.03234919591341168, Final Batch Loss: 0.001440992928110063\n",
      "Epoch 1351, Loss: 0.041756568709388375, Final Batch Loss: 0.005342787131667137\n",
      "Epoch 1352, Loss: 0.028606254723854363, Final Batch Loss: 0.00651633832603693\n",
      "Epoch 1353, Loss: 0.027568513294681907, Final Batch Loss: 0.002101070713251829\n",
      "Epoch 1354, Loss: 0.027387504698708653, Final Batch Loss: 0.0030243275687098503\n",
      "Epoch 1355, Loss: 0.02145956817548722, Final Batch Loss: 0.011681518517434597\n",
      "Epoch 1356, Loss: 0.018535337934736162, Final Batch Loss: 0.007841505110263824\n",
      "Epoch 1357, Loss: 0.031129182083532214, Final Batch Loss: 0.00419598538428545\n",
      "Epoch 1358, Loss: 0.028775419457815588, Final Batch Loss: 0.0017189843347296119\n",
      "Epoch 1359, Loss: 0.04285851516760886, Final Batch Loss: 0.016100488603115082\n",
      "Epoch 1360, Loss: 0.03279971540905535, Final Batch Loss: 0.007035744842141867\n",
      "Epoch 1361, Loss: 0.06401538266800344, Final Batch Loss: 0.012351266108453274\n",
      "Epoch 1362, Loss: 0.01806578505784273, Final Batch Loss: 0.0015541862230747938\n",
      "Epoch 1363, Loss: 0.05476136179640889, Final Batch Loss: 0.03336518257856369\n",
      "Epoch 1364, Loss: 0.018633109517395496, Final Batch Loss: 0.0013540075160562992\n",
      "Epoch 1365, Loss: 0.05294122511986643, Final Batch Loss: 0.021651098504662514\n",
      "Epoch 1366, Loss: 0.026889849570579827, Final Batch Loss: 0.008536403998732567\n",
      "Epoch 1367, Loss: 0.03337350452784449, Final Batch Loss: 0.018967505544424057\n",
      "Epoch 1368, Loss: 0.03199529857374728, Final Batch Loss: 0.0027074681129306555\n",
      "Epoch 1369, Loss: 0.06962435692548752, Final Batch Loss: 0.010347261093556881\n",
      "Epoch 1370, Loss: 0.10454928991384804, Final Batch Loss: 0.001655651954934001\n",
      "Epoch 1371, Loss: 0.03545261058025062, Final Batch Loss: 0.0019150772131979465\n",
      "Epoch 1372, Loss: 0.04193971143104136, Final Batch Loss: 0.003930977545678616\n",
      "Epoch 1373, Loss: 0.05150822037830949, Final Batch Loss: 0.0030591213144361973\n",
      "Epoch 1374, Loss: 0.0232295875903219, Final Batch Loss: 0.0030146120116114616\n",
      "Epoch 1375, Loss: 0.0486206472851336, Final Batch Loss: 0.02371569536626339\n",
      "Epoch 1376, Loss: 0.02806822268757969, Final Batch Loss: 0.013084870763123035\n",
      "Epoch 1377, Loss: 0.03742119949311018, Final Batch Loss: 0.00106267468072474\n",
      "Epoch 1378, Loss: 0.054574888898059726, Final Batch Loss: 0.021255970001220703\n",
      "Epoch 1379, Loss: 0.0178247457370162, Final Batch Loss: 0.0027346224524080753\n",
      "Epoch 1380, Loss: 0.04384373186621815, Final Batch Loss: 0.015861550346016884\n",
      "Epoch 1381, Loss: 0.06042267568409443, Final Batch Loss: 0.01958557218313217\n",
      "Epoch 1382, Loss: 0.048233287408947945, Final Batch Loss: 0.003517881501466036\n",
      "Epoch 1383, Loss: 0.07562768133357167, Final Batch Loss: 0.02219095453619957\n",
      "Epoch 1384, Loss: 0.09217626322060823, Final Batch Loss: 0.013065367937088013\n",
      "Epoch 1385, Loss: 0.027535739121958613, Final Batch Loss: 0.0053367954678833485\n",
      "Epoch 1386, Loss: 0.03865249338559806, Final Batch Loss: 0.0036265996750444174\n",
      "Epoch 1387, Loss: 0.04720139317214489, Final Batch Loss: 0.006014225073158741\n",
      "Epoch 1388, Loss: 0.04326111177215353, Final Batch Loss: 0.037334345281124115\n",
      "Epoch 1389, Loss: 0.04565918608568609, Final Batch Loss: 0.001545239589177072\n",
      "Epoch 1390, Loss: 0.03798225661739707, Final Batch Loss: 0.00981335062533617\n",
      "Epoch 1391, Loss: 0.02679882582742721, Final Batch Loss: 0.004594055935740471\n",
      "Epoch 1392, Loss: 0.044511616695672274, Final Batch Loss: 0.00490160658955574\n",
      "Epoch 1393, Loss: 0.04123416333459318, Final Batch Loss: 0.0011512392666190863\n",
      "Epoch 1394, Loss: 0.04492193786427379, Final Batch Loss: 0.022923162207007408\n",
      "Epoch 1395, Loss: 0.035629154182970524, Final Batch Loss: 0.008953857235610485\n",
      "Epoch 1396, Loss: 0.08622170239686966, Final Batch Loss: 0.004074634052813053\n",
      "Epoch 1397, Loss: 0.031095941842067987, Final Batch Loss: 0.018339743837714195\n",
      "Epoch 1398, Loss: 0.02291768474970013, Final Batch Loss: 0.0014586575562134385\n",
      "Epoch 1399, Loss: 0.049166526179760695, Final Batch Loss: 0.022758277133107185\n",
      "Epoch 1400, Loss: 0.02671414113137871, Final Batch Loss: 0.0013843895867466927\n",
      "Epoch 1401, Loss: 0.03842495277058333, Final Batch Loss: 0.00539429672062397\n",
      "Epoch 1402, Loss: 0.04244095005560666, Final Batch Loss: 0.004214778542518616\n",
      "Epoch 1403, Loss: 0.05794467683881521, Final Batch Loss: 0.007655023597180843\n",
      "Epoch 1404, Loss: 0.03797723678871989, Final Batch Loss: 0.015633797273039818\n",
      "Epoch 1405, Loss: 0.04671099246479571, Final Batch Loss: 0.0016322624869644642\n",
      "Epoch 1406, Loss: 0.05341073265299201, Final Batch Loss: 0.0111568383872509\n",
      "Epoch 1407, Loss: 0.04945373581722379, Final Batch Loss: 0.01439609657973051\n",
      "Epoch 1408, Loss: 0.055994478054344654, Final Batch Loss: 0.0014316842425614595\n",
      "Epoch 1409, Loss: 0.11844690167345107, Final Batch Loss: 0.07873810827732086\n",
      "Epoch 1410, Loss: 0.04976351326331496, Final Batch Loss: 0.007063477300107479\n",
      "Epoch 1411, Loss: 0.03869417938403785, Final Batch Loss: 0.0029545631259679794\n",
      "Epoch 1412, Loss: 0.023536587483249605, Final Batch Loss: 0.003548302920535207\n",
      "Epoch 1413, Loss: 0.06859778461512178, Final Batch Loss: 0.013864382170140743\n",
      "Epoch 1414, Loss: 0.043414575164206326, Final Batch Loss: 0.0024727240670472383\n",
      "Epoch 1415, Loss: 0.017899909522384405, Final Batch Loss: 0.002691336441785097\n",
      "Epoch 1416, Loss: 0.02013280161190778, Final Batch Loss: 0.009765959344804287\n",
      "Epoch 1417, Loss: 0.0286889155395329, Final Batch Loss: 0.011179784312844276\n",
      "Epoch 1418, Loss: 0.05750983615871519, Final Batch Loss: 0.014269083738327026\n",
      "Epoch 1419, Loss: 0.035206517204642296, Final Batch Loss: 0.006784466095268726\n",
      "Epoch 1420, Loss: 0.04290837369626388, Final Batch Loss: 0.004854424390941858\n",
      "Epoch 1421, Loss: 0.04683780437335372, Final Batch Loss: 0.006221540737897158\n",
      "Epoch 1422, Loss: 0.0415175324305892, Final Batch Loss: 0.008775131776928902\n",
      "Epoch 1423, Loss: 0.03740555595140904, Final Batch Loss: 0.00269125122576952\n",
      "Epoch 1424, Loss: 0.030773484497331083, Final Batch Loss: 0.009335202164947987\n",
      "Epoch 1425, Loss: 0.030885271029546857, Final Batch Loss: 0.013072721660137177\n",
      "Epoch 1426, Loss: 0.024440478766337037, Final Batch Loss: 0.0013513903832063079\n",
      "Epoch 1427, Loss: 0.0375916282646358, Final Batch Loss: 0.007018097210675478\n",
      "Epoch 1428, Loss: 0.032707858947105706, Final Batch Loss: 0.003463657107204199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1429, Loss: 0.03812999790534377, Final Batch Loss: 0.00943142268806696\n",
      "Epoch 1430, Loss: 0.026040160446427763, Final Batch Loss: 0.0006021683802828193\n",
      "Epoch 1431, Loss: 0.07166308723390102, Final Batch Loss: 0.0015845673624426126\n",
      "Epoch 1432, Loss: 0.05316386593040079, Final Batch Loss: 0.02009064331650734\n",
      "Epoch 1433, Loss: 0.031910368357785046, Final Batch Loss: 0.003925773315131664\n",
      "Epoch 1434, Loss: 0.026516078040003777, Final Batch Loss: 0.003131027100607753\n",
      "Epoch 1435, Loss: 0.01837170182261616, Final Batch Loss: 0.0028949379920959473\n",
      "Epoch 1436, Loss: 0.02196594199631363, Final Batch Loss: 0.0034325807355344296\n",
      "Epoch 1437, Loss: 0.044563884381204844, Final Batch Loss: 0.006026714574545622\n",
      "Epoch 1438, Loss: 0.020069415913894773, Final Batch Loss: 0.004421962890774012\n",
      "Epoch 1439, Loss: 0.07596410461701453, Final Batch Loss: 0.06491544842720032\n",
      "Epoch 1440, Loss: 0.03019250021316111, Final Batch Loss: 0.0030415428336709738\n",
      "Epoch 1441, Loss: 0.03609462536405772, Final Batch Loss: 0.011906861327588558\n",
      "Epoch 1442, Loss: 0.044662663189228624, Final Batch Loss: 0.013004503212869167\n",
      "Epoch 1443, Loss: 0.040482336247805506, Final Batch Loss: 0.02065192721784115\n",
      "Epoch 1444, Loss: 0.043143318383954465, Final Batch Loss: 0.007635657675564289\n",
      "Epoch 1445, Loss: 0.03907559020444751, Final Batch Loss: 0.01830713264644146\n",
      "Epoch 1446, Loss: 0.031788486638106406, Final Batch Loss: 0.003264995291829109\n",
      "Epoch 1447, Loss: 0.021715544629842043, Final Batch Loss: 0.005485094618052244\n",
      "Epoch 1448, Loss: 0.05497468123212457, Final Batch Loss: 0.012496082112193108\n",
      "Epoch 1449, Loss: 0.05888962259632535, Final Batch Loss: 0.0024701026268303394\n",
      "Epoch 1450, Loss: 0.03356345766223967, Final Batch Loss: 0.006454572547227144\n",
      "Epoch 1451, Loss: 0.02343949896749109, Final Batch Loss: 0.002213655970990658\n",
      "Epoch 1452, Loss: 0.022219980834051967, Final Batch Loss: 0.0032843532972037792\n",
      "Epoch 1453, Loss: 0.03413933044066653, Final Batch Loss: 0.013261505402624607\n",
      "Epoch 1454, Loss: 0.027204061043448746, Final Batch Loss: 0.007733136415481567\n",
      "Epoch 1455, Loss: 0.04395506129367277, Final Batch Loss: 0.02275920659303665\n",
      "Epoch 1456, Loss: 0.022264273604378104, Final Batch Loss: 0.002254295162856579\n",
      "Epoch 1457, Loss: 0.027651646232698113, Final Batch Loss: 0.004231716506183147\n",
      "Epoch 1458, Loss: 0.02758669131435454, Final Batch Loss: 0.0011837028432637453\n",
      "Epoch 1459, Loss: 0.02147319447249174, Final Batch Loss: 0.005394155625253916\n",
      "Epoch 1460, Loss: 0.06257155258208513, Final Batch Loss: 0.010959943756461143\n",
      "Epoch 1461, Loss: 0.03278080839663744, Final Batch Loss: 0.01685337722301483\n",
      "Epoch 1462, Loss: 0.028525357076432556, Final Batch Loss: 0.000772323866840452\n",
      "Epoch 1463, Loss: 0.021103590610437095, Final Batch Loss: 0.0006595508893951774\n",
      "Epoch 1464, Loss: 0.030958789982832968, Final Batch Loss: 0.00035606720484793186\n",
      "Epoch 1465, Loss: 0.017753003223333508, Final Batch Loss: 0.006194888148456812\n",
      "Epoch 1466, Loss: 0.024573937756940722, Final Batch Loss: 0.00697081396356225\n",
      "Epoch 1467, Loss: 0.016856207512319088, Final Batch Loss: 0.0016153666656464338\n",
      "Epoch 1468, Loss: 0.025039924308657646, Final Batch Loss: 0.007933284156024456\n",
      "Epoch 1469, Loss: 0.03598150238394737, Final Batch Loss: 0.007255845237523317\n",
      "Epoch 1470, Loss: 0.024820109421852976, Final Batch Loss: 0.004245426040142775\n",
      "Epoch 1471, Loss: 0.035436284029856324, Final Batch Loss: 0.02060490846633911\n",
      "Epoch 1472, Loss: 0.04808490595314652, Final Batch Loss: 0.005725903436541557\n",
      "Epoch 1473, Loss: 0.01407241157721728, Final Batch Loss: 0.002383375307545066\n",
      "Epoch 1474, Loss: 0.02028932562097907, Final Batch Loss: 0.002104243030771613\n",
      "Epoch 1475, Loss: 0.0323415455641225, Final Batch Loss: 0.00531733687967062\n",
      "Epoch 1476, Loss: 0.035695386468432844, Final Batch Loss: 0.0014585525495931506\n",
      "Epoch 1477, Loss: 0.03510356100741774, Final Batch Loss: 0.009154707193374634\n",
      "Epoch 1478, Loss: 0.0278196653816849, Final Batch Loss: 0.013881183229386806\n",
      "Epoch 1479, Loss: 0.028464797243941575, Final Batch Loss: 0.012332508340477943\n",
      "Epoch 1480, Loss: 0.06074682914186269, Final Batch Loss: 0.009989733807742596\n",
      "Epoch 1481, Loss: 0.05611025227699429, Final Batch Loss: 0.04270884394645691\n",
      "Epoch 1482, Loss: 0.023734345682896674, Final Batch Loss: 0.014526856131851673\n",
      "Epoch 1483, Loss: 0.017919537029229105, Final Batch Loss: 0.0012971599353477359\n",
      "Epoch 1484, Loss: 0.03905927494633943, Final Batch Loss: 0.025828028097748756\n",
      "Epoch 1485, Loss: 0.02179136930499226, Final Batch Loss: 0.0020341738127171993\n",
      "Epoch 1486, Loss: 0.08630059740971774, Final Batch Loss: 0.006584463641047478\n",
      "Epoch 1487, Loss: 0.04331657849252224, Final Batch Loss: 0.019665902480483055\n",
      "Epoch 1488, Loss: 0.017293899436481297, Final Batch Loss: 0.0012302525574341416\n",
      "Epoch 1489, Loss: 0.010947023110929877, Final Batch Loss: 0.004389833193272352\n",
      "Epoch 1490, Loss: 0.10166637180373073, Final Batch Loss: 0.005152293946594\n",
      "Epoch 1491, Loss: 0.04333123750984669, Final Batch Loss: 0.005037579219788313\n",
      "Epoch 1492, Loss: 0.023654297168832272, Final Batch Loss: 0.005540722515434027\n",
      "Epoch 1493, Loss: 0.0368687188019976, Final Batch Loss: 0.005520309321582317\n",
      "Epoch 1494, Loss: 0.013834294863045216, Final Batch Loss: 0.004328614100813866\n",
      "Epoch 1495, Loss: 0.05314194317907095, Final Batch Loss: 0.0004281187430024147\n",
      "Epoch 1496, Loss: 0.0647678857203573, Final Batch Loss: 0.0047612241469323635\n",
      "Epoch 1497, Loss: 0.09764208644628525, Final Batch Loss: 0.0038897045888006687\n",
      "Epoch 1498, Loss: 0.033827462233603, Final Batch Loss: 0.005536441225558519\n",
      "Epoch 1499, Loss: 0.010252597101498395, Final Batch Loss: 0.0007992371101863682\n",
      "Epoch 1500, Loss: 0.026058560935780406, Final Batch Loss: 0.015117672272026539\n",
      "Epoch 1501, Loss: 0.014271977648604661, Final Batch Loss: 0.0005362379015423357\n",
      "Epoch 1502, Loss: 0.022898840892594308, Final Batch Loss: 0.0006937538855709136\n",
      "Epoch 1503, Loss: 0.036115919006988406, Final Batch Loss: 0.011635646224021912\n",
      "Epoch 1504, Loss: 0.025449447799474, Final Batch Loss: 0.0016630323370918632\n",
      "Epoch 1505, Loss: 0.010090399184264243, Final Batch Loss: 0.001424332964234054\n",
      "Epoch 1506, Loss: 0.05631588096730411, Final Batch Loss: 0.0035625470336526632\n",
      "Epoch 1507, Loss: 0.03218273964012042, Final Batch Loss: 0.0012883145827800035\n",
      "Epoch 1508, Loss: 0.06446949439123273, Final Batch Loss: 0.00541006401181221\n",
      "Epoch 1509, Loss: 0.05252515466418117, Final Batch Loss: 0.02899700589478016\n",
      "Epoch 1510, Loss: 0.037105291325133294, Final Batch Loss: 0.00048757222248241305\n",
      "Epoch 1511, Loss: 0.018630358099471778, Final Batch Loss: 0.007376410532742739\n",
      "Epoch 1512, Loss: 0.012583293544594198, Final Batch Loss: 0.0008192189852707088\n",
      "Epoch 1513, Loss: 0.04986231727525592, Final Batch Loss: 0.004658848512917757\n",
      "Epoch 1514, Loss: 0.046153059345670044, Final Batch Loss: 0.018074704334139824\n",
      "Epoch 1515, Loss: 0.026868958491832018, Final Batch Loss: 0.0044747693464159966\n",
      "Epoch 1516, Loss: 0.031369566451758146, Final Batch Loss: 0.019595636054873466\n",
      "Epoch 1517, Loss: 0.039479213824961334, Final Batch Loss: 0.000787918281275779\n",
      "Epoch 1518, Loss: 0.04551220126450062, Final Batch Loss: 0.014034810476005077\n",
      "Epoch 1519, Loss: 0.04052157775731757, Final Batch Loss: 0.013095091097056866\n",
      "Epoch 1520, Loss: 0.04336596489883959, Final Batch Loss: 0.026029031723737717\n",
      "Epoch 1521, Loss: 0.04737016203580424, Final Batch Loss: 0.016950519755482674\n",
      "Epoch 1522, Loss: 0.03492681321222335, Final Batch Loss: 0.0009885748149827123\n",
      "Epoch 1523, Loss: 0.07766876416280866, Final Batch Loss: 0.0024014818482100964\n",
      "Epoch 1524, Loss: 0.03900229965802282, Final Batch Loss: 0.006439125165343285\n",
      "Epoch 1525, Loss: 0.06547391787171364, Final Batch Loss: 0.005174446851015091\n",
      "Epoch 1526, Loss: 0.03680904977954924, Final Batch Loss: 0.0007981979288160801\n",
      "Epoch 1527, Loss: 0.022111808066256344, Final Batch Loss: 0.0016573407920077443\n",
      "Epoch 1528, Loss: 0.021963095292448997, Final Batch Loss: 0.002843238180503249\n",
      "Epoch 1529, Loss: 0.04260611056815833, Final Batch Loss: 0.0018520265584811568\n",
      "Epoch 1530, Loss: 0.05171885865274817, Final Batch Loss: 0.0052146692760288715\n",
      "Epoch 1531, Loss: 0.02617545030079782, Final Batch Loss: 0.013231485150754452\n",
      "Epoch 1532, Loss: 0.0510953877819702, Final Batch Loss: 0.011882747523486614\n",
      "Epoch 1533, Loss: 0.055233756080269814, Final Batch Loss: 0.01995127461850643\n",
      "Epoch 1534, Loss: 0.05842705839313567, Final Batch Loss: 0.0055809104815125465\n",
      "Epoch 1535, Loss: 0.03918163268826902, Final Batch Loss: 0.014498737640678883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1536, Loss: 0.02843566716182977, Final Batch Loss: 0.002319890772923827\n",
      "Epoch 1537, Loss: 0.04224839527159929, Final Batch Loss: 0.009106386452913284\n",
      "Epoch 1538, Loss: 0.04721641552168876, Final Batch Loss: 0.0005444359267130494\n",
      "Epoch 1539, Loss: 0.11000777897424996, Final Batch Loss: 0.0583302341401577\n",
      "Epoch 1540, Loss: 0.026361406256910414, Final Batch Loss: 0.0004675071104429662\n",
      "Epoch 1541, Loss: 0.03287118370644748, Final Batch Loss: 0.01399453729391098\n",
      "Epoch 1542, Loss: 0.02370172133669257, Final Batch Loss: 0.005273839458823204\n",
      "Epoch 1543, Loss: 0.031916758976876736, Final Batch Loss: 0.0038435691967606544\n",
      "Epoch 1544, Loss: 0.019980796263553202, Final Batch Loss: 0.001182613312266767\n",
      "Epoch 1545, Loss: 0.02895925077609718, Final Batch Loss: 0.013240798376500607\n",
      "Epoch 1546, Loss: 0.01919700432335958, Final Batch Loss: 0.0035743648186326027\n",
      "Epoch 1547, Loss: 0.05537565564736724, Final Batch Loss: 0.031234651803970337\n",
      "Epoch 1548, Loss: 0.02923061593901366, Final Batch Loss: 0.0036320227663964033\n",
      "Epoch 1549, Loss: 0.044673135969787836, Final Batch Loss: 0.0021650411654263735\n",
      "Epoch 1550, Loss: 0.049374849535524845, Final Batch Loss: 0.02378605306148529\n",
      "Epoch 1551, Loss: 0.05132464074995369, Final Batch Loss: 0.008335158228874207\n",
      "Epoch 1552, Loss: 0.02567636501044035, Final Batch Loss: 0.004795237444341183\n",
      "Epoch 1553, Loss: 0.028681785217486322, Final Batch Loss: 0.007969558238983154\n",
      "Epoch 1554, Loss: 0.08298492955509573, Final Batch Loss: 0.0075663491152226925\n",
      "Epoch 1555, Loss: 0.028130600228905678, Final Batch Loss: 0.004487183876335621\n",
      "Epoch 1556, Loss: 0.022574023576453328, Final Batch Loss: 0.005344130098819733\n",
      "Epoch 1557, Loss: 0.019722193013876677, Final Batch Loss: 0.004460054449737072\n",
      "Epoch 1558, Loss: 0.05811329709831625, Final Batch Loss: 0.004748016130179167\n",
      "Epoch 1559, Loss: 0.07219442585483193, Final Batch Loss: 0.025702929124236107\n",
      "Epoch 1560, Loss: 0.02394619071856141, Final Batch Loss: 0.001021430129185319\n",
      "Epoch 1561, Loss: 0.03776944207493216, Final Batch Loss: 0.01200244203209877\n",
      "Epoch 1562, Loss: 0.06434377527330071, Final Batch Loss: 0.0009632403962314129\n",
      "Epoch 1563, Loss: 0.050498680444434285, Final Batch Loss: 0.013310017064213753\n",
      "Epoch 1564, Loss: 0.09794423182029277, Final Batch Loss: 0.013288572430610657\n",
      "Epoch 1565, Loss: 0.040399570745648816, Final Batch Loss: 0.014318815432488918\n",
      "Epoch 1566, Loss: 0.03611460071988404, Final Batch Loss: 0.013270935975015163\n",
      "Epoch 1567, Loss: 0.03428439376875758, Final Batch Loss: 0.0032968735322356224\n",
      "Epoch 1568, Loss: 0.029196024406701326, Final Batch Loss: 0.003002966521307826\n",
      "Epoch 1569, Loss: 0.017966901417821646, Final Batch Loss: 0.0049352701753377914\n",
      "Epoch 1570, Loss: 0.029033182770945132, Final Batch Loss: 0.007540146354585886\n",
      "Epoch 1571, Loss: 0.072277627652511, Final Batch Loss: 0.036921653896570206\n",
      "Epoch 1572, Loss: 0.025585597730241716, Final Batch Loss: 0.0020855343900620937\n",
      "Epoch 1573, Loss: 0.029916997998952866, Final Batch Loss: 0.012720770202577114\n",
      "Epoch 1574, Loss: 0.057632339652627707, Final Batch Loss: 0.020785877481102943\n",
      "Epoch 1575, Loss: 0.03940499224700034, Final Batch Loss: 0.007666987832635641\n",
      "Epoch 1576, Loss: 0.026465272763743997, Final Batch Loss: 0.0009924410842359066\n",
      "Epoch 1577, Loss: 0.08998735481873155, Final Batch Loss: 0.0031346967443823814\n",
      "Epoch 1578, Loss: 0.04143806197680533, Final Batch Loss: 0.0038721023593097925\n",
      "Epoch 1579, Loss: 0.03963077499065548, Final Batch Loss: 0.002637658966705203\n",
      "Epoch 1580, Loss: 0.06194065371528268, Final Batch Loss: 0.007337108254432678\n",
      "Epoch 1581, Loss: 0.04572066431865096, Final Batch Loss: 0.0030203021597117186\n",
      "Epoch 1582, Loss: 0.016255840309895575, Final Batch Loss: 0.0023474509362131357\n",
      "Epoch 1583, Loss: 0.027482294710353017, Final Batch Loss: 0.0034075018484145403\n",
      "Epoch 1584, Loss: 0.04405299562495202, Final Batch Loss: 0.020945707336068153\n",
      "Epoch 1585, Loss: 0.07108972733840346, Final Batch Loss: 0.004625906702131033\n",
      "Epoch 1586, Loss: 0.05750729632563889, Final Batch Loss: 0.01658967137336731\n",
      "Epoch 1587, Loss: 0.06665423000231385, Final Batch Loss: 0.016023922711610794\n",
      "Epoch 1588, Loss: 0.01853964984184131, Final Batch Loss: 0.0009356168447993696\n",
      "Epoch 1589, Loss: 0.030578967882320285, Final Batch Loss: 0.003004153724759817\n",
      "Epoch 1590, Loss: 0.040618219471070915, Final Batch Loss: 0.0005185348563827574\n",
      "Epoch 1591, Loss: 0.03443133970722556, Final Batch Loss: 0.01015985757112503\n",
      "Epoch 1592, Loss: 0.05439600156387314, Final Batch Loss: 0.001499871606938541\n",
      "Epoch 1593, Loss: 0.03703123680315912, Final Batch Loss: 0.003798677120357752\n",
      "Epoch 1594, Loss: 0.027861167211085558, Final Batch Loss: 0.0025966025423258543\n",
      "Epoch 1595, Loss: 0.01694853708613664, Final Batch Loss: 0.004329257179051638\n",
      "Epoch 1596, Loss: 0.029379580984823406, Final Batch Loss: 0.008349577896296978\n",
      "Epoch 1597, Loss: 0.012795904418453574, Final Batch Loss: 0.002306329319253564\n",
      "Epoch 1598, Loss: 0.026431686943396926, Final Batch Loss: 0.017191356047987938\n",
      "Epoch 1599, Loss: 0.030701448442414403, Final Batch Loss: 0.006097858771681786\n",
      "Epoch 1600, Loss: 0.028878159588202834, Final Batch Loss: 0.005555737763643265\n",
      "Epoch 1601, Loss: 0.035439135041087866, Final Batch Loss: 0.009479210712015629\n",
      "Epoch 1602, Loss: 0.0438863675808534, Final Batch Loss: 0.004844178911298513\n",
      "Epoch 1603, Loss: 0.057757383794523776, Final Batch Loss: 0.0016015706351026893\n",
      "Epoch 1604, Loss: 0.01641016930807382, Final Batch Loss: 0.0014049120945855975\n",
      "Epoch 1605, Loss: 0.039288015803322196, Final Batch Loss: 0.0037464131601154804\n",
      "Epoch 1606, Loss: 0.038036675890907645, Final Batch Loss: 0.0015653104055672884\n",
      "Epoch 1607, Loss: 0.028269817354157567, Final Batch Loss: 0.0037132317665964365\n",
      "Epoch 1608, Loss: 0.034405473386868834, Final Batch Loss: 0.011863532476127148\n",
      "Epoch 1609, Loss: 0.04375034314580262, Final Batch Loss: 0.0018624335061758757\n",
      "Epoch 1610, Loss: 0.02729667699895799, Final Batch Loss: 0.0022645595017820597\n",
      "Epoch 1611, Loss: 0.049182777758687735, Final Batch Loss: 0.005560755264014006\n",
      "Epoch 1612, Loss: 0.06897913629654795, Final Batch Loss: 0.0017167696496471763\n",
      "Epoch 1613, Loss: 0.053770678117871284, Final Batch Loss: 0.020037157461047173\n",
      "Epoch 1614, Loss: 0.039045555400662124, Final Batch Loss: 0.005511194001883268\n",
      "Epoch 1615, Loss: 0.044440898578613997, Final Batch Loss: 0.0019107905682176352\n",
      "Epoch 1616, Loss: 0.048746928689070046, Final Batch Loss: 0.0014235986163839698\n",
      "Epoch 1617, Loss: 0.03212882374646142, Final Batch Loss: 0.003944440744817257\n",
      "Epoch 1618, Loss: 0.02351034729508683, Final Batch Loss: 0.0037538581527769566\n",
      "Epoch 1619, Loss: 0.02840870968066156, Final Batch Loss: 0.005882317200303078\n",
      "Epoch 1620, Loss: 0.038820076268166304, Final Batch Loss: 0.009423848241567612\n",
      "Epoch 1621, Loss: 0.013916071853600442, Final Batch Loss: 0.001020521973259747\n",
      "Epoch 1622, Loss: 0.025823108153417706, Final Batch Loss: 0.0014683515764772892\n",
      "Epoch 1623, Loss: 0.024355807749088854, Final Batch Loss: 0.0029007140547037125\n",
      "Epoch 1624, Loss: 0.012910810648463666, Final Batch Loss: 0.002094665076583624\n",
      "Epoch 1625, Loss: 0.03914463263936341, Final Batch Loss: 0.02343173325061798\n",
      "Epoch 1626, Loss: 0.04769328178372234, Final Batch Loss: 0.004339574836194515\n",
      "Epoch 1627, Loss: 0.02041493309661746, Final Batch Loss: 0.006543547846376896\n",
      "Epoch 1628, Loss: 0.016067322983872145, Final Batch Loss: 0.0006622897344641387\n",
      "Epoch 1629, Loss: 0.030934666749089956, Final Batch Loss: 0.0040655252523720264\n",
      "Epoch 1630, Loss: 0.055595834855921566, Final Batch Loss: 0.03159957751631737\n",
      "Epoch 1631, Loss: 0.02442103554494679, Final Batch Loss: 0.0015112378168851137\n",
      "Epoch 1632, Loss: 0.05384938628412783, Final Batch Loss: 0.02597135864198208\n",
      "Epoch 1633, Loss: 0.029946100432425737, Final Batch Loss: 0.001947963610291481\n",
      "Epoch 1634, Loss: 0.02280882652848959, Final Batch Loss: 0.004574808292090893\n",
      "Epoch 1635, Loss: 0.08416391490027308, Final Batch Loss: 0.00749491760507226\n",
      "Epoch 1636, Loss: 0.018774430849589407, Final Batch Loss: 0.0023064662236720324\n",
      "Epoch 1637, Loss: 0.038115080213174224, Final Batch Loss: 0.005296945571899414\n",
      "Epoch 1638, Loss: 0.020390019053593278, Final Batch Loss: 0.0017708642408251762\n",
      "Epoch 1639, Loss: 0.017108833882957697, Final Batch Loss: 0.003941437229514122\n",
      "Epoch 1640, Loss: 0.04330202180426568, Final Batch Loss: 0.02001221850514412\n",
      "Epoch 1641, Loss: 0.03511219873325899, Final Batch Loss: 0.0013859522296115756\n",
      "Epoch 1642, Loss: 0.02965883380966261, Final Batch Loss: 0.010414084419608116\n",
      "Epoch 1643, Loss: 0.009563142724800855, Final Batch Loss: 0.004425313789397478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1644, Loss: 0.011770766926929355, Final Batch Loss: 0.0008377813501283526\n",
      "Epoch 1645, Loss: 0.028267055400647223, Final Batch Loss: 0.0016107730334624648\n",
      "Epoch 1646, Loss: 0.025402150116860867, Final Batch Loss: 0.002176479436457157\n",
      "Epoch 1647, Loss: 0.017948493652511388, Final Batch Loss: 0.011672028340399265\n",
      "Epoch 1648, Loss: 0.026655284455046058, Final Batch Loss: 0.01819966733455658\n",
      "Epoch 1649, Loss: 0.028960642288438976, Final Batch Loss: 0.012144479900598526\n",
      "Epoch 1650, Loss: 0.023587152594700456, Final Batch Loss: 0.007996727712452412\n",
      "Epoch 1651, Loss: 0.029953206016216427, Final Batch Loss: 0.0010024391813203692\n",
      "Epoch 1652, Loss: 0.014558253402356058, Final Batch Loss: 0.000791564176324755\n",
      "Epoch 1653, Loss: 0.011191249126568437, Final Batch Loss: 0.004818348679691553\n",
      "Epoch 1654, Loss: 0.012488082371419296, Final Batch Loss: 0.00046316892257891595\n",
      "Epoch 1655, Loss: 0.04177687398623675, Final Batch Loss: 0.0008008952718228102\n",
      "Epoch 1656, Loss: 0.01938569825142622, Final Batch Loss: 0.004881731234490871\n",
      "Epoch 1657, Loss: 0.025065924040973186, Final Batch Loss: 0.0011005114065483212\n",
      "Epoch 1658, Loss: 0.031940790358930826, Final Batch Loss: 0.004862819332629442\n",
      "Epoch 1659, Loss: 0.019562377245165408, Final Batch Loss: 0.0012678634375333786\n",
      "Epoch 1660, Loss: 0.01968740881420672, Final Batch Loss: 0.003065448720008135\n",
      "Epoch 1661, Loss: 0.013354652212001383, Final Batch Loss: 0.001110656769014895\n",
      "Epoch 1662, Loss: 0.010003977513406426, Final Batch Loss: 0.0014997433172538877\n",
      "Epoch 1663, Loss: 0.045152377686463296, Final Batch Loss: 0.0007648757309652865\n",
      "Epoch 1664, Loss: 0.02443452354054898, Final Batch Loss: 0.0029391369316726923\n",
      "Epoch 1665, Loss: 0.012775290408171713, Final Batch Loss: 0.0006248306017369032\n",
      "Epoch 1666, Loss: 0.013166518416255713, Final Batch Loss: 0.0015435701934620738\n",
      "Epoch 1667, Loss: 0.010246168356388807, Final Batch Loss: 0.0018091056263074279\n",
      "Epoch 1668, Loss: 0.012546391662908718, Final Batch Loss: 0.001906971214339137\n",
      "Epoch 1669, Loss: 0.008698559482581913, Final Batch Loss: 0.0012493564281612635\n",
      "Epoch 1670, Loss: 0.031181993574136868, Final Batch Loss: 0.00015533101395703852\n",
      "Epoch 1671, Loss: 0.00839867175091058, Final Batch Loss: 0.0012821033596992493\n",
      "Epoch 1672, Loss: 0.03767935803625733, Final Batch Loss: 0.0014725257642567158\n",
      "Epoch 1673, Loss: 0.030887017375789583, Final Batch Loss: 0.0032140729017555714\n",
      "Epoch 1674, Loss: 0.02309195138514042, Final Batch Loss: 0.006065326277166605\n",
      "Epoch 1675, Loss: 0.017665949242655188, Final Batch Loss: 0.0034463393967598677\n",
      "Epoch 1676, Loss: 0.06141742959152907, Final Batch Loss: 0.0012386664748191833\n",
      "Epoch 1677, Loss: 0.05922026207554154, Final Batch Loss: 0.03765243664383888\n",
      "Epoch 1678, Loss: 0.034333863703068346, Final Batch Loss: 0.0005875409697182477\n",
      "Epoch 1679, Loss: 0.07403029594570398, Final Batch Loss: 0.05387508496642113\n",
      "Epoch 1680, Loss: 0.07260859536472708, Final Batch Loss: 0.012991665862500668\n",
      "Epoch 1681, Loss: 0.023256281623616815, Final Batch Loss: 0.002316575963050127\n",
      "Epoch 1682, Loss: 0.04388226894661784, Final Batch Loss: 0.026563387364149094\n",
      "Epoch 1683, Loss: 0.049749950179830194, Final Batch Loss: 0.006217634305357933\n",
      "Epoch 1684, Loss: 0.04652977967634797, Final Batch Loss: 0.0032780200708657503\n",
      "Epoch 1685, Loss: 0.07471970724873245, Final Batch Loss: 0.05198711156845093\n",
      "Epoch 1686, Loss: 0.04100870271213353, Final Batch Loss: 0.002080129226669669\n",
      "Epoch 1687, Loss: 0.021991212153807282, Final Batch Loss: 0.0030978398863226175\n",
      "Epoch 1688, Loss: 0.03910564398393035, Final Batch Loss: 0.0024971819948405027\n",
      "Epoch 1689, Loss: 0.04189551324816421, Final Batch Loss: 0.009330704808235168\n",
      "Epoch 1690, Loss: 0.04726363095687702, Final Batch Loss: 0.03818400204181671\n",
      "Epoch 1691, Loss: 0.027935749385505915, Final Batch Loss: 0.005771673284471035\n",
      "Epoch 1692, Loss: 0.0415400400524959, Final Batch Loss: 0.0021154277492314577\n",
      "Epoch 1693, Loss: 0.027033040183596313, Final Batch Loss: 0.010311603546142578\n",
      "Epoch 1694, Loss: 0.027141839294927195, Final Batch Loss: 0.0003876989067066461\n",
      "Epoch 1695, Loss: 0.0238909394829534, Final Batch Loss: 0.0008154860115610063\n",
      "Epoch 1696, Loss: 0.022848035325296223, Final Batch Loss: 0.004721369128674269\n",
      "Epoch 1697, Loss: 0.020077629829756916, Final Batch Loss: 0.0022886411752551794\n",
      "Epoch 1698, Loss: 0.02955505106365308, Final Batch Loss: 0.0008035217761062086\n",
      "Epoch 1699, Loss: 0.014357660547830164, Final Batch Loss: 0.007945080287754536\n",
      "Epoch 1700, Loss: 0.03952653257874772, Final Batch Loss: 0.012043513357639313\n",
      "Epoch 1701, Loss: 0.02451060462044552, Final Batch Loss: 0.006654755678027868\n",
      "Epoch 1702, Loss: 0.03147394064581022, Final Batch Loss: 0.012620327062904835\n",
      "Epoch 1703, Loss: 0.02836333157029003, Final Batch Loss: 0.0011116015957668424\n",
      "Epoch 1704, Loss: 0.026312106812838465, Final Batch Loss: 0.0012077194405719638\n",
      "Epoch 1705, Loss: 0.02886606880929321, Final Batch Loss: 0.004181246738880873\n",
      "Epoch 1706, Loss: 0.018591077416203916, Final Batch Loss: 0.001468533300794661\n",
      "Epoch 1707, Loss: 0.06814527534879744, Final Batch Loss: 0.01686246693134308\n",
      "Epoch 1708, Loss: 0.04075659601949155, Final Batch Loss: 0.02061687596142292\n",
      "Epoch 1709, Loss: 0.025378956808708608, Final Batch Loss: 0.01039863657206297\n",
      "Epoch 1710, Loss: 0.023931570118293166, Final Batch Loss: 0.0012301851529628038\n",
      "Epoch 1711, Loss: 0.017270798329263926, Final Batch Loss: 0.0004742452874779701\n",
      "Epoch 1712, Loss: 0.013613028102554381, Final Batch Loss: 0.0012302653631195426\n",
      "Epoch 1713, Loss: 0.017293601238634437, Final Batch Loss: 0.0010709465714171529\n",
      "Epoch 1714, Loss: 0.04405225766822696, Final Batch Loss: 0.02592521905899048\n",
      "Epoch 1715, Loss: 0.029825766338035464, Final Batch Loss: 0.016861457377672195\n",
      "Epoch 1716, Loss: 0.007832869072444737, Final Batch Loss: 0.0008496335940435529\n",
      "Epoch 1717, Loss: 0.014755251351743937, Final Batch Loss: 0.003052028128877282\n",
      "Epoch 1718, Loss: 0.0173484492697753, Final Batch Loss: 0.00541200814768672\n",
      "Epoch 1719, Loss: 0.030949443171266466, Final Batch Loss: 0.004451792221516371\n",
      "Epoch 1720, Loss: 0.044050553697161376, Final Batch Loss: 0.013060475699603558\n",
      "Epoch 1721, Loss: 0.009417529683560133, Final Batch Loss: 0.0012185957748442888\n",
      "Epoch 1722, Loss: 0.036189180915243924, Final Batch Loss: 0.0021737096831202507\n",
      "Epoch 1723, Loss: 0.03198803751729429, Final Batch Loss: 0.006378299556672573\n",
      "Epoch 1724, Loss: 0.016308040008880198, Final Batch Loss: 0.0007523600943386555\n",
      "Epoch 1725, Loss: 0.015092040121089667, Final Batch Loss: 0.000390096683986485\n",
      "Epoch 1726, Loss: 0.016091638011857867, Final Batch Loss: 0.0030445086304098368\n",
      "Epoch 1727, Loss: 0.011004694097209722, Final Batch Loss: 0.0024376846849918365\n",
      "Epoch 1728, Loss: 0.08771760086528957, Final Batch Loss: 0.03994186222553253\n",
      "Epoch 1729, Loss: 0.03805878886487335, Final Batch Loss: 0.011616772040724754\n",
      "Epoch 1730, Loss: 0.04410882736556232, Final Batch Loss: 0.019676795229315758\n",
      "Epoch 1731, Loss: 0.06703223777003586, Final Batch Loss: 0.03260539472103119\n",
      "Epoch 1732, Loss: 0.07461114460602403, Final Batch Loss: 0.0443846695125103\n",
      "Epoch 1733, Loss: 0.058819849975407124, Final Batch Loss: 0.017846057191491127\n",
      "Epoch 1734, Loss: 0.01894195209024474, Final Batch Loss: 0.0018711368320509791\n",
      "Epoch 1735, Loss: 0.028455781051889062, Final Batch Loss: 0.004424078855663538\n",
      "Epoch 1736, Loss: 0.09081340793636627, Final Batch Loss: 0.00044647257891483605\n",
      "Epoch 1737, Loss: 0.032149877049960196, Final Batch Loss: 0.002613683231174946\n",
      "Epoch 1738, Loss: 0.03597501222975552, Final Batch Loss: 0.025182029232382774\n",
      "Epoch 1739, Loss: 0.05585189815610647, Final Batch Loss: 0.01681850105524063\n",
      "Epoch 1740, Loss: 0.04084014939144254, Final Batch Loss: 0.014940408058464527\n",
      "Epoch 1741, Loss: 0.043481345870532095, Final Batch Loss: 0.028181564062833786\n",
      "Epoch 1742, Loss: 0.024006068240851164, Final Batch Loss: 0.0009927572682499886\n",
      "Epoch 1743, Loss: 0.025391415460035205, Final Batch Loss: 0.007434668019413948\n",
      "Epoch 1744, Loss: 0.08599028293974698, Final Batch Loss: 0.05943714454770088\n",
      "Epoch 1745, Loss: 0.07876854983624071, Final Batch Loss: 0.0664207711815834\n",
      "Epoch 1746, Loss: 0.03674604115076363, Final Batch Loss: 0.0019555685576051474\n",
      "Epoch 1747, Loss: 0.03047150222118944, Final Batch Loss: 0.006312565412372351\n",
      "Epoch 1748, Loss: 0.011476867366582155, Final Batch Loss: 0.000550443830434233\n",
      "Epoch 1749, Loss: 0.01710700266994536, Final Batch Loss: 0.0014188241912052035\n",
      "Epoch 1750, Loss: 0.03495602624025196, Final Batch Loss: 0.0011800021165981889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1751, Loss: 0.020134265767410398, Final Batch Loss: 0.008108248002827168\n",
      "Epoch 1752, Loss: 0.04940059781074524, Final Batch Loss: 0.006424921099096537\n",
      "Epoch 1753, Loss: 0.048954545287415385, Final Batch Loss: 0.022730709984898567\n",
      "Epoch 1754, Loss: 0.036697763949632645, Final Batch Loss: 0.005809946916997433\n",
      "Epoch 1755, Loss: 0.07253637351095676, Final Batch Loss: 0.002678449032828212\n",
      "Epoch 1756, Loss: 0.05087315593846142, Final Batch Loss: 0.017437269911170006\n",
      "Epoch 1757, Loss: 0.14111716346815228, Final Batch Loss: 0.03447508439421654\n",
      "Epoch 1758, Loss: 0.04483125684782863, Final Batch Loss: 0.006188178434967995\n",
      "Epoch 1759, Loss: 0.05062489688862115, Final Batch Loss: 0.02147257700562477\n",
      "Epoch 1760, Loss: 0.04828652588184923, Final Batch Loss: 0.026887405663728714\n",
      "Epoch 1761, Loss: 0.060637484304606915, Final Batch Loss: 0.003554078284651041\n",
      "Epoch 1762, Loss: 0.11324758641421795, Final Batch Loss: 0.00669627683237195\n",
      "Epoch 1763, Loss: 0.07122170715592802, Final Batch Loss: 0.053090520203113556\n",
      "Epoch 1764, Loss: 0.039334191707894206, Final Batch Loss: 0.01315255742520094\n",
      "Epoch 1765, Loss: 0.055627720663324, Final Batch Loss: 0.01091870479285717\n",
      "Epoch 1766, Loss: 0.040293181547895074, Final Batch Loss: 0.018163256347179413\n",
      "Epoch 1767, Loss: 0.020674309344030917, Final Batch Loss: 0.001837496762163937\n",
      "Epoch 1768, Loss: 0.047321032267063856, Final Batch Loss: 0.005436807405203581\n",
      "Epoch 1769, Loss: 0.03743317467160523, Final Batch Loss: 0.007790370844304562\n",
      "Epoch 1770, Loss: 0.06229020515456796, Final Batch Loss: 0.010316915810108185\n",
      "Epoch 1771, Loss: 0.04623424680903554, Final Batch Loss: 0.0022821302991360426\n",
      "Epoch 1772, Loss: 0.019595213292632252, Final Batch Loss: 0.005145882721990347\n",
      "Epoch 1773, Loss: 0.038455573725514114, Final Batch Loss: 0.0022049969993531704\n",
      "Epoch 1774, Loss: 0.0156853087246418, Final Batch Loss: 0.0025819328147917986\n",
      "Epoch 1775, Loss: 0.011288627109024674, Final Batch Loss: 0.0013951041037216783\n",
      "Epoch 1776, Loss: 0.016435354482382536, Final Batch Loss: 0.008180364966392517\n",
      "Epoch 1777, Loss: 0.016969483462162316, Final Batch Loss: 0.002966428641229868\n",
      "Epoch 1778, Loss: 0.026362394564785063, Final Batch Loss: 0.00151769258081913\n",
      "Epoch 1779, Loss: 0.040665047243237495, Final Batch Loss: 0.0008388514397665858\n",
      "Epoch 1780, Loss: 0.03843607730232179, Final Batch Loss: 0.007900997065007687\n",
      "Epoch 1781, Loss: 0.01738722249865532, Final Batch Loss: 0.0006833605002611876\n",
      "Epoch 1782, Loss: 0.022484195418655872, Final Batch Loss: 0.0011093632783740759\n",
      "Epoch 1783, Loss: 0.02499199571320787, Final Batch Loss: 0.0033139882143586874\n",
      "Epoch 1784, Loss: 0.011867338558658957, Final Batch Loss: 0.000866612303070724\n",
      "Epoch 1785, Loss: 0.020307371392846107, Final Batch Loss: 0.0024313312023878098\n",
      "Epoch 1786, Loss: 0.04931331390980631, Final Batch Loss: 0.017292244359850883\n",
      "Epoch 1787, Loss: 0.03344959550304338, Final Batch Loss: 0.00660216249525547\n",
      "Epoch 1788, Loss: 0.008854493906255811, Final Batch Loss: 0.004382222890853882\n",
      "Epoch 1789, Loss: 0.04355080344248563, Final Batch Loss: 0.03821363300085068\n",
      "Epoch 1790, Loss: 0.04400358675047755, Final Batch Loss: 0.004286542069166899\n",
      "Epoch 1791, Loss: 0.02924381592310965, Final Batch Loss: 0.007069433107972145\n",
      "Epoch 1792, Loss: 0.03713250870350748, Final Batch Loss: 0.008244982920587063\n",
      "Epoch 1793, Loss: 0.06332525238394737, Final Batch Loss: 0.014436799101531506\n",
      "Epoch 1794, Loss: 0.03265049651963636, Final Batch Loss: 0.001869146479293704\n",
      "Epoch 1795, Loss: 0.0469774414668791, Final Batch Loss: 0.01017517689615488\n",
      "Epoch 1796, Loss: 0.026162011665292084, Final Batch Loss: 0.013201046735048294\n",
      "Epoch 1797, Loss: 0.035877374990377575, Final Batch Loss: 0.010160334408283234\n",
      "Epoch 1798, Loss: 0.03153132100123912, Final Batch Loss: 0.0010546798584982753\n",
      "Epoch 1799, Loss: 0.019105388782918453, Final Batch Loss: 0.0011321606580168009\n",
      "Epoch 1800, Loss: 0.03612191624415573, Final Batch Loss: 0.0037877345457673073\n",
      "Epoch 1801, Loss: 0.01879904611269012, Final Batch Loss: 0.0009453786187805235\n",
      "Epoch 1802, Loss: 0.03156992490403354, Final Batch Loss: 0.008433557115495205\n",
      "Epoch 1803, Loss: 0.025829452788457274, Final Batch Loss: 0.010090514086186886\n",
      "Epoch 1804, Loss: 0.011584580584894866, Final Batch Loss: 0.0020100590772926807\n",
      "Epoch 1805, Loss: 0.016874122840818018, Final Batch Loss: 0.0015265500405803323\n",
      "Epoch 1806, Loss: 0.05063895141938701, Final Batch Loss: 0.0010472915600985289\n",
      "Epoch 1807, Loss: 0.03168087103404105, Final Batch Loss: 0.0024555071722716093\n",
      "Epoch 1808, Loss: 0.019359192345291376, Final Batch Loss: 0.0058827330358326435\n",
      "Epoch 1809, Loss: 0.017665423161815852, Final Batch Loss: 0.0016179888043552637\n",
      "Epoch 1810, Loss: 0.0439842899213545, Final Batch Loss: 0.0019716122187674046\n",
      "Epoch 1811, Loss: 0.013267392409034073, Final Batch Loss: 0.0032526489812880754\n",
      "Epoch 1812, Loss: 0.034387128660455346, Final Batch Loss: 0.00842643529176712\n",
      "Epoch 1813, Loss: 0.02459778217598796, Final Batch Loss: 0.002631498733535409\n",
      "Epoch 1814, Loss: 0.040983329352457076, Final Batch Loss: 0.01739075966179371\n",
      "Epoch 1815, Loss: 0.009695417873444967, Final Batch Loss: 0.005361198913305998\n",
      "Epoch 1816, Loss: 0.03175388020463288, Final Batch Loss: 0.013410001061856747\n",
      "Epoch 1817, Loss: 0.04038089781533927, Final Batch Loss: 0.009332098998129368\n",
      "Epoch 1818, Loss: 0.03330491064116359, Final Batch Loss: 0.0018944095354527235\n",
      "Epoch 1819, Loss: 0.02084408764494583, Final Batch Loss: 0.004537359811365604\n",
      "Epoch 1820, Loss: 0.02565730648348108, Final Batch Loss: 0.014228937216103077\n",
      "Epoch 1821, Loss: 0.024565547588281333, Final Batch Loss: 0.004736738279461861\n",
      "Epoch 1822, Loss: 0.03976296412292868, Final Batch Loss: 0.0017815141472965479\n",
      "Epoch 1823, Loss: 0.016270450316369534, Final Batch Loss: 0.0006083741318434477\n",
      "Epoch 1824, Loss: 0.02826755022397265, Final Batch Loss: 0.005131009500473738\n",
      "Epoch 1825, Loss: 0.04671872965991497, Final Batch Loss: 0.026085710152983665\n",
      "Epoch 1826, Loss: 0.023221271578222513, Final Batch Loss: 0.01212890911847353\n",
      "Epoch 1827, Loss: 0.016138727369252592, Final Batch Loss: 0.001092325896024704\n",
      "Epoch 1828, Loss: 0.020947511424310505, Final Batch Loss: 0.0027942112646996975\n",
      "Epoch 1829, Loss: 0.008990489644929767, Final Batch Loss: 0.0014739326434209943\n",
      "Epoch 1830, Loss: 0.027490084408782423, Final Batch Loss: 0.004134641960263252\n",
      "Epoch 1831, Loss: 0.01634620994445868, Final Batch Loss: 0.0008164683822542429\n",
      "Epoch 1832, Loss: 0.013496115891030058, Final Batch Loss: 0.001026810030452907\n",
      "Epoch 1833, Loss: 0.01187451439909637, Final Batch Loss: 0.0020027807913720608\n",
      "Epoch 1834, Loss: 0.0229823897825554, Final Batch Loss: 0.0003711667377501726\n",
      "Epoch 1835, Loss: 0.009360734577057883, Final Batch Loss: 0.00042475012014620006\n",
      "Epoch 1836, Loss: 0.03606841282453388, Final Batch Loss: 0.0012733408948406577\n",
      "Epoch 1837, Loss: 0.043961075076367706, Final Batch Loss: 0.026398541405797005\n",
      "Epoch 1838, Loss: 0.014266614394728094, Final Batch Loss: 0.0022518662735819817\n",
      "Epoch 1839, Loss: 0.05712428252445534, Final Batch Loss: 0.0013930578716099262\n",
      "Epoch 1840, Loss: 0.03200214373646304, Final Batch Loss: 0.00135875737760216\n",
      "Epoch 1841, Loss: 0.013450835773255676, Final Batch Loss: 0.0007594079943373799\n",
      "Epoch 1842, Loss: 0.053036266821436584, Final Batch Loss: 0.03522517532110214\n",
      "Epoch 1843, Loss: 0.027790092979557812, Final Batch Loss: 0.011078192852437496\n",
      "Epoch 1844, Loss: 0.017978547257371247, Final Batch Loss: 0.001769032096490264\n",
      "Epoch 1845, Loss: 0.008783518453128636, Final Batch Loss: 0.004151970148086548\n",
      "Epoch 1846, Loss: 0.019192257546819746, Final Batch Loss: 0.0006149249384179711\n",
      "Epoch 1847, Loss: 0.025359724648296833, Final Batch Loss: 0.01943606697022915\n",
      "Epoch 1848, Loss: 0.010497383365873247, Final Batch Loss: 0.001525053521618247\n",
      "Epoch 1849, Loss: 0.011574768112041056, Final Batch Loss: 0.002614326775074005\n",
      "Epoch 1850, Loss: 0.02545708540128544, Final Batch Loss: 0.00046659779036417603\n",
      "Epoch 1851, Loss: 0.032018442056141794, Final Batch Loss: 0.0005110350321047008\n",
      "Epoch 1852, Loss: 0.012963532062713057, Final Batch Loss: 0.0008827933925203979\n",
      "Epoch 1853, Loss: 0.03479777026223019, Final Batch Loss: 0.0007473953883163631\n",
      "Epoch 1854, Loss: 0.01736118271946907, Final Batch Loss: 0.005501460749655962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1855, Loss: 0.01366811478510499, Final Batch Loss: 0.0008354302262887359\n",
      "Epoch 1856, Loss: 0.04268141300417483, Final Batch Loss: 0.03647012263536453\n",
      "Epoch 1857, Loss: 0.014466232620179653, Final Batch Loss: 0.0007434553117491305\n",
      "Epoch 1858, Loss: 0.02769697061739862, Final Batch Loss: 0.02065902389585972\n",
      "Epoch 1859, Loss: 0.009879585588350892, Final Batch Loss: 0.003055099630728364\n",
      "Epoch 1860, Loss: 0.020255733106750995, Final Batch Loss: 0.0021354579366743565\n",
      "Epoch 1861, Loss: 0.008739200711715966, Final Batch Loss: 0.0005836933851242065\n",
      "Epoch 1862, Loss: 0.017330889299046248, Final Batch Loss: 0.0010315999388694763\n",
      "Epoch 1863, Loss: 0.03015231207245961, Final Batch Loss: 0.0023996629752218723\n",
      "Epoch 1864, Loss: 0.04038073681294918, Final Batch Loss: 0.028218500316143036\n",
      "Epoch 1865, Loss: 0.018833509122487158, Final Batch Loss: 0.0025507162790745497\n",
      "Epoch 1866, Loss: 0.0219797802856192, Final Batch Loss: 0.005687111988663673\n",
      "Epoch 1867, Loss: 0.017452872736612335, Final Batch Loss: 0.0017882110550999641\n",
      "Epoch 1868, Loss: 0.009750821307534352, Final Batch Loss: 0.001463199034333229\n",
      "Epoch 1869, Loss: 0.01899924105964601, Final Batch Loss: 0.0005647979560308158\n",
      "Epoch 1870, Loss: 0.03566739265806973, Final Batch Loss: 0.001913566724397242\n",
      "Epoch 1871, Loss: 0.036640657344833016, Final Batch Loss: 0.008188904263079166\n",
      "Epoch 1872, Loss: 0.01011381420539692, Final Batch Loss: 0.0019102953374385834\n",
      "Epoch 1873, Loss: 0.044554779364261776, Final Batch Loss: 0.03424719721078873\n",
      "Epoch 1874, Loss: 0.0330507064354606, Final Batch Loss: 0.0032199856359511614\n",
      "Epoch 1875, Loss: 0.013384449761360884, Final Batch Loss: 0.0010612786281853914\n",
      "Epoch 1876, Loss: 0.07002522214315832, Final Batch Loss: 0.034239787608385086\n",
      "Epoch 1877, Loss: 0.0753576128045097, Final Batch Loss: 0.004401529673486948\n",
      "Epoch 1878, Loss: 0.03537986415904015, Final Batch Loss: 0.002963956678286195\n",
      "Epoch 1879, Loss: 0.02805848338175565, Final Batch Loss: 0.0011270494433119893\n",
      "Epoch 1880, Loss: 0.019085512962192297, Final Batch Loss: 0.0023700715973973274\n",
      "Epoch 1881, Loss: 0.021768923616036773, Final Batch Loss: 0.002121579833328724\n",
      "Epoch 1882, Loss: 0.050614571082405746, Final Batch Loss: 0.011533339507877827\n",
      "Epoch 1883, Loss: 0.042719557764939964, Final Batch Loss: 0.0016764997271820903\n",
      "Epoch 1884, Loss: 0.03616801893804222, Final Batch Loss: 0.019967077299952507\n",
      "Epoch 1885, Loss: 0.007741737004835159, Final Batch Loss: 0.0009202960063703358\n",
      "Epoch 1886, Loss: 0.03799274022458121, Final Batch Loss: 0.0013332836097106338\n",
      "Epoch 1887, Loss: 0.016600737115368247, Final Batch Loss: 0.0012089350493624806\n",
      "Epoch 1888, Loss: 0.03716819075634703, Final Batch Loss: 0.0012101830216124654\n",
      "Epoch 1889, Loss: 0.0735631404677406, Final Batch Loss: 0.0010358119616284966\n",
      "Epoch 1890, Loss: 0.05320256086997688, Final Batch Loss: 0.036601562052965164\n",
      "Epoch 1891, Loss: 0.027732369082514197, Final Batch Loss: 0.014100555330514908\n",
      "Epoch 1892, Loss: 0.024205687033827417, Final Batch Loss: 0.0006226029945537448\n",
      "Epoch 1893, Loss: 0.025416870310436934, Final Batch Loss: 0.0006677129422314465\n",
      "Epoch 1894, Loss: 0.03504045074805617, Final Batch Loss: 0.0015130214160308242\n",
      "Epoch 1895, Loss: 0.05509943136712536, Final Batch Loss: 0.012126291170716286\n",
      "Epoch 1896, Loss: 0.06473242084030062, Final Batch Loss: 0.0017735679866746068\n",
      "Epoch 1897, Loss: 0.06959113175980747, Final Batch Loss: 0.020406605675816536\n",
      "Epoch 1898, Loss: 0.04570825526025146, Final Batch Loss: 0.002018221188336611\n",
      "Epoch 1899, Loss: 0.023954892996698618, Final Batch Loss: 0.0018799696117639542\n",
      "Epoch 1900, Loss: 0.026916638016700745, Final Batch Loss: 0.0025445115752518177\n",
      "Epoch 1901, Loss: 0.0407076136325486, Final Batch Loss: 0.013807285577058792\n",
      "Epoch 1902, Loss: 0.06424112984677777, Final Batch Loss: 0.004277283791452646\n",
      "Epoch 1903, Loss: 0.034011370793450624, Final Batch Loss: 0.005162455607205629\n",
      "Epoch 1904, Loss: 0.0238535221433267, Final Batch Loss: 0.0029263359028846025\n",
      "Epoch 1905, Loss: 0.02310526033397764, Final Batch Loss: 0.0032248124480247498\n",
      "Epoch 1906, Loss: 0.01613627327606082, Final Batch Loss: 0.002924025058746338\n",
      "Epoch 1907, Loss: 0.01768690289463848, Final Batch Loss: 0.0026516548823565245\n",
      "Epoch 1908, Loss: 0.08576926036039367, Final Batch Loss: 0.0015024968888610601\n",
      "Epoch 1909, Loss: 0.02777305420022458, Final Batch Loss: 0.005186385940760374\n",
      "Epoch 1910, Loss: 0.019915645942091942, Final Batch Loss: 0.0017360225319862366\n",
      "Epoch 1911, Loss: 0.019985568418633193, Final Batch Loss: 0.0007601211545988917\n",
      "Epoch 1912, Loss: 0.018184792483225465, Final Batch Loss: 0.004729869309812784\n",
      "Epoch 1913, Loss: 0.07310769660398364, Final Batch Loss: 0.017233997583389282\n",
      "Epoch 1914, Loss: 0.029355938371736556, Final Batch Loss: 0.006398337893188\n",
      "Epoch 1915, Loss: 0.034123570658266544, Final Batch Loss: 0.004260161425918341\n",
      "Epoch 1916, Loss: 0.043330563115887344, Final Batch Loss: 0.003652944229543209\n",
      "Epoch 1917, Loss: 0.020174131670501083, Final Batch Loss: 0.0006247541750781238\n",
      "Epoch 1918, Loss: 0.01151622156612575, Final Batch Loss: 0.001589620136655867\n",
      "Epoch 1919, Loss: 0.05009767157025635, Final Batch Loss: 0.0030221049673855305\n",
      "Epoch 1920, Loss: 0.021428398380521685, Final Batch Loss: 0.0016066766111180186\n",
      "Epoch 1921, Loss: 0.03775405389023945, Final Batch Loss: 0.0016877782763913274\n",
      "Epoch 1922, Loss: 0.015551348042208701, Final Batch Loss: 0.0035593113861978054\n",
      "Epoch 1923, Loss: 0.02823307807557285, Final Batch Loss: 0.0017892352771013975\n",
      "Epoch 1924, Loss: 0.019157150134560652, Final Batch Loss: 0.002253107260912657\n",
      "Epoch 1925, Loss: 0.03980587958358228, Final Batch Loss: 0.0032890564762055874\n",
      "Epoch 1926, Loss: 0.06076414653216489, Final Batch Loss: 0.009667779318988323\n",
      "Epoch 1927, Loss: 0.03352461929898709, Final Batch Loss: 0.000621078594122082\n",
      "Epoch 1928, Loss: 0.04126645770156756, Final Batch Loss: 0.03277834504842758\n",
      "Epoch 1929, Loss: 0.016835060203447938, Final Batch Loss: 0.000652397284284234\n",
      "Epoch 1930, Loss: 0.024587927386164665, Final Batch Loss: 0.0025906863156706095\n",
      "Epoch 1931, Loss: 0.02245167933870107, Final Batch Loss: 0.006413853727281094\n",
      "Epoch 1932, Loss: 0.02417683758540079, Final Batch Loss: 0.002656601369380951\n",
      "Epoch 1933, Loss: 0.011194731574505568, Final Batch Loss: 0.001166010508313775\n",
      "Epoch 1934, Loss: 0.06146545405499637, Final Batch Loss: 0.007115185726433992\n",
      "Epoch 1935, Loss: 0.020881289499811828, Final Batch Loss: 0.0028216028586030006\n",
      "Epoch 1936, Loss: 0.05443165940232575, Final Batch Loss: 0.003725355491042137\n",
      "Epoch 1937, Loss: 0.014334496634546667, Final Batch Loss: 0.0004187860176898539\n",
      "Epoch 1938, Loss: 0.028178422362543643, Final Batch Loss: 0.012386251240968704\n",
      "Epoch 1939, Loss: 0.08768452506046742, Final Batch Loss: 0.07912196964025497\n",
      "Epoch 1940, Loss: 0.06923167263448704, Final Batch Loss: 0.010240974836051464\n",
      "Epoch 1941, Loss: 0.02604950580280274, Final Batch Loss: 0.003411337034776807\n",
      "Epoch 1942, Loss: 0.05233676475472748, Final Batch Loss: 0.005626612342894077\n",
      "Epoch 1943, Loss: 0.06597047299146652, Final Batch Loss: 0.025282688438892365\n",
      "Epoch 1944, Loss: 0.016326508834026754, Final Batch Loss: 0.002715243259444833\n",
      "Epoch 1945, Loss: 0.016817981144413352, Final Batch Loss: 0.0043374220840632915\n",
      "Epoch 1946, Loss: 0.05276403285097331, Final Batch Loss: 0.008411934599280357\n",
      "Epoch 1947, Loss: 0.03606616461183876, Final Batch Loss: 0.001505696796812117\n",
      "Epoch 1948, Loss: 0.024840892292559147, Final Batch Loss: 0.0026101390831172466\n",
      "Epoch 1949, Loss: 0.04761908132059034, Final Batch Loss: 0.011982353404164314\n",
      "Epoch 1950, Loss: 0.01652119873324409, Final Batch Loss: 0.0016953746089711785\n",
      "Epoch 1951, Loss: 0.03470148739870638, Final Batch Loss: 0.010339710861444473\n",
      "Epoch 1952, Loss: 0.02912301430478692, Final Batch Loss: 0.003241932950913906\n",
      "Epoch 1953, Loss: 0.04918629638268612, Final Batch Loss: 0.002334622433409095\n",
      "Epoch 1954, Loss: 0.035692282079253346, Final Batch Loss: 0.016390368342399597\n",
      "Epoch 1955, Loss: 0.027738937293179333, Final Batch Loss: 0.0020317479502409697\n",
      "Epoch 1956, Loss: 0.01008594740414992, Final Batch Loss: 0.0035264859907329082\n",
      "Epoch 1957, Loss: 0.02823908720165491, Final Batch Loss: 0.015362951904535294\n",
      "Epoch 1958, Loss: 0.03832096478436142, Final Batch Loss: 0.004389884416013956\n",
      "Epoch 1959, Loss: 0.014896556502208114, Final Batch Loss: 0.003842645324766636\n",
      "Epoch 1960, Loss: 0.03812432975973934, Final Batch Loss: 0.02492903172969818\n",
      "Epoch 1961, Loss: 0.035937170847319067, Final Batch Loss: 0.009703414514660835\n",
      "Epoch 1962, Loss: 0.02444975869730115, Final Batch Loss: 0.01422695629298687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1963, Loss: 0.017879253602586687, Final Batch Loss: 0.0017092096386477351\n",
      "Epoch 1964, Loss: 0.055849631782621145, Final Batch Loss: 0.04451505467295647\n",
      "Epoch 1965, Loss: 0.03878498019184917, Final Batch Loss: 0.003531447844579816\n",
      "Epoch 1966, Loss: 0.0643865056335926, Final Batch Loss: 0.022884732112288475\n",
      "Epoch 1967, Loss: 0.02237931825220585, Final Batch Loss: 0.000762342126108706\n",
      "Epoch 1968, Loss: 0.05152784916572273, Final Batch Loss: 0.008025029674172401\n",
      "Epoch 1969, Loss: 0.02587085892446339, Final Batch Loss: 0.0035126409493386745\n",
      "Epoch 1970, Loss: 0.03225836297497153, Final Batch Loss: 0.0009746169671416283\n",
      "Epoch 1971, Loss: 0.028886496787890792, Final Batch Loss: 0.00767141068354249\n",
      "Epoch 1972, Loss: 0.028826391557231545, Final Batch Loss: 0.0013410249957814813\n",
      "Epoch 1973, Loss: 0.017828212934546173, Final Batch Loss: 0.0018911558436229825\n",
      "Epoch 1974, Loss: 0.030177353648468852, Final Batch Loss: 0.0045013874769210815\n",
      "Epoch 1975, Loss: 0.05710439558606595, Final Batch Loss: 0.008186313323676586\n",
      "Epoch 1976, Loss: 0.030627613188698888, Final Batch Loss: 0.0018857312388718128\n",
      "Epoch 1977, Loss: 0.01600470917765051, Final Batch Loss: 0.0014486765721812844\n",
      "Epoch 1978, Loss: 0.007964516174979508, Final Batch Loss: 0.001842060824856162\n",
      "Epoch 1979, Loss: 0.02621736191213131, Final Batch Loss: 0.00910300761461258\n",
      "Epoch 1980, Loss: 0.0665008888754528, Final Batch Loss: 0.006531015504151583\n",
      "Epoch 1981, Loss: 0.0227972132852301, Final Batch Loss: 0.0011272735428065062\n",
      "Epoch 1982, Loss: 0.010507934901397675, Final Batch Loss: 0.0020947405137121677\n",
      "Epoch 1983, Loss: 0.04812868623412214, Final Batch Loss: 0.0010029477998614311\n",
      "Epoch 1984, Loss: 0.01299795025261119, Final Batch Loss: 0.0007768544601276517\n",
      "Epoch 1985, Loss: 0.007048422819934785, Final Batch Loss: 0.0007771719247102737\n",
      "Epoch 1986, Loss: 0.010955097299301997, Final Batch Loss: 0.0003580941411200911\n",
      "Epoch 1987, Loss: 0.04856301250401884, Final Batch Loss: 0.003067971207201481\n",
      "Epoch 1988, Loss: 0.016292201122269034, Final Batch Loss: 0.00769462576135993\n",
      "Epoch 1989, Loss: 0.010731369024142623, Final Batch Loss: 0.002300394931808114\n",
      "Epoch 1990, Loss: 0.007566278742160648, Final Batch Loss: 0.0004973575123585761\n",
      "Epoch 1991, Loss: 0.005486706737428904, Final Batch Loss: 0.0007519533392041922\n",
      "Epoch 1992, Loss: 0.006461748213041574, Final Batch Loss: 0.0020387396216392517\n",
      "Epoch 1993, Loss: 0.01270336113520898, Final Batch Loss: 0.0016940165078267455\n",
      "Epoch 1994, Loss: 0.03025817626621574, Final Batch Loss: 0.0016177035868167877\n",
      "Epoch 1995, Loss: 0.018472984666004777, Final Batch Loss: 0.0012387860333546996\n",
      "Epoch 1996, Loss: 0.012808722676709294, Final Batch Loss: 0.00485303346067667\n",
      "Epoch 1997, Loss: 0.04955769656226039, Final Batch Loss: 0.03424183651804924\n",
      "Epoch 1998, Loss: 0.03996749783982523, Final Batch Loss: 0.02147722616791725\n",
      "Epoch 1999, Loss: 0.03609756822697818, Final Batch Loss: 0.0016047166427597404\n",
      "Epoch 2000, Loss: 0.016021091490983963, Final Batch Loss: 0.002837788313627243\n",
      "Epoch 2001, Loss: 0.011937381757888943, Final Batch Loss: 0.0008038878440856934\n",
      "Epoch 2002, Loss: 0.024767650291323662, Final Batch Loss: 0.0026629623025655746\n",
      "Epoch 2003, Loss: 0.03390185860916972, Final Batch Loss: 0.009308923967182636\n",
      "Epoch 2004, Loss: 0.03522065206198022, Final Batch Loss: 0.0005554559174925089\n",
      "Epoch 2005, Loss: 0.04387858510017395, Final Batch Loss: 0.0048216646537184715\n",
      "Epoch 2006, Loss: 0.033689930860418826, Final Batch Loss: 0.003044258803129196\n",
      "Epoch 2007, Loss: 0.028585828316863626, Final Batch Loss: 0.0014866963028907776\n",
      "Epoch 2008, Loss: 0.012977329373825341, Final Batch Loss: 0.0005536591052077711\n",
      "Epoch 2009, Loss: 0.03354687592945993, Final Batch Loss: 0.027930092066526413\n",
      "Epoch 2010, Loss: 0.0058478243299759924, Final Batch Loss: 0.000631113420240581\n",
      "Epoch 2011, Loss: 0.035755146062001586, Final Batch Loss: 0.013558515347540379\n",
      "Epoch 2012, Loss: 0.04361625132150948, Final Batch Loss: 0.013347484171390533\n",
      "Epoch 2013, Loss: 0.031904769130051136, Final Batch Loss: 0.00045541516738012433\n",
      "Epoch 2014, Loss: 0.055490132683189586, Final Batch Loss: 0.01656997762620449\n",
      "Epoch 2015, Loss: 0.01259867602493614, Final Batch Loss: 0.0041203550063073635\n",
      "Epoch 2016, Loss: 0.05754092463757843, Final Batch Loss: 0.0011925427243113518\n",
      "Epoch 2017, Loss: 0.010045102564617991, Final Batch Loss: 0.002789280144497752\n",
      "Epoch 2018, Loss: 0.02462396811461076, Final Batch Loss: 0.001332316780462861\n",
      "Epoch 2019, Loss: 0.017827215953730047, Final Batch Loss: 0.003452469129115343\n",
      "Epoch 2020, Loss: 0.039903103141114116, Final Batch Loss: 0.001988470321521163\n",
      "Epoch 2021, Loss: 0.022418975713662803, Final Batch Loss: 0.0006501367897726595\n",
      "Epoch 2022, Loss: 0.029899692279286683, Final Batch Loss: 0.002147635444998741\n",
      "Epoch 2023, Loss: 0.02537068526726216, Final Batch Loss: 0.011555550619959831\n",
      "Epoch 2024, Loss: 0.042873190715909004, Final Batch Loss: 0.008597275242209435\n",
      "Epoch 2025, Loss: 0.021415540482848883, Final Batch Loss: 0.004405243322253227\n",
      "Epoch 2026, Loss: 0.025141727761365473, Final Batch Loss: 0.013372845016419888\n",
      "Epoch 2027, Loss: 0.02159552212106064, Final Batch Loss: 0.006430556066334248\n",
      "Epoch 2028, Loss: 0.028823840722907335, Final Batch Loss: 0.01220636535435915\n",
      "Epoch 2029, Loss: 0.017476175213232636, Final Batch Loss: 0.001967767486348748\n",
      "Epoch 2030, Loss: 0.029449580993968993, Final Batch Loss: 0.00048752932343631983\n",
      "Epoch 2031, Loss: 0.017841767170466483, Final Batch Loss: 0.001150296302512288\n",
      "Epoch 2032, Loss: 0.0546748680062592, Final Batch Loss: 0.012368153780698776\n",
      "Epoch 2033, Loss: 0.05330331710865721, Final Batch Loss: 0.0005309832631610334\n",
      "Epoch 2034, Loss: 0.032374365255236626, Final Batch Loss: 0.0008249671082012355\n",
      "Epoch 2035, Loss: 0.024451426637824625, Final Batch Loss: 0.0027454474475234747\n",
      "Epoch 2036, Loss: 0.02010567087563686, Final Batch Loss: 0.00027119307196699083\n",
      "Epoch 2037, Loss: 0.011665177415125072, Final Batch Loss: 0.0007133437320590019\n",
      "Epoch 2038, Loss: 0.027018433320336044, Final Batch Loss: 0.008928447961807251\n",
      "Epoch 2039, Loss: 0.028739044268149883, Final Batch Loss: 0.0008513251668773592\n",
      "Epoch 2040, Loss: 0.016087127500213683, Final Batch Loss: 0.00025701988488435745\n",
      "Epoch 2041, Loss: 0.008423857158049941, Final Batch Loss: 0.002218841342255473\n",
      "Epoch 2042, Loss: 0.007972391322255135, Final Batch Loss: 0.001430989126674831\n",
      "Epoch 2043, Loss: 0.007187482202425599, Final Batch Loss: 0.002380619989708066\n",
      "Epoch 2044, Loss: 0.00657904744730331, Final Batch Loss: 0.001439774758182466\n",
      "Epoch 2045, Loss: 0.0038558633241336793, Final Batch Loss: 0.00041246553882956505\n",
      "Epoch 2046, Loss: 0.04442070284858346, Final Batch Loss: 0.018727028742432594\n",
      "Epoch 2047, Loss: 0.08369001199025661, Final Batch Loss: 0.03599104285240173\n",
      "Epoch 2048, Loss: 0.01994475079118274, Final Batch Loss: 0.005271200556308031\n",
      "Epoch 2049, Loss: 0.027425191539805382, Final Batch Loss: 0.0007852077833376825\n",
      "Epoch 2050, Loss: 0.05855756835080683, Final Batch Loss: 0.0034002296160906553\n",
      "Epoch 2051, Loss: 0.03780416108202189, Final Batch Loss: 0.0061801220290362835\n",
      "Epoch 2052, Loss: 0.024885018880013376, Final Batch Loss: 0.004443636164069176\n",
      "Epoch 2053, Loss: 0.015332181414123625, Final Batch Loss: 0.0021125448402017355\n",
      "Epoch 2054, Loss: 0.021156497852643952, Final Batch Loss: 0.0003146648814436048\n",
      "Epoch 2055, Loss: 0.040445040445774794, Final Batch Loss: 0.011549015529453754\n",
      "Epoch 2056, Loss: 0.02838665503077209, Final Batch Loss: 0.0023168418556451797\n",
      "Epoch 2057, Loss: 0.04148134315619245, Final Batch Loss: 0.01820308342576027\n",
      "Epoch 2058, Loss: 0.041419022250920534, Final Batch Loss: 0.0016665414441376925\n",
      "Epoch 2059, Loss: 0.043612885870970786, Final Batch Loss: 0.0015074600232765079\n",
      "Epoch 2060, Loss: 0.01064578618388623, Final Batch Loss: 0.0006779339746572077\n",
      "Epoch 2061, Loss: 0.037832737201824784, Final Batch Loss: 0.016306083649396896\n",
      "Epoch 2062, Loss: 0.04149154058541171, Final Batch Loss: 0.018579723313450813\n",
      "Epoch 2063, Loss: 0.014899116707965732, Final Batch Loss: 0.001623865682631731\n",
      "Epoch 2064, Loss: 0.030603580409660935, Final Batch Loss: 0.0008610172662883997\n",
      "Epoch 2065, Loss: 0.017360143014229834, Final Batch Loss: 0.001064626150764525\n",
      "Epoch 2066, Loss: 0.02739769601612352, Final Batch Loss: 0.00033669002004899085\n",
      "Epoch 2067, Loss: 0.03970511915395036, Final Batch Loss: 0.000966754334513098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2068, Loss: 0.04111682582879439, Final Batch Loss: 0.020455408841371536\n",
      "Epoch 2069, Loss: 0.022061218740418553, Final Batch Loss: 0.002578173065558076\n",
      "Epoch 2070, Loss: 0.02872870536521077, Final Batch Loss: 0.003687401534989476\n",
      "Epoch 2071, Loss: 0.011885702086146921, Final Batch Loss: 0.002098344499245286\n",
      "Epoch 2072, Loss: 0.022140120272524655, Final Batch Loss: 0.0032410838175565004\n",
      "Epoch 2073, Loss: 0.009112631203606725, Final Batch Loss: 0.002445307094603777\n",
      "Epoch 2074, Loss: 0.0509040734032169, Final Batch Loss: 0.0055604008957743645\n",
      "Epoch 2075, Loss: 0.02113098301924765, Final Batch Loss: 0.0034700154792517424\n",
      "Epoch 2076, Loss: 0.04825678444467485, Final Batch Loss: 0.003031727857887745\n",
      "Epoch 2077, Loss: 0.03952947084326297, Final Batch Loss: 0.0033218504395335913\n",
      "Epoch 2078, Loss: 0.011136049346532673, Final Batch Loss: 0.0033151886891573668\n",
      "Epoch 2079, Loss: 0.01683688978664577, Final Batch Loss: 0.004531691316515207\n",
      "Epoch 2080, Loss: 0.009664633427746594, Final Batch Loss: 0.0012995629804208875\n",
      "Epoch 2081, Loss: 0.024445177987217903, Final Batch Loss: 0.017378512769937515\n",
      "Epoch 2082, Loss: 0.03569545573554933, Final Batch Loss: 0.0038149268366396427\n",
      "Epoch 2083, Loss: 0.021290458738803864, Final Batch Loss: 0.014857868663966656\n",
      "Epoch 2084, Loss: 0.06739735827432014, Final Batch Loss: 0.002503738971427083\n",
      "Epoch 2085, Loss: 0.04988566099200398, Final Batch Loss: 0.001513239461928606\n",
      "Epoch 2086, Loss: 0.021319978637620807, Final Batch Loss: 0.015816882252693176\n",
      "Epoch 2087, Loss: 0.026451983605511487, Final Batch Loss: 0.0011306910309940577\n",
      "Epoch 2088, Loss: 0.0354653102112934, Final Batch Loss: 0.0012355284998193383\n",
      "Epoch 2089, Loss: 0.023602173081599176, Final Batch Loss: 0.005823785904794931\n",
      "Epoch 2090, Loss: 0.04901898675598204, Final Batch Loss: 0.002104277489706874\n",
      "Epoch 2091, Loss: 0.01314782234840095, Final Batch Loss: 0.0013846459332853556\n",
      "Epoch 2092, Loss: 0.037543641636148095, Final Batch Loss: 0.005363220814615488\n",
      "Epoch 2093, Loss: 0.024461429857183248, Final Batch Loss: 0.0009084256016649306\n",
      "Epoch 2094, Loss: 0.04721959796734154, Final Batch Loss: 0.005374295171350241\n",
      "Epoch 2095, Loss: 0.03181059751659632, Final Batch Loss: 0.0009300081292167306\n",
      "Epoch 2096, Loss: 0.02805787732359022, Final Batch Loss: 0.003061142982915044\n",
      "Epoch 2097, Loss: 0.010734095296356827, Final Batch Loss: 0.0006802591378800571\n",
      "Epoch 2098, Loss: 0.012826398597098887, Final Batch Loss: 0.0005882390541955829\n",
      "Epoch 2099, Loss: 0.04296269663609564, Final Batch Loss: 0.022444795817136765\n",
      "Epoch 2100, Loss: 0.038106624968349934, Final Batch Loss: 0.014109277166426182\n",
      "Epoch 2101, Loss: 0.02831330307526514, Final Batch Loss: 0.00036663730861619115\n",
      "Epoch 2102, Loss: 0.029214621288701892, Final Batch Loss: 0.01898706890642643\n",
      "Epoch 2103, Loss: 0.016122862696647644, Final Batch Loss: 0.0005559272831305861\n",
      "Epoch 2104, Loss: 0.03208017296856269, Final Batch Loss: 0.002526934025809169\n",
      "Epoch 2105, Loss: 0.05580554570769891, Final Batch Loss: 0.0036736999172717333\n",
      "Epoch 2106, Loss: 0.03155361150857061, Final Batch Loss: 0.016050975769758224\n",
      "Epoch 2107, Loss: 0.07922782516106963, Final Batch Loss: 0.006873006466776133\n",
      "Epoch 2108, Loss: 0.012369304313324392, Final Batch Loss: 0.0021046085748821497\n",
      "Epoch 2109, Loss: 0.041434029350057244, Final Batch Loss: 0.025672584772109985\n",
      "Epoch 2110, Loss: 0.04605157859623432, Final Batch Loss: 0.01053706556558609\n",
      "Epoch 2111, Loss: 0.02445758762769401, Final Batch Loss: 0.005108100362122059\n",
      "Epoch 2112, Loss: 0.021262341702822596, Final Batch Loss: 0.010002345778048038\n",
      "Epoch 2113, Loss: 0.02122797560878098, Final Batch Loss: 0.003925336059182882\n",
      "Epoch 2114, Loss: 0.05727330828085542, Final Batch Loss: 0.011118746362626553\n",
      "Epoch 2115, Loss: 0.054159455525223166, Final Batch Loss: 0.0009078798466362059\n",
      "Epoch 2116, Loss: 0.05336923763388768, Final Batch Loss: 0.02027583308517933\n",
      "Epoch 2117, Loss: 0.01275239116512239, Final Batch Loss: 0.005522421561181545\n",
      "Epoch 2118, Loss: 0.024900380638428032, Final Batch Loss: 0.005869242828339338\n",
      "Epoch 2119, Loss: 0.04150958964601159, Final Batch Loss: 0.024858206510543823\n",
      "Epoch 2120, Loss: 0.059461335302330554, Final Batch Loss: 0.006536679808050394\n",
      "Epoch 2121, Loss: 0.022860289667733014, Final Batch Loss: 0.0018963768379762769\n",
      "Epoch 2122, Loss: 0.025230114813894033, Final Batch Loss: 0.006540918722748756\n",
      "Epoch 2123, Loss: 0.02112177445087582, Final Batch Loss: 0.002306642709299922\n",
      "Epoch 2124, Loss: 0.009827436821069568, Final Batch Loss: 0.0035030387807637453\n",
      "Epoch 2125, Loss: 0.009965971228666604, Final Batch Loss: 0.006686320994049311\n",
      "Epoch 2126, Loss: 0.02169433530070819, Final Batch Loss: 0.014344196766614914\n",
      "Epoch 2127, Loss: 0.006874566315673292, Final Batch Loss: 0.0006826316239312291\n",
      "Epoch 2128, Loss: 0.016868643317138776, Final Batch Loss: 0.001815440016798675\n",
      "Epoch 2129, Loss: 0.011888303444720805, Final Batch Loss: 0.001381386537104845\n",
      "Epoch 2130, Loss: 0.009158598782960325, Final Batch Loss: 0.0006550425314344466\n",
      "Epoch 2131, Loss: 0.058153388439677656, Final Batch Loss: 0.012595322914421558\n",
      "Epoch 2132, Loss: 0.015454630134627223, Final Batch Loss: 0.00571797089651227\n",
      "Epoch 2133, Loss: 0.05353207333246246, Final Batch Loss: 0.00017440487863495946\n",
      "Epoch 2134, Loss: 0.009888704516924918, Final Batch Loss: 0.0010825227946043015\n",
      "Epoch 2135, Loss: 0.013735610409639776, Final Batch Loss: 0.0063526928424835205\n",
      "Epoch 2136, Loss: 0.024081405019387603, Final Batch Loss: 0.002076131757348776\n",
      "Epoch 2137, Loss: 0.008881109883077443, Final Batch Loss: 0.0017693029949441552\n",
      "Epoch 2138, Loss: 0.007341135176829994, Final Batch Loss: 0.002463759621605277\n",
      "Epoch 2139, Loss: 0.006001327303238213, Final Batch Loss: 0.0018657639157027006\n",
      "Epoch 2140, Loss: 0.10376264018123038, Final Batch Loss: 0.09501665830612183\n",
      "Epoch 2141, Loss: 0.013711500563658774, Final Batch Loss: 0.0030921977013349533\n",
      "Epoch 2142, Loss: 0.01816498499829322, Final Batch Loss: 0.0020173536613583565\n",
      "Epoch 2143, Loss: 0.029185491643147543, Final Batch Loss: 0.0010272437939420342\n",
      "Epoch 2144, Loss: 0.023863848648034036, Final Batch Loss: 0.01632073149085045\n",
      "Epoch 2145, Loss: 0.030271928058937192, Final Batch Loss: 0.019203977659344673\n",
      "Epoch 2146, Loss: 0.011921728786546737, Final Batch Loss: 0.002762122079730034\n",
      "Epoch 2147, Loss: 0.025300828740000725, Final Batch Loss: 0.003028684528544545\n",
      "Epoch 2148, Loss: 0.04207394132390618, Final Batch Loss: 0.00485489284619689\n",
      "Epoch 2149, Loss: 0.024702545604668558, Final Batch Loss: 0.0005126815522089601\n",
      "Epoch 2150, Loss: 0.037196918041445315, Final Batch Loss: 0.003463532542809844\n",
      "Epoch 2151, Loss: 0.01995220163371414, Final Batch Loss: 0.0010314405662938952\n",
      "Epoch 2152, Loss: 0.06611680999048986, Final Batch Loss: 0.0002718234318308532\n",
      "Epoch 2153, Loss: 0.05239268625155091, Final Batch Loss: 0.0057631852105259895\n",
      "Epoch 2154, Loss: 0.009860085061518475, Final Batch Loss: 0.0008911399636417627\n",
      "Epoch 2155, Loss: 0.0068506803654599935, Final Batch Loss: 0.0006409319466911256\n",
      "Epoch 2156, Loss: 0.04433995476574637, Final Batch Loss: 0.0002715728187467903\n",
      "Epoch 2157, Loss: 0.009019295335747302, Final Batch Loss: 0.0025399811565876007\n",
      "Epoch 2158, Loss: 0.04920755379134789, Final Batch Loss: 0.005951875355094671\n",
      "Epoch 2159, Loss: 0.03505002288147807, Final Batch Loss: 0.00038298568688333035\n",
      "Epoch 2160, Loss: 0.008185698563465849, Final Batch Loss: 0.0004046460671816021\n",
      "Epoch 2161, Loss: 0.027963242726400495, Final Batch Loss: 0.0009847620967775583\n",
      "Epoch 2162, Loss: 0.024154010054189712, Final Batch Loss: 0.020352620631456375\n",
      "Epoch 2163, Loss: 0.013019223872106522, Final Batch Loss: 0.007780629210174084\n",
      "Epoch 2164, Loss: 0.02074450912186876, Final Batch Loss: 0.000958318414632231\n",
      "Epoch 2165, Loss: 0.026455552695551887, Final Batch Loss: 0.0005428724689409137\n",
      "Epoch 2166, Loss: 0.008409168309299275, Final Batch Loss: 0.0035826638340950012\n",
      "Epoch 2167, Loss: 0.05584334969171323, Final Batch Loss: 0.002481130650267005\n",
      "Epoch 2168, Loss: 0.024281184159917757, Final Batch Loss: 0.0004265755123924464\n",
      "Epoch 2169, Loss: 0.009600148594472557, Final Batch Loss: 0.002030801959335804\n",
      "Epoch 2170, Loss: 0.023246120021212846, Final Batch Loss: 0.00179427454713732\n",
      "Epoch 2171, Loss: 0.012180119229014963, Final Batch Loss: 0.007794350851327181\n",
      "Epoch 2172, Loss: 0.03510305128293112, Final Batch Loss: 0.0029132673516869545\n",
      "Epoch 2173, Loss: 0.008434346527792513, Final Batch Loss: 0.0008325992384925485\n",
      "Epoch 2174, Loss: 0.017402777157258242, Final Batch Loss: 0.0015127189690247178\n",
      "Epoch 2175, Loss: 0.010073095705593005, Final Batch Loss: 0.0009354797657579184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2176, Loss: 0.031146725348662585, Final Batch Loss: 0.008531366474926472\n",
      "Epoch 2177, Loss: 0.031196866533719003, Final Batch Loss: 0.009770427830517292\n",
      "Epoch 2178, Loss: 0.02185449661919847, Final Batch Loss: 0.0007755149272270501\n",
      "Epoch 2179, Loss: 0.00796092115342617, Final Batch Loss: 0.0020698357839137316\n",
      "Epoch 2180, Loss: 0.0171815146895824, Final Batch Loss: 0.00032184788142330945\n",
      "Epoch 2181, Loss: 0.003669494326459244, Final Batch Loss: 0.0002179756120312959\n",
      "Epoch 2182, Loss: 0.014665628761576954, Final Batch Loss: 0.0002783954842016101\n",
      "Epoch 2183, Loss: 0.006041895074304193, Final Batch Loss: 0.001271202228963375\n",
      "Epoch 2184, Loss: 0.020539800927508622, Final Batch Loss: 0.006849395111203194\n",
      "Epoch 2185, Loss: 0.007061757263727486, Final Batch Loss: 0.0009751759935170412\n",
      "Epoch 2186, Loss: 0.009334472626505885, Final Batch Loss: 0.003953547682613134\n",
      "Epoch 2187, Loss: 0.02669490803964436, Final Batch Loss: 0.012398818507790565\n",
      "Epoch 2188, Loss: 0.003334329230710864, Final Batch Loss: 0.0007898767944425344\n",
      "Epoch 2189, Loss: 0.06513530644588172, Final Batch Loss: 0.019936472177505493\n",
      "Epoch 2190, Loss: 0.026286336636985652, Final Batch Loss: 0.00022718041145708412\n",
      "Epoch 2191, Loss: 0.009869303205050528, Final Batch Loss: 0.005455148871988058\n",
      "Epoch 2192, Loss: 0.006022399029461667, Final Batch Loss: 0.0015941890887916088\n",
      "Epoch 2193, Loss: 0.02656324312556535, Final Batch Loss: 0.011769440025091171\n",
      "Epoch 2194, Loss: 0.039240469865035266, Final Batch Loss: 0.0021480810828506947\n",
      "Epoch 2195, Loss: 0.11314710055012256, Final Batch Loss: 0.0016572307795286179\n",
      "Epoch 2196, Loss: 0.09199732693377882, Final Batch Loss: 0.022411007434129715\n",
      "Epoch 2197, Loss: 0.05059220246039331, Final Batch Loss: 0.02294219844043255\n",
      "Epoch 2198, Loss: 0.03158343769609928, Final Batch Loss: 0.0033302060328423977\n",
      "Epoch 2199, Loss: 0.03445669717621058, Final Batch Loss: 0.0019591660238802433\n",
      "Epoch 2200, Loss: 0.06884332478512079, Final Batch Loss: 0.0006745987921021879\n",
      "Epoch 2201, Loss: 0.01686970831360668, Final Batch Loss: 0.0013893613358959556\n",
      "Epoch 2202, Loss: 0.022675281506963074, Final Batch Loss: 0.0011632206151261926\n",
      "Epoch 2203, Loss: 0.0344122716342099, Final Batch Loss: 0.004017440602183342\n",
      "Epoch 2204, Loss: 0.020307944854721427, Final Batch Loss: 0.00163451605476439\n",
      "Epoch 2205, Loss: 0.009233684919308871, Final Batch Loss: 0.0019966282416135073\n",
      "Epoch 2206, Loss: 0.009385275596287102, Final Batch Loss: 0.0018674044404178858\n",
      "Epoch 2207, Loss: 0.03090433985926211, Final Batch Loss: 0.021503891795873642\n",
      "Epoch 2208, Loss: 0.01760263042524457, Final Batch Loss: 0.011097640730440617\n",
      "Epoch 2209, Loss: 0.019895306671969593, Final Batch Loss: 0.0012634268496185541\n",
      "Epoch 2210, Loss: 0.005616960697807372, Final Batch Loss: 0.0025929976254701614\n",
      "Epoch 2211, Loss: 0.01767063667648472, Final Batch Loss: 0.0003777698439080268\n",
      "Epoch 2212, Loss: 0.016300780000165105, Final Batch Loss: 0.0069604674354195595\n",
      "Epoch 2213, Loss: 0.02056836790870875, Final Batch Loss: 0.00495942635461688\n",
      "Epoch 2214, Loss: 0.0164010125445202, Final Batch Loss: 0.0013158014044165611\n",
      "Epoch 2215, Loss: 0.06195794587256387, Final Batch Loss: 0.011947291903197765\n",
      "Epoch 2216, Loss: 0.015381016710307449, Final Batch Loss: 0.0010045295348390937\n",
      "Epoch 2217, Loss: 0.019703845726326108, Final Batch Loss: 0.0049003781750798225\n",
      "Epoch 2218, Loss: 0.016180151520529762, Final Batch Loss: 0.0004563836264424026\n",
      "Epoch 2219, Loss: 0.009927846025675535, Final Batch Loss: 0.0027932969387620687\n",
      "Epoch 2220, Loss: 0.028227485687239096, Final Batch Loss: 0.0012424300657585263\n",
      "Epoch 2221, Loss: 0.014499537879601121, Final Batch Loss: 0.0017077465308830142\n",
      "Epoch 2222, Loss: 0.029910992714576423, Final Batch Loss: 0.00737197557464242\n",
      "Epoch 2223, Loss: 0.01162986273993738, Final Batch Loss: 0.0024476887192577124\n",
      "Epoch 2224, Loss: 0.010026215109974146, Final Batch Loss: 0.0006848548073321581\n",
      "Epoch 2225, Loss: 0.0173538567032665, Final Batch Loss: 0.0030698750633746386\n",
      "Epoch 2226, Loss: 0.021552502061240375, Final Batch Loss: 0.0007504787063226104\n",
      "Epoch 2227, Loss: 0.04819002121803351, Final Batch Loss: 0.0002764564997050911\n",
      "Epoch 2228, Loss: 0.04851798026356846, Final Batch Loss: 0.01542887557297945\n",
      "Epoch 2229, Loss: 0.023409874469507486, Final Batch Loss: 0.001736417762003839\n",
      "Epoch 2230, Loss: 0.013078770891297609, Final Batch Loss: 0.0018970569362863898\n",
      "Epoch 2231, Loss: 0.0255769751383923, Final Batch Loss: 0.00047912768786773086\n",
      "Epoch 2232, Loss: 0.01853208357351832, Final Batch Loss: 0.000923092185985297\n",
      "Epoch 2233, Loss: 0.017972627421841025, Final Batch Loss: 0.0014746944652870297\n",
      "Epoch 2234, Loss: 0.018372961872955784, Final Batch Loss: 0.009284201078116894\n",
      "Epoch 2235, Loss: 0.01569775666575879, Final Batch Loss: 0.005057710688561201\n",
      "Epoch 2236, Loss: 0.011836677964311093, Final Batch Loss: 0.0008628969662822783\n",
      "Epoch 2237, Loss: 0.020434329577255994, Final Batch Loss: 0.0011049462482333183\n",
      "Epoch 2238, Loss: 0.021051782154245302, Final Batch Loss: 0.004117525182664394\n",
      "Epoch 2239, Loss: 0.011319215671392158, Final Batch Loss: 0.0011582085862755775\n",
      "Epoch 2240, Loss: 0.01013742436771281, Final Batch Loss: 0.005756182596087456\n",
      "Epoch 2241, Loss: 0.005249784473562613, Final Batch Loss: 0.0005401967791840434\n",
      "Epoch 2242, Loss: 0.0076783415279351175, Final Batch Loss: 0.0011603411985561252\n",
      "Epoch 2243, Loss: 0.014424972236156464, Final Batch Loss: 0.005344388075172901\n",
      "Epoch 2244, Loss: 0.007488782590371557, Final Batch Loss: 0.00030022364808246493\n",
      "Epoch 2245, Loss: 0.007695180480368435, Final Batch Loss: 0.0033655718434602022\n",
      "Epoch 2246, Loss: 0.011325807601679116, Final Batch Loss: 0.002396504394710064\n",
      "Epoch 2247, Loss: 0.011150953534524888, Final Batch Loss: 0.004189825151115656\n",
      "Epoch 2248, Loss: 0.02354454091982916, Final Batch Loss: 0.010989452712237835\n",
      "Epoch 2249, Loss: 0.027801183256087825, Final Batch Loss: 0.001686585252173245\n",
      "Epoch 2250, Loss: 0.04360591195290908, Final Batch Loss: 0.0003526205546222627\n",
      "Epoch 2251, Loss: 0.01079335156828165, Final Batch Loss: 0.002472075168043375\n",
      "Epoch 2252, Loss: 0.04776202666107565, Final Batch Loss: 0.0011690795654430985\n",
      "Epoch 2253, Loss: 0.05805711378343403, Final Batch Loss: 0.0010512449080124497\n",
      "Epoch 2254, Loss: 0.02002029650611803, Final Batch Loss: 0.00936103519052267\n",
      "Epoch 2255, Loss: 0.034059002762660384, Final Batch Loss: 0.01475765835493803\n",
      "Epoch 2256, Loss: 0.044491397857200354, Final Batch Loss: 0.0013264799490571022\n",
      "Epoch 2257, Loss: 0.02236230473499745, Final Batch Loss: 0.007358454167842865\n",
      "Epoch 2258, Loss: 0.06962355016730726, Final Batch Loss: 0.0034669549204409122\n",
      "Epoch 2259, Loss: 0.03428600600454956, Final Batch Loss: 0.01732541061937809\n",
      "Epoch 2260, Loss: 0.015571293042739853, Final Batch Loss: 0.011465792544186115\n",
      "Epoch 2261, Loss: 0.030321863829158247, Final Batch Loss: 0.0008003796683624387\n",
      "Epoch 2262, Loss: 0.032811062526889145, Final Batch Loss: 0.0006085621425881982\n",
      "Epoch 2263, Loss: 0.005997449479764327, Final Batch Loss: 0.0013264698209241033\n",
      "Epoch 2264, Loss: 0.013607558270450681, Final Batch Loss: 0.0037441560998559\n",
      "Epoch 2265, Loss: 0.03809160046512261, Final Batch Loss: 0.009259631857275963\n",
      "Epoch 2266, Loss: 0.009587295062374324, Final Batch Loss: 0.003675428917631507\n",
      "Epoch 2267, Loss: 0.04089242935879156, Final Batch Loss: 0.0007225427543744445\n",
      "Epoch 2268, Loss: 0.055019660387188196, Final Batch Loss: 0.026836087927222252\n",
      "Epoch 2269, Loss: 0.01126364860101603, Final Batch Loss: 0.001455022837035358\n",
      "Epoch 2270, Loss: 0.009703894902486354, Final Batch Loss: 0.004766165278851986\n",
      "Epoch 2271, Loss: 0.03680659586098045, Final Batch Loss: 0.003618406830355525\n",
      "Epoch 2272, Loss: 0.009473503538174555, Final Batch Loss: 0.001162279979325831\n",
      "Epoch 2273, Loss: 0.011734568339306861, Final Batch Loss: 0.00405561039224267\n",
      "Epoch 2274, Loss: 0.011383991222828627, Final Batch Loss: 0.0021382237318903208\n",
      "Epoch 2275, Loss: 0.03569840081036091, Final Batch Loss: 0.023462427780032158\n",
      "Epoch 2276, Loss: 0.008074781420873478, Final Batch Loss: 0.0003042649768758565\n",
      "Epoch 2277, Loss: 0.023795064087607898, Final Batch Loss: 0.0004737324779853225\n",
      "Epoch 2278, Loss: 0.026845485997910146, Final Batch Loss: 0.003239614889025688\n",
      "Epoch 2279, Loss: 0.013950066000688821, Final Batch Loss: 0.0029467467684298754\n",
      "Epoch 2280, Loss: 0.028872803901322186, Final Batch Loss: 0.0023739412426948547\n",
      "Epoch 2281, Loss: 0.030221961467759684, Final Batch Loss: 0.0003682948008645326\n",
      "Epoch 2282, Loss: 0.01765211153542623, Final Batch Loss: 0.004369010683149099\n",
      "Epoch 2283, Loss: 0.01544107383233495, Final Batch Loss: 0.001625416916795075\n",
      "Epoch 2284, Loss: 0.012559534021420404, Final Batch Loss: 0.0011048961896449327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2285, Loss: 0.02566649380605668, Final Batch Loss: 0.013217286206781864\n",
      "Epoch 2286, Loss: 0.0062122842646203935, Final Batch Loss: 0.003519228892400861\n",
      "Epoch 2287, Loss: 0.011043524835258722, Final Batch Loss: 0.0004968296270817518\n",
      "Epoch 2288, Loss: 0.031591343809850514, Final Batch Loss: 0.01554045919328928\n",
      "Epoch 2289, Loss: 0.04784227286290843, Final Batch Loss: 8.147490734700114e-05\n",
      "Epoch 2290, Loss: 0.014631438185460865, Final Batch Loss: 0.004460382740944624\n",
      "Epoch 2291, Loss: 0.015271589742042124, Final Batch Loss: 0.00579462293535471\n",
      "Epoch 2292, Loss: 0.009198084575473331, Final Batch Loss: 0.0005835776682943106\n",
      "Epoch 2293, Loss: 0.011530022078659385, Final Batch Loss: 0.002509484300389886\n",
      "Epoch 2294, Loss: 0.026922097662463784, Final Batch Loss: 0.002067678142338991\n",
      "Epoch 2295, Loss: 0.025969945010729134, Final Batch Loss: 0.0017728102393448353\n",
      "Epoch 2296, Loss: 0.0566204278729856, Final Batch Loss: 0.02280571311712265\n",
      "Epoch 2297, Loss: 0.04087867250200361, Final Batch Loss: 0.023613572120666504\n",
      "Epoch 2298, Loss: 0.03317146698827855, Final Batch Loss: 0.0011472158366814256\n",
      "Epoch 2299, Loss: 0.02024721005000174, Final Batch Loss: 0.002329030307009816\n",
      "Epoch 2300, Loss: 0.044075435085687786, Final Batch Loss: 0.001912012929096818\n",
      "Epoch 2301, Loss: 0.026240844919811934, Final Batch Loss: 0.014421557076275349\n",
      "Epoch 2302, Loss: 0.03299483057344332, Final Batch Loss: 0.01403449010103941\n",
      "Epoch 2303, Loss: 0.04080707108369097, Final Batch Loss: 0.022046728059649467\n",
      "Epoch 2304, Loss: 0.015896103403065354, Final Batch Loss: 0.0006152394344098866\n",
      "Epoch 2305, Loss: 0.048408645612653345, Final Batch Loss: 0.0003698445507325232\n",
      "Epoch 2306, Loss: 0.018635638494743034, Final Batch Loss: 0.00046104282955639064\n",
      "Epoch 2307, Loss: 0.023485871497541666, Final Batch Loss: 0.001997924642637372\n",
      "Epoch 2308, Loss: 0.03828483138931915, Final Batch Loss: 0.024436496198177338\n",
      "Epoch 2309, Loss: 0.02125757932662964, Final Batch Loss: 0.003643286181613803\n",
      "Epoch 2310, Loss: 0.010373774624895304, Final Batch Loss: 0.0013637511292472482\n",
      "Epoch 2311, Loss: 0.015426639234647155, Final Batch Loss: 0.005423129070550203\n",
      "Epoch 2312, Loss: 0.0066071461187675595, Final Batch Loss: 0.0008988140034489334\n",
      "Epoch 2313, Loss: 0.013867173722246662, Final Batch Loss: 0.0001943075330927968\n",
      "Epoch 2314, Loss: 0.015240785898640752, Final Batch Loss: 0.0016139487270265818\n",
      "Epoch 2315, Loss: 0.018131400574930012, Final Batch Loss: 0.000893857388291508\n",
      "Epoch 2316, Loss: 0.01977547415299341, Final Batch Loss: 0.0008393090101890266\n",
      "Epoch 2317, Loss: 0.019452944223303348, Final Batch Loss: 0.0006773386267013848\n",
      "Epoch 2318, Loss: 0.038905730471014977, Final Batch Loss: 0.0012572265695780516\n",
      "Epoch 2319, Loss: 0.06734711048193276, Final Batch Loss: 0.005468552000820637\n",
      "Epoch 2320, Loss: 0.016612574632745236, Final Batch Loss: 0.0076533714309334755\n",
      "Epoch 2321, Loss: 0.05835704365745187, Final Batch Loss: 0.01990567333996296\n",
      "Epoch 2322, Loss: 0.023703657323494554, Final Batch Loss: 0.00392564432695508\n",
      "Epoch 2323, Loss: 0.02291675820015371, Final Batch Loss: 0.00494509469717741\n",
      "Epoch 2324, Loss: 0.012945838097948581, Final Batch Loss: 0.0011707618832588196\n",
      "Epoch 2325, Loss: 0.02426896884571761, Final Batch Loss: 0.009263531304895878\n",
      "Epoch 2326, Loss: 0.014632761536631733, Final Batch Loss: 0.0009309740271419287\n",
      "Epoch 2327, Loss: 0.01770561741432175, Final Batch Loss: 0.004571970086544752\n",
      "Epoch 2328, Loss: 0.03573284327285364, Final Batch Loss: 0.0035360639449208975\n",
      "Epoch 2329, Loss: 0.019106947351247072, Final Batch Loss: 0.002638798439875245\n",
      "Epoch 2330, Loss: 0.0156069865624886, Final Batch Loss: 0.005592129658907652\n",
      "Epoch 2331, Loss: 0.01949504332151264, Final Batch Loss: 0.013971168547868729\n",
      "Epoch 2332, Loss: 0.012780782853951678, Final Batch Loss: 0.0014121263520792127\n",
      "Epoch 2333, Loss: 0.006046064343536273, Final Batch Loss: 0.0003815088130068034\n",
      "Epoch 2334, Loss: 0.007073446351569146, Final Batch Loss: 0.0009133669664151967\n",
      "Epoch 2335, Loss: 0.012653317680815235, Final Batch Loss: 0.0015577544691041112\n",
      "Epoch 2336, Loss: 0.006472423585364595, Final Batch Loss: 0.00024276578915305436\n",
      "Epoch 2337, Loss: 0.023608411836903542, Final Batch Loss: 0.012532140128314495\n",
      "Epoch 2338, Loss: 0.008506158264935948, Final Batch Loss: 0.006397192366421223\n",
      "Epoch 2339, Loss: 0.07841140677919611, Final Batch Loss: 0.0006181353819556534\n",
      "Epoch 2340, Loss: 0.006252957653487101, Final Batch Loss: 0.0008966373279690742\n",
      "Epoch 2341, Loss: 0.03556407880387269, Final Batch Loss: 0.0006858855485916138\n",
      "Epoch 2342, Loss: 0.008571024460252374, Final Batch Loss: 0.001872053719125688\n",
      "Epoch 2343, Loss: 0.022393419494619593, Final Batch Loss: 0.008516870439052582\n",
      "Epoch 2344, Loss: 0.007477948951418512, Final Batch Loss: 0.0010123789543285966\n",
      "Epoch 2345, Loss: 0.02439011406386271, Final Batch Loss: 0.0013945953687652946\n",
      "Epoch 2346, Loss: 0.01512996293604374, Final Batch Loss: 0.0018152829725295305\n",
      "Epoch 2347, Loss: 0.04339916826575063, Final Batch Loss: 0.0006717551732435822\n",
      "Epoch 2348, Loss: 0.007453847007127479, Final Batch Loss: 0.0013024131767451763\n",
      "Epoch 2349, Loss: 0.012436016142601147, Final Batch Loss: 0.002290134783834219\n",
      "Epoch 2350, Loss: 0.03702347609214485, Final Batch Loss: 0.0006132973940111697\n",
      "Epoch 2351, Loss: 0.003770916344365105, Final Batch Loss: 0.0010809196392074227\n",
      "Epoch 2352, Loss: 0.05045219415842439, Final Batch Loss: 0.011956180445849895\n",
      "Epoch 2353, Loss: 0.047290765622165054, Final Batch Loss: 0.00041693885577842593\n",
      "Epoch 2354, Loss: 0.05384790411335416, Final Batch Loss: 0.015736069530248642\n",
      "Epoch 2355, Loss: 0.007551970251370221, Final Batch Loss: 0.0008372326847165823\n",
      "Epoch 2356, Loss: 0.02205268552643247, Final Batch Loss: 0.00037030354724265635\n",
      "Epoch 2357, Loss: 0.023736942719551735, Final Batch Loss: 0.004781189374625683\n",
      "Epoch 2358, Loss: 0.04456583870342001, Final Batch Loss: 0.03972853347659111\n",
      "Epoch 2359, Loss: 0.040247837372589856, Final Batch Loss: 0.0028308341279625893\n",
      "Epoch 2360, Loss: 0.0059358563157729805, Final Batch Loss: 0.002051012357696891\n",
      "Epoch 2361, Loss: 0.04785044980235398, Final Batch Loss: 0.0173422209918499\n",
      "Epoch 2362, Loss: 0.00901565549429506, Final Batch Loss: 0.005591538269072771\n",
      "Epoch 2363, Loss: 0.0143203538027592, Final Batch Loss: 0.0016516384202986956\n",
      "Epoch 2364, Loss: 0.02638808113988489, Final Batch Loss: 0.0010379776358604431\n",
      "Epoch 2365, Loss: 0.05773506476543844, Final Batch Loss: 0.0033708109986037016\n",
      "Epoch 2366, Loss: 0.02198749134549871, Final Batch Loss: 0.01708989590406418\n",
      "Epoch 2367, Loss: 0.007328434003284201, Final Batch Loss: 0.0007959664217196405\n",
      "Epoch 2368, Loss: 0.014314016385469586, Final Batch Loss: 0.0001920059439726174\n",
      "Epoch 2369, Loss: 0.03574362787185237, Final Batch Loss: 0.025127962231636047\n",
      "Epoch 2370, Loss: 0.016043616749811918, Final Batch Loss: 0.0008742619538679719\n",
      "Epoch 2371, Loss: 0.06734118313761428, Final Batch Loss: 0.0029920523520559072\n",
      "Epoch 2372, Loss: 0.008576189284212887, Final Batch Loss: 0.0018343861447647214\n",
      "Epoch 2373, Loss: 0.030394336965400726, Final Batch Loss: 0.002950780326500535\n",
      "Epoch 2374, Loss: 0.0124493672628887, Final Batch Loss: 0.002114290138706565\n",
      "Epoch 2375, Loss: 0.012157355202361941, Final Batch Loss: 0.0006800648989155889\n",
      "Epoch 2376, Loss: 0.024011216475628316, Final Batch Loss: 0.010478273965418339\n",
      "Epoch 2377, Loss: 0.013803021021885797, Final Batch Loss: 0.0050938366912305355\n",
      "Epoch 2378, Loss: 0.01711721229366958, Final Batch Loss: 0.006159628741443157\n",
      "Epoch 2379, Loss: 0.031755935051478446, Final Batch Loss: 0.0009614899172447622\n",
      "Epoch 2380, Loss: 0.016142131004016846, Final Batch Loss: 0.0005862837424501777\n",
      "Epoch 2381, Loss: 0.012151605798862875, Final Batch Loss: 0.0013635809300467372\n",
      "Epoch 2382, Loss: 0.0236336117668543, Final Batch Loss: 0.0017429058207198977\n",
      "Epoch 2383, Loss: 0.012905405426863581, Final Batch Loss: 0.0005278227035887539\n",
      "Epoch 2384, Loss: 0.04344546783249825, Final Batch Loss: 0.03446653112769127\n",
      "Epoch 2385, Loss: 0.01152599963825196, Final Batch Loss: 0.001271811779588461\n",
      "Epoch 2386, Loss: 0.024438670836389065, Final Batch Loss: 0.014953015372157097\n",
      "Epoch 2387, Loss: 0.019306978734675795, Final Batch Loss: 0.0008256936562247574\n",
      "Epoch 2388, Loss: 0.012582486000610515, Final Batch Loss: 0.0005831813323311508\n",
      "Epoch 2389, Loss: 0.00940903159789741, Final Batch Loss: 0.0013389551313593984\n",
      "Epoch 2390, Loss: 0.025086905050557107, Final Batch Loss: 0.008908484131097794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2391, Loss: 0.033938932756427675, Final Batch Loss: 0.01994091086089611\n",
      "Epoch 2392, Loss: 0.02250669797649607, Final Batch Loss: 0.013247494585812092\n",
      "Epoch 2393, Loss: 0.049354064802173525, Final Batch Loss: 0.007703094277530909\n",
      "Epoch 2394, Loss: 0.005983846916933544, Final Batch Loss: 0.0034418459981679916\n",
      "Epoch 2395, Loss: 0.02738938364200294, Final Batch Loss: 0.01524270884692669\n",
      "Epoch 2396, Loss: 0.006509014157927595, Final Batch Loss: 0.004843330010771751\n",
      "Epoch 2397, Loss: 0.0072322630730923265, Final Batch Loss: 0.0018457998521625996\n",
      "Epoch 2398, Loss: 0.015273969562258571, Final Batch Loss: 0.0004449274856597185\n",
      "Epoch 2399, Loss: 0.03309382771840319, Final Batch Loss: 0.0004241540445946157\n",
      "Epoch 2400, Loss: 0.004606536531355232, Final Batch Loss: 0.0005421341629698873\n",
      "Epoch 2401, Loss: 0.01923306327080354, Final Batch Loss: 0.0023561047855764627\n",
      "Epoch 2402, Loss: 0.0055153987486846745, Final Batch Loss: 0.001031806692481041\n",
      "Epoch 2403, Loss: 0.012905072828289121, Final Batch Loss: 0.006820228416472673\n",
      "Epoch 2404, Loss: 0.01463743754720781, Final Batch Loss: 0.0002359674108447507\n",
      "Epoch 2405, Loss: 0.012030497717205435, Final Batch Loss: 0.0037779496051371098\n",
      "Epoch 2406, Loss: 0.009395097411470488, Final Batch Loss: 0.00044599748798646033\n",
      "Epoch 2407, Loss: 0.048263461503665894, Final Batch Loss: 0.0018653389997780323\n",
      "Epoch 2408, Loss: 0.02668039913987741, Final Batch Loss: 0.002411764580756426\n",
      "Epoch 2409, Loss: 0.04039932868909091, Final Batch Loss: 0.023323258385062218\n",
      "Epoch 2410, Loss: 0.016138212638907135, Final Batch Loss: 0.009457549080252647\n",
      "Epoch 2411, Loss: 0.02010274212807417, Final Batch Loss: 0.007831661961972713\n",
      "Epoch 2412, Loss: 0.03381924005225301, Final Batch Loss: 0.0027241006027907133\n",
      "Epoch 2413, Loss: 0.022214228665689006, Final Batch Loss: 0.01844625174999237\n",
      "Epoch 2414, Loss: 0.011232451244723052, Final Batch Loss: 0.005851980298757553\n",
      "Epoch 2415, Loss: 0.06608487269841135, Final Batch Loss: 0.004901489708572626\n",
      "Epoch 2416, Loss: 0.014252357766963542, Final Batch Loss: 0.005362901370972395\n",
      "Epoch 2417, Loss: 0.03294969740090892, Final Batch Loss: 0.0020534514915198088\n",
      "Epoch 2418, Loss: 0.034076744923368096, Final Batch Loss: 0.0012921617599204183\n",
      "Epoch 2419, Loss: 0.04538158650393598, Final Batch Loss: 0.021950827911496162\n",
      "Epoch 2420, Loss: 0.033348903816659003, Final Batch Loss: 0.002329351147636771\n",
      "Epoch 2421, Loss: 0.03422527352813631, Final Batch Loss: 0.026700878515839577\n",
      "Epoch 2422, Loss: 0.01977314567193389, Final Batch Loss: 0.002897868864238262\n",
      "Epoch 2423, Loss: 0.013360064127482474, Final Batch Loss: 0.0075206272304058075\n",
      "Epoch 2424, Loss: 0.00721220055129379, Final Batch Loss: 0.001413098769262433\n",
      "Epoch 2425, Loss: 0.01359077321831137, Final Batch Loss: 0.0024302322417497635\n",
      "Epoch 2426, Loss: 0.00580290803918615, Final Batch Loss: 0.0012640646891668439\n",
      "Epoch 2427, Loss: 0.04196214285911992, Final Batch Loss: 0.0006636417820118368\n",
      "Epoch 2428, Loss: 0.011885977612109855, Final Batch Loss: 0.0018230804707854986\n",
      "Epoch 2429, Loss: 0.010209486732492223, Final Batch Loss: 0.0012125561479479074\n",
      "Epoch 2430, Loss: 0.01007680658949539, Final Batch Loss: 0.0026543925050646067\n",
      "Epoch 2431, Loss: 0.011157692933920771, Final Batch Loss: 0.0004206294543109834\n",
      "Epoch 2432, Loss: 0.005057540169218555, Final Batch Loss: 0.000294494820991531\n",
      "Epoch 2433, Loss: 0.007323347250348888, Final Batch Loss: 0.00019172833708580583\n",
      "Epoch 2434, Loss: 0.024879112606868148, Final Batch Loss: 0.0006990681868046522\n",
      "Epoch 2435, Loss: 0.00866768150444841, Final Batch Loss: 0.0027948387432843447\n",
      "Epoch 2436, Loss: 0.015711481450125575, Final Batch Loss: 0.0023279336746782064\n",
      "Epoch 2437, Loss: 0.03111233036906924, Final Batch Loss: 0.0005464114947244525\n",
      "Epoch 2438, Loss: 0.006829333899077028, Final Batch Loss: 0.000999797717668116\n",
      "Epoch 2439, Loss: 0.022935301705729216, Final Batch Loss: 0.018328268080949783\n",
      "Epoch 2440, Loss: 0.013776655949186534, Final Batch Loss: 0.010849171318113804\n",
      "Epoch 2441, Loss: 0.015471201069885865, Final Batch Loss: 0.001995381899178028\n",
      "Epoch 2442, Loss: 0.009939660259988159, Final Batch Loss: 0.002373710973188281\n",
      "Epoch 2443, Loss: 0.017927895241882652, Final Batch Loss: 0.0033731753937900066\n",
      "Epoch 2444, Loss: 0.02226911546313204, Final Batch Loss: 0.017545482143759727\n",
      "Epoch 2445, Loss: 0.04213628312572837, Final Batch Loss: 0.0013171862810850143\n",
      "Epoch 2446, Loss: 0.028875098912976682, Final Batch Loss: 0.0048119081184268\n",
      "Epoch 2447, Loss: 0.00923472674912773, Final Batch Loss: 0.003603673307225108\n",
      "Epoch 2448, Loss: 0.022484783665277064, Final Batch Loss: 0.0010500262724235654\n",
      "Epoch 2449, Loss: 0.026468865136848763, Final Batch Loss: 0.014363846741616726\n",
      "Epoch 2450, Loss: 0.010755789378890768, Final Batch Loss: 0.0009482756722718477\n",
      "Epoch 2451, Loss: 0.0178218386718072, Final Batch Loss: 0.01217039953917265\n",
      "Epoch 2452, Loss: 0.05894086172338575, Final Batch Loss: 0.041011881083250046\n",
      "Epoch 2453, Loss: 0.05338974285405129, Final Batch Loss: 0.018448632210493088\n",
      "Epoch 2454, Loss: 0.04100319652934559, Final Batch Loss: 0.018280405551195145\n",
      "Epoch 2455, Loss: 0.012490220047766343, Final Batch Loss: 0.0012636336032301188\n",
      "Epoch 2456, Loss: 0.020852043860941194, Final Batch Loss: 0.01633726805448532\n",
      "Epoch 2457, Loss: 0.021761651732958853, Final Batch Loss: 0.0008353395969606936\n",
      "Epoch 2458, Loss: 0.01791056920774281, Final Batch Loss: 0.0013289705384522676\n",
      "Epoch 2459, Loss: 0.004449538071639836, Final Batch Loss: 0.001090025994926691\n",
      "Epoch 2460, Loss: 0.017774940701201558, Final Batch Loss: 0.008106181398034096\n",
      "Epoch 2461, Loss: 0.013051911926595494, Final Batch Loss: 0.00013809787924401462\n",
      "Epoch 2462, Loss: 0.007736730593023822, Final Batch Loss: 0.0017672779504209757\n",
      "Epoch 2463, Loss: 0.01872370284399949, Final Batch Loss: 0.0041157957166433334\n",
      "Epoch 2464, Loss: 0.02929167909314856, Final Batch Loss: 0.005881474819034338\n",
      "Epoch 2465, Loss: 0.015743718831799924, Final Batch Loss: 0.0018535511335358024\n",
      "Epoch 2466, Loss: 0.01570686072227545, Final Batch Loss: 0.008893296122550964\n",
      "Epoch 2467, Loss: 0.006582698377314955, Final Batch Loss: 0.0017824830720201135\n",
      "Epoch 2468, Loss: 0.003655147651443258, Final Batch Loss: 0.0012625842355191708\n",
      "Epoch 2469, Loss: 0.03996674154768698, Final Batch Loss: 0.0001246481842827052\n",
      "Epoch 2470, Loss: 0.10411089181434363, Final Batch Loss: 0.0031102930661290884\n",
      "Epoch 2471, Loss: 0.008750225009862334, Final Batch Loss: 0.002380294492468238\n",
      "Epoch 2472, Loss: 0.005473605764564127, Final Batch Loss: 0.0005830211448483169\n",
      "Epoch 2473, Loss: 0.038872433186043054, Final Batch Loss: 0.01125624030828476\n",
      "Epoch 2474, Loss: 0.05667334282770753, Final Batch Loss: 0.00643159169703722\n",
      "Epoch 2475, Loss: 0.028187639662064612, Final Batch Loss: 0.0038131277542561293\n",
      "Epoch 2476, Loss: 0.01564611482899636, Final Batch Loss: 0.0010833489941433072\n",
      "Epoch 2477, Loss: 0.015874132339376956, Final Batch Loss: 0.00947283860296011\n",
      "Epoch 2478, Loss: 0.010924560367129743, Final Batch Loss: 0.003389189951121807\n",
      "Epoch 2479, Loss: 0.005724270216887817, Final Batch Loss: 0.000341632665367797\n",
      "Epoch 2480, Loss: 0.009929691994329914, Final Batch Loss: 0.004271689802408218\n",
      "Epoch 2481, Loss: 0.004149582469835877, Final Batch Loss: 0.0007081099902279675\n",
      "Epoch 2482, Loss: 0.016538684387342073, Final Batch Loss: 0.0001905139215523377\n",
      "Epoch 2483, Loss: 0.02373467234428972, Final Batch Loss: 0.0010120468214154243\n",
      "Epoch 2484, Loss: 0.009018174023367465, Final Batch Loss: 0.0007430296973325312\n",
      "Epoch 2485, Loss: 0.004371596092823893, Final Batch Loss: 0.0007171374745666981\n",
      "Epoch 2486, Loss: 0.004345114983152598, Final Batch Loss: 0.0010113617172464728\n",
      "Epoch 2487, Loss: 0.02176837855949998, Final Batch Loss: 0.0002832473546732217\n",
      "Epoch 2488, Loss: 0.01700493588577956, Final Batch Loss: 0.0004036333120893687\n",
      "Epoch 2489, Loss: 0.03064565165550448, Final Batch Loss: 0.002577719511464238\n",
      "Epoch 2490, Loss: 0.006500213348772377, Final Batch Loss: 0.0004214511136524379\n",
      "Epoch 2491, Loss: 0.012071203920640983, Final Batch Loss: 0.0001718438434181735\n",
      "Epoch 2492, Loss: 0.023131495516281575, Final Batch Loss: 0.0005657849251292646\n",
      "Epoch 2493, Loss: 0.015125911799259484, Final Batch Loss: 0.0064813531935215\n",
      "Epoch 2494, Loss: 0.03830089124676306, Final Batch Loss: 0.002265416318550706\n",
      "Epoch 2495, Loss: 0.03209479013457894, Final Batch Loss: 0.0014353379374369979\n",
      "Epoch 2496, Loss: 0.03319992031902075, Final Batch Loss: 0.0008933983044698834\n",
      "Epoch 2497, Loss: 0.025380927720107138, Final Batch Loss: 0.0011799013009294868\n",
      "Epoch 2498, Loss: 0.06269279622938484, Final Batch Loss: 0.005333018023520708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2499, Loss: 0.012647014169488102, Final Batch Loss: 0.001539568998850882\n",
      "Epoch 2500, Loss: 0.03328857134329155, Final Batch Loss: 0.016514495015144348\n",
      "Epoch 2501, Loss: 0.021751343621872365, Final Batch Loss: 0.0029331196565181017\n",
      "Epoch 2502, Loss: 0.019240978901507333, Final Batch Loss: 0.0006072406540624797\n",
      "Epoch 2503, Loss: 0.014673446887172759, Final Batch Loss: 0.0033464322332292795\n",
      "Epoch 2504, Loss: 0.01055537792854011, Final Batch Loss: 0.0006805216544307768\n",
      "Epoch 2505, Loss: 0.01361162995453924, Final Batch Loss: 0.001607235986739397\n",
      "Epoch 2506, Loss: 0.005489879491506144, Final Batch Loss: 0.0002496243978384882\n",
      "Epoch 2507, Loss: 0.047517477185465395, Final Batch Loss: 0.022073613479733467\n",
      "Epoch 2508, Loss: 0.07579447084572166, Final Batch Loss: 0.013853945769369602\n",
      "Epoch 2509, Loss: 0.014914509491063654, Final Batch Loss: 0.0018548996886238456\n",
      "Epoch 2510, Loss: 0.057423250284045935, Final Batch Loss: 0.004370472393929958\n",
      "Epoch 2511, Loss: 0.030331030284287408, Final Batch Loss: 0.00046995372395031154\n",
      "Epoch 2512, Loss: 0.02297515666577965, Final Batch Loss: 0.0010869837133213878\n",
      "Epoch 2513, Loss: 0.02127229270990938, Final Batch Loss: 0.0005209353985264897\n",
      "Epoch 2514, Loss: 0.006405444932170212, Final Batch Loss: 0.0011382250813767314\n",
      "Epoch 2515, Loss: 0.033443132881075144, Final Batch Loss: 0.0011068900348618627\n",
      "Epoch 2516, Loss: 0.04511566791916266, Final Batch Loss: 0.00733904680237174\n",
      "Epoch 2517, Loss: 0.07629295939113945, Final Batch Loss: 0.0016019567847251892\n",
      "Epoch 2518, Loss: 0.02807018131716177, Final Batch Loss: 0.0029750708490610123\n",
      "Epoch 2519, Loss: 0.02296110158204101, Final Batch Loss: 0.008075624704360962\n",
      "Epoch 2520, Loss: 0.009982735035009682, Final Batch Loss: 0.002289809286594391\n",
      "Epoch 2521, Loss: 0.015167000703513622, Final Batch Loss: 0.004981484264135361\n",
      "Epoch 2522, Loss: 0.07188605712144636, Final Batch Loss: 0.04141400754451752\n",
      "Epoch 2523, Loss: 0.016719834122341126, Final Batch Loss: 0.007351566106081009\n",
      "Epoch 2524, Loss: 0.014054534607566893, Final Batch Loss: 0.005453529767692089\n",
      "Epoch 2525, Loss: 0.007686688972171396, Final Batch Loss: 0.0038429175037890673\n",
      "Epoch 2526, Loss: 0.009124395437538624, Final Batch Loss: 0.0011895564384758472\n",
      "Epoch 2527, Loss: 0.01261080356198363, Final Batch Loss: 0.0038606692105531693\n",
      "Epoch 2528, Loss: 0.021008694893680513, Final Batch Loss: 0.0028331871144473553\n",
      "Epoch 2529, Loss: 0.010980600083712488, Final Batch Loss: 0.0012312462786212564\n",
      "Epoch 2530, Loss: 0.013202106347307563, Final Batch Loss: 0.003977117128670216\n",
      "Epoch 2531, Loss: 0.009921842138282955, Final Batch Loss: 0.00019340659491717815\n",
      "Epoch 2532, Loss: 0.012341000518063083, Final Batch Loss: 0.006942827254533768\n",
      "Epoch 2533, Loss: 0.007254199997987598, Final Batch Loss: 0.0019540484063327312\n",
      "Epoch 2534, Loss: 0.003761941159609705, Final Batch Loss: 0.0009295853669755161\n",
      "Epoch 2535, Loss: 0.006822532450314611, Final Batch Loss: 0.00041628972394391894\n",
      "Epoch 2536, Loss: 0.007679244561586529, Final Batch Loss: 0.0022430347744375467\n",
      "Epoch 2537, Loss: 0.01375763159012422, Final Batch Loss: 0.0008574820822104812\n",
      "Epoch 2538, Loss: 0.005371398408897221, Final Batch Loss: 0.0017326060915365815\n",
      "Epoch 2539, Loss: 0.01015920884674415, Final Batch Loss: 0.00040473739500157535\n",
      "Epoch 2540, Loss: 0.00829576633987017, Final Batch Loss: 0.0011452257167547941\n",
      "Epoch 2541, Loss: 0.004260180692654103, Final Batch Loss: 0.0006072341348044574\n",
      "Epoch 2542, Loss: 0.007110351056326181, Final Batch Loss: 0.0012962085893377662\n",
      "Epoch 2543, Loss: 0.01146794763917569, Final Batch Loss: 0.0024782358668744564\n",
      "Epoch 2544, Loss: 0.04238239917322062, Final Batch Loss: 0.0004188916936982423\n",
      "Epoch 2545, Loss: 0.062030702305492014, Final Batch Loss: 0.02754429168999195\n",
      "Epoch 2546, Loss: 0.01781387475784868, Final Batch Loss: 0.003947646822780371\n",
      "Epoch 2547, Loss: 0.007352286716923118, Final Batch Loss: 0.0009808661416172981\n",
      "Epoch 2548, Loss: 0.00911089516011998, Final Batch Loss: 0.003886043792590499\n",
      "Epoch 2549, Loss: 0.0061932612443342805, Final Batch Loss: 0.0004444285004865378\n",
      "Epoch 2550, Loss: 0.027635011123493314, Final Batch Loss: 0.020815534517169\n",
      "Epoch 2551, Loss: 0.00532729213591665, Final Batch Loss: 0.000800914887804538\n",
      "Epoch 2552, Loss: 0.02935856574913487, Final Batch Loss: 0.007963837124407291\n",
      "Epoch 2553, Loss: 0.006011199613567442, Final Batch Loss: 0.002772526815533638\n",
      "Epoch 2554, Loss: 0.007319644879316911, Final Batch Loss: 0.0005299519980326295\n",
      "Epoch 2555, Loss: 0.03731258487096056, Final Batch Loss: 0.017599565908312798\n",
      "Epoch 2556, Loss: 0.012626236173673533, Final Batch Loss: 0.008039949461817741\n",
      "Epoch 2557, Loss: 0.024718719389056787, Final Batch Loss: 0.0004176196816843003\n",
      "Epoch 2558, Loss: 0.0643920130096376, Final Batch Loss: 0.02029411494731903\n",
      "Epoch 2559, Loss: 0.04018766572698951, Final Batch Loss: 0.0318867564201355\n",
      "Epoch 2560, Loss: 0.04010116378776729, Final Batch Loss: 0.002291179494932294\n",
      "Epoch 2561, Loss: 0.07723883196013048, Final Batch Loss: 0.0005022361292503774\n",
      "Epoch 2562, Loss: 0.06434417003765702, Final Batch Loss: 0.0005033495835959911\n",
      "Epoch 2563, Loss: 0.053810397104825824, Final Batch Loss: 0.03641669824719429\n",
      "Epoch 2564, Loss: 0.06341554735263344, Final Batch Loss: 0.0002421866956865415\n",
      "Epoch 2565, Loss: 0.021103538514580578, Final Batch Loss: 0.008976434357464314\n",
      "Epoch 2566, Loss: 0.017681557801552117, Final Batch Loss: 0.0005593012319877744\n",
      "Epoch 2567, Loss: 0.03803087864071131, Final Batch Loss: 0.0037872421089559793\n",
      "Epoch 2568, Loss: 0.015004838351160288, Final Batch Loss: 0.0019880947656929493\n",
      "Epoch 2569, Loss: 0.005074578162748367, Final Batch Loss: 0.00045705377124249935\n",
      "Epoch 2570, Loss: 0.026989256744855084, Final Batch Loss: 0.0022889876272529364\n",
      "Epoch 2571, Loss: 0.04279867809964344, Final Batch Loss: 0.0006624081870540977\n",
      "Epoch 2572, Loss: 0.033367188181728125, Final Batch Loss: 0.0020645984914153814\n",
      "Epoch 2573, Loss: 0.009079683688469231, Final Batch Loss: 0.004402502905577421\n",
      "Epoch 2574, Loss: 0.0037060969334561378, Final Batch Loss: 0.001209054491482675\n",
      "Epoch 2575, Loss: 0.0048537132097408175, Final Batch Loss: 0.0005328368861228228\n",
      "Epoch 2576, Loss: 0.013122197182383388, Final Batch Loss: 0.007545031141489744\n",
      "Epoch 2577, Loss: 0.011445332027506083, Final Batch Loss: 0.0007253517978824675\n",
      "Epoch 2578, Loss: 0.07063093886245042, Final Batch Loss: 0.037455249577760696\n",
      "Epoch 2579, Loss: 0.007050268875900656, Final Batch Loss: 0.0011051598703488708\n",
      "Epoch 2580, Loss: 0.010612581390887499, Final Batch Loss: 0.0014071118785068393\n",
      "Epoch 2581, Loss: 0.0074611802701838315, Final Batch Loss: 0.0005453094490803778\n",
      "Epoch 2582, Loss: 0.00873859942657873, Final Batch Loss: 0.0003845042665489018\n",
      "Epoch 2583, Loss: 0.018409768934361637, Final Batch Loss: 0.00938995648175478\n",
      "Epoch 2584, Loss: 0.004749198793433607, Final Batch Loss: 0.0007300392026081681\n",
      "Epoch 2585, Loss: 0.011779928696341813, Final Batch Loss: 0.0009089384111575782\n",
      "Epoch 2586, Loss: 0.014265549056290183, Final Batch Loss: 6.70581721351482e-05\n",
      "Epoch 2587, Loss: 0.014268149738200009, Final Batch Loss: 0.001332165440544486\n",
      "Epoch 2588, Loss: 0.016337747685611248, Final Batch Loss: 0.0012330565368756652\n",
      "Epoch 2589, Loss: 0.0202375102089718, Final Batch Loss: 0.0018786978907883167\n",
      "Epoch 2590, Loss: 0.003086236276431009, Final Batch Loss: 0.0005408969591371715\n",
      "Epoch 2591, Loss: 0.03511704702395946, Final Batch Loss: 0.0011300485348328948\n",
      "Epoch 2592, Loss: 0.008331702076247893, Final Batch Loss: 0.003791298484429717\n",
      "Epoch 2593, Loss: 0.008064792898949236, Final Batch Loss: 0.002948611043393612\n",
      "Epoch 2594, Loss: 0.023751298373099416, Final Batch Loss: 0.004166606813669205\n",
      "Epoch 2595, Loss: 0.03304175756056793, Final Batch Loss: 0.002106098458170891\n",
      "Epoch 2596, Loss: 0.016541194985620677, Final Batch Loss: 0.0021530429366976023\n",
      "Epoch 2597, Loss: 0.011985901248408481, Final Batch Loss: 0.00016612684703432024\n",
      "Epoch 2598, Loss: 0.01938925898866728, Final Batch Loss: 0.000607020512688905\n",
      "Epoch 2599, Loss: 0.018117304774932563, Final Batch Loss: 0.001436778111383319\n",
      "Epoch 2600, Loss: 0.011824697372503579, Final Batch Loss: 0.00048001721734181046\n",
      "Epoch 2601, Loss: 0.03303125154343434, Final Batch Loss: 0.0004152223700657487\n",
      "Epoch 2602, Loss: 0.002776496250589844, Final Batch Loss: 0.00034841784508898854\n",
      "Epoch 2603, Loss: 0.015583768661599606, Final Batch Loss: 0.00044206035090610385\n",
      "Epoch 2604, Loss: 0.009586635685991496, Final Batch Loss: 0.002123096492141485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2605, Loss: 0.009230923591530882, Final Batch Loss: 0.0026161547284573317\n",
      "Epoch 2606, Loss: 0.054174234392121434, Final Batch Loss: 0.02161642722785473\n",
      "Epoch 2607, Loss: 0.014680147200124338, Final Batch Loss: 0.01110801286995411\n",
      "Epoch 2608, Loss: 0.021611664284137078, Final Batch Loss: 0.0011853575706481934\n",
      "Epoch 2609, Loss: 0.006604203314054757, Final Batch Loss: 0.0005585948820225894\n",
      "Epoch 2610, Loss: 0.02334108398645185, Final Batch Loss: 0.002495056251063943\n",
      "Epoch 2611, Loss: 0.014193661510944366, Final Batch Loss: 0.0013938196934759617\n",
      "Epoch 2612, Loss: 0.011261251813266426, Final Batch Loss: 0.0006897060084156692\n",
      "Epoch 2613, Loss: 0.00659786825417541, Final Batch Loss: 0.000811667472589761\n",
      "Epoch 2614, Loss: 0.034311726019950584, Final Batch Loss: 0.006774005480110645\n",
      "Epoch 2615, Loss: 0.025865179195534438, Final Batch Loss: 0.005085128825157881\n",
      "Epoch 2616, Loss: 0.012380007246974856, Final Batch Loss: 0.006849648896604776\n",
      "Epoch 2617, Loss: 0.01690206938656047, Final Batch Loss: 0.0039566513150930405\n",
      "Epoch 2618, Loss: 0.06612277659587562, Final Batch Loss: 0.028190944343805313\n",
      "Epoch 2619, Loss: 0.032269707444356754, Final Batch Loss: 0.015117478556931019\n",
      "Epoch 2620, Loss: 0.004778755479492247, Final Batch Loss: 0.001346121309325099\n",
      "Epoch 2621, Loss: 0.009495286212768406, Final Batch Loss: 0.0032718332950025797\n",
      "Epoch 2622, Loss: 0.019541672430932522, Final Batch Loss: 0.0013433379353955388\n",
      "Epoch 2623, Loss: 0.010912375611951575, Final Batch Loss: 0.002573428675532341\n",
      "Epoch 2624, Loss: 0.011478745349450037, Final Batch Loss: 0.0008363414672203362\n",
      "Epoch 2625, Loss: 0.016678540006978437, Final Batch Loss: 0.009003734216094017\n",
      "Epoch 2626, Loss: 0.005535849777515978, Final Batch Loss: 0.002204218180850148\n",
      "Epoch 2627, Loss: 0.00878731906414032, Final Batch Loss: 0.0048541356809437275\n",
      "Epoch 2628, Loss: 0.021784127660794184, Final Batch Loss: 0.0002410834131296724\n",
      "Epoch 2629, Loss: 0.003442152199568227, Final Batch Loss: 0.0006600408814847469\n",
      "Epoch 2630, Loss: 0.017580959014594555, Final Batch Loss: 0.0009425000753253698\n",
      "Epoch 2631, Loss: 0.018137065693736076, Final Batch Loss: 0.012484668754041195\n",
      "Epoch 2632, Loss: 0.020799343394173775, Final Batch Loss: 0.0005681572947651148\n",
      "Epoch 2633, Loss: 0.017975879949517548, Final Batch Loss: 0.010199383832514286\n",
      "Epoch 2634, Loss: 0.00848601825418882, Final Batch Loss: 0.00537105044350028\n",
      "Epoch 2635, Loss: 0.012515154929133132, Final Batch Loss: 0.00033717506448738277\n",
      "Epoch 2636, Loss: 0.024562922248151153, Final Batch Loss: 0.0009966972284018993\n",
      "Epoch 2637, Loss: 0.05877088106353767, Final Batch Loss: 0.03468869999051094\n",
      "Epoch 2638, Loss: 0.04687167890369892, Final Batch Loss: 0.0018960318993777037\n",
      "Epoch 2639, Loss: 0.0074233292834833264, Final Batch Loss: 0.0014140154235064983\n",
      "Epoch 2640, Loss: 0.012880243331892416, Final Batch Loss: 0.0013270622584968805\n",
      "Epoch 2641, Loss: 0.024433786515146494, Final Batch Loss: 0.010033783502876759\n",
      "Epoch 2642, Loss: 0.0244780964567326, Final Batch Loss: 0.00010714022209867835\n",
      "Epoch 2643, Loss: 0.04106262751156464, Final Batch Loss: 0.0006479480070993304\n",
      "Epoch 2644, Loss: 0.01735567202558741, Final Batch Loss: 0.00495133688673377\n",
      "Epoch 2645, Loss: 0.021526856915443204, Final Batch Loss: 0.00017741885676514357\n",
      "Epoch 2646, Loss: 0.00576093205017969, Final Batch Loss: 0.0009687211713753641\n",
      "Epoch 2647, Loss: 0.03504656569566578, Final Batch Loss: 0.008680563420057297\n",
      "Epoch 2648, Loss: 0.010130832175491378, Final Batch Loss: 0.0012971673859283328\n",
      "Epoch 2649, Loss: 0.037856857874430716, Final Batch Loss: 0.0017482477705925703\n",
      "Epoch 2650, Loss: 0.016384988761274144, Final Batch Loss: 0.0030002454295754433\n",
      "Epoch 2651, Loss: 0.007701781811192632, Final Batch Loss: 0.0006653181626461446\n",
      "Epoch 2652, Loss: 0.038319022743962705, Final Batch Loss: 0.0187670961022377\n",
      "Epoch 2653, Loss: 0.013780022272840142, Final Batch Loss: 0.002325559500604868\n",
      "Epoch 2654, Loss: 0.0053831815894227475, Final Batch Loss: 0.0004909425624646246\n",
      "Epoch 2655, Loss: 0.0075356283923611045, Final Batch Loss: 0.0004222554271109402\n",
      "Epoch 2656, Loss: 0.01925039962225128, Final Batch Loss: 0.00012853370571974665\n",
      "Epoch 2657, Loss: 0.03237469174200669, Final Batch Loss: 0.005308339837938547\n",
      "Epoch 2658, Loss: 0.006918074213899672, Final Batch Loss: 0.004899554420262575\n",
      "Epoch 2659, Loss: 0.002494267057045363, Final Batch Loss: 0.0005450969911180437\n",
      "Epoch 2660, Loss: 0.006395726959453896, Final Batch Loss: 0.0005131464567966759\n",
      "Epoch 2661, Loss: 0.04116542200790718, Final Batch Loss: 0.0005496665253303945\n",
      "Epoch 2662, Loss: 0.012846521691244561, Final Batch Loss: 0.00010101209772983566\n",
      "Epoch 2663, Loss: 0.0651228260830976, Final Batch Loss: 0.052313003689050674\n",
      "Epoch 2664, Loss: 0.006655521923676133, Final Batch Loss: 0.0012881197035312653\n",
      "Epoch 2665, Loss: 0.0016608021542197093, Final Batch Loss: 8.609170618001372e-05\n",
      "Epoch 2666, Loss: 0.005982920614769682, Final Batch Loss: 0.00034624236286617815\n",
      "Epoch 2667, Loss: 0.017043782339897007, Final Batch Loss: 0.0006666192202828825\n",
      "Epoch 2668, Loss: 0.0468067713372875, Final Batch Loss: 0.04097473621368408\n",
      "Epoch 2669, Loss: 0.013768458447884768, Final Batch Loss: 0.0006475383997894824\n",
      "Epoch 2670, Loss: 0.004840919122216292, Final Batch Loss: 9.111432882491499e-05\n",
      "Epoch 2671, Loss: 0.004418888405780308, Final Batch Loss: 0.0030265578534454107\n",
      "Epoch 2672, Loss: 0.017532448793645017, Final Batch Loss: 0.0011179595021530986\n",
      "Epoch 2673, Loss: 0.007489253766834736, Final Batch Loss: 0.001170602161437273\n",
      "Epoch 2674, Loss: 0.05392257883795537, Final Batch Loss: 0.00394724914804101\n",
      "Epoch 2675, Loss: 0.0138823967135977, Final Batch Loss: 0.0014579114504158497\n",
      "Epoch 2676, Loss: 0.0276188560819719, Final Batch Loss: 0.0004762750759255141\n",
      "Epoch 2677, Loss: 0.017573194636497647, Final Batch Loss: 0.008290616795420647\n",
      "Epoch 2678, Loss: 0.013728294492466375, Final Batch Loss: 0.0007950672297738492\n",
      "Epoch 2679, Loss: 0.012049432611092925, Final Batch Loss: 0.001447145128622651\n",
      "Epoch 2680, Loss: 0.007113249797839671, Final Batch Loss: 0.000526385847479105\n",
      "Epoch 2681, Loss: 0.022347534759319387, Final Batch Loss: 0.00015482364688068628\n",
      "Epoch 2682, Loss: 0.04051229206379503, Final Batch Loss: 0.0008583085727877915\n",
      "Epoch 2683, Loss: 0.00552512270223815, Final Batch Loss: 0.00016542781668249518\n",
      "Epoch 2684, Loss: 0.0072614531381987035, Final Batch Loss: 0.004065284039825201\n",
      "Epoch 2685, Loss: 0.023252631654031575, Final Batch Loss: 0.0003385675372555852\n",
      "Epoch 2686, Loss: 0.009084908931981772, Final Batch Loss: 0.0067196497693657875\n",
      "Epoch 2687, Loss: 0.01916232424264308, Final Batch Loss: 0.0009835599921643734\n",
      "Epoch 2688, Loss: 0.011472789541585371, Final Batch Loss: 0.0002949260815512389\n",
      "Epoch 2689, Loss: 0.02548431378090754, Final Batch Loss: 0.005152919329702854\n",
      "Epoch 2690, Loss: 0.0044416709424695, Final Batch Loss: 0.0005436636274680495\n",
      "Epoch 2691, Loss: 0.04488747802679427, Final Batch Loss: 0.0021200256887823343\n",
      "Epoch 2692, Loss: 0.058286551968194544, Final Batch Loss: 0.05447966232895851\n",
      "Epoch 2693, Loss: 0.010669825423974544, Final Batch Loss: 0.000676735769957304\n",
      "Epoch 2694, Loss: 0.014793724927585572, Final Batch Loss: 0.000802739814389497\n",
      "Epoch 2695, Loss: 0.015699543117079884, Final Batch Loss: 0.004313858225941658\n",
      "Epoch 2696, Loss: 0.02133040502667427, Final Batch Loss: 0.0003306970465928316\n",
      "Epoch 2697, Loss: 0.010767173953354359, Final Batch Loss: 0.0037687341682612896\n",
      "Epoch 2698, Loss: 0.026610863103996962, Final Batch Loss: 0.011774265207350254\n",
      "Epoch 2699, Loss: 0.013582749277702533, Final Batch Loss: 0.0018931932281702757\n",
      "Epoch 2700, Loss: 0.011464537659776397, Final Batch Loss: 0.00034396423143334687\n",
      "Epoch 2701, Loss: 0.021028843126259744, Final Batch Loss: 0.006812132894992828\n",
      "Epoch 2702, Loss: 0.022439359250711277, Final Batch Loss: 0.0004684925661422312\n",
      "Epoch 2703, Loss: 0.012110853538615629, Final Batch Loss: 0.007794659119099379\n",
      "Epoch 2704, Loss: 0.011895056275534444, Final Batch Loss: 0.0004268791526556015\n",
      "Epoch 2705, Loss: 0.00826771579158958, Final Batch Loss: 0.00526377372443676\n",
      "Epoch 2706, Loss: 0.020702465786598623, Final Batch Loss: 0.001758313155733049\n",
      "Epoch 2707, Loss: 0.025948931724997237, Final Batch Loss: 0.0005111676291562617\n",
      "Epoch 2708, Loss: 0.0072695763083174825, Final Batch Loss: 0.002278987318277359\n",
      "Epoch 2709, Loss: 0.00934092397801578, Final Batch Loss: 0.005648410879075527\n",
      "Epoch 2710, Loss: 0.01622313234838657, Final Batch Loss: 0.0002854234480764717\n",
      "Epoch 2711, Loss: 0.06134657375514507, Final Batch Loss: 0.03238769248127937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2712, Loss: 0.007726473500952125, Final Batch Loss: 0.0009492650278843939\n",
      "Epoch 2713, Loss: 0.032517039217054844, Final Batch Loss: 0.022287633270025253\n",
      "Epoch 2714, Loss: 0.03007599408738315, Final Batch Loss: 0.0005094907246530056\n",
      "Epoch 2715, Loss: 0.03441373907844536, Final Batch Loss: 0.009355766698718071\n",
      "Epoch 2716, Loss: 0.005573861708398908, Final Batch Loss: 0.000642028811853379\n",
      "Epoch 2717, Loss: 0.0033286282559856772, Final Batch Loss: 0.0002445723512209952\n",
      "Epoch 2718, Loss: 0.03468780743423849, Final Batch Loss: 0.0004152560723014176\n",
      "Epoch 2719, Loss: 0.033608013938646764, Final Batch Loss: 0.024314135313034058\n",
      "Epoch 2720, Loss: 0.012607171142008156, Final Batch Loss: 0.0005523259751498699\n",
      "Epoch 2721, Loss: 0.007008235726971179, Final Batch Loss: 0.0008495553629472852\n",
      "Epoch 2722, Loss: 0.010183127829805017, Final Batch Loss: 0.0019312858348712325\n",
      "Epoch 2723, Loss: 0.004914755059871823, Final Batch Loss: 0.0010641987901180983\n",
      "Epoch 2724, Loss: 0.020888071187073365, Final Batch Loss: 0.007498858030885458\n",
      "Epoch 2725, Loss: 0.021426429244456813, Final Batch Loss: 0.00038501061499118805\n",
      "Epoch 2726, Loss: 0.08147551846923307, Final Batch Loss: 0.001295731053687632\n",
      "Epoch 2727, Loss: 0.05448463771608658, Final Batch Loss: 0.000892707786988467\n",
      "Epoch 2728, Loss: 0.005749185453169048, Final Batch Loss: 0.00023895935737527907\n",
      "Epoch 2729, Loss: 0.005138046544743702, Final Batch Loss: 0.000729244842659682\n",
      "Epoch 2730, Loss: 0.005151884397491813, Final Batch Loss: 0.0010652190539985895\n",
      "Epoch 2731, Loss: 0.0031540930503979325, Final Batch Loss: 0.0007106738048605621\n",
      "Epoch 2732, Loss: 0.041647595673566684, Final Batch Loss: 0.0009990112157538533\n",
      "Epoch 2733, Loss: 0.04489129921421409, Final Batch Loss: 0.003151277545839548\n",
      "Epoch 2734, Loss: 0.02535139606334269, Final Batch Loss: 0.002407195745036006\n",
      "Epoch 2735, Loss: 0.029197804658906534, Final Batch Loss: 0.0008986890316009521\n",
      "Epoch 2736, Loss: 0.005975966196274385, Final Batch Loss: 0.0008299463079310954\n",
      "Epoch 2737, Loss: 0.015271744108758867, Final Batch Loss: 0.0017454235348850489\n",
      "Epoch 2738, Loss: 0.020055652945302427, Final Batch Loss: 0.0015397354727610946\n",
      "Epoch 2739, Loss: 0.012644820410059765, Final Batch Loss: 0.0002162048767786473\n",
      "Epoch 2740, Loss: 0.019500120950397104, Final Batch Loss: 0.0007320900913327932\n",
      "Epoch 2741, Loss: 0.012867893790826201, Final Batch Loss: 0.002109362045302987\n",
      "Epoch 2742, Loss: 0.01979572931304574, Final Batch Loss: 0.0005778438062407076\n",
      "Epoch 2743, Loss: 0.010081303131300956, Final Batch Loss: 0.000348531233612448\n",
      "Epoch 2744, Loss: 0.008357430371688679, Final Batch Loss: 0.0004145021375734359\n",
      "Epoch 2745, Loss: 0.016572912863921374, Final Batch Loss: 0.0010540543589740992\n",
      "Epoch 2746, Loss: 0.01729772002727259, Final Batch Loss: 0.00018928608915302902\n",
      "Epoch 2747, Loss: 0.010816577007062733, Final Batch Loss: 0.007036099676042795\n",
      "Epoch 2748, Loss: 0.0063437149656238034, Final Batch Loss: 0.00015264212561305612\n",
      "Epoch 2749, Loss: 0.005824644278618507, Final Batch Loss: 0.0015112666878849268\n",
      "Epoch 2750, Loss: 0.004591586199239828, Final Batch Loss: 0.0005699171451851726\n",
      "Epoch 2751, Loss: 0.04123115881520789, Final Batch Loss: 0.00047285243635997176\n",
      "Epoch 2752, Loss: 0.010262139927363023, Final Batch Loss: 0.0009689684957265854\n",
      "Epoch 2753, Loss: 0.0038216495304368436, Final Batch Loss: 0.0004864631046075374\n",
      "Epoch 2754, Loss: 0.06308061769232154, Final Batch Loss: 0.002514870138838887\n",
      "Epoch 2755, Loss: 0.025001489702844992, Final Batch Loss: 0.02051926776766777\n",
      "Epoch 2756, Loss: 0.009135855303611606, Final Batch Loss: 0.0008836059714667499\n",
      "Epoch 2757, Loss: 0.0035835742455674335, Final Batch Loss: 0.0009325566352345049\n",
      "Epoch 2758, Loss: 0.009835333446972072, Final Batch Loss: 0.0016100088832899928\n",
      "Epoch 2759, Loss: 0.00478990247938782, Final Batch Loss: 0.0007705684984102845\n",
      "Epoch 2760, Loss: 0.021915793127845973, Final Batch Loss: 0.007981817238032818\n",
      "Epoch 2761, Loss: 0.015111127577256411, Final Batch Loss: 0.0067809149622917175\n",
      "Epoch 2762, Loss: 0.008577674248954281, Final Batch Loss: 0.000583824934437871\n",
      "Epoch 2763, Loss: 0.0052780655096285045, Final Batch Loss: 0.00028992866282351315\n",
      "Epoch 2764, Loss: 0.004553540056804195, Final Batch Loss: 0.00029186453321017325\n",
      "Epoch 2765, Loss: 0.030126916244626045, Final Batch Loss: 0.00024976395070552826\n",
      "Epoch 2766, Loss: 0.03330431912036147, Final Batch Loss: 0.00019827326468657702\n",
      "Epoch 2767, Loss: 0.02735783829120919, Final Batch Loss: 0.002210777485743165\n",
      "Epoch 2768, Loss: 0.02253022018703632, Final Batch Loss: 0.01979544572532177\n",
      "Epoch 2769, Loss: 0.00617412175051868, Final Batch Loss: 0.0016095831524580717\n",
      "Epoch 2770, Loss: 0.009275023126974702, Final Batch Loss: 0.0035470095463097095\n",
      "Epoch 2771, Loss: 0.039868440508143976, Final Batch Loss: 0.01588158868253231\n",
      "Epoch 2772, Loss: 0.025661954656243324, Final Batch Loss: 0.0037863533943891525\n",
      "Epoch 2773, Loss: 0.010276601475197822, Final Batch Loss: 0.0036011964548379183\n",
      "Epoch 2774, Loss: 0.07461653590144124, Final Batch Loss: 0.011508962139487267\n",
      "Epoch 2775, Loss: 0.01330431949463673, Final Batch Loss: 0.001249907654710114\n",
      "Epoch 2776, Loss: 0.005827933287946507, Final Batch Loss: 0.0008929854375310242\n",
      "Epoch 2777, Loss: 0.02100585273001343, Final Batch Loss: 0.002545130904763937\n",
      "Epoch 2778, Loss: 0.012452879396732897, Final Batch Loss: 0.0012501798337325454\n",
      "Epoch 2779, Loss: 0.015382335986942053, Final Batch Loss: 0.000999082112684846\n",
      "Epoch 2780, Loss: 0.032232139936240856, Final Batch Loss: 0.00485038198530674\n",
      "Epoch 2781, Loss: 0.013869099144358188, Final Batch Loss: 0.0028616287745535374\n",
      "Epoch 2782, Loss: 0.012770123372320086, Final Batch Loss: 0.005711800884455442\n",
      "Epoch 2783, Loss: 0.03140505816554651, Final Batch Loss: 0.0005227600922808051\n",
      "Epoch 2784, Loss: 0.06826920120511204, Final Batch Loss: 0.0012133294949308038\n",
      "Epoch 2785, Loss: 0.03135330480290577, Final Batch Loss: 0.000882533029653132\n",
      "Epoch 2786, Loss: 0.018966941657708958, Final Batch Loss: 0.00015764485578984022\n",
      "Epoch 2787, Loss: 0.012632013414986432, Final Batch Loss: 0.0005023439880460501\n",
      "Epoch 2788, Loss: 0.010589641984552145, Final Batch Loss: 0.00035998172825202346\n",
      "Epoch 2789, Loss: 0.041912854299880564, Final Batch Loss: 0.0011142812436446548\n",
      "Epoch 2790, Loss: 0.030691488296724856, Final Batch Loss: 0.0011918997624889016\n",
      "Epoch 2791, Loss: 0.028396572219207883, Final Batch Loss: 0.00325531093403697\n",
      "Epoch 2792, Loss: 0.009079144598217681, Final Batch Loss: 0.0005952795036137104\n",
      "Epoch 2793, Loss: 0.006024508038535714, Final Batch Loss: 0.0019335654797032475\n",
      "Epoch 2794, Loss: 0.019954292511101812, Final Batch Loss: 0.0006995878648012877\n",
      "Epoch 2795, Loss: 0.016550113417906687, Final Batch Loss: 0.00041745844646357\n",
      "Epoch 2796, Loss: 0.007660614908672869, Final Batch Loss: 0.0005936853121966124\n",
      "Epoch 2797, Loss: 0.016455806995509192, Final Batch Loss: 0.00014666913193650544\n",
      "Epoch 2798, Loss: 0.00835205102339387, Final Batch Loss: 0.0012663531815633178\n",
      "Epoch 2799, Loss: 0.013227079587522894, Final Batch Loss: 0.0005003410042263567\n",
      "Epoch 2800, Loss: 0.025165769417071715, Final Batch Loss: 0.000496502558235079\n",
      "Epoch 2801, Loss: 0.003191259893355891, Final Batch Loss: 0.0005025442806072533\n",
      "Epoch 2802, Loss: 0.010578533823718317, Final Batch Loss: 0.0006104502244852483\n",
      "Epoch 2803, Loss: 0.05912244162755087, Final Batch Loss: 0.015465025790035725\n",
      "Epoch 2804, Loss: 0.009982436313293874, Final Batch Loss: 0.0018919926369562745\n",
      "Epoch 2805, Loss: 0.029984046035679057, Final Batch Loss: 0.0004136672941967845\n",
      "Epoch 2806, Loss: 0.022275890631135553, Final Batch Loss: 0.017415158450603485\n",
      "Epoch 2807, Loss: 0.004892846802249551, Final Batch Loss: 0.0005107186734676361\n",
      "Epoch 2808, Loss: 0.015489744488149881, Final Batch Loss: 0.0036614793352782726\n",
      "Epoch 2809, Loss: 0.00613869316293858, Final Batch Loss: 0.002519875066354871\n",
      "Epoch 2810, Loss: 0.015616511693224311, Final Batch Loss: 0.0012626985553652048\n",
      "Epoch 2811, Loss: 0.011787336145062, Final Batch Loss: 0.0006145958323031664\n",
      "Epoch 2812, Loss: 0.020989933254895732, Final Batch Loss: 0.003939805086702108\n",
      "Epoch 2813, Loss: 0.05893819103948772, Final Batch Loss: 0.0007714617531746626\n",
      "Epoch 2814, Loss: 0.008161876117810607, Final Batch Loss: 0.002471307758241892\n",
      "Epoch 2815, Loss: 0.0053509077115450054, Final Batch Loss: 0.0013932809233665466\n",
      "Epoch 2816, Loss: 0.03138361359015107, Final Batch Loss: 0.017622198909521103\n",
      "Epoch 2817, Loss: 0.004178256320301443, Final Batch Loss: 0.0004542878596112132\n",
      "Epoch 2818, Loss: 0.0060629964573308825, Final Batch Loss: 0.0014969768235459924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2819, Loss: 0.01984782630461268, Final Batch Loss: 0.0005214775446802378\n",
      "Epoch 2820, Loss: 0.010455338138854131, Final Batch Loss: 0.0012822181452065706\n",
      "Epoch 2821, Loss: 0.012777945172274485, Final Batch Loss: 0.0004923630622215569\n",
      "Epoch 2822, Loss: 0.014109310257481411, Final Batch Loss: 0.0016484590014442801\n",
      "Epoch 2823, Loss: 0.022173804027261212, Final Batch Loss: 0.004368911497294903\n",
      "Epoch 2824, Loss: 0.02408952929545194, Final Batch Loss: 0.0010347784264013171\n",
      "Epoch 2825, Loss: 0.021954055526293814, Final Batch Loss: 0.004323623143136501\n",
      "Epoch 2826, Loss: 0.0032926460553426296, Final Batch Loss: 0.000595242134295404\n",
      "Epoch 2827, Loss: 0.008331715274835005, Final Batch Loss: 0.00048179415171034634\n",
      "Epoch 2828, Loss: 0.007742581481579691, Final Batch Loss: 0.0010488276602700353\n",
      "Epoch 2829, Loss: 0.048340647204895504, Final Batch Loss: 0.03665517270565033\n",
      "Epoch 2830, Loss: 0.011088868996012025, Final Batch Loss: 0.00336294318549335\n",
      "Epoch 2831, Loss: 0.01865541868028231, Final Batch Loss: 0.0014592220541089773\n",
      "Epoch 2832, Loss: 0.04248398920753971, Final Batch Loss: 0.003896671812981367\n",
      "Epoch 2833, Loss: 0.03073367429897189, Final Batch Loss: 0.0009834365919232368\n",
      "Epoch 2834, Loss: 0.026891989109572023, Final Batch Loss: 0.0010087407426908612\n",
      "Epoch 2835, Loss: 0.01089704514015466, Final Batch Loss: 0.003331341315060854\n",
      "Epoch 2836, Loss: 0.019460692594293505, Final Batch Loss: 0.007116662338376045\n",
      "Epoch 2837, Loss: 0.03415958391269669, Final Batch Loss: 0.0014089848846197128\n",
      "Epoch 2838, Loss: 0.01195651659509167, Final Batch Loss: 0.000775400607381016\n",
      "Epoch 2839, Loss: 0.016136435442604125, Final Batch Loss: 0.009438006207346916\n",
      "Epoch 2840, Loss: 0.037959610170219094, Final Batch Loss: 0.02661154791712761\n",
      "Epoch 2841, Loss: 0.011278621444944292, Final Batch Loss: 0.003952061757445335\n",
      "Epoch 2842, Loss: 0.021898366569075733, Final Batch Loss: 0.013035702519118786\n",
      "Epoch 2843, Loss: 0.007606148355989717, Final Batch Loss: 0.0014243681216612458\n",
      "Epoch 2844, Loss: 0.05840967386029661, Final Batch Loss: 0.0030603790655732155\n",
      "Epoch 2845, Loss: 0.029582132876385003, Final Batch Loss: 0.02526228502392769\n",
      "Epoch 2846, Loss: 0.03318437025882304, Final Batch Loss: 0.0004127735737711191\n",
      "Epoch 2847, Loss: 0.04764650890138, Final Batch Loss: 0.009921307675540447\n",
      "Epoch 2848, Loss: 0.007312607456697151, Final Batch Loss: 0.001005882048048079\n",
      "Epoch 2849, Loss: 0.038734581554308534, Final Batch Loss: 0.019525635987520218\n",
      "Epoch 2850, Loss: 0.0119161163456738, Final Batch Loss: 0.0004105283005628735\n",
      "Epoch 2851, Loss: 0.02870927624462638, Final Batch Loss: 0.008402246981859207\n",
      "Epoch 2852, Loss: 0.022183986846357584, Final Batch Loss: 0.005718203727155924\n",
      "Epoch 2853, Loss: 0.012215089751407504, Final Batch Loss: 0.003499211510643363\n",
      "Epoch 2854, Loss: 0.017886298708617687, Final Batch Loss: 0.0008646567584946752\n",
      "Epoch 2855, Loss: 0.017602164210984483, Final Batch Loss: 0.0006024205940775573\n",
      "Epoch 2856, Loss: 0.015849492163397372, Final Batch Loss: 0.001330364728346467\n",
      "Epoch 2857, Loss: 0.06170976551948115, Final Batch Loss: 0.0372975617647171\n",
      "Epoch 2858, Loss: 0.009976151312002912, Final Batch Loss: 0.0027278338093310595\n",
      "Epoch 2859, Loss: 0.0033738888159859926, Final Batch Loss: 0.00033518558484502137\n",
      "Epoch 2860, Loss: 0.015591669274726883, Final Batch Loss: 0.00702701136469841\n",
      "Epoch 2861, Loss: 0.02209567608224461, Final Batch Loss: 0.019446270540356636\n",
      "Epoch 2862, Loss: 0.007443571550538763, Final Batch Loss: 0.0012934469850733876\n",
      "Epoch 2863, Loss: 0.0037619693466695026, Final Batch Loss: 0.0005647859652526677\n",
      "Epoch 2864, Loss: 0.004832626334973611, Final Batch Loss: 0.0007778368890285492\n",
      "Epoch 2865, Loss: 0.011254105513216928, Final Batch Loss: 0.0019211835460737348\n",
      "Epoch 2866, Loss: 0.028714216750813648, Final Batch Loss: 0.004896956495940685\n",
      "Epoch 2867, Loss: 0.0014079546017455868, Final Batch Loss: 0.00020304499776102602\n",
      "Epoch 2868, Loss: 0.010325452836696059, Final Batch Loss: 0.0005322085926309228\n",
      "Epoch 2869, Loss: 0.005412165861343965, Final Batch Loss: 0.0005736143211834133\n",
      "Epoch 2870, Loss: 0.015618294826708734, Final Batch Loss: 0.0009911925299093127\n",
      "Epoch 2871, Loss: 0.01790750394866336, Final Batch Loss: 0.0002156530536012724\n",
      "Epoch 2872, Loss: 0.00242522350163199, Final Batch Loss: 0.0007613350171595812\n",
      "Epoch 2873, Loss: 0.0212297838224913, Final Batch Loss: 0.004424696322530508\n",
      "Epoch 2874, Loss: 0.01059132203226909, Final Batch Loss: 0.00011635878763627261\n",
      "Epoch 2875, Loss: 0.043541049846680835, Final Batch Loss: 0.023625100031495094\n",
      "Epoch 2876, Loss: 0.037457776386872865, Final Batch Loss: 0.024135710671544075\n",
      "Epoch 2877, Loss: 0.029017842702160124, Final Batch Loss: 0.00025049896794371307\n",
      "Epoch 2878, Loss: 0.03994705015793443, Final Batch Loss: 0.004946460016071796\n",
      "Epoch 2879, Loss: 0.006397131131961942, Final Batch Loss: 0.0015783518319949508\n",
      "Epoch 2880, Loss: 0.005835405783727765, Final Batch Loss: 0.0009981089970096946\n",
      "Epoch 2881, Loss: 0.007311250898055732, Final Batch Loss: 0.0009985832730308175\n",
      "Epoch 2882, Loss: 0.018592950989841484, Final Batch Loss: 0.0009706421988084912\n",
      "Epoch 2883, Loss: 0.020416233310243115, Final Batch Loss: 0.011735010892152786\n",
      "Epoch 2884, Loss: 0.00370019645197317, Final Batch Loss: 0.0009391632047481835\n",
      "Epoch 2885, Loss: 0.010938997256744187, Final Batch Loss: 0.001646772725507617\n",
      "Epoch 2886, Loss: 0.010314618513802998, Final Batch Loss: 0.00045155358384363353\n",
      "Epoch 2887, Loss: 0.02582088956842199, Final Batch Loss: 0.00011466810246929526\n",
      "Epoch 2888, Loss: 0.022438829153543338, Final Batch Loss: 0.00010179131641052663\n",
      "Epoch 2889, Loss: 0.009410635801032186, Final Batch Loss: 0.001267742831259966\n",
      "Epoch 2890, Loss: 0.007430972560541704, Final Batch Loss: 0.0011555927339941263\n",
      "Epoch 2891, Loss: 0.013469818513840437, Final Batch Loss: 0.0012985736830160022\n",
      "Epoch 2892, Loss: 0.030954917972849216, Final Batch Loss: 0.00010300717985955998\n",
      "Epoch 2893, Loss: 0.025365259178215638, Final Batch Loss: 0.003203758504241705\n",
      "Epoch 2894, Loss: 0.030527092923875898, Final Batch Loss: 0.006536678411066532\n",
      "Epoch 2895, Loss: 0.03965719144616742, Final Batch Loss: 0.010338851250708103\n",
      "Epoch 2896, Loss: 0.018687762727495283, Final Batch Loss: 0.012050537392497063\n",
      "Epoch 2897, Loss: 0.005884460872039199, Final Batch Loss: 0.0004405936924740672\n",
      "Epoch 2898, Loss: 0.048224417318124324, Final Batch Loss: 0.03125700354576111\n",
      "Epoch 2899, Loss: 0.03258527858997695, Final Batch Loss: 0.00016568764112889767\n",
      "Epoch 2900, Loss: 0.006135432922746986, Final Batch Loss: 0.0023684115149080753\n",
      "Epoch 2901, Loss: 0.008117219374980778, Final Batch Loss: 0.0020558468531817198\n",
      "Epoch 2902, Loss: 0.020658846100559458, Final Batch Loss: 0.0015932972310110927\n",
      "Epoch 2903, Loss: 0.014099148567765951, Final Batch Loss: 0.00015998753951862454\n",
      "Epoch 2904, Loss: 0.004733551206300035, Final Batch Loss: 0.0004363332409411669\n",
      "Epoch 2905, Loss: 0.027068555442383513, Final Batch Loss: 0.0005041888798587024\n",
      "Epoch 2906, Loss: 0.017498055720352568, Final Batch Loss: 0.0001835926523199305\n",
      "Epoch 2907, Loss: 0.0040867115603759885, Final Batch Loss: 0.0005631641251966357\n",
      "Epoch 2908, Loss: 0.02891729390830733, Final Batch Loss: 0.006394665222615004\n",
      "Epoch 2909, Loss: 0.008231829458964057, Final Batch Loss: 0.0005775042809545994\n",
      "Epoch 2910, Loss: 0.02314286754699424, Final Batch Loss: 0.020588580518960953\n",
      "Epoch 2911, Loss: 0.0077627785940421745, Final Batch Loss: 0.0029225177131593227\n",
      "Epoch 2912, Loss: 0.010851708735572174, Final Batch Loss: 0.00046981876948848367\n",
      "Epoch 2913, Loss: 0.007373891698080115, Final Batch Loss: 0.0002384061663178727\n",
      "Epoch 2914, Loss: 0.012461101752705872, Final Batch Loss: 0.0003427756018936634\n",
      "Epoch 2915, Loss: 0.00768011633772403, Final Batch Loss: 0.0006497513386420906\n",
      "Epoch 2916, Loss: 0.035946256131865084, Final Batch Loss: 0.0012167453533038497\n",
      "Epoch 2917, Loss: 0.020860892807831988, Final Batch Loss: 0.0017231206875294447\n",
      "Epoch 2918, Loss: 0.0027764880505856127, Final Batch Loss: 0.0006675198092125356\n",
      "Epoch 2919, Loss: 0.036797028937144205, Final Batch Loss: 0.0003329637984279543\n",
      "Epoch 2920, Loss: 0.032015304212109186, Final Batch Loss: 0.02702828124165535\n",
      "Epoch 2921, Loss: 0.020007870174595155, Final Batch Loss: 0.0007710506324656308\n",
      "Epoch 2922, Loss: 0.03663528279867023, Final Batch Loss: 0.002277662744745612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2923, Loss: 0.010111769428476691, Final Batch Loss: 0.005680781789124012\n",
      "Epoch 2924, Loss: 0.029605313553474844, Final Batch Loss: 0.0004733990062959492\n",
      "Epoch 2925, Loss: 0.04039821866899729, Final Batch Loss: 0.0006033374229446054\n",
      "Epoch 2926, Loss: 0.0030025018932064995, Final Batch Loss: 0.0007110841106623411\n",
      "Epoch 2927, Loss: 0.08846474438905716, Final Batch Loss: 0.002368429210036993\n",
      "Epoch 2928, Loss: 0.04111044993624091, Final Batch Loss: 0.0023532966151833534\n",
      "Epoch 2929, Loss: 0.020776944467797875, Final Batch Loss: 0.010635236278176308\n",
      "Epoch 2930, Loss: 0.005397063490818255, Final Batch Loss: 0.00019846843497361988\n",
      "Epoch 2931, Loss: 0.038540017674677074, Final Batch Loss: 0.0014650792581960559\n",
      "Epoch 2932, Loss: 0.04023128445260227, Final Batch Loss: 0.0025251114275306463\n",
      "Epoch 2933, Loss: 0.012416425975970924, Final Batch Loss: 0.0019489090191200376\n",
      "Epoch 2934, Loss: 0.013221869681729004, Final Batch Loss: 0.0028800664003938437\n",
      "Epoch 2935, Loss: 0.010876961809117347, Final Batch Loss: 0.000839643704239279\n",
      "Epoch 2936, Loss: 0.04352612962247804, Final Batch Loss: 0.011057069525122643\n",
      "Epoch 2937, Loss: 0.005625829478958622, Final Batch Loss: 0.00029131563496775925\n",
      "Epoch 2938, Loss: 0.01608338406367693, Final Batch Loss: 0.01208037231117487\n",
      "Epoch 2939, Loss: 0.006631943571846932, Final Batch Loss: 0.004709672648459673\n",
      "Epoch 2940, Loss: 0.008969189162598923, Final Batch Loss: 0.00026543682906776667\n",
      "Epoch 2941, Loss: 0.028441335671232082, Final Batch Loss: 0.003968372475355864\n",
      "Epoch 2942, Loss: 0.0075070306193083525, Final Batch Loss: 0.0022241987753659487\n",
      "Epoch 2943, Loss: 0.010385817236965522, Final Batch Loss: 0.00024029199266806245\n",
      "Epoch 2944, Loss: 0.026298339798813686, Final Batch Loss: 0.00031543258228339255\n",
      "Epoch 2945, Loss: 0.004839147732127458, Final Batch Loss: 0.0010183998383581638\n",
      "Epoch 2946, Loss: 0.012349520868156105, Final Batch Loss: 0.002461998490616679\n",
      "Epoch 2947, Loss: 0.015659894095733762, Final Batch Loss: 0.002424913924187422\n",
      "Epoch 2948, Loss: 0.005833415416418575, Final Batch Loss: 0.0006348075694404542\n",
      "Epoch 2949, Loss: 0.00840907238307409, Final Batch Loss: 0.0009021025616675615\n",
      "Epoch 2950, Loss: 0.03469843347556889, Final Batch Loss: 0.0013879771577194333\n",
      "Epoch 2951, Loss: 0.0539072067476809, Final Batch Loss: 0.0069987052120268345\n",
      "Epoch 2952, Loss: 0.00935231288895011, Final Batch Loss: 0.0008014768827706575\n",
      "Epoch 2953, Loss: 0.04044011305086315, Final Batch Loss: 0.0009759730892255902\n",
      "Epoch 2954, Loss: 0.010611424804665148, Final Batch Loss: 0.00044555883505381644\n",
      "Epoch 2955, Loss: 0.010811318788910285, Final Batch Loss: 0.0008502131677232683\n",
      "Epoch 2956, Loss: 0.009908522246405482, Final Batch Loss: 0.0004648866015486419\n",
      "Epoch 2957, Loss: 0.010307889606337994, Final Batch Loss: 0.000899719656445086\n",
      "Epoch 2958, Loss: 0.006549850106239319, Final Batch Loss: 0.0011179758002981544\n",
      "Epoch 2959, Loss: 0.041611346969148144, Final Batch Loss: 0.0006598016479983926\n",
      "Epoch 2960, Loss: 0.007810519717168063, Final Batch Loss: 0.002526035765185952\n",
      "Epoch 2961, Loss: 0.007123817049432546, Final Batch Loss: 0.0013610473833978176\n",
      "Epoch 2962, Loss: 0.010761875368189067, Final Batch Loss: 0.0011817114427685738\n",
      "Epoch 2963, Loss: 0.01662793435389176, Final Batch Loss: 0.00036218174500390887\n",
      "Epoch 2964, Loss: 0.004914879551506601, Final Batch Loss: 0.00019547999545466155\n",
      "Epoch 2965, Loss: 0.009132997773122042, Final Batch Loss: 0.0002796029730234295\n",
      "Epoch 2966, Loss: 0.01222311265883036, Final Batch Loss: 0.0006165650556795299\n",
      "Epoch 2967, Loss: 0.005391936516389251, Final Batch Loss: 0.0008786302641965449\n",
      "Epoch 2968, Loss: 0.003115480831183959, Final Batch Loss: 0.0005401430535130203\n",
      "Epoch 2969, Loss: 0.012211027118610218, Final Batch Loss: 0.0013839075108990073\n",
      "Epoch 2970, Loss: 0.009406490040419158, Final Batch Loss: 0.0044087134301662445\n",
      "Epoch 2971, Loss: 0.00514925442985259, Final Batch Loss: 0.0009288263390772045\n",
      "Epoch 2972, Loss: 0.013334737217519432, Final Batch Loss: 0.010468740016222\n",
      "Epoch 2973, Loss: 0.002989849373989273, Final Batch Loss: 0.00010645567817846313\n",
      "Epoch 2974, Loss: 0.00762307527475059, Final Batch Loss: 0.0011407769052311778\n",
      "Epoch 2975, Loss: 0.00339767373225186, Final Batch Loss: 0.0010299397399649024\n",
      "Epoch 2976, Loss: 0.004437692885403521, Final Batch Loss: 0.000469269638415426\n",
      "Epoch 2977, Loss: 0.006014292914187536, Final Batch Loss: 0.0014523884747177362\n",
      "Epoch 2978, Loss: 0.050260919641004875, Final Batch Loss: 0.000518390501383692\n",
      "Epoch 2979, Loss: 0.11862500503775664, Final Batch Loss: 0.0901656374335289\n",
      "Epoch 2980, Loss: 0.018197916288045235, Final Batch Loss: 0.01141250692307949\n",
      "Epoch 2981, Loss: 0.0343308280716883, Final Batch Loss: 0.00022177300706971437\n",
      "Epoch 2982, Loss: 0.02029244964069221, Final Batch Loss: 0.014217071235179901\n",
      "Epoch 2983, Loss: 0.011497440631501377, Final Batch Loss: 0.0005096831009723246\n",
      "Epoch 2984, Loss: 0.05687458242755383, Final Batch Loss: 0.023079631850123405\n",
      "Epoch 2985, Loss: 0.047625276347389445, Final Batch Loss: 0.0001956990163307637\n",
      "Epoch 2986, Loss: 0.009922355704475194, Final Batch Loss: 0.00040278417873196304\n",
      "Epoch 2987, Loss: 0.016601332055870444, Final Batch Loss: 0.004588386509567499\n",
      "Epoch 2988, Loss: 0.022436030791141093, Final Batch Loss: 0.004170015454292297\n",
      "Epoch 2989, Loss: 0.0021924391767242923, Final Batch Loss: 0.00020099800894968212\n",
      "Epoch 2990, Loss: 0.01336371191428043, Final Batch Loss: 0.00693220691755414\n",
      "Epoch 2991, Loss: 0.019823685550363734, Final Batch Loss: 0.0003125723742414266\n",
      "Epoch 2992, Loss: 0.010413287411211058, Final Batch Loss: 0.0003702843387145549\n",
      "Epoch 2993, Loss: 0.012067902134731412, Final Batch Loss: 0.0005747871473431587\n",
      "Epoch 2994, Loss: 0.012703782893368043, Final Batch Loss: 0.010238628834486008\n",
      "Epoch 2995, Loss: 0.015982866869308054, Final Batch Loss: 0.0007841979968361557\n",
      "Epoch 2996, Loss: 0.03478374588303268, Final Batch Loss: 0.027845533564686775\n",
      "Epoch 2997, Loss: 0.0694685016060248, Final Batch Loss: 0.032208822667598724\n",
      "Epoch 2998, Loss: 0.012009800135274418, Final Batch Loss: 0.0016414489364251494\n",
      "Epoch 2999, Loss: 0.012516161834355444, Final Batch Loss: 0.0035973135381937027\n",
      "Epoch 3000, Loss: 0.008485084224957973, Final Batch Loss: 0.0007068373379297554\n",
      "Epoch 3001, Loss: 0.015441321942489594, Final Batch Loss: 0.0014656834537163377\n",
      "Epoch 3002, Loss: 0.0171420719998423, Final Batch Loss: 0.001274713547900319\n",
      "Epoch 3003, Loss: 0.019138102332362905, Final Batch Loss: 0.0003736282524187118\n",
      "Epoch 3004, Loss: 0.01576270640362054, Final Batch Loss: 0.006609828677028418\n",
      "Epoch 3005, Loss: 0.011560373604879715, Final Batch Loss: 0.003924495540559292\n",
      "Epoch 3006, Loss: 0.010342111461795866, Final Batch Loss: 0.0005072207422927022\n",
      "Epoch 3007, Loss: 0.0142114037880674, Final Batch Loss: 0.0010621527908369899\n",
      "Epoch 3008, Loss: 0.008531602506991476, Final Batch Loss: 0.0006318642408587039\n",
      "Epoch 3009, Loss: 0.012147541594458744, Final Batch Loss: 0.0007640906260348856\n",
      "Epoch 3010, Loss: 0.0027810600149678066, Final Batch Loss: 0.00024394366482738405\n",
      "Epoch 3011, Loss: 0.018110706441802904, Final Batch Loss: 0.0005046146106906235\n",
      "Epoch 3012, Loss: 0.0056361866008955985, Final Batch Loss: 0.003102268325164914\n",
      "Epoch 3013, Loss: 0.00663760703173466, Final Batch Loss: 0.00041094704647548497\n",
      "Epoch 3014, Loss: 0.00659310063929297, Final Batch Loss: 0.003573274239897728\n",
      "Epoch 3015, Loss: 0.0047214466030709445, Final Batch Loss: 0.00045440514804795384\n",
      "Epoch 3016, Loss: 0.019641933031380177, Final Batch Loss: 0.009676886722445488\n",
      "Epoch 3017, Loss: 0.03018880751915276, Final Batch Loss: 0.0018155360594391823\n",
      "Epoch 3018, Loss: 0.016348648066923488, Final Batch Loss: 0.0006291624740697443\n",
      "Epoch 3019, Loss: 0.021350442722905427, Final Batch Loss: 0.0019896216690540314\n",
      "Epoch 3020, Loss: 0.005825435524457134, Final Batch Loss: 0.0008719228790141642\n",
      "Epoch 3021, Loss: 0.01960321021033451, Final Batch Loss: 0.001249362831003964\n",
      "Epoch 3022, Loss: 0.021684499690309167, Final Batch Loss: 0.00110558420419693\n",
      "Epoch 3023, Loss: 0.01535122375935316, Final Batch Loss: 0.003999910783022642\n",
      "Epoch 3024, Loss: 0.036715923459269106, Final Batch Loss: 0.0010270902421325445\n",
      "Epoch 3025, Loss: 0.03732756635872647, Final Batch Loss: 0.017773432657122612\n",
      "Epoch 3026, Loss: 0.02409919718047604, Final Batch Loss: 0.000980014679953456\n",
      "Epoch 3027, Loss: 0.035592771018855274, Final Batch Loss: 0.009910086169838905\n",
      "Epoch 3028, Loss: 0.013522073946660385, Final Batch Loss: 0.004002284724265337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3029, Loss: 0.012084589863661677, Final Batch Loss: 0.0008723635692149401\n",
      "Epoch 3030, Loss: 0.006936733407201245, Final Batch Loss: 0.00047378550516441464\n",
      "Epoch 3031, Loss: 0.013976156827993691, Final Batch Loss: 0.0027031556237488985\n",
      "Epoch 3032, Loss: 0.03456451342208311, Final Batch Loss: 0.000987102510407567\n",
      "Epoch 3033, Loss: 0.009565478103468195, Final Batch Loss: 0.0016545006074011326\n",
      "Epoch 3034, Loss: 0.0156294631306082, Final Batch Loss: 0.0012726534623652697\n",
      "Epoch 3035, Loss: 0.005340055955457501, Final Batch Loss: 0.0020767604000866413\n",
      "Epoch 3036, Loss: 0.01319131557829678, Final Batch Loss: 0.003525644773617387\n",
      "Epoch 3037, Loss: 0.02013239386724308, Final Batch Loss: 0.004020079970359802\n",
      "Epoch 3038, Loss: 0.0035684548201970756, Final Batch Loss: 0.0015439034905284643\n",
      "Epoch 3039, Loss: 0.012171654147095978, Final Batch Loss: 0.0032086079008877277\n",
      "Epoch 3040, Loss: 0.007556897588074207, Final Batch Loss: 0.0013182290131226182\n",
      "Epoch 3041, Loss: 0.014049781952053308, Final Batch Loss: 0.0009343168349005282\n",
      "Epoch 3042, Loss: 0.002845943352440372, Final Batch Loss: 0.00037700484972447157\n",
      "Epoch 3043, Loss: 0.0036716745235025883, Final Batch Loss: 0.0006708510336466134\n",
      "Epoch 3044, Loss: 0.020032254775287583, Final Batch Loss: 0.0003705251729115844\n",
      "Epoch 3045, Loss: 0.032812794830533676, Final Batch Loss: 0.0138536486774683\n",
      "Epoch 3046, Loss: 0.008419705263804644, Final Batch Loss: 0.0015895183896645904\n",
      "Epoch 3047, Loss: 0.03525169403292239, Final Batch Loss: 0.001185092143714428\n",
      "Epoch 3048, Loss: 0.01621347409673035, Final Batch Loss: 0.0017408364219591022\n",
      "Epoch 3049, Loss: 0.0054198987782001495, Final Batch Loss: 0.00101128697860986\n",
      "Epoch 3050, Loss: 0.02978069384698756, Final Batch Loss: 0.0004015838203486055\n",
      "Epoch 3051, Loss: 0.03261603129794821, Final Batch Loss: 0.01180514506995678\n",
      "Epoch 3052, Loss: 0.04729124474397395, Final Batch Loss: 0.000394254777347669\n",
      "Epoch 3053, Loss: 0.003183764696586877, Final Batch Loss: 0.0004264328454155475\n",
      "Epoch 3054, Loss: 0.022400207177270204, Final Batch Loss: 0.0004937531775794923\n",
      "Epoch 3055, Loss: 0.015386055049020797, Final Batch Loss: 0.0009075812413357198\n",
      "Epoch 3056, Loss: 0.016552733723074198, Final Batch Loss: 0.0012733289040625095\n",
      "Epoch 3057, Loss: 0.007310684930416755, Final Batch Loss: 0.00033388237352482975\n",
      "Epoch 3058, Loss: 0.010476909374119714, Final Batch Loss: 0.0030263219960033894\n",
      "Epoch 3059, Loss: 0.025908800365868956, Final Batch Loss: 0.006130166817456484\n",
      "Epoch 3060, Loss: 0.011833104770630598, Final Batch Loss: 0.0025936956517398357\n",
      "Epoch 3061, Loss: 0.024685042502824217, Final Batch Loss: 0.0007178863743320107\n",
      "Epoch 3062, Loss: 0.006020842818543315, Final Batch Loss: 0.0002327112597413361\n",
      "Epoch 3063, Loss: 0.01545662636635825, Final Batch Loss: 0.0005325895035639405\n",
      "Epoch 3064, Loss: 0.0036616148572647944, Final Batch Loss: 0.00028694578213617206\n",
      "Epoch 3065, Loss: 0.005475546233355999, Final Batch Loss: 0.002257152460515499\n",
      "Epoch 3066, Loss: 0.0210865612316411, Final Batch Loss: 0.00013657944509759545\n",
      "Epoch 3067, Loss: 0.04716738598654047, Final Batch Loss: 0.0007964091491885483\n",
      "Epoch 3068, Loss: 0.010834650718607008, Final Batch Loss: 0.0007793688564561307\n",
      "Epoch 3069, Loss: 0.019224325369577855, Final Batch Loss: 0.0009780729888007045\n",
      "Epoch 3070, Loss: 0.032317578967195004, Final Batch Loss: 0.000765087956096977\n",
      "Epoch 3071, Loss: 0.028577634453540668, Final Batch Loss: 0.005104349926114082\n",
      "Epoch 3072, Loss: 0.03516040794784203, Final Batch Loss: 0.005068142898380756\n",
      "Epoch 3073, Loss: 0.03379422557190992, Final Batch Loss: 0.005537351593375206\n",
      "Epoch 3074, Loss: 0.046301278547616675, Final Batch Loss: 0.00047374379937537014\n",
      "Epoch 3075, Loss: 0.07295407966012135, Final Batch Loss: 0.0473022498190403\n",
      "Epoch 3076, Loss: 0.012621736619621515, Final Batch Loss: 0.0011354011949151754\n",
      "Epoch 3077, Loss: 0.02039605559548363, Final Batch Loss: 0.0031890319660305977\n",
      "Epoch 3078, Loss: 0.012011570914182812, Final Batch Loss: 0.0006754868663847446\n",
      "Epoch 3079, Loss: 0.015315330994781107, Final Batch Loss: 0.0008074416546151042\n",
      "Epoch 3080, Loss: 0.012753447139402851, Final Batch Loss: 0.00475117564201355\n",
      "Epoch 3081, Loss: 0.04344679741188884, Final Batch Loss: 0.013057270087301731\n",
      "Epoch 3082, Loss: 0.018672330916160718, Final Batch Loss: 0.0006675865733996034\n",
      "Epoch 3083, Loss: 0.057773138280026615, Final Batch Loss: 0.003921092953532934\n",
      "Epoch 3084, Loss: 0.019201121060177684, Final Batch Loss: 0.006932376883924007\n",
      "Epoch 3085, Loss: 0.03792669373797253, Final Batch Loss: 0.02799532748758793\n",
      "Epoch 3086, Loss: 0.039506176428403705, Final Batch Loss: 0.0025030956603586674\n",
      "Epoch 3087, Loss: 0.02256701816804707, Final Batch Loss: 0.004966154228895903\n",
      "Epoch 3088, Loss: 0.011727449193131179, Final Batch Loss: 0.007123970426619053\n",
      "Epoch 3089, Loss: 0.00889916461892426, Final Batch Loss: 0.0027637386228889227\n",
      "Epoch 3090, Loss: 0.025879938737489283, Final Batch Loss: 0.004737318493425846\n",
      "Epoch 3091, Loss: 0.00794068886898458, Final Batch Loss: 0.004000816959887743\n",
      "Epoch 3092, Loss: 0.029896165593527257, Final Batch Loss: 0.0008439406519755721\n",
      "Epoch 3093, Loss: 0.02495258059934713, Final Batch Loss: 0.0023151743225753307\n",
      "Epoch 3094, Loss: 0.015274505858542398, Final Batch Loss: 0.0004606792062986642\n",
      "Epoch 3095, Loss: 0.024177284038159996, Final Batch Loss: 0.00025137682678177953\n",
      "Epoch 3096, Loss: 0.01593481353484094, Final Batch Loss: 0.0029072528705000877\n",
      "Epoch 3097, Loss: 0.01811117972829379, Final Batch Loss: 0.004224006552249193\n",
      "Epoch 3098, Loss: 0.04050843893492129, Final Batch Loss: 0.00010597133950795978\n",
      "Epoch 3099, Loss: 0.012011287966743112, Final Batch Loss: 0.0005965091404505074\n",
      "Epoch 3100, Loss: 0.012678605853579938, Final Batch Loss: 0.0008098444086499512\n",
      "Epoch 3101, Loss: 0.029571485240012407, Final Batch Loss: 0.0005008733714930713\n",
      "Epoch 3102, Loss: 0.026103770593181252, Final Batch Loss: 0.0017612316878512502\n",
      "Epoch 3103, Loss: 0.010296552674844861, Final Batch Loss: 0.0005591644439846277\n",
      "Epoch 3104, Loss: 0.006050008407328278, Final Batch Loss: 0.001978512154892087\n",
      "Epoch 3105, Loss: 0.022833514638477936, Final Batch Loss: 0.0010700387647375464\n",
      "Epoch 3106, Loss: 0.013658089039381593, Final Batch Loss: 0.007806397043168545\n",
      "Epoch 3107, Loss: 0.011482072994112968, Final Batch Loss: 0.003500335616990924\n",
      "Epoch 3108, Loss: 0.03375696745933965, Final Batch Loss: 0.019619377329945564\n",
      "Epoch 3109, Loss: 0.013632968708407134, Final Batch Loss: 0.002040340332314372\n",
      "Epoch 3110, Loss: 0.038629923918051645, Final Batch Loss: 0.0004287610936444253\n",
      "Epoch 3111, Loss: 0.039289364067371935, Final Batch Loss: 0.0014248136430978775\n",
      "Epoch 3112, Loss: 0.03877101160469465, Final Batch Loss: 0.013862567953765392\n",
      "Epoch 3113, Loss: 0.0043484375346452, Final Batch Loss: 0.00145324831828475\n",
      "Epoch 3114, Loss: 0.06050536400289275, Final Batch Loss: 0.05333476513624191\n",
      "Epoch 3115, Loss: 0.018859388248529285, Final Batch Loss: 0.011996439658105373\n",
      "Epoch 3116, Loss: 0.022867060150019825, Final Batch Loss: 0.012502762489020824\n",
      "Epoch 3117, Loss: 0.01956104999408126, Final Batch Loss: 0.0026599476113915443\n",
      "Epoch 3118, Loss: 0.018074570994940586, Final Batch Loss: 0.001607340294867754\n",
      "Epoch 3119, Loss: 0.026030049426481128, Final Batch Loss: 0.000608578440733254\n",
      "Epoch 3120, Loss: 0.004013333003968, Final Batch Loss: 0.0010312815429642797\n",
      "Epoch 3121, Loss: 0.0040356990284635685, Final Batch Loss: 0.001041049836203456\n",
      "Epoch 3122, Loss: 0.028651810833252966, Final Batch Loss: 0.01812988705933094\n",
      "Epoch 3123, Loss: 0.005237137840595096, Final Batch Loss: 0.0010044093942269683\n",
      "Epoch 3124, Loss: 0.010653161007212475, Final Batch Loss: 0.007533217780292034\n",
      "Epoch 3125, Loss: 0.041902989585651085, Final Batch Loss: 0.00036284205270931125\n",
      "Epoch 3126, Loss: 0.020729940210003406, Final Batch Loss: 0.0009056681301444769\n",
      "Epoch 3127, Loss: 0.031122752581723034, Final Batch Loss: 0.001890400075353682\n",
      "Epoch 3128, Loss: 0.04156726823566714, Final Batch Loss: 0.02086034044623375\n",
      "Epoch 3129, Loss: 0.029446956468746066, Final Batch Loss: 0.002464412245899439\n",
      "Epoch 3130, Loss: 0.007243799802381545, Final Batch Loss: 0.0009019991266541183\n",
      "Epoch 3131, Loss: 0.03221143822884187, Final Batch Loss: 0.00908342469483614\n",
      "Epoch 3132, Loss: 0.004598043771693483, Final Batch Loss: 0.0010637608356773853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3133, Loss: 0.054784148596809246, Final Batch Loss: 0.00015992736734915525\n",
      "Epoch 3134, Loss: 0.019191931001842022, Final Batch Loss: 0.006649981252849102\n",
      "Epoch 3135, Loss: 0.049681122589390725, Final Batch Loss: 0.02766798622906208\n",
      "Epoch 3136, Loss: 0.01181812334107235, Final Batch Loss: 0.0007679643458686769\n",
      "Epoch 3137, Loss: 0.00718182569835335, Final Batch Loss: 0.001506892847828567\n",
      "Epoch 3138, Loss: 0.015734515502117574, Final Batch Loss: 0.0034486486110836267\n",
      "Epoch 3139, Loss: 0.022303856210783124, Final Batch Loss: 0.0003809212939813733\n",
      "Epoch 3140, Loss: 0.0014529939944623038, Final Batch Loss: 0.00036578354774974287\n",
      "Epoch 3141, Loss: 0.007381501549389213, Final Batch Loss: 0.001243422506377101\n",
      "Epoch 3142, Loss: 0.011338810145389289, Final Batch Loss: 0.004278827924281359\n",
      "Epoch 3143, Loss: 0.0083413078536978, Final Batch Loss: 0.00012671622971538454\n",
      "Epoch 3144, Loss: 0.005955785571131855, Final Batch Loss: 0.0006130631663836539\n",
      "Epoch 3145, Loss: 0.010321002162527293, Final Batch Loss: 0.0005123199662193656\n",
      "Epoch 3146, Loss: 0.02817465602129232, Final Batch Loss: 0.002008239272981882\n",
      "Epoch 3147, Loss: 0.0027521461161086336, Final Batch Loss: 0.0007224579458124936\n",
      "Epoch 3148, Loss: 0.022296715644188225, Final Batch Loss: 0.0002955211093649268\n",
      "Epoch 3149, Loss: 0.012567527504870668, Final Batch Loss: 0.0017301449552178383\n",
      "Epoch 3150, Loss: 0.004867999436100945, Final Batch Loss: 0.0008070602198131382\n",
      "Epoch 3151, Loss: 0.007430303070577793, Final Batch Loss: 0.0019367795903235674\n",
      "Epoch 3152, Loss: 0.003976167747168802, Final Batch Loss: 0.0004125653358642012\n",
      "Epoch 3153, Loss: 0.00949558793217875, Final Batch Loss: 0.00025000664754770696\n",
      "Epoch 3154, Loss: 0.005359808972571045, Final Batch Loss: 0.0007788324146531522\n",
      "Epoch 3155, Loss: 0.032544361951295286, Final Batch Loss: 0.024998215958476067\n",
      "Epoch 3156, Loss: 0.0046161138016032055, Final Batch Loss: 0.0006223153322935104\n",
      "Epoch 3157, Loss: 0.0062580694211646914, Final Batch Loss: 0.0002830797166097909\n",
      "Epoch 3158, Loss: 0.0034724646975519136, Final Batch Loss: 0.0001664042501943186\n",
      "Epoch 3159, Loss: 0.015207732853014022, Final Batch Loss: 0.0012499328004196286\n",
      "Epoch 3160, Loss: 0.03844569362991024, Final Batch Loss: 0.0003900105075445026\n",
      "Epoch 3161, Loss: 0.07901458776905201, Final Batch Loss: 0.0006598121835850179\n",
      "Epoch 3162, Loss: 0.03788455086760223, Final Batch Loss: 0.0006996472366154194\n",
      "Epoch 3163, Loss: 0.01036811142694205, Final Batch Loss: 0.0024199250619858503\n",
      "Epoch 3164, Loss: 0.013290988776134327, Final Batch Loss: 0.0010367075446993113\n",
      "Epoch 3165, Loss: 0.029917442239820957, Final Batch Loss: 0.0074568744748830795\n",
      "Epoch 3166, Loss: 0.026558933837804943, Final Batch Loss: 0.017322901636362076\n",
      "Epoch 3167, Loss: 0.006686687935143709, Final Batch Loss: 0.0013639881508424878\n",
      "Epoch 3168, Loss: 0.01213712795288302, Final Batch Loss: 0.0018907530466094613\n",
      "Epoch 3169, Loss: 0.029249923798488453, Final Batch Loss: 0.0013749863719567657\n",
      "Epoch 3170, Loss: 0.010425020358525217, Final Batch Loss: 0.0010497996117919683\n",
      "Epoch 3171, Loss: 0.02678466000361368, Final Batch Loss: 0.002459353068843484\n",
      "Epoch 3172, Loss: 0.0044641368731390685, Final Batch Loss: 0.0006912780809216201\n",
      "Epoch 3173, Loss: 0.005004631704650819, Final Batch Loss: 0.0010678981197997928\n",
      "Epoch 3174, Loss: 0.011643034173175693, Final Batch Loss: 0.0008127865730784833\n",
      "Epoch 3175, Loss: 0.02416639367584139, Final Batch Loss: 0.0064327954314649105\n",
      "Epoch 3176, Loss: 0.0021361024992074817, Final Batch Loss: 0.00034984684316441417\n",
      "Epoch 3177, Loss: 0.021786035416880623, Final Batch Loss: 0.0005537080578505993\n",
      "Epoch 3178, Loss: 0.009578524885000661, Final Batch Loss: 0.0038010738790035248\n",
      "Epoch 3179, Loss: 0.013900968246161938, Final Batch Loss: 0.008727168664336205\n",
      "Epoch 3180, Loss: 0.007666575780604035, Final Batch Loss: 0.0019063173094764352\n",
      "Epoch 3181, Loss: 0.0081720131856855, Final Batch Loss: 0.00042308622505515814\n",
      "Epoch 3182, Loss: 0.010964703164063394, Final Batch Loss: 0.002134758746251464\n",
      "Epoch 3183, Loss: 0.005573895388806704, Final Batch Loss: 0.0038609912153333426\n",
      "Epoch 3184, Loss: 0.005701939284335822, Final Batch Loss: 0.0010741254081949592\n",
      "Epoch 3185, Loss: 0.002485867837094702, Final Batch Loss: 0.0004395511350594461\n",
      "Epoch 3186, Loss: 0.01662818403565325, Final Batch Loss: 0.0007459814078174531\n",
      "Epoch 3187, Loss: 0.028610165638383478, Final Batch Loss: 0.019726097583770752\n",
      "Epoch 3188, Loss: 0.07199989189393818, Final Batch Loss: 0.008111406117677689\n",
      "Epoch 3189, Loss: 0.009398299036547542, Final Batch Loss: 0.0017053636256605387\n",
      "Epoch 3190, Loss: 0.011914669536054134, Final Batch Loss: 0.0009109675884246826\n",
      "Epoch 3191, Loss: 0.0093480805226136, Final Batch Loss: 0.0009307784494012594\n",
      "Epoch 3192, Loss: 0.02109805602231063, Final Batch Loss: 0.0007535643526352942\n",
      "Epoch 3193, Loss: 0.03406628972152248, Final Batch Loss: 0.016508447006344795\n",
      "Epoch 3194, Loss: 0.02285563776968047, Final Batch Loss: 0.005831653717905283\n",
      "Epoch 3195, Loss: 0.005603227735264227, Final Batch Loss: 0.0006936342106200755\n",
      "Epoch 3196, Loss: 0.02358183648175327, Final Batch Loss: 0.0021738973446190357\n",
      "Epoch 3197, Loss: 0.010277917346684262, Final Batch Loss: 0.00016738075646571815\n",
      "Epoch 3198, Loss: 0.009329976091976278, Final Batch Loss: 0.005239532329142094\n",
      "Epoch 3199, Loss: 0.015506028008530848, Final Batch Loss: 0.002683620899915695\n",
      "Epoch 3200, Loss: 0.00547069100139197, Final Batch Loss: 8.839006477501243e-05\n",
      "Epoch 3201, Loss: 0.0046026010531932116, Final Batch Loss: 0.0006419337587431073\n",
      "Epoch 3202, Loss: 0.014128281851299107, Final Batch Loss: 0.00028416526038199663\n",
      "Epoch 3203, Loss: 0.009850295435171574, Final Batch Loss: 0.00084849534323439\n",
      "Epoch 3204, Loss: 0.01459763408638537, Final Batch Loss: 0.0008116571698337793\n",
      "Epoch 3205, Loss: 0.03554277346120216, Final Batch Loss: 0.001411644509062171\n",
      "Epoch 3206, Loss: 0.004921533080050722, Final Batch Loss: 0.0016779174329712987\n",
      "Epoch 3207, Loss: 0.034863819484598935, Final Batch Loss: 0.001304764300584793\n",
      "Epoch 3208, Loss: 0.03766865422949195, Final Batch Loss: 0.00037361017893999815\n",
      "Epoch 3209, Loss: 0.0298592442413792, Final Batch Loss: 0.007216688711196184\n",
      "Epoch 3210, Loss: 0.029324634262593463, Final Batch Loss: 0.0010223506251350045\n",
      "Epoch 3211, Loss: 0.0038938084908295423, Final Batch Loss: 0.0008885423885658383\n",
      "Epoch 3212, Loss: 0.014527393454045523, Final Batch Loss: 0.0009547265945002437\n",
      "Epoch 3213, Loss: 0.01866561104543507, Final Batch Loss: 0.014320517890155315\n",
      "Epoch 3214, Loss: 0.008185597282135859, Final Batch Loss: 0.003774357261136174\n",
      "Epoch 3215, Loss: 0.009066578873898834, Final Batch Loss: 0.0005087991594336927\n",
      "Epoch 3216, Loss: 0.014904436335200444, Final Batch Loss: 0.00033079073182307184\n",
      "Epoch 3217, Loss: 0.01282412197906524, Final Batch Loss: 0.0006592421559616923\n",
      "Epoch 3218, Loss: 0.02270801752456464, Final Batch Loss: 0.0004721327277366072\n",
      "Epoch 3219, Loss: 0.046902335074264556, Final Batch Loss: 0.006955753080546856\n",
      "Epoch 3220, Loss: 0.020252706890460104, Final Batch Loss: 0.017581991851329803\n",
      "Epoch 3221, Loss: 0.003772551841393579, Final Batch Loss: 0.000674014154355973\n",
      "Epoch 3222, Loss: 0.022216915676835924, Final Batch Loss: 0.0008747470565140247\n",
      "Epoch 3223, Loss: 0.003509491856675595, Final Batch Loss: 0.0003597088507376611\n",
      "Epoch 3224, Loss: 0.004171484266407788, Final Batch Loss: 0.0004534393083304167\n",
      "Epoch 3225, Loss: 0.004414182330947369, Final Batch Loss: 0.0012459403369575739\n",
      "Epoch 3226, Loss: 0.0030666735547129065, Final Batch Loss: 0.00026755122235044837\n",
      "Epoch 3227, Loss: 0.011491727738757618, Final Batch Loss: 0.0003512128896545619\n",
      "Epoch 3228, Loss: 0.011436041997512802, Final Batch Loss: 0.005182250868529081\n",
      "Epoch 3229, Loss: 0.01942451865761541, Final Batch Loss: 0.00024562719045206904\n",
      "Epoch 3230, Loss: 0.0113439092237968, Final Batch Loss: 0.001017447910271585\n",
      "Epoch 3231, Loss: 0.012225510654388927, Final Batch Loss: 0.003715467406436801\n",
      "Epoch 3232, Loss: 0.03584571124520153, Final Batch Loss: 0.023850589990615845\n",
      "Epoch 3233, Loss: 0.014714388409629464, Final Batch Loss: 0.00402159383520484\n",
      "Epoch 3234, Loss: 0.012965124275069684, Final Batch Loss: 0.0018139368621632457\n",
      "Epoch 3235, Loss: 0.022551751462742686, Final Batch Loss: 0.002369218971580267\n",
      "Epoch 3236, Loss: 0.034099074808182195, Final Batch Loss: 0.00040209240978583694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3237, Loss: 0.06584780482808128, Final Batch Loss: 0.04977318271994591\n",
      "Epoch 3238, Loss: 0.01466595137026161, Final Batch Loss: 0.001043830648995936\n",
      "Epoch 3239, Loss: 0.043292658403515816, Final Batch Loss: 0.006759416311979294\n",
      "Epoch 3240, Loss: 0.03299201070331037, Final Batch Loss: 0.003990144934505224\n",
      "Epoch 3241, Loss: 0.018910134735051543, Final Batch Loss: 0.00044211148633621633\n",
      "Epoch 3242, Loss: 0.021690550696803257, Final Batch Loss: 0.007974456995725632\n",
      "Epoch 3243, Loss: 0.057055544544709846, Final Batch Loss: 0.0463130921125412\n",
      "Epoch 3244, Loss: 0.011835492099635303, Final Batch Loss: 0.0007548417197540402\n",
      "Epoch 3245, Loss: 0.013700021299882792, Final Batch Loss: 0.009845887310802937\n",
      "Epoch 3246, Loss: 0.014662651112303138, Final Batch Loss: 0.0002646208740770817\n",
      "Epoch 3247, Loss: 0.007814332522684708, Final Batch Loss: 0.00047033376176841557\n",
      "Epoch 3248, Loss: 0.015394276182632893, Final Batch Loss: 0.004227958619594574\n",
      "Epoch 3249, Loss: 0.0025403447943972424, Final Batch Loss: 0.00045225484063848853\n",
      "Epoch 3250, Loss: 0.010445722786244005, Final Batch Loss: 0.0012795327929779887\n",
      "Epoch 3251, Loss: 0.0030146158824209124, Final Batch Loss: 0.00020594961824826896\n",
      "Epoch 3252, Loss: 0.01002524292562157, Final Batch Loss: 0.0012104592751711607\n",
      "Epoch 3253, Loss: 0.007071694824844599, Final Batch Loss: 0.00039344371180050075\n",
      "Epoch 3254, Loss: 0.005989172786939889, Final Batch Loss: 0.0009568994864821434\n",
      "Epoch 3255, Loss: 0.0305880306113977, Final Batch Loss: 0.0002681952028069645\n",
      "Epoch 3256, Loss: 0.0038965265193837695, Final Batch Loss: 0.00025460930191911757\n",
      "Epoch 3257, Loss: 0.031553861386782955, Final Batch Loss: 0.0001172682605101727\n",
      "Epoch 3258, Loss: 0.012660279811825603, Final Batch Loss: 0.007537310943007469\n",
      "Epoch 3259, Loss: 0.0060131346399430186, Final Batch Loss: 0.00036072879447601736\n",
      "Epoch 3260, Loss: 0.001493376592407003, Final Batch Loss: 0.00022858417651150376\n",
      "Epoch 3261, Loss: 0.0047739599540364, Final Batch Loss: 8.445166167803109e-05\n",
      "Epoch 3262, Loss: 0.005520051345229149, Final Batch Loss: 0.0009984447387978435\n",
      "Epoch 3263, Loss: 0.012336125335423276, Final Batch Loss: 0.0004199781105853617\n",
      "Epoch 3264, Loss: 0.004188535283901729, Final Batch Loss: 0.00037066012737341225\n",
      "Epoch 3265, Loss: 0.003481293468212243, Final Batch Loss: 0.0015976384747773409\n",
      "Epoch 3266, Loss: 0.0027432328060967848, Final Batch Loss: 9.467975178267807e-05\n",
      "Epoch 3267, Loss: 0.01624306771554984, Final Batch Loss: 0.00039475024095736444\n",
      "Epoch 3268, Loss: 0.003838702803477645, Final Batch Loss: 0.00032653010566718876\n",
      "Epoch 3269, Loss: 0.002639105703565292, Final Batch Loss: 7.893724250607193e-05\n",
      "Epoch 3270, Loss: 0.017360115074552596, Final Batch Loss: 0.0020603903103619814\n",
      "Epoch 3271, Loss: 0.011826914182165638, Final Batch Loss: 0.00037574200541712344\n",
      "Epoch 3272, Loss: 0.003088044293690473, Final Batch Loss: 0.0006280685774981976\n",
      "Epoch 3273, Loss: 0.010296978827682324, Final Batch Loss: 0.0008175827679224312\n",
      "Epoch 3274, Loss: 0.007325219674385153, Final Batch Loss: 0.0002414681512163952\n",
      "Epoch 3275, Loss: 0.0239510782121215, Final Batch Loss: 0.0014763397630304098\n",
      "Epoch 3276, Loss: 0.004113558039534837, Final Batch Loss: 0.0008532555075362325\n",
      "Epoch 3277, Loss: 0.0062358888098970056, Final Batch Loss: 0.0005183190223760903\n",
      "Epoch 3278, Loss: 0.0014229832304408774, Final Batch Loss: 0.0005923996213823557\n",
      "Epoch 3279, Loss: 0.0026153473954764195, Final Batch Loss: 7.816972356522456e-05\n",
      "Epoch 3280, Loss: 0.008573270300985314, Final Batch Loss: 0.0001726092305034399\n",
      "Epoch 3281, Loss: 0.010796058253617957, Final Batch Loss: 0.007561062928289175\n",
      "Epoch 3282, Loss: 0.011385268036974594, Final Batch Loss: 0.00023582507856190205\n",
      "Epoch 3283, Loss: 0.011960344869294204, Final Batch Loss: 0.0011349058477208018\n",
      "Epoch 3284, Loss: 0.05156208566040732, Final Batch Loss: 0.0003805199230555445\n",
      "Epoch 3285, Loss: 0.001994958656723611, Final Batch Loss: 0.0005241064936853945\n",
      "Epoch 3286, Loss: 0.05465788731817156, Final Batch Loss: 0.0010991422459483147\n",
      "Epoch 3287, Loss: 0.004626181296771392, Final Batch Loss: 0.0003814506344497204\n",
      "Epoch 3288, Loss: 0.00432662625098601, Final Batch Loss: 0.000742863048799336\n",
      "Epoch 3289, Loss: 0.005097886212752201, Final Batch Loss: 0.0005745189846493304\n",
      "Epoch 3290, Loss: 0.027826159639516845, Final Batch Loss: 0.00038831299752928317\n",
      "Epoch 3291, Loss: 0.036367075037560426, Final Batch Loss: 0.00020864616089966148\n",
      "Epoch 3292, Loss: 0.005314509515301324, Final Batch Loss: 0.0001047811092576012\n",
      "Epoch 3293, Loss: 0.0026588217151584104, Final Batch Loss: 0.00013407731603365391\n",
      "Epoch 3294, Loss: 0.03725865475280443, Final Batch Loss: 3.5592012864071876e-05\n",
      "Epoch 3295, Loss: 0.042081637802766636, Final Batch Loss: 0.009672953747212887\n",
      "Epoch 3296, Loss: 0.02305745181365637, Final Batch Loss: 0.00022539967903867364\n",
      "Epoch 3297, Loss: 0.021881872904486954, Final Batch Loss: 0.003043878125026822\n",
      "Epoch 3298, Loss: 0.007046706276014447, Final Batch Loss: 0.003261564765125513\n",
      "Epoch 3299, Loss: 0.014028960300493054, Final Batch Loss: 0.00012798163515981287\n",
      "Epoch 3300, Loss: 0.007632478227606043, Final Batch Loss: 0.0007240026607178152\n",
      "Epoch 3301, Loss: 0.0027927203045692295, Final Batch Loss: 0.0007503094966523349\n",
      "Epoch 3302, Loss: 0.018934096180601045, Final Batch Loss: 0.00024297559866681695\n",
      "Epoch 3303, Loss: 0.009692514024209231, Final Batch Loss: 0.0005920777330175042\n",
      "Epoch 3304, Loss: 0.007065483718179166, Final Batch Loss: 0.005025916267186403\n",
      "Epoch 3305, Loss: 0.005750035110395402, Final Batch Loss: 0.004583241418004036\n",
      "Epoch 3306, Loss: 0.0048403473338112235, Final Batch Loss: 0.0036926937755197287\n",
      "Epoch 3307, Loss: 0.006609904128708877, Final Batch Loss: 0.0010589307639747858\n",
      "Epoch 3308, Loss: 0.013852652497007512, Final Batch Loss: 0.006154623348265886\n",
      "Epoch 3309, Loss: 0.00995238278846955, Final Batch Loss: 0.0007841941551305354\n",
      "Epoch 3310, Loss: 0.02516905020456761, Final Batch Loss: 0.017147164791822433\n",
      "Epoch 3311, Loss: 0.010409803478978574, Final Batch Loss: 0.0003742123662959784\n",
      "Epoch 3312, Loss: 0.002688171560293995, Final Batch Loss: 0.0006131884292699397\n",
      "Epoch 3313, Loss: 0.003764631721423939, Final Batch Loss: 0.0006889094365760684\n",
      "Epoch 3314, Loss: 0.0023637591366423294, Final Batch Loss: 7.456827734131366e-05\n",
      "Epoch 3315, Loss: 0.04407154595901375, Final Batch Loss: 0.03875240311026573\n",
      "Epoch 3316, Loss: 0.027947516093263403, Final Batch Loss: 0.00044051630538888276\n",
      "Epoch 3317, Loss: 0.05013692335342057, Final Batch Loss: 0.011003048159182072\n",
      "Epoch 3318, Loss: 0.0018102671747328714, Final Batch Loss: 0.0005801275256089866\n",
      "Epoch 3319, Loss: 0.03226432960946113, Final Batch Loss: 0.023016493767499924\n",
      "Epoch 3320, Loss: 0.04893849010113627, Final Batch Loss: 0.004668082110583782\n",
      "Epoch 3321, Loss: 0.0576641148654744, Final Batch Loss: 0.029972989112138748\n",
      "Epoch 3322, Loss: 0.006363847089232877, Final Batch Loss: 0.0006066387868486345\n",
      "Epoch 3323, Loss: 0.008231241954490542, Final Batch Loss: 0.00025839090812951326\n",
      "Epoch 3324, Loss: 0.04361554447677918, Final Batch Loss: 0.038144420832395554\n",
      "Epoch 3325, Loss: 0.013336621057533193, Final Batch Loss: 0.01164824329316616\n",
      "Epoch 3326, Loss: 0.02614125781110488, Final Batch Loss: 0.0008720983751118183\n",
      "Epoch 3327, Loss: 0.0706026339030359, Final Batch Loss: 0.03479660674929619\n",
      "Epoch 3328, Loss: 0.0720164566882886, Final Batch Loss: 0.06713929027318954\n",
      "Epoch 3329, Loss: 0.06420925504062325, Final Batch Loss: 0.029908765107393265\n",
      "Epoch 3330, Loss: 0.0861502771731466, Final Batch Loss: 0.0017289352836087346\n",
      "Epoch 3331, Loss: 0.028082593868020922, Final Batch Loss: 0.002506629563868046\n",
      "Epoch 3332, Loss: 0.02765430335421115, Final Batch Loss: 0.0015636887401342392\n",
      "Epoch 3333, Loss: 0.022129429096821696, Final Batch Loss: 0.009232320822775364\n",
      "Epoch 3334, Loss: 0.01395704410970211, Final Batch Loss: 0.0009979112073779106\n",
      "Epoch 3335, Loss: 0.030642047931905836, Final Batch Loss: 0.0006528011290356517\n",
      "Epoch 3336, Loss: 0.004777679045218974, Final Batch Loss: 0.0007912456057965755\n",
      "Epoch 3337, Loss: 0.020639505848521367, Final Batch Loss: 0.0022720510605722666\n",
      "Epoch 3338, Loss: 0.04192727408371866, Final Batch Loss: 0.0314268134534359\n",
      "Epoch 3339, Loss: 0.00412143548601307, Final Batch Loss: 0.00098520889878273\n",
      "Epoch 3340, Loss: 0.006051721691619605, Final Batch Loss: 0.0022422082256525755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3341, Loss: 0.006679555663140491, Final Batch Loss: 0.0002843450347427279\n",
      "Epoch 3342, Loss: 0.004491819068789482, Final Batch Loss: 0.0008296619052998722\n",
      "Epoch 3343, Loss: 0.041820280719548464, Final Batch Loss: 0.00618415093049407\n",
      "Epoch 3344, Loss: 0.018176074721850455, Final Batch Loss: 0.0040712980553507805\n",
      "Epoch 3345, Loss: 0.021487601727130823, Final Batch Loss: 0.011722221039235592\n",
      "Epoch 3346, Loss: 0.012508305255323648, Final Batch Loss: 0.0016585122793912888\n",
      "Epoch 3347, Loss: 0.0900895400554873, Final Batch Loss: 0.01148289255797863\n",
      "Epoch 3348, Loss: 0.019661077647469938, Final Batch Loss: 0.001834545866586268\n",
      "Epoch 3349, Loss: 0.0244026065338403, Final Batch Loss: 0.006030147895216942\n",
      "Epoch 3350, Loss: 0.022146724280901253, Final Batch Loss: 0.0015855309320613742\n",
      "Epoch 3351, Loss: 0.007271845519426279, Final Batch Loss: 0.00022040730982553214\n",
      "Epoch 3352, Loss: 0.014555777015630156, Final Batch Loss: 0.003970245365053415\n",
      "Epoch 3353, Loss: 0.06727888976456597, Final Batch Loss: 0.00021203403593972325\n",
      "Epoch 3354, Loss: 0.010964940374833532, Final Batch Loss: 0.00046169330016709864\n",
      "Epoch 3355, Loss: 0.03305727196857333, Final Batch Loss: 0.0085951779037714\n",
      "Epoch 3356, Loss: 0.01471482650958933, Final Batch Loss: 0.0038067467976361513\n",
      "Epoch 3357, Loss: 0.01449433685047552, Final Batch Loss: 0.001760554499924183\n",
      "Epoch 3358, Loss: 0.030928103253245354, Final Batch Loss: 0.0006847077165730298\n",
      "Epoch 3359, Loss: 0.07973955571651459, Final Batch Loss: 0.022766970098018646\n",
      "Epoch 3360, Loss: 0.01611388975288719, Final Batch Loss: 0.0025699082762002945\n",
      "Epoch 3361, Loss: 0.01124460162827745, Final Batch Loss: 0.00047239489504136145\n",
      "Epoch 3362, Loss: 0.012449697765987366, Final Batch Loss: 0.0022634551860392094\n",
      "Epoch 3363, Loss: 0.016505266132298857, Final Batch Loss: 0.0021830538753420115\n",
      "Epoch 3364, Loss: 0.008735889161471277, Final Batch Loss: 0.0019629932940006256\n",
      "Epoch 3365, Loss: 0.004204463359201327, Final Batch Loss: 0.00014159883721731603\n",
      "Epoch 3366, Loss: 0.015547039307421073, Final Batch Loss: 0.0045987521298229694\n",
      "Epoch 3367, Loss: 0.00726564327487722, Final Batch Loss: 0.000515222956892103\n",
      "Epoch 3368, Loss: 0.004166077385889366, Final Batch Loss: 0.0010441710473969579\n",
      "Epoch 3369, Loss: 0.030219259700970724, Final Batch Loss: 0.0003541399200912565\n",
      "Epoch 3370, Loss: 0.0085008226451464, Final Batch Loss: 0.00228899740613997\n",
      "Epoch 3371, Loss: 0.02671825123252347, Final Batch Loss: 0.001177875674329698\n",
      "Epoch 3372, Loss: 0.004650976567063481, Final Batch Loss: 0.001949777826666832\n",
      "Epoch 3373, Loss: 0.01687461056280881, Final Batch Loss: 0.000763171527069062\n",
      "Epoch 3374, Loss: 0.016782225953647867, Final Batch Loss: 0.0003158703038934618\n",
      "Epoch 3375, Loss: 0.0039722759975120425, Final Batch Loss: 0.0002824202529154718\n",
      "Epoch 3376, Loss: 0.010070586577057838, Final Batch Loss: 0.0004124398692511022\n",
      "Epoch 3377, Loss: 0.004067058704094961, Final Batch Loss: 0.00041154210339300334\n",
      "Epoch 3378, Loss: 0.028624300961382687, Final Batch Loss: 0.009542487561702728\n",
      "Epoch 3379, Loss: 0.01863067949307151, Final Batch Loss: 0.0010194883216172457\n",
      "Epoch 3380, Loss: 0.0027174056449439377, Final Batch Loss: 0.0009723989642225206\n",
      "Epoch 3381, Loss: 0.0443473428604193, Final Batch Loss: 0.012345291674137115\n",
      "Epoch 3382, Loss: 0.020302698234445415, Final Batch Loss: 0.004196551628410816\n",
      "Epoch 3383, Loss: 0.008154033042956144, Final Batch Loss: 0.001073881285265088\n",
      "Epoch 3384, Loss: 0.06700355489738286, Final Batch Loss: 0.0175884161144495\n",
      "Epoch 3385, Loss: 0.03358502555056475, Final Batch Loss: 0.021937616169452667\n",
      "Epoch 3386, Loss: 0.02319309615995735, Final Batch Loss: 0.0004825990181416273\n",
      "Epoch 3387, Loss: 0.005035099224187434, Final Batch Loss: 0.0017758121248334646\n",
      "Epoch 3388, Loss: 0.005790918701677583, Final Batch Loss: 0.00023515771317761391\n",
      "Epoch 3389, Loss: 0.016140729043399915, Final Batch Loss: 0.0004726758925244212\n",
      "Epoch 3390, Loss: 0.009155184437986463, Final Batch Loss: 0.0005210730596445501\n",
      "Epoch 3391, Loss: 0.031089454190805554, Final Batch Loss: 0.013958340510725975\n",
      "Epoch 3392, Loss: 0.005796363664558157, Final Batch Loss: 0.0005069623584859073\n",
      "Epoch 3393, Loss: 0.016652980106300674, Final Batch Loss: 0.000286569120362401\n",
      "Epoch 3394, Loss: 0.03675735794240609, Final Batch Loss: 0.007196872029453516\n",
      "Epoch 3395, Loss: 0.013734627704252489, Final Batch Loss: 0.0014624991454184055\n",
      "Epoch 3396, Loss: 0.016476427321322262, Final Batch Loss: 0.003928839694708586\n",
      "Epoch 3397, Loss: 0.0132358687260421, Final Batch Loss: 0.00020980947010684758\n",
      "Epoch 3398, Loss: 0.006264727184316143, Final Batch Loss: 0.0016792926471680403\n",
      "Epoch 3399, Loss: 0.002623591193696484, Final Batch Loss: 0.0007624701829627156\n",
      "Epoch 3400, Loss: 0.007386351499008015, Final Batch Loss: 0.0004894465091638267\n",
      "Epoch 3401, Loss: 0.008578449749620631, Final Batch Loss: 0.0008559726411476731\n",
      "Epoch 3402, Loss: 0.005898242350667715, Final Batch Loss: 0.0007905488600954413\n",
      "Epoch 3403, Loss: 0.005896397422475275, Final Batch Loss: 0.00045712236897088587\n",
      "Epoch 3404, Loss: 0.010224114317679778, Final Batch Loss: 0.0014957016101107001\n",
      "Epoch 3405, Loss: 0.018675653263926506, Final Batch Loss: 0.013437713496387005\n",
      "Epoch 3406, Loss: 0.004747892671730369, Final Batch Loss: 0.003226072993129492\n",
      "Epoch 3407, Loss: 0.014199547294992954, Final Batch Loss: 0.0008578366250731051\n",
      "Epoch 3408, Loss: 0.02898774202913046, Final Batch Loss: 0.02628602832555771\n",
      "Epoch 3409, Loss: 0.002724043428315781, Final Batch Loss: 0.0013261905405670404\n",
      "Epoch 3410, Loss: 0.018198734062025324, Final Batch Loss: 0.016820592805743217\n",
      "Epoch 3411, Loss: 0.007928353501483798, Final Batch Loss: 0.0015496008563786745\n",
      "Epoch 3412, Loss: 0.031220989578287117, Final Batch Loss: 0.00021434416703414172\n",
      "Epoch 3413, Loss: 0.006400344002031488, Final Batch Loss: 0.0007125002448447049\n",
      "Epoch 3414, Loss: 0.07314783267793246, Final Batch Loss: 0.0006718424847349524\n",
      "Epoch 3415, Loss: 0.029285123862791806, Final Batch Loss: 0.00014858698705211282\n",
      "Epoch 3416, Loss: 0.04701809385733213, Final Batch Loss: 0.008392452262341976\n",
      "Epoch 3417, Loss: 0.03880271315574646, Final Batch Loss: 0.0018359932582825422\n",
      "Epoch 3418, Loss: 0.02190365025307983, Final Batch Loss: 0.0011249908711761236\n",
      "Epoch 3419, Loss: 0.05528622679412365, Final Batch Loss: 0.024393267929553986\n",
      "Epoch 3420, Loss: 0.010895387618802488, Final Batch Loss: 0.00038079873775132\n",
      "Epoch 3421, Loss: 0.04287391182151623, Final Batch Loss: 0.025040673092007637\n",
      "Epoch 3422, Loss: 0.02814175677485764, Final Batch Loss: 0.01776161976158619\n",
      "Epoch 3423, Loss: 0.016364295210223645, Final Batch Loss: 0.0006154144648462534\n",
      "Epoch 3424, Loss: 0.004163971490925178, Final Batch Loss: 0.0011158750858157873\n",
      "Epoch 3425, Loss: 0.022618472459726036, Final Batch Loss: 0.0008059311658143997\n",
      "Epoch 3426, Loss: 0.0068782398593612015, Final Batch Loss: 0.001102612353861332\n",
      "Epoch 3427, Loss: 0.022124952578451484, Final Batch Loss: 0.0006970785325393081\n",
      "Epoch 3428, Loss: 0.017487539793364704, Final Batch Loss: 0.0006812687497586012\n",
      "Epoch 3429, Loss: 0.007367822181549855, Final Batch Loss: 0.00010819618182722479\n",
      "Epoch 3430, Loss: 0.02014760157908313, Final Batch Loss: 0.0008317316533066332\n",
      "Epoch 3431, Loss: 0.03923990984912962, Final Batch Loss: 0.002770214807242155\n",
      "Epoch 3432, Loss: 0.041926060992409475, Final Batch Loss: 0.0015913067618384957\n",
      "Epoch 3433, Loss: 0.05001899809576571, Final Batch Loss: 0.019565371796488762\n",
      "Epoch 3434, Loss: 0.023081716266460717, Final Batch Loss: 0.0009074066765606403\n",
      "Epoch 3435, Loss: 0.055178733775392175, Final Batch Loss: 0.005099544767290354\n",
      "Epoch 3436, Loss: 0.011691650375723839, Final Batch Loss: 0.0034573175944387913\n",
      "Epoch 3437, Loss: 0.02755997347412631, Final Batch Loss: 0.001147713279351592\n",
      "Epoch 3438, Loss: 0.008110458671581, Final Batch Loss: 0.0004969072178937495\n",
      "Epoch 3439, Loss: 0.019086842657998204, Final Batch Loss: 0.0008697081939317286\n",
      "Epoch 3440, Loss: 0.009168756558210589, Final Batch Loss: 0.00021811023179907352\n",
      "Epoch 3441, Loss: 0.011275421275058761, Final Batch Loss: 0.0007607046281918883\n",
      "Epoch 3442, Loss: 0.020862927427515388, Final Batch Loss: 0.018943101167678833\n",
      "Epoch 3443, Loss: 0.021229542675428092, Final Batch Loss: 0.004522327799350023\n",
      "Epoch 3444, Loss: 0.027187187573872507, Final Batch Loss: 0.0006496162386611104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3445, Loss: 0.005341402953490615, Final Batch Loss: 0.0008957153186202049\n",
      "Epoch 3446, Loss: 0.018929747137008235, Final Batch Loss: 0.00266973371617496\n",
      "Epoch 3447, Loss: 0.009042512945597991, Final Batch Loss: 0.004067962523549795\n",
      "Epoch 3448, Loss: 0.06102196831488982, Final Batch Loss: 0.0003468171344138682\n",
      "Epoch 3449, Loss: 0.004242828028509393, Final Batch Loss: 0.0011148771736770868\n",
      "Epoch 3450, Loss: 0.0054520429112017155, Final Batch Loss: 0.0007818667218089104\n",
      "Epoch 3451, Loss: 0.012086323142284527, Final Batch Loss: 0.00656015845015645\n",
      "Epoch 3452, Loss: 0.02171804869431071, Final Batch Loss: 0.001313521759584546\n",
      "Epoch 3453, Loss: 0.01820649014553055, Final Batch Loss: 0.013976016081869602\n",
      "Epoch 3454, Loss: 0.06320934765972197, Final Batch Loss: 0.001128685544244945\n",
      "Epoch 3455, Loss: 0.007092456566169858, Final Batch Loss: 0.0009232352022081614\n",
      "Epoch 3456, Loss: 0.04252413130598143, Final Batch Loss: 0.0023767556995153427\n",
      "Epoch 3457, Loss: 0.01171739530400373, Final Batch Loss: 0.00035569857573136687\n",
      "Epoch 3458, Loss: 0.009868270572042093, Final Batch Loss: 0.0003066759090870619\n",
      "Epoch 3459, Loss: 0.005238205769273918, Final Batch Loss: 0.00041043246164917946\n",
      "Epoch 3460, Loss: 0.029428037931211293, Final Batch Loss: 0.0006387485773302615\n",
      "Epoch 3461, Loss: 0.009322730533313006, Final Batch Loss: 0.0025333852972835302\n",
      "Epoch 3462, Loss: 0.06286692386493087, Final Batch Loss: 0.004027748014777899\n",
      "Epoch 3463, Loss: 0.015357158466940746, Final Batch Loss: 0.00027098829741589725\n",
      "Epoch 3464, Loss: 0.02361117341206409, Final Batch Loss: 0.016805216670036316\n",
      "Epoch 3465, Loss: 0.015124239231226966, Final Batch Loss: 0.0008717726450413465\n",
      "Epoch 3466, Loss: 0.005568950800807215, Final Batch Loss: 0.0001766446075635031\n",
      "Epoch 3467, Loss: 0.015120023555937223, Final Batch Loss: 0.002905291272327304\n",
      "Epoch 3468, Loss: 0.011686793048284017, Final Batch Loss: 0.004753073211759329\n",
      "Epoch 3469, Loss: 0.03114408242981881, Final Batch Loss: 0.005714403465390205\n",
      "Epoch 3470, Loss: 0.02169675142795313, Final Batch Loss: 0.012951897457242012\n",
      "Epoch 3471, Loss: 0.01891419242019765, Final Batch Loss: 0.0015781363472342491\n",
      "Epoch 3472, Loss: 0.05391961132409051, Final Batch Loss: 0.017252614721655846\n",
      "Epoch 3473, Loss: 0.009809460956603289, Final Batch Loss: 0.006598369684070349\n",
      "Epoch 3474, Loss: 0.004645963592338376, Final Batch Loss: 0.0023050883319228888\n",
      "Epoch 3475, Loss: 0.010897847067099065, Final Batch Loss: 0.001987434457987547\n",
      "Epoch 3476, Loss: 0.015280393825378269, Final Batch Loss: 0.0006673315656371415\n",
      "Epoch 3477, Loss: 0.015869550814386457, Final Batch Loss: 0.0005134732346050441\n",
      "Epoch 3478, Loss: 0.0203886185772717, Final Batch Loss: 0.0019878435414284468\n",
      "Epoch 3479, Loss: 0.018392072786809877, Final Batch Loss: 0.0002906009613070637\n",
      "Epoch 3480, Loss: 0.005808132700622082, Final Batch Loss: 0.0020792479626834393\n",
      "Epoch 3481, Loss: 0.010078357066959143, Final Batch Loss: 0.002059503458440304\n",
      "Epoch 3482, Loss: 0.0061657686019316316, Final Batch Loss: 0.00041194551158696413\n",
      "Epoch 3483, Loss: 0.0020575627222569892, Final Batch Loss: 0.00038569560274481773\n",
      "Epoch 3484, Loss: 0.005607569881249219, Final Batch Loss: 0.003419591812416911\n",
      "Epoch 3485, Loss: 0.009606571416952647, Final Batch Loss: 0.00023173321096692234\n",
      "Epoch 3486, Loss: 0.012617378844879568, Final Batch Loss: 0.0009566447697579861\n",
      "Epoch 3487, Loss: 0.007020287768682465, Final Batch Loss: 0.00012713164323940873\n",
      "Epoch 3488, Loss: 0.0025334475067211315, Final Batch Loss: 0.000639614649116993\n",
      "Epoch 3489, Loss: 0.00813149573514238, Final Batch Loss: 0.004444428253918886\n",
      "Epoch 3490, Loss: 0.009979294758522883, Final Batch Loss: 0.0002587142225820571\n",
      "Epoch 3491, Loss: 0.0029955959471408278, Final Batch Loss: 0.00039746082620695233\n",
      "Epoch 3492, Loss: 0.009011640213429928, Final Batch Loss: 0.0048219142481684685\n",
      "Epoch 3493, Loss: 0.0380676613713149, Final Batch Loss: 0.0005457431543618441\n",
      "Epoch 3494, Loss: 0.0033878726098919287, Final Batch Loss: 0.0011117082322016358\n",
      "Epoch 3495, Loss: 0.0024443379079457372, Final Batch Loss: 0.00015366544539574534\n",
      "Epoch 3496, Loss: 0.009289311390602961, Final Batch Loss: 0.001350266276858747\n",
      "Epoch 3497, Loss: 0.004137583076953888, Final Batch Loss: 0.00011818503844551742\n",
      "Epoch 3498, Loss: 0.0030299429927254096, Final Batch Loss: 0.0004714488168247044\n",
      "Epoch 3499, Loss: 0.0034687242878135294, Final Batch Loss: 0.0004206477024126798\n",
      "Epoch 3500, Loss: 0.02329447767988313, Final Batch Loss: 0.0008715757285244763\n",
      "Epoch 3501, Loss: 0.01179604884237051, Final Batch Loss: 0.0024337326176464558\n",
      "Epoch 3502, Loss: 0.023535569314844906, Final Batch Loss: 0.00014841390657238662\n",
      "Epoch 3503, Loss: 0.007589182670926675, Final Batch Loss: 0.0024630173575133085\n",
      "Epoch 3504, Loss: 0.005884743790375069, Final Batch Loss: 0.00014740688493475318\n",
      "Epoch 3505, Loss: 0.011264499757089652, Final Batch Loss: 0.0072502391412854195\n",
      "Epoch 3506, Loss: 0.026220301020657644, Final Batch Loss: 0.0025347929913550615\n",
      "Epoch 3507, Loss: 0.035582904107286595, Final Batch Loss: 0.00014890932652633637\n",
      "Epoch 3508, Loss: 0.0033947601259569637, Final Batch Loss: 0.00010134954209206626\n",
      "Epoch 3509, Loss: 0.030168579192832112, Final Batch Loss: 0.002563046058639884\n",
      "Epoch 3510, Loss: 0.005316403956385329, Final Batch Loss: 0.0012160483747720718\n",
      "Epoch 3511, Loss: 0.07629247941076756, Final Batch Loss: 0.0005381665541790426\n",
      "Epoch 3512, Loss: 0.022159236752486322, Final Batch Loss: 0.003289300249889493\n",
      "Epoch 3513, Loss: 0.002888724018703215, Final Batch Loss: 0.0003578899777494371\n",
      "Epoch 3514, Loss: 0.01565180908073671, Final Batch Loss: 0.0002908234891947359\n",
      "Epoch 3515, Loss: 0.019836846986436285, Final Batch Loss: 0.00020857526396866888\n",
      "Epoch 3516, Loss: 0.042157588875852525, Final Batch Loss: 0.0015806591836735606\n",
      "Epoch 3517, Loss: 0.004130721194087528, Final Batch Loss: 0.00014663985348306596\n",
      "Epoch 3518, Loss: 0.005583706311881542, Final Batch Loss: 0.002002498833462596\n",
      "Epoch 3519, Loss: 0.004108531604288146, Final Batch Loss: 0.00027343331021256745\n",
      "Epoch 3520, Loss: 0.003870899585308507, Final Batch Loss: 0.00041078656795434654\n",
      "Epoch 3521, Loss: 0.00917405828658957, Final Batch Loss: 0.00022668235760647804\n",
      "Epoch 3522, Loss: 0.016797161049908027, Final Batch Loss: 0.0014201313024386764\n",
      "Epoch 3523, Loss: 0.004505714983679354, Final Batch Loss: 0.0004204457509331405\n",
      "Epoch 3524, Loss: 0.004911877273116261, Final Batch Loss: 0.00024080966250039637\n",
      "Epoch 3525, Loss: 0.0058190213749185205, Final Batch Loss: 0.0014753409195691347\n",
      "Epoch 3526, Loss: 0.018048753452603705, Final Batch Loss: 0.00016877784219104797\n",
      "Epoch 3527, Loss: 0.004548350727418438, Final Batch Loss: 0.000559036445338279\n",
      "Epoch 3528, Loss: 0.004565404975437559, Final Batch Loss: 0.002684325445443392\n",
      "Epoch 3529, Loss: 0.01649740952416323, Final Batch Loss: 0.0010309257777407765\n",
      "Epoch 3530, Loss: 0.02013529386022128, Final Batch Loss: 0.00036509340861812234\n",
      "Epoch 3531, Loss: 0.015754934342112392, Final Batch Loss: 0.00519939512014389\n",
      "Epoch 3532, Loss: 0.047465581526921596, Final Batch Loss: 0.00010207013838225976\n",
      "Epoch 3533, Loss: 0.008883074973709881, Final Batch Loss: 0.0005640137242153287\n",
      "Epoch 3534, Loss: 0.019026935231522657, Final Batch Loss: 0.00034322115243412554\n",
      "Epoch 3535, Loss: 0.0034061467013088986, Final Batch Loss: 0.0003107358352281153\n",
      "Epoch 3536, Loss: 0.007169634394813329, Final Batch Loss: 0.002042733132839203\n",
      "Epoch 3537, Loss: 0.04032254277262837, Final Batch Loss: 0.0012500436278060079\n",
      "Epoch 3538, Loss: 0.004717420015367679, Final Batch Loss: 0.0007438454777002335\n",
      "Epoch 3539, Loss: 0.009203298985084984, Final Batch Loss: 0.001204378204420209\n",
      "Epoch 3540, Loss: 0.018053202831652015, Final Batch Loss: 0.010415429249405861\n",
      "Epoch 3541, Loss: 0.007140078145312145, Final Batch Loss: 0.0015389452455565333\n",
      "Epoch 3542, Loss: 0.045138131419662386, Final Batch Loss: 0.0006868771743029356\n",
      "Epoch 3543, Loss: 0.015146872960031033, Final Batch Loss: 0.007416968699544668\n",
      "Epoch 3544, Loss: 0.007131933773052879, Final Batch Loss: 0.00011885569256264716\n",
      "Epoch 3545, Loss: 0.010451717709656805, Final Batch Loss: 0.0007110813749022782\n",
      "Epoch 3546, Loss: 0.016302191594149917, Final Batch Loss: 0.006432047113776207\n",
      "Epoch 3547, Loss: 0.0050821090408135206, Final Batch Loss: 0.0008629673975519836\n",
      "Epoch 3548, Loss: 0.01077866691048257, Final Batch Loss: 0.00025360388099215925\n",
      "Epoch 3549, Loss: 0.012896675892989151, Final Batch Loss: 0.0004136996576562524\n",
      "Epoch 3550, Loss: 0.026610364657244645, Final Batch Loss: 0.01835617981851101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3551, Loss: 0.011190766912477557, Final Batch Loss: 0.00020055606728419662\n",
      "Epoch 3552, Loss: 0.025722232370753773, Final Batch Loss: 0.0004807344521395862\n",
      "Epoch 3553, Loss: 0.0392974938149564, Final Batch Loss: 0.0008183469763025641\n",
      "Epoch 3554, Loss: 0.01570198975969106, Final Batch Loss: 0.0002651781542226672\n",
      "Epoch 3555, Loss: 0.044746101564669516, Final Batch Loss: 0.005894444417208433\n",
      "Epoch 3556, Loss: 0.048598711990052834, Final Batch Loss: 0.0004192056367173791\n",
      "Epoch 3557, Loss: 0.006020560103934258, Final Batch Loss: 0.0002990759094245732\n",
      "Epoch 3558, Loss: 0.01365141457063146, Final Batch Loss: 0.002627139911055565\n",
      "Epoch 3559, Loss: 0.009498866653302684, Final Batch Loss: 0.004735113121569157\n",
      "Epoch 3560, Loss: 0.021920806873822585, Final Batch Loss: 0.01776108145713806\n",
      "Epoch 3561, Loss: 0.03388038676348515, Final Batch Loss: 0.006330440286546946\n",
      "Epoch 3562, Loss: 0.019375693693291396, Final Batch Loss: 0.001392628182657063\n",
      "Epoch 3563, Loss: 0.005804662563605234, Final Batch Loss: 0.0007582915714010596\n",
      "Epoch 3564, Loss: 0.027825143421068788, Final Batch Loss: 0.0010761963203549385\n",
      "Epoch 3565, Loss: 0.0026525304856477305, Final Batch Loss: 0.00015992460248526186\n",
      "Epoch 3566, Loss: 0.009910921973641962, Final Batch Loss: 0.00041424966184422374\n",
      "Epoch 3567, Loss: 0.006716300820698962, Final Batch Loss: 0.0009969223756343126\n",
      "Epoch 3568, Loss: 0.03877870988799259, Final Batch Loss: 0.0020726139191538095\n",
      "Epoch 3569, Loss: 0.02257274282601429, Final Batch Loss: 0.011299369856715202\n",
      "Epoch 3570, Loss: 0.009053196234162897, Final Batch Loss: 0.0012338748201727867\n",
      "Epoch 3571, Loss: 0.020788557128980756, Final Batch Loss: 0.0027067435439676046\n",
      "Epoch 3572, Loss: 0.028204993228428066, Final Batch Loss: 0.004965742118656635\n",
      "Epoch 3573, Loss: 0.019576471124310046, Final Batch Loss: 0.0004077802295796573\n",
      "Epoch 3574, Loss: 0.023631223011761904, Final Batch Loss: 0.0014982360880821943\n",
      "Epoch 3575, Loss: 0.01685307768639177, Final Batch Loss: 0.0016096982872113585\n",
      "Epoch 3576, Loss: 0.03105688455980271, Final Batch Loss: 0.0008704083156771958\n",
      "Epoch 3577, Loss: 0.021029906783951446, Final Batch Loss: 0.0007085020188242197\n",
      "Epoch 3578, Loss: 0.009925840888172388, Final Batch Loss: 0.0002329571871086955\n",
      "Epoch 3579, Loss: 0.016299683949910104, Final Batch Loss: 0.012468201108276844\n",
      "Epoch 3580, Loss: 0.003892876993631944, Final Batch Loss: 0.0009427510085515678\n",
      "Epoch 3581, Loss: 0.016199919919017702, Final Batch Loss: 0.0011925575090572238\n",
      "Epoch 3582, Loss: 0.009951970539987087, Final Batch Loss: 0.00038040539948269725\n",
      "Epoch 3583, Loss: 0.024429503246210515, Final Batch Loss: 0.00020549132023006678\n",
      "Epoch 3584, Loss: 0.017592915217392147, Final Batch Loss: 0.00043898148578591645\n",
      "Epoch 3585, Loss: 0.0037289940082700923, Final Batch Loss: 0.00021857932733837515\n",
      "Epoch 3586, Loss: 0.013118794391630217, Final Batch Loss: 0.005141032859683037\n",
      "Epoch 3587, Loss: 0.00437167267955374, Final Batch Loss: 0.00037464831257238984\n",
      "Epoch 3588, Loss: 0.004658395133446902, Final Batch Loss: 0.00043009317596443\n",
      "Epoch 3589, Loss: 0.009570587135385722, Final Batch Loss: 0.0002593222016002983\n",
      "Epoch 3590, Loss: 0.00847225051256828, Final Batch Loss: 0.00042656445293687284\n",
      "Epoch 3591, Loss: 0.0035312445143063087, Final Batch Loss: 0.0005798637284897268\n",
      "Epoch 3592, Loss: 0.03262790193548426, Final Batch Loss: 0.0004988614819012582\n",
      "Epoch 3593, Loss: 0.015442727686604485, Final Batch Loss: 0.00013739665155299008\n",
      "Epoch 3594, Loss: 0.0068951950161135755, Final Batch Loss: 0.00029768599779345095\n",
      "Epoch 3595, Loss: 0.014809692089329474, Final Batch Loss: 0.0003231067967135459\n",
      "Epoch 3596, Loss: 0.009470041448366828, Final Batch Loss: 0.002258667489513755\n",
      "Epoch 3597, Loss: 0.004561599769658642, Final Batch Loss: 0.0015792284393683076\n",
      "Epoch 3598, Loss: 0.020196186524117365, Final Batch Loss: 0.00037556883762590587\n",
      "Epoch 3599, Loss: 0.010287096782121807, Final Batch Loss: 0.0007312697707675397\n",
      "Epoch 3600, Loss: 0.005463548863190226, Final Batch Loss: 0.0010821038158610463\n",
      "Epoch 3601, Loss: 0.0025833936379058287, Final Batch Loss: 0.0004941944498568773\n",
      "Epoch 3602, Loss: 0.005965007265331224, Final Batch Loss: 0.00040121760684996843\n",
      "Epoch 3603, Loss: 0.004625581641448662, Final Batch Loss: 0.00010705241584219038\n",
      "Epoch 3604, Loss: 0.03032741998322308, Final Batch Loss: 0.001256604795344174\n",
      "Epoch 3605, Loss: 0.010894043793086894, Final Batch Loss: 0.0006473691901192069\n",
      "Epoch 3606, Loss: 0.010903250313276658, Final Batch Loss: 0.0011183968745172024\n",
      "Epoch 3607, Loss: 0.057351479830686, Final Batch Loss: 0.01156869437545538\n",
      "Epoch 3608, Loss: 0.006127347791334614, Final Batch Loss: 0.002212453167885542\n",
      "Epoch 3609, Loss: 0.028710028331261128, Final Batch Loss: 0.0013234830694273114\n",
      "Epoch 3610, Loss: 0.00817880682006944, Final Batch Loss: 0.00017095108341891319\n",
      "Epoch 3611, Loss: 0.02869932299654465, Final Batch Loss: 0.013551456853747368\n",
      "Epoch 3612, Loss: 0.0024693590676179156, Final Batch Loss: 0.00020501970720943063\n",
      "Epoch 3613, Loss: 0.04034868179587647, Final Batch Loss: 0.03682969883084297\n",
      "Epoch 3614, Loss: 0.0036652064009103924, Final Batch Loss: 0.0005393503233790398\n",
      "Epoch 3615, Loss: 0.016784733219537884, Final Batch Loss: 0.0013364024925976992\n",
      "Epoch 3616, Loss: 0.00239432358648628, Final Batch Loss: 0.00018741066742222756\n",
      "Epoch 3617, Loss: 0.031032744314870797, Final Batch Loss: 0.00015875710232648998\n",
      "Epoch 3618, Loss: 0.014636853113188408, Final Batch Loss: 0.010898713953793049\n",
      "Epoch 3619, Loss: 0.0031880974274827167, Final Batch Loss: 0.0010892251739278436\n",
      "Epoch 3620, Loss: 0.00377156853210181, Final Batch Loss: 0.0008094854420050979\n",
      "Epoch 3621, Loss: 0.007857315242290497, Final Batch Loss: 0.001713508740067482\n",
      "Epoch 3622, Loss: 0.05904096068115905, Final Batch Loss: 0.052740830928087234\n",
      "Epoch 3623, Loss: 0.012915097991935909, Final Batch Loss: 0.0007523417007178068\n",
      "Epoch 3624, Loss: 0.029519345182052348, Final Batch Loss: 6.820102134952322e-05\n",
      "Epoch 3625, Loss: 0.01421500276774168, Final Batch Loss: 0.00011460870155133307\n",
      "Epoch 3626, Loss: 0.025402813393156976, Final Batch Loss: 0.01457975059747696\n",
      "Epoch 3627, Loss: 0.005497952894074842, Final Batch Loss: 0.0013885725056752563\n",
      "Epoch 3628, Loss: 0.02198593399953097, Final Batch Loss: 0.0022111490834504366\n",
      "Epoch 3629, Loss: 0.01961669756565243, Final Batch Loss: 0.0003140526241622865\n",
      "Epoch 3630, Loss: 0.004450996231753379, Final Batch Loss: 0.0006277397042140365\n",
      "Epoch 3631, Loss: 0.026342926314100623, Final Batch Loss: 0.0029508606530725956\n",
      "Epoch 3632, Loss: 0.054548910760786384, Final Batch Loss: 0.01601031981408596\n",
      "Epoch 3633, Loss: 0.027613433660008013, Final Batch Loss: 0.021158317103981972\n",
      "Epoch 3634, Loss: 0.008484981590299867, Final Batch Loss: 0.0007202912238426507\n",
      "Epoch 3635, Loss: 0.030251958029111847, Final Batch Loss: 0.007387065328657627\n",
      "Epoch 3636, Loss: 0.01008146908134222, Final Batch Loss: 0.000676980649586767\n",
      "Epoch 3637, Loss: 0.004682477039750665, Final Batch Loss: 0.002773507498204708\n",
      "Epoch 3638, Loss: 0.026431943842908368, Final Batch Loss: 0.0007469348493032157\n",
      "Epoch 3639, Loss: 0.002844538103090599, Final Batch Loss: 0.0004092741583008319\n",
      "Epoch 3640, Loss: 0.006982109407545067, Final Batch Loss: 0.00019622802210506052\n",
      "Epoch 3641, Loss: 0.0738988799857907, Final Batch Loss: 0.00044543849071487784\n",
      "Epoch 3642, Loss: 0.017566511203767732, Final Batch Loss: 0.015303047373890877\n",
      "Epoch 3643, Loss: 0.007050965301459655, Final Batch Loss: 0.000436901900684461\n",
      "Epoch 3644, Loss: 0.023097582830814645, Final Batch Loss: 0.011019082739949226\n",
      "Epoch 3645, Loss: 0.018739147402811795, Final Batch Loss: 0.0007507949485443532\n",
      "Epoch 3646, Loss: 0.015906733053270727, Final Batch Loss: 0.0019314480014145374\n",
      "Epoch 3647, Loss: 0.017640136822592467, Final Batch Loss: 0.0034167745616286993\n",
      "Epoch 3648, Loss: 0.009236312587745488, Final Batch Loss: 0.0006881473818793893\n",
      "Epoch 3649, Loss: 0.0155933083005948, Final Batch Loss: 0.011524004861712456\n",
      "Epoch 3650, Loss: 0.026401930663269013, Final Batch Loss: 0.00031772430520504713\n",
      "Epoch 3651, Loss: 0.030749476980417967, Final Batch Loss: 0.0002603940083645284\n",
      "Epoch 3652, Loss: 0.0033899910777108744, Final Batch Loss: 0.0006719892262481153\n",
      "Epoch 3653, Loss: 0.026200297405011952, Final Batch Loss: 0.0010500256903469563\n",
      "Epoch 3654, Loss: 0.016240842727711424, Final Batch Loss: 0.006759158801287413\n",
      "Epoch 3655, Loss: 0.006375708559062332, Final Batch Loss: 0.0009519184241071343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3656, Loss: 0.009933176392223686, Final Batch Loss: 0.003777483943849802\n",
      "Epoch 3657, Loss: 0.01695902262144955, Final Batch Loss: 0.0038371330592781305\n",
      "Epoch 3658, Loss: 0.005488835973665118, Final Batch Loss: 0.0019642766565084457\n",
      "Epoch 3659, Loss: 0.010065243317512795, Final Batch Loss: 0.0003429642238188535\n",
      "Epoch 3660, Loss: 0.013388077473791782, Final Batch Loss: 0.00301568815484643\n",
      "Epoch 3661, Loss: 0.0563564341282472, Final Batch Loss: 0.028587549924850464\n",
      "Epoch 3662, Loss: 0.02105534414295107, Final Batch Loss: 0.0038845802191644907\n",
      "Epoch 3663, Loss: 0.006054383818991482, Final Batch Loss: 0.0003028824576176703\n",
      "Epoch 3664, Loss: 0.012156259326729923, Final Batch Loss: 0.0006238574860617518\n",
      "Epoch 3665, Loss: 0.003795895871007815, Final Batch Loss: 0.0006843967130407691\n",
      "Epoch 3666, Loss: 0.004006073788332287, Final Batch Loss: 0.0022781372535973787\n",
      "Epoch 3667, Loss: 0.017536561455926858, Final Batch Loss: 0.00043045158963650465\n",
      "Epoch 3668, Loss: 0.014542241377057508, Final Batch Loss: 0.0010438786121085286\n",
      "Epoch 3669, Loss: 0.019183621741831303, Final Batch Loss: 0.00036866110167466104\n",
      "Epoch 3670, Loss: 0.002099770586937666, Final Batch Loss: 0.0006630322895944118\n",
      "Epoch 3671, Loss: 0.011068967476603575, Final Batch Loss: 0.00022432779951486737\n",
      "Epoch 3672, Loss: 0.0035483524698065594, Final Batch Loss: 0.0015135383000597358\n",
      "Epoch 3673, Loss: 0.006534453568747267, Final Batch Loss: 0.0007318513235077262\n",
      "Epoch 3674, Loss: 0.00781314070627559, Final Batch Loss: 0.001110091689042747\n",
      "Epoch 3675, Loss: 0.004407530970638618, Final Batch Loss: 0.0017562470166012645\n",
      "Epoch 3676, Loss: 0.0035141266125719994, Final Batch Loss: 0.0016028727404773235\n",
      "Epoch 3677, Loss: 0.006498842965811491, Final Batch Loss: 0.000652930757496506\n",
      "Epoch 3678, Loss: 0.006782001946703531, Final Batch Loss: 0.004941790364682674\n",
      "Epoch 3679, Loss: 0.022275439550867304, Final Batch Loss: 0.01017252542078495\n",
      "Epoch 3680, Loss: 0.02050667317234911, Final Batch Loss: 0.0003741992695722729\n",
      "Epoch 3681, Loss: 0.013362598834646633, Final Batch Loss: 0.0021760680247098207\n",
      "Epoch 3682, Loss: 0.013622360871522687, Final Batch Loss: 0.00018474592070560902\n",
      "Epoch 3683, Loss: 0.00594737523351796, Final Batch Loss: 0.003354401793330908\n",
      "Epoch 3684, Loss: 0.003444459871388972, Final Batch Loss: 0.00032201979774981737\n",
      "Epoch 3685, Loss: 0.02187442730064504, Final Batch Loss: 0.00035574156208895147\n",
      "Epoch 3686, Loss: 0.006290462071774527, Final Batch Loss: 0.0003673639439512044\n",
      "Epoch 3687, Loss: 0.01259570213733241, Final Batch Loss: 0.0007856527809053659\n",
      "Epoch 3688, Loss: 0.021626471891067922, Final Batch Loss: 0.0003556909505277872\n",
      "Epoch 3689, Loss: 0.06122965758549981, Final Batch Loss: 0.024556510150432587\n",
      "Epoch 3690, Loss: 0.014970658012316562, Final Batch Loss: 0.012558051384985447\n",
      "Epoch 3691, Loss: 0.01754976728989277, Final Batch Loss: 0.00021719066717196256\n",
      "Epoch 3692, Loss: 0.014381498331204057, Final Batch Loss: 0.0004374566487967968\n",
      "Epoch 3693, Loss: 0.018629304540809244, Final Batch Loss: 0.0013726267497986555\n",
      "Epoch 3694, Loss: 0.031648399017285556, Final Batch Loss: 0.0009284447878599167\n",
      "Epoch 3695, Loss: 0.010089371120557189, Final Batch Loss: 0.0017152917571365833\n",
      "Epoch 3696, Loss: 0.012966556765604764, Final Batch Loss: 0.0004480496281757951\n",
      "Epoch 3697, Loss: 0.025148057931801304, Final Batch Loss: 0.0006193977314978838\n",
      "Epoch 3698, Loss: 0.006066917900170665, Final Batch Loss: 0.0027697335463017225\n",
      "Epoch 3699, Loss: 0.005903136858250946, Final Batch Loss: 0.0008313078433275223\n",
      "Epoch 3700, Loss: 0.02040666302491445, Final Batch Loss: 0.0005882822442799807\n",
      "Epoch 3701, Loss: 0.004744861391372979, Final Batch Loss: 0.001168382354080677\n",
      "Epoch 3702, Loss: 0.008181124285329133, Final Batch Loss: 0.0008006553980521858\n",
      "Epoch 3703, Loss: 0.0021158270974410698, Final Batch Loss: 0.00017941741680260748\n",
      "Epoch 3704, Loss: 0.03484480619954411, Final Batch Loss: 0.02030525542795658\n",
      "Epoch 3705, Loss: 0.011635985676548444, Final Batch Loss: 0.00926758162677288\n",
      "Epoch 3706, Loss: 0.009398734546266496, Final Batch Loss: 0.0005432185716927052\n",
      "Epoch 3707, Loss: 0.007832571340259165, Final Batch Loss: 0.0032626374159008265\n",
      "Epoch 3708, Loss: 0.02129930816590786, Final Batch Loss: 0.0018495196709409356\n",
      "Epoch 3709, Loss: 0.0376526391482912, Final Batch Loss: 0.005722285248339176\n",
      "Epoch 3710, Loss: 0.00541994244849775, Final Batch Loss: 0.002695067087188363\n",
      "Epoch 3711, Loss: 0.015006352521595545, Final Batch Loss: 0.0024464549496769905\n",
      "Epoch 3712, Loss: 0.011700830422341824, Final Batch Loss: 0.0004364397027529776\n",
      "Epoch 3713, Loss: 0.007266072614584118, Final Batch Loss: 0.000439735158579424\n",
      "Epoch 3714, Loss: 0.004018542342237197, Final Batch Loss: 0.00011367397382855415\n",
      "Epoch 3715, Loss: 0.005500101775396615, Final Batch Loss: 0.001074586994946003\n",
      "Epoch 3716, Loss: 0.006875430204672739, Final Batch Loss: 0.0018658834742382169\n",
      "Epoch 3717, Loss: 0.0025727317042765208, Final Batch Loss: 0.0007810475653968751\n",
      "Epoch 3718, Loss: 0.00340957852313295, Final Batch Loss: 0.0006506130448542535\n",
      "Epoch 3719, Loss: 0.008435084273514803, Final Batch Loss: 0.0007683859439566731\n",
      "Epoch 3720, Loss: 0.0028929836116731167, Final Batch Loss: 0.0005585059989243746\n",
      "Epoch 3721, Loss: 0.012386041198624298, Final Batch Loss: 0.001335896085947752\n",
      "Epoch 3722, Loss: 0.01311639508639928, Final Batch Loss: 0.011887943372130394\n",
      "Epoch 3723, Loss: 0.00799333819122694, Final Batch Loss: 0.004475622903555632\n",
      "Epoch 3724, Loss: 0.006142930156784132, Final Batch Loss: 0.0035258345305919647\n",
      "Epoch 3725, Loss: 0.0044778263254556805, Final Batch Loss: 0.0016773917013779283\n",
      "Epoch 3726, Loss: 0.006589332479052246, Final Batch Loss: 0.0010940859792754054\n",
      "Epoch 3727, Loss: 0.03110742347780615, Final Batch Loss: 0.007039123214781284\n",
      "Epoch 3728, Loss: 0.005128821387188509, Final Batch Loss: 0.0016163223190233111\n",
      "Epoch 3729, Loss: 0.026205239817500114, Final Batch Loss: 0.0008109479094855487\n",
      "Epoch 3730, Loss: 0.004132175294216722, Final Batch Loss: 0.0004135513736400753\n",
      "Epoch 3731, Loss: 0.00523227833036799, Final Batch Loss: 0.0006860315916128457\n",
      "Epoch 3732, Loss: 0.00317087399889715, Final Batch Loss: 0.0006462812307290733\n",
      "Epoch 3733, Loss: 0.0035838052499457262, Final Batch Loss: 0.0005788847920484841\n",
      "Epoch 3734, Loss: 0.002634104195749387, Final Batch Loss: 0.0006553908460773528\n",
      "Epoch 3735, Loss: 0.013776831416180357, Final Batch Loss: 0.0017213517567142844\n",
      "Epoch 3736, Loss: 0.0038883646338945255, Final Batch Loss: 0.00023781055642757565\n",
      "Epoch 3737, Loss: 0.019645703461719677, Final Batch Loss: 0.00048371581942774355\n",
      "Epoch 3738, Loss: 0.004014141857624054, Final Batch Loss: 0.00032837368780747056\n",
      "Epoch 3739, Loss: 0.011086694728874136, Final Batch Loss: 0.00026966282166540623\n",
      "Epoch 3740, Loss: 0.016370923942304216, Final Batch Loss: 0.0001687738549662754\n",
      "Epoch 3741, Loss: 0.005999820132274181, Final Batch Loss: 0.0009097994188778102\n",
      "Epoch 3742, Loss: 0.01003498406498693, Final Batch Loss: 0.003980460111051798\n",
      "Epoch 3743, Loss: 0.004715366878372151, Final Batch Loss: 6.930863310117275e-05\n",
      "Epoch 3744, Loss: 0.012143467436544597, Final Batch Loss: 0.0010660408297553658\n",
      "Epoch 3745, Loss: 0.003223042585887015, Final Batch Loss: 0.0015946043422445655\n",
      "Epoch 3746, Loss: 0.001824180748371873, Final Batch Loss: 0.0005928377504460514\n",
      "Epoch 3747, Loss: 0.03716277633793652, Final Batch Loss: 0.014290792867541313\n",
      "Epoch 3748, Loss: 0.015858682832913473, Final Batch Loss: 0.002357127843424678\n",
      "Epoch 3749, Loss: 0.024286662373924628, Final Batch Loss: 0.0031901791226118803\n",
      "Epoch 3750, Loss: 0.04991163381782826, Final Batch Loss: 0.00014145996829029173\n",
      "Epoch 3751, Loss: 0.017837207880802453, Final Batch Loss: 0.0007866303203627467\n",
      "Epoch 3752, Loss: 0.015938500087941065, Final Batch Loss: 0.0003812340146396309\n",
      "Epoch 3753, Loss: 0.017574396173586138, Final Batch Loss: 0.000506251584738493\n",
      "Epoch 3754, Loss: 0.004458441195311025, Final Batch Loss: 0.0013004450593143702\n",
      "Epoch 3755, Loss: 0.0074892093434755225, Final Batch Loss: 0.0017241438617929816\n",
      "Epoch 3756, Loss: 0.027521235722815618, Final Batch Loss: 0.0038878419436514378\n",
      "Epoch 3757, Loss: 0.06591460865456611, Final Batch Loss: 0.05055813863873482\n",
      "Epoch 3758, Loss: 0.0030186403018888086, Final Batch Loss: 0.00024028407642617822\n",
      "Epoch 3759, Loss: 0.0015843442524783313, Final Batch Loss: 0.00015484281175304204\n",
      "Epoch 3760, Loss: 0.006455918497522362, Final Batch Loss: 0.0001993186306208372\n",
      "Epoch 3761, Loss: 0.07132392660423648, Final Batch Loss: 0.00048618530854582787\n",
      "Epoch 3762, Loss: 0.010666553804185241, Final Batch Loss: 0.0008813734166324139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3763, Loss: 0.02084678284882102, Final Batch Loss: 0.0019117550691589713\n",
      "Epoch 3764, Loss: 0.025205549580277875, Final Batch Loss: 0.020117444917559624\n",
      "Epoch 3765, Loss: 0.013246286223875359, Final Batch Loss: 0.0038650904316455126\n",
      "Epoch 3766, Loss: 0.0053768601501360536, Final Batch Loss: 0.0008402711828239262\n",
      "Epoch 3767, Loss: 0.003547797299688682, Final Batch Loss: 0.000590016832575202\n",
      "Epoch 3768, Loss: 0.0014459372041528695, Final Batch Loss: 0.0001936346961883828\n",
      "Epoch 3769, Loss: 0.013993244210723788, Final Batch Loss: 0.0013588842703029513\n",
      "Epoch 3770, Loss: 0.006436292409489397, Final Batch Loss: 5.27088632225059e-05\n",
      "Epoch 3771, Loss: 0.01335637513329857, Final Batch Loss: 4.940037979395129e-05\n",
      "Epoch 3772, Loss: 0.006931472365977243, Final Batch Loss: 0.0006012637750245631\n",
      "Epoch 3773, Loss: 0.008729362452868372, Final Batch Loss: 0.0003843789454549551\n",
      "Epoch 3774, Loss: 0.00470619642874226, Final Batch Loss: 0.0013177820947021246\n",
      "Epoch 3775, Loss: 0.0052051316379220225, Final Batch Loss: 0.001431482844054699\n",
      "Epoch 3776, Loss: 0.005517948797205463, Final Batch Loss: 0.00013927038526162505\n",
      "Epoch 3777, Loss: 0.005955395361525007, Final Batch Loss: 0.00013656298688147217\n",
      "Epoch 3778, Loss: 0.009996433713240549, Final Batch Loss: 0.0008343900553882122\n",
      "Epoch 3779, Loss: 0.008712178081623279, Final Batch Loss: 0.00018052583618555218\n",
      "Epoch 3780, Loss: 0.0016388191434089094, Final Batch Loss: 0.0001162921980721876\n",
      "Epoch 3781, Loss: 0.0036186825891491026, Final Batch Loss: 0.0007662290590815246\n",
      "Epoch 3782, Loss: 0.0007223123575386126, Final Batch Loss: 0.00019085065287072212\n",
      "Epoch 3783, Loss: 0.008341964188730344, Final Batch Loss: 0.0038471634034067392\n",
      "Epoch 3784, Loss: 0.005586762199527584, Final Batch Loss: 0.0006235574837774038\n",
      "Epoch 3785, Loss: 0.002932906078058295, Final Batch Loss: 0.00036130216903984547\n",
      "Epoch 3786, Loss: 0.026391355611849576, Final Batch Loss: 0.0007078838534653187\n",
      "Epoch 3787, Loss: 0.011650167478364892, Final Batch Loss: 0.0010423585772514343\n",
      "Epoch 3788, Loss: 0.005040296658989973, Final Batch Loss: 0.0012688415590673685\n",
      "Epoch 3789, Loss: 0.003414756662095897, Final Batch Loss: 0.00023479005903936923\n",
      "Epoch 3790, Loss: 0.0064299570803996176, Final Batch Loss: 0.00036210601683706045\n",
      "Epoch 3791, Loss: 0.0014910142563167028, Final Batch Loss: 0.0004074180033057928\n",
      "Epoch 3792, Loss: 0.006360523766488768, Final Batch Loss: 0.00014717398153152317\n",
      "Epoch 3793, Loss: 0.006355025645461865, Final Batch Loss: 0.005187649745494127\n",
      "Epoch 3794, Loss: 0.002218483336037025, Final Batch Loss: 0.0002087691827910021\n",
      "Epoch 3795, Loss: 0.03289474724442698, Final Batch Loss: 0.0006546408985741436\n",
      "Epoch 3796, Loss: 0.0028219104860909283, Final Batch Loss: 0.00032110593747347593\n",
      "Epoch 3797, Loss: 0.0018960562156280503, Final Batch Loss: 0.00044183581485413015\n",
      "Epoch 3798, Loss: 0.01852015469557955, Final Batch Loss: 0.002881336957216263\n",
      "Epoch 3799, Loss: 0.004193426058918703, Final Batch Loss: 0.0001970332086784765\n",
      "Epoch 3800, Loss: 0.005589400476310402, Final Batch Loss: 0.0016113384626805782\n",
      "Epoch 3801, Loss: 0.014051202379050665, Final Batch Loss: 0.00534390565007925\n",
      "Epoch 3802, Loss: 0.004582607893098611, Final Batch Loss: 0.0008880668319761753\n",
      "Epoch 3803, Loss: 0.021507888130145147, Final Batch Loss: 0.0006553211132995784\n",
      "Epoch 3804, Loss: 0.028620519253308885, Final Batch Loss: 9.502259490545839e-05\n",
      "Epoch 3805, Loss: 0.03172439186164411, Final Batch Loss: 0.001369881210848689\n",
      "Epoch 3806, Loss: 0.01126278901938349, Final Batch Loss: 0.00041781237814575434\n",
      "Epoch 3807, Loss: 0.006854725506855175, Final Batch Loss: 0.005064242519438267\n",
      "Epoch 3808, Loss: 0.022398778761271387, Final Batch Loss: 0.0017053253250196576\n",
      "Epoch 3809, Loss: 0.06015663004654925, Final Batch Loss: 0.05783349275588989\n",
      "Epoch 3810, Loss: 0.0019416229479247704, Final Batch Loss: 0.0002999546704813838\n",
      "Epoch 3811, Loss: 0.03722642522188835, Final Batch Loss: 0.016036979854106903\n",
      "Epoch 3812, Loss: 0.007792433490976691, Final Batch Loss: 0.0016837810399010777\n",
      "Epoch 3813, Loss: 0.005938667643931694, Final Batch Loss: 0.0007559657096862793\n",
      "Epoch 3814, Loss: 0.023747507308144122, Final Batch Loss: 0.006634098943322897\n",
      "Epoch 3815, Loss: 0.005645525176078081, Final Batch Loss: 0.0011451630853116512\n",
      "Epoch 3816, Loss: 0.0072282499459106475, Final Batch Loss: 0.003450164571404457\n",
      "Epoch 3817, Loss: 0.009840303129749373, Final Batch Loss: 0.002103312872350216\n",
      "Epoch 3818, Loss: 0.008505893201800063, Final Batch Loss: 0.000774877262301743\n",
      "Epoch 3819, Loss: 0.04083493506186642, Final Batch Loss: 0.0035858494229614735\n",
      "Epoch 3820, Loss: 0.00530996554880403, Final Batch Loss: 0.0006532481638714671\n",
      "Epoch 3821, Loss: 0.008271330952993594, Final Batch Loss: 0.0005516543169505894\n",
      "Epoch 3822, Loss: 0.004501378512941301, Final Batch Loss: 0.0002880801330320537\n",
      "Epoch 3823, Loss: 0.0029892660895711742, Final Batch Loss: 0.00043788939365185797\n",
      "Epoch 3824, Loss: 0.009581977479683701, Final Batch Loss: 0.000848283467348665\n",
      "Epoch 3825, Loss: 0.00915389053261606, Final Batch Loss: 0.0003279480151832104\n",
      "Epoch 3826, Loss: 0.005042257354944013, Final Batch Loss: 0.0006983538623899221\n",
      "Epoch 3827, Loss: 0.03178975183982402, Final Batch Loss: 0.0001284206664422527\n",
      "Epoch 3828, Loss: 0.018188315545557998, Final Batch Loss: 0.00033604007330723107\n",
      "Epoch 3829, Loss: 0.008343885274371132, Final Batch Loss: 0.0004630130424629897\n",
      "Epoch 3830, Loss: 0.01635685381188523, Final Batch Loss: 0.002721004420891404\n",
      "Epoch 3831, Loss: 0.011392627347959206, Final Batch Loss: 0.009087820537388325\n",
      "Epoch 3832, Loss: 0.04014120375359198, Final Batch Loss: 7.132361497497186e-05\n",
      "Epoch 3833, Loss: 0.00913613494776655, Final Batch Loss: 0.00023921666434034705\n",
      "Epoch 3834, Loss: 0.0065276756504317746, Final Batch Loss: 7.771582750137895e-05\n",
      "Epoch 3835, Loss: 0.001689140968665015, Final Batch Loss: 7.556229684269056e-05\n",
      "Epoch 3836, Loss: 0.005494173841725569, Final Batch Loss: 0.00023241898452397436\n",
      "Epoch 3837, Loss: 0.04318555877398467, Final Batch Loss: 0.03036615252494812\n",
      "Epoch 3838, Loss: 0.021355119766667485, Final Batch Loss: 0.01606639474630356\n",
      "Epoch 3839, Loss: 0.00841759832110256, Final Batch Loss: 0.005464863032102585\n",
      "Epoch 3840, Loss: 0.05268131689081201, Final Batch Loss: 7.607244333485141e-05\n",
      "Epoch 3841, Loss: 0.00826776678150054, Final Batch Loss: 0.0032407904509454966\n",
      "Epoch 3842, Loss: 0.009120011469349265, Final Batch Loss: 0.0032411094289273024\n",
      "Epoch 3843, Loss: 0.01399965607561171, Final Batch Loss: 0.003803886938840151\n",
      "Epoch 3844, Loss: 0.024281009966216516, Final Batch Loss: 0.00030508788768202066\n",
      "Epoch 3845, Loss: 0.029784426267724484, Final Batch Loss: 0.0031246684957295656\n",
      "Epoch 3846, Loss: 0.003579604992410168, Final Batch Loss: 0.0005318914190866053\n",
      "Epoch 3847, Loss: 0.007783579567330889, Final Batch Loss: 0.00010680097329895943\n",
      "Epoch 3848, Loss: 0.017312038282398134, Final Batch Loss: 0.012311777099967003\n",
      "Epoch 3849, Loss: 0.01222161139594391, Final Batch Loss: 0.006783858872950077\n",
      "Epoch 3850, Loss: 0.018138498402549885, Final Batch Loss: 0.010739403776824474\n",
      "Epoch 3851, Loss: 0.04589207185199484, Final Batch Loss: 0.0028939435724169016\n",
      "Epoch 3852, Loss: 0.028313641916611232, Final Batch Loss: 0.0007885118247941136\n",
      "Epoch 3853, Loss: 0.02935833839001134, Final Batch Loss: 0.006492369342595339\n",
      "Epoch 3854, Loss: 0.04235260980203748, Final Batch Loss: 0.00021384208230301738\n",
      "Epoch 3855, Loss: 0.004496008492424153, Final Batch Loss: 0.00020788656547665596\n",
      "Epoch 3856, Loss: 0.07016333180945367, Final Batch Loss: 0.002673462498933077\n",
      "Epoch 3857, Loss: 0.010551498882705346, Final Batch Loss: 0.00024136187857948244\n",
      "Epoch 3858, Loss: 0.020546586252748966, Final Batch Loss: 0.0007959076901897788\n",
      "Epoch 3859, Loss: 0.036231787293218076, Final Batch Loss: 0.0013615868519991636\n",
      "Epoch 3860, Loss: 0.006354858123813756, Final Batch Loss: 0.00019140781660098583\n",
      "Epoch 3861, Loss: 0.009091276151593775, Final Batch Loss: 0.0017893194453790784\n",
      "Epoch 3862, Loss: 0.00291803918662481, Final Batch Loss: 0.0006900040898472071\n",
      "Epoch 3863, Loss: 0.00814956141402945, Final Batch Loss: 0.0012181594502180815\n",
      "Epoch 3864, Loss: 0.012211337030748837, Final Batch Loss: 0.00019733810040634125\n",
      "Epoch 3865, Loss: 0.009037896321387962, Final Batch Loss: 0.0011673647677525878\n",
      "Epoch 3866, Loss: 0.04341867030598223, Final Batch Loss: 0.0004971636226400733\n",
      "Epoch 3867, Loss: 0.005170505108253565, Final Batch Loss: 0.0012234053574502468\n",
      "Epoch 3868, Loss: 0.0028180807566968724, Final Batch Loss: 0.0008131750510074198\n",
      "Epoch 3869, Loss: 0.022088648795033805, Final Batch Loss: 0.002652790630236268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3870, Loss: 0.00636179797584191, Final Batch Loss: 0.0007518472266383469\n",
      "Epoch 3871, Loss: 0.004093213035957888, Final Batch Loss: 0.0006550619145855308\n",
      "Epoch 3872, Loss: 0.024375729306484573, Final Batch Loss: 0.0008646701462566853\n",
      "Epoch 3873, Loss: 0.0023039475272526033, Final Batch Loss: 0.00010930839198408648\n",
      "Epoch 3874, Loss: 0.01307819620706141, Final Batch Loss: 0.00039508158806711435\n",
      "Epoch 3875, Loss: 0.028480917098931968, Final Batch Loss: 0.009850537404417992\n",
      "Epoch 3876, Loss: 0.032383145095082, Final Batch Loss: 0.021439988166093826\n",
      "Epoch 3877, Loss: 0.032736884750192985, Final Batch Loss: 0.020390482619404793\n",
      "Epoch 3878, Loss: 0.008946811598434579, Final Batch Loss: 0.003854301990941167\n",
      "Epoch 3879, Loss: 0.04227073123911396, Final Batch Loss: 0.028882725164294243\n",
      "Epoch 3880, Loss: 0.10176518047228456, Final Batch Loss: 0.04377350956201553\n",
      "Epoch 3881, Loss: 0.008471674125758, Final Batch Loss: 0.00045462188427336514\n",
      "Epoch 3882, Loss: 0.014456250122748315, Final Batch Loss: 0.010306792333722115\n",
      "Epoch 3883, Loss: 0.004963934421539307, Final Batch Loss: 0.0005187970236875117\n",
      "Epoch 3884, Loss: 0.005331754684448242, Final Batch Loss: 0.0005003087571822107\n",
      "Epoch 3885, Loss: 0.026557887700619176, Final Batch Loss: 0.001836742158047855\n",
      "Epoch 3886, Loss: 0.00920546829001978, Final Batch Loss: 0.0009520318126305938\n",
      "Epoch 3887, Loss: 0.011995657405350357, Final Batch Loss: 0.0005023967823944986\n",
      "Epoch 3888, Loss: 0.0341898983460851, Final Batch Loss: 0.006748422048985958\n",
      "Epoch 3889, Loss: 0.006379556405590847, Final Batch Loss: 0.0007732283556833863\n",
      "Epoch 3890, Loss: 0.005196281170356087, Final Batch Loss: 0.0019580444786697626\n",
      "Epoch 3891, Loss: 0.004649011374567635, Final Batch Loss: 0.001659719506278634\n",
      "Epoch 3892, Loss: 0.003027746395673603, Final Batch Loss: 0.00016653412603773177\n",
      "Epoch 3893, Loss: 0.0017633316019782797, Final Batch Loss: 0.00012017771950922906\n",
      "Epoch 3894, Loss: 0.012731180730042979, Final Batch Loss: 0.00015579385217279196\n",
      "Epoch 3895, Loss: 0.004039844585349783, Final Batch Loss: 0.000353561423253268\n",
      "Epoch 3896, Loss: 0.034867925132857636, Final Batch Loss: 0.00032159758848138154\n",
      "Epoch 3897, Loss: 0.00469839051947929, Final Batch Loss: 0.00035059908987022936\n",
      "Epoch 3898, Loss: 0.02406945284747053, Final Batch Loss: 0.0011640964075922966\n",
      "Epoch 3899, Loss: 0.01572036780999042, Final Batch Loss: 0.0015474315732717514\n",
      "Epoch 3900, Loss: 0.022589720494579524, Final Batch Loss: 0.0011305005755275488\n",
      "Epoch 3901, Loss: 0.005788336566183716, Final Batch Loss: 0.000747813144698739\n",
      "Epoch 3902, Loss: 0.01628539845114574, Final Batch Loss: 0.014457234181463718\n",
      "Epoch 3903, Loss: 0.02061068103648722, Final Batch Loss: 0.0007068362319841981\n",
      "Epoch 3904, Loss: 0.03837130271131173, Final Batch Loss: 0.0006784560973756015\n",
      "Epoch 3905, Loss: 0.022346771438606083, Final Batch Loss: 0.0027039023116230965\n",
      "Epoch 3906, Loss: 0.016059759000199847, Final Batch Loss: 0.00017878452490549535\n",
      "Epoch 3907, Loss: 0.03503464368986897, Final Batch Loss: 0.005743863061070442\n",
      "Epoch 3908, Loss: 0.014992695054388605, Final Batch Loss: 0.006150299683213234\n",
      "Epoch 3909, Loss: 0.018444136243488174, Final Batch Loss: 0.0003787917667068541\n",
      "Epoch 3910, Loss: 0.008177539275493473, Final Batch Loss: 0.002672323491424322\n",
      "Epoch 3911, Loss: 0.005555224779527634, Final Batch Loss: 0.0005680493195541203\n",
      "Epoch 3912, Loss: 0.032122389107826166, Final Batch Loss: 0.00028855004347860813\n",
      "Epoch 3913, Loss: 0.00962383377191145, Final Batch Loss: 0.0008089906768873334\n",
      "Epoch 3914, Loss: 0.0167104973224923, Final Batch Loss: 0.00046488712541759014\n",
      "Epoch 3915, Loss: 0.008427022643445525, Final Batch Loss: 0.004016861319541931\n",
      "Epoch 3916, Loss: 0.02719298926240299, Final Batch Loss: 0.00016242686251644045\n",
      "Epoch 3917, Loss: 0.0192168696958106, Final Batch Loss: 0.0013525892281904817\n",
      "Epoch 3918, Loss: 0.013598217978142202, Final Batch Loss: 0.004096530843526125\n",
      "Epoch 3919, Loss: 0.010024321905802935, Final Batch Loss: 0.0005113931256346405\n",
      "Epoch 3920, Loss: 0.03283280416508205, Final Batch Loss: 0.00010972624295391142\n",
      "Epoch 3921, Loss: 0.033950839715544134, Final Batch Loss: 0.006023653317242861\n",
      "Epoch 3922, Loss: 0.013372365850955248, Final Batch Loss: 0.004419091157615185\n",
      "Epoch 3923, Loss: 0.05532950605265796, Final Batch Loss: 0.0007016464369371533\n",
      "Epoch 3924, Loss: 0.0027489221247378737, Final Batch Loss: 0.0003324432473164052\n",
      "Epoch 3925, Loss: 0.007383529795333743, Final Batch Loss: 0.0007587465224787593\n",
      "Epoch 3926, Loss: 0.0024351952015422285, Final Batch Loss: 0.0006119873141869903\n",
      "Epoch 3927, Loss: 0.00906571646919474, Final Batch Loss: 0.00037947320379316807\n",
      "Epoch 3928, Loss: 0.008565737633034587, Final Batch Loss: 0.0008741368656046689\n",
      "Epoch 3929, Loss: 0.018838616058928892, Final Batch Loss: 0.00758442934602499\n",
      "Epoch 3930, Loss: 0.006033748031768482, Final Batch Loss: 0.00011098410323029384\n",
      "Epoch 3931, Loss: 0.009930797277775127, Final Batch Loss: 8.585346949985251e-05\n",
      "Epoch 3932, Loss: 0.00900159458979033, Final Batch Loss: 0.0005817306227982044\n",
      "Epoch 3933, Loss: 0.0063770070373720955, Final Batch Loss: 4.0405819163424894e-05\n",
      "Epoch 3934, Loss: 0.005883473641006276, Final Batch Loss: 0.0002878208179026842\n",
      "Epoch 3935, Loss: 0.0041423431248404086, Final Batch Loss: 0.0005178045830689371\n",
      "Epoch 3936, Loss: 0.0015053846000228077, Final Batch Loss: 0.00027211368433199823\n",
      "Epoch 3937, Loss: 0.005929545688559301, Final Batch Loss: 0.0004078493220731616\n",
      "Epoch 3938, Loss: 0.008956214987847488, Final Batch Loss: 0.00012106124631827697\n",
      "Epoch 3939, Loss: 0.010079023850266822, Final Batch Loss: 0.0048488532193005085\n",
      "Epoch 3940, Loss: 0.011292307259282097, Final Batch Loss: 0.009038799442350864\n",
      "Epoch 3941, Loss: 0.011670710329781286, Final Batch Loss: 0.0008681874605827034\n",
      "Epoch 3942, Loss: 0.0017162391377496533, Final Batch Loss: 0.0007318899733945727\n",
      "Epoch 3943, Loss: 0.007547342967882287, Final Batch Loss: 0.0004218655521981418\n",
      "Epoch 3944, Loss: 0.0043357819522498176, Final Batch Loss: 0.00017983821453526616\n",
      "Epoch 3945, Loss: 0.009093208471313119, Final Batch Loss: 0.00014155454118736088\n",
      "Epoch 3946, Loss: 0.0018307381396880373, Final Batch Loss: 0.00029457322671078146\n",
      "Epoch 3947, Loss: 0.001423504829290323, Final Batch Loss: 0.00024057322298176587\n",
      "Epoch 3948, Loss: 0.005519860009371769, Final Batch Loss: 0.0039037230890244246\n",
      "Epoch 3949, Loss: 0.002529674209654331, Final Batch Loss: 0.00014803631347604096\n",
      "Epoch 3950, Loss: 0.032425349731056485, Final Batch Loss: 0.006820399779826403\n",
      "Epoch 3951, Loss: 0.03171236644266173, Final Batch Loss: 0.0015966969076544046\n",
      "Epoch 3952, Loss: 0.004277818214177387, Final Batch Loss: 0.0002548885822761804\n",
      "Epoch 3953, Loss: 0.030097429116722196, Final Batch Loss: 0.02333074063062668\n",
      "Epoch 3954, Loss: 0.03157778041349957, Final Batch Loss: 0.0015369064640253782\n",
      "Epoch 3955, Loss: 0.012454716939828359, Final Batch Loss: 0.009341361932456493\n",
      "Epoch 3956, Loss: 0.03340788680361584, Final Batch Loss: 0.011208844371140003\n",
      "Epoch 3957, Loss: 0.006072732823668048, Final Batch Loss: 0.0007268300978466868\n",
      "Epoch 3958, Loss: 0.00586557958740741, Final Batch Loss: 0.0010184850543737411\n",
      "Epoch 3959, Loss: 0.020005899321404286, Final Batch Loss: 0.0012935166014358401\n",
      "Epoch 3960, Loss: 0.004779982118634507, Final Batch Loss: 0.002604608191177249\n",
      "Epoch 3961, Loss: 0.05376171710668132, Final Batch Loss: 0.0010464390506967902\n",
      "Epoch 3962, Loss: 0.0012217662515467964, Final Batch Loss: 0.00042878524982370436\n",
      "Epoch 3963, Loss: 0.007108389363565948, Final Batch Loss: 0.0002592273522168398\n",
      "Epoch 3964, Loss: 0.008295170875499025, Final Batch Loss: 0.0003843237936962396\n",
      "Epoch 3965, Loss: 0.04803328274283558, Final Batch Loss: 0.0006506078643724322\n",
      "Epoch 3966, Loss: 0.029744980041868985, Final Batch Loss: 0.024225512519478798\n",
      "Epoch 3967, Loss: 0.015449363258085214, Final Batch Loss: 0.00016014384164009243\n",
      "Epoch 3968, Loss: 0.013780499109998345, Final Batch Loss: 0.0015561467735096812\n",
      "Epoch 3969, Loss: 0.018715044134296477, Final Batch Loss: 0.0009609017288312316\n",
      "Epoch 3970, Loss: 0.01027988143323455, Final Batch Loss: 0.003463401459157467\n",
      "Epoch 3971, Loss: 0.0505786385765532, Final Batch Loss: 0.00017297112208325416\n",
      "Epoch 3972, Loss: 0.005409050441812724, Final Batch Loss: 0.0009795700898393989\n",
      "Epoch 3973, Loss: 0.0053444792501977645, Final Batch Loss: 0.00046300061512738466\n",
      "Epoch 3974, Loss: 0.003296293711173348, Final Batch Loss: 0.00019701245764736086\n",
      "Epoch 3975, Loss: 0.0032033674942795187, Final Batch Loss: 0.0002268279786221683\n",
      "Epoch 3976, Loss: 0.023084398068021983, Final Batch Loss: 0.0004920961800962687\n",
      "Epoch 3977, Loss: 0.018669293611310422, Final Batch Loss: 0.006675891112536192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3978, Loss: 0.005795431265141815, Final Batch Loss: 0.004070211201906204\n",
      "Epoch 3979, Loss: 0.013084225342026912, Final Batch Loss: 0.00896899402141571\n",
      "Epoch 3980, Loss: 0.00806910078972578, Final Batch Loss: 0.002183932811021805\n",
      "Epoch 3981, Loss: 0.014444202010054141, Final Batch Loss: 0.0005546698230318725\n",
      "Epoch 3982, Loss: 0.026639889809302986, Final Batch Loss: 0.0029783688951283693\n",
      "Epoch 3983, Loss: 0.01915446039492963, Final Batch Loss: 0.00021721425582654774\n",
      "Epoch 3984, Loss: 0.016620609356323257, Final Batch Loss: 0.015594993717968464\n",
      "Epoch 3985, Loss: 0.013307538582012057, Final Batch Loss: 0.00532790319994092\n",
      "Epoch 3986, Loss: 0.011111475265352055, Final Batch Loss: 0.0021856960374861956\n",
      "Epoch 3987, Loss: 0.00899526935245376, Final Batch Loss: 0.0002089633053401485\n",
      "Epoch 3988, Loss: 0.011216585779038724, Final Batch Loss: 8.951380004873499e-05\n",
      "Epoch 3989, Loss: 0.007423129689414054, Final Batch Loss: 0.00042846117867156863\n",
      "Epoch 3990, Loss: 0.04107391738216393, Final Batch Loss: 0.025679290294647217\n",
      "Epoch 3991, Loss: 0.01736721009365283, Final Batch Loss: 0.00029668802744708955\n",
      "Epoch 3992, Loss: 0.012199592820252292, Final Batch Loss: 0.0001705323375063017\n",
      "Epoch 3993, Loss: 0.013488690834492445, Final Batch Loss: 0.0011567942565307021\n",
      "Epoch 3994, Loss: 0.018935222571599297, Final Batch Loss: 0.0020119044929742813\n",
      "Epoch 3995, Loss: 0.016172179457498714, Final Batch Loss: 0.00011397511116228998\n",
      "Epoch 3996, Loss: 0.008150331370416097, Final Batch Loss: 0.0007221827399916947\n",
      "Epoch 3997, Loss: 0.0077669112652074546, Final Batch Loss: 0.00041025731479749084\n",
      "Epoch 3998, Loss: 0.003432181416428648, Final Batch Loss: 0.0003776935045607388\n",
      "Epoch 3999, Loss: 0.01540045120054856, Final Batch Loss: 0.0002634158299770206\n",
      "Epoch 4000, Loss: 0.002731794462306425, Final Batch Loss: 0.00047433379222638905\n",
      "Epoch 4001, Loss: 0.006766700113075785, Final Batch Loss: 0.0011591896181926131\n",
      "Epoch 4002, Loss: 0.01729889487614855, Final Batch Loss: 0.000969324610196054\n",
      "Epoch 4003, Loss: 0.04828934071701951, Final Batch Loss: 0.03268696367740631\n",
      "Epoch 4004, Loss: 0.0019830816163448617, Final Batch Loss: 0.00039422561530955136\n",
      "Epoch 4005, Loss: 0.04695184822776355, Final Batch Loss: 0.015566367655992508\n",
      "Epoch 4006, Loss: 0.03163099000812508, Final Batch Loss: 0.000284966838080436\n",
      "Epoch 4007, Loss: 0.0059941315921605565, Final Batch Loss: 0.004108169116079807\n",
      "Epoch 4008, Loss: 0.009790405150852166, Final Batch Loss: 0.001492472947575152\n",
      "Epoch 4009, Loss: 0.008837165747536346, Final Batch Loss: 0.0008069589966908097\n",
      "Epoch 4010, Loss: 0.015420835523400456, Final Batch Loss: 0.011633029207587242\n",
      "Epoch 4011, Loss: 0.011052400339394808, Final Batch Loss: 0.00017369890701957047\n",
      "Epoch 4012, Loss: 0.022170796160935424, Final Batch Loss: 0.0031460695900022984\n",
      "Epoch 4013, Loss: 0.02671085205656709, Final Batch Loss: 0.0006907593342475593\n",
      "Epoch 4014, Loss: 0.02779167341941502, Final Batch Loss: 6.463886529672891e-05\n",
      "Epoch 4015, Loss: 0.012332014739513397, Final Batch Loss: 0.0012503709876909852\n",
      "Epoch 4016, Loss: 0.016643421386106638, Final Batch Loss: 5.737505489378236e-05\n",
      "Epoch 4017, Loss: 0.0026992358616553247, Final Batch Loss: 0.00028019302408210933\n",
      "Epoch 4018, Loss: 0.0064595252624712884, Final Batch Loss: 0.0006347119924612343\n",
      "Epoch 4019, Loss: 0.004362930339993909, Final Batch Loss: 0.0013051582500338554\n",
      "Epoch 4020, Loss: 0.003452406555879861, Final Batch Loss: 0.0018582931952551007\n",
      "Epoch 4021, Loss: 0.003021363911102526, Final Batch Loss: 0.00011426360288169235\n",
      "Epoch 4022, Loss: 0.004150815977482125, Final Batch Loss: 0.0024817094672471285\n",
      "Epoch 4023, Loss: 0.0015813824720680714, Final Batch Loss: 0.00010834565910045058\n",
      "Epoch 4024, Loss: 0.00805675217998214, Final Batch Loss: 0.0010385621571913362\n",
      "Epoch 4025, Loss: 0.031308761273976415, Final Batch Loss: 0.003849107539281249\n",
      "Epoch 4026, Loss: 0.0037701838518842123, Final Batch Loss: 0.0010551433078944683\n",
      "Epoch 4027, Loss: 0.021062730520498008, Final Batch Loss: 0.012793852016329765\n",
      "Epoch 4028, Loss: 0.011403448905184632, Final Batch Loss: 0.0019592733588069677\n",
      "Epoch 4029, Loss: 0.00353829626692459, Final Batch Loss: 0.0012895057443529367\n",
      "Epoch 4030, Loss: 0.003295441303635016, Final Batch Loss: 0.0002484256692696363\n",
      "Epoch 4031, Loss: 0.005062938027549535, Final Batch Loss: 0.0004614026111084968\n",
      "Epoch 4032, Loss: 0.021048352122306824, Final Batch Loss: 0.0181408803910017\n",
      "Epoch 4033, Loss: 0.008475895308947656, Final Batch Loss: 0.00037106071249581873\n",
      "Epoch 4034, Loss: 0.011104685807367787, Final Batch Loss: 0.007016170769929886\n",
      "Epoch 4035, Loss: 0.016023492702515796, Final Batch Loss: 0.005093690007925034\n",
      "Epoch 4036, Loss: 0.01360274205217138, Final Batch Loss: 0.004573251120746136\n",
      "Epoch 4037, Loss: 0.010595070198178291, Final Batch Loss: 0.0005126101314090192\n",
      "Epoch 4038, Loss: 0.01852542577762506, Final Batch Loss: 0.00033116008853539824\n",
      "Epoch 4039, Loss: 0.0017750824481481686, Final Batch Loss: 0.00047780413297005\n",
      "Epoch 4040, Loss: 0.015555287194729317, Final Batch Loss: 0.011495504528284073\n",
      "Epoch 4041, Loss: 0.0021038586055510677, Final Batch Loss: 7.395419379463419e-05\n",
      "Epoch 4042, Loss: 0.022842714955913834, Final Batch Loss: 0.005519545637071133\n",
      "Epoch 4043, Loss: 0.030422008065215778, Final Batch Loss: 0.0002542926522437483\n",
      "Epoch 4044, Loss: 0.004320068372180685, Final Batch Loss: 0.0021643282379955053\n",
      "Epoch 4045, Loss: 0.037313018066925, Final Batch Loss: 0.00019707520550582558\n",
      "Epoch 4046, Loss: 0.0036370430170791224, Final Batch Loss: 0.000142212156788446\n",
      "Epoch 4047, Loss: 0.018076801687129773, Final Batch Loss: 5.8916586567647755e-05\n",
      "Epoch 4048, Loss: 0.01104031188879162, Final Batch Loss: 0.0006566097727045417\n",
      "Epoch 4049, Loss: 0.015372759400634095, Final Batch Loss: 0.00039798137731850147\n",
      "Epoch 4050, Loss: 0.02211855715722777, Final Batch Loss: 0.0005143304006196558\n",
      "Epoch 4051, Loss: 0.026856559212319553, Final Batch Loss: 0.0021156014408916235\n",
      "Epoch 4052, Loss: 0.043116836342960596, Final Batch Loss: 0.003494929987937212\n",
      "Epoch 4053, Loss: 0.009303866419941187, Final Batch Loss: 0.003203286789357662\n",
      "Epoch 4054, Loss: 0.015910836722468957, Final Batch Loss: 0.005036058370023966\n",
      "Epoch 4055, Loss: 0.020401832298375666, Final Batch Loss: 0.0027676280587911606\n",
      "Epoch 4056, Loss: 0.005343825207091868, Final Batch Loss: 0.00017226564523298293\n",
      "Epoch 4057, Loss: 0.012064412600011565, Final Batch Loss: 9.286326530855149e-05\n",
      "Epoch 4058, Loss: 0.003786361718084663, Final Batch Loss: 0.002399464137852192\n",
      "Epoch 4059, Loss: 0.04072619084035978, Final Batch Loss: 0.024661747738718987\n",
      "Epoch 4060, Loss: 0.037655620923032984, Final Batch Loss: 0.02835051529109478\n",
      "Epoch 4061, Loss: 0.006350104173179716, Final Batch Loss: 0.00132213463075459\n",
      "Epoch 4062, Loss: 0.07323419535532594, Final Batch Loss: 0.0003678606008179486\n",
      "Epoch 4063, Loss: 0.012239562551258132, Final Batch Loss: 0.001944354153238237\n",
      "Epoch 4064, Loss: 0.03454966962453909, Final Batch Loss: 0.0003095050051342696\n",
      "Epoch 4065, Loss: 0.040686885942704976, Final Batch Loss: 0.011182804591953754\n",
      "Epoch 4066, Loss: 0.01617376579088159, Final Batch Loss: 0.0003316644870210439\n",
      "Epoch 4067, Loss: 0.010027068346971646, Final Batch Loss: 0.0005824019317515194\n",
      "Epoch 4068, Loss: 0.04361897805938497, Final Batch Loss: 0.02643181011080742\n",
      "Epoch 4069, Loss: 0.0564946603262797, Final Batch Loss: 0.0005933739012107253\n",
      "Epoch 4070, Loss: 0.003038085254956968, Final Batch Loss: 0.0012477858690544963\n",
      "Epoch 4071, Loss: 0.0072965749859577045, Final Batch Loss: 0.002991654444485903\n",
      "Epoch 4072, Loss: 0.011027409316739067, Final Batch Loss: 0.0007650711922906339\n",
      "Epoch 4073, Loss: 0.027854820480570197, Final Batch Loss: 0.024825643748044968\n",
      "Epoch 4074, Loss: 0.006956835131859407, Final Batch Loss: 0.0009454932296648622\n",
      "Epoch 4075, Loss: 0.021743921417510137, Final Batch Loss: 0.001768141402862966\n",
      "Epoch 4076, Loss: 0.025295620958786458, Final Batch Loss: 0.01231509167701006\n",
      "Epoch 4077, Loss: 0.014606606215238571, Final Batch Loss: 9.890308137983084e-05\n",
      "Epoch 4078, Loss: 0.005053853208664805, Final Batch Loss: 0.000975819886662066\n",
      "Epoch 4079, Loss: 0.03271815582411364, Final Batch Loss: 0.0013996808556839824\n",
      "Epoch 4080, Loss: 0.008823295880574733, Final Batch Loss: 0.0008804641547612846\n",
      "Epoch 4081, Loss: 0.009502736444119364, Final Batch Loss: 0.0005227591609582305\n",
      "Epoch 4082, Loss: 0.020634432905353606, Final Batch Loss: 0.0008674971759319305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4083, Loss: 0.020144239446381107, Final Batch Loss: 0.0009978115558624268\n",
      "Epoch 4084, Loss: 0.017879989492939785, Final Batch Loss: 0.0002225337375421077\n",
      "Epoch 4085, Loss: 0.010833962325705215, Final Batch Loss: 0.0009379350231029093\n",
      "Epoch 4086, Loss: 0.017858982959296554, Final Batch Loss: 0.0003491095849312842\n",
      "Epoch 4087, Loss: 0.00380696592037566, Final Batch Loss: 0.000878799706697464\n",
      "Epoch 4088, Loss: 0.020437044644495472, Final Batch Loss: 0.0012920026201754808\n",
      "Epoch 4089, Loss: 0.00620457052718848, Final Batch Loss: 0.0003509975504130125\n",
      "Epoch 4090, Loss: 0.00867136416491121, Final Batch Loss: 0.0015748640289530158\n",
      "Epoch 4091, Loss: 0.0027809974417323247, Final Batch Loss: 0.0014358272310346365\n",
      "Epoch 4092, Loss: 0.0020480027233134024, Final Batch Loss: 0.00027048736228607595\n",
      "Epoch 4093, Loss: 0.007229030878079357, Final Batch Loss: 4.8854552005650476e-05\n",
      "Epoch 4094, Loss: 0.008144495412125252, Final Batch Loss: 0.00024271740403492004\n",
      "Epoch 4095, Loss: 0.004293708654586226, Final Batch Loss: 0.0017479953821748495\n",
      "Epoch 4096, Loss: 0.009082419448532164, Final Batch Loss: 0.00662599503993988\n",
      "Epoch 4097, Loss: 0.002491281775292009, Final Batch Loss: 0.0003469260991550982\n",
      "Epoch 4098, Loss: 0.005145185670698993, Final Batch Loss: 0.00016842248442117125\n",
      "Epoch 4099, Loss: 0.0019392141693970188, Final Batch Loss: 0.00033703650115057826\n",
      "Epoch 4100, Loss: 0.010804531120811589, Final Batch Loss: 9.673765453044325e-05\n",
      "Epoch 4101, Loss: 0.0024307738931383938, Final Batch Loss: 0.0011387007543817163\n",
      "Epoch 4102, Loss: 0.0054792356822872534, Final Batch Loss: 0.0002910285256803036\n",
      "Epoch 4103, Loss: 0.11272882603225298, Final Batch Loss: 0.00022384049952961504\n",
      "Epoch 4104, Loss: 0.03172830412222538, Final Batch Loss: 0.0002865311107598245\n",
      "Epoch 4105, Loss: 0.007338842100580223, Final Batch Loss: 0.00028625386767089367\n",
      "Epoch 4106, Loss: 0.021351931034587324, Final Batch Loss: 0.0007629754254594445\n",
      "Epoch 4107, Loss: 0.02156631846446544, Final Batch Loss: 0.0031229257583618164\n",
      "Epoch 4108, Loss: 0.010860842012334615, Final Batch Loss: 0.004638812970370054\n",
      "Epoch 4109, Loss: 0.003621198105975054, Final Batch Loss: 0.00037708779564127326\n",
      "Epoch 4110, Loss: 0.014708005910506472, Final Batch Loss: 0.0003800872073043138\n",
      "Epoch 4111, Loss: 0.012891246580693405, Final Batch Loss: 0.0033534702379256487\n",
      "Epoch 4112, Loss: 0.0380123702052515, Final Batch Loss: 0.00013693180517293513\n",
      "Epoch 4113, Loss: 0.003480042709270492, Final Batch Loss: 0.0005363734671846032\n",
      "Epoch 4114, Loss: 0.0086777075775899, Final Batch Loss: 0.001081419992260635\n",
      "Epoch 4115, Loss: 0.02071081670874264, Final Batch Loss: 0.0009035589755512774\n",
      "Epoch 4116, Loss: 0.013934895738202613, Final Batch Loss: 0.0019846800714731216\n",
      "Epoch 4117, Loss: 0.0020147942268522456, Final Batch Loss: 0.0003265385748818517\n",
      "Epoch 4118, Loss: 0.013908402383094653, Final Batch Loss: 0.00043997642933391035\n",
      "Epoch 4119, Loss: 0.009111266801482998, Final Batch Loss: 0.004964323714375496\n",
      "Epoch 4120, Loss: 0.014244024743675254, Final Batch Loss: 0.0004612461489159614\n",
      "Epoch 4121, Loss: 0.007594635993882548, Final Batch Loss: 0.0010038787731900811\n",
      "Epoch 4122, Loss: 0.002792969375150278, Final Batch Loss: 0.0003513014817144722\n",
      "Epoch 4123, Loss: 0.01402975420205621, Final Batch Loss: 0.009319504722952843\n",
      "Epoch 4124, Loss: 0.00653645493730437, Final Batch Loss: 0.00012661739310715348\n",
      "Epoch 4125, Loss: 0.006039193511242047, Final Batch Loss: 0.001211667899042368\n",
      "Epoch 4126, Loss: 0.013530637457733974, Final Batch Loss: 0.000987336621619761\n",
      "Epoch 4127, Loss: 0.00870488677173853, Final Batch Loss: 0.0012334828497841954\n",
      "Epoch 4128, Loss: 0.004113777162274346, Final Batch Loss: 0.0005596480332314968\n",
      "Epoch 4129, Loss: 0.02140563484863378, Final Batch Loss: 0.000268652947852388\n",
      "Epoch 4130, Loss: 0.05308497743681073, Final Batch Loss: 0.0449492447078228\n",
      "Epoch 4131, Loss: 0.017020345316268504, Final Batch Loss: 0.0002649809466674924\n",
      "Epoch 4132, Loss: 0.00698367232689634, Final Batch Loss: 0.0008992108050733805\n",
      "Epoch 4133, Loss: 0.009547313558869064, Final Batch Loss: 0.005017690826207399\n",
      "Epoch 4134, Loss: 0.00992993448744528, Final Batch Loss: 0.0020107701420783997\n",
      "Epoch 4135, Loss: 0.003795877652009949, Final Batch Loss: 0.0003772467316593975\n",
      "Epoch 4136, Loss: 0.007187642419012263, Final Batch Loss: 0.0002558995329309255\n",
      "Epoch 4137, Loss: 0.01218487294681836, Final Batch Loss: 0.0065942914225161076\n",
      "Epoch 4138, Loss: 0.006545998607180081, Final Batch Loss: 0.0004017490427941084\n",
      "Epoch 4139, Loss: 0.0055086902575567365, Final Batch Loss: 0.0005088059115223587\n",
      "Epoch 4140, Loss: 0.004816218905034475, Final Batch Loss: 0.0009385133162140846\n",
      "Epoch 4141, Loss: 0.014328023276902968, Final Batch Loss: 0.0002512744686100632\n",
      "Epoch 4142, Loss: 0.01718006332521327, Final Batch Loss: 0.0004010362608823925\n",
      "Epoch 4143, Loss: 0.00177160651946906, Final Batch Loss: 0.00028029613895341754\n",
      "Epoch 4144, Loss: 0.003258649609051645, Final Batch Loss: 0.000143900397233665\n",
      "Epoch 4145, Loss: 0.008540568553144112, Final Batch Loss: 0.00036478060064837337\n",
      "Epoch 4146, Loss: 0.002873985649785027, Final Batch Loss: 0.0011194928083568811\n",
      "Epoch 4147, Loss: 0.013046138803474605, Final Batch Loss: 0.0005113991210237145\n",
      "Epoch 4148, Loss: 0.03187184780836105, Final Batch Loss: 0.000368086330126971\n",
      "Epoch 4149, Loss: 0.018968540462083183, Final Batch Loss: 0.0001868941617431119\n",
      "Epoch 4150, Loss: 0.0636419987276895, Final Batch Loss: 0.05738290399312973\n",
      "Epoch 4151, Loss: 0.024023626989219338, Final Batch Loss: 0.016798757016658783\n",
      "Epoch 4152, Loss: 0.013753189952694811, Final Batch Loss: 0.003922292031347752\n",
      "Epoch 4153, Loss: 0.02700896331225522, Final Batch Loss: 0.0013259267434477806\n",
      "Epoch 4154, Loss: 0.07780729077057913, Final Batch Loss: 0.0009183878428302705\n",
      "Epoch 4155, Loss: 0.03921450424240902, Final Batch Loss: 0.012842027470469475\n",
      "Epoch 4156, Loss: 0.0020431983284652233, Final Batch Loss: 0.00016321793373208493\n",
      "Epoch 4157, Loss: 0.012999304351978935, Final Batch Loss: 0.00014124451263342053\n",
      "Epoch 4158, Loss: 0.02544988780573476, Final Batch Loss: 0.0021654157899320126\n",
      "Epoch 4159, Loss: 0.011528258444741368, Final Batch Loss: 0.0013617199147120118\n",
      "Epoch 4160, Loss: 0.004599120264174417, Final Batch Loss: 0.0003140030021313578\n",
      "Epoch 4161, Loss: 0.01180112456495408, Final Batch Loss: 0.00035970957833342254\n",
      "Epoch 4162, Loss: 0.012252892280230299, Final Batch Loss: 0.008587831631302834\n",
      "Epoch 4163, Loss: 0.005018527735956013, Final Batch Loss: 0.0004996735951863229\n",
      "Epoch 4164, Loss: 0.03134842436702456, Final Batch Loss: 0.021036794409155846\n",
      "Epoch 4165, Loss: 0.008879421140591148, Final Batch Loss: 0.0033955860417336226\n",
      "Epoch 4166, Loss: 0.0076469615451060236, Final Batch Loss: 0.004515247885137796\n",
      "Epoch 4167, Loss: 0.008783376950304955, Final Batch Loss: 0.0009370770421810448\n",
      "Epoch 4168, Loss: 0.011310409332509153, Final Batch Loss: 0.00014639452274423093\n",
      "Epoch 4169, Loss: 0.007527963287429884, Final Batch Loss: 0.005074405577033758\n",
      "Epoch 4170, Loss: 0.008050231903325766, Final Batch Loss: 0.0045516593381762505\n",
      "Epoch 4171, Loss: 0.005029667852795683, Final Batch Loss: 0.0032103199046105146\n",
      "Epoch 4172, Loss: 0.008074038443737663, Final Batch Loss: 0.00010848329111468047\n",
      "Epoch 4173, Loss: 0.007292532944120467, Final Batch Loss: 0.0014804013771936297\n",
      "Epoch 4174, Loss: 0.0018282689197803847, Final Batch Loss: 0.00025134652969427407\n",
      "Epoch 4175, Loss: 0.004466737766051665, Final Batch Loss: 0.00029554826323874295\n",
      "Epoch 4176, Loss: 0.015376109629869461, Final Batch Loss: 0.010729660280048847\n",
      "Epoch 4177, Loss: 0.005047030354035087, Final Batch Loss: 0.001061156508512795\n",
      "Epoch 4178, Loss: 0.019128536609059665, Final Batch Loss: 9.167849930236116e-05\n",
      "Epoch 4179, Loss: 0.0038123714402900077, Final Batch Loss: 0.000345110718626529\n",
      "Epoch 4180, Loss: 0.005718970249290578, Final Batch Loss: 0.002224249765276909\n",
      "Epoch 4181, Loss: 0.00698234501760453, Final Batch Loss: 0.0040337392129004\n",
      "Epoch 4182, Loss: 0.003411123456317, Final Batch Loss: 0.0005378769710659981\n",
      "Epoch 4183, Loss: 0.003997556814283598, Final Batch Loss: 0.0009758378146216273\n",
      "Epoch 4184, Loss: 0.005317985749570653, Final Batch Loss: 0.00013643837883137167\n",
      "Epoch 4185, Loss: 0.030946019804105163, Final Batch Loss: 0.028615733608603477\n",
      "Epoch 4186, Loss: 0.0017022713873302564, Final Batch Loss: 0.0003403780865482986\n",
      "Epoch 4187, Loss: 0.0034243597619934008, Final Batch Loss: 0.0013415946159511805\n",
      "Epoch 4188, Loss: 0.002577569779532496, Final Batch Loss: 9.416603279532865e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4189, Loss: 0.0013637329684570432, Final Batch Loss: 0.0006107647204771638\n",
      "Epoch 4190, Loss: 0.0034110687993234023, Final Batch Loss: 0.00020578352268785238\n",
      "Epoch 4191, Loss: 0.0018376752123003826, Final Batch Loss: 0.0005269762477837503\n",
      "Epoch 4192, Loss: 0.02847839501919225, Final Batch Loss: 0.026924630627036095\n",
      "Epoch 4193, Loss: 0.006311159173492342, Final Batch Loss: 0.002622741973027587\n",
      "Epoch 4194, Loss: 0.026630158361513168, Final Batch Loss: 0.012305906973779202\n",
      "Epoch 4195, Loss: 0.0035659268323797733, Final Batch Loss: 0.0020050143357366323\n",
      "Epoch 4196, Loss: 0.009301667101681232, Final Batch Loss: 0.003744004759937525\n",
      "Epoch 4197, Loss: 0.004561571578960866, Final Batch Loss: 0.0002314881276106462\n",
      "Epoch 4198, Loss: 0.005120228743180633, Final Batch Loss: 0.00394185446202755\n",
      "Epoch 4199, Loss: 0.010998151788953692, Final Batch Loss: 0.00021961081074550748\n",
      "Epoch 4200, Loss: 0.004797158282599412, Final Batch Loss: 0.0002942239516414702\n",
      "Epoch 4201, Loss: 0.0026926728314720094, Final Batch Loss: 0.0003502060135360807\n",
      "Epoch 4202, Loss: 0.003708541473315563, Final Batch Loss: 0.0013833065750077367\n",
      "Epoch 4203, Loss: 0.005027614024584182, Final Batch Loss: 0.0010893198195844889\n",
      "Epoch 4204, Loss: 0.004999295480956789, Final Batch Loss: 0.0012224277015775442\n",
      "Epoch 4205, Loss: 0.010725069834734313, Final Batch Loss: 0.0004686083411797881\n",
      "Epoch 4206, Loss: 0.0021593944693449885, Final Batch Loss: 0.0007109603029675782\n",
      "Epoch 4207, Loss: 0.002534027076762868, Final Batch Loss: 6.143814243841916e-05\n",
      "Epoch 4208, Loss: 0.008075237201410346, Final Batch Loss: 0.00018114694103132933\n",
      "Epoch 4209, Loss: 0.0036175062487018295, Final Batch Loss: 0.00011557456309674308\n",
      "Epoch 4210, Loss: 0.0024738499923842028, Final Batch Loss: 0.0002984169695992023\n",
      "Epoch 4211, Loss: 0.011763437290937873, Final Batch Loss: 0.0003969011886510998\n",
      "Epoch 4212, Loss: 0.036558464671543334, Final Batch Loss: 0.0002716815797612071\n",
      "Epoch 4213, Loss: 0.005148289612407098, Final Batch Loss: 5.3417152230395004e-05\n",
      "Epoch 4214, Loss: 0.0015474438478122465, Final Batch Loss: 0.00014460879901889712\n",
      "Epoch 4215, Loss: 0.040647914371220395, Final Batch Loss: 0.017208559438586235\n",
      "Epoch 4216, Loss: 0.02089333195181098, Final Batch Loss: 0.0002130258799297735\n",
      "Epoch 4217, Loss: 0.06675557198468596, Final Batch Loss: 0.048921436071395874\n",
      "Epoch 4218, Loss: 0.027091559546533972, Final Batch Loss: 0.0012874185340479016\n",
      "Epoch 4219, Loss: 0.015483521856367588, Final Batch Loss: 0.0013012472772970796\n",
      "Epoch 4220, Loss: 0.022746503993403167, Final Batch Loss: 0.003955264110118151\n",
      "Epoch 4221, Loss: 0.02195102721452713, Final Batch Loss: 0.002193204592913389\n",
      "Epoch 4222, Loss: 0.012228723266161978, Final Batch Loss: 0.0008844379335641861\n",
      "Epoch 4223, Loss: 0.010162544785998762, Final Batch Loss: 0.001873910310678184\n",
      "Epoch 4224, Loss: 0.0207882422109833, Final Batch Loss: 0.00012154235446359962\n",
      "Epoch 4225, Loss: 0.01412143852940062, Final Batch Loss: 0.010765988379716873\n",
      "Epoch 4226, Loss: 0.00814219328458421, Final Batch Loss: 0.00046788438339717686\n",
      "Epoch 4227, Loss: 0.0032740818787715398, Final Batch Loss: 0.0014498281525447965\n",
      "Epoch 4228, Loss: 0.03471421211725101, Final Batch Loss: 0.005700345616787672\n",
      "Epoch 4229, Loss: 0.011344577244017273, Final Batch Loss: 9.90435219136998e-05\n",
      "Epoch 4230, Loss: 0.0053995237103663385, Final Batch Loss: 0.0028784689493477345\n",
      "Epoch 4231, Loss: 0.007036487295408733, Final Batch Loss: 0.0036040530540049076\n",
      "Epoch 4232, Loss: 0.00667318249179516, Final Batch Loss: 0.00019382570462767035\n",
      "Epoch 4233, Loss: 0.0026938745577353984, Final Batch Loss: 0.0002612127864267677\n",
      "Epoch 4234, Loss: 0.0035636549873743206, Final Batch Loss: 0.0018973303958773613\n",
      "Epoch 4235, Loss: 0.005604030753602274, Final Batch Loss: 0.00012190539564471692\n",
      "Epoch 4236, Loss: 0.0015756387438159436, Final Batch Loss: 0.0002890453324653208\n",
      "Epoch 4237, Loss: 0.01248199024121277, Final Batch Loss: 0.0007230504415929317\n",
      "Epoch 4238, Loss: 0.0031178171338979155, Final Batch Loss: 0.000711413158569485\n",
      "Epoch 4239, Loss: 0.004012723307823762, Final Batch Loss: 0.001129705342464149\n",
      "Epoch 4240, Loss: 0.021954823736450635, Final Batch Loss: 0.0027310133446007967\n",
      "Epoch 4241, Loss: 0.009224935434758663, Final Batch Loss: 0.00040965137304738164\n",
      "Epoch 4242, Loss: 0.0017654676557867788, Final Batch Loss: 0.00034828478237614036\n",
      "Epoch 4243, Loss: 0.023193914217699785, Final Batch Loss: 0.0005433006444945931\n",
      "Epoch 4244, Loss: 0.003570438304450363, Final Batch Loss: 0.0015784453134983778\n",
      "Epoch 4245, Loss: 0.0017074709394364618, Final Batch Loss: 0.00010648737224983051\n",
      "Epoch 4246, Loss: 0.030335979579831474, Final Batch Loss: 0.00012773594062309712\n",
      "Epoch 4247, Loss: 0.007592598260089289, Final Batch Loss: 0.00031709307222627103\n",
      "Epoch 4248, Loss: 0.003235691983718425, Final Batch Loss: 0.00015304832777474076\n",
      "Epoch 4249, Loss: 0.007857418167986907, Final Batch Loss: 0.0030584821943193674\n",
      "Epoch 4250, Loss: 0.003681036934722215, Final Batch Loss: 0.0010998566867783666\n",
      "Epoch 4251, Loss: 0.012435999378794804, Final Batch Loss: 0.00017159065464511514\n",
      "Epoch 4252, Loss: 0.01858202162111411, Final Batch Loss: 0.002987853018566966\n",
      "Epoch 4253, Loss: 0.02988631273910869, Final Batch Loss: 0.028466857969760895\n",
      "Epoch 4254, Loss: 0.0017454265180276707, Final Batch Loss: 0.000706578022800386\n",
      "Epoch 4255, Loss: 0.017037782582519867, Final Batch Loss: 1.5097391042218078e-05\n",
      "Epoch 4256, Loss: 0.029044198265182786, Final Batch Loss: 0.0017784291412681341\n",
      "Epoch 4257, Loss: 0.014229301479645073, Final Batch Loss: 0.008810948580503464\n",
      "Epoch 4258, Loss: 0.001793364601326175, Final Batch Loss: 0.00040494254790246487\n",
      "Epoch 4259, Loss: 0.009267822961191996, Final Batch Loss: 1.3231065167929046e-05\n",
      "Epoch 4260, Loss: 0.001572930719703436, Final Batch Loss: 0.0004698210395872593\n",
      "Epoch 4261, Loss: 0.015296903293346986, Final Batch Loss: 0.011411118321120739\n",
      "Epoch 4262, Loss: 0.024633107794215903, Final Batch Loss: 0.000608612026553601\n",
      "Epoch 4263, Loss: 0.0041229241687688045, Final Batch Loss: 7.117207132978365e-05\n",
      "Epoch 4264, Loss: 0.016443090375105385, Final Batch Loss: 0.0010652263881638646\n",
      "Epoch 4265, Loss: 0.003942409850424156, Final Batch Loss: 0.0009357533417642117\n",
      "Epoch 4266, Loss: 0.01619274544646032, Final Batch Loss: 0.001830852939747274\n",
      "Epoch 4267, Loss: 0.05457124272652436, Final Batch Loss: 0.00016370393859688193\n",
      "Epoch 4268, Loss: 0.033699379389872774, Final Batch Loss: 0.027397429570555687\n",
      "Epoch 4269, Loss: 0.05924063117709011, Final Batch Loss: 0.006409179884940386\n",
      "Epoch 4270, Loss: 0.010345685142965522, Final Batch Loss: 0.005841371603310108\n",
      "Epoch 4271, Loss: 0.032253614510409534, Final Batch Loss: 0.009135929867625237\n",
      "Epoch 4272, Loss: 0.008372172713279724, Final Batch Loss: 0.003327510552480817\n",
      "Epoch 4273, Loss: 0.010488795349374413, Final Batch Loss: 0.002295045182108879\n",
      "Epoch 4274, Loss: 0.005870583205251023, Final Batch Loss: 0.002323593245819211\n",
      "Epoch 4275, Loss: 0.01209194038528949, Final Batch Loss: 0.002340334467589855\n",
      "Epoch 4276, Loss: 0.01381414575735107, Final Batch Loss: 0.001875156071037054\n",
      "Epoch 4277, Loss: 0.005009023181628436, Final Batch Loss: 0.0011380295036360621\n",
      "Epoch 4278, Loss: 0.020470486611884553, Final Batch Loss: 0.00020324824436102062\n",
      "Epoch 4279, Loss: 0.019749659622902982, Final Batch Loss: 0.011601944454014301\n",
      "Epoch 4280, Loss: 0.0074749759151018225, Final Batch Loss: 3.9269762055482715e-05\n",
      "Epoch 4281, Loss: 0.02196371281752363, Final Batch Loss: 0.018252447247505188\n",
      "Epoch 4282, Loss: 0.009572526767442469, Final Batch Loss: 0.0007468871190212667\n",
      "Epoch 4283, Loss: 0.007968405057908967, Final Batch Loss: 0.0006445817416533828\n",
      "Epoch 4284, Loss: 0.006615357211558148, Final Batch Loss: 0.00038078692159615457\n",
      "Epoch 4285, Loss: 0.005846664833370596, Final Batch Loss: 0.0003238859062548727\n",
      "Epoch 4286, Loss: 0.0026963632117258385, Final Batch Loss: 0.0014331693528220057\n",
      "Epoch 4287, Loss: 0.005119980974995997, Final Batch Loss: 0.0009738394874148071\n",
      "Epoch 4288, Loss: 0.03223288278968539, Final Batch Loss: 0.00854532141238451\n",
      "Epoch 4289, Loss: 0.004374894720967859, Final Batch Loss: 0.00021092008682899177\n",
      "Epoch 4290, Loss: 0.0020911327374051325, Final Batch Loss: 0.0001078023633453995\n",
      "Epoch 4291, Loss: 0.004474694389500655, Final Batch Loss: 0.0012863653246313334\n",
      "Epoch 4292, Loss: 0.007017142110271379, Final Batch Loss: 0.00062771380180493\n",
      "Epoch 4293, Loss: 0.022464739755378105, Final Batch Loss: 0.00011244826600886881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4294, Loss: 0.037685151415644214, Final Batch Loss: 0.00016075928579084575\n",
      "Epoch 4295, Loss: 0.030739672569325194, Final Batch Loss: 0.02462128922343254\n",
      "Epoch 4296, Loss: 0.03895555740746204, Final Batch Loss: 0.028581760823726654\n",
      "Epoch 4297, Loss: 0.0028933958456036635, Final Batch Loss: 0.0005496352096088231\n",
      "Epoch 4298, Loss: 0.008240034170739818, Final Batch Loss: 0.002370486268773675\n",
      "Epoch 4299, Loss: 0.004257007414707914, Final Batch Loss: 0.00024357113579753786\n",
      "Epoch 4300, Loss: 0.005573117268795613, Final Batch Loss: 0.0011577095137909055\n",
      "Epoch 4301, Loss: 0.04851001278439071, Final Batch Loss: 0.00044455224997363985\n",
      "Epoch 4302, Loss: 0.018495378622901626, Final Batch Loss: 0.006199097726494074\n",
      "Epoch 4303, Loss: 0.04737460828619078, Final Batch Loss: 0.02785939909517765\n",
      "Epoch 4304, Loss: 0.006042269000317901, Final Batch Loss: 0.0008919580141082406\n",
      "Epoch 4305, Loss: 0.027472535148262978, Final Batch Loss: 0.01249011792242527\n",
      "Epoch 4306, Loss: 0.02234303965815343, Final Batch Loss: 0.000404211605200544\n",
      "Epoch 4307, Loss: 0.009915364906191826, Final Batch Loss: 0.0003194028977304697\n",
      "Epoch 4308, Loss: 0.0077324330632109195, Final Batch Loss: 0.000312091811792925\n",
      "Epoch 4309, Loss: 0.00640298961661756, Final Batch Loss: 0.00038339279126375914\n",
      "Epoch 4310, Loss: 0.00523810728918761, Final Batch Loss: 0.0007635841611772776\n",
      "Epoch 4311, Loss: 0.028592820453923196, Final Batch Loss: 0.025424282997846603\n",
      "Epoch 4312, Loss: 0.0014444906737480778, Final Batch Loss: 0.0001791127579053864\n",
      "Epoch 4313, Loss: 0.008278789857286029, Final Batch Loss: 0.003389769932255149\n",
      "Epoch 4314, Loss: 0.0009257251440430991, Final Batch Loss: 0.00013973705063108355\n",
      "Epoch 4315, Loss: 0.0021740624506492168, Final Batch Loss: 0.00031857730937190354\n",
      "Epoch 4316, Loss: 0.00407937441195827, Final Batch Loss: 0.0005600315635092556\n",
      "Epoch 4317, Loss: 0.029211180204583798, Final Batch Loss: 0.028128212317824364\n",
      "Epoch 4318, Loss: 0.016088332777144387, Final Batch Loss: 0.0034226924180984497\n",
      "Epoch 4319, Loss: 0.018911783015937544, Final Batch Loss: 0.0018261198420077562\n",
      "Epoch 4320, Loss: 0.011894235183717683, Final Batch Loss: 0.000473779218737036\n",
      "Epoch 4321, Loss: 0.02037387399468571, Final Batch Loss: 0.01153752114623785\n",
      "Epoch 4322, Loss: 0.03037979044893291, Final Batch Loss: 0.011699709109961987\n",
      "Epoch 4323, Loss: 0.0663009844138287, Final Batch Loss: 0.0008165289764292538\n",
      "Epoch 4324, Loss: 0.010003049537772313, Final Batch Loss: 0.0007894823793321848\n",
      "Epoch 4325, Loss: 0.015401617914903909, Final Batch Loss: 0.006214796099811792\n",
      "Epoch 4326, Loss: 0.05359110387507826, Final Batch Loss: 0.0010611203033477068\n",
      "Epoch 4327, Loss: 0.031438198435353115, Final Batch Loss: 0.0015791591722518206\n",
      "Epoch 4328, Loss: 0.00567307835444808, Final Batch Loss: 0.0007734131067991257\n",
      "Epoch 4329, Loss: 0.023778321919962764, Final Batch Loss: 0.0029443243984133005\n",
      "Epoch 4330, Loss: 0.026047417224617675, Final Batch Loss: 0.006257119122892618\n",
      "Epoch 4331, Loss: 0.017343259838526137, Final Batch Loss: 0.0069615477696061134\n",
      "Epoch 4332, Loss: 0.01622040377696976, Final Batch Loss: 0.0009001668076962233\n",
      "Epoch 4333, Loss: 0.04355698172003031, Final Batch Loss: 0.03292311728000641\n",
      "Epoch 4334, Loss: 0.010717014010879211, Final Batch Loss: 0.0023238302674144506\n",
      "Epoch 4335, Loss: 0.019224681134801358, Final Batch Loss: 0.0028648623265326023\n",
      "Epoch 4336, Loss: 0.0051173807587474585, Final Batch Loss: 0.0007952700834721327\n",
      "Epoch 4337, Loss: 0.04997004853794351, Final Batch Loss: 0.04440322518348694\n",
      "Epoch 4338, Loss: 0.004474634595680982, Final Batch Loss: 0.0002473226631991565\n",
      "Epoch 4339, Loss: 0.046133649040712044, Final Batch Loss: 0.00046331589692272246\n",
      "Epoch 4340, Loss: 0.007688340294407681, Final Batch Loss: 0.0007285116007551551\n",
      "Epoch 4341, Loss: 0.017993207497056574, Final Batch Loss: 0.001584887271746993\n",
      "Epoch 4342, Loss: 0.010555363551247865, Final Batch Loss: 0.0001234183000633493\n",
      "Epoch 4343, Loss: 0.01180289602780249, Final Batch Loss: 0.0011388958664610982\n",
      "Epoch 4344, Loss: 0.016557538852794096, Final Batch Loss: 0.00020826849504373968\n",
      "Epoch 4345, Loss: 0.04925428400747478, Final Batch Loss: 0.026555396616458893\n",
      "Epoch 4346, Loss: 0.00812501710606739, Final Batch Loss: 0.00024319914518855512\n",
      "Epoch 4347, Loss: 0.02451786515302956, Final Batch Loss: 0.001136070117354393\n",
      "Epoch 4348, Loss: 0.04938012681668624, Final Batch Loss: 0.0007119363290257752\n",
      "Epoch 4349, Loss: 0.024449243341223337, Final Batch Loss: 0.0016822775360196829\n",
      "Epoch 4350, Loss: 0.03320984705351293, Final Batch Loss: 0.006408268585801125\n",
      "Epoch 4351, Loss: 0.029102885026077274, Final Batch Loss: 0.002082833321765065\n",
      "Epoch 4352, Loss: 0.02021299599437043, Final Batch Loss: 0.010912913829088211\n",
      "Epoch 4353, Loss: 0.0259489479358308, Final Batch Loss: 0.0005563829909078777\n",
      "Epoch 4354, Loss: 0.010170165332965553, Final Batch Loss: 0.002722335746511817\n",
      "Epoch 4355, Loss: 0.02369596780044958, Final Batch Loss: 0.001169651746749878\n",
      "Epoch 4356, Loss: 0.02455145970452577, Final Batch Loss: 0.016810748726129532\n",
      "Epoch 4357, Loss: 0.005465682887006551, Final Batch Loss: 0.0009436610853299499\n",
      "Epoch 4358, Loss: 0.009282542101573199, Final Batch Loss: 0.00040580565109848976\n",
      "Epoch 4359, Loss: 0.005607050319667906, Final Batch Loss: 0.0010853703133761883\n",
      "Epoch 4360, Loss: 0.02265779147273861, Final Batch Loss: 9.623260120861232e-05\n",
      "Epoch 4361, Loss: 0.04255609482061118, Final Batch Loss: 0.012083424255251884\n",
      "Epoch 4362, Loss: 0.004280652152374387, Final Batch Loss: 0.0002556215040385723\n",
      "Epoch 4363, Loss: 0.015363419544883072, Final Batch Loss: 0.000265059235971421\n",
      "Epoch 4364, Loss: 0.015049041365273297, Final Batch Loss: 0.0009148817043751478\n",
      "Epoch 4365, Loss: 0.026918672170722857, Final Batch Loss: 0.004154226277023554\n",
      "Epoch 4366, Loss: 0.04767849447671324, Final Batch Loss: 0.00629241019487381\n",
      "Epoch 4367, Loss: 0.02563578329863958, Final Batch Loss: 0.012472514063119888\n",
      "Epoch 4368, Loss: 0.018818297423422337, Final Batch Loss: 0.0004932770971208811\n",
      "Epoch 4369, Loss: 0.019131381981424056, Final Batch Loss: 0.0006188051193021238\n",
      "Epoch 4370, Loss: 0.01124966332281474, Final Batch Loss: 0.0035717675928026438\n",
      "Epoch 4371, Loss: 0.02661953305141651, Final Batch Loss: 0.00039303069934248924\n",
      "Epoch 4372, Loss: 0.027997355806292035, Final Batch Loss: 0.0001319761067861691\n",
      "Epoch 4373, Loss: 0.015692103654146194, Final Batch Loss: 0.0005871958564966917\n",
      "Epoch 4374, Loss: 0.05377651489106938, Final Batch Loss: 0.007788125891238451\n",
      "Epoch 4375, Loss: 0.016901229180803057, Final Batch Loss: 0.0004814418498426676\n",
      "Epoch 4376, Loss: 0.02496384199548629, Final Batch Loss: 0.005650924518704414\n",
      "Epoch 4377, Loss: 0.0740313105925452, Final Batch Loss: 0.06707186996936798\n",
      "Epoch 4378, Loss: 0.005430315766716376, Final Batch Loss: 0.00047248799819499254\n",
      "Epoch 4379, Loss: 0.01863987901015207, Final Batch Loss: 0.0008721450576558709\n",
      "Epoch 4380, Loss: 0.009915819740854204, Final Batch Loss: 0.0018047570483759046\n",
      "Epoch 4381, Loss: 0.023658744292333722, Final Batch Loss: 0.004786728881299496\n",
      "Epoch 4382, Loss: 0.009713240433484316, Final Batch Loss: 0.002046288223937154\n",
      "Epoch 4383, Loss: 0.04569262804579921, Final Batch Loss: 0.012935897335410118\n",
      "Epoch 4384, Loss: 0.011786759743699804, Final Batch Loss: 0.0019241721602156758\n",
      "Epoch 4385, Loss: 0.010559282323811203, Final Batch Loss: 0.0068437280133366585\n",
      "Epoch 4386, Loss: 0.014578520094801206, Final Batch Loss: 0.009101660922169685\n",
      "Epoch 4387, Loss: 0.02121579363301862, Final Batch Loss: 0.0001307844213442877\n",
      "Epoch 4388, Loss: 0.017640094098169357, Final Batch Loss: 0.0016265589511021972\n",
      "Epoch 4389, Loss: 0.00787449604831636, Final Batch Loss: 0.0007015395676717162\n",
      "Epoch 4390, Loss: 0.007349573512328789, Final Batch Loss: 0.0004082799714524299\n",
      "Epoch 4391, Loss: 0.005724028989789076, Final Batch Loss: 0.0017889109440147877\n",
      "Epoch 4392, Loss: 0.007892871028161608, Final Batch Loss: 0.00021718481730204076\n",
      "Epoch 4393, Loss: 0.006179720221552998, Final Batch Loss: 0.0005509245675057173\n",
      "Epoch 4394, Loss: 0.0014401013468159363, Final Batch Loss: 0.00043388488120399415\n",
      "Epoch 4395, Loss: 0.018299551273230463, Final Batch Loss: 0.007312461733818054\n",
      "Epoch 4396, Loss: 0.006765127065591514, Final Batch Loss: 0.0030863143038004637\n",
      "Epoch 4397, Loss: 0.0026942668482661247, Final Batch Loss: 0.0004710320208687335\n",
      "Epoch 4398, Loss: 0.004141026387515012, Final Batch Loss: 0.0005249538808129728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4399, Loss: 0.013193076476454735, Final Batch Loss: 0.00016849972598720342\n",
      "Epoch 4400, Loss: 0.0034422585886204615, Final Batch Loss: 0.00033917700056917965\n",
      "Epoch 4401, Loss: 0.002975696261273697, Final Batch Loss: 0.0003205174289178103\n",
      "Epoch 4402, Loss: 0.008014159408048727, Final Batch Loss: 0.00022192805772647262\n",
      "Epoch 4403, Loss: 0.0069084815550013445, Final Batch Loss: 0.0003039501316379756\n",
      "Epoch 4404, Loss: 0.0021054686767456587, Final Batch Loss: 5.519572368939407e-05\n",
      "Epoch 4405, Loss: 0.01438255283574108, Final Batch Loss: 0.00929289497435093\n",
      "Epoch 4406, Loss: 0.02168386873381678, Final Batch Loss: 0.00021415700030047446\n",
      "Epoch 4407, Loss: 0.010581102935248055, Final Batch Loss: 9.033259993884712e-05\n",
      "Epoch 4408, Loss: 0.0088879258473753, Final Batch Loss: 9.655547182774171e-05\n",
      "Epoch 4409, Loss: 0.00585229649732355, Final Batch Loss: 0.00032881239894777536\n",
      "Epoch 4410, Loss: 0.019439210562268272, Final Batch Loss: 0.003503889311105013\n",
      "Epoch 4411, Loss: 0.011086584177974146, Final Batch Loss: 0.00932209100574255\n",
      "Epoch 4412, Loss: 0.05302578548435122, Final Batch Loss: 0.020691197365522385\n",
      "Epoch 4413, Loss: 0.007148144562961534, Final Batch Loss: 0.0038788816891610622\n",
      "Epoch 4414, Loss: 0.0042010690667666495, Final Batch Loss: 0.0002234946732642129\n",
      "Epoch 4415, Loss: 0.007207912363810465, Final Batch Loss: 0.0004992406466044486\n",
      "Epoch 4416, Loss: 0.0023759328505548183, Final Batch Loss: 0.0005008915904909372\n",
      "Epoch 4417, Loss: 0.013570435643487144, Final Batch Loss: 0.0002529917692299932\n",
      "Epoch 4418, Loss: 0.014376205421285704, Final Batch Loss: 8.147134212777019e-05\n",
      "Epoch 4419, Loss: 0.004924807217321359, Final Batch Loss: 0.0005679490277543664\n",
      "Epoch 4420, Loss: 0.003635511704487726, Final Batch Loss: 0.0004042042710352689\n",
      "Epoch 4421, Loss: 0.004600185246090405, Final Batch Loss: 0.0019424059428274632\n",
      "Epoch 4422, Loss: 0.005207109952607425, Final Batch Loss: 7.840219041099772e-05\n",
      "Epoch 4423, Loss: 0.0016932235012063757, Final Batch Loss: 0.00015345973952207714\n",
      "Epoch 4424, Loss: 0.06064736322150566, Final Batch Loss: 0.0007028791005723178\n",
      "Epoch 4425, Loss: 0.029558420239482075, Final Batch Loss: 0.009513352066278458\n",
      "Epoch 4426, Loss: 0.030773918464547023, Final Batch Loss: 0.0016435584984719753\n",
      "Epoch 4427, Loss: 0.010375858415500261, Final Batch Loss: 0.004464205354452133\n",
      "Epoch 4428, Loss: 0.031679655774496496, Final Batch Loss: 0.0003351718478370458\n",
      "Epoch 4429, Loss: 0.029117519792634994, Final Batch Loss: 0.004834722261875868\n",
      "Epoch 4430, Loss: 0.024126537959091365, Final Batch Loss: 0.0058390237390995026\n",
      "Epoch 4431, Loss: 0.012583516538143158, Final Batch Loss: 0.004563500639051199\n",
      "Epoch 4432, Loss: 0.05563439684920013, Final Batch Loss: 0.0005105002201162279\n",
      "Epoch 4433, Loss: 0.03477669978747144, Final Batch Loss: 0.00035984758869744837\n",
      "Epoch 4434, Loss: 0.008615135360741988, Final Batch Loss: 0.0002706187078729272\n",
      "Epoch 4435, Loss: 0.004469406005227938, Final Batch Loss: 0.0005231807590462267\n",
      "Epoch 4436, Loss: 0.006796773202950135, Final Batch Loss: 0.0003950145037379116\n",
      "Epoch 4437, Loss: 0.012118877239117865, Final Batch Loss: 0.008653626777231693\n",
      "Epoch 4438, Loss: 0.007376616951660253, Final Batch Loss: 0.0006015398539602757\n",
      "Epoch 4439, Loss: 0.037806188978720456, Final Batch Loss: 0.03645482659339905\n",
      "Epoch 4440, Loss: 0.0026559100078884512, Final Batch Loss: 0.00022583625104743987\n",
      "Epoch 4441, Loss: 0.0227283607237041, Final Batch Loss: 0.001656386535614729\n",
      "Epoch 4442, Loss: 0.005682027054717764, Final Batch Loss: 9.625437087379396e-05\n",
      "Epoch 4443, Loss: 0.011242966982536018, Final Batch Loss: 0.0003862622252199799\n",
      "Epoch 4444, Loss: 0.0033573453983990476, Final Batch Loss: 0.0011204398469999433\n",
      "Epoch 4445, Loss: 0.0031054263236001134, Final Batch Loss: 0.00021043297601863742\n",
      "Epoch 4446, Loss: 0.020728641975438222, Final Batch Loss: 0.00016185091226361692\n",
      "Epoch 4447, Loss: 0.004098715377040207, Final Batch Loss: 0.00019492878345772624\n",
      "Epoch 4448, Loss: 0.00875935223302804, Final Batch Loss: 0.0018290772568434477\n",
      "Epoch 4449, Loss: 0.0024310102162417024, Final Batch Loss: 0.0004204573342576623\n",
      "Epoch 4450, Loss: 0.022623555152677, Final Batch Loss: 0.003763128537684679\n",
      "Epoch 4451, Loss: 0.03403951275686268, Final Batch Loss: 0.00020218381541781127\n",
      "Epoch 4452, Loss: 0.0071152374148368835, Final Batch Loss: 0.0020547122694551945\n",
      "Epoch 4453, Loss: 0.009890065877698362, Final Batch Loss: 0.0016825287602841854\n",
      "Epoch 4454, Loss: 0.02251565431652125, Final Batch Loss: 0.003112337552011013\n",
      "Epoch 4455, Loss: 0.00660029161372222, Final Batch Loss: 0.00028542146901600063\n",
      "Epoch 4456, Loss: 0.023807902121916413, Final Batch Loss: 0.007853522896766663\n",
      "Epoch 4457, Loss: 0.0064303290855605155, Final Batch Loss: 0.0002618062135297805\n",
      "Epoch 4458, Loss: 0.009899161872453988, Final Batch Loss: 0.005201028659939766\n",
      "Epoch 4459, Loss: 0.006177666989970021, Final Batch Loss: 0.0001514425384812057\n",
      "Epoch 4460, Loss: 0.0014897300861775875, Final Batch Loss: 0.0002788082347251475\n",
      "Epoch 4461, Loss: 0.006294190679909661, Final Batch Loss: 0.0037331744097173214\n",
      "Epoch 4462, Loss: 0.03488932835170999, Final Batch Loss: 0.00020981761917937547\n",
      "Epoch 4463, Loss: 0.0024030770146055147, Final Batch Loss: 0.0004042167856823653\n",
      "Epoch 4464, Loss: 0.00706328663181921, Final Batch Loss: 0.001004040241241455\n",
      "Epoch 4465, Loss: 0.029541397496359423, Final Batch Loss: 0.00010582222603261471\n",
      "Epoch 4466, Loss: 0.013169757276045857, Final Batch Loss: 0.0023962294217199087\n",
      "Epoch 4467, Loss: 0.0039352890744339675, Final Batch Loss: 0.0002835298946592957\n",
      "Epoch 4468, Loss: 0.00863137244596146, Final Batch Loss: 0.00205381796695292\n",
      "Epoch 4469, Loss: 0.020502620725892484, Final Batch Loss: 0.0003171887656208128\n",
      "Epoch 4470, Loss: 0.008817700727377087, Final Batch Loss: 0.0011165594914928079\n",
      "Epoch 4471, Loss: 0.011640034790616482, Final Batch Loss: 0.0033602139446884394\n",
      "Epoch 4472, Loss: 0.0036365735140861943, Final Batch Loss: 0.0003244773542974144\n",
      "Epoch 4473, Loss: 0.0034344848863838706, Final Batch Loss: 0.00036884305882267654\n",
      "Epoch 4474, Loss: 0.0016443893910036422, Final Batch Loss: 7.984748663147911e-05\n",
      "Epoch 4475, Loss: 0.005553803755901754, Final Batch Loss: 0.003553364658728242\n",
      "Epoch 4476, Loss: 0.025084522596444003, Final Batch Loss: 0.00010295140236848965\n",
      "Epoch 4477, Loss: 0.0025098523328779265, Final Batch Loss: 0.0008619719301350415\n",
      "Epoch 4478, Loss: 0.044472894554928644, Final Batch Loss: 0.0014722113264724612\n",
      "Epoch 4479, Loss: 0.02717612194828689, Final Batch Loss: 0.0021194845903664827\n",
      "Epoch 4480, Loss: 0.011271774535998702, Final Batch Loss: 0.007639709860086441\n",
      "Epoch 4481, Loss: 0.0017485749995103106, Final Batch Loss: 0.000420063006458804\n",
      "Epoch 4482, Loss: 0.018249939021188766, Final Batch Loss: 0.00039703474612906575\n",
      "Epoch 4483, Loss: 0.005417679261881858, Final Batch Loss: 0.0006275021005421877\n",
      "Epoch 4484, Loss: 0.01742360243224539, Final Batch Loss: 0.0008403180981986225\n",
      "Epoch 4485, Loss: 0.003567908177501522, Final Batch Loss: 0.00010248493344988674\n",
      "Epoch 4486, Loss: 0.004825668453122489, Final Batch Loss: 0.0001538788346806541\n",
      "Epoch 4487, Loss: 0.0044434738520067185, Final Batch Loss: 0.00032041381928138435\n",
      "Epoch 4488, Loss: 0.0028056580922566354, Final Batch Loss: 0.00016140723892021924\n",
      "Epoch 4489, Loss: 0.002088588909828104, Final Batch Loss: 0.00014686734357383102\n",
      "Epoch 4490, Loss: 0.0021993601112626493, Final Batch Loss: 0.0011546320747584105\n",
      "Epoch 4491, Loss: 0.004009481846878771, Final Batch Loss: 0.0002802173257805407\n",
      "Epoch 4492, Loss: 0.017153010237962008, Final Batch Loss: 0.0005547466571442783\n",
      "Epoch 4493, Loss: 0.004242900791723514, Final Batch Loss: 0.00010298812412656844\n",
      "Epoch 4494, Loss: 0.012779597120243125, Final Batch Loss: 0.0010376586578786373\n",
      "Epoch 4495, Loss: 0.0015472143277293071, Final Batch Loss: 0.00020155245147179812\n",
      "Epoch 4496, Loss: 0.0040883290575948195, Final Batch Loss: 1.1905119208677206e-05\n",
      "Epoch 4497, Loss: 0.007288758273716667, Final Batch Loss: 0.000244682130869478\n",
      "Epoch 4498, Loss: 0.0047646876773796976, Final Batch Loss: 0.00044618715764954686\n",
      "Epoch 4499, Loss: 0.0029298429944901727, Final Batch Loss: 0.0007360718445852399\n",
      "Epoch 4500, Loss: 0.008336196366144577, Final Batch Loss: 0.002870119409635663\n",
      "Epoch 4501, Loss: 0.006592618141439743, Final Batch Loss: 0.00017419397772755474\n",
      "Epoch 4502, Loss: 0.005478044302435592, Final Batch Loss: 0.00020891563326586038\n",
      "Epoch 4503, Loss: 0.0017637078435654985, Final Batch Loss: 0.001394056947901845\n",
      "Epoch 4504, Loss: 0.003612779109971598, Final Batch Loss: 0.0018641569186002016\n",
      "Epoch 4505, Loss: 0.0044866619718959555, Final Batch Loss: 0.0031104031950235367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4506, Loss: 0.0028867837609141134, Final Batch Loss: 0.002138035837560892\n",
      "Epoch 4507, Loss: 0.018563863064628094, Final Batch Loss: 0.0013546019326895475\n",
      "Epoch 4508, Loss: 0.002733130269916728, Final Batch Loss: 0.00023135339142754674\n",
      "Epoch 4509, Loss: 0.015806603005330544, Final Batch Loss: 0.00043219656799919903\n",
      "Epoch 4510, Loss: 0.0030610734538640827, Final Batch Loss: 0.0008411607705056667\n",
      "Epoch 4511, Loss: 0.02112647853209637, Final Batch Loss: 0.0003382080467417836\n",
      "Epoch 4512, Loss: 0.009546535606204998, Final Batch Loss: 7.444776565534994e-05\n",
      "Epoch 4513, Loss: 0.017549925745697692, Final Batch Loss: 0.003280292497947812\n",
      "Epoch 4514, Loss: 0.08119332323985873, Final Batch Loss: 0.07977276295423508\n",
      "Epoch 4515, Loss: 0.012267393329238985, Final Batch Loss: 0.00010042051144409925\n",
      "Epoch 4516, Loss: 0.019789064608630724, Final Batch Loss: 0.0003127630043309182\n",
      "Epoch 4517, Loss: 0.0017180775612359866, Final Batch Loss: 0.0007977275527082384\n",
      "Epoch 4518, Loss: 0.027601928275544196, Final Batch Loss: 0.0018727411516010761\n",
      "Epoch 4519, Loss: 0.002105673644109629, Final Batch Loss: 0.0001491716830059886\n",
      "Epoch 4520, Loss: 0.003660758273326792, Final Batch Loss: 0.00020053079060744494\n",
      "Epoch 4521, Loss: 0.004687356922659092, Final Batch Loss: 0.0029220234137028456\n",
      "Epoch 4522, Loss: 0.01160394394537434, Final Batch Loss: 0.005503782071173191\n",
      "Epoch 4523, Loss: 0.006391762901330367, Final Batch Loss: 0.0038783184718340635\n",
      "Epoch 4524, Loss: 0.007884283331804909, Final Batch Loss: 0.006019501481205225\n",
      "Epoch 4525, Loss: 0.03120328448130749, Final Batch Loss: 0.0017327124951407313\n",
      "Epoch 4526, Loss: 0.00904470898967702, Final Batch Loss: 0.005316870752722025\n",
      "Epoch 4527, Loss: 0.022293843881925568, Final Batch Loss: 0.005725543014705181\n",
      "Epoch 4528, Loss: 0.006515903405670542, Final Batch Loss: 0.0003937190631404519\n",
      "Epoch 4529, Loss: 0.009981226787203923, Final Batch Loss: 0.0005994134698994458\n",
      "Epoch 4530, Loss: 0.010400740618933924, Final Batch Loss: 0.0019039892358705401\n",
      "Epoch 4531, Loss: 0.012275625398615375, Final Batch Loss: 0.008680746890604496\n",
      "Epoch 4532, Loss: 0.0016561377851758152, Final Batch Loss: 0.00033522266312502325\n",
      "Epoch 4533, Loss: 0.017378852047841065, Final Batch Loss: 0.00012759806122630835\n",
      "Epoch 4534, Loss: 0.0065439935024187434, Final Batch Loss: 0.00035610838676802814\n",
      "Epoch 4535, Loss: 0.008528191930963658, Final Batch Loss: 0.006806191988289356\n",
      "Epoch 4536, Loss: 0.004897248552879319, Final Batch Loss: 0.00019974539463873953\n",
      "Epoch 4537, Loss: 0.005001022189389914, Final Batch Loss: 0.0003563743084669113\n",
      "Epoch 4538, Loss: 0.011166498850798234, Final Batch Loss: 0.0004779465089086443\n",
      "Epoch 4539, Loss: 0.014337948930915445, Final Batch Loss: 0.0011618854478001595\n",
      "Epoch 4540, Loss: 0.028975993598578498, Final Batch Loss: 0.005543703678995371\n",
      "Epoch 4541, Loss: 0.04242773645091802, Final Batch Loss: 0.027349218726158142\n",
      "Epoch 4542, Loss: 0.026404797099530697, Final Batch Loss: 0.0001311926171183586\n",
      "Epoch 4543, Loss: 0.024081753537757322, Final Batch Loss: 8.9688524894882e-05\n",
      "Epoch 4544, Loss: 0.002695717877941206, Final Batch Loss: 0.00022208873997442424\n",
      "Epoch 4545, Loss: 0.005359623071854003, Final Batch Loss: 0.002328207716345787\n",
      "Epoch 4546, Loss: 0.0022846862848382443, Final Batch Loss: 0.000513158505782485\n",
      "Epoch 4547, Loss: 0.007939934232126689, Final Batch Loss: 0.0012469537323340774\n",
      "Epoch 4548, Loss: 0.0022067578393034637, Final Batch Loss: 0.0002426634746370837\n",
      "Epoch 4549, Loss: 0.004700778779806569, Final Batch Loss: 0.00017427088459953666\n",
      "Epoch 4550, Loss: 0.0027018322580261156, Final Batch Loss: 0.0005712502170354128\n",
      "Epoch 4551, Loss: 0.0006366313245962374, Final Batch Loss: 4.903695662505925e-05\n",
      "Epoch 4552, Loss: 0.03921062653535046, Final Batch Loss: 0.0008471595938317478\n",
      "Epoch 4553, Loss: 0.009806576137634693, Final Batch Loss: 3.444387039053254e-05\n",
      "Epoch 4554, Loss: 0.04181855775823351, Final Batch Loss: 0.00013756602129433304\n",
      "Epoch 4555, Loss: 0.00732408846670296, Final Batch Loss: 0.00020701400353573263\n",
      "Epoch 4556, Loss: 0.005850084518897347, Final Batch Loss: 0.004808053839951754\n",
      "Epoch 4557, Loss: 0.006212710359250195, Final Batch Loss: 0.00015976479335222393\n",
      "Epoch 4558, Loss: 0.006833844454376958, Final Batch Loss: 0.0012005309108644724\n",
      "Epoch 4559, Loss: 0.02539354414329864, Final Batch Loss: 0.0003952890110667795\n",
      "Epoch 4560, Loss: 0.01423181951395236, Final Batch Loss: 0.00010853302956093103\n",
      "Epoch 4561, Loss: 0.006420947698643431, Final Batch Loss: 0.0025221158284693956\n",
      "Epoch 4562, Loss: 0.0028583815874299034, Final Batch Loss: 0.00023909215815365314\n",
      "Epoch 4563, Loss: 0.006569004006450996, Final Batch Loss: 0.0013590821763500571\n",
      "Epoch 4564, Loss: 0.026154599792789668, Final Batch Loss: 0.0001569264568388462\n",
      "Epoch 4565, Loss: 0.041331052663736045, Final Batch Loss: 0.00025956574245356023\n",
      "Epoch 4566, Loss: 0.005055316214566119, Final Batch Loss: 0.0023457773495465517\n",
      "Epoch 4567, Loss: 0.04897088906727731, Final Batch Loss: 0.0024993696715682745\n",
      "Epoch 4568, Loss: 0.0053841408516746014, Final Batch Loss: 0.00040313240606337786\n",
      "Epoch 4569, Loss: 0.02526868460699916, Final Batch Loss: 0.0029927538707852364\n",
      "Epoch 4570, Loss: 0.0064023332597571425, Final Batch Loss: 0.0017198830610141158\n",
      "Epoch 4571, Loss: 0.026420745416544378, Final Batch Loss: 0.0008197755087167025\n",
      "Epoch 4572, Loss: 0.018552718247519806, Final Batch Loss: 0.013342386111617088\n",
      "Epoch 4573, Loss: 0.024288731772685423, Final Batch Loss: 0.0007071303552947938\n",
      "Epoch 4574, Loss: 0.03708814699348295, Final Batch Loss: 0.00046455173287540674\n",
      "Epoch 4575, Loss: 0.009826900321058929, Final Batch Loss: 0.004137857351452112\n",
      "Epoch 4576, Loss: 0.050145691726356745, Final Batch Loss: 0.026487773284316063\n",
      "Epoch 4577, Loss: 0.036198392044752836, Final Batch Loss: 0.013169116340577602\n",
      "Epoch 4578, Loss: 0.047459447756409645, Final Batch Loss: 0.02227269858121872\n",
      "Epoch 4579, Loss: 0.003636048117186874, Final Batch Loss: 0.0018836784875020385\n",
      "Epoch 4580, Loss: 0.01354358933167532, Final Batch Loss: 0.004379356279969215\n",
      "Epoch 4581, Loss: 0.0054767859692219645, Final Batch Loss: 0.0008627248462289572\n",
      "Epoch 4582, Loss: 0.03756501281168312, Final Batch Loss: 0.027983225882053375\n",
      "Epoch 4583, Loss: 0.07913113723043352, Final Batch Loss: 0.04714128002524376\n",
      "Epoch 4584, Loss: 0.039813710129237734, Final Batch Loss: 0.0016202694969251752\n",
      "Epoch 4585, Loss: 0.02886095328722149, Final Batch Loss: 0.000726716301869601\n",
      "Epoch 4586, Loss: 0.017776550172129646, Final Batch Loss: 0.0044140806421637535\n",
      "Epoch 4587, Loss: 0.02414193988079205, Final Batch Loss: 0.01419114414602518\n",
      "Epoch 4588, Loss: 0.01629960653372109, Final Batch Loss: 0.0008778433548286557\n",
      "Epoch 4589, Loss: 0.012023458490148187, Final Batch Loss: 0.0023608915507793427\n",
      "Epoch 4590, Loss: 0.011980866431258619, Final Batch Loss: 0.0017884334083646536\n",
      "Epoch 4591, Loss: 0.003078544163145125, Final Batch Loss: 0.0003455577534623444\n",
      "Epoch 4592, Loss: 0.013530801341403276, Final Batch Loss: 0.00016394889098592103\n",
      "Epoch 4593, Loss: 0.03851591725833714, Final Batch Loss: 0.007044566795229912\n",
      "Epoch 4594, Loss: 0.0064383673161501065, Final Batch Loss: 0.000215945256059058\n",
      "Epoch 4595, Loss: 0.047296315897256136, Final Batch Loss: 0.0012795052025467157\n",
      "Epoch 4596, Loss: 0.009775414117029868, Final Batch Loss: 0.0004993213806301355\n",
      "Epoch 4597, Loss: 0.021351863717427477, Final Batch Loss: 0.00035029806895181537\n",
      "Epoch 4598, Loss: 0.013820984459016472, Final Batch Loss: 0.005479799117892981\n",
      "Epoch 4599, Loss: 0.012977243954082951, Final Batch Loss: 0.004358851816505194\n",
      "Epoch 4600, Loss: 0.016464191721752286, Final Batch Loss: 0.0036539174616336823\n",
      "Epoch 4601, Loss: 0.014123033441137522, Final Batch Loss: 0.0006080354214645922\n",
      "Epoch 4602, Loss: 0.030684865079820156, Final Batch Loss: 0.01407085731625557\n",
      "Epoch 4603, Loss: 0.014447162771830335, Final Batch Loss: 0.0003194496384821832\n",
      "Epoch 4604, Loss: 0.034786265110597014, Final Batch Loss: 0.0032946360297501087\n",
      "Epoch 4605, Loss: 0.0075557075324468315, Final Batch Loss: 0.000795551692135632\n",
      "Epoch 4606, Loss: 0.006486203521490097, Final Batch Loss: 0.0006475994596257806\n",
      "Epoch 4607, Loss: 0.009795301069971174, Final Batch Loss: 0.004549065139144659\n",
      "Epoch 4608, Loss: 0.01885703334119171, Final Batch Loss: 0.00029396574245765805\n",
      "Epoch 4609, Loss: 0.007948917394969612, Final Batch Loss: 0.0019841338507831097\n",
      "Epoch 4610, Loss: 0.010098226179252379, Final Batch Loss: 0.00029435602482408285\n",
      "Epoch 4611, Loss: 0.006544328360178042, Final Batch Loss: 0.002166137332096696\n",
      "Epoch 4612, Loss: 0.004481115756789222, Final Batch Loss: 6.193306762725115e-05\n",
      "Epoch 4613, Loss: 0.0033010636980179697, Final Batch Loss: 0.0013550055446103215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4614, Loss: 0.00860356108751148, Final Batch Loss: 0.0008546984172426164\n",
      "Epoch 4615, Loss: 0.023456732771592215, Final Batch Loss: 0.003343484364449978\n",
      "Epoch 4616, Loss: 0.013605955988168716, Final Batch Loss: 0.0030184099450707436\n",
      "Epoch 4617, Loss: 0.020273064001230523, Final Batch Loss: 0.0012155643198639154\n",
      "Epoch 4618, Loss: 0.010516786787775345, Final Batch Loss: 0.0011064053978770971\n",
      "Epoch 4619, Loss: 0.004232145787682384, Final Batch Loss: 0.00035188274341635406\n",
      "Epoch 4620, Loss: 0.0064169373363256454, Final Batch Loss: 0.0023656843695789576\n",
      "Epoch 4621, Loss: 0.0020000840740976855, Final Batch Loss: 0.00021972466493025422\n",
      "Epoch 4622, Loss: 0.006939111226529349, Final Batch Loss: 0.0001151644901256077\n",
      "Epoch 4623, Loss: 0.024528923313482665, Final Batch Loss: 0.0007388799567706883\n",
      "Epoch 4624, Loss: 0.0034928125387523323, Final Batch Loss: 0.00034236180363222957\n",
      "Epoch 4625, Loss: 0.009286353801144287, Final Batch Loss: 0.0002773428277578205\n",
      "Epoch 4626, Loss: 0.007582318619824946, Final Batch Loss: 0.002261012326925993\n",
      "Epoch 4627, Loss: 0.015910648507997394, Final Batch Loss: 0.0002715274749789387\n",
      "Epoch 4628, Loss: 0.036832781945122406, Final Batch Loss: 0.000673960370477289\n",
      "Epoch 4629, Loss: 0.00348582498554606, Final Batch Loss: 0.0011227442882955074\n",
      "Epoch 4630, Loss: 0.002137012532330118, Final Batch Loss: 0.00048039754619821906\n",
      "Epoch 4631, Loss: 0.03748727438505739, Final Batch Loss: 0.004986480809748173\n",
      "Epoch 4632, Loss: 0.01586701841733884, Final Batch Loss: 0.006378689780831337\n",
      "Epoch 4633, Loss: 0.025036781502421945, Final Batch Loss: 0.0001993408950511366\n",
      "Epoch 4634, Loss: 0.042025786999147385, Final Batch Loss: 0.0024770423769950867\n",
      "Epoch 4635, Loss: 0.006827904609963298, Final Batch Loss: 0.0003066842909902334\n",
      "Epoch 4636, Loss: 0.005384586867876351, Final Batch Loss: 0.0014477913500741124\n",
      "Epoch 4637, Loss: 0.004354939432232641, Final Batch Loss: 9.555705764796585e-05\n",
      "Epoch 4638, Loss: 0.014482820726698264, Final Batch Loss: 0.0010569404112175107\n",
      "Epoch 4639, Loss: 0.010422535124234855, Final Batch Loss: 0.0005639665760099888\n",
      "Epoch 4640, Loss: 0.032011971503379755, Final Batch Loss: 0.00014280439063441008\n",
      "Epoch 4641, Loss: 0.006864165305159986, Final Batch Loss: 0.00028014887357130647\n",
      "Epoch 4642, Loss: 0.008838084973831428, Final Batch Loss: 0.007405981887131929\n",
      "Epoch 4643, Loss: 0.0017794853847590275, Final Batch Loss: 0.0006425430765375495\n",
      "Epoch 4644, Loss: 0.013887343608075753, Final Batch Loss: 0.00024274902534671128\n",
      "Epoch 4645, Loss: 0.022001281788107008, Final Batch Loss: 0.0011558239348232746\n",
      "Epoch 4646, Loss: 0.006783418066333979, Final Batch Loss: 0.00018884250312112272\n",
      "Epoch 4647, Loss: 0.034302199113881215, Final Batch Loss: 0.011618854478001595\n",
      "Epoch 4648, Loss: 0.014007525314809754, Final Batch Loss: 0.012265719473361969\n",
      "Epoch 4649, Loss: 0.009113684995099902, Final Batch Loss: 0.0005602019373327494\n",
      "Epoch 4650, Loss: 0.03612045574118383, Final Batch Loss: 0.019568270072340965\n",
      "Epoch 4651, Loss: 0.00959864049218595, Final Batch Loss: 0.004136773757636547\n",
      "Epoch 4652, Loss: 0.01421450063935481, Final Batch Loss: 6.882453453727067e-05\n",
      "Epoch 4653, Loss: 0.006289695360464975, Final Batch Loss: 0.0003038757131434977\n",
      "Epoch 4654, Loss: 0.027074623591033742, Final Batch Loss: 0.002535398816689849\n",
      "Epoch 4655, Loss: 0.03643253308837302, Final Batch Loss: 0.004860302433371544\n",
      "Epoch 4656, Loss: 0.026881014520768076, Final Batch Loss: 0.00043292954796925187\n",
      "Epoch 4657, Loss: 0.03248489846009761, Final Batch Loss: 0.006290074437856674\n",
      "Epoch 4658, Loss: 0.01714238536078483, Final Batch Loss: 0.005464321933686733\n",
      "Epoch 4659, Loss: 0.04200597258750349, Final Batch Loss: 0.0006447479827329516\n",
      "Epoch 4660, Loss: 0.02059143700171262, Final Batch Loss: 0.004016341641545296\n",
      "Epoch 4661, Loss: 0.025767663697479293, Final Batch Loss: 0.003448057919740677\n",
      "Epoch 4662, Loss: 0.009969187114620581, Final Batch Loss: 0.007478644605726004\n",
      "Epoch 4663, Loss: 0.037753972108475864, Final Batch Loss: 0.0004333035321906209\n",
      "Epoch 4664, Loss: 0.01295378185750451, Final Batch Loss: 0.0009224463137798011\n",
      "Epoch 4665, Loss: 0.022724342648871243, Final Batch Loss: 0.000823340960778296\n",
      "Epoch 4666, Loss: 0.013781502260826528, Final Batch Loss: 0.001653945306316018\n",
      "Epoch 4667, Loss: 0.02698126033646986, Final Batch Loss: 0.0009176242747344077\n",
      "Epoch 4668, Loss: 0.0067060913279419765, Final Batch Loss: 0.0002162908058380708\n",
      "Epoch 4669, Loss: 0.01731898944126442, Final Batch Loss: 0.0014751831768080592\n",
      "Epoch 4670, Loss: 0.01389231375651434, Final Batch Loss: 0.0007311229128390551\n",
      "Epoch 4671, Loss: 0.014243528858060017, Final Batch Loss: 0.011389224790036678\n",
      "Epoch 4672, Loss: 0.019810849567875266, Final Batch Loss: 0.0007404135540127754\n",
      "Epoch 4673, Loss: 0.008000005575013347, Final Batch Loss: 0.005495305638760328\n",
      "Epoch 4674, Loss: 0.0148012267309241, Final Batch Loss: 0.00039053644286468625\n",
      "Epoch 4675, Loss: 0.00905475887702778, Final Batch Loss: 0.0009473199024796486\n",
      "Epoch 4676, Loss: 0.008798519309493713, Final Batch Loss: 0.0011857347562909126\n",
      "Epoch 4677, Loss: 0.010072437718918081, Final Batch Loss: 0.008044544607400894\n",
      "Epoch 4678, Loss: 0.0033235985501960386, Final Batch Loss: 0.00021914159879088402\n",
      "Epoch 4679, Loss: 0.011346505343681201, Final Batch Loss: 0.00037587477709166706\n",
      "Epoch 4680, Loss: 0.007874517585150898, Final Batch Loss: 0.0007671262137591839\n",
      "Epoch 4681, Loss: 0.009047573905263562, Final Batch Loss: 0.0019221070688217878\n",
      "Epoch 4682, Loss: 0.003963784198276699, Final Batch Loss: 0.00019665365107357502\n",
      "Epoch 4683, Loss: 0.005474131088703871, Final Batch Loss: 0.0007309393840841949\n",
      "Epoch 4684, Loss: 0.001671344980422873, Final Batch Loss: 0.0003011494118254632\n",
      "Epoch 4685, Loss: 0.007401755341561511, Final Batch Loss: 0.0003674177860375494\n",
      "Epoch 4686, Loss: 0.004262828137143515, Final Batch Loss: 0.0011428165016695857\n",
      "Epoch 4687, Loss: 0.013509199590771459, Final Batch Loss: 0.0005162959569133818\n",
      "Epoch 4688, Loss: 0.002377855096710846, Final Batch Loss: 0.0005882065743207932\n",
      "Epoch 4689, Loss: 0.004101008868019562, Final Batch Loss: 0.00018380011897534132\n",
      "Epoch 4690, Loss: 0.001976685402041767, Final Batch Loss: 0.00020806692191399634\n",
      "Epoch 4691, Loss: 0.0026128987810807303, Final Batch Loss: 3.858181298710406e-05\n",
      "Epoch 4692, Loss: 0.009295538038713858, Final Batch Loss: 0.00018953514518216252\n",
      "Epoch 4693, Loss: 0.005501544768776512, Final Batch Loss: 0.0024635957088321447\n",
      "Epoch 4694, Loss: 0.008228241887991317, Final Batch Loss: 0.0024443333968520164\n",
      "Epoch 4695, Loss: 0.005946977256826358, Final Batch Loss: 0.0011686377692967653\n",
      "Epoch 4696, Loss: 0.009846277418546379, Final Batch Loss: 0.0002245982177555561\n",
      "Epoch 4697, Loss: 0.01058553826806019, Final Batch Loss: 0.007014818489551544\n",
      "Epoch 4698, Loss: 0.0024989708908833563, Final Batch Loss: 0.0003623340744525194\n",
      "Epoch 4699, Loss: 0.042773632128955796, Final Batch Loss: 0.00031118866172619164\n",
      "Epoch 4700, Loss: 0.016407146060373634, Final Batch Loss: 9.167419193545356e-05\n",
      "Epoch 4701, Loss: 0.005696638658264419, Final Batch Loss: 0.0027517969720065594\n",
      "Epoch 4702, Loss: 0.007555114869319368, Final Batch Loss: 0.0012596427695825696\n",
      "Epoch 4703, Loss: 0.012104441193514504, Final Batch Loss: 0.00023547680757474154\n",
      "Epoch 4704, Loss: 0.006618668019655161, Final Batch Loss: 0.0003135597798973322\n",
      "Epoch 4705, Loss: 0.038068751120590605, Final Batch Loss: 0.0014424255350604653\n",
      "Epoch 4706, Loss: 0.0023122080965549685, Final Batch Loss: 0.0007334795081987977\n",
      "Epoch 4707, Loss: 0.01044490320055047, Final Batch Loss: 0.006240502931177616\n",
      "Epoch 4708, Loss: 0.0017156084213638678, Final Batch Loss: 0.00027260591741651297\n",
      "Epoch 4709, Loss: 0.0033587728248676285, Final Batch Loss: 0.0006402789149433374\n",
      "Epoch 4710, Loss: 0.006917314130987506, Final Batch Loss: 0.0005535610252991319\n",
      "Epoch 4711, Loss: 0.014321400129119866, Final Batch Loss: 0.00022934509615879506\n",
      "Epoch 4712, Loss: 0.001250640460057184, Final Batch Loss: 0.00036144466139376163\n",
      "Epoch 4713, Loss: 0.003797229132032953, Final Batch Loss: 9.16909339139238e-05\n",
      "Epoch 4714, Loss: 0.01268331344545004, Final Batch Loss: 0.008688647300004959\n",
      "Epoch 4715, Loss: 0.016478104837005958, Final Batch Loss: 0.000278265040833503\n",
      "Epoch 4716, Loss: 0.010018450622737873, Final Batch Loss: 0.004052660893648863\n",
      "Epoch 4717, Loss: 0.004931762348860502, Final Batch Loss: 0.001413456629961729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4718, Loss: 0.0029237572198326234, Final Batch Loss: 0.0010946582769975066\n",
      "Epoch 4719, Loss: 0.012156956057879142, Final Batch Loss: 0.011270172894001007\n",
      "Epoch 4720, Loss: 0.01115022980957292, Final Batch Loss: 0.0007953917956911027\n",
      "Epoch 4721, Loss: 0.008200930424209218, Final Batch Loss: 0.005844928324222565\n",
      "Epoch 4722, Loss: 0.01309128777938895, Final Batch Loss: 0.0002990240463986993\n",
      "Epoch 4723, Loss: 0.0014400940563064069, Final Batch Loss: 9.7392410680186e-05\n",
      "Epoch 4724, Loss: 0.0011439902518759482, Final Batch Loss: 4.503832315094769e-05\n",
      "Epoch 4725, Loss: 0.004080227925442159, Final Batch Loss: 0.00030612520640715957\n",
      "Epoch 4726, Loss: 0.0016784309045760892, Final Batch Loss: 0.0004943463136442006\n",
      "Epoch 4727, Loss: 0.04022489272756502, Final Batch Loss: 5.0244329031556845e-05\n",
      "Epoch 4728, Loss: 0.00686070242227288, Final Batch Loss: 0.0003232724848203361\n",
      "Epoch 4729, Loss: 0.0026002129452535883, Final Batch Loss: 0.0006318985833786428\n",
      "Epoch 4730, Loss: 0.029328260017791763, Final Batch Loss: 0.0005779122584499419\n",
      "Epoch 4731, Loss: 0.005211125375353731, Final Batch Loss: 0.00020830122230108827\n",
      "Epoch 4732, Loss: 0.001652503928198712, Final Batch Loss: 1.6230322216870263e-05\n",
      "Epoch 4733, Loss: 0.0014115950907580554, Final Batch Loss: 0.00028460947214625776\n",
      "Epoch 4734, Loss: 0.009373180466354825, Final Batch Loss: 0.0002544824092183262\n",
      "Epoch 4735, Loss: 0.01788190394290723, Final Batch Loss: 0.0001415540318703279\n",
      "Epoch 4736, Loss: 0.011525485751917586, Final Batch Loss: 0.009231798350811005\n",
      "Epoch 4737, Loss: 0.012014143278065603, Final Batch Loss: 0.0002573914243839681\n",
      "Epoch 4738, Loss: 0.0033036755630746484, Final Batch Loss: 0.00044558904482983053\n",
      "Epoch 4739, Loss: 0.001715760627121199, Final Batch Loss: 0.000502979673910886\n",
      "Epoch 4740, Loss: 0.0320453657041071, Final Batch Loss: 0.0006144448416307569\n",
      "Epoch 4741, Loss: 0.0024797059813863598, Final Batch Loss: 0.001164975925348699\n",
      "Epoch 4742, Loss: 0.008845307820593007, Final Batch Loss: 0.0007495877798646688\n",
      "Epoch 4743, Loss: 0.007086530335072894, Final Batch Loss: 4.621542029781267e-05\n",
      "Epoch 4744, Loss: 0.012923412330565043, Final Batch Loss: 0.0047493185847997665\n",
      "Epoch 4745, Loss: 0.015796225736266933, Final Batch Loss: 0.00031443845364265144\n",
      "Epoch 4746, Loss: 0.018301969845197164, Final Batch Loss: 0.00024365114222746342\n",
      "Epoch 4747, Loss: 0.004828709177672863, Final Batch Loss: 0.0005692605627700686\n",
      "Epoch 4748, Loss: 0.07086142699699849, Final Batch Loss: 0.00023114134091883898\n",
      "Epoch 4749, Loss: 0.008341567649040371, Final Batch Loss: 0.0006844281451776624\n",
      "Epoch 4750, Loss: 0.03247932928206865, Final Batch Loss: 0.0001437798055121675\n",
      "Epoch 4751, Loss: 0.025861972928396426, Final Batch Loss: 0.00029753183480352163\n",
      "Epoch 4752, Loss: 0.03035590733634308, Final Batch Loss: 0.0014686713693663478\n",
      "Epoch 4753, Loss: 0.004882383640506305, Final Batch Loss: 0.00044513214379549026\n",
      "Epoch 4754, Loss: 0.009813329204916954, Final Batch Loss: 0.00478332256898284\n",
      "Epoch 4755, Loss: 0.04127086331573082, Final Batch Loss: 0.0007804131601005793\n",
      "Epoch 4756, Loss: 0.0027318976062815636, Final Batch Loss: 0.0003672477905638516\n",
      "Epoch 4757, Loss: 0.004224808151775505, Final Batch Loss: 0.0008642658940516412\n",
      "Epoch 4758, Loss: 0.010285862517775968, Final Batch Loss: 0.008917354978621006\n",
      "Epoch 4759, Loss: 0.010810172214405611, Final Batch Loss: 0.008916500955820084\n",
      "Epoch 4760, Loss: 0.010585936601273715, Final Batch Loss: 0.0041092210449278355\n",
      "Epoch 4761, Loss: 0.07941732244216837, Final Batch Loss: 0.002277412684634328\n",
      "Epoch 4762, Loss: 0.0019399151642573997, Final Batch Loss: 0.0003998648899141699\n",
      "Epoch 4763, Loss: 0.006409419132978655, Final Batch Loss: 0.004385750740766525\n",
      "Epoch 4764, Loss: 0.002985816419823095, Final Batch Loss: 0.0005174237885512412\n",
      "Epoch 4765, Loss: 0.0052502828621072695, Final Batch Loss: 0.0027738227508962154\n",
      "Epoch 4766, Loss: 0.010093333774420898, Final Batch Loss: 9.465641778660938e-05\n",
      "Epoch 4767, Loss: 0.001971457211766392, Final Batch Loss: 0.000575886748265475\n",
      "Epoch 4768, Loss: 0.014615618565585464, Final Batch Loss: 0.0010232384083792567\n",
      "Epoch 4769, Loss: 0.018009759194683284, Final Batch Loss: 0.00039600825402885675\n",
      "Epoch 4770, Loss: 0.019651967333629727, Final Batch Loss: 0.004765501245856285\n",
      "Epoch 4771, Loss: 0.005846355459652841, Final Batch Loss: 0.0004811274993699044\n",
      "Epoch 4772, Loss: 0.016144163069839124, Final Batch Loss: 0.000380380020942539\n",
      "Epoch 4773, Loss: 0.011177896260051057, Final Batch Loss: 0.0008178073912858963\n",
      "Epoch 4774, Loss: 0.019909522001398727, Final Batch Loss: 0.00045311453868635\n",
      "Epoch 4775, Loss: 0.009110830666031688, Final Batch Loss: 0.0010196564253419638\n",
      "Epoch 4776, Loss: 0.02961275498091709, Final Batch Loss: 0.01118732150644064\n",
      "Epoch 4777, Loss: 0.009556522476486862, Final Batch Loss: 0.00027937328559346497\n",
      "Epoch 4778, Loss: 0.006478436931502074, Final Batch Loss: 0.003858105279505253\n",
      "Epoch 4779, Loss: 0.01710320625716122, Final Batch Loss: 0.010023415088653564\n",
      "Epoch 4780, Loss: 0.037371225844253786, Final Batch Loss: 0.00019534921739250422\n",
      "Epoch 4781, Loss: 0.0017517132291686721, Final Batch Loss: 7.200896652648225e-05\n",
      "Epoch 4782, Loss: 0.009265890694223344, Final Batch Loss: 0.000589468574617058\n",
      "Epoch 4783, Loss: 0.0065920708439080045, Final Batch Loss: 0.0014129860792309046\n",
      "Epoch 4784, Loss: 0.0032374718284700066, Final Batch Loss: 0.0002194567641709\n",
      "Epoch 4785, Loss: 0.01038132785470225, Final Batch Loss: 0.002578944433480501\n",
      "Epoch 4786, Loss: 0.010710741044022143, Final Batch Loss: 0.003811843693256378\n",
      "Epoch 4787, Loss: 0.016339869995135814, Final Batch Loss: 0.00029343500500544906\n",
      "Epoch 4788, Loss: 0.003491030900477199, Final Batch Loss: 0.00026598162367008626\n",
      "Epoch 4789, Loss: 0.007255627369886497, Final Batch Loss: 0.0009197206236422062\n",
      "Epoch 4790, Loss: 0.01792775831563631, Final Batch Loss: 0.00022893935965839773\n",
      "Epoch 4791, Loss: 0.02298647735733539, Final Batch Loss: 0.0005465899012051523\n",
      "Epoch 4792, Loss: 0.026781303851748817, Final Batch Loss: 0.007984710857272148\n",
      "Epoch 4793, Loss: 0.0034746581222862005, Final Batch Loss: 0.000938611919991672\n",
      "Epoch 4794, Loss: 0.04428951005684212, Final Batch Loss: 0.015020297840237617\n",
      "Epoch 4795, Loss: 0.07413813294260763, Final Batch Loss: 0.001130404300056398\n",
      "Epoch 4796, Loss: 0.02756164185120724, Final Batch Loss: 0.018101025372743607\n",
      "Epoch 4797, Loss: 0.003505917004076764, Final Batch Loss: 0.001002023695036769\n",
      "Epoch 4798, Loss: 0.02322912913223263, Final Batch Loss: 0.007804252672940493\n",
      "Epoch 4799, Loss: 0.03308654291322455, Final Batch Loss: 0.00036538892891258\n",
      "Epoch 4800, Loss: 0.010855222979444079, Final Batch Loss: 0.0004978221259079874\n",
      "Epoch 4801, Loss: 0.006746325787389651, Final Batch Loss: 0.00018379706307314336\n",
      "Epoch 4802, Loss: 0.014123415923677385, Final Batch Loss: 0.0007935881149023771\n",
      "Epoch 4803, Loss: 0.030316871707327664, Final Batch Loss: 0.01803196221590042\n",
      "Epoch 4804, Loss: 0.013478502340149134, Final Batch Loss: 0.0008301466004922986\n",
      "Epoch 4805, Loss: 0.0197408188105328, Final Batch Loss: 0.012166661210358143\n",
      "Epoch 4806, Loss: 0.003650733851827681, Final Batch Loss: 0.0009017562842927873\n",
      "Epoch 4807, Loss: 0.05168281571241096, Final Batch Loss: 0.0022961124777793884\n",
      "Epoch 4808, Loss: 0.016777660304796882, Final Batch Loss: 0.004958080127835274\n",
      "Epoch 4809, Loss: 0.006694444251479581, Final Batch Loss: 0.0009062894387170672\n",
      "Epoch 4810, Loss: 0.010198746429523453, Final Batch Loss: 0.005091242957860231\n",
      "Epoch 4811, Loss: 0.02160162247309927, Final Batch Loss: 0.0045813326723873615\n",
      "Epoch 4812, Loss: 0.007542037274106406, Final Batch Loss: 0.0011838768841698766\n",
      "Epoch 4813, Loss: 0.011170486119226553, Final Batch Loss: 0.006041390355676413\n",
      "Epoch 4814, Loss: 0.03372498422686476, Final Batch Loss: 0.00019725105084944516\n",
      "Epoch 4815, Loss: 0.006888807089126203, Final Batch Loss: 0.000108574669866357\n",
      "Epoch 4816, Loss: 0.003173786390107125, Final Batch Loss: 0.00029882026137784123\n",
      "Epoch 4817, Loss: 0.008021185174584389, Final Batch Loss: 0.000260400352999568\n",
      "Epoch 4818, Loss: 0.010809349565533921, Final Batch Loss: 0.00735728582367301\n",
      "Epoch 4819, Loss: 0.014337235639686696, Final Batch Loss: 0.00014040456153452396\n",
      "Epoch 4820, Loss: 0.008495703048538417, Final Batch Loss: 0.00032734143314883113\n",
      "Epoch 4821, Loss: 0.007798427526722662, Final Batch Loss: 0.0022658335510641336\n",
      "Epoch 4822, Loss: 0.006934191886102781, Final Batch Loss: 0.0029686493799090385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4823, Loss: 0.02176446039811708, Final Batch Loss: 0.00037915026769042015\n",
      "Epoch 4824, Loss: 0.005764388159150258, Final Batch Loss: 0.0013764416798949242\n",
      "Epoch 4825, Loss: 0.005319887521181954, Final Batch Loss: 4.555079431156628e-05\n",
      "Epoch 4826, Loss: 0.010539635713939788, Final Batch Loss: 0.003786677960306406\n",
      "Epoch 4827, Loss: 0.03428956439893227, Final Batch Loss: 0.00029255799017846584\n",
      "Epoch 4828, Loss: 0.0017300939507549629, Final Batch Loss: 0.0003421037981752306\n",
      "Epoch 4829, Loss: 0.007164401933550835, Final Batch Loss: 0.0001802608894649893\n",
      "Epoch 4830, Loss: 0.007949827573611401, Final Batch Loss: 0.0004130275337956846\n",
      "Epoch 4831, Loss: 0.006246608158107847, Final Batch Loss: 0.0010879969922825694\n",
      "Epoch 4832, Loss: 0.02144163631601259, Final Batch Loss: 0.014907445758581161\n",
      "Epoch 4833, Loss: 0.008582966460380703, Final Batch Loss: 0.0059656850062310696\n",
      "Epoch 4834, Loss: 0.013166855707822833, Final Batch Loss: 0.0012306876014918089\n",
      "Epoch 4835, Loss: 0.005049506857176311, Final Batch Loss: 0.0003639048954937607\n",
      "Epoch 4836, Loss: 0.0025349565112264827, Final Batch Loss: 0.00017924346320796758\n",
      "Epoch 4837, Loss: 0.004356243734946474, Final Batch Loss: 0.0005067487945780158\n",
      "Epoch 4838, Loss: 0.0034764540032483637, Final Batch Loss: 0.0006800242699682713\n",
      "Epoch 4839, Loss: 0.004708435855718562, Final Batch Loss: 0.0019292386714369059\n",
      "Epoch 4840, Loss: 0.045482791334507056, Final Batch Loss: 0.0002973720256704837\n",
      "Epoch 4841, Loss: 0.0026408082521811593, Final Batch Loss: 0.0012771536130458117\n",
      "Epoch 4842, Loss: 0.01104874224984087, Final Batch Loss: 0.0009621541830711067\n",
      "Epoch 4843, Loss: 0.010146841683308594, Final Batch Loss: 0.0001161217805929482\n",
      "Epoch 4844, Loss: 0.005246386586804874, Final Batch Loss: 0.0012106538051739335\n",
      "Epoch 4845, Loss: 0.006331010954454541, Final Batch Loss: 0.0026104161515831947\n",
      "Epoch 4846, Loss: 0.0052826083556283265, Final Batch Loss: 0.0004920770297758281\n",
      "Epoch 4847, Loss: 0.007404140640574042, Final Batch Loss: 0.00012044182949466631\n",
      "Epoch 4848, Loss: 0.002571204589912668, Final Batch Loss: 0.00031452783150598407\n",
      "Epoch 4849, Loss: 0.004808468082046602, Final Batch Loss: 0.0002821264788508415\n",
      "Epoch 4850, Loss: 0.0244007869696361, Final Batch Loss: 0.01684449426829815\n",
      "Epoch 4851, Loss: 0.0007049769810691942, Final Batch Loss: 9.108013182412833e-05\n",
      "Epoch 4852, Loss: 0.006249691476114094, Final Batch Loss: 0.00023703598708380014\n",
      "Epoch 4853, Loss: 0.0021259136556182057, Final Batch Loss: 0.00010960712097585201\n",
      "Epoch 4854, Loss: 0.03303257509833202, Final Batch Loss: 0.0015337137738242745\n",
      "Epoch 4855, Loss: 0.057374311501916964, Final Batch Loss: 0.05575024336576462\n",
      "Epoch 4856, Loss: 0.021196245768805966, Final Batch Loss: 0.0008585331961512566\n",
      "Epoch 4857, Loss: 0.002667737251613289, Final Batch Loss: 0.00018091169476974756\n",
      "Epoch 4858, Loss: 0.01418147467484232, Final Batch Loss: 0.00022261915728449821\n",
      "Epoch 4859, Loss: 0.0017846290429588407, Final Batch Loss: 0.0004362167965155095\n",
      "Epoch 4860, Loss: 0.0019367941422387958, Final Batch Loss: 0.0008064735447987914\n",
      "Epoch 4861, Loss: 0.004248873476171866, Final Batch Loss: 0.0006926249479874969\n",
      "Epoch 4862, Loss: 0.006508448874228634, Final Batch Loss: 0.002638767007738352\n",
      "Epoch 4863, Loss: 0.001971102334209718, Final Batch Loss: 0.00018302042735740542\n",
      "Epoch 4864, Loss: 0.0041219827107852325, Final Batch Loss: 0.001966137206181884\n",
      "Epoch 4865, Loss: 0.0026388480691821314, Final Batch Loss: 0.00011642769823083654\n",
      "Epoch 4866, Loss: 0.0019205454518669285, Final Batch Loss: 0.0008538918918929994\n",
      "Epoch 4867, Loss: 0.00203886600502301, Final Batch Loss: 0.0005031398613937199\n",
      "Epoch 4868, Loss: 0.009145379292021971, Final Batch Loss: 0.008338254876434803\n",
      "Epoch 4869, Loss: 0.00453969172667712, Final Batch Loss: 6.615421443711966e-05\n",
      "Epoch 4870, Loss: 0.0077247909212019295, Final Batch Loss: 0.002713223686441779\n",
      "Epoch 4871, Loss: 0.020034534769365564, Final Batch Loss: 0.0003600087366066873\n",
      "Epoch 4872, Loss: 0.001390682675264543, Final Batch Loss: 0.00028767099138349295\n",
      "Epoch 4873, Loss: 0.00687480007763952, Final Batch Loss: 0.0034064126666635275\n",
      "Epoch 4874, Loss: 0.009954074819688685, Final Batch Loss: 0.00016740613500587642\n",
      "Epoch 4875, Loss: 0.0010630074775690446, Final Batch Loss: 2.82029159279773e-05\n",
      "Epoch 4876, Loss: 0.030660579854156822, Final Batch Loss: 7.80872069299221e-05\n",
      "Epoch 4877, Loss: 0.0044785438003600575, Final Batch Loss: 0.00021070671209599823\n",
      "Epoch 4878, Loss: 0.034332917333813384, Final Batch Loss: 0.032252684235572815\n",
      "Epoch 4879, Loss: 0.0027407319139456376, Final Batch Loss: 0.0006774957873858511\n",
      "Epoch 4880, Loss: 0.0232171244933852, Final Batch Loss: 0.0013672892237082124\n",
      "Epoch 4881, Loss: 0.04012841660551203, Final Batch Loss: 0.0037197426427155733\n",
      "Epoch 4882, Loss: 0.026303196034859866, Final Batch Loss: 0.00015941000310704112\n",
      "Epoch 4883, Loss: 0.0037947481614537537, Final Batch Loss: 0.0006771118496544659\n",
      "Epoch 4884, Loss: 0.0029238092683954164, Final Batch Loss: 0.00022826976783107966\n",
      "Epoch 4885, Loss: 0.012874725704023149, Final Batch Loss: 0.0008857781067490578\n",
      "Epoch 4886, Loss: 0.044847596902400255, Final Batch Loss: 0.007211671676486731\n",
      "Epoch 4887, Loss: 0.022852197900647298, Final Batch Loss: 8.275665459223092e-05\n",
      "Epoch 4888, Loss: 0.00707918819898623, Final Batch Loss: 5.864248305442743e-05\n",
      "Epoch 4889, Loss: 0.009966624464141205, Final Batch Loss: 0.006539515219628811\n",
      "Epoch 4890, Loss: 0.0018648978220880963, Final Batch Loss: 0.0005251222173683345\n",
      "Epoch 4891, Loss: 0.015452761130291037, Final Batch Loss: 0.0003361387352924794\n",
      "Epoch 4892, Loss: 0.0037179755672696047, Final Batch Loss: 0.0006394687225110829\n",
      "Epoch 4893, Loss: 0.015893345116637647, Final Batch Loss: 0.0009797511156648397\n",
      "Epoch 4894, Loss: 0.00937079191498924, Final Batch Loss: 0.0021956926211714745\n",
      "Epoch 4895, Loss: 0.009274125550291501, Final Batch Loss: 0.0001827435044106096\n",
      "Epoch 4896, Loss: 0.005820096499519423, Final Batch Loss: 0.0003822824510280043\n",
      "Epoch 4897, Loss: 0.001236293770489283, Final Batch Loss: 0.00022297944815363735\n",
      "Epoch 4898, Loss: 0.005613043191260658, Final Batch Loss: 0.0009728895383886993\n",
      "Epoch 4899, Loss: 0.010566874610958621, Final Batch Loss: 5.453944322653115e-05\n",
      "Epoch 4900, Loss: 0.005946268240222707, Final Batch Loss: 0.00028649860178120434\n",
      "Epoch 4901, Loss: 0.028516183323517907, Final Batch Loss: 0.0001117387946578674\n",
      "Epoch 4902, Loss: 0.0026975932851200923, Final Batch Loss: 0.00020643837342504412\n",
      "Epoch 4903, Loss: 0.006744437327142805, Final Batch Loss: 0.0005062634008936584\n",
      "Epoch 4904, Loss: 0.0043547207897063345, Final Batch Loss: 0.0008209577645175159\n",
      "Epoch 4905, Loss: 0.013424105796730146, Final Batch Loss: 0.0002797039342112839\n",
      "Epoch 4906, Loss: 0.02015917029348202, Final Batch Loss: 0.0006198883638717234\n",
      "Epoch 4907, Loss: 0.002007012939429842, Final Batch Loss: 0.0002727429964579642\n",
      "Epoch 4908, Loss: 0.002010216354392469, Final Batch Loss: 0.0005693935672752559\n",
      "Epoch 4909, Loss: 0.0033223495120182633, Final Batch Loss: 0.0007597925141453743\n",
      "Epoch 4910, Loss: 0.002503324612916913, Final Batch Loss: 0.00010842652409337461\n",
      "Epoch 4911, Loss: 0.017085052997572348, Final Batch Loss: 0.00048264875658787787\n",
      "Epoch 4912, Loss: 0.016851399770530406, Final Batch Loss: 0.00010226533777313307\n",
      "Epoch 4913, Loss: 0.0016406341892434284, Final Batch Loss: 0.00019718028488568962\n",
      "Epoch 4914, Loss: 0.013859148755727801, Final Batch Loss: 0.012000327929854393\n",
      "Epoch 4915, Loss: 0.0012110859097447246, Final Batch Loss: 0.00019120557408314198\n",
      "Epoch 4916, Loss: 0.006832274375483394, Final Batch Loss: 0.0025562073569744825\n",
      "Epoch 4917, Loss: 0.02085459462250583, Final Batch Loss: 0.0012876054970547557\n",
      "Epoch 4918, Loss: 0.008584601542679593, Final Batch Loss: 0.006024210248142481\n",
      "Epoch 4919, Loss: 0.0014626546326326206, Final Batch Loss: 0.00029027427081018686\n",
      "Epoch 4920, Loss: 0.004882926514255814, Final Batch Loss: 0.00014178823039401323\n",
      "Epoch 4921, Loss: 0.012639456392207649, Final Batch Loss: 0.006878074258565903\n",
      "Epoch 4922, Loss: 0.006268523356993683, Final Batch Loss: 0.00023191471700556576\n",
      "Epoch 4923, Loss: 0.003197941099642776, Final Batch Loss: 0.0012832980137318373\n",
      "Epoch 4924, Loss: 0.004779865797900129, Final Batch Loss: 0.003471007337793708\n",
      "Epoch 4925, Loss: 0.007106096833012998, Final Batch Loss: 0.00027975934790447354\n",
      "Epoch 4926, Loss: 0.00742983294185251, Final Batch Loss: 0.006536948960274458\n",
      "Epoch 4927, Loss: 0.00326032096563722, Final Batch Loss: 0.0009956160793080926\n",
      "Epoch 4928, Loss: 0.024223938256909605, Final Batch Loss: 0.00018338888185098767\n",
      "Epoch 4929, Loss: 0.003841716083115898, Final Batch Loss: 0.00047193930367939174\n",
      "Epoch 4930, Loss: 0.0021239435009192675, Final Batch Loss: 0.0007122012902982533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4931, Loss: 0.0031620395748177543, Final Batch Loss: 0.0003793472424149513\n",
      "Epoch 4932, Loss: 0.003382790178875439, Final Batch Loss: 0.0002222609327873215\n",
      "Epoch 4933, Loss: 0.004701413679867983, Final Batch Loss: 0.0032517763320356607\n",
      "Epoch 4934, Loss: 0.0010348884352424648, Final Batch Loss: 5.65512636967469e-05\n",
      "Epoch 4935, Loss: 0.010982943058479577, Final Batch Loss: 0.0003085044154431671\n",
      "Epoch 4936, Loss: 0.0018880110001191497, Final Batch Loss: 0.00030985509511083364\n",
      "Epoch 4937, Loss: 0.003781179642828647, Final Batch Loss: 0.00011982633441220969\n",
      "Epoch 4938, Loss: 0.0143055384978652, Final Batch Loss: 0.0001896523026516661\n",
      "Epoch 4939, Loss: 0.008457460768113378, Final Batch Loss: 7.695084059378132e-05\n",
      "Epoch 4940, Loss: 0.012906283496704418, Final Batch Loss: 0.00011226820788579062\n",
      "Epoch 4941, Loss: 0.021992684465658385, Final Batch Loss: 0.0013102858792990446\n",
      "Epoch 4942, Loss: 0.003575784299755469, Final Batch Loss: 0.0003659914364106953\n",
      "Epoch 4943, Loss: 0.012699270599114243, Final Batch Loss: 0.00025194528279826045\n",
      "Epoch 4944, Loss: 0.0009847396540862974, Final Batch Loss: 0.0004037174803670496\n",
      "Epoch 4945, Loss: 0.0026039768927148543, Final Batch Loss: 0.0016196263022720814\n",
      "Epoch 4946, Loss: 0.002042037289356813, Final Batch Loss: 0.00011577596887946129\n",
      "Epoch 4947, Loss: 0.004026186750706984, Final Batch Loss: 0.00019151618471369147\n",
      "Epoch 4948, Loss: 0.0013472577556967735, Final Batch Loss: 0.00010097269841935486\n",
      "Epoch 4949, Loss: 0.007510503110097488, Final Batch Loss: 0.0004499780188780278\n",
      "Epoch 4950, Loss: 0.008964024527813308, Final Batch Loss: 0.006826562806963921\n",
      "Epoch 4951, Loss: 0.0028969611012144014, Final Batch Loss: 0.00021734005713369697\n",
      "Epoch 4952, Loss: 0.0260300483496394, Final Batch Loss: 0.0010837161680683494\n",
      "Epoch 4953, Loss: 0.0014278611124609597, Final Batch Loss: 0.0002705954248085618\n",
      "Epoch 4954, Loss: 0.004265549883712083, Final Batch Loss: 0.0006304779672063887\n",
      "Epoch 4955, Loss: 0.018891032610554248, Final Batch Loss: 0.00043539650505408645\n",
      "Epoch 4956, Loss: 0.0033260814016102813, Final Batch Loss: 0.0001233716175192967\n",
      "Epoch 4957, Loss: 0.003539805198670365, Final Batch Loss: 0.00023901306849438697\n",
      "Epoch 4958, Loss: 0.0037343838484957814, Final Batch Loss: 0.0006996598676778376\n",
      "Epoch 4959, Loss: 0.005416788801085204, Final Batch Loss: 0.0007323467289097607\n",
      "Epoch 4960, Loss: 0.01382063805067446, Final Batch Loss: 0.00012522900942713022\n",
      "Epoch 4961, Loss: 0.0027584191120695323, Final Batch Loss: 0.0004506677796598524\n",
      "Epoch 4962, Loss: 0.009991249676204461, Final Batch Loss: 0.0001646210002945736\n",
      "Epoch 4963, Loss: 0.008363627406652085, Final Batch Loss: 0.004765774589031935\n",
      "Epoch 4964, Loss: 0.03354910972484504, Final Batch Loss: 0.00042469220352359116\n",
      "Epoch 4965, Loss: 0.003319592447951436, Final Batch Loss: 0.00013770535588264465\n",
      "Epoch 4966, Loss: 0.013251384800241794, Final Batch Loss: 6.0963775467826054e-05\n",
      "Epoch 4967, Loss: 0.01097093493444845, Final Batch Loss: 0.0013898503966629505\n",
      "Epoch 4968, Loss: 0.02246271002513822, Final Batch Loss: 0.004115813411772251\n",
      "Epoch 4969, Loss: 0.0033765218395274132, Final Batch Loss: 0.0001630803890293464\n",
      "Epoch 4970, Loss: 0.04018079196976032, Final Batch Loss: 0.0001818798336898908\n",
      "Epoch 4971, Loss: 0.017151409352663904, Final Batch Loss: 0.0010115484474226832\n",
      "Epoch 4972, Loss: 0.024565152962168213, Final Batch Loss: 0.01730385050177574\n",
      "Epoch 4973, Loss: 0.005652977270074189, Final Batch Loss: 0.0002859218802768737\n",
      "Epoch 4974, Loss: 0.04638165791402571, Final Batch Loss: 0.03047083504498005\n",
      "Epoch 4975, Loss: 0.035318233887664974, Final Batch Loss: 0.012351824901998043\n",
      "Epoch 4976, Loss: 0.018956007494125515, Final Batch Loss: 0.0003477261634543538\n",
      "Epoch 4977, Loss: 0.03165105963125825, Final Batch Loss: 0.00985807180404663\n",
      "Epoch 4978, Loss: 0.0157257371902233, Final Batch Loss: 0.0008051783661358058\n",
      "Epoch 4979, Loss: 0.02564676797192078, Final Batch Loss: 0.0002290044940309599\n",
      "Epoch 4980, Loss: 0.024213585231336765, Final Batch Loss: 0.004893003962934017\n",
      "Epoch 4981, Loss: 0.006207613678270718, Final Batch Loss: 0.00023777465685270727\n",
      "Epoch 4982, Loss: 0.01484636379609583, Final Batch Loss: 0.0004973356262780726\n",
      "Epoch 4983, Loss: 0.006029257405316457, Final Batch Loss: 0.0022318288683891296\n",
      "Epoch 4984, Loss: 0.012722337945888285, Final Batch Loss: 6.496426794910803e-05\n",
      "Epoch 4985, Loss: 0.016064004783402197, Final Batch Loss: 6.1000013374723494e-05\n",
      "Epoch 4986, Loss: 0.006742613171809353, Final Batch Loss: 0.0002582693414296955\n",
      "Epoch 4987, Loss: 0.006209364277310669, Final Batch Loss: 0.0010638422099873424\n",
      "Epoch 4988, Loss: 0.013127453050401527, Final Batch Loss: 0.00019246985903009772\n",
      "Epoch 4989, Loss: 0.006728163760271855, Final Batch Loss: 0.002616357523947954\n",
      "Epoch 4990, Loss: 0.004095318436156958, Final Batch Loss: 0.0013339847791939974\n",
      "Epoch 4991, Loss: 0.019513183506205678, Final Batch Loss: 0.011782689020037651\n",
      "Epoch 4992, Loss: 0.001814677885704441, Final Batch Loss: 7.872027345001698e-05\n",
      "Epoch 4993, Loss: 0.007718781613220926, Final Batch Loss: 4.358427395345643e-05\n",
      "Epoch 4994, Loss: 0.006303151429165155, Final Batch Loss: 6.92266330588609e-05\n",
      "Epoch 4995, Loss: 0.015051347216285649, Final Batch Loss: 0.00582889374345541\n",
      "Epoch 4996, Loss: 0.002189614489907399, Final Batch Loss: 8.676637662574649e-05\n",
      "Epoch 4997, Loss: 0.007223360713396687, Final Batch Loss: 3.272914182161912e-05\n",
      "Epoch 4998, Loss: 0.0023169716587290168, Final Batch Loss: 0.0002810231817420572\n",
      "Epoch 4999, Loss: 0.0030535070873156656, Final Batch Loss: 0.0019544283859431744\n",
      "Epoch 5000, Loss: 0.007780500469380058, Final Batch Loss: 0.0003095268039032817\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[75  1  0]\n",
      " [ 3 67  0]\n",
      " [ 0  0 69]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.962     0.987     0.974        76\n",
      "           1      0.985     0.957     0.971        70\n",
      "           2      1.000     1.000     1.000        69\n",
      "\n",
      "    accuracy                          0.981       215\n",
      "   macro avg      0.982     0.981     0.982       215\n",
      "weighted avg      0.982     0.981     0.981       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'../../../saved_models/UCI 3 Label 9 Subject Classifier Ablation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
