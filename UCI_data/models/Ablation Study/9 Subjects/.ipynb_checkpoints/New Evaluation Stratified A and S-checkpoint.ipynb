{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '141 tBodyGyro-iqr()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '434 fBodyGyro-max()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 35),\n",
    "            classifier_block(35, 30),\n",
    "            classifier_block(30, 25),\n",
    "            classifier_block(25, 25),\n",
    "            nn.Linear(25, 27)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "    \n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_10 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_11 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_12 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_13 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_14 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_15 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_16 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_17 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_18 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_19 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_20 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_21 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_22 = GAN_data[(GAN_data['Subject'] == 17) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_23 = GAN_data[(GAN_data['Subject'] == 17) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_24 = GAN_data[(GAN_data['Subject'] == 17) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_25 = GAN_data[(GAN_data['Subject'] == 19) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_26 = GAN_data[(GAN_data['Subject'] == 19) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_27 = GAN_data[(GAN_data['Subject'] == 19) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9, X_10, X_11, X_12, X_13, X_14, X_15, X_16, X_17, X_18, X_19, X_20, X_21, X_22, X_23, X_24, X_25, X_26, X_27))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9) + [9] * len(X_10) + [10] * len(X_11) + [11] * len(X_12) + [12] * len(X_13) + [13] * len(X_14) + [14] * len(X_15) + [15] * len(X_16) + [16] * len(X_17) + [17] * len(X_18) + [18] * len(X_19) + [19] * len(X_20) + [20] * len(X_21) + [21] * len(X_22) + [22] * len(X_23) + [23] * len(X_24) + [24] * len(X_25) + [25] * len(X_26) + [26] * len(X_27)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5, 7, 8, 11, 14, 17, 19]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 15.957932710647583, Final Batch Loss: 3.2285239696502686\n",
      "Epoch 2, Loss: 15.904506921768188, Final Batch Loss: 3.1900508403778076\n",
      "Epoch 3, Loss: 15.916915893554688, Final Batch Loss: 3.2135887145996094\n",
      "Epoch 4, Loss: 15.841309547424316, Final Batch Loss: 3.1550381183624268\n",
      "Epoch 5, Loss: 15.862035036087036, Final Batch Loss: 3.189518690109253\n",
      "Epoch 6, Loss: 15.84599757194519, Final Batch Loss: 3.2025513648986816\n",
      "Epoch 7, Loss: 15.743285179138184, Final Batch Loss: 3.121537685394287\n",
      "Epoch 8, Loss: 15.695988416671753, Final Batch Loss: 3.114802122116089\n",
      "Epoch 9, Loss: 15.633151531219482, Final Batch Loss: 3.115732431411743\n",
      "Epoch 10, Loss: 15.574915885925293, Final Batch Loss: 3.144605875015259\n",
      "Epoch 11, Loss: 15.367133140563965, Final Batch Loss: 3.0854246616363525\n",
      "Epoch 12, Loss: 15.162415981292725, Final Batch Loss: 3.0667977333068848\n",
      "Epoch 13, Loss: 14.81406021118164, Final Batch Loss: 2.974008798599243\n",
      "Epoch 14, Loss: 14.369290351867676, Final Batch Loss: 2.8641960620880127\n",
      "Epoch 15, Loss: 14.16333818435669, Final Batch Loss: 2.951519727706909\n",
      "Epoch 16, Loss: 13.687440872192383, Final Batch Loss: 2.728977680206299\n",
      "Epoch 17, Loss: 13.170493841171265, Final Batch Loss: 2.4231631755828857\n",
      "Epoch 18, Loss: 12.941234588623047, Final Batch Loss: 2.3860507011413574\n",
      "Epoch 19, Loss: 13.04645323753357, Final Batch Loss: 2.637341260910034\n",
      "Epoch 20, Loss: 12.674789190292358, Final Batch Loss: 2.3323628902435303\n",
      "Epoch 21, Loss: 12.730803966522217, Final Batch Loss: 2.5509018898010254\n",
      "Epoch 22, Loss: 12.63399863243103, Final Batch Loss: 2.512735605239868\n",
      "Epoch 23, Loss: 12.607908248901367, Final Batch Loss: 2.6269145011901855\n",
      "Epoch 24, Loss: 12.20857310295105, Final Batch Loss: 2.4130001068115234\n",
      "Epoch 25, Loss: 11.95654010772705, Final Batch Loss: 2.2638771533966064\n",
      "Epoch 26, Loss: 12.016597747802734, Final Batch Loss: 2.4492485523223877\n",
      "Epoch 27, Loss: 11.586138725280762, Final Batch Loss: 2.1375720500946045\n",
      "Epoch 28, Loss: 11.434966325759888, Final Batch Loss: 2.072585105895996\n",
      "Epoch 29, Loss: 11.42181921005249, Final Batch Loss: 2.140359401702881\n",
      "Epoch 30, Loss: 11.533557891845703, Final Batch Loss: 2.3768436908721924\n",
      "Epoch 31, Loss: 11.2162926197052, Final Batch Loss: 2.1673219203948975\n",
      "Epoch 32, Loss: 10.966674327850342, Final Batch Loss: 2.072208881378174\n",
      "Epoch 33, Loss: 11.291887760162354, Final Batch Loss: 2.4182231426239014\n",
      "Epoch 34, Loss: 10.717018842697144, Final Batch Loss: 1.8917696475982666\n",
      "Epoch 35, Loss: 10.856646537780762, Final Batch Loss: 2.2137601375579834\n",
      "Epoch 36, Loss: 10.720172882080078, Final Batch Loss: 2.0083718299865723\n",
      "Epoch 37, Loss: 10.555383682250977, Final Batch Loss: 2.007883310317993\n",
      "Epoch 38, Loss: 10.447343587875366, Final Batch Loss: 1.9756519794464111\n",
      "Epoch 39, Loss: 10.472445726394653, Final Batch Loss: 2.1489064693450928\n",
      "Epoch 40, Loss: 10.19729197025299, Final Batch Loss: 1.934759259223938\n",
      "Epoch 41, Loss: 10.257432579994202, Final Batch Loss: 1.9695438146591187\n",
      "Epoch 42, Loss: 10.027297139167786, Final Batch Loss: 1.8190819025039673\n",
      "Epoch 43, Loss: 10.177453994750977, Final Batch Loss: 1.996936559677124\n",
      "Epoch 44, Loss: 10.187474370002747, Final Batch Loss: 2.1260106563568115\n",
      "Epoch 45, Loss: 10.1382896900177, Final Batch Loss: 2.1067614555358887\n",
      "Epoch 46, Loss: 10.226479649543762, Final Batch Loss: 2.3010919094085693\n",
      "Epoch 47, Loss: 9.926681876182556, Final Batch Loss: 1.8781603574752808\n",
      "Epoch 48, Loss: 9.889956593513489, Final Batch Loss: 1.9103881120681763\n",
      "Epoch 49, Loss: 10.041413426399231, Final Batch Loss: 2.0745351314544678\n",
      "Epoch 50, Loss: 10.21045434474945, Final Batch Loss: 2.4645164012908936\n",
      "Epoch 51, Loss: 9.596406817436218, Final Batch Loss: 1.7765322923660278\n",
      "Epoch 52, Loss: 9.43826425075531, Final Batch Loss: 1.6610723733901978\n",
      "Epoch 53, Loss: 9.659537196159363, Final Batch Loss: 2.1150777339935303\n",
      "Epoch 54, Loss: 9.870744705200195, Final Batch Loss: 2.14970326423645\n",
      "Epoch 55, Loss: 9.796209931373596, Final Batch Loss: 2.146616220474243\n",
      "Epoch 56, Loss: 9.34641146659851, Final Batch Loss: 1.8288065195083618\n",
      "Epoch 57, Loss: 9.289440870285034, Final Batch Loss: 1.8649208545684814\n",
      "Epoch 58, Loss: 9.475490808486938, Final Batch Loss: 1.9474103450775146\n",
      "Epoch 59, Loss: 9.436787009239197, Final Batch Loss: 2.0113260746002197\n",
      "Epoch 60, Loss: 8.913226962089539, Final Batch Loss: 1.5503294467926025\n",
      "Epoch 61, Loss: 9.516769886016846, Final Batch Loss: 2.3005805015563965\n",
      "Epoch 62, Loss: 9.216979503631592, Final Batch Loss: 1.919324517250061\n",
      "Epoch 63, Loss: 8.95076847076416, Final Batch Loss: 1.629574179649353\n",
      "Epoch 64, Loss: 8.95351755619049, Final Batch Loss: 1.7888392210006714\n",
      "Epoch 65, Loss: 8.935252070426941, Final Batch Loss: 1.795625925064087\n",
      "Epoch 66, Loss: 9.020418643951416, Final Batch Loss: 1.8056849241256714\n",
      "Epoch 67, Loss: 8.826680183410645, Final Batch Loss: 1.6437517404556274\n",
      "Epoch 68, Loss: 8.874810218811035, Final Batch Loss: 1.7776397466659546\n",
      "Epoch 69, Loss: 8.924518823623657, Final Batch Loss: 1.840320348739624\n",
      "Epoch 70, Loss: 9.13668978214264, Final Batch Loss: 2.1497933864593506\n",
      "Epoch 71, Loss: 8.693261742591858, Final Batch Loss: 1.7374027967453003\n",
      "Epoch 72, Loss: 8.588313341140747, Final Batch Loss: 1.685551643371582\n",
      "Epoch 73, Loss: 8.954520583152771, Final Batch Loss: 2.0487613677978516\n",
      "Epoch 74, Loss: 8.561092734336853, Final Batch Loss: 1.6731557846069336\n",
      "Epoch 75, Loss: 8.68755853176117, Final Batch Loss: 1.7920116186141968\n",
      "Epoch 76, Loss: 8.642920732498169, Final Batch Loss: 1.8634673357009888\n",
      "Epoch 77, Loss: 8.774644494056702, Final Batch Loss: 1.9073702096939087\n",
      "Epoch 78, Loss: 8.414871215820312, Final Batch Loss: 1.5748523473739624\n",
      "Epoch 79, Loss: 8.236753463745117, Final Batch Loss: 1.542876124382019\n",
      "Epoch 80, Loss: 8.197474479675293, Final Batch Loss: 1.5050030946731567\n",
      "Epoch 81, Loss: 7.98722505569458, Final Batch Loss: 1.364155650138855\n",
      "Epoch 82, Loss: 8.361255764961243, Final Batch Loss: 1.5853928327560425\n",
      "Epoch 83, Loss: 7.629408121109009, Final Batch Loss: 1.018216848373413\n",
      "Epoch 84, Loss: 8.056212663650513, Final Batch Loss: 1.4301389455795288\n",
      "Epoch 85, Loss: 7.982632875442505, Final Batch Loss: 1.4379730224609375\n",
      "Epoch 86, Loss: 8.230626344680786, Final Batch Loss: 1.696239709854126\n",
      "Epoch 87, Loss: 7.835055470466614, Final Batch Loss: 1.4990371465682983\n",
      "Epoch 88, Loss: 8.106875538825989, Final Batch Loss: 1.7030855417251587\n",
      "Epoch 89, Loss: 8.039437055587769, Final Batch Loss: 1.6912720203399658\n",
      "Epoch 90, Loss: 8.140403151512146, Final Batch Loss: 1.8509235382080078\n",
      "Epoch 91, Loss: 8.051156520843506, Final Batch Loss: 1.7240580320358276\n",
      "Epoch 92, Loss: 7.8194580078125, Final Batch Loss: 1.4522838592529297\n",
      "Epoch 93, Loss: 8.084652781486511, Final Batch Loss: 1.79249107837677\n",
      "Epoch 94, Loss: 7.854494214057922, Final Batch Loss: 1.6648619174957275\n",
      "Epoch 95, Loss: 8.023114800453186, Final Batch Loss: 1.850838541984558\n",
      "Epoch 96, Loss: 8.121189594268799, Final Batch Loss: 1.7710239887237549\n",
      "Epoch 97, Loss: 8.028586864471436, Final Batch Loss: 1.8321563005447388\n",
      "Epoch 98, Loss: 7.7576011419296265, Final Batch Loss: 1.5043680667877197\n",
      "Epoch 99, Loss: 7.78699517250061, Final Batch Loss: 1.630743384361267\n",
      "Epoch 100, Loss: 7.773656487464905, Final Batch Loss: 1.74418044090271\n",
      "Epoch 101, Loss: 7.710705757141113, Final Batch Loss: 1.6232359409332275\n",
      "Epoch 102, Loss: 7.386475443840027, Final Batch Loss: 1.3337770700454712\n",
      "Epoch 103, Loss: 7.648916840553284, Final Batch Loss: 1.5033015012741089\n",
      "Epoch 104, Loss: 7.685745000839233, Final Batch Loss: 1.5330396890640259\n",
      "Epoch 105, Loss: 6.996927618980408, Final Batch Loss: 1.0560134649276733\n",
      "Epoch 106, Loss: 7.329819083213806, Final Batch Loss: 1.3314508199691772\n",
      "Epoch 107, Loss: 7.482103705406189, Final Batch Loss: 1.598544955253601\n",
      "Epoch 108, Loss: 7.401780366897583, Final Batch Loss: 1.3631572723388672\n",
      "Epoch 109, Loss: 7.311253666877747, Final Batch Loss: 1.3412760496139526\n",
      "Epoch 110, Loss: 7.166976690292358, Final Batch Loss: 1.2702950239181519\n",
      "Epoch 111, Loss: 7.174952507019043, Final Batch Loss: 1.2176028490066528\n",
      "Epoch 112, Loss: 7.239028573036194, Final Batch Loss: 1.3584216833114624\n",
      "Epoch 113, Loss: 7.467930912971497, Final Batch Loss: 1.4658032655715942\n",
      "Epoch 114, Loss: 7.1385791301727295, Final Batch Loss: 1.3214730024337769\n",
      "Epoch 115, Loss: 7.594377756118774, Final Batch Loss: 1.8731672763824463\n",
      "Epoch 116, Loss: 7.536272048950195, Final Batch Loss: 1.7009094953536987\n",
      "Epoch 117, Loss: 7.112073659896851, Final Batch Loss: 1.3001937866210938\n",
      "Epoch 118, Loss: 7.379090905189514, Final Batch Loss: 1.6405147314071655\n",
      "Epoch 119, Loss: 7.010136961936951, Final Batch Loss: 1.2735668420791626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120, Loss: 7.314055681228638, Final Batch Loss: 1.623931884765625\n",
      "Epoch 121, Loss: 6.801501154899597, Final Batch Loss: 1.2371593713760376\n",
      "Epoch 122, Loss: 7.298414826393127, Final Batch Loss: 1.3711096048355103\n",
      "Epoch 123, Loss: 6.906215310096741, Final Batch Loss: 1.2179244756698608\n",
      "Epoch 124, Loss: 6.873498320579529, Final Batch Loss: 1.3042128086090088\n",
      "Epoch 125, Loss: 7.4830217361450195, Final Batch Loss: 1.8438835144042969\n",
      "Epoch 126, Loss: 7.020467400550842, Final Batch Loss: 1.4762098789215088\n",
      "Epoch 127, Loss: 7.642934679985046, Final Batch Loss: 1.9847753047943115\n",
      "Epoch 128, Loss: 7.570393800735474, Final Batch Loss: 1.7546831369400024\n",
      "Epoch 129, Loss: 6.799211382865906, Final Batch Loss: 1.1934734582901\n",
      "Epoch 130, Loss: 6.960472106933594, Final Batch Loss: 1.3991308212280273\n",
      "Epoch 131, Loss: 7.393840670585632, Final Batch Loss: 1.7617267370224\n",
      "Epoch 132, Loss: 6.992254376411438, Final Batch Loss: 1.4446848630905151\n",
      "Epoch 133, Loss: 7.0494585037231445, Final Batch Loss: 1.6172981262207031\n",
      "Epoch 134, Loss: 6.705833554267883, Final Batch Loss: 1.1466017961502075\n",
      "Epoch 135, Loss: 6.783455729484558, Final Batch Loss: 1.3042793273925781\n",
      "Epoch 136, Loss: 6.747898697853088, Final Batch Loss: 1.368155598640442\n",
      "Epoch 137, Loss: 6.5844316482543945, Final Batch Loss: 1.1891385316848755\n",
      "Epoch 138, Loss: 7.013477802276611, Final Batch Loss: 1.5840339660644531\n",
      "Epoch 139, Loss: 6.656605124473572, Final Batch Loss: 1.2558015584945679\n",
      "Epoch 140, Loss: 6.551687955856323, Final Batch Loss: 1.189205288887024\n",
      "Epoch 141, Loss: 6.986730575561523, Final Batch Loss: 1.5200296640396118\n",
      "Epoch 142, Loss: 6.857475399971008, Final Batch Loss: 1.4314602613449097\n",
      "Epoch 143, Loss: 6.5905948877334595, Final Batch Loss: 1.3103817701339722\n",
      "Epoch 144, Loss: 6.570424437522888, Final Batch Loss: 1.2218043804168701\n",
      "Epoch 145, Loss: 6.423662781715393, Final Batch Loss: 1.1537024974822998\n",
      "Epoch 146, Loss: 6.58829402923584, Final Batch Loss: 1.3035049438476562\n",
      "Epoch 147, Loss: 6.730793595314026, Final Batch Loss: 1.4020179510116577\n",
      "Epoch 148, Loss: 6.7954806089401245, Final Batch Loss: 1.4888551235198975\n",
      "Epoch 149, Loss: 6.706997036933899, Final Batch Loss: 1.478643536567688\n",
      "Epoch 150, Loss: 6.705278635025024, Final Batch Loss: 1.3397853374481201\n",
      "Epoch 151, Loss: 6.669521450996399, Final Batch Loss: 1.2774804830551147\n",
      "Epoch 152, Loss: 6.642034411430359, Final Batch Loss: 1.3894840478897095\n",
      "Epoch 153, Loss: 6.232454597949982, Final Batch Loss: 0.958164632320404\n",
      "Epoch 154, Loss: 6.7328468561172485, Final Batch Loss: 1.5113213062286377\n",
      "Epoch 155, Loss: 6.8008259534835815, Final Batch Loss: 1.692530870437622\n",
      "Epoch 156, Loss: 6.675390720367432, Final Batch Loss: 1.5387217998504639\n",
      "Epoch 157, Loss: 6.484604239463806, Final Batch Loss: 1.2252920866012573\n",
      "Epoch 158, Loss: 6.680356025695801, Final Batch Loss: 1.4739588499069214\n",
      "Epoch 159, Loss: 6.359046578407288, Final Batch Loss: 1.1689622402191162\n",
      "Epoch 160, Loss: 6.618154764175415, Final Batch Loss: 1.4121484756469727\n",
      "Epoch 161, Loss: 6.307983994483948, Final Batch Loss: 1.184049367904663\n",
      "Epoch 162, Loss: 6.3704811334609985, Final Batch Loss: 1.3509093523025513\n",
      "Epoch 163, Loss: 6.5232627391815186, Final Batch Loss: 1.476287603378296\n",
      "Epoch 164, Loss: 6.080788969993591, Final Batch Loss: 1.0272496938705444\n",
      "Epoch 165, Loss: 6.667604327201843, Final Batch Loss: 1.4585272073745728\n",
      "Epoch 166, Loss: 6.09444385766983, Final Batch Loss: 0.9187769293785095\n",
      "Epoch 167, Loss: 6.520302772521973, Final Batch Loss: 1.3868999481201172\n",
      "Epoch 168, Loss: 6.5736706256866455, Final Batch Loss: 1.538726568222046\n",
      "Epoch 169, Loss: 6.48835027217865, Final Batch Loss: 1.4018248319625854\n",
      "Epoch 170, Loss: 6.116501212120056, Final Batch Loss: 1.069585919380188\n",
      "Epoch 171, Loss: 6.520634651184082, Final Batch Loss: 1.565804123878479\n",
      "Epoch 172, Loss: 6.348230957984924, Final Batch Loss: 1.3408129215240479\n",
      "Epoch 173, Loss: 6.285938382148743, Final Batch Loss: 1.2169984579086304\n",
      "Epoch 174, Loss: 6.255728840827942, Final Batch Loss: 1.1919667720794678\n",
      "Epoch 175, Loss: 6.031228303909302, Final Batch Loss: 1.0144073963165283\n",
      "Epoch 176, Loss: 6.263331174850464, Final Batch Loss: 1.2047086954116821\n",
      "Epoch 177, Loss: 6.204785943031311, Final Batch Loss: 1.2215591669082642\n",
      "Epoch 178, Loss: 6.309582591056824, Final Batch Loss: 1.3472403287887573\n",
      "Epoch 179, Loss: 6.2790021896362305, Final Batch Loss: 1.2978135347366333\n",
      "Epoch 180, Loss: 6.093469262123108, Final Batch Loss: 1.1516649723052979\n",
      "Epoch 181, Loss: 6.000573039054871, Final Batch Loss: 1.0613800287246704\n",
      "Epoch 182, Loss: 6.226477384567261, Final Batch Loss: 1.2238882780075073\n",
      "Epoch 183, Loss: 5.942475914955139, Final Batch Loss: 1.0108020305633545\n",
      "Epoch 184, Loss: 6.256247639656067, Final Batch Loss: 1.3961271047592163\n",
      "Epoch 185, Loss: 6.454682469367981, Final Batch Loss: 1.5137251615524292\n",
      "Epoch 186, Loss: 5.8895180225372314, Final Batch Loss: 1.0132248401641846\n",
      "Epoch 187, Loss: 6.187838196754456, Final Batch Loss: 1.2466942071914673\n",
      "Epoch 188, Loss: 6.084715962409973, Final Batch Loss: 1.2938576936721802\n",
      "Epoch 189, Loss: 6.190482497215271, Final Batch Loss: 1.3581033945083618\n",
      "Epoch 190, Loss: 5.572855055332184, Final Batch Loss: 0.6587820649147034\n",
      "Epoch 191, Loss: 6.199950695037842, Final Batch Loss: 1.4528251886367798\n",
      "Epoch 192, Loss: 6.564863920211792, Final Batch Loss: 1.7260740995407104\n",
      "Epoch 193, Loss: 6.515086054801941, Final Batch Loss: 1.73368239402771\n",
      "Epoch 194, Loss: 5.92487370967865, Final Batch Loss: 1.0840544700622559\n",
      "Epoch 195, Loss: 6.0191779136657715, Final Batch Loss: 1.1953340768814087\n",
      "Epoch 196, Loss: 6.268136978149414, Final Batch Loss: 1.5717709064483643\n",
      "Epoch 197, Loss: 6.197811126708984, Final Batch Loss: 1.4371273517608643\n",
      "Epoch 198, Loss: 6.0570080280303955, Final Batch Loss: 1.3238449096679688\n",
      "Epoch 199, Loss: 5.780883550643921, Final Batch Loss: 1.0716168880462646\n",
      "Epoch 200, Loss: 6.059358239173889, Final Batch Loss: 1.5089553594589233\n",
      "Epoch 201, Loss: 5.499470233917236, Final Batch Loss: 0.7930344343185425\n",
      "Epoch 202, Loss: 5.794635057449341, Final Batch Loss: 1.028988003730774\n",
      "Epoch 203, Loss: 6.009949207305908, Final Batch Loss: 1.2642244100570679\n",
      "Epoch 204, Loss: 6.284485578536987, Final Batch Loss: 1.6666327714920044\n",
      "Epoch 205, Loss: 6.069978833198547, Final Batch Loss: 1.3813552856445312\n",
      "Epoch 206, Loss: 6.052898287773132, Final Batch Loss: 1.228826642036438\n",
      "Epoch 207, Loss: 6.130860209465027, Final Batch Loss: 1.542676568031311\n",
      "Epoch 208, Loss: 5.399925172328949, Final Batch Loss: 0.8054825663566589\n",
      "Epoch 209, Loss: 5.985550045967102, Final Batch Loss: 1.0307157039642334\n",
      "Epoch 210, Loss: 5.63768458366394, Final Batch Loss: 1.0814608335494995\n",
      "Epoch 211, Loss: 6.027243137359619, Final Batch Loss: 1.2754042148590088\n",
      "Epoch 212, Loss: 5.981738924980164, Final Batch Loss: 1.4225599765777588\n",
      "Epoch 213, Loss: 6.157280087471008, Final Batch Loss: 1.4431250095367432\n",
      "Epoch 214, Loss: 6.110107064247131, Final Batch Loss: 1.4258962869644165\n",
      "Epoch 215, Loss: 5.584508419036865, Final Batch Loss: 1.0049455165863037\n",
      "Epoch 216, Loss: 5.86309027671814, Final Batch Loss: 1.2647173404693604\n",
      "Epoch 217, Loss: 5.817748188972473, Final Batch Loss: 1.1646708250045776\n",
      "Epoch 218, Loss: 5.789772987365723, Final Batch Loss: 1.063259243965149\n",
      "Epoch 219, Loss: 5.821174740791321, Final Batch Loss: 1.1627098321914673\n",
      "Epoch 220, Loss: 5.8173874616622925, Final Batch Loss: 1.1776388883590698\n",
      "Epoch 221, Loss: 5.411304533481598, Final Batch Loss: 0.8164457678794861\n",
      "Epoch 222, Loss: 5.871496677398682, Final Batch Loss: 1.2362449169158936\n",
      "Epoch 223, Loss: 5.596404433250427, Final Batch Loss: 1.0038217306137085\n",
      "Epoch 224, Loss: 5.746945738792419, Final Batch Loss: 1.2965611219406128\n",
      "Epoch 225, Loss: 6.054680347442627, Final Batch Loss: 1.3713880777359009\n",
      "Epoch 226, Loss: 5.512090682983398, Final Batch Loss: 1.0656836032867432\n",
      "Epoch 227, Loss: 6.033168315887451, Final Batch Loss: 1.5059702396392822\n",
      "Epoch 228, Loss: 5.721193313598633, Final Batch Loss: 1.2131623029708862\n",
      "Epoch 229, Loss: 5.738889932632446, Final Batch Loss: 1.1613504886627197\n",
      "Epoch 230, Loss: 6.136901617050171, Final Batch Loss: 1.6353366374969482\n",
      "Epoch 231, Loss: 6.136202931404114, Final Batch Loss: 1.5889474153518677\n",
      "Epoch 232, Loss: 5.422311365604401, Final Batch Loss: 0.9914209246635437\n",
      "Epoch 233, Loss: 5.6141340136528015, Final Batch Loss: 0.8177518248558044\n",
      "Epoch 234, Loss: 5.270132541656494, Final Batch Loss: 0.7912216186523438\n",
      "Epoch 235, Loss: 5.4078089594841, Final Batch Loss: 0.9487804770469666\n",
      "Epoch 236, Loss: 5.707429647445679, Final Batch Loss: 1.3779385089874268\n",
      "Epoch 237, Loss: 5.694684743881226, Final Batch Loss: 1.2677128314971924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238, Loss: 5.859850764274597, Final Batch Loss: 1.4621020555496216\n",
      "Epoch 239, Loss: 5.581784129142761, Final Batch Loss: 1.1884925365447998\n",
      "Epoch 240, Loss: 5.376122713088989, Final Batch Loss: 0.9211024045944214\n",
      "Epoch 241, Loss: 5.574279546737671, Final Batch Loss: 1.1727831363677979\n",
      "Epoch 242, Loss: 5.537105679512024, Final Batch Loss: 1.0370510816574097\n",
      "Epoch 243, Loss: 5.615569591522217, Final Batch Loss: 1.1947081089019775\n",
      "Epoch 244, Loss: 5.347487688064575, Final Batch Loss: 0.8697748184204102\n",
      "Epoch 245, Loss: 5.092279434204102, Final Batch Loss: 0.722077488899231\n",
      "Epoch 246, Loss: 5.136792719364166, Final Batch Loss: 0.7645347714424133\n",
      "Epoch 247, Loss: 5.203782558441162, Final Batch Loss: 0.8866415023803711\n",
      "Epoch 248, Loss: 5.709216833114624, Final Batch Loss: 1.265973448753357\n",
      "Epoch 249, Loss: 5.321224749088287, Final Batch Loss: 1.0317726135253906\n",
      "Epoch 250, Loss: 5.447241187095642, Final Batch Loss: 1.0079429149627686\n",
      "Epoch 251, Loss: 5.0049333572387695, Final Batch Loss: 0.6373600959777832\n",
      "Epoch 252, Loss: 5.1592695116996765, Final Batch Loss: 0.8769492506980896\n",
      "Epoch 253, Loss: 5.022994101047516, Final Batch Loss: 0.567936360836029\n",
      "Epoch 254, Loss: 5.206517279148102, Final Batch Loss: 0.8645704388618469\n",
      "Epoch 255, Loss: 5.190992832183838, Final Batch Loss: 0.7557276487350464\n",
      "Epoch 256, Loss: 5.108645021915436, Final Batch Loss: 0.7579894661903381\n",
      "Epoch 257, Loss: 5.715936899185181, Final Batch Loss: 1.264316201210022\n",
      "Epoch 258, Loss: 5.4059895277023315, Final Batch Loss: 1.0976990461349487\n",
      "Epoch 259, Loss: 5.0677876472473145, Final Batch Loss: 0.772013247013092\n",
      "Epoch 260, Loss: 5.213013589382172, Final Batch Loss: 0.9452566504478455\n",
      "Epoch 261, Loss: 5.571713328361511, Final Batch Loss: 1.2521549463272095\n",
      "Epoch 262, Loss: 5.58970034122467, Final Batch Loss: 1.233311653137207\n",
      "Epoch 263, Loss: 5.617907524108887, Final Batch Loss: 1.2750493288040161\n",
      "Epoch 264, Loss: 5.233941912651062, Final Batch Loss: 0.9957007169723511\n",
      "Epoch 265, Loss: 5.605795383453369, Final Batch Loss: 1.42014479637146\n",
      "Epoch 266, Loss: 6.168092310428619, Final Batch Loss: 1.8863441944122314\n",
      "Epoch 267, Loss: 5.666676044464111, Final Batch Loss: 1.2962034940719604\n",
      "Epoch 268, Loss: 5.532589316368103, Final Batch Loss: 1.2525831460952759\n",
      "Epoch 269, Loss: 5.05178827047348, Final Batch Loss: 0.8045569062232971\n",
      "Epoch 270, Loss: 5.060208380222321, Final Batch Loss: 0.6647668480873108\n",
      "Epoch 271, Loss: 5.277737259864807, Final Batch Loss: 1.0644210577011108\n",
      "Epoch 272, Loss: 5.63572633266449, Final Batch Loss: 1.371638298034668\n",
      "Epoch 273, Loss: 5.647216081619263, Final Batch Loss: 1.1667875051498413\n",
      "Epoch 274, Loss: 5.181119740009308, Final Batch Loss: 0.9910382032394409\n",
      "Epoch 275, Loss: 5.101231038570404, Final Batch Loss: 0.8050742745399475\n",
      "Epoch 276, Loss: 5.208413541316986, Final Batch Loss: 0.9150108695030212\n",
      "Epoch 277, Loss: 5.187310576438904, Final Batch Loss: 0.8298503160476685\n",
      "Epoch 278, Loss: 5.308435082435608, Final Batch Loss: 1.0054142475128174\n",
      "Epoch 279, Loss: 5.435064017772675, Final Batch Loss: 1.3194082975387573\n",
      "Epoch 280, Loss: 5.661603808403015, Final Batch Loss: 1.3769152164459229\n",
      "Epoch 281, Loss: 5.328579843044281, Final Batch Loss: 1.1419885158538818\n",
      "Epoch 282, Loss: 5.0189709067344666, Final Batch Loss: 0.7142212986946106\n",
      "Epoch 283, Loss: 4.8958364725112915, Final Batch Loss: 0.5841355323791504\n",
      "Epoch 284, Loss: 5.364118814468384, Final Batch Loss: 1.3138542175292969\n",
      "Epoch 285, Loss: 5.130847454071045, Final Batch Loss: 0.8785547614097595\n",
      "Epoch 286, Loss: 5.041539430618286, Final Batch Loss: 0.7452050447463989\n",
      "Epoch 287, Loss: 5.18886935710907, Final Batch Loss: 1.0135971307754517\n",
      "Epoch 288, Loss: 5.387802720069885, Final Batch Loss: 1.1490534543991089\n",
      "Epoch 289, Loss: 5.156160771846771, Final Batch Loss: 0.9673956632614136\n",
      "Epoch 290, Loss: 5.19589227437973, Final Batch Loss: 0.9671043753623962\n",
      "Epoch 291, Loss: 5.455318212509155, Final Batch Loss: 1.422580599784851\n",
      "Epoch 292, Loss: 5.927046895027161, Final Batch Loss: 1.7467114925384521\n",
      "Epoch 293, Loss: 4.846150457859039, Final Batch Loss: 0.7052188515663147\n",
      "Epoch 294, Loss: 5.005644917488098, Final Batch Loss: 0.8917074203491211\n",
      "Epoch 295, Loss: 4.976343214511871, Final Batch Loss: 0.9196726679801941\n",
      "Epoch 296, Loss: 5.445850074291229, Final Batch Loss: 1.3828049898147583\n",
      "Epoch 297, Loss: 4.813511312007904, Final Batch Loss: 0.8454236388206482\n",
      "Epoch 298, Loss: 4.852857351303101, Final Batch Loss: 0.7356362342834473\n",
      "Epoch 299, Loss: 4.679062843322754, Final Batch Loss: 0.6178159117698669\n",
      "Epoch 300, Loss: 4.989673674106598, Final Batch Loss: 0.9480240941047668\n",
      "Epoch 301, Loss: 4.721603751182556, Final Batch Loss: 0.7641416192054749\n",
      "Epoch 302, Loss: 5.261651456356049, Final Batch Loss: 1.1532480716705322\n",
      "Epoch 303, Loss: 4.96568375825882, Final Batch Loss: 0.9953952431678772\n",
      "Epoch 304, Loss: 5.085932374000549, Final Batch Loss: 1.039260745048523\n",
      "Epoch 305, Loss: 5.054551124572754, Final Batch Loss: 0.8391751050949097\n",
      "Epoch 306, Loss: 5.144669353961945, Final Batch Loss: 0.8758023381233215\n",
      "Epoch 307, Loss: 4.835745096206665, Final Batch Loss: 0.7362640500068665\n",
      "Epoch 308, Loss: 4.897970259189606, Final Batch Loss: 0.7900859117507935\n",
      "Epoch 309, Loss: 4.903306424617767, Final Batch Loss: 0.8961934447288513\n",
      "Epoch 310, Loss: 5.1912761926651, Final Batch Loss: 1.1928309202194214\n",
      "Epoch 311, Loss: 5.17543488740921, Final Batch Loss: 1.1475508213043213\n",
      "Epoch 312, Loss: 5.017320692539215, Final Batch Loss: 0.9204520583152771\n",
      "Epoch 313, Loss: 5.339361786842346, Final Batch Loss: 1.1410497426986694\n",
      "Epoch 314, Loss: 4.916481375694275, Final Batch Loss: 0.8821917772293091\n",
      "Epoch 315, Loss: 4.985478341579437, Final Batch Loss: 1.039646029472351\n",
      "Epoch 316, Loss: 5.2472739815711975, Final Batch Loss: 1.290419340133667\n",
      "Epoch 317, Loss: 5.35854560136795, Final Batch Loss: 1.3827905654907227\n",
      "Epoch 318, Loss: 5.395617365837097, Final Batch Loss: 1.2328994274139404\n",
      "Epoch 319, Loss: 5.013467907905579, Final Batch Loss: 0.8973830938339233\n",
      "Epoch 320, Loss: 4.853031754493713, Final Batch Loss: 0.823114812374115\n",
      "Epoch 321, Loss: 4.799459397792816, Final Batch Loss: 0.8004316091537476\n",
      "Epoch 322, Loss: 4.906946837902069, Final Batch Loss: 0.9248872995376587\n",
      "Epoch 323, Loss: 5.29433536529541, Final Batch Loss: 1.3390238285064697\n",
      "Epoch 324, Loss: 5.10454785823822, Final Batch Loss: 1.0436923503875732\n",
      "Epoch 325, Loss: 5.0694175362586975, Final Batch Loss: 1.0003405809402466\n",
      "Epoch 326, Loss: 4.834278404712677, Final Batch Loss: 0.8276003003120422\n",
      "Epoch 327, Loss: 4.888888001441956, Final Batch Loss: 0.9169823527336121\n",
      "Epoch 328, Loss: 4.894533574581146, Final Batch Loss: 0.9742047190666199\n",
      "Epoch 329, Loss: 4.620932400226593, Final Batch Loss: 0.5583999156951904\n",
      "Epoch 330, Loss: 4.804397761821747, Final Batch Loss: 0.7093144655227661\n",
      "Epoch 331, Loss: 5.3631879687309265, Final Batch Loss: 1.2389951944351196\n",
      "Epoch 332, Loss: 5.155047237873077, Final Batch Loss: 1.130494236946106\n",
      "Epoch 333, Loss: 4.825721502304077, Final Batch Loss: 0.7627684473991394\n",
      "Epoch 334, Loss: 4.803646564483643, Final Batch Loss: 0.8447696566581726\n",
      "Epoch 335, Loss: 4.776564240455627, Final Batch Loss: 0.8714423179626465\n",
      "Epoch 336, Loss: 4.6708455085754395, Final Batch Loss: 0.8366336822509766\n",
      "Epoch 337, Loss: 4.786587476730347, Final Batch Loss: 0.808705747127533\n",
      "Epoch 338, Loss: 5.187138080596924, Final Batch Loss: 1.2343018054962158\n",
      "Epoch 339, Loss: 5.085316181182861, Final Batch Loss: 1.0984152555465698\n",
      "Epoch 340, Loss: 4.636827290058136, Final Batch Loss: 0.6832000017166138\n",
      "Epoch 341, Loss: 4.6731536984443665, Final Batch Loss: 0.6538978219032288\n",
      "Epoch 342, Loss: 5.154362082481384, Final Batch Loss: 1.1954033374786377\n",
      "Epoch 343, Loss: 4.768901526927948, Final Batch Loss: 0.8787025809288025\n",
      "Epoch 344, Loss: 5.450589537620544, Final Batch Loss: 1.564005732536316\n",
      "Epoch 345, Loss: 4.676174759864807, Final Batch Loss: 0.8580325841903687\n",
      "Epoch 346, Loss: 4.763802707195282, Final Batch Loss: 0.8139477372169495\n",
      "Epoch 347, Loss: 4.5127891302108765, Final Batch Loss: 0.6003475189208984\n",
      "Epoch 348, Loss: 4.961250424385071, Final Batch Loss: 1.0388009548187256\n",
      "Epoch 349, Loss: 4.551255583763123, Final Batch Loss: 0.6568794250488281\n",
      "Epoch 350, Loss: 4.932578206062317, Final Batch Loss: 0.9460833668708801\n",
      "Epoch 351, Loss: 4.624114692211151, Final Batch Loss: 0.832768440246582\n",
      "Epoch 352, Loss: 5.125991940498352, Final Batch Loss: 1.226241946220398\n",
      "Epoch 353, Loss: 4.691442608833313, Final Batch Loss: 0.8793291449546814\n",
      "Epoch 354, Loss: 4.900443196296692, Final Batch Loss: 1.1181674003601074\n",
      "Epoch 355, Loss: 4.679083168506622, Final Batch Loss: 0.7810881733894348\n",
      "Epoch 356, Loss: 4.608541429042816, Final Batch Loss: 0.7140018343925476\n",
      "Epoch 357, Loss: 4.3832045793533325, Final Batch Loss: 0.5755410194396973\n",
      "Epoch 358, Loss: 4.7230876088142395, Final Batch Loss: 0.7899070978164673\n",
      "Epoch 359, Loss: 4.920502424240112, Final Batch Loss: 1.1540316343307495\n",
      "Epoch 360, Loss: 5.023751616477966, Final Batch Loss: 1.1714694499969482\n",
      "Epoch 361, Loss: 4.982939958572388, Final Batch Loss: 1.0402237176895142\n",
      "Epoch 362, Loss: 5.046411693096161, Final Batch Loss: 1.1379711627960205\n",
      "Epoch 363, Loss: 5.213631212711334, Final Batch Loss: 1.4141489267349243\n",
      "Epoch 364, Loss: 4.799957096576691, Final Batch Loss: 1.0707859992980957\n",
      "Epoch 365, Loss: 4.6644686460494995, Final Batch Loss: 0.8574221730232239\n",
      "Epoch 366, Loss: 5.173482894897461, Final Batch Loss: 1.379134178161621\n",
      "Epoch 367, Loss: 4.936251997947693, Final Batch Loss: 1.0275906324386597\n",
      "Epoch 368, Loss: 4.890324890613556, Final Batch Loss: 1.1375230550765991\n",
      "Epoch 369, Loss: 5.360398054122925, Final Batch Loss: 1.3354947566986084\n",
      "Epoch 370, Loss: 4.810496807098389, Final Batch Loss: 0.8746896386146545\n",
      "Epoch 371, Loss: 5.193438172340393, Final Batch Loss: 1.393814206123352\n",
      "Epoch 372, Loss: 4.5430551171302795, Final Batch Loss: 0.6943748593330383\n",
      "Epoch 373, Loss: 4.508458197116852, Final Batch Loss: 0.6344391703605652\n",
      "Epoch 374, Loss: 4.71570748090744, Final Batch Loss: 0.8183764219284058\n",
      "Epoch 375, Loss: 4.3278443813323975, Final Batch Loss: 0.6068750023841858\n",
      "Epoch 376, Loss: 5.169149577617645, Final Batch Loss: 1.4193142652511597\n",
      "Epoch 377, Loss: 4.753572881221771, Final Batch Loss: 0.9848572015762329\n",
      "Epoch 378, Loss: 4.774849474430084, Final Batch Loss: 0.9666258692741394\n",
      "Epoch 379, Loss: 4.238576650619507, Final Batch Loss: 0.38303613662719727\n",
      "Epoch 380, Loss: 4.359132647514343, Final Batch Loss: 0.6169314384460449\n",
      "Epoch 381, Loss: 4.576248407363892, Final Batch Loss: 0.8884329795837402\n",
      "Epoch 382, Loss: 4.413448452949524, Final Batch Loss: 0.7863816022872925\n",
      "Epoch 383, Loss: 4.718301773071289, Final Batch Loss: 0.9969226121902466\n",
      "Epoch 384, Loss: 5.106276631355286, Final Batch Loss: 1.3236969709396362\n",
      "Epoch 385, Loss: 4.770334899425507, Final Batch Loss: 0.9868325591087341\n",
      "Epoch 386, Loss: 4.660995244979858, Final Batch Loss: 0.9168939590454102\n",
      "Epoch 387, Loss: 4.691043853759766, Final Batch Loss: 0.9743965268135071\n",
      "Epoch 388, Loss: 4.965315759181976, Final Batch Loss: 1.28360915184021\n",
      "Epoch 389, Loss: 4.611178040504456, Final Batch Loss: 0.909516453742981\n",
      "Epoch 390, Loss: 4.647324621677399, Final Batch Loss: 1.0015619993209839\n",
      "Epoch 391, Loss: 5.148783564567566, Final Batch Loss: 1.3403236865997314\n",
      "Epoch 392, Loss: 4.577634632587433, Final Batch Loss: 0.8065098524093628\n",
      "Epoch 393, Loss: 4.735557556152344, Final Batch Loss: 1.0219347476959229\n",
      "Epoch 394, Loss: 4.650281071662903, Final Batch Loss: 1.0362530946731567\n",
      "Epoch 395, Loss: 4.821200609207153, Final Batch Loss: 1.1163069009780884\n",
      "Epoch 396, Loss: 4.614838778972626, Final Batch Loss: 0.8996776938438416\n",
      "Epoch 397, Loss: 4.582885026931763, Final Batch Loss: 0.8608059883117676\n",
      "Epoch 398, Loss: 4.742386341094971, Final Batch Loss: 1.104160189628601\n",
      "Epoch 399, Loss: 4.655721664428711, Final Batch Loss: 1.006397008895874\n",
      "Epoch 400, Loss: 4.681895136833191, Final Batch Loss: 0.9702631831169128\n",
      "Epoch 401, Loss: 4.94524872303009, Final Batch Loss: 1.2578672170639038\n",
      "Epoch 402, Loss: 4.4238104820251465, Final Batch Loss: 0.7925435900688171\n",
      "Epoch 403, Loss: 4.6665167808532715, Final Batch Loss: 0.9373769760131836\n",
      "Epoch 404, Loss: 4.5276747941970825, Final Batch Loss: 0.7887184023857117\n",
      "Epoch 405, Loss: 4.209279716014862, Final Batch Loss: 0.5843083262443542\n",
      "Epoch 406, Loss: 4.874555826187134, Final Batch Loss: 1.1789815425872803\n",
      "Epoch 407, Loss: 4.997097015380859, Final Batch Loss: 1.340969443321228\n",
      "Epoch 408, Loss: 4.299322962760925, Final Batch Loss: 0.6032764315605164\n",
      "Epoch 409, Loss: 4.529285252094269, Final Batch Loss: 0.7873407602310181\n",
      "Epoch 410, Loss: 4.359395205974579, Final Batch Loss: 0.6651867628097534\n",
      "Epoch 411, Loss: 4.992572665214539, Final Batch Loss: 1.3462589979171753\n",
      "Epoch 412, Loss: 4.207295477390289, Final Batch Loss: 0.49206846952438354\n",
      "Epoch 413, Loss: 4.666533291339874, Final Batch Loss: 0.9766625165939331\n",
      "Epoch 414, Loss: 4.384846746921539, Final Batch Loss: 0.689491331577301\n",
      "Epoch 415, Loss: 4.5931665897369385, Final Batch Loss: 0.8765110373497009\n",
      "Epoch 416, Loss: 4.508768081665039, Final Batch Loss: 0.9817582964897156\n",
      "Epoch 417, Loss: 4.614192605018616, Final Batch Loss: 1.039749026298523\n",
      "Epoch 418, Loss: 4.4853546023368835, Final Batch Loss: 0.8096803426742554\n",
      "Epoch 419, Loss: 4.670747816562653, Final Batch Loss: 0.9965828657150269\n",
      "Epoch 420, Loss: 4.535450458526611, Final Batch Loss: 0.7759308218955994\n",
      "Epoch 421, Loss: 4.502754330635071, Final Batch Loss: 0.8609529733657837\n",
      "Epoch 422, Loss: 4.966998994350433, Final Batch Loss: 1.2628530263900757\n",
      "Epoch 423, Loss: 4.987263739109039, Final Batch Loss: 1.3435689210891724\n",
      "Epoch 424, Loss: 4.456389307975769, Final Batch Loss: 0.874204695224762\n",
      "Epoch 425, Loss: 4.715915262699127, Final Batch Loss: 1.0952885150909424\n",
      "Epoch 426, Loss: 4.166385591030121, Final Batch Loss: 0.5215676426887512\n",
      "Epoch 427, Loss: 4.723013877868652, Final Batch Loss: 1.164746880531311\n",
      "Epoch 428, Loss: 4.3678348660469055, Final Batch Loss: 0.7326640486717224\n",
      "Epoch 429, Loss: 4.782787501811981, Final Batch Loss: 1.0669559240341187\n",
      "Epoch 430, Loss: 4.0650893449783325, Final Batch Loss: 0.5444194674491882\n",
      "Epoch 431, Loss: 4.848191142082214, Final Batch Loss: 1.2362215518951416\n",
      "Epoch 432, Loss: 4.26008141040802, Final Batch Loss: 0.6800144910812378\n",
      "Epoch 433, Loss: 4.689003944396973, Final Batch Loss: 1.0988471508026123\n",
      "Epoch 434, Loss: 5.055379748344421, Final Batch Loss: 1.3349123001098633\n",
      "Epoch 435, Loss: 4.6148810386657715, Final Batch Loss: 1.0028387308120728\n",
      "Epoch 436, Loss: 4.673349559307098, Final Batch Loss: 0.9802342057228088\n",
      "Epoch 437, Loss: 4.574812293052673, Final Batch Loss: 0.9062213897705078\n",
      "Epoch 438, Loss: 4.4269797801971436, Final Batch Loss: 0.6993066668510437\n",
      "Epoch 439, Loss: 4.258890390396118, Final Batch Loss: 0.7469187378883362\n",
      "Epoch 440, Loss: 4.350188612937927, Final Batch Loss: 0.6015286445617676\n",
      "Epoch 441, Loss: 4.329039096832275, Final Batch Loss: 0.6404343843460083\n",
      "Epoch 442, Loss: 4.298813760280609, Final Batch Loss: 0.6028677225112915\n",
      "Epoch 443, Loss: 4.505422651767731, Final Batch Loss: 1.0439361333847046\n",
      "Epoch 444, Loss: 4.466396987438202, Final Batch Loss: 0.8625220656394958\n",
      "Epoch 445, Loss: 4.5722596645355225, Final Batch Loss: 1.0757856369018555\n",
      "Epoch 446, Loss: 4.285454094409943, Final Batch Loss: 0.7173406481742859\n",
      "Epoch 447, Loss: 4.496792435646057, Final Batch Loss: 0.8924819231033325\n",
      "Epoch 448, Loss: 4.549343407154083, Final Batch Loss: 0.9709770083427429\n",
      "Epoch 449, Loss: 4.413950324058533, Final Batch Loss: 0.9456177949905396\n",
      "Epoch 450, Loss: 4.4712899923324585, Final Batch Loss: 0.8862618803977966\n",
      "Epoch 451, Loss: 4.45906388759613, Final Batch Loss: 0.909930408000946\n",
      "Epoch 452, Loss: 5.2878753542900085, Final Batch Loss: 1.7607144117355347\n",
      "Epoch 453, Loss: 4.695157885551453, Final Batch Loss: 1.0924077033996582\n",
      "Epoch 454, Loss: 4.8608593344688416, Final Batch Loss: 1.3346372842788696\n",
      "Epoch 455, Loss: 4.416535377502441, Final Batch Loss: 0.9194779992103577\n",
      "Epoch 456, Loss: 4.510165989398956, Final Batch Loss: 0.7979503870010376\n",
      "Epoch 457, Loss: 4.494365811347961, Final Batch Loss: 0.7956722378730774\n",
      "Epoch 458, Loss: 4.6546854972839355, Final Batch Loss: 1.1072651147842407\n",
      "Epoch 459, Loss: 4.337783634662628, Final Batch Loss: 0.7681946158409119\n",
      "Epoch 460, Loss: 4.138550281524658, Final Batch Loss: 0.5518341660499573\n",
      "Epoch 461, Loss: 4.7562591433525085, Final Batch Loss: 1.100878119468689\n",
      "Epoch 462, Loss: 4.282862067222595, Final Batch Loss: 0.7868543267250061\n",
      "Epoch 463, Loss: 4.311285197734833, Final Batch Loss: 0.8573595881462097\n",
      "Epoch 464, Loss: 4.398098111152649, Final Batch Loss: 0.7197244763374329\n",
      "Epoch 465, Loss: 4.410315096378326, Final Batch Loss: 0.9640397429466248\n",
      "Epoch 466, Loss: 4.493445038795471, Final Batch Loss: 1.0201895236968994\n",
      "Epoch 467, Loss: 4.678453505039215, Final Batch Loss: 1.3255860805511475\n",
      "Epoch 468, Loss: 4.14976841211319, Final Batch Loss: 0.5995833277702332\n",
      "Epoch 469, Loss: 4.1146615743637085, Final Batch Loss: 0.5906367897987366\n",
      "Epoch 470, Loss: 4.552629053592682, Final Batch Loss: 1.0710351467132568\n",
      "Epoch 471, Loss: 4.316977322101593, Final Batch Loss: 0.7557131052017212\n",
      "Epoch 472, Loss: 4.412704050540924, Final Batch Loss: 0.9381837248802185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473, Loss: 4.844442188739777, Final Batch Loss: 1.3258272409439087\n",
      "Epoch 474, Loss: 4.416086256504059, Final Batch Loss: 0.9262993931770325\n",
      "Epoch 475, Loss: 4.654585659503937, Final Batch Loss: 1.1350584030151367\n",
      "Epoch 476, Loss: 4.387983977794647, Final Batch Loss: 0.9185787439346313\n",
      "Epoch 477, Loss: 4.369311273097992, Final Batch Loss: 0.9380397200584412\n",
      "Epoch 478, Loss: 4.062699615955353, Final Batch Loss: 0.6522327065467834\n",
      "Epoch 479, Loss: 4.476694107055664, Final Batch Loss: 1.1070183515548706\n",
      "Epoch 480, Loss: 4.378560960292816, Final Batch Loss: 0.9096383452415466\n",
      "Epoch 481, Loss: 4.4504459500312805, Final Batch Loss: 0.9646663665771484\n",
      "Epoch 482, Loss: 3.9444637298583984, Final Batch Loss: 0.5420902967453003\n",
      "Epoch 483, Loss: 3.9373836517333984, Final Batch Loss: 0.5606001019477844\n",
      "Epoch 484, Loss: 4.008157342672348, Final Batch Loss: 0.43985167145729065\n",
      "Epoch 485, Loss: 4.644037306308746, Final Batch Loss: 1.2299535274505615\n",
      "Epoch 486, Loss: 4.5812326073646545, Final Batch Loss: 0.9702330231666565\n",
      "Epoch 487, Loss: 4.5937288999557495, Final Batch Loss: 1.083479881286621\n",
      "Epoch 488, Loss: 4.4783660769462585, Final Batch Loss: 1.0857226848602295\n",
      "Epoch 489, Loss: 4.003595650196075, Final Batch Loss: 0.5981026291847229\n",
      "Epoch 490, Loss: 4.247731924057007, Final Batch Loss: 0.8365558981895447\n",
      "Epoch 491, Loss: 4.311406433582306, Final Batch Loss: 0.8629005551338196\n",
      "Epoch 492, Loss: 4.1523237228393555, Final Batch Loss: 0.7625991702079773\n",
      "Epoch 493, Loss: 4.010831534862518, Final Batch Loss: 0.6662895083427429\n",
      "Epoch 494, Loss: 4.346957147121429, Final Batch Loss: 0.8669699430465698\n",
      "Epoch 495, Loss: 4.385225415229797, Final Batch Loss: 0.9985770583152771\n",
      "Epoch 496, Loss: 4.56902277469635, Final Batch Loss: 1.156233787536621\n",
      "Epoch 497, Loss: 4.248766183853149, Final Batch Loss: 0.7967678308486938\n",
      "Epoch 498, Loss: 4.339710295200348, Final Batch Loss: 0.9786311984062195\n",
      "Epoch 499, Loss: 4.294806778430939, Final Batch Loss: 0.8317475914955139\n",
      "Epoch 500, Loss: 4.357027530670166, Final Batch Loss: 0.7603327631950378\n",
      "Epoch 501, Loss: 4.16466748714447, Final Batch Loss: 0.6484142541885376\n",
      "Epoch 502, Loss: 4.2462722063064575, Final Batch Loss: 0.9337873458862305\n",
      "Epoch 503, Loss: 3.9598960280418396, Final Batch Loss: 0.7232763171195984\n",
      "Epoch 504, Loss: 3.8952831029891968, Final Batch Loss: 0.5001584887504578\n",
      "Epoch 505, Loss: 4.030289530754089, Final Batch Loss: 0.6584782600402832\n",
      "Epoch 506, Loss: 4.252299427986145, Final Batch Loss: 0.8442862629890442\n",
      "Epoch 507, Loss: 3.999355375766754, Final Batch Loss: 0.6787480711936951\n",
      "Epoch 508, Loss: 4.017729640007019, Final Batch Loss: 0.5680437684059143\n",
      "Epoch 509, Loss: 4.115242958068848, Final Batch Loss: 0.7367376685142517\n",
      "Epoch 510, Loss: 4.1714377999305725, Final Batch Loss: 0.8961111903190613\n",
      "Epoch 511, Loss: 4.666576266288757, Final Batch Loss: 1.2837514877319336\n",
      "Epoch 512, Loss: 4.658132553100586, Final Batch Loss: 1.1834367513656616\n",
      "Epoch 513, Loss: 4.31254118680954, Final Batch Loss: 0.8708643913269043\n",
      "Epoch 514, Loss: 4.044423282146454, Final Batch Loss: 0.6829982995986938\n",
      "Epoch 515, Loss: 4.419032573699951, Final Batch Loss: 0.9297745823860168\n",
      "Epoch 516, Loss: 4.155507385730743, Final Batch Loss: 0.838249683380127\n",
      "Epoch 517, Loss: 3.9770005345344543, Final Batch Loss: 0.519623875617981\n",
      "Epoch 518, Loss: 4.740112364292145, Final Batch Loss: 1.3698185682296753\n",
      "Epoch 519, Loss: 4.385983824729919, Final Batch Loss: 0.9918102025985718\n",
      "Epoch 520, Loss: 4.182902157306671, Final Batch Loss: 0.8931140303611755\n",
      "Epoch 521, Loss: 4.1101967096328735, Final Batch Loss: 0.6865636706352234\n",
      "Epoch 522, Loss: 4.289975821971893, Final Batch Loss: 0.8578119277954102\n",
      "Epoch 523, Loss: 4.072279632091522, Final Batch Loss: 0.6700071692466736\n",
      "Epoch 524, Loss: 4.030485212802887, Final Batch Loss: 0.5971126556396484\n",
      "Epoch 525, Loss: 4.99012815952301, Final Batch Loss: 1.7051395177841187\n",
      "Epoch 526, Loss: 4.060344219207764, Final Batch Loss: 0.6390476226806641\n",
      "Epoch 527, Loss: 4.237017869949341, Final Batch Loss: 0.8000108599662781\n",
      "Epoch 528, Loss: 4.148714780807495, Final Batch Loss: 0.7347426414489746\n",
      "Epoch 529, Loss: 3.684997409582138, Final Batch Loss: 0.3671473562717438\n",
      "Epoch 530, Loss: 4.058129727840424, Final Batch Loss: 0.7319496870040894\n",
      "Epoch 531, Loss: 4.176652014255524, Final Batch Loss: 0.8734039664268494\n",
      "Epoch 532, Loss: 4.4847771525383, Final Batch Loss: 1.0485668182373047\n",
      "Epoch 533, Loss: 4.1939475536346436, Final Batch Loss: 0.8035611510276794\n",
      "Epoch 534, Loss: 4.042401492595673, Final Batch Loss: 0.7129503488540649\n",
      "Epoch 535, Loss: 3.8282931447029114, Final Batch Loss: 0.5329186320304871\n",
      "Epoch 536, Loss: 4.362346827983856, Final Batch Loss: 1.0199674367904663\n",
      "Epoch 537, Loss: 4.188206493854523, Final Batch Loss: 0.9512828588485718\n",
      "Epoch 538, Loss: 4.387474060058594, Final Batch Loss: 1.1136358976364136\n",
      "Epoch 539, Loss: 4.181701898574829, Final Batch Loss: 0.8118395209312439\n",
      "Epoch 540, Loss: 4.297444939613342, Final Batch Loss: 0.949645459651947\n",
      "Epoch 541, Loss: 4.237471759319305, Final Batch Loss: 0.8579829335212708\n",
      "Epoch 542, Loss: 4.142646074295044, Final Batch Loss: 0.8634399771690369\n",
      "Epoch 543, Loss: 4.2066508531570435, Final Batch Loss: 0.7966762185096741\n",
      "Epoch 544, Loss: 4.569857835769653, Final Batch Loss: 1.3906011581420898\n",
      "Epoch 545, Loss: 3.8841676115989685, Final Batch Loss: 0.6307505965232849\n",
      "Epoch 546, Loss: 3.8618967533111572, Final Batch Loss: 0.4553844928741455\n",
      "Epoch 547, Loss: 4.221124053001404, Final Batch Loss: 0.8327004313468933\n",
      "Epoch 548, Loss: 4.015167951583862, Final Batch Loss: 0.7179268598556519\n",
      "Epoch 549, Loss: 3.8323447704315186, Final Batch Loss: 0.5718945264816284\n",
      "Epoch 550, Loss: 4.142098486423492, Final Batch Loss: 0.7359916567802429\n",
      "Epoch 551, Loss: 3.937386393547058, Final Batch Loss: 0.667117714881897\n",
      "Epoch 552, Loss: 4.474645137786865, Final Batch Loss: 1.0794278383255005\n",
      "Epoch 553, Loss: 4.096476972103119, Final Batch Loss: 0.8246442675590515\n",
      "Epoch 554, Loss: 3.940639615058899, Final Batch Loss: 0.6683544516563416\n",
      "Epoch 555, Loss: 4.380890607833862, Final Batch Loss: 1.0317710638046265\n",
      "Epoch 556, Loss: 4.3073641657829285, Final Batch Loss: 0.8824893236160278\n",
      "Epoch 557, Loss: 4.178573787212372, Final Batch Loss: 0.7955949902534485\n",
      "Epoch 558, Loss: 4.195783197879791, Final Batch Loss: 0.8762200474739075\n",
      "Epoch 559, Loss: 3.9432228803634644, Final Batch Loss: 0.658122181892395\n",
      "Epoch 560, Loss: 4.284987032413483, Final Batch Loss: 1.0325015783309937\n",
      "Epoch 561, Loss: 3.9706167578697205, Final Batch Loss: 0.6218412518501282\n",
      "Epoch 562, Loss: 3.9901527166366577, Final Batch Loss: 0.7398248910903931\n",
      "Epoch 563, Loss: 4.039725124835968, Final Batch Loss: 0.7853809595108032\n",
      "Epoch 564, Loss: 3.780571311712265, Final Batch Loss: 0.4734441339969635\n",
      "Epoch 565, Loss: 3.8215954899787903, Final Batch Loss: 0.661224901676178\n",
      "Epoch 566, Loss: 3.9621185064315796, Final Batch Loss: 0.7943516373634338\n",
      "Epoch 567, Loss: 3.724791079759598, Final Batch Loss: 0.4723576605319977\n",
      "Epoch 568, Loss: 3.853450119495392, Final Batch Loss: 0.6273908019065857\n",
      "Epoch 569, Loss: 4.266003727912903, Final Batch Loss: 1.105491280555725\n",
      "Epoch 570, Loss: 3.784051299095154, Final Batch Loss: 0.5990045666694641\n",
      "Epoch 571, Loss: 3.9537327885627747, Final Batch Loss: 0.7503235936164856\n",
      "Epoch 572, Loss: 4.573087811470032, Final Batch Loss: 1.255627989768982\n",
      "Epoch 573, Loss: 3.979089319705963, Final Batch Loss: 0.7472273111343384\n",
      "Epoch 574, Loss: 4.407656371593475, Final Batch Loss: 1.1135892868041992\n",
      "Epoch 575, Loss: 4.340904116630554, Final Batch Loss: 1.0569226741790771\n",
      "Epoch 576, Loss: 4.081067383289337, Final Batch Loss: 0.8254147171974182\n",
      "Epoch 577, Loss: 3.964720904827118, Final Batch Loss: 0.5849998593330383\n",
      "Epoch 578, Loss: 4.039839208126068, Final Batch Loss: 0.8145513534545898\n",
      "Epoch 579, Loss: 3.826849937438965, Final Batch Loss: 0.6575409173965454\n",
      "Epoch 580, Loss: 4.4615771770477295, Final Batch Loss: 1.0908716917037964\n",
      "Epoch 581, Loss: 3.7904669046401978, Final Batch Loss: 0.6293983459472656\n",
      "Epoch 582, Loss: 4.0841227769851685, Final Batch Loss: 0.8732978701591492\n",
      "Epoch 583, Loss: 3.9317098259925842, Final Batch Loss: 0.7195643186569214\n",
      "Epoch 584, Loss: 4.743522465229034, Final Batch Loss: 1.5272293090820312\n",
      "Epoch 585, Loss: 4.13741397857666, Final Batch Loss: 0.9563805460929871\n",
      "Epoch 586, Loss: 4.554375886917114, Final Batch Loss: 1.2121858596801758\n",
      "Epoch 587, Loss: 4.065605938434601, Final Batch Loss: 0.7754884958267212\n",
      "Epoch 588, Loss: 4.2839601039886475, Final Batch Loss: 1.095124363899231\n",
      "Epoch 589, Loss: 3.8283543586730957, Final Batch Loss: 0.5757381319999695\n",
      "Epoch 590, Loss: 4.185352325439453, Final Batch Loss: 0.9567145109176636\n",
      "Epoch 591, Loss: 4.093783915042877, Final Batch Loss: 0.8873162865638733\n",
      "Epoch 592, Loss: 4.0018123388290405, Final Batch Loss: 0.7668455839157104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 593, Loss: 3.5193435847759247, Final Batch Loss: 0.3826119005680084\n",
      "Epoch 594, Loss: 3.8594560623168945, Final Batch Loss: 0.6318354606628418\n",
      "Epoch 595, Loss: 3.7938426733016968, Final Batch Loss: 0.5934720039367676\n",
      "Epoch 596, Loss: 4.323070645332336, Final Batch Loss: 1.094582438468933\n",
      "Epoch 597, Loss: 3.879803419113159, Final Batch Loss: 0.7333498001098633\n",
      "Epoch 598, Loss: 3.6164044737815857, Final Batch Loss: 0.41882091760635376\n",
      "Epoch 599, Loss: 4.197253406047821, Final Batch Loss: 1.0159492492675781\n",
      "Epoch 600, Loss: 4.000782072544098, Final Batch Loss: 0.9034056067466736\n",
      "Epoch 601, Loss: 3.8428151607513428, Final Batch Loss: 0.7001506090164185\n",
      "Epoch 602, Loss: 3.801253080368042, Final Batch Loss: 0.707209587097168\n",
      "Epoch 603, Loss: 3.8791571855545044, Final Batch Loss: 0.7415987849235535\n",
      "Epoch 604, Loss: 3.6840293407440186, Final Batch Loss: 0.6614674925804138\n",
      "Epoch 605, Loss: 4.133839786052704, Final Batch Loss: 0.8929067850112915\n",
      "Epoch 606, Loss: 4.630938410758972, Final Batch Loss: 1.3456757068634033\n",
      "Epoch 607, Loss: 4.321214258670807, Final Batch Loss: 1.0879656076431274\n",
      "Epoch 608, Loss: 3.9446476697921753, Final Batch Loss: 0.854284942150116\n",
      "Epoch 609, Loss: 4.121484339237213, Final Batch Loss: 0.9300064444541931\n",
      "Epoch 610, Loss: 3.955643594264984, Final Batch Loss: 0.7212901711463928\n",
      "Epoch 611, Loss: 3.976036250591278, Final Batch Loss: 0.7560335993766785\n",
      "Epoch 612, Loss: 4.448134422302246, Final Batch Loss: 1.3416725397109985\n",
      "Epoch 613, Loss: 3.9209157824516296, Final Batch Loss: 0.6621788144111633\n",
      "Epoch 614, Loss: 4.497749149799347, Final Batch Loss: 1.2220059633255005\n",
      "Epoch 615, Loss: 4.0838072299957275, Final Batch Loss: 0.948675274848938\n",
      "Epoch 616, Loss: 3.999873399734497, Final Batch Loss: 0.8442381620407104\n",
      "Epoch 617, Loss: 3.5198043286800385, Final Batch Loss: 0.3233790695667267\n",
      "Epoch 618, Loss: 3.7876322269439697, Final Batch Loss: 0.6404547095298767\n",
      "Epoch 619, Loss: 3.802660048007965, Final Batch Loss: 0.7002630233764648\n",
      "Epoch 620, Loss: 4.119486093521118, Final Batch Loss: 0.9304046630859375\n",
      "Epoch 621, Loss: 4.160128474235535, Final Batch Loss: 0.9550471305847168\n",
      "Epoch 622, Loss: 3.7721424102783203, Final Batch Loss: 0.6448002457618713\n",
      "Epoch 623, Loss: 4.432879984378815, Final Batch Loss: 1.2950541973114014\n",
      "Epoch 624, Loss: 4.393347382545471, Final Batch Loss: 1.3233492374420166\n",
      "Epoch 625, Loss: 4.082737445831299, Final Batch Loss: 0.8422109484672546\n",
      "Epoch 626, Loss: 4.798926591873169, Final Batch Loss: 1.6038106679916382\n",
      "Epoch 627, Loss: 3.7192253470420837, Final Batch Loss: 0.5819852948188782\n",
      "Epoch 628, Loss: 4.185097396373749, Final Batch Loss: 1.0440250635147095\n",
      "Epoch 629, Loss: 3.9151326417922974, Final Batch Loss: 0.7174112200737\n",
      "Epoch 630, Loss: 3.8121939301490784, Final Batch Loss: 0.6619869470596313\n",
      "Epoch 631, Loss: 4.065149366855621, Final Batch Loss: 0.9270497560501099\n",
      "Epoch 632, Loss: 4.121031224727631, Final Batch Loss: 0.8755466341972351\n",
      "Epoch 633, Loss: 4.011035680770874, Final Batch Loss: 0.987632691860199\n",
      "Epoch 634, Loss: 4.013876557350159, Final Batch Loss: 0.9447619318962097\n",
      "Epoch 635, Loss: 4.075929284095764, Final Batch Loss: 0.9234622716903687\n",
      "Epoch 636, Loss: 3.828841984272003, Final Batch Loss: 0.7109490633010864\n",
      "Epoch 637, Loss: 3.6336366534233093, Final Batch Loss: 0.5183948874473572\n",
      "Epoch 638, Loss: 3.671667605638504, Final Batch Loss: 0.39480939507484436\n",
      "Epoch 639, Loss: 4.019944965839386, Final Batch Loss: 0.9881514310836792\n",
      "Epoch 640, Loss: 4.103302180767059, Final Batch Loss: 0.8923152685165405\n",
      "Epoch 641, Loss: 4.0927024483680725, Final Batch Loss: 0.9196498990058899\n",
      "Epoch 642, Loss: 3.8234416246414185, Final Batch Loss: 0.7032214999198914\n",
      "Epoch 643, Loss: 3.968312621116638, Final Batch Loss: 0.899203896522522\n",
      "Epoch 644, Loss: 3.581285744905472, Final Batch Loss: 0.39461371302604675\n",
      "Epoch 645, Loss: 4.194456875324249, Final Batch Loss: 1.0501919984817505\n",
      "Epoch 646, Loss: 3.9580761194229126, Final Batch Loss: 0.8879006505012512\n",
      "Epoch 647, Loss: 3.7007091641426086, Final Batch Loss: 0.5741826295852661\n",
      "Epoch 648, Loss: 3.876521587371826, Final Batch Loss: 0.7312431335449219\n",
      "Epoch 649, Loss: 4.127900183200836, Final Batch Loss: 1.02292799949646\n",
      "Epoch 650, Loss: 4.062107026576996, Final Batch Loss: 0.9414032697677612\n",
      "Epoch 651, Loss: 4.0174989104270935, Final Batch Loss: 0.7756504416465759\n",
      "Epoch 652, Loss: 3.7637534737586975, Final Batch Loss: 0.6778658628463745\n",
      "Epoch 653, Loss: 4.242580890655518, Final Batch Loss: 0.9782605171203613\n",
      "Epoch 654, Loss: 3.9257333278656006, Final Batch Loss: 0.7888993620872498\n",
      "Epoch 655, Loss: 3.9533809423446655, Final Batch Loss: 0.6419748663902283\n",
      "Epoch 656, Loss: 3.6227734088897705, Final Batch Loss: 0.5251823663711548\n",
      "Epoch 657, Loss: 4.537344813346863, Final Batch Loss: 1.4061346054077148\n",
      "Epoch 658, Loss: 3.951425015926361, Final Batch Loss: 0.8952844738960266\n",
      "Epoch 659, Loss: 3.7395243048667908, Final Batch Loss: 0.5688411593437195\n",
      "Epoch 660, Loss: 4.151174604892731, Final Batch Loss: 1.1000220775604248\n",
      "Epoch 661, Loss: 3.810780704021454, Final Batch Loss: 0.7944408655166626\n",
      "Epoch 662, Loss: 4.419744551181793, Final Batch Loss: 1.2354234457015991\n",
      "Epoch 663, Loss: 3.524503529071808, Final Batch Loss: 0.4378213882446289\n",
      "Epoch 664, Loss: 3.591303139925003, Final Batch Loss: 0.4177774488925934\n",
      "Epoch 665, Loss: 4.454610824584961, Final Batch Loss: 1.3899248838424683\n",
      "Epoch 666, Loss: 4.03112256526947, Final Batch Loss: 0.9150457978248596\n",
      "Epoch 667, Loss: 4.189777612686157, Final Batch Loss: 1.075134038925171\n",
      "Epoch 668, Loss: 4.097155690193176, Final Batch Loss: 1.0192229747772217\n",
      "Epoch 669, Loss: 4.177368998527527, Final Batch Loss: 1.0980231761932373\n",
      "Epoch 670, Loss: 3.6282448768615723, Final Batch Loss: 0.5488830804824829\n",
      "Epoch 671, Loss: 3.9982604384422302, Final Batch Loss: 0.9037030339241028\n",
      "Epoch 672, Loss: 3.7401317358016968, Final Batch Loss: 0.6652249693870544\n",
      "Epoch 673, Loss: 3.8589720726013184, Final Batch Loss: 0.7557513117790222\n",
      "Epoch 674, Loss: 4.153915584087372, Final Batch Loss: 1.092583417892456\n",
      "Epoch 675, Loss: 3.6854329705238342, Final Batch Loss: 0.6880022287368774\n",
      "Epoch 676, Loss: 4.388999283313751, Final Batch Loss: 1.2916096448898315\n",
      "Epoch 677, Loss: 3.9757404923439026, Final Batch Loss: 0.8830299377441406\n",
      "Epoch 678, Loss: 4.052151918411255, Final Batch Loss: 0.9880009889602661\n",
      "Epoch 679, Loss: 4.005777299404144, Final Batch Loss: 0.9767352342605591\n",
      "Epoch 680, Loss: 3.525625705718994, Final Batch Loss: 0.5609776377677917\n",
      "Epoch 681, Loss: 4.233767092227936, Final Batch Loss: 0.9925400614738464\n",
      "Epoch 682, Loss: 4.093229353427887, Final Batch Loss: 0.9941654205322266\n",
      "Epoch 683, Loss: 3.8665115237236023, Final Batch Loss: 0.8946309685707092\n",
      "Epoch 684, Loss: 3.5595396757125854, Final Batch Loss: 0.5305418968200684\n",
      "Epoch 685, Loss: 3.897760808467865, Final Batch Loss: 0.7626331448554993\n",
      "Epoch 686, Loss: 3.7673665285110474, Final Batch Loss: 0.6845305562019348\n",
      "Epoch 687, Loss: 3.801439106464386, Final Batch Loss: 0.7664928436279297\n",
      "Epoch 688, Loss: 3.8088656663894653, Final Batch Loss: 0.6168397068977356\n",
      "Epoch 689, Loss: 3.72185218334198, Final Batch Loss: 0.7467454075813293\n",
      "Epoch 690, Loss: 3.601089596748352, Final Batch Loss: 0.6864385008811951\n",
      "Epoch 691, Loss: 3.6639546751976013, Final Batch Loss: 0.7260755300521851\n",
      "Epoch 692, Loss: 3.3313074707984924, Final Batch Loss: 0.30913251638412476\n",
      "Epoch 693, Loss: 4.050566792488098, Final Batch Loss: 1.0818603038787842\n",
      "Epoch 694, Loss: 3.7280330061912537, Final Batch Loss: 0.7607104182243347\n",
      "Epoch 695, Loss: 3.877417206764221, Final Batch Loss: 0.8257026672363281\n",
      "Epoch 696, Loss: 3.5733014345169067, Final Batch Loss: 0.563144862651825\n",
      "Epoch 697, Loss: 3.6886650919914246, Final Batch Loss: 0.6870895028114319\n",
      "Epoch 698, Loss: 4.017223656177521, Final Batch Loss: 0.9287570714950562\n",
      "Epoch 699, Loss: 4.059308350086212, Final Batch Loss: 0.861430287361145\n",
      "Epoch 700, Loss: 4.282374978065491, Final Batch Loss: 1.1050177812576294\n",
      "Epoch 701, Loss: 3.8519757986068726, Final Batch Loss: 0.7137417197227478\n",
      "Epoch 702, Loss: 3.334790527820587, Final Batch Loss: 0.3720340132713318\n",
      "Epoch 703, Loss: 3.8963794112205505, Final Batch Loss: 0.9954029321670532\n",
      "Epoch 704, Loss: 3.70862740278244, Final Batch Loss: 0.6658469438552856\n",
      "Epoch 705, Loss: 3.994975745677948, Final Batch Loss: 0.9647678136825562\n",
      "Epoch 706, Loss: 3.8100229501724243, Final Batch Loss: 0.8387510180473328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 707, Loss: 4.095272243022919, Final Batch Loss: 0.9919201135635376\n",
      "Epoch 708, Loss: 3.7437337040901184, Final Batch Loss: 0.8170729279518127\n",
      "Epoch 709, Loss: 4.374635994434357, Final Batch Loss: 1.3388563394546509\n",
      "Epoch 710, Loss: 3.74583899974823, Final Batch Loss: 0.7971184849739075\n",
      "Epoch 711, Loss: 3.7605080008506775, Final Batch Loss: 0.6361276507377625\n",
      "Epoch 712, Loss: 3.6549558639526367, Final Batch Loss: 0.6609655022621155\n",
      "Epoch 713, Loss: 4.097437858581543, Final Batch Loss: 0.90715092420578\n",
      "Epoch 714, Loss: 3.4800162613391876, Final Batch Loss: 0.3950135409832001\n",
      "Epoch 715, Loss: 3.7429935336112976, Final Batch Loss: 0.7250267267227173\n",
      "Epoch 716, Loss: 3.498529076576233, Final Batch Loss: 0.660566508769989\n",
      "Epoch 717, Loss: 3.838971972465515, Final Batch Loss: 0.9023926854133606\n",
      "Epoch 718, Loss: 3.5389716029167175, Final Batch Loss: 0.5703507661819458\n",
      "Epoch 719, Loss: 4.002231657505035, Final Batch Loss: 1.034849762916565\n",
      "Epoch 720, Loss: 3.918358087539673, Final Batch Loss: 0.9920632243156433\n",
      "Epoch 721, Loss: 3.7317416071891785, Final Batch Loss: 0.7658818960189819\n",
      "Epoch 722, Loss: 3.43389692902565, Final Batch Loss: 0.4908970296382904\n",
      "Epoch 723, Loss: 4.092960238456726, Final Batch Loss: 1.0380078554153442\n",
      "Epoch 724, Loss: 3.625557065010071, Final Batch Loss: 0.669211745262146\n",
      "Epoch 725, Loss: 3.8104411959648132, Final Batch Loss: 0.8444681167602539\n",
      "Epoch 726, Loss: 3.7973775267601013, Final Batch Loss: 0.8669236302375793\n",
      "Epoch 727, Loss: 4.094829320907593, Final Batch Loss: 1.071894645690918\n",
      "Epoch 728, Loss: 3.7466827630996704, Final Batch Loss: 0.743563711643219\n",
      "Epoch 729, Loss: 3.829982340335846, Final Batch Loss: 0.854590892791748\n",
      "Epoch 730, Loss: 3.7523332238197327, Final Batch Loss: 0.7133630514144897\n",
      "Epoch 731, Loss: 3.8178000450134277, Final Batch Loss: 0.885837972164154\n",
      "Epoch 732, Loss: 3.5683205127716064, Final Batch Loss: 0.5089840292930603\n",
      "Epoch 733, Loss: 3.7185206413269043, Final Batch Loss: 0.7414889335632324\n",
      "Epoch 734, Loss: 3.7168936729431152, Final Batch Loss: 0.7227156758308411\n",
      "Epoch 735, Loss: 3.652198851108551, Final Batch Loss: 0.614176869392395\n",
      "Epoch 736, Loss: 3.678428143262863, Final Batch Loss: 0.46745017170906067\n",
      "Epoch 737, Loss: 3.4836543798446655, Final Batch Loss: 0.4508787989616394\n",
      "Epoch 738, Loss: 4.256255507469177, Final Batch Loss: 1.222603678703308\n",
      "Epoch 739, Loss: 3.591518819332123, Final Batch Loss: 0.6693168878555298\n",
      "Epoch 740, Loss: 3.7354953289031982, Final Batch Loss: 0.7481806874275208\n",
      "Epoch 741, Loss: 3.695921242237091, Final Batch Loss: 0.677376925945282\n",
      "Epoch 742, Loss: 3.87496018409729, Final Batch Loss: 0.825668454170227\n",
      "Epoch 743, Loss: 3.589688777923584, Final Batch Loss: 0.5532738566398621\n",
      "Epoch 744, Loss: 3.4329271018505096, Final Batch Loss: 0.40594229102134705\n",
      "Epoch 745, Loss: 3.620357632637024, Final Batch Loss: 0.7708352208137512\n",
      "Epoch 746, Loss: 3.57037615776062, Final Batch Loss: 0.6449034810066223\n",
      "Epoch 747, Loss: 3.462984323501587, Final Batch Loss: 0.5713964700698853\n",
      "Epoch 748, Loss: 3.842835247516632, Final Batch Loss: 0.958135187625885\n",
      "Epoch 749, Loss: 3.5453047156333923, Final Batch Loss: 0.5534150004386902\n",
      "Epoch 750, Loss: 3.4133896231651306, Final Batch Loss: 0.469602108001709\n",
      "Epoch 751, Loss: 3.546147644519806, Final Batch Loss: 0.5471908450126648\n",
      "Epoch 752, Loss: 3.547508478164673, Final Batch Loss: 0.5360512137413025\n",
      "Epoch 753, Loss: 4.110119998455048, Final Batch Loss: 1.2091976404190063\n",
      "Epoch 754, Loss: 3.4904614090919495, Final Batch Loss: 0.5620729327201843\n",
      "Epoch 755, Loss: 4.056991815567017, Final Batch Loss: 1.0810543298721313\n",
      "Epoch 756, Loss: 4.029546797275543, Final Batch Loss: 1.1442739963531494\n",
      "Epoch 757, Loss: 3.6325626373291016, Final Batch Loss: 0.7065884470939636\n",
      "Epoch 758, Loss: 3.728354811668396, Final Batch Loss: 0.886986255645752\n",
      "Epoch 759, Loss: 3.805412530899048, Final Batch Loss: 0.8859954476356506\n",
      "Epoch 760, Loss: 3.3698550760746, Final Batch Loss: 0.472336083650589\n",
      "Epoch 761, Loss: 3.8781890869140625, Final Batch Loss: 0.9663549065589905\n",
      "Epoch 762, Loss: 3.931098222732544, Final Batch Loss: 1.0103871822357178\n",
      "Epoch 763, Loss: 3.3968154788017273, Final Batch Loss: 0.6111146211624146\n",
      "Epoch 764, Loss: 3.4445119500160217, Final Batch Loss: 0.5579477548599243\n",
      "Epoch 765, Loss: 3.576953887939453, Final Batch Loss: 0.6044450998306274\n",
      "Epoch 766, Loss: 3.9823893308639526, Final Batch Loss: 0.9579644799232483\n",
      "Epoch 767, Loss: 3.881578028202057, Final Batch Loss: 0.9879277348518372\n",
      "Epoch 768, Loss: 3.645556151866913, Final Batch Loss: 0.6028116345405579\n",
      "Epoch 769, Loss: 3.9241459369659424, Final Batch Loss: 0.9708067178726196\n",
      "Epoch 770, Loss: 3.622708559036255, Final Batch Loss: 0.608123779296875\n",
      "Epoch 771, Loss: 3.679942011833191, Final Batch Loss: 0.8336165547370911\n",
      "Epoch 772, Loss: 4.152873337268829, Final Batch Loss: 1.3116761445999146\n",
      "Epoch 773, Loss: 3.9798185229301453, Final Batch Loss: 1.0763832330703735\n",
      "Epoch 774, Loss: 3.7150497436523438, Final Batch Loss: 0.7228400111198425\n",
      "Epoch 775, Loss: 3.6871232986450195, Final Batch Loss: 0.8858363032341003\n",
      "Epoch 776, Loss: 3.5810486674308777, Final Batch Loss: 0.7093929648399353\n",
      "Epoch 777, Loss: 3.5695770382881165, Final Batch Loss: 0.6137757301330566\n",
      "Epoch 778, Loss: 3.46106493473053, Final Batch Loss: 0.5446546673774719\n",
      "Epoch 779, Loss: 4.041426479816437, Final Batch Loss: 1.1326667070388794\n",
      "Epoch 780, Loss: 3.7168959975242615, Final Batch Loss: 0.8220938444137573\n",
      "Epoch 781, Loss: 3.525925040245056, Final Batch Loss: 0.5878568291664124\n",
      "Epoch 782, Loss: 3.3586230874061584, Final Batch Loss: 0.5212187767028809\n",
      "Epoch 783, Loss: 4.107974529266357, Final Batch Loss: 1.176440954208374\n",
      "Epoch 784, Loss: 3.4037920236587524, Final Batch Loss: 0.5766037702560425\n",
      "Epoch 785, Loss: 3.8165040612220764, Final Batch Loss: 0.9272209405899048\n",
      "Epoch 786, Loss: 3.4872822165489197, Final Batch Loss: 0.5536815524101257\n",
      "Epoch 787, Loss: 3.8050413131713867, Final Batch Loss: 1.0336287021636963\n",
      "Epoch 788, Loss: 3.6715811491012573, Final Batch Loss: 0.9193063378334045\n",
      "Epoch 789, Loss: 3.7464486956596375, Final Batch Loss: 0.9657194018363953\n",
      "Epoch 790, Loss: 3.8469889760017395, Final Batch Loss: 0.9942751526832581\n",
      "Epoch 791, Loss: 3.665696859359741, Final Batch Loss: 0.7835259437561035\n",
      "Epoch 792, Loss: 3.5196115374565125, Final Batch Loss: 0.6158885955810547\n",
      "Epoch 793, Loss: 3.5861452221870422, Final Batch Loss: 0.7739343643188477\n",
      "Epoch 794, Loss: 3.6735870242118835, Final Batch Loss: 0.7623395919799805\n",
      "Epoch 795, Loss: 3.581543982028961, Final Batch Loss: 0.6282792091369629\n",
      "Epoch 796, Loss: 3.2517585456371307, Final Batch Loss: 0.4380365312099457\n",
      "Epoch 797, Loss: 3.695371091365814, Final Batch Loss: 0.8527769446372986\n",
      "Epoch 798, Loss: 3.443378448486328, Final Batch Loss: 0.6171349287033081\n",
      "Epoch 799, Loss: 3.7077099680900574, Final Batch Loss: 0.9071028828620911\n",
      "Epoch 800, Loss: 3.5223538279533386, Final Batch Loss: 0.6623159050941467\n",
      "Epoch 801, Loss: 3.460937261581421, Final Batch Loss: 0.5776335000991821\n",
      "Epoch 802, Loss: 3.465362012386322, Final Batch Loss: 0.5696927905082703\n",
      "Epoch 803, Loss: 3.328569084405899, Final Batch Loss: 0.4895385205745697\n",
      "Epoch 804, Loss: 3.3447641134262085, Final Batch Loss: 0.5668158531188965\n",
      "Epoch 805, Loss: 3.615597426891327, Final Batch Loss: 0.8501545786857605\n",
      "Epoch 806, Loss: 3.5750144720077515, Final Batch Loss: 0.6909183263778687\n",
      "Epoch 807, Loss: 3.6389282941818237, Final Batch Loss: 0.6950165629386902\n",
      "Epoch 808, Loss: 3.6463234424591064, Final Batch Loss: 0.7071858644485474\n",
      "Epoch 809, Loss: 3.1917174756526947, Final Batch Loss: 0.46563205122947693\n",
      "Epoch 810, Loss: 3.445681393146515, Final Batch Loss: 0.5981799364089966\n",
      "Epoch 811, Loss: 3.4494041204452515, Final Batch Loss: 0.5980693697929382\n",
      "Epoch 812, Loss: 3.36047899723053, Final Batch Loss: 0.5793963670730591\n",
      "Epoch 813, Loss: 3.5833122730255127, Final Batch Loss: 0.7400155067443848\n",
      "Epoch 814, Loss: 3.813115656375885, Final Batch Loss: 0.9110546112060547\n",
      "Epoch 815, Loss: 3.7403191328048706, Final Batch Loss: 0.9500184059143066\n",
      "Epoch 816, Loss: 3.5407447814941406, Final Batch Loss: 0.7323786020278931\n",
      "Epoch 817, Loss: 3.385679602622986, Final Batch Loss: 0.4680320620536804\n",
      "Epoch 818, Loss: 3.2215627431869507, Final Batch Loss: 0.5715050101280212\n",
      "Epoch 819, Loss: 3.6110551357269287, Final Batch Loss: 0.7946816682815552\n",
      "Epoch 820, Loss: 3.6187999844551086, Final Batch Loss: 0.7749156355857849\n",
      "Epoch 821, Loss: 3.304535448551178, Final Batch Loss: 0.42629194259643555\n",
      "Epoch 822, Loss: 3.9340628385543823, Final Batch Loss: 1.1588468551635742\n",
      "Epoch 823, Loss: 3.5958829522132874, Final Batch Loss: 0.802603542804718\n",
      "Epoch 824, Loss: 3.6869238018989563, Final Batch Loss: 0.9723168015480042\n",
      "Epoch 825, Loss: 3.987531840801239, Final Batch Loss: 1.1040645837783813\n",
      "Epoch 826, Loss: 3.6155819296836853, Final Batch Loss: 0.7724728584289551\n",
      "Epoch 827, Loss: 3.553924322128296, Final Batch Loss: 0.792690098285675\n",
      "Epoch 828, Loss: 3.4682705402374268, Final Batch Loss: 0.5852118730545044\n",
      "Epoch 829, Loss: 3.8864513635635376, Final Batch Loss: 1.1433827877044678\n",
      "Epoch 830, Loss: 3.672392725944519, Final Batch Loss: 0.8087977170944214\n",
      "Epoch 831, Loss: 3.5865493416786194, Final Batch Loss: 0.6711254715919495\n",
      "Epoch 832, Loss: 3.5483192205429077, Final Batch Loss: 0.799411952495575\n",
      "Epoch 833, Loss: 3.381412386894226, Final Batch Loss: 0.5777058601379395\n",
      "Epoch 834, Loss: 3.785977602005005, Final Batch Loss: 0.9689537882804871\n",
      "Epoch 835, Loss: 3.6697657704353333, Final Batch Loss: 0.79639732837677\n",
      "Epoch 836, Loss: 3.8335363268852234, Final Batch Loss: 1.226191520690918\n",
      "Epoch 837, Loss: 4.4050604701042175, Final Batch Loss: 1.6177480220794678\n",
      "Epoch 838, Loss: 3.3096867203712463, Final Batch Loss: 0.5338225364685059\n",
      "Epoch 839, Loss: 3.62849098443985, Final Batch Loss: 0.6850485801696777\n",
      "Epoch 840, Loss: 3.7167348861694336, Final Batch Loss: 0.9576897025108337\n",
      "Epoch 841, Loss: 3.617998957633972, Final Batch Loss: 0.7336885333061218\n",
      "Epoch 842, Loss: 3.1151320040225983, Final Batch Loss: 0.23134145140647888\n",
      "Epoch 843, Loss: 3.6509923338890076, Final Batch Loss: 0.699227511882782\n",
      "Epoch 844, Loss: 3.5459030866622925, Final Batch Loss: 0.7146053910255432\n",
      "Epoch 845, Loss: 3.4927937388420105, Final Batch Loss: 0.6880846619606018\n",
      "Epoch 846, Loss: 3.710981547832489, Final Batch Loss: 0.9558767676353455\n",
      "Epoch 847, Loss: 3.5434343218803406, Final Batch Loss: 0.6834498047828674\n",
      "Epoch 848, Loss: 3.6464548110961914, Final Batch Loss: 0.7650968432426453\n",
      "Epoch 849, Loss: 3.8036876916885376, Final Batch Loss: 1.0456629991531372\n",
      "Epoch 850, Loss: 3.410516142845154, Final Batch Loss: 0.5082710981369019\n",
      "Epoch 851, Loss: 3.313909113407135, Final Batch Loss: 0.5326805114746094\n",
      "Epoch 852, Loss: 3.1822106540203094, Final Batch Loss: 0.3801424205303192\n",
      "Epoch 853, Loss: 3.407228112220764, Final Batch Loss: 0.5643795728683472\n",
      "Epoch 854, Loss: 3.146871119737625, Final Batch Loss: 0.34486135840415955\n",
      "Epoch 855, Loss: 3.47093802690506, Final Batch Loss: 0.5637949705123901\n",
      "Epoch 856, Loss: 3.7418702244758606, Final Batch Loss: 0.9770695567131042\n",
      "Epoch 857, Loss: 3.500764012336731, Final Batch Loss: 0.7761587500572205\n",
      "Epoch 858, Loss: 3.7383806109428406, Final Batch Loss: 0.8899539709091187\n",
      "Epoch 859, Loss: 3.126109004020691, Final Batch Loss: 0.40697628259658813\n",
      "Epoch 860, Loss: 3.293658882379532, Final Batch Loss: 0.39919158816337585\n",
      "Epoch 861, Loss: 3.1077882647514343, Final Batch Loss: 0.384690523147583\n",
      "Epoch 862, Loss: 3.132154554128647, Final Batch Loss: 0.3433135449886322\n",
      "Epoch 863, Loss: 3.35854709148407, Final Batch Loss: 0.6388865113258362\n",
      "Epoch 864, Loss: 3.4610012769699097, Final Batch Loss: 0.7367912530899048\n",
      "Epoch 865, Loss: 3.6964027285575867, Final Batch Loss: 1.1603899002075195\n",
      "Epoch 866, Loss: 3.6327815651893616, Final Batch Loss: 0.9039620161056519\n",
      "Epoch 867, Loss: 3.779590904712677, Final Batch Loss: 0.8532324433326721\n",
      "Epoch 868, Loss: 3.2326455414295197, Final Batch Loss: 0.37514886260032654\n",
      "Epoch 869, Loss: 3.192852348089218, Final Batch Loss: 0.3993239104747772\n",
      "Epoch 870, Loss: 3.3060073852539062, Final Batch Loss: 0.5755390524864197\n",
      "Epoch 871, Loss: 3.4125036001205444, Final Batch Loss: 0.8141518831253052\n",
      "Epoch 872, Loss: 3.954730212688446, Final Batch Loss: 1.2476937770843506\n",
      "Epoch 873, Loss: 3.0833163261413574, Final Batch Loss: 0.28750890493392944\n",
      "Epoch 874, Loss: 3.4745052456855774, Final Batch Loss: 0.8032082319259644\n",
      "Epoch 875, Loss: 3.2660233974456787, Final Batch Loss: 0.6039828062057495\n",
      "Epoch 876, Loss: 3.456361472606659, Final Batch Loss: 0.7114046812057495\n",
      "Epoch 877, Loss: 3.3228570222854614, Final Batch Loss: 0.5846648812294006\n",
      "Epoch 878, Loss: 3.793294370174408, Final Batch Loss: 1.1130292415618896\n",
      "Epoch 879, Loss: 3.577539801597595, Final Batch Loss: 0.9028592109680176\n",
      "Epoch 880, Loss: 3.3526150584220886, Final Batch Loss: 0.7039040923118591\n",
      "Epoch 881, Loss: 3.070327967405319, Final Batch Loss: 0.2723430097103119\n",
      "Epoch 882, Loss: 3.3814054131507874, Final Batch Loss: 0.7475677132606506\n",
      "Epoch 883, Loss: 3.445071280002594, Final Batch Loss: 0.6888089179992676\n",
      "Epoch 884, Loss: 3.3170215487480164, Final Batch Loss: 0.5779142379760742\n",
      "Epoch 885, Loss: 3.390587031841278, Final Batch Loss: 0.6824052929878235\n",
      "Epoch 886, Loss: 3.3660399317741394, Final Batch Loss: 0.6345377564430237\n",
      "Epoch 887, Loss: 3.4562745094299316, Final Batch Loss: 0.5817096829414368\n",
      "Epoch 888, Loss: 3.389597535133362, Final Batch Loss: 0.5695505738258362\n",
      "Epoch 889, Loss: 3.3568387627601624, Final Batch Loss: 0.5151572823524475\n",
      "Epoch 890, Loss: 3.512332022190094, Final Batch Loss: 0.7649696469306946\n",
      "Epoch 891, Loss: 3.663720726966858, Final Batch Loss: 0.9407333135604858\n",
      "Epoch 892, Loss: 3.608645737171173, Final Batch Loss: 0.780304491519928\n",
      "Epoch 893, Loss: 3.794197976589203, Final Batch Loss: 0.8723446130752563\n",
      "Epoch 894, Loss: 3.2758615612983704, Final Batch Loss: 0.5993362069129944\n",
      "Epoch 895, Loss: 3.259124755859375, Final Batch Loss: 0.5183906555175781\n",
      "Epoch 896, Loss: 3.076300173997879, Final Batch Loss: 0.45032212138175964\n",
      "Epoch 897, Loss: 4.013072729110718, Final Batch Loss: 1.199528694152832\n",
      "Epoch 898, Loss: 3.1775417625904083, Final Batch Loss: 0.42016664147377014\n",
      "Epoch 899, Loss: 3.765933573246002, Final Batch Loss: 1.0673850774765015\n",
      "Epoch 900, Loss: 3.190441071987152, Final Batch Loss: 0.49005818367004395\n",
      "Epoch 901, Loss: 2.8827260434627533, Final Batch Loss: 0.2551702558994293\n",
      "Epoch 902, Loss: 3.296144187450409, Final Batch Loss: 0.5127620100975037\n",
      "Epoch 903, Loss: 3.3533632159233093, Final Batch Loss: 0.6621251106262207\n",
      "Epoch 904, Loss: 3.442416191101074, Final Batch Loss: 0.8001347184181213\n",
      "Epoch 905, Loss: 3.1515281200408936, Final Batch Loss: 0.5030678510665894\n",
      "Epoch 906, Loss: 3.457709550857544, Final Batch Loss: 0.827275276184082\n",
      "Epoch 907, Loss: 3.493424415588379, Final Batch Loss: 0.8277600407600403\n",
      "Epoch 908, Loss: 3.3432291746139526, Final Batch Loss: 0.6293624043464661\n",
      "Epoch 909, Loss: 3.2634666562080383, Final Batch Loss: 0.5478006601333618\n",
      "Epoch 910, Loss: 3.6361297965049744, Final Batch Loss: 0.8826795816421509\n",
      "Epoch 911, Loss: 3.06155127286911, Final Batch Loss: 0.372300386428833\n",
      "Epoch 912, Loss: 3.3337035179138184, Final Batch Loss: 0.6624898910522461\n",
      "Epoch 913, Loss: 3.5162307024002075, Final Batch Loss: 0.8209454417228699\n",
      "Epoch 914, Loss: 3.352264642715454, Final Batch Loss: 0.6841057538986206\n",
      "Epoch 915, Loss: 3.726485788822174, Final Batch Loss: 1.0229665040969849\n",
      "Epoch 916, Loss: 3.5050952434539795, Final Batch Loss: 0.7680792808532715\n",
      "Epoch 917, Loss: 3.235316038131714, Final Batch Loss: 0.5646660923957825\n",
      "Epoch 918, Loss: 3.2046434581279755, Final Batch Loss: 0.4634036123752594\n",
      "Epoch 919, Loss: 3.4282729029655457, Final Batch Loss: 0.7483116388320923\n",
      "Epoch 920, Loss: 3.0181501507759094, Final Batch Loss: 0.5061551928520203\n",
      "Epoch 921, Loss: 3.1851438879966736, Final Batch Loss: 0.5990009903907776\n",
      "Epoch 922, Loss: 3.2768659591674805, Final Batch Loss: 0.547336995601654\n",
      "Epoch 923, Loss: 3.350973904132843, Final Batch Loss: 0.7318287491798401\n",
      "Epoch 924, Loss: 3.7479183077812195, Final Batch Loss: 0.9699026942253113\n",
      "Epoch 925, Loss: 3.8337791562080383, Final Batch Loss: 1.1183221340179443\n",
      "Epoch 926, Loss: 3.1271907091140747, Final Batch Loss: 0.380645751953125\n",
      "Epoch 927, Loss: 3.019186794757843, Final Batch Loss: 0.40632086992263794\n",
      "Epoch 928, Loss: 3.231470674276352, Final Batch Loss: 0.44575944542884827\n",
      "Epoch 929, Loss: 3.0421432554721832, Final Batch Loss: 0.43036648631095886\n",
      "Epoch 930, Loss: 3.3052802085876465, Final Batch Loss: 0.6261422038078308\n",
      "Epoch 931, Loss: 3.379447638988495, Final Batch Loss: 0.6172650456428528\n",
      "Epoch 932, Loss: 3.5124067664146423, Final Batch Loss: 0.8143308758735657\n",
      "Epoch 933, Loss: 3.43950092792511, Final Batch Loss: 0.7351740002632141\n",
      "Epoch 934, Loss: 3.102820336818695, Final Batch Loss: 0.3997378945350647\n",
      "Epoch 935, Loss: 3.529516339302063, Final Batch Loss: 0.8818022608757019\n",
      "Epoch 936, Loss: 3.566172420978546, Final Batch Loss: 0.9589484930038452\n",
      "Epoch 937, Loss: 3.341764986515045, Final Batch Loss: 0.6806342005729675\n",
      "Epoch 938, Loss: 3.1305810809135437, Final Batch Loss: 0.4222438335418701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 939, Loss: 3.259388506412506, Final Batch Loss: 0.5765279531478882\n",
      "Epoch 940, Loss: 3.1356123089790344, Final Batch Loss: 0.42002278566360474\n",
      "Epoch 941, Loss: 3.56554639339447, Final Batch Loss: 0.7604991793632507\n",
      "Epoch 942, Loss: 3.0706858336925507, Final Batch Loss: 0.45952948927879333\n",
      "Epoch 943, Loss: 3.384318232536316, Final Batch Loss: 0.7845349311828613\n",
      "Epoch 944, Loss: 3.043053448200226, Final Batch Loss: 0.4389037489891052\n",
      "Epoch 945, Loss: 3.191647469997406, Final Batch Loss: 0.44925570487976074\n",
      "Epoch 946, Loss: 3.568716824054718, Final Batch Loss: 0.8492668271064758\n",
      "Epoch 947, Loss: 3.0089460015296936, Final Batch Loss: 0.49438196420669556\n",
      "Epoch 948, Loss: 3.267027497291565, Final Batch Loss: 0.46103304624557495\n",
      "Epoch 949, Loss: 3.4038038849830627, Final Batch Loss: 0.7662758231163025\n",
      "Epoch 950, Loss: 3.089183419942856, Final Batch Loss: 0.46510788798332214\n",
      "Epoch 951, Loss: 3.2716440558433533, Final Batch Loss: 0.5225448608398438\n",
      "Epoch 952, Loss: 3.115342676639557, Final Batch Loss: 0.5807029008865356\n",
      "Epoch 953, Loss: 2.9233568012714386, Final Batch Loss: 0.327191561460495\n",
      "Epoch 954, Loss: 3.4950010180473328, Final Batch Loss: 0.8994467854499817\n",
      "Epoch 955, Loss: 3.3518839478492737, Final Batch Loss: 0.7177047729492188\n",
      "Epoch 956, Loss: 3.1455191373825073, Final Batch Loss: 0.5218566656112671\n",
      "Epoch 957, Loss: 3.0716284811496735, Final Batch Loss: 0.45571598410606384\n",
      "Epoch 958, Loss: 3.756796360015869, Final Batch Loss: 1.1626462936401367\n",
      "Epoch 959, Loss: 3.169574052095413, Final Batch Loss: 0.43864789605140686\n",
      "Epoch 960, Loss: 3.6213444471359253, Final Batch Loss: 0.8313641548156738\n",
      "Epoch 961, Loss: 2.9732680916786194, Final Batch Loss: 0.20626354217529297\n",
      "Epoch 962, Loss: 3.809218466281891, Final Batch Loss: 1.0152932405471802\n",
      "Epoch 963, Loss: 3.1099789440631866, Final Batch Loss: 0.3925943672657013\n",
      "Epoch 964, Loss: 3.0549434423446655, Final Batch Loss: 0.5402345657348633\n",
      "Epoch 965, Loss: 3.0459780395030975, Final Batch Loss: 0.3539140522480011\n",
      "Epoch 966, Loss: 3.4475167393684387, Final Batch Loss: 0.7898586392402649\n",
      "Epoch 967, Loss: 3.0957503616809845, Final Batch Loss: 0.4931294620037079\n",
      "Epoch 968, Loss: 3.5327905416488647, Final Batch Loss: 0.7185856699943542\n",
      "Epoch 969, Loss: 3.3408167362213135, Final Batch Loss: 0.5527864694595337\n",
      "Epoch 970, Loss: 3.3061306476593018, Final Batch Loss: 0.5996743440628052\n",
      "Epoch 971, Loss: 3.840856194496155, Final Batch Loss: 0.9862083196640015\n",
      "Epoch 972, Loss: 2.979616552591324, Final Batch Loss: 0.36853477358818054\n",
      "Epoch 973, Loss: 3.3416402339935303, Final Batch Loss: 0.7648893594741821\n",
      "Epoch 974, Loss: 3.0238058865070343, Final Batch Loss: 0.27523449063301086\n",
      "Epoch 975, Loss: 3.2990365028381348, Final Batch Loss: 0.681212842464447\n",
      "Epoch 976, Loss: 3.627139627933502, Final Batch Loss: 0.8685612678527832\n",
      "Epoch 977, Loss: 3.368955969810486, Final Batch Loss: 0.6198893189430237\n",
      "Epoch 978, Loss: 3.139421820640564, Final Batch Loss: 0.5656728744506836\n",
      "Epoch 979, Loss: 3.668366253376007, Final Batch Loss: 0.9726824760437012\n",
      "Epoch 980, Loss: 3.0420661568641663, Final Batch Loss: 0.5365155935287476\n",
      "Epoch 981, Loss: 3.104406177997589, Final Batch Loss: 0.4495982527732849\n",
      "Epoch 982, Loss: 3.0341443717479706, Final Batch Loss: 0.459959477186203\n",
      "Epoch 983, Loss: 3.022662937641144, Final Batch Loss: 0.5746999979019165\n",
      "Epoch 984, Loss: 3.337781071662903, Final Batch Loss: 0.5762212872505188\n",
      "Epoch 985, Loss: 3.502904534339905, Final Batch Loss: 0.9390980005264282\n",
      "Epoch 986, Loss: 3.518555223941803, Final Batch Loss: 0.9076574444770813\n",
      "Epoch 987, Loss: 3.099599450826645, Final Batch Loss: 0.4519490897655487\n",
      "Epoch 988, Loss: 3.2283855080604553, Final Batch Loss: 0.5156611204147339\n",
      "Epoch 989, Loss: 3.4710379242897034, Final Batch Loss: 0.7715553641319275\n",
      "Epoch 990, Loss: 3.6896846890449524, Final Batch Loss: 1.0138055086135864\n",
      "Epoch 991, Loss: 3.404462993144989, Final Batch Loss: 0.6375356316566467\n",
      "Epoch 992, Loss: 3.30708909034729, Final Batch Loss: 0.6947758793830872\n",
      "Epoch 993, Loss: 3.313820540904999, Final Batch Loss: 0.6775282621383667\n",
      "Epoch 994, Loss: 3.4477163553237915, Final Batch Loss: 0.7829837203025818\n",
      "Epoch 995, Loss: 3.271790862083435, Final Batch Loss: 0.7017377018928528\n",
      "Epoch 996, Loss: 3.1919400691986084, Final Batch Loss: 0.6327667832374573\n",
      "Epoch 997, Loss: 3.102022171020508, Final Batch Loss: 0.58016437292099\n",
      "Epoch 998, Loss: 3.1245410442352295, Final Batch Loss: 0.5498878359794617\n",
      "Epoch 999, Loss: 2.951043516397476, Final Batch Loss: 0.3352835476398468\n",
      "Epoch 1000, Loss: 2.854411393404007, Final Batch Loss: 0.282433420419693\n",
      "Epoch 1001, Loss: 3.3898181915283203, Final Batch Loss: 0.715525209903717\n",
      "Epoch 1002, Loss: 3.16164630651474, Final Batch Loss: 0.556801974773407\n",
      "Epoch 1003, Loss: 3.206410527229309, Final Batch Loss: 0.6486386060714722\n",
      "Epoch 1004, Loss: 3.6439977288246155, Final Batch Loss: 1.0686043500900269\n",
      "Epoch 1005, Loss: 3.3889852166175842, Final Batch Loss: 0.8812030553817749\n",
      "Epoch 1006, Loss: 3.4033936262130737, Final Batch Loss: 0.7540817260742188\n",
      "Epoch 1007, Loss: 3.6964228749275208, Final Batch Loss: 1.140282154083252\n",
      "Epoch 1008, Loss: 3.29002845287323, Final Batch Loss: 0.5673937797546387\n",
      "Epoch 1009, Loss: 3.5068851709365845, Final Batch Loss: 0.6872825622558594\n",
      "Epoch 1010, Loss: 3.1839886605739594, Final Batch Loss: 0.43380942940711975\n",
      "Epoch 1011, Loss: 3.1543683409690857, Final Batch Loss: 0.5003476738929749\n",
      "Epoch 1012, Loss: 3.0210825502872467, Final Batch Loss: 0.39445820450782776\n",
      "Epoch 1013, Loss: 3.750730335712433, Final Batch Loss: 1.225392460823059\n",
      "Epoch 1014, Loss: 3.1061435639858246, Final Batch Loss: 0.4953633248806\n",
      "Epoch 1015, Loss: 3.141377776861191, Final Batch Loss: 0.3322688639163971\n",
      "Epoch 1016, Loss: 3.5056886076927185, Final Batch Loss: 0.9132710695266724\n",
      "Epoch 1017, Loss: 3.47630912065506, Final Batch Loss: 0.7424142956733704\n",
      "Epoch 1018, Loss: 2.839244991540909, Final Batch Loss: 0.37067708373069763\n",
      "Epoch 1019, Loss: 2.990561842918396, Final Batch Loss: 0.4117346405982971\n",
      "Epoch 1020, Loss: 2.768579512834549, Final Batch Loss: 0.2569943368434906\n",
      "Epoch 1021, Loss: 2.8768317997455597, Final Batch Loss: 0.3018796741962433\n",
      "Epoch 1022, Loss: 3.020616739988327, Final Batch Loss: 0.3973805010318756\n",
      "Epoch 1023, Loss: 2.9477216601371765, Final Batch Loss: 0.44716763496398926\n",
      "Epoch 1024, Loss: 2.8899982571601868, Final Batch Loss: 0.366726815700531\n",
      "Epoch 1025, Loss: 3.368560492992401, Final Batch Loss: 0.7503204941749573\n",
      "Epoch 1026, Loss: 2.9354478120803833, Final Batch Loss: 0.4723859429359436\n",
      "Epoch 1027, Loss: 3.4990216493606567, Final Batch Loss: 0.8934877514839172\n",
      "Epoch 1028, Loss: 3.2028440833091736, Final Batch Loss: 0.7239095568656921\n",
      "Epoch 1029, Loss: 3.4071803092956543, Final Batch Loss: 0.8643871545791626\n",
      "Epoch 1030, Loss: 3.126325845718384, Final Batch Loss: 0.6128396987915039\n",
      "Epoch 1031, Loss: 3.1794289350509644, Final Batch Loss: 0.579349935054779\n",
      "Epoch 1032, Loss: 3.3805041909217834, Final Batch Loss: 0.6128674745559692\n",
      "Epoch 1033, Loss: 3.317122220993042, Final Batch Loss: 0.8056045174598694\n",
      "Epoch 1034, Loss: 3.04494047164917, Final Batch Loss: 0.4598718285560608\n",
      "Epoch 1035, Loss: 3.242158889770508, Final Batch Loss: 0.7144978642463684\n",
      "Epoch 1036, Loss: 3.080324113368988, Final Batch Loss: 0.5540657639503479\n",
      "Epoch 1037, Loss: 2.882359802722931, Final Batch Loss: 0.49826523661613464\n",
      "Epoch 1038, Loss: 3.915373146533966, Final Batch Loss: 1.3861806392669678\n",
      "Epoch 1039, Loss: 3.3030771613121033, Final Batch Loss: 0.7496333122253418\n",
      "Epoch 1040, Loss: 3.4273096323013306, Final Batch Loss: 0.8680704832077026\n",
      "Epoch 1041, Loss: 3.574920177459717, Final Batch Loss: 1.1247254610061646\n",
      "Epoch 1042, Loss: 2.9856682121753693, Final Batch Loss: 0.49125936627388\n",
      "Epoch 1043, Loss: 3.2477519512176514, Final Batch Loss: 0.7401608228683472\n",
      "Epoch 1044, Loss: 3.0617286562919617, Final Batch Loss: 0.5001631379127502\n",
      "Epoch 1045, Loss: 3.4060810208320618, Final Batch Loss: 0.7302164435386658\n",
      "Epoch 1046, Loss: 3.191181004047394, Final Batch Loss: 0.6419883370399475\n",
      "Epoch 1047, Loss: 3.290380120277405, Final Batch Loss: 0.6780010461807251\n",
      "Epoch 1048, Loss: 3.2479857802391052, Final Batch Loss: 0.5110834836959839\n",
      "Epoch 1049, Loss: 3.1451340913772583, Final Batch Loss: 0.6874955296516418\n",
      "Epoch 1050, Loss: 2.8782475292682648, Final Batch Loss: 0.3849428594112396\n",
      "Epoch 1051, Loss: 2.6938232332468033, Final Batch Loss: 0.21610207855701447\n",
      "Epoch 1052, Loss: 2.878119647502899, Final Batch Loss: 0.4104733467102051\n",
      "Epoch 1053, Loss: 3.084306240081787, Final Batch Loss: 0.6594310402870178\n",
      "Epoch 1054, Loss: 3.499727725982666, Final Batch Loss: 1.0844000577926636\n",
      "Epoch 1055, Loss: 3.358253538608551, Final Batch Loss: 0.7907059788703918\n",
      "Epoch 1056, Loss: 3.347707509994507, Final Batch Loss: 0.9037812948226929\n",
      "Epoch 1057, Loss: 3.0649994611740112, Final Batch Loss: 0.6112971305847168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1058, Loss: 3.3554699420928955, Final Batch Loss: 0.7781497240066528\n",
      "Epoch 1059, Loss: 2.9218006134033203, Final Batch Loss: 0.30766409635543823\n",
      "Epoch 1060, Loss: 3.244171440601349, Final Batch Loss: 0.6963351368904114\n",
      "Epoch 1061, Loss: 3.48270446062088, Final Batch Loss: 0.9955788850784302\n",
      "Epoch 1062, Loss: 3.705984354019165, Final Batch Loss: 1.0753166675567627\n",
      "Epoch 1063, Loss: 4.293280899524689, Final Batch Loss: 1.5279427766799927\n",
      "Epoch 1064, Loss: 3.148041605949402, Final Batch Loss: 0.599617063999176\n",
      "Epoch 1065, Loss: 3.7669114470481873, Final Batch Loss: 1.1903550624847412\n",
      "Epoch 1066, Loss: 3.1759374737739563, Final Batch Loss: 0.6671451926231384\n",
      "Epoch 1067, Loss: 3.2632071375846863, Final Batch Loss: 0.7778407335281372\n",
      "Epoch 1068, Loss: 3.3435882925987244, Final Batch Loss: 0.6647005081176758\n",
      "Epoch 1069, Loss: 3.2463185787200928, Final Batch Loss: 0.7113857269287109\n",
      "Epoch 1070, Loss: 2.672427013516426, Final Batch Loss: 0.248867467045784\n",
      "Epoch 1071, Loss: 3.070478856563568, Final Batch Loss: 0.5558248162269592\n",
      "Epoch 1072, Loss: 3.2109842896461487, Final Batch Loss: 0.7315746545791626\n",
      "Epoch 1073, Loss: 2.791309177875519, Final Batch Loss: 0.357257604598999\n",
      "Epoch 1074, Loss: 2.9126046001911163, Final Batch Loss: 0.41042569279670715\n",
      "Epoch 1075, Loss: 2.9266916811466217, Final Batch Loss: 0.48650792241096497\n",
      "Epoch 1076, Loss: 2.8468472063541412, Final Batch Loss: 0.4292103946208954\n",
      "Epoch 1077, Loss: 2.7478785663843155, Final Batch Loss: 0.22783763706684113\n",
      "Epoch 1078, Loss: 3.0356680750846863, Final Batch Loss: 0.6485950350761414\n",
      "Epoch 1079, Loss: 3.06582909822464, Final Batch Loss: 0.613771915435791\n",
      "Epoch 1080, Loss: 2.98519703745842, Final Batch Loss: 0.47199007868766785\n",
      "Epoch 1081, Loss: 3.333660066127777, Final Batch Loss: 0.7908605337142944\n",
      "Epoch 1082, Loss: 3.1708576679229736, Final Batch Loss: 0.6458210349082947\n",
      "Epoch 1083, Loss: 3.321134567260742, Final Batch Loss: 0.8507054448127747\n",
      "Epoch 1084, Loss: 2.918804258108139, Final Batch Loss: 0.38135358691215515\n",
      "Epoch 1085, Loss: 3.1781593561172485, Final Batch Loss: 0.6942473649978638\n",
      "Epoch 1086, Loss: 3.051048904657364, Final Batch Loss: 0.4312605559825897\n",
      "Epoch 1087, Loss: 2.9667443335056305, Final Batch Loss: 0.47467127442359924\n",
      "Epoch 1088, Loss: 3.0487931072711945, Final Batch Loss: 0.43540629744529724\n",
      "Epoch 1089, Loss: 3.204854369163513, Final Batch Loss: 0.5826369524002075\n",
      "Epoch 1090, Loss: 3.126997172832489, Final Batch Loss: 0.5030099153518677\n",
      "Epoch 1091, Loss: 3.2047707438468933, Final Batch Loss: 0.6711088418960571\n",
      "Epoch 1092, Loss: 2.9733698666095734, Final Batch Loss: 0.48097196221351624\n",
      "Epoch 1093, Loss: 3.454645335674286, Final Batch Loss: 0.9752371907234192\n",
      "Epoch 1094, Loss: 3.324373185634613, Final Batch Loss: 0.7763366103172302\n",
      "Epoch 1095, Loss: 2.835146188735962, Final Batch Loss: 0.3528648018836975\n",
      "Epoch 1096, Loss: 3.085439622402191, Final Batch Loss: 0.5716219544410706\n",
      "Epoch 1097, Loss: 3.302062749862671, Final Batch Loss: 0.8169733881950378\n",
      "Epoch 1098, Loss: 3.12754887342453, Final Batch Loss: 0.6716653108596802\n",
      "Epoch 1099, Loss: 2.9818312227725983, Final Batch Loss: 0.4014316499233246\n",
      "Epoch 1100, Loss: 2.9552654325962067, Final Batch Loss: 0.3978976309299469\n",
      "Epoch 1101, Loss: 3.223195731639862, Final Batch Loss: 0.8197739720344543\n",
      "Epoch 1102, Loss: 3.2656659483909607, Final Batch Loss: 0.8062401413917542\n",
      "Epoch 1103, Loss: 2.9775134921073914, Final Batch Loss: 0.41078662872314453\n",
      "Epoch 1104, Loss: 3.267787516117096, Final Batch Loss: 0.6780827641487122\n",
      "Epoch 1105, Loss: 2.9865420758724213, Final Batch Loss: 0.33363011479377747\n",
      "Epoch 1106, Loss: 3.198152720928192, Final Batch Loss: 0.753274142742157\n",
      "Epoch 1107, Loss: 3.371467351913452, Final Batch Loss: 0.8764991760253906\n",
      "Epoch 1108, Loss: 3.0317420959472656, Final Batch Loss: 0.5579605102539062\n",
      "Epoch 1109, Loss: 2.704246699810028, Final Batch Loss: 0.3301823139190674\n",
      "Epoch 1110, Loss: 3.273330807685852, Final Batch Loss: 0.779752254486084\n",
      "Epoch 1111, Loss: 3.523767113685608, Final Batch Loss: 1.0683088302612305\n",
      "Epoch 1112, Loss: 3.033574938774109, Final Batch Loss: 0.5794771909713745\n",
      "Epoch 1113, Loss: 2.947619676589966, Final Batch Loss: 0.4517938494682312\n",
      "Epoch 1114, Loss: 3.287971019744873, Final Batch Loss: 0.7906127572059631\n",
      "Epoch 1115, Loss: 2.9047382175922394, Final Batch Loss: 0.42041316628456116\n",
      "Epoch 1116, Loss: 3.2185654044151306, Final Batch Loss: 0.7640523314476013\n",
      "Epoch 1117, Loss: 3.3322978615760803, Final Batch Loss: 0.8012572526931763\n",
      "Epoch 1118, Loss: 2.9254353642463684, Final Batch Loss: 0.41561049222946167\n",
      "Epoch 1119, Loss: 2.968292474746704, Final Batch Loss: 0.5443728566169739\n",
      "Epoch 1120, Loss: 3.0750045776367188, Final Batch Loss: 0.6293386220932007\n",
      "Epoch 1121, Loss: 3.1674510538578033, Final Batch Loss: 0.7897123098373413\n",
      "Epoch 1122, Loss: 2.9166212379932404, Final Batch Loss: 0.4295402467250824\n",
      "Epoch 1123, Loss: 3.2553427815437317, Final Batch Loss: 0.808873176574707\n",
      "Epoch 1124, Loss: 2.8227058053016663, Final Batch Loss: 0.3410746455192566\n",
      "Epoch 1125, Loss: 2.9401695132255554, Final Batch Loss: 0.528404951095581\n",
      "Epoch 1126, Loss: 2.929490625858307, Final Batch Loss: 0.4706955552101135\n",
      "Epoch 1127, Loss: 3.3123884201049805, Final Batch Loss: 0.7370875477790833\n",
      "Epoch 1128, Loss: 3.06413197517395, Final Batch Loss: 0.5472221970558167\n",
      "Epoch 1129, Loss: 2.737501949071884, Final Batch Loss: 0.3490845859050751\n",
      "Epoch 1130, Loss: 3.080700159072876, Final Batch Loss: 0.6576040387153625\n",
      "Epoch 1131, Loss: 3.122584044933319, Final Batch Loss: 0.6639283895492554\n",
      "Epoch 1132, Loss: 2.9822555482387543, Final Batch Loss: 0.4274296462535858\n",
      "Epoch 1133, Loss: 2.8569288849830627, Final Batch Loss: 0.433820903301239\n",
      "Epoch 1134, Loss: 3.0834651589393616, Final Batch Loss: 0.6100447773933411\n",
      "Epoch 1135, Loss: 3.3837220668792725, Final Batch Loss: 0.9061756134033203\n",
      "Epoch 1136, Loss: 3.0501208305358887, Final Batch Loss: 0.8092103600502014\n",
      "Epoch 1137, Loss: 2.8536257445812225, Final Batch Loss: 0.41860488057136536\n",
      "Epoch 1138, Loss: 3.146616578102112, Final Batch Loss: 0.6041436791419983\n",
      "Epoch 1139, Loss: 3.1658347249031067, Final Batch Loss: 0.6623403429985046\n",
      "Epoch 1140, Loss: 3.1398662328720093, Final Batch Loss: 0.8509635925292969\n",
      "Epoch 1141, Loss: 2.97966730594635, Final Batch Loss: 0.5330467820167542\n",
      "Epoch 1142, Loss: 3.3848764896392822, Final Batch Loss: 0.903571605682373\n",
      "Epoch 1143, Loss: 3.11602246761322, Final Batch Loss: 0.6152516007423401\n",
      "Epoch 1144, Loss: 3.1332443356513977, Final Batch Loss: 0.6775928735733032\n",
      "Epoch 1145, Loss: 2.878114551305771, Final Batch Loss: 0.31808438897132874\n",
      "Epoch 1146, Loss: 2.928024083375931, Final Batch Loss: 0.47299817204475403\n",
      "Epoch 1147, Loss: 2.8908397257328033, Final Batch Loss: 0.392648309469223\n",
      "Epoch 1148, Loss: 3.2680269479751587, Final Batch Loss: 0.8169512748718262\n",
      "Epoch 1149, Loss: 3.049244463443756, Final Batch Loss: 0.6541374921798706\n",
      "Epoch 1150, Loss: 2.9834922552108765, Final Batch Loss: 0.5266409516334534\n",
      "Epoch 1151, Loss: 3.2361666560173035, Final Batch Loss: 0.8384730219841003\n",
      "Epoch 1152, Loss: 3.042459189891815, Final Batch Loss: 0.6570162177085876\n",
      "Epoch 1153, Loss: 3.0678958892822266, Final Batch Loss: 0.6215822100639343\n",
      "Epoch 1154, Loss: 2.860040158033371, Final Batch Loss: 0.36813822388648987\n",
      "Epoch 1155, Loss: 3.1562076807022095, Final Batch Loss: 0.749601423740387\n",
      "Epoch 1156, Loss: 2.9471826553344727, Final Batch Loss: 0.6079620122909546\n",
      "Epoch 1157, Loss: 3.4803450107574463, Final Batch Loss: 1.024312973022461\n",
      "Epoch 1158, Loss: 2.949230521917343, Final Batch Loss: 0.39868858456611633\n",
      "Epoch 1159, Loss: 3.2367480993270874, Final Batch Loss: 0.8444861769676208\n",
      "Epoch 1160, Loss: 3.0201725363731384, Final Batch Loss: 0.6189621090888977\n",
      "Epoch 1161, Loss: 2.9059664607048035, Final Batch Loss: 0.4220857620239258\n",
      "Epoch 1162, Loss: 2.726357012987137, Final Batch Loss: 0.3223593533039093\n",
      "Epoch 1163, Loss: 3.0833314657211304, Final Batch Loss: 0.7449949979782104\n",
      "Epoch 1164, Loss: 2.8075811564922333, Final Batch Loss: 0.4138946235179901\n",
      "Epoch 1165, Loss: 2.827735871076584, Final Batch Loss: 0.4634460508823395\n",
      "Epoch 1166, Loss: 3.0656672716140747, Final Batch Loss: 0.6564714312553406\n",
      "Epoch 1167, Loss: 2.82679882645607, Final Batch Loss: 0.4600042402744293\n",
      "Epoch 1168, Loss: 2.5858545154333115, Final Batch Loss: 0.23981992900371552\n",
      "Epoch 1169, Loss: 3.2154206037521362, Final Batch Loss: 0.8325199484825134\n",
      "Epoch 1170, Loss: 2.957962930202484, Final Batch Loss: 0.5009355545043945\n",
      "Epoch 1171, Loss: 3.0407294034957886, Final Batch Loss: 0.7011659741401672\n",
      "Epoch 1172, Loss: 3.0133708715438843, Final Batch Loss: 0.6023656725883484\n",
      "Epoch 1173, Loss: 3.060113310813904, Final Batch Loss: 0.6352010369300842\n",
      "Epoch 1174, Loss: 2.9882927536964417, Final Batch Loss: 0.5740850567817688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1175, Loss: 3.0408410727977753, Final Batch Loss: 0.48298314213752747\n",
      "Epoch 1176, Loss: 3.118131160736084, Final Batch Loss: 0.5724318623542786\n",
      "Epoch 1177, Loss: 3.2731635570526123, Final Batch Loss: 0.8512075543403625\n",
      "Epoch 1178, Loss: 2.8745066225528717, Final Batch Loss: 0.3868066370487213\n",
      "Epoch 1179, Loss: 3.094133138656616, Final Batch Loss: 0.584349513053894\n",
      "Epoch 1180, Loss: 3.0795704126358032, Final Batch Loss: 0.6908672451972961\n",
      "Epoch 1181, Loss: 3.126193642616272, Final Batch Loss: 0.6324173808097839\n",
      "Epoch 1182, Loss: 3.3566176891326904, Final Batch Loss: 0.8873705267906189\n",
      "Epoch 1183, Loss: 2.6276360750198364, Final Batch Loss: 0.30508953332901\n",
      "Epoch 1184, Loss: 3.185011148452759, Final Batch Loss: 0.8414322137832642\n",
      "Epoch 1185, Loss: 3.040553867816925, Final Batch Loss: 0.6715244054794312\n",
      "Epoch 1186, Loss: 2.849962890148163, Final Batch Loss: 0.5089173913002014\n",
      "Epoch 1187, Loss: 2.8617283701896667, Final Batch Loss: 0.41169238090515137\n",
      "Epoch 1188, Loss: 3.0191813111305237, Final Batch Loss: 0.6304613351821899\n",
      "Epoch 1189, Loss: 3.0373148322105408, Final Batch Loss: 0.5418751239776611\n",
      "Epoch 1190, Loss: 2.947033941745758, Final Batch Loss: 0.6009160280227661\n",
      "Epoch 1191, Loss: 2.8784656822681427, Final Batch Loss: 0.3540027439594269\n",
      "Epoch 1192, Loss: 3.0678285360336304, Final Batch Loss: 0.7292633056640625\n",
      "Epoch 1193, Loss: 3.1819756031036377, Final Batch Loss: 0.806704044342041\n",
      "Epoch 1194, Loss: 3.190493106842041, Final Batch Loss: 0.8094989061355591\n",
      "Epoch 1195, Loss: 3.260115385055542, Final Batch Loss: 0.7308129668235779\n",
      "Epoch 1196, Loss: 3.517365574836731, Final Batch Loss: 1.1003338098526\n",
      "Epoch 1197, Loss: 3.0673633217811584, Final Batch Loss: 0.6617181897163391\n",
      "Epoch 1198, Loss: 3.018595725297928, Final Batch Loss: 0.44913801550865173\n",
      "Epoch 1199, Loss: 3.0812244415283203, Final Batch Loss: 0.6178559064865112\n",
      "Epoch 1200, Loss: 3.049992620944977, Final Batch Loss: 0.5365900993347168\n",
      "Epoch 1201, Loss: 3.0997844338417053, Final Batch Loss: 0.5014388561248779\n",
      "Epoch 1202, Loss: 3.2473273277282715, Final Batch Loss: 0.8010140657424927\n",
      "Epoch 1203, Loss: 3.025852143764496, Final Batch Loss: 0.5814805030822754\n",
      "Epoch 1204, Loss: 3.2187471985816956, Final Batch Loss: 0.7771132588386536\n",
      "Epoch 1205, Loss: 2.9118843376636505, Final Batch Loss: 0.4791167080402374\n",
      "Epoch 1206, Loss: 3.0705370903015137, Final Batch Loss: 0.5678528547286987\n",
      "Epoch 1207, Loss: 2.796143054962158, Final Batch Loss: 0.27898210287094116\n",
      "Epoch 1208, Loss: 3.078951895236969, Final Batch Loss: 0.7079182863235474\n",
      "Epoch 1209, Loss: 2.7102989554405212, Final Batch Loss: 0.37495845556259155\n",
      "Epoch 1210, Loss: 3.243986427783966, Final Batch Loss: 0.8557153344154358\n",
      "Epoch 1211, Loss: 2.7780095040798187, Final Batch Loss: 0.3204655349254608\n",
      "Epoch 1212, Loss: 2.748555213212967, Final Batch Loss: 0.3743269145488739\n",
      "Epoch 1213, Loss: 3.618849217891693, Final Batch Loss: 1.2142232656478882\n",
      "Epoch 1214, Loss: 2.9327855706214905, Final Batch Loss: 0.5780120491981506\n",
      "Epoch 1215, Loss: 2.958213299512863, Final Batch Loss: 0.4890548884868622\n",
      "Epoch 1216, Loss: 3.0763341784477234, Final Batch Loss: 0.6585201025009155\n",
      "Epoch 1217, Loss: 3.407090425491333, Final Batch Loss: 0.9785918593406677\n",
      "Epoch 1218, Loss: 2.8769418597221375, Final Batch Loss: 0.5331102013587952\n",
      "Epoch 1219, Loss: 3.2679906487464905, Final Batch Loss: 0.8685376048088074\n",
      "Epoch 1220, Loss: 2.974428951740265, Final Batch Loss: 0.5190072655677795\n",
      "Epoch 1221, Loss: 2.7488473653793335, Final Batch Loss: 0.4521135687828064\n",
      "Epoch 1222, Loss: 2.8309750258922577, Final Batch Loss: 0.4412969648838043\n",
      "Epoch 1223, Loss: 2.7959083318710327, Final Batch Loss: 0.48799800872802734\n",
      "Epoch 1224, Loss: 3.1662012934684753, Final Batch Loss: 0.7482457756996155\n",
      "Epoch 1225, Loss: 2.748370796442032, Final Batch Loss: 0.37512096762657166\n",
      "Epoch 1226, Loss: 2.8361085951328278, Final Batch Loss: 0.4214361608028412\n",
      "Epoch 1227, Loss: 2.7641866207122803, Final Batch Loss: 0.4392077922821045\n",
      "Epoch 1228, Loss: 3.125986635684967, Final Batch Loss: 0.7369276881217957\n",
      "Epoch 1229, Loss: 3.0505807399749756, Final Batch Loss: 0.7202335596084595\n",
      "Epoch 1230, Loss: 3.0953081250190735, Final Batch Loss: 0.6480581164360046\n",
      "Epoch 1231, Loss: 3.0042425096035004, Final Batch Loss: 0.6944623589515686\n",
      "Epoch 1232, Loss: 3.3309749960899353, Final Batch Loss: 0.9303533434867859\n",
      "Epoch 1233, Loss: 3.0419209003448486, Final Batch Loss: 0.722429096698761\n",
      "Epoch 1234, Loss: 3.0505924820899963, Final Batch Loss: 0.5836232900619507\n",
      "Epoch 1235, Loss: 3.1833500266075134, Final Batch Loss: 0.7228822112083435\n",
      "Epoch 1236, Loss: 2.8347886502742767, Final Batch Loss: 0.4077241122722626\n",
      "Epoch 1237, Loss: 2.7246197164058685, Final Batch Loss: 0.36542776226997375\n",
      "Epoch 1238, Loss: 2.634714812040329, Final Batch Loss: 0.3501833975315094\n",
      "Epoch 1239, Loss: 2.8196170926094055, Final Batch Loss: 0.4704703688621521\n",
      "Epoch 1240, Loss: 3.3579012751579285, Final Batch Loss: 0.9901232123374939\n",
      "Epoch 1241, Loss: 2.739176243543625, Final Batch Loss: 0.3514573872089386\n",
      "Epoch 1242, Loss: 2.987468123435974, Final Batch Loss: 0.5987990498542786\n",
      "Epoch 1243, Loss: 3.2609812021255493, Final Batch Loss: 0.9239201545715332\n",
      "Epoch 1244, Loss: 2.6100502610206604, Final Batch Loss: 0.2719506025314331\n",
      "Epoch 1245, Loss: 3.246224582195282, Final Batch Loss: 0.7697173953056335\n",
      "Epoch 1246, Loss: 2.952116698026657, Final Batch Loss: 0.49972209334373474\n",
      "Epoch 1247, Loss: 2.917133331298828, Final Batch Loss: 0.5146751403808594\n",
      "Epoch 1248, Loss: 3.44594806432724, Final Batch Loss: 0.991191565990448\n",
      "Epoch 1249, Loss: 2.7268747091293335, Final Batch Loss: 0.366970956325531\n",
      "Epoch 1250, Loss: 2.6336124539375305, Final Batch Loss: 0.278495728969574\n",
      "Epoch 1251, Loss: 2.656795844435692, Final Batch Loss: 0.2391444891691208\n",
      "Epoch 1252, Loss: 3.14194393157959, Final Batch Loss: 0.8607608675956726\n",
      "Epoch 1253, Loss: 3.3812102675437927, Final Batch Loss: 0.9743776917457581\n",
      "Epoch 1254, Loss: 2.770264893770218, Final Batch Loss: 0.5651217103004456\n",
      "Epoch 1255, Loss: 2.701922297477722, Final Batch Loss: 0.29294222593307495\n",
      "Epoch 1256, Loss: 3.4666170477867126, Final Batch Loss: 0.9897741675376892\n",
      "Epoch 1257, Loss: 2.7679741084575653, Final Batch Loss: 0.42459622025489807\n",
      "Epoch 1258, Loss: 2.823099195957184, Final Batch Loss: 0.5720244646072388\n",
      "Epoch 1259, Loss: 2.864469289779663, Final Batch Loss: 0.4406917095184326\n",
      "Epoch 1260, Loss: 3.008576512336731, Final Batch Loss: 0.6288796067237854\n",
      "Epoch 1261, Loss: 2.7884635031223297, Final Batch Loss: 0.4100188910961151\n",
      "Epoch 1262, Loss: 3.4713292717933655, Final Batch Loss: 1.1759302616119385\n",
      "Epoch 1263, Loss: 2.559818312525749, Final Batch Loss: 0.20816443860530853\n",
      "Epoch 1264, Loss: 3.088530957698822, Final Batch Loss: 0.7449050545692444\n",
      "Epoch 1265, Loss: 2.9105392396450043, Final Batch Loss: 0.4477636516094208\n",
      "Epoch 1266, Loss: 2.928384244441986, Final Batch Loss: 0.5387290120124817\n",
      "Epoch 1267, Loss: 2.7669413685798645, Final Batch Loss: 0.485332190990448\n",
      "Epoch 1268, Loss: 2.840655207633972, Final Batch Loss: 0.5824256539344788\n",
      "Epoch 1269, Loss: 2.6589078307151794, Final Batch Loss: 0.36536723375320435\n",
      "Epoch 1270, Loss: 2.6459651589393616, Final Batch Loss: 0.3935498595237732\n",
      "Epoch 1271, Loss: 2.7872732877731323, Final Batch Loss: 0.3618462085723877\n",
      "Epoch 1272, Loss: 3.3692039251327515, Final Batch Loss: 1.0468636751174927\n",
      "Epoch 1273, Loss: 2.647002637386322, Final Batch Loss: 0.4139264225959778\n",
      "Epoch 1274, Loss: 3.04479843378067, Final Batch Loss: 0.6855255961418152\n",
      "Epoch 1275, Loss: 3.136199116706848, Final Batch Loss: 0.6787191033363342\n",
      "Epoch 1276, Loss: 3.0386542677879333, Final Batch Loss: 0.701221764087677\n",
      "Epoch 1277, Loss: 3.076607823371887, Final Batch Loss: 0.6817318797111511\n",
      "Epoch 1278, Loss: 3.462030053138733, Final Batch Loss: 1.18134605884552\n",
      "Epoch 1279, Loss: 2.699102431535721, Final Batch Loss: 0.37130990624427795\n",
      "Epoch 1280, Loss: 2.65568245947361, Final Batch Loss: 0.19333572685718536\n",
      "Epoch 1281, Loss: 2.748738944530487, Final Batch Loss: 0.4237820506095886\n",
      "Epoch 1282, Loss: 2.7701306641101837, Final Batch Loss: 0.4640100300312042\n",
      "Epoch 1283, Loss: 2.839966505765915, Final Batch Loss: 0.4233153164386749\n",
      "Epoch 1284, Loss: 2.8320948481559753, Final Batch Loss: 0.4373621940612793\n",
      "Epoch 1285, Loss: 2.889204263687134, Final Batch Loss: 0.6097238659858704\n",
      "Epoch 1286, Loss: 2.801537424325943, Final Batch Loss: 0.43274107575416565\n",
      "Epoch 1287, Loss: 2.928538203239441, Final Batch Loss: 0.5210639834403992\n",
      "Epoch 1288, Loss: 2.9933059215545654, Final Batch Loss: 0.6977909803390503\n",
      "Epoch 1289, Loss: 3.2732256650924683, Final Batch Loss: 0.8809171915054321\n",
      "Epoch 1290, Loss: 2.687719911336899, Final Batch Loss: 0.33974626660346985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1291, Loss: 2.838979423046112, Final Batch Loss: 0.5443788170814514\n",
      "Epoch 1292, Loss: 2.9481117725372314, Final Batch Loss: 0.44687461853027344\n",
      "Epoch 1293, Loss: 2.685199499130249, Final Batch Loss: 0.26127737760543823\n",
      "Epoch 1294, Loss: 3.071665346622467, Final Batch Loss: 0.7197021842002869\n",
      "Epoch 1295, Loss: 2.9117239117622375, Final Batch Loss: 0.5455886721611023\n",
      "Epoch 1296, Loss: 3.0747971534729004, Final Batch Loss: 0.7244700193405151\n",
      "Epoch 1297, Loss: 2.9636009335517883, Final Batch Loss: 0.5815581679344177\n",
      "Epoch 1298, Loss: 2.7137370109558105, Final Batch Loss: 0.4088360667228699\n",
      "Epoch 1299, Loss: 2.556688576936722, Final Batch Loss: 0.23285230994224548\n",
      "Epoch 1300, Loss: 2.918007791042328, Final Batch Loss: 0.5562705397605896\n",
      "Epoch 1301, Loss: 3.003235936164856, Final Batch Loss: 0.70620197057724\n",
      "Epoch 1302, Loss: 2.58763924241066, Final Batch Loss: 0.33626440167427063\n",
      "Epoch 1303, Loss: 2.8057830929756165, Final Batch Loss: 0.44692450761795044\n",
      "Epoch 1304, Loss: 3.288245379924774, Final Batch Loss: 0.9183567762374878\n",
      "Epoch 1305, Loss: 2.8392192125320435, Final Batch Loss: 0.5866135954856873\n",
      "Epoch 1306, Loss: 2.926324248313904, Final Batch Loss: 0.5728726387023926\n",
      "Epoch 1307, Loss: 2.617107778787613, Final Batch Loss: 0.38176921010017395\n",
      "Epoch 1308, Loss: 2.7126184701919556, Final Batch Loss: 0.4125043749809265\n",
      "Epoch 1309, Loss: 2.5362278670072556, Final Batch Loss: 0.19637702405452728\n",
      "Epoch 1310, Loss: 2.651593640446663, Final Batch Loss: 0.23155106604099274\n",
      "Epoch 1311, Loss: 2.6991376280784607, Final Batch Loss: 0.3740997910499573\n",
      "Epoch 1312, Loss: 2.8111122250556946, Final Batch Loss: 0.5096777677536011\n",
      "Epoch 1313, Loss: 2.847415119409561, Final Batch Loss: 0.3798340857028961\n",
      "Epoch 1314, Loss: 2.7412026822566986, Final Batch Loss: 0.48703405261039734\n",
      "Epoch 1315, Loss: 3.290246844291687, Final Batch Loss: 0.8354535698890686\n",
      "Epoch 1316, Loss: 2.8980778455734253, Final Batch Loss: 0.6137133240699768\n",
      "Epoch 1317, Loss: 2.9918695092201233, Final Batch Loss: 0.6859503984451294\n",
      "Epoch 1318, Loss: 2.508972242474556, Final Batch Loss: 0.18511410057544708\n",
      "Epoch 1319, Loss: 2.7742632627487183, Final Batch Loss: 0.5062039494514465\n",
      "Epoch 1320, Loss: 3.2829055786132812, Final Batch Loss: 0.950198233127594\n",
      "Epoch 1321, Loss: 2.61194908618927, Final Batch Loss: 0.33692097663879395\n",
      "Epoch 1322, Loss: 2.935510814189911, Final Batch Loss: 0.661485493183136\n",
      "Epoch 1323, Loss: 2.984112471342087, Final Batch Loss: 0.605671763420105\n",
      "Epoch 1324, Loss: 2.718727320432663, Final Batch Loss: 0.4766555726528168\n",
      "Epoch 1325, Loss: 2.8894155025482178, Final Batch Loss: 0.6560686826705933\n",
      "Epoch 1326, Loss: 2.91469544172287, Final Batch Loss: 0.7354010343551636\n",
      "Epoch 1327, Loss: 2.697575718164444, Final Batch Loss: 0.3336222469806671\n",
      "Epoch 1328, Loss: 3.0836456418037415, Final Batch Loss: 0.8023328185081482\n",
      "Epoch 1329, Loss: 3.468243360519409, Final Batch Loss: 1.1846667528152466\n",
      "Epoch 1330, Loss: 3.3764756321907043, Final Batch Loss: 0.9903350472450256\n",
      "Epoch 1331, Loss: 3.1075614392757416, Final Batch Loss: 0.7786750793457031\n",
      "Epoch 1332, Loss: 2.9250664114952087, Final Batch Loss: 0.5767201781272888\n",
      "Epoch 1333, Loss: 2.7899220287799835, Final Batch Loss: 0.3569289743900299\n",
      "Epoch 1334, Loss: 2.9925769567489624, Final Batch Loss: 0.73432457447052\n",
      "Epoch 1335, Loss: 2.5468850433826447, Final Batch Loss: 0.21585199236869812\n",
      "Epoch 1336, Loss: 2.600173443555832, Final Batch Loss: 0.35850831866264343\n",
      "Epoch 1337, Loss: 2.7615006268024445, Final Batch Loss: 0.4178694188594818\n",
      "Epoch 1338, Loss: 2.81685072183609, Final Batch Loss: 0.5207574963569641\n",
      "Epoch 1339, Loss: 3.337648332118988, Final Batch Loss: 1.0644773244857788\n",
      "Epoch 1340, Loss: 2.6462539732456207, Final Batch Loss: 0.44125738739967346\n",
      "Epoch 1341, Loss: 2.8436997532844543, Final Batch Loss: 0.5274338126182556\n",
      "Epoch 1342, Loss: 2.712149977684021, Final Batch Loss: 0.5320442914962769\n",
      "Epoch 1343, Loss: 3.033229887485504, Final Batch Loss: 0.6608617901802063\n",
      "Epoch 1344, Loss: 2.9592966437339783, Final Batch Loss: 0.6597331166267395\n",
      "Epoch 1345, Loss: 3.0533512234687805, Final Batch Loss: 0.698115885257721\n",
      "Epoch 1346, Loss: 2.836871862411499, Final Batch Loss: 0.6157605051994324\n",
      "Epoch 1347, Loss: 2.7427905201911926, Final Batch Loss: 0.49290186166763306\n",
      "Epoch 1348, Loss: 2.904754877090454, Final Batch Loss: 0.6263421773910522\n",
      "Epoch 1349, Loss: 2.6648003458976746, Final Batch Loss: 0.5253363251686096\n",
      "Epoch 1350, Loss: 2.9479724168777466, Final Batch Loss: 0.6325212121009827\n",
      "Epoch 1351, Loss: 3.031730055809021, Final Batch Loss: 0.964178740978241\n",
      "Epoch 1352, Loss: 2.590984970331192, Final Batch Loss: 0.40468141436576843\n",
      "Epoch 1353, Loss: 2.641330122947693, Final Batch Loss: 0.5020845532417297\n",
      "Epoch 1354, Loss: 2.9323970675468445, Final Batch Loss: 0.6117442846298218\n",
      "Epoch 1355, Loss: 2.9492246508598328, Final Batch Loss: 0.6326061487197876\n",
      "Epoch 1356, Loss: 2.9723557233810425, Final Batch Loss: 0.6888637542724609\n",
      "Epoch 1357, Loss: 2.911241441965103, Final Batch Loss: 0.48834410309791565\n",
      "Epoch 1358, Loss: 2.6789292097091675, Final Batch Loss: 0.45221656560897827\n",
      "Epoch 1359, Loss: 2.9316932559013367, Final Batch Loss: 0.550642192363739\n",
      "Epoch 1360, Loss: 2.753847360610962, Final Batch Loss: 0.3398261070251465\n",
      "Epoch 1361, Loss: 2.6617791652679443, Final Batch Loss: 0.36285263299942017\n",
      "Epoch 1362, Loss: 2.6322690546512604, Final Batch Loss: 0.4028721749782562\n",
      "Epoch 1363, Loss: 3.0129890739917755, Final Batch Loss: 0.8060263991355896\n",
      "Epoch 1364, Loss: 3.0457040667533875, Final Batch Loss: 0.7606518864631653\n",
      "Epoch 1365, Loss: 2.474133998155594, Final Batch Loss: 0.23824498057365417\n",
      "Epoch 1366, Loss: 2.9381772577762604, Final Batch Loss: 0.6775304079055786\n",
      "Epoch 1367, Loss: 2.8667103946208954, Final Batch Loss: 0.6781784296035767\n",
      "Epoch 1368, Loss: 2.662790209054947, Final Batch Loss: 0.45340895652770996\n",
      "Epoch 1369, Loss: 2.8488087952136993, Final Batch Loss: 0.5617246627807617\n",
      "Epoch 1370, Loss: 2.5964421033859253, Final Batch Loss: 0.3791239857673645\n",
      "Epoch 1371, Loss: 2.8875925838947296, Final Batch Loss: 0.615003764629364\n",
      "Epoch 1372, Loss: 2.537875324487686, Final Batch Loss: 0.3002837300300598\n",
      "Epoch 1373, Loss: 2.849839359521866, Final Batch Loss: 0.6887313723564148\n",
      "Epoch 1374, Loss: 2.8620855808258057, Final Batch Loss: 0.5269709825515747\n",
      "Epoch 1375, Loss: 3.2448115944862366, Final Batch Loss: 0.8030242919921875\n",
      "Epoch 1376, Loss: 2.8621177971363068, Final Batch Loss: 0.46582481265068054\n",
      "Epoch 1377, Loss: 2.5464974343776703, Final Batch Loss: 0.15064296126365662\n",
      "Epoch 1378, Loss: 3.037444770336151, Final Batch Loss: 0.7134328484535217\n",
      "Epoch 1379, Loss: 2.9940671920776367, Final Batch Loss: 0.7413202524185181\n",
      "Epoch 1380, Loss: 2.7773549258708954, Final Batch Loss: 0.45132043957710266\n",
      "Epoch 1381, Loss: 2.907123625278473, Final Batch Loss: 0.6407362818717957\n",
      "Epoch 1382, Loss: 2.762704372406006, Final Batch Loss: 0.5394843816757202\n",
      "Epoch 1383, Loss: 2.8076299130916595, Final Batch Loss: 0.48929139971733093\n",
      "Epoch 1384, Loss: 3.538446366786957, Final Batch Loss: 1.265587568283081\n",
      "Epoch 1385, Loss: 3.0022518038749695, Final Batch Loss: 0.8061949610710144\n",
      "Epoch 1386, Loss: 2.8952073454856873, Final Batch Loss: 0.6184072494506836\n",
      "Epoch 1387, Loss: 3.249172806739807, Final Batch Loss: 1.0044361352920532\n",
      "Epoch 1388, Loss: 3.0316399931907654, Final Batch Loss: 0.671000599861145\n",
      "Epoch 1389, Loss: 2.7605443000793457, Final Batch Loss: 0.5241428017616272\n",
      "Epoch 1390, Loss: 2.7118581533432007, Final Batch Loss: 0.510002613067627\n",
      "Epoch 1391, Loss: 3.31323179602623, Final Batch Loss: 1.0592339038848877\n",
      "Epoch 1392, Loss: 2.803734302520752, Final Batch Loss: 0.5134822130203247\n",
      "Epoch 1393, Loss: 2.8653255701065063, Final Batch Loss: 0.6452299356460571\n",
      "Epoch 1394, Loss: 2.477753847837448, Final Batch Loss: 0.27778109908103943\n",
      "Epoch 1395, Loss: 2.816698521375656, Final Batch Loss: 0.4853965938091278\n",
      "Epoch 1396, Loss: 2.9995149970054626, Final Batch Loss: 0.6350498795509338\n",
      "Epoch 1397, Loss: 2.647235631942749, Final Batch Loss: 0.38334012031555176\n",
      "Epoch 1398, Loss: 2.679590880870819, Final Batch Loss: 0.5089148283004761\n",
      "Epoch 1399, Loss: 2.919068396091461, Final Batch Loss: 0.7479777932167053\n",
      "Epoch 1400, Loss: 2.8929248452186584, Final Batch Loss: 0.635859489440918\n",
      "Epoch 1401, Loss: 3.138238549232483, Final Batch Loss: 0.9315872192382812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1402, Loss: 3.0736719965934753, Final Batch Loss: 0.6430676579475403\n",
      "Epoch 1403, Loss: 2.823414385318756, Final Batch Loss: 0.6192232966423035\n",
      "Epoch 1404, Loss: 3.081067055463791, Final Batch Loss: 0.7695940732955933\n",
      "Epoch 1405, Loss: 2.692020505666733, Final Batch Loss: 0.3878818452358246\n",
      "Epoch 1406, Loss: 2.7883099019527435, Final Batch Loss: 0.6518108248710632\n",
      "Epoch 1407, Loss: 2.7220002710819244, Final Batch Loss: 0.47800952196121216\n",
      "Epoch 1408, Loss: 3.0795835852622986, Final Batch Loss: 0.8581562638282776\n",
      "Epoch 1409, Loss: 2.8884266018867493, Final Batch Loss: 0.5887490510940552\n",
      "Epoch 1410, Loss: 3.086833119392395, Final Batch Loss: 0.8455743193626404\n",
      "Epoch 1411, Loss: 2.8525902032852173, Final Batch Loss: 0.4707948565483093\n",
      "Epoch 1412, Loss: 3.0764968395233154, Final Batch Loss: 0.7479053735733032\n",
      "Epoch 1413, Loss: 2.937505006790161, Final Batch Loss: 0.5160831809043884\n",
      "Epoch 1414, Loss: 2.830162227153778, Final Batch Loss: 0.6449403166770935\n",
      "Epoch 1415, Loss: 3.024041026830673, Final Batch Loss: 0.7426401972770691\n",
      "Epoch 1416, Loss: 2.740547865629196, Final Batch Loss: 0.471556693315506\n",
      "Epoch 1417, Loss: 2.968609035015106, Final Batch Loss: 0.6617465615272522\n",
      "Epoch 1418, Loss: 2.7139166593551636, Final Batch Loss: 0.38095933198928833\n",
      "Epoch 1419, Loss: 2.5679915845394135, Final Batch Loss: 0.34792208671569824\n",
      "Epoch 1420, Loss: 2.4462817013263702, Final Batch Loss: 0.23535314202308655\n",
      "Epoch 1421, Loss: 3.0837917625904083, Final Batch Loss: 0.7753028869628906\n",
      "Epoch 1422, Loss: 2.6097690165042877, Final Batch Loss: 0.33933672308921814\n",
      "Epoch 1423, Loss: 2.632145792245865, Final Batch Loss: 0.4107530415058136\n",
      "Epoch 1424, Loss: 2.7681366205215454, Final Batch Loss: 0.5345364212989807\n",
      "Epoch 1425, Loss: 2.8159865140914917, Final Batch Loss: 0.5115389227867126\n",
      "Epoch 1426, Loss: 2.6454511284828186, Final Batch Loss: 0.37611857056617737\n",
      "Epoch 1427, Loss: 2.8843774795532227, Final Batch Loss: 0.6355222463607788\n",
      "Epoch 1428, Loss: 2.7488556504249573, Final Batch Loss: 0.45766109228134155\n",
      "Epoch 1429, Loss: 2.7718995809555054, Final Batch Loss: 0.48364710807800293\n",
      "Epoch 1430, Loss: 2.6671173870563507, Final Batch Loss: 0.47303804755210876\n",
      "Epoch 1431, Loss: 2.9007511138916016, Final Batch Loss: 0.7046422958374023\n",
      "Epoch 1432, Loss: 2.5073298513889313, Final Batch Loss: 0.29265472292900085\n",
      "Epoch 1433, Loss: 2.7471253275871277, Final Batch Loss: 0.4985649883747101\n",
      "Epoch 1434, Loss: 2.932056725025177, Final Batch Loss: 0.570936381816864\n",
      "Epoch 1435, Loss: 2.434848368167877, Final Batch Loss: 0.38078975677490234\n",
      "Epoch 1436, Loss: 2.4395653754472733, Final Batch Loss: 0.24277599155902863\n",
      "Epoch 1437, Loss: 3.0686373710632324, Final Batch Loss: 0.8947678208351135\n",
      "Epoch 1438, Loss: 2.84078648686409, Final Batch Loss: 0.6564309000968933\n",
      "Epoch 1439, Loss: 2.5581122636795044, Final Batch Loss: 0.38971108198165894\n",
      "Epoch 1440, Loss: 2.8901526927948, Final Batch Loss: 0.7066727876663208\n",
      "Epoch 1441, Loss: 2.8981502652168274, Final Batch Loss: 0.6550674438476562\n",
      "Epoch 1442, Loss: 2.894400954246521, Final Batch Loss: 0.780571460723877\n",
      "Epoch 1443, Loss: 3.090581387281418, Final Batch Loss: 0.9444130659103394\n",
      "Epoch 1444, Loss: 2.7413649559020996, Final Batch Loss: 0.5043566823005676\n",
      "Epoch 1445, Loss: 2.724283516407013, Final Batch Loss: 0.6308560967445374\n",
      "Epoch 1446, Loss: 2.817759692668915, Final Batch Loss: 0.5987705588340759\n",
      "Epoch 1447, Loss: 2.8002256751060486, Final Batch Loss: 0.5643789768218994\n",
      "Epoch 1448, Loss: 2.9923240542411804, Final Batch Loss: 0.849391758441925\n",
      "Epoch 1449, Loss: 2.6623091399669647, Final Batch Loss: 0.5425673723220825\n",
      "Epoch 1450, Loss: 2.7228354811668396, Final Batch Loss: 0.50123131275177\n",
      "Epoch 1451, Loss: 2.651847928762436, Final Batch Loss: 0.4109887480735779\n",
      "Epoch 1452, Loss: 2.773653984069824, Final Batch Loss: 0.618370532989502\n",
      "Epoch 1453, Loss: 2.9451757669448853, Final Batch Loss: 0.6173697113990784\n",
      "Epoch 1454, Loss: 2.9314498901367188, Final Batch Loss: 0.5829788446426392\n",
      "Epoch 1455, Loss: 3.406422793865204, Final Batch Loss: 1.014750361442566\n",
      "Epoch 1456, Loss: 3.3803653717041016, Final Batch Loss: 0.8556014895439148\n",
      "Epoch 1457, Loss: 2.8717653155326843, Final Batch Loss: 0.588337242603302\n",
      "Epoch 1458, Loss: 2.965126872062683, Final Batch Loss: 0.6406143307685852\n",
      "Epoch 1459, Loss: 2.413351222872734, Final Batch Loss: 0.1775168627500534\n",
      "Epoch 1460, Loss: 2.6843141317367554, Final Batch Loss: 0.38469213247299194\n",
      "Epoch 1461, Loss: 2.9240695536136627, Final Batch Loss: 0.6994053721427917\n",
      "Epoch 1462, Loss: 2.7433774769306183, Final Batch Loss: 0.4948066174983978\n",
      "Epoch 1463, Loss: 2.766530394554138, Final Batch Loss: 0.4407009482383728\n",
      "Epoch 1464, Loss: 2.935375928878784, Final Batch Loss: 0.6613589525222778\n",
      "Epoch 1465, Loss: 2.9348810613155365, Final Batch Loss: 0.789043128490448\n",
      "Epoch 1466, Loss: 2.521558105945587, Final Batch Loss: 0.33084675669670105\n",
      "Epoch 1467, Loss: 2.903332829475403, Final Batch Loss: 0.5796229243278503\n",
      "Epoch 1468, Loss: 2.446657583117485, Final Batch Loss: 0.17101572453975677\n",
      "Epoch 1469, Loss: 2.5683561265468597, Final Batch Loss: 0.2927697002887726\n",
      "Epoch 1470, Loss: 2.6377772390842438, Final Batch Loss: 0.47055622935295105\n",
      "Epoch 1471, Loss: 2.519856244325638, Final Batch Loss: 0.2900511920452118\n",
      "Epoch 1472, Loss: 2.6926363110542297, Final Batch Loss: 0.6506659388542175\n",
      "Epoch 1473, Loss: 2.965317189693451, Final Batch Loss: 0.8031484484672546\n",
      "Epoch 1474, Loss: 2.63253515958786, Final Batch Loss: 0.4983782172203064\n",
      "Epoch 1475, Loss: 2.663112759590149, Final Batch Loss: 0.47293204069137573\n",
      "Epoch 1476, Loss: 2.7882614731788635, Final Batch Loss: 0.5406777262687683\n",
      "Epoch 1477, Loss: 2.5113678574562073, Final Batch Loss: 0.3304196000099182\n",
      "Epoch 1478, Loss: 2.7440383434295654, Final Batch Loss: 0.5365342497825623\n",
      "Epoch 1479, Loss: 2.4759591817855835, Final Batch Loss: 0.32393258810043335\n",
      "Epoch 1480, Loss: 2.4455105364322662, Final Batch Loss: 0.29121437668800354\n",
      "Epoch 1481, Loss: 2.6605490148067474, Final Batch Loss: 0.6365739107131958\n",
      "Epoch 1482, Loss: 2.3958420753479004, Final Batch Loss: 0.20335319638252258\n",
      "Epoch 1483, Loss: 2.4663141667842865, Final Batch Loss: 0.27942582964897156\n",
      "Epoch 1484, Loss: 2.846315950155258, Final Batch Loss: 0.739854633808136\n",
      "Epoch 1485, Loss: 2.761963337659836, Final Batch Loss: 0.5997762084007263\n",
      "Epoch 1486, Loss: 2.901691108942032, Final Batch Loss: 0.7968467473983765\n",
      "Epoch 1487, Loss: 2.7272789478302, Final Batch Loss: 0.5547260642051697\n",
      "Epoch 1488, Loss: 2.690182536840439, Final Batch Loss: 0.4996049106121063\n",
      "Epoch 1489, Loss: 2.607026070356369, Final Batch Loss: 0.39102426171302795\n",
      "Epoch 1490, Loss: 3.1147877275943756, Final Batch Loss: 0.9259033203125\n",
      "Epoch 1491, Loss: 2.6593873500823975, Final Batch Loss: 0.5316401720046997\n",
      "Epoch 1492, Loss: 2.807472199201584, Final Batch Loss: 0.47316256165504456\n",
      "Epoch 1493, Loss: 2.5481037199497223, Final Batch Loss: 0.35927072167396545\n",
      "Epoch 1494, Loss: 2.8934773206710815, Final Batch Loss: 0.6475085020065308\n",
      "Epoch 1495, Loss: 2.7412424087524414, Final Batch Loss: 0.5634540319442749\n",
      "Epoch 1496, Loss: 2.5579716861248016, Final Batch Loss: 0.39176592230796814\n",
      "Epoch 1497, Loss: 3.122579336166382, Final Batch Loss: 0.8023437857627869\n",
      "Epoch 1498, Loss: 2.4766299426555634, Final Batch Loss: 0.3145776689052582\n",
      "Epoch 1499, Loss: 2.7339419424533844, Final Batch Loss: 0.3803670108318329\n",
      "Epoch 1500, Loss: 3.2482443153858185, Final Batch Loss: 0.9997302293777466\n",
      "Epoch 1501, Loss: 2.8065016865730286, Final Batch Loss: 0.6063014268875122\n",
      "Epoch 1502, Loss: 2.630563199520111, Final Batch Loss: 0.42924588918685913\n",
      "Epoch 1503, Loss: 2.6225574910640717, Final Batch Loss: 0.47180142998695374\n",
      "Epoch 1504, Loss: 2.638171523809433, Final Batch Loss: 0.3575020432472229\n",
      "Epoch 1505, Loss: 2.608540713787079, Final Batch Loss: 0.5585659742355347\n",
      "Epoch 1506, Loss: 2.5931206941604614, Final Batch Loss: 0.34130358695983887\n",
      "Epoch 1507, Loss: 2.833348035812378, Final Batch Loss: 0.6602402329444885\n",
      "Epoch 1508, Loss: 2.333777755498886, Final Batch Loss: 0.3012903332710266\n",
      "Epoch 1509, Loss: 2.877696841955185, Final Batch Loss: 0.6991905570030212\n",
      "Epoch 1510, Loss: 2.750421106815338, Final Batch Loss: 0.5127456188201904\n",
      "Epoch 1511, Loss: 2.7235617637634277, Final Batch Loss: 0.5701766014099121\n",
      "Epoch 1512, Loss: 2.7438145577907562, Final Batch Loss: 0.5522238612174988\n",
      "Epoch 1513, Loss: 2.823366105556488, Final Batch Loss: 0.5993403792381287\n",
      "Epoch 1514, Loss: 2.6102151572704315, Final Batch Loss: 0.4960165321826935\n",
      "Epoch 1515, Loss: 3.1634532809257507, Final Batch Loss: 0.7837806940078735\n",
      "Epoch 1516, Loss: 3.022866427898407, Final Batch Loss: 0.7245541214942932\n",
      "Epoch 1517, Loss: 2.870751738548279, Final Batch Loss: 0.6368900537490845\n",
      "Epoch 1518, Loss: 2.5746414065361023, Final Batch Loss: 0.4166431128978729\n",
      "Epoch 1519, Loss: 2.619734048843384, Final Batch Loss: 0.28647691011428833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1520, Loss: 2.6319801211357117, Final Batch Loss: 0.3504268527030945\n",
      "Epoch 1521, Loss: 2.616368889808655, Final Batch Loss: 0.48717832565307617\n",
      "Epoch 1522, Loss: 2.7533660531044006, Final Batch Loss: 0.6097729802131653\n",
      "Epoch 1523, Loss: 2.5741963386535645, Final Batch Loss: 0.5322979688644409\n",
      "Epoch 1524, Loss: 2.655592679977417, Final Batch Loss: 0.5299104452133179\n",
      "Epoch 1525, Loss: 2.8292156457901, Final Batch Loss: 0.6842842698097229\n",
      "Epoch 1526, Loss: 2.65026319026947, Final Batch Loss: 0.5193026065826416\n",
      "Epoch 1527, Loss: 3.269419312477112, Final Batch Loss: 1.0144041776657104\n",
      "Epoch 1528, Loss: 2.708470195531845, Final Batch Loss: 0.4466532766819\n",
      "Epoch 1529, Loss: 2.773940622806549, Final Batch Loss: 0.6093066930770874\n",
      "Epoch 1530, Loss: 2.8532300889492035, Final Batch Loss: 0.6819776296615601\n",
      "Epoch 1531, Loss: 2.6404963433742523, Final Batch Loss: 0.41756585240364075\n",
      "Epoch 1532, Loss: 2.8827912509441376, Final Batch Loss: 0.7668713927268982\n",
      "Epoch 1533, Loss: 2.8868232667446136, Final Batch Loss: 0.7287936806678772\n",
      "Epoch 1534, Loss: 2.826146721839905, Final Batch Loss: 0.6010022163391113\n",
      "Epoch 1535, Loss: 2.4794597923755646, Final Batch Loss: 0.3972642421722412\n",
      "Epoch 1536, Loss: 2.402984321117401, Final Batch Loss: 0.28320634365081787\n",
      "Epoch 1537, Loss: 2.5974671840667725, Final Batch Loss: 0.4135834574699402\n",
      "Epoch 1538, Loss: 2.618757903575897, Final Batch Loss: 0.5277172327041626\n",
      "Epoch 1539, Loss: 2.608146160840988, Final Batch Loss: 0.5948674082756042\n",
      "Epoch 1540, Loss: 2.663209766149521, Final Batch Loss: 0.5086371898651123\n",
      "Epoch 1541, Loss: 2.636579245328903, Final Batch Loss: 0.6526155471801758\n",
      "Epoch 1542, Loss: 2.742802858352661, Final Batch Loss: 0.520736813545227\n",
      "Epoch 1543, Loss: 3.017247796058655, Final Batch Loss: 0.6957834959030151\n",
      "Epoch 1544, Loss: 2.547849327325821, Final Batch Loss: 0.33110347390174866\n",
      "Epoch 1545, Loss: 2.4916444420814514, Final Batch Loss: 0.33834800124168396\n",
      "Epoch 1546, Loss: 2.8551124036312103, Final Batch Loss: 0.6892279982566833\n",
      "Epoch 1547, Loss: 2.3342991173267365, Final Batch Loss: 0.2703203856945038\n",
      "Epoch 1548, Loss: 2.3273865580558777, Final Batch Loss: 0.27884441614151\n",
      "Epoch 1549, Loss: 2.5835903584957123, Final Batch Loss: 0.4722099006175995\n",
      "Epoch 1550, Loss: 2.6068720519542694, Final Batch Loss: 0.41390520334243774\n",
      "Epoch 1551, Loss: 2.736351490020752, Final Batch Loss: 0.524333119392395\n",
      "Epoch 1552, Loss: 2.4589178562164307, Final Batch Loss: 0.30158907175064087\n",
      "Epoch 1553, Loss: 2.557534843683243, Final Batch Loss: 0.386644184589386\n",
      "Epoch 1554, Loss: 2.6004592180252075, Final Batch Loss: 0.5693300366401672\n",
      "Epoch 1555, Loss: 2.635661542415619, Final Batch Loss: 0.5244102478027344\n",
      "Epoch 1556, Loss: 2.6185251772403717, Final Batch Loss: 0.5069335699081421\n",
      "Epoch 1557, Loss: 2.955667018890381, Final Batch Loss: 0.6178354620933533\n",
      "Epoch 1558, Loss: 2.564219683408737, Final Batch Loss: 0.4226072430610657\n",
      "Epoch 1559, Loss: 2.4589403867721558, Final Batch Loss: 0.35217124223709106\n",
      "Epoch 1560, Loss: 2.5716701447963715, Final Batch Loss: 0.44140908122062683\n",
      "Epoch 1561, Loss: 2.588791072368622, Final Batch Loss: 0.5105264782905579\n",
      "Epoch 1562, Loss: 2.6421682238578796, Final Batch Loss: 0.5480021834373474\n",
      "Epoch 1563, Loss: 2.504795551300049, Final Batch Loss: 0.40548473596572876\n",
      "Epoch 1564, Loss: 2.209752008318901, Final Batch Loss: 0.18551190197467804\n",
      "Epoch 1565, Loss: 2.8035408556461334, Final Batch Loss: 0.6156595945358276\n",
      "Epoch 1566, Loss: 3.073076069355011, Final Batch Loss: 1.0059573650360107\n",
      "Epoch 1567, Loss: 2.459109753370285, Final Batch Loss: 0.27120140194892883\n",
      "Epoch 1568, Loss: 2.6749847531318665, Final Batch Loss: 0.5654957890510559\n",
      "Epoch 1569, Loss: 3.1354638040065765, Final Batch Loss: 1.025470495223999\n",
      "Epoch 1570, Loss: 2.565481036901474, Final Batch Loss: 0.3755391538143158\n",
      "Epoch 1571, Loss: 2.359094262123108, Final Batch Loss: 0.30888280272483826\n",
      "Epoch 1572, Loss: 2.517094165086746, Final Batch Loss: 0.3581596910953522\n",
      "Epoch 1573, Loss: 2.4936854243278503, Final Batch Loss: 0.2857239842414856\n",
      "Epoch 1574, Loss: 2.8560671210289, Final Batch Loss: 0.6426882147789001\n",
      "Epoch 1575, Loss: 2.774956226348877, Final Batch Loss: 0.6973867416381836\n",
      "Epoch 1576, Loss: 2.8769382536411285, Final Batch Loss: 0.7729366421699524\n",
      "Epoch 1577, Loss: 2.7387243509292603, Final Batch Loss: 0.5444694757461548\n",
      "Epoch 1578, Loss: 2.299011841416359, Final Batch Loss: 0.2410915046930313\n",
      "Epoch 1579, Loss: 2.692203998565674, Final Batch Loss: 0.4327654242515564\n",
      "Epoch 1580, Loss: 2.9111183881759644, Final Batch Loss: 0.725993275642395\n",
      "Epoch 1581, Loss: 2.31963512301445, Final Batch Loss: 0.1549663245677948\n",
      "Epoch 1582, Loss: 2.7140614986419678, Final Batch Loss: 0.6396073698997498\n",
      "Epoch 1583, Loss: 2.5020067393779755, Final Batch Loss: 0.3833235204219818\n",
      "Epoch 1584, Loss: 2.7065875828266144, Final Batch Loss: 0.5353057980537415\n",
      "Epoch 1585, Loss: 2.81684273481369, Final Batch Loss: 0.5709487795829773\n",
      "Epoch 1586, Loss: 3.3491452634334564, Final Batch Loss: 1.150144338607788\n",
      "Epoch 1587, Loss: 2.689941018819809, Final Batch Loss: 0.459413081407547\n",
      "Epoch 1588, Loss: 2.4588687121868134, Final Batch Loss: 0.27459535002708435\n",
      "Epoch 1589, Loss: 2.8504403829574585, Final Batch Loss: 0.7059588432312012\n",
      "Epoch 1590, Loss: 2.795889675617218, Final Batch Loss: 0.6549628973007202\n",
      "Epoch 1591, Loss: 2.3806354701519012, Final Batch Loss: 0.1954040825366974\n",
      "Epoch 1592, Loss: 2.631844848394394, Final Batch Loss: 0.5803641080856323\n",
      "Epoch 1593, Loss: 2.591564178466797, Final Batch Loss: 0.5483784079551697\n",
      "Epoch 1594, Loss: 2.575453907251358, Final Batch Loss: 0.36041250824928284\n",
      "Epoch 1595, Loss: 2.4284340739250183, Final Batch Loss: 0.374866247177124\n",
      "Epoch 1596, Loss: 2.8143051266670227, Final Batch Loss: 0.6791497468948364\n",
      "Epoch 1597, Loss: 2.4774840772151947, Final Batch Loss: 0.5234891772270203\n",
      "Epoch 1598, Loss: 2.3746430575847626, Final Batch Loss: 0.26544153690338135\n",
      "Epoch 1599, Loss: 2.481140047311783, Final Batch Loss: 0.41257885098457336\n",
      "Epoch 1600, Loss: 2.7957835495471954, Final Batch Loss: 0.6816188097000122\n",
      "Epoch 1601, Loss: 2.8254358172416687, Final Batch Loss: 0.7147073745727539\n",
      "Epoch 1602, Loss: 2.3772748708724976, Final Batch Loss: 0.39789071679115295\n",
      "Epoch 1603, Loss: 2.6929188668727875, Final Batch Loss: 0.6450809836387634\n",
      "Epoch 1604, Loss: 2.6658482253551483, Final Batch Loss: 0.5135783553123474\n",
      "Epoch 1605, Loss: 2.976334661245346, Final Batch Loss: 0.6846678853034973\n",
      "Epoch 1606, Loss: 2.6582001745700836, Final Batch Loss: 0.5320044159889221\n",
      "Epoch 1607, Loss: 2.2964118272066116, Final Batch Loss: 0.2065534144639969\n",
      "Epoch 1608, Loss: 2.696137696504593, Final Batch Loss: 0.5676530599594116\n",
      "Epoch 1609, Loss: 2.735080599784851, Final Batch Loss: 0.7463569045066833\n",
      "Epoch 1610, Loss: 2.7079963088035583, Final Batch Loss: 0.6277902722358704\n",
      "Epoch 1611, Loss: 2.8598251342773438, Final Batch Loss: 0.6449963450431824\n",
      "Epoch 1612, Loss: 2.5786416232585907, Final Batch Loss: 0.4267861545085907\n",
      "Epoch 1613, Loss: 2.1656011193990707, Final Batch Loss: 0.06435205042362213\n",
      "Epoch 1614, Loss: 3.0456225275993347, Final Batch Loss: 0.9835500121116638\n",
      "Epoch 1615, Loss: 2.578011602163315, Final Batch Loss: 0.4714203476905823\n",
      "Epoch 1616, Loss: 2.889917403459549, Final Batch Loss: 0.6808091998100281\n",
      "Epoch 1617, Loss: 2.60060715675354, Final Batch Loss: 0.4503810703754425\n",
      "Epoch 1618, Loss: 2.7870171666145325, Final Batch Loss: 0.6127004623413086\n",
      "Epoch 1619, Loss: 2.4186221063137054, Final Batch Loss: 0.26297566294670105\n",
      "Epoch 1620, Loss: 2.569398194551468, Final Batch Loss: 0.43900081515312195\n",
      "Epoch 1621, Loss: 2.839457631111145, Final Batch Loss: 0.6545451879501343\n",
      "Epoch 1622, Loss: 2.6883324086666107, Final Batch Loss: 0.48803308606147766\n",
      "Epoch 1623, Loss: 2.3618029803037643, Final Batch Loss: 0.18018583953380585\n",
      "Epoch 1624, Loss: 2.688141644001007, Final Batch Loss: 0.5297194123268127\n",
      "Epoch 1625, Loss: 2.5497161149978638, Final Batch Loss: 0.29435962438583374\n",
      "Epoch 1626, Loss: 2.6082444190979004, Final Batch Loss: 0.3743991255760193\n",
      "Epoch 1627, Loss: 2.7330983877182007, Final Batch Loss: 0.5631735920906067\n",
      "Epoch 1628, Loss: 2.895214945077896, Final Batch Loss: 0.9062356948852539\n",
      "Epoch 1629, Loss: 2.531051903963089, Final Batch Loss: 0.45258408784866333\n",
      "Epoch 1630, Loss: 2.7129116356372833, Final Batch Loss: 0.6330409049987793\n",
      "Epoch 1631, Loss: 2.8131925761699677, Final Batch Loss: 0.8230863809585571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1632, Loss: 2.6143722534179688, Final Batch Loss: 0.44023609161376953\n",
      "Epoch 1633, Loss: 3.0330065190792084, Final Batch Loss: 0.9331197142601013\n",
      "Epoch 1634, Loss: 2.458304911851883, Final Batch Loss: 0.3748684525489807\n",
      "Epoch 1635, Loss: 2.5916802287101746, Final Batch Loss: 0.5920724272727966\n",
      "Epoch 1636, Loss: 2.362042710185051, Final Batch Loss: 0.1908452957868576\n",
      "Epoch 1637, Loss: 2.545074611902237, Final Batch Loss: 0.3214717209339142\n",
      "Epoch 1638, Loss: 2.54127037525177, Final Batch Loss: 0.4676010310649872\n",
      "Epoch 1639, Loss: 2.448206663131714, Final Batch Loss: 0.30166491866111755\n",
      "Epoch 1640, Loss: 2.3863717019557953, Final Batch Loss: 0.3761458396911621\n",
      "Epoch 1641, Loss: 2.6087907254695892, Final Batch Loss: 0.5448734164237976\n",
      "Epoch 1642, Loss: 2.5116266012191772, Final Batch Loss: 0.4393605589866638\n",
      "Epoch 1643, Loss: 2.455647110939026, Final Batch Loss: 0.3038606643676758\n",
      "Epoch 1644, Loss: 2.38913294672966, Final Batch Loss: 0.3521537780761719\n",
      "Epoch 1645, Loss: 2.568147659301758, Final Batch Loss: 0.5153967142105103\n",
      "Epoch 1646, Loss: 2.6246855556964874, Final Batch Loss: 0.4292323589324951\n",
      "Epoch 1647, Loss: 2.465845912694931, Final Batch Loss: 0.37141385674476624\n",
      "Epoch 1648, Loss: 3.0547187328338623, Final Batch Loss: 0.9987831711769104\n",
      "Epoch 1649, Loss: 2.5005584359169006, Final Batch Loss: 0.48482462763786316\n",
      "Epoch 1650, Loss: 2.517400026321411, Final Batch Loss: 0.36759692430496216\n",
      "Epoch 1651, Loss: 2.6748819053173065, Final Batch Loss: 0.6811426877975464\n",
      "Epoch 1652, Loss: 2.835967779159546, Final Batch Loss: 0.8264773488044739\n",
      "Epoch 1653, Loss: 2.5999666452407837, Final Batch Loss: 0.5142656564712524\n",
      "Epoch 1654, Loss: 2.8266618251800537, Final Batch Loss: 0.7016380429267883\n",
      "Epoch 1655, Loss: 2.6429216265678406, Final Batch Loss: 0.4133095145225525\n",
      "Epoch 1656, Loss: 2.392787754535675, Final Batch Loss: 0.305254727602005\n",
      "Epoch 1657, Loss: 2.8343074917793274, Final Batch Loss: 0.6195060014724731\n",
      "Epoch 1658, Loss: 2.423297107219696, Final Batch Loss: 0.3399595618247986\n",
      "Epoch 1659, Loss: 2.803227514028549, Final Batch Loss: 0.7597893476486206\n",
      "Epoch 1660, Loss: 2.4865705370903015, Final Batch Loss: 0.43849459290504456\n",
      "Epoch 1661, Loss: 2.5183179080486298, Final Batch Loss: 0.42177215218544006\n",
      "Epoch 1662, Loss: 2.669919401407242, Final Batch Loss: 0.6063204407691956\n",
      "Epoch 1663, Loss: 2.378253996372223, Final Batch Loss: 0.3536888062953949\n",
      "Epoch 1664, Loss: 2.8211807906627655, Final Batch Loss: 0.5846408009529114\n",
      "Epoch 1665, Loss: 2.4229572415351868, Final Batch Loss: 0.31485071778297424\n",
      "Epoch 1666, Loss: 2.356722891330719, Final Batch Loss: 0.2740236520767212\n",
      "Epoch 1667, Loss: 2.7263529896736145, Final Batch Loss: 0.61203533411026\n",
      "Epoch 1668, Loss: 2.622797578573227, Final Batch Loss: 0.5595353245735168\n",
      "Epoch 1669, Loss: 2.5753866136074066, Final Batch Loss: 0.447988897562027\n",
      "Epoch 1670, Loss: 2.702989786863327, Final Batch Loss: 0.7006956934928894\n",
      "Epoch 1671, Loss: 2.757334589958191, Final Batch Loss: 0.6190714836120605\n",
      "Epoch 1672, Loss: 2.8036718666553497, Final Batch Loss: 0.712897002696991\n",
      "Epoch 1673, Loss: 2.459014743566513, Final Batch Loss: 0.391375333070755\n",
      "Epoch 1674, Loss: 2.6845558285713196, Final Batch Loss: 0.6251856684684753\n",
      "Epoch 1675, Loss: 2.324749678373337, Final Batch Loss: 0.2659966051578522\n",
      "Epoch 1676, Loss: 2.3129172027111053, Final Batch Loss: 0.2873220443725586\n",
      "Epoch 1677, Loss: 2.7639189064502716, Final Batch Loss: 0.7276401519775391\n",
      "Epoch 1678, Loss: 2.8878490328788757, Final Batch Loss: 0.8542093634605408\n",
      "Epoch 1679, Loss: 2.70474436879158, Final Batch Loss: 0.6477986574172974\n",
      "Epoch 1680, Loss: 2.9281683564186096, Final Batch Loss: 0.8260803818702698\n",
      "Epoch 1681, Loss: 2.721198797225952, Final Batch Loss: 0.6188098192214966\n",
      "Epoch 1682, Loss: 2.57604917883873, Final Batch Loss: 0.507496178150177\n",
      "Epoch 1683, Loss: 2.4751724004745483, Final Batch Loss: 0.4352858364582062\n",
      "Epoch 1684, Loss: 2.4391692876815796, Final Batch Loss: 0.36774274706840515\n",
      "Epoch 1685, Loss: 2.5557604134082794, Final Batch Loss: 0.5249027013778687\n",
      "Epoch 1686, Loss: 2.28139328956604, Final Batch Loss: 0.2903125286102295\n",
      "Epoch 1687, Loss: 3.2513171434402466, Final Batch Loss: 1.1412875652313232\n",
      "Epoch 1688, Loss: 2.6695332527160645, Final Batch Loss: 0.5770438313484192\n",
      "Epoch 1689, Loss: 2.4946609139442444, Final Batch Loss: 0.33779531717300415\n",
      "Epoch 1690, Loss: 2.602727919816971, Final Batch Loss: 0.526646077632904\n",
      "Epoch 1691, Loss: 2.5620036125183105, Final Batch Loss: 0.500566303730011\n",
      "Epoch 1692, Loss: 2.5866445899009705, Final Batch Loss: 0.5409289598464966\n",
      "Epoch 1693, Loss: 2.33401820063591, Final Batch Loss: 0.3153002858161926\n",
      "Epoch 1694, Loss: 2.3182816207408905, Final Batch Loss: 0.31474313139915466\n",
      "Epoch 1695, Loss: 2.4483108520507812, Final Batch Loss: 0.3589417040348053\n",
      "Epoch 1696, Loss: 2.5151758193969727, Final Batch Loss: 0.5247212648391724\n",
      "Epoch 1697, Loss: 2.6695112884044647, Final Batch Loss: 0.619664192199707\n",
      "Epoch 1698, Loss: 2.6676024794578552, Final Batch Loss: 0.4984644055366516\n",
      "Epoch 1699, Loss: 2.3236756324768066, Final Batch Loss: 0.4165134131908417\n",
      "Epoch 1700, Loss: 2.3257690966129303, Final Batch Loss: 0.40997737646102905\n",
      "Epoch 1701, Loss: 2.4525581896305084, Final Batch Loss: 0.4737645983695984\n",
      "Epoch 1702, Loss: 2.4627977311611176, Final Batch Loss: 0.3609333634376526\n",
      "Epoch 1703, Loss: 2.243358224630356, Final Batch Loss: 0.2539573013782501\n",
      "Epoch 1704, Loss: 2.578750967979431, Final Batch Loss: 0.5946508049964905\n",
      "Epoch 1705, Loss: 2.6464283764362335, Final Batch Loss: 0.6051844954490662\n",
      "Epoch 1706, Loss: 2.1669967025518417, Final Batch Loss: 0.13529016077518463\n",
      "Epoch 1707, Loss: 2.4393628239631653, Final Batch Loss: 0.48931655287742615\n",
      "Epoch 1708, Loss: 2.5535351932048798, Final Batch Loss: 0.4655973017215729\n",
      "Epoch 1709, Loss: 2.4724964797496796, Final Batch Loss: 0.30249229073524475\n",
      "Epoch 1710, Loss: 2.586852192878723, Final Batch Loss: 0.4983941614627838\n",
      "Epoch 1711, Loss: 2.2561091780662537, Final Batch Loss: 0.27524346113204956\n",
      "Epoch 1712, Loss: 2.288436710834503, Final Batch Loss: 0.236365407705307\n",
      "Epoch 1713, Loss: 2.2992638647556305, Final Batch Loss: 0.30787864327430725\n",
      "Epoch 1714, Loss: 2.6464351415634155, Final Batch Loss: 0.6893094182014465\n",
      "Epoch 1715, Loss: 2.924578011035919, Final Batch Loss: 0.8888615369796753\n",
      "Epoch 1716, Loss: 2.6195493936538696, Final Batch Loss: 0.5476794242858887\n",
      "Epoch 1717, Loss: 2.431352347135544, Final Batch Loss: 0.3109971880912781\n",
      "Epoch 1718, Loss: 2.2262084037065506, Final Batch Loss: 0.24026505649089813\n",
      "Epoch 1719, Loss: 2.2325227558612823, Final Batch Loss: 0.2735840678215027\n",
      "Epoch 1720, Loss: 2.5386589765548706, Final Batch Loss: 0.5016117691993713\n",
      "Epoch 1721, Loss: 2.438832551240921, Final Batch Loss: 0.36955907940864563\n",
      "Epoch 1722, Loss: 2.652017503976822, Final Batch Loss: 0.6414061188697815\n",
      "Epoch 1723, Loss: 2.4373205602169037, Final Batch Loss: 0.5090100169181824\n",
      "Epoch 1724, Loss: 2.5096790492534637, Final Batch Loss: 0.6035265326499939\n",
      "Epoch 1725, Loss: 2.548445612192154, Final Batch Loss: 0.497618705034256\n",
      "Epoch 1726, Loss: 2.4461461901664734, Final Batch Loss: 0.37131020426750183\n",
      "Epoch 1727, Loss: 2.393068015575409, Final Batch Loss: 0.3379536271095276\n",
      "Epoch 1728, Loss: 3.227741986513138, Final Batch Loss: 1.333762764930725\n",
      "Epoch 1729, Loss: 2.59539932012558, Final Batch Loss: 0.4579823911190033\n",
      "Epoch 1730, Loss: 2.3034067302942276, Final Batch Loss: 0.22818441689014435\n",
      "Epoch 1731, Loss: 3.3322682082653046, Final Batch Loss: 1.3249651193618774\n",
      "Epoch 1732, Loss: 2.825757920742035, Final Batch Loss: 0.7907583117485046\n",
      "Epoch 1733, Loss: 3.075411379337311, Final Batch Loss: 0.957771360874176\n",
      "Epoch 1734, Loss: 2.7730569541454315, Final Batch Loss: 0.46622946858406067\n",
      "Epoch 1735, Loss: 2.9192795753479004, Final Batch Loss: 0.6419109106063843\n",
      "Epoch 1736, Loss: 2.5346790552139282, Final Batch Loss: 0.4493729770183563\n",
      "Epoch 1737, Loss: 2.736160457134247, Final Batch Loss: 0.6115154027938843\n",
      "Epoch 1738, Loss: 2.5168546736240387, Final Batch Loss: 0.526368260383606\n",
      "Epoch 1739, Loss: 2.4846816062927246, Final Batch Loss: 0.43889278173446655\n",
      "Epoch 1740, Loss: 2.6899865865707397, Final Batch Loss: 0.621343731880188\n",
      "Epoch 1741, Loss: 2.793199509382248, Final Batch Loss: 0.7706689834594727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1742, Loss: 2.572640299797058, Final Batch Loss: 0.5328397750854492\n",
      "Epoch 1743, Loss: 2.445865660905838, Final Batch Loss: 0.28320637345314026\n",
      "Epoch 1744, Loss: 2.788389712572098, Final Batch Loss: 0.7306909561157227\n",
      "Epoch 1745, Loss: 2.4759705662727356, Final Batch Loss: 0.5392928719520569\n",
      "Epoch 1746, Loss: 2.7029620707035065, Final Batch Loss: 0.49219465255737305\n",
      "Epoch 1747, Loss: 2.3935556411743164, Final Batch Loss: 0.3492453992366791\n",
      "Epoch 1748, Loss: 2.4698521494865417, Final Batch Loss: 0.3671104311943054\n",
      "Epoch 1749, Loss: 2.462196320295334, Final Batch Loss: 0.5137072801589966\n",
      "Epoch 1750, Loss: 2.6227728128433228, Final Batch Loss: 0.6874950528144836\n",
      "Epoch 1751, Loss: 2.583771735429764, Final Batch Loss: 0.5610899925231934\n",
      "Epoch 1752, Loss: 2.9195329546928406, Final Batch Loss: 0.79708331823349\n",
      "Epoch 1753, Loss: 2.763476550579071, Final Batch Loss: 0.6902264952659607\n",
      "Epoch 1754, Loss: 2.994764983654022, Final Batch Loss: 0.9700629115104675\n",
      "Epoch 1755, Loss: 2.753119856119156, Final Batch Loss: 0.5389890670776367\n",
      "Epoch 1756, Loss: 2.5374867618083954, Final Batch Loss: 0.44210758805274963\n",
      "Epoch 1757, Loss: 2.7967221438884735, Final Batch Loss: 0.7223017811775208\n",
      "Epoch 1758, Loss: 2.569860100746155, Final Batch Loss: 0.6069148182868958\n",
      "Epoch 1759, Loss: 2.142328664660454, Final Batch Loss: 0.2153237909078598\n",
      "Epoch 1760, Loss: 2.471535086631775, Final Batch Loss: 0.40439411997795105\n",
      "Epoch 1761, Loss: 2.367142766714096, Final Batch Loss: 0.3493879735469818\n",
      "Epoch 1762, Loss: 2.421200543642044, Final Batch Loss: 0.4810621440410614\n",
      "Epoch 1763, Loss: 2.357456237077713, Final Batch Loss: 0.30504804849624634\n",
      "Epoch 1764, Loss: 2.574343830347061, Final Batch Loss: 0.4592925012111664\n",
      "Epoch 1765, Loss: 2.441681057214737, Final Batch Loss: 0.38637176156044006\n",
      "Epoch 1766, Loss: 2.6789093911647797, Final Batch Loss: 0.8209585547447205\n",
      "Epoch 1767, Loss: 2.4052314162254333, Final Batch Loss: 0.31972965598106384\n",
      "Epoch 1768, Loss: 2.6670930981636047, Final Batch Loss: 0.607245147228241\n",
      "Epoch 1769, Loss: 3.149182617664337, Final Batch Loss: 1.1491237878799438\n",
      "Epoch 1770, Loss: 2.691632777452469, Final Batch Loss: 0.6960472464561462\n",
      "Epoch 1771, Loss: 2.5441194474697113, Final Batch Loss: 0.5549734234809875\n",
      "Epoch 1772, Loss: 2.4854533970355988, Final Batch Loss: 0.4708561599254608\n",
      "Epoch 1773, Loss: 2.4132666885852814, Final Batch Loss: 0.43899890780448914\n",
      "Epoch 1774, Loss: 2.123172417283058, Final Batch Loss: 0.10062359273433685\n",
      "Epoch 1775, Loss: 2.593177378177643, Final Batch Loss: 0.5192307233810425\n",
      "Epoch 1776, Loss: 2.3841172754764557, Final Batch Loss: 0.3822975754737854\n",
      "Epoch 1777, Loss: 2.4663659632205963, Final Batch Loss: 0.5563943982124329\n",
      "Epoch 1778, Loss: 2.162186525762081, Final Batch Loss: 0.11676641553640366\n",
      "Epoch 1779, Loss: 2.8217231035232544, Final Batch Loss: 0.9026599526405334\n",
      "Epoch 1780, Loss: 2.2758539617061615, Final Batch Loss: 0.37457770109176636\n",
      "Epoch 1781, Loss: 2.6436988711357117, Final Batch Loss: 0.6648240685462952\n",
      "Epoch 1782, Loss: 2.843426764011383, Final Batch Loss: 0.5660406947135925\n",
      "Epoch 1783, Loss: 2.3219231963157654, Final Batch Loss: 0.29239317774772644\n",
      "Epoch 1784, Loss: 2.2568949162960052, Final Batch Loss: 0.27232107520103455\n",
      "Epoch 1785, Loss: 2.8607919812202454, Final Batch Loss: 0.7945102453231812\n",
      "Epoch 1786, Loss: 2.4891882836818695, Final Batch Loss: 0.4120495915412903\n",
      "Epoch 1787, Loss: 2.4475159645080566, Final Batch Loss: 0.5425196886062622\n",
      "Epoch 1788, Loss: 2.7914476096630096, Final Batch Loss: 0.840326189994812\n",
      "Epoch 1789, Loss: 2.2671972513198853, Final Batch Loss: 0.3016854226589203\n",
      "Epoch 1790, Loss: 2.6403645873069763, Final Batch Loss: 0.6179267168045044\n",
      "Epoch 1791, Loss: 2.6096365451812744, Final Batch Loss: 0.4954512417316437\n",
      "Epoch 1792, Loss: 2.595221310853958, Final Batch Loss: 0.6077319383621216\n",
      "Epoch 1793, Loss: 2.3657441437244415, Final Batch Loss: 0.49817195534706116\n",
      "Epoch 1794, Loss: 2.601164549589157, Final Batch Loss: 0.6437053084373474\n",
      "Epoch 1795, Loss: 2.527620553970337, Final Batch Loss: 0.44958189129829407\n",
      "Epoch 1796, Loss: 2.6478767096996307, Final Batch Loss: 0.6622018814086914\n",
      "Epoch 1797, Loss: 2.552926182746887, Final Batch Loss: 0.5537003874778748\n",
      "Epoch 1798, Loss: 2.4669054746627808, Final Batch Loss: 0.4754505455493927\n",
      "Epoch 1799, Loss: 2.5688313841819763, Final Batch Loss: 0.47659581899642944\n",
      "Epoch 1800, Loss: 2.1427140682935715, Final Batch Loss: 0.14616109430789948\n",
      "Epoch 1801, Loss: 2.218389183282852, Final Batch Loss: 0.33307138085365295\n",
      "Epoch 1802, Loss: 2.7377392947673798, Final Batch Loss: 0.7348920702934265\n",
      "Epoch 1803, Loss: 2.5795837342739105, Final Batch Loss: 0.6754369735717773\n",
      "Epoch 1804, Loss: 2.4144466817379, Final Batch Loss: 0.3153200149536133\n",
      "Epoch 1805, Loss: 2.811952292919159, Final Batch Loss: 0.7865625619888306\n",
      "Epoch 1806, Loss: 2.1566931307315826, Final Batch Loss: 0.33676987886428833\n",
      "Epoch 1807, Loss: 2.4328553080558777, Final Batch Loss: 0.3440406024456024\n",
      "Epoch 1808, Loss: 2.307491719722748, Final Batch Loss: 0.31214818358421326\n",
      "Epoch 1809, Loss: 2.8949168026447296, Final Batch Loss: 0.8729501962661743\n",
      "Epoch 1810, Loss: 2.634513169527054, Final Batch Loss: 0.5879524350166321\n",
      "Epoch 1811, Loss: 2.7384228706359863, Final Batch Loss: 0.6710430383682251\n",
      "Epoch 1812, Loss: 2.395048588514328, Final Batch Loss: 0.34157794713974\n",
      "Epoch 1813, Loss: 2.4579292833805084, Final Batch Loss: 0.5054540038108826\n",
      "Epoch 1814, Loss: 2.4899844229221344, Final Batch Loss: 0.39826837182044983\n",
      "Epoch 1815, Loss: 2.494142860174179, Final Batch Loss: 0.4546482563018799\n",
      "Epoch 1816, Loss: 2.5797140896320343, Final Batch Loss: 0.4428011476993561\n",
      "Epoch 1817, Loss: 2.8799625039100647, Final Batch Loss: 0.6536461114883423\n",
      "Epoch 1818, Loss: 2.7119828164577484, Final Batch Loss: 0.48761847615242004\n",
      "Epoch 1819, Loss: 2.099159926176071, Final Batch Loss: 0.181259423494339\n",
      "Epoch 1820, Loss: 3.0579188466072083, Final Batch Loss: 0.9079390168190002\n",
      "Epoch 1821, Loss: 3.654397875070572, Final Batch Loss: 1.6324735879898071\n",
      "Epoch 1822, Loss: 2.7117265164852142, Final Batch Loss: 0.7302700877189636\n",
      "Epoch 1823, Loss: 2.5207786858081818, Final Batch Loss: 0.4718015491962433\n",
      "Epoch 1824, Loss: 2.7708026468753815, Final Batch Loss: 0.7779688239097595\n",
      "Epoch 1825, Loss: 2.7907004058361053, Final Batch Loss: 0.7482460737228394\n",
      "Epoch 1826, Loss: 2.601973980665207, Final Batch Loss: 0.6287315487861633\n",
      "Epoch 1827, Loss: 2.694141983985901, Final Batch Loss: 0.512801468372345\n",
      "Epoch 1828, Loss: 2.6574157178401947, Final Batch Loss: 0.7248481512069702\n",
      "Epoch 1829, Loss: 2.467667669057846, Final Batch Loss: 0.41723236441612244\n",
      "Epoch 1830, Loss: 2.5441949367523193, Final Batch Loss: 0.4806302487850189\n",
      "Epoch 1831, Loss: 2.172224074602127, Final Batch Loss: 0.10631352663040161\n",
      "Epoch 1832, Loss: 2.3881985247135162, Final Batch Loss: 0.43122872710227966\n",
      "Epoch 1833, Loss: 2.5320584177970886, Final Batch Loss: 0.5536026954650879\n",
      "Epoch 1834, Loss: 2.196363180875778, Final Batch Loss: 0.2988622188568115\n",
      "Epoch 1835, Loss: 2.345499187707901, Final Batch Loss: 0.3731241822242737\n",
      "Epoch 1836, Loss: 2.3041252493858337, Final Batch Loss: 0.31960415840148926\n",
      "Epoch 1837, Loss: 2.3882960081100464, Final Batch Loss: 0.3842332363128662\n",
      "Epoch 1838, Loss: 2.4606123566627502, Final Batch Loss: 0.5691830515861511\n",
      "Epoch 1839, Loss: 2.397727221250534, Final Batch Loss: 0.3729960322380066\n",
      "Epoch 1840, Loss: 2.406843662261963, Final Batch Loss: 0.4404887557029724\n",
      "Epoch 1841, Loss: 2.8569000363349915, Final Batch Loss: 0.8957993388175964\n",
      "Epoch 1842, Loss: 2.678493231534958, Final Batch Loss: 0.7928924560546875\n",
      "Epoch 1843, Loss: 2.787744700908661, Final Batch Loss: 0.9760466814041138\n",
      "Epoch 1844, Loss: 2.5989039838314056, Final Batch Loss: 0.5557734966278076\n",
      "Epoch 1845, Loss: 2.3307663798332214, Final Batch Loss: 0.29757922887802124\n",
      "Epoch 1846, Loss: 2.3935084342956543, Final Batch Loss: 0.4210461974143982\n",
      "Epoch 1847, Loss: 2.8431050181388855, Final Batch Loss: 0.7501693964004517\n",
      "Epoch 1848, Loss: 2.773449033498764, Final Batch Loss: 0.7970780730247498\n",
      "Epoch 1849, Loss: 2.339970201253891, Final Batch Loss: 0.3102293610572815\n",
      "Epoch 1850, Loss: 2.208830028772354, Final Batch Loss: 0.12846726179122925\n",
      "Epoch 1851, Loss: 2.441579669713974, Final Batch Loss: 0.37894177436828613\n",
      "Epoch 1852, Loss: 2.4699362218379974, Final Batch Loss: 0.40476110577583313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1853, Loss: 2.936554580926895, Final Batch Loss: 0.9721828699111938\n",
      "Epoch 1854, Loss: 2.4986759424209595, Final Batch Loss: 0.4162493050098419\n",
      "Epoch 1855, Loss: 3.1174307763576508, Final Batch Loss: 1.1307666301727295\n",
      "Epoch 1856, Loss: 2.3866138458251953, Final Batch Loss: 0.4673464894294739\n",
      "Epoch 1857, Loss: 2.1801264882087708, Final Batch Loss: 0.3066466450691223\n",
      "Epoch 1858, Loss: 2.74649840593338, Final Batch Loss: 0.818315863609314\n",
      "Epoch 1859, Loss: 2.407680720090866, Final Batch Loss: 0.4020538926124573\n",
      "Epoch 1860, Loss: 2.1986562609672546, Final Batch Loss: 0.28715747594833374\n",
      "Epoch 1861, Loss: 2.4676262736320496, Final Batch Loss: 0.5246519446372986\n",
      "Epoch 1862, Loss: 2.427992880344391, Final Batch Loss: 0.3915015757083893\n",
      "Epoch 1863, Loss: 2.390241503715515, Final Batch Loss: 0.5323396325111389\n",
      "Epoch 1864, Loss: 2.446440726518631, Final Batch Loss: 0.45455998182296753\n",
      "Epoch 1865, Loss: 2.4604744017124176, Final Batch Loss: 0.4383425712585449\n",
      "Epoch 1866, Loss: 2.196811482310295, Final Batch Loss: 0.19746844470500946\n",
      "Epoch 1867, Loss: 2.419762372970581, Final Batch Loss: 0.4010866582393646\n",
      "Epoch 1868, Loss: 2.3791724145412445, Final Batch Loss: 0.45276421308517456\n",
      "Epoch 1869, Loss: 2.4156106412410736, Final Batch Loss: 0.39092549681663513\n",
      "Epoch 1870, Loss: 2.5783001482486725, Final Batch Loss: 0.6497222781181335\n",
      "Epoch 1871, Loss: 2.728504329919815, Final Batch Loss: 0.8270346522331238\n",
      "Epoch 1872, Loss: 2.539309501647949, Final Batch Loss: 0.6797360181808472\n",
      "Epoch 1873, Loss: 2.097432419657707, Final Batch Loss: 0.1610238403081894\n",
      "Epoch 1874, Loss: 2.239671692252159, Final Batch Loss: 0.14752833545207977\n",
      "Epoch 1875, Loss: 2.643274664878845, Final Batch Loss: 0.5655669569969177\n",
      "Epoch 1876, Loss: 2.2538565397262573, Final Batch Loss: 0.2718923091888428\n",
      "Epoch 1877, Loss: 2.899206429719925, Final Batch Loss: 0.9017452597618103\n",
      "Epoch 1878, Loss: 2.301400750875473, Final Batch Loss: 0.359565794467926\n",
      "Epoch 1879, Loss: 2.4878889620304108, Final Batch Loss: 0.3023517429828644\n",
      "Epoch 1880, Loss: 2.309293657541275, Final Batch Loss: 0.32277700304985046\n",
      "Epoch 1881, Loss: 2.5021274089813232, Final Batch Loss: 0.4982360005378723\n",
      "Epoch 1882, Loss: 2.4729338586330414, Final Batch Loss: 0.6214436888694763\n",
      "Epoch 1883, Loss: 2.382167249917984, Final Batch Loss: 0.5187885165214539\n",
      "Epoch 1884, Loss: 2.4024911522865295, Final Batch Loss: 0.4070451557636261\n",
      "Epoch 1885, Loss: 2.491477906703949, Final Batch Loss: 0.3540729582309723\n",
      "Epoch 1886, Loss: 2.757609009742737, Final Batch Loss: 0.7236887812614441\n",
      "Epoch 1887, Loss: 2.3761767148971558, Final Batch Loss: 0.44360488653182983\n",
      "Epoch 1888, Loss: 2.172709286212921, Final Batch Loss: 0.33120712637901306\n",
      "Epoch 1889, Loss: 2.4310725927352905, Final Batch Loss: 0.4924883246421814\n",
      "Epoch 1890, Loss: 2.377847522497177, Final Batch Loss: 0.4463232159614563\n",
      "Epoch 1891, Loss: 2.721281111240387, Final Batch Loss: 0.8536664843559265\n",
      "Epoch 1892, Loss: 2.412245064973831, Final Batch Loss: 0.5405969619750977\n",
      "Epoch 1893, Loss: 2.3120628595352173, Final Batch Loss: 0.4458356201648712\n",
      "Epoch 1894, Loss: 2.272609382867813, Final Batch Loss: 0.41570499539375305\n",
      "Epoch 1895, Loss: 2.568456530570984, Final Batch Loss: 0.5541990399360657\n",
      "Epoch 1896, Loss: 2.470732867717743, Final Batch Loss: 0.543245255947113\n",
      "Epoch 1897, Loss: 2.5035250186920166, Final Batch Loss: 0.5307427644729614\n",
      "Epoch 1898, Loss: 2.339917004108429, Final Batch Loss: 0.3963780701160431\n",
      "Epoch 1899, Loss: 2.7504932582378387, Final Batch Loss: 0.8557307124137878\n",
      "Epoch 1900, Loss: 2.210683286190033, Final Batch Loss: 0.229976087808609\n",
      "Epoch 1901, Loss: 2.3020047545433044, Final Batch Loss: 0.3092150092124939\n",
      "Epoch 1902, Loss: 2.6882607340812683, Final Batch Loss: 0.7196624875068665\n",
      "Epoch 1903, Loss: 2.74905064702034, Final Batch Loss: 0.7079756855964661\n",
      "Epoch 1904, Loss: 2.3716547787189484, Final Batch Loss: 0.4059220254421234\n",
      "Epoch 1905, Loss: 2.5541133284568787, Final Batch Loss: 0.520488440990448\n",
      "Epoch 1906, Loss: 2.5101666152477264, Final Batch Loss: 0.5295040011405945\n",
      "Epoch 1907, Loss: 2.487432539463043, Final Batch Loss: 0.34399357438087463\n",
      "Epoch 1908, Loss: 2.417175680398941, Final Batch Loss: 0.5143464207649231\n",
      "Epoch 1909, Loss: 2.921653062105179, Final Batch Loss: 0.9925777316093445\n",
      "Epoch 1910, Loss: 2.3560091853141785, Final Batch Loss: 0.394192636013031\n",
      "Epoch 1911, Loss: 2.3560741543769836, Final Batch Loss: 0.37228628993034363\n",
      "Epoch 1912, Loss: 2.565284103155136, Final Batch Loss: 0.6823737025260925\n",
      "Epoch 1913, Loss: 2.6099412739276886, Final Batch Loss: 0.6565118432044983\n",
      "Epoch 1914, Loss: 2.5163177251815796, Final Batch Loss: 0.5823224782943726\n",
      "Epoch 1915, Loss: 2.514326900243759, Final Batch Loss: 0.4720208942890167\n",
      "Epoch 1916, Loss: 2.545815110206604, Final Batch Loss: 0.5362814664840698\n",
      "Epoch 1917, Loss: 3.074644535779953, Final Batch Loss: 1.060211420059204\n",
      "Epoch 1918, Loss: 2.7250864505767822, Final Batch Loss: 0.6510135531425476\n",
      "Epoch 1919, Loss: 2.607328563928604, Final Batch Loss: 0.5767213106155396\n",
      "Epoch 1920, Loss: 2.6569066643714905, Final Batch Loss: 0.6840319037437439\n",
      "Epoch 1921, Loss: 2.3226458132267, Final Batch Loss: 0.36638686060905457\n",
      "Epoch 1922, Loss: 2.20939202606678, Final Batch Loss: 0.14400900900363922\n",
      "Epoch 1923, Loss: 2.655488044023514, Final Batch Loss: 0.6717432737350464\n",
      "Epoch 1924, Loss: 2.2237235009670258, Final Batch Loss: 0.32386869192123413\n",
      "Epoch 1925, Loss: 2.248651385307312, Final Batch Loss: 0.2805827558040619\n",
      "Epoch 1926, Loss: 2.5293752551078796, Final Batch Loss: 0.4950416684150696\n",
      "Epoch 1927, Loss: 2.6966553032398224, Final Batch Loss: 0.7809826135635376\n",
      "Epoch 1928, Loss: 2.283018112182617, Final Batch Loss: 0.27335402369499207\n",
      "Epoch 1929, Loss: 2.4702024161815643, Final Batch Loss: 0.5768507719039917\n",
      "Epoch 1930, Loss: 2.6839334666728973, Final Batch Loss: 0.8286189436912537\n",
      "Epoch 1931, Loss: 2.389151006937027, Final Batch Loss: 0.4732135236263275\n",
      "Epoch 1932, Loss: 2.529386192560196, Final Batch Loss: 0.5043526887893677\n",
      "Epoch 1933, Loss: 2.6952519714832306, Final Batch Loss: 0.7786638140678406\n",
      "Epoch 1934, Loss: 2.7526579797267914, Final Batch Loss: 0.7071765661239624\n",
      "Epoch 1935, Loss: 2.4361573457717896, Final Batch Loss: 0.5088687539100647\n",
      "Epoch 1936, Loss: 2.3893691897392273, Final Batch Loss: 0.44423407316207886\n",
      "Epoch 1937, Loss: 2.0846460461616516, Final Batch Loss: 0.129440039396286\n",
      "Epoch 1938, Loss: 2.568463772535324, Final Batch Loss: 0.6518321633338928\n",
      "Epoch 1939, Loss: 2.0913770645856857, Final Batch Loss: 0.1912691444158554\n",
      "Epoch 1940, Loss: 2.2102569937705994, Final Batch Loss: 0.33164575695991516\n",
      "Epoch 1941, Loss: 3.126175284385681, Final Batch Loss: 1.2004860639572144\n",
      "Epoch 1942, Loss: 2.5980502367019653, Final Batch Loss: 0.729256272315979\n",
      "Epoch 1943, Loss: 2.7536812722682953, Final Batch Loss: 0.9079117178916931\n",
      "Epoch 1944, Loss: 2.302690029144287, Final Batch Loss: 0.35819488763809204\n",
      "Epoch 1945, Loss: 2.4719096422195435, Final Batch Loss: 0.3665432333946228\n",
      "Epoch 1946, Loss: 2.177417069673538, Final Batch Loss: 0.14035436511039734\n",
      "Epoch 1947, Loss: 2.3971027135849, Final Batch Loss: 0.5377594828605652\n",
      "Epoch 1948, Loss: 2.124926269054413, Final Batch Loss: 0.29427099227905273\n",
      "Epoch 1949, Loss: 2.3729342818260193, Final Batch Loss: 0.41045042872428894\n",
      "Epoch 1950, Loss: 2.2648991644382477, Final Batch Loss: 0.3792072832584381\n",
      "Epoch 1951, Loss: 2.180517166852951, Final Batch Loss: 0.2897537350654602\n",
      "Epoch 1952, Loss: 2.4531392455101013, Final Batch Loss: 0.5494986772537231\n",
      "Epoch 1953, Loss: 2.2438826262950897, Final Batch Loss: 0.44170984625816345\n",
      "Epoch 1954, Loss: 2.5528594255447388, Final Batch Loss: 0.5773892402648926\n",
      "Epoch 1955, Loss: 2.360671639442444, Final Batch Loss: 0.47723838686943054\n",
      "Epoch 1956, Loss: 2.3954973816871643, Final Batch Loss: 0.4759473502635956\n",
      "Epoch 1957, Loss: 2.165347546339035, Final Batch Loss: 0.28946226835250854\n",
      "Epoch 1958, Loss: 2.148444578051567, Final Batch Loss: 0.18133606016635895\n",
      "Epoch 1959, Loss: 2.2583045065402985, Final Batch Loss: 0.3677351176738739\n",
      "Epoch 1960, Loss: 2.3828497231006622, Final Batch Loss: 0.32594045996665955\n",
      "Epoch 1961, Loss: 2.628229022026062, Final Batch Loss: 0.5757497549057007\n",
      "Epoch 1962, Loss: 2.3720697462558746, Final Batch Loss: 0.4333348572254181\n",
      "Epoch 1963, Loss: 2.4882860481739044, Final Batch Loss: 0.557385265827179\n",
      "Epoch 1964, Loss: 2.4747878313064575, Final Batch Loss: 0.6639288663864136\n",
      "Epoch 1965, Loss: 2.285336673259735, Final Batch Loss: 0.2864609360694885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1966, Loss: 2.4564533829689026, Final Batch Loss: 0.516281247138977\n",
      "Epoch 1967, Loss: 2.5586961209774017, Final Batch Loss: 0.5792843699455261\n",
      "Epoch 1968, Loss: 2.304281324148178, Final Batch Loss: 0.3381492495536804\n",
      "Epoch 1969, Loss: 2.054132506251335, Final Batch Loss: 0.18575651943683624\n",
      "Epoch 1970, Loss: 2.399894058704376, Final Batch Loss: 0.33169397711753845\n",
      "Epoch 1971, Loss: 2.496996134519577, Final Batch Loss: 0.5786951780319214\n",
      "Epoch 1972, Loss: 2.27655827999115, Final Batch Loss: 0.31496143341064453\n",
      "Epoch 1973, Loss: 2.4315084517002106, Final Batch Loss: 0.4878815710544586\n",
      "Epoch 1974, Loss: 2.3137983083724976, Final Batch Loss: 0.2576822340488434\n",
      "Epoch 1975, Loss: 2.0546756237745285, Final Batch Loss: 0.2031078189611435\n",
      "Epoch 1976, Loss: 2.2672702372074127, Final Batch Loss: 0.29907041788101196\n",
      "Epoch 1977, Loss: 2.3577962815761566, Final Batch Loss: 0.4476298987865448\n",
      "Epoch 1978, Loss: 2.5237697660923004, Final Batch Loss: 0.6868405938148499\n",
      "Epoch 1979, Loss: 2.1299188137054443, Final Batch Loss: 0.25949937105178833\n",
      "Epoch 1980, Loss: 2.1974779963493347, Final Batch Loss: 0.37845227122306824\n",
      "Epoch 1981, Loss: 2.158679246902466, Final Batch Loss: 0.3583756387233734\n",
      "Epoch 1982, Loss: 2.1746849417686462, Final Batch Loss: 0.3095158040523529\n",
      "Epoch 1983, Loss: 1.9039119333028793, Final Batch Loss: 0.1438625007867813\n",
      "Epoch 1984, Loss: 2.410602033138275, Final Batch Loss: 0.5129262208938599\n",
      "Epoch 1985, Loss: 2.2899888157844543, Final Batch Loss: 0.4476740062236786\n",
      "Epoch 1986, Loss: 2.128038078546524, Final Batch Loss: 0.27449366450309753\n",
      "Epoch 1987, Loss: 2.3179028034210205, Final Batch Loss: 0.4454251229763031\n",
      "Epoch 1988, Loss: 2.121854305267334, Final Batch Loss: 0.23121392726898193\n",
      "Epoch 1989, Loss: 2.1832486540079117, Final Batch Loss: 0.23629240691661835\n",
      "Epoch 1990, Loss: 2.486304759979248, Final Batch Loss: 0.6196539998054504\n",
      "Epoch 1991, Loss: 2.260358840227127, Final Batch Loss: 0.3401666581630707\n",
      "Epoch 1992, Loss: 2.6803755164146423, Final Batch Loss: 0.8663636445999146\n",
      "Epoch 1993, Loss: 3.0002883672714233, Final Batch Loss: 1.138165831565857\n",
      "Epoch 1994, Loss: 2.6247019171714783, Final Batch Loss: 0.6940478086471558\n",
      "Epoch 1995, Loss: 2.1895740926265717, Final Batch Loss: 0.16574645042419434\n",
      "Epoch 1996, Loss: 2.7220212817192078, Final Batch Loss: 0.7329201698303223\n",
      "Epoch 1997, Loss: 2.0784800834953785, Final Batch Loss: 0.05384134128689766\n",
      "Epoch 1998, Loss: 2.6355764865875244, Final Batch Loss: 0.73112553358078\n",
      "Epoch 1999, Loss: 2.4082572162151337, Final Batch Loss: 0.528899610042572\n",
      "Epoch 2000, Loss: 2.284734308719635, Final Batch Loss: 0.4123474955558777\n",
      "Epoch 2001, Loss: 2.516912877559662, Final Batch Loss: 0.6464437246322632\n",
      "Epoch 2002, Loss: 2.455191910266876, Final Batch Loss: 0.5483717322349548\n",
      "Epoch 2003, Loss: 2.3295393586158752, Final Batch Loss: 0.3891039490699768\n",
      "Epoch 2004, Loss: 2.5104568004608154, Final Batch Loss: 0.5934985876083374\n",
      "Epoch 2005, Loss: 2.407254844903946, Final Batch Loss: 0.5406021475791931\n",
      "Epoch 2006, Loss: 2.256329596042633, Final Batch Loss: 0.3632969856262207\n",
      "Epoch 2007, Loss: 2.1459319293498993, Final Batch Loss: 0.18294331431388855\n",
      "Epoch 2008, Loss: 2.3828384578227997, Final Batch Loss: 0.5091648697853088\n",
      "Epoch 2009, Loss: 2.4940617084503174, Final Batch Loss: 0.5470243096351624\n",
      "Epoch 2010, Loss: 2.365863621234894, Final Batch Loss: 0.5839695334434509\n",
      "Epoch 2011, Loss: 2.2624059915542603, Final Batch Loss: 0.354840487241745\n",
      "Epoch 2012, Loss: 2.6538130044937134, Final Batch Loss: 0.8608559966087341\n",
      "Epoch 2013, Loss: 2.634908974170685, Final Batch Loss: 0.7318972945213318\n",
      "Epoch 2014, Loss: 2.2906517684459686, Final Batch Loss: 0.41818150877952576\n",
      "Epoch 2015, Loss: 2.3132284283638, Final Batch Loss: 0.4353180229663849\n",
      "Epoch 2016, Loss: 2.322902113199234, Final Batch Loss: 0.38556697964668274\n",
      "Epoch 2017, Loss: 2.666509211063385, Final Batch Loss: 0.766577422618866\n",
      "Epoch 2018, Loss: 2.4055036902427673, Final Batch Loss: 0.5802181363105774\n",
      "Epoch 2019, Loss: 2.2452955543994904, Final Batch Loss: 0.3602440357208252\n",
      "Epoch 2020, Loss: 2.230268716812134, Final Batch Loss: 0.40192899107933044\n",
      "Epoch 2021, Loss: 2.3155478835105896, Final Batch Loss: 0.45764461159706116\n",
      "Epoch 2022, Loss: 2.922930210828781, Final Batch Loss: 0.9170413613319397\n",
      "Epoch 2023, Loss: 2.3557447493076324, Final Batch Loss: 0.4248032569885254\n",
      "Epoch 2024, Loss: 2.4158772230148315, Final Batch Loss: 0.40308424830436707\n",
      "Epoch 2025, Loss: 2.774280995130539, Final Batch Loss: 0.7936612963676453\n",
      "Epoch 2026, Loss: 2.195732593536377, Final Batch Loss: 0.2544277310371399\n",
      "Epoch 2027, Loss: 2.4012617468833923, Final Batch Loss: 0.370449036359787\n",
      "Epoch 2028, Loss: 2.4315004646778107, Final Batch Loss: 0.5554503798484802\n",
      "Epoch 2029, Loss: 2.4629190266132355, Final Batch Loss: 0.5455253720283508\n",
      "Epoch 2030, Loss: 2.2931277453899384, Final Batch Loss: 0.38865846395492554\n",
      "Epoch 2031, Loss: 2.365417718887329, Final Batch Loss: 0.4298935532569885\n",
      "Epoch 2032, Loss: 2.41304150223732, Final Batch Loss: 0.4182888865470886\n",
      "Epoch 2033, Loss: 2.3565852344036102, Final Batch Loss: 0.5087950825691223\n",
      "Epoch 2034, Loss: 2.3671535551548004, Final Batch Loss: 0.5380129218101501\n",
      "Epoch 2035, Loss: 2.203251838684082, Final Batch Loss: 0.3718755841255188\n",
      "Epoch 2036, Loss: 2.337018758058548, Final Batch Loss: 0.5210140943527222\n",
      "Epoch 2037, Loss: 2.1164986938238144, Final Batch Loss: 0.1481114774942398\n",
      "Epoch 2038, Loss: 2.566503554582596, Final Batch Loss: 0.7565197348594666\n",
      "Epoch 2039, Loss: 2.21047842502594, Final Batch Loss: 0.3841015696525574\n",
      "Epoch 2040, Loss: 2.988994061946869, Final Batch Loss: 0.9806832671165466\n",
      "Epoch 2041, Loss: 2.1358789801597595, Final Batch Loss: 0.2881552278995514\n",
      "Epoch 2042, Loss: 2.209726393222809, Final Batch Loss: 0.3938312232494354\n",
      "Epoch 2043, Loss: 2.437900960445404, Final Batch Loss: 0.5086539387702942\n",
      "Epoch 2044, Loss: 2.288077563047409, Final Batch Loss: 0.3669789731502533\n",
      "Epoch 2045, Loss: 2.0999103784561157, Final Batch Loss: 0.33231499791145325\n",
      "Epoch 2046, Loss: 2.369898945093155, Final Batch Loss: 0.511904776096344\n",
      "Epoch 2047, Loss: 2.328907310962677, Final Batch Loss: 0.47890111804008484\n",
      "Epoch 2048, Loss: 2.4332949817180634, Final Batch Loss: 0.6372528672218323\n",
      "Epoch 2049, Loss: 2.2117999494075775, Final Batch Loss: 0.38342830538749695\n",
      "Epoch 2050, Loss: 3.0490337014198303, Final Batch Loss: 1.2144429683685303\n",
      "Epoch 2051, Loss: 1.9813349097967148, Final Batch Loss: 0.16755680739879608\n",
      "Epoch 2052, Loss: 2.285646229982376, Final Batch Loss: 0.3643588423728943\n",
      "Epoch 2053, Loss: 2.224770039319992, Final Batch Loss: 0.4278031885623932\n",
      "Epoch 2054, Loss: 2.603395015001297, Final Batch Loss: 0.6027487516403198\n",
      "Epoch 2055, Loss: 2.6884775161743164, Final Batch Loss: 0.7993432879447937\n",
      "Epoch 2056, Loss: 2.1211394369602203, Final Batch Loss: 0.2579912543296814\n",
      "Epoch 2057, Loss: 2.1309410631656647, Final Batch Loss: 0.34400537610054016\n",
      "Epoch 2058, Loss: 2.375675916671753, Final Batch Loss: 0.4776912331581116\n",
      "Epoch 2059, Loss: 2.1870684027671814, Final Batch Loss: 0.35427984595298767\n",
      "Epoch 2060, Loss: 2.1659879982471466, Final Batch Loss: 0.3997156620025635\n",
      "Epoch 2061, Loss: 2.4789901971817017, Final Batch Loss: 0.5928736329078674\n",
      "Epoch 2062, Loss: 1.974250704050064, Final Batch Loss: 0.19287410378456116\n",
      "Epoch 2063, Loss: 2.1184670627117157, Final Batch Loss: 0.3041287064552307\n",
      "Epoch 2064, Loss: 2.3123936653137207, Final Batch Loss: 0.4901730418205261\n",
      "Epoch 2065, Loss: 2.0028759986162186, Final Batch Loss: 0.24202163517475128\n",
      "Epoch 2066, Loss: 1.9453209340572357, Final Batch Loss: 0.2772102653980255\n",
      "Epoch 2067, Loss: 2.3123504519462585, Final Batch Loss: 0.6054139137268066\n",
      "Epoch 2068, Loss: 2.3672847151756287, Final Batch Loss: 0.554955780506134\n",
      "Epoch 2069, Loss: 2.0432336628437042, Final Batch Loss: 0.17630448937416077\n",
      "Epoch 2070, Loss: 2.422425776720047, Final Batch Loss: 0.5458906888961792\n",
      "Epoch 2071, Loss: 2.350509434938431, Final Batch Loss: 0.4892519414424896\n",
      "Epoch 2072, Loss: 2.2315653860569, Final Batch Loss: 0.39537283778190613\n",
      "Epoch 2073, Loss: 2.3613367080688477, Final Batch Loss: 0.48701831698417664\n",
      "Epoch 2074, Loss: 3.3992123305797577, Final Batch Loss: 1.4605761766433716\n",
      "Epoch 2075, Loss: 2.3703875839710236, Final Batch Loss: 0.5310563445091248\n",
      "Epoch 2076, Loss: 2.3258552253246307, Final Batch Loss: 0.46240878105163574\n",
      "Epoch 2077, Loss: 2.5345177054405212, Final Batch Loss: 0.6599101424217224\n",
      "Epoch 2078, Loss: 2.8745358288288116, Final Batch Loss: 1.0154426097869873\n",
      "Epoch 2079, Loss: 2.1973093450069427, Final Batch Loss: 0.31071075797080994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2080, Loss: 2.4417537450790405, Final Batch Loss: 0.4235515296459198\n",
      "Epoch 2081, Loss: 2.5730217695236206, Final Batch Loss: 0.6877058744430542\n",
      "Epoch 2082, Loss: 2.2298110723495483, Final Batch Loss: 0.3090283274650574\n",
      "Epoch 2083, Loss: 2.698771059513092, Final Batch Loss: 0.7228127121925354\n",
      "Epoch 2084, Loss: 3.2993146777153015, Final Batch Loss: 0.9180096983909607\n",
      "Epoch 2085, Loss: 2.831430345773697, Final Batch Loss: 0.7324118614196777\n",
      "Epoch 2086, Loss: 2.6585395336151123, Final Batch Loss: 0.6755087971687317\n",
      "Epoch 2087, Loss: 2.348616451025009, Final Batch Loss: 0.3379577100276947\n",
      "Epoch 2088, Loss: 2.4524187445640564, Final Batch Loss: 0.43544802069664\n",
      "Epoch 2089, Loss: 2.28815421462059, Final Batch Loss: 0.43173137307167053\n",
      "Epoch 2090, Loss: 2.0181643664836884, Final Batch Loss: 0.18856987357139587\n",
      "Epoch 2091, Loss: 2.3955895602703094, Final Batch Loss: 0.5454381704330444\n",
      "Epoch 2092, Loss: 2.268981158733368, Final Batch Loss: 0.2946702837944031\n",
      "Epoch 2093, Loss: 2.4935691356658936, Final Batch Loss: 0.699196457862854\n",
      "Epoch 2094, Loss: 2.54544335603714, Final Batch Loss: 0.6375135183334351\n",
      "Epoch 2095, Loss: 2.520630955696106, Final Batch Loss: 0.6953431963920593\n",
      "Epoch 2096, Loss: 2.303624391555786, Final Batch Loss: 0.43340715765953064\n",
      "Epoch 2097, Loss: 2.520506829023361, Final Batch Loss: 0.6625218391418457\n",
      "Epoch 2098, Loss: 2.52044540643692, Final Batch Loss: 0.5318666100502014\n",
      "Epoch 2099, Loss: 2.327867031097412, Final Batch Loss: 0.4624888300895691\n",
      "Epoch 2100, Loss: 2.1432480216026306, Final Batch Loss: 0.37493783235549927\n",
      "Epoch 2101, Loss: 2.208516001701355, Final Batch Loss: 0.2953438460826874\n",
      "Epoch 2102, Loss: 2.1338539123535156, Final Batch Loss: 0.2257319688796997\n",
      "Epoch 2103, Loss: 2.0556904822587967, Final Batch Loss: 0.24972639977931976\n",
      "Epoch 2104, Loss: 2.182248055934906, Final Batch Loss: 0.3139399588108063\n",
      "Epoch 2105, Loss: 2.160222679376602, Final Batch Loss: 0.33830884099006653\n",
      "Epoch 2106, Loss: 2.5626841485500336, Final Batch Loss: 0.7289509177207947\n",
      "Epoch 2107, Loss: 2.276794046163559, Final Batch Loss: 0.5392753481864929\n",
      "Epoch 2108, Loss: 2.0742078721523285, Final Batch Loss: 0.3369465470314026\n",
      "Epoch 2109, Loss: 2.3791926205158234, Final Batch Loss: 0.5407267212867737\n",
      "Epoch 2110, Loss: 2.3551494777202606, Final Batch Loss: 0.34471556544303894\n",
      "Epoch 2111, Loss: 2.9284389913082123, Final Batch Loss: 0.8315736651420593\n",
      "Epoch 2112, Loss: 2.9572747349739075, Final Batch Loss: 0.7246466279029846\n",
      "Epoch 2113, Loss: 2.5040380656719208, Final Batch Loss: 0.5133239030838013\n",
      "Epoch 2114, Loss: 2.3660331070423126, Final Batch Loss: 0.49304941296577454\n",
      "Epoch 2115, Loss: 2.322779595851898, Final Batch Loss: 0.40747734904289246\n",
      "Epoch 2116, Loss: 2.2424391508102417, Final Batch Loss: 0.3977832794189453\n",
      "Epoch 2117, Loss: 2.3221521973609924, Final Batch Loss: 0.41518086194992065\n",
      "Epoch 2118, Loss: 2.141321748495102, Final Batch Loss: 0.32014206051826477\n",
      "Epoch 2119, Loss: 2.1234975159168243, Final Batch Loss: 0.17746934294700623\n",
      "Epoch 2120, Loss: 2.237407386302948, Final Batch Loss: 0.4734880030155182\n",
      "Epoch 2121, Loss: 2.117709457874298, Final Batch Loss: 0.3274056613445282\n",
      "Epoch 2122, Loss: 2.3047042787075043, Final Batch Loss: 0.5333544611930847\n",
      "Epoch 2123, Loss: 1.9753485172986984, Final Batch Loss: 0.19842158257961273\n",
      "Epoch 2124, Loss: 2.2594750225543976, Final Batch Loss: 0.44841986894607544\n",
      "Epoch 2125, Loss: 2.087470233440399, Final Batch Loss: 0.27808791399002075\n",
      "Epoch 2126, Loss: 2.2619414627552032, Final Batch Loss: 0.523776113986969\n",
      "Epoch 2127, Loss: 2.3569948971271515, Final Batch Loss: 0.6170931458473206\n",
      "Epoch 2128, Loss: 2.6283487677574158, Final Batch Loss: 0.720888614654541\n",
      "Epoch 2129, Loss: 2.5487936437129974, Final Batch Loss: 0.6615110039710999\n",
      "Epoch 2130, Loss: 2.354550302028656, Final Batch Loss: 0.4965032935142517\n",
      "Epoch 2131, Loss: 2.215472459793091, Final Batch Loss: 0.3022501468658447\n",
      "Epoch 2132, Loss: 2.3690634667873383, Final Batch Loss: 0.5388758182525635\n",
      "Epoch 2133, Loss: 2.534905433654785, Final Batch Loss: 0.6504572033882141\n",
      "Epoch 2134, Loss: 2.337519347667694, Final Batch Loss: 0.48468202352523804\n",
      "Epoch 2135, Loss: 2.249301850795746, Final Batch Loss: 0.32268911600112915\n",
      "Epoch 2136, Loss: 2.2334267497062683, Final Batch Loss: 0.2455136775970459\n",
      "Epoch 2137, Loss: 2.6588751077651978, Final Batch Loss: 0.8560217022895813\n",
      "Epoch 2138, Loss: 2.501734346151352, Final Batch Loss: 0.6242126822471619\n",
      "Epoch 2139, Loss: 2.0959824323654175, Final Batch Loss: 0.21902430057525635\n",
      "Epoch 2140, Loss: 2.191855788230896, Final Batch Loss: 0.3907950818538666\n",
      "Epoch 2141, Loss: 2.3389753997325897, Final Batch Loss: 0.5525282025337219\n",
      "Epoch 2142, Loss: 2.1481679677963257, Final Batch Loss: 0.37312692403793335\n",
      "Epoch 2143, Loss: 2.251840204000473, Final Batch Loss: 0.30434584617614746\n",
      "Epoch 2144, Loss: 2.562801271677017, Final Batch Loss: 0.7810754776000977\n",
      "Epoch 2145, Loss: 2.4403916895389557, Final Batch Loss: 0.5821863412857056\n",
      "Epoch 2146, Loss: 2.4963142573833466, Final Batch Loss: 0.7351017594337463\n",
      "Epoch 2147, Loss: 2.0233642607927322, Final Batch Loss: 0.2211996465921402\n",
      "Epoch 2148, Loss: 2.346537321805954, Final Batch Loss: 0.5990610718727112\n",
      "Epoch 2149, Loss: 2.3975282311439514, Final Batch Loss: 0.6765680313110352\n",
      "Epoch 2150, Loss: 2.5367843210697174, Final Batch Loss: 0.8334048390388489\n",
      "Epoch 2151, Loss: 2.0901471078395844, Final Batch Loss: 0.20307302474975586\n",
      "Epoch 2152, Loss: 2.3667542934417725, Final Batch Loss: 0.5108686685562134\n",
      "Epoch 2153, Loss: 2.115807741880417, Final Batch Loss: 0.3255135416984558\n",
      "Epoch 2154, Loss: 2.427919328212738, Final Batch Loss: 0.5579167604446411\n",
      "Epoch 2155, Loss: 1.9679135829210281, Final Batch Loss: 0.2122618407011032\n",
      "Epoch 2156, Loss: 2.5954699218273163, Final Batch Loss: 0.6647717356681824\n",
      "Epoch 2157, Loss: 2.5835833847522736, Final Batch Loss: 0.6994159817695618\n",
      "Epoch 2158, Loss: 2.1085134148597717, Final Batch Loss: 0.24803468585014343\n",
      "Epoch 2159, Loss: 2.071123167872429, Final Batch Loss: 0.23542533814907074\n",
      "Epoch 2160, Loss: 2.0274520814418793, Final Batch Loss: 0.28503718972206116\n",
      "Epoch 2161, Loss: 2.0504237711429596, Final Batch Loss: 0.26739659905433655\n",
      "Epoch 2162, Loss: 2.506931096315384, Final Batch Loss: 0.6877257227897644\n",
      "Epoch 2163, Loss: 1.9477558583021164, Final Batch Loss: 0.10618232190608978\n",
      "Epoch 2164, Loss: 2.230057954788208, Final Batch Loss: 0.3727494180202484\n",
      "Epoch 2165, Loss: 2.3764744102954865, Final Batch Loss: 0.4880213439464569\n",
      "Epoch 2166, Loss: 2.3060933649539948, Final Batch Loss: 0.5249776244163513\n",
      "Epoch 2167, Loss: 2.2210143506526947, Final Batch Loss: 0.3995989263057709\n",
      "Epoch 2168, Loss: 2.734282523393631, Final Batch Loss: 0.843481719493866\n",
      "Epoch 2169, Loss: 2.105045884847641, Final Batch Loss: 0.30960342288017273\n",
      "Epoch 2170, Loss: 2.398911714553833, Final Batch Loss: 0.4532599151134491\n",
      "Epoch 2171, Loss: 2.39591121673584, Final Batch Loss: 0.6414378881454468\n",
      "Epoch 2172, Loss: 2.290184587240219, Final Batch Loss: 0.46944865584373474\n",
      "Epoch 2173, Loss: 2.520696133375168, Final Batch Loss: 0.7590485215187073\n",
      "Epoch 2174, Loss: 2.3365057706832886, Final Batch Loss: 0.5407634377479553\n",
      "Epoch 2175, Loss: 2.501150667667389, Final Batch Loss: 0.5388638377189636\n",
      "Epoch 2176, Loss: 2.3717198371887207, Final Batch Loss: 0.4664899408817291\n",
      "Epoch 2177, Loss: 2.3828297555446625, Final Batch Loss: 0.6071124076843262\n",
      "Epoch 2178, Loss: 2.6602637469768524, Final Batch Loss: 0.8625503778457642\n",
      "Epoch 2179, Loss: 2.3813653588294983, Final Batch Loss: 0.41955849528312683\n",
      "Epoch 2180, Loss: 2.4264549911022186, Final Batch Loss: 0.45957666635513306\n",
      "Epoch 2181, Loss: 2.547251731157303, Final Batch Loss: 0.6892949342727661\n",
      "Epoch 2182, Loss: 2.204115867614746, Final Batch Loss: 0.36546632647514343\n",
      "Epoch 2183, Loss: 2.27464896440506, Final Batch Loss: 0.4782440960407257\n",
      "Epoch 2184, Loss: 2.410833418369293, Final Batch Loss: 0.6134957075119019\n",
      "Epoch 2185, Loss: 1.9353421777486801, Final Batch Loss: 0.2190217524766922\n",
      "Epoch 2186, Loss: 2.199638932943344, Final Batch Loss: 0.36789625883102417\n",
      "Epoch 2187, Loss: 1.9991164207458496, Final Batch Loss: 0.19012925028800964\n",
      "Epoch 2188, Loss: 2.373722553253174, Final Batch Loss: 0.5045545697212219\n",
      "Epoch 2189, Loss: 2.2474798560142517, Final Batch Loss: 0.4178597331047058\n",
      "Epoch 2190, Loss: 2.3632087111473083, Final Batch Loss: 0.5180043578147888\n",
      "Epoch 2191, Loss: 2.425257831811905, Final Batch Loss: 0.6362834572792053\n",
      "Epoch 2192, Loss: 2.115206718444824, Final Batch Loss: 0.27474910020828247\n",
      "Epoch 2193, Loss: 2.227179616689682, Final Batch Loss: 0.4949175715446472\n",
      "Epoch 2194, Loss: 2.0032352805137634, Final Batch Loss: 0.2367078959941864\n",
      "Epoch 2195, Loss: 2.4140379428863525, Final Batch Loss: 0.5338204503059387\n",
      "Epoch 2196, Loss: 2.503130078315735, Final Batch Loss: 0.737443745136261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2197, Loss: 2.128837287425995, Final Batch Loss: 0.43250277638435364\n",
      "Epoch 2198, Loss: 2.258399099111557, Final Batch Loss: 0.5157923698425293\n",
      "Epoch 2199, Loss: 2.0476696491241455, Final Batch Loss: 0.3321516215801239\n",
      "Epoch 2200, Loss: 2.559229791164398, Final Batch Loss: 0.6497908234596252\n",
      "Epoch 2201, Loss: 2.4922046959400177, Final Batch Loss: 0.6673588752746582\n",
      "Epoch 2202, Loss: 2.154192417860031, Final Batch Loss: 0.2984609603881836\n",
      "Epoch 2203, Loss: 2.3934612572193146, Final Batch Loss: 0.599977433681488\n",
      "Epoch 2204, Loss: 2.361304610967636, Final Batch Loss: 0.4423540234565735\n",
      "Epoch 2205, Loss: 2.6108132004737854, Final Batch Loss: 0.761455237865448\n",
      "Epoch 2206, Loss: 2.2737047374248505, Final Batch Loss: 0.48143911361694336\n",
      "Epoch 2207, Loss: 2.6109458804130554, Final Batch Loss: 0.7918576002120972\n",
      "Epoch 2208, Loss: 2.647075831890106, Final Batch Loss: 0.7640272974967957\n",
      "Epoch 2209, Loss: 2.208318531513214, Final Batch Loss: 0.37241798639297485\n",
      "Epoch 2210, Loss: 2.282549798488617, Final Batch Loss: 0.4106813073158264\n",
      "Epoch 2211, Loss: 2.264632433652878, Final Batch Loss: 0.4904329478740692\n",
      "Epoch 2212, Loss: 2.182477831840515, Final Batch Loss: 0.39012855291366577\n",
      "Epoch 2213, Loss: 2.1364420652389526, Final Batch Loss: 0.43758898973464966\n",
      "Epoch 2214, Loss: 2.0018335580825806, Final Batch Loss: 0.24772360920906067\n",
      "Epoch 2215, Loss: 2.230585962533951, Final Batch Loss: 0.40060392022132874\n",
      "Epoch 2216, Loss: 2.3392225205898285, Final Batch Loss: 0.5029352903366089\n",
      "Epoch 2217, Loss: 2.2643105685710907, Final Batch Loss: 0.4217798113822937\n",
      "Epoch 2218, Loss: 2.0452801436185837, Final Batch Loss: 0.18524669110774994\n",
      "Epoch 2219, Loss: 2.2225389182567596, Final Batch Loss: 0.3998944163322449\n",
      "Epoch 2220, Loss: 2.0360997915267944, Final Batch Loss: 0.26655152440071106\n",
      "Epoch 2221, Loss: 2.1503075063228607, Final Batch Loss: 0.42899081110954285\n",
      "Epoch 2222, Loss: 2.3818124532699585, Final Batch Loss: 0.5161961317062378\n",
      "Epoch 2223, Loss: 2.5466253459453583, Final Batch Loss: 0.7124713659286499\n",
      "Epoch 2224, Loss: 2.1960445642471313, Final Batch Loss: 0.4651983678340912\n",
      "Epoch 2225, Loss: 2.2411026060581207, Final Batch Loss: 0.4353705048561096\n",
      "Epoch 2226, Loss: 2.4822229743003845, Final Batch Loss: 0.620331883430481\n",
      "Epoch 2227, Loss: 2.1784275472164154, Final Batch Loss: 0.4320453107357025\n",
      "Epoch 2228, Loss: 2.547254055738449, Final Batch Loss: 0.8620540499687195\n",
      "Epoch 2229, Loss: 1.9360974878072739, Final Batch Loss: 0.16057519614696503\n",
      "Epoch 2230, Loss: 2.4285076558589935, Final Batch Loss: 0.6105323433876038\n",
      "Epoch 2231, Loss: 1.9545584470033646, Final Batch Loss: 0.20775626599788666\n",
      "Epoch 2232, Loss: 2.1544520556926727, Final Batch Loss: 0.36183613538742065\n",
      "Epoch 2233, Loss: 2.373879075050354, Final Batch Loss: 0.5501075983047485\n",
      "Epoch 2234, Loss: 2.1964770555496216, Final Batch Loss: 0.3992238938808441\n",
      "Epoch 2235, Loss: 2.3967897593975067, Final Batch Loss: 0.5012126564979553\n",
      "Epoch 2236, Loss: 2.3504072427749634, Final Batch Loss: 0.5548228025436401\n",
      "Epoch 2237, Loss: 2.3001181185245514, Final Batch Loss: 0.42807435989379883\n",
      "Epoch 2238, Loss: 2.0780223309993744, Final Batch Loss: 0.32560697197914124\n",
      "Epoch 2239, Loss: 1.866453379392624, Final Batch Loss: 0.1342376470565796\n",
      "Epoch 2240, Loss: 2.263083577156067, Final Batch Loss: 0.5392356514930725\n",
      "Epoch 2241, Loss: 1.9499961286783218, Final Batch Loss: 0.18428216874599457\n",
      "Epoch 2242, Loss: 2.2295970022678375, Final Batch Loss: 0.4538431465625763\n",
      "Epoch 2243, Loss: 2.178440570831299, Final Batch Loss: 0.36293739080429077\n",
      "Epoch 2244, Loss: 2.046482563018799, Final Batch Loss: 0.2635754942893982\n",
      "Epoch 2245, Loss: 2.6968376636505127, Final Batch Loss: 0.9044527411460876\n",
      "Epoch 2246, Loss: 2.3730440735816956, Final Batch Loss: 0.5599232316017151\n",
      "Epoch 2247, Loss: 2.036623328924179, Final Batch Loss: 0.2720588445663452\n",
      "Epoch 2248, Loss: 2.0189825370907784, Final Batch Loss: 0.09320194274187088\n",
      "Epoch 2249, Loss: 2.4346276819705963, Final Batch Loss: 0.5886315107345581\n",
      "Epoch 2250, Loss: 2.0876194834709167, Final Batch Loss: 0.29941439628601074\n",
      "Epoch 2251, Loss: 2.054765045642853, Final Batch Loss: 0.17242708802223206\n",
      "Epoch 2252, Loss: 2.2640193104743958, Final Batch Loss: 0.34839925169944763\n",
      "Epoch 2253, Loss: 2.637797713279724, Final Batch Loss: 0.7632396817207336\n",
      "Epoch 2254, Loss: 1.995786875486374, Final Batch Loss: 0.1992325782775879\n",
      "Epoch 2255, Loss: 2.4505984485149384, Final Batch Loss: 0.18819740414619446\n",
      "Epoch 2256, Loss: 2.3103584945201874, Final Batch Loss: 0.31647706031799316\n",
      "Epoch 2257, Loss: 2.273054540157318, Final Batch Loss: 0.46133676171302795\n",
      "Epoch 2258, Loss: 2.215842455625534, Final Batch Loss: 0.33599337935447693\n",
      "Epoch 2259, Loss: 2.530893474817276, Final Batch Loss: 0.6063517928123474\n",
      "Epoch 2260, Loss: 2.1055651009082794, Final Batch Loss: 0.36422792077064514\n",
      "Epoch 2261, Loss: 2.121583968400955, Final Batch Loss: 0.36422020196914673\n",
      "Epoch 2262, Loss: 2.1812936663627625, Final Batch Loss: 0.4799523651599884\n",
      "Epoch 2263, Loss: 2.3251003324985504, Final Batch Loss: 0.6483698487281799\n",
      "Epoch 2264, Loss: 1.9694004356861115, Final Batch Loss: 0.20148435235023499\n",
      "Epoch 2265, Loss: 1.9961947053670883, Final Batch Loss: 0.24357400834560394\n",
      "Epoch 2266, Loss: 2.441037893295288, Final Batch Loss: 0.6374896168708801\n",
      "Epoch 2267, Loss: 2.0442698895931244, Final Batch Loss: 0.36743292212486267\n",
      "Epoch 2268, Loss: 2.1105481386184692, Final Batch Loss: 0.3379985988140106\n",
      "Epoch 2269, Loss: 2.0819039046764374, Final Batch Loss: 0.3064858913421631\n",
      "Epoch 2270, Loss: 1.8980505615472794, Final Batch Loss: 0.18370212614536285\n",
      "Epoch 2271, Loss: 2.278087079524994, Final Batch Loss: 0.40994992852211\n",
      "Epoch 2272, Loss: 2.2894919216632843, Final Batch Loss: 0.3633897304534912\n",
      "Epoch 2273, Loss: 2.0264802277088165, Final Batch Loss: 0.251619428396225\n",
      "Epoch 2274, Loss: 1.974745124578476, Final Batch Loss: 0.1718730628490448\n",
      "Epoch 2275, Loss: 2.155183881521225, Final Batch Loss: 0.410271018743515\n",
      "Epoch 2276, Loss: 2.1636196076869965, Final Batch Loss: 0.33098429441452026\n",
      "Epoch 2277, Loss: 2.604214698076248, Final Batch Loss: 0.6544978022575378\n",
      "Epoch 2278, Loss: 2.51107981801033, Final Batch Loss: 0.8026856184005737\n",
      "Epoch 2279, Loss: 2.1860026121139526, Final Batch Loss: 0.4430255889892578\n",
      "Epoch 2280, Loss: 2.301743060350418, Final Batch Loss: 0.44699111580848694\n",
      "Epoch 2281, Loss: 2.418026626110077, Final Batch Loss: 0.7406719923019409\n",
      "Epoch 2282, Loss: 2.2857438027858734, Final Batch Loss: 0.5145156979560852\n",
      "Epoch 2283, Loss: 2.2082941234111786, Final Batch Loss: 0.3982459604740143\n",
      "Epoch 2284, Loss: 2.081144154071808, Final Batch Loss: 0.2891665995121002\n",
      "Epoch 2285, Loss: 2.319978713989258, Final Batch Loss: 0.6332902908325195\n",
      "Epoch 2286, Loss: 2.3381735384464264, Final Batch Loss: 0.5816637873649597\n",
      "Epoch 2287, Loss: 2.62820503115654, Final Batch Loss: 0.7917360663414001\n",
      "Epoch 2288, Loss: 2.3249362111091614, Final Batch Loss: 0.6837064623832703\n",
      "Epoch 2289, Loss: 1.9150990694761276, Final Batch Loss: 0.19439275562763214\n",
      "Epoch 2290, Loss: 2.2068585455417633, Final Batch Loss: 0.515921950340271\n",
      "Epoch 2291, Loss: 2.625686138868332, Final Batch Loss: 0.9089458584785461\n",
      "Epoch 2292, Loss: 1.9723570346832275, Final Batch Loss: 0.23127532005310059\n",
      "Epoch 2293, Loss: 1.9349420368671417, Final Batch Loss: 0.20009127259254456\n",
      "Epoch 2294, Loss: 2.539483815431595, Final Batch Loss: 0.7777529954910278\n",
      "Epoch 2295, Loss: 2.2957980930805206, Final Batch Loss: 0.5869424939155579\n",
      "Epoch 2296, Loss: 2.059772253036499, Final Batch Loss: 0.31548410654067993\n",
      "Epoch 2297, Loss: 2.2430717051029205, Final Batch Loss: 0.36798396706581116\n",
      "Epoch 2298, Loss: 2.176033318042755, Final Batch Loss: 0.2535276710987091\n",
      "Epoch 2299, Loss: 1.9577717781066895, Final Batch Loss: 0.1565030813217163\n",
      "Epoch 2300, Loss: 2.1456648111343384, Final Batch Loss: 0.3583974242210388\n",
      "Epoch 2301, Loss: 2.061893120408058, Final Batch Loss: 0.19130943715572357\n",
      "Epoch 2302, Loss: 2.4360252618789673, Final Batch Loss: 0.6906470060348511\n",
      "Epoch 2303, Loss: 2.1665171086788177, Final Batch Loss: 0.39252233505249023\n",
      "Epoch 2304, Loss: 2.1776829957962036, Final Batch Loss: 0.4875759184360504\n",
      "Epoch 2305, Loss: 2.641871839761734, Final Batch Loss: 1.0359431505203247\n",
      "Epoch 2306, Loss: 2.1953071653842926, Final Batch Loss: 0.442685604095459\n",
      "Epoch 2307, Loss: 2.2091641426086426, Final Batch Loss: 0.40179046988487244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2308, Loss: 2.258460193872452, Final Batch Loss: 0.6577926874160767\n",
      "Epoch 2309, Loss: 2.3827092051506042, Final Batch Loss: 0.6330245137214661\n",
      "Epoch 2310, Loss: 2.156973123550415, Final Batch Loss: 0.3092908263206482\n",
      "Epoch 2311, Loss: 2.052410677075386, Final Batch Loss: 0.22905434668064117\n",
      "Epoch 2312, Loss: 2.1339231729507446, Final Batch Loss: 0.37271371483802795\n",
      "Epoch 2313, Loss: 2.0340266823768616, Final Batch Loss: 0.3089229464530945\n",
      "Epoch 2314, Loss: 2.2397642731666565, Final Batch Loss: 0.3724540174007416\n",
      "Epoch 2315, Loss: 2.3807985186576843, Final Batch Loss: 0.6369448900222778\n",
      "Epoch 2316, Loss: 2.235296368598938, Final Batch Loss: 0.5591364502906799\n",
      "Epoch 2317, Loss: 2.4091935753822327, Final Batch Loss: 0.6354103088378906\n",
      "Epoch 2318, Loss: 2.1935538947582245, Final Batch Loss: 0.5056318044662476\n",
      "Epoch 2319, Loss: 1.9571418166160583, Final Batch Loss: 0.2536817193031311\n",
      "Epoch 2320, Loss: 2.3797736763954163, Final Batch Loss: 0.63532954454422\n",
      "Epoch 2321, Loss: 2.016881287097931, Final Batch Loss: 0.2606509327888489\n",
      "Epoch 2322, Loss: 1.9804414808750153, Final Batch Loss: 0.22034141421318054\n",
      "Epoch 2323, Loss: 2.4446583688259125, Final Batch Loss: 0.5617872476577759\n",
      "Epoch 2324, Loss: 2.245929926633835, Final Batch Loss: 0.3287404179573059\n",
      "Epoch 2325, Loss: 2.189395993947983, Final Batch Loss: 0.37661370635032654\n",
      "Epoch 2326, Loss: 2.2560491859912872, Final Batch Loss: 0.5077748894691467\n",
      "Epoch 2327, Loss: 1.9896340370178223, Final Batch Loss: 0.1906350553035736\n",
      "Epoch 2328, Loss: 2.151528298854828, Final Batch Loss: 0.388827383518219\n",
      "Epoch 2329, Loss: 2.1030601263046265, Final Batch Loss: 0.36952370405197144\n",
      "Epoch 2330, Loss: 1.8517821356654167, Final Batch Loss: 0.10939919203519821\n",
      "Epoch 2331, Loss: 1.8310925960540771, Final Batch Loss: 0.12802743911743164\n",
      "Epoch 2332, Loss: 1.910529464483261, Final Batch Loss: 0.1661655604839325\n",
      "Epoch 2333, Loss: 2.1527130901813507, Final Batch Loss: 0.5002197623252869\n",
      "Epoch 2334, Loss: 2.221372604370117, Final Batch Loss: 0.5257743000984192\n",
      "Epoch 2335, Loss: 2.1731400191783905, Final Batch Loss: 0.5020265579223633\n",
      "Epoch 2336, Loss: 2.1020739376544952, Final Batch Loss: 0.3623267710208893\n",
      "Epoch 2337, Loss: 1.9569106698036194, Final Batch Loss: 0.3097706437110901\n",
      "Epoch 2338, Loss: 2.337461769580841, Final Batch Loss: 0.5559881925582886\n",
      "Epoch 2339, Loss: 2.180924206972122, Final Batch Loss: 0.38861656188964844\n",
      "Epoch 2340, Loss: 2.280824065208435, Final Batch Loss: 0.5457339286804199\n",
      "Epoch 2341, Loss: 2.227977305650711, Final Batch Loss: 0.2875359058380127\n",
      "Epoch 2342, Loss: 1.8765364736318588, Final Batch Loss: 0.18204791843891144\n",
      "Epoch 2343, Loss: 1.9628694355487823, Final Batch Loss: 0.13612577319145203\n",
      "Epoch 2344, Loss: 2.612517535686493, Final Batch Loss: 0.9250279068946838\n",
      "Epoch 2345, Loss: 2.25122007727623, Final Batch Loss: 0.580528736114502\n",
      "Epoch 2346, Loss: 1.739139936864376, Final Batch Loss: 0.042496733367443085\n",
      "Epoch 2347, Loss: 2.2480300962924957, Final Batch Loss: 0.39903566241264343\n",
      "Epoch 2348, Loss: 2.239714950323105, Final Batch Loss: 0.4380917251110077\n",
      "Epoch 2349, Loss: 2.239785999059677, Final Batch Loss: 0.4328193962574005\n",
      "Epoch 2350, Loss: 2.172221064567566, Final Batch Loss: 0.44142475724220276\n",
      "Epoch 2351, Loss: 2.197263777256012, Final Batch Loss: 0.4536021649837494\n",
      "Epoch 2352, Loss: 1.8659704625606537, Final Batch Loss: 0.16348478198051453\n",
      "Epoch 2353, Loss: 2.5407460927963257, Final Batch Loss: 0.824482262134552\n",
      "Epoch 2354, Loss: 1.8195670694112778, Final Batch Loss: 0.16040818393230438\n",
      "Epoch 2355, Loss: 2.061170816421509, Final Batch Loss: 0.3262438178062439\n",
      "Epoch 2356, Loss: 2.3120442926883698, Final Batch Loss: 0.5753617286682129\n",
      "Epoch 2357, Loss: 2.077016919851303, Final Batch Loss: 0.35751035809516907\n",
      "Epoch 2358, Loss: 2.533498913049698, Final Batch Loss: 0.9151565432548523\n",
      "Epoch 2359, Loss: 2.3953797817230225, Final Batch Loss: 0.6480414271354675\n",
      "Epoch 2360, Loss: 1.8832189589738846, Final Batch Loss: 0.21671916544437408\n",
      "Epoch 2361, Loss: 2.5705297887325287, Final Batch Loss: 0.8766953349113464\n",
      "Epoch 2362, Loss: 2.193366289138794, Final Batch Loss: 0.3795010447502136\n",
      "Epoch 2363, Loss: 2.2472055554389954, Final Batch Loss: 0.5061451196670532\n",
      "Epoch 2364, Loss: 2.2483968436717987, Final Batch Loss: 0.5744995474815369\n",
      "Epoch 2365, Loss: 2.178353935480118, Final Batch Loss: 0.3513561189174652\n",
      "Epoch 2366, Loss: 2.1629913449287415, Final Batch Loss: 0.3680572211742401\n",
      "Epoch 2367, Loss: 1.9865305721759796, Final Batch Loss: 0.20660844445228577\n",
      "Epoch 2368, Loss: 2.1061600148677826, Final Batch Loss: 0.36758360266685486\n",
      "Epoch 2369, Loss: 1.9880802035331726, Final Batch Loss: 0.2517603933811188\n",
      "Epoch 2370, Loss: 2.363788366317749, Final Batch Loss: 0.46305808424949646\n",
      "Epoch 2371, Loss: 2.041863441467285, Final Batch Loss: 0.3241216242313385\n",
      "Epoch 2372, Loss: 2.0252875983715057, Final Batch Loss: 0.2543199062347412\n",
      "Epoch 2373, Loss: 2.393759101629257, Final Batch Loss: 0.4290139079093933\n",
      "Epoch 2374, Loss: 2.120690107345581, Final Batch Loss: 0.38671717047691345\n",
      "Epoch 2375, Loss: 1.9873756170272827, Final Batch Loss: 0.2618209421634674\n",
      "Epoch 2376, Loss: 2.165811836719513, Final Batch Loss: 0.315976619720459\n",
      "Epoch 2377, Loss: 1.9015422314405441, Final Batch Loss: 0.21959884464740753\n",
      "Epoch 2378, Loss: 2.1849431097507477, Final Batch Loss: 0.595029354095459\n",
      "Epoch 2379, Loss: 1.9628859162330627, Final Batch Loss: 0.27814093232154846\n",
      "Epoch 2380, Loss: 2.1169204115867615, Final Batch Loss: 0.41881829500198364\n",
      "Epoch 2381, Loss: 1.9576995372772217, Final Batch Loss: 0.28777509927749634\n",
      "Epoch 2382, Loss: 1.8695142567157745, Final Batch Loss: 0.22740241885185242\n",
      "Epoch 2383, Loss: 1.8918639868497849, Final Batch Loss: 0.20534859597682953\n",
      "Epoch 2384, Loss: 2.109483629465103, Final Batch Loss: 0.3826129734516144\n",
      "Epoch 2385, Loss: 2.3640032708644867, Final Batch Loss: 0.7000505328178406\n",
      "Epoch 2386, Loss: 2.461139053106308, Final Batch Loss: 0.7233439683914185\n",
      "Epoch 2387, Loss: 2.297069698572159, Final Batch Loss: 0.4822167754173279\n",
      "Epoch 2388, Loss: 1.9239287450909615, Final Batch Loss: 0.12460575252771378\n",
      "Epoch 2389, Loss: 2.20438152551651, Final Batch Loss: 0.4440579414367676\n",
      "Epoch 2390, Loss: 2.263648957014084, Final Batch Loss: 0.48133447766304016\n",
      "Epoch 2391, Loss: 2.208411693572998, Final Batch Loss: 0.5099284052848816\n",
      "Epoch 2392, Loss: 2.314832240343094, Final Batch Loss: 0.6234235763549805\n",
      "Epoch 2393, Loss: 2.5098018050193787, Final Batch Loss: 0.7097492814064026\n",
      "Epoch 2394, Loss: 1.9342028573155403, Final Batch Loss: 0.1179174855351448\n",
      "Epoch 2395, Loss: 2.138672351837158, Final Batch Loss: 0.5100847482681274\n",
      "Epoch 2396, Loss: 1.8540247529745102, Final Batch Loss: 0.24128572642803192\n",
      "Epoch 2397, Loss: 2.132075250148773, Final Batch Loss: 0.33716264367103577\n",
      "Epoch 2398, Loss: 1.837399147450924, Final Batch Loss: 0.1149774119257927\n",
      "Epoch 2399, Loss: 2.3033876717090607, Final Batch Loss: 0.6877026557922363\n",
      "Epoch 2400, Loss: 2.677563488483429, Final Batch Loss: 1.0791071653366089\n",
      "Epoch 2401, Loss: 2.094607412815094, Final Batch Loss: 0.4370613098144531\n",
      "Epoch 2402, Loss: 1.9710123799741268, Final Batch Loss: 0.045513447374105453\n",
      "Epoch 2403, Loss: 2.2907232642173767, Final Batch Loss: 0.4763903021812439\n",
      "Epoch 2404, Loss: 2.293009400367737, Final Batch Loss: 0.3722216784954071\n",
      "Epoch 2405, Loss: 2.3973875045776367, Final Batch Loss: 0.6052969694137573\n",
      "Epoch 2406, Loss: 2.272400379180908, Final Batch Loss: 0.5436869859695435\n",
      "Epoch 2407, Loss: 2.003984123468399, Final Batch Loss: 0.3081834614276886\n",
      "Epoch 2408, Loss: 2.2929647266864777, Final Batch Loss: 0.495866984128952\n",
      "Epoch 2409, Loss: 2.392904818058014, Final Batch Loss: 0.621478259563446\n",
      "Epoch 2410, Loss: 2.645217627286911, Final Batch Loss: 0.855178952217102\n",
      "Epoch 2411, Loss: 2.343662053346634, Final Batch Loss: 0.5062572956085205\n",
      "Epoch 2412, Loss: 2.3813765347003937, Final Batch Loss: 0.6105071306228638\n",
      "Epoch 2413, Loss: 2.379974663257599, Final Batch Loss: 0.6722625494003296\n",
      "Epoch 2414, Loss: 2.2251990139484406, Final Batch Loss: 0.4487583637237549\n",
      "Epoch 2415, Loss: 2.075982540845871, Final Batch Loss: 0.34863996505737305\n",
      "Epoch 2416, Loss: 1.8867618590593338, Final Batch Loss: 0.2293035238981247\n",
      "Epoch 2417, Loss: 1.8365365713834763, Final Batch Loss: 0.24220533668994904\n",
      "Epoch 2418, Loss: 2.0501502454280853, Final Batch Loss: 0.33968809247016907\n",
      "Epoch 2419, Loss: 2.034675806760788, Final Batch Loss: 0.2645055949687958\n",
      "Epoch 2420, Loss: 2.148682713508606, Final Batch Loss: 0.3900475800037384\n",
      "Epoch 2421, Loss: 1.89667047560215, Final Batch Loss: 0.06876085698604584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2422, Loss: 2.218241363763809, Final Batch Loss: 0.5021147131919861\n",
      "Epoch 2423, Loss: 2.141904443502426, Final Batch Loss: 0.45464274287223816\n",
      "Epoch 2424, Loss: 2.1937107741832733, Final Batch Loss: 0.46515360474586487\n",
      "Epoch 2425, Loss: 2.269559621810913, Final Batch Loss: 0.43035826086997986\n",
      "Epoch 2426, Loss: 2.0738607943058014, Final Batch Loss: 0.32025763392448425\n",
      "Epoch 2427, Loss: 2.4835540056228638, Final Batch Loss: 0.7555252313613892\n",
      "Epoch 2428, Loss: 2.2108408510684967, Final Batch Loss: 0.3847558796405792\n",
      "Epoch 2429, Loss: 1.9491267502307892, Final Batch Loss: 0.24084123969078064\n",
      "Epoch 2430, Loss: 2.29887193441391, Final Batch Loss: 0.6223275065422058\n",
      "Epoch 2431, Loss: 2.0281459987163544, Final Batch Loss: 0.2589742839336395\n",
      "Epoch 2432, Loss: 2.247987449169159, Final Batch Loss: 0.5984667539596558\n",
      "Epoch 2433, Loss: 2.099999040365219, Final Batch Loss: 0.3686273396015167\n",
      "Epoch 2434, Loss: 1.8600173890590668, Final Batch Loss: 0.21009239554405212\n",
      "Epoch 2435, Loss: 1.887398548424244, Final Batch Loss: 0.09835521131753922\n",
      "Epoch 2436, Loss: 2.332347720861435, Final Batch Loss: 0.7548068761825562\n",
      "Epoch 2437, Loss: 2.027586966753006, Final Batch Loss: 0.34174755215644836\n",
      "Epoch 2438, Loss: 2.7357603013515472, Final Batch Loss: 0.8698301911354065\n",
      "Epoch 2439, Loss: 2.046467214822769, Final Batch Loss: 0.29688888788223267\n",
      "Epoch 2440, Loss: 1.9022021889686584, Final Batch Loss: 0.250406414270401\n",
      "Epoch 2441, Loss: 2.6682628393173218, Final Batch Loss: 0.9777344465255737\n",
      "Epoch 2442, Loss: 2.282456338405609, Final Batch Loss: 0.5577647686004639\n",
      "Epoch 2443, Loss: 1.827376201748848, Final Batch Loss: 0.0908081978559494\n",
      "Epoch 2444, Loss: 2.7874301373958588, Final Batch Loss: 1.0610731840133667\n",
      "Epoch 2445, Loss: 1.9583100527524948, Final Batch Loss: 0.20895560085773468\n",
      "Epoch 2446, Loss: 2.551400810480118, Final Batch Loss: 0.7567747831344604\n",
      "Epoch 2447, Loss: 1.9880418181419373, Final Batch Loss: 0.2769372761249542\n",
      "Epoch 2448, Loss: 2.6571492552757263, Final Batch Loss: 1.0032869577407837\n",
      "Epoch 2449, Loss: 2.009121060371399, Final Batch Loss: 0.46568113565444946\n",
      "Epoch 2450, Loss: 1.8864507377147675, Final Batch Loss: 0.289045512676239\n",
      "Epoch 2451, Loss: 2.106219530105591, Final Batch Loss: 0.315260648727417\n",
      "Epoch 2452, Loss: 2.084388107061386, Final Batch Loss: 0.37005093693733215\n",
      "Epoch 2453, Loss: 2.068602830171585, Final Batch Loss: 0.31478238105773926\n",
      "Epoch 2454, Loss: 2.0403667837381363, Final Batch Loss: 0.21332375705242157\n",
      "Epoch 2455, Loss: 2.2030908465385437, Final Batch Loss: 0.41908860206604004\n",
      "Epoch 2456, Loss: 2.430338740348816, Final Batch Loss: 0.6760342717170715\n",
      "Epoch 2457, Loss: 2.180732309818268, Final Batch Loss: 0.4040907919406891\n",
      "Epoch 2458, Loss: 2.6703583002090454, Final Batch Loss: 0.7835465669631958\n",
      "Epoch 2459, Loss: 1.8874436020851135, Final Batch Loss: 0.25195834040641785\n",
      "Epoch 2460, Loss: 2.548053801059723, Final Batch Loss: 0.8994235992431641\n",
      "Epoch 2461, Loss: 2.067406803369522, Final Batch Loss: 0.4616861939430237\n",
      "Epoch 2462, Loss: 2.0512931048870087, Final Batch Loss: 0.29549407958984375\n",
      "Epoch 2463, Loss: 2.1837210953235626, Final Batch Loss: 0.25576749444007874\n",
      "Epoch 2464, Loss: 1.9944220185279846, Final Batch Loss: 0.2984330952167511\n",
      "Epoch 2465, Loss: 2.2409359514713287, Final Batch Loss: 0.5676137208938599\n",
      "Epoch 2466, Loss: 2.260867953300476, Final Batch Loss: 0.4574943482875824\n",
      "Epoch 2467, Loss: 2.222925305366516, Final Batch Loss: 0.49742767214775085\n",
      "Epoch 2468, Loss: 2.3727293610572815, Final Batch Loss: 0.6993914842605591\n",
      "Epoch 2469, Loss: 2.0044599175453186, Final Batch Loss: 0.38882970809936523\n",
      "Epoch 2470, Loss: 2.1181766986846924, Final Batch Loss: 0.5141924619674683\n",
      "Epoch 2471, Loss: 2.390907943248749, Final Batch Loss: 0.6653370261192322\n",
      "Epoch 2472, Loss: 2.1467228829860687, Final Batch Loss: 0.3915955722332001\n",
      "Epoch 2473, Loss: 1.8630254864692688, Final Batch Loss: 0.23813167214393616\n",
      "Epoch 2474, Loss: 1.9542065560817719, Final Batch Loss: 0.3557988703250885\n",
      "Epoch 2475, Loss: 2.269705981016159, Final Batch Loss: 0.46759599447250366\n",
      "Epoch 2476, Loss: 2.140108823776245, Final Batch Loss: 0.5436782240867615\n",
      "Epoch 2477, Loss: 2.11535981297493, Final Batch Loss: 0.32557445764541626\n",
      "Epoch 2478, Loss: 2.066784054040909, Final Batch Loss: 0.4012020230293274\n",
      "Epoch 2479, Loss: 2.1299287378787994, Final Batch Loss: 0.34617742896080017\n",
      "Epoch 2480, Loss: 2.5001164972782135, Final Batch Loss: 0.8469899296760559\n",
      "Epoch 2481, Loss: 2.2683575451374054, Final Batch Loss: 0.5455079674720764\n",
      "Epoch 2482, Loss: 2.1506682485342026, Final Batch Loss: 0.23091788589954376\n",
      "Epoch 2483, Loss: 2.3139249980449677, Final Batch Loss: 0.4195484519004822\n",
      "Epoch 2484, Loss: 2.402101546525955, Final Batch Loss: 0.6855678558349609\n",
      "Epoch 2485, Loss: 2.4810441732406616, Final Batch Loss: 0.7066732048988342\n",
      "Epoch 2486, Loss: 3.000814139842987, Final Batch Loss: 1.1270825862884521\n",
      "Epoch 2487, Loss: 2.1962296068668365, Final Batch Loss: 0.3336891531944275\n",
      "Epoch 2488, Loss: 2.4023846983909607, Final Batch Loss: 0.500224232673645\n",
      "Epoch 2489, Loss: 2.6980031430721283, Final Batch Loss: 0.7804573774337769\n",
      "Epoch 2490, Loss: 2.392017364501953, Final Batch Loss: 0.6182705163955688\n",
      "Epoch 2491, Loss: 2.4602942764759064, Final Batch Loss: 0.4378969669342041\n",
      "Epoch 2492, Loss: 2.820986270904541, Final Batch Loss: 0.7125163078308105\n",
      "Epoch 2493, Loss: 2.2615044564008713, Final Batch Loss: 0.16839660704135895\n",
      "Epoch 2494, Loss: 2.5750471353530884, Final Batch Loss: 0.7153007388114929\n",
      "Epoch 2495, Loss: 2.0975007712841034, Final Batch Loss: 0.31211045384407043\n",
      "Epoch 2496, Loss: 2.215781956911087, Final Batch Loss: 0.3648264408111572\n",
      "Epoch 2497, Loss: 2.696024090051651, Final Batch Loss: 0.8467273712158203\n",
      "Epoch 2498, Loss: 2.132396310567856, Final Batch Loss: 0.37601032853126526\n",
      "Epoch 2499, Loss: 1.9521383792161942, Final Batch Loss: 0.19685561954975128\n",
      "Epoch 2500, Loss: 2.2467454075813293, Final Batch Loss: 0.4684617221355438\n",
      "Epoch 2501, Loss: 2.054620772600174, Final Batch Loss: 0.36596712470054626\n",
      "Epoch 2502, Loss: 2.1236505806446075, Final Batch Loss: 0.31062451004981995\n",
      "Epoch 2503, Loss: 2.285572350025177, Final Batch Loss: 0.49822649359703064\n",
      "Epoch 2504, Loss: 1.9774423241615295, Final Batch Loss: 0.26715919375419617\n",
      "Epoch 2505, Loss: 1.9727161824703217, Final Batch Loss: 0.27646365761756897\n",
      "Epoch 2506, Loss: 2.065250128507614, Final Batch Loss: 0.4372286796569824\n",
      "Epoch 2507, Loss: 1.8348646983504295, Final Batch Loss: 0.12414737790822983\n",
      "Epoch 2508, Loss: 2.1082589626312256, Final Batch Loss: 0.33549585938453674\n",
      "Epoch 2509, Loss: 1.9827998578548431, Final Batch Loss: 0.31900453567504883\n",
      "Epoch 2510, Loss: 2.5181240141391754, Final Batch Loss: 0.816473662853241\n",
      "Epoch 2511, Loss: 2.015345424413681, Final Batch Loss: 0.4204646646976471\n",
      "Epoch 2512, Loss: 2.1392170786857605, Final Batch Loss: 0.4702289402484894\n",
      "Epoch 2513, Loss: 1.9444230943918228, Final Batch Loss: 0.24464987218379974\n",
      "Epoch 2514, Loss: 2.4946600794792175, Final Batch Loss: 0.8291321396827698\n",
      "Epoch 2515, Loss: 2.2296797335147858, Final Batch Loss: 0.6485342383384705\n",
      "Epoch 2516, Loss: 1.9217502027750015, Final Batch Loss: 0.1831677407026291\n",
      "Epoch 2517, Loss: 2.1943401396274567, Final Batch Loss: 0.36668163537979126\n",
      "Epoch 2518, Loss: 2.251300573348999, Final Batch Loss: 0.47341278195381165\n",
      "Epoch 2519, Loss: 2.499832034111023, Final Batch Loss: 0.6685435175895691\n",
      "Epoch 2520, Loss: 2.1175955533981323, Final Batch Loss: 0.5115301609039307\n",
      "Epoch 2521, Loss: 2.1572110652923584, Final Batch Loss: 0.5121435523033142\n",
      "Epoch 2522, Loss: 1.8634956181049347, Final Batch Loss: 0.1692049205303192\n",
      "Epoch 2523, Loss: 2.1726214587688446, Final Batch Loss: 0.5025738477706909\n",
      "Epoch 2524, Loss: 1.792409010231495, Final Batch Loss: 0.11945167928934097\n",
      "Epoch 2525, Loss: 2.2991180419921875, Final Batch Loss: 0.44922661781311035\n",
      "Epoch 2526, Loss: 2.282855838537216, Final Batch Loss: 0.6680736541748047\n",
      "Epoch 2527, Loss: 2.0346925258636475, Final Batch Loss: 0.22955617308616638\n",
      "Epoch 2528, Loss: 1.99674454331398, Final Batch Loss: 0.322735071182251\n",
      "Epoch 2529, Loss: 2.3977766036987305, Final Batch Loss: 0.7239693403244019\n",
      "Epoch 2530, Loss: 2.0085312128067017, Final Batch Loss: 0.3170738220214844\n",
      "Epoch 2531, Loss: 2.002514123916626, Final Batch Loss: 0.4108559489250183\n",
      "Epoch 2532, Loss: 2.6815257370471954, Final Batch Loss: 0.9280884861946106\n",
      "Epoch 2533, Loss: 2.37307471036911, Final Batch Loss: 0.6009799242019653\n",
      "Epoch 2534, Loss: 1.6521752998232841, Final Batch Loss: 0.053500495851039886\n",
      "Epoch 2535, Loss: 2.1819536983966827, Final Batch Loss: 0.5506235361099243\n",
      "Epoch 2536, Loss: 2.0163762271404266, Final Batch Loss: 0.2074289321899414\n",
      "Epoch 2537, Loss: 2.533728927373886, Final Batch Loss: 0.5678231120109558\n",
      "Epoch 2538, Loss: 2.431696802377701, Final Batch Loss: 0.5966872572898865\n",
      "Epoch 2539, Loss: 2.22049942612648, Final Batch Loss: 0.4359065890312195\n",
      "Epoch 2540, Loss: 1.9209805727005005, Final Batch Loss: 0.20384535193443298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2541, Loss: 2.0117301791906357, Final Batch Loss: 0.20977725088596344\n",
      "Epoch 2542, Loss: 2.056748241186142, Final Batch Loss: 0.22444534301757812\n",
      "Epoch 2543, Loss: 2.0251772850751877, Final Batch Loss: 0.23433129489421844\n",
      "Epoch 2544, Loss: 2.453485459089279, Final Batch Loss: 0.4216645359992981\n",
      "Epoch 2545, Loss: 2.2871440052986145, Final Batch Loss: 0.6223084330558777\n",
      "Epoch 2546, Loss: 2.4865599274635315, Final Batch Loss: 0.8678029179573059\n",
      "Epoch 2547, Loss: 2.181356370449066, Final Batch Loss: 0.443652480840683\n",
      "Epoch 2548, Loss: 2.2484582364559174, Final Batch Loss: 0.5984748601913452\n",
      "Epoch 2549, Loss: 2.1220900118350983, Final Batch Loss: 0.3534388840198517\n",
      "Epoch 2550, Loss: 1.8175697326660156, Final Batch Loss: 0.18703123927116394\n",
      "Epoch 2551, Loss: 2.2150551676750183, Final Batch Loss: 0.45118337869644165\n",
      "Epoch 2552, Loss: 2.1089155077934265, Final Batch Loss: 0.27640941739082336\n",
      "Epoch 2553, Loss: 1.9157490581274033, Final Batch Loss: 0.24963368475437164\n",
      "Epoch 2554, Loss: 2.16741481423378, Final Batch Loss: 0.4862862527370453\n",
      "Epoch 2555, Loss: 2.5566526353359222, Final Batch Loss: 0.9364333152770996\n",
      "Epoch 2556, Loss: 1.9415889382362366, Final Batch Loss: 0.24450549483299255\n",
      "Epoch 2557, Loss: 2.321357101202011, Final Batch Loss: 0.5063195824623108\n",
      "Epoch 2558, Loss: 1.9305685013532639, Final Batch Loss: 0.20451383292675018\n",
      "Epoch 2559, Loss: 2.3939066529273987, Final Batch Loss: 0.788713812828064\n",
      "Epoch 2560, Loss: 1.8228475898504257, Final Batch Loss: 0.15368612110614777\n",
      "Epoch 2561, Loss: 1.8768116533756256, Final Batch Loss: 0.18293240666389465\n",
      "Epoch 2562, Loss: 2.1541203260421753, Final Batch Loss: 0.38448458909988403\n",
      "Epoch 2563, Loss: 1.932832047343254, Final Batch Loss: 0.18326710164546967\n",
      "Epoch 2564, Loss: 2.39252108335495, Final Batch Loss: 0.6247367262840271\n",
      "Epoch 2565, Loss: 1.9664123952388763, Final Batch Loss: 0.34171226620674133\n",
      "Epoch 2566, Loss: 2.1506002247333527, Final Batch Loss: 0.38668158650398254\n",
      "Epoch 2567, Loss: 2.0632818937301636, Final Batch Loss: 0.3900822103023529\n",
      "Epoch 2568, Loss: 2.298138976097107, Final Batch Loss: 0.5551048517227173\n",
      "Epoch 2569, Loss: 2.144603282213211, Final Batch Loss: 0.36884015798568726\n",
      "Epoch 2570, Loss: 2.1214049756526947, Final Batch Loss: 0.42993563413619995\n",
      "Epoch 2571, Loss: 2.249751955270767, Final Batch Loss: 0.5321329832077026\n",
      "Epoch 2572, Loss: 1.8082604855298996, Final Batch Loss: 0.16207002103328705\n",
      "Epoch 2573, Loss: 2.2254902124404907, Final Batch Loss: 0.44765540957450867\n",
      "Epoch 2574, Loss: 2.173941195011139, Final Batch Loss: 0.575828492641449\n",
      "Epoch 2575, Loss: 1.9915066361427307, Final Batch Loss: 0.308234840631485\n",
      "Epoch 2576, Loss: 2.4874436259269714, Final Batch Loss: 0.6834045052528381\n",
      "Epoch 2577, Loss: 2.023708999156952, Final Batch Loss: 0.25932231545448303\n",
      "Epoch 2578, Loss: 2.2131617963314056, Final Batch Loss: 0.38971400260925293\n",
      "Epoch 2579, Loss: 2.2311137318611145, Final Batch Loss: 0.572422444820404\n",
      "Epoch 2580, Loss: 2.0103826820850372, Final Batch Loss: 0.27764442563056946\n",
      "Epoch 2581, Loss: 2.2926913797855377, Final Batch Loss: 0.436283677816391\n",
      "Epoch 2582, Loss: 2.003746062517166, Final Batch Loss: 0.3065502643585205\n",
      "Epoch 2583, Loss: 1.7721106261014938, Final Batch Loss: 0.21032311022281647\n",
      "Epoch 2584, Loss: 1.9797634184360504, Final Batch Loss: 0.31525373458862305\n",
      "Epoch 2585, Loss: 1.9587966799736023, Final Batch Loss: 0.29663529992103577\n",
      "Epoch 2586, Loss: 2.0182067453861237, Final Batch Loss: 0.4426620900630951\n",
      "Epoch 2587, Loss: 2.0052520632743835, Final Batch Loss: 0.39745357632637024\n",
      "Epoch 2588, Loss: 1.818833813071251, Final Batch Loss: 0.18239478766918182\n",
      "Epoch 2589, Loss: 1.989608883857727, Final Batch Loss: 0.2965170741081238\n",
      "Epoch 2590, Loss: 2.285118818283081, Final Batch Loss: 0.6905478239059448\n",
      "Epoch 2591, Loss: 1.9367863237857819, Final Batch Loss: 0.2758345603942871\n",
      "Epoch 2592, Loss: 1.9740556478500366, Final Batch Loss: 0.2566147744655609\n",
      "Epoch 2593, Loss: 2.0510640144348145, Final Batch Loss: 0.3719908595085144\n",
      "Epoch 2594, Loss: 2.1174231469631195, Final Batch Loss: 0.4500660300254822\n",
      "Epoch 2595, Loss: 2.0263624489307404, Final Batch Loss: 0.3408719003200531\n",
      "Epoch 2596, Loss: 2.1487662494182587, Final Batch Loss: 0.5417687296867371\n",
      "Epoch 2597, Loss: 1.8107770681381226, Final Batch Loss: 0.20145872235298157\n",
      "Epoch 2598, Loss: 2.4002954363822937, Final Batch Loss: 0.7458881139755249\n",
      "Epoch 2599, Loss: 2.1499026119709015, Final Batch Loss: 0.48269152641296387\n",
      "Epoch 2600, Loss: 1.756871536374092, Final Batch Loss: 0.10053785145282745\n",
      "Epoch 2601, Loss: 2.1778705418109894, Final Batch Loss: 0.5058799982070923\n",
      "Epoch 2602, Loss: 1.9852865934371948, Final Batch Loss: 0.2782120406627655\n",
      "Epoch 2603, Loss: 2.298697680234909, Final Batch Loss: 0.6214345693588257\n",
      "Epoch 2604, Loss: 1.9069255739450455, Final Batch Loss: 0.22234322130680084\n",
      "Epoch 2605, Loss: 2.06062051653862, Final Batch Loss: 0.49763134121894836\n",
      "Epoch 2606, Loss: 2.073394685983658, Final Batch Loss: 0.3043041527271271\n",
      "Epoch 2607, Loss: 2.3977307975292206, Final Batch Loss: 0.5375017523765564\n",
      "Epoch 2608, Loss: 2.3728543519973755, Final Batch Loss: 0.5580044984817505\n",
      "Epoch 2609, Loss: 2.085156351327896, Final Batch Loss: 0.3631359934806824\n",
      "Epoch 2610, Loss: 2.3741469383239746, Final Batch Loss: 0.6565800309181213\n",
      "Epoch 2611, Loss: 1.9670914709568024, Final Batch Loss: 0.22810539603233337\n",
      "Epoch 2612, Loss: 1.9421453922986984, Final Batch Loss: 0.24763406813144684\n",
      "Epoch 2613, Loss: 1.9986773431301117, Final Batch Loss: 0.2613140940666199\n",
      "Epoch 2614, Loss: 2.2430723905563354, Final Batch Loss: 0.596746563911438\n",
      "Epoch 2615, Loss: 2.3238451182842255, Final Batch Loss: 0.6228658556938171\n",
      "Epoch 2616, Loss: 2.1299956142902374, Final Batch Loss: 0.3465802073478699\n",
      "Epoch 2617, Loss: 2.0879379510879517, Final Batch Loss: 0.4634542465209961\n",
      "Epoch 2618, Loss: 2.074308693408966, Final Batch Loss: 0.3339506983757019\n",
      "Epoch 2619, Loss: 2.1042179465293884, Final Batch Loss: 0.35239648818969727\n",
      "Epoch 2620, Loss: 2.2796112298965454, Final Batch Loss: 0.5489142537117004\n",
      "Epoch 2621, Loss: 2.416958451271057, Final Batch Loss: 0.8323693871498108\n",
      "Epoch 2622, Loss: 2.404716283082962, Final Batch Loss: 0.635852038860321\n",
      "Epoch 2623, Loss: 2.125224679708481, Final Batch Loss: 0.3735235631465912\n",
      "Epoch 2624, Loss: 2.5263995230197906, Final Batch Loss: 0.6697221398353577\n",
      "Epoch 2625, Loss: 2.100769966840744, Final Batch Loss: 0.3653440773487091\n",
      "Epoch 2626, Loss: 2.4151911437511444, Final Batch Loss: 0.4554196298122406\n",
      "Epoch 2627, Loss: 2.5927767753601074, Final Batch Loss: 0.6759452223777771\n",
      "Epoch 2628, Loss: 2.659214973449707, Final Batch Loss: 0.8360893130302429\n",
      "Epoch 2629, Loss: 2.1977336704730988, Final Batch Loss: 0.2873370051383972\n",
      "Epoch 2630, Loss: 2.4799045622348785, Final Batch Loss: 0.6082139015197754\n",
      "Epoch 2631, Loss: 2.646888345479965, Final Batch Loss: 0.7621224522590637\n",
      "Epoch 2632, Loss: 2.287359118461609, Final Batch Loss: 0.42188793420791626\n",
      "Epoch 2633, Loss: 2.162809744477272, Final Batch Loss: 0.20853759348392487\n",
      "Epoch 2634, Loss: 2.59351247549057, Final Batch Loss: 0.7098787426948547\n",
      "Epoch 2635, Loss: 2.081102102994919, Final Batch Loss: 0.3450542390346527\n",
      "Epoch 2636, Loss: 2.2385659217834473, Final Batch Loss: 0.4414920210838318\n",
      "Epoch 2637, Loss: 2.2793799340724945, Final Batch Loss: 0.5266335010528564\n",
      "Epoch 2638, Loss: 2.2669393718242645, Final Batch Loss: 0.4647488594055176\n",
      "Epoch 2639, Loss: 2.007035568356514, Final Batch Loss: 0.13837434351444244\n",
      "Epoch 2640, Loss: 2.061430275440216, Final Batch Loss: 0.3749054968357086\n",
      "Epoch 2641, Loss: 2.099564015865326, Final Batch Loss: 0.40459495782852173\n",
      "Epoch 2642, Loss: 2.1842676401138306, Final Batch Loss: 0.37494394183158875\n",
      "Epoch 2643, Loss: 2.17454069852829, Final Batch Loss: 0.390407532453537\n",
      "Epoch 2644, Loss: 2.2496020197868347, Final Batch Loss: 0.6093153357505798\n",
      "Epoch 2645, Loss: 2.178363412618637, Final Batch Loss: 0.43570369482040405\n",
      "Epoch 2646, Loss: 2.1898784935474396, Final Batch Loss: 0.5137251615524292\n",
      "Epoch 2647, Loss: 2.404984474182129, Final Batch Loss: 0.5755341649055481\n",
      "Epoch 2648, Loss: 1.934431552886963, Final Batch Loss: 0.30574244260787964\n",
      "Epoch 2649, Loss: 1.860445350408554, Final Batch Loss: 0.1767221987247467\n",
      "Epoch 2650, Loss: 2.16823211312294, Final Batch Loss: 0.5980142951011658\n",
      "Epoch 2651, Loss: 2.0984172224998474, Final Batch Loss: 0.3436106741428375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2652, Loss: 2.3055390119552612, Final Batch Loss: 0.593399167060852\n",
      "Epoch 2653, Loss: 2.622838109731674, Final Batch Loss: 0.9081020951271057\n",
      "Epoch 2654, Loss: 1.958068385720253, Final Batch Loss: 0.22529257833957672\n",
      "Epoch 2655, Loss: 2.3825674653053284, Final Batch Loss: 0.7349897623062134\n",
      "Epoch 2656, Loss: 2.1112276911735535, Final Batch Loss: 0.3528827130794525\n",
      "Epoch 2657, Loss: 2.0476669371128082, Final Batch Loss: 0.36203351616859436\n",
      "Epoch 2658, Loss: 1.9601760506629944, Final Batch Loss: 0.20688366889953613\n",
      "Epoch 2659, Loss: 2.115323543548584, Final Batch Loss: 0.43092599511146545\n",
      "Epoch 2660, Loss: 2.127498894929886, Final Batch Loss: 0.4561297297477722\n",
      "Epoch 2661, Loss: 2.232244461774826, Final Batch Loss: 0.5294370055198669\n",
      "Epoch 2662, Loss: 2.5323868095874786, Final Batch Loss: 0.8715137839317322\n",
      "Epoch 2663, Loss: 2.1946927905082703, Final Batch Loss: 0.4645652770996094\n",
      "Epoch 2664, Loss: 2.4101579189300537, Final Batch Loss: 0.715774416923523\n",
      "Epoch 2665, Loss: 2.2020713090896606, Final Batch Loss: 0.529084324836731\n",
      "Epoch 2666, Loss: 2.060992479324341, Final Batch Loss: 0.30243119597435\n",
      "Epoch 2667, Loss: 1.793230414390564, Final Batch Loss: 0.18281349539756775\n",
      "Epoch 2668, Loss: 1.8983561992645264, Final Batch Loss: 0.28942838311195374\n",
      "Epoch 2669, Loss: 1.889484703540802, Final Batch Loss: 0.3108256161212921\n",
      "Epoch 2670, Loss: 2.2769519686698914, Final Batch Loss: 0.5932426452636719\n",
      "Epoch 2671, Loss: 1.9725528359413147, Final Batch Loss: 0.3055150508880615\n",
      "Epoch 2672, Loss: 2.2707358598709106, Final Batch Loss: 0.6702920198440552\n",
      "Epoch 2673, Loss: 2.2991979122161865, Final Batch Loss: 0.6779584884643555\n",
      "Epoch 2674, Loss: 1.7854211777448654, Final Batch Loss: 0.14424188435077667\n",
      "Epoch 2675, Loss: 2.325803726911545, Final Batch Loss: 0.5311136841773987\n",
      "Epoch 2676, Loss: 1.9118401110172272, Final Batch Loss: 0.3138635456562042\n",
      "Epoch 2677, Loss: 2.199727565050125, Final Batch Loss: 0.5176758170127869\n",
      "Epoch 2678, Loss: 2.241528958082199, Final Batch Loss: 0.4935378134250641\n",
      "Epoch 2679, Loss: 2.055915355682373, Final Batch Loss: 0.4516792893409729\n",
      "Epoch 2680, Loss: 1.934989109635353, Final Batch Loss: 0.24906353652477264\n",
      "Epoch 2681, Loss: 2.1019898653030396, Final Batch Loss: 0.36100468039512634\n",
      "Epoch 2682, Loss: 1.9903640151023865, Final Batch Loss: 0.399087131023407\n",
      "Epoch 2683, Loss: 1.9948206543922424, Final Batch Loss: 0.3666597306728363\n",
      "Epoch 2684, Loss: 1.9172528088092804, Final Batch Loss: 0.2645268738269806\n",
      "Epoch 2685, Loss: 2.3311309814453125, Final Batch Loss: 0.7094163298606873\n",
      "Epoch 2686, Loss: 2.12593612074852, Final Batch Loss: 0.38714689016342163\n",
      "Epoch 2687, Loss: 2.224896728992462, Final Batch Loss: 0.48273012042045593\n",
      "Epoch 2688, Loss: 2.1131022572517395, Final Batch Loss: 0.45470044016838074\n",
      "Epoch 2689, Loss: 2.1562438905239105, Final Batch Loss: 0.5273987650871277\n",
      "Epoch 2690, Loss: 1.9945840239524841, Final Batch Loss: 0.3861250877380371\n",
      "Epoch 2691, Loss: 1.9197160005569458, Final Batch Loss: 0.31827235221862793\n",
      "Epoch 2692, Loss: 1.726811632514, Final Batch Loss: 0.1550273448228836\n",
      "Epoch 2693, Loss: 2.046130120754242, Final Batch Loss: 0.45693239569664\n",
      "Epoch 2694, Loss: 2.4195953607559204, Final Batch Loss: 0.7078151106834412\n",
      "Epoch 2695, Loss: 2.759247839450836, Final Batch Loss: 1.2228925228118896\n",
      "Epoch 2696, Loss: 1.8385547697544098, Final Batch Loss: 0.22531938552856445\n",
      "Epoch 2697, Loss: 2.082089751958847, Final Batch Loss: 0.4781922399997711\n",
      "Epoch 2698, Loss: 2.412461578845978, Final Batch Loss: 0.6994480490684509\n",
      "Epoch 2699, Loss: 2.1574655771255493, Final Batch Loss: 0.5023276209831238\n",
      "Epoch 2700, Loss: 2.4220320284366608, Final Batch Loss: 0.7706311345100403\n",
      "Epoch 2701, Loss: 2.319521516561508, Final Batch Loss: 0.5080596208572388\n",
      "Epoch 2702, Loss: 2.1144517958164215, Final Batch Loss: 0.4158703684806824\n",
      "Epoch 2703, Loss: 1.8762798756361008, Final Batch Loss: 0.20254512131214142\n",
      "Epoch 2704, Loss: 1.9100627601146698, Final Batch Loss: 0.23466911911964417\n",
      "Epoch 2705, Loss: 2.4609014987945557, Final Batch Loss: 0.7477834820747375\n",
      "Epoch 2706, Loss: 1.9689911007881165, Final Batch Loss: 0.3150765001773834\n",
      "Epoch 2707, Loss: 2.054403692483902, Final Batch Loss: 0.2835538685321808\n",
      "Epoch 2708, Loss: 2.168083071708679, Final Batch Loss: 0.5435358285903931\n",
      "Epoch 2709, Loss: 2.2130492627620697, Final Batch Loss: 0.5420946478843689\n",
      "Epoch 2710, Loss: 2.284049391746521, Final Batch Loss: 0.5441388487815857\n",
      "Epoch 2711, Loss: 2.2222979068756104, Final Batch Loss: 0.616949200630188\n",
      "Epoch 2712, Loss: 2.3457241654396057, Final Batch Loss: 0.6004114151000977\n",
      "Epoch 2713, Loss: 2.090446799993515, Final Batch Loss: 0.40850892663002014\n",
      "Epoch 2714, Loss: 2.0274812281131744, Final Batch Loss: 0.4544336497783661\n",
      "Epoch 2715, Loss: 2.042501598596573, Final Batch Loss: 0.4437125623226166\n",
      "Epoch 2716, Loss: 2.6669468581676483, Final Batch Loss: 0.9164003133773804\n",
      "Epoch 2717, Loss: 2.627629429101944, Final Batch Loss: 0.8749123215675354\n",
      "Epoch 2718, Loss: 2.122233062982559, Final Batch Loss: 0.4996935725212097\n",
      "Epoch 2719, Loss: 2.205123335123062, Final Batch Loss: 0.48438066244125366\n",
      "Epoch 2720, Loss: 2.2954630851745605, Final Batch Loss: 0.514854371547699\n",
      "Epoch 2721, Loss: 2.0781910121440887, Final Batch Loss: 0.46545594930648804\n",
      "Epoch 2722, Loss: 2.0529488623142242, Final Batch Loss: 0.3767666518688202\n",
      "Epoch 2723, Loss: 2.070302277803421, Final Batch Loss: 0.4089885354042053\n",
      "Epoch 2724, Loss: 1.8582590818405151, Final Batch Loss: 0.23376348614692688\n",
      "Epoch 2725, Loss: 1.9236392080783844, Final Batch Loss: 0.28993090987205505\n",
      "Epoch 2726, Loss: 1.8436138182878494, Final Batch Loss: 0.22532661259174347\n",
      "Epoch 2727, Loss: 1.8244391828775406, Final Batch Loss: 0.14031563699245453\n",
      "Epoch 2728, Loss: 1.8773400485515594, Final Batch Loss: 0.26155397295951843\n",
      "Epoch 2729, Loss: 2.266633480787277, Final Batch Loss: 0.579205334186554\n",
      "Epoch 2730, Loss: 1.9682531654834747, Final Batch Loss: 0.32280632853507996\n",
      "Epoch 2731, Loss: 1.8409882485866547, Final Batch Loss: 0.2098802626132965\n",
      "Epoch 2732, Loss: 2.1235391497612, Final Batch Loss: 0.520105242729187\n",
      "Epoch 2733, Loss: 2.022796854376793, Final Batch Loss: 0.2389792650938034\n",
      "Epoch 2734, Loss: 1.8376290500164032, Final Batch Loss: 0.22259950637817383\n",
      "Epoch 2735, Loss: 2.0840558111667633, Final Batch Loss: 0.4586992561817169\n",
      "Epoch 2736, Loss: 1.8599694818258286, Final Batch Loss: 0.21225471794605255\n",
      "Epoch 2737, Loss: 2.03082138299942, Final Batch Loss: 0.29651665687561035\n",
      "Epoch 2738, Loss: 1.7700641453266144, Final Batch Loss: 0.19322168827056885\n",
      "Epoch 2739, Loss: 2.097435086965561, Final Batch Loss: 0.5632443428039551\n",
      "Epoch 2740, Loss: 1.9998214542865753, Final Batch Loss: 0.42371121048927307\n",
      "Epoch 2741, Loss: 1.978434830904007, Final Batch Loss: 0.3097045123577118\n",
      "Epoch 2742, Loss: 1.789809376001358, Final Batch Loss: 0.25361719727516174\n",
      "Epoch 2743, Loss: 2.035008519887924, Final Batch Loss: 0.4274695813655853\n",
      "Epoch 2744, Loss: 2.115751087665558, Final Batch Loss: 0.44428926706314087\n",
      "Epoch 2745, Loss: 1.8877258151769638, Final Batch Loss: 0.22234497964382172\n",
      "Epoch 2746, Loss: 2.29346039891243, Final Batch Loss: 0.5542832016944885\n",
      "Epoch 2747, Loss: 1.8380362316966057, Final Batch Loss: 0.10135137289762497\n",
      "Epoch 2748, Loss: 1.9870412051677704, Final Batch Loss: 0.2748549282550812\n",
      "Epoch 2749, Loss: 1.9335418045520782, Final Batch Loss: 0.25297656655311584\n",
      "Epoch 2750, Loss: 1.8544175177812576, Final Batch Loss: 0.1865607053041458\n",
      "Epoch 2751, Loss: 1.990756779909134, Final Batch Loss: 0.3849153518676758\n",
      "Epoch 2752, Loss: 1.9747113287448883, Final Batch Loss: 0.35234740376472473\n",
      "Epoch 2753, Loss: 1.796332597732544, Final Batch Loss: 0.18720778822898865\n",
      "Epoch 2754, Loss: 1.812005415558815, Final Batch Loss: 0.23579950630664825\n",
      "Epoch 2755, Loss: 1.895460695028305, Final Batch Loss: 0.28791433572769165\n",
      "Epoch 2756, Loss: 1.9675083458423615, Final Batch Loss: 0.41212108731269836\n",
      "Epoch 2757, Loss: 2.201040506362915, Final Batch Loss: 0.46804532408714294\n",
      "Epoch 2758, Loss: 2.119402140378952, Final Batch Loss: 0.5389477014541626\n",
      "Epoch 2759, Loss: 1.8160829693078995, Final Batch Loss: 0.1984347254037857\n",
      "Epoch 2760, Loss: 1.8741798400878906, Final Batch Loss: 0.2952386438846588\n",
      "Epoch 2761, Loss: 2.061945468187332, Final Batch Loss: 0.3197624385356903\n",
      "Epoch 2762, Loss: 1.776105985045433, Final Batch Loss: 0.17797310650348663\n",
      "Epoch 2763, Loss: 1.9524125456809998, Final Batch Loss: 0.36206552386283875\n",
      "Epoch 2764, Loss: 2.3911834359169006, Final Batch Loss: 0.8786884546279907\n",
      "Epoch 2765, Loss: 2.0972391963005066, Final Batch Loss: 0.418914258480072\n",
      "Epoch 2766, Loss: 2.1066282391548157, Final Batch Loss: 0.40910324454307556\n",
      "Epoch 2767, Loss: 2.0331033170223236, Final Batch Loss: 0.3438700735569\n",
      "Epoch 2768, Loss: 2.0207499563694, Final Batch Loss: 0.39399877190589905\n",
      "Epoch 2769, Loss: 2.221320480108261, Final Batch Loss: 0.6417492628097534\n",
      "Epoch 2770, Loss: 1.9116139858961105, Final Batch Loss: 0.19206871092319489\n",
      "Epoch 2771, Loss: 2.035311758518219, Final Batch Loss: 0.4138335585594177\n",
      "Epoch 2772, Loss: 1.9474593102931976, Final Batch Loss: 0.36987966299057007\n",
      "Epoch 2773, Loss: 1.736781969666481, Final Batch Loss: 0.13978548347949982\n",
      "Epoch 2774, Loss: 1.7818684130907059, Final Batch Loss: 0.1921137422323227\n",
      "Epoch 2775, Loss: 2.053991913795471, Final Batch Loss: 0.4104253351688385\n",
      "Epoch 2776, Loss: 2.30499467253685, Final Batch Loss: 0.6764610409736633\n",
      "Epoch 2777, Loss: 1.9908831119537354, Final Batch Loss: 0.3652651607990265\n",
      "Epoch 2778, Loss: 2.273423880338669, Final Batch Loss: 0.6227691769599915\n",
      "Epoch 2779, Loss: 1.863274872303009, Final Batch Loss: 0.22780248522758484\n",
      "Epoch 2780, Loss: 2.2613188922405243, Final Batch Loss: 0.5688623189926147\n",
      "Epoch 2781, Loss: 2.227375239133835, Final Batch Loss: 0.5971054434776306\n",
      "Epoch 2782, Loss: 1.8730067014694214, Final Batch Loss: 0.16667082905769348\n",
      "Epoch 2783, Loss: 1.9114153981208801, Final Batch Loss: 0.3088712990283966\n",
      "Epoch 2784, Loss: 2.1060094833374023, Final Batch Loss: 0.553866982460022\n",
      "Epoch 2785, Loss: 1.9637273102998734, Final Batch Loss: 0.22709394991397858\n",
      "Epoch 2786, Loss: 1.9184865057468414, Final Batch Loss: 0.3094998002052307\n",
      "Epoch 2787, Loss: 2.140004336833954, Final Batch Loss: 0.5782405734062195\n",
      "Epoch 2788, Loss: 2.428400218486786, Final Batch Loss: 0.8283969163894653\n",
      "Epoch 2789, Loss: 1.8375395685434341, Final Batch Loss: 0.23872430622577667\n",
      "Epoch 2790, Loss: 2.3662611842155457, Final Batch Loss: 0.8833739161491394\n",
      "Epoch 2791, Loss: 1.9564031958580017, Final Batch Loss: 0.28310972452163696\n",
      "Epoch 2792, Loss: 2.012694537639618, Final Batch Loss: 0.4264748692512512\n",
      "Epoch 2793, Loss: 2.246669113636017, Final Batch Loss: 0.591040313243866\n",
      "Epoch 2794, Loss: 2.4591344594955444, Final Batch Loss: 0.7489120364189148\n",
      "Epoch 2795, Loss: 1.9523226022720337, Final Batch Loss: 0.3189258873462677\n",
      "Epoch 2796, Loss: 2.2438563406467438, Final Batch Loss: 0.567206859588623\n",
      "Epoch 2797, Loss: 1.9831869006156921, Final Batch Loss: 0.29919371008872986\n",
      "Epoch 2798, Loss: 1.8019553422927856, Final Batch Loss: 0.25131532549858093\n",
      "Epoch 2799, Loss: 2.102599859237671, Final Batch Loss: 0.5240631699562073\n",
      "Epoch 2800, Loss: 2.0444449186325073, Final Batch Loss: 0.4755832850933075\n",
      "Epoch 2801, Loss: 2.0454377830028534, Final Batch Loss: 0.4760148525238037\n",
      "Epoch 2802, Loss: 2.2344070076942444, Final Batch Loss: 0.6109424233436584\n",
      "Epoch 2803, Loss: 2.077376827597618, Final Batch Loss: 0.24407438933849335\n",
      "Epoch 2804, Loss: 1.9869108200073242, Final Batch Loss: 0.42835789918899536\n",
      "Epoch 2805, Loss: 2.4223391711711884, Final Batch Loss: 0.873931884765625\n",
      "Epoch 2806, Loss: 1.944786936044693, Final Batch Loss: 0.1949736773967743\n",
      "Epoch 2807, Loss: 2.142515689134598, Final Batch Loss: 0.562576413154602\n",
      "Epoch 2808, Loss: 1.7887192964553833, Final Batch Loss: 0.26789724826812744\n",
      "Epoch 2809, Loss: 1.9150109589099884, Final Batch Loss: 0.4045012593269348\n",
      "Epoch 2810, Loss: 2.183891922235489, Final Batch Loss: 0.43401068449020386\n",
      "Epoch 2811, Loss: 1.7434123307466507, Final Batch Loss: 0.16453661024570465\n",
      "Epoch 2812, Loss: 2.892662763595581, Final Batch Loss: 1.3107835054397583\n",
      "Epoch 2813, Loss: 2.1264231204986572, Final Batch Loss: 0.5703366994857788\n",
      "Epoch 2814, Loss: 2.0674713402986526, Final Batch Loss: 0.2450631707906723\n",
      "Epoch 2815, Loss: 2.0492768585681915, Final Batch Loss: 0.3434261381626129\n",
      "Epoch 2816, Loss: 1.9748768210411072, Final Batch Loss: 0.4023694097995758\n",
      "Epoch 2817, Loss: 2.189417004585266, Final Batch Loss: 0.5472623109817505\n",
      "Epoch 2818, Loss: 2.064805030822754, Final Batch Loss: 0.3743777573108673\n",
      "Epoch 2819, Loss: 1.9040996134281158, Final Batch Loss: 0.2470625638961792\n",
      "Epoch 2820, Loss: 2.3113572001457214, Final Batch Loss: 0.7042250037193298\n",
      "Epoch 2821, Loss: 1.8366241753101349, Final Batch Loss: 0.28417035937309265\n",
      "Epoch 2822, Loss: 2.071951061487198, Final Batch Loss: 0.41398805379867554\n",
      "Epoch 2823, Loss: 2.0489712059497833, Final Batch Loss: 0.4186418056488037\n",
      "Epoch 2824, Loss: 1.7250407114624977, Final Batch Loss: 0.11115974932909012\n",
      "Epoch 2825, Loss: 1.8956600427627563, Final Batch Loss: 0.3017102777957916\n",
      "Epoch 2826, Loss: 2.319928228855133, Final Batch Loss: 0.7429285049438477\n",
      "Epoch 2827, Loss: 2.379040867090225, Final Batch Loss: 0.6701489686965942\n",
      "Epoch 2828, Loss: 2.1495777666568756, Final Batch Loss: 0.5376008152961731\n",
      "Epoch 2829, Loss: 1.8516471534967422, Final Batch Loss: 0.1916104406118393\n",
      "Epoch 2830, Loss: 2.0958105325698853, Final Batch Loss: 0.42715343832969666\n",
      "Epoch 2831, Loss: 2.2791302502155304, Final Batch Loss: 0.6479535102844238\n",
      "Epoch 2832, Loss: 1.8216537535190582, Final Batch Loss: 0.25281959772109985\n",
      "Epoch 2833, Loss: 3.7260974645614624, Final Batch Loss: 2.0590569972991943\n",
      "Epoch 2834, Loss: 2.094280630350113, Final Batch Loss: 0.4418262839317322\n",
      "Epoch 2835, Loss: 1.9981539249420166, Final Batch Loss: 0.3790353834629059\n",
      "Epoch 2836, Loss: 1.709903433918953, Final Batch Loss: 0.16964368522167206\n",
      "Epoch 2837, Loss: 2.1369921565055847, Final Batch Loss: 0.44119054079055786\n",
      "Epoch 2838, Loss: 2.003886893391609, Final Batch Loss: 0.2487514168024063\n",
      "Epoch 2839, Loss: 2.3521010279655457, Final Batch Loss: 0.5103345513343811\n",
      "Epoch 2840, Loss: 2.0527144968509674, Final Batch Loss: 0.3785950839519501\n",
      "Epoch 2841, Loss: 2.0870683193206787, Final Batch Loss: 0.3078705966472626\n",
      "Epoch 2842, Loss: 1.9527305960655212, Final Batch Loss: 0.2957780063152313\n",
      "Epoch 2843, Loss: 2.1090205907821655, Final Batch Loss: 0.42269110679626465\n",
      "Epoch 2844, Loss: 2.170885533094406, Final Batch Loss: 0.4713304042816162\n",
      "Epoch 2845, Loss: 2.2536064088344574, Final Batch Loss: 0.4981318414211273\n",
      "Epoch 2846, Loss: 2.419196605682373, Final Batch Loss: 0.6809827089309692\n",
      "Epoch 2847, Loss: 1.7825405895709991, Final Batch Loss: 0.1708398163318634\n",
      "Epoch 2848, Loss: 2.3288425505161285, Final Batch Loss: 0.6985787153244019\n",
      "Epoch 2849, Loss: 2.2423603534698486, Final Batch Loss: 0.6497825980186462\n",
      "Epoch 2850, Loss: 2.087712824344635, Final Batch Loss: 0.34496036171913147\n",
      "Epoch 2851, Loss: 2.1814737617969513, Final Batch Loss: 0.5731807947158813\n",
      "Epoch 2852, Loss: 2.159021109342575, Final Batch Loss: 0.5356188416481018\n",
      "Epoch 2853, Loss: 1.7584166377782822, Final Batch Loss: 0.1789478212594986\n",
      "Epoch 2854, Loss: 1.9803588092327118, Final Batch Loss: 0.3334045112133026\n",
      "Epoch 2855, Loss: 1.9780024290084839, Final Batch Loss: 0.2868598401546478\n",
      "Epoch 2856, Loss: 2.141248404979706, Final Batch Loss: 0.6077671051025391\n",
      "Epoch 2857, Loss: 2.059645116329193, Final Batch Loss: 0.43093353509902954\n",
      "Epoch 2858, Loss: 1.6722442209720612, Final Batch Loss: 0.12075221538543701\n",
      "Epoch 2859, Loss: 2.475805550813675, Final Batch Loss: 0.8870928883552551\n",
      "Epoch 2860, Loss: 1.7388266623020172, Final Batch Loss: 0.24072769284248352\n",
      "Epoch 2861, Loss: 1.9824645817279816, Final Batch Loss: 0.47018495202064514\n",
      "Epoch 2862, Loss: 1.9044924825429916, Final Batch Loss: 0.22620414197444916\n",
      "Epoch 2863, Loss: 2.1761993169784546, Final Batch Loss: 0.6130031943321228\n",
      "Epoch 2864, Loss: 1.7589153796434402, Final Batch Loss: 0.18585996329784393\n",
      "Epoch 2865, Loss: 1.701352208852768, Final Batch Loss: 0.1675918996334076\n",
      "Epoch 2866, Loss: 1.9403852224349976, Final Batch Loss: 0.4102065861225128\n",
      "Epoch 2867, Loss: 1.9031521677970886, Final Batch Loss: 0.40517953038215637\n",
      "Epoch 2868, Loss: 2.1666738986968994, Final Batch Loss: 0.6801143884658813\n",
      "Epoch 2869, Loss: 1.8744084537029266, Final Batch Loss: 0.2756328880786896\n",
      "Epoch 2870, Loss: 2.053794175386429, Final Batch Loss: 0.48266085982322693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2871, Loss: 2.555158495903015, Final Batch Loss: 0.92589271068573\n",
      "Epoch 2872, Loss: 2.1103257536888123, Final Batch Loss: 0.3771703541278839\n",
      "Epoch 2873, Loss: 1.802776776254177, Final Batch Loss: 0.05305015295743942\n",
      "Epoch 2874, Loss: 2.90620219707489, Final Batch Loss: 1.1581048965454102\n",
      "Epoch 2875, Loss: 1.7205207124352455, Final Batch Loss: 0.06534747034311295\n",
      "Epoch 2876, Loss: 1.759844720363617, Final Batch Loss: 0.1531289517879486\n",
      "Epoch 2877, Loss: 2.0330415964126587, Final Batch Loss: 0.5198306441307068\n",
      "Epoch 2878, Loss: 2.2134463489055634, Final Batch Loss: 0.6628760695457458\n",
      "Epoch 2879, Loss: 1.742702379822731, Final Batch Loss: 0.15984465181827545\n",
      "Epoch 2880, Loss: 1.9232307970523834, Final Batch Loss: 0.33071908354759216\n",
      "Epoch 2881, Loss: 1.6914769411087036, Final Batch Loss: 0.2291375696659088\n",
      "Epoch 2882, Loss: 2.034063845872879, Final Batch Loss: 0.4275842010974884\n",
      "Epoch 2883, Loss: 1.767298936843872, Final Batch Loss: 0.2041316032409668\n",
      "Epoch 2884, Loss: 1.8253300487995148, Final Batch Loss: 0.33794572949409485\n",
      "Epoch 2885, Loss: 2.359953433275223, Final Batch Loss: 0.7887870073318481\n",
      "Epoch 2886, Loss: 1.9383464455604553, Final Batch Loss: 0.27468693256378174\n",
      "Epoch 2887, Loss: 1.785555973649025, Final Batch Loss: 0.15565307438373566\n",
      "Epoch 2888, Loss: 2.0679123401641846, Final Batch Loss: 0.48582273721694946\n",
      "Epoch 2889, Loss: 2.0617618560791016, Final Batch Loss: 0.3820754587650299\n",
      "Epoch 2890, Loss: 2.1475076377391815, Final Batch Loss: 0.4854006767272949\n",
      "Epoch 2891, Loss: 2.2852461636066437, Final Batch Loss: 0.5560735464096069\n",
      "Epoch 2892, Loss: 2.0142884850502014, Final Batch Loss: 0.4063756763935089\n",
      "Epoch 2893, Loss: 1.98952317237854, Final Batch Loss: 0.3616965115070343\n",
      "Epoch 2894, Loss: 2.152751326560974, Final Batch Loss: 0.4556157886981964\n",
      "Epoch 2895, Loss: 1.7807620018720627, Final Batch Loss: 0.21229462325572968\n",
      "Epoch 2896, Loss: 2.1678411066532135, Final Batch Loss: 0.5579555630683899\n",
      "Epoch 2897, Loss: 1.8999794721603394, Final Batch Loss: 0.26435771584510803\n",
      "Epoch 2898, Loss: 2.6122847199440002, Final Batch Loss: 0.9920353293418884\n",
      "Epoch 2899, Loss: 2.1349202394485474, Final Batch Loss: 0.5640320181846619\n",
      "Epoch 2900, Loss: 2.2937623262405396, Final Batch Loss: 0.31310105323791504\n",
      "Epoch 2901, Loss: 2.3190689980983734, Final Batch Loss: 0.5040521025657654\n",
      "Epoch 2902, Loss: 2.5129710733890533, Final Batch Loss: 0.6040163040161133\n",
      "Epoch 2903, Loss: 2.1753480434417725, Final Batch Loss: 0.25970858335494995\n",
      "Epoch 2904, Loss: 2.0143121778964996, Final Batch Loss: 0.3698425889015198\n",
      "Epoch 2905, Loss: 2.157046854496002, Final Batch Loss: 0.5215498208999634\n",
      "Epoch 2906, Loss: 2.057530641555786, Final Batch Loss: 0.32448071241378784\n",
      "Epoch 2907, Loss: 1.879707857966423, Final Batch Loss: 0.08923302590847015\n",
      "Epoch 2908, Loss: 1.9808253943920135, Final Batch Loss: 0.3480117917060852\n",
      "Epoch 2909, Loss: 1.9228795915842056, Final Batch Loss: 0.24990342557430267\n",
      "Epoch 2910, Loss: 1.7472960948944092, Final Batch Loss: 0.1932835578918457\n",
      "Epoch 2911, Loss: 1.9461072087287903, Final Batch Loss: 0.33491095900535583\n",
      "Epoch 2912, Loss: 1.7312519401311874, Final Batch Loss: 0.2038821130990982\n",
      "Epoch 2913, Loss: 1.9377818703651428, Final Batch Loss: 0.30593639612197876\n",
      "Epoch 2914, Loss: 1.9641081988811493, Final Batch Loss: 0.43323150277137756\n",
      "Epoch 2915, Loss: 2.1817812621593475, Final Batch Loss: 0.5398215055465698\n",
      "Epoch 2916, Loss: 2.1022489964962006, Final Batch Loss: 0.41020649671554565\n",
      "Epoch 2917, Loss: 2.0444149672985077, Final Batch Loss: 0.48756691813468933\n",
      "Epoch 2918, Loss: 1.9876120388507843, Final Batch Loss: 0.38350701332092285\n",
      "Epoch 2919, Loss: 1.8419926762580872, Final Batch Loss: 0.25113144516944885\n",
      "Epoch 2920, Loss: 1.9810694456100464, Final Batch Loss: 0.322805255651474\n",
      "Epoch 2921, Loss: 2.086190104484558, Final Batch Loss: 0.6077081561088562\n",
      "Epoch 2922, Loss: 1.8149481117725372, Final Batch Loss: 0.2234625518321991\n",
      "Epoch 2923, Loss: 2.013813018798828, Final Batch Loss: 0.4918668866157532\n",
      "Epoch 2924, Loss: 2.418615996837616, Final Batch Loss: 0.784387469291687\n",
      "Epoch 2925, Loss: 1.9232398867607117, Final Batch Loss: 0.32476717233657837\n",
      "Epoch 2926, Loss: 1.9880222380161285, Final Batch Loss: 0.4361373782157898\n",
      "Epoch 2927, Loss: 1.7950287461280823, Final Batch Loss: 0.1358538568019867\n",
      "Epoch 2928, Loss: 2.2559950053691864, Final Batch Loss: 0.45930203795433044\n",
      "Epoch 2929, Loss: 2.359954684972763, Final Batch Loss: 0.686594545841217\n",
      "Epoch 2930, Loss: 1.7583597302436829, Final Batch Loss: 0.2559451758861542\n",
      "Epoch 2931, Loss: 1.7244341671466827, Final Batch Loss: 0.2641483247280121\n",
      "Epoch 2932, Loss: 1.7597807794809341, Final Batch Loss: 0.17705677449703217\n",
      "Epoch 2933, Loss: 1.9168245196342468, Final Batch Loss: 0.40341946482658386\n",
      "Epoch 2934, Loss: 1.924619808793068, Final Batch Loss: 0.24653266370296478\n",
      "Epoch 2935, Loss: 1.744945004582405, Final Batch Loss: 0.20771144330501556\n",
      "Epoch 2936, Loss: 2.0763933062553406, Final Batch Loss: 0.5251021981239319\n",
      "Epoch 2937, Loss: 1.6688133627176285, Final Batch Loss: 0.20731060206890106\n",
      "Epoch 2938, Loss: 2.134630709886551, Final Batch Loss: 0.612814724445343\n",
      "Epoch 2939, Loss: 1.8529293537139893, Final Batch Loss: 0.2533222436904907\n",
      "Epoch 2940, Loss: 2.028692603111267, Final Batch Loss: 0.4854348599910736\n",
      "Epoch 2941, Loss: 1.9770841896533966, Final Batch Loss: 0.38809797167778015\n",
      "Epoch 2942, Loss: 1.4933851845562458, Final Batch Loss: 0.056191641837358475\n",
      "Epoch 2943, Loss: 1.8761276304721832, Final Batch Loss: 0.26854437589645386\n",
      "Epoch 2944, Loss: 2.1165722608566284, Final Batch Loss: 0.5056965947151184\n",
      "Epoch 2945, Loss: 2.233274519443512, Final Batch Loss: 0.5835608243942261\n",
      "Epoch 2946, Loss: 1.8639650642871857, Final Batch Loss: 0.3483860194683075\n",
      "Epoch 2947, Loss: 2.4935879707336426, Final Batch Loss: 0.8754290342330933\n",
      "Epoch 2948, Loss: 2.35730242729187, Final Batch Loss: 0.5205575227737427\n",
      "Epoch 2949, Loss: 2.6545402109622955, Final Batch Loss: 1.0200905799865723\n",
      "Epoch 2950, Loss: 2.0605375170707703, Final Batch Loss: 0.4418065547943115\n",
      "Epoch 2951, Loss: 1.9492506682872772, Final Batch Loss: 0.4159679412841797\n",
      "Epoch 2952, Loss: 2.2258085906505585, Final Batch Loss: 0.6953416466712952\n",
      "Epoch 2953, Loss: 2.0439073741436005, Final Batch Loss: 0.48083725571632385\n",
      "Epoch 2954, Loss: 1.8993458449840546, Final Batch Loss: 0.3411751687526703\n",
      "Epoch 2955, Loss: 2.111821711063385, Final Batch Loss: 0.46228960156440735\n",
      "Epoch 2956, Loss: 2.149357557296753, Final Batch Loss: 0.5668970346450806\n",
      "Epoch 2957, Loss: 1.7075459510087967, Final Batch Loss: 0.15725009143352509\n",
      "Epoch 2958, Loss: 2.0810117423534393, Final Batch Loss: 0.5136153697967529\n",
      "Epoch 2959, Loss: 1.9270676970481873, Final Batch Loss: 0.39614495635032654\n",
      "Epoch 2960, Loss: 2.174582839012146, Final Batch Loss: 0.6079854369163513\n",
      "Epoch 2961, Loss: 1.986810028553009, Final Batch Loss: 0.4180707633495331\n",
      "Epoch 2962, Loss: 1.7812092900276184, Final Batch Loss: 0.29133185744285583\n",
      "Epoch 2963, Loss: 2.3247025310993195, Final Batch Loss: 0.671351969242096\n",
      "Epoch 2964, Loss: 1.894388496875763, Final Batch Loss: 0.3230707347393036\n",
      "Epoch 2965, Loss: 1.837953120470047, Final Batch Loss: 0.262285977602005\n",
      "Epoch 2966, Loss: 1.8241464048624039, Final Batch Loss: 0.22905351221561432\n",
      "Epoch 2967, Loss: 1.8322250545024872, Final Batch Loss: 0.2820655107498169\n",
      "Epoch 2968, Loss: 1.7911670207977295, Final Batch Loss: 0.2793673574924469\n",
      "Epoch 2969, Loss: 2.206960380077362, Final Batch Loss: 0.5808407664299011\n",
      "Epoch 2970, Loss: 1.9593167006969452, Final Batch Loss: 0.44664040207862854\n",
      "Epoch 2971, Loss: 2.0834473371505737, Final Batch Loss: 0.43687310814857483\n",
      "Epoch 2972, Loss: 1.85318922996521, Final Batch Loss: 0.19758233428001404\n",
      "Epoch 2973, Loss: 1.8699378669261932, Final Batch Loss: 0.33593639731407166\n",
      "Epoch 2974, Loss: 1.8977150917053223, Final Batch Loss: 0.2962341606616974\n",
      "Epoch 2975, Loss: 1.8100106865167618, Final Batch Loss: 0.24116550385951996\n",
      "Epoch 2976, Loss: 1.9415885508060455, Final Batch Loss: 0.3990057408809662\n",
      "Epoch 2977, Loss: 2.2617498636245728, Final Batch Loss: 0.7677822709083557\n",
      "Epoch 2978, Loss: 2.682379812002182, Final Batch Loss: 1.097551703453064\n",
      "Epoch 2979, Loss: 2.301824927330017, Final Batch Loss: 0.6249396204948425\n",
      "Epoch 2980, Loss: 1.85737644135952, Final Batch Loss: 0.19990940392017365\n",
      "Epoch 2981, Loss: 2.1740592420101166, Final Batch Loss: 0.342088907957077\n",
      "Epoch 2982, Loss: 2.121142655611038, Final Batch Loss: 0.3749873638153076\n",
      "Epoch 2983, Loss: 2.0554052889347076, Final Batch Loss: 0.486510694026947\n",
      "Epoch 2984, Loss: 2.047471344470978, Final Batch Loss: 0.3596402704715729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2985, Loss: 2.273076057434082, Final Batch Loss: 0.5362380146980286\n",
      "Epoch 2986, Loss: 2.089717596769333, Final Batch Loss: 0.3579253852367401\n",
      "Epoch 2987, Loss: 2.3081343173980713, Final Batch Loss: 0.6823967695236206\n",
      "Epoch 2988, Loss: 2.0149733126163483, Final Batch Loss: 0.38461002707481384\n",
      "Epoch 2989, Loss: 1.8124901503324509, Final Batch Loss: 0.18632911145687103\n",
      "Epoch 2990, Loss: 2.521972179412842, Final Batch Loss: 0.8865867853164673\n",
      "Epoch 2991, Loss: 2.001463383436203, Final Batch Loss: 0.37692350149154663\n",
      "Epoch 2992, Loss: 2.0177289247512817, Final Batch Loss: 0.3844335079193115\n",
      "Epoch 2993, Loss: 1.8654430210590363, Final Batch Loss: 0.36754482984542847\n",
      "Epoch 2994, Loss: 2.164354592561722, Final Batch Loss: 0.6024307012557983\n",
      "Epoch 2995, Loss: 1.7749296128749847, Final Batch Loss: 0.1690981686115265\n",
      "Epoch 2996, Loss: 1.9036603271961212, Final Batch Loss: 0.25423482060432434\n",
      "Epoch 2997, Loss: 1.8025625050067902, Final Batch Loss: 0.2352856695652008\n",
      "Epoch 2998, Loss: 2.0206028521060944, Final Batch Loss: 0.3549143373966217\n",
      "Epoch 2999, Loss: 2.1468291580677032, Final Batch Loss: 0.6547835469245911\n",
      "Epoch 3000, Loss: 1.9185561537742615, Final Batch Loss: 0.3887541890144348\n",
      "Epoch 3001, Loss: 1.839885652065277, Final Batch Loss: 0.3065052330493927\n",
      "Epoch 3002, Loss: 1.7111822590231895, Final Batch Loss: 0.08783232420682907\n",
      "Epoch 3003, Loss: 1.6710033565759659, Final Batch Loss: 0.13710059225559235\n",
      "Epoch 3004, Loss: 1.7939582467079163, Final Batch Loss: 0.26512396335601807\n",
      "Epoch 3005, Loss: 1.855577141046524, Final Batch Loss: 0.27148061990737915\n",
      "Epoch 3006, Loss: 1.9024848639965057, Final Batch Loss: 0.4257270395755768\n",
      "Epoch 3007, Loss: 2.006897956132889, Final Batch Loss: 0.31854256987571716\n",
      "Epoch 3008, Loss: 1.6708928793668747, Final Batch Loss: 0.21053646504878998\n",
      "Epoch 3009, Loss: 1.7495758831501007, Final Batch Loss: 0.21079126000404358\n",
      "Epoch 3010, Loss: 2.13526713848114, Final Batch Loss: 0.5217745900154114\n",
      "Epoch 3011, Loss: 2.2042481005191803, Final Batch Loss: 0.5996192693710327\n",
      "Epoch 3012, Loss: 2.095664232969284, Final Batch Loss: 0.5211054682731628\n",
      "Epoch 3013, Loss: 1.787816122174263, Final Batch Loss: 0.12797115743160248\n",
      "Epoch 3014, Loss: 2.068882554769516, Final Batch Loss: 0.5255737900733948\n",
      "Epoch 3015, Loss: 1.9041439145803452, Final Batch Loss: 0.2130654901266098\n",
      "Epoch 3016, Loss: 1.771730750799179, Final Batch Loss: 0.2257765829563141\n",
      "Epoch 3017, Loss: 1.7093922346830368, Final Batch Loss: 0.17342348396778107\n",
      "Epoch 3018, Loss: 1.9486823081970215, Final Batch Loss: 0.44366464018821716\n",
      "Epoch 3019, Loss: 2.2336425185203552, Final Batch Loss: 0.7280114889144897\n",
      "Epoch 3020, Loss: 2.43720406293869, Final Batch Loss: 0.8090576529502869\n",
      "Epoch 3021, Loss: 2.2297779321670532, Final Batch Loss: 0.47529107332229614\n",
      "Epoch 3022, Loss: 2.069123297929764, Final Batch Loss: 0.3978942930698395\n",
      "Epoch 3023, Loss: 2.2186191380023956, Final Batch Loss: 0.4394266605377197\n",
      "Epoch 3024, Loss: 2.2506069242954254, Final Batch Loss: 0.39336320757865906\n",
      "Epoch 3025, Loss: 2.38372141122818, Final Batch Loss: 0.419801265001297\n",
      "Epoch 3026, Loss: 2.237059324979782, Final Batch Loss: 0.5178828239440918\n",
      "Epoch 3027, Loss: 1.7245710045099258, Final Batch Loss: 0.1338365226984024\n",
      "Epoch 3028, Loss: 2.024584323167801, Final Batch Loss: 0.2601725161075592\n",
      "Epoch 3029, Loss: 1.841273307800293, Final Batch Loss: 0.26749756932258606\n",
      "Epoch 3030, Loss: 1.9411825835704803, Final Batch Loss: 0.22935175895690918\n",
      "Epoch 3031, Loss: 1.7713621258735657, Final Batch Loss: 0.25594812631607056\n",
      "Epoch 3032, Loss: 2.0493692457675934, Final Batch Loss: 0.48092737793922424\n",
      "Epoch 3033, Loss: 2.0638160705566406, Final Batch Loss: 0.4859449863433838\n",
      "Epoch 3034, Loss: 1.7730750143527985, Final Batch Loss: 0.2451481819152832\n",
      "Epoch 3035, Loss: 1.8639199137687683, Final Batch Loss: 0.38643696904182434\n",
      "Epoch 3036, Loss: 2.0312729477882385, Final Batch Loss: 0.5215356945991516\n",
      "Epoch 3037, Loss: 1.9381622076034546, Final Batch Loss: 0.4202243685722351\n",
      "Epoch 3038, Loss: 1.9748822152614594, Final Batch Loss: 0.3532092869281769\n",
      "Epoch 3039, Loss: 1.978313148021698, Final Batch Loss: 0.4137100577354431\n",
      "Epoch 3040, Loss: 2.1375365257263184, Final Batch Loss: 0.5915430188179016\n",
      "Epoch 3041, Loss: 1.929429978132248, Final Batch Loss: 0.32879918813705444\n",
      "Epoch 3042, Loss: 2.2382725179195404, Final Batch Loss: 0.5804458260536194\n",
      "Epoch 3043, Loss: 1.989153504371643, Final Batch Loss: 0.33266571164131165\n",
      "Epoch 3044, Loss: 2.1428963243961334, Final Batch Loss: 0.5648173093795776\n",
      "Epoch 3045, Loss: 2.0357267260551453, Final Batch Loss: 0.39045092463493347\n",
      "Epoch 3046, Loss: 1.7485419064760208, Final Batch Loss: 0.12711967527866364\n",
      "Epoch 3047, Loss: 2.072546511888504, Final Batch Loss: 0.600330650806427\n",
      "Epoch 3048, Loss: 2.185612440109253, Final Batch Loss: 0.6028981804847717\n",
      "Epoch 3049, Loss: 1.8565770387649536, Final Batch Loss: 0.3640596568584442\n",
      "Epoch 3050, Loss: 1.7261012494564056, Final Batch Loss: 0.28502488136291504\n",
      "Epoch 3051, Loss: 2.6110803484916687, Final Batch Loss: 1.0959360599517822\n",
      "Epoch 3052, Loss: 2.086745023727417, Final Batch Loss: 0.6035648584365845\n",
      "Epoch 3053, Loss: 1.647393599152565, Final Batch Loss: 0.1426495760679245\n",
      "Epoch 3054, Loss: 1.8754040002822876, Final Batch Loss: 0.4513755142688751\n",
      "Epoch 3055, Loss: 2.015690267086029, Final Batch Loss: 0.4146619439125061\n",
      "Epoch 3056, Loss: 1.7574195712804794, Final Batch Loss: 0.2476855367422104\n",
      "Epoch 3057, Loss: 1.863943874835968, Final Batch Loss: 0.3630710244178772\n",
      "Epoch 3058, Loss: 2.286014646291733, Final Batch Loss: 0.6256033182144165\n",
      "Epoch 3059, Loss: 2.117006480693817, Final Batch Loss: 0.5292968153953552\n",
      "Epoch 3060, Loss: 1.6553204506635666, Final Batch Loss: 0.14828406274318695\n",
      "Epoch 3061, Loss: 1.8999013304710388, Final Batch Loss: 0.42229774594306946\n",
      "Epoch 3062, Loss: 1.7749731540679932, Final Batch Loss: 0.27585387229919434\n",
      "Epoch 3063, Loss: 2.213955044746399, Final Batch Loss: 0.6851028203964233\n",
      "Epoch 3064, Loss: 1.9883008301258087, Final Batch Loss: 0.5003942251205444\n",
      "Epoch 3065, Loss: 1.8652423024177551, Final Batch Loss: 0.355324923992157\n",
      "Epoch 3066, Loss: 1.92899551987648, Final Batch Loss: 0.417385071516037\n",
      "Epoch 3067, Loss: 1.7670919001102448, Final Batch Loss: 0.3594847023487091\n",
      "Epoch 3068, Loss: 2.1848058700561523, Final Batch Loss: 0.6469659209251404\n",
      "Epoch 3069, Loss: 1.89222452044487, Final Batch Loss: 0.36037635803222656\n",
      "Epoch 3070, Loss: 1.733120173215866, Final Batch Loss: 0.276770681142807\n",
      "Epoch 3071, Loss: 2.0454944372177124, Final Batch Loss: 0.559615433216095\n",
      "Epoch 3072, Loss: 2.123979926109314, Final Batch Loss: 0.5290277600288391\n",
      "Epoch 3073, Loss: 2.168561726808548, Final Batch Loss: 0.5201417803764343\n",
      "Epoch 3074, Loss: 1.9623449444770813, Final Batch Loss: 0.41255539655685425\n",
      "Epoch 3075, Loss: 1.8887879252433777, Final Batch Loss: 0.35733774304389954\n",
      "Epoch 3076, Loss: 1.9671235084533691, Final Batch Loss: 0.22046545147895813\n",
      "Epoch 3077, Loss: 1.8915346264839172, Final Batch Loss: 0.41986602544784546\n",
      "Epoch 3078, Loss: 1.8239771127700806, Final Batch Loss: 0.32799047231674194\n",
      "Epoch 3079, Loss: 1.6644838899374008, Final Batch Loss: 0.2019730657339096\n",
      "Epoch 3080, Loss: 1.7425689399242401, Final Batch Loss: 0.2013266384601593\n",
      "Epoch 3081, Loss: 1.9590264856815338, Final Batch Loss: 0.5105281472206116\n",
      "Epoch 3082, Loss: 2.075887143611908, Final Batch Loss: 0.5195651650428772\n",
      "Epoch 3083, Loss: 1.716971755027771, Final Batch Loss: 0.2578514516353607\n",
      "Epoch 3084, Loss: 1.9109055399894714, Final Batch Loss: 0.31738921999931335\n",
      "Epoch 3085, Loss: 1.6814670264720917, Final Batch Loss: 0.13981866836547852\n",
      "Epoch 3086, Loss: 1.7845694720745087, Final Batch Loss: 0.2734934985637665\n",
      "Epoch 3087, Loss: 1.7803188860416412, Final Batch Loss: 0.2871023118495941\n",
      "Epoch 3088, Loss: 1.7966745495796204, Final Batch Loss: 0.3174010217189789\n",
      "Epoch 3089, Loss: 1.567215472459793, Final Batch Loss: 0.0844992995262146\n",
      "Epoch 3090, Loss: 1.7674405127763748, Final Batch Loss: 0.23216833174228668\n",
      "Epoch 3091, Loss: 1.7916913628578186, Final Batch Loss: 0.36295071244239807\n",
      "Epoch 3092, Loss: 2.1106765270233154, Final Batch Loss: 0.4877074062824249\n",
      "Epoch 3093, Loss: 1.779700607061386, Final Batch Loss: 0.30469754338264465\n",
      "Epoch 3094, Loss: 2.337624877691269, Final Batch Loss: 0.718738853931427\n",
      "Epoch 3095, Loss: 1.7541459500789642, Final Batch Loss: 0.25842219591140747\n",
      "Epoch 3096, Loss: 2.069354832172394, Final Batch Loss: 0.5475930571556091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3097, Loss: 1.5462810173630714, Final Batch Loss: 0.04966513067483902\n",
      "Epoch 3098, Loss: 2.4105859994888306, Final Batch Loss: 0.8677650094032288\n",
      "Epoch 3099, Loss: 1.855133593082428, Final Batch Loss: 0.29496583342552185\n",
      "Epoch 3100, Loss: 1.694681815803051, Final Batch Loss: 0.12466266006231308\n",
      "Epoch 3101, Loss: 1.8599657118320465, Final Batch Loss: 0.3263317942619324\n",
      "Epoch 3102, Loss: 1.8595690429210663, Final Batch Loss: 0.2524711489677429\n",
      "Epoch 3103, Loss: 1.828670620918274, Final Batch Loss: 0.34821954369544983\n",
      "Epoch 3104, Loss: 1.8390432596206665, Final Batch Loss: 0.32701587677001953\n",
      "Epoch 3105, Loss: 2.0515066385269165, Final Batch Loss: 0.4006434977054596\n",
      "Epoch 3106, Loss: 1.937461644411087, Final Batch Loss: 0.3912637233734131\n",
      "Epoch 3107, Loss: 2.2423384487628937, Final Batch Loss: 0.8093878626823425\n",
      "Epoch 3108, Loss: 2.08464452624321, Final Batch Loss: 0.6375904083251953\n",
      "Epoch 3109, Loss: 2.023097723722458, Final Batch Loss: 0.4171026647090912\n",
      "Epoch 3110, Loss: 1.9526050984859467, Final Batch Loss: 0.3929954171180725\n",
      "Epoch 3111, Loss: 2.04089692234993, Final Batch Loss: 0.5521132349967957\n",
      "Epoch 3112, Loss: 2.1129393875598907, Final Batch Loss: 0.6173576712608337\n",
      "Epoch 3113, Loss: 2.3444041907787323, Final Batch Loss: 0.8783111572265625\n",
      "Epoch 3114, Loss: 1.8681953847408295, Final Batch Loss: 0.42303964495658875\n",
      "Epoch 3115, Loss: 1.6612289398908615, Final Batch Loss: 0.14618562161922455\n",
      "Epoch 3116, Loss: 1.566951870918274, Final Batch Loss: 0.17341509461402893\n",
      "Epoch 3117, Loss: 1.794429987668991, Final Batch Loss: 0.2864660620689392\n",
      "Epoch 3118, Loss: 2.0763188898563385, Final Batch Loss: 0.47939082980155945\n",
      "Epoch 3119, Loss: 1.9430420994758606, Final Batch Loss: 0.4866868853569031\n",
      "Epoch 3120, Loss: 1.613892838358879, Final Batch Loss: 0.14577986299991608\n",
      "Epoch 3121, Loss: 1.7283067405223846, Final Batch Loss: 0.27168089151382446\n",
      "Epoch 3122, Loss: 1.9620254635810852, Final Batch Loss: 0.4765152633190155\n",
      "Epoch 3123, Loss: 1.7265986502170563, Final Batch Loss: 0.28012582659721375\n",
      "Epoch 3124, Loss: 2.0660917460918427, Final Batch Loss: 0.5736614465713501\n",
      "Epoch 3125, Loss: 1.8039252758026123, Final Batch Loss: 0.2379317283630371\n",
      "Epoch 3126, Loss: 2.057512044906616, Final Batch Loss: 0.48956018686294556\n",
      "Epoch 3127, Loss: 1.9770566523075104, Final Batch Loss: 0.4495740532875061\n",
      "Epoch 3128, Loss: 1.9909279644489288, Final Batch Loss: 0.48358631134033203\n",
      "Epoch 3129, Loss: 2.0739279091358185, Final Batch Loss: 0.5033214688301086\n",
      "Epoch 3130, Loss: 1.8956205546855927, Final Batch Loss: 0.31692004203796387\n",
      "Epoch 3131, Loss: 1.6489951610565186, Final Batch Loss: 0.11429727077484131\n",
      "Epoch 3132, Loss: 1.9420181810855865, Final Batch Loss: 0.28937390446662903\n",
      "Epoch 3133, Loss: 1.878563791513443, Final Batch Loss: 0.2684759497642517\n",
      "Epoch 3134, Loss: 2.302857756614685, Final Batch Loss: 0.744576096534729\n",
      "Epoch 3135, Loss: 1.9109915494918823, Final Batch Loss: 0.3671892583370209\n",
      "Epoch 3136, Loss: 1.658983901143074, Final Batch Loss: 0.16930602490901947\n",
      "Epoch 3137, Loss: 1.798793911933899, Final Batch Loss: 0.33758920431137085\n",
      "Epoch 3138, Loss: 1.901306927204132, Final Batch Loss: 0.3593362867832184\n",
      "Epoch 3139, Loss: 1.8453843295574188, Final Batch Loss: 0.17123523354530334\n",
      "Epoch 3140, Loss: 2.235369771718979, Final Batch Loss: 0.45910316705703735\n",
      "Epoch 3141, Loss: 1.9299529790878296, Final Batch Loss: 0.5269944071769714\n",
      "Epoch 3142, Loss: 1.938719481229782, Final Batch Loss: 0.5254825353622437\n",
      "Epoch 3143, Loss: 2.1421644389629364, Final Batch Loss: 0.5515723824501038\n",
      "Epoch 3144, Loss: 1.9220877885818481, Final Batch Loss: 0.44225654006004333\n",
      "Epoch 3145, Loss: 2.3183944821357727, Final Batch Loss: 0.7492273449897766\n",
      "Epoch 3146, Loss: 1.792327731847763, Final Batch Loss: 0.256630003452301\n",
      "Epoch 3147, Loss: 1.7797567546367645, Final Batch Loss: 0.3232833743095398\n",
      "Epoch 3148, Loss: 1.851749747991562, Final Batch Loss: 0.33150508999824524\n",
      "Epoch 3149, Loss: 2.0471980273723602, Final Batch Loss: 0.6271548271179199\n",
      "Epoch 3150, Loss: 1.7863405048847198, Final Batch Loss: 0.28050729632377625\n",
      "Epoch 3151, Loss: 1.941673457622528, Final Batch Loss: 0.3045726418495178\n",
      "Epoch 3152, Loss: 1.96025750041008, Final Batch Loss: 0.4660472869873047\n",
      "Epoch 3153, Loss: 2.3686850368976593, Final Batch Loss: 0.634361207485199\n",
      "Epoch 3154, Loss: 1.9252222329378128, Final Batch Loss: 0.12569154798984528\n",
      "Epoch 3155, Loss: 1.867226466536522, Final Batch Loss: 0.17869935929775238\n",
      "Epoch 3156, Loss: 2.297712951898575, Final Batch Loss: 0.5307533740997314\n",
      "Epoch 3157, Loss: 2.3580309748649597, Final Batch Loss: 0.49466291069984436\n",
      "Epoch 3158, Loss: 1.9475229978561401, Final Batch Loss: 0.404610812664032\n",
      "Epoch 3159, Loss: 2.0719943940639496, Final Batch Loss: 0.3992784023284912\n",
      "Epoch 3160, Loss: 1.5730938166379929, Final Batch Loss: 0.0642799586057663\n",
      "Epoch 3161, Loss: 2.2725198566913605, Final Batch Loss: 0.5905750393867493\n",
      "Epoch 3162, Loss: 2.0733627378940582, Final Batch Loss: 0.38333868980407715\n",
      "Epoch 3163, Loss: 2.3974954783916473, Final Batch Loss: 0.6081627011299133\n",
      "Epoch 3164, Loss: 2.006661355495453, Final Batch Loss: 0.42035725712776184\n",
      "Epoch 3165, Loss: 2.4010762572288513, Final Batch Loss: 0.8372553586959839\n",
      "Epoch 3166, Loss: 1.9098873138427734, Final Batch Loss: 0.3859488070011139\n",
      "Epoch 3167, Loss: 1.8487894833087921, Final Batch Loss: 0.3223569393157959\n",
      "Epoch 3168, Loss: 1.699643224477768, Final Batch Loss: 0.12297356128692627\n",
      "Epoch 3169, Loss: 1.596348911523819, Final Batch Loss: 0.16768601536750793\n",
      "Epoch 3170, Loss: 1.690322607755661, Final Batch Loss: 0.17365369200706482\n",
      "Epoch 3171, Loss: 1.927097648382187, Final Batch Loss: 0.4882105886936188\n",
      "Epoch 3172, Loss: 1.761554628610611, Final Batch Loss: 0.2932472229003906\n",
      "Epoch 3173, Loss: 2.0425869822502136, Final Batch Loss: 0.4532417953014374\n",
      "Epoch 3174, Loss: 1.7544248402118683, Final Batch Loss: 0.32660046219825745\n",
      "Epoch 3175, Loss: 1.5145476311445236, Final Batch Loss: 0.17792309820652008\n",
      "Epoch 3176, Loss: 1.824022114276886, Final Batch Loss: 0.29219502210617065\n",
      "Epoch 3177, Loss: 2.581895560026169, Final Batch Loss: 0.9445492625236511\n",
      "Epoch 3178, Loss: 2.2994116246700287, Final Batch Loss: 0.4859289824962616\n",
      "Epoch 3179, Loss: 1.6779844164848328, Final Batch Loss: 0.15665778517723083\n",
      "Epoch 3180, Loss: 2.6035511791706085, Final Batch Loss: 1.1024309396743774\n",
      "Epoch 3181, Loss: 2.0077916979789734, Final Batch Loss: 0.45074501633644104\n",
      "Epoch 3182, Loss: 2.2845676839351654, Final Batch Loss: 0.47340747714042664\n",
      "Epoch 3183, Loss: 2.193382829427719, Final Batch Loss: 0.44685810804367065\n",
      "Epoch 3184, Loss: 2.613290399312973, Final Batch Loss: 0.7054619193077087\n",
      "Epoch 3185, Loss: 2.199502110481262, Final Batch Loss: 0.521119236946106\n",
      "Epoch 3186, Loss: 2.121815860271454, Final Batch Loss: 0.43384021520614624\n",
      "Epoch 3187, Loss: 1.9444943368434906, Final Batch Loss: 0.31085532903671265\n",
      "Epoch 3188, Loss: 1.662751391530037, Final Batch Loss: 0.14798451960086823\n",
      "Epoch 3189, Loss: 1.9283927381038666, Final Batch Loss: 0.3292854130268097\n",
      "Epoch 3190, Loss: 1.8082648813724518, Final Batch Loss: 0.3450759947299957\n",
      "Epoch 3191, Loss: 2.1295977234840393, Final Batch Loss: 0.5592411756515503\n",
      "Epoch 3192, Loss: 1.7868865430355072, Final Batch Loss: 0.2788090109825134\n",
      "Epoch 3193, Loss: 1.82827228307724, Final Batch Loss: 0.33569151163101196\n",
      "Epoch 3194, Loss: 2.066243350505829, Final Batch Loss: 0.5990533828735352\n",
      "Epoch 3195, Loss: 2.119040995836258, Final Batch Loss: 0.625512421131134\n",
      "Epoch 3196, Loss: 1.9009442925453186, Final Batch Loss: 0.40585991740226746\n",
      "Epoch 3197, Loss: 1.9390885829925537, Final Batch Loss: 0.39902809262275696\n",
      "Epoch 3198, Loss: 2.553961902856827, Final Batch Loss: 0.9160819053649902\n",
      "Epoch 3199, Loss: 2.039873868227005, Final Batch Loss: 0.41678357124328613\n",
      "Epoch 3200, Loss: 1.8899236023426056, Final Batch Loss: 0.44658467173576355\n",
      "Epoch 3201, Loss: 2.104525774717331, Final Batch Loss: 0.51505047082901\n",
      "Epoch 3202, Loss: 1.9201812148094177, Final Batch Loss: 0.4247642159461975\n",
      "Epoch 3203, Loss: 1.9239690899848938, Final Batch Loss: 0.36729589104652405\n",
      "Epoch 3204, Loss: 1.7729459553956985, Final Batch Loss: 0.12314106523990631\n",
      "Epoch 3205, Loss: 1.8400585353374481, Final Batch Loss: 0.37135735154151917\n",
      "Epoch 3206, Loss: 1.891345053911209, Final Batch Loss: 0.3013969361782074\n",
      "Epoch 3207, Loss: 2.091812878847122, Final Batch Loss: 0.5655155777931213\n",
      "Epoch 3208, Loss: 2.2827916145324707, Final Batch Loss: 0.7916002869606018\n",
      "Epoch 3209, Loss: 1.7303572595119476, Final Batch Loss: 0.1959494650363922\n",
      "Epoch 3210, Loss: 2.0136366486549377, Final Batch Loss: 0.41211047768592834\n",
      "Epoch 3211, Loss: 2.04858136177063, Final Batch Loss: 0.4162132740020752\n",
      "Epoch 3212, Loss: 1.8855149447917938, Final Batch Loss: 0.37307125329971313\n",
      "Epoch 3213, Loss: 1.6834307238459587, Final Batch Loss: 0.09477349370718002\n",
      "Epoch 3214, Loss: 1.5797055065631866, Final Batch Loss: 0.1493539810180664\n",
      "Epoch 3215, Loss: 1.9687981605529785, Final Batch Loss: 0.39514583349227905\n",
      "Epoch 3216, Loss: 1.974850356578827, Final Batch Loss: 0.4970613121986389\n",
      "Epoch 3217, Loss: 2.076823502779007, Final Batch Loss: 0.6119754910469055\n",
      "Epoch 3218, Loss: 1.6662844866514206, Final Batch Loss: 0.13453902304172516\n",
      "Epoch 3219, Loss: 1.6644854322075844, Final Batch Loss: 0.11625490337610245\n",
      "Epoch 3220, Loss: 1.9034593105316162, Final Batch Loss: 0.3503044545650482\n",
      "Epoch 3221, Loss: 1.8567346334457397, Final Batch Loss: 0.38759487867355347\n",
      "Epoch 3222, Loss: 1.7293850779533386, Final Batch Loss: 0.245845764875412\n",
      "Epoch 3223, Loss: 1.8251662850379944, Final Batch Loss: 0.31136074662208557\n",
      "Epoch 3224, Loss: 2.0528095960617065, Final Batch Loss: 0.5215351581573486\n",
      "Epoch 3225, Loss: 1.9715077877044678, Final Batch Loss: 0.36550644040107727\n",
      "Epoch 3226, Loss: 1.807655617594719, Final Batch Loss: 0.19229330122470856\n",
      "Epoch 3227, Loss: 2.1351314783096313, Final Batch Loss: 0.506553590297699\n",
      "Epoch 3228, Loss: 1.8559283912181854, Final Batch Loss: 0.3774067759513855\n",
      "Epoch 3229, Loss: 1.9070308208465576, Final Batch Loss: 0.32085856795310974\n",
      "Epoch 3230, Loss: 2.5164585411548615, Final Batch Loss: 0.981634795665741\n",
      "Epoch 3231, Loss: 1.6222241818904877, Final Batch Loss: 0.18796417117118835\n",
      "Epoch 3232, Loss: 2.511056423187256, Final Batch Loss: 0.9128404855728149\n",
      "Epoch 3233, Loss: 1.8899377286434174, Final Batch Loss: 0.30242618918418884\n",
      "Epoch 3234, Loss: 2.121102601289749, Final Batch Loss: 0.4279211163520813\n",
      "Epoch 3235, Loss: 1.8897608518600464, Final Batch Loss: 0.326882541179657\n",
      "Epoch 3236, Loss: 1.882118582725525, Final Batch Loss: 0.4053773581981659\n",
      "Epoch 3237, Loss: 1.9274384379386902, Final Batch Loss: 0.34945687651634216\n",
      "Epoch 3238, Loss: 1.6533766835927963, Final Batch Loss: 0.1468489021062851\n",
      "Epoch 3239, Loss: 1.9472191631793976, Final Batch Loss: 0.3777066767215729\n",
      "Epoch 3240, Loss: 1.9762888550758362, Final Batch Loss: 0.4774169623851776\n",
      "Epoch 3241, Loss: 1.9144930243492126, Final Batch Loss: 0.36573705077171326\n",
      "Epoch 3242, Loss: 1.995096355676651, Final Batch Loss: 0.5476462244987488\n",
      "Epoch 3243, Loss: 1.5784202367067337, Final Batch Loss: 0.15332789719104767\n",
      "Epoch 3244, Loss: 2.0081607699394226, Final Batch Loss: 0.30552592873573303\n",
      "Epoch 3245, Loss: 2.0580166578292847, Final Batch Loss: 0.34705397486686707\n",
      "Epoch 3246, Loss: 2.5022807121276855, Final Batch Loss: 0.8421004414558411\n",
      "Epoch 3247, Loss: 1.7574339509010315, Final Batch Loss: 0.28657665848731995\n",
      "Epoch 3248, Loss: 1.7798228114843369, Final Batch Loss: 0.08549566566944122\n",
      "Epoch 3249, Loss: 1.9927514493465424, Final Batch Loss: 0.298135370016098\n",
      "Epoch 3250, Loss: 2.2292617559432983, Final Batch Loss: 0.5206502676010132\n",
      "Epoch 3251, Loss: 2.0035580098629, Final Batch Loss: 0.4218723475933075\n",
      "Epoch 3252, Loss: 1.6329670697450638, Final Batch Loss: 0.1392376869916916\n",
      "Epoch 3253, Loss: 2.1139021515846252, Final Batch Loss: 0.5947586297988892\n",
      "Epoch 3254, Loss: 1.8596014082431793, Final Batch Loss: 0.3474145829677582\n",
      "Epoch 3255, Loss: 1.4654727131128311, Final Batch Loss: 0.08739472925662994\n",
      "Epoch 3256, Loss: 1.9411468207836151, Final Batch Loss: 0.44294530153274536\n",
      "Epoch 3257, Loss: 1.7238117456436157, Final Batch Loss: 0.3862108290195465\n",
      "Epoch 3258, Loss: 1.9533738195896149, Final Batch Loss: 0.3689822554588318\n",
      "Epoch 3259, Loss: 1.6559930741786957, Final Batch Loss: 0.21341267228126526\n",
      "Epoch 3260, Loss: 1.9286801218986511, Final Batch Loss: 0.3925410807132721\n",
      "Epoch 3261, Loss: 2.113186329603195, Final Batch Loss: 0.5300277471542358\n",
      "Epoch 3262, Loss: 1.6521862745285034, Final Batch Loss: 0.25639113783836365\n",
      "Epoch 3263, Loss: 1.7390091121196747, Final Batch Loss: 0.2517606317996979\n",
      "Epoch 3264, Loss: 1.8161408305168152, Final Batch Loss: 0.3750135004520416\n",
      "Epoch 3265, Loss: 1.652345433831215, Final Batch Loss: 0.24050574004650116\n",
      "Epoch 3266, Loss: 2.006772816181183, Final Batch Loss: 0.46557721495628357\n",
      "Epoch 3267, Loss: 1.8121939599514008, Final Batch Loss: 0.32329750061035156\n",
      "Epoch 3268, Loss: 1.882304698228836, Final Batch Loss: 0.29503175616264343\n",
      "Epoch 3269, Loss: 1.545456424355507, Final Batch Loss: 0.1302724927663803\n",
      "Epoch 3270, Loss: 1.7549583315849304, Final Batch Loss: 0.27076080441474915\n",
      "Epoch 3271, Loss: 1.603806532919407, Final Batch Loss: 0.10586658865213394\n",
      "Epoch 3272, Loss: 1.6304504126310349, Final Batch Loss: 0.24566535651683807\n",
      "Epoch 3273, Loss: 2.034561425447464, Final Batch Loss: 0.532555878162384\n",
      "Epoch 3274, Loss: 1.7066503018140793, Final Batch Loss: 0.2243374139070511\n",
      "Epoch 3275, Loss: 1.9527693390846252, Final Batch Loss: 0.33287909626960754\n",
      "Epoch 3276, Loss: 1.5973973274230957, Final Batch Loss: 0.2367059290409088\n",
      "Epoch 3277, Loss: 1.8098376393318176, Final Batch Loss: 0.3642655909061432\n",
      "Epoch 3278, Loss: 1.9696852564811707, Final Batch Loss: 0.4717928469181061\n",
      "Epoch 3279, Loss: 1.6967612653970718, Final Batch Loss: 0.1803775578737259\n",
      "Epoch 3280, Loss: 1.6360936164855957, Final Batch Loss: 0.14737066626548767\n",
      "Epoch 3281, Loss: 1.5324025936424732, Final Batch Loss: 0.050078947097063065\n",
      "Epoch 3282, Loss: 1.6900243908166885, Final Batch Loss: 0.15759272873401642\n",
      "Epoch 3283, Loss: 2.1336834728717804, Final Batch Loss: 0.5709741115570068\n",
      "Epoch 3284, Loss: 1.9232778251171112, Final Batch Loss: 0.4189581274986267\n",
      "Epoch 3285, Loss: 1.88069286942482, Final Batch Loss: 0.4025203287601471\n",
      "Epoch 3286, Loss: 1.6611382812261581, Final Batch Loss: 0.1710684448480606\n",
      "Epoch 3287, Loss: 1.6284545958042145, Final Batch Loss: 0.16970515251159668\n",
      "Epoch 3288, Loss: 2.2703645825386047, Final Batch Loss: 0.8353110551834106\n",
      "Epoch 3289, Loss: 2.2148396968841553, Final Batch Loss: 0.6402471661567688\n",
      "Epoch 3290, Loss: 1.7012889236211777, Final Batch Loss: 0.21901346743106842\n",
      "Epoch 3291, Loss: 1.5533407628536224, Final Batch Loss: 0.17167708277702332\n",
      "Epoch 3292, Loss: 1.7716589272022247, Final Batch Loss: 0.35277512669563293\n",
      "Epoch 3293, Loss: 2.000422179698944, Final Batch Loss: 0.5982590913772583\n",
      "Epoch 3294, Loss: 2.094215005636215, Final Batch Loss: 0.5128415822982788\n",
      "Epoch 3295, Loss: 1.6332729011774063, Final Batch Loss: 0.11312068998813629\n",
      "Epoch 3296, Loss: 1.7799183428287506, Final Batch Loss: 0.2488696575164795\n",
      "Epoch 3297, Loss: 2.0016709566116333, Final Batch Loss: 0.39182141423225403\n",
      "Epoch 3298, Loss: 2.037340074777603, Final Batch Loss: 0.42528727650642395\n",
      "Epoch 3299, Loss: 2.204066574573517, Final Batch Loss: 0.5293728113174438\n",
      "Epoch 3300, Loss: 2.2714515924453735, Final Batch Loss: 0.6318215131759644\n",
      "Epoch 3301, Loss: 2.190487325191498, Final Batch Loss: 0.5727194547653198\n",
      "Epoch 3302, Loss: 2.407717078924179, Final Batch Loss: 0.7600515484809875\n",
      "Epoch 3303, Loss: 2.1531234681606293, Final Batch Loss: 0.5653539299964905\n",
      "Epoch 3304, Loss: 1.9919784367084503, Final Batch Loss: 0.25788912177085876\n",
      "Epoch 3305, Loss: 1.838214561343193, Final Batch Loss: 0.19466416537761688\n",
      "Epoch 3306, Loss: 2.0155374109745026, Final Batch Loss: 0.30721575021743774\n",
      "Epoch 3307, Loss: 2.162118434906006, Final Batch Loss: 0.6057927012443542\n",
      "Epoch 3308, Loss: 1.7397397756576538, Final Batch Loss: 0.24156510829925537\n",
      "Epoch 3309, Loss: 1.988171398639679, Final Batch Loss: 0.299679696559906\n",
      "Epoch 3310, Loss: 1.810564860701561, Final Batch Loss: 0.20186863839626312\n",
      "Epoch 3311, Loss: 2.1261186599731445, Final Batch Loss: 0.6050804257392883\n",
      "Epoch 3312, Loss: 2.1485242545604706, Final Batch Loss: 0.7024261355400085\n",
      "Epoch 3313, Loss: 1.8281909823417664, Final Batch Loss: 0.25113019347190857\n",
      "Epoch 3314, Loss: 2.011828690767288, Final Batch Loss: 0.5296816229820251\n",
      "Epoch 3315, Loss: 1.8708601891994476, Final Batch Loss: 0.3652648627758026\n",
      "Epoch 3316, Loss: 1.767839103937149, Final Batch Loss: 0.3059101700782776\n",
      "Epoch 3317, Loss: 2.1016896069049835, Final Batch Loss: 0.6882723569869995\n",
      "Epoch 3318, Loss: 1.9296652972698212, Final Batch Loss: 0.49679794907569885\n",
      "Epoch 3319, Loss: 1.9195663332939148, Final Batch Loss: 0.4146568179130554\n",
      "Epoch 3320, Loss: 1.8488282561302185, Final Batch Loss: 0.2997979521751404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3321, Loss: 1.7173980325460434, Final Batch Loss: 0.18148337304592133\n",
      "Epoch 3322, Loss: 1.845168262720108, Final Batch Loss: 0.336346834897995\n",
      "Epoch 3323, Loss: 1.6745800524950027, Final Batch Loss: 0.12652291357517242\n",
      "Epoch 3324, Loss: 1.8416809439659119, Final Batch Loss: 0.2501281201839447\n",
      "Epoch 3325, Loss: 1.5065539367496967, Final Batch Loss: 0.045639682561159134\n",
      "Epoch 3326, Loss: 1.8635481894016266, Final Batch Loss: 0.35397928953170776\n",
      "Epoch 3327, Loss: 1.7585580199956894, Final Batch Loss: 0.21447692811489105\n",
      "Epoch 3328, Loss: 1.900626927614212, Final Batch Loss: 0.37837740778923035\n",
      "Epoch 3329, Loss: 1.8824642151594162, Final Batch Loss: 0.22866301238536835\n",
      "Epoch 3330, Loss: 1.6929689347743988, Final Batch Loss: 0.2658056616783142\n",
      "Epoch 3331, Loss: 1.4376861089840531, Final Batch Loss: 0.008990907110273838\n",
      "Epoch 3332, Loss: 1.6399645954370499, Final Batch Loss: 0.2498740702867508\n",
      "Epoch 3333, Loss: 1.8783342838287354, Final Batch Loss: 0.4530377984046936\n",
      "Epoch 3334, Loss: 1.9459992349147797, Final Batch Loss: 0.457753986120224\n",
      "Epoch 3335, Loss: 2.1342786848545074, Final Batch Loss: 0.5995308756828308\n",
      "Epoch 3336, Loss: 1.5555037707090378, Final Batch Loss: 0.10774348676204681\n",
      "Epoch 3337, Loss: 1.7724511325359344, Final Batch Loss: 0.3920287489891052\n",
      "Epoch 3338, Loss: 2.2490315437316895, Final Batch Loss: 0.7194830179214478\n",
      "Epoch 3339, Loss: 1.6789808720350266, Final Batch Loss: 0.1899852603673935\n",
      "Epoch 3340, Loss: 1.9305581450462341, Final Batch Loss: 0.44314709305763245\n",
      "Epoch 3341, Loss: 1.698174238204956, Final Batch Loss: 0.1694389283657074\n",
      "Epoch 3342, Loss: 1.680411621928215, Final Batch Loss: 0.20626787841320038\n",
      "Epoch 3343, Loss: 1.6214265376329422, Final Batch Loss: 0.21160246431827545\n",
      "Epoch 3344, Loss: 1.6805180460214615, Final Batch Loss: 0.1751113384962082\n",
      "Epoch 3345, Loss: 1.7786043733358383, Final Batch Loss: 0.2221052497625351\n",
      "Epoch 3346, Loss: 1.7105634361505508, Final Batch Loss: 0.1353694349527359\n",
      "Epoch 3347, Loss: 2.0510068237781525, Final Batch Loss: 0.6638519167900085\n",
      "Epoch 3348, Loss: 1.9819944202899933, Final Batch Loss: 0.5798231959342957\n",
      "Epoch 3349, Loss: 1.6688922345638275, Final Batch Loss: 0.15409627556800842\n",
      "Epoch 3350, Loss: 1.73206827044487, Final Batch Loss: 0.3468441367149353\n",
      "Epoch 3351, Loss: 1.628655031323433, Final Batch Loss: 0.22644798457622528\n",
      "Epoch 3352, Loss: 1.994456946849823, Final Batch Loss: 0.5487324595451355\n",
      "Epoch 3353, Loss: 2.044106036424637, Final Batch Loss: 0.6261361241340637\n",
      "Epoch 3354, Loss: 1.8555578291416168, Final Batch Loss: 0.34914430975914\n",
      "Epoch 3355, Loss: 1.781720608472824, Final Batch Loss: 0.331543892621994\n",
      "Epoch 3356, Loss: 1.8993299305438995, Final Batch Loss: 0.42208927869796753\n",
      "Epoch 3357, Loss: 1.647240698337555, Final Batch Loss: 0.30242225527763367\n",
      "Epoch 3358, Loss: 1.8420478701591492, Final Batch Loss: 0.3470492362976074\n",
      "Epoch 3359, Loss: 1.8695449829101562, Final Batch Loss: 0.3270767331123352\n",
      "Epoch 3360, Loss: 1.6776749193668365, Final Batch Loss: 0.2559013366699219\n",
      "Epoch 3361, Loss: 1.9399471282958984, Final Batch Loss: 0.4929451644420624\n",
      "Epoch 3362, Loss: 1.86622753739357, Final Batch Loss: 0.44636625051498413\n",
      "Epoch 3363, Loss: 2.26444736123085, Final Batch Loss: 0.6776095032691956\n",
      "Epoch 3364, Loss: 1.6800874397158623, Final Batch Loss: 0.11661612242460251\n",
      "Epoch 3365, Loss: 1.7725439667701721, Final Batch Loss: 0.2615145146846771\n",
      "Epoch 3366, Loss: 2.124940365552902, Final Batch Loss: 0.5813928246498108\n",
      "Epoch 3367, Loss: 1.6380823776125908, Final Batch Loss: 0.12428992241621017\n",
      "Epoch 3368, Loss: 1.6061594188213348, Final Batch Loss: 0.1709827482700348\n",
      "Epoch 3369, Loss: 1.8601227700710297, Final Batch Loss: 0.40010854601860046\n",
      "Epoch 3370, Loss: 1.7511157095432281, Final Batch Loss: 0.3088575303554535\n",
      "Epoch 3371, Loss: 1.6126579493284225, Final Batch Loss: 0.2382335215806961\n",
      "Epoch 3372, Loss: 1.8709766268730164, Final Batch Loss: 0.31569904088974\n",
      "Epoch 3373, Loss: 1.624539166688919, Final Batch Loss: 0.19878017902374268\n",
      "Epoch 3374, Loss: 1.741622507572174, Final Batch Loss: 0.4665978252887726\n",
      "Epoch 3375, Loss: 1.6656741499900818, Final Batch Loss: 0.26689237356185913\n",
      "Epoch 3376, Loss: 1.7247318774461746, Final Batch Loss: 0.20006246864795685\n",
      "Epoch 3377, Loss: 1.8369892239570618, Final Batch Loss: 0.3994152247905731\n",
      "Epoch 3378, Loss: 1.860911786556244, Final Batch Loss: 0.4076527953147888\n",
      "Epoch 3379, Loss: 1.5712223127484322, Final Batch Loss: 0.12442145496606827\n",
      "Epoch 3380, Loss: 2.0192832350730896, Final Batch Loss: 0.550233781337738\n",
      "Epoch 3381, Loss: 2.5432857275009155, Final Batch Loss: 1.1257736682891846\n",
      "Epoch 3382, Loss: 1.682399421930313, Final Batch Loss: 0.27022120356559753\n",
      "Epoch 3383, Loss: 2.05344295501709, Final Batch Loss: 0.5612079501152039\n",
      "Epoch 3384, Loss: 1.7001747786998749, Final Batch Loss: 0.31538280844688416\n",
      "Epoch 3385, Loss: 1.6021834388375282, Final Batch Loss: 0.12116513401269913\n",
      "Epoch 3386, Loss: 1.6205307617783546, Final Batch Loss: 0.08702460676431656\n",
      "Epoch 3387, Loss: 1.8703356087207794, Final Batch Loss: 0.3120678961277008\n",
      "Epoch 3388, Loss: 2.1795870661735535, Final Batch Loss: 0.5692581534385681\n",
      "Epoch 3389, Loss: 2.5991297364234924, Final Batch Loss: 1.124790072441101\n",
      "Epoch 3390, Loss: 1.6777371019124985, Final Batch Loss: 0.13928814232349396\n",
      "Epoch 3391, Loss: 2.199350953102112, Final Batch Loss: 0.6327036619186401\n",
      "Epoch 3392, Loss: 1.9618765711784363, Final Batch Loss: 0.39987045526504517\n",
      "Epoch 3393, Loss: 2.246613711118698, Final Batch Loss: 0.55425626039505\n",
      "Epoch 3394, Loss: 2.125172972679138, Final Batch Loss: 0.4762763977050781\n",
      "Epoch 3395, Loss: 1.9870902597904205, Final Batch Loss: 0.3401300013065338\n",
      "Epoch 3396, Loss: 2.168859302997589, Final Batch Loss: 0.4552095830440521\n",
      "Epoch 3397, Loss: 1.6878325939178467, Final Batch Loss: 0.27461478114128113\n",
      "Epoch 3398, Loss: 2.5981236398220062, Final Batch Loss: 1.146219253540039\n",
      "Epoch 3399, Loss: 2.0255375802516937, Final Batch Loss: 0.44889765977859497\n",
      "Epoch 3400, Loss: 2.0831763446331024, Final Batch Loss: 0.4133203625679016\n",
      "Epoch 3401, Loss: 2.2359341979026794, Final Batch Loss: 0.73529052734375\n",
      "Epoch 3402, Loss: 2.0901563465595245, Final Batch Loss: 0.4406283497810364\n",
      "Epoch 3403, Loss: 1.8067280650138855, Final Batch Loss: 0.416188508272171\n",
      "Epoch 3404, Loss: 1.8379689157009125, Final Batch Loss: 0.4147261679172516\n",
      "Epoch 3405, Loss: 1.6227079331874847, Final Batch Loss: 0.2581711709499359\n",
      "Epoch 3406, Loss: 1.9977622628211975, Final Batch Loss: 0.5229048728942871\n",
      "Epoch 3407, Loss: 1.8801972270011902, Final Batch Loss: 0.43657663464546204\n",
      "Epoch 3408, Loss: 1.5063312500715256, Final Batch Loss: 0.1484972983598709\n",
      "Epoch 3409, Loss: 2.3618821501731873, Final Batch Loss: 0.8406999707221985\n",
      "Epoch 3410, Loss: 1.9469289779663086, Final Batch Loss: 0.3351036012172699\n",
      "Epoch 3411, Loss: 1.7490840256214142, Final Batch Loss: 0.2497311532497406\n",
      "Epoch 3412, Loss: 1.722802996635437, Final Batch Loss: 0.22891178727149963\n",
      "Epoch 3413, Loss: 1.920694261789322, Final Batch Loss: 0.4964706599712372\n",
      "Epoch 3414, Loss: 1.6088245585560799, Final Batch Loss: 0.09535924345254898\n",
      "Epoch 3415, Loss: 2.3952088057994843, Final Batch Loss: 1.01844322681427\n",
      "Epoch 3416, Loss: 2.10387459397316, Final Batch Loss: 0.6178160905838013\n",
      "Epoch 3417, Loss: 1.9237752854824066, Final Batch Loss: 0.530124306678772\n",
      "Epoch 3418, Loss: 1.8296520709991455, Final Batch Loss: 0.4399932026863098\n",
      "Epoch 3419, Loss: 1.6464370638132095, Final Batch Loss: 0.17196841537952423\n",
      "Epoch 3420, Loss: 1.9005005359649658, Final Batch Loss: 0.36260098218917847\n",
      "Epoch 3421, Loss: 1.75200055539608, Final Batch Loss: 0.20576457679271698\n",
      "Epoch 3422, Loss: 1.8698972165584564, Final Batch Loss: 0.35822901129722595\n",
      "Epoch 3423, Loss: 2.1315994560718536, Final Batch Loss: 0.6512309312820435\n",
      "Epoch 3424, Loss: 1.548329845070839, Final Batch Loss: 0.14716775715351105\n",
      "Epoch 3425, Loss: 1.8611332178115845, Final Batch Loss: 0.3012816607952118\n",
      "Epoch 3426, Loss: 1.866818368434906, Final Batch Loss: 0.3298250734806061\n",
      "Epoch 3427, Loss: 2.18089497089386, Final Batch Loss: 0.5308891534805298\n",
      "Epoch 3428, Loss: 1.7021573632955551, Final Batch Loss: 0.2320651262998581\n",
      "Epoch 3429, Loss: 1.685501217842102, Final Batch Loss: 0.3176325857639313\n",
      "Epoch 3430, Loss: 1.4815659448504448, Final Batch Loss: 0.10239662975072861\n",
      "Epoch 3431, Loss: 1.8095334768295288, Final Batch Loss: 0.3679649233818054\n",
      "Epoch 3432, Loss: 1.8054601848125458, Final Batch Loss: 0.3768238127231598\n",
      "Epoch 3433, Loss: 1.4788317680358887, Final Batch Loss: 0.09120017290115356\n",
      "Epoch 3434, Loss: 1.791066974401474, Final Batch Loss: 0.21385493874549866\n",
      "Epoch 3435, Loss: 1.984600454568863, Final Batch Loss: 0.5395784974098206\n",
      "Epoch 3436, Loss: 1.7321081161499023, Final Batch Loss: 0.31931546330451965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3437, Loss: 1.9535043835639954, Final Batch Loss: 0.5723969340324402\n",
      "Epoch 3438, Loss: 1.757685899734497, Final Batch Loss: 0.2570502758026123\n",
      "Epoch 3439, Loss: 1.7420913577079773, Final Batch Loss: 0.27991873025894165\n",
      "Epoch 3440, Loss: 1.585837960243225, Final Batch Loss: 0.13950210809707642\n",
      "Epoch 3441, Loss: 1.5531568229198456, Final Batch Loss: 0.19176748394966125\n",
      "Epoch 3442, Loss: 2.152790367603302, Final Batch Loss: 0.6505526900291443\n",
      "Epoch 3443, Loss: 2.2840125262737274, Final Batch Loss: 0.6845284700393677\n",
      "Epoch 3444, Loss: 2.116383671760559, Final Batch Loss: 0.6640820503234863\n",
      "Epoch 3445, Loss: 1.6625325083732605, Final Batch Loss: 0.23832586407661438\n",
      "Epoch 3446, Loss: 1.7269541174173355, Final Batch Loss: 0.23298127949237823\n",
      "Epoch 3447, Loss: 1.8305064737796783, Final Batch Loss: 0.39851894974708557\n",
      "Epoch 3448, Loss: 1.9946815967559814, Final Batch Loss: 0.5502615571022034\n",
      "Epoch 3449, Loss: 1.779325157403946, Final Batch Loss: 0.3265068829059601\n",
      "Epoch 3450, Loss: 1.9478383660316467, Final Batch Loss: 0.4922346770763397\n",
      "Epoch 3451, Loss: 1.91297447681427, Final Batch Loss: 0.3327992856502533\n",
      "Epoch 3452, Loss: 1.7124979794025421, Final Batch Loss: 0.27806586027145386\n",
      "Epoch 3453, Loss: 2.1081041395664215, Final Batch Loss: 0.5709343552589417\n",
      "Epoch 3454, Loss: 1.8789003193378448, Final Batch Loss: 0.37174955010414124\n",
      "Epoch 3455, Loss: 1.6541493237018585, Final Batch Loss: 0.13889354467391968\n",
      "Epoch 3456, Loss: 1.829540267586708, Final Batch Loss: 0.16714490950107574\n",
      "Epoch 3457, Loss: 1.6002188175916672, Final Batch Loss: 0.14968226850032806\n",
      "Epoch 3458, Loss: 1.8184125423431396, Final Batch Loss: 0.2867353856563568\n",
      "Epoch 3459, Loss: 1.6874080300331116, Final Batch Loss: 0.3406093716621399\n",
      "Epoch 3460, Loss: 1.8203133046627045, Final Batch Loss: 0.3968293368816376\n",
      "Epoch 3461, Loss: 2.155080884695053, Final Batch Loss: 0.60038822889328\n",
      "Epoch 3462, Loss: 1.828692764043808, Final Batch Loss: 0.38552922010421753\n",
      "Epoch 3463, Loss: 1.880753368139267, Final Batch Loss: 0.41901183128356934\n",
      "Epoch 3464, Loss: 1.6545257717370987, Final Batch Loss: 0.09869207441806793\n",
      "Epoch 3465, Loss: 1.772946611046791, Final Batch Loss: 0.2351960390806198\n",
      "Epoch 3466, Loss: 1.8798863589763641, Final Batch Loss: 0.5557447671890259\n",
      "Epoch 3467, Loss: 1.7945139110088348, Final Batch Loss: 0.4323491156101227\n",
      "Epoch 3468, Loss: 1.9656407833099365, Final Batch Loss: 0.41075822710990906\n",
      "Epoch 3469, Loss: 2.0971754789352417, Final Batch Loss: 0.590423583984375\n",
      "Epoch 3470, Loss: 1.7767899930477142, Final Batch Loss: 0.27668431401252747\n",
      "Epoch 3471, Loss: 1.6434625685214996, Final Batch Loss: 0.2009393274784088\n",
      "Epoch 3472, Loss: 1.6044127568602562, Final Batch Loss: 0.11272137612104416\n",
      "Epoch 3473, Loss: 1.7730231285095215, Final Batch Loss: 0.35995882749557495\n",
      "Epoch 3474, Loss: 1.7047652900218964, Final Batch Loss: 0.2780175507068634\n",
      "Epoch 3475, Loss: 1.7004580199718475, Final Batch Loss: 0.29916056990623474\n",
      "Epoch 3476, Loss: 1.6451063454151154, Final Batch Loss: 0.25160714983940125\n",
      "Epoch 3477, Loss: 1.7289546430110931, Final Batch Loss: 0.29191526770591736\n",
      "Epoch 3478, Loss: 1.5348345190286636, Final Batch Loss: 0.1670772284269333\n",
      "Epoch 3479, Loss: 1.6604717373847961, Final Batch Loss: 0.216038316488266\n",
      "Epoch 3480, Loss: 1.7404985427856445, Final Batch Loss: 0.2882571816444397\n",
      "Epoch 3481, Loss: 1.5947204381227493, Final Batch Loss: 0.1858769804239273\n",
      "Epoch 3482, Loss: 1.9819673001766205, Final Batch Loss: 0.5974422693252563\n",
      "Epoch 3483, Loss: 1.5080752968788147, Final Batch Loss: 0.14650413393974304\n",
      "Epoch 3484, Loss: 1.7496258020401, Final Batch Loss: 0.3891820013523102\n",
      "Epoch 3485, Loss: 1.4630561769008636, Final Batch Loss: 0.17311155796051025\n",
      "Epoch 3486, Loss: 1.6124979853630066, Final Batch Loss: 0.28272315859794617\n",
      "Epoch 3487, Loss: 1.7813660800457, Final Batch Loss: 0.2970272898674011\n",
      "Epoch 3488, Loss: 1.3910179995000362, Final Batch Loss: 0.031007196754217148\n",
      "Epoch 3489, Loss: 1.5386523604393005, Final Batch Loss: 0.2083183228969574\n",
      "Epoch 3490, Loss: 1.7257812917232513, Final Batch Loss: 0.30967122316360474\n",
      "Epoch 3491, Loss: 2.0111870169639587, Final Batch Loss: 0.4621157646179199\n",
      "Epoch 3492, Loss: 1.7190918028354645, Final Batch Loss: 0.3239644169807434\n",
      "Epoch 3493, Loss: 1.6653889119625092, Final Batch Loss: 0.2641026973724365\n",
      "Epoch 3494, Loss: 1.4481196701526642, Final Batch Loss: 0.18725848197937012\n",
      "Epoch 3495, Loss: 1.7501779794692993, Final Batch Loss: 0.39178791642189026\n",
      "Epoch 3496, Loss: 1.663042113184929, Final Batch Loss: 0.23173467814922333\n",
      "Epoch 3497, Loss: 1.63874951004982, Final Batch Loss: 0.2845899164676666\n",
      "Epoch 3498, Loss: 1.7052883356809616, Final Batch Loss: 0.18846912682056427\n",
      "Epoch 3499, Loss: 1.724245771765709, Final Batch Loss: 0.21950994431972504\n",
      "Epoch 3500, Loss: 2.141570419073105, Final Batch Loss: 0.6065638661384583\n",
      "Epoch 3501, Loss: 2.2560964822769165, Final Batch Loss: 0.7365815043449402\n",
      "Epoch 3502, Loss: 1.748677596449852, Final Batch Loss: 0.17658372223377228\n",
      "Epoch 3503, Loss: 1.7256732881069183, Final Batch Loss: 0.34864839911460876\n",
      "Epoch 3504, Loss: 1.5999652296304703, Final Batch Loss: 0.19788490235805511\n",
      "Epoch 3505, Loss: 1.9711117148399353, Final Batch Loss: 0.5249336361885071\n",
      "Epoch 3506, Loss: 1.9395986497402191, Final Batch Loss: 0.5183075666427612\n",
      "Epoch 3507, Loss: 1.9437317550182343, Final Batch Loss: 0.527747631072998\n",
      "Epoch 3508, Loss: 2.0444298684597015, Final Batch Loss: 0.4677982032299042\n",
      "Epoch 3509, Loss: 2.201366662979126, Final Batch Loss: 0.4618028998374939\n",
      "Epoch 3510, Loss: 2.055001348257065, Final Batch Loss: 0.5937569737434387\n",
      "Epoch 3511, Loss: 1.9585082232952118, Final Batch Loss: 0.49265217781066895\n",
      "Epoch 3512, Loss: 2.1669970750808716, Final Batch Loss: 0.6670469045639038\n",
      "Epoch 3513, Loss: 2.5013727843761444, Final Batch Loss: 0.9315396547317505\n",
      "Epoch 3514, Loss: 1.7667385041713715, Final Batch Loss: 0.2708669900894165\n",
      "Epoch 3515, Loss: 1.839787870645523, Final Batch Loss: 0.36400601267814636\n",
      "Epoch 3516, Loss: 1.6514088362455368, Final Batch Loss: 0.09464530646800995\n",
      "Epoch 3517, Loss: 1.7454385310411453, Final Batch Loss: 0.20986850559711456\n",
      "Epoch 3518, Loss: 1.8515766859054565, Final Batch Loss: 0.3891770541667938\n",
      "Epoch 3519, Loss: 1.8463760018348694, Final Batch Loss: 0.33026862144470215\n",
      "Epoch 3520, Loss: 1.7175425440073013, Final Batch Loss: 0.13504450023174286\n",
      "Epoch 3521, Loss: 1.7492369413375854, Final Batch Loss: 0.22402337193489075\n",
      "Epoch 3522, Loss: 1.6182491928339005, Final Batch Loss: 0.21866066753864288\n",
      "Epoch 3523, Loss: 1.6060263961553574, Final Batch Loss: 0.20399166643619537\n",
      "Epoch 3524, Loss: 1.707331269979477, Final Batch Loss: 0.3012673854827881\n",
      "Epoch 3525, Loss: 1.798047512769699, Final Batch Loss: 0.35040321946144104\n",
      "Epoch 3526, Loss: 1.6465566754341125, Final Batch Loss: 0.21878433227539062\n",
      "Epoch 3527, Loss: 1.6647013127803802, Final Batch Loss: 0.17425188422203064\n",
      "Epoch 3528, Loss: 1.7823632657527924, Final Batch Loss: 0.2571536600589752\n",
      "Epoch 3529, Loss: 1.5572622418403625, Final Batch Loss: 0.2812877893447876\n",
      "Epoch 3530, Loss: 1.7119183242321014, Final Batch Loss: 0.37859395146369934\n",
      "Epoch 3531, Loss: 1.7167295068502426, Final Batch Loss: 0.2407924383878708\n",
      "Epoch 3532, Loss: 1.793175756931305, Final Batch Loss: 0.41560348868370056\n",
      "Epoch 3533, Loss: 1.813568353652954, Final Batch Loss: 0.35371002554893494\n",
      "Epoch 3534, Loss: 1.584308996796608, Final Batch Loss: 0.24737142026424408\n",
      "Epoch 3535, Loss: 1.6101241260766983, Final Batch Loss: 0.22395683825016022\n",
      "Epoch 3536, Loss: 1.7623446583747864, Final Batch Loss: 0.31075188517570496\n",
      "Epoch 3537, Loss: 1.9232758581638336, Final Batch Loss: 0.562066912651062\n",
      "Epoch 3538, Loss: 1.7203299403190613, Final Batch Loss: 0.25292596220970154\n",
      "Epoch 3539, Loss: 1.9378080666065216, Final Batch Loss: 0.4717826247215271\n",
      "Epoch 3540, Loss: 1.8441171646118164, Final Batch Loss: 0.32866984605789185\n",
      "Epoch 3541, Loss: 1.8915229439735413, Final Batch Loss: 0.46470484137535095\n",
      "Epoch 3542, Loss: 1.8503488302230835, Final Batch Loss: 0.46408316493034363\n",
      "Epoch 3543, Loss: 1.5403001606464386, Final Batch Loss: 0.09767073392868042\n",
      "Epoch 3544, Loss: 1.6350443065166473, Final Batch Loss: 0.29106345772743225\n",
      "Epoch 3545, Loss: 1.95643949508667, Final Batch Loss: 0.5776602029800415\n",
      "Epoch 3546, Loss: 2.029286712408066, Final Batch Loss: 0.4973777234554291\n",
      "Epoch 3547, Loss: 2.0957092344760895, Final Batch Loss: 0.6007811427116394\n",
      "Epoch 3548, Loss: 1.7477128058671951, Final Batch Loss: 0.1993223875761032\n",
      "Epoch 3549, Loss: 1.6969523429870605, Final Batch Loss: 0.21581324934959412\n",
      "Epoch 3550, Loss: 1.9256477057933807, Final Batch Loss: 0.47908371686935425\n",
      "Epoch 3551, Loss: 1.8169806003570557, Final Batch Loss: 0.424709290266037\n",
      "Epoch 3552, Loss: 2.137994199991226, Final Batch Loss: 0.7395936846733093\n",
      "Epoch 3553, Loss: 1.7807562053203583, Final Batch Loss: 0.2862215042114258\n",
      "Epoch 3554, Loss: 2.249764084815979, Final Batch Loss: 0.8472851514816284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3555, Loss: 2.1095812022686005, Final Batch Loss: 0.6622928380966187\n",
      "Epoch 3556, Loss: 1.9840935468673706, Final Batch Loss: 0.28864219784736633\n",
      "Epoch 3557, Loss: 1.9278355538845062, Final Batch Loss: 0.5085343718528748\n",
      "Epoch 3558, Loss: 1.7684449553489685, Final Batch Loss: 0.4143705666065216\n",
      "Epoch 3559, Loss: 1.9893772900104523, Final Batch Loss: 0.2836996912956238\n",
      "Epoch 3560, Loss: 1.7278822362422943, Final Batch Loss: 0.2740151882171631\n",
      "Epoch 3561, Loss: 1.6488079503178596, Final Batch Loss: 0.08772275596857071\n",
      "Epoch 3562, Loss: 1.9617004692554474, Final Batch Loss: 0.580730140209198\n",
      "Epoch 3563, Loss: 1.5797698646783829, Final Batch Loss: 0.19598065316677094\n",
      "Epoch 3564, Loss: 1.9081933200359344, Final Batch Loss: 0.3470821678638458\n",
      "Epoch 3565, Loss: 1.7313343286514282, Final Batch Loss: 0.2661200165748596\n",
      "Epoch 3566, Loss: 2.010833293199539, Final Batch Loss: 0.6613039970397949\n",
      "Epoch 3567, Loss: 2.193116784095764, Final Batch Loss: 0.6824677586555481\n",
      "Epoch 3568, Loss: 1.635678619146347, Final Batch Loss: 0.199555903673172\n",
      "Epoch 3569, Loss: 1.8656990230083466, Final Batch Loss: 0.5371688008308411\n",
      "Epoch 3570, Loss: 2.3311325311660767, Final Batch Loss: 0.8462085723876953\n",
      "Epoch 3571, Loss: 1.769737333059311, Final Batch Loss: 0.3125278055667877\n",
      "Epoch 3572, Loss: 1.9267190992832184, Final Batch Loss: 0.5909777879714966\n",
      "Epoch 3573, Loss: 1.7658164203166962, Final Batch Loss: 0.30114981532096863\n",
      "Epoch 3574, Loss: 2.120903432369232, Final Batch Loss: 0.6779817342758179\n",
      "Epoch 3575, Loss: 2.0077690482139587, Final Batch Loss: 0.5291520953178406\n",
      "Epoch 3576, Loss: 1.6729536354541779, Final Batch Loss: 0.27539756894111633\n",
      "Epoch 3577, Loss: 1.6480273008346558, Final Batch Loss: 0.2795666754245758\n",
      "Epoch 3578, Loss: 1.7989397644996643, Final Batch Loss: 0.26016828417778015\n",
      "Epoch 3579, Loss: 1.5476736798882484, Final Batch Loss: 0.08912090212106705\n",
      "Epoch 3580, Loss: 1.6667990982532501, Final Batch Loss: 0.2643492519855499\n",
      "Epoch 3581, Loss: 1.8088572025299072, Final Batch Loss: 0.39436888694763184\n",
      "Epoch 3582, Loss: 1.7493809759616852, Final Batch Loss: 0.27994033694267273\n",
      "Epoch 3583, Loss: 1.6909369230270386, Final Batch Loss: 0.3841497004032135\n",
      "Epoch 3584, Loss: 1.6864590048789978, Final Batch Loss: 0.32058340311050415\n",
      "Epoch 3585, Loss: 1.95438352227211, Final Batch Loss: 0.5467066764831543\n",
      "Epoch 3586, Loss: 1.5271899551153183, Final Batch Loss: 0.1532139927148819\n",
      "Epoch 3587, Loss: 1.7738503515720367, Final Batch Loss: 0.31948336958885193\n",
      "Epoch 3588, Loss: 1.7021520733833313, Final Batch Loss: 0.22382852435112\n",
      "Epoch 3589, Loss: 1.7271242439746857, Final Batch Loss: 0.22951140999794006\n",
      "Epoch 3590, Loss: 1.9009528458118439, Final Batch Loss: 0.5227161049842834\n",
      "Epoch 3591, Loss: 1.5477284044027328, Final Batch Loss: 0.215598925948143\n",
      "Epoch 3592, Loss: 1.5650437027215958, Final Batch Loss: 0.2432250827550888\n",
      "Epoch 3593, Loss: 1.6374527513980865, Final Batch Loss: 0.3043782114982605\n",
      "Epoch 3594, Loss: 1.6116155982017517, Final Batch Loss: 0.17619529366493225\n",
      "Epoch 3595, Loss: 1.8254238367080688, Final Batch Loss: 0.3478556275367737\n",
      "Epoch 3596, Loss: 1.7660823166370392, Final Batch Loss: 0.3239279091358185\n",
      "Epoch 3597, Loss: 1.7391074001789093, Final Batch Loss: 0.2534187436103821\n",
      "Epoch 3598, Loss: 1.8876652717590332, Final Batch Loss: 0.590599536895752\n",
      "Epoch 3599, Loss: 1.725208580493927, Final Batch Loss: 0.43345850706100464\n",
      "Epoch 3600, Loss: 1.7979196906089783, Final Batch Loss: 0.34389886260032654\n",
      "Epoch 3601, Loss: 1.8239535689353943, Final Batch Loss: 0.4390193819999695\n",
      "Epoch 3602, Loss: 1.5923519283533096, Final Batch Loss: 0.16593550145626068\n",
      "Epoch 3603, Loss: 1.6148189008235931, Final Batch Loss: 0.2419135570526123\n",
      "Epoch 3604, Loss: 1.8877475559711456, Final Batch Loss: 0.5194754004478455\n",
      "Epoch 3605, Loss: 1.664209246635437, Final Batch Loss: 0.28497686982154846\n",
      "Epoch 3606, Loss: 1.4783479645848274, Final Batch Loss: 0.09292653948068619\n",
      "Epoch 3607, Loss: 1.6307730227708817, Final Batch Loss: 0.22993992269039154\n",
      "Epoch 3608, Loss: 2.0089536011219025, Final Batch Loss: 0.552680492401123\n",
      "Epoch 3609, Loss: 1.8199102133512497, Final Batch Loss: 0.4217546582221985\n",
      "Epoch 3610, Loss: 1.7334451377391815, Final Batch Loss: 0.410356342792511\n",
      "Epoch 3611, Loss: 1.9484614431858063, Final Batch Loss: 0.5555872321128845\n",
      "Epoch 3612, Loss: 1.6799869686365128, Final Batch Loss: 0.18953074514865875\n",
      "Epoch 3613, Loss: 1.7506338953971863, Final Batch Loss: 0.2603679895401001\n",
      "Epoch 3614, Loss: 1.5526831895112991, Final Batch Loss: 0.21036826074123383\n",
      "Epoch 3615, Loss: 1.6290142983198166, Final Batch Loss: 0.06701211631298065\n",
      "Epoch 3616, Loss: 1.7173263132572174, Final Batch Loss: 0.2746794521808624\n",
      "Epoch 3617, Loss: 1.550008863210678, Final Batch Loss: 0.22208574414253235\n",
      "Epoch 3618, Loss: 1.9276142418384552, Final Batch Loss: 0.5727145075798035\n",
      "Epoch 3619, Loss: 1.5876847952604294, Final Batch Loss: 0.1946440488100052\n",
      "Epoch 3620, Loss: 1.8380485475063324, Final Batch Loss: 0.407123327255249\n",
      "Epoch 3621, Loss: 1.9309742152690887, Final Batch Loss: 0.49480292201042175\n",
      "Epoch 3622, Loss: 1.6296243965625763, Final Batch Loss: 0.25763052701950073\n",
      "Epoch 3623, Loss: 1.6943712830543518, Final Batch Loss: 0.3420410752296448\n",
      "Epoch 3624, Loss: 1.8133414685726166, Final Batch Loss: 0.4255419075489044\n",
      "Epoch 3625, Loss: 2.040097415447235, Final Batch Loss: 0.5535966157913208\n",
      "Epoch 3626, Loss: 2.235421121120453, Final Batch Loss: 0.8982620239257812\n",
      "Epoch 3627, Loss: 1.513971209526062, Final Batch Loss: 0.16294369101524353\n",
      "Epoch 3628, Loss: 2.061738669872284, Final Batch Loss: 0.6015742421150208\n",
      "Epoch 3629, Loss: 1.5795079991221428, Final Batch Loss: 0.07061626762151718\n",
      "Epoch 3630, Loss: 1.7705189883708954, Final Batch Loss: 0.34446921944618225\n",
      "Epoch 3631, Loss: 1.9960073232650757, Final Batch Loss: 0.3666885793209076\n",
      "Epoch 3632, Loss: 2.0693027675151825, Final Batch Loss: 0.5820090174674988\n",
      "Epoch 3633, Loss: 2.026285469532013, Final Batch Loss: 0.5136844515800476\n",
      "Epoch 3634, Loss: 1.6877376288175583, Final Batch Loss: 0.20005615055561066\n",
      "Epoch 3635, Loss: 2.0011355876922607, Final Batch Loss: 0.6557963490486145\n",
      "Epoch 3636, Loss: 1.610048532485962, Final Batch Loss: 0.28223875164985657\n",
      "Epoch 3637, Loss: 2.0486315488815308, Final Batch Loss: 0.5126317143440247\n",
      "Epoch 3638, Loss: 1.9811678528785706, Final Batch Loss: 0.4030894637107849\n",
      "Epoch 3639, Loss: 1.8298687636852264, Final Batch Loss: 0.17936500906944275\n",
      "Epoch 3640, Loss: 1.6818573325872421, Final Batch Loss: 0.1938340812921524\n",
      "Epoch 3641, Loss: 1.8898794651031494, Final Batch Loss: 0.44750073552131653\n",
      "Epoch 3642, Loss: 1.4499323889613152, Final Batch Loss: 0.060556285083293915\n",
      "Epoch 3643, Loss: 1.5675703808665276, Final Batch Loss: 0.11412962526082993\n",
      "Epoch 3644, Loss: 1.5604480504989624, Final Batch Loss: 0.13444796204566956\n",
      "Epoch 3645, Loss: 1.8448818624019623, Final Batch Loss: 0.36189982295036316\n",
      "Epoch 3646, Loss: 1.970904916524887, Final Batch Loss: 0.5273813009262085\n",
      "Epoch 3647, Loss: 2.024760901927948, Final Batch Loss: 0.5762669444084167\n",
      "Epoch 3648, Loss: 1.5157013721764088, Final Batch Loss: 0.03266139701008797\n",
      "Epoch 3649, Loss: 2.387860983610153, Final Batch Loss: 0.9169415831565857\n",
      "Epoch 3650, Loss: 1.5064658373594284, Final Batch Loss: 0.1858738511800766\n",
      "Epoch 3651, Loss: 2.023958683013916, Final Batch Loss: 0.5063314437866211\n",
      "Epoch 3652, Loss: 1.6921956986188889, Final Batch Loss: 0.2476591020822525\n",
      "Epoch 3653, Loss: 1.8239611089229584, Final Batch Loss: 0.40518373250961304\n",
      "Epoch 3654, Loss: 2.3097466826438904, Final Batch Loss: 0.8752108812332153\n",
      "Epoch 3655, Loss: 1.9877550303936005, Final Batch Loss: 0.5040537714958191\n",
      "Epoch 3656, Loss: 2.021053731441498, Final Batch Loss: 0.37090688943862915\n",
      "Epoch 3657, Loss: 1.8194915056228638, Final Batch Loss: 0.27090486884117126\n",
      "Epoch 3658, Loss: 2.52759450674057, Final Batch Loss: 1.0440988540649414\n",
      "Epoch 3659, Loss: 2.062176376581192, Final Batch Loss: 0.6120008826255798\n",
      "Epoch 3660, Loss: 2.291706919670105, Final Batch Loss: 0.5204639434814453\n",
      "Epoch 3661, Loss: 2.1355727314949036, Final Batch Loss: 0.3266589939594269\n",
      "Epoch 3662, Loss: 2.1965006291866302, Final Batch Loss: 0.5534963011741638\n",
      "Epoch 3663, Loss: 2.228818655014038, Final Batch Loss: 0.7074061632156372\n",
      "Epoch 3664, Loss: 1.8734692484140396, Final Batch Loss: 0.17990051209926605\n",
      "Epoch 3665, Loss: 1.5634747818112373, Final Batch Loss: 0.11235261708498001\n",
      "Epoch 3666, Loss: 1.4560437873005867, Final Batch Loss: 0.0383625403046608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3667, Loss: 1.9639087617397308, Final Batch Loss: 0.488993376493454\n",
      "Epoch 3668, Loss: 1.5416473150253296, Final Batch Loss: 0.0716894268989563\n",
      "Epoch 3669, Loss: 1.999182552099228, Final Batch Loss: 0.6151136159896851\n",
      "Epoch 3670, Loss: 1.802603542804718, Final Batch Loss: 0.41950860619544983\n",
      "Epoch 3671, Loss: 1.6927694380283356, Final Batch Loss: 0.25235098600387573\n",
      "Epoch 3672, Loss: 1.8324167132377625, Final Batch Loss: 0.35347840189933777\n",
      "Epoch 3673, Loss: 1.4782419130206108, Final Batch Loss: 0.11138629168272018\n",
      "Epoch 3674, Loss: 2.037977635860443, Final Batch Loss: 0.6804167032241821\n",
      "Epoch 3675, Loss: 1.6178898215293884, Final Batch Loss: 0.2027176320552826\n",
      "Epoch 3676, Loss: 1.5445109829306602, Final Batch Loss: 0.11502542346715927\n",
      "Epoch 3677, Loss: 1.860879361629486, Final Batch Loss: 0.3613400459289551\n",
      "Epoch 3678, Loss: 1.739575982093811, Final Batch Loss: 0.3485133945941925\n",
      "Epoch 3679, Loss: 1.6731244325637817, Final Batch Loss: 0.2927227020263672\n",
      "Epoch 3680, Loss: 1.9396462738513947, Final Batch Loss: 0.39457425475120544\n",
      "Epoch 3681, Loss: 2.116658180952072, Final Batch Loss: 0.5110030770301819\n",
      "Epoch 3682, Loss: 2.075394243001938, Final Batch Loss: 0.5395047068595886\n",
      "Epoch 3683, Loss: 1.7868880480527878, Final Batch Loss: 0.23203785717487335\n",
      "Epoch 3684, Loss: 1.8955152332782745, Final Batch Loss: 0.5405409932136536\n",
      "Epoch 3685, Loss: 2.028893232345581, Final Batch Loss: 0.5028151273727417\n",
      "Epoch 3686, Loss: 1.79546719789505, Final Batch Loss: 0.3335478603839874\n",
      "Epoch 3687, Loss: 1.5211110189557076, Final Batch Loss: 0.1089923158288002\n",
      "Epoch 3688, Loss: 1.837838590145111, Final Batch Loss: 0.44897422194480896\n",
      "Epoch 3689, Loss: 1.517394244670868, Final Batch Loss: 0.21710661053657532\n",
      "Epoch 3690, Loss: 1.470523338764906, Final Batch Loss: 0.04467860981822014\n",
      "Epoch 3691, Loss: 1.7978745996952057, Final Batch Loss: 0.38704532384872437\n",
      "Epoch 3692, Loss: 1.8392216861248016, Final Batch Loss: 0.49422192573547363\n",
      "Epoch 3693, Loss: 2.051529288291931, Final Batch Loss: 0.507005512714386\n",
      "Epoch 3694, Loss: 1.5969047248363495, Final Batch Loss: 0.29412510991096497\n",
      "Epoch 3695, Loss: 1.6323002874851227, Final Batch Loss: 0.2170579731464386\n",
      "Epoch 3696, Loss: 1.8938718140125275, Final Batch Loss: 0.572108805179596\n",
      "Epoch 3697, Loss: 1.64492167532444, Final Batch Loss: 0.22381491959095\n",
      "Epoch 3698, Loss: 1.9888689517974854, Final Batch Loss: 0.48647695779800415\n",
      "Epoch 3699, Loss: 1.7991061806678772, Final Batch Loss: 0.316330224275589\n",
      "Epoch 3700, Loss: 1.9916698336601257, Final Batch Loss: 0.5417697429656982\n",
      "Epoch 3701, Loss: 1.7572686672210693, Final Batch Loss: 0.2548430860042572\n",
      "Epoch 3702, Loss: 1.929124802350998, Final Batch Loss: 0.4302431643009186\n",
      "Epoch 3703, Loss: 1.4652423188090324, Final Batch Loss: 0.0782451257109642\n",
      "Epoch 3704, Loss: 1.9787233173847198, Final Batch Loss: 0.3979075253009796\n",
      "Epoch 3705, Loss: 1.6628777086734772, Final Batch Loss: 0.20168069005012512\n",
      "Epoch 3706, Loss: 1.7131298780441284, Final Batch Loss: 0.26212581992149353\n",
      "Epoch 3707, Loss: 1.6418419405817986, Final Batch Loss: 0.11488424986600876\n",
      "Epoch 3708, Loss: 1.909978300333023, Final Batch Loss: 0.5368436574935913\n",
      "Epoch 3709, Loss: 1.6531967669725418, Final Batch Loss: 0.19349108636379242\n",
      "Epoch 3710, Loss: 1.8926838338375092, Final Batch Loss: 0.4077757000923157\n",
      "Epoch 3711, Loss: 1.68306165933609, Final Batch Loss: 0.2816677987575531\n",
      "Epoch 3712, Loss: 1.4134579002857208, Final Batch Loss: 0.040594130754470825\n",
      "Epoch 3713, Loss: 1.5520817786455154, Final Batch Loss: 0.20257051289081573\n",
      "Epoch 3714, Loss: 1.8748602867126465, Final Batch Loss: 0.5423130393028259\n",
      "Epoch 3715, Loss: 1.4681351780891418, Final Batch Loss: 0.06742534041404724\n",
      "Epoch 3716, Loss: 1.9246159493923187, Final Batch Loss: 0.5822625160217285\n",
      "Epoch 3717, Loss: 2.3347522914409637, Final Batch Loss: 0.8862411379814148\n",
      "Epoch 3718, Loss: 2.311365932226181, Final Batch Loss: 0.8271244168281555\n",
      "Epoch 3719, Loss: 2.0362925827503204, Final Batch Loss: 0.5362098813056946\n",
      "Epoch 3720, Loss: 1.8215674459934235, Final Batch Loss: 0.41234660148620605\n",
      "Epoch 3721, Loss: 1.9512161016464233, Final Batch Loss: 0.5161003470420837\n",
      "Epoch 3722, Loss: 1.792627066373825, Final Batch Loss: 0.2756368815898895\n",
      "Epoch 3723, Loss: 2.277858704328537, Final Batch Loss: 0.6642287373542786\n",
      "Epoch 3724, Loss: 1.8764563500881195, Final Batch Loss: 0.329761266708374\n",
      "Epoch 3725, Loss: 1.8071303069591522, Final Batch Loss: 0.3148063123226166\n",
      "Epoch 3726, Loss: 1.8582828044891357, Final Batch Loss: 0.35674962401390076\n",
      "Epoch 3727, Loss: 1.9581087529659271, Final Batch Loss: 0.4115728735923767\n",
      "Epoch 3728, Loss: 1.6652166843414307, Final Batch Loss: 0.3011595904827118\n",
      "Epoch 3729, Loss: 1.9873123168945312, Final Batch Loss: 0.5169073343276978\n",
      "Epoch 3730, Loss: 1.932155817747116, Final Batch Loss: 0.4923548996448517\n",
      "Epoch 3731, Loss: 2.0005608201026917, Final Batch Loss: 0.3381212651729584\n",
      "Epoch 3732, Loss: 1.8297676146030426, Final Batch Loss: 0.39983394742012024\n",
      "Epoch 3733, Loss: 1.5912794768810272, Final Batch Loss: 0.1863161027431488\n",
      "Epoch 3734, Loss: 1.7614479959011078, Final Batch Loss: 0.4012153744697571\n",
      "Epoch 3735, Loss: 2.087693691253662, Final Batch Loss: 0.5712720155715942\n",
      "Epoch 3736, Loss: 1.5663215443491936, Final Batch Loss: 0.09390469640493393\n",
      "Epoch 3737, Loss: 1.4887265413999557, Final Batch Loss: 0.19690744578838348\n",
      "Epoch 3738, Loss: 1.8884888291358948, Final Batch Loss: 0.6157987713813782\n",
      "Epoch 3739, Loss: 1.697962999343872, Final Batch Loss: 0.27480998635292053\n",
      "Epoch 3740, Loss: 1.6176478862762451, Final Batch Loss: 0.22081756591796875\n",
      "Epoch 3741, Loss: 1.77131986618042, Final Batch Loss: 0.32997938990592957\n",
      "Epoch 3742, Loss: 1.5342711135745049, Final Batch Loss: 0.08170001953840256\n",
      "Epoch 3743, Loss: 1.7884861826896667, Final Batch Loss: 0.30921030044555664\n",
      "Epoch 3744, Loss: 1.9859901070594788, Final Batch Loss: 0.5302327275276184\n",
      "Epoch 3745, Loss: 2.1947520673274994, Final Batch Loss: 0.7797452807426453\n",
      "Epoch 3746, Loss: 2.307714879512787, Final Batch Loss: 0.8233981132507324\n",
      "Epoch 3747, Loss: 1.7361387610435486, Final Batch Loss: 0.27047020196914673\n",
      "Epoch 3748, Loss: 1.871166855096817, Final Batch Loss: 0.36507347226142883\n",
      "Epoch 3749, Loss: 1.8833444714546204, Final Batch Loss: 0.5093252062797546\n",
      "Epoch 3750, Loss: 1.90065136551857, Final Batch Loss: 0.5657854676246643\n",
      "Epoch 3751, Loss: 1.7888234853744507, Final Batch Loss: 0.4039502739906311\n",
      "Epoch 3752, Loss: 1.6015895307064056, Final Batch Loss: 0.2779364585876465\n",
      "Epoch 3753, Loss: 1.766737312078476, Final Batch Loss: 0.47952383756637573\n",
      "Epoch 3754, Loss: 1.768441379070282, Final Batch Loss: 0.3766319453716278\n",
      "Epoch 3755, Loss: 1.6707502901554108, Final Batch Loss: 0.3880597949028015\n",
      "Epoch 3756, Loss: 1.6558550596237183, Final Batch Loss: 0.2682655155658722\n",
      "Epoch 3757, Loss: 2.0050588250160217, Final Batch Loss: 0.6468410491943359\n",
      "Epoch 3758, Loss: 1.7103673964738846, Final Batch Loss: 0.17350108921527863\n",
      "Epoch 3759, Loss: 1.9583176374435425, Final Batch Loss: 0.6014791131019592\n",
      "Epoch 3760, Loss: 2.2843902111053467, Final Batch Loss: 0.6915939450263977\n",
      "Epoch 3761, Loss: 2.0915925204753876, Final Batch Loss: 0.6708914041519165\n",
      "Epoch 3762, Loss: 1.8661883473396301, Final Batch Loss: 0.49982157349586487\n",
      "Epoch 3763, Loss: 1.764568492770195, Final Batch Loss: 0.24582059681415558\n",
      "Epoch 3764, Loss: 1.677110455930233, Final Batch Loss: 0.06712447851896286\n",
      "Epoch 3765, Loss: 1.7164573073387146, Final Batch Loss: 0.28218874335289\n",
      "Epoch 3766, Loss: 1.8106728792190552, Final Batch Loss: 0.38834819197654724\n",
      "Epoch 3767, Loss: 2.246635913848877, Final Batch Loss: 0.8332772850990295\n",
      "Epoch 3768, Loss: 2.1806055307388306, Final Batch Loss: 0.6045385599136353\n",
      "Epoch 3769, Loss: 1.7302383780479431, Final Batch Loss: 0.22603079676628113\n",
      "Epoch 3770, Loss: 1.9154902398586273, Final Batch Loss: 0.32609573006629944\n",
      "Epoch 3771, Loss: 1.8150852918624878, Final Batch Loss: 0.28721752762794495\n",
      "Epoch 3772, Loss: 2.1962684392929077, Final Batch Loss: 0.7855685353279114\n",
      "Epoch 3773, Loss: 1.4916852787137032, Final Batch Loss: 0.08136022835969925\n",
      "Epoch 3774, Loss: 1.83390673995018, Final Batch Loss: 0.38826343417167664\n",
      "Epoch 3775, Loss: 1.9354976415634155, Final Batch Loss: 0.5438801646232605\n",
      "Epoch 3776, Loss: 1.7208499014377594, Final Batch Loss: 0.41637519001960754\n",
      "Epoch 3777, Loss: 1.445234626531601, Final Batch Loss: 0.09321326017379761\n",
      "Epoch 3778, Loss: 1.8052392601966858, Final Batch Loss: 0.35161176323890686\n",
      "Epoch 3779, Loss: 1.7668612897396088, Final Batch Loss: 0.4207766056060791\n",
      "Epoch 3780, Loss: 1.5603610277175903, Final Batch Loss: 0.13783776760101318\n",
      "Epoch 3781, Loss: 1.5154450088739395, Final Batch Loss: 0.18218593299388885\n",
      "Epoch 3782, Loss: 1.4988713823258877, Final Batch Loss: 0.05629953369498253\n",
      "Epoch 3783, Loss: 1.7351710796356201, Final Batch Loss: 0.27546146512031555\n",
      "Epoch 3784, Loss: 1.730072945356369, Final Batch Loss: 0.3642711341381073\n",
      "Epoch 3785, Loss: 1.75440514087677, Final Batch Loss: 0.3892030715942383\n",
      "Epoch 3786, Loss: 1.8351391851902008, Final Batch Loss: 0.3986051678657532\n",
      "Epoch 3787, Loss: 1.7578914761543274, Final Batch Loss: 0.44667983055114746\n",
      "Epoch 3788, Loss: 1.6607994586229324, Final Batch Loss: 0.2326003760099411\n",
      "Epoch 3789, Loss: 1.6846538931131363, Final Batch Loss: 0.13882260024547577\n",
      "Epoch 3790, Loss: 1.826051950454712, Final Batch Loss: 0.28167951107025146\n",
      "Epoch 3791, Loss: 1.8358222544193268, Final Batch Loss: 0.5244441628456116\n",
      "Epoch 3792, Loss: 1.983708679676056, Final Batch Loss: 0.4943011403083801\n",
      "Epoch 3793, Loss: 2.016351342201233, Final Batch Loss: 0.5389312505722046\n",
      "Epoch 3794, Loss: 1.9765141010284424, Final Batch Loss: 0.5275399088859558\n",
      "Epoch 3795, Loss: 1.7354902923107147, Final Batch Loss: 0.35921937227249146\n",
      "Epoch 3796, Loss: 1.8111340403556824, Final Batch Loss: 0.29762622714042664\n",
      "Epoch 3797, Loss: 1.8722946345806122, Final Batch Loss: 0.4105878174304962\n",
      "Epoch 3798, Loss: 1.9734004139900208, Final Batch Loss: 0.5722209811210632\n",
      "Epoch 3799, Loss: 2.1599749624729156, Final Batch Loss: 0.6802889108657837\n",
      "Epoch 3800, Loss: 2.21013543009758, Final Batch Loss: 0.6048746109008789\n",
      "Epoch 3801, Loss: 1.7364831566810608, Final Batch Loss: 0.4263122081756592\n",
      "Epoch 3802, Loss: 1.6575210839509964, Final Batch Loss: 0.1942421942949295\n",
      "Epoch 3803, Loss: 1.7238694429397583, Final Batch Loss: 0.32119616866111755\n",
      "Epoch 3804, Loss: 1.815921425819397, Final Batch Loss: 0.38567671179771423\n",
      "Epoch 3805, Loss: 2.1212587654590607, Final Batch Loss: 0.6918205618858337\n",
      "Epoch 3806, Loss: 2.011432319879532, Final Batch Loss: 0.5573450922966003\n",
      "Epoch 3807, Loss: 2.3308518528938293, Final Batch Loss: 0.8264009356498718\n",
      "Epoch 3808, Loss: 1.5438482910394669, Final Batch Loss: 0.19721408188343048\n",
      "Epoch 3809, Loss: 1.8380034267902374, Final Batch Loss: 0.33785173296928406\n",
      "Epoch 3810, Loss: 1.7426699250936508, Final Batch Loss: 0.24289314448833466\n",
      "Epoch 3811, Loss: 1.8574274331331253, Final Batch Loss: 0.2331169992685318\n",
      "Epoch 3812, Loss: 2.360196202993393, Final Batch Loss: 0.897053062915802\n",
      "Epoch 3813, Loss: 1.569893702864647, Final Batch Loss: 0.12954135239124298\n",
      "Epoch 3814, Loss: 2.12791508436203, Final Batch Loss: 0.5936322808265686\n",
      "Epoch 3815, Loss: 1.6692213267087936, Final Batch Loss: 0.13572977483272552\n",
      "Epoch 3816, Loss: 1.846811681985855, Final Batch Loss: 0.4902377724647522\n",
      "Epoch 3817, Loss: 1.8476741015911102, Final Batch Loss: 0.40791842341423035\n",
      "Epoch 3818, Loss: 1.8001869022846222, Final Batch Loss: 0.40599074959754944\n",
      "Epoch 3819, Loss: 1.7095311731100082, Final Batch Loss: 0.2084893137216568\n",
      "Epoch 3820, Loss: 1.6393878757953644, Final Batch Loss: 0.25013846158981323\n",
      "Epoch 3821, Loss: 2.2049277424812317, Final Batch Loss: 0.6175639033317566\n",
      "Epoch 3822, Loss: 2.07967072725296, Final Batch Loss: 0.6956878900527954\n",
      "Epoch 3823, Loss: 1.659639686346054, Final Batch Loss: 0.2175140082836151\n",
      "Epoch 3824, Loss: 1.763387143611908, Final Batch Loss: 0.36652225255966187\n",
      "Epoch 3825, Loss: 1.5089921057224274, Final Batch Loss: 0.10268932580947876\n",
      "Epoch 3826, Loss: 1.6593689918518066, Final Batch Loss: 0.3498702943325043\n",
      "Epoch 3827, Loss: 1.6508035361766815, Final Batch Loss: 0.24524426460266113\n",
      "Epoch 3828, Loss: 1.6018784195184708, Final Batch Loss: 0.08535237610340118\n",
      "Epoch 3829, Loss: 2.0706861913204193, Final Batch Loss: 0.6461318731307983\n",
      "Epoch 3830, Loss: 1.827226161956787, Final Batch Loss: 0.4342209994792938\n",
      "Epoch 3831, Loss: 1.781960517168045, Final Batch Loss: 0.3871515095233917\n",
      "Epoch 3832, Loss: 1.7959842383861542, Final Batch Loss: 0.34405845403671265\n",
      "Epoch 3833, Loss: 1.6841189563274384, Final Batch Loss: 0.28687578439712524\n",
      "Epoch 3834, Loss: 1.6532954275608063, Final Batch Loss: 0.2438223659992218\n",
      "Epoch 3835, Loss: 1.7564089894294739, Final Batch Loss: 0.36898306012153625\n",
      "Epoch 3836, Loss: 1.7490408718585968, Final Batch Loss: 0.3323683738708496\n",
      "Epoch 3837, Loss: 1.7399626672267914, Final Batch Loss: 0.4113195538520813\n",
      "Epoch 3838, Loss: 1.6099883615970612, Final Batch Loss: 0.14018914103507996\n",
      "Epoch 3839, Loss: 1.9033430516719818, Final Batch Loss: 0.5185049772262573\n",
      "Epoch 3840, Loss: 1.6326796114444733, Final Batch Loss: 0.21865889430046082\n",
      "Epoch 3841, Loss: 1.9479475617408752, Final Batch Loss: 0.5547724366188049\n",
      "Epoch 3842, Loss: 1.6097002029418945, Final Batch Loss: 0.25421473383903503\n",
      "Epoch 3843, Loss: 1.5881489217281342, Final Batch Loss: 0.2658802270889282\n",
      "Epoch 3844, Loss: 1.7067125588655472, Final Batch Loss: 0.2298770397901535\n",
      "Epoch 3845, Loss: 1.5610885173082352, Final Batch Loss: 0.15909503400325775\n",
      "Epoch 3846, Loss: 1.8622688055038452, Final Batch Loss: 0.6109113097190857\n",
      "Epoch 3847, Loss: 1.7497591376304626, Final Batch Loss: 0.2852839231491089\n",
      "Epoch 3848, Loss: 1.6089604496955872, Final Batch Loss: 0.1842024326324463\n",
      "Epoch 3849, Loss: 1.4944503903388977, Final Batch Loss: 0.15890344977378845\n",
      "Epoch 3850, Loss: 1.6433728486299515, Final Batch Loss: 0.22072918713092804\n",
      "Epoch 3851, Loss: 1.4676547944545746, Final Batch Loss: 0.15495026111602783\n",
      "Epoch 3852, Loss: 1.7151562571525574, Final Batch Loss: 0.5235209465026855\n",
      "Epoch 3853, Loss: 1.4422669485211372, Final Batch Loss: 0.0665348544716835\n",
      "Epoch 3854, Loss: 1.8070805072784424, Final Batch Loss: 0.4139523208141327\n",
      "Epoch 3855, Loss: 2.3318630158901215, Final Batch Loss: 0.8709314465522766\n",
      "Epoch 3856, Loss: 1.872021198272705, Final Batch Loss: 0.5251602530479431\n",
      "Epoch 3857, Loss: 1.8542088866233826, Final Batch Loss: 0.455784410238266\n",
      "Epoch 3858, Loss: 1.4634094461798668, Final Batch Loss: 0.088703952729702\n",
      "Epoch 3859, Loss: 1.6898671388626099, Final Batch Loss: 0.2389371395111084\n",
      "Epoch 3860, Loss: 1.6769002079963684, Final Batch Loss: 0.39239242672920227\n",
      "Epoch 3861, Loss: 1.6171559691429138, Final Batch Loss: 0.158407062292099\n",
      "Epoch 3862, Loss: 1.5545313656330109, Final Batch Loss: 0.1994551420211792\n",
      "Epoch 3863, Loss: 1.9198266565799713, Final Batch Loss: 0.569291353225708\n",
      "Epoch 3864, Loss: 1.641568660736084, Final Batch Loss: 0.2878401577472687\n",
      "Epoch 3865, Loss: 1.831899255514145, Final Batch Loss: 0.3069867789745331\n",
      "Epoch 3866, Loss: 1.5492315292358398, Final Batch Loss: 0.12123352289199829\n",
      "Epoch 3867, Loss: 1.3958698473870754, Final Batch Loss: 0.031417813152074814\n",
      "Epoch 3868, Loss: 1.672752857208252, Final Batch Loss: 0.28715115785598755\n",
      "Epoch 3869, Loss: 1.503047451376915, Final Batch Loss: 0.2275964766740799\n",
      "Epoch 3870, Loss: 1.5506035387516022, Final Batch Loss: 0.21664369106292725\n",
      "Epoch 3871, Loss: 1.9827735126018524, Final Batch Loss: 0.7088010907173157\n",
      "Epoch 3872, Loss: 1.7223349213600159, Final Batch Loss: 0.3630557358264923\n",
      "Epoch 3873, Loss: 2.080215871334076, Final Batch Loss: 0.5652467608451843\n",
      "Epoch 3874, Loss: 1.519970491528511, Final Batch Loss: 0.21804724633693695\n",
      "Epoch 3875, Loss: 1.7215277254581451, Final Batch Loss: 0.3364602029323578\n",
      "Epoch 3876, Loss: 1.742970883846283, Final Batch Loss: 0.4506845474243164\n",
      "Epoch 3877, Loss: 1.6168895065784454, Final Batch Loss: 0.2068120539188385\n",
      "Epoch 3878, Loss: 1.9069444239139557, Final Batch Loss: 0.4769093990325928\n",
      "Epoch 3879, Loss: 1.6162116080522537, Final Batch Loss: 0.18372757732868195\n",
      "Epoch 3880, Loss: 1.6191060245037079, Final Batch Loss: 0.35730645060539246\n",
      "Epoch 3881, Loss: 1.6867312788963318, Final Batch Loss: 0.37112492322921753\n",
      "Epoch 3882, Loss: 1.7792259454727173, Final Batch Loss: 0.3549465537071228\n",
      "Epoch 3883, Loss: 1.8126750886440277, Final Batch Loss: 0.6189435124397278\n",
      "Epoch 3884, Loss: 1.6881465315818787, Final Batch Loss: 0.37695762515068054\n",
      "Epoch 3885, Loss: 1.5915983021259308, Final Batch Loss: 0.3131346106529236\n",
      "Epoch 3886, Loss: 1.7907003611326218, Final Batch Loss: 0.3872866630554199\n",
      "Epoch 3887, Loss: 1.5003031492233276, Final Batch Loss: 0.23583808541297913\n",
      "Epoch 3888, Loss: 1.735808104276657, Final Batch Loss: 0.38211849331855774\n",
      "Epoch 3889, Loss: 1.720007598400116, Final Batch Loss: 0.4001311659812927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3890, Loss: 1.4886693507432938, Final Batch Loss: 0.1649990826845169\n",
      "Epoch 3891, Loss: 1.6831147372722626, Final Batch Loss: 0.2580674886703491\n",
      "Epoch 3892, Loss: 1.5275740921497345, Final Batch Loss: 0.23971202969551086\n",
      "Epoch 3893, Loss: 1.584205001592636, Final Batch Loss: 0.17769911885261536\n",
      "Epoch 3894, Loss: 1.5341535657644272, Final Batch Loss: 0.18612070381641388\n",
      "Epoch 3895, Loss: 1.8959052562713623, Final Batch Loss: 0.4671897292137146\n",
      "Epoch 3896, Loss: 1.7611712515354156, Final Batch Loss: 0.2797628343105316\n",
      "Epoch 3897, Loss: 1.9069669246673584, Final Batch Loss: 0.4471350610256195\n",
      "Epoch 3898, Loss: 1.936752200126648, Final Batch Loss: 0.6042882800102234\n",
      "Epoch 3899, Loss: 1.4988693594932556, Final Batch Loss: 0.16133490204811096\n",
      "Epoch 3900, Loss: 1.5178556442260742, Final Batch Loss: 0.3400948941707611\n",
      "Epoch 3901, Loss: 1.8342786133289337, Final Batch Loss: 0.368478000164032\n",
      "Epoch 3902, Loss: 1.7306205928325653, Final Batch Loss: 0.3047849237918854\n",
      "Epoch 3903, Loss: 1.5174640268087387, Final Batch Loss: 0.18488822877407074\n",
      "Epoch 3904, Loss: 1.7438407838344574, Final Batch Loss: 0.4405941963195801\n",
      "Epoch 3905, Loss: 1.709953784942627, Final Batch Loss: 0.2696472108364105\n",
      "Epoch 3906, Loss: 1.7004052996635437, Final Batch Loss: 0.3785420358181\n",
      "Epoch 3907, Loss: 1.5172258615493774, Final Batch Loss: 0.11673277616500854\n",
      "Epoch 3908, Loss: 1.6452465951442719, Final Batch Loss: 0.3414735198020935\n",
      "Epoch 3909, Loss: 1.9308470487594604, Final Batch Loss: 0.5900976061820984\n",
      "Epoch 3910, Loss: 1.5155676752328873, Final Batch Loss: 0.15532217919826508\n",
      "Epoch 3911, Loss: 1.6753212213516235, Final Batch Loss: 0.43354371190071106\n",
      "Epoch 3912, Loss: 1.547030046582222, Final Batch Loss: 0.22372372448444366\n",
      "Epoch 3913, Loss: 1.5939870476722717, Final Batch Loss: 0.2977272868156433\n",
      "Epoch 3914, Loss: 1.659042850136757, Final Batch Loss: 0.18815703690052032\n",
      "Epoch 3915, Loss: 1.650751680135727, Final Batch Loss: 0.3451789915561676\n",
      "Epoch 3916, Loss: 1.78580641746521, Final Batch Loss: 0.32901647686958313\n",
      "Epoch 3917, Loss: 1.788822591304779, Final Batch Loss: 0.32596153020858765\n",
      "Epoch 3918, Loss: 1.59755277633667, Final Batch Loss: 0.27488628029823303\n",
      "Epoch 3919, Loss: 1.5783087462186813, Final Batch Loss: 0.24061600863933563\n",
      "Epoch 3920, Loss: 2.1586849689483643, Final Batch Loss: 0.7124723196029663\n",
      "Epoch 3921, Loss: 1.7407482266426086, Final Batch Loss: 0.3267880380153656\n",
      "Epoch 3922, Loss: 1.7301702797412872, Final Batch Loss: 0.37039661407470703\n",
      "Epoch 3923, Loss: 1.645048975944519, Final Batch Loss: 0.2674643397331238\n",
      "Epoch 3924, Loss: 1.4776080697774887, Final Batch Loss: 0.11210913956165314\n",
      "Epoch 3925, Loss: 1.5884822756052017, Final Batch Loss: 0.16753487288951874\n",
      "Epoch 3926, Loss: 1.636235624551773, Final Batch Loss: 0.27194899320602417\n",
      "Epoch 3927, Loss: 1.9845007210969925, Final Batch Loss: 0.641720712184906\n",
      "Epoch 3928, Loss: 2.1094315946102142, Final Batch Loss: 0.757573664188385\n",
      "Epoch 3929, Loss: 1.9051822125911713, Final Batch Loss: 0.5767953991889954\n",
      "Epoch 3930, Loss: 1.409405443817377, Final Batch Loss: 0.061239663511514664\n",
      "Epoch 3931, Loss: 1.8283063769340515, Final Batch Loss: 0.534469485282898\n",
      "Epoch 3932, Loss: 1.911036491394043, Final Batch Loss: 0.5449442267417908\n",
      "Epoch 3933, Loss: 1.5435432270169258, Final Batch Loss: 0.11716765910387039\n",
      "Epoch 3934, Loss: 1.7153972685337067, Final Batch Loss: 0.346341609954834\n",
      "Epoch 3935, Loss: 1.6947873532772064, Final Batch Loss: 0.36336538195610046\n",
      "Epoch 3936, Loss: 1.6840405762195587, Final Batch Loss: 0.37692806124687195\n",
      "Epoch 3937, Loss: 1.6025719344615936, Final Batch Loss: 0.3039998412132263\n",
      "Epoch 3938, Loss: 1.9421340823173523, Final Batch Loss: 0.5004116296768188\n",
      "Epoch 3939, Loss: 1.5817598849534988, Final Batch Loss: 0.16913284361362457\n",
      "Epoch 3940, Loss: 1.5964966714382172, Final Batch Loss: 0.25086793303489685\n",
      "Epoch 3941, Loss: 1.4518146365880966, Final Batch Loss: 0.1742294579744339\n",
      "Epoch 3942, Loss: 1.864991843700409, Final Batch Loss: 0.559182345867157\n",
      "Epoch 3943, Loss: 1.864685744047165, Final Batch Loss: 0.447710245847702\n",
      "Epoch 3944, Loss: 2.124056249856949, Final Batch Loss: 0.6982725262641907\n",
      "Epoch 3945, Loss: 1.7002682983875275, Final Batch Loss: 0.305375874042511\n",
      "Epoch 3946, Loss: 1.5393328964710236, Final Batch Loss: 0.1620873212814331\n",
      "Epoch 3947, Loss: 1.6675477623939514, Final Batch Loss: 0.26779061555862427\n",
      "Epoch 3948, Loss: 2.0883638858795166, Final Batch Loss: 0.5366528630256653\n",
      "Epoch 3949, Loss: 1.8721478283405304, Final Batch Loss: 0.4445820748806\n",
      "Epoch 3950, Loss: 1.735527127981186, Final Batch Loss: 0.43359676003456116\n",
      "Epoch 3951, Loss: 1.5443923771381378, Final Batch Loss: 0.08061796426773071\n",
      "Epoch 3952, Loss: 1.9094861447811127, Final Batch Loss: 0.5538591742515564\n",
      "Epoch 3953, Loss: 1.7492375671863556, Final Batch Loss: 0.4542776942253113\n",
      "Epoch 3954, Loss: 1.6781525313854218, Final Batch Loss: 0.36854201555252075\n",
      "Epoch 3955, Loss: 1.5488785356283188, Final Batch Loss: 0.17722482979297638\n",
      "Epoch 3956, Loss: 1.528216391801834, Final Batch Loss: 0.19165238738059998\n",
      "Epoch 3957, Loss: 1.6390689760446548, Final Batch Loss: 0.22262851893901825\n",
      "Epoch 3958, Loss: 1.649735927581787, Final Batch Loss: 0.1912030279636383\n",
      "Epoch 3959, Loss: 1.836768090724945, Final Batch Loss: 0.45844054222106934\n",
      "Epoch 3960, Loss: 1.8019360601902008, Final Batch Loss: 0.31122809648513794\n",
      "Epoch 3961, Loss: 1.7216741740703583, Final Batch Loss: 0.3793482184410095\n",
      "Epoch 3962, Loss: 1.6616538316011429, Final Batch Loss: 0.19246940314769745\n",
      "Epoch 3963, Loss: 1.8071410655975342, Final Batch Loss: 0.3899935781955719\n",
      "Epoch 3964, Loss: 1.8335162997245789, Final Batch Loss: 0.5483387112617493\n",
      "Epoch 3965, Loss: 1.6829960495233536, Final Batch Loss: 0.23031829297542572\n",
      "Epoch 3966, Loss: 1.7660456895828247, Final Batch Loss: 0.4232293665409088\n",
      "Epoch 3967, Loss: 1.707513839006424, Final Batch Loss: 0.35225966572761536\n",
      "Epoch 3968, Loss: 1.9208416044712067, Final Batch Loss: 0.5409036874771118\n",
      "Epoch 3969, Loss: 1.6326407939195633, Final Batch Loss: 0.21856559813022614\n",
      "Epoch 3970, Loss: 1.561321184039116, Final Batch Loss: 0.22466571629047394\n",
      "Epoch 3971, Loss: 1.8526335060596466, Final Batch Loss: 0.4327845871448517\n",
      "Epoch 3972, Loss: 2.295494943857193, Final Batch Loss: 0.8827869296073914\n",
      "Epoch 3973, Loss: 2.379569798707962, Final Batch Loss: 0.6283888816833496\n",
      "Epoch 3974, Loss: 1.928632229566574, Final Batch Loss: 0.30828455090522766\n",
      "Epoch 3975, Loss: 1.9050959050655365, Final Batch Loss: 0.4343879222869873\n",
      "Epoch 3976, Loss: 1.6495265662670135, Final Batch Loss: 0.2751564681529999\n",
      "Epoch 3977, Loss: 1.4812689870595932, Final Batch Loss: 0.22351787984371185\n",
      "Epoch 3978, Loss: 1.6824819445610046, Final Batch Loss: 0.17357829213142395\n",
      "Epoch 3979, Loss: 1.4281429201364517, Final Batch Loss: 0.11075837910175323\n",
      "Epoch 3980, Loss: 1.5480832755565643, Final Batch Loss: 0.1944381296634674\n",
      "Epoch 3981, Loss: 1.5326155424118042, Final Batch Loss: 0.323775053024292\n",
      "Epoch 3982, Loss: 1.664955735206604, Final Batch Loss: 0.28060054779052734\n",
      "Epoch 3983, Loss: 1.5035033524036407, Final Batch Loss: 0.12500056624412537\n",
      "Epoch 3984, Loss: 1.7119854390621185, Final Batch Loss: 0.3592072129249573\n",
      "Epoch 3985, Loss: 1.8450257778167725, Final Batch Loss: 0.44217926263809204\n",
      "Epoch 3986, Loss: 1.747593879699707, Final Batch Loss: 0.43510788679122925\n",
      "Epoch 3987, Loss: 1.5686766803264618, Final Batch Loss: 0.33097246289253235\n",
      "Epoch 3988, Loss: 1.7404438853263855, Final Batch Loss: 0.40254759788513184\n",
      "Epoch 3989, Loss: 1.699831247329712, Final Batch Loss: 0.2484208643436432\n",
      "Epoch 3990, Loss: 1.5824298858642578, Final Batch Loss: 0.29270026087760925\n",
      "Epoch 3991, Loss: 1.9358579218387604, Final Batch Loss: 0.6177159547805786\n",
      "Epoch 3992, Loss: 2.1987142264842987, Final Batch Loss: 0.8277280926704407\n",
      "Epoch 3993, Loss: 1.7283096611499786, Final Batch Loss: 0.3649345934391022\n",
      "Epoch 3994, Loss: 2.1348606646060944, Final Batch Loss: 0.6622223258018494\n",
      "Epoch 3995, Loss: 1.7301743924617767, Final Batch Loss: 0.39396223425865173\n",
      "Epoch 3996, Loss: 1.6966869831085205, Final Batch Loss: 0.37483009696006775\n",
      "Epoch 3997, Loss: 1.3574482947587967, Final Batch Loss: 0.09249766170978546\n",
      "Epoch 3998, Loss: 1.929506242275238, Final Batch Loss: 0.6710343956947327\n",
      "Epoch 3999, Loss: 1.5344839096069336, Final Batch Loss: 0.23547574877738953\n",
      "Epoch 4000, Loss: 1.4738938212394714, Final Batch Loss: 0.15092149376869202\n",
      "Epoch 4001, Loss: 1.7034241259098053, Final Batch Loss: 0.3152977526187897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4002, Loss: 1.696795016527176, Final Batch Loss: 0.34920862317085266\n",
      "Epoch 4003, Loss: 1.7329806685447693, Final Batch Loss: 0.36794742941856384\n",
      "Epoch 4004, Loss: 1.5388228297233582, Final Batch Loss: 0.2737324833869934\n",
      "Epoch 4005, Loss: 1.9127526879310608, Final Batch Loss: 0.4579053521156311\n",
      "Epoch 4006, Loss: 1.77158322930336, Final Batch Loss: 0.40861254930496216\n",
      "Epoch 4007, Loss: 1.7413465976715088, Final Batch Loss: 0.32770881056785583\n",
      "Epoch 4008, Loss: 1.5219846069812775, Final Batch Loss: 0.0382845401763916\n",
      "Epoch 4009, Loss: 1.4526539482176304, Final Batch Loss: 0.032388124614953995\n",
      "Epoch 4010, Loss: 1.31832904368639, Final Batch Loss: 0.04609809070825577\n",
      "Epoch 4011, Loss: 1.357976719737053, Final Batch Loss: 0.12615923583507538\n",
      "Epoch 4012, Loss: 1.6148289144039154, Final Batch Loss: 0.35593628883361816\n",
      "Epoch 4013, Loss: 1.940729409456253, Final Batch Loss: 0.614526629447937\n",
      "Epoch 4014, Loss: 1.9316474795341492, Final Batch Loss: 0.695432186126709\n",
      "Epoch 4015, Loss: 2.143310457468033, Final Batch Loss: 0.8734696507453918\n",
      "Epoch 4016, Loss: 1.7075393199920654, Final Batch Loss: 0.38129907846450806\n",
      "Epoch 4017, Loss: 1.5765204280614853, Final Batch Loss: 0.24899066984653473\n",
      "Epoch 4018, Loss: 1.6428649872541428, Final Batch Loss: 0.21755735576152802\n",
      "Epoch 4019, Loss: 1.9602358639240265, Final Batch Loss: 0.5442147850990295\n",
      "Epoch 4020, Loss: 1.4838732033967972, Final Batch Loss: 0.09448190033435822\n",
      "Epoch 4021, Loss: 1.5852444469928741, Final Batch Loss: 0.2624357044696808\n",
      "Epoch 4022, Loss: 1.866161733865738, Final Batch Loss: 0.4597526490688324\n",
      "Epoch 4023, Loss: 2.0623323023319244, Final Batch Loss: 0.7387934923171997\n",
      "Epoch 4024, Loss: 2.0882430374622345, Final Batch Loss: 0.4710816740989685\n",
      "Epoch 4025, Loss: 1.3924353420734406, Final Batch Loss: 0.23111465573310852\n",
      "Epoch 4026, Loss: 2.1455256044864655, Final Batch Loss: 0.6019254326820374\n",
      "Epoch 4027, Loss: 1.866079717874527, Final Batch Loss: 0.5893301963806152\n",
      "Epoch 4028, Loss: 1.802873283624649, Final Batch Loss: 0.532070517539978\n",
      "Epoch 4029, Loss: 1.6755031198263168, Final Batch Loss: 0.46222737431526184\n",
      "Epoch 4030, Loss: 1.758275955915451, Final Batch Loss: 0.3936462700366974\n",
      "Epoch 4031, Loss: 1.5984406471252441, Final Batch Loss: 0.1952458918094635\n",
      "Epoch 4032, Loss: 1.5441018342971802, Final Batch Loss: 0.2560034692287445\n",
      "Epoch 4033, Loss: 1.7465671002864838, Final Batch Loss: 0.3933691084384918\n",
      "Epoch 4034, Loss: 1.5171254873275757, Final Batch Loss: 0.18466031551361084\n",
      "Epoch 4035, Loss: 1.7579946219921112, Final Batch Loss: 0.22441479563713074\n",
      "Epoch 4036, Loss: 1.6509405076503754, Final Batch Loss: 0.26635512709617615\n",
      "Epoch 4037, Loss: 1.5328420847654343, Final Batch Loss: 0.22986193001270294\n",
      "Epoch 4038, Loss: 1.7875880599021912, Final Batch Loss: 0.30582982301712036\n",
      "Epoch 4039, Loss: 1.7587044835090637, Final Batch Loss: 0.41519370675086975\n",
      "Epoch 4040, Loss: 2.0852769911289215, Final Batch Loss: 0.6202694177627563\n",
      "Epoch 4041, Loss: 1.7092535644769669, Final Batch Loss: 0.3922455906867981\n",
      "Epoch 4042, Loss: 1.6380926966667175, Final Batch Loss: 0.3589375913143158\n",
      "Epoch 4043, Loss: 1.7838841676712036, Final Batch Loss: 0.45601609349250793\n",
      "Epoch 4044, Loss: 1.663339078426361, Final Batch Loss: 0.35704511404037476\n",
      "Epoch 4045, Loss: 1.6588026732206345, Final Batch Loss: 0.23949696123600006\n",
      "Epoch 4046, Loss: 1.7867244184017181, Final Batch Loss: 0.37118059396743774\n",
      "Epoch 4047, Loss: 1.5326739847660065, Final Batch Loss: 0.2773206830024719\n",
      "Epoch 4048, Loss: 1.6651150286197662, Final Batch Loss: 0.36512550711631775\n",
      "Epoch 4049, Loss: 1.5620953291654587, Final Batch Loss: 0.16823388636112213\n",
      "Epoch 4050, Loss: 1.5771773010492325, Final Batch Loss: 0.14184938371181488\n",
      "Epoch 4051, Loss: 1.8795567154884338, Final Batch Loss: 0.46479716897010803\n",
      "Epoch 4052, Loss: 1.9738760888576508, Final Batch Loss: 0.3503841459751129\n",
      "Epoch 4053, Loss: 1.6578460335731506, Final Batch Loss: 0.27660852670669556\n",
      "Epoch 4054, Loss: 1.6359670013189316, Final Batch Loss: 0.1454881876707077\n",
      "Epoch 4055, Loss: 1.591460958123207, Final Batch Loss: 0.24279285967350006\n",
      "Epoch 4056, Loss: 1.7246938049793243, Final Batch Loss: 0.4664441645145416\n",
      "Epoch 4057, Loss: 1.4440515488386154, Final Batch Loss: 0.15914739668369293\n",
      "Epoch 4058, Loss: 1.5935997366905212, Final Batch Loss: 0.301984041929245\n",
      "Epoch 4059, Loss: 1.610016107559204, Final Batch Loss: 0.273375928401947\n",
      "Epoch 4060, Loss: 1.5021037608385086, Final Batch Loss: 0.20869000256061554\n",
      "Epoch 4061, Loss: 1.8390044271945953, Final Batch Loss: 0.4430480897426605\n",
      "Epoch 4062, Loss: 1.6229522228240967, Final Batch Loss: 0.20680710673332214\n",
      "Epoch 4063, Loss: 1.6393979787826538, Final Batch Loss: 0.2662796974182129\n",
      "Epoch 4064, Loss: 1.8069854974746704, Final Batch Loss: 0.453396737575531\n",
      "Epoch 4065, Loss: 1.6736983060836792, Final Batch Loss: 0.32237252593040466\n",
      "Epoch 4066, Loss: 1.909977287054062, Final Batch Loss: 0.4987811744213104\n",
      "Epoch 4067, Loss: 1.7227358222007751, Final Batch Loss: 0.27075693011283875\n",
      "Epoch 4068, Loss: 1.5786408931016922, Final Batch Loss: 0.16571100056171417\n",
      "Epoch 4069, Loss: 1.8012605011463165, Final Batch Loss: 0.42863035202026367\n",
      "Epoch 4070, Loss: 1.7468495965003967, Final Batch Loss: 0.3034129738807678\n",
      "Epoch 4071, Loss: 1.6156456172466278, Final Batch Loss: 0.3082672655582428\n",
      "Epoch 4072, Loss: 1.8404844403266907, Final Batch Loss: 0.44704222679138184\n",
      "Epoch 4073, Loss: 1.8912229537963867, Final Batch Loss: 0.5850073099136353\n",
      "Epoch 4074, Loss: 2.0361509323120117, Final Batch Loss: 0.5341817140579224\n",
      "Epoch 4075, Loss: 2.4452938735485077, Final Batch Loss: 0.889071524143219\n",
      "Epoch 4076, Loss: 1.673664629459381, Final Batch Loss: 0.2742007076740265\n",
      "Epoch 4077, Loss: 1.8574872016906738, Final Batch Loss: 0.4258398413658142\n",
      "Epoch 4078, Loss: 1.531845584511757, Final Batch Loss: 0.13769538700580597\n",
      "Epoch 4079, Loss: 2.2126517295837402, Final Batch Loss: 0.6370884776115417\n",
      "Epoch 4080, Loss: 1.616245299577713, Final Batch Loss: 0.2606881558895111\n",
      "Epoch 4081, Loss: 1.7985994815826416, Final Batch Loss: 0.34508100152015686\n",
      "Epoch 4082, Loss: 2.056043565273285, Final Batch Loss: 0.6413108706474304\n",
      "Epoch 4083, Loss: 1.3989271372556686, Final Batch Loss: 0.14275987446308136\n",
      "Epoch 4084, Loss: 1.8132553100585938, Final Batch Loss: 0.4813997447490692\n",
      "Epoch 4085, Loss: 1.6463807225227356, Final Batch Loss: 0.27459582686424255\n",
      "Epoch 4086, Loss: 1.6282514035701752, Final Batch Loss: 0.27953213453292847\n",
      "Epoch 4087, Loss: 1.7249820232391357, Final Batch Loss: 0.3640037178993225\n",
      "Epoch 4088, Loss: 1.6609261333942413, Final Batch Loss: 0.26763686537742615\n",
      "Epoch 4089, Loss: 1.7909571826457977, Final Batch Loss: 0.46438834071159363\n",
      "Epoch 4090, Loss: 1.8202700018882751, Final Batch Loss: 0.4723040461540222\n",
      "Epoch 4091, Loss: 1.7612452805042267, Final Batch Loss: 0.39668509364128113\n",
      "Epoch 4092, Loss: 1.7468044459819794, Final Batch Loss: 0.28825315833091736\n",
      "Epoch 4093, Loss: 1.6087986528873444, Final Batch Loss: 0.38350528478622437\n",
      "Epoch 4094, Loss: 2.18013796210289, Final Batch Loss: 0.7328242063522339\n",
      "Epoch 4095, Loss: 1.7335914969444275, Final Batch Loss: 0.33565211296081543\n",
      "Epoch 4096, Loss: 1.5676711946725845, Final Batch Loss: 0.204356387257576\n",
      "Epoch 4097, Loss: 1.8561863601207733, Final Batch Loss: 0.5384567975997925\n",
      "Epoch 4098, Loss: 1.5106735080480576, Final Batch Loss: 0.1569991558790207\n",
      "Epoch 4099, Loss: 2.177367329597473, Final Batch Loss: 0.8644987344741821\n",
      "Epoch 4100, Loss: 1.484065242111683, Final Batch Loss: 0.10065368562936783\n",
      "Epoch 4101, Loss: 2.1254883110523224, Final Batch Loss: 0.7004373669624329\n",
      "Epoch 4102, Loss: 1.9319697320461273, Final Batch Loss: 0.5040163397789001\n",
      "Epoch 4103, Loss: 1.6153198331594467, Final Batch Loss: 0.23839791119098663\n",
      "Epoch 4104, Loss: 1.7327525317668915, Final Batch Loss: 0.39424845576286316\n",
      "Epoch 4105, Loss: 1.5812844783067703, Final Batch Loss: 0.13017995655536652\n",
      "Epoch 4106, Loss: 1.5990218371152878, Final Batch Loss: 0.2128824144601822\n",
      "Epoch 4107, Loss: 1.5102304741740227, Final Batch Loss: 0.12327875941991806\n",
      "Epoch 4108, Loss: 1.6606831848621368, Final Batch Loss: 0.3037957549095154\n",
      "Epoch 4109, Loss: 1.742944598197937, Final Batch Loss: 0.47375568747520447\n",
      "Epoch 4110, Loss: 1.4939422607421875, Final Batch Loss: 0.16637051105499268\n",
      "Epoch 4111, Loss: 2.2402663230895996, Final Batch Loss: 0.6924217343330383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4112, Loss: 1.7728561908006668, Final Batch Loss: 0.21299274265766144\n",
      "Epoch 4113, Loss: 1.6298666298389435, Final Batch Loss: 0.31414324045181274\n",
      "Epoch 4114, Loss: 1.663857862353325, Final Batch Loss: 0.2471640259027481\n",
      "Epoch 4115, Loss: 1.3743001446127892, Final Batch Loss: 0.0829174742102623\n",
      "Epoch 4116, Loss: 2.0068318843841553, Final Batch Loss: 0.4883723258972168\n",
      "Epoch 4117, Loss: 1.774895966053009, Final Batch Loss: 0.3846369683742523\n",
      "Epoch 4118, Loss: 1.5959689915180206, Final Batch Loss: 0.3364008367061615\n",
      "Epoch 4119, Loss: 1.6298618018627167, Final Batch Loss: 0.22482940554618835\n",
      "Epoch 4120, Loss: 1.9068835079669952, Final Batch Loss: 0.602914035320282\n",
      "Epoch 4121, Loss: 1.6825635433197021, Final Batch Loss: 0.31398606300354004\n",
      "Epoch 4122, Loss: 1.5825465321540833, Final Batch Loss: 0.1281178891658783\n",
      "Epoch 4123, Loss: 1.8662050068378448, Final Batch Loss: 0.5324734449386597\n",
      "Epoch 4124, Loss: 1.7288700938224792, Final Batch Loss: 0.3457067012786865\n",
      "Epoch 4125, Loss: 1.4323729053139687, Final Batch Loss: 0.10595161467790604\n",
      "Epoch 4126, Loss: 1.836695909500122, Final Batch Loss: 0.3830319046974182\n",
      "Epoch 4127, Loss: 2.066497176885605, Final Batch Loss: 0.700512707233429\n",
      "Epoch 4128, Loss: 1.5703750103712082, Final Batch Loss: 0.1795303076505661\n",
      "Epoch 4129, Loss: 2.3860740959644318, Final Batch Loss: 0.6887815594673157\n",
      "Epoch 4130, Loss: 1.5541770607233047, Final Batch Loss: 0.09452785551548004\n",
      "Epoch 4131, Loss: 2.32151597738266, Final Batch Loss: 0.8250325322151184\n",
      "Epoch 4132, Loss: 2.024089366197586, Final Batch Loss: 0.5438393950462341\n",
      "Epoch 4133, Loss: 1.9759985208511353, Final Batch Loss: 0.41237708926200867\n",
      "Epoch 4134, Loss: 2.2209106385707855, Final Batch Loss: 0.658825695514679\n",
      "Epoch 4135, Loss: 1.8545284271240234, Final Batch Loss: 0.2690723240375519\n",
      "Epoch 4136, Loss: 1.684590369462967, Final Batch Loss: 0.15987896919250488\n",
      "Epoch 4137, Loss: 1.899836540222168, Final Batch Loss: 0.5136743187904358\n",
      "Epoch 4138, Loss: 1.724956750869751, Final Batch Loss: 0.3309152126312256\n",
      "Epoch 4139, Loss: 1.6806649565696716, Final Batch Loss: 0.26707595586776733\n",
      "Epoch 4140, Loss: 1.5660674422979355, Final Batch Loss: 0.2217472344636917\n",
      "Epoch 4141, Loss: 1.5916044265031815, Final Batch Loss: 0.1296425312757492\n",
      "Epoch 4142, Loss: 1.6602312475442886, Final Batch Loss: 0.44113248586654663\n",
      "Epoch 4143, Loss: 1.7414696663618088, Final Batch Loss: 0.4183175563812256\n",
      "Epoch 4144, Loss: 1.5547803044319153, Final Batch Loss: 0.17451611161231995\n",
      "Epoch 4145, Loss: 1.6965119540691376, Final Batch Loss: 0.33841952681541443\n",
      "Epoch 4146, Loss: 1.4713323190808296, Final Batch Loss: 0.08070606738328934\n",
      "Epoch 4147, Loss: 2.2271625995635986, Final Batch Loss: 0.8319883346557617\n",
      "Epoch 4148, Loss: 1.512375682592392, Final Batch Loss: 0.22029855847358704\n",
      "Epoch 4149, Loss: 1.6160121411085129, Final Batch Loss: 0.19544617831707\n",
      "Epoch 4150, Loss: 1.5726934671401978, Final Batch Loss: 0.16678014397621155\n",
      "Epoch 4151, Loss: 1.4625326544046402, Final Batch Loss: 0.15852247178554535\n",
      "Epoch 4152, Loss: 1.8836633563041687, Final Batch Loss: 0.41932955384254456\n",
      "Epoch 4153, Loss: 1.4992071390151978, Final Batch Loss: 0.2195964902639389\n",
      "Epoch 4154, Loss: 1.8608371019363403, Final Batch Loss: 0.48851582407951355\n",
      "Epoch 4155, Loss: 1.906478077173233, Final Batch Loss: 0.6656702756881714\n",
      "Epoch 4156, Loss: 1.5005643889307976, Final Batch Loss: 0.09244438260793686\n",
      "Epoch 4157, Loss: 1.401501551270485, Final Batch Loss: 0.17562590539455414\n",
      "Epoch 4158, Loss: 1.8396620750427246, Final Batch Loss: 0.531653881072998\n",
      "Epoch 4159, Loss: 1.932141900062561, Final Batch Loss: 0.5845934748649597\n",
      "Epoch 4160, Loss: 1.885944813489914, Final Batch Loss: 0.4597357213497162\n",
      "Epoch 4161, Loss: 1.6136369854211807, Final Batch Loss: 0.22895930707454681\n",
      "Epoch 4162, Loss: 1.7814519107341766, Final Batch Loss: 0.4753446877002716\n",
      "Epoch 4163, Loss: 1.47758175060153, Final Batch Loss: 0.04753054305911064\n",
      "Epoch 4164, Loss: 1.5698416382074356, Final Batch Loss: 0.14119063317775726\n",
      "Epoch 4165, Loss: 1.345296986401081, Final Batch Loss: 0.06440690904855728\n",
      "Epoch 4166, Loss: 1.676043227314949, Final Batch Loss: 0.1945687085390091\n",
      "Epoch 4167, Loss: 1.3095822054892778, Final Batch Loss: 0.02099168486893177\n",
      "Epoch 4168, Loss: 1.5389242470264435, Final Batch Loss: 0.21538662910461426\n",
      "Epoch 4169, Loss: 1.4458308219909668, Final Batch Loss: 0.05145666003227234\n",
      "Epoch 4170, Loss: 1.4269979447126389, Final Batch Loss: 0.14573277533054352\n",
      "Epoch 4171, Loss: 1.6358721256256104, Final Batch Loss: 0.39323848485946655\n",
      "Epoch 4172, Loss: 1.6588536500930786, Final Batch Loss: 0.3508540987968445\n",
      "Epoch 4173, Loss: 1.8108764588832855, Final Batch Loss: 0.4269223213195801\n",
      "Epoch 4174, Loss: 1.7097728699445724, Final Batch Loss: 0.2050812691450119\n",
      "Epoch 4175, Loss: 1.8347733914852142, Final Batch Loss: 0.467862069606781\n",
      "Epoch 4176, Loss: 1.6772916465997696, Final Batch Loss: 0.2350700944662094\n",
      "Epoch 4177, Loss: 1.778888076543808, Final Batch Loss: 0.3532237708568573\n",
      "Epoch 4178, Loss: 1.7346095442771912, Final Batch Loss: 0.3420241177082062\n",
      "Epoch 4179, Loss: 1.5645835399627686, Final Batch Loss: 0.26454129815101624\n",
      "Epoch 4180, Loss: 1.6697213798761368, Final Batch Loss: 0.21812714636325836\n",
      "Epoch 4181, Loss: 1.6029887199401855, Final Batch Loss: 0.29832199215888977\n",
      "Epoch 4182, Loss: 1.4184898287057877, Final Batch Loss: 0.15795953571796417\n",
      "Epoch 4183, Loss: 1.614016056060791, Final Batch Loss: 0.2736166715621948\n",
      "Epoch 4184, Loss: 1.6337742507457733, Final Batch Loss: 0.381213515996933\n",
      "Epoch 4185, Loss: 1.7165228128433228, Final Batch Loss: 0.43118271231651306\n",
      "Epoch 4186, Loss: 1.5149462819099426, Final Batch Loss: 0.27549153566360474\n",
      "Epoch 4187, Loss: 1.8249867856502533, Final Batch Loss: 0.3595609664916992\n",
      "Epoch 4188, Loss: 1.787932127714157, Final Batch Loss: 0.2776770293712616\n",
      "Epoch 4189, Loss: 1.704304426908493, Final Batch Loss: 0.2536470293998718\n",
      "Epoch 4190, Loss: 1.534221112728119, Final Batch Loss: 0.27377626299858093\n",
      "Epoch 4191, Loss: 1.4768814444541931, Final Batch Loss: 0.12228292226791382\n",
      "Epoch 4192, Loss: 1.7716315686702728, Final Batch Loss: 0.3486159145832062\n",
      "Epoch 4193, Loss: 2.136530578136444, Final Batch Loss: 0.7557798027992249\n",
      "Epoch 4194, Loss: 1.3988864943385124, Final Batch Loss: 0.08814319223165512\n",
      "Epoch 4195, Loss: 1.8898323476314545, Final Batch Loss: 0.3251335918903351\n",
      "Epoch 4196, Loss: 1.487720474600792, Final Batch Loss: 0.1900387853384018\n",
      "Epoch 4197, Loss: 1.6346123069524765, Final Batch Loss: 0.24324701726436615\n",
      "Epoch 4198, Loss: 1.8524486720561981, Final Batch Loss: 0.4908582270145416\n",
      "Epoch 4199, Loss: 1.9908459782600403, Final Batch Loss: 0.6389986276626587\n",
      "Epoch 4200, Loss: 1.9072205126285553, Final Batch Loss: 0.5030214786529541\n",
      "Epoch 4201, Loss: 1.447013109922409, Final Batch Loss: 0.0606539249420166\n",
      "Epoch 4202, Loss: 1.9095936119556427, Final Batch Loss: 0.4561897814273834\n",
      "Epoch 4203, Loss: 1.6321226060390472, Final Batch Loss: 0.33253219723701477\n",
      "Epoch 4204, Loss: 1.766920119524002, Final Batch Loss: 0.4895710349082947\n",
      "Epoch 4205, Loss: 1.647041916847229, Final Batch Loss: 0.16344663500785828\n",
      "Epoch 4206, Loss: 1.5517704486846924, Final Batch Loss: 0.3027576804161072\n",
      "Epoch 4207, Loss: 1.9875216484069824, Final Batch Loss: 0.4810916781425476\n",
      "Epoch 4208, Loss: 1.7430414259433746, Final Batch Loss: 0.45242294669151306\n",
      "Epoch 4209, Loss: 2.018806040287018, Final Batch Loss: 0.5110118985176086\n",
      "Epoch 4210, Loss: 1.8371807932853699, Final Batch Loss: 0.34595054388046265\n",
      "Epoch 4211, Loss: 1.7368227243423462, Final Batch Loss: 0.2965976595878601\n",
      "Epoch 4212, Loss: 1.679872453212738, Final Batch Loss: 0.3355838358402252\n",
      "Epoch 4213, Loss: 1.8795890510082245, Final Batch Loss: 0.45530930161476135\n",
      "Epoch 4214, Loss: 1.4339491799473763, Final Batch Loss: 0.10083932429552078\n",
      "Epoch 4215, Loss: 1.597015157341957, Final Batch Loss: 0.22826452553272247\n",
      "Epoch 4216, Loss: 2.0449464917182922, Final Batch Loss: 0.7225379347801208\n",
      "Epoch 4217, Loss: 1.597693145275116, Final Batch Loss: 0.2788877785205841\n",
      "Epoch 4218, Loss: 1.652858316898346, Final Batch Loss: 0.25759249925613403\n",
      "Epoch 4219, Loss: 1.560839831829071, Final Batch Loss: 0.26595258712768555\n",
      "Epoch 4220, Loss: 1.8116833865642548, Final Batch Loss: 0.43670350313186646\n",
      "Epoch 4221, Loss: 1.5420786291360855, Final Batch Loss: 0.17616184055805206\n",
      "Epoch 4222, Loss: 1.5334446281194687, Final Batch Loss: 0.2446841150522232\n",
      "Epoch 4223, Loss: 1.7282335758209229, Final Batch Loss: 0.37382546067237854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4224, Loss: 1.8162549138069153, Final Batch Loss: 0.5319006443023682\n",
      "Epoch 4225, Loss: 1.7128390967845917, Final Batch Loss: 0.38875940442085266\n",
      "Epoch 4226, Loss: 1.65522500872612, Final Batch Loss: 0.3750026226043701\n",
      "Epoch 4227, Loss: 1.6452042758464813, Final Batch Loss: 0.4155557453632355\n",
      "Epoch 4228, Loss: 1.5542055070400238, Final Batch Loss: 0.2924877107143402\n",
      "Epoch 4229, Loss: 1.7030295729637146, Final Batch Loss: 0.3679184317588806\n",
      "Epoch 4230, Loss: 2.308479219675064, Final Batch Loss: 0.6886754631996155\n",
      "Epoch 4231, Loss: 2.429182916879654, Final Batch Loss: 0.447757750749588\n",
      "Epoch 4232, Loss: 2.107435777783394, Final Batch Loss: 0.2092549353837967\n",
      "Epoch 4233, Loss: 2.217771500349045, Final Batch Loss: 0.6235169172286987\n",
      "Epoch 4234, Loss: 1.9910772740840912, Final Batch Loss: 0.4065806269645691\n",
      "Epoch 4235, Loss: 1.664057731628418, Final Batch Loss: 0.18215572834014893\n",
      "Epoch 4236, Loss: 1.896073341369629, Final Batch Loss: 0.4639119803905487\n",
      "Epoch 4237, Loss: 1.5942268520593643, Final Batch Loss: 0.1813919097185135\n",
      "Epoch 4238, Loss: 1.7993842661380768, Final Batch Loss: 0.3324820101261139\n",
      "Epoch 4239, Loss: 1.8408308327198029, Final Batch Loss: 0.4783478081226349\n",
      "Epoch 4240, Loss: 1.504161536693573, Final Batch Loss: 0.14502152800559998\n",
      "Epoch 4241, Loss: 1.747982680797577, Final Batch Loss: 0.4421745836734772\n",
      "Epoch 4242, Loss: 1.6886995732784271, Final Batch Loss: 0.31155404448509216\n",
      "Epoch 4243, Loss: 1.7861512303352356, Final Batch Loss: 0.4264812171459198\n",
      "Epoch 4244, Loss: 1.7045499682426453, Final Batch Loss: 0.30455923080444336\n",
      "Epoch 4245, Loss: 1.5450451970100403, Final Batch Loss: 0.3109476864337921\n",
      "Epoch 4246, Loss: 1.3502387627959251, Final Batch Loss: 0.03606598824262619\n",
      "Epoch 4247, Loss: 1.4947823733091354, Final Batch Loss: 0.2217549830675125\n",
      "Epoch 4248, Loss: 1.4153131023049355, Final Batch Loss: 0.11986137181520462\n",
      "Epoch 4249, Loss: 1.5253296494483948, Final Batch Loss: 0.19765210151672363\n",
      "Epoch 4250, Loss: 1.4486634135246277, Final Batch Loss: 0.23367759585380554\n",
      "Epoch 4251, Loss: 1.7048819065093994, Final Batch Loss: 0.4474339485168457\n",
      "Epoch 4252, Loss: 1.539107620716095, Final Batch Loss: 0.26202836632728577\n",
      "Epoch 4253, Loss: 1.695806622505188, Final Batch Loss: 0.4455724358558655\n",
      "Epoch 4254, Loss: 1.7388513386249542, Final Batch Loss: 0.3956175744533539\n",
      "Epoch 4255, Loss: 1.9193370044231415, Final Batch Loss: 0.52813321352005\n",
      "Epoch 4256, Loss: 1.717032641172409, Final Batch Loss: 0.3825865387916565\n",
      "Epoch 4257, Loss: 1.7099429965019226, Final Batch Loss: 0.2408435046672821\n",
      "Epoch 4258, Loss: 1.5736153721809387, Final Batch Loss: 0.3493165969848633\n",
      "Epoch 4259, Loss: 1.3728086352348328, Final Batch Loss: 0.1325087994337082\n",
      "Epoch 4260, Loss: 1.4933673590421677, Final Batch Loss: 0.08817599713802338\n",
      "Epoch 4261, Loss: 1.438469409942627, Final Batch Loss: 0.033613622188568115\n",
      "Epoch 4262, Loss: 1.5594161301851273, Final Batch Loss: 0.24062944948673248\n",
      "Epoch 4263, Loss: 1.58872389793396, Final Batch Loss: 0.3146844506263733\n",
      "Epoch 4264, Loss: 1.972446858882904, Final Batch Loss: 0.561156690120697\n",
      "Epoch 4265, Loss: 1.4182767048478127, Final Batch Loss: 0.06342671066522598\n",
      "Epoch 4266, Loss: 1.8969728350639343, Final Batch Loss: 0.4845126271247864\n",
      "Epoch 4267, Loss: 1.9017996490001678, Final Batch Loss: 0.6077680587768555\n",
      "Epoch 4268, Loss: 1.441839538514614, Final Batch Loss: 0.11934035271406174\n",
      "Epoch 4269, Loss: 1.6926537156105042, Final Batch Loss: 0.37285977602005005\n",
      "Epoch 4270, Loss: 1.5221987664699554, Final Batch Loss: 0.2175149917602539\n",
      "Epoch 4271, Loss: 1.3998421281576157, Final Batch Loss: 0.18054820597171783\n",
      "Epoch 4272, Loss: 1.4638580083847046, Final Batch Loss: 0.09571796655654907\n",
      "Epoch 4273, Loss: 1.620163083076477, Final Batch Loss: 0.3299575746059418\n",
      "Epoch 4274, Loss: 1.2676253281533718, Final Batch Loss: 0.01889866217970848\n",
      "Epoch 4275, Loss: 1.7242310643196106, Final Batch Loss: 0.4644031822681427\n",
      "Epoch 4276, Loss: 1.3304090425372124, Final Batch Loss: 0.05962864309549332\n",
      "Epoch 4277, Loss: 1.6342913210391998, Final Batch Loss: 0.27382808923721313\n",
      "Epoch 4278, Loss: 1.6902266144752502, Final Batch Loss: 0.2954023778438568\n",
      "Epoch 4279, Loss: 1.7465735077857971, Final Batch Loss: 0.4251801669597626\n",
      "Epoch 4280, Loss: 1.830662578344345, Final Batch Loss: 0.3584788739681244\n",
      "Epoch 4281, Loss: 1.8453556597232819, Final Batch Loss: 0.39244046807289124\n",
      "Epoch 4282, Loss: 1.8972646594047546, Final Batch Loss: 0.4325319230556488\n",
      "Epoch 4283, Loss: 1.6269679218530655, Final Batch Loss: 0.35082897543907166\n",
      "Epoch 4284, Loss: 1.7235613465309143, Final Batch Loss: 0.3216409683227539\n",
      "Epoch 4285, Loss: 2.000204771757126, Final Batch Loss: 0.39779144525527954\n",
      "Epoch 4286, Loss: 1.7870245277881622, Final Batch Loss: 0.285066694021225\n",
      "Epoch 4287, Loss: 1.640708088874817, Final Batch Loss: 0.30669668316841125\n",
      "Epoch 4288, Loss: 2.0047480762004852, Final Batch Loss: 0.7330525517463684\n",
      "Epoch 4289, Loss: 1.8592975437641144, Final Batch Loss: 0.5674823522567749\n",
      "Epoch 4290, Loss: 1.4624168276786804, Final Batch Loss: 0.17039644718170166\n",
      "Epoch 4291, Loss: 1.6796539723873138, Final Batch Loss: 0.45236483216285706\n",
      "Epoch 4292, Loss: 1.9182493090629578, Final Batch Loss: 0.49433305859565735\n",
      "Epoch 4293, Loss: 2.1455538272857666, Final Batch Loss: 0.5912110209465027\n",
      "Epoch 4294, Loss: 1.8937199711799622, Final Batch Loss: 0.5024392008781433\n",
      "Epoch 4295, Loss: 1.3042916543781757, Final Batch Loss: 0.032975275069475174\n",
      "Epoch 4296, Loss: 1.7407483160495758, Final Batch Loss: 0.3694131374359131\n",
      "Epoch 4297, Loss: 1.4337137788534164, Final Batch Loss: 0.18292765319347382\n",
      "Epoch 4298, Loss: 1.4002353698015213, Final Batch Loss: 0.13790731132030487\n",
      "Epoch 4299, Loss: 1.6205039620399475, Final Batch Loss: 0.25135186314582825\n",
      "Epoch 4300, Loss: 1.9324177503585815, Final Batch Loss: 0.6143933534622192\n",
      "Epoch 4301, Loss: 1.4279860854148865, Final Batch Loss: 0.10997539758682251\n",
      "Epoch 4302, Loss: 1.5728748142719269, Final Batch Loss: 0.1922614574432373\n",
      "Epoch 4303, Loss: 1.6765713095664978, Final Batch Loss: 0.4609067142009735\n",
      "Epoch 4304, Loss: 1.7160589098930359, Final Batch Loss: 0.4578406810760498\n",
      "Epoch 4305, Loss: 1.527935966849327, Final Batch Loss: 0.14065586030483246\n",
      "Epoch 4306, Loss: 1.663596361875534, Final Batch Loss: 0.3324689269065857\n",
      "Epoch 4307, Loss: 1.7240698486566544, Final Batch Loss: 0.5008188486099243\n",
      "Epoch 4308, Loss: 2.2924139499664307, Final Batch Loss: 0.9384409785270691\n",
      "Epoch 4309, Loss: 1.7804888486862183, Final Batch Loss: 0.36260658502578735\n",
      "Epoch 4310, Loss: 1.9279844164848328, Final Batch Loss: 0.6065930724143982\n",
      "Epoch 4311, Loss: 1.8145121335983276, Final Batch Loss: 0.41666314005851746\n",
      "Epoch 4312, Loss: 1.708364725112915, Final Batch Loss: 0.4265671372413635\n",
      "Epoch 4313, Loss: 1.512142837047577, Final Batch Loss: 0.1280827522277832\n",
      "Epoch 4314, Loss: 1.7461628019809723, Final Batch Loss: 0.2820744812488556\n",
      "Epoch 4315, Loss: 1.5536531805992126, Final Batch Loss: 0.2833578884601593\n",
      "Epoch 4316, Loss: 1.8286608755588531, Final Batch Loss: 0.5280510783195496\n",
      "Epoch 4317, Loss: 1.9025710225105286, Final Batch Loss: 0.649147629737854\n",
      "Epoch 4318, Loss: 1.4688093960285187, Final Batch Loss: 0.13595885038375854\n",
      "Epoch 4319, Loss: 1.734150916337967, Final Batch Loss: 0.46732833981513977\n",
      "Epoch 4320, Loss: 1.6589376330375671, Final Batch Loss: 0.27621766924858093\n",
      "Epoch 4321, Loss: 1.8537840247154236, Final Batch Loss: 0.36504656076431274\n",
      "Epoch 4322, Loss: 1.7534145414829254, Final Batch Loss: 0.4883415102958679\n",
      "Epoch 4323, Loss: 1.4688589796423912, Final Batch Loss: 0.11678250879049301\n",
      "Epoch 4324, Loss: 1.6316328793764114, Final Batch Loss: 0.1765039712190628\n",
      "Epoch 4325, Loss: 1.4334054291248322, Final Batch Loss: 0.23775146901607513\n",
      "Epoch 4326, Loss: 2.1166179478168488, Final Batch Loss: 0.7480728030204773\n",
      "Epoch 4327, Loss: 1.5752819925546646, Final Batch Loss: 0.1510428637266159\n",
      "Epoch 4328, Loss: 1.4648243859410286, Final Batch Loss: 0.10397032648324966\n",
      "Epoch 4329, Loss: 1.7671778798103333, Final Batch Loss: 0.27469637989997864\n",
      "Epoch 4330, Loss: 1.8174699544906616, Final Batch Loss: 0.4002296030521393\n",
      "Epoch 4331, Loss: 1.4224625080823898, Final Batch Loss: 0.1747031956911087\n",
      "Epoch 4332, Loss: 1.8182841837406158, Final Batch Loss: 0.4483412206172943\n",
      "Epoch 4333, Loss: 1.5179757326841354, Final Batch Loss: 0.20593257248401642\n",
      "Epoch 4334, Loss: 1.7641652822494507, Final Batch Loss: 0.36372795701026917\n",
      "Epoch 4335, Loss: 1.701891079545021, Final Batch Loss: 0.22964350879192352\n",
      "Epoch 4336, Loss: 1.679497480392456, Final Batch Loss: 0.3011741042137146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4337, Loss: 1.6546214520931244, Final Batch Loss: 0.3532007336616516\n",
      "Epoch 4338, Loss: 1.796201467514038, Final Batch Loss: 0.44706991314888\n",
      "Epoch 4339, Loss: 1.6189647614955902, Final Batch Loss: 0.18916013836860657\n",
      "Epoch 4340, Loss: 1.5363263487815857, Final Batch Loss: 0.15310943126678467\n",
      "Epoch 4341, Loss: 1.641925036907196, Final Batch Loss: 0.3660326898097992\n",
      "Epoch 4342, Loss: 2.3782476782798767, Final Batch Loss: 1.0700734853744507\n",
      "Epoch 4343, Loss: 1.5827664658427238, Final Batch Loss: 0.10086148232221603\n",
      "Epoch 4344, Loss: 1.9405674934387207, Final Batch Loss: 0.4072149693965912\n",
      "Epoch 4345, Loss: 1.8858081102371216, Final Batch Loss: 0.5894302725791931\n",
      "Epoch 4346, Loss: 1.4830218106508255, Final Batch Loss: 0.1498698741197586\n",
      "Epoch 4347, Loss: 1.613755702972412, Final Batch Loss: 0.2292478382587433\n",
      "Epoch 4348, Loss: 1.5926759392023087, Final Batch Loss: 0.15721668303012848\n",
      "Epoch 4349, Loss: 1.6806874573230743, Final Batch Loss: 0.2241567075252533\n",
      "Epoch 4350, Loss: 1.4656939953565598, Final Batch Loss: 0.17510481178760529\n",
      "Epoch 4351, Loss: 1.7325311601161957, Final Batch Loss: 0.476158082485199\n",
      "Epoch 4352, Loss: 1.5744220614433289, Final Batch Loss: 0.2290719449520111\n",
      "Epoch 4353, Loss: 1.6688827723264694, Final Batch Loss: 0.3741030991077423\n",
      "Epoch 4354, Loss: 2.0128926634788513, Final Batch Loss: 0.7605825066566467\n",
      "Epoch 4355, Loss: 2.1510236263275146, Final Batch Loss: 0.7341670989990234\n",
      "Epoch 4356, Loss: 1.7166363894939423, Final Batch Loss: 0.3702864944934845\n",
      "Epoch 4357, Loss: 1.8466117084026337, Final Batch Loss: 0.5282346606254578\n",
      "Epoch 4358, Loss: 1.5620727092027664, Final Batch Loss: 0.26361027359962463\n",
      "Epoch 4359, Loss: 1.4940987601876259, Final Batch Loss: 0.11273089796304703\n",
      "Epoch 4360, Loss: 1.6980340778827667, Final Batch Loss: 0.4059654772281647\n",
      "Epoch 4361, Loss: 1.5990340113639832, Final Batch Loss: 0.21971821784973145\n",
      "Epoch 4362, Loss: 1.4536450952291489, Final Batch Loss: 0.20993967354297638\n",
      "Epoch 4363, Loss: 1.5227976739406586, Final Batch Loss: 0.16638946533203125\n",
      "Epoch 4364, Loss: 1.4275207370519638, Final Batch Loss: 0.13315413892269135\n",
      "Epoch 4365, Loss: 1.5636657178401947, Final Batch Loss: 0.2924155294895172\n",
      "Epoch 4366, Loss: 1.749619871377945, Final Batch Loss: 0.37725135684013367\n",
      "Epoch 4367, Loss: 1.7195525169372559, Final Batch Loss: 0.350829541683197\n",
      "Epoch 4368, Loss: 1.6959397196769714, Final Batch Loss: 0.4275701642036438\n",
      "Epoch 4369, Loss: 2.1181029975414276, Final Batch Loss: 0.7686521410942078\n",
      "Epoch 4370, Loss: 1.7630959451198578, Final Batch Loss: 0.4309164881706238\n",
      "Epoch 4371, Loss: 1.3302636966109276, Final Batch Loss: 0.07050155848264694\n",
      "Epoch 4372, Loss: 1.7772933542728424, Final Batch Loss: 0.3993033766746521\n",
      "Epoch 4373, Loss: 1.8436758816242218, Final Batch Loss: 0.5353171825408936\n",
      "Epoch 4374, Loss: 1.581672489643097, Final Batch Loss: 0.237233966588974\n",
      "Epoch 4375, Loss: 1.7409091591835022, Final Batch Loss: 0.3112143874168396\n",
      "Epoch 4376, Loss: 1.5855225026607513, Final Batch Loss: 0.2729812562465668\n",
      "Epoch 4377, Loss: 1.5585061460733414, Final Batch Loss: 0.1912773698568344\n",
      "Epoch 4378, Loss: 1.5029276907444, Final Batch Loss: 0.17708906531333923\n",
      "Epoch 4379, Loss: 1.5938875079154968, Final Batch Loss: 0.40130025148391724\n",
      "Epoch 4380, Loss: 1.828937441110611, Final Batch Loss: 0.504084587097168\n",
      "Epoch 4381, Loss: 1.6172996163368225, Final Batch Loss: 0.38336798548698425\n",
      "Epoch 4382, Loss: 1.632850557565689, Final Batch Loss: 0.3285439908504486\n",
      "Epoch 4383, Loss: 1.6915331184864044, Final Batch Loss: 0.3780912756919861\n",
      "Epoch 4384, Loss: 1.788207858800888, Final Batch Loss: 0.3025391101837158\n",
      "Epoch 4385, Loss: 1.9428484439849854, Final Batch Loss: 0.5983161330223083\n",
      "Epoch 4386, Loss: 1.6005131006240845, Final Batch Loss: 0.17670133709907532\n",
      "Epoch 4387, Loss: 1.9334341287612915, Final Batch Loss: 0.5371216535568237\n",
      "Epoch 4388, Loss: 1.4701732844114304, Final Batch Loss: 0.1827002316713333\n",
      "Epoch 4389, Loss: 1.7333835661411285, Final Batch Loss: 0.3993357717990875\n",
      "Epoch 4390, Loss: 1.6319270581007004, Final Batch Loss: 0.22871056199073792\n",
      "Epoch 4391, Loss: 1.640723466873169, Final Batch Loss: 0.3251788914203644\n",
      "Epoch 4392, Loss: 1.5517012476921082, Final Batch Loss: 0.2505532205104828\n",
      "Epoch 4393, Loss: 1.5953966528177261, Final Batch Loss: 0.39294523000717163\n",
      "Epoch 4394, Loss: 1.9139670729637146, Final Batch Loss: 0.5745015144348145\n",
      "Epoch 4395, Loss: 1.5126675963401794, Final Batch Loss: 0.33839765191078186\n",
      "Epoch 4396, Loss: 1.6515178978443146, Final Batch Loss: 0.28411418199539185\n",
      "Epoch 4397, Loss: 1.5974115133285522, Final Batch Loss: 0.2924441695213318\n",
      "Epoch 4398, Loss: 1.6178337633609772, Final Batch Loss: 0.3714495003223419\n",
      "Epoch 4399, Loss: 1.5269394777715206, Final Batch Loss: 0.02833324298262596\n",
      "Epoch 4400, Loss: 1.6786647140979767, Final Batch Loss: 0.2559927701950073\n",
      "Epoch 4401, Loss: 1.6677657961845398, Final Batch Loss: 0.33453625440597534\n",
      "Epoch 4402, Loss: 1.5969334244728088, Final Batch Loss: 0.39226651191711426\n",
      "Epoch 4403, Loss: 1.8032230734825134, Final Batch Loss: 0.40840819478034973\n",
      "Epoch 4404, Loss: 1.544220209121704, Final Batch Loss: 0.2791297137737274\n",
      "Epoch 4405, Loss: 1.6115914583206177, Final Batch Loss: 0.203788161277771\n",
      "Epoch 4406, Loss: 1.6190622746944427, Final Batch Loss: 0.2130272388458252\n",
      "Epoch 4407, Loss: 1.8859035819768906, Final Batch Loss: 0.702443540096283\n",
      "Epoch 4408, Loss: 1.5893036127090454, Final Batch Loss: 0.34951338171958923\n",
      "Epoch 4409, Loss: 1.746831476688385, Final Batch Loss: 0.31777337193489075\n",
      "Epoch 4410, Loss: 1.3987389355897903, Final Batch Loss: 0.12853313982486725\n",
      "Epoch 4411, Loss: 1.6268190741539001, Final Batch Loss: 0.23930540680885315\n",
      "Epoch 4412, Loss: 1.6015738248825073, Final Batch Loss: 0.2990286648273468\n",
      "Epoch 4413, Loss: 1.5732515901327133, Final Batch Loss: 0.22999541461467743\n",
      "Epoch 4414, Loss: 1.9366959929466248, Final Batch Loss: 0.3938281238079071\n",
      "Epoch 4415, Loss: 1.7470521032810211, Final Batch Loss: 0.3852355480194092\n",
      "Epoch 4416, Loss: 1.8548944890499115, Final Batch Loss: 0.5834165215492249\n",
      "Epoch 4417, Loss: 1.566061943769455, Final Batch Loss: 0.29180651903152466\n",
      "Epoch 4418, Loss: 1.4882851541042328, Final Batch Loss: 0.2792488634586334\n",
      "Epoch 4419, Loss: 1.746320754289627, Final Batch Loss: 0.4762970507144928\n",
      "Epoch 4420, Loss: 2.046629458665848, Final Batch Loss: 0.7849031090736389\n",
      "Epoch 4421, Loss: 1.7099089622497559, Final Batch Loss: 0.4569251835346222\n",
      "Epoch 4422, Loss: 1.4731781333684921, Final Batch Loss: 0.15649892389774323\n",
      "Epoch 4423, Loss: 1.592829406261444, Final Batch Loss: 0.2561626136302948\n",
      "Epoch 4424, Loss: 1.5091680884361267, Final Batch Loss: 0.28333792090415955\n",
      "Epoch 4425, Loss: 1.6892973482608795, Final Batch Loss: 0.3717793822288513\n",
      "Epoch 4426, Loss: 1.4393114894628525, Final Batch Loss: 0.09283880889415741\n",
      "Epoch 4427, Loss: 1.881184607744217, Final Batch Loss: 0.5531900525093079\n",
      "Epoch 4428, Loss: 1.9421762526035309, Final Batch Loss: 0.5399982333183289\n",
      "Epoch 4429, Loss: 1.6316809952259064, Final Batch Loss: 0.36267587542533875\n",
      "Epoch 4430, Loss: 1.6708777993917465, Final Batch Loss: 0.23314036428928375\n",
      "Epoch 4431, Loss: 1.6907065510749817, Final Batch Loss: 0.3286750316619873\n",
      "Epoch 4432, Loss: 2.63356551527977, Final Batch Loss: 1.1520313024520874\n",
      "Epoch 4433, Loss: 1.8651387393474579, Final Batch Loss: 0.5375317931175232\n",
      "Epoch 4434, Loss: 1.4561254680156708, Final Batch Loss: 0.13219308853149414\n",
      "Epoch 4435, Loss: 1.7550931870937347, Final Batch Loss: 0.43070441484451294\n",
      "Epoch 4436, Loss: 1.3771972432732582, Final Batch Loss: 0.08924973756074905\n",
      "Epoch 4437, Loss: 2.0534384548664093, Final Batch Loss: 0.7421548962593079\n",
      "Epoch 4438, Loss: 1.6145644783973694, Final Batch Loss: 0.3604222238063812\n",
      "Epoch 4439, Loss: 1.6080163419246674, Final Batch Loss: 0.2884484827518463\n",
      "Epoch 4440, Loss: 1.73321732878685, Final Batch Loss: 0.4786199927330017\n",
      "Epoch 4441, Loss: 1.4222782105207443, Final Batch Loss: 0.1605486124753952\n",
      "Epoch 4442, Loss: 1.444533348083496, Final Batch Loss: 0.1482463777065277\n",
      "Epoch 4443, Loss: 1.9035635590553284, Final Batch Loss: 0.6605266332626343\n",
      "Epoch 4444, Loss: 1.7236525267362595, Final Batch Loss: 0.4391095042228699\n",
      "Epoch 4445, Loss: 1.5845462679862976, Final Batch Loss: 0.3534236550331116\n",
      "Epoch 4446, Loss: 1.6961416602134705, Final Batch Loss: 0.31160783767700195\n",
      "Epoch 4447, Loss: 1.935673475265503, Final Batch Loss: 0.5233713984489441\n",
      "Epoch 4448, Loss: 1.5547322630882263, Final Batch Loss: 0.3011362552642822\n",
      "Epoch 4449, Loss: 1.4224104136228561, Final Batch Loss: 0.1725444346666336\n",
      "Epoch 4450, Loss: 1.4032669812440872, Final Batch Loss: 0.06319363415241241\n",
      "Epoch 4451, Loss: 1.7364870309829712, Final Batch Loss: 0.4365350604057312\n",
      "Epoch 4452, Loss: 1.4506022334098816, Final Batch Loss: 0.2877805233001709\n",
      "Epoch 4453, Loss: 1.530125916004181, Final Batch Loss: 0.2416534423828125\n",
      "Epoch 4454, Loss: 1.467625916004181, Final Batch Loss: 0.18575772643089294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4455, Loss: 1.6415332555770874, Final Batch Loss: 0.28583571314811707\n",
      "Epoch 4456, Loss: 1.5456819832324982, Final Batch Loss: 0.25916212797164917\n",
      "Epoch 4457, Loss: 1.5525126159191132, Final Batch Loss: 0.30650466680526733\n",
      "Epoch 4458, Loss: 1.9259413480758667, Final Batch Loss: 0.5619300603866577\n",
      "Epoch 4459, Loss: 1.6739206910133362, Final Batch Loss: 0.29121914505958557\n",
      "Epoch 4460, Loss: 1.654981255531311, Final Batch Loss: 0.34480807185173035\n",
      "Epoch 4461, Loss: 1.4954090118408203, Final Batch Loss: 0.2819056808948517\n",
      "Epoch 4462, Loss: 1.4244615212082863, Final Batch Loss: 0.08928731828927994\n",
      "Epoch 4463, Loss: 1.4047804474830627, Final Batch Loss: 0.148712158203125\n",
      "Epoch 4464, Loss: 1.5914576947689056, Final Batch Loss: 0.25508788228034973\n",
      "Epoch 4465, Loss: 1.9093294590711594, Final Batch Loss: 0.7347360849380493\n",
      "Epoch 4466, Loss: 1.552806556224823, Final Batch Loss: 0.19316259026527405\n",
      "Epoch 4467, Loss: 1.4963684976100922, Final Batch Loss: 0.08785104751586914\n",
      "Epoch 4468, Loss: 1.519377514719963, Final Batch Loss: 0.13114796578884125\n",
      "Epoch 4469, Loss: 1.8515527248382568, Final Batch Loss: 0.5111037492752075\n",
      "Epoch 4470, Loss: 2.2424131631851196, Final Batch Loss: 0.944475531578064\n",
      "Epoch 4471, Loss: 1.7183497548103333, Final Batch Loss: 0.46619144082069397\n",
      "Epoch 4472, Loss: 1.8952077627182007, Final Batch Loss: 0.6421272158622742\n",
      "Epoch 4473, Loss: 1.568726658821106, Final Batch Loss: 0.2484097182750702\n",
      "Epoch 4474, Loss: 1.9679778218269348, Final Batch Loss: 0.7120567560195923\n",
      "Epoch 4475, Loss: 1.3648950308561325, Final Batch Loss: 0.13091181218624115\n",
      "Epoch 4476, Loss: 1.4897928088903427, Final Batch Loss: 0.1638687402009964\n",
      "Epoch 4477, Loss: 1.712382584810257, Final Batch Loss: 0.350861519575119\n",
      "Epoch 4478, Loss: 1.4888406246900558, Final Batch Loss: 0.20146210491657257\n",
      "Epoch 4479, Loss: 1.4058613628149033, Final Batch Loss: 0.15492339432239532\n",
      "Epoch 4480, Loss: 1.8996024131774902, Final Batch Loss: 0.7316097617149353\n",
      "Epoch 4481, Loss: 1.564350202679634, Final Batch Loss: 0.2360672801733017\n",
      "Epoch 4482, Loss: 1.5347425043582916, Final Batch Loss: 0.17563870549201965\n",
      "Epoch 4483, Loss: 1.500152438879013, Final Batch Loss: 0.22242355346679688\n",
      "Epoch 4484, Loss: 1.571634441614151, Final Batch Loss: 0.2552300691604614\n",
      "Epoch 4485, Loss: 1.6115808486938477, Final Batch Loss: 0.23613858222961426\n",
      "Epoch 4486, Loss: 1.487753912806511, Final Batch Loss: 0.15732167661190033\n",
      "Epoch 4487, Loss: 1.4158939570188522, Final Batch Loss: 0.18125057220458984\n",
      "Epoch 4488, Loss: 1.4941316395998, Final Batch Loss: 0.24436669051647186\n",
      "Epoch 4489, Loss: 1.6034442782402039, Final Batch Loss: 0.34540027379989624\n",
      "Epoch 4490, Loss: 1.685398280620575, Final Batch Loss: 0.4471682906150818\n",
      "Epoch 4491, Loss: 1.5074771344661713, Final Batch Loss: 0.19484373927116394\n",
      "Epoch 4492, Loss: 1.4865490794181824, Final Batch Loss: 0.2269553244113922\n",
      "Epoch 4493, Loss: 1.3262579441070557, Final Batch Loss: 0.13046392798423767\n",
      "Epoch 4494, Loss: 1.274872563779354, Final Batch Loss: 0.09785177558660507\n",
      "Epoch 4495, Loss: 1.5270735919475555, Final Batch Loss: 0.2539962828159332\n",
      "Epoch 4496, Loss: 1.6300599575042725, Final Batch Loss: 0.2414085566997528\n",
      "Epoch 4497, Loss: 1.479330614209175, Final Batch Loss: 0.22466589510440826\n",
      "Epoch 4498, Loss: 1.637764036655426, Final Batch Loss: 0.43939492106437683\n",
      "Epoch 4499, Loss: 1.6718063354492188, Final Batch Loss: 0.28253164887428284\n",
      "Epoch 4500, Loss: 1.6264642775058746, Final Batch Loss: 0.4135919213294983\n",
      "Epoch 4501, Loss: 1.75511434674263, Final Batch Loss: 0.3745148777961731\n",
      "Epoch 4502, Loss: 1.562081590294838, Final Batch Loss: 0.18089722096920013\n",
      "Epoch 4503, Loss: 2.089926093816757, Final Batch Loss: 0.7089248299598694\n",
      "Epoch 4504, Loss: 1.595600962638855, Final Batch Loss: 0.35358431935310364\n",
      "Epoch 4505, Loss: 1.9190727174282074, Final Batch Loss: 0.6129630208015442\n",
      "Epoch 4506, Loss: 1.4321798011660576, Final Batch Loss: 0.12189381569623947\n",
      "Epoch 4507, Loss: 1.6786217391490936, Final Batch Loss: 0.35379543900489807\n",
      "Epoch 4508, Loss: 1.4327820017933846, Final Batch Loss: 0.12374424189329147\n",
      "Epoch 4509, Loss: 1.3557686917483807, Final Batch Loss: 0.05420234426856041\n",
      "Epoch 4510, Loss: 1.384499303996563, Final Batch Loss: 0.11608006805181503\n",
      "Epoch 4511, Loss: 1.823901891708374, Final Batch Loss: 0.5618610978126526\n",
      "Epoch 4512, Loss: 1.431341364979744, Final Batch Loss: 0.2276804894208908\n",
      "Epoch 4513, Loss: 1.3872958645224571, Final Batch Loss: 0.11558356136083603\n",
      "Epoch 4514, Loss: 1.5804697275161743, Final Batch Loss: 0.3432137072086334\n",
      "Epoch 4515, Loss: 1.510580100119114, Final Batch Loss: 0.08901415020227432\n",
      "Epoch 4516, Loss: 1.7128648161888123, Final Batch Loss: 0.3726873993873596\n",
      "Epoch 4517, Loss: 1.594548910856247, Final Batch Loss: 0.15030276775360107\n",
      "Epoch 4518, Loss: 1.9148388803005219, Final Batch Loss: 0.4927862286567688\n",
      "Epoch 4519, Loss: 1.6709580272436142, Final Batch Loss: 0.309034138917923\n",
      "Epoch 4520, Loss: 1.8225189745426178, Final Batch Loss: 0.4397209584712982\n",
      "Epoch 4521, Loss: 1.7302434742450714, Final Batch Loss: 0.4669002890586853\n",
      "Epoch 4522, Loss: 1.5916063636541367, Final Batch Loss: 0.22870700061321259\n",
      "Epoch 4523, Loss: 2.070712298154831, Final Batch Loss: 0.6017255783081055\n",
      "Epoch 4524, Loss: 1.6595175564289093, Final Batch Loss: 0.4069522023200989\n",
      "Epoch 4525, Loss: 1.555714726448059, Final Batch Loss: 0.27090272307395935\n",
      "Epoch 4526, Loss: 1.367999091744423, Final Batch Loss: 0.11152724921703339\n",
      "Epoch 4527, Loss: 1.4086754471063614, Final Batch Loss: 0.14408080279827118\n",
      "Epoch 4528, Loss: 1.7263996601104736, Final Batch Loss: 0.3417304456233978\n",
      "Epoch 4529, Loss: 1.6392980068922043, Final Batch Loss: 0.2385030835866928\n",
      "Epoch 4530, Loss: 1.847894698381424, Final Batch Loss: 0.3624042570590973\n",
      "Epoch 4531, Loss: 1.5730803310871124, Final Batch Loss: 0.31014227867126465\n",
      "Epoch 4532, Loss: 1.8365101218223572, Final Batch Loss: 0.43410277366638184\n",
      "Epoch 4533, Loss: 1.6729505956172943, Final Batch Loss: 0.3673850893974304\n",
      "Epoch 4534, Loss: 1.6086021065711975, Final Batch Loss: 0.2824702858924866\n",
      "Epoch 4535, Loss: 1.4051202982664108, Final Batch Loss: 0.19424419105052948\n",
      "Epoch 4536, Loss: 1.4285707622766495, Final Batch Loss: 0.12139193713665009\n",
      "Epoch 4537, Loss: 1.868638515472412, Final Batch Loss: 0.5845369696617126\n",
      "Epoch 4538, Loss: 1.5206744372844696, Final Batch Loss: 0.2621743977069855\n",
      "Epoch 4539, Loss: 1.3818857371807098, Final Batch Loss: 0.1952289640903473\n",
      "Epoch 4540, Loss: 1.7068045139312744, Final Batch Loss: 0.32257309556007385\n",
      "Epoch 4541, Loss: 1.4190032109618187, Final Batch Loss: 0.08111601322889328\n",
      "Epoch 4542, Loss: 1.5837297141551971, Final Batch Loss: 0.39369240403175354\n",
      "Epoch 4543, Loss: 1.4940242022275925, Final Batch Loss: 0.23132266104221344\n",
      "Epoch 4544, Loss: 2.135116845369339, Final Batch Loss: 0.7065597176551819\n",
      "Epoch 4545, Loss: 1.721180260181427, Final Batch Loss: 0.40537697076797485\n",
      "Epoch 4546, Loss: 1.5763283967971802, Final Batch Loss: 0.31789398193359375\n",
      "Epoch 4547, Loss: 1.3591774478554726, Final Batch Loss: 0.04862780123949051\n",
      "Epoch 4548, Loss: 1.6879330277442932, Final Batch Loss: 0.3519376218318939\n",
      "Epoch 4549, Loss: 1.6226662397384644, Final Batch Loss: 0.32807657122612\n",
      "Epoch 4550, Loss: 1.4504934698343277, Final Batch Loss: 0.21914894878864288\n",
      "Epoch 4551, Loss: 1.5411189198493958, Final Batch Loss: 0.17821884155273438\n",
      "Epoch 4552, Loss: 1.4282546639442444, Final Batch Loss: 0.09924298524856567\n",
      "Epoch 4553, Loss: 1.6008641719818115, Final Batch Loss: 0.31152215600013733\n",
      "Epoch 4554, Loss: 1.646760880947113, Final Batch Loss: 0.4228898882865906\n",
      "Epoch 4555, Loss: 1.7661197483539581, Final Batch Loss: 0.364575058221817\n",
      "Epoch 4556, Loss: 1.7164151072502136, Final Batch Loss: 0.2913398742675781\n",
      "Epoch 4557, Loss: 1.6050156652927399, Final Batch Loss: 0.3158873915672302\n",
      "Epoch 4558, Loss: 1.8255256116390228, Final Batch Loss: 0.5531672835350037\n",
      "Epoch 4559, Loss: 1.7413146197795868, Final Batch Loss: 0.36487340927124023\n",
      "Epoch 4560, Loss: 1.7114218473434448, Final Batch Loss: 0.3483857810497284\n",
      "Epoch 4561, Loss: 1.9312965869903564, Final Batch Loss: 0.6603972315788269\n",
      "Epoch 4562, Loss: 1.6663224399089813, Final Batch Loss: 0.32314321398735046\n",
      "Epoch 4563, Loss: 1.5248377174139023, Final Batch Loss: 0.24951623380184174\n",
      "Epoch 4564, Loss: 1.719253033399582, Final Batch Loss: 0.4146972596645355\n",
      "Epoch 4565, Loss: 1.9797708094120026, Final Batch Loss: 0.6188302040100098\n",
      "Epoch 4566, Loss: 1.6856554448604584, Final Batch Loss: 0.40865781903266907\n",
      "Epoch 4567, Loss: 1.7131387591362, Final Batch Loss: 0.5688349604606628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4568, Loss: 1.4512807242572308, Final Batch Loss: 0.044641297310590744\n",
      "Epoch 4569, Loss: 1.562732458114624, Final Batch Loss: 0.2816073000431061\n",
      "Epoch 4570, Loss: 1.6043932735919952, Final Batch Loss: 0.3859855532646179\n",
      "Epoch 4571, Loss: 1.5062057822942734, Final Batch Loss: 0.1983652561903\n",
      "Epoch 4572, Loss: 1.3764111995697021, Final Batch Loss: 0.2190345674753189\n",
      "Epoch 4573, Loss: 1.5068318247795105, Final Batch Loss: 0.2747897803783417\n",
      "Epoch 4574, Loss: 1.9133107662200928, Final Batch Loss: 0.577427089214325\n",
      "Epoch 4575, Loss: 1.5139685422182083, Final Batch Loss: 0.1985059231519699\n",
      "Epoch 4576, Loss: 1.5338909178972244, Final Batch Loss: 0.1766592413187027\n",
      "Epoch 4577, Loss: 1.5292581617832184, Final Batch Loss: 0.3275739550590515\n",
      "Epoch 4578, Loss: 1.7172591388225555, Final Batch Loss: 0.4002152383327484\n",
      "Epoch 4579, Loss: 1.4080354124307632, Final Batch Loss: 0.24298426508903503\n",
      "Epoch 4580, Loss: 1.5230454057455063, Final Batch Loss: 0.20771203935146332\n",
      "Epoch 4581, Loss: 1.344522051513195, Final Batch Loss: 0.06677599996328354\n",
      "Epoch 4582, Loss: 1.4209069162607193, Final Batch Loss: 0.1422388106584549\n",
      "Epoch 4583, Loss: 1.6960110366344452, Final Batch Loss: 0.4104017913341522\n",
      "Epoch 4584, Loss: 1.7085094451904297, Final Batch Loss: 0.486261785030365\n",
      "Epoch 4585, Loss: 1.787728101015091, Final Batch Loss: 0.5112594962120056\n",
      "Epoch 4586, Loss: 1.8089658617973328, Final Batch Loss: 0.44056084752082825\n",
      "Epoch 4587, Loss: 1.608875423669815, Final Batch Loss: 0.3297897279262543\n",
      "Epoch 4588, Loss: 1.4780061095952988, Final Batch Loss: 0.16363872587680817\n",
      "Epoch 4589, Loss: 1.6238575875759125, Final Batch Loss: 0.4266428053379059\n",
      "Epoch 4590, Loss: 1.6410896480083466, Final Batch Loss: 0.40248292684555054\n",
      "Epoch 4591, Loss: 1.4589519649744034, Final Batch Loss: 0.14537902176380157\n",
      "Epoch 4592, Loss: 1.6000586450099945, Final Batch Loss: 0.3509412109851837\n",
      "Epoch 4593, Loss: 1.4648127257823944, Final Batch Loss: 0.21565482020378113\n",
      "Epoch 4594, Loss: 1.368676871061325, Final Batch Loss: 0.19225147366523743\n",
      "Epoch 4595, Loss: 1.5814432054758072, Final Batch Loss: 0.21238170564174652\n",
      "Epoch 4596, Loss: 1.4191107749938965, Final Batch Loss: 0.13219836354255676\n",
      "Epoch 4597, Loss: 1.7967980802059174, Final Batch Loss: 0.4712657332420349\n",
      "Epoch 4598, Loss: 1.482821062207222, Final Batch Loss: 0.3340723216533661\n",
      "Epoch 4599, Loss: 1.6597700119018555, Final Batch Loss: 0.4275961220264435\n",
      "Epoch 4600, Loss: 1.6483744382858276, Final Batch Loss: 0.37320223450660706\n",
      "Epoch 4601, Loss: 1.7439322173595428, Final Batch Loss: 0.37111982703208923\n",
      "Epoch 4602, Loss: 1.9146713614463806, Final Batch Loss: 0.686003565788269\n",
      "Epoch 4603, Loss: 2.1620197892189026, Final Batch Loss: 0.7879218459129333\n",
      "Epoch 4604, Loss: 2.100661814212799, Final Batch Loss: 0.6478880047798157\n",
      "Epoch 4605, Loss: 1.5340473651885986, Final Batch Loss: 0.21994554996490479\n",
      "Epoch 4606, Loss: 1.7164945304393768, Final Batch Loss: 0.40290337800979614\n",
      "Epoch 4607, Loss: 1.738738477230072, Final Batch Loss: 0.536790132522583\n",
      "Epoch 4608, Loss: 1.5378057658672333, Final Batch Loss: 0.38848620653152466\n",
      "Epoch 4609, Loss: 1.5985642075538635, Final Batch Loss: 0.31927233934402466\n",
      "Epoch 4610, Loss: 1.7461613416671753, Final Batch Loss: 0.4426581561565399\n",
      "Epoch 4611, Loss: 2.0405020117759705, Final Batch Loss: 0.6979233622550964\n",
      "Epoch 4612, Loss: 1.7705008536577225, Final Batch Loss: 0.16993959248065948\n",
      "Epoch 4613, Loss: 1.4898298233747482, Final Batch Loss: 0.1467273086309433\n",
      "Epoch 4614, Loss: 1.5340991169214249, Final Batch Loss: 0.24010388553142548\n",
      "Epoch 4615, Loss: 2.0819757729768753, Final Batch Loss: 0.825420081615448\n",
      "Epoch 4616, Loss: 1.5861598253250122, Final Batch Loss: 0.22702673077583313\n",
      "Epoch 4617, Loss: 1.5622566044330597, Final Batch Loss: 0.1871224343776703\n",
      "Epoch 4618, Loss: 1.4253799766302109, Final Batch Loss: 0.13576652109622955\n",
      "Epoch 4619, Loss: 1.61380073428154, Final Batch Loss: 0.33792179822921753\n",
      "Epoch 4620, Loss: 1.570038378238678, Final Batch Loss: 0.3050047755241394\n",
      "Epoch 4621, Loss: 1.9654860198497772, Final Batch Loss: 0.563362181186676\n",
      "Epoch 4622, Loss: 1.3826559409499168, Final Batch Loss: 0.11049108952283859\n",
      "Epoch 4623, Loss: 1.6295682936906815, Final Batch Loss: 0.18564917147159576\n",
      "Epoch 4624, Loss: 1.476149007678032, Final Batch Loss: 0.14815013110637665\n",
      "Epoch 4625, Loss: 1.519665852189064, Final Batch Loss: 0.20924289524555206\n",
      "Epoch 4626, Loss: 1.8233492076396942, Final Batch Loss: 0.5498984456062317\n",
      "Epoch 4627, Loss: 1.693470299243927, Final Batch Loss: 0.23275411128997803\n",
      "Epoch 4628, Loss: 1.804774671792984, Final Batch Loss: 0.5169582962989807\n",
      "Epoch 4629, Loss: 2.297307640314102, Final Batch Loss: 1.0111876726150513\n",
      "Epoch 4630, Loss: 1.562321126461029, Final Batch Loss: 0.2692568004131317\n",
      "Epoch 4631, Loss: 2.2042321264743805, Final Batch Loss: 0.8305104970932007\n",
      "Epoch 4632, Loss: 1.5892012417316437, Final Batch Loss: 0.1856212317943573\n",
      "Epoch 4633, Loss: 2.3008416295051575, Final Batch Loss: 0.579257607460022\n",
      "Epoch 4634, Loss: 2.1613747775554657, Final Batch Loss: 0.4564830958843231\n",
      "Epoch 4635, Loss: 1.7247253209352493, Final Batch Loss: 0.23945944011211395\n",
      "Epoch 4636, Loss: 1.930323749780655, Final Batch Loss: 0.3735128939151764\n",
      "Epoch 4637, Loss: 1.8575127124786377, Final Batch Loss: 0.39240598678588867\n",
      "Epoch 4638, Loss: 1.794018030166626, Final Batch Loss: 0.2751479744911194\n",
      "Epoch 4639, Loss: 1.844797521829605, Final Batch Loss: 0.47262945771217346\n",
      "Epoch 4640, Loss: 1.7498788237571716, Final Batch Loss: 0.5134682655334473\n",
      "Epoch 4641, Loss: 1.5861474722623825, Final Batch Loss: 0.23682402074337006\n",
      "Epoch 4642, Loss: 1.750168651342392, Final Batch Loss: 0.4259176552295685\n",
      "Epoch 4643, Loss: 1.7103097140789032, Final Batch Loss: 0.3545176386833191\n",
      "Epoch 4644, Loss: 1.92630335688591, Final Batch Loss: 0.584599494934082\n",
      "Epoch 4645, Loss: 1.418886847794056, Final Batch Loss: 0.12406525760889053\n",
      "Epoch 4646, Loss: 1.7372173964977264, Final Batch Loss: 0.3101101815700531\n",
      "Epoch 4647, Loss: 1.387108489871025, Final Batch Loss: 0.16210009157657623\n",
      "Epoch 4648, Loss: 1.4783511757850647, Final Batch Loss: 0.23737919330596924\n",
      "Epoch 4649, Loss: 1.5330734848976135, Final Batch Loss: 0.25171521306037903\n",
      "Epoch 4650, Loss: 1.5001771599054337, Final Batch Loss: 0.22282351553440094\n",
      "Epoch 4651, Loss: 1.6002389043569565, Final Batch Loss: 0.31060531735420227\n",
      "Epoch 4652, Loss: 1.498328059911728, Final Batch Loss: 0.18282672762870789\n",
      "Epoch 4653, Loss: 1.3647392243146896, Final Batch Loss: 0.168661430478096\n",
      "Epoch 4654, Loss: 1.4890684336423874, Final Batch Loss: 0.17259515821933746\n",
      "Epoch 4655, Loss: 1.5258281081914902, Final Batch Loss: 0.2743697464466095\n",
      "Epoch 4656, Loss: 1.831181287765503, Final Batch Loss: 0.5102737545967102\n",
      "Epoch 4657, Loss: 1.3618793338537216, Final Batch Loss: 0.14678023755550385\n",
      "Epoch 4658, Loss: 1.8355427980422974, Final Batch Loss: 0.5605233907699585\n",
      "Epoch 4659, Loss: 1.7371000945568085, Final Batch Loss: 0.45975396037101746\n",
      "Epoch 4660, Loss: 1.7829713821411133, Final Batch Loss: 0.5150651931762695\n",
      "Epoch 4661, Loss: 2.3616788387298584, Final Batch Loss: 1.1157000064849854\n",
      "Epoch 4662, Loss: 1.722534418106079, Final Batch Loss: 0.3279268741607666\n",
      "Epoch 4663, Loss: 1.3146355990320444, Final Batch Loss: 0.022662723436951637\n",
      "Epoch 4664, Loss: 1.6619246304035187, Final Batch Loss: 0.3000836968421936\n",
      "Epoch 4665, Loss: 1.6575382351875305, Final Batch Loss: 0.4218657910823822\n",
      "Epoch 4666, Loss: 1.5794909596443176, Final Batch Loss: 0.332838773727417\n",
      "Epoch 4667, Loss: 1.8688279688358307, Final Batch Loss: 0.5586629509925842\n",
      "Epoch 4668, Loss: 1.4682658463716507, Final Batch Loss: 0.11980120837688446\n",
      "Epoch 4669, Loss: 1.8824374377727509, Final Batch Loss: 0.6746312975883484\n",
      "Epoch 4670, Loss: 1.6119895577430725, Final Batch Loss: 0.33810070157051086\n",
      "Epoch 4671, Loss: 1.5488267838954926, Final Batch Loss: 0.34922876954078674\n",
      "Epoch 4672, Loss: 1.5102094560861588, Final Batch Loss: 0.21660877764225006\n",
      "Epoch 4673, Loss: 1.7215318977832794, Final Batch Loss: 0.40871375799179077\n",
      "Epoch 4674, Loss: 1.3783152475953102, Final Batch Loss: 0.09684557467699051\n",
      "Epoch 4675, Loss: 1.6903259456157684, Final Batch Loss: 0.39225703477859497\n",
      "Epoch 4676, Loss: 1.5146265625953674, Final Batch Loss: 0.32957252860069275\n",
      "Epoch 4677, Loss: 1.4701676964759827, Final Batch Loss: 0.21524369716644287\n",
      "Epoch 4678, Loss: 1.4363099485635757, Final Batch Loss: 0.19662736356258392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4679, Loss: 1.5449349880218506, Final Batch Loss: 0.32533058524131775\n",
      "Epoch 4680, Loss: 1.6152018904685974, Final Batch Loss: 0.36603760719299316\n",
      "Epoch 4681, Loss: 1.6665715277194977, Final Batch Loss: 0.4073956310749054\n",
      "Epoch 4682, Loss: 1.294925644993782, Final Batch Loss: 0.03225244581699371\n",
      "Epoch 4683, Loss: 1.597668617963791, Final Batch Loss: 0.23228013515472412\n",
      "Epoch 4684, Loss: 1.8319109976291656, Final Batch Loss: 0.5289333462715149\n",
      "Epoch 4685, Loss: 1.512275606393814, Final Batch Loss: 0.31368523836135864\n",
      "Epoch 4686, Loss: 1.5144819170236588, Final Batch Loss: 0.2320026010274887\n",
      "Epoch 4687, Loss: 1.5483357161283493, Final Batch Loss: 0.30526915192604065\n",
      "Epoch 4688, Loss: 1.8015620410442352, Final Batch Loss: 0.46224245429039\n",
      "Epoch 4689, Loss: 1.3565364331007004, Final Batch Loss: 0.19972245395183563\n",
      "Epoch 4690, Loss: 1.7798129618167877, Final Batch Loss: 0.4151283800601959\n",
      "Epoch 4691, Loss: 1.318269396200776, Final Batch Loss: 0.03097330965101719\n",
      "Epoch 4692, Loss: 1.7652616798877716, Final Batch Loss: 0.2791331708431244\n",
      "Epoch 4693, Loss: 1.5077607929706573, Final Batch Loss: 0.24754086136817932\n",
      "Epoch 4694, Loss: 1.7767897546291351, Final Batch Loss: 0.43909305334091187\n",
      "Epoch 4695, Loss: 1.8414479494094849, Final Batch Loss: 0.5686500072479248\n",
      "Epoch 4696, Loss: 1.7757449448108673, Final Batch Loss: 0.36405983567237854\n",
      "Epoch 4697, Loss: 1.6558822989463806, Final Batch Loss: 0.32401150465011597\n",
      "Epoch 4698, Loss: 1.7948842942714691, Final Batch Loss: 0.48921385407447815\n",
      "Epoch 4699, Loss: 1.523037314414978, Final Batch Loss: 0.2603095471858978\n",
      "Epoch 4700, Loss: 1.5046231746673584, Final Batch Loss: 0.22292575240135193\n",
      "Epoch 4701, Loss: 1.5929200649261475, Final Batch Loss: 0.2737356722354889\n",
      "Epoch 4702, Loss: 1.4982087761163712, Final Batch Loss: 0.2300887107849121\n",
      "Epoch 4703, Loss: 1.3871208131313324, Final Batch Loss: 0.05040258169174194\n",
      "Epoch 4704, Loss: 2.634090304374695, Final Batch Loss: 0.9934478998184204\n",
      "Epoch 4705, Loss: 1.546862617135048, Final Batch Loss: 0.22692711651325226\n",
      "Epoch 4706, Loss: 1.768113613128662, Final Batch Loss: 0.4776288568973541\n",
      "Epoch 4707, Loss: 1.6861876845359802, Final Batch Loss: 0.34218400716781616\n",
      "Epoch 4708, Loss: 1.6088367700576782, Final Batch Loss: 0.18457937240600586\n",
      "Epoch 4709, Loss: 1.8106534481048584, Final Batch Loss: 0.3241274952888489\n",
      "Epoch 4710, Loss: 1.6896627247333527, Final Batch Loss: 0.2776332497596741\n",
      "Epoch 4711, Loss: 1.851354569196701, Final Batch Loss: 0.4603275656700134\n",
      "Epoch 4712, Loss: 1.8101799488067627, Final Batch Loss: 0.47081753611564636\n",
      "Epoch 4713, Loss: 1.4314180463552475, Final Batch Loss: 0.10571195185184479\n",
      "Epoch 4714, Loss: 1.6939829587936401, Final Batch Loss: 0.3885488212108612\n",
      "Epoch 4715, Loss: 1.4105914235115051, Final Batch Loss: 0.06507909297943115\n",
      "Epoch 4716, Loss: 1.650250867009163, Final Batch Loss: 0.4476025700569153\n",
      "Epoch 4717, Loss: 1.6319949626922607, Final Batch Loss: 0.36089056730270386\n",
      "Epoch 4718, Loss: 1.699863225221634, Final Batch Loss: 0.45292210578918457\n",
      "Epoch 4719, Loss: 1.6300404369831085, Final Batch Loss: 0.46319955587387085\n",
      "Epoch 4720, Loss: 1.6771075129508972, Final Batch Loss: 0.3583686947822571\n",
      "Epoch 4721, Loss: 1.6892680525779724, Final Batch Loss: 0.3574889600276947\n",
      "Epoch 4722, Loss: 1.4882271140813828, Final Batch Loss: 0.14972399175167084\n",
      "Epoch 4723, Loss: 1.457903653383255, Final Batch Loss: 0.1753777265548706\n",
      "Epoch 4724, Loss: 1.7587279379367828, Final Batch Loss: 0.5155143141746521\n",
      "Epoch 4725, Loss: 1.6051625609397888, Final Batch Loss: 0.4151175022125244\n",
      "Epoch 4726, Loss: 2.1183687448501587, Final Batch Loss: 0.6254405379295349\n",
      "Epoch 4727, Loss: 1.4211183339357376, Final Batch Loss: 0.1293341964483261\n",
      "Epoch 4728, Loss: 1.5693138539791107, Final Batch Loss: 0.3348126709461212\n",
      "Epoch 4729, Loss: 1.815294086933136, Final Batch Loss: 0.48784881830215454\n",
      "Epoch 4730, Loss: 1.3919364959001541, Final Batch Loss: 0.17884095013141632\n",
      "Epoch 4731, Loss: 1.790877789258957, Final Batch Loss: 0.45243075489997864\n",
      "Epoch 4732, Loss: 1.4160292446613312, Final Batch Loss: 0.21207889914512634\n",
      "Epoch 4733, Loss: 1.4555867165327072, Final Batch Loss: 0.20410139858722687\n",
      "Epoch 4734, Loss: 1.657180279493332, Final Batch Loss: 0.3591589033603668\n",
      "Epoch 4735, Loss: 1.7501349449157715, Final Batch Loss: 0.5220149755477905\n",
      "Epoch 4736, Loss: 1.5899190604686737, Final Batch Loss: 0.3393511474132538\n",
      "Epoch 4737, Loss: 2.062161773443222, Final Batch Loss: 0.7581853866577148\n",
      "Epoch 4738, Loss: 1.6949564516544342, Final Batch Loss: 0.29848867654800415\n",
      "Epoch 4739, Loss: 1.5937498807907104, Final Batch Loss: 0.27803125977516174\n",
      "Epoch 4740, Loss: 1.4356103390455246, Final Batch Loss: 0.13327060639858246\n",
      "Epoch 4741, Loss: 1.606143057346344, Final Batch Loss: 0.2614869773387909\n",
      "Epoch 4742, Loss: 1.4418478161096573, Final Batch Loss: 0.10587696731090546\n",
      "Epoch 4743, Loss: 2.0352875888347626, Final Batch Loss: 0.6602858304977417\n",
      "Epoch 4744, Loss: 1.8693731129169464, Final Batch Loss: 0.5311071276664734\n",
      "Epoch 4745, Loss: 1.5873764753341675, Final Batch Loss: 0.32608890533447266\n",
      "Epoch 4746, Loss: 1.8692651838064194, Final Batch Loss: 0.5755439400672913\n",
      "Epoch 4747, Loss: 1.5861887633800507, Final Batch Loss: 0.20967260003089905\n",
      "Epoch 4748, Loss: 1.6221642792224884, Final Batch Loss: 0.37215378880500793\n",
      "Epoch 4749, Loss: 1.6302469968795776, Final Batch Loss: 0.2737204134464264\n",
      "Epoch 4750, Loss: 2.2321887016296387, Final Batch Loss: 0.8195029497146606\n",
      "Epoch 4751, Loss: 1.47653329372406, Final Batch Loss: 0.18468740582466125\n",
      "Epoch 4752, Loss: 1.6692306399345398, Final Batch Loss: 0.4008972644805908\n",
      "Epoch 4753, Loss: 1.5301398187875748, Final Batch Loss: 0.21244843304157257\n",
      "Epoch 4754, Loss: 1.6248616576194763, Final Batch Loss: 0.34289970993995667\n",
      "Epoch 4755, Loss: 1.8185924291610718, Final Batch Loss: 0.4470885694026947\n",
      "Epoch 4756, Loss: 1.4919231832027435, Final Batch Loss: 0.16974690556526184\n",
      "Epoch 4757, Loss: 1.851671427488327, Final Batch Loss: 0.612461268901825\n",
      "Epoch 4758, Loss: 1.3848798871040344, Final Batch Loss: 0.1209031343460083\n",
      "Epoch 4759, Loss: 1.7192657589912415, Final Batch Loss: 0.4547240436077118\n",
      "Epoch 4760, Loss: 1.4994891211390495, Final Batch Loss: 0.12006708234548569\n",
      "Epoch 4761, Loss: 1.3547821752727032, Final Batch Loss: 0.020958315581083298\n",
      "Epoch 4762, Loss: 1.7471031248569489, Final Batch Loss: 0.4290011525154114\n",
      "Epoch 4763, Loss: 2.2175722420215607, Final Batch Loss: 0.9844458699226379\n",
      "Epoch 4764, Loss: 1.5211935341358185, Final Batch Loss: 0.16206666827201843\n",
      "Epoch 4765, Loss: 2.0904808938503265, Final Batch Loss: 0.5517692565917969\n",
      "Epoch 4766, Loss: 1.5179634541273117, Final Batch Loss: 0.16413240134716034\n",
      "Epoch 4767, Loss: 1.6226805001497269, Final Batch Loss: 0.31013980507850647\n",
      "Epoch 4768, Loss: 1.6820087879896164, Final Batch Loss: 0.23837770521640778\n",
      "Epoch 4769, Loss: 1.8705139458179474, Final Batch Loss: 0.5526094436645508\n",
      "Epoch 4770, Loss: 1.5430892556905746, Final Batch Loss: 0.23350362479686737\n",
      "Epoch 4771, Loss: 1.59156733751297, Final Batch Loss: 0.28327009081840515\n",
      "Epoch 4772, Loss: 1.5824940204620361, Final Batch Loss: 0.2543017864227295\n",
      "Epoch 4773, Loss: 1.7000098526477814, Final Batch Loss: 0.45632097125053406\n",
      "Epoch 4774, Loss: 1.5614250898361206, Final Batch Loss: 0.2965368330478668\n",
      "Epoch 4775, Loss: 1.5629687309265137, Final Batch Loss: 0.33251234889030457\n",
      "Epoch 4776, Loss: 1.5333926677703857, Final Batch Loss: 0.3483552038669586\n",
      "Epoch 4777, Loss: 1.9337248802185059, Final Batch Loss: 0.628743052482605\n",
      "Epoch 4778, Loss: 1.4854137003421783, Final Batch Loss: 0.2569655179977417\n",
      "Epoch 4779, Loss: 1.9147634208202362, Final Batch Loss: 0.6384282112121582\n",
      "Epoch 4780, Loss: 2.4514653384685516, Final Batch Loss: 0.9443206191062927\n",
      "Epoch 4781, Loss: 1.647682636976242, Final Batch Loss: 0.26305004954338074\n",
      "Epoch 4782, Loss: 1.3579223304986954, Final Batch Loss: 0.15341706573963165\n",
      "Epoch 4783, Loss: 1.5228832364082336, Final Batch Loss: 0.21076706051826477\n",
      "Epoch 4784, Loss: 1.7275802195072174, Final Batch Loss: 0.3723348081111908\n",
      "Epoch 4785, Loss: 2.006558060646057, Final Batch Loss: 0.6790415048599243\n",
      "Epoch 4786, Loss: 1.4932551383972168, Final Batch Loss: 0.29333236813545227\n",
      "Epoch 4787, Loss: 1.6900907158851624, Final Batch Loss: 0.31233444809913635\n",
      "Epoch 4788, Loss: 1.5335181057453156, Final Batch Loss: 0.18005597591400146\n",
      "Epoch 4789, Loss: 1.7243694067001343, Final Batch Loss: 0.38775724172592163\n",
      "Epoch 4790, Loss: 2.35949444770813, Final Batch Loss: 1.0996824502944946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4791, Loss: 1.9750336110591888, Final Batch Loss: 0.6061705946922302\n",
      "Epoch 4792, Loss: 2.4202335476875305, Final Batch Loss: 0.8987671732902527\n",
      "Epoch 4793, Loss: 1.7221658527851105, Final Batch Loss: 0.41965585947036743\n",
      "Epoch 4794, Loss: 1.6300819218158722, Final Batch Loss: 0.2903015911579132\n",
      "Epoch 4795, Loss: 2.0425597727298737, Final Batch Loss: 0.6524918675422668\n",
      "Epoch 4796, Loss: 1.45322285592556, Final Batch Loss: 0.14071668684482574\n",
      "Epoch 4797, Loss: 1.6978724896907806, Final Batch Loss: 0.2582235038280487\n",
      "Epoch 4798, Loss: 1.52157923579216, Final Batch Loss: 0.22068260610103607\n",
      "Epoch 4799, Loss: 1.6273585110902786, Final Batch Loss: 0.2266339808702469\n",
      "Epoch 4800, Loss: 1.594935268163681, Final Batch Loss: 0.19080772995948792\n",
      "Epoch 4801, Loss: 1.410685956478119, Final Batch Loss: 0.24664363265037537\n",
      "Epoch 4802, Loss: 1.8409893214702606, Final Batch Loss: 0.45838722586631775\n",
      "Epoch 4803, Loss: 1.8243130445480347, Final Batch Loss: 0.4728385806083679\n",
      "Epoch 4804, Loss: 1.5092568397521973, Final Batch Loss: 0.17962908744812012\n",
      "Epoch 4805, Loss: 1.52738618850708, Final Batch Loss: 0.16680356860160828\n",
      "Epoch 4806, Loss: 1.644730031490326, Final Batch Loss: 0.3691543936729431\n",
      "Epoch 4807, Loss: 1.722786158323288, Final Batch Loss: 0.37955862283706665\n",
      "Epoch 4808, Loss: 1.4258010387420654, Final Batch Loss: 0.17976781725883484\n",
      "Epoch 4809, Loss: 1.621713936328888, Final Batch Loss: 0.3528987765312195\n",
      "Epoch 4810, Loss: 1.7772579789161682, Final Batch Loss: 0.5374534726142883\n",
      "Epoch 4811, Loss: 1.6475812792778015, Final Batch Loss: 0.3719480037689209\n",
      "Epoch 4812, Loss: 1.4695825576782227, Final Batch Loss: 0.286266028881073\n",
      "Epoch 4813, Loss: 1.2918465919792652, Final Batch Loss: 0.046120960265398026\n",
      "Epoch 4814, Loss: 1.6641593277454376, Final Batch Loss: 0.31932333111763\n",
      "Epoch 4815, Loss: 1.2214653007686138, Final Batch Loss: 0.05224386975169182\n",
      "Epoch 4816, Loss: 1.5753793120384216, Final Batch Loss: 0.3914962112903595\n",
      "Epoch 4817, Loss: 1.6808549761772156, Final Batch Loss: 0.4485529065132141\n",
      "Epoch 4818, Loss: 1.3614184707403183, Final Batch Loss: 0.18797393143177032\n",
      "Epoch 4819, Loss: 1.3948043882846832, Final Batch Loss: 0.1511031687259674\n",
      "Epoch 4820, Loss: 2.1333388090133667, Final Batch Loss: 0.862187385559082\n",
      "Epoch 4821, Loss: 1.6101207584142685, Final Batch Loss: 0.3910380005836487\n",
      "Epoch 4822, Loss: 1.592907726764679, Final Batch Loss: 0.22409605979919434\n",
      "Epoch 4823, Loss: 1.4638230353593826, Final Batch Loss: 0.17816193401813507\n",
      "Epoch 4824, Loss: 1.7762309610843658, Final Batch Loss: 0.4169934093952179\n",
      "Epoch 4825, Loss: 1.5990540981292725, Final Batch Loss: 0.3522081673145294\n",
      "Epoch 4826, Loss: 1.5805711448192596, Final Batch Loss: 0.22401869297027588\n",
      "Epoch 4827, Loss: 1.6999011039733887, Final Batch Loss: 0.38542822003364563\n",
      "Epoch 4828, Loss: 1.4315445572137833, Final Batch Loss: 0.36152246594429016\n",
      "Epoch 4829, Loss: 1.5356432497501373, Final Batch Loss: 0.20443105697631836\n",
      "Epoch 4830, Loss: 1.8373572528362274, Final Batch Loss: 0.5657326579093933\n",
      "Epoch 4831, Loss: 1.6771763265132904, Final Batch Loss: 0.3654206693172455\n",
      "Epoch 4832, Loss: 1.777226835489273, Final Batch Loss: 0.5434967875480652\n",
      "Epoch 4833, Loss: 1.651652604341507, Final Batch Loss: 0.39752861857414246\n",
      "Epoch 4834, Loss: 1.845886379480362, Final Batch Loss: 0.5661505460739136\n",
      "Epoch 4835, Loss: 1.4921665042638779, Final Batch Loss: 0.1973189264535904\n",
      "Epoch 4836, Loss: 1.4918718039989471, Final Batch Loss: 0.2957589030265808\n",
      "Epoch 4837, Loss: 1.503187894821167, Final Batch Loss: 0.2748158574104309\n",
      "Epoch 4838, Loss: 1.3106144964694977, Final Batch Loss: 0.03999394178390503\n",
      "Epoch 4839, Loss: 1.4030060172080994, Final Batch Loss: 0.22610019147396088\n",
      "Epoch 4840, Loss: 1.4631437063217163, Final Batch Loss: 0.3191607594490051\n",
      "Epoch 4841, Loss: 1.500236988067627, Final Batch Loss: 0.28264644742012024\n",
      "Epoch 4842, Loss: 1.4749706238508224, Final Batch Loss: 0.23570895195007324\n",
      "Epoch 4843, Loss: 1.333282694220543, Final Batch Loss: 0.12621106207370758\n",
      "Epoch 4844, Loss: 1.6009838283061981, Final Batch Loss: 0.278045654296875\n",
      "Epoch 4845, Loss: 1.526863619685173, Final Batch Loss: 0.231178417801857\n",
      "Epoch 4846, Loss: 1.3393297344446182, Final Batch Loss: 0.06303177773952484\n",
      "Epoch 4847, Loss: 1.4537368267774582, Final Batch Loss: 0.18318983912467957\n",
      "Epoch 4848, Loss: 1.7939221560955048, Final Batch Loss: 0.626698911190033\n",
      "Epoch 4849, Loss: 1.5931276381015778, Final Batch Loss: 0.40795016288757324\n",
      "Epoch 4850, Loss: 1.4734236299991608, Final Batch Loss: 0.2842143177986145\n",
      "Epoch 4851, Loss: 1.2757966592907906, Final Batch Loss: 0.12256208807229996\n",
      "Epoch 4852, Loss: 1.5151934623718262, Final Batch Loss: 0.26083454489707947\n",
      "Epoch 4853, Loss: 1.806404709815979, Final Batch Loss: 0.521248996257782\n",
      "Epoch 4854, Loss: 1.4532664865255356, Final Batch Loss: 0.29076775908470154\n",
      "Epoch 4855, Loss: 1.7876278758049011, Final Batch Loss: 0.45080843567848206\n",
      "Epoch 4856, Loss: 1.6613779067993164, Final Batch Loss: 0.42642709612846375\n",
      "Epoch 4857, Loss: 1.7005694508552551, Final Batch Loss: 0.5082559585571289\n",
      "Epoch 4858, Loss: 1.5974292755126953, Final Batch Loss: 0.3069380223751068\n",
      "Epoch 4859, Loss: 1.474866360425949, Final Batch Loss: 0.1811748445034027\n",
      "Epoch 4860, Loss: 1.4846522510051727, Final Batch Loss: 0.21329805254936218\n",
      "Epoch 4861, Loss: 1.5438517928123474, Final Batch Loss: 0.2712504267692566\n",
      "Epoch 4862, Loss: 1.5605098009109497, Final Batch Loss: 0.22059974074363708\n",
      "Epoch 4863, Loss: 1.616591066122055, Final Batch Loss: 0.3146957755088806\n",
      "Epoch 4864, Loss: 1.5337996780872345, Final Batch Loss: 0.3180360794067383\n",
      "Epoch 4865, Loss: 1.502230778336525, Final Batch Loss: 0.23873236775398254\n",
      "Epoch 4866, Loss: 1.374817207455635, Final Batch Loss: 0.11883299052715302\n",
      "Epoch 4867, Loss: 1.563220888376236, Final Batch Loss: 0.308462530374527\n",
      "Epoch 4868, Loss: 1.796082764863968, Final Batch Loss: 0.6432210803031921\n",
      "Epoch 4869, Loss: 1.7282076478004456, Final Batch Loss: 0.503874659538269\n",
      "Epoch 4870, Loss: 1.4497783184051514, Final Batch Loss: 0.16365566849708557\n",
      "Epoch 4871, Loss: 2.074430286884308, Final Batch Loss: 0.7862971425056458\n",
      "Epoch 4872, Loss: 1.490202471613884, Final Batch Loss: 0.24203559756278992\n",
      "Epoch 4873, Loss: 1.3388811945915222, Final Batch Loss: 0.11417961120605469\n",
      "Epoch 4874, Loss: 1.6992176473140717, Final Batch Loss: 0.5211004614830017\n",
      "Epoch 4875, Loss: 1.240826390683651, Final Batch Loss: 0.1102064773440361\n",
      "Epoch 4876, Loss: 1.3776645958423615, Final Batch Loss: 0.13425084948539734\n",
      "Epoch 4877, Loss: 1.6261969208717346, Final Batch Loss: 0.2949715554714203\n",
      "Epoch 4878, Loss: 1.4201571643352509, Final Batch Loss: 0.21543458104133606\n",
      "Epoch 4879, Loss: 1.4710967689752579, Final Batch Loss: 0.2041284292936325\n",
      "Epoch 4880, Loss: 1.3883301466703415, Final Batch Loss: 0.16159798204898834\n",
      "Epoch 4881, Loss: 1.473563626408577, Final Batch Loss: 0.18498699367046356\n",
      "Epoch 4882, Loss: 1.3482634276151657, Final Batch Loss: 0.1264553815126419\n",
      "Epoch 4883, Loss: 1.3204677253961563, Final Batch Loss: 0.1625872105360031\n",
      "Epoch 4884, Loss: 1.3376298025250435, Final Batch Loss: 0.10158327966928482\n",
      "Epoch 4885, Loss: 1.3803196102380753, Final Batch Loss: 0.10766912996768951\n",
      "Epoch 4886, Loss: 2.2756657004356384, Final Batch Loss: 1.0914374589920044\n",
      "Epoch 4887, Loss: 1.7970179319381714, Final Batch Loss: 0.4173426032066345\n",
      "Epoch 4888, Loss: 1.9993262588977814, Final Batch Loss: 0.5943711400032043\n",
      "Epoch 4889, Loss: 1.7890715450048447, Final Batch Loss: 0.16418634355068207\n",
      "Epoch 4890, Loss: 1.7315064519643784, Final Batch Loss: 0.22899620234966278\n",
      "Epoch 4891, Loss: 1.584981992840767, Final Batch Loss: 0.24351461231708527\n",
      "Epoch 4892, Loss: 1.5691752135753632, Final Batch Loss: 0.3241978585720062\n",
      "Epoch 4893, Loss: 1.6826155483722687, Final Batch Loss: 0.4175061583518982\n",
      "Epoch 4894, Loss: 1.6836395263671875, Final Batch Loss: 0.33553627133369446\n",
      "Epoch 4895, Loss: 1.6244331300258636, Final Batch Loss: 0.3805381953716278\n",
      "Epoch 4896, Loss: 1.6898919641971588, Final Batch Loss: 0.32304757833480835\n",
      "Epoch 4897, Loss: 1.6521068215370178, Final Batch Loss: 0.4019884765148163\n",
      "Epoch 4898, Loss: 1.6818058490753174, Final Batch Loss: 0.28864923119544983\n",
      "Epoch 4899, Loss: 1.7071028351783752, Final Batch Loss: 0.44909176230430603\n",
      "Epoch 4900, Loss: 1.3507769405841827, Final Batch Loss: 0.2302885353565216\n",
      "Epoch 4901, Loss: 1.855531483888626, Final Batch Loss: 0.5839338302612305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4902, Loss: 1.7427094280719757, Final Batch Loss: 0.4592389166355133\n",
      "Epoch 4903, Loss: 2.1786637604236603, Final Batch Loss: 0.6155887246131897\n",
      "Epoch 4904, Loss: 1.495793953537941, Final Batch Loss: 0.19603784382343292\n",
      "Epoch 4905, Loss: 1.5134776383638382, Final Batch Loss: 0.22791288793087006\n",
      "Epoch 4906, Loss: 1.7119382619857788, Final Batch Loss: 0.4171786308288574\n",
      "Epoch 4907, Loss: 1.5493252724409103, Final Batch Loss: 0.13613303005695343\n",
      "Epoch 4908, Loss: 1.3542420640587807, Final Batch Loss: 0.1189814880490303\n",
      "Epoch 4909, Loss: 1.3547870479524136, Final Batch Loss: 0.020308073610067368\n",
      "Epoch 4910, Loss: 1.8660517632961273, Final Batch Loss: 0.6346325278282166\n",
      "Epoch 4911, Loss: 1.5452356338500977, Final Batch Loss: 0.2001553475856781\n",
      "Epoch 4912, Loss: 1.5802464187145233, Final Batch Loss: 0.15559569001197815\n",
      "Epoch 4913, Loss: 1.44336998462677, Final Batch Loss: 0.22538116574287415\n",
      "Epoch 4914, Loss: 1.6885794699192047, Final Batch Loss: 0.42091602087020874\n",
      "Epoch 4915, Loss: 1.6812639832496643, Final Batch Loss: 0.44051438570022583\n",
      "Epoch 4916, Loss: 1.6405852138996124, Final Batch Loss: 0.2594447731971741\n",
      "Epoch 4917, Loss: 1.4025583416223526, Final Batch Loss: 0.16492979228496552\n",
      "Epoch 4918, Loss: 1.5079702883958817, Final Batch Loss: 0.15102680027484894\n",
      "Epoch 4919, Loss: 1.6182500123977661, Final Batch Loss: 0.42114022374153137\n",
      "Epoch 4920, Loss: 1.6372180581092834, Final Batch Loss: 0.3099690079689026\n",
      "Epoch 4921, Loss: 1.3210063558071852, Final Batch Loss: 0.030695704743266106\n",
      "Epoch 4922, Loss: 1.5619416236877441, Final Batch Loss: 0.3322570025920868\n",
      "Epoch 4923, Loss: 1.3046656996011734, Final Batch Loss: 0.16103841364383698\n",
      "Epoch 4924, Loss: 1.3293627463281155, Final Batch Loss: 0.05306142196059227\n",
      "Epoch 4925, Loss: 1.5695666819810867, Final Batch Loss: 0.38466542959213257\n",
      "Epoch 4926, Loss: 1.681308090686798, Final Batch Loss: 0.4896076023578644\n",
      "Epoch 4927, Loss: 2.1120864748954773, Final Batch Loss: 0.826463520526886\n",
      "Epoch 4928, Loss: 1.7752667963504791, Final Batch Loss: 0.44474703073501587\n",
      "Epoch 4929, Loss: 1.8867251574993134, Final Batch Loss: 0.7656740546226501\n",
      "Epoch 4930, Loss: 1.5904849469661713, Final Batch Loss: 0.426232248544693\n",
      "Epoch 4931, Loss: 1.5579461753368378, Final Batch Loss: 0.2959058880805969\n",
      "Epoch 4932, Loss: 1.2197562456130981, Final Batch Loss: 0.08952619135379791\n",
      "Epoch 4933, Loss: 1.5647588968276978, Final Batch Loss: 0.45571333169937134\n",
      "Epoch 4934, Loss: 1.3621947169303894, Final Batch Loss: 0.16621734201908112\n",
      "Epoch 4935, Loss: 1.5365244150161743, Final Batch Loss: 0.21035310626029968\n",
      "Epoch 4936, Loss: 1.396877482533455, Final Batch Loss: 0.10714118182659149\n",
      "Epoch 4937, Loss: 1.5409667491912842, Final Batch Loss: 0.39881056547164917\n",
      "Epoch 4938, Loss: 1.81390181183815, Final Batch Loss: 0.6084398627281189\n",
      "Epoch 4939, Loss: 1.214073233306408, Final Batch Loss: 0.09995516389608383\n",
      "Epoch 4940, Loss: 1.4569449573755264, Final Batch Loss: 0.1510380357503891\n",
      "Epoch 4941, Loss: 1.3410507142543793, Final Batch Loss: 0.15193983912467957\n",
      "Epoch 4942, Loss: 1.58340784907341, Final Batch Loss: 0.2734220325946808\n",
      "Epoch 4943, Loss: 1.590697854757309, Final Batch Loss: 0.4335573613643646\n",
      "Epoch 4944, Loss: 2.163659989833832, Final Batch Loss: 0.9823594689369202\n",
      "Epoch 4945, Loss: 1.4209250211715698, Final Batch Loss: 0.2164762318134308\n",
      "Epoch 4946, Loss: 1.5077870786190033, Final Batch Loss: 0.2790740430355072\n",
      "Epoch 4947, Loss: 1.6003609001636505, Final Batch Loss: 0.3976748585700989\n",
      "Epoch 4948, Loss: 1.4306847229599953, Final Batch Loss: 0.06053880602121353\n",
      "Epoch 4949, Loss: 1.9271591901779175, Final Batch Loss: 0.4803776144981384\n",
      "Epoch 4950, Loss: 1.6955008804798126, Final Batch Loss: 0.20450928807258606\n",
      "Epoch 4951, Loss: 1.4454996213316917, Final Batch Loss: 0.11671989411115646\n",
      "Epoch 4952, Loss: 1.81315016746521, Final Batch Loss: 0.49556151032447815\n",
      "Epoch 4953, Loss: 1.8057204484939575, Final Batch Loss: 0.43623432517051697\n",
      "Epoch 4954, Loss: 1.5942985266447067, Final Batch Loss: 0.23926259577274323\n",
      "Epoch 4955, Loss: 1.7857607305049896, Final Batch Loss: 0.5111994743347168\n",
      "Epoch 4956, Loss: 1.3884047605097294, Final Batch Loss: 0.045388225466012955\n",
      "Epoch 4957, Loss: 1.637838825583458, Final Batch Loss: 0.4371799826622009\n",
      "Epoch 4958, Loss: 1.7223166823387146, Final Batch Loss: 0.4951370656490326\n",
      "Epoch 4959, Loss: 1.7423226833343506, Final Batch Loss: 0.45029130578041077\n",
      "Epoch 4960, Loss: 1.6673117578029633, Final Batch Loss: 0.39841288328170776\n",
      "Epoch 4961, Loss: 1.3000667244195938, Final Batch Loss: 0.1443011313676834\n",
      "Epoch 4962, Loss: 1.361443966627121, Final Batch Loss: 0.15676072239875793\n",
      "Epoch 4963, Loss: 1.3520236983895302, Final Batch Loss: 0.10499844700098038\n",
      "Epoch 4964, Loss: 1.4750481992959976, Final Batch Loss: 0.19534428417682648\n",
      "Epoch 4965, Loss: 1.5220873355865479, Final Batch Loss: 0.2753142714500427\n",
      "Epoch 4966, Loss: 1.673146665096283, Final Batch Loss: 0.39558154344558716\n",
      "Epoch 4967, Loss: 1.594653993844986, Final Batch Loss: 0.3955019414424896\n",
      "Epoch 4968, Loss: 1.8268954157829285, Final Batch Loss: 0.529060959815979\n",
      "Epoch 4969, Loss: 1.384766310453415, Final Batch Loss: 0.2451774775981903\n",
      "Epoch 4970, Loss: 1.6265588849782944, Final Batch Loss: 0.3056473135948181\n",
      "Epoch 4971, Loss: 1.7690508663654327, Final Batch Loss: 0.3384351432323456\n",
      "Epoch 4972, Loss: 1.7922987639904022, Final Batch Loss: 0.4805928170681\n",
      "Epoch 4973, Loss: 1.7091472446918488, Final Batch Loss: 0.3172491490840912\n",
      "Epoch 4974, Loss: 1.7323450148105621, Final Batch Loss: 0.38571181893348694\n",
      "Epoch 4975, Loss: 1.3945903442800045, Final Batch Loss: 0.03328215703368187\n",
      "Epoch 4976, Loss: 1.7722482681274414, Final Batch Loss: 0.49470824003219604\n",
      "Epoch 4977, Loss: 1.794458419084549, Final Batch Loss: 0.5607045292854309\n",
      "Epoch 4978, Loss: 1.702361822128296, Final Batch Loss: 0.4746232032775879\n",
      "Epoch 4979, Loss: 1.529098093509674, Final Batch Loss: 0.34129688143730164\n",
      "Epoch 4980, Loss: 1.3950395518913865, Final Batch Loss: 0.012238495983183384\n",
      "Epoch 4981, Loss: 1.4707909226417542, Final Batch Loss: 0.1804267168045044\n",
      "Epoch 4982, Loss: 1.3740734681487083, Final Batch Loss: 0.09557687491178513\n",
      "Epoch 4983, Loss: 1.2517099305987358, Final Batch Loss: 0.0805731937289238\n",
      "Epoch 4984, Loss: 1.8429079949855804, Final Batch Loss: 0.653795599937439\n",
      "Epoch 4985, Loss: 1.875911921262741, Final Batch Loss: 0.5058014988899231\n",
      "Epoch 4986, Loss: 1.721093863248825, Final Batch Loss: 0.3954910337924957\n",
      "Epoch 4987, Loss: 1.4794938117265701, Final Batch Loss: 0.15315814316272736\n",
      "Epoch 4988, Loss: 1.7595570385456085, Final Batch Loss: 0.41763994097709656\n",
      "Epoch 4989, Loss: 1.6345027387142181, Final Batch Loss: 0.2768400013446808\n",
      "Epoch 4990, Loss: 2.2322663962841034, Final Batch Loss: 0.8486051559448242\n",
      "Epoch 4991, Loss: 1.2946140971034765, Final Batch Loss: 0.025785496458411217\n",
      "Epoch 4992, Loss: 1.5287934392690659, Final Batch Loss: 0.24923066794872284\n",
      "Epoch 4993, Loss: 1.7065907716751099, Final Batch Loss: 0.5062469840049744\n",
      "Epoch 4994, Loss: 1.775172233581543, Final Batch Loss: 0.4391775131225586\n",
      "Epoch 4995, Loss: 1.453910157084465, Final Batch Loss: 0.15277032554149628\n",
      "Epoch 4996, Loss: 1.4414905980229378, Final Batch Loss: 0.08377469331026077\n",
      "Epoch 4997, Loss: 1.5765661746263504, Final Batch Loss: 0.3726314604282379\n",
      "Epoch 4998, Loss: 1.4805239737033844, Final Batch Loss: 0.23015233874320984\n",
      "Epoch 4999, Loss: 1.455234870314598, Final Batch Loss: 0.20965655148029327\n",
      "Epoch 5000, Loss: 1.6731804460287094, Final Batch Loss: 0.23021890223026276\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 16  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  1  0  0  3  0  0  1  0  0  0  0  0  1  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 12  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  4  0  0  1  0  0  0  0  0  3  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 14  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  0  1  0  0  0  0  0  0  0  0  7  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        18\n",
      "           1    1.00000   0.94118   0.96970        17\n",
      "           2    0.66667   1.00000   0.80000         8\n",
      "           3    0.92308   1.00000   0.96000        12\n",
      "           4    1.00000   0.88889   0.94118         9\n",
      "           5    0.33333   0.42857   0.37500         7\n",
      "           6    1.00000   1.00000   1.00000        12\n",
      "           7    1.00000   1.00000   1.00000        11\n",
      "           8    0.71429   1.00000   0.83333         5\n",
      "           9    1.00000   1.00000   1.00000        17\n",
      "          10    1.00000   1.00000   1.00000         9\n",
      "          11    0.88889   0.80000   0.84211        10\n",
      "          12    1.00000   1.00000   1.00000        12\n",
      "          13    0.83333   1.00000   0.90909         5\n",
      "          14    0.75000   0.37500   0.50000         8\n",
      "          15    1.00000   1.00000   1.00000        10\n",
      "          16    1.00000   1.00000   1.00000         8\n",
      "          17    0.88889   0.88889   0.88889         9\n",
      "          18    1.00000   1.00000   1.00000        11\n",
      "          19    1.00000   1.00000   1.00000        14\n",
      "          20    1.00000   0.77778   0.87500         9\n",
      "          21    1.00000   0.90909   0.95238        11\n",
      "          22    0.91667   1.00000   0.95652        11\n",
      "          23    1.00000   0.90909   0.95238        11\n",
      "\n",
      "    accuracy                        0.92913       254\n",
      "   macro avg    0.91313   0.91327   0.90648       254\n",
      "weighted avg    0.93879   0.92913   0.92907       254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_1 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_1 = np.zeros(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_2 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_2 = np.ones(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_3 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_3 = np.ones(n_samples) + 1\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_4 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_4 = np.ones(n_samples) + 2\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_5 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_5 = np.ones(n_samples) + 3\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_6 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_6 = np.ones(n_samples) + 4\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_7 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_7 = np.ones(n_samples) + 5\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_8 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_8 = np.ones(n_samples) + 6\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_9 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_9 = np.ones(n_samples) + 7\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U3A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_10 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_10 = np.ones(n_samples) + 8\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U3A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_11 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_11 = np.ones(n_samples) + 9\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U3A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_12 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_12 = np.ones(n_samples) + 10\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U4A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_13 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_13 = np.ones(n_samples) + 11\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U4A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_14 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_14 = np.ones(n_samples) + 12\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U4A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_15 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_15 = np.ones(n_samples) + 13\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U5A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_16 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_16 = np.ones(n_samples) + 14\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U5A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_17 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_17 = np.ones(n_samples) + 15\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U5A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_18 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_18 = np.ones(n_samples) + 16\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U6A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_19 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_19 = np.ones(n_samples) + 17\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U6A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_20 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_20 = np.ones(n_samples) + 18\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U6A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_21 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_21 = np.ones(n_samples) + 19\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U7A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_22 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_22 = np.ones(n_samples) + 20\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U7A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_23 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_23 = np.ones(n_samples) + 21\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U7A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_24 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_24 = np.ones(n_samples) + 22\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U8A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_25 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_25 = np.ones(n_samples) + 23\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U8A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_26 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_26 = np.ones(n_samples) + 24\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U8A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_27 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_27 = np.ones(n_samples) + 25\n",
    "\n",
    "fake_features = np.concatenate((fake_features_1, fake_features_2, fake_features_3, fake_features_4, fake_features_5, fake_features_6,\n",
    "                         fake_features_7, fake_features_8, fake_features_9, fake_features_10, fake_features_11, fake_features_12,\n",
    "                               fake_features_13, fake_features_14, fake_features_15, fake_features_16, fake_features_17, fake_features_18,\n",
    "                               fake_features_19, fake_features_20, fake_features_21, fake_features_22, fake_features_23, fake_features_24,\n",
    "                               fake_features_25, fake_features_26, fake_features_27))\n",
    "fake_labels = np.concatenate((y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9, y_10, y_11, y_12, y_13, y_14, y_15, y_16, y_17, y_18, y_19, y_20, y_21, y_22, y_23, y_24, y_25, y_26, y_27))\n",
    "\n",
    "fake_features = torch.Tensor(fake_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 18  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  1  0  0  0  0]\n",
      " [ 0  0  7  0  0  2  0  0  4  0  0  1  0  0  2  0  0  3  0  0  0  0  0  1]\n",
      " [ 0  0  0 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  4  0  0  5  0  0  2  0  0  1  0  0  3  0  0  5  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 19  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 14  0  0  2  0  0  1  0  0  0  0  0  2  0  0  1]\n",
      " [ 0  1  0  0  0  0  0  0  0 18  1  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0 16  0  0  0  0  0  0  0  0  2  0  0  1  0]\n",
      " [ 0  0  0  0  0  4  0  0  3  0  0  5  0  0  0  0  0  1  0  0  1  0  0  6]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  5 13  0  0  0  0  0  0  2  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  2  0  0 18  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 12  0  0  0  0  0  2  0  0  0  0  0  5  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0 19  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0 14  4  0  0  0  0  0  0]\n",
      " [ 0  0  3  0  0  1  0  0  4  0  0  0  0  0  0  0  0 12  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 20  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 20  0  0  0  0]\n",
      " [ 0  0  1  0  0  2  0  0  1  0  0  1  0  0  0  0  0  2  0  0  7  0  0  6]\n",
      " [ 0  0  0  5  0  5  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0 16  0]\n",
      " [ 0  0  0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    1.00000   1.00000   1.00000        20\n",
      "         1.0    0.94737   0.90000   0.92308        20\n",
      "         2.0    0.25926   0.35000   0.29787        20\n",
      "         3.0    0.80000   1.00000   0.88889        20\n",
      "         4.0    0.95238   1.00000   0.97561        20\n",
      "         5.0    0.19231   0.25000   0.21739        20\n",
      "         6.0    1.00000   1.00000   1.00000        20\n",
      "         7.0    0.90476   0.95000   0.92683        20\n",
      "         8.0    0.45161   0.70000   0.54902        20\n",
      "         9.0    1.00000   0.90000   0.94737        20\n",
      "        10.0    0.66667   0.80000   0.72727        20\n",
      "        11.0    0.45455   0.25000   0.32258        20\n",
      "        12.0    0.83333   0.25000   0.38462        20\n",
      "        13.0    0.58065   0.90000   0.70588        20\n",
      "        14.0    0.45455   0.25000   0.32258        20\n",
      "        15.0    1.00000   0.95000   0.97436        20\n",
      "        16.0    1.00000   0.70000   0.82353        20\n",
      "        17.0    0.36364   0.60000   0.45283        20\n",
      "        18.0    1.00000   1.00000   1.00000        20\n",
      "        19.0    0.86957   1.00000   0.93023        20\n",
      "        20.0    0.58333   0.35000   0.43750        20\n",
      "        21.0    1.00000   0.45000   0.62069        20\n",
      "        22.0    0.94118   0.80000   0.86486        20\n",
      "        23.0    0.36364   0.40000   0.38095        20\n",
      "\n",
      "    accuracy                        0.69792       480\n",
      "   macro avg    0.73412   0.69792   0.69475       480\n",
      "weighted avg    0.73412   0.69792   0.69475       480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5, zero_division = 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
