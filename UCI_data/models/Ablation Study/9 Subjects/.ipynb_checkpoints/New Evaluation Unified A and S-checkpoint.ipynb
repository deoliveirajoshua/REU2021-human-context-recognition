{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 40),\n",
    "            classifier_block(40, 30),\n",
    "            classifier_block(30, 25),\n",
    "            classifier_block(25, 25),\n",
    "            nn.Linear(25, 27)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def get_act_matrix(batch_size, a_dim):\n",
    "    indexes = np.random.randint(a_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((len(indexes), indexes.max()+1))\n",
    "    one_hot[np.arange(len(indexes)),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "    \n",
    "def get_usr_matrix(batch_size, u_dim):\n",
    "    indexes = np.random.randint(u_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((indexes.size, indexes.max()+1))\n",
    "    one_hot[np.arange(indexes.size),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Real Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "    \n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_10 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_11 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_12 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_13 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_14 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_15 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_16 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_17 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_18 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_19 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_20 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_21 = GAN_data[(GAN_data['Subject'] == 14) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_22 = GAN_data[(GAN_data['Subject'] == 17) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_23 = GAN_data[(GAN_data['Subject'] == 17) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_24 = GAN_data[(GAN_data['Subject'] == 17) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_25 = GAN_data[(GAN_data['Subject'] == 19) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_26 = GAN_data[(GAN_data['Subject'] == 19) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_27 = GAN_data[(GAN_data['Subject'] == 19) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9, X_10, X_11, X_12, X_13, X_14, X_15, X_16, X_17, X_18, X_19, X_20, X_21, X_22, X_23, X_24, X_25, X_26, X_27))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9) + [9] * len(X_10) + [10] * len(X_11) + [11] * len(X_12) + [12] * len(X_13) + [13] * len(X_14) + [14] * len(X_15) + [15] * len(X_16) + [16] * len(X_17) + [17] * len(X_18) + [18] * len(X_19) + [19] * len(X_20) + [20] * len(X_21) + [21] * len(X_22) + [22] * len(X_23) + [23] * len(X_24) + [24] * len(X_25) + [25] * len(X_26) + [26] * len(X_27)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5, 7, 8, 11, 14, 17, 19]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 16.505478858947754, Final Batch Loss: 3.3067429065704346\n",
      "Epoch 2, Loss: 16.490118741989136, Final Batch Loss: 3.297823429107666\n",
      "Epoch 3, Loss: 16.476760149002075, Final Batch Loss: 3.2878243923187256\n",
      "Epoch 4, Loss: 16.466208696365356, Final Batch Loss: 3.298814535140991\n",
      "Epoch 5, Loss: 16.44110941886902, Final Batch Loss: 3.2898590564727783\n",
      "Epoch 6, Loss: 16.40273141860962, Final Batch Loss: 3.277898073196411\n",
      "Epoch 7, Loss: 16.35971736907959, Final Batch Loss: 3.2479093074798584\n",
      "Epoch 8, Loss: 16.297749519348145, Final Batch Loss: 3.26343035697937\n",
      "Epoch 9, Loss: 16.18058490753174, Final Batch Loss: 3.2468974590301514\n",
      "Epoch 10, Loss: 16.045623064041138, Final Batch Loss: 3.22884202003479\n",
      "Epoch 11, Loss: 15.78481912612915, Final Batch Loss: 3.1396026611328125\n",
      "Epoch 12, Loss: 15.400501489639282, Final Batch Loss: 2.9925537109375\n",
      "Epoch 13, Loss: 14.991413831710815, Final Batch Loss: 2.951744318008423\n",
      "Epoch 14, Loss: 14.63778305053711, Final Batch Loss: 2.922180652618408\n",
      "Epoch 15, Loss: 14.23664665222168, Final Batch Loss: 2.794924736022949\n",
      "Epoch 16, Loss: 13.900716066360474, Final Batch Loss: 2.8226308822631836\n",
      "Epoch 17, Loss: 13.620710849761963, Final Batch Loss: 2.701040744781494\n",
      "Epoch 18, Loss: 13.222179889678955, Final Batch Loss: 2.6138811111450195\n",
      "Epoch 19, Loss: 12.971922159194946, Final Batch Loss: 2.600609540939331\n",
      "Epoch 20, Loss: 12.63610029220581, Final Batch Loss: 2.489830493927002\n",
      "Epoch 21, Loss: 12.267093181610107, Final Batch Loss: 2.3689794540405273\n",
      "Epoch 22, Loss: 11.980807304382324, Final Batch Loss: 2.3770837783813477\n",
      "Epoch 23, Loss: 11.60095500946045, Final Batch Loss: 2.2084507942199707\n",
      "Epoch 24, Loss: 11.424198150634766, Final Batch Loss: 2.2642180919647217\n",
      "Epoch 25, Loss: 11.293057203292847, Final Batch Loss: 2.2220396995544434\n",
      "Epoch 26, Loss: 11.170197248458862, Final Batch Loss: 2.27109432220459\n",
      "Epoch 27, Loss: 10.82297968864441, Final Batch Loss: 2.1718530654907227\n",
      "Epoch 28, Loss: 10.74697494506836, Final Batch Loss: 2.1905362606048584\n",
      "Epoch 29, Loss: 10.597474098205566, Final Batch Loss: 2.140571355819702\n",
      "Epoch 30, Loss: 10.454549074172974, Final Batch Loss: 2.0923538208007812\n",
      "Epoch 31, Loss: 10.312642574310303, Final Batch Loss: 2.084911346435547\n",
      "Epoch 32, Loss: 10.184767365455627, Final Batch Loss: 2.0367672443389893\n",
      "Epoch 33, Loss: 10.020036578178406, Final Batch Loss: 1.983683466911316\n",
      "Epoch 34, Loss: 10.132393836975098, Final Batch Loss: 2.0424773693084717\n",
      "Epoch 35, Loss: 9.924162149429321, Final Batch Loss: 1.9660972356796265\n",
      "Epoch 36, Loss: 9.779573559761047, Final Batch Loss: 1.9442037343978882\n",
      "Epoch 37, Loss: 9.691027164459229, Final Batch Loss: 1.8319413661956787\n",
      "Epoch 38, Loss: 9.635823845863342, Final Batch Loss: 2.0539462566375732\n",
      "Epoch 39, Loss: 9.454954743385315, Final Batch Loss: 1.9182868003845215\n",
      "Epoch 40, Loss: 9.383676052093506, Final Batch Loss: 1.8805651664733887\n",
      "Epoch 41, Loss: 9.514571189880371, Final Batch Loss: 1.9740501642227173\n",
      "Epoch 42, Loss: 9.214840292930603, Final Batch Loss: 1.8767271041870117\n",
      "Epoch 43, Loss: 9.174175024032593, Final Batch Loss: 1.7578309774398804\n",
      "Epoch 44, Loss: 8.98000705242157, Final Batch Loss: 1.7636024951934814\n",
      "Epoch 45, Loss: 9.050081849098206, Final Batch Loss: 1.7823305130004883\n",
      "Epoch 46, Loss: 8.844387888908386, Final Batch Loss: 1.7581709623336792\n",
      "Epoch 47, Loss: 8.821081280708313, Final Batch Loss: 1.7797163724899292\n",
      "Epoch 48, Loss: 8.610345840454102, Final Batch Loss: 1.69499671459198\n",
      "Epoch 49, Loss: 8.641845345497131, Final Batch Loss: 1.7610108852386475\n",
      "Epoch 50, Loss: 8.595591068267822, Final Batch Loss: 1.7950668334960938\n",
      "Epoch 51, Loss: 8.37478482723236, Final Batch Loss: 1.6189846992492676\n",
      "Epoch 52, Loss: 8.431171178817749, Final Batch Loss: 1.735290765762329\n",
      "Epoch 53, Loss: 8.479738116264343, Final Batch Loss: 1.7917838096618652\n",
      "Epoch 54, Loss: 8.22526729106903, Final Batch Loss: 1.7277494668960571\n",
      "Epoch 55, Loss: 8.271462082862854, Final Batch Loss: 1.6957573890686035\n",
      "Epoch 56, Loss: 8.293276309967041, Final Batch Loss: 1.6333507299423218\n",
      "Epoch 57, Loss: 8.266261577606201, Final Batch Loss: 1.5788993835449219\n",
      "Epoch 58, Loss: 8.005457520484924, Final Batch Loss: 1.5647032260894775\n",
      "Epoch 59, Loss: 8.012696623802185, Final Batch Loss: 1.6152902841567993\n",
      "Epoch 60, Loss: 8.17212176322937, Final Batch Loss: 1.681698203086853\n",
      "Epoch 61, Loss: 7.936773419380188, Final Batch Loss: 1.6069004535675049\n",
      "Epoch 62, Loss: 7.763139247894287, Final Batch Loss: 1.5687801837921143\n",
      "Epoch 63, Loss: 7.844882130622864, Final Batch Loss: 1.6347955465316772\n",
      "Epoch 64, Loss: 7.897310376167297, Final Batch Loss: 1.5734219551086426\n",
      "Epoch 65, Loss: 7.788132548332214, Final Batch Loss: 1.4983247518539429\n",
      "Epoch 66, Loss: 7.6714677810668945, Final Batch Loss: 1.4204668998718262\n",
      "Epoch 67, Loss: 7.657242059707642, Final Batch Loss: 1.5021672248840332\n",
      "Epoch 68, Loss: 7.743832111358643, Final Batch Loss: 1.6466588973999023\n",
      "Epoch 69, Loss: 7.467347741127014, Final Batch Loss: 1.4191533327102661\n",
      "Epoch 70, Loss: 7.3839640617370605, Final Batch Loss: 1.4013745784759521\n",
      "Epoch 71, Loss: 7.450716018676758, Final Batch Loss: 1.5443905591964722\n",
      "Epoch 72, Loss: 7.64183783531189, Final Batch Loss: 1.7298250198364258\n",
      "Epoch 73, Loss: 7.667459011077881, Final Batch Loss: 1.5833842754364014\n",
      "Epoch 74, Loss: 7.376847743988037, Final Batch Loss: 1.4825377464294434\n",
      "Epoch 75, Loss: 7.4672545194625854, Final Batch Loss: 1.6381032466888428\n",
      "Epoch 76, Loss: 7.378258585929871, Final Batch Loss: 1.4149389266967773\n",
      "Epoch 77, Loss: 7.457850933074951, Final Batch Loss: 1.518076777458191\n",
      "Epoch 78, Loss: 7.352744221687317, Final Batch Loss: 1.5488510131835938\n",
      "Epoch 79, Loss: 7.455402731895447, Final Batch Loss: 1.5646240711212158\n",
      "Epoch 80, Loss: 7.302078366279602, Final Batch Loss: 1.3023498058319092\n",
      "Epoch 81, Loss: 7.318428874015808, Final Batch Loss: 1.6134495735168457\n",
      "Epoch 82, Loss: 7.2721253633499146, Final Batch Loss: 1.486806035041809\n",
      "Epoch 83, Loss: 7.419382214546204, Final Batch Loss: 1.4741307497024536\n",
      "Epoch 84, Loss: 7.111653804779053, Final Batch Loss: 1.3654359579086304\n",
      "Epoch 85, Loss: 7.0136798620224, Final Batch Loss: 1.223642349243164\n",
      "Epoch 86, Loss: 7.234440445899963, Final Batch Loss: 1.524970531463623\n",
      "Epoch 87, Loss: 7.11508047580719, Final Batch Loss: 1.37705397605896\n",
      "Epoch 88, Loss: 6.952887654304504, Final Batch Loss: 1.3926289081573486\n",
      "Epoch 89, Loss: 7.046558499336243, Final Batch Loss: 1.4333592653274536\n",
      "Epoch 90, Loss: 7.11204469203949, Final Batch Loss: 1.419819951057434\n",
      "Epoch 91, Loss: 7.002304315567017, Final Batch Loss: 1.3255548477172852\n",
      "Epoch 92, Loss: 6.943388938903809, Final Batch Loss: 1.3389055728912354\n",
      "Epoch 93, Loss: 7.1775062084198, Final Batch Loss: 1.5568641424179077\n",
      "Epoch 94, Loss: 6.910370826721191, Final Batch Loss: 1.3546762466430664\n",
      "Epoch 95, Loss: 6.991029381752014, Final Batch Loss: 1.3573722839355469\n",
      "Epoch 96, Loss: 6.735121488571167, Final Batch Loss: 1.487736701965332\n",
      "Epoch 97, Loss: 6.936524391174316, Final Batch Loss: 1.4530962705612183\n",
      "Epoch 98, Loss: 6.891146302223206, Final Batch Loss: 1.3196442127227783\n",
      "Epoch 99, Loss: 6.727062463760376, Final Batch Loss: 1.278359055519104\n",
      "Epoch 100, Loss: 6.859418511390686, Final Batch Loss: 1.3117775917053223\n",
      "Epoch 101, Loss: 6.729674577713013, Final Batch Loss: 1.3260563611984253\n",
      "Epoch 102, Loss: 6.70767343044281, Final Batch Loss: 1.3958334922790527\n",
      "Epoch 103, Loss: 6.58864438533783, Final Batch Loss: 1.36007821559906\n",
      "Epoch 104, Loss: 6.77146303653717, Final Batch Loss: 1.404887080192566\n",
      "Epoch 105, Loss: 6.810898780822754, Final Batch Loss: 1.4122564792633057\n",
      "Epoch 106, Loss: 6.705257773399353, Final Batch Loss: 1.36051344871521\n",
      "Epoch 107, Loss: 6.729362964630127, Final Batch Loss: 1.2798808813095093\n",
      "Epoch 108, Loss: 6.642456889152527, Final Batch Loss: 1.3337790966033936\n",
      "Epoch 109, Loss: 6.6506853103637695, Final Batch Loss: 1.3063589334487915\n",
      "Epoch 110, Loss: 6.733379125595093, Final Batch Loss: 1.2702125310897827\n",
      "Epoch 111, Loss: 6.6441651582717896, Final Batch Loss: 1.2831919193267822\n",
      "Epoch 112, Loss: 6.647315502166748, Final Batch Loss: 1.3662900924682617\n",
      "Epoch 113, Loss: 6.751452088356018, Final Batch Loss: 1.3290315866470337\n",
      "Epoch 114, Loss: 6.655388832092285, Final Batch Loss: 1.4651494026184082\n",
      "Epoch 115, Loss: 6.644729137420654, Final Batch Loss: 1.2819812297821045\n",
      "Epoch 116, Loss: 6.577977418899536, Final Batch Loss: 1.3033595085144043\n",
      "Epoch 117, Loss: 6.4954670667648315, Final Batch Loss: 1.2351915836334229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118, Loss: 6.6184492111206055, Final Batch Loss: 1.320454716682434\n",
      "Epoch 119, Loss: 6.455546259880066, Final Batch Loss: 1.3355871438980103\n",
      "Epoch 120, Loss: 6.665382623672485, Final Batch Loss: 1.277972936630249\n",
      "Epoch 121, Loss: 6.559099078178406, Final Batch Loss: 1.1676918268203735\n",
      "Epoch 122, Loss: 6.641663074493408, Final Batch Loss: 1.3091872930526733\n",
      "Epoch 123, Loss: 6.455274820327759, Final Batch Loss: 1.2672171592712402\n",
      "Epoch 124, Loss: 6.540860176086426, Final Batch Loss: 1.3517279624938965\n",
      "Epoch 125, Loss: 6.40455961227417, Final Batch Loss: 1.3054262399673462\n",
      "Epoch 126, Loss: 6.352289915084839, Final Batch Loss: 1.2171385288238525\n",
      "Epoch 127, Loss: 6.529070734977722, Final Batch Loss: 1.426863670349121\n",
      "Epoch 128, Loss: 6.424088001251221, Final Batch Loss: 1.3234577178955078\n",
      "Epoch 129, Loss: 6.313281297683716, Final Batch Loss: 1.2204432487487793\n",
      "Epoch 130, Loss: 6.413975715637207, Final Batch Loss: 1.2240418195724487\n",
      "Epoch 131, Loss: 6.374463438987732, Final Batch Loss: 1.335155963897705\n",
      "Epoch 132, Loss: 6.39701509475708, Final Batch Loss: 1.368859052658081\n",
      "Epoch 133, Loss: 6.273531675338745, Final Batch Loss: 1.163684368133545\n",
      "Epoch 134, Loss: 6.322559475898743, Final Batch Loss: 1.2142571210861206\n",
      "Epoch 135, Loss: 6.340234994888306, Final Batch Loss: 1.3635190725326538\n",
      "Epoch 136, Loss: 6.324099898338318, Final Batch Loss: 1.166947603225708\n",
      "Epoch 137, Loss: 6.435818314552307, Final Batch Loss: 1.3836607933044434\n",
      "Epoch 138, Loss: 6.283056616783142, Final Batch Loss: 1.290940284729004\n",
      "Epoch 139, Loss: 6.359817028045654, Final Batch Loss: 1.2650078535079956\n",
      "Epoch 140, Loss: 6.311304926872253, Final Batch Loss: 1.2715414762496948\n",
      "Epoch 141, Loss: 6.187225818634033, Final Batch Loss: 1.176681637763977\n",
      "Epoch 142, Loss: 6.4797526597976685, Final Batch Loss: 1.3360607624053955\n",
      "Epoch 143, Loss: 6.079866290092468, Final Batch Loss: 1.252257227897644\n",
      "Epoch 144, Loss: 6.17627477645874, Final Batch Loss: 1.1475684642791748\n",
      "Epoch 145, Loss: 6.202456474304199, Final Batch Loss: 1.1447248458862305\n",
      "Epoch 146, Loss: 6.177310585975647, Final Batch Loss: 1.1475480794906616\n",
      "Epoch 147, Loss: 6.093536853790283, Final Batch Loss: 1.2038553953170776\n",
      "Epoch 148, Loss: 6.104599237442017, Final Batch Loss: 1.3959189653396606\n",
      "Epoch 149, Loss: 5.987157940864563, Final Batch Loss: 1.1941148042678833\n",
      "Epoch 150, Loss: 6.179078221321106, Final Batch Loss: 1.2159333229064941\n",
      "Epoch 151, Loss: 6.176207065582275, Final Batch Loss: 1.245139718055725\n",
      "Epoch 152, Loss: 6.168395280838013, Final Batch Loss: 1.2081233263015747\n",
      "Epoch 153, Loss: 6.061959505081177, Final Batch Loss: 1.206120491027832\n",
      "Epoch 154, Loss: 5.939520955085754, Final Batch Loss: 1.1419885158538818\n",
      "Epoch 155, Loss: 6.086818099021912, Final Batch Loss: 1.244857907295227\n",
      "Epoch 156, Loss: 6.107937932014465, Final Batch Loss: 1.2358040809631348\n",
      "Epoch 157, Loss: 6.060092806816101, Final Batch Loss: 1.1700036525726318\n",
      "Epoch 158, Loss: 5.969856023788452, Final Batch Loss: 1.2434269189834595\n",
      "Epoch 159, Loss: 6.065243482589722, Final Batch Loss: 1.1694843769073486\n",
      "Epoch 160, Loss: 6.074080944061279, Final Batch Loss: 1.2514468431472778\n",
      "Epoch 161, Loss: 6.028241872787476, Final Batch Loss: 1.091254711151123\n",
      "Epoch 162, Loss: 5.994802236557007, Final Batch Loss: 1.136651635169983\n",
      "Epoch 163, Loss: 6.008963346481323, Final Batch Loss: 1.0859344005584717\n",
      "Epoch 164, Loss: 6.03536331653595, Final Batch Loss: 1.2602565288543701\n",
      "Epoch 165, Loss: 6.230185151100159, Final Batch Loss: 1.2472412586212158\n",
      "Epoch 166, Loss: 5.798015236854553, Final Batch Loss: 1.1264238357543945\n",
      "Epoch 167, Loss: 6.017529606819153, Final Batch Loss: 1.255129337310791\n",
      "Epoch 168, Loss: 5.811769366264343, Final Batch Loss: 1.056220531463623\n",
      "Epoch 169, Loss: 5.959444642066956, Final Batch Loss: 1.2088749408721924\n",
      "Epoch 170, Loss: 5.87325119972229, Final Batch Loss: 1.0735896825790405\n",
      "Epoch 171, Loss: 6.03355872631073, Final Batch Loss: 1.0776708126068115\n",
      "Epoch 172, Loss: 5.922942280769348, Final Batch Loss: 1.2180631160736084\n",
      "Epoch 173, Loss: 6.0768842697143555, Final Batch Loss: 1.297847867012024\n",
      "Epoch 174, Loss: 5.975456118583679, Final Batch Loss: 1.0802478790283203\n",
      "Epoch 175, Loss: 5.775454163551331, Final Batch Loss: 1.0348412990570068\n",
      "Epoch 176, Loss: 5.841039061546326, Final Batch Loss: 1.1829084157943726\n",
      "Epoch 177, Loss: 6.062052130699158, Final Batch Loss: 1.2631611824035645\n",
      "Epoch 178, Loss: 6.1256208419799805, Final Batch Loss: 1.224331259727478\n",
      "Epoch 179, Loss: 5.77646005153656, Final Batch Loss: 1.028161883354187\n",
      "Epoch 180, Loss: 5.946518421173096, Final Batch Loss: 1.2456650733947754\n",
      "Epoch 181, Loss: 5.86007559299469, Final Batch Loss: 1.1697138547897339\n",
      "Epoch 182, Loss: 5.844946265220642, Final Batch Loss: 1.0404012203216553\n",
      "Epoch 183, Loss: 5.751416087150574, Final Batch Loss: 1.1064956188201904\n",
      "Epoch 184, Loss: 5.881866455078125, Final Batch Loss: 1.1896064281463623\n",
      "Epoch 185, Loss: 5.95709490776062, Final Batch Loss: 1.2425626516342163\n",
      "Epoch 186, Loss: 5.847307085990906, Final Batch Loss: 1.2221652269363403\n",
      "Epoch 187, Loss: 5.8450517654418945, Final Batch Loss: 1.0694304704666138\n",
      "Epoch 188, Loss: 5.9333815574646, Final Batch Loss: 1.2812316417694092\n",
      "Epoch 189, Loss: 5.910454273223877, Final Batch Loss: 1.2800612449645996\n",
      "Epoch 190, Loss: 5.878080725669861, Final Batch Loss: 1.1976032257080078\n",
      "Epoch 191, Loss: 5.81900155544281, Final Batch Loss: 1.1487796306610107\n",
      "Epoch 192, Loss: 5.852283000946045, Final Batch Loss: 1.1220269203186035\n",
      "Epoch 193, Loss: 5.899873971939087, Final Batch Loss: 1.1405609846115112\n",
      "Epoch 194, Loss: 5.6949684619903564, Final Batch Loss: 1.0340648889541626\n",
      "Epoch 195, Loss: 5.6037293672561646, Final Batch Loss: 1.1303284168243408\n",
      "Epoch 196, Loss: 5.754953384399414, Final Batch Loss: 1.0609304904937744\n",
      "Epoch 197, Loss: 5.858371734619141, Final Batch Loss: 1.1509180068969727\n",
      "Epoch 198, Loss: 5.780546307563782, Final Batch Loss: 1.096587061882019\n",
      "Epoch 199, Loss: 5.986741065979004, Final Batch Loss: 1.1383780241012573\n",
      "Epoch 200, Loss: 5.707395553588867, Final Batch Loss: 1.0494670867919922\n",
      "Epoch 201, Loss: 5.628387808799744, Final Batch Loss: 1.2278456687927246\n",
      "Epoch 202, Loss: 5.707803010940552, Final Batch Loss: 1.1125564575195312\n",
      "Epoch 203, Loss: 5.833757996559143, Final Batch Loss: 1.2362395524978638\n",
      "Epoch 204, Loss: 5.647278189659119, Final Batch Loss: 1.1153706312179565\n",
      "Epoch 205, Loss: 5.7665393352508545, Final Batch Loss: 1.071993112564087\n",
      "Epoch 206, Loss: 5.649900197982788, Final Batch Loss: 1.143442988395691\n",
      "Epoch 207, Loss: 5.812427282333374, Final Batch Loss: 1.2226078510284424\n",
      "Epoch 208, Loss: 5.67108690738678, Final Batch Loss: 1.101120948791504\n",
      "Epoch 209, Loss: 5.560214877128601, Final Batch Loss: 1.0591167211532593\n",
      "Epoch 210, Loss: 5.734798312187195, Final Batch Loss: 1.1611371040344238\n",
      "Epoch 211, Loss: 5.748194456100464, Final Batch Loss: 1.1063005924224854\n",
      "Epoch 212, Loss: 5.678945422172546, Final Batch Loss: 1.0807671546936035\n",
      "Epoch 213, Loss: 5.482565760612488, Final Batch Loss: 1.0327414274215698\n",
      "Epoch 214, Loss: 5.700605630874634, Final Batch Loss: 1.0907659530639648\n",
      "Epoch 215, Loss: 5.617500185966492, Final Batch Loss: 1.138964056968689\n",
      "Epoch 216, Loss: 5.7230658531188965, Final Batch Loss: 1.2761743068695068\n",
      "Epoch 217, Loss: 5.639828085899353, Final Batch Loss: 1.124051809310913\n",
      "Epoch 218, Loss: 5.680825591087341, Final Batch Loss: 1.312143325805664\n",
      "Epoch 219, Loss: 5.625466465950012, Final Batch Loss: 1.1096124649047852\n",
      "Epoch 220, Loss: 5.710502207279205, Final Batch Loss: 1.1805479526519775\n",
      "Epoch 221, Loss: 5.525785386562347, Final Batch Loss: 1.1931121349334717\n",
      "Epoch 222, Loss: 5.674839615821838, Final Batch Loss: 1.0601933002471924\n",
      "Epoch 223, Loss: 5.67550802230835, Final Batch Loss: 1.1521921157836914\n",
      "Epoch 224, Loss: 5.605612277984619, Final Batch Loss: 1.1474519968032837\n",
      "Epoch 225, Loss: 5.570859849452972, Final Batch Loss: 1.1049919128417969\n",
      "Epoch 226, Loss: 5.540498971939087, Final Batch Loss: 1.1562682390213013\n",
      "Epoch 227, Loss: 5.708531379699707, Final Batch Loss: 1.0811395645141602\n",
      "Epoch 228, Loss: 5.520005941390991, Final Batch Loss: 0.9153121709823608\n",
      "Epoch 229, Loss: 5.452714562416077, Final Batch Loss: 1.1050419807434082\n",
      "Epoch 230, Loss: 5.608729839324951, Final Batch Loss: 1.0245842933654785\n",
      "Epoch 231, Loss: 5.4834223985672, Final Batch Loss: 1.0668308734893799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232, Loss: 5.6061296463012695, Final Batch Loss: 1.0679527521133423\n",
      "Epoch 233, Loss: 5.563033699989319, Final Batch Loss: 1.0629878044128418\n",
      "Epoch 234, Loss: 5.616850972175598, Final Batch Loss: 1.2043125629425049\n",
      "Epoch 235, Loss: 5.302647829055786, Final Batch Loss: 0.9159921407699585\n",
      "Epoch 236, Loss: 5.566902160644531, Final Batch Loss: 1.2495670318603516\n",
      "Epoch 237, Loss: 5.645352721214294, Final Batch Loss: 1.0977498292922974\n",
      "Epoch 238, Loss: 5.658755660057068, Final Batch Loss: 1.3949236869812012\n",
      "Epoch 239, Loss: 5.570740699768066, Final Batch Loss: 1.2214813232421875\n",
      "Epoch 240, Loss: 5.613604187965393, Final Batch Loss: 1.081877589225769\n",
      "Epoch 241, Loss: 5.463521599769592, Final Batch Loss: 1.0589697360992432\n",
      "Epoch 242, Loss: 5.621071696281433, Final Batch Loss: 1.1515741348266602\n",
      "Epoch 243, Loss: 5.578575611114502, Final Batch Loss: 1.0596535205841064\n",
      "Epoch 244, Loss: 5.38246214389801, Final Batch Loss: 0.9752969741821289\n",
      "Epoch 245, Loss: 5.584788799285889, Final Batch Loss: 1.2232853174209595\n",
      "Epoch 246, Loss: 5.441380143165588, Final Batch Loss: 1.0471683740615845\n",
      "Epoch 247, Loss: 5.620758533477783, Final Batch Loss: 1.1626999378204346\n",
      "Epoch 248, Loss: 5.409433841705322, Final Batch Loss: 1.0735043287277222\n",
      "Epoch 249, Loss: 5.5068652629852295, Final Batch Loss: 1.1016814708709717\n",
      "Epoch 250, Loss: 5.36312210559845, Final Batch Loss: 1.0613065958023071\n",
      "Epoch 251, Loss: 5.573973536491394, Final Batch Loss: 1.139490008354187\n",
      "Epoch 252, Loss: 5.553249478340149, Final Batch Loss: 1.1891627311706543\n",
      "Epoch 253, Loss: 5.394331455230713, Final Batch Loss: 0.9331645965576172\n",
      "Epoch 254, Loss: 5.53714919090271, Final Batch Loss: 1.1628682613372803\n",
      "Epoch 255, Loss: 5.478341698646545, Final Batch Loss: 1.1209465265274048\n",
      "Epoch 256, Loss: 5.567186951637268, Final Batch Loss: 1.1642171144485474\n",
      "Epoch 257, Loss: 5.393117070198059, Final Batch Loss: 1.0162138938903809\n",
      "Epoch 258, Loss: 5.641115665435791, Final Batch Loss: 1.1661783456802368\n",
      "Epoch 259, Loss: 5.689885377883911, Final Batch Loss: 1.2216002941131592\n",
      "Epoch 260, Loss: 5.343375563621521, Final Batch Loss: 1.01201331615448\n",
      "Epoch 261, Loss: 5.499849200248718, Final Batch Loss: 1.2373968362808228\n",
      "Epoch 262, Loss: 5.510367274284363, Final Batch Loss: 0.942996621131897\n",
      "Epoch 263, Loss: 5.5159560441970825, Final Batch Loss: 1.20193612575531\n",
      "Epoch 264, Loss: 5.3857200145721436, Final Batch Loss: 1.1291061639785767\n",
      "Epoch 265, Loss: 5.469461143016815, Final Batch Loss: 1.06173574924469\n",
      "Epoch 266, Loss: 5.318036735057831, Final Batch Loss: 1.1651941537857056\n",
      "Epoch 267, Loss: 5.172690510749817, Final Batch Loss: 0.9705660939216614\n",
      "Epoch 268, Loss: 5.41260838508606, Final Batch Loss: 1.1432580947875977\n",
      "Epoch 269, Loss: 5.460030555725098, Final Batch Loss: 1.1701594591140747\n",
      "Epoch 270, Loss: 5.280830264091492, Final Batch Loss: 1.085137963294983\n",
      "Epoch 271, Loss: 5.497798323631287, Final Batch Loss: 1.1848376989364624\n",
      "Epoch 272, Loss: 5.332225918769836, Final Batch Loss: 1.031964898109436\n",
      "Epoch 273, Loss: 5.4411232471466064, Final Batch Loss: 1.1153099536895752\n",
      "Epoch 274, Loss: 5.2594804763793945, Final Batch Loss: 1.15351402759552\n",
      "Epoch 275, Loss: 5.192655086517334, Final Batch Loss: 0.978416919708252\n",
      "Epoch 276, Loss: 5.295356392860413, Final Batch Loss: 1.0518757104873657\n",
      "Epoch 277, Loss: 5.368524610996246, Final Batch Loss: 1.1021723747253418\n",
      "Epoch 278, Loss: 5.258828520774841, Final Batch Loss: 0.994972825050354\n",
      "Epoch 279, Loss: 5.459119081497192, Final Batch Loss: 1.1630452871322632\n",
      "Epoch 280, Loss: 5.265008509159088, Final Batch Loss: 0.911387026309967\n",
      "Epoch 281, Loss: 5.321504592895508, Final Batch Loss: 1.0016626119613647\n",
      "Epoch 282, Loss: 5.2581000328063965, Final Batch Loss: 1.0473889112472534\n",
      "Epoch 283, Loss: 5.206293344497681, Final Batch Loss: 1.0251226425170898\n",
      "Epoch 284, Loss: 5.222666501998901, Final Batch Loss: 1.0196428298950195\n",
      "Epoch 285, Loss: 5.173076391220093, Final Batch Loss: 0.9563906192779541\n",
      "Epoch 286, Loss: 5.16547155380249, Final Batch Loss: 0.9741671085357666\n",
      "Epoch 287, Loss: 5.390120148658752, Final Batch Loss: 1.0601096153259277\n",
      "Epoch 288, Loss: 5.324637413024902, Final Batch Loss: 1.0882105827331543\n",
      "Epoch 289, Loss: 5.398998498916626, Final Batch Loss: 1.1194207668304443\n",
      "Epoch 290, Loss: 5.270928621292114, Final Batch Loss: 1.0960582494735718\n",
      "Epoch 291, Loss: 5.218670547008514, Final Batch Loss: 1.032169222831726\n",
      "Epoch 292, Loss: 5.288791060447693, Final Batch Loss: 1.0603598356246948\n",
      "Epoch 293, Loss: 5.191903114318848, Final Batch Loss: 1.0045989751815796\n",
      "Epoch 294, Loss: 5.240306258201599, Final Batch Loss: 1.090654969215393\n",
      "Epoch 295, Loss: 5.204360008239746, Final Batch Loss: 1.0030796527862549\n",
      "Epoch 296, Loss: 5.298918962478638, Final Batch Loss: 1.051526665687561\n",
      "Epoch 297, Loss: 5.1295006275177, Final Batch Loss: 1.0704673528671265\n",
      "Epoch 298, Loss: 5.218861103057861, Final Batch Loss: 1.112406611442566\n",
      "Epoch 299, Loss: 5.2054243087768555, Final Batch Loss: 1.1209681034088135\n",
      "Epoch 300, Loss: 5.1298558712005615, Final Batch Loss: 0.9611830115318298\n",
      "Epoch 301, Loss: 5.091103434562683, Final Batch Loss: 1.023573637008667\n",
      "Epoch 302, Loss: 5.239364743232727, Final Batch Loss: 1.013852834701538\n",
      "Epoch 303, Loss: 5.128361344337463, Final Batch Loss: 1.0517873764038086\n",
      "Epoch 304, Loss: 5.087582468986511, Final Batch Loss: 0.9764453768730164\n",
      "Epoch 305, Loss: 5.220180094242096, Final Batch Loss: 1.0982035398483276\n",
      "Epoch 306, Loss: 5.241228222846985, Final Batch Loss: 1.058728575706482\n",
      "Epoch 307, Loss: 5.207501769065857, Final Batch Loss: 0.9743931293487549\n",
      "Epoch 308, Loss: 5.301734566688538, Final Batch Loss: 1.0394175052642822\n",
      "Epoch 309, Loss: 5.140674114227295, Final Batch Loss: 0.964134693145752\n",
      "Epoch 310, Loss: 5.113515675067902, Final Batch Loss: 0.9580569267272949\n",
      "Epoch 311, Loss: 5.108050882816315, Final Batch Loss: 0.9387975335121155\n",
      "Epoch 312, Loss: 5.050450384616852, Final Batch Loss: 1.0158624649047852\n",
      "Epoch 313, Loss: 5.11469566822052, Final Batch Loss: 0.9591782093048096\n",
      "Epoch 314, Loss: 5.138696372509003, Final Batch Loss: 1.0599613189697266\n",
      "Epoch 315, Loss: 5.118272662162781, Final Batch Loss: 1.0174100399017334\n",
      "Epoch 316, Loss: 5.120748221874237, Final Batch Loss: 1.0151044130325317\n",
      "Epoch 317, Loss: 5.129944682121277, Final Batch Loss: 1.0358928442001343\n",
      "Epoch 318, Loss: 5.100256025791168, Final Batch Loss: 1.0456018447875977\n",
      "Epoch 319, Loss: 5.121266663074493, Final Batch Loss: 1.0275300741195679\n",
      "Epoch 320, Loss: 5.23466694355011, Final Batch Loss: 1.0014164447784424\n",
      "Epoch 321, Loss: 5.158072650432587, Final Batch Loss: 1.0646628141403198\n",
      "Epoch 322, Loss: 5.043541491031647, Final Batch Loss: 0.9548159837722778\n",
      "Epoch 323, Loss: 5.208277940750122, Final Batch Loss: 1.0234311819076538\n",
      "Epoch 324, Loss: 5.177598237991333, Final Batch Loss: 1.0438683032989502\n",
      "Epoch 325, Loss: 5.184302747249603, Final Batch Loss: 1.1561009883880615\n",
      "Epoch 326, Loss: 5.229625701904297, Final Batch Loss: 1.0559300184249878\n",
      "Epoch 327, Loss: 5.120994448661804, Final Batch Loss: 0.9830253720283508\n",
      "Epoch 328, Loss: 5.019112586975098, Final Batch Loss: 1.0801997184753418\n",
      "Epoch 329, Loss: 5.070035755634308, Final Batch Loss: 1.087316870689392\n",
      "Epoch 330, Loss: 5.105321526527405, Final Batch Loss: 1.016306757926941\n",
      "Epoch 331, Loss: 4.915168225765228, Final Batch Loss: 0.9396457076072693\n",
      "Epoch 332, Loss: 5.082135617733002, Final Batch Loss: 1.0679106712341309\n",
      "Epoch 333, Loss: 4.988921821117401, Final Batch Loss: 0.9478176832199097\n",
      "Epoch 334, Loss: 4.965560555458069, Final Batch Loss: 1.0263572931289673\n",
      "Epoch 335, Loss: 5.064233243465424, Final Batch Loss: 0.9794516563415527\n",
      "Epoch 336, Loss: 4.942440509796143, Final Batch Loss: 0.9516727924346924\n",
      "Epoch 337, Loss: 4.898863852024078, Final Batch Loss: 0.9157040119171143\n",
      "Epoch 338, Loss: 5.104339301586151, Final Batch Loss: 1.092133641242981\n",
      "Epoch 339, Loss: 5.125072956085205, Final Batch Loss: 0.9824398756027222\n",
      "Epoch 340, Loss: 4.946625709533691, Final Batch Loss: 0.947958767414093\n",
      "Epoch 341, Loss: 5.032810091972351, Final Batch Loss: 1.0005369186401367\n",
      "Epoch 342, Loss: 4.9526883363723755, Final Batch Loss: 1.074552297592163\n",
      "Epoch 343, Loss: 5.154565513134003, Final Batch Loss: 1.0259313583374023\n",
      "Epoch 344, Loss: 5.04463130235672, Final Batch Loss: 1.1483899354934692\n",
      "Epoch 345, Loss: 5.0382362604141235, Final Batch Loss: 0.9228029251098633\n",
      "Epoch 346, Loss: 4.99509984254837, Final Batch Loss: 1.0097887516021729\n",
      "Epoch 347, Loss: 4.957873165607452, Final Batch Loss: 1.080612301826477\n",
      "Epoch 348, Loss: 4.996833920478821, Final Batch Loss: 1.0297653675079346\n",
      "Epoch 349, Loss: 4.887791991233826, Final Batch Loss: 0.9891631007194519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350, Loss: 4.914102673530579, Final Batch Loss: 0.9604529142379761\n",
      "Epoch 351, Loss: 5.037102699279785, Final Batch Loss: 1.00411057472229\n",
      "Epoch 352, Loss: 5.12628835439682, Final Batch Loss: 1.0726748704910278\n",
      "Epoch 353, Loss: 4.963432192802429, Final Batch Loss: 0.9592660665512085\n",
      "Epoch 354, Loss: 4.985727369785309, Final Batch Loss: 1.0726711750030518\n",
      "Epoch 355, Loss: 5.079389929771423, Final Batch Loss: 1.0405670404434204\n",
      "Epoch 356, Loss: 5.060293555259705, Final Batch Loss: 1.0306192636489868\n",
      "Epoch 357, Loss: 5.0418906807899475, Final Batch Loss: 1.0090638399124146\n",
      "Epoch 358, Loss: 4.923640191555023, Final Batch Loss: 0.9607962965965271\n",
      "Epoch 359, Loss: 4.890185594558716, Final Batch Loss: 0.9858558177947998\n",
      "Epoch 360, Loss: 4.988581001758575, Final Batch Loss: 0.8825023174285889\n",
      "Epoch 361, Loss: 4.9076040387153625, Final Batch Loss: 1.0162402391433716\n",
      "Epoch 362, Loss: 4.859933018684387, Final Batch Loss: 0.8804076313972473\n",
      "Epoch 363, Loss: 4.797532856464386, Final Batch Loss: 1.058423638343811\n",
      "Epoch 364, Loss: 4.909206986427307, Final Batch Loss: 0.9967715740203857\n",
      "Epoch 365, Loss: 4.783073604106903, Final Batch Loss: 0.9177560806274414\n",
      "Epoch 366, Loss: 5.008025527000427, Final Batch Loss: 1.043591856956482\n",
      "Epoch 367, Loss: 4.833235144615173, Final Batch Loss: 0.9270603656768799\n",
      "Epoch 368, Loss: 4.925548017024994, Final Batch Loss: 1.0285558700561523\n",
      "Epoch 369, Loss: 4.910076081752777, Final Batch Loss: 0.9665049314498901\n",
      "Epoch 370, Loss: 4.912953436374664, Final Batch Loss: 0.9375926852226257\n",
      "Epoch 371, Loss: 4.958430290222168, Final Batch Loss: 0.9824419617652893\n",
      "Epoch 372, Loss: 4.880920767784119, Final Batch Loss: 0.9084928035736084\n",
      "Epoch 373, Loss: 4.815754234790802, Final Batch Loss: 1.0436244010925293\n",
      "Epoch 374, Loss: 4.951692461967468, Final Batch Loss: 1.1340157985687256\n",
      "Epoch 375, Loss: 4.791368067264557, Final Batch Loss: 1.0152796506881714\n",
      "Epoch 376, Loss: 4.94064861536026, Final Batch Loss: 1.022544264793396\n",
      "Epoch 377, Loss: 4.941702604293823, Final Batch Loss: 0.976399302482605\n",
      "Epoch 378, Loss: 4.840369522571564, Final Batch Loss: 0.9644216895103455\n",
      "Epoch 379, Loss: 4.824753820896149, Final Batch Loss: 0.8778433203697205\n",
      "Epoch 380, Loss: 4.781212508678436, Final Batch Loss: 0.9662972092628479\n",
      "Epoch 381, Loss: 4.764870345592499, Final Batch Loss: 0.8994749784469604\n",
      "Epoch 382, Loss: 4.701924026012421, Final Batch Loss: 0.8638275861740112\n",
      "Epoch 383, Loss: 4.950962603092194, Final Batch Loss: 1.031112790107727\n",
      "Epoch 384, Loss: 4.912857830524445, Final Batch Loss: 1.0007113218307495\n",
      "Epoch 385, Loss: 4.799368798732758, Final Batch Loss: 0.9440454840660095\n",
      "Epoch 386, Loss: 4.778446853160858, Final Batch Loss: 0.9213001132011414\n",
      "Epoch 387, Loss: 4.801999449729919, Final Batch Loss: 0.8471512198448181\n",
      "Epoch 388, Loss: 4.651350617408752, Final Batch Loss: 0.9168545603752136\n",
      "Epoch 389, Loss: 4.778603672981262, Final Batch Loss: 0.9364248514175415\n",
      "Epoch 390, Loss: 4.818754494190216, Final Batch Loss: 1.0834568738937378\n",
      "Epoch 391, Loss: 4.861369431018829, Final Batch Loss: 1.0306519269943237\n",
      "Epoch 392, Loss: 4.849175691604614, Final Batch Loss: 1.00955331325531\n",
      "Epoch 393, Loss: 4.8232468366622925, Final Batch Loss: 1.0422391891479492\n",
      "Epoch 394, Loss: 4.853674948215485, Final Batch Loss: 1.0441638231277466\n",
      "Epoch 395, Loss: 4.707239329814911, Final Batch Loss: 1.0405627489089966\n",
      "Epoch 396, Loss: 4.790356516838074, Final Batch Loss: 0.8508239984512329\n",
      "Epoch 397, Loss: 4.786526381969452, Final Batch Loss: 0.9399250745773315\n",
      "Epoch 398, Loss: 4.880907475948334, Final Batch Loss: 0.982688844203949\n",
      "Epoch 399, Loss: 4.829237163066864, Final Batch Loss: 1.061787486076355\n",
      "Epoch 400, Loss: 4.809684872627258, Final Batch Loss: 0.8277424573898315\n",
      "Epoch 401, Loss: 4.8423808217048645, Final Batch Loss: 0.9749683141708374\n",
      "Epoch 402, Loss: 4.6809937953948975, Final Batch Loss: 0.8764601349830627\n",
      "Epoch 403, Loss: 4.805619120597839, Final Batch Loss: 0.9498692750930786\n",
      "Epoch 404, Loss: 4.732016682624817, Final Batch Loss: 0.9598385691642761\n",
      "Epoch 405, Loss: 4.760756254196167, Final Batch Loss: 0.9915717840194702\n",
      "Epoch 406, Loss: 4.90694135427475, Final Batch Loss: 0.9974985122680664\n",
      "Epoch 407, Loss: 4.80064457654953, Final Batch Loss: 0.9390829205513\n",
      "Epoch 408, Loss: 4.6306740045547485, Final Batch Loss: 0.8134974241256714\n",
      "Epoch 409, Loss: 4.757270693778992, Final Batch Loss: 0.9574039578437805\n",
      "Epoch 410, Loss: 4.696015298366547, Final Batch Loss: 0.8672307729721069\n",
      "Epoch 411, Loss: 4.7942399978637695, Final Batch Loss: 0.9787576794624329\n",
      "Epoch 412, Loss: 4.849126815795898, Final Batch Loss: 1.0230880975723267\n",
      "Epoch 413, Loss: 4.715951561927795, Final Batch Loss: 0.8868236541748047\n",
      "Epoch 414, Loss: 4.705943167209625, Final Batch Loss: 0.918212354183197\n",
      "Epoch 415, Loss: 4.941406428813934, Final Batch Loss: 1.0378782749176025\n",
      "Epoch 416, Loss: 4.725698530673981, Final Batch Loss: 1.0166606903076172\n",
      "Epoch 417, Loss: 4.789044380187988, Final Batch Loss: 0.9022090435028076\n",
      "Epoch 418, Loss: 4.8183236718177795, Final Batch Loss: 0.9350301027297974\n",
      "Epoch 419, Loss: 4.771384537220001, Final Batch Loss: 0.9323118925094604\n",
      "Epoch 420, Loss: 4.695503890514374, Final Batch Loss: 0.8712549805641174\n",
      "Epoch 421, Loss: 4.734345555305481, Final Batch Loss: 1.000564455986023\n",
      "Epoch 422, Loss: 4.791358113288879, Final Batch Loss: 0.9506926536560059\n",
      "Epoch 423, Loss: 4.727108001708984, Final Batch Loss: 0.9694653749465942\n",
      "Epoch 424, Loss: 4.724419355392456, Final Batch Loss: 0.9500722885131836\n",
      "Epoch 425, Loss: 4.568800628185272, Final Batch Loss: 0.9003046751022339\n",
      "Epoch 426, Loss: 4.774497091770172, Final Batch Loss: 0.9533856511116028\n",
      "Epoch 427, Loss: 4.706425189971924, Final Batch Loss: 0.8529613018035889\n",
      "Epoch 428, Loss: 4.732306063175201, Final Batch Loss: 1.0523245334625244\n",
      "Epoch 429, Loss: 4.680629789829254, Final Batch Loss: 0.8865607976913452\n",
      "Epoch 430, Loss: 4.740745961666107, Final Batch Loss: 1.0036486387252808\n",
      "Epoch 431, Loss: 4.699659705162048, Final Batch Loss: 0.9198572635650635\n",
      "Epoch 432, Loss: 4.599747061729431, Final Batch Loss: 0.8383142352104187\n",
      "Epoch 433, Loss: 4.624781906604767, Final Batch Loss: 0.8612121343612671\n",
      "Epoch 434, Loss: 4.738023221492767, Final Batch Loss: 0.9225189089775085\n",
      "Epoch 435, Loss: 4.725108563899994, Final Batch Loss: 0.9335116744041443\n",
      "Epoch 436, Loss: 4.6517099142074585, Final Batch Loss: 0.9472711086273193\n",
      "Epoch 437, Loss: 4.644119083881378, Final Batch Loss: 0.8820633292198181\n",
      "Epoch 438, Loss: 4.699777960777283, Final Batch Loss: 0.9519503116607666\n",
      "Epoch 439, Loss: 4.477244198322296, Final Batch Loss: 0.7596480250358582\n",
      "Epoch 440, Loss: 4.706808567047119, Final Batch Loss: 0.9075449109077454\n",
      "Epoch 441, Loss: 4.546761214733124, Final Batch Loss: 0.786907970905304\n",
      "Epoch 442, Loss: 4.561614990234375, Final Batch Loss: 0.8481356501579285\n",
      "Epoch 443, Loss: 4.672814071178436, Final Batch Loss: 1.0239720344543457\n",
      "Epoch 444, Loss: 4.7766799330711365, Final Batch Loss: 0.9537672400474548\n",
      "Epoch 445, Loss: 4.687440454959869, Final Batch Loss: 0.9199833273887634\n",
      "Epoch 446, Loss: 4.845856487751007, Final Batch Loss: 1.1574313640594482\n",
      "Epoch 447, Loss: 4.643249332904816, Final Batch Loss: 0.9718711972236633\n",
      "Epoch 448, Loss: 4.511430382728577, Final Batch Loss: 0.8101359009742737\n",
      "Epoch 449, Loss: 4.630853354930878, Final Batch Loss: 0.8930966854095459\n",
      "Epoch 450, Loss: 4.7454617619514465, Final Batch Loss: 0.9973166584968567\n",
      "Epoch 451, Loss: 4.600731730461121, Final Batch Loss: 0.9563677310943604\n",
      "Epoch 452, Loss: 4.634121060371399, Final Batch Loss: 0.9351462721824646\n",
      "Epoch 453, Loss: 4.614994466304779, Final Batch Loss: 0.927009105682373\n",
      "Epoch 454, Loss: 4.591968297958374, Final Batch Loss: 0.8836765885353088\n",
      "Epoch 455, Loss: 4.739857614040375, Final Batch Loss: 0.9928074479103088\n",
      "Epoch 456, Loss: 4.630667448043823, Final Batch Loss: 1.0003236532211304\n",
      "Epoch 457, Loss: 4.737774193286896, Final Batch Loss: 1.037699580192566\n",
      "Epoch 458, Loss: 4.591647624969482, Final Batch Loss: 0.9803123474121094\n",
      "Epoch 459, Loss: 4.603236258029938, Final Batch Loss: 0.9154667258262634\n",
      "Epoch 460, Loss: 4.510981440544128, Final Batch Loss: 0.9319334626197815\n",
      "Epoch 461, Loss: 4.575950086116791, Final Batch Loss: 0.8932431936264038\n",
      "Epoch 462, Loss: 4.588341116905212, Final Batch Loss: 0.9400732517242432\n",
      "Epoch 463, Loss: 4.480889618396759, Final Batch Loss: 0.8820925354957581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 464, Loss: 4.522627949714661, Final Batch Loss: 0.916621744632721\n",
      "Epoch 465, Loss: 4.426447153091431, Final Batch Loss: 0.915871798992157\n",
      "Epoch 466, Loss: 4.519600570201874, Final Batch Loss: 0.8338197469711304\n",
      "Epoch 467, Loss: 4.597135782241821, Final Batch Loss: 0.9392505288124084\n",
      "Epoch 468, Loss: 4.377130568027496, Final Batch Loss: 0.8347110152244568\n",
      "Epoch 469, Loss: 4.591818869113922, Final Batch Loss: 0.8208867311477661\n",
      "Epoch 470, Loss: 4.39230340719223, Final Batch Loss: 0.9611878395080566\n",
      "Epoch 471, Loss: 4.447760343551636, Final Batch Loss: 0.7779467105865479\n",
      "Epoch 472, Loss: 4.621935725212097, Final Batch Loss: 1.0190246105194092\n",
      "Epoch 473, Loss: 4.50722998380661, Final Batch Loss: 0.8831626176834106\n",
      "Epoch 474, Loss: 4.489015758037567, Final Batch Loss: 0.897365391254425\n",
      "Epoch 475, Loss: 4.43985915184021, Final Batch Loss: 0.8610922694206238\n",
      "Epoch 476, Loss: 4.43778383731842, Final Batch Loss: 0.9276560544967651\n",
      "Epoch 477, Loss: 4.528825461864471, Final Batch Loss: 0.8863850831985474\n",
      "Epoch 478, Loss: 4.432241141796112, Final Batch Loss: 0.8742305636405945\n",
      "Epoch 479, Loss: 4.364671766757965, Final Batch Loss: 0.769756019115448\n",
      "Epoch 480, Loss: 4.533199310302734, Final Batch Loss: 0.9254351258277893\n",
      "Epoch 481, Loss: 4.558835983276367, Final Batch Loss: 1.0956273078918457\n",
      "Epoch 482, Loss: 4.425347566604614, Final Batch Loss: 0.7448495030403137\n",
      "Epoch 483, Loss: 4.482651650905609, Final Batch Loss: 0.9201635718345642\n",
      "Epoch 484, Loss: 4.360884487628937, Final Batch Loss: 0.909298837184906\n",
      "Epoch 485, Loss: 4.494599282741547, Final Batch Loss: 0.9146824479103088\n",
      "Epoch 486, Loss: 4.317682445049286, Final Batch Loss: 0.8202604651451111\n",
      "Epoch 487, Loss: 4.508500337600708, Final Batch Loss: 0.8710991740226746\n",
      "Epoch 488, Loss: 4.436543703079224, Final Batch Loss: 0.8581454753875732\n",
      "Epoch 489, Loss: 4.384572863578796, Final Batch Loss: 0.7742658853530884\n",
      "Epoch 490, Loss: 4.609527409076691, Final Batch Loss: 1.0864771604537964\n",
      "Epoch 491, Loss: 4.495385944843292, Final Batch Loss: 0.8905050158500671\n",
      "Epoch 492, Loss: 4.4435319900512695, Final Batch Loss: 0.8566386103630066\n",
      "Epoch 493, Loss: 4.44681590795517, Final Batch Loss: 0.8868408203125\n",
      "Epoch 494, Loss: 4.323876798152924, Final Batch Loss: 0.7859617471694946\n",
      "Epoch 495, Loss: 4.534916281700134, Final Batch Loss: 0.9947099089622498\n",
      "Epoch 496, Loss: 4.427396178245544, Final Batch Loss: 0.8469720482826233\n",
      "Epoch 497, Loss: 4.481530666351318, Final Batch Loss: 0.8694003820419312\n",
      "Epoch 498, Loss: 4.488609790802002, Final Batch Loss: 0.8504644632339478\n",
      "Epoch 499, Loss: 4.36910617351532, Final Batch Loss: 0.6968625783920288\n",
      "Epoch 500, Loss: 4.420875668525696, Final Batch Loss: 0.8890975713729858\n",
      "Epoch 501, Loss: 4.556107699871063, Final Batch Loss: 0.8652940988540649\n",
      "Epoch 502, Loss: 4.342216491699219, Final Batch Loss: 0.8828811049461365\n",
      "Epoch 503, Loss: 4.35466331243515, Final Batch Loss: 0.8200438022613525\n",
      "Epoch 504, Loss: 4.4684107303619385, Final Batch Loss: 0.9164531826972961\n",
      "Epoch 505, Loss: 4.423279345035553, Final Batch Loss: 0.976276695728302\n",
      "Epoch 506, Loss: 4.452824115753174, Final Batch Loss: 0.9216749668121338\n",
      "Epoch 507, Loss: 4.388287782669067, Final Batch Loss: 0.9057138562202454\n",
      "Epoch 508, Loss: 4.376525044441223, Final Batch Loss: 0.8361802697181702\n",
      "Epoch 509, Loss: 4.37736451625824, Final Batch Loss: 0.9310605525970459\n",
      "Epoch 510, Loss: 4.2975693345069885, Final Batch Loss: 0.8245303630828857\n",
      "Epoch 511, Loss: 4.527519702911377, Final Batch Loss: 1.000318169593811\n",
      "Epoch 512, Loss: 4.551143765449524, Final Batch Loss: 0.9542418122291565\n",
      "Epoch 513, Loss: 4.488102316856384, Final Batch Loss: 0.9373229146003723\n",
      "Epoch 514, Loss: 4.453702688217163, Final Batch Loss: 1.0185985565185547\n",
      "Epoch 515, Loss: 4.391363263130188, Final Batch Loss: 0.8061159253120422\n",
      "Epoch 516, Loss: 4.425495982170105, Final Batch Loss: 0.9601272344589233\n",
      "Epoch 517, Loss: 4.367859482765198, Final Batch Loss: 0.8590574860572815\n",
      "Epoch 518, Loss: 4.400049388408661, Final Batch Loss: 0.8539699912071228\n",
      "Epoch 519, Loss: 4.448191821575165, Final Batch Loss: 0.8764824867248535\n",
      "Epoch 520, Loss: 4.346092462539673, Final Batch Loss: 0.7404351234436035\n",
      "Epoch 521, Loss: 4.4231332540512085, Final Batch Loss: 0.8412060737609863\n",
      "Epoch 522, Loss: 4.49969220161438, Final Batch Loss: 0.9222455024719238\n",
      "Epoch 523, Loss: 4.249531328678131, Final Batch Loss: 0.8369213938713074\n",
      "Epoch 524, Loss: 4.349193215370178, Final Batch Loss: 0.9333193302154541\n",
      "Epoch 525, Loss: 4.391092479228973, Final Batch Loss: 0.9469185471534729\n",
      "Epoch 526, Loss: 4.31612366437912, Final Batch Loss: 0.8522637486457825\n",
      "Epoch 527, Loss: 4.225323021411896, Final Batch Loss: 0.7537381649017334\n",
      "Epoch 528, Loss: 4.280372142791748, Final Batch Loss: 0.8197019696235657\n",
      "Epoch 529, Loss: 4.488107442855835, Final Batch Loss: 1.0056315660476685\n",
      "Epoch 530, Loss: 4.370692312717438, Final Batch Loss: 0.9302573204040527\n",
      "Epoch 531, Loss: 4.432259678840637, Final Batch Loss: 0.9309519529342651\n",
      "Epoch 532, Loss: 4.256919860839844, Final Batch Loss: 0.8588552474975586\n",
      "Epoch 533, Loss: 4.227473258972168, Final Batch Loss: 0.7238527536392212\n",
      "Epoch 534, Loss: 4.368623077869415, Final Batch Loss: 0.8943033218383789\n",
      "Epoch 535, Loss: 4.37443071603775, Final Batch Loss: 0.8110949397087097\n",
      "Epoch 536, Loss: 4.3742212653160095, Final Batch Loss: 0.9203660488128662\n",
      "Epoch 537, Loss: 4.265139043331146, Final Batch Loss: 0.8899273872375488\n",
      "Epoch 538, Loss: 4.448193609714508, Final Batch Loss: 1.0015528202056885\n",
      "Epoch 539, Loss: 4.301109433174133, Final Batch Loss: 0.857328474521637\n",
      "Epoch 540, Loss: 4.263875186443329, Final Batch Loss: 0.8593414425849915\n",
      "Epoch 541, Loss: 4.382261872291565, Final Batch Loss: 0.9155105352401733\n",
      "Epoch 542, Loss: 4.236394286155701, Final Batch Loss: 0.8620944023132324\n",
      "Epoch 543, Loss: 4.411766052246094, Final Batch Loss: 0.9405326247215271\n",
      "Epoch 544, Loss: 4.382928371429443, Final Batch Loss: 0.9171244502067566\n",
      "Epoch 545, Loss: 4.375941812992096, Final Batch Loss: 0.9709461331367493\n",
      "Epoch 546, Loss: 4.40299779176712, Final Batch Loss: 0.8887558579444885\n",
      "Epoch 547, Loss: 4.222520589828491, Final Batch Loss: 0.8615283966064453\n",
      "Epoch 548, Loss: 4.33377730846405, Final Batch Loss: 0.9070966243743896\n",
      "Epoch 549, Loss: 4.204849064350128, Final Batch Loss: 0.8171533942222595\n",
      "Epoch 550, Loss: 4.2358474135398865, Final Batch Loss: 0.8280087113380432\n",
      "Epoch 551, Loss: 4.388794541358948, Final Batch Loss: 0.8651209473609924\n",
      "Epoch 552, Loss: 4.427106320858002, Final Batch Loss: 0.8197687864303589\n",
      "Epoch 553, Loss: 4.199769496917725, Final Batch Loss: 0.7355039715766907\n",
      "Epoch 554, Loss: 4.156648576259613, Final Batch Loss: 0.7982125878334045\n",
      "Epoch 555, Loss: 4.1085426807403564, Final Batch Loss: 0.7798379063606262\n",
      "Epoch 556, Loss: 4.2239291071891785, Final Batch Loss: 0.8251782059669495\n",
      "Epoch 557, Loss: 4.268274247646332, Final Batch Loss: 0.8040019273757935\n",
      "Epoch 558, Loss: 4.169951915740967, Final Batch Loss: 0.9045312404632568\n",
      "Epoch 559, Loss: 4.096417248249054, Final Batch Loss: 0.7594514489173889\n",
      "Epoch 560, Loss: 4.121393501758575, Final Batch Loss: 0.8179551362991333\n",
      "Epoch 561, Loss: 4.346251845359802, Final Batch Loss: 0.8386403322219849\n",
      "Epoch 562, Loss: 4.23191624879837, Final Batch Loss: 0.9009931087493896\n",
      "Epoch 563, Loss: 4.13005405664444, Final Batch Loss: 0.870049774646759\n",
      "Epoch 564, Loss: 4.217075347900391, Final Batch Loss: 0.8610007762908936\n",
      "Epoch 565, Loss: 4.185230910778046, Final Batch Loss: 0.8642470836639404\n",
      "Epoch 566, Loss: 4.107195019721985, Final Batch Loss: 0.713434100151062\n",
      "Epoch 567, Loss: 4.134590446949005, Final Batch Loss: 0.8185296058654785\n",
      "Epoch 568, Loss: 4.06948721408844, Final Batch Loss: 0.7120359539985657\n",
      "Epoch 569, Loss: 4.061244010925293, Final Batch Loss: 0.7958007454872131\n",
      "Epoch 570, Loss: 4.119465589523315, Final Batch Loss: 0.7607728838920593\n",
      "Epoch 571, Loss: 4.197726130485535, Final Batch Loss: 0.8319715261459351\n",
      "Epoch 572, Loss: 4.137558102607727, Final Batch Loss: 0.8609818816184998\n",
      "Epoch 573, Loss: 4.173487961292267, Final Batch Loss: 0.9170933961868286\n",
      "Epoch 574, Loss: 4.390840172767639, Final Batch Loss: 0.9337782263755798\n",
      "Epoch 575, Loss: 4.238915979862213, Final Batch Loss: 0.9054219126701355\n",
      "Epoch 576, Loss: 4.177338898181915, Final Batch Loss: 0.8339535593986511\n",
      "Epoch 577, Loss: 4.2118136286735535, Final Batch Loss: 0.892302393913269\n",
      "Epoch 578, Loss: 4.087969183921814, Final Batch Loss: 0.8440178036689758\n",
      "Epoch 579, Loss: 4.118885099887848, Final Batch Loss: 0.7953129410743713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 580, Loss: 4.168844044208527, Final Batch Loss: 0.7987053394317627\n",
      "Epoch 581, Loss: 4.262225449085236, Final Batch Loss: 0.8005096316337585\n",
      "Epoch 582, Loss: 4.110136985778809, Final Batch Loss: 0.8350328803062439\n",
      "Epoch 583, Loss: 4.205905497074127, Final Batch Loss: 0.849386990070343\n",
      "Epoch 584, Loss: 4.0391510128974915, Final Batch Loss: 0.8009005188941956\n",
      "Epoch 585, Loss: 4.077816724777222, Final Batch Loss: 0.810789942741394\n",
      "Epoch 586, Loss: 4.172682762145996, Final Batch Loss: 0.8781291842460632\n",
      "Epoch 587, Loss: 4.120654940605164, Final Batch Loss: 0.9087859988212585\n",
      "Epoch 588, Loss: 4.106136858463287, Final Batch Loss: 0.9208748936653137\n",
      "Epoch 589, Loss: 4.20406299829483, Final Batch Loss: 0.8621399402618408\n",
      "Epoch 590, Loss: 4.031780004501343, Final Batch Loss: 0.8086838126182556\n",
      "Epoch 591, Loss: 4.24570232629776, Final Batch Loss: 0.9260871410369873\n",
      "Epoch 592, Loss: 4.141911089420319, Final Batch Loss: 0.8061749339103699\n",
      "Epoch 593, Loss: 4.1237863302230835, Final Batch Loss: 0.8275119662284851\n",
      "Epoch 594, Loss: 4.017986357212067, Final Batch Loss: 0.8902499675750732\n",
      "Epoch 595, Loss: 4.203997552394867, Final Batch Loss: 0.8462318181991577\n",
      "Epoch 596, Loss: 4.107385277748108, Final Batch Loss: 0.8107364773750305\n",
      "Epoch 597, Loss: 4.09155410528183, Final Batch Loss: 0.8835504055023193\n",
      "Epoch 598, Loss: 4.1523367166519165, Final Batch Loss: 0.8406746983528137\n",
      "Epoch 599, Loss: 3.9825425148010254, Final Batch Loss: 0.8312504291534424\n",
      "Epoch 600, Loss: 4.048380553722382, Final Batch Loss: 0.8958947658538818\n",
      "Epoch 601, Loss: 4.031269371509552, Final Batch Loss: 0.822968602180481\n",
      "Epoch 602, Loss: 4.139948725700378, Final Batch Loss: 0.8206576704978943\n",
      "Epoch 603, Loss: 4.075805068016052, Final Batch Loss: 0.8129494190216064\n",
      "Epoch 604, Loss: 4.095505714416504, Final Batch Loss: 0.7843196988105774\n",
      "Epoch 605, Loss: 3.9413859248161316, Final Batch Loss: 0.7654304504394531\n",
      "Epoch 606, Loss: 4.001226603984833, Final Batch Loss: 0.8699501156806946\n",
      "Epoch 607, Loss: 3.92596173286438, Final Batch Loss: 0.7527424097061157\n",
      "Epoch 608, Loss: 3.973116636276245, Final Batch Loss: 0.7473024725914001\n",
      "Epoch 609, Loss: 3.9904013872146606, Final Batch Loss: 0.9467095732688904\n",
      "Epoch 610, Loss: 4.043905735015869, Final Batch Loss: 0.8367448449134827\n",
      "Epoch 611, Loss: 4.086452603340149, Final Batch Loss: 0.7727582454681396\n",
      "Epoch 612, Loss: 4.06297242641449, Final Batch Loss: 0.812356173992157\n",
      "Epoch 613, Loss: 4.058596909046173, Final Batch Loss: 0.7372000813484192\n",
      "Epoch 614, Loss: 3.8877434134483337, Final Batch Loss: 0.6999738216400146\n",
      "Epoch 615, Loss: 4.062492609024048, Final Batch Loss: 0.9215855002403259\n",
      "Epoch 616, Loss: 4.029068112373352, Final Batch Loss: 0.8843330144882202\n",
      "Epoch 617, Loss: 3.995721936225891, Final Batch Loss: 0.7439067959785461\n",
      "Epoch 618, Loss: 3.9666977524757385, Final Batch Loss: 0.845587432384491\n",
      "Epoch 619, Loss: 4.013367235660553, Final Batch Loss: 0.9205514788627625\n",
      "Epoch 620, Loss: 3.9721966981887817, Final Batch Loss: 0.8040984869003296\n",
      "Epoch 621, Loss: 3.911795496940613, Final Batch Loss: 0.7668209075927734\n",
      "Epoch 622, Loss: 4.071361780166626, Final Batch Loss: 0.8106122016906738\n",
      "Epoch 623, Loss: 3.902316629886627, Final Batch Loss: 0.688113272190094\n",
      "Epoch 624, Loss: 4.147644460201263, Final Batch Loss: 0.9183576107025146\n",
      "Epoch 625, Loss: 3.9480178356170654, Final Batch Loss: 0.7809581160545349\n",
      "Epoch 626, Loss: 3.88750684261322, Final Batch Loss: 0.7214354872703552\n",
      "Epoch 627, Loss: 4.0211886167526245, Final Batch Loss: 0.8960354328155518\n",
      "Epoch 628, Loss: 3.9303725957870483, Final Batch Loss: 0.7239541411399841\n",
      "Epoch 629, Loss: 3.9606345295906067, Final Batch Loss: 0.8041374087333679\n",
      "Epoch 630, Loss: 3.9820204377174377, Final Batch Loss: 0.8330351114273071\n",
      "Epoch 631, Loss: 3.9759382009506226, Final Batch Loss: 0.8594236969947815\n",
      "Epoch 632, Loss: 3.9467740058898926, Final Batch Loss: 0.7826276421546936\n",
      "Epoch 633, Loss: 3.914695620536804, Final Batch Loss: 0.7686400413513184\n",
      "Epoch 634, Loss: 3.9971842765808105, Final Batch Loss: 0.7417299747467041\n",
      "Epoch 635, Loss: 3.986932873725891, Final Batch Loss: 0.8073744177818298\n",
      "Epoch 636, Loss: 3.883310377597809, Final Batch Loss: 0.6290224194526672\n",
      "Epoch 637, Loss: 3.928131937980652, Final Batch Loss: 0.8793292045593262\n",
      "Epoch 638, Loss: 3.8990042209625244, Final Batch Loss: 0.7475929260253906\n",
      "Epoch 639, Loss: 3.93050217628479, Final Batch Loss: 0.7573451995849609\n",
      "Epoch 640, Loss: 4.0030946135520935, Final Batch Loss: 0.9054465293884277\n",
      "Epoch 641, Loss: 3.7118260860443115, Final Batch Loss: 0.6998898983001709\n",
      "Epoch 642, Loss: 3.862511992454529, Final Batch Loss: 0.6921729445457458\n",
      "Epoch 643, Loss: 4.023423790931702, Final Batch Loss: 0.8127798438072205\n",
      "Epoch 644, Loss: 4.000296354293823, Final Batch Loss: 0.8170706033706665\n",
      "Epoch 645, Loss: 3.917661964893341, Final Batch Loss: 0.7700892090797424\n",
      "Epoch 646, Loss: 3.794054388999939, Final Batch Loss: 0.7384035587310791\n",
      "Epoch 647, Loss: 3.921412765979767, Final Batch Loss: 0.8696412444114685\n",
      "Epoch 648, Loss: 3.9555054903030396, Final Batch Loss: 0.8477441668510437\n",
      "Epoch 649, Loss: 3.958081603050232, Final Batch Loss: 0.8385224342346191\n",
      "Epoch 650, Loss: 3.8809556365013123, Final Batch Loss: 0.7533092498779297\n",
      "Epoch 651, Loss: 3.930933713912964, Final Batch Loss: 0.7859815955162048\n",
      "Epoch 652, Loss: 3.9463974833488464, Final Batch Loss: 0.864774227142334\n",
      "Epoch 653, Loss: 3.741472899913788, Final Batch Loss: 0.7202737927436829\n",
      "Epoch 654, Loss: 3.955355644226074, Final Batch Loss: 0.862007200717926\n",
      "Epoch 655, Loss: 3.8445593118667603, Final Batch Loss: 0.7546329498291016\n",
      "Epoch 656, Loss: 3.8243702054023743, Final Batch Loss: 0.7706933617591858\n",
      "Epoch 657, Loss: 3.909591019153595, Final Batch Loss: 0.7380260825157166\n",
      "Epoch 658, Loss: 3.9632431864738464, Final Batch Loss: 0.812049388885498\n",
      "Epoch 659, Loss: 3.897434115409851, Final Batch Loss: 0.6948004364967346\n",
      "Epoch 660, Loss: 3.817406177520752, Final Batch Loss: 0.7338146567344666\n",
      "Epoch 661, Loss: 3.9621800780296326, Final Batch Loss: 0.9024698138237\n",
      "Epoch 662, Loss: 3.9139116406440735, Final Batch Loss: 0.7865511775016785\n",
      "Epoch 663, Loss: 3.797967255115509, Final Batch Loss: 0.7404742240905762\n",
      "Epoch 664, Loss: 3.908271014690399, Final Batch Loss: 0.7706444263458252\n",
      "Epoch 665, Loss: 3.8660188913345337, Final Batch Loss: 0.8783797025680542\n",
      "Epoch 666, Loss: 3.796452820301056, Final Batch Loss: 0.7791618704795837\n",
      "Epoch 667, Loss: 3.865863621234894, Final Batch Loss: 0.7692636847496033\n",
      "Epoch 668, Loss: 3.8910948634147644, Final Batch Loss: 0.8375231623649597\n",
      "Epoch 669, Loss: 3.8474280834198, Final Batch Loss: 0.7532849311828613\n",
      "Epoch 670, Loss: 3.761784315109253, Final Batch Loss: 0.6823400855064392\n",
      "Epoch 671, Loss: 3.959088087081909, Final Batch Loss: 0.7955408692359924\n",
      "Epoch 672, Loss: 3.8151469826698303, Final Batch Loss: 0.7491345405578613\n",
      "Epoch 673, Loss: 3.722456693649292, Final Batch Loss: 0.6392162442207336\n",
      "Epoch 674, Loss: 3.83799147605896, Final Batch Loss: 0.8671547174453735\n",
      "Epoch 675, Loss: 3.869924008846283, Final Batch Loss: 0.8216702938079834\n",
      "Epoch 676, Loss: 3.784977376461029, Final Batch Loss: 0.7026591300964355\n",
      "Epoch 677, Loss: 3.7134236097335815, Final Batch Loss: 0.6938716769218445\n",
      "Epoch 678, Loss: 3.7588277459144592, Final Batch Loss: 0.7716259956359863\n",
      "Epoch 679, Loss: 3.8943971991539, Final Batch Loss: 0.7700871825218201\n",
      "Epoch 680, Loss: 4.020311534404755, Final Batch Loss: 0.8051468133926392\n",
      "Epoch 681, Loss: 3.775689661502838, Final Batch Loss: 0.7784246802330017\n",
      "Epoch 682, Loss: 3.860122263431549, Final Batch Loss: 0.8570566773414612\n",
      "Epoch 683, Loss: 3.772948980331421, Final Batch Loss: 0.7579301595687866\n",
      "Epoch 684, Loss: 3.797026038169861, Final Batch Loss: 0.7508729100227356\n",
      "Epoch 685, Loss: 3.898098051548004, Final Batch Loss: 0.8332382440567017\n",
      "Epoch 686, Loss: 3.7465876936912537, Final Batch Loss: 0.6902803778648376\n",
      "Epoch 687, Loss: 3.8628491163253784, Final Batch Loss: 0.7132009863853455\n",
      "Epoch 688, Loss: 3.7775779962539673, Final Batch Loss: 0.8015395998954773\n",
      "Epoch 689, Loss: 3.7674193382263184, Final Batch Loss: 0.7621672749519348\n",
      "Epoch 690, Loss: 3.8377674221992493, Final Batch Loss: 0.8898756504058838\n",
      "Epoch 691, Loss: 3.7526060342788696, Final Batch Loss: 0.8336583375930786\n",
      "Epoch 692, Loss: 3.6399673223495483, Final Batch Loss: 0.6753273606300354\n",
      "Epoch 693, Loss: 3.7813905477523804, Final Batch Loss: 0.8105730414390564\n",
      "Epoch 694, Loss: 3.813297390937805, Final Batch Loss: 0.7824969291687012\n",
      "Epoch 695, Loss: 3.6349693536758423, Final Batch Loss: 0.6806878447532654\n",
      "Epoch 696, Loss: 3.7911908626556396, Final Batch Loss: 0.8455848693847656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 697, Loss: 3.716529607772827, Final Batch Loss: 0.6938865780830383\n",
      "Epoch 698, Loss: 3.532962918281555, Final Batch Loss: 0.6328360438346863\n",
      "Epoch 699, Loss: 3.653672754764557, Final Batch Loss: 0.7012669444084167\n",
      "Epoch 700, Loss: 3.7830817699432373, Final Batch Loss: 0.6871197819709778\n",
      "Epoch 701, Loss: 3.6001639366149902, Final Batch Loss: 0.7293767333030701\n",
      "Epoch 702, Loss: 3.6555724143981934, Final Batch Loss: 0.7444049715995789\n",
      "Epoch 703, Loss: 3.68376886844635, Final Batch Loss: 0.7265691757202148\n",
      "Epoch 704, Loss: 3.8844282627105713, Final Batch Loss: 0.8327984809875488\n",
      "Epoch 705, Loss: 3.716096878051758, Final Batch Loss: 0.8260348439216614\n",
      "Epoch 706, Loss: 3.723373234272003, Final Batch Loss: 0.7379162907600403\n",
      "Epoch 707, Loss: 3.5822678208351135, Final Batch Loss: 0.6884417533874512\n",
      "Epoch 708, Loss: 3.704018533229828, Final Batch Loss: 0.7983202934265137\n",
      "Epoch 709, Loss: 3.864307999610901, Final Batch Loss: 0.8747301697731018\n",
      "Epoch 710, Loss: 3.6500054001808167, Final Batch Loss: 0.6750335693359375\n",
      "Epoch 711, Loss: 3.8923134207725525, Final Batch Loss: 0.8260581493377686\n",
      "Epoch 712, Loss: 3.657658874988556, Final Batch Loss: 0.7251242995262146\n",
      "Epoch 713, Loss: 3.8963456749916077, Final Batch Loss: 0.7945742607116699\n",
      "Epoch 714, Loss: 3.6909311413764954, Final Batch Loss: 0.7350209355354309\n",
      "Epoch 715, Loss: 3.6816269755363464, Final Batch Loss: 0.8206366896629333\n",
      "Epoch 716, Loss: 3.604807496070862, Final Batch Loss: 0.7158164978027344\n",
      "Epoch 717, Loss: 3.625476658344269, Final Batch Loss: 0.7202014327049255\n",
      "Epoch 718, Loss: 3.745416283607483, Final Batch Loss: 0.8490778207778931\n",
      "Epoch 719, Loss: 3.547245144844055, Final Batch Loss: 0.8023253083229065\n",
      "Epoch 720, Loss: 3.71599143743515, Final Batch Loss: 0.6584592461585999\n",
      "Epoch 721, Loss: 3.7492100596427917, Final Batch Loss: 0.829839289188385\n",
      "Epoch 722, Loss: 3.624642252922058, Final Batch Loss: 0.7089794874191284\n",
      "Epoch 723, Loss: 3.5060022473335266, Final Batch Loss: 0.6814576387405396\n",
      "Epoch 724, Loss: 3.693904399871826, Final Batch Loss: 0.8055137991905212\n",
      "Epoch 725, Loss: 3.717372715473175, Final Batch Loss: 0.7578933238983154\n",
      "Epoch 726, Loss: 3.6972559094429016, Final Batch Loss: 0.6565815806388855\n",
      "Epoch 727, Loss: 3.6867581605911255, Final Batch Loss: 0.7370358109474182\n",
      "Epoch 728, Loss: 3.6968435049057007, Final Batch Loss: 0.6605067253112793\n",
      "Epoch 729, Loss: 3.6568769216537476, Final Batch Loss: 0.7700276970863342\n",
      "Epoch 730, Loss: 3.57980877161026, Final Batch Loss: 0.6524488925933838\n",
      "Epoch 731, Loss: 3.5993438959121704, Final Batch Loss: 0.6948217749595642\n",
      "Epoch 732, Loss: 3.82900470495224, Final Batch Loss: 0.772598922252655\n",
      "Epoch 733, Loss: 3.727042317390442, Final Batch Loss: 0.7985084652900696\n",
      "Epoch 734, Loss: 3.5781850814819336, Final Batch Loss: 0.6500794887542725\n",
      "Epoch 735, Loss: 3.6430686712265015, Final Batch Loss: 0.6731283068656921\n",
      "Epoch 736, Loss: 3.5421107411384583, Final Batch Loss: 0.7566038966178894\n",
      "Epoch 737, Loss: 3.60232675075531, Final Batch Loss: 0.6746517419815063\n",
      "Epoch 738, Loss: 3.6856529116630554, Final Batch Loss: 0.775578498840332\n",
      "Epoch 739, Loss: 3.528726637363434, Final Batch Loss: 0.6570898294448853\n",
      "Epoch 740, Loss: 3.7116276025772095, Final Batch Loss: 0.8249291777610779\n",
      "Epoch 741, Loss: 3.5810269117355347, Final Batch Loss: 0.6362140774726868\n",
      "Epoch 742, Loss: 3.574755072593689, Final Batch Loss: 0.6830005049705505\n",
      "Epoch 743, Loss: 3.4841058254241943, Final Batch Loss: 0.6318783760070801\n",
      "Epoch 744, Loss: 3.5640947222709656, Final Batch Loss: 0.7268577814102173\n",
      "Epoch 745, Loss: 3.640529453754425, Final Batch Loss: 0.6859726309776306\n",
      "Epoch 746, Loss: 3.531486690044403, Final Batch Loss: 0.7363568544387817\n",
      "Epoch 747, Loss: 3.6259589195251465, Final Batch Loss: 0.8077225089073181\n",
      "Epoch 748, Loss: 3.555611252784729, Final Batch Loss: 0.7175434231758118\n",
      "Epoch 749, Loss: 3.5557844042778015, Final Batch Loss: 0.6479297280311584\n",
      "Epoch 750, Loss: 3.6096739768981934, Final Batch Loss: 0.7177191972732544\n",
      "Epoch 751, Loss: 3.742744207382202, Final Batch Loss: 0.7955930233001709\n",
      "Epoch 752, Loss: 3.472003996372223, Final Batch Loss: 0.6791138052940369\n",
      "Epoch 753, Loss: 3.5992193818092346, Final Batch Loss: 0.7572430968284607\n",
      "Epoch 754, Loss: 3.528534233570099, Final Batch Loss: 0.7213242053985596\n",
      "Epoch 755, Loss: 3.526769995689392, Final Batch Loss: 0.6432013511657715\n",
      "Epoch 756, Loss: 3.6055705547332764, Final Batch Loss: 0.7630312442779541\n",
      "Epoch 757, Loss: 3.5977609753608704, Final Batch Loss: 0.6825748682022095\n",
      "Epoch 758, Loss: 3.4235870838165283, Final Batch Loss: 0.589577853679657\n",
      "Epoch 759, Loss: 3.5365666151046753, Final Batch Loss: 0.5869147181510925\n",
      "Epoch 760, Loss: 3.5651118755340576, Final Batch Loss: 0.7547856569290161\n",
      "Epoch 761, Loss: 3.5728474855422974, Final Batch Loss: 0.7222287654876709\n",
      "Epoch 762, Loss: 3.5911033153533936, Final Batch Loss: 0.6514834761619568\n",
      "Epoch 763, Loss: 3.6521324515342712, Final Batch Loss: 0.7619211077690125\n",
      "Epoch 764, Loss: 3.5567805767059326, Final Batch Loss: 0.8171684741973877\n",
      "Epoch 765, Loss: 3.500684082508087, Final Batch Loss: 0.6417026519775391\n",
      "Epoch 766, Loss: 3.625067889690399, Final Batch Loss: 0.674483597278595\n",
      "Epoch 767, Loss: 3.767875850200653, Final Batch Loss: 0.8579642176628113\n",
      "Epoch 768, Loss: 3.5105875730514526, Final Batch Loss: 0.6111510396003723\n",
      "Epoch 769, Loss: 3.568250596523285, Final Batch Loss: 0.7007840275764465\n",
      "Epoch 770, Loss: 3.389270007610321, Final Batch Loss: 0.6389725804328918\n",
      "Epoch 771, Loss: 3.6578513383865356, Final Batch Loss: 0.7542929649353027\n",
      "Epoch 772, Loss: 3.6870036125183105, Final Batch Loss: 0.9610593318939209\n",
      "Epoch 773, Loss: 3.3773210644721985, Final Batch Loss: 0.6047325134277344\n",
      "Epoch 774, Loss: 3.581173062324524, Final Batch Loss: 0.7244552373886108\n",
      "Epoch 775, Loss: 3.4828580617904663, Final Batch Loss: 0.7099571824073792\n",
      "Epoch 776, Loss: 3.5995105504989624, Final Batch Loss: 0.5879548192024231\n",
      "Epoch 777, Loss: 3.4993779063224792, Final Batch Loss: 0.814154863357544\n",
      "Epoch 778, Loss: 3.49490624666214, Final Batch Loss: 0.6725882887840271\n",
      "Epoch 779, Loss: 3.42995423078537, Final Batch Loss: 0.7405100464820862\n",
      "Epoch 780, Loss: 3.5115107893943787, Final Batch Loss: 0.6736071109771729\n",
      "Epoch 781, Loss: 3.550559401512146, Final Batch Loss: 0.7409120202064514\n",
      "Epoch 782, Loss: 3.4726754426956177, Final Batch Loss: 0.6735363602638245\n",
      "Epoch 783, Loss: 3.614221751689911, Final Batch Loss: 0.8428232669830322\n",
      "Epoch 784, Loss: 3.5044647455215454, Final Batch Loss: 0.6937212347984314\n",
      "Epoch 785, Loss: 3.62595933675766, Final Batch Loss: 0.6089063882827759\n",
      "Epoch 786, Loss: 3.4383952021598816, Final Batch Loss: 0.6402007937431335\n",
      "Epoch 787, Loss: 3.4753089547157288, Final Batch Loss: 0.7363120913505554\n",
      "Epoch 788, Loss: 3.5445134043693542, Final Batch Loss: 0.7618162631988525\n",
      "Epoch 789, Loss: 3.4877254962921143, Final Batch Loss: 0.6845868825912476\n",
      "Epoch 790, Loss: 3.6393538117408752, Final Batch Loss: 0.7655029892921448\n",
      "Epoch 791, Loss: 3.5336681604385376, Final Batch Loss: 0.6461912393569946\n",
      "Epoch 792, Loss: 3.5118414163589478, Final Batch Loss: 0.6795464158058167\n",
      "Epoch 793, Loss: 3.4447264671325684, Final Batch Loss: 0.7296766638755798\n",
      "Epoch 794, Loss: 3.4975557923316956, Final Batch Loss: 0.6941519975662231\n",
      "Epoch 795, Loss: 3.530778169631958, Final Batch Loss: 0.6877981424331665\n",
      "Epoch 796, Loss: 3.4378104209899902, Final Batch Loss: 0.7057138085365295\n",
      "Epoch 797, Loss: 3.467089533805847, Final Batch Loss: 0.7389481663703918\n",
      "Epoch 798, Loss: 3.4529284834861755, Final Batch Loss: 0.7624921202659607\n",
      "Epoch 799, Loss: 3.5987011790275574, Final Batch Loss: 0.656210720539093\n",
      "Epoch 800, Loss: 3.6679914593696594, Final Batch Loss: 0.7169712781906128\n",
      "Epoch 801, Loss: 3.311919152736664, Final Batch Loss: 0.6706454753875732\n",
      "Epoch 802, Loss: 3.446401596069336, Final Batch Loss: 0.705340564250946\n",
      "Epoch 803, Loss: 3.50094336271286, Final Batch Loss: 0.7229008674621582\n",
      "Epoch 804, Loss: 3.5475035309791565, Final Batch Loss: 0.6944475769996643\n",
      "Epoch 805, Loss: 3.4655858278274536, Final Batch Loss: 0.8256363272666931\n",
      "Epoch 806, Loss: 3.330271005630493, Final Batch Loss: 0.6365688443183899\n",
      "Epoch 807, Loss: 3.4971835017204285, Final Batch Loss: 0.6907680630683899\n",
      "Epoch 808, Loss: 3.3792041540145874, Final Batch Loss: 0.6338091492652893\n",
      "Epoch 809, Loss: 3.3472941517829895, Final Batch Loss: 0.6415840983390808\n",
      "Epoch 810, Loss: 3.49136883020401, Final Batch Loss: 0.6982168555259705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 811, Loss: 3.4102506041526794, Final Batch Loss: 0.6763964891433716\n",
      "Epoch 812, Loss: 3.6843689680099487, Final Batch Loss: 0.6522147059440613\n",
      "Epoch 813, Loss: 3.3527416586875916, Final Batch Loss: 0.5590016841888428\n",
      "Epoch 814, Loss: 3.231222152709961, Final Batch Loss: 0.673271894454956\n",
      "Epoch 815, Loss: 3.495944082736969, Final Batch Loss: 0.7170877456665039\n",
      "Epoch 816, Loss: 3.4352234601974487, Final Batch Loss: 0.7281908988952637\n",
      "Epoch 817, Loss: 3.461795687675476, Final Batch Loss: 0.6663221120834351\n",
      "Epoch 818, Loss: 3.42517226934433, Final Batch Loss: 0.7432747483253479\n",
      "Epoch 819, Loss: 3.3944461941719055, Final Batch Loss: 0.5950057506561279\n",
      "Epoch 820, Loss: 3.3791274428367615, Final Batch Loss: 0.6083627343177795\n",
      "Epoch 821, Loss: 3.3810354471206665, Final Batch Loss: 0.5952372550964355\n",
      "Epoch 822, Loss: 3.3910977244377136, Final Batch Loss: 0.7924555540084839\n",
      "Epoch 823, Loss: 3.5144628286361694, Final Batch Loss: 0.6572515964508057\n",
      "Epoch 824, Loss: 3.517727255821228, Final Batch Loss: 0.6967851519584656\n",
      "Epoch 825, Loss: 3.376815438270569, Final Batch Loss: 0.6889135837554932\n",
      "Epoch 826, Loss: 3.398792862892151, Final Batch Loss: 0.6657393574714661\n",
      "Epoch 827, Loss: 3.505762040615082, Final Batch Loss: 0.6544462442398071\n",
      "Epoch 828, Loss: 3.435912847518921, Final Batch Loss: 0.7932813167572021\n",
      "Epoch 829, Loss: 3.5128178000450134, Final Batch Loss: 0.7162184715270996\n",
      "Epoch 830, Loss: 3.3659642338752747, Final Batch Loss: 0.7405281066894531\n",
      "Epoch 831, Loss: 3.43446946144104, Final Batch Loss: 0.6915414929389954\n",
      "Epoch 832, Loss: 3.2714362740516663, Final Batch Loss: 0.5689940452575684\n",
      "Epoch 833, Loss: 3.290139079093933, Final Batch Loss: 0.5594767332077026\n",
      "Epoch 834, Loss: 3.428210496902466, Final Batch Loss: 0.8059271574020386\n",
      "Epoch 835, Loss: 3.321968197822571, Final Batch Loss: 0.6451197266578674\n",
      "Epoch 836, Loss: 3.2942572236061096, Final Batch Loss: 0.5806962847709656\n",
      "Epoch 837, Loss: 3.3919056057929993, Final Batch Loss: 0.7683958411216736\n",
      "Epoch 838, Loss: 3.2995067834854126, Final Batch Loss: 0.6759281754493713\n",
      "Epoch 839, Loss: 3.329153001308441, Final Batch Loss: 0.636894702911377\n",
      "Epoch 840, Loss: 3.357886254787445, Final Batch Loss: 0.7310568690299988\n",
      "Epoch 841, Loss: 3.1821532249450684, Final Batch Loss: 0.6347904205322266\n",
      "Epoch 842, Loss: 3.3688700199127197, Final Batch Loss: 0.7395288348197937\n",
      "Epoch 843, Loss: 3.4794154167175293, Final Batch Loss: 0.7106068134307861\n",
      "Epoch 844, Loss: 3.299866557121277, Final Batch Loss: 0.6171057224273682\n",
      "Epoch 845, Loss: 3.2776865363121033, Final Batch Loss: 0.5880693793296814\n",
      "Epoch 846, Loss: 3.3725146651268005, Final Batch Loss: 0.7111412882804871\n",
      "Epoch 847, Loss: 3.3923937678337097, Final Batch Loss: 0.6558026075363159\n",
      "Epoch 848, Loss: 3.235085904598236, Final Batch Loss: 0.6231523752212524\n",
      "Epoch 849, Loss: 3.32566636800766, Final Batch Loss: 0.6599745154380798\n",
      "Epoch 850, Loss: 3.4076490998268127, Final Batch Loss: 0.6973307132720947\n",
      "Epoch 851, Loss: 3.405215322971344, Final Batch Loss: 0.8255095481872559\n",
      "Epoch 852, Loss: 3.297218441963196, Final Batch Loss: 0.5762409567832947\n",
      "Epoch 853, Loss: 3.4290125370025635, Final Batch Loss: 0.6359612941741943\n",
      "Epoch 854, Loss: 3.2649898529052734, Final Batch Loss: 0.6463690400123596\n",
      "Epoch 855, Loss: 3.2015982270240784, Final Batch Loss: 0.6485198736190796\n",
      "Epoch 856, Loss: 3.2033493518829346, Final Batch Loss: 0.6597616076469421\n",
      "Epoch 857, Loss: 3.424186885356903, Final Batch Loss: 0.6350141763687134\n",
      "Epoch 858, Loss: 3.322958827018738, Final Batch Loss: 0.6644303202629089\n",
      "Epoch 859, Loss: 3.4255999326705933, Final Batch Loss: 0.6274369359016418\n",
      "Epoch 860, Loss: 3.3075586557388306, Final Batch Loss: 0.591396152973175\n",
      "Epoch 861, Loss: 3.261620581150055, Final Batch Loss: 0.6745011210441589\n",
      "Epoch 862, Loss: 3.1973581314086914, Final Batch Loss: 0.573902428150177\n",
      "Epoch 863, Loss: 3.315402030944824, Final Batch Loss: 0.6142710447311401\n",
      "Epoch 864, Loss: 3.360211670398712, Final Batch Loss: 0.7363094687461853\n",
      "Epoch 865, Loss: 3.304451048374176, Final Batch Loss: 0.6477360725402832\n",
      "Epoch 866, Loss: 3.2148216366767883, Final Batch Loss: 0.6429147720336914\n",
      "Epoch 867, Loss: 3.348868429660797, Final Batch Loss: 0.7543585300445557\n",
      "Epoch 868, Loss: 3.389929175376892, Final Batch Loss: 0.7758716344833374\n",
      "Epoch 869, Loss: 3.3005473613739014, Final Batch Loss: 0.6670942306518555\n",
      "Epoch 870, Loss: 3.370171904563904, Final Batch Loss: 0.6221126317977905\n",
      "Epoch 871, Loss: 3.272692561149597, Final Batch Loss: 0.5488828420639038\n",
      "Epoch 872, Loss: 3.2418829798698425, Final Batch Loss: 0.7213125824928284\n",
      "Epoch 873, Loss: 3.432282507419586, Final Batch Loss: 0.7050033807754517\n",
      "Epoch 874, Loss: 3.3398929834365845, Final Batch Loss: 0.6271212697029114\n",
      "Epoch 875, Loss: 3.142020523548126, Final Batch Loss: 0.5728090405464172\n",
      "Epoch 876, Loss: 3.192326068878174, Final Batch Loss: 0.5690357685089111\n",
      "Epoch 877, Loss: 3.273720383644104, Final Batch Loss: 0.5692383050918579\n",
      "Epoch 878, Loss: 3.3708799481391907, Final Batch Loss: 0.67831951379776\n",
      "Epoch 879, Loss: 3.18636691570282, Final Batch Loss: 0.5576402544975281\n",
      "Epoch 880, Loss: 3.424535810947418, Final Batch Loss: 0.7463371753692627\n",
      "Epoch 881, Loss: 3.2639659643173218, Final Batch Loss: 0.5219754576683044\n",
      "Epoch 882, Loss: 3.2255725264549255, Final Batch Loss: 0.6113104820251465\n",
      "Epoch 883, Loss: 3.265090823173523, Final Batch Loss: 0.7197903394699097\n",
      "Epoch 884, Loss: 3.2817073464393616, Final Batch Loss: 0.6299225091934204\n",
      "Epoch 885, Loss: 3.3827872276306152, Final Batch Loss: 0.7657989859580994\n",
      "Epoch 886, Loss: 3.4615378379821777, Final Batch Loss: 0.6000396013259888\n",
      "Epoch 887, Loss: 3.1975521445274353, Final Batch Loss: 0.5791507959365845\n",
      "Epoch 888, Loss: 3.3205162286758423, Final Batch Loss: 0.6447935700416565\n",
      "Epoch 889, Loss: 3.1665889024734497, Final Batch Loss: 0.5920533537864685\n",
      "Epoch 890, Loss: 3.319741904735565, Final Batch Loss: 0.6845006346702576\n",
      "Epoch 891, Loss: 3.0724398493766785, Final Batch Loss: 0.5202158689498901\n",
      "Epoch 892, Loss: 3.4103519320487976, Final Batch Loss: 0.5994641780853271\n",
      "Epoch 893, Loss: 3.3813451528549194, Final Batch Loss: 0.6968545317649841\n",
      "Epoch 894, Loss: 3.1086032390594482, Final Batch Loss: 0.6073234677314758\n",
      "Epoch 895, Loss: 3.169251799583435, Final Batch Loss: 0.5548025965690613\n",
      "Epoch 896, Loss: 3.112446963787079, Final Batch Loss: 0.6480375528335571\n",
      "Epoch 897, Loss: 3.2737717628479004, Final Batch Loss: 0.628842294216156\n",
      "Epoch 898, Loss: 3.1728623509407043, Final Batch Loss: 0.6161648631095886\n",
      "Epoch 899, Loss: 3.242962598800659, Final Batch Loss: 0.6927816867828369\n",
      "Epoch 900, Loss: 3.065109372138977, Final Batch Loss: 0.6216606497764587\n",
      "Epoch 901, Loss: 3.0610188245773315, Final Batch Loss: 0.578916072845459\n",
      "Epoch 902, Loss: 3.0936323404312134, Final Batch Loss: 0.5445327758789062\n",
      "Epoch 903, Loss: 3.1213083267211914, Final Batch Loss: 0.5561665296554565\n",
      "Epoch 904, Loss: 3.2705758213996887, Final Batch Loss: 0.7228171229362488\n",
      "Epoch 905, Loss: 3.267007350921631, Final Batch Loss: 0.7411834597587585\n",
      "Epoch 906, Loss: 3.2969623804092407, Final Batch Loss: 0.6962471008300781\n",
      "Epoch 907, Loss: 3.156955361366272, Final Batch Loss: 0.5845944285392761\n",
      "Epoch 908, Loss: 3.236203372478485, Final Batch Loss: 0.5445181131362915\n",
      "Epoch 909, Loss: 3.2648539543151855, Final Batch Loss: 0.6568489074707031\n",
      "Epoch 910, Loss: 3.257465958595276, Final Batch Loss: 0.617677628993988\n",
      "Epoch 911, Loss: 3.134149730205536, Final Batch Loss: 0.6992090940475464\n",
      "Epoch 912, Loss: 3.295121490955353, Final Batch Loss: 0.76222163438797\n",
      "Epoch 913, Loss: 3.311530292034149, Final Batch Loss: 0.7067905068397522\n",
      "Epoch 914, Loss: 3.255251705646515, Final Batch Loss: 0.6541075706481934\n",
      "Epoch 915, Loss: 3.3137388229370117, Final Batch Loss: 0.7075546979904175\n",
      "Epoch 916, Loss: 3.059560716152191, Final Batch Loss: 0.6139415502548218\n",
      "Epoch 917, Loss: 3.3353625535964966, Final Batch Loss: 0.6708993911743164\n",
      "Epoch 918, Loss: 3.2512189745903015, Final Batch Loss: 0.6993097066879272\n",
      "Epoch 919, Loss: 3.1534228324890137, Final Batch Loss: 0.5523109436035156\n",
      "Epoch 920, Loss: 3.0372630953788757, Final Batch Loss: 0.66874760389328\n",
      "Epoch 921, Loss: 3.3027977347373962, Final Batch Loss: 0.6087567806243896\n",
      "Epoch 922, Loss: 2.9830374717712402, Final Batch Loss: 0.5081672072410583\n",
      "Epoch 923, Loss: 3.2719566822052, Final Batch Loss: 0.7427753806114197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 924, Loss: 2.974147766828537, Final Batch Loss: 0.4335218369960785\n",
      "Epoch 925, Loss: 3.1435129642486572, Final Batch Loss: 0.5847077369689941\n",
      "Epoch 926, Loss: 3.200652778148651, Final Batch Loss: 0.5956541299819946\n",
      "Epoch 927, Loss: 3.195224642753601, Final Batch Loss: 0.6233301758766174\n",
      "Epoch 928, Loss: 3.2413281202316284, Final Batch Loss: 0.6598315834999084\n",
      "Epoch 929, Loss: 3.0698843002319336, Final Batch Loss: 0.612053632736206\n",
      "Epoch 930, Loss: 3.178348958492279, Final Batch Loss: 0.6541647911071777\n",
      "Epoch 931, Loss: 3.122186928987503, Final Batch Loss: 0.4461419880390167\n",
      "Epoch 932, Loss: 3.0935729146003723, Final Batch Loss: 0.678164541721344\n",
      "Epoch 933, Loss: 2.992961823940277, Final Batch Loss: 0.541286051273346\n",
      "Epoch 934, Loss: 3.10811710357666, Final Batch Loss: 0.5373117327690125\n",
      "Epoch 935, Loss: 3.1190337538719177, Final Batch Loss: 0.5753951072692871\n",
      "Epoch 936, Loss: 3.121927559375763, Final Batch Loss: 0.6550750732421875\n",
      "Epoch 937, Loss: 3.0916406512260437, Final Batch Loss: 0.5899958610534668\n",
      "Epoch 938, Loss: 3.060127079486847, Final Batch Loss: 0.6379960179328918\n",
      "Epoch 939, Loss: 3.145733058452606, Final Batch Loss: 0.6918351650238037\n",
      "Epoch 940, Loss: 3.096088707447052, Final Batch Loss: 0.5954234004020691\n",
      "Epoch 941, Loss: 3.2056577801704407, Final Batch Loss: 0.6524358987808228\n",
      "Epoch 942, Loss: 3.0896155834198, Final Batch Loss: 0.49530982971191406\n",
      "Epoch 943, Loss: 3.096115529537201, Final Batch Loss: 0.6764950156211853\n",
      "Epoch 944, Loss: 3.0805299878120422, Final Batch Loss: 0.6438606977462769\n",
      "Epoch 945, Loss: 3.132599711418152, Final Batch Loss: 0.5338935852050781\n",
      "Epoch 946, Loss: 3.1638395190238953, Final Batch Loss: 0.6673742532730103\n",
      "Epoch 947, Loss: 3.1440062522888184, Final Batch Loss: 0.6469329595565796\n",
      "Epoch 948, Loss: 3.1795886158943176, Final Batch Loss: 0.6671885848045349\n",
      "Epoch 949, Loss: 3.238640308380127, Final Batch Loss: 0.7013759016990662\n",
      "Epoch 950, Loss: 3.075063943862915, Final Batch Loss: 0.7281385064125061\n",
      "Epoch 951, Loss: 3.378016233444214, Final Batch Loss: 0.863329291343689\n",
      "Epoch 952, Loss: 3.0435610711574554, Final Batch Loss: 0.4843468964099884\n",
      "Epoch 953, Loss: 3.2670158743858337, Final Batch Loss: 0.6383558511734009\n",
      "Epoch 954, Loss: 3.0827536582946777, Final Batch Loss: 0.5647885799407959\n",
      "Epoch 955, Loss: 3.147150695323944, Final Batch Loss: 0.6201859712600708\n",
      "Epoch 956, Loss: 3.13831102848053, Final Batch Loss: 0.5621760487556458\n",
      "Epoch 957, Loss: 3.0473386645317078, Final Batch Loss: 0.5887443423271179\n",
      "Epoch 958, Loss: 3.085889518260956, Final Batch Loss: 0.5555838942527771\n",
      "Epoch 959, Loss: 3.0798377990722656, Final Batch Loss: 0.557864785194397\n",
      "Epoch 960, Loss: 3.1763965487480164, Final Batch Loss: 0.6393353343009949\n",
      "Epoch 961, Loss: 3.0613794922828674, Final Batch Loss: 0.5288025736808777\n",
      "Epoch 962, Loss: 2.8895804286003113, Final Batch Loss: 0.5749803781509399\n",
      "Epoch 963, Loss: 3.0206408500671387, Final Batch Loss: 0.5907620787620544\n",
      "Epoch 964, Loss: 3.1401957273483276, Final Batch Loss: 0.6810910701751709\n",
      "Epoch 965, Loss: 3.089605987071991, Final Batch Loss: 0.570627748966217\n",
      "Epoch 966, Loss: 3.176885426044464, Final Batch Loss: 0.6636875867843628\n",
      "Epoch 967, Loss: 3.011330544948578, Final Batch Loss: 0.5998454093933105\n",
      "Epoch 968, Loss: 3.0344273447990417, Final Batch Loss: 0.6829613447189331\n",
      "Epoch 969, Loss: 3.0387825965881348, Final Batch Loss: 0.6313480138778687\n",
      "Epoch 970, Loss: 3.0710052847862244, Final Batch Loss: 0.5776005983352661\n",
      "Epoch 971, Loss: 3.1058766841888428, Final Batch Loss: 0.6224178671836853\n",
      "Epoch 972, Loss: 3.0567301511764526, Final Batch Loss: 0.6771538853645325\n",
      "Epoch 973, Loss: 3.072930335998535, Final Batch Loss: 0.5389100909233093\n",
      "Epoch 974, Loss: 3.059435188770294, Final Batch Loss: 0.6319672465324402\n",
      "Epoch 975, Loss: 2.9512133598327637, Final Batch Loss: 0.567434549331665\n",
      "Epoch 976, Loss: 3.0407777428627014, Final Batch Loss: 0.6740961670875549\n",
      "Epoch 977, Loss: 3.032645285129547, Final Batch Loss: 0.581796407699585\n",
      "Epoch 978, Loss: 2.9843244552612305, Final Batch Loss: 0.5391595959663391\n",
      "Epoch 979, Loss: 2.914092481136322, Final Batch Loss: 0.5415895581245422\n",
      "Epoch 980, Loss: 2.930519461631775, Final Batch Loss: 0.5397909283638\n",
      "Epoch 981, Loss: 3.0539100766181946, Final Batch Loss: 0.6062473058700562\n",
      "Epoch 982, Loss: 3.079294979572296, Final Batch Loss: 0.616347074508667\n",
      "Epoch 983, Loss: 3.1259116530418396, Final Batch Loss: 0.6094455718994141\n",
      "Epoch 984, Loss: 3.23349130153656, Final Batch Loss: 0.8289353251457214\n",
      "Epoch 985, Loss: 3.038316786289215, Final Batch Loss: 0.6279009580612183\n",
      "Epoch 986, Loss: 3.0311065912246704, Final Batch Loss: 0.7310879826545715\n",
      "Epoch 987, Loss: 3.011577934026718, Final Batch Loss: 0.654226541519165\n",
      "Epoch 988, Loss: 3.143075406551361, Final Batch Loss: 0.5792577862739563\n",
      "Epoch 989, Loss: 2.984683871269226, Final Batch Loss: 0.6065062880516052\n",
      "Epoch 990, Loss: 3.0390268564224243, Final Batch Loss: 0.6497111320495605\n",
      "Epoch 991, Loss: 3.0154263377189636, Final Batch Loss: 0.5102614164352417\n",
      "Epoch 992, Loss: 3.0237817764282227, Final Batch Loss: 0.5497386455535889\n",
      "Epoch 993, Loss: 3.003078520298004, Final Batch Loss: 0.5552865862846375\n",
      "Epoch 994, Loss: 3.0495700240135193, Final Batch Loss: 0.5732935667037964\n",
      "Epoch 995, Loss: 3.0099302530288696, Final Batch Loss: 0.6289239525794983\n",
      "Epoch 996, Loss: 2.9625872373580933, Final Batch Loss: 0.5070073008537292\n",
      "Epoch 997, Loss: 2.9739763736724854, Final Batch Loss: 0.5748923420906067\n",
      "Epoch 998, Loss: 3.066505491733551, Final Batch Loss: 0.5922786593437195\n",
      "Epoch 999, Loss: 3.1279488801956177, Final Batch Loss: 0.6287263035774231\n",
      "Epoch 1000, Loss: 2.9645876288414, Final Batch Loss: 0.5473163723945618\n",
      "Epoch 1001, Loss: 2.8443684577941895, Final Batch Loss: 0.5372920632362366\n",
      "Epoch 1002, Loss: 3.0510915517807007, Final Batch Loss: 0.6018847823143005\n",
      "Epoch 1003, Loss: 2.939622402191162, Final Batch Loss: 0.6365371942520142\n",
      "Epoch 1004, Loss: 3.018323540687561, Final Batch Loss: 0.6036667823791504\n",
      "Epoch 1005, Loss: 3.026783049106598, Final Batch Loss: 0.669475793838501\n",
      "Epoch 1006, Loss: 2.9911099672317505, Final Batch Loss: 0.6212091445922852\n",
      "Epoch 1007, Loss: 3.169464409351349, Final Batch Loss: 0.6772708296775818\n",
      "Epoch 1008, Loss: 2.9614790081977844, Final Batch Loss: 0.5506949424743652\n",
      "Epoch 1009, Loss: 3.100363790988922, Final Batch Loss: 0.720590353012085\n",
      "Epoch 1010, Loss: 2.8739959001541138, Final Batch Loss: 0.45723140239715576\n",
      "Epoch 1011, Loss: 3.0206055641174316, Final Batch Loss: 0.6019129157066345\n",
      "Epoch 1012, Loss: 2.9095430970191956, Final Batch Loss: 0.6218382120132446\n",
      "Epoch 1013, Loss: 2.913999021053314, Final Batch Loss: 0.6017886400222778\n",
      "Epoch 1014, Loss: 2.8920050859451294, Final Batch Loss: 0.513227641582489\n",
      "Epoch 1015, Loss: 3.048671305179596, Final Batch Loss: 0.6181329488754272\n",
      "Epoch 1016, Loss: 2.8828351497650146, Final Batch Loss: 0.5688302516937256\n",
      "Epoch 1017, Loss: 2.8826279044151306, Final Batch Loss: 0.5639091730117798\n",
      "Epoch 1018, Loss: 2.912651300430298, Final Batch Loss: 0.5828375220298767\n",
      "Epoch 1019, Loss: 3.054906874895096, Final Batch Loss: 0.7776806354522705\n",
      "Epoch 1020, Loss: 2.9328686594963074, Final Batch Loss: 0.5775983333587646\n",
      "Epoch 1021, Loss: 3.0175105929374695, Final Batch Loss: 0.571770429611206\n",
      "Epoch 1022, Loss: 2.9837635159492493, Final Batch Loss: 0.5730636119842529\n",
      "Epoch 1023, Loss: 3.1436386108398438, Final Batch Loss: 0.6022994518280029\n",
      "Epoch 1024, Loss: 2.9593571424484253, Final Batch Loss: 0.6125398278236389\n",
      "Epoch 1025, Loss: 2.92155522108078, Final Batch Loss: 0.6124560832977295\n",
      "Epoch 1026, Loss: 2.8424877524375916, Final Batch Loss: 0.47962450981140137\n",
      "Epoch 1027, Loss: 2.962087392807007, Final Batch Loss: 0.591601550579071\n",
      "Epoch 1028, Loss: 3.012161433696747, Final Batch Loss: 0.6520208120346069\n",
      "Epoch 1029, Loss: 2.8571857810020447, Final Batch Loss: 0.5933232307434082\n",
      "Epoch 1030, Loss: 2.851377785205841, Final Batch Loss: 0.6104029417037964\n",
      "Epoch 1031, Loss: 2.9495641589164734, Final Batch Loss: 0.504301905632019\n",
      "Epoch 1032, Loss: 2.951734244823456, Final Batch Loss: 0.5460432171821594\n",
      "Epoch 1033, Loss: 2.9829790592193604, Final Batch Loss: 0.6475184559822083\n",
      "Epoch 1034, Loss: 2.8566154837608337, Final Batch Loss: 0.5338783860206604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1035, Loss: 2.8946545720100403, Final Batch Loss: 0.5648218393325806\n",
      "Epoch 1036, Loss: 2.8244133591651917, Final Batch Loss: 0.601374626159668\n",
      "Epoch 1037, Loss: 2.9868427515029907, Final Batch Loss: 0.6242193579673767\n",
      "Epoch 1038, Loss: 2.8765141367912292, Final Batch Loss: 0.6290039420127869\n",
      "Epoch 1039, Loss: 2.859381765127182, Final Batch Loss: 0.4884723126888275\n",
      "Epoch 1040, Loss: 2.8647910356521606, Final Batch Loss: 0.6482465863227844\n",
      "Epoch 1041, Loss: 2.9107757806777954, Final Batch Loss: 0.45522183179855347\n",
      "Epoch 1042, Loss: 2.8316888213157654, Final Batch Loss: 0.5904293060302734\n",
      "Epoch 1043, Loss: 2.8421728014945984, Final Batch Loss: 0.5702946186065674\n",
      "Epoch 1044, Loss: 3.0512895584106445, Final Batch Loss: 0.6287983059883118\n",
      "Epoch 1045, Loss: 2.9123711585998535, Final Batch Loss: 0.655417263507843\n",
      "Epoch 1046, Loss: 2.841807186603546, Final Batch Loss: 0.5369348526000977\n",
      "Epoch 1047, Loss: 2.773333817720413, Final Batch Loss: 0.43646904826164246\n",
      "Epoch 1048, Loss: 2.7543143033981323, Final Batch Loss: 0.51036137342453\n",
      "Epoch 1049, Loss: 2.8628236651420593, Final Batch Loss: 0.5820393562316895\n",
      "Epoch 1050, Loss: 2.8590831756591797, Final Batch Loss: 0.5794389843940735\n",
      "Epoch 1051, Loss: 2.9896820187568665, Final Batch Loss: 0.6591827869415283\n",
      "Epoch 1052, Loss: 2.7731448113918304, Final Batch Loss: 0.5808065533638\n",
      "Epoch 1053, Loss: 3.008272111415863, Final Batch Loss: 0.5852164030075073\n",
      "Epoch 1054, Loss: 2.6888783872127533, Final Batch Loss: 0.5223960280418396\n",
      "Epoch 1055, Loss: 2.861991763114929, Final Batch Loss: 0.5274106860160828\n",
      "Epoch 1056, Loss: 3.013470411300659, Final Batch Loss: 0.5508856177330017\n",
      "Epoch 1057, Loss: 2.8472046852111816, Final Batch Loss: 0.5674817562103271\n",
      "Epoch 1058, Loss: 2.893225371837616, Final Batch Loss: 0.5452067255973816\n",
      "Epoch 1059, Loss: 2.9587721824645996, Final Batch Loss: 0.5561648607254028\n",
      "Epoch 1060, Loss: 2.8800867199897766, Final Batch Loss: 0.5674356818199158\n",
      "Epoch 1061, Loss: 3.065244495868683, Final Batch Loss: 0.6029893159866333\n",
      "Epoch 1062, Loss: 2.769945502281189, Final Batch Loss: 0.5332680940628052\n",
      "Epoch 1063, Loss: 2.937933772802353, Final Batch Loss: 0.7242034673690796\n",
      "Epoch 1064, Loss: 2.887331873178482, Final Batch Loss: 0.6368998289108276\n",
      "Epoch 1065, Loss: 2.9464580416679382, Final Batch Loss: 0.6280965209007263\n",
      "Epoch 1066, Loss: 2.9777340292930603, Final Batch Loss: 0.5828036665916443\n",
      "Epoch 1067, Loss: 2.783396452665329, Final Batch Loss: 0.5553507208824158\n",
      "Epoch 1068, Loss: 2.7880718111991882, Final Batch Loss: 0.5611823201179504\n",
      "Epoch 1069, Loss: 2.87197208404541, Final Batch Loss: 0.5735983848571777\n",
      "Epoch 1070, Loss: 2.8585405945777893, Final Batch Loss: 0.5627126097679138\n",
      "Epoch 1071, Loss: 2.8641650080680847, Final Batch Loss: 0.6370689868927002\n",
      "Epoch 1072, Loss: 2.9664678871631622, Final Batch Loss: 0.602166473865509\n",
      "Epoch 1073, Loss: 2.8487512469291687, Final Batch Loss: 0.5911532640457153\n",
      "Epoch 1074, Loss: 3.007371425628662, Final Batch Loss: 0.5779467821121216\n",
      "Epoch 1075, Loss: 2.8284692764282227, Final Batch Loss: 0.584075927734375\n",
      "Epoch 1076, Loss: 2.8200907707214355, Final Batch Loss: 0.5642469525337219\n",
      "Epoch 1077, Loss: 2.906722605228424, Final Batch Loss: 0.554180383682251\n",
      "Epoch 1078, Loss: 2.8234256505966187, Final Batch Loss: 0.6204261779785156\n",
      "Epoch 1079, Loss: 2.8073891401290894, Final Batch Loss: 0.5803876519203186\n",
      "Epoch 1080, Loss: 2.8130288422107697, Final Batch Loss: 0.4963870346546173\n",
      "Epoch 1081, Loss: 2.85068079829216, Final Batch Loss: 0.5429015159606934\n",
      "Epoch 1082, Loss: 2.7254806756973267, Final Batch Loss: 0.5552980899810791\n",
      "Epoch 1083, Loss: 2.659853219985962, Final Batch Loss: 0.48387813568115234\n",
      "Epoch 1084, Loss: 2.9565683007240295, Final Batch Loss: 0.5723982453346252\n",
      "Epoch 1085, Loss: 2.6881563663482666, Final Batch Loss: 0.5620462894439697\n",
      "Epoch 1086, Loss: 2.7684189677238464, Final Batch Loss: 0.5631441473960876\n",
      "Epoch 1087, Loss: 2.7384955883026123, Final Batch Loss: 0.5555641651153564\n",
      "Epoch 1088, Loss: 2.8372647762298584, Final Batch Loss: 0.5338195562362671\n",
      "Epoch 1089, Loss: 2.8685445189476013, Final Batch Loss: 0.6393590569496155\n",
      "Epoch 1090, Loss: 2.7887802720069885, Final Batch Loss: 0.5364109873771667\n",
      "Epoch 1091, Loss: 2.8832266330718994, Final Batch Loss: 0.6291227340698242\n",
      "Epoch 1092, Loss: 2.6940546929836273, Final Batch Loss: 0.5345380902290344\n",
      "Epoch 1093, Loss: 2.835103213787079, Final Batch Loss: 0.5197107195854187\n",
      "Epoch 1094, Loss: 2.868077516555786, Final Batch Loss: 0.6438700556755066\n",
      "Epoch 1095, Loss: 2.74053293466568, Final Batch Loss: 0.5041766166687012\n",
      "Epoch 1096, Loss: 2.710276484489441, Final Batch Loss: 0.5478757619857788\n",
      "Epoch 1097, Loss: 2.8693259954452515, Final Batch Loss: 0.6782801747322083\n",
      "Epoch 1098, Loss: 2.9083600640296936, Final Batch Loss: 0.5912936925888062\n",
      "Epoch 1099, Loss: 2.766178637742996, Final Batch Loss: 0.499014288187027\n",
      "Epoch 1100, Loss: 2.813921630382538, Final Batch Loss: 0.5163592100143433\n",
      "Epoch 1101, Loss: 2.896952748298645, Final Batch Loss: 0.6991692781448364\n",
      "Epoch 1102, Loss: 2.63869708776474, Final Batch Loss: 0.4649801254272461\n",
      "Epoch 1103, Loss: 2.9038164615631104, Final Batch Loss: 0.5300630331039429\n",
      "Epoch 1104, Loss: 2.805075764656067, Final Batch Loss: 0.6138501167297363\n",
      "Epoch 1105, Loss: 2.7951236069202423, Final Batch Loss: 0.5877780914306641\n",
      "Epoch 1106, Loss: 2.9338406920433044, Final Batch Loss: 0.702790379524231\n",
      "Epoch 1107, Loss: 2.650431752204895, Final Batch Loss: 0.4149419069290161\n",
      "Epoch 1108, Loss: 2.754441201686859, Final Batch Loss: 0.5048006176948547\n",
      "Epoch 1109, Loss: 2.772465318441391, Final Batch Loss: 0.5403291583061218\n",
      "Epoch 1110, Loss: 2.82965886592865, Final Batch Loss: 0.4706959128379822\n",
      "Epoch 1111, Loss: 2.767927974462509, Final Batch Loss: 0.47876259684562683\n",
      "Epoch 1112, Loss: 2.803906887769699, Final Batch Loss: 0.48482075333595276\n",
      "Epoch 1113, Loss: 2.754604011774063, Final Batch Loss: 0.5721679329872131\n",
      "Epoch 1114, Loss: 2.7320611774921417, Final Batch Loss: 0.5014899969100952\n",
      "Epoch 1115, Loss: 2.802896022796631, Final Batch Loss: 0.5705885887145996\n",
      "Epoch 1116, Loss: 2.5995018780231476, Final Batch Loss: 0.4581628143787384\n",
      "Epoch 1117, Loss: 2.724730134010315, Final Batch Loss: 0.5275751352310181\n",
      "Epoch 1118, Loss: 2.858413577079773, Final Batch Loss: 0.5570367574691772\n",
      "Epoch 1119, Loss: 2.8628861904144287, Final Batch Loss: 0.5878270864486694\n",
      "Epoch 1120, Loss: 2.651217758655548, Final Batch Loss: 0.5700229406356812\n",
      "Epoch 1121, Loss: 2.8765334486961365, Final Batch Loss: 0.5924069285392761\n",
      "Epoch 1122, Loss: 2.846281409263611, Final Batch Loss: 0.5455530881881714\n",
      "Epoch 1123, Loss: 2.704501360654831, Final Batch Loss: 0.616790235042572\n",
      "Epoch 1124, Loss: 2.7258216440677643, Final Batch Loss: 0.506084144115448\n",
      "Epoch 1125, Loss: 2.854283392429352, Final Batch Loss: 0.4492954611778259\n",
      "Epoch 1126, Loss: 2.8763557970523834, Final Batch Loss: 0.4525674283504486\n",
      "Epoch 1127, Loss: 2.790409207344055, Final Batch Loss: 0.4866448640823364\n",
      "Epoch 1128, Loss: 2.7474498748779297, Final Batch Loss: 0.5721039772033691\n",
      "Epoch 1129, Loss: 2.684323251247406, Final Batch Loss: 0.5011213421821594\n",
      "Epoch 1130, Loss: 2.802597165107727, Final Batch Loss: 0.5075320601463318\n",
      "Epoch 1131, Loss: 2.813926935195923, Final Batch Loss: 0.5506604909896851\n",
      "Epoch 1132, Loss: 2.781720221042633, Final Batch Loss: 0.5221646428108215\n",
      "Epoch 1133, Loss: 2.799443691968918, Final Batch Loss: 0.6502330899238586\n",
      "Epoch 1134, Loss: 2.7652092576026917, Final Batch Loss: 0.5068782567977905\n",
      "Epoch 1135, Loss: 2.7401928305625916, Final Batch Loss: 0.561994194984436\n",
      "Epoch 1136, Loss: 2.7952340245246887, Final Batch Loss: 0.7133491635322571\n",
      "Epoch 1137, Loss: 2.9023483991622925, Final Batch Loss: 0.5643535256385803\n",
      "Epoch 1138, Loss: 2.785915434360504, Final Batch Loss: 0.5846877098083496\n",
      "Epoch 1139, Loss: 2.7186742424964905, Final Batch Loss: 0.571243166923523\n",
      "Epoch 1140, Loss: 2.785306394100189, Final Batch Loss: 0.5298047661781311\n",
      "Epoch 1141, Loss: 2.7591516971588135, Final Batch Loss: 0.656856119632721\n",
      "Epoch 1142, Loss: 2.651951164007187, Final Batch Loss: 0.5631299018859863\n",
      "Epoch 1143, Loss: 2.7714850306510925, Final Batch Loss: 0.6110413670539856\n",
      "Epoch 1144, Loss: 2.6576929688453674, Final Batch Loss: 0.4557585120201111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1145, Loss: 2.7025516033172607, Final Batch Loss: 0.5940864682197571\n",
      "Epoch 1146, Loss: 2.6968770027160645, Final Batch Loss: 0.47197359800338745\n",
      "Epoch 1147, Loss: 2.504333108663559, Final Batch Loss: 0.44743847846984863\n",
      "Epoch 1148, Loss: 2.807664394378662, Final Batch Loss: 0.5346298813819885\n",
      "Epoch 1149, Loss: 2.700034499168396, Final Batch Loss: 0.5242018103599548\n",
      "Epoch 1150, Loss: 2.741174340248108, Final Batch Loss: 0.5500497817993164\n",
      "Epoch 1151, Loss: 2.594493955373764, Final Batch Loss: 0.5540130138397217\n",
      "Epoch 1152, Loss: 2.569875121116638, Final Batch Loss: 0.4458678662776947\n",
      "Epoch 1153, Loss: 2.7385878562927246, Final Batch Loss: 0.5113052129745483\n",
      "Epoch 1154, Loss: 2.6674580574035645, Final Batch Loss: 0.47391247749328613\n",
      "Epoch 1155, Loss: 2.658758759498596, Final Batch Loss: 0.5053215622901917\n",
      "Epoch 1156, Loss: 2.671639084815979, Final Batch Loss: 0.46601319313049316\n",
      "Epoch 1157, Loss: 2.777245432138443, Final Batch Loss: 0.5726511478424072\n",
      "Epoch 1158, Loss: 2.7010801434516907, Final Batch Loss: 0.5992352366447449\n",
      "Epoch 1159, Loss: 2.7045310735702515, Final Batch Loss: 0.5539348125457764\n",
      "Epoch 1160, Loss: 2.787848651409149, Final Batch Loss: 0.5781334042549133\n",
      "Epoch 1161, Loss: 2.739147901535034, Final Batch Loss: 0.5944634079933167\n",
      "Epoch 1162, Loss: 2.684263914823532, Final Batch Loss: 0.6091638803482056\n",
      "Epoch 1163, Loss: 2.652544915676117, Final Batch Loss: 0.4794572591781616\n",
      "Epoch 1164, Loss: 2.756450593471527, Final Batch Loss: 0.5350775718688965\n",
      "Epoch 1165, Loss: 2.7796612977981567, Final Batch Loss: 0.4772719144821167\n",
      "Epoch 1166, Loss: 2.582260847091675, Final Batch Loss: 0.5070179104804993\n",
      "Epoch 1167, Loss: 2.7251951694488525, Final Batch Loss: 0.6280661225318909\n",
      "Epoch 1168, Loss: 2.5823107063770294, Final Batch Loss: 0.5868635773658752\n",
      "Epoch 1169, Loss: 2.593748927116394, Final Batch Loss: 0.49303698539733887\n",
      "Epoch 1170, Loss: 2.6597855389118195, Final Batch Loss: 0.42428264021873474\n",
      "Epoch 1171, Loss: 2.659937769174576, Final Batch Loss: 0.5545023679733276\n",
      "Epoch 1172, Loss: 2.6335758566856384, Final Batch Loss: 0.5034120082855225\n",
      "Epoch 1173, Loss: 2.6393517553806305, Final Batch Loss: 0.5174347758293152\n",
      "Epoch 1174, Loss: 2.5209189653396606, Final Batch Loss: 0.49485206604003906\n",
      "Epoch 1175, Loss: 2.628474175930023, Final Batch Loss: 0.5260931849479675\n",
      "Epoch 1176, Loss: 2.5292887687683105, Final Batch Loss: 0.4889476001262665\n",
      "Epoch 1177, Loss: 2.5599039793014526, Final Batch Loss: 0.4021313786506653\n",
      "Epoch 1178, Loss: 2.6385190188884735, Final Batch Loss: 0.41037699580192566\n",
      "Epoch 1179, Loss: 2.722873270511627, Final Batch Loss: 0.5902712345123291\n",
      "Epoch 1180, Loss: 2.648923337459564, Final Batch Loss: 0.599019467830658\n",
      "Epoch 1181, Loss: 2.75238835811615, Final Batch Loss: 0.5150148272514343\n",
      "Epoch 1182, Loss: 2.6587852239608765, Final Batch Loss: 0.5492779612541199\n",
      "Epoch 1183, Loss: 2.655782997608185, Final Batch Loss: 0.6212916374206543\n",
      "Epoch 1184, Loss: 2.7334965467453003, Final Batch Loss: 0.5316238403320312\n",
      "Epoch 1185, Loss: 2.5154488384723663, Final Batch Loss: 0.45467665791511536\n",
      "Epoch 1186, Loss: 2.592078775167465, Final Batch Loss: 0.5436698794364929\n",
      "Epoch 1187, Loss: 2.715321898460388, Final Batch Loss: 0.6056562066078186\n",
      "Epoch 1188, Loss: 2.7344870567321777, Final Batch Loss: 0.5824797749519348\n",
      "Epoch 1189, Loss: 2.5903048515319824, Final Batch Loss: 0.4658767580986023\n",
      "Epoch 1190, Loss: 2.644529789686203, Final Batch Loss: 0.5831178426742554\n",
      "Epoch 1191, Loss: 2.7468657195568085, Final Batch Loss: 0.4717962443828583\n",
      "Epoch 1192, Loss: 2.709853917360306, Final Batch Loss: 0.6356077790260315\n",
      "Epoch 1193, Loss: 2.6806794106960297, Final Batch Loss: 0.5513847470283508\n",
      "Epoch 1194, Loss: 2.660435527563095, Final Batch Loss: 0.4952642619609833\n",
      "Epoch 1195, Loss: 2.677832841873169, Final Batch Loss: 0.5452566742897034\n",
      "Epoch 1196, Loss: 2.732077717781067, Final Batch Loss: 0.4638935625553131\n",
      "Epoch 1197, Loss: 2.8560133576393127, Final Batch Loss: 0.5812314748764038\n",
      "Epoch 1198, Loss: 2.6435896456241608, Final Batch Loss: 0.5881673097610474\n",
      "Epoch 1199, Loss: 2.712430715560913, Final Batch Loss: 0.49727898836135864\n",
      "Epoch 1200, Loss: 2.656941682100296, Final Batch Loss: 0.52906733751297\n",
      "Epoch 1201, Loss: 2.624681204557419, Final Batch Loss: 0.5732875466346741\n",
      "Epoch 1202, Loss: 2.66216841340065, Final Batch Loss: 0.49920743703842163\n",
      "Epoch 1203, Loss: 2.632380098104477, Final Batch Loss: 0.541538655757904\n",
      "Epoch 1204, Loss: 2.686048984527588, Final Batch Loss: 0.5275670289993286\n",
      "Epoch 1205, Loss: 2.5138734579086304, Final Batch Loss: 0.4849995970726013\n",
      "Epoch 1206, Loss: 2.644289195537567, Final Batch Loss: 0.5439586043357849\n",
      "Epoch 1207, Loss: 2.645232528448105, Final Batch Loss: 0.49614378809928894\n",
      "Epoch 1208, Loss: 2.4994532763957977, Final Batch Loss: 0.4622630774974823\n",
      "Epoch 1209, Loss: 2.4614619612693787, Final Batch Loss: 0.4570292532444\n",
      "Epoch 1210, Loss: 2.474298059940338, Final Batch Loss: 0.5415288805961609\n",
      "Epoch 1211, Loss: 2.5558196902275085, Final Batch Loss: 0.5605885982513428\n",
      "Epoch 1212, Loss: 2.617663949728012, Final Batch Loss: 0.5206616520881653\n",
      "Epoch 1213, Loss: 2.9181678891181946, Final Batch Loss: 0.7186498641967773\n",
      "Epoch 1214, Loss: 2.5171766579151154, Final Batch Loss: 0.4524826109409332\n",
      "Epoch 1215, Loss: 2.53050634264946, Final Batch Loss: 0.4784791171550751\n",
      "Epoch 1216, Loss: 2.631285697221756, Final Batch Loss: 0.6087357997894287\n",
      "Epoch 1217, Loss: 2.5445424914360046, Final Batch Loss: 0.44853758811950684\n",
      "Epoch 1218, Loss: 2.7317331731319427, Final Batch Loss: 0.628463089466095\n",
      "Epoch 1219, Loss: 2.6824282109737396, Final Batch Loss: 0.682638943195343\n",
      "Epoch 1220, Loss: 2.4507046043872833, Final Batch Loss: 0.34839409589767456\n",
      "Epoch 1221, Loss: 2.5605135560035706, Final Batch Loss: 0.5130798816680908\n",
      "Epoch 1222, Loss: 2.4470501840114594, Final Batch Loss: 0.4665548503398895\n",
      "Epoch 1223, Loss: 2.620209038257599, Final Batch Loss: 0.6014109253883362\n",
      "Epoch 1224, Loss: 2.6387076377868652, Final Batch Loss: 0.4989924430847168\n",
      "Epoch 1225, Loss: 2.5679116249084473, Final Batch Loss: 0.5053957104682922\n",
      "Epoch 1226, Loss: 2.5426643192768097, Final Batch Loss: 0.5203518271446228\n",
      "Epoch 1227, Loss: 2.4922911524772644, Final Batch Loss: 0.4231320023536682\n",
      "Epoch 1228, Loss: 2.576749235391617, Final Batch Loss: 0.5177026987075806\n",
      "Epoch 1229, Loss: 2.6139892637729645, Final Batch Loss: 0.5592274069786072\n",
      "Epoch 1230, Loss: 2.627716451883316, Final Batch Loss: 0.4694816768169403\n",
      "Epoch 1231, Loss: 2.7440855503082275, Final Batch Loss: 0.4906705617904663\n",
      "Epoch 1232, Loss: 2.689196527004242, Final Batch Loss: 0.5368794202804565\n",
      "Epoch 1233, Loss: 2.648096948862076, Final Batch Loss: 0.5541570782661438\n",
      "Epoch 1234, Loss: 2.6710972785949707, Final Batch Loss: 0.6066659092903137\n",
      "Epoch 1235, Loss: 2.687562018632889, Final Batch Loss: 0.5787254571914673\n",
      "Epoch 1236, Loss: 2.6273961663246155, Final Batch Loss: 0.5238447785377502\n",
      "Epoch 1237, Loss: 2.725020855665207, Final Batch Loss: 0.5566451549530029\n",
      "Epoch 1238, Loss: 2.649115025997162, Final Batch Loss: 0.6285802125930786\n",
      "Epoch 1239, Loss: 2.5463203489780426, Final Batch Loss: 0.377932071685791\n",
      "Epoch 1240, Loss: 2.501352995634079, Final Batch Loss: 0.5290089845657349\n",
      "Epoch 1241, Loss: 2.6690553724765778, Final Batch Loss: 0.44758084416389465\n",
      "Epoch 1242, Loss: 2.5809762477874756, Final Batch Loss: 0.4839652180671692\n",
      "Epoch 1243, Loss: 2.3960475623607635, Final Batch Loss: 0.4655556082725525\n",
      "Epoch 1244, Loss: 2.3921234607696533, Final Batch Loss: 0.4475223422050476\n",
      "Epoch 1245, Loss: 2.7341061532497406, Final Batch Loss: 0.5409898161888123\n",
      "Epoch 1246, Loss: 2.616785317659378, Final Batch Loss: 0.5039398670196533\n",
      "Epoch 1247, Loss: 2.6019494235515594, Final Batch Loss: 0.5173771977424622\n",
      "Epoch 1248, Loss: 2.6050013303756714, Final Batch Loss: 0.4865071773529053\n",
      "Epoch 1249, Loss: 2.62845516204834, Final Batch Loss: 0.5055127739906311\n",
      "Epoch 1250, Loss: 2.564845561981201, Final Batch Loss: 0.4672144949436188\n",
      "Epoch 1251, Loss: 2.791364550590515, Final Batch Loss: 0.5702328681945801\n",
      "Epoch 1252, Loss: 2.5314495265483856, Final Batch Loss: 0.4784999489784241\n",
      "Epoch 1253, Loss: 2.4854839146137238, Final Batch Loss: 0.4392704367637634\n",
      "Epoch 1254, Loss: 2.54649817943573, Final Batch Loss: 0.4996442496776581\n",
      "Epoch 1255, Loss: 2.471733272075653, Final Batch Loss: 0.5444584488868713\n",
      "Epoch 1256, Loss: 2.467488646507263, Final Batch Loss: 0.47647473216056824\n",
      "Epoch 1257, Loss: 2.6187661588191986, Final Batch Loss: 0.4835527539253235\n",
      "Epoch 1258, Loss: 2.4304191768169403, Final Batch Loss: 0.4916040301322937\n",
      "Epoch 1259, Loss: 2.573515474796295, Final Batch Loss: 0.49755510687828064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1260, Loss: 2.5053070783615112, Final Batch Loss: 0.493671715259552\n",
      "Epoch 1261, Loss: 2.5459273755550385, Final Batch Loss: 0.497020959854126\n",
      "Epoch 1262, Loss: 2.4496628046035767, Final Batch Loss: 0.47417423129081726\n",
      "Epoch 1263, Loss: 2.563075214624405, Final Batch Loss: 0.5756961107254028\n",
      "Epoch 1264, Loss: 2.544648915529251, Final Batch Loss: 0.5272437334060669\n",
      "Epoch 1265, Loss: 2.6099072098731995, Final Batch Loss: 0.5039635896682739\n",
      "Epoch 1266, Loss: 2.516997277736664, Final Batch Loss: 0.592525064945221\n",
      "Epoch 1267, Loss: 2.5147709250450134, Final Batch Loss: 0.4154771864414215\n",
      "Epoch 1268, Loss: 2.464544504880905, Final Batch Loss: 0.47029367089271545\n",
      "Epoch 1269, Loss: 2.539364129304886, Final Batch Loss: 0.5051536560058594\n",
      "Epoch 1270, Loss: 2.5837060511112213, Final Batch Loss: 0.5009336471557617\n",
      "Epoch 1271, Loss: 2.7508845925331116, Final Batch Loss: 0.5535887479782104\n",
      "Epoch 1272, Loss: 2.672309786081314, Final Batch Loss: 0.5460983514785767\n",
      "Epoch 1273, Loss: 2.447595924139023, Final Batch Loss: 0.4501896798610687\n",
      "Epoch 1274, Loss: 2.4949769973754883, Final Batch Loss: 0.49790018796920776\n",
      "Epoch 1275, Loss: 2.503375619649887, Final Batch Loss: 0.4817233383655548\n",
      "Epoch 1276, Loss: 2.4912265837192535, Final Batch Loss: 0.5436839461326599\n",
      "Epoch 1277, Loss: 2.4949125945568085, Final Batch Loss: 0.47448134422302246\n",
      "Epoch 1278, Loss: 2.594380646944046, Final Batch Loss: 0.4624287188053131\n",
      "Epoch 1279, Loss: 2.542402982711792, Final Batch Loss: 0.5796401500701904\n",
      "Epoch 1280, Loss: 2.4661402702331543, Final Batch Loss: 0.4707169532775879\n",
      "Epoch 1281, Loss: 2.622391939163208, Final Batch Loss: 0.6013373136520386\n",
      "Epoch 1282, Loss: 2.4463897049427032, Final Batch Loss: 0.4294631779193878\n",
      "Epoch 1283, Loss: 2.4320985078811646, Final Batch Loss: 0.4545183777809143\n",
      "Epoch 1284, Loss: 2.6179037988185883, Final Batch Loss: 0.4464792311191559\n",
      "Epoch 1285, Loss: 2.5340197682380676, Final Batch Loss: 0.5175204277038574\n",
      "Epoch 1286, Loss: 2.4593980610370636, Final Batch Loss: 0.5523853302001953\n",
      "Epoch 1287, Loss: 2.434471845626831, Final Batch Loss: 0.5002452731132507\n",
      "Epoch 1288, Loss: 2.6266005635261536, Final Batch Loss: 0.54913729429245\n",
      "Epoch 1289, Loss: 2.625497430562973, Final Batch Loss: 0.6452962160110474\n",
      "Epoch 1290, Loss: 2.4859951734542847, Final Batch Loss: 0.5427191853523254\n",
      "Epoch 1291, Loss: 2.5819950103759766, Final Batch Loss: 0.4947683811187744\n",
      "Epoch 1292, Loss: 2.7946346402168274, Final Batch Loss: 0.7120489478111267\n",
      "Epoch 1293, Loss: 2.6815183758735657, Final Batch Loss: 0.5527337789535522\n",
      "Epoch 1294, Loss: 2.5523502826690674, Final Batch Loss: 0.6228458881378174\n",
      "Epoch 1295, Loss: 2.5941697359085083, Final Batch Loss: 0.5134010314941406\n",
      "Epoch 1296, Loss: 2.366698682308197, Final Batch Loss: 0.40391555428504944\n",
      "Epoch 1297, Loss: 2.6063146889209747, Final Batch Loss: 0.5797407031059265\n",
      "Epoch 1298, Loss: 2.5755372643470764, Final Batch Loss: 0.5122604370117188\n",
      "Epoch 1299, Loss: 2.573659509420395, Final Batch Loss: 0.5567476153373718\n",
      "Epoch 1300, Loss: 2.5165102779865265, Final Batch Loss: 0.5401747226715088\n",
      "Epoch 1301, Loss: 2.541097730398178, Final Batch Loss: 0.5532245635986328\n",
      "Epoch 1302, Loss: 2.516338676214218, Final Batch Loss: 0.43935176730155945\n",
      "Epoch 1303, Loss: 2.3430806696414948, Final Batch Loss: 0.33106958866119385\n",
      "Epoch 1304, Loss: 2.4959959387779236, Final Batch Loss: 0.4984399080276489\n",
      "Epoch 1305, Loss: 2.533818393945694, Final Batch Loss: 0.558300256729126\n",
      "Epoch 1306, Loss: 2.3950392305850983, Final Batch Loss: 0.43417295813560486\n",
      "Epoch 1307, Loss: 2.583710163831711, Final Batch Loss: 0.6297107934951782\n",
      "Epoch 1308, Loss: 2.5791397392749786, Final Batch Loss: 0.5177547931671143\n",
      "Epoch 1309, Loss: 2.630006492137909, Final Batch Loss: 0.6786984205245972\n",
      "Epoch 1310, Loss: 2.5490444898605347, Final Batch Loss: 0.5624892711639404\n",
      "Epoch 1311, Loss: 2.7813956141471863, Final Batch Loss: 0.6493162512779236\n",
      "Epoch 1312, Loss: 2.464630573987961, Final Batch Loss: 0.462537556886673\n",
      "Epoch 1313, Loss: 2.583380788564682, Final Batch Loss: 0.5045924186706543\n",
      "Epoch 1314, Loss: 2.49212446808815, Final Batch Loss: 0.5231760740280151\n",
      "Epoch 1315, Loss: 2.431651383638382, Final Batch Loss: 0.4701913297176361\n",
      "Epoch 1316, Loss: 2.631514012813568, Final Batch Loss: 0.6328253746032715\n",
      "Epoch 1317, Loss: 2.4802087545394897, Final Batch Loss: 0.49783045053482056\n",
      "Epoch 1318, Loss: 2.509568929672241, Final Batch Loss: 0.5148965120315552\n",
      "Epoch 1319, Loss: 2.4236945509910583, Final Batch Loss: 0.4929068088531494\n",
      "Epoch 1320, Loss: 2.523653030395508, Final Batch Loss: 0.6069955229759216\n",
      "Epoch 1321, Loss: 2.686596155166626, Final Batch Loss: 0.6532737612724304\n",
      "Epoch 1322, Loss: 2.628142833709717, Final Batch Loss: 0.5398693680763245\n",
      "Epoch 1323, Loss: 2.5663148164749146, Final Batch Loss: 0.5679591298103333\n",
      "Epoch 1324, Loss: 2.4580666720867157, Final Batch Loss: 0.5249029994010925\n",
      "Epoch 1325, Loss: 2.4950634837150574, Final Batch Loss: 0.43135637044906616\n",
      "Epoch 1326, Loss: 2.415812313556671, Final Batch Loss: 0.5289447903633118\n",
      "Epoch 1327, Loss: 2.511973798274994, Final Batch Loss: 0.5099256038665771\n",
      "Epoch 1328, Loss: 2.501629203557968, Final Batch Loss: 0.5787364840507507\n",
      "Epoch 1329, Loss: 2.5337124466896057, Final Batch Loss: 0.4527609050273895\n",
      "Epoch 1330, Loss: 2.3814735412597656, Final Batch Loss: 0.4500488340854645\n",
      "Epoch 1331, Loss: 2.5831696689128876, Final Batch Loss: 0.5110152959823608\n",
      "Epoch 1332, Loss: 2.4870721995830536, Final Batch Loss: 0.4708273410797119\n",
      "Epoch 1333, Loss: 2.4156602919101715, Final Batch Loss: 0.5698361992835999\n",
      "Epoch 1334, Loss: 2.739804267883301, Final Batch Loss: 0.533150851726532\n",
      "Epoch 1335, Loss: 2.468715190887451, Final Batch Loss: 0.48980945348739624\n",
      "Epoch 1336, Loss: 2.5198054909706116, Final Batch Loss: 0.4716392159461975\n",
      "Epoch 1337, Loss: 2.5255236625671387, Final Batch Loss: 0.5478312969207764\n",
      "Epoch 1338, Loss: 2.482418417930603, Final Batch Loss: 0.47325676679611206\n",
      "Epoch 1339, Loss: 2.5369314551353455, Final Batch Loss: 0.45177069306373596\n",
      "Epoch 1340, Loss: 2.5212651193141937, Final Batch Loss: 0.4695453643798828\n",
      "Epoch 1341, Loss: 2.3692561388015747, Final Batch Loss: 0.43744561076164246\n",
      "Epoch 1342, Loss: 2.3898624181747437, Final Batch Loss: 0.41144534945487976\n",
      "Epoch 1343, Loss: 2.355067104101181, Final Batch Loss: 0.443256139755249\n",
      "Epoch 1344, Loss: 2.5111067295074463, Final Batch Loss: 0.5193157196044922\n",
      "Epoch 1345, Loss: 2.480079025030136, Final Batch Loss: 0.49084460735321045\n",
      "Epoch 1346, Loss: 2.4212589859962463, Final Batch Loss: 0.39483389258384705\n",
      "Epoch 1347, Loss: 2.542208671569824, Final Batch Loss: 0.5702448487281799\n",
      "Epoch 1348, Loss: 2.3970232009887695, Final Batch Loss: 0.4511267840862274\n",
      "Epoch 1349, Loss: 2.386449456214905, Final Batch Loss: 0.4713461101055145\n",
      "Epoch 1350, Loss: 2.45566663146019, Final Batch Loss: 0.5030596256256104\n",
      "Epoch 1351, Loss: 2.4984160661697388, Final Batch Loss: 0.5972862243652344\n",
      "Epoch 1352, Loss: 2.3329357504844666, Final Batch Loss: 0.49504610896110535\n",
      "Epoch 1353, Loss: 2.4835971295833588, Final Batch Loss: 0.5191310048103333\n",
      "Epoch 1354, Loss: 2.506431430578232, Final Batch Loss: 0.4402036964893341\n",
      "Epoch 1355, Loss: 2.4918188750743866, Final Batch Loss: 0.3972417116165161\n",
      "Epoch 1356, Loss: 2.579650968313217, Final Batch Loss: 0.4849768877029419\n",
      "Epoch 1357, Loss: 2.5250162184238434, Final Batch Loss: 0.5606411099433899\n",
      "Epoch 1358, Loss: 2.3138771057128906, Final Batch Loss: 0.43239787220954895\n",
      "Epoch 1359, Loss: 2.661553382873535, Final Batch Loss: 0.5653715133666992\n",
      "Epoch 1360, Loss: 2.357248067855835, Final Batch Loss: 0.42559167742729187\n",
      "Epoch 1361, Loss: 2.450658828020096, Final Batch Loss: 0.5260699391365051\n",
      "Epoch 1362, Loss: 2.4179271161556244, Final Batch Loss: 0.45746007561683655\n",
      "Epoch 1363, Loss: 2.408982038497925, Final Batch Loss: 0.5253202319145203\n",
      "Epoch 1364, Loss: 2.3762848675251007, Final Batch Loss: 0.430152028799057\n",
      "Epoch 1365, Loss: 2.350652426481247, Final Batch Loss: 0.39772605895996094\n",
      "Epoch 1366, Loss: 2.3882206082344055, Final Batch Loss: 0.4948347806930542\n",
      "Epoch 1367, Loss: 2.5697523951530457, Final Batch Loss: 0.41121232509613037\n",
      "Epoch 1368, Loss: 2.408236175775528, Final Batch Loss: 0.5264015197753906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1369, Loss: 2.3956122994422913, Final Batch Loss: 0.4668295085430145\n",
      "Epoch 1370, Loss: 2.386483520269394, Final Batch Loss: 0.3990671932697296\n",
      "Epoch 1371, Loss: 2.3576747477054596, Final Batch Loss: 0.482090026140213\n",
      "Epoch 1372, Loss: 2.4483515918254852, Final Batch Loss: 0.451025128364563\n",
      "Epoch 1373, Loss: 2.4033219516277313, Final Batch Loss: 0.4363783299922943\n",
      "Epoch 1374, Loss: 2.424659937620163, Final Batch Loss: 0.4218691289424896\n",
      "Epoch 1375, Loss: 2.323865294456482, Final Batch Loss: 0.5610435605049133\n",
      "Epoch 1376, Loss: 2.2803408801555634, Final Batch Loss: 0.4593086540699005\n",
      "Epoch 1377, Loss: 2.2631274461746216, Final Batch Loss: 0.34997043013572693\n",
      "Epoch 1378, Loss: 2.6386932134628296, Final Batch Loss: 0.5918862223625183\n",
      "Epoch 1379, Loss: 2.4533999264240265, Final Batch Loss: 0.39786410331726074\n",
      "Epoch 1380, Loss: 2.2861374020576477, Final Batch Loss: 0.39079970121383667\n",
      "Epoch 1381, Loss: 2.3249056339263916, Final Batch Loss: 0.46314698457717896\n",
      "Epoch 1382, Loss: 2.425950735807419, Final Batch Loss: 0.5067468285560608\n",
      "Epoch 1383, Loss: 2.208017021417618, Final Batch Loss: 0.36170756816864014\n",
      "Epoch 1384, Loss: 2.46711465716362, Final Batch Loss: 0.528564453125\n",
      "Epoch 1385, Loss: 2.6049347519874573, Final Batch Loss: 0.5422360897064209\n",
      "Epoch 1386, Loss: 2.5315977931022644, Final Batch Loss: 0.5206385254859924\n",
      "Epoch 1387, Loss: 2.4816365838050842, Final Batch Loss: 0.5574485659599304\n",
      "Epoch 1388, Loss: 2.5377548933029175, Final Batch Loss: 0.6079174280166626\n",
      "Epoch 1389, Loss: 2.420845776796341, Final Batch Loss: 0.5513611435890198\n",
      "Epoch 1390, Loss: 2.3008288145065308, Final Batch Loss: 0.3641683757305145\n",
      "Epoch 1391, Loss: 2.4573024213314056, Final Batch Loss: 0.5197049379348755\n",
      "Epoch 1392, Loss: 2.425308406352997, Final Batch Loss: 0.4898335933685303\n",
      "Epoch 1393, Loss: 2.594352036714554, Final Batch Loss: 0.5144476890563965\n",
      "Epoch 1394, Loss: 2.6225954592227936, Final Batch Loss: 0.553739070892334\n",
      "Epoch 1395, Loss: 2.509695738554001, Final Batch Loss: 0.5875332355499268\n",
      "Epoch 1396, Loss: 2.4783416986465454, Final Batch Loss: 0.580716073513031\n",
      "Epoch 1397, Loss: 2.4603400826454163, Final Batch Loss: 0.5422273874282837\n",
      "Epoch 1398, Loss: 2.391787439584732, Final Batch Loss: 0.458048015832901\n",
      "Epoch 1399, Loss: 2.41618874669075, Final Batch Loss: 0.45162296295166016\n",
      "Epoch 1400, Loss: 2.5278894007205963, Final Batch Loss: 0.5702827572822571\n",
      "Epoch 1401, Loss: 2.3981529474258423, Final Batch Loss: 0.4337408244609833\n",
      "Epoch 1402, Loss: 2.362342119216919, Final Batch Loss: 0.44047534465789795\n",
      "Epoch 1403, Loss: 2.5116055607795715, Final Batch Loss: 0.5416486263275146\n",
      "Epoch 1404, Loss: 2.5775500535964966, Final Batch Loss: 0.5819737315177917\n",
      "Epoch 1405, Loss: 2.3523342609405518, Final Batch Loss: 0.39132624864578247\n",
      "Epoch 1406, Loss: 2.433553159236908, Final Batch Loss: 0.5032729506492615\n",
      "Epoch 1407, Loss: 2.6081830859184265, Final Batch Loss: 0.5832363963127136\n",
      "Epoch 1408, Loss: 2.3037826120853424, Final Batch Loss: 0.40497148036956787\n",
      "Epoch 1409, Loss: 2.3833468556404114, Final Batch Loss: 0.5036305785179138\n",
      "Epoch 1410, Loss: 2.427099645137787, Final Batch Loss: 0.48716098070144653\n",
      "Epoch 1411, Loss: 2.3845711052417755, Final Batch Loss: 0.407014936208725\n",
      "Epoch 1412, Loss: 2.2653192579746246, Final Batch Loss: 0.4506042003631592\n",
      "Epoch 1413, Loss: 2.3772911429405212, Final Batch Loss: 0.46194010972976685\n",
      "Epoch 1414, Loss: 2.6602059602737427, Final Batch Loss: 0.6032910346984863\n",
      "Epoch 1415, Loss: 2.456026464700699, Final Batch Loss: 0.4187454283237457\n",
      "Epoch 1416, Loss: 2.4812816977500916, Final Batch Loss: 0.5582078099250793\n",
      "Epoch 1417, Loss: 2.3613515198230743, Final Batch Loss: 0.47958138585090637\n",
      "Epoch 1418, Loss: 2.3944043815135956, Final Batch Loss: 0.4805947542190552\n",
      "Epoch 1419, Loss: 2.417033851146698, Final Batch Loss: 0.3930755853652954\n",
      "Epoch 1420, Loss: 2.4008085131645203, Final Batch Loss: 0.5308783650398254\n",
      "Epoch 1421, Loss: 2.527511954307556, Final Batch Loss: 0.4779801368713379\n",
      "Epoch 1422, Loss: 2.3763695657253265, Final Batch Loss: 0.4631189703941345\n",
      "Epoch 1423, Loss: 2.4067287147045135, Final Batch Loss: 0.4490167498588562\n",
      "Epoch 1424, Loss: 2.525146096944809, Final Batch Loss: 0.5987451076507568\n",
      "Epoch 1425, Loss: 2.3357068300247192, Final Batch Loss: 0.5037042498588562\n",
      "Epoch 1426, Loss: 2.39757177233696, Final Batch Loss: 0.46659302711486816\n",
      "Epoch 1427, Loss: 2.4086686968803406, Final Batch Loss: 0.43734827637672424\n",
      "Epoch 1428, Loss: 2.4870049357414246, Final Batch Loss: 0.4777314364910126\n",
      "Epoch 1429, Loss: 2.330025553703308, Final Batch Loss: 0.40427127480506897\n",
      "Epoch 1430, Loss: 2.2486341297626495, Final Batch Loss: 0.4559548795223236\n",
      "Epoch 1431, Loss: 2.506045490503311, Final Batch Loss: 0.4667458236217499\n",
      "Epoch 1432, Loss: 2.363955557346344, Final Batch Loss: 0.3692452907562256\n",
      "Epoch 1433, Loss: 2.3088004887104034, Final Batch Loss: 0.4435921013355255\n",
      "Epoch 1434, Loss: 2.4427225589752197, Final Batch Loss: 0.48160478472709656\n",
      "Epoch 1435, Loss: 2.4007877111434937, Final Batch Loss: 0.4151461124420166\n",
      "Epoch 1436, Loss: 2.3764485120773315, Final Batch Loss: 0.4196576476097107\n",
      "Epoch 1437, Loss: 2.3709707260131836, Final Batch Loss: 0.4752117395401001\n",
      "Epoch 1438, Loss: 2.3349520564079285, Final Batch Loss: 0.4459543228149414\n",
      "Epoch 1439, Loss: 2.414851278066635, Final Batch Loss: 0.47880586981773376\n",
      "Epoch 1440, Loss: 2.616332709789276, Final Batch Loss: 0.5432621836662292\n",
      "Epoch 1441, Loss: 2.4480774104595184, Final Batch Loss: 0.5206942558288574\n",
      "Epoch 1442, Loss: 2.305544078350067, Final Batch Loss: 0.45598456263542175\n",
      "Epoch 1443, Loss: 2.3419129550457, Final Batch Loss: 0.45075705647468567\n",
      "Epoch 1444, Loss: 2.3439632058143616, Final Batch Loss: 0.41241392493247986\n",
      "Epoch 1445, Loss: 2.4373588263988495, Final Batch Loss: 0.493141233921051\n",
      "Epoch 1446, Loss: 2.397267162799835, Final Batch Loss: 0.5198132991790771\n",
      "Epoch 1447, Loss: 2.443250447511673, Final Batch Loss: 0.44979509711265564\n",
      "Epoch 1448, Loss: 2.561004012823105, Final Batch Loss: 0.5443624258041382\n",
      "Epoch 1449, Loss: 2.357953280210495, Final Batch Loss: 0.48298925161361694\n",
      "Epoch 1450, Loss: 2.3791588842868805, Final Batch Loss: 0.5128421783447266\n",
      "Epoch 1451, Loss: 2.322292923927307, Final Batch Loss: 0.47759494185447693\n",
      "Epoch 1452, Loss: 2.40704482793808, Final Batch Loss: 0.48377174139022827\n",
      "Epoch 1453, Loss: 2.3343587815761566, Final Batch Loss: 0.4750497341156006\n",
      "Epoch 1454, Loss: 2.385830134153366, Final Batch Loss: 0.5435295104980469\n",
      "Epoch 1455, Loss: 2.4757650792598724, Final Batch Loss: 0.4679698646068573\n",
      "Epoch 1456, Loss: 2.56257626414299, Final Batch Loss: 0.5124427676200867\n",
      "Epoch 1457, Loss: 2.233349949121475, Final Batch Loss: 0.4427863359451294\n",
      "Epoch 1458, Loss: 2.4278549253940582, Final Batch Loss: 0.4531896710395813\n",
      "Epoch 1459, Loss: 2.498974531888962, Final Batch Loss: 0.5352293252944946\n",
      "Epoch 1460, Loss: 2.381555140018463, Final Batch Loss: 0.4645770788192749\n",
      "Epoch 1461, Loss: 2.180544227361679, Final Batch Loss: 0.41431495547294617\n",
      "Epoch 1462, Loss: 2.278991311788559, Final Batch Loss: 0.5071658492088318\n",
      "Epoch 1463, Loss: 2.4830878376960754, Final Batch Loss: 0.5813217759132385\n",
      "Epoch 1464, Loss: 2.3154147267341614, Final Batch Loss: 0.3653583526611328\n",
      "Epoch 1465, Loss: 2.213379591703415, Final Batch Loss: 0.40419530868530273\n",
      "Epoch 1466, Loss: 2.325091004371643, Final Batch Loss: 0.4535096287727356\n",
      "Epoch 1467, Loss: 2.3699913024902344, Final Batch Loss: 0.507327139377594\n",
      "Epoch 1468, Loss: 2.250755786895752, Final Batch Loss: 0.38347578048706055\n",
      "Epoch 1469, Loss: 2.305116981267929, Final Batch Loss: 0.4807557165622711\n",
      "Epoch 1470, Loss: 2.158999890089035, Final Batch Loss: 0.3356178402900696\n",
      "Epoch 1471, Loss: 2.2430363297462463, Final Batch Loss: 0.42488613724708557\n",
      "Epoch 1472, Loss: 2.2987679541110992, Final Batch Loss: 0.5081706643104553\n",
      "Epoch 1473, Loss: 2.196867048740387, Final Batch Loss: 0.3636961877346039\n",
      "Epoch 1474, Loss: 2.2392121255397797, Final Batch Loss: 0.38224130868911743\n",
      "Epoch 1475, Loss: 2.3710624873638153, Final Batch Loss: 0.5147068500518799\n",
      "Epoch 1476, Loss: 2.1801967322826385, Final Batch Loss: 0.3639065623283386\n",
      "Epoch 1477, Loss: 2.4001854956150055, Final Batch Loss: 0.4765787124633789\n",
      "Epoch 1478, Loss: 2.3764561116695404, Final Batch Loss: 0.5366566777229309\n",
      "Epoch 1479, Loss: 2.2107530534267426, Final Batch Loss: 0.48781996965408325\n",
      "Epoch 1480, Loss: 2.3061847388744354, Final Batch Loss: 0.4100572168827057\n",
      "Epoch 1481, Loss: 2.3650364577770233, Final Batch Loss: 0.5038135051727295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1482, Loss: 2.359293222427368, Final Batch Loss: 0.4554463326931\n",
      "Epoch 1483, Loss: 2.3338928520679474, Final Batch Loss: 0.472188800573349\n",
      "Epoch 1484, Loss: 2.375437408685684, Final Batch Loss: 0.41541266441345215\n",
      "Epoch 1485, Loss: 2.453642815351486, Final Batch Loss: 0.47308027744293213\n",
      "Epoch 1486, Loss: 2.29212287068367, Final Batch Loss: 0.41414588689804077\n",
      "Epoch 1487, Loss: 2.2129468619823456, Final Batch Loss: 0.4433492124080658\n",
      "Epoch 1488, Loss: 2.328772932291031, Final Batch Loss: 0.6374102830886841\n",
      "Epoch 1489, Loss: 2.2617460191249847, Final Batch Loss: 0.439771831035614\n",
      "Epoch 1490, Loss: 2.2592680156230927, Final Batch Loss: 0.43084433674812317\n",
      "Epoch 1491, Loss: 2.180453062057495, Final Batch Loss: 0.3918954133987427\n",
      "Epoch 1492, Loss: 2.3262623250484467, Final Batch Loss: 0.4621363580226898\n",
      "Epoch 1493, Loss: 2.505058318376541, Final Batch Loss: 0.5202354788780212\n",
      "Epoch 1494, Loss: 2.4557951986789703, Final Batch Loss: 0.5440452694892883\n",
      "Epoch 1495, Loss: 2.4712782204151154, Final Batch Loss: 0.5246223211288452\n",
      "Epoch 1496, Loss: 2.3571050465106964, Final Batch Loss: 0.4268839359283447\n",
      "Epoch 1497, Loss: 2.3513020277023315, Final Batch Loss: 0.44869059324264526\n",
      "Epoch 1498, Loss: 2.4131723046302795, Final Batch Loss: 0.558808445930481\n",
      "Epoch 1499, Loss: 2.240655541419983, Final Batch Loss: 0.4757058024406433\n",
      "Epoch 1500, Loss: 2.3705449402332306, Final Batch Loss: 0.48189544677734375\n",
      "Epoch 1501, Loss: 2.409090518951416, Final Batch Loss: 0.46590232849121094\n",
      "Epoch 1502, Loss: 2.3383349180221558, Final Batch Loss: 0.5341124534606934\n",
      "Epoch 1503, Loss: 2.4328593313694, Final Batch Loss: 0.5710972547531128\n",
      "Epoch 1504, Loss: 2.2792234420776367, Final Batch Loss: 0.512151300907135\n",
      "Epoch 1505, Loss: 2.2101716697216034, Final Batch Loss: 0.4415910840034485\n",
      "Epoch 1506, Loss: 2.2858592867851257, Final Batch Loss: 0.5234758853912354\n",
      "Epoch 1507, Loss: 2.320675492286682, Final Batch Loss: 0.4618139863014221\n",
      "Epoch 1508, Loss: 2.3107713758945465, Final Batch Loss: 0.5741136074066162\n",
      "Epoch 1509, Loss: 2.3791137635707855, Final Batch Loss: 0.5789758563041687\n",
      "Epoch 1510, Loss: 2.253193199634552, Final Batch Loss: 0.4740709066390991\n",
      "Epoch 1511, Loss: 2.44992259144783, Final Batch Loss: 0.45129159092903137\n",
      "Epoch 1512, Loss: 2.35381355881691, Final Batch Loss: 0.42399778962135315\n",
      "Epoch 1513, Loss: 2.3307440280914307, Final Batch Loss: 0.3921326994895935\n",
      "Epoch 1514, Loss: 2.3186983168125153, Final Batch Loss: 0.45696285367012024\n",
      "Epoch 1515, Loss: 2.315051168203354, Final Batch Loss: 0.43540605902671814\n",
      "Epoch 1516, Loss: 2.2521473467350006, Final Batch Loss: 0.4561369717121124\n",
      "Epoch 1517, Loss: 2.2041072249412537, Final Batch Loss: 0.4518046975135803\n",
      "Epoch 1518, Loss: 2.313926041126251, Final Batch Loss: 0.43860167264938354\n",
      "Epoch 1519, Loss: 2.3315539062023163, Final Batch Loss: 0.4050242602825165\n",
      "Epoch 1520, Loss: 2.295036882162094, Final Batch Loss: 0.41684672236442566\n",
      "Epoch 1521, Loss: 2.3910309076309204, Final Batch Loss: 0.43346914649009705\n",
      "Epoch 1522, Loss: 2.464013457298279, Final Batch Loss: 0.5573750734329224\n",
      "Epoch 1523, Loss: 2.247343212366104, Final Batch Loss: 0.3830626308917999\n",
      "Epoch 1524, Loss: 2.289033532142639, Final Batch Loss: 0.3680477738380432\n",
      "Epoch 1525, Loss: 2.4139885008335114, Final Batch Loss: 0.48567336797714233\n",
      "Epoch 1526, Loss: 2.232982248067856, Final Batch Loss: 0.45538589358329773\n",
      "Epoch 1527, Loss: 2.357708215713501, Final Batch Loss: 0.416420578956604\n",
      "Epoch 1528, Loss: 2.28875008225441, Final Batch Loss: 0.4497359097003937\n",
      "Epoch 1529, Loss: 2.264547288417816, Final Batch Loss: 0.5037521719932556\n",
      "Epoch 1530, Loss: 2.14132758975029, Final Batch Loss: 0.4237527549266815\n",
      "Epoch 1531, Loss: 2.2798834145069122, Final Batch Loss: 0.4907742142677307\n",
      "Epoch 1532, Loss: 2.3886967599391937, Final Batch Loss: 0.5507540702819824\n",
      "Epoch 1533, Loss: 2.2290259301662445, Final Batch Loss: 0.45610058307647705\n",
      "Epoch 1534, Loss: 2.244747132062912, Final Batch Loss: 0.40615198016166687\n",
      "Epoch 1535, Loss: 2.412791281938553, Final Batch Loss: 0.4749283492565155\n",
      "Epoch 1536, Loss: 2.507630556821823, Final Batch Loss: 0.5852416753768921\n",
      "Epoch 1537, Loss: 2.3509481251239777, Final Batch Loss: 0.4056808054447174\n",
      "Epoch 1538, Loss: 2.3886092007160187, Final Batch Loss: 0.5146402716636658\n",
      "Epoch 1539, Loss: 2.297824651002884, Final Batch Loss: 0.40575143694877625\n",
      "Epoch 1540, Loss: 2.165423035621643, Final Batch Loss: 0.43971487879753113\n",
      "Epoch 1541, Loss: 2.3299446403980255, Final Batch Loss: 0.4354010224342346\n",
      "Epoch 1542, Loss: 2.2795601189136505, Final Batch Loss: 0.5169755816459656\n",
      "Epoch 1543, Loss: 2.292085438966751, Final Batch Loss: 0.496748149394989\n",
      "Epoch 1544, Loss: 2.2964878380298615, Final Batch Loss: 0.4437031149864197\n",
      "Epoch 1545, Loss: 2.425545185804367, Final Batch Loss: 0.4816436767578125\n",
      "Epoch 1546, Loss: 2.262372136116028, Final Batch Loss: 0.4613875150680542\n",
      "Epoch 1547, Loss: 2.253627210855484, Final Batch Loss: 0.4485127329826355\n",
      "Epoch 1548, Loss: 2.213167041540146, Final Batch Loss: 0.3856448531150818\n",
      "Epoch 1549, Loss: 2.291920393705368, Final Batch Loss: 0.4369949996471405\n",
      "Epoch 1550, Loss: 2.258700728416443, Final Batch Loss: 0.48759081959724426\n",
      "Epoch 1551, Loss: 2.210287392139435, Final Batch Loss: 0.4197681248188019\n",
      "Epoch 1552, Loss: 2.262439101934433, Final Batch Loss: 0.5233392715454102\n",
      "Epoch 1553, Loss: 2.263516277074814, Final Batch Loss: 0.30450084805488586\n",
      "Epoch 1554, Loss: 2.369645595550537, Final Batch Loss: 0.5531757473945618\n",
      "Epoch 1555, Loss: 2.2387032508850098, Final Batch Loss: 0.47846749424934387\n",
      "Epoch 1556, Loss: 2.32881361246109, Final Batch Loss: 0.4186481535434723\n",
      "Epoch 1557, Loss: 2.2105876207351685, Final Batch Loss: 0.45045775175094604\n",
      "Epoch 1558, Loss: 2.140502780675888, Final Batch Loss: 0.3929964303970337\n",
      "Epoch 1559, Loss: 2.289072096347809, Final Batch Loss: 0.46578338742256165\n",
      "Epoch 1560, Loss: 2.3144776821136475, Final Batch Loss: 0.5884641408920288\n",
      "Epoch 1561, Loss: 2.2189905047416687, Final Batch Loss: 0.2918391823768616\n",
      "Epoch 1562, Loss: 2.266001433134079, Final Batch Loss: 0.49874347448349\n",
      "Epoch 1563, Loss: 2.107078403234482, Final Batch Loss: 0.3371713161468506\n",
      "Epoch 1564, Loss: 2.1784838140010834, Final Batch Loss: 0.4070023000240326\n",
      "Epoch 1565, Loss: 2.2996744513511658, Final Batch Loss: 0.39200833439826965\n",
      "Epoch 1566, Loss: 2.1912187039852142, Final Batch Loss: 0.4103359580039978\n",
      "Epoch 1567, Loss: 2.15934956073761, Final Batch Loss: 0.474372923374176\n",
      "Epoch 1568, Loss: 2.291082262992859, Final Batch Loss: 0.5592501163482666\n",
      "Epoch 1569, Loss: 2.242631733417511, Final Batch Loss: 0.4691756069660187\n",
      "Epoch 1570, Loss: 2.15478652715683, Final Batch Loss: 0.40668991208076477\n",
      "Epoch 1571, Loss: 2.199180006980896, Final Batch Loss: 0.4940322935581207\n",
      "Epoch 1572, Loss: 2.236079901456833, Final Batch Loss: 0.4640423059463501\n",
      "Epoch 1573, Loss: 2.458133488893509, Final Batch Loss: 0.4482056796550751\n",
      "Epoch 1574, Loss: 2.173914849758148, Final Batch Loss: 0.45783287286758423\n",
      "Epoch 1575, Loss: 2.095073878765106, Final Batch Loss: 0.36145684123039246\n",
      "Epoch 1576, Loss: 2.2800672948360443, Final Batch Loss: 0.351659893989563\n",
      "Epoch 1577, Loss: 2.3982620239257812, Final Batch Loss: 0.46373844146728516\n",
      "Epoch 1578, Loss: 2.257813900709152, Final Batch Loss: 0.5037172436714172\n",
      "Epoch 1579, Loss: 2.2117594480514526, Final Batch Loss: 0.4466211199760437\n",
      "Epoch 1580, Loss: 2.233440399169922, Final Batch Loss: 0.4012278616428375\n",
      "Epoch 1581, Loss: 2.1742597818374634, Final Batch Loss: 0.42756417393684387\n",
      "Epoch 1582, Loss: 2.2444334030151367, Final Batch Loss: 0.45985302329063416\n",
      "Epoch 1583, Loss: 2.2577864825725555, Final Batch Loss: 0.5237327814102173\n",
      "Epoch 1584, Loss: 2.266352653503418, Final Batch Loss: 0.4298950433731079\n",
      "Epoch 1585, Loss: 2.1394015848636627, Final Batch Loss: 0.39699241518974304\n",
      "Epoch 1586, Loss: 2.171948492527008, Final Batch Loss: 0.424463152885437\n",
      "Epoch 1587, Loss: 2.299927830696106, Final Batch Loss: 0.505269467830658\n",
      "Epoch 1588, Loss: 2.218339204788208, Final Batch Loss: 0.41393840312957764\n",
      "Epoch 1589, Loss: 2.2904944121837616, Final Batch Loss: 0.48041388392448425\n",
      "Epoch 1590, Loss: 2.2080140113830566, Final Batch Loss: 0.4071415364742279\n",
      "Epoch 1591, Loss: 2.2987464666366577, Final Batch Loss: 0.4945013225078583\n",
      "Epoch 1592, Loss: 2.27032208442688, Final Batch Loss: 0.3879639506340027\n",
      "Epoch 1593, Loss: 2.170319378376007, Final Batch Loss: 0.4796787202358246\n",
      "Epoch 1594, Loss: 2.117051422595978, Final Batch Loss: 0.3727949857711792\n",
      "Epoch 1595, Loss: 2.2957459092140198, Final Batch Loss: 0.504202663898468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1596, Loss: 2.265772044658661, Final Batch Loss: 0.403411328792572\n",
      "Epoch 1597, Loss: 2.3121918737888336, Final Batch Loss: 0.5270805358886719\n",
      "Epoch 1598, Loss: 2.1839620172977448, Final Batch Loss: 0.49410954117774963\n",
      "Epoch 1599, Loss: 2.1822812855243683, Final Batch Loss: 0.4971613883972168\n",
      "Epoch 1600, Loss: 2.288472682237625, Final Batch Loss: 0.434562087059021\n",
      "Epoch 1601, Loss: 2.2782832384109497, Final Batch Loss: 0.3891434073448181\n",
      "Epoch 1602, Loss: 2.1561318039894104, Final Batch Loss: 0.34282320737838745\n",
      "Epoch 1603, Loss: 2.2399232983589172, Final Batch Loss: 0.4338245689868927\n",
      "Epoch 1604, Loss: 2.3133987188339233, Final Batch Loss: 0.5144978761672974\n",
      "Epoch 1605, Loss: 2.2395833134651184, Final Batch Loss: 0.4115682542324066\n",
      "Epoch 1606, Loss: 2.2339040338993073, Final Batch Loss: 0.5001429915428162\n",
      "Epoch 1607, Loss: 2.3797945082187653, Final Batch Loss: 0.5928677320480347\n",
      "Epoch 1608, Loss: 2.2047792971134186, Final Batch Loss: 0.47849375009536743\n",
      "Epoch 1609, Loss: 2.180573046207428, Final Batch Loss: 0.397007018327713\n",
      "Epoch 1610, Loss: 2.190501034259796, Final Batch Loss: 0.4012494683265686\n",
      "Epoch 1611, Loss: 2.21434623003006, Final Batch Loss: 0.42926374077796936\n",
      "Epoch 1612, Loss: 2.162413567304611, Final Batch Loss: 0.3676750063896179\n",
      "Epoch 1613, Loss: 2.089046448469162, Final Batch Loss: 0.4434259831905365\n",
      "Epoch 1614, Loss: 2.2902643978595734, Final Batch Loss: 0.4901057779788971\n",
      "Epoch 1615, Loss: 2.2376084327697754, Final Batch Loss: 0.49610984325408936\n",
      "Epoch 1616, Loss: 2.207167536020279, Final Batch Loss: 0.43051087856292725\n",
      "Epoch 1617, Loss: 2.443288028240204, Final Batch Loss: 0.5515645146369934\n",
      "Epoch 1618, Loss: 2.3603669106960297, Final Batch Loss: 0.5127519369125366\n",
      "Epoch 1619, Loss: 2.3195572197437286, Final Batch Loss: 0.5228821635246277\n",
      "Epoch 1620, Loss: 2.265299618244171, Final Batch Loss: 0.4804251790046692\n",
      "Epoch 1621, Loss: 2.221833884716034, Final Batch Loss: 0.38049980998039246\n",
      "Epoch 1622, Loss: 2.124926447868347, Final Batch Loss: 0.3528731167316437\n",
      "Epoch 1623, Loss: 2.0984804034233093, Final Batch Loss: 0.37222275137901306\n",
      "Epoch 1624, Loss: 2.175633728504181, Final Batch Loss: 0.4458937644958496\n",
      "Epoch 1625, Loss: 2.273042172193527, Final Batch Loss: 0.44900795817375183\n",
      "Epoch 1626, Loss: 2.1966867446899414, Final Batch Loss: 0.4089265763759613\n",
      "Epoch 1627, Loss: 2.168740689754486, Final Batch Loss: 0.3532400131225586\n",
      "Epoch 1628, Loss: 2.1506999731063843, Final Batch Loss: 0.3591448664665222\n",
      "Epoch 1629, Loss: 2.283462345600128, Final Batch Loss: 0.46636950969696045\n",
      "Epoch 1630, Loss: 2.2140044271945953, Final Batch Loss: 0.40746939182281494\n",
      "Epoch 1631, Loss: 2.1933301091194153, Final Batch Loss: 0.3823532164096832\n",
      "Epoch 1632, Loss: 2.2621961534023285, Final Batch Loss: 0.40255120396614075\n",
      "Epoch 1633, Loss: 2.1946349143981934, Final Batch Loss: 0.45147326588630676\n",
      "Epoch 1634, Loss: 2.36815944314003, Final Batch Loss: 0.4834030568599701\n",
      "Epoch 1635, Loss: 2.330507218837738, Final Batch Loss: 0.413748562335968\n",
      "Epoch 1636, Loss: 2.0946822464466095, Final Batch Loss: 0.31694966554641724\n",
      "Epoch 1637, Loss: 2.150779068470001, Final Batch Loss: 0.4359763264656067\n",
      "Epoch 1638, Loss: 2.209259510040283, Final Batch Loss: 0.4820270538330078\n",
      "Epoch 1639, Loss: 2.2681600749492645, Final Batch Loss: 0.5549392700195312\n",
      "Epoch 1640, Loss: 2.2663543820381165, Final Batch Loss: 0.4778370261192322\n",
      "Epoch 1641, Loss: 2.1905601024627686, Final Batch Loss: 0.457841694355011\n",
      "Epoch 1642, Loss: 2.188417226076126, Final Batch Loss: 0.4490051865577698\n",
      "Epoch 1643, Loss: 2.1784230172634125, Final Batch Loss: 0.3733440041542053\n",
      "Epoch 1644, Loss: 2.308477371931076, Final Batch Loss: 0.4425906538963318\n",
      "Epoch 1645, Loss: 2.392551213502884, Final Batch Loss: 0.6402182579040527\n",
      "Epoch 1646, Loss: 2.252238243818283, Final Batch Loss: 0.5270267724990845\n",
      "Epoch 1647, Loss: 2.2553828060626984, Final Batch Loss: 0.5094755291938782\n",
      "Epoch 1648, Loss: 2.2691463828086853, Final Batch Loss: 0.5262818336486816\n",
      "Epoch 1649, Loss: 2.2059888541698456, Final Batch Loss: 0.4184025824069977\n",
      "Epoch 1650, Loss: 2.173849195241928, Final Batch Loss: 0.5254935026168823\n",
      "Epoch 1651, Loss: 2.2165132761001587, Final Batch Loss: 0.4347176253795624\n",
      "Epoch 1652, Loss: 2.324713408946991, Final Batch Loss: 0.49164271354675293\n",
      "Epoch 1653, Loss: 2.2419546246528625, Final Batch Loss: 0.45175686478614807\n",
      "Epoch 1654, Loss: 2.2596534490585327, Final Batch Loss: 0.4421830177307129\n",
      "Epoch 1655, Loss: 2.2271532118320465, Final Batch Loss: 0.4409073293209076\n",
      "Epoch 1656, Loss: 2.2686167657375336, Final Batch Loss: 0.41557633876800537\n",
      "Epoch 1657, Loss: 2.03617262840271, Final Batch Loss: 0.37559637427330017\n",
      "Epoch 1658, Loss: 2.1991371512413025, Final Batch Loss: 0.4726959764957428\n",
      "Epoch 1659, Loss: 2.152353823184967, Final Batch Loss: 0.4262811541557312\n",
      "Epoch 1660, Loss: 2.235338121652603, Final Batch Loss: 0.4296303987503052\n",
      "Epoch 1661, Loss: 2.255784422159195, Final Batch Loss: 0.48288649320602417\n",
      "Epoch 1662, Loss: 2.16889688372612, Final Batch Loss: 0.43096140027046204\n",
      "Epoch 1663, Loss: 2.227838844060898, Final Batch Loss: 0.45216453075408936\n",
      "Epoch 1664, Loss: 2.287567585706711, Final Batch Loss: 0.4704855978488922\n",
      "Epoch 1665, Loss: 2.2784845530986786, Final Batch Loss: 0.4888778626918793\n",
      "Epoch 1666, Loss: 2.1725437343120575, Final Batch Loss: 0.48540928959846497\n",
      "Epoch 1667, Loss: 2.1361523270606995, Final Batch Loss: 0.45432060956954956\n",
      "Epoch 1668, Loss: 2.311288297176361, Final Batch Loss: 0.4087633788585663\n",
      "Epoch 1669, Loss: 2.1025990545749664, Final Batch Loss: 0.31859639286994934\n",
      "Epoch 1670, Loss: 2.256884455680847, Final Batch Loss: 0.4549258053302765\n",
      "Epoch 1671, Loss: 2.0832594335079193, Final Batch Loss: 0.4165876805782318\n",
      "Epoch 1672, Loss: 2.3697774708271027, Final Batch Loss: 0.5564717054367065\n",
      "Epoch 1673, Loss: 2.1582557559013367, Final Batch Loss: 0.4434070587158203\n",
      "Epoch 1674, Loss: 2.0986803472042084, Final Batch Loss: 0.491256445646286\n",
      "Epoch 1675, Loss: 2.3848409950733185, Final Batch Loss: 0.47351568937301636\n",
      "Epoch 1676, Loss: 2.1357110142707825, Final Batch Loss: 0.43850404024124146\n",
      "Epoch 1677, Loss: 2.243292987346649, Final Batch Loss: 0.3707304894924164\n",
      "Epoch 1678, Loss: 2.2413830161094666, Final Batch Loss: 0.4126128852367401\n",
      "Epoch 1679, Loss: 2.1196832060813904, Final Batch Loss: 0.34955474734306335\n",
      "Epoch 1680, Loss: 2.1604950428009033, Final Batch Loss: 0.44643813371658325\n",
      "Epoch 1681, Loss: 2.4042156040668488, Final Batch Loss: 0.4787881672382355\n",
      "Epoch 1682, Loss: 2.2842338383197784, Final Batch Loss: 0.4106549322605133\n",
      "Epoch 1683, Loss: 2.1373954713344574, Final Batch Loss: 0.4030616581439972\n",
      "Epoch 1684, Loss: 2.2227306962013245, Final Batch Loss: 0.47947457432746887\n",
      "Epoch 1685, Loss: 2.305300772190094, Final Batch Loss: 0.46829456090927124\n",
      "Epoch 1686, Loss: 2.2207216322422028, Final Batch Loss: 0.4363456666469574\n",
      "Epoch 1687, Loss: 2.2362412810325623, Final Batch Loss: 0.41806429624557495\n",
      "Epoch 1688, Loss: 2.141243040561676, Final Batch Loss: 0.4298287034034729\n",
      "Epoch 1689, Loss: 2.181413412094116, Final Batch Loss: 0.4499995708465576\n",
      "Epoch 1690, Loss: 2.2147796154022217, Final Batch Loss: 0.45496731996536255\n",
      "Epoch 1691, Loss: 2.363031893968582, Final Batch Loss: 0.4922201633453369\n",
      "Epoch 1692, Loss: 2.1327166855335236, Final Batch Loss: 0.3545781373977661\n",
      "Epoch 1693, Loss: 2.1172201931476593, Final Batch Loss: 0.37272658944129944\n",
      "Epoch 1694, Loss: 2.1548507809638977, Final Batch Loss: 0.45311683416366577\n",
      "Epoch 1695, Loss: 2.24588605761528, Final Batch Loss: 0.4502856135368347\n",
      "Epoch 1696, Loss: 2.0577561259269714, Final Batch Loss: 0.3388277590274811\n",
      "Epoch 1697, Loss: 2.189409375190735, Final Batch Loss: 0.4104306399822235\n",
      "Epoch 1698, Loss: 2.101620614528656, Final Batch Loss: 0.3651596009731293\n",
      "Epoch 1699, Loss: 2.059672176837921, Final Batch Loss: 0.3449999988079071\n",
      "Epoch 1700, Loss: 2.1555016934871674, Final Batch Loss: 0.43372318148612976\n",
      "Epoch 1701, Loss: 2.244427740573883, Final Batch Loss: 0.39028674364089966\n",
      "Epoch 1702, Loss: 2.168907880783081, Final Batch Loss: 0.3709717094898224\n",
      "Epoch 1703, Loss: 2.2022912204265594, Final Batch Loss: 0.43752601742744446\n",
      "Epoch 1704, Loss: 2.056371033191681, Final Batch Loss: 0.38569915294647217\n",
      "Epoch 1705, Loss: 2.097204178571701, Final Batch Loss: 0.5103914737701416\n",
      "Epoch 1706, Loss: 2.174877882003784, Final Batch Loss: 0.3866557478904724\n",
      "Epoch 1707, Loss: 2.2318591475486755, Final Batch Loss: 0.5004467964172363\n",
      "Epoch 1708, Loss: 2.2703643143177032, Final Batch Loss: 0.46682974696159363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1709, Loss: 2.2629546523094177, Final Batch Loss: 0.38008251786231995\n",
      "Epoch 1710, Loss: 2.069099932909012, Final Batch Loss: 0.4109603762626648\n",
      "Epoch 1711, Loss: 2.1990252137184143, Final Batch Loss: 0.40631815791130066\n",
      "Epoch 1712, Loss: 2.2787366211414337, Final Batch Loss: 0.4632752537727356\n",
      "Epoch 1713, Loss: 2.132868528366089, Final Batch Loss: 0.3859163224697113\n",
      "Epoch 1714, Loss: 2.0863750874996185, Final Batch Loss: 0.4858388900756836\n",
      "Epoch 1715, Loss: 2.2266758382320404, Final Batch Loss: 0.46694326400756836\n",
      "Epoch 1716, Loss: 2.148478716611862, Final Batch Loss: 0.4694647490978241\n",
      "Epoch 1717, Loss: 2.124830812215805, Final Batch Loss: 0.3225891888141632\n",
      "Epoch 1718, Loss: 2.2534410655498505, Final Batch Loss: 0.5458955764770508\n",
      "Epoch 1719, Loss: 2.1187395453453064, Final Batch Loss: 0.46912115812301636\n",
      "Epoch 1720, Loss: 2.2173549830913544, Final Batch Loss: 0.5032351016998291\n",
      "Epoch 1721, Loss: 2.0768429338932037, Final Batch Loss: 0.4234732687473297\n",
      "Epoch 1722, Loss: 1.988171935081482, Final Batch Loss: 0.3419024646282196\n",
      "Epoch 1723, Loss: 2.1258871853351593, Final Batch Loss: 0.3522641956806183\n",
      "Epoch 1724, Loss: 2.1723280549049377, Final Batch Loss: 0.45887520909309387\n",
      "Epoch 1725, Loss: 1.9809641540050507, Final Batch Loss: 0.3921935558319092\n",
      "Epoch 1726, Loss: 2.1894009709358215, Final Batch Loss: 0.47705793380737305\n",
      "Epoch 1727, Loss: 2.16170272231102, Final Batch Loss: 0.45146116614341736\n",
      "Epoch 1728, Loss: 2.0079497694969177, Final Batch Loss: 0.35050252079963684\n",
      "Epoch 1729, Loss: 2.086671322584152, Final Batch Loss: 0.4165116846561432\n",
      "Epoch 1730, Loss: 2.14360049366951, Final Batch Loss: 0.44196635484695435\n",
      "Epoch 1731, Loss: 2.066282719373703, Final Batch Loss: 0.38999807834625244\n",
      "Epoch 1732, Loss: 2.1662985384464264, Final Batch Loss: 0.3561646342277527\n",
      "Epoch 1733, Loss: 2.2642291486263275, Final Batch Loss: 0.42251935601234436\n",
      "Epoch 1734, Loss: 2.2173820436000824, Final Batch Loss: 0.4109647870063782\n",
      "Epoch 1735, Loss: 2.122356504201889, Final Batch Loss: 0.4673976004123688\n",
      "Epoch 1736, Loss: 2.130319058895111, Final Batch Loss: 0.4017731845378876\n",
      "Epoch 1737, Loss: 2.258425235748291, Final Batch Loss: 0.4787766635417938\n",
      "Epoch 1738, Loss: 2.1777187883853912, Final Batch Loss: 0.4105800688266754\n",
      "Epoch 1739, Loss: 2.238328218460083, Final Batch Loss: 0.3663316071033478\n",
      "Epoch 1740, Loss: 2.235833764076233, Final Batch Loss: 0.4796334207057953\n",
      "Epoch 1741, Loss: 2.1051135659217834, Final Batch Loss: 0.39992284774780273\n",
      "Epoch 1742, Loss: 2.333760619163513, Final Batch Loss: 0.3944942355155945\n",
      "Epoch 1743, Loss: 2.249198079109192, Final Batch Loss: 0.3304291367530823\n",
      "Epoch 1744, Loss: 2.196035534143448, Final Batch Loss: 0.4656696617603302\n",
      "Epoch 1745, Loss: 2.191586673259735, Final Batch Loss: 0.4670923054218292\n",
      "Epoch 1746, Loss: 2.162993609905243, Final Batch Loss: 0.4140620231628418\n",
      "Epoch 1747, Loss: 2.1709434390068054, Final Batch Loss: 0.5954825282096863\n",
      "Epoch 1748, Loss: 2.205287843942642, Final Batch Loss: 0.519271969795227\n",
      "Epoch 1749, Loss: 2.1211884021759033, Final Batch Loss: 0.4155716598033905\n",
      "Epoch 1750, Loss: 2.1576314866542816, Final Batch Loss: 0.4359988868236542\n",
      "Epoch 1751, Loss: 2.0600902438163757, Final Batch Loss: 0.3987358808517456\n",
      "Epoch 1752, Loss: 2.120146006345749, Final Batch Loss: 0.346813440322876\n",
      "Epoch 1753, Loss: 2.205481767654419, Final Batch Loss: 0.4696623682975769\n",
      "Epoch 1754, Loss: 1.9769167006015778, Final Batch Loss: 0.3342091143131256\n",
      "Epoch 1755, Loss: 2.2200540900230408, Final Batch Loss: 0.4956584572792053\n",
      "Epoch 1756, Loss: 2.1432659924030304, Final Batch Loss: 0.41892799735069275\n",
      "Epoch 1757, Loss: 2.2719753086566925, Final Batch Loss: 0.529475748538971\n",
      "Epoch 1758, Loss: 2.0922666788101196, Final Batch Loss: 0.3742109537124634\n",
      "Epoch 1759, Loss: 2.1196255683898926, Final Batch Loss: 0.46235957741737366\n",
      "Epoch 1760, Loss: 2.104577451944351, Final Batch Loss: 0.4095099866390228\n",
      "Epoch 1761, Loss: 2.2385690212249756, Final Batch Loss: 0.5334124565124512\n",
      "Epoch 1762, Loss: 2.2436869740486145, Final Batch Loss: 0.5031762719154358\n",
      "Epoch 1763, Loss: 2.259574234485626, Final Batch Loss: 0.4703197479248047\n",
      "Epoch 1764, Loss: 2.1271019279956818, Final Batch Loss: 0.49354368448257446\n",
      "Epoch 1765, Loss: 2.324467748403549, Final Batch Loss: 0.7254340052604675\n",
      "Epoch 1766, Loss: 2.2069013118743896, Final Batch Loss: 0.5038303732872009\n",
      "Epoch 1767, Loss: 2.209300607442856, Final Batch Loss: 0.47590163350105286\n",
      "Epoch 1768, Loss: 2.2641949355602264, Final Batch Loss: 0.49263978004455566\n",
      "Epoch 1769, Loss: 2.1187051236629486, Final Batch Loss: 0.36056268215179443\n",
      "Epoch 1770, Loss: 2.080764979124069, Final Batch Loss: 0.38471633195877075\n",
      "Epoch 1771, Loss: 2.170070379972458, Final Batch Loss: 0.468824565410614\n",
      "Epoch 1772, Loss: 2.055280327796936, Final Batch Loss: 0.392459511756897\n",
      "Epoch 1773, Loss: 2.204794019460678, Final Batch Loss: 0.5126623511314392\n",
      "Epoch 1774, Loss: 2.1383557617664337, Final Batch Loss: 0.40956759452819824\n",
      "Epoch 1775, Loss: 2.2227870523929596, Final Batch Loss: 0.4351954460144043\n",
      "Epoch 1776, Loss: 2.3008148968219757, Final Batch Loss: 0.5541380047798157\n",
      "Epoch 1777, Loss: 2.1462633907794952, Final Batch Loss: 0.4210369884967804\n",
      "Epoch 1778, Loss: 2.0080772042274475, Final Batch Loss: 0.36944833397865295\n",
      "Epoch 1779, Loss: 2.3047998249530792, Final Batch Loss: 0.5951822996139526\n",
      "Epoch 1780, Loss: 2.2276609241962433, Final Batch Loss: 0.4901275038719177\n",
      "Epoch 1781, Loss: 2.076295346021652, Final Batch Loss: 0.3514159023761749\n",
      "Epoch 1782, Loss: 2.1187769770622253, Final Batch Loss: 0.4429776966571808\n",
      "Epoch 1783, Loss: 2.1219589710235596, Final Batch Loss: 0.39854228496551514\n",
      "Epoch 1784, Loss: 2.020333409309387, Final Batch Loss: 0.37096673250198364\n",
      "Epoch 1785, Loss: 2.2357776165008545, Final Batch Loss: 0.5288417339324951\n",
      "Epoch 1786, Loss: 2.271569788455963, Final Batch Loss: 0.4819323420524597\n",
      "Epoch 1787, Loss: 2.0279425382614136, Final Batch Loss: 0.3702769875526428\n",
      "Epoch 1788, Loss: 2.2065179646015167, Final Batch Loss: 0.49475187063217163\n",
      "Epoch 1789, Loss: 2.125530421733856, Final Batch Loss: 0.5244873762130737\n",
      "Epoch 1790, Loss: 2.3052572309970856, Final Batch Loss: 0.5394320487976074\n",
      "Epoch 1791, Loss: 2.0859577357769012, Final Batch Loss: 0.41457119584083557\n",
      "Epoch 1792, Loss: 2.106550246477127, Final Batch Loss: 0.37953802943229675\n",
      "Epoch 1793, Loss: 2.161145508289337, Final Batch Loss: 0.42083975672721863\n",
      "Epoch 1794, Loss: 2.224702775478363, Final Batch Loss: 0.4354689121246338\n",
      "Epoch 1795, Loss: 2.011608898639679, Final Batch Loss: 0.3706928491592407\n",
      "Epoch 1796, Loss: 2.1901406943798065, Final Batch Loss: 0.4583538770675659\n",
      "Epoch 1797, Loss: 2.249811202287674, Final Batch Loss: 0.4715999662876129\n",
      "Epoch 1798, Loss: 2.2556007504463196, Final Batch Loss: 0.397442489862442\n",
      "Epoch 1799, Loss: 2.2032371163368225, Final Batch Loss: 0.5104719400405884\n",
      "Epoch 1800, Loss: 2.2033844590187073, Final Batch Loss: 0.49883192777633667\n",
      "Epoch 1801, Loss: 2.1498957574367523, Final Batch Loss: 0.4427357614040375\n",
      "Epoch 1802, Loss: 2.2846231758594513, Final Batch Loss: 0.3657929599285126\n",
      "Epoch 1803, Loss: 2.018494039773941, Final Batch Loss: 0.3769429624080658\n",
      "Epoch 1804, Loss: 2.01138174533844, Final Batch Loss: 0.3683198392391205\n",
      "Epoch 1805, Loss: 2.2223912179470062, Final Batch Loss: 0.4813729226589203\n",
      "Epoch 1806, Loss: 2.1943549513816833, Final Batch Loss: 0.43172332644462585\n",
      "Epoch 1807, Loss: 2.12075012922287, Final Batch Loss: 0.4004632234573364\n",
      "Epoch 1808, Loss: 2.2936303317546844, Final Batch Loss: 0.4372533857822418\n",
      "Epoch 1809, Loss: 2.16497465968132, Final Batch Loss: 0.4399057626724243\n",
      "Epoch 1810, Loss: 2.039181172847748, Final Batch Loss: 0.39142072200775146\n",
      "Epoch 1811, Loss: 2.233341634273529, Final Batch Loss: 0.445091187953949\n",
      "Epoch 1812, Loss: 2.0880374014377594, Final Batch Loss: 0.3560170531272888\n",
      "Epoch 1813, Loss: 2.0743285417556763, Final Batch Loss: 0.3723014295101166\n",
      "Epoch 1814, Loss: 2.088160663843155, Final Batch Loss: 0.44854432344436646\n",
      "Epoch 1815, Loss: 2.117516726255417, Final Batch Loss: 0.40053731203079224\n",
      "Epoch 1816, Loss: 2.089229851961136, Final Batch Loss: 0.4140256643295288\n",
      "Epoch 1817, Loss: 2.1175970435142517, Final Batch Loss: 0.4422803223133087\n",
      "Epoch 1818, Loss: 1.9112536013126373, Final Batch Loss: 0.38205528259277344\n",
      "Epoch 1819, Loss: 2.1843269169330597, Final Batch Loss: 0.38992056250572205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1820, Loss: 2.081406742334366, Final Batch Loss: 0.4935833513736725\n",
      "Epoch 1821, Loss: 1.998160719871521, Final Batch Loss: 0.4634273648262024\n",
      "Epoch 1822, Loss: 2.085662215948105, Final Batch Loss: 0.4365602731704712\n",
      "Epoch 1823, Loss: 2.161109745502472, Final Batch Loss: 0.4211416244506836\n",
      "Epoch 1824, Loss: 2.1600320041179657, Final Batch Loss: 0.46875515580177307\n",
      "Epoch 1825, Loss: 2.007531762123108, Final Batch Loss: 0.4288593530654907\n",
      "Epoch 1826, Loss: 2.0702930092811584, Final Batch Loss: 0.39400869607925415\n",
      "Epoch 1827, Loss: 2.0879459381103516, Final Batch Loss: 0.3894784450531006\n",
      "Epoch 1828, Loss: 2.155205875635147, Final Batch Loss: 0.42219382524490356\n",
      "Epoch 1829, Loss: 2.021812289953232, Final Batch Loss: 0.48426109552383423\n",
      "Epoch 1830, Loss: 2.005103290081024, Final Batch Loss: 0.3553129732608795\n",
      "Epoch 1831, Loss: 2.0051689445972443, Final Batch Loss: 0.328062504529953\n",
      "Epoch 1832, Loss: 2.0754797756671906, Final Batch Loss: 0.41766491532325745\n",
      "Epoch 1833, Loss: 2.117302507162094, Final Batch Loss: 0.4033396244049072\n",
      "Epoch 1834, Loss: 2.255030632019043, Final Batch Loss: 0.4897865653038025\n",
      "Epoch 1835, Loss: 2.0363037288188934, Final Batch Loss: 0.3660995364189148\n",
      "Epoch 1836, Loss: 2.0757707357406616, Final Batch Loss: 0.3892397880554199\n",
      "Epoch 1837, Loss: 2.008712202310562, Final Batch Loss: 0.37458160519599915\n",
      "Epoch 1838, Loss: 2.0864641964435577, Final Batch Loss: 0.4098099172115326\n",
      "Epoch 1839, Loss: 2.105982184410095, Final Batch Loss: 0.41547444462776184\n",
      "Epoch 1840, Loss: 2.176532566547394, Final Batch Loss: 0.44905686378479004\n",
      "Epoch 1841, Loss: 2.0729089975357056, Final Batch Loss: 0.45060044527053833\n",
      "Epoch 1842, Loss: 2.1730536818504333, Final Batch Loss: 0.5686379075050354\n",
      "Epoch 1843, Loss: 2.0176742672920227, Final Batch Loss: 0.37328866124153137\n",
      "Epoch 1844, Loss: 2.1321483850479126, Final Batch Loss: 0.44904884696006775\n",
      "Epoch 1845, Loss: 2.118033230304718, Final Batch Loss: 0.3626684546470642\n",
      "Epoch 1846, Loss: 2.020890325307846, Final Batch Loss: 0.3870105445384979\n",
      "Epoch 1847, Loss: 2.1410355269908905, Final Batch Loss: 0.575591504573822\n",
      "Epoch 1848, Loss: 1.9958161115646362, Final Batch Loss: 0.3593580722808838\n",
      "Epoch 1849, Loss: 2.2049616277217865, Final Batch Loss: 0.4465324580669403\n",
      "Epoch 1850, Loss: 2.106187492609024, Final Batch Loss: 0.4038699269294739\n",
      "Epoch 1851, Loss: 2.06134432554245, Final Batch Loss: 0.3806016445159912\n",
      "Epoch 1852, Loss: 2.096998691558838, Final Batch Loss: 0.44889116287231445\n",
      "Epoch 1853, Loss: 2.1030654907226562, Final Batch Loss: 0.3942837119102478\n",
      "Epoch 1854, Loss: 2.232809454202652, Final Batch Loss: 0.4524642825126648\n",
      "Epoch 1855, Loss: 2.0871309638023376, Final Batch Loss: 0.31302231550216675\n",
      "Epoch 1856, Loss: 2.0407787561416626, Final Batch Loss: 0.3932665288448334\n",
      "Epoch 1857, Loss: 1.946523904800415, Final Batch Loss: 0.38741710782051086\n",
      "Epoch 1858, Loss: 2.0804532170295715, Final Batch Loss: 0.4148064851760864\n",
      "Epoch 1859, Loss: 2.175956279039383, Final Batch Loss: 0.4892538785934448\n",
      "Epoch 1860, Loss: 2.0367903411388397, Final Batch Loss: 0.3824821710586548\n",
      "Epoch 1861, Loss: 1.941367745399475, Final Batch Loss: 0.40741166472435\n",
      "Epoch 1862, Loss: 2.0720337629318237, Final Batch Loss: 0.3873426914215088\n",
      "Epoch 1863, Loss: 2.0019306242465973, Final Batch Loss: 0.46785345673561096\n",
      "Epoch 1864, Loss: 2.0911263525485992, Final Batch Loss: 0.4267220199108124\n",
      "Epoch 1865, Loss: 2.0885048508644104, Final Batch Loss: 0.39175906777381897\n",
      "Epoch 1866, Loss: 2.0176625847816467, Final Batch Loss: 0.4293447434902191\n",
      "Epoch 1867, Loss: 2.0415200293064117, Final Batch Loss: 0.34665003418922424\n",
      "Epoch 1868, Loss: 2.119874507188797, Final Batch Loss: 0.44209009408950806\n",
      "Epoch 1869, Loss: 2.0659379363059998, Final Batch Loss: 0.48946595191955566\n",
      "Epoch 1870, Loss: 2.046515107154846, Final Batch Loss: 0.36849358677864075\n",
      "Epoch 1871, Loss: 2.1712666749954224, Final Batch Loss: 0.47150641679763794\n",
      "Epoch 1872, Loss: 2.2466039061546326, Final Batch Loss: 0.521446168422699\n",
      "Epoch 1873, Loss: 2.159626930952072, Final Batch Loss: 0.45354315638542175\n",
      "Epoch 1874, Loss: 2.050608456134796, Final Batch Loss: 0.41239240765571594\n",
      "Epoch 1875, Loss: 1.9495733082294464, Final Batch Loss: 0.41930684447288513\n",
      "Epoch 1876, Loss: 2.080845445394516, Final Batch Loss: 0.5188990831375122\n",
      "Epoch 1877, Loss: 2.088896006345749, Final Batch Loss: 0.40098807215690613\n",
      "Epoch 1878, Loss: 2.0320873856544495, Final Batch Loss: 0.3975924849510193\n",
      "Epoch 1879, Loss: 2.082669287919998, Final Batch Loss: 0.3970012664794922\n",
      "Epoch 1880, Loss: 2.1025442481040955, Final Batch Loss: 0.3511100709438324\n",
      "Epoch 1881, Loss: 2.1775979697704315, Final Batch Loss: 0.39803433418273926\n",
      "Epoch 1882, Loss: 2.1081449389457703, Final Batch Loss: 0.47938820719718933\n",
      "Epoch 1883, Loss: 1.9878735840320587, Final Batch Loss: 0.3442694842815399\n",
      "Epoch 1884, Loss: 2.1083531975746155, Final Batch Loss: 0.4021158218383789\n",
      "Epoch 1885, Loss: 2.084661692380905, Final Batch Loss: 0.41396215558052063\n",
      "Epoch 1886, Loss: 2.2328986525535583, Final Batch Loss: 0.5307727456092834\n",
      "Epoch 1887, Loss: 2.0969137847423553, Final Batch Loss: 0.485378235578537\n",
      "Epoch 1888, Loss: 2.1099043488502502, Final Batch Loss: 0.4174996316432953\n",
      "Epoch 1889, Loss: 2.157747507095337, Final Batch Loss: 0.43912559747695923\n",
      "Epoch 1890, Loss: 2.119759500026703, Final Batch Loss: 0.5404016375541687\n",
      "Epoch 1891, Loss: 2.030509442090988, Final Batch Loss: 0.42730775475502014\n",
      "Epoch 1892, Loss: 1.988277018070221, Final Batch Loss: 0.3232751488685608\n",
      "Epoch 1893, Loss: 2.113801658153534, Final Batch Loss: 0.342902809381485\n",
      "Epoch 1894, Loss: 2.050200641155243, Final Batch Loss: 0.37094029784202576\n",
      "Epoch 1895, Loss: 1.9012633562088013, Final Batch Loss: 0.3271881937980652\n",
      "Epoch 1896, Loss: 2.0545120537281036, Final Batch Loss: 0.39667344093322754\n",
      "Epoch 1897, Loss: 2.1190720200538635, Final Batch Loss: 0.4330712556838989\n",
      "Epoch 1898, Loss: 1.9586587250232697, Final Batch Loss: 0.3417617380619049\n",
      "Epoch 1899, Loss: 2.017924427986145, Final Batch Loss: 0.43103885650634766\n",
      "Epoch 1900, Loss: 1.9848447740077972, Final Batch Loss: 0.41541534662246704\n",
      "Epoch 1901, Loss: 2.001491069793701, Final Batch Loss: 0.32220402359962463\n",
      "Epoch 1902, Loss: 2.222161054611206, Final Batch Loss: 0.44426554441452026\n",
      "Epoch 1903, Loss: 2.2858877778053284, Final Batch Loss: 0.4728374183177948\n",
      "Epoch 1904, Loss: 2.08591365814209, Final Batch Loss: 0.44657108187675476\n",
      "Epoch 1905, Loss: 2.118783414363861, Final Batch Loss: 0.4351195693016052\n",
      "Epoch 1906, Loss: 2.127116173505783, Final Batch Loss: 0.445760041475296\n",
      "Epoch 1907, Loss: 2.0790611505508423, Final Batch Loss: 0.43582192063331604\n",
      "Epoch 1908, Loss: 2.002484381198883, Final Batch Loss: 0.4647168815135956\n",
      "Epoch 1909, Loss: 1.9789918065071106, Final Batch Loss: 0.34492409229278564\n",
      "Epoch 1910, Loss: 2.0602020621299744, Final Batch Loss: 0.47703519463539124\n",
      "Epoch 1911, Loss: 2.2269638180732727, Final Batch Loss: 0.4132651388645172\n",
      "Epoch 1912, Loss: 1.9836276471614838, Final Batch Loss: 0.378512442111969\n",
      "Epoch 1913, Loss: 2.1003033220767975, Final Batch Loss: 0.4040066599845886\n",
      "Epoch 1914, Loss: 2.019873231649399, Final Batch Loss: 0.400604784488678\n",
      "Epoch 1915, Loss: 2.0482640862464905, Final Batch Loss: 0.5457953214645386\n",
      "Epoch 1916, Loss: 2.177368253469467, Final Batch Loss: 0.4675526022911072\n",
      "Epoch 1917, Loss: 2.052838444709778, Final Batch Loss: 0.3545028567314148\n",
      "Epoch 1918, Loss: 2.0961952805519104, Final Batch Loss: 0.4251458942890167\n",
      "Epoch 1919, Loss: 2.069032311439514, Final Batch Loss: 0.3910174071788788\n",
      "Epoch 1920, Loss: 1.9908673167228699, Final Batch Loss: 0.3594930171966553\n",
      "Epoch 1921, Loss: 2.0626393258571625, Final Batch Loss: 0.3263137936592102\n",
      "Epoch 1922, Loss: 2.1143091320991516, Final Batch Loss: 0.32297179102897644\n",
      "Epoch 1923, Loss: 2.082832992076874, Final Batch Loss: 0.5132169127464294\n",
      "Epoch 1924, Loss: 2.0073901414871216, Final Batch Loss: 0.48237890005111694\n",
      "Epoch 1925, Loss: 2.097071021795273, Final Batch Loss: 0.303039014339447\n",
      "Epoch 1926, Loss: 2.113497644662857, Final Batch Loss: 0.5154020190238953\n",
      "Epoch 1927, Loss: 2.0509448051452637, Final Batch Loss: 0.49894383549690247\n",
      "Epoch 1928, Loss: 2.007998049259186, Final Batch Loss: 0.4345918893814087\n",
      "Epoch 1929, Loss: 1.9302944540977478, Final Batch Loss: 0.4210495352745056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1930, Loss: 2.104011297225952, Final Batch Loss: 0.38125643134117126\n",
      "Epoch 1931, Loss: 2.0454381108283997, Final Batch Loss: 0.41995540261268616\n",
      "Epoch 1932, Loss: 2.0139297544956207, Final Batch Loss: 0.4222010672092438\n",
      "Epoch 1933, Loss: 2.123050183057785, Final Batch Loss: 0.414793461561203\n",
      "Epoch 1934, Loss: 1.9269811511039734, Final Batch Loss: 0.41492968797683716\n",
      "Epoch 1935, Loss: 2.015267789363861, Final Batch Loss: 0.4640459716320038\n",
      "Epoch 1936, Loss: 2.028671056032181, Final Batch Loss: 0.44094187021255493\n",
      "Epoch 1937, Loss: 2.1373440623283386, Final Batch Loss: 0.3435843288898468\n",
      "Epoch 1938, Loss: 2.045753985643387, Final Batch Loss: 0.5088529586791992\n",
      "Epoch 1939, Loss: 1.9409043490886688, Final Batch Loss: 0.3464123606681824\n",
      "Epoch 1940, Loss: 2.1544568240642548, Final Batch Loss: 0.40510818362236023\n",
      "Epoch 1941, Loss: 1.9717553555965424, Final Batch Loss: 0.42765703797340393\n",
      "Epoch 1942, Loss: 1.9813490211963654, Final Batch Loss: 0.3105173707008362\n",
      "Epoch 1943, Loss: 2.0783800184726715, Final Batch Loss: 0.3822469413280487\n",
      "Epoch 1944, Loss: 2.122332751750946, Final Batch Loss: 0.4402659833431244\n",
      "Epoch 1945, Loss: 1.9700282514095306, Final Batch Loss: 0.3849146068096161\n",
      "Epoch 1946, Loss: 1.9818808138370514, Final Batch Loss: 0.3780403435230255\n",
      "Epoch 1947, Loss: 2.1522412598133087, Final Batch Loss: 0.48404768109321594\n",
      "Epoch 1948, Loss: 2.0877898037433624, Final Batch Loss: 0.43216177821159363\n",
      "Epoch 1949, Loss: 2.114428788423538, Final Batch Loss: 0.3556554615497589\n",
      "Epoch 1950, Loss: 2.218502700328827, Final Batch Loss: 0.4573209583759308\n",
      "Epoch 1951, Loss: 1.9007843136787415, Final Batch Loss: 0.36614665389060974\n",
      "Epoch 1952, Loss: 2.0079212188720703, Final Batch Loss: 0.44676724076271057\n",
      "Epoch 1953, Loss: 2.178829103708267, Final Batch Loss: 0.4891881048679352\n",
      "Epoch 1954, Loss: 1.9590621590614319, Final Batch Loss: 0.4329823851585388\n",
      "Epoch 1955, Loss: 2.1134256422519684, Final Batch Loss: 0.46937745809555054\n",
      "Epoch 1956, Loss: 1.959669977426529, Final Batch Loss: 0.3669579327106476\n",
      "Epoch 1957, Loss: 2.037355989217758, Final Batch Loss: 0.3920232355594635\n",
      "Epoch 1958, Loss: 2.0432542860507965, Final Batch Loss: 0.34856748580932617\n",
      "Epoch 1959, Loss: 2.1287953853607178, Final Batch Loss: 0.4778749644756317\n",
      "Epoch 1960, Loss: 2.240443766117096, Final Batch Loss: 0.5033704042434692\n",
      "Epoch 1961, Loss: 2.0791279077529907, Final Batch Loss: 0.4512562155723572\n",
      "Epoch 1962, Loss: 1.9898126423358917, Final Batch Loss: 0.40836605429649353\n",
      "Epoch 1963, Loss: 2.221847802400589, Final Batch Loss: 0.5358120203018188\n",
      "Epoch 1964, Loss: 2.0531516671180725, Final Batch Loss: 0.3851979970932007\n",
      "Epoch 1965, Loss: 2.0148870050907135, Final Batch Loss: 0.39726221561431885\n",
      "Epoch 1966, Loss: 2.0583448112010956, Final Batch Loss: 0.47730186581611633\n",
      "Epoch 1967, Loss: 1.9965322315692902, Final Batch Loss: 0.3650493919849396\n",
      "Epoch 1968, Loss: 1.9921513199806213, Final Batch Loss: 0.36327415704727173\n",
      "Epoch 1969, Loss: 1.9988340437412262, Final Batch Loss: 0.40584754943847656\n",
      "Epoch 1970, Loss: 2.156737118959427, Final Batch Loss: 0.43295904994010925\n",
      "Epoch 1971, Loss: 2.0262556076049805, Final Batch Loss: 0.34229567646980286\n",
      "Epoch 1972, Loss: 2.0162829160690308, Final Batch Loss: 0.4229232966899872\n",
      "Epoch 1973, Loss: 2.0642725825309753, Final Batch Loss: 0.45853719115257263\n",
      "Epoch 1974, Loss: 2.204062908887863, Final Batch Loss: 0.47510045766830444\n",
      "Epoch 1975, Loss: 1.9517058730125427, Final Batch Loss: 0.45706793665885925\n",
      "Epoch 1976, Loss: 2.0159428417682648, Final Batch Loss: 0.4206368029117584\n",
      "Epoch 1977, Loss: 2.1450862884521484, Final Batch Loss: 0.5100254416465759\n",
      "Epoch 1978, Loss: 2.105789363384247, Final Batch Loss: 0.4417034387588501\n",
      "Epoch 1979, Loss: 2.0921421349048615, Final Batch Loss: 0.5064514875411987\n",
      "Epoch 1980, Loss: 2.096263200044632, Final Batch Loss: 0.44962695240974426\n",
      "Epoch 1981, Loss: 1.953913390636444, Final Batch Loss: 0.3680323362350464\n",
      "Epoch 1982, Loss: 2.0306729078292847, Final Batch Loss: 0.3607958257198334\n",
      "Epoch 1983, Loss: 2.0814783573150635, Final Batch Loss: 0.47505760192871094\n",
      "Epoch 1984, Loss: 1.9925684332847595, Final Batch Loss: 0.4247468411922455\n",
      "Epoch 1985, Loss: 1.9919418394565582, Final Batch Loss: 0.3548487424850464\n",
      "Epoch 1986, Loss: 1.927232027053833, Final Batch Loss: 0.30422648787498474\n",
      "Epoch 1987, Loss: 2.0656782686710358, Final Batch Loss: 0.4057939648628235\n",
      "Epoch 1988, Loss: 1.9410340487957, Final Batch Loss: 0.4466865658760071\n",
      "Epoch 1989, Loss: 2.0878711342811584, Final Batch Loss: 0.33392903208732605\n",
      "Epoch 1990, Loss: 2.015773832798004, Final Batch Loss: 0.3789740204811096\n",
      "Epoch 1991, Loss: 2.0415457487106323, Final Batch Loss: 0.45481613278388977\n",
      "Epoch 1992, Loss: 2.0572025179862976, Final Batch Loss: 0.5514876842498779\n",
      "Epoch 1993, Loss: 2.062673717737198, Final Batch Loss: 0.4737410545349121\n",
      "Epoch 1994, Loss: 2.095266044139862, Final Batch Loss: 0.4927856922149658\n",
      "Epoch 1995, Loss: 1.9911912381649017, Final Batch Loss: 0.35058870911598206\n",
      "Epoch 1996, Loss: 1.9919155836105347, Final Batch Loss: 0.4619307816028595\n",
      "Epoch 1997, Loss: 2.075362116098404, Final Batch Loss: 0.4365573525428772\n",
      "Epoch 1998, Loss: 2.1374238431453705, Final Batch Loss: 0.5013877749443054\n",
      "Epoch 1999, Loss: 2.0524367690086365, Final Batch Loss: 0.41874033212661743\n",
      "Epoch 2000, Loss: 2.108072817325592, Final Batch Loss: 0.47492140531539917\n",
      "Epoch 2001, Loss: 1.9643940031528473, Final Batch Loss: 0.4322861135005951\n",
      "Epoch 2002, Loss: 1.998501032590866, Final Batch Loss: 0.2677851617336273\n",
      "Epoch 2003, Loss: 1.9873742163181305, Final Batch Loss: 0.39818376302719116\n",
      "Epoch 2004, Loss: 2.0409558713436127, Final Batch Loss: 0.37524914741516113\n",
      "Epoch 2005, Loss: 2.028351843357086, Final Batch Loss: 0.45994535088539124\n",
      "Epoch 2006, Loss: 1.8946033120155334, Final Batch Loss: 0.32241034507751465\n",
      "Epoch 2007, Loss: 2.0546740889549255, Final Batch Loss: 0.5331881642341614\n",
      "Epoch 2008, Loss: 2.0328460335731506, Final Batch Loss: 0.462247759103775\n",
      "Epoch 2009, Loss: 2.003791570663452, Final Batch Loss: 0.317344069480896\n",
      "Epoch 2010, Loss: 2.0286575853824615, Final Batch Loss: 0.40900611877441406\n",
      "Epoch 2011, Loss: 1.9738812148571014, Final Batch Loss: 0.3182212710380554\n",
      "Epoch 2012, Loss: 1.874855101108551, Final Batch Loss: 0.27621203660964966\n",
      "Epoch 2013, Loss: 2.249350219964981, Final Batch Loss: 0.49840739369392395\n",
      "Epoch 2014, Loss: 1.8653692603111267, Final Batch Loss: 0.3351590633392334\n",
      "Epoch 2015, Loss: 2.065162867307663, Final Batch Loss: 0.4274901747703552\n",
      "Epoch 2016, Loss: 1.9424926936626434, Final Batch Loss: 0.38852858543395996\n",
      "Epoch 2017, Loss: 2.0857978463172913, Final Batch Loss: 0.4355587065219879\n",
      "Epoch 2018, Loss: 2.0540363490581512, Final Batch Loss: 0.4463782012462616\n",
      "Epoch 2019, Loss: 1.9735690653324127, Final Batch Loss: 0.3183639645576477\n",
      "Epoch 2020, Loss: 2.0561036467552185, Final Batch Loss: 0.47410905361175537\n",
      "Epoch 2021, Loss: 2.1250595450401306, Final Batch Loss: 0.4849727749824524\n",
      "Epoch 2022, Loss: 2.055361419916153, Final Batch Loss: 0.3974420130252838\n",
      "Epoch 2023, Loss: 1.9620929658412933, Final Batch Loss: 0.2937297821044922\n",
      "Epoch 2024, Loss: 2.005557268857956, Final Batch Loss: 0.340946227312088\n",
      "Epoch 2025, Loss: 2.012764722108841, Final Batch Loss: 0.32111597061157227\n",
      "Epoch 2026, Loss: 2.0662491023540497, Final Batch Loss: 0.5333153605461121\n",
      "Epoch 2027, Loss: 1.9654180705547333, Final Batch Loss: 0.36865538358688354\n",
      "Epoch 2028, Loss: 2.1151550710201263, Final Batch Loss: 0.37709030508995056\n",
      "Epoch 2029, Loss: 2.016927093267441, Final Batch Loss: 0.36367854475975037\n",
      "Epoch 2030, Loss: 1.9920934438705444, Final Batch Loss: 0.49779069423675537\n",
      "Epoch 2031, Loss: 2.105984181165695, Final Batch Loss: 0.3901096284389496\n",
      "Epoch 2032, Loss: 2.0925932228565216, Final Batch Loss: 0.47829753160476685\n",
      "Epoch 2033, Loss: 2.0170136988162994, Final Batch Loss: 0.39313361048698425\n",
      "Epoch 2034, Loss: 2.0880350470542908, Final Batch Loss: 0.40507131814956665\n",
      "Epoch 2035, Loss: 2.10989773273468, Final Batch Loss: 0.33696526288986206\n",
      "Epoch 2036, Loss: 2.008261203765869, Final Batch Loss: 0.37224745750427246\n",
      "Epoch 2037, Loss: 1.948753535747528, Final Batch Loss: 0.407309353351593\n",
      "Epoch 2038, Loss: 1.9972314834594727, Final Batch Loss: 0.3371640145778656\n",
      "Epoch 2039, Loss: 1.8757260143756866, Final Batch Loss: 0.41576507687568665\n",
      "Epoch 2040, Loss: 2.0354616940021515, Final Batch Loss: 0.38248345255851746\n",
      "Epoch 2041, Loss: 1.9197004437446594, Final Batch Loss: 0.3743642568588257\n",
      "Epoch 2042, Loss: 2.13522207736969, Final Batch Loss: 0.4074934720993042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2043, Loss: 1.942415714263916, Final Batch Loss: 0.3535015881061554\n",
      "Epoch 2044, Loss: 1.9242708086967468, Final Batch Loss: 0.38040637969970703\n",
      "Epoch 2045, Loss: 1.877975881099701, Final Batch Loss: 0.33202439546585083\n",
      "Epoch 2046, Loss: 2.0211829841136932, Final Batch Loss: 0.4202880859375\n",
      "Epoch 2047, Loss: 1.8966522812843323, Final Batch Loss: 0.39955639839172363\n",
      "Epoch 2048, Loss: 1.865777462720871, Final Batch Loss: 0.3319288492202759\n",
      "Epoch 2049, Loss: 2.0398833751678467, Final Batch Loss: 0.48095062375068665\n",
      "Epoch 2050, Loss: 2.1053945422172546, Final Batch Loss: 0.42641115188598633\n",
      "Epoch 2051, Loss: 2.0879261791706085, Final Batch Loss: 0.4808485805988312\n",
      "Epoch 2052, Loss: 1.9386443495750427, Final Batch Loss: 0.40088820457458496\n",
      "Epoch 2053, Loss: 2.077167183160782, Final Batch Loss: 0.4279803931713104\n",
      "Epoch 2054, Loss: 2.034093499183655, Final Batch Loss: 0.35693588852882385\n",
      "Epoch 2055, Loss: 2.0495846569538116, Final Batch Loss: 0.46878063678741455\n",
      "Epoch 2056, Loss: 2.026835948228836, Final Batch Loss: 0.4051465094089508\n",
      "Epoch 2057, Loss: 2.127749443054199, Final Batch Loss: 0.3978811204433441\n",
      "Epoch 2058, Loss: 2.1631410717964172, Final Batch Loss: 0.48013609647750854\n",
      "Epoch 2059, Loss: 2.027501791715622, Final Batch Loss: 0.37729135155677795\n",
      "Epoch 2060, Loss: 1.9940315783023834, Final Batch Loss: 0.4344092309474945\n",
      "Epoch 2061, Loss: 1.9683806896209717, Final Batch Loss: 0.3913307189941406\n",
      "Epoch 2062, Loss: 2.1968441903591156, Final Batch Loss: 0.49661627411842346\n",
      "Epoch 2063, Loss: 1.9691004157066345, Final Batch Loss: 0.468111127614975\n",
      "Epoch 2064, Loss: 2.189759224653244, Final Batch Loss: 0.43069589138031006\n",
      "Epoch 2065, Loss: 1.9814341962337494, Final Batch Loss: 0.32430100440979004\n",
      "Epoch 2066, Loss: 1.9655678272247314, Final Batch Loss: 0.4181598126888275\n",
      "Epoch 2067, Loss: 1.9250701367855072, Final Batch Loss: 0.4498993456363678\n",
      "Epoch 2068, Loss: 1.9337055385112762, Final Batch Loss: 0.3613165020942688\n",
      "Epoch 2069, Loss: 2.0255912840366364, Final Batch Loss: 0.38594850897789\n",
      "Epoch 2070, Loss: 2.0981752276420593, Final Batch Loss: 0.48096728324890137\n",
      "Epoch 2071, Loss: 1.98490309715271, Final Batch Loss: 0.3888724446296692\n",
      "Epoch 2072, Loss: 1.9794181287288666, Final Batch Loss: 0.3763123154640198\n",
      "Epoch 2073, Loss: 2.056697964668274, Final Batch Loss: 0.3888322114944458\n",
      "Epoch 2074, Loss: 1.9578961431980133, Final Batch Loss: 0.33907029032707214\n",
      "Epoch 2075, Loss: 2.0662530064582825, Final Batch Loss: 0.3036913275718689\n",
      "Epoch 2076, Loss: 1.9731040596961975, Final Batch Loss: 0.41438958048820496\n",
      "Epoch 2077, Loss: 2.027416914701462, Final Batch Loss: 0.4167923927307129\n",
      "Epoch 2078, Loss: 1.8850396871566772, Final Batch Loss: 0.32422664761543274\n",
      "Epoch 2079, Loss: 1.9829942286014557, Final Batch Loss: 0.31764501333236694\n",
      "Epoch 2080, Loss: 1.9384068250656128, Final Batch Loss: 0.43462055921554565\n",
      "Epoch 2081, Loss: 1.9225363433361053, Final Batch Loss: 0.3505208492279053\n",
      "Epoch 2082, Loss: 1.9733637571334839, Final Batch Loss: 0.39754989743232727\n",
      "Epoch 2083, Loss: 1.96981942653656, Final Batch Loss: 0.4185105562210083\n",
      "Epoch 2084, Loss: 1.937666803598404, Final Batch Loss: 0.45969605445861816\n",
      "Epoch 2085, Loss: 2.105930835008621, Final Batch Loss: 0.48878273367881775\n",
      "Epoch 2086, Loss: 1.8753053843975067, Final Batch Loss: 0.4252675175666809\n",
      "Epoch 2087, Loss: 2.1153651773929596, Final Batch Loss: 0.48016947507858276\n",
      "Epoch 2088, Loss: 1.8272488117218018, Final Batch Loss: 0.41369596123695374\n",
      "Epoch 2089, Loss: 2.062122642993927, Final Batch Loss: 0.38460642099380493\n",
      "Epoch 2090, Loss: 1.940659761428833, Final Batch Loss: 0.36291784048080444\n",
      "Epoch 2091, Loss: 1.8345627188682556, Final Batch Loss: 0.35742175579071045\n",
      "Epoch 2092, Loss: 1.9301750361919403, Final Batch Loss: 0.35369136929512024\n",
      "Epoch 2093, Loss: 2.0856934785842896, Final Batch Loss: 0.4965941905975342\n",
      "Epoch 2094, Loss: 1.8932582139968872, Final Batch Loss: 0.3071553111076355\n",
      "Epoch 2095, Loss: 1.9789786338806152, Final Batch Loss: 0.29159241914749146\n",
      "Epoch 2096, Loss: 2.0590720176696777, Final Batch Loss: 0.33862724900245667\n",
      "Epoch 2097, Loss: 2.103072077035904, Final Batch Loss: 0.42988163232803345\n",
      "Epoch 2098, Loss: 2.0336029529571533, Final Batch Loss: 0.49758774042129517\n",
      "Epoch 2099, Loss: 1.904907375574112, Final Batch Loss: 0.3662300705909729\n",
      "Epoch 2100, Loss: 1.93412247300148, Final Batch Loss: 0.33712705969810486\n",
      "Epoch 2101, Loss: 2.1067817211151123, Final Batch Loss: 0.5192095041275024\n",
      "Epoch 2102, Loss: 1.882655143737793, Final Batch Loss: 0.39383384585380554\n",
      "Epoch 2103, Loss: 2.00227290391922, Final Batch Loss: 0.4406804144382477\n",
      "Epoch 2104, Loss: 1.8893117606639862, Final Batch Loss: 0.3266974687576294\n",
      "Epoch 2105, Loss: 2.0537520051002502, Final Batch Loss: 0.42110925912857056\n",
      "Epoch 2106, Loss: 2.044029802083969, Final Batch Loss: 0.31547561287879944\n",
      "Epoch 2107, Loss: 1.9381881058216095, Final Batch Loss: 0.3530111610889435\n",
      "Epoch 2108, Loss: 1.931378573179245, Final Batch Loss: 0.3827461302280426\n",
      "Epoch 2109, Loss: 2.0229987800121307, Final Batch Loss: 0.42396625876426697\n",
      "Epoch 2110, Loss: 2.0841221511363983, Final Batch Loss: 0.47031211853027344\n",
      "Epoch 2111, Loss: 1.8800631761550903, Final Batch Loss: 0.32222509384155273\n",
      "Epoch 2112, Loss: 2.0820678770542145, Final Batch Loss: 0.4843767285346985\n",
      "Epoch 2113, Loss: 1.9659352600574493, Final Batch Loss: 0.32241320610046387\n",
      "Epoch 2114, Loss: 2.069764196872711, Final Batch Loss: 0.4613426923751831\n",
      "Epoch 2115, Loss: 2.049358516931534, Final Batch Loss: 0.44838255643844604\n",
      "Epoch 2116, Loss: 1.8787652552127838, Final Batch Loss: 0.34258902072906494\n",
      "Epoch 2117, Loss: 1.988408625125885, Final Batch Loss: 0.34914711117744446\n",
      "Epoch 2118, Loss: 2.05253067612648, Final Batch Loss: 0.4705730676651001\n",
      "Epoch 2119, Loss: 1.917796015739441, Final Batch Loss: 0.41910025477409363\n",
      "Epoch 2120, Loss: 2.0439424216747284, Final Batch Loss: 0.43955153226852417\n",
      "Epoch 2121, Loss: 1.8856698274612427, Final Batch Loss: 0.3913732171058655\n",
      "Epoch 2122, Loss: 1.904041826725006, Final Batch Loss: 0.35501644015312195\n",
      "Epoch 2123, Loss: 2.0041555166244507, Final Batch Loss: 0.4259384274482727\n",
      "Epoch 2124, Loss: 2.0130451917648315, Final Batch Loss: 0.4026266038417816\n",
      "Epoch 2125, Loss: 2.050860434770584, Final Batch Loss: 0.46480727195739746\n",
      "Epoch 2126, Loss: 1.8703938722610474, Final Batch Loss: 0.43876680731773376\n",
      "Epoch 2127, Loss: 1.99666428565979, Final Batch Loss: 0.3617076873779297\n",
      "Epoch 2128, Loss: 2.10476815700531, Final Batch Loss: 0.49009642004966736\n",
      "Epoch 2129, Loss: 2.0809832513332367, Final Batch Loss: 0.45726361870765686\n",
      "Epoch 2130, Loss: 1.9135911762714386, Final Batch Loss: 0.36838242411613464\n",
      "Epoch 2131, Loss: 1.8497739434242249, Final Batch Loss: 0.3275336027145386\n",
      "Epoch 2132, Loss: 2.0354384183883667, Final Batch Loss: 0.3559618592262268\n",
      "Epoch 2133, Loss: 2.114073693752289, Final Batch Loss: 0.4501860439777374\n",
      "Epoch 2134, Loss: 1.9396140575408936, Final Batch Loss: 0.3092876076698303\n",
      "Epoch 2135, Loss: 1.9784600138664246, Final Batch Loss: 0.29875507950782776\n",
      "Epoch 2136, Loss: 1.8699771165847778, Final Batch Loss: 0.3313872218132019\n",
      "Epoch 2137, Loss: 1.9577282965183258, Final Batch Loss: 0.48287469148635864\n",
      "Epoch 2138, Loss: 1.8546741902828217, Final Batch Loss: 0.32219579815864563\n",
      "Epoch 2139, Loss: 1.903839647769928, Final Batch Loss: 0.411796897649765\n",
      "Epoch 2140, Loss: 1.82423934340477, Final Batch Loss: 0.36330506205558777\n",
      "Epoch 2141, Loss: 1.8578940331935883, Final Batch Loss: 0.3307401239871979\n",
      "Epoch 2142, Loss: 1.9724109768867493, Final Batch Loss: 0.3973071575164795\n",
      "Epoch 2143, Loss: 1.85995551943779, Final Batch Loss: 0.3648228645324707\n",
      "Epoch 2144, Loss: 2.008857786655426, Final Batch Loss: 0.4550071060657501\n",
      "Epoch 2145, Loss: 1.8318687677383423, Final Batch Loss: 0.3611738085746765\n",
      "Epoch 2146, Loss: 1.969368577003479, Final Batch Loss: 0.3438122570514679\n",
      "Epoch 2147, Loss: 1.8680250346660614, Final Batch Loss: 0.3581582307815552\n",
      "Epoch 2148, Loss: 1.9931064546108246, Final Batch Loss: 0.36480894684791565\n",
      "Epoch 2149, Loss: 1.9366741478443146, Final Batch Loss: 0.3691962659358978\n",
      "Epoch 2150, Loss: 2.02076655626297, Final Batch Loss: 0.42192745208740234\n",
      "Epoch 2151, Loss: 1.9574952721595764, Final Batch Loss: 0.4719502925872803\n",
      "Epoch 2152, Loss: 2.0436553955078125, Final Batch Loss: 0.44858741760253906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2153, Loss: 1.9393028020858765, Final Batch Loss: 0.38066357374191284\n",
      "Epoch 2154, Loss: 1.8777765035629272, Final Batch Loss: 0.3163260221481323\n",
      "Epoch 2155, Loss: 2.0916959643363953, Final Batch Loss: 0.4450851380825043\n",
      "Epoch 2156, Loss: 2.029009521007538, Final Batch Loss: 0.45589563250541687\n",
      "Epoch 2157, Loss: 2.0258300602436066, Final Batch Loss: 0.3866002559661865\n",
      "Epoch 2158, Loss: 1.9691927433013916, Final Batch Loss: 0.4185430109500885\n",
      "Epoch 2159, Loss: 2.056788295507431, Final Batch Loss: 0.47910818457603455\n",
      "Epoch 2160, Loss: 1.959287405014038, Final Batch Loss: 0.4196089804172516\n",
      "Epoch 2161, Loss: 1.930525302886963, Final Batch Loss: 0.43093791604042053\n",
      "Epoch 2162, Loss: 1.9743074476718903, Final Batch Loss: 0.413337379693985\n",
      "Epoch 2163, Loss: 1.9663668870925903, Final Batch Loss: 0.43109092116355896\n",
      "Epoch 2164, Loss: 1.9752734005451202, Final Batch Loss: 0.3622293174266815\n",
      "Epoch 2165, Loss: 2.009969562292099, Final Batch Loss: 0.43111640214920044\n",
      "Epoch 2166, Loss: 2.002293348312378, Final Batch Loss: 0.4800209403038025\n",
      "Epoch 2167, Loss: 1.9124626815319061, Final Batch Loss: 0.3568778932094574\n",
      "Epoch 2168, Loss: 1.9075143933296204, Final Batch Loss: 0.3811883330345154\n",
      "Epoch 2169, Loss: 1.977807492017746, Final Batch Loss: 0.43161216378211975\n",
      "Epoch 2170, Loss: 2.0173484683036804, Final Batch Loss: 0.555494487285614\n",
      "Epoch 2171, Loss: 2.085965096950531, Final Batch Loss: 0.3658081293106079\n",
      "Epoch 2172, Loss: 2.024045169353485, Final Batch Loss: 0.3776843547821045\n",
      "Epoch 2173, Loss: 1.8667262196540833, Final Batch Loss: 0.271045446395874\n",
      "Epoch 2174, Loss: 1.960272341966629, Final Batch Loss: 0.3433952033519745\n",
      "Epoch 2175, Loss: 2.0151194632053375, Final Batch Loss: 0.42207545042037964\n",
      "Epoch 2176, Loss: 1.9539388418197632, Final Batch Loss: 0.3930174708366394\n",
      "Epoch 2177, Loss: 1.9805263876914978, Final Batch Loss: 0.42733076214790344\n",
      "Epoch 2178, Loss: 2.000083565711975, Final Batch Loss: 0.4058068096637726\n",
      "Epoch 2179, Loss: 1.9938668012619019, Final Batch Loss: 0.4511469304561615\n",
      "Epoch 2180, Loss: 1.9265927970409393, Final Batch Loss: 0.3548727035522461\n",
      "Epoch 2181, Loss: 1.7511995732784271, Final Batch Loss: 0.27181166410446167\n",
      "Epoch 2182, Loss: 1.933987945318222, Final Batch Loss: 0.3625935912132263\n",
      "Epoch 2183, Loss: 1.8530760705471039, Final Batch Loss: 0.291788786649704\n",
      "Epoch 2184, Loss: 2.0930162966251373, Final Batch Loss: 0.35773658752441406\n",
      "Epoch 2185, Loss: 2.0055573284626007, Final Batch Loss: 0.42012572288513184\n",
      "Epoch 2186, Loss: 1.9284383952617645, Final Batch Loss: 0.3774554431438446\n",
      "Epoch 2187, Loss: 2.0334328711032867, Final Batch Loss: 0.4141896963119507\n",
      "Epoch 2188, Loss: 2.0280096232891083, Final Batch Loss: 0.39398935437202454\n",
      "Epoch 2189, Loss: 1.8746663331985474, Final Batch Loss: 0.3337719738483429\n",
      "Epoch 2190, Loss: 2.0573290586471558, Final Batch Loss: 0.3937545120716095\n",
      "Epoch 2191, Loss: 1.9364990890026093, Final Batch Loss: 0.40337586402893066\n",
      "Epoch 2192, Loss: 1.9763676822185516, Final Batch Loss: 0.33568763732910156\n",
      "Epoch 2193, Loss: 1.9885615110397339, Final Batch Loss: 0.4620351791381836\n",
      "Epoch 2194, Loss: 1.9269266724586487, Final Batch Loss: 0.41971486806869507\n",
      "Epoch 2195, Loss: 2.0117739737033844, Final Batch Loss: 0.36963874101638794\n",
      "Epoch 2196, Loss: 2.1596412658691406, Final Batch Loss: 0.546377420425415\n",
      "Epoch 2197, Loss: 1.9337500035762787, Final Batch Loss: 0.3916182518005371\n",
      "Epoch 2198, Loss: 1.9599210917949677, Final Batch Loss: 0.3803694248199463\n",
      "Epoch 2199, Loss: 2.1250198781490326, Final Batch Loss: 0.4174421429634094\n",
      "Epoch 2200, Loss: 1.8288449943065643, Final Batch Loss: 0.30926164984703064\n",
      "Epoch 2201, Loss: 1.9598489701747894, Final Batch Loss: 0.42640045285224915\n",
      "Epoch 2202, Loss: 2.028415411710739, Final Batch Loss: 0.4653603434562683\n",
      "Epoch 2203, Loss: 1.9278170466423035, Final Batch Loss: 0.3153296113014221\n",
      "Epoch 2204, Loss: 1.9833715558052063, Final Batch Loss: 0.4344213604927063\n",
      "Epoch 2205, Loss: 1.8646849691867828, Final Batch Loss: 0.41883984208106995\n",
      "Epoch 2206, Loss: 1.972503811120987, Final Batch Loss: 0.361336886882782\n",
      "Epoch 2207, Loss: 1.8579360842704773, Final Batch Loss: 0.38515129685401917\n",
      "Epoch 2208, Loss: 1.867091953754425, Final Batch Loss: 0.3802553415298462\n",
      "Epoch 2209, Loss: 1.9381778538227081, Final Batch Loss: 0.4290847182273865\n",
      "Epoch 2210, Loss: 2.012246161699295, Final Batch Loss: 0.4530643820762634\n",
      "Epoch 2211, Loss: 1.9113614857196808, Final Batch Loss: 0.3737965226173401\n",
      "Epoch 2212, Loss: 1.8715164065361023, Final Batch Loss: 0.3378332555294037\n",
      "Epoch 2213, Loss: 1.9726543426513672, Final Batch Loss: 0.3475207984447479\n",
      "Epoch 2214, Loss: 2.043617904186249, Final Batch Loss: 0.360366553068161\n",
      "Epoch 2215, Loss: 1.897500455379486, Final Batch Loss: 0.46378225088119507\n",
      "Epoch 2216, Loss: 1.8133322894573212, Final Batch Loss: 0.46095454692840576\n",
      "Epoch 2217, Loss: 1.9588797092437744, Final Batch Loss: 0.2910107374191284\n",
      "Epoch 2218, Loss: 1.8386033773422241, Final Batch Loss: 0.37444353103637695\n",
      "Epoch 2219, Loss: 1.987502545118332, Final Batch Loss: 0.4707632064819336\n",
      "Epoch 2220, Loss: 1.9384945333003998, Final Batch Loss: 0.3178524076938629\n",
      "Epoch 2221, Loss: 1.8463493287563324, Final Batch Loss: 0.3883454501628876\n",
      "Epoch 2222, Loss: 1.860756665468216, Final Batch Loss: 0.42077189683914185\n",
      "Epoch 2223, Loss: 2.0012817084789276, Final Batch Loss: 0.3704643249511719\n",
      "Epoch 2224, Loss: 1.92763289809227, Final Batch Loss: 0.40089327096939087\n",
      "Epoch 2225, Loss: 1.9193716943264008, Final Batch Loss: 0.362684428691864\n",
      "Epoch 2226, Loss: 2.008784145116806, Final Batch Loss: 0.46721455454826355\n",
      "Epoch 2227, Loss: 1.9051285088062286, Final Batch Loss: 0.3808686137199402\n",
      "Epoch 2228, Loss: 1.8136818706989288, Final Batch Loss: 0.3549233376979828\n",
      "Epoch 2229, Loss: 1.917704999446869, Final Batch Loss: 0.34825557470321655\n",
      "Epoch 2230, Loss: 2.028845429420471, Final Batch Loss: 0.3977031111717224\n",
      "Epoch 2231, Loss: 1.9206702709197998, Final Batch Loss: 0.31001219153404236\n",
      "Epoch 2232, Loss: 1.9369227588176727, Final Batch Loss: 0.42330917716026306\n",
      "Epoch 2233, Loss: 1.8572044372558594, Final Batch Loss: 0.4142158031463623\n",
      "Epoch 2234, Loss: 1.8806330561637878, Final Batch Loss: 0.3940364420413971\n",
      "Epoch 2235, Loss: 1.9158735275268555, Final Batch Loss: 0.3846917450428009\n",
      "Epoch 2236, Loss: 1.933771789073944, Final Batch Loss: 0.31871750950813293\n",
      "Epoch 2237, Loss: 1.9412517547607422, Final Batch Loss: 0.36960363388061523\n",
      "Epoch 2238, Loss: 1.9653603732585907, Final Batch Loss: 0.5018792748451233\n",
      "Epoch 2239, Loss: 1.916873723268509, Final Batch Loss: 0.42611244320869446\n",
      "Epoch 2240, Loss: 2.033298373222351, Final Batch Loss: 0.40626785159111023\n",
      "Epoch 2241, Loss: 1.8763694763183594, Final Batch Loss: 0.39551958441734314\n",
      "Epoch 2242, Loss: 1.93000328540802, Final Batch Loss: 0.3054088056087494\n",
      "Epoch 2243, Loss: 1.7157733142375946, Final Batch Loss: 0.296991765499115\n",
      "Epoch 2244, Loss: 1.87581005692482, Final Batch Loss: 0.2905270457267761\n",
      "Epoch 2245, Loss: 1.7727640867233276, Final Batch Loss: 0.31756314635276794\n",
      "Epoch 2246, Loss: 1.9326348304748535, Final Batch Loss: 0.34801504015922546\n",
      "Epoch 2247, Loss: 1.8457427620887756, Final Batch Loss: 0.37923213839530945\n",
      "Epoch 2248, Loss: 1.9082046449184418, Final Batch Loss: 0.38818684220314026\n",
      "Epoch 2249, Loss: 1.8894071280956268, Final Batch Loss: 0.3215213119983673\n",
      "Epoch 2250, Loss: 1.8882152140140533, Final Batch Loss: 0.3344426453113556\n",
      "Epoch 2251, Loss: 1.8367693424224854, Final Batch Loss: 0.27668145298957825\n",
      "Epoch 2252, Loss: 1.9611017107963562, Final Batch Loss: 0.40181413292884827\n",
      "Epoch 2253, Loss: 1.6828996539115906, Final Batch Loss: 0.2932323217391968\n",
      "Epoch 2254, Loss: 1.8267383575439453, Final Batch Loss: 0.3640728294849396\n",
      "Epoch 2255, Loss: 1.8478007912635803, Final Batch Loss: 0.25315478444099426\n",
      "Epoch 2256, Loss: 1.9928034245967865, Final Batch Loss: 0.4995560646057129\n",
      "Epoch 2257, Loss: 1.884643405675888, Final Batch Loss: 0.3565276861190796\n",
      "Epoch 2258, Loss: 1.7952852249145508, Final Batch Loss: 0.2768317461013794\n",
      "Epoch 2259, Loss: 1.8503410518169403, Final Batch Loss: 0.2900806665420532\n",
      "Epoch 2260, Loss: 1.8767673671245575, Final Batch Loss: 0.4666345417499542\n",
      "Epoch 2261, Loss: 1.930826097726822, Final Batch Loss: 0.3557610511779785\n",
      "Epoch 2262, Loss: 1.8677131831645966, Final Batch Loss: 0.36328935623168945\n",
      "Epoch 2263, Loss: 1.8121828436851501, Final Batch Loss: 0.32704392075538635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2264, Loss: 1.824070155620575, Final Batch Loss: 0.3700392246246338\n",
      "Epoch 2265, Loss: 1.897981584072113, Final Batch Loss: 0.3321791887283325\n",
      "Epoch 2266, Loss: 1.8960923254489899, Final Batch Loss: 0.35776975750923157\n",
      "Epoch 2267, Loss: 1.7633013427257538, Final Batch Loss: 0.31842663884162903\n",
      "Epoch 2268, Loss: 1.9725496470928192, Final Batch Loss: 0.42744410037994385\n",
      "Epoch 2269, Loss: 1.8672073483467102, Final Batch Loss: 0.3230832815170288\n",
      "Epoch 2270, Loss: 2.0165365636348724, Final Batch Loss: 0.41752246022224426\n",
      "Epoch 2271, Loss: 1.9747823476791382, Final Batch Loss: 0.3828412890434265\n",
      "Epoch 2272, Loss: 1.8462633788585663, Final Batch Loss: 0.3546614348888397\n",
      "Epoch 2273, Loss: 1.9309393167495728, Final Batch Loss: 0.4406905472278595\n",
      "Epoch 2274, Loss: 1.9005293250083923, Final Batch Loss: 0.35714736580848694\n",
      "Epoch 2275, Loss: 1.8779246807098389, Final Batch Loss: 0.35680291056632996\n",
      "Epoch 2276, Loss: 1.8666088283061981, Final Batch Loss: 0.32868659496307373\n",
      "Epoch 2277, Loss: 1.8794289529323578, Final Batch Loss: 0.4125801622867584\n",
      "Epoch 2278, Loss: 1.8034278750419617, Final Batch Loss: 0.3782057762145996\n",
      "Epoch 2279, Loss: 1.9198599755764008, Final Batch Loss: 0.4232833683490753\n",
      "Epoch 2280, Loss: 1.8196983337402344, Final Batch Loss: 0.3155161142349243\n",
      "Epoch 2281, Loss: 1.9988766312599182, Final Batch Loss: 0.4675739109516144\n",
      "Epoch 2282, Loss: 2.1766968071460724, Final Batch Loss: 0.34469306468963623\n",
      "Epoch 2283, Loss: 1.8207020163536072, Final Batch Loss: 0.36341553926467896\n",
      "Epoch 2284, Loss: 1.902477741241455, Final Batch Loss: 0.2860735356807709\n",
      "Epoch 2285, Loss: 1.9017451107501984, Final Batch Loss: 0.3495721220970154\n",
      "Epoch 2286, Loss: 1.824601262807846, Final Batch Loss: 0.26836925745010376\n",
      "Epoch 2287, Loss: 1.752206414937973, Final Batch Loss: 0.351542204618454\n",
      "Epoch 2288, Loss: 1.8726745247840881, Final Batch Loss: 0.3778562843799591\n",
      "Epoch 2289, Loss: 1.8689717650413513, Final Batch Loss: 0.41521739959716797\n",
      "Epoch 2290, Loss: 1.823585867881775, Final Batch Loss: 0.38730940222740173\n",
      "Epoch 2291, Loss: 1.8392097055912018, Final Batch Loss: 0.29500001668930054\n",
      "Epoch 2292, Loss: 1.9176371097564697, Final Batch Loss: 0.37496790289878845\n",
      "Epoch 2293, Loss: 1.8781490325927734, Final Batch Loss: 0.3718673586845398\n",
      "Epoch 2294, Loss: 1.999029666185379, Final Batch Loss: 0.42852783203125\n",
      "Epoch 2295, Loss: 1.903867244720459, Final Batch Loss: 0.4039503037929535\n",
      "Epoch 2296, Loss: 1.8491977751255035, Final Batch Loss: 0.37545523047447205\n",
      "Epoch 2297, Loss: 1.9414101541042328, Final Batch Loss: 0.34810489416122437\n",
      "Epoch 2298, Loss: 2.052221328020096, Final Batch Loss: 0.42749524116516113\n",
      "Epoch 2299, Loss: 1.9769262671470642, Final Batch Loss: 0.4109567403793335\n",
      "Epoch 2300, Loss: 1.7886978685855865, Final Batch Loss: 0.36996522545814514\n",
      "Epoch 2301, Loss: 2.027799040079117, Final Batch Loss: 0.41333073377609253\n",
      "Epoch 2302, Loss: 1.9860816895961761, Final Batch Loss: 0.4563243091106415\n",
      "Epoch 2303, Loss: 1.877559781074524, Final Batch Loss: 0.32224053144454956\n",
      "Epoch 2304, Loss: 1.972289353609085, Final Batch Loss: 0.3647928833961487\n",
      "Epoch 2305, Loss: 1.8290398120880127, Final Batch Loss: 0.30820372700691223\n",
      "Epoch 2306, Loss: 1.907627135515213, Final Batch Loss: 0.42860233783721924\n",
      "Epoch 2307, Loss: 2.011935204267502, Final Batch Loss: 0.4781685173511505\n",
      "Epoch 2308, Loss: 2.0538210570812225, Final Batch Loss: 0.43979600071907043\n",
      "Epoch 2309, Loss: 1.8500776290893555, Final Batch Loss: 0.3278636634349823\n",
      "Epoch 2310, Loss: 1.818626195192337, Final Batch Loss: 0.38461360335350037\n",
      "Epoch 2311, Loss: 1.766604632139206, Final Batch Loss: 0.3483041524887085\n",
      "Epoch 2312, Loss: 1.982884258031845, Final Batch Loss: 0.35002443194389343\n",
      "Epoch 2313, Loss: 1.9287367463111877, Final Batch Loss: 0.40167102217674255\n",
      "Epoch 2314, Loss: 1.916229009628296, Final Batch Loss: 0.38147690892219543\n",
      "Epoch 2315, Loss: 1.9244962334632874, Final Batch Loss: 0.3284800350666046\n",
      "Epoch 2316, Loss: 1.8629953563213348, Final Batch Loss: 0.3470289707183838\n",
      "Epoch 2317, Loss: 1.8355390429496765, Final Batch Loss: 0.3025047481060028\n",
      "Epoch 2318, Loss: 1.7637139856815338, Final Batch Loss: 0.3886793255805969\n",
      "Epoch 2319, Loss: 2.0647252202033997, Final Batch Loss: 0.45347583293914795\n",
      "Epoch 2320, Loss: 1.972386211156845, Final Batch Loss: 0.43033310770988464\n",
      "Epoch 2321, Loss: 1.8591452240943909, Final Batch Loss: 0.297834187746048\n",
      "Epoch 2322, Loss: 1.8986104428768158, Final Batch Loss: 0.4025964140892029\n",
      "Epoch 2323, Loss: 1.833172082901001, Final Batch Loss: 0.3422623574733734\n",
      "Epoch 2324, Loss: 1.8617208302021027, Final Batch Loss: 0.35957416892051697\n",
      "Epoch 2325, Loss: 1.835306465625763, Final Batch Loss: 0.38344550132751465\n",
      "Epoch 2326, Loss: 1.902827799320221, Final Batch Loss: 0.4501696825027466\n",
      "Epoch 2327, Loss: 1.9196756184101105, Final Batch Loss: 0.3514038622379303\n",
      "Epoch 2328, Loss: 1.776789665222168, Final Batch Loss: 0.39028066396713257\n",
      "Epoch 2329, Loss: 1.9106177985668182, Final Batch Loss: 0.4155741333961487\n",
      "Epoch 2330, Loss: 1.8249634206295013, Final Batch Loss: 0.3339286148548126\n",
      "Epoch 2331, Loss: 1.8841867446899414, Final Batch Loss: 0.3494358956813812\n",
      "Epoch 2332, Loss: 1.8797574043273926, Final Batch Loss: 0.3989545404911041\n",
      "Epoch 2333, Loss: 2.037050634622574, Final Batch Loss: 0.4283250570297241\n",
      "Epoch 2334, Loss: 1.9730133414268494, Final Batch Loss: 0.4284789562225342\n",
      "Epoch 2335, Loss: 1.91287499666214, Final Batch Loss: 0.41501879692077637\n",
      "Epoch 2336, Loss: 1.8680714666843414, Final Batch Loss: 0.44548872113227844\n",
      "Epoch 2337, Loss: 1.93105947971344, Final Batch Loss: 0.38184890151023865\n",
      "Epoch 2338, Loss: 1.8479925394058228, Final Batch Loss: 0.43365538120269775\n",
      "Epoch 2339, Loss: 1.9448966085910797, Final Batch Loss: 0.2919546961784363\n",
      "Epoch 2340, Loss: 1.8126075863838196, Final Batch Loss: 0.401975154876709\n",
      "Epoch 2341, Loss: 1.7871353179216385, Final Batch Loss: 0.24963344633579254\n",
      "Epoch 2342, Loss: 2.0273774564266205, Final Batch Loss: 0.4689274728298187\n",
      "Epoch 2343, Loss: 2.004396915435791, Final Batch Loss: 0.4024971127510071\n",
      "Epoch 2344, Loss: 1.8947957754135132, Final Batch Loss: 0.4369542598724365\n",
      "Epoch 2345, Loss: 1.9230553209781647, Final Batch Loss: 0.36047834157943726\n",
      "Epoch 2346, Loss: 1.9252043962478638, Final Batch Loss: 0.40295860171318054\n",
      "Epoch 2347, Loss: 1.8584338128566742, Final Batch Loss: 0.35754629969596863\n",
      "Epoch 2348, Loss: 1.8077659606933594, Final Batch Loss: 0.37322330474853516\n",
      "Epoch 2349, Loss: 1.7345872521400452, Final Batch Loss: 0.38485217094421387\n",
      "Epoch 2350, Loss: 1.7656073868274689, Final Batch Loss: 0.3761357367038727\n",
      "Epoch 2351, Loss: 1.7484904825687408, Final Batch Loss: 0.3422222137451172\n",
      "Epoch 2352, Loss: 1.7635693550109863, Final Batch Loss: 0.3451381027698517\n",
      "Epoch 2353, Loss: 1.7649544775485992, Final Batch Loss: 0.30665311217308044\n",
      "Epoch 2354, Loss: 1.9191133975982666, Final Batch Loss: 0.5103601217269897\n",
      "Epoch 2355, Loss: 2.0287359952926636, Final Batch Loss: 0.405910462141037\n",
      "Epoch 2356, Loss: 1.8876993060112, Final Batch Loss: 0.3554510474205017\n",
      "Epoch 2357, Loss: 1.91412353515625, Final Batch Loss: 0.34367161989212036\n",
      "Epoch 2358, Loss: 1.9546184539794922, Final Batch Loss: 0.45980003476142883\n",
      "Epoch 2359, Loss: 1.718640387058258, Final Batch Loss: 0.261020302772522\n",
      "Epoch 2360, Loss: 1.8857043981552124, Final Batch Loss: 0.4184768795967102\n",
      "Epoch 2361, Loss: 1.8673799633979797, Final Batch Loss: 0.30210456252098083\n",
      "Epoch 2362, Loss: 1.8610844314098358, Final Batch Loss: 0.42450934648513794\n",
      "Epoch 2363, Loss: 1.7535022497177124, Final Batch Loss: 0.31628939509391785\n",
      "Epoch 2364, Loss: 1.9409364759922028, Final Batch Loss: 0.32223090529441833\n",
      "Epoch 2365, Loss: 1.9434096217155457, Final Batch Loss: 0.4327455461025238\n",
      "Epoch 2366, Loss: 1.8219601511955261, Final Batch Loss: 0.3024459183216095\n",
      "Epoch 2367, Loss: 1.8253990709781647, Final Batch Loss: 0.3592187762260437\n",
      "Epoch 2368, Loss: 1.795352280139923, Final Batch Loss: 0.34881508350372314\n",
      "Epoch 2369, Loss: 1.8876899480819702, Final Batch Loss: 0.40562742948532104\n",
      "Epoch 2370, Loss: 1.9952062666416168, Final Batch Loss: 0.3167346119880676\n",
      "Epoch 2371, Loss: 1.8918341100215912, Final Batch Loss: 0.5261732339859009\n",
      "Epoch 2372, Loss: 1.794697344303131, Final Batch Loss: 0.4010676145553589\n",
      "Epoch 2373, Loss: 1.790032297372818, Final Batch Loss: 0.3095818758010864\n",
      "Epoch 2374, Loss: 1.865961492061615, Final Batch Loss: 0.3665392994880676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2375, Loss: 1.8127972483634949, Final Batch Loss: 0.28575581312179565\n",
      "Epoch 2376, Loss: 1.7708887308835983, Final Batch Loss: 0.22958610951900482\n",
      "Epoch 2377, Loss: 1.9563701748847961, Final Batch Loss: 0.44772976636886597\n",
      "Epoch 2378, Loss: 1.797810584306717, Final Batch Loss: 0.2965860664844513\n",
      "Epoch 2379, Loss: 1.908117651939392, Final Batch Loss: 0.44547367095947266\n",
      "Epoch 2380, Loss: 1.7046065926551819, Final Batch Loss: 0.2764587998390198\n",
      "Epoch 2381, Loss: 1.8813126385211945, Final Batch Loss: 0.41992929577827454\n",
      "Epoch 2382, Loss: 1.8860008418560028, Final Batch Loss: 0.2665427625179291\n",
      "Epoch 2383, Loss: 1.8742684721946716, Final Batch Loss: 0.3386073708534241\n",
      "Epoch 2384, Loss: 1.8677528500556946, Final Batch Loss: 0.37227994203567505\n",
      "Epoch 2385, Loss: 1.9190973341464996, Final Batch Loss: 0.35607779026031494\n",
      "Epoch 2386, Loss: 1.8368453085422516, Final Batch Loss: 0.3836916983127594\n",
      "Epoch 2387, Loss: 1.9097630679607391, Final Batch Loss: 0.3983616232872009\n",
      "Epoch 2388, Loss: 1.7854702472686768, Final Batch Loss: 0.2725883424282074\n",
      "Epoch 2389, Loss: 1.8674677610397339, Final Batch Loss: 0.4610465168952942\n",
      "Epoch 2390, Loss: 1.7929238080978394, Final Batch Loss: 0.4022347331047058\n",
      "Epoch 2391, Loss: 1.8725379705429077, Final Batch Loss: 0.3763126730918884\n",
      "Epoch 2392, Loss: 1.718287318944931, Final Batch Loss: 0.3666004538536072\n",
      "Epoch 2393, Loss: 1.8542246222496033, Final Batch Loss: 0.415797621011734\n",
      "Epoch 2394, Loss: 1.8214461505413055, Final Batch Loss: 0.36262062191963196\n",
      "Epoch 2395, Loss: 1.8457353115081787, Final Batch Loss: 0.46014824509620667\n",
      "Epoch 2396, Loss: 1.7215741574764252, Final Batch Loss: 0.34822824597358704\n",
      "Epoch 2397, Loss: 1.692500740289688, Final Batch Loss: 0.2902181148529053\n",
      "Epoch 2398, Loss: 1.832362949848175, Final Batch Loss: 0.2899587154388428\n",
      "Epoch 2399, Loss: 1.872105211019516, Final Batch Loss: 0.44865933060646057\n",
      "Epoch 2400, Loss: 1.9199193716049194, Final Batch Loss: 0.38686561584472656\n",
      "Epoch 2401, Loss: 1.8723599016666412, Final Batch Loss: 0.39171865582466125\n",
      "Epoch 2402, Loss: 1.8929260969161987, Final Batch Loss: 0.3475607633590698\n",
      "Epoch 2403, Loss: 1.8985616564750671, Final Batch Loss: 0.40276283025741577\n",
      "Epoch 2404, Loss: 1.8776960670948029, Final Batch Loss: 0.2985878884792328\n",
      "Epoch 2405, Loss: 1.956067055463791, Final Batch Loss: 0.5448756217956543\n",
      "Epoch 2406, Loss: 1.9701372385025024, Final Batch Loss: 0.40890005230903625\n",
      "Epoch 2407, Loss: 1.7492574453353882, Final Batch Loss: 0.4497239291667938\n",
      "Epoch 2408, Loss: 1.7832939326763153, Final Batch Loss: 0.24637126922607422\n",
      "Epoch 2409, Loss: 1.9339082837104797, Final Batch Loss: 0.30960866808891296\n",
      "Epoch 2410, Loss: 1.7564830482006073, Final Batch Loss: 0.3040449619293213\n",
      "Epoch 2411, Loss: 1.962995857000351, Final Batch Loss: 0.4337744414806366\n",
      "Epoch 2412, Loss: 1.8828027844429016, Final Batch Loss: 0.3837535083293915\n",
      "Epoch 2413, Loss: 1.7602152526378632, Final Batch Loss: 0.4079359769821167\n",
      "Epoch 2414, Loss: 1.7779163718223572, Final Batch Loss: 0.3458894193172455\n",
      "Epoch 2415, Loss: 1.806425929069519, Final Batch Loss: 0.33545419573783875\n",
      "Epoch 2416, Loss: 1.9319793581962585, Final Batch Loss: 0.33630049228668213\n",
      "Epoch 2417, Loss: 1.9645027816295624, Final Batch Loss: 0.4543195068836212\n",
      "Epoch 2418, Loss: 2.060670256614685, Final Batch Loss: 0.47954681515693665\n",
      "Epoch 2419, Loss: 1.8570466339588165, Final Batch Loss: 0.3695680797100067\n",
      "Epoch 2420, Loss: 1.8966340124607086, Final Batch Loss: 0.3488316833972931\n",
      "Epoch 2421, Loss: 1.9184320271015167, Final Batch Loss: 0.47429195046424866\n",
      "Epoch 2422, Loss: 1.867452710866928, Final Batch Loss: 0.40265193581581116\n",
      "Epoch 2423, Loss: 1.7968232929706573, Final Batch Loss: 0.2694832384586334\n",
      "Epoch 2424, Loss: 2.1198763847351074, Final Batch Loss: 0.604764997959137\n",
      "Epoch 2425, Loss: 1.8278384506702423, Final Batch Loss: 0.35384833812713623\n",
      "Epoch 2426, Loss: 1.9842369854450226, Final Batch Loss: 0.43863460421562195\n",
      "Epoch 2427, Loss: 1.8433524370193481, Final Batch Loss: 0.43698757886886597\n",
      "Epoch 2428, Loss: 1.888613522052765, Final Batch Loss: 0.4232495427131653\n",
      "Epoch 2429, Loss: 1.9227314591407776, Final Batch Loss: 0.381786584854126\n",
      "Epoch 2430, Loss: 1.932043880224228, Final Batch Loss: 0.517515242099762\n",
      "Epoch 2431, Loss: 1.8494720458984375, Final Batch Loss: 0.43338778614997864\n",
      "Epoch 2432, Loss: 1.9270340204238892, Final Batch Loss: 0.34189581871032715\n",
      "Epoch 2433, Loss: 1.8169177770614624, Final Batch Loss: 0.3272280991077423\n",
      "Epoch 2434, Loss: 1.8568553924560547, Final Batch Loss: 0.32192638516426086\n",
      "Epoch 2435, Loss: 1.8348608314990997, Final Batch Loss: 0.3197614848613739\n",
      "Epoch 2436, Loss: 1.9442173540592194, Final Batch Loss: 0.4144238829612732\n",
      "Epoch 2437, Loss: 1.8087951838970184, Final Batch Loss: 0.2665015161037445\n",
      "Epoch 2438, Loss: 1.872385025024414, Final Batch Loss: 0.3856460452079773\n",
      "Epoch 2439, Loss: 1.8718183934688568, Final Batch Loss: 0.41082966327667236\n",
      "Epoch 2440, Loss: 1.86414834856987, Final Batch Loss: 0.4033525288105011\n",
      "Epoch 2441, Loss: 1.8200970888137817, Final Batch Loss: 0.3091256022453308\n",
      "Epoch 2442, Loss: 1.9233370423316956, Final Batch Loss: 0.38842830061912537\n",
      "Epoch 2443, Loss: 1.9750987589359283, Final Batch Loss: 0.4943707287311554\n",
      "Epoch 2444, Loss: 1.8078347146511078, Final Batch Loss: 0.309315025806427\n",
      "Epoch 2445, Loss: 1.8331709802150726, Final Batch Loss: 0.32836079597473145\n",
      "Epoch 2446, Loss: 1.82135808467865, Final Batch Loss: 0.35888856649398804\n",
      "Epoch 2447, Loss: 1.9583508968353271, Final Batch Loss: 0.3820876479148865\n",
      "Epoch 2448, Loss: 1.753879338502884, Final Batch Loss: 0.35773634910583496\n",
      "Epoch 2449, Loss: 1.761415183544159, Final Batch Loss: 0.30848318338394165\n",
      "Epoch 2450, Loss: 1.809430718421936, Final Batch Loss: 0.30614474415779114\n",
      "Epoch 2451, Loss: 1.7172480523586273, Final Batch Loss: 0.27665454149246216\n",
      "Epoch 2452, Loss: 1.8771550357341766, Final Batch Loss: 0.48753824830055237\n",
      "Epoch 2453, Loss: 1.8151705861091614, Final Batch Loss: 0.3548004925251007\n",
      "Epoch 2454, Loss: 1.7515498995780945, Final Batch Loss: 0.3676927387714386\n",
      "Epoch 2455, Loss: 1.7640181183815002, Final Batch Loss: 0.3763958215713501\n",
      "Epoch 2456, Loss: 1.84364253282547, Final Batch Loss: 0.3127027451992035\n",
      "Epoch 2457, Loss: 1.7951349020004272, Final Batch Loss: 0.347875714302063\n",
      "Epoch 2458, Loss: 1.733322948217392, Final Batch Loss: 0.3904403746128082\n",
      "Epoch 2459, Loss: 1.7363449037075043, Final Batch Loss: 0.34231802821159363\n",
      "Epoch 2460, Loss: 1.7529143989086151, Final Batch Loss: 0.2995357811450958\n",
      "Epoch 2461, Loss: 1.9556750357151031, Final Batch Loss: 0.3106066584587097\n",
      "Epoch 2462, Loss: 1.8222638368606567, Final Batch Loss: 0.3665051758289337\n",
      "Epoch 2463, Loss: 1.5900112390518188, Final Batch Loss: 0.27759692072868347\n",
      "Epoch 2464, Loss: 1.9477653503417969, Final Batch Loss: 0.4082604944705963\n",
      "Epoch 2465, Loss: 1.8697812855243683, Final Batch Loss: 0.411489874124527\n",
      "Epoch 2466, Loss: 1.8106975555419922, Final Batch Loss: 0.30698293447494507\n",
      "Epoch 2467, Loss: 1.89032444357872, Final Batch Loss: 0.3732575476169586\n",
      "Epoch 2468, Loss: 1.8131145238876343, Final Batch Loss: 0.33944958448410034\n",
      "Epoch 2469, Loss: 1.8371795117855072, Final Batch Loss: 0.34408849477767944\n",
      "Epoch 2470, Loss: 1.8266958892345428, Final Batch Loss: 0.40873098373413086\n",
      "Epoch 2471, Loss: 1.889500468969345, Final Batch Loss: 0.3266448378562927\n",
      "Epoch 2472, Loss: 1.7411249577999115, Final Batch Loss: 0.3052525222301483\n",
      "Epoch 2473, Loss: 1.6600952744483948, Final Batch Loss: 0.30882683396339417\n",
      "Epoch 2474, Loss: 1.7661718726158142, Final Batch Loss: 0.30883148312568665\n",
      "Epoch 2475, Loss: 1.8420123159885406, Final Batch Loss: 0.36755257844924927\n",
      "Epoch 2476, Loss: 1.7476874589920044, Final Batch Loss: 0.338522344827652\n",
      "Epoch 2477, Loss: 1.8561238944530487, Final Batch Loss: 0.38015592098236084\n",
      "Epoch 2478, Loss: 1.7804622948169708, Final Batch Loss: 0.3349216878414154\n",
      "Epoch 2479, Loss: 1.8205287456512451, Final Batch Loss: 0.40596771240234375\n",
      "Epoch 2480, Loss: 1.7891712486743927, Final Batch Loss: 0.27751824259757996\n",
      "Epoch 2481, Loss: 1.8142439126968384, Final Batch Loss: 0.33208760619163513\n",
      "Epoch 2482, Loss: 1.9607137739658356, Final Batch Loss: 0.5003711581230164\n",
      "Epoch 2483, Loss: 2.016877830028534, Final Batch Loss: 0.4146101474761963\n",
      "Epoch 2484, Loss: 2.0269646644592285, Final Batch Loss: 0.32979685068130493\n",
      "Epoch 2485, Loss: 1.888437181711197, Final Batch Loss: 0.4380876123905182\n",
      "Epoch 2486, Loss: 1.7899868786334991, Final Batch Loss: 0.3381654918193817\n",
      "Epoch 2487, Loss: 1.845251590013504, Final Batch Loss: 0.30033162236213684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2488, Loss: 1.7715016901493073, Final Batch Loss: 0.29548996686935425\n",
      "Epoch 2489, Loss: 1.8681403696537018, Final Batch Loss: 0.3834422528743744\n",
      "Epoch 2490, Loss: 1.7633508443832397, Final Batch Loss: 0.35849079489707947\n",
      "Epoch 2491, Loss: 1.9663049578666687, Final Batch Loss: 0.47816213965415955\n",
      "Epoch 2492, Loss: 1.9776036143302917, Final Batch Loss: 0.4792335331439972\n",
      "Epoch 2493, Loss: 1.777581512928009, Final Batch Loss: 0.4075283408164978\n",
      "Epoch 2494, Loss: 1.8424280285835266, Final Batch Loss: 0.28954190015792847\n",
      "Epoch 2495, Loss: 1.933128535747528, Final Batch Loss: 0.4190663695335388\n",
      "Epoch 2496, Loss: 1.739032730460167, Final Batch Loss: 0.3551604747772217\n",
      "Epoch 2497, Loss: 1.6766614019870758, Final Batch Loss: 0.25464317202568054\n",
      "Epoch 2498, Loss: 1.9368586540222168, Final Batch Loss: 0.4699666500091553\n",
      "Epoch 2499, Loss: 1.62936070561409, Final Batch Loss: 0.29904478788375854\n",
      "Epoch 2500, Loss: 1.7843818068504333, Final Batch Loss: 0.3918115198612213\n",
      "Epoch 2501, Loss: 1.832381933927536, Final Batch Loss: 0.4283607602119446\n",
      "Epoch 2502, Loss: 1.8697461187839508, Final Batch Loss: 0.4619058072566986\n",
      "Epoch 2503, Loss: 1.896618366241455, Final Batch Loss: 0.39653751254081726\n",
      "Epoch 2504, Loss: 1.8666844069957733, Final Batch Loss: 0.3141716420650482\n",
      "Epoch 2505, Loss: 1.7758337557315826, Final Batch Loss: 0.3508230447769165\n",
      "Epoch 2506, Loss: 1.789771854877472, Final Batch Loss: 0.3936084806919098\n",
      "Epoch 2507, Loss: 1.802506297826767, Final Batch Loss: 0.3809143900871277\n",
      "Epoch 2508, Loss: 1.7248587608337402, Final Batch Loss: 0.3415330946445465\n",
      "Epoch 2509, Loss: 1.8748958110809326, Final Batch Loss: 0.4504470229148865\n",
      "Epoch 2510, Loss: 1.9093143939971924, Final Batch Loss: 0.513264536857605\n",
      "Epoch 2511, Loss: 1.8133285641670227, Final Batch Loss: 0.27578210830688477\n",
      "Epoch 2512, Loss: 1.80270716547966, Final Batch Loss: 0.34988701343536377\n",
      "Epoch 2513, Loss: 1.7342505156993866, Final Batch Loss: 0.3520765006542206\n",
      "Epoch 2514, Loss: 1.829176664352417, Final Batch Loss: 0.3528102934360504\n",
      "Epoch 2515, Loss: 1.9646241664886475, Final Batch Loss: 0.3581784963607788\n",
      "Epoch 2516, Loss: 1.8817728161811829, Final Batch Loss: 0.4096168279647827\n",
      "Epoch 2517, Loss: 1.7721780836582184, Final Batch Loss: 0.3552820384502411\n",
      "Epoch 2518, Loss: 1.8367176353931427, Final Batch Loss: 0.27219000458717346\n",
      "Epoch 2519, Loss: 1.8273987174034119, Final Batch Loss: 0.37874749302864075\n",
      "Epoch 2520, Loss: 1.9566771686077118, Final Batch Loss: 0.42733034491539\n",
      "Epoch 2521, Loss: 1.7626441717147827, Final Batch Loss: 0.3454414904117584\n",
      "Epoch 2522, Loss: 1.9376672804355621, Final Batch Loss: 0.35661962628364563\n",
      "Epoch 2523, Loss: 1.8720391690731049, Final Batch Loss: 0.39812880754470825\n",
      "Epoch 2524, Loss: 1.855233758687973, Final Batch Loss: 0.2969011068344116\n",
      "Epoch 2525, Loss: 1.9004338383674622, Final Batch Loss: 0.4013735353946686\n",
      "Epoch 2526, Loss: 1.7486690282821655, Final Batch Loss: 0.382915198802948\n",
      "Epoch 2527, Loss: 1.7192723155021667, Final Batch Loss: 0.3425871431827545\n",
      "Epoch 2528, Loss: 1.6912382692098618, Final Batch Loss: 0.39403074979782104\n",
      "Epoch 2529, Loss: 1.78033447265625, Final Batch Loss: 0.31497177481651306\n",
      "Epoch 2530, Loss: 1.8287073075771332, Final Batch Loss: 0.32227838039398193\n",
      "Epoch 2531, Loss: 1.7727482318878174, Final Batch Loss: 0.38799822330474854\n",
      "Epoch 2532, Loss: 1.7763017117977142, Final Batch Loss: 0.39615505933761597\n",
      "Epoch 2533, Loss: 1.820287972688675, Final Batch Loss: 0.3602239787578583\n",
      "Epoch 2534, Loss: 1.9761343598365784, Final Batch Loss: 0.4485192596912384\n",
      "Epoch 2535, Loss: 1.793988674879074, Final Batch Loss: 0.41861286759376526\n",
      "Epoch 2536, Loss: 1.8542718291282654, Final Batch Loss: 0.3756360411643982\n",
      "Epoch 2537, Loss: 1.7729125320911407, Final Batch Loss: 0.37054163217544556\n",
      "Epoch 2538, Loss: 1.7512810230255127, Final Batch Loss: 0.36752286553382874\n",
      "Epoch 2539, Loss: 1.7846229672431946, Final Batch Loss: 0.4413789212703705\n",
      "Epoch 2540, Loss: 1.867582231760025, Final Batch Loss: 0.2686466574668884\n",
      "Epoch 2541, Loss: 1.766767978668213, Final Batch Loss: 0.41026291251182556\n",
      "Epoch 2542, Loss: 1.616166740655899, Final Batch Loss: 0.26112136244773865\n",
      "Epoch 2543, Loss: 1.7338810861110687, Final Batch Loss: 0.3418566882610321\n",
      "Epoch 2544, Loss: 1.6473619490861893, Final Batch Loss: 0.23217158019542694\n",
      "Epoch 2545, Loss: 1.7053305208683014, Final Batch Loss: 0.28134143352508545\n",
      "Epoch 2546, Loss: 1.681477576494217, Final Batch Loss: 0.32972773909568787\n",
      "Epoch 2547, Loss: 1.7243072986602783, Final Batch Loss: 0.2547215223312378\n",
      "Epoch 2548, Loss: 1.7418015003204346, Final Batch Loss: 0.3471246361732483\n",
      "Epoch 2549, Loss: 1.789011150598526, Final Batch Loss: 0.3286396265029907\n",
      "Epoch 2550, Loss: 1.7860682010650635, Final Batch Loss: 0.38422757387161255\n",
      "Epoch 2551, Loss: 1.960611641407013, Final Batch Loss: 0.4262845516204834\n",
      "Epoch 2552, Loss: 1.7400741279125214, Final Batch Loss: 0.30139774084091187\n",
      "Epoch 2553, Loss: 1.8115781843662262, Final Batch Loss: 0.31761863827705383\n",
      "Epoch 2554, Loss: 1.7304975390434265, Final Batch Loss: 0.3581925332546234\n",
      "Epoch 2555, Loss: 1.7747398614883423, Final Batch Loss: 0.3686477541923523\n",
      "Epoch 2556, Loss: 1.7729172110557556, Final Batch Loss: 0.3911571204662323\n",
      "Epoch 2557, Loss: 1.7461768984794617, Final Batch Loss: 0.34516361355781555\n",
      "Epoch 2558, Loss: 1.7807967960834503, Final Batch Loss: 0.3530904948711395\n",
      "Epoch 2559, Loss: 1.7843005657196045, Final Batch Loss: 0.4037525951862335\n",
      "Epoch 2560, Loss: 1.9460598826408386, Final Batch Loss: 0.46999749541282654\n",
      "Epoch 2561, Loss: 1.9332398772239685, Final Batch Loss: 0.42415741086006165\n",
      "Epoch 2562, Loss: 1.7651886641979218, Final Batch Loss: 0.3201305568218231\n",
      "Epoch 2563, Loss: 1.8970177173614502, Final Batch Loss: 0.3175818920135498\n",
      "Epoch 2564, Loss: 1.86154243350029, Final Batch Loss: 0.41253674030303955\n",
      "Epoch 2565, Loss: 1.911557674407959, Final Batch Loss: 0.38984036445617676\n",
      "Epoch 2566, Loss: 1.9454480111598969, Final Batch Loss: 0.3989502787590027\n",
      "Epoch 2567, Loss: 1.6791551113128662, Final Batch Loss: 0.3499889373779297\n",
      "Epoch 2568, Loss: 1.789658933877945, Final Batch Loss: 0.33537793159484863\n",
      "Epoch 2569, Loss: 1.8189750015735626, Final Batch Loss: 0.3425406217575073\n",
      "Epoch 2570, Loss: 1.8323734104633331, Final Batch Loss: 0.331439733505249\n",
      "Epoch 2571, Loss: 1.6577942669391632, Final Batch Loss: 0.2615921199321747\n",
      "Epoch 2572, Loss: 1.7930611371994019, Final Batch Loss: 0.4135361909866333\n",
      "Epoch 2573, Loss: 1.7412613928318024, Final Batch Loss: 0.3843643069267273\n",
      "Epoch 2574, Loss: 1.7662808001041412, Final Batch Loss: 0.42811551690101624\n",
      "Epoch 2575, Loss: 1.8022519946098328, Final Batch Loss: 0.35461917519569397\n",
      "Epoch 2576, Loss: 1.7567084729671478, Final Batch Loss: 0.3256927728652954\n",
      "Epoch 2577, Loss: 1.6912980377674103, Final Batch Loss: 0.3366771936416626\n",
      "Epoch 2578, Loss: 1.7909899652004242, Final Batch Loss: 0.3231668174266815\n",
      "Epoch 2579, Loss: 1.7585893273353577, Final Batch Loss: 0.2695944309234619\n",
      "Epoch 2580, Loss: 1.8938753008842468, Final Batch Loss: 0.4520006775856018\n",
      "Epoch 2581, Loss: 1.6890097856521606, Final Batch Loss: 0.31861186027526855\n",
      "Epoch 2582, Loss: 1.7724312841892242, Final Batch Loss: 0.32631391286849976\n",
      "Epoch 2583, Loss: 1.939263015985489, Final Batch Loss: 0.43289458751678467\n",
      "Epoch 2584, Loss: 1.733542948961258, Final Batch Loss: 0.37412890791893005\n",
      "Epoch 2585, Loss: 1.8753000497817993, Final Batch Loss: 0.4512923061847687\n",
      "Epoch 2586, Loss: 1.7232048511505127, Final Batch Loss: 0.38412195444107056\n",
      "Epoch 2587, Loss: 1.780910849571228, Final Batch Loss: 0.3453822731971741\n",
      "Epoch 2588, Loss: 1.823795199394226, Final Batch Loss: 0.3683694899082184\n",
      "Epoch 2589, Loss: 1.8083856999874115, Final Batch Loss: 0.32785680890083313\n",
      "Epoch 2590, Loss: 1.724929392337799, Final Batch Loss: 0.38893866539001465\n",
      "Epoch 2591, Loss: 1.7711778581142426, Final Batch Loss: 0.39746707677841187\n",
      "Epoch 2592, Loss: 1.7798887193202972, Final Batch Loss: 0.3745785057544708\n",
      "Epoch 2593, Loss: 1.8825443387031555, Final Batch Loss: 0.3628077507019043\n",
      "Epoch 2594, Loss: 1.686976045370102, Final Batch Loss: 0.4166816174983978\n",
      "Epoch 2595, Loss: 1.8386865258216858, Final Batch Loss: 0.357720285654068\n",
      "Epoch 2596, Loss: 1.9855014085769653, Final Batch Loss: 0.47994905710220337\n",
      "Epoch 2597, Loss: 1.8239188194274902, Final Batch Loss: 0.36856868863105774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2598, Loss: 1.7901197671890259, Final Batch Loss: 0.30670401453971863\n",
      "Epoch 2599, Loss: 1.815997451543808, Final Batch Loss: 0.3243445158004761\n",
      "Epoch 2600, Loss: 1.6977551877498627, Final Batch Loss: 0.32118916511535645\n",
      "Epoch 2601, Loss: 1.9448088705539703, Final Batch Loss: 0.3975363075733185\n",
      "Epoch 2602, Loss: 1.8569195866584778, Final Batch Loss: 0.32204702496528625\n",
      "Epoch 2603, Loss: 1.8534241020679474, Final Batch Loss: 0.4267238974571228\n",
      "Epoch 2604, Loss: 1.8897367417812347, Final Batch Loss: 0.3266211152076721\n",
      "Epoch 2605, Loss: 1.7840163707733154, Final Batch Loss: 0.32281702756881714\n",
      "Epoch 2606, Loss: 1.7864506542682648, Final Batch Loss: 0.3802832365036011\n",
      "Epoch 2607, Loss: 1.7694933414459229, Final Batch Loss: 0.3135400414466858\n",
      "Epoch 2608, Loss: 1.7207216620445251, Final Batch Loss: 0.2973485589027405\n",
      "Epoch 2609, Loss: 1.6817757785320282, Final Batch Loss: 0.2600052058696747\n",
      "Epoch 2610, Loss: 1.8461055159568787, Final Batch Loss: 0.35905545949935913\n",
      "Epoch 2611, Loss: 1.8206726908683777, Final Batch Loss: 0.3949262797832489\n",
      "Epoch 2612, Loss: 1.799405962228775, Final Batch Loss: 0.3458903431892395\n",
      "Epoch 2613, Loss: 1.9407236874103546, Final Batch Loss: 0.44882145524024963\n",
      "Epoch 2614, Loss: 1.6552618443965912, Final Batch Loss: 0.27970609068870544\n",
      "Epoch 2615, Loss: 1.7835863530635834, Final Batch Loss: 0.32527685165405273\n",
      "Epoch 2616, Loss: 1.8411180078983307, Final Batch Loss: 0.38804489374160767\n",
      "Epoch 2617, Loss: 1.9571735262870789, Final Batch Loss: 0.35895049571990967\n",
      "Epoch 2618, Loss: 2.1029857993125916, Final Batch Loss: 0.4895381033420563\n",
      "Epoch 2619, Loss: 1.7116334736347198, Final Batch Loss: 0.2900514304637909\n",
      "Epoch 2620, Loss: 1.819599986076355, Final Batch Loss: 0.45782366394996643\n",
      "Epoch 2621, Loss: 1.7345612347126007, Final Batch Loss: 0.3542635142803192\n",
      "Epoch 2622, Loss: 1.809940755367279, Final Batch Loss: 0.3168438673019409\n",
      "Epoch 2623, Loss: 1.8587426543235779, Final Batch Loss: 0.34281274676322937\n",
      "Epoch 2624, Loss: 1.7590698897838593, Final Batch Loss: 0.4278603792190552\n",
      "Epoch 2625, Loss: 1.8198561668395996, Final Batch Loss: 0.32166075706481934\n",
      "Epoch 2626, Loss: 1.8529345393180847, Final Batch Loss: 0.3758736252784729\n",
      "Epoch 2627, Loss: 1.8671387732028961, Final Batch Loss: 0.343414843082428\n",
      "Epoch 2628, Loss: 1.808029294013977, Final Batch Loss: 0.30677250027656555\n",
      "Epoch 2629, Loss: 1.9229553043842316, Final Batch Loss: 0.5019240975379944\n",
      "Epoch 2630, Loss: 1.7770731747150421, Final Batch Loss: 0.29035934805870056\n",
      "Epoch 2631, Loss: 1.7611373662948608, Final Batch Loss: 0.3544912040233612\n",
      "Epoch 2632, Loss: 1.918627291917801, Final Batch Loss: 0.4074093997478485\n",
      "Epoch 2633, Loss: 1.8252500891685486, Final Batch Loss: 0.36024460196495056\n",
      "Epoch 2634, Loss: 1.7328514754772186, Final Batch Loss: 0.2520826756954193\n",
      "Epoch 2635, Loss: 1.7108568251132965, Final Batch Loss: 0.3259134888648987\n",
      "Epoch 2636, Loss: 1.8007814288139343, Final Batch Loss: 0.3116244673728943\n",
      "Epoch 2637, Loss: 1.8090512156486511, Final Batch Loss: 0.41510656476020813\n",
      "Epoch 2638, Loss: 1.8095669746398926, Final Batch Loss: 0.3421711325645447\n",
      "Epoch 2639, Loss: 1.6724663376808167, Final Batch Loss: 0.3438611328601837\n",
      "Epoch 2640, Loss: 1.7271260023117065, Final Batch Loss: 0.33153969049453735\n",
      "Epoch 2641, Loss: 1.7442717850208282, Final Batch Loss: 0.4107930064201355\n",
      "Epoch 2642, Loss: 1.6504409611225128, Final Batch Loss: 0.2718198001384735\n",
      "Epoch 2643, Loss: 1.6898656487464905, Final Batch Loss: 0.29420435428619385\n",
      "Epoch 2644, Loss: 1.8327138721942902, Final Batch Loss: 0.34450969099998474\n",
      "Epoch 2645, Loss: 1.8458823561668396, Final Batch Loss: 0.3451455533504486\n",
      "Epoch 2646, Loss: 1.6798083186149597, Final Batch Loss: 0.3930957615375519\n",
      "Epoch 2647, Loss: 1.7445470690727234, Final Batch Loss: 0.29256781935691833\n",
      "Epoch 2648, Loss: 1.6986514031887054, Final Batch Loss: 0.36897677183151245\n",
      "Epoch 2649, Loss: 1.8655121624469757, Final Batch Loss: 0.43061745166778564\n",
      "Epoch 2650, Loss: 1.697986662387848, Final Batch Loss: 0.3098580837249756\n",
      "Epoch 2651, Loss: 1.7231970131397247, Final Batch Loss: 0.30522117018699646\n",
      "Epoch 2652, Loss: 1.7652964293956757, Final Batch Loss: 0.34435829520225525\n",
      "Epoch 2653, Loss: 1.8147452473640442, Final Batch Loss: 0.256987065076828\n",
      "Epoch 2654, Loss: 1.9426333010196686, Final Batch Loss: 0.35224810242652893\n",
      "Epoch 2655, Loss: 1.887202799320221, Final Batch Loss: 0.44176140427589417\n",
      "Epoch 2656, Loss: 1.8711894750595093, Final Batch Loss: 0.38462889194488525\n",
      "Epoch 2657, Loss: 1.796934187412262, Final Batch Loss: 0.28093603253364563\n",
      "Epoch 2658, Loss: 1.6191280782222748, Final Batch Loss: 0.2590058445930481\n",
      "Epoch 2659, Loss: 1.8432561457157135, Final Batch Loss: 0.3728879392147064\n",
      "Epoch 2660, Loss: 1.8935216665267944, Final Batch Loss: 0.4255006015300751\n",
      "Epoch 2661, Loss: 1.6924943327903748, Final Batch Loss: 0.36369064450263977\n",
      "Epoch 2662, Loss: 1.8275501430034637, Final Batch Loss: 0.38658303022384644\n",
      "Epoch 2663, Loss: 1.6571392714977264, Final Batch Loss: 0.38179510831832886\n",
      "Epoch 2664, Loss: 1.887271225452423, Final Batch Loss: 0.4570307731628418\n",
      "Epoch 2665, Loss: 1.8113264739513397, Final Batch Loss: 0.36992281675338745\n",
      "Epoch 2666, Loss: 1.7817493379116058, Final Batch Loss: 0.42576026916503906\n",
      "Epoch 2667, Loss: 1.6760910749435425, Final Batch Loss: 0.3407374620437622\n",
      "Epoch 2668, Loss: 1.691590577363968, Final Batch Loss: 0.3349907696247101\n",
      "Epoch 2669, Loss: 1.5899438261985779, Final Batch Loss: 0.3199903666973114\n",
      "Epoch 2670, Loss: 1.9028227031230927, Final Batch Loss: 0.36232873797416687\n",
      "Epoch 2671, Loss: 1.6109416782855988, Final Batch Loss: 0.32227203249931335\n",
      "Epoch 2672, Loss: 1.707488626241684, Final Batch Loss: 0.31934523582458496\n",
      "Epoch 2673, Loss: 1.8527045547962189, Final Batch Loss: 0.44284382462501526\n",
      "Epoch 2674, Loss: 1.8059903681278229, Final Batch Loss: 0.43868759274482727\n",
      "Epoch 2675, Loss: 1.8210559785366058, Final Batch Loss: 0.409091055393219\n",
      "Epoch 2676, Loss: 1.7676208913326263, Final Batch Loss: 0.32333728671073914\n",
      "Epoch 2677, Loss: 1.638453632593155, Final Batch Loss: 0.27258288860321045\n",
      "Epoch 2678, Loss: 1.8219371140003204, Final Batch Loss: 0.35949334502220154\n",
      "Epoch 2679, Loss: 1.7997390627861023, Final Batch Loss: 0.41208595037460327\n",
      "Epoch 2680, Loss: 1.718360424041748, Final Batch Loss: 0.4017519950866699\n",
      "Epoch 2681, Loss: 1.7884794771671295, Final Batch Loss: 0.41395458579063416\n",
      "Epoch 2682, Loss: 1.6569432020187378, Final Batch Loss: 0.257020503282547\n",
      "Epoch 2683, Loss: 1.8835076093673706, Final Batch Loss: 0.3675946593284607\n",
      "Epoch 2684, Loss: 1.7154020965099335, Final Batch Loss: 0.3313940167427063\n",
      "Epoch 2685, Loss: 1.94003164768219, Final Batch Loss: 0.4543357193470001\n",
      "Epoch 2686, Loss: 1.7987712025642395, Final Batch Loss: 0.27930644154548645\n",
      "Epoch 2687, Loss: 1.7054412364959717, Final Batch Loss: 0.2812345623970032\n",
      "Epoch 2688, Loss: 1.7352528274059296, Final Batch Loss: 0.42429834604263306\n",
      "Epoch 2689, Loss: 1.6974683701992035, Final Batch Loss: 0.30742838978767395\n",
      "Epoch 2690, Loss: 1.9093015789985657, Final Batch Loss: 0.2893838584423065\n",
      "Epoch 2691, Loss: 1.723417967557907, Final Batch Loss: 0.361611545085907\n",
      "Epoch 2692, Loss: 2.025994122028351, Final Batch Loss: 0.347414493560791\n",
      "Epoch 2693, Loss: 1.6953679323196411, Final Batch Loss: 0.3310345709323883\n",
      "Epoch 2694, Loss: 1.7996209859848022, Final Batch Loss: 0.41350311040878296\n",
      "Epoch 2695, Loss: 1.7206700891256332, Final Batch Loss: 0.39316704869270325\n",
      "Epoch 2696, Loss: 1.663510799407959, Final Batch Loss: 0.35699227452278137\n",
      "Epoch 2697, Loss: 1.6484674215316772, Final Batch Loss: 0.39247027039527893\n",
      "Epoch 2698, Loss: 1.746099591255188, Final Batch Loss: 0.3657155930995941\n",
      "Epoch 2699, Loss: 1.7340327203273773, Final Batch Loss: 0.3465072810649872\n",
      "Epoch 2700, Loss: 1.8281151056289673, Final Batch Loss: 0.48043596744537354\n",
      "Epoch 2701, Loss: 1.7846313565969467, Final Batch Loss: 0.40983104705810547\n",
      "Epoch 2702, Loss: 1.7997856438159943, Final Batch Loss: 0.36957991123199463\n",
      "Epoch 2703, Loss: 1.6077270805835724, Final Batch Loss: 0.28721705079078674\n",
      "Epoch 2704, Loss: 1.8044638931751251, Final Batch Loss: 0.3690613806247711\n",
      "Epoch 2705, Loss: 1.8171992897987366, Final Batch Loss: 0.3306952714920044\n",
      "Epoch 2706, Loss: 1.7025518715381622, Final Batch Loss: 0.3471744656562805\n",
      "Epoch 2707, Loss: 1.872680276632309, Final Batch Loss: 0.3913206458091736\n",
      "Epoch 2708, Loss: 1.6931418776512146, Final Batch Loss: 0.35741010308265686\n",
      "Epoch 2709, Loss: 1.9549447894096375, Final Batch Loss: 0.38469552993774414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2710, Loss: 1.8241674304008484, Final Batch Loss: 0.33035898208618164\n",
      "Epoch 2711, Loss: 1.7397897243499756, Final Batch Loss: 0.29416748881340027\n",
      "Epoch 2712, Loss: 1.8236807584762573, Final Batch Loss: 0.396321177482605\n",
      "Epoch 2713, Loss: 1.7951379716396332, Final Batch Loss: 0.32617130875587463\n",
      "Epoch 2714, Loss: 1.7947660386562347, Final Batch Loss: 0.3445581793785095\n",
      "Epoch 2715, Loss: 1.6919542253017426, Final Batch Loss: 0.26561838388442993\n",
      "Epoch 2716, Loss: 1.8925946801900864, Final Batch Loss: 0.40644723176956177\n",
      "Epoch 2717, Loss: 1.8249151706695557, Final Batch Loss: 0.42087051272392273\n",
      "Epoch 2718, Loss: 1.8115784376859665, Final Batch Loss: 0.4824129343032837\n",
      "Epoch 2719, Loss: 1.6558360755443573, Final Batch Loss: 0.2827046513557434\n",
      "Epoch 2720, Loss: 1.790330857038498, Final Batch Loss: 0.396592915058136\n",
      "Epoch 2721, Loss: 1.8952772915363312, Final Batch Loss: 0.4732985496520996\n",
      "Epoch 2722, Loss: 1.7535933554172516, Final Batch Loss: 0.3963957726955414\n",
      "Epoch 2723, Loss: 1.6139276325702667, Final Batch Loss: 0.23130258917808533\n",
      "Epoch 2724, Loss: 1.6474136710166931, Final Batch Loss: 0.2700715661048889\n",
      "Epoch 2725, Loss: 1.840036392211914, Final Batch Loss: 0.36619696021080017\n",
      "Epoch 2726, Loss: 1.801942765712738, Final Batch Loss: 0.29112333059310913\n",
      "Epoch 2727, Loss: 1.721020370721817, Final Batch Loss: 0.34353914856910706\n",
      "Epoch 2728, Loss: 1.8584705293178558, Final Batch Loss: 0.40688222646713257\n",
      "Epoch 2729, Loss: 1.7408945560455322, Final Batch Loss: 0.34574094414711\n",
      "Epoch 2730, Loss: 1.730662614107132, Final Batch Loss: 0.2786485254764557\n",
      "Epoch 2731, Loss: 1.7308848202228546, Final Batch Loss: 0.29364755749702454\n",
      "Epoch 2732, Loss: 1.7507867217063904, Final Batch Loss: 0.38818326592445374\n",
      "Epoch 2733, Loss: 1.8611209094524384, Final Batch Loss: 0.44049733877182007\n",
      "Epoch 2734, Loss: 1.89164400100708, Final Batch Loss: 0.40097424387931824\n",
      "Epoch 2735, Loss: 1.7303617596626282, Final Batch Loss: 0.31408894062042236\n",
      "Epoch 2736, Loss: 1.7269981801509857, Final Batch Loss: 0.3498982787132263\n",
      "Epoch 2737, Loss: 1.7972886562347412, Final Batch Loss: 0.33189383149147034\n",
      "Epoch 2738, Loss: 1.781740814447403, Final Batch Loss: 0.3353690803050995\n",
      "Epoch 2739, Loss: 1.7227322459220886, Final Batch Loss: 0.32790735363960266\n",
      "Epoch 2740, Loss: 1.6232472658157349, Final Batch Loss: 0.2532549798488617\n",
      "Epoch 2741, Loss: 1.6594523787498474, Final Batch Loss: 0.37572529911994934\n",
      "Epoch 2742, Loss: 1.6301917433738708, Final Batch Loss: 0.29413631558418274\n",
      "Epoch 2743, Loss: 1.6855454444885254, Final Batch Loss: 0.37822988629341125\n",
      "Epoch 2744, Loss: 1.7189006507396698, Final Batch Loss: 0.3459002375602722\n",
      "Epoch 2745, Loss: 1.7317538857460022, Final Batch Loss: 0.3274988830089569\n",
      "Epoch 2746, Loss: 1.7308304011821747, Final Batch Loss: 0.4410895109176636\n",
      "Epoch 2747, Loss: 1.7760763764381409, Final Batch Loss: 0.3980192244052887\n",
      "Epoch 2748, Loss: 1.7554266154766083, Final Batch Loss: 0.2790219485759735\n",
      "Epoch 2749, Loss: 1.9090363383293152, Final Batch Loss: 0.4427807927131653\n",
      "Epoch 2750, Loss: 1.7812831699848175, Final Batch Loss: 0.43857115507125854\n",
      "Epoch 2751, Loss: 1.678588718175888, Final Batch Loss: 0.3653903901576996\n",
      "Epoch 2752, Loss: 1.743699461221695, Final Batch Loss: 0.2985219657421112\n",
      "Epoch 2753, Loss: 1.7976650595664978, Final Batch Loss: 0.3493421971797943\n",
      "Epoch 2754, Loss: 1.6198271214962006, Final Batch Loss: 0.295154333114624\n",
      "Epoch 2755, Loss: 1.7035782635211945, Final Batch Loss: 0.39033088088035583\n",
      "Epoch 2756, Loss: 1.6315025389194489, Final Batch Loss: 0.30249524116516113\n",
      "Epoch 2757, Loss: 1.7782462537288666, Final Batch Loss: 0.3124716281890869\n",
      "Epoch 2758, Loss: 1.6125690639019012, Final Batch Loss: 0.31246137619018555\n",
      "Epoch 2759, Loss: 1.6929154098033905, Final Batch Loss: 0.2554805278778076\n",
      "Epoch 2760, Loss: 1.6386102139949799, Final Batch Loss: 0.2818891108036041\n",
      "Epoch 2761, Loss: 1.6678049266338348, Final Batch Loss: 0.2743276357650757\n",
      "Epoch 2762, Loss: 1.6993378698825836, Final Batch Loss: 0.25477489829063416\n",
      "Epoch 2763, Loss: 1.8836870789527893, Final Batch Loss: 0.4404812157154083\n",
      "Epoch 2764, Loss: 1.6638060212135315, Final Batch Loss: 0.32199352979660034\n",
      "Epoch 2765, Loss: 1.7545412480831146, Final Batch Loss: 0.31877127289772034\n",
      "Epoch 2766, Loss: 1.683365523815155, Final Batch Loss: 0.34434741735458374\n",
      "Epoch 2767, Loss: 1.7395748794078827, Final Batch Loss: 0.3907739520072937\n",
      "Epoch 2768, Loss: 1.6291071474552155, Final Batch Loss: 0.363309383392334\n",
      "Epoch 2769, Loss: 1.777464598417282, Final Batch Loss: 0.40839576721191406\n",
      "Epoch 2770, Loss: 1.6111339926719666, Final Batch Loss: 0.2868582606315613\n",
      "Epoch 2771, Loss: 1.812712550163269, Final Batch Loss: 0.3242775797843933\n",
      "Epoch 2772, Loss: 1.7711729109287262, Final Batch Loss: 0.30611881613731384\n",
      "Epoch 2773, Loss: 1.7603538930416107, Final Batch Loss: 0.3362572193145752\n",
      "Epoch 2774, Loss: 1.6899286806583405, Final Batch Loss: 0.3065134286880493\n",
      "Epoch 2775, Loss: 1.641537606716156, Final Batch Loss: 0.36959758400917053\n",
      "Epoch 2776, Loss: 1.7013745307922363, Final Batch Loss: 0.3131009042263031\n",
      "Epoch 2777, Loss: 1.696442425251007, Final Batch Loss: 0.3759393095970154\n",
      "Epoch 2778, Loss: 1.7544825375080109, Final Batch Loss: 0.39082491397857666\n",
      "Epoch 2779, Loss: 1.6738402545452118, Final Batch Loss: 0.43969041109085083\n",
      "Epoch 2780, Loss: 1.7412631809711456, Final Batch Loss: 0.3096679747104645\n",
      "Epoch 2781, Loss: 1.821969360113144, Final Batch Loss: 0.36263227462768555\n",
      "Epoch 2782, Loss: 1.8866521418094635, Final Batch Loss: 0.4041910469532013\n",
      "Epoch 2783, Loss: 1.6204149425029755, Final Batch Loss: 0.2190321385860443\n",
      "Epoch 2784, Loss: 1.7733885645866394, Final Batch Loss: 0.26926228404045105\n",
      "Epoch 2785, Loss: 1.7786765098571777, Final Batch Loss: 0.33294087648391724\n",
      "Epoch 2786, Loss: 1.8199300169944763, Final Batch Loss: 0.3876148760318756\n",
      "Epoch 2787, Loss: 1.832301914691925, Final Batch Loss: 0.29556548595428467\n",
      "Epoch 2788, Loss: 1.6839908063411713, Final Batch Loss: 0.3821517825126648\n",
      "Epoch 2789, Loss: 1.9244962334632874, Final Batch Loss: 0.35478419065475464\n",
      "Epoch 2790, Loss: 1.8448262512683868, Final Batch Loss: 0.4389053285121918\n",
      "Epoch 2791, Loss: 1.7604381442070007, Final Batch Loss: 0.35887807607650757\n",
      "Epoch 2792, Loss: 1.6473309099674225, Final Batch Loss: 0.350410133600235\n",
      "Epoch 2793, Loss: 1.7674293220043182, Final Batch Loss: 0.3849625289440155\n",
      "Epoch 2794, Loss: 1.7962099015712738, Final Batch Loss: 0.3700041174888611\n",
      "Epoch 2795, Loss: 1.7376309931278229, Final Batch Loss: 0.34051063656806946\n",
      "Epoch 2796, Loss: 1.843422681093216, Final Batch Loss: 0.40308040380477905\n",
      "Epoch 2797, Loss: 1.6978903114795685, Final Batch Loss: 0.33020731806755066\n",
      "Epoch 2798, Loss: 1.7124398350715637, Final Batch Loss: 0.3565199077129364\n",
      "Epoch 2799, Loss: 1.6115734577178955, Final Batch Loss: 0.26379942893981934\n",
      "Epoch 2800, Loss: 1.6784050464630127, Final Batch Loss: 0.27626946568489075\n",
      "Epoch 2801, Loss: 1.6553544402122498, Final Batch Loss: 0.28506210446357727\n",
      "Epoch 2802, Loss: 1.7493786364793777, Final Batch Loss: 0.4482477009296417\n",
      "Epoch 2803, Loss: 1.8772016167640686, Final Batch Loss: 0.4169858992099762\n",
      "Epoch 2804, Loss: 1.7551308870315552, Final Batch Loss: 0.29100704193115234\n",
      "Epoch 2805, Loss: 1.7029201090335846, Final Batch Loss: 0.3693443536758423\n",
      "Epoch 2806, Loss: 1.7676428854465485, Final Batch Loss: 0.2795378267765045\n",
      "Epoch 2807, Loss: 1.7505030035972595, Final Batch Loss: 0.2743837833404541\n",
      "Epoch 2808, Loss: 1.7207036912441254, Final Batch Loss: 0.2835589051246643\n",
      "Epoch 2809, Loss: 1.8413532972335815, Final Batch Loss: 0.38661226630210876\n",
      "Epoch 2810, Loss: 1.7220096588134766, Final Batch Loss: 0.33713454008102417\n",
      "Epoch 2811, Loss: 1.8492753505706787, Final Batch Loss: 0.3549432158470154\n",
      "Epoch 2812, Loss: 1.8280797004699707, Final Batch Loss: 0.43801847100257874\n",
      "Epoch 2813, Loss: 1.8565895557403564, Final Batch Loss: 0.4255845248699188\n",
      "Epoch 2814, Loss: 1.7893954813480377, Final Batch Loss: 0.46737727522850037\n",
      "Epoch 2815, Loss: 1.7432463467121124, Final Batch Loss: 0.3085961937904358\n",
      "Epoch 2816, Loss: 1.7862845361232758, Final Batch Loss: 0.24609550833702087\n",
      "Epoch 2817, Loss: 1.6564741134643555, Final Batch Loss: 0.3214678466320038\n",
      "Epoch 2818, Loss: 1.6638562679290771, Final Batch Loss: 0.34593167901039124\n",
      "Epoch 2819, Loss: 1.778533697128296, Final Batch Loss: 0.3803945481777191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2820, Loss: 1.7327103316783905, Final Batch Loss: 0.35732272267341614\n",
      "Epoch 2821, Loss: 1.8095302879810333, Final Batch Loss: 0.4284704029560089\n",
      "Epoch 2822, Loss: 1.6896397769451141, Final Batch Loss: 0.31047648191452026\n",
      "Epoch 2823, Loss: 1.8453288078308105, Final Batch Loss: 0.4875187277793884\n",
      "Epoch 2824, Loss: 1.6306670904159546, Final Batch Loss: 0.25528639554977417\n",
      "Epoch 2825, Loss: 1.674001932144165, Final Batch Loss: 0.4452076256275177\n",
      "Epoch 2826, Loss: 1.6761691272258759, Final Batch Loss: 0.29135754704475403\n",
      "Epoch 2827, Loss: 1.6512554287910461, Final Batch Loss: 0.37350594997406006\n",
      "Epoch 2828, Loss: 1.7212271094322205, Final Batch Loss: 0.35352781414985657\n",
      "Epoch 2829, Loss: 1.7275197207927704, Final Batch Loss: 0.33693256974220276\n",
      "Epoch 2830, Loss: 1.704787701368332, Final Batch Loss: 0.42276623845100403\n",
      "Epoch 2831, Loss: 1.6542907059192657, Final Batch Loss: 0.27199503779411316\n",
      "Epoch 2832, Loss: 1.6938339471817017, Final Batch Loss: 0.2785731852054596\n",
      "Epoch 2833, Loss: 1.573812574148178, Final Batch Loss: 0.290447473526001\n",
      "Epoch 2834, Loss: 1.7397754192352295, Final Batch Loss: 0.2974700331687927\n",
      "Epoch 2835, Loss: 1.9415516555309296, Final Batch Loss: 0.38011425733566284\n",
      "Epoch 2836, Loss: 1.6990235149860382, Final Batch Loss: 0.27956724166870117\n",
      "Epoch 2837, Loss: 1.6347740292549133, Final Batch Loss: 0.26536664366722107\n",
      "Epoch 2838, Loss: 1.7206259965896606, Final Batch Loss: 0.3018365800380707\n",
      "Epoch 2839, Loss: 1.6156807839870453, Final Batch Loss: 0.2589310109615326\n",
      "Epoch 2840, Loss: 1.661206305027008, Final Batch Loss: 0.3226792812347412\n",
      "Epoch 2841, Loss: 1.8268746137619019, Final Batch Loss: 0.3253910541534424\n",
      "Epoch 2842, Loss: 1.6405832171440125, Final Batch Loss: 0.2820124328136444\n",
      "Epoch 2843, Loss: 1.9753609001636505, Final Batch Loss: 0.45478183031082153\n",
      "Epoch 2844, Loss: 1.5546257346868515, Final Batch Loss: 0.24787594377994537\n",
      "Epoch 2845, Loss: 1.647608608007431, Final Batch Loss: 0.3725212812423706\n",
      "Epoch 2846, Loss: 1.7044667601585388, Final Batch Loss: 0.2829989492893219\n",
      "Epoch 2847, Loss: 1.5787900388240814, Final Batch Loss: 0.27937063574790955\n",
      "Epoch 2848, Loss: 1.6534986197948456, Final Batch Loss: 0.340938001871109\n",
      "Epoch 2849, Loss: 1.6423743516206741, Final Batch Loss: 0.3411335051059723\n",
      "Epoch 2850, Loss: 1.5898147821426392, Final Batch Loss: 0.34321072697639465\n",
      "Epoch 2851, Loss: 1.8253101408481598, Final Batch Loss: 0.38097551465034485\n",
      "Epoch 2852, Loss: 1.7462878823280334, Final Batch Loss: 0.3982124924659729\n",
      "Epoch 2853, Loss: 1.7210171520709991, Final Batch Loss: 0.33289098739624023\n",
      "Epoch 2854, Loss: 1.7298280000686646, Final Batch Loss: 0.36173924803733826\n",
      "Epoch 2855, Loss: 1.6408693492412567, Final Batch Loss: 0.42647963762283325\n",
      "Epoch 2856, Loss: 1.7914082705974579, Final Batch Loss: 0.32988181710243225\n",
      "Epoch 2857, Loss: 1.670931100845337, Final Batch Loss: 0.3292767107486725\n",
      "Epoch 2858, Loss: 1.7258841693401337, Final Batch Loss: 0.2482854425907135\n",
      "Epoch 2859, Loss: 1.7671626210212708, Final Batch Loss: 0.29829928278923035\n",
      "Epoch 2860, Loss: 1.6953229308128357, Final Batch Loss: 0.28233110904693604\n",
      "Epoch 2861, Loss: 1.6117463260889053, Final Batch Loss: 0.22996218502521515\n",
      "Epoch 2862, Loss: 1.7943342924118042, Final Batch Loss: 0.2798212766647339\n",
      "Epoch 2863, Loss: 1.8320362269878387, Final Batch Loss: 0.37567824125289917\n",
      "Epoch 2864, Loss: 1.870089828968048, Final Batch Loss: 0.43508821725845337\n",
      "Epoch 2865, Loss: 1.6305871307849884, Final Batch Loss: 0.40897640585899353\n",
      "Epoch 2866, Loss: 1.6352660059928894, Final Batch Loss: 0.26662182807922363\n",
      "Epoch 2867, Loss: 1.7037641108036041, Final Batch Loss: 0.4075775146484375\n",
      "Epoch 2868, Loss: 1.7187797725200653, Final Batch Loss: 0.32972678542137146\n",
      "Epoch 2869, Loss: 1.653868317604065, Final Batch Loss: 0.3350500464439392\n",
      "Epoch 2870, Loss: 1.7380335628986359, Final Batch Loss: 0.4212387800216675\n",
      "Epoch 2871, Loss: 1.7031745314598083, Final Batch Loss: 0.40240421891212463\n",
      "Epoch 2872, Loss: 1.7453586161136627, Final Batch Loss: 0.4142190217971802\n",
      "Epoch 2873, Loss: 1.7998612225055695, Final Batch Loss: 0.4469018876552582\n",
      "Epoch 2874, Loss: 1.750077098608017, Final Batch Loss: 0.39806392788887024\n",
      "Epoch 2875, Loss: 1.6407392621040344, Final Batch Loss: 0.30485793948173523\n",
      "Epoch 2876, Loss: 1.8213720619678497, Final Batch Loss: 0.4063009023666382\n",
      "Epoch 2877, Loss: 1.7133349776268005, Final Batch Loss: 0.4024285078048706\n",
      "Epoch 2878, Loss: 1.7883577942848206, Final Batch Loss: 0.4044564366340637\n",
      "Epoch 2879, Loss: 1.7793305218219757, Final Batch Loss: 0.33195018768310547\n",
      "Epoch 2880, Loss: 1.6340282559394836, Final Batch Loss: 0.3580282926559448\n",
      "Epoch 2881, Loss: 1.7734997868537903, Final Batch Loss: 0.4924629330635071\n",
      "Epoch 2882, Loss: 1.6373021006584167, Final Batch Loss: 0.3264002203941345\n",
      "Epoch 2883, Loss: 1.6966435313224792, Final Batch Loss: 0.3828285038471222\n",
      "Epoch 2884, Loss: 1.7203795909881592, Final Batch Loss: 0.32312682271003723\n",
      "Epoch 2885, Loss: 1.8057907223701477, Final Batch Loss: 0.36723390221595764\n",
      "Epoch 2886, Loss: 1.687264621257782, Final Batch Loss: 0.41385361552238464\n",
      "Epoch 2887, Loss: 1.6534514725208282, Final Batch Loss: 0.3625124990940094\n",
      "Epoch 2888, Loss: 1.7485902607440948, Final Batch Loss: 0.2002185881137848\n",
      "Epoch 2889, Loss: 1.758535921573639, Final Batch Loss: 0.4101845622062683\n",
      "Epoch 2890, Loss: 1.74833145737648, Final Batch Loss: 0.36004626750946045\n",
      "Epoch 2891, Loss: 1.808011382818222, Final Batch Loss: 0.360073447227478\n",
      "Epoch 2892, Loss: 1.6723036468029022, Final Batch Loss: 0.3285028040409088\n",
      "Epoch 2893, Loss: 1.7508653402328491, Final Batch Loss: 0.3792557716369629\n",
      "Epoch 2894, Loss: 1.6874934434890747, Final Batch Loss: 0.354870080947876\n",
      "Epoch 2895, Loss: 1.7734581530094147, Final Batch Loss: 0.40454617142677307\n",
      "Epoch 2896, Loss: 1.7743213772773743, Final Batch Loss: 0.42486482858657837\n",
      "Epoch 2897, Loss: 1.8430436253547668, Final Batch Loss: 0.27827388048171997\n",
      "Epoch 2898, Loss: 1.6768809258937836, Final Batch Loss: 0.33166220784187317\n",
      "Epoch 2899, Loss: 1.744347557425499, Final Batch Loss: 0.32666945457458496\n",
      "Epoch 2900, Loss: 1.6600050926208496, Final Batch Loss: 0.32509109377861023\n",
      "Epoch 2901, Loss: 1.5939360857009888, Final Batch Loss: 0.32660752534866333\n",
      "Epoch 2902, Loss: 1.7094590961933136, Final Batch Loss: 0.3225478529930115\n",
      "Epoch 2903, Loss: 1.6202989220619202, Final Batch Loss: 0.22969821095466614\n",
      "Epoch 2904, Loss: 1.6576837003231049, Final Batch Loss: 0.2676079273223877\n",
      "Epoch 2905, Loss: 1.691173642873764, Final Batch Loss: 0.333162397146225\n",
      "Epoch 2906, Loss: 1.6122475415468216, Final Batch Loss: 0.3040570616722107\n",
      "Epoch 2907, Loss: 1.8724207878112793, Final Batch Loss: 0.4289984703063965\n",
      "Epoch 2908, Loss: 1.6881099343299866, Final Batch Loss: 0.33167681097984314\n",
      "Epoch 2909, Loss: 1.6586589515209198, Final Batch Loss: 0.288568377494812\n",
      "Epoch 2910, Loss: 1.7009503841400146, Final Batch Loss: 0.3181140422821045\n",
      "Epoch 2911, Loss: 1.7175448834896088, Final Batch Loss: 0.2963431477546692\n",
      "Epoch 2912, Loss: 1.6896006762981415, Final Batch Loss: 0.3111672103404999\n",
      "Epoch 2913, Loss: 1.7379423677921295, Final Batch Loss: 0.2739637792110443\n",
      "Epoch 2914, Loss: 1.7281264066696167, Final Batch Loss: 0.3250683546066284\n",
      "Epoch 2915, Loss: 1.7788970172405243, Final Batch Loss: 0.37350910902023315\n",
      "Epoch 2916, Loss: 1.7221600413322449, Final Batch Loss: 0.379345178604126\n",
      "Epoch 2917, Loss: 1.748849630355835, Final Batch Loss: 0.4324646294116974\n",
      "Epoch 2918, Loss: 1.68770033121109, Final Batch Loss: 0.37456008791923523\n",
      "Epoch 2919, Loss: 1.8250617980957031, Final Batch Loss: 0.5287359356880188\n",
      "Epoch 2920, Loss: 1.7022470086812973, Final Batch Loss: 0.24326802790164948\n",
      "Epoch 2921, Loss: 1.7877240180969238, Final Batch Loss: 0.401542603969574\n",
      "Epoch 2922, Loss: 1.6500786542892456, Final Batch Loss: 0.37252968549728394\n",
      "Epoch 2923, Loss: 1.743228167295456, Final Batch Loss: 0.39110973477363586\n",
      "Epoch 2924, Loss: 1.5929918587207794, Final Batch Loss: 0.31045737862586975\n",
      "Epoch 2925, Loss: 1.7240996211767197, Final Batch Loss: 0.24522458016872406\n",
      "Epoch 2926, Loss: 1.6869829595088959, Final Batch Loss: 0.2592241168022156\n",
      "Epoch 2927, Loss: 1.9120554625988007, Final Batch Loss: 0.5407240986824036\n",
      "Epoch 2928, Loss: 1.6528221368789673, Final Batch Loss: 0.28149688243865967\n",
      "Epoch 2929, Loss: 1.5875253975391388, Final Batch Loss: 0.36037391424179077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2930, Loss: 1.8483985364437103, Final Batch Loss: 0.36835843324661255\n",
      "Epoch 2931, Loss: 1.7301495969295502, Final Batch Loss: 0.384401798248291\n",
      "Epoch 2932, Loss: 1.6152470111846924, Final Batch Loss: 0.2808266282081604\n",
      "Epoch 2933, Loss: 1.7649873197078705, Final Batch Loss: 0.43700316548347473\n",
      "Epoch 2934, Loss: 1.6343117952346802, Final Batch Loss: 0.30939388275146484\n",
      "Epoch 2935, Loss: 1.563569650053978, Final Batch Loss: 0.37352001667022705\n",
      "Epoch 2936, Loss: 1.8370744585990906, Final Batch Loss: 0.43915700912475586\n",
      "Epoch 2937, Loss: 1.6677175462245941, Final Batch Loss: 0.3243107795715332\n",
      "Epoch 2938, Loss: 1.5823955535888672, Final Batch Loss: 0.2736434042453766\n",
      "Epoch 2939, Loss: 1.6740565598011017, Final Batch Loss: 0.39097148180007935\n",
      "Epoch 2940, Loss: 1.8501933217048645, Final Batch Loss: 0.41889408230781555\n",
      "Epoch 2941, Loss: 1.736085832118988, Final Batch Loss: 0.31257712841033936\n",
      "Epoch 2942, Loss: 1.6078068912029266, Final Batch Loss: 0.2650422751903534\n",
      "Epoch 2943, Loss: 1.8379474580287933, Final Batch Loss: 0.40309277176856995\n",
      "Epoch 2944, Loss: 1.709826111793518, Final Batch Loss: 0.3185982406139374\n",
      "Epoch 2945, Loss: 1.6930091679096222, Final Batch Loss: 0.31184715032577515\n",
      "Epoch 2946, Loss: 1.7095866799354553, Final Batch Loss: 0.38348203897476196\n",
      "Epoch 2947, Loss: 1.6321903765201569, Final Batch Loss: 0.32940390706062317\n",
      "Epoch 2948, Loss: 1.8090455830097198, Final Batch Loss: 0.36703750491142273\n",
      "Epoch 2949, Loss: 1.8500114381313324, Final Batch Loss: 0.5674206614494324\n",
      "Epoch 2950, Loss: 1.6831886172294617, Final Batch Loss: 0.29249975085258484\n",
      "Epoch 2951, Loss: 1.6711394488811493, Final Batch Loss: 0.3015862703323364\n",
      "Epoch 2952, Loss: 1.8441387116909027, Final Batch Loss: 0.4956746995449066\n",
      "Epoch 2953, Loss: 1.7851227819919586, Final Batch Loss: 0.4015098512172699\n",
      "Epoch 2954, Loss: 1.6195867359638214, Final Batch Loss: 0.32730451226234436\n",
      "Epoch 2955, Loss: 1.7273322343826294, Final Batch Loss: 0.2706838548183441\n",
      "Epoch 2956, Loss: 1.743064284324646, Final Batch Loss: 0.4492826759815216\n",
      "Epoch 2957, Loss: 1.6400011777877808, Final Batch Loss: 0.2722339928150177\n",
      "Epoch 2958, Loss: 1.6870043277740479, Final Batch Loss: 0.3492298126220703\n",
      "Epoch 2959, Loss: 1.73883056640625, Final Batch Loss: 0.3646806478500366\n",
      "Epoch 2960, Loss: 1.738478571176529, Final Batch Loss: 0.35121625661849976\n",
      "Epoch 2961, Loss: 1.73743936419487, Final Batch Loss: 0.33003079891204834\n",
      "Epoch 2962, Loss: 1.6668323278427124, Final Batch Loss: 0.4790925085544586\n",
      "Epoch 2963, Loss: 1.5956818759441376, Final Batch Loss: 0.3243062496185303\n",
      "Epoch 2964, Loss: 1.6676689684391022, Final Batch Loss: 0.3760129511356354\n",
      "Epoch 2965, Loss: 1.6280561089515686, Final Batch Loss: 0.3801223635673523\n",
      "Epoch 2966, Loss: 1.9393925666809082, Final Batch Loss: 0.43051043152809143\n",
      "Epoch 2967, Loss: 1.6379273533821106, Final Batch Loss: 0.24671730399131775\n",
      "Epoch 2968, Loss: 1.7992119491100311, Final Batch Loss: 0.28522685170173645\n",
      "Epoch 2969, Loss: 1.6960294544696808, Final Batch Loss: 0.43233194947242737\n",
      "Epoch 2970, Loss: 1.7491578459739685, Final Batch Loss: 0.22852927446365356\n",
      "Epoch 2971, Loss: 1.6600733399391174, Final Batch Loss: 0.28515273332595825\n",
      "Epoch 2972, Loss: 1.6491195112466812, Final Batch Loss: 0.31775733828544617\n",
      "Epoch 2973, Loss: 1.7471826374530792, Final Batch Loss: 0.34781527519226074\n",
      "Epoch 2974, Loss: 1.684597223997116, Final Batch Loss: 0.31780117750167847\n",
      "Epoch 2975, Loss: 1.6893948018550873, Final Batch Loss: 0.3009825050830841\n",
      "Epoch 2976, Loss: 1.560983031988144, Final Batch Loss: 0.2964151203632355\n",
      "Epoch 2977, Loss: 1.8280684649944305, Final Batch Loss: 0.4813145101070404\n",
      "Epoch 2978, Loss: 1.6875108629465103, Final Batch Loss: 0.42529451847076416\n",
      "Epoch 2979, Loss: 1.7573526501655579, Final Batch Loss: 0.37530460953712463\n",
      "Epoch 2980, Loss: 1.6515664160251617, Final Batch Loss: 0.29786691069602966\n",
      "Epoch 2981, Loss: 1.7354198694229126, Final Batch Loss: 0.4031226634979248\n",
      "Epoch 2982, Loss: 1.8322324454784393, Final Batch Loss: 0.3293219208717346\n",
      "Epoch 2983, Loss: 1.7533835172653198, Final Batch Loss: 0.32431817054748535\n",
      "Epoch 2984, Loss: 1.5812861919403076, Final Batch Loss: 0.27993056178092957\n",
      "Epoch 2985, Loss: 1.6398891806602478, Final Batch Loss: 0.32429933547973633\n",
      "Epoch 2986, Loss: 1.79587322473526, Final Batch Loss: 0.43187597393989563\n",
      "Epoch 2987, Loss: 1.765078455209732, Final Batch Loss: 0.3219846487045288\n",
      "Epoch 2988, Loss: 1.685067057609558, Final Batch Loss: 0.3377138376235962\n",
      "Epoch 2989, Loss: 1.7776431143283844, Final Batch Loss: 0.362089604139328\n",
      "Epoch 2990, Loss: 1.6389595866203308, Final Batch Loss: 0.3311304450035095\n",
      "Epoch 2991, Loss: 1.8235267102718353, Final Batch Loss: 0.3520779311656952\n",
      "Epoch 2992, Loss: 1.7349864542484283, Final Batch Loss: 0.3856295347213745\n",
      "Epoch 2993, Loss: 1.657948523759842, Final Batch Loss: 0.2894124388694763\n",
      "Epoch 2994, Loss: 1.665140450000763, Final Batch Loss: 0.31638437509536743\n",
      "Epoch 2995, Loss: 1.7177476584911346, Final Batch Loss: 0.4068145453929901\n",
      "Epoch 2996, Loss: 1.7561004757881165, Final Batch Loss: 0.40953683853149414\n",
      "Epoch 2997, Loss: 1.655990868806839, Final Batch Loss: 0.37897494435310364\n",
      "Epoch 2998, Loss: 1.7137124836444855, Final Batch Loss: 0.34363117814064026\n",
      "Epoch 2999, Loss: 1.7411602139472961, Final Batch Loss: 0.3479410707950592\n",
      "Epoch 3000, Loss: 1.76439967751503, Final Batch Loss: 0.4566579759120941\n",
      "Epoch 3001, Loss: 1.7727965116500854, Final Batch Loss: 0.32601743936538696\n",
      "Epoch 3002, Loss: 1.716635823249817, Final Batch Loss: 0.3324475586414337\n",
      "Epoch 3003, Loss: 1.6898265182971954, Final Batch Loss: 0.42768824100494385\n",
      "Epoch 3004, Loss: 1.5620977282524109, Final Batch Loss: 0.29495152831077576\n",
      "Epoch 3005, Loss: 1.6120261549949646, Final Batch Loss: 0.2640460729598999\n",
      "Epoch 3006, Loss: 1.6703878045082092, Final Batch Loss: 0.3833361566066742\n",
      "Epoch 3007, Loss: 1.6017711460590363, Final Batch Loss: 0.24295374751091003\n",
      "Epoch 3008, Loss: 1.7019383013248444, Final Batch Loss: 0.3995972275733948\n",
      "Epoch 3009, Loss: 1.7913900911808014, Final Batch Loss: 0.3388245105743408\n",
      "Epoch 3010, Loss: 1.7640400826931, Final Batch Loss: 0.3060189187526703\n",
      "Epoch 3011, Loss: 1.6958138048648834, Final Batch Loss: 0.426310271024704\n",
      "Epoch 3012, Loss: 1.6676119267940521, Final Batch Loss: 0.29116740822792053\n",
      "Epoch 3013, Loss: 1.719283103942871, Final Batch Loss: 0.33935385942459106\n",
      "Epoch 3014, Loss: 1.763698935508728, Final Batch Loss: 0.35828903317451477\n",
      "Epoch 3015, Loss: 1.7628215849399567, Final Batch Loss: 0.3504861295223236\n",
      "Epoch 3016, Loss: 1.6592724025249481, Final Batch Loss: 0.2893017828464508\n",
      "Epoch 3017, Loss: 1.7940171360969543, Final Batch Loss: 0.38680821657180786\n",
      "Epoch 3018, Loss: 1.707442194223404, Final Batch Loss: 0.37750890851020813\n",
      "Epoch 3019, Loss: 1.6551108956336975, Final Batch Loss: 0.3561084270477295\n",
      "Epoch 3020, Loss: 1.7829079031944275, Final Batch Loss: 0.3667876720428467\n",
      "Epoch 3021, Loss: 1.7758672535419464, Final Batch Loss: 0.37713122367858887\n",
      "Epoch 3022, Loss: 1.733593910932541, Final Batch Loss: 0.3189467787742615\n",
      "Epoch 3023, Loss: 1.7125243246555328, Final Batch Loss: 0.2962284982204437\n",
      "Epoch 3024, Loss: 1.5745547115802765, Final Batch Loss: 0.2872322201728821\n",
      "Epoch 3025, Loss: 1.6212535202503204, Final Batch Loss: 0.33721137046813965\n",
      "Epoch 3026, Loss: 1.6705729067325592, Final Batch Loss: 0.28513169288635254\n",
      "Epoch 3027, Loss: 1.6905543208122253, Final Batch Loss: 0.295598566532135\n",
      "Epoch 3028, Loss: 1.6526239216327667, Final Batch Loss: 0.39294686913490295\n",
      "Epoch 3029, Loss: 1.7618407011032104, Final Batch Loss: 0.5276635885238647\n",
      "Epoch 3030, Loss: 1.7360584437847137, Final Batch Loss: 0.38284918665885925\n",
      "Epoch 3031, Loss: 1.724840134382248, Final Batch Loss: 0.31708961725234985\n",
      "Epoch 3032, Loss: 1.571401447057724, Final Batch Loss: 0.34472474455833435\n",
      "Epoch 3033, Loss: 1.7192682921886444, Final Batch Loss: 0.38265711069107056\n",
      "Epoch 3034, Loss: 1.732334703207016, Final Batch Loss: 0.42169609665870667\n",
      "Epoch 3035, Loss: 1.743041068315506, Final Batch Loss: 0.34081289172172546\n",
      "Epoch 3036, Loss: 1.7483245730400085, Final Batch Loss: 0.32134580612182617\n",
      "Epoch 3037, Loss: 1.8043956458568573, Final Batch Loss: 0.34587234258651733\n",
      "Epoch 3038, Loss: 1.577125608921051, Final Batch Loss: 0.259158194065094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3039, Loss: 1.7241553962230682, Final Batch Loss: 0.2699947655200958\n",
      "Epoch 3040, Loss: 1.6179610788822174, Final Batch Loss: 0.34479033946990967\n",
      "Epoch 3041, Loss: 1.533644050359726, Final Batch Loss: 0.2705988883972168\n",
      "Epoch 3042, Loss: 1.6513116657733917, Final Batch Loss: 0.3431280851364136\n",
      "Epoch 3043, Loss: 1.6184264123439789, Final Batch Loss: 0.35630691051483154\n",
      "Epoch 3044, Loss: 1.6701339483261108, Final Batch Loss: 0.3308616578578949\n",
      "Epoch 3045, Loss: 1.7445701658725739, Final Batch Loss: 0.33563849329948425\n",
      "Epoch 3046, Loss: 1.6679809093475342, Final Batch Loss: 0.30406951904296875\n",
      "Epoch 3047, Loss: 1.759001761674881, Final Batch Loss: 0.300979882478714\n",
      "Epoch 3048, Loss: 1.7444934248924255, Final Batch Loss: 0.28913238644599915\n",
      "Epoch 3049, Loss: 1.6649098992347717, Final Batch Loss: 0.3306903839111328\n",
      "Epoch 3050, Loss: 1.6270081400871277, Final Batch Loss: 0.27940088510513306\n",
      "Epoch 3051, Loss: 1.6798105537891388, Final Batch Loss: 0.37999460101127625\n",
      "Epoch 3052, Loss: 1.6447733938694, Final Batch Loss: 0.26294851303100586\n",
      "Epoch 3053, Loss: 1.7411546111106873, Final Batch Loss: 0.3325110673904419\n",
      "Epoch 3054, Loss: 1.8024768233299255, Final Batch Loss: 0.35071367025375366\n",
      "Epoch 3055, Loss: 1.769461989402771, Final Batch Loss: 0.3310929834842682\n",
      "Epoch 3056, Loss: 1.6157564520835876, Final Batch Loss: 0.3800138235092163\n",
      "Epoch 3057, Loss: 1.7102102041244507, Final Batch Loss: 0.368870347738266\n",
      "Epoch 3058, Loss: 1.7235674560070038, Final Batch Loss: 0.34064701199531555\n",
      "Epoch 3059, Loss: 1.6324157118797302, Final Batch Loss: 0.26845377683639526\n",
      "Epoch 3060, Loss: 1.6089073717594147, Final Batch Loss: 0.3782669007778168\n",
      "Epoch 3061, Loss: 1.6970735788345337, Final Batch Loss: 0.30786213278770447\n",
      "Epoch 3062, Loss: 1.6930416822433472, Final Batch Loss: 0.41228237748146057\n",
      "Epoch 3063, Loss: 1.7051221132278442, Final Batch Loss: 0.3557508587837219\n",
      "Epoch 3064, Loss: 1.7241968214511871, Final Batch Loss: 0.2669004201889038\n",
      "Epoch 3065, Loss: 1.5838682055473328, Final Batch Loss: 0.3339392840862274\n",
      "Epoch 3066, Loss: 1.702855259180069, Final Batch Loss: 0.29220879077911377\n",
      "Epoch 3067, Loss: 1.673046350479126, Final Batch Loss: 0.3206457197666168\n",
      "Epoch 3068, Loss: 1.6700091660022736, Final Batch Loss: 0.3095448315143585\n",
      "Epoch 3069, Loss: 1.6570270657539368, Final Batch Loss: 0.3087884485721588\n",
      "Epoch 3070, Loss: 1.7562609314918518, Final Batch Loss: 0.3767278492450714\n",
      "Epoch 3071, Loss: 1.6395606696605682, Final Batch Loss: 0.35821565985679626\n",
      "Epoch 3072, Loss: 1.5479809045791626, Final Batch Loss: 0.2939502000808716\n",
      "Epoch 3073, Loss: 1.6685424447059631, Final Batch Loss: 0.3818633258342743\n",
      "Epoch 3074, Loss: 1.6636648178100586, Final Batch Loss: 0.3376547694206238\n",
      "Epoch 3075, Loss: 1.6860126554965973, Final Batch Loss: 0.41264015436172485\n",
      "Epoch 3076, Loss: 1.6418810784816742, Final Batch Loss: 0.3284410834312439\n",
      "Epoch 3077, Loss: 1.6434365510940552, Final Batch Loss: 0.30764874815940857\n",
      "Epoch 3078, Loss: 1.637557864189148, Final Batch Loss: 0.38546022772789\n",
      "Epoch 3079, Loss: 1.6659611761569977, Final Batch Loss: 0.342487633228302\n",
      "Epoch 3080, Loss: 1.7679712772369385, Final Batch Loss: 0.41898927092552185\n",
      "Epoch 3081, Loss: 1.707575410604477, Final Batch Loss: 0.3308190107345581\n",
      "Epoch 3082, Loss: 1.6851935982704163, Final Batch Loss: 0.402588814496994\n",
      "Epoch 3083, Loss: 1.6716072410345078, Final Batch Loss: 0.3417340815067291\n",
      "Epoch 3084, Loss: 1.672406554222107, Final Batch Loss: 0.26054486632347107\n",
      "Epoch 3085, Loss: 1.6758044064044952, Final Batch Loss: 0.2700394093990326\n",
      "Epoch 3086, Loss: 1.7332059144973755, Final Batch Loss: 0.3422933518886566\n",
      "Epoch 3087, Loss: 1.5904928147792816, Final Batch Loss: 0.29025617241859436\n",
      "Epoch 3088, Loss: 1.6043153703212738, Final Batch Loss: 0.3506630063056946\n",
      "Epoch 3089, Loss: 1.6436949968338013, Final Batch Loss: 0.3012981414794922\n",
      "Epoch 3090, Loss: 1.7180903553962708, Final Batch Loss: 0.42589128017425537\n",
      "Epoch 3091, Loss: 1.598179280757904, Final Batch Loss: 0.28272220492362976\n",
      "Epoch 3092, Loss: 1.5266129672527313, Final Batch Loss: 0.2792634963989258\n",
      "Epoch 3093, Loss: 1.772032082080841, Final Batch Loss: 0.34986963868141174\n",
      "Epoch 3094, Loss: 1.8361369967460632, Final Batch Loss: 0.3712577819824219\n",
      "Epoch 3095, Loss: 1.5569131672382355, Final Batch Loss: 0.33387070894241333\n",
      "Epoch 3096, Loss: 1.6854866743087769, Final Batch Loss: 0.3527672588825226\n",
      "Epoch 3097, Loss: 1.676411360502243, Final Batch Loss: 0.3474966287612915\n",
      "Epoch 3098, Loss: 1.6608203649520874, Final Batch Loss: 0.3436252474784851\n",
      "Epoch 3099, Loss: 1.5760251879692078, Final Batch Loss: 0.2751573920249939\n",
      "Epoch 3100, Loss: 1.6949527561664581, Final Batch Loss: 0.35187652707099915\n",
      "Epoch 3101, Loss: 1.6464582085609436, Final Batch Loss: 0.4033527672290802\n",
      "Epoch 3102, Loss: 1.6558066308498383, Final Batch Loss: 0.3125131130218506\n",
      "Epoch 3103, Loss: 1.6652846932411194, Final Batch Loss: 0.3889670670032501\n",
      "Epoch 3104, Loss: 1.712007462978363, Final Batch Loss: 0.3608020842075348\n",
      "Epoch 3105, Loss: 1.664221704006195, Final Batch Loss: 0.3282085359096527\n",
      "Epoch 3106, Loss: 1.6732624769210815, Final Batch Loss: 0.32976582646369934\n",
      "Epoch 3107, Loss: 1.6383835077285767, Final Batch Loss: 0.33446788787841797\n",
      "Epoch 3108, Loss: 1.5501269698143005, Final Batch Loss: 0.28666675090789795\n",
      "Epoch 3109, Loss: 1.6676568984985352, Final Batch Loss: 0.38061097264289856\n",
      "Epoch 3110, Loss: 1.5845198333263397, Final Batch Loss: 0.2412409782409668\n",
      "Epoch 3111, Loss: 1.6412338018417358, Final Batch Loss: 0.3087720274925232\n",
      "Epoch 3112, Loss: 1.5592441856861115, Final Batch Loss: 0.3157224953174591\n",
      "Epoch 3113, Loss: 1.590288370847702, Final Batch Loss: 0.2860434055328369\n",
      "Epoch 3114, Loss: 1.6519332826137543, Final Batch Loss: 0.31440427899360657\n",
      "Epoch 3115, Loss: 1.6400704681873322, Final Batch Loss: 0.34229254722595215\n",
      "Epoch 3116, Loss: 1.6373724341392517, Final Batch Loss: 0.33058756589889526\n",
      "Epoch 3117, Loss: 1.7383435666561127, Final Batch Loss: 0.3231571614742279\n",
      "Epoch 3118, Loss: 1.797432154417038, Final Batch Loss: 0.3677665591239929\n",
      "Epoch 3119, Loss: 1.5570799112319946, Final Batch Loss: 0.3905278444290161\n",
      "Epoch 3120, Loss: 1.7761267125606537, Final Batch Loss: 0.394932359457016\n",
      "Epoch 3121, Loss: 1.712908387184143, Final Batch Loss: 0.4259898364543915\n",
      "Epoch 3122, Loss: 1.6296919882297516, Final Batch Loss: 0.2992447018623352\n",
      "Epoch 3123, Loss: 1.700010597705841, Final Batch Loss: 0.27757811546325684\n",
      "Epoch 3124, Loss: 1.6175666153430939, Final Batch Loss: 0.3104439973831177\n",
      "Epoch 3125, Loss: 1.6689263582229614, Final Batch Loss: 0.29677724838256836\n",
      "Epoch 3126, Loss: 1.5834637582302094, Final Batch Loss: 0.313800185918808\n",
      "Epoch 3127, Loss: 1.910402625799179, Final Batch Loss: 0.4081882834434509\n",
      "Epoch 3128, Loss: 1.777536153793335, Final Batch Loss: 0.37363120913505554\n",
      "Epoch 3129, Loss: 1.6572121679782867, Final Batch Loss: 0.3330881893634796\n",
      "Epoch 3130, Loss: 1.6327335834503174, Final Batch Loss: 0.30406901240348816\n",
      "Epoch 3131, Loss: 1.7611409723758698, Final Batch Loss: 0.3724692463874817\n",
      "Epoch 3132, Loss: 1.6180806457996368, Final Batch Loss: 0.25097039341926575\n",
      "Epoch 3133, Loss: 1.7880269289016724, Final Batch Loss: 0.3557754456996918\n",
      "Epoch 3134, Loss: 1.6205693483352661, Final Batch Loss: 0.31804898381233215\n",
      "Epoch 3135, Loss: 1.682613492012024, Final Batch Loss: 0.3776286542415619\n",
      "Epoch 3136, Loss: 1.6297995448112488, Final Batch Loss: 0.4281509518623352\n",
      "Epoch 3137, Loss: 1.595422476530075, Final Batch Loss: 0.38698050379753113\n",
      "Epoch 3138, Loss: 1.6031671315431595, Final Batch Loss: 0.3207101821899414\n",
      "Epoch 3139, Loss: 1.7729180753231049, Final Batch Loss: 0.45609942078590393\n",
      "Epoch 3140, Loss: 1.6153801381587982, Final Batch Loss: 0.25372862815856934\n",
      "Epoch 3141, Loss: 1.6441481113433838, Final Batch Loss: 0.3470642864704132\n",
      "Epoch 3142, Loss: 1.5509560704231262, Final Batch Loss: 0.2860667109489441\n",
      "Epoch 3143, Loss: 1.705996423959732, Final Batch Loss: 0.32870858907699585\n",
      "Epoch 3144, Loss: 1.5640653371810913, Final Batch Loss: 0.26435282826423645\n",
      "Epoch 3145, Loss: 1.6748703122138977, Final Batch Loss: 0.3351743817329407\n",
      "Epoch 3146, Loss: 1.7513734698295593, Final Batch Loss: 0.36120688915252686\n",
      "Epoch 3147, Loss: 1.6520242094993591, Final Batch Loss: 0.3043287694454193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3148, Loss: 1.5797999799251556, Final Batch Loss: 0.3368445634841919\n",
      "Epoch 3149, Loss: 1.6412220299243927, Final Batch Loss: 0.3049236834049225\n",
      "Epoch 3150, Loss: 1.5809932351112366, Final Batch Loss: 0.266873836517334\n",
      "Epoch 3151, Loss: 1.764777511358261, Final Batch Loss: 0.31186434626579285\n",
      "Epoch 3152, Loss: 1.6530820429325104, Final Batch Loss: 0.4103339612483978\n",
      "Epoch 3153, Loss: 1.601826548576355, Final Batch Loss: 0.29537612199783325\n",
      "Epoch 3154, Loss: 1.7477399706840515, Final Batch Loss: 0.40635278820991516\n",
      "Epoch 3155, Loss: 1.6271204054355621, Final Batch Loss: 0.26317229866981506\n",
      "Epoch 3156, Loss: 1.7568420469760895, Final Batch Loss: 0.4369964301586151\n",
      "Epoch 3157, Loss: 1.750736951828003, Final Batch Loss: 0.380315899848938\n",
      "Epoch 3158, Loss: 1.6022021770477295, Final Batch Loss: 0.337430864572525\n",
      "Epoch 3159, Loss: 1.5918045938014984, Final Batch Loss: 0.36465466022491455\n",
      "Epoch 3160, Loss: 1.7847692668437958, Final Batch Loss: 0.40915393829345703\n",
      "Epoch 3161, Loss: 1.6196962893009186, Final Batch Loss: 0.3633647859096527\n",
      "Epoch 3162, Loss: 1.6899915635585785, Final Batch Loss: 0.35890936851501465\n",
      "Epoch 3163, Loss: 1.628048300743103, Final Batch Loss: 0.24958503246307373\n",
      "Epoch 3164, Loss: 1.6326650083065033, Final Batch Loss: 0.38566726446151733\n",
      "Epoch 3165, Loss: 1.6910678446292877, Final Batch Loss: 0.402529776096344\n",
      "Epoch 3166, Loss: 1.6335141360759735, Final Batch Loss: 0.39617204666137695\n",
      "Epoch 3167, Loss: 1.7010158002376556, Final Batch Loss: 0.2806119918823242\n",
      "Epoch 3168, Loss: 1.6563728153705597, Final Batch Loss: 0.34839773178100586\n",
      "Epoch 3169, Loss: 1.7730332911014557, Final Batch Loss: 0.4294930696487427\n",
      "Epoch 3170, Loss: 1.658024787902832, Final Batch Loss: 0.2965119183063507\n",
      "Epoch 3171, Loss: 1.7906637489795685, Final Batch Loss: 0.328409343957901\n",
      "Epoch 3172, Loss: 1.6532961428165436, Final Batch Loss: 0.3279098868370056\n",
      "Epoch 3173, Loss: 1.7065456807613373, Final Batch Loss: 0.3673762381076813\n",
      "Epoch 3174, Loss: 1.6229210495948792, Final Batch Loss: 0.30656641721725464\n",
      "Epoch 3175, Loss: 1.7315024435520172, Final Batch Loss: 0.36219245195388794\n",
      "Epoch 3176, Loss: 1.5336562395095825, Final Batch Loss: 0.2997639775276184\n",
      "Epoch 3177, Loss: 1.6687024235725403, Final Batch Loss: 0.3438001275062561\n",
      "Epoch 3178, Loss: 1.6572945713996887, Final Batch Loss: 0.35302796959877014\n",
      "Epoch 3179, Loss: 1.6259147822856903, Final Batch Loss: 0.27966469526290894\n",
      "Epoch 3180, Loss: 1.6641652286052704, Final Batch Loss: 0.37855976819992065\n",
      "Epoch 3181, Loss: 1.6065615713596344, Final Batch Loss: 0.29190611839294434\n",
      "Epoch 3182, Loss: 1.6437033116817474, Final Batch Loss: 0.38807591795921326\n",
      "Epoch 3183, Loss: 1.5912696421146393, Final Batch Loss: 0.25811484456062317\n",
      "Epoch 3184, Loss: 1.6596563756465912, Final Batch Loss: 0.3208376169204712\n",
      "Epoch 3185, Loss: 1.7058756351470947, Final Batch Loss: 0.3802928924560547\n",
      "Epoch 3186, Loss: 1.738288164138794, Final Batch Loss: 0.27045273780822754\n",
      "Epoch 3187, Loss: 1.5051429569721222, Final Batch Loss: 0.3807514011859894\n",
      "Epoch 3188, Loss: 1.6326517760753632, Final Batch Loss: 0.4314488470554352\n",
      "Epoch 3189, Loss: 1.6235723197460175, Final Batch Loss: 0.2883404791355133\n",
      "Epoch 3190, Loss: 1.6338965594768524, Final Batch Loss: 0.3870232403278351\n",
      "Epoch 3191, Loss: 1.5135281085968018, Final Batch Loss: 0.30821463465690613\n",
      "Epoch 3192, Loss: 1.5811847448349, Final Batch Loss: 0.3543626070022583\n",
      "Epoch 3193, Loss: 1.5997788906097412, Final Batch Loss: 0.2814759314060211\n",
      "Epoch 3194, Loss: 1.5206336975097656, Final Batch Loss: 0.3223321735858917\n",
      "Epoch 3195, Loss: 1.6112542301416397, Final Batch Loss: 0.1991407722234726\n",
      "Epoch 3196, Loss: 1.6630251109600067, Final Batch Loss: 0.2607945203781128\n",
      "Epoch 3197, Loss: 1.6250346899032593, Final Batch Loss: 0.2720660865306854\n",
      "Epoch 3198, Loss: 1.7189739346504211, Final Batch Loss: 0.4082227349281311\n",
      "Epoch 3199, Loss: 1.68398317694664, Final Batch Loss: 0.331154465675354\n",
      "Epoch 3200, Loss: 1.534880667924881, Final Batch Loss: 0.3195390999317169\n",
      "Epoch 3201, Loss: 1.7014649510383606, Final Batch Loss: 0.26063331961631775\n",
      "Epoch 3202, Loss: 1.6059798300266266, Final Batch Loss: 0.2688853442668915\n",
      "Epoch 3203, Loss: 1.636393427848816, Final Batch Loss: 0.3485347032546997\n",
      "Epoch 3204, Loss: 1.7047143876552582, Final Batch Loss: 0.4208417236804962\n",
      "Epoch 3205, Loss: 1.5667074918746948, Final Batch Loss: 0.30090588331222534\n",
      "Epoch 3206, Loss: 1.6817242801189423, Final Batch Loss: 0.34394365549087524\n",
      "Epoch 3207, Loss: 1.628072589635849, Final Batch Loss: 0.3310130536556244\n",
      "Epoch 3208, Loss: 1.5902979522943497, Final Batch Loss: 0.23242934048175812\n",
      "Epoch 3209, Loss: 1.8065058887004852, Final Batch Loss: 0.5010301470756531\n",
      "Epoch 3210, Loss: 1.5813776552677155, Final Batch Loss: 0.2896363437175751\n",
      "Epoch 3211, Loss: 1.7004605531692505, Final Batch Loss: 0.4170445501804352\n",
      "Epoch 3212, Loss: 1.5860072076320648, Final Batch Loss: 0.3571773171424866\n",
      "Epoch 3213, Loss: 1.6471344232559204, Final Batch Loss: 0.3219344913959503\n",
      "Epoch 3214, Loss: 1.729593425989151, Final Batch Loss: 0.3260365426540375\n",
      "Epoch 3215, Loss: 1.715246856212616, Final Batch Loss: 0.4633945822715759\n",
      "Epoch 3216, Loss: 1.5940527617931366, Final Batch Loss: 0.2565462589263916\n",
      "Epoch 3217, Loss: 1.8377711474895477, Final Batch Loss: 0.46895208954811096\n",
      "Epoch 3218, Loss: 1.5956090688705444, Final Batch Loss: 0.3475070595741272\n",
      "Epoch 3219, Loss: 1.780776709318161, Final Batch Loss: 0.5026192665100098\n",
      "Epoch 3220, Loss: 1.590476393699646, Final Batch Loss: 0.3251575827598572\n",
      "Epoch 3221, Loss: 1.6343991458415985, Final Batch Loss: 0.3005661964416504\n",
      "Epoch 3222, Loss: 1.801524132490158, Final Batch Loss: 0.4545864164829254\n",
      "Epoch 3223, Loss: 1.5751319825649261, Final Batch Loss: 0.2930518388748169\n",
      "Epoch 3224, Loss: 1.6882801055908203, Final Batch Loss: 0.4284590482711792\n",
      "Epoch 3225, Loss: 1.6337564587593079, Final Batch Loss: 0.4065808951854706\n",
      "Epoch 3226, Loss: 1.6644267737865448, Final Batch Loss: 0.2625429630279541\n",
      "Epoch 3227, Loss: 1.568742424249649, Final Batch Loss: 0.29045283794403076\n",
      "Epoch 3228, Loss: 1.581524282693863, Final Batch Loss: 0.35225367546081543\n",
      "Epoch 3229, Loss: 1.6076871454715729, Final Batch Loss: 0.3713437020778656\n",
      "Epoch 3230, Loss: 1.6190602779388428, Final Batch Loss: 0.32213473320007324\n",
      "Epoch 3231, Loss: 1.5610265135765076, Final Batch Loss: 0.3714660108089447\n",
      "Epoch 3232, Loss: 1.668811947107315, Final Batch Loss: 0.3257750868797302\n",
      "Epoch 3233, Loss: 1.55372554063797, Final Batch Loss: 0.28717413544654846\n",
      "Epoch 3234, Loss: 1.6239384561777115, Final Batch Loss: 0.3283916115760803\n",
      "Epoch 3235, Loss: 1.6244159936904907, Final Batch Loss: 0.31832030415534973\n",
      "Epoch 3236, Loss: 1.6154166460037231, Final Batch Loss: 0.3334490954875946\n",
      "Epoch 3237, Loss: 1.6482303440570831, Final Batch Loss: 0.28753188252449036\n",
      "Epoch 3238, Loss: 1.7455209791660309, Final Batch Loss: 0.4182104170322418\n",
      "Epoch 3239, Loss: 1.5454552322626114, Final Batch Loss: 0.26922607421875\n",
      "Epoch 3240, Loss: 1.8625394999980927, Final Batch Loss: 0.42713943123817444\n",
      "Epoch 3241, Loss: 2.0143229365348816, Final Batch Loss: 0.39263254404067993\n",
      "Epoch 3242, Loss: 1.7706947922706604, Final Batch Loss: 0.3861817717552185\n",
      "Epoch 3243, Loss: 1.883418470621109, Final Batch Loss: 0.4883027672767639\n",
      "Epoch 3244, Loss: 1.6367823779582977, Final Batch Loss: 0.37216073274612427\n",
      "Epoch 3245, Loss: 1.9176719188690186, Final Batch Loss: 0.3013014495372772\n",
      "Epoch 3246, Loss: 1.6068004071712494, Final Batch Loss: 0.35239365696907043\n",
      "Epoch 3247, Loss: 1.7079168558120728, Final Batch Loss: 0.23040500283241272\n",
      "Epoch 3248, Loss: 1.7687490284442902, Final Batch Loss: 0.4078550636768341\n",
      "Epoch 3249, Loss: 1.596864327788353, Final Batch Loss: 0.37777796387672424\n",
      "Epoch 3250, Loss: 1.6247113645076752, Final Batch Loss: 0.2919636070728302\n",
      "Epoch 3251, Loss: 1.8063448369503021, Final Batch Loss: 0.4140329360961914\n",
      "Epoch 3252, Loss: 1.6506788730621338, Final Batch Loss: 0.2787737548351288\n",
      "Epoch 3253, Loss: 1.5105574131011963, Final Batch Loss: 0.26564738154411316\n",
      "Epoch 3254, Loss: 1.5669209957122803, Final Batch Loss: 0.26918283104896545\n",
      "Epoch 3255, Loss: 1.5190904140472412, Final Batch Loss: 0.32398179173469543\n",
      "Epoch 3256, Loss: 1.568644866347313, Final Batch Loss: 0.38759633898735046\n",
      "Epoch 3257, Loss: 1.605824500322342, Final Batch Loss: 0.33366039395332336\n",
      "Epoch 3258, Loss: 1.6729035675525665, Final Batch Loss: 0.30683594942092896\n",
      "Epoch 3259, Loss: 1.6663524508476257, Final Batch Loss: 0.28325575590133667\n",
      "Epoch 3260, Loss: 1.598020851612091, Final Batch Loss: 0.3255598545074463\n",
      "Epoch 3261, Loss: 1.622582495212555, Final Batch Loss: 0.29384902119636536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3262, Loss: 1.5421073883771896, Final Batch Loss: 0.247334286570549\n",
      "Epoch 3263, Loss: 1.5549822449684143, Final Batch Loss: 0.2473643273115158\n",
      "Epoch 3264, Loss: 1.4567998945713043, Final Batch Loss: 0.21885815262794495\n",
      "Epoch 3265, Loss: 1.5446215867996216, Final Batch Loss: 0.3082902729511261\n",
      "Epoch 3266, Loss: 1.691420316696167, Final Batch Loss: 0.3715206980705261\n",
      "Epoch 3267, Loss: 1.7872931957244873, Final Batch Loss: 0.4871903657913208\n",
      "Epoch 3268, Loss: 1.6023361831903458, Final Batch Loss: 0.29640939831733704\n",
      "Epoch 3269, Loss: 1.5746228396892548, Final Batch Loss: 0.2925173044204712\n",
      "Epoch 3270, Loss: 1.683652549982071, Final Batch Loss: 0.34355106949806213\n",
      "Epoch 3271, Loss: 1.5336729139089584, Final Batch Loss: 0.2993811368942261\n",
      "Epoch 3272, Loss: 1.6359052509069443, Final Batch Loss: 0.4504692256450653\n",
      "Epoch 3273, Loss: 1.7229528725147247, Final Batch Loss: 0.42844563722610474\n",
      "Epoch 3274, Loss: 1.546084314584732, Final Batch Loss: 0.33842670917510986\n",
      "Epoch 3275, Loss: 1.6063123047351837, Final Batch Loss: 0.29167744517326355\n",
      "Epoch 3276, Loss: 1.5086172968149185, Final Batch Loss: 0.23765434324741364\n",
      "Epoch 3277, Loss: 1.6388507783412933, Final Batch Loss: 0.3075881004333496\n",
      "Epoch 3278, Loss: 1.7269211262464523, Final Batch Loss: 0.39204081892967224\n",
      "Epoch 3279, Loss: 1.4667288661003113, Final Batch Loss: 0.23566533625125885\n",
      "Epoch 3280, Loss: 1.4118043035268784, Final Batch Loss: 0.2544429898262024\n",
      "Epoch 3281, Loss: 1.6566231846809387, Final Batch Loss: 0.3358753025531769\n",
      "Epoch 3282, Loss: 1.6200122833251953, Final Batch Loss: 0.385845422744751\n",
      "Epoch 3283, Loss: 1.5360520482063293, Final Batch Loss: 0.32946887612342834\n",
      "Epoch 3284, Loss: 1.5715351551771164, Final Batch Loss: 0.2742109000682831\n",
      "Epoch 3285, Loss: 1.581917941570282, Final Batch Loss: 0.33930841088294983\n",
      "Epoch 3286, Loss: 1.5869376957416534, Final Batch Loss: 0.31501948833465576\n",
      "Epoch 3287, Loss: 1.8321109116077423, Final Batch Loss: 0.4532088041305542\n",
      "Epoch 3288, Loss: 1.5184814631938934, Final Batch Loss: 0.28980687260627747\n",
      "Epoch 3289, Loss: 1.7703863382339478, Final Batch Loss: 0.3555252254009247\n",
      "Epoch 3290, Loss: 1.5752297043800354, Final Batch Loss: 0.36380627751350403\n",
      "Epoch 3291, Loss: 1.5116827487945557, Final Batch Loss: 0.3076882064342499\n",
      "Epoch 3292, Loss: 1.5614564418792725, Final Batch Loss: 0.23957622051239014\n",
      "Epoch 3293, Loss: 1.546216607093811, Final Batch Loss: 0.2588408589363098\n",
      "Epoch 3294, Loss: 1.7566254138946533, Final Batch Loss: 0.44332990050315857\n",
      "Epoch 3295, Loss: 1.5459341704845428, Final Batch Loss: 0.252821683883667\n",
      "Epoch 3296, Loss: 1.5253819823265076, Final Batch Loss: 0.290237158536911\n",
      "Epoch 3297, Loss: 1.5560856461524963, Final Batch Loss: 0.30107808113098145\n",
      "Epoch 3298, Loss: 1.6764309406280518, Final Batch Loss: 0.3852942883968353\n",
      "Epoch 3299, Loss: 1.6573761105537415, Final Batch Loss: 0.32455095648765564\n",
      "Epoch 3300, Loss: 1.5256787985563278, Final Batch Loss: 0.2996465563774109\n",
      "Epoch 3301, Loss: 1.654913604259491, Final Batch Loss: 0.37184762954711914\n",
      "Epoch 3302, Loss: 1.6534585654735565, Final Batch Loss: 0.3836175203323364\n",
      "Epoch 3303, Loss: 1.7059170007705688, Final Batch Loss: 0.3698680102825165\n",
      "Epoch 3304, Loss: 1.6326879858970642, Final Batch Loss: 0.3970111310482025\n",
      "Epoch 3305, Loss: 1.66497141122818, Final Batch Loss: 0.31891217827796936\n",
      "Epoch 3306, Loss: 1.6457367837429047, Final Batch Loss: 0.3047127425670624\n",
      "Epoch 3307, Loss: 1.6236232221126556, Final Batch Loss: 0.34851711988449097\n",
      "Epoch 3308, Loss: 1.6147083640098572, Final Batch Loss: 0.36185982823371887\n",
      "Epoch 3309, Loss: 1.8134992718696594, Final Batch Loss: 0.3899924159049988\n",
      "Epoch 3310, Loss: 1.7543806433677673, Final Batch Loss: 0.42719733715057373\n",
      "Epoch 3311, Loss: 1.6306535601615906, Final Batch Loss: 0.3683619201183319\n",
      "Epoch 3312, Loss: 1.62128846347332, Final Batch Loss: 0.46348103880882263\n",
      "Epoch 3313, Loss: 1.7913515865802765, Final Batch Loss: 0.3323788344860077\n",
      "Epoch 3314, Loss: 1.5698750019073486, Final Batch Loss: 0.3613303601741791\n",
      "Epoch 3315, Loss: 1.65308079123497, Final Batch Loss: 0.3708598017692566\n",
      "Epoch 3316, Loss: 1.5052583515644073, Final Batch Loss: 0.2688432037830353\n",
      "Epoch 3317, Loss: 1.7119419574737549, Final Batch Loss: 0.3698462247848511\n",
      "Epoch 3318, Loss: 1.650503009557724, Final Batch Loss: 0.3774717152118683\n",
      "Epoch 3319, Loss: 1.5174424350261688, Final Batch Loss: 0.27953484654426575\n",
      "Epoch 3320, Loss: 1.7483371645212173, Final Batch Loss: 0.4566737115383148\n",
      "Epoch 3321, Loss: 1.5539916455745697, Final Batch Loss: 0.28130042552948\n",
      "Epoch 3322, Loss: 1.5126152038574219, Final Batch Loss: 0.2625121772289276\n",
      "Epoch 3323, Loss: 1.6728754937648773, Final Batch Loss: 0.3503226339817047\n",
      "Epoch 3324, Loss: 1.6596693396568298, Final Batch Loss: 0.3380834758281708\n",
      "Epoch 3325, Loss: 1.545137882232666, Final Batch Loss: 0.26081687211990356\n",
      "Epoch 3326, Loss: 1.6144754588603973, Final Batch Loss: 0.3429432213306427\n",
      "Epoch 3327, Loss: 1.7584031522274017, Final Batch Loss: 0.28496047854423523\n",
      "Epoch 3328, Loss: 1.5215198397636414, Final Batch Loss: 0.29403039813041687\n",
      "Epoch 3329, Loss: 1.5850758850574493, Final Batch Loss: 0.25147461891174316\n",
      "Epoch 3330, Loss: 1.658182829618454, Final Batch Loss: 0.3200986683368683\n",
      "Epoch 3331, Loss: 1.6225660145282745, Final Batch Loss: 0.3423311710357666\n",
      "Epoch 3332, Loss: 1.5866027772426605, Final Batch Loss: 0.3178730010986328\n",
      "Epoch 3333, Loss: 1.6556785106658936, Final Batch Loss: 0.35328438878059387\n",
      "Epoch 3334, Loss: 1.663805216550827, Final Batch Loss: 0.3648541271686554\n",
      "Epoch 3335, Loss: 1.4855030477046967, Final Batch Loss: 0.2733082175254822\n",
      "Epoch 3336, Loss: 1.4397142678499222, Final Batch Loss: 0.2676524519920349\n",
      "Epoch 3337, Loss: 1.6371484398841858, Final Batch Loss: 0.35610464215278625\n",
      "Epoch 3338, Loss: 1.6868918240070343, Final Batch Loss: 0.39958706498146057\n",
      "Epoch 3339, Loss: 1.726751670241356, Final Batch Loss: 0.41454580426216125\n",
      "Epoch 3340, Loss: 1.77028027176857, Final Batch Loss: 0.4591124951839447\n",
      "Epoch 3341, Loss: 1.6052643656730652, Final Batch Loss: 0.3420288562774658\n",
      "Epoch 3342, Loss: 1.4384836554527283, Final Batch Loss: 0.3117673695087433\n",
      "Epoch 3343, Loss: 1.539171040058136, Final Batch Loss: 0.38135090470314026\n",
      "Epoch 3344, Loss: 1.471108764410019, Final Batch Loss: 0.29082533717155457\n",
      "Epoch 3345, Loss: 1.4922106862068176, Final Batch Loss: 0.2624383568763733\n",
      "Epoch 3346, Loss: 1.5443481802940369, Final Batch Loss: 0.33326849341392517\n",
      "Epoch 3347, Loss: 1.7210101783275604, Final Batch Loss: 0.26235833764076233\n",
      "Epoch 3348, Loss: 1.5086492896080017, Final Batch Loss: 0.2716624140739441\n",
      "Epoch 3349, Loss: 1.579682782292366, Final Batch Loss: 0.3259652256965637\n",
      "Epoch 3350, Loss: 1.5483149588108063, Final Batch Loss: 0.291573703289032\n",
      "Epoch 3351, Loss: 1.6110442876815796, Final Batch Loss: 0.28947338461875916\n",
      "Epoch 3352, Loss: 1.66033935546875, Final Batch Loss: 0.3704698085784912\n",
      "Epoch 3353, Loss: 1.5320155918598175, Final Batch Loss: 0.23566487431526184\n",
      "Epoch 3354, Loss: 1.667352318763733, Final Batch Loss: 0.3107214868068695\n",
      "Epoch 3355, Loss: 1.6094424724578857, Final Batch Loss: 0.3479371964931488\n",
      "Epoch 3356, Loss: 1.655197262763977, Final Batch Loss: 0.35821425914764404\n",
      "Epoch 3357, Loss: 1.6422510147094727, Final Batch Loss: 0.28747278451919556\n",
      "Epoch 3358, Loss: 1.6205089688301086, Final Batch Loss: 0.3023034930229187\n",
      "Epoch 3359, Loss: 1.5129796415567398, Final Batch Loss: 0.2777956426143646\n",
      "Epoch 3360, Loss: 1.5744423568248749, Final Batch Loss: 0.3179059624671936\n",
      "Epoch 3361, Loss: 1.7130467295646667, Final Batch Loss: 0.3075568675994873\n",
      "Epoch 3362, Loss: 1.6863612532615662, Final Batch Loss: 0.46152395009994507\n",
      "Epoch 3363, Loss: 1.6905574202537537, Final Batch Loss: 0.41391226649284363\n",
      "Epoch 3364, Loss: 1.5805624425411224, Final Batch Loss: 0.29467132687568665\n",
      "Epoch 3365, Loss: 1.4931251406669617, Final Batch Loss: 0.27594152092933655\n",
      "Epoch 3366, Loss: 1.5496730506420135, Final Batch Loss: 0.4036219120025635\n",
      "Epoch 3367, Loss: 1.6136756837368011, Final Batch Loss: 0.3040505051612854\n",
      "Epoch 3368, Loss: 1.4929383099079132, Final Batch Loss: 0.28247714042663574\n",
      "Epoch 3369, Loss: 1.6341354846954346, Final Batch Loss: 0.3845823109149933\n",
      "Epoch 3370, Loss: 1.7650768458843231, Final Batch Loss: 0.4473007321357727\n",
      "Epoch 3371, Loss: 1.5945169031620026, Final Batch Loss: 0.27304694056510925\n",
      "Epoch 3372, Loss: 1.674682468175888, Final Batch Loss: 0.3024270534515381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3373, Loss: 1.7480315268039703, Final Batch Loss: 0.3661932945251465\n",
      "Epoch 3374, Loss: 1.689467966556549, Final Batch Loss: 0.35660383105278015\n",
      "Epoch 3375, Loss: 1.6549626290798187, Final Batch Loss: 0.38065826892852783\n",
      "Epoch 3376, Loss: 1.764360100030899, Final Batch Loss: 0.3260754942893982\n",
      "Epoch 3377, Loss: 1.5803201496601105, Final Batch Loss: 0.2844817340373993\n",
      "Epoch 3378, Loss: 1.7061836421489716, Final Batch Loss: 0.44552165269851685\n",
      "Epoch 3379, Loss: 1.6001113057136536, Final Batch Loss: 0.28677985072135925\n",
      "Epoch 3380, Loss: 1.6218771934509277, Final Batch Loss: 0.2995070219039917\n",
      "Epoch 3381, Loss: 1.5746427774429321, Final Batch Loss: 0.29975026845932007\n",
      "Epoch 3382, Loss: 1.7364530861377716, Final Batch Loss: 0.37798041105270386\n",
      "Epoch 3383, Loss: 1.553698867559433, Final Batch Loss: 0.28774136304855347\n",
      "Epoch 3384, Loss: 1.4514354169368744, Final Batch Loss: 0.3402024209499359\n",
      "Epoch 3385, Loss: 1.553220495581627, Final Batch Loss: 0.3886837363243103\n",
      "Epoch 3386, Loss: 1.560616672039032, Final Batch Loss: 0.31304028630256653\n",
      "Epoch 3387, Loss: 1.5130476355552673, Final Batch Loss: 0.2834589183330536\n",
      "Epoch 3388, Loss: 1.540067970752716, Final Batch Loss: 0.2533864378929138\n",
      "Epoch 3389, Loss: 1.5088105499744415, Final Batch Loss: 0.32409313321113586\n",
      "Epoch 3390, Loss: 1.4856916069984436, Final Batch Loss: 0.24586528539657593\n",
      "Epoch 3391, Loss: 1.5637397766113281, Final Batch Loss: 0.26874402165412903\n",
      "Epoch 3392, Loss: 1.4627575874328613, Final Batch Loss: 0.3298904299736023\n",
      "Epoch 3393, Loss: 1.5004403740167618, Final Batch Loss: 0.32141321897506714\n",
      "Epoch 3394, Loss: 1.4879849702119827, Final Batch Loss: 0.32653629779815674\n",
      "Epoch 3395, Loss: 1.6515767276287079, Final Batch Loss: 0.33114901185035706\n",
      "Epoch 3396, Loss: 1.5669628381729126, Final Batch Loss: 0.28026774525642395\n",
      "Epoch 3397, Loss: 1.725775495171547, Final Batch Loss: 0.3604739308357239\n",
      "Epoch 3398, Loss: 1.5108495503664017, Final Batch Loss: 0.261782169342041\n",
      "Epoch 3399, Loss: 1.4888383895158768, Final Batch Loss: 0.22900472581386566\n",
      "Epoch 3400, Loss: 1.6281756162643433, Final Batch Loss: 0.3200248181819916\n",
      "Epoch 3401, Loss: 1.7021617591381073, Final Batch Loss: 0.3492647111415863\n",
      "Epoch 3402, Loss: 1.456950843334198, Final Batch Loss: 0.3116569519042969\n",
      "Epoch 3403, Loss: 1.657826006412506, Final Batch Loss: 0.25430095195770264\n",
      "Epoch 3404, Loss: 1.7154107242822647, Final Batch Loss: 0.44684523344039917\n",
      "Epoch 3405, Loss: 1.5825333893299103, Final Batch Loss: 0.31380677223205566\n",
      "Epoch 3406, Loss: 1.4978381097316742, Final Batch Loss: 0.28673747181892395\n",
      "Epoch 3407, Loss: 1.6401426196098328, Final Batch Loss: 0.2964087128639221\n",
      "Epoch 3408, Loss: 1.5663250535726547, Final Batch Loss: 0.37615329027175903\n",
      "Epoch 3409, Loss: 1.7202176451683044, Final Batch Loss: 0.36267268657684326\n",
      "Epoch 3410, Loss: 1.644685298204422, Final Batch Loss: 0.32668963074684143\n",
      "Epoch 3411, Loss: 1.6304840445518494, Final Batch Loss: 0.36523720622062683\n",
      "Epoch 3412, Loss: 1.5974231660366058, Final Batch Loss: 0.31661251187324524\n",
      "Epoch 3413, Loss: 1.4946592450141907, Final Batch Loss: 0.3013963997364044\n",
      "Epoch 3414, Loss: 1.619354784488678, Final Batch Loss: 0.3098224103450775\n",
      "Epoch 3415, Loss: 1.5956811904907227, Final Batch Loss: 0.2799135446548462\n",
      "Epoch 3416, Loss: 1.5641665756702423, Final Batch Loss: 0.3016791045665741\n",
      "Epoch 3417, Loss: 1.6515610814094543, Final Batch Loss: 0.3339094817638397\n",
      "Epoch 3418, Loss: 1.5475309193134308, Final Batch Loss: 0.28787270188331604\n",
      "Epoch 3419, Loss: 1.5507743954658508, Final Batch Loss: 0.33960893750190735\n",
      "Epoch 3420, Loss: 1.4243472665548325, Final Batch Loss: 0.21592146158218384\n",
      "Epoch 3421, Loss: 1.5676990151405334, Final Batch Loss: 0.2844361662864685\n",
      "Epoch 3422, Loss: 1.602569192647934, Final Batch Loss: 0.3088386654853821\n",
      "Epoch 3423, Loss: 1.6427634954452515, Final Batch Loss: 0.3047557473182678\n",
      "Epoch 3424, Loss: 1.5593580901622772, Final Batch Loss: 0.326622873544693\n",
      "Epoch 3425, Loss: 1.6344342529773712, Final Batch Loss: 0.31857115030288696\n",
      "Epoch 3426, Loss: 1.523077815771103, Final Batch Loss: 0.3121837079524994\n",
      "Epoch 3427, Loss: 1.6840655207633972, Final Batch Loss: 0.3810594975948334\n",
      "Epoch 3428, Loss: 1.4538305699825287, Final Batch Loss: 0.23263844847679138\n",
      "Epoch 3429, Loss: 1.5535188913345337, Final Batch Loss: 0.2856608033180237\n",
      "Epoch 3430, Loss: 1.8395188748836517, Final Batch Loss: 0.2605641484260559\n",
      "Epoch 3431, Loss: 1.6141389906406403, Final Batch Loss: 0.2802206575870514\n",
      "Epoch 3432, Loss: 1.4601311832666397, Final Batch Loss: 0.22933124005794525\n",
      "Epoch 3433, Loss: 1.5403099060058594, Final Batch Loss: 0.30504748225212097\n",
      "Epoch 3434, Loss: 1.5351401567459106, Final Batch Loss: 0.3714623749256134\n",
      "Epoch 3435, Loss: 1.6292645335197449, Final Batch Loss: 0.28147611021995544\n",
      "Epoch 3436, Loss: 1.4700044691562653, Final Batch Loss: 0.24686111509799957\n",
      "Epoch 3437, Loss: 1.6424830853939056, Final Batch Loss: 0.2051703929901123\n",
      "Epoch 3438, Loss: 1.4582810252904892, Final Batch Loss: 0.3071056008338928\n",
      "Epoch 3439, Loss: 1.526902824640274, Final Batch Loss: 0.24887865781784058\n",
      "Epoch 3440, Loss: 1.6668587923049927, Final Batch Loss: 0.43281272053718567\n",
      "Epoch 3441, Loss: 1.6316061615943909, Final Batch Loss: 0.28583192825317383\n",
      "Epoch 3442, Loss: 1.6289451718330383, Final Batch Loss: 0.326976478099823\n",
      "Epoch 3443, Loss: 1.4346650391817093, Final Batch Loss: 0.2398812621831894\n",
      "Epoch 3444, Loss: 1.5138172060251236, Final Batch Loss: 0.3288044333457947\n",
      "Epoch 3445, Loss: 1.5568078011274338, Final Batch Loss: 0.2874521315097809\n",
      "Epoch 3446, Loss: 1.603196531534195, Final Batch Loss: 0.3176441788673401\n",
      "Epoch 3447, Loss: 1.5944492816925049, Final Batch Loss: 0.3851088285446167\n",
      "Epoch 3448, Loss: 1.4287683218717575, Final Batch Loss: 0.22404903173446655\n",
      "Epoch 3449, Loss: 1.614644706249237, Final Batch Loss: 0.3087266981601715\n",
      "Epoch 3450, Loss: 1.8824287056922913, Final Batch Loss: 0.4682575762271881\n",
      "Epoch 3451, Loss: 1.5638927519321442, Final Batch Loss: 0.3423466682434082\n",
      "Epoch 3452, Loss: 1.6237408220767975, Final Batch Loss: 0.19855067133903503\n",
      "Epoch 3453, Loss: 1.4743773639202118, Final Batch Loss: 0.2613688111305237\n",
      "Epoch 3454, Loss: 1.6534905433654785, Final Batch Loss: 0.3857029378414154\n",
      "Epoch 3455, Loss: 1.5517482608556747, Final Batch Loss: 0.2353324145078659\n",
      "Epoch 3456, Loss: 1.7146281599998474, Final Batch Loss: 0.287897527217865\n",
      "Epoch 3457, Loss: 1.4006849527359009, Final Batch Loss: 0.27781206369400024\n",
      "Epoch 3458, Loss: 1.6944228410720825, Final Batch Loss: 0.42897576093673706\n",
      "Epoch 3459, Loss: 1.6077760308980942, Final Batch Loss: 0.32948359847068787\n",
      "Epoch 3460, Loss: 1.6081548035144806, Final Batch Loss: 0.35740575194358826\n",
      "Epoch 3461, Loss: 1.5129628330469131, Final Batch Loss: 0.2294112890958786\n",
      "Epoch 3462, Loss: 1.7951161414384842, Final Batch Loss: 0.5003001689910889\n",
      "Epoch 3463, Loss: 1.7321068942546844, Final Batch Loss: 0.30016541481018066\n",
      "Epoch 3464, Loss: 1.5500544905662537, Final Batch Loss: 0.31538957357406616\n",
      "Epoch 3465, Loss: 1.6650860011577606, Final Batch Loss: 0.3147290050983429\n",
      "Epoch 3466, Loss: 1.565471351146698, Final Batch Loss: 0.3236232101917267\n",
      "Epoch 3467, Loss: 1.5500507652759552, Final Batch Loss: 0.2902536690235138\n",
      "Epoch 3468, Loss: 1.553885042667389, Final Batch Loss: 0.2699154019355774\n",
      "Epoch 3469, Loss: 1.491582214832306, Final Batch Loss: 0.3154148757457733\n",
      "Epoch 3470, Loss: 1.6152136623859406, Final Batch Loss: 0.3844621777534485\n",
      "Epoch 3471, Loss: 1.4992667138576508, Final Batch Loss: 0.2722393870353699\n",
      "Epoch 3472, Loss: 1.5156936347484589, Final Batch Loss: 0.2817269563674927\n",
      "Epoch 3473, Loss: 1.5935890972614288, Final Batch Loss: 0.25892889499664307\n",
      "Epoch 3474, Loss: 1.7064035832881927, Final Batch Loss: 0.3551819324493408\n",
      "Epoch 3475, Loss: 1.591116338968277, Final Batch Loss: 0.4041591286659241\n",
      "Epoch 3476, Loss: 1.5897805392742157, Final Batch Loss: 0.3173608183860779\n",
      "Epoch 3477, Loss: 1.6282715499401093, Final Batch Loss: 0.3831452429294586\n",
      "Epoch 3478, Loss: 1.490055114030838, Final Batch Loss: 0.2554664611816406\n",
      "Epoch 3479, Loss: 1.6393150687217712, Final Batch Loss: 0.35175737738609314\n",
      "Epoch 3480, Loss: 1.6685831844806671, Final Batch Loss: 0.3953576683998108\n",
      "Epoch 3481, Loss: 1.4948295801877975, Final Batch Loss: 0.3497941493988037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3482, Loss: 1.4813610315322876, Final Batch Loss: 0.299360454082489\n",
      "Epoch 3483, Loss: 1.5692329108715057, Final Batch Loss: 0.2514948546886444\n",
      "Epoch 3484, Loss: 1.738396555185318, Final Batch Loss: 0.3889906406402588\n",
      "Epoch 3485, Loss: 1.7436296343803406, Final Batch Loss: 0.48405155539512634\n",
      "Epoch 3486, Loss: 1.6567274034023285, Final Batch Loss: 0.35481175780296326\n",
      "Epoch 3487, Loss: 1.52171291410923, Final Batch Loss: 0.26647433638572693\n",
      "Epoch 3488, Loss: 1.4621215164661407, Final Batch Loss: 0.2404012680053711\n",
      "Epoch 3489, Loss: 1.6828957200050354, Final Batch Loss: 0.40944144129753113\n",
      "Epoch 3490, Loss: 1.6593144834041595, Final Batch Loss: 0.27636486291885376\n",
      "Epoch 3491, Loss: 1.5366077721118927, Final Batch Loss: 0.20427532494068146\n",
      "Epoch 3492, Loss: 1.5448855012655258, Final Batch Loss: 0.2440228909254074\n",
      "Epoch 3493, Loss: 1.5011253952980042, Final Batch Loss: 0.2812294661998749\n",
      "Epoch 3494, Loss: 1.5212628245353699, Final Batch Loss: 0.34476763010025024\n",
      "Epoch 3495, Loss: 1.5311358273029327, Final Batch Loss: 0.31988903880119324\n",
      "Epoch 3496, Loss: 1.643494427204132, Final Batch Loss: 0.35505086183547974\n",
      "Epoch 3497, Loss: 1.6272314190864563, Final Batch Loss: 0.386076420545578\n",
      "Epoch 3498, Loss: 1.5995721220970154, Final Batch Loss: 0.39522045850753784\n",
      "Epoch 3499, Loss: 1.603820025920868, Final Batch Loss: 0.30541035532951355\n",
      "Epoch 3500, Loss: 1.49448361992836, Final Batch Loss: 0.3480691611766815\n",
      "Epoch 3501, Loss: 1.7090013921260834, Final Batch Loss: 0.31825944781303406\n",
      "Epoch 3502, Loss: 1.736419677734375, Final Batch Loss: 0.3799433410167694\n",
      "Epoch 3503, Loss: 1.6350737512111664, Final Batch Loss: 0.33637678623199463\n",
      "Epoch 3504, Loss: 1.4585075974464417, Final Batch Loss: 0.27643316984176636\n",
      "Epoch 3505, Loss: 1.5307268500328064, Final Batch Loss: 0.331886887550354\n",
      "Epoch 3506, Loss: 1.6182920634746552, Final Batch Loss: 0.28396284580230713\n",
      "Epoch 3507, Loss: 1.5285515189170837, Final Batch Loss: 0.30027589201927185\n",
      "Epoch 3508, Loss: 1.6052130460739136, Final Batch Loss: 0.3140968680381775\n",
      "Epoch 3509, Loss: 1.588013470172882, Final Batch Loss: 0.31416553258895874\n",
      "Epoch 3510, Loss: 1.4261531829833984, Final Batch Loss: 0.2906017303466797\n",
      "Epoch 3511, Loss: 1.5288888216018677, Final Batch Loss: 0.2794697880744934\n",
      "Epoch 3512, Loss: 1.4653874933719635, Final Batch Loss: 0.26633331179618835\n",
      "Epoch 3513, Loss: 1.556838110089302, Final Batch Loss: 0.31214115023612976\n",
      "Epoch 3514, Loss: 1.462247222661972, Final Batch Loss: 0.3341820538043976\n",
      "Epoch 3515, Loss: 1.706633985042572, Final Batch Loss: 0.4308655858039856\n",
      "Epoch 3516, Loss: 1.5428488850593567, Final Batch Loss: 0.27115604281425476\n",
      "Epoch 3517, Loss: 1.6310758590698242, Final Batch Loss: 0.3166177272796631\n",
      "Epoch 3518, Loss: 1.5500672161579132, Final Batch Loss: 0.35886335372924805\n",
      "Epoch 3519, Loss: 1.688626378774643, Final Batch Loss: 0.2881479263305664\n",
      "Epoch 3520, Loss: 1.5282974541187286, Final Batch Loss: 0.32515907287597656\n",
      "Epoch 3521, Loss: 1.6537759006023407, Final Batch Loss: 0.420797199010849\n",
      "Epoch 3522, Loss: 1.5296196341514587, Final Batch Loss: 0.25325724482536316\n",
      "Epoch 3523, Loss: 1.6025190502405167, Final Batch Loss: 0.21422313153743744\n",
      "Epoch 3524, Loss: 1.5865475237369537, Final Batch Loss: 0.2692912518978119\n",
      "Epoch 3525, Loss: 1.6143786311149597, Final Batch Loss: 0.3702065348625183\n",
      "Epoch 3526, Loss: 1.4606861174106598, Final Batch Loss: 0.26666760444641113\n",
      "Epoch 3527, Loss: 1.5782348811626434, Final Batch Loss: 0.2889508903026581\n",
      "Epoch 3528, Loss: 1.6234011203050613, Final Batch Loss: 0.3989602327346802\n",
      "Epoch 3529, Loss: 1.5787691175937653, Final Batch Loss: 0.32318490743637085\n",
      "Epoch 3530, Loss: 1.467211365699768, Final Batch Loss: 0.3243151009082794\n",
      "Epoch 3531, Loss: 1.6313283443450928, Final Batch Loss: 0.36564210057258606\n",
      "Epoch 3532, Loss: 1.5402467548847198, Final Batch Loss: 0.296073317527771\n",
      "Epoch 3533, Loss: 1.533594861626625, Final Batch Loss: 0.2554759979248047\n",
      "Epoch 3534, Loss: 1.6297229528427124, Final Batch Loss: 0.3775678277015686\n",
      "Epoch 3535, Loss: 1.6715841889381409, Final Batch Loss: 0.3155893385410309\n",
      "Epoch 3536, Loss: 1.6698411405086517, Final Batch Loss: 0.3152359426021576\n",
      "Epoch 3537, Loss: 1.5056550353765488, Final Batch Loss: 0.2829265892505646\n",
      "Epoch 3538, Loss: 1.6823812127113342, Final Batch Loss: 0.3362179696559906\n",
      "Epoch 3539, Loss: 1.6245253682136536, Final Batch Loss: 0.327759325504303\n",
      "Epoch 3540, Loss: 1.7165383696556091, Final Batch Loss: 0.28560057282447815\n",
      "Epoch 3541, Loss: 1.5562299191951752, Final Batch Loss: 0.31512632966041565\n",
      "Epoch 3542, Loss: 1.520218163728714, Final Batch Loss: 0.3184819221496582\n",
      "Epoch 3543, Loss: 1.6236258745193481, Final Batch Loss: 0.31006965041160583\n",
      "Epoch 3544, Loss: 1.6188337802886963, Final Batch Loss: 0.4210786819458008\n",
      "Epoch 3545, Loss: 1.6285533010959625, Final Batch Loss: 0.35263943672180176\n",
      "Epoch 3546, Loss: 1.843749225139618, Final Batch Loss: 0.38573557138442993\n",
      "Epoch 3547, Loss: 1.5140048265457153, Final Batch Loss: 0.3184784948825836\n",
      "Epoch 3548, Loss: 1.6989606320858002, Final Batch Loss: 0.4181235730648041\n",
      "Epoch 3549, Loss: 1.6380060315132141, Final Batch Loss: 0.3653044104576111\n",
      "Epoch 3550, Loss: 1.4719907641410828, Final Batch Loss: 0.25173303484916687\n",
      "Epoch 3551, Loss: 1.5991047620773315, Final Batch Loss: 0.3471037447452545\n",
      "Epoch 3552, Loss: 1.5369292795658112, Final Batch Loss: 0.3044477105140686\n",
      "Epoch 3553, Loss: 1.467208907008171, Final Batch Loss: 0.3589988946914673\n",
      "Epoch 3554, Loss: 1.5005314946174622, Final Batch Loss: 0.2857404947280884\n",
      "Epoch 3555, Loss: 1.564212054014206, Final Batch Loss: 0.250097393989563\n",
      "Epoch 3556, Loss: 1.5877306759357452, Final Batch Loss: 0.24335837364196777\n",
      "Epoch 3557, Loss: 1.6145036220550537, Final Batch Loss: 0.2743770182132721\n",
      "Epoch 3558, Loss: 1.6246030628681183, Final Batch Loss: 0.29362091422080994\n",
      "Epoch 3559, Loss: 1.6225553154945374, Final Batch Loss: 0.3074222505092621\n",
      "Epoch 3560, Loss: 1.5751997381448746, Final Batch Loss: 0.24642305076122284\n",
      "Epoch 3561, Loss: 1.5461614727973938, Final Batch Loss: 0.30773457884788513\n",
      "Epoch 3562, Loss: 1.6267021745443344, Final Batch Loss: 0.24057017266750336\n",
      "Epoch 3563, Loss: 1.5100874602794647, Final Batch Loss: 0.23676595091819763\n",
      "Epoch 3564, Loss: 1.460285872220993, Final Batch Loss: 0.25903645157814026\n",
      "Epoch 3565, Loss: 1.600013941526413, Final Batch Loss: 0.32727959752082825\n",
      "Epoch 3566, Loss: 1.64766526222229, Final Batch Loss: 0.274003267288208\n",
      "Epoch 3567, Loss: 1.4354950338602066, Final Batch Loss: 0.30061355233192444\n",
      "Epoch 3568, Loss: 1.494268536567688, Final Batch Loss: 0.31049713492393494\n",
      "Epoch 3569, Loss: 1.496322214603424, Final Batch Loss: 0.2575152814388275\n",
      "Epoch 3570, Loss: 1.5007034540176392, Final Batch Loss: 0.2579898536205292\n",
      "Epoch 3571, Loss: 1.5741958320140839, Final Batch Loss: 0.35309427976608276\n",
      "Epoch 3572, Loss: 1.5305638909339905, Final Batch Loss: 0.29364413022994995\n",
      "Epoch 3573, Loss: 1.5541486293077469, Final Batch Loss: 0.24105127155780792\n",
      "Epoch 3574, Loss: 1.6342675387859344, Final Batch Loss: 0.27661633491516113\n",
      "Epoch 3575, Loss: 1.5575501024723053, Final Batch Loss: 0.2909735143184662\n",
      "Epoch 3576, Loss: 1.6840123534202576, Final Batch Loss: 0.3523159623146057\n",
      "Epoch 3577, Loss: 1.667504370212555, Final Batch Loss: 0.3659891188144684\n",
      "Epoch 3578, Loss: 1.5064431875944138, Final Batch Loss: 0.32182231545448303\n",
      "Epoch 3579, Loss: 1.592128872871399, Final Batch Loss: 0.35450756549835205\n",
      "Epoch 3580, Loss: 1.6704311668872833, Final Batch Loss: 0.429805189371109\n",
      "Epoch 3581, Loss: 1.575175017118454, Final Batch Loss: 0.3367908000946045\n",
      "Epoch 3582, Loss: 1.6423946022987366, Final Batch Loss: 0.27766722440719604\n",
      "Epoch 3583, Loss: 1.5601648688316345, Final Batch Loss: 0.293427973985672\n",
      "Epoch 3584, Loss: 1.5970098376274109, Final Batch Loss: 0.3017532229423523\n",
      "Epoch 3585, Loss: 1.4979261457920074, Final Batch Loss: 0.26638057827949524\n",
      "Epoch 3586, Loss: 1.5580725371837616, Final Batch Loss: 0.35564836859703064\n",
      "Epoch 3587, Loss: 1.505795195698738, Final Batch Loss: 0.2323598712682724\n",
      "Epoch 3588, Loss: 1.4530717432498932, Final Batch Loss: 0.2886015474796295\n",
      "Epoch 3589, Loss: 1.4841546714305878, Final Batch Loss: 0.27701878547668457\n",
      "Epoch 3590, Loss: 1.6764334589242935, Final Batch Loss: 0.4139682948589325\n",
      "Epoch 3591, Loss: 1.5483686923980713, Final Batch Loss: 0.3349873423576355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3592, Loss: 1.4967796504497528, Final Batch Loss: 0.300846666097641\n",
      "Epoch 3593, Loss: 1.5614593625068665, Final Batch Loss: 0.29492494463920593\n",
      "Epoch 3594, Loss: 1.6043915152549744, Final Batch Loss: 0.2995476722717285\n",
      "Epoch 3595, Loss: 1.725402444601059, Final Batch Loss: 0.316102534532547\n",
      "Epoch 3596, Loss: 1.5287320911884308, Final Batch Loss: 0.2719367742538452\n",
      "Epoch 3597, Loss: 1.4552587121725082, Final Batch Loss: 0.23891185224056244\n",
      "Epoch 3598, Loss: 1.6880854666233063, Final Batch Loss: 0.36516478657722473\n",
      "Epoch 3599, Loss: 1.7300321459770203, Final Batch Loss: 0.37510743737220764\n",
      "Epoch 3600, Loss: 1.5803479552268982, Final Batch Loss: 0.3218282461166382\n",
      "Epoch 3601, Loss: 1.6117781698703766, Final Batch Loss: 0.36808639764785767\n",
      "Epoch 3602, Loss: 1.6520804166793823, Final Batch Loss: 0.3514680862426758\n",
      "Epoch 3603, Loss: 1.5092848092317581, Final Batch Loss: 0.23298345506191254\n",
      "Epoch 3604, Loss: 1.482355922460556, Final Batch Loss: 0.2404327690601349\n",
      "Epoch 3605, Loss: 1.548571616411209, Final Batch Loss: 0.18338903784751892\n",
      "Epoch 3606, Loss: 1.4510670900344849, Final Batch Loss: 0.2672634422779083\n",
      "Epoch 3607, Loss: 1.6742561161518097, Final Batch Loss: 0.317988783121109\n",
      "Epoch 3608, Loss: 1.5202774703502655, Final Batch Loss: 0.2430088222026825\n",
      "Epoch 3609, Loss: 1.5692960321903229, Final Batch Loss: 0.2767164707183838\n",
      "Epoch 3610, Loss: 1.4156078845262527, Final Batch Loss: 0.22567085921764374\n",
      "Epoch 3611, Loss: 1.455060064792633, Final Batch Loss: 0.28181368112564087\n",
      "Epoch 3612, Loss: 1.645116239786148, Final Batch Loss: 0.3536270260810852\n",
      "Epoch 3613, Loss: 1.5673910677433014, Final Batch Loss: 0.3929249048233032\n",
      "Epoch 3614, Loss: 1.5891274809837341, Final Batch Loss: 0.39021459221839905\n",
      "Epoch 3615, Loss: 1.6552886962890625, Final Batch Loss: 0.38687652349472046\n",
      "Epoch 3616, Loss: 1.3624011129140854, Final Batch Loss: 0.23261839151382446\n",
      "Epoch 3617, Loss: 1.5322065651416779, Final Batch Loss: 0.28090912103652954\n",
      "Epoch 3618, Loss: 1.548855572938919, Final Batch Loss: 0.3210071623325348\n",
      "Epoch 3619, Loss: 1.4485275149345398, Final Batch Loss: 0.2679188847541809\n",
      "Epoch 3620, Loss: 1.440819188952446, Final Batch Loss: 0.22108696401119232\n",
      "Epoch 3621, Loss: 1.6205102801322937, Final Batch Loss: 0.41195765137672424\n",
      "Epoch 3622, Loss: 1.4530033022165298, Final Batch Loss: 0.2425030916929245\n",
      "Epoch 3623, Loss: 1.4680896699428558, Final Batch Loss: 0.3282271921634674\n",
      "Epoch 3624, Loss: 1.5447075366973877, Final Batch Loss: 0.26807618141174316\n",
      "Epoch 3625, Loss: 1.487698495388031, Final Batch Loss: 0.2553654909133911\n",
      "Epoch 3626, Loss: 1.5279015600681305, Final Batch Loss: 0.2998131215572357\n",
      "Epoch 3627, Loss: 1.4736033976078033, Final Batch Loss: 0.21185964345932007\n",
      "Epoch 3628, Loss: 1.5456486642360687, Final Batch Loss: 0.3158343732357025\n",
      "Epoch 3629, Loss: 1.425887405872345, Final Batch Loss: 0.1743687093257904\n",
      "Epoch 3630, Loss: 1.554734706878662, Final Batch Loss: 0.4088428020477295\n",
      "Epoch 3631, Loss: 1.5649597644805908, Final Batch Loss: 0.3635714650154114\n",
      "Epoch 3632, Loss: 1.6537642180919647, Final Batch Loss: 0.2840672433376312\n",
      "Epoch 3633, Loss: 1.401203989982605, Final Batch Loss: 0.2758505940437317\n",
      "Epoch 3634, Loss: 1.5661443918943405, Final Batch Loss: 0.21127884089946747\n",
      "Epoch 3635, Loss: 1.5822140276432037, Final Batch Loss: 0.2993628978729248\n",
      "Epoch 3636, Loss: 1.423204556107521, Final Batch Loss: 0.23658137023448944\n",
      "Epoch 3637, Loss: 1.6358809173107147, Final Batch Loss: 0.29975613951683044\n",
      "Epoch 3638, Loss: 1.5177006721496582, Final Batch Loss: 0.2560957670211792\n",
      "Epoch 3639, Loss: 1.4248489439487457, Final Batch Loss: 0.3104060888290405\n",
      "Epoch 3640, Loss: 1.6303853690624237, Final Batch Loss: 0.28439509868621826\n",
      "Epoch 3641, Loss: 1.522072821855545, Final Batch Loss: 0.25402700901031494\n",
      "Epoch 3642, Loss: 1.635843425989151, Final Batch Loss: 0.2905173897743225\n",
      "Epoch 3643, Loss: 1.5784242749214172, Final Batch Loss: 0.29903507232666016\n",
      "Epoch 3644, Loss: 1.5146740972995758, Final Batch Loss: 0.2711026966571808\n",
      "Epoch 3645, Loss: 1.6287956833839417, Final Batch Loss: 0.34537333250045776\n",
      "Epoch 3646, Loss: 1.489306092262268, Final Batch Loss: 0.35326769948005676\n",
      "Epoch 3647, Loss: 1.5390833616256714, Final Batch Loss: 0.2960348427295685\n",
      "Epoch 3648, Loss: 1.6408560574054718, Final Batch Loss: 0.23885327577590942\n",
      "Epoch 3649, Loss: 1.6310758292675018, Final Batch Loss: 0.39304566383361816\n",
      "Epoch 3650, Loss: 1.7141790688037872, Final Batch Loss: 0.40451690554618835\n",
      "Epoch 3651, Loss: 1.6303028464317322, Final Batch Loss: 0.3008360266685486\n",
      "Epoch 3652, Loss: 1.5956048667430878, Final Batch Loss: 0.40234264731407166\n",
      "Epoch 3653, Loss: 1.52593532204628, Final Batch Loss: 0.3377026915550232\n",
      "Epoch 3654, Loss: 1.45853590965271, Final Batch Loss: 0.2893555462360382\n",
      "Epoch 3655, Loss: 1.5639091432094574, Final Batch Loss: 0.3443759083747864\n",
      "Epoch 3656, Loss: 1.5383552312850952, Final Batch Loss: 0.46164730191230774\n",
      "Epoch 3657, Loss: 1.6077586710453033, Final Batch Loss: 0.22570198774337769\n",
      "Epoch 3658, Loss: 1.6323256194591522, Final Batch Loss: 0.3312346637248993\n",
      "Epoch 3659, Loss: 1.447390466928482, Final Batch Loss: 0.28717389702796936\n",
      "Epoch 3660, Loss: 1.4192507714033127, Final Batch Loss: 0.22416742146015167\n",
      "Epoch 3661, Loss: 1.5596387833356857, Final Batch Loss: 0.2144247442483902\n",
      "Epoch 3662, Loss: 1.58368980884552, Final Batch Loss: 0.3152802586555481\n",
      "Epoch 3663, Loss: 1.5660858750343323, Final Batch Loss: 0.37050145864486694\n",
      "Epoch 3664, Loss: 1.4638353139162064, Final Batch Loss: 0.2450537532567978\n",
      "Epoch 3665, Loss: 1.4899219274520874, Final Batch Loss: 0.28900596499443054\n",
      "Epoch 3666, Loss: 1.5063482522964478, Final Batch Loss: 0.3164370357990265\n",
      "Epoch 3667, Loss: 1.4764783680438995, Final Batch Loss: 0.25387805700302124\n",
      "Epoch 3668, Loss: 1.425451621413231, Final Batch Loss: 0.24796690046787262\n",
      "Epoch 3669, Loss: 1.470195859670639, Final Batch Loss: 0.30030298233032227\n",
      "Epoch 3670, Loss: 1.7385042607784271, Final Batch Loss: 0.30968040227890015\n",
      "Epoch 3671, Loss: 1.6356427669525146, Final Batch Loss: 0.32775014638900757\n",
      "Epoch 3672, Loss: 1.472936987876892, Final Batch Loss: 0.28947126865386963\n",
      "Epoch 3673, Loss: 1.4105691313743591, Final Batch Loss: 0.2722547650337219\n",
      "Epoch 3674, Loss: 1.5389774143695831, Final Batch Loss: 0.32313230633735657\n",
      "Epoch 3675, Loss: 1.6998127698898315, Final Batch Loss: 0.39058780670166016\n",
      "Epoch 3676, Loss: 1.5950294435024261, Final Batch Loss: 0.2864011526107788\n",
      "Epoch 3677, Loss: 1.5918853282928467, Final Batch Loss: 0.3560801148414612\n",
      "Epoch 3678, Loss: 1.6169770061969757, Final Batch Loss: 0.36568957567214966\n",
      "Epoch 3679, Loss: 1.5518961399793625, Final Batch Loss: 0.24589262902736664\n",
      "Epoch 3680, Loss: 1.6152821481227875, Final Batch Loss: 0.4043598175048828\n",
      "Epoch 3681, Loss: 1.6689894497394562, Final Batch Loss: 0.31322789192199707\n",
      "Epoch 3682, Loss: 1.5532331317663193, Final Batch Loss: 0.3318529725074768\n",
      "Epoch 3683, Loss: 1.6288590729236603, Final Batch Loss: 0.36286279559135437\n",
      "Epoch 3684, Loss: 1.5588978677988052, Final Batch Loss: 0.3912293612957001\n",
      "Epoch 3685, Loss: 1.5812081396579742, Final Batch Loss: 0.27080440521240234\n",
      "Epoch 3686, Loss: 1.556969165802002, Final Batch Loss: 0.2951515018939972\n",
      "Epoch 3687, Loss: 1.582124650478363, Final Batch Loss: 0.3269694745540619\n",
      "Epoch 3688, Loss: 1.5789627879858017, Final Batch Loss: 0.2458428144454956\n",
      "Epoch 3689, Loss: 1.6233090162277222, Final Batch Loss: 0.28973299264907837\n",
      "Epoch 3690, Loss: 1.5899884700775146, Final Batch Loss: 0.31059277057647705\n",
      "Epoch 3691, Loss: 1.5427587926387787, Final Batch Loss: 0.3626613914966583\n",
      "Epoch 3692, Loss: 1.4280258417129517, Final Batch Loss: 0.25717246532440186\n",
      "Epoch 3693, Loss: 1.648260623216629, Final Batch Loss: 0.333599328994751\n",
      "Epoch 3694, Loss: 1.5859728753566742, Final Batch Loss: 0.37982627749443054\n",
      "Epoch 3695, Loss: 1.606442630290985, Final Batch Loss: 0.3299434185028076\n",
      "Epoch 3696, Loss: 1.524250477552414, Final Batch Loss: 0.2910958528518677\n",
      "Epoch 3697, Loss: 1.60121288895607, Final Batch Loss: 0.292977511882782\n",
      "Epoch 3698, Loss: 1.6194249093532562, Final Batch Loss: 0.3689923882484436\n",
      "Epoch 3699, Loss: 1.4259694814682007, Final Batch Loss: 0.30394452810287476\n",
      "Epoch 3700, Loss: 1.6738384366035461, Final Batch Loss: 0.33020755648612976\n",
      "Epoch 3701, Loss: 1.6743999123573303, Final Batch Loss: 0.3498007357120514\n",
      "Epoch 3702, Loss: 1.6632477045059204, Final Batch Loss: 0.39201584458351135\n",
      "Epoch 3703, Loss: 1.4763121157884598, Final Batch Loss: 0.25138169527053833\n",
      "Epoch 3704, Loss: 1.597425401210785, Final Batch Loss: 0.3870365023612976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3705, Loss: 1.6593584418296814, Final Batch Loss: 0.327534943819046\n",
      "Epoch 3706, Loss: 1.46715347468853, Final Batch Loss: 0.32176077365875244\n",
      "Epoch 3707, Loss: 1.51967291533947, Final Batch Loss: 0.2271679788827896\n",
      "Epoch 3708, Loss: 1.3718535900115967, Final Batch Loss: 0.21165506541728973\n",
      "Epoch 3709, Loss: 1.530362531542778, Final Batch Loss: 0.3739669620990753\n",
      "Epoch 3710, Loss: 1.7318506240844727, Final Batch Loss: 0.3408469259738922\n",
      "Epoch 3711, Loss: 1.4396581798791885, Final Batch Loss: 0.24878792464733124\n",
      "Epoch 3712, Loss: 1.591511458158493, Final Batch Loss: 0.31403282284736633\n",
      "Epoch 3713, Loss: 1.6585854887962341, Final Batch Loss: 0.40158769488334656\n",
      "Epoch 3714, Loss: 1.5790113508701324, Final Batch Loss: 0.3376046419143677\n",
      "Epoch 3715, Loss: 1.698000729084015, Final Batch Loss: 0.313865065574646\n",
      "Epoch 3716, Loss: 1.6216937899589539, Final Batch Loss: 0.34969767928123474\n",
      "Epoch 3717, Loss: 1.6213619709014893, Final Batch Loss: 0.3331626057624817\n",
      "Epoch 3718, Loss: 1.5319742411375046, Final Batch Loss: 0.2367779165506363\n",
      "Epoch 3719, Loss: 1.5686872005462646, Final Batch Loss: 0.271076500415802\n",
      "Epoch 3720, Loss: 1.6411842107772827, Final Batch Loss: 0.39562538266181946\n",
      "Epoch 3721, Loss: 1.4391362965106964, Final Batch Loss: 0.45807090401649475\n",
      "Epoch 3722, Loss: 1.5289424359798431, Final Batch Loss: 0.3127087354660034\n",
      "Epoch 3723, Loss: 1.893659770488739, Final Batch Loss: 0.2878677248954773\n",
      "Epoch 3724, Loss: 1.5433950126171112, Final Batch Loss: 0.2608787417411804\n",
      "Epoch 3725, Loss: 1.5701192319393158, Final Batch Loss: 0.293653279542923\n",
      "Epoch 3726, Loss: 1.4541982412338257, Final Batch Loss: 0.2820669114589691\n",
      "Epoch 3727, Loss: 1.5110838413238525, Final Batch Loss: 0.2614758312702179\n",
      "Epoch 3728, Loss: 1.5649592578411102, Final Batch Loss: 0.31486648321151733\n",
      "Epoch 3729, Loss: 1.5046346783638, Final Batch Loss: 0.26167142391204834\n",
      "Epoch 3730, Loss: 1.5810281932353973, Final Batch Loss: 0.32758697867393494\n",
      "Epoch 3731, Loss: 1.4388344436883926, Final Batch Loss: 0.24771378934383392\n",
      "Epoch 3732, Loss: 1.5974918007850647, Final Batch Loss: 0.25206512212753296\n",
      "Epoch 3733, Loss: 1.4828345775604248, Final Batch Loss: 0.3307424485683441\n",
      "Epoch 3734, Loss: 1.366655394434929, Final Batch Loss: 0.23665545880794525\n",
      "Epoch 3735, Loss: 1.4192905724048615, Final Batch Loss: 0.2884708642959595\n",
      "Epoch 3736, Loss: 1.4696279466152191, Final Batch Loss: 0.29915788769721985\n",
      "Epoch 3737, Loss: 1.5515790581703186, Final Batch Loss: 0.37722861766815186\n",
      "Epoch 3738, Loss: 1.6337344646453857, Final Batch Loss: 0.3471270799636841\n",
      "Epoch 3739, Loss: 1.6907836496829987, Final Batch Loss: 0.37029704451560974\n",
      "Epoch 3740, Loss: 1.5259028375148773, Final Batch Loss: 0.3349078297615051\n",
      "Epoch 3741, Loss: 1.7515084743499756, Final Batch Loss: 0.3374153971672058\n",
      "Epoch 3742, Loss: 1.5586755275726318, Final Batch Loss: 0.24545809626579285\n",
      "Epoch 3743, Loss: 1.7346124947071075, Final Batch Loss: 0.35340723395347595\n",
      "Epoch 3744, Loss: 1.6493339240550995, Final Batch Loss: 0.40535715222358704\n",
      "Epoch 3745, Loss: 1.6279211640357971, Final Batch Loss: 0.28774550557136536\n",
      "Epoch 3746, Loss: 1.4333900958299637, Final Batch Loss: 0.2913668155670166\n",
      "Epoch 3747, Loss: 1.4842990338802338, Final Batch Loss: 0.25717252492904663\n",
      "Epoch 3748, Loss: 1.4066157937049866, Final Batch Loss: 0.2764045298099518\n",
      "Epoch 3749, Loss: 1.5517865717411041, Final Batch Loss: 0.29860806465148926\n",
      "Epoch 3750, Loss: 1.8230972290039062, Final Batch Loss: 0.5570157766342163\n",
      "Epoch 3751, Loss: 1.5363849997520447, Final Batch Loss: 0.32391825318336487\n",
      "Epoch 3752, Loss: 1.586883693933487, Final Batch Loss: 0.3248853087425232\n",
      "Epoch 3753, Loss: 1.3160262405872345, Final Batch Loss: 0.2639657258987427\n",
      "Epoch 3754, Loss: 1.462987944483757, Final Batch Loss: 0.22526271641254425\n",
      "Epoch 3755, Loss: 1.6308174133300781, Final Batch Loss: 0.3960932493209839\n",
      "Epoch 3756, Loss: 1.4781430959701538, Final Batch Loss: 0.26877063512802124\n",
      "Epoch 3757, Loss: 1.5202579200267792, Final Batch Loss: 0.26589447259902954\n",
      "Epoch 3758, Loss: 1.7967539131641388, Final Batch Loss: 0.4236711859703064\n",
      "Epoch 3759, Loss: 1.503429263830185, Final Batch Loss: 0.3460528552532196\n",
      "Epoch 3760, Loss: 1.4617728888988495, Final Batch Loss: 0.28722521662712097\n",
      "Epoch 3761, Loss: 1.4389822483062744, Final Batch Loss: 0.2928007245063782\n",
      "Epoch 3762, Loss: 1.6158539652824402, Final Batch Loss: 0.3266313076019287\n",
      "Epoch 3763, Loss: 1.5269542634487152, Final Batch Loss: 0.3071717619895935\n",
      "Epoch 3764, Loss: 1.6245339214801788, Final Batch Loss: 0.3734987676143646\n",
      "Epoch 3765, Loss: 1.5145409554243088, Final Batch Loss: 0.4704851806163788\n",
      "Epoch 3766, Loss: 1.5392499268054962, Final Batch Loss: 0.2541256248950958\n",
      "Epoch 3767, Loss: 1.4421632438898087, Final Batch Loss: 0.21857671439647675\n",
      "Epoch 3768, Loss: 1.579211950302124, Final Batch Loss: 0.276460200548172\n",
      "Epoch 3769, Loss: 1.7499913275241852, Final Batch Loss: 0.5201533436775208\n",
      "Epoch 3770, Loss: 1.3834041059017181, Final Batch Loss: 0.24677923321723938\n",
      "Epoch 3771, Loss: 1.466237723827362, Final Batch Loss: 0.254645437002182\n",
      "Epoch 3772, Loss: 1.5460385978221893, Final Batch Loss: 0.27667614817619324\n",
      "Epoch 3773, Loss: 1.4670912027359009, Final Batch Loss: 0.2528577446937561\n",
      "Epoch 3774, Loss: 1.512255698442459, Final Batch Loss: 0.27922123670578003\n",
      "Epoch 3775, Loss: 1.5053438544273376, Final Batch Loss: 0.24128705263137817\n",
      "Epoch 3776, Loss: 1.5596782714128494, Final Batch Loss: 0.37825483083724976\n",
      "Epoch 3777, Loss: 1.564422994852066, Final Batch Loss: 0.3917105495929718\n",
      "Epoch 3778, Loss: 1.4158417582511902, Final Batch Loss: 0.26586511731147766\n",
      "Epoch 3779, Loss: 1.6653141975402832, Final Batch Loss: 0.3379688858985901\n",
      "Epoch 3780, Loss: 1.4998511373996735, Final Batch Loss: 0.33585456013679504\n",
      "Epoch 3781, Loss: 1.3152940422296524, Final Batch Loss: 0.22638066112995148\n",
      "Epoch 3782, Loss: 1.5347829163074493, Final Batch Loss: 0.3281831741333008\n",
      "Epoch 3783, Loss: 1.6159234941005707, Final Batch Loss: 0.3697405457496643\n",
      "Epoch 3784, Loss: 1.5494555234909058, Final Batch Loss: 0.366889089345932\n",
      "Epoch 3785, Loss: 1.5927974879741669, Final Batch Loss: 0.34037235379219055\n",
      "Epoch 3786, Loss: 1.4906362742185593, Final Batch Loss: 0.19527743756771088\n",
      "Epoch 3787, Loss: 1.374923750758171, Final Batch Loss: 0.2681387662887573\n",
      "Epoch 3788, Loss: 1.5285367965698242, Final Batch Loss: 0.32605576515197754\n",
      "Epoch 3789, Loss: 1.429978683590889, Final Batch Loss: 0.21983467042446136\n",
      "Epoch 3790, Loss: 1.4846756160259247, Final Batch Loss: 0.3230679929256439\n",
      "Epoch 3791, Loss: 1.6659909784793854, Final Batch Loss: 0.40275922417640686\n",
      "Epoch 3792, Loss: 1.4658063352108002, Final Batch Loss: 0.28947174549102783\n",
      "Epoch 3793, Loss: 1.681103229522705, Final Batch Loss: 0.2817312479019165\n",
      "Epoch 3794, Loss: 1.5223488509654999, Final Batch Loss: 0.30556389689445496\n",
      "Epoch 3795, Loss: 1.4105122983455658, Final Batch Loss: 0.21237561106681824\n",
      "Epoch 3796, Loss: 1.600211888551712, Final Batch Loss: 0.4111976623535156\n",
      "Epoch 3797, Loss: 1.6103397905826569, Final Batch Loss: 0.3076360821723938\n",
      "Epoch 3798, Loss: 1.4620911180973053, Final Batch Loss: 0.2881890535354614\n",
      "Epoch 3799, Loss: 1.4855987429618835, Final Batch Loss: 0.3038215637207031\n",
      "Epoch 3800, Loss: 1.4303266406059265, Final Batch Loss: 0.2664444148540497\n",
      "Epoch 3801, Loss: 1.6137566268444061, Final Batch Loss: 0.3489916920661926\n",
      "Epoch 3802, Loss: 1.5922144949436188, Final Batch Loss: 0.33224546909332275\n",
      "Epoch 3803, Loss: 1.6743510961532593, Final Batch Loss: 0.38031741976737976\n",
      "Epoch 3804, Loss: 1.5178110301494598, Final Batch Loss: 0.3784424662590027\n",
      "Epoch 3805, Loss: 1.500531554222107, Final Batch Loss: 0.26756566762924194\n",
      "Epoch 3806, Loss: 1.4833708107471466, Final Batch Loss: 0.30168843269348145\n",
      "Epoch 3807, Loss: 1.5052468329668045, Final Batch Loss: 0.32603055238723755\n",
      "Epoch 3808, Loss: 1.4713233411312103, Final Batch Loss: 0.3040110766887665\n",
      "Epoch 3809, Loss: 1.4895898550748825, Final Batch Loss: 0.2908782958984375\n",
      "Epoch 3810, Loss: 1.5144003927707672, Final Batch Loss: 0.3210565149784088\n",
      "Epoch 3811, Loss: 1.4766065925359726, Final Batch Loss: 0.30734071135520935\n",
      "Epoch 3812, Loss: 1.704184651374817, Final Batch Loss: 0.2884800136089325\n",
      "Epoch 3813, Loss: 1.608286440372467, Final Batch Loss: 0.4100722372531891\n",
      "Epoch 3814, Loss: 1.5165212452411652, Final Batch Loss: 0.27596211433410645\n",
      "Epoch 3815, Loss: 1.5593677461147308, Final Batch Loss: 0.20611432194709778\n",
      "Epoch 3816, Loss: 1.5081954300403595, Final Batch Loss: 0.37502121925354004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3817, Loss: 1.4695958197116852, Final Batch Loss: 0.2702805697917938\n",
      "Epoch 3818, Loss: 1.489798828959465, Final Batch Loss: 0.20670683681964874\n",
      "Epoch 3819, Loss: 1.6146671175956726, Final Batch Loss: 0.3842162489891052\n",
      "Epoch 3820, Loss: 1.6550325155258179, Final Batch Loss: 0.3964512050151825\n",
      "Epoch 3821, Loss: 1.515188753604889, Final Batch Loss: 0.3930819034576416\n",
      "Epoch 3822, Loss: 1.478892594575882, Final Batch Loss: 0.35471004247665405\n",
      "Epoch 3823, Loss: 1.410387098789215, Final Batch Loss: 0.2802331745624542\n",
      "Epoch 3824, Loss: 1.5121774971485138, Final Batch Loss: 0.2926713228225708\n",
      "Epoch 3825, Loss: 1.3685427159070969, Final Batch Loss: 0.23754136264324188\n",
      "Epoch 3826, Loss: 1.5363963842391968, Final Batch Loss: 0.3873811364173889\n",
      "Epoch 3827, Loss: 1.445409581065178, Final Batch Loss: 0.322224885225296\n",
      "Epoch 3828, Loss: 1.4322337955236435, Final Batch Loss: 0.22674068808555603\n",
      "Epoch 3829, Loss: 1.5251276195049286, Final Batch Loss: 0.3069564700126648\n",
      "Epoch 3830, Loss: 1.7370950877666473, Final Batch Loss: 0.45534735918045044\n",
      "Epoch 3831, Loss: 1.5939268469810486, Final Batch Loss: 0.35435131192207336\n",
      "Epoch 3832, Loss: 1.466394081711769, Final Batch Loss: 0.3525661826133728\n",
      "Epoch 3833, Loss: 1.5499410331249237, Final Batch Loss: 0.30762168765068054\n",
      "Epoch 3834, Loss: 1.48167285323143, Final Batch Loss: 0.2764050364494324\n",
      "Epoch 3835, Loss: 1.4692824184894562, Final Batch Loss: 0.29231399297714233\n",
      "Epoch 3836, Loss: 1.4105162620544434, Final Batch Loss: 0.20901475846767426\n",
      "Epoch 3837, Loss: 1.4710839688777924, Final Batch Loss: 0.3184678852558136\n",
      "Epoch 3838, Loss: 1.5084053725004196, Final Batch Loss: 0.36838576197624207\n",
      "Epoch 3839, Loss: 1.3740641325712204, Final Batch Loss: 0.28830474615097046\n",
      "Epoch 3840, Loss: 1.534249171614647, Final Batch Loss: 0.23571880161762238\n",
      "Epoch 3841, Loss: 1.5583336353302002, Final Batch Loss: 0.34503859281539917\n",
      "Epoch 3842, Loss: 1.4422224760055542, Final Batch Loss: 0.26379090547561646\n",
      "Epoch 3843, Loss: 1.455631509423256, Final Batch Loss: 0.3207613527774811\n",
      "Epoch 3844, Loss: 1.5714440643787384, Final Batch Loss: 0.33065229654312134\n",
      "Epoch 3845, Loss: 1.5688054114580154, Final Batch Loss: 0.34726932644844055\n",
      "Epoch 3846, Loss: 1.416626587510109, Final Batch Loss: 0.28341105580329895\n",
      "Epoch 3847, Loss: 1.3687403202056885, Final Batch Loss: 0.2984651029109955\n",
      "Epoch 3848, Loss: 1.5092020630836487, Final Batch Loss: 0.29771366715431213\n",
      "Epoch 3849, Loss: 1.5383585691452026, Final Batch Loss: 0.3203827738761902\n",
      "Epoch 3850, Loss: 1.4755652844905853, Final Batch Loss: 0.31192949414253235\n",
      "Epoch 3851, Loss: 1.4619741141796112, Final Batch Loss: 0.3105607330799103\n",
      "Epoch 3852, Loss: 1.6823486387729645, Final Batch Loss: 0.3473893702030182\n",
      "Epoch 3853, Loss: 1.5456633567810059, Final Batch Loss: 0.32469555735588074\n",
      "Epoch 3854, Loss: 1.3970839828252792, Final Batch Loss: 0.32581833004951477\n",
      "Epoch 3855, Loss: 1.5044539868831635, Final Batch Loss: 0.2982778549194336\n",
      "Epoch 3856, Loss: 1.3470343202352524, Final Batch Loss: 0.18029196560382843\n",
      "Epoch 3857, Loss: 1.6576252579689026, Final Batch Loss: 0.4361169934272766\n",
      "Epoch 3858, Loss: 1.4034796953201294, Final Batch Loss: 0.36221787333488464\n",
      "Epoch 3859, Loss: 1.6076699495315552, Final Batch Loss: 0.3087595999240875\n",
      "Epoch 3860, Loss: 1.374360203742981, Final Batch Loss: 0.22569380700588226\n",
      "Epoch 3861, Loss: 1.6186657100915909, Final Batch Loss: 0.4270364046096802\n",
      "Epoch 3862, Loss: 1.3818113803863525, Final Batch Loss: 0.2937866449356079\n",
      "Epoch 3863, Loss: 1.5406676530838013, Final Batch Loss: 0.31868889927864075\n",
      "Epoch 3864, Loss: 1.3921761810779572, Final Batch Loss: 0.2770322561264038\n",
      "Epoch 3865, Loss: 1.6461617946624756, Final Batch Loss: 0.38875141739845276\n",
      "Epoch 3866, Loss: 1.4816358983516693, Final Batch Loss: 0.2526419162750244\n",
      "Epoch 3867, Loss: 1.452204018831253, Final Batch Loss: 0.2254452407360077\n",
      "Epoch 3868, Loss: 1.4274034351110458, Final Batch Loss: 0.3204912841320038\n",
      "Epoch 3869, Loss: 1.6766538619995117, Final Batch Loss: 0.3476942777633667\n",
      "Epoch 3870, Loss: 1.3493786007165909, Final Batch Loss: 0.2706163823604584\n",
      "Epoch 3871, Loss: 1.3724099397659302, Final Batch Loss: 0.26431334018707275\n",
      "Epoch 3872, Loss: 1.6019737422466278, Final Batch Loss: 0.3564068377017975\n",
      "Epoch 3873, Loss: 1.5800028443336487, Final Batch Loss: 0.30184632539749146\n",
      "Epoch 3874, Loss: 1.544636756181717, Final Batch Loss: 0.25607720017433167\n",
      "Epoch 3875, Loss: 1.5735209584236145, Final Batch Loss: 0.32361042499542236\n",
      "Epoch 3876, Loss: 1.6105276942253113, Final Batch Loss: 0.30727419257164\n",
      "Epoch 3877, Loss: 1.4592767357826233, Final Batch Loss: 0.31476157903671265\n",
      "Epoch 3878, Loss: 1.4632033705711365, Final Batch Loss: 0.275884747505188\n",
      "Epoch 3879, Loss: 1.3580055832862854, Final Batch Loss: 0.21534320712089539\n",
      "Epoch 3880, Loss: 1.58584925532341, Final Batch Loss: 0.3845743238925934\n",
      "Epoch 3881, Loss: 1.436047002673149, Final Batch Loss: 0.23343461751937866\n",
      "Epoch 3882, Loss: 1.4697335511446, Final Batch Loss: 0.4010736048221588\n",
      "Epoch 3883, Loss: 1.5287841856479645, Final Batch Loss: 0.26685631275177\n",
      "Epoch 3884, Loss: 1.6051762700080872, Final Batch Loss: 0.2782173156738281\n",
      "Epoch 3885, Loss: 1.5737827569246292, Final Batch Loss: 0.2862924635410309\n",
      "Epoch 3886, Loss: 1.5374248027801514, Final Batch Loss: 0.3017383813858032\n",
      "Epoch 3887, Loss: 1.7449593842029572, Final Batch Loss: 0.39546674489974976\n",
      "Epoch 3888, Loss: 1.6361463963985443, Final Batch Loss: 0.3047071397304535\n",
      "Epoch 3889, Loss: 1.4233092218637466, Final Batch Loss: 0.36569517850875854\n",
      "Epoch 3890, Loss: 1.6423465311527252, Final Batch Loss: 0.4312882721424103\n",
      "Epoch 3891, Loss: 1.3614543080329895, Final Batch Loss: 0.1829339563846588\n",
      "Epoch 3892, Loss: 1.4970166385173798, Final Batch Loss: 0.3357827663421631\n",
      "Epoch 3893, Loss: 1.3146971762180328, Final Batch Loss: 0.30529236793518066\n",
      "Epoch 3894, Loss: 1.4941908121109009, Final Batch Loss: 0.3508264124393463\n",
      "Epoch 3895, Loss: 1.4868579804897308, Final Batch Loss: 0.3258536756038666\n",
      "Epoch 3896, Loss: 1.4726612120866776, Final Batch Loss: 0.2496686428785324\n",
      "Epoch 3897, Loss: 1.5510457754135132, Final Batch Loss: 0.2310160994529724\n",
      "Epoch 3898, Loss: 1.4483327567577362, Final Batch Loss: 0.26795142889022827\n",
      "Epoch 3899, Loss: 1.575624406337738, Final Batch Loss: 0.26761099696159363\n",
      "Epoch 3900, Loss: 1.673628181219101, Final Batch Loss: 0.26035642623901367\n",
      "Epoch 3901, Loss: 1.554893970489502, Final Batch Loss: 0.3921240568161011\n",
      "Epoch 3902, Loss: 1.5078973472118378, Final Batch Loss: 0.31761181354522705\n",
      "Epoch 3903, Loss: 1.5825998187065125, Final Batch Loss: 0.2823103666305542\n",
      "Epoch 3904, Loss: 1.5188955217599869, Final Batch Loss: 0.3444434404373169\n",
      "Epoch 3905, Loss: 1.4435555934906006, Final Batch Loss: 0.3139105439186096\n",
      "Epoch 3906, Loss: 1.5168453454971313, Final Batch Loss: 0.3253750801086426\n",
      "Epoch 3907, Loss: 1.4771367311477661, Final Batch Loss: 0.25523093342781067\n",
      "Epoch 3908, Loss: 1.6076718270778656, Final Batch Loss: 0.3189690113067627\n",
      "Epoch 3909, Loss: 1.5430721640586853, Final Batch Loss: 0.32725006341934204\n",
      "Epoch 3910, Loss: 1.6386516094207764, Final Batch Loss: 0.386902391910553\n",
      "Epoch 3911, Loss: 1.4727036654949188, Final Batch Loss: 0.24602669477462769\n",
      "Epoch 3912, Loss: 1.5191587209701538, Final Batch Loss: 0.3006419837474823\n",
      "Epoch 3913, Loss: 1.4852120876312256, Final Batch Loss: 0.32392480969429016\n",
      "Epoch 3914, Loss: 1.5908736288547516, Final Batch Loss: 0.33724313974380493\n",
      "Epoch 3915, Loss: 1.6674679219722748, Final Batch Loss: 0.36406397819519043\n",
      "Epoch 3916, Loss: 1.5922558903694153, Final Batch Loss: 0.32743772864341736\n",
      "Epoch 3917, Loss: 1.622140571475029, Final Batch Loss: 0.3784000873565674\n",
      "Epoch 3918, Loss: 1.5281618386507034, Final Batch Loss: 0.3583757281303406\n",
      "Epoch 3919, Loss: 1.4188320636749268, Final Batch Loss: 0.2773459255695343\n",
      "Epoch 3920, Loss: 1.435736894607544, Final Batch Loss: 0.17435765266418457\n",
      "Epoch 3921, Loss: 1.4596951603889465, Final Batch Loss: 0.2739860713481903\n",
      "Epoch 3922, Loss: 1.5883242189884186, Final Batch Loss: 0.30895116925239563\n",
      "Epoch 3923, Loss: 1.7380722165107727, Final Batch Loss: 0.2776108682155609\n",
      "Epoch 3924, Loss: 1.4970720708370209, Final Batch Loss: 0.2759314775466919\n",
      "Epoch 3925, Loss: 1.561594009399414, Final Batch Loss: 0.287510484457016\n",
      "Epoch 3926, Loss: 1.6292436122894287, Final Batch Loss: 0.27196842432022095\n",
      "Epoch 3927, Loss: 1.4829127490520477, Final Batch Loss: 0.3175494372844696\n",
      "Epoch 3928, Loss: 1.349299132823944, Final Batch Loss: 0.26039475202560425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3929, Loss: 1.4665203392505646, Final Batch Loss: 0.3072323799133301\n",
      "Epoch 3930, Loss: 1.4654997289180756, Final Batch Loss: 0.3142618238925934\n",
      "Epoch 3931, Loss: 1.3962927758693695, Final Batch Loss: 0.29738521575927734\n",
      "Epoch 3932, Loss: 1.6302238702774048, Final Batch Loss: 0.43322980403900146\n",
      "Epoch 3933, Loss: 1.4449429512023926, Final Batch Loss: 0.237463116645813\n",
      "Epoch 3934, Loss: 1.4531871974468231, Final Batch Loss: 0.32540789246559143\n",
      "Epoch 3935, Loss: 1.5491947382688522, Final Batch Loss: 0.36978912353515625\n",
      "Epoch 3936, Loss: 1.5629469156265259, Final Batch Loss: 0.3154425323009491\n",
      "Epoch 3937, Loss: 1.4909878075122833, Final Batch Loss: 0.31516319513320923\n",
      "Epoch 3938, Loss: 1.4362351894378662, Final Batch Loss: 0.2800636291503906\n",
      "Epoch 3939, Loss: 1.4559137225151062, Final Batch Loss: 0.29670125246047974\n",
      "Epoch 3940, Loss: 1.521271213889122, Final Batch Loss: 0.24396373331546783\n",
      "Epoch 3941, Loss: 1.5602033883333206, Final Batch Loss: 0.34684500098228455\n",
      "Epoch 3942, Loss: 1.509086236357689, Final Batch Loss: 0.3733556568622589\n",
      "Epoch 3943, Loss: 1.468809813261032, Final Batch Loss: 0.29384082555770874\n",
      "Epoch 3944, Loss: 1.4130717366933823, Final Batch Loss: 0.2627950608730316\n",
      "Epoch 3945, Loss: 1.4059102833271027, Final Batch Loss: 0.24128857254981995\n",
      "Epoch 3946, Loss: 1.5366222858428955, Final Batch Loss: 0.22743645310401917\n",
      "Epoch 3947, Loss: 1.4944956451654434, Final Batch Loss: 0.2475663274526596\n",
      "Epoch 3948, Loss: 1.5149554908275604, Final Batch Loss: 0.2959515154361725\n",
      "Epoch 3949, Loss: 1.4410079717636108, Final Batch Loss: 0.2640724182128906\n",
      "Epoch 3950, Loss: 1.586437612771988, Final Batch Loss: 0.3129521906375885\n",
      "Epoch 3951, Loss: 1.8283669650554657, Final Batch Loss: 0.5451623797416687\n",
      "Epoch 3952, Loss: 1.5271127820014954, Final Batch Loss: 0.3018818795681\n",
      "Epoch 3953, Loss: 1.5096645653247833, Final Batch Loss: 0.2440360188484192\n",
      "Epoch 3954, Loss: 1.509899228811264, Final Batch Loss: 0.28918465971946716\n",
      "Epoch 3955, Loss: 1.4939895868301392, Final Batch Loss: 0.28426849842071533\n",
      "Epoch 3956, Loss: 1.444940984249115, Final Batch Loss: 0.3403462767601013\n",
      "Epoch 3957, Loss: 1.4431603252887726, Final Batch Loss: 0.3036319613456726\n",
      "Epoch 3958, Loss: 1.4306062757968903, Final Batch Loss: 0.27207157015800476\n",
      "Epoch 3959, Loss: 1.5809358656406403, Final Batch Loss: 0.30498915910720825\n",
      "Epoch 3960, Loss: 1.6135189831256866, Final Batch Loss: 0.37946778535842896\n",
      "Epoch 3961, Loss: 1.4294974505901337, Final Batch Loss: 0.35045352578163147\n",
      "Epoch 3962, Loss: 1.5237110257148743, Final Batch Loss: 0.27982112765312195\n",
      "Epoch 3963, Loss: 1.586901307106018, Final Batch Loss: 0.38946858048439026\n",
      "Epoch 3964, Loss: 1.507331371307373, Final Batch Loss: 0.3733932673931122\n",
      "Epoch 3965, Loss: 1.6018874496221542, Final Batch Loss: 0.34263670444488525\n",
      "Epoch 3966, Loss: 1.4984960556030273, Final Batch Loss: 0.30396631360054016\n",
      "Epoch 3967, Loss: 1.5141843557357788, Final Batch Loss: 0.2369375228881836\n",
      "Epoch 3968, Loss: 1.4710614085197449, Final Batch Loss: 0.31832027435302734\n",
      "Epoch 3969, Loss: 1.4719959050416946, Final Batch Loss: 0.2639141380786896\n",
      "Epoch 3970, Loss: 1.5088385343551636, Final Batch Loss: 0.3055424392223358\n",
      "Epoch 3971, Loss: 1.4878595173358917, Final Batch Loss: 0.25993287563323975\n",
      "Epoch 3972, Loss: 1.3328238278627396, Final Batch Loss: 0.2422056794166565\n",
      "Epoch 3973, Loss: 1.4140584468841553, Final Batch Loss: 0.23561570048332214\n",
      "Epoch 3974, Loss: 1.4619702994823456, Final Batch Loss: 0.27054041624069214\n",
      "Epoch 3975, Loss: 1.5700271874666214, Final Batch Loss: 0.3876180052757263\n",
      "Epoch 3976, Loss: 1.4516517966985703, Final Batch Loss: 0.17788411676883698\n",
      "Epoch 3977, Loss: 1.3073547333478928, Final Batch Loss: 0.18164627254009247\n",
      "Epoch 3978, Loss: 1.4825754910707474, Final Batch Loss: 0.34648141264915466\n",
      "Epoch 3979, Loss: 1.5147346556186676, Final Batch Loss: 0.4156585931777954\n",
      "Epoch 3980, Loss: 1.5475295782089233, Final Batch Loss: 0.33608174324035645\n",
      "Epoch 3981, Loss: 1.3555611670017242, Final Batch Loss: 0.23401600122451782\n",
      "Epoch 3982, Loss: 1.4098943620920181, Final Batch Loss: 0.3193661868572235\n",
      "Epoch 3983, Loss: 1.6064969301223755, Final Batch Loss: 0.3695365786552429\n",
      "Epoch 3984, Loss: 1.4773881435394287, Final Batch Loss: 0.3223359286785126\n",
      "Epoch 3985, Loss: 1.4148965328931808, Final Batch Loss: 0.25115856528282166\n",
      "Epoch 3986, Loss: 1.4947689175605774, Final Batch Loss: 0.29234886169433594\n",
      "Epoch 3987, Loss: 1.4551323503255844, Final Batch Loss: 0.4211089313030243\n",
      "Epoch 3988, Loss: 1.4001569896936417, Final Batch Loss: 0.23637603223323822\n",
      "Epoch 3989, Loss: 1.4336480349302292, Final Batch Loss: 0.2857150137424469\n",
      "Epoch 3990, Loss: 1.3791950196027756, Final Batch Loss: 0.26830464601516724\n",
      "Epoch 3991, Loss: 1.4394334852695465, Final Batch Loss: 0.33521172404289246\n",
      "Epoch 3992, Loss: 1.4963313937187195, Final Batch Loss: 0.32718217372894287\n",
      "Epoch 3993, Loss: 1.4268538355827332, Final Batch Loss: 0.26506558060646057\n",
      "Epoch 3994, Loss: 1.4107151329517365, Final Batch Loss: 0.27309390902519226\n",
      "Epoch 3995, Loss: 1.6309157311916351, Final Batch Loss: 0.2866026759147644\n",
      "Epoch 3996, Loss: 1.4154241234064102, Final Batch Loss: 0.21322007477283478\n",
      "Epoch 3997, Loss: 1.53421550989151, Final Batch Loss: 0.29800403118133545\n",
      "Epoch 3998, Loss: 1.3886402249336243, Final Batch Loss: 0.2837400734424591\n",
      "Epoch 3999, Loss: 1.3752935826778412, Final Batch Loss: 0.2451634705066681\n",
      "Epoch 4000, Loss: 1.5420633852481842, Final Batch Loss: 0.31054675579071045\n",
      "Epoch 4001, Loss: 1.7424917817115784, Final Batch Loss: 0.4106515347957611\n",
      "Epoch 4002, Loss: 1.4953143298625946, Final Batch Loss: 0.32604295015335083\n",
      "Epoch 4003, Loss: 1.4666065275669098, Final Batch Loss: 0.2711174786090851\n",
      "Epoch 4004, Loss: 1.483261227607727, Final Batch Loss: 0.32061854004859924\n",
      "Epoch 4005, Loss: 1.5081487894058228, Final Batch Loss: 0.41027629375457764\n",
      "Epoch 4006, Loss: 1.4939300119876862, Final Batch Loss: 0.3168103098869324\n",
      "Epoch 4007, Loss: 1.53941810131073, Final Batch Loss: 0.25970593094825745\n",
      "Epoch 4008, Loss: 1.4333987832069397, Final Batch Loss: 0.331288605928421\n",
      "Epoch 4009, Loss: 1.66275355219841, Final Batch Loss: 0.4044058620929718\n",
      "Epoch 4010, Loss: 1.5444189310073853, Final Batch Loss: 0.3113442659378052\n",
      "Epoch 4011, Loss: 1.4210369288921356, Final Batch Loss: 0.1952592432498932\n",
      "Epoch 4012, Loss: 1.503607839345932, Final Batch Loss: 0.2813093662261963\n",
      "Epoch 4013, Loss: 1.4339178204536438, Final Batch Loss: 0.21607837080955505\n",
      "Epoch 4014, Loss: 1.7032028585672379, Final Batch Loss: 0.3800942003726959\n",
      "Epoch 4015, Loss: 1.6300565302371979, Final Batch Loss: 0.5081056356430054\n",
      "Epoch 4016, Loss: 1.5169560760259628, Final Batch Loss: 0.21747033298015594\n",
      "Epoch 4017, Loss: 1.4779827743768692, Final Batch Loss: 0.2399582415819168\n",
      "Epoch 4018, Loss: 1.495478793978691, Final Batch Loss: 0.29714056849479675\n",
      "Epoch 4019, Loss: 1.6283042132854462, Final Batch Loss: 0.408878892660141\n",
      "Epoch 4020, Loss: 1.6155645549297333, Final Batch Loss: 0.34448888897895813\n",
      "Epoch 4021, Loss: 1.5889756679534912, Final Batch Loss: 0.28279969096183777\n",
      "Epoch 4022, Loss: 1.4305742383003235, Final Batch Loss: 0.2714897692203522\n",
      "Epoch 4023, Loss: 1.5100651979446411, Final Batch Loss: 0.25363343954086304\n",
      "Epoch 4024, Loss: 1.4141780585050583, Final Batch Loss: 0.2615368664264679\n",
      "Epoch 4025, Loss: 1.4871968924999237, Final Batch Loss: 0.3666006922721863\n",
      "Epoch 4026, Loss: 1.5451434701681137, Final Batch Loss: 0.21182295680046082\n",
      "Epoch 4027, Loss: 1.4076316356658936, Final Batch Loss: 0.26094338297843933\n",
      "Epoch 4028, Loss: 1.402555674314499, Final Batch Loss: 0.2256263941526413\n",
      "Epoch 4029, Loss: 1.4915020763874054, Final Batch Loss: 0.3070214092731476\n",
      "Epoch 4030, Loss: 1.5575628280639648, Final Batch Loss: 0.3171537518501282\n",
      "Epoch 4031, Loss: 1.4217758774757385, Final Batch Loss: 0.3024037182331085\n",
      "Epoch 4032, Loss: 1.4981674700975418, Final Batch Loss: 0.38164934515953064\n",
      "Epoch 4033, Loss: 1.508057862520218, Final Batch Loss: 0.2961810529232025\n",
      "Epoch 4034, Loss: 1.576718583703041, Final Batch Loss: 0.23530273139476776\n",
      "Epoch 4035, Loss: 1.3867101073265076, Final Batch Loss: 0.3239031434059143\n",
      "Epoch 4036, Loss: 1.4815404266119003, Final Batch Loss: 0.353677362203598\n",
      "Epoch 4037, Loss: 1.4689586162567139, Final Batch Loss: 0.2876412570476532\n",
      "Epoch 4038, Loss: 1.525570198893547, Final Batch Loss: 0.3646988570690155\n",
      "Epoch 4039, Loss: 1.4575296938419342, Final Batch Loss: 0.267456591129303\n",
      "Epoch 4040, Loss: 1.2663329392671585, Final Batch Loss: 0.17705614864826202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4041, Loss: 1.3963001519441605, Final Batch Loss: 0.3239140212535858\n",
      "Epoch 4042, Loss: 1.344373270869255, Final Batch Loss: 0.22679206728935242\n",
      "Epoch 4043, Loss: 1.4605814218521118, Final Batch Loss: 0.29111534357070923\n",
      "Epoch 4044, Loss: 1.4322810769081116, Final Batch Loss: 0.324491947889328\n",
      "Epoch 4045, Loss: 1.4166663587093353, Final Batch Loss: 0.26281705498695374\n",
      "Epoch 4046, Loss: 1.4522770941257477, Final Batch Loss: 0.23456108570098877\n",
      "Epoch 4047, Loss: 1.3948712199926376, Final Batch Loss: 0.24966983497142792\n",
      "Epoch 4048, Loss: 1.5193645507097244, Final Batch Loss: 0.258714497089386\n",
      "Epoch 4049, Loss: 1.4599331617355347, Final Batch Loss: 0.2924581468105316\n",
      "Epoch 4050, Loss: 1.4928743243217468, Final Batch Loss: 0.31817132234573364\n",
      "Epoch 4051, Loss: 1.428985819220543, Final Batch Loss: 0.2691524028778076\n",
      "Epoch 4052, Loss: 1.5556171536445618, Final Batch Loss: 0.4241744875907898\n",
      "Epoch 4053, Loss: 1.5674458146095276, Final Batch Loss: 0.3554772734642029\n",
      "Epoch 4054, Loss: 1.4361956864595413, Final Batch Loss: 0.3031918704509735\n",
      "Epoch 4055, Loss: 1.5465353429317474, Final Batch Loss: 0.2669561803340912\n",
      "Epoch 4056, Loss: 1.5240470319986343, Final Batch Loss: 0.40141382813453674\n",
      "Epoch 4057, Loss: 1.5004789233207703, Final Batch Loss: 0.27355387806892395\n",
      "Epoch 4058, Loss: 1.4902210533618927, Final Batch Loss: 0.3349284529685974\n",
      "Epoch 4059, Loss: 1.609335571527481, Final Batch Loss: 0.28018951416015625\n",
      "Epoch 4060, Loss: 1.4209191501140594, Final Batch Loss: 0.36015626788139343\n",
      "Epoch 4061, Loss: 1.5109162628650665, Final Batch Loss: 0.27579858899116516\n",
      "Epoch 4062, Loss: 1.3530294746160507, Final Batch Loss: 0.17576910555362701\n",
      "Epoch 4063, Loss: 1.460827186703682, Final Batch Loss: 0.3104674816131592\n",
      "Epoch 4064, Loss: 1.501207321882248, Final Batch Loss: 0.33161550760269165\n",
      "Epoch 4065, Loss: 1.5674595534801483, Final Batch Loss: 0.36274099349975586\n",
      "Epoch 4066, Loss: 1.495614930987358, Final Batch Loss: 0.32557111978530884\n",
      "Epoch 4067, Loss: 1.5996553599834442, Final Batch Loss: 0.3104369640350342\n",
      "Epoch 4068, Loss: 1.4428242146968842, Final Batch Loss: 0.2651858627796173\n",
      "Epoch 4069, Loss: 1.3868986815214157, Final Batch Loss: 0.2784419655799866\n",
      "Epoch 4070, Loss: 1.5062533468008041, Final Batch Loss: 0.31777825951576233\n",
      "Epoch 4071, Loss: 1.4580549448728561, Final Batch Loss: 0.21426405012607574\n",
      "Epoch 4072, Loss: 1.3523205667734146, Final Batch Loss: 0.28827789425849915\n",
      "Epoch 4073, Loss: 1.5266735255718231, Final Batch Loss: 0.40490585565567017\n",
      "Epoch 4074, Loss: 1.7013528943061829, Final Batch Loss: 0.4754326045513153\n",
      "Epoch 4075, Loss: 1.39000803232193, Final Batch Loss: 0.20598533749580383\n",
      "Epoch 4076, Loss: 1.4946739375591278, Final Batch Loss: 0.2920546531677246\n",
      "Epoch 4077, Loss: 1.4618895500898361, Final Batch Loss: 0.27547669410705566\n",
      "Epoch 4078, Loss: 1.3504442125558853, Final Batch Loss: 0.2549455761909485\n",
      "Epoch 4079, Loss: 1.5319639146327972, Final Batch Loss: 0.3086758255958557\n",
      "Epoch 4080, Loss: 1.4064250588417053, Final Batch Loss: 0.3236716389656067\n",
      "Epoch 4081, Loss: 1.3272931575775146, Final Batch Loss: 0.26605507731437683\n",
      "Epoch 4082, Loss: 1.647160828113556, Final Batch Loss: 0.4298771917819977\n",
      "Epoch 4083, Loss: 1.34343583881855, Final Batch Loss: 0.24334637820720673\n",
      "Epoch 4084, Loss: 1.3511884659528732, Final Batch Loss: 0.22211001813411713\n",
      "Epoch 4085, Loss: 1.63279590010643, Final Batch Loss: 0.3674635887145996\n",
      "Epoch 4086, Loss: 1.419572576880455, Final Batch Loss: 0.23959894478321075\n",
      "Epoch 4087, Loss: 1.4543874263763428, Final Batch Loss: 0.30636146664619446\n",
      "Epoch 4088, Loss: 1.5400047600269318, Final Batch Loss: 0.2517922818660736\n",
      "Epoch 4089, Loss: 1.4049892276525497, Final Batch Loss: 0.23220278322696686\n",
      "Epoch 4090, Loss: 1.4073631167411804, Final Batch Loss: 0.2605666518211365\n",
      "Epoch 4091, Loss: 1.4984975904226303, Final Batch Loss: 0.2013372927904129\n",
      "Epoch 4092, Loss: 1.4794017374515533, Final Batch Loss: 0.2981240749359131\n",
      "Epoch 4093, Loss: 1.5321599692106247, Final Batch Loss: 0.3186061978340149\n",
      "Epoch 4094, Loss: 1.4961171597242355, Final Batch Loss: 0.2970447540283203\n",
      "Epoch 4095, Loss: 1.39900504052639, Final Batch Loss: 0.2552645802497864\n",
      "Epoch 4096, Loss: 1.3920349925756454, Final Batch Loss: 0.2558799088001251\n",
      "Epoch 4097, Loss: 1.4980877935886383, Final Batch Loss: 0.26638123393058777\n",
      "Epoch 4098, Loss: 1.3331064879894257, Final Batch Loss: 0.2934305667877197\n",
      "Epoch 4099, Loss: 1.5206674635410309, Final Batch Loss: 0.2359747588634491\n",
      "Epoch 4100, Loss: 1.641417235136032, Final Batch Loss: 0.290631502866745\n",
      "Epoch 4101, Loss: 1.5120906829833984, Final Batch Loss: 0.21537819504737854\n",
      "Epoch 4102, Loss: 1.40632264316082, Final Batch Loss: 0.21234078705310822\n",
      "Epoch 4103, Loss: 1.5196765661239624, Final Batch Loss: 0.2986268103122711\n",
      "Epoch 4104, Loss: 1.5466657280921936, Final Batch Loss: 0.26845645904541016\n",
      "Epoch 4105, Loss: 1.4268994480371475, Final Batch Loss: 0.2291443645954132\n",
      "Epoch 4106, Loss: 1.3410892486572266, Final Batch Loss: 0.22267690300941467\n",
      "Epoch 4107, Loss: 1.4337677657604218, Final Batch Loss: 0.28697529435157776\n",
      "Epoch 4108, Loss: 1.425864726305008, Final Batch Loss: 0.3525596559047699\n",
      "Epoch 4109, Loss: 1.4620900750160217, Final Batch Loss: 0.3258703947067261\n",
      "Epoch 4110, Loss: 1.6126232147216797, Final Batch Loss: 0.4024296998977661\n",
      "Epoch 4111, Loss: 1.5010717511177063, Final Batch Loss: 0.2876684367656708\n",
      "Epoch 4112, Loss: 1.5430776178836823, Final Batch Loss: 0.3886249363422394\n",
      "Epoch 4113, Loss: 1.4501604288816452, Final Batch Loss: 0.3300741910934448\n",
      "Epoch 4114, Loss: 1.452918991446495, Final Batch Loss: 0.3910835087299347\n",
      "Epoch 4115, Loss: 1.408274918794632, Final Batch Loss: 0.3303101360797882\n",
      "Epoch 4116, Loss: 1.3880223631858826, Final Batch Loss: 0.17737236618995667\n",
      "Epoch 4117, Loss: 1.6360501945018768, Final Batch Loss: 0.45860612392425537\n",
      "Epoch 4118, Loss: 1.532028079032898, Final Batch Loss: 0.32443442940711975\n",
      "Epoch 4119, Loss: 1.3392374068498611, Final Batch Loss: 0.29051363468170166\n",
      "Epoch 4120, Loss: 1.3981331139802933, Final Batch Loss: 0.3217543959617615\n",
      "Epoch 4121, Loss: 1.305108681321144, Final Batch Loss: 0.2595929503440857\n",
      "Epoch 4122, Loss: 1.6164678931236267, Final Batch Loss: 0.2757647931575775\n",
      "Epoch 4123, Loss: 1.4659614562988281, Final Batch Loss: 0.30059555172920227\n",
      "Epoch 4124, Loss: 1.3409504443407059, Final Batch Loss: 0.25307294726371765\n",
      "Epoch 4125, Loss: 1.472589910030365, Final Batch Loss: 0.29324281215667725\n",
      "Epoch 4126, Loss: 1.469339281320572, Final Batch Loss: 0.2892052233219147\n",
      "Epoch 4127, Loss: 1.5949328541755676, Final Batch Loss: 0.2197360396385193\n",
      "Epoch 4128, Loss: 1.4477896690368652, Final Batch Loss: 0.2891939878463745\n",
      "Epoch 4129, Loss: 1.330224633216858, Final Batch Loss: 0.26750385761260986\n",
      "Epoch 4130, Loss: 1.5136797428131104, Final Batch Loss: 0.3363351821899414\n",
      "Epoch 4131, Loss: 1.4760509729385376, Final Batch Loss: 0.26368042826652527\n",
      "Epoch 4132, Loss: 1.3270399868488312, Final Batch Loss: 0.33703622221946716\n",
      "Epoch 4133, Loss: 1.3416011482477188, Final Batch Loss: 0.2800419330596924\n",
      "Epoch 4134, Loss: 1.5366125106811523, Final Batch Loss: 0.31889376044273376\n",
      "Epoch 4135, Loss: 1.5423881709575653, Final Batch Loss: 0.2691064476966858\n",
      "Epoch 4136, Loss: 1.4442956298589706, Final Batch Loss: 0.3402954936027527\n",
      "Epoch 4137, Loss: 1.5196565389633179, Final Batch Loss: 0.33234694600105286\n",
      "Epoch 4138, Loss: 1.5079402923583984, Final Batch Loss: 0.3294815719127655\n",
      "Epoch 4139, Loss: 1.440944567322731, Final Batch Loss: 0.21041958034038544\n",
      "Epoch 4140, Loss: 1.392067402601242, Final Batch Loss: 0.3165600895881653\n",
      "Epoch 4141, Loss: 1.391801357269287, Final Batch Loss: 0.27549248933792114\n",
      "Epoch 4142, Loss: 1.4663451313972473, Final Batch Loss: 0.3485158681869507\n",
      "Epoch 4143, Loss: 1.311065062880516, Final Batch Loss: 0.24412770569324493\n",
      "Epoch 4144, Loss: 1.421747773885727, Final Batch Loss: 0.2879287600517273\n",
      "Epoch 4145, Loss: 1.3978115320205688, Final Batch Loss: 0.2543613016605377\n",
      "Epoch 4146, Loss: 1.5320574343204498, Final Batch Loss: 0.35103899240493774\n",
      "Epoch 4147, Loss: 1.3352133333683014, Final Batch Loss: 0.2985720932483673\n",
      "Epoch 4148, Loss: 1.4375752210617065, Final Batch Loss: 0.3102293014526367\n",
      "Epoch 4149, Loss: 1.3362807482481003, Final Batch Loss: 0.2672385573387146\n",
      "Epoch 4150, Loss: 1.4331770539283752, Final Batch Loss: 0.278777152299881\n",
      "Epoch 4151, Loss: 1.4072738140821457, Final Batch Loss: 0.2166200876235962\n",
      "Epoch 4152, Loss: 1.4351151287555695, Final Batch Loss: 0.24867135286331177\n",
      "Epoch 4153, Loss: 1.408358782529831, Final Batch Loss: 0.26819759607315063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4154, Loss: 1.6280384063720703, Final Batch Loss: 0.4362789988517761\n",
      "Epoch 4155, Loss: 1.4474734365940094, Final Batch Loss: 0.262094110250473\n",
      "Epoch 4156, Loss: 1.4800252318382263, Final Batch Loss: 0.3123398423194885\n",
      "Epoch 4157, Loss: 1.594704031944275, Final Batch Loss: 0.4484189748764038\n",
      "Epoch 4158, Loss: 1.5482403934001923, Final Batch Loss: 0.3747189939022064\n",
      "Epoch 4159, Loss: 1.5579789578914642, Final Batch Loss: 0.375963419675827\n",
      "Epoch 4160, Loss: 1.4876789301633835, Final Batch Loss: 0.24714045226573944\n",
      "Epoch 4161, Loss: 1.5363661497831345, Final Batch Loss: 0.3346991240978241\n",
      "Epoch 4162, Loss: 1.5725654065608978, Final Batch Loss: 0.4097212851047516\n",
      "Epoch 4163, Loss: 1.5596983879804611, Final Batch Loss: 0.3638424873352051\n",
      "Epoch 4164, Loss: 1.501922219991684, Final Batch Loss: 0.2543283998966217\n",
      "Epoch 4165, Loss: 1.4310914129018784, Final Batch Loss: 0.31374382972717285\n",
      "Epoch 4166, Loss: 1.3474183231592178, Final Batch Loss: 0.23370353877544403\n",
      "Epoch 4167, Loss: 1.5085780620574951, Final Batch Loss: 0.2969774603843689\n",
      "Epoch 4168, Loss: 1.4022133499383926, Final Batch Loss: 0.2626967132091522\n",
      "Epoch 4169, Loss: 1.4883211702108383, Final Batch Loss: 0.4172864854335785\n",
      "Epoch 4170, Loss: 1.572897493839264, Final Batch Loss: 0.3264002799987793\n",
      "Epoch 4171, Loss: 1.421151340007782, Final Batch Loss: 0.29181140661239624\n",
      "Epoch 4172, Loss: 1.4108858108520508, Final Batch Loss: 0.2672189176082611\n",
      "Epoch 4173, Loss: 1.3978560119867325, Final Batch Loss: 0.3869427740573883\n",
      "Epoch 4174, Loss: 1.527819886803627, Final Batch Loss: 0.28182849287986755\n",
      "Epoch 4175, Loss: 1.3450826555490494, Final Batch Loss: 0.21160265803337097\n",
      "Epoch 4176, Loss: 1.442976102232933, Final Batch Loss: 0.28891444206237793\n",
      "Epoch 4177, Loss: 1.517966315150261, Final Batch Loss: 0.234141543507576\n",
      "Epoch 4178, Loss: 1.3835908621549606, Final Batch Loss: 0.24133025109767914\n",
      "Epoch 4179, Loss: 1.487673506140709, Final Batch Loss: 0.39401644468307495\n",
      "Epoch 4180, Loss: 1.5489978343248367, Final Batch Loss: 0.30560359358787537\n",
      "Epoch 4181, Loss: 1.4466632008552551, Final Batch Loss: 0.1973884403705597\n",
      "Epoch 4182, Loss: 1.5160509198904037, Final Batch Loss: 0.3019239008426666\n",
      "Epoch 4183, Loss: 1.5336773693561554, Final Batch Loss: 0.30033421516418457\n",
      "Epoch 4184, Loss: 1.5218170583248138, Final Batch Loss: 0.32529115676879883\n",
      "Epoch 4185, Loss: 1.4021739959716797, Final Batch Loss: 0.25366783142089844\n",
      "Epoch 4186, Loss: 1.5161878764629364, Final Batch Loss: 0.4356710612773895\n",
      "Epoch 4187, Loss: 1.3674170523881912, Final Batch Loss: 0.2576267719268799\n",
      "Epoch 4188, Loss: 1.351770430803299, Final Batch Loss: 0.2765936553478241\n",
      "Epoch 4189, Loss: 1.4959907829761505, Final Batch Loss: 0.2981474697589874\n",
      "Epoch 4190, Loss: 1.4897496849298477, Final Batch Loss: 0.26864567399024963\n",
      "Epoch 4191, Loss: 1.5672311186790466, Final Batch Loss: 0.3407761752605438\n",
      "Epoch 4192, Loss: 1.3787077069282532, Final Batch Loss: 0.23427066206932068\n",
      "Epoch 4193, Loss: 1.4528012573719025, Final Batch Loss: 0.2624009847640991\n",
      "Epoch 4194, Loss: 1.6191199123859406, Final Batch Loss: 0.35527944564819336\n",
      "Epoch 4195, Loss: 1.3723572492599487, Final Batch Loss: 0.2858293056488037\n",
      "Epoch 4196, Loss: 1.4361643940210342, Final Batch Loss: 0.26595091819763184\n",
      "Epoch 4197, Loss: 1.4088610708713531, Final Batch Loss: 0.28769463300704956\n",
      "Epoch 4198, Loss: 1.558532953262329, Final Batch Loss: 0.37025490403175354\n",
      "Epoch 4199, Loss: 1.4513012170791626, Final Batch Loss: 0.30242571234703064\n",
      "Epoch 4200, Loss: 1.5861890763044357, Final Batch Loss: 0.22523638606071472\n",
      "Epoch 4201, Loss: 1.3978810906410217, Final Batch Loss: 0.2578467130661011\n",
      "Epoch 4202, Loss: 1.4095633178949356, Final Batch Loss: 0.3055979013442993\n",
      "Epoch 4203, Loss: 1.364799290895462, Final Batch Loss: 0.27046236395835876\n",
      "Epoch 4204, Loss: 1.4325069785118103, Final Batch Loss: 0.20894861221313477\n",
      "Epoch 4205, Loss: 1.57812038064003, Final Batch Loss: 0.33524203300476074\n",
      "Epoch 4206, Loss: 1.3574011325836182, Final Batch Loss: 0.19052636623382568\n",
      "Epoch 4207, Loss: 1.6872383952140808, Final Batch Loss: 0.38104376196861267\n",
      "Epoch 4208, Loss: 1.3502124547958374, Final Batch Loss: 0.2976648211479187\n",
      "Epoch 4209, Loss: 1.4458976686000824, Final Batch Loss: 0.36207088828086853\n",
      "Epoch 4210, Loss: 1.5909714996814728, Final Batch Loss: 0.25359371304512024\n",
      "Epoch 4211, Loss: 1.481270045042038, Final Batch Loss: 0.286705881357193\n",
      "Epoch 4212, Loss: 1.5059422850608826, Final Batch Loss: 0.25458601117134094\n",
      "Epoch 4213, Loss: 1.404411718249321, Final Batch Loss: 0.23589108884334564\n",
      "Epoch 4214, Loss: 1.3883400708436966, Final Batch Loss: 0.27766916155815125\n",
      "Epoch 4215, Loss: 1.3346514403820038, Final Batch Loss: 0.28449201583862305\n",
      "Epoch 4216, Loss: 1.371007189154625, Final Batch Loss: 0.28908100724220276\n",
      "Epoch 4217, Loss: 1.4430681020021439, Final Batch Loss: 0.3381384313106537\n",
      "Epoch 4218, Loss: 1.4221791625022888, Final Batch Loss: 0.35069260001182556\n",
      "Epoch 4219, Loss: 1.4834545105695724, Final Batch Loss: 0.3617985248565674\n",
      "Epoch 4220, Loss: 1.3873518854379654, Final Batch Loss: 0.3386092483997345\n",
      "Epoch 4221, Loss: 1.4091128259897232, Final Batch Loss: 0.34352898597717285\n",
      "Epoch 4222, Loss: 1.5079928189516068, Final Batch Loss: 0.2807687520980835\n",
      "Epoch 4223, Loss: 1.34668530523777, Final Batch Loss: 0.226241797208786\n",
      "Epoch 4224, Loss: 1.4346056878566742, Final Batch Loss: 0.34292837977409363\n",
      "Epoch 4225, Loss: 1.7057586014270782, Final Batch Loss: 0.3365248441696167\n",
      "Epoch 4226, Loss: 1.475103884935379, Final Batch Loss: 0.36622151732444763\n",
      "Epoch 4227, Loss: 1.6640740036964417, Final Batch Loss: 0.26317915320396423\n",
      "Epoch 4228, Loss: 1.534732848405838, Final Batch Loss: 0.339095801115036\n",
      "Epoch 4229, Loss: 1.4512440264225006, Final Batch Loss: 0.31671473383903503\n",
      "Epoch 4230, Loss: 1.364697590470314, Final Batch Loss: 0.30016326904296875\n",
      "Epoch 4231, Loss: 1.3691997677087784, Final Batch Loss: 0.32249319553375244\n",
      "Epoch 4232, Loss: 1.405388429760933, Final Batch Loss: 0.28137195110321045\n",
      "Epoch 4233, Loss: 1.3363863676786423, Final Batch Loss: 0.2708742022514343\n",
      "Epoch 4234, Loss: 1.4897115528583527, Final Batch Loss: 0.2705994248390198\n",
      "Epoch 4235, Loss: 1.4994439631700516, Final Batch Loss: 0.29993560910224915\n",
      "Epoch 4236, Loss: 1.4663277119398117, Final Batch Loss: 0.28678861260414124\n",
      "Epoch 4237, Loss: 1.3909290581941605, Final Batch Loss: 0.1940280944108963\n",
      "Epoch 4238, Loss: 1.4917560517787933, Final Batch Loss: 0.3345019519329071\n",
      "Epoch 4239, Loss: 1.368363931775093, Final Batch Loss: 0.2792210578918457\n",
      "Epoch 4240, Loss: 1.4920045882463455, Final Batch Loss: 0.3030588626861572\n",
      "Epoch 4241, Loss: 1.3271284997463226, Final Batch Loss: 0.31326425075531006\n",
      "Epoch 4242, Loss: 1.3149401247501373, Final Batch Loss: 0.24831795692443848\n",
      "Epoch 4243, Loss: 1.4441222846508026, Final Batch Loss: 0.2784058451652527\n",
      "Epoch 4244, Loss: 1.6028572618961334, Final Batch Loss: 0.45834675431251526\n",
      "Epoch 4245, Loss: 1.5039108991622925, Final Batch Loss: 0.3220422863960266\n",
      "Epoch 4246, Loss: 1.6418146789073944, Final Batch Loss: 0.321828693151474\n",
      "Epoch 4247, Loss: 1.3398379534482956, Final Batch Loss: 0.2606405019760132\n",
      "Epoch 4248, Loss: 1.548183649778366, Final Batch Loss: 0.283248633146286\n",
      "Epoch 4249, Loss: 1.5808006823062897, Final Batch Loss: 0.33175918459892273\n",
      "Epoch 4250, Loss: 1.4250580668449402, Final Batch Loss: 0.31678566336631775\n",
      "Epoch 4251, Loss: 1.4428928792476654, Final Batch Loss: 0.32797929644584656\n",
      "Epoch 4252, Loss: 1.5770967900753021, Final Batch Loss: 0.2816145718097687\n",
      "Epoch 4253, Loss: 1.4500360488891602, Final Batch Loss: 0.28146570920944214\n",
      "Epoch 4254, Loss: 1.5719145238399506, Final Batch Loss: 0.3393571972846985\n",
      "Epoch 4255, Loss: 1.4374110400676727, Final Batch Loss: 0.25234028697013855\n",
      "Epoch 4256, Loss: 1.3231253325939178, Final Batch Loss: 0.2300446331501007\n",
      "Epoch 4257, Loss: 1.4669967740774155, Final Batch Loss: 0.38997340202331543\n",
      "Epoch 4258, Loss: 1.3722267597913742, Final Batch Loss: 0.31042155623435974\n",
      "Epoch 4259, Loss: 1.4229969084262848, Final Batch Loss: 0.30165866017341614\n",
      "Epoch 4260, Loss: 1.4905321300029755, Final Batch Loss: 0.3051774501800537\n",
      "Epoch 4261, Loss: 1.5456548631191254, Final Batch Loss: 0.22650496661663055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4262, Loss: 1.5282763689756393, Final Batch Loss: 0.23307418823242188\n",
      "Epoch 4263, Loss: 1.5326052457094193, Final Batch Loss: 0.4092020094394684\n",
      "Epoch 4264, Loss: 1.5126832276582718, Final Batch Loss: 0.3668981194496155\n",
      "Epoch 4265, Loss: 1.5853325724601746, Final Batch Loss: 0.4320760667324066\n",
      "Epoch 4266, Loss: 1.4119911938905716, Final Batch Loss: 0.28448250889778137\n",
      "Epoch 4267, Loss: 1.4381131082773209, Final Batch Loss: 0.24451996386051178\n",
      "Epoch 4268, Loss: 1.502736508846283, Final Batch Loss: 0.30956220626831055\n",
      "Epoch 4269, Loss: 1.4378361105918884, Final Batch Loss: 0.34218183159828186\n",
      "Epoch 4270, Loss: 1.4482549130916595, Final Batch Loss: 0.3210585415363312\n",
      "Epoch 4271, Loss: 1.4875873178243637, Final Batch Loss: 0.28644368052482605\n",
      "Epoch 4272, Loss: 1.4010304808616638, Final Batch Loss: 0.2851610779762268\n",
      "Epoch 4273, Loss: 1.3851336389780045, Final Batch Loss: 0.3403377830982208\n",
      "Epoch 4274, Loss: 1.4714798778295517, Final Batch Loss: 0.33495184779167175\n",
      "Epoch 4275, Loss: 1.3973788619041443, Final Batch Loss: 0.20914223790168762\n",
      "Epoch 4276, Loss: 1.3414085060358047, Final Batch Loss: 0.2774357199668884\n",
      "Epoch 4277, Loss: 1.4166128188371658, Final Batch Loss: 0.20110072195529938\n",
      "Epoch 4278, Loss: 1.4967134296894073, Final Batch Loss: 0.2721417546272278\n",
      "Epoch 4279, Loss: 1.3848900496959686, Final Batch Loss: 0.24277746677398682\n",
      "Epoch 4280, Loss: 1.525142639875412, Final Batch Loss: 0.3440857231616974\n",
      "Epoch 4281, Loss: 1.345228835940361, Final Batch Loss: 0.21721093356609344\n",
      "Epoch 4282, Loss: 1.3623835444450378, Final Batch Loss: 0.27341124415397644\n",
      "Epoch 4283, Loss: 1.5318917632102966, Final Batch Loss: 0.35732072591781616\n",
      "Epoch 4284, Loss: 1.363148719072342, Final Batch Loss: 0.2502000331878662\n",
      "Epoch 4285, Loss: 1.4252306669950485, Final Batch Loss: 0.36290866136550903\n",
      "Epoch 4286, Loss: 1.4700337946414948, Final Batch Loss: 0.2994624674320221\n",
      "Epoch 4287, Loss: 1.4440964460372925, Final Batch Loss: 0.2699398994445801\n",
      "Epoch 4288, Loss: 1.3227962851524353, Final Batch Loss: 0.26341021060943604\n",
      "Epoch 4289, Loss: 1.4642181694507599, Final Batch Loss: 0.2411816567182541\n",
      "Epoch 4290, Loss: 1.4832723736763, Final Batch Loss: 0.2912408411502838\n",
      "Epoch 4291, Loss: 1.510167047381401, Final Batch Loss: 0.2833290100097656\n",
      "Epoch 4292, Loss: 1.354364961385727, Final Batch Loss: 0.22671622037887573\n",
      "Epoch 4293, Loss: 1.4331617653369904, Final Batch Loss: 0.27182942628860474\n",
      "Epoch 4294, Loss: 1.2942495495080948, Final Batch Loss: 0.28406429290771484\n",
      "Epoch 4295, Loss: 1.4237968027591705, Final Batch Loss: 0.31159424781799316\n",
      "Epoch 4296, Loss: 1.5795440375804901, Final Batch Loss: 0.39494267106056213\n",
      "Epoch 4297, Loss: 1.42525976896286, Final Batch Loss: 0.30782508850097656\n",
      "Epoch 4298, Loss: 1.6051979064941406, Final Batch Loss: 0.2932807207107544\n",
      "Epoch 4299, Loss: 1.463163435459137, Final Batch Loss: 0.33674025535583496\n",
      "Epoch 4300, Loss: 1.3591117709875107, Final Batch Loss: 0.24723516404628754\n",
      "Epoch 4301, Loss: 1.290089562535286, Final Batch Loss: 0.26004689931869507\n",
      "Epoch 4302, Loss: 1.4849925339221954, Final Batch Loss: 0.2990513741970062\n",
      "Epoch 4303, Loss: 1.6817520558834076, Final Batch Loss: 0.3283032774925232\n",
      "Epoch 4304, Loss: 1.4158547967672348, Final Batch Loss: 0.2484491616487503\n",
      "Epoch 4305, Loss: 1.4770425260066986, Final Batch Loss: 0.34132614731788635\n",
      "Epoch 4306, Loss: 1.5573879182338715, Final Batch Loss: 0.35952746868133545\n",
      "Epoch 4307, Loss: 1.510624572634697, Final Batch Loss: 0.2200573831796646\n",
      "Epoch 4308, Loss: 1.458043783903122, Final Batch Loss: 0.22530817985534668\n",
      "Epoch 4309, Loss: 1.416928842663765, Final Batch Loss: 0.3030053377151489\n",
      "Epoch 4310, Loss: 1.3921961784362793, Final Batch Loss: 0.30271804332733154\n",
      "Epoch 4311, Loss: 1.3276776820421219, Final Batch Loss: 0.3009848892688751\n",
      "Epoch 4312, Loss: 1.5025611221790314, Final Batch Loss: 0.3332952857017517\n",
      "Epoch 4313, Loss: 1.4167302697896957, Final Batch Loss: 0.3822799026966095\n",
      "Epoch 4314, Loss: 1.5813232213258743, Final Batch Loss: 0.46435388922691345\n",
      "Epoch 4315, Loss: 1.4106077551841736, Final Batch Loss: 0.2259584367275238\n",
      "Epoch 4316, Loss: 1.4409983605146408, Final Batch Loss: 0.30811333656311035\n",
      "Epoch 4317, Loss: 1.4767981469631195, Final Batch Loss: 0.22992396354675293\n",
      "Epoch 4318, Loss: 1.4787282794713974, Final Batch Loss: 0.20799453556537628\n",
      "Epoch 4319, Loss: 1.3495337665081024, Final Batch Loss: 0.21767161786556244\n",
      "Epoch 4320, Loss: 1.4988336265087128, Final Batch Loss: 0.2618211507797241\n",
      "Epoch 4321, Loss: 1.3815037310123444, Final Batch Loss: 0.23321828246116638\n",
      "Epoch 4322, Loss: 1.3511184304952621, Final Batch Loss: 0.21339993178844452\n",
      "Epoch 4323, Loss: 1.336345374584198, Final Batch Loss: 0.29054397344589233\n",
      "Epoch 4324, Loss: 1.4267254769802094, Final Batch Loss: 0.23929241299629211\n",
      "Epoch 4325, Loss: 1.4287324249744415, Final Batch Loss: 0.28663256764411926\n",
      "Epoch 4326, Loss: 1.5011605769395828, Final Batch Loss: 0.3863239288330078\n",
      "Epoch 4327, Loss: 1.4134013801813126, Final Batch Loss: 0.3329123556613922\n",
      "Epoch 4328, Loss: 1.4651986360549927, Final Batch Loss: 0.2665156126022339\n",
      "Epoch 4329, Loss: 1.4004925936460495, Final Batch Loss: 0.2568844258785248\n",
      "Epoch 4330, Loss: 1.6652149558067322, Final Batch Loss: 0.31696292757987976\n",
      "Epoch 4331, Loss: 1.3189253509044647, Final Batch Loss: 0.2897167205810547\n",
      "Epoch 4332, Loss: 1.414953574538231, Final Batch Loss: 0.34318357706069946\n",
      "Epoch 4333, Loss: 1.4402370154857635, Final Batch Loss: 0.3573879897594452\n",
      "Epoch 4334, Loss: 1.6235404461622238, Final Batch Loss: 0.3507046699523926\n",
      "Epoch 4335, Loss: 1.4282488822937012, Final Batch Loss: 0.23125115036964417\n",
      "Epoch 4336, Loss: 1.3999531865119934, Final Batch Loss: 0.2587164640426636\n",
      "Epoch 4337, Loss: 1.4010572135448456, Final Batch Loss: 0.22460108995437622\n",
      "Epoch 4338, Loss: 1.4795161187648773, Final Batch Loss: 0.18966951966285706\n",
      "Epoch 4339, Loss: 1.4244312793016434, Final Batch Loss: 0.2731034457683563\n",
      "Epoch 4340, Loss: 1.479821041226387, Final Batch Loss: 0.31816405057907104\n",
      "Epoch 4341, Loss: 1.4058863371610641, Final Batch Loss: 0.3227579891681671\n",
      "Epoch 4342, Loss: 1.5990815162658691, Final Batch Loss: 0.31611907482147217\n",
      "Epoch 4343, Loss: 1.468990370631218, Final Batch Loss: 0.26360028982162476\n",
      "Epoch 4344, Loss: 1.4736450910568237, Final Batch Loss: 0.27615320682525635\n",
      "Epoch 4345, Loss: 1.2838903665542603, Final Batch Loss: 0.21623282134532928\n",
      "Epoch 4346, Loss: 1.4140370935201645, Final Batch Loss: 0.2387469857931137\n",
      "Epoch 4347, Loss: 1.5263310074806213, Final Batch Loss: 0.35584312677383423\n",
      "Epoch 4348, Loss: 1.4582838118076324, Final Batch Loss: 0.2489893138408661\n",
      "Epoch 4349, Loss: 1.4796576499938965, Final Batch Loss: 0.2676408588886261\n",
      "Epoch 4350, Loss: 1.4412352591753006, Final Batch Loss: 0.442861944437027\n",
      "Epoch 4351, Loss: 1.4161727875471115, Final Batch Loss: 0.29355588555336\n",
      "Epoch 4352, Loss: 1.4241564124822617, Final Batch Loss: 0.22590403258800507\n",
      "Epoch 4353, Loss: 1.3658093959093094, Final Batch Loss: 0.32809269428253174\n",
      "Epoch 4354, Loss: 1.4005622863769531, Final Batch Loss: 0.32069656252861023\n",
      "Epoch 4355, Loss: 1.3614301085472107, Final Batch Loss: 0.32393383979797363\n",
      "Epoch 4356, Loss: 1.4818415939807892, Final Batch Loss: 0.3015630543231964\n",
      "Epoch 4357, Loss: 1.317204087972641, Final Batch Loss: 0.26069164276123047\n",
      "Epoch 4358, Loss: 1.4690524190664291, Final Batch Loss: 0.3611290156841278\n",
      "Epoch 4359, Loss: 1.3077490627765656, Final Batch Loss: 0.23463045060634613\n",
      "Epoch 4360, Loss: 1.4849041998386383, Final Batch Loss: 0.24345776438713074\n",
      "Epoch 4361, Loss: 1.3576878756284714, Final Batch Loss: 0.31363150477409363\n",
      "Epoch 4362, Loss: 1.537810057401657, Final Batch Loss: 0.3426574468612671\n",
      "Epoch 4363, Loss: 1.3368982672691345, Final Batch Loss: 0.26689043641090393\n",
      "Epoch 4364, Loss: 1.4080785810947418, Final Batch Loss: 0.3018486499786377\n",
      "Epoch 4365, Loss: 1.491186410188675, Final Batch Loss: 0.2804822623729706\n",
      "Epoch 4366, Loss: 1.4600453972816467, Final Batch Loss: 0.35940006375312805\n",
      "Epoch 4367, Loss: 1.4384311586618423, Final Batch Loss: 0.3075248599052429\n",
      "Epoch 4368, Loss: 1.2906910926103592, Final Batch Loss: 0.21671129763126373\n",
      "Epoch 4369, Loss: 1.3660408854484558, Final Batch Loss: 0.28812313079833984\n",
      "Epoch 4370, Loss: 1.571905642747879, Final Batch Loss: 0.26820433139801025\n",
      "Epoch 4371, Loss: 1.4873498976230621, Final Batch Loss: 0.25930288434028625\n",
      "Epoch 4372, Loss: 1.2911953181028366, Final Batch Loss: 0.2466088980436325\n",
      "Epoch 4373, Loss: 1.383230373263359, Final Batch Loss: 0.29920607805252075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4374, Loss: 1.4792750626802444, Final Batch Loss: 0.2432357221841812\n",
      "Epoch 4375, Loss: 1.4396143853664398, Final Batch Loss: 0.35667723417282104\n",
      "Epoch 4376, Loss: 1.4891908168792725, Final Batch Loss: 0.2578529417514801\n",
      "Epoch 4377, Loss: 1.3522141724824905, Final Batch Loss: 0.31395846605300903\n",
      "Epoch 4378, Loss: 1.5096964538097382, Final Batch Loss: 0.39983510971069336\n",
      "Epoch 4379, Loss: 1.5116628408432007, Final Batch Loss: 0.3493365943431854\n",
      "Epoch 4380, Loss: 1.5159467309713364, Final Batch Loss: 0.40890559554100037\n",
      "Epoch 4381, Loss: 1.4910653978586197, Final Batch Loss: 0.3525993525981903\n",
      "Epoch 4382, Loss: 1.5459162592887878, Final Batch Loss: 0.33391159772872925\n",
      "Epoch 4383, Loss: 1.4993204474449158, Final Batch Loss: 0.2852495610713959\n",
      "Epoch 4384, Loss: 1.4800187945365906, Final Batch Loss: 0.3260660767555237\n",
      "Epoch 4385, Loss: 1.4680891633033752, Final Batch Loss: 0.2419070601463318\n",
      "Epoch 4386, Loss: 1.4492230415344238, Final Batch Loss: 0.25875920057296753\n",
      "Epoch 4387, Loss: 1.4766347855329514, Final Batch Loss: 0.2271142154932022\n",
      "Epoch 4388, Loss: 1.3861395865678787, Final Batch Loss: 0.21592260897159576\n",
      "Epoch 4389, Loss: 1.41516675055027, Final Batch Loss: 0.27392399311065674\n",
      "Epoch 4390, Loss: 1.3719016015529633, Final Batch Loss: 0.21711701154708862\n",
      "Epoch 4391, Loss: 1.4786598980426788, Final Batch Loss: 0.24913059175014496\n",
      "Epoch 4392, Loss: 1.404836282134056, Final Batch Loss: 0.25585296750068665\n",
      "Epoch 4393, Loss: 1.4653292745351791, Final Batch Loss: 0.3286080062389374\n",
      "Epoch 4394, Loss: 1.4013487100601196, Final Batch Loss: 0.24364203214645386\n",
      "Epoch 4395, Loss: 1.498523861169815, Final Batch Loss: 0.3246074616909027\n",
      "Epoch 4396, Loss: 1.2318524569272995, Final Batch Loss: 0.1992366462945938\n",
      "Epoch 4397, Loss: 1.522498369216919, Final Batch Loss: 0.3720734119415283\n",
      "Epoch 4398, Loss: 1.4127012193202972, Final Batch Loss: 0.2970176041126251\n",
      "Epoch 4399, Loss: 1.4886052310466766, Final Batch Loss: 0.22548702359199524\n",
      "Epoch 4400, Loss: 1.423891767859459, Final Batch Loss: 0.25871479511260986\n",
      "Epoch 4401, Loss: 1.2855106443166733, Final Batch Loss: 0.1891225129365921\n",
      "Epoch 4402, Loss: 1.3958507776260376, Final Batch Loss: 0.291240930557251\n",
      "Epoch 4403, Loss: 1.3635172694921494, Final Batch Loss: 0.3077283799648285\n",
      "Epoch 4404, Loss: 1.4635797888040543, Final Batch Loss: 0.24478895962238312\n",
      "Epoch 4405, Loss: 1.5388475209474564, Final Batch Loss: 0.23946891725063324\n",
      "Epoch 4406, Loss: 1.5515137016773224, Final Batch Loss: 0.2950243055820465\n",
      "Epoch 4407, Loss: 1.4450171291828156, Final Batch Loss: 0.29878437519073486\n",
      "Epoch 4408, Loss: 1.3254746198654175, Final Batch Loss: 0.2501589059829712\n",
      "Epoch 4409, Loss: 1.4583967924118042, Final Batch Loss: 0.2725430727005005\n",
      "Epoch 4410, Loss: 1.4351630061864853, Final Batch Loss: 0.2826673984527588\n",
      "Epoch 4411, Loss: 1.3978760093450546, Final Batch Loss: 0.299370139837265\n",
      "Epoch 4412, Loss: 1.3662229776382446, Final Batch Loss: 0.33264192938804626\n",
      "Epoch 4413, Loss: 1.463940441608429, Final Batch Loss: 0.30603665113449097\n",
      "Epoch 4414, Loss: 1.4164117127656937, Final Batch Loss: 0.24555769562721252\n",
      "Epoch 4415, Loss: 1.3593769669532776, Final Batch Loss: 0.2324373424053192\n",
      "Epoch 4416, Loss: 1.282484009861946, Final Batch Loss: 0.23748910427093506\n",
      "Epoch 4417, Loss: 1.5044034123420715, Final Batch Loss: 0.3134833574295044\n",
      "Epoch 4418, Loss: 1.3712824583053589, Final Batch Loss: 0.29301467537879944\n",
      "Epoch 4419, Loss: 1.3944372832775116, Final Batch Loss: 0.21742531657218933\n",
      "Epoch 4420, Loss: 1.401075690984726, Final Batch Loss: 0.35756486654281616\n",
      "Epoch 4421, Loss: 1.3702380806207657, Final Batch Loss: 0.2850981056690216\n",
      "Epoch 4422, Loss: 1.4485152065753937, Final Batch Loss: 0.2652532458305359\n",
      "Epoch 4423, Loss: 1.4607610702514648, Final Batch Loss: 0.32308873534202576\n",
      "Epoch 4424, Loss: 1.365452453494072, Final Batch Loss: 0.2486046850681305\n",
      "Epoch 4425, Loss: 1.3900507986545563, Final Batch Loss: 0.31859710812568665\n",
      "Epoch 4426, Loss: 1.470139056444168, Final Batch Loss: 0.3350484073162079\n",
      "Epoch 4427, Loss: 1.4775457829236984, Final Batch Loss: 0.36453431844711304\n",
      "Epoch 4428, Loss: 1.4049581289291382, Final Batch Loss: 0.27798208594322205\n",
      "Epoch 4429, Loss: 1.523990973830223, Final Batch Loss: 0.3288308084011078\n",
      "Epoch 4430, Loss: 1.510040059685707, Final Batch Loss: 0.3316860496997833\n",
      "Epoch 4431, Loss: 1.4311045408248901, Final Batch Loss: 0.29294151067733765\n",
      "Epoch 4432, Loss: 1.4611366987228394, Final Batch Loss: 0.23516327142715454\n",
      "Epoch 4433, Loss: 1.3194209039211273, Final Batch Loss: 0.33086180686950684\n",
      "Epoch 4434, Loss: 1.4660984873771667, Final Batch Loss: 0.2584451138973236\n",
      "Epoch 4435, Loss: 1.581439197063446, Final Batch Loss: 0.40952539443969727\n",
      "Epoch 4436, Loss: 1.4163208901882172, Final Batch Loss: 0.26033052802085876\n",
      "Epoch 4437, Loss: 1.560529664158821, Final Batch Loss: 0.3842376172542572\n",
      "Epoch 4438, Loss: 1.4614822268486023, Final Batch Loss: 0.2961342930793762\n",
      "Epoch 4439, Loss: 1.3251650780439377, Final Batch Loss: 0.3132621645927429\n",
      "Epoch 4440, Loss: 1.472425863146782, Final Batch Loss: 0.3081115186214447\n",
      "Epoch 4441, Loss: 1.5766337215900421, Final Batch Loss: 0.3021831512451172\n",
      "Epoch 4442, Loss: 1.4230494499206543, Final Batch Loss: 0.25558754801750183\n",
      "Epoch 4443, Loss: 1.6242823898792267, Final Batch Loss: 0.4179799556732178\n",
      "Epoch 4444, Loss: 1.3774651139974594, Final Batch Loss: 0.3319115936756134\n",
      "Epoch 4445, Loss: 1.4769848138093948, Final Batch Loss: 0.3597869873046875\n",
      "Epoch 4446, Loss: 1.358017936348915, Final Batch Loss: 0.2610068619251251\n",
      "Epoch 4447, Loss: 1.3381739407777786, Final Batch Loss: 0.22160115838050842\n",
      "Epoch 4448, Loss: 1.4787605255842209, Final Batch Loss: 0.355617880821228\n",
      "Epoch 4449, Loss: 1.361849695444107, Final Batch Loss: 0.24238131940364838\n",
      "Epoch 4450, Loss: 1.5544278919696808, Final Batch Loss: 0.3420812487602234\n",
      "Epoch 4451, Loss: 1.4017316401004791, Final Batch Loss: 0.27762719988822937\n",
      "Epoch 4452, Loss: 1.4807027578353882, Final Batch Loss: 0.22636055946350098\n",
      "Epoch 4453, Loss: 1.4249612092971802, Final Batch Loss: 0.3893216848373413\n",
      "Epoch 4454, Loss: 1.4741349518299103, Final Batch Loss: 0.2750556766986847\n",
      "Epoch 4455, Loss: 1.591340333223343, Final Batch Loss: 0.34186282753944397\n",
      "Epoch 4456, Loss: 1.4123191237449646, Final Batch Loss: 0.30896300077438354\n",
      "Epoch 4457, Loss: 1.4267709106206894, Final Batch Loss: 0.306423544883728\n",
      "Epoch 4458, Loss: 1.469222828745842, Final Batch Loss: 0.2026328295469284\n",
      "Epoch 4459, Loss: 1.3675022274255753, Final Batch Loss: 0.2716030776500702\n",
      "Epoch 4460, Loss: 1.3463139533996582, Final Batch Loss: 0.244979128241539\n",
      "Epoch 4461, Loss: 1.3678220361471176, Final Batch Loss: 0.2425914704799652\n",
      "Epoch 4462, Loss: 1.4782450050115585, Final Batch Loss: 0.42897582054138184\n",
      "Epoch 4463, Loss: 1.374982476234436, Final Batch Loss: 0.29075902700424194\n",
      "Epoch 4464, Loss: 1.307153433561325, Final Batch Loss: 0.24620097875595093\n",
      "Epoch 4465, Loss: 1.3805275708436966, Final Batch Loss: 0.3290137052536011\n",
      "Epoch 4466, Loss: 1.3284469544887543, Final Batch Loss: 0.20124325156211853\n",
      "Epoch 4467, Loss: 1.480832353234291, Final Batch Loss: 0.2904317080974579\n",
      "Epoch 4468, Loss: 1.3604693412780762, Final Batch Loss: 0.30878862738609314\n",
      "Epoch 4469, Loss: 1.3457454591989517, Final Batch Loss: 0.22712191939353943\n",
      "Epoch 4470, Loss: 1.4714588224887848, Final Batch Loss: 0.2546919286251068\n",
      "Epoch 4471, Loss: 1.3177132308483124, Final Batch Loss: 0.24363715946674347\n",
      "Epoch 4472, Loss: 1.3640542924404144, Final Batch Loss: 0.26301002502441406\n",
      "Epoch 4473, Loss: 1.519898682832718, Final Batch Loss: 0.2786226272583008\n",
      "Epoch 4474, Loss: 1.3071247935295105, Final Batch Loss: 0.3030565083026886\n",
      "Epoch 4475, Loss: 1.5601784735918045, Final Batch Loss: 0.32836785912513733\n",
      "Epoch 4476, Loss: 1.3936118185520172, Final Batch Loss: 0.268458753824234\n",
      "Epoch 4477, Loss: 1.3139492720365524, Final Batch Loss: 0.2128290832042694\n",
      "Epoch 4478, Loss: 1.4366934299468994, Final Batch Loss: 0.27474111318588257\n",
      "Epoch 4479, Loss: 1.3905003815889359, Final Batch Loss: 0.24638758599758148\n",
      "Epoch 4480, Loss: 1.4682172238826752, Final Batch Loss: 0.23521465063095093\n",
      "Epoch 4481, Loss: 1.3005117774009705, Final Batch Loss: 0.2827901840209961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4482, Loss: 1.3794673383235931, Final Batch Loss: 0.3149467706680298\n",
      "Epoch 4483, Loss: 1.3716305196285248, Final Batch Loss: 0.27675342559814453\n",
      "Epoch 4484, Loss: 1.3868289142847061, Final Batch Loss: 0.35422953963279724\n",
      "Epoch 4485, Loss: 1.4639737010002136, Final Batch Loss: 0.2798301875591278\n",
      "Epoch 4486, Loss: 1.451229840517044, Final Batch Loss: 0.24154701828956604\n",
      "Epoch 4487, Loss: 1.528078943490982, Final Batch Loss: 0.35599473118782043\n",
      "Epoch 4488, Loss: 1.4712002575397491, Final Batch Loss: 0.29378050565719604\n",
      "Epoch 4489, Loss: 1.5642710030078888, Final Batch Loss: 0.3161327838897705\n",
      "Epoch 4490, Loss: 1.4218148440122604, Final Batch Loss: 0.35875922441482544\n",
      "Epoch 4491, Loss: 1.3865301460027695, Final Batch Loss: 0.24814210832118988\n",
      "Epoch 4492, Loss: 1.7215997576713562, Final Batch Loss: 0.4326619505882263\n",
      "Epoch 4493, Loss: 1.356339082121849, Final Batch Loss: 0.31319284439086914\n",
      "Epoch 4494, Loss: 1.2962770611047745, Final Batch Loss: 0.2444286197423935\n",
      "Epoch 4495, Loss: 1.4625245928764343, Final Batch Loss: 0.3572525978088379\n",
      "Epoch 4496, Loss: 1.4455255419015884, Final Batch Loss: 0.3111782968044281\n",
      "Epoch 4497, Loss: 1.5084444880485535, Final Batch Loss: 0.3156599700450897\n",
      "Epoch 4498, Loss: 1.4875924587249756, Final Batch Loss: 0.3216516375541687\n",
      "Epoch 4499, Loss: 1.4978070855140686, Final Batch Loss: 0.28678423166275024\n",
      "Epoch 4500, Loss: 1.3706880062818527, Final Batch Loss: 0.3395456075668335\n",
      "Epoch 4501, Loss: 1.2981194257736206, Final Batch Loss: 0.2110486626625061\n",
      "Epoch 4502, Loss: 1.5053023397922516, Final Batch Loss: 0.2572513520717621\n",
      "Epoch 4503, Loss: 1.4561711251735687, Final Batch Loss: 0.21307110786437988\n",
      "Epoch 4504, Loss: 1.5267354547977448, Final Batch Loss: 0.26390063762664795\n",
      "Epoch 4505, Loss: 1.4484042674303055, Final Batch Loss: 0.24551112949848175\n",
      "Epoch 4506, Loss: 1.3086941093206406, Final Batch Loss: 0.24080152809619904\n",
      "Epoch 4507, Loss: 1.5762351751327515, Final Batch Loss: 0.406520277261734\n",
      "Epoch 4508, Loss: 1.5397908389568329, Final Batch Loss: 0.4132716655731201\n",
      "Epoch 4509, Loss: 1.4300408065319061, Final Batch Loss: 0.27355703711509705\n",
      "Epoch 4510, Loss: 1.5336496233940125, Final Batch Loss: 0.2803328037261963\n",
      "Epoch 4511, Loss: 1.4677252024412155, Final Batch Loss: 0.3272133767604828\n",
      "Epoch 4512, Loss: 1.3858794867992401, Final Batch Loss: 0.28036174178123474\n",
      "Epoch 4513, Loss: 1.2545710653066635, Final Batch Loss: 0.17538928985595703\n",
      "Epoch 4514, Loss: 1.2522163838148117, Final Batch Loss: 0.178182452917099\n",
      "Epoch 4515, Loss: 1.43815116584301, Final Batch Loss: 0.32241207361221313\n",
      "Epoch 4516, Loss: 1.375865787267685, Final Batch Loss: 0.24812164902687073\n",
      "Epoch 4517, Loss: 1.292998045682907, Final Batch Loss: 0.31060346961021423\n",
      "Epoch 4518, Loss: 1.2980609685182571, Final Batch Loss: 0.2252645045518875\n",
      "Epoch 4519, Loss: 1.3427141606807709, Final Batch Loss: 0.2764609754085541\n",
      "Epoch 4520, Loss: 1.2919747829437256, Final Batch Loss: 0.2633230686187744\n",
      "Epoch 4521, Loss: 1.4891088008880615, Final Batch Loss: 0.3270818591117859\n",
      "Epoch 4522, Loss: 1.4077418446540833, Final Batch Loss: 0.2164483219385147\n",
      "Epoch 4523, Loss: 1.3451461642980576, Final Batch Loss: 0.22259603440761566\n",
      "Epoch 4524, Loss: 1.3851986229419708, Final Batch Loss: 0.32346311211586\n",
      "Epoch 4525, Loss: 1.405112862586975, Final Batch Loss: 0.22227461636066437\n",
      "Epoch 4526, Loss: 1.4312295466661453, Final Batch Loss: 0.27816087007522583\n",
      "Epoch 4527, Loss: 1.4299505949020386, Final Batch Loss: 0.27889105677604675\n",
      "Epoch 4528, Loss: 1.3818196952342987, Final Batch Loss: 0.35526326298713684\n",
      "Epoch 4529, Loss: 1.384034276008606, Final Batch Loss: 0.25052306056022644\n",
      "Epoch 4530, Loss: 1.4000602662563324, Final Batch Loss: 0.2721979320049286\n",
      "Epoch 4531, Loss: 1.4562770277261734, Final Batch Loss: 0.2611265182495117\n",
      "Epoch 4532, Loss: 1.3684038072824478, Final Batch Loss: 0.18849483132362366\n",
      "Epoch 4533, Loss: 1.4627454578876495, Final Batch Loss: 0.2575109899044037\n",
      "Epoch 4534, Loss: 1.4863035082817078, Final Batch Loss: 0.32702532410621643\n",
      "Epoch 4535, Loss: 1.3184115290641785, Final Batch Loss: 0.29483428597450256\n",
      "Epoch 4536, Loss: 1.3721744269132614, Final Batch Loss: 0.24138136208057404\n",
      "Epoch 4537, Loss: 1.4340937584638596, Final Batch Loss: 0.18940269947052002\n",
      "Epoch 4538, Loss: 1.3680886328220367, Final Batch Loss: 0.23276180028915405\n",
      "Epoch 4539, Loss: 1.3658795058727264, Final Batch Loss: 0.29836347699165344\n",
      "Epoch 4540, Loss: 1.4102914482355118, Final Batch Loss: 0.37984898686408997\n",
      "Epoch 4541, Loss: 1.377223938703537, Final Batch Loss: 0.25895342230796814\n",
      "Epoch 4542, Loss: 1.4857692569494247, Final Batch Loss: 0.43213409185409546\n",
      "Epoch 4543, Loss: 1.4886987209320068, Final Batch Loss: 0.3501212000846863\n",
      "Epoch 4544, Loss: 1.495815485715866, Final Batch Loss: 0.2875867486000061\n",
      "Epoch 4545, Loss: 1.4230273962020874, Final Batch Loss: 0.2777811884880066\n",
      "Epoch 4546, Loss: 1.4030560702085495, Final Batch Loss: 0.24290908873081207\n",
      "Epoch 4547, Loss: 1.417700171470642, Final Batch Loss: 0.23656593263149261\n",
      "Epoch 4548, Loss: 1.484403908252716, Final Batch Loss: 0.2923949062824249\n",
      "Epoch 4549, Loss: 1.3557350486516953, Final Batch Loss: 0.26124098896980286\n",
      "Epoch 4550, Loss: 1.4173516780138016, Final Batch Loss: 0.3071750998497009\n",
      "Epoch 4551, Loss: 1.4392243176698685, Final Batch Loss: 0.24192644655704498\n",
      "Epoch 4552, Loss: 1.2577836960554123, Final Batch Loss: 0.222743958234787\n",
      "Epoch 4553, Loss: 1.4329435527324677, Final Batch Loss: 0.2678050994873047\n",
      "Epoch 4554, Loss: 1.3063360303640366, Final Batch Loss: 0.2571582794189453\n",
      "Epoch 4555, Loss: 1.3818675130605698, Final Batch Loss: 0.28267207741737366\n",
      "Epoch 4556, Loss: 1.3757733553647995, Final Batch Loss: 0.26561418175697327\n",
      "Epoch 4557, Loss: 1.403005987405777, Final Batch Loss: 0.2670513689517975\n",
      "Epoch 4558, Loss: 1.3315251767635345, Final Batch Loss: 0.28170135617256165\n",
      "Epoch 4559, Loss: 1.5729508846998215, Final Batch Loss: 0.35141679644584656\n",
      "Epoch 4560, Loss: 1.290096640586853, Final Batch Loss: 0.23189471662044525\n",
      "Epoch 4561, Loss: 1.3727293014526367, Final Batch Loss: 0.21483002603054047\n",
      "Epoch 4562, Loss: 1.432390108704567, Final Batch Loss: 0.3683309555053711\n",
      "Epoch 4563, Loss: 1.303035944700241, Final Batch Loss: 0.28053662180900574\n",
      "Epoch 4564, Loss: 1.5753692388534546, Final Batch Loss: 0.27322083711624146\n",
      "Epoch 4565, Loss: 1.4151336997747421, Final Batch Loss: 0.2388402223587036\n",
      "Epoch 4566, Loss: 1.4560512006282806, Final Batch Loss: 0.3245600759983063\n",
      "Epoch 4567, Loss: 1.3490049839019775, Final Batch Loss: 0.2597748041152954\n",
      "Epoch 4568, Loss: 1.262213945388794, Final Batch Loss: 0.25799110531806946\n",
      "Epoch 4569, Loss: 1.3097512274980545, Final Batch Loss: 0.2512875199317932\n",
      "Epoch 4570, Loss: 1.309294044971466, Final Batch Loss: 0.3250003457069397\n",
      "Epoch 4571, Loss: 1.406556487083435, Final Batch Loss: 0.3514557480812073\n",
      "Epoch 4572, Loss: 1.2753318846225739, Final Batch Loss: 0.2312006652355194\n",
      "Epoch 4573, Loss: 1.6800436079502106, Final Batch Loss: 0.46665534377098083\n",
      "Epoch 4574, Loss: 1.4513607919216156, Final Batch Loss: 0.34279659390449524\n",
      "Epoch 4575, Loss: 1.4119258373975754, Final Batch Loss: 0.34778037667274475\n",
      "Epoch 4576, Loss: 1.4119368344545364, Final Batch Loss: 0.2852889895439148\n",
      "Epoch 4577, Loss: 1.289961576461792, Final Batch Loss: 0.25761425495147705\n",
      "Epoch 4578, Loss: 1.324011817574501, Final Batch Loss: 0.26371684670448303\n",
      "Epoch 4579, Loss: 1.437102049589157, Final Batch Loss: 0.29281696677207947\n",
      "Epoch 4580, Loss: 1.4399074018001556, Final Batch Loss: 0.26339268684387207\n",
      "Epoch 4581, Loss: 1.3164781630039215, Final Batch Loss: 0.28331732749938965\n",
      "Epoch 4582, Loss: 1.3086951673030853, Final Batch Loss: 0.23262545466423035\n",
      "Epoch 4583, Loss: 1.380748838186264, Final Batch Loss: 0.23290923237800598\n",
      "Epoch 4584, Loss: 1.3319547027349472, Final Batch Loss: 0.29442012310028076\n",
      "Epoch 4585, Loss: 1.3529413789510727, Final Batch Loss: 0.32271474599838257\n",
      "Epoch 4586, Loss: 1.374815210700035, Final Batch Loss: 0.23448599874973297\n",
      "Epoch 4587, Loss: 1.409143090248108, Final Batch Loss: 0.21451866626739502\n",
      "Epoch 4588, Loss: 1.2946334481239319, Final Batch Loss: 0.24002191424369812\n",
      "Epoch 4589, Loss: 1.3858327716588974, Final Batch Loss: 0.23685599863529205\n",
      "Epoch 4590, Loss: 1.4006837457418442, Final Batch Loss: 0.335384339094162\n",
      "Epoch 4591, Loss: 1.4359505325555801, Final Batch Loss: 0.30675315856933594\n",
      "Epoch 4592, Loss: 1.379994422197342, Final Batch Loss: 0.24369794130325317\n",
      "Epoch 4593, Loss: 1.4275663793087006, Final Batch Loss: 0.26544445753097534\n",
      "Epoch 4594, Loss: 1.341619148850441, Final Batch Loss: 0.2668365240097046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4595, Loss: 1.3693108409643173, Final Batch Loss: 0.3454481065273285\n",
      "Epoch 4596, Loss: 1.7200629711151123, Final Batch Loss: 0.42610201239585876\n",
      "Epoch 4597, Loss: 1.4026943296194077, Final Batch Loss: 0.29689493775367737\n",
      "Epoch 4598, Loss: 1.4112889617681503, Final Batch Loss: 0.4119621515274048\n",
      "Epoch 4599, Loss: 1.5359580218791962, Final Batch Loss: 0.32374539971351624\n",
      "Epoch 4600, Loss: 1.3687408864498138, Final Batch Loss: 0.28218674659729004\n",
      "Epoch 4601, Loss: 1.3948226422071457, Final Batch Loss: 0.3152041435241699\n",
      "Epoch 4602, Loss: 1.3462525457143784, Final Batch Loss: 0.2094365954399109\n",
      "Epoch 4603, Loss: 1.3816570192575455, Final Batch Loss: 0.3201456367969513\n",
      "Epoch 4604, Loss: 1.2497546821832657, Final Batch Loss: 0.1628057360649109\n",
      "Epoch 4605, Loss: 1.4559652507305145, Final Batch Loss: 0.30186009407043457\n",
      "Epoch 4606, Loss: 1.4514989256858826, Final Batch Loss: 0.2840862274169922\n",
      "Epoch 4607, Loss: 1.3703297823667526, Final Batch Loss: 0.2686794698238373\n",
      "Epoch 4608, Loss: 1.4988726377487183, Final Batch Loss: 0.3460586965084076\n",
      "Epoch 4609, Loss: 1.344618797302246, Final Batch Loss: 0.2697705328464508\n",
      "Epoch 4610, Loss: 1.484962821006775, Final Batch Loss: 0.35467445850372314\n",
      "Epoch 4611, Loss: 1.4278726428747177, Final Batch Loss: 0.24323083460330963\n",
      "Epoch 4612, Loss: 1.2552348673343658, Final Batch Loss: 0.3120366036891937\n",
      "Epoch 4613, Loss: 1.4294915795326233, Final Batch Loss: 0.2963865399360657\n",
      "Epoch 4614, Loss: 1.4624873101711273, Final Batch Loss: 0.24895527958869934\n",
      "Epoch 4615, Loss: 1.6687200665473938, Final Batch Loss: 0.3452000916004181\n",
      "Epoch 4616, Loss: 1.4634126275777817, Final Batch Loss: 0.4183407723903656\n",
      "Epoch 4617, Loss: 1.3829712271690369, Final Batch Loss: 0.2607702612876892\n",
      "Epoch 4618, Loss: 1.2198936939239502, Final Batch Loss: 0.25621724128723145\n",
      "Epoch 4619, Loss: 1.4197722673416138, Final Batch Loss: 0.2876945734024048\n",
      "Epoch 4620, Loss: 1.5096332132816315, Final Batch Loss: 0.3418550491333008\n",
      "Epoch 4621, Loss: 1.3762658387422562, Final Batch Loss: 0.25600841641426086\n",
      "Epoch 4622, Loss: 1.3337045013904572, Final Batch Loss: 0.2206079065799713\n",
      "Epoch 4623, Loss: 1.2805625349283218, Final Batch Loss: 0.2961440086364746\n",
      "Epoch 4624, Loss: 1.4796780496835709, Final Batch Loss: 0.30085882544517517\n",
      "Epoch 4625, Loss: 1.3973398655653, Final Batch Loss: 0.3341195285320282\n",
      "Epoch 4626, Loss: 1.2885015308856964, Final Batch Loss: 0.2578723430633545\n",
      "Epoch 4627, Loss: 1.6087678670883179, Final Batch Loss: 0.349073588848114\n",
      "Epoch 4628, Loss: 1.4697729647159576, Final Batch Loss: 0.36658596992492676\n",
      "Epoch 4629, Loss: 1.293746218085289, Final Batch Loss: 0.270408570766449\n",
      "Epoch 4630, Loss: 1.3246487081050873, Final Batch Loss: 0.22345417737960815\n",
      "Epoch 4631, Loss: 1.4253843426704407, Final Batch Loss: 0.15210944414138794\n",
      "Epoch 4632, Loss: 1.359362617135048, Final Batch Loss: 0.23892277479171753\n",
      "Epoch 4633, Loss: 1.407665804028511, Final Batch Loss: 0.24525107443332672\n",
      "Epoch 4634, Loss: 1.4282528758049011, Final Batch Loss: 0.2862638831138611\n",
      "Epoch 4635, Loss: 1.4357561320066452, Final Batch Loss: 0.20809881389141083\n",
      "Epoch 4636, Loss: 1.3723367303609848, Final Batch Loss: 0.2227015197277069\n",
      "Epoch 4637, Loss: 1.3861660808324814, Final Batch Loss: 0.22312146425247192\n",
      "Epoch 4638, Loss: 1.3117335736751556, Final Batch Loss: 0.2767658531665802\n",
      "Epoch 4639, Loss: 1.3991383463144302, Final Batch Loss: 0.32595616579055786\n",
      "Epoch 4640, Loss: 1.2648484110832214, Final Batch Loss: 0.2975611388683319\n",
      "Epoch 4641, Loss: 1.3888144195079803, Final Batch Loss: 0.29511964321136475\n",
      "Epoch 4642, Loss: 1.41822449862957, Final Batch Loss: 0.2834365963935852\n",
      "Epoch 4643, Loss: 1.397068053483963, Final Batch Loss: 0.2615739703178406\n",
      "Epoch 4644, Loss: 1.3821227699518204, Final Batch Loss: 0.28960317373275757\n",
      "Epoch 4645, Loss: 1.5767764300107956, Final Batch Loss: 0.24257120490074158\n",
      "Epoch 4646, Loss: 1.3846179842948914, Final Batch Loss: 0.27085164189338684\n",
      "Epoch 4647, Loss: 1.5127970278263092, Final Batch Loss: 0.30582642555236816\n",
      "Epoch 4648, Loss: 1.4166986793279648, Final Batch Loss: 0.3771226108074188\n",
      "Epoch 4649, Loss: 1.3498035073280334, Final Batch Loss: 0.3216294050216675\n",
      "Epoch 4650, Loss: 1.3428975343704224, Final Batch Loss: 0.298008531332016\n",
      "Epoch 4651, Loss: 1.3573076128959656, Final Batch Loss: 0.2861766219139099\n",
      "Epoch 4652, Loss: 1.4418022334575653, Final Batch Loss: 0.3151363730430603\n",
      "Epoch 4653, Loss: 1.4902641475200653, Final Batch Loss: 0.3656741678714752\n",
      "Epoch 4654, Loss: 1.2925955653190613, Final Batch Loss: 0.32126063108444214\n",
      "Epoch 4655, Loss: 1.434539869427681, Final Batch Loss: 0.31571412086486816\n",
      "Epoch 4656, Loss: 1.3849019408226013, Final Batch Loss: 0.2999316155910492\n",
      "Epoch 4657, Loss: 1.5008155852556229, Final Batch Loss: 0.3851528465747833\n",
      "Epoch 4658, Loss: 1.294472098350525, Final Batch Loss: 0.323942095041275\n",
      "Epoch 4659, Loss: 1.6358826160430908, Final Batch Loss: 0.25680989027023315\n",
      "Epoch 4660, Loss: 1.4130507856607437, Final Batch Loss: 0.23847456276416779\n",
      "Epoch 4661, Loss: 1.3348373919725418, Final Batch Loss: 0.3143090009689331\n",
      "Epoch 4662, Loss: 1.3539876639842987, Final Batch Loss: 0.242730051279068\n",
      "Epoch 4663, Loss: 1.3380630314350128, Final Batch Loss: 0.2618042826652527\n",
      "Epoch 4664, Loss: 1.308038979768753, Final Batch Loss: 0.29602673649787903\n",
      "Epoch 4665, Loss: 1.493783324956894, Final Batch Loss: 0.37811556458473206\n",
      "Epoch 4666, Loss: 1.3949849903583527, Final Batch Loss: 0.2319168895483017\n",
      "Epoch 4667, Loss: 1.3553532510995865, Final Batch Loss: 0.23350535333156586\n",
      "Epoch 4668, Loss: 1.3058345168828964, Final Batch Loss: 0.25547823309898376\n",
      "Epoch 4669, Loss: 1.3357840031385422, Final Batch Loss: 0.27776795625686646\n",
      "Epoch 4670, Loss: 1.4405369758605957, Final Batch Loss: 0.2756091356277466\n",
      "Epoch 4671, Loss: 1.3351593911647797, Final Batch Loss: 0.2593178153038025\n",
      "Epoch 4672, Loss: 1.3245850950479507, Final Batch Loss: 0.336073100566864\n",
      "Epoch 4673, Loss: 1.4757191836833954, Final Batch Loss: 0.42815712094306946\n",
      "Epoch 4674, Loss: 1.3858453035354614, Final Batch Loss: 0.3391215205192566\n",
      "Epoch 4675, Loss: 1.604290097951889, Final Batch Loss: 0.36245957016944885\n",
      "Epoch 4676, Loss: 1.4617811739444733, Final Batch Loss: 0.24157780408859253\n",
      "Epoch 4677, Loss: 1.381403163075447, Final Batch Loss: 0.22934041917324066\n",
      "Epoch 4678, Loss: 1.5474512726068497, Final Batch Loss: 0.2215624302625656\n",
      "Epoch 4679, Loss: 1.4975592195987701, Final Batch Loss: 0.37914153933525085\n",
      "Epoch 4680, Loss: 1.6111088693141937, Final Batch Loss: 0.41046640276908875\n",
      "Epoch 4681, Loss: 1.5254334509372711, Final Batch Loss: 0.2760937809944153\n",
      "Epoch 4682, Loss: 1.351308286190033, Final Batch Loss: 0.21607260406017303\n",
      "Epoch 4683, Loss: 1.4402477741241455, Final Batch Loss: 0.28033190965652466\n",
      "Epoch 4684, Loss: 1.5415161550045013, Final Batch Loss: 0.24979713559150696\n",
      "Epoch 4685, Loss: 1.3642107248306274, Final Batch Loss: 0.25071412324905396\n",
      "Epoch 4686, Loss: 1.3218829333782196, Final Batch Loss: 0.2312360554933548\n",
      "Epoch 4687, Loss: 1.3276327848434448, Final Batch Loss: 0.23886719346046448\n",
      "Epoch 4688, Loss: 1.4315151870250702, Final Batch Loss: 0.3352073132991791\n",
      "Epoch 4689, Loss: 1.3612235635519028, Final Batch Loss: 0.26890361309051514\n",
      "Epoch 4690, Loss: 1.6281669735908508, Final Batch Loss: 0.4646710157394409\n",
      "Epoch 4691, Loss: 1.3579848408699036, Final Batch Loss: 0.29937744140625\n",
      "Epoch 4692, Loss: 1.4493030309677124, Final Batch Loss: 0.2771977484226227\n",
      "Epoch 4693, Loss: 1.4563927352428436, Final Batch Loss: 0.3385283052921295\n",
      "Epoch 4694, Loss: 1.2618481516838074, Final Batch Loss: 0.14722678065299988\n",
      "Epoch 4695, Loss: 1.3598572909832, Final Batch Loss: 0.28920039534568787\n",
      "Epoch 4696, Loss: 1.488657295703888, Final Batch Loss: 0.24275991320610046\n",
      "Epoch 4697, Loss: 1.3101750612258911, Final Batch Loss: 0.22204944491386414\n",
      "Epoch 4698, Loss: 1.3415866941213608, Final Batch Loss: 0.23056769371032715\n",
      "Epoch 4699, Loss: 1.3956207782030106, Final Batch Loss: 0.22589409351348877\n",
      "Epoch 4700, Loss: 1.3196378648281097, Final Batch Loss: 0.2524227499961853\n",
      "Epoch 4701, Loss: 1.463446095585823, Final Batch Loss: 0.3670956790447235\n",
      "Epoch 4702, Loss: 1.3983573913574219, Final Batch Loss: 0.24581043422222137\n",
      "Epoch 4703, Loss: 1.4209517240524292, Final Batch Loss: 0.238243967294693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4704, Loss: 1.40388223528862, Final Batch Loss: 0.3343307375907898\n",
      "Epoch 4705, Loss: 1.5142685174942017, Final Batch Loss: 0.23805150389671326\n",
      "Epoch 4706, Loss: 1.3401524722576141, Final Batch Loss: 0.31655600666999817\n",
      "Epoch 4707, Loss: 1.502029538154602, Final Batch Loss: 0.3724460303783417\n",
      "Epoch 4708, Loss: 1.412722334265709, Final Batch Loss: 0.2869187593460083\n",
      "Epoch 4709, Loss: 1.454043760895729, Final Batch Loss: 0.24469523131847382\n",
      "Epoch 4710, Loss: 1.3200305104255676, Final Batch Loss: 0.21288418769836426\n",
      "Epoch 4711, Loss: 1.3619694113731384, Final Batch Loss: 0.23574811220169067\n",
      "Epoch 4712, Loss: 1.3799008429050446, Final Batch Loss: 0.23866485059261322\n",
      "Epoch 4713, Loss: 1.6278861463069916, Final Batch Loss: 0.4625062644481659\n",
      "Epoch 4714, Loss: 1.3892932683229446, Final Batch Loss: 0.3509638011455536\n",
      "Epoch 4715, Loss: 1.488582968711853, Final Batch Loss: 0.2612456679344177\n",
      "Epoch 4716, Loss: 1.4429599344730377, Final Batch Loss: 0.2559073865413666\n",
      "Epoch 4717, Loss: 1.33087520301342, Final Batch Loss: 0.24823139607906342\n",
      "Epoch 4718, Loss: 1.4547324031591415, Final Batch Loss: 0.23270829021930695\n",
      "Epoch 4719, Loss: 1.3976836502552032, Final Batch Loss: 0.29576995968818665\n",
      "Epoch 4720, Loss: 1.3420019894838333, Final Batch Loss: 0.1906735599040985\n",
      "Epoch 4721, Loss: 1.3832543343305588, Final Batch Loss: 0.2489258348941803\n",
      "Epoch 4722, Loss: 1.4193793535232544, Final Batch Loss: 0.2109985500574112\n",
      "Epoch 4723, Loss: 1.374096840620041, Final Batch Loss: 0.2577116787433624\n",
      "Epoch 4724, Loss: 1.306750163435936, Final Batch Loss: 0.19943559169769287\n",
      "Epoch 4725, Loss: 1.5054535865783691, Final Batch Loss: 0.38706281781196594\n",
      "Epoch 4726, Loss: 1.3767477571964264, Final Batch Loss: 0.2955797016620636\n",
      "Epoch 4727, Loss: 1.3813404589891434, Final Batch Loss: 0.288318008184433\n",
      "Epoch 4728, Loss: 1.425511121749878, Final Batch Loss: 0.31805136799812317\n",
      "Epoch 4729, Loss: 1.3830361515283585, Final Batch Loss: 0.28755173087120056\n",
      "Epoch 4730, Loss: 1.3588239550590515, Final Batch Loss: 0.2924033999443054\n",
      "Epoch 4731, Loss: 1.4277078211307526, Final Batch Loss: 0.2724446952342987\n",
      "Epoch 4732, Loss: 1.497853696346283, Final Batch Loss: 0.27050134539604187\n",
      "Epoch 4733, Loss: 1.424173280596733, Final Batch Loss: 0.2706553041934967\n",
      "Epoch 4734, Loss: 1.4926815032958984, Final Batch Loss: 0.29356345534324646\n",
      "Epoch 4735, Loss: 1.4518277049064636, Final Batch Loss: 0.34119680523872375\n",
      "Epoch 4736, Loss: 1.230974942445755, Final Batch Loss: 0.13669706881046295\n",
      "Epoch 4737, Loss: 1.4914825111627579, Final Batch Loss: 0.32535111904144287\n",
      "Epoch 4738, Loss: 1.3660976886749268, Final Batch Loss: 0.27678242325782776\n",
      "Epoch 4739, Loss: 1.372335895895958, Final Batch Loss: 0.30311092734336853\n",
      "Epoch 4740, Loss: 1.3534561097621918, Final Batch Loss: 0.2645382881164551\n",
      "Epoch 4741, Loss: 1.4777968525886536, Final Batch Loss: 0.28508663177490234\n",
      "Epoch 4742, Loss: 1.2865031957626343, Final Batch Loss: 0.2051304131746292\n",
      "Epoch 4743, Loss: 1.4165859073400497, Final Batch Loss: 0.32172486186027527\n",
      "Epoch 4744, Loss: 1.6642691791057587, Final Batch Loss: 0.5113974213600159\n",
      "Epoch 4745, Loss: 1.4692920595407486, Final Batch Loss: 0.30638357996940613\n",
      "Epoch 4746, Loss: 1.4120597690343857, Final Batch Loss: 0.24177177250385284\n",
      "Epoch 4747, Loss: 1.4024791419506073, Final Batch Loss: 0.31204482913017273\n",
      "Epoch 4748, Loss: 1.5165716111660004, Final Batch Loss: 0.3523664176464081\n",
      "Epoch 4749, Loss: 1.3114833384752274, Final Batch Loss: 0.28442689776420593\n",
      "Epoch 4750, Loss: 1.5154836475849152, Final Batch Loss: 0.3023652732372284\n",
      "Epoch 4751, Loss: 1.2710414975881577, Final Batch Loss: 0.1864398568868637\n",
      "Epoch 4752, Loss: 1.354325145483017, Final Batch Loss: 0.22973546385765076\n",
      "Epoch 4753, Loss: 1.3682579696178436, Final Batch Loss: 0.33364564180374146\n",
      "Epoch 4754, Loss: 1.4679969400167465, Final Batch Loss: 0.3390634059906006\n",
      "Epoch 4755, Loss: 1.3722188472747803, Final Batch Loss: 0.28633058071136475\n",
      "Epoch 4756, Loss: 1.4646912515163422, Final Batch Loss: 0.4131011664867401\n",
      "Epoch 4757, Loss: 1.3669358938932419, Final Batch Loss: 0.3391607701778412\n",
      "Epoch 4758, Loss: 1.4337219297885895, Final Batch Loss: 0.3109544515609741\n",
      "Epoch 4759, Loss: 1.4612199068069458, Final Batch Loss: 0.30380943417549133\n",
      "Epoch 4760, Loss: 1.3811724483966827, Final Batch Loss: 0.2927238345146179\n",
      "Epoch 4761, Loss: 1.4574429392814636, Final Batch Loss: 0.32969018816947937\n",
      "Epoch 4762, Loss: 1.4281712025403976, Final Batch Loss: 0.25522544980049133\n",
      "Epoch 4763, Loss: 1.380046859383583, Final Batch Loss: 0.2650929093360901\n",
      "Epoch 4764, Loss: 1.373023435473442, Final Batch Loss: 0.21176773309707642\n",
      "Epoch 4765, Loss: 1.3920026421546936, Final Batch Loss: 0.2556128203868866\n",
      "Epoch 4766, Loss: 1.4156879633665085, Final Batch Loss: 0.23986148834228516\n",
      "Epoch 4767, Loss: 1.4059509336948395, Final Batch Loss: 0.3410991430282593\n",
      "Epoch 4768, Loss: 1.2999355345964432, Final Batch Loss: 0.30162057280540466\n",
      "Epoch 4769, Loss: 1.4100878536701202, Final Batch Loss: 0.30389803647994995\n",
      "Epoch 4770, Loss: 1.5333724319934845, Final Batch Loss: 0.2940962612628937\n",
      "Epoch 4771, Loss: 1.451181560754776, Final Batch Loss: 0.2922912836074829\n",
      "Epoch 4772, Loss: 1.3594809919595718, Final Batch Loss: 0.2757267951965332\n",
      "Epoch 4773, Loss: 1.3893031626939774, Final Batch Loss: 0.24423860013484955\n",
      "Epoch 4774, Loss: 1.4200499206781387, Final Batch Loss: 0.3226523697376251\n",
      "Epoch 4775, Loss: 1.3599810749292374, Final Batch Loss: 0.2730114758014679\n",
      "Epoch 4776, Loss: 1.3822605907917023, Final Batch Loss: 0.28144437074661255\n",
      "Epoch 4777, Loss: 1.2881735116243362, Final Batch Loss: 0.30191007256507874\n",
      "Epoch 4778, Loss: 1.2022954374551773, Final Batch Loss: 0.21542960405349731\n",
      "Epoch 4779, Loss: 1.4070276319980621, Final Batch Loss: 0.3012445271015167\n",
      "Epoch 4780, Loss: 1.4155070781707764, Final Batch Loss: 0.29540008306503296\n",
      "Epoch 4781, Loss: 1.3697810471057892, Final Batch Loss: 0.3272262513637543\n",
      "Epoch 4782, Loss: 1.4503081291913986, Final Batch Loss: 0.33500269055366516\n",
      "Epoch 4783, Loss: 1.4051911383867264, Final Batch Loss: 0.23881451785564423\n",
      "Epoch 4784, Loss: 1.4734328240156174, Final Batch Loss: 0.31008610129356384\n",
      "Epoch 4785, Loss: 1.3615982085466385, Final Batch Loss: 0.23884953558444977\n",
      "Epoch 4786, Loss: 1.3367328941822052, Final Batch Loss: 0.2535255551338196\n",
      "Epoch 4787, Loss: 1.2575131207704544, Final Batch Loss: 0.20648521184921265\n",
      "Epoch 4788, Loss: 1.194368988275528, Final Batch Loss: 0.2643446922302246\n",
      "Epoch 4789, Loss: 1.3058338165283203, Final Batch Loss: 0.30490273237228394\n",
      "Epoch 4790, Loss: 1.2981211990118027, Final Batch Loss: 0.20132362842559814\n",
      "Epoch 4791, Loss: 1.3257358074188232, Final Batch Loss: 0.298573762178421\n",
      "Epoch 4792, Loss: 1.3311478793621063, Final Batch Loss: 0.2729129493236542\n",
      "Epoch 4793, Loss: 1.2731718122959137, Final Batch Loss: 0.2380974292755127\n",
      "Epoch 4794, Loss: 1.44681715965271, Final Batch Loss: 0.3019762337207794\n",
      "Epoch 4795, Loss: 1.4589909613132477, Final Batch Loss: 0.2325403094291687\n",
      "Epoch 4796, Loss: 1.3842924535274506, Final Batch Loss: 0.2662259638309479\n",
      "Epoch 4797, Loss: 1.3323045670986176, Final Batch Loss: 0.3291957974433899\n",
      "Epoch 4798, Loss: 1.357708528637886, Final Batch Loss: 0.2379222959280014\n",
      "Epoch 4799, Loss: 1.3915510475635529, Final Batch Loss: 0.2535470426082611\n",
      "Epoch 4800, Loss: 1.3575124591588974, Final Batch Loss: 0.2455424815416336\n",
      "Epoch 4801, Loss: 1.4369891285896301, Final Batch Loss: 0.3655785918235779\n",
      "Epoch 4802, Loss: 1.3506070971488953, Final Batch Loss: 0.32053354382514954\n",
      "Epoch 4803, Loss: 1.3881151676177979, Final Batch Loss: 0.23227128386497498\n",
      "Epoch 4804, Loss: 1.3382517099380493, Final Batch Loss: 0.26457861065864563\n",
      "Epoch 4805, Loss: 1.462752491235733, Final Batch Loss: 0.33911147713661194\n",
      "Epoch 4806, Loss: 1.4569312632083893, Final Batch Loss: 0.24685461819171906\n",
      "Epoch 4807, Loss: 1.3851300925016403, Final Batch Loss: 0.2931581735610962\n",
      "Epoch 4808, Loss: 1.3286438137292862, Final Batch Loss: 0.2458553910255432\n",
      "Epoch 4809, Loss: 1.268158733844757, Final Batch Loss: 0.16920354962348938\n",
      "Epoch 4810, Loss: 1.3368481695652008, Final Batch Loss: 0.26862993836402893\n",
      "Epoch 4811, Loss: 1.5592974722385406, Final Batch Loss: 0.2857343554496765\n",
      "Epoch 4812, Loss: 1.3324130326509476, Final Batch Loss: 0.3268371820449829\n",
      "Epoch 4813, Loss: 1.3917382955551147, Final Batch Loss: 0.28244587779045105\n",
      "Epoch 4814, Loss: 1.3338639587163925, Final Batch Loss: 0.2298574298620224\n",
      "Epoch 4815, Loss: 1.3191794157028198, Final Batch Loss: 0.2601000666618347\n",
      "Epoch 4816, Loss: 1.3913586884737015, Final Batch Loss: 0.2795827388763428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4817, Loss: 1.3726776838302612, Final Batch Loss: 0.24008594453334808\n",
      "Epoch 4818, Loss: 1.3359842002391815, Final Batch Loss: 0.23973402380943298\n",
      "Epoch 4819, Loss: 1.2302511185407639, Final Batch Loss: 0.24165058135986328\n",
      "Epoch 4820, Loss: 1.360974445939064, Final Batch Loss: 0.20118875801563263\n",
      "Epoch 4821, Loss: 1.4147828370332718, Final Batch Loss: 0.3095092475414276\n",
      "Epoch 4822, Loss: 1.6417720019817352, Final Batch Loss: 0.3985663950443268\n",
      "Epoch 4823, Loss: 1.378714844584465, Final Batch Loss: 0.27037209272384644\n",
      "Epoch 4824, Loss: 1.4727988690137863, Final Batch Loss: 0.39866727590560913\n",
      "Epoch 4825, Loss: 1.2987442910671234, Final Batch Loss: 0.25449514389038086\n",
      "Epoch 4826, Loss: 1.3864396661520004, Final Batch Loss: 0.22316084802150726\n",
      "Epoch 4827, Loss: 1.4454704821109772, Final Batch Loss: 0.25118353962898254\n",
      "Epoch 4828, Loss: 1.2221512645483017, Final Batch Loss: 0.2405805140733719\n",
      "Epoch 4829, Loss: 1.4121105074882507, Final Batch Loss: 0.4148446321487427\n",
      "Epoch 4830, Loss: 1.3718245327472687, Final Batch Loss: 0.2780740261077881\n",
      "Epoch 4831, Loss: 1.5261582881212234, Final Batch Loss: 0.254752516746521\n",
      "Epoch 4832, Loss: 1.3429989516735077, Final Batch Loss: 0.3331916332244873\n",
      "Epoch 4833, Loss: 1.276565045118332, Final Batch Loss: 0.22401604056358337\n",
      "Epoch 4834, Loss: 1.4241675734519958, Final Batch Loss: 0.25640398263931274\n",
      "Epoch 4835, Loss: 1.5030175149440765, Final Batch Loss: 0.35577741265296936\n",
      "Epoch 4836, Loss: 1.4216804802417755, Final Batch Loss: 0.29854616522789\n",
      "Epoch 4837, Loss: 1.3097285330295563, Final Batch Loss: 0.23314106464385986\n",
      "Epoch 4838, Loss: 1.3512894362211227, Final Batch Loss: 0.3632129728794098\n",
      "Epoch 4839, Loss: 1.4239470660686493, Final Batch Loss: 0.3194018006324768\n",
      "Epoch 4840, Loss: 1.288418397307396, Final Batch Loss: 0.2557968199253082\n",
      "Epoch 4841, Loss: 1.2403303682804108, Final Batch Loss: 0.24111583828926086\n",
      "Epoch 4842, Loss: 1.3592466861009598, Final Batch Loss: 0.2096552848815918\n",
      "Epoch 4843, Loss: 1.4745166450738907, Final Batch Loss: 0.38104695081710815\n",
      "Epoch 4844, Loss: 1.4327588379383087, Final Batch Loss: 0.2550543546676636\n",
      "Epoch 4845, Loss: 1.2741547524929047, Final Batch Loss: 0.26788970828056335\n",
      "Epoch 4846, Loss: 1.2840956449508667, Final Batch Loss: 0.2089555710554123\n",
      "Epoch 4847, Loss: 1.3283319473266602, Final Batch Loss: 0.23719695210456848\n",
      "Epoch 4848, Loss: 1.4579533636569977, Final Batch Loss: 0.2634027302265167\n",
      "Epoch 4849, Loss: 1.500727266073227, Final Batch Loss: 0.3246650993824005\n",
      "Epoch 4850, Loss: 1.5324812978506088, Final Batch Loss: 0.19379349052906036\n",
      "Epoch 4851, Loss: 1.2566581517457962, Final Batch Loss: 0.27441489696502686\n",
      "Epoch 4852, Loss: 1.2645128071308136, Final Batch Loss: 0.19880008697509766\n",
      "Epoch 4853, Loss: 1.3688960075378418, Final Batch Loss: 0.31621742248535156\n",
      "Epoch 4854, Loss: 1.3669505566358566, Final Batch Loss: 0.24673424661159515\n",
      "Epoch 4855, Loss: 1.3340318500995636, Final Batch Loss: 0.3256497085094452\n",
      "Epoch 4856, Loss: 1.3770267814397812, Final Batch Loss: 0.3284960985183716\n",
      "Epoch 4857, Loss: 1.2370475828647614, Final Batch Loss: 0.25485116243362427\n",
      "Epoch 4858, Loss: 1.4197111427783966, Final Batch Loss: 0.31989479064941406\n",
      "Epoch 4859, Loss: 1.3728217035531998, Final Batch Loss: 0.22889196872711182\n",
      "Epoch 4860, Loss: 1.4787333607673645, Final Batch Loss: 0.3928912878036499\n",
      "Epoch 4861, Loss: 1.3523143380880356, Final Batch Loss: 0.27275121212005615\n",
      "Epoch 4862, Loss: 1.4704675078392029, Final Batch Loss: 0.40065833926200867\n",
      "Epoch 4863, Loss: 1.2722619473934174, Final Batch Loss: 0.27008283138275146\n",
      "Epoch 4864, Loss: 1.3484091609716415, Final Batch Loss: 0.27426424622535706\n",
      "Epoch 4865, Loss: 1.390739992260933, Final Batch Loss: 0.3228897154331207\n",
      "Epoch 4866, Loss: 1.4847357422113419, Final Batch Loss: 0.3312391936779022\n",
      "Epoch 4867, Loss: 1.3746434599161148, Final Batch Loss: 0.28220683336257935\n",
      "Epoch 4868, Loss: 1.3228136152029037, Final Batch Loss: 0.30944713950157166\n",
      "Epoch 4869, Loss: 1.3315780609846115, Final Batch Loss: 0.2378578931093216\n",
      "Epoch 4870, Loss: 1.3168840855360031, Final Batch Loss: 0.24114041030406952\n",
      "Epoch 4871, Loss: 1.3561171144247055, Final Batch Loss: 0.2136368453502655\n",
      "Epoch 4872, Loss: 1.3308724164962769, Final Batch Loss: 0.26247793436050415\n",
      "Epoch 4873, Loss: 1.4127702564001083, Final Batch Loss: 0.24147869646549225\n",
      "Epoch 4874, Loss: 1.3314739912748337, Final Batch Loss: 0.24641872942447662\n",
      "Epoch 4875, Loss: 1.266526997089386, Final Batch Loss: 0.20208050310611725\n",
      "Epoch 4876, Loss: 1.2251606434583664, Final Batch Loss: 0.23792003095149994\n",
      "Epoch 4877, Loss: 1.3189764022827148, Final Batch Loss: 0.2537645101547241\n",
      "Epoch 4878, Loss: 1.3212763518095016, Final Batch Loss: 0.2882356345653534\n",
      "Epoch 4879, Loss: 1.3236049264669418, Final Batch Loss: 0.23175592720508575\n",
      "Epoch 4880, Loss: 1.342568352818489, Final Batch Loss: 0.2194713056087494\n",
      "Epoch 4881, Loss: 1.385609745979309, Final Batch Loss: 0.2714957892894745\n",
      "Epoch 4882, Loss: 1.3801734745502472, Final Batch Loss: 0.22565992176532745\n",
      "Epoch 4883, Loss: 1.314660757780075, Final Batch Loss: 0.2766415774822235\n",
      "Epoch 4884, Loss: 1.3852820247411728, Final Batch Loss: 0.2881627380847931\n",
      "Epoch 4885, Loss: 1.4653471261262894, Final Batch Loss: 0.2963246703147888\n",
      "Epoch 4886, Loss: 1.4230623692274094, Final Batch Loss: 0.22713123261928558\n",
      "Epoch 4887, Loss: 1.3889473527669907, Final Batch Loss: 0.2289762794971466\n",
      "Epoch 4888, Loss: 1.3772228509187698, Final Batch Loss: 0.2260146588087082\n",
      "Epoch 4889, Loss: 1.325243592262268, Final Batch Loss: 0.23280474543571472\n",
      "Epoch 4890, Loss: 1.2612746059894562, Final Batch Loss: 0.2721289396286011\n",
      "Epoch 4891, Loss: 1.2895286828279495, Final Batch Loss: 0.18868255615234375\n",
      "Epoch 4892, Loss: 1.4711605906486511, Final Batch Loss: 0.3298753798007965\n",
      "Epoch 4893, Loss: 1.294626846909523, Final Batch Loss: 0.23174919188022614\n",
      "Epoch 4894, Loss: 1.2399678379297256, Final Batch Loss: 0.27938196063041687\n",
      "Epoch 4895, Loss: 1.2310047298669815, Final Batch Loss: 0.2773132622241974\n",
      "Epoch 4896, Loss: 1.2504864633083344, Final Batch Loss: 0.31089186668395996\n",
      "Epoch 4897, Loss: 1.4172334223985672, Final Batch Loss: 0.23444192111492157\n",
      "Epoch 4898, Loss: 1.347012683749199, Final Batch Loss: 0.1902586668729782\n",
      "Epoch 4899, Loss: 1.3836271315813065, Final Batch Loss: 0.3195640444755554\n",
      "Epoch 4900, Loss: 1.4242383390665054, Final Batch Loss: 0.24428395926952362\n",
      "Epoch 4901, Loss: 1.3618102073669434, Final Batch Loss: 0.25552722811698914\n",
      "Epoch 4902, Loss: 1.2933205664157867, Final Batch Loss: 0.19563886523246765\n",
      "Epoch 4903, Loss: 1.4335564970970154, Final Batch Loss: 0.23639214038848877\n",
      "Epoch 4904, Loss: 1.306641235947609, Final Batch Loss: 0.20226876437664032\n",
      "Epoch 4905, Loss: 1.3072887659072876, Final Batch Loss: 0.2751821279525757\n",
      "Epoch 4906, Loss: 1.4277398139238358, Final Batch Loss: 0.2924671173095703\n",
      "Epoch 4907, Loss: 1.3552247136831284, Final Batch Loss: 0.24328523874282837\n",
      "Epoch 4908, Loss: 1.4133507013320923, Final Batch Loss: 0.31199032068252563\n",
      "Epoch 4909, Loss: 1.175381362438202, Final Batch Loss: 0.19767890870571136\n",
      "Epoch 4910, Loss: 1.3039999157190323, Final Batch Loss: 0.22973233461380005\n",
      "Epoch 4911, Loss: 1.3694941848516464, Final Batch Loss: 0.247550830245018\n",
      "Epoch 4912, Loss: 1.4214725196361542, Final Batch Loss: 0.31677505373954773\n",
      "Epoch 4913, Loss: 1.2852576971054077, Final Batch Loss: 0.2018018662929535\n",
      "Epoch 4914, Loss: 1.346359759569168, Final Batch Loss: 0.28640952706336975\n",
      "Epoch 4915, Loss: 1.3627728670835495, Final Batch Loss: 0.24730269610881805\n",
      "Epoch 4916, Loss: 1.3572749346494675, Final Batch Loss: 0.23777060210704803\n",
      "Epoch 4917, Loss: 1.348720908164978, Final Batch Loss: 0.26090285181999207\n",
      "Epoch 4918, Loss: 1.3836398720741272, Final Batch Loss: 0.24033671617507935\n",
      "Epoch 4919, Loss: 1.3347205966711044, Final Batch Loss: 0.23704108595848083\n",
      "Epoch 4920, Loss: 1.4807265549898148, Final Batch Loss: 0.3218936324119568\n",
      "Epoch 4921, Loss: 1.4151359796524048, Final Batch Loss: 0.30350461602211\n",
      "Epoch 4922, Loss: 1.3752371072769165, Final Batch Loss: 0.26938045024871826\n",
      "Epoch 4923, Loss: 1.3142888247966766, Final Batch Loss: 0.23157116770744324\n",
      "Epoch 4924, Loss: 1.3876374959945679, Final Batch Loss: 0.2611212134361267\n",
      "Epoch 4925, Loss: 1.3308701515197754, Final Batch Loss: 0.19202502071857452\n",
      "Epoch 4926, Loss: 1.2380999028682709, Final Batch Loss: 0.2142745405435562\n",
      "Epoch 4927, Loss: 1.472012221813202, Final Batch Loss: 0.40001994371414185\n",
      "Epoch 4928, Loss: 1.1849079728126526, Final Batch Loss: 0.17328862845897675\n",
      "Epoch 4929, Loss: 1.3480660021305084, Final Batch Loss: 0.20559081435203552\n",
      "Epoch 4930, Loss: 1.3131321370601654, Final Batch Loss: 0.26230689883232117\n",
      "Epoch 4931, Loss: 1.2897970080375671, Final Batch Loss: 0.274954229593277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4932, Loss: 1.2748288810253143, Final Batch Loss: 0.3089393079280853\n",
      "Epoch 4933, Loss: 1.404331773519516, Final Batch Loss: 0.3732761740684509\n",
      "Epoch 4934, Loss: 1.2747384011745453, Final Batch Loss: 0.2681877017021179\n",
      "Epoch 4935, Loss: 1.3502983748912811, Final Batch Loss: 0.32182154059410095\n",
      "Epoch 4936, Loss: 1.4673156440258026, Final Batch Loss: 0.3570214509963989\n",
      "Epoch 4937, Loss: 1.2973094582557678, Final Batch Loss: 0.26664671301841736\n",
      "Epoch 4938, Loss: 1.4843515753746033, Final Batch Loss: 0.31237560510635376\n",
      "Epoch 4939, Loss: 1.5379494577646255, Final Batch Loss: 0.4421100616455078\n",
      "Epoch 4940, Loss: 1.381883680820465, Final Batch Loss: 0.25995883345603943\n",
      "Epoch 4941, Loss: 1.420852705836296, Final Batch Loss: 0.22433942556381226\n",
      "Epoch 4942, Loss: 1.172381803393364, Final Batch Loss: 0.26922938227653503\n",
      "Epoch 4943, Loss: 1.3209745287895203, Final Batch Loss: 0.24417488276958466\n",
      "Epoch 4944, Loss: 1.296840250492096, Final Batch Loss: 0.25074347853660583\n",
      "Epoch 4945, Loss: 1.3015708476305008, Final Batch Loss: 0.23348723351955414\n",
      "Epoch 4946, Loss: 1.235187515616417, Final Batch Loss: 0.23238401114940643\n",
      "Epoch 4947, Loss: 1.2024473547935486, Final Batch Loss: 0.24448636174201965\n",
      "Epoch 4948, Loss: 1.3031290024518967, Final Batch Loss: 0.22252129018306732\n",
      "Epoch 4949, Loss: 1.4548375606536865, Final Batch Loss: 0.33138608932495117\n",
      "Epoch 4950, Loss: 1.4397819936275482, Final Batch Loss: 0.28815022110939026\n",
      "Epoch 4951, Loss: 1.2610178291797638, Final Batch Loss: 0.2225583791732788\n",
      "Epoch 4952, Loss: 1.5022462010383606, Final Batch Loss: 0.34728503227233887\n",
      "Epoch 4953, Loss: 1.3834284991025925, Final Batch Loss: 0.24007835984230042\n",
      "Epoch 4954, Loss: 1.4011887162923813, Final Batch Loss: 0.28473925590515137\n",
      "Epoch 4955, Loss: 1.2790158987045288, Final Batch Loss: 0.20569999516010284\n",
      "Epoch 4956, Loss: 1.5061991214752197, Final Batch Loss: 0.3079693615436554\n",
      "Epoch 4957, Loss: 1.2857917249202728, Final Batch Loss: 0.236809641122818\n",
      "Epoch 4958, Loss: 1.2761318236589432, Final Batch Loss: 0.24956904351711273\n",
      "Epoch 4959, Loss: 1.143230989575386, Final Batch Loss: 0.23814192414283752\n",
      "Epoch 4960, Loss: 1.42966528236866, Final Batch Loss: 0.28950658440589905\n",
      "Epoch 4961, Loss: 1.4188758730888367, Final Batch Loss: 0.34158647060394287\n",
      "Epoch 4962, Loss: 1.5211980640888214, Final Batch Loss: 0.39816632866859436\n",
      "Epoch 4963, Loss: 1.4426893591880798, Final Batch Loss: 0.35160958766937256\n",
      "Epoch 4964, Loss: 1.3422530442476273, Final Batch Loss: 0.2798391878604889\n",
      "Epoch 4965, Loss: 1.4491848051548004, Final Batch Loss: 0.2639518082141876\n",
      "Epoch 4966, Loss: 1.3900386542081833, Final Batch Loss: 0.2927002012729645\n",
      "Epoch 4967, Loss: 1.2405288964509964, Final Batch Loss: 0.29070886969566345\n",
      "Epoch 4968, Loss: 1.4428420662879944, Final Batch Loss: 0.3049662113189697\n",
      "Epoch 4969, Loss: 1.3660341799259186, Final Batch Loss: 0.26878127455711365\n",
      "Epoch 4970, Loss: 1.2523734271526337, Final Batch Loss: 0.27633532881736755\n",
      "Epoch 4971, Loss: 1.4211200922727585, Final Batch Loss: 0.3442405164241791\n",
      "Epoch 4972, Loss: 1.4124476462602615, Final Batch Loss: 0.2950161397457123\n",
      "Epoch 4973, Loss: 1.345737874507904, Final Batch Loss: 0.26187336444854736\n",
      "Epoch 4974, Loss: 1.381760135293007, Final Batch Loss: 0.3378226161003113\n",
      "Epoch 4975, Loss: 1.4937162846326828, Final Batch Loss: 0.3403871953487396\n",
      "Epoch 4976, Loss: 1.4634357690811157, Final Batch Loss: 0.2833491265773773\n",
      "Epoch 4977, Loss: 1.2471999824047089, Final Batch Loss: 0.24151216447353363\n",
      "Epoch 4978, Loss: 1.5108033418655396, Final Batch Loss: 0.30918559432029724\n",
      "Epoch 4979, Loss: 1.243157982826233, Final Batch Loss: 0.13857266306877136\n",
      "Epoch 4980, Loss: 1.3621608763933182, Final Batch Loss: 0.33148065209388733\n",
      "Epoch 4981, Loss: 1.4002122282981873, Final Batch Loss: 0.22871722280979156\n",
      "Epoch 4982, Loss: 1.4208612740039825, Final Batch Loss: 0.3348025977611542\n",
      "Epoch 4983, Loss: 1.4649135172367096, Final Batch Loss: 0.27533531188964844\n",
      "Epoch 4984, Loss: 1.2760065197944641, Final Batch Loss: 0.16148293018341064\n",
      "Epoch 4985, Loss: 1.279433861374855, Final Batch Loss: 0.2270849049091339\n",
      "Epoch 4986, Loss: 1.3196191936731339, Final Batch Loss: 0.248824343085289\n",
      "Epoch 4987, Loss: 1.3873983025550842, Final Batch Loss: 0.24191008508205414\n",
      "Epoch 4988, Loss: 1.2592128962278366, Final Batch Loss: 0.2539994716644287\n",
      "Epoch 4989, Loss: 1.2906082421541214, Final Batch Loss: 0.26772913336753845\n",
      "Epoch 4990, Loss: 1.1946483105421066, Final Batch Loss: 0.1869753748178482\n",
      "Epoch 4991, Loss: 1.2858838438987732, Final Batch Loss: 0.1730785071849823\n",
      "Epoch 4992, Loss: 1.3515631705522537, Final Batch Loss: 0.15929070115089417\n",
      "Epoch 4993, Loss: 1.3348572701215744, Final Batch Loss: 0.2851216197013855\n",
      "Epoch 4994, Loss: 1.2778979688882828, Final Batch Loss: 0.19438402354717255\n",
      "Epoch 4995, Loss: 1.3922389149665833, Final Batch Loss: 0.28907135128974915\n",
      "Epoch 4996, Loss: 1.3537678718566895, Final Batch Loss: 0.3648311197757721\n",
      "Epoch 4997, Loss: 1.5006475746631622, Final Batch Loss: 0.28115665912628174\n",
      "Epoch 4998, Loss: 1.409119039773941, Final Batch Loss: 0.26005783677101135\n",
      "Epoch 4999, Loss: 1.4991803765296936, Final Batch Loss: 0.30659595131874084\n",
      "Epoch 5000, Loss: 1.3465487360954285, Final Batch Loss: 0.26670950651168823\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   1  0  0]\n",
      " [ 0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  1]\n",
      " [ 0  0  0 15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  2  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  1\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  9  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  5  0  0  1  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0 14  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 13  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "   0  0  0]\n",
      " [ 2  0  0  0  0  0  0  0  0  0  0  0  6  1  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  2  0  0  1  0  0  2  0  0  1  0  0  2  0  0  0  0  0  0  0  0  0\n",
      "   0  0  4]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  6  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  3  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  0  1  0  0  0  0  0  0  0  0  5  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0\n",
      "   0  2  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  1  1  0  0  0  0  0  0  0  0  0  0\n",
      "   4  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0 10  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0 12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.92000   1.00000   0.95833        23\n",
      "           1    1.00000   0.87500   0.93333         8\n",
      "           2    0.80000   0.88889   0.84211         9\n",
      "           3    1.00000   1.00000   1.00000        15\n",
      "           4    0.90000   1.00000   0.94737         9\n",
      "           5    0.40000   0.50000   0.44444         4\n",
      "           6    1.00000   0.90000   0.94737        10\n",
      "           7    1.00000   1.00000   1.00000         9\n",
      "           8    0.41667   0.71429   0.52632         7\n",
      "           9    1.00000   1.00000   1.00000        17\n",
      "          10    0.93333   0.93333   0.93333        15\n",
      "          11    0.76471   0.92857   0.83871        14\n",
      "          12    0.85714   0.66667   0.75000         9\n",
      "          13    0.81818   1.00000   0.90000         9\n",
      "          14    0.66667   0.16667   0.26667        12\n",
      "          15    1.00000   1.00000   1.00000        10\n",
      "          16    1.00000   0.85714   0.92308         7\n",
      "          17    1.00000   0.71429   0.83333        14\n",
      "          18    1.00000   1.00000   1.00000        10\n",
      "          19    1.00000   1.00000   1.00000         8\n",
      "          20    0.83333   0.71429   0.76923         7\n",
      "          21    1.00000   1.00000   1.00000        11\n",
      "          22    1.00000   0.77778   0.87500         9\n",
      "          23    0.92308   1.00000   0.96000        12\n",
      "          24    0.80000   0.66667   0.72727         6\n",
      "          25    0.83333   1.00000   0.90909        10\n",
      "          26    0.70588   0.92308   0.80000        13\n",
      "\n",
      "    accuracy                        0.88153       287\n",
      "   macro avg    0.87305   0.86025   0.85500       287\n",
      "weighted avg    0.89192   0.88153   0.87493       287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.train()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Fake Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20,000 epochs DEFAULT PARAMETERS FOR THRESHOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (gen): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=112, out_features=80, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=80, out_features=60, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=60, out_features=50, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Linear(in_features=50, out_features=30, bias=True)\n",
       "    (4): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator(z_dim = 112)\n",
    "load_model(gen, \"3 Label 9 Subject GAN Ablation_gen.param\")\n",
    "gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(X_test)\n",
    "latent_vectors = get_noise(size, 100)\n",
    "act_vectors = get_act_matrix(size, 3)\n",
    "usr_vectors = get_usr_matrix(size, 9)\n",
    "\n",
    "fake_labels = []\n",
    "\n",
    "for k in range(size):\n",
    "    if act_vectors[0][k] == 0 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(0)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(1)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(2)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(3)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(4)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(5)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(6)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(7)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(8)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 3:\n",
    "        fake_labels.append(9)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 3:\n",
    "        fake_labels.append(10)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 3:\n",
    "        fake_labels.append(11)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 4:\n",
    "        fake_labels.append(12)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 4:\n",
    "        fake_labels.append(13)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 4:\n",
    "        fake_labels.append(14)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 5:\n",
    "        fake_labels.append(15)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 5:\n",
    "        fake_labels.append(16)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 5:\n",
    "        fake_labels.append(17)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 6:\n",
    "        fake_labels.append(18)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 6:\n",
    "        fake_labels.append(19)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 6:\n",
    "        fake_labels.append(20)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 7:\n",
    "        fake_labels.append(21)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 7:\n",
    "        fake_labels.append(22)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 7:\n",
    "        fake_labels.append(23)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 8:\n",
    "        fake_labels.append(24)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 8:\n",
    "        fake_labels.append(25)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 8:\n",
    "        fake_labels.append(26)\n",
    "        \n",
    "fake_labels = np.asarray(fake_labels)\n",
    "to_gen = torch.cat((latent_vectors, act_vectors[1], usr_vectors[1]), 1)\n",
    "fake_features = gen(to_gen).detach()\n",
    "#fake_features = gen(to_gen).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  1  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0 15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  1  0  0 12  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  7  0  0  0  0  0  1  0  0  1  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  4  0  3  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  1 10  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  4  0  0  3  0  0  0  0  0  2  0  0  0  0  0  0  0  0  1\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 13  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  9  0  0  1  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 15  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  6  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8\n",
      "   0  0  0]\n",
      " [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   7  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0 11  0]\n",
      " [ 0  0  1  0  0  9  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "   0  0  3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.50000   0.90000   0.64286        10\n",
      "           1    0.82353   1.00000   0.90323        14\n",
      "           2    0.80000   1.00000   0.88889         8\n",
      "           3    1.00000   0.16667   0.28571         6\n",
      "           4    0.78947   1.00000   0.88235        15\n",
      "           5    0.46154   0.80000   0.58537        15\n",
      "           6    0.80000   1.00000   0.88889        12\n",
      "           7    1.00000   1.00000   1.00000        12\n",
      "           8    0.63636   0.77778   0.70000         9\n",
      "           9    1.00000   0.12500   0.22222         8\n",
      "          10    0.90909   1.00000   0.95238        10\n",
      "          11    1.00000   1.00000   1.00000        10\n",
      "          12    0.00000   0.00000   0.00000         9\n",
      "          13    0.90909   0.90909   0.90909        11\n",
      "          14    0.50000   0.20000   0.28571        10\n",
      "          15    1.00000   1.00000   1.00000        13\n",
      "          16    1.00000   0.81818   0.90000        11\n",
      "          17    0.87500   1.00000   0.93333         7\n",
      "          18    1.00000   1.00000   1.00000        12\n",
      "          19    0.91667   1.00000   0.95652        11\n",
      "          20    1.00000   1.00000   1.00000        15\n",
      "          21    0.90000   1.00000   0.94737         9\n",
      "          22    1.00000   0.85714   0.92308         7\n",
      "          23    0.80000   0.88889   0.84211         9\n",
      "          24    1.00000   0.77778   0.87500         9\n",
      "          25    1.00000   1.00000   1.00000        11\n",
      "          26    1.00000   0.21429   0.35294        14\n",
      "\n",
      "    accuracy                        0.81533       287\n",
      "   macro avg    0.83781   0.79388   0.77322       287\n",
      "weighted avg    0.83840   0.81533   0.78877       287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5, zero_division = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
